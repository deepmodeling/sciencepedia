{"hands_on_practices": [{"introduction": "The method of steepest descent is a powerful analytical tool for approximating integrals dominated by a large parameter. The core idea is that the value of the integral is overwhelmingly determined by the contributions from points where the exponent reaches its minimum. This first exercise [@problem_id:920264] provides a clean, foundational example of applying this principle, guiding you through the process of identifying these critical points and summing their contributions to find the integral's leading-order behavior.", "problem": "Consider the integral given by\n$$I(\\lambda) = \\int_{-\\infty}^\\infty \\exp\\left[-\\lambda(t^2-a^2)^2\\right] dt$$\nwhere $\\lambda$ is a large positive parameter and $a$ is a positive real constant. The evaluation of such integrals in the limit of large $\\lambda$ is a common task in various fields of physics and engineering, often solvable using the method of steepest descent (also known as the saddle-point method).\n\nYour task is to find the leading-order asymptotic expression for $I(\\lambda)$ as $\\lambda \\to \\infty$.", "solution": "The problem is to find the asymptotic behavior of the integral $I(\\lambda) = \\int_{-\\infty}^\\infty \\exp\\left[-\\lambda(t^2-a^2)^2\\right] dt$ for $\\lambda \\to \\infty$. This integral is in the standard form for the method of steepest descent, $\\int e^{-\\lambda \\phi(t)} dt$, where the phase function is $\\phi(t) = (t^2-a^2)^2$.\n\nThe method of steepest descent approximates the integral by contributions from the saddle points of the integrand, which correspond to the minima of the phase function $\\phi(t)$.\n\n1.  **Find the saddle points:**\n    The saddle points are the stationary points of $\\phi(t)$, found by setting its first derivative to zero.\n    $$\\phi'(t) = \\frac{d}{dt}(t^2-a^2)^2 = 2(t^2-a^2)(2t) = 4t(t^2-a^2)$$\n    Setting $\\phi'(t) = 0$ gives $4t(t-a)(t+a) = 0$. The saddle points are located at $t_1 = 0$, $t_2 = a$, and $t_3 = -a$.\n\n2.  **Evaluate the phase function at the saddle points:**\n    To determine which saddle points give the dominant contribution, we evaluate $\\phi(t)$ at each point. The integral is dominated by the points where $\\phi(t)$ is at its minimum value, since the exponential term $e^{-\\lambda \\phi(t)}$ will be largest there.\n    $$\\phi(t_1) = \\phi(0) = (0^2 - a^2)^2 = a^4$$\n    $$\\phi(t_2) = \\phi(a) = (a^2 - a^2)^2 = 0$$\n    $$\\phi(t_3) = \\phi(-a) = ((-a)^2 - a^2)^2 = 0$$\n    Since $a$ is a positive constant, $a^4 > 0$. The minimum value of $\\phi(t)$ is $0$, which occurs at the two saddle points $t = a$ and $t = -a$. Therefore, the leading-order asymptotic behavior of the integral is given by the sum of the contributions from the neighborhoods of these two points. The contribution from $t=0$ will be suppressed by a factor of $e^{-\\lambda a^4}$ and is thus negligible for large $\\lambda$.\n\n3.  **Analyze the behavior near the dominant saddle points:**\n    The contribution of a saddle point $t_k$ to the integral is given by the Gaussian integral approximation in its vicinity. The standard formula for the leading-order contribution from a simple saddle point $t_k$ is:\n    $$I_k(\\lambda) \\sim \\sqrt{\\frac{2\\pi}{\\lambda \\phi''(t_k)}} e^{-\\lambda \\phi(t_k)}$$\n    We need the second derivative of $\\phi(t)$:\n    $$\\phi''(t) = \\frac{d}{dt}(4t^3 - 4a^2t) = 12t^2 - 4a^2$$\n    Now, we evaluate $\\phi''(t)$ at the two dominant saddle points:\n    For $t=a$:\n    $$\\phi''(a) = 12a^2 - 4a^2 = 8a^2$$\n    For $t=-a$:\n    $$\\phi''(-a) = 12(-a)^2 - 4a^2 = 12a^2 - 4a^2 = 8a^2$$\n    Since $\\phi''(a)$ and $\\phi''(-a)$ are positive, these points are indeed minima along the real axis, and the path of integration (the real line) is a path of steepest descent through these saddles.\n\n4.  **Calculate the contributions and sum them:**\n    The contribution from the saddle point at $t=a$ is:\n    $$I_a(\\lambda) \\sim \\sqrt{\\frac{2\\pi}{\\lambda \\phi''(a)}} e^{-\\lambda \\phi(a)} = \\sqrt{\\frac{2\\pi}{\\lambda (8a^2)}} e^{-\\lambda(0)} = \\sqrt{\\frac{\\pi}{4\\lambda a^2}} = \\frac{\\sqrt{\\pi}}{2a\\sqrt{\\lambda}}$$\n    The contribution from the saddle point at $t=-a$ is:\n    $$I_{-a}(\\lambda) \\sim \\sqrt{\\frac{2\\pi}{\\lambda \\phi''(-a)}} e^{-\\lambda \\phi(-a)} = \\sqrt{\\frac{2\\pi}{\\lambda (8a^2)}} e^{-\\lambda(0)} = \\frac{\\sqrt{\\pi}}{2a\\sqrt{\\lambda}}$$\n    The total leading-order asymptotic approximation for the integral is the sum of these two contributions:\n    $$I(\\lambda) \\sim I_a(\\lambda) + I_{-a}(\\lambda) = \\frac{\\sqrt{\\pi}}{2a\\sqrt{\\lambda}} + \\frac{\\sqrt{\\pi}}{2a\\sqrt{\\lambda}} = \\frac{2\\sqrt{\\pi}}{2a\\sqrt{\\lambda}} = \\frac{\\sqrt{\\pi}}{a\\sqrt{\\lambda}}$$", "answer": "$$\\boxed{\\frac{\\sqrt{\\pi}}{a\\sqrt{\\lambda}}}$$", "id": "920264"}, {"introduction": "Building on the fundamental case, this practice introduces a common real-world complexity: a finite domain of integration. When the integration range is bounded, the global minimum of the exponent might occur either at a stationary point within the interval or at one of its endpoints. This problem [@problem_id:920333] requires a careful analysis to compare the magnitudes of these different types of contributions, a crucial skill for correctly applying the method in more constrained scenarios.", "problem": "Consider the integral $I(\\lambda)$ defined by\n$$\nI(\\lambda) = \\int_{-1/2}^{2} \\exp\\left[\\lambda\\left(3t^2 - 2t^3\\right)\\right] dt\n$$\nwhere $\\lambda$ is a large, positive, real parameter.\n\nDetermine the leading-order asymptotic behavior of $I(\\lambda)$ as $\\lambda \\to \\infty$. Your answer should be a closed-form analytic expression in terms of $\\lambda$ and standard mathematical constants.", "solution": "We apply Laplace’s method to \n$$I(\\lambda)=\\int_{-1/2}^{2}e^{\\lambda f(t)}dt,\\qquad f(t)=3t^2-2t^3.$$\n1. Critical points: \n$$f'(t)=6t-6t^2=6t(1-t)=0\\implies t=0,1.$$\nEndpoints are $t=-1/2,\\,2.$  \n2. Values of $f$: \n$$f(-\\tfrac12)=\\tfrac34+ \\tfrac14=1,\\quad f(0)=0,\\quad f(1)=1,\\quad f(2)=-4.$$\nHence the global maxima occur at $t=-\\tfrac12$ and $t=1$, both giving $f=1.$  \n3. Contribution from the interior maximum $t=1$: \n$$f''(1)=6-12=-6<0,\\quad \n\\hbox{so }\n\\int_{\\,\\text{near }1}e^{\\lambda f(t)}dt\\sim e^{\\lambda}\\sqrt{\\frac{2\\pi}{\\lambda\\,|f''(1)|}}\n=e^{\\lambda}\\sqrt{\\frac{2\\pi}{6\\lambda}}\n=e^{\\lambda}\\sqrt{\\frac{\\pi}{3\\lambda}}\\,. $$\n4. Contribution from the endpoint maximum $t=-\\tfrac12$: since $f'(-\\tfrac12)=-\\tfrac92<0$, \n$$\\int_{-1/2}^{\\,\\text{near }-1/2}e^{\\lambda f(t)}dt\\sim\n\\frac{e^{\\lambda f(-1/2)}}{\\lambda|f'(-1/2)|}\n=\\frac{e^{\\lambda}}{\\lambda\\,(9/2)}\n=\\frac{2}{9\\lambda}e^{\\lambda}\\,. $$\n5. Summing and retaining the dominant term as $\\lambda\\to\\infty$ gives\n$$I(\\lambda)\\sim e^{\\lambda}\\sqrt{\\frac{\\pi}{3\\lambda}}+\\frac{2}{9\\lambda}e^{\\lambda}\n\\sim e^{\\lambda}\\sqrt{\\frac{\\pi}{3\\lambda}}\\,. $$", "answer": "$$\\boxed{e^{\\lambda}\\sqrt{\\frac{\\pi}{3\\lambda}}}$$", "id": "920333"}, {"introduction": "The principle of moving in the direction of steepest descent extends far beyond analytical approximations into the realm of computational algorithms. This hands-on coding practice [@problem_id:3159949] bridges the gap between theory and application by asking you to implement the numerical method of steepest descent (often called gradient descent) to find the minimum of a function. By comparing its performance and robustness against another classic technique, Newton's method, you will gain practical insight into the foundations of modern iterative optimization.", "problem": "You are to implement and compare two iterative optimization methods on a smooth scalar field, strictly from first principles: the method of steepest descent and Newton’s method. The target function is the two-dimensional real-valued function defined by $f(\\mathbf{x}) = f(x,y) = x^{2} + y^{4}$. The aim is to demonstrate, by explicit computation, both the underlying principles and the numerical behavior of the methods, including a case where Newton’s method fails because the Hessian matrix is not positive definite in the sense of symmetric positive definiteness (Symmetric Positive Definite (SPD)).\n\nYour tasks are:\n\n- Use only the base definitions from multivariable calculus: differentiability, directional derivative, gradient as the direction of steepest ascent in the Euclidean norm, and the second-order Taylor approximation to derive the search directions chosen by the method of steepest descent and by Newton’s method. Do not assume any prepackaged iteration formula.\n- Implement the method of steepest descent with a backtracking line search that enforces a standard sufficient decrease condition based on a first-order model. The method must be stable and produce a monotonically decreasing sequence of function values for this problem when started from any of the provided initial points.\n- Implement Newton’s method as the minimization of the quadratic model obtained from the second-order Taylor expansion; at each iteration, compute the step by solving a linear system. Declare Newton’s method as failed at an iterate if the Hessian is not invertible (for example, not SPD in a way that prevents solving the linear system), making the Newton step undefined.\n- Use the Euclidean norm in $\\mathbb{R}^{2}$. Use the following generic stopping criterion for both methods: stop successfully when $\\lVert \\nabla f(\\mathbf{x}_{k}) \\rVert_{2} \\le \\varepsilon$ with tolerance $\\varepsilon = 10^{-10}$. Use a maximum of $50$ iterations for Newton’s method and a maximum of $10000$ iterations for steepest descent. For the backtracking line search in steepest descent, use an initial step length $\\alpha_{0} = 1$, a sufficient decrease parameter $c_{1} = 10^{-4}$, and a contraction factor $\\tau = \\tfrac{1}{2}$. If the step length falls below $10^{-16}$ without satisfying the sufficient decrease condition, declare the steepest descent method as failed at that iteration.\n- The program must treat the Hessian singularity explicitly: if the linear system for Newton’s step cannot be solved because the Hessian is singular (or otherwise prevents a unique solution), report a failure for that test case in Newton’s method.\n\nTest suite:\nImplement your program to run the following initial points $(x_{0},y_{0})$:\n- Case A (general “happy path”): $(x_{0},y_{0}) = (1,1)$.\n- Case B (Hessian singular along one axis): $(x_{0},y_{0}) = (1,0)$.\n- Case C (stationary optimum already at start): $(x_{0},y_{0}) = (0,0)$.\n- Case D (negative coordinate, quartic asymmetry test): $(x_{0},y_{0}) = (5,-1)$.\n\nQuantifiable outputs:\nFor each test case $i$ in the order A, B, C, D, compute and report the following six values in order:\n- $n\\_status\\_i$: an integer where $1$ means Newton’s method converged within the iteration limit, and $0$ means it failed to converge or could not compute a step due to a non-invertible Hessian.\n- $n\\_iters\\_i$: the integer number of iterations actually performed by Newton’s method when it stopped (either by convergence or by failure).\n- $n\\_f\\_i$: the final function value $f$ at termination of Newton’s method as a floating-point number.\n- $sd\\_status\\_i$: an integer where $1$ means the method of steepest descent converged within the iteration limit, and $0$ means it failed to converge under the given line search rules.\n- $sd\\_iters\\_i$: the integer number of iterations actually performed by the method of steepest descent when it stopped.\n- $sd\\_f\\_i$: the final function value $f$ at termination of the method of steepest descent as a floating-point number.\n\nFinal output format:\nYour program should produce a single line of output containing the concatenated results for all four cases as a comma-separated list enclosed in square brackets. That is, the output must be\n$[n\\_status\\_A,n\\_iters\\_A,n\\_f\\_A,sd\\_status\\_A,sd\\_iters\\_A,sd\\_f\\_A,n\\_status\\_B,n\\_iters\\_B,n\\_f\\_B,sd\\_status\\_B,sd\\_iters\\_B,sd\\_f\\_B,n\\_status\\_C,n\\_iters\\_C,n\\_f\\_C,sd\\_status\\_C,sd\\_iters\\_C,sd\\_f\\_C,n\\_status\\_D,n\\_iters\\_D,n\\_f\\_D,sd\\_status\\_D,sd\\_iters\\_D,sd\\_f\\_D]$.", "solution": "The problem requires the implementation and comparative analysis of two fundamental iterative optimization algorithms, the method of steepest descent and Newton's method, applied to the minimization of the multivariate function $f(\\mathbf{x}) = f(x, y) = x^2 + y^4$. The derivation of the iterative steps must be performed from first principles of multivariable calculus, and the implementation must adhere to specified parameters and handle a specific failure case for Newton's method involving a singular Hessian matrix.\n\nFirst, we establish the necessary mathematical objects for the function $f(\\mathbf{x})$ where $\\mathbf{x} = [x, y]^T \\in \\mathbb{R}^2$. The function is $f(x, y) = x^2 + y^4$.\nThe gradient of $f$, which is a vector of its partial derivatives, is required for both methods. The gradient points in the direction of the steepest local ascent.\n$$\n\\nabla f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{bmatrix} = \\begin{bmatrix} 2x \\\\ 4y^3 \\end{bmatrix}\n$$\nA necessary condition for a point $\\mathbf{x}^*$ to be a minimizer is that the gradient vanishes, $\\nabla f(\\mathbf{x}^*) = \\mathbf{0}$. For the given function, this condition is $2x = 0$ and $4y^3 = 0$, which uniquely yields the point $(0, 0)$. Thus, $\\mathbf{x}^* = (0, 0)^T$ is the only stationary point, and since $f(\\mathbf{x}) > 0$ for all $\\mathbf{x} \\neq \\mathbf{0}$ and $f(\\mathbf{0}) = 0$, it is the global minimum.\n\nFor Newton's method, the Hessian matrix, which is the matrix of second-order partial derivatives, is also required.\n$$\nH_f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial y^2} \\end{bmatrix} = \\begin{bmatrix} 2 & 0 \\\\ 0 & 12y^2 \\end{bmatrix}\n$$\nThe Hessian matrix describes the local curvature of the function.\n\n**Method of Steepest Descent**\n\nThe method of steepest descent is an iterative algorithm that moves in the direction opposite to the gradient at each step. This direction, $-\\nabla f(\\mathbf{x})$, is the direction of steepest local descent.\nThe derivation begins with the first-order Taylor approximation of $f$ around a point $\\mathbf{x}_k$:\n$$\nf(\\mathbf{x}_k + \\mathbf{p}) \\approx f(\\mathbf{x}_k) + \\nabla f(\\mathbf{x}_k)^T \\mathbf{p}\n$$\nTo achieve the most significant decrease in $f$ for a step $\\mathbf{p}$ of a fixed small length, say $\\lVert \\mathbf{p} \\rVert_2 = \\delta$, we must minimize $\\nabla f(\\mathbf{x}_k)^T \\mathbf{p}$. By the Cauchy-Schwarz inequality, this dot product is minimized when $\\mathbf{p}$ is antiparallel to $\\nabla f(\\mathbf{x}_k)$. Thus, the search direction $\\mathbf{p}_k$ is chosen as:\n$$\n\\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)\n$$\nThe iterative update rule is therefore:\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k = \\mathbf{x}_k - \\alpha_k \\nabla f(\\mathbf{x}_k)\n$$\nwhere $\\alpha_k > 0$ is the step length. An appropriate value for $\\alpha_k$ is crucial for convergence. A backtracking line search is employed to find a suitable $\\alpha_k$. Starting with an initial guess, $\\alpha = \\alpha_0$, the step length is repeatedly reduced by a contraction factor $\\tau$ until the Armijo sufficient decrease condition is met:\n$$\nf(\\mathbf{x}_k + \\alpha \\mathbf{p}_k) \\le f(\\mathbf{x}_k) + c_1 \\alpha \\nabla f(\\mathbf{x}_k)^T \\mathbf{p}_k\n$$\nFor our choice of $\\mathbf{p}_k = - \\nabla f(\\mathbf{x}_k)$, this simplifies to:\n$$\nf(\\mathbf{x}_k - \\alpha \\nabla f(\\mathbf{x}_k)) \\le f(\\mathbf{x}_k) - c_1 \\alpha \\lVert \\nabla f(\\mathbf{x}_k) \\rVert_2^2\n$$\nThe parameters for the line search are specified as $\\alpha_0 = 1$, $c_1 = 10^{-4}$, and $\\tau = 1/2$. The method terminates when $\\lVert \\nabla f(\\mathbf{x}_k) \\rVert_2 \\le \\varepsilon = 10^{-10}$ or if the maximum number of iterations, $10000$, is reached. A failure is declared if the step length $\\alpha$ becomes smaller than $10^{-16}$ during the line search without satisfying the condition.\n\n**Newton's Method**\n\nNewton's method uses a more sophisticated model of the function by employing the second-order Taylor expansion around the current iterate $\\mathbf{x}_k$:\n$$\nf(\\mathbf{x}_k + \\mathbf{p}) \\approx m_k(\\mathbf{p}) = f(\\mathbf{x}_k) + \\nabla f(\\mathbf{x}_k)^T \\mathbf{p} + \\frac{1}{2}\\mathbf{p}^T H_f(\\mathbf{x}_k) \\mathbf{p}\n$$\nThe method determines the next step $\\mathbf{p}_k$ by finding the minimizer of this quadratic model $m_k(\\mathbf{p})$. A necessary condition for $\\mathbf{p}_k$ to minimize $m_k(\\mathbf{p})$ is that the gradient of the model with respect to $\\mathbf{p}$ is zero:\n$$\n\\nabla m_k(\\mathbf{p}) = \\nabla f(\\mathbf{x}_k) + H_f(\\mathbf{x}_k) \\mathbf{p} = \\mathbf{0}\n$$\nThis yields the Newton system, a system of linear equations for the step $\\mathbf{p}_k$:\n$$\nH_f(\\mathbf{x}_k) \\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)\n$$\nThe next iterate is then $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\mathbf{p}_k$. No line search is required in this pure form of the method (step length $\\alpha_k = 1$ is always used).\n\nA unique solution for $\\mathbf{p}_k$ exists if and only if the Hessian matrix $H_f(\\mathbf{x}_k)$ is invertible. If $H_f(\\mathbf{x}_k)$ is singular (not invertible), the Newton step is not well-defined, and the method fails. For the function $f(x,y) = x^2 + y^4$, the Hessian $H_f(\\mathbf{x}) = \\text{diag}(2, 12y^2)$ has determinant $\\det(H_f(\\mathbf{x})) = 24y^2$. The Hessian is singular if and only if $y=0$. Therefore, Newton's method will fail if an iterate $\\mathbf{x}_k$ has its y-component equal to zero, unless the gradient is already zero at that point.\n\nThe method terminates when $\\lVert \\nabla f(\\mathbf{x}_k) \\rVert_2 \\le \\varepsilon = 10^{-10}$ or if the maximum number of iterations, $50$, is reached. Failure is reported if the Newton system cannot be solved due to a singular Hessian.\n\n**Analysis of Test Cases**\n\n- **Case A: $\\mathbf{x}_0 = (1, 1)^T$.** Here, $y_0 \\ne 0$, so the initial Hessian $H_f(1, 1) = \\text{diag}(2, 12)$ is positive-definite. Both methods are expected to converge to the minimum at $(0, 0)^T$. Newton's method is expected to exhibit quadratic convergence and be significantly faster.\n\n- **Case B: $\\mathbf{x}_0 = (1, 0)^T$.** At this point, $y_0 = 0$. The gradient is $\\nabla f(1, 0) = [2, 0]^T \\neq \\mathbf{0}$, so we are not at the minimum. The Hessian is $H_f(1, 0) = \\text{diag}(2, 0)$, which is singular. The Newton system $H_f(\\mathbf{x}_0) \\mathbf{p}_0 = -\\nabla f(\\mathbf{x}_0)$ is inconsistent and does not have a unique solution. Therefore, Newton's method is expected to fail at the first iteration. Steepest descent will proceed with search direction $\\mathbf{p}_0 = -[2, 0]^T$, moving along the x-axis towards the origin, and is expected to converge.\n\n- **Case C: $\\mathbf{x}_0 = (0, 0)^T$.** This is the optimal solution. The initial gradient is $\\nabla f(0, 0) = [0, 0]^T$, and its norm is $0$, which satisfies the stopping criterion. Both methods should terminate immediately with $0$ iterations.\n\n- **Case D: $\\mathbf{x}_0 = (5, -1)^T$.** Similar to Case A, $y_0 \\ne 0$, so the initial Hessian is positive-definite. Both methods are expected to converge, with Newton's method being much faster. The negative y-coordinate has no qualitative effect since the function depends on $y^4$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the optimization algorithms on the specified test cases\n    and print the results in the required format.\n    \"\"\"\n\n    # --- Problem Definition ---\n    def f(x_vec):\n        \"\"\"Target function f(x, y) = x^2 + y^4.\"\"\"\n        x, y = x_vec\n        return x**2 + y**4\n\n    def grad_f(x_vec):\n        \"\"\"Gradient of the target function.\"\"\"\n        x, y = x_vec\n        return np.array([2 * x, 4 * y**3], dtype=float)\n\n    def hess_f(x_vec):\n        \"\"\"Hessian of the target function.\"\"\"\n        x, y = x_vec\n        return np.array([[2.0, 0.0], [0.0, 12.0 * y**2]], dtype=float)\n\n    # --- Newton's Method Implementation ---\n    def newtons_method(x0, tol=1e-10, max_iter=50):\n        \"\"\"\n        Implements Newton's method for optimization.\n\n        Returns:\n            (status, iterations, final_f_val)\n            status: 1 for convergence, 0 for failure.\n            iterations: Number of iterations performed.\n            final_f_val: Function value at the final point.\n        \"\"\"\n        xk = np.array(x0, dtype=float)\n        \n        for k in range(max_iter):\n            gk = grad_f(xk)\n            \n            # Check for convergence\n            if np.linalg.norm(gk) = tol:\n                return 1, k, f(xk)\n            \n            Hk = hess_f(xk)\n            \n            try:\n                # Solve the Newton system: Hk * p = -gk\n                pk = np.linalg.solve(Hk, -gk)\n            except np.linalg.LinAlgError:\n                # Failure due to singular Hessian\n                return 0, k, f(xk)\n                \n            # Update step\n            xk = xk + pk\n            \n        # Failure due to exceeding max iterations\n        gk = grad_f(xk)\n        if np.linalg.norm(gk) = tol:\n             return 1, max_iter, f(xk)\n        return 0, max_iter, f(xk)\n\n    # --- Steepest Descent Implementation ---\n    def steepest_descent(x0, tol=1e-10, max_iter=10000):\n        \"\"\"\n        Implements the method of steepest descent with backtracking line search.\n\n        Returns:\n            (status, iterations, final_f_val)\n            status: 1 for convergence, 0 for failure.\n            iterations: Number of iterations performed.\n            final_f_val: Function value at the final point.\n        \"\"\"\n        xk = np.array(x0, dtype=float)\n        \n        # Line search parameters\n        alpha0 = 1.0\n        c1 = 1e-4\n        tau = 0.5\n        alpha_min = 1e-16\n\n        for k in range(max_iter):\n            fk = f(xk)\n            gk = grad_f(xk)\n            \n            # Check for convergence\n            if np.linalg.norm(gk) = tol:\n                return 1, k, fk\n            \n            # Search direction\n            pk = -gk\n            \n            # Backtracking line search\n            alpha = alpha0\n            gk_pk_dot = np.dot(gk, pk)\n            \n            while f(xk + alpha * pk) > fk + c1 * alpha * gk_pk_dot:\n                alpha *= tau\n                if alpha  alpha_min:\n                    return 0, k, fk # Line search failure\n            \n            # Update step\n            xk = xk + alpha * pk\n\n        # Failure due to exceeding max iterations\n        fk = f(xk)\n        gk = grad_f(xk)\n        if np.linalg.norm(gk) = tol:\n             return 1, max_iter, fk\n        return 0, max_iter, fk\n\n    # --- Test Suite ---\n    test_cases = [\n        (1.0, 1.0),   # Case A\n        (1.0, 0.0),   # Case B\n        (0.0, 0.0),   # Case C\n        (5.0, -1.0),  # Case D\n    ]\n\n    results = []\n    for x0 in test_cases:\n        # Run Newton's method\n        n_status, n_iters, n_f_val = newtons_method(x0)\n        results.extend([n_status, n_iters, n_f_val])\n        \n        # Run Steepest Descent\n        sd_status, sd_iters, sd_f_val = steepest_descent(x0)\n        results.extend([sd_status, sd_iters, sd_f_val])\n\n    # --- Final Output Formatting ---\n    # Python's default float representation is sufficient here.\n    output_str = ','.join(map(str, results))\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "3159949"}]}