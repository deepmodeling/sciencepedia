{"hands_on_practices": [{"introduction": "Gradient descent can struggle when the optimization landscape is poorly scaled, resembling a long, narrow valley where the curvature varies drastically along different directions. In such cases, the algorithm often takes an inefficient zig-zag path towards the minimum. This exercise [@problem_id:3139501] allows you to construct a controlled two-dimensional example of this ill-conditioning and demonstrate a powerful remedy: rotating the coordinate system to align with the valley's principal axes, which dramatically accelerates convergence by decoupling the optimization problem.", "problem": "You will implement, analyze, and compare two variants of the gradient descent method on a convex quadratic in two dimensions. The goal is to construct a mathematically controlled case where the basic method exhibits a zig-zag trajectory due to anisotropy (ill-conditioning), and then demonstrate how a rotation-based change of variables combined with axis-aligned per-coordinate step sizes aligns the update directions with the principal axes of the contours and accelerates convergence.\n\nStart from the following core definitions and well-tested facts in numerical optimization and linear algebra:\n- A twice continuously differentiable function with a symmetric positive definite (Positive Definite (PD)) Hessian matrix is strongly convex. In particular, the quadratic function $$f(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^{\\top} \\mathbf{Q}\\,\\mathbf{x}$$ with symmetric PD matrix $$\\mathbf{Q} \\in \\mathbb{R}^{2\\times 2}$$ has a unique minimizer at $$\\mathbf{x}^{\\star} = \\mathbf{0}$$ and gradient $$\\nabla f(\\mathbf{x}) = \\mathbf{Q}\\,\\mathbf{x}.$$\n- The gradient descent update with a fixed step size $$\\alpha \\in (0, 2/L)$$ for minimizing $$f$$ is $$\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\alpha\\,\\nabla f(\\mathbf{x}_{k}).$$ For the quadratic above, this is $$\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\alpha\\,\\mathbf{Q}\\,\\mathbf{x}_{k}.$$\n- For an orthogonal rotation matrix $$\\mathbf{R}(\\theta) = \\begin{bmatrix}\\cos\\theta  -\\sin\\theta\\\\ \\sin\\theta  \\cos\\theta\\end{bmatrix}$$ with angle $$\\theta$$ measured in radians, the transformation $$\\mathbf{y} = \\mathbf{R}^{\\top}\\mathbf{x}$$ changes coordinates without changing Euclidean norms, that is $$\\|\\mathbf{y}\\|_2 = \\|\\mathbf{x}\\|_2.$$\n\nYou will consider a family of convex quadratics whose Hessians are constructed by rotating a diagonal matrix of eigenvalues. Let $$\\lambda_{1}  \\lambda_{2}  0$$ and set $$\\mathbf{\\Lambda} = \\mathrm{diag}(\\lambda_{1}, \\lambda_{2}), \\quad \\mathbf{Q} = \\mathbf{R}(\\theta)\\,\\mathbf{\\Lambda}\\,\\mathbf{R}(\\theta)^{\\top}.$$ Large ratios $$\\lambda_{1}/\\lambda_{2}$$ induce anisotropy (ill-conditioning), which causes the basic gradient descent method to take a zig-zag path in the original coordinate system when $$\\theta \\neq 0.$$\n\nYour tasks are:\n- Implement baseline gradient descent in the original coordinates with a fixed step size $$\\alpha = 1/\\lambda_{1}$$, so that $$\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\alpha\\,\\mathbf{Q}\\,\\mathbf{x}_{k}.$$ Use initial condition $$\\mathbf{x}_{0} = (1,\\,1)^{\\top}$$. Terminate when the Euclidean norm satisfies $$\\|\\mathbf{x}_k\\|_2 \\le \\varepsilon.$$\n- Implement a rotated-coordinate, axis-aligned, per-coordinate step-size variant. Define the rotated variable $$\\mathbf{y} = \\mathbf{R}(\\theta)^{\\top}\\mathbf{x}$$ and update in the rotated coordinates using per-axis learning rates $$\\alpha_1 = c/\\lambda_1, \\ \\alpha_2 = c/\\lambda_2$$ with a shared scalar $$c \\in (0,2)$$, that is, $$\\mathbf{y}_{k+1} = \\mathbf{y}_{k} - \\mathrm{diag}(\\alpha_1, \\alpha_2)\\,\\nabla_{\\mathbf{y}} f(\\mathbf{y}_{k}), \\quad \\nabla_{\\mathbf{y}} f(\\mathbf{y}) = \\mathbf{\\Lambda}\\,\\mathbf{y}.$$ Use the same initial condition expressed in rotated coordinates $$\\mathbf{y}_{0} = \\mathbf{R}(\\theta)^{\\top}\\mathbf{x}_{0}$$, and terminate when $$\\|\\mathbf{y}_k\\|_2 \\le \\varepsilon.$$ Note that this choice aligns updates with the principal axes and decouples the dynamics along each axis. You must use the same $$\\varepsilon$$ as in the baseline.\n- For each test case below, compute the number of iterations $$N_{\\mathrm{plain}}$$ required by the baseline and $$N_{\\mathrm{rot}}$$ required by the rotated-coordinate, per-axis variant, and report the speedup factor defined as the float $$S = N_{\\mathrm{plain}}/N_{\\mathrm{rot}}.$$\n\nAngle unit specification: all angles $$\\theta$$ are provided in radians and must be interpreted in radians.\n\nNumerical termination and safety: if the stopping criterion is not met within $$10^{6}$$ iterations for either method, stop and use the achieved count; however, all the provided test cases are well within this limit.\n\nTest suite (happy path, significant anisotropy; moderate anisotropy; near-isotropic edge case):\n- Test $$1$$: $$(\\lambda_1, \\lambda_2, \\theta, \\varepsilon, c) = (200.0,\\,1.0,\\,\\pi/4,\\,10^{-8},\\,1.6).$$\n- Test $$2$$: $$(\\lambda_1, \\lambda_2, \\theta, \\varepsilon, c) = (10.0,\\,1.0,\\,0.4,\\,10^{-8},\\,1.6).$$\n- Test $$3$$: $$(\\lambda_1, \\lambda_2, \\theta, \\varepsilon, c) = (1.0,\\,1.0,\\,0.7,\\,10^{-8},\\,1.6).$$\n\nYour program must:\n- Construct $$\\mathbf{Q}$$ from $$(\\lambda_1, \\lambda_2, \\theta)$$ as specified.\n- Run both methods from $$\\mathbf{x}_{0} = (1,\\,1)^{\\top}$$ using the corresponding step rules and stopping conditions.\n- For each test, compute $$S = N_{\\mathrm{plain}}/N_{\\mathrm{rot}}$$ and round to $$3$$ decimal places.\n\nFinal output format: Your program should produce a single line of output containing the three rounded results as a comma-separated list enclosed in square brackets, for example $$[S_{1},S_{2},S_{3}]$$, with each $$S_{i}$$ being a float rounded to $$3$$ decimals. No other text should be printed.", "solution": "The user-provided problem statement is valid. It is scientifically grounded in the principles of numerical optimization and linear algebra, well-posed with all necessary parameters and conditions defined, and objective in its formulation. The task is to analyze the performance of two gradient descent variants on a two-dimensional convex quadratic function, which is a classic and instructive problem in computational science.\n\nThe solution proceeds by first implementing the baseline gradient descent method and analyzing its behavior, then implementing the coordinate-rotated variant and analyzing its improved convergence properties in the face of anisotropy. Finally, for each test case, the number of iterations for both methods is computed to determine the speedup factor.\n\nThe objective function for this problem is the convex quadratic form $f(\\mathbf{x}) = \\frac{1}{2}\\,\\mathbf{x}^{\\top} \\mathbf{Q}\\,\\mathbf{x}$. The unique minimizer is at $\\mathbf{x}^{\\star} = \\mathbf{0}$, and the gradient is $\\nabla f(\\mathbf{x}) = \\mathbf{Q}\\,\\mathbf{x}$. The Hessian matrix $\\mathbf{Q}$ is constructed as $\\mathbf{Q} = \\mathbf{R}(\\theta)\\,\\mathbf{\\Lambda}\\,\\mathbf{R}(\\theta)^{\\top}$, where $\\mathbf{\\Lambda} = \\mathrm{diag}(\\lambda_{1}, \\lambda_{2})$ with eigenvalues $\\lambda_{1}  \\lambda_{2}  0$, and $\\mathbf{R}(\\theta)$ is the standard $2\\times 2$ rotation matrix for an angle $\\theta$. This construction creates a function whose level sets are ellipses with principal axes aligned with the columns of $\\mathbf{R}(\\theta)$. The ratio $\\kappa = \\lambda_{1}/\\lambda_{2}$, known as the condition number, measures the anisotropy of these ellipses. A large $\\kappa$ signifies an ill-conditioned problem.\n\n### Method 1: Baseline Gradient Descent\n\nThe standard gradient descent algorithm updates the current iterate $\\mathbf{x}_{k}$ by taking a step in the direction of the negative gradient:\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\alpha\\,\\nabla f(\\mathbf{x}_{k})\n$$\nwhere $\\alpha  0$ is the step size or learning rate. For the given quadratic function, this becomes:\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\alpha\\,\\mathbf{Q}\\,\\mathbf{x}_{k} = (\\mathbf{I} - \\alpha\\,\\mathbf{Q})\\,\\mathbf{x}_{k}\n$$\nThe problem specifies a fixed step size $\\alpha = 1/\\lambda_{\\max}(\\mathbf{Q}) = 1/\\lambda_{1}$. The convergence of this iterative process is determined by the spectral radius of the iteration matrix $\\mathbf{M} = \\mathbf{I} - \\alpha\\,\\mathbf{Q}$. The eigenvalues of $\\mathbf{M}$ are $1-\\alpha\\lambda_{i}$. With $\\alpha=1/\\lambda_1$, the eigenvalues are $1 - \\lambda_1/\\lambda_1 = 0$ and $1 - \\lambda_2/\\lambda_1$. The spectral radius is therefore $\\rho(\\mathbf{M}) = |1 - \\lambda_2/\\lambda_1| = 1 - 1/\\kappa$. When the condition number $\\kappa$ is large, $\\rho(\\mathbf{M})$ is very close to $1$, leading to slow convergence. The algorithm starts from $\\mathbf{x}_{0} = (1,\\,1)^{\\top}$ and terminates when the Euclidean norm $\\|\\mathbf{x}_{k}\\|_{2} \\le \\varepsilon$, yielding the iteration count $N_{\\mathrm{plain}}$.\n\n### Method 2: Rotated-Coordinate Gradient Descent\n\nThis method first performs a change of variables to align the coordinate system with the principal axes of the function's level sets. The new coordinate vector is defined as $\\mathbf{y} = \\mathbf{R}(\\theta)^{\\top}\\mathbf{x}$. The inverse transformation is $\\mathbf{x} = \\mathbf{R}(\\theta)\\mathbf{y}$. Substituting this into the objective function yields:\n$$\nf(\\mathbf{y}) = \\frac{1}{2}\\,\\left(\\mathbf{R}(\\theta)\\mathbf{y}\\right)^{\\top} \\left(\\mathbf{R}(\\theta)\\mathbf{\\Lambda}\\mathbf{R}(\\theta)^{\\top}\\right) \\left(\\mathbf{R}(\\theta)\\mathbf{y}\\right)\n$$\nSince $\\mathbf{R}(\\theta)$ is orthogonal, $\\mathbf{R}(\\theta)^{\\top}\\mathbf{R}(\\theta) = \\mathbf{I}$. The expression simplifies to:\n$$\nf(\\mathbf{y}) = \\frac{1}{2}\\,\\mathbf{y}^{\\top} \\mathbf{R}(\\theta)^{\\top}\\mathbf{R}(\\theta)\\mathbf{\\Lambda}\\mathbf{R}(\\theta)^{\\top}\\mathbf{R}(\\theta)\\mathbf{y} = \\frac{1}{2}\\,\\mathbf{y}^{\\top} \\mathbf{\\Lambda}\\,\\mathbf{y}\n$$\nIn this new coordinate system, the Hessian is the diagonal matrix $\\mathbf{\\Lambda}$, meaning the problem decouples into two independent one-dimensional optimization problems. The gradient in the $\\mathbf{y}$-coordinates is $\\nabla_{\\mathbf{y}} f(\\mathbf{y}) = \\mathbf{\\Lambda}\\mathbf{y}$.\n\nThe update rule uses per-coordinate step sizes, $\\alpha_{1} = c/\\lambda_{1}$ and $\\alpha_{2} = c/\\lambda_{2}$, where $c \\in (0, 2)$ is a shared scalar. The update is:\n$$\n\\mathbf{y}_{k+1} = \\mathbf{y}_{k} - \\mathrm{diag}(\\alpha_{1}, \\alpha_{2})\\,\\nabla_{\\mathbf{y}} f(\\mathbf{y}_{k}) = \\mathbf{y}_{k} - \\mathrm{diag}(c/\\lambda_{1}, c/\\lambda_{2})\\,(\\mathbf{\\Lambda}\\mathbf{y}_{k})\n$$\nSince $\\mathbf{\\Lambda} = \\mathrm{diag}(\\lambda_1, \\lambda_2)$, the product of the diagonal matrices simplifies:\n$$\n\\mathrm{diag}(c/\\lambda_{1}, c/\\lambda_{2})\\,\\mathrm{diag}(\\lambda_{1}, \\lambda_{2}) = \\mathrm{diag}(c, c) = c\\mathbf{I}\n$$\nThe update rule thus becomes remarkably simple:\n$$\n\\mathbf{y}_{k+1} = \\mathbf{y}_{k} - c\\mathbf{I}\\mathbf{y}_{k} = (1-c)\\mathbf{y}_{k}\n$$\nThis is a simple geometric contraction. The convergence rate is $|1-c|$, which is independent of the eigenvalues $\\lambda_1, \\lambda_2$ and thus independent of the original problem's condition number. The algorithm starts from the transformed initial condition $\\mathbf{y}_{0} = \\mathbf{R}(\\theta)^{\\top}\\mathbf{x}_{0}$ and terminates when $\\|\\mathbf{y}_{k}\\|_{2} \\le \\varepsilon$, giving the iteration count $N_{\\mathrm{rot}}$. Note that $\\|\\mathbf{y}_{k}\\|_{2} = \\|\\mathbf{R}(\\theta)^{\\top}\\mathbf{x}_{k}\\|_{2} = \\|\\mathbf{x}_{k}\\|_{2}$ because rotations preserve the Euclidean norm.\n\n### Speedup Calculation\n\nThe performance improvement of the rotated-coordinate method over the baseline is quantified by the speedup factor $S = N_{\\mathrm{plain}}/N_{\\mathrm{rot}}$. For each test case, we implement both algorithms, count their iterations, and compute this ratio.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares two gradient descent methods on a 2D convex quadratic,\n    calculates the speedup factor for three test cases.\n    \"\"\"\n    test_cases = [\n        # (lambda_1, lambda_2, theta, epsilon, c)\n        (200.0, 1.0, np.pi/4, 1e-8, 1.6),\n        (10.0, 1.0, 0.4, 1e-8, 1.6),\n        (1.0, 1.0, 0.7, 1e-8, 1.6),\n    ]\n\n    results = []\n    max_iterations = 1_000_000\n\n    for case in test_cases:\n        lambda_1, lambda_2, theta, epsilon, c = case\n\n        # Common initial condition\n        x0 = np.array([1.0, 1.0])\n\n        # --- Task 1: Baseline Gradient Descent ---\n        \n        # Construct the Hessian matrix Q\n        cos_th, sin_th = np.cos(theta), np.sin(theta)\n        R = np.array([[cos_th, -sin_th], [sin_th, cos_th]])\n        Lambda = np.diag([lambda_1, lambda_2])\n        Q = R @ Lambda @ R.T\n\n        # Set up and run the iteration\n        alpha = 1.0 / lambda_1\n        x_k = np.copy(x0)\n        N_plain = 0\n        while np.linalg.norm(x_k) > epsilon and N_plain  max_iterations:\n            gradient = Q @ x_k\n            x_k = x_k - alpha * gradient\n            N_plain += 1\n\n        # --- Task 2: Rotated-Coordinate Gradient Descent ---\n\n        # Initial condition in rotated coordinates\n        y0 = R.T @ x0\n        \n        # Rate of contraction is (1-c)\n        contraction_factor = 1.0 - c\n        \n        y_k = np.copy(y0)\n        N_rot = 0\n        while np.linalg.norm(y_k) > epsilon and N_rot  max_iterations:\n            # The update rule y_{k+1} = (1-c) * y_k is computationally simpler\n            # than re-calculating the gradient at each step, but we implement\n            # the specified update rule for formal correctness.\n            # grad_y = Lambda @ y_k\n            # step_sizes = np.diag([c / lambda_1, c / lambda_2])\n            # y_k = y_k - step_sizes @ grad_y\n            # The above is equivalent to:\n            y_k = contraction_factor * y_k\n            N_rot += 1\n\n        # --- Task 3: Compute Speedup Factor ---\n        if N_rot > 0:\n            speedup = N_plain / N_rot\n        else:\n            # Handle the case where N_rot could be zero, though not expected\n            # for the given test cases and epsilon > 0.\n            speedup = float('inf') if N_plain > 0 else 1.0\n\n        results.append(round(speedup, 3))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3139501"}, {"introduction": "While linear transformations can fix simple elliptical landscapes, many real-world objective functions feature more complex, curved valleys. The famous Rosenbrock function is a classic benchmark for such challenges, known for its narrow, parabolic trough that is difficult for basic gradient descent to navigate. Through this practice [@problem_id:3139529], you will see how a clever non-linear change of coordinates can \"straighten\" this curved valley, transforming a difficult optimization problem into a much simpler one and highlighting a more general principle of preconditioning.", "problem": "You are asked to examine how coordinate transformations can reshape the geometry of an objective function and thereby accelerate Gradient Descent (GD). Work with a two-variable function whose level sets exhibit a narrow, curved valley. The study must be carried out in purely mathematical and algorithmic terms.\n\nDefine the Rosenbrock function $$f(x,y) = (1 - x)^2 + a\\,(y - x^2)^2$$ where $a$ is a positive real parameter controlling the curvature and narrowness of the valley. The unique minimizer is at $(x,y) = (1,1)$ for any $a  0$. Implement two optimization procedures:\n\n- Standard Gradient Descent (GD) in the original coordinates $(x,y)$: Starting from an initial point $(x_0,y_0)$, iteratively update by moving a step of size $\\alpha  0$ along the negative gradient direction of $f$ until a stopping criterion is met.\n- Transformed Gradient Descent (GD) under a coordinate change that straightens the valley: Use the nonlinear bijection $$T:\\mathbb{R}^2 \\to \\mathbb{R}^2,\\quad (x,y)\\mapsto (u,v) = (x,\\; y - x^2).$$ In these coordinates, the minimizer satisfies $(u,v) = (1,0)$ and the objective becomes $$g(u,v) = f\\big(u,\\,v + u^2\\big) = (1 - u)^2 + a\\,v^2.$$ Starting from $(u_0,v_0) = T(x_0,y_0)$, perform GD updates in $(u,v)$ space using the gradient of $g$, and at each iteration map back to $(x,y)$ through $T^{-1}(u,v) = \\big(u,\\, v + u^2\\big)$ to evaluate proximity to the minimizer in the original coordinates. Use the same step size $\\alpha$ and stopping rule as in the standard GD for a fair comparison.\n\nScientific base for design:\n- The gradient of a differentiable scalar field points in the direction of steepest increase; taking small steps opposite to the gradient reduces the function value locally. The first-order Taylor expansion provides the local linear approximation that motivates a steepest-descent step.\n- Coordinate transformations can alter the conditioning of the optimization landscape. In this problem, the transformation $T$ straightens the curved valley of $f$ along $y = x^2$ into a straight line $v = 0$ for $g$, which can improve convergence characteristics when applying GD in $(u,v)$.\n\nImplementation requirements:\n- Compute gradients analytically for both $f$ and $g$; do not use numerical finite differences.\n- Use a fixed step size $\\alpha$ and a stopping criterion based on proximity to the minimizer: stop when the Euclidean distance $$\\sqrt{(x - 1)^2 + (y - 1)^2}$$ is less than or equal to a tolerance $\\varepsilon$ or when the iteration count reaches a maximum $N_{\\max}$.\n- For robustness, if any iterate becomes non-finite (not a real number), treat the run as not converged and return the maximum iteration count $N_{\\max}$ for that case.\n\nTest suite:\nFor each test case, the parameters are $(a, x_0, y_0, \\alpha, \\varepsilon, N_{\\max})$. Use exactly the following set of test cases to cover a typical case, varying curvature, boundary conditions, and challenging step sizes:\n\n1. $(a, x_0, y_0, \\alpha, \\varepsilon, N_{\\max}) = (100,\\; -1.2,\\; 1.0,\\; 0.001,\\; 10^{-6},\\; 50000)$\n2. $(a, x_0, y_0, \\alpha, \\varepsilon, N_{\\max}) = (1000,\\; -1.2,\\; 1.0,\\; 0.0003,\\; 10^{-6},\\; 50000)$\n3. $(a, x_0, y_0, \\alpha, \\varepsilon, N_{\\max}) = (10,\\; -1.2,\\; 1.0,\\; 0.002,\\; 10^{-6},\\; 50000)$\n4. $(a, x_0, y_0, \\alpha, \\varepsilon, N_{\\max}) = (100,\\; 1.0,\\; 1.0,\\; 0.001,\\; 10^{-12},\\; 1)$\n5. $(a, x_0, y_0, \\alpha, \\varepsilon, N_{\\max}) = (100,\\; -1.2,\\; 1.0,\\; 0.005,\\; 10^{-6},\\; 50000)$\n6. $(a, x_0, y_0, \\alpha, \\varepsilon, N_{\\max}) = (100,\\; 2.0,\\; 2.0,\\; 0.001,\\; 10^{-6},\\; 50000)$\n\nRequired outputs:\n- For each test case, run both methods and return a pair of integers $[n_{\\mathrm{std}}, n_{\\mathrm{trans}}]$ where $n_{\\mathrm{std}}$ is the number of iterations used by standard GD and $n_{\\mathrm{trans}}$ is the number used by transformed GD, each capped at $N_{\\max}$.\n- Your program should produce a single line of output containing the results as a comma-separated list of these pairs, enclosed in square brackets. For example: $$\\texttt{[[n\\_std\\_1,n\\_trans\\_1],[n\\_std\\_2,n\\_trans\\_2],...]}$$\n\nAngles and physical units are not involved in this problem; all quantities are dimensionless real numbers. Ensure that all numerical outputs are integers and adhere exactly to the specified format.", "solution": "The problem statement has been validated and is deemed well-posed, scientifically sound, and computationally tractable. It represents a standard and insightful exercise in numerical optimization, suitable for demonstrating the effect of coordinate transformations on the convergence of gradient-based methods.\n\nThe task is to implement and compare two Gradient Descent (GD) algorithms for minimizing the Rosenbrock function, a common benchmark for optimization algorithms known for its narrow, parabolic valley. The first algorithm is the standard GD in the original Euclidean coordinates. The second employs a coordinate transformation designed to simplify the geometry of the function's level sets, thereby accelerating convergence.\n\nThe Rosenbrock function is given by:\n$$f(x,y) = (1 - x)^2 + a(y - x^2)^2$$\nwhere $a  0$ is a parameter. The unique global minimum is at $(x,y)=(1,1)$, where $f(1,1)=0$.\n\n**1. Standard Gradient Descent (GD)**\n\nThis method iteratively updates the current position $(x_k, y_k)$ by taking a step of size $\\alpha$ in the direction opposite to the gradient, $\\nabla f(x_k, y_k)$. The update rule is:\n$$(x_{k+1}, y_{k+1}) = (x_k, y_k) - \\alpha \\nabla f(x_k, y_k)$$\n\nTo implement this, we must first compute the analytical gradient of $f(x,y)$. The partial derivatives are:\n$$\\frac{\\partial f}{\\partial x} = \\frac{\\partial}{\\partial x} \\left[ (1 - x)^2 + a(y - x^2)^2 \\right] = 2(1 - x)(-1) + a \\cdot 2(y - x^2)(-2x) = -2(1 - x) - 4ax(y - x^2)$$\n$$\\frac{\\partial f}{\\partial y} = \\frac{\\partial}{\\partial y} \\left[ (1 - x)^2 + a(y - x^2)^2 \\right] = a \\cdot 2(y - x^2)(1) = 2a(y - x^2)$$\n\nThe gradient vector is therefore:\n$$\\nabla f(x,y) = \\begin{pmatrix} -2(1 - x) - 4ax(y - x^2) \\\\ 2a(y - x^2) \\end{pmatrix}$$\nThe algorithm proceeds by starting at an initial point $(x_0, y_0)$ and applying the update rule iteratively. The process terminates when the Euclidean distance to the minimizer, $\\sqrt{(x - 1)^2 + (y - 1)^2}$, falls below a tolerance $\\varepsilon$, or if the number of iterations reaches a maximum $N_{\\max}$. The steep, curved walls of the Rosenbrock valley often cause the standard GD algorithm to take many small, zig-zagging steps, leading to slow convergence, especially for large values of $a$.\n\n**2. Transformed Gradient Descent (GD)**\n\nThis method first applies a nonlinear coordinate transformation $T(x,y) = (u,v)$ that \"straightens\" the valley of the Rosenbrock function. The transformation is:\n$$(u, v) = T(x,y) = (x, y - x^2)$$\nThe inverse transformation, which maps from the new coordinates back to the original, is:\n$$(x, y) = T^{-1}(u,v) = (u, v + u^2)$$\n\nSubstituting the inverse transformation into $f(x,y)$ gives the objective function in the new coordinates, $g(u,v)$:\n$$g(u,v) = f(u, v+u^2) = (1 - u)^2 + a((v+u^2) - u^2)^2 = (1 - u)^2 + av^2$$\nThis transformed function $g(u,v)$ is significantly simpler. Its level sets are ellipses centered at the minimizer $(u,v)=(1,0)$, with axes aligned with the coordinate axes. This structure is much more amenable to Gradient Descent.\n\nThe optimization is performed in the $(u,v)$ space. The update rule is:\n$$(u_{k+1}, v_{k+1}) = (u_k, v_k) - \\alpha \\nabla g(u_k, v_k)$$\nThe gradient of $g(u,v)$ is computed as:\n$$\\frac{\\partial g}{\\partial u} = \\frac{\\partial}{\\partial u} \\left[ (1 - u)^2 + av^2 \\right] = -2(1 - u)$$\n$$\\frac{\\partial g}{\\partial v} = \\frac{\\partial}{\\partial v} \\left[ (1 - u)^2 + av^2 \\right] = 2av$$\nThe gradient vector in the transformed space is:\n$$\\nabla g(u,v) = \\begin{pmatrix} -2(1 - u) \\\\ 2av \\end{pmatrix}$$\nThe algorithm starts by transforming the initial point $(x_0, y_0)$ to $(u_0, v_0) = T(x_0, y_0)$. It then iteratively applies the GD updates in the $(u,v)$ space. For the stopping criterion, at each step $k$, the current point $(u_k, v_k)$ is mapped back to the original space, $(x_k, y_k) = T^{-1}(u_k, v_k)$, to evaluate the distance to the true minimizer $(1,1)$. The decoupling of the variables $u$ and $v$ in the gradient $\\nabla g$ (i.e., $\\frac{\\partial g}{\\partial u}$ only depends on $u$, and $\\frac{\\partial g}{\\partial v}$ only depends on $v$) allows the algorithm to converge much more efficiently toward the minimum.\n\nThe implementation will consist of two functions, one for each method, which are called for each test case. Robustness is handled by checking for non-finite numbers after each update step. If an iterate diverges, the run is considered not converged, and the maximum iteration count $N_{\\max}$ is returned.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries are permitted.\n\ndef run_gd_standard(a, x0, y0, alpha, eps, n_max):\n    \"\"\"\n    Performs standard Gradient Descent on the Rosenbrock function.\n    \"\"\"\n    x, y = float(x0), float(y0)\n    minimizer = np.array([1.0, 1.0])\n\n    for k in range(n_max):\n        current_pos = np.array([x, y])\n        # Check for convergence before the update\n        if np.linalg.norm(current_pos - minimizer) = eps:\n            return k\n\n        # Calculate gradient analytically\n        grad_x = -2.0 * (1.0 - x) - 4.0 * a * x * (y - x**2)\n        grad_y = 2.0 * a * (y - x**2)\n\n        # Update step\n        x -= alpha * grad_x\n        y -= alpha * grad_y\n\n        # Check for non-finite values\n        if not np.isfinite([x, y]).all():\n            return n_max\n    \n    # After n_max iterations, check one last time. If it just converged,\n    # it took n_max iterations. Otherwise, it didn't converge.\n    if np.linalg.norm(np.array([x, y]) - minimizer) = eps:\n        return n_max\n\n    return n_max\n\ndef run_gd_transformed(a, x0, y0, alpha, eps, n_max):\n    \"\"\"\n    Performs Gradient Descent on the transformed Rosenbrock function.\n    \"\"\"\n    # Initial transform to (u, v) space\n    u = float(x0)\n    v = float(y0) - float(x0)**2\n    \n    # Store original coordinates for distance check\n    x, y = float(x0), float(y0)\n    \n    minimizer = np.array([1.0, 1.0])\n\n    for k in range(n_max):\n        # Check for convergence in (x, y) space before the update\n        current_pos = np.array([x, y])\n        if np.linalg.norm(current_pos - minimizer) = eps:\n            return k\n\n        # Calculate gradient analytically in (u, v) space\n        grad_u = -2.0 * (1.0 - u)\n        grad_v = 2.0 * a * v\n\n        # Update step in (u, v) space\n        u -= alpha * grad_u\n        v -= alpha * grad_v\n        \n        # Check for non-finite values in the transformed space\n        if not np.isfinite([u, v]).all():\n            return n_max\n\n        # Map back to (x, y) space for the next distance check\n        x = u\n        y = v + u**2\n\n    # After n_max iterations, check one last time.\n    if np.linalg.norm(np.array([x, y]) - minimizer) = eps:\n        return n_max\n\n    return n_max\n\ndef solve():\n    \"\"\"\n    Runs the test suite and prints the results.\n    \"\"\"\n    test_cases = [\n        (100, -1.2, 1.0, 0.001, 1e-6, 50000),\n        (1000, -1.2, 1.0, 0.0003, 1e-6, 50000),\n        (10, -1.2, 1.0, 0.002, 1e-6, 50000),\n        (100, 1.0, 1.0, 0.001, 1e-12, 1),\n        (100, -1.2, 1.0, 0.005, 1e-6, 50000),\n        (100, 2.0, 2.0, 0.001, 1e-6, 50000)\n    ]\n\n    results = []\n    for case in test_cases:\n        a, x0, y0, alpha, eps, n_max = case\n        \n        n_std = run_gd_standard(a, x0, y0, alpha, eps, n_max)\n        n_trans = run_gd_transformed(a, x0, y0, alpha, eps, n_max)\n        \n        results.append([n_std, n_trans])\n\n    # Format the final output string exactly as specified\n    formatted_results = [f\"[{r[0]},{r[1]}]\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3139529"}, {"introduction": "Many applications, especially in statistics and machine learning, require that variables satisfy constraints, such as probabilities in a vector $\\mathbf{p}$ that must be non-negative and sum to one ($p_i \\ge 0$, $\\sum_i p_i = 1$). This exercise [@problem_id:3139517] moves beyond unconstrained optimization to tackle minimization over this constrained space, known as the probability simplex. You will implement and contrast two standard, principled techniques: projected gradient descent, which takes a standard gradient step and then projects the result back onto the feasible set, and reparameterization using the softmax function, which transforms the constrained problem into an unconstrained one.", "problem": "You will implement and compare two variants of gradient descent for minimizing smooth objectives over a probability vector. A probability vector is a vector $\\mathbf{p} \\in \\mathbb{R}^n$ with $p_i \\ge 0$ for all $i$ and $\\sum_{i=1}^n p_i = 1$. The feasible set is the probability simplex $\\Delta^n := \\{\\mathbf{p} \\in \\mathbb{R}^n \\mid p_i \\ge 0, \\sum_i p_i = 1\\}$. You must use only first principles and core definitions of gradients and constrained optimization, without shortcut formulas provided here. Your program must be a single, standalone script that takes no input and prints exactly one line as specified below.\n\nFundamental base and target. Start from the definition that, for a differentiable objective $f(\\mathbf{p})$, a basic gradient descent step in an unconstrained space updates $\\mathbf{z}$ by $\\mathbf{z}^{(t+1)} = \\mathbf{z}^{(t)} - \\eta_t \\nabla f(\\mathbf{z}^{(t)})$, where $\\eta_t$ is a positive step size and $\\nabla f$ is the gradient. When constraints are present, one principled approach is to reparameterize to an unconstrained variable and use the chain rule, and another is to descend in the original variable and then project back to the feasible set with respect to the Euclidean norm. You must derive the necessary expressions from these bases.\n\nAlgorithms to implement and compare:\n- Method A (Projected Gradient on the Simplex): Work directly on $\\mathbf{p} \\in \\Delta^n$. At each iteration, take an unconstrained gradient descent step in $\\mathbf{p}$ and then compute the Euclidean projection back onto $\\Delta^n$. You must derive and implement the Euclidean projection onto $\\Delta^n$ by solving the corresponding constrained least-squares problem using first-order optimality conditions; do not use any pre-packaged projection routine.\n- Method B (Softmax Reparameterization): Introduce an unconstrained parameter $\\boldsymbol{\\theta} \\in \\mathbb{R}^n$ and define $\\mathbf{p} = \\mathrm{softmax}(\\boldsymbol{\\theta})$, where the softmax is given by $p_i = \\exp(\\theta_i) / \\sum_{j=1}^n \\exp(\\theta_j)$. Apply gradient descent in $\\boldsymbol{\\theta}$ for the composite objective $f(\\mathrm{softmax}(\\boldsymbol{\\theta}))$. You must derive the explicit expression for $\\nabla_{\\boldsymbol{\\theta}} f(\\mathrm{softmax}(\\boldsymbol{\\theta}))$ from the chain rule and the Jacobian of the softmax function; do not assume the expression is given.\n\nStep size schedule and initialization. For both methods use the same diminishing step size schedule $\\eta_t = \\eta_0 / \\sqrt{t+1}$ with $\\eta_0 = 0.3$, and a fixed iteration budget of $T = 4000$. Initialize $\\mathbf{p}^{(0)}$ for the projected method as the uniform vector, that is $p_i^{(0)} = 1/n$. Initialize $\\boldsymbol{\\theta}^{(0)}$ as the zero vector so that $\\mathrm{softmax}(\\boldsymbol{\\theta}^{(0)})$ is also uniform. Use $n = 4$ throughout.\n\nObjectives and gradients. You must implement the following objectives $f(\\mathbf{p})$ on $\\Delta^4$ and their gradients $\\nabla_{\\mathbf{p}} f(\\mathbf{p})$ from definitions. In all cases, $\\|\\cdot\\|_2$ denotes the Euclidean norm. All computations are dimensionless; no physical units are involved.\n- Case $1$ (Quadratic toward a target vector): $f(\\mathbf{p}) = \\tfrac{1}{2} \\|\\mathbf{p} - \\mathbf{q}\\|_2^2$, with $\\mathbf{q} = [0.7, 0.1, 0.1, 0.1]$.\n- Case $2$ (Linear objective): $f(\\mathbf{p}) = \\mathbf{c}^\\top \\mathbf{p}$, with $\\mathbf{c} = [0.5, -1.0, 0.2, 0.2]$.\n- Case $3$ (Quadratic data fit): $f(\\mathbf{p}) = \\tfrac{1}{2} \\|A \\mathbf{p} - \\mathbf{b}\\|_2^2$, with\n  $$\n  A = \\begin{bmatrix}\n  2  -1  0  0 \\\\\n  -1  2  -1  0 \\\\\n  0  -1  2  -1 \\\\\n  0  0  -1  2\n  \\end{bmatrix}, \\quad \\mathbf{b} = [1.0, 0.0, 0.0, 0.0].\n  $$\n- Case $4$ (Quadratic toward uniform): $f(\\mathbf{p}) = \\tfrac{1}{2} \\|\\mathbf{p} - \\mathbf{u}\\|_2^2$, with $\\mathbf{u} = [0.25, 0.25, 0.25, 0.25]$.\n\nFor each case, implement $\\nabla_{\\mathbf{p}} f(\\mathbf{p})$ from first principles. For the linear objective, recall that the gradient is constant. For the quadratic objectives, use the standard rule that for $f(\\mathbf{p}) = \\tfrac{1}{2}\\|\\mathbf{M}\\mathbf{p} - \\mathbf{r}\\|_2^2$, one has $\\nabla_{\\mathbf{p}} f(\\mathbf{p}) = \\mathbf{M}^\\top (\\mathbf{M}\\mathbf{p} - \\mathbf{r})$, which follows from the chain rule and linearity of differentiation.\n\nComparison metric. For each case, after running both methods for exactly $T$ iterations with the specified schedule and initializations, compute the $\\ell_1$ distance between the resulting probability vectors $\\mathbf{p}_{\\mathrm{softmax}}$ and $\\mathbf{p}_{\\mathrm{proj}}$, namely $\\|\\mathbf{p}_{\\mathrm{softmax}} - \\mathbf{p}_{\\mathrm{proj}}\\|_1 = \\sum_{i=1}^4 |p_{\\mathrm{softmax},i} - p_{\\mathrm{proj},i}|$.\n\nTest suite. Use the four cases above as the test suite. These cover: an interior-like target (Case $1$), a boundary optimum (Case $2$), a structured quadratic with coupling (Case $3$), and an already optimal starting point (Case $4$).\n\nRequired final output format. Your program should produce a single line of output containing a list of four floating-point numbers, each equal to the $\\ell_1$ distance for the corresponding case, rounded to six decimal places, as a comma-separated list enclosed in square brackets, for example, \"[0.000123,0.045678,0.001000,0.000000]\".", "solution": "We present principled derivations and algorithmic designs grounded in core definitions of gradients, the chain rule, and constrained optimization via projection.\n\nFeasible set and objectives. The feasible set is the probability simplex $\\Delta^n = \\{\\mathbf{p} \\in \\mathbb{R}^n \\mid p_i \\ge 0,\\ \\sum_{i=1}^n p_i = 1\\}$. We consider $n = 4$ and the four objectives specified. For linear and quadratic functions, the gradients with respect to $\\mathbf{p}$ follow from basic rules:\n- For $f(\\mathbf{p}) = \\mathbf{c}^\\top \\mathbf{p}$, the gradient is $\\nabla_{\\mathbf{p}} f(\\mathbf{p}) = \\mathbf{c}$, because differentiation is linear and $\\partial (\\mathbf{c}^\\top \\mathbf{p}) / \\partial \\mathbf{p} = \\mathbf{c}$.\n- For $f(\\mathbf{p}) = \\tfrac{1}{2}\\|\\mathbf{M} \\mathbf{p} - \\mathbf{r}\\|_2^2$, define $\\mathbf{y}(\\mathbf{p}) = \\mathbf{M}\\mathbf{p} - \\mathbf{r}$ and $g(\\mathbf{y}) = \\tfrac{1}{2} \\|\\mathbf{y}\\|_2^2$. Then by the chain rule,\n$$\n\\nabla_{\\mathbf{p}} f(\\mathbf{p}) = \\mathbf{M}^\\top \\nabla_{\\mathbf{y}} g(\\mathbf{y}(\\mathbf{p})) = \\mathbf{M}^\\top \\mathbf{y}(\\mathbf{p}) = \\mathbf{M}^\\top (\\mathbf{M}\\mathbf{p} - \\mathbf{r}).\n$$\nTaking $\\mathbf{M} = \\mathbf{I}$ and $\\mathbf{r} = \\mathbf{q}$ yields $\\nabla_{\\mathbf{p}} f(\\mathbf{p}) = \\mathbf{p} - \\mathbf{q}$. Taking $\\mathbf{M} = A$ and $\\mathbf{r} = \\mathbf{b}$ yields $\\nabla_{\\mathbf{p}} f(\\mathbf{p}) = A^\\top( A\\mathbf{p} - \\mathbf{b})$.\n\nMethod A (Projected Gradient on the Simplex). Start from the unconstrained gradient descent update\n$$\n\\tilde{\\mathbf{p}}^{(t+1)} = \\mathbf{p}^{(t)} - \\eta_t \\nabla_{\\mathbf{p}} f(\\mathbf{p}^{(t)}),\n$$\nthen enforce feasibility by computing the Euclidean projection back onto $\\Delta^n$:\n$$\n\\mathbf{p}^{(t+1)} = \\operatorname{Proj}_{\\Delta^n}(\\tilde{\\mathbf{p}}^{(t+1)}) := \\arg\\min_{\\mathbf{x} \\in \\Delta^n} \\|\\mathbf{x} - \\tilde{\\mathbf{p}}^{(t+1)}\\|_2^2.\n$$\nWe derive the projection using the method of Lagrange multipliers and optimality conditions. Consider the optimization problem\n$$\n\\min_{\\mathbf{x} \\in \\mathbb{R}^n} \\tfrac{1}{2}\\|\\mathbf{x} - \\mathbf{v}\\|_2^2 \\quad \\text{subject to} \\quad \\sum_{i=1}^n x_i = 1,\\ \\ x_i \\ge 0 \\ \\forall i,\n$$\nwhere $\\mathbf{v} \\in \\mathbb{R}^n$ is given. Form the Lagrangian\n$$\n\\mathcal{L}(\\mathbf{x}, \\tau, \\boldsymbol{\\mu}) = \\tfrac{1}{2}\\|\\mathbf{x} - \\mathbf{v}\\|_2^2 + \\tau \\left(\\sum_{i=1}^n x_i - 1\\right) - \\sum_{i=1}^n \\mu_i x_i,\n$$\nwith $\\tau \\in \\mathbb{R}$ for the equality constraint and $\\mu_i \\ge 0$ for the inequality constraints. The Karush-Kuhn-Tucker (KKT) conditions (stationarity, primal feasibility, dual feasibility, complementary slackness) are:\n- Stationarity:\n$$\n\\nabla_{\\mathbf{x}} \\mathcal{L} = \\mathbf{x} - \\mathbf{v} + \\tau \\mathbf{1} - \\boldsymbol{\\mu} = \\mathbf{0}.\n$$\n- Primal feasibility: $\\sum_i x_i = 1$, $x_i \\ge 0$ for all $i$.\n- Dual feasibility: $\\mu_i \\ge 0$ for all $i$.\n- Complementary slackness: $\\mu_i x_i = 0$ for all $i$.\n\nFrom stationarity, for each $i$,\n$$\nx_i = v_i - \\tau + \\mu_i.\n$$\nIf $x_i  0$, then complementary slackness implies $\\mu_i = 0$, hence $x_i = v_i - \\tau$. If $x_i = 0$, then $\\mu_i \\ge 0$ and $0 = v_i - \\tau + \\mu_i$ implies $v_i - \\tau \\le 0$. Therefore the solution has the thresholding form\n$$\nx_i^\\star = \\max\\{v_i - \\tau, 0\\}\n$$\nfor some scalar $\\tau$ chosen such that $\\sum_{i=1}^n x_i^\\star = 1$. The scalar $\\tau$ is uniquely determined because the map $\\tau \\mapsto \\sum_i \\max\\{v_i - \\tau, 0\\}$ is continuous and strictly decreasing. An efficient way to compute $\\tau$ is to sort the entries of $\\mathbf{v}$ in descending order, find the largest index $\\rho$ such that $u_\\rho - \\frac{1}{\\rho+1}\\left(\\sum_{j=0}^{\\rho} u_j - 1\\right) > 0$ for the sorted vector $\\mathbf{u}$, then set\n$$\n\\tau = \\frac{1}{\\rho+1}\\left(\\sum_{j=0}^{\\rho} u_j - 1\\right),\n$$\nand output $\\mathbf{x}^\\star = \\max\\{\\mathbf{v} - \\tau, 0\\}$ applied elementwise. This implements the Euclidean projection onto $\\Delta^n$.\n\nMethod B (Softmax Reparameterization). Define the unconstrained parameter $\\boldsymbol{\\theta} \\in \\mathbb{R}^n$ and set\n$$\np_i(\\boldsymbol{\\theta}) = \\frac{\\exp(\\theta_i)}{\\sum_{j=1}^n \\exp(\\theta_j)}.\n$$\nWe perform gradient descent on the composite function $F(\\boldsymbol{\\theta}) = f(\\mathbf{p}(\\boldsymbol{\\theta}))$. By the chain rule,\n$$\n\\nabla_{\\boldsymbol{\\theta}} F(\\boldsymbol{\\theta}) = J(\\boldsymbol{\\theta})^\\top \\nabla_{\\mathbf{p}} f(\\mathbf{p}(\\boldsymbol{\\theta})),\n$$\nwhere $J(\\boldsymbol{\\theta}) \\in \\mathbb{R}^{n \\times n}$ is the Jacobian of the softmax. The derivative of the softmax is a standard result obtained by differentiating $p_i = \\exp(\\theta_i)/\\sum_k \\exp(\\theta_k)$. For each $i,k$,\n$$\n\\frac{\\partial p_i}{\\partial \\theta_k} = p_i(\\delta_{ik} - p_k),\n$$\nwhere $\\delta_{ik}$ is the Kronecker delta. Therefore, if we denote $\\mathbf{g} = \\nabla_{\\mathbf{p}} f(\\mathbf{p}(\\boldsymbol{\\theta}))$, the $k$-th component of $\\nabla_{\\boldsymbol{\\theta}} F(\\boldsymbol{\\theta})$ is\n$$\n\\left[\\nabla_{\\boldsymbol{\\theta}} F(\\boldsymbol{\\theta})\\right]_k\n= \\sum_{i=1}^n g_i \\frac{\\partial p_i}{\\partial \\theta_k}\n= \\sum_{i=1}^n g_i p_i(\\delta_{ik} - p_k)\n= p_k \\left(g_k - \\sum_{i=1}^n g_i p_i\\right).\n$$\nIn vector form,\n$$\n\\nabla_{\\boldsymbol{\\theta}} F(\\boldsymbol{\\theta}) = \\mathbf{p} \\odot \\left(\\mathbf{g} - (\\mathbf{g}^\\top \\mathbf{p}) \\mathbf{1}\\right),\n$$\nwhere $\\odot$ denotes elementwise multiplication. The gradient descent update becomes\n$$\n\\boldsymbol{\\theta}^{(t+1)} = \\boldsymbol{\\theta}^{(t)} - \\eta_t \\left[ \\mathbf{p}^{(t)} \\odot \\left(\\nabla_{\\mathbf{p}} f(\\mathbf{p}^{(t)}) - \\left(\\nabla_{\\mathbf{p}} f(\\mathbf{p}^{(t)})\\right)^\\top \\mathbf{p}^{(t)} \\cdot \\mathbf{1} \\right) \\right],\n$$\nwith $\\mathbf{p}^{(t)} = \\mathrm{softmax}(\\boldsymbol{\\theta}^{(t)})$. This update ensures that the corresponding $\\mathbf{p}^{(t)}$ stays in $\\Delta^n$ by construction.\n\nStep size and initialization. We use the diminishing schedule $\\eta_t = \\eta_0/\\sqrt{t+1}$ with $\\eta_0 = 0.3$, which satisfies the common heuristic of decreasing step size to promote stability. For both methods, the initialization is the uniform distribution: directly for Method A by setting $p_i^{(0)} = 1/n$, and for Method B by setting $\\boldsymbol{\\theta}^{(0)} = \\mathbf{0}$ so that softmax yields the uniform distribution.\n\nTest cases and expectations. We implement four test cases:\n- Case $1$: $f(\\mathbf{p}) = \\tfrac{1}{2}\\|\\mathbf{p} - \\mathbf{q}\\|_2^2$ with $\\mathbf{q} = [0.7, 0.1, 0.1, 0.1]$. The unconstrained minimizer is $\\mathbf{q}$, which lies in the simplex, so the constrained minimizer is also $\\mathbf{q}$. Both methods should converge close to $\\mathbf{q}$.\n- Case $2$: $f(\\mathbf{p}) = \\mathbf{c}^\\top \\mathbf{p}$ with $\\mathbf{c} = [0.5, -1.0, 0.2, 0.2]$. The constrained minimizer is the vertex corresponding to the smallest component of $\\mathbf{c}$, namely the second coordinate; both methods should approach a distribution concentrated on that coordinate.\n- Case $3$: $f(\\mathbf{p}) = \\tfrac{1}{2}\\|A\\mathbf{p} - \\mathbf{b}\\|_2^2$ with the tridiagonal $A$ and $\\mathbf{b}$ given. The optimum may be interior or on the boundary; both methods should approach a similar solution.\n- Case $4$: $f(\\mathbf{p}) = \\tfrac{1}{2}\\|\\mathbf{p} - \\mathbf{u}\\|_2^2$ with $\\mathbf{u}$ uniform. The uniform initialization is already optimal, so both methods should remain close to uniform.\n\nComparison metric. After $T = 4000$ iterations for each case, we compute the $\\ell_1$ distance $\\|\\mathbf{p}_{\\mathrm{softmax}} - \\mathbf{p}_{\\mathrm{proj}}\\|_1 = \\sum_{i=1}^4 |p_{\\mathrm{softmax},i} - p_{\\mathrm{proj},i}|$. This metric is nonnegative, equals zero if and only if the two probability vectors match, and is sensitive to differences across coordinates.\n\nProgram design. The program will:\n- Implement the projection onto $\\Delta^n$ using the KKT-derived thresholding method described above.\n- Implement Method A and Method B with the common step schedule and initializations.\n- Define the objective and gradient functions for the four cases.\n- Run both methods for $T = 4000$ iterations on each case and collect the $\\ell_1$ distances.\n- Print a single line containing the list of four distances rounded to six decimal places, in the specified format.\n\nThis approach integrates the fundamental rules of differentiation, the chain rule, and the Karush-Kuhn-Tucker conditions to derive implementable algorithms for constrained gradient descent and reparameterized gradient descent on the probability simplex.", "answer": "```python\nimport numpy as np\n\ndef softmax(theta: np.ndarray) - np.ndarray:\n    # Numerically stable softmax\n    z = theta - np.max(theta)\n    e = np.exp(z)\n    return e / np.sum(e)\n\ndef grad_softmax_chain(p: np.ndarray, grad_p: np.ndarray) - np.ndarray:\n    # Computes grad wrt theta: p * (grad_p - grad_p, p)\n    s = float(np.dot(grad_p, p))\n    return p * (grad_p - s)\n\ndef project_to_simplex(v: np.ndarray) - np.ndarray:\n    # Euclidean projection onto the probability simplex\n    # Implements the KKT-derived thresholding with sorting\n    n = v.size\n    u = np.sort(v)[::-1]\n    cssv = np.cumsum(u)\n    rho = -1\n    for j in range(n):\n        t = u[j] - (cssv[j] - 1.0) / (j + 1)\n        if t > 0:\n            rho = j\n    if rho == -1:\n        # In pathological case, project to uniform\n        return np.full(n, 1.0 / n)\n    theta = (cssv[rho] - 1.0) / (rho + 1)\n    w = v - theta\n    w[w  0] = 0.0\n    # Numerical safeguard to ensure sum to 1\n    s = w.sum()\n    if s = 0:\n        return np.full(n, 1.0 / n)\n    return w / s\n\ndef projected_gradient(f_grad, n=4, T=4000, eta0=0.3):\n    # Initialize at uniform distribution\n    p = np.full(n, 1.0 / n)\n    for t in range(T):\n        eta = eta0 / np.sqrt(t + 1.0)\n        g = f_grad(p)\n        p = p - eta * g\n        p = project_to_simplex(p)\n    return p\n\ndef softmax_reparam_gradient(f_grad, n=4, T=4000, eta0=0.3):\n    theta = np.zeros(n)\n    for t in range(T):\n        eta = eta0 / np.sqrt(t + 1.0)\n        p = softmax(theta)\n        g_p = f_grad(p)\n        g_theta = grad_softmax_chain(p, g_p)\n        theta = theta - eta * g_theta\n    p_final = softmax(theta)\n    return p_final\n\n# Define objectives and gradients for the four cases\ndef make_case1(q):\n    q = np.asarray(q, dtype=float)\n    def f_grad(p):\n        return p - q\n    return f_grad\n\ndef make_case2(c):\n    c = np.asarray(c, dtype=float)\n    def f_grad(p):\n        return c\n    return f_grad\n\ndef make_case3(A, b):\n    A = np.asarray(A, dtype=float)\n    b = np.asarray(b, dtype=float)\n    AT = A.T\n    def f_grad(p):\n        return AT @ (A @ p - b)\n    return f_grad\n\ndef make_case4(u):\n    u = np.asarray(u, dtype=float)\n    def f_grad(p):\n        return p - u\n    return f_grad\n\ndef run_case(f_grad):\n    n = 4\n    T = 4000\n    eta0 = 0.3\n    p_proj = projected_gradient(f_grad, n=n, T=T, eta0=eta0)\n    p_soft = softmax_reparam_gradient(f_grad, n=n, T=T, eta0=eta0)\n    l1 = float(np.sum(np.abs(p_proj - p_soft)))\n    return l1\n\ndef solve():\n    # Test suite definitions\n    case1_grad = make_case1([0.7, 0.1, 0.1, 0.1])\n    case2_grad = make_case2([0.5, -1.0, 0.2, 0.2])\n    A = np.array([[2.0, -1.0, 0.0, 0.0],\n                  [-1.0, 2.0, -1.0, 0.0],\n                  [0.0, -1.0, 2.0, -1.0],\n                  [0.0, 0.0, -1.0, 2.0]], dtype=float)\n    b = np.array([1.0, 0.0, 0.0, 0.0], dtype=float)\n    case3_grad = make_case3(A, b)\n    case4_grad = make_case4([0.25, 0.25, 0.25, 0.25])\n\n    test_cases = [case1_grad, case2_grad, case3_grad, case4_grad]\n\n    results = []\n    for grad_fun in test_cases:\n        l1 = run_case(grad_fun)\n        results.append(l1)\n\n    # Print with required formatting: six decimal places, single line\n    print(\"[\" + \",\".join(f\"{x:.6f}\" for x in results) + \"]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3139517"}]}