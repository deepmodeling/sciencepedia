{"hands_on_practices": [{"introduction": "This problem grounds the abstract theory of Lagrange multipliers in a clear, geometric context. You will find the points on a hyperbola that are closest to the origin, a task that forces you to engage with the core principle: at a constrained optimum, the gradient of the objective function must be parallel to the gradient of the constraint function [@problem_id:3150396]. This exercise reinforces the derivation of first-order conditions and introduces the use of second-order conditions to classify the nature of these stationary points.", "problem": "In computational modeling, one often minimizes a smooth objective over a smooth constraint manifold to obtain stable parameter estimates. Consider the problem of minimizing the objective function $f(x,y)=x^{2}+y^{2}$ subject to the equality constraint $g(x,y)=xy-1=0$. The constraint defines a smooth one-dimensional manifold in $\\mathbb{R}^{2}$ wherever $\\nabla g(x,y)\\neq \\mathbf{0}$. Using only the core definition that a constrained stationary point occurs where the directional derivative of $f$ along all directions tangent to the constraint manifold vanishes, and the principle that at such points the gradient of $f$ must lie in the span of the gradient of the constraint, do the following:\n\n1. Derive the necessary conditions for constrained stationarity and solve for all constrained stationary points $(x,y)$ on the manifold $xy=1$.\n2. Using the second differential restricted to the tangent space of the constraint manifold at each stationary point, determine whether each stationary point is a constrained local minimum, a constrained local maximum, or neither.\n3. State whether a constrained global maximum exists, and justify your conclusion using the structure of the manifold or a one-dimensional reduction.\n4. As your final reported value, provide the minimal value of $f$ attained on the constraint manifold as a single exact number. Do not include units. No rounding is required.", "solution": "The problem as stated is valid. It is a well-posed, self-contained problem in constrained optimization, a core topic in computational science and applied mathematics. It is scientifically grounded and uses precise, objective language. All necessary information is provided, and the problem is free of contradictions or fallacies. We may proceed with the solution.\n\nThe problem asks to find and classify the extrema of the objective function $f(x,y) = x^2 + y^2$ subject to the equality constraint $g(x,y) = xy - 1 = 0$. The function $f(x,y)$ represents the squared Euclidean distance from the origin to a point $(x,y)$ in $\\mathbb{R}^2$. The constraint $g(x,y)=0$ confines the points $(x,y)$ to a hyperbola.\n\nFirst, we will derive the necessary conditions for a constrained stationary point. A point $(x_0, y_0)$ on the constraint manifold is a stationary point of $f$ if the directional derivative of $f$ in any direction tangent to the manifold at $(x_0, y_0)$ is zero.\nLet $\\mathbf{v}$ be a vector in the tangent space to the manifold $g(x,y)=0$ at the point $(x_0, y_0)$. The tangent space is orthogonal to the gradient of the constraint function, $\\nabla g(x_0, y_0)$. Thus, any such vector $\\mathbf{v}$ must satisfy $\\nabla g(x_0, y_0) \\cdot \\mathbf{v} = 0$.\nThe condition for stationarity is that the directional derivative of $f$ in the direction of $\\mathbf{v}$ is zero: $D_{\\mathbf{v}}f(x_0, y_0) = \\nabla f(x_0, y_0) \\cdot \\mathbf{v} = 0$.\nThis must hold for all vectors $\\mathbf{v}$ in the tangent space. Since the tangent space consists of all vectors orthogonal to $\\nabla g(x_0, y_0)$, the condition $\\nabla f(x_0, y_0) \\cdot \\mathbf{v} = 0$ implies that $\\nabla f(x_0, y_0)$ must also be orthogonal to the tangent space. In a Euclidean space, this means that $\\nabla f(x_0, y_0)$ must be parallel to the normal vector $\\nabla g(x_0, y_0)$. This geometric condition is expressed algebraically as the existence of a scalar $\\lambda$, the Lagrange multiplier, such that:\n$$ \\nabla f(x,y) = \\lambda \\nabla g(x,y) $$\nThis equation, combined with the original constraint, provides the necessary conditions for a constrained stationary point.\n\n1. Derivation and solution for constrained stationary points.\n\nFirst, we compute the gradients of $f$ and $g$:\n$$ \\nabla f(x,y) = \\left\\langle \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right\\rangle = \\langle 2x, 2y \\rangle $$\n$$ \\nabla g(x,y) = \\left\\langle \\frac{\\partial g}{\\partial x}, \\frac{\\partial g}{\\partial y} \\right\\rangle = \\langle y, x \\rangle $$\nThe constraint manifold $xy=1$ is smooth everywhere, as $\\nabla g = \\langle y, x \\rangle$ is only zero at $(0,0)$, which is not a point on the manifold.\n\nThe necessary conditions for a stationary point $(x,y)$ are given by the system of equations:\n$$\n\\begin{align*}\n2x &= \\lambda y \\quad &(1) \\\\\n2y &= \\lambda x \\quad &(2) \\\\\nxy - 1 &= 0 \\quad &(3)\n\\end{align*}\n$$\nFrom equation $(3)$, we know that neither $x$ nor $y$ can be zero. We can therefore divide by $x$ and $y$. From $(1)$, we can express $\\lambda$ as $\\lambda = \\frac{2x}{y}$. Substituting this into $(2)$:\n$$ 2y = \\left(\\frac{2x}{y}\\right)x \\implies 2y^2 = 2x^2 \\implies y^2 = x^2 $$\nThis implies that $y = x$ or $y = -x$. We analyze these two cases.\n\nCase 1: $y = x$.\nSubstituting this into the constraint equation $(3)$:\n$$ x(x) - 1 = 0 \\implies x^2 = 1 \\implies x = \\pm 1 $$\nIf $x=1$, then $y=1$. This gives the stationary point $(1,1)$.\nIf $x=-1$, then $y=-1$. This gives the stationary point $(-1,-1)$.\n\nCase 2: $y = -x$.\nSubstituting this into the constraint equation $(3)$:\n$$ x(-x) - 1 = 0 \\implies -x^2 = 1 \\implies x^2 = -1 $$\nThis equation has no real solutions for $x$.\n\nThus, the only constrained stationary points on the manifold are $(1,1)$ and $(-1,-1)$.\n\n2. Classification of stationary points using the second differential.\n\nTo classify these points, we use the second-order sufficiency condition, which involves examining the sign of the second differential of the Lagrangian, $L(x,y,\\lambda) = f(x,y) - \\lambda g(x,y)$, restricted to the tangent space of the constraint.\nThe Lagrangian is $L(x,y,\\lambda) = x^2 + y^2 - \\lambda(xy-1)$.\nThe Hessian of the Lagrangian with respect to the variables $x$ and $y$ is:\n$$ H_L = \\begin{pmatrix} \\frac{\\partial^2 L}{\\partial x^2} & \\frac{\\partial^2 L}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 L}{\\partial y \\partial x} & \\frac{\\partial^2 L}{\\partial y^2} \\end{pmatrix} = \\begin{pmatrix} 2 & -\\lambda \\\\ -\\lambda & 2 \\end{pmatrix} $$\nWe must evaluate the quadratic form $\\mathbf{v}^T H_L \\mathbf{v}$ for any non-zero vector $\\mathbf{v}$ in the tangent space at each stationary point.\n\nFor the point $(1,1)$:\nFirst, we find the value of $\\lambda$. Using equation $(1)$, $2(1) = \\lambda(1)$, which gives $\\lambda=2$.\nThe Hessian at $(1,1)$ is $H_L(1,1) = \\begin{pmatrix} 2 & -2 \\\\ -2 & 2 \\end{pmatrix}$.\nThe tangent space at $(1,1)$ is the set of vectors $\\mathbf{v} = \\langle v_x, v_y \\rangle$ satisfying $\\nabla g(1,1) \\cdot \\mathbf{v} = 0$. Since $\\nabla g(1,1) = \\langle 1,1 \\rangle$, the condition is $1 \\cdot v_x + 1 \\cdot v_y = 0$, or $v_y = -v_x$.\nA basis for this one-dimensional tangent space is the vector $\\mathbf{v} = \\langle 1, -1 \\rangle$.\nWe evaluate the quadratic form for this vector:\n$$ \\mathbf{v}^T H_L \\mathbf{v} = \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} 2 & -2 \\\\ -2 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} 2(1) - 2(-1) \\\\ -2(1) + 2(-1) \\end{pmatrix} = \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ -4 \\end{pmatrix} = 1(4) - 1(-4) = 8 $$\nSince $8 > 0$, the Hessian of the Lagrangian is positive definite on the tangent space. This indicates that the point $(1,1)$ is a constrained local minimum.\n\nFor the point $(-1,-1)$:\nAgain, we find $\\lambda$ from equation $(1)$: $2(-1) = \\lambda(-1)$, which gives $\\lambda=2$.\nThe Hessian at $(-1,-1)$ is identical: $H_L(-1,-1) = \\begin{pmatrix} 2 & -2 \\\\ -2 & 2 \\end{pmatrix}$.\nThe tangent space at $(-1,-1)$ is defined by $\\nabla g(-1,-1) \\cdot \\mathbf{v} = 0$. Since $\\nabla g(-1,-1) = \\langle -1,-1 \\rangle$, the condition is $-1 \\cdot v_x - 1 \\cdot v_y = 0$, or $v_y = -v_x$.\nThe tangent space is the same, with basis vector $\\mathbf{v} = \\langle 1, -1 \\rangle$.\nThe quadratic form is therefore identical:\n$$ \\mathbf{v}^T H_L \\mathbf{v} = 8 $$\nSince $8 > 0$, the point $(-1,-1)$ is also a constrained local minimum.\n\n3. Existence of a constrained global maximum.\n\nThe constraint manifold is the hyperbola defined by $xy=1$. This manifold is unbounded. It consists of two branches, one in the first quadrant and one in the third quadrant.\nThe objective function is $f(x,y) = x^2 + y^2$. To see if a global maximum exists, we can examine the behavior of $f$ on the manifold as we move far from the origin.\nConsider a sequence of points $(x_n, y_n)$ on the manifold such that $x_n \\to \\infty$. Since $y_n = 1/x_n$, we have $y_n \\to 0$. The value of the objective function for this sequence is:\n$$ f(x_n, y_n) = x_n^2 + \\left(\\frac{1}{x_n}\\right)^2 $$\nAs $n \\to \\infty$, $x_n \\to \\infty$ and $x_n^2 \\to \\infty$. Thus, $\\lim_{n \\to \\infty} f(x_n, y_n) = \\infty$.\nSince the objective function $f$ is not bounded above on the constraint set, a constrained global maximum does not exist. The same conclusion is reached by considering points where $x \\to -\\infty$ or $x \\to 0$.\n\n4. Minimal value of $f$ on the constraint manifold.\n\nThe stationary points $(1,1)$ and $(-1,-1)$ are both constrained local minima. Since the function $f(x,y)$ grows without bound as a point on the manifold moves to infinity, the global minimum value must be achieved at one of these local minima.\nLet's evaluate the function $f$ at these points:\nAt $(1,1)$:\n$$ f(1,1) = 1^2 + 1^2 = 1 + 1 = 2 $$\nAt $(-1,-1)$:\n$$ f(-1,-1) = (-1)^2 + (-1)^2 = 1 + 1 = 2 $$\nBoth points yield the same minimal value. Therefore, the constrained global minimum value of $f(x,y)$ on the manifold $xy=1$ is $2$.", "answer": "$$\\boxed{2}$$", "id": "3150396"}, {"introduction": "Many optimization problems in science and engineering involve not just equalities, but also inequalities, such as resource limitations or physical boundaries. This practice extends the method of Lagrange multipliers to handle such scenarios by introducing the powerful Karush-Kuhn-Tucker (KKT) conditions [@problem_id:3150385]. You will learn to determine if a constraint is \"active\" at the solution and apply the concepts of dual feasibility and complementary slackness, which are fundamental to modern optimization.", "problem": "Consider the constrained optimization problem of minimizing the quadratic objective\n$$f(x,y) = (x - 3)^{2} + (y - 1)^{2}$$\nsubject to the single inequality constraint\n$$g(x,y) = x + 2y - 4 \\leq 0.$$\nWork from first principles appropriate to introductory computational science: use calculus-based optimality (stationary points of the objective), the method of Lagrange multipliers generalized to inequalities, and the definitions of primal feasibility, dual feasibility, and complementary slackness. \n\nTasks:\n1. Manually determine whether the inequality constraint is active at the constrained minimizer by comparing the unconstrained minimizer of the objective to the feasible set defined by the inequality. Clearly justify your conclusion.\n2. Using the method of Lagrange multipliers, derive the Karush–Kuhn–Tucker (KKT) conditions (historically, Karush conditions) specialized to this problem and to the active set status you determined. Set up the Lagrangian and write down the stationarity conditions with the appropriate multiplier sign convention for an inequality constraint.\n3. Solve the resulting system to obtain the optimal point $(x^{*}, y^{*})$ and the corresponding multiplier $\\lambda^{*}$, verifying primal feasibility, dual feasibility, and complementary slackness explicitly by hand.\n4. Report the exact value of the optimal objective $f^{*} = f(x^{*}, y^{*})$. Provide the exact value and do not round.", "solution": "The problem is a constrained optimization task, which is valid as it is mathematically well-posed and self-contained. The objective function is strictly convex and the feasible region is a closed, non-empty convex set, guaranteeing a unique minimizer.\n\nThe problem asks to minimize the objective function $f(x,y) = (x - 3)^{2} + (y - 1)^{2}$ subject to the inequality constraint $g(x,y) = x + 2y - 4 \\leq 0$. Geometrically, this is equivalent to finding the point $(x,y)$ in the closed half-plane defined by $x + 2y \\leq 4$ that is closest to the point $(3,1)$.\n\n**1. Activity of the Constraint**\n\nFirst, we determine if the constraint is active at the solution. An active constraint holds with equality, i.e., $g(x,y) = 0$. A constraint is inactive if the solution lies in the interior of the feasible region, i.e., $g(x,y) < 0$.\n\nWe begin by finding the unconstrained minimizer of $f(x,y)$. The function $f(x,y)$ represents the squared Euclidean distance from a point $(x,y)$ to the center $(3,1)$. This function is a sum of non-negative terms and is minimized when each term is zero. This occurs when $x=3$ and $y=1$. The unconstrained minimizer is thus $(x_u, y_u) = (3,1)$.\n\nNext, we check if this unconstrained minimizer lies within the feasible region defined by the constraint $g(x,y) \\leq 0$. We evaluate $g(x,y)$ at the point $(3,1)$:\n$$g(3,1) = (3) + 2(1) - 4 = 3 + 2 - 4 = 1$$\nSince $g(3,1) = 1$, and $1 \\not\\leq 0$, the unconstrained minimizer $(3,1)$ violates the constraint and is not in the feasible set.\n\nBecause the unconstrained minimizer is outside the feasible region, the constrained minimizer cannot be in the interior of the feasible region. Therefore, the constrained minimizer must lie on the boundary of the feasible set. This means the inequality constraint must be **active** at the optimal point $(x^*, y^*)$, satisfying the equality $g(x^*, y^*) = x^* + 2y^* - 4 = 0$.\n\n**2. Karush–Kuhn–Tucker (KKT) Conditions**\n\nWe use the method of Lagrange multipliers, generalized to inequalities by the KKT conditions. The Lagrangian function $\\mathcal{L}(x, y, \\lambda)$ for this problem is defined as:\n$$\\mathcal{L}(x, y, \\lambda) = f(x, y) + \\lambda g(x, y)$$\n$$\\mathcal{L}(x, y, \\lambda) = (x - 3)^{2} + (y - 1)^{2} + \\lambda (x + 2y - 4)$$\nFor a minimization problem with a constraint of the form $g(x,y) \\leq 0$, the Lagrange multiplier must be non-negative, $\\lambda \\geq 0$.\n\nThe KKT conditions are:\n1.  **Stationarity:** The gradient of the Lagrangian with respect to the primal variables $(x,y)$ must be zero:\n    $$\\frac{\\partial \\mathcal{L}}{\\partial x} = 2(x - 3) + \\lambda = 0$$\n    $$\\frac{\\partial \\mathcal{L}}{\\partial y} = 2(y - 1) + 2\\lambda = 0$$\n2.  **Primal Feasibility:** The constraint must be satisfied:\n    $$g(x,y) = x + 2y - 4 \\leq 0$$\n3.  **Dual Feasibility:** The Lagrange multiplier must be non-negative:\n    $$\\lambda \\geq 0$$\n4.  **Complementary Slackness:** The product of the multiplier and the constraint function must be zero:\n    $$\\lambda g(x,y) = \\lambda (x + 2y - 4) = 0$$\n\nSince we determined the constraint is active, we know that $g(x^*, y^*) = 0$. This satisfies the primal feasibility condition ($0 \\leq 0$) and the complementary slackness condition ($\\lambda \\cdot 0 = 0$) for any $\\lambda$. The problem reduces to solving the system formed by the stationarity conditions and the active constraint equation.\n\n**3. Solving the System**\n\nThe system of equations to find the optimal point $(x^*, y^*)$ and the multiplier $\\lambda^*$ is:\n(i) $2(x - 3) + \\lambda = 0 \\implies 2x - 6 + \\lambda = 0$\n(ii) $2(y - 1) + 2\\lambda = 0 \\implies 2y - 2 + 2\\lambda = 0$\n(iii) $x + 2y - 4 = 0$\n\nFrom (i), we solve for $x$:\n$$x = \\frac{6 - \\lambda}{2} = 3 - \\frac{\\lambda}{2}$$\nFrom (ii), we solve for $y$:\n$$y = \\frac{2 - 2\\lambda}{2} = 1 - \\lambda$$\nNow we substitute these expressions for $x$ and $y$ into the constraint equation (iii):\n$$(3 - \\frac{\\lambda}{2}) + 2(1 - \\lambda) - 4 = 0$$\n$$3 - \\frac{\\lambda}{2} + 2 - 2\\lambda - 4 = 0$$\n$$1 - \\frac{5}{2}\\lambda = 0$$\n$$1 = \\frac{5}{2}\\lambda$$\nSolving for $\\lambda$, we find the optimal multiplier:\n$$\\lambda^* = \\frac{2}{5}$$\nNow, we substitute $\\lambda^*$ back into the expressions for $x$ and $y$:\n$$x^* = 3 - \\frac{1}{2}\\left(\\frac{2}{5}\\right) = 3 - \\frac{1}{5} = \\frac{15 - 1}{5} = \\frac{14}{5}$$\n$$y^* = 1 - \\frac{2}{5} = \\frac{5 - 2}{5} = \\frac{3}{5}$$\nThe optimal point is $(x^*, y^*) = (\\frac{14}{5}, \\frac{3}{5})$.\n\nWe explicitly verify the KKT conditions with $(x^*, y^*, \\lambda^*) = (\\frac{14}{5}, \\frac{3}{5}, \\frac{2}{5})$:\n- **Stationarity:** The equations are satisfied by construction.\n- **Primal Feasibility:** $g(\\frac{14}{5}, \\frac{3}{5}) = \\frac{14}{5} + 2(\\frac{3}{5}) - 4 = \\frac{14+6}{5} - \\frac{20}{5} = \\frac{20}{5} - \\frac{20}{5} = 0$. Since $0 \\leq 0$, this is satisfied.\n- **Dual Feasibility:** $\\lambda^* = \\frac{2}{5}$. Since $\\frac{2}{5} > 0$, this is satisfied.\n- **Complementary Slackness:** $\\lambda^* g(x^*, y^*) = (\\frac{2}{5})(0) = 0$. This is satisfied.\nAll KKT conditions are satisfied.\n\n**4. Optimal Objective Value**\n\nFinally, we compute the value of the objective function at the optimal point $(x^*, y^*) = (\\frac{14}{5}, \\frac{3}{5})$:\n$$f^* = f(x^*, y^*) = \\left(\\frac{14}{5} - 3\\right)^{2} + \\left(\\frac{3}{5} - 1\\right)^{2}$$\n$$f^* = \\left(\\frac{14 - 15}{5}\\right)^{2} + \\left(\\frac{3 - 5}{5}\\right)^{2}$$\n$$f^* = \\left(-\\frac{1}{5}\\right)^{2} + \\left(-\\frac{2}{5}\\right)^{2}$$\n$$f^* = \\frac{1}{25} + \\frac{4}{25} = \\frac{5}{25} = \\frac{1}{5}$$\nThe exact value of the optimal objective is $\\frac{1}{5}$.", "answer": "$$\\boxed{\\frac{1}{5}}$$", "id": "3150385"}, {"introduction": "This practice bridges the gap from analytical derivation to computational implementation, a crucial step in modern scientific computing. You will develop a numerical solver for a quadratic program (QP), a common subproblem in fields ranging from machine learning to finance, by constructing and solving the linear system derived from the KKT conditions [@problem_id:3150367]. Furthermore, this exercise guides you to investigate the numerical stability of your solver by analyzing the condition number of the KKT matrix, a key concept in assessing the reliability of computational algorithms.", "problem": "You are asked to implement a numerical solver for a linearly constrained quadratic program and to study how the conditioning of the resulting Karush–Kuhn–Tucker system (also known as the bordered Hessian system) changes as the quadratic form becomes nearly singular. The mathematical program is to minimize the quadratic objective $$f(x)=\\frac{1}{2}x^\\mathsf{T} Q x$$ subject to the linear equality constraints $$A x = b,$$ where $Q \\in \\mathbb{R}^{n \\times n}$ is symmetric and positive semidefinite, $A \\in \\mathbb{R}^{m \\times n}$ has full row rank, and $b \\in \\mathbb{R}^{m}$. The method of Lagrange multipliers constructs the Lagrangian $$\\mathcal{L}(x,\\lambda)=\\frac{1}{2}x^\\mathsf{T} Q x+\\lambda^\\mathsf{T}(Ax-b),$$ where $\\lambda \\in \\mathbb{R}^m$ are Lagrange multipliers. From principles, the first-order necessary conditions for optimality are that the gradient of $\\mathcal{L}$ with respect to $x$ and $\\lambda$ vanishes and the constraints are satisfied. These conditions can be written as a linear system by differentiating $\\mathcal{L}$ and enforcing feasibility, yielding a block system with a bordered Hessian. You must derive this system directly from these definitions and solve it numerically for $x$ and $\\lambda$.\n\nYour program must:\n- Construct the bordered Hessian system from the first-order conditions starting with the definitions above, then solve it to recover $x$ for each test case.\n- Estimate the $2$-norm condition number of the bordered Hessian matrix using singular values, defined as $$\\kappa_2(K) = \\dfrac{\\sigma_{\\max}(K)}{\\sigma_{\\min}(K)},$$ where $\\sigma_{\\max}(K)$ and $\\sigma_{\\min}(K)$ are the largest and smallest singular values of $K$, respectively. Report the base-$10$ logarithm of this condition number, i.e., $$\\log_{10}\\bigl(\\kappa_2(K)\\bigr).$$\n\nScientific and numerical details:\n- Angles are not involved in this problem.\n- There are no physical units; all quantities are dimensionless real numbers.\n- You must express each solution component $x_i$ rounded to six decimal places, and the reported $\\log_{10}\\bigl(\\kappa_2(K)\\bigr)$ rounded to three decimal places.\n- If the bordered Hessian is nonsingular, a unique solution $(x,\\lambda)$ exists. The test suite below is designed so that a unique minimizer exists in each case.\n\nTest suite:\nFor each case, use the given $Q$, $A$, and $b$, and return the vector $x$ and the scalar $\\log_{10}\\bigl(\\kappa_2(K)\\bigr)$. The matrices and vectors are:\n1. Happy path case with well-conditioned $Q$:\n   - $n=2$, $m=1$\n   - $$ Q=\\begin{bmatrix}1 & 0 \\\\ 0 & 1\\end{bmatrix}, A=\\begin{bmatrix}1 & 1\\end{bmatrix}, b=\\begin{bmatrix}1\\end{bmatrix} $$\n2. Nearly singular curvature along the constrained direction:\n   - $n=3$, $m=1$\n   - $Q=\\operatorname{diag}\\!\\bigl(1,\\,1,\\,10^{-6}\\bigr)$, $A=\\begin{bmatrix}0 & 0 & 1\\end{bmatrix}$, $b=\\begin{bmatrix}1\\end{bmatrix}$\n3. Nearly singular curvature along a direction fixed by the constraint:\n   - $n=3$, $m=1$\n   - $Q=\\operatorname{diag}\\!\\bigl(10^{-6},\\,1,\\,1\\bigr)$, $A=\\begin{bmatrix}1 & 0 & 0\\end{bmatrix}$, $b=\\begin{bmatrix}1\\end{bmatrix}$\n4. Exactly singular $Q$ with constraints removing the nullspace:\n   - $n=2$, $m=1$\n   - $$ Q=\\operatorname{diag}\\!\\bigl(1,\\,0\\bigr), A=\\begin{bmatrix}0 & 1\\end{bmatrix}, b=\\begin{bmatrix}0\\end{bmatrix} $$\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- Each test case result must itself be a bracketed, comma-separated list with entries in the order $[x_1,x_2,\\dots,x_n,\\log_{10}(\\kappa_2(K))]$, where each $x_i$ is rounded to six decimal places and $\\log_{10}(\\kappa_2(K))$ is rounded to three decimal places.\n- For example, the overall structure should look like $$\\bigl[\\,[\\dots],\\,[\\dots],\\,[\\dots],\\,[\\dots]\\,\\bigr].$$", "solution": "The user requests a numerical solution to a linearly constrained quadratic programming problem and an analysis of the conditioning of the associated Karush-Kuhn-Tucker (KKT) system.\n\n### Step 1: Extract Givens\n- **Objective function**: Minimize $f(x) = \\frac{1}{2} x^\\mathsf{T} Q x$.\n- **Constraints**: $Ax = b$.\n- **Variable and Matrix/Vector Definitions**: $x \\in \\mathbb{R}^{n \\times 1}$, $Q \\in \\mathbb{R}^{n \\times n}$ is symmetric and positive semidefinite, $A \\in \\mathbb{R}^{m \\times n}$ has full row rank, $b \\in \\mathbb{R}^{m \\times 1}$.\n- **Method**: Lagrange multipliers. The Lagrangian is defined as $\\mathcal{L}(x, \\lambda) = \\frac{1}{2} x^\\mathsf{T} Q x + \\lambda^\\mathsf{T} (Ax - b)$, where $\\lambda \\in \\mathbb{R}^m$ are the Lagrange multipliers.\n- **Task**:\n    1. Derive the bordered Hessian system from the first-order necessary conditions ($\\nabla \\mathcal{L} = 0$).\n    2. Solve the system numerically for $x$.\n    3. Estimate the $2$-norm condition number, $\\kappa_2(K) = \\frac{\\sigma_{\\max}(K)}{\\sigma_{\\min}(K)}$, of the bordered Hessian matrix $K$.\n    4. Report $\\log_{10}(\\kappa_2(K))$.\n- **Test Cases**:\n    1. $n=2, m=1$, $Q=\\begin{bmatrix}1 & 0 \\\\ 0 & 1\\end{bmatrix}$, $A=\\begin{bmatrix}1 & 1\\end{bmatrix}$, $b=\\begin{bmatrix}1\\end{bmatrix}$.\n    2. $n=3, m=1$, $Q=\\operatorname{diag}(1, 1, 10^{-6})$, $A=\\begin{bmatrix}0 & 0 & 1\\end{bmatrix}$, $b=\\begin{bmatrix}1\\end{bmatrix}$.\n    3. $n=3, m=1$, $Q=\\operatorname{diag}(10^{-6}, 1, 1)$, $A=\\begin{bmatrix}1 & 0 & 0\\end{bmatrix}$, $b=\\begin{bmatrix}1\\end{bmatrix}$.\n    4. $n=2, m=1$, $Q=\\operatorname{diag}(1, 0)$, $A=\\begin{bmatrix}0 & 1\\end{bmatrix}$, $b=\\begin{bmatrix}0\\end{bmatrix}$.\n- **Formatting**: Each component $x_i$ rounded to six decimal places, and $\\log_{10}(\\kappa_2(K))$ rounded to three decimal places. The final output is a list of lists.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It poses a standard task in numerical optimization and linear algebra. All terms are formally defined, the data are consistent, and the problem is self-contained. The provided test cases are designed to explore different aspects of the KKT system's conditioning, which is a key topic in computational science. The problem is valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Principle-Based Solution\nThe problem of minimizing a quadratic function subject to linear equality constraints is a canonical problem in optimization. The method of Lagrange multipliers provides a systematic approach to finding the solution by converting the constrained problem into an unconstrained one.\n\n**1. Derivation of the Karush-Kuhn-Tucker (KKT) System**\n\nWe are given the objective function $f(x) = \\frac{1}{2} x^\\mathsf{T} Q x$ and the linear constraint $Ax = b$. The Lagrangian function $\\mathcal{L}(x, \\lambda)$ incorporates the constraint into the objective function using a vector of Lagrange multipliers $\\lambda \\in \\mathbb{R}^m$:\n$$\n\\mathcal{L}(x, \\lambda) = \\frac{1}{2} x^\\mathsf{T} Q x + \\lambda^\\mathsf{T}(Ax - b)\n$$\nThe first-order necessary conditions for a point $(x^*, \\lambda^*)$ to be a candidate for a minimum are that the gradient of the Lagrangian with respect to both $x$ and $\\lambda$ must be zero.\n\nThe partial derivative with respect to $x$ is:\n$$\n\\nabla_x \\mathcal{L}(x, \\lambda) = \\frac{1}{2}(Q + Q^\\mathsf{T})x + A^\\mathsf{T} \\lambda\n$$\nSince $Q$ is symmetric ($Q = Q^\\mathsf{T}$), this simplifies to:\n$$\n\\nabla_x \\mathcal{L}(x, \\lambda) = Qx + A^\\mathsf{T} \\lambda\n$$\nSetting this gradient to zero gives the first condition:\n$$\nQx + A^\\mathsf{T} \\lambda = \\mathbf{0}\n$$\nThe partial derivative with respect to $\\lambda$ is:\n$$\n\\nabla_\\lambda \\mathcal{L}(x, \\lambda) = Ax - b\n$$\nSetting this gradient to zero gives the second condition, which is simply the original feasibility constraint:\n$$\nAx = b\n$$\nThese two matrix equations form a single linear system. We can write them in a block matrix form:\n$$\n\\begin{pmatrix} Q & A^\\mathsf{T} \\\\ A & \\mathbf{0}_{m \\times m} \\end{pmatrix}\n\\begin{pmatrix} x \\\\ \\lambda \\end{pmatrix}\n=\n\\begin{pmatrix} \\mathbf{0}_{n \\times 1} \\\\ b \\end{pmatrix}\n$$\nThis is the Karush-Kuhn-Tucker (KKT) system, also known as the bordered Hessian system. The matrix on the left, which we denote as $K$, is the KKT matrix:\n$$\nK = \\begin{pmatrix} Q & A^\\mathsf{T} \\\\ A & \\mathbf{0} \\end{pmatrix}\n$$\nThis matrix is of size $(n+m) \\times (n+m)$. The problem statement guarantees that a unique solution exists for each test case, which implies this KKT matrix is non-singular.\n\n**2. Numerical Solution and Conditioning Analysis**\n\nThe solution procedure for each test case is as follows:\n1.  Construct the KKT matrix $K$ and the right-hand side vector $v = \\begin{pmatrix} \\mathbf{0} \\\\ b \\end{pmatrix}$ using the given matrices $Q$, $A$, and vector $b$.\n2.  Solve the linear system $Kz = v$ for the unknown vector $z = \\begin{pmatrix} x \\\\ \\lambda \\end{pmatrix}$. This can be done using a standard linear solver, such as `numpy.linalg.solve`. The first $n$ components of the solution vector $z$ constitute the desired solution $x$.\n3.  Compute the $2$-norm condition number of the KKT matrix $K$. The condition number $\\kappa_2(K)$ measures the sensitivity of the solution $z$ to perturbations in $K$ or $v$. It is defined using the singular values of $K$:\n    $$\n    \\kappa_2(K) = \\frac{\\sigma_{\\max}(K)}{\\sigma_{\\min}(K)}\n    $$\n    where $\\sigma_{\\max}(K)$ and $\\sigma_{\\min}(K)$ are the largest and smallest singular values of $K$, respectively. These are computed via Singular Value Decomposition (SVD).\n4.  Calculate the base-$10$ logarithm of the condition number, $\\log_{10}(\\kappa_2(K))$. This value provides a more manageable scale for comparing conditioning across different orders of magnitude. A higher value indicates poorer conditioning, meaning the problem is more sensitive to numerical errors.\n\nThis procedure will be applied to each of the four test cases provided. The results, including the components of the solution vector $x$ and the calculated $\\log_{10}(\\kappa_2(K))$, will be formatted according to the specified rounding rules.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the series of constrained quadratic programs.\n    \"\"\"\n    test_cases = [\n        # Case 1: Happy path case with well-conditioned Q\n        {\n            \"Q\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"A\": np.array([[1.0, 1.0]]),\n            \"b\": np.array([1.0])\n        },\n        # Case 2: Nearly singular curvature along the constrained direction\n        {\n            \"Q\": np.diag([1.0, 1.0, 1e-6]),\n            \"A\": np.array([[0.0, 0.0, 1.0]]),\n            \"b\": np.array([1.0])\n        },\n        # Case 3: Nearly singular curvature along a direction fixed by the constraint\n        {\n            \"Q\": np.diag([1e-6, 1.0, 1.0]),\n            \"A\": np.array([[1.0, 0.0, 0.0]]),\n            \"b\": np.array([1.0])\n        },\n        # Case 4: Exactly singular Q with constraints removing the nullspace\n        {\n            \"Q\": np.diag([1.0, 0.0]),\n            \"A\": np.array([[0.0, 1.0]]),\n            \"b\": np.array([0.0])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        Q, A, b = case[\"Q\"], case[\"A\"], case[\"b\"]\n\n        # Get dimensions n (variables) and m (constraints)\n        n = Q.shape[0]\n        m = A.shape[0]\n\n        # Construct the (n+m) x (n+m) KKT matrix (bordered Hessian) K\n        K = np.zeros((n + m, n + m))\n        K[:n, :n] = Q\n        K[:n, n:] = A.T\n        K[n:, :n] = A\n\n        # Construct the right-hand-side vector v of size (n+m)\n        v = np.zeros(n + m)\n        if b is not None:\n             v[n:] = b.flatten()\n\n        # Solve the KKT linear system K * z = v for z = [x, lambda]\n        z = np.linalg.solve(K, v)\n\n        # Extract the solution vector x\n        x_sol = z[:n]\n\n        # Estimate the 2-norm condition number of K using SVD\n        # A more direct method is np.linalg.cond(K, 2)\n        singular_values = np.linalg.svd(K, compute_uv=False)\n        cond_K = singular_values[0] / singular_values[-1]\n\n        # Calculate the base-10 logarithm of the condition number\n        log10_cond_K = np.log10(cond_K)\n        \n        # Format the results according to the specified precision\n        x_strs = [f\"{val:.6f}\" for val in x_sol]\n        log10_cond_str = f\"{log10_cond_K:.3f}\"\n        \n        case_result_str = f\"[{','.join(x_strs)},{log10_cond_str}]\"\n        results.append(case_result_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[[{'],['.join(result[1:-1] for result in results)}]]\")\n\nsolve()\n```", "id": "3150367"}]}