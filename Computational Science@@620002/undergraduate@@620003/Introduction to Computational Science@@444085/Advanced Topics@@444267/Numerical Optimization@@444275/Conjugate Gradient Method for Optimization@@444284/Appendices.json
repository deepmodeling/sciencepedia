{"hands_on_practices": [{"introduction": "The journey of optimization using the Conjugate Gradient method begins with a single step. This first step, which involves moving from an initial point $\\mathbf{x}_0$, is identical to the steepest descent method and provides the foundation for the algorithm's subsequent iterations. This exercise [@problem_id:2211287] will guide you through calculating the initial search direction $\\mathbf{d}_0$ and the optimal step size $\\alpha_0$ for a given quadratic function, solidifying your understanding of how the process is initiated.", "problem": "Consider the problem of minimizing the quadratic objective function $f(\\mathbf{x}) = f(x_1, x_2)$ given by:\n$$f(x_1, x_2) = \\frac{3}{2}x_1^2 + x_1x_2 + x_2^2 - x_1 - 4x_2$$\nwhere $\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$.\n\nWe wish to apply the conjugate gradient method, starting from the initial point $\\mathbf{x}_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. The first iteration of the algorithm moves from $\\mathbf{x}_0$ to a new point $\\mathbf{x}_1 = \\mathbf{x}_0 + \\alpha_0 \\mathbf{d}_0$, where $\\mathbf{d}_0$ is the initial search direction and $\\alpha_0$ is the optimal step size.\n\nDetermine the components of the initial search direction vector, $\\mathbf{d}_0 = \\begin{pmatrix} d_{0,1} \\\\ d_{0,2} \\end{pmatrix}$, and the exact value of the optimal step size $\\alpha_0$.\n\nPresent your final answer as a $1 \\times 3$ row matrix containing the values of $d_{0,1}$, $d_{0,2}$, and $\\alpha_0$, in that specific order. Express all values as exact fractions where necessary.", "solution": "We rewrite the quadratic objective in the standard form $f(\\mathbf{x})=\\frac{1}{2}\\mathbf{x}^{T}Q\\mathbf{x}-\\mathbf{c}^{T}\\mathbf{x}$ by identifying the symmetric matrix $Q$ and vector $\\mathbf{c}$. From\n$$f(x_{1},x_{2})=\\frac{3}{2}x_{1}^{2}+x_{1}x_{2}+x_{2}^{2}-x_{1}-4x_{2},$$\nwe have\n$$Q=\\begin{pmatrix}3  1 \\\\ 1  2\\end{pmatrix},\\qquad \\mathbf{c}=\\begin{pmatrix}1 \\\\ 4\\end{pmatrix}.$$\nThe gradient is $\\nabla f(\\mathbf{x})=Q\\mathbf{x}-\\mathbf{c}$. At the initial point $\\mathbf{x}_{0}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$, the gradient is\n$$\\mathbf{g}_{0}=\\nabla f(\\mathbf{x}_{0})=Q\\mathbf{x}_{0}-\\mathbf{c}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}-\\begin{pmatrix}1 \\\\ 4\\end{pmatrix}=\\begin{pmatrix}-1 \\\\ -4\\end{pmatrix}.$$\nIn the conjugate gradient method, the initial search direction is the negative gradient:\n$$\\mathbf{d}_{0}=-\\mathbf{g}_{0}=\\begin{pmatrix}1 \\\\ 4\\end{pmatrix}.$$\nThe exact line-search step size along $\\mathbf{d}_{0}$ is obtained by minimizing $\\phi(\\alpha)=f(\\mathbf{x}_{0}+\\alpha\\mathbf{d}_{0})$, which yields\n$$\\alpha_{0}=-\\frac{\\mathbf{d}_{0}^{T}\\mathbf{g}_{0}}{\\mathbf{d}_{0}^{T}Q\\mathbf{d}_{0}}=\\frac{\\mathbf{g}_{0}^{T}\\mathbf{g}_{0}}{\\mathbf{d}_{0}^{T}Q\\mathbf{d}_{0}},$$\nsince $\\mathbf{d}_{0}=-\\mathbf{g}_{0}$. Compute the numerator:\n$$\\mathbf{g}_{0}^{T}\\mathbf{g}_{0}=(-1)^{2}+(-4)^{2}=1+16=17.$$\nCompute the denominator:\n$$Q\\mathbf{d}_{0}=\\begin{pmatrix}3  1 \\\\ 1  2\\end{pmatrix}\\begin{pmatrix}1 \\\\ 4\\end{pmatrix}=\\begin{pmatrix}7 \\\\ 9\\end{pmatrix},\\qquad \\mathbf{d}_{0}^{T}Q\\mathbf{d}_{0}=\\begin{pmatrix}1  4\\end{pmatrix}\\begin{pmatrix}7 \\\\ 9\\end{pmatrix}=7+36=43.$$\nTherefore,\n$$\\alpha_{0}=\\frac{17}{43}.$$\nThus, $d_{0,1}=1$, $d_{0,2}=4$, and $\\alpha_{0}=\\frac{17}{43}$, as exact fractions where necessary.", "answer": "$$\\boxed{\\begin{pmatrix} 1  4  \\frac{17}{43} \\end{pmatrix}}$$", "id": "2211287"}, {"introduction": "What sets the Conjugate Gradient method apart is its intelligent choice of search directions, which are mutually \"conjugate\" with respect to the Hessian matrix $A$. This property, expressed as $\\mathbf{p}_i^T A \\mathbf{p}_j = 0$ for $i \\neq j$, ensures that minimization along a new direction does not spoil progress made in previous steps. In this practice [@problem_id:2211315], you will compute the first two search directions and directly verify this conjugacy property, revealing the mathematical elegance behind the algorithm's efficiency.", "problem": "Consider the unconstrained optimization problem of minimizing a two-dimensional quadratic function $f(\\mathbf{x})$, where $\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$. The function is defined as $f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^T A \\mathbf{x} - \\mathbf{b}^T \\mathbf{x}$, with the symmetric positive-definite matrix $A$ and vector $\\mathbf{b}$ given by:\n$$\nA = \\begin{pmatrix} 5  2 \\\\ 2  1 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\nWe will use the Fletcher-Reeves variant of the Conjugate Gradient (CG) method to find the minimum of this function. The gradient of the function is $\\nabla f(\\mathbf{x}) = A\\mathbf{x} - \\mathbf{b}$. The iterative steps of the CG method, starting from a point $\\mathbf{x}_k$, are as follows:\n\n1.  Let $g_k = \\nabla f(\\mathbf{x}_k)$ be the gradient at the current point.\n2.  The search direction $\\mathbf{p}_k$ is determined by the rule:\n    -   For the first iteration ($k=0$): $\\mathbf{p}_0 = -g_0$.\n    -   For subsequent iterations ($k  0$): $\\mathbf{p}_k = -g_k + \\beta_k \\mathbf{p}_{k-1}$, where $\\beta_k = \\frac{g_k^T g_k}{g_{k-1}^T g_{k-1}}$.\n3.  The step size $\\alpha_k$ is calculated as $\\alpha_k = \\frac{g_k^T g_k}{\\mathbf{p}_k^T A \\mathbf{p}_k}$.\n4.  The position is updated as $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$.\n5.  The gradient for the next iteration is updated using the efficient formula $g_{k+1} = g_k + \\alpha_k A \\mathbf{p}_k$.\n\nTwo vectors $\\mathbf{u}$ and $\\mathbf{v}$ are said to be conjugate with respect to the matrix $A$ if the quantity $\\mathbf{u}^T A \\mathbf{v}$ is equal to zero. A key property of the CG method is that it generates a sequence of mutually conjugate search directions.\n\nYour task is to apply the CG method starting from the point $\\mathbf{x}_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ to generate the first two search directions, $\\mathbf{p}_0$ and $\\mathbf{p}_1$. Then, compute the value of the expression $\\mathbf{p}_0^T A \\mathbf{p}_1$.", "solution": "We minimize $f(\\mathbf{x})=\\frac{1}{2}\\mathbf{x}^{T}A\\mathbf{x}-\\mathbf{b}^{T}\\mathbf{x}$ with $A=\\begin{pmatrix}52\\\\21\\end{pmatrix}$, $\\mathbf{b}=\\begin{pmatrix}1\\\\1\\end{pmatrix}$, starting at $\\mathbf{x}_{0}=\\begin{pmatrix}0\\\\0\\end{pmatrix}$. The gradient is $\\nabla f(\\mathbf{x})=A\\mathbf{x}-\\mathbf{b}$.\n\nCompute the initial gradient:\n$$\n\\mathbf{g}_{0}=\\nabla f(\\mathbf{x}_{0})=A\\mathbf{x}_{0}-\\mathbf{b}=\\begin{pmatrix}0\\\\0\\end{pmatrix}-\\begin{pmatrix}1\\\\1\\end{pmatrix}=\\begin{pmatrix}-1\\\\-1\\end{pmatrix}.\n$$\nFirst search direction:\n$$\n\\mathbf{p}_{0}=-\\mathbf{g}_{0}=\\begin{pmatrix}1\\\\1\\end{pmatrix}.\n$$\nStep size $\\alpha_{0}$:\n$$\n\\alpha_{0}=\\frac{\\mathbf{g}_{0}^{T}\\mathbf{g}_{0}}{\\mathbf{p}_{0}^{T}A\\mathbf{p}_{0}},\\quad \\mathbf{g}_{0}^{T}\\mathbf{g}_{0}=(-1)^{2}+(-1)^{2}=2,\n$$\n$$\nA\\mathbf{p}_{0}=\\begin{pmatrix}52\\\\21\\end{pmatrix}\\begin{pmatrix}1\\\\1\\end{pmatrix}=\\begin{pmatrix}7\\\\3\\end{pmatrix},\\quad \\mathbf{p}_{0}^{T}A\\mathbf{p}_{0}=\\begin{pmatrix}11\\end{pmatrix}\\begin{pmatrix}7\\\\3\\end{pmatrix}=10,\n$$\n$$\n\\alpha_{0}=\\frac{2}{10}=\\frac{1}{5}.\n$$\nUpdate the position and gradient:\n$$\n\\mathbf{x}_{1}=\\mathbf{x}_{0}+\\alpha_{0}\\mathbf{p}_{0}=\\begin{pmatrix}0\\\\0\\end{pmatrix}+\\frac{1}{5}\\begin{pmatrix}1\\\\1\\end{pmatrix}=\\begin{pmatrix}\\frac{1}{5}\\\\\\frac{1}{5}\\end{pmatrix},\n$$\n$$\n\\mathbf{g}_{1}=\\mathbf{g}_{0}+\\alpha_{0}A\\mathbf{p}_{0}=\\begin{pmatrix}-1\\\\-1\\end{pmatrix}+\\frac{1}{5}\\begin{pmatrix}7\\\\3\\end{pmatrix}=\\begin{pmatrix}\\frac{2}{5}\\\\-\\frac{2}{5}\\end{pmatrix}.\n$$\nCompute $\\beta_{1}$ and the second search direction $\\mathbf{p}_{1}$:\n$$\n\\beta_{1}=\\frac{\\mathbf{g}_{1}^{T}\\mathbf{g}_{1}}{\\mathbf{g}_{0}^{T}\\mathbf{g}_{0}}=\\frac{\\left(\\frac{2}{5}\\right)^{2}+\\left(-\\frac{2}{5}\\right)^{2}}{2}=\\frac{\\frac{8}{25}}{2}=\\frac{4}{25},\n$$\n$$\n\\mathbf{p}_{1}=-\\mathbf{g}_{1}+\\beta_{1}\\mathbf{p}_{0}=-\\begin{pmatrix}\\frac{2}{5}\\\\-\\frac{2}{5}\\end{pmatrix}+\\frac{4}{25}\\begin{pmatrix}1\\\\1\\end{pmatrix}=\\begin{pmatrix}-\\frac{6}{25}\\\\\\frac{14}{25}\\end{pmatrix}.\n$$\nFinally, compute $\\mathbf{p}_{0}^{T}A\\mathbf{p}_{1}$:\n$$\nA\\mathbf{p}_{1}=\\begin{pmatrix}52\\\\21\\end{pmatrix}\\begin{pmatrix}-\\frac{6}{25}\\\\\\frac{14}{25}\\end{pmatrix}=\\begin{pmatrix}-\\frac{2}{25}\\\\\\frac{2}{25}\\end{pmatrix},\n$$\n$$\n\\mathbf{p}_{0}^{T}A\\mathbf{p}_{1}=\\begin{pmatrix}11\\end{pmatrix}\\begin{pmatrix}-\\frac{2}{25}\\\\\\frac{2}{25}\\end{pmatrix}=0.\n$$\nThis confirms that the first two CG search directions are $A$-conjugate.", "answer": "$$\\boxed{0}$$", "id": "2211315"}, {"introduction": "The true power of an algorithm is revealed when it is applied to solve real-world problems. This hands-on coding exercise [@problem_id:3110688] challenges you to apply the Conjugate Gradient method to determine the equilibrium state of a physical mass-spring system, a common problem in engineering and physics. You will investigate how varying physical parameters can impact the solver's convergence and explore how preconditioning can be used as a powerful tool to accelerate the solution of such practical, and often large-scale, systems.", "problem": "You are given a one-dimensional chain of linear springs connected to fixed supports at both ends. Let $n$ be the number of interior nodes whose displacements are unknown, indexed by $i \\in \\{1,2,\\dots,n\\}$. Let the boundary nodes be indexed by $0$ and $n+1$ and fixed at zero displacement. Let the spring connecting node $i$ to node $i+1$ have stiffness $k_i > 0$ for $i \\in \\{0,1,\\dots,n\\}$. Under static equilibrium governed by Hooke's law and force balance, the displacements $u \\in \\mathbb{R}^n$ satisfy the linear system\n$$\nK u = f,\n$$\nwhere $K \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite (SPD) with entries\n$$\nK_{i,i} = k_{i-1} + k_i \\quad \\text{for} \\; i=1,\\dots,n,\n$$\n$$\nK_{i,i+1} = -k_i \\quad \\text{for} \\; i=1,\\dots,n-1,\n$$\n$$\nK_{i,i-1} = -k_{i-1} \\quad \\text{for} \\; i=2,\\dots,n,\n$$\nand $f \\in \\mathbb{R}^n$ is the vector of external forces applied to the interior nodes. The Conjugate Gradient (CG) method is a Krylov subspace method for solving SPD systems and can be interpreted as minimizing the quadratic energy functional\n$$\n\\Phi(u) = \\tfrac{1}{2} u^\\top K u - f^\\top u,\n$$\nwith search directions that are mutually conjugate with respect to the $K$-inner product.\n\nYour task is to write a complete program that:\n- Constructs $K$ implicitly via its tridiagonal structure from the given stiffnesses $k_i$, without forming a dense matrix.\n- Solves $K u = f$ with the Conjugate Gradient method, starting from the zero vector, and stops when the relative residual norm $\\|r_k\\|_2/\\|f\\|_2 \\leq \\varepsilon$ or when the iteration count reaches a specified maximum. Here $r_k = f - K u_k$ is the residual at iteration $k$.\n- Repeats the solve with a simple diagonal (Jacobi) preconditioner, which uses $M = \\mathrm{diag}(K)$ so that the preconditioned system is $M^{-1} K u = M^{-1} f$.\n- Reports, for each test case and each solver variant, the integer iteration count used and the final relative residual $\\|r_k\\|_2/\\|f\\|_2$ as a float.\n\nUse the following fixed tolerance and iteration cap for all runs:\n- Tolerance: $\\varepsilon = 10^{-8}$.\n- Maximum iterations: $n$.\n\nSet the force vector to be a unit load on the last interior node,\n$$\nf = [0, 0, \\dots, 0, 1]^\\top \\in \\mathbb{R}^n.\n$$\n\nTest Suite. Use $n=200$ and the following four stiffness patterns; in each case, ensure all $k_i$ are strictly positive:\n1. Uniform stiffness (happy path): $k_i = 1$ for all $i \\in \\{0,1,\\dots,n\\}$.\n2. Block contrast (moderate heterogeneity): $k_i = 1$ for all $i$, except $k_i = 10$ for indices $i \\in \\{80,81,\\dots,120\\}$.\n3. Log-normal variability (strong heterogeneity): $k_i = \\exp(Z_i)$ for all $i$, where $Z_i$ are independent and identically distributed normal random variables with mean $0$ and standard deviation $1$, generated deterministically with a fixed seed $42$. That is, $Z_i \\sim \\mathcal{N}(0,1)$ for all $i$ and the random number generator must be initialized to $42$ to make results reproducible.\n4. Near-singular bottleneck (edge case): $k_i = 1$ for all $i$, except $k_{100} = 10^{-6}$.\n\nOutput Specification. For each test case, run both the unpreconditioned Conjugate Gradient and the Jacobi-preconditioned Conjugate Gradient. Record:\n- The integer iteration count taken by the method (equal to the iteration at which the stopping criterion is met, or the maximum iterations $n$ if the criterion is not met).\n- The final relative residual as a float.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain the values in the following order, flattened across all test cases:\n$$\n[\\text{it}_1^\\text{none}, \\text{it}_1^\\text{jac}, \\text{rel}_1^\\text{none}, \\text{rel}_1^\\text{jac}, \\text{it}_2^\\text{none}, \\text{it}_2^\\text{jac}, \\text{rel}_2^\\text{none}, \\text{rel}_2^\\text{jac}, \\text{it}_3^\\text{none}, \\text{it}_3^\\text{jac}, \\text{rel}_3^\\text{none}, \\text{rel}_3^\\text{jac}, \\text{it}_4^\\text{none}, \\text{it}_4^\\text{jac}, \\text{rel}_4^\\text{none}, \\text{rel}_4^\\text{jac}],\n$$\nwhere $\\text{it}_j^\\cdot$ are integers and $\\text{rel}_j^\\cdot$ are floats for test case $j \\in \\{1,2,3,4\\}$ and solver variant indicated by the superscript. No physical units are required in the output because the reported quantities are dimensionless counts and norms. The program must be self-contained and produce identical results on repeated runs.", "solution": "We begin from Hooke's law for linear springs, which states that the force in a spring is proportional to its extension, $F = k \\Delta$, where $k$ is the stiffness and $\\Delta$ is the change in length. In a one-dimensional chain of nodes connected by springs with fixed endpoints, static equilibrium is achieved when the net force on each interior node equals the applied external force. Denoting $u_i$ as the displacement at interior node $i$, the force imparted to node $i$ by the spring on its left is $k_{i-1}(u_{i-1} - u_i)$ (with $u_0 = 0$ because the left boundary is fixed), and the force from the spring on its right is $k_{i}(u_{i+1} - u_i)$ (with $u_{n+1} = 0$ at the fixed right boundary). Summing these and setting equal to the external force $f_i$ yields the equilibrium equations\n$$\nk_{i-1}(u_{i-1} - u_i) + k_{i}(u_{i+1} - u_i) = f_i, \\quad i=1,\\dots,n.\n$$\nRearranging terms gives the linear system $K u = f$ with\n$$\nK_{i,i} = k_{i-1} + k_{i}, \\quad K_{i,i-1} = -k_{i-1}, \\quad K_{i,i+1} = -k_i,\n$$\nand zero elsewhere, confirming that $K$ is tridiagonal and symmetric. Moreover, $K$ is symmetric positive definite (SPD) because it arises from the second derivative (Hessian) of the strictly convex quadratic energy functional\n$$\n\\Phi(u) = \\tfrac{1}{2} \\sum_{i=0}^{n} k_i (u_{i+1} - u_i)^2 - \\sum_{i=1}^{n} f_i u_i,\n$$\nwhere $u_0 = u_{n+1} = 0$. Since $k_i > 0$ for all $i$, the quadratic form $u^\\top K u$ is positive for all nonzero $u$, implying positive definiteness.\n\nThe Conjugate Gradient (CG) method solves SPD systems by minimizing $\\Phi(u)$ over expanding Krylov subspaces. It constructs search directions $\\{p_k\\}$ that are $K$-conjugate, meaning $p_i^\\top K p_j = 0$ for $i \\neq j$, and performs line searches along these directions. Starting with $u_0 = 0$, the residual $r_0 = f - K u_0 = f$, and setting $p_0 = r_0$, the algorithm iteratively updates\n$$\n\\alpha_k = \\frac{r_k^\\top r_k}{p_k^\\top K p_k}, \\quad u_{k+1} = u_k + \\alpha_k p_k, \\quad r_{k+1} = r_k - \\alpha_k K p_k,\n$$\nand computes\n$$\n\\beta_{k+1} = \\frac{r_{k+1}^\\top r_{k+1}}{r_k^\\top r_k}, \\quad p_{k+1} = r_{k+1} + \\beta_{k+1} p_k.\n$$\nAn equivalent and often more robust variant uses the preconditioned residual $z_k = M^{-1} r_k$ with a preconditioner $M$ that approximates $K$ in some sense. The Preconditioned Conjugate Gradient (PCG) recursions become\n$$\n\\alpha_k = \\frac{r_k^\\top z_k}{p_k^\\top K p_k}, \\quad u_{k+1} = u_k + \\alpha_k p_k, \\quad r_{k+1} = r_k - \\alpha_k K p_k,\n$$\n$$\nz_{k+1} = M^{-1} r_{k+1}, \\quad \\beta_{k+1} = \\frac{r_{k+1}^\\top z_{k+1}}{r_k^\\top z_k}, \\quad p_{k+1} = z_{k+1} + \\beta_{k+1} p_k.\n$$\nA simple and widely used choice is the Jacobi preconditioner, $M = \\mathrm{diag}(K)$, which is cheap to apply and can reduce the spread of eigenvalues.\n\nThe convergence rate of CG is governed by the condition number $\\kappa(K) = \\lambda_{\\max}(K)/\\lambda_{\\min}(K)$. A classical bound in the energy norm is\n$$\n\\frac{\\|e_k\\|_K}{\\|e_0\\|_K} \\leq 2 \\left( \\frac{\\sqrt{\\kappa(K)} - 1}{\\sqrt{\\kappa(K)} + 1} \\right)^k,\n$$\nwhere $e_k = u^\\star - u_k$ and $u^\\star$ is the exact solution. When stiffness values vary strongly across the chain, the eigenvalues of $K$ spread out and $\\kappa(K)$ increases, slowing CG. Preconditioning aims to reduce $\\kappa(M^{-1}K)$ to cluster eigenvalues and accelerate convergence.\n\nAlgorithmic design for this problem:\n- Construct $K$ implicitly through its tridiagonal entries using the stiffness edge list $\\{k_i\\}_{i=0}^{n}$. The diagonal entries are $d_i = k_{i-1} + k_i$ for $i=1,\\dots,n$, and the off-diagonal entries are $o_i = -k_i$ for $i=1,\\dots,n-1$. For efficient multiplication $y = K x$, compute\n$$\ny_i = d_i x_i + o_i x_{i+1} + o_{i-1} x_{i-1},\n$$\nwith appropriate handling for end indices.\n- Implement CG and PCG, using the zero vector as the initial guess. The stopping criterion is $\\|r_k\\|_2 / \\|f\\|_2 \\leq 10^{-8}$ or $k$ reaching $n$.\n- For the Jacobi preconditioner, set $M = \\mathrm{diag}(K)$, so $M^{-1}$ acts by dividing each component by the corresponding diagonal entry $d_i$.\n- Prepare the four test cases as specified. The third test case must be generated deterministically using a fixed seed for the normal random numbers, then transformed as $k_i = \\exp(Z_i)$, which ensures $k_i > 0$.\n- For each test case, run both solvers, record the iteration counts and final relative residuals, and print the aggregate results in the prescribed single-line list format.\n\nInterpretation of results:\n- In the uniform case, eigenvalues are well-behaved and CG should converge in relatively few iterations; the Jacobi preconditioner may offer modest improvement.\n- In the block-contrast case, increased stiffness in a segment creates localized high-frequency modes, increasing the condition number. Jacobi preconditioning typically reduces iteration counts compared to unpreconditioned CG.\n- In the log-normal case, large variability in $k_i$ leads to strong heterogeneity and a wider eigenvalue spectrum; preconditioning should have a more pronounced effect.\n- In the near-singular bottleneck case, a very small stiffness creates a nearly disconnected system with an extremely small eigenvalue and large condition number; the unpreconditioned CG may approach the iteration cap, and even Jacobi preconditioning might only partially mitigate the difficulty.\n\nThe final program implements these steps cleanly and deterministically, and emits the required single-line output as an ordered list of integers and floats.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef build_tridiag_from_edges(k_edges: np.ndarray):\n    \"\"\"\n    Given spring stiffnesses between consecutive nodes k_edges of length n+1\n    (including boundary-to-node springs), construct the tridiagonal representation\n    of the SPD stiffness matrix K for n interior unknowns.\n\n    Returns:\n        diag: length-n diagonal entries of K\n        off:  length-(n-1) off-diagonal entries (both upper and lower, symmetric)\n    \"\"\"\n    # k_edges shape: (n+1,)\n    n = k_edges.shape[0] - 1\n    # Diagonal: d_i = k_{i-1} + k_i for i = 1..n, but using 0-based indexing\n    diag = k_edges[:-1] + k_edges[1:]\n    # Off-diagonal entries correspond to interior springs between nodes i and i+1: -k_i for i=1..n-1\n    off = -k_edges[1:-1]\n    return diag, off\n\ndef matvec_tridiag(diag: np.ndarray, off: np.ndarray, x: np.ndarray):\n    \"\"\"\n    Multiply y = K x for a symmetric tridiagonal K specified by diag and off.\n    diag length n, off length n-1.\n    \"\"\"\n    n = diag.shape[0]\n    y = diag * x\n    if n > 1:\n        y[:-1] += off * x[1:]\n        y[1:] += off * x[:-1]\n    return y\n\ndef pcg_tridiag(diag: np.ndarray, off: np.ndarray, b: np.ndarray, M_inv: np.ndarray | None,\n                tol: float, max_iter: int):\n    \"\"\"\n    Preconditioned Conjugate Gradient for symmetric positive definite tridiagonal system.\n    If M_inv is None, performs unpreconditioned CG.\n\n    Returns:\n        x: solution vector\n        iters: iteration count used (int)\n        rel_res: final relative residual ||r|| / ||b|| (float)\n    \"\"\"\n    n = diag.shape[0]\n    x = np.zeros(n, dtype=float)\n    r = b.copy()  # since A x0 = 0 for x0 = 0\n    b_norm = np.linalg.norm(b)\n    # Handle the trivial case b == 0 to avoid division by zero in relative residual\n    if b_norm == 0.0:\n        return x, 0, 0.0\n\n    # Apply preconditioner\n    if M_inv is None:\n        z = r.copy()\n    else:\n        z = M_inv * r\n\n    p = z.copy()\n    rz_old = float(np.dot(r, z))\n\n    rel_res = np.linalg.norm(r) / b_norm\n    if rel_res = tol:\n        return x, 0, rel_res\n\n    iters = 0\n    for k in range(max_iter):\n        Ap = matvec_tridiag(diag, off, p)\n        pAp = float(np.dot(p, Ap))\n        # Guard against breakdown (should not occur for SPD and proper preconditioning)\n        if pAp = 0.0:\n            iters = k\n            rel_res = np.linalg.norm(r) / b_norm\n            return x, iters, rel_res\n        alpha = rz_old / pAp\n        x += alpha * p\n        r -= alpha * Ap\n        rel_res = np.linalg.norm(r) / b_norm\n        iters = k + 1\n        if rel_res = tol:\n            break\n        if M_inv is None:\n            z = r.copy()\n        else:\n            z = M_inv * r\n        rz_new = float(np.dot(r, z))\n        # Guard against breakdown\n        if rz_old == 0.0:\n            break\n        beta = rz_new / rz_old\n        p = z + beta * p\n        rz_old = rz_new\n\n    # Return final values\n    return x, iters, rel_res\n\ndef make_test_cases(n: int):\n    \"\"\"\n    Create the four test cases as specified:\n      1) Uniform stiffness: k_i = 1\n      2) Block contrast: k_i = 1 except k_i = 10 for i in [80, 120]\n      3) Log-normal: k_i = exp(Z_i), Z_i ~ N(0,1) with seed 42\n      4) Bottleneck: k_i = 1 except k_100 = 1e-6\n    Returns a list of k_edges arrays.\n    \"\"\"\n    cases = []\n\n    # Case 1: Uniform\n    k_uniform = np.ones(n + 1, dtype=float)\n    cases.append(k_uniform)\n\n    # Case 2: Block contrast\n    k_block = np.ones(n + 1, dtype=float)\n    # Indices 80..120 inclusive (safe within 0..n)\n    low = 80\n    high = min(120, n)  # ensure bound for n=200\n    k_block[low:high + 1] = 10.0\n    cases.append(k_block)\n\n    # Case 3: Log-normal with seed 42\n    rng = np.random.default_rng(42)\n    Z = rng.normal(loc=0.0, scale=1.0, size=n + 1)\n    k_logn = np.exp(Z)\n    cases.append(k_logn)\n\n    # Case 4: Near-singular bottleneck\n    k_bottle = np.ones(n + 1, dtype=float)\n    bottleneck_index = 100\n    if 0 = bottleneck_index = n:\n        k_bottle[bottleneck_index] = 1e-6\n    cases.append(k_bottle)\n\n    return cases\n\ndef run_case(k_edges: np.ndarray, tol: float):\n    \"\"\"\n    Build K from k_edges, run CG and Jacobi-PCG, return iteration counts and final relative residuals.\n    \"\"\"\n    diag, off = build_tridiag_from_edges(k_edges)\n    n = diag.shape[0]\n    b = np.zeros(n, dtype=float)\n    b[-1] = 1.0  # unit load at the last interior node\n    max_iter = n\n\n    # Unpreconditioned CG\n    x_none, it_none, rel_none = pcg_tridiag(diag, off, b, M_inv=None, tol=tol, max_iter=max_iter)\n\n    # Jacobi preconditioner M = diag(K)\n    M_inv = 1.0 / diag\n    x_jac, it_jac, rel_jac = pcg_tridiag(diag, off, b, M_inv=M_inv, tol=tol, max_iter=max_iter)\n\n    return it_none, it_jac, rel_none, rel_jac\n\ndef solve():\n    # Define the test cases from the problem statement.\n    n = 200\n    tol = 1e-8\n    k_cases = make_test_cases(n)\n\n    results = []\n    for k_edges in k_cases:\n        it_none, it_jac, rel_none, rel_jac = run_case(k_edges, tol)\n        # Append in the specified flattened order: it_none, it_jac, rel_none, rel_jac\n        results.extend([it_none, it_jac, rel_none, rel_jac])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3110688"}]}