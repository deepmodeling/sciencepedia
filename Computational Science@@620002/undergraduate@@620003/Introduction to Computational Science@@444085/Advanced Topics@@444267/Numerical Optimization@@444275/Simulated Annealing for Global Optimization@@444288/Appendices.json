{"hands_on_practices": [{"introduction": "The best way to master an algorithm is to build it from scratch. This first exercise guides you through implementing a complete Simulated Annealing solver for the notoriously difficult Rastrigin function, a standard benchmark for global optimization methods due to its vast number of local minima. By tackling this challenge, you will gain hands-on experience with all the essential components of SA, from proposal generation and boundary handling to the core Metropolis acceptance criterion and the cooling schedule [@problem_id:3193392].", "problem": "Implement a complete Simulated Annealing (SA) solver to approximately minimize the multivariate Rastrigin energy function over a bounded continuous domain and empirically study how convergence quality scales with dimension. Your program must be self-contained and produce results for a fixed test suite specified below.\n\nFundamental base: build your algorithm from the following cornerstones.\n- The Rastrigin energy in $n$ dimensions is given by\n$$E(\\mathbf{x}) = A n + \\sum_{i=1}^{n} \\left(x_i^2 - A \\cos(2\\pi x_i)\\right),$$\nwith domain constraints $\\mathbf{x} \\in [-B,B]^n$. Use $A = 10$ and $B = 5.12$. All trigonometric function arguments are in radians.\n- The target stationary distribution for a configuration $\\mathbf{x}$ at temperature $T$ in Boltzmann form is proportional to $\\exp\\!\\left(-E(\\mathbf{x})/T\\right)$.\n- The Metropolis mechanism must enforce detailed balance with respect to the stationary distribution when the proposal distribution is symmetric.\n- Use a geometric cooling schedule with $T_{k+1} = \\alpha T_k$, where $0 < \\alpha < 1$, and $T_0 > 0$.\n- Use a symmetric Gaussian proposal mechanism in $\\mathbb{R}^n$.\n\nDesign constraints and requirements:\n1. Proposals and domain handling.\n   - From current state $\\mathbf{x}$, propose $\\mathbf{y} = \\mathbf{x} + \\boldsymbol{\\eta}$, where $\\boldsymbol{\\eta} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I})$ with an iteration-dependent scalar $\\sigma$.\n   - Enforce bounds $\\mathbf{y} \\in [-B,B]^n$ by specular reflection at the boundaries, not by rejection or clamping. That is, if any coordinate exceeds the interval, reflect it back into the interval by mirror symmetry; repeat as needed until all coordinates are inside $[-B,B]$.\n   - To maintain a dimension-agnostic step size, scale the nominal step parameter $s_0$ as $\\sigma = \\left(s_0/\\sqrt{n}\\right)\\sqrt{T/T_0}$ at each iteration.\n2. Acceptance rule.\n   - Derive an acceptance rule starting from the requirement of detailed balance for a symmetric proposal distribution and the Boltzmann stationary distribution. Do not assume any shortcut formulas; explicitly justify how your rule enforces detailed balance.\n3. Cooling schedule and stopping.\n   - Use a geometric schedule with given $\\alpha$ and $T_0$.\n   - Use a fixed evaluation budget per test case equal to the specified count of proposed moves. Each proposal counts as one objective evaluation.\n4. Performance metric.\n   - For each test case, report only the final best energy found by the SA run, rounded to six decimal places. No physical units are involved.\n5. Angle unit.\n   - All trigonometric calculations must use radians.\n\nTest suite and parameters:\n- Use $A = 10$ and $B = 5.12$ for all cases.\n- Run the algorithm on the following cases, each specified as a tuple $(n, \\text{seed}, \\text{evals}, T_0, \\alpha, s_0)$:\n  - Case $1$: $(1, 1, 4000, 5.0, 0.995, 0.5)$\n  - Case $2$: $(2, 2, 6000, 5.0, 0.995, 0.5)$\n  - Case $3$: $(5, 3, 10000, 5.0, 0.995, 0.5)$\n  - Case $4$: $(10, 4, 15000, 5.0, 0.995, 0.5)$\n  - Case $5$: $(20, 5, 20000, 5.0, 0.995, 0.5)$\n  - Case $6$ (schedule sensitivity at fixed dimension): $(10, 6, 15000, 5.0, 0.999, 0.5)$\n\nWhat your program must do:\n- Implement SA as specified, with reflection handling, Gaussian symmetric proposals, Boltzmann-consistent acceptance, and geometric cooling.\n- For each test case, start from an initial point sampled uniformly from $[-B,B]^n$ using the provided pseudorandom seed (deterministic reproducibility).\n- Return the final best energy found after exactly the given number of evaluations.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of floating-point numbers rounded to six decimal places, enclosed in square brackets, with no spaces. The $i$-th entry corresponds to Case $i$ in the order listed above. For example, a syntactically correct output looks like\n$[\\text{r}_1,\\text{r}_2,\\text{r}_3,\\text{r}_4,\\text{r}_5,\\text{r}_6]$\nwhere each $\\text{r}_i$ is a decimal number with exactly six digits after the decimal point.", "solution": "The problem statement is assessed to be valid. It is scientifically grounded, well-posed, objective, and contains all necessary information to construct a unique, verifiable solution. The task is to implement a Simulated Annealing (SA) algorithm to find an approximate global minimum of the multivariate Rastrigin function, with all algorithmic components and parameters precisely specified.\n\nThe solution is developed based on the foundational principles of statistical mechanics and stochastic optimization.\n\n1.  **Objective Function**\n    The energy function to be minimized is the $n$-dimensional Rastrigin function, defined as:\n    $$E(\\mathbf{x}) = A n + \\sum_{i=1}^{n} \\left(x_i^2 - A \\cos(2\\pi x_i)\\right)$$\n    The specified constants are $A = 10$ and $B = 5.12$. The domain for any state vector $\\mathbf{x} = (x_1, x_2, \\ldots, x_n)$ is the hypercube $\\mathbf{x} \\in [-B, B]^n$. The global minimum of this function is $E(\\mathbf{0}) = 0$ at $\\mathbf{x} = \\mathbf{0}$.\n\n2.  **Simulated Annealing and the Metropolis-Hastings Algorithm**\n    Simulated Annealing is a probabilistic metaheuristic inspired by the physical process of annealing in metallurgy. A system's state is evolved iteratively, gradually lowering a control parameter called temperature, $T$. At high $T$, the system explores the state space broadly. As $T$ decreases, the system is guided to settle into a state of low energy.\n\n    The evolution of the system state is governed by a Markov chain designed to converge to a stationary Boltzmann distribution at any given temperature $T$:\n    $$P(\\mathbf{x}; T) = \\frac{1}{Z(T)} \\exp\\left(-\\frac{E(\\mathbf{x})}{T}\\right)$$\n    where $Z(T)$ is the partition function. This distribution gives higher probability to lower-energy states.\n\n    To ensure convergence to $P(\\mathbf{x}; T)$, the transition mechanism must satisfy the detailed balance condition. Let $W(\\mathbf{x} \\to \\mathbf{y})$ be the transition probability from state $\\mathbf{x}$ to state $\\mathbf{y}$. Detailed balance requires:\n    $$P(\\mathbf{x}) W(\\mathbf{x} \\to \\mathbf{y}) = P(\\mathbf{y}) W(\\mathbf{y} \\to \\mathbf{x})$$\n    The transition probability can be decomposed into a proposal probability $g(\\mathbf{y}|\\mathbf{x})$ and an acceptance probability $A(\\mathbf{y}|\\mathbf{x})$, such that $W(\\mathbf{x} \\to \\mathbf{y}) = g(\\mathbf{y}|\\mathbf{x}) A(\\mathbf{y}|\\mathbf{x})$. Substituting this and the Boltzmann distribution into the detailed balance equation gives:\n    $$e^{-E(\\mathbf{x})/T} g(\\mathbf{y}|\\mathbf{x}) A(\\mathbf{y}|\\mathbf{x}) = e^{-E(\\mathbf{y})/T} g(\\mathbf{x}|\\mathbf{y}) A(\\mathbf{x}|\\mathbf{y})$$\n    The problem specifies a symmetric proposal distribution, $g(\\mathbf{y}|\\mathbf{x}) = g(\\mathbf{x}|\\mathbf{y})$, based on an isotropic Gaussian step. This simplifies the condition to:\n    $$\\frac{A(\\mathbf{y}|\\mathbf{x})}{A(\\mathbf{x}|\\mathbf{y})} = \\frac{e^{-E(\\mathbf{y})/T}}{e^{-E(\\mathbf{x})/T}} = \\exp\\left(-\\frac{E(\\mathbf{y}) - E(\\mathbf{x})}{T}\\right)$$\n    The standard Metropolis choice for the acceptance probability, which satisfies this ratio, is:\n    $$A(\\mathbf{y}|\\mathbf{x}) = \\min\\left(1, \\exp\\left(-\\frac{\\Delta E}{T}\\right)\\right)$$\n    where $\\Delta E = E(\\mathbf{y}) - E(\\mathbf{x})$. This is the acceptance rule implemented. A new state is always accepted if it has lower energy ($\\Delta E  0$). If it has higher energy ($\\Delta E > 0$), it is accepted with a probability that decreases as the energy increase becomes larger or the temperature becomes lower. This allows the algorithm to escape local minima.\n\n3.  **Algorithmic Implementation**\n    The SA solver is implemented by adhering to the specified design constraints.\n\n    _Initialization_:\n    - The initial temperature is set to $T_0$.\n    - A pseudorandom number generator is seeded for reproducibility.\n    - An initial state $\\mathbf{x}_{\\text{current}}$ is drawn from a uniform distribution over the domain $[-B, B]^n$.\n    - The energy $E_{\\text{current}} = E(\\mathbf{x}_{\\text{current}})$ is calculated.\n    - The best-found state and energy are initialized: $\\mathbf{x}_{\\text{best}} = \\mathbf{x}_{\\text{current}}$ and $E_{\\text{best}} = E_{\\text{current}}$.\n\n    _Iteration Loop_: The algorithm proceeds for a fixed number of evaluations (`evals`).\n    1.  **Proposal Generation**: A new candidate state $\\mathbf{y}_{\\text{new}}$ is generated from the current state $\\mathbf{x}_{\\text{current}}$. A random step vector $\\boldsymbol{\\eta}$ is drawn from an $n$-dimensional normal distribution $\\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I})$. The proposal variance $\\sigma^2$ is temperature-dependent to adapt the search scale:\n        $$\\sigma(T) = \\frac{s_0}{\\sqrt{n}}\\sqrt{\\frac{T}{T_0}}$$\n        where $s_0$ is a nominal step size parameter. The scaling by $1/\\sqrt{n}$ ensures that the expected magnitude of the step vector $\\|\\boldsymbol{\\eta}\\|$ is roughly independent of the dimension $n$. The raw proposal is $\\mathbf{y}_{\\text{raw}} = \\mathbf{x}_{\\text{current}} + \\boldsymbol{\\eta}$.\n\n    2.  **Boundary Handling**: The domain constraints $\\mathbf{x} \\in [-B, B]^n$ are enforced using specular reflection. Any coordinate of $\\mathbf{y}_{\\text{raw}}$ that falls outside the interval $[-B, B]$ is reflected back into it. This process is repeated until the point lies within the domain. For a single coordinate $y_i$ and interval $[-B, B]$ of width $W=2B$, this mapping is implemented by considering the shifted coordinate $y'_i = y_i + B$ on the interval $[0, W]$. The number of full interval widths traversed is $k = \\lfloor y'_i / W \\rfloor$. The position within the $[0, W]$ segment is $y''_i = y'_i \\pmod{W}$. If $k$ is even, the final position is $y_i''-B$. If $k$ is odd, the particle has been reflected an odd number of times, and its position is $(W-y_i'')-B$. The resulting state is $\\mathbf{y}_{\\text{new}}$. This deterministic mapping preserves the symmetry assumption of the Metropolis rule in a practical sense, as commonly assumed in SA implementations.\n\n    3.  **Acceptance**: The energy $E_{\\text{new}} = E(\\mathbf{y}_{\\text{new}})$ is computed. The change in energy is $\\Delta E = E_{\\text{new}} - E_{\\text{current}}$. The new state is accepted, i.e., $\\mathbf{x}_{\\text{current}} \\leftarrow \\mathbf{y}_{\\text{new}}$, with the Metropolis probability $A(\\mathbf{y}_{\\text{new}}|\\mathbf{x}_{\\text{current}})$ defined above.\n\n    4.  **Best State Tracking**: After the potential update of $\\mathbf{x}_{\\text{current}}$, its energy is compared to the best energy found so far, $E_{\\text{best}}$. If $E_{\\text{current}}  E_{\\text{best}}$, then we set $E_{\\text{best}} = E_{\\text{current}}$.\n\n    5.  **Cooling**: The temperature is lowered according to the geometric cooling schedule: $T_{k+1} = \\alpha T_k$.\n\n    _Termination_: The loop terminates after `evals` proposals. The final value of $E_{\\text{best}}$ is reported as the result.\n\n4.  **Test Suite Execution**\n    The algorithm is executed for the six specified test cases, each with its own set of parameters $(n, \\text{seed}, \\text{evals}, T_0, \\alpha, s_0)$. The final optimized energy for each case is rounded to six decimal places and formatted as requested.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and runs a Simulated Annealing solver for the Rastrigin function\n    as per the problem specification.\n    \"\"\"\n\n    # --- Constants specified in the problem ---\n    A = 10.0\n    B = 5.12\n\n    def rastrigin(x: np.ndarray, n: int) - float:\n        \"\"\"\n        Calculates the Rastrigin energy function for a given vector x.\n        E(x) = An + sum(x_i^2 - A*cos(2*pi*x_i))\n        \"\"\"\n        if n == 0:\n            return 0.0\n        return A * n + np.sum(x**2 - A * np.cos(2 * np.pi * x))\n\n    def reflect_bounds(y: np.ndarray, B_val: float) - np.ndarray:\n        \"\"\"\n        Enforces domain bounds [-B_val, B_val]^n by specular reflection.\n        Handles proposals that may be far outside the boundaries by repeated\n        reflection until the point is inside.\n        \"\"\"\n        min_val = -B_val\n        max_val = B_val\n        width = max_val - min_val\n\n        y_shifted = y - min_val\n        \n        # Calculate how many times the point has \"wrapped\" around the interval\n        num_wraps = np.floor(y_shifted / width)\n        \n        # The position within the base interval [0, width]\n        y_wrapped = y_shifted % width\n        \n        # Create masks for even and odd numbers of wraps\n        even_mask = num_wraps % 2 == 0\n        odd_mask = ~even_mask\n        \n        y_reflected = np.zeros_like(y)\n        \n        # If even wraps, position is relative to min_val\n        y_reflected[even_mask] = min_val + y_wrapped[even_mask]\n        \n        # If odd wraps, position is reflected relative to max_val\n        y_reflected[odd_mask] = max_val - y_wrapped[odd_mask]\n        \n        return y_reflected\n\n    def run_simulated_annealing(n: int, seed: int, evals: int, T0: float, alpha: float, s0: float) - float:\n        \"\"\"\n        Executes a single run of the Simulated Annealing algorithm.\n        \"\"\"\n        # 1. Initialization\n        rng = np.random.default_rng(seed)\n        \n        T = T0\n        \n        # Initial point sampled uniformly from the domain\n        x_current = rng.uniform(-B, B, size=n)\n        E_current = rastrigin(x_current, n)\n        \n        E_best = E_current\n\n        # 2. Main Loop\n        for _ in range(evals):\n            # Propose a new state\n            sigma = (s0 / np.sqrt(n)) * np.sqrt(T / T0) if n > 0 else s0 * np.sqrt(T/T0)\n            eta = rng.normal(loc=0.0, scale=sigma, size=n)\n            y_raw = x_current + eta\n            \n            # Enforce boundary conditions via reflection\n            y_new = reflect_bounds(y_raw, B)\n            \n            # Evaluate the new state\n            E_new = rastrigin(y_new, n)\n            \n            # Metropolis acceptance criterion\n            delta_E = E_new - E_current\n            \n            # Accept if better or with probability exp(-delta_E / T)\n            if delta_E  0 or rng.random()  np.exp(-delta_E / T):\n                x_current = y_new\n                E_current = E_new\n            \n            # Update the best energy found so far (from accepted states)\n            if E_current  E_best:\n                E_best = E_current\n                \n            # Cool down the temperature\n            T *= alpha\n            \n        return E_best\n\n    # --- Test Suite ---\n    test_cases = [\n        # (n, seed, evals, T0, alpha, s0)\n        (1, 1, 4000, 5.0, 0.995, 0.5),\n        (2, 2, 6000, 5.0, 0.995, 0.5),\n        (5, 3, 10000, 5.0, 0.995, 0.5),\n        (10, 4, 15000, 5.0, 0.995, 0.5),\n        (20, 5, 20000, 5.0, 0.995, 0.5),\n        (10, 6, 15000, 5.0, 0.999, 0.5),\n    ]\n\n    results = []\n    for case in test_cases:\n        best_energy = run_simulated_annealing(*case)\n        # Round to 6 decimal places and format to ensure trailing zeros\n        results.append(f\"{round(best_energy, 6):.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3193392"}, {"introduction": "Simulated Annealing's power extends far beyond continuous optimization into the realm of combinatorial problems. This practice applies SA to sparse signal reconstruction, a fundamental task in modern data science and compressed sensing, where the goal is to find a simple explanation for complex data [@problem_id:3193389]. You will frame the search as an optimization over subsets of features and discover how SA can navigate the non-convex energy landscape to find superior solutions that a myopic, greedy approach would miss.", "problem": "You are given a sparse signal reconstruction task framed as a global optimization problem with a nonconvex sparsity penalty. Let $A \\in \\mathbb{R}^{m \\times n}$ be a measurement matrix with column vectors normalized to unit $\\ell_2$-norm, $y \\in \\mathbb{R}^{m}$ be the observed data vector, and $x \\in \\mathbb{R}^{n}$ be the unknown $K$-sparse coefficient vector. Consider the objective function\n$$\nf(x) \\;=\\; \\lVert A x - y \\rVert_2^2 \\;+\\; \\lambda \\sum_{i=1}^n \\phi\\!\\left(\\lvert x_i \\rvert\\right),\n\\quad\\text{where}\\quad\n\\phi(t) \\;=\\; \\log\\!\\left(1 + \\gamma t\\right),\n$$\nwith $\\lambda  0$ and $\\gamma  0$. The penalty $\\phi(\\cdot)$ is nonconvex in $x$, which makes $f(\\cdot)$ generally nonconvex.\n\nYou must implement Simulated Annealing (SA) based on the canonical ensemble from statistical mechanics. Treat the objective $f(\\cdot)$ as an energy function, and use the Metropolis acceptance probability derived from the Boltzmann distribution. In particular, when proposing a move from the current state to a candidate state, accept the candidate with probability\n$$\np_{\\text{accept}} \\;=\\; \\min\\!\\left(1, \\exp\\!\\left(-\\frac{\\Delta f}{T}\\right)\\right),\n$$\nwhere $\\Delta f$ is the change in objective and $T$ is the current temperature. Use an exponential cooling schedule $T_k = T_0 \\, r^k$ with a specified initial temperature $T_0  0$ and cooling rate $0  r  1$. Define the SA state as a support set $S \\subset \\{1,2,\\dots,n\\}$ of fixed size $\\lvert S \\rvert = K$, and define one-step neighbors by swapping out one index $i \\in S$ with one index $j \\notin S$. For any support $S$, let $x^\\star(S)$ denote the least squares (LS) solution restricted to $S$, namely the minimizer of $\\lVert A_S x_S - y \\rVert_2^2$ over $x_S \\in \\mathbb{R}^{K}$, with $x_i = 0$ for all $i \\notin S$. Evaluate $f$ for a support $S$ by plugging $x^\\star(S)$ into $f(\\cdot)$:\n$$\nf\\big(x^\\star(S)\\big) \\;=\\; \\left\\| A_S x^\\star_S - y \\right\\|_2^2 \\;+\\; \\lambda \\sum_{i \\in S} \\log\\!\\left(1 + \\gamma \\left|x^\\star_i\\right|\\right).\n$$\n\nAs a baseline “greedy thresholding” method, construct $S_{\\text{greedy}}$ by taking the $K$ indices with largest absolute correlation $\\lvert (A^\\top y)_i \\rvert$, then compute the LS coefficients $x^\\star(S_{\\text{greedy}})$ and its objective value.\n\nYour task is to write a complete, runnable program that:\n- Constructs the specified test suite instances,\n- Runs the greedy thresholding baseline to obtain $f\\big(x^\\star(S_{\\text{greedy}})\\big)$,\n- Runs Simulated Annealing over support sets starting at $S_{\\text{greedy}}$, using the specified $(T_0, r)$ and a fixed number of iterations, to obtain a final support $S_{\\text{SA}}$ and $f\\big(x^\\star(S_{\\text{SA}})\\big)$,\n- For each test case, outputs a boolean indicating whether SA strictly improved the objective, that is, whether $f\\big(x^\\star(S_{\\text{SA}})\\big)  f\\big(x^\\star(S_{\\text{greedy}})\\big)$ holds.\n\nThe test suite must be constructed as follows. For each case, all random draws use the provided seed for reproducibility, and $A$ must have its columns normalized to unit $\\ell_2$-norm:\n\n1. Happy-path case with moderate sparsity and noise:\n   - Parameters: $m=32$, $n=64$, $K=5$, $\\lambda=0.10$, $\\gamma=10$, noise standard deviation $\\sigma=0.05$.\n   - Seed: $12345$.\n   - Construction: draw $A$ with independent standard normal entries and normalize its columns; draw a $K$-sparse ground-truth $x_{\\text{true}}$ with nonzero entries drawn from a standard normal distribution; set $y = A x_{\\text{true}} + \\sigma \\cdot \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, I_m)$.\n   - SA schedule: $T_0 = 1.0$, $r = 0.995$, iterations $N_{\\text{SA}} = 3000$.\n\n2. Adversarial correlation case to challenge greedy thresholding:\n   - Parameters: $m=32$, $n=64$, $K=5$, $\\lambda=0.10$, $\\gamma=10$, noise standard deviation $\\sigma=0.05$.\n   - Seed: $54321$.\n   - Construction: draw $A$ and $x_{\\text{true}}$ as in case 1, form $y$ similarly; then choose a random index $j_{\\text{bad}} \\notin \\operatorname{supp}(x_{\\text{true}})$ and replace the $j_{\\text{bad}}$-th column of $A$ by the normalized vector proportional to $y$ plus a small perturbation, i.e., set $A_{\\cdot j_{\\text{bad}}} \\leftarrow \\frac{y + 0.01 \\eta}{\\lVert y + 0.01 \\eta \\rVert_2}$ with $\\eta \\sim \\mathcal{N}(0, I_m)$, and renormalize the column. This makes $\\lvert (A^\\top y)_{j_{\\text{bad}}} \\rvert$ very large and thus misleads greedy thresholding.\n   - SA schedule: $T_0 = 1.0$, $r = 0.995$, iterations $N_{\\text{SA}} = 4000$.\n\n3. High-noise case:\n   - Parameters: $m=24$, $n=48$, $K=4$, $\\lambda=0.20$, $\\gamma=8$, noise standard deviation $\\sigma=0.20$.\n   - Seed: $111$.\n   - Construction: as in case 1.\n   - SA schedule: $T_0 = 1.0$, $r = 0.995$, iterations $N_{\\text{SA}} = 3500$.\n\n4. Boundary sparsity case ($K=1$) with low noise:\n   - Parameters: $m=16$, $n=32$, $K=1$, $\\lambda=0.05$, $\\gamma=12$, noise standard deviation $\\sigma=0.01$.\n   - Seed: $222$.\n   - Construction: as in case 1.\n   - SA schedule: $T_0 = 1.0$, $r = 0.995$, iterations $N_{\\text{SA}} = 2500$.\n\nYour program must produce a single line of output containing the results for the four test cases as a comma-separated list enclosed in square brackets, for example, $\\texttt{[true,false,true,true]}$ but using Python boolean formatting. There are no physical units, angles, or percentages involved; all quantities are real-valued and unitless. The final output must exactly follow the described format.", "solution": "The problem requires the implementation and comparison of two methods for sparse signal reconstruction by minimizing a non-convex objective function. The objective is to find a $K$-sparse vector $x \\in \\mathbb{R}^{n}$ that minimizes\n$$\nf(x) \\;=\\; \\lVert A x - y \\rVert_2^2 \\;+\\; \\lambda \\sum_{i=1}^n \\phi\\!\\left(\\lvert x_i \\rvert\\right),\n\\quad\\text{where}\\quad\n\\phi(t) \\;=\\; \\log\\!\\left(1 + \\gamma t\\right).\n$$\nThe first term, $\\lVert A x - y \\rVert_2^2$, is a standard least-squares data fidelity term that measures how well the model $A x$ fits the observations $y$. The second term is a sparsity-promoting penalty, where $\\lambda  0$ controls the trade-off. The function $\\phi(t) = \\log(1 + \\gamma t)$ is a non-convex penalty, which makes the overall objective function $f(x)$ non-convex, leading to multiple local minima. This non-convexity is the primary challenge, as simple gradient-based methods are prone to getting trapped in suboptimal solutions.\n\nThe problem frames this as a combinatorial optimization over support sets. A state in our search space is a support set $S \\subset \\{1, 2, \\dots, n\\}$ of fixed cardinality $\\lvert S \\rvert = K$. For any given support $S$, the optimal coefficients are found by solving a standard least-squares problem restricted to the columns of $A$ indexed by $S$. Let $A_S$ be the submatrix of $A$ containing these columns. The optimal coefficients on this support, denoted $x^\\star_S$, are the solution to $\\min_{z_S \\in \\mathbb{R}^K} \\lVert A_S z_S - y \\rVert_2^2$. This solution can be computed using a standard linear least-squares solver, yielding $x^\\star_S = (A_S^\\top A_S)^{-1} A_S^\\top y$. The full vector $x^\\star(S)$ is then constructed by setting components not in $S$ to zero. The \"energy\" of the state $S$ is the objective function evaluated at this $x^\\star(S)$:\n$$\nE(S) \\;=\\; f\\big(x^\\star(S)\\big) \\;=\\; \\left\\| A_S x^\\star_S - y \\right\\|_2^2 \\;+\\; \\lambda \\sum_{i \\in S} \\log\\!\\left(1 + \\gamma \\left|x^\\star_i\\right|\\right).\n$$\n\nA baseline \"greedy thresholding\" method is used for comparison. This heuristic constructs an initial support, $S_{\\text{greedy}}$, by selecting the $K$ indices corresponding to the largest absolute values of the correlation vector $c = A^\\top y$. This method is computationally cheap but myopic, as it does not account for interactions between selected columns and can be easily misled, for instance, by columns that are highly correlated with the measurement vector $y$ but are not part of the true underlying support.\n\nSimulated Annealing (SA) is employed as a more sophisticated global optimization metaheuristic to overcome the limitations of the greedy approach. SA draws an analogy to the annealing process in metallurgy, where a material is heated and then slowly cooled to increase crystal size and reduce defects.\n1.  **Initialization**: The search begins with the support set provided by the greedy method, $S_{\\text{current}} = S_{\\text{greedy}}$. This provides a reasonable starting point.\n2.  **Neighborhood**: The neighborhood of a support $S$ is defined by all supports that can be reached by swapping a single index $i \\in S$ with a single index $j \\notin S$. In each iteration, a random neighbor $S_{\\text{candidate}}$ is generated from the current support $S_{\\text{current}}$.\n3.  **Metropolis Acceptance Criterion**: The algorithm evaluates the change in energy, $\\Delta E = E(S_{\\text{candidate}}) - E(S_{\\text{current}})$. If $\\Delta E  0$, the candidate state is better, and the move is always accepted. If $\\Delta E \\ge 0$, the move is to a worse state, but it may still be accepted with a probability given by the Boltzmann factor, $p_{\\text{accept}} = \\exp(-\\Delta E/T)$. This ability to make \"uphill\" moves is crucial for escaping local minima. The parameter $T$ is the \"temperature,\" which moderates this probability.\n4.  **Cooling Schedule**: The temperature $T$ is gradually lowered throughout the search process. The specified exponential cooling schedule is $T_k = T_0 r^k$, where $T_0$ is the initial temperature, $r \\in (0, 1)$ is the cooling rate, and $k$ is the iteration number. At high temperatures, the algorithm explores the search space broadly, readily accepting worse solutions. As $T$ decreases, the algorithm becomes more selective, predominantly accepting better solutions and eventually converging towards a low-energy state.\n\nFor each test case, we generate the problem data $(A, y)$ according to the specified parameters and random seed. We compute the greedy solution $S_{\\text{greedy}}$ and its objective value $f_{\\text{greedy}}$. Then, we run the SA algorithm, starting from $S_{\\text{greedy}}$, for a fixed number of iterations. The best support found during the entire SA run, $S_{\\text{SA}}$, and its corresponding objective value, $f_{\\text{SA}}$, are recorded. Finally, we determine if the SA algorithm found a strictly better solution by checking if $f_{\\text{SA}}  f_{\\text{greedy}}$. The boolean result of this comparison is reported for each case. The adversarial case, in particular, is constructed to demonstrate a scenario where $S_{\\text{greedy}}$ is deliberately suboptimal, providing a clear opportunity for SA to demonstrate its superior global search capability.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Case 1: Happy-path\n        {\n            'm': 32, 'n': 64, 'K': 5, 'lam': 0.10, 'gam': 10, 'sigma': 0.05,\n            'seed': 12345, 'T0': 1.0, 'r': 0.995, 'iters': 3000, 'adversarial': False\n        },\n        # Case 2: Adversarial correlation\n        {\n            'm': 32, 'n': 64, 'K': 5, 'lam': 0.10, 'gam': 10, 'sigma': 0.05,\n            'seed': 54321, 'T0': 1.0, 'r': 0.995, 'iters': 4000, 'adversarial': True\n        },\n        # Case 3: High-noise\n        {\n            'm': 24, 'n': 48, 'K': 4, 'lam': 0.20, 'gam': 8, 'sigma': 0.20,\n            'seed': 111, 'T0': 1.0, 'r': 0.995, 'iters': 3500, 'adversarial': False\n        },\n        # Case 4: Boundary sparsity (K=1)\n        {\n            'm': 16, 'n': 32, 'K': 1, 'lam': 0.05, 'gam': 12, 'sigma': 0.01,\n            'seed': 222, 'T0': 1.0, 'r': 0.995, 'iters': 2500, 'adversarial': False\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_case(**case)\n        results.append(result)\n\n    # Final print statement in the exact required format. The prompt's example\n    # uses lowercase booleans, so we ensure the output matches.\n    print(f\"[{','.join([str(r).lower() for r in results])}]\")\n\ndef run_case(m, n, K, lam, gam, sigma, seed, T0, r, iters, adversarial):\n    \"\"\"\n    Runs a single test case for the sparse reconstruction problem.\n    \"\"\"\n    np.random.seed(seed)\n\n    # Generate data\n    A = np.random.randn(m, n)\n    A /= np.linalg.norm(A, axis=0)\n\n    x_true = np.zeros(n)\n    true_support_indices = np.random.choice(n, K, replace=False)\n    x_true[true_support_indices] = np.random.randn(K)\n\n    noise = np.random.randn(m)\n    y = A @ x_true + sigma * noise\n\n    if adversarial:\n        all_indices = set(range(n))\n        true_support_set = set(true_support_indices)\n        outside_support = list(all_indices - true_support_set)\n        j_bad = np.random.choice(outside_support)\n        \n        eta = np.random.randn(m)\n        new_col = y + 0.01 * eta\n        new_col /= np.linalg.norm(new_col)\n        A[:, j_bad] = new_col\n\n    def evaluate_objective(support_indices, A_mat, y_vec, lam_param, gam_param):\n        \"\"\"\n        Calculates the objective function for a given support set.\n        \"\"\"\n        S_list = sorted(list(support_indices))\n        if not S_list:\n            resid_norm_sq = np.linalg.norm(y_vec)**2\n            penalty = 0.0\n        else:\n            A_S = A_mat[:, S_list]\n            # Solve the least squares problem: A_S x_S = y\n            try:\n                x_S = np.linalg.lstsq(A_S, y_vec, rcond=None)[0]\n                resid_norm_sq = np.linalg.norm(A_S @ x_S - y_vec)**2\n                penalty = lam_param * np.sum(np.log(1 + gam_param * np.abs(x_S)))\n            except np.linalg.LinAlgError:\n                return np.inf # Ill-conditioned matrix\n            \n        return resid_norm_sq + penalty\n\n    # Greedy Thresholding Baseline\n    correlations = np.abs(A.T @ y)\n    S_greedy = set(np.argsort(correlations)[-K:])\n    f_greedy = evaluate_objective(S_greedy, A, y, lam, gam)\n\n    # Simulated Annealing\n    S_current = S_greedy\n    f_current = f_greedy\n    S_best = S_current\n    f_best = f_greedy\n\n    all_indices = set(range(n))\n    T = T0\n\n    for _ in range(iters):\n        # Generate a neighbor by swapping one index\n        if K == 0 or K == n: # Cannot swap\n            break\n            \n        S_outside = all_indices - S_current\n        idx_to_remove = np.random.choice(list(S_current))\n        idx_to_add = np.random.choice(list(S_outside))\n\n        S_candidate = S_current.copy()\n        S_candidate.remove(idx_to_remove)\n        S_candidate.add(idx_to_add)\n\n        f_candidate = evaluate_objective(S_candidate, A, y, lam, gam)\n        delta_f = f_candidate - f_current\n\n        # Metropolis acceptance criterion\n        if delta_f  0:\n            accept = True\n        else:\n            prob = np.exp(-delta_f / T)\n            if np.random.rand()  prob:\n                accept = True\n            else:\n                accept = False\n        \n        if accept:\n            S_current = S_candidate\n            f_current = f_candidate\n\n        if f_current  f_best:\n            S_best = S_current\n            f_best = f_current\n            \n        # Exponential cooling\n        T *= r\n        \n    f_SA = f_best\n\n    return f_SA  f_greedy\n\nif __name__ == '__main__':\n    solve()\n\n```", "id": "3193389"}, {"introduction": "While a standard SA implementation uses a fixed proposal strategy, performance can be greatly enhanced by adapting to the local energy landscape. This advanced exercise challenges you to design an intelligent proposal mechanism that uses local curvature, given by the second derivative $E''(x)$, to automatically adjust its step size—taking large leaps in flat regions and small, careful steps near minima [@problem_id:3193403]. This requires implementing the full Metropolis-Hastings acceptance rule for asymmetric proposals, providing a deeper understanding of the theoretical guarantees that ensure convergence.", "problem": "You are asked to design, analyze, and test a simulated annealing transition kernel that proposes new states using a neighborhood function that adapts its step size to local curvature. The goal is to reason from first principles and verify two properties: global exploration improvement and preservation of detailed balance at fixed temperature.\n\nConsider a one-dimensional energy landscape with coordinate $x \\in \\mathbb{R}$ and energy function\n$$\nE(x) = \\frac{x^2}{50} - \\sum_{i=1}^{3} a_i \\exp\\left(-\\frac{(x - b_i)^2}{2 c_i^2}\\right),\n$$\nwith parameters $a_1 = 3.5$, $a_2 = 4.0$, $a_3 = 3.0$, $b_1 = -6.0$, $b_2 = 0.0$, $b_3 = 5.0$, $c_1 = 1.0$, $c_2 = 0.7$, $c_3 = 1.5$. The function $E(x)$ has multiple local minima induced by the negative Gaussian wells and a weak quadratic term that bounds the landscape. Let $E''(x)$ denote the second derivative of $E(x)$, which coincides with the Laplacian $\\nabla^2 E(x)$ in one dimension.\n\nThe base of the reasoning must be the following well-tested definitions and principles:\n- The Boltzmann stationary distribution at fixed temperature $T$ is proportional to $\\exp(-E(x)/T)$.\n- A Markov Chain Monte Carlo (MCMC) method can preserve detailed balance if its transition kernel satisfies the detailed balance condition with respect to the target distribution.\n- The Metropolis-Hastings (MH) acceptance rule enforces detailed balance for possibly asymmetric proposal distributions by correcting using the ratio of proposal densities.\n\nYou must construct two proposal mechanisms $q(\\cdot \\mid x)$ for candidate generation:\n- Baseline proposal: a Gaussian proposal $y \\sim \\mathcal{N}(x, \\sigma_0^2)$ with constant step size $\\sigma_0$ (independent of $x$).\n- Curvature-aware proposal: a Gaussian proposal $y \\sim \\mathcal{N}(x, \\sigma(x)^2)$ whose standard deviation adapts to local curvature according to\n$$\n\\sigma(x) = \\mathrm{clip}\\left(\\frac{s_0}{\\sqrt{1 + \\alpha \\, |E''(x)|}}, \\ \\sigma_{\\min}, \\ \\sigma_{\\max}\\right),\n$$\nwhere $s_0  0$ is a base scale, $\\alpha \\ge 0$ controls sensitivity to curvature magnitude $|E''(x)|$, and $\\mathrm{clip}(u, \\ell, r)$ denotes bounding $u$ to lie in the interval $[\\ell, r]$.\n\nFor both proposals, enforce detailed balance at fixed temperature $T$ by implementing the Metropolis-Hastings acceptance probability\n$$\na(x,y) = \\min\\left(1,\\ \\exp\\left(-\\frac{E(y)-E(x)}{T}\\right)\\ \\frac{q(x \\mid y)}{q(y \\mid x)}\\right),\n$$\nwhere $q(y \\mid x)$ is the proposal density of $y$ given $x$. For the baseline proposal, $q$ is symmetric and the ratio simplifies; for the curvature-aware proposal, $q$ is generally asymmetric and the ratio must be evaluated explicitly.\n\nDefine three basin centers based on the Gaussian wells at $b_1 = -6$, $b_2 = 0$, and $b_3 = 5$. A visit to basin $i$ is recorded when the chain has a state $x$ satisfying $|x - b_i| \\le c_i$. The global exploration score is the number of distinct basins visited at least once during a run.\n\nYour program must implement the following test suite. All angles are nonexistent here and there are no physical units. All parameters and outputs are pure numbers.\n\nTest suite specification:\n- Test $1$ (baseline, happy path): baseline proposal with constant step size $\\sigma_0 = 1.0$, temperature $T = 0.6$, total steps $N = 5000$, initial state $x_0 = 0.0$, random seed $42$. Output the integer global exploration score as described above.\n- Test $2$ (curvature-aware, happy path): curvature-aware proposal with $s_0 = 1.0$, $\\alpha = 5.0$, $\\sigma_{\\min} = 0.05$, $\\sigma_{\\max} = 2.0$, temperature $T = 0.6$, total steps $N = 5000$, initial state $x_0 = 0.0$, random seed $42$. Output the integer global exploration score.\n- Test $3$ (numerical detailed balance check at fixed $T$): using curvature-aware parameters $s_0 = 1.0$, $\\alpha = 5.0$, $\\sigma_{\\min} = 0.05$, $\\sigma_{\\max} = 2.0$, temperature $T = 0.6$, evaluate the detailed balance equality between two specific states $x = 0.5$ and $y = -2.1$ by computing\n$$\nL = \\exp\\left(-\\frac{E(x)}{T}\\right)\\ q(y \\mid x)\\ a(x,y), \\quad R = \\exp\\left(-\\frac{E(y)}{T}\\right)\\ q(x \\mid y)\\ a(y,x),\n$$\nwith $a(\\cdot,\\cdot)$ given above. Return the boolean value of whether the absolute relative difference\n$$\n\\delta = \\frac{|L - R|}{\\max\\left(|L|, |R|, 10^{-12}\\right)}\n$$\nis less than a tolerance of $10^{-6}$.\n- Test $4$ (edge case for sensitivity and clipping): curvature-aware proposal with $s_0 = 1.0$, $\\alpha = 100.0$, $\\sigma_{\\min} = 0.05$, $\\sigma_{\\max} = 2.0$, temperature $T = 0.6$, total steps $N = 5000$, initial state $x_0 = 0.0$, random seed $7$. Output the integer global exploration score.\n\nFinal output format requirement:\nYour program should produce a single line of output containing the results of the four tests as a comma-separated list enclosed in square brackets, for example $[r_1,r_2,r_3,r_4]$, where $r_1$ and $r_2$ and $r_4$ are integers and $r_3$ is a boolean. No other text should be printed.", "solution": "The problem statement has been rigorously validated and is determined to be sound, well-posed, and scientifically grounded. It presents a formal computational science problem suitable for analysis and implementation. All necessary functions, parameters, and test conditions are explicitly defined, allowing for a unique and verifiable solution.\n\nThe core of the problem is to design and analyze a Markov Chain Monte Carlo (MCMC) simulation based on the Metropolis-Hastings algorithm to sample from a target probability distribution. The target distribution is the Boltzmann distribution for a one-dimensional energy landscape $E(x)$ at a fixed temperature $T$. The stationary probability of finding the system in state $x$ is given by $P(x) \\propto \\exp(-E(x)/T)$.\n\nThe given energy landscape is defined as:\n$$\nE(x) = \\frac{x^2}{50} - \\sum_{i=1}^{3} a_i \\exp\\left(-\\frac{(x - b_i)^2}{2 c_i^2}\\right)\n$$\nThis function consists of a broad quadratic potential, which ensures the landscape is confining ($E(x) \\to \\infty$ as $|x| \\to \\infty$), and three Gaussian wells that create local energy minima.\n\nA key component of the proposed adaptive algorithm is the local curvature of the energy landscape, given by its second derivative $E''(x)$. We first derive this quantity. The first derivative is:\n$$\nE'(x) = \\frac{d}{dx} \\left[ \\frac{x^2}{50} - \\sum_{i=1}^{3} a_i \\exp\\left(-\\frac{(x - b_i)^2}{2 c_i^2}\\right) \\right] = \\frac{x}{25} - \\sum_{i=1}^{3} a_i \\exp\\left(-\\frac{(x - b_i)^2}{2 c_i^2}\\right) \\left(-\\frac{x - b_i}{c_i^2}\\right)\n$$\n$$\nE'(x) = \\frac{x}{25} + \\sum_{i=1}^{3} \\frac{a_i(x - b_i)}{c_i^2} \\exp\\left(-\\frac{(x - b_i)^2}{2 c_i^2}\\right)\n$$\nDifferentiating a second time yields:\n$$\nE''(x) = \\frac{d}{dx} E'(x) = \\frac{1}{25} + \\sum_{i=1}^{3} \\frac{a_i}{c_i^2} \\frac{d}{dx} \\left[ (x - b_i) \\exp\\left(-\\frac{(x - b_i)^2}{2 c_i^2}\\right) \\right]\n$$\nUsing the product rule, we get:\n$$\nE''(x) = \\frac{1}{25} + \\sum_{i=1}^{3} \\frac{a_i}{c_i^2} \\left[ 1 \\cdot \\exp\\left(-\\frac{(x - b_i)^2}{2 c_i^2}\\right) + (x - b_i) \\left( \\exp\\left(-\\frac{(x - b_i)^2}{2 c_i^2}\\right) \\left(-\\frac{x - b_i}{c_i^2}\\right) \\right) \\right]\n$$\n$$\nE''(x) = \\frac{1}{25} + \\sum_{i=1}^{3} \\frac{a_i}{c_i^2} \\exp\\left(-\\frac{(x - b_i)^2}{2 c_i^2}\\right) \\left[ 1 - \\frac{(x-b_i)^2}{c_i^2} \\right]\n$$\nThis expression for $E''(x)$ is fundamental for the curvature-aware proposal kernel.\n\nThe Metropolis-Hastings algorithm generates a sequence of states where each new state $y$ is proposed from the current state $x$ according to a proposal distribution $q(y|x)$ and is accepted with probability $a(x,y)$:\n$$\na(x,y) = \\min\\left(1, \\frac{P(y)q(x \\mid y)}{P(x)q(y \\mid x)}\\right) = \\min\\left(1, \\exp\\left(-\\frac{E(y)-E(x)}{T}\\right) \\frac{q(x \\mid y)}{q(y \\mid x)}\\right)\n$$\nThis construction ensures that the resulting Markov chain satisfies the detailed balance condition with respect to $P(x)$, guaranteeing that the chain's stationary distribution is indeed $P(x)$.\n\nWe analyze two proposal kernels:\n\n1.  **Baseline Proposal**: A Gaussian centered at the current state, $y \\sim \\mathcal{N}(x, \\sigma_0^2)$. The proposal probability density is $q(y|x) = (2\\pi\\sigma_0^2)^{-1/2} \\exp(-(y-x)^2/(2\\sigma_0^2))$. This kernel is symmetric, meaning $q(y|x) = q(x|y)$ because the step size $\\sigma_0$ is constant. The ratio of proposal densities is $\\frac{q(x|y)}{q(y|x)} = 1$. The acceptance probability simplifies to the Metropolis form:\n    $$\n    a_{\\text{baseline}}(x,y) = \\min\\left(1, \\exp\\left(-\\frac{E(y)-E(x)}{T}\\right)\\right)\n    $$\n    A fixed step size $\\sigma_0$ presents a trade-off: if it is large, acceptance rates in steep regions (minima) are low; if it is small, the chain mixes slowly and struggles to traverse flat regions between minima.\n\n2.  **Curvature-Aware Proposal**: A Gaussian with a state-dependent step size, $y \\sim \\mathcal{N}(x, \\sigma(x)^2)$. The step size $\\sigma(x)$ is defined as:\n    $$\n    \\sigma(x) = \\mathrm{clip}\\left(\\frac{s_0}{\\sqrt{1 + \\alpha \\, |E''(x)|}}, \\ \\sigma_{\\min}, \\ \\sigma_{\\max}\\right)\n    $$\n    This design is intended to improve sampling efficiency. In regions of high curvature (large $|E''(x)|$), such as inside the energy wells, $\\sigma(x)$ is small, promoting fine-grained local exploration. In regions of low curvature (small $|E''(x)|$), such as the plateaus between wells, $\\sigma(x)$ is large, facilitating large jumps that can cross energy barriers.\n    Since $\\sigma(x)$ depends on the state, this proposal kernel is generally asymmetric: $q(y|x) \\neq q(x|y)$ if $\\sigma(x) \\neq \\sigma(y)$. We must therefore calculate the full proposal density ratio:\n    $$\n    q(y|x) = \\frac{1}{\\sqrt{2\\pi\\sigma(x)^2}} \\exp\\left(-\\frac{(y-x)^2}{2\\sigma(x)^2}\\right)\n    $$\n    $$\n    \\frac{q(x|y)}{q(y|x)} = \\frac{\\frac{1}{\\sqrt{2\\pi\\sigma(y)^2}} \\exp\\left(-\\frac{(x-y)^2}{2\\sigma(y)^2}\\right)}{\\frac{1}{\\sqrt{2\\pi\\sigma(x)^2}} \\exp\\left(-\\frac{(y-x)^2}{2\\sigma(x)^2}\\right)} = \\frac{\\sigma(x)}{\\sigma(y)} \\exp\\left(\\frac{(x-y)^2}{2} \\left[ \\frac{1}{\\sigma(x)^2} - \\frac{1}{\\sigma(y)^2} \\right] \\right)\n    $$\n    This ratio correctly accounts for the asymmetry and must be included in the acceptance probability calculation to preserve detailed balance.\n\nThe third test case provides a numerical verification of the detailed balance condition. The condition states that for any two states $x$ and $y$, the rate of flow from $x$ to $y$ equals the rate of flow from $y$ to $x$ in the stationary distribution: $P(x) K(y|x) = P(y) K(x|y)$, where $K(y|x) = q(y|x)a(x,y)$ is the full transition probability. Substituting the definitions, we must verify:\n$$\n\\exp(-E(x)/T) \\cdot q(y|x) \\cdot a(x,y) = \\exp(-E(y)/T) \\cdot q(x|y) \\cdot a(y,x)\n$$\nThe quantities $L$ and $R$ in the problem statement are precisely the left-hand side and right-hand side of this equation (up to a global normalization constant which cancels). The detailed balance property implies $L=R$. The test confirms this equality holds up to numerical floating-point precision.\n\nThe implementation will proceed by first defining the energy $E(x)$ and its second derivative $E''(x)$. Then, a general MCMC simulation function will be constructed to handle both proposal types. For the curvature-aware case, it will compute $\\sigma(x)$ and the asymmetric proposal ratio. The global exploration score is computed post-simulation by checking the MCMC trajectory against the defined basin regions $|x - b_i| \\le c_i$. The detailed balance test will be a separate function performing a direct calculation of $L$ and $R$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    # Define global parameters for the energy function\n    A = np.array([3.5, 4.0, 3.0])\n    B = np.array([-6.0, 0.0, 5.0])\n    C = np.array([1.0, 0.7, 1.5])\n\n    def E(x):\n        \"\"\"Computes the energy E(x).\"\"\"\n        gauss_term = A * np.exp(-(x - B)**2 / (2 * C**2))\n        return x**2 / 50 - np.sum(gauss_term)\n\n    def E_ddot(x):\n        \"\"\"Computes the second derivative E''(x).\"\"\"\n        term = (A / C**2) * (1 - (x - B)**2 / C**2) * np.exp(-(x - B)**2 / (2 * C**2))\n        return 1/25 + np.sum(term)\n\n    def sigma_x(x, s0, alpha, s_min, s_max):\n        \"\"\"Computes the adaptive step size sigma(x).\"\"\"\n        val = s0 / np.sqrt(1 + alpha * np.abs(E_ddot(x)))\n        return np.clip(val, s_min, s_max)\n\n    def run_mcmc(params):\n        \"\"\"Runs a single MCMC simulation and returns the exploration score.\"\"\"\n        np.random.seed(params['seed'])\n        \n        T = params['T']\n        N = params['N']\n        x0 = params['x0']\n        \n        x_chain = np.zeros(N)\n        x_chain[0] = x0\n        x_current = x0\n        \n        is_baseline = params['type'] == 'baseline'\n\n        for i in range(1, N):\n            # 1. Propose a new state y\n            if is_baseline:\n                sigma_0 = params['sigma0']\n                y = np.random.normal(x_current, sigma_0)\n                sigma_current = sigma_y = sigma_0\n            else: # Curvature-aware\n                s0, alpha, s_min, s_max = params['s0'], params['alpha'], params['s_min'], params['s_max']\n                sigma_current = sigma_x(x_current, s0, alpha, s_min, s_max)\n                y = np.random.normal(x_current, sigma_current)\n                sigma_y = sigma_x(y, s0, alpha, s_min, s_max)\n\n            # 2. Calculate acceptance probability\n            delta_E = E(y) - E(x_current)\n\n            # For asymmetric proposal, calculate q(x|y)/q(y|x)\n            if is_baseline:\n                q_ratio = 1.0\n            else:\n                # Handle potential division by zero\n                if sigma_y == 0 or sigma_current == 0:\n                     q_ratio = 1.0 if sigma_y == sigma_current else np.inf\n                else:\n                    log_q_ratio = np.log(sigma_current) - np.log(sigma_y) + \\\n                                  0.5 * (y - x_current)**2 * (1/sigma_current**2 - 1/sigma_y**2)\n                    q_ratio = np.exp(log_q_ratio)\n            \n            acceptance_prob = min(1.0, np.exp(-delta_E / T) * q_ratio)\n\n            # 3. Accept or reject\n            if np.random.rand()  acceptance_prob:\n                x_current = y\n            \n            x_chain[i] = x_current\n\n        # Calculate exploration score\n        visited_basins = set()\n        for j, b_j in enumerate(B):\n            c_j = C[j]\n            if np.any(np.abs(x_chain - b_j) = c_j):\n                visited_basins.add(j)\n        \n        return len(visited_basins)\n\n    def check_detailed_balance(params):\n        \"\"\"Numerically verifies the detailed balance condition for two states.\"\"\"\n        x, y, T = params['x'], params['y'], params['T']\n        s0, alpha, s_min, s_max = params['s0'], params['alpha'], params['s_min'], params['s_max']\n\n        # Proposal densities q(y|x) and q(x|y)\n        sigma_at_x = sigma_x(x, s0, alpha, s_min, s_max)\n        sigma_at_y = sigma_x(y, s0, alpha, s_min, s_max)\n\n        log_q_yx = -0.5 * np.log(2 * np.pi * sigma_at_x**2) - (y - x)**2 / (2 * sigma_at_x**2)\n        log_q_xy = -0.5 * np.log(2 * np.pi * sigma_at_y**2) - (x - y)**2 / (2 * sigma_at_y**2)\n        q_yx = np.exp(log_q_yx)\n        q_xy = np.exp(log_q_xy)\n\n        # Acceptance probabilities a(x,y) and a(y,x)\n        p_ratio_xy = np.exp(-(E(y) - E(x)) / T) * (q_xy / q_yx)\n        a_xy = min(1.0, p_ratio_xy)\n\n        p_ratio_yx = np.exp(-(E(x) - E(y)) / T) * (q_yx / q_xy)\n        a_yx = min(1.0, p_ratio_yx)\n\n        # LHS and RHS of detailed balance equation\n        L = np.exp(-E(x) / T) * q_yx * a_xy\n        R = np.exp(-E(y) / T) * q_xy * a_yx\n        \n        # Relative difference check\n        abs_diff = np.abs(L - R)\n        norm = max(np.abs(L), np.abs(R), 1e-12)\n        delta = abs_diff / norm\n        \n        return delta  1e-6\n\n    # Test cases from the problem statement\n    test_cases = [\n        # Test 1: Baseline\n        {'type': 'baseline', 'sigma0': 1.0, 'T': 0.6, 'N': 5000, 'x0': 0.0, 'seed': 42},\n        # Test 2: Curvature-aware\n        {'type': 'curvature', 's0': 1.0, 'alpha': 5.0, 's_min': 0.05, 's_max': 2.0, 'T': 0.6, 'N': 5000, 'x0': 0.0, 'seed': 42},\n        # Test 3: Detailed Balance Check\n        {'type': 'db_check', 's0': 1.0, 'alpha': 5.0, 's_min': 0.05, 's_max': 2.0, 'T': 0.6, 'x': 0.5, 'y': -2.1},\n        # Test 4: Curvature-aware (high alpha)\n        {'type': 'curvature', 's0': 1.0, 'alpha': 100.0, 's_min': 0.05, 's_max': 2.0, 'T': 0.6, 'N': 5000, 'x0': 0.0, 'seed': 7}\n    ]\n\n    results = []\n    for case in test_cases:\n        if case['type'] == 'db_check':\n            result = check_detailed_balance(case)\n        else:\n            result = run_mcmc(case)\n        results.append(result)\n\n    # Format boolean as lowercase for final output\n    formatted_results = [str(r).lower() if isinstance(r, bool) else str(r) for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3193403"}]}