{"hands_on_practices": [{"introduction": "The first step in any optimization problem is to precisely define what we want to minimize. In non-linear least squares, this objective is the sum of squared errors, which is built from the residual vector, $r(x)$. This practice will guide you through calculating this vector for a common problem: fitting a circle to a set of data points, providing a concrete understanding of how we quantify the \"misfit\" between a model and the data. [@problem_id:2217008]", "problem": "A common task in optimization is fitting a geometric model to a set of data points. Consider the problem of fitting a circle to a set of points in a 2D plane. The circle is defined by a parameter vector $\\mathbf{p} = [x_c, y_c, R]^T$, where $(x_c, y_c)$ is the center of the circle and $R$ is its radius.\n\nFor a set of $n$ data points $(x_i, y_i)$, the goal of a non-linear least squares fit is to find the parameter vector $\\mathbf{p}$ that minimizes the sum of squared residuals. The residual for the $i$-th data point, $r_i(\\mathbf{p})$, is defined as the difference between the point's distance to the proposed center $(x_c, y_c)$ and the proposed radius $R$.\n\nYou are given three data points representing measurements in a Cartesian coordinate system where all coordinates are in meters:\n$P_1 = (1.0, 7.0)$\n$P_2 = (6.0, 2.0)$\n$P_3 = (9.0, 8.0)$\n\nAn iterative optimization algorithm, such as the Levenberg-Marquardt algorithm, begins with an initial guess for the parameters. The initial guess for the circle's parameters is given as $\\mathbf{p}_0 = [x_{c,0}, y_{c,0}, R_0]^T = [5.0, 5.0, 4.0]^T$.\n\nCalculate the residual vector $\\mathbf{r}(\\mathbf{p}_0) = [r_1(\\mathbf{p}_0), r_2(\\mathbf{p}_0), r_3(\\mathbf{p}_0)]^T$ for this initial guess. Express each component of the resulting vector in meters, rounded to four significant figures. Present your final answer as a single row matrix.", "solution": "For a circle with parameters $\\mathbf{p} = [x_{c}, y_{c}, R]^{T}$ and a data point $(x_{i}, y_{i})$, the residual is defined as the difference between the Euclidean distance from the point to the center and the radius:\n$$\nr_{i}(\\mathbf{p}) = \\sqrt{(x_{i} - x_{c})^{2} + (y_{i} - y_{c})^{2}} - R.\n$$\nWith the initial guess $\\mathbf{p}_{0} = [5.0, 5.0, 4.0]^{T}$, compute each residual.\n\nFor $P_{1} = (1.0, 7.0)$:\n$$\nd_{1} = \\sqrt{(1.0 - 5.0)^{2} + (7.0 - 5.0)^{2}} = \\sqrt{(-4.0)^{2} + (2.0)^{2}} = \\sqrt{16 + 4} = \\sqrt{20},\n$$\n$$\nr_{1}(\\mathbf{p}_{0}) = \\sqrt{20} - 4.0 \\approx 0.4721 \\text{ (to four significant figures)}.\n$$\n\nFor $P_{2} = (6.0, 2.0)$:\n$$\nd_{2} = \\sqrt{(6.0 - 5.0)^{2} + (2.0 - 5.0)^{2}} = \\sqrt{(1.0)^{2} + (-3.0)^{2}} = \\sqrt{1 + 9} = \\sqrt{10},\n$$\n$$\nr_{2}(\\mathbf{p}_{0}) = \\sqrt{10} - 4.0 \\approx -0.8377 \\text{ (to four significant figures)}.\n$$\n\nFor $P_{3} = (9.0, 8.0)$:\n$$\nd_{3} = \\sqrt{(9.0 - 5.0)^{2} + (8.0 - 5.0)^{2}} = \\sqrt{(4.0)^{2} + (3.0)^{2}} = \\sqrt{16 + 9} = \\sqrt{25} = 5.0,\n$$\n$$\nr_{3}(\\mathbf{p}_{0}) = 5.0 - 4.0 = 1.000 \\text{ (to four significant figures)}.\n$$\n\nThus the residual vector, as a row matrix, is:\n$$\n\\begin{pmatrix}\n0.4721 & -0.8377 & 1.000\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix} 0.4721 & -0.8377 & 1.000 \\end{pmatrix}}$$", "id": "2217008"}, {"introduction": "To intelligently update our model's parameters, we need to know how the error changes as we adjust them. The Jacobian matrix, $J(x)$, is the key tool that provides this information, offering a linear approximation of our non-linear system's sensitivity to parameter changes. In this exercise, you will compute the Jacobian for a rational function model, a fundamental skill for implementing gradient-based optimization algorithms like Levenberg-Marquardt. [@problem_id:2217052]", "problem": "In the field of non-linear optimization, algorithms like the Levenberg-Marquardt algorithm are used to fit a model function to a set of data points by minimizing the sum of the squares of the residuals. A key component in this process is the Jacobian matrix of the model function.\n\nConsider a model used to describe the concentration of a substance over time, given by the two-parameter rational function:\n$$f(t; a, b) = \\frac{a}{1 + bt}$$\nwhere $t$ is the independent variable (e.g., time), and $\\mathbf{p} = [a, b]^T$ is the vector of parameters to be determined.\n\nSuppose we have collected the following three data points $(t_i, y_i)$:\n- $(t_1, y_1) = (1.0, 2.5)$\n- $(t_2, y_2) = (2.0, 1.8)$\n- $(t_3, y_3) = (4.0, 1.1)$\n\nThe Jacobian matrix $\\mathbf{J}$ for this fitting problem is an $m \\times n$ matrix, where $m$ is the number of data points and $n$ is the number of parameters. Its elements are defined by $J_{ij} = \\frac{\\partial f(t_i; \\mathbf{p})}{\\partial p_j}$.\n\nCalculate the numerical value of the Jacobian matrix $\\mathbf{J}$ evaluated at the initial parameter guess $\\mathbf{p}_0 = [a_0, b_0]^T = [3.0, 0.5]^T$. All given numerical values are dimensionless.\n\nExpress your final answer as a $3 \\times 2$ matrix, with each element rounded to three significant figures.", "solution": "We are given the model function $f(t; a, b) = \\dfrac{a}{1 + bt}$ and the Jacobian matrix defined by $J_{ij} = \\dfrac{\\partial f(t_{i}; \\mathbf{p})}{\\partial p_{j}}$ for the parameter vector $\\mathbf{p} = [a, b]^{T}$. Thus, each row of $\\mathbf{J}$ corresponds to a data point $t_{i}$, and the two columns correspond to the derivatives with respect to $a$ and $b$.\n\nFirst, compute the partial derivatives symbolically. Write $f(t; a, b) = a(1 + bt)^{-1}$. Then, using the power rule and chain rule:\n$$\n\\frac{\\partial f}{\\partial a} = (1 + bt)^{-1} = \\frac{1}{1 + bt},\n$$\n$$\n\\frac{\\partial f}{\\partial b} = a \\cdot \\frac{\\partial}{\\partial b}(1 + bt)^{-1} = a \\cdot \\left[-(1 + bt)^{-2} \\cdot t\\right] = -\\frac{a t}{(1 + bt)^{2}}.\n$$\n\nTherefore, for each data point $t_{i}$, the Jacobian row is\n$$\n\\left[\\frac{1}{1 + b t_{i}},\\ -\\frac{a t_{i}}{(1 + b t_{i})^{2}}\\right].\n$$\n\nEvaluate at the initial guess $\\mathbf{p}_{0} = [a_{0}, b_{0}]^{T} = [3.0, 0.5]^{T}$ and the given $t$ values.\n\nFor $t_{1} = 1$:\n$$\n1 + b_{0} t_{1} = 1 + 0.5 \\cdot 1 = \\frac{3}{2},\\quad \\frac{\\partial f}{\\partial a} = \\frac{2}{3},\\quad \\frac{\\partial f}{\\partial b} = -\\frac{3 \\cdot 1}{(3/2)^{2}} = -\\frac{12}{9} = -\\frac{4}{3}.\n$$\n\nFor $t_{2} = 2$:\n$$\n1 + b_{0} t_{2} = 1 + 0.5 \\cdot 2 = 2,\\quad \\frac{\\partial f}{\\partial a} = \\frac{1}{2},\\quad \\frac{\\partial f}{\\partial b} = -\\frac{3 \\cdot 2}{2^{2}} = -\\frac{6}{4} = -\\frac{3}{2}.\n$$\n\nFor $t_{3} = 4$:\n$$\n1 + b_{0} t_{3} = 1 + 0.5 \\cdot 4 = 3,\\quad \\frac{\\partial f}{\\partial a} = \\frac{1}{3},\\quad \\frac{\\partial f}{\\partial b} = -\\frac{3 \\cdot 4}{3^{2}} = -\\frac{12}{9} = -\\frac{4}{3}.\n$$\n\nAssemble the Jacobian and round each element to three significant figures:\n$$\n\\mathbf{J}(\\mathbf{p}_{0}) \\approx\n\\begin{pmatrix}\n0.667 & -1.33 \\\\\n0.500 & -1.50 \\\\\n0.333 & -1.33\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}0.667 & -1.33 \\\\ 0.500 & -1.50 \\\\ 0.333 & -1.33\\end{pmatrix}}$$", "id": "2217052"}, {"introduction": "The Levenberg-Marquardt algorithm, like many optimization methods, relies on the gradient to navigate the error landscape. This exercise bridges the gap between the components we've seen—the residual vector and the Jacobian matrix—and the goal of minimizing the error function, $S(x)$. You will derive a compact and essential expression for the gradient, $\\nabla S(x)$, revealing the elegant relationship between these core concepts. [@problem_id:2216997]", "problem": "In the field of numerical optimization, a common task is to solve nonlinear least squares problems, which involve finding model parameters that best fit a set of observed data.\n\nLet a model be described by a differentiable vector-valued function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$, which maps a parameter vector $x \\in \\mathbb{R}^n$ to a predicted outcome vector. Let $y \\in \\mathbb{R}^m$ be a vector of corresponding measured data. The discrepancy between the model's predictions and the measurements is captured by the residual vector $r(x) = f(x) - y$.\n\nThe objective is to minimize the sum-of-squares error function, $S(x)$, defined as half the squared Euclidean norm of the residual vector:\n$$ S(x) = \\frac{1}{2} r(x)^T r(x) = \\frac{1}{2} \\sum_{i=1}^{m} [r_i(x)]^2 $$\nwhere $r_i(x)$ is the $i$-th component of $r(x)$.\n\nGradient-based optimization algorithms, such as the Levenberg-Marquardt algorithm, require the gradient of this error function. The gradient, denoted $\\nabla S(x)$, is the $n \\times 1$ column vector of first partial derivatives, where the $j$-th component is $(\\nabla S(x))_j = \\frac{\\partial S}{\\partial x_j}$.\n\nThe Jacobian matrix of the residual vector, $J(x)$, is the $m \\times n$ matrix defined by its entries $J_{ij}(x) = \\frac{\\partial r_i}{\\partial x_j}$.\n\nDetermine the expression for the gradient $\\nabla S(x)$. Your answer should be a compact expression in terms of the Jacobian matrix $J(x)$ and the residual vector $r(x)$.", "solution": "We are given $S(x) = \\frac{1}{2} r(x)^{T} r(x)$ with $r(x) \\in \\mathbb{R}^{m}$ and $J(x) \\in \\mathbb{R}^{m \\times n}$ where $J_{ij}(x) = \\frac{\\partial r_{i}}{\\partial x_{j}}$. We compute the gradient using differentials and the chain rule.\n\nFirst, take the differential of $S(x)$:\n$$\ndS = \\frac{1}{2} d\\big(r^{T} r\\big) = \\frac{1}{2}\\big(dr^{T} r + r^{T} dr\\big) = r^{T} dr,\n$$\nsince $dr^{T} r = r^{T} dr$ as both are scalars.\n\nUsing the Jacobian, the differential of the residual is\n$$\ndr = J(x)\\,dx.\n$$\nSubstituting into the expression for $dS$ gives\n$$\ndS = r(x)^{T} J(x)\\, dx = \\big(J(x)^{T} r(x)\\big)^{T} dx.\n$$\nBy the definition of the gradient as the unique vector satisfying $dS = \\big(\\nabla S(x)\\big)^{T} dx$ for all $dx$, we conclude\n$$\n\\nabla S(x) = J(x)^{T} r(x).\n$$\n\nFor verification in index notation, write\n$$\nS(x) = \\frac{1}{2} \\sum_{i=1}^{m} r_{i}(x)^{2},\n$$\nthen for each component $j \\in \\{1,\\dots,n\\}$,\n$$\n\\frac{\\partial S}{\\partial x_{j}} = \\sum_{i=1}^{m} r_{i}(x)\\, \\frac{\\partial r_{i}(x)}{\\partial x_{j}} = \\sum_{i=1}^{m} J_{ij}(x)\\, r_{i}(x),\n$$\nwhich is exactly the $j$-th component of $J(x)^{T} r(x)$. Hence,\n$$\n\\nabla S(x) = J(x)^{T} r(x).\n$$", "answer": "$$\\boxed{J(x)^{T} r(x)}$$", "id": "2216997"}]}