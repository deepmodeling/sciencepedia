{"hands_on_practices": [{"introduction": "Our exploration begins with a foundational concept that lies at the heart of constrained optimization. An unconstrained function might exhibit a saddle point, which is neither a minimum nor a maximum, yet this very same point can become a strict local minimum when confined to a specific path or surface. This practice [@problem_id:3201338] provides a crystal-clear illustration of this phenomenon, guiding you to verify how restricting the domain transforms a saddle point into a true minimizer by analyzing the function's curvature only along feasible directions.", "problem": "Consider the unconstrained objective function $f:\\mathbb{R}^{2}\\rightarrow\\mathbb{R}$ defined by $f(x_{1},x_{2})=x_{1}^{2}-x_{2}^{2}$. Impose the equality constraint $g(x_{1},x_{2})=x_{2}=0$ so that the feasible manifold is $\\mathcal{M}=\\{(x_{1},x_{2})\\in\\mathbb{R}^{2}\\mid x_{2}=0\\}$. Using foundational definitions from multivariate calculus and the Karush–Kuhn–Tucker (KKT) conditions for equality-constrained optimization, do the following:\n\n1. Identify the unconstrained stationary point(s) of $f$ and classify the point at $(0,0)$ using the Hessian matrix definition of local curvature. Explain why it is a saddle point of $f$ on $\\mathbb{R}^{2}$.\n\n2. Formulate the equality-constrained problem $\\min f(x_{1},x_{2})$ subject to $g(x_{1},x_{2})=0$, define the Lagrangian $\\mathcal{L}(x_{1},x_{2},\\lambda)=f(x_{1},x_{2})+\\lambda g(x_{1},x_{2})$, and determine the candidate constrained optimizer $(x^{\\star},\\lambda^{\\star})$ by the first-order KKT conditions.\n\n3. Compute the tangent space at $x^{\\star}$, defined by $T_{x^{\\star}}\\mathcal{M}=\\{v\\in\\mathbb{R}^{2}\\mid \\nabla g(x^{\\star})^{\\top}v=0\\}$, and verify the second-order condition for a local minimum by evaluating the quadratic form $v^{\\top}\\nabla^{2}\\mathcal{L}(x^{\\star},\\lambda^{\\star})v$ on $T_{x^{\\star}}\\mathcal{M}$.\n\n4. Finally, calculate the minimum value of the restricted quadratic form $v^{\\top}\\nabla^{2}\\mathcal{L}(x^{\\star},\\lambda^{\\star})v$ over all unit vectors $v\\in T_{x^{\\star}}\\mathcal{M}$, that is,\n$$\n\\min\\left\\{v^{\\top}\\nabla^{2}\\mathcal{L}(x^{\\star},\\lambda^{\\star})v \\,\\middle|\\, v\\in T_{x^{\\star}}\\mathcal{M},\\, \\|v\\|_{2}=1\\right\\}.\n$$\nExpress your final answer as a single exact real number. No rounding is required.", "solution": "The user has provided a multi-part problem concerning unconstrained and constrained optimization. I will first validate the problem statement and then proceed with a step-by-step solution if it is deemed valid.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Objective function: $f(x_{1},x_{2})=x_{1}^{2}-x_{2}^{2}$, with $f:\\mathbb{R}^{2}\\rightarrow\\mathbb{R}$.\n- Equality constraint: $g(x_{1},x_{2})=x_{2}=0$.\n- Feasible manifold: $\\mathcal{M}=\\{(x_{1},x_{2})\\in\\mathbb{R}^{2}\\mid x_{2}=0\\}$.\n- Lagrangian definition: $\\mathcal{L}(x_{1},x_{2},\\lambda)=f(x_{1},x_{2})+\\lambda g(x_{1},x_{2})$.\n- Tangent space definition at $x^{\\star}$: $T_{x^{\\star}}\\mathcal{M}=\\{v\\in\\mathbb{R}^{2}\\mid \\nabla g(x^{\\star})^{\\top}v=0\\}$.\n- Task 1: Identify unconstrained stationary point(s) of $f$ and classify the point at $(0,0)$ using its Hessian.\n- Task 2: Formulate the constrained problem and find the candidate optimizer $(x^{\\star},\\lambda^{\\star})$ using first-order KKT conditions.\n- Task 3: Compute the tangent space $T_{x^{\\star}}\\mathcal{M}$ and verify the second-order condition for a local minimum using $v^{\\top}\\nabla^{2}\\mathcal{L}(x^{\\star},\\lambda^{\\star})v$.\n- Task 4: Calculate $\\min\\left\\{v^{\\top}\\nabla^{2}\\mathcal{L}(x^{\\star},\\lambda^{\\star})v \\,\\middle|\\, v\\in T_{x^{\\star}}\\mathcal{M},\\, \\|v\\|_{2}=1\\right\\}$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is a standard exercise in multivariable calculus and optimization theory. All definitions (Lagrangian, KKT conditions, Hessian, tangent space) are standard and mathematically correct.\n- **Well-Posed:** The problem is clearly stated with a sequence of logical tasks. The functions are well-behaved, ensuring that a unique solution exists for each part.\n- **Objective:** The language is formal, precise, and free of any subjective or ambiguous terms.\n- **Completeness and Consistency:** All necessary information, including the functions and definitions, is provided. The problem is self-contained and free of contradictions.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is a well-posed, scientifically grounded exercise in constrained optimization. I will proceed with the solution.\n\n### Solution\n\n**Part 1: Unconstrained Analysis**\n\nFirst, we find the stationary points of the unconstrained objective function $f(x_{1},x_{2})=x_{1}^{2}-x_{2}^{2}$ by finding where its gradient is zero. The gradient of $f$ is:\n$$\n\\nabla f(x_{1},x_{2}) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_{1}} \\\\ \\frac{\\partial f}{\\partial x_{2}} \\end{pmatrix} = \\begin{pmatrix} 2x_{1} \\\\ -2x_{2} \\end{pmatrix}\n$$\nSetting the gradient to the zero vector, $\\nabla f(x_{1},x_{2}) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, yields the system of equations:\n$$\n2x_{1} = 0 \\implies x_{1} = 0\n$$\n$$\n-2x_{2} = 0 \\implies x_{2} = 0\n$$\nThus, the only unconstrained stationary point is $(0,0)$.\n\nNext, we classify this point by examining the Hessian matrix of $f$, which describes the local curvature. The Hessian matrix $\\nabla^{2}f$ is the matrix of second-order partial derivatives:\n$$\n\\nabla^{2}f(x_{1},x_{2}) = \\begin{pmatrix} \\frac{\\partial^{2} f}{\\partial x_{1}^{2}} & \\frac{\\partial^{2} f}{\\partial x_{1}\\partial x_{2}} \\\\ \\frac{\\partial^{2} f}{\\partial x_{2}\\partial x_{1}} & \\frac{\\partial^{2} f}{\\partial x_{2}^{2}} \\end{pmatrix} = \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix}\n$$\nThe Hessian is a constant matrix. At the stationary point $(0,0)$, we have $\\nabla^{2}f(0,0) = \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix}$. To classify the point, we check the definiteness of this matrix by finding its eigenvalues. The eigenvalues are the diagonal entries, $\\lambda_{1}=2$ and $\\lambda_{2}=-2$. Since one eigenvalue is positive and one is negative, the Hessian matrix is indefinite. An indefinite Hessian at a stationary point indicates that the point is a saddle point. The function has positive curvature (is concave up) along the $x_{1}$-axis and negative curvature (is concave down) along the $x_{2}$-axis.\n\n**Part 2: Constrained Optimization and KKT Conditions**\n\nWe consider the problem of minimizing $f(x_{1},x_{2})$ subject to the equality constraint $g(x_{1},x_{2})=x_{2}=0$. The Lagrangian for this problem is:\n$$\n\\mathcal{L}(x_{1},x_{2},\\lambda) = f(x_{1},x_{2}) + \\lambda g(x_{1},x_{2}) = x_{1}^{2}-x_{2}^{2} + \\lambda x_{2}\n$$\nThe first-order Karush–Kuhn–Tucker (KKT) conditions require that the gradient of the Lagrangian with respect to all variables is zero.\n$$\n\\nabla\\mathcal{L}(x_{1},x_{2},\\lambda) = \\begin{pmatrix} \\frac{\\partial \\mathcal{L}}{\\partial x_{1}} \\\\ \\frac{\\partial \\mathcal{L}}{\\partial x_{2}} \\\\ \\frac{\\partial \\mathcal{L}}{\\partial \\lambda} \\end{pmatrix} = \\begin{pmatrix} 2x_{1} \\\\ -2x_{2}+\\lambda \\\\ x_{2} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThis gives us a system of three equations:\n1. $2x_{1} = 0 \\implies x_{1} = 0$\n2. $-2x_{2}+\\lambda = 0$\n3. $x_{2} = 0$\n\nFrom equation (1), we have $x_{1}=0$. From equation (3), we have $x_{2}=0$. Substituting $x_{2}=0$ into equation (2) gives $-2(0)+\\lambda=0$, which implies $\\lambda=0$.\nThe candidate constrained optimizer is $x^{\\star}=(x_{1}^{\\star},x_{2}^{\\star})=(0,0)$ with the corresponding Lagrange multiplier $\\lambda^{\\star}=0$.\n\n**Part 3: Second-Order Condition and Tangent Space**\n\nNow we verify the second-order condition for a local minimum. First, we determine the tangent space $T_{x^{\\star}}\\mathcal{M}$ at the point $x^{\\star}=(0,0)$. The constraint function is $g(x_{1},x_{2})=x_{2}$. Its gradient is:\n$$\n\\nabla g(x_{1},x_{2}) = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n$$\nThis gradient is constant. At $x^{\\star}=(0,0)$, we have $\\nabla g(x^{\\star}) = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$. The tangent space is the set of all vectors $v=(v_{1}, v_{2})^{\\top} \\in \\mathbb{R}^{2}$ that are orthogonal to $\\nabla g(x^{\\star})$:\n$$\nT_{x^{\\star}}\\mathcal{M} = \\{ v \\in \\mathbb{R}^{2} \\mid \\nabla g(x^{\\star})^{\\top}v = 0 \\}\n$$\n$$\n\\begin{pmatrix} 0 & 1 \\end{pmatrix} \\begin{pmatrix} v_{1} \\\\ v_{2} \\end{pmatrix} = 0 \\cdot v_{1} + 1 \\cdot v_{2} = v_{2} = 0\n$$\nTherefore, the tangent space is the set of all vectors of the form $v=(v_{1}, 0)^{\\top}$, which is the $x_{1}$-axis. $T_{x^{\\star}}\\mathcal{M} = \\text{span}\\left\\{\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\right\\}$.\n\nNext, we compute the Hessian of the Lagrangian with respect to the variables $x=(x_{1},x_{2})$:\n$$\n\\nabla_{x}^{2}\\mathcal{L}(x_{1},x_{2},\\lambda) = \\begin{pmatrix} \\frac{\\partial^{2} \\mathcal{L}}{\\partial x_{1}^{2}} & \\frac{\\partial^{2} \\mathcal{L}}{\\partial x_{1}\\partial x_{2}} \\\\ \\frac{\\partial^{2} \\mathcal{L}}{\\partial x_{2}\\partial x_{1}} & \\frac{\\partial^{2} \\mathcal{L}}{\\partial x_{2}^{2}} \\end{pmatrix} = \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix}\n$$\nWe evaluate this Hessian at the candidate point $(x^{\\star}, \\lambda^{\\star})=((0,0),0)$. Since the Hessian is constant, $\\nabla_{x}^{2}\\mathcal{L}(x^{\\star},\\lambda^{\\star}) = \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix}$.\n\nThe second-order sufficient condition for a strict local minimum requires that the quadratic form $v^{\\top}\\nabla_{x}^{2}\\mathcal{L}(x^{\\star},\\lambda^{\\star})v$ is positive definite for all non-zero vectors $v \\in T_{x^{\\star}}\\mathcal{M}$. Let $v \\in T_{x^{\\star}}\\mathcal{M}$ be a non-zero vector, so $v=(v_{1},0)^{\\top}$ with $v_{1} \\neq 0$.\n$$\nv^{\\top}\\nabla_{x}^{2}\\mathcal{L}(x^{\\star},\\lambda^{\\star})v = \\begin{pmatrix} v_{1} & 0 \\end{pmatrix} \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix} \\begin{pmatrix} v_{1} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2v_{1} & 0 \\end{pmatrix} \\begin{pmatrix} v_{1} \\\\ 0 \\end{pmatrix} = 2v_{1}^{2}\n$$\nSince $v_{1} \\neq 0$, $v_{1}^{2}>0$, and thus $2v_{1}^{2}>0$. The condition is satisfied, confirming that $x^{\\star}=(0,0)$ is a strict local minimum of the constrained problem.\n\n**Part 4: Final Calculation**\n\nWe need to calculate the minimum value of the quadratic form $v^{\\top}\\nabla_{x}^{2}\\mathcal{L}(x^{\\star},\\lambda^{\\star})v$ over all unit vectors $v$ in the tangent space $T_{x^{\\star}}\\mathcal{M}$. The set of such vectors is:\n$$\n\\{v \\in T_{x^{\\star}}\\mathcal{M} \\mid \\|v\\|_{2}=1 \\}\n$$\nA vector $v$ in the tangent space has the form $v=(v_{1},0)^{\\top}$. The unit norm condition means $\\|v\\|_{2} = \\sqrt{v_{1}^{2}+0^{2}} = |v_{1}| = 1$. This implies $v_{1}=1$ or $v_{1}=-1$. So there are two such vectors: $v^{(1)}=(1,0)^{\\top}$ and $v^{(2)}=(-1,0)^{\\top}$.\n\nFrom Part 3, we know the value of the quadratic form is $2v_{1}^{2}$. We need to find its minimum value under the condition $|v_{1}|=1$.\nIf $v_{1}=1$, the value is $2(1)^{2}=2$.\nIf $v_{1}=-1$, the value is $2(-1)^{2}=2$.\nIn both cases, the value is $2$. Therefore, the minimum (and maximum) value of the restricted quadratic form is $2$.\nThis value represents the curvature of the objective function along the feasible direction at the constrained minimum.", "answer": "$$\n\\boxed{2}\n$$", "id": "3201338"}, {"introduction": "Having established why constraints fundamentally alter the nature of an optimum, we now turn to practical algorithms. A powerful strategy is to convert a constrained problem into an unconstrained one by adding a \"penalty\" to the objective that penalizes any violation of the constraints. This exercise [@problem_id:3201293] challenges you to implement and contrast two related approaches: the straightforward quadratic penalty method and the more sophisticated augmented Lagrangian method, revealing how explicitly tracking Lagrange multipliers leads to a more robust and efficient algorithm.", "problem": "You are to implement and compare two unconstrained optimization approaches for solving a quadratic equality-constrained least squares problem, using only linear algebra and iterative updates. The goal is to examine how an augmented Lagrangian update of multipliers compares to a pure quadratic penalty approach on a fixed equality constraint, thereby exploring the relationship between unconstrained and constrained optimization from first principles.\n\nFundamental base and definitions to use:\n- An unconstrained minimum of a differentiable function occurs where the gradient is zero.\n- A constrained minimum of a differentiable function subject to equality constraints is described by the Karush-Kuhn-Tucker (KKT) conditions, derived by forming the Lagrangian and setting gradients with respect to both the decision variables and the multipliers to zero.\n- The Euclidean norm is denoted by $\\|\\cdot\\|_2$.\n- A least squares objective can be written as $f(x) = \\tfrac{1}{2}\\|A x - b\\|_2^2$ for a matrix $A$ and a vector $b$.\n- An equality constraint can be written as $h(x) = C x - d = 0$ for a matrix $C$ and a vector $d$.\n- The augmented Lagrangian for equality constraints is $\\mathcal{L}_\\rho(x,\\lambda) = f(x) + \\lambda^\\top h(x) + \\tfrac{\\rho}{2}\\|h(x)\\|_2^2$, where $\\lambda$ is the vector of Lagrange multipliers and $\\rho$ is a positive penalty parameter.\n\nYour program must:\n- Implement the augmented Lagrangian method by alternately minimizing $\\mathcal{L}_\\rho(x,\\lambda)$ with respect to $x$ for fixed $\\lambda$, and then updating $\\lambda$ using the standard multiplier update rule based on the current constraint residual. Perform a fixed number of iterations and report the final constraint violation $\\|h(x)\\|_2$.\n- Implement the pure quadratic penalty method by minimizing $f(x) + \\tfrac{\\rho}{2}\\|h(x)\\|_2^2$ without multipliers. For one test, use a single value of $\\rho$; for another, use an increasing schedule of $\\rho$ values to mimic tightening constraints. Report the final constraint violation $\\|h(x)\\|_2$.\n\nScientific realism conditions:\n- Use exact linear algebra solves for the quadratic subproblems.\n- Iterative updates for multipliers must be based on the current constraint residual.\n- No step size heuristics or shortcuts beyond the fundamental definitions above.\n\nAngle units and physical units: No physical quantities or angles are involved; therefore no units or angle specifications are required.\n\nTest suite:\n- All problems are in $\\mathbb{R}^n$ with $n=2$ and equality constraints of dimension $m \\in \\{1,2\\}$.\n- For each case, report two quantities: the final augmented Lagrangian constraint violation and the final pure penalty constraint violation. These must be floating-point numbers.\n\nCase $\\mathbf{1}$ (happy path, well-conditioned data):\n- $A = \\begin{bmatrix} 2 & -1 \\\\ 0 & 3 \\\\ 1 & 1 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\\\ 4 \\\\ 2 \\end{bmatrix}$.\n- $C = \\begin{bmatrix} 1 & 2 \\end{bmatrix}$, $d = \\begin{bmatrix} 3 \\end{bmatrix}$.\n- Augmented Lagrangian parameters: $\\rho = 1.0$, iterations $K = 10$, initial multiplier $\\lambda^{(0)} = \\begin{bmatrix} 0 \\end{bmatrix}$.\n- Pure penalty parameter: single $\\rho = 1.0$.\n\nCase $\\mathbf{2}$ (boundary condition: weak penalty, mildly ill-conditioned $A^\\top A$):\n- $A = \\begin{bmatrix} 10^{-3} & 0 \\\\ 0 & 1 \\\\ 1 & -1 \\end{bmatrix}$, $b = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0.5 \\end{bmatrix}$.\n- $C = \\begin{bmatrix} 1 & 0 \\end{bmatrix}$, $d = \\begin{bmatrix} 0.2 \\end{bmatrix}$.\n- Augmented Lagrangian parameters: $\\rho = 10^{-2}$, iterations $K = 50$, initial multiplier $\\lambda^{(0)} = \\begin{bmatrix} 0 \\end{bmatrix}$.\n- Pure penalty parameter: single $\\rho = 10^{-2}$.\n\nCase $\\mathbf{3}$ (edge case: redundant equality constraints):\n- $A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix}$, $b = \\begin{bmatrix} 7 \\\\ 8 \\\\ 9 \\end{bmatrix}$.\n- $C = \\begin{bmatrix} 1 & 1 \\\\ 2 & 2 \\end{bmatrix}$, $d = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n- Augmented Lagrangian parameters: $\\rho = 1.0$, iterations $K = 15$, initial multiplier $\\lambda^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n- Pure penalty parameters: increasing schedule $\\rho \\in \\{10^{-1}, 1.0, 10.0\\}$.\n\nAlgorithmic requirements:\n- For the augmented Lagrangian method, at each iteration $k \\in \\{0,1,\\dots,K-1\\}$: compute the exact minimizer of $\\mathcal{L}_\\rho(x,\\lambda^{(k)})$ with respect to $x$, then update the multiplier by adding $\\rho$ times the current constraint residual. After $K$ iterations, report $\\|h(x^{(K)})\\|_2$.\n- For the pure penalty method, compute the exact minimizer for the given $\\rho$ (or for each $\\rho$ in the schedule, sequentially), and report the final $\\|h(x)\\|_2$ after the last solve.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case contributes a list of two floating-point numbers in the order $\\left[\\|h(x)\\|_2 \\text{ from augmented Lagrangian}, \\|h(x)\\|_2 \\text{ from pure penalty}\\right]$. The final output must therefore be of the form $\\big[ [a_1, b_1], [a_2, b_2], [a_3, b_3] \\big]$ where each $a_i$ and $b_i$ are floating-point numbers corresponding to Cases $\\mathbf{1}$, $\\mathbf{2}$, and $\\mathbf{3}$ respectively.", "solution": "The problem is valid. It presents a well-posed task in the field of computational science and optimization, grounded in established mathematical principles. The problem is self-contained, with all necessary data and definitions provided. It is objective, scientifically sound, and free of contradictions or ambiguities.\n\nThe core of the problem is to find the minimizer $x \\in \\mathbb{R}^n$ for the equality-constrained quadratic program (ECQP):\n$$\n\\min_{x} f(x) = \\frac{1}{2}\\|A x - b\\|_2^2 \\quad \\text{subject to} \\quad h(x) = C x - d = 0\n$$\nwhere $A \\in \\mathbb{R}^{p \\times n}$, $b \\in \\mathbb{R}^p$, $C \\in \\mathbb{R}^{m \\times n}$, and $d \\in \\mathbb{R}^m$. We will implement and compare two iterative methods that solve this by tackling a sequence of unconstrained quadratic subproblems.\n\nFirst, we expand the objective function and its gradient:\n$f(x) = \\frac{1}{2}(x^\\top A^\\top A x - 2 b^\\top A x + b^\\top b)$\nThe gradient with respect to $x$ is:\n$\\nabla_x f(x) = A^\\top A x - A^\\top b$.\n\nWe also need the gradient of the squared norm of the constraint function:\n$\\frac{1}{2}\\|h(x)\\|_2^2 = \\frac{1}{2}\\|Cx - d\\|_2^2 = \\frac{1}{2}(x^\\top C^\\top C x - 2 d^\\top C x + d^\\top d)$\nThe gradient of this term is:\n$\\nabla_x \\left(\\frac{1}{2}\\|h(x)\\|_2^2\\right) = C^\\top(Cx-d) = C^\\top C x - C^\\top d$.\n\n**1. Augmented Lagrangian Method**\n\nThe augmented Lagrangian for this problem is given by:\n$$\n\\mathcal{L}_\\rho(x, \\lambda) = f(x) + \\lambda^\\top h(x) + \\frac{\\rho}{2}\\|h(x)\\|_2^2\n$$\nwhere $\\lambda \\in \\mathbb{R}^m$ is the vector of Lagrange multipliers and $\\rho > 0$ is the penalty parameter.\n\nThe augmented Lagrangian method is an iterative procedure. At each iteration $k$, for a fixed multiplier $\\lambda^{(k)}$, we find the next iterate $x^{(k+1)}$ by minimizing $\\mathcal{L}_\\rho(x, \\lambda^{(k)})$ with respect to $x$. Since $\\mathcal{L}_\\rho(x, \\lambda^{(k)})$ is a quadratic function of $x$, its minimum is found where its gradient with respect to $x$ is zero:\n$$\n\\nabla_x \\mathcal{L}_\\rho(x, \\lambda^{(k)}) = \\nabla_x f(x) + \\nabla_x(\\lambda^{(k)\\top}(Cx-d)) + \\nabla_x\\left(\\frac{\\rho}{2}\\|Cx - d\\|_2^2\\right) = 0\n$$\nSubstituting the gradient expressions:\n$$\n(A^\\top A x - A^\\top b) + C^\\top \\lambda^{(k)} + \\rho (C^\\top C x - C^\\top d) = 0\n$$\nTo solve for $x$, we group the terms involving $x$:\n$$\n(A^\\top A + \\rho C^\\top C) x = A^\\top b - C^\\top \\lambda^{(k)} + \\rho C^\\top d\n$$\nThis is a linear system of the form $H x = g$, where:\n- $H = A^\\top A + \\rho C^\\top C$\n- $g = A^\\top b + C^\\top(\\rho d - \\lambda^{(k)})$\n\nThe algorithm proceeds as follows for $k = 0, 1, \\dots, K-1$:\n1.  Solve for $x^{(k+1)}$: $x^{(k+1)} = (A^\\top A + \\rho C^\\top C)^{-1} (A^\\top b + C^\\top(\\rho d - \\lambda^{(k)}))$.\n2.  Update the Lagrange multiplier: $\\lambda^{(k+1)} = \\lambda^{(k)} + \\rho h(x^{(k+1)}) = \\lambda^{(k)} + \\rho(C x^{(k+1)} - d)$.\n\nAfter $K$ iterations, the final constraint violation is computed as $\\|h(x^{(K)})\\|_2 = \\|C x^{(K)} - d\\|_2$.\n\n**2. Pure Quadratic Penalty Method**\n\nThe pure quadratic penalty method aims to solve the constrained problem by minimizing the unconstrained penalty function:\n$$\nP_\\rho(x) = f(x) + \\frac{\\rho}{2}\\|h(x)\\|_2^2\n$$\nThis function is also quadratic in $x$. Its minimum is found by setting its gradient to zero:\n$$\n\\nabla_x P_\\rho(x) = \\nabla_x f(x) + \\rho \\nabla_x\\left(\\frac{1}{2}\\|h(x)\\|_2^2\\right) = 0\n$$\nSubstituting the gradient expressions:\n$$\n(A^\\top A x - A^\\top b) + \\rho (C^\\top C x - C^\\top d) = 0\n$$\nAgain, we group the terms involving $x$ to form a linear system:\n$$\n(A^\\top A + \\rho C^\\top C) x = A^\\top b + \\rho C^\\top d\n$$\nThis is a linear system of the form $H x = g'$, where:\n- $H = A^\\top A + \\rho C^\\top C$\n- $g' = A^\\top b + \\rho C^\\top d$\n\nThis corresponds to a single step of the augmented Lagrangian method with $\\lambda$ fixed at $0$. For the test cases, we solve this system once for a given $\\rho$. For the case with a schedule of $\\rho$ values, we solve the system for the final value in the schedule, as specified by the problem. The final constraint violation is then $\\|h(x)\\|_2 = \\|C x - d\\|_2$.\n\nThe implementation will follow these derived equations, using `numpy.linalg.solve` for the exact linear algebra solves of the subproblems.", "answer": "```python\nimport numpy as np\n\ndef run_augmented_lagrangian(A, b, C, d, rho, K, lambda0):\n    \"\"\"\n    Implements the Augmented Lagrangian method for a quadratic equality-constrained LS problem.\n\n    Args:\n        A (np.ndarray): Matrix for the LS objective.\n        b (np.ndarray): Vector for the LS objective.\n        C (np.ndarray): Matrix for the equality constraint.\n        d (np.ndarray): Vector for the equality constraint.\n        rho (float): Penalty parameter.\n        K (int): Number of iterations.\n        lambda0 (np.ndarray): Initial Lagrange multipliers.\n\n    Returns:\n        float: The final constraint violation norm ||Cx - d||_2.\n    \"\"\"\n    AtA = A.T @ A\n    Atb = A.T @ b\n    CtC = C.T @ C\n    Ct = C.T\n\n    # The Hessian of the Lagrangian is constant for all iterations\n    H = AtA + rho * CtC\n\n    lambda_k = lambda0.copy()\n    x_k = np.zeros(A.shape[1]) # Initialize x\n\n    for _ in range(K):\n        # Form the right-hand side vector g\n        g = Atb + Ct @ (rho * d - lambda_k)\n        \n        # Solve the linear system for x_{k+1}\n        x_k = np.linalg.solve(H, g)\n        \n        # Update the Lagrange multiplier\n        constraint_residual = C @ x_k - d\n        lambda_k = lambda_k + rho * constraint_residual\n    \n    final_constraint_violation = np.linalg.norm(C @ x_k - d)\n    return final_constraint_violation\n\ndef run_penalty_method(A, b, C, d, rho_values):\n    \"\"\"\n    Implements the pure quadratic penalty method.\n\n    Args:\n        A (np.ndarray): Matrix for the LS objective.\n        b (np.ndarray): Vector for the LS objective.\n        C (np.ndarray): Matrix for the equality constraint.\n        d (np.ndarray): Vector for the equality constraint.\n        rho_values (list or tuple): A single rho or a schedule of rho values.\n\n    Returns:\n        float: The final constraint violation norm ||Cx - d||_2.\n    \"\"\"\n    # Use the final rho from the schedule as per problem description\n    rho = rho_values[-1]\n\n    AtA = A.T @ A\n    Atb = A.T @ b\n    CtC = C.T @ C\n    Ctd = C.T @ d\n    \n    H = AtA + rho * CtC\n    g_prime = Atb + rho * Ctd\n    \n    x = np.linalg.solve(H, g_prime)\n    \n    constraint_violation = np.linalg.norm(C @ x - d)\n    return constraint_violation\n\ndef solve():\n    \"\"\"\n    Defines test cases and runs the optimization algorithms.\n    \"\"\"\n    test_cases = [\n        {\n            # Case 1 (happy path, well-conditioned data)\n            \"A\": np.array([[2., -1.], [0., 3.], [1., 1.]]),\n            \"b\": np.array([1., 4., 2.]),\n            \"C\": np.array([[1., 2.]]),\n            \"d\": np.array([3.]),\n            \"alm_params\": {\"rho\": 1.0, \"K\": 10, \"lambda0\": np.array([0.])},\n            \"penalty_params\": {\"rho_values\": [1.0]}\n        },\n        {\n            # Case 2 (boundary condition: weak penalty, mildly ill-conditioned A^T A)\n            \"A\": np.array([[1e-3, 0.], [0., 1.], [1., -1.]]),\n            \"b\": np.array([0., 1., 0.5]),\n            \"C\": np.array([[1., 0.]]),\n            \"d\": np.array([0.2]),\n            \"alm_params\": {\"rho\": 1e-2, \"K\": 50, \"lambda0\": np.array([0.])},\n            \"penalty_params\": {\"rho_values\": [1e-2]}\n        },\n        {\n            # Case 3 (edge case: redundant equality constraints)\n            \"A\": np.array([[1., 2.], [3., 4.], [5., 6.]]),\n            \"b\": np.array([7., 8., 9.]),\n            \"C\": np.array([[1., 1.], [2., 2.]]),\n            \"d\": np.array([0., 0.]),\n            \"alm_params\": {\"rho\": 1.0, \"K\": 15, \"lambda0\": np.array([0., 0.])},\n            \"penalty_params\": {\"rho_values\": [1e-1, 1.0, 10.0]}\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        A, b, C, d = case[\"A\"], case[\"b\"], case[\"C\"], case[\"d\"]\n        \n        # Run Augmented Lagrangian Method\n        alm_params = case[\"alm_params\"]\n        alm_violation = run_augmented_lagrangian(A, b, C, d, **alm_params)\n        \n        # Run Pure Penalty Method\n        penalty_params = case[\"penalty_params\"]\n        penalty_violation = run_penalty_method(A, b, C, d, **penalty_params)\n        \n        results.append(f\"[{alm_violation},{penalty_violation}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3201293"}, {"introduction": "Real-world optimization problems often feature complex, nonconvex feasible regions, which pose significant challenges. In this final practice [@problem_id:3201335], we explore two fundamentally different philosophies for navigating such domains: the penalty method, which, as you've seen, approximates constraints, and the projected gradient method, which strictly enforces them at every step. By implementing both on nonconvex sets, you will gain firsthand insight into potential failure modes and understand why guaranteeing feasibility can be critical for finding a valid solution.", "problem": "You are to implement and compare two iterative methods for solving a constrained optimization problem in two dimensions where the feasible set is nonconvex but has smooth boundaries. The goal is to identify when an unconstrained surrogate can have infeasible local minimizers and to verify that orthogonal projection after each gradient step enforces feasibility. The base setting is a smooth objective, smooth inequality constraints, and standard Euclidean geometry.\n\nFundamental definitions to use as a base:\n- A constrained optimization problem is defined as minimizing a smooth objective $f(\\mathbf{x})$ over a feasible set $\\mathcal{F}$ defined by smooth inequality constraints $c_i(\\mathbf{x}) \\le 0$ for $i$ in a finite index set.\n- A point $\\mathbf{x}$ is feasible if and only if all inequalities satisfy $c_i(\\mathbf{x}) \\le 0$.\n- The orthogonal projection of a point $\\mathbf{y}$ onto a closed set $\\mathcal{F}$ is a point $\\Pi_{\\mathcal{F}}(\\mathbf{y})$ in $\\mathcal{F}$ that minimizes the Euclidean distance $\\|\\mathbf{x}-\\mathbf{y}\\|$ among all $\\mathbf{x}\\in\\mathcal{F}$.\n\nYour program must implement and compare:\n- A method that alternates a gradient step on $f(\\mathbf{x})$ and an orthogonal projection onto $\\mathcal{F}$ after each step. You must justify the form of the projection for the specific feasible sets given below from first principles.\n- A method that replaces the constrained problem by an unconstrained surrogate with a smooth quadratic growth in the violation, and applies gradient descent to that surrogate. You must justify the surrogate construction from first principles.\n\nUse the following two feasible sets, both nonconvex with smooth boundaries:\n- Case type A (annulus): $\\mathcal{F} = \\{\\mathbf{x} \\in \\mathbb{R}^2 \\mid r_{\\mathrm{in}}^2 \\le \\|\\mathbf{x}\\|^2 \\le R_{\\mathrm{out}}^2\\}$, with two smooth inequality constraints $c_1(\\mathbf{x}) = r_{\\mathrm{in}}^2 - \\|\\mathbf{x}\\|^2 \\le 0$ and $c_2(\\mathbf{x}) = \\|\\mathbf{x}\\|^2 - R_{\\mathrm{out}}^2 \\le 0$. This set is nonconvex because it contains a hole.\n- Case type B (union of two disks): $\\mathcal{F} = \\{\\mathbf{x} \\in \\mathbb{R}^2 \\mid (\\|\\mathbf{x}-\\mathbf{c}_1\\|^2 - r^2)(\\|\\mathbf{x}-\\mathbf{c}_2\\|^2 - r^2) \\le 0\\}$, the union of two disjoint closed disks of radius $r$ centered at $\\mathbf{c}_1$ and $\\mathbf{c}_2$. The boundary is smooth (disjoint circles), and the set is nonconvex (disconnected components).\n\nThe objective is the smooth function $f(\\mathbf{x}) = \\|\\mathbf{x}\\|^2$.\n\nImplement the following two algorithms:\n- Projected Gradient Descent: iterate $\\mathbf{y}_{k} = \\mathbf{x}_{k} - \\alpha \\nabla f(\\mathbf{x}_{k})$, then $\\mathbf{x}_{k+1} = \\Pi_{\\mathcal{F}}(\\mathbf{y}_{k})$, where $\\alpha > 0$ is a fixed step size. For projection onto the annulus, justify radial clipping from the orthogonal projection definition. For projection onto the union of two disks, justify that the orthogonal projection onto a union equals the nearest projection onto either disk if $\\mathbf{y}$ is infeasible, and equals $\\mathbf{y}$ if it is already feasible.\n- Quadratic Penalty Gradient Descent: build an unconstrained surrogate $F_{\\mu}(\\mathbf{x}) = f(\\mathbf{x}) + \\mu \\sum_i \\phi(c_i(\\mathbf{x}))$ with $\\phi(t)$ a smooth function with quadratic growth for $t > 0$ and zero for $t \\le 0$, and $\\mu > 0$ a penalty parameter. For the annulus, use two inequality penalties. For the union of disks, use one inequality penalty on the product constraint. Apply gradient descent $\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\beta \\nabla F_{\\mu}(\\mathbf{x}_k)$ with fixed step size $\\beta > 0$.\n\nTest suite:\n- Test $1$ (annulus, infeasible unconstrained minimizer): $r_{\\mathrm{in}} = 1$, $R_{\\mathrm{out}} = 2$, initial point $\\mathbf{x}_0 = (0.3, 0.4)$, projected step size $\\alpha = 0.2$, penalty step size $\\beta = 0.05$, penalty parameter $\\mu = 0.5$, iterations $N = 300$.\n- Test $2$ (union of disks, infeasible unconstrained minimizer): disk radius $r = 1$, centers $\\mathbf{c}_1 = (1.5, 0)$ and $\\mathbf{c}_2 = (-1.5, 0)$, initial point $\\mathbf{x}_0 = (0, 0)$, $\\alpha = 0.2$, $\\beta = 0.05$, $\\mu = 0.1$, $N = 300$.\n- Test $3$ (annulus, boundary start, strong penalty): $r_{\\mathrm{in}} = 1$, $R_{\\mathrm{out}} = 2$, initial point $\\mathbf{x}_0 = (1.0, 0.0)$, $\\alpha = 0.2$, $\\beta = 0.05$, $\\mu = 2.0$, $N = 300$.\n\nFor each test case, run both methods and report four quantities in the following order:\n- The final value of $f(\\mathbf{x})$ after Projected Gradient Descent (a float).\n- A feasibility indicator for the Projected Gradient Descent final point (a boolean using the inequalities as specified).\n- The final value of $f(\\mathbf{x})$ after Quadratic Penalty Gradient Descent (a float).\n- A feasibility indicator for the Quadratic Penalty final point (a boolean).\n\nYour program should produce a single line of output containing all results from the test suite, flattened into a single list as a comma-separated list enclosed in square brackets (e.g., \"[$r_1, b_1, r_2, b_2, r_3, b_3, r_4, b_4, r_5, b_5, r_6, b_6$]\"), where $r_i$ are floats and $b_i$ are booleans. No physical units are involved. Angles, if any, must be in radians; however, this problem does not require any trigonometric angle inputs or outputs.\n\nThe design intentionally includes cases where the unconstrained surrogate can have infeasible local minima for small $\\mu$, and a case with a larger $\\mu$ to reduce infeasibility. Your implementation must be fully deterministic and require no user input. Use only the specified runtime environment.", "solution": "The problem requires the implementation and comparison of two distinct iterative methods for solving a constrained optimization problem: Projected Gradient Descent (PGD) and the Quadratic Penalty method. The objective is to minimize the smooth function $f(\\mathbf{x}) = \\|\\mathbf{x}\\|^2$ for $\\mathbf{x} \\in \\mathbb{R}^2$ over two different nonconvex feasible sets, $\\mathcal{F}$.\n\nFirst, we establish the gradient of the objective function. Given $f(\\mathbf{x}) = \\|\\mathbf{x}\\|^2 = x_1^2 + x_2^2$, its gradient is:\n$$\n\\nabla f(\\mathbf{x}) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 2x_1 \\\\ 2x_2 \\end{pmatrix} = 2\\mathbf{x}\n$$\n\n### Method 1: Projected Gradient Descent (PGD)\n\nThe PGD method is an iterative algorithm for constrained optimization. Each iteration consists of two steps: a standard gradient descent step on the objective function, followed by an orthogonal projection back onto the feasible set $\\mathcal{F}$. The iteration is defined as:\n$$\n\\mathbf{y}_{k} = \\mathbf{x}_{k} - \\alpha \\nabla f(\\mathbf{x}_{k})\n$$\n$$\n\\mathbf{x}_{k+1} = \\Pi_{\\mathcal{F}}(\\mathbf{y}_{k})\n$$\nwhere $\\alpha > 0$ is the step size and $\\Pi_{\\mathcal{F}}(\\mathbf{y})$ is the orthogonal projection of a point $\\mathbf{y}$ onto the set $\\mathcal{F}$. The projection is the point in $\\mathcal{F}$ closest to $\\mathbf{y}$ in the Euclidean norm sense:\n$$\n\\Pi_{\\mathcal{F}}(\\mathbf{y}) = \\arg\\min_{\\mathbf{z} \\in \\mathcal{F}} \\|\\mathbf{z} - \\mathbf{y}\\|\n$$\nSubstituting $\\nabla f(\\mathbf{x}_k) = 2\\mathbf{x}_k$, the gradient step becomes $\\mathbf{y}_k = \\mathbf{x}_k - \\alpha(2\\mathbf{x}_k) = (1 - 2\\alpha)\\mathbf{x}_k$.\n\nWe must now justify the projection operators for the given feasible sets.\n\n**Case A: Annulus**\nThe feasible set is $\\mathcal{F}_A = \\{\\mathbf{x} \\in \\mathbb{R}^2 \\mid r_{\\mathrm{in}} \\le \\|\\mathbf{x}\\| \\le R_{\\mathrm{out}}\\}$. To find the projection $\\Pi_{\\mathcal{F}_A}(\\mathbf{y})$, we minimize $\\|\\mathbf{z} - \\mathbf{y}\\|^2$ for $\\mathbf{z} \\in \\mathcal{F}_A$. Due to the rotational symmetry of both the objective $\\|\\mathbf{z} - \\mathbf{y}\\|^2$ and the set $\\mathcal{F}_A$ about the origin, the optimal $\\mathbf{z}$ must lie on the ray from the origin through $\\mathbf{y}$. Thus, $\\mathbf{z}$ must be of the form $\\mathbf{z} = \\rho \\frac{\\mathbf{y}}{\\|\\mathbf{y}\\|}$ for some scalar magnitude $\\rho$, assuming $\\mathbf{y} \\ne \\mathbf{0}$. The constraint on $\\mathbf{z}$ becomes $r_{\\mathrm{in}} \\le \\rho \\le R_{\\mathrm{out}}$.\nThe minimization problem reduces to finding $\\rho$ in the interval $[r_{\\mathrm{in}}, R_{\\mathrm{out}}]$ that minimizes $\\|\\rho \\frac{\\mathbf{y}}{\\|\\mathbf{y}\\|} - \\mathbf{y}\\|^2 = (\\rho - \\|\\mathbf{y}\\|)^2$. This is a one-dimensional problem of finding the closest point in the interval $[r_{\\mathrm{in}}, R_{\\mathrm{out}}]$ to the value $\\|\\mathbf{y}\\|$. The solution is to clamp $\\|\\mathbf{y}\\|$ to this interval:\n- If $\\|\\mathbf{y}\\| < r_{\\mathrm{in}}$, then $\\rho = r_{\\mathrm{in}}$.\n- If $r_{\\mathrm{in}} \\le \\|\\mathbf{y}\\| \\le R_{\\mathrm{out}}$, then $\\rho = \\|\\mathbf{y}\\|$.\n- If $\\|\\mathbf{y}\\| > R_{\\mathrm{out}}$, then $\\rho = R_{\\mathrm{out}}$.\nThis can be written as $\\rho = \\text{clip}(\\|\\mathbf{y}\\|, r_{\\mathrm{in}}, R_{\\mathrm{out}})$. The projection is therefore:\n$$\n\\Pi_{\\mathcal{F}_A}(\\mathbf{y}) = \\text{clip}(\\|\\mathbf{y}\\|, r_{\\mathrm{in}}, R_{\\mathrm{out}}) \\frac{\\mathbf{y}}{\\|\\mathbf{y}\\|}\n$$\nIf $\\mathbf{y} = \\mathbf{0}$, its norm is $0 < r_{\\mathrm{in}}$. The closest points in $\\mathcal{F}_A$ are all points on the inner circle of radius $r_{\\mathrm{in}}$. Any such point is a valid projection. For determinism, we can choose a specific point, e.g., $(r_{\\mathrm{in}}, 0)$.\n\n**Case B: Union of Two Disks**\nThe feasible set is $\\mathcal{F}_B = D_1 \\cup D_2$, where $D_1 = \\{\\mathbf{x} \\mid \\|\\mathbf{x}-\\mathbf{c}_1\\| \\le r\\}$ and $D_2 = \\{\\mathbf{x} \\mid \\|\\mathbf{x}-\\mathbf{c}_2\\| \\le r\\}$ are closed, disjoint disks. The projection $\\Pi_{\\mathcal{F}_B}(\\mathbf{y})$ is the point $\\mathbf{z} \\in D_1 \\cup D_2$ that minimizes $\\|\\mathbf{z} - \\mathbf{y}\\|$. This minimum must be achieved either in $D_1$ or in $D_2$. Let $\\mathbf{p}_1 = \\Pi_{D_1}(\\mathbf{y})$ and $\\mathbf{p}_2 = \\Pi_{D_2}(\\mathbf{y})$ be the projections of $\\mathbf{y}$ onto the individual disks. The projection onto the union is simply the one that is closer to $\\mathbf{y}$:\n$$\n\\Pi_{\\mathcal{F}_B}(\\mathbf{y}) = \n\\begin{cases} \n\\mathbf{p}_1 & \\text{if } \\|\\mathbf{y} - \\mathbf{p}_1\\| \\le \\|\\mathbf{y} - \\mathbf{p}_2\\| \\\\\n\\mathbf{p}_2 & \\text{otherwise}\n\\end{cases}\n$$\nThe projection onto a single disk $D = \\{\\mathbf{x} \\mid \\|\\mathbf{x}-\\mathbf{c}\\| \\le r\\}$ is:\n- If $\\mathbf{y} \\in D$, then $\\Pi_D(\\mathbf{y}) = \\mathbf{y}$.\n- If $\\mathbf{y} \\notin D$, the projection lies on the boundary, on the line segment connecting the center $\\mathbf{c}$ to $\\mathbf{y}$. Thus, $\\Pi_D(\\mathbf{y}) = \\mathbf{c} + r \\frac{\\mathbf{y}-\\mathbf{c}}{\\|\\mathbf{y}-\\mathbf{c}\\|}$.\n\n### Method 2: Quadratic Penalty Gradient Descent\n\nThis method transforms the constrained problem into a sequence of unconstrained problems by adding a penalty term to the objective function, which penalizes constraint violations. The unconstrained surrogate function is:\n$$\nF_{\\mu}(\\mathbf{x}) = f(\\mathbf{x}) + \\mu \\sum_i \\phi(c_i(\\mathbf{x}))\n$$\nwhere $\\mu > 0$ is the penalty parameter and $\\phi(t)$ is a penalty function. We use $\\phi(t) = (\\max(0, t))^2$, which is smooth ($C^1$) with derivative $\\phi'(t) = 2\\max(0, t)$. We then apply gradient descent to $F_{\\mu}(\\mathbf{x})$:\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\beta \\nabla F_{\\mu}(\\mathbf{x}_k)\n$$\nwith step size $\\beta > 0$. The gradient of the surrogate, using the chain rule, is:\n$$\n\\nabla F_{\\mu}(\\mathbf{x}) = \\nabla f(\\mathbf{x}) + \\mu \\sum_i \\phi'(c_i(\\mathbf{x})) \\nabla c_i(\\mathbf{x}) = 2\\mathbf{x} + \\mu \\sum_i 2\\max(0, c_i(\\mathbf{x})) \\nabla c_i(\\mathbf{x})\n$$\n\n**Case A: Annulus**\nThe constraints are $c_1(\\mathbf{x}) = r_{\\mathrm{in}}^2 - \\|\\mathbf{x}\\|^2 \\le 0$ and $c_2(\\mathbf{x}) = \\|\\mathbf{x}\\|^2 - R_{\\mathrm{out}}^2 \\le 0$. Their gradients are $\\nabla c_1(\\mathbf{x}) = -2\\mathbf{x}$ and $\\nabla c_2(\\mathbf{x}) = 2\\mathbf{x}$. The gradient of the surrogate function $F_\\mu(\\mathbf{x})$ is:\n$$\n\\nabla F_{\\mu}(\\mathbf{x}) = 2\\mathbf{x} + 2\\mu \\left( \\max(0, r_{\\mathrm{in}}^2 - \\|\\mathbf{x}\\|^2)(-2\\mathbf{x}) + \\max(0, \\|\\mathbf{x}\\|^2 - R_{\\mathrm{out}}^2)(2\\mathbf{x}) \\right)\n$$\nFor small values of the penalty parameter $\\mu$, the surrogate $F_{\\mu}(\\mathbf{x})$ can have an infeasible local minimum at $\\mathbf{x}=\\mathbf{0}$, which can trap the gradient descent algorithm.\n\n**Case B: Union of Two Disks**\nThe feasible set is described by a single inequality: $c(\\mathbf{x}) = (\\|\\mathbf{x}-\\mathbf{c}_1\\|^2 - r^2)(\\|\\mathbf{x}-\\mathbf{c}_2\\|^2 - r^2) \\le 0$. Let $g_1(\\mathbf{x}) = \\|\\mathbf{x}-\\mathbf{c}_1\\|^2 - r^2$ and $g_2(\\mathbf{x}) = \\|\\mathbf{x}-\\mathbf{c}_2\\|^2 - r^2$. Then $c(\\mathbf{x}) = g_1(\\mathbf{x})g_2(\\mathbf{x})$. The gradient of $c(\\mathbf{x})$ is, by the product rule:\n$$\n\\nabla c(\\mathbf{x}) = g_2(\\mathbf{x})\\nabla g_1(\\mathbf{x}) + g_1(\\mathbf{x})\\nabla g_2(\\mathbf{x}) = g_2(\\mathbf{x})(2(\\mathbf{x}-\\mathbf{c}_1)) + g_1(\\mathbf{x})(2(\\mathbf{x}-\\mathbf{c}_2))\n$$\nThe gradient of the surrogate function $F_\\mu(\\mathbf{x})$ is:\n$$\n\\nabla F_{\\mu}(\\mathbf{x}) = 2\\mathbf{x} + 2\\mu \\max(0, c(\\mathbf{x})) \\nabla c(\\mathbf{x})\n$$\nSimilar to the annulus case, this surrogate can have an infeasible local minimum at $\\mathbf{x}=\\mathbf{0}$ (which is equidistant from $\\mathbf{c}_1$ and $-\\mathbf{c}_2$), potentially trapping the algorithm.\n\n### Feasibility Check\n\nAfter $N$ iterations, for a final point $\\mathbf{x}_{\\text{final}}$, we must check if it lies in the feasible set $\\mathcal{F}$.\n- **For the Annulus:** Check if both $r_{\\mathrm{in}}^2 - \\|\\mathbf{x}_{\\text{final}}\\|^2 \\le 0$ and $\\|\\mathbf{x}_{\\text{final}}\\|^2 - R_{\\mathrm{out}}^2 \\le 0$ are satisfied.\n- **For the Union of Disks:** Check if $(\\|\\mathbf{x}_{\\text{final}}-\\mathbf{c}_1\\|^2 - r^2)(\\|\\mathbf{x}_{\\text{final}}-\\mathbf{c}_2\\|^2 - r^2) \\le 0$ is satisfied.\n\nThe implementation will execute these two algorithms for the specified test cases and report the final objective value and feasibility status for each.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares Projected Gradient Descent and Quadratic Penalty methods\n    for constrained optimization on nonconvex sets.\n    \"\"\"\n\n    # --- Helper Functions for Algorithms ---\n\n    def f_obj(x):\n        \"\"\"Objective function f(x) = ||x||^2.\"\"\"\n        return np.dot(x, x)\n\n    def grad_f_obj(x):\n        \"\"\"Gradient of the objective function.\"\"\"\n        return 2 * x\n\n    # --- Projected Gradient Descent (PGD) Implementation ---\n\n    def project_annulus(y, r_in, R_out):\n        \"\"\"Projects a point y onto the annulus.\"\"\"\n        norm_y = np.linalg.norm(y)\n        if norm_y == 0.0:\n            # Project origin to the closest point in the annulus.\n            # Arbitrarily choose (r_in, 0) for determinism.\n            return np.array([r_in, 0.0])\n        \n        clamped_norm = np.clip(norm_y, r_in, R_out)\n        return y * (clamped_norm / norm_y)\n\n    def project_disk(y, c, r):\n        \"\"\"Projects a point y onto a single disk.\"\"\"\n        vec_to_center = y - c\n        dist_from_center = np.linalg.norm(vec_to_center)\n        if dist_from_center <= r:\n            return y\n        else:\n            return c + r * vec_to_center / dist_from_center\n\n    def project_union_disks(y, c1, c2, r):\n        \"\"\"Projects a point y onto the union of two disks.\"\"\"\n        p1 = project_disk(y, c1, r)\n        p2 = project_disk(y, c2, r)\n        \n        dist_sq_1 = np.sum((y - p1)**2)\n        dist_sq_2 = np.sum((y - p2)**2)\n        \n        if dist_sq_1 <= dist_sq_2:\n            return p1\n        else:\n            return p2\n\n    def run_pgd(x0, alpha, N, proj_func, proj_params):\n        \"\"\"Runs the Projected Gradient Descent algorithm.\"\"\"\n        x = np.array(x0, dtype=float)\n        for _ in range(N):\n            y = x - alpha * grad_f_obj(x)\n            x = proj_func(y, **proj_params)\n        return x\n\n    # --- Quadratic Penalty Method Implementation ---\n\n    def grad_penalty_annulus(x, mu, r_in, R_out):\n        \"\"\"Computes the gradient of the penalty surrogate for the annulus.\"\"\"\n        norm_x_sq = np.dot(x, x)\n        c1 = r_in**2 - norm_x_sq\n        c2 = norm_x_sq - R_out**2\n        \n        grad_c1 = -2 * x\n        grad_c2 = 2 * x\n        \n        # phi'(t) = 2 * max(0, t)\n        penalty_grad = mu * (2 * np.maximum(0, c1) * grad_c1 + 2 * np.maximum(0, c2) * grad_c2)\n        return grad_f_obj(x) + penalty_grad\n\n    def grad_penalty_disks(x, mu, c1, c2, r):\n        \"\"\"Computes the gradient of the penalty surrogate for the union of disks.\"\"\"\n        g1 = np.sum((x - c1)**2) - r**2\n        g2 = np.sum((x - c2)**2) - r**2\n        c_prod = g1 * g2\n\n        if c_prod > 0:\n            grad_g1 = 2 * (x - c1)\n            grad_g2 = 2 * (x - c2)\n            grad_c_prod = g2 * grad_g1 + g1 * grad_g2\n            \n            # phi'(t) = 2 * max(0, t)\n            penalty_grad = mu * (2 * c_prod * grad_c_prod)\n            return grad_f_obj(x) + penalty_grad\n        else:\n            return grad_f_obj(x)\n\n    def run_penalty_method(x0, beta, N, grad_penalty_func, grad_params):\n        \"\"\"Runs the Gradient Descent on the quadratic penalty surrogate.\"\"\"\n        x = np.array(x0, dtype=float)\n        for _ in range(N):\n            grad = grad_penalty_func(x, **grad_params)\n            x = x - beta * grad\n        return x\n\n    # --- Feasibility Checkers ---\n    \n    def is_feasible_annulus(x, r_in, R_out):\n        \"\"\"Checks if a point is in the annulus.\"\"\"\n        norm_x_sq = np.dot(x, x)\n        return r_in**2 <= norm_x_sq <= R_out**2\n\n    def is_feasible_disks(x, c1, c2, r):\n        \"\"\"Checks if a point is in the union of two disks.\"\"\"\n        g1 = np.sum((x - c1)**2) - r**2\n        g2 = np.sum((x - c2)**2) - r**2\n        return (g1 * g2) <= 0\n\n    # --- Test Suite ---\n    \n    test_cases = [\n        # Test 1: Annulus, infeasible unconstrained minimizer\n        {\n            \"type\": \"annulus\", \"r_in\": 1.0, \"R_out\": 2.0, \"x0\": (0.3, 0.4),\n            \"alpha\": 0.2, \"beta\": 0.05, \"mu\": 0.5, \"N\": 300\n        },\n        # Test 2: Union of disks, infeasible unconstrained minimizer\n        {\n            \"type\": \"disks\", \"r\": 1.0, \"c1\": (1.5, 0.0), \"c2\": (-1.5, 0.0),\n            \"x0\": (0.0, 0.0), \"alpha\": 0.2, \"beta\": 0.05, \"mu\": 0.1, \"N\": 300\n        },\n        # Test 3: Annulus, boundary start, strong penalty\n        {\n            \"type\": \"annulus\", \"r_in\": 1.0, \"R_out\": 2.0, \"x0\": (1.0, 0.0),\n            \"alpha\": 0.2, \"beta\": 0.05, \"mu\": 2.0, \"N\": 300\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        x0 = case[\"x0\"]\n        alpha = case[\"alpha\"]\n        beta = case[\"beta\"]\n        mu = case[\"mu\"]\n        N = case[\"N\"]\n\n        if case[\"type\"] == \"annulus\":\n            r_in, R_out = case[\"r_in\"], case[\"R_out\"]\n            \n            # PGD\n            proj_params = {\"r_in\": r_in, \"R_out\": R_out}\n            x_pgd = run_pgd(x0, alpha, N, project_annulus, proj_params)\n            f_pgd = f_obj(x_pgd)\n            feasible_pgd = is_feasible_annulus(x_pgd, r_in, R_out)\n            \n            # Penalty\n            grad_params = {\"mu\": mu, \"r_in\": r_in, \"R_out\": R_out}\n            x_penalty = run_penalty_method(x0, beta, N, grad_penalty_annulus, grad_params)\n            f_penalty = f_obj(x_penalty)\n            feasible_penalty = is_feasible_annulus(x_penalty, r_in, R_out)\n\n        elif case[\"type\"] == \"disks\":\n            r, c1, c2 = case[\"r\"], np.array(case[\"c1\"]), np.array(case[\"c2\"])\n\n            # PGD\n            proj_params = {\"c1\": c1, \"c2\": c2, \"r\": r}\n            x_pgd = run_pgd(x0, alpha, N, project_union_disks, proj_params)\n            f_pgd = f_obj(x_pgd)\n            feasible_pgd = is_feasible_disks(x_pgd, c1, c2, r)\n\n            # Penalty\n            grad_params = {\"mu\": mu, \"c1\": c1, \"c2\": c2, \"r\": r}\n            x_penalty = run_penalty_method(x0, beta, N, grad_penalty_disks, grad_params)\n            f_penalty = f_obj(x_penalty)\n            feasible_penalty = is_feasible_disks(x_penalty, c1, c2, r)\n\n        results.extend([f_pgd, feasible_pgd, f_penalty, feasible_penalty])\n\n    # Final print statement in the exact required format.\n    # Convert booleans to lowercase 'true'/'false' as str() does by default.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3201335"}]}