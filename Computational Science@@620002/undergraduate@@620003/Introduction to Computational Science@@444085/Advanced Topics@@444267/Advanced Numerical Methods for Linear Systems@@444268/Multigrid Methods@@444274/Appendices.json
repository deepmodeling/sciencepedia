{"hands_on_practices": [{"introduction": "The effectiveness of multigrid methods hinges on a key principle known as the \"smoothing property.\" Simple iterative methods, such as the Jacobi or Gauss-Seidel schemes, are remarkably efficient at damping high-frequency (highly oscillatory) components of the error in a solution. This hands-on exercise [@problem_id:2188704] allows you to directly observe this phenomenon by applying a basic relaxation scheme to a \"sawtooth\" error vector, which represents the highest possible frequency on the grid.", "problem": "In the analysis of iterative methods for solving discretized partial differential equations, understanding the behavior of the error is crucial. Consider a one-dimensional problem discretized on a grid. Let the interior grid points be indexed from $i=1$ to $i=7$. At a particular stage of an iterative process, the error vector corresponding to these interior points is given by $e^{(0)} = [1, -1, 1, -1, 1, -1, 1]$.\n\nThe physical problem is subject to boundary conditions that fix the values at the edges, ensuring the error at the boundary points is always zero. For our grid, this means $e_0 = 0$ and $e_8 = 0$.\n\nA single step of a simple relaxation scheme is applied to update the error vector. The update rule for the error at each interior grid point $i$ is defined by averaging the error values of its immediate neighbors from the previous step. The formula for the new error vector $e^{(1)}$ is:\n$$e_i^{(1)} = \\frac{1}{2} \\left( e_{i-1}^{(0)} + e_{i+1}^{(0)} \\right)$$\nwhere $i$ ranges from 1 to 7.\n\nCalculate the new error vector $e^{(1)}$ after one application of this relaxation scheme. Present your final answer as a row vector of 7 elements, using fractional or integer values.", "solution": "We are given interior indices $i=1,\\dots,7$, boundary errors $e_{0}^{(0)}=0$ and $e_{8}^{(0)}=0$, and the Jacobi-type relaxation update\n$$\ne_{i}^{(1)}=\\frac{1}{2}\\left(e_{i-1}^{(0)}+e_{i+1}^{(0)}\\right), \\quad i=1,\\dots,7.\n$$\nUsing the initial error vector $e^{(0)}=[1,-1,1,-1,1,-1,1]$ and the boundary values, we compute each component:\n$$\ne_{1}^{(1)}=\\frac{1}{2}\\left(e_{0}^{(0)}+e_{2}^{(0)}\\right)=\\frac{1}{2}\\left(0+(-1)\\right)=-\\frac{1}{2},\n$$\n$$\ne_{2}^{(1)}=\\frac{1}{2}\\left(e_{1}^{(0)}+e_{3}^{(0)}\\right)=\\frac{1}{2}\\left(1+1\\right)=1,\n$$\n$$\ne_{3}^{(1)}=\\frac{1}{2}\\left(e_{2}^{(0)}+e_{4}^{(0)}\\right)=\\frac{1}{2}\\left(-1+(-1)\\right)=-1,\n$$\n$$\ne_{4}^{(1)}=\\frac{1}{2}\\left(e_{3}^{(0)}+e_{5}^{(0)}\\right)=\\frac{1}{2}\\left(1+1\\right)=1,\n$$\n$$\ne_{5}^{(1)}=\\frac{1}{2}\\left(e_{4}^{(0)}+e_{6}^{(0)}\\right)=\\frac{1}{2}\\left(-1+(-1)\\right)=-1,\n$$\n$$\ne_{6}^{(1)}=\\frac{1}{2}\\left(e_{5}^{(0)}+e_{7}^{(0)}\\right)=\\frac{1}{2}\\left(1+1\\right)=1,\n$$\n$$\ne_{7}^{(1)}=\\frac{1}{2}\\left(e_{6}^{(0)}+e_{8}^{(0)}\\right)=\\frac{1}{2}\\left(-1+0\\right)=-\\frac{1}{2}.\n$$\nTherefore, the updated error vector is\n$$\ne^{(1)}=\\begin{pmatrix}-\\frac{1}{2} & 1 & -1 & 1 & -1 & 1 & -\\frac{1}{2}\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}-\\frac{1}{2} & 1 & -1 & 1 & -1 & 1 & -\\frac{1}{2}\\end{pmatrix}}$$", "id": "2188704"}, {"introduction": "While smoothers handle high-frequency errors, they are notoriously slow at reducing low-frequency (smooth) error components. The core idea of multigrid is to tackle these smooth errors on a coarser grid, where they appear more oscillatory and are easier to solve. This process requires transferring information between the grids using restriction (fine-to-coarse) and prolongation (coarse-to-fine) operators. This practice [@problem_id:2188706] focuses on the prolongation step, where you will use linear interpolation to transfer a correction computed on the coarse grid back to the fine grid.", "problem": "In the context of a multigrid method for solving a one-dimensional boundary value problem on the domain $[0, 1]$, a fine grid $\\Omega_h$ and a coarse grid $\\Omega_{2h}$ are used. The fine grid $\\Omega_h$ is defined by nodes $x_i = i \\cdot h$ for $i = 0, 1, \\dots, 8$, with a grid spacing of $h = 1/8$. The coarse grid $\\Omega_{2h}$ is a subset of the fine grid, containing only the nodes $x_j = j \\cdot (2h)$ for $j = 0, 1, \\dots, 4$. Consequently, the interior points of the fine grid are $\\{x_1, x_2, x_3, x_4, x_5, x_6, x_7\\}$, and the interior points of the coarse grid correspond to $\\{x_2, x_4, x_6\\}$ on the fine grid's indexing.\n\nIn one step of the algorithm, an error correction equation is solved on the coarse grid, yielding a correction vector $e_{2h}$. This vector is defined at the interior nodes of $\\Omega_{2h}$ with the values $e_{2h}(x_2) = c_1$, $e_{2h}(x_4) = c_2$, and $e_{2h}(x_6) = c_3$, where $c_1, c_2, c_3$ are symbolic constants. The correction is known to be zero at the boundaries of the domain, so $e_{2h}(x_0) = 0$ and $e_{2h}(x_8) = 0$.\n\nThis coarse-grid correction must be transferred back to the fine grid using a prolongation (interpolation) operator, denoted $I_{2h}^h$, to produce a fine-grid correction vector $e_h = I_{2h}^h e_{2h}$. The interpolation is linear and defined as follows:\n1. For any fine-grid node $x_i$ that is also a coarse-grid node (i.e., $i$ is an even number), the value is directly injected: $e_h(x_i) = e_{2h}(x_i)$.\n2. For any fine-grid node $x_i$ that is not a coarse-grid node (i.e., $i$ is an odd number), the value is the arithmetic mean of the values at its two nearest neighboring coarse-grid nodes.\n\nDetermine the complete correction vector $e_h$ at all seven interior points of the fine grid, $\\{x_1, x_2, \\dots, x_7\\}$. Express your answer as a single row vector with 7 components, ordered from $x_1$ to $x_7$, in terms of the constants $c_1$, $c_2$, and $c_3$.", "solution": "We are given two nested uniform grids on $[0,1]$: the fine grid $\\Omega_{h}$ with nodes $x_{i}=i h$ for $i=0,\\dots,8$ and $h=1/8$, and the coarse grid $\\Omega_{2h}$ consisting of the subset of even-indexed fine nodes $x_{j}=j(2h)$ for $j=0,\\dots,4$, i.e., indices $i=0,2,4,6,8$. The interior fine nodes are $x_{1},\\dots,x_{7}$, and the interior coarse nodes correspond to $x_{2},x_{4},x_{6}$. The coarse-grid correction $e_{2h}$ is specified by\n$$\ne_{2h}(x_{2})=c_{1},\\quad e_{2h}(x_{4})=c_{2},\\quad e_{2h}(x_{6})=c_{3},\\quad e_{2h}(x_{0})=0,\\quad e_{2h}(x_{8})=0.\n$$\nWe construct the fine-grid correction $e_{h}=I_{2h}^{h} e_{2h}$ using linear prolongation. The interpolation rule is:\n- For even $i$ (fine node coincides with coarse node): injection,\n$$\ne_{h}(x_{i})=e_{2h}(x_{i}).\n$$\n- For odd $i$ (fine node midway between two neighboring coarse nodes): arithmetic mean,\n$$\ne_{h}(x_{i})=\\frac{e_{2h}(x_{i-1})+e_{2h}(x_{i+1})}{2}.\n$$\n\nWe now evaluate $e_{h}$ at the seven interior fine nodes:\n\n1) For $i=1$ (odd), neighbors are $x_{0}$ and $x_{2}$:\n$$\ne_{h}(x_{1})=\\frac{e_{2h}(x_{0})+e_{2h}(x_{2})}{2}=\\frac{0+c_{1}}{2}=\\frac{c_{1}}{2}.\n$$\n\n2) For $i=2$ (even), injection:\n$$\ne_{h}(x_{2})=e_{2h}(x_{2})=c_{1}.\n$$\n\n3) For $i=3$ (odd), neighbors are $x_{2}$ and $x_{4}$:\n$$\ne_{h}(x_{3})=\\frac{e_{2h}(x_{2})+e_{2h}(x_{4})}{2}=\\frac{c_{1}+c_{2}}{2}.\n$$\n\n4) For $i=4$ (even), injection:\n$$\ne_{h}(x_{4})=e_{2h}(x_{4})=c_{2}.\n$$\n\n5) For $i=5$ (odd), neighbors are $x_{4}$ and $x_{6}$:\n$$\ne_{h}(x_{5})=\\frac{e_{2h}(x_{4})+e_{2h}(x_{6})}{2}=\\frac{c_{2}+c_{3}}{2}.\n$$\n\n6) For $i=6$ (even), injection:\n$$\ne_{h}(x_{6})=e_{2h}(x_{6})=c_{3}.\n$$\n\n7) For $i=7$ (odd), neighbors are $x_{6}$ and $x_{8}$:\n$$\ne_{h}(x_{7})=\\frac{e_{2h}(x_{6})+e_{2h}(x_{8})}{2}=\\frac{c_{3}+0}{2}=\\frac{c_{3}}{2}.\n$$\n\nCollecting the components in order from $x_{1}$ to $x_{7}$ yields the row vector\n$$\n\\begin{pmatrix}\n\\frac{c_{1}}{2} & c_{1} & \\frac{c_{1}+c_{2}}{2} & c_{2} & \\frac{c_{2}+c_{3}}{2} & c_{3} & \\frac{c_{3}}{2}\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{c_{1}}{2} & c_{1} & \\frac{c_{1}+c_{2}}{2} & c_{2} & \\frac{c_{2}+c_{3}}{2} & c_{3} & \\frac{c_{3}}{2}\\end{pmatrix}}$$", "id": "2188706"}, {"introduction": "This final practice synthesizes the individual components of the multigrid method into a complete analysis. This advanced practice [@problem_id:3163227] guides you through a computational investigation to verify the complementary nature of the smoother and the coarse-grid correction. By implementing the core operators and analyzing their effects on different error frequencies, you will gain a deep, quantitative understanding of why multigrid methods are so powerful and efficient.", "problem": "You will investigate the smoothing property and coarse-grid correction of a two-grid method for the one-dimensional Poisson problem. Consider the boundary value problem $-u''(x) = f(x)$ on the interval $(0,1)$ with homogeneous Dirichlet boundary conditions $u(0)=u(1)=0$. Discretize the operator using the standard second-order centered finite difference on a uniform grid with $N$ interior unknowns and spacing $h = 1/(N+1)$. The resulting linear system is $A \\mathbf{u} = \\mathbf{f}$, where $A \\in \\mathbb{R}^{N \\times N}$ is the symmetric positive definite tridiagonal matrix with main diagonal entries $2/h^2$ and first off-diagonal entries $-1/h^2$. All trigonometric functions use radians.\n\nYou will analyze the behavior of error modes $\\mathbf{e}_k$ with components $(\\mathbf{e}_k)_i = \\sin(k \\pi i h)$ for $i=1,\\dots,N$ and integers $k$ with $1 \\le k \\le N$. The objectives are twofold:\n\n1. Smoothing property: Examine how a single or multiple iterations of the weighted Jacobi (WJ) smoother reduce the Euclidean norm of high-frequency versus low-frequency error components. The WJ iteration updates an error vector $\\mathbf{e}$ via an iteration matrix $S$ defined from $A$ and the diagonal matrix $D$ of $A$, and uses a weight parameter $\\omega \\in (0,1)$. You must derive, from the discrete eigenstructure of $A$ and the definition of the WJ method, the exact spectral reduction factor per iteration for the mode $\\mathbf{e}_k$ as a function of $k$, $h$, and $\\omega$, and then raise it to the power of the number of smoothing steps $\\nu \\in \\mathbb{N}$ to obtain a theoretical prediction. Your program must also compute the measured reduction factor by applying $\\nu$ iterations of WJ to $\\mathbf{e}_k$ and taking the ratio of Euclidean norms.\n\n2. Coarse-grid correction: Consider a two-grid correction with restriction $R$ given by full weighting and prolongation $P$ given by linear interpolation. Construct the Galerkin coarse operator $A_H = R A P$. Define the coarse-grid correction operator $C = I - P A_H^{-1} R A$. Using the definition of $C$, show from first principles that $C$ annihilates any error component in the range of $P$, and explain why this targets low-frequency error. Your program must compute the measured reduction factor for coarse-grid correction alone by applying $C$ once to the initial error $\\mathbf{e}_k$ and taking the ratio of Euclidean norms.\n\nUse the following fundamental base to guide your derivations and implementation:\n- The second-order centered finite difference approximation of the second derivative.\n- The definition of the weighted Jacobi method for linear systems and its error propagation matrix.\n- The fact that the discrete Laplacian with homogeneous Dirichlet boundary conditions has a complete set of discrete sine eigenvectors.\n- The Galerkin definition $A_H = R A P$ for the coarse operator, with $R$ as full weighting and $P$ as linear interpolation.\n\nYour task is to write a complete, runnable program that:\n- Constructs $A$, $R$, $P$, and $A_H$ for each test case.\n- Forms the error mode $\\mathbf{e}_k$ for specified values of $N$ and $k$.\n- Computes three quantities for each test case: \n  1) the measured smoothing reduction factor after $\\nu$ weighted Jacobi iterations with weight $\\omega$, \n  2) the theoretical smoothing reduction factor you derived for the same $\\nu$ and $\\omega$, and \n  3) the measured coarse-grid correction reduction factor for one application of $C$.\n- Outputs all results for the entire test suite on a single line.\n\nTest suite:\n- Case 1: $N=63$, $k=31$, $\\omega=2/3$, $\\nu=1$.\n- Case 2: $N=63$, $k=2$, $\\omega=2/3$, $\\nu=1$.\n- Case 3: $N=63$, $k=16$, $\\omega=2/3$, $\\nu=3$.\n- Case 4: $N=15$, $k=7$, $\\omega=0.8$, $\\nu=2$.\n- Case 5: $N=127$, $k=1$, $\\omega=0.6$, $\\nu=5$.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of lists, one inner list per test case in the same order as listed above. Each inner list must contain exactly three floating-point numbers rounded to six digits after the decimal point in the order: $[\\text{measured smoothing}, \\text{theoretical smoothing}, \\text{measured coarse correction}]$. For example, the output line should look like:\n[[0.123456,0.123457,0.987654],[...],...]\nNo additional text should be printed.", "solution": "The problem requires an analysis of the two primary components of a multigrid method—smoothing and coarse-grid correction—for the one-dimensional Poisson problem $-u''(x) = f(x)$ on the interval $(0,1)$ with homogeneous Dirichlet boundary conditions $u(0)=u(1)=0$.\n\nThe problem is discretized using a second-order centered finite difference scheme on a uniform grid with $N$ interior points and grid spacing $h = 1/(N+1)$. This results in a linear system of equations $A \\mathbf{u} = \\mathbf{f}$, where the matrix $A \\in \\mathbb{R}^{N \\times N}$ is given by:\n$$\nA = \\frac{1}{h^2}\n\\begin{pmatrix}\n2 & -1 & & & \\\\\n-1 & 2 & -1 & & \\\\\n& \\ddots & \\ddots & \\ddots & \\\\\n& & -1 & 2 & -1 \\\\\n& & & -1 & 2\n\\end{pmatrix}\n$$\nThe problem asks us to analyze the effect of smoothing and coarse-grid correction on specific error modes. These error modes are defined by the vectors $\\mathbf{e}_k$ with components $(\\mathbf{e}_k)_i = \\sin(k \\pi i h)$ for $i=1, \\dots, N$ and integer wave numbers $k=1, \\dots, N$.\n\nFirst, we establish the eigenstructure of the discrete operator $A$. The vectors $\\mathbf{e}_k$ are the eigenvectors of $A$. To demonstrate this, we apply $A$ to $\\mathbf{e}_k$ and examine the $i$-th component of the resulting vector:\n$$ (A \\mathbf{e}_k)_i = \\frac{1}{h^2} \\left[ -(\\mathbf{e}_k)_{i-1} + 2(\\mathbf{e}_k)_i - (\\mathbf{e}_k)_{i+1} \\right] $$\nSubstituting the definition of $(\\mathbf{e}_k)_i$:\n$$ (A \\mathbf{e}_k)_i = \\frac{1}{h^2} \\left[ -\\sin(k \\pi (i-1) h) + 2\\sin(k \\pi i h) - \\sin(k \\pi (i+1) h) \\right] $$\nUsing the trigonometric identity $\\sin(\\alpha - \\beta) + \\sin(\\alpha + \\beta) = 2\\sin(\\alpha)\\cos(\\beta)$, where $\\alpha = k \\pi i h$ and $\\beta = k \\pi h$, we can simplify the expression:\n$$ (A \\mathbf{e}_k)_i = \\frac{1}{h^2} \\left[ 2\\sin(k \\pi i h) - 2\\sin(k \\pi i h)\\cos(k \\pi h) \\right] $$\n$$ (A \\mathbf{e}_k)_i = \\frac{2}{h^2} (1 - \\cos(k \\pi h)) \\sin(k \\pi i h) $$\nApplying the half-angle identity $1 - \\cos(\\theta) = 2\\sin^2(\\theta/2)$:\n$$ (A \\mathbf{e}_k)_i = \\frac{2}{h^2} \\left( 2\\sin^2\\left(\\frac{k \\pi h}{2}\\right) \\right) \\sin(k \\pi i h) = \\left[ \\frac{4}{h^2} \\sin^2\\left(\\frac{k \\pi h}{2}\\right) \\right] (\\mathbf{e}_k)_i $$\nThis confirms that $\\mathbf{e}_k$ is an eigenvector of $A$ with the corresponding eigenvalue $\\lambda_k$:\n$$ \\lambda_k = \\frac{4}{h^2} \\sin^2\\left(\\frac{k \\pi h}{2}\\right) $$\n\n**1. Smoothing Property of Weighted Jacobi**\n\nThe weighted Jacobi (WJ) method is an iterative smoother for the system $A \\mathbf{u} = \\mathbf{f}$. The update rule for the solution vector $\\mathbf{u}$ is $\\mathbf{u}^{(m+1)} = \\mathbf{u}^{(m)} + \\omega D^{-1} (\\mathbf{f} - A \\mathbf{u}^{(m)})$, where $D$ is the diagonal of $A$ and $\\omega$ is a weight parameter. The error $\\mathbf{e}^{(m)} = \\mathbf{u} - \\mathbf{u}^{(m)}$ propagates according to:\n$$ \\mathbf{e}^{(m+1)} = (I - \\omega D^{-1} A) \\mathbf{e}^{(m)} = S \\mathbf{e}^{(m)} $$\nwhere $S = I - \\omega D^{-1} A$ is the iteration matrix. For our matrix $A$, the diagonal is $D = \\frac{2}{h^2}I$, so $D^{-1} = \\frac{h^2}{2}I$. The iteration matrix becomes:\n$$ S = I - \\omega \\frac{h^2}{2} A $$\nSince the eigenvectors of $S$ are the same as those of $A$, we can find the eigenvalues $\\mu_k$ of $S$ corresponding to the eigenvectors $\\mathbf{e}_k$:\n$$ S \\mathbf{e}_k = (I - \\omega \\frac{h^2}{2} A) \\mathbf{e}_k = (1 - \\omega \\frac{h^2}{2} \\lambda_k) \\mathbf{e}_k = \\mu_k \\mathbf{e}_k $$\nSubstituting the expression for $\\lambda_k$:\n$$ \\mu_k = 1 - \\omega \\frac{h^2}{2} \\left(\\frac{4}{h^2} \\sin^2\\left(\\frac{k \\pi h}{2}\\right)\\right) = 1 - 2\\omega \\sin^2\\left(\\frac{k \\pi h}{2}\\right) $$\nThe value $\\mu_k$ is the spectral reduction factor for the error mode $\\mathbf{e}_k$ in a single WJ iteration. After $\\nu$ iterations, the amplitude of the $\\mathbf{e}_k$ component of the error is reduced by a factor of $\\mu_k^\\nu$. The Euclidean norm of the vector $\\mathbf{e}_k$ is therefore reduced by $|\\mu_k|^\\nu$. The theoretical smoothing reduction factor is thus:\n$$ R_{\\text{smooth, theory}} = \\left| 1 - 2\\omega \\sin^2\\left(\\frac{k \\pi h}{2}\\right) \\right|^\\nu $$\nWJ is an effective smoother because for high-frequency modes (large $k$, where $k \\approx N/2, \\dots, N$), the term $\\sin^2(k \\pi h/2)$ is close to $1$, making $|\\mu_k|$ small for an appropriate choice of $\\omega$ (e.g., $\\omega=2/3$). For low-frequency modes (small $k$), $\\sin^2(k \\pi h/2)$ is close to $0$, making $|\\mu_k|$ close to $1$, so these modes are reduced very slowly.\n\n**2. Coarse-Grid Correction**\n\nThe coarse-grid correction step is designed to eliminate the low-frequency error components that are not efficiently handled by the smoother. It involves projecting the residual onto a coarser grid, solving the error equation on that grid, and interpolating the correction back to the fine grid. The coarse-grid correction operator is $C = I - P A_H^{-1} R A$, where $R$ is the restriction operator, $P$ is the prolongation (interpolation) operator, and $A_H = R A P$ is the Galerkin coarse-grid operator.\n\nThe problem requires us to show that $C$ annihilates any error component in the range of the prolongation operator $P$. Let $\\mathbf{e}$ be an error vector that lies in the range of $P$. This implies that there exists a coarse-grid vector $\\mathbf{e}_H$ such that $\\mathbf{e} = P \\mathbf{e}_H$. Applying the operator $C$ to $\\mathbf{e}$:\n$$ C \\mathbf{e} = C (P \\mathbf{e}_H) = (I - P A_H^{-1} R A) (P \\mathbf{e}_H) $$\n$$ C \\mathbf{e} = P \\mathbf{e}_H - P A_H^{-1} R A P \\mathbf{e}_H $$\nSubstituting the definition $A_H = R A P$:\n$$ C \\mathbf{e} = P \\mathbf{e}_H - P (R A P)^{-1} (R A P) \\mathbf{e}_H $$\nSince $R A P$ is an invertible matrix (the operator $A_H$) and $\\mathbf{e}_H$ is a vector, $(R A P)^{-1}(R A P) = I$. Therefore:\n$$ C \\mathbf{e} = P \\mathbf{e}_H - P (I \\mathbf{e}_H) = P \\mathbf{e}_H - P \\mathbf{e}_H = \\mathbf{0} $$\nThis proves that the coarse-grid correction operator exactly annihilates any error that can be perfectly represented on the coarse grid (i.e., is in the range of $P$).\n\nThe prolongation operator $P$, defined as linear interpolation, maps coarse-grid functions to smooth, slowly varying functions on the fine grid. These functions are, by their nature, low-frequency. High-frequency, oscillatory components cannot be accurately represented by simple interpolation from a much coarser grid. Consequently, the range of $P$ consists predominantly of the low-frequency modes of the error. Since the coarse-grid correction operator $C$ annihilates vectors in the range of $P$, it effectively targets and eliminates the low-frequency components of the error, complementing the action of the smoother.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef construct_matrices(N):\n    \"\"\"\n    Constructs the matrices A, R, P, and A_H for a given grid size N.\n    \"\"\"\n    h = 1.0 / (N + 1)\n    Nc = (N - 1) // 2\n\n    # Construct the fine-grid matrix A\n    main_diag = np.full(N, 2.0 / h**2)\n    off_diag = np.full(N - 1, -1.0 / h**2)\n    A = np.diag(main_diag) + np.diag(off_diag, k=1) + np.diag(off_diag, k=-1)\n\n    # Construct the prolongation (linear interpolation) matrix P\n    # P maps from Nc coarse points to N fine points\n    P = np.zeros((N, Nc))\n    for j in range(Nc):\n        # A value at coarse index j affects fine indices around fine index 2j+1\n        # It contributes to fine points 2*j, 2*j+1, 2*j+2 (0-indexed)\n        \n        # Contribution to the odd-indexed fine point (midpoint interpolation)\n        P[2 * j, j] = 0.5\n        \n        # Contribution to the even-indexed fine point (injection)\n        P[2 * j + 1, j] = 1.0\n        \n        # Contribution to the next odd-indexed fine point\n        if 2 * j + 2 < N:\n            P[2 * j + 2, j] = 0.5\n            \n    # Construct the restriction (full weighting) matrix R\n    # R is the scaled transpose of P for this standard choice\n    R = 0.5 * P.T\n    \n    # Construct the Galerkin coarse-grid operator A_H\n    A_H = R @ A @ P\n    \n    return A, R, P, A_H\n\n\ndef solve():\n    \"\"\"\n    Runs the full analysis for all test cases and prints the results.\n    \"\"\"\n    test_cases = [\n        (63, 31, 2/3, 1),\n        (63, 2, 2/3, 1),\n        (63, 16, 2/3, 3),\n        (15, 7, 0.8, 2),\n        (127, 1, 0.6, 5),\n    ]\n\n    all_results = []\n    for N, k, omega, nu in test_cases:\n        h = 1.0 / (N + 1)\n        A, R, P, A_H = construct_matrices(N)\n\n        # Create the initial error mode vector e_k\n        i_vals = np.arange(1, N + 1)\n        e_k = np.sin(k * np.pi * i_vals * h)\n        \n        initial_norm = np.linalg.norm(e_k)\n        if initial_norm == 0:\n            # Should not happen for valid k, but as a safeguard\n            all_results.append([0.0, 0.0, 0.0])\n            continue\n            \n        # 1. Measured smoothing reduction factor\n        # Error propagation matrix for weighted Jacobi: S = I - omega * D^-1 * A\n        # D = (2/h^2)*I, so D^-1 = (h^2/2)*I\n        S = np.eye(N) - omega * (h**2 / 2.0) * A\n        e_smoothed = np.copy(e_k)\n        for _ in range(nu):\n            e_smoothed = S @ e_smoothed\n        \n        measured_smoothing = np.linalg.norm(e_smoothed) / initial_norm\n\n        # 2. Theoretical smoothing reduction factor\n        mu_k = 1.0 - 2.0 * omega * np.sin(k * np.pi * h / 2.0)**2\n        theoretical_smoothing = np.abs(mu_k)**nu\n\n        # 3. Measured coarse-grid correction reduction factor\n        # Correction operator C = I - P * A_H^-1 * R * A\n        A_H_inv = np.linalg.inv(A_H)\n        C = np.eye(N) - P @ A_H_inv @ R @ A\n        e_corrected = C @ e_k\n        \n        measured_coarse_correction = np.linalg.norm(e_corrected) / initial_norm\n\n        all_results.append([measured_smoothing, theoretical_smoothing, measured_coarse_correction])\n\n    # Format the final output string\n    output_str = \"[\"\n    for i, res in enumerate(all_results):\n        formatted_res = f\"[{res[0]:.6f},{res[1]:.6f},{res[2]:.6f}]\"\n        output_str += formatted_res\n        if i < len(all_results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    print(output_str)\n\nsolve()\n```", "id": "3163227"}]}