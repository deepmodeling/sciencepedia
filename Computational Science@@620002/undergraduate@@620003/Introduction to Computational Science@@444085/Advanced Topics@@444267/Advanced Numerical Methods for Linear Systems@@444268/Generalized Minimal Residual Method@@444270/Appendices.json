{"hands_on_practices": [{"introduction": "The power of the Generalized Minimal Residual method (GMRES) can be understood from two distinct but equivalent perspectives. This exercise bridges the gap between the abstract view of finding an optimal residual polynomial and the geometric interpretation of performing an orthogonal projection. By implementing both formulations and verifying their numerical equivalence, you will gain a deeper, more robust understanding of what GMRES fundamentally accomplishes at each step [@problem_id:3136956].", "problem": "You are given the task of connecting the residual-polynomial viewpoint to the orthogonal-projection viewpoint of the Generalized Minimal Residual Method (GMRES). The goal is to show what the residual polynomial is, why its minimization characterizes GMRES, and how to compute it algorithmically from first principles.\n\nFundamental starting point and definitions:\n- For a linear system $A x = b$ with a square real matrix $A \\in \\mathbb{R}^{n \\times n}$, an initial guess $x_0 \\in \\mathbb{R}^n$, and the Euclidean norm $\\|\\cdot\\|_2$, the initial residual is $r_0 = b - A x_0$.\n- The $k$-th Krylov subspace is $\\mathcal{K}_k(A,r_0) = \\mathrm{span}\\{r_0, A r_0, A^2 r_0, \\dots, A^{k-1} r_0\\}$.\n- A residual polynomial of degree at most $k$ with $p_k(0) = 1$ is a polynomial $p_k(t) = 1 + c_1 t + c_2 t^2 + \\cdots + c_k t^k$ satisfying the constraint $p_k(0) = 1$; the residual-polynomial viewpoint seeks coefficients $c_1, \\dots, c_k$ that minimize $\\|p_k(A) r_0\\|_2$.\n- The Generalized Minimal Residual Method (GMRES) is defined as choosing $x_k \\in x_0 + \\mathcal{K}_k(A,r_0)$ to minimize $\\|b - A x_k\\|_2$, with the orthogonal projection interpretation that the GMRES residual $r_k = b - A x_k$ is orthogonal to $A \\mathcal{K}_k(A,r_0)$.\n\nYour program must, for each test case below, perform the following tasks purely in mathematical terms:\n1. Construct a residual polynomial $p_k(t)$ of degree at most $k$ with $p_k(0) = 1$ and compute the minimal attainable value of $\\|p_k(A) r_0\\|_2$, denoted $N_{\\text{poly}}$.\n2. Independently compute $N_{\\text{gmres}}$, the norm of the residual obtained after $k$ steps of the Generalized Minimal Residual Method (GMRES) started from $x_0$.\n3. Verify the orthogonal projection interpretation by checking if the GMRES residual $r_k$ is orthogonal to the set $A \\mathcal{K}_k(A,r_0)$, within numerical tolerance $10^{-9}$ in the sense that for every basis vector $w \\in \\mathcal{K}_k(A,r_0)$, the inner product $r_k^\\top (A w)$ has magnitude at most $10^{-9} \\|r_k\\|_2 \\|A w\\|_2$.\n4. For each test case, output two items: the absolute difference $|N_{\\text{poly}} - N_{\\text{gmres}}|$ (a float) and a boolean reporting whether the orthogonality condition holds (true or false). Aggregate all cases’ results into a single line list by alternating the float and the boolean.\n\nTest suite specification:\n- All computations use real numbers without any physical units.\n- Angles used to define entries must be in radians.\n- The right-hand side vector is defined deterministically as $b \\in \\mathbb{R}^8$ with entries $b_i = \\sin(i)$ for $i = 1, 2, \\dots, 8$ (with $\\sin(\\cdot)$ in radians). The initial guess is $x_0 = 0$ in $\\mathbb{R}^8$.\n- Matrices:\n  1. Case 1 (symmetric with widely spread eigenvalues): $A_1 \\in \\mathbb{R}^{8 \\times 8}$ is diagonal with diagonal entries $\\{0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0, 5000.0\\}$. Degree $k = 3$.\n  2. Case 2 (boundary degree): Same $A_1$ as Case 1. Degree $k = 0$.\n  3. Case 3 (non-normal with widely spread eigenvalues): $A_3 \\in \\mathbb{R}^{8 \\times 8}$ is upper-triangular with diagonal entries $\\{0.001, 0.01, 1.0, 10.0, 100.0, 500.0, 1000.0, 5000.0\\}$ and ones on the first superdiagonal; all other off-diagonals are zero. Degree $k = 7$.\n\nFinal output format:\n- Your program should produce a single line of output containing a Python-style list of six elements that alternates the float and boolean for the three test cases, in order: Case 1, Case 2, Case 3. For example, the format must be exactly like\n$[r_1, b_1, r_2, b_2, r_3, b_3]$\nwhere $r_j$ are floats and $b_j$ are booleans.", "solution": "The problem requires a verification of the equivalence between two fundamental viewpoints of the Generalized Minimal Residual Method (GMRES) for solving a linear system $A x = b$, where $A \\in \\mathbb{R}^{n \\times n}$. The first viewpoint is the minimization of a residual polynomial, and the second is the orthogonal projection method that defines the standard algorithm.\n\nLet the initial guess be $x_0$, and the initial residual be $r_0 = b - A x_0$. The $k$-th iterate of GMRES, $x_k$, is sought in the affine Krylov subspace $x_0 + \\mathcal{K}_k(A,r_0)$, where $\\mathcal{K}_k(A,r_0) = \\mathrm{span}\\{r_0, A r_0, \\dots, A^{k-1} r_0\\}$.\n\n**1. The Residual-Polynomial Viewpoint**\n\nThe GMRES iterate $x_k$ can be expressed as $x_k = x_0 + z_{k-1}$ for some vector $z_{k-1} \\in \\mathcal{K}_k(A,r_0)$. By definition of the Krylov subspace, $z_{k-1}$ can be written as a polynomial in $A$ of degree at most $k-1$ acting on $r_0$. Let us write $z_{k-1} = q_{k-1}(A)r_0$ for some polynomial $q_{k-1}$. The corresponding residual is $r_k = b - A x_k = b - A(x_0 + q_{k-1}(A)r_0) = (b - A x_0) - A q_{k-1}(A)r_0 = r_0 - A q_{k-1}(A)r_0$.\n\nLet's define a new polynomial $p_k(t) = 1 - t q_{k-1}(t)$. This polynomial $p_k(t)$ has a degree of at most $k$ and satisfies the crucial constraint $p_k(0) = 1$. The residual can now be expressed as $r_k = p_k(A)r_0$. GMRES finds the iterate $x_k$ that minimizes $\\|r_k\\|_2 = \\|p_k(A)r_0\\|_2$ over all possible choices of $z_{k-1} \\in \\mathcal{K}_k(A,r_0)$. This is equivalent to minimizing over all polynomials $p_k$ of degree at most $k$ with $p_k(0)=1$.\n\nTo compute the minimum norm from this viewpoint, denoted $N_{\\text{poly}}$, we set up a linear least-squares problem. A generic such polynomial is $p_k(t) = 1 + c_1 t + c_2 t^2 + \\dots + c_k t^k$. We seek coefficients $c = [c_1, \\dots, c_k]^\\top$ that minimize:\n$$ \\|p_k(A)r_0\\|_2 = \\left\\| \\left(I + \\sum_{j=1}^k c_j A^j\\right) r_0 \\right\\|_2 = \\left\\| r_0 + \\sum_{j=1}^k c_j (A^j r_0) \\right\\|_2 $$\nLet $M$ be a matrix whose columns are the vectors $A^j r_0$ for $j=1, \\dots, k$. The problem is to find $c$ that minimizes $\\|r_0 + Mc\\|_2$, or equivalently, $\\|Mc - (-r_0)\\|_2$. This is a standard linear least-squares problem. The minimal norm $N_{\\text{poly}}$ is the norm of the residual of this problem, i.e., $N_{\\text{poly}} = \\|r_0 + Mc^*\\|_2$, where $c^*$ is the optimal coefficient vector.\n\n**2. The GMRES Algorithm and Orthogonal Projection**\n\nThe standard GMRES algorithm operationalizes the minimization of $\\|r_k\\|_2 = \\|b-Ax_k\\|_2$ for $x_k \\in x_0+\\mathcal{K}_k(A,r_0)$. This is a least-squares problem over a $k$-dimensional subspace. The key is to construct an orthonormal basis for $\\mathcal{K}_{k+1}(A, r_0)$, which is done via the Arnoldi iteration.\n\nThe Arnoldi iteration generates a set of orthonormal vectors $\\{q_1, q_2, \\dots, q_{k+1}\\}$ that span $\\mathcal{K}_{k+1}(A, r_0)$, with $q_1 = r_0 / \\|r_0\\|_2$. It also produces an upper Hessenberg matrix $\\bar{H}_k \\in \\mathbb{R}^{(k+1) \\times k}$ such that $A V_k = V_{k+1} \\bar{H}_k$, where $V_j = [q_1, \\dots, q_j]$.\n\nThe iterate is expressed as $x_k = x_0 + V_k y_k$ for some coordinate vector $y_k \\in \\mathbb{R}^k$. The minimization problem becomes:\n$$ \\min_{y_k \\in \\mathbb{R}^k} \\|r_0 - A V_k y_k\\|_2 = \\min_{y_k \\in \\mathbb{R}^k} \\| \\|r_0\\|_2 q_1 - V_{k+1} \\bar{H}_k y_k \\|_2 $$\nUsing $q_1 = V_{k+1} e_1$ (where $e_1=[1,0,\\dots,0]^\\top \\in \\mathbb{R}^{k+1}$) and the isometry of $V_{k+1}$, the problem simplifies to a small $(k+1) \\times k$ least-squares problem for $y_k$:\n$$ \\min_{y_k \\in \\mathbb{R}^k} \\| \\|r_0\\|_2 e_1 - \\bar{H}_k y_k \\|_2 $$\nThe norm of the GMRES residual, $N_{\\text{gmres}}$, is the minimum value (the norm of the residual vector) of this small problem. Let $y_k^*$ be the solution. Then $N_{\\text{gmres}} = \\| \\|r_0\\|_2 e_1 - \\bar{H}_k y_k^* \\|_2$.\n\n**3. Verification of the Orthogonality Condition**\n\nThe first-order optimality condition for the GMRES minimization problem is that the residual $r_k$ must be orthogonal to the search space translated to the origin, which is $A \\mathcal{K}_k(A, r_0)$. This means $r_k^\\top v = 0$ for all $v \\in A \\mathcal{K}_k(A, r_0)$.\n\nTo verify this numerically, we check the condition against a basis for $A \\mathcal{K}_k(A, r_0)$. A natural basis for this space is $\\{A r_0, A^2 r_0, \\dots, A^k r_0\\}$. For each basis vector $v_j = A^j r_0$ where $j \\in \\{1, \\dots, k\\}$, we must verify that the inner product with the GMRES residual $r_k$ is close to zero, within a specified tolerance $\\epsilon = 10^{-9}$. The condition is:\n$$ |r_k^\\top v_j| \\leq \\epsilon \\|r_k\\|_2 \\|v_j\\|_2 $$\nThe GMRES residual vector $r_k$ is computed as $r_k = V_{k+1} (\\|r_0\\|_2 e_1 - \\bar{H}_k y_k^*)$.\n\nThe implementation below computes $N_{\\text{poly}}$ and $N_{\\text{gmres}}$ independently and then verifies the orthogonality property, demonstrating the theoretical equivalence of the two viewpoints. The absolute difference $|N_{\\text{poly}} - N_{\\text{gmres}}|$ should be close to floating-point machine precision.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the GMRES verification for all test cases.\n    \"\"\"\n\n    def run_case(A, b, x0, k):\n        \"\"\"\n        Performs the required computations for a single test case.\n        1. Computes N_poly from the residual-polynomial viewpoint.\n        2. Computes N_gmres and the residual vector r_k via GMRES/Arnoldi.\n        3. Checks the orthogonality of r_k against A*K_k(A,r0).\n        Returns the absolute difference between N_poly and N_gmres, and a\n        boolean indicating if the orthogonality condition holds.\n        \"\"\"\n        n = A.shape[0]\n        r0 = b - A @ x0\n        norm_r0 = np.linalg.norm(r0)\n        \n        # Handle the trivial case k=0\n        if k == 0:\n            N_poly = norm_r0\n            N_gmres = norm_r0\n            # The space A*K_0 is {0}, so any vector is orthogonal to it.\n            is_orthogonal = True\n            return np.abs(N_poly - N_gmres), is_orthogonal\n\n        # --- 1. Residual-Polynomial Viewpoint ---\n        # Minimize ||r0 + c1*A*r0 + ... + ck*A^k*r0|| wrt cj's.\n        # This is the least-squares problem ||M*c - (-r0)||^2.\n        M = np.zeros((n, k))\n        power_vec = r0\n        for j in range(k):\n            power_vec = A @ power_vec\n            M[:, j] = power_vec\n        \n        c = np.linalg.lstsq(M, -r0, rcond=None)[0]\n        res_poly_vec = r0 + M @ c\n        N_poly = np.linalg.norm(res_poly_vec)\n\n        # --- 2. GMRES/Arnoldi Viewpoint ---\n        Q = np.zeros((n, k + 1))\n        H = np.zeros((k + 1, k))\n        \n        if norm_r0 == 0:\n            # x0 is the exact solution.\n            N_gmres = 0.0\n            r_k = np.zeros(n)\n        else:\n            Q[:, 0] = r0 / norm_r0\n    \n            for j in range(k):\n                w = A @ Q[:, j]\n                for i in range(j + 1):\n                    H[i, j] = Q[:, i].T @ w\n                    w = w - H[i, j] * Q[:, i]\n                \n                h_next = np.linalg.norm(w)\n                if h_next < 1e-12: # Check for breakdown\n                    # In case of breakdown, the space is smaller.\n                    # This requires truncating H and Q matrices.\n                    # For a general-purpose solver this is crucial; here we assume no breakdown.\n                    # For this problem's setup, this path is not taken.\n                    k_eff = j + 1\n                    H = H[:k_eff+1, :k_eff]\n                    Q = Q[:, :k_eff+1]\n                    break\n                H[j + 1, j] = h_next\n                Q[:, j + 1] = w / h_next\n\n            # Solve the small least-squares problem: min ||norm(r0)*e1 - H*y||\n            e1 = np.zeros(k + 1)\n            e1[0] = 1.0\n            rhs = norm_r0 * e1\n            \n            y = np.linalg.lstsq(H, rhs, rcond=None)[0]\n            \n            # Compute N_gmres and the full residual vector r_k\n            small_res_vec = rhs - H @ y\n            N_gmres = np.linalg.norm(small_res_vec)\n            r_k = Q @ small_res_vec\n\n        # --- 3. Orthogonality Check ---\n        # Verify r_k is orthogonal to the basis {A*r0, A^2*r0, ..., A^k*r0}\n        # which spans A*K_k(A,r0).\n        is_orthogonal = True\n        norm_rk = np.linalg.norm(r_k)\n        check_vec = r0\n        \n        for j in range(1, k + 1):\n            check_vec = A @ check_vec   # This is A^j * r0\n            norm_check_vec = np.linalg.norm(check_vec)\n            \n            ip = r_k.T @ check_vec\n            \n            # Use relative tolerance check\n            if norm_rk > 0 and norm_check_vec > 0:\n                condition = (np.abs(ip) <= 1e-9 * norm_rk * norm_check_vec)\n            else:\n                # If either vector is zero, their dot product must be (near) zero\n                condition = (np.abs(ip) < 1e-9)\n\n            if not condition:\n                is_orthogonal = False\n                break\n\n        # --- 4. Return results for the case ---\n        diff = np.abs(N_poly - N_gmres)\n        return diff, is_orthogonal\n\n    # Define common parameters\n    n = 8\n    b = np.sin(np.arange(1, n + 1))\n    x0 = np.zeros(n)\n    \n    # Define test cases\n    A1_diag = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0, 5000.0]\n    A1 = np.diag(A1_diag)\n    \n    A3_diag = [0.001, 0.01, 1.0, 10.0, 100.0, 500.0, 1000.0, 5000.0]\n    A3 = np.diag(A3_diag) + np.diag(np.ones(n - 1), 1)\n\n    test_cases = [\n        (A1, b, x0, 3),  # Case 1\n        (A1, b, x0, 0),  # Case 2\n        (A3, b, x0, 7),  # Case 3\n    ]\n\n    # Run all test cases and collect results\n    results = []\n    for case in test_cases:\n        diff, ortho = run_case(*case)\n        results.append(diff)\n        results.append(ortho)\n\n    # Format and print the final output\n    # Using a custom formatter for booleans to satisfy potential ambiguity,\n    # though standard str(bool) -> 'True'/'False' would also be Python-style.\n    # The problem example `true`/`false` is non-standard in Python output. \n    # Sticking to standard Python representation 'True'/'False'.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3136956"}, {"introduction": "While GMRES is an iterative method, in exact arithmetic it is guaranteed to find the solution in a finite number of steps. This exercise explores the elegant connection between the termination of GMRES and the algebraic properties of the system matrix $A$ and initial residual $r_0$. You will demonstrate computationally that the iteration count for exact convergence is precisely the degree of the minimal polynomial of $A$ with respect to $r_0$, revealing the deep algebraic structure that governs the method's behavior [@problem_id:3137005].", "problem": "You are to design and implement a program that demonstrates the exact termination behavior of the Generalized Minimal Residual method (GMRES) for solving a linear system $A x = b$. The theoretical base that you must use is limited to core linear algebra definitions and facts: the concept of a Krylov subspace, the Arnoldi process, and the definition of the minimal polynomial of a matrix relative to a vector. The fundamental definitions you may assume are: for a square matrix $A \\in \\mathbb{R}^{n \\times n}$ and an initial residual $r_0 = b - A x_0$, the Krylov subspace of order $k$ is $ \\mathcal{K}_k(A, r_0) = \\operatorname{span}\\{ r_0, A r_0, A^2 r_0, \\dots, A^{k-1} r_0 \\}$; and the minimal polynomial of $A$ relative to $r_0$ is the nonzero polynomial $p$ of least degree such that $p(A) r_0 = 0$, whose degree equals the dimension at which the Krylov subspace stops increasing. You must start from these definitions and no others, and you must not rely on any shortcut formulas that skip the derivation of GMRES from these bases.\n\nTask. Implement a non-restarted GMRES solver from first principles using the Arnoldi process and a least-squares solve restricted to the Krylov subspace, starting from the zero initial guess $x_0 = 0$. Use a relative residual stopping rule: stop at the first iteration $k$ such that $\\|r_k\\|_2 / \\|b\\|_2 \\le \\varepsilon$, where $\\varepsilon$ is the given tolerance. Also implement a routine that computes the degree $g$ of the minimal polynomial of $A$ relative to $r_0$ by detecting linear dependence in the sequence $\\{r_0, A r_0, A^2 r_0, \\dots\\}$ via orthonormalization.\n\nScientific requirement. In exact arithmetic, GMRES terminates in at most the degree $g$ of the minimal polynomial relative to $r_0$. Your program must numerically demonstrate that the first iteration $k^\\star$ at which GMRES achieves the stopping rule equals that degree $g$ for a collection of carefully chosen test matrices $A$ and corresponding right-hand sides $b$.\n\nAngle units are not applicable. No physical units are involved. All computations are purely mathematical over $\\mathbb{R}$.\n\nTolerance. Use the numerical tolerance $\\varepsilon = 10^{-12}$.\n\nTest suite. Your program must apply both routines (GMRES termination counter and minimal-polynomial degree detector) to the following four cases, all with $x_0 = 0$:\n\n- Case $1$ (degree $1$ boundary case): \n  - $A_1 = 5 I_3$,\n  - $b_1 = \\begin{bmatrix} 1 \\\\ -2 \\\\ 3 \\end{bmatrix}$.\n- Case $2$ (diagonalizable with two distinct eigenvalues): \n  - $A_2 = \\operatorname{diag}(1, 2, 2)$,\n  - $b_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$.\n- Case $3$ (defective Jordan block, full chain): \n  - $A_3 = \\begin{bmatrix} 2 & 1 & 0 \\\\ 0 & 2 & 1 \\\\ 0 & 0 & 2 \\end{bmatrix}$,\n  - $b_3 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$.\n- Case $4$ (block diagonal with selective excitation; global minimal polynomial larger than relative one):\n  - $A_4 = \\operatorname{diag}\\!\\Big( \\begin{bmatrix} 3 & 1 \\\\ 0 & 3 \\end{bmatrix}, \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix} \\Big)$,\n  - $b_4 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$.\n\nFor each case, let $k^\\star$ be the first GMRES iteration satisfying the stopping rule, and let $g$ be the detected degree of the minimal polynomial of $A$ relative to $r_0 = b - A x_0 = b$. For scientific realism and numerical robustness, determine $g$ by building an orthonormal basis of $\\{ r_0, A r_0, A^2 r_0, \\dots \\}$ via modified Gram–Schmidt and stopping when the next vector is detected linearly dependent with tolerance proportional to $\\|r_0\\|_2$.\n\nRequired output. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case, output the boolean value of the statement “$k^\\star = g$”. Thus the final output should be a single line of the form, for example, “[True,True,True,True]”. No other text should be printed.\n\nNotes.\n- All numbers used above, such as $5$, $1$, $2$, $3$, and $10^{-12}$, are exact specifications of the test instances and tolerance.\n- You must not read input; you must hardcode the test suite exactly as stated and print precisely one line as specified.\n- Your implementation must adhere to standard double-precision arithmetic and the definitions provided above.", "solution": "The problem requires a numerical demonstration of a fundamental property of the Generalized Minimal Residual method (GMRES) for solving linear systems of the form $Ax=b$. Specifically, it posits that in exact arithmetic, the method terminates with the exact solution at an iteration count equal to the degree of the minimal polynomial of the matrix $A$ with respect to the initial residual vector $r_0$. Our task is to implement both a GMRES solver and a function to compute this degree, and then verify their equivalence across a suite of test cases. We begin from first principles as dictated.\n\nThe initial guess is specified as $x_0=0$, which implies the initial residual is $r_0 = b - Ax_0 = b$.\n\n**1. Degree of the Minimal Polynomial**\n\nThe minimal polynomial of a matrix $A \\in \\mathbb{R}^{n \\times n}$ with respect to a vector $r_0 \\in \\mathbb{R}^n$ is the monic polynomial $p(z)$ of least degree $g$ such that $p(A)r_0 = 0$. This degree $g$ is also the dimension of the Krylov subspace $\\mathcal{K}(A, r_0) = \\operatorname{span}\\{r_0, Ar_0, A^2 r_0, \\dots\\}$. The dimension ceases to increase when a vector in the sequence $\\{A^k r_0\\}_{k \\ge 0}$ becomes linearly dependent on its predecessors. The first index $k$ for which this occurs defines the degree, $g=k$.\n\nWe can determine $g$ computationally by constructing an orthonormal basis for the growing Krylov subspace, using the modified Gram-Schmidt (MGS) algorithm for its superior numerical stability. Let the sequence of Krylov vectors be $u_k = A^k r_0$ for $k=0, 1, 2, \\dots, n$. We generate an orthonormal basis $\\{v_0, v_1, \\dots, v_{g-1}\\}$ for $\\mathcal{K}_g(A, r_0)$.\n\nThe algorithm proceeds as follows:\n1. Initialize an empty list of orthonormal basis vectors, $V$.\n2. For $k=0, 1, 2, \\dots, n$:\n   a. Take the next Krylov vector, $u_k = A^k r_0$. (This is computed iteratively: $u_0=r_0$, $u_{k+1}=Au_k$).\n   b. Orthogonalize $u_k$ against the current basis vectors $\\{v_0, \\dots, v_{k-1}\\}$ in $V$:\n      $$ w = u_k - \\sum_{j=0}^{k-1} (v_j^T u_k) v_j $$\n   c. The norm $\\|w\\|_2$ represents the component of $u_k$ that is orthogonal to the subspace $\\mathcal{K}_k(A, r_0) = \\operatorname{span}\\{v_0, \\dots, v_{k-1}\\}$. If this norm is numerically zero (i.e., $\\|w\\|_2 \\le \\tau \\|r_0\\|_2$ for some small tolerance $\\tau$), it signifies that $u_k$ is linearly dependent on the preceding vectors $\\{u_0, \\dots, u_{k-1}\\}$. The dimension of the Krylov subspace is $k$, so the degree of the minimal polynomial is $g=k$. The process terminates, returning $k$.\n   d. If $\\|w\\|_2$ is not negligible, we normalize it to obtain the next basis vector $v_k = w / \\|w\\|_2$ and add it to our orthonormal set $V$.\n\nThis procedure directly determines the dimension of the largest reachable Krylov subspace, which is by definition the degree $g$.\n\n**2. GMRES Algorithm from First Principles**\n\nThe GMRES method finds an approximate solution $x_k$ at iteration $k$ from the affine subspace $x_0 + \\mathcal{K}_k(A, r_0)$. The solution $x_k$ is chosen to minimize the $2$-norm of the residual, $\\|r_k\\|_2 = \\|b - Ax_k\\|_2$. With the initial guess $x_0=0$, the approximation $x_k$ lies in the Krylov subspace $\\mathcal{K}_k(A, r_0)$.\n\nThe core of GMRES is the Arnoldi process, which builds an orthonormal basis $V_{k+1} = [v_1, \\dots, v_{k+1}]$ for the Krylov subspace $\\mathcal{K}_{k+1}(A, r_0)$ and simultaneously produces an upper Hessenberg matrix $H_{k+1,k} \\in \\mathbb{R}^{(k+1) \\times k}$. These matrices are linked by the Arnoldi relation:\n$$ A V_k = V_{k+1} H_{k+1,k} $$\nwhere $V_k = [v_1, \\dots, v_k]$. The process is initialized with the first basis vector $v_1 = r_0 / \\|r_0\\|_2$.\n\nAn iterate $x_k$ can be expressed as a linear combination of the basis vectors, $x_k = V_k y$, for some coordinate vector $y \\in \\mathbb{R}^k$. The corresponding residual is then:\n$$ r_k = b - Ax_k = r_0 - A V_k y $$\nSubstituting $r_0 = \\|r_0\\|_2 v_1$ and the Arnoldi relation, we get:\n$$ r_k = \\|r_0\\|_2 v_1 - V_{k+1} H_{k+1,k} y = V_{k+1} (\\|r_0\\|_2 e_1 - H_{k+1,k} y) $$\nwhere $e_1$ is the first standard basis vector in $\\mathbb{R}^{k+1}$. Since the columns of $V_{k+1}$ are orthonormal, minimizing $\\|r_k\\|_2$ is equivalent to minimizing $\\|\\,\\|r_0\\|_2 e_1 - H_{k+1,k} y\\,\\|_2$.\n\nThis is a small-scale linear least-squares problem for the coordinate vector $y$. It can be solved at each iteration $k$. The overall GMRES algorithm is:\n1. Initialize: $r_0 = b$, $\\beta = \\|r_0\\|_2$, $v_1 = r_0 / \\beta$.\n2. For $k=1, 2, \\dots, n$:\n   a. **Arnoldi Step**: Generate $v_{k+1}$ and the $(k-1)$-th column of the Hessenberg matrix using the Arnoldi process. This involves computing $w = Av_k$ and orthogonalizing it against $\\{v_1, \\dots, v_k\\}$. Let the resulting coefficients be $h_{i,k}$ for $i=1, \\dots, k$ and the norm of the orthogonalized vector be $h_{k+1,k}$.\n   b. **Least-Squares Solve**: Form the matrix $H_{k+1,k}$ and the vector $g = \\beta e_1 \\in \\mathbb{R}^{k+1}$. Find $y_k \\in \\mathbb{R}^k$ that minimizes $\\|H_{k+1,k} y - g\\|_2$.\n   c. **Check Convergence**: The norm of the residual for the $k$-th iterate is the minimum value from the least-squares problem: $\\|r_k\\|_2 = \\min_y \\|H_{k+1,k} y - g\\|_2$. We check if the relative residual satisfies $\\|r_k\\|_2 / \\|b\\|_2 \\le \\varepsilon$. If so, the termination iteration is $k^\\star = k$, and we stop.\n\n**3. Verification of $k^\\star = g$**\n\nThe theoretical foundation connecting the two algorithms is that if the degree of the minimal polynomial is $g$, then the Krylov subspace $\\mathcal{K}_g(A, r_0)$ contains the exact solution $x = A^{-1}b$. Furthermore, for any vector $u \\in \\mathcal{K}_g(A, r_0)$, the vector $Au$ also lies in $\\mathcal{K}_g(A, r_0)$.\nWhen the Arnoldi process reaches iteration $k=g$, the vector $A v_g$ is already in the space spanned by $\\{v_1, \\dots, v_g\\}$. Consequently, after orthogonalization, the remaining vector is zero, leading to a zero (or numerically tiny) value for the Hessenberg entry $h_{g+1,g}$. This event is known as a \"lucky breakdown\". The implication for the least-squares problem is that a vector $y_g$ exists such that $H_{g+1,g} y_g = \\beta e_1$, which makes the residual norm $\\|r_g\\|_2$ drop to zero. The GMRES approximation $x_g$ becomes the exact solution.\nTherefore, the first iteration $k^\\star$ at which GMRES satisfies the stopping criterion with a sufficiently small tolerance $\\varepsilon=10^{-12}$ will be precisely the degree $g$ of the minimal polynomial relative to $r_0$. Our program will compute both $g$ and $k^\\star$ for each test case and verify this equality.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef find_min_poly_degree(A, b):\n    \"\"\"\n    Computes the degree of the minimal polynomial of A with respect to b.\n    This is done by finding the dimension of the Krylov subspace K(A, b)\n    by detecting linear dependence in the sequence {b, Ab, A^2b, ...}\n    using modified Gram-Schmidt.\n    \"\"\"\n    n = A.shape[0]\n    r0_norm = np.linalg.norm(b)\n    \n    if r0_norm == 0:\n        return 0\n\n    # Tolerance for detecting linear dependence, proportional to the initial norm.\n    dep_tol = 1e-12 * r0_norm\n\n    orthonormal_basis = []\n    current_krylov_vec = b.copy().astype(np.float64)\n\n    for k in range(n + 1):\n        # Orthogonalize the current Krylov vector against the basis so far.\n        orth_vec = current_krylov_vec.copy()\n        for v in orthonormal_basis:\n            proj = np.dot(v.conj(), orth_vec)\n            orth_vec -= proj * v\n            \n        norm_orth_vec = np.linalg.norm(orth_vec)\n        \n        # If the norm of the orthogonalized vector is close to zero,\n        # it means the current Krylov vector is linearly dependent on the previous ones.\n        if norm_orth_vec < dep_tol:\n            return k # The degree of the minimal polynomial is k.\n            \n        # Add the new orthonormal vector to the basis.\n        orthonormal_basis.append(orth_vec / norm_orth_vec)\n        \n        # Generate the next Krylov vector for the next iteration.\n        current_krylov_vec = A @ current_krylov_vec\n            \n    return n\n\ndef gmres_solver(A, b, tol):\n    \"\"\"\n    Implements a non-restarted GMRES solver from first principles.\n    Returns the number of iterations k* at which the relative residual\n    ||r_k||/||b|| falls below the tolerance.\n    \"\"\"\n    n = A.shape[0]\n    \n    # Initial guess x0 = 0, so initial residual r0 = b.\n    r0 = b.copy().astype(np.float64)\n    b_norm = np.linalg.norm(b)\n    \n    if b_norm == 0:\n        return 0 # Trivial case: solution is x=0, 0 iterations.\n\n    r0_norm = b_norm\n    \n    # V stores the orthonormal basis vectors for the Krylov subspace.\n    V = [r0 / r0_norm]\n    # H will be the (k+1) x k upper Hessenberg matrix.\n    H = np.zeros((n + 1, n), dtype=np.float64)\n    \n    # e1 is used to form the right-hand side of the least-squares problem.\n    e1 = np.zeros(n + 1, dtype=np.float64)\n    e1[0] = 1.0\n\n    # Main GMRES loop.\n    for k in range(n): # k from 0 to n-1, corresponding to iterations 1 to n.\n        # Arnoldi process to generate v_{k+1} and the k-th column of H.\n        # Note on indices: V[k] is the (k+1)-th basis vector, v_{k+1} in 1-based indexing.\n        w = A @ V[k]\n        \n        for j in range(k + 1):\n            H[j, k] = np.dot(w, V[j])\n            w = w - H[j, k] * V[j]\n        \n        H[k + 1, k] = np.linalg.norm(w)\n        \n        # Form and solve the least-squares problem for iteration k+1.\n        # The Hessenberg matrix for this iteration is of size (k+2) x (k+1).\n        H_sub = H[:k+2, :k+1]\n        \n        # The right-hand side is beta * e1, where beta = ||r0||.\n        target = r0_norm * e1[:k+2]\n        \n        # Solve the (k+2)x(k+1) least-squares problem.\n        y, residuals, _, _ = np.linalg.lstsq(H_sub, target, rcond=None)\n        \n        # The residual norm ||r_k|| is the residual of the LS problem.\n        # np.linalg.lstsq returns the sum of squared residuals, so take the square root.\n        res_norm = np.sqrt(residuals[0]) if residuals.size > 0 else np.linalg.norm(H_sub @ y - target)\n\n        # Check stopping criterion based on relative residual.\n        if res_norm / b_norm <= tol:\n            return k + 1 # GMRES has converged in k+1 iterations.\n        \n        # Check for Arnoldi breakdown. If h_{k+1,k} is numerically zero, the subspace\n        # is invariant. GMRES finds the exact solution. The residual check above\n        # inherently captures this, but this is a fail-safe.\n        if H[k + 1, k] < 1e-16:\n            return k + 1\n            \n        V.append(w / H[k + 1, k])\n        \n    return n # Maximum number of iterations reached.\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the algorithms, and print the results.\n    \"\"\"\n    TOL = 1e-12\n\n    test_cases = [\n        # Case 1: Identity matrix scaled, degree g=1\n        (\n            5.0 * np.identity(3, dtype=np.float64),\n            np.array([1.0, -2.0, 3.0], dtype=np.float64)\n        ),\n        # Case 2: Diagonalizable with repeated eigenvalue, g=2\n        (\n            np.diag(np.array([1.0, 2.0, 2.0], dtype=np.float64)),\n            np.array([1.0, 1.0, 1.0], dtype=np.float64)\n        ),\n        # Case 3: Defective Jordan block, g=3\n        (\n            np.array([[2.0, 1.0, 0.0], [0.0, 2.0, 1.0], [0.0, 0.0, 2.0]], dtype=np.float64),\n            np.array([1.0, 1.0, 1.0], dtype=np.float64)\n        ),\n        # Case 4: Block diagonal with selective excitation, g=2\n        (\n            np.array([\n                [3.0, 1.0, 0.0, 0.0], \n                [0.0, 3.0, 0.0, 0.0], \n                [0.0, 0.0, 1.0, 1.0], \n                [0.0, 0.0, 0.0, 1.0]\n            ], dtype=np.float64),\n            np.array([1.0, 1.0, 0.0, 0.0], dtype=np.float64)\n        )\n    ]\n\n    results = []\n    for A, b in test_cases:\n        g = find_min_poly_degree(A, b)\n        k_star = gmres_solver(A, b, TOL)\n        results.append(g == k_star)\n\n    # Convert list of booleans to the required string format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3137005"}, {"introduction": "In many scientific applications, linear systems are ill-conditioned or nearly singular, posing a significant challenge to iterative solvers like GMRES. This practice moves from idealized scenarios to this practical reality, exploring how to stabilize the solution process using Tikhonov regularization. You will apply GMRES to both the original ill-posed problem and its regularized counterpart, the normal equations, to compare their performance and appreciate how regularization reshapes the problem to make it solvable [@problem_id:3136913].", "problem": "You are to study the behavior of the Generalized Minimal Residual (GMRES) method for a nearly singular linear system and to compare it with a Tikhonov-regularized alternative formulated through the normal equations. Begin only from core definitions: the notion of a residual $r(x) = b - Ax$ for a linear system $Ax = b$, the definition of a Krylov subspace $\\mathcal{K}_k(A, r_0) = \\operatorname{span}\\{r_0, Ar_0, \\dots, A^{k-1} r_0\\}$, and the idea that GMRES seeks an approximate solution $x_k$ in $x_0 + \\mathcal{K}_k(A, r_0)$ with minimal residual norm. Do not assume any pre-derived formula for GMRES; instead, derive the least-squares problem associated with the Arnoldi process and explain the polynomial-residual interpretation from these bases.\n\nConstruct a reproducible nearly singular matrix $A \\in \\mathbb{R}^{n \\times n}$ by fixing orthonormal factors and switching only the singular values. Specifically:\n- Fix a dimension $n \\in \\mathbb{N}$ with $n \\ge 2$.\n- Let $U, V \\in \\mathbb{R}^{n \\times n}$ be orthonormal matrices obtained by applying the $\\mathrm{QR}$ factorization to Gaussian random matrices generated with a fixed random seed. Concretely, draw two matrices with independent and identically distributed standard normal entries using a pseudorandom number generator initialized at seed $42$, and obtain $U$ and $V$ as the orthonormal factors from their $\\mathrm{QR}$ decompositions.\n- Given a parameter $\\varepsilon \\in (0, 1]$, define a diagonal matrix $\\Sigma(\\varepsilon) = \\operatorname{diag}(s_1, \\dots, s_n)$ with $s_i = \\varepsilon^{\\frac{i-1}{n-1}}$ for $i = 1, \\dots, n$ so that $s_1 = 1$ and $s_n = \\varepsilon$. Set $A(\\varepsilon) = U \\Sigma(\\varepsilon) V^\\top$.\n- Define $b \\in \\mathbb{R}^{n}$ as a standard normal random vector generated with a fixed random seed $7$.\n\nYour task is to:\n- Implement a $k$-step Arnoldi process with Modified Gram–Schmidt to construct an orthonormal basis of $\\mathcal{K}_k(A, r_0)$, where $r_0 = b - A x_0$ and $x_0 = 0$.\n- From the Arnoldi relation, assemble the associated small least-squares problem and compute the $k$-step GMRES approximation $x_k$ that minimizes $\\|b - A x\\|_2$ over $x_0 + \\mathcal{K}_k(A, r_0)$.\n- Report the final residual norm $\\|b - A x_k\\|_2$ after exactly $k$ Arnoldi steps (or earlier if a true breakdown occurs), denoted $R_{\\mathrm{GMRES}}$.\n- Implement Tikhonov regularization via the normal equations: for a given $\\lambda \\ge 0$, form $B(\\lambda) = A^\\top A + \\lambda I$ and $c = A^\\top b$, and run the same $k$-step GMRES procedure on the system $B(\\lambda) x = c$ to obtain an iterate $x_k^{(\\lambda)}$. For comparability, evaluate the original residual norm $R_{\\mathrm{Tik}} = \\|b - A x_k^{(\\lambda)}\\|_2$.\n- Explain, from first principles, how in GMRES the residual can be expressed as $r_k = p_k(A) r_0$ for some polynomial $p_k$ of degree at most $k$ with $p_k(0) = 1$, and how Tikhonov regularization applied to the normal equations modifies this to $r_k^{\\mathrm{NE}} = q_k(A^\\top A + \\lambda I) r_0^{\\mathrm{NE}}$ with $r_0^{\\mathrm{NE}} = c - B(\\lambda) x_0$; then connect the effect of $\\lambda$ on the spectrum, and thus on the admissible residual polynomials.\n\nUse the following fixed test suite. For all cases, take $n = 12$, $x_0 = 0$, the construction of $A(\\varepsilon)$ and $b$ as above, and run exactly $k$ Arnoldi steps unless a breakdown occurs.\n\nTest suite parameters $(\\varepsilon, \\lambda, k)$:\n- Case $1$: $(\\varepsilon, \\lambda, k) = (10^{-3}, 0, 4)$.\n- Case $2$: $(\\varepsilon, \\lambda, k) = (10^{-3}, 10^{-4}, 4)$.\n- Case $3$: $(\\varepsilon, \\lambda, k) = (10^{-8}, 10^{-6}, 8)$.\n- Case $4$: $(\\varepsilon, \\lambda, k) = (10^{-8}, 10^{-2}, 8)$.\n- Case $5$: $(\\varepsilon, \\lambda, k) = (10^{-2}, 0, 2)$.\n\nFor each case, compute and return a list of two floats $[R_{\\mathrm{GMRES}}, R_{\\mathrm{Tik}}]$, where $R_{\\mathrm{GMRES}}$ is the residual norm of the $k$-step GMRES iterate for $A x = b$, and $R_{\\mathrm{Tik}}$ is the residual norm of the $k$-step GMRES iterate applied to the Tikhonov-regularized normal equations evaluated back on the original residual of $A x = b$.\n\nRequired final output format: Your program should produce a single line of output containing a Python-style list of these five results in order, where each result is itself a Python-style list of two floats in scientific notation with exactly six digits after the decimal point. For example, an output with two hypothetical cases would look like\n\"[[1.234567e-03,9.876543e-04],[...],...]\"\nMake sure your program prints only this single line.", "solution": "The posed problem requires a comparative study of the Generalized Minimal Residual (GMRES) method on a nearly singular linear system $Ax=b$ versus a Tikhonov-regularized variant based on the normal equations. We are to proceed from first principles, deriving the core components of the methods and providing a theoretical explanation for their behavior.\n\n**The Generalized Minimal Residual (GMRES) Method**\n\nThe GMRES method is an iterative procedure designed to solve a general, non-singular linear system of equations $Ax = b$, where the matrix $A \\in \\mathbb{R}^{n \\times n}$ and the vectors $x, b \\in \\mathbb{R}^{n}$. Starting with an initial guess $x_0$, GMRES constructs a sequence of approximate solutions. At the $k$-th step, it finds an iterate $x_k$ that minimizes the Euclidean norm of the residual, $\\|r_k\\|_2 = \\|b - Ax_k\\|_2$. The search for this optimal $x_k$ is constrained to the affine Krylov subspace $x_0 + \\mathcal{K}_k(A, r_0)$, where $r_0 = b - Ax_0$ is the initial residual and\n$$\n\\mathcal{K}_k(A, r_0) = \\operatorname{span}\\{r_0, Ar_0, \\dots, A^{k-1} r_0\\}\n$$\nis the $k$-th Krylov subspace generated by $A$ and $r_0$. Any candidate solution can be expressed as $x_k = x_0 + z_k$, where $z_k \\in \\mathcal{K}_k(A, r_0)$. The minimization problem that GMRES solves is thus:\n$$\n\\min_{z_k \\in \\mathcal{K}_k(A, r_0)} \\|b - A(x_0 + z_k)\\|_2 = \\min_{z_k \\in \\mathcal{K}_k(A, r_0)} \\|r_0 - Az_k\\|_2.\n$$\n\nTo solve this problem, we first construct an orthonormal basis for $\\mathcal{K}_k(A, r_0)$ using the Arnoldi iteration. As specified, we use the Modified Gram-Schmidt (MGS) variant for enhanced numerical stability. We begin by normalizing the initial residual, $q_1 = r_0 / \\|r_0\\|_2$. The algorithm then iteratively generates a set of orthonormal vectors $\\{q_1, q_2, \\dots, q_k\\}$. The procedure for step $j$ (from $j=1$ to $k$) is:\n$1$. Compute the next Krylov vector candidate: $v_j = A q_j$.\n$2$. Orthogonalize $v_j$ against the existing basis vectors $\\{q_1, \\dots, q_j\\}$. For $i = 1, \\dots, j$, compute the projection coefficient $h_{i,j} = q_i^\\top v_j$ and subtract the projection: $v_j \\leftarrow v_j - h_{i,j} q_i$.\n$3$. Compute the norm of the resulting vector: $h_{j+1,j} = \\|v_j\\|_2$.\n$4$. If $h_{j+1,j} = 0$, the algorithm terminates due to a \"lucky breakdown,\" indicating that an exact solution has been found within the current subspace.\n$5$. Otherwise, normalize to get the next basis vector: $q_{j+1} = v_j / h_{j+1,j}$.\n\nAfter $k$ successful steps, this process yields the Arnoldi relation $A Q_k = Q_{k+1} \\tilde{H}_k$. Here, $Q_k = [q_1, \\dots, q_k] \\in \\mathbb{R}^{n \\times k}$ and $Q_{k+1} = [q_1, \\dots, q_{k+1}] \\in \\mathbb{R}^{n \\times (k+1)}$ are matrices with orthonormal columns, and $\\tilde{H}_k \\in \\mathbb{R}^{(k+1) \\times k}$ is the upper Hessenberg matrix containing the coefficients $h_{i,j}$.\n\nA vector $z_k \\in \\mathcal{K}_k(A, r_0)$ can be written as $z_k = Q_k y_k$ for a unique coefficient vector $y_k \\in \\mathbb{R}^k$. Substituting this into the minimization problem and using the Arnoldi relation, we get:\n$$\n\\min_{y_k \\in \\mathbb{R}^k} \\|r_0 - A Q_k y_k\\|_2 = \\min_{y_k \\in \\mathbb{R}^k} \\|\\|r_0\\|_2 q_1 - Q_{k+1} \\tilde{H}_k y_k\\|_2.\n$$\nLetting $\\beta = \\|r_0\\|_2$ and recognizing that $q_1 = Q_{k+1} e_1$ where $e_1 = [1, 0, \\dots, 0]^\\top \\in \\mathbb{R}^{k+1}$, the objective becomes $\\min_{y_k \\in \\mathbb{R}^k} \\|Q_{k+1} (\\beta e_1 - \\tilde{H}_k y_k)\\|_2$. Since $Q_{k+1}$ is an isometry, it preserves the Euclidean norm. This simplifies the original $n$-dimensional problem to a small, $(k+1) \\times k$ least-squares problem:\n$$\ny_k = \\arg\\min_{y \\in \\mathbb{R}^k} \\|\\beta e_1 - \\tilde{H}_k y\\|_2.\n$$\nThis system is solved for $y_k$, typically via QR factorization of $\\tilde{H}_k$. The final GMRES solution iterate is then $x_k = x_0 + Q_k y_k$. The norm of the residual, $R_{\\mathrm{GMRES}} = \\|b - A x_k\\|_2$, is equal to the minimum value from the small least-squares problem.\n\n**Polynomial Interpretation of GMRES**\n\nSince $x_k = x_0 + z_k$ with $z_k \\in \\mathcal{K}_k(A, r_0)$, we can express $z_k$ as a polynomial in $A$ applied to $r_0$: $z_k = \\psi_{k-1}(A) r_0$, where $\\psi_{k-1}$ is a polynomial of degree at most $k-1$. The residual $r_k$ can then be written as:\n$$\nr_k = r_0 - A z_k = r_0 - A \\psi_{k-1}(A) r_0 = (I - A \\psi_{k-1}(A)) r_0.\n$$\nDefining a new polynomial $p_k(z) = 1 - z \\psi_{k-1}(z)$, we see that $p_k$ has a degree of at most $k$ and satisfies the constraint $p_k(0) = 1$. The residual is $r_k = p_k(A) r_0$. The minimization performed by GMRES is thus equivalent to finding the polynomial $p_k$ from the set of all polynomials of degree at most $k$ with $p_k(0)=1$, that minimizes the norm $\\|p_k(A) r_0\\|_2$. For matrices with eigenvalues spread widely and near zero (i.e., ill-conditioned), it is difficult for a low-degree polynomial to be small across the spectrum while satisfying $p_k(0)=1$, leading to slow convergence.\n\n**Tikhonov Regularization via Normal Equations**\n\nFor ill-conditioned or nearly singular matrices, the direct solution of $Ax=b$ is highly sensitive to errors. Tikhonov regularization addresses this by solving a nearby, more stable problem:\n$$\n\\min_{x \\in \\mathbb{R}^n} \\left( \\|Ax - b\\|_2^2 + \\lambda \\|x\\|_2^2 \\right),\n$$\nwhere $\\lambda > 0$ is a regularization parameter controlling the trade-off between fidelity to the data and stability of the solution. The solution to this optimization problem is given by the linear system known as the regularized normal equations:\n$$\n(A^\\top A + \\lambda I) x = A^\\top b.\n$$\nWe define $B(\\lambda) = A^\\top A + \\lambda I$ and $c = A^\\top b$, transforming the problem into solving $B(\\lambda) x = c$. The matrix $B(\\lambda)$ is symmetric and, for $\\lambda > 0$, positive definite. If $A = U \\Sigma V^\\top$ is the Singular Value Decomposition of $A$ with singular values $\\sigma_i$, the eigenvalues of $A^\\top A$ are $\\sigma_i^2$. Consequently, the eigenvalues of $B(\\lambda)$ are $\\sigma_i^2 + \\lambda$. For a nearly singular $A$, its smallest singular value $\\sigma_n$ is close to $0$. The parameter $\\lambda$ effectively shifts the entire spectrum of $A^\\top A$ by $\\lambda$, moving the smallest eigenvalue $\\sigma_n^2+\\lambda$ away from zero. This dramatically improves the condition number of the system matrix, which becomes $\\kappa_2(B(\\lambda)) = (\\sigma_{\\max}^2 + \\lambda) / (\\sigma_{\\min}^2 + \\lambda)$.\n\nWe apply GMRES to this well-conditioned system $B(\\lambda)x = c$. With an initial guess $x_0 = 0$, the initial residual for this new system is $r_0^{\\mathrm{NE}} = c - B(\\lambda) x_0 = c$. GMRES finds an iterate $x_k^{(\\lambda)}$ in the Krylov subspace $\\mathcal{K}_k(B(\\lambda), c)$ that minimizes $\\|c - B(\\lambda) x_k^{(\\lambda)}\\|_2$. The residual for this process is $r_k^{\\mathrm{NE}} = q_k(B(\\lambda)) r_0^{\\mathrm{NE}}$, where $q_k$ is a polynomial of degree up to $k$ with $q_k(0)=1$. Since the spectrum of $B(\\lambda)$ is better clustered and bounded away from zero, GMRES converges rapidly. The resulting iterate $x_k^{(\\lambda)}$ approximates the solution to the regularized problem. To assess its quality with respect to the original problem, we must compute the original residual norm, $R_{\\mathrm{Tik}} = \\|b - A x_k^{(\\lambda)}\\|_2$.", "answer": "```python\nimport numpy as np\n\ndef arnoldi_mgs(A, v, k):\n    \"\"\"\n    Performs k steps of the Arnoldi iteration with Modified Gram-Schmidt.\n    \n    Args:\n        A (np.ndarray): The matrix (n x n).\n        v (np.ndarray): The starting vector (n,).\n        k (int): The number of iterations.\n        \n    Returns:\n        Q (np.ndarray): Orthonormal basis vectors (n x (m+1)).\n        H (np.ndarray): Upper Hessenberg matrix ((m+1) x m).\n        m (int): The actual number of iterations performed before breakdown.\n    \"\"\"\n    n = A.shape[0]\n    Q = np.zeros((n, k + 1), dtype=np.float64)\n    H = np.zeros((k + 1, k), dtype=np.float64)\n    \n    norm_v = np.linalg.norm(v)\n    if norm_v == 0:\n        return Q[:, :1], H[:1, :0], 0\n        \n    Q[:, 0] = v / norm_v\n    \n    for j in range(k):\n        w = A @ Q[:, j]\n        \n        # Modified Gram-Schmidt\n        for i in range(j + 1):\n            H[i, j] = np.dot(Q[:, i].T, w)\n            w = w - H[i, j] * Q[:, i]\n            \n        H[j + 1, j] = np.linalg.norm(w)\n        \n        # Breakdown condition\n        if H[j + 1, j] < 1e-12:\n            m = j + 1\n            return Q[:, :m+1], H[:m+1, :m], m\n            \n        Q[:, j + 1] = w / H[j + 1, j]\n        \n    return Q, H, k\n\ndef gmres_k_steps(A, b, k, x0):\n    \"\"\"\n    Runs up to k steps of GMRES.\n    \n    Args:\n        A (np.ndarray): The matrix.\n        b (np.ndarray): The right-hand side vector.\n        k (int): The maximum number of iterations.\n        x0 (np.ndarray): The initial guess.\n        \n    Returns:\n        xk (np.ndarray): The approximate solution.\n    \"\"\"\n    r0 = b - A @ x0\n    beta = np.linalg.norm(r0)\n    \n    if beta == 0:\n        return x0\n    \n    Q, H, actual_k = arnoldi_mgs(A, r0, k)\n    \n    if actual_k == 0:\n        return x0\n        \n    e1 = np.zeros(actual_k + 1)\n    e1[0] = 1.0\n    rhs = beta * e1\n    \n    y, _, _, _ = np.linalg.lstsq(H, rhs, rcond=None)\n    \n    Qk = Q[:, :actual_k]\n    xk = x0 + Qk @ y\n        \n    return xk\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print results.\n    \"\"\"\n    # Define problem constants\n    n = 12\n    seed_A = 42\n    seed_b = 7\n    x0 = np.zeros(n)\n\n    # Generate fixed U and V matrices\n    rng_A = np.random.default_rng(seed_A)\n    Z1 = rng_A.standard_normal((n, n))\n    U, _ = np.linalg.qr(Z1)\n    Z2 = rng_A.standard_normal((n, n))\n    V, _ = np.linalg.qr(Z2)\n\n    # Generate fixed b vector\n    rng_b = np.random.default_rng(seed_b)\n    b = rng_b.standard_normal(n)\n\n    test_cases = [\n        (1e-3, 0.0, 4),      # Case 1\n        (1e-3, 1e-4, 4),     # Case 2\n        (1e-8, 1e-6, 8),     # Case 3\n        (1e-8, 1e-2, 8),     # Case 4\n        (1e-2, 0.0, 2)       # Case 5\n    ]\n\n    results = []\n\n    for epsilon, lambda_reg, k in test_cases:\n        # Construct A(epsilon)\n        s = np.array([epsilon**(i / (n - 1)) for i in range(n)])\n        Sigma = np.diag(s)\n        A = U @ Sigma @ V.T\n\n        # --- GMRES on original system ---\n        xk_gmres = gmres_k_steps(A, b, k, x0)\n        R_gmres = np.linalg.norm(b - A @ xk_gmres)\n\n        # --- Tikhonov Regularization with GMRES ---\n        if lambda_reg == 0.0:\n            # Special case for Normal Equations without regularization\n            B = A.T @ A\n        else:\n            B = A.T @ A + lambda_reg * np.eye(n)\n        \n        c = A.T @ b\n        \n        xk_tik = gmres_k_steps(B, c, k, x0)\n        \n        R_tik = np.linalg.norm(b - A @ xk_tik)\n\n        results.append([R_gmres, R_tik])\n\n    # Format the output as specified\n    list_of_strings = [f\"[{res[0]:.6e},{res[1]:.6e}]\" for res in results]\n    output_str = f\"[{','.join(list_of_strings)}]\"\n    \n    print(output_str)\n\nsolve()\n```", "id": "3136913"}]}