{"hands_on_practices": [{"introduction": "Mastering a numerical method begins with building it from first principles. This first practice challenges you to implement the Biconjugate Gradient Stabilized (BiCGSTAB) method and investigate one of the most critical factors influencing its performance: the condition number of the system matrix [@problem_id:3102168]. By programmatically constructing matrices with a prescribed condition number, you will gain firsthand experience in how problem difficulty impacts convergence speed and reliability, a fundamental concept in computational science.", "problem": "You are asked to write a complete program that implements the Biconjugate Gradient Stabilized method (BiCGSTAB) from first principles and uses it to study the statistical convergence behavior on families of dense, random, nonsymmetric linear systems with controlled conditioning. The task is grounded in the following core definitions: a linear system $A x = b$, the iterative refinement of an approximate solution $x_k$ by minimizing a residual $r_k = b - A x_k$ over a Krylov subspace, and the notion of conditioning based on singular values. No pre-existing solver may be called; you must implement the core iteration yourself based on the definitions of Krylov subspace methods and residual minimization without relying on any specialized library routine for BiCGSTAB.\n\nGiven a square matrix $A \\in \\mathbb{R}^{n \\times n}$ and a right-hand side vector $b \\in \\mathbb{R}^n$, BiCGSTAB seeks an $x \\in \\mathbb{R}^n$ such that $A x = b$ by iteratively updating $x_k$ using only matrix-vector products with $A$ and inner products, starting from the initial guess $x_0 = 0$. The method uses a fixed auxiliary vector $\\hat{r}$ and constructs a bi-orthogonal sequence that stabilizes the short-recurrence Biconjugate Gradient steps. Convergence is assessed using the relative residual norm $\\|r_k\\|_2 / \\|b\\|_2 \\le \\varepsilon$.\n\nYou must generate random dense nonsymmetric test matrices $A$ with a prescribed condition number by using the singular value decomposition (SVD) as a construction tool: draw a standard normal matrix $G \\in \\mathbb{R}^{n \\times n}$, compute its singular value decomposition $G = U \\Sigma V^{\\top}$ with $U, V \\in \\mathbb{R}^{n \\times n}$ orthogonal, then replace $\\Sigma$ by a diagonal matrix whose diagonal contains a prescribed sequence of singular values $\\{ \\sigma_i \\}_{i=1}^n$ that are geometrically spaced between $\\sigma_{\\max} = 1$ and $\\sigma_{\\min} = 1/\\kappa$, where $\\kappa \\ge 1$ is the desired condition number. Define $A := U \\operatorname{diag}(\\sigma_1,\\dots,\\sigma_n) V^{\\top}$. This $A$ is typically nonsymmetric when $U \\ne V$. For each realization, draw $b$ with independent standard normal entries and scale it to unit two-norm, i.e., enforce $\\|b\\|_2 = 1$.\n\nYour program must:\n- Implement the Biconjugate Gradient Stabilized method (BiCGSTAB) to solve $A x = b$ with initial guess $x_0 = 0$, no preconditioner, and stopping criterion based on the relative residual norm $\\|r_k\\|_2 / \\|b\\|_2 \\le \\varepsilon$ or reaching a maximum number of iterations $k_{\\max}$. If a division by zero would occur in the iteration (for example, due to a vanishing denominator in a step of the recurrence), treat it as a breakdown and mark that run as a failure to converge within $k_{\\max}$ iterations.\n- For each condition number $\\kappa$ and over multiple independent realizations, record for each run: the number $k$ of iterations taken to reach the tolerance if successful, or $k_{\\max}$ if unsuccessful; and a Boolean success flag defined by whether $\\|r_k\\|_2 / \\|b\\|_2 \\le \\varepsilon$ before or at iteration $k_{\\max}$.\n- Aggregate, for each $\\kappa$, the following statistics across realizations: the mean number of iterations among successful runs, the standard deviation of iterations among successful runs, and the success rate defined as the fraction of realizations that met the tolerance within $k_{\\max}$ iterations. If there are zero successful runs for a given $\\kappa$, define the mean and standard deviation to be $k_{\\max}$ and $0$, respectively.\n\nUse the following fixed test suite to ensure reproducibility and coverage:\n- Matrix dimension: $n = 60$.\n- Condition numbers: $\\kappa \\in \\{ 10^{1}, 10^{5}, 10^{12} \\}$.\n- Number of independent realizations per $\\kappa$: $8$.\n- Tolerance: $\\varepsilon = 10^{-8}$.\n- Maximum iterations: $k_{\\max} = 300$.\n- Random seed: initialize a pseudorandom number generator with the fixed seed $12345$ and draw all random matrices and vectors from this generator to ensure repeatability.\n\nYour program must output a single line containing a comma-separated list of $9$ floating-point numbers rounded to exactly $6$ decimal places, in the following order:\n- For $\\kappa = 10^{1}$: mean iterations among successes, standard deviation of iterations among successes, success rate as a decimal in $[0,1]$.\n- For $\\kappa = 10^{5}$: mean iterations among successes, standard deviation of iterations among successes, success rate as a decimal in $[0,1]$.\n- For $\\kappa = 10^{12}$: mean iterations among successes, standard deviation of iterations among successes, success rate as a decimal in $[0,1]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, in the schematic form [$r_1$,$r_2$,$r_3$,$r_4$,$r_5$,$r_6$,$r_7$,$r_8$,$r_9$], where each $r_i$ is a real number formatted with exactly $6$ digits after the decimal point. No other text should be printed.", "solution": "The problem is valid as it presents a well-defined task in computational science that is scientifically grounded, self-contained, and objective. It requires the implementation of the Biconjugate Gradient Stabilized (BiCGSTAB) method and its application to a set of programmatically generated linear systems with controlled conditioning, followed by a statistical analysis of the performance. All parameters, procedures, and output formats are specified unambiguously, and the use of a fixed random seed ensures reproducibility. I will proceed with a solution.\n\nThe solution is structured into three main parts:\n1.  A detailed description of the BiCGSTAB algorithm to be implemented.\n2.  The procedure for constructing test matrices $A$ with a prescribed condition number $\\kappa$.\n3.  The design of the numerical experiment to gather and process the convergence statistics.\n\n### 1. The Biconjugate Gradient Stabilized (BiCGSTAB) Algorithm\n\nBiCGSTAB is an iterative method for solving nonsymmetric linear systems of the form $A x = b$, where $A \\in \\mathbb{R}^{n \\times n}$ and $x, b \\in \\mathbb{R}^n$. It is a Krylov subspace method that aims to smooth the often erratic convergence of the Biconjugate Gradient (BiCG) method. The implementation will follow the standard algorithm, starting with an initial guess $x_0 = 0$.\n\nThe algorithm proceeds as follows:\n\n**Initialization:**\n1.  Set the initial guess $x_0 = 0$.\n2.  Compute the initial residual $r_0 = b - A x_0 = b$.\n3.  Choose the auxiliary residual vector $\\hat{r}_0 = r_0$.\n4.  Initialize scalars: $\\rho_0 = 1$, $\\alpha_0 = 1$, $\\omega_0 = 1$.\n5.  Initialize vectors: $p_0 = 0$, $v_0 = 0$.\n6.  Set the iteration counter $k = 1$.\n7.  Pre-compute the norm of the right-hand side, $\\|b\\|_2$. As the problem specifies that $b$ is normalized, $\\|b\\|_2 = 1$.\n8.  The convergence criterion is $\\|r_k\\|_2 \\le \\varepsilon$.\n\n**Main Iteration Loop (for $k = 1, 2, \\dots, k_{\\max}$):**\n1.  Compute $\\rho_k = \\hat{r}_0^\\top r_{k-1}$. A breakdown occurs if $\\rho_k \\approx 0$.\n2.  Calculate $\\beta_{k-1} = \\frac{\\rho_k}{\\rho_{k-1}} \\frac{\\alpha_{k-1}}{\\omega_{k-1}}$. A breakdown occurs if $\\rho_{k-1} \\approx 0$ or $\\omega_{k-1} \\approx 0$.\n3.  Update the search direction: $p_k = r_{k-1} + \\beta_{k-1} (p_{k-1} - \\omega_{k-1} v_{k-1})$.\n4.  Compute the matrix-vector product $v_k = A p_k$.\n5.  Compute the denominator for $\\alpha_k$: $d_\\alpha = \\hat{r}_0^\\top v_k$. A breakdown occurs if $d_\\alpha \\approx 0$.\n6.  Calculate the step length $\\alpha_k = \\rho_k / d_\\alpha$.\n7.  Compute an intermediate residual $s_k = r_{k-1} - \\alpha_k v_k$.\n8.  Compute the matrix-vector product $t_k = A s_k$.\n9.  Compute the denominator for $\\omega_k$: $d_\\omega = t_k^\\top t_k$. A breakdown or stagnation occurs if $d_\\omega \\approx 0$.\n10. Calculate the stabilization step length $\\omega_k = (t_k^\\top s_k) / d_\\omega$.\n11. Update the solution: $x_k = x_{k-1} + \\alpha_k p_k + \\omega_k s_k$.\n12. Update the residual: $r_k = s_k - \\omega_k t_k$.\n13. Check for convergence: If $\\|r_k\\|_2 \\le \\varepsilon$, the method has succeeded. The current iteration count $k$ is recorded.\n14. The variables for the next iteration are $x_k, r_k, p_k, v_k$, and the scalars are updated with $\\rho_k, \\alpha_k, \\omega_k$.\n\nIf the loop completes without convergence, the run is considered a failure, and the number of iterations is recorded as $k_{\\max}$. Any division by a near-zero value (a breakdown) is also treated as a failure.\n\n### 2. Test Matrix Construction\n\nTo study the algorithm's performance as a function of the problem's conditioning, we need to generate matrices with a specific condition number $\\kappa = \\sigma_{\\max} / \\sigma_{\\min}$. The singular value decomposition (SVD) provides a tool for this construction.\n\nThe procedure for a given dimension $n$ and target condition number $\\kappa$ is:\n1.  Initialize a pseudorandom number generator with the specified seed, $12345$.\n2.  Generate a random matrix $G \\in \\mathbb{R}^{n \\times n}$ where each entry is drawn from the standard normal distribution $\\mathcal{N}(0, 1)$.\n3.  Compute the SVD of $G$: $G = U \\Sigma' V^{\\top}$, where $U, V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices and $\\Sigma'$ is a diagonal matrix of singular values of $G$.\n4.  Define a new set of singular values $\\{\\sigma_i\\}_{i=1}^n$ that are geometrically spaced between $\\sigma_{\\max} = 1$ and $\\sigma_{\\min} = 1/\\kappa$. The values are sorted in descending order and given by the formula:\n    $$ \\sigma_i = \\kappa^{-\\frac{i-1}{n-1}} \\quad \\text{for } i = 1, \\dots, n $$\n    This ensures $\\sigma_1 = 1$ and $\\sigma_n = 1/\\kappa$, so the condition number of the resulting matrix is exactly $\\kappa$.\n5.  Construct the new matrix $A$ by reassembling the SVD components with the desired singular values:\n    $$ A = U \\operatorname{diag}(\\sigma_1, \\sigma_2, \\dots, \\sigma_n) V^{\\top} $$\n    Since $U$ and $V$ are generally different for a random nonsymmetric matrix $G$, the resulting matrix $A$ will also be nonsymmetric.\n\n### 3. Experimental Design and Statistical Analysis\n\nThe core of the task is to run a series of numerical experiments and aggregate the results. The fixed parameters are: matrix dimension $n=60$, tolerance $\\varepsilon = 10^{-8}$, maximum iterations $k_{\\max} = 300$, and number of realizations per $\\kappa$ is $8$.\n\nThe overall process is as follows:\n1.  Initialize the random number generator with seed $12345$.\n2.  Loop through each specified condition number $\\kappa \\in \\{ 10^{1}, 10^{5}, 10^{12} \\}$.\n3.  For each $\\kappa$, start a loop for $8$ independent realizations.\n4.  Inside this loop:\n    a. Construct a matrix $A$ of size $60 \\times 60$ with condition number $\\kappa$ using the SVD method described above.\n    b. Generate a right-hand side vector $b$ by drawing $60$ entries from $\\mathcal{N}(0, 1)$ and then normalizing the vector to have unit L2-norm, $\\|b\\|_2 = 1$.\n    c. Solve the system $A x = b$ using the implemented BiCGSTAB solver with $x_0 = 0$, $\\varepsilon = 10^{-8}$, and $k_{\\max} = 300$.\n    d. Record the number of iterations taken and a boolean flag indicating whether convergence was achieved.\n5.  After the $8$ realizations for a given $\\kappa$ are complete:\n    a. Filter the results to get a list of iteration counts for only the successful runs.\n    b. Calculate the success rate as (number of successful runs) / $8$.\n    c. If there are one or more successful runs, calculate the mean and population standard deviation of the iteration counts.\n    d. If there are zero successful runs, the mean is defined as $k_{\\max}=300$ and the standard deviation as $0$.\n6.  Collect the three statistics (mean iterations, standard deviation, success rate) for each $\\kappa$.\n7.  Finally, format these $9$ numbers ($3$ statistics for $3$ values of $\\kappa$) into a single comma-separated string, with each number rounded to $6$ decimal places.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef bicgstab(A, b, x0, tol, max_iter):\n    \"\"\"\n    Implements the Biconjugate Gradient Stabilized (BiCGSTAB) method.\n\n    Args:\n        A (np.ndarray): The square matrix of the linear system.\n        b (np.ndarray): The right-hand side vector.\n        x0 (np.ndarray): The initial guess for the solution.\n        tol (float): The convergence tolerance for the relative residual norm.\n        max_iter (int): The maximum number of iterations.\n\n    Returns:\n        tuple: A tuple containing:\n            - k (int): The number of iterations performed.\n            - success (bool): True if converged, False otherwise.\n    \"\"\"\n    n = A.shape[0]\n    x = x0.copy()\n    r = b - A @ x\n    r_hat = r.copy()\n\n    # Per the problem description, ||b||_2 is 1, so the relative residual\n    # check ||r||/||b|| <= tol simplifies to ||r|| <= tol.\n    res_norm = np.linalg.norm(r)\n    if res_norm <= tol:\n        return 0, True\n\n    # Initialize algorithm parameters for k=0 state\n    rho_prev = 1.0\n    alpha = 1.0\n    omega = 1.0\n    v = np.zeros(n)\n    p = np.zeros(n)\n\n    # A very small number to check for breakdown (division by zero)\n    breakdown_tol = 1e-50\n\n    for k in range(1, max_iter + 1):\n        rho = r_hat.T @ r\n\n        if abs(rho) < breakdown_tol:\n            return max_iter, False  # Breakdown\n\n        if k > 1:\n            if abs(rho_prev) < breakdown_tol or abs(omega) < breakdown_tol:\n                return max_iter, False # Breakdown\n            beta = (rho / rho_prev) * (alpha / omega)\n            p = r + beta * (p - omega * v)\n        else: # for k = 1\n            p = r\n\n        v = A @ p\n\n        r_hat_dot_v = r_hat.T @ v\n        if abs(r_hat_dot_v) < breakdown_tol:\n            return max_iter, False  # Breakdown\n\n        alpha = rho / r_hat_dot_v\n        \n        s = r - alpha * v\n\n        t = A @ s\n\n        t_dot_t = t.T @ t\n        if abs(t_dot_t) < breakdown_tol:\n            return max_iter, False # Stagnation or breakdown\n\n        omega = (t.T @ s) / t_dot_t\n\n        x += alpha * p + omega * s\n        r = s - omega * t\n\n        rho_prev = rho\n\n        res_norm = np.linalg.norm(r)\n        if res_norm <= tol:\n            return k, True\n\n    return max_iter, False\n\n\ndef generate_matrix(n, kappa, rng):\n    \"\"\"\n    Generates a random dense nonsymmetric matrix with a specified condition number.\n\n    Args:\n        n (int): The dimension of the square matrix.\n        kappa (float): The desired condition number.\n        rng (np.random.Generator): The random number generator instance.\n\n    Returns:\n        np.ndarray: The generated n x n matrix.\n    \"\"\"\n    # Generate a random matrix from a standard normal distribution\n    G = rng.standard_normal((n, n))\n\n    # Compute the SVD\n    U, _, Vt = np.linalg.svd(G)\n    V = Vt.T\n\n    # Create a geometrically spaced sequence of singular values\n    # from sigma_max = 1 to sigma_min = 1/kappa\n    singular_values = np.logspace(0, -np.log10(kappa), n)\n    \n    # Construct the final matrix A = U * diag(singular_values) * V^T\n    A = U @ np.diag(singular_values) @ Vt\n\n    return A\n\n\ndef solve():\n    \"\"\"\n    Main function to run the experiment and produce the final output.\n    \"\"\"\n    # Fixed parameters from the problem statement\n    n = 60\n    kappas = [1e1, 1e5, 1e12]\n    num_realizations = 8\n    tol = 1e-8\n    max_iter = 300\n    seed = 12345\n\n    rng = np.random.default_rng(seed)\n\n    all_stats = []\n\n    for kappa in kappas:\n        iterations_list = []\n        success_list = []\n\n        for _ in range(num_realizations):\n            # Generate test matrix and vector\n            A = generate_matrix(n, kappa, rng)\n            b_unscaled = rng.standard_normal(n)\n            b = b_unscaled / np.linalg.norm(b_unscaled) # Enforce ||b||_2 = 1\n\n            x0 = np.zeros(n)\n\n            # Run BiCGSTAB\n            k, success = bicgstab(A, b, x0, tol, max_iter)\n\n            iterations_list.append(k)\n            success_list.append(success)\n\n        # Aggregate statistics for the current kappa\n        successful_iters = [k for k, s in zip(iterations_list, success_list) if s]\n        num_success = len(successful_iters)\n        success_rate = num_success / num_realizations\n\n        if num_success > 0:\n            mean_iters = np.mean(successful_iters)\n            # Use population standard deviation (ddof=0 is the default in np.std)\n            std_iters = np.std(successful_iters)\n        else:\n            # As per problem spec for zero successful runs\n            mean_iters = float(max_iter)\n            std_iters = 0.0\n\n        all_stats.extend([mean_iters, std_iters, success_rate])\n\n    # Format the final output string exactly as required\n    print(f\"[{','.join(f'{x:.6f}' for x in all_stats)}]\")\n\nsolve()\n```", "id": "3102168"}, {"introduction": "Now that you have a working implementation, let's explore a fascinating corner case that reveals the deep connection between iterative methods and linear algebra. This exercise guides you to construct specific systems that trigger a \"lucky breakdown,\" where BiCGSTAB finds the exact solution in a single step [@problem_id:3102131]. Understanding this phenomenon illuminates the algebraic conditions under which the search directions perfectly align to resolve the residual, providing a deeper intuition for how Krylov subspace methods operate.", "problem": "You are asked to implement a complete, runnable program that constructs a small test harness to demonstrate a specific phenomenon in iterative Krylov subspace methods, namely a “lucky breakdown” in the Biconjugate Gradient Stabilized method (BiCGSTAB). Work in exact arithmetic as modeled by double-precision floating point, and treat vectors and matrices as arrays over the real numbers.\n\nYour program must do the following.\n\n1. Core definitions to be used as the fundamental base:\n   - Let a linear system be given by $A x = b$, where $A \\in \\mathbb{R}^{n \\times n}$, $x \\in \\mathbb{R}^{n}$, and $b \\in \\mathbb{R}^{n}$.\n   - Given an initial guess $x_0 \\in \\mathbb{R}^{n}$, define the initial residual $r_0 = b - A x_0$.\n   - Use the standard Euclidean inner product $(u, v) = \\sum_{i=1}^{n} u_i v_i$ and the induced $2$-norm $\\lVert u \\rVert_2 = \\sqrt{(u,u)}$.\n   - Implement the Biconjugate Gradient Stabilized method (BiCGSTAB), using $r_0$ also as the fixed shadow residual. Use a standard stopping criterion based on the residual norm falling below a tolerance.\n\n2. Lucky breakdown event to detect:\n   - In each BiCGSTAB iteration, after computing the scalar step size $\\alpha_k$ and the intermediate vector $s_k = r_{k-1} - \\alpha_k A p_{k-1}$, a “lucky breakdown” is said to occur if $s_k = 0$ (in floating point, treat $\\lVert s_k \\rVert_2 \\le \\varepsilon$ as zero) before computing the next stabilization step. In this case, the exact solution has been found early within that iteration via $x_k = x_{k-1} + \\alpha_k p_{k-1}$.\n   - Your program must declare a lucky breakdown only in this sense. Do not count the trivial case $r_0 = 0$ as lucky; and do not count convergence that occurs only after completing the $\\omega_k$ stabilization step as lucky.\n\n3. Numerical tolerances:\n   - Use a convergence and zero-detection tolerance of $\\varepsilon = 10^{-12}$ for checking $\\lVert s_k \\rVert_2$ and $\\lVert r_k \\rVert_2$.\n   - Use a maximum iteration cap of $k_{\\max} = 1000$ per test case to avoid infinite loops in pathological inputs.\n\n4. Test suite design and construction:\n   - Construct four test cases by explicitly specifying $A$, $b$, and $x_0$ so that the harness covers the following behaviors. All entries and sizes must be stated explicitly, and all matrices must be nonsingular.\n     - Case L1 (symmetric lucky breakdown): Let $A \\in \\mathbb{R}^{3 \\times 3}$ be diagonal with entries $A = \\mathrm{diag}(2, 3, 4)$, let $x_0 = (0, 0, 0)^{\\top}$, and let $b = (1, 0, 0)^{\\top}$. This configuration should be constructed so that $r_0$ is an eigenvector of $A$, which algebraically enables early exact solution via $s_0 = 0$.\n     - Case L2 (nonsymmetric lucky breakdown): Let $A \\in \\mathbb{R}^{2 \\times 2}$ be the upper triangular Jordan-like block $A = \\begin{bmatrix} 5 & 1 \\\\ 0 & 5 \\end{bmatrix}$, let $x_0 = (0, 0)^{\\top}$, and let $b = (1, 0)^{\\top}$. This configuration should again ensure that $r_0$ is an eigenvector of $A$, so the same algebraic mechanism can trigger $s_0 = 0$ even though $A$ is not symmetric.\n     - Case B1 (boundary, no-lucky): Let $A \\in \\mathbb{R}^{2 \\times 2}$ be diagonal $A = \\mathrm{diag}(7, 8)$, choose $x_{\\star} = (1, 2)^{\\top}$, set $b = A x_{\\star}$, and set the initial guess equal to the exact solution $x_0 = x_{\\star}$. This yields $r_0 = 0$. This case must not be counted as lucky because no iteration is needed.\n     - Case G1 (general, no-lucky): Let $A \\in \\mathbb{R}^{2 \\times 2}$ be diagonal $A = \\mathrm{diag}(2, 3)$, let $x_0 = (0, 0)^{\\top}$, and let $b = (1, 1)^{\\top}$. This setup should produce a normal iteration sequence in which $s_0 \\ne 0$ and convergence, if reached, occurs after the stabilization step or subsequent iterations.\n\n5. Output specification:\n   - For each test case, your program must output the integer $1$ if a lucky breakdown (as defined above) occurred at some iteration’s $\\alpha$-step, and the integer $0$ otherwise (including the trivial $r_0 = 0$ case and any convergence after the $\\omega$-step).\n   - Aggregate the four results into a single line printed as a comma-separated list enclosed in square brackets, with no spaces, in the order $[\\text{L1},\\text{L2},\\text{B1},\\text{G1}]$. For example, a valid output might look like $[1,1,0,0]$.\n\n6. Angle units, physical units, and percentages:\n   - No physical quantities, angles, or percentages are involved in this task.\n\nYour program must be fully self-contained, require no user input, and must obey the exact output format described above.", "solution": "The problem requires the implementation of the Biconjugate Gradient Stabilized (BiCGSTAB) iterative method to solve a linear system $A x = b$ and to construct a test suite to demonstrate a specific phenomenon known as a \"lucky breakdown.\"\n\nA linear system is given by $A x = b$, where $A \\in \\mathbb{R}^{n \\times n}$ is a nonsingular matrix, and $x, b \\in \\mathbb{R}^{n}$ are vectors. Given an initial guess $x_0$, the initial residual is $r_0 = b - A x_0$. The BiCGSTAB method iteratively refines the solution guess $x_k$ to minimize the residual $r_k = b - A x_k$.\n\nThe BiCGSTAB algorithm, using a fixed shadow residual $\\hat{r}_0$ (set to $r_0$ as per the problem), can be formulated as follows:\n\n1.  Initialize:\n    $x_0$ is the initial guess.\n    $r_0 = b - A x_0$.\n    If $\\lVert r_0 \\rVert_2$ is close to zero, the initial guess is already the solution.\n    Set the fixed shadow residual $\\hat{r}_0 = r_0$.\n    Set the initial search direction $p_0 = r_0$.\n    Compute the initial squared norm of the residual projection $\\rho_0 = (\\hat{r}_0, r_0)$.\n\n2.  Iterate for $k = 1, 2, \\dots$ until convergence or maximum iterations:\n    a. Compute the matrix-vector product with the search direction: $v_{k-1} = A p_{k-1}$.\n    b. Compute the step size $\\alpha_k$:\n       $$ \\alpha_k = \\frac{\\rho_{k-1}}{(\\hat{r}_0, v_{k-1})} = \\frac{(\\hat{r}_0, r_{k-1})}{(\\hat{r}_0, A p_{k-1})} $$\n    c. Update the solution along the search direction $p_{k-1}$ and compute an intermediate residual $s_k$:\n       $$ s_k = r_{k-1} - \\alpha_k v_{k-1} = r_{k-1} - \\alpha_k A p_{k-1} $$\n    d. **Lucky Breakdown Check**: If $\\lVert s_k \\rVert_2 \\le \\varepsilon$ (where $\\varepsilon=10^{-12}$ is a small tolerance), a \"lucky breakdown\" occurs. The exact solution has been found prematurely. The final solution is $x_k = x_{k-1} + \\alpha_k p_{k-1}$. We report this event and terminate.\n    e. If no lucky breakdown, proceed with the stabilization step. Compute the matrix-vector product $t_k = A s_k$.\n    f. Compute the stabilization parameter $\\omega_k$:\n       $$ \\omega_k = \\frac{(t_k, s_k)}{(t_k, t_k)} $$\n    g. Perform the full update for the solution and the final residual for this iteration:\n       $$ x_k = x_{k-1} + \\alpha_k p_{k-1} + \\omega_k s_k $$\n       $$ r_k = s_k - \\omega_k t_k $$\n    h. Check for normal convergence: if $\\lVert r_k \\rVert_2 \\le \\varepsilon$, terminate.\n    i. Prepare for the next iteration. Update the projection norm and the search direction:\n       $$ \\rho_k = (\\hat{r}_0, r_k) $$\n       $$ \\beta_k = \\frac{\\rho_k}{\\rho_{k-1}} \\frac{\\alpha_k}{\\omega_k} $$\n       $$ p_k = r_k + \\beta_k (p_{k-1} - \\omega_k v_{k-1}) $$\n\nThe core of the problem is to detect the lucky breakdown. This happens if $s_k=0$. Let's analyze this condition for the first iteration ($k=1$). The initial search direction is $p_0 = r_0$. The condition $s_1 = 0$ implies:\n$$ s_1 = r_0 - \\alpha_1 A p_0 = r_0 - \\alpha_1 A r_0 = 0 $$\n$$ \\implies A r_0 = \\frac{1}{\\alpha_1} r_0 $$\nThis equation shows that a lucky breakdown occurs in the first iteration if and only if the initial residual $r_0$ is an eigenvector of the matrix $A$. The corresponding step length $\\alpha_1$ will be the reciprocal of the eigenvalue. The test cases are designed to exploit this property.\n\n- **Case L1 (symmetric lucky breakdown)**: $A = \\mathrm{diag}(2, 3, 4)$, $x_0 = (0, 0, 0)^{\\top}$, $b = (1, 0, 0)^{\\top}$.\n  The initial residual is $r_0 = b - A x_0 = (1, 0, 0)^{\\top}$. This is an eigenvector of the diagonal matrix $A$ with eigenvalue $\\lambda=2$. Thus, a lucky breakdown is expected, and the algorithm should report $1$.\n\n- **Case L2 (nonsymmetric lucky breakdown)**: $A = \\begin{bmatrix} 5 & 1 \\\\ 0 & 5 \\end{bmatrix}$, $x_0 = (0, 0)^{\\top}$, $b = (1, 0)^{\\top}$.\n  The initial residual is $r_0 = b - A x_0 = (1, 0)^{\\top}$. We check if it is an eigenvector: $A r_0 = \\begin{pmatrix} 5 \\\\ 0 \\end{pmatrix} = 5 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 5 r_0$. It is an eigenvector with eigenvalue $\\lambda=5$. Therefore, a lucky breakdown is also expected here, and the result is $1$.\n\n- **Case B1 (boundary, no-lucky)**: $A = \\mathrm{diag}(7, 8)$, $x_0 = x_{\\star} = (1, 2)^{\\top}$, $b = A x_{\\star}$.\n  The initial residual is $r_0 = b - A x_0 = A x_{\\star} - A x_{\\star} = 0$. The check $\\lVert r_0 \\rVert_2 \\le \\varepsilon$ is met before any iterations begin. According to the problem statement, this trivial case is not considered a lucky breakdown. The program should report $0$.\n\n- **Case G1 (general, no-lucky)**: $A = \\mathrm{diag}(2, 3)$, $x_0 = (0, 0)^{\\top}$, $b = (1, 1)^{\\top}$.\n  The initial residual is $r_0 = b - A x_0 = (1, 1)^{\\top}$. We check if it is an eigenvector: $A r_0 = \\mathrm{diag}(2, 3) (1, 1)^{\\top} = (2, 3)^{\\top}$. Since $(2, 3)^{\\top}$ is not a scalar multiple of $(1, 1)^{\\top}$, $r_0$ is not an eigenvector of $A$. Therefore, $s_1 \\ne 0$ and no lucky breakdown will occur in the first iteration. The algorithm will proceed normally. The expected output is $0$.\n\nThe provided program implements this logic, correctly instantiates each test case, runs the BiCGSTAB algorithm, and reports a $1$ or $0$ based on the precise definition of a lucky breakdown.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef bicgstab(A, b, x0, tol, max_iter):\n    \"\"\"\n    Implements the Biconjugate Gradient Stabilized (BiCGSTAB) method and detects \"lucky breakdowns\".\n\n    A \"lucky breakdown\" is detected if the intermediate residual 's' becomes zero\n    before the stabilization step.\n\n    Args:\n        A (np.ndarray): The matrix of the linear system.\n        b (np.ndarray): The right-hand side vector of the linear system.\n        x0 (np.ndarray): The initial guess for the solution.\n        tol (float): The tolerance for convergence and zero-detection.\n        max_iter (int): The maximum number of iterations.\n\n    Returns:\n        int: Returns 1 if a lucky breakdown occurred, 0 otherwise.\n    \"\"\"\n    x = np.copy(x0)\n    r = b - A @ x\n\n    # Per problem: do not count trivial convergence as a lucky breakdown.\n    if np.linalg.norm(r) < tol:\n        return 0\n\n    # Fixed shadow residual as per problem specification.\n    r_hat = np.copy(r)\n\n    rho = r_hat @ r\n    # Handle potential breakdown if rho is zero initially.\n    if abs(rho) < 1e-50:\n        return 0 # Breakdown, not lucky.\n        \n    p = np.copy(r)\n    \n    is_lucky = 0\n\n    for _ in range(max_iter):\n        v = A @ p\n        \n        denom = r_hat @ v\n        # Handle potential breakdown.\n        if abs(denom) < 1e-50:\n            return 0 # Breakdown, not lucky.\n\n        alpha = rho / denom\n        \n        s = r - alpha * v\n\n        # The specific \"lucky breakdown\" check as defined in the problem.\n        if np.linalg.norm(s) < tol:\n            x = x + alpha * p\n            is_lucky = 1\n            break\n\n        t = A @ s\n        \n        denom = t @ t\n        # If s is not zero, but As is, it's a breakdown. A must be singular, but problem guarantees nonsingular A.\n        # This check is for numerical stability with floating point arithmetic.\n        if abs(denom) < 1e-50:\n            return 0 # Breakdown, not lucky.\n        \n        omega = (t @ s) / denom\n        \n        # Full update for x and r.\n        x = x + alpha * p + omega * s\n        r_new = s - omega * t\n\n        # Normal convergence check after the full stabilization step.\n        if np.linalg.norm(r_new) < tol:\n            break\n\n        rho_new = r_hat @ r_new\n\n        # Handle potential breakdown from rho or omega being zero.\n        if abs(rho) < 1e-50 or abs(omega) < 1e-50:\n            return 0 # Breakdown, not lucky.\n\n        beta = (rho_new / rho) * (alpha / omega)\n\n        # Update search direction p.\n        p = r_new + beta * (p - omega * v)\n\n        # Update r and rho for the next iteration.\n        r = r_new\n        rho = rho_new\n\n    return is_lucky\n\ndef solve():\n    # Define the parameters and test cases from the problem statement.\n    tol = 1e-12\n    max_iter = 1000\n\n    # Case L1 (symmetric lucky breakdown)\n    A1 = np.diag([2.0, 3.0, 4.0])\n    x0_1 = np.zeros(3)\n    b1 = np.array([1.0, 0.0, 0.0])\n\n    # Case L2 (nonsymmetric lucky breakdown)\n    A2 = np.array([[5.0, 1.0], [0.0, 5.0]])\n    x0_2 = np.zeros(2)\n    b2 = np.array([1.0, 0.0])\n\n    # Case B1 (boundary, no-lucky)\n    A3 = np.diag([7.0, 8.0])\n    x_star3 = np.array([1.0, 2.0])\n    b3 = A3 @ x_star3\n    x0_3 = np.copy(x_star3)\n\n    # Case G1 (general, no-lucky)\n    A4 = np.diag([2.0, 3.0])\n    x0_4 = np.zeros(2)\n    b4 = np.array([1.0, 1.0])\n\n    test_cases = [\n        (A1, b1, x0_1),  # L1\n        (A2, b2, x0_2),  # L2\n        (A3, b3, x0_3),  # B1\n        (A4, b4, x0_4),  # G1\n    ]\n\n    results = []\n    for A, b, x0 in test_cases:\n        is_lucky = bicgstab(A, b, x0, tol, max_iter)\n        results.append(is_lucky)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3102131"}, {"introduction": "Real-world problems are often too large or ill-conditioned for a basic solver to be efficient, which is where preconditioning becomes essential. This final practice moves into this crucial area by transforming the system to make it easier to solve, using an Incomplete LU (ILU) factorization as a preconditioner. You will investigate the stability of BiCGSTAB when this preconditioner is applied *inexactly*, a sophisticated trade-off between computational cost and convergence rate that is key in high-performance scientific computing [@problem_id:3102124].", "problem": "You are asked to investigate the stability of the Biconjugate Gradient Stabilized method (BiCGSTAB) under inexact preconditioning arising from approximate triangular solves in an incomplete lower-upper factorization (ILU). The goal is to quantify how much inexactness in the inner triangular solves can be tolerated without causing divergence of the outer BiCGSTAB iterations. Consider the linear system $A x = b,$ where $A$ is a nonsymmetric, sparse matrix obtained from a finite-difference discretization of a two-dimensional steady convection-diffusion operator on a square grid with homogeneous Dirichlet boundary conditions. Use the following construction:\n\n- Let the grid have $N \\times N$ interior points with $N = 15$, grid spacing $h = 1/(N+1)$, diffusion coefficient $\\kappa = 1$, and constant convection velocities $v_x = 1$ and $v_y = 1$.\n- Discretize the operator $-\\kappa \\Delta u + v_x \\frac{\\partial u}{\\partial x} + v_y \\frac{\\partial u}{\\partial y}$ using the standard five-point Laplacian and centered finite differences for the first derivatives. For an interior grid point $(i,j)$ mapped to a single index $k$, set\n  $$A_{k,k} = 4 \\kappa,$$\n  $$A_{k,k-N} = -\\kappa - \\frac{v_x}{2h} \\quad \\text{if } i > 0,$$\n  $$A_{k,k+N} = -\\kappa + \\frac{v_x}{2h} \\quad \\text{if } i  N-1,$$\n  $$A_{k,k-1} = -\\kappa - \\frac{v_y}{2h} \\quad \\text{if } j > 0,$$\n  $$A_{k,k+1} = -\\kappa + \\frac{v_y}{2h} \\quad \\text{if } j  N-1.$$\n  All other entries are zero. Let $b$ be the vector of all ones of length $N^2$.\n- Form an incomplete lower-upper factorization (ILU) of $A$ using a practical drop tolerance (for the program, an ILU routine is employed that produces sparse triangular factors $L$ and $U$ along with permutation vectors to stabilize the factorization).\n\nDefine inexact preconditioning by replacing the exact inner triangular solves $L y = \\tilde{b}$ and $U z = y$ with approximate solutions computed by a damped Jacobi iteration that terminates when the inner residual $\\|r\\|_2 = \\|\\tilde{b} - T x\\|_2$ first satisfies $\\|r\\|_2 \\le \\tau \\|\\tilde{b}\\|_2,$ where $T$ is either $L$ or $U$ and $\\tau \\ge 0$ is a user-specified inner tolerance controlling the inexactness. When $\\tau = 0$, use exact sparse triangular solves.\n\nImplement the right-preconditioned BiCGSTAB algorithm to solve $$A M^{-1} y = b,$$ where $M \\approx A$ is the ILU preconditioner, and recover $x = M^{-1} y$. Determine, for each given $\\tau$, whether BiCGSTAB converges to the target relative residual tolerance $\\varepsilon = 10^{-8}$ within a maximum of $500$ outer iterations without breakdown (no zero denominators and no non-numeric values). Convergence is defined by $\\frac{\\|b - A x\\|_2}{\\|b\\|_2} \\le \\varepsilon.$\n\nTest Suite:\nEvaluate the program on the following set of inner tolerances for the inexact triangular solves:\n$$\\tau \\in \\{0, 10^{-4}, 10^{-3}, 10^{-2}, 0.05, 0.1, 0.2, 0.3, 0.5\\}.$$\n\nAnswer Specification:\n- For each $\\tau$ in the test suite, output a boolean indicating whether BiCGSTAB converged under that level of inexactness (true means convergence within the iteration limit and without breakdown, false otherwise).\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[true,false,true]\"). Use lowercase \"true\"/\"false\" or Python boolean literals \"True\"/\"False\"; either is acceptable.\n\nYour final program must be self-contained and runnable, require no input, produce no extra text, and adhere to the specified libraries and environment.", "solution": "The user has requested an analysis of the BiCGSTAB algorithm's stability under inexact ILU preconditioning. The problem is to determine the maximum tolerance for the inner iterative solves of the preconditioner that still allows the outer BiCGSTAB iteration to converge.\n\n### Problem Validation\n\nFirst, a meticulous validation of the problem statement is performed.\n\n**Step 1: Extract Givens**\n\n- **Linear System**: $A x = b$.\n- **Matrix A**: Nonsymmetric, sparse, from a finite-difference discretization of the operator $-\\kappa \\Delta u + v_x \\frac{\\partial u}{\\partial x} + v_y \\frac{\\partial u}{\\partial y}$.\n- **Grid Parameters**: $N \\times N$ interior grid with $N = 15$. Grid spacing $h = 1/(N+1)$.\n- **Physical Coefficients**: Diffusion $\\kappa = 1$, convection velocities $v_x = 1$ and $v_y = 1$.\n- **Matrix Discretization Formulas**: For a grid point $(i,j)$ mapped to index $k$:\n    - Diagonal: $A_{k,k} = 4 \\kappa$.\n    - Off-diagonals:\n        - $A_{k,k-N} = -\\kappa - \\frac{v_x}{2h}$ (for $i  0$).\n        - $A_{k,k+N} = -\\kappa + \\frac{v_x}{2h}$ (for $i  N-1$).\n        - $A_{k,k-1} = -\\kappa - \\frac{v_y}{2h}$ (for $j  0$).\n        - $A_{k,k+1} = -\\kappa + \\frac{v_y}{2h}$ (for $j  N-1$).\n- **Right-Hand Side**: $b$ is a vector of all ones, with length $N^2$.\n- **Preconditioner**: $M \\approx A$ is an incomplete LU factorization (ILU) with sparse triangular factors $L$ and $U$ and permutation vectors.\n- **Inexact Preconditioning**: The triangular solves $L y = \\tilde{b}$ and $U z = y$ are approximated.\n- **Inner Solver**: Damped Jacobi iteration. The specific damping factor is not provided, so a standard choice of undamped Jacobi ($\\omega=1$) is appropriate.\n- **Inner Solver Tolerance**: The inner Jacobi solver terminates when the 2-norm of its residual, $\\|r_{inner}\\|_2$, satisfies $\\|r_{inner}\\|_2 \\le \\tau \\|\\tilde{b}\\|_2$ for a given tolerance $\\tau \\ge 0$. For $\\tau=0$, an exact sparse triangular solve is used.\n- **Outer Solver**: Right-preconditioned BiCGSTAB for the system $A M^{-1} y = b$, with the final solution recovered as $x = M^{-1} y$.\n- **Outer Solver Convergence**: Relative residual $\\frac{\\|b - A x\\|_2}{\\|b\\|_2} \\le \\varepsilon = 10^{-8}$.\n- **Outer Solver Limits**: Maximum of $500$ iterations.\n- **Breakdown Conditions**: Detection of zero denominators or non-numeric values (NaN/Inf) during iteration.\n- **Test Suite**: $\\tau \\in \\{0, 10^{-4}, 10^{-3}, 10^{-2}, 0.05, 0.1, 0.2, 0.3, 0.5\\}$.\n- **Output**: A list of booleans indicating convergence for each $\\tau$.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientific Grounding**: The problem is well-grounded in numerical linear algebra and computational science. The discretization of the convection-diffusion equation, the use of BiCGSTAB for nonsymmetric systems, and ILU preconditioning are all standard and fundamental topics. The study of inexact preconditioning is a relevant area of research and practice.\n- **Well-Posedness**: The problem is well-posed. For each input value of $\\tau$, the task is to determine a binary outcome (convergence or non-convergence) based on precisely defined numerical criteria. A unique result exists for the specified setup.\n- **Objectivity**: The problem is stated in precise, objective mathematical and algorithmic language, free from subjectivity.\n- **Completeness**: The problem provides all necessary parameters to construct the matrix $A$ and vector $b$. While it mentions \"a practical drop tolerance\" for ILU and a \"damped Jacobi\" method without specifying the damping factor, these are standard implementation choices left to the practitioner. A standard ILU routine (like `scipy.sparse.linalg.spilu`) with a reasonable drop tolerance and an undamped Jacobi solver are canonical choices that align with the problem's intent. The problem is therefore sufficiently specified to permit a unique and verifiable solution.\n\n**Step 3: Verdict and Action**\n\nThe problem is deemed **valid**. It is a standard numerical experiment in computational science. A solution will be provided.\n\n### Solution Design\n\nThe solution proceeds in several steps:\n1.  **Matrix and Vector Construction**: The sparse matrix $A$ of size $N^2 \\times N^2$ is assembled according to the specified five-point finite difference stencil. The vector $b$ is created as a vector of ones.\n2.  **ILU Factorization**: A single ILU factorization of $A$ is computed using `scipy.sparse.linalg.spilu`. This factorization produces the sparse triangular factors $L$ and $U$, as well as permutation vectors `perm_c` and `perm_r` that handle pivoting for stability. The factorization is $P_r A P_c^T \\approx LU$. This factorization is computed once and reused for all test cases.\n3.  **Inexact Triangular Solver**: A function implementing the Jacobi iteration is created to solve triangular systems of the form $T x_{approx} = d$, where $T$ is either $L$ or $U$. The iteration proceeds until the relative residual norm falls below the specified inner tolerance $\\tau$, or a maximum number of inner iterations is reached. For the case $\\tau=0$, an exact solve using `scipy.sparse.linalg.spsolve_triangular` is performed.\n4.  **Preconditioner Application**: A function is designed to apply the inverse of the preconditioner, $M^{-1}$, to a vector $v$. For right preconditioning, $M = P_r^{-1} L U P_c^{-1}$. Applying its inverse, $x = M^{-1}v$, involves these steps:\n    a. Permute the input vector: $v_{perm} = P_r v$.\n    b. Solve $L z = v_{perm}$ (inexactly or exactly, based on $\\tau$).\n    c. Solve $U y = z$ (inexactly or exactly, based on $\\tau$).\n    d. Un-permute the result: $x[perm_c] = y$.\n5.  **BiCGSTAB Implementation**: A custom implementation of the right-preconditioned BiCGSTAB algorithm is developed. This is necessary to precisely control the iteration, check for the specified breakdown conditions (division by very small numbers, which can lead to instability or NaN/Inf values), and adhere to the maximum iteration count. The algorithm iteratively updates the solution vector $x$ and terminates upon reaching the desired relative residual tolerance or upon failure.\n6.  **Main Test Loop**: The program iterates through the provided list of $\\tau$ values. For each $\\tau$, it configures the preconditioner application function and runs the custom BiCGSTAB solver. The convergence status (True/False) for each run is recorded.\n7.  **Output**: The final list of boolean results is formatted and printed as specified.\n\nThis structured approach ensures that each component of the problem is addressed correctly and leads to a verifiable numerical experiment.", "answer": "```python\n# Final Answer Code\n# Language: Python 3.12\n# Libraries: numpy==1.23.5, scipy==1.11.4\n\nimport numpy as np\nfrom scipy.sparse import lil_matrix, csc_matrix\nfrom scipy.sparse.linalg import spilu, spsolve_triangular\n\ndef construct_matrix_and_rhs(N):\n    \"\"\"\n    Constructs the sparse matrix A and right-hand side vector b for the\n    convection-diffusion problem.\n    \"\"\"\n    dim = N * N\n    A = lil_matrix((dim, dim))\n    \n    h = 1.0 / (N + 1)\n    kappa = 1.0\n    v_x = 1.0\n    v_y = 1.0\n\n    off_diag_x_neg = -kappa - v_x / (2 * h)\n    off_diag_x_pos = -kappa + v_x / (2 * h)\n    off_diag_y_neg = -kappa - v_y / (2 * h)\n    off_diag_y_pos = -kappa + v_y / (2 * h)\n    \n    for i in range(N):\n        for j in range(N):\n            k = i * N + j\n            \n            # Diagonal entry\n            A[k, k] = 4.0 * kappa\n            \n            # Off-diagonal entries\n            if i  0:   # North neighbor\n                A[k, k - N] = off_diag_x_neg\n            if i  N - 1: # South neighbor\n                A[k, k + N] = off_diag_x_pos\n            if j  0:   # West neighbor\n                A[k, k - 1] = off_diag_y_neg\n            if j  N - 1: # East neighbor\n                A[k, k + 1] = off_diag_y_pos\n\n    b = np.ones(dim)\n    return A.tocsc(), b\n\ndef inexact_tri_solve(T, b_vec, tau, max_inner_iter=50):\n    \"\"\"\n    Solves Tx = b_vec approximately using Jacobi iteration.\n    T is a sparse triangular matrix (L or U).\n    \"\"\"\n    if not isinstance(T, csc_matrix):\n        T = T.tocsc()\n\n    b_norm = np.linalg.norm(b_vec)\n    if b_norm == 0:\n        return np.zeros_like(b_vec)\n\n    stop_tol = tau * b_norm\n    \n    x = np.zeros_like(b_vec)\n    diag_T = T.diagonal()\n    if np.any(diag_T == 0): # Should not happen with stable ILU\n        raise ValueError(\"Zero diagonal element in triangular factor.\")\n    \n    inv_diag_T = 1.0 / diag_T\n    \n    for _ in range(max_inner_iter):\n        r_inner = b_vec - T @ x\n        if np.linalg.norm(r_inner) = stop_tol:\n            break\n        # Jacobi update: x_{k+1} = x_k + D^{-1} * (b - T @ x_k)\n        x += inv_diag_T * r_inner\n\n    return x\n\ndef get_preconditioner_solver(ilu_obj, tau):\n    \"\"\"\n    Returns a function that applies the inexact preconditioner M^{-1}.\n    \"\"\"\n    L = ilu_obj.L\n    U = ilu_obj.U\n    perm_r = ilu_obj.perm_r\n    perm_c = ilu_obj.perm_c\n\n    def apply(v):\n        # Permute rhs: w = P_r * v\n        v_perm = v[perm_r]\n        \n        # Solve L z = w\n        if tau == 0:\n            z = spsolve_triangular(L, v_perm, lower=True, unit_diagonal=True)\n        else:\n            z = inexact_tri_solve(L, v_perm, tau)\n        \n        # Solve U y_perm = z\n        if tau == 0:\n            y_perm = spsolve_triangular(U, z, lower=False)\n        else:\n            y_perm = inexact_tri_solve(U, z, tau)\n\n        # Un-permute solution: y = P_c * y_perm\n        y = np.zeros_like(v)\n        y[perm_c] = y_perm\n        return y\n\n    return apply\n\ndef custom_bicgstab(A, b, precon_solver, x0, tol, maxiter):\n    \"\"\"\n    Custom BiCGSTAB implementation with breakdown checks.\n    \"\"\"\n    x = x0.copy()\n    r = b - A @ x\n    r_hat = r.copy()\n    \n    b_norm = np.linalg.norm(b)\n    if b_norm == 0:\n        b_norm = 1.0\n\n    rho_prev, alpha, omega = 1.0, 1.0, 1.0\n    p = np.zeros_like(r)\n    v = np.zeros_like(r)\n\n    # Epsilon for breakdown checks\n    breakdown_tol = np.finfo(r.dtype).eps\n    \n    for i in range(maxiter):\n        rho = np.dot(r_hat, r)\n        if abs(rho)  breakdown_tol:\n            return x, False\n\n        beta = (rho / rho_prev) * (alpha / omega)\n        p = r + beta * (p - omega * v)\n\n        p_hat = precon_solver(p)\n        v = A @ p_hat\n        \n        alpha_denom = np.dot(r_hat, v)\n        if abs(alpha_denom)  breakdown_tol:\n            return x, False\n            \n        alpha = rho / alpha_denom\n        s = r - alpha * v\n        \n        s_hat = precon_solver(s)\n        t = A @ s_hat\n        \n        omega_denom = np.dot(t, t)\n        if abs(omega_denom)  breakdown_tol:\n            return x, False\n        \n        omega = np.dot(t, s) / omega_denom\n        \n        x += alpha * p_hat + omega * s_hat\n        r = s - omega * t\n\n        rho_prev = rho\n\n        # Check for NaN/Inf in solution\n        if np.any(np.isnan(x)) or np.any(np.isinf(x)):\n            return x, False\n\n        # Convergence check on the true residual\n        final_resid = b - A @ x\n        if np.linalg.norm(final_resid) / b_norm  tol:\n            return x, True\n\n    return x, False\n\ndef solve():\n    \"\"\"\n    Main function to run the numerical experiment.\n    \"\"\"\n    N = 15\n    A, b = construct_matrix_and_rhs(N)\n    \n    # ILU factorization with a practical drop tolerance and fill factor\n    # Note: Scipy's spilu might produce L with an implicit unit diagonal\n    try:\n        ilu = spilu(A, drop_tol=1e-4, fill_factor=20)\n    except RuntimeError:\n        # If factorization fails, cannot proceed.\n        # For this specific problem, it should succeed.\n        print(\"ILU factorization failed.\")\n        return\n\n    test_taus = [0, 1e-4, 1e-3, 1e-2, 0.05, 0.1, 0.2, 0.3, 0.5]\n    results = []\n    \n    for tau in test_taus:\n        precon_solver = get_preconditioner_solver(ilu, tau)\n        x0 = np.zeros(N * N)\n        _x, converged = custom_bicgstab(\n            A, b, precon_solver, x0, tol=1e-8, maxiter=500\n        )\n        results.append(converged)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3102124"}]}