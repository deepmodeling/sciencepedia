{"hands_on_practices": [{"introduction": "The emergence of a spanning cluster at the percolation threshold is a classic phase transition, and the fraction of the system occupied by this cluster acts as the order parameter, $P_{\\infty}$. This first exercise focuses on measuring the critical exponent $\\beta$, which describes how the order parameter scales with system size $L$ precisely at the critical point [@problem_id:3171679]. By simulating percolation on lattices of different sizes and applying the principles of finite-size scaling, you will perform a foundational measurement in the study of critical phenomena.", "problem": "Consider site percolation on a finite two-dimensional square lattice of linear size $L$ with open boundary conditions. Each site is independently occupied with probability $p$ and empty with probability $1-p$. Define the order parameter $P_{\\infty}(L,p)$ for a finite system as the expected fraction of sites that belong to the largest connected occupied cluster under nearest-neighbor connectivity (four neighbors). According to the finite-size scaling hypothesis in percolation, near the critical occupation probability $p_c$, the order parameter obeys the scaling form\n$$\nP_{\\infty}(L,p) \\sim L^{-\\beta/\\nu} \\, \\tilde{P}\\!\\left((p-p_c) L^{1/\\nu}\\right),\n$$\nwhere $\\beta$ is the order parameter critical exponent, $\\nu$ is the correlation length exponent, and $\\tilde{P}(\\cdot)$ is a scaling function that is regular near the origin. For two-dimensional site percolation, it is a well-tested fact that the correlation length exponent is $\\nu = 4/3$.\n\nYour task is to write a complete, runnable program that estimates $\\beta$ by exploiting the above scaling form at $p \\approx p_c$. At $p = p_c$, the argument of $\\tilde{P}(\\cdot)$ tends to $0$ as $L \\to \\infty$, so that $P_{\\infty}(L,p_c)$ behaves as\n$$\nP_{\\infty}(L,p_c) \\propto L^{-\\beta/\\nu},\n$$\nwhich implies that a linear fit of $\\log P_{\\infty}(L,p_c)$ as a function of $\\log L$ has slope $-\\beta/\\nu$. Therefore, an estimate of $\\beta$ can be obtained from the fitted slope $s$ via $\\beta \\approx -s \\, \\nu$.\n\nImplement the following algorithmic steps in your program:\n- For a given tuple $(L,p)$, generate independent realizations of site percolation and, for each realization, compute the largest-cluster fraction $S_{\\max}/N$, where $S_{\\max}$ is the size (number of occupied sites) of the largest connected occupied cluster and $N = L^2$ is the total number of sites. Use nearest-neighbor connectivity on the square lattice.\n- Estimate $P_{\\infty}(L,p)$ by averaging $S_{\\max}/N$ over a specified number of independent realizations.\n- For a collection of system sizes $\\{L_k\\}$ at a fixed $p$, compute $\\log P_{\\infty}(L_k,p)$ versus $\\log L_k$ and perform a linear regression to obtain the slope $s$.\n- Use $\\nu = 4/3$ to estimate $\\beta$ via $\\beta \\approx -s \\, \\nu$.\n\nScientific realism requirements:\n- Use open boundary conditions and nearest-neighbor connections on a square lattice.\n- Use independent site occupation events with probability $p$.\n- Ensure that all random trials are reproducible by seeding the random number generator.\n\nYour program must implement the above methodology and evaluate the following test suite. For each test case, use the provided list of lattice sizes, occupation probability, number of realizations per lattice size, and random seed. The tests are designed to cover a general case at $p = p_c$, small-system boundaries at $p = p_c$, and near-critical offsets $p = p_c \\pm \\Delta p$.\n\nTest suite:\n- Case $1$ (happy path): $L$ values $\\{\\,32,48,64,96,128\\,\\}$, $p = 0.592746$, trials per $L$ equal to $40$, seed $= 123$, with $\\nu = 4/3$.\n- Case $2$ (slightly above critical): $L$ values $\\{\\,24,32,40,48,64\\,\\}$, $p = 0.602746$, trials per $L$ equal to $35$, seed $= 456$, with $\\nu = 4/3$.\n- Case $3$ (slightly below critical): $L$ values $\\{\\,16,24,32,48\\,\\}$, $p = 0.582746$, trials per $L$ equal to $35$, seed $= 789$, with $\\nu = 4/3$.\n- Case $4$ (small-size boundary): $L$ values $\\{\\,8,12,16,20,24\\,\\}$, $p = 0.592746$, trials per $L$ equal to $40$, seed $= 1011$, with $\\nu = 4/3$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain the estimated $\\beta$ for each test case in the same order as listed above, each reported as a float rounded to three decimal places. For example, your program should print an output such as $[0.139,0.142,0.135,0.140]$ (the numerical values are illustrative).\n\nNo physical units are involved in this problem. Angles are not required. All reported numeric answers must be floats in the specified output format.", "solution": "The problem requires the estimation of the critical exponent $\\beta$ for two-dimensional site percolation on a square lattice. This is accomplished through a numerical simulation and finite-size scaling analysis. The procedure is based on established principles of statistical physics, specifically the theory of critical phenomena and phase transitions.\n\nThe fundamental premise is the finite-size scaling hypothesis for the order parameter, $P_{\\infty}(L,p)$. For a finite system of linear size $L$, the order parameter is defined as the expected fraction of sites belonging to the largest connected cluster of occupied sites, $P_{\\infty}(L,p) = \\mathbb{E}[S_{\\max} / L^2]$, where $S_{\\max}$ is the size of the largest cluster. Near the critical occupation probability $p_c$, the order parameter is postulated to obey the scaling relation:\n$$\nP_{\\infty}(L,p) \\sim L^{-\\beta/\\nu} \\, \\tilde{P}\\!\\left((p-p_c) L^{1/\\nu}\\right)\n$$\nHere, $\\beta$ is the critical exponent for the order parameter (the probability of belonging to the infinite cluster in an infinite system), and $\\nu$ is the critical exponent for the correlation length. The function $\\tilde{P}(x)$ is a universal scaling function, which is regular for small arguments $x$.\n\nThe proposed methodology exploits this scaling form precisely at the critical point, $p=p_c$. At this point, the argument of the scaling function, $(p-p_c)L^{1/\\nu}$, is zero. Assuming that $\\tilde{P}(0)$ is a non-zero finite constant, the scaling relation simplifies to a power law:\n$$\nP_{\\infty}(L,p_c) \\propto L^{-\\beta/\\nu}\n$$\nThis power-law relationship can be linearized by taking the natural logarithm of both sides:\n$$\n\\log P_{\\infty}(L,p_c) = -\\frac{\\beta}{\\nu} \\log L + \\text{constant}\n$$\nThis equation has the form of a straight line, $y=s x+c$, where $y = \\log P_{\\infty}(L,p_c)$, $x = \\log L$, and the slope is $s = -\\beta/\\nu$. By performing a linear regression on numerically obtained data for $\\log P_{\\infty}(L,p_c)$ versus $\\log L$, we can determine the slope $s$. Given the well-established theoretical value for the correlation length exponent in two dimensions, $\\nu = 4/3$, we can then extract an estimate for $\\beta$ using the formula:\n$$\n\\beta = -s \\nu\n$$\nThe overall algorithm to implement this procedure for each test case is as follows:\n\n1.  **Initialization**: For a given test case specified by a set of lattice sizes $\\{L_k\\}$, an occupation probability $p$, a number of trials, and a random seed, initialize data structures to store the results. The random number generator is seeded to ensure reproducibility.\n\n2.  **Outer Loop (over lattice sizes)**: Iterate through each linear size $L_k$ provided in the test case.\n\n3.  **Inner Loop (Monte Carlo Simulation)**: For each $L_k$, perform a specified number of independent `trials` to estimate $P_{\\infty}(L_k,p)$.\n    a. **Lattice Generation**: An $L_k \\times L_k$ grid is created. Each site is marked as occupied with probability $p$ or empty with probability $1-p$.\n    b. **Cluster Identification**: The largest connected cluster of occupied sites must be found. This standard graph traversal problem is solved using a Breadth-First Search (BFS) algorithm. We maintain an $L_k \\times L_k$ boolean grid, `visited`, initialized to `False`. We iterate through every site $(i,j)$ of the lattice. If site $(i,j)$ is occupied and has not yet been visited, it signifies the start of a new, unexplored cluster. A BFS is initiated from $(i,j)$:\n        i. A queue is initialized with the starting site $(i,j)$, which is then marked as visited.\n        ii. A counter for the current cluster's size is initialized.\n        iii. While the queue is not empty, a site is dequeued. Its four nearest neighbors (up, down, left, right) are examined.\n        iv. For each neighbor, if it is within the lattice boundaries (implementing open boundary conditions), is occupied, and has not been visited, it is marked as visited and enqueued.\n        v. The process continues until the queue is empty, at which point all sites in the current connected component have been visited and counted.\n    c. **Largest Cluster Fraction**: The size of the largest cluster found in the grid, $S_{\\max}$, is recorded for the current trial. The fraction $S_{\\max}/(L_k^2)$ is computed and stored.\n    d. **Averaging**: After completing all `trials` for the size $L_k$, the stored fractions are averaged to produce the estimate for $P_{\\infty}(L_k,p)$.\n\n4.  **Data Preparation for Regression**: After computing $P_{\\infty}(L_k,p)$ for all $L_k$ in the test case, two arrays are created: one containing the logarithms of the lattice sizes, $x_k = \\log L_k$, and the other containing the logarithms of the corresponding order parameter estimates, $y_k = \\log P_{\\infty}(L_k,p)$. Data points where $P_{\\infty}(L_k,p) = 0$ are excluded, as $\\log(0)$ is undefined.\n\n5.  **Linear Regression**: A simple linear regression is performed on the $(x_k, y_k)$ data points to find the slope $s$ of the best-fit line.\n\n6.  **Exponent Calculation**: The estimate for the critical exponent $\\beta$ is calculated using the determined slope $s$ and the given value $\\nu=4/3$, according to $\\beta \\approx -s \\nu$.\n\nThis entire process is repeated for each test case provided in the problem statement, and the resulting estimates for $\\beta$ are collected for the final output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\nimport collections\n\ndef solve():\n    \"\"\"\n    Main function to run the percolation simulations and estimate the critical exponent beta.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"L_values\": [32, 48, 64, 96, 128],\n            \"p\": 0.592746,\n            \"trials\": 40,\n            \"seed\": 123\n        },\n        {\n            \"L_values\": [24, 32, 40, 48, 64],\n            \"p\": 0.602746,\n            \"trials\": 35,\n            \"seed\": 456\n        },\n        {\n            \"L_values\": [16, 24, 32, 48],\n            \"p\": 0.582746,\n            \"trials\": 35,\n            \"seed\": 789\n        },\n        {\n            \"L_values\": [8, 12, 16, 20, 24],\n            \"p\": 0.592746,\n            \"trials\": 40,\n            \"seed\": 1011\n        }\n    ]\n\n    nu = 4.0 / 3.0\n    results = []\n\n    for case in test_cases:\n        beta_estimate = calculate_beta_for_case(case, nu)\n        results.append(round(beta_estimate, 3))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef calculate_beta_for_case(case_params, nu):\n    \"\"\"\n    Calculates the beta exponent for a single test case.\n    \"\"\"\n    L_values = case_params[\"L_values\"]\n    p = case_params[\"p\"]\n    trials = case_params[\"trials\"]\n    seed = case_params[\"seed\"]\n\n    rng = np.random.default_rng(seed)\n    \n    log_L_values = []\n    log_P_inf_values = []\n\n    for L in L_values:\n        total_sites = L * L\n        largest_cluster_fractions = []\n\n        for _ in range(trials):\n            grid = rng.random((L, L)) < p\n            \n            # Simple check to skip empty lattices, though unlikely for these p values\n            if not np.any(grid):\n                largest_cluster_fractions.append(0.0)\n                continue\n\n            max_cluster_size = 0\n            visited = np.zeros((L, L), dtype=bool)\n\n            for r in range(L):\n                for c in range(L):\n                    if grid[r, c] and not visited[r, c]:\n                        current_cluster_size = 0\n                        q = collections.deque([(r, c)])\n                        visited[r, c] = True\n                        \n                        while q:\n                            row, col = q.popleft()\n                            current_cluster_size += 1\n                            \n                            # Check 4 nearest neighbors\n                            for dr, dc in [(0, 1), (0, -1), (1, 0), (-1, 0)]:\n                                nr, nc = row + dr, col + dc\n                                \n                                # Check boundaries (open boundary conditions)\n                                if 0 <= nr < L and 0 <= nc < L:\n                                    if grid[nr, nc] and not visited[nr, nc]:\n                                        visited[nr, nc] = True\n                                        q.append((nr, nc))\n                        \n                        if current_cluster_size > max_cluster_size:\n                            max_cluster_size = current_cluster_size\n            \n            largest_cluster_fractions.append(max_cluster_size / total_sites)\n\n        P_inf = np.mean(largest_cluster_fractions)\n        \n        # Avoid log(0) error if a run results in no spanning cluster\n        if P_inf > 0:\n            log_L_values.append(np.log(L))\n            log_P_inf_values.append(np.log(P_inf))\n\n    # Perform linear regression to find the slope\n    # y = log(P_inf), x = log(L)\n    if len(log_L_values) < 2:\n        # Not enough data points for a fit.\n        # This should not happen with the given test cases.\n        return np.nan\n\n    slope, _, _, _, _ = stats.linregress(log_L_values, log_P_inf_values)\n    \n    # Calculate beta from the slope\n    beta = -slope * nu\n    return beta\n\nsolve()\n```", "id": "3171679"}, {"introduction": "Having established that a large cluster emerges at criticality, we now investigate its intricate geometric structure. The incipient infinite cluster is not a dense, space-filling object; instead, it is a fractal with a dimension $d_f$ that is less than the dimension of the embedding space. This practice challenges you to estimate this fractal dimension using two independent and powerful techniques: mass scaling and box counting [@problem_id:3171745]. This will give you a tangible feel for the concept of statistical self-similarity that characterizes critical points.", "problem": "You will implement a computational experiment to estimate the fractal dimension $d_f$ of the incipient infinite cluster in site percolation on a two-dimensional square lattice at the percolation threshold $p_c$. Start from the fundamental base that scale invariance at criticality implies power-law relations between counts measured at different scales, and that the fractal dimension $d_f$ quantifies the scaling of mass with spatial scale for clusters that are statistically self-similar. Use nearest-neighbor connectivity (von Neumann, $4$-connected) on an $L \\times L$ lattice with open boundaries.\n\nDefinitions and constraints:\n- Site percolation on a square lattice occupies each site independently with probability $p$. The percolation threshold for two-dimensional site percolation on the square lattice is $p_c \\approx 0.592746$.\n- A cluster is a set of occupied sites connected through nearest-neighbor bonds (up, down, left, right).\n- A cluster spans if it touches both the top and bottom boundaries or both the left and right boundaries of the lattice.\n- The incipient infinite cluster at finite size is operationally approximated by selecting the spanning cluster if one exists; otherwise, select the largest cluster by mass (the number of sites it contains).\n- Estimate $d_f$ by two independent methods derived from scale invariance:\n  1. Box counting: count how the number of occupied boxes at scale $s$ changes as the scale varies.\n  2. Mass scaling: count how the mass within Euclidean radius $R$ from a reference point changes with $R$.\n\nAlgorithmic requirements:\n1. Generate an $L \\times L$ Boolean lattice where each site is occupied with probability $p$ using a fixed pseudorandom seed to ensure reproducibility. Use open boundaries.\n2. Identify connected clusters using Breadth-First Search (BFS). Record, for each cluster, its mass, whether it touches the top, bottom, left, and right boundaries, and its site coordinates.\n3. Select the target cluster as the spanning cluster with the largest mass if at least one spanning cluster exists; otherwise, select the largest cluster by mass.\n4. Box counting dimension estimate:\n   - For scales $s$ chosen as powers of $2$ within the lattice size (e.g., $s \\in \\{2,4,8,16,\\dots\\}$ and $s \\le \\lfloor L/2 \\rfloor$), partition the lattice into non-overlapping boxes of side $s$.\n   - Count the number of boxes that contain at least one site from the selected cluster. Use only the selected cluster’s sites for counting coverage.\n   - Perform a linear regression of the appropriate logarithmic variables implied by scale invariance to obtain $d_f$ from the slope, using natural logarithms.\n5. Mass scaling dimension estimate:\n   - Choose the reference point as the geometric center of the selected cluster (the arithmetic mean of row and column indices of the cluster’s sites).\n   - For radii $R$ chosen as powers of $2$ up to a fraction of the lattice size (e.g., $R \\in \\{2,4,8,16,\\dots\\}$ and $R \\le \\lfloor L/4 \\rfloor$), count the number of cluster sites whose Euclidean distance from the reference point is less than or equal to $R$.\n   - Perform a linear regression of the appropriate logarithmic variables implied by scale invariance to obtain $d_f$ from the slope, using natural logarithms.\n6. If the number of usable scales for either method is less than $2$, you should return a not-a-number float for that estimate.\n\nNumerical choices:\n- Use natural logarithms.\n- Use Ordinary Least Squares (OLS) linear regression to estimate the slope that yields $d_f$ for each method.\n\nTest suite:\nRun your program on the following test cases, each specified as $(L, p, \\text{seed}, \\text{method})$, where $\\text{method} \\in \\{\\text{\"box\"}, \\text{\"mass\"}\\}$:\n- Case $1$: $(64, 0.592746, 1234, \\text{\"box\"})$.\n- Case $2$: $(64, 0.592746, 1234, \\text{\"mass\"})$.\n- Case $3$: $(128, 0.592746, 42, \\text{\"box\"})$.\n- Case $4$: $(128, 0.592746, 42, \\text{\"mass\"})$.\n- Case $5$: $(32, 0.592746, 7, \\text{\"box\"})$.\n- Case $6$: $(32, 0.58, 7, \\text{\"mass\"})$.\n\nAnswer specification:\n- For each case, output a single float equal to the estimated $d_f$ for the selected method, rounded to $3$ decimal places.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[d_1,d_2,d_3,d_4,d_5,d_6]$), where each $d_i$ is the float for case $i$.", "solution": "The problem requires the estimation of the fractal dimension $d_f$ of the incipient infinite cluster in two-dimensional site percolation. This is a classic problem in computational statistical physics. The solution involves generating a percolating system, identifying the critical cluster, and then applying two distinct methods derived from the principle of scale invariance at the critical point: box counting and mass scaling.\n\nThe fundamental principle is that at the percolation threshold, $p=p_c$, the system is statistically self-similar. This means its geometric properties are independent of the length scale at which they are observed. This self-similarity or scale invariance is characteristic of a fractal. The fractal dimension, $d_f$, is an exponent that quantifies how the 'mass' or 'content' of the object scales with its linear size. For a $d$-dimensional Euclidean object, its mass scales with its radius $R$ as $M \\propto R^d$. For a fractal object, this relationship is generalized to $M(R) \\propto R^{d_f}$, where $d_f$ is typically non-integer. For the incipient infinite cluster in $2$D percolation, theory predicts $d_f = 91/48 \\approx 1.8958$.\n\nThe overall computational procedure is as follows:\n1.  Generate a random $L \\times L$ lattice where each site is occupied with probability $p$.\n2.  Use a graph traversal algorithm, Breadth-First Search (BFS), to identify all connected clusters of occupied sites.\n3.  Select a single 'target cluster' that serves as the finite-size approximation of the incipient infinite cluster. This is chosen as the largest spanning cluster if one exists, or the largest cluster by mass otherwise.\n4.  Apply two different methods to estimate $d_f$ for this target cluster.\n\n**Method 1: Mass Scaling Dimension**\n\nThis method directly uses the defining relationship for mass fractals. The mass $M(R)$ of the cluster contained within a Euclidean radius $R$ from a central point is expected to scale as a power law:\n$$M(R) \\propto R^{d_f}$$\nTo extract the exponent $d_f$, we can linearize this relationship by taking the natural logarithm of both sides:\n$$\\ln(M(R)) = d_f \\ln(R) + C$$\nwhere $C$ is a constant. This equation has the form of a line, $y = m x + c$, where the dependent variable is $y = \\ln(M(R))$, the independent variable is $x = \\ln(R)$, and the slope is $m = d_f$.\n\nThe algorithm is therefore:\n1.  Calculate the geometric center of the target cluster by taking the arithmetic mean of the coordinates of all its sites.\n2.  For a sequence of increasing radii $R$ (chosen as powers of $2$ up to $L/4$), count the number of cluster sites $M(R)$ that lie within a Euclidean distance $R$ of the center.\n3.  Perform an Ordinary Least Squares (OLS) linear regression on the set of points $(\\ln(R), \\ln(M(R)))$.\n4.  The slope of the resulting regression line is the estimate for the fractal dimension, $d_f$.\n\n**Method 2: Box Counting Dimension**\n\nThe box counting method provides an alternative way to measure fractal dimension. It quantifies how the number of 'boxes' of a given size $s$ needed to cover the object changes as $s$ changes. For a fractal object, this number $N(s)$ scales as a power law:\n$$N(s) \\propto \\left(\\frac{1}{s}\\right)^{d_f} = s^{-d_f}$$\nAs $s$ decreases, the number of boxes needed to cover the object increases. Again, we linearize this by taking the natural logarithm:\n$$\\ln(N(s)) = -d_f \\ln(s) + C'$$\nThis is also a linear equation, $y = m x + c$, where $y = \\ln(N(s))$, $x = \\ln(s)$, and the slope is $m = -d_f$. Therefore, the fractal dimension can be estimated as $d_f = -m$.\n\nThe algorithm is:\n1.  For a sequence of box sizes $s$ (chosen as powers of $2$ up to $L/2$), partition the $L \\times L$ lattice into a grid of non-overlapping $s \\times s$ boxes.\n2.  Count the number of boxes, $N(s)$, that contain at least one site belonging to the target cluster.\n3.  Perform an OLS linear regression on the set of points $(\\ln(s), \\ln(N(s)))$.\n4.  The estimate for the fractal dimension is the negative of the slope of the regression line, $d_f = -m$.\n\n**Implementation Details**\n\nThe lattice is generated using `numpy.random.default_rng` with a specified seed for reproducibility. Cluster identification is performed by iterating through all sites of the lattice. If an occupied site has not yet been visited, a Breadth-First Search (BFS) is initiated from that site to find all connected sites belonging to that cluster, marking them as visited. During the BFS, the cluster's properties (mass, site coordinates, and whether it touches each of the four boundaries) are collected. After all clusters are found, the target cluster is selected according to the specified rules. The linear regression is performed using the `scipy.stats.linregress` function, which provides a robust implementation of OLS. If a method yields fewer than two data points for regression, a not-a-number value (`nan`) is returned as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\nfrom collections import deque\n\ndef _bfs_cluster_find(grid, start_node, visited):\n    \"\"\"\n    Performs a Breadth-First Search to find a single cluster.\n    \"\"\"\n    L = grid.shape[0]\n    q = deque([start_node])\n    visited[start_node] = True\n    \n    cluster_sites = []\n    mass = 0\n    touches_top = False\n    touches_bottom = False\n    touches_left = False\n    touches_right = False\n    \n    while q:\n        r, c = q.popleft()\n        \n        cluster_sites.append((r, c))\n        mass += 1\n        \n        if r == 0: touches_top = True\n        if r == L - 1: touches_bottom = True\n        if c == 0: touches_left = True\n        if c == L - 1: touches_right = True\n        \n        # Von Neumann neighbors\n        for dr, dc in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n            nr, nc = r + dr, c + dc\n            \n            if 0 <= nr < L and 0 <= nc < L and grid[nr, nc] and not visited[nr, nc]:\n                visited[nr, nc] = True\n                q.append((nr, nc))\n                \n    return {\n        \"mass\": mass,\n        \"sites\": cluster_sites,\n        \"touches_top\": touches_top,\n        \"touches_bottom\": touches_bottom,\n        \"touches_left\": touches_left,\n        \"touches_right\": touches_right,\n    }\n\ndef find_clusters(grid):\n    \"\"\"\n    Identifies all connected clusters in the grid using BFS.\n    \"\"\"\n    L = grid.shape[0]\n    visited = np.zeros_like(grid, dtype=bool)\n    clusters = []\n    \n    for r in range(L):\n        for c in range(L):\n            if grid[r, c] and not visited[r, c]:\n                cluster = _bfs_cluster_find(grid, (r, c), visited)\n                clusters.append(cluster)\n    return clusters\n\ndef select_target_cluster(clusters, L):\n    \"\"\"\n    Selects the target cluster based on spanning/mass criteria.\n    \"\"\"\n    if not clusters:\n        return None\n        \n    spanning_clusters = []\n    for c in clusters:\n        is_spanning_v = c['touches_top'] and c['touches_bottom']\n        is_spanning_h = c['touches_left'] and c['touches_right']\n        if is_spanning_v or is_spanning_h:\n            spanning_clusters.append(c)\n            \n    if spanning_clusters:\n        return max(spanning_clusters, key=lambda c: c['mass'])\n    else:\n        return max(clusters, key=lambda c: c['mass'])\n\ndef estimate_df_box(cluster, L):\n    \"\"\"\n    Estimates fractal dimension using the box counting method.\n    \"\"\"\n    sites = cluster['sites']\n    \n    log_s_vals = []\n    log_N_vals = []\n    \n    s = 2\n    while s <= L // 2:\n        if s > 0:\n            occupied_boxes = set()\n            for r, c in sites:\n                occupied_boxes.add((r // s, c // s))\n            \n            N_s = len(occupied_boxes)\n            if N_s > 0:\n                log_s_vals.append(np.log(s))\n                log_N_vals.append(np.log(N_s))\n        s *= 2\n        \n    if len(log_s_vals) < 2:\n        return np.nan\n        \n    slope, _, _, _, _ = stats.linregress(log_s_vals, log_N_vals)\n    return -slope\n\ndef estimate_df_mass(cluster, L):\n    \"\"\"\n    Estimates fractal dimension using the mass scaling method.\n    \"\"\"\n    sites_arr = np.array(cluster['sites'])\n    if sites_arr.shape[0] == 0:\n        return np.nan\n        \n    center = np.mean(sites_arr, axis=0)\n    distances = np.sqrt(np.sum((sites_arr - center)**2, axis=1))\n    \n    log_R_vals = []\n    log_M_vals = []\n    \n    R = 2\n    while R <= L // 4:\n        if R > 0:\n            mass_in_radius = np.sum(distances <= R)\n            if mass_in_radius > 0:\n                log_R_vals.append(np.log(R))\n                log_M_vals.append(np.log(mass_in_radius))\n        R *= 2\n        \n    if len(log_R_vals) < 2:\n        return np.nan\n\n    slope, _, _, _, _ = stats.linregress(log_R_vals, log_M_vals)\n    return slope\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and compute fractal dimensions.\n    \"\"\"\n    test_cases = [\n        (64, 0.592746, 1234, \"box\"),\n        (64, 0.592746, 1234, \"mass\"),\n        (128, 0.592746, 42, \"box\"),\n        (128, 0.592746, 42, \"mass\"),\n        (32, 0.592746, 7, \"box\"),\n        (32, 0.58, 7, \"mass\"),\n    ]\n\n    results = []\n    lattice_cache = {}\n\n    for L, p, seed, method in test_cases:\n        cache_key = (L, p, seed)\n        \n        if cache_key in lattice_cache:\n            target_cluster = lattice_cache[cache_key]\n        else:\n            # 1. Generate grid\n            rng = np.random.default_rng(seed)\n            grid = rng.random((L, L)) < p\n            \n            # 2. Find clusters\n            clusters = find_clusters(grid)\n            \n            # 3. Select target cluster\n            target_cluster = select_target_cluster(clusters, L)\n            lattice_cache[cache_key] = target_cluster\n        \n        df_estimate = np.nan\n        if target_cluster is not None:\n            if method == \"box\":\n                df_estimate = estimate_df_box(target_cluster, L)\n            elif method == \"mass\":\n                df_estimate = estimate_df_mass(target_cluster, L)\n        \n        results.append(df_estimate)\n\n    formatted_results = []\n    for r in results:\n        if np.isnan(r):\n            formatted_results.append(\"nan\")\n        else:\n            formatted_results.append(f\"{r:.3f}\")\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3171745"}, {"introduction": "The geometric connectivity of the percolating cluster has profound physical consequences, particularly for transport phenomena like electrical conduction. This final practice bridges the gap between abstract geometry and a measurable physical property by modeling the system as a random resistor network [@problem_id:3171738]. You will compute the effective conductivity $\\sigma(p)$, which follows a universal power law near the critical point governed by the exponent $t$, demonstrating how percolation serves as a fundamental model for transport in disordered materials.", "problem": "You will write a complete, runnable program that estimates the conductivity critical exponent $t$ for two-dimensional bond percolation by mapping percolation to a random resistor network where each occupied bond has conductance $g=1$ and each unoccupied bond is absent. The estimation must be done by computing the effective bulk conductivity $\\sigma(p)$ between two opposite sides of a square lattice near the percolation threshold $p_c$ and then fitting the scaling relation $\\sigma(p)\\propto (p-p_c)^t$ for $p>p_c$.\n\nFoundational base and assumptions. Use the following fundamental and well-tested principles:\n- Ohm’s law relates current $I$ and voltage difference $\\Delta V$ across a resistor with conductance $g$ by $I=g\\,\\Delta V$.\n- Kirchhoff’s current law states that for any internal node of a resistor network, the algebraic sum of currents leaving the node is $0$ when the system is in steady state.\n- The network solution under fixed boundary voltages is obtained by solving the discrete Poisson equation (graph Laplacian with Dirichlet boundary conditions) derived from Kirchhoff’s current law and Ohm’s law.\n- For bond percolation on an infinite square lattice, the bond percolation threshold is $p_c=\\frac{1}{2}$ (this is an exactly known and widely accepted fact).\n\nDefinitions and numerical setup. Consider an $L\\times L$ square lattice of nodes with bonds only between nearest neighbors (horizontal and vertical). Each possible bond is independently occupied with probability $p\\in(0,1)$, and each occupied bond is assigned conductance $g=1$ while unoccupied bonds are absent. Impose Dirichlet boundary conditions $V=1$ on all nodes on the left boundary (column index $x=0$) and $V=0$ on all nodes on the right boundary (column index $x=L-1$). The top and bottom boundaries (row indices $y=0$ and $y=L-1$) are electrically free (no fixed potential). For any realization, the effective conductivity $\\sigma(p)$ is defined as the total net current injected at the left boundary under the imposed unit voltage drop $\\Delta V=1$, which equals the effective two-terminal conductance between left and right boundaries in arbitrary units.\n\nTo ensure numerical solvability, any interior connected component of nodes that is not connected to either the left boundary or the right boundary should be excluded from the linear system because its potential is otherwise undetermined under Dirichlet boundary conditions. Practically, you must restrict the solve to the set of interior nodes that are connected via occupied bonds to at least one of the two Dirichlet boundaries.\n\nEstimator for the exponent $t$. For each test case, you will:\n- For each $p$ in a provided set of values above $p_c$, generate multiple independent network realizations and compute the effective conductivity $\\sigma(p)$ for each, then take the sample mean $\\overline{\\sigma}(p)$.\n- Using only those $p$ values where $\\overline{\\sigma}(p)>0$, fit the linear relation $\\log \\overline{\\sigma}(p)=t\\log(p-p_c)+C$ by ordinary least squares (unweighted) as a function of $\\log(p-p_c)$ to estimate $t$. The logarithm may be any base; use the natural logarithm for definiteness.\n- If fewer than $2$ distinct $p$ values remain with $\\overline{\\sigma}(p)>0$, return the sentinel value $-1.0$ for that test case.\n\nImplementation details the program must follow:\n- Construct the graph Laplacian on the set of interior nodes connected to at least one Dirichlet boundary. For an interior node $i$, the diagonal entry equals the sum of conductances to its occupied neighbors, and off-diagonal entries equal $-g$ for occupied bonds to other interior nodes. Contributions to the right-hand side $b$ come from occupied bonds to boundary nodes with fixed voltages ($gV_{\\text{boundary}}$).\n- Solve the sparse linear system $A\\mathbf{v}=\\mathbf{b}$ to obtain interior node voltages $\\mathbf{v}$; then compute the net current injected at the left boundary as the sum, over each occupied bond between a left boundary node and its nearest interior neighbor, of $g\\,(1-V_{\\text{neighbor}})$.\n- Use $g=1$ for every occupied bond and $g=0$ for unoccupied bonds.\n- Use the percolation threshold $p_c=\\frac{1}{2}$.\n- Use a pseudorandom number generator with controlled seeds to ensure reproducibility. For test case index $k\\in\\{0,1,2\\}$ and sample index $s\\in\\{0,1,\\dots,S-1\\}$, set the seed to $12345+1000k+s$.\n\nTest suite. Your program must estimate $t$ for the following three test cases, where for each case you use the specified lattice size $L$, the set of occupation probabilities $p$ (each strictly greater than $p_c$), and the number of independent samples $S$ per $p$:\n- Case $0$: $L=24$, $p\\in\\{0.55,\\,0.60,\\,0.65,\\,0.70\\}$, $S=12$.\n- Case $1$: $L=16$, $p\\in\\{0.52,\\,0.56,\\,0.60,\\,0.64\\}$, $S=12$.\n- Case $2$: $L=12$, $p\\in\\{0.505,\\,0.530,\\,0.560,\\,0.620\\}$, $S=20$.\n\nQuantifiable outputs. For each case, the output is a single real number: the estimated exponent $t$ obtained by the linear fit described above, computed using the natural logarithm. If fewer than $2$ usable $p$ values remain after filtering by $\\overline{\\sigma}(p)>0$, output the sentinel $-1.0$ for that case. The final numerical outputs have no physical units and must be printed rounded to three decimal places.\n\nFinal output format. Your program should produce a single line of output containing the three results in order of the cases, as a comma-separated list enclosed in square brackets, for example: $[1.234,1.289,1.301]$. No other text should be printed.", "solution": "We outline a principled computational approach grounded in fundamental electrical network theory and percolation definitions to estimate the conductivity exponent $t$.\n\nThe target scaling relation is that, for bond percolation on a two-dimensional square lattice near its percolation threshold $p_c$, the effective bulk conductivity behaves as $\\sigma(p)\\propto (p-p_c)^t$ as $p\\downarrow p_c$ from above. Taking logarithms yields $\\log \\sigma(p) = t \\log(p-p_c) + C$, which motivates estimating $t$ as the slope of $\\log \\sigma(p)$ versus $\\log(p-p_c)$ for $p>p_c$.\n\nWe construct $\\sigma(p)$ from a random resistor network obtained from percolation as follows, based entirely on Ohm’s law and Kirchhoff’s current law. Consider an $L\\times L$ square grid of nodes with bonds between nearest neighbors horizontally and vertically. Each possible bond is independently occupied with probability $p$. Each occupied bond is assigned conductance $g=1$ (arbitrary units), while unoccupied bonds are absent ($g=0$). Impose Dirichlet boundary conditions by setting the potentials of all nodes on the left boundary to $V=1$ and on the right boundary to $V=0$. The top and bottom boundaries are left electrically free (no fixed potential). For a given network realization, the interior node voltages are determined by Kirchhoff’s current law at each interior node $i$:\n$$\n\\sum_{j\\in \\mathcal{N}(i)} g_{ij}\\,(V_i - V_j) = 0\n$$,\nwhere $\\mathcal{N}(i)$ denotes the set of neighbors of node $i$ and $g_{ij}\\in\\{0,1\\}$ is the conductance of the bond between $i$ and $j$. This yields a linear system $A\\mathbf{v}=\\mathbf{b}$ for the voltages $\\mathbf{v}$ at interior nodes, where $A$ is the graph Laplacian restricted to interior nodes and $\\mathbf{b}$ collects contributions from occupied bonds to Dirichlet boundary nodes. Specifically, for an interior node $i$, the diagonal entry of $A$ is the sum of conductances from $i$ to all its occupied neighbors, and each off-diagonal entry $A_{ij}$ is $-g_{ij}$ for occupied bonds between interior nodes $i$ and $j$. The right-hand side entry $b_i$ is $\\sum_{j\\in \\partial \\Omega} g_{ij}\\,V_j$, summing over occupied bonds from $i$ to boundary nodes $j$ with fixed potential $V_j$.\n\nA subtlety arises if an interior connected component is not attached to any Dirichlet boundary: then the potentials within that component are undetermined up to an additive constant, and the submatrix for that component is singular. To avoid this, we restrict the domain of the linear system to the set of interior nodes that are connected via occupied bonds to at least one of the Dirichlet boundaries (left or right). This is achieved by a graph traversal, such as breadth-first search, from the left boundary and from the right boundary across occupied bonds, and retaining the union of interior nodes reachable from either side. Nodes not connected to either boundary are excluded from $A$ and $\\mathbf{b}$ since they cannot affect the current between the boundaries.\n\nAfter solving $A\\mathbf{v}=\\mathbf{b}$, we compute the total current injected at the left boundary using Ohm’s law across occupied bonds that connect left boundary nodes (fixed at $V=1$) to their immediate interior neighbors. If an occupied bond connects a left boundary node to an interior node with voltage $V_{\\text{n}}$, the current through that bond from left to interior is $g\\,(1 - V_{\\text{n}})$. Summing over all such bonds yields the net injected current $I_{\\text{L}}$. Since the right boundary is fixed at $V=0$, the total voltage drop between boundaries is $\\Delta V=1$, so the effective conductance (bulk conductivity in these arbitrary units) equals $\\sigma = I_{\\text{L}}$.\n\nTo estimate $t$, we proceed as follows for each test case:\n- For a prescribed set of occupation probabilities $p>p_c$ and a number of independent samples $S$, we generate $S$ independent bond configurations using a pseudorandom number generator seeded deterministically as $12345+1000k+s$ for test case index $k$ and sample index $s\\in\\{0,1,\\dots,S-1\\}$. For each configuration, we compute $\\sigma(p)$ as described above. We then compute the sample mean $\\overline{\\sigma}(p)$ over the $S$ samples.\n- We discard any $p$ for which $\\overline{\\sigma}(p)\\le 0$ (which can occur due to finite-size effects or insufficient sampling near $p_c$). If at least $2$ $p$ values remain, we fit the linear model $\\log \\overline{\\sigma}(p) = t \\log(p - p_c) + C$ by ordinary least squares regression using the natural logarithm. The fitted slope is our estimate of $t$. If fewer than $2$ usable points remain, we output the sentinel value $-1.0$.\n\nThe percolation threshold for bond percolation on a square lattice is $p_c=\\frac{1}{2}$, which we use for all test cases. The test suite consists of three cases:\n- Case $0$: $L=24$, $p\\in\\{0.55,\\,0.60,\\,0.65,\\,0.70\\}$, $S=12$.\n- Case $1$: $L=16$, $p\\in\\{0.52,\\,0.56,\\,0.60,\\,0.64\\}$, $S=12$.\n- Case $2$: $L=12$, $p\\in\\{0.505,\\,0.530,\\,0.560,\\,0.620\\}$, $S=20$.\n\nThese cases probe, respectively, a moderate system with multiple $p$ values safely above $p_c$, a smaller system including $p$ values closer to $p_c$ to test sensitivity and potential finite-size effects, and a small system with additional sampling to mitigate noise near $p_c$. The final program prints a single line with the three estimated slopes rounded to three decimal places in the exact format $[t_0,t_1,t_2]$, for example $[1.234,1.289,1.301]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse import coo_matrix, csr_matrix\nfrom scipy.sparse.linalg import spsolve\nfrom collections import deque\n\ndef generate_bonds(L, p, rng):\n    \"\"\"\n    Generate occupied bond arrays for an LxL lattice.\n    Horizontal bonds H[y, x] connect (x,y) <-> (x+1,y) for x in [0..L-2], y in [0..L-1].\n    Vertical bonds V[y, x] connect (x,y) <-> (x,y+1) for y in [0..L-2], x in [0..L-1].\n    Returns boolean arrays H (shape L x (L-1)) and V ((L-1) x L).\n    \"\"\"\n    H = rng.random((L, L - 1)) < p\n    V = rng.random((L - 1, L)) < p\n    return H, V\n\ndef bfs_reachable(L, H, V, from_left=True):\n    \"\"\"\n    BFS to find nodes reachable from the left (x=0) or right (x=L-1) boundary via occupied bonds.\n    Returns a boolean array reach[y, x] of shape (L, L).\n    \"\"\"\n    reach = np.zeros((L, L), dtype=bool)\n    q = deque()\n    if from_left:\n        x0 = 0\n        for y in range(L):\n            reach[y, x0] = True\n            q.append((x0, y))\n    else:\n        x0 = L - 1\n        for y in range(L):\n            reach[y, x0] = True\n            q.append((x0, y))\n\n    while q:\n        x, y = q.popleft()\n        # Left neighbor\n        if x > 0 and H[y, x - 1]:\n            nx, ny = x - 1, y\n            if not reach[ny, nx]:\n                reach[ny, nx] = True\n                q.append((nx, ny))\n        # Right neighbor\n        if x < L - 1 and H[y, x]:\n            nx, ny = x + 1, y\n            if not reach[ny, nx]:\n                reach[ny, nx] = True\n                q.append((nx, ny))\n        # Up neighbor\n        if y > 0 and V[y - 1, x]:\n            nx, ny = x, y - 1\n            if not reach[ny, nx]:\n                reach[ny, nx] = True\n                q.append((nx, ny))\n        # Down neighbor\n        if y < L - 1 and V[y, x]:\n            nx, ny = x, y + 1\n            if not reach[ny, nx]:\n                reach[ny, nx] = True\n                q.append((nx, ny))\n\n    return reach\n\ndef build_system(L, H, V, reach_union):\n    \"\"\"\n    Build the sparse linear system A v = b for interior nodes that are reachable from either boundary.\n    Interior nodes are those with x in [1..L-2]. Boundary nodes at x=0 (V=1) and x=L-1 (V=0) are Dirichlet.\n    reach_union is boolean mask of nodes reachable from left or right boundaries.\n    Returns (A_csr, b, node_index, index_to_node).\n    node_index maps (x,y) -> idx in [0..n-1] for interior reachable nodes.\n    index_to_node is list mapping idx -> (x,y).\n    \"\"\"\n    # Map interior reachable nodes to indices\n    node_index = {}\n    index_to_node = []\n    for y in range(L):\n        for x in range(1, L - 1):\n            if reach_union[y, x]:\n                node_index[(x, y)] = len(index_to_node)\n                index_to_node.append((x, y))\n\n    n = len(index_to_node)\n    if n == 0:\n        # Empty system\n        A = coo_matrix((0, 0), dtype=float).tocsr()\n        b = np.zeros((0,), dtype=float)\n        return A, b, node_index, index_to_node\n\n    rows = []\n    cols = []\n    data = []\n    b = np.zeros(n, dtype=float)\n\n    def add_entry(i, j, val):\n        rows.append(i)\n        cols.append(j)\n        data.append(val)\n\n    for idx, (x, y) in enumerate(index_to_node):\n        diag = 0.0\n\n        # Neighbor to the left: (x-1, y) via H[y, x-1]\n        if x - 1 >= 0 and H[y, x - 1]:\n            diag += 1.0\n            if x - 1 == 0:\n                # Left boundary at V=1\n                b[idx] += 1.0 * 1.0\n            else:\n                # Interior node at (x-1, y)\n                if reach_union[y, x - 1]:\n                    jdx = node_index[(x - 1, y)]\n                    add_entry(idx, jdx, -1.0)\n                else:\n                    # Not in system (should not happen for an occupied bond unless excluded)\n                    pass\n\n        # Neighbor to the right: (x+1, y) via H[y, x]\n        if x < L - 1 and H[y, x]:\n            diag += 1.0\n            if x + 1 == L - 1:\n                # Right boundary at V=0 => no contribution to b\n                pass\n            else:\n                if reach_union[y, x + 1]:\n                    jdx = node_index[(x + 1, y)]\n                    add_entry(idx, jdx, -1.0)\n\n        # Neighbor above: (x, y-1) via V[y-1, x]\n        if y - 1 >= 0 and V[y - 1, x]:\n            diag += 1.0\n            if reach_union[y - 1, x]:\n                jdx = node_index.get((x, y - 1))\n                if jdx is not None:\n                    add_entry(idx, jdx, -1.0)\n                else:\n                    # Could be boundary column (but y-1 same column), handled by reach mask\n                    pass\n\n        # Neighbor below: (x, y+1) via V[y, x]\n        if y < L - 1 and V[y, x]:\n            diag += 1.0\n            if reach_union[y + 1, x]:\n                jdx = node_index.get((x, y + 1))\n                if jdx is not None:\n                    add_entry(idx, jdx, -1.0)\n\n        # Diagonal entry\n        add_entry(idx, idx, diag)\n\n    A = coo_matrix((data, (rows, cols)), shape=(n, n), dtype=float).tocsr()\n    return A, b, node_index, index_to_node\n\ndef compute_sigma(L, p, seed):\n    \"\"\"\n    Compute effective conductance sigma for one random instance at given L, p, with RNG seed.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    H, V = generate_bonds(L, p, rng)\n\n    # Reachable from left and right boundaries\n    reach_left = bfs_reachable(L, H, V, from_left=True)\n    reach_right = bfs_reachable(L, H, V, from_left=False)\n    reach_union = np.logical_or(reach_left, reach_right)\n\n    # Build and solve system\n    A, b, node_index, index_to_node = build_system(L, H, V, reach_union)\n\n    if A.shape[0] == 0:\n        # No interior reachable nodes => no conduction\n        return 0.0\n\n    try:\n        v = spsolve(A, b)\n    except Exception:\n        # Numerical fallback: treat as zero conductance if solve fails\n        return 0.0\n\n    # Map voltages back for easy lookup\n    volt = {}\n    for idx, (x, y) in enumerate(index_to_node):\n        volt[(x, y)] = v[idx]\n\n    # Net current injected at left boundary across occupied bonds to column x=1\n    total_current = 0.0\n    x_left_neighbor = 1\n    for y in range(L):\n        if H[y, 0]:  # Bond between (0,y) and (1,y)\n            vn = volt.get((x_left_neighbor, y))\n            if vn is not None:\n                total_current += (1.0 - vn)  # g=1\n\n    # sigma equals total current for unit voltage drop\n    return float(total_current)\n\ndef mean_sigma_over_samples(L, p, case_index, S):\n    \"\"\"\n    Compute mean sigma over S samples for given L and p with deterministic seeding.\n    \"\"\"\n    sigmas = []\n    for s in range(S):\n        seed = 12345 + 1000 * case_index + s\n        sigmas.append(compute_sigma(L, p, seed))\n    return float(np.mean(sigmas))\n\ndef estimate_t(p_values, sigma_means, pc=0.5):\n    \"\"\"\n    Estimate exponent t by linear regression of log(sigma) vs log(p - pc).\n    Returns -1.0 if fewer than two usable points.\n    \"\"\"\n    x = []\n    y = []\n    for p, sm in zip(p_values, sigma_means):\n        if sm > 0.0 and p > pc:\n            x.append(np.log(p - pc))\n            y.append(np.log(sm))\n    if len(x) < 2:\n        return -1.0\n    x = np.array(x)\n    y = np.array(y)\n    # Ordinary least squares fit: y = t * x + c\n    t, c = np.polyfit(x, y, 1)\n    return float(t)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (case_index, L, p_list, samples S)\n        (0, 24, [0.55, 0.60, 0.65, 0.70], 12),\n        (1, 16, [0.52, 0.56, 0.60, 0.64], 12),\n        (2, 12, [0.505, 0.530, 0.560, 0.620], 20),\n    ]\n\n    pc = 0.5\n    results = []\n    for case_index, L, p_list, S in test_cases:\n        sigma_means = []\n        for p in p_list:\n            m = mean_sigma_over_samples(L, p, case_index, S)\n            sigma_means.append(m)\n        t_est = estimate_t(p_list, sigma_means, pc=pc)\n        # Round to three decimals for output\n        if np.isnan(t_est) or np.isinf(t_est):\n            t_est = -1.0\n        t_est_rounded = f\"{t_est:.3f}\"\n        results.append(t_est_rounded)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3171738"}]}