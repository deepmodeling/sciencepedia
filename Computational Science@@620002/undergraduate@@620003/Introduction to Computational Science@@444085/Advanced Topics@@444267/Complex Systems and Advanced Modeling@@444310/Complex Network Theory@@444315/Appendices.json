{"hands_on_practices": [{"introduction": "Networks often exhibit a modular organization, where nodes cluster into densely connected communities. This exercise [@problem_id:3108222] challenges you to uncover this structure using two distinct approaches: one based on the principle of structural similarity between nodes, and another using the popular Label Propagation Algorithm. By implementing both and comparing their outputs using Normalized Mutual Information ($NMI$), you will gain practical experience in community detection and develop a nuanced understanding of how different methods define and identify network modules.", "problem": "You are given simple, undirected, unweighted graphs with node labels from the set $\\{0,1,2,\\dots,n-1\\}$. For a node $i$, define its open neighborhood $N(i)$ as the set of nodes adjacent to $i$ (that is, $i \\notin N(i)$). From first principles of set theory and information theory, proceed as follows for each test graph:\n\n1. Using only the definitions of set intersection, set union, and set cardinality, compute a neighborhood-overlap similarity between every unordered pair of nodes $\\{i,j\\}$ based on how much their neighborhoods overlap relative to the total distinct neighbors across both neighborhoods. If the union of neighborhoods has cardinality $0$, define the pairwise similarity for $i \\neq j$ to be $0$ by convention, and define the self-similarity to be $1$ for all $i$.\n2. Transform these pairwise similarities into clusters by building a similarity graph on the same node set with an undirected edge between nodes $i$ and $j$ if and only if their similarity is greater than or equal to a given threshold $\\tau$. Define clusters as the connected components of this similarity graph.\n3. Independently compute a community assignment on the original graph using the Label Propagation Algorithm (LPA), defined as follows to ensure determinism. Initialize the label of each node $i$ to its own index $i$. Repeatedly perform full asynchronous sweeps over nodes in increasing index order; when visiting node $i$ with nonempty neighborhood $N(i)$, update its label to the most frequent label among $\\{ \\text{label}(u) : u \\in N(i) \\}$. Break ties by choosing the smallest label value. If $N(i)$ is empty, keep its current label. Terminate when a full sweep causes no label to change or when the number of sweeps reaches $100$.\n4. Quantify the consistency between the two partitions (the thresholded-similarity clusters and the Label Propagation Algorithm communities) using a normalized mutual information from information theory. Construct the empirical joint distribution of cluster labels across the two partitions from counts over the node set. Then compute the mutual information from this joint distribution and normalize it using the geometric mean of the entropies of the two partitions. If both partitions have zero entropy, define the normalized mutual information to be $1$. If exactly one partition has zero entropy while the other has nonzero entropy, define the normalized mutual information to be $0$. Express the result for each test graph as a real number in the closed interval $[0,1]$ rounded to $6$ decimal places.\n\nUse the following test suite of graphs and thresholds. Each graph is simple, undirected, and unweighted, with no self-loops.\n\n- Test graph $1$ with $n=8$ and threshold $\\tau=0.5$. Edge set\n  $E_1 = \\{(0,1),(0,2),(0,3),(1,2),(1,3),(2,3),(4,5),(4,6),(4,7),(5,6),(5,7),(6,7)\\}$.\n- Test graph $2$ with $n=8$ and threshold $\\tau=0.5$. Edge set\n  $E_2 = E_1 \\cup \\{(3,4)\\}$.\n- Test graph $3$ with $n=5$ and threshold $\\tau=0.3$. Edge set\n  $E_3 = \\{(0,1),(1,2),(2,0)\\}$, and nodes $3$ and $4$ are isolated.\n\nYour program must implement, for each test graph, all steps described above and produce a single line of output containing the normalized mutual information values for the three tests as a comma-separated list of floats rounded to $6$ decimal places, enclosed in square brackets, for example, $[x_1,x_2,x_3]$ where each $x_k$ is the result for test $k$ rounded to $6$ decimals.", "solution": "The problem statement has been meticulously validated and is determined to be valid. It is scientifically grounded in complex network theory, well-posed with deterministic algorithmic definitions, and objectively formulated. All necessary data and conventions are provided, ensuring a unique and verifiable solution exists.\n\nThe problem requires a four-step analysis for each of three test graphs: 1. Computation of a pairwise neighborhood-overlap similarity matrix. 2. Clustering based on thresholding this similarity matrix. 3. Community detection using a deterministic Label Propagation Algorithm (LPA). 4. Quantification of the agreement between the two resulting partitions using Normalized Mutual Information (NMI).\n\nThe solution proceeds by implementing each step from first principles, as stipulated.\n\n**Step 1: Neighborhood-Overlap Similarity**\n\nFor a given simple, undirected graph with $n$ nodes labeled $\\{0, 1, \\dots, n-1\\}$, let the open neighborhood of a node $i$ be the set $N(i)$ of its adjacent nodes. The similarity $S(i,j)$ between two distinct nodes $i$ and $j$ is defined by the overlap of their neighborhoods relative to the total number of distinct neighbors they have collectively. This is formally the Jaccard index of their neighborhood sets:\n$$\nS(i,j) = \\frac{|N(i) \\cap N(j)|}{|N(i) \\cup N(j)|}\n$$\nThe problem provides two conventions:\n1. If $|N(i) \\cup N(j)| = 0$ (i.e., both $i$ and $j$ are isolated) and $i \\neq j$, the similarity is $S(i,j) = 0$.\n2. The self-similarity for any node $i$ is $S(i,i) = 1$.\n\nAn $n \\times n$ symmetric similarity matrix $S$ is computed where each entry $S_{ij}$ stores the value $S(i,j)$.\n\n**Step 2: Thresholded Similarity Clustering**\n\nA partition of the nodes, which we will call Partition A, is derived from the similarity matrix $S$. A new \"similarity graph\" is constructed on the same $n$ nodes. An undirected edge is placed between nodes $i$ and $j$ if and only if their similarity $S(i,j)$ meets or exceeds a given threshold $\\tau$:\n$$\n(i,j) \\in E_{\\text{sim}} \\iff S(i,j) \\ge \\tau\n$$\nThe clusters are then defined as the connected components of this similarity graph. These components can be identified using a standard graph traversal algorithm, such as Breadth-First Search (BFS) or Depth-First Search (DFS). Each node is assigned a cluster label corresponding to the component it belongs to.\n\n**Step 3: Label Propagation Algorithm (LPA)**\n\nA second, independent partition of the nodes, Partition B, is obtained using the Label Propagation Algorithm (LPA). The algorithm is specified with deterministic rules to ensure a unique result.\n- **Initialization**: Each node $i$ is assigned its own index as its initial label, $L(i) \\leftarrow i$.\n- **Iteration**: The algorithm proceeds in sweeps. In each sweep, nodes are visited in increasing order of their indices, from $0$ to $n-1$. When visiting node $i$, its label is updated based on the labels of its neighbors in the original graph.\n- **Update Rule**: For a node $i$ with a non-empty neighborhood $N(i)$, its new label becomes the most frequent label among its neighbors, $\\{L(u) : u \\in N(i)\\}$. This update is asynchronous, meaning the label of a node $j  i$ updated in the current sweep is immediately used for the update of node $i$.\n- **Tie-Breaking**: If multiple labels share the highest frequency in the neighborhood, the one with the smallest numerical value is chosen.\n- **Isolated Nodes**: If $N(i)$ is empty, $L(i)$ remains unchanged.\n- **Termination**: The process stops when a full sweep over all nodes results in no label changes, or after a maximum of $100$ sweeps.\n\nThe final labels $L(i)$ for all nodes define the communities of Partition B.\n\n**Step 4: Normalized Mutual Information (NMI)**\n\nThe consistency between Partition A (from similarity clustering) and Partition B (from LPA) is quantified using Normalized Mutual Information (NMI). This requires concepts from information theory.\n\nGiven two partitions, A and B, of the $n$ nodes, we first construct their joint probability distribution. Let the clusters in A be $\\{a_k\\}$ and communities in B be $\\{b_l\\}$. The probability of a node belonging to cluster $a_k$ is $P(a_k) = |a_k|/n$, and to community $b_l$ is $P(b_l) = |b_l|/n$. The joint probability is $P(a_k, b_l) = |a_k \\cap b_l|/n$.\n\nThe Shannon entropy of a partition, say A, measures its information content or uncertainty:\n$$\nH(A) = - \\sum_{k} P(a_k) \\log P(a_k)\n$$\nThe mutual information $I(A,B)$ measures the shared information between the two partitions:\n$$\nI(A,B) = \\sum_{k} \\sum_{l} P(a_k, b_l) \\log \\frac{P(a_k, b_l)}{P(a_k)P(b_l)}\n$$\nThe NMI is the mutual information normalized by the geometric mean of the individual entropies, scaling the result to the range $[0,1]$:\n$$\n\\text{NMI}(A,B) = \\frac{I(A,B)}{\\sqrt{H(A)H(B)}}\n$$\nThe problem specifies conventions for cases where the denominator is zero:\n1. If $H(A)=0$ and $H(B)=0$ (both partitions are trivial, containing one group), $\\text{NMI} = 1$.\n2. If exactly one of $H(A)$ or $H(B)$ is zero (one partition is trivial, the other is not), $\\text{NMI} = 0$.\n\nThe final result for each test case is this NMI value, rounded to $6$ decimal places.", "answer": "```python\nimport numpy as np\n\ndef _calculate_similarity_matrix(adj, n):\n    \"\"\"Computes the Jaccard similarity matrix for node neighborhoods.\"\"\"\n    S = np.zeros((n, n), dtype=float)\n    for i in range(n):\n        S[i, i] = 1.0\n        for j in range(i + 1, n):\n            N_i = adj[i]\n            N_j = adj[j]\n            \n            intersection_size = len(N_i.intersection(N_j))\n            union_size = len(N_i.union(N_j))\n            \n            if union_size == 0:\n                similarity = 0.0\n            else:\n                similarity = intersection_size / union_size\n            \n            S[i, j] = S[j, i] = similarity\n    return S\n\ndef _get_similarity_clusters(S, n, tau):\n    \"\"\"Finds clusters as connected components of the thresholded similarity graph.\"\"\"\n    sim_adj = [set() for _ in range(n)]\n    for i in range(n):\n        for j in range(i + 1, n):\n            if S[i, j] >= tau:\n                sim_adj[i].add(j)\n                sim_adj[j].add(i)\n\n    visited = np.full(n, False, dtype=bool)\n    clusters = np.full(n, -1, dtype=int)\n    cluster_id = 0\n    for i in range(n):\n        if not visited[i]:\n            queue = [i]\n            visited[i] = True\n            while queue:\n                u = queue.pop(0)\n                clusters[u] = cluster_id\n                for v in sim_adj[u]:\n                    if not visited[v]:\n                        visited[v] = True\n                        queue.append(v)\n            cluster_id += 1\n    return clusters\n\ndef _get_most_frequent_label(neighbor_labels):\n    \"\"\"Finds the most frequent label, with tie-breaking by smallest value.\"\"\"\n    if not neighbor_labels:\n        return None\n    \n    counts = {}\n    for label in neighbor_labels:\n        counts[label] = counts.get(label, 0) + 1\n    \n    max_freq = 0\n    # In Python 3.7+ dicts are insertion ordered. No assumptions made here.\n    for count in counts.values():\n        if count > max_freq:\n            max_freq = count\n\n    tied_labels = []\n    for label, count in counts.items():\n        if count == max_freq:\n            tied_labels.append(label)\n    \n    return min(tied_labels)\n\ndef _run_lpa(adj, n):\n    \"\"\"Runs the deterministic Label Propagation Algorithm.\"\"\"\n    labels = np.arange(n)\n    max_sweeps = 100\n    \n    for _ in range(max_sweeps):\n        changed = False\n        for i in range(n):\n            neighbors = adj[i]\n            if not neighbors:\n                continue\n            \n            neighbor_labels = [labels[neighbor] for neighbor in neighbors]\n            new_label = _get_most_frequent_label(neighbor_labels)\n            \n            if labels[i] != new_label:\n                labels[i] = new_label\n                changed = True\n        \n        if not changed:\n            break\n            \n    return labels\n\ndef _calculate_nmi(partition_A, partition_B, n):\n    \"\"\"Computes the Normalized Mutual Information between two partitions.\"\"\"\n    unique_labels_A = np.unique(partition_A)\n    unique_labels_B = np.unique(partition_B)\n    num_clusters_A = len(unique_labels_A)\n    num_clusters_B = len(unique_labels_B)\n    \n    map_A = {label: i for i, label in enumerate(unique_labels_A)}\n    map_B = {label: i for i, label in enumerate(unique_labels_B)}\n\n    contingency = np.zeros((num_clusters_A, num_clusters_B), dtype=float)\n    for i in range(n):\n        a_idx = map_A[partition_A[i]]\n        b_idx = map_B[partition_B[i]]\n        contingency[a_idx, b_idx] += 1\n    \n    p_ab = contingency / n\n    p_a = np.sum(p_ab, axis=1)\n    p_b = np.sum(p_ab, axis=0)\n\n    # Calculate entropies\n    h_a = -np.sum(p_a[p_a > 0] * np.log(p_a[p_a > 0]))\n    h_b = -np.sum(p_b[p_b > 0] * np.log(p_b[p_b > 0]))\n\n    # Handle zero entropy cases as per problem convention\n    is_ha_zero = np.isclose(h_a, 0)\n    is_hb_zero = np.isclose(h_b, 0)\n    \n    if is_ha_zero and is_hb_zero:\n        return 1.0\n    if is_ha_zero or is_hb_zero:\n        return 0.0\n\n    # Calculate mutual information\n    i_ab = 0.0\n    for r in range(num_clusters_A):\n        for c in range(num_clusters_B):\n            if p_ab[r, c] > 0:\n                i_ab += p_ab[r, c] * np.log(p_ab[r, c] / (p_a[r] * p_b[c]))\n    \n    # Calculate NMI\n    nmi = i_ab / np.sqrt(h_a * h_b)\n    return nmi\n\ndef solve_one_case(n, edges, tau):\n    \"\"\"Processes a single test graph through all steps.\"\"\"\n    # Build adjacency list (list of sets for efficient operations)\n    adj = [set() for _ in range(n)]\n    for u, v in edges:\n        adj[u].add(v)\n        adj[v].add(u)\n    \n    # 1. Compute neighborhood-overlap similarity\n    similarity_matrix = _calculate_similarity_matrix(adj, n)\n    \n    # 2. Get clusters from similarity graph\n    partition_A = _get_similarity_clusters(similarity_matrix, n, tau)\n    \n    # 3. Get communities from LPA\n    partition_B = _run_lpa(adj, n)\n    \n    # 4. Quantify consistency with NMI\n    nmi = _calculate_nmi(partition_A, partition_B, n)\n    \n    return nmi\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    E1 = {(0,1),(0,2),(0,3),(1,2),(1,3),(2,3),(4,5),(4,6),(4,7),(5,6),(5,7),(6,7)}\n    E2 = E1.union({(3,4)})\n    E3 = {(0,1),(1,2),(2,0)}\n\n    test_cases = [\n        (8, E1, 0.5),\n        (8, E2, 0.5),\n        (5, E3, 0.3)\n    ]\n\n    results = []\n    for n, edges, tau in test_cases:\n        result = solve_one_case(n, edges, tau)\n        results.append(f\"{result:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3108222"}, {"introduction": "How can we characterize massive networks that are too large to analyze in their entirety? This practice [@problem_id:3108301] delves into the critical task of network sampling by implementing a random walk to estimate the degree distribution. You will discover firsthand the 'friendship paradox'—a sampling bias where random walks preferentially visit high-degree nodes—and then apply the powerful technique of importance sampling to correct this bias and achieve an accurate estimate. This exercise is key to understanding the challenges and solutions for working with large-scale network data.", "problem": "You are given the task of implementing a simple random walk sampler on undirected graphs and using it to estimate the degree distribution of the graph. The degree distribution is denoted by $P(k)$, the probability that a uniformly chosen node has degree $k$. A simple random walk is biased toward visiting high-degree nodes, a phenomenon related to the friendship paradox. Your goal is to quantify this bias and correct it by using importance sampling weights derived from first principles. The final deliverable is a complete, runnable program that performs the required computations for a specified test suite and outputs a single line with aggregate numerical results.\n\nDefinitions and foundational base:\n- Let $G = (V, E)$ be a finite, simple, undirected, connected graph. Let $|V| = n$ and $|E| = m$.\n- For $i \\in V$, let $k_i$ denote the degree of node $i$, that is, the number of neighbors of node $i$.\n- The degree distribution is $P(k) = \\frac{1}{n}\\sum_{i \\in V} \\mathbf{1}\\{k_i = k\\}$, where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n- A simple random walk on $G$ is a Markov chain on $V$ such that from node $i$ at time $t$, the next state at time $t+1$ is a uniformly random neighbor of $i$.\n- The stationary distribution $\\pi$ of a simple random walk on an undirected graph exists and satisfies detailed balance. From detailed balance and the undirectedness of $G$, one can derive that $\\pi_i$ is proportional to $k_i$, hence $\\pi_i = \\frac{k_i}{2m}$, where $2m = \\sum_{i \\in V} k_i$.\n- Because the random walk samples nodes according to $\\pi$, degrees are sampled with a size bias. The friendship paradox follows from this size bias: neighbors tend to have higher expected degree than a uniformly sampled node.\n- To estimate $P(k)$ from the random walk samples, you must correct the size bias. Using importance sampling based on the fact that the sampling probability is proportional to $k_i$, you should design a self-normalized estimator that uses weights inversely proportional to $k_i$ so that the bias cancels out while ensuring the estimator is a valid probability distribution.\n\nTasks to implement:\n1. Implement a function to simulate a simple random walk on a given undirected graph specified by an adjacency list. The walk must:\n   - Initialize the starting node by sampling uniformly at random from $V$.\n   - Evolve for a burn-in of $B$ steps, which you must discard.\n   - Then collect $S$ subsequent node visits as samples.\n   - Use a pseudorandom number generator initialized with a fixed seed for reproducibility.\n2. Compute the exact degree distribution $P(k)$ of the graph by enumeration over all nodes.\n3. Compute the naive estimator $\\widehat{P}_{\\text{naive}}(k)$ from the $S$ visited nodes by taking the empirical frequency of observed degrees.\n4. Derive and implement a self-normalized importance sampling estimator $\\widehat{P}_{\\text{w}}(k)$ that corrects the bias implied by $\\pi_i \\propto k_i$. Justify the use of weights inversely proportional to $k_i$ based on the fundamental base described above, and implement this estimator in code.\n5. Quantify the estimation error for both estimators by the $L_1$ distance (also called the $L_1$ norm distance) to the true degree distribution:\n   $$\\|\\widehat{P} - P\\|_1 = \\sum_{k \\in \\mathcal{K}} \\left|\\widehat{P}(k) - P(k)\\right|,$$\n   where $\\mathcal{K}$ is the union of all degrees present in the graph. Report this error for both the naive and the weighted estimators.\n6. Rounding rule: For each test case, round both $L_1$ errors to $6$ decimal places before producing the final output.\n\nTest suite:\nImplement and run your program on the following graphs and parameters. All graphs are simple, undirected, and connected. Represent graphs by their adjacency lists, where each node is labeled by an integer. All numerical values are to be interpreted exactly as given.\n\n- Test case $1$ (heterogeneous hub-and-spoke structure: a star graph)\n  - Graph: star on $n = 8$ nodes, with center at node $0$ connected to nodes $1,2,3,4,5,6,7$. Degrees are: node $0$ has degree $7$; nodes $1$ through $7$ each have degree $1$.\n  - Burn-in $B = 1000$\n  - Samples $S = 200000$\n  - Random seed $= 314159$\n\n- Test case $2$ (regular graph: a cycle)\n  - Graph: cycle on $n = 10$ nodes labeled $0,1,\\dots,9$, with edges $\\{(0,1),(1,2),(2,3),(3,4),(4,5),(5,6),(6,7),(7,8),(8,9),(9,0)\\}$. Each node has degree $2$.\n  - Burn-in $B = 1000$\n  - Samples $S = 100000$\n  - Random seed $= 271828$\n\n- Test case $3$ (heterogeneous connected graph)\n  - Nodes $= \\{0,1,2,3,4,5,6,7,8,9,10,11\\}$\n  - Edges consist of a chain plus extra links:\n    - Chain edges: $(0,1),(1,2),(2,3),(3,4),(4,5),(5,6),(6,7),(7,8),(8,9),(9,10),(10,11)$\n    - Additional edges: $(5,0),(5,2),(5,7),(5,9),(3,8),(3,10),(1,9),(11,7)$\n    - This yields degrees: $k_0=2$, $k_1=3$, $k_2=3$, $k_3=4$, $k_4=2$, $k_5=6$, $k_6=2$, $k_7=4$, $k_8=3$, $k_9=4$, $k_{10}=3$, $k_{11}=2$.\n  - Burn-in $B = 1000$\n  - Samples $S = 200000$\n  - Random seed $= 424242$\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the three test cases as a list of pairs, one pair per test case. Each pair is the $L_1$ errors for the naive and the weighted estimators, respectively, rounded to $6$ decimal places.\n- For example, the output should look like $[[a_1,b_1],[a_2,b_2],[a_3,b_3]]$, where $a_i$ is the naive error and $b_i$ is the weighted error for test case $i$.\n- The output must be exactly one line, with no extra text, spaces are optional, and the numbers must be decimals with exactly $6$ digits after the decimal point.", "solution": "We begin from the fundamental definitions and a well-known property of simple random walks on undirected graphs. Let $G=(V,E)$ be a finite, simple, undirected, connected graph. For a node $i \\in V$ with degree $k_i$, the simple random walk transitions from $i$ to any neighbor $j$ with probability $1/k_i$. A stationary distribution $\\pi$ is a vector that satisfies $\\pi = \\pi P$, where $P$ is the transition matrix. For undirected graphs, detailed balance provides a core identity:\n$$\n\\pi_i P_{ij} = \\pi_j P_{ji} \\quad \\text{for all edges } (i,j)\\in E.\n$$\nSince $P_{ij} = 1/k_i$ and $P_{ji} = 1/k_j$ for edges $(i,j)$, detailed balance implies\n$$\n\\frac{\\pi_i}{k_i} = \\frac{\\pi_j}{k_j} \\quad \\text{whenever } (i,j) \\in E,\n$$\nand by connectivity, this ratio is constant over $V$. Let that constant be $c$. Then $\\pi_i = c k_i$. Normalization to $\\sum_{i \\in V} \\pi_i = 1$ yields\n$$\n\\pi_i = \\frac{k_i}{\\sum_{\\ell \\in V} k_\\ell} = \\frac{k_i}{2m}.\n$$\nTherefore, in stationarity, the probability that the random walk visits node $i$ at any step is $\\pi_i$, which is proportional to $k_i$. This immediately shows the size bias: the induced degree distribution in the walk’s samples is not $P(k)$ but\n$$\nQ(k) = \\sum_{i \\in V : k_i = k} \\pi_i = \\sum_{i \\in V : k_i = k} \\frac{k_i}{2m} = \\frac{k}{2m} \\cdot \\#\\{i \\in V : k_i = k\\}.\n$$\nSince $P(k) = \\frac{1}{n}\\#\\{i \\in V : k_i = k\\}$, we have $Q(k) \\propto k \\, P(k)$, that is, the random walk samples degrees from a size-biased version of $P(k)$ that upweights larger $k$. This explains the friendship paradox: the average degree of a randomly chosen neighbor is higher than the average degree of a uniformly sampled node.\n\nTo estimate $P(k)$ from random walk samples, we must correct the bias induced by $\\pi$. Importance sampling provides the principled approach. If samples are drawn according to a distribution proportional to $k_i$, then a Horvitz–Thompson style correction reweights each sample inversely proportional to its selection probability. Since $\\pi_i \\propto k_i$, the appropriate importance weight is proportional to $1/k_i$. Using self-normalized importance sampling, for a set of sampled nodes $X_1, X_2, \\dots, X_S$ and their degrees $K_s = k_{X_s}$, define weights $w_s = 1/K_s$. The self-normalized estimator is\n$$\n\\widehat{P}_{\\mathrm{w}}(k) = \\frac{\\sum_{s=1}^{S} w_s \\, \\mathbf{1}\\{K_s = k\\}}{\\sum_{s=1}^{S} w_s}.\n$$\nThis estimator is a valid probability mass function by construction (nonnegative and sums to $1$), and it cancels the size bias because $w_s$ inverts the factor $K_s$ arising from the sampling distribution. In contrast, the naive estimator\n$$\n\\widehat{P}_{\\mathrm{naive}}(k) = \\frac{1}{S} \\sum_{s=1}^{S} \\mathbf{1}\\{K_s = k\\}\n$$\nis biased toward larger degrees under the random walk sampling.\n\nTo quantify estimation error, we use the $L_1$ distance between the true degree distribution $P(k)$ and an estimate $\\widehat{P}(k)$:\n$$\n\\|\\widehat{P} - P\\|_1 = \\sum_{k \\in \\mathcal{K}} \\left|\\widehat{P}(k) - P(k)\\right|,\n$$\nwhere $\\mathcal{K}$ is the union of degrees present in the graph. This metric is interpretable as twice the total variation distance and lies in $[0,2]$.\n\nAlgorithmic design:\n- Compute exact degrees $\\{k_i\\}_{i\\in V}$ from the adjacency list and obtain the exact $P(k)$ by enumeration over $V$.\n- Simulate a simple random walk:\n  - Initialize the current node uniformly at random from $V$ using a pseudorandom number generator with a fixed seed for reproducibility.\n  - For $B$ burn-in steps, move to a uniformly random neighbor of the current node; do not record samples.\n  - Then for $S$ steps, repeat the move and record the visited node (or its degree).\n- From the collected degrees $K_1,\\dots,K_S$, compute:\n  - $\\widehat{P}_{\\mathrm{naive}}(k)$ as the empirical frequency over the observed degrees.\n  - $\\widehat{P}_{\\mathrm{w}}(k)$ using self-normalized importance weights $w_s = 1/K_s$.\n- Compute $L_1$ errors $\\|\\widehat{P}_{\\mathrm{naive}} - P\\|_1$ and $\\|\\widehat{P}_{\\mathrm{w}} - P\\|_1$ using the union support of degrees in the graph.\n- For numerical stability and a clean output, round both errors to $6$ decimal places.\n\nExpected qualitative outcomes for the test suite:\n- Star graph (test case $1$): The naive estimator will significantly overestimate the mass at high degrees and underestimate at degree $1$. The weighted estimator should markedly reduce the error.\n- Cycle graph (test case $2$): The graph is regular with $k_i = 2$ for all $i$, so there is no size bias. Both naive and weighted estimators should match the truth closely, yielding near-zero error.\n- Heterogeneous graph (test case $3$): The naive estimator will again be biased toward larger degrees; the weighted estimator should reduce the error compared to the naive one.\n\nThe program will implement these steps for the specified test suite and print a single line with the three pairs of $L_1$ errors, each pair as $[\\text{naive},\\text{weighted}]$, with each number formatted to exactly $6$ decimal places, in the form $[[a_1,b_1],[a_2,b_2],[a_3,b_3]]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef build_star_graph(n):\n    # Node 0 is center; nodes 1..n-1 are leaves\n    adj = [[] for _ in range(n)]\n    for v in range(1, n):\n        adj[0].append(v)\n        adj[v].append(0)\n    return adj\n\ndef build_cycle_graph(n):\n    adj = [[] for _ in range(n)]\n    for i in range(n):\n        j = (i + 1) % n\n        adj[i].append(j)\n        adj[j].append(i)\n    return adj\n\ndef build_custom_graph_case3():\n    # Nodes 0..11\n    n = 12\n    edges = []\n    # Chain edges\n    chain_edges = [(0,1),(1,2),(2,3),(3,4),(4,5),(5,6),(6,7),(7,8),(8,9),(9,10),(10,11)]\n    edges.extend(chain_edges)\n    # Additional edges\n    extra_edges = [(5,0),(5,2),(5,7),(5,9),(3,8),(3,10),(1,9),(11,7)]\n    edges.extend(extra_edges)\n    adj = [[] for _ in range(n)]\n    for u,v in edges:\n        adj[u].append(v)\n        adj[v].append(u)\n    return adj\n\ndef degrees_from_adj(adj):\n    return np.array([len(neigh) for neigh in adj], dtype=int)\n\ndef true_degree_distribution(adj):\n    degs = degrees_from_adj(adj)\n    n = len(degs)\n    unique, counts = np.unique(degs, return_counts=True)\n    pk = {int(k): c / n for k, c in zip(unique, counts)}\n    return pk\n\ndef random_walk_samples(adj, burn_in, samples, seed):\n    rng = np.random.default_rng(seed)\n    n = len(adj)\n    if n == 0:\n        return []\n    current = int(rng.integers(0, n))\n    # Burn-in\n    for _ in range(burn_in):\n        neighbors = adj[current]\n        current = int(neighbors[rng.integers(0, len(neighbors))])\n    # Sampling\n    visited = []\n    for _ in range(samples):\n        neighbors = adj[current]\n        current = int(neighbors[rng.integers(0, len(neighbors))])\n        visited.append(current)\n    return visited\n\ndef naive_degree_estimate(adj, visited_nodes):\n    degs = degrees_from_adj(adj)\n    sample_degrees = degs[visited_nodes]\n    if len(sample_degrees) == 0:\n        return {}\n    unique, counts = np.unique(sample_degrees, return_counts=True)\n    total = sample_degrees.size\n    est = {int(k): c / total for k, c in zip(unique, counts)}\n    return est\n\ndef weighted_degree_estimate(adj, visited_nodes):\n    degs = degrees_from_adj(adj)\n    sample_degrees = degs[visited_nodes].astype(float)\n    if len(sample_degrees) == 0:\n        return {}\n    weights = 1.0 / sample_degrees\n    # Self-normalized importance sampling over observed degrees\n    unique_degrees = np.unique(sample_degrees).astype(int)\n    weight_sum = np.sum(weights)\n    # Avoid division by zero, though degrees are >= 1 in simple graphs\n    if weight_sum == 0.0:\n        return {int(k): 0.0 for k in unique_degrees}\n    est = {}\n    for k in unique_degrees:\n        mask = (sample_degrees == k)\n        wk = np.sum(weights[mask])\n        est[int(k)] = wk / weight_sum\n    return est\n\ndef l1_distance(true_pk, est_pk, support=None):\n    # support is union of keys if None\n    if support is None:\n        support = set(true_pk.keys()).union(set(est_pk.keys()))\n    dist = 0.0\n    for k in support:\n        dist += abs(est_pk.get(k, 0.0) - true_pk.get(k, 0.0))\n    return dist\n\ndef format_results(results):\n    # results is list of (naive_error, weighted_error) floats\n    # Return a string like [[0.123456,0.000001],[...],...]\n    pairs = []\n    for a, b in results:\n        pairs.append(f\"[{a:.6f},{b:.6f}]\")\n    return \"[\" + \",\".join(pairs) + \"]\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = []\n\n    # Test case 1: Star graph with n=8\n    adj1 = build_star_graph(8)\n    burn_in1 = 1000\n    samples1 = 200000\n    seed1 = 314159\n    test_cases.append((adj1, burn_in1, samples1, seed1))\n\n    # Test case 2: Cycle graph with n=10\n    adj2 = build_cycle_graph(10)\n    burn_in2 = 1000\n    samples2 = 100000\n    seed2 = 271828\n    test_cases.append((adj2, burn_in2, samples2, seed2))\n\n    # Test case 3: Custom heterogeneous graph\n    adj3 = build_custom_graph_case3()\n    burn_in3 = 1000\n    samples3 = 200000\n    seed3 = 424242\n    test_cases.append((adj3, burn_in3, samples3, seed3))\n\n    results = []\n    for adj, burn_in, samples, seed in test_cases:\n        # True distribution\n        pk_true = true_degree_distribution(adj)\n        # Samples\n        visited = random_walk_samples(adj, burn_in, samples, seed)\n        # Naive estimate\n        pk_naive = naive_degree_estimate(adj, visited)\n        # Weighted estimate\n        pk_weighted = weighted_degree_estimate(adj, visited)\n        # L1 distances over true support\n        support = set(pk_true.keys())\n        e_naive = l1_distance(pk_true, pk_naive, support=support)\n        e_weighted = l1_distance(pk_true, pk_weighted, support=support)\n        results.append((e_naive, e_weighted))\n\n    # Final print statement in the exact required format.\n    print(format_results(results))\n\nsolve()\n```", "id": "3108301"}, {"introduction": "Networks are dynamic entities, constantly evolving as new connections form. This practice [@problem_id:3108231] introduces you to the predictive challenge of link prediction: forecasting which new edges are likely to appear. You will implement and benchmark three foundational methods, ranging from simple local heuristics like Common Neighbors to a more global approach based on spectral embeddings. This hands-on comparison will build your skills in developing and evaluating predictive models for evolving network structures.", "problem": "You are given a temporal link prediction task on simple, undirected, unweighted networks. The goal is to implement three link prediction scoring functions and benchmark them on multiple temporal snapshots against ground truth future edges. Your program must compute a ranking over all possible non-edges in the first snapshot and evaluate how well each method identifies the edges that actually appear in the second snapshot.\n\nFundamental base for reasoning and implementation:\n- Use the definitions of a simple undirected graph, a node neighbor set, and node degree, all derived from the adjacency relation of the first snapshot.\n- Use the notion of a similarity score between node pairs derived from topology (common neighbors and degree-aware weighting) and from a spectral embedding of the graph.\n\nRequired methods to implement using only the first snapshot at time $t=0$:\n- Common neighbors: assign a score to each candidate pair based solely on the size of the intersection of their neighbor sets in the first snapshot.\n- Adamic–Adar: assign a score to each candidate pair based on the common neighbors in the first snapshot, with larger-degree common neighbors contributing less than smaller-degree ones.\n- Embedding-based method: construct a $d$-dimensional spectral embedding of the nodes from the first snapshot by taking the leading $d$ eigenvectors of a symmetric normalization of the adjacency structure, then score pairs by cosine similarity of their embedding vectors.\n\nEvaluation protocol:\n- Candidate pairs are all unordered node pairs $(u,v)$ with $uv$ that are not edges in the first snapshot at time $t=0$.\n- Ground truth positives are exactly those candidate pairs that become edges in the second snapshot at time $t=1$ but were not edges at time $t=0$.\n- Let $L$ be the number of such ground-truth positive pairs (a nonnegative integer). Compute “precision at top-$L$” as the fraction of correctly identified positives within the top $L$ pairs of the sorted ranking, where sorting is by descending score, with deterministic tie-breaking by lexicographic order of pairs $(u,v)$ (ascending by $u$, then by $v$).\n- If a pair’s embedding vector has zero norm in the embedding-based method, define its cosine similarity with any other vector to be $0$.\n\nInput data to be hard-coded in your program (test suite):\n- Case $1$:\n  - Nodes: $\\{0,1,2,3,4,5,6\\}$.\n  - Edges at time $t=0$: $\\{(0,1),(0,2),(1,2),(2,3),(3,4),(4,5)\\}$.\n  - Edges at time $t=1$: add $\\{(0,3),(1,3),(5,6)\\}$ to the edges present at time $t=0$.\n  - Embedding dimension: $d=2$.\n- Case $2$:\n  - Nodes: $\\{0,1,2,3,4,5\\}$.\n  - Edges at time $t=0$: $\\{(0,1),(0,2),(0,3),(3,4),(4,5)\\}$.\n  - Edges at time $t=1$: add $\\{(1,2),(1,3),(2,3)\\}$ to the edges present at time $t=0$.\n  - Embedding dimension: $d=2$.\n- Case $3$:\n  - Nodes: $\\{0,1,2,3,4\\}$.\n  - Edges at time $t=0$: $\\{(0,1)\\}$.\n  - Edges at time $t=1$: add $\\{(2,3)\\}$ to the edges present at time $t=0$.\n  - Embedding dimension: $d=1$.\n\nOutput required:\n- For each case, compute the “precision at top-$L$” for each method in the order: common neighbors, Adamic–Adar, embedding-based. This yields $3$ floating-point results per case.\n- Aggregate the results for all cases into a single list of $9$ floating-point numbers, in the order: case $1$ (common neighbors, Adamic–Adar, embedding-based), case $2$ (common neighbors, Adamic–Adar, embedding-based), case $3$ (common neighbors, Adamic–Adar, embedding-based).\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets; each number must be rounded to exactly $3$ decimal places. For example, print a line of the form [$r_1$,$r_2$,$r_3$,$r_4$,$r_5$,$r_6$,$r_7$,$r_8$,$r_9$], where each $r_i$ is a floating-point number with exactly $3$ digits after the decimal point.\n\nNotes and constraints:\n- All graphs are simple, undirected, and unweighted.\n- Angles, if any are used internally (e.g., in cosine similarity), are unitless and not part of the required output.\n- The final output is purely numerical; no physical units are involved.\n- Ensure deterministic tie-breaking by lexicographic order of pairs $(u,v)$ when scores tie.", "solution": "The problem presented is a well-defined task in computational network science, specifically temporal link prediction. It asks for the implementation and evaluation of three standard link prediction scoring functions on a set of network snapshots. The problem is scientifically grounded, objective, and provides all necessary information for a unique solution.\n\nThe core of the problem is to predict future connections in a network based on its current topology. We are given a simple, undirected, unweighted graph at an initial time $t=0$, represented by its set of nodes $V$ and edges $E_0$. We are also given the set of new edges, $P$, that appear at a future time $t=1$. The goal is to rank all non-existent edges at $t=0$, known as candidate pairs, by their likelihood of appearing at $t=1$.\n\nLet the graph at time $t=0$ be $G_0=(V, E_0)$. A candidate pair is any unordered pair of nodes $(u,v)$ such that $u  v$ and $(u,v) \\notin E_0$. The set of ground truth positives, denoted $P$, consists of all candidate pairs that form an edge at $t=1$. The size of this set is $L = |P|$. We must implement three scoring methods, rank the candidate pairs in descending order of their scores, and evaluate the performance of each method using \"precision at top-$L$\". This metric is defined as the fraction of the top $L$ predicted links that are in the ground truth set $P$.\n\nThe three required scoring methods are:\n\n1.  **Common Neighbors (CN)**: This is a local similarity metric based on the principle of triadic closure, which posits that two nodes are more likely to connect if they share a common neighbor. The score for a pair $(u,v)$ is the number of neighbors they share in $G_0$:\n    $$S_{CN}(u,v) = |\\Gamma_0(u) \\cap \\Gamma_0(v)|$$\n    where $\\Gamma_0(x)$ is the set of neighbors of node $x$ in the graph $G_0$.\n\n2.  **Adamic–Adar (AA)**: This method refines the common neighbors score by assigning more weight to common neighbors that are themselves less connected. The intuition is that sharing a low-degree neighbor is a stronger indicator of similarity than sharing a high-degree hub. The score is calculated by summing the inverse logarithmic degrees of the common neighbors:\n    $$S_{AA}(u,v) = \\sum_{z \\in \\Gamma_0(u) \\cap \\Gamma_0(v)} \\frac{1}{\\log k_z}$$\n    where $k_z$ is the degree of the common neighbor $z$ in $G_0$. Since any common neighbor $z$ of $u$ and $v$ must have edges to both, its degree $k_z$ is at least $2$, ensuring $\\log k_z  0$.\n\n3.  **Embedding-based (EB) Method**: This is a global method that captures the network's structure by embedding nodes into a low-dimensional vector space. The procedure is as follows:\n    a. Construct the adjacency matrix $A$ and the diagonal degree matrix $D$ from the graph $G_0$.\n    b. Compute the symmetrically normalized adjacency matrix, defined as $A_{sym} = D^{-1/2} A D^{-1/2}$. For any node $i$ with degree $k_i=0$, the corresponding diagonal entry of $D^{-1/2}$ is taken to be $0$, which results in the $i$-th row and column of $A_{sym}$ being zero.\n    c. Perform an eigendecomposition of the symmetric matrix $A_{sym}$. The node embeddings are constructed from the $d$ eigenvectors corresponding to the $d$ largest eigenvalues. These eigenvectors form the columns of an $N \\times d$ embedding matrix, where $N = |V|$. The embedding for node $u$ is the $u$-th row of this matrix, denoted $\\vec{x}_u$.\n    d. The score for a pair $(u,v)$ is the cosine similarity of their embedding vectors:\n    $$S_{EB}(u,v) = \\frac{\\vec{x}_u \\cdot \\vec{x}_v}{\\|\\vec{x}_u\\| \\|\\vec{x}_v\\|}$$\n    As per the problem specification, if either $\\|\\vec{x}_u\\|$ or $\\|\\vec{x}_v\\|$ is zero, the score is defined as $0$.\n\nFor each of these methods, a list of candidate pairs and their scores is generated. This list is then sorted in descending order of score. Any ties in score are broken deterministically by sorting the pairs $(u,v)$ in lexicographical order (ascending by $u$, then by $v$). The top $L$ pairs from this sorted list constitute the predictions. The precision is then calculated as:\n$$\\text{Precision@L} = \\frac{|\\{\\text{Top } L \\text{ predicted pairs}\\} \\cap P|}{L}$$\n\nThis entire procedure is applied to each of the three test cases provided. The final output is an aggregation of the precision scores for all methods across all cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh\n\ndef solve():\n    \"\"\"\n    Computes link prediction precision for three methods on three test cases.\n    \"\"\"\n\n    def solve_case(nodes, edges_t0, new_edges_t1, d):\n        \"\"\"\n        Processes a single test case for link prediction evaluation.\n\n        Args:\n            nodes (list): A list of node identifiers.\n            edges_t0 (list of tuples): Edges in the graph at time t=0.\n            new_edges_t1 (list of tuples): Edges added at time t=1.\n            d (int): The dimension for the spectral embedding.\n\n        Returns:\n            list: A list of three float values representing the precision-at-top-L\n                  for Common Neighbors, Adamic-Adar, and Embedding-based methods.\n        \"\"\"\n        num_nodes = len(nodes)\n\n        # 1. Build graph representations at t=0\n        adj_matrix = np.zeros((num_nodes, num_nodes), dtype=int)\n        adj_list = {i: set() for i in range(num_nodes)}\n        for u, v in edges_t0:\n            adj_matrix[u, v] = 1\n            adj_matrix[v, u] = 1\n            adj_list[u].add(v)\n            adj_list[v].add(u)\n        \n        degrees = np.sum(adj_matrix, axis=1)\n\n        # 2. Identify candidate and ground truth pairs\n        candidate_pairs = []\n        for i in range(num_nodes):\n            for j in range(i + 1, num_nodes):\n                if adj_matrix[i, j] == 0:\n                    candidate_pairs.append((i, j))\n        \n        ground_truth_positives = {tuple(sorted(edge)) for edge in new_edges_t1}\n        L = len(ground_truth_positives)\n        \n        # If L is 0, precision is typically considered 1.0 or undefined.\n        # The problem cases ensure L > 0.\n        if L == 0:\n            return [1.0, 1.0, 1.0]\n\n        precisions = []\n\n        # -- Method 1: Common Neighbors --\n        cn_scores = []\n        for u, v in candidate_pairs:\n            score = float(len(adj_list[u].intersection(adj_list[v])))\n            cn_scores.append((score, u, v))\n\n        cn_scores.sort(key=lambda x: (-x[0], x[1], x[2]))\n        top_L_cn = { (u,v) for score, u, v in cn_scores[:L] }\n        hits_cn = len(top_L_cn.intersection(ground_truth_positives))\n        precisions.append(hits_cn / L)\n\n        # -- Method 2: Adamic-Adar --\n        aa_scores = []\n        for u, v in candidate_pairs:\n            score = 0.0\n            common_neighbors = adj_list[u].intersection(adj_list[v])\n            for z in common_neighbors:\n                # Degree of a common neighbor is at least 2, so log(deg) > 0.\n                score += 1 / np.log(degrees[z])\n            aa_scores.append((score, u, v))\n        \n        aa_scores.sort(key=lambda x: (-x[0], x[1], x[2]))\n        top_L_aa = { (u,v) for score, u, v in aa_scores[:L] }\n        hits_aa = len(top_L_aa.intersection(ground_truth_positives))\n        precisions.append(hits_aa / L)\n\n        # -- Method 3: Embedding-based --\n        eb_scores = []\n        # Calculate embedding only if dimension d is positive\n        if d > 0:\n            D_inv_sqrt_diag = [1/np.sqrt(deg) if deg > 0 else 0 for deg in degrees]\n            D_inv_sqrt = np.diag(D_inv_sqrt_diag)\n            A_sym = D_inv_sqrt @ adj_matrix @ D_inv_sqrt\n            \n            # eigh returns eigenvalues in ascending order.\n            _, eigenvectors = eigh(A_sym)\n            \n            # Take eigenvectors corresponding to the d largest eigenvalues.\n            embedding = eigenvectors[:, -d:]\n\n            for u, v in candidate_pairs:\n                vec_u = embedding[u]\n                vec_v = embedding[v]\n                norm_u = np.linalg.norm(vec_u)\n                norm_v = np.linalg.norm(vec_v)\n                \n                if norm_u == 0 or norm_v == 0:\n                    score = 0.0\n                else:\n                    score = np.dot(vec_u, vec_v) / (norm_u * norm_v)\n                eb_scores.append((score, u, v))\n        else: # If d=0, embedding is trivial, all scores are 0.\n            for u, v in candidate_pairs:\n                eb_scores.append((0.0, u, v))\n\n        eb_scores.sort(key=lambda x: (-x[0], x[1], x[2]))\n        top_L_eb = { (u,v) for score, u, v in eb_scores[:L] }\n        hits_eb = len(top_L_eb.intersection(ground_truth_positives))\n        precisions.append(hits_eb / L)\n        \n        return precisions\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"nodes\": list(range(7)),\n            \"edges_t0\": [(0, 1), (0, 2), (1, 2), (2, 3), (3, 4), (4, 5)],\n            \"new_edges_t1\": [(0, 3), (1, 3), (5, 6)],\n            \"d\": 2\n        },\n        {\n            \"nodes\": list(range(6)),\n            \"edges_t0\": [(0, 1), (0, 2), (0, 3), (3, 4), (4, 5)],\n            \"new_edges_t1\": [(1, 2), (1, 3), (2, 3)],\n            \"d\": 2\n        },\n        {\n            \"nodes\": list(range(5)),\n            \"edges_t0\": [(0, 1)],\n            \"new_edges_t1\": [(2, 3)],\n            \"d\": 1\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        results = solve_case(case[\"nodes\"], case[\"edges_t0\"], case[\"new_edges_t1\"], case[\"d\"])\n        all_results.extend(results)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{x:.3f}\" for x in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3108231"}]}