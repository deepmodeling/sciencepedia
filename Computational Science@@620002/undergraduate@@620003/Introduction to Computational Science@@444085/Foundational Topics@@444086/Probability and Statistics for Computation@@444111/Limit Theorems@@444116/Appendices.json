{"hands_on_practices": [{"introduction": "A core challenge in computational science is determining how much data is sufficient for a reliable estimate. This first practice explores this question in the context of a real-time control system, where precision must be balanced against latency [@problem_id:3153047]. By applying the Central Limit Theorem, you will determine the optimal number of sensor readings needed to meet a strict error tolerance without violating a critical time deadline, demonstrating a foundational trade-off in system design.", "problem": "A real-time controller must estimate a constant signal by averaging $n$ independent and identically distributed sensor readings before issuing an actuation command. Each reading is modeled as $X_{i}=\\theta+\\eta_{i}$, where $\\theta$ is the constant signal over the averaging window and $\\eta_{i}$ are independent noise terms with mean $0$ and standard deviation $\\sigma$. The controller uses the sample mean $\\bar{X}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$ as the estimate. Collecting each reading requires a fixed time $\\Delta t$, so the sensing-to-actuation latency equals $n\\Delta t$. The controller must respect a hard deadline $T_{\\max}$, so $n\\Delta t\\leq T_{\\max}$. The design requirement is that, at the moment of actuation, the absolute estimation error satisfies $|\\bar{X}_{n}-\\theta|\\leq \\varepsilon$ with probability at least $p_{0}$, justified by the Law of Large Numbers (LLN) and quantified using the Central Limit Theorem (CLT) approximation for large $n$.\n\nYou are given:\n- Noise standard deviation $\\sigma=0.8$.\n- Tolerance $\\varepsilon=0.10$.\n- Target confidence $p_{0}=0.95$.\n- Per-sample time $\\Delta t=2\\times 10^{-3}$ seconds.\n- Maximum allowable latency $T_{\\max}=0.50$ seconds.\n\nStarting from the definitions of the sample mean and variance, and invoking the Central Limit Theorem to approximate the distribution of $\\bar{X}_{n}$, derive the smallest integer $n$ that simultaneously satisfies the probability requirement and the latency constraint. Report the integer value of $n$ only.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and all necessary information is provided. The problem is a standard application of the Central Limit Theorem (CLT) to determine the required sample size for an estimation problem, subject to a latency constraint. All parameters are provided and are physically consistent. The problem is deemed valid.\n\nThe solution proceeds as follows. Each sensor reading $X_i$ is a random variable given by $X_{i}=\\theta+\\eta_{i}$, where $\\theta$ is a constant signal and $\\eta_{i}$ are independent and identically distributed (i.i.d.) noise terms with mean $E[\\eta_i] = 0$ and standard deviation $\\sigma$.\n\nFirst, we determine the statistical properties of a single reading $X_i$:\nThe expected value of $X_i$ is $E[X_i] = E[\\theta + \\eta_i] = \\theta + E[\\eta_i] = \\theta + 0 = \\theta$.\nThe variance of $X_i$ is $Var(X_i) = Var(\\theta + \\eta_i) = Var(\\eta_i) = \\sigma^2$.\n\nThe estimate for $\\theta$ is the sample mean of $n$ readings, $\\bar{X}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$. We determine the statistical properties of this estimator.\nThe expected value of the sample mean is $E[\\bar{X}_n] = E\\left[\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\right] = \\frac{1}{n}\\sum_{i=1}^{n}E[X_i] = \\frac{1}{n}(n\\theta) = \\theta$. This confirms that the sample mean is an unbiased estimator of $\\theta$.\n\nThe variance of the sample mean, given that the $X_i$ are independent, is:\n$$Var(\\bar{X}_n) = Var\\left(\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\right) = \\frac{1}{n^2}\\sum_{i=1}^{n}Var(X_i) = \\frac{1}{n^2}(n\\sigma^2) = \\frac{\\sigma^2}{n}$$\nThe standard deviation of the sample mean is therefore $SD(\\bar{X}_n) = \\sqrt{\\frac{\\sigma^2}{n}} = \\frac{\\sigma}{\\sqrt{n}}$.\n\nAccording to the Central Limit Theorem, for a sufficiently large sample size $n$, the distribution of the sample mean $\\bar{X}_n$ is approximately normal, with mean $\\theta$ and variance $\\frac{\\sigma^2}{n}$. We can write this as $\\bar{X}_n \\sim \\mathcal{N}\\left(\\theta, \\frac{\\sigma^2}{n}\\right)$.\nTo analyze the probability requirement, we standardize the random variable $\\bar{X}_n$ to obtain a standard normal variable $Z$:\n$$Z = \\frac{\\bar{X}_n - E[\\bar{X}_n]}{SD(\\bar{X}_n)} = \\frac{\\bar{X}_n - \\theta}{\\sigma/\\sqrt{n}}$$\nThe variable $Z$ is approximately distributed as a standard normal distribution, $Z \\sim \\mathcal{N}(0,1)$.\n\nThe design requirement is that the absolute estimation error $|\\bar{X}_n - \\theta|$ is no more than $\\varepsilon$ with a probability of at least $p_0$. This is expressed as:\n$$P(|\\bar{X}_n - \\theta| \\leq \\varepsilon) \\geq p_0$$\nWe can rewrite the inequality inside the probability as $-\\varepsilon \\leq \\bar{X}_n - \\theta \\leq \\varepsilon$. Dividing all parts of the inequality by the standard deviation of $\\bar{X}_n$ gives:\n$$-\\frac{\\varepsilon}{\\sigma/\\sqrt{n}} \\leq \\frac{\\bar{X}_n - \\theta}{\\sigma/\\sqrt{n}} \\leq \\frac{\\varepsilon}{\\sigma/\\sqrt{n}}$$\nThis is equivalent to $-\\frac{\\varepsilon\\sqrt{n}}{\\sigma} \\leq Z \\leq \\frac{\\varepsilon\\sqrt{n}}{\\sigma}$. The probability requirement becomes:\n$$P\\left(-\\frac{\\varepsilon\\sqrt{n}}{\\sigma} \\leq Z \\leq \\frac{\\varepsilon\\sqrt{n}}{\\sigma}\\right) \\geq p_0$$\nLet $\\Phi(z)$ be the cumulative distribution function (CDF) of the standard normal distribution. The probability is given by $\\Phi\\left(\\frac{\\varepsilon\\sqrt{n}}{\\sigma}\\right) - \\Phi\\left(-\\frac{\\varepsilon\\sqrt{n}}{\\sigma}\\right)$. Due to the symmetry of the normal distribution, $\\Phi(-z) = 1 - \\Phi(z)$, so the probability is $2\\Phi\\left(\\frac{\\varepsilon\\sqrt{n}}{\\sigma}\\right) - 1$.\nThe inequality is $2\\Phi\\left(\\frac{\\varepsilon\\sqrt{n}}{\\sigma}\\right) - 1 \\geq p_0$, which can be rearranged to:\n$$\\Phi\\left(\\frac{\\varepsilon\\sqrt{n}}{\\sigma}\\right) \\geq \\frac{1+p_0}{2}$$\nWe are given $p_0 = 0.95$, so we need $\\Phi\\left(\\frac{\\varepsilon\\sqrt{n}}{\\sigma}\\right) \\geq \\frac{1+0.95}{2} = 0.975$.\nLet $z_c$ be the critical value from the standard normal distribution such that $\\Phi(z_c) = 0.975$. A standard lookup gives $z_c \\approx 1.96$.\nThus, we require $\\frac{\\varepsilon\\sqrt{n}}{\\sigma} \\geq z_c$. Solving for $n$:\n$$\\sqrt{n} \\geq \\frac{\\sigma z_c}{\\varepsilon} \\implies n \\geq \\left(\\frac{\\sigma z_c}{\\varepsilon}\\right)^2$$\nSubstituting the given values $\\sigma=0.8$, $\\varepsilon=0.10$, and $z_c \\approx 1.96$:\n$$n \\geq \\left(\\frac{0.8 \\times 1.96}{0.10}\\right)^2 = \\left(\\frac{1.568}{0.10}\\right)^2 = (15.68)^2 = 245.8624$$\nSince $n$ must be an integer, the smallest number of samples that satisfies the probability requirement is $n = 246$.\n\nNext, we evaluate the latency constraint. The total time for sensing is $n\\Delta t$, which must not exceed the deadline $T_{\\max}$:\n$$n\\Delta t \\leq T_{\\max}$$\nThis imposes an upper bound on $n$:\n$$n \\leq \\frac{T_{\\max}}{\\Delta t}$$\nSubstituting the given values $\\Delta t=2\\times 10^{-3}$ seconds and $T_{\\max}=0.50$ seconds:\n$$n \\leq \\frac{0.50}{2 \\times 10^{-3}} = \\frac{0.50}{0.002} = 250$$\nSo, $n$ must be an integer less than or equal to $250$.\n\nWe must find the smallest integer $n$ that satisfies both conditions simultaneously:\n1. $n \\geq 246$ (from the probability requirement)\n2. $n \\leq 250$ (from the latency constraint)\n\nThe range of possible integer values for $n$ is $[246, 250]$. The problem asks for the smallest integer $n$ in this range.\nTherefore, the smallest valid value for $n$ is $246$.\nLet's check this value:\nFor $n=246$, the latency is $246 \\times (2 \\times 10^{-3}) = 0.492$ seconds, which is less than $T_{\\max}=0.50$ seconds.\nFor $n=246$, the condition $n \\geq 245.8624$ is met, satisfying the probability requirement.\nFor $n=245$, the probability requirement would not be met.\nThus, the smallest integer $n$ is $246$.", "answer": "$$\\boxed{246}$$", "id": "3153047"}, {"introduction": "While many applications assume independent data, real-world measurements are often correlated. This exercise simulates a digital communication channel experiencing \"burst errors,\" where errors are not independent but cluster together within data packets [@problem_id:3153114]. Here, you will see how the Central Limit Theorem can be adapted to handle such dependencies, allowing you to accurately calculate the required simulation size even when the classic i.i.d. assumption is not met.", "problem": "A computational science team is planning a Monte Carlo study of a binary communication link to estimate the bit error rate. Let the bit error indicator be a Bernoulli random variable $X$ with $X=1$ if a bit is received in error and $X=0$ otherwise. The team will collect data in packets (bursts) of equal length. Within each packet $j$, the $m$ bit error indicators $(X_{j,1},X_{j,2},\\dots,X_{j,m})$ share the same pairwise correlation coefficient $\\rho>0$, while packets are mutually independent. The estimator for the bit error rate is the sample mean $\\hat{p}=\\bar{X}$ over all collected bits.\n\nA small pilot run produced a planning value $\\hat{p}_{0}=0.010$. The production run will use packet size $m=100$ bits and the intra-packet pairwise correlation coefficient $\\rho=0.05$. Using the Law of Large Numbers (LLN) and the Central Limit Theorem (CLT), together with a block-based CLT argument appropriate for positively correlated bursts, determine the smallest integer number of packets $K$ required so that a two-sided $95\\%$ normal-approximation confidence interval for the bit error rate has half-width at most $h=0.001$.\n\nExpress your final answer as the smallest integer $K$ that guarantees the required half-width. No rounding by significant figures is needed because $K$ is an integer count.", "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard problem in statistical simulation and communication theory that can be solved using the Central Limit Theorem applied to correlated data. All necessary parameters are provided, and no contradictions exist.\n\nThe objective is to find the smallest integer number of packets, $K$, such that the half-width of a $95\\%$ confidence interval for the bit error rate, $p$, is at most $h=0.001$. The estimator for $p$ is the sample mean $\\hat{p} = \\bar{X}$ over all $N=Km$ bits.\n\nA two-sided normal-approximation confidence interval for $p$ is given by $\\hat{p} \\pm h$, where the half-width $h$ is defined as:\n$$h = z_{\\alpha/2} \\cdot \\sqrt{\\text{Var}(\\hat{p})}$$\nFor a $95\\%$ confidence level, $1-\\alpha = 0.95$, so $\\alpha=0.05$ and $\\alpha/2 = 0.025$. The corresponding critical value from the standard normal distribution is $z_{0.025} \\approx 1.96$.\n\nThe estimator $\\hat{p}$ is the average of all bit error indicators:\n$$\\hat{p} = \\frac{1}{Km} \\sum_{j=1}^{K} \\sum_{i=1}^{m} X_{j,i}$$\nwhere $X_{j,i}$ is the Bernoulli random variable for the $i$-th bit in the $j$-th packet. The total number of bits is $N=Km$.\n\nThe problem states that packets are mutually independent, but bits within a packet are correlated. This structure suggests a block-based application of the Central Limit Theorem (CLT). We define packet-level averages $Y_j = \\frac{1}{m} \\sum_{i=1}^{m} X_{j,i}$. The overall estimator $\\hat{p}$ is the mean of these packet averages:\n$$\\hat{p} = \\frac{1}{K} \\sum_{j=1}^{K} Y_j$$\nSince the packets are independent and identically distributed, the random variables $Y_j$ are independent and identically distributed (i.i.d.). For a sufficiently large number of packets $K$, the CLT ensures that the distribution of $\\hat{p}$ is approximately normal, with mean $E[\\hat{p}] = E[Y_j] = p$ and variance:\n$$\\text{Var}(\\hat{p}) = \\text{Var}\\left(\\frac{1}{K} \\sum_{j=1}^{K} Y_j\\right) = \\frac{1}{K^2} \\sum_{j=1}^{K} \\text{Var}(Y_j) = \\frac{K \\cdot \\text{Var}(Y_1)}{K^2} = \\frac{\\text{Var}(Y_1)}{K}$$\nWe now need to find the variance of a single packet average, $\\text{Var}(Y_1)$. Let's drop the packet index $j=1$ for simplicity.\n$$\\text{Var}(Y) = \\text{Var}\\left(\\frac{1}{m} \\sum_{i=1}^{m} X_i\\right) = \\frac{1}{m^2} \\text{Var}\\left(\\sum_{i=1}^{m} X_i\\right)$$\nThe variance of the sum of correlated random variables is given by:\n$$\\text{Var}\\left(\\sum_{i=1}^{m} X_i\\right) = \\sum_{i=1}^{m}\\sum_{k=1}^{m} \\text{Cov}(X_i, X_k) = \\sum_{i=1}^{m} \\text{Var}(X_i) + \\sum_{i \\neq k} \\text{Cov}(X_i, X_k)$$\nEach $X_i$ is a Bernoulli random variable with parameter $p$. Thus, $\\text{Var}(X_i) = p(1-p)$.\nThe pairwise correlation coefficient for $i \\neq k$ is given as $\\rho$:\n$$\\rho = \\frac{\\text{Cov}(X_i, X_k)}{\\sqrt{\\text{Var}(X_i)\\text{Var}(X_k)}} = \\frac{\\text{Cov}(X_i, X_k)}{p(1-p)}$$\nTherefore, $\\text{Cov}(X_i, X_k) = \\rho p(1-p)$ for $i \\neq k$.\nThere are $m$ variance terms and $m(m-1)$ covariance terms in the double summation.\n$$\\text{Var}\\left(\\sum_{i=1}^{m} X_i\\right) = m \\cdot p(1-p) + m(m-1) \\cdot \\rho p(1-p) = m p(1-p) [1 + (m-1)\\rho]$$\nSubstituting this back into the expression for $\\text{Var}(Y)$:\n$$\\text{Var}(Y) = \\frac{1}{m^2} \\left( m p(1-p) [1 + (m-1)\\rho] \\right) = \\frac{p(1-p)}{m} [1 + (m-1)\\rho]$$\nFinally, the variance of the overall estimator $\\hat{p}$ is:\n$$\\text{Var}(\\hat{p}) = \\frac{\\text{Var}(Y)}{K} = \\frac{p(1-p)}{Km} [1 + (m-1)\\rho]$$\nThe term $[1 + (m-1)\\rho]$ is the variance inflation factor due to the positive intra-packet correlation.\n\nWe are given the constraint that the half-width is at most $h$:\n$$z_{0.025} \\sqrt{\\text{Var}(\\hat{p})} \\le h$$\n$$z_{0.025} \\sqrt{\\frac{p(1-p)}{Km} [1 + (m-1)\\rho]} \\le h$$\nSquaring both sides and solving for $K$:\n$$(z_{0.025})^2 \\frac{p(1-p)}{Km} [1 + (m-1)\\rho] \\le h^2$$\n$$K \\ge \\frac{(z_{0.025})^2 p(1-p) [1 + (m-1)\\rho]}{h^2 m}$$\nThe true bit error rate $p$ is unknown. We use the planning value from the pilot run, $\\hat{p}_0 = 0.010$, as an estimate for $p$. We are given:\n- Planning value for $p$: $\\hat{p}_0 = 0.010$\n- Packet size $m = 100$\n- Correlation coefficient $\\rho = 0.05$\n- Maximum half-width $h = 0.001$\n- z-score $z_{0.025} \\approx 1.96$\n\nSubstituting these values into the inequality for $K$:\n$$K \\ge \\frac{(1.96)^2 (0.010)(1-0.010) [1 + (100-1)(0.05)]}{(0.001)^2 (100)}$$\n$$K \\ge \\frac{(3.8416) (0.010)(0.99) [1 + (99)(0.05)]}{10^{-6} \\cdot 100}$$\n$$K \\ge \\frac{(3.8416) (0.0099) [1 + 4.95]}{10^{-4}}$$\n$$K \\ge \\frac{0.03803184 \\cdot [5.95]}{10^{-4}}$$\n$$K \\ge \\frac{0.226289448}{10^{-4}}$$\n$$K \\ge 2262.89448$$\nSince the number of packets $K$ must be an integer, we must take the smallest integer value that satisfies this condition, which is the ceiling of the result.\n$$K = \\lceil 2262.89448 \\rceil = 2263$$\nThus, a minimum of $2263$ packets are required to achieve the desired precision.", "answer": "$$\n\\boxed{2263}\n$$", "id": "3153114"}, {"introduction": "The limit theorems do more than just help us analyze error; they can guide the design of more intelligent algorithms. This practice places you in the role of designing an efficient Monte Carlo ray tracer, a cornerstone of modern computer graphics [@problem_id:3153099]. You will use the principles of the Central Limit Theorem to derive an adaptive sampling strategy, allocating your computational budget wisely to the most \"difficult\" pixels to minimize overall image noise, transforming a theoretical concept into a powerful optimization tool.", "problem": "Consider a computational imaging task where each pixel corresponds to a random luminance variable with independent and identically distributed samples. For pixel index $i \\in \\{1,\\dots,P\\}$, let the luminance samples be independent draws of a real-valued random variable $L_i$ with unknown mean $\\mu_i$ and finite variance $\\sigma_i^2$. The per-pixel Monte Carlo estimate of the pixel luminance is the sample mean $\\bar{L}_{i,n_i}$ computed from $n_i$ samples. Assume that an initial pilot pass provides $n_{0,i}$ samples per pixel and an unbiased variance estimate $s_i^2$ of $\\sigma_i^2$ from these pilot samples, and that we have a total sample budget $N_{\\text{total}}$ for the final rendering pass such that $\\sum_{i=1}^P n_i = N_{\\text{total}}$ and $n_i \\ge n_{0,i}$ for all $i$.\n\nUse the Law of Large Numbers (LLN) and the Central Limit Theorem (CLT) as the only probabilistic foundations to design an allocation rule for the integer sample counts $\\{n_i\\}_{i=1}^P$ that minimizes the maximum predicted two-sided confidence interval half-width across pixels at a given confidence level $1-\\alpha$, under the constraints $\\sum_{i=1}^P n_i = N_{\\text{total}}$ and $n_i \\ge n_{0,i}$. Specifically:\n\n- Under the Central Limit Theorem, for large $n_i$, the distribution of $\\bar{L}_{i,n_i}$ can be approximated by a normal distribution with mean $\\mu_i$ and variance $\\sigma_i^2/n_i$. Thus, the two-sided $(1-\\alpha)$ confidence interval half-width for $\\mu_i$ is approximately $z_{1-\\alpha/2}\\sqrt{\\sigma_i^2/n_i}$, where $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$ quantile of the standard normal distribution.\n- Use $s_i^2$ in place of $\\sigma_i^2$ as a plug-in estimate, and restrict yourself to algorithmic decisions justified by the LLN and CLT alone (for example, separability across pixels, convexity arguments, and monotonicity of marginal variance reduction with respect to $n_i$ are admissible consequences to use).\n\nYour program must:\n1. Derive from first principles how to choose integer $\\{n_i\\}$ that minimize the maximum predicted half-width across pixels, subject to the constraints $\\sum_{i=1}^P n_i = N_{\\text{total}}$ and $n_i \\ge n_{0,i}$.\n2. Compute the predicted maximum half-width value\n   $$H_{\\max} \\equiv \\max_{1 \\le i \\le P} \\left( z_{1-\\alpha/2}\\sqrt{\\frac{s_i^2}{n_i}} \\right),$$\n   using the derived allocation $\\{n_i\\}$.\n3. For integer feasibility, if the derived optimal allocation has non-integer values, convert it to integers while preserving the constraints and ensuring that the objective is not worsened by any single-sample reassignment. Your method must be based on a principled marginal-improvement argument justified by the CLT-based objective.\n\nInput is embedded in the code via a fixed test suite. For each test case $t$, you are given: the per-pixel variance estimates $(s_1^2,\\dots,s_P^2)$, the pilot sample counts $(n_{0,1},\\dots,n_{0,P})$, the total budget $N_{\\text{total}}$, and the significance level $\\alpha$. Your program must output, for each test case, the scalar $H_{\\max}$ rounded to six decimal places.\n\nTest suite (five cases):\n- Case A: $P=1$, $(s_1^2) = (4.0)$, $(n_{0,1}) = (0)$, $N_{\\text{total}} = 100$, $\\alpha = 0.05$.\n- Case B: $P=2$, $(s_1^2,s_2^2) = (1.0,1.0)$, $(n_{0,1},n_{0,2}) = (0,0)$, $N_{\\text{total}} = 100$, $\\alpha = 0.05$.\n- Case C: $P=2$, $(s_1^2,s_2^2) = (4.0,1.0)$, $(n_{0,1},n_{0,2}) = (0,0)$, $N_{\\text{total}} = 100$, $\\alpha = 0.05$.\n- Case D: $P=3$, $(s_1^2,s_2^2,s_3^2) = (4.0,1.0,0.25)$, $(n_{0,1},n_{0,2},n_{0,3}) = (5,5,5)$, $N_{\\text{total}} = 30$, $\\alpha = 0.10$.\n- Case E: $P=3$, $(s_1^2,s_2^2,s_3^2) = (0.0,2.25,0.25)$, $(n_{0,1},n_{0,2},n_{0,3}) = (2,2,2)$, $N_{\\text{total}} = 12$, $\\alpha = 0.01$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated Python-style list of floating-point numbers in the order of cases A through E, each rounded to six decimal places and enclosed in square brackets. For example, a valid output line has the form $[\\text{A},\\text{B},\\text{C},\\text{D},\\text{E}]$, where each symbol denotes the rounded value for that case.", "solution": "The problem asks for an integer sample allocation $\\{n_i\\}_{i=1}^P$ that minimizes the maximum predicted confidence interval half-width across a set of $P$ pixels, subject to a total sample budget $N_{\\text{total}}$ and per-pixel minimum sample counts $\\{n_{0,i}\\}$. This is a minimax optimization problem grounded in the Central Limit Theorem (CLT) and the Law of Large Numbers (LLN).\n\n### Step 1: Formulation of the Optimization Problem\n\nThe problem is to determine the integer sample counts $n_1, n_2, \\dots, n_P$ that solve:\n$$\n\\begin{aligned}\n& \\underset{\\{n_i\\}}{\\text{minimize}} & & \\max_{1 \\le i \\le P} H_i \\\\\n& \\text{subject to} & & \\sum_{i=1}^P n_i = N_{\\text{total}} \\\\\n& & & n_i \\ge n_{0,i} \\quad \\text{for all } i \\in \\{1, \\dots, P\\} \\\\\n& & & n_i \\in \\mathbb{Z}^+\n\\end{aligned}\n$$\nwhere $H_i$ is the confidence interval half-width for pixel $i$. The CLT provides the approximation for large $n_i$:\n$$H_i = z_{1-\\alpha/2}\\sqrt{\\frac{\\sigma_i^2}{n_i}}$$\nWe use the unbiased variance estimates $s_i^2$ from a pilot pass as plug-in estimators for the true unknown variances $\\sigma_i^2$, a substitution justified by the LLN. The objective function thus becomes:\n$$\\text{minimize} \\max_{1 \\le i \\le P} \\left( z_{1-\\alpha/2}\\sqrt{\\frac{s_i^2}{n_i}} \\right)$$\nSince $z_{1-\\alpha/2}$ is a positive constant for a given confidence level $1-\\alpha$, minimizing the maximum half-width is equivalent to minimizing the maximum of the term $\\sqrt{s_i^2/n_i}$, which is in turn equivalent to minimizing:\n$$\\max_{1 \\le i \\le P} \\left( \\frac{s_i^2}{n_i} \\right)$$\nThis term, $s_i^2/n_i$, represents the estimated variance of the sample mean $\\bar{L}_{i,n_i}$.\n\n### Step 2: Optimal Allocation Strategy for Real-Valued $n_i$\n\nThe core principle for solving a minimax problem of this form is to equalize the terms being maximized. To minimize $\\max_i(E_i)$, where $E_i = s_i^2/n_i$ is a decreasing function of the resource $n_i$, the optimal solution is achieved when all non-zero error terms are equal:\n$$\\frac{s_1^2}{n_1} = \\frac{s_2^2}{n_2} = \\dots = \\frac{s_P^2}{n_P} = C$$\nfor some constant $C$ (for all pixels with $s_i^2 > 0$). This gives a relationship for the optimal (real-valued) allocation:\n$$n_i = \\frac{s_i^2}{C}$$\nThis shows that the optimal number of samples $n_i$ is directly proportional to the variance $s_i^2$. We can find the constant of proportionality using the total budget constraint, $\\sum n_i = N_{\\text{total}}$:\n$$\\sum_{i=1}^P \\frac{s_i^2}{C} = N_{\\text{total}} \\implies \\frac{1}{C} \\sum_{i=1}^P s_i^2 = N_{\\text{total}} \\implies \\frac{1}{C} = \\frac{N_{\\text{total}}}{\\sum_{j=1}^P s_j^2}$$\nSubstituting this back, we get the ideal real-valued allocation (ignoring the $n_i \\ge n_{0,i}$ constraint for a moment):\n$$n_i^* = N_{\\text{total}} \\frac{s_i^2}{\\sum_{j=1}^P s_j^2}$$\n\n### Step 3: Incorporating Minimum Sample Constraints\n\nThe constraint $n_i \\ge n_{0,i}$ must be incorporated. The allocation $n_i^*$ might violate this for some pixels. If $n_i^* < n_{0,i}$ for a pixel $i$, we are forced to allocate at least $n_{0,i}$ samples to it. For such a pixel, we \"lock\" its sample count at $n_i = n_{0,i}$. The remaining budget is then optimally re-allocated among the remaining \"active\" pixels according to the same principle. This leads to an iterative algorithm:\n1.  Initialize the set of active pixels $A = \\{1, \\dots, P\\}$ and the budget $N = N_{\\text{total}}$.\n2.  In a loop, calculate the ideal allocation $n_i^{\\text{ideal}} = N \\frac{s_i^2}{\\sum_{j \\in A} s_j^2}$ for all $i \\in A$.\n3.  Identify the set of pixels $V \\subseteq A$ for which $n_i^{\\text{ideal}} < n_{0,i}$.\n4.  If $V$ is empty, the current allocation $n_i^{\\text{ideal}}$ for $i \\in A$ is optimal and respects all minimums. The final real-valued allocation is found.\n5.  If $V$ is not empty, for each $i \\in V$, fix its allocation to $n_i = n_{0,i}$. Remove these pixels from $A$ and reduce the budget $N$ by $\\sum_{i \\in V} n_{0,i}$. Repeat the loop with the smaller active set and reduced budget.\n\nThis iterative process yields a real-valued allocation $\\{n_i^{\\text{real}}\\}$ that satisfies all constraints.\n\n### Step 4: Integer Sample Allocation\n\nThe derived real-valued allocation $\\{n_i^{\\text{real}}\\}$ must be converted to an integer allocation $\\{n_i\\}$ that sums to $N_{\\text{total}}$. A principled method, consistent with the objective of minimizing the maximum error, is as follows:\n1.  Initialize the integer allocation by taking the floor of the real solution: $n_i' = \\lfloor n_i^{\\text{real}} \\rfloor$. This satisfies $n_i' \\ge n_{0,i}$ as $n_i^{\\text{real}} \\ge n_{0,i}$ and $n_{0,i}$ is an integer.\n2.  Calculate the remaining number of samples to be distributed: $R = N_{\\text{total}} - \\sum_{i=1}^P n_i'$.\n3.  Distribute these $R$ samples one by one. In each of the $R$ steps, add a single sample to the pixel which currently has the largest error term $s_i^2/n_i'$. This is a greedy approach that directly tackles the minimax objective at each step. If $n_i'=0$ for a pixel with $s_i^2>0$, its error is infinite, ensuring it receives a sample first.\n\n### Step 5: Final Calculation\n\nOnce the final integer allocation $\\{n_i\\}$ is determined, the maximum half-width is calculated by finding the maximum error term across all pixels and multiplying by the appropriate normal quantile:\n$$ H_{\\max} = \\max_{1 \\le i \\le P} \\left( z_{1-\\alpha/2}\\sqrt{\\frac{s_i^2}{n_i}} \\right) = z_{1-\\alpha/2} \\sqrt{\\max_{1 \\le i \\le P} \\left(\\frac{s_i^2}{n_i}\\right)} $$\nwhere $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$ quantile of the standard normal distribution, found using `scipy.stats.norm.ppf(1 - \\alpha/2)`. If a sample count $n_i$ for a pixel with $s_i^2>0$ is zero, its half-width is considered infinite; however, the allocation algorithm prevents this if $N_{\\text{total}}$ is sufficient.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main solver function to process all test cases and print the results.\n    \"\"\"\n    # Test suite (five cases): P, s^2, n_0, N_total, alpha\n    test_cases = [\n        # Case A: P=1, (s_1^2)=(4.0), (n_0,1)=(0), N_total=100, alpha=0.05\n        (1, np.array([4.0]), np.array([0]), 100, 0.05),\n        # Case B: P=2, (s_1^2,s_2^2)=(1.0,1.0), (n_0,1,n_0,2)=(0,0), N_total=100, alpha=0.05\n        (2, np.array([1.0, 1.0]), np.array([0, 0]), 100, 0.05),\n        # Case C: P=2, (s_1^2,s_2^2)=(4.0,1.0), (n_0,1,n_0,2)=(0,0), N_total=100, alpha=0.05\n        (2, np.array([4.0, 1.0]), np.array([0, 0]), 100, 0.05),\n        # Case D: P=3, (s_1^2,s_2^2,s_3^2)=(4.0,1.0,0.25), (n_0,...)=(5,5,5), N_total=30, alpha=0.10\n        (3, np.array([4.0, 1.0, 0.25]), np.array([5, 5, 5]), 30, 0.10),\n        # Case E: P=3, (s_1^2,s_2^2,s_3^2)=(0.0,2.25,0.25), (n_0,...)=(2,2,2), N_total=12, alpha=0.01\n        (3, np.array([0.0, 2.25, 0.25]), np.array([2, 2, 2]), 12, 0.01),\n    ]\n\n    results = []\n    for P, s2_vals, n0_vals, N_total, alpha in test_cases:\n        result = _calculate_max_half_width(P, s2_vals, n0_vals, N_total, alpha)\n        results.append(result)\n\n    # Format the final output string\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef _calculate_max_half_width(P, s2_vals, n0_vals, N_total, alpha):\n    \"\"\"\n    Calculates the maximum confidence interval half-width for a single test case.\n    \"\"\"\n    \n    # --- Step 1: Real-valued allocation with constraints ---\n    n_real = np.zeros(P)\n    active_indices = list(range(P))\n    budget = float(N_total)\n    \n    # Iteratively lock pixels that don't meet their minimum sample count n_0\n    for _ in range(P + 1):  # Loop guard to prevent infinite loops\n        if not active_indices:\n            break\n            \n        sum_s2_active = sum(s2_vals[i] for i in active_indices)\n        \n        # If all remaining active pixels have zero variance, their allocation is minimal.\n        if sum_s2_active == 0:\n            for i in active_indices:\n                n_real[i] = n0_vals[i]\n                budget -= n_real[i]\n            # Remaining budget for zero-variance pixels can be distributed arbitrarily.\n            # To be deterministic, we add it to the first such pixel.\n            if len(active_indices) > 0 and budget > 0:\n                n_real[active_indices[0]] += budget\n            break\n\n        n_ideal = {i: budget * s2_vals[i] / sum_s2_active for i in active_indices}\n        \n        violators = {i for i in active_indices if n_ideal[i]  n0_vals[i]}\n\n        if not violators:\n            for i in active_indices:\n                n_real[i] = n_ideal[i]\n            break\n        \n        newly_locked_indices = []\n        for i in violators:\n            n_real[i] = float(n0_vals[i])\n            budget -= n_real[i]\n            newly_locked_indices.append(i)\n        \n        active_indices = [i for i in active_indices if i not in newly_locked_indices]\n        \n    # --- Step 2: Convert real allocation to integer allocation ---\n    n_alloc = np.floor(n_real).astype(int)\n    \n    # Distribute remainder samples using a greedy approach\n    remainder_samples = N_total - np.sum(n_alloc)\n    \n    for _ in range(remainder_samples):\n        errors = np.zeros(P)\n        for i in range(P):\n            if s2_vals[i] > 0:\n                if n_alloc[i] == 0:\n                    errors[i] = np.inf\n                else:\n                    errors[i] = s2_vals[i] / n_alloc[i]\n            else:\n                errors[i] = -np.inf # Ensure zero-variance pixels are never chosen\n\n        # Find pixel with max error to give the next sample\n        idx_to_increment = np.argmax(errors)\n        n_alloc[idx_to_increment] += 1\n        \n    # --- Step 3: Calculate the maximum half-width ---\n    z_val = norm.ppf(1 - alpha / 2.0)\n    \n    max_error_term = 0.0\n    for i in range(P):\n        # A pixel with no samples and non-zero variance would have infinite error,\n        # but the algorithm ensures this doesn't happen if N_total is sufficient.\n        if n_alloc[i] > 0:\n            error_term = s2_vals[i] / n_alloc[i]\n            if error_term > max_error_term:\n                max_error_term = error_term\n    \n    h_max = z_val * np.sqrt(max_error_term)\n    return h_max\n\nsolve()\n```", "id": "3153099"}]}