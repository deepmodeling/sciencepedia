{"hands_on_practices": [{"introduction": "A fundamental skill in computational science is the ability to connect abstract mathematical theory with concrete numerical results. This practice provides a hands-on opportunity to do just that within the framework of Bayesian inference [@problem_id:3126330]. You will first analytically derive the posterior distribution and its moments for a Beta-Bernoulli model, then validate your findings using Monte Carlo simulation, building a strong intuition for how theory and computation reinforce each other.", "problem": "You are asked to implement and validate the law of total expectation and moment calculations in a Bayesian updating scenario using a Beta prior and Bernoulli data. The unknown parameter is a probability $X \\in [0,1]$ with prior $X \\sim \\mathrm{Beta}(\\alpha,\\beta)$. You observe $n$ Independent and Identically Distributed (IID) Bernoulli trials with success probability $X$, summarized as a count of successes $k$. Your tasks are:\n1) Analytically derive, from first principles starting with the definitions of expectation, variance, and Bayes’ theorem, the posterior distribution of $X$ given the data. From that posterior, compute the posterior mean and posterior variance. Also compute the prior mean and the unconditional expected number of successes $E[K]$ using the law of total expectation. Do not use any prepackaged “shortcut” formulas; your derivation must start from the definition of the Beta density, the Bernoulli likelihood, Bayes’ theorem, and the definitions of expectation and variance.\n2) Numerically approximate the posterior mean and posterior variance using Monte Carlo sampling from the posterior distribution. In addition, numerically validate the law of total expectation for the total number of successes by simulating the hierarchical process: first draw $X$ from the prior and then draw $K$ from a Binomial distribution with parameters $n$ and $X$, and estimate $E[K]$.\n3) For numerical reproducibility, use a fixed random seed equal to $123456$. Use $M_{\\text{post}}=200000$ samples to estimate posterior moments and $M_{\\text{tot}}=200000$ samples to estimate $E[K]$ under the law of total expectation.\n4) For each test case, compute and report the following seven quantities in this exact order:\n- Analytic prior mean $E[X]$.\n- Analytic unconditional expected successes $E[K]$ computed via the law of total expectation.\n- Analytic posterior mean $E[X \\mid \\text{data}]$.\n- Analytic posterior variance $\\mathrm{Var}[X \\mid \\text{data}]$.\n- Monte Carlo posterior mean estimate $\\widehat{E}[X \\mid \\text{data}]$.\n- Monte Carlo posterior variance estimate $\\widehat{\\mathrm{Var}}[X \\mid \\text{data}]$.\n- Monte Carlo unconditional expected successes estimate $\\widehat{E}[K]$ via the hierarchical simulation.\nRound each reported value to exactly $6$ decimal places.\nTest suite:\nProvide results for the following $5$ test cases, each specified as a quadruple $(\\alpha,\\beta,n,k)$:\n- Case $1$: $(\\alpha,\\beta,n,k)=(2.0,3.0,10,4)$.\n- Case $2$: $(\\alpha,\\beta,n,k)=(2.0,5.0,0,0)$.\n- Case $3$: $(\\alpha,\\beta,n,k)=(0.5,0.5,5,5)$.\n- Case $4$: $(\\alpha,\\beta,n,k)=(50.0,50.0,10,8)$.\n- Case $5$: $(\\alpha,\\beta,n,k)=(1.5,0.5,3,0)$.\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the list of the seven values for one test case in the specified order, rounded to $6$ decimal places. For example, the output structure must look like $[[v_{1,1},\\dots,v_{1,7}],[v_{2,1},\\dots,v_{2,7}],\\dots,[v_{5,1},\\dots,v_{5,7}]]$ with no additional text.", "solution": "We start from the fundamental definitions. Let $X \\in [0,1]$ denote an unknown probability. The prior density of $X$ is the Beta density with parameters $\\alpha>0$ and $\\beta>0$,\n$$\np(x \\mid \\alpha,\\beta)=\\frac{1}{B(\\alpha,\\beta)} x^{\\alpha-1} (1-x)^{\\beta-1}, \\quad 0<x<1\n$$\nwhere $B(\\alpha,\\beta)$ is the Beta function. The data consist of $n$ Independent and Identically Distributed (IID) Bernoulli trials with success probability $X$, summarized by the number of successes $K=\\sum_{i=1}^{n} Y_i$, where $Y_i \\in \\{0,1\\}$. Conditional on $X=x$, the likelihood for $K=k$ is the Binomial mass function:\n$$\np(k \\mid x,n) = \\binom{n}{k} x^{k} (1-x)^{n-k}, \\quad k \\in \\{0,1,\\dots,n\\}\n$$\nBy Bayes’ theorem, the posterior density for $X$ given $K=k$ is proportional to the product $p(k \\mid x,n) p(x \\mid \\alpha,\\beta)$:\n$$\np(x \\mid k,n,\\alpha,\\beta) \\propto x^{k}(1-x)^{n-k} \\cdot x^{\\alpha-1} (1-x)^{\\beta-1} = x^{(\\alpha+k)-1} (1-x)^{(\\beta + n - k)-1}\n$$\nRecognizing the kernel of a Beta density, we obtain the conjugate posterior\n$$\nX \\mid (k,n,\\alpha,\\beta) \\sim \\mathrm{Beta}(\\alpha',\\beta'), \\quad \\alpha'=\\alpha+k,\\ \\beta'=\\beta+n-k\n$$\nNext, we compute moments. For a Beta random variable $Z \\sim \\mathrm{Beta}(a,b)$ with $a>0$ and $b>0$, the mean and variance follow from the definitions of expectation and variance using the Beta density and Beta function identities:\n$$\n\\mathbb{E}[Z] = \\int_{0}^{1} z \\, \\frac{1}{B(a,b)} z^{a-1} (1-z)^{b-1} \\, dz = \\frac{a}{a+b}\n$$\n$$\n\\mathrm{Var}(Z) = \\mathbb{E}[Z^{2}] - (\\mathbb{E}[Z])^{2} = \\frac{ab}{(a+b)^{2}(a+b+1)}\n$$\nApplying these to the prior yields\n$$\n\\mathbb{E}[X] = \\frac{\\alpha}{\\alpha+\\beta}.\n$$\nApplying them to the posterior with $(a,b)=(\\alpha',\\beta')$ yields\n$$\n\\mathbb{E}[X \\mid k] = \\frac{\\alpha+k}{\\alpha+\\beta+n}, \\quad \\mathrm{Var}(X \\mid k) = \\frac{(\\alpha+k)(\\beta+n-k)}{(\\alpha+\\beta+n)^{2}(\\alpha+\\beta+n+1)}\n$$\nWe now connect to the law of total expectation. Let $K=\\sum_{i=1}^{n} Y_i$ denote the total number of successes from $n$ Bernoulli trials with success probability $X$. The law of total expectation states\n$$\n\\mathbb{E}[K] = \\mathbb{E}\\big[\\mathbb{E}[K \\mid X]\\big]\n$$\nConditional on $X$, $K \\mid X \\sim \\mathrm{Binomial}(n,X)$, so $\\mathbb{E}[K \\mid X]=n X$. Therefore,\n$$\n\\mathbb{E}[K] = \\mathbb{E}[n X] = n \\, \\mathbb{E}[X] = n \\, \\frac{\\alpha}{\\alpha+\\beta}\n$$\nThis is a direct consequence of linearity of expectation and the definition of conditional expectation.\n\nAlgorithmic plan for each test case $(\\alpha,\\beta,n,k)$:\n1) Compute analytic prior mean $\\mathbb{E}[X]=\\alpha/(\\alpha+\\beta)$ from the Beta mean.\n2) Compute analytic unconditional expected successes $\\mathbb{E}[K]=n \\, \\alpha/(\\alpha+\\beta)$ using the law of total expectation.\n3) Compute analytic posterior parameters $\\alpha'=\\alpha+k$ and $\\beta'=\\beta+n-k$.\n4) Compute analytic posterior mean and variance using the Beta moments with $(a,b)=(\\alpha',\\beta')$:\n$$\n\\mathbb{E}[X \\mid k] = \\frac{\\alpha'}{\\alpha'+\\beta'}, \\quad \\mathrm{Var}(X \\mid k) = \\frac{\\alpha' \\beta'}{(\\alpha'+\\beta')^{2}(\\alpha'+\\beta'+1)}\n$$\n5) Monte Carlo posterior approximation: draw $M_{\\text{post}}$ samples from $\\mathrm{Beta}(\\alpha',\\beta')$, compute the empirical mean and variance (using population variance, not sample-variance with degrees-of-freedom correction).\n6) Monte Carlo validation of the law of total expectation: draw $M_{\\text{tot}}$ prior samples $x_j \\sim \\mathrm{Beta}(\\alpha,\\beta)$, then for each draw $k_j \\sim \\mathrm{Binomial}(n,x_j)$, and estimate $\\widehat{\\mathbb{E}}[K]$ as the empirical mean of $\\{k_j\\}$.\n7) Use a fixed random seed $123456$ for reproducibility and set $M_{\\text{post}}=200000$ and $M_{\\text{tot}}=200000$.\n8) Round each of the seven outputs to $6$ decimal places and emit the results for all test cases as a single bracketed list of lists on one line in the specified order:\n- Analytic prior mean $\\mathbb{E}[X]$,\n- Analytic $\\mathbb{E}[K]$,\n- Analytic posterior mean $\\mathbb{E}[X \\mid \\text{data}]$,\n- Analytic posterior variance $\\mathrm{Var}[X \\mid \\text{data}]$,\n- Monte Carlo posterior mean estimate,\n- Monte Carlo posterior variance estimate,\n- Monte Carlo estimate of $\\mathbb{E}[K]$ via hierarchical simulation.\n\nDesign considerations:\n- Vectorized simulation with NumPy ensures both accuracy and performance for $M_{\\text{post}}$ and $M_{\\text{tot}}$.\n- Population variance is used to match the analytic variance $\\mathrm{Var}(X \\mid k)$.\n- Boundary cases such as $n=0$ are handled naturally: the posterior reverts to the prior and $\\mathbb{E}[K]=0$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef beta_posterior_params(alpha, beta, n, k):\n    a_post = alpha + k\n    b_post = beta + n - k\n    return a_post, b_post\n\ndef beta_moments(a, b):\n    mean = a / (a + b)\n    var = (a * b) / ((a + b) ** 2 * (a + b + 1.0))\n    return mean, var\n\ndef monte_carlo_posterior_moments(rng, a_post, b_post, m_samples):\n    samples = rng.beta(a_post, b_post, size=m_samples)\n    mean = float(np.mean(samples))\n    # population variance (ddof=0) to match analytic Var\n    var = float(np.var(samples))\n    return mean, var\n\ndef monte_carlo_total_expectation_EK(rng, alpha, beta, n, m_samples):\n    # Draw hierarchical samples: X ~ Beta(alpha,beta), then K ~ Binomial(n, X)\n    x = rng.beta(alpha, beta, size=m_samples)\n    if n == 0:\n        # degenerate at 0 successes\n        k = np.zeros(m_samples, dtype=np.int64)\n    else:\n        k = rng.binomial(n, x, size=m_samples)\n    return float(np.mean(k))\n\ndef solve():\n    # Define the test cases from the problem statement: (alpha, beta, n, k)\n    test_cases = [\n        (2.0, 3.0, 10, 4),      # Case 1: general\n        (2.0, 5.0, 0, 0),       # Case 2: boundary n=0\n        (0.5, 0.5, 5, 5),       # Case 3: U-shaped prior, all successes\n        (50.0, 50.0, 10, 8),    # Case 4: strong prior\n        (1.5, 0.5, 3, 0),       # Case 5: skewed prior, no successes\n    ]\n\n    # Fixed random seed and sample sizes as specified\n    seed = 123456\n    m_post = 200000\n    m_tot = 200000\n    rng = np.random.default_rng(seed)\n\n    results_str_blocks = []\n\n    for alpha, beta, n, k in test_cases:\n        # Analytic prior mean\n        prior_mean, _ = beta_moments(alpha, beta)\n\n        # Law of total expectation analytic E[K] = n * E[X]\n        analytic_EK = n * prior_mean\n\n        # Posterior params and analytic posterior moments\n        a_post, b_post = beta_posterior_params(alpha, beta, n, k)\n        post_mean, post_var = beta_moments(a_post, b_post)\n\n        # Monte Carlo posterior moments\n        mc_post_mean, mc_post_var = monte_carlo_posterior_moments(rng, a_post, b_post, m_post)\n\n        # Monte Carlo law of total expectation for E[K]\n        mc_EK = monte_carlo_total_expectation_EK(rng, alpha, beta, n, m_tot)\n\n        # Round to 6 decimals and format\n        vals = [\n            prior_mean,\n            analytic_EK,\n            post_mean,\n            post_var,\n            mc_post_mean,\n            mc_post_var,\n            mc_EK\n        ]\n        formatted = \"[\" + \",\".join(f\"{v:.6f}\" for v in vals) + \"]\"\n        results_str_blocks.append(formatted)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_str_blocks)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3126330"}, {"introduction": "Many problems in science and engineering involve integration over a large number of variables, a domain where traditional numerical methods often fail due to the \"curse of dimensionality.\" This exercise provides a direct and quantitative comparison between deterministic grid-based quadrature and stochastic Monte Carlo integration [@problem_id:3145824]. By deriving and comparing the number of function evaluations each method requires, you will discover why the dimension-independent convergence of Monte Carlo methods makes them essential tools for high-dimensional problems.", "problem": "Consider estimating the integral of a separable function over a unit hypercube using two methods: Monte Carlo (MC) sampling and a tensor-product grid composite trapezoidal quadrature. Let the function be defined by $f(\\mathbf{x}) = \\prod_{i=1}^{d} g(x_i)$ on $[0,1]^d$, where $g(t) = \\sin(\\pi t)$ and angles are in radians. The integral of interest is $I_d = \\int_{[0,1]^d} f(\\mathbf{x}) \\, d\\mathbf{x}$. Your task is to implement a program that, for a set of dimensions $d$ and tolerances $\\varepsilon$, computes and compares the minimum sample counts needed by Monte Carlo and tensor-grid quadrature to guarantee a root mean square error at most $\\varepsilon$.\n\nStart from the following foundational bases:\n- Definitions of expectation and variance for random variables under the uniform distribution on $[0,1]^d$.\n- The classical error bound for the composite trapezoidal rule in one dimension: for a twice continuously differentiable function on $[a,b]$ partitioned into $m$ equal panels (so the step size is $h = (b-a)/m$), the absolute error satisfies $\\left|\\int_{a}^{b} f(x)\\,dx - T_m\\right| \\le \\dfrac{(b-a)}{12} h^2 \\sup_{x \\in [a,b]} |f''(x)|$. For $[0,1]$, this reduces to $\\left|\\int_{0}^{1} f(x)\\,dx - T_m\\right| \\le \\dfrac{1}{12 m^2} \\sup_{x \\in [0,1]} |f''(x)|$.\n\nUsing these bases and the separable structure of $f$:\n- Derive the variance of $f(\\mathbf{X})$ under the uniform distribution on $[0,1]^d$, with $\\mathbf{X}$ uniform on $[0,1]^d$. Use this to determine the minimal integer number of independent samples $n_{\\mathrm{MC}}$ such that the MC estimator has standard error at most $\\varepsilon$; that is, find $n_{\\mathrm{MC}}$ so that the square root of the variance of the sample mean is at most $\\varepsilon$.\n- For the tensor-grid quadrature, use the one-dimensional composite trapezoidal rule in each coordinate with the same number $m$ of panels per dimension, and take the tensor product to obtain the $d$-dimensional quadrature. Exploit the separability of $f$ to construct a computable upper bound on the $d$-dimensional quadrature error in terms of $d$, $m$, the one-dimensional integral $I_1 = \\int_{0}^{1} g(t)\\,dt$, and the one-dimensional trapezoidal error bound for $g$. From this bound, determine the minimal integer $m$ such that the absolute error is at most $\\varepsilon$, and then compute the total number of grid points $n_{\\mathrm{TG}} = (m+1)^d$ used by the tensor-grid rule.\n\nYour program should, for each test case $(d,\\varepsilon)$, compute:\n- $n_{\\mathrm{MC}}$, the minimal integer number of samples for Monte Carlo to achieve standard error at most $\\varepsilon$.\n- $n_{\\mathrm{TG}}$, the minimal integer number of tensor-grid points to achieve an upper bound on the absolute error at most $\\varepsilon$.\n- A boolean indicating whether MC dominates in sample efficiency, defined as $n_{\\mathrm{MC}} < n_{\\mathrm{TG}}$.\n\nTest Suite:\nUse the following set of $(d,\\varepsilon)$ pairs to exercise different regimes:\n- Case $1$: $d = 1$, $\\varepsilon = 10^{-3}$ (one-dimensional baseline).\n- Case $2$: $d = 5$, $\\varepsilon = 10^{-3}$ (moderate dimension, tight tolerance).\n- Case $3$: $d = 10$, $\\varepsilon = 10^{-3}$ (higher dimension, tight tolerance).\n- Case $4$: $d = 20$, $\\varepsilon = 10^{-3}$ (high dimension, tight tolerance).\n- Case $5$: $d = 1$, $\\varepsilon = 10^{-2}$ (one-dimensional, looser tolerance).\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes an inner list of the form $[n_{\\mathrm{MC}},n_{\\mathrm{TG}},\\mathrm{MC\\_dominates}]$. The booleans should be printed as either `True` or `False`. No spaces are permitted in the output. For example, the output should look like $[[\\text{case1}],[\\text{case2}],\\dots]$ with no spaces anywhere.", "solution": "The problem requires the determination of the minimum number of function evaluations for two numerical integration methods—Monte Carlo (MC) sampling and tensor-product composite trapezoidal quadrature—to estimate the integral $I_d = \\int_{[0,1]^d} f(\\mathbf{x}) \\, d\\mathbf{x}$ to a specified tolerance $\\varepsilon$. The integrand is a separable function $f(\\mathbf{x}) = \\prod_{i=1}^{d} g(x_i)$ with $g(t) = \\sin(\\pi t)$ on the unit hypercube $[0,1]^d$.\n\nFirst, we validate the problem statement.\n**Step 1: Extract Givens**\n- **Function**: $f(\\mathbf{x}) = \\prod_{i=1}^{d} g(x_i)$ on the domain $[0,1]^d$.\n- **Constituent function**: $g(t) = \\sin(\\pi t)$, with angles in radians.\n- **Integral**: $I_d = \\int_{[0,1]^d} f(\\mathbf{x}) \\, d\\mathbf{x}$.\n- **Methods**: Monte Carlo (MC) sampling and tensor-product grid composite trapezoidal quadrature.\n- **Tolerance**: $\\varepsilon$.\n- **MC Condition**: The standard error of the MC estimator must be at most $\\varepsilon$.\n- **Tensor-Grid (TG) Condition**: The absolute error of the TG quadrature must be at most $\\varepsilon$.\n- **1D Trapezoidal Error Bound**: For $m$ panels on $[0,1]$, $|\\int_{0}^{1} \\phi(x)\\,dx - T_m[\\phi]| \\le \\frac{1}{12 m^2} \\sup_{x \\in [0,1]} |\\phi''(x)|$.\n- **Outputs per test case $(d, \\varepsilon)$**:\n    1.  $n_{\\mathrm{MC}}$: minimal integer number of MC samples.\n    2.  $n_{\\mathrm{TG}}$: minimal integer number of TG grid points.\n    3.  A boolean for $n_{\\mathrm{MC}} < n_{\\mathrm{TG}}$.\n- **Test Suite**: $(d,\\varepsilon)$ pairs are $(1, 10^{-3}), (5, 10^{-3}), (10, 10^{-3}), (20, 10^{-3}), (1, 10^{-2})$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded in standard principles of numerical analysis and probability theory. The function $g(t) = \\sin(\\pi t)$ is well-behaved and infinitely differentiable. The problem is well-posed, as it asks for minimal integer sample counts based on derived error bounds. The definitions and constraints are self-contained and mathematically precise, allowing for a unique solution. The setup is not contradictory, unrealistic, or ill-posed.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. We proceed with deriving the necessary formulae.\n\n### Monte Carlo Method Analysis\n\nThe Monte Carlo estimator for $I_d$ based on $n$ independent samples $\\mathbf{X}_j$ drawn uniformly from $[0,1]^d$ is the sample mean $\\hat{I}_{d,n} = \\frac{1}{n} \\sum_{j=1}^{n} f(\\mathbf{X}_j)$. This estimator is unbiased, i.e., $E[\\hat{I}_{d,n}] = I_d$.\n\nThe standard error of the estimator is the square root of its variance:\n$$\n\\mathrm{SE}(\\hat{I}_{d,n}) = \\sqrt{\\mathrm{Var}(\\hat{I}_{d,n})} = \\sqrt{\\mathrm{Var}\\left(\\frac{1}{n} \\sum_{j=1}^{n} f(\\mathbf{X}_j)\\right)} = \\frac{1}{\\sqrt{n}} \\sqrt{\\mathrm{Var}(f(\\mathbf{X}))}\n$$\nThe condition is $\\mathrm{SE}(\\hat{I}_{d,n}) \\le \\varepsilon$, which implies $\\frac{\\sqrt{\\mathrm{Var}(f(\\mathbf{X}))}}{\\sqrt{n}} \\le \\varepsilon$, or $n \\ge \\frac{\\mathrm{Var}(f(\\mathbf{X}))}{\\varepsilon^2}$.\nThe minimal integer number of samples is $n_{\\mathrm{MC}} = \\left\\lceil \\frac{\\mathrm{Var}(f(\\mathbf{X}))}{\\varepsilon^2} \\right\\rceil$.\n\nWe must compute $\\mathrm{Var}(f(\\mathbf{X})) = E[f(\\mathbf{X})^2] - (E[f(\\mathbf{X})])^2$.\nDue to the separability of $f$ and the independence of the components of $\\mathbf{X}$, the expectations are products of one-dimensional expectations. Let $X \\sim U[0,1]$.\n\n1.  The expectation of $g(X)$:\n    $$\n    E[g(X)] = \\int_0^1 g(t)\\,dt = \\int_0^1 \\sin(\\pi t)\\,dt = \\left[-\\frac{1}{\\pi}\\cos(\\pi t)\\right]_0^1 = -\\frac{1}{\\pi}(\\cos(\\pi) - \\cos(0)) = \\frac{2}{\\pi}\n    $$\n    Let this be $I_1 = \\frac{2}{\\pi}$. Then $I_d = E[f(\\mathbf{X})] = (I_1)^d = \\left(\\frac{2}{\\pi}\\right)^d$.\n\n2.  The expectation of $g(X)^2$:\n    $$\n    E[g(X)^2] = \\int_0^1 g(t)^2\\,dt = \\int_0^1 \\sin^2(\\pi t)\\,dt = \\int_0^1 \\frac{1 - \\cos(2\\pi t)}{2}\\,dt = \\frac{1}{2}\\left[t - \\frac{\\sin(2\\pi t)}{2\\pi}\\right]_0^1 = \\frac{1}{2}\n    $$\n    Then $E[f(\\mathbf{X})^2] = \\prod_{i=1}^d E[g(X_i)^2] = \\left(\\frac{1}{2}\\right)^d$.\n\nThe variance of $f(\\mathbf{X})$ is:\n$$\n\\mathrm{Var}(f(\\mathbf{X})) = \\left(\\frac{1}{2}\\right)^d - \\left(\\left(\\frac{2}{\\pi}\\right)^d\\right)^2 = \\left(\\frac{1}{2}\\right)^d - \\left(\\frac{2}{\\pi}\\right)^{2d}\n$$\nSo, the minimal number of MC samples is:\n$$\nn_{\\mathrm{MC}} = \\left\\lceil \\frac{(1/2)^d - (2/\\pi)^{2d}}{\\varepsilon^2} \\right\\rceil\n$$\n\n### Tensor-Product Quadrature Analysis\n\nThe $d$-dimensional tensor-product trapezoidal rule $T_{m,d}$ with $m$ panels in each dimension is the product of $d$ one-dimensional trapezoidal rules $T_m$. For a separable function $f(\\mathbf{x}) = \\prod_{i=1}^d g(x_i)$, the integral is $T_{m,d}[f] = \\prod_{i=1}^d T_m[g]$.\n\nThe error is $I_d - T_{m,d}[f] = (I_1)^d - (T_m[g])^d$. We can express this difference using the identity $A^d - B^d = (A-B)\\sum_{k=0}^{d-1} A^{d-1-k} B^k$. Let $E_m = I_1 - T_m[g]$.\nThe total error is $(I_1 - T_m[g]) \\sum_{k=0}^{d-1} (I_1)^{d-1-k} (T_m[g])^k = E_m \\sum_{k=0}^{d-1} (I_1)^{d-1-k} (T_m[g])^k$.\nTo bound this, we need to bound $|T_m[g]|$. The function $g(t)=\\sin(\\pi t)$ is concave on $[0,1]$ because $g''(t) = -\\pi^2 \\sin(\\pi t) \\le 0$ for $t \\in [0,1]$. For a concave function, the trapezoidal rule underestimates the integral, so $T_m[g] \\le I_1$. Since $g(t) \\ge 0$, we have $0 \\le T_m[g] \\le I_1$.\nThus, $|T_m[g]| \\le I_1 = |I_1|$.\n\nThe absolute error is bounded by:\n$$\n|I_d - T_{m,d}[f]| \\le |E_m| \\sum_{k=0}^{d-1} |I_1|^{d-1-k} |I_1|^k = |E_m| \\sum_{k=0}^{d-1} |I_1|^{d-1} = d |I_1|^{d-1} |E_m|\n$$\nNow we use the given 1D error bound $|E_m| = |I_1 - T_m[g]| \\le \\frac{1}{12 m^2} \\sup_{t \\in [0,1]} |g''(t)|$.\nFor $g(t) = \\sin(\\pi t)$, $g''(t) = -\\pi^2 \\sin(\\pi t)$. The supremum is $\\sup_{t \\in [0,1]} |-\\pi^2 \\sin(\\pi t)| = \\pi^2$.\nSo, $|E_m| \\le \\frac{\\pi^2}{12m^2}$.\n\nThe total error bound becomes:\n$$\n|I_d - T_{m,d}[f]| \\le d \\left(\\frac{2}{\\pi}\\right)^{d-1} \\frac{\\pi^2}{12m^2}\n$$\nWe require this bound to be at most $\\varepsilon$:\n$$\nd \\left(\\frac{2}{\\pi}\\right)^{d-1} \\frac{\\pi^2}{12m^2} \\le \\varepsilon \\implies m^2 \\ge \\frac{d \\pi^2}{12 \\varepsilon} \\left(\\frac{2}{\\pi}\\right)^{d-1}\n$$\nThe minimal integer number of panels $m$ per dimension is:\n$$\nm = \\left\\lceil \\sqrt{\\frac{d \\pi^2}{12 \\varepsilon} \\left(\\frac{2}{\\pi}\\right)^{d-1}} \\right\\rceil\n$$\nThe composite trapezoidal rule with $m$ panels uses $m+1$ points in each dimension. For the $d$-dimensional tensor grid, the total number of grid points is $n_{\\mathrm{TG}} = (m+1)^d$.\n\n### Summary and Computation\nFor each test case $(d, \\varepsilon)$, we compute:\n1.  $n_{\\mathrm{MC}} = \\left\\lceil \\frac{(1/2)^d - (2/\\pi)^{2d}}{\\varepsilon^2} \\right\\rceil$\n2.  $m = \\left\\lceil \\sqrt{\\frac{d \\pi^2}{12 \\varepsilon} (2/\\pi)^{d-1}} \\right\\rceil$ and $n_{\\mathrm{TG}} = (m+1)^d$.\n3.  The boolean value of $n_{\\mathrm{MC}} < n_{\\mathrm{TG}}$.\n\nThese calculations will be implemented in the final program.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares the sample counts for Monte Carlo and tensor-grid\n    quadrature for a given set of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1, 10**-3),  # Case 1\n        (5, 10**-3),  # Case 2\n        (10, 10**-3), # Case 3\n        (20, 10**-3), # Case 4\n        (1, 10**-2),  # Case 5\n    ]\n\n    results = []\n    for d, epsilon in test_cases:\n        # --- Monte Carlo Calculation ---\n        # Variance of f(X), where X is uniform on [0,1]^d\n        # Var(f) = E[f^2] - (E[f])^2\n        # E[f] = (integral_0^1 sin(pi*t) dt)^d = (2/pi)^d\n        # E[f^2] = (integral_0^1 sin^2(pi*t) dt)^d = (1/2)^d\n        var_f = (0.5)**d - (2 / np.pi)**(2 * d)\n        \n        # n_MC >= Var(f) / epsilon^2\n        # The number of samples must be an integer, so we take the ceiling.\n        n_mc = np.ceil(var_f / epsilon**2)\n        \n        # --- Tensor-Grid Quadrature Calculation ---\n        # Error_d <= d * |I_1|^(d-1) * Error_1\n        # Error_1 <= sup|g''| / (12 * m^2) = pi^2 / (12 * m^2)\n        # We need Error_d <= epsilon\n        # d * (2/pi)^(d-1) * pi^2 / (12 * m^2) <= epsilon\n        # m^2 >= (d * pi^2 / (12 * epsilon)) * (2/pi)^(d-1)\n        \n        m_squared = (d * np.pi**2 / (12 * epsilon)) * (2 / np.pi)**(d - 1)\n        \n        # Minimal integer number of panels m\n        m = np.ceil(np.sqrt(m_squared))\n        \n        # Total number of grid points is (m+1)^d.\n        # Python's int handles arbitrary-precision integers, so overflow is not an issue.\n        n_tg = (int(m) + 1)**d\n        \n        # --- Comparison ---\n        mc_dominates = bool(n_mc < n_tg)\n        \n        # Append results as integers and a boolean.\n        # Use int() to convert from numpy float types.\n        results.append([int(n_mc), n_tg, mc_dominates])\n\n    # Final print statement in the exact required format.\n    # The format is [[case1_val1,case1_val2,...],[case2_val1,...],...] with no spaces.\n    # Using str() and then replace() is a robust way to achieve this.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3145824"}, {"introduction": "Beyond estimating quantities, probability theory provides a powerful toolkit for analyzing and certifying the performance of randomized algorithms. This practice demonstrates a classic application of concentration inequalities to algorithm design, where you will use Chernoff bounds to analyze a randomized rounding scheme for a linear program relaxation [@problem_id:3145804]. Your goal is to determine the number of independent trials required to guarantee that a set of constraints holds with high probability, a critical skill for designing reliable algorithms in the face of uncertainty.", "problem": "You will design and implement a deterministic program that computes how many independent repetitions of a randomized rounding procedure are sufficient to make a system of linear constraints hold within a specified multiplicative tolerance with high probability. The scenario is as follows. You are given a binary matrix $A \\in \\{0,1\\}^{m \\times n}$ and a fractional vector $x \\in [0,1]^n$ that represents a feasible solution to a linear programming relaxation. Consider randomized rounding that independently sets $X_i \\in \\{0,1\\}$ with $\\mathbb{P}(X_i = 1) = x_i$ for $i \\in \\{1,\\dots,n\\}$. For each constraint index $j \\in \\{1,\\dots,m\\}$ define the random sum $S_j = \\sum_{i=1}^n A_{j,i} X_i$ and its expectation $\\mu_j = \\mathbb{E}[S_j] = \\sum_{i=1}^n A_{j,i} x_i$. We say a single rounding trial is successful if $S_j \\le (1+\\varepsilon)\\,\\mu_j$ for all $j \\in \\{1,\\dots,m\\}$. You will not simulate $X_i$; instead, you will bound failure probabilities analytically.\n\nStarting from fundamental principles of probability, specifically Markov's inequality applied to the moment generating function (MGF) of a sum of independent Bernoulli random variables, derive a valid multiplicative Chernoff-style upper-tail bound on $\\mathbb{P}(S_j > (1+\\varepsilon)\\,\\mu_j)$ as a function of $\\mu_j$ and $\\varepsilon$ for any fixed $j$ and any fixed $\\varepsilon > 0$. Then, using the union bound across $m$ constraints, obtain an upper bound $q \\in [0,1]$ on the per-round failure probability that at least one constraint violates the $(1+\\varepsilon)$ multiplicative tolerance. Finally, use independence across repeated, fresh randomized rounding trials to determine the minimal integer $t \\ge 1$ such that the probability that at least one of the $t$ trials is successful is at least $1 - \\alpha$, where $\\alpha \\in (0,1)$ is a given target failure probability. If your bound is too weak to certify success for any finite number of trials (i.e., your per-round failure bound $q$ is greater than or equal to $1$), then report the special value $-1$ for that test case.\n\nYour program must implement this pipeline deterministically, using only the derived analytical bounds and the following rules for edge cases:\n- If $\\mu_j = 0$ for some row $j$, then $S_j = 0$ almost surely under the model, so set its single-constraint failure bound to $0$.\n- Combine single-constraint bounds using the union bound to obtain the per-round failure bound $q = \\min\\left(1, \\sum_{j=1}^m \\text{bound}_j\\right)$.\n- If $q = 0$, return $t = 1$.\n- If $q \\ge 1$, return $t = -1$.\n- Otherwise, return the smallest integer $t \\ge 1$ that certifies the target reliability level $1 - \\alpha$ using only independence across rounds and your per-round failure bound.\n\nUse natural logarithms for any required logarithmic calculations. Do not use randomness. All outputs are unitless numbers.\n\nTest suite to implement and evaluate:\n- Case A: $A = \\begin{bmatrix} 1 & 1 & 0 & 1 & 0 \\\\ 0 & 1 & 1 & 0 & 1 \\\\ 1 & 0 & 1 & 0 & 1 \\end{bmatrix}$, $x = [\\,0.4,\\,0.5,\\,0.3,\\,0.6,\\,0.2\\,]$, $\\varepsilon = 2.0$, $\\alpha = 0.0001$.\n- Case B: $A$ is $2 \\times 10$ with all entries equal to $1$, $x = [\\,0.5,\\,0.5,\\,0.5,\\,0.5,\\,0.5,\\,0.5,\\,0.5,\\,0.5,\\,0.5,\\,0.5\\,]$, $\\varepsilon = 2.5$, $\\alpha = 0.0003$.\n- Case C: $A = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix}$, $x = [\\,0.3,\\,0.3,\\,0.0\\,]$, $\\varepsilon = 0.1$, $\\alpha = 0.01$.\n- Case D: $A = \\begin{bmatrix} 0 & 0 & 0 & 0 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix}$, $x = [\\,0.0,\\,0.0,\\,0.0,\\,0.5\\,]$, $\\varepsilon = 1.0$, $\\alpha = 0.1$.\n- Case E: $A$ is $3 \\times 8$ with all entries equal to $1$, $x = [\\,0.6,\\,0.6,\\,0.6,\\,0.6,\\,0.6,\\,0.6,\\,0.6,\\,0.6\\,]$, $\\varepsilon = 1.5$, $\\alpha = 0.01$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4,r_5]$), where $r_k$ is the integer result for the $k$-th test case in the order A, B, C, D, E, and equals either the minimal number of trials $t$ or $-1$ when certification is impossible under the stated bound.", "solution": "The problem requires the determination of the minimum number of independent trials, $t$, of a randomized rounding procedure to ensure that a system of linear constraints holds with a specified high probability, $1-\\alpha$. This will be accomplished by first deriving a Chernoff-style probability bound for the violation of a single constraint, then combining these bounds for all constraints using the union bound, and finally calculating the number of trials needed to meet the overall reliability target.\n\nThe problem is validated as scientifically sound, well-posed, objective, and self-contained, based on fundamental principles of probability theory.\n\n### Step 1: Derivation of the Single-Constraint Failure Probability Bound\n\nLet $S_j = \\sum_{i=1}^n A_{j,i} X_i$ be the random sum for the $j$-th constraint, where $X_i$ are independent Bernoulli random variables with $\\mathbb{P}(X_i = 1) = x_i$. The expectation is $\\mu_j = \\mathbb{E}[S_j] = \\sum_{i=1}^n A_{j,i} x_i$. We want to find an upper bound for the failure probability $\\mathbb{P}(S_j > (1+\\varepsilon)\\mu_j)$ for a given $\\varepsilon > 0$.\n\nWe start with Markov's inequality, which states that for any non-negative random variable $Y$ and any constant $a > 0$, $\\mathbb{P}(Y \\ge a) \\le \\frac{\\mathbb{E}[Y]}{a}$.\nTo obtain a tighter bound, we apply this inequality to the exponential of our random variable. For any $\\lambda > 0$, the event $S_j > (1+\\varepsilon)\\mu_j$ is equivalent to $e^{\\lambda S_j} > e^{\\lambda(1+\\varepsilon)\\mu_j}$. Applying Markov's inequality with $Y = e^{\\lambda S_j}$ and $a = e^{\\lambda(1+\\varepsilon)\\mu_j}$:\n$$\n\\mathbb{P}(S_j > (1+\\varepsilon)\\mu_j) = \\mathbb{P}(e^{\\lambda S_j} > e^{\\lambda(1+\\varepsilon)\\mu_j}) \\le \\frac{\\mathbb{E}[e^{\\lambda S_j}]}{e^{\\lambda(1+\\varepsilon)\\mu_j}}\n$$\nThe term $\\mathbb{E}[e^{\\lambda S_j}]$ is the moment generating function (MGF) of $S_j$. Since $S_j = \\sum_{i=1}^n A_{j,i} X_i$ and the $X_i$ are independent, the MGF of the sum is the product of the MGFs:\n$$\n\\mathbb{E}[e^{\\lambda S_j}] = \\mathbb{E}\\left[\\exp\\left(\\lambda \\sum_{i=1}^n A_{j,i} X_i\\right)\\right] = \\mathbb{E}\\left[\\prod_{i=1}^n e^{\\lambda A_{j,i} X_i}\\right] = \\prod_{i=1}^n \\mathbb{E}[e^{\\lambda A_{j,i} X_i}]\n$$\nSince $A_{j,i} \\in \\{0,1\\}$, if $A_{j,i}=0$, the term is $\\mathbb{E}[e^0] = 1$. If $A_{j,i}=1$, the term is the MGF of $X_i$:\n$$\n\\mathbb{E}[e^{\\lambda X_i}] = (1-x_i)e^{\\lambda \\cdot 0} + x_i e^{\\lambda \\cdot 1} = 1 - x_i + x_i e^\\lambda = 1 + x_i(e^\\lambda - 1)\n$$\nWe use the inequality $1+z \\le e^z$ for any real $z$. Setting $z = x_i(e^\\lambda - 1)$, we get $1 + x_i(e^\\lambda - 1) \\le e^{x_i(e^\\lambda-1)}$.\nSubstituting this into the product form of the MGF:\n$$\n\\mathbb{E}[e^{\\lambda S_j}] \\le \\prod_{i: A_{j,i}=1} e^{x_i(e^\\lambda - 1)} = \\exp\\left(\\sum_{i: A_{j,i}=1} x_i(e^\\lambda - 1)\\right) = \\exp\\left((e^\\lambda - 1)\\sum_{i: A_{j,i}=1} x_i\\right)\n$$\nBy definition, $\\mu_j = \\sum_{i=1}^n A_{j,i} x_i = \\sum_{i: A_{j,i}=1} x_i$. Thus, we have the bound:\n$$\n\\mathbb{E}[e^{\\lambda S_j}] \\le e^{\\mu_j(e^\\lambda - 1)}\n$$\nSubstituting this back into the Markov inequality expression:\n$$\n\\mathbb{P}(S_j > (1+\\varepsilon)\\mu_j) \\le \\frac{e^{\\mu_j(e^\\lambda - 1)}}{e^{\\lambda(1+\\varepsilon)\\mu_j}} = \\exp\\left(\\mu_j(e^\\lambda - 1) - \\lambda(1+\\varepsilon)\\mu_j\\right)\n$$\nThis bound holds for any $\\lambda > 0$. To obtain the tightest bound, we minimize the exponent with respect to $\\lambda$. Let $f(\\lambda) = \\mu_j(e^\\lambda - 1) - \\lambda(1+\\varepsilon)\\mu_j$. The derivative is $f'(\\lambda) = \\mu_j e^\\lambda - (1+\\varepsilon)\\mu_j$. Setting $f'(\\lambda) = 0$ gives $e^\\lambda = 1+\\varepsilon$, which means $\\lambda = \\ln(1+\\varepsilon)$. Since $\\varepsilon > 0$, we have $\\lambda > 0$ as required.\n\nSubstituting this optimal $\\lambda$ back into the exponent:\n$$\n\\mu_j(e^{\\ln(1+\\varepsilon)} - 1) - \\ln(1+\\varepsilon)(1+\\varepsilon)\\mu_j = \\mu_j((1+\\varepsilon) - 1) - (1+\\varepsilon)\\mu_j\\ln(1+\\varepsilon) = \\mu_j(\\varepsilon - (1+\\varepsilon)\\ln(1+\\varepsilon))\n$$\nThus, the Chernoff bound for the failure of a single constraint $j$ is:\n$$\n\\mathbb{P}(S_j > (1+\\varepsilon)\\mu_j) \\le \\exp\\left(\\mu_j(\\varepsilon - (1+\\varepsilon)\\ln(1+\\varepsilon))\\right) = \\left[\\frac{e^\\varepsilon}{(1+\\varepsilon)^{1+\\varepsilon}}\\right]^{\\mu_j}\n$$\nLet's denote this bound by $\\text{bound}_j$. As per the problem statement, if $\\mu_j=0$, then $S_j=0$ almost surely, so its failure probability is $0$. We set $\\text{bound}_j = 0$ in this case.\n\n### Step 2: Per-Round Failure Probability\n\nA single rounding trial fails if at least one constraint is violated. Let $F_j$ be the event $S_j > (1+\\varepsilon)\\mu_j$. The total failure probability for one trial is $\\mathbb{P}(\\cup_{j=1}^m F_j)$. Using the union bound (Boole's inequality):\n$$\n\\mathbb{P}\\left(\\bigcup_{j=1}^m F_j\\right) \\le \\sum_{j=1}^m \\mathbb{P}(F_j) \\le \\sum_{j=1}^m \\text{bound}_j\n$$\nThe problem defines the upper bound on the per-round failure probability, $q$, as:\n$$\nq = \\min\\left(1, \\sum_{j=1}^m \\text{bound}_j\\right)\n$$\n\n### Step 3: Minimal Number of Trials\n\nWe need to find the minimum number of independent trials, $t \\ge 1$, such that the probability of having at least one successful trial is at least $1-\\alpha$. Let $q$ be our upper bound on the failure probability of a single trial.\nThe probability of at least one success in $t$ trials is $1 - \\mathbb{P}(\\text{all } t \\text{ trials fail})$.\nThe trials are independent, so $\\mathbb{P}(\\text{all } t \\text{ trials fail}) = (\\mathbb{P}(\\text{one trial fails}))^t$. Since $\\mathbb{P}(\\text{one trial fails}) \\le q$, we have $\\mathbb{P}(\\text{all } t \\text{ trials fail}) \\le q^t$.\nWe require $1 - \\mathbb{P}(\\text{all } t \\text{ trials fail}) \\ge 1-\\alpha$, which is equivalent to $\\mathbb{P}(\\text{all } t \\text{ trials fail}) \\le \\alpha$.\nTo satisfy this, we must find the smallest integer $t \\ge 1$ such that $q^t \\le \\alpha$.\n\nWe analyze the cases for $q$:\n1.  If $q = 0$: Failure is impossible according to our bound. A single trial is guaranteed to succeed. Thus, $t=1$.\n2.  If $q \\ge 1$: The bound is too weak. For any $t \\ge 1$, $q^t \\ge 1$. Since $\\alpha \\in (0,1)$, the condition $q^t \\le \\alpha$ can never be satisfied. In this case, certification is impossible, and we report $t=-1$.\n3.  If $0 < q < 1$: We take the natural logarithm of both sides of $q^t \\le \\alpha$:\n    $t \\ln(q) \\le \\ln(\\alpha)$.\n    Since $0 < q < 1$, $\\ln(q)$ is negative. Dividing by $\\ln(q)$ reverses the inequality sign:\n    $t \\ge \\frac{\\ln(\\alpha)}{\\ln(q)}$.\n    Since $t$ must be an integer, we take the ceiling of the right-hand side. The smallest integer $t \\ge 1$ satisfying this is:\n    $$\n    t = \\left\\lceil \\frac{\\ln(\\alpha)}{\\ln(q)} \\right\\rceil\n    $$\n\nThe overall algorithm is as follows:\n1. For a given test case ($A, x, \\varepsilon, \\alpha$), compute the vector of expectations $\\mu = Ax$.\n2. For each constraint $j=1,\\dots,m$, calculate the Chernoff bound:\n   - If $\\mu_j=0$, $\\text{bound}_j = 0$.\n   - Otherwise, $\\text{bound}_j = \\exp(\\mu_j(\\varepsilon - (1+\\varepsilon)\\ln(1+\\varepsilon)))$.\n3. Compute the per-round failure probability bound $q = \\min(1, \\sum_{j=1}^m \\text{bound}_j)$.\n4. Determine the number of trials $t$ based on $q$ and $\\alpha$ using the rules derived above.\n\nThis process is deterministic and relies solely on the provided inputs and derived analytical bounds.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the minimal number of randomized rounding trials required\n    to satisfy a system of linear constraints with high probability.\n    \"\"\"\n    \n    test_cases = [\n        # Case A\n        (\n            np.array([\n                [1, 1, 0, 1, 0],\n                [0, 1, 1, 0, 1],\n                [1, 0, 1, 0, 1]\n            ]),\n            np.array([0.4, 0.5, 0.3, 0.6, 0.2]),\n            2.0,\n            0.0001\n        ),\n        # Case B\n        (\n            np.ones((2, 10)),\n            np.full(10, 0.5),\n            2.5,\n            0.0003\n        ),\n        # Case C\n        (\n            np.array([\n                [1, 0, 0],\n                [0, 1, 0]\n            ]),\n            np.array([0.3, 0.3, 0.0]),\n            0.1,\n            0.01\n        ),\n        # Case D\n        (\n            np.array([\n                [0, 0, 0, 0],\n                [1, 1, 1, 1]\n            ]),\n            np.array([0.0, 0.0, 0.0, 0.5]),\n            1.0,\n            0.1\n        ),\n        # Case E\n        (\n            np.ones((3, 8)),\n            np.full(8, 0.6),\n            1.5,\n            0.01\n        )\n    ]\n\n    results = []\n\n    for A, x, epsilon, alpha in test_cases:\n        # Step 1: Calculate expectations mu_j for each constraint\n        mu = A @ x\n        \n        # Step 2: Calculate the Chernoff bound for each constraint's failure probability\n        single_constraint_bounds = []\n        if epsilon > 0:\n            # Pre-calculate the log of the base of the Chernoff bound for numerical stability\n            log_chernoff_base = epsilon - (1 + epsilon) * np.log(1 + epsilon)\n        \n        for mu_j in mu:\n            if mu_j == 0:\n                # If mu_j is 0, S_j is 0, so the constraint cannot be violated\n                bound_j = 0.0\n            else:\n                # The bound is (base)^mu_j, or exp(mu_j * log(base))\n                bound_j = np.exp(mu_j * log_chernoff_base)\n            single_constraint_bounds.append(bound_j)\n            \n        # Step 3: Use the union bound to get the per-round failure probability q\n        q = min(1.0, np.sum(single_constraint_bounds))\n        \n        # Step 4: Determine the minimal number of trials t\n        t = 0\n        if q == 0.0:\n            # If failure is impossible, 1 trial is sufficient\n            t = 1\n        elif q >= 1.0:\n            # If the failure bound is >= 1, the bound is too weak to certify success\n            t = -1\n        else: # 0 < q < 1\n            # We need q^t <= alpha, which means t >= log(alpha) / log(q)\n            # Since t must be an integer, we take the ceiling.\n            t = int(np.ceil(np.log(alpha) / np.log(q)))\n            \n        results.append(t)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3145804"}]}