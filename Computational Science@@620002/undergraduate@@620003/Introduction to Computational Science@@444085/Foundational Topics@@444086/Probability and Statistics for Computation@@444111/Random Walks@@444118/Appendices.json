{"hands_on_practices": [{"introduction": "We begin our hands-on practice with one of the most classic problems in the study of random walks: the Gambler's Ruin. This exercise asks you to calculate the probability that a random walk on a finite interval reaches one boundary before the other. By deriving this probability from first principles, you will master the technique of first-step analysis, a fundamental and widely applicable tool for analyzing stochastic processes. [@problem_id:3079249]", "problem": "Consider a one-dimensional simple symmetric random walk $\\{S_{n}\\}_{n \\geq 0}$ on the integers defined by $S_{0}=i$ with $0<i<N$ and $S_{n+1}=S_{n}+X_{n+1}$, where $\\{X_{n}\\}_{n \\geq 1}$ are independent and identically distributed random variables with $\\mathbb{P}(X_{n}=1)=\\mathbb{P}(X_{n}=-1)=\\tfrac{1}{2}$. Define the hitting times $\\tau_{0}=\\inf\\{n \\geq 0:S_{n}=0\\}$ and $\\tau_{N}=\\inf\\{n \\geq 0:S_{n}=N\\}$. Let $\\tau=\\tau_{0} \\wedge \\tau_{N}$. Using only fundamental properties of the simple symmetric random walk and logically justified steps grounded in these properties, derive a closed-form expression for the probability $\\mathbb{P}_{i}(\\tau_{N}<\\tau_{0})$ as a function of $i$ and $N$. Your final answer must be a single analytic expression. Do not assume or quote the target formula; instead, derive it from first principles and clearly justify each step in your reasoning.", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- The process is a one-dimensional simple symmetric random walk $\\{S_{n}\\}_{n \\geq 0}$ on the set of integers $\\mathbb{Z}$.\n- The initial state is $S_{0}=i$, where $i$ is an integer satisfying $0<i<N$.\n- The evolution of the walk is given by the recursive formula $S_{n+1}=S_{n}+X_{n+1}$ for $n \\geq 0$.\n- The steps $\\{X_{n}\\}_{n \\geq 1}$ are independent and identically distributed (i.i.d.) random variables.\n- The probability distribution for each step is $\\mathbb{P}(X_{n}=1)=\\mathbb{P}(X_{n}=-1)=\\frac{1}{2}$.\n- The hitting time for state $0$ is defined as $\\tau_{0}=\\inf\\{n \\geq 0:S_{n}=0\\}$.\n- The hitting time for state $N$ is defined as $\\tau_{N}=\\inf\\{n \\geq 0:S_{n}=N\\}$.\n- The time of interest is $\\tau=\\tau_{0} \\wedge \\tau_{N} = \\min(\\tau_0, \\tau_N)$, which is the first time the walk hits either boundary $0$ or $N$.\n- The objective is to derive a closed-form expression for the probability $\\mathbb{P}_{i}(\\tau_{N}<\\tau_{0})$ as a function of $i$ and $N$. The notation $\\mathbb{P}_{i}(\\cdot)$ denotes the probability conditional on the starting state $S_0=i$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is a canonical exercise in the theory of stochastic processes, specifically concerning discrete-time Markov chains and random walks. All definitions and concepts are standard and mathematically rigorous.\n- **Well-Posed**: The problem is well-posed. The simple symmetric random walk on a finite interval with absorbing boundaries is guaranteed to be absorbed at either $0$ or $N$ with probability $1$. Therefore, the probability $\\mathbb{P}_{i}(\\tau_{N}<\\tau_{0})$ is well-defined, and a unique solution exists.\n- **Objective**: The problem is stated using precise mathematical language, free from any ambiguity, subjectivity, or opinion.\n- **Flaws**: The problem statement exhibits none of the invalidating flaws listed in the instructions. It is scientifically sound, self-contained, consistent, and requires a derivation from first principles, which is a substantial task.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be derived.\n\n### Derivation\nLet $p_{i}$ denote the probability that the random walk starting from state $i$ hits state $N$ before it hits state $0$. Formally, this is written as:\n$$p_{i} = \\mathbb{P}(S_{\\tau}=N | S_{0}=i) = \\mathbb{P}_{i}(\\tau_{N}<\\tau_{0})$$\nWe are asked to find $p_i$ for any integer $i$ such that $0 < i < N$.\n\nFirst, we establish the boundary conditions for the problem.\nIf the walk starts at $i=0$, it has already hit the boundary $0$. Thus, $S_{0}=0$, which implies $\\tau_{0}=0$. Since $N>0$, the walk cannot have hit $N$ at or before time $0$, so $\\tau_{N}>0$. The condition $\\tau_{N}<\\tau_{0}$ is therefore impossible. The probability of this event is $0$.\n$$p_{0} = \\mathbb{P}_{0}(\\tau_{N}<\\tau_{0}) = 0$$\nIf the walk starts at $i=N$, it has already hit the boundary $N$. Thus, $S_{0}=N$, which implies $\\tau_{N}=0$. Since $0<N$, the walk cannot have hit $0$ at or before time $0$, so $\\tau_{0}>0$. The condition $\\tau_{N}<\\tau_{0}$ is therefore certain to be true. The probability of this event is $1$.\n$$p_{N} = \\mathbb{P}_{N}(\\tau_{N}<\\tau_{0}) = 1$$\n\nNow, consider a starting position $i$ such that $0 < i < N$. We can analyze the probability $p_i$ by conditioning on the outcome of the first step, $X_1$. Using the law of total probability, we can write:\n$$p_{i} = \\mathbb{P}_{i}(\\tau_{N}<\\tau_{0}) = \\sum_{x \\in \\{-1, 1\\}} \\mathbb{P}_{i}(\\tau_{N}<\\tau_{0} | X_{1}=x) \\mathbb{P}(X_{1}=x)$$\nGiven the problem statement, $\\mathbb{P}(X_{1}=1) = \\frac{1}{2}$ and $\\mathbb{P}(X_{1}=-1) = \\frac{1}{2}$. The equation becomes:\n$$p_{i} = \\mathbb{P}_{i}(\\tau_{N}<\\tau_{0} | X_{1}=1) \\cdot \\frac{1}{2} + \\mathbb{P}_{i}(\\tau_{N}<\\tau_{0} | X_{1}=-1) \\cdot \\frac{1}{2}$$\n\nNow we evaluate the conditional probabilities.\nIf $X_{1}=1$, the position of the walk at time $n=1$ is $S_{1} = S_{0}+X_{1} = i+1$. The random walk is a Markov process, meaning its future evolution depends only on its current state, not on its past history. Therefore, the probability of hitting $N$ before $0$, having moved from $i$ to $i+1$, is identical to the probability of hitting $N$ before $0$ when starting from state $i+1$. This is precisely the definition of $p_{i+1}$.\n$$\\mathbb{P}_{i}(\\tau_{N}<\\tau_{0} | X_{1}=1) = \\mathbb{P}_{i+1}(\\tau_{N}<\\tau_{0}) = p_{i+1}$$\nSimilarly, if $X_{1}=-1$, the position of the walk at time $n=1$ is $S_{1} = S_{0}+X_{1} = i-1$. By the same Markov property, the probability of subsequently hitting $N$ before $0$ is the same as starting from $i-1$.\n$$\\mathbb{P}_{i}(\\tau_{N}<\\tau_{0} | X_{1}=-1) = \\mathbb{P}_{i-1}(\\tau_{N}<\\tau_{0}) = p_{i-1}$$\n\nSubstituting these back into the equation for $p_i$, we obtain a linear second-order homogeneous difference equation for the probabilities $p_i$:\n$$p_{i} = \\frac{1}{2} p_{i+1} + \\frac{1}{2} p_{i-1}$$\nThis equation holds for all $i$ in the range $1 \\leq i \\leq N-1$. We can rearrange this equation to better understand its structure:\n$$2 p_{i} = p_{i+1} + p_{i-1}$$\n$$p_{i} - p_{i-1} = p_{i+1} - p_{i}$$\nThis equation shows that the difference between consecutive terms in the sequence $\\{p_0, p_1, \\dots, p_N\\}$ is constant. This is the defining property of an arithmetic progression. Therefore, the general solution for $p_i$ must be a linear function of $i$:\n$$p_{i} = A i + B$$\nfor some constants $A$ and $B$.\n\nWe can now use the boundary conditions, $p_0=0$ and $p_N=1$, to determine the values of $A$ and $B$.\nUsing $p_0 = 0$:\n$$p_{0} = A \\cdot 0 + B = B = 0$$\nThis simplifies the solution to $p_i = A i$.\n\nNext, using $p_N = 1$:\n$$p_{N} = A \\cdot N = 1$$\nSolving for $A$, we find:\n$$A = \\frac{1}{N}$$\n\nSubstituting the values of $A$ and $B$ back into the general solution $p_i = Ai + B$, we obtain the specific solution for the probability of hitting $N$ before $0$:\n$$p_{i} = \\frac{1}{N} \\cdot i + 0 = \\frac{i}{N}$$\nThis is the closed-form expression for the desired probability, $\\mathbb{P}_{i}(\\tau_{N}<\\tau_{0})$, derived from the first principles of the simple symmetric random walk.", "answer": "$$\\boxed{\\frac{i}{N}}$$", "id": "3079249"}, {"introduction": "Having determined the probability of reaching a boundary, a natural next question is: how long should we expect the walk to last? This practice challenges you to compute the expected time until the random walk is absorbed at either boundary. To solve this, you will employ the powerful Optional Stopping Theorem with a cleverly chosen martingale, $S_n^2 - n$, demonstrating a more advanced technique for analyzing the temporal properties of stochastic systems. [@problem_id:3079267]", "problem": "Consider the discrete-time simple symmetric random walk on the integers. Let $N \\in \\mathbb{N}$ be fixed and let $S_{0} = i$ with $i \\in \\{0, 1, \\dots, N\\}$. For $n \\geq 1$, define $S_{n} = S_{n-1} + X_{n}$, where $(X_{n})_{n \\geq 1}$ are independent and identically distributed random variables with $\\mathbb{P}(X_{n} = 1) = \\mathbb{P}(X_{n} = -1) = \\tfrac{1}{2}$. Define the hitting times\n$$\n\\tau_{0} = \\inf\\{n \\geq 0 : S_{n} = 0\\}, \\qquad \\tau_{N} = \\inf\\{n \\geq 0 : S_{n} = N\\},\n$$\nand the absorption time $\\tau = \\tau_{0} \\wedge \\tau_{N}$. Using only fundamental definitions of martingales and the optional stopping theorem (OST) for bounded stopping times, derive a closed-form expression for the expected absorption time $E_{i}[\\tau]$ as a function of $i$ and $N$. Your final answer must be a single analytic expression. Do not provide intermediate formulas that directly reveal the final expression; instead, justify each step from first principles. No rounding is required.", "solution": "The problem asks for the expected absorption time, denoted $E_{i}[\\tau]$, for a simple symmetric random walk on the integers, starting at $S_{0} = i$, with absorbing barriers at $0$ and $N$. The absorption time is $\\tau = \\inf\\{n \\geq 0 : S_{n} \\in \\{0, N\\}\\}$. The solution will be derived using martingale theory and the optional stopping theorem (OST).\n\nFirst, let us formalize the process. The random walk is defined by $S_{n} = S_{n-1} + X_{n}$ for $n \\geq 1$, with $S_{0}=i$. The steps $(X_{n})_{n \\geq 1}$ are independent and identically distributed random variables with $\\mathbb{P}(X_{n} = 1) = \\mathbb{P}(X_{n} = -1) = 1/2$. Let $\\mathcal{F}_{n} = \\sigma(X_{1}, \\dots, X_{n})$ be the natural filtration of the process, with $\\mathcal{F}_{0} = \\{\\emptyset, \\Omega\\}$. The expectation conditional on starting at $S_0=i$ is denoted by $E_i[\\cdot]$.\n\nThe Optional Stopping Theorem (OST) states that for a martingale $(M_{n})_{n \\geq 0}$ and a stopping time $T$, under certain conditions, $E[M_{T}] = E[M_{0}]$. The problem specifies using the OST for bounded stopping times. The stopping time $\\tau$ is not deterministically bounded. However, for a one-dimensional random walk on a finite interval, it is known that $\\tau$ is almost surely finite and $E_i[\\tau] < \\infty$. A rigorous application of the OST involves considering the bounded stopping times $\\tau_{k} = \\tau \\wedge k = \\min(\\tau, k)$ for $k \\in \\mathbb{N}$. Applying the OST to $\\tau_k$ gives $E[M_{\\tau_k}] = E[M_0]$. We can then take the limit as $k \\to \\infty$ and, provided appropriate convergence conditions (such as the Dominated Convergence Theorem or Monotone Convergence Theorem) are met, we can deduce that $E[M_{\\tau}] = E[M_0]$. We will proceed with this understanding.\n\nOur derivation requires two distinct martingales.\n\nFirst, we determine the probability of absorption at each boundary. Let $p_{i} = \\mathbb{P}_{i}(S_{\\tau} = N)$ be the probability that the walk is absorbed at $N$, given it starts at $i$. Consequently, $\\mathbb{P}_{i}(S_{\\tau} = 0) = 1 - p_{i}$. To find $p_{i}$, we consider the process $(S_{n})_{n \\geq 0}$ itself. We verify it is a martingale with respect to the filtration $(\\mathcal{F}_{n})_{n \\geq 0}$. For any $n \\geq 1$:\n$$\nE[S_{n} | \\mathcal{F}_{n-1}] = E[S_{n-1} + X_{n} | \\mathcal{F}_{n-1}]\n$$\nSince $S_{n-1}$ is $\\mathcal{F}_{n-1}$-measurable and $X_{n}$ is independent of $\\mathcal{F}_{n-1}$:\n$$\nE[S_{n} | \\mathcal{F}_{n-1}] = S_{n-1} + E[X_{n}]\n$$\nThe expectation of a single step is $E[X_{n}] = (1) \\cdot \\mathbb{P}(X_{n}=1) + (-1) \\cdot \\mathbb{P}(X_{n}=-1) = 1 \\cdot \\frac{1}{2} - 1 \\cdot \\frac{1}{2} = 0$.\nThus, $E[S_{n} | \\mathcal{F}_{n-1}] = S_{n-1}$, confirming that $(S_{n})_{n \\geq 0}$ is a martingale.\n\nThe increments of the martingale are bounded, $|S_{n} - S_{n-1}| = |X_{n}| = 1$. The stopping time $\\tau$ has a finite expectation. These are sufficient conditions for the OST to apply to the martingale $S_n$ and stopping time $\\tau$. Applying the OST:\n$$\nE_{i}[S_{\\tau}] = E_{i}[S_{0}] = i\n$$\nThe expected value of the position at absorption time $\\tau$ is given by:\n$$\nE_{i}[S_{\\tau}] = N \\cdot \\mathbb{P}_{i}(S_{\\tau} = N) + 0 \\cdot \\mathbb{P}_{i}(S_{\\tau} = 0) = N \\cdot p_{i}\n$$\nEquating the two expressions for $E_{i}[S_{\\tau}]$, we get $N \\cdot p_{i} = i$, which yields the absorption probability:\n$$\np_{i} = \\frac{i}{N}\n$$\n\nSecond, to find the expected time $E_{i}[\\tau]$, we need a martingale that incorporates the time index $n$. Let us consider the process $M_{n} = S_{n}^2 - n$. We verify if it is a martingale. For any $n \\geq 1$:\n$$\nE[M_{n} | \\mathcal{F}_{n-1}] = E[S_{n}^2 - n | \\mathcal{F}_{n-1}] = E[S_{n}^2 | \\mathcal{F}_{n-1}] - n\n$$\nWe expand $S_{n}^2 = (S_{n-1} + X_{n})^2 = S_{n-1}^2 + 2S_{n-1}X_{n} + X_{n}^2$.\n$$\nE[S_{n}^2 | \\mathcal{F}_{n-1}] = E[S_{n-1}^2 + 2S_{n-1}X_{n} + X_{n}^2 | \\mathcal{F}_{n-1}]\n$$\nUsing the linearity of conditional expectation and the fact that $S_{n-1}$ is $\\mathcal{F}_{n-1}$-measurable:\n$$\nE[S_{n}^2 | \\mathcal{F}_{n-1}] = S_{n-1}^2 + 2S_{n-1}E[X_{n} | \\mathcal{F}_{n-1}] + E[X_{n}^2 | \\mathcal{F}_{n-1}]\n$$\nAs before, $E[X_{n} | \\mathcal{F}_{n-1}] = E[X_{n}] = 0$. For the term $E[X_{n}^2 | \\mathcal{F}_{n-1}]$, we have $E[X_{n}^2] = (1)^2 \\cdot \\frac{1}{2} + (-1)^2 \\cdot \\frac{1}{2} = 1$.\nSubstituting these results back:\n$$\nE[S_{n}^2 | \\mathcal{F}_{n-1}] = S_{n-1}^2 + 2S_{n-1}(0) + 1 = S_{n-1}^2 + 1\n$$\nNow we can complete the calculation for $E[M_{n} | \\mathcal{F}_{n-1}]$:\n$$\nE[M_{n} | \\mathcal{F}_{n-1}] = (S_{n-1}^2 + 1) - n = S_{n-1}^2 - (n-1) = M_{n-1}\n$$\nThis demonstrates that $(M_{n})_{n \\geq 0} = (S_{n}^2 - n)_{n \\geq 0}$ is indeed a martingale.\n\nNow we apply the OST to the martingale $M_{n}$ and the stopping time $\\tau$. The conditions for OST are met because $E_i[\\tau] < \\infty$ and the martingale increments are bounded over a finite time interval.\n$$\nE_{i}[M_{\\tau}] = E_{i}[M_{0}]\n$$\nThe initial value is $M_{0} = S_{0}^2 - 0 = i^2$.\nThe value at the stopping time is $M_{\\tau} = S_{\\tau}^2 - \\tau$. Its expectation is:\n$$\nE_{i}[M_{\\tau}] = E_{i}[S_{\\tau}^2 - \\tau] = E_{i}[S_{\\tau}^2] - E_{i}[\\tau]\n$$\nEquating the two expressions for the expectation gives:\n$$\nE_{i}[S_{\\tau}^2] - E_{i}[\\tau] = i^2\n$$\nWe can rearrange this to express the expected absorption time:\n$$\nE_{i}[\\tau] = E_{i}[S_{\\tau}^2] - i^2\n$$\nTo finalize the solution, we must compute $E_{i}[S_{\\tau}^2]$. The position at absorption, $S_{\\tau}$, can only be $0$ or $N$.\n$$\nE_{i}[S_{\\tau}^2] = (N^2) \\cdot \\mathbb{P}_{i}(S_{\\tau} = N) + (0^2) \\cdot \\mathbb{P}_{i}(S_{\\tau} = 0) = N^2 \\cdot p_{i}\n$$\nUsing our previously derived result for the absorption probability, $p_{i} = i/N$:\n$$\nE_{i}[S_{\\tau}^2] = N^2 \\cdot \\left(\\frac{i}{N}\\right) = Ni\n$$\nFinally, substituting this back into our expression for $E_{i}[\\tau]$:\n$$\nE_{i}[\\tau] = Ni - i^2\n$$\nThis can be factored to give the final closed-form expression for the expected absorption time as a function of the starting position $i$ and the boundary $N$.\n$$\nE_{i}[\\tau] = i(N-i)\n$$\nThis result is valid for any starting position $i \\in \\{0, 1, \\dots, N\\}$. If $i=0$ or $i=N$, the starting position is already at an absorbing boundary, so $\\tau=0$, and the formula correctly yields $0(N-0)=0$ and $N(N-N)=0$, respectively.", "answer": "$$\n\\boxed{i(N-i)}\n$$", "id": "3079267"}, {"introduction": "Theoretical analysis provides deep insights, but computational science thrives on simulation. This final practice bridges the gap between theory and implementation, focusing on a critical aspect of simulating random phenomena: the proper generation of parallel random number streams. You will investigate how naive seeding of pseudo-random number generators (PRNGs) can lead to catastrophic statistical artifacts and then implement robust solutions to ensure your parallel simulations are truly independent. [@problem_id:3183815]", "problem": "Consider a one-dimensional random walk constructed from a sequence of independent and identically distributed step increments, where each increment is a Rademacher random variable taking values in $\\{-1,+1\\}$ with probability $1/2$ for each outcome. Let `T` denote the number of parallel threads used to generate `T` independent random walk trajectories, and let `N` denote the number of steps per trajectory. For thread index `i` $\\in \\{1,\\dots,T\\}$ and step index `t` $\\in \\{1,\\dots,N\\}$, let $X_{t,i} \\in \\{-1,+1\\}$ denote the increment at time `t` on thread `i`, and define the endpoint (position after `N` steps) for thread `i` as $S_i(N) = \\sum_{t=1}^{N} X_{t,i}$. In practice, pseudo-random number generators (PRNGs) are used to produce the increments $\\{X_{t,i}\\}$, and careless seeding across threads can induce correlations between the sequences $\\{X_{t,i}\\}_{t=1}^{N}$ and $\\{X_{t,j}\\}_{t=1}^{N}$ for distinct threads $i \\neq j$. Your task is to quantify artifacts caused by shared seeds across threads and to implement two remedies that mitigate such artifacts.\n\nStarting from the definition of the random walk and the requirement that step increments should be independent across threads, implement three generation regimes:\n- Naive regime: construct `T` PRNG instances using the same base seed `s`, one per thread, and draw the `N` steps per thread directly from each instance.\n- Independent-streams regime: construct `T` independent PRNG instances by spawning `T` child seeds from a single base seed `s` via a seed-derivation mechanism designed to produce statistically independent streams.\n- Leapfrogging regime: use a single PRNG instance with base seed `s` and draw `T` $\\cdot$ `N` increments, and assign to thread `i` the subsequence $\\{X_{k}\\}_{k=i,i+T,i+2T,\\dots}$ so that each thread receives every `T`-th value, starting at its thread index.\n\nFor each regime and parameter set, compute the following diagnostics:\n1. The average of the Pearson correlation coefficients between all distinct pairs of threads,\n   $$\n   \\bar{\\rho}\n   =\n   \\frac{2}{T(T-1)}\n   \\sum_{1 \\le i < j \\le T}\n   \\rho_{ij},\n   \\quad\n   \\rho_{ij}\n   =\n   \\frac{\\sum_{t=1}^{N} (X_{t,i} - \\bar{X}_i)(X_{t,j} - \\bar{X}_j)}{\\sqrt{\\sum_{t=1}^{N} (X_{t,i} - \\bar{X}_i)^2} \\sqrt{\\sum_{t=1}^{N} (X_{t,j} - \\bar{X}_j)^2}},\n   $$\n   where $\\bar{X}_i = \\frac{1}{N}\\sum_{t=1}^{N} X_{t,i}$ is the sample mean of thread `i`.\n2. The maximum Pearson correlation coefficient over all distinct thread pairs,\n   $$\n   \\rho_{\\max} = \\max_{1 \\le i < j \\le T} \\rho_{ij}.\n   $$\n3. The normalized endpoint standard deviation across threads,\n   $$\n   \\sigma_{\\text{end}} = \\frac{\\operatorname{std}\\left(\\{S_i(N)\\}_{i=1}^{T}\\right)}{\\sqrt{N}},\n   $$\n   where $\\operatorname{std}$ denotes the sample standard deviation across the `T` endpoint values. If `T`=1, define $\\bar{\\rho} = 0$, $\\rho_{\\max} = 0$, and use the population standard deviation for the single endpoint, which yields $\\sigma_{\\text{end}} = 0$.\n\nUse the same fixed base seed `s` = 123456789 for all regimes and parameter sets. Generate the Rademacher increments by mapping uniform integer draws to $\\{-1,+1\\}$ as $X_{t,i} = 2U_{t,i}-1$ where $U_{t,i} \\in \\{0,1\\}$. Round each reported float to $6$ decimal places.\n\nTest Suite:\n- Case $1$: Naive regime with $T=4$, $N=10000$.\n- Case $2$: Independent-streams regime with $T=4$, $N=10000$.\n- Case $3$: Leapfrogging regime with $T=4$, $N=10000$.\n- Case $4$: Naive regime with boundary condition $T=1$, $N=100$.\n- Case $5$: Independent-streams regime with edge condition $T=8$, $N=50$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case result and is itself a list of three floats in the order $[\\bar{\\rho}, \\rho_{\\max}, \\sigma_{\\text{end}}]$. For example, your output should look like $[[a_1,b_1,c_1],[a_2,b_2,c_2],\\dots]$ with all floats rounded to $6$ decimal places. No physical units are involved, and angles do not appear in this problem; all floats must be reported in decimal form.", "solution": "The problem statement is examined and found to be valid. It is scientifically grounded, well-posed, objective, and contains all necessary information to compute a unique solution. The problem addresses a fundamental topic in computational science: the correct generation of parallel streams of pseudo-random numbers to ensure statistical independence in Monte Carlo simulations. We proceed to the solution.\n\nThe core task is to quantify the statistical artifacts arising from different parallel pseudo-random number generator (PRNG) seeding strategies for simulating multiple one-dimensional random walks. We will implement three distinct generation regimes and compute a set of diagnostics to measure the inter-thread correlations.\n\nA one-dimensional random walk for a given thread $i \\in \\{1, \\dots, T\\}$ is defined by its position $S_i(t)$ after $t$ steps. The position is updated at each step by a random increment. The final position, or endpoint, after $N$ steps is given by the sum of all increments:\n$$\nS_i(N) = \\sum_{t=1}^{N} X_{t,i}\n$$\nThe increments $X_{t,i}$ are independent and identically distributed (i.i.d.) Rademacher random variables, taking values in $\\{-1, +1\\}$ with equal probability $P(X_{t,i}=+1) = P(X_{t,i}=-1) = 1/2$. The key assumption for a set of $T$ parallel, independent simulations is that the increment sequences $\\{X_{t,i}\\}_{t=1}^N$ and $\\{X_{t,j}\\}_{t=1}^N$ are statistically independent for any two distinct threads $i \\neq j$.\n\nThe moments of each step increment are $E[X_{t,i}] = (+1) \\cdot (1/2) + (-1) \\cdot (1/2) = 0$ and $Var(X_{t,i}) = E[X_{t,i}^2] - (E[X_{t,i}])^2 = ((+1)^2 \\cdot (1/2) + (-1)^2 \\cdot (1/2)) - 0^2 = 1$. Due to the independence of steps within a walk, the variance of the endpoint is $Var(S_i(N)) = \\sum_{t=1}^{N} Var(X_{t,i}) = N$. The theoretical standard deviation of the endpoint of a single walk is therefore $\\sqrt{N}$.\n\nWe will now describe the three generation regimes and the expected outcome for the diagnostic statistics.\n\n1.  **Naive Regime**: In this regime, each of the $T$ parallel threads initializes its own PRNG instance using the exact same base seed $s$. Since a PRNG is a deterministic algorithm, providing the same seed guarantees the same initial state and, consequently, the exact same sequence of pseudo-random draws. This means $X_{t,i} = X_{t,j}$ for all steps $t$ and all threads $i, j$.\n    -   **Expected Diagnostics**: The perfect identity of the step sequences implies a perfect positive linear relationship. Thus, the Pearson correlation coefficient $\\rho_{ij}$ will be exactly $1$ for all pairs $i \\neq j$. This leads to an average correlation $\\bar{\\rho} = 1$ and a maximum correlation $\\rho_{\\max} = 1$. Since all trajectories are identical, their endpoints $S_i(N)$ will all be equal. The standard deviation of a set of identical numbers is $0$, so the normalized endpoint standard deviation will be $\\sigma_{\\text{end}} = 0$. This regime exemplifies a catastrophic failure of statistical independence.\n\n2.  **Independent-Streams Regime**: This approach uses a modern PRNG framework designed for parallel applications. A single base seed $s$ is used to initialize a `SeedSequence`, which then \"spawns\" $T$ child `SeedSequence` objects. Each of these child objects is used to seed one of the $T$ PRNG instances. This mechanism is engineered to produce streams of random numbers that are statistically independent.\n    -   **Expected Diagnostics**: As the generated step sequences are designed to be independent, the sample Pearson correlations $\\rho_{ij}$ should be small values fluctuating around $0$. For large $N$, the standard deviation of the sample correlation between two independent random sequences is on the order of $1/\\sqrt{N}$. We therefore expect $\\bar{\\rho} \\approx 0$ and a small $\\rho_{\\max}$. The endpoints $\\{S_i(N)\\}_{i=1}^T$ constitute a sample of $T$ i.i.d. random variables, each with a theoretical standard deviation of $\\sqrt{N}$. The normalized sample standard deviation $\\sigma_{\\text{end}}$ is therefore expected to be close to $1$.\n\n3.  **Leapfrogging Regime**: Here, a single PRNG instance is initialized with the base seed $s$. It generates one long sequence of $T \\cdot N$ increments. This master sequence is then partitioned among the $T$ threads. Thread $i$ receives the subsequence of values at indices $i, i+T, i+2T, \\dots$ (using $1$-based indexing as in the problem description). This is also known as a striding technique.\n    -   **Expected Diagnostics**: For high-quality, long-period PRNGs (such as the PCG64 generator used by default in `numpy.random.default_rng`), the subsequences produced by striding are also expected to be statistically independent. Therefore, similar to the independent-streams regime, we anticipate that $\\bar{\\rho} \\approx 0$, $\\rho_{\\max}$ will be small, and $\\sigma_{\\text{end}} \\approx 1$.\n\nThe algorithm for the solution is as follows:\n- For each test case, specified by a regime, the number of threads $T$, and the number of steps $N$:\n    1.  First, handle the special case where $T=1$. As per the problem definition, the diagnostics are $[\\bar{\\rho}, \\rho_{\\max}, \\sigma_{\\text{end}}] = [0.0, 0.0, 0.0]$.\n    2.  If $T>1$, generate the $T \\times N$ matrix of Rademacher increments, $\\{X_{t,i}\\}$, according to the specified regime. The base seed $s = 123456789$ is used consistently. The Rademacher variates $X_{t,i} \\in \\{-1, +1\\}$ are produced from uniform integers $U_{t,i} \\in \\{0, 1\\}$ via the transformation $X_{t,i} = 2U_{t,i} - 1$.\n    3.  Compute the diagnostic metrics:\n        a.  Calculate the $T \\times T$ Pearson correlation matrix from the $T \\times N$ step matrix. Extract the correlation coefficients $\\rho_{ij}$ for all distinct pairs $i < j$. Compute their average $\\bar{\\rho}$ and maximum $\\rho_{\\max}$.\n        b.  Calculate the endpoint $S_i(N)$ for each of the $T$ threads by summing the increments along each row of the step matrix.\n        c.  Compute the sample standard deviation of the $T$ endpoints (using a denominator of $T-1$, i.e., `ddof=1` in `NumPy`).\n        d.  Normalize this standard deviation by dividing by $\\sqrt{N}$ to obtain $\\sigma_{\\text{end}}$.\n- The final results for each case are collected, rounded to $6$ decimal places, and formatted into the required output string.", "answer": "[[1.000000,1.000000,0.000000],[-0.001618,0.012282,0.732890],[-0.000632,0.005548,1.205934],[0.000000,0.000000,0.000000],[-0.038161,0.222960,0.963471]]", "id": "3183815"}]}