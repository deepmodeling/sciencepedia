{"hands_on_practices": [{"introduction": "A truly uniform sequence should behave like pure noise, with no hidden periodic patterns. This practice guides you to build a powerful statistical test from first principles to detect such patterns by measuring the sequence's correlation with simple sine waves [@problem_id:3201350]. You will derive the test statistic from fundamental probability theory, apply the Central Limit Theorem, and learn to manage error rates when performing multiple simultaneous tests.", "problem": "Consider a sequence of independent and identically distributed (i.i.d.) random variables $U_1,U_2,\\dots,U_n$ intended to be samples from the continuous uniform distribution on the unit interval $[0,1]$. For any fixed positive integer $k$, define the Monte Carlo estimator of the integral $$\\int_0^1 \\sin(2\\pi k x)\\,dx$$ by the empirical average $$\\widehat{\\mu}_k = \\frac{1}{n}\\sum_{i=1}^n \\sin(2\\pi k U_i),$$ where the sine function is evaluated in radians. Under perfect uniformity, the integral equals $0$, so a well-designed estimator should concentrate near $0$ as $n$ grows large. Your task is to design and implement a uniformity test that uses the deviations of $\\widehat{\\mu}_k$ from $0$ to flag non-uniformity.\n\nStarting from fundamental definitions in probability and statistics and without using any pre-given shortcut formulas, derive a decision rule that:\n- Models the sampling distribution of $\\widehat{\\mu}_k$ under the null hypothesis that $U_i$ are i.i.d. uniform on $[0,1]$.\n- Computes, from first principles, the variance of $\\sin(2\\pi k U)$ for integer $k$ under the uniform distribution on $[0,1]$.\n- Uses the Central Limit Theorem (CLT) to approximate the distribution of $\\widehat{\\mu}_k$ and obtain a threshold for declaring a statistically significant deviation from $0$.\n- Controls the family-wise error rate at a pre-specified significance level $\\alpha$ when testing multiple frequencies $k$ simultaneously.\n- Flags non-uniformity if any tested frequency $k$ exhibits a statistically significant deviation according to your derived threshold.\n\nImplement a complete, runnable program that performs this test on the following test suite. In all cases, angles must be interpreted in radians, and the significance level $\\alpha$ is expressed as a decimal (not with a percentage sign). Each case specifies the sample size $n$, the list of frequencies $\\{k_j\\}$ to be tested, the significance level $\\alpha$, a random seed for reproducibility, and a method for generating the sequence $\\{U_i\\}$:\n\n- Case $1$ (happy path, i.i.d. uniform): $n=5000$, $\\{k_j\\} = \\{1,2,3,4,5\\}$, $\\alpha=0.01$, seed $42$, generator: i.i.d. uniform on $[0,1]$.\n- Case $2$ (non-uniform with a sine tilt): $n=4000$, $\\{k_j\\} = \\{1,2,3,4\\}$, $\\alpha=0.01$, seed $123$, generator: acceptance-rejection sampling from a density proportional to $1+\\varepsilon\\sin(2\\pi x)$ on $[0,1]$ with $\\varepsilon=0.3$. Use a uniform envelope and accept a proposal $x$ with probability $\\frac{1+\\varepsilon\\sin(2\\pi x)}{1+\\varepsilon}$; repeat until $n$ samples are accepted.\n- Case $3$ (boundary condition, small sample, i.i.d. uniform): $n=200$, $\\{k_j\\} = \\{1\\}$, $\\alpha=0.01$, seed $987$, generator: i.i.d. uniform on $[0,1]$.\n- Case $4$ (non-uniform mixture with a point mass): $n=3000$, $\\{k_j\\} = \\{1,2,3\\}$, $\\alpha=0.01$, seed $7$, generator: for each index $i$, draw $B_i$ from a Bernoulli distribution with success probability $p=0.2$ and set $U_i = 0.9$ if $B_i=1$; otherwise draw $U_i$ i.i.d. uniform on $[0,1]$.\n\nYour program must, for each case, output a boolean indicating whether the sequence passes the uniformity test ($\\text{True}$ for \"no significant deviation detected\" and $\\text{False}$ for \"flagged as non-uniform\"). The final output must be a single line containing the four booleans in order for the four cases, formatted as a comma-separated list enclosed in square brackets (e.g., \"[True,False,True,False]\").", "solution": "The problem requires the formulation and implementation of a statistical test to assess the uniformity of a sequence of random numbers $U_1, U_2, \\dots, U_n$ on the interval $[0,1]$. The test is based on the Monte Carlo estimators of the integrals of sinusoidal functions, $\\widehat{\\mu}_k = \\frac{1}{n}\\sum_{i=1}^n \\sin(2\\pi k U_i)$, for a set of positive integer frequencies $\\{k_j\\}$.\n\nThe null hypothesis, $H_0$, is that the samples $U_i$ are independent and identically distributed (i.i.d.) from the continuous uniform distribution on $[0,1]$, denoted $U_i \\sim \\text{i.i.d. Uniform}[0,1]$. The test must flag the sequence as non-uniform if, for any tested frequency $k_j$, the deviation of $\\widehat{\\mu}_{k_j}$ from its expected value under $H_0$ is statistically significant. The derivation must proceed from first principles.\n\n**Step 1: Mean and Variance of the Test Statistic under the Null Hypothesis**\n\nLet $X_i^{(k)} = \\sin(2\\pi k U_i)$ for a fixed positive integer $k$. The estimator is the sample mean $\\widehat{\\mu}_k = \\frac{1}{n} \\sum_{i=1}^n X_i^{(k)}$.\nUnder $H_0$, the probability density function of $U_i$ is $f_U(u) = 1$ for $u \\in [0,1]$ and $0$ otherwise.\n\nFirst, we compute the expected value of $X_i^{(k)}$:\n$$\n\\mu_X = E[X_i^{(k)}] = E[\\sin(2\\pi k U_i)] = \\int_{-\\infty}^{\\infty} \\sin(2\\pi k u) f_U(u) \\,du = \\int_0^1 \\sin(2\\pi k u) \\cdot 1 \\,du\n$$\nEvaluating the integral:\n$$\n\\mu_X = \\left[ -\\frac{\\cos(2\\pi k u)}{2\\pi k} \\right]_0^1 = -\\frac{\\cos(2\\pi k)}{2\\pi k} - \\left(-\\frac{\\cos(0)}{2\\pi k}\\right)\n$$\nSince $k$ is a positive integer, $\\cos(2\\pi k) = 1$ and $\\cos(0) = 1$.\n$$\n\\mu_X = -\\frac{1}{2\\pi k} + \\frac{1}{2\\pi k} = 0\n$$\nThe expected value of the estimator $\\widehat{\\mu}_k$ is therefore $E[\\widehat{\\mu}_k] = E\\left[\\frac{1}{n}\\sum_{i=1}^n X_i^{(k)}\\right] = \\frac{1}{n}\\sum_{i=1}^n E[X_i^{(k)}] = 0$.\n\nNext, we compute the variance of $X_i^{(k)}$ from first principles:\n$$\n\\sigma_X^2 = \\text{Var}(X_i^{(k)}) = E\\left[(X_i^{(k)} - \\mu_X)^2\\right] = E\\left[(X_i^{(k)})^2\\right] - (\\mu_X)^2 = E[\\sin^2(2\\pi k U_i)] - 0^2\n$$\nWe compute the expectation by integrating:\n$$\nE[\\sin^2(2\\pi k U_i)] = \\int_0^1 \\sin^2(2\\pi k u) \\,du\n$$\nUsing the trigonometric identity $\\sin^2(\\theta) = \\frac{1 - \\cos(2\\theta)}{2}$:\n$$\n\\int_0^1 \\frac{1 - \\cos(2 \\cdot 2\\pi k u)}{2} \\,du = \\frac{1}{2} \\int_0^1 (1 - \\cos(4\\pi k u)) \\,du\n$$\n$$\n= \\frac{1}{2} \\left[ u - \\frac{\\sin(4\\pi k u)}{4\\pi k} \\right]_0^1 = \\frac{1}{2} \\left( \\left(1 - \\frac{\\sin(4\\pi k)}{4\\pi k}\\right) - \\left(0 - \\frac{\\sin(0)}{4\\pi k}\\right) \\right)\n$$\nSince $k$ is an integer, $\\sin(4\\pi k) = 0$ and $\\sin(0) = 0$.\n$$\n\\sigma_X^2 = E[\\sin^2(2\\pi k U_i)] = \\frac{1}{2}(1 - 0) = \\frac{1}{2}\n$$\nThe variance of the estimator $\\widehat{\\mu}_k$ is then calculated. Because the $U_i$ are i.i.d., the $X_i^{(k)}$ are also i.i.d.\n$$\n\\text{Var}(\\widehat{\\mu}_k) = \\text{Var}\\left(\\frac{1}{n}\\sum_{i=1}^n X_i^{(k)}\\right) = \\frac{1}{n^2} \\sum_{i=1}^n \\text{Var}(X_i^{(k)}) = \\frac{1}{n^2} (n \\cdot \\sigma_X^2) = \\frac{\\sigma_X^2}{n} = \\frac{1}{2n}\n$$\n\n**Step 2: Distribution of the Test Statistic via the Central Limit Theorem (CLT)**\n\nThe Central Limit Theorem states that for a sufficiently large sample size $n$, the distribution of the sample mean $\\widehat{\\mu}_k$ of i.i.d. random variables is approximately normal.\n$$\n\\widehat{\\mu}_k \\approx \\mathcal{N}\\left(E[\\widehat{\\mu}_k], \\text{Var}(\\widehat{\\mu}_k)\\right) \\implies \\widehat{\\mu}_k \\approx \\mathcal{N}\\left(0, \\frac{1}{2n}\\right)\n$$\nTo construct a hypothesis test, we define a standardized test statistic $Z_k$ which, under $H_0$, follows a standard normal distribution, $\\mathcal{N}(0,1)$.\n$$\nZ_k = \\frac{\\widehat{\\mu}_k - E[\\widehat{\\mu}_k]}{\\sqrt{\\text{Var}(\\widehat{\\mu}_k)}} = \\frac{\\widehat{\\mu}_k - 0}{\\sqrt{1/(2n)}} = \\widehat{\\mu}_k \\sqrt{2n}\n$$\nThus, under $H_0$, $Z_k \\approx \\mathcal{N}(0,1)$.\n\n**Step 3: Handling Multiple Comparisons and Formulating the Decision Rule**\n\nThe problem requires testing a set of $m$ frequencies, denoted $\\{k_1, k_2, \\dots, k_m\\}$. This involves performing $m$ simultaneous hypothesis tests. If we test each hypothesis at a significance level $\\alpha$, the probability of making at least one Type I error (a false positive), known as the family-wise error rate (FWER), would be significantly higher than $\\alpha$.\n\nTo control the FWER at a specified level $\\alpha$, we use the Bonferroni correction. This method adjusts the significance level for each individual test to $\\alpha' = \\alpha/m$. By the union bound, this ensures that the FWER is less than or equal to $\\alpha$.\n\nFor a single test at significance $\\alpha'$, the decision rule is to reject $H_0$ if the observed test statistic is in the rejection region. For a two-tailed test on a standard normal variable $Z$, the rejection region is $|Z| > z_{\\text{crit}}$, where $z_{\\text{crit}}$ is the critical value such that $P(|Z| > z_{\\text{crit}}) = \\alpha'$. This critical value is given by $z_{\\text{crit}} = \\Phi^{-1}(1 - \\alpha'/2)$, where $\\Phi^{-1}$ is the quantile function (inverse CDF) of the standard normal distribution.\n\nCombining these elements, the final decision rule for the family of tests is as follows:\n1.  Let the set of frequencies be $\\{k_1, k_2, \\dots, k_m\\}$. The number of tests is $m$.\n2.  The overall significance level is $\\alpha$.\n3.  The Bonferroni-corrected significance level for each test is $\\alpha' = \\alpha / m$.\n4.  The critical value for all tests is $z_{\\text{crit}} = \\Phi^{-1}(1 - \\frac{\\alpha'}{2}) = \\Phi^{-1}(1 - \\frac{\\alpha}{2m})$.\n5.  For each frequency $k_j$ in the set:\n    a. Generate the sequence of $n$ samples $U_1, \\dots, U_n$.\n    b. Calculate the estimator $\\widehat{\\mu}_{k_j} = \\frac{1}{n}\\sum_{i=1}^n \\sin(2\\pi k_j U_i)$.\n    c. Calculate the test statistic $Z_{k_j} = \\widehat{\\mu}_{k_j} \\sqrt{2n}$.\n6.  The overall null hypothesis $H_0$ (that the sequence is uniform) is rejected if there exists *any* $j \\in \\{1, \\dots, m\\}$ such that $|Z_{k_j}| > z_{\\text{crit}}$. If this condition is met, we flag the sequence as non-uniform (`False`).\n7.  If for all $j \\in \\{1, \\dots, m\\}$, we have $|Z_{k_j}| \\le z_{\\text{crit}}$, we fail to reject $H_0$ and conclude that the sequence passes the uniformity test (`True`).\n\nThis comprehensive procedure forms the basis for the implementation.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Performs uniformity tests on four specified cases and prints the results.\n    \"\"\"\n    test_cases = [\n        {'n': 5000, 'k_list': [1, 2, 3, 4, 5], 'alpha': 0.01, 'seed': 42, 'generator': 'uniform'},\n        {'n': 4000, 'k_list': [1, 2, 3, 4], 'alpha': 0.01, 'seed': 123, 'generator': 'sine_tilt', 'params': {'epsilon': 0.3}},\n        {'n': 200, 'k_list': [1], 'alpha': 0.01, 'seed': 987, 'generator': 'uniform'},\n        {'n': 3000, 'k_list': [1, 2, 3], 'alpha': 0.01, 'seed': 7, 'generator': 'point_mass_mixture', 'params': {'p': 0.2, 'loc': 0.9}},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        n = case['n']\n        k_list = case['k_list']\n        alpha = case['alpha']\n        seed = case['seed']\n        generator = case['generator']\n        params = case.get('params', {})\n\n        np.random.seed(seed)\n\n        # Step 1: Generate the sequence of random numbers U\n        samples = np.array([])\n        if generator == 'uniform':\n            samples = np.random.uniform(0, 1, n)\n        \n        elif generator == 'sine_tilt':\n            # Acceptance-rejection sampling from a density proportional to 1 + ε*sin(2*pi*x)\n            epsilon = params['epsilon']\n            # The normalizing constant for the envelope is M = 1 + ε\n            M = 1.0 + epsilon\n            generated_samples = []\n            while len(generated_samples) < n:\n                # Propose from a uniform distribution\n                x_proposal = np.random.uniform(0, 1)\n                # Generate a uniform random number for the acceptance check\n                u_check = np.random.uniform(0, 1)\n                # Calculate the acceptance probability\n                prob_accept = (1.0 + epsilon * np.sin(2 * np.pi * x_proposal)) / M\n                if u_check < prob_accept:\n                    generated_samples.append(x_proposal)\n            samples = np.array(generated_samples)\n\n        elif generator == 'point_mass_mixture':\n            # Mixture model: with probability p, U_i = loc; otherwise, U_i is uniform\n            p = params['p']\n            loc = params['loc']\n            # Generate Bernoulli trials to decide between the two components\n            bernoulli_draws = np.random.binomial(1, p, n)\n            # Generate uniform samples for the continuous part\n            uniform_draws = np.random.uniform(0, 1, n)\n            # Combine them: if B=1, use loc; if B=0, use uniform_draw\n            samples = (1 - bernoulli_draws) * uniform_draws + bernoulli_draws * loc\n            \n        # Step 2: Set up the hypothesis test parameters\n        m = len(k_list)  # Number of simultaneous tests\n        alpha_prime = alpha / m  # Bonferroni corrected significance level\n        \n        # Calculate the two-tailed critical value from the standard normal distribution\n        z_crit = norm.ppf(1 - alpha_prime / 2)\n\n        # Step 3: Perform the test for each frequency k\n        is_uniform = True  # Assume uniform until a deviation is found\n        for k in k_list:\n            # Calculate the Monte Carlo estimator\n            mu_hat_k = np.mean(np.sin(2 * np.pi * k * samples))\n            \n            # Calculate the test statistic Z_k\n            z_k = mu_hat_k * np.sqrt(2 * n)\n            \n            # Check if the statistic exceeds the critical value\n            if np.abs(z_k) > z_crit:\n                is_uniform = False\n                break  # A single failure is enough to reject uniformity\n\n        results.append(is_uniform)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3201350"}, {"introduction": "This practice explores the fascinating mathematical concept of equidistribution, which predicts that sequences of the form $\\{n\\alpha\\}$ should be uniformly distributed when $\\alpha$ is irrational. You will apply standard, powerful goodness-of-fit tools—the Kolmogorov-Smirnov and Pearson Chi-Square tests—to verify this theoretical property and see how it breaks down for rational $\\alpha$ [@problem_id:3201353]. This exercise provides a hands-on feel for the practical application, strengths, and potential pitfalls of these common statistical tests.", "problem": "Consider the sequence $x_n = \\{n\\alpha\\}$ where $\\{y\\}$ denotes the fractional part of $y$, that is, the unique value in $[0,1)$ satisfying $y = \\lfloor y \\rfloor + \\{y\\}$. You will investigate the uniformity of the points $\\{x_n\\}_{n=1}^N$ on the interval $[0,1)$ by formalizing two independent tests based on fundamental definitions of uniform distribution and empirical approximation.\n\nFundamental base:\n- A sequence $\\{x_n\\}$ in $[0,1)$ is said to be uniformly distributed if, for every subinterval $[a,b) \\subseteq [0,1)$, the fraction of indices $n \\in \\{1,2,\\dots,N\\}$ with $x_n \\in [a,b)$ approaches $b-a$ as $N \\to \\infty$.\n- The Empirical Cumulative Distribution Function (ECDF) at a point $t \\in [0,1)$ is the fraction of the sample less than or equal to $t$.\n- The Pearson Chi-Square test compares observed bin counts to the counts predicted by a uniform distribution across equal-width bins.\n\nTasks:\n1. For given parameters $(\\alpha, N, B, \\gamma)$, construct the finite sequence $\\{x_n\\}_{n=1}^N$ with $x_n = \\{n\\alpha\\}$ using standard floating-point arithmetic and reduction modulo $1$.\n2. Implement two formal tests of uniformity on $[0,1)$:\n   - Kolmogorov–Smirnov (KS) test: quantify the deviation between the ECDF of the sample and the identity function on $[0,1)$, and obtain a $p$-value under the null hypothesis of a continuous uniform distribution on $[0,1)$.\n   - Pearson Chi-Square test on a histogram with $B$ equal-width bins on $[0,1)$: compute observed frequencies and compare them to the expected frequency $N/B$ per bin, and obtain a $p$-value under the null hypothesis of a uniform distribution.\n3. Declare a case \"uniform\" if and only if both tests individually fail to reject at significance level $\\gamma$; that is, both $p$-values are greater than or equal to $\\gamma$. Otherwise, declare the case \"non-uniform\".\n4. Apply this procedure to the following test suite of parameter values $(\\alpha,N,B,\\gamma)$:\n   - Case $1$: $\\alpha = \\sqrt{2}$, $N = 10000$, $B = 20$, $\\gamma = 0.01$.\n   - Case $2$: $\\alpha = \\frac{1+\\sqrt{5}}{2}$, $N = 12000$, $B = 50$, $\\gamma = 0.01$.\n   - Case $3$: $\\alpha = \\frac{1}{2}$, $N = 2000$, $B = 20$, $\\gamma = 0.01$.\n   - Case $4$: $\\alpha = \\frac{1}{4}$, $N = 2000$, $B = 20$, $\\gamma = 0.01$.\n   - Case $5$: $\\alpha = \\frac{3}{8}$, $N = 16000$, $B = 8$, $\\gamma = 0.01$.\n   - Case $6$: $\\alpha = \\frac{3}{8}$, $N = 16000$, $B = 16$, $\\gamma = 0.01$.\n\nDesign for coverage:\n- Cases $1$ and $2$ probe irrational $\\alpha$, where theoretical equidistribution suggests acceptance by both tests for sufficiently large $N$.\n- Cases $3$ and $4$ probe rational $\\alpha$ with small denominators, where periodicity yields strong non-uniformity detectable by both tests.\n- Case $5$ probes a rational $\\alpha$ whose support aligns exactly with $B$ equal-width bins, potentially causing the histogram-based test to return observed counts equal to expected counts, while the KS test still detects the stepwise deviation from a continuous uniform distribution.\n- Case $6$ probes the same rational $\\alpha$ with a bin count that does not align with the period, making histogram-based non-uniformity apparent.\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry corresponds to one case and is a boolean indicating \"uniform\" ($\\text{True}$) or \"non-uniform\" ($\\text{False}$) according to Task $3$. For example, the output should look like $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5,\\text{result}_6]$.", "solution": "The task is to assess the uniformity of six finite sequences generated by the rule $x_n = \\{n\\alpha\\}$ for $n = 1, \\dots, N$. This assessment is performed using two independent statistical tests, and a final verdict is reached based on a clear decision rule.\n\n**1. Sequence Generation**\nFor each parameter set $(\\alpha, N, B, \\gamma)$, we generate the sequence $\\{x_n\\}_{n=1}^N$. The term $x_n$ is the fractional part of $n\\alpha$. This is equivalent to calculating $n\\alpha \\pmod 1$. Computationally, for an array of indices $n = 1, 2, \\ldots, N$, we can generate the entire sequence vectorially.\n\n**2. Uniformity Tests**\nThe null hypothesis, $H_0$, for both tests is that the sample $\\{x_n\\}$ is drawn from a continuous uniform distribution on the interval $[0,1)$.\n\n- **Kolmogorov–Smirnov (KS) Test**: This test compares the Empirical Cumulative Distribution Function (ECDF) of the data, $F_N(t)$, with the cumulative distribution function (CDF) of the hypothesized distribution, $F(t)$. For the uniform distribution on $[0,1)$, the CDF is $F(t) = t$. The KS statistic, $D_N$, is the maximum absolute difference between these two functions:\n$$D_N = \\sup_{t \\in [0,1)} |F_N(t) - F(t)|$$\nwhere $F_N(t)$ is the proportion of sample points less than or equal to $t$. A large value of $D_N$ suggests the data does not follow the hypothesized distribution. We compute the $p$-value, $p_{KS}$, which is the probability of observing a test statistic at least as extreme as $D_N$ under $H_0$.\n\n- **Pearson's Chi-Square ($\\chi^2$) Test**: This test is applied to binned data. The interval $[0,1)$ is divided into $B$ disjoint subintervals (bins) of equal width, $1/B$. For each bin $i=1, \\dots, B$, we count the number of data points, $O_i$, that fall into it. Under the null hypothesis of uniformity, the expected number of points in each bin is $E_i = N/B$. The $\\chi^2$ statistic measures the discrepancy between observed and expected frequencies:\n$$\\chi^2 = \\sum_{i=1}^{B} \\frac{(O_i - E_i)^2}{E_i}$$\nThis statistic follows a $\\chi^2$ distribution with $B-1$ degrees of freedom. We compute the corresponding $p$-value, $p_{\\chi^2}$.\n\n**3. Decision Rule**\nFor each case, we determine the outcome based on the provided significance level $\\gamma = 0.01$. The sequence is classified as \"uniform\" if and only if both tests fail to reject the null hypothesis at this level. Mathematically, the condition is:\n$$ (p_{KS} \\ge \\gamma) \\wedge (p_{\\chi^2} \\ge \\gamma) $$\nIf this condition holds, the result is $\\text{True}$; otherwise, it is $\\text{False}$.\n\n**4. Analysis of Test Cases**\n\n- **Case 1 & 2**: $\\alpha=\\sqrt{2}$ and $\\alpha=\\frac{1+\\sqrt{5}}{2}$ (the golden ratio). Both are irrational numbers. According to Weyl's criterion, the infinite sequence $\\{n\\alpha\\}$ is uniformly distributed on $[0,1)$. For large $N$ ($10000$ and $12000$), the finite sequences should approximate a uniform distribution well. Therefore, we expect both $p_{KS}$ and $p_{\\chi^2}$ to be greater than $\\gamma=0.01$, leading to a \"uniform\" ($\\text{True}$) classification.\n\n- **Case 3 & 4**: $\\alpha=1/2$ and $\\alpha=1/4$. Both are rational numbers with small denominators. The sequence $\\{n\\alpha\\}$ is periodic and visits only a small, finite set of values. For $\\alpha=1/2$, the sequence is $\\{0.5, 0, 0.5, 0, \\dots\\}$. For $\\alpha=1/4$, the sequence is $\\{0.25, 0.5, 0.75, 0, \\dots\\}$. These sequences are patently non-uniform. Both the KS test (detecting the large gaps in the ECDF) and the $\\chi^2$ test (detecting highly uneven bin counts) should strongly reject $H_0$, yielding $p$-values close to $0$. The classification for both cases should be \"non-uniform\" ($\\text{False}$).\n\n- **Case 5**: $\\alpha=3/8$ and $B=8$. Here, $\\alpha$ is rational with denominator $8$. The sequence consists of $8$ distinct values: $\\{0, 1/8, 2/8, \\dots, 7/8\\}$, each appearing $N/8 = 16000/8 = 2000$ times. The number of bins, $B=8$, aligns perfectly with the denominator. The bins are $[0, 1/8), [1/8, 2/8), \\dots$. The points in the sequence are $\\{0, 3/8, 6/8, 1/8, 4/8, 7/8, 2/8, 5/8\\}$. Each of the $8$ values $\\{k/8 \\mid k=0,\\dots,7\\}$ falls into a unique bin. As each value appears $2000$ times, the observed count for each bin is $O_i = 2000$. The expected count is $E_i = N/B = 16000/8 = 2000$. Thus, $O_i = E_i$ for all $i$, yielding a $\\chi^2$ statistic of $0$ and a $p_{\\chi^2}$ of $1$. The $\\chi^2$ test fails to detect the non-uniformity. However, the ECDF is a step function with $8$ large steps, a significant deviation from the linear CDF of a uniform distribution. The KS test should capture this, yielding a very small $p_{KS}$. Since $p_{KS} < 0.01$, the combined result is \"non-uniform\" ($\\text{False}$).\n\n- **Case 6**: $\\alpha=3/8$ and $B=16$. The sequence is the same as in Case 5, containing only values of the form $k/8$. When binned into $16$ bins of width $1/16$, the points $\\{0, 2/16, 4/16, \\dots, 14/16\\}$ will only occupy bins with even indices. All bins with odd indices will be empty. The observed counts will be highly non-uniform, leading to a large $\\chi^2$ statistic and a $p_{\\chi^2}$ near $0$. The KS test will also detect the discrete nature of the data. Thus, both tests should reject $H_0$, resulting in a \"non-uniform\" ($\\text{False}$) classification.\n\nThe implementation will proceed by calculating these values for each case and applying the decision rule.", "answer": "```python\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Performs uniformity testing on sequences x_n = {n*alpha} for six test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1: alpha is irrational (sqrt(2))\n        (np.sqrt(2), 10000, 20, 0.01),\n        # Case 2: alpha is irrational (golden ratio)\n        ((1 + np.sqrt(5)) / 2, 12000, 50, 0.01),\n        # Case 3: alpha is rational (1/2), small period\n        (1/2, 2000, 20, 0.01),\n        # Case 4: alpha is rational (1/4), small period\n        (1/4, 2000, 20, 0.01),\n        # Case 5: alpha is rational (3/8), B matches period\n        (3/8, 16000, 8, 0.01),\n        # Case 6: alpha is rational (3/8), B does not match period\n        (3/8, 16000, 16, 0.01),\n    ]\n\n    results = []\n    for alpha, N, B, gamma in test_cases:\n        # Task 1: Construct the finite sequence\n        n_values = np.arange(1, N + 1)\n        x_sequence = (n_values * alpha) % 1\n\n        # Task 2: Implement two formal tests of uniformity\n        \n        # Kolmogorov-Smirnov test\n        # H0: The sample is drawn from a continuous uniform distribution on [0, 1)\n        # The CDF for 'uniform' in scipy.stats.kstest is the identity function on [0,1].\n        ks_statistic, p_value_ks = stats.kstest(x_sequence, 'uniform')\n\n        # Pearson Chi-Square test\n        # H0: The sample is drawn from a uniform distribution.\n        # 1. Create a histogram with B equal-width bins on [0, 1).\n        observed_frequencies, _ = np.histogram(x_sequence, bins=B, range=(0, 1))\n        \n        # 2. Compute the p-value.\n        # stats.chisquare by default assumes expected frequencies are uniform,\n        # which is exactly sum(observed_frequencies) / len(observed_frequencies) = N/B.\n        chi2_statistic, p_value_chi2 = stats.chisquare(f_obs=observed_frequencies)\n\n        # Task 3: Declare \"uniform\" or \"non-uniform\"\n        # \"uniform\" if and only if both tests fail to reject H0 at level gamma.\n        is_uniform = (p_value_ks >= gamma) and (p_value_chi2 >= gamma)\n        results.append(is_uniform)\n\n    # Final output specification\n    # A single line with a comma-separated list of booleans enclosed in square brackets.\n    # Python's str() on a boolean produces \"True\" or \"False\", which is what is needed.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3201353"}, {"introduction": "Moving beyond fixed-batch analysis, this practice challenges you to design a sequential monitor that flags deviations from uniformity as they emerge in a data stream. You will derive principled, time-varying stopping boundaries for a random walk using Hoeffding's inequality and the union bound to control the overall false alarm rate [@problem_id:3201429]. This exercise introduces the powerful paradigm of sequential analysis, which is crucial for real-time quality control and anomaly detection.", "problem": "Consider a sequence of independent Uniform random numbers on the unit interval, denoted by $U_1, U_2, \\dots, U_N$, where each $U_i$ is independent and identically distributed (i.i.d.) as Uniform$(0,1)$. The fundamental base is that $U_i$ has expectation $\\mathbb{E}[U_i] = \\tfrac{1}{2}$ and is bounded in the interval $[0,1]$. Define the centered partial sums $S_k = \\sum_{i=1}^{k} \\left(U_i - \\tfrac{1}{2}\\right)$, which form a zero-mean bounded random walk under the null hypothesis of uniformity.\n\nYour task is to design a sequential uniformity monitor that triggers a flag the first time $k$ such that the cumulative sum $S_k$ leaves a prescribed acceptance region bounded by symmetric time-varying thresholds $\\pm h_k$. The acceptance boundaries $\\{h_k\\}_{k=1}^N$ must be derived from well-tested concentration principles for bounded independent variables using Hoeffding's inequality and a union bound argument so that, under the null hypothesis that $U_i$ are i.i.d. Uniform$(0,1)$, the probability that any boundary is crossed at some time $k \\in \\{1,2,\\dots,N\\}$ is at most a preassigned level $\\alpha \\in (0,1)$.\n\nThe monitor must operate as follows: for each time $k = 1, 2, \\dots, N$, compare $S_k$ to the boundaries $\\pm h_k$ and stop immediately when $S_k \\ge h_k$ or $S_k \\le -h_k$. The monitor outputs the signed index of the first crossing time: if the upper boundary is crossed first at time $k^\\star$, output the integer $k^\\star$; if the lower boundary is crossed first at time $k^\\star$, output the integer $-k^\\star$; if no crossing occurs for all $k \\le N$, output the integer $0$.\n\nImplement this monitor for the following test suite. Each test case specifies the sequence length $N$, the false alarm level $\\alpha$, and how to generate the sequence $U_1, \\dots, U_N$. Random sequences must be generated deterministically using the specified seed to ensure reproducibility.\n\nTest suite:\n1. Case A (happy path): $N = 500$, $\\alpha = 0.01$, $U_i$ generated as i.i.d. Uniform$(0,1)$ with pseudorandom generator seed $123$.\n2. Case B (positive bias): $N = 200$, $\\alpha = 0.01$, $U_i$ generated as $U_i = \\min\\{1, V_i + 0.2\\}$ where $V_i$ are i.i.d. Uniform$(0,1)$ with seed $456$.\n3. Case C (negative bias): $N = 200$, $\\alpha = 0.01$, $U_i$ generated as $U_i = \\max\\{0, V_i - 0.2\\}$ where $V_i$ are i.i.d. Uniform$(0,1)$ with seed $789$.\n4. Case D (boundary condition, short sequence): $N = 1$, $\\alpha = 0.01$, $U_1 = 0.9$ deterministically.\n5. Case E (edge case, perfectly centered): $N = 300$, $\\alpha = 0.01$, $U_i = \\tfrac{1}{2}$ deterministically for all $i$.\n\nYour program must compute the signed first crossing index for each case, as defined above, using boundaries derived from the concentration inequality approach with overall false alarm level $\\alpha$ across the horizon $N$ under the null hypothesis. The final output format must be a single line containing the results for the five cases as a comma-separated list enclosed in square brackets, with no spaces, in the order of the cases listed above. For example, if the results were integers $r_1, r_2, r_3, r_4, r_5$, the output must be printed exactly as $[r_1,r_2,r_3,r_4,r_5]$.", "solution": "The problem statement has been critically examined and is determined to be valid. It is scientifically grounded in probability theory and statistics, well-posed with a clear objective and sufficient information, and formulated with objective, unambiguous language. The task is to implement a sequential uniformity test for random numbers based on a random walk of centered partial sums, with stopping boundaries derived from Hoeffding's inequality and a union bound argument to control the overall false alarm probability.\n\nThe core of the problem is to construct time-varying boundaries $\\pm h_k$ for a sequence of cumulative sums $S_k = \\sum_{i=1}^{k} \\left(U_i - \\tfrac{1}{2}\\right)$, where under the null hypothesis, $\\{U_i\\}_{i=1}^N$ are independent and identically distributed (i.i.d.) as $\\text{Uniform}(0,1)$. The boundaries must ensure that the probability of the random walk crossing them at any point up to time $N$ is no more than a specified level $\\alpha$.\n$$ P\\left(\\bigcup_{k=1}^{N} \\{|S_k| \\ge h_k\\}\\right) \\le \\alpha $$\n\nWe begin by defining the random variables $X_i = U_i - \\tfrac{1}{2}$. Under the null hypothesis, the $X_i$ are i.i.d. with expectation $\\mathbb{E}[X_i] = 0$. Since $U_i \\in [0,1]$, each $X_i$ is bounded in the interval $[-\\tfrac{1}{2}, \\tfrac{1}{2}]$. The range of each $X_i$ is $(\\tfrac{1}{2}) - (-\\tfrac{1}{2}) = 1$. The sum is $S_k = \\sum_{i=1}^k X_i$.\n\nWe apply Hoeffding's inequality, which provides a bound on the tail probability of a sum of bounded, independent random variables. For our sum $S_k$, the inequality is:\n$$ P(S_k \\ge t) \\le \\exp\\left( - \\frac{2t^2}{\\sum_{i=1}^k (b_i - a_i)^2} \\right) $$\nWith $b_i-a_i = 1$ for all $i$, the denominator becomes $\\sum_{i=1}^k 1^2 = k$. Thus,\n$$ P(S_k \\ge t) \\le \\exp\\left( - \\frac{2t^2}{k} \\right) $$\nBy symmetry of the setup ($X_i$ vs $-X_i$), the same bound applies for $P(S_k \\le -t)$. Using a union bound for a single time step $k$, we get a two-sided bound:\n$$ P(|S_k| \\ge t) = P(S_k \\ge t \\text{ or } S_k \\le -t) \\le 2 \\exp\\left( - \\frac{2t^2}{k} \\right) $$\n\nTo control the overall false alarm rate over the entire horizon $k=1, \\dots, N$, we use the union bound (also known as Boole's inequality) across all time steps. Let $A_k$ be the event $|S_k| \\ge h_k$.\n$$ P(\\text{any crossing}) = P\\left(\\bigcup_{k=1}^{N} A_k\\right) \\le \\sum_{k=1}^{N} P(A_k) = \\sum_{k=1}^{N} P(|S_k| \\ge h_k) $$\nWe can substitute our Hoeffding bound for each term in the sum:\n$$ \\sum_{k=1}^{N} P(|S_k| \\ge h_k) \\le \\sum_{k=1}^{N} 2 \\exp\\left( - \\frac{2h_k^2}{k} \\right) $$\nTo ensure the total probability is bounded by $\\alpha$, we need to select the thresholds $\\{h_k\\}$ such that this sum is less than or equal to $\\alpha$. A straightforward and common approach is to set the bound on the probability for each individual test $P(|S_k| \\ge h_k)$ to be a constant fraction of $\\alpha$, such as $\\alpha/N$. This is an application of the Bonferroni correction.\nWe set:\n$$ 2 \\exp\\left( - \\frac{2h_k^2}{k} \\right) = \\frac{\\alpha}{N} $$\nSolving this equation for $h_k$ gives us the explicit formula for the boundaries:\n$$ \\exp\\left( - \\frac{2h_k^2}{k} \\right) = \\frac{\\alpha}{2N} $$\n$$ - \\frac{2h_k^2}{k} = \\ln\\left(\\frac{\\alpha}{2N}\\right) $$\n$$ \\frac{2h_k^2}{k} = -\\ln\\left(\\frac{\\alpha}{2N}\\right) = \\ln\\left(\\frac{2N}{\\alpha}\\right) $$\n$$ h_k^2 = \\frac{k}{2} \\ln\\left(\\frac{2N}{\\alpha}\\right) $$\n$$ h_k = \\sqrt{\\frac{k}{2} \\ln\\left(\\frac{2N}{\\alpha}\\right)} $$\nWith this choice, the sum of the probability bounds is $\\sum_{k=1}^N \\frac{\\alpha}{N} = \\alpha$, satisfying the design requirement.\n\nThe monitoring procedure is then implemented as follows:\n1. For a given test case ($N$, $\\alpha$, and sequence generation rule), pre-compute the constant factor for the boundaries: $C = \\sqrt{\\tfrac{1}{2} \\ln(\\frac{2N}{\\alpha})}$. The boundary at time $k$ is then $h_k = C\\sqrt{k}$.\n2. Initialize the cumulative sum $S_0 = 0$.\n3. Iterate from $k=1$ to $N$:\n    a. Generate or retrieve the $k$-th value, $U_k$.\n    b. Update the centered sum: $S_k = S_{k-1} + (U_k - \\tfrac{1}{2})$.\n    c. Compare $S_k$ with the boundaries $\\pm h_k$.\n    d. If $S_k \\ge h_k$, stop and report the crossing time as $k$.\n    e. If $S_k \\le -h_k$, stop and report the crossing time as $-k$.\n4. If the loop completes without any boundary crossing, report $0$.\n\nThis procedure is deterministic once the sequence of $U_i$ values is fixed, and it will be applied to each of the specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_monitor(N, alpha, U_sequence):\n    \"\"\"\n    Runs the sequential uniformity monitor for a given sequence.\n\n    Args:\n        N (int): The maximum length of the sequence to monitor.\n        alpha (float): The desired overall false alarm probability.\n        U_sequence (np.ndarray): The sequence of numbers to test.\n\n    Returns:\n        int: The signed index of the first crossing time, or 0 if no crossing occurs.\n    \"\"\"\n    # Defensive check in case N=0 or alpha is invalid, although problem constraints\n    # make this unlikely. The log would be undefined for N=0.\n    if N == 0:\n        return 0\n    if not (0 < alpha < 1):\n        raise ValueError(\"alpha must be in (0,1)\")\n\n    # Pre-compute the constant part of the boundary calculation\n    # h_k = sqrt(k/2 * ln(2N/alpha)) = C * sqrt(k)\n    boundary_constant = np.sqrt(0.5 * np.log(2 * N / alpha))\n\n    # Initialize the centered cumulative sum\n    S_k = 0.0\n\n    # Iterate through the sequence, updating the sum and checking boundaries\n    for k_idx, u_val in enumerate(U_sequence):\n        k = k_idx + 1  # Time index is 1-based\n        S_k += u_val - 0.5\n        h_k = boundary_constant * np.sqrt(k)\n\n        if S_k >= h_k:\n            return k\n        if S_k <= -h_k:\n            return -k\n\n    # If the loop completes without crossing, return 0\n    return 0\n\ndef solve():\n    \"\"\"\n    Solves the problem by running the monitor on all specified test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'id': 'A', 'N': 500, 'alpha': 0.01, 'gen_type': 'uniform', 'seed': 123},\n        {'id': 'B', 'N': 200, 'alpha': 0.01, 'gen_type': 'positive_bias', 'seed': 456},\n        {'id': 'C', 'N': 200, 'alpha': 0.01, 'gen_type': 'negative_bias', 'seed': 789},\n        {'id': 'D', 'N': 1, 'alpha': 0.01, 'gen_type': 'deterministic', 'U_seq': np.array([0.9])},\n        {'id': 'E', 'N': 300, 'alpha': 0.01, 'gen_type': 'deterministic', 'U_seq': np.full(300, 0.5)}\n    ]\n\n    results = []\n    for case in test_cases:\n        N = case['N']\n        alpha = case['alpha']\n        \n        if case['gen_type'] == 'uniform':\n            rng = np.random.default_rng(seed=case['seed'])\n            U = rng.uniform(0, 1, size=N)\n        elif case['gen_type'] == 'positive_bias':\n            rng = np.random.default_rng(seed=case['seed'])\n            V = rng.uniform(0, 1, size=N)\n            U = np.minimum(1.0, V + 0.2)\n        elif case['gen_type'] == 'negative_bias':\n            rng = np.random.default_rng(seed=case['seed'])\n            V = rng.uniform(0, 1, size=N)\n            U = np.maximum(0.0, V - 0.2)\n        elif case['gen_type'] == 'deterministic':\n            U = case['U_seq']\n        \n        result = run_monitor(N, alpha, U)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3201429"}]}