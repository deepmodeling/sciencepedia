{"hands_on_practices": [{"introduction": "We begin by exploring how a fundamental discrete probability distribution can emerge from the basic principles of statistical mechanics. This exercise [@problem_id:1961995] uses the classic thought experiment of mixing two ideal gases. By counting the possible spatial arrangements of particles, you will derive the binomial distribution, revealing its deep connection to the postulate of equal a priori probabilities for microstates in an isolated system.", "problem": "A rigid, insulated container of total volume $V$ is divided by a removable partition into two sub-volumes, $V_1$ and $V_2$. The volume $V_1$ contains $N_A$ particles of an ideal gas A, while volume $V_2$ contains $N_B$ particles of a different ideal gas B. The fractional volume occupied by gas A is given by $\\alpha$, such that $V_1 = \\alpha V$ and $V_2 = (1-\\alpha)V$, where $0 < \\alpha < 1$.\n\nInitially, both gases are in thermal equilibrium. The partition is then removed, allowing the two gases to mix and the combined system to reach a new state of thermodynamic equilibrium. Based on the fundamental postulates of statistical mechanics, derive an expression for the probability, $P(n)$, of finding exactly $n$ particles of gas A within the spatial region that was originally volume $V_2$. The variable $n$ is an integer that can range from $0$ to $N_A$.", "solution": "After the partition is removed, the combined system evolves to equilibrium. By the fundamental postulate of statistical mechanics (equal a priori probabilities), the microcanonical measure is uniform over the accessible region of phase space. For an ideal gas mixture with no interparticle interactions and hard-wall confinement to the volume $V$, the Hamiltonian separates as\n$$\nH=\\sum_{i=1}^{N_{A}} \\frac{|\\mathbf{p}_{i}|^{2}}{2 m_{A}}+\\sum_{j=1}^{N_{B}} \\frac{|\\mathbf{p}'_{j}|^{2}}{2 m_{B}},\n$$\nwith all positions constrained to lie within $V$. Hence, in the microcanonical ensemble at fixed total energy, momentum and position variables factorize. For any event that constrains only the spatial positions (such as the number of gas A particles lying in a specified subvolume), the ratio of its probability to the total is given by the ratio of the corresponding configuration-space volumes; the momentum integrals are common factors and cancel.\n\nLet $V_{1}=\\alpha V$ and $V_{2}=(1-\\alpha) V$ denote the original subvolumes. At equilibrium, the marginal distribution of each gas A particle’s position is uniform over $V$. Therefore, the probability that a given A particle lies in $V_{2}$ is $V_{2}/V=1-\\alpha$, and in $V_{1}$ is $V_{1}/V=\\alpha$. Because the particles are noninteracting, the joint distribution over the $N_{A}$ positions factorizes, and one may equivalently compute probabilities by configuration-space volume counting.\n\nCompute $P(n)$ as a ratio of configuration-space measures. For definiteness, treat gas A particles as distinguishable during the counting (the indistinguishability Gibbs factors cancel in the final ratio). The total configuration-space volume for the $N_{A}$ positions is $V^{N_{A}}$. The configuration-space volume corresponding to exactly $n$ of the $N_{A}$ A particles lying in $V_{2}$ and the remaining $N_{A}-n$ in $V_{1}$ is\n$$\n\\binom{N_{A}}{n} V_{2}^{n} V_{1}^{N_{A}-n},\n$$\nwhere the binomial coefficient counts the choices of which $n$ particles lie in $V_{2}$. Therefore,\n$$\nP(n)=\\frac{\\binom{N_{A}}{n} V_{2}^{n} V_{1}^{N_{A}-n}}{V^{N_{A}}}\n=\\binom{N_{A}}{n} \\left(\\frac{V_{2}}{V}\\right)^{n} \\left(\\frac{V_{1}}{V}\\right)^{N_{A}-n}.\n$$\nSubstituting $V_{1}=\\alpha V$ and $V_{2}=(1-\\alpha) V$ gives\n$$\nP(n)=\\binom{N_{A}}{n} (1-\\alpha)^{n} \\alpha^{N_{A}-n},\n$$\nvalid for all integers $n$ with $0 \\leq n \\leq N_{A}$. The presence of gas B only contributes a multiplicative factor to the phase-space volume that cancels in the probability ratio, so $P(n)$ is independent of $N_{B}$ for an ideal mixture.", "answer": "$$\\boxed{\\binom{N_{A}}{n} (1-\\alpha)^{n} \\alpha^{N_{A}-n}}$$", "id": "1961995"}, {"introduction": "As systems become large, calculating with discrete distributions can be cumbersome, making continuous approximations powerful tools. This practice [@problem_id:1896384] investigates the conditions under which the discrete Poisson distribution, which models events like photon arrivals, can be accurately approximated by a continuous Gaussian distribution. You will determine a quantitative threshold for when this approximation is valid, a crucial skill for simplifying complex statistical models.", "problem": "An astrophysicist is analyzing images from a space telescope. The number of photons, $k$, collected by a single pixel of the telescope's Charge-Coupled Device (CCD) during a short, fixed exposure time is governed by the Poisson distribution, $P(k;\\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}$, where $\\lambda$ is the mean number of photons expected during the exposure.\n\nFor planning longer observations where $\\lambda$ is expected to be large, it is computationally convenient to approximate the discrete Poisson distribution with a continuous Gaussian (Normal) distribution with mean $\\mu = \\lambda$ and variance $\\sigma^2 = \\lambda$. A common quick check on the validity of this approximation involves comparing the probability of the most likely event. For an integer mean $\\lambda$, the most likely number of detected photons is $k=\\lambda$. The corresponding approximation from the continuous model is derived from the probability density of the Gaussian at its peak, $f(x=\\lambda)$, which is given by $f(\\lambda) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} = \\frac{1}{\\sqrt{2\\pi\\lambda}}$. This value approximates the probability of observing exactly $k=\\lambda$ photons.\n\nDetermine the smallest integer value of the mean photon count $\\lambda$ for which this Gaussian peak approximation for $P(k=\\lambda; \\lambda)$ has a relative error of less than 1.0%. The relative error is defined as $\\frac{|P_{approx} - P_{exact}|}{P_{exact}}$, where $P_{exact} = P(k=\\lambda; \\lambda)$ and $P_{approx} = \\frac{1}{\\sqrt{2\\pi\\lambda}}$.", "solution": "We need the smallest integer $\\lambda$ such that the Gaussian peak approximation $P_{\\text{approx}}=1/\\sqrt{2\\pi\\lambda}$ for $P(k=\\lambda;\\lambda)$ has relative error less than $0.01$ compared to the exact Poisson probability $P_{\\text{exact}}=\\lambda^{\\lambda}\\exp(-\\lambda)/\\lambda!$.\n\nUse Stirling’s formula with a rigorous remainder (Robbins’ bounds): for integer $n$ there exists $r_{n}$ with\n$$\nn!=\\sqrt{2\\pi n}\\left(\\frac{n}{\\exp(1)}\\right)^{n}\\exp(r_{n}),\\quad \\frac{1}{12n+1}<r_{n}<\\frac{1}{12n}.\n$$\nSet $n=\\lambda$. Then\n$$\nP_{\\text{exact}}=\\frac{\\lambda^{\\lambda}\\exp(-\\lambda)}{\\lambda!}\n=\\frac{\\lambda^{\\lambda}\\exp(-\\lambda)}{\\sqrt{2\\pi \\lambda}\\left(\\frac{\\lambda}{\\exp(1)}\\right)^{\\lambda}\\exp(r_{\\lambda})}\n=\\frac{1}{\\sqrt{2\\pi\\lambda}\\exp(r_{\\lambda})}.\n$$\nHence\n$$\n\\frac{P_{\\text{approx}}}{P_{\\text{exact}}}\n=\\frac{\\frac{1}{\\sqrt{2\\pi\\lambda}}}{\\frac{1}{\\sqrt{2\\pi\\lambda}\\exp(r_{\\lambda})}}\n=\\exp(r_{\\lambda}),\n$$\nso the relative error is\n$$\n\\frac{|P_{\\text{approx}}-P_{\\text{exact}}|}{P_{\\text{exact}}}\n=\\exp(r_{\\lambda})-1,\n$$\nwhich is strictly increasing in $r_{\\lambda}$. Using the bounds on $r_{\\lambda}$ gives\n$$\n\\exp\\!\\left(\\frac{1}{12\\lambda+1}\\right)-1\n< \\frac{|P_{\\text{approx}}-P_{\\text{exact}}|}{P_{\\text{exact}}}\n< \\exp\\!\\left(\\frac{1}{12\\lambda}\\right)-1.\n$$\nTo ensure the error is less than $0.01$ for a given $\\lambda$, it suffices that\n$$\n\\exp\\!\\left(\\frac{1}{12\\lambda}\\right)-1<0.01\n\\quad\\Longleftrightarrow\\quad\n\\frac{1}{12\\lambda}<\\ln(1.01)\n\\quad\\Longleftrightarrow\\quad\n\\lambda>\\frac{1}{12\\,\\ln(1.01)}.\n$$\nCompute $\\ln(1.01)\\approx 0.00995033$, so\n$$\n\\frac{1}{12\\,\\ln(1.01)}\\approx \\frac{1}{0.119404}\\approx 8.37.\n$$\nThus $\\lambda=9$ satisfies the sufficient condition. Indeed,\n$$\n\\exp\\!\\left(\\frac{1}{12\\cdot 9}\\right)-1=\\exp\\!\\left(\\frac{1}{108}\\right)-1\\approx 0.00930<0.01.\n$$\nConversely, for $\\lambda=8$, the lower bound on the error is\n$$\n\\exp\\!\\left(\\frac{1}{12\\cdot 8+1}\\right)-1=\\exp\\!\\left(\\frac{1}{97}\\right)-1\\approx 0.01036>0.01,\n$$\nso the relative error cannot be below $0.01$ at $\\lambda=8$.\n\nTherefore, the smallest integer $\\lambda$ for which the Gaussian peak approximation has relative error less than $0.01$ is $\\lambda=9$.", "answer": "$$\\boxed{9}$$", "id": "1896384"}, {"introduction": "To truly understand the bridge between discrete and continuous distributions, it is invaluable to implement the approximation yourself. This computational exercise [@problem_id:3119292] guides you through validating the normal approximation to the binomial distribution. You will not only apply the Central Limit Theorem but also explore the concept of a continuity correction, a subtle but important technique that significantly improves accuracy by accounting for the integer nature of the original data.", "problem": "You are given independent and identically distributed Bernoulli random variables $Y_1, Y_2, \\dots, Y_n$ with parameter $p$, that is, $Y_i \\in \\{0,1\\}$ and $\\mathbb{P}(Y_i = 1) = p$. Define the binomial count $X = \\sum_{i=1}^{n} Y_i$. Starting from the core definitions $\\mathbb{E}[Y_i] = p$, $\\mathrm{Var}(Y_i) = p(1-p)$, and the Central Limit Theorem (CLT), derive in words the continuous approximation for the cumulative distribution function $\\mathbb{P}(X \\leq k)$ using a normal distribution, both without and with continuity correction. Implement a program to numerically validate these approximations against the exact binomial cumulative distribution function.\n\nYour program must:\n\n1. Compute the exact cumulative probability $\\mathbb{P}(X \\leq k)$ using an analytically correct method for the binomial distribution.\n2. Compute two normal approximations to $\\mathbb{P}(X \\leq k)$:\n   - A direct approximation based on CLT without continuity correction.\n   - An approximation with continuity correction by shifting the discrete threshold to a half-integer before applying the continuous model.\n3. Simulate binomial counts to estimate $\\mathbb{P}(X \\leq k)$ empirically using a fixed random seed and a fixed number of Monte Carlo trials. Use $N_{\\text{sim}} = 100000$ trials and the seed $12345$ so that the simulation is reproducible. The simulation is for illustration; the acceptance checks below must compare approximations against the exact analytic binomial probabilities.\n4. For each test case, compute the absolute errors of both normal approximations relative to the exact binomial cumulative probability. Let the tolerance be $\\tau = 0.02$. For each case, produce three booleans:\n   - Whether the error without continuity correction is at most $\\tau$.\n   - Whether the error with continuity correction is at most $\\tau$.\n   - Whether the error with continuity correction is less than or equal to the error without continuity correction.\n\nTest suite:\nUse the following $(n,p,k)$ cases to test a range of regimes (large $n$ and small $p$, near-mean thresholds, and boundary conditions):\n- Case 1: $(n=1000,\\; p=0.01,\\; k=0)$.\n- Case 2: $(n=1000,\\; p=0.01,\\; k=12)$.\n- Case 3: $(n=10000,\\; p=0.001,\\; k=20)$.\n- Case 4: $(n=500,\\; p=0.02,\\; k=15)$.\n- Case 5: $(n=200,\\; p=0.001,\\; k=0)$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must be the flattened concatenation, in order of the test cases, of the three booleans per case: $[$no-correction-pass, continuity-correction-pass, improvement$]$ for Case $1$, then the same triple for Case $2$, and so on. For example, a five-case run would look like\n$[$b$_{1,1}$, b$_{1,2}$, b$_{1,3}$, b$_{2,1}$, b$_{2,2}$, b$_{2,3}$, \\dots, b$_{5,1}$, b$_{5,2}$, b$_{5,3}]$,\nwhere each $b_{i,j}$ is a boolean.", "solution": "The problem requires the derivation of the normal approximation to the binomial distribution, with and without continuity correction, and a numerical validation of these approximations against exact values. The derivation will proceed from the foundational properties of Bernoulli variables and the Central Limit Theorem (CLT).\n\nA binomial random variable $X$ represents the total number of successes in $n$ independent trials, where each trial has a success probability of $p$. It is defined as the sum of $n$ independent and identically distributed (i.i.d.) Bernoulli random variables, $Y_1, Y_2, \\dots, Y_n$.\n$$X = \\sum_{i=1}^{n} Y_i$$\nEach $Y_i$ takes a value of $1$ (success) with probability $p$ or $0$ (failure) with probability $1-p$. The problem provides the expectation and variance for a single Bernoulli trial:\n- Expectation: $\\mathbb{E}[Y_i] = 1 \\cdot p + 0 \\cdot (1-p) = p$\n- Variance: $\\mathrm{Var}(Y_i) = \\mathbb{E}[Y_i^2] - (\\mathbb{E}[Y_i])^2 = (1^2 \\cdot p + 0^2 \\cdot (1-p)) - p^2 = p - p^2 = p(1-p)$\n\nDue to the linearity of expectation and the fact that the variables are independent for variance summation, the expectation and variance of the binomial random variable $X$ are:\n- Expectation of $X$: $\\mu = \\mathbb{E}[X] = \\mathbb{E}\\left[\\sum_{i=1}^{n} Y_i\\right] = \\sum_{i=1}^{n} \\mathbb{E}[Y_i] = \\sum_{i=1}^{n} p = np$\n- Variance of $X$: $\\sigma^2 = \\mathrm{Var}(X) = \\mathrm{Var}\\left(\\sum_{i=1}^{n} Y_i\\right) = \\sum_{i=1}^{n} \\mathrm{Var}(Y_i) = \\sum_{i=1}^{n} p(1-p) = np(1-p)$\n\nThe Central Limit Theorem (CLT) states that for a sufficiently large number of i.i.d. random variables, their sum (or average) will be approximately normally distributed. Applying the CLT to $X$, we can approximate its distribution with a normal distribution having the same mean $\\mu = np$ and variance $\\sigma^2 = np(1-p)$. Let this approximating normal variable be $X_{norm} \\sim \\mathcal{N}(np, np(1-p))$. The standardized version of $X$ is $Z = \\frac{X - \\mu}{\\sigma} = \\frac{X - np}{\\sqrt{np(1-p)}}$, which converges in distribution to the standard normal distribution $\\mathcal{N}(0,1)$ as $n \\to \\infty$.\n\nWe wish to approximate the cumulative distribution function (CDF) of the binomial distribution, $\\mathbb{P}(X \\leq k) = \\sum_{j=0}^{k} \\mathbb{P}(X=j)$.\n\n**1. Normal Approximation without Continuity Correction**\n\nThis is the most direct application of the CLT. We approximate the discrete binomial variable $X$ with the continuous normal variable $X_{norm}$. The probability $\\mathbb{P}(X \\leq k)$ is approximated by $\\mathbb{P}(X_{norm} \\leq k)$. To compute this probability, we standardize the variable:\n$$ \\mathbb{P}(X \\leq k) \\approx \\mathbb{P}(X_{norm} \\leq k) = \\mathbb{P}\\left(\\frac{X_{norm} - \\mu}{\\sigma} \\leq \\frac{k - \\mu}{\\sigma}\\right) = \\mathbb{P}\\left(Z \\leq \\frac{k - np}{\\sqrt{np(1-p)}}\\right) $$\nThis probability is given by the CDF of the standard normal distribution, denoted by $\\Phi(z)$.\n$$ \\mathbb{P}(X \\leq k) \\approx \\Phi\\left(\\frac{k - np}{\\sqrt{np(1-p)}}\\right) $$\n\n**2. Normal Approximation with Continuity Correction**\n\nThis method provides a more refined approximation by accounting for the fact that we are using a continuous distribution to model a discrete one. The probability mass function (PMF) of a binomial distribution is defined only at integer values. A common visualization is a histogram where the probability $\\mathbb{P}(X=j)$ is represented by a bar of width $1$ centered at the integer $j$. This bar spans the interval $[j-0.5, j+0.5]$.\nThe cumulative probability $\\mathbb{P}(X \\leq k)$ is the sum of probabilities for all integers from $0$ up to $k$. In the histogram representation, this corresponds to the total area of the bars for $j=0, 1, \\dots, k$. The total region covered by these bars extends up to $k+0.5$ on the continuous axis.\nTherefore, to better approximate this sum, we integrate the normal probability density function up to $k+0.5$.\n$$ \\mathbb{P}(X \\leq k) = \\sum_{j=0}^{k} \\mathbb{P}(X=j) \\approx \\mathbb{P}(X_{norm} \\leq k+0.5) $$\nStandardizing this corrected value gives the continuity-corrected approximation:\n$$ \\mathbb{P}(X \\leq k) \\approx \\mathbb{P}\\left(Z \\leq \\frac{(k+0.5) - np}{\\sqrt{np(1-p)}}\\right) = \\Phi\\left(\\frac{k+0.5 - np}{\\sqrt{np(1-p)}}\\right) $$\nThis correction generally improves the accuracy of the normal approximation, especially when $n$ is not excessively large or when $p$ is close to $0$ or $1$.\n\n**3. Numerical Validation**\n\nThe program will implement the following computations for each test case $(n,p,k)$:\n- **Exact Binomial CDF**: $\\mathbb{P}(X \\leq k) = \\sum_{j=0}^{k} \\binom{n}{j} p^j (1-p)^{n-j}$, computed using `scipy.stats.binom.cdf`.\n- **Normal Approximation (no CC)**: $\\Phi\\left(\\frac{k - np}{\\sqrt{np(1-p)}}\\right)$, computed using `scipy.stats.norm.cdf`.\n- **Normal Approximation (with CC)**: $\\Phi\\left(\\frac{k+0.5 - np}{\\sqrt{np(1-p)}}\\right)$, computed using `scipy.stats.norm.cdf`.\n- **Monte Carlo Simulation**: An empirical estimate is obtained by generating $N_{\\text{sim}} = 100000$ samples from a binomial distribution with parameters $n$ and $p$, and then calculating the fraction of samples that are less than or equal to $k$. This is for illustration purposes.\n- **Error Analysis**: The absolute errors of the two normal approximations relative to the exact binomial CDF are calculated. These errors are compared against a tolerance $\\tau = 0.02$ to determine if each approximation is acceptable. It is also checked whether the continuity correction provides an improvement (i.e., a smaller or equal error).", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import binom, norm\n\ndef solve():\n    \"\"\"\n    Validates normal approximations to the binomial distribution.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, p, k)\n        (1000, 0.01, 0),\n        (1000, 0.01, 12),\n        (10000, 0.001, 20),\n        (500, 0.02, 15),\n        (200, 0.001, 0),\n    ]\n\n    # Constants for simulation and validation\n    N_sim = 100000\n    seed = 12345\n    tau = 0.02\n    \n    # Initialize a random number generator for reproducibility\n    rng = np.random.default_rng(seed)\n    \n    results = []\n    for case in test_cases:\n        n, p, k = case\n        \n        # Calculate mean and standard deviation of the binomial distribution\n        mu = n * p\n        sigma_sq = n * p * (1 - p)\n\n        # 1. Compute the exact cumulative probability P(X <= k)\n        exact_prob = binom.cdf(k, n, p)\n\n        # 2. Compute normal approximations\n        # Handle the edge case where variance is zero (p=0 or p=1),\n        # though not present in the test suite.\n        if sigma_sq > 0:\n            sigma = np.sqrt(sigma_sq)\n            \n            # 2a. Direct approximation without continuity correction\n            z_no_cc = (k - mu) / sigma\n            approx_no_cc = norm.cdf(z_no_cc)\n            \n            # 2b. Approximation with continuity correction\n            z_cc = (k + 0.5 - mu) / sigma\n            approx_cc = norm.cdf(z_cc)\n        else:\n            # If sigma is 0, the distribution is deterministic.\n            # X = mu with probability 1. The CDF is a step function.\n            approx_no_cc = 1.0 if k >= mu else 0.0\n            approx_cc = 1.0 if k + 0.5 >= mu else 0.0\n        \n        # 3. Simulate binomial counts to estimate P(X <= k) empirically (for illustration)\n        # This part is required by the prompt but its result is not used in the final checks.\n        sim_samples = rng.binomial(n, p, size=N_sim)\n        mc_prob = np.mean(sim_samples <= k)\n\n        # 4. Compute absolute errors and perform validation checks.\n        # The checks compare the approximations against the exact analytical probability.\n        err_no_cc = abs(approx_no_cc - exact_prob)\n        err_cc = abs(approx_cc - exact_prob)\n        \n        # Boolean check 1: Is error without correction within tolerance?\n        no_cc_pass = err_no_cc <= tau\n        \n        # Boolean check 2: Is error with correction within tolerance?\n        cc_pass = err_cc <= tau\n        \n        # Boolean check 3: Is continuity correction an improvement?\n        cc_improves = err_cc <= err_no_cc\n        \n        results.extend([no_cc_pass, cc_pass, cc_improves])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3119292"}]}