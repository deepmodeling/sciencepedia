{"hands_on_practices": [{"introduction": "The ability to construct an orthonormal basis is a cornerstone of computational linear algebra. This exercise [@problem_id:2309884] provides hands-on practice with the Gram-Schmidt process, a systematic procedure for transforming any set of linearly independent vectors into a set of mutually orthogonal unit vectors. Mastering this technique is crucial as orthonormal bases simplify countless calculations, from finding projection coordinates to implementing algorithms in signal processing and physics.", "problem": "In the study of a particular crystal structure, the behavior of electrons is analyzed within a 3-dimensional space represented by $\\mathbb{R}^3$, equipped with the standard Euclidean inner product (dot product). Three fundamental, linearly independent direction vectors are identified:\n$$v_1 = (1, 1, 0)$$\n$$v_2 = (1, 0, 1)$$\n$$v_3 = (0, 1, 1)$$\nThese vectors form a basis for $\\mathbb{R}^3$, but they are not mutually orthogonal, which complicates physical calculations. To simplify the analysis, one must construct a new orthonormal basis $\\{e_1, e_2, e_3\\}$ from the set $\\{v_1, v_2, v_3\\}$ by applying the Gram-Schmidt process in the specified order. The process requires that for each new vector $e_k$, the inner product $\\langle e_k, u_k \\rangle$ is positive, where $u_k$ is the intermediate orthogonal vector from which $e_k$ is derived.\n\nWhich of the following sets represents the resulting orthonormal basis $\\{e_1, e_2, e_3\\}$?\n\nA. $\\left\\{ \\left(\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}, 0\\right), \\left(\\frac{1}{\\sqrt{2}}, -\\frac{1}{\\sqrt{2}}, 0\\right), (0, 0, 1) \\right\\}$\n\nB. $\\left\\{ (1, 1, 0), \\left(\\frac{1}{2}, -\\frac{1}{2}, 1\\right), \\left(-\\frac{2}{3}, \\frac{2}{3}, \\frac{2}{3}\\right) \\right\\}$\n\nC. $\\left\\{ \\left(\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}, 0\\right), \\left(\\frac{1}{\\sqrt{6}}, -\\frac{1}{\\sqrt{6}}, \\frac{2}{\\sqrt{6}}\\right), \\left(-\\frac{1}{\\sqrt{3}}, \\frac{1}{\\sqrt{3}}, \\frac{1}{\\sqrt{3}}\\right) \\right\\}$\n\nD. $\\left\\{ (1, 1, 0), (1, 0, 1), (0, 1, 1) \\right\\}$\n\nE. $\\left\\{ \\left(\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}, 0\\right), \\left(\\frac{1}{\\sqrt{3}}, -\\frac{1}{\\sqrt{3}}, \\frac{1}{\\sqrt{3}}\\right), \\left(\\frac{1}{\\sqrt{6}}, \\frac{1}{\\sqrt{6}}, -\\frac{2}{\\sqrt{6}}\\right) \\right\\}$", "solution": "We apply Gramâ€“Schmidt to $v_{1}=(1,1,0)$, $v_{2}=(1,0,1)$, $v_{3}=(0,1,1)$ in this order, using the standard dot product, and choose signs so that $\\langle e_{k},u_{k}\\rangle>0$.\n\nFirst, set $u_{1}=v_{1}$, so\n$$\n\\|u_{1}\\|=\\sqrt{1^{2}+1^{2}+0^{2}}=\\sqrt{2},\\quad e_{1}=\\frac{u_{1}}{\\|u_{1}\\|}=\\left(\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}},0\\right).\n$$\nThen\n$$\nu_{2}=v_{2}-\\operatorname{proj}_{e_{1}}(v_{2})=v_{2}-(\\langle v_{2},e_{1}\\rangle)e_{1}.\n$$\nCompute $\\langle v_{2},e_{1}\\rangle=\\left\\langle (1,0,1),\\left(\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}},0\\right)\\right\\rangle=\\frac{1}{\\sqrt{2}}$, hence\n$$\nu_{2}=(1,0,1)-\\frac{1}{\\sqrt{2}}\\left(\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}},0\\right)=\\left(1,0,1\\right)-\\left(\\frac{1}{2},\\frac{1}{2},0\\right)=\\left(\\frac{1}{2},-\\frac{1}{2},1\\right).\n$$\nIts norm is\n$$\n\\|u_{2}\\|=\\sqrt{\\left(\\frac{1}{2}\\right)^{2}+\\left(-\\frac{1}{2}\\right)^{2}+1^{2}}=\\sqrt{\\frac{3}{2}},\n$$\nso\n$$\ne_{2}=\\frac{u_{2}}{\\|u_{2}\\|}=\\sqrt{\\frac{2}{3}}\\left(\\frac{1}{2},-\\frac{1}{2},1\\right)=\\left(\\frac{1}{\\sqrt{6}},-\\frac{1}{\\sqrt{6}},\\frac{2}{\\sqrt{6}}\\right),\n$$\nwhich satisfies $\\langle e_{2},u_{2}\\rangle=\\|u_{2}\\|>0$.\n\nNext,\n$$\nu_{3}=v_{3}-\\operatorname{proj}_{e_{1}}(v_{3})-\\operatorname{proj}_{e_{2}}(v_{3})=v_{3}-(\\langle v_{3},e_{1}\\rangle)e_{1}-(\\langle v_{3},e_{2}\\rangle)e_{2}.\n$$\nCompute $\\langle v_{3},e_{1}\\rangle=\\left\\langle (0,1,1),\\left(\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}},0\\right)\\right\\rangle=\\frac{1}{\\sqrt{2}}$, hence $(\\langle v_{3},e_{1}\\rangle)e_{1}=\\left(\\frac{1}{2},\\frac{1}{2},0\\right)$. Also,\n$$\n\\langle v_{3},e_{2}\\rangle=\\left\\langle (0,1,1),\\left(\\frac{1}{\\sqrt{6}},-\\frac{1}{\\sqrt{6}},\\frac{2}{\\sqrt{6}}\\right)\\right\\rangle=\\frac{1}{\\sqrt{6}},\n$$\nso $(\\langle v_{3},e_{2}\\rangle)e_{2}=\\frac{1}{\\sqrt{6}}\\left(\\frac{1}{\\sqrt{6}},-\\frac{1}{\\sqrt{6}},\\frac{2}{\\sqrt{6}}\\right)=\\left(\\frac{1}{6},-\\frac{1}{6},\\frac{1}{3}\\right)$. Therefore\n$$\nu_{3}=(0,1,1)-\\left(\\frac{1}{2},\\frac{1}{2},0\\right)-\\left(\\frac{1}{6},-\\frac{1}{6},\\frac{1}{3}\\right)=\\left(-\\frac{2}{3},\\frac{2}{3},\\frac{2}{3}\\right).\n$$\nIts norm is\n$$\n\\|u_{3}\\|=\\sqrt{\\left(-\\frac{2}{3}\\right)^{2}+\\left(\\frac{2}{3}\\right)^{2}+\\left(\\frac{2}{3}\\right)^{2}}=\\sqrt{\\frac{4}{3}}=\\frac{2}{\\sqrt{3}},\n$$\nso\n$$\ne_{3}=\\frac{u_{3}}{\\|u_{3}\\|}=\\frac{\\sqrt{3}}{2}\\left(-\\frac{2}{3},\\frac{2}{3},\\frac{2}{3}\\right)=\\left(-\\frac{1}{\\sqrt{3}},\\frac{1}{\\sqrt{3}},\\frac{1}{\\sqrt{3}}\\right),\n$$\nwith $\\langle e_{3},u_{3}\\rangle=\\|u_{3}\\|>0$. The resulting orthonormal basis is\n$$\n\\left\\{\\left(\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}},0\\right),\\left(\\frac{1}{\\sqrt{6}},-\\frac{1}{\\sqrt{6}},\\frac{2}{\\sqrt{6}}\\right),\\left(-\\frac{1}{\\sqrt{3}},\\frac{1}{\\sqrt{3}},\\frac{1}{\\sqrt{3}}\\right)\\right\\},\n$$\nwhich corresponds to option C.", "answer": "$$\\boxed{C}$$", "id": "2309884"}, {"introduction": "Orthogonality and projection are powerful concepts that extend far beyond the familiar geometry of $\\mathbb{R}^n$. This problem [@problem_id:2309898] challenges you to apply these ideas within the vector space of $2 \\times 2$ matrices, using the Frobenius inner product, $\\langle A, B \\rangle = \\operatorname{tr}(A^T B)$. By finding the component of a matrix orthogonal to the subspace of symmetric matrices, you'll gain insight into how these geometric tools can be used to decompose and analyze more abstract data structures.", "problem": "Consider the vector space $V = M_{2 \\times 2}(\\mathbb{R})$ of all $2 \\times 2$ matrices with real entries. This space is equipped with the Frobenius inner product, defined as $\\langle A, B \\rangle = \\operatorname{tr}(A^T B)$ for any two matrices $A, B \\in V$, where $\\operatorname{tr}(M)$ denotes the trace of a matrix $M$. Let $S$ be the subspace of $V$ consisting of all symmetric matrices.\n\nGiven the matrix $A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}$, find its component that is orthogonal to the subspace $S$.\n\nPresent your answer, which will be a $2 \\times 2$ matrix $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$, as a row matrix of its components in the format $\\begin{pmatrix} a & b & c & d \\end{pmatrix}$.", "solution": "Let $V = M_{2 \\times 2}(\\mathbb{R})$ be the vector space of $2 \\times 2$ real matrices, and let $S$ be the subspace of symmetric matrices. The given matrix is $A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}$. We are asked to find the component of $A$ that is orthogonal to the subspace $S$.\n\nAny vector $A$ in an inner product space can be uniquely decomposed into two components: one that lies in a subspace $S$ and one that is orthogonal to it. This is written as $A = A_S + A_{S^\\perp}$, where $A_S = \\operatorname{proj}_S(A)$ is the orthogonal projection of $A$ onto $S$, and $A_{S^\\perp}$ is the component of $A$ orthogonal to $S$. We need to find $A_{S^\\perp}$, which can be calculated as $A_{S^\\perp} = A - \\operatorname{proj}_S(A)$.\n\nOur first step is to find the orthogonal projection of $A$ onto $S$. To do this, we need an orthogonal basis for the subspace $S$. A general symmetric $2 \\times 2$ matrix has the form $\\begin{pmatrix} x & y \\\\ y & z \\end{pmatrix}$. We can write this as a linear combination of basis matrices:\n$$\n\\begin{pmatrix} x & y \\\\ y & z \\end{pmatrix} = x \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + z \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} + y \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}\n$$\nSo, a basis for $S$ is $\\{B_1, B_2, B_3\\}$, where $B_1 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}$, $B_2 = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}$, and $B_3 = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}$.\n\nNext, we check if this basis is orthogonal with respect to the Frobenius inner product, $\\langle X, Y \\rangle = \\operatorname{tr}(X^T Y)$. Since the basis matrices are symmetric, $X^T = X$.\n$$\n\\langle B_1, B_2 \\rangle = \\operatorname{tr}(B_1^T B_2) = \\operatorname{tr}(B_1 B_2) = \\operatorname{tr}\\left( \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} \\right) = \\operatorname{tr}\\left( \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix} \\right) = 0\n$$\n$$\n\\langle B_1, B_3 \\rangle = \\operatorname{tr}(B_1^T B_3) = \\operatorname{tr}(B_1 B_3) = \\operatorname{tr}\\left( \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} \\right) = \\operatorname{tr}\\left( \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} \\right) = 0\n$$\n$$\n\\langle B_2, B_3 \\rangle = \\operatorname{tr}(B_2^T B_3) = \\operatorname{tr}(B_2 B_3) = \\operatorname{tr}\\left( \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} \\right) = \\operatorname{tr}\\left( \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} \\right) = 0\n$$\nThe basis is indeed orthogonal. The formula for the orthogonal projection of $A$ onto $S$ is:\n$$\n\\operatorname{proj}_S(A) = \\frac{\\langle A, B_1 \\rangle}{\\langle B_1, B_1 \\rangle} B_1 + \\frac{\\langle A, B_2 \\rangle}{\\langle B_2, B_2 \\rangle} B_2 + \\frac{\\langle A, B_3 \\rangle}{\\langle B_3, B_3 \\rangle} B_3\n$$\nWe compute the required inner products. First, the denominators (squared norms of basis vectors):\n$$\n\\langle B_1, B_1 \\rangle = \\operatorname{tr}(B_1^T B_1) = \\operatorname{tr}(B_1^2) = \\operatorname{tr}\\left( \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\right) = 1\n$$\n$$\n\\langle B_2, B_2 \\rangle = \\operatorname{tr}(B_2^T B_2) = \\operatorname{tr}(B_2^2) = \\operatorname{tr}\\left( \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} \\right) = 1\n$$\n$$\n\\langle B_3, B_3 \\rangle = \\operatorname{tr}(B_3^T B_3) = \\operatorname{tr}(B_3^2) = \\operatorname{tr}\\left( \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}^2 \\right) = \\operatorname{tr}\\left( \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\right) = 2\n$$\nNext, the numerators. For $A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}$, its transpose is $A^T = \\begin{pmatrix} 1 & 3 \\\\ 2 & 4 \\end{pmatrix}$.\n$$\n\\langle A, B_1 \\rangle = \\operatorname{tr}(A^T B_1) = \\operatorname{tr}\\left( \\begin{pmatrix} 1 & 3 \\\\ 2 & 4 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\right) = \\operatorname{tr}\\left( \\begin{pmatrix} 1 & 0 \\\\ 2 & 0 \\end{pmatrix} \\right) = 1\n$$\n$$\n\\langle A, B_2 \\rangle = \\operatorname{tr}(A^T B_2) = \\operatorname{tr}\\left( \\begin{pmatrix} 1 & 3 \\\\ 2 & 4 \\end{pmatrix} \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} \\right) = \\operatorname{tr}\\left( \\begin{pmatrix} 0 & 3 \\\\ 0 & 4 \\end{pmatrix} \\right) = 4\n$$\n$$\n\\langle A, B_3 \\rangle = \\operatorname{tr}(A^T B_3) = \\operatorname{tr}\\left( \\begin{pmatrix} 1 & 3 \\\\ 2 & 4 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} \\right) = \\operatorname{tr}\\left( \\begin{pmatrix} 3 & 1 \\\\ 4 & 2 \\end{pmatrix} \\right) = 3+2 = 5\n$$\nNow, we can assemble the projection:\n$$\n\\operatorname{proj}_S(A) = \\frac{1}{1} B_1 + \\frac{4}{1} B_2 + \\frac{5}{2} B_3 = 1 \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + 4 \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\frac{5}{2} \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}\n$$\n$$\n\\operatorname{proj}_S(A) = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + \\begin{pmatrix} 0 & 0 \\\\ 0 & 4 \\end{pmatrix} + \\begin{pmatrix} 0 & 5/2 \\\\ 5/2 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 5/2 \\\\ 5/2 & 4 \\end{pmatrix}\n$$\nFinally, we find the orthogonal component $A_{S^\\perp}$ by subtracting the projection from the original matrix $A$:\n$$\nA_{S^\\perp} = A - \\operatorname{proj}_S(A) = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix} - \\begin{pmatrix} 1 & 5/2 \\\\ 5/2 & 4 \\end{pmatrix}\n$$\n$$\nA_{S^\\perp} = \\begin{pmatrix} 1-1 & 2 - 5/2 \\\\ 3 - 5/2 & 4-4 \\end{pmatrix} = \\begin{pmatrix} 0 & 4/2 - 5/2 \\\\ 6/2 - 5/2 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & -1/2 \\\\ 1/2 & 0 \\end{pmatrix}\n$$\nThis resulting matrix is skew-symmetric, which is expected as the space of skew-symmetric matrices is the orthogonal complement of the space of symmetric matrices.\n\nThe resulting matrix is $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} = \\begin{pmatrix} 0 & -1/2 \\\\ 1/2 & 0 \\end{pmatrix}$. Following the specified output format, we present this as a row matrix.", "answer": "$$\\boxed{\\begin{pmatrix} 0 & -\\frac{1}{2} & \\frac{1}{2} & 0 \\end{pmatrix}}$$", "id": "2309898"}, {"introduction": "This advanced practice [@problem_id:3168150] delves into the dynamic behavior of projections by exploring the method of alternating projections, a powerful iterative technique used in optimization and data analysis. You will implement the algorithm to find the point at the intersection of two affine subspaces and analyze its convergence. This exercise provides a concrete link between an abstract geometric propertyâ€”the angle between subspacesâ€”and a critical performance metric of a numerical algorithm: its rate of convergence, which is asymptotically equal to $\\cos^2\\theta$.", "problem": "You will implement and analyze alternating orthogonal projections between two affine subspaces to explore the relationship between geometric angle and convergence. Work in the real Euclidean space $\\mathbb{R}^3$ with the standard inner product $\\langle \\cdot,\\cdot \\rangle$ and induced norm $\\|\\cdot\\|_2$. An affine subspace is a translate of a linear subspace. The orthogonal projection of a point $\\mathbf{x} \\in \\mathbb{R}^3$ onto an affine line $\\mathcal{L} = \\mathbf{p} + \\operatorname{span}\\{\\mathbf{u}\\}$, where $\\mathbf{u}$ is a unit vector and $\\mathbf{p}$ is a point on the line, is the unique point in $\\mathcal{L}$ minimizing the Euclidean distance to $\\mathbf{x}$. Using only these definitions and facts, derive from first principles an estimation method for the geometric angle between two affine lines based on the asymptotic contraction of the alternating projection mapping.\n\nConstruct your instances as follows. Fix the base point $\\mathbf{p} = (0.6,-0.3,0.2)^\\top \\in \\mathbb{R}^3$ and the starting point $\\mathbf{x}_0 = (2.0,1.5,-0.7)^\\top \\in \\mathbb{R}^3$. For each given angle parameter $\\theta \\in (0,\\pi/2)$, define two affine lines that intersect at $\\mathbf{p}$ with directions\n- $\\mathbf{u} = (1,0,0)^\\top$,\n- $\\mathbf{v}(\\theta) = (\\cos \\theta,\\ \\sin \\theta,\\ 0)^\\top$,\nso that $\\mathcal{A}(\\theta) = \\mathbf{p} + \\operatorname{span}\\{\\mathbf{u}\\}$ and $\\mathcal{B}(\\theta) = \\mathbf{p} + \\operatorname{span}\\{\\mathbf{v}(\\theta)\\}$.\n\nLet $P_{\\mathcal{A}}$ and $P_{\\mathcal{B}}$ denote the orthogonal projections onto $\\mathcal{A}(\\theta)$ and $\\mathcal{B}(\\theta)$, respectively. Consider the alternating projection iteration $\\mathbf{x}_{k+1} = P_{\\mathcal{B}}(P_{\\mathcal{A}}(\\mathbf{x}_k))$ for $k \\in \\{0,1,2,\\dots\\}$, and define $d_k = \\|\\mathbf{x}_k - \\mathbf{p}\\|_2$. For each $\\theta$, generate a sufficiently long sequence $\\{d_k\\}_{k=0}^K$ with $K = 200$. Estimate the asymptotic contraction constant $r$ by computing the median of the ratios $d_{k+1}/d_k$ over a window of consecutive indices $k$ that excludes an initial transient and avoids numerical underflow; more precisely, use the set of indices $k \\in \\{2,3,\\dots,\\min(2+20,K-1)\\}$ such that both $d_k$ and $d_{k+1}$ exceed the threshold $10^{-300}$. If this set is empty, fall back to using the earliest available ratios that satisfy the threshold. Then invert the theoretical relationship between the contraction constant and the geometric angle to produce an estimate $\\widehat{\\theta}$ in radians.\n\nAngle unit requirement: all angles must be expressed in radians.\n\nYour program must implement the above construction and procedure and output the estimates $\\widehat{\\theta}$ for the following test suite of angle parameters:\n- $\\theta_1 = \\pi/6$,\n- $\\theta_2 = \\pi/3$,\n- $\\theta_3 = \\pi/36$,\n- $\\theta_4 = 17\\pi/36$.\n\nFinal output format: your program should produce a single line of output containing the four estimated angles as a comma-separated list enclosed in square brackets, in radians, each rounded to six decimal places (for example, $[0.523599,1.047198,0.087266,1.483530]$). No other text should be printed.", "solution": "The problem is valid as it is scientifically grounded in the theory of linear algebra and numerical analysis, specifically concerning projections onto convex sets. It is well-posed, objective, and internally consistent.\n\nThe objective is to derive and implement a method for estimating the angle $\\theta$ between two intersecting affine lines, $\\mathcal{A}$ and $\\mathcal{B}$, by analyzing the convergence rate of an iterative projection scheme. The lines are defined in $\\mathbb{R}^3$ as $\\mathcal{A} = \\mathbf{p} + \\operatorname{span}\\{\\mathbf{u}\\}$ and $\\mathcal{B} = \\mathbf{p} + \\operatorname{span}\\{\\mathbf{v}(\\theta)\\}$, where $\\mathbf{p}$ is their common intersection point and $\\mathbf{u}, \\mathbf{v}(\\theta)$ are unit direction vectors. The angle between the lines is the angle $\\theta$ between their direction vectors.\n\nFirst, we derive the formula for the orthogonal projection of a point $\\mathbf{x} \\in \\mathbb{R}^3$ onto a generic affine line $\\mathcal{L} = \\mathbf{p}_{\\mathcal{L}} + \\operatorname{span}\\{\\mathbf{w}\\}$, where $\\mathbf{w}$ is a unit vector. The projection $P_{\\mathcal{L}}(\\mathbf{x})$ is the point $\\mathbf{y} \\in \\mathcal{L}$ that minimizes the squared Euclidean distance $f(\\mathbf{y}) = \\|\\mathbf{x} - \\mathbf{y}\\|_2^2$. Any point $\\mathbf{y} \\in \\mathcal{L}$ can be parameterized by a scalar $t \\in \\mathbb{R}$ as $\\mathbf{y}(t) = \\mathbf{p}_{\\mathcal{L}} + t\\mathbf{w}$. The objective function becomes:\n$$f(t) = \\|\\mathbf{x} - (\\mathbf{p}_{\\mathcal{L}} + t\\mathbf{w})\\|_2^2 = \\langle (\\mathbf{x} - \\mathbf{p}_{\\mathcal{L}}) - t\\mathbf{w}, (\\mathbf{x} - \\mathbf{p}_{\\mathcal{L}}) - t\\mathbf{w} \\rangle$$\nExpanding the inner product gives:\n$$f(t) = \\|\\mathbf{x} - \\mathbf{p}_{\\mathcal{L}}\\|_2^2 - 2t \\langle \\mathbf{x} - \\mathbf{p}_{\\mathcal{L}}, \\mathbf{w} \\rangle + t^2 \\|\\mathbf{w}\\|_2^2$$\nSince $\\mathbf{w}$ is a unit vector, $\\|\\mathbf{w}\\|_2^2=1$. To find the minimum, we differentiate with respect to $t$ and set the result to zero:\n$$\\frac{df}{dt} = -2 \\langle \\mathbf{x} - \\mathbf{p}_{\\mathcal{L}}, \\mathbf{w} \\rangle + 2t = 0$$\nSolving for $t$ yields the optimal value $t^* = \\langle \\mathbf{x} - \\mathbf{p}_{\\mathcal{L}}, \\mathbf{w} \\rangle$. The orthogonal projection is thus:\n$$P_{\\mathcal{L}}(\\mathbf{x}) = \\mathbf{p}_{\\mathcal{L}} + t^*\\mathbf{w} = \\mathbf{p}_{\\mathcal{L}} + \\langle \\mathbf{x} - \\mathbf{p}_{\\mathcal{L}}, \\mathbf{w} \\rangle \\mathbf{w}$$\n\nNext, we analyze the iterative sequence $\\mathbf{x}_{k+1} = P_{\\mathcal{B}}(P_{\\mathcal{A}}(\\mathbf{x}_k))$. It is more convenient to analyze the dynamics of the vector relative to the intersection point $\\mathbf{p}$. Let $\\tilde{\\mathbf{x}}_k = \\mathbf{x}_k - \\mathbf{p}$. The projection operator $P_{\\mathcal{L}}$ onto $\\mathcal{L} = \\mathbf{p} + \\operatorname{span}\\{\\mathbf{w}\\}$ can be related to the linear projection operator $P_S$ onto the corresponding linear subspace $S=\\operatorname{span}\\{\\mathbf{w}\\}$. The formula for $P_S$ is $P_S(\\mathbf{z}) = \\langle \\mathbf{z}, \\mathbf{w} \\rangle \\mathbf{w}$.\n$$P_{\\mathcal{L}}(\\mathbf{x}) - \\mathbf{p} = (\\mathbf{p} + \\langle \\mathbf{x} - \\mathbf{p}, \\mathbf{w} \\rangle \\mathbf{w}) - \\mathbf{p} = \\langle \\mathbf{x} - \\mathbf{p}, \\mathbf{w} \\rangle \\mathbf{w} = P_S(\\mathbf{x} - \\mathbf{p})$$\nLet $S_A = \\operatorname{span}\\{\\mathbf{u}\\}$ and $S_B = \\operatorname{span}\\{\\mathbf{v}(\\theta)\\}$. The iteration for the centered vectors is:\n$$\\tilde{\\mathbf{x}}_{k+1} = \\mathbf{x}_{k+1} - \\mathbf{p} = P_{\\mathcal{B}}(P_{\\mathcal{A}}(\\mathbf{x}_k)) - \\mathbf{p} = P_{S_B}(P_{\\mathcal{A}}(\\mathbf{x}_k) - \\mathbf{p})$$\nLet $\\mathbf{y}_k = P_{\\mathcal{A}}(\\mathbf{x}_k)$. Then $P_{\\mathcal{A}}(\\mathbf{x}_k) - \\mathbf{p} = P_{S_A}(\\mathbf{x}_k - \\mathbf{p}) = P_{S_A}(\\tilde{\\mathbf{x}}_k)$. Substituting this back, we get:\n$$\\tilde{\\mathbf{x}}_{k+1} = P_{S_B}(P_{S_A}(\\tilde{\\mathbf{x}}_k))$$\nThis shows that the affine iteration is equivalent to a linear iteration on the vectors translated by $-\\mathbf{p}$.\n\nNow we derive the contraction rate. The vector $\\tilde{\\mathbf{x}}_{k+1}$ is the result of a projection onto $S_B$, so it must lie within $S_B$. This means $\\tilde{\\mathbf{x}}_{k+1}$ must be a scalar multiple of the direction vector $\\mathbf{v}(\\theta)$ for all $k \\ge 0$. Consequently, for all $k \\ge 1$, the vector $\\tilde{\\mathbf{x}}_k$ is parallel to $\\mathbf{v}(\\theta)$. Let $\\tilde{\\mathbf{x}}_k = c_k \\mathbf{v}(\\theta)$ for some scalar $c_k$ and for all $k \\ge 1$.\nThe iteration for $k \\ge 1$ becomes:\n$$\\tilde{\\mathbf{x}}_{k+1} = P_{S_B}(P_{S_A}(\\tilde{\\mathbf{x}}_k)) = P_{S_B}(P_{S_A}(c_k \\mathbf{v}(\\theta))) = c_k P_{S_B}(P_{S_A}(\\mathbf{v}(\\theta)))$$\nWe compute the composed linear projection $P_{S_B}(P_{S_A}(\\mathbf{v}(\\theta)))$:\n$$P_{S_A}(\\mathbf{v}(\\theta)) = \\langle \\mathbf{v}(\\theta), \\mathbf{u} \\rangle \\mathbf{u}$$\nThe inner product $\\langle \\mathbf{v}(\\theta), \\mathbf{u} \\rangle$ is the cosine of the angle between the direction vectors, which is $\\cos\\theta$.\n$$P_{S_A}(\\mathbf{v}(\\theta)) = (\\cos\\theta) \\mathbf{u}$$\nNext, we project this result onto $S_B$:\n$$P_{S_B}((\\cos\\theta) \\mathbf{u}) = (\\cos\\theta) P_{S_B}(\\mathbf{u}) = (\\cos\\theta) \\langle \\mathbf{u}, \\mathbf{v}(\\theta) \\rangle \\mathbf{v}(\\theta) = (\\cos\\theta) (\\cos\\theta) \\mathbf{v}(\\theta) = (\\cos^2\\theta) \\mathbf{v}(\\theta)$$\nSubstituting this back into the iteration for $\\tilde{\\mathbf{x}}_{k+1}$:\n$$\\tilde{\\mathbf{x}}_{k+1} = c_k (\\cos^2\\theta) \\mathbf{v}(\\theta) = (\\cos^2\\theta) (c_k \\mathbf{v}(\\theta)) = (\\cos^2\\theta) \\tilde{\\mathbf{x}}_k$$\nThis relation holds for all $k \\ge 1$. Taking the Euclidean norm of both sides:\n$$d_{k+1} = \\|\\tilde{\\mathbf{x}}_{k+1}\\|_2 = \\|(\\cos^2\\theta) \\tilde{\\mathbf{x}}_k\\|_2 = |\\cos^2\\theta| \\|\\tilde{\\mathbf{x}}_k\\|_2 = (\\cos^2\\theta) d_k$$\nThe absolute value is removed because $\\theta \\in (0, \\pi/2)$ implies $\\cos\\theta > 0$. The ratio of successive distances to the intersection point is constant for $k \\ge 1$:\n$$\\frac{d_{k+1}}{d_k} = \\cos^2\\theta$$\nThe ratio $d_1/d_0$ is a transient that depends on the initial point $\\mathbf{x}_0$, but for all subsequent steps, the contraction is governed by the constant factor $r = \\cos^2\\theta$.\n\nThe estimation procedure involves numerically generating the sequence $\\{\\mathbf{x}_k\\}$ and then calculating the sequence of distances $\\{d_k\\}$. The asymptotic contraction constant $r$ is estimated by taking the median, $\\widehat{r}$, of the ratios $d_{k+1}/d_k$ over a window of indices $k \\ge 2$ to ensure the transient initial step is excluded.\nGiven the estimate $\\widehat{r}$, the theoretical relationship $r = \\cos^2\\theta$ is inverted to find the angle estimate $\\widehat{\\theta}$:\n$$\\widehat{r} \\approx \\cos^2\\theta \\implies \\cos\\theta \\approx \\sqrt{\\widehat{r}}$$\nWe take the positive square root because $\\theta \\in (0, \\pi/2)$ ensures $\\cos\\theta > 0$. The final estimate is:\n$$\\widehat{\\theta} = \\arccos(\\sqrt{\\widehat{r}})$$\nThe algorithm implemented will perform these steps for each given value of $\\theta$ in the test suite.", "answer": "$$\\boxed{\\texttt{[0.523599,1.047198,0.087266,1.483530]}}$$", "id": "3168150"}]}