{"hands_on_practices": [{"introduction": "Before tackling complex applications, it is crucial to master the fundamental algebraic rules that govern determinants. This exercise focuses on two cornerstone properties: the determinant of a product of matrices and the determinant of an inverse matrix. By working through this problem [@problem_id:16962], you will solidify your ability to manipulate expressions involving determinants, a skill that is indispensable for both theoretical proofs and practical calculations in linear algebra.", "problem": "Let $A$ be an $n \\times n$ invertible matrix over the real numbers. The determinant of $A$ is a non-zero scalar, denoted by $d$. The inverse of the matrix $A$ is denoted by $A^{-1}$, and the identity matrix is denoted by $I$. The matrix $A^{-2}$ is defined as the product of the inverse matrix with itself, i.e., $A^{-2} = A^{-1}A^{-1}$.\n\nUsing the fundamental properties of determinants, derive an expression for $\\det(A^{-2})$ in terms of $d$.", "solution": "From the property $\\det(A^{-1})=(\\det A)^{-1}$, we get\n$$\\det(A^{-1})=d^{-1}.$$\nUsing the multiplicative property $\\det(BC)=\\det B\\cdot\\det C$, we have\n$$\\det(A^{-2})=\\det(A^{-1}A^{-1})=\\det(A^{-1})^2=(d^{-1})^2=d^{-2}.$$", "answer": "$$\\boxed{d^{-2}}$$", "id": "16962"}, {"introduction": "The concepts of invertibility and determinants are deeply connected to the structure of vector spaces. This practice problem [@problem_id:28828] challenges you to investigate whether the set of all invertible matrices forms a vector subspace. By using the identity matrix and its additive inverse as a test case, you will use the determinant to quickly check for invertibility and discover a crucial structural property of the set of invertible matrices.", "problem": "Let $V = M_{2 \\times 2}(\\mathbb{R})$ be the vector space of all $2 \\times 2$ matrices with real entries. A subset $W$ of a vector space $V$ is a subspace if it satisfies three criteria:\n1. The zero vector of $V$ is in $W$.\n2. $W$ is closed under vector addition: for any $\\mathbf{u}, \\mathbf{v} \\in W$, their sum $\\mathbf{u} + \\mathbf{v}$ is also in $W$.\n3. $W$ is closed under scalar multiplication: for any $\\mathbf{u} \\in W$ and any scalar $c$, the product $c\\mathbf{u}$ is also in $W$.\n\nConsider the subset $W \\subset V$ consisting of all invertible $2 \\times 2$ matrices. A matrix $A$ is invertible if and only if its determinant is non-zero, i.e., $\\det(A) \\neq 0$.\n\nTo test if $W$ is a subspace, we can investigate its closure under addition. Take two matrices from $W$: the $2 \\times 2$ identity matrix, $I_2$, and its additive inverse, $-I_2$. Let their sum be denoted by the matrix $S$.\n\nCalculate the determinant of the resulting matrix $S$.", "solution": "We take two invertible matrices in $W$, namely the identity and its additive inverse, and form their sum. \n\nThe $2\\times2$ identity and its negative are\n$$\nI_2 = \\begin{pmatrix}1 & 0 \\\\[6pt] 0 & 1\\end{pmatrix}, \n\\quad\n-\\,I_2 = \\begin{pmatrix}-1 & 0 \\\\[6pt] 0 & -1\\end{pmatrix}.\n$$\n\nTheir sum is the zero matrix:\n$$\nS \\;=\\; I_2 + (-I_2)\n\\;=\\;\n\\begin{pmatrix}1 & 0 \\\\[3pt] 0 & 1\\end{pmatrix}\n+\n\\begin{pmatrix}-1 & 0 \\\\[3pt] 0 & -1\\end{pmatrix}\n=\n\\begin{pmatrix}0 & 0 \\\\[3pt] 0 & 0\\end{pmatrix}.\n$$\n\nSince the determinant of the zero matrix is zero, we have\n$$\n\\det(S) \\;=\\; \\det\\!\\bigl(0_{2\\times2}\\bigr) \\;=\\; 0.\n$$", "answer": "$$\\boxed{0}$$", "id": "28828"}, {"introduction": "We now transition from abstract properties to a powerful computational application: analyzing geometric transformations. The determinant of a transformation's Jacobian matrix, denoted $\\det(J)$, provides vital information about how the transformation locally affects area and orientation. This hands-on coding exercise [@problem_id:3141219] asks you to implement a program that numerically tests whether a 2D mapping preserves orientation by checking if $\\det(J) \\gt 0$ across a domain, offering practical experience in computational geometry and numerical analysis.", "problem": "You are given smooth functions that define mappings from the two-dimensional real plane to itself, written as $F:\\mathbb{R}^2\\rightarrow\\mathbb{R}^2$, where $F(x,y)=\\left(f_1(x,y),f_2(x,y)\\right)$. In the context of computational science, orientation preservation and local invertibility of such mappings can be analyzed through the Jacobian matrix and its determinant. The Jacobian matrix $J$ at a point $\\left(x,y\\right)$ is defined as the matrix of first-order partial derivatives of $F$ evaluated at $\\left(x,y\\right)$. The sign of $\\det J$ encodes whether the mapping preserves orientation ($\\det J>0$), reverses orientation ($\\det J<0$), or is locally singular ($\\det J=0$). Your task is to implement a robust computational test that, for a given mapping and rectangular domain, samples a uniform grid of points, approximates the entries of $J$ using finite differences, and classifies the mappingâ€™s orientation behavior everywhere on the grid.\n\nStarting point and fundamental base:\n- Use the definition of the Jacobian matrix $J$ as the matrix of partial derivatives of $F$.\n- Use well-known facts: the determinant $\\det J$ gives the signed area-scaling factor of the local linear approximation of $F$; local invertibility holds where $\\det J\\neq 0$, and orientation is preserved where $\\det J>0$ and reversed where $\\det J<0$.\n\nImplementation requirements:\n- Approximate the partial derivatives at each grid point using symmetric finite differences in the interior and one-sided differences at boundaries to maintain numerical stability.\n- Use a robust nonzero threshold $\\tau$ (a small positive real number) when classifying signs to mitigate floating-point and discretization errors. Specifically:\n  - Classify a grid point as orientation-preserving if $\\det J>\\tau$.\n  - Classify a grid point as orientation-reversing if $\\det J<-\\tau$.\n  - Classify a grid point as indeterminate/singular if $|\\det J|\\le \\tau$.\n- The mapping is classified over the entire grid by aggregating per-point classifications:\n  - Output $+1$ if all grid points are orientation-preserving.\n  - Output $-1$ if all grid points are orientation-reversing.\n  - Output $0$ otherwise (mixed signs and/or near-zero determinants occur anywhere on the grid).\n\nGrid sampling:\n- Use uniform sampling of a closed rectangular domain $\\left[x_{\\min},x_{\\max}\\right]\\times\\left[y_{\\min},y_{\\max}\\right]$ with $N_x$ points in the $x$-direction and $N_y$ points in the $y$-direction, inclusive of endpoints.\n- Let the spacings be $h_x=\\dfrac{x_{\\max}-x_{\\min}}{N_x-1}$ and $h_y=\\dfrac{y_{\\max}-y_{\\min}}{N_y-1}$.\n\nTest suite:\nImplement your program to classify the following five mappings, domains, and grid resolutions. For the rotation mapping, use angles in radians.\n- Case $1$ (identity, happy path): $F(x,y)=\\left(x,y\\right)$ on $\\left[-1,1\\right]\\times\\left[-1,1\\right]$ with $N_x=21$, $N_y=21$, and $\\tau=10^{-10}$.\n- Case $2$ (rotation by $\\pi/4$, happy path): $F(x,y)=\\left(\\cos\\left(\\frac{\\pi}{4}\\right)x-\\sin\\left(\\frac{\\pi}{4}\\right)y,\\ \\sin\\left(\\frac{\\pi}{4}\\right)x+\\cos\\left(\\frac{\\pi}{4}\\right)y\\right)$ on $\\left[-1,1\\right]\\times\\left[-1,1\\right]$ with $N_x=21$, $N_y=21$, and $\\tau=10^{-10}$.\n- Case $3$ (reflection, orientation reversing): $F(x,y)=\\left(-x,y\\right)$ on $\\left[-1,1\\right]\\times\\left[-1,1\\right]$ with $N_x=21$, $N_y=21$, and $\\tau=10^{-10}$.\n- Case $4$ (nonlinear, mixed and singular): $F(x,y)=\\left(x^2,y\\right)$ on $\\left[-1,1\\right]\\times\\left[-1,1\\right]$ with $N_x=21$, $N_y=21$, and $\\tau=10^{-10}$.\n- Case $5$ (uniform scaling with small determinant, boundary and numerical robustness): $F(x,y)=\\left(\\varepsilon x,\\varepsilon y\\right)$ with $\\varepsilon=10^{-4}$ on $\\left[-1,1\\right]\\times\\left[-1,1\\right]$ with $N_x=21$, $N_y=21$, and $\\tau=10^{-10}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the classification results for the five cases as a comma-separated list enclosed in square brackets; for example, $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5\\right]$, where each entry is one of $+1$, $-1$, or $0$ as defined above.\n- No physical units are involved, and no angles other than the specified radian angle appear. The outputs are integers.\n\nScientific realism and derivation emphasis:\n- The program must derive orientation classification from the fundamental definitions of the Jacobian and determinant and must not rely on any precomputed analytical determinant formula for the test suite.\n- The grid-based approximation must be implemented in a way that is self-consistent, numerically plausible, and robust to boundary conditions and small determinants.", "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in multivariable calculus and numerical analysis, well-posed with a clear objective and constraints, and free of contradictions or ambiguities. The problem requires the implementation of a computational method to classify the orientation-preserving nature of two-dimensional mappings by analyzing the determinant of the Jacobian matrix, which is a standard and meaningful task in computational science.\n\nThe solution proceeds by first establishing the theoretical foundation, then detailing the numerical algorithm, and finally applying it to the specified test cases.\n\n### 1. Theoretical Foundation: Jacobian Matrix and Orientation\n\nA smooth mapping $F: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2$, defined by $F(x,y) = (f_1(x,y), f_2(x,y))$, can be locally approximated by a linear transformation at any point $(x_0, y_0)$. The matrix representing this linear transformation is the Jacobian matrix, $J$, evaluated at that point. The Jacobian matrix is the matrix of all first-order partial derivatives of the vector-valued function $F$:\n\n$$\nJ(x,y) =\n\\begin{pmatrix}\n\\frac{\\partial f_1}{\\partial x}(x,y) & \\frac{\\partial f_1}{\\partial y}(x,y) \\\\\n\\frac{\\partial f_2}{\\partial x}(x,y) & \\frac{\\partial f_2}{\\partial y}(x,y)\n\\end{pmatrix}\n$$\n\nThe determinant of the Jacobian matrix, $\\det(J)$, is a scalar value that describes how the mapping scales area locally. The sign of the determinant indicates the effect of the mapping on orientation:\n- If $\\det(J) > 0$, the mapping is **orientation-preserving**. It transforms a small region without \"flipping\" it.\n- If $\\det(J) < 0$, the mapping is **orientation-reversing**. It \"flips\" the orientation of a small region, akin to a reflection.\n- If $\\det(J) = 0$, the mapping is **singular** at that point. It collapses area, and the Inverse Function Theorem does not guarantee local invertibility.\n\n### 2. Numerical Approximation of the Jacobian\n\nSince we must create a general tool without relying on analytical derivatives, we approximate the partial derivatives numerically using finite differences. We operate on a discrete grid of points $(x_i, y_j)$ spanning the domain $[x_{\\min}, x_{\\max}] \\times [y_{\\min}, y_{\\max}]$. The grid spacings are $h_x = \\frac{x_{\\max} - x_{\\min}}{N_x - 1}$ and $h_y = \\frac{y_{\\max} - y_{\\min}}{N_y - 1}$, where $N_x$ and $N_y$ are the number of points in each direction.\n\nTo maintain numerical accuracy and stability across the entire grid, we use different finite difference formulas for interior points and boundary points.\n\n**For an interior point $(x_i, y_j)$ where $0 < i < N_x-1$ and $0 < j < N_y-1$:**\nWe use the second-order accurate **symmetric (central) difference** formula. For a generic function $g(x,y)$:\n$$ \\frac{\\partial g}{\\partial x}(x_i, y_j) \\approx \\frac{g(x_i + h_x, y_j) - g(x_i - h_x, y_j)}{2h_x} $$\n$$ \\frac{\\partial g}{\\partial y}(x_i, y_j) \\approx \\frac{g(x_i, y_j + h_y) - g(x_i, y_j - h_y)}{2h_y} $$\n\n**For boundary points:**\nWe must use one-sided formulas.\n- At the left boundary ($i=0$): **Forward difference** for $\\frac{\\partial}{\\partial x}$.\n  $$ \\frac{\\partial g}{\\partial x}(x_0, y_j) \\approx \\frac{g(x_0 + h_x, y_j) - g(x_0, y_j)}{h_x} $$\n- At the right boundary ($i=N_x-1$): **Backward difference** for $\\frac{\\partial}{\\partial x}$.\n  $$ \\frac{\\partial g}{\\partial x}(x_{N_x-1}, y_j) \\approx \\frac{g(x_{N_x-1}, y_j) - g(x_{N_x-1} - h_x, y_j)}{h_x} $$\nAnalogous forward and backward difference formulas are used for $\\frac{\\partial}{\\partial y}$ at the bottom ($j=0$) and top ($j=N_y-1$) boundaries, respectively. These formulas are first-order accurate.\n\n### 3. Algorithmic Procedure\n\nThe overall algorithm to classify a given mapping $F$ on a specified domain is as follows:\n\n1.  **Grid Generation**: Define the sampling grid by creating two arrays of coordinates, one for the $x$-axis from $x_{\\min}$ to $x_{\\max}$ with $N_x$ points, and one for the $y$-axis from $y_{\\min}$ to $y_{\\max}$ with $N_y$ points. Calculate step sizes $h_x$ and $h_y$.\n\n2.  **Iterate and Classify**: Loop through each point $(x_i, y_j)$ on the grid. For each point:\n    a.  Calculate the four partial derivatives that form the Jacobian matrix: $\\frac{\\partial f_1}{\\partial x}$, $\\frac{\\partial f_1}{\\partial y}$, $\\frac{\\partial f_2}{\\partial x}$, and $\\frac{\\partial f_2}{\\partial y}$. Select the appropriate finite difference formula (central, forward, or backward) based on the point's position $(i,j)$ relative to the grid boundaries.\n    b.  Construct the Jacobian matrix $J_{ij}$ from these four approximate values.\n    c.  Compute its determinant: $\\det(J_{ij}) = \\frac{\\partial f_1}{\\partial x}\\frac{\\partial f_2}{\\partial y} - \\frac{\\partial f_1}{\\partial y}\\frac{\\partial f_2}{\\partial x}$.\n    d.  Classify the point's local behavior using the given threshold $\\tau$:\n        - If $\\det(J_{ij}) > \\tau$, the point is `orientation-preserving`.\n        - If $\\det(J_{ij}) < -\\tau$, the point is `orientation-reversing`.\n        - If $|\\det(J_{ij})| \\le \\tau$, the point is `indeterminate/singular`.\n    e.  Record the classification type found. We only need to know if we have encountered at least one of each type.\n\n3.  **Aggregate Results**: After evaluating all points on the grid, combine the individual classifications into a single result for the entire mapping over the domain:\n    a.  If all points were classified as `orientation-preserving`, the final output is $+1$.\n    b.  If all points were classified as `orientation-reversing`, the final output is $-1$.\n    c.  If there is any mix of classifications (e.g., some preserving and some reversing) or if any point was classified as `indeterminate/singular`, the final output is $0$.\n\nThis procedure is implemented for each of the five test cases provided in the problem statement. The results are then aggregated into the required final format.\n- For $F(x,y)=(x,y)$, $\\det(J) = 1$, which is greater than any small positive $\\tau$. The result is $+1$.\n- For the rotation, $\\det(J) = \\cos^2(\\frac{\\pi}{4}) + \\sin^2(\\frac{\\pi}{4}) = 1$. The result is $+1$.\n- For $F(x,y)=(-x,y)$, $\\det(J) = -1$, which is less than any small negative $-\\tau$. The result is $-1$.\n- For $F(x,y)=(x^2,y)$, $\\det(J) = 2x$. On the domain $[-1,1]\\times[-1,1]$, this determinant takes positive, negative, and zero values. Thus, it is a mixed case, resulting in $0$.\n- For $F(x,y)=(\\varepsilon x, \\varepsilon y)$ with $\\varepsilon=10^{-4}$ and $\\tau=10^{-10}$, a small value, the determinant is $\\det(J) = \\varepsilon^2 = 10^{-8}$. Since $10^{-8} > \\tau = 10^{-10}$, the mapping is consistently orientation-preserving. The result is $+1$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef classify_mapping(F, domain, grid_size, tau):\n    \"\"\"\n    Classifies a mapping's orientation behavior on a grid.\n    \n    Args:\n        F (callable): The mapping R^2 -> R^2, F(x, y) = (f1(x,y), f2(x,y)).\n        domain (tuple): A tuple (xmin, xmax, ymin, ymax) defining the rectangular domain.\n        grid_size (tuple): A tuple (Nx, Ny) with the number of grid points.\n        tau (float): The non-zero threshold for classification.\n        \n    Returns:\n        int: +1 for preserving, -1 for reversing, 0 for mixed/singular.\n    \"\"\"\n    xmin, xmax, ymin, ymax = domain\n    Nx, Ny = grid_size\n    \n    # Handle cases where grid is a single line/point. Derivatives require >1 point.\n    if Nx <= 1 or Ny <= 1:\n        # If Nx > 1 and Ny = 1, we can't compute y derivatives.\n        # As per problem, N_x, N_y are 21, so this is for robustness.\n        # Fallback to singular classification for ill-defined grids.\n        return 0\n\n    hx = (xmax - xmin) / (Nx - 1)\n    hy = (ymax - ymin) / (Ny - 1)\n    \n    x_coords = np.linspace(xmin, xmax, Nx)\n    y_coords = np.linspace(ymin, ymax, Ny)\n\n    # Use component functions for clarity\n    def F1(x, y):\n        return F(x, y)[0]\n\n    def F2(x, y):\n        return F(x, y)[1]\n\n    found_preserving = False\n    found_reversing = False\n    found_singular = False\n    \n    for i in range(Nx):\n        for j in range(Ny):\n            x = x_coords[i]\n            y = y_coords[j]\n            \n            # Calculate partial derivatives using appropriate finite difference schemes\n            \n            # Partial derivative with respect to x\n            if i == 0:  # Forward difference at left boundary\n                df1_dx = (F1(x + hx, y) - F1(x, y)) / hx\n                df2_dx = (F2(x + hx, y) - F2(x, y)) / hx\n            elif i == Nx - 1:  # Backward difference at right boundary\n                df1_dx = (F1(x, y) - F1(x - hx, y)) / hx\n                df2_dx = (F2(x, y) - F2(x - hx, y)) / hx\n            else:  # Central difference for interior\n                df1_dx = (F1(x + hx, y) - F1(x - hx, y)) / (2 * hx)\n                df2_dx = (F2(x + hx, y) - F2(x - hx, y)) / (2 * hx)\n            \n            # Partial derivative with respect to y\n            if j == 0:  # Forward difference at bottom boundary\n                df1_dy = (F1(x, y + hy) - F1(x, y)) / hy\n                df2_dy = (F2(x, y + hy) - F2(x, y)) / hy\n            elif j == Ny - 1:  # Backward difference at top boundary\n                df1_dy = (F1(x, y) - F1(x, y - hy)) / hy\n                df2_dy = (F2(x, y) - F2(x, y - hy)) / hy\n            else:  # Central difference for interior\n                df1_dy = (F1(x, y + hy) - F1(x, y - hy)) / (2 * hy)\n                df2_dy = (F2(x, y + hy) - F2(x, y - hy)) / (2 * hy)\n\n            # Calculate Jacobian determinant\n            det_J = df1_dx * df2_dy - df1_dy * df2_dx\n            \n            # Classify the point and update flags\n            if det_J > tau:\n                found_preserving = True\n            elif det_J < -tau:\n                found_reversing = True\n            else:\n                found_singular = True\n            \n            # Optimization: if mixed behavior is found, we know the result is 0\n            if found_singular or (found_preserving and found_reversing):\n                return 0\n    \n    # Aggregate results after checking all points\n    # The optimization above means we only reach here if all points are of one type.\n    if found_preserving and not found_reversing and not found_singular:\n        return 1\n    elif found_reversing and not found_preserving and not found_singular:\n        return -1\n    else:\n        # This branch handles any cases missed by the loop optimization,\n        # e.g., if the grid was only singular points.\n        return 0\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define test cases\n    theta = np.pi / 4\n    c, s = np.cos(theta), np.sin(theta)\n    epsilon = 1e-4\n\n    test_cases = [\n        {\n            \"F\": lambda x, y: (x, y),\n            \"domain\": (-1.0, 1.0, -1.0, 1.0),\n            \"grid_size\": (21, 21),\n            \"tau\": 1e-10\n        },\n        {\n            \"F\": lambda x, y: (c * x - s * y, s * x + c * y),\n            \"domain\": (-1.0, 1.0, -1.0, 1.0),\n            \"grid_size\": (21, 21),\n            \"tau\": 1e-10\n        },\n        {\n            \"F\": lambda x, y: (-x, y),\n            \"domain\": (-1.0, 1.0, -1.0, 1.0),\n            \"grid_size\": (21, 21),\n            \"tau\": 1e-10\n        },\n        {\n            \"F\": lambda x, y: (x**2, y),\n            \"domain\": (-1.0, 1.0, -1.0, 1.0),\n            \"grid_size\": (21, 21),\n            \"tau\": 1e-10\n        },\n        {\n            \"F\": lambda x, y: (epsilon * x, epsilon * y),\n            \"domain\": (-1.0, 1.0, -1.0, 1.0),\n            \"grid_size\": (21, 21),\n            \"tau\": 1e-10\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = classify_mapping(**case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3141219"}]}