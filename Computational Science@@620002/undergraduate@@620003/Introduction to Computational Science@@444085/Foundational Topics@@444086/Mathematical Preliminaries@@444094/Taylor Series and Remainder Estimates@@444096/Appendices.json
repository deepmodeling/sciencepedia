{"hands_on_practices": [{"introduction": "Many important functions in science and engineering, such as the error function $\\operatorname{erf}(x)$ used in probability and heat transfer, do not have simple closed-form expressions. To work with them computationally, we must rely on approximations. This exercise demonstrates how to use a Maclaurin series to create a highly accurate polynomial approximation of $\\operatorname{erf}(x)$, giving you hands-on practice with the Alternating Series Remainder Theorem to determine exactly how many terms are needed to meet a strict error tolerance [@problem_id:2442184].", "problem": "In computational engineering, the error function (erf) is defined for real input by\n$$\\operatorname{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{x} \\exp\\!\\left(-t^{2}\\right)\\, dt.$$\nFor the Maclaurin series of $\\operatorname{erf}(x)$, consider the truncation\n$$S_{N}(x) = \\sum_{n=0}^{N} c_{n}\\, x^{2n+1},$$\nwhere $c_{n}$ are the coefficients obtained from the Maclaurin expansion of $\\operatorname{erf}(x)$ about $x=0$. Determine the smallest integer $N$ such that truncating at $S_{N}(x)$ guarantees that the absolute approximation error for $x=0$ to $x=0.5$ satisfies\n$$\\left|\\operatorname{erf}(0.5) - S_{N}(0.5)\\right| < 1.0 \\times 10^{-8}.$$\nProvide only the integer $N$ as your final answer.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- The error function is defined as $\\operatorname{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{x} \\exp(-t^{2})\\, dt$.\n- The truncated Maclaurin series is $S_{N}(x) = \\sum_{n=0}^{N} c_{n}\\, x^{2n+1}$.\n- The point of evaluation is $x=0.5$.\n- The required absolute error tolerance is $|\\operatorname{erf}(0.5) - S_{N}(0.5)| < 1.0 \\times 10^{-8}$.\n- The objective is to find the smallest integer $N$ that satisfies this condition.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective.\n- **Scientific Grounding**: The error function, its series expansion, and the analysis of truncation error are standard topics in mathematical physics, numerical analysis, and computational engineering. All definitions are standard and correct.\n- **Well-Posedness**: The problem asks for the smallest integer $N$ satisfying a specific inequality. The nature of the series ensures that such an integer exists and is unique.\n- **Objectivity**: The problem is stated using precise mathematical language, free from ambiguity or subjective content.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A complete solution will be developed.\n\nThe solution process begins by determining the Maclaurin series for $\\operatorname{erf}(x)$. The process starts with the known Maclaurin series for the exponential function:\n$$ \\exp(u) = \\sum_{k=0}^{\\infty} \\frac{u^{k}}{k!} $$\nBy substituting $u = -t^2$, we obtain the series for the integrand:\n$$ \\exp(-t^2) = \\sum_{k=0}^{\\infty} \\frac{(-t^2)^{k}}{k!} = \\sum_{k=0}^{\\infty} \\frac{(-1)^k t^{2k}}{k!} $$\nThis series has an infinite radius of convergence, which permits term-by-term integration over any finite interval. Integrating from $t=0$ to $t=x$:\n$$ \\int_{0}^{x} \\exp(-t^{2})\\, dt = \\int_{0}^{x} \\left( \\sum_{k=0}^{\\infty} \\frac{(-1)^k t^{2k}}{k!} \\right) dt = \\sum_{k=0}^{\\infty} \\frac{(-1)^k}{k!} \\int_{0}^{x} t^{2k}\\, dt $$\n$$ = \\sum_{k=0}^{\\infty} \\frac{(-1)^k}{k!} \\left[ \\frac{t^{2k+1}}{2k+1} \\right]_{0}^{x} = \\sum_{k=0}^{\\infty} \\frac{(-1)^k x^{2k+1}}{k!(2k+1)} $$\nMultiplying by the constant factor $\\frac{2}{\\sqrt{\\pi}}$ from the definition of $\\operatorname{erf}(x)$ gives its Maclaurin series. To match the problem's notation, we use the index $n$ instead of $k$:\n$$ \\operatorname{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\sum_{n=0}^{\\infty} \\frac{(-1)^n x^{2n+1}}{n!(2n+1)} $$\nThe truncated series is given as $S_N(x) = \\sum_{n=0}^{N} c_n x^{2n+1}$, where the coefficients are $c_n = \\frac{2}{\\sqrt{\\pi}} \\frac{(-1)^n}{n!(2n+1)}$. The approximation error is the remainder term, $R_N(x) = \\operatorname{erf}(x) - S_N(x)$.\n$$ R_N(x) = \\frac{2}{\\sqrt{\\pi}} \\sum_{n=N+1}^{\\infty} \\frac{(-1)^n x^{2n+1}}{n!(2n+1)} $$\nFor any fixed $x > 0$, the Maclaurin series for $\\operatorname{erf}(x)$ is an alternating series. To apply the Alternating Series Remainder Theorem, we must verify that for $x=0.5$, the absolute values of the terms are monotonically decreasing to zero. Let $a_n = \\frac{2}{\\sqrt{\\pi}} \\frac{x^{2n+1}}{n!(2n+1)}$. The ratio of the magnitudes of consecutive terms is:\n$$ \\frac{a_{n+1}}{a_n} = \\frac{x^{2(n+1)+1}}{(n+1)!(2(n+1)+1)} \\cdot \\frac{n!(2n+1)}{x^{2n+1}} = \\frac{x^2 (2n+1)}{(n+1)(2n+3)} $$\nFor $x=0.5$, this ratio becomes:\n$$ \\frac{(0.5)^2 (2n+1)}{(n+1)(2n+3)} = \\frac{0.25(2n+1)}{2n^2+5n+3} $$\nFor all $n \\ge 0$, the denominator $2n^2+5n+3$ is strictly greater than the numerator $0.5n+0.25$. Thus, the ratio is always less than $1$, confirming that the terms are monotonically decreasing in magnitude. The limit of the terms as $n \\to \\infty$ is zero due to the factorial in the denominator.\n\nThe Alternating Series Remainder Theorem states that the absolute error $|R_N(x)|$ is less than the absolute value of the first omitted term, which corresponds to $n=N+1$.\n$$ |\\operatorname{erf}(0.5) - S_N(0.5)| = |R_N(0.5)| \\le \\left| c_{N+1} (0.5)^{2(N+1)+1} \\right| $$\n$$ |R_N(0.5)| \\le \\frac{2}{\\sqrt{\\pi}} \\frac{(0.5)^{2N+3}}{(N+1)!(2N+3)} $$\nWe require this error bound to be less than $1.0 \\times 10^{-8}$:\n$$ \\frac{2}{\\sqrt{\\pi}} \\frac{1}{2^{2N+3}(N+1)!(2N+3)} < 10^{-8} $$\nThis simplifies to:\n$$ \\frac{1}{\\sqrt{\\pi} \\cdot 2^{2N+2}(N+1)!(2N+3)} < 10^{-8} $$\nWe must find the smallest integer $N$ that satisfies this inequality. We test values of $N$.\nFor $N=5$:\nThe error bound is $\\frac{1}{\\sqrt{\\pi} \\cdot 2^{12} \\cdot 6! \\cdot (13)} = \\frac{1}{\\sqrt{\\pi} \\cdot 4096 \\cdot 720 \\cdot 13} = \\frac{1}{\\sqrt{\\pi} \\cdot 38338560}$.\nUsing $\\sqrt{\\pi} \\approx 1.77245$, the denominator is approximately $1.77245 \\times 38338560 \\approx 6.795 \\times 10^7$.\nThe error bound is approximately $\\frac{1}{6.795 \\times 10^7} \\approx 1.47 \\times 10^{-8}$.\nSince $1.47 \\times 10^{-8} > 1.0 \\times 10^{-8}$, $N=5$ is not sufficient.\n\nFor $N=6$:\nThe error bound is $\\frac{1}{\\sqrt{\\pi} \\cdot 2^{14} \\cdot 7! \\cdot (15)} = \\frac{1}{\\sqrt{\\pi} \\cdot 16384 \\cdot 5040 \\cdot 15} = \\frac{1}{\\sqrt{\\pi} \\cdot 1238630400}$.\nUsing $\\sqrt{\\pi} \\approx 1.77245$, the denominator is approximately $1.77245 \\times 1238630400 \\approx 2.196 \\times 10^9$.\nThe error bound is approximately $\\frac{1}{2.196 \\times 10^9} \\approx 4.55 \\times 10^{-10}$.\nSince $4.55 \\times 10^{-10} < 1.0 \\times 10^{-8}$, $N=6$ is sufficient.\n\nTherefore, the smallest integer value for $N$ that guarantees the required accuracy is $6$.", "answer": "$$ \\boxed{6} $$", "id": "2442184"}, {"introduction": "While Taylor series are powerful approximation tools, their true value in computational science often lies in resolving numerical instabilities. A classic example is the sinc function, $f(x) = \\sin(x)/x$, which presents a 'divide by zero' issue at $x=0$ despite having a well-defined limit. This practice challenges you to analyze how a Maclaurin polynomial can replace the original expression near this removable singularity, ensuring both mathematical rigor and guaranteed accuracy in a finite-precision environment [@problem_id:3200326].", "problem": "A computational scientist needs a reliable way to evaluate the function $f(x) = \\sin x / x$ for small $x$ in finite-precision arithmetic. The direct expression $f(x) = \\sin x / x$ is undefined at $x=0$, yet the function exhibits a removable singularity there. The scientist decides to use a Maclaurin expansion near $x=0$ and a rigorous remainder bound to guarantee a specified accuracy on the interval $|x| \\le 0.1$. The target is an absolute error at most $10^{-8}$.\n\nAs foundational starting points, recall the following widely accepted definitions from calculus: (i) the Maclaurin polynomial of degree $N$ for a function $g$ is $P_N(x) = \\sum_{k=0}^{N} \\frac{g^{(k)}(0)}{k!} x^k$, and (ii) the Lagrange form of the remainder is $R_N(x) = \\frac{g^{(N+1)}(\\xi)}{(N+1)!} x^{N+1}$ for some $\\xi$ between $0$ and $x$ whenever $g$ is $(N+1)$ times differentiable on the interval between $0$ and $x$. Also recall that a singularity is removable when $\\lim_{x \\to x_0} g(x)$ exists and one defines $g(x_0)$ to equal that limit, producing a continuous extension.\n\nSelect all options that correctly ensure both mathematically sound handling of the removable singularity and a guaranteed absolute error at most $10^{-8}$ for all $|x| \\le 0.1$.\n\nA. Define $f(0)$ to be the limit value $1$. For $|x| \\le 0.1$, approximate with the polynomial $p_4(x) = 1 - \\frac{x^2}{6} + \\frac{x^4}{120}$ and bound the truncation error by the magnitude of the first omitted term of the alternating series, namely $\\frac{|x|^6}{5040}$, which is at most $2 \\cdot 10^{-10}$ on $|x| \\le 0.1$, hence below $10^{-8}$.\n\nB. Define $f(0) = 1$. For $|x| \\le 0.1$, approximate with the polynomial $p_4(x) = 1 - \\frac{x^2}{6} + \\frac{x^4}{120}$ and bound the truncation error by the Lagrange remainder of the sine Maclaurin polynomial truncated at degree $5$, namely $\\frac{|x|^5}{720}$, concluding this guarantees an absolute error at most $10^{-8}$ on $|x| \\le 0.1$.\n\nC. Use the original expression $f(x) = \\sin x / x$ for all $x$ without redefining the value at $x=0$, relying on floating-point arithmetic to produce the correct limiting value $1$ by cancellation at $x=0$.\n\nD. Define $f(0) = 1$. For $|x| \\le 0.1$, approximate with $p_2(x) = 1 - \\frac{x^2}{6}$ and bound the truncation error by $\\frac{|x|^3}{6}$, concluding this meets the $10^{-8}$ accuracy target on $|x| \\le 0.1$.\n\nE. Define $f(0) = 1$. For $|x| \\le 0.1$, approximate with $p_6(x) = 1 - \\frac{x^2}{6} + \\frac{x^4}{120} - \\frac{x^6}{5040}$ and bound the truncation error using the Lagrange form of the remainder for the sine Maclaurin polynomial truncated at degree $7$, yielding $\\frac{|x|^7}{40320}$, which is at most $3 \\cdot 10^{-12}$ on $|x| \\le 0.1$, hence below $10^{-8}$.", "solution": "First, we address the removable singularity of $f(x) = \\frac{\\sin x}{x}$ at $x=0$. Using L'Hôpital's rule or the definition of the derivative, we find the limit:\n$$ \\lim_{x \\to 0} f(x) = \\lim_{x \\to 0} \\frac{\\sin x}{x} = \\lim_{x \\to 0} \\frac{\\cos x}{1} = 1 $$\nTherefore, the continuous extension of the function is properly defined as $f(0)=1$. Any valid method must begin with this step.\n\nNext, we derive the Maclaurin series for $f(x)$. We start with the well-known series for $\\sin x$:\n$$ \\sin x = \\sum_{k=0}^{\\infty} \\frac{(-1)^k}{(2k+1)!} x^{2k+1} = x - \\frac{x^3}{3!} + \\frac{x^5}{5!} - \\frac{x^7}{7!} + \\dots $$\nFor $x \\neq 0$, we can divide by $x$:\n$$ f(x) = \\frac{\\sin x}{x} = \\sum_{k=0}^{\\infty} \\frac{(-1)^k}{(2k+1)!} x^{2k} = 1 - \\frac{x^2}{3!} + \\frac{x^4}{5!} - \\frac{x^6}{7!} + \\dots $$\nThis series also converges to $1$ at $x=0$, so it represents the continuous function on the entire interval. Let $p_{2n}(x)$ be the Maclaurin polynomial of degree $2n$ for $f(x)$, which is obtained by truncating this series. The problem requires finding a polynomial that guarantees an absolute error $|f(x) - p_{2n}(x)| \\le 10^{-8}$ for all $x$ such that $|x| \\le 0.1$.\n\nThe error, or remainder term, can be bounded. Since the series for a fixed $x$ is an alternating series with terms that decrease in magnitude, the absolute error is bounded by the magnitude of the first omitted term (Alternating Series Estimation Theorem).\n$$ |R_{2n}(x)| = |f(x) - p_{2n}(x)| \\le \\frac{|x|^{2n+2}}{(2n+3)!} $$\nWe seek the smallest integer $n$ such that this bound is $\\le 10^{-8}$ for $|x| \\le 0.1$.\n$$ \\frac{(0.1)^{2n+2}}{(2n+3)!} \\le 10^{-8} $$\nLet's test values of $n$:\n- For $n=0$, $p_0(x) = 1$. Error $\\le \\frac{(0.1)^2}{3!} = \\frac{0.01}{6} \\approx 1.67 \\times 10^{-3}$. Not sufficient.\n- For $n=1$, $p_2(x) = 1 - \\frac{x^2}{6}$. Error $\\le \\frac{(0.1)^4}{5!} = \\frac{10^{-4}}{120} \\approx 8.33 \\times 10^{-7}$. Not sufficient.\n- For $n=2$, $p_4(x) = 1 - \\frac{x^2}{6} + \\frac{x^4}{120}$. Error $\\le \\frac{(0.1)^6}{7!} = \\frac{10^{-6}}{5040} \\approx 1.984 \\times 10^{-10}$. This is less than $10^{-8}$, so this is sufficient.\n- For $n=3$, $p_6(x) = 1 - \\frac{x^2}{6} + \\frac{x^4}{120} - \\frac{x^6}{5040}$. Error $\\le \\frac{(0.1)^8}{9!} = \\frac{10^{-8}}{362880} \\approx 2.75 \\times 10^{-14}$. This is also sufficient.\n\nNow we evaluate the given options.\n\nA. **Define $f(0)$ to be the limit value $1$. For $|x| \\le 0.1$, approximate with the polynomial $p_4(x) = 1 - \\frac{x^2}{6} + \\frac{x^4}{120}$ and bound the truncation error by the magnitude of the first omitted term of the alternating series, namely $\\frac{|x|^6}{5040}$, which is at most $2 \\cdot 10^{-10}$ on $|x| \\le 0.1$, hence below $10^{-8}$.**\nThis option correctly defines $f(0)=1$. It uses the polynomial $p_4(x)$ which we found to be sufficient. The error analysis uses the Alternating Series Estimation Theorem, which is a rigorous and applicable method here. The first omitted term is indeed $-\\frac{x^6}{7!} = -\\frac{x^6}{5040}$. The bound on the error's magnitude is $\\frac{|x|^6}{5040}$. For $|x| \\le 0.1$, the maximum error is $\\frac{(0.1)^6}{5040} = \\frac{10^{-6}}{5040} \\approx 1.984 \\times 10^{-10}$. Stating this is at most $2 \\cdot 10^{-10}$ is correct. Since $2 \\cdot 10^{-10} < 10^{-8}$, the conclusion is correct. Every part of this statement is mathematically sound.\nVerdict: **Correct**.\n\nB. **Define $f(0) = 1$. For $|x| \\le 0.1$, approximate with the polynomial $p_4(x) = 1 - \\frac{x^2}{6} + \\frac{x^4}{120}$ and bound the truncation error by the Lagrange remainder of the sine Maclaurin polynomial truncated at degree $5$, namely $\\frac{|x|^5}{720}$, concluding this guarantees an absolute error at most $10^{-8}$ on $|x| \\le 0.1$.**\nThis option uses the correct polynomial $p_4(x)$. However, the error analysis is flawed. The truncation error for $p_4(x)$ is of order $x^6$, not $x^5$. The error bound for approximating $f(x)$ with $p_4(x)$ is $\\frac{|x|^6}{5040}$, as shown in A. The stated bound $\\frac{|x|^5}{720}$ is dimensionally incorrect (wrong power of $x$) and stems from a misunderstanding of how the remainder terms relate. The error in approximating $\\sin(x)$ with its degree $5$ polynomial $P_5(x)$ is $R_5(x) = \\frac{\\sin^{(6)}(\\xi)}{6!}x^6$, with a magnitude bounded by $\\frac{|x|^6}{6!} = \\frac{|x|^6}{720}$. The remainder for $f(x)$ would relate to this, modified by a division by $x$, but the option misstates the remainder as $\\frac{|x|^5}{720}$. The entire error analysis is fundamentally incorrect.\nVerdict: **Incorrect**.\n\nC. **Use the original expression $f(x) = \\sin x / x$ for all $x$ without redefining the value at $x=0$, relying on floating-point arithmetic to produce the correct limiting value $1$ by cancellation at $x=0$.**\nThis approach is computationally and mathematically unsound. At $x=0$, the expression is a $0/0$ division, which results in an error or `NaN` in standard floating-point systems. It is not a \"reliable\" or \"guaranteed\" method. The goal of the problem is specifically to find a robust alternative to direct evaluation near $x=0$. This option does the opposite.\nVerdict: **Incorrect**.\n\nD. **Define $f(0) = 1$. For $|x| \\le 0.1$, approximate with $p_2(x) = 1 - \\frac{x^2}{6}$ and bound the truncation error by $\\frac{|x|^3}{6}$, concluding this meets the $10^{-8}$ accuracy target on $|x| \\le 0.1$.**\nThis option uses $p_2(x)$. The error for this approximation is dominated by the $\\frac{x^4}{120}$ term. The stated error bound of $\\frac{|x|^3}{6}$ is incorrect; it is the magnitude of the next term in the *sine* series, not the $f(x)$ series. Furthermore, as calculated in our preliminary analysis, the actual maximum error for $p_2(x)$ on the interval is approximately $8.33 \\times 10^{-7}$, which does not meet the $10^{-8}$ target. The option is incorrect on two counts: the error bound formula and the final conclusion about accuracy.\nVerdict: **Incorrect**.\n\nE. **Define $f(0) = 1$. For $|x| \\le 0.1$, approximate with $p_6(x) = 1 - \\frac{x^2}{6} + \\frac{x^4}{120} - \\frac{x^6}{5040}$ and bound the truncation error using the Lagrange form of the remainder for the sine Maclaurin polynomial truncated at degree $7$, yielding $\\frac{|x|^7}{40320}$, which is at most $3 \\cdot 10^{-12}$ on $|x| \\le 0.1$, hence below $10^{-8}$.**\nThis option correctly defines $f(0)=1$ and uses the polynomial $p_6(x)$. Let's analyze the error bound derivation. The polynomial $p_6(x)$ can be written as $p_6(x) = \\frac{1}{x} P_7^{\\sin}(x)$, where $P_7^{\\sin}(x)$ is the degree-$7$ Maclaurin polynomial for $\\sin x$. The error is $f(x) - p_6(x) = \\frac{\\sin x}{x} - \\frac{P_7^{\\sin}(x)}{x} = \\frac{R_7^{\\sin}(x)}{x}$, where $R_7^{\\sin}(x)$ is the Lagrange remainder for the sine series. The remainder is $R_7^{\\sin}(x) = \\frac{\\sin^{(8)}(\\xi)}{8!}x^8 = \\frac{\\sin \\xi}{8!}x^8$ for some $\\xi$ between $0$ and $x$. The error for $f(x)$ is thus $\\frac{\\sin \\xi}{8!}x^7$. To find a guaranteed bound, we can use the fact that $|\\sin \\xi| \\le 1$. This gives $|f(x) - p_6(x)| \\le \\frac{|x|^7}{8!} = \\frac{|x|^7}{40320}$. While this is not the tightest possible bound (a tighter bound is $\\frac{|x|^8}{9!}$), it is a mathematically valid and rigorous bound. The calculation follows: the maximum value of this bound on $|x| \\le 0.1$ is $\\frac{(0.1)^7}{40320} = \\frac{10^{-7}}{40320} \\approx 2.48 \\times 10^{-12}$. The statement that this is at most $3 \\cdot 10^{-12}$ is correct. This value is well below the $10^{-8}$ target. The entire chain of reasoning is sound.\nVerdict: **Correct**.\n\nFinal Conclusion: Both options A and E describe mathematically sound procedures that guarantee the required accuracy.", "answer": "$$\\boxed{AE}$$", "id": "3200326"}, {"introduction": "A Taylor series provides an excellent approximation *near* its center of expansion, but what happens when we need to evaluate a function far from that point? Direct evaluation of a Maclaurin series for large inputs is a recipe for numerical disaster, plagued by slow convergence and catastrophic cancellation. This capstone exercise guides you through the process of building a robust and efficient sine function by combining a local Maclaurin series approximation with a technique called argument reduction, a fundamental strategy used in professional scientific computing libraries [@problem_id:2442233].", "problem": "You are to write a complete, runnable program that evaluates the sine function using a Maclaurin series with rigorously controlled truncation error, and that analyzes the numerical stability of this approach for large arguments. Work entirely in radians.\n\nStart from the following fundamental base:\n- The definition of the Maclaurin series for analytic functions and the Lagrange form of the remainder. For a sufficiently differentiable function $f$, its Maclaurin polynomial of degree $m$ at $x=0$ has a remainder $R_{m}(x) = \\dfrac{f^{(m+1)}(\\xi)}{(m+1)!} x^{m+1}$ for some $\\xi$ between $0$ and $x$.\n- The periodicity and parity identities for trigonometric functions: $ \\sin(x + 2\\pi k) = \\sin(x)$ for any integer $k$, and the angle-sum and quadrant symmetries that relate $ \\sin(x)$ to $ \\sin(r)$ or $ \\cos(r)$ for a small residual $r$ after reduction by integer multiples of $ \\dfrac{\\pi}{2}$.\n\nYour tasks:\n1) Derive, from the Maclaurin definition and the Lagrange remainder formula, a stopping criterion for truncating the Maclaurin series of $ \\sin(x)$ and $ \\cos(x)$ to achieve a user-prescribed absolute error tolerance $ \\varepsilon > 0$. In particular, justify bounds of the form\n$$\n\\left| R_{2N+1}^{\\sin}(x) \\right| \\le \\frac{|x|^{2N+3}}{(2N+3)!}, \n\\qquad\n\\left| R_{2N}^{\\cos}(x) \\right| \\le \\frac{|x|^{2N+2}}{(2N+2)!},\n$$\nby using that the absolute value of any derivative of $ \\sin(x)$ or $ \\cos(x)$ is at most $1$ on the real line. Then formulate a procedure that, given a small argument $z$, finds the smallest integer $N \\ge 0$ such that the corresponding remainder bound is less than or equal to $ \\varepsilon$.\n\n2) Devise and implement an argument reduction strategy that, given any real $x$, uses integer multiples of $ \\dfrac{\\pi}{2}$ to map $x$ to a residual $r$ in the interval $[-\\dfrac{\\pi}{4}, \\dfrac{\\pi}{4}]$ and a quadrant index $q \\in \\{0,1,2,3\\}$ such that\n$$\n\\sin(x) = \n\\begin{cases}\n\\phantom{-}\\sin(r), & q \\equiv 0 \\ (\\text{mod } 4),\\\\\n\\phantom{-}\\cos(r), & q \\equiv 1 \\ (\\text{mod } 4),\\\\\n-\\sin(r), & q \\equiv 2 \\ (\\text{mod } 4),\\\\\n-\\cos(r), & q \\equiv 3 \\ (\\text{mod } 4).\n\\end{cases}\n$$\nUse this mapping to select which Maclaurin series (sine or cosine) to evaluate at the small residual $r$. Use the stopping criterion from Task $1$ to determine the minimal number of series terms needed to guarantee that the truncation error does not exceed $ \\varepsilon$.\n\n3) Explain, from first principles, why the direct Maclaurin evaluation at a large $|x|$ is numerically unstable: analyze how the magnitude of intermediate terms behaves before factorial growth dominates and how finite-precision arithmetic exacerbates cancellation. Then contrast this with the stability of the reduced-argument strategy, and also explain the limitation that for extremely large $|x|$ the floating-point reduction $x \\mapsto r$ can itself lose accuracy because of limited mantissa bits.\n\n4) Implement the following programmatic outputs for a test suite of inputs. For each test case parameter pair $(x, \\varepsilon)$:\n- Compute $s_{\\text{approx}}$ using your argument-reduced Maclaurin method.\n- Compute a boolean $b_{\\text{ok}}$ that is true if and only if $|s_{\\text{approx}} - \\sin(x)| \\le \\varepsilon$, where $ \\sin(x)$ on the right-hand side is evaluated using a high-quality library function as a reference.\n- Report the minimal number of nonzero Maclaurin terms actually summed for the chosen series, denoted $T$ (for $ \\sin$ this is $T = N+1$ corresponding to degrees $1,3,\\dots,2N+1$, and for $ \\cos$ this is $T = N+1$ corresponding to degrees $0,2,\\dots,2N$).\n- Assess whether a naive, unreduced Maclaurin series for $ \\sin(x)$ is practically unusable under the same tolerance by checking if, before the remainder bound falls below $ \\varepsilon$, one must exceed a cap of $T_{\\max} = 1000$ nonzero terms, or if intermediate remainder terms overflow to non-finite values. If so, record a boolean $b_{\\text{naive\\_impractical}}$ as true, otherwise false.\n\nAngle unit: radians. There are no physical units. All percentages, if any, must be expressed as decimals.\n\nTest suite:\n- Case $1$: $(x, \\varepsilon) = (100.0, 10^{-12})$.\n- Case $2$: $(x, \\varepsilon) = (10^{6} + 0.1, 10^{-12})$.\n- Case $3$: $(x, \\varepsilon) = (10^{16} + 0.1, 10^{-12})$.\n- Case $4$: $(x, \\varepsilon) = \\left(\\dfrac{\\pi}{2} + 10^{-8}, 10^{-15}\\right)$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For the $k$-th test case in the order above, output the triple $[b_{\\text{ok}}, T, b_{\\text{naive\\_impractical}}]$ and flatten these triples into a single list in test-case order. For example, the final line should look like\n$[b_{\\text{ok},1}, T_{1}, b_{\\text{naive\\_impractical},1}, b_{\\text{ok},2}, T_{2}, b_{\\text{naive\\_impractical},2}, b_{\\text{ok},3}, T_{3}, b_{\\text{naive\\_impractical},3}, b_{\\text{ok},4}, T_{4}, b_{\\text{naive\\_impractical},4}]$.", "solution": "The problem as stated is well-defined, internally consistent, and grounded in the fundamental principles of numerical analysis and calculus. It is neither ambiguous nor scientifically unsound. We shall therefore proceed with a rigorous derivation and implementation as requested.\n\nThe task is to implement a numerically stable method for evaluating the sine function, $\\sin(x)$, using its Maclaurin series expansion, and to analyze its performance against a naive, direct evaluation. The solution is structured into three principal parts: the derivation of a series truncation criterion, the development of an argument reduction strategy, and an analysis of numerical stability.\n\n**1. Derivation of the Truncation Criterion**\n\nThe Maclaurin series for an analytic function $f(x)$ expanded around $x=0$ is given by $f(x) = \\sum_{k=0}^{\\infty} \\frac{f^{(k)}(0)}{k!} x^k$. When this series is truncated after the term of degree $m$, the result is the Maclaurin polynomial $P_m(x)$, and the error is the remainder term $R_m(x)$. According to the Lagrange form of the remainder, $R_m(x) = \\frac{f^{(m+1)}(\\xi)}{(m+1)!} x^{m+1}$ for some $\\xi$ between $0$ and $x$.\n\nFor $f(x) = \\sin(x)$, the derivatives are cyclical: $\\sin'(x) = \\cos(x)$, $\\sin''(x) = -\\sin(x)$, and so on. Critically, for any integer $k \\ge 0$, the $k$-th derivative satisfies $|f^{(k)}(x)| \\le 1$ for all real $x$. The Maclaurin series for $\\sin(x)$ contains only odd powers of $x$:\n$$\n\\sin(x) = \\sum_{k=0}^{N} \\frac{(-1)^k}{(2k+1)!} x^{2k+1} + R_{2N+1}(x)\n$$\nHere, we have truncated the series after the term corresponding to $k=N$, which has degree $2N+1$. The polynomial part, $P_{2N+1}(x)$, is identical to $P_{2N+2}(x)$ because the coefficient of $x^{2N+2}$ is zero. The remainder is therefore $R_{2N+2}(x)$, given by:\n$$\nR_{2N+2}(x) = \\frac{f^{(2N+3)}(\\xi)}{(2N+3)!} x^{2N+3}\n$$\nwhere $f^{(2N+3)}(x)$ is either $\\pm\\sin(x)$ or $\\pm\\cos(x)$. Using the bound $|f^{(2N+3)}(\\xi)| \\le 1$, we establish the error bound for the sine series truncated at degree $2N+1$:\n$$\n\\left| R_{2N+1}^{\\sin}(x) \\right| = \\left| R_{2N+2}^{\\sin}(x) \\right| \\le \\frac{|x|^{2N+3}}{(2N+3)!}\n$$\nThis confirms the inequality given in the problem statement.\n\nFor $f(x) = \\cos(x)$, the Maclaurin series contains only even powers of $x$:\n$$\n\\cos(x) = \\sum_{k=0}^{N} \\frac{(-1)^k}{(2k)!} x^{2k} + R_{2N}(x)\n$$\nThe polynomial $P_{2N}(x)$ is identical to $P_{2N+1}(x)$. The remainder is $R_{2N+1}(x)$:\n$$\nR_{2N+1}(x) = \\frac{f^{(2N+2)}(\\xi)}{(2N+2)!} x^{2N+2}\n$$\nAgain, using the bound $|f^{(2N+2)}(\\xi)| \\le 1$, we obtain the error bound for the cosine series truncated at degree $2N$:\n$$\n\\left| R_{2N}^{\\cos}(x) \\right| = \\left| R_{2N+1}^{\\cos}(x) \\right| \\le \\frac{|x|^{2N+2}}{(2N+2)!}\n$$\nThis also confirms the problem's formulation.\n\nTo satisfy a prescribed absolute error tolerance $\\varepsilon > 0$ for a small argument $z$, we must find the smallest non-negative integer $N$ such that the error bound does not exceed $\\varepsilon$.\nFor $\\sin(z)$, we must find the minimum $N \\ge 0$ such that $\\frac{|z|^{2N+3}}{(2N+3)!} \\le \\varepsilon$.\nFor $\\cos(z)$, we must find the minimum $N \\ge 0$ such that $\\frac{|z|^{2N+2}}{(2N+2)!} \\le \\varepsilon$.\nThis requires an iterative search, starting from $N=0$ and incrementing $N$ until the condition is met. The number of non-zero terms to be summed is then $T = N+1$.\n\n**2. Argument Reduction Strategy**\n\nDirect evaluation of the Maclaurin series is inefficient and numerically unstable for large $|x|$. A standard and robust technique is argument reduction, which leverages the periodic properties of trigonometric functions. Any real number $x$ can be expressed as $x = q \\cdot \\frac{\\pi}{2} + r$, where $q$ is an integer and $r$ is a small residual. We choose $q$ to be the integer nearest to the value of $x / (\\pi/2)$, which ensures that the residual $r$ lies in the interval $[-\\frac{\\pi}{4}, \\frac{\\pi}{4}]$.\nThe procedure is as follows:\n1. Compute $y = x / (\\pi/2)$.\n2. Find the nearest integer $q = \\text{round}(y)$.\n3. Calculate the residual $r = x - q \\cdot (\\pi/2)$. By construction, $|r| \\le \\frac_1{2} \\cdot \\frac{\\pi}{2} = \\frac{\\pi}{4}$.\n\nThe value of $\\sin(x)$ is then related to either $\\sin(r)$ or $\\cos(r)$ based on the value of $q$ modulo $4$. Let $q_{\\text{mod} 4} = q \\pmod 4$. We use the angle-sum identities:\n- If $q_{\\text{mod} 4} = 0$: $\\sin(x) = \\sin(4k \\cdot \\frac{\\pi}{2} + r) = \\sin(2k\\pi + r) = \\sin(r)$.\n- If $q_{\\text{mod} 4} = 1$: $\\sin(x) = \\sin((4k+1) \\cdot \\frac{\\pi}{2} + r) = \\sin(2k\\pi + \\frac{\\pi}{2} + r) = \\cos(r)$.\n- If $q_{\\text{mod} 4} = 2$: $\\sin(x) = \\sin((4k+2) \\cdot \\frac{\\pi}{2} + r) = \\sin(2k\\pi + \\pi + r) = -\\sin(r)$.\n- If $q_{\\text{mod} 4} = 3$: $\\sin(x) = \\sin((4k+3) \\cdot \\frac{\\pi}{2} + r) = \\sin(2k\\pi + \\frac{3\\pi}{2} + r) = -\\cos(r)$.\n\nThis strategy reduces the problem of computing $\\sin(x)$ for any $x$ to computing either $\\sin(r)$ or $\\cos(r)$ for a small argument $|r| \\le \\pi/4$. For such small arguments, the Maclaurin series converges rapidly, and the truncation criterion derived in the previous section can be applied efficiently to determine the required number of terms.\n\n**3. Analysis of Numerical Stability**\n\nThe direct evaluation of the Maclaurin series for $\\sin(x)$ with large $|x|$ is numerically unstable due to two primary issues: intermediate term overflow and catastrophic cancellation.\n\n- **Intermediate Term Growth and Overflow**: The $k$-th term of the series for $\\sin(x)$ is $t_k = \\frac{(-1)^k x^{2k+1}}{(2k+1)!}$. The magnitude of these terms initially grows rapidly with $k$. The maximum term magnitude occurs when the ratio $|t_{k+1}/t_k| = \\frac{|x|^2}{(2k+2)(2k+3)} \\approx 1$, which implies $2k+2 \\approx |x|$. For a large value such as $x=100$, the terms grow to an astronomical size (e.g., for $k=49$, the term is on the order of $100^{99}/99!$, which can easily exceed the range of standard double-precision floating-point numbers, $\\approx 10^{308}$).\n\n- **Catastrophic Cancellation**: The final result, $\\sin(x)$, must lie in $[-1, 1]$. The series evaluation for large $x$ involves summing very large positive and negative terms to produce a small final result. Standard floating-point numbers have a fixed number of significant digits (the mantissa). When two large, nearly equal numbers are subtracted, the leading significant digits cancel, leaving a result with far fewer significant digits of precision. This loss of relative accuracy is known as catastrophic cancellation and renders the final result meaningless.\n\nThe reduced-argument strategy circumvents these issues entirely. The argument $r$ is small ($|r| \\le \\pi/4 \\approx 0.785$), so the Maclaurin series terms $\\frac{r^k}{k!}$ decrease monotonically in magnitude from the start. There is no growth of intermediate terms and thus no catastrophic cancellation. The method is numerically stable.\n\nHowever, the argument reduction itself has a limitation. The step $r = x - q \\cdot (\\pi/2)$ can suffer from catastrophic cancellation when $x$ is extremely large. The reason is that standard floating-point representations of $\\pi$ have finite precision. For a very large $x$ (e.g., $x=10^{16}+0.1$), the value of $x$ itself may be subject to rounding in standard `float64` arithmetic (the unit in the last place for $10^{16}$ is greater than $0.1$, so $10^{16}+0.1$ is stored as exactly $10^{16}$). Even if $x$ is representable, the product $q \\cdot (\\pi/2)$ will be a large number close to $x$. The finite precision of the `float64` representation of $\\pi$ introduces an absolute error into this product that scales with $q$. When $x$ is large, $q$ is large, and this error can become significant, potentially larger than $\\pi/2$ itself. The subtraction $x - q \\cdot (\\pi/2)$ then cancels most significant digits, yielding a value for $r$ with few, if any, correct digits. This demonstrates the fundamental limit of computations with fixed-precision arithmetic for extremely large arguments. This phenomenon is expected to cause the test case for $x = 10^{16} + 0.1$ to fail a precision check.\n\nThe provided program implements these principles to compute the sine function and analyze its numerical properties across the specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef get_required_terms_sin(z_abs, ε):\n    \"\"\"\n    Calculates the minimum number of terms T for the sin Maclaurin series.\n    The error bound after T terms (degree 2T-1) is |z|^(2T+1)/(2T+1)!.\n    \"\"\"\n    if z_abs == 0.0:\n        return 1\n    \n    z2 = z_abs * z_abs\n    \n    # Bound for T=1 term (degree 1)\n    bound = (z_abs**3) / 6.0\n    T = 1\n    \n    denom_fac1 = 4\n    denom_fac2 = 5\n    \n    while bound > ε:\n        T += 1\n        bound *= z2 / (denom_fac1 * denom_fac2)\n        denom_fac1 += 2\n        denom_fac2 += 2\n        if T > 1000: # Safety break\n            return T\n            \n    return T\n\ndef get_required_terms_cos(z_abs, ε):\n    \"\"\"\n    Calculates the minimum number of terms T for the cos Maclaurin series.\n    The error bound after T terms (degree 2T-2) is |z|^(2T)/(2T)!.\n    \"\"\"\n    if z_abs == 0.0:\n        return 1\n    \n    z2 = z_abs * z_abs\n    \n    # Bound for T=1 term (degree 0)\n    bound = z2 / 2.0\n    T = 1\n    \n    denom_fac1 = 3\n    denom_fac2 = 4\n\n    while bound > ε:\n        T += 1\n        bound *= z2 / (denom_fac1 * denom_fac2)\n        denom_fac1 += 2\n        denom_fac2 += 2\n        if T > 1000: # Safety break\n            return T\n            \n    return T\n\ndef eval_sin_series(z, T):\n    \"\"\"Evaluates the sin Maclaurin series for T terms.\"\"\"\n    if z == 0.0:\n        return 0.0\n\n    z2 = z * z\n    term = z\n    total = term\n    for k in range(1, T):\n        # term_k = -term_{k-1} * z^2 / ((2k)(2k+1))\n        term *= -z2 / ((2 * k) * (2 * k + 1))\n        total += term\n    return total\n\ndef eval_cos_series(z, T):\n    \"\"\"Evaluates the cos Maclaurin series for T terms.\"\"\"\n    z2 = z * z\n    term = 1.0\n    total = term\n    for k in range(1, T):\n        # term_k = -term_{k-1} * z^2 / ((2k-1)(2k))\n        term *= -z2 / ((2 * k - 1) * (2 * k))\n        total += term\n    return total\n\ndef compute_sin_reduced(x, ε):\n    \"\"\"\n    Computes sin(x) using argument reduction and Maclaurin series.\n    Returns the computed value and the number of terms used.\n    \"\"\"\n    pi_over_2 = np.pi / 2.0\n    \n    # Argument reduction\n    q_float = x / pi_over_2\n    q = np.round(q_float)\n    r = x - q * pi_over_2\n    \n    q_int = int(q)\n    quadrant = q_int % 4\n    \n    r_abs = abs(r)\n\n    if quadrant == 0:  # sin(r)\n        T = get_required_terms_sin(r_abs, ε)\n        val = eval_sin_series(r, T)\n        return val, T\n    elif quadrant == 1:  # cos(r)\n        T = get_required_terms_cos(r_abs, ε)\n        val = eval_cos_series(r, T)\n        return val, T\n    elif quadrant == 2:  # -sin(r)\n        T = get_required_terms_sin(r_abs, ε)\n        val = eval_sin_series(r, T)\n        return -val, T\n    else:  # quadrant == 3, -cos(r)\n        T = get_required_terms_cos(r_abs, ε)\n        val = eval_cos_series(r, T)\n        return -val, T\n\ndef check_naive_impractical(x, ε, T_max):\n    \"\"\"\n    Checks if a naive Maclaurin series evaluation of sin(x) is impractical.\n    Impractical if > T_max terms are needed or if intermediate terms overflow.\n    \"\"\"\n    x_abs = abs(x)\n    if x_abs == 0.0:\n        return False\n        \n    x2 = x_abs * x_abs\n    \n    # Check terms and remainder bound iteratively for T = 1, 2, ...\n    \n    # T=1 term magnitude (|x|)\n    term_mag = x_abs\n    if np.isinf(term_mag):\n        return True # Overflow\n\n    # Remainder bound for T=1 term\n    remainder_bound = term_mag * x2 / 6.0\n    if remainder_bound = ε:\n        return False # Practical\n    \n    for T in range(2, T_max + 1):\n        # Magnitude of the T-th term\n        # term_mag(T) = term_mag(T-1) * x^2 / ((2T-2)*(2T-1))\n        term_mag *= x2 / ((2*T - 2) * (2*T - 1))\n        if np.isinf(term_mag):\n            return True # Overflow of intermediate term\n\n        # Remainder bound for T terms\n        # bound(T) = term_mag(T) * x^2 / ((2T)*(2T+1))\n        remainder_bound = term_mag * x2 / ((2*T) * (2*T + 1))\n        if np.isinf(remainder_bound):\n            # This can happen if term_mag is huge but finite\n            # and gets multiplied by a large x2\n            return True\n            \n        if remainder_bound = ε:\n            return False # Practical, convergence within T_max terms\n\n    return True # Not converged within T_max terms\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and produce the final output.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (100.0, 1e-12),\n        (10**6 + 0.1, 1e-12),\n        (10**16 + 0.1, 1e-12),\n        (np.pi/2 + 1e-8, 1e-15),\n    ]\n\n    results = []\n    for x, ε in test_cases:\n        # 1. Compute s_approx and T using the reduced method.\n        s_approx, T = compute_sin_reduced(x, ε)\n        \n        # 2. Compute b_ok by comparing with a high-quality reference.\n        # Use np.longdouble for reference calculation where precision matters\n        ref_val = np.sin(np.longdouble(x))\n        b_ok = np.abs(s_approx - ref_val) = ε\n        \n        # 3. Assess if naive method is impractical.\n        T_max = 1000\n        b_naive_impractical = check_naive_impractical(x, ε, T_max)\n        \n        results.extend([b_ok, T, b_naive_impractical])\n\n    # Final print statement in the exact required format.\n    # Python's str() for a boolean is 'True' or 'False'.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2442233"}]}