{"hands_on_practices": [{"introduction": "This first practice will explore one of the most common pitfalls in numerical computation: catastrophic cancellation. This issue arises when subtracting two nearly-equal floating-point numbers, which can lead to a drastic loss of significant figures and a highly inaccurate result. By analyzing a simple function where this problem occurs for large input values, you will see how a clever algebraic reformulation can lead to a numerically stable algorithm that completely avoids the issue [@problem_id:2205457].", "problem": "Consider the function $f(x) = \\sqrt{x+1} - \\sqrt{x}$. For very large, positive values of $x$, computing $f(x)$ directly using standard floating-point arithmetic can lead to a significant loss of precision due to the subtraction of two nearly equal numbers. Your task is to first find an algebraically equivalent expression for $f(x)$ that is more numerically stable for large $x$ because it avoids this subtraction. Then, use this improved expression to calculate the value of $f(x)$ for $x = 4 \\times 10^{16}$.\n\nExpress your answer as a real number in scientific notation of the form $a \\times 10^b$, rounded to three significant figures.", "solution": "We want to avoid catastrophic cancellation in $f(x) = \\sqrt{x+1} - \\sqrt{x}$ for large positive $x$. Multiply numerator and denominator by the conjugate to get an algebraically equivalent form:\n$$\nf(x) = \\left(\\sqrt{x+1} - \\sqrt{x}\\right)\\frac{\\sqrt{x+1} + \\sqrt{x}}{\\sqrt{x+1} + \\sqrt{x}} = \\frac{(x+1) - x}{\\sqrt{x+1} + \\sqrt{x}} = \\frac{1}{\\sqrt{x+1} + \\sqrt{x}}.\n$$\nThis expression avoids subtracting two nearly equal large numbers and is therefore more numerically stable.\n\nFor $x = 4 \\times 10^{16}$, compute using the stable form:\n$$\nf(4 \\times 10^{16}) = \\frac{1}{\\sqrt{4 \\times 10^{16} + 1} + \\sqrt{4 \\times 10^{16}}}.\n$$\nNote that $\\sqrt{4 \\times 10^{16}} = 2 \\times 10^{8}$. Also,\n$$\n\\sqrt{4 \\times 10^{16} + 1} = 2 \\times 10^{8}\\sqrt{1 + \\frac{1}{4 \\times 10^{16}}}.\n$$\nLet $\\epsilon = \\frac{1}{4 \\times 10^{16}}$. Using the binomial expansion $\\sqrt{1+\\epsilon} = 1 + \\frac{\\epsilon}{2} - \\frac{\\epsilon^{2}}{8} + \\cdots$, we have\n$$\n\\sqrt{4 \\times 10^{16} + 1} = 2 \\times 10^{8}\\left(1 + \\frac{\\epsilon}{2} - \\frac{\\epsilon^{2}}{8} + \\cdots \\right) = 2 \\times 10^{8} + 10^{8}\\epsilon + O(\\epsilon^{2}).\n$$\nThus the denominator is\n$$\n\\sqrt{4 \\times 10^{16} + 1} + \\sqrt{4 \\times 10^{16}} = 4 \\times 10^{8} + 10^{8}\\epsilon + O(\\epsilon^{2}).\n$$\nTherefore,\n$$\nf(4 \\times 10^{16}) = \\frac{1}{4 \\times 10^{8} + 10^{8}\\epsilon + O(\\epsilon^{2})} = \\frac{1}{4 \\times 10^{8}}\\left(1 - \\frac{10^{8}\\epsilon}{4 \\times 10^{8}} + O(\\epsilon^{2})\\right).\n$$\nSince $\\epsilon = 2.5 \\times 10^{-17}$, the leading term is\n$$\n\\frac{1}{4 \\times 10^{8}} = 2.5 \\times 10^{-9},\n$$\nand the correction is of order $10^{-26}$, which does not affect rounding to three significant figures. Hence, to three significant figures,\n$$\nf(4 \\times 10^{16}) \\approx 2.50 \\times 10^{-9}.\n$$", "answer": "$$\\boxed{2.50 \\times 10^{-9}}$$", "id": "2205457"}, {"introduction": "Moving from single expressions to dynamic systems, this exercise demonstrates the concept of numerical stability in the context of solving ordinary differential equations (ODEs). We will use the explicit Euler method, a fundamental numerical integration technique, to model a simple decay process. This practice highlights how the choice of a key parameter, the time step $h$, is critical for obtaining a stable and physically meaningful solution, and reveals the unphysical behavior that results when the stability condition is violated [@problem_id:2205446].", "problem": "A scientist is modeling the concentration of a chemical reactant, $C(t)$, in a simple first-order reaction. The rate of change of the reactant's concentration is directly proportional to the concentration itself, following the differential equation:\n$$\n\\frac{dC}{dt} = -k C(t)\n$$\nThe rate constant is given as $k = 50.0 \\text{ s}^{-1}$, and the initial concentration at time $t=0$ is $C(0) = 100.0 \\text{ mol/L}$.\n\nTo predict the concentration at future times, the scientist employs a numerical scheme using a fixed time step, $h$. This method approximates the concentration at time $t_{n+1} = (n+1)h$ using the value at the previous time step, $t_n = nh$, according to the explicit Euler update rule:\n$$\nC_{n+1} = C_n + h \\left( \\frac{dC}{dt} \\right)_{t=t_n}\n$$\nThe scientist decides to use a time step of $h = 0.0410 \\text{ s}$. Calculate the numerical value of the concentration, $C_3$, predicted by this method after three time steps. Express your answer in mol/L, rounded to three significant figures.", "solution": "We are given the first-order linear ODE $\\frac{dC}{dt}=-kC(t)$ and the explicit Euler update\n$$\nC_{n+1}=C_{n}+h\\left(\\frac{dC}{dt}\\right)_{t=t_{n}}=C_{n}+h(-kC_{n})=(1-kh)C_{n}.\n$$\nBy recursion, this yields\n$$\nC_{n}=C_{0}(1-kh)^{n}.\n$$\nWith $C_{0}=100.0$, $k=50.0\\,\\text{s}^{-1}$, and $h=0.0410\\,\\text{s}$, the dimensionless product is\n$$\nkh=(50.0)(0.0410)=2.05,\\quad 1-kh=1-2.05=-1.05.\n$$\nThus after three steps,\n$$\nC_{3}=100.0\\,(-1.05)^{3}.\n$$\nCompute the power:\n$$\n(-1.05)^{2}=1.1025,\\quad (-1.05)^{3}=1.1025\\times(-1.05)=-1.157625,\n$$\nso\n$$\nC_{3}=100.0\\times(-1.157625)=-115.7625.\n$$\nRounding to three significant figures gives $-1.16\\times 10^{2}$, i.e., $-116$. Note that the explicit Euler factor satisfies $|1-kh|=1.05  1$, indicating instability for this step size, which explains the unphysical sign change.", "answer": "$$\\boxed{-116}$$", "id": "2205446"}, {"introduction": "This final practice bridges theory and application by tackling a common task in data analysis: calculating the variance of a dataset. You will compare the well-known \"textbook\" formula for variance with the robust, one-pass Welford's algorithm, demonstrating how the former can fail spectacularly due to catastrophic cancellation. This exercise underscores the vital importance of choosing numerically stable algorithms for reliable scientific and statistical computing, especially when dealing with large datasets or data with small variance relative to its mean [@problem_id:3197369].", "problem": "You are asked to write a complete, runnable program that investigates the numerical stability of two ways to compute the sample mean and sample variance for real-valued data: a numerically stable one-pass method known as Welford’s algorithm, and a classical two-pass formula that uses the identity involving the sum of squares. The task must be carried out in a purely mathematical and algorithmic manner, with the logic applicable in any modern programming language. The aim is to demonstrate how cancellation in the expression $$\\sum_{i=1}^{n} x_i^2 - \\frac{\\left(\\sum_{i=1}^{n} x_i\\right)^2}{n}$$ can lead to large loss of accuracy in finite-precision arithmetic when the data have large offsets and small spread. The computation must use standard binary floating point with the Institute of Electrical and Electronics Engineers (IEEE) 754 double precision format for the algorithm under test, and an independent high-precision baseline to assess error. No physical units are involved in this problem.\n\nFundamental base for the task:\n- The sample mean of a real dataset $x_1, x_2, \\dots, x_n$ is defined as $$\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i.$$\n- The unbiased sample variance is defined as $$s^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right)^2.$$\n- Finite-precision floating-point arithmetic (specifically, the Institute of Electrical and Electronics Engineers (IEEE) 754 double precision format) introduces rounding at each operation, and subtraction of nearly equal large quantities can suffer catastrophic cancellation, amplifying relative error.\n\nYour program must:\n- Implement a function that computes the sample mean and unbiased sample variance using Welford’s algorithm (one pass, updating the mean and an accumulated sum of squared deviations as each datum arrives).\n- Implement a second function that computes the sample mean and unbiased sample variance using a classical two-pass approach that first computes the sum and the sum of squares in floating point and then computes the sum of squared deviations using the expression $$\\sum_{i=1}^{n} x_i^2 - \\frac{\\left(\\sum_{i=1}^{n} x_i\\right)^2}{n}.$$\n- Implement a high-precision baseline using arbitrary-precision decimal arithmetic to compute the sample mean and unbiased sample variance from their definitions above, based on exact decimal representations of the datasets. Use at least $50$ digits of precision in the decimal arithmetic to ensure the baseline is effectively free of rounding error for the given datasets.\n\nTest suite design:\nCompute the required quantities for the following four datasets, each designed to test different facets of stability. For each dataset below, $n$ denotes the number of data points.\n1. Happy path, moderate scale: the integers from $1$ to $1000$, that is $x_i = i$ for $i \\in \\{1, 2, \\dots, 1000\\}$, with $n = 1000$.\n2. Large offset, moderate spread: $x_i = 10^{9} + i$ for $i \\in \\{-500, -499, \\dots, 498, 499\\}$, with $n = 1000$.\n3. Very large offset, tiny alternating jitter: $x_i = 10^{12} + \\delta_i$ for $i \\in \\{0, 1, \\dots, 1999\\}$ with $n = 2000$, where $\\delta_i = 10^{-3}$ if $i$ is even and $\\delta_i = -10^{-3}$ if $i$ is odd.\n4. Large offset, extremely small linear drift: $x_i = 10^{9} + 10^{-8} i$ for $i \\in \\{0, 1, \\dots, 99999\\}$, with $n = 100000$.\n\nAngle units are not used in this problem. No physical units are required. Percentages must not appear in any outputs; all quantities are numerical values in standard real-number arithmetic.\n\nFor each dataset, compute:\n- The sample mean and unbiased sample variance using the two-pass floating-point method.\n- The sample mean and unbiased sample variance using Welford’s one-pass floating-point method.\n- The high-precision baseline sample mean and unbiased sample variance using decimal arithmetic with at least $50$ digits of precision.\n\nThen produce the following metrics per dataset:\n- $E_{\\text{mean, two}} = \\left| \\bar{x}_{\\text{two}} - \\bar{x}_{\\text{baseline}} \\right|$,\n- $E_{\\text{mean, wel}} = \\left| \\bar{x}_{\\text{wel}} - \\bar{x}_{\\text{baseline}} \\right|$,\n- $E_{\\text{var, two}} = \\left| s^2_{\\text{two}} - s^2_{\\text{baseline}} \\right|$,\n- $E_{\\text{var, wel}} = \\left| s^2_{\\text{wel}} - s^2_{\\text{baseline}} \\right|$,\n- A boolean flag $C$ indicating “catastrophic cancellation suspected” in the two-pass variance, defined as true if $$E_{\\text{var, two}} \\ge 1000 \\times \\max\\left(E_{\\text{var, wel}}, 10^{-300}\\right),$$ and false otherwise.\n\nFinal output format:\nYour program should produce a single line of output containing all metrics concatenated across the four datasets, in order, as a comma-separated list enclosed in square brackets. For each dataset, append the five metrics in the exact order listed above, resulting in a top-level list of length $20$ numeric values and $4$ boolean values. For example, the output must look like $$[\\text{m2\\_err\\_ds1}, \\text{mw\\_err\\_ds1}, \\text{v2\\_err\\_ds1}, \\text{vw\\_err\\_ds1}, \\text{C\\_ds1}, \\dots, \\text{m2\\_err\\_ds4}, \\text{mw\\_err\\_ds4}, \\text{v2\\_err\\_ds4}, \\text{vw\\_err\\_ds4}, \\text{C\\_ds4}]$$ where each placeholder is replaced by its computed value. Do not include any other text.", "solution": "The problem requires an investigation into the numerical stability of two distinct algorithms for computing the sample mean and variance of a dataset. The core of the analysis rests upon the properties of finite-precision floating-point arithmetic, specifically the IEEE $754$ double-precision standard. We are to compare a classical two-pass algorithm with a more robust one-pass method known as Welford's algorithm, using a high-precision calculation as a baseline for measuring accuracy.\n\nThe fundamental quantities are the sample mean, $\\bar{x}$, and the unbiased sample variance, $s^2$, for a dataset $\\{x_1, x_2, \\dots, x_n\\}$:\n$$\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n$$\n$$\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n$$\n\nThe analysis hinges on three computational approaches:\n\n1.  **High-Precision Baseline**: To quantify the error of finite-precision algorithms, a \"ground truth\" or baseline is necessary. This baseline is computed using arbitrary-precision decimal arithmetic, with a precision of at least $50$ digits. This ensures that rounding errors in the baseline computation are negligible compared to the errors in the double-precision algorithms being tested. The mean and variance are calculated directly from their definitions above, using high-precision data types for all intermediate and final values.\n\n2.  **Two-Pass Algorithm (Numerically Unstable)**: This common method involves two passes over the data. In the first pass, two sums are accumulated: $\\sum_{i=1}^{n} x_i$ and $\\sum_{i=1}^{n} x_i^2$. In the second pass, these sums are used to compute the variance via the following algebraic identity:\n    $$\n    \\sum_{i=1}^{n} (x_i - \\bar{x})^2 = \\sum_{i=1}^{n} x_i^2 - \\frac{\\left(\\sum_{i=1}^{n} x_i\\right)^2}{n}\n    $$\n    Thus, the sample variance is computed as:\n    $$\n    s^2_{\\text{two}} = \\frac{1}{n-1} \\left( \\sum_{i=1}^{n} x_i^2 - \\frac{\\left(\\sum_{i=1}^{n} x_i\\right)^2}{n} \\right)\n    $$\n    The numerical instability of this formula, known as the \"textbook formula\", arises from the subtraction of two large, nearly equal numbers. When the data points have a large common offset (i.e., the mean $\\bar{x}$ is large) and a small spread (i.e., the standard deviation $s$ is small), the term $\\sum x_i^2 \\approx n\\bar{x}^2$ and the term $(\\sum x_i)^2/n = n\\bar{x}^2$ will be very close in value. In finite-precision arithmetic, the subtraction of these two quantities can lead to a phenomenon called \"catastrophic cancellation,\" where most of the significant digits are lost, resulting in a large relative error in the final computed variance.\n\n3.  **Welford's Algorithm (Numerically Stable)**: This is a one-pass algorithm that avoids the catastrophic cancellation problem by updating the mean and a quantity related to the sum of squared differences incrementally. Let $M_k$ be the running mean of the first $k$ data points, and let $S_k$ be the running sum of squared deviations from the mean, i.e., $S_k = \\sum_{i=1}^{k} (x_i - M_k)^2$. Welford's algorithm is based on the following recurrence relations:\n    $$\n    M_k = M_{k-1} + \\frac{x_k - M_{k-1}}{k}\n    $$\n    $$\n    S_k = S_{k-1} + (x_k - M_{k-1})(x_k - M_k)\n    $$\n    with initial conditions $M_1 = x_1$ and $S_1 = 0$. After processing all $n$ data points, the unbiased sample variance is $s^2_{\\text{wel}} = S_n / (n-1)$. This method is numerically stable because it computes the variance using deviations from the *running* mean. The terms $(x_k - M_{k-1})$ and $(x_k - M_k)$ are small, preventing the accumulation of large numbers and the subsequent catastrophic subtraction.\n\nThe program will implement these three methods, apply them to four specified datasets designed to expose the instability of the two-pass method, and calculate the absolute errors for the mean and variance of each floating-point method relative to the high-precision baseline. A boolean flag $C$ is computed to detect suspected catastrophic cancellation by comparing the error of the two-pass variance calculation to that of Welford's algorithm.\n\nThe datasets are designed as follows:\n-   Dataset 1: A simple case with moderate scale and no large offset, where both methods are expected to perform well.\n-   Dataset 2: Data with a large offset ($10^9$), where the two-pass method may start to show some degradation.\n-   Dataset 3: Data with a very large offset ($10^{12}$) and an extremely small spread, designed to induce severe catastrophic cancellation in the two-pass method.\n-   Dataset 4: Data with a large offset ($10^9$) and a very small, systematic drift. This case tests how finite-precision representation of the input data itself can affect the algorithms, in addition to the instability of the variance formula.\n\nBy comparing the errors $E_{\\text{mean, two}}$, $E_{\\text{mean, wel}}$, $E_{\\text{var, two}}$, and $E_{\\text{var, wel}}$, the superior numerical stability of Welford's algorithm over the naive two-pass formula will be demonstrated, particularly in the computation of variance for data with a coefficient of variation ($\\sigma / \\mu$) close to zero.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom decimal import Decimal, getcontext\n\ndef solve():\n    \"\"\"\n    Computes sample mean and variance using three different methods\n    (two-pass float, Welford's float, and high-precision baseline)\n    for four datasets, and reports the errors and a cancellation flag.\n    \"\"\"\n\n    def two_pass_algorithm(data: np.ndarray):\n        \"\"\"Computes mean and variance using the naive two-pass formula.\"\"\"\n        n = data.shape[0]\n        if n  2:\n            return np.nan, np.nan\n        \n        sum_x = np.sum(data)\n        sum_x_sq = np.sum(data**2)\n        \n        mean = sum_x / n\n        # This subtraction is prone to catastrophic cancellation\n        variance = (sum_x_sq - (sum_x**2) / n) / (n - 1)\n        \n        return mean, variance\n\n    def welford_algorithm(data: np.ndarray):\n        \"\"\"Computes mean and variance using Welford's stable one-pass algorithm.\"\"\"\n        n = 0\n        mean = 0.0\n        M2 = 0.0 # This is the running sum of squared deviations from the mean\n\n        for x in data:\n            n += 1\n            delta = x - mean\n            mean += delta / n\n            delta2 = x - mean\n            M2 += delta * delta2\n        \n        if n  2:\n            return mean, 0.0\n        \n        variance = M2 / (n - 1)\n        return mean, variance\n\n    def high_precision_baseline(data_str: list[str]):\n        \"\"\"\n        Computes mean and variance using high-precision decimal arithmetic\n        to establish a ground truth.\n        \"\"\"\n        getcontext().prec = 60 # Set precision to 60 digits\n        \n        data_decimal = [Decimal(s) for s in data_str]\n        n = len(data_decimal)\n        \n        if n  2:\n            return Decimal(0), Decimal(0)\n            \n        mean = sum(data_decimal) / Decimal(n)\n        \n        # Compute sum of squared deviations directly from the definition\n        sum_sq_dev = sum((x - mean)**2 for x in data_decimal)\n        variance = sum_sq_dev / Decimal(n - 1)\n        \n        return mean, variance\n\n    test_cases_defs = [\n        {\n            \"name\": \"ds1\", \"n\": 1000, \n            \"gen_float\": lambda n: np.arange(1, n + 1, dtype=np.float64),\n            \"gen_str\": lambda n: [str(i) for i in range(1, n + 1)],\n        },\n        {\n            \"name\": \"ds2\", \"n\": 1000,\n            \"gen_float\": lambda n: 1e9 + np.arange(-n//2, n//2, dtype=np.float64),\n            \"gen_str\": lambda n: [str(10**9 + i) for i in range(-n//2, n//2)],\n        },\n        {\n            \"name\": \"ds3\", \"n\": 2000,\n            \"gen_float\": lambda n: 1e12 + np.array([1e-3 if i % 2 == 0 else -1e-3 for i in range(n)], dtype=np.float64),\n            \"gen_str\": lambda n: ['1000000000000.001' if i % 2 == 0 else '999999999999.999' for i in range(n)],\n        },\n        {\n            \"name\": \"ds4\", \"n\": 100000,\n            \"gen_float\": lambda n: 1e9 + 1e-8 * np.arange(n, dtype=np.float64),\n            \"gen_str\": lambda n: [str(Decimal('1e9') + Decimal('1e-8') * Decimal(i)) for i in range(n)],\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases_defs:\n        n = case[\"n\"]\n        \n        # Generate data for floating-point and high-precision calculations\n        float_data = case[\"gen_float\"](n)\n        str_data = case[\"gen_str\"](n)\n        \n        # Run all three algorithms\n        mean_two, var_two = two_pass_algorithm(float_data)\n        mean_wel, var_wel = welford_algorithm(float_data)\n        mean_base_dec, var_base_dec = high_precision_baseline(str_data)\n\n        # Convert baseline results to float for comparison\n        mean_base = float(mean_base_dec)\n        var_base = float(var_base_dec)\n        \n        # Calculate error metrics\n        E_mean_two = abs(mean_two - mean_base)\n        E_mean_wel = abs(mean_wel - mean_base)\n        E_var_two = abs(var_two - var_base)\n        E_var_wel = abs(var_wel - var_base)\n        \n        # Determine the catastrophic cancellation flag\n        # The max with 1e-300 prevents division by zero or issues with tiny numbers\n        cancellation_threshold = 1000 * max(E_var_wel, 1e-300)\n        C_flag = E_var_two = cancellation_threshold\n\n        results.extend([E_mean_two, E_mean_wel, E_var_two, E_var_wel, C_flag])\n\n    # Final print statement in the exact required format.\n    # The default str() for a boolean is 'True' or 'False'.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3197369"}]}