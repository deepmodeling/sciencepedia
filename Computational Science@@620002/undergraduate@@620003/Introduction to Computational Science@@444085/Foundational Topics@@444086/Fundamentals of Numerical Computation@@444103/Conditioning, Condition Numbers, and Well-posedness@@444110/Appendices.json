{"hands_on_practices": [{"introduction": "We begin our hands-on exploration with the most fundamental question in numerical analysis: how sensitive is the output of a function to small errors in its input? This exercise [@problem_id:3110304] asks you to analyze the conditioning of evaluating the exponential function $f(x) = e^x$, a cornerstone of scientific computing. By deriving the relative condition number from first principles and comparing it to the actual error from a Taylor series approximation, you will gain a concrete understanding of how input magnitude can amplify relative errors, even for a mathematically smooth function.", "problem": "You are to analyze the numerical conditioning of evaluating the exponential function using a truncated Taylor series and quantify the relation between the range magnitude and relative error amplification. Consider the function $f(x) = e^{x}$ and the algorithm that approximates $f(x)$ by the Taylor polynomial about $x=0$ of degree $n$, denoted $T_{n}(x) = \\sum_{k=0}^{n} \\frac{x^{k}}{k!}$. Your task is to connect the conditioning of the problem of evaluating $f(x)$ with the observed forward relative error of this truncated series at different values of $x$.\n\nUse the following fundamental bases only:\n- The definition of relative condition number for a differentiable scalar function $f$: the relative condition number at $x$ is the limit of the ratio of relative output change to relative input change as the input perturbation tends to zero.\n- The Taylor polynomial with the Lagrange form of the remainder for a smooth function about $x=0$.\n- The standard properties of the exponential function.\n\nTasks:\n1) Starting from the definition of relative condition number, derive the relative condition number $\\kappa_{f}(x)$ for the function $f(x)=e^{x}$ at a point $x$ with $f(x) \\neq 0$. Do not use any pre-memorized shortcut formula; start from the definition of relative condition number in the limit of vanishing perturbations.\n2) Using the Lagrange form of the Taylor remainder, derive a computable upper bound $B(x,n)$ on the forward relative error $\\frac{|e^{x} - T_{n}(x)|}{|e^{x}|}$ for $x \\ge 0$.\n3) Implement a program that, for each test case $(x,n)$ in the test suite below, computes:\n   - The derived relative condition number $\\kappa_{f}(x)$.\n   - The observed forward relative error $r(x,n) = \\frac{|T_{n}(x) - e^{x}|}{|e^{x}|}$ using floating-point arithmetic.\n   - A boolean value $\\mathrm{BoundOK}(x,n)$ that is $\\mathrm{True}$ if and only if $r(x,n) \\le B(x,n)$.\n4) The program must not read any input. It must compute the values for the test suite and print a single line of output in the following exact format: a comma-separated list of per-case triplets enclosed in square brackets, where each triplet is of the form $[\\kappa_{f}(x),r(x,n),\\mathrm{BoundOK}(x,n)]$. The list must contain no spaces, floating-point numbers must be rounded to exactly $10$ digits after the decimal point, and booleans must be printed as $\\mathrm{True}$ or $\\mathrm{False}$.\n\nTest suite (each pair $(x,n)$ is a test case):\n- $(x,n) = (0,0)$\n- $(x,n) = (0.1,3)$\n- $(x,n) = (1,5)$\n- $(x,n) = (5,10)$\n- $(x,n) = (10,10)$\n- $(x,n) = (20,10)$\n\nNotes:\n- There are no physical units involved.\n- Angles are not involved.\n- The output should be a single line containing the list of results for the six test cases in the exact specified format, for example $[[a,b,c],[d,e,f],\\dots]$ but with the numerical values substituted and booleans as specified.\n- The design of the test suite includes an edge case at $x=0$, moderate values, and a large value $x=20$ to probe the connection between range magnitude and relative error amplification.\n\nYour program should produce a single line of output containing the results as a comma-separated list of the six triplets enclosed in square brackets, with no spaces, for example: $[[\\kappa_{1},r_{1},\\mathrm{BoundOK}_{1}],[\\kappa_{2},r_{2},\\mathrm{BoundOK}_{2}],\\dots,[\\kappa_{6},r_{6},\\mathrm{BoundOK}_{6}]]$.", "solution": "**1. Derivation of the Relative Condition Number $\\kappa_{f}(x)$**\n\nThe relative condition number, $\\kappa_{f}(x)$, quantifies the sensitivity of the function's relative output change to a relative input change. By definition, for a small perturbation $\\delta x$ in the input $x$ (where $x \\neq 0$), it is given by the limit:\n$$ \\kappa_{f}(x) = \\lim_{\\delta x \\to 0} \\left| \\frac{\\text{relative change in } f(x)}{\\text{relative change in } x} \\right| = \\lim_{\\delta x \\to 0} \\left| \\frac{(f(x+\\delta x) - f(x))/f(x)}{(\\delta x)/x} \\right| $$\nWe can rearrange the expression inside the limit:\n$$ \\kappa_{f}(x) = \\lim_{\\delta x \\to 0} \\left| \\frac{x}{f(x)} \\cdot \\frac{f(x+\\delta x) - f(x)}{\\delta x} \\right| $$\nBy the properties of limits, we can move the terms not dependent on $\\delta x$ outside:\n$$ \\kappa_{f}(x) = \\left| \\frac{x}{f(x)} \\right| \\lim_{\\delta x \\to 0} \\left| \\frac{f(x+\\delta x) - f(x)}{\\delta x} \\right| $$\nThe limit term is the definition of the absolute value of the derivative of $f$ at $x$, i.e., $|f'(x)|$.\n$$ \\kappa_{f}(x) = \\left| \\frac{x f'(x)}{f(x)} \\right| $$\nFor the specific function $f(x) = e^{x}$, its derivative is $f'(x) = e^{x}$. Substituting these into the formula yields:\n$$ \\kappa_{f}(x) = \\left| \\frac{x \\cdot e^{x}}{e^{x}} \\right| = |x| $$\nThis formula is derived for $x \\neq 0$. However, it is well-defined at $x=0$, where $\\kappa_{f}(0) = |0| = 0$. This indicates that the problem of evaluating $e^{0}$ is perfectly conditioned. We will use the formula $\\kappa_{f}(x) = |x|$ for all $x$.\n\n**2. Derivation of the Upper Bound on Relative Error $B(x,n)$**\n\nWe are tasked with finding an upper bound on the forward relative error of approximating $f(x)=e^x$ with its Taylor polynomial $T_n(x)$ for $x \\ge 0$. The forward relative error is defined as:\n$$ r(x,n) = \\frac{|f(x) - T_{n}(x)|}{|f(x)|} = \\frac{|e^{x} - T_{n}(x)|}{|e^{x}|} $$\nThe absolute error, $|e^{x} - T_{n}(x)|$, is given by the remainder term of the Taylor expansion. Using the Lagrange form of the remainder, $R_n(x) = f(x) - T_n(x)$, we have:\n$$ R_n(x) = \\frac{f^{(n+1)}(c)}{(n+1)!} x^{n+1} $$\nfor some value $c$ between $0$ and $x$. For $f(x)=e^x$, all of its derivatives are also $e^x$, so $f^{(n+1)}(c) = e^c$. Thus, the absolute error is:\n$$ e^x - T_n(x) = \\frac{e^c}{(n+1)!} x^{n+1} $$\nSubstituting this into the relative error formula:\n$$ r(x,n) = \\frac{\\left| \\frac{e^c}{(n+1)!} x^{n+1} \\right|}{|e^x|} $$\nThe problem specifies that $x \\ge 0$. This implies that $x^{n+1} \\ge 0$ and $e^x > 0$. The value $c$ is in the interval $[0, x]$.\n$$ r(x,n) = \\frac{e^c x^{n+1}}{(n+1)! e^x} = e^{c-x} \\frac{x^{n+1}}{(n+1)!} $$\nTo find an upper bound for $r(x,n)$, we need to find the maximum possible value of the term $e^{c-x}$. Since $c \\in [0, x]$, the exponent $c-x$ is in the interval $[-x, 0]$. The exponential function is monotonically increasing, so its maximum value on this interval occurs at the right endpoint, $c-x=0$ (which corresponds to $c=x$).\n$$ e^{c-x} \\le e^0 = 1 $$\nBy applying this inequality, we obtain an upper bound $B(x,n)$ on the relative error:\n$$ r(x,n) \\le 1 \\cdot \\frac{x^{n+1}}{(n+1)!} $$\nTherefore, a computable upper bound is:\n$$ B(x,n) = \\frac{x^{n+1}}{(n+1)!} $$\n\n**3. Algorithmic Implementation**\n\nThe program will iterate through each $(x,n)$ pair in the test suite and perform the following calculations:\n\n*   **Relative Condition Number $\\kappa_f(x)$**: This is computed directly using the derived formula $\\kappa_f(x) = |x|$.\n*   **Observed Forward Relative Error $r(x,n)$**: This requires computing $e^x$ and $T_n(x) = \\sum_{k=0}^{n} \\frac{x^k}{k!}$. The sum for $T_n(x)$ will be calculated iteratively to maintain numerical stability and avoid computing large powers and factorials separately. The process is as follows:\n    1. Initialize `total = 1.0` (for the $k=0$ term) and `term = 1.0`.\n    2. Loop for $k$ from $1$ to $n$: update the term as `term = term * x / k` and add it to the total: `total = total + term`.\n    3. Compute `exp_x = numpy.exp(x)`.\n    4. Calculate the relative error $r(x,n) = \\frac{|total - exp\\_x|}{|exp\\_x|}$.\n*   **Upper Bound $B(x,n)$**: This is computed from the derived formula $B(x,n) = \\frac{x^{n+1}}{(n+1)!}$. The power will be computed using `numpy.power` and the factorial using `scipy.special.factorial` which can handle non-integer arguments and return floating-point results, preventing overflow for large $n$.\n*   **Bound Check $\\mathrm{BoundOK}(x,n)$**: This is a boolean value determined by the logical comparison $r(x,n) \\le B(x,n)$.\n\nThe results for each test case—$\\kappa_f(x)$, $r(x,n)$, and $\\mathrm{BoundOK}(x,n)$—will be formatted into a string `[k,r,b]` with floating-point numbers rounded to $10$ decimal places. These strings will then be joined by commas and enclosed in square brackets to produce the final output line.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import factorial\n\ndef solve():\n    \"\"\"\n    Analyzes the numerical conditioning of evaluating e^x using a truncated\n    Taylor series for a given set of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.0, 0),\n        (0.1, 3),\n        (1.0, 5),\n        (5.0, 10),\n        (10.0, 10),\n        (20.0, 10),\n    ]\n\n    all_results_str = []\n    \n    for x, n in test_cases:\n        # 1. Compute the relative condition number kappa_f(x) = |x|\n        kappa_fx = np.abs(x)\n\n        # 2. Compute the observed forward relative error r(x,n)\n        # 2a. Compute T_n(x) = sum_{k=0 to n} x^k/k!\n        # An iterative method is used for better numerical stability.\n        # term_{k} = term_{k-1} * x / k\n        tn_x = 0.0\n        term = 1.0  # k=0 term: x^0/0! = 1\n        tn_x += term\n        for k in range(1, n + 1):\n            term = term * x / k\n            tn_x += term\n        \n        # 2b. Compute e^x\n        exp_x = np.exp(x)\n\n        # 2c. Compute the relative error r(x,n) = |T_n(x) - e^x| / |e^x|\n        # The denominator |e^x| is never zero for real x.\n        if exp_x == 0:\n            # This case is physically impossible for f(x)=e^x and real x,\n            # but included for robustness.\n            r_xn = np.inf if tn_x != 0 else 0.0\n        else:\n            r_xn = np.abs(tn_x - exp_x) / np.abs(exp_x)\n\n        # 3. Compute the upper bound B(x,n) on the relative error\n        # B(x,n) = x^(n+1) / (n+1)! for x >= 0\n        # Use exact=False to get a float result from factorial and avoid overflow.\n        b_xn = np.power(x, n + 1) / factorial(n + 1, exact=False)\n\n        # 4. Check if the observed error is within the derived bound\n        # A small tolerance could be used for floating point comparisons,\n        # but for this problem, direct comparison is sufficient.\n        bound_ok = r_xn <= b_xn\n        \n        # Format the results for this case according to the problem specification\n        case_result_str = (\n            f\"[{kappa_fx:.10f},\"\n            f\"{r_xn:.10f},\"\n            f\"{bound_ok}]\"\n        )\n        all_results_str.append(case_result_str)\n\n    # Print the final result string in the exact required format.\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```", "id": "3110304"}, {"introduction": "Having explored the conditioning of a function evaluation, we now move to a more complex task: solving an equation. This practice [@problem_id:3110311] investigates the well-posedness of the root-finding problem, asking how the solution to $f(x)=0$ changes when the function $f$ itself is slightly perturbed. You will implement two classic root-finding algorithms, Newton's method and the bisection method, to empirically measure a root's sensitivity and compare it against a theoretical prediction, revealing that the problem's inherent conditioning is governed by the derivative at the root, $|f'(x^\\star)|$.", "problem": "Consider the nonlinear equation $f(x) = x^3 - 3x + 1 = 0$. The task is to analyze the sensitivity of computed roots with respect to small perturbations in both the function and, for iterative methods that require an initial guess, the initial guess. The analysis should be grounded in fundamental definitions from numerical analysis: stability of algorithms, well-posedness of a problem, and conditioning via the sensitivity of the solution mapping.\n\nDefine the perturbed function family $f_\\varepsilon(x) = f(x) + \\varepsilon\\,g(x)$, where $\\varepsilon$ is a small real parameter and $g(x)$ is a smooth function. You will compare the behavior of two root-finding methods:\n- Newton-Raphson method (Newton’s method), which requires a differentiable function and an initial guess.\n- The bisection method, which requires a bracketing interval with a sign change and does not require derivatives.\n\nThe analysis must address the well-posedness of the root mapping $f \\mapsto x^\\star$ at simple roots and empirically estimate the absolute conditioning of the root with respect to additive perturbations of the function. You must not use any closed-form solution of the cubic to obtain the roots; instead, implement the two root-finding methods to compute numerical approximations. Angle quantities in trigonometric functions must be interpreted in radians.\n\nYour program must implement:\n- Newton’s method for $f_\\varepsilon$ using the derivative of $f_\\varepsilon$.\n- The bisection method for $f_\\varepsilon$ on specified intervals that exhibit a sign change.\n\nUse the following test suite, where each case specifies the method, the perturbation $\\varepsilon$, the perturbation function $g(x)$, and either the initial guess for Newton’s method or the bracketing interval $[a,b]$ for the bisection method. All trigonometric evaluations must be in radians. For each case, compute the specified outputs, adhering to the definitions below.\n\nDefinitions for outputs:\n- For cases with nonzero $\\varepsilon$, compute the observed shift $\\Delta x_{\\text{obs}} = \\lvert x_\\varepsilon - x_0 \\rvert$, where $x_\\varepsilon$ is the root of $f_\\varepsilon$ computed by the specified method and $x_0$ is the corresponding baseline root of $f$ computed by the same selection rule (same method and bracket or same basin determined by initial guess).\n- For cases with nonzero $\\varepsilon$, compute the predicted first-order shift magnitude $\\Delta x_{\\text{pred}} = \\left| \\dfrac{\\varepsilon\\,g(x_0)}{f'(x_0)} \\right|$, where $f'(x) = 3x^2 - 3$ is the derivative of the unperturbed function evaluated at the baseline root $x_0$.\n- For cases with zero $\\varepsilon$, compute the absolute difference in the returned root relative to the corresponding baseline, and report the number of iterations taken by the method.\n\nThe test suite consists of the following seven cases:\n\n1. Bisection on $[1, 2]$, with $\\varepsilon = 10^{-6}$ and $g(x) = x$. Outputs: $\\Delta x_{\\text{obs}}$ and $\\Delta x_{\\text{pred}}$ for the root in $[1,2]$.\n2. Newton’s method with initial guess $x_{\\text{init}} = 1.3$, with $\\varepsilon = 10^{-6}$ and $g(x) = x$. Outputs: $\\Delta x_{\\text{obs}}$ and $\\Delta x_{\\text{pred}}$ for the converged root from this initial guess.\n3. Newton’s method with initial guess $x_{\\text{init}} = 0.2$, with $\\varepsilon = 0$ and $g$ irrelevant. Outputs: the absolute difference in the root relative to the baseline Newton root from case 2 (with $\\varepsilon = 0$), and the number of iterations taken.\n4. Bisection on $[1.1, 2.0]$, with $\\varepsilon = 0$ and $g$ irrelevant. Outputs: the absolute difference in the root relative to the baseline bisection root from case 1 (with $\\varepsilon = 0$), and the number of iterations taken.\n5. Bisection on $[0, 1]$, with $\\varepsilon = 10^{-6}$ and $g(x) = x$. Outputs: $\\Delta x_{\\text{obs}}$ and $\\Delta x_{\\text{pred}}$ for the root in $[0,1]$.\n6. Bisection on $[-2, -1]$, with $\\varepsilon = 10^{-6}$ and $g(x) = x$. Outputs: $\\Delta x_{\\text{obs}}$ and $\\Delta x_{\\text{pred}}$ for the root in $[-2,-1]$.\n7. Newton’s method with initial guess $x_{\\text{init}} = 0.3$, with $\\varepsilon = 10^{-12}$ and $g(x) = \\sin(x)$ (radians). Outputs: $\\Delta x_{\\text{obs}}$ and $\\Delta x_{\\text{pred}}$ for the converged root from this initial guess.\n\nAlgorithmic requirements:\n- For the bisection method, terminate when the interval width is less than $10^{-14}$ or a maximum of $200$ iterations is reached.\n- For Newton’s method, terminate when the absolute step size is less than $10^{-14}$ or a maximum of $200$ iterations is reached. If the derivative magnitude falls below $10^{-14}$ at an iterate, proceed with the iteration without special handling; the method’s behavior under such conditions is part of the analysis.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the following order:\n$[\\Delta x_{\\text{obs}}^{(1)}, \\Delta x_{\\text{pred}}^{(1)}, \\Delta x_{\\text{obs}}^{(2)}, \\Delta x_{\\text{pred}}^{(2)}, \\Delta x^{(3)}, N^{(3)}, \\Delta x^{(4)}, N^{(4)}, \\Delta x_{\\text{obs}}^{(5)}, \\Delta x_{\\text{pred}}^{(5)}, \\Delta x_{\\text{obs}}^{(6)}, \\Delta x_{\\text{pred}}^{(6)}, \\Delta x_{\\text{obs}}^{(7)}, \\Delta x_{\\text{pred}}^{(7)}]$, where superscripts indicate the case number and $N^{(k)}$ is the iteration count for case $k$ with $\\varepsilon = 0$.\n\nAll outputs must be real numbers or integers. No physical units are involved in this problem, and all angle quantities for $g(x) = \\sin(x)$ must be interpreted in radians.", "solution": "The problem requires an analysis of the sensitivity of roots of the nonlinear equation $f(x) = x^3 - 3x + 1 = 0$ to small perturbations. This analysis involves concepts of well-posedness, conditioning, and the stability of numerical algorithms, specifically the Newton-Raphson and Bisection methods.\n\n### 1. Theoretical Foundation: Well-Posedness and Conditioning\n\nA mathematical problem is considered **well-posed** if a solution exists, is unique, and depends continuously on the input data. For the root-finding problem $f(x) = 0$, the \"input data\" is the function $f$ itself. We are interested in how the root $x^\\star$ changes when $f$ is perturbed. The problem of finding a simple root (where $f'(x^\\star) \\neq 0$) is well-posed.\n\nThe **conditioning** of a problem quantifies this sensitivity. We analyze the perturbed function family $f_\\varepsilon(x) = f(x) + \\varepsilon g(x)$, where $\\varepsilon$ is a small parameter. Let $x(\\varepsilon)$ be the root of $f_\\varepsilon(x) = 0$. By definition, $x(0) = x^\\star$ is the root of the unperturbed problem $f(x)=0$.\n\nTo determine the first-order sensitivity, we can use the Implicit Function Theorem. Since $x^\\star$ is a simple root, $f'(x^\\star) \\neq 0$. This ensures that $x(\\varepsilon)$ is a differentiable function of $\\varepsilon$ near $\\varepsilon = 0$. We differentiate the identity $f(x(\\varepsilon)) + \\varepsilon g(x(\\varepsilon)) = 0$ with respect to $\\varepsilon$:\n$$\n\\frac{d}{d\\varepsilon} \\left[ f(x(\\varepsilon)) + \\varepsilon g(x(\\varepsilon)) \\right] = 0\n$$\nApplying the chain rule, we get:\n$$\nf'(x(\\varepsilon)) \\frac{dx}{d\\varepsilon} + g(x(\\varepsilon)) + \\varepsilon g'(x(\\varepsilon)) \\frac{dx}{d\\varepsilon} = 0\n$$\nEvaluating at $\\varepsilon = 0$, and noting that $x(0) = x^\\star$, the equation simplifies:\n$$\nf'(x^\\star) \\frac{dx}{d\\varepsilon}\\bigg|_{\\varepsilon=0} + g(x^\\star) = 0\n$$\nSolving for the derivative, which represents the sensitivity of the root to the perturbation, yields:\n$$\n\\frac{dx}{d\\varepsilon}\\bigg|_{\\varepsilon=0} = - \\frac{g(x^\\star)}{f'(x^\\star)}\n$$\nFor a small but non-zero $\\varepsilon$, the change in the root, $\\Delta x = x(\\varepsilon) - x^\\star$, can be approximated by a first-order Taylor expansion:\n$$\n\\Delta x \\approx \\varepsilon \\frac{dx}{d\\varepsilon}\\bigg|_{\\varepsilon=0} = -\\varepsilon \\frac{g(x^\\star)}{f'(x^\\star)}\n$$\nThe magnitude of this predicted shift is thus:\n$$\n\\Delta x_{\\text{pred}} = |\\Delta x| \\approx \\left| \\frac{\\varepsilon g(x^\\star)}{f'(x^\\star)} \\right|\n$$\nIn our numerical implementation, the exact root $x^\\star$ is not known. We replace it with its numerical approximation $x_0$, computed from the unperturbed equation ($\\varepsilon=0$). This gives the formula specified in the problem: $\\Delta x_{\\text{pred}} = \\left| \\dfrac{\\varepsilon\\,g(x_0)}{f'(x_0)} \\right|$. The term $\\left| \\frac{g(x_0)}{f'(x_0)} \\right|$ is the estimated absolute condition number of the root with respect to the perturbation $g(x)$. A large value indicates an ill-conditioned problem where small perturbations in the function can lead to large changes in the root.\n\n### 2. Numerical Algorithms\n\nWe will implement two standard root-finding algorithms.\n\n**Bisection Method**: This method is guaranteed to converge for a continuous function $h(x)$ on an interval $[a, b]$ if $h(a)$ and $h(b)$ have opposite signs. It iteratively halves the interval while keeping the root bracketed. At each step, the midpoint $c = (a+b)/2$ is computed. The new interval becomes $[a, c]$ if $h(a)h(c) < 0$, or $[c, b]$ otherwise. We will terminate when the interval width $b-a$ is less than a tolerance of $10^{-14}$.\n\n**Newton's Method**: This iterative method finds successively better approximations to the roots of a real-valued function $h(x)$. The iteration is defined by:\n$$\nx_{k+1} = x_k - \\frac{h(x_k)}{h'(x_k)}\n$$\nThis method converges quadratically if the initial guess $x_0$ is sufficiently close to a simple root. We will apply this method to the perturbed function $h(x) = f_\\varepsilon(x) = x^3 - 3x + 1 + \\varepsilon g(x)$, using its derivative $h'(x) = f'_\\varepsilon(x) = 3x^2 - 3 + \\varepsilon g'(x)$. We will terminate when the absolute step size, $|x_{k+1} - x_k|$, is less than a tolerance of $10^{-14}$.\n\n### 3. Step-by-Step Execution of Test Cases\n\nFor each test case, we compute the required outputs by implementing these algorithms and applying the derived sensitivity formula.\n\nThe unperturbed function is $f(x) = x^3 - 3x + 1$ with derivative $f'(x) = 3x^2 - 3$. The function has three distinct real roots, located in the intervals $[-2, -1]$, $[0, 1]$, and $[1, 2]$.\n\n- **Case 1 & 4**: Bisection on $[1, 2]$ and $[1.1, 2.0]$.\n  For Case 1, we first find the baseline root $x_0$ for $f(x)=0$ on $[1, 2]$. Then, we find the perturbed root $x_\\varepsilon$ for $f_\\varepsilon(x) = f(x) + 10^{-6}x$ on the same interval. We compute $\\Delta x_{\\text{obs}} = |x_\\varepsilon - x_0|$ and $\\Delta x_{\\text{pred}} = |\\varepsilon x_0 / f'(x_0)|$.\n  For Case 4, we compute the root of $f(x)=0$ on $[1.1, 2.0]$ and find its absolute difference from the baseline root of Case 1, along with the iteration count. This tests the bisection method's robustness to the initial bracketing interval.\n\n- **Case 2 & 3**: Newton's method with initial guesses.\n  For Case 2, with $x_{\\text{init}} = 1.3$, we find the baseline root $x_0$ for $f(x)=0$ and the perturbed root $x_\\varepsilon$ for $f_\\varepsilon(x) = f(x) + 10^{-6}x$. The derivative for Newton's method is $f'_\\varepsilon(x) = 3x^2 - 3 + 10^{-6}$. We then compute $\\Delta x_{\\text{obs}}$ and $\\Delta x_{\\text{pred}}$.\n  For Case 3, we use $x_{\\text{init}} = 0.2$ for $f(x)=0$. This initial guess lies in the basin of attraction of a different root than the one found in Case 2. We report the number of iterations and the absolute difference between this root and the baseline root from Case 2.\n\n- **Case 5**: Bisection on $[0, 1]$.\n  Similar to Case 1, but for the root located in the interval $[0, 1]$. We find a baseline root $x_0$ and a perturbed root $x_\\varepsilon$ for $f_\\varepsilon(x) = f(x) + 10^{-6}x$, then compute $\\Delta x_{\\text{obs}}$ and $\\Delta x_{\\text{pred}}$.\n\n- **Case 6**: Bisection on $[-2, -1]$.\n  Similar to Case 1 and 5, but for the root located in the interval $[-2, -1]$. We find $x_0$ and $x_\\varepsilon$ for $f_\\varepsilon(x) = f(x) + 10^{-6}x$, then compute $\\Delta x_{\\text{obs}}$ and $\\Delta x_{\\text{pred}}$.\n\n- **Case 7**: Newton's method with a different perturbation.\n  With $x_{\\text{init}} = 0.3$, we find the baseline root $x_0$. Then we find the perturbed root $x_\\varepsilon$ for $f_\\varepsilon(x) = f(x) + 10^{-12}\\sin(x)$, using the derivative $f'_\\varepsilon(x) = 3x^2 - 3 + 10^{-12}\\cos(x)$. Finally, we compute $\\Delta x_{\\text{obs}}$ and $\\Delta x_{\\text{pred}} = |\\varepsilon \\sin(x_0) / f'(x_0)|$.\n\nThe agreement between $\\Delta x_{\\text{obs}}$ and $\\Delta x_{\\text{pred}}$ for small $\\varepsilon$ provides empirical validation of our first-order sensitivity analysis. The condition number, which is large when $|f'(x_0)|$ is small, determines the magnitude of the root's shift for a given perturbation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the root-finding algorithms and computes the required outputs for the seven test cases.\n    \"\"\"\n    # Define global parameters for algorithms\n    TOL = 1e-14\n    MAX_ITER = 200\n\n    # Define the base function and its derivative\n    def f(x):\n        return x**3 - 3*x + 1\n\n    def f_prime(x):\n        return 3*x**2 - 3\n\n    # Define perturbation functions and their derivatives\n    def g_x(x): return x\n    def g_x_prime(x): return 1.0\n    def g_sin(x): return np.sin(x)\n    def g_sin_prime(x): return np.cos(x)\n\n    # --- Numerical Method Implementations ---\n\n    def bisection(func, a, b, tol=TOL, max_iter=MAX_ITER):\n        \"\"\"\n        Bisection method for root finding.\n        Returns the root and the number of iterations.\n        \"\"\"\n        fa = func(a)\n        if fa * func(b) >= 0:\n            # This should not happen with the given problem setup.\n            raise ValueError(\"Function has the same sign at interval endpoints.\")\n        \n        iterations = 0\n        while (b - a) > tol and iterations < max_iter:\n            iterations += 1\n            c = a + (b - a) / 2.0\n            fc = func(c)\n            if fc == 0.0:\n                break\n            if fa * fc < 0:\n                b = c\n            else:\n                a = c\n                fa = fc\n        return a + (b - a) / 2.0, iterations\n\n    def newton(func, func_prime, x0, tol=TOL, max_iter=MAX_ITER):\n        \"\"\"\n        Newton-Raphson method for root finding.\n        Returns the root and the number of iterations.\n        \"\"\"\n        xk = x0\n        iterations = 0\n        while iterations < max_iter:\n            iterations += 1\n            f_val = func(xk)\n            fp_val = func_prime(xk)\n            \n            # The problem specifies to proceed even if fp_val is very small.\n            if fp_val == 0.0:\n                # This would cause a division by zero.\n                # A robust solver might stop, but we follow instructions.\n                # In this specific problem, it doesn't happen at relevant points.\n                break \n\n            step = f_val / fp_val\n            xk_next = xk - step\n            \n            if abs(step) < tol:\n                return xk_next, iterations\n            \n            xk = xk_next\n        return xk, iterations\n\n    # --- Test Case Execution ---\n\n    results = []\n\n    # Case 1: Bisection on [1, 2], eps=1e-6, g(x)=x\n    eps1 = 1e-6\n    g1, _ = g_x, g_x_prime\n    f_eps1 = lambda x: f(x) + eps1 * g1(x)\n    \n    x0_c1, _ = bisection(f, 1.0, 2.0)\n    xe_c1, _ = bisection(f_eps1, 1.0, 2.0)\n    \n    dx_obs1 = abs(xe_c1 - x0_c1)\n    dx_pred1 = abs((eps1 * g1(x0_c1)) / f_prime(x0_c1))\n    results.extend([dx_obs1, dx_pred1])\n    \n    # Stash baseline root for Case 4\n    baseline_bisection_root_from_c1 = x0_c1\n\n    # Case 2: Newton, x_init=1.3, eps=1e-6, g(x)=x\n    eps2 = 1e-6\n    g2, g2_prime = g_x, g_x_prime\n    f_eps2 = lambda x: f(x) + eps2 * g2(x)\n    fp_eps2 = lambda x: f_prime(x) + eps2 * g2_prime(x)\n\n    x0_c2, _ = newton(f, f_prime, 1.3)\n    xe_c2, _ = newton(f_eps2, fp_eps2, 1.3)\n\n    dx_obs2 = abs(xe_c2 - x0_c2)\n    dx_pred2 = abs((eps2 * g2(x0_c2)) / f_prime(x0_c2))\n    results.extend([dx_obs2, dx_pred2])\n\n    # Stash baseline root for Case 3\n    baseline_newton_root_from_c2 = x0_c2\n\n    # Case 3: Newton, x_init=0.2, eps=0\n    x_c3, n_c3 = newton(f, f_prime, 0.2)\n    dx_c3 = abs(x_c3 - baseline_newton_root_from_c2)\n    results.extend([dx_c3, n_c3])\n\n    # Case 4: Bisection, [1.1, 2.0], eps=0\n    x_c4, n_c4 = bisection(f, 1.1, 2.0)\n    dx_c4 = abs(x_c4 - baseline_bisection_root_from_c1)\n    results.extend([dx_c4, n_c4])\n\n    # Case 5: Bisection on [0, 1], eps=1e-6, g(x)=x\n    eps5 = 1e-6\n    g5, _ = g_x, g_x_prime\n    f_eps5 = lambda x: f(x) + eps5 * g5(x)\n\n    x0_c5, _ = bisection(f, 0.0, 1.0)\n    xe_c5, _ = bisection(f_eps5, 0.0, 1.0)\n\n    dx_obs5 = abs(xe_c5 - x0_c5)\n    dx_pred5 = abs((eps5 * g5(x0_c5)) / f_prime(x0_c5))\n    results.extend([dx_obs5, dx_pred5])\n\n    # Case 6: Bisection on [-2, -1], eps=1e-6, g(x)=x\n    eps6 = 1e-6\n    g6, _ = g_x, g_x_prime\n    f_eps6 = lambda x: f(x) + eps6 * g6(x)\n\n    x0_c6, _ = bisection(f, -2.0, -1.0)\n    xe_c6, _ = bisection(f_eps6, -2.0, -1.0)\n\n    dx_obs6 = abs(xe_c6 - x0_c6)\n    dx_pred6 = abs((eps6 * g6(x0_c6)) / f_prime(x0_c6))\n    results.extend([dx_obs6, dx_pred6])\n\n    # Case 7: Newton, x_init=0.3, eps=1e-12, g(x)=sin(x)\n    eps7 = 1e-12\n    g7, g7_prime = g_sin, g_sin_prime\n    f_eps7 = lambda x: f(x) + eps7 * g7(x)\n    fp_eps7 = lambda x: f_prime(x) + eps7 * g7_prime(x)\n\n    x0_c7, _ = newton(f, f_prime, 0.3)\n    xe_c7, _ = newton(f_eps7, fp_eps7, 0.3)\n\n    dx_obs7 = abs(xe_c7 - x0_c7)\n    dx_pred7 = abs((eps7 * g7(x0_c7)) / f_prime(x0_c7))\n    results.extend([dx_obs7, dx_pred7])\n\n    # --- Final Output ---\n    # Convert all results to string, join with commas, and enclose in brackets.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3110311"}, {"introduction": "Our final practice scales up to the realm of linear algebra, tackling the eigenvalue problem, which is central to countless applications from quantum mechanics to data analysis. This exercise [@problem_id:3110268] demonstrates the critical role that matrix structure plays in numerical stability, contrasting the behavior of symmetric matrices with highly non-normal ones. By quantifying the sensitivity of eigenvalues and eigenvectors to small perturbations, you will discover why symmetric matrices are numerically \"nice\" (possessing well-conditioned eigenvalues) and how non-normal matrices can exhibit extreme sensitivity, a crucial lesson for any computational scientist.", "problem": "Consider perturbations of eigenvalues and eigenvectors in finite-dimensional real matrices under small additive changes. Use the following fundamental base: the definition of an eigenvalue and eigenvector of a matrix, the induced operator $2$-norm of a matrix, the spectral theorem for real symmetric matrices, and the notion of left and right eigenvectors for a non-symmetric diagonalizable matrix. Your program must quantify sensitivity using two complementary ideas: the geometry of eigenvector rotations for symmetric matrices, and the eigenvalue condition numbers based on left and right eigenvectors for non-normal matrices.\n\nDefinitions to be used in reasoning and implementation:\n- For a real matrix $M$, the induced operator $2$-norm is $\\|M\\|_{2}$, defined as the largest singular value of $M$.\n- For a real symmetric matrix $A$, let $\\lambda_{i}(A)$ denote its eigenvalues and $u_{i}(A)$ denote corresponding unit-norm eigenvectors.\n- For a diagonalizable real (possibly non-symmetric) matrix $B$, let $x_{i}(B)$ denote a right eigenvector and $y_{i}(B)$ denote a left eigenvector associated with the same (simple) eigenvalue $\\lambda_{i}(B)$, satisfying $B x_{i}(B) = \\lambda_{i}(B) x_{i}(B)$ and $y_{i}(B)^{\\mathsf{T}} B = \\lambda_{i}(B) y_{i}(B)^{\\mathsf{T}}$. The eigenvalue condition number is defined by\n$$\n\\kappa_{i}(B) = \\frac{\\|x_{i}(B)\\|_{2}\\,\\|y_{i}(B)\\|_{2}}{\\left|y_{i}(B)^{\\mathsf{T}} x_{i}(B)\\right|},\n$$\nwhich is independent of the scaling of $x_{i}(B)$ and $y_{i}(B)$.\n- For a target simple eigenvector $u_{i}(A)$ and a perturbed matrix $A + \\Delta$, define the principal angle $\\theta_{i}$ between $u_{i}(A)$ and the matched perturbed unit eigenvector $u_{i}(A+\\Delta)$ by $\\cos(\\theta_{i}) = \\left|u_{i}(A)^{\\mathsf{T}} u_{i}(A+\\Delta)\\right|$ and $\\sin(\\theta_{i}) = \\sqrt{1 - \\cos^{2}(\\theta_{i})}$. The spectral gap for $\\lambda_{i}(A)$ is $\\mathrm{gap}_{i} = \\min_{j \\neq i} \\left|\\lambda_{i}(A) - \\lambda_{j}(A)\\right|$.\n\nYour task is to implement a program that computes, for the specified test suite, the following sensitivity ratios:\n- Symmetric eigenvalue sensitivity ratio:\n$$\nr_{\\text{sym-eig}} = \\frac{\\max_{i} \\left|\\lambda_{i}(A+\\Delta) - \\lambda_{i}(A)\\right|}{\\|\\Delta\\|_{2}}.\n$$\nEigenvalues should be matched by proximity when comparing $A$ to $A+\\Delta$.\n- Symmetric eigenvector rotation ratio with a Davis–Kahan-type upper bound:\n$$\nr_{\\text{DK}} = \\frac{\\sin(\\theta_{i})}{\\min\\left(1, \\|\\Delta\\|_{2} / \\mathrm{gap}_{i}\\right)},\n$$\ncomputed for the index $i$ that achieves the minimum spectral gap.\n- Non-normal eigenvalue sensitivity ratio normalized by the eigenvalue condition numbers:\n$$\nr_{\\text{non-normal}} = \\max_{i} \\frac{\\left|\\lambda_{i}(B+\\Delta) - \\lambda_{i}(B)\\right|}{\\kappa_{i}(B)\\,\\|\\Delta\\|_{2}},\n$$\nwhere $\\kappa_{i}(B)$ is computed using the left and right eigenvectors of $B$ matched by eigenvalue proximity. Also report $\\kappa_{\\max}(B) = \\max_{i} \\kappa_{i}(B)$.\n\nTest suite matrices and perturbations:\n1. Symmetric, well-separated spectrum (happy path):\n$$\nA_{1} = \\begin{bmatrix}\n2 & 0.1 & 0 \\\\\n0.1 & 3 & 0.2 \\\\\n0 & 0.2 & 5\n\\end{bmatrix},\\quad\n\\Delta_{1} = 10^{-8}\\begin{bmatrix}\n0.5 & 0.1 & -0.05 \\\\\n0.1 & -0.2 & 0.02 \\\\\n-0.05 & 0.02 & 0.3\n\\end{bmatrix}.\n$$\nCompute $r_{\\text{sym-eig}}$ for $(A_{1}, \\Delta_{1})$.\n2. Symmetric, small spectral gap (boundary condition for eigenvector sensitivity):\n$$\nA_{2} = \\begin{bmatrix}\n1 & 10^{-3} & 0 \\\\\n10^{-3} & 1 & 0 \\\\\n0 & 0 & 4\n\\end{bmatrix},\\quad\n\\Delta_{2} = 10^{-6}\\begin{bmatrix}\n0.3 & -0.1 & 0 \\\\\n-0.1 & 0.2 & 0 \\\\\n0 & 0 & -0.4\n\\end{bmatrix}.\n$$\nIdentify the index $i$ in $A_{2}$ with the smallest spectral gap, match its eigenvector in $A_{2} + \\Delta_{2}$ by eigenvalue proximity, and compute $r_{\\text{DK}}$ for $(A_{2}, \\Delta_{2})$.\n3. Strongly non-normal, near-defective behavior (edge case for eigenvalue sensitivity):\n$$\nB_{1} = \\begin{bmatrix}\n1 & 1000 \\\\\n0 & 1.0001\n\\end{bmatrix},\\quad\n\\Delta_{3} = 10^{-8}\\begin{bmatrix}\n0 & 1 \\\\\n0 & 0\n\\end{bmatrix}.\n$$\nCompute $r_{\\text{non-normal}}$ for $(B_{1}, \\Delta_{3})$ and also compute $\\kappa_{\\max}(B_{1})$.\n\nFinal output format:\nYour program should produce a single line of output containing the four results\n$$\n\\left[r_{\\text{sym-eig}},\\; r_{\\text{DK}},\\; r_{\\text{non-normal}},\\; \\kappa_{\\max}(B_{1})\\right]\n$$\nas a comma-separated list enclosed in square brackets, in the exact order shown and using standard decimal notation for real numbers (no percentage signs).\n\nNo user input is required; all data are provided above. The program must be runnable as-is in a modern environment.", "solution": "The problem requires the computation of several sensitivity metrics for eigenvalues and eigenvectors of matrices under small additive perturbations. The analysis is divided into three cases, each designed to probe a specific aspect of matrix perturbation theory. The calculations rely on fundamental principles of numerical linear algebra, including the spectral theorem for symmetric matrices and the role of left and right eigenvectors in characterizing the conditioning of the eigenvalue problem for non-symmetric matrices. All mathematical entities are rendered in LaTeX as required.\n\nFirst, we address the sensitivity of eigenvalues of a real symmetric matrix. For a symmetric matrix $A$ and a symmetric perturbation $\\Delta$, the Weyl's inequality provides a strict bound on the perturbation of eigenvalues:\n$$\n\\max_{i} \\left|\\lambda_{i}(A+\\Delta) - \\lambda_{i}(A)\\right| \\le \\|\\Delta\\|_{2}.\n$$\nThis implies that the eigenvalue problem for a real symmetric matrix is always well-conditioned, with a condition number of $1$. The quantity $r_{\\text{sym-eig}}$ measures this relationship directly. For the first test case, we are given:\n$$\nA_{1} = \\begin{bmatrix}\n2 & 0.1 & 0 \\\\\n0.1 & 3 & 0.2 \\\\\n0 & 0.2 & 5\n\\end{bmatrix},\\quad\n\\Delta_{1} = 10^{-8}\\begin{bmatrix}\n0.5 & 0.1 & -0.05 \\\\\n0.1 & -0.2 & 0.02 \\\\\n-0.05 & 0.02 & 0.3\n\\end{bmatrix}.\n$$\nTo compute $r_{\\text{sym-eig}}$, we perform the following steps:\n$1$. Compute the eigenvalues of $A_1$ and $A_1 + \\Delta_1$. Since these are symmetric matrices, we use a numerically stable algorithm that returns sorted real eigenvalues. Let these be $\\{\\lambda_{i}(A_1)\\}$ and $\\{\\lambda_{i}(A_1+\\Delta_1)\\}$, respectively, ordered non-decreasingly.\n$2$. Calculate the maximum absolute difference between corresponding eigenvalues: $\\max_{i} |\\lambda_{i}(A_1+\\Delta_1) - \\lambda_{i}(A_1)|$.\n$3$. Compute the induced $2$-norm of the perturbation, $\\|\\Delta_1\\|_2$, which is its largest singular value. For a symmetric matrix, this is equal to the largest absolute eigenvalue of $\\Delta_1$.\n$4$. The ratio is then $r_{\\text{sym-eig}} = (\\max_{i} |\\lambda_{i}(A_1+\\Delta_1) - \\lambda_{i}(A_1)|) / \\|\\Delta_1\\|_2$. Based on Weyl's inequality, we expect $r_{\\text{sym-eig}} \\le 1$.\n\nSecond, we examine the sensitivity of eigenvectors of a symmetric matrix, which, unlike eigenvalues, can be highly sensitive if the corresponding eigenvalues are close. The Davis-Kahan theorem bounds the angle $\\theta_i$ between an original eigenvector $u_i(A)$ and its perturbed counterpart. A simplified version of this bound is $\\sin(\\theta_i) \\lesssim \\|\\Delta\\|_2 / \\mathrm{gap}_i$, where $\\mathrm{gap}_i = \\min_{j \\neq i} |\\lambda_i(A) - \\lambda_j(A)|$. The ratio $r_{\\text{DK}}$ is designed to test the tightness of this bound. We are given the second test case:\n$$\nA_{2} = \\begin{bmatrix}\n1 & 10^{-3} & 0 \\\\\n10^{-3} & 1 & 0 \\\\\n0 & 0 & 4\n\\end{bmatrix},\\quad\n\\Delta_{2} = 10^{-6}\\begin{bmatrix}\n0.3 & -0.1 & 0 \\\\\n-0.1 & 0.2 & 0 \\\\\n0 & 0 & -0.4\n\\end{bmatrix}.\n$$\nThe eigenvalues of $A_2$ are $\\lambda_1 = 1 - 10^{-3} = 0.999$, $\\lambda_2 = 1 + 10^{-3} = 1.001$, and $\\lambda_3 = 4$. The spectral gaps are $\\mathrm{gap}_1 = \\lambda_2 - \\lambda_1 = 0.002$, $\\mathrm{gap}_2 = \\lambda_2 - \\lambda_1 = 0.002$, and $\\mathrm{gap}_3 = \\lambda_3 - \\lambda_2 = 2.999$. The minimum spectral gap is $0.002$, associated with both $\\lambda_1$ and $\\lambda_2$. We choose the eigenvector corresponding to $\\lambda_1$ (index $i=1$, assuming $1$-based indexing or $i=0$ for $0$-based).\nThe steps to compute $r_{\\text{DK}}$ are:\n$1$. Compute the eigenvalues and eigenvectors of $A_2$. Identify the index $i$ with the minimum spectral gap, and the corresponding eigenvector $u_i(A_2)$ and gap $\\mathrm{gap}_i$.\n$2$. Compute the eigenvalues and eigenvectors of the perturbed matrix $A_2 + \\Delta_2$.\n$3$. Identify the perturbed eigenvector $u_i(A_2 + \\Delta_2)$ by finding the eigenvector whose corresponding eigenvalue is closest to $\\lambda_i(A_2)$.\n$4$. Calculate the sine of the principal angle between the two unit-norm eigenvectors: $\\sin(\\theta_i) = \\sqrt{1 - (u_i(A_2)^{\\mathsf{T}} u_i(A_2+\\Delta_2))^2}$. Note the absolute value in the problem's definition of cosine handles the sign ambiguity of eigenvectors.\n$5$. Compute the $2$-norm $\\|\\Delta_2\\|_2$.\n$6$. Compute the ratio $r_{\\text{DK}} = \\sin(\\theta_i) / \\min(1, \\|\\Delta_2\\|_2 / \\mathrm{gap}_i)$. We expect this ratio to be of order $1$.\n\nThird, we investigate the eigenvalue sensitivity of a non-symmetric (and non-normal) matrix. For a diagonalizable matrix $B$, the sensitivity of a simple eigenvalue $\\lambda_i$ is governed by its condition number $\\kappa_i(B) = \\frac{\\|x_i\\|_2\\|y_i\\|_2}{|y_i^{\\mathsf{T}}x_i|}$, where $x_i$ and $y_i$ are the corresponding right and left eigenvectors. A first-order perturbation bound is $|\\lambda_i(B+\\Delta) - \\lambda_i(B)| \\lesssim \\kappa_i(B)\\|\\Delta\\|_2$. The third test case uses a matrix known to be highly non-normal:\n$$\nB_{1} = \\begin{bmatrix}\n1 & 1000 \\\\\n0 & 1.0001\n\\end{bmatrix},\\quad\n\\Delta_{3} = 10^{-8}\\begin{bmatrix}\n0 & 1 \\\\\n0 & 0\n\\end{bmatrix}.\n$$\nThe eigenvalues of $B_1$ are its diagonal entries, $\\lambda_1=1$ and $\\lambda_2=1.0001$. They are very close.\nThe calculation requires two parts: finding $\\kappa_{\\max}(B_1)$ and $r_{\\text{non-normal}}$.\nTo find $\\kappa_i(B_1)$:\n$1$. For each eigenvalue $\\lambda_i$ of $B_1$, compute the right eigenvector $x_i$ from $B_1 x_i = \\lambda_i x_i$ and the left eigenvector $y_i$ from $y_i^{\\mathsf{T}} B_1 = \\lambda_i y_i^{\\mathsf{T}}$ (or $B_1^{\\mathsf{T}} y_i = \\lambda_i y_i$).\n$2$. Standard numerical libraries provide unit-norm eigenvectors, so $\\|x_i\\|_2 = 1$ and $\\|y_i\\|_2 = 1$. The formula simplifies to $\\kappa_i(B_1) = 1/|y_i^{\\mathsf{T}}x_i|$. Care must be taken to pair the correct left and right eigenvectors.\n$3$. $\\kappa_{\\max}(B_1)$ is the maximum of the computed $\\kappa_i(B_1)$.\nTo find $r_{\\text{non-normal}}$:\n$1$. Compute the eigenvalues of $B_1$ and the perturbed matrix $B_1 + \\Delta_3$.\n$2$. The eigenvalues of an upper triangular matrix are its diagonal entries. For $B_1 + \\Delta_3 = \\begin{bmatrix} 1  1000+10^{-8} \\\\ 0  1.0001 \\end{bmatrix}$, the eigenvalues remain $1$ and $1.0001$.\n$3$. The eigenvalue difference $|\\lambda_i(B_1+\\Delta_3) - \\lambda_i(B_1)|$ is exactly $0$ for both eigenvalues in this specific case.\n$4$. Compute $\\|\\Delta_3\\|_2$.\n$5$. The ratio terms are $\\frac{|\\lambda_i(B+\\Delta) - \\lambda_i(B)|}{\\kappa_{i}(B)\\,\\|\\Delta\\|_{2}} = \\frac{0}{\\kappa_{i}(B)\\,\\|\\Delta\\|_{2}} = 0$.\n$6$. Thus, $r_{\\text{non-normal}} = \\max_i(0) = 0$. This specific perturbation does not alter the eigenvalues, despite the large condition numbers, illustrating that the bound is not always tight and sensitivity is directional.\nWe now proceed with the numerical implementation of these steps.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes sensitivity ratios for eigenvalues and eigenvectors for three test cases.\n    \"\"\"\n    \n    # --- Case 1: Symmetric eigenvalue sensitivity ---\n    A1 = np.array([\n        [2.0, 0.1, 0.0],\n        [0.1, 3.0, 0.2],\n        [0.0, 0.2, 5.0]\n    ])\n    Delta1 = 1e-8 * np.array([\n        [0.5, 0.1, -0.05],\n        [0.1, -0.2, 0.02],\n        [-0.05, 0.02, 0.3]\n    ])\n\n    A1_pert = A1 + Delta1\n    \n    # numpy.linalg.eigh returns sorted eigenvalues for symmetric matrices\n    e_vals_A1, _ = np.linalg.eigh(A1)\n    e_vals_A1_pert, _ = np.linalg.eigh(A1_pert)\n    \n    max_eig_diff_A1 = np.max(np.abs(e_vals_A1_pert - e_vals_A1))\n    norm_Delta1 = np.linalg.norm(Delta1, ord=2)\n    \n    r_sym_eig = max_eig_diff_A1 / norm_Delta1\n\n    # --- Case 2: Symmetric eigenvector rotation ---\n    A2 = np.array([\n        [1.0, 1e-3, 0.0],\n        [1e-3, 1.0, 0.0],\n        [0.0, 0.0, 4.0]\n    ])\n    Delta2 = 1e-6 * np.array([\n        [0.3, -0.1, 0.0],\n        [-0.1, 0.2, 0.0],\n        [0.0, 0.0, -0.4]\n    ])\n\n    A2_pert = A2 + Delta2\n    \n    e_vals_A2, e_vecs_A2 = np.linalg.eigh(A2)\n    e_vals_A2_pert, e_vecs_A2_pert = np.linalg.eigh(A2_pert)\n\n    # Identify index `i` with the minimum spectral gap.\n    # eigh sorts eigenvalues, so the smallest gap is between adjacent eigenvalues.\n    gaps = np.diff(e_vals_A2)\n    min_gap = np.min(gaps)\n    # The minimum gap is between the first two eigenvalues. We select i=0.\n    i_min_gap = np.argmin(gaps)\n    \n    gap_i = min_gap\n    u_i = e_vecs_A2[:, i_min_gap]\n    \n    # The perturbed eigenvector is also at index 0 because the perturbation is small.\n    u_i_pert = e_vecs_A2_pert[:, i_min_gap]\n    \n    # Calculate sin(theta)\n    cos_theta = np.abs(u_i.T @ u_i_pert)\n    # Clamp to 1.0 to avoid domain errors with sqrt due to floating point inaccuracies\n    if cos_theta  1.0:\n        cos_theta = 1.0\n    sin_theta = np.sqrt(1.0 - cos_theta**2)\n\n    norm_Delta2 = np.linalg.norm(Delta2, ord=2)\n    \n    r_DK_denominator = min(1.0, norm_Delta2 / gap_i)\n    r_DK = sin_theta / r_DK_denominator if r_DK_denominator  0 else 0.0\n    \n    # --- Case 3: Non-normal eigenvalue sensitivity ---\n    B1 = np.array([\n        [1.0, 1000.0],\n        [0.0, 1.0001]\n    ])\n    Delta3 = 1e-8 * np.array([\n        [0.0, 1.0],\n        [0.0, 0.0]\n    ])\n\n    # Eigenvalues and right eigenvectors of B1\n    e_vals_B1, r_vecs_B1 = np.linalg.eig(B1)\n\n    # Eigenvalues and left eigenvectors of B1 (eigenvectors of B1.T)\n    # The eigenvalues are complex-typed by default, so we sort on the real part\n    e_vals_B1T, l_vecs_B1_raw = np.linalg.eig(B1.T)\n\n    # Sort eigenvalues and eigenvectors to ensure correct pairing\n    sort_idx_r = np.argsort(e_vals_B1.real)\n    e_vals_B1_s = e_vals_B1[sort_idx_r]\n    r_vecs_B1_s = r_vecs_B1[:, sort_idx_r]\n\n    sort_idx_l = np.argsort(e_vals_B1T.real)\n    l_vecs_B1_s = l_vecs_B1_raw[:, sort_idx_l]\n    \n    # Calculate eigenvalue condition numbers kappa_i\n    kappas = np.zeros_like(e_vals_B1_s, dtype=float)\n    for i in range(len(e_vals_B1_s)):\n        # Eigenvectors from np.linalg.eig are already normalized to unit 2-norm\n        # kappa_i = ||x||*||y|| / |y.T @ x| simplifies to 1 / |y.T @ x|\n        # Use .conj() for robustness, though not strictly needed for real matrices\n        denom = np.abs(np.dot(l_vecs_B1_s[:, i].conj(), r_vecs_B1_s[:, i]))\n        kappas[i] = 1.0 / denom if denom  0 else np.inf\n    \n    kappa_max_B1 = np.max(kappas)\n    \n    # Calculate r_non_normal\n    B1_pert = B1 + Delta3\n    e_vals_B1_pert, _ = np.linalg.eig(B1_pert)\n    e_vals_B1_pert_s = np.sort(e_vals_B1_pert.real) # Sort for matching\n\n    max_eig_diff_B1 = np.abs(e_vals_B1_pert_s - e_vals_B1_s)\n    \n    norm_Delta3 = np.linalg.norm(Delta3, ord=2)\n    \n    # Denominator for the ratio\n    sens_denom = kappas * norm_Delta3\n    r_non_normal_terms = np.zeros_like(sens_denom)\n    # Avoid division by zero if sensitivity denominator is zero\n    non_zero_idx = sens_denom  0\n    r_non_normal_terms[non_zero_idx] = max_eig_diff_B1[non_zero_idx] / sens_denom[non_zero_idx]\n    \n    r_non_normal = np.max(r_non_normal_terms)\n\n    # --- Final Output ---\n    results = [r_sym_eig, r_DK, r_non_normal, kappa_max_B1]\n    \n    # Print in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3110268"}]}