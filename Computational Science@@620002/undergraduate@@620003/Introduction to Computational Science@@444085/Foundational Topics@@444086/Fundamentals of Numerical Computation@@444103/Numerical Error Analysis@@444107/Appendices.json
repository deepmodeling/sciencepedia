{"hands_on_practices": [{"introduction": "Directly evaluating a mathematically sound formula can sometimes lead to surprisingly inaccurate results in floating-point arithmetic. This exercise [@problem_id:3212241] explores the classic case of the sinc function, $f(x) = \\frac{\\sin x}{x}$, where computing the small decrement $d(x) = 1 - f(x)$ for $x$ near zero suffers from catastrophic cancellation. You will analyze this instability and implement a robust hybrid algorithm that switches to a Taylor series expansion to preserve accuracy, a fundamental technique in scientific software development.", "problem": "Consider the function $f(x) = \\begin{cases}\\dfrac{\\sin x}{x}, & x \\neq 0,\\\\ 1, & x = 0.\\end{cases}$ evaluated in radians. The aim is to study loss of significance for small $x$ when computing $f(x)$ using standard floating-point arithmetic and to design a numerically stable alternative for accurately capturing the small decrement $d(x) = 1 - f(x)$.\n\nStart from the following foundational bases:\n- The Taylor series for the sine function about $x = 0$: $\\sin x = x - \\dfrac{x^{3}}{3!} + \\dfrac{x^{5}}{5!} - \\cdots$.\n- The induced series for $f(x)$ and the associated decrement $d(x)$:\n$$f(x) = 1 - \\dfrac{x^{2}}{3!} + \\dfrac{x^{4}}{5!} - \\cdots,\\quad d(x) = 1 - f(x) = \\dfrac{x^{2}}{3!} - \\dfrac{x^{4}}{5!} + \\dfrac{x^{6}}{7!} - \\cdots.$$\n- The standard floating-point model: for a basic operation $\\circ \\in \\{+,-,\\times,\\div\\}$, $\\operatorname{fl}(a \\circ b) = (a \\circ b)(1 + \\delta)$ with $|\\delta| \\le u$, where $u$ is the unit roundoff of the working precision.\n\nTasks:\n1. Using the above bases, reason about why directly computing $\\operatorname{fl}(\\sin x)/\\operatorname{fl}(x)$ has small relative error for $f(x)$ when $x$ is small, yet can completely lose information about the decrement $d(x)$ due to subtractive cancellation in forming $1 - \\operatorname{fl}(\\sin x)/\\operatorname{fl}(x)$. Derive a scale for the threshold in terms of $u$ at which the naive computation cannot reliably resolve $d(x)$ (express this threshold asymptotically in $u$).\n2. Design a stable series-based evaluation that:\n   - For $|x|$ below a threshold $\\,\\tau\\,$ based on $u$, computes $d(x)$ via its alternating series and then forms $f(x) = 1 - d(x)$ only at the end, using a numerically stable summation strategy.\n   - For $|x| \\ge \\tau$, uses the direct evaluation $f(x) = \\sin(x)/x$.\n   - Handles the removable singularity at $x = 0$ by returning $f(0) = 1$.\n3. Implement a complete, runnable program that:\n   - Uses standard double-precision floating-point arithmetic for the primary computations.\n   - Uses a high-precision reference based on a truncated series computed with arbitrary precision arithmetic to approximate the true values of $f(x)$ and $d(x)$ to many correct digits.\n   - For each test input $x$, computes three error metrics:\n     - The relative error of the naive $f(x)$: $\\left|\\dfrac{f_{\\text{naive}} - f_{\\text{ref}}}{f_{\\text{ref}}}\\right|$.\n     - The relative error of the naive decrement $d_{\\text{naive}} = 1 - f_{\\text{naive}}$ against $d_{\\text{ref}}$: $\\left|\\dfrac{d_{\\text{naive}} - d_{\\text{ref}}}{d_{\\text{ref}}}\\right|$ (define this as $0$ if $d_{\\text{ref}} = 0$).\n     - The relative error of the stable series decrement $d_{\\text{stable}}$ against $d_{\\text{ref}}$: $\\left|\\dfrac{d_{\\text{stable}} - d_{\\text{ref}}}{d_{\\text{ref}}}\\right|$ (define this as $0$ if $d_{\\text{ref}} = 0$).\n   - Uses radians and no physical units.\n\nTest suite:\n- Evaluate the program for the inputs $x \\in \\{0,\\;10^{-12},\\;10^{-10},\\;10^{-8},\\;-10^{-7},\\;10^{-7},\\;10^{-4},\\;10^{-1}\\}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of lists, where each inner list corresponds to one test input and contains the three error metrics in the order specified above. For example: \"[[e11,e12,e13],[e21,e22,e23],...]\" with all entries as floating-point numbers. The angle unit is radians, and no percentage symbols are to be used anywhere in the computation or output.", "solution": "The problem is subjected to validation against the established criteria.\n\n### Step 1: Extract Givens\n- **Function definition**: $f(x) = \\begin{cases}\\dfrac{\\sin x}{x}, & x \\neq 0,\\\\ 1, & x = 0.\\end{cases}$ evaluated in radians.\n- **Decrement definition**: $d(x) = 1 - f(x)$.\n- **Taylor series for sine**: $\\sin x = x - \\dfrac{x^{3}}{3!} + \\dfrac{x^{5}}{5!} - \\cdots$.\n- **Series for $f(x)$ and $d(x)$**: $f(x) = 1 - \\dfrac{x^{2}}{3!} + \\dfrac{x^{4}}{5!} - \\cdots$, and $d(x) = \\dfrac{x^{2}}{3!} - \\dfrac{x^{4}}{5!} + \\dfrac{x^{6}}{7!} - \\cdots$.\n- **Floating-point model**: $\\operatorname{fl}(a \\circ b) = (a \\circ b)(1 + \\delta)$ with $|\\delta| \\le u$, where $u$ is the unit roundoff.\n- **Task 1**: Analyze why direct computation of $f(x)$ has small relative error but computing $d(x)$ via $1 - f(x)$ suffers from catastrophic cancellation for small $x$. Derive the threshold scale in terms of $u$.\n- **Task 2**: Design a stable hybrid algorithm using a series expansion for $|x| < \\tau$ and direct evaluation for $|x| \\ge \\tau$.\n- **Task 3**: Implement a program to compute three specified error metrics (relative error of naive $f(x)$, naive $d(x)$, and stable $d(x)$) against a high-precision reference for a given test suite.\n- **Execution Environment and Precision**: Primary computations in double-precision, reference in arbitrary-precision.\n- **Test Suite**: $x \\in \\{0,\\;10^{-12},\\;10^{-10},\\;10^{-8},\\;-10^{-7},\\;10^{-7},\\;10^{-4},\\;10^{-1}\\}$.\n- **Output Format**: A single line string representing a list of lists of error metrics: `[[e11,e12,e13],[e21,e22,e23],...]`.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard exercise in numerical analysis focusing on catastrophic cancellation, a core concept in scientific computing.\n- **Scientifically Grounded (Critical)**: The problem is fundamentally based on Taylor series expansions and the standard model of floating-point arithmetic, which are cornerstones of numerical methods. It is scientifically sound.\n- **Well-Posed**: The problem is well-posed. It clearly defines the function, the quantities to be computed, the methods to be analyzed, and the expected output. A unique and meaningful solution exists.\n- **Objective (Critical)**: The language is precise and mathematical. There are no subjective or opinion-based statements.\nThe problem is self-contained and consistent. The request for an \"arbitrary precision\" reference is satisfied by using the `decimal` module from Python's standard library, which is permitted. All other criteria are met.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n---\n\n### Analysis of Numerical Error and Algorithm Design\n\nThis solution addresses the three tasks outlined in the problem statement: analyzing the numerical instability, designing a stable algorithm, and implementing it for verification.\n\n#### 1. Analysis of Numerical Errors in Naive Computation\n\nWe analyze the errors in computing $f(x)$ and $d(x)$ for small values of $x$ using standard floating-point arithmetic.\n\n**Error in computing $f(x)$:**\nThe \"naive\" computation of $f(x)$ for $x \\neq 0$ is $f_{\\text{naive}}(x) = \\operatorname{fl}(\\sin(x) / x)$. Let's model the floating-point errors. The evaluation of the sine function and the division introduce relative errors bounded by the unit roundoff $u$.\nFirst, the sine function is computed: $\\operatorname{fl}(\\sin x) = (\\sin x)(1 + \\delta_1)$, where $|\\delta_1| \\le u$.\nThen, the division is performed:\n$$ \\operatorname{fl}\\left(\\frac{\\operatorname{fl}(\\sin x)}{x}\\right) = \\left(\\frac{(\\sin x)(1 + \\delta_1)}{x}\\right)(1 + \\delta_2) = \\frac{\\sin x}{x}(1 + \\delta_1)(1 + \\delta_2) $$\nwhere $|\\delta_2| \\le u$. Assuming $x$ is an exact floating-point number.\nLet $\\hat{f}(x)$ denote the computed value. Expanding the error terms, we get:\n$$ \\hat{f}(x) = f(x)(1 + \\delta_1 + \\delta_2 + \\delta_1\\delta_2) \\approx f(x)(1 + \\delta_{\\text{f}}) $$\nwhere $\\delta_{\\text{f}} = \\delta_1 + \\delta_2$. The total relative error in computing $f(x)$ is:\n$$ \\left|\\frac{\\hat{f}(x) - f(x)}{f(x)}\\right| \\approx |\\delta_{\\text{f}}| \\le |\\delta_1| + |\\delta_2| \\le 2u $$\nThis relative error is small, on the order of the unit roundoff $u$. Therefore, the direct computation of $f(x)$ is numerically stable across its domain.\n\n**Error in computing $d(x) = 1 - f(x)$:**\nThe naive computation of the decrement is $\\hat{d}(x) = \\operatorname{fl}(1 - \\hat{f}(x))$. For small $x$, we know from the Taylor series that $f(x) = 1 - \\frac{x^2}{6} + O(x^4)$, which is very close to $1$. The subtraction $1 - \\hat{f}(x)$ is a classic example of catastrophic cancellation, where two nearly equal numbers are subtracted, leading to a potential loss of relative precision.\n\nLet's analyze this more formally. The computed decrement is:\n$$ \\hat{d}(x) = \\operatorname{fl}(1 - \\hat{f}(x)) = (1 - \\hat{f}(x))(1 + \\delta_3) \\quad \\text{with } |\\delta_3| \\le u $$\nSubstituting the expression for $\\hat{f}(x)$:\n$$ \\hat{d}(x) = (1 - f(x)(1 + \\delta_{\\text{f}}))(1 + \\delta_3) = (1 - f(x) - f(x)\\delta_{\\text{f}})(1 + \\delta_3) $$\nSince $d(x) = 1 - f(x)$, this becomes:\n$$ \\hat{d}(x) = (d(x) - f(x)\\delta_{\\text{f}})(1 + \\delta_3) = d(x) - f(x)\\delta_{\\text{f}} + d(x)\\delta_3 - f(x)\\delta_{\\text{f}}\\delta_3 $$\nThe absolute error is $\\hat{d}(x) - d(x) \\approx -f(x)\\delta_{\\text{f}} + d(x)\\delta_3$.\nThe relative error in $d(x)$ is:\n$$ \\frac{\\hat{d}(x) - d(x)}{d(x)} \\approx \\frac{-f(x)\\delta_{\\text{f}} + d(x)\\delta_3}{d(x)} = -\\frac{f(x)}{d(x)}\\delta_{\\text{f}} + \\delta_3 $$\nFor small $x$, we have $f(x) \\approx 1$ and $d(x) \\approx x^2/6$. The error is dominated by the first term:\n$$ \\left|\\frac{\\hat{d}(x) - d(x)}{d(x)}\\right| \\approx \\left|-\\frac{f(x)}{d(x)}\\delta_{\\text{f}}\\right| \\approx \\frac{1}{x^2/6}|\\delta_{\\text{f}}| = \\frac{6|\\delta_{\\text{f}}|}{x^2} $$\nUsing the bound $|\\delta_{\\text{f}}| \\le 2u$, the relative error is bounded by:\n$$ \\left|\\frac{\\hat{d}(x) - d(x)}{d(x)}\\right| \\lesssim \\frac{12u}{x^2} $$\nThe naive computation becomes unreliable when this relative error is of order $1$ or greater. This happens when $12u/x^2 \\approx 1$, which gives the threshold:\n$$ |x| \\approx \\sqrt{12u} $$\nFor double-precision arithmetic, $u = 2^{-53} \\approx 1.11 \\times 10^{-16}$. The threshold is $|x| \\approx \\sqrt{12 \\times 1.11 \\times 10^{-16}} \\approx 3.65 \\times 10^{-8}$. For values of $|x|$ at or below this scale, the naive computation of $d(x)$ loses most or all of its significant digits. The threshold is asymptotically of order $O(\\sqrt{u})$.\n\n#### 2. Design of a Stable Algorithm\n\nTo overcome the catastrophic cancellation, we design a hybrid algorithm that avoids the problematic subtraction for small $|x|$.\nThe algorithm is as follows:\n- A threshold $\\tau$ is chosen based on the error analysis. A practical choice for double precision is $\\tau = 10^{-7}$, which is slightly larger than the theoretical onset of total significance loss ($\\approx 3.65 \\times 10^{-8}$), providing a safe margin.\n- **For $|x| < \\tau$**: Instead of computing $f(x)$ first, we directly compute the decrement $d(x)$ using its Taylor series expansion:\n  $$ d(x) = \\frac{x^{2}}{3!} - \\frac{x^{4}}{5!} + \\frac{x^{6}}{7!} - \\cdots = \\sum_{k=1}^{\\infty} (-1)^{k-1} \\frac{x^{2k}}{(2k+1)!} $$\n  This is an alternating series whose terms decrease rapidly in magnitude for $|x| < 1$. We can compute $d(x)$ accurately by summing a few terms. This approach is stable because it constructs the small value $d(x)$ by summing other small, accurately-represented values, rather than as the difference of two large, nearly-equal numbers. Once an accurate value for $d_{\\text{stable}}(x)$ is found, $f(x)$ can be computed as $f_{\\text{stable}}(x) = 1 - d_{\\text{stable}}(x)$. This final subtraction is not subject to catastrophic cancellation because $d_{\\text{stable}}(x)$ is a small, accurately computed number.\n- **For $|x| \\ge \\tau$**: The naive computation $f(x) = \\sin(x)/x$ is used. In this regime, $f(x)$ is not sufficiently close to $1$ for the subtraction $1-f(x)$ to cause a catastrophic loss of precision in the context of double-precision numbers. The series evaluation would also become less efficient and potentially less accurate as more terms are needed for convergence.\n- **For $x=0$**: The function is handled as a special case, returning $f(0) = 1$ and $d(0)=0$ as per the definition.\n\n#### 3. Implementation and Verification\n\nThe implementation will consist of three main parts:\n1.  A high-precision reference calculation for $f_{\\text{ref}}$ and $d_{\\text{ref}}$ using Python's `decimal` module. The series for $d(x)$ is well-suited for this, summed to a high number of terms to ensure convergence to a precision of $50$ digits.\n2.  The naive computation functions, $f_{\\text{naive}}(x) = \\sin(x)/x$ and $d_{\\text{naive}}(x) = 1 - f_{\\text{naive}}(x)$.\n3.  The stable computation function, which implements the hybrid algorithm described above to find $d_{\\text{stable}}(x)$.\n\nFor each input $x$ from the test suite, we will compute the three requested relative errors:\n- $\\epsilon_1 = \\left|\\dfrac{f_{\\text{naive}} - f_{\\text{ref}}}{f_{\\text{ref}}}\\right|$\n- $\\epsilon_2 = \\left|\\dfrac{d_{\\text{naive}} - d_{\\text{ref}}}{d_{\\text{ref}}}\\right|$\n- $\\epsilon_3 = \\left|\\dfrac{d_{\\text{stable}} - d_{\\text{ref}}}{d_{\\text{ref}}}\\right|$\n\nFor the case $x=0$, $d_{\\text{ref}}=0$, and the error metrics $\\epsilon_2$ and $\\epsilon_3$ are defined to be $0$.", "answer": "```python\nimport numpy as np\nfrom decimal import Decimal, getcontext\n\ndef solve():\n    \"\"\"\n    Solves the numerical analysis problem of computing f(x) = sin(x)/x and d(x) = 1-f(x).\n    It compares a naive method with a stable, series-based method against a high-precision reference.\n    \"\"\"\n\n    # Set precision for the high-precision reference calculation. 50 digits is sufficient.\n    getcontext().prec = 50\n\n    def reference_d(x_str: str) -> Decimal:\n        \"\"\"Computes d(x) to high precision using its Taylor series.\"\"\"\n        if x_str == '0':\n            return Decimal(0)\n\n        x = Decimal(x_str)\n        x_sq = x * x\n        \n        # d(x) = sum_{k=1 to inf} (-1)^(k-1) * x^(2k) / (2k+1)!\n        # term_{k} = (-1)^(k-1) * x^(2k) / (2k+1)!\n        # The ratio term_{k+1} / term_{k} = -x^2 / ((2k+2)(2k+3))\n\n        k = 1\n        term = x_sq / Decimal(6)  # First term (k=1)\n        d_val = term\n        \n        # Sum until the next term is smaller than the context precision\n        while abs(term) > Decimal('1e-50'):\n            k += 1\n            term *= -x_sq / Decimal((2 * k) * (2 * k + 1))\n            d_val += term\n        \n        return d_val\n\n    def naive_f(x: float) -> float:\n        \"\"\"Computes f(x) = sin(x)/x naively.\"\"\"\n        if x == 0.0:\n            return 1.0\n        return np.sin(x) / x\n\n    def stable_d(x: float, threshold: float) -> float:\n        \"\"\"\n        Computes d(x) using a stable hybrid algorithm.\n        \"\"\"\n        if x == 0.0:\n            return 0.0\n        \n        if abs(x) < threshold:\n            # For small |x|, use the Taylor series for d(x) to avoid cancellation.\n            # d(x) = x^2/6 - x^4/120 + x^6/5040 - ...\n            x_sq = x * x\n            \n            # term_{k+1} = term_{k} * (-x^2) / ((2k+2)(2k+3))\n            k = 1\n            term = x_sq / 6.0  # First term (k=1)\n            d_val = term\n            \n            # Sum until convergence at double precision\n            for k_iter in range(2, 15): # 15 iterations is more than enough\n                term *= -x_sq / ((2 * k_iter) * (2 * k_iter + 1))\n                d_val_prev = d_val\n                d_val += term\n                if d_val == d_val_prev:\n                    break\n            return d_val\n        else:\n            # For larger |x|, direct computation is stable enough.\n            f_val = np.sin(x) / x\n            return 1.0 - f_val\n\n    # Define test cases from the problem statement\n    test_cases_str = ['0', '1e-12', '1e-10', '1e-8', '-1e-7', '1e-7', '1e-4', '1e-1']\n    test_cases_float = [float(x) for x in test_cases_str]\n    \n    # Threshold for switching to series expansion, based on O(sqrt(u)) analysis\n    # For double precision, u ~ 10^-16, sqrt(u) ~ 10^-8. 10^-7 is a safe choice.\n    threshold = 1e-7\n\n    all_results = []\n    for x_str, x_float in zip(test_cases_str, test_cases_float):\n        # 1. High-precision reference calculation\n        d_ref_dec = reference_d(x_str)\n        f_ref_dec = Decimal(1) - d_ref_dec\n        d_ref = float(d_ref_dec)\n        f_ref = float(f_ref_dec)\n\n        # 2. Naive computation\n        f_naive_val = naive_f(x_float)\n        d_naive_val = 1.0 - f_naive_val\n\n        # 3. Stable computation for the decrement\n        d_stable_val = stable_d(x_float, threshold)\n\n        # 4. Compute error metrics\n        if x_float == 0.0:\n            err_f_naive = 0.0\n            err_d_naive = 0.0\n            err_d_stable = 0.0\n        else:\n            # Relative error of naive f(x)\n            err_f_naive = abs((f_naive_val - f_ref) / f_ref) if f_ref != 0 else 0.0\n            \n            if d_ref == 0.0:\n                # This branch should not be taken for x != 0\n                err_d_naive = 0.0 if d_naive_val == 0.0 else float('inf')\n                err_d_stable = 0.0 if d_stable_val == 0.0 else float('inf')\n            else:\n                # Relative error of naive d(x)\n                err_d_naive = abs((d_naive_val - d_ref) / d_ref)\n                # Relative error of stable d(x)\n                err_d_stable = abs((d_stable_val - d_ref) / d_ref)\n        \n        all_results.append([err_f_naive, err_d_naive, err_d_stable])\n\n    # Format output as a string representing a list of lists, without spaces.\n    formatted_results = [f\"[{r[0]},{r[1]},{r[2]}]\" for r in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3212241"}, {"introduction": "Building on the concept of catastrophic cancellation, this practice [@problem_id:3165829] examines the subtraction of two nearly equal exponential terms, a common pattern in models of decay processes. Rather than relying on a series expansion, you will use algebraic manipulation to reformulate the problematic expression, $S(t; \\tau_1, \\tau_2) = \\exp(-t/\\tau_1) - \\exp(-t/\\tau_2)$, into a stable equivalent. This exercise highlights the power of leveraging built-in, highly accurate functions like $\\mathrm{expm1}(x)$ to create code that is both efficient and numerically sound.", "problem": "Consider computing the bi-exponential difference $S(t;\\tau_1,\\tau_2) = \\exp(-t/\\tau_1) - \\exp(-t/\\tau_2)$ for $t \\ge 0$ and strictly positive time constants $\\tau_1 > 0$ and $\\tau_2 > 0$ that may be close, $\\tau_1 \\approx \\tau_2$. Work in the standard floating-point rounding model of the Institute of Electrical and Electronics Engineers (IEEE) $754$ binary64 arithmetic (commonly called double precision), where each basic operation is modeled as $\\mathrm{fl}(z) = z(1+\\delta)$ with $|\\delta| \\le \\epsilon_{\\mathrm{mach}}$ for machine epsilon $\\epsilon_{\\mathrm{mach}}$. Use the core definitions of absolute error $|x - \\hat{x}|$ and relative error $|x - \\hat{x}|/|x|$, and the notion of catastrophic cancellation when subtracting nearly equal quantities.\n\nStarting from these foundations, analyze the numerical stability of the naive subtraction that evaluates $S(t;\\tau_1,\\tau_2)$ as computed in floating point by forming the two exponentials separately and then subtracting. Derive a bound on the relative error of the naive subtraction in terms of $\\epsilon_{\\mathrm{mach}}$, $|\\exp(-t/\\tau_1)|$, $|\\exp(-t/\\tau_2)|$, and $|S(t;\\tau_1,\\tau_2)|$, and explain why the bound becomes large as $\\tau_1 \\to \\tau_2$ (catastrophic cancellation). Then, by exact algebraic manipulation grounded in the properties of the exponential function and exploiting the special function $\\mathrm{expm1}(x)$, which evaluates $\\exp(x) - 1$ with high relative accuracy for small $|x|$, derive an algebraically equivalent expression for $S(t;\\tau_1,\\tau_2)$ that avoids the subtraction of nearly equal numbers by expressing the difference as a single scaled $\\mathrm{expm1}$ call with a small argument when $\\tau_1 \\approx \\tau_2$.\n\nReport only the final closed-form stable expression for $S(t;\\tau_1,\\tau_2)$ in terms of $t$, $\\tau_1$, $\\tau_2$, and $\\mathrm{expm1}(\\cdot)$. No numerical evaluation is required.", "solution": "The problem requires an analysis of the numerical stability of computing the bi-exponential difference $S(t;\\tau_1,\\tau_2) = \\exp(-t/\\tau_1) - \\exp(-t/\\tau_2)$ and the derivation of a numerically stable alternative expression. The analysis will proceed in two parts as requested: first, an error analysis of the naive computational approach, and second, the derivation of a stable formula using algebraic reformulation.\n\nWe begin with the error analysis. The problem is stated for $t \\ge 0$, $\\tau_1 > 0$, and $\\tau_2 > 0$, with a focus on the case where $\\tau_1 \\approx \\tau_2$. Let the true values of the terms be $x_1 = \\exp(-t/\\tau_1)$ and $x_2 = \\exp(-t/\\tau_2)$. The exact difference is $S = x_1 - x_2$.\n\nIn floating-point arithmetic, the naive computation proceeds by first evaluating the two exponential terms and then subtracting them. Let $\\hat{x}_1$ and $\\hat{x}_2$ be the computed, floating-point representations of $x_1$ and $x_2$. We use the standard model of floating-point arithmetic, where the evaluation of a function like $\\exp$ and a basic arithmetic operation introduces a small relative error. Thus, we can model the computed values as:\n$$ \\hat{x}_1 = \\mathrm{fl}(\\exp(-t/\\tau_1)) = x_1(1+\\delta_1), \\quad |\\delta_1| \\le \\epsilon_{\\mathrm{mach}} $$\n$$ \\hat{x}_2 = \\mathrm{fl}(\\exp(-t/\\tau_2)) = x_2(1+\\delta_2), \\quad |\\delta_2| \\le \\epsilon_{\\mathrm{mach}} $$\nHere, $\\epsilon_{\\mathrm{mach}}$ is the machine epsilon, and we assume that the errors from computing the arguments $-t/\\tau_1$ and $-t/\\tau_2$ are incorporated into the total error of evaluating the exponentials, which is a standard simplification for this level of analysis. The final computed value, $\\hat{S}$, is the result of subtracting these two computed values in floating-point arithmetic:\n$$ \\hat{S} = \\mathrm{fl}(\\hat{x}_1 - \\hat{x}_2) = (\\hat{x}_1 - \\hat{x}_2)(1+\\delta_3), \\quad |\\delta_3| \\le \\epsilon_{\\mathrm{mach}} $$\nTo analyze the error, we find the difference between the computed value $\\hat{S}$ and the true value $S$.\n$$ \\hat{S} = (x_1(1+\\delta_1) - x_2(1+\\delta_2))(1+\\delta_3) $$\n$$ \\hat{S} = (x_1 - x_2 + x_1\\delta_1 - x_2\\delta_2)(1+\\delta_3) $$\nSince $S = x_1 - x_2$, we have:\n$$ \\hat{S} = (S + x_1\\delta_1 - x_2\\delta_2)(1+\\delta_3) $$\nExpanding this expression gives:\n$$ \\hat{S} = S + S\\delta_3 + x_1\\delta_1 - x_2\\delta_2 + (x_1\\delta_1 - x_2\\delta_2)\\delta_3 $$\nThe absolute error is $\\hat{S} - S$:\n$$ \\hat{S} - S = S\\delta_3 + x_1\\delta_1 - x_2\\delta_2 + (x_1\\delta_1 - x_2\\delta_2)\\delta_3 $$\nNeglecting the higher-order term $(x_1\\delta_1 - x_2\\delta_2)\\delta_3$, which is proportional to $\\epsilon_{\\mathrm{mach}}^2$, the absolute error is approximately:\n$$ \\hat{S} - S \\approx x_1\\delta_1 - x_2\\delta_2 + S\\delta_3 $$\nThe relative error is $\\frac{|\\hat{S} - S|}{|S|}$. Using the triangle inequality on the approximation for the absolute error, we can establish a bound:\n$$ |\\hat{S} - S| \\lesssim |x_1\\delta_1| + |x_2\\delta_2| + |S\\delta_3| $$\n$$ |\\hat{S} - S| \\lesssim |x_1||\\delta_1| + |x_2||\\delta_2| + |S||\\delta_3| $$\nSubstituting the bound $|\\delta_i| \\le \\epsilon_{\\mathrm{mach}}$ for each $\\delta_i$:\n$$ |\\hat{S} - S| \\lesssim (|x_1| + |x_2| + |S|) \\epsilon_{\\mathrm{mach}} $$\nThe bound on the relative error is therefore:\n$$ \\frac{|\\hat{S} - S|}{|S|} \\lesssim \\frac{|x_1| + |x_2| + |S|}{|S|} \\epsilon_{\\mathrm{mach}} = \\left(1 + \\frac{|x_1| + |x_2|}{|S|}\\right) \\epsilon_{\\mathrm{mach}} $$\nSubstituting the original expressions for $x_1$, $x_2$, and $S$:\n$$ \\frac{|\\hat{S} - S|}{|S|} \\lesssim \\left(1 + \\frac{|\\exp(-t/\\tau_1)| + |\\exp(-t/\\tau_2)|}{|\\exp(-t/\\tau_1) - \\exp(-t/\\tau_2)|}\\right) \\epsilon_{\\mathrm{mach}} $$\nThis bound explains the numerical instability. As $\\tau_1 \\to \\tau_2$, the arguments of the exponentials become nearly equal, meaning $-t/\\tau_1 \\approx -t/\\tau_2$. Consequently, their values become nearly equal: $\\exp(-t/\\tau_1) \\approx \\exp(-t/\\tau_2)$. The denominator of the fraction in the error bound, $|S| = |\\exp(-t/\\tau_1) - \\exp(-t/\\tau_2)|$, approaches $0$. However, for $t>0$, the numerator, $|\\exp(-t/\\tau_1)| + |\\exp(-t/\\tau_2)|$, approaches $2\\exp(-t/\\tau)$, where $\\tau$ is the common limit of $\\tau_1$ and $\\tau_2$. The ratio $\\frac{|\\exp(-t/\\tau_1)| + |\\exp(-t/\\tau_2)|}{|S|}$ thus grows without bound. This large factor multiplies the machine epsilon, leading to a potentially large relative error in the computed result $\\hat{S}$. This phenomenon, where the subtraction of two nearly equal numbers results in a loss of relative accuracy, is known as catastrophic cancellation.\n\nTo avoid this numerical instability, we must reformulate the expression $S(t;\\tau_1,\\tau_2)$ algebraically. The goal is to avoid the direct subtraction of nearly equal quantities. The problem suggests using the function $\\mathrm{expm1}(x)$, which is defined as $\\mathrm{expm1}(x) = \\exp(x) - 1$ and is implemented to return a result with high relative accuracy even when $|x|$ is small.\n\nWe start with the original expression:\n$$ S(t;\\tau_1,\\tau_2) = \\exp(-t/\\tau_1) - \\exp(-t/\\tau_2) $$\nWe can factor out one of the exponential terms. Let us factor out $\\exp(-t/\\tau_2)$:\n$$ S = \\exp(-t/\\tau_2) \\left[ \\frac{\\exp(-t/\\tau_1)}{\\exp(-t/\\tau_2)} - 1 \\right] $$\nUsing the property $\\exp(a)/\\exp(b) = \\exp(a-b)$, we simplify the term inside the brackets:\n$$ S = \\exp(-t/\\tau_2) \\left[ \\exp\\left(-\\frac{t}{\\tau_1} - \\left(-\\frac{t}{\\tau_2}\\right)\\right) - 1 \\right] $$\n$$ S = \\exp(-t/\\tau_2) \\left[ \\exp\\left(-\\frac{t}{\\tau_1} + \\frac{t}{\\tau_2}\\right) - 1 \\right] $$\nWe can factor out $t$ from the exponent and combine the fractions:\n$$ -\\frac{t}{\\tau_1} + \\frac{t}{\\tau_2} = t \\left(\\frac{1}{\\tau_2} - \\frac{1}{\\tau_1}\\right) = t \\left(\\frac{\\tau_1 - \\tau_2}{\\tau_1 \\tau_2}\\right) $$\nSubstituting this back into the expression for $S$:\n$$ S = \\exp(-t/\\tau_2) \\left[ \\exp\\left(t \\frac{\\tau_1 - \\tau_2}{\\tau_1 \\tau_2}\\right) - 1 \\right] $$\nThe term inside the brackets is now of the form $\\exp(x) - 1$, where $x = t \\frac{\\tau_1 - \\tau_2}{\\tau_1 \\tau_2}$. We can thus express this using the $\\mathrm{expm1}$ function:\n$$ S = \\exp(-t/\\tau_2) \\cdot \\mathrm{expm1}\\left(t \\frac{\\tau_1 - \\tau_2}{\\tau_1 \\tau_2}\\right) $$\nThis expression is numerically stable. When $\\tau_1 \\approx \\tau_2$, the argument $x$ to the $\\mathrm{expm1}$ function becomes small. The function $\\mathrm{expm1}$ is specifically designed to handle small arguments with high relative accuracy. The subtraction $\\tau_1 - \\tau_2$ is of the input data, which are typically exact floating-point numbers; this subtraction itself is often exact by Sterbenz's Lemma. The remaining operations are multiplications and divisions, which are numerically well-behaved. This reformulated expression avoids the catastrophic cancellation of the naive approach, providing an accurate result even when $\\tau_1$ and $\\tau_2$ are close.", "answer": "$$\\boxed{\\exp\\left(-\\frac{t}{\\tau_2}\\right) \\mathrm{expm1}\\left(t\\frac{\\tau_1 - \\tau_2}{\\tau_1 \\tau_2}\\right)}$$", "id": "3165829"}, {"introduction": "Numerical accuracy is not just a matter of software algorithms; it is also influenced by the underlying computer hardware. This hands-on investigation [@problem_id:3165813] explores the impact of the Fused Multiply-Add (FMA) instruction, a feature of modern processors that performs $a \\times b + c$ with a single rounding. By comparing the accuracy of polynomial evaluation and dot product calculations with and without FMA, you will quantify how hardware-level optimizations can dramatically reduce numerical errors in routine scientific computations.", "problem": "Consider floating-point computation in a modern processor that supports fused multiply-add (Fused Multiply-Add (FMA)). Treat the floating-point operations using the standard rounding model: each primitive arithmetic operation returns the exact real result multiplied by a factor of the form $1 + \\delta$, where $\\delta$ is a small number whose magnitude is bounded by the machine precision $ \\epsilon $. In the absence of FMA, a compound operation such as $a\\times b + c$ undergoes separate rounding at multiplication and at addition. With FMA, $a\\times b + c$ is executed as a single operation with a single rounding. Catastrophic cancellation occurs when two numbers of nearly equal magnitude and opposite sign are added, causing severe loss of significant digits. Round-off error arises from finite precision rounding in arithmetic, and truncation error arises from approximations to continuous or infinite processes; in this problem, truncation error does not appear, but round-off and cancellation do.\n\nYou are to evaluate specific polynomials and dot products, measure numerical errors with and without using FMA, and quantify how FMA mitigates cancellation. Let the absolute error be defined as $E_{\\text{abs}} = \\lvert \\hat{y} - y \\rvert$ and the relative error as $E_{\\text{rel}} = \\frac{\\lvert \\hat{y} - y \\rvert}{\\lvert y \\rvert}$, where $\\hat{y}$ is the computed value and $y$ is a high-precision reference value. When $y = 0$, use absolute error; otherwise, use relative error.\n\nThe task is:\n\n1. Evaluate a polynomial using Horner’s scheme both with standard multiply-then-add and with FMA at given inputs. The polynomials are $p_1(x) = (x - 1)^8$ and $p_2(x) = (x - 1)^{12}$. Also include a baseline evaluation for $p_3(x) = (x - 1)^8$ at a value far from cancellation.\n\n2. Compute dot products of specified vector pairs with and without FMA accumulation.\n\n3. For each test case, compute the improvement factor $I = \\frac{E_{\\text{noFMA}}}{E_{\\text{FMA}}}$, using relative error when $y \\neq 0$ and absolute error when $y = 0$.\n\nUse a high-precision reference $y$ computed via arbitrary precision arithmetic to approximate the exact real arithmetic result for comparison. Then compute the improvement factor for each test case.\n\nTest Suite:\n- Polynomial tests via Horner’s method:\n  - $T_1$: $p_1(x)$ at $x = 1 + 10^{-8}$.\n  - $T_2$: $p_2(x)$ at $x = 1 + 10^{-10}$.\n  - $T_3$: $p_3(x)$ at $x = 2$.\n- Dot product tests:\n  - $T_4$: $v^{(A)} = [10^8, 10^8, 10^8, 10^8, 10^8, 10^8, 10^8, 10^8]$ and $w^{(A)} = [10^{-8}, -10^{-8}, 10^{-8}, -10^{-8}, 10^{-8}, -10^{-8}, 10^{-8}, -10^{-8}]$.\n  - $T_5$: $v^{(B)} = [10^{16}, 1, -10^{16}, 1, 10^{16}, -1]$ and $w^{(B)} = [10^{-16}, 1, 10^{-16}, 1, -10^{-16}, 1]$.\n  - $T_6$: $v^{(C)}$ of length $100$ with entries $v_i = (-1)^i \\cdot 10^8$ and $w^{(C)}$ of length $100$ with entries $w_i = 10^{-8}$, for $i = 1, \\dots, 100$.\n\nImplementation requirements:\n- Evaluate polynomials using Horner’s scheme in the form $y \\leftarrow y \\cdot x + a_i$, where $a_i$ are coefficients ordered from highest degree to constant term. The FMA variant should implement this update using a single fused operation $y \\leftarrow \\mathrm{fma}(y, x, a_i)$, that is, $y \\leftarrow (y \\cdot x + a_i)$ with one rounding.\n- Evaluate dot products as $\\sum_{i=1}^{n} v_i w_i$; the FMA variant should accumulate with $y \\leftarrow \\mathrm{fma}(v_i, w_i, y)$.\n\nReference values:\n- Obtain $y$ using arbitrary precision arithmetic applied to the same mathematical expressions, with numbers specified in decimal form (for instance, $10^{-8}$) so that the reference approximates exact real arithmetic closely.\n\nFinal Output:\n- Your program should produce a single line of output containing the six improvement factors for $T_1$ through $T_6$ as a comma-separated list enclosed in square brackets, in order $[I_{T_1}, I_{T_2}, I_{T_3}, I_{T_4}, I_{T_5}, I_{T_6}]$, each formatted to $12$ significant digits (for example, $[1.23456789012,2.0,3.14159265359, ...]$).", "solution": "The user's request is to solve a numerical analysis problem involving the comparison of standard floating-point arithmetic with arithmetic using Fused Multiply-Add (FMA) capabilities. The goal is to quantify the improvement FMA offers in mitigating numerical errors, specifically round-off error and catastrophic cancellation.\n\n### Problem Validation\n\nFirst, I will validate the problem statement according to the required procedure.\n\n#### Step 1: Extract Givens\n\n-   **Floating-Point Model**: Each primitive arithmetic operation is subject to rounding, modeled as multiplication by a factor of $(1 + \\delta)$, where $|\\delta| \\le \\epsilon$ ($\\epsilon$ is the machine precision).\n-   **Standard Computation**: An operation like $a \\times b + c$ involves two rounding steps: one for the multiplication and one for the addition, i.e., $\\mathrm{fl}(\\mathrm{fl}(a \\times b) + c)$.\n-   **FMA Computation**: An operation like $a \\times b + c$ is executed as a single unit with only one rounding step, i.e., $\\mathrm{fl}(a \\times b + c)$.\n-   **Error Metrics**: Absolute error $E_{\\text{abs}} = |\\hat{y} - y|$ and relative error $E_{\\text{rel}} = \\frac{|\\hat{y} - y|}{|y|}$. Relative error is used unless the true value $y=0$, in which case absolute error is used.\n-   **Improvement Factor**: $I = \\frac{E_{\\text{noFMA}}}{E_{\\text{FMA}}}$, where $E$ is the error (relative or absolute as appropriate).\n-   **Reference Value ($y$)**: A high-precision value computed using arbitrary-precision arithmetic to serve as a proxy for the exact real result.\n-   **Tasks**:\n    1.  Evaluate polynomials $p_1(x) = (x - 1)^8$ and $p_2(x) = (x - 1)^{12}$ using Horner's scheme, with and without FMA. A baseline case $p_3(x) = (x-1)^8$ is also tested.\n    2.  Compute dot products of specified vector pairs, with and without FMA accumulation.\n-   **Implementations**:\n    -   Horner's scheme update: $y \\leftarrow y \\cdot x + a_i$ (standard) and $y \\leftarrow \\mathrm{fma}(y, x, a_i)$ (FMA).\n    -   Dot product accumulation: $y \\leftarrow y + v_i \\cdot w_i$ (standard loop) and $y \\leftarrow \\mathrm{fma}(v_i, w_i, y)$ (FMA loop).\n-   **Test Suite**:\n    -   $T_1$: $p_1(x) = (x - 1)^8$ at $x = 1 + 10^{-8}$.\n    -   $T_2$: $p_2(x) = (x - 1)^{12}$ at $x = 1 + 10^{-10}$.\n    -   $T_3$: $p_3(x) = (x - 1)^8$ at $x = 2$.\n    -   $T_4$: $v^{(A)} = [10^8, \\dots, 10^8]$ (8 elements), $w^{(A)} = [10^{-8}, -10^{-8}, \\dots]$.\n    -   $T_5$: $v^{(B)} = [10^{16}, 1, -10^{16}, 1, 10^{16}, -1]$, $w^{(B)} = [10^{-16}, 1, 10^{-16}, 1, -10^{-16}, 1]$.\n    -   $T_6$: `v_i = (-1)^i \\cdot 10^8`, `w_i = 10^{-8}` for $i=1, \\dots, 100$.\n-   **Output Format**: A single-line list of six improvement factors, `[I_T1, ..., I_T6]`, formatted to $12$ significant digits.\n\n#### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded**: The problem is fundamentally sound. It addresses core concepts of numerical analysis: floating-point arithmetic, round-off error, catastrophic cancellation, Horner's method, and the FMA instruction. The error models and definitions are standard.\n-   **Well-Posed**: The problem is well-posed. The inputs, required computations, and desired output are all explicitly defined. The tasks lead to a unique and meaningful set of numerical results. The derivation of polynomial coefficients from $(x-1)^n$ is a standard application of the binomial theorem and not a missing piece of information.\n-   **Objective**: The problem is stated in precise, objective, and technical language, free from ambiguity or subjective claims.\n\nThe problem does not exhibit any of the invalidity flaws. It is a well-formulated, standard problem in an introductory computational science or numerical analysis course.\n\n#### Step 3: Verdict and Action\n\nThe problem is **valid**. A solution will be provided.\n\n### Principle-Based Solution Design\n\nThe core of this problem lies in the difference between fused and non-fused computations.\n\n1.  **Fundamental Principle**: A standard floating-point operation like $z = a \\times b + c$ is executed as two separate operations: a multiplication followed by an addition. Each operation incurs a rounding error.\n    -   $\\hat{p} = \\mathrm{fl}(a \\times b) = (a \\times b)(1+\\delta_1)$\n    -   $\\hat{z} = \\mathrm{fl}(\\hat{p} + c) = (\\hat{p} + c)(1+\\delta_2)$\n    The Fused Multiply-Add (FMA) instruction, available on modern processors, performs this entire operation with a single rounding:\n    -   $\\hat{z}_{\\text{FMA}} = \\mathrm{fl}(a \\times b + c) = (a \\times b + c)(1+\\delta_3)$\n    FMA computes the product $a \\times b$ to a higher intermediate precision before adding $c$, thereby reducing the total round-off error. This is especially impactful in two scenarios:\n    a.  When the product $a \\times b$ is itself not exactly representable as a standard float, FMA prevents the loss of information that would occur from rounding it before the addition.\n    b.  When $\\mathrm{fl}(a \\times b)$ and $c$ are nearly equal in magnitude and opposite in sign, their sum suffers from catastrophic cancellation. FMA can mitigate this by adding $c$ to the un-rounded, high-precision product $a \\times b$.\n\n2.  **Polynomial Evaluation (Horner's Method)**: For a polynomial $P(x) = \\sum_{i=0}^{n} c_i x^i$, Horner's method is an efficient evaluation algorithm defined by the recurrence: $y_n = c_n$, $y_k = y_{k+1} \\cdot x + c_k$ for $k = n-1, \\dots, 0$.\n    The polynomials $(x-1)^n$, when expanded, have large binomial coefficients with alternating signs. For example, $(x-1)^8 = x^8 - 8x^7 + 28x^6 - 56x^5 + 70x^4 - 56x^3 + 28x^2 - 8x + 1$. When evaluating this near $x=1$, the intermediate terms in Horner's method become large, and their additions and subtractions lead to significant catastrophic cancellation. FMA performs each step $y \\cdot x + c_k$ with higher precision, preserving significant digits and yielding a much more accurate result. Test case $T_3$, at $x=2$, serves as a baseline where no such cancellation occurs.\n\n3.  **Dot Product Evaluation**: The dot product $\\sum_{i=1}^{n} v_i w_i$ is a sum of products. A naive implementation involves a loop that accumulates the sum.\n    -   _Without FMA_: `sum = sum + (v_i * w_i)`. This involves a rounding after the product `v_i * w_i` and another after the addition to `sum`.\n    -   _With FMA_: `sum = fma(v_i, w_i, sum)`. Here, the full high-precision product of `v_i` and `w_i` is calculated and then added to `sum` before a single final rounding.\n    Test cases $T_4, T_5, T_6$ are designed such that the exact sum is zero, but the individual terms are not. This is a classic scenario for catastrophic cancellation in summation, where FMA's ability to avoid intermediate rounding of products becomes crucial for accuracy.\n\n4.  **Handling of Edge Cases**: In computing the improvement factor $I = E_{\\text{noFMA}} / E_{\\text{FMA}}$, division by zero can occur if $E_{\\text{FMA}} = 0$.\n    -   If both $E_{\\text{noFMA}}$ and $E_{\\text{FMA}}$ are zero (as in $T_3$), the methods are equally accurate, and the improvement factor is logically $1$.\n    -   If $E_{\\text{FMA}}=0$ but $E_{\\text{noFMA}} > 0$ (as in $T_5$), the improvement is theoretically infinite. To provide a numerical output as required, we can use a proxy for the FMA error. A sensible choice is the smallest positive normalized float, `numpy.finfo(float).tiny`, representing the limit of floating-point resolution. This yields a very large, finite number for the improvement factor.\n    -   If $E_{\\text{noFMA}}=0$ but $E_{\\text{FMA}} > 0$ (as in $T_4, T_6$), this indicates a scenario where the standard method fortuitously produces the exact answer while the FMA method accumulates a small error. In this case, the improvement factor is correctly computed as $0$.\n\nThe implementation will use Python's `math` module for standard operations and `math.fma` for fused operations. High-precision reference values will be computed using the `decimal` module.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport math\nimport decimal\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes numerical errors with and without Fused Multiply-Add (FMA)\n    and calculates the improvement factor for a suite of test cases.\n    \"\"\"\n    # Set precision for high-accuracy reference calculations.\n    decimal.getcontext().prec = 100\n\n    # --- Helper Functions ---\n\n    def get_poly_coeffs(n):\n        \"\"\"Computes coefficients for the expanded form of (x-1)^n.\"\"\"\n        coeffs = []\n        # Horner's method requires coefficients from highest degree (c_n) down to c_0.\n        # The coefficient of x^k in (x-1)^n is C(n, n-k) * (-1)^(n-k).\n        # We order them for c_n, c_{n-1}, ..., c_0\n        for i in range(n, -1, -1):\n            k = n - i\n            coeff = math.comb(n, k) * ((-1)**k)\n            coeffs.append(float(coeff))\n        return coeffs\n\n    def horner_nofma(coeffs, x):\n        \"\"\"Evaluates a polynomial using Horner's scheme with standard operators.\"\"\"\n        y = 0.0\n        for c in coeffs:\n            y = y * x + c\n        return y\n\n    def horner_fma(coeffs, x):\n        \"\"\"Evaluates a polynomial using Horner's scheme with FMA.\"\"\"\n        y = 0.0\n        for c in coeffs:\n            y = math.fma(y, x, c)\n        return y\n\n    def dot_nofma(v, w):\n        \"\"\"Computes a dot product with standard operators.\"\"\"\n        s = 0.0\n        for i in range(len(v)):\n            s += v[i] * w[i]\n        return s\n\n    def dot_fma(v, w):\n        \"\"\"Computes a dot product using FMA for accumulation.\"\"\"\n        s = 0.0\n        for i in range(len(v)):\n            s = math.fma(v[i], w[i], s)\n        return s\n\n    def get_improvement(y_ref, y_nofma, y_fma):\n        \"\"\"\n        Calculates the improvement factor I = E_noFMA / E_FMA.\n        Handles edge cases like division by zero.\n        \"\"\"\n        y_ref_f = float(y_ref)\n\n        if y_ref_f == 0.0:\n            err_nofma = abs(y_nofma)\n            err_fma = abs(y_fma)\n        else:\n            err_nofma = abs(y_nofma - y_ref_f) / abs(y_ref_f)\n            err_fma = abs(y_fma - y_ref_f) / abs(y_ref_f)\n\n        if err_fma == err_nofma:\n            return 1.0\n        \n        if err_fma == 0.0:\n            # Improvement is theoretically infinite. Use a very large number\n            # by dividing by the smallest possible float value as a proxy for error.\n            if y_ref_f == 0.0:\n                err_fma_proxy = np.finfo(float).tiny\n            else:\n                err_fma_proxy = np.finfo(float).tiny / abs(y_ref_f)\n            \n            if err_fma_proxy == 0.0: # Avoid division by zero if y_ref is huge\n                return np.inf\n\n            return err_nofma / err_fma_proxy\n\n        return err_nofma / err_fma\n\n    # --- Test Case Definitions ---\n\n    test_cases = [\n        # T1: p1(x) = (x-1)^8 at x = 1 + 1e-8\n        {\n            'eval_func_nofma': lambda: horner_nofma(get_poly_coeffs(8), 1.0 + 1e-8),\n            'eval_func_fma': lambda: horner_fma(get_poly_coeffs(8), 1.0 + 1e-8),\n            'ref_val': (decimal.Decimal('1') + decimal.Decimal('1e-8') - decimal.Decimal('1'))**8\n        },\n        # T2: p2(x) = (x-1)^12 at x = 1 + 1e-10\n        {\n            'eval_func_nofma': lambda: horner_nofma(get_poly_coeffs(12), 1.0 + 1e-10),\n            'eval_func_fma': lambda: horner_fma(get_poly_coeffs(12), 1.0 + 1e-10),\n            'ref_val': (decimal.Decimal('1') + decimal.Decimal('1e-10') - decimal.Decimal('1'))**12\n        },\n        # T3: p3(x) = (x-1)^8 at x = 2\n        {\n            'eval_func_nofma': lambda: horner_nofma(get_poly_coeffs(8), 2.0),\n            'eval_func_fma': lambda: horner_fma(get_poly_coeffs(8), 2.0),\n            'ref_val': (decimal.Decimal('2') - decimal.Decimal('1'))**8\n        },\n        # T4: Dot product vA . wA\n        {\n            'eval_func_nofma': lambda: dot_nofma([1e8] * 8, [1e-8, -1e-8] * 4),\n            'eval_func_fma': lambda: dot_fma([1e8] * 8, [1e-8, -1e-8] * 4),\n            'ref_val': decimal.Decimal('0')\n        },\n        # T5: Dot product vB . wB\n        {\n            'v': [1e16, 1.0, -1e16, 1.0, 1e16, -1.0],\n            'w': [1e-16, 1.0, 1e-16, 1.0, -1e-16, 1.0],\n            'eval_func_nofma': lambda: dot_nofma([1e16, 1.0, -1e16, 1.0, 1e16, -1.0], [1e-16, 1.0, 1e-16, 1.0, -1e-16, 1.0]),\n            'eval_func_fma': lambda: dot_fma([1e16, 1.0, -1e16, 1.0, 1e16, -1.0], [1e-16, 1.0, 1e-16, 1.0, -1e-16, 1.0]),\n            'ref_val': decimal.Decimal('0')\n        },\n        # T6: Dot product vC . wC\n        {\n            'v': [(-1.0)**i * 1e8 for i in range(1, 101)],\n            'w': [1e-8] * 100,\n            'eval_func_nofma': lambda: dot_nofma([(-1.0)**i * 1e8 for i in range(1, 101)], [1e-8] * 100),\n            'eval_func_fma': lambda: dot_fma([(-1.0)**i * 1e8 for i in range(1, 101)], [1e-8] * 100),\n            'ref_val': decimal.Decimal('0')\n        }\n    ]\n\n    # --- Main Execution Loop ---\n\n    results = []\n    for case in test_cases:\n        y_ref = case['ref_val']\n        y_nofma = case['eval_func_nofma']()\n        y_fma = case['eval_func_fma']()\n        \n        improvement = get_improvement(y_ref, y_nofma, y_fma)\n        results.append(improvement)\n\n    # Final print statement in the exact required format.\n    # The spec asks for 12 significant digits. Using .12g is robust.\n    result_str = ','.join(format(res, '.12g') for res in results)\n    print(f\"[{result_str}]\")\n\nsolve()\n```", "id": "3165813"}]}