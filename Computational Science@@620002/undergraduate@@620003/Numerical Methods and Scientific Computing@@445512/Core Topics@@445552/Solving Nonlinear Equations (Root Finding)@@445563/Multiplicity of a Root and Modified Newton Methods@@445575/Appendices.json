{"hands_on_practices": [{"introduction": "Standard Newton's method is celebrated for its speed, but it has an Achilles' heel: roots with multiplicity greater than one. When faced with such a root, the method's convergence degrades from quadratic to painstakingly linear. This first practice provides a direct, computational demonstration of this phenomenon and its solution. By comparing the performance of the standard Newton's method against a modified version tailored for a known multiplicity $m$, you will gain a concrete appreciation for why the modification is necessary and how effectively it restores rapid convergence [@problem_id:3254008].", "problem": "Consider the task of solving $f(x)=0$ when the root has known multiplicity. A point $x^{\\star}$ is a root of multiplicity $m$ for a sufficiently differentiable function $f$ if $f(x^{\\star})=0$, $f^{(k)}(x^{\\star})=0$ for all integers $k$ with $1 \\le k \\le m-1$, and $f^{(m)}(x^{\\star}) \\ne 0$. The standard Newton method is derived by replacing $f$ locally by its first-order Taylor approximation at the current iterate and solving the resulting linearized equation. When a root has multiplicity greater than one, the unmodified iteration may lose its usual quadratic convergence. A modified Newton method tailored to a known multiplicity $m$ rescales the iteration to recover rapid convergence.\n\nYour task is to write a complete, runnable program that, for the family of functions $f_m(x)=(x-1)^m$ with $m \\in \\{2,3,5\\}$, compares the standard Newton method against a multiplicity-aware modified Newton method. For each function, use the three initial guesses $x_0 \\in \\{2,1,-5\\}$ and apply both methods to compute the number of iterations required to achieve a residual below a fixed tolerance. The residual should be measured as $|f_m(x_n)|$, and the stopping condition is $|f_m(x_n)| \\le \\tau$, with $\\tau=10^{-12}$. If the initial guess already satisfies $|f_m(x_0)| \\le \\tau$, the iteration count is $0$. If the method fails to reach the tolerance within a maximum of $N_{\\text{max}}=1000$ iterations, report the integer $-1$ as the iteration count for that method on that test case.\n\nImplementation requirements:\n- Implement the standard Newton method using the tangent-line linearization principle for root-finding.\n- Implement the modified Newton method tailored to a known multiplicity $m$, rescaling the step appropriately to address repeated roots.\n- Use $f_m(x)=(x-1)^m$ and its derivative $f_m'(x)$ in both methods.\n- Use the residual $|f_m(x_n)|$ for the stopping check with the tolerance $\\tau=10^{-12}$.\n- Use the maximum iteration cap $N_{\\text{max}}=1000$.\n\nTest suite specification:\n- The ordered list of test cases is the Cartesian product of $m \\in \\{2,3,5\\}$ and $x_0 \\in \\{2,1,-5\\}$, ordered lexicographically by $m$ and then by $x_0$. Explicitly, the test cases are $(m,x_0)$ equal to $(2,2)$, $(2,1)$, $(2,-5)$, $(3,2)$, $(3,1)$, $(3,-5)$, $(5,2)$, $(5,1)$, $(5,-5)$.\n\nFor each test case, your program must output a pair of integers $[n_{\\text{std}},n_{\\text{mod}}]$, where $n_{\\text{std}}$ is the iteration count for the standard Newton method and $n_{\\text{mod}}$ is the iteration count for the modified Newton method. The final output must be a single line containing all nine pairs in the exact test-case order specified, as a comma-separated list enclosed in square brackets, with no spaces. For example, an output with three hypothetical pairs would look like $[[1,1],[0,0],[7,2]]$. Your program should produce the exact final format:\n- A single line: $[[n_{\\text{std},1},n_{\\text{mod},1}],[n_{\\text{std},2},n_{\\text{mod},2}],\\dots,[n_{\\text{std},9},n_{\\text{mod},9}]]$.", "solution": "The problem requires a comparison of the standard Newton method and a modified Newton method for finding a root of known multiplicity $m$. The analysis will be performed on the function family $f_m(x)=(x-1)^m$, which has a root $x^{\\star}=1$ of multiplicity $m$.\n\nA point $x^{\\star}$ is defined as a root of multiplicity $m$ for a function $f$ if $f(x^{\\star})=0$ and its first $m-1$ derivatives are zero at $x^{\\star}$, i.e., $f^{(k)}(x^{\\star})=0$ for $1 \\le k \\le m-1$, while the $m$-th derivative is non-zero, $f^{(m)}(x^{\\star}) \\ne 0$.\n\n### Standard Newton's Method\n\nThe standard Newton-Raphson method finds roots of an equation $f(x)=0$ by constructing a sequence of successively better approximations. The iteration is derived from the first-order Taylor series expansion of $f(x)$ around an iterate $x_n$:\n$$f(x) \\approx f(x_n) + f'(x_n)(x - x_n)$$\nTo find the root, we set $f(x)=0$ and solve for $x$, which becomes the next iterate $x_{n+1}$:\n$$0 = f(x_n) + f'(x_n)(x_{n+1} - x_n)$$\nThis yields the standard Newton iteration formula:\n$$x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$$\nThis method exhibits quadratic convergence for simple roots (where $m=1$), meaning the number of correct decimal places roughly doubles with each iteration. However, for a root of multiplicity $m>1$, the convergence degrades to linear, with a convergence rate of $(m-1)/m$.\n\nFor the specific function $f_m(x)=(x-1)^m$, the derivative is $f_m'(x)=m(x-1)^{m-1}$. Substituting these into the iteration formula gives:\n$$x_{n+1} = x_n - \\frac{(x_n-1)^m}{m(x_n-1)^{m-1}} = x_n - \\frac{x_n-1}{m}$$\nLet the error at step $n$ be $e_n = x_n-1$. The error update rule is:\n$$e_{n+1} = x_{n+1}-1 = \\left(x_n - \\frac{x_n-1}{m}\\right) - 1 = (x_n - 1) - \\frac{x_n-1}{m} = e_n \\left(1 - \\frac{1}{m}\\right)$$\nThis confirms the linear convergence, as the error is reduced by a constant factor $1 - 1/m$ at each step.\n\n### Modified Newton's Method for Multiple Roots\n\nTo restore quadratic convergence for a root of known multiplicity $m$, the standard Newton step can be modified. The modified iteration is given by:\n$$x_{n+1} = x_n - m \\frac{f(x_n)}{f'(x_n)}$$\nThere are several justifications for this formula. One is to apply the standard Newton's method to an auxiliary function $u(x) = [f(x)]^{1/m}$. If $f(x)$ has a root of multiplicity $m$ at $x^{\\star}$, then $u(x)$ has a simple root at $x^{\\star}$, for which the standard Newton's method converges quadratically. The derivative of $u(x)$ is $u'(x) = \\frac{1}{m} f(x)^{(1/m)-1}f'(x)$. The Newton iteration for $u(x)$ is:\n$$x_{n+1} = x_n - \\frac{u(x_n)}{u'(x_n)} = x_n - \\frac{f(x_n)^{1/m}}{\\frac{1}{m} f(x_n)^{(1/m)-1}f'(x_n)} = x_n - m \\frac{f(x_n)}{f'(x_n)}$$\nThis is precisely the modified formula.\n\nFor the specific function $f_m(x)=(x-1)^m$, the modified iteration becomes:\n$$x_{n+1} = x_n - m \\frac{(x_n-1)^m}{m(x_n-1)^{m-1}} = x_n - (x_n-1) = 1$$\nThis remarkable result shows that for the function family $f_m(x)=(x-1)^m$, the modified Newton's method converges to the exact root $x^{\\star}=1$ in a single iteration, for any initial guess $x_0 \\ne 1$.\n\n### Algorithmic Implementation\n\nFor each test case defined by the pair $(m, x_0)$, where $m \\in \\{2,3,5\\}$ and $x_0 \\in \\{2,1,-5\\}$, we will perform the following steps for both the standard and modified methods:\n\n1.  Initialize the number of iterations $n=0$ and the current guess $x=x_0$.\n2.  Check the initial stopping condition: if the residual $|f_m(x_0)| = |(x_0 - 1)^m|$ is less than or equal to the tolerance $\\tau=10^{-12}$, the process terminates and the iteration count is $0$. This case occurs if $x_0=1$.\n3.  Begin an iterative loop, up to a maximum of $N_{\\text{max}}=1000$ iterations. In each step $n = 1, 2, \\dots, N_{\\text{max}}$:\n    a. Compute the next iterate $x_n$ using the respective formula (standard or modified).\n    b. Update the current guess: $x \\leftarrow x_n$.\n    c. Calculate the residual $|f_m(x)| = |(x-1)^m|$.\n    d. If the residual is less than or equal to $\\tau$, the loop terminates, and the current iteration count $n$ is returned.\n4.  If the loop completes without meeting the tolerance (i.e., $n=N_{\\text{max}}$), the method has failed to converge within the allowed number of iterations. In this case, the value $-1$ is reported.\n\nThis procedure will be executed for all nine specified test cases, and the pairs of iteration counts $[n_{\\text{std}}, n_{\\text{mod}}]$ will be collected and formatted as the final output. Based on the analysis above, for any initial guess $x_0 \\ne 1$, the modified method is expected to converge in $n_{\\text{mod}}=1$ iteration. For $x_0=1$, both methods will report $n=0$ iterations. The standard method will exhibit linear convergence, requiring a number of iterations that increases with the multiplicity $m$ and the initial distance from the root.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef newton_standard(m, x0, tau, n_max):\n    \"\"\"\n    Performs the standard Newton's method for f(x) = (x-1)^m.\n    The update rule simplifies to x_next = x - (x-1)/m.\n\n    Args:\n        m (int): The multiplicity of the root.\n        x0 (float): The initial guess.\n        tau (float): The tolerance for the residual |f(x)|.\n        n_max (int): The maximum number of iterations.\n\n    Returns:\n        int: The number of iterations required, or -1 if not converged.\n    \"\"\"\n    x = np.float64(x0)\n\n    # Check if the initial guess already satisfies the tolerance.\n    # The exponentiation can be numerically sensitive, but for the given\n    # test cases, the values are well within float64 range.\n    try:\n        initial_residual = np.abs(np.power(x - 1.0, m))\n    except OverflowError:\n        # If the initial guess is very far, residual could overflow.\n        # This is safe for the given test cases but good practice.\n        initial_residual = np.inf\n\n    if initial_residual = tau:\n        return 0\n\n    for n in range(1, n_max + 1):\n        # The update rule is derived as:\n        # f(x) = (x-1)^m\n        # f'(x) = m * (x-1)^(m-1)\n        # x_next = x - f(x)/f'(x) = x - (x-1)/m\n        if x == 1.0:\n            # Landed exactly on the root. This will satisfy the check below,\n            # but this handles the case explicitly.\n            return n\n        \n        x = x - (x - 1.0) / np.float64(m)\n\n        try:\n            residual = np.abs(np.power(x - 1.0, m))\n        except OverflowError:\n            residual = np.inf\n\n        if residual = tau:\n            return n\n\n    return -1\n\ndef newton_modified(m, x0, tau, n_max):\n    \"\"\"\n    Performs the modified Newton's method for f(x) = (x-1)^m.\n    The update rule simplifies to x_next = 1.\n\n    Args:\n        m (int): The multiplicity of the root.\n        x0 (float): The initial guess.\n        tau (float): The tolerance for the residual |f(x)|.\n        n_max (int): The maximum number of iterations.\n\n    Returns:\n        int: The number of iterations required.\n    \"\"\"\n    x = np.float64(x0)\n\n    try:\n        initial_residual = np.abs(np.power(x - 1.0, m))\n    except OverflowError:\n        initial_residual = np.inf\n\n    if initial_residual = tau:\n        return 0\n\n    # The modified Newton's method for f(x) = (x-1)^m converges in one step:\n    # x_next = x - m * f(x)/f'(x) = x - m * (x-1)^m / (m*(x-1)^(m-1))\n    #        = x - (x-1) = 1\n    # The residual at step 1 will be |(1-1)^m| = 0, which is = tau.\n    return 1\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Define the problem parameters\n    tau = 1.0e-12\n    n_max = 1000\n    multiplicities = [2, 3, 5]\n    initial_guesses = [2, 1, -5]\n\n    # Generate the ordered list of test cases\n    test_cases = []\n    for m in multiplicities:\n        for x0 in initial_guesses:\n            test_cases.append((m, x0))\n\n    results = []\n    for m, x0 in test_cases:\n        n_std = newton_standard(m, x0, tau, n_max)\n        n_mod = newton_modified(m, x0, tau, n_max)\n        results.append([n_std, n_mod])\n\n    # Format the output string as specified: [[r1_1,r1_2],[r2_1,r2_2],...]\n    # with no spaces.\n    result_strings = [f\"[{r[0]},{r[1]}]\" for r in results]\n    final_output = f\"[{','.join(result_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "3254008"}, {"introduction": "The previous exercise assumed a crucial piece of information: the multiplicity $m$ of the root. In many real-world applications, this value is unknown. This next practice addresses this challenge by guiding you to develop a method for estimating $m$. You will first derive an elegant formula for $m$ based on the local behavior of the function and its derivatives, and then implement a numerical experiment to probe the estimator's accuracy and uncover its vulnerabilities in the world of finite-precision arithmetic [@problem_id:3254009].", "problem": "Consider a real-valued function $f$ that has a root at $x=\\alpha$ of known multiplicity structure. Specifically, assume $f(x)=(x-\\alpha)^m g(x)$ for an integer $m\\ge 1$ and a twice continuously differentiable function $g$ with $g(\\alpha)\\ne 0$. This guarantees $f(\\alpha)=0$ and that $m$ equals the multiplicity of the root. In exact arithmetic, one can infer $m$ from local differential information near $x=\\alpha$. In floating-point arithmetic following the Institute of Electrical and Electronics Engineers (IEEE) 754 double-precision standard, such inferences can exhibit bias due to rounding and cancellation.\n\nTasks:\n- Derive, in terms of $m$ only, the exact limit of the ratio $\\dfrac{f(x) f''(x)}{\\left(f'(x)\\right)^2}$ as $x\\to \\alpha$, given the structural assumption $f(x)=(x-\\alpha)^m g(x)$ with $g(\\alpha)\\ne 0$.\n- Propose and justify a pointwise estimator $\\hat m(x)$ for the multiplicity $m$ as a function of $x\\ne \\alpha$ using only $f(x)$, $f'(x)$, and $f''(x)$.\n- Implement a numerical experiment to assess the bias in IEEE 754 double precision of this estimator when evaluated at points $x$ very close to $\\alpha$.\n\nUse the following test suite of functions and parameters. In each case, evaluate the estimator at the five offsets $\\delta\\in\\{10^{-1},10^{-4},10^{-8},10^{-12},10^{-16}\\}$ using points $x=\\alpha+\\delta$.\n1. Case A (happy path): $m=3$, $\\alpha=0$, $g(x)=1+x$.\n2. Case B (nonzero $\\alpha$ and curved $g$): $m=2$, $\\alpha=0.3$, $g(x)=e^{x}$.\n3. Case C (simple root edge case): $m=1$, $\\alpha=-0.2$, $g(x)=1+x^2$.\n4. Case D (higher multiplicity and oscillatory $g$): $m=5$, $\\alpha=0$, $g(x)=\\cos(x)$.\n\nFor each case, define $f(x)=(x-\\alpha)^m g(x)$, compute $f'(x)$ and $f''(x)$ symbolically from the product rule, evaluate the estimator $\\hat m(x)$ at each specified $\\delta$, and then report a single numeric summary per case: the mean absolute bias over the five offsets, defined as\n$$\n\\frac{1}{5}\\sum_{\\delta\\in\\{10^{-1},10^{-4},10^{-8},10^{-12},10^{-16}\\}} \\left|\\hat m(\\alpha+\\delta)-m\\right|.\n$$\nAll computations are to be performed in IEEE 754 double-precision floating-point arithmetic. No physical units arise. Angles, when present as arguments to trigonometric functions, must be interpreted in radians.\n\nYour program should produce a single line of output containing the results for the four cases as a comma-separated list enclosed in square brackets, in the order A, B, C, D, for example, $\\texttt{[r_A,r_B,r_C,r_D]}$, where each $r_\\cdot$ is the mean absolute bias for that case represented as a floating-point number.", "solution": "The problem requires the derivation of a limit related to a function $f(x)$ with a root of known multiplicity structure, the proposal of an estimator for the multiplicity $m$, and a numerical assessment of the estimator's bias in floating-point arithmetic.\n\n### Part 1: Derivation of the Limit\n\nWe are given a function $f(x)$ with a root of multiplicity $m$ at $x=\\alpha$. This is formally expressed as $f(x) = (x-\\alpha)^m g(x)$, where $m \\ge 1$ is an integer, $g(x)$ is a twice continuously differentiable function, and $g(\\alpha) \\ne 0$. Our goal is to find the limit of the ratio $Q(x) = \\dfrac{f(x) f''(x)}{\\left(f'(x)\\right)^2}$ as $x \\to \\alpha$.\n\nFirst, we compute the first and second derivatives of $f(x)$ using the product rule.\nThe function is:\n$$f(x) = (x-\\alpha)^m g(x)$$\n\nThe first derivative, $f'(x)$, is:\n$$f'(x) = \\frac{d}{dx}\\left[(x-\\alpha)^m g(x)\\right] = m(x-\\alpha)^{m-1}g(x) + (x-\\alpha)^m g'(x)$$\nFactoring out the term $(x-\\alpha)^{m-1}$, we get:\n$$f'(x) = (x-\\alpha)^{m-1} [m g(x) + (x-\\alpha) g'(x)]$$\n\nThe second derivative, $f''(x)$, is obtained by differentiating $f'(x)$:\n$$f''(x) = \\frac{d}{dx}\\left[(x-\\alpha)^{m-1} [m g(x) + (x-\\alpha) g'(x)]\\right]$$\n$$f''(x) = (m-1)(x-\\alpha)^{m-2}[m g(x) + (x-\\alpha) g'(x)] + (x-\\alpha)^{m-1}[m g'(x) + g'(x) + (x-\\alpha) g''(x)]$$\nFactoring out the term $(x-\\alpha)^{m-2}$:\n$$f''(x) = (x-\\alpha)^{m-2} \\left[ (m-1)(m g(x) + (x-\\alpha) g'(x)) + (x-\\alpha)((m+1)g'(x) + (x-\\alpha)g''(x)) \\right]$$\nExpanding and collecting terms:\n$$f''(x) = (x-\\alpha)^{m-2} \\left[ m(m-1)g(x) + (m-1+m+1)(x-\\alpha)g'(x) + (x-\\alpha)^2 g''(x) \\right]$$\n$$f''(x) = (x-\\alpha)^{m-2} \\left[ m(m-1)g(x) + 2m(x-\\alpha)g'(x) + (x-\\alpha)^2 g''(x) \\right]$$\n\nNow we construct the necessary components of the ratio $Q(x)$.\nThe product $f(x) f''(x)$ is:\n$$f(x) f''(x) = (x-\\alpha)^m g(x) \\cdot (x-\\alpha)^{m-2} \\left[ m(m-1)g(x) + 2m(x-\\alpha)g'(x) + (x-\\alpha)^2 g''(x) \\right]$$\n$$f(x) f''(x) = (x-\\alpha)^{2m-2} g(x) \\left[ m(m-1)g(x) + 2m(x-\\alpha)g'(x) + (x-\\alpha)^2 g''(x) \\right]$$\nThe term $(f'(x))^2$ is:\n$$\\left(f'(x)\\right)^2 = \\left( (x-\\alpha)^{m-1} [m g(x) + (x-\\alpha) g'(x)] \\right)^2$$\n$$\\left(f'(x)\\right)^2 = (x-\\alpha)^{2m-2} [m g(x) + (x-\\alpha) g'(x)]^2$$\n\nFor $x \\ne \\alpha$, we can form the ratio $Q(x)$ and cancel the common factor $(x-\\alpha)^{2m-2}$:\n$$Q(x) = \\frac{g(x) \\left[ m(m-1)g(x) + 2m(x-\\alpha)g'(x) + (x-\\alpha)^2 g''(x) \\right]}{[m g(x) + (x-\\alpha) g'(x)]^2}$$\n\nTo find the limit as $x \\to \\alpha$, we use the continuity of $g(x)$, $g'(x)$, and $g''(x)$. All terms containing a factor of $(x-\\alpha)$ will go to zero.\n$$\\lim_{x\\to \\alpha} Q(x) = \\frac{g(\\alpha) \\left[ m(m-1)g(\\alpha) + 2m(0)g'(\\alpha) + (0)^2 g''(\\alpha) \\right]}{[m g(\\alpha) + (0) g'(\\alpha)]^2}$$\n$$\\lim_{x\\to \\alpha} Q(x) = \\frac{g(\\alpha) [m(m-1)g(\\alpha)]}{[m g(\\alpha)]^2} = \\frac{m(m-1)g(\\alpha)^2}{m^2 g(\\alpha)^2}$$\nSince $m \\ge 1$ and $g(\\alpha) \\ne 0$, we can simplify this expression. For $m=1$, the numerator is $0$, so the limit is $0$. For $m > 1$, we can cancel $m g(\\alpha)^2$. The resulting formula holds for all $m \\ge 1$:\n$$\\lim_{x\\to \\alpha} \\frac{f(x) f''(x)}{\\left(f'(x)\\right)^2} = \\frac{m-1}{m} = 1 - \\frac{1}{m}$$\n\n### Part 2: Multiplicity Estimator\n\nThe derived limit establishes a relationship between the multiplicity $m$ and the asymptotic value of the ratio $Q(x)$. Let $L = \\lim_{x\\to \\alpha} Q(x)$. We have $L = 1 - \\frac{1}{m}$. We can solve this equation for $m$:\n$$\\frac{1}{m} = 1 - L$$\n$$m = \\frac{1}{1 - L}$$\nThis suggests a pointwise estimator $\\hat{m}(x)$ for the multiplicity by replacing the limit $L$ with the ratio $Q(x)$ evaluated at a point $x$ near but not equal to $\\alpha$.\n$$\\hat{m}(x) = \\frac{1}{1 - Q(x)} = \\frac{1}{1 - \\frac{f(x) f''(x)}{(f'(x))^2}}$$\nSimplifying the expression for $\\hat{m}(x)$:\n$$\\hat{m}(x) = \\frac{(f'(x))^2}{(f'(x))^2 - f(x) f''(x)}$$\nThis is a standard formula used for multiplicity estimation, and it forms the basis of modified Newton's methods that restore quadratic convergence for multiple roots. The justification for this estimator is that as $x \\to \\alpha$, $\\hat{m}(x) \\to m$.\n\n### Part 3: Numerical Experiment Rationale\n\nThe numerical experiment aims to quantify the bias of this estimator in finite-precision arithmetic (IEEE 754 double precision). The error in $\\hat{m}(x)$ arises from two sources:\n1.  **Approximation Error**: For $x \\ne \\alpha$, $\\hat{m}(x)$ is only an approximation of $m$. This error is generally larger for $x$ further from $\\alpha$.\n2.  **Rounding Error**: For $x$ very close to $\\alpha$, both the numerator, $(f'(x))^2$, and the denominator, $(f'(x))^2 - f(x) f''(x)$, approach zero. The denominator involves the subtraction of two nearly equal quantities, since their ratio $\\frac{f(x) f''(x)}{(f'(x))^2} \\to 1 - \\frac{1}{m}$. This subtraction leads to catastrophic cancellation, a significant loss of relative precision in floating-point arithmetic.\n\nThe experiment evaluates $\\hat{m}(\\alpha+\\delta)$ for a range of offsets $\\delta$. We expect to observe a U-shaped total error curve as a function of $\\log(\\delta)$: high bias for large $\\delta$ due to approximation error, and high bias for very small $\\delta$ due to rounding error, with a minimum bias at an intermediate value of $\\delta$. The exception is the case $m=1$, where the term $f(x)f''(x)$ tends to zero faster than $(f'(x))^2$, so catastrophic cancellation is not an issue, and the bias should decrease monotonically with $\\delta$.\n\nThe Python code below implements this experiment, calculating the derivatives analytically and then evaluating the estimator to compute the mean absolute bias for each test case.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs a numerical experiment to assess the bias of a multiplicity estimator\n    for roots of a function.\n    \"\"\"\n\n    # Define the set of offsets from the root\n    deltas = np.array([1e-1, 1e-4, 1e-8, 1e-12, 1e-16])\n\n    # Case A: m=3, alpha=0, g(x)=1+x\n    m_a, alpha_a = 3, 0.0\n    g_a = lambda x: 1.0 + x\n    gp_a = lambda x: 1.0\n    gpp_a = lambda x: 0.0\n\n    # Case B: m=2, alpha=0.3, g(x)=e^x\n    m_b, alpha_b = 2, 0.3\n    g_b = lambda x: np.exp(x)\n    gp_b = lambda x: np.exp(x)\n    gpp_b = lambda x: np.exp(x)\n\n    # Case C: m=1, alpha=-0.2, g(x)=1+x^2\n    m_c, alpha_c = 1, -0.2\n    g_c = lambda x: 1.0 + x**2\n    gp_c = lambda x: 2.0 * x\n    gpp_c = lambda x: 2.0\n\n    # Case D: m=5, alpha=0, g(x)=cos(x)\n    m_d, alpha_d = 5, 0.0\n    g_d = lambda x: np.cos(x)\n    gp_d = lambda x: -np.sin(x)\n    gpp_d = lambda x: -np.cos(x)\n\n    test_cases = [\n        (m_a, alpha_a, g_a, gp_a, gpp_a),\n        (m_b, alpha_b, g_b, gp_b, gpp_b),\n        (m_c, alpha_c, g_c, gp_c, gpp_c),\n        (m_d, alpha_d, g_d, gp_d, gpp_d),\n    ]\n\n    def calculate_m_hat(m, alpha, g, gp, gpp, x):\n        \"\"\"\n        Calculates the estimated multiplicity m_hat at a point x.\n        \"\"\"\n        xa = x - alpha\n        \n        gx = g(x)\n        gpx = gp(x)\n        gppx = gpp(x)\n\n        # f(x) = (x-alpha)^m * g(x)\n        f = (xa**m) * gx\n        \n        # f'(x) = (x-alpha)^(m-1) * [m*g(x) + (x-alpha)*g'(x)]\n        fp = (xa**(m-1)) * (m * gx + xa * gpx)\n        \n        # f''(x) is computed based on m to ensure numerical stability.\n        # For m=1, the general formula involves 0 * inf which can be NaN.\n        # f''(x) = (x-alpha)^(m-2) * [m(m-1)g + 2m(x-alpha)g' + (x-alpha)^2 g'']\n        if m == 1:\n            # Simplified formula for m=1: f''(x) = 2g'(x) + (x-alpha)g''(x)\n            fpp = 2.0 * gpx + xa * gppx\n        else:\n            fpp = (xa**(m - 2)) * (m * (m - 1) * gx + xa * (2 * m * gpx + xa * gppx))\n\n        # Estimator: m_hat = (f')^2 / ((f')^2 - f*f'')\n        numerator = fp**2\n        denominator = fp**2 - f * fpp\n        \n        if denominator == 0.0:\n            # Handle cases where the denominator is exactly zero.\n            return np.nan if numerator == 0.0 else np.inf\n\n        return numerator / denominator\n\n    all_mean_biases = []\n    for m, alpha, g, gp, gpp in test_cases:\n        biases = []\n        for delta in deltas:\n            x = alpha + delta\n            m_hat = calculate_m_hat(m, alpha, g, gp, gpp, x)\n            bias = np.abs(m_hat - m)\n            biases.append(bias)\n        \n        mean_abs_bias = np.mean(biases)\n        all_mean_biases.append(mean_abs_bias)\n\n    # Print the results in the specified format\n    print(f\"[{','.join(map(str, all_mean_biases))}]\")\n\nsolve()\n```", "id": "3254009"}, {"introduction": "We now synthesize our understanding to build a truly intelligent numerical tool. This capstone practice challenges you to design and implement an adaptive root-finding algorithm that operates without prior knowledge of the root's multiplicity. Your algorithm will start with the standard Newton's method, actively monitor its own performance to detect the signature of slow, linear convergence, and then automatically estimate the root's multiplicity from the iteration history to switch to the appropriate accelerated method. This exercise moves beyond applying formulas to the practical art of designing robust, self-correcting numerical solvers [@problem_id:3253971].", "problem": "Design and implement a complete, runnable program that constructs a hybrid root-finding algorithm for scalar nonlinear equations based on Newton's method. The algorithm must begin with the standard Newton iteration and, during the run, autonomously detect whether the observed asymptotic convergence appears linear, estimate the multiplicity $m$ of the root from the observed iterates, and, if the estimated multiplicity indicates a multiple root, switch to a modified Newton strategy that compensates for multiplicity. The underlying foundational base you may assume includes: the definition of root multiplicity for a sufficiently smooth function, the standard Newton iteration for a simple root, and basic properties of Taylor expansions for smooth functions. You must not assume any specialized multiplicity-acceleration formula a priori; instead, derive all necessary relationships from the fundamental model of a multiple root and the behavior of the Newton iteration near such a root.\n\nYour program must follow these requirements.\n\n- Inputs are fixed within the program; no user input is allowed. You will implement the hybrid method and apply it to the specified test suite.\n- Start from the fundamental definition: a point $r$ is a root of multiplicity $m \\in \\mathbb{N}$ of a sufficiently smooth function $f$ if around $r$ one can write $f(x) = (x - r)^m g(x)$ with $g$ smooth and $g(r) \\neq 0$. You may also assume the standard Newton iteration $x_{k+1} = x_k - f(x_k)/f'(x_k)$ for a differentiable function $f$ near a root where $f'(x_k) \\neq 0$.\n- Your algorithm must, during the run, determine whether the current iteration appears to converge linearly and, if so, estimate the multiplicity $m$ on the fly from observed quantities that do not depend on knowing $r$. After estimating $m$, the algorithm must switch from the standard Newton iteration to an appropriate modified Newton iteration that restores fast convergence for multiple roots.\n- Detection must be robust: base the decision on a short window of recent iterations and require stability of an observable ratio to decide linear convergence. Avoid misclassifying quadratic convergence as linear. Provide reasonable numerical safeguards.\n- Stopping criteria must include both a function-value tolerance and a step-size tolerance. Use a function-value tolerance of $|f(x_k)| \\le 10^{-12}$, a step-size tolerance of $|x_{k+1} - x_k| \\le 10^{-14}$, and a maximum of $100$ iterations. Angles, where applicable, must be in radians.\n\nTest suite. Apply your algorithm to each of the following test cases; for each case, the function $f$, its derivative $f'$, and the initial guess $x_0$ are specified. All constants and numbers below are exact and must be used as given.\n\n- Case 1: $f(x) = (x - 2)^3$, $f'(x) = 3(x - 2)^2$, $x_0 = 2.8$.\n- Case 2: $f(x) = (x + 1)^2$, $f'(x) = 2(x + 1)$, $x_0 = -1.6$.\n- Case 3: $f(x) = \\sin(x)$, $f'(x) = \\cos(x)$, $x_0 = 0.1$. Angles are in radians.\n- Case 4: $f(x) = (x - 0.5)^4$, $f'(x) = 4(x - 0.5)^3$, $x_0 = 1.25$.\n- Case 5: $f(x) = (x - 1)^3 e^{x}$, $f'(x) = e^{x}(x - 1)^2(x + 2)$, $x_0 = 1.3$.\n- Case 6: $f(x) = x^3 - x - 2$, $f'(x) = 3x^2 - 1$, $x_0 = 1.0$.\n\nFor each test case, your program must return a list of exactly three values:\n- the final estimated multiplicity $m$ as a nonnegative integer (use $m = 1$ if the method determines that no switch is warranted),\n- an integer indicator equal to $1$ if the algorithm switched to the modified Newton phase at least once and $0$ otherwise,\n- the total number of iterations performed until convergence or until the maximum number of iterations is reached.\n\nFinal output format. Your program should produce a single line of output containing the results for the $6$ cases as a comma-separated list enclosed in square brackets, where each element is itself a three-element list formatted without spaces. For example, the output must look like $[[a_1,b_1,c_1],[a_2,b_2,c_2],\\dots,[a_6,b_6,c_6]]$ where each $a_i$, $b_i$, and $c_i$ is an integer as specified above.", "solution": "We begin from the definitions and local models. Let $f$ be a sufficiently smooth scalar function with a root $r$ of multiplicity $m \\in \\mathbb{N}$. By definition, there exists a smooth function $g$ with $g(r) \\neq 0$ such that $f(x) = (x - r)^m g(x)$ in a neighborhood of $r$. The standard Newton iteration for solving $f(x) = 0$ is $x_{k+1} = x_k - f(x_k)/f'(x_k)$, assuming $f'(x_k) \\neq 0$. We denote the error by $e_k = x_k - r$.\n\nLocal behavior near a multiple root. Using $f(x) = (x - r)^m g(x)$ with $g(r) \\neq 0$, we have $f'(x) = m(x - r)^{m-1} g(x) + (x - r)^m g'(x)$. At $x = r$, if $m \\ge 2$, then $f'(r) = 0$, which explains the degradation of the standard Newton method from quadratic to linear convergence. A first-order approximation near $r$ uses $g(x) \\approx g(r)$ and yields\n$$\n\\frac{f(x)}{f'(x)} \\approx \\frac{(x - r)^m g(r)}{m (x - r)^{m-1} g(r)} = \\frac{x - r}{m}.\n$$\nSubstituting into the Newton update gives\n$$\nx_{k+1} \\approx x_k - \\frac{e_k}{m} = r + \\left(1 - \\frac{1}{m}\\right) e_k.\n$$\nTherefore the asymptotic error recurrence for the standard Newton method at a multiple root is\n$$\ne_{k+1} \\approx q e_k, \\quad q = \\frac{m - 1}{m},\n$$\nwhich is linear convergence with rate $q \\in (0,1)$ when $m \\ge 2$. For a simple root with $m = 1$, a more refined Taylor expansion shows $e_{k+1} \\approx C e_k^2$ with some constant $C \\neq 0$, i.e., quadratic convergence.\n\nEstimating multiplicity from observed iterates. Since the true root $r$ is unknown, the error $e_k$ is not directly available. We instead use the observed Newton step\n$$\ns_k = x_k - x_{k+1}.\n$$\nFrom $e_{k+1} \\approx q e_k$ we have\n$$\ns_k = x_k - x_{k+1} = e_k - e_{k+1} \\approx (1 - q) e_k.\n$$\nSimilarly, $s_{k+1} \\approx (1 - q) e_{k+1} \\approx (1 - q) q e_k$. Thus the ratio of consecutive step magnitudes stabilizes to\n$$\n\\frac{|s_{k+1}|}{|s_k|} \\approx q = \\frac{m - 1}{m}.\n$$\nThis provides an observable criterion for detecting linear convergence: if the ratio of successive step magnitudes stabilizes to a value in $(0,1)$ away from zero, the iteration is behaving linearly rather than quadratically. Moreover, we can estimate $q$ by a stable statistic (such as the median) over a short window of recent ratios, and then infer\n$$\nm \\approx \\frac{1}{1 - q}.\n$$\nRounding to the nearest integer $\\ge 2$ yields an integer multiplicity estimate $\\widehat{m}$ that we can use in a modified iteration.\n\nModified Newton iteration. The fundamental local model $f(x) = (x - r)^m g(x)$ with $g(r) \\neq 0$ suggests compensating for the multiple root by scaling the Newton correction. Consider the modified update\n$$\nx_{k+1} = x_k - \\widehat{m} \\frac{f(x_k)}{f'(x_k)}.\n$$\nSubstituting the local approximation for $f/f'$ yields\n$$\nx_{k+1} \\approx x_k - \\widehat{m} \\frac{e_k}{m} = r + \\left(1 - \\frac{\\widehat{m}}{m}\\right) e_k.\n$$\nIf $\\widehat{m} = m$, the first-order term vanishes and the next term in the Taylor expansion dictates a higher-order (typically quadratic) convergence. Therefore, once a reliable estimate $\\widehat{m}$ is available, switching to the modified update restores fast convergence.\n\nDetection logic and safeguards. To avoid misclassifying quadratic convergence (simple roots) as linear, we impose:\n- a stabilization requirement: over a window of recent ratios $r_i = |s_{i+1}|/|s_i|$, their relative variation must be small;\n- a magnitude requirement: the stabilized ratio must lie clearly away from zero, e.g., $r_i \\in [\\rho_{\\min}, \\rho_{\\max}]$ with $\\rho_{\\min} > 0$;\n- a consistency requirement: no strong downward trend across the window, as quadratic convergence would decrease the ratio toward zero.\n\nWith these in place, we compute $\\widehat{q}$ as the median of the windowed ratios and set $\\widehat{m} = \\max\\{2, \\min\\{M_{\\max}, \\mathrm{round}(1/(1 - \\widehat{q}))\\}\\}$ with a reasonable cap $M_{\\max}$ for robustness. We only switch once per run and we do not switch if the criteria are not met.\n\nStopping criteria. We stop if $|f(x_k)| \\le 10^{-12}$ or $|x_{k+1} - x_k| \\le 10^{-14}$, or if $k$ reaches $100$. We also guard against near-zero derivatives by aborting gracefully if $|f'(x_k)|$ is too small.\n\nAlgorithm summary.\n- Initialize with the standard Newton update $x_{k+1} = x_k - f(x_k)/f'(x_k)$.\n- Maintain a sliding window of the last few step ratios $|s_{k+1}|/|s_k|$.\n- If the ratios stabilize within a tolerance and lie in a prescribed interval, declare linear convergence and estimate $\\widehat{m} \\approx 1/(1 - \\widehat{q})$.\n- If $\\widehat{m} \\ge 2$, switch to the modified update $x_{k+1} = x_k - \\widehat{m} f(x_k)/f'(x_k)$ and continue until convergence.\n- Output, for each test case, the final $\\widehat{m}$ (or $1$ if no switch), an indicator of whether the switch occurred, and the total iteration count.\n\nApplication to the test suite. The cases include multiple roots of known multiplicities:\n- $(x - 2)^3$ with $m = 3$,\n- $(x + 1)^2$ with $m = 2$,\n- $(x - 0.5)^4$ with $m = 4$,\n- $(x - 1)^3 e^{x}$ with $m = 3$,\nand two simple-root cases:\n- $\\sin(x)$ at $x = 0$,\n- $x^3 - x - 2$ at the unique real root near $x \\approx 1.521\\dots$.\nFor multiple roots, the step ratios stabilize to $q = (m - 1)/m$, the algorithm estimates $\\widehat{m}$ accordingly, switches, and achieves rapid convergence. For simple roots, the step ratios decrease toward zero without stabilization away from zero, so the algorithm does not switch and $\\widehat{m} = 1$. The final program computes the requested triple for each case and prints them as a single bracketed list in the specified format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef hybrid_newton(f, df, x0, tol_f=1e-12, tol_x=1e-14, max_iter=100):\n    \"\"\"\n    Hybrid Newton solver:\n    - Starts with standard Newton.\n    - Detects linear convergence via stabilization of step ratios.\n    - Estimates multiplicity m_hat = round(1/(1 - q_hat)) from ratios and switches to modified Newton.\n    Returns: (m_hat_final, switched_flag, iterations)\n    \"\"\"\n    x = float(x0)\n    switched = False\n    m_hat = 1  # default multiplicity estimate\n    step_history = []\n    ratio_window = []\n    window_size = 4\n\n    # thresholds for detection of linear convergence\n    rho_min = 0.15   # away from zero to avoid misclassifying quadratic convergence\n    rho_max = 0.98   # bounded away from 1 to avoid extremely slow cases\n    stab_rel_tol = 0.03  # max relative spread across the window\n    deriv_eps = 1e-16\n\n    it = 0\n    # We'll run at most max_iter iterations\n    while it  max_iter:\n        fx = f(x)\n        dfx = df(x)\n        if abs(fx) = tol_f:\n            break\n        if abs(dfx)  deriv_eps:\n            # Derivative too small, abort iteration\n            break\n\n        # Choose the update depending on switch state\n        if not switched:\n            delta = -fx / dfx\n        else:\n            delta = -m_hat * fx / dfx\n\n        x_new = x + delta\n\n        # Check step tolerance\n        if abs(delta) = tol_x:\n            x = x_new\n            it += 1\n            break\n\n        # Collect step and ratio info only during standard Newton before switching\n        if not switched:\n            step_history.append(abs(delta))\n            # compute ratio if possible\n            if len(step_history) >= 2:\n                r = step_history[-1] / step_history[-2] if step_history[-2] != 0.0 else np.inf\n                ratio_window.append(r)\n                if len(ratio_window) > window_size:\n                    ratio_window.pop(0)\n\n                # Detection: require full window\n                if len(ratio_window) == window_size:\n                    # Check bounds and stability\n                    ratios = np.array(ratio_window)\n                    # Ensure all ratios in [rho_min, rho_max]\n                    in_range = np.all((ratios >= rho_min)  (ratios = rho_max)  np.isfinite(ratios))\n                    # Stability via relative spread: (max - min)/max = tol\n                    rel_spread = (ratios.max() - ratios.min()) / max(ratios.max(), 1e-300)\n                    stable = rel_spread = stab_rel_tol\n                    # Optional: ensure no strong downward trend\n                    non_decreasingish = ratios[-1] >= 0.9 * ratios[0]\n                    if in_range and stable and non_decreasingish:\n                        q_hat = float(np.median(ratios))\n                        if q_hat  1.0:\n                            m_est = 1.0 / (1.0 - q_hat)\n                            # Round to nearest integer and clamp to [2, 20]\n                            m_hat_candidate = int(max(2, min(20, int(np.floor(m_est + 0.5)))))\n                            # Switch only if candidate >= 2\n                            if m_hat_candidate >= 2:\n                                m_hat = m_hat_candidate\n                                switched = True\n                                # Clear windows to avoid re-triggering\n                                ratio_window.clear()\n                                step_history.clear()\n\n        # Advance iteration\n        x = x_new\n        it += 1\n\n    # Ensure m_hat reported as 1 if no switch occurred\n    if not switched:\n        m_hat_report = 1\n    else:\n        m_hat_report = int(m_hat)\n\n    return (int(m_hat_report), int(1 if switched else 0), int(it))\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (f, df, x0)\n    def f1(x): return (x - 2.0) ** 3\n    def df1(x): return 3.0 * (x - 2.0) ** 2\n\n    def f2(x): return (x + 1.0) ** 2\n    def df2(x): return 2.0 * (x + 1.0)\n\n    def f3(x): return np.sin(x)\n    def df3(x): return np.cos(x)\n\n    def f4(x): return (x - 0.5) ** 4\n    def df4(x): return 4.0 * (x - 0.5) ** 3\n\n    def f5(x): return ((x - 1.0) ** 3) * np.exp(x)\n    def df5(x): return np.exp(x) * (x - 1.0) ** 2 * (x + 2.0)\n\n    def f6(x): return x ** 3 - x - 2.0\n    def df6(x): return 3.0 * x ** 2 - 1.0\n\n    test_cases = [\n        (f1, df1, 2.8),\n        (f2, df2, -1.6),\n        (f3, df3, 0.1),\n        (f4, df4, 1.25),\n        (f5, df5, 1.3),\n        (f6, df6, 1.0),\n    ]\n\n    results = []\n    for f, df, x0 in test_cases:\n        m_hat, switched, iters = hybrid_newton(f, df, x0, tol_f=1e-12, tol_x=1e-14, max_iter=100)\n        results.append([m_hat, switched, iters])\n\n    # Format without spaces as required: [[a,b,c],[...],...]\n    inner = \",\".join(\"[\" + \",\".join(str(v) for v in triple) + \"]\" for triple in results)\n    print(f\"[{inner}]\")\n\nsolve()\n```", "id": "3253971"}]}