{"hands_on_practices": [{"introduction": "To truly understand an algorithm, it's often best to start with a pencil-and-paper application. This first practice grounds the bisection method in a tangible physical problem: calculating the necessary thickness of lead shielding to protect against gamma radiation [@problem_id:3210875]. By working through this problem manually, you will reinforce your understanding of the Intermediate Value Theorem, the method's convergence guarantee, and how to determine the required number of iterations to achieve a desired accuracy.", "problem": "A narrow beam of gamma radiation passes through a slab of lead of thickness $x$ (in centimeters), and its intensity after the slab is modeled by the attenuation law $I(x) = I_0 \\exp(-\\mu x)$, where $I_0$ is the incident intensity and $\\mu$ is the linear attenuation coefficient of lead at the relevant photon energy. A safety threshold requires the transmitted intensity to satisfy $I(x) = I_{\\text{safe}}$. Consider the root-finding problem defined by $f(x) = I(x) - I_{\\text{safe}}$, so the target thickness solves $f(x) = 0$. Take $I_0 = 8.0 \\times 10^{-3}$ Sv/h, $\\mu = 1.25$ cm$^{-1}$, and $I_{\\text{safe}} = 1.0 \\times 10^{-6}$ Sv/h. \n\nUsing only the Intermediate Value Theorem and the definition of the bisection method, do the following:\n- Justify the existence and uniqueness of the root of $f(x)$ on a physically reasonable interval.\n- Initialize the bisection method on the interval $[0, 10]$ (in centimeters), and determine the minimal number of bisection iterations required to guarantee an absolute error in the thickness less than $\\varepsilon = 0.01$ cm.\n- Execute that number of bisection iterations to obtain a numerical approximation of the required thickness.\n\nRound your final thickness to $4$ significant figures. Express the final thickness in centimeters (cm).", "solution": "The problem asks for an analysis of the thickness of a lead slab required to attenuate gamma radiation to a safe level, using the bisection method. First, the problem is validated and found to be scientifically sound and well-posed. We will address each part of the problem in sequence.\n\nThe root-finding problem is to solve $f(x) = 0$, where the function $f(x)$ is defined as:\n$$f(x) = I_0 \\exp(-\\mu x) - I_{\\text{safe}}$$\nThe given parameters are the incident intensity $I_0 = 8.0 \\times 10^{-3}$ Sv/h, the linear attenuation coefficient $\\mu = 1.25$ cm$^{-1}$, and the safe intensity threshold $I_{\\text{safe}} = 1.0 \\times 10^{-6}$ Sv/h. The variable $x$ represents the slab thickness in centimeters.\n\n**Justification of Existence and Uniqueness of the Root**\n\nFirst, we establish the existence of a root in a physically reasonable interval. The thickness $x$ must be non-negative, so the domain of interest is $[0, \\infty)$. The problem suggests the interval $[0, 10]$.\n\n1.  **Continuity**: The function $f(x)$ is a composition of the exponential function and linear functions, which are continuous for all real numbers. Therefore, $f(x)$ is continuous on the interval $[0, \\infty)$.\n\n2.  **Intermediate Value Theorem (IVT)**: We evaluate the function $f(x)$ at the endpoints of the interval $[0, 10]$.\n    At $x=0$:\n    $$f(0) = I_0 \\exp(0) - I_{\\text{safe}} = I_0 - I_{\\text{safe}} = 8.0 \\times 10^{-3} - 1.0 \\times 10^{-6} = 7.999 \\times 10^{-3}$$\n    Since $f(0)  0$, the function is positive at the left endpoint.\n    At $x=10$:\n    $$f(10) = I_0 \\exp(-\\mu \\times 10) - I_{\\text{safe}} = (8.0 \\times 10^{-3}) \\exp(-1.25 \\times 10) - 1.0 \\times 10^{-6}$$\n    $$f(10) = (8.0 \\times 10^{-3}) \\exp(-12.5) - 1.0 \\times 10^{-6}$$\n    Using the approximation $\\exp(-12.5) \\approx 3.727 \\times 10^{-6}$:\n    $$f(10) \\approx (8.0 \\times 10^{-3})(3.727 \\times 10^{-6}) - 1.0 \\times 10^{-6} \\approx 2.981 \\times 10^{-8} - 1.0 \\times 10^{-6} \\approx -9.702 \\times 10^{-7}$$\n    Since $f(10)  0$, the function is negative at the right endpoint.\n    Because $f(x)$ is continuous on $[0, 10]$ and $f(0)  0$ and $f(10)  0$, the Intermediate Value Theorem guarantees that there is at least one root $x^*$ in the open interval $(0, 10)$.\n\n3.  **Uniqueness**: To prove uniqueness, we show that $f(x)$ is a strictly monotonic function. Let $x_1$ and $x_2$ be any two distinct points in the domain $[0, \\infty)$ such that $x_2  x_1$.\n    Since the coefficient $\\mu = 1.25$ is positive, multiplying the inequality by $-\\mu$ reverses its direction:\n    $$-\\mu x_2  -\\mu x_1$$\n    The exponential function $y(z) = \\exp(z)$ is strictly increasing for all real $z$. Applying it to the inequality preserves the direction:\n    $$\\exp(-\\mu x_2)  \\exp(-\\mu x_1)$$\n    The incident intensity $I_0 = 8.0 \\times 10^{-3}$ is also positive, so multiplying by $I_0$ again preserves the inequality:\n    $$I_0 \\exp(-\\mu x_2)  I_0 \\exp(-\\mu x_1)$$\n    Finally, subtracting the constant $I_{\\text{safe}}$ from both sides does not change the inequality:\n    $$I_0 \\exp(-\\mu x_2) - I_{\\text{safe}}  I_0 \\exp(-\\mu x_1) - I_{\\text{safe}}$$\n    This is equivalent to $f(x_2)  f(x_1)$.\n    Thus, for any $x_2  x_1$, we have $f(x_2)  f(x_1)$, which is the definition of a strictly decreasing function. A strictly monotonic function can cross any horizontal line (including $y=0$) at most once. Since we have already established the existence of at least one root, this proves the root is unique.\n\n**Minimal Number of Bisection Iterations**\n\nThe bisection method starts with an interval $[a, b]$ of length $L = b-a$. After $n$ iterations, the approximation $c_n$ (the midpoint of the $(n-1)$-th interval) is guaranteed to have an absolute error with respect to the true root $x^*$ that is bounded by:\n$$|c_n - x^*| \\leq \\frac{b-a}{2^n}$$\nWe are given the initial interval $[a, b] = [0, 10]$ and the required error tolerance $\\varepsilon = 0.01$ cm. We need to find the smallest integer $n$ such that:\n$$\\frac{10 - 0}{2^n}  0.01$$\n$$\\frac{10}{2^n}  \\frac{1}{100} \\implies 1000  2^n$$\nTo solve for $n$, we take the base-$2$ logarithm of both sides:\n$$n  \\log_2(1000)$$\nWe can compute this value:\n$$n  \\frac{\\ln(1000)}{\\ln(2)} \\approx \\frac{6.907755}{0.693147} \\approx 9.96578$$\nSince the number of iterations $n$ must be an integer, the minimal number of iterations required to guarantee the specified accuracy is $n = 10$.\n\n**Execution of Bisection Iterations**\n\nWe perform $10$ iterations of the bisection method on the interval $[a_0, b_0] = [0, 10]$. At each step $k$, we compute the midpoint $c_k = (a_{k-1} + b_{k-1})/2$. The sign of $f(c_k)$ determines the new interval $[a_k, b_k]$. The sign of $f(c_k) = I_0 \\exp(-\\mu c_k) - I_{\\text{safe}}$ is positive if $I_0 \\exp(-\\mu c_k)  I_{\\text{safe}}$ and negative otherwise. This is equivalent to comparing $c_k$ to the true root $x^* = \\frac{1}{\\mu}\\ln(I_0/I_{\\text{safe}}) \\approx 7.1898$. If $c_k  x^*$, then $f(c_k)0$. If $c_k  x^*$, then $f(c_k)0$.\n\nThe iterations are summarized below:\n- **k=1**: $a_0 = 0, b_0 = 10$. $c_1 = 5.0$. $f(5.0)  0$. New interval $[a_1, b_1] = [5.0, 10.0]$.\n- **k=2**: $a_1 = 5.0, b_1 = 10.0$. $c_2 = 7.5$. $f(7.5)  0$. New interval $[a_2, b_2] = [5.0, 7.5]$.\n- **k=3**: $a_2 = 5.0, b_2 = 7.5$. $c_3 = 6.25$. $f(6.25)  0$. New interval $[a_3, b_3] = [6.25, 7.5]$.\n- **k=4**: $a_3 = 6.25, b_3 = 7.5$. $c_4 = 6.875$. $f(6.875)  0$. New interval $[a_4, b_4] = [6.875, 7.5]$.\n- **k=5**: $a_4 = 6.875, b_4 = 7.5$. $c_5 = 7.1875$. $f(7.1875)  0$. New interval $[a_5, b_5] = [7.1875, 7.5]$.\n- **k=6**: $a_5 = 7.1875, b_5 = 7.5$. $c_6 = 7.34375$. $f(7.34375)  0$. New interval $[a_6, b_6] = [7.1875, 7.34375]$.\n- **k=7**: $a_6 = 7.1875, b_6 = 7.34375$. $c_7 = 7.265625$. $f(7.265625)  0$. New interval $[a_7, b_7] = [7.1875, 7.265625]$.\n- **k=8**: $a_7 = 7.1875, b_7 = 7.265625$. $c_8 = 7.2265625$. $f(7.2265625)  0$. New interval $[a_8, b_8] = [7.1875, 7.2265625]$.\n- **k=9**: $a_8 = 7.1875, b_8 = 7.2265625$. $c_9 = 7.20703125$. $f(7.20703125)  0$. New interval $[a_9, b_9] = [7.1875, 7.20703125]$.\n- **k=10**: $a_9 = 7.1875, b_9 = 7.20703125$. $c_{10} = 7.197265625$.\n\nAfter $10$ iterations, the numerical approximation for the root is the last computed midpoint, $c_{10}$.\nThe required thickness is approximately $x \\approx 7.197265625$ cm. The problem requires rounding this result to $4$ significant figures.\nThe first four significant figures are $7, 1, 9, 7$. The fifth digit is $2$, which is less than $5$, so we round down.\nThe final numerical approximation is $7.197$ cm.", "answer": "$$\\boxed{7.197}$$", "id": "3210875"}, {"introduction": "Having applied the method by hand, the next step is to implement it in code. This exercise guides you through building a bisection solver and using it to explore the behavior of the function $f(x) = \\sin(10x) - x$, which has multiple roots [@problem_id:3211006]. This practice highlights a critical aspect of root-finding: the outcome is entirely dependent on the initial bracket you provide, demonstrating the importance of initial analysis in numerical problem-solving.", "problem": "Consider the continuous function $f(x) = \\sin(10x) - x$, where the sine function uses angles in radians. The Bisection Method is applied to locate a real root of $f(x)$ in a closed interval $[a,b]$ satisfying $f(a) \\cdot f(b)  0$. Assume no floating-point underflow or overflow occurs in the evaluations. Your task is to implement a complete Bisection Method program that, for each interval in the test suite, does all of the following without using any external input: \n- Select the midpoint $m = \\frac{a+b}{2}$ at each iteration.\n- Decide the next interval by preserving the sign change condition.\n- Terminate either when $f(m) = 0$ exactly or when the interval length is at most a prescribed tolerance.\n- Report the approximate root and a worst-case absolute error bound implied by the final interval length at termination.\n\nThe derivation and algorithmic design must start explicitly from fundamental mathematical principles applicable to the Bisection Method, especially the Intermediate Value Theorem for continuous functions on closed intervals and basic properties of nested intervals. You must not rely on any domain-specific shortcuts beyond these principles.\n\nImplement the program for the following test suite, which spans a standard case, symmetry checks, and multi-root bracketing. In every case, use a termination tolerance of $10^{-12}$ on the interval length and a maximum of $1000$ iterations.\n- Test $1$: interval $[-2,2]$.\n- Test $2$: interval $[0.25,0.3]$.\n- Test $3$: interval $[-0.3,-0.25]$.\n- Test $4$: interval $[0.7,0.75]$.\n- Test $5$: interval $[-1,1]$.\n\nFor each test, compute:\n- The approximate root $r$ returned by the algorithm.\n- The worst-case absolute error bound $E$, defined as half the final interval length at termination.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test’s result is itself a list $[r,E]$. Both $r$ and $E$ must be expressed as decimal floats rounded to $10$ decimal places. For example, output should look like $[[r_1,E_1],[r_2,E_2],\\dots]$ with no spaces. Angles must be treated in radians throughout.", "solution": "The problem requires the implementation of the Bisection Method to find a real root of the continuous function $f(x) = \\sin(10x) - x$ on a given closed interval $[a, b]$, provided the initial condition $f(a) \\cdot f(b)  0$ is met. The process must terminate when the interval length is at most a tolerance of $10^{-12}$, or after a maximum of $1000$ iterations. The final output for each test case must include the approximate root and the worst-case absolute error bound.\n\nThe design of the algorithm is grounded in the Intermediate Value Theorem and the properties of nested intervals.\n\n**Fundamental Principle: The Intermediate Value Theorem (IVT)**\nThe IVT states that if a function $g(x)$ is continuous on a closed interval $[a, b]$, then for any value $k$ between $g(a)$ and $g(b)$, there exists at least one number $c$ in the open interval $(a, b)$ such that $g(c) = k$.\n\nFor the purpose of root-finding, we consider the case where $k=0$. If a continuous function $f(x)$ on an interval $[a,b]$ has values $f(a)$ and $f(b)$ of opposite sign, meaning their product $f(a) \\cdot f(b)  0$, then $0$ is a value between $f(a)$ and $f(b)$. The IVT then guarantees the existence of at least one root $c \\in (a, b)$ where $f(c) = 0$. The function provided, $f(x) = \\sin(10x) - x$, is a sum of the sine function and a linear function, both of which are continuous for all real numbers. Therefore, $f(x)$ is continuous on any closed interval, and the IVT is applicable.\n\n**Algorithmic Design: The Bisection Method**\nThe Bisection Method leverages the IVT in an iterative fashion to narrow down the interval containing a root.\n\n1.  **Initialization**: Start with a given interval $[a_0, b_0]$ and verify that $f(a_0) \\cdot f(b_0)  0$. This ensures at least one root lies within $[a_0, b_0]$. The problem statement guarantees this condition for all test cases. Set an iteration counter $k=0$.\n\n2.  **Iteration**: For the current interval $[a_k, b_k]$, perform the following steps:\n    a.  **Calculate Midpoint**: Compute the midpoint of the interval, $m_k = \\frac{a_k + b_k}{2}$. This midpoint $m_k$ serves as the current approximation of the root.\n    b.  **Evaluate Function**: Calculate the value of the function at the midpoint, $f(m_k)$.\n    c.  **Update Interval**: Based on the sign of $f(m_k)$, decide which half of the interval $[a_k, b_k]$ to keep for the next iteration, $[a_{k+1}, b_{k+1}]$, such that the sign-change property is preserved.\n        i.  If $f(m_k) = 0$ (a highly unlikely event in floating-point arithmetic), the root has been found exactly. The algorithm terminates.\n        ii. If $f(a_k) \\cdot f(m_k)  0$, the root must lie in the left subinterval. The new interval for the next iteration becomes $[a_{k+1}, b_{k+1}] = [a_k, m_k]$.\n        iii. If $f(a_k) \\cdot f(m_k)  0$, then $f(a_k)$ and $f(m_k)$ have the same sign. Since we know $f(a_k)$ and $f(b_k)$ have opposite signs, it must be that $f(m_k)$ and $f(b_k)$ have opposite signs. Thus, the root must lie in the right subinterval. The new interval becomes $[a_{k+1}, b_{k+1}] = [m_k, b_k]$.\n    d.  Increment the iteration counter $k \\leftarrow k+1$.\n\n3.  **Termination**: The iterative process continues until a termination criterion is met.\n    a.  The length of the interval, $b_k - a_k$, becomes less than or equal to the specified tolerance, $\\epsilon_{tol} = 10^{-12}$.\n    b.  An exact root is found, $f(m_k) = 0$.\n    c.  The maximum number of iterations, $N_{max} = 1000$, is reached. This acts as a safeguard.\n\n**Convergence and Error Analysis**\nThe sequence of intervals $[a_k, b_k]$ forms a set of nested intervals, i.e., $[a_{k+1}, b_{k+1}] \\subset [a_k, b_k]$ for all $k \\geq 0$. The length of the interval at iteration $k$, denoted by $L_k = b_k - a_k$, is halved at each step:\n$$L_k = \\frac{L_{k-1}}{2} = \\dots = \\frac{L_0}{2^k}$$\nwhere $L_0 = b_0 - a_0$ is the initial interval length. As $k \\to \\infty$, the length $L_k \\to 0$. The Nested Interval Theorem ensures that the intersection of this infinite sequence of closed, nested intervals converges to a single point. Since the IVT guarantees a root exists in every interval $[a_k, b_k]$, this single point of convergence must be a root of the function.\n\nWhen the algorithm terminates, say at iteration $N$, we have a final interval $[a_N, b_N]$ that contains a root $r^*$. The best estimate for this root is the midpoint of this final interval, which we report as the approximate root $r$:\n$$r = \\frac{a_N + b_N}{2}$$\nThe true root $r^*$ can be any point within $[a_N, b_N]$. The absolute error of our approximation is $|r - r^*|$. The maximum possible distance from the midpoint $r$ to any point in the interval $[a_N, b_N]$ occurs at the endpoints $a_N$ or $b_N$. This maximum distance is half the length of the interval. Therefore, a guaranteed upper bound on the absolute error, the worst-case absolute error $E$, is given by:\n$$E = \\frac{b_N - a_N}{2}$$\nSince termination occurs when $b_N - a_N \\leq \\epsilon_{tol}$, the error bound on the reported root $r$ will satisfy $E \\leq \\frac{\\epsilon_{tol}}{2}$, which is $E \\leq \\frac{10^{-12}}{2} = 5 \\times 10^{-13}$. The program will compute and report $r$ and $E$ for each test case provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the Bisection Method to find roots of f(x) = sin(10x) - x\n    for a given suite of test cases and formats the output as specified.\n    \"\"\"\n\n    # Define the function f(x)\n    def f(x: float) - float:\n        return np.sin(10 * x) - x\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        [-2.0, 2.0],        # Test 1\n        [0.25, 0.3],        # Test 2\n        [-0.3, -0.25],      # Test 3\n        [0.7, 0.75],        # Test 4\n        [-1.0, 1.0],        # Test 5\n    ]\n\n    # Define constants from the problem statement\n    TOLERANCE = 1e-12\n    MAX_ITER = 1000\n\n    results_list = []\n\n    for case in test_cases:\n        a, b = case[0], case[1]\n        \n        # Ensure the interval is correctly ordered\n        if a  b:\n            a, b = b, a\n\n        f_a = f(a)\n        \n        # The problem guarantees f(a) * f(b)  0, so no check is needed.\n\n        for _ in range(MAX_ITER):\n            # Check termination condition on interval length\n            if (b - a) = TOLERANCE:\n                break\n            \n            # Use this formulation for midpoint to avoid potential overflow with large a,b\n            # and maintain precision.\n            m = a + (b - a) / 2.0\n            f_m = f(m)\n\n            # Check for exact root found\n            if f_m == 0.0:\n                a, b = m, m\n                break\n\n            # Update the interval based on the sign change\n            if f_a * f_m  0:\n                b = m\n            else:\n                a = m\n                f_a = f_m # Update f_a since a has changed\n\n        # The final approximation is the midpoint of the last interval\n        approx_root = a + (b - a) / 2.0\n        \n        # The worst-case absolute error bound is half the final interval length\n        error_bound = (b - a) / 2.0\n\n        # Round results to 10 decimal places as specified\n        rounded_root = np.round(approx_root, 10)\n        rounded_error = np.round(error_bound, 10)\n        \n        # Format numbers to avoid scientific notation and ensure 10 decimal places\n        # This is for consistent string formatting, especially for very small errors\n        formatted_root = f\"{rounded_root:.10f}\"\n        formatted_error = f\"{rounded_error:.10f}\"\n        \n        results_list.append(f\"[{formatted_root},{formatted_error}]\")\n\n    # Final print statement in the exact required format: [[r1,E1],[r2,E2],...]\n    print(f\"[{','.join(results_list)}]\")\n\nsolve()\n```", "id": "3211006"}, {"introduction": "The power of a numerical primitive like the bisection method lies in its adaptability. This final practice challenges you to modify the core root-finding algorithm to solve a different class of problem: optimization [@problem_id:3210918]. By applying bisection to find the zero of a function's derivative, you will effectively locate the function's minimum, turning a root-finder into a basic optimization tool.", "problem": "Consider a differentiable and unimodal real-valued function $f : \\mathbb{R} \\to \\mathbb{R}$ with a unique local minimum at $x^\\star$. A unimodal function with a unique local minimum satisfies the property that its derivative $f'(x)$ is strictly negative for all $x  x^\\star$, is strictly positive for all $x  x^\\star$, and equals zero at $x^\\star$. The existence of a root of $f'(x)$ in an interval $[a,b]$ can be guaranteed by the Intermediate Value Theorem (IVT), provided that $f'(x)$ is continuous and $f'(a)  0  f'(b)$. By Fermat's theorem for stationary points, a local minimum of a differentiable function inside an open interval occurs at a point where the derivative is zero. Your task is to design and implement a modification of the classical bisection method that uses only sign checks of $f'(x)$ to locate the unique local minimum $x^\\star$ of a given unimodal function.\n\nAlgorithm requirements:\n- The algorithm must maintain a bracketing interval $[a,b]$ and rely solely on the sign of $f'(x)$ to update the bracket. No magnitude of $f'(x)$ or higher derivatives may be used.\n- If $f'(a) = 0$, return $a$ immediately. If $f'(b) = 0$, return $b$ immediately.\n- If $f'(a)  0  f'(b)$, perform bisection on $f'(x)$: at each iteration, compute $m = \\frac{a+b}{2}$, evaluate the sign of $f'(m)$, and update $a \\leftarrow m$ if $f'(m)  0$, or $b \\leftarrow m$ if $f'(m)  0$. If $f'(m) = 0$, return $m$.\n- If $f'(a) \\ge 0$ and $f'(b) \\ge 0$, then the function is nondecreasing over $[a,b]$ and the minimum over $[a,b]$ is attained at $a$; return $a$.\n- If $f'(a) \\le 0$ and $f'(b) \\le 0$, then the function is nonincreasing over $[a,b]$ and the minimum over $[a,b]$ is attained at $b$; return $b$.\n- If the bracket does not satisfy any of the above preconditions for a unimodal minimum (for example, $f'(a)  0$ and $f'(b)  0$), return a special value indicating the precondition failure.\n\nTermination requirement:\n- Use a tolerance $\\varepsilon  0$ and perform a number of iterations $N$ sufficient to ensure the final bracket length is at most $\\varepsilon$. The bisection update halves the bracket length each iteration.\n\nConvergence analysis objective:\n- Explain from first principles why the modified bisection method converges linearly in the interval length and yields an approximation within $\\varepsilon$ of $x^\\star$ under the stated unimodality and continuity assumptions.\n\nTest suite:\nImplement your program to run the following test cases. For each case, return a floating-point number representing the approximated minimizer:\n- Case $1$ (happy path): $f(x) = (x - 2)^2 + 5$, $f'(x) = 2(x - 2)$, $[a,b] = [0,5]$, $\\varepsilon = 10^{-8}$.\n- Case $2$ (happy path with nontrivial derivative): $f(x) = e^{x} + (x - 3)^2$, $f'(x) = e^{x} + 2(x - 3)$, $[a,b] = [-5,5]$, $\\varepsilon = 10^{-8}$. Angles do not appear; no angle units are needed.\n- Case $3$ (boundary minimum due to zero derivative at an endpoint): $f(x) = x^2$, $f'(x) = 2x$, $[a,b] = [0,3]$, $\\varepsilon = 10^{-12}$.\n- Case $4$ (monotone increasing over the interval): $f(x) = e^{x}$, $f'(x) = e^{x}$, $[a,b] = [-3,1]$, $\\varepsilon = 10^{-8}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases listed above (for example, $[x_1,x_2,x_3,x_4]$). If a precondition failure occurs, output the floating-point value $\\mathrm{nan}$ for that case. No physical units or angle units are involved; output pure real numbers.", "solution": "### Principle-Based Solution\n\nThe core task is to find the unique minimizer $x^\\star$ of a differentiable, unimodal function $f(x)$. A key property of such a function is that its derivative, $f'(x)$, is zero at the minimum, i.e., $f'(x^\\star)=0$. Furthermore, the unimodality definition provided implies that $f'(x)  0$ for $x  x^\\star$ and $f'(x)  0$ for $x  x^\\star$. This means that $f'(x)$ is a strictly increasing function that crosses the x-axis exactly once, at $x=x^\\star$.\n\nThis transforms the minimization problem for $f(x)$ into a root-finding problem for $f'(x)$. The bisection method is an ideal candidate for this task, as it relies only on the sign of the function whose root is being sought, which is exactly the information provided by $f'(x)$.\n\nThe proposed algorithm first handles edge cases and preconditions based on the behavior of $f'(x)$ at the boundaries of the search interval $[a,b]$.\n\n1.  **Direct Root Check**: If $f'(a)=0$ or $f'(b)=0$, then by definition, an endpoint is the minimizer $x^\\star$. The algorithm correctly returns $a$ or $b$.\n\n2.  **Monotonicity Check**:\n    - If $f'(a) \\ge 0$ and $f'(b) \\ge 0$: Since $f'(x)$ is a non-decreasing function (a consequence of the unimodality of $f$), this implies $f'(x) \\ge 0$ for all $x \\ge a$. Thus, $f(x)$ is non-decreasing over the interval $[a,b]$. The minimum value of $f(x)$ on this interval is therefore at the left endpoint, $x=a$.\n    - If $f'(a) \\le 0$ and $f'(b) \\le 0$: This implies that the root $x^\\star$ must lie at or to the right of $b$, i.e., $x^\\star \\ge b$. Consequently, $f'(x) \\le 0$ for all $x \\in [a,b]$, meaning $f(x)$ is non-increasing over the interval. The minimum value of $f(x)$ on this interval is at the right endpoint, $x=b$.\n\n3.  **Precondition Violation**: If $f'(a)  0$ and $f'(b)  0$, this would imply that the non-decreasing function $f'(x)$ has a value at $a$ that is greater than its value at $b$, which is a contradiction for $ab$. Such a condition violates the premise that $f(x)$ is unimodal with a unique minimum. The algorithm correctly identifies this as a failure case.\n\n4.  **Bisection Iteration**: The primary case for iteration is when $f'(a)  0$ and $f'(b)  0$. By the Intermediate Value Theorem, since $f'(x)$ is continuous, there must be at least one root $x^\\star \\in (a,b)$. Given the unimodality property, this root is unique. The bisection algorithm systematically shrinks this bracketing interval:\n    - Let the initial interval be $[a_0, b_0]$.\n    - At each iteration $k$, the midpoint $m_k = \\frac{a_k + b_k}{2}$ is calculated.\n    - The sign of $f'(m_k)$ determines which half of the interval contains the root $x^\\star$:\n        - If $f'(m_k)  0$, then $m_k  x^\\star$. Since we know $x^\\star  b_k$, the root must be in $[m_k, b_k]$. We set $a_{k+1} = m_k$ and $b_{k+1} = b_k$.\n        - If $f'(m_k)  0$, then $m_k  x^\\star$. Since we know $x^\\star  a_k$, the root must be in $[a_k, m_k]$. We set $a_{k+1} = a_k$ and $b_{k+1} = m_k$.\n        - If $f'(m_k) = 0$, the root is found exactly, and the algorithm terminates.\n\n### Convergence Analysis\n\nThe convergence of this method is guaranteed and can be precisely quantified.\n\n- **Interval Reduction**: Let the length of the interval at iteration $k$ be $L_k = b_k - a_k$. At the next iteration, the new interval $[a_{k+1}, b_{k+1}]$ will have length $L_{k+1} = \\frac{L_k}{2}$. In every case, the interval length is exactly halved at each step.\n\n- **Linear Convergence**: After $k$ iterations, the length of the interval is $L_k = L_0 \\cdot (1/2)^k$, where $L_0 = b_0 - a_0$ is the initial length. The error in approximating the root $x^\\star$ with the midpoint of the interval, $m_k = (a_k+b_k)/2$, is bounded by half the interval length: $|m_k - x^\\star| \\le L_k/2$. Let's consider the interval length $L_k$ as a measure of the error bound. The ratio of successive error bounds is $L_{k+1}/L_k = 1/2$. This constant ratio indicates that the error decreases by a fixed factor at each step, which is the definition of linear convergence with a rate of $1/2$.\n\n- **Termination and Accuracy**: The algorithm is required to terminate when the interval length $L_N = b_N - a_N$ is at most the tolerance $\\varepsilon$. To find the number of iterations $N$ required, we solve the inequality:\n$$L_N = (b_0 - a_0) \\cdot 2^{-N} \\le \\varepsilon$$\n$$2^N \\ge \\frac{b_0 - a_0}{\\varepsilon}$$\n$$N \\ge \\log_2\\left(\\frac{b_0 - a_0}{\\varepsilon}\\right)$$\nBy performing $N = \\lceil \\log_2((b_0 - a_0)/\\varepsilon) \\rceil$ iterations, or more simply by iterating with a `while` loop until $b-a \\le \\varepsilon$, we guarantee that the final interval $[a_N, b_N]$ contains the minimizer $x^\\star$ and has a width no greater than $\\varepsilon$. Any point chosen from this interval, such as its midpoint, will be an approximation of $x^\\star$ with an absolute error of at most $\\varepsilon$. If we return the midpoint, the error is at most $\\varepsilon/2$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef find_minimum(fp, a, b, eps):\n    \"\"\"\n    Finds the minimizer of a unimodal function f by applying bisection to its derivative fp.\n\n    Args:\n        fp (callable): The derivative of the function f.\n        a (float): The left endpoint of the initial interval.\n        b (float): The right endpoint of the initial interval.\n        eps (float): The tolerance for the final interval length.\n\n    Returns:\n        float: The approximated minimizer x*, or np.nan if preconditions are violated.\n    \"\"\"\n    # Evaluate the derivative at the endpoints\n    fp_a = fp(a)\n    fp_b = fp(b)\n\n    # Case 1  2: Minimum is at an endpoint if the derivative is zero there.\n    if np.isclose(fp_a, 0):\n        return a\n    if np.isclose(fp_b, 0):\n        return b\n\n    # Case 6: Precondition violation (not unimodal with a minimum)\n    if fp_a  0 and fp_b  0:\n        return np.nan\n\n    # Case 3: Function is non-decreasing on [a,b], min is at a.\n    if fp_a = 0 and fp_b = 0:\n        return a\n\n    # Case 4: Function is non-increasing on [a,b], min is at b.\n    if fp_a = 0 and fp_b = 0:\n        return b\n        \n    # Case 5: Root is bracketed, perform bisection.\n    # This is the only remaining case: fp_a  0 and fp_b  0.\n    while (b - a)  eps:\n        m = a + (b - a) / 2.0\n        fp_m = fp(m)\n\n        if np.isclose(fp_m, 0):\n            return m\n        elif fp_m  0:\n            # The root is in the right half of the interval\n            a = m\n        else: # fp_m  0\n            # The root is in the left half of the interval\n            b = m\n    \n    # Return the midpoint of the final interval\n    return a + (b - a) / 2.0\n\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'fp': lambda x: 2 * (x - 2), 'a': 0.0, 'b': 5.0, 'eps': 1e-8},\n        {'fp': lambda x: np.exp(x) + 2 * (x - 3), 'a': -5.0, 'b': 5.0, 'eps': 1e-8},\n        {'fp': lambda x: 2 * x, 'a': 0.0, 'b': 3.0, 'eps': 1e-12},\n        {'fp': lambda x: np.exp(x), 'a': -3.0, 'b': 1.0, 'eps': 1e-8},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = find_minimum(case['fp'], case['a'], case['b'], case['eps'])\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3210918"}]}