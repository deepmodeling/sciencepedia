{"hands_on_practices": [{"introduction": "Root-finding algorithms are not just for solving pre-defined equations; they are powerful tools for tackling problems from other domains. This first practice challenges you to reframe a geometric optimization problem—finding the closest point on a parabola to a given point—as a root-finding task for the derivative of a distance function. By implementing both Newton's method and the secant method, you will gain hands-on experience with the foundational open methods in a tangible, applied context.", "problem": "You are to write a complete, runnable program that computes the closest point on the parabola $y = x^2$ to a fixed point $P = (2, -1/2)$ by posing the task as a one-dimensional root-finding problem and solving it with open methods. The fundamental base for the derivation is the Euclidean distance in the plane and the calculus fact that for a differentiable function, a local minimum occurs at a point where the derivative is zero, provided appropriate second-order conditions hold.\n\nConsider the squared Euclidean distance from a point on the curve to the fixed point. If a point on the curve has coordinates $(x, x^2)$, the squared distance to $P$ is $D(x) = (x - 2)^2 + (x^2 + 1/2)^2$. To find the closest point, minimize $D(x)$ by characterizing stationary points using the first derivative test. The minimizer must satisfy $\\dfrac{d}{dx}D(x) = 0$. This condition defines a scalar nonlinear equation in $x$ whose unique solution corresponds to the $x$-coordinate of the closest point on the parabola to $P$. Because we want to use open methods, you must solve this equation using both Newton's method and the secant method. You must use the analytical derivative for Newton's method and purely function evaluations for the secant method.\n\nYour program must implement the following components:\n- A function $f(x)$ equal to the first derivative $\\dfrac{d}{dx}D(x)$.\n- A function $f'(x)$ equal to the derivative of $f(x)$, used by Newton's method.\n- An implementation of Newton's method that iterates $x_{k+1} = x_k - \\dfrac{f(x_k)}{f'(x_k)}$ until convergence.\n- An implementation of the secant method that iterates $x_{k+1} = x_k - f(x_k)\\dfrac{x_k - x_{k-1}}{f(x_k) - f(x_{k-1})}$ until convergence.\n- A stopping criterion that terminates when either $|x_{k+1} - x_k| < \\varepsilon$ or $|f(x_{k+1})| < \\varepsilon$, or when the iteration count reaches a specified maximum without convergence.\n\nFor each converged root $x^\\star$, report the closest point $(x^\\star, (x^\\star)^2)$ and the Euclidean distance to $P$, namely $\\sqrt{(x^\\star - 2)^2 + ((x^\\star)^2 + 1/2)^2}$. No physical units apply here; all quantities are dimensionless.\n\nUse the following test suite of parameter values to exercise your implementation. Each test case specifies the method and its initial guess parameters, along with the absolute tolerance and maximum iteration count:\n- Test $1$: Newton's method with initial guess $x_0 = 0$, tolerance $\\varepsilon = 10^{-12}$, maximum iterations $50$.\n- Test $2$: Newton's method with initial guess $x_0 = 2$, tolerance $\\varepsilon = 10^{-12}$, maximum iterations $50$.\n- Test $3$: Secant method with initial guesses $x_0 = 0$, $x_1 = 1$, tolerance $\\varepsilon = 10^{-12}$, maximum iterations $100$.\n- Test $4$: Secant method with initial guesses $x_0 = -1$, $x_1 = 1$, tolerance $\\varepsilon = 10^{-12}$, maximum iterations $100$.\n- Test $5$: Newton's method with initial guess $x_0 = -5$, tolerance $\\varepsilon = 10^{-12}$, maximum iterations $50$.\n\nFinal output format requirement:\n- Your program should produce a single line of output containing a list of results for all test cases in order. Each result must be the list $[x^\\star, (x^\\star)^2, \\text{dist}]$, where $x^\\star$ is the approximated minimizer, $(x^\\star)^2$ is the corresponding $y$-coordinate, and $\\text{dist}$ is the Euclidean distance to $P$. Each number must be rounded to exactly $12$ decimal places.\n- The final output must be a single line, formatted as a comma-separated list of the five triples enclosed in square brackets, for example, $[[\\dots],[\\dots],[\\dots],[\\dots],[\\dots]]$.", "solution": "### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Curve**: Parabola $y = x^2$.\n- **Fixed Point**: $P = (2, -1/2)$.\n- **Objective**: Find the point $(x, x^2)$ on the parabola closest to $P$.\n- **Methodology**:\n    1. Define the squared Euclidean distance $D(x) = (x - 2)^2 + (x^2 + 1/2)^2$.\n    2. Formulate a root-finding problem for the equation $\\frac{d}{dx}D(x) = 0$.\n    3. Solve this equation using Newton's method and the secant method.\n- **Root-finding Function**: $f(x) = \\frac{d}{dx}D(x)$.\n- **Derivative for Newton's Method**: $f'(x) = \\frac{d}{dx}f(x)$.\n- **Newton's Method Formula**: $x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}$.\n- **Secant Method Formula**: $x_{k+1} = x_k - f(x_k)\\frac{x_k - x_{k-1}}{f(x_k) - f(x_{k-1})}$.\n- **Stopping Criterion**: $|x_{k+1} - x_k| < \\varepsilon$ or $|f(x_{k+1})| < \\varepsilon$.\n- **Test Cases**:\n    - **Test 1**: Newton's method, $x_0 = 0$, $\\varepsilon = 10^{-12}$, max iterations $= 50$.\n    - **Test 2**: Newton's method, $x_0 = 2$, $\\varepsilon = 10^{-12}$, max iterations $= 50$.\n    - **Test 3**: Secant method, $x_0 = 0$, $x_1 = 1$, $\\varepsilon = 10^{-12}$, max iterations $= 100$.\n    - **Test 4**: Secant method, $x_0 = -1$, $x_1 = 1$, $\\varepsilon = 10^{-12}$, max iterations $= 100$.\n    - **Test 5**: Newton's method, $x_0 = -5$, $\\varepsilon = 10^{-12}$, max iterations $= 50$.\n- **Output Requirement**: For each converged root $x^\\star$, report $[x^\\star, (x^\\star)^2, \\text{dist}]$ where $\\text{dist} = \\sqrt{(x^\\star - 2)^2 + ((x^\\star)^2 + 1/2)^2}$. Each number is rounded to $12$ decimal places. The final output is a single-line list of these results.\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientifically Grounded & Well-Posed**: The problem is a standard application of calculus and numerical methods. To find the minimum distance, we can minimize the squared distance $D(x)$ to simplify differentiation. A minimum of a differentiable function occurs at a critical point, where the first derivative is zero.\n    Let's derive the function $f(x)$ and its derivative $f'(x)$.\n    The squared distance is $D(x) = (x - 2)^2 + (x^2 + \\frac{1}{2})^2$.\n    The derivative of $D(x)$ with respect to $x$ is:\n    $$ f(x) = \\frac{d}{dx}D(x) = 2(x - 2) \\cdot 1 + 2(x^2 + \\frac{1}{2}) \\cdot (2x) $$\n    $$ f(x) = (2x - 4) + (4x^3 + 2x) = 4x^3 + 4x - 4 $$\n    The root-finding problem is to solve $f(x) = 0$, which is equivalent to solving $x^3 + x - 1 = 0$.\n    For Newton's method, the derivative of $f(x)$ is required:\n    $$ f'(x) = \\frac{d}{dx}(4x^3 + 4x - 4) = 12x^2 + 4 $$\n    To confirm that the root of $f(x)=0$ corresponds to a minimum, we check the second derivative of $D(x)$, which is $f'(x)$.\n    $$ D''(x) = f'(x) = 12x^2 + 4 $$\n    Since $x^2 \\ge 0$ for any real $x$, the second derivative $D''(x)$ is always positive ($D''(x) \\ge 4$). This proves that $D(x)$ is a strictly convex function. Therefore, it has a unique global minimum, and the single real root of $f(x)=0$ corresponds to this minimum. The problem is well-posed.\n\n2.  **Objective**: The problem is stated using precise mathematical language, free from subjectivity.\n\n3.  **Incomplete or Contradictory Setup**: The problem is self-contained. All necessary functions, parameters, initial conditions, and stopping criteria are provided. There are no contradictions.\n\n4.  **Unrealistic or Infeasible**: The setup is a standard textbook problem in optimization and numerical analysis. It is entirely feasible and realistic within a mathematical context.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. It is scientifically grounded, well-posed, objective, and complete.\n\n### Solution Design\n\nThe core of the problem is to find the root of the nonlinear scalar equation $f(x) = 4x^3 + 4x - 4 = 0$. This will be accomplished using two open root-finding methods: Newton's method and the secant method.\n\n**1. Function Definitions**\nThe primary function and its derivative are defined as derived above:\n- $f(x) = 4x^3 + 4x - 4$\n- $f'(x) = 12x^2 + 4$\n\n**2. Newton's Method Implementation**\nNewton's method finds a root by iterating the formula:\n$$ x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)} $$\nstarting from an initial guess $x_0$. The implementation will be a function that takes $f$, $f'$, the initial guess $x_0$, a tolerance $\\varepsilon$, and a maximum number of iterations as input. The iteration will stop if either the step size $|x_{k+1} - x_k|$ or the residual $|f(x_{k+1})|$ falls below the tolerance $\\varepsilon$. Given that $f'(x) = 12x^2+4 \\ge 4$, there is no risk of division by zero.\n\n**3. Secant Method Implementation**\nThe secant method is similar to Newton's method but approximates the derivative using a finite difference based on the two previous iterates:\n$$ f'(x_k) \\approx \\frac{f(x_k) - f(x_{k-1})}{x_k - x_{k-1}} $$\nThis leads to the iteration formula:\n$$ x_{k+1} = x_k - f(x_k)\\frac{x_k - x_{k-1}}{f(x_k) - f(x_{k-1})} $$\nThis method requires two initial guesses, $x_0$ and $x_1$. The implementation will be a function that accepts $f$, the initial guesses $x_0$ and $x_1$, the tolerance $\\varepsilon$, and a maximum iteration count. The stopping criteria are identical to those for Newton's method. Since $f(x)$ is strictly monotonic, $f(x_k) - f(x_{k-1})$ will not be zero unless $x_k = x_{k-1}$, so division by zero is not a concern for distinct iterates.\n\n**4. Processing Test Cases**\nA loop will iterate through the five specified test cases. For each case, the appropriate solver (Newton or secant) will be called with the given parameters to find the root $x^\\star$.\n\n**5. Final Computations and Formatting**\nOnce the root $x^\\star$ is found for a given test case, the following quantities are calculated:\n- The corresponding $y$-coordinate on the parabola: $y^\\star = (x^\\star)^2$.\n- The Euclidean distance to the point $P(2, -1/2)$: $\\text{dist} = \\sqrt{(x^\\star - 2)^2 + (y^\\star + \\frac{1}{2})^2}$.\n\nThese three values, $[x^\\star, y^\\star, \\text{dist}]$, will be formatted as floating-point numbers with $12$ decimal places and collected for all test cases. The final output will be a single string representing a list of these result lists.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the closest point on the parabola y = x^2 to a fixed point P\n    by solving a root-finding problem with Newton's and secant methods.\n    \"\"\"\n\n    # The function f(x) is the derivative of the squared distance function D(x).\n    # D(x) = (x - 2)^2 + (x^2 + 1/2)^2\n    # f(x) = D'(x) = 2(x - 2) + 2(x^2 + 1/2)(2x) = 4x^3 + 4x - 4\n    def f(x):\n        return 4 * x**3 + 4 * x - 4\n\n    # The derivative of f(x), required for Newton's method.\n    # f'(x) = 12x^2 + 4\n    def f_prime(x):\n        return 12 * x**2 + 4\n\n    def newton_method(x0, tol, max_iter):\n        \"\"\"\n        Implementation of Newton's method for root finding.\n        x_{k+1} = x_k - f(x_k) / f'(x_k)\n        \"\"\"\n        xk = float(x0)\n        for _ in range(max_iter):\n            fxk = f(xk)\n            f_prime_xk = f_prime(xk)\n            \n            # Avoid division by zero, although not possible for this specific f_prime(x)\n            if abs(f_prime_xk) < 1e-15:\n                # This would indicate failure, but our f' is always >= 4.\n                return None  \n            \n            xk_plus_1 = xk - fxk / f_prime_xk\n            \n            # Stopping criteria: step size or residual is below tolerance\n            if abs(xk_plus_1 - xk) < tol or abs(f(xk_plus_1)) < tol:\n                return xk_plus_1\n            \n            xk = xk_plus_1\n        \n        # Return the last computed value if max_iter is reached\n        return xk\n\n    def secant_method(x0, x1, tol, max_iter):\n        \"\"\"\n        Implementation of the secant method for root finding.\n        x_{k+1} = x_k - f(x_k) * (x_k - x_{k-1}) / (f(x_k) - f(x_{k-1}))\n        \"\"\"\n        xk_minus_1 = float(x0)\n        xk = float(x1)\n        fxk_minus_1 = f(xk_minus_1)\n\n        for _ in range(max_iter):\n            fxk = f(xk)\n            \n            denominator = fxk - fxk_minus_1\n            # Avoid division by zero. f(x) is monotonic, so this only happens\n            # if iterates are identical or due to precision loss.\n            if abs(denominator) < 1e-15:\n                return xk\n\n            xk_plus_1 = xk - fxk * (xk - xk_minus_1) / denominator\n\n            # Stopping criteria: step size or residual is below tolerance\n            if abs(xk_plus_1 - xk) < tol or abs(f(xk_plus_1)) < tol:\n                return xk_plus_1\n            \n            xk_minus_1 = xk\n            fxk_minus_1 = fxk\n            xk = xk_plus_1\n            \n        # Return the last computed value if max_iter is reached\n        return xk\n\n    # Test cases as defined in the problem statement.\n    # (method_name, [initial_guesses], tolerance, max_iterations)\n    test_cases = [\n        ('newton', [0], 1e-12, 50),\n        ('newton', [2], 1e-12, 50),\n        ('secant', [0, 1], 1e-12, 100),\n        ('secant', [-1, 1], 1e-12, 100),\n        ('newton', [-5], 1e-12, 50),\n    ]\n\n    all_results = []\n    fixed_point_p = (2.0, -0.5)\n\n    for case in test_cases:\n        method_name, initial_guesses, tol, max_iter = case\n        \n        root = None\n        if method_name == 'newton':\n            root = newton_method(initial_guesses[0], tol, max_iter)\n        elif method_name == 'secant':\n            root = secant_method(initial_guesses[0], initial_guesses[1], tol, max_iter)\n        \n        if root is not None:\n            x_star = root\n            y_star = x_star**2\n            \n            # Calculate Euclidean distance to P\n            distance = np.sqrt((x_star - fixed_point_p[0])**2 + (y_star - fixed_point_p[1])**2)\n            \n            # Format results to 12 decimal places as strings\n            result_str = f\"[{x_star:.12f},{y_star:.12f},{distance:.12f}]\"\n            all_results.append(result_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "3259979"}, {"introduction": "While Newton's method is fast, its convergence is not always guaranteed, a reality you may have observed in the previous exercise. This practice introduces the crucial engineering concept of 'safeguarding' by having you build a hybrid algorithm that combines the speed of Newton's method with the guaranteed convergence of the bisection method. By learning to create a robust solver that intelligently switches strategies, you take a significant step from simply using algorithms to designing practical and reliable numerical tools [@problem_id:3259979].", "problem": "You are to design and implement a program that computes approximations of simple real roots using a safeguarded open method. Your method must combine Newton’s method with the bisection method, switching to bisection whenever the Newton trial step would leave a bracketing interval or when the derivative magnitude is too small to define a stable Newton step. The derivation must begin from the first-order Taylor approximation and proceed to the iterative update rule; the code must reflect this principle. The method must maintain a bracketing invariant at every iteration.\n\nAssume the following foundational facts:\n- If a function $f$ is continuous on an interval $[a,b]$ and $f(a)\\,f(b)\\le 0$, then there exists at least one $x^{\\ast}\\in[a,b]$ with $f(x^{\\ast})=0$.\n- The first-order Taylor approximation of a differentiable function $f$ near a point $x_k$ is $f(x_k+s)\\approx f(x_k)+f'(x_k)\\,s$.\n\nYour tasks:\n- From the first-order Taylor approximation, derive an iteration that attempts to make the next residual vanish and thus produces a Newton trial point. Your method must use the bracketing information as a safeguard: if the Newton trial point leaves $[a_k,b_k]$, or if the derivative has magnitude below a specified safeguarding threshold, then take a bisection step instead.\n- Maintain a valid bracket $[a_k,b_k]$ at each iteration by updating the interval endpoints according to the sign of $f$ evaluated at the accepted next iterate. If $f(a_k)=0$ or $f(b_k)=0$ at any time, the corresponding endpoint is a root and must be returned immediately.\n- Use the following parameter values in your implementation:\n  - Function tolerance $\\,\\varepsilon_f = 10^{-10}\\,$,\n  - Interval half-width tolerance $\\,\\varepsilon_x = 10^{-10}\\,$,\n  - Maximum number of iterations $\\,N_{\\max}=100\\,$,\n  - Derivative safeguarding threshold $\\,\\delta = 10^{-14}\\,$.\n- Use the initial iterate $\\,x_0=(a_0+b_0)/2\\,$.\n- Stopping criteria: stop and return the current approximation $\\,x_k\\,$ as soon as any of the following hold:\n  - $\\,|f(x_k)|\\le \\varepsilon_f\\,$,\n  - $\\,\\tfrac{1}{2}(b_k-a_k)\\le \\varepsilon_x\\,$,\n  - $\\,k\\ge N_{\\max}\\,$, in which case return the current midpoint $\\,\\tfrac{a_k+b_k}{2}\\,$.\n\nImplementation contract:\n- Implement a function that applies the above hybrid rule to compute a single root for a given function $f$, its derivative $f'$, and an initial bracket $[a_0,b_0]$.\n- The Newton trial must be rejected and replaced by a bisection step whenever the trial lies outside $[a_k,b_k]$ or when $\\,|f'(x_k)|<\\delta\\,$.\n\nTest suite:\nCompute the root approximations for the following six cases. Angles for trigonometric functions must be interpreted in radians.\n- Case $1$: $\\,f_1(x)=\\cos(x)-x\\,$ on $[0,1]$.\n- Case $2$: $\\,f_2(x)=x^3-x-2\\,$ on $[1,2]$.\n- Case $3$: $\\,f_3(x)=e^x-3x^2\\,$ on $[0,1]$.\n- Case $4$: $\\,f_4(x)=(x-0.1)^3\\,$ on $[0,1]$.\n- Case $5$: $\\,f_5(x)=x^3-2x+2\\,$ on $[-2,0]$.\n- Case $6$: $\\,f_6(x)=\\sin(x)\\,$ on $[0,4]$.\n\nOutput specification:\n- Your program must produce a single line of output containing the six computed roots in the order of the cases above, formatted as a comma-separated list of floating-point numbers enclosed in square brackets, with no spaces.\n- The output type for each test case is a floating-point number. No physical units apply. Angles are in radians.", "solution": "The problem requires the design and implementation of a safeguarded root-finding algorithm. This algorithm combines the rapid convergence of Newton's method with the guaranteed convergence of the bisection method. The bisection method acts as a safeguard, ensuring that the iterate remains within a valid bracket containing the root and that the algorithm remains stable even when the derivative is near zero.\n\n### Principle and Derivation\n\nThe fundamental goal is to find a root $x^{\\ast}$ for a function $f(x)$, which is a value such that $f(x^{\\ast}) = 0$. The method assumes we are given an initial interval $[a_0, b_0]$ where the function is continuous and for which $f(a_0)f(b_0) \\le 0$. The Intermediate Value Theorem guarantees that at least one root exists within this interval.\n\n**1. Newton's Method Trial Step**\n\nNewton's method is an open method that approximates the function with its tangent line at the current iterate. The next iterate is chosen as the root of this tangent line. This is derived from the first-order Taylor series expansion of $f(x)$ around a point $x_k$:\n$$f(x) \\approx f(x_k) + f'(x_k)(x - x_k)$$\nWe seek the next iterate, which we will call $x_{k+1}$, such that $f(x_{k+1}) = 0$. Substituting $x = x_{k+1}$ and setting the approximation to zero gives:\n$$0 \\approx f(x_k) + f'(x_k)(x_{k+1} - x_k)$$\nAssuming the derivative $f'(x_k)$ is non-zero, we can solve for $x_{k+1}$:\n$$f'(x_k)(x_{k+1} - x_k) = -f(x_k)$$\n$$x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}$$\nThis is the iterative formula for Newton's method. It generally exhibits quadratic convergence when the initial guess is sufficiently close to a simple root. However, it can diverge if the initial guess is poor, or converge slowly or fail if the derivative $f'(x_k)$ is close to or equal to zero.\n\n**2. Bisection Method Safeguard**\n\nThe bisection method is a bracketing method that is slower (linear convergence) but guaranteed to converge. Given a bracket $[a_k, b_k]$ such that $f(a_k)f(b_k) \\le 0$, the method proposes the midpoint as the next test point:\n$$x_{k+1} = \\frac{a_k + b_k}{2}$$\nThe bracket is then updated by replacing either $a_k$ or $b_k$ with $x_{k+1}$, ensuring the new, smaller bracket $[a_{k+1}, b_{k+1}]$ still contains a root. This process systematically halves the interval of uncertainty.\n\n**3. The Hybrid Algorithm**\n\nThe specified algorithm leverages the strengths of both methods. At each iteration $k$, starting with an iterate $x_k$ and a bracket $[a_k, b_k]$:\n\n1.  A Newton step is proposed: $x_{N} = x_k - f(x_k)/f'(x_k)$.\n2.  This step is checked against two safeguarding conditions:\n    a.  **Stability Condition:** The magnitude of the derivative must be sufficiently large, $|f'(x_k)| \\ge \\delta$, where $\\delta$ is a small positive threshold (given as $10^{-14}$). A derivative close to zero can cause the Newton step to be erratically large and numerically unstable.\n    b.  **Bracketing Condition:** The Newton trial point $x_{N}$ must lie strictly within the current bracket, i.e., $x_{N} \\in (a_k, b_k)$. This prevents the iterate from leaving the region where a root is known to exist.\n3.  If either safeguard is violated, the algorithm rejects the Newton step and falls back to the robust bisection step: $x_{k+1} = (a_k + b_k)/2$.\n4.  If the Newton step is accepted, the next iterate is $x_{k+1} = x_{N}$.\n5.  After determining the next iterate $x_{k+1}$, the bracketing interval is updated. Let $f_a = f(a_k)$ and $f_{new} = f(x_{k+1})$. If $f_a \\cdot f_{new} < 0$, the root is in $[a_k, x_{k+1}]$, so the new bracket becomes $[a_{k+1}, b_{k+1}] = [a_k, x_{k+1}]$. Otherwise, the root must be in $[x_{k+1}, b_k]$, and the new bracket becomes $[a_{k+1}, b_{k+1}] = [x_{k+1}, b_k]$. This ensures the bracketing invariant $f(a_{k+1})f(b_{k+1}) \\le 0$ is maintained.\n\nThe process is initiated with $x_0 = (a_0 + b_0)/2$ and continues until one of the following stopping criteria is met:\n-   The function value is sufficiently close to zero: $|f(x_k)| \\le \\varepsilon_f$.\n-   The interval of uncertainty is sufficiently small: $\\frac{1}{2}(b_k - a_k) \\le \\varepsilon_x$.\n-   A maximum number of iterations, $N_{\\max}$, is reached.\n\n### Algorithmic Steps\n\nThe procedure can be formally stated as follows:\n\n1.  **Initialization**:\n    -   Given a function $f$, its derivative $f'$, an initial interval $[a, b]$, and parameters $\\varepsilon_f=10^{-10}$, $\\varepsilon_x=10^{-10}$, $\\delta=10^{-14}$, $N_{\\max}=100$.\n    -   Set $a_k \\leftarrow a$, $b_k \\leftarrow b$. Compute $f(a_k)$ and $f(b_k)$.\n    -   Check for initial conditions: If $f(a_k) \\cdot f(b_k) > 0$, the initial interval is invalid. If $f(a_k)=0$ or $f(b_k)=0$, return the corresponding endpoint.\n    -   Set the initial iterate $x_k \\leftarrow (a_k + b_k)/2$.\n\n2.  **Iteration**: For $k = 0, 1, 2, \\ldots, N_{\\max}-1$:\n    a.  **Check stopping criteria**: Evaluate $f(x_k)$. If $|f(x_k)| \\le \\varepsilon_f$ or if $(b_k - a_k)/2 \\le \\varepsilon_x$, terminate and return the current iterate $x_k$.\n    b.  **Propose next iterate**:\n        i.  Evaluate $f'(x_k)$.\n        ii. If $|f'(x_k)| < \\delta$, set the next iterate $x_{next} \\leftarrow (a_k + b_k)/2$.\n        iii. Else, calculate the Newton iterate $x_{N} \\leftarrow x_k - f(x_k)/f'(x_k)$. If $a_k < x_{N} < b_k$, set $x_{next} \\leftarrow x_{N}$. Otherwise, set $x_{next} \\leftarrow (a_k + b_k)/2$.\n    c.  **Update bracket and iterate**:\n        i.  Evaluate $f(x_{next})$. If $f(x_{next})=0$, return $x_{next}$.\n        ii. If $f(a_k) \\cdot f(x_{next}) < 0$, set $b_k \\leftarrow x_{next}$. Else, set $a_k \\leftarrow x_{next}$ (and update corresponding cached function values).\n        iii. Set $x_k \\leftarrow x_{next}$ for the subsequent iteration.\n\n3.  **Termination**: If the loop completes without meeting the stopping criteria, return the midpoint of the final bracket, $(a_k + b_k)/2$, as the best available approximation.\n\nThis structured, safeguarded approach ensures both efficiency and robustness, making it a reliable method for finding real roots of differentiable functions.", "answer": "```python\nimport numpy as np\n\ndef find_root_safeguarded(f, fp, a, b, eps_f, eps_x, delta, n_max):\n    \"\"\"\n    Computes a root of f(x)=0 within the interval [a, b] using a safeguarded\n    Newton's method.\n\n    The method combines Newton's method with bisection as a fallback.\n    \"\"\"\n    ak, bk = float(a), float(b)\n    fak, fbk = f(ak), f(bk)\n\n    # Initial validation and endpoint checks\n    if fak * fbk > 0.0:\n        # According to the problem statement, all initial brackets are valid.\n        # This is for robustness in a general context.\n        raise ValueError(\"Root not bracketed or multiple roots in initial interval.\")\n    \n    if fak == 0.0:\n        return ak\n    if fbk == 0.0:\n        return bk\n\n    # Per problem statement: \"Use the initial iterate x_0=(a_0+b_0)/2\"\n    xk = (ak + bk) / 2.0\n    \n    for _ in range(n_max):\n        # 1. Check stopping criteria at the beginning of the iteration\n        fxk = f(xk)\n        if abs(fxk) <= eps_f:\n            return xk\n        \n        # The problem asks to return xk if interval is small, which is the\n        # point from the previous iteration.\n        if (bk - ak) / 2.0 <= eps_x:\n            return xk\n\n        # 2. Propose the next iterate, x_next\n        fpxk = fp(xk)\n        \n        # Default to bisection step (safeguard)\n        x_bisection = (ak + bk) / 2.0\n        \n        # Decide whether to use the Newton step\n        if abs(fpxk) >= delta:\n            x_newton = xk - fxk / fpxk\n            # Check if Newton step is safely within the bracket\n            if ak < x_newton < bk:\n                x_next = x_newton\n            else:\n                x_next = x_bisection # Fallback to bisection\n        else:\n            x_next = x_bisection # Fallback to bisection\n            \n        # 3. Update the bracket [ak, bk] using the new point x_next\n        fx_next = f(x_next)\n        \n        # Check for an exact root hit\n        if fx_next == 0.0:\n            return x_next\n\n        # Update the bracket while maintaining the invariant f(a)*f(b) < 0\n        if fak * fx_next < 0.0:\n            bk, fbk = x_next, fx_next\n        else:\n            ak, fak = x_next, fx_next\n        \n        # 4. The new point becomes the iterate for the next loop\n        xk = x_next\n\n    # 5. If max iterations reached, return the midpoint of the final bracket\n    return (ak + bk) / 2.0\n\ndef solve():\n    \"\"\"\n    Solves the root-finding problem for the specified test suite.\n    \"\"\"\n    # Define parameters from the problem statement\n    params = {\n        'eps_f': 1e-10,\n        'eps_x': 1e-10,\n        'n_max': 100,\n        'delta': 1e-14\n    }\n\n    # Define the test cases\n    test_cases = [\n        # Case 1: f(x) = cos(x) - x on [0, 1]\n        (lambda x: np.cos(x) - x, lambda x: -np.sin(x) - 1, 0.0, 1.0),\n        # Case 2: f(x) = x^3 - x - 2 on [1, 2]\n        (lambda x: x**3 - x - 2, lambda x: 3*x**2 - 1, 1.0, 2.0),\n        # Case 3: f(x) = e^x - 3x^2 on [0, 1]\n        (lambda x: np.exp(x) - 3*x**2, lambda x: np.exp(x) - 6*x, 0.0, 1.0),\n        # Case 4: f(x) = (x - 0.1)^3 on [0, 1]\n        (lambda x: (x - 0.1)**3, lambda x: 3*(x - 0.1)**2, 0.0, 1.0),\n        # Case 5: f(x) = x^3 - 2x + 2 on [-2, 0]\n        (lambda x: x**3 - 2*x + 2, lambda x: 3*x**2 - 2, -2.0, 0.0),\n        # Case 6: f(x) = sin(x) on [0, 4]\n        (lambda x: np.sin(x), lambda x: np.cos(x), 0.0, 4.0),\n    ]\n\n    results = []\n    for f, fp, a, b in test_cases:\n        root = find_root_safeguarded(f, fp, a, b, **params)\n        results.append(root)\n\n    # Print the results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3260144"}, {"introduction": "Our exploration of root finding can be expanded in two key directions: using higher-order approximations and moving into the complex plane. This practice introduces Müller's method, which approximates a function with a parabola rather than a line, and applies it to find the complex roots of the polynomial $f(z) = z^4 + 1$. Completing this exercise will demonstrate how open methods can be generalized to tackle a broader and more intricate class of problems that are fundamental in fields like physics and engineering [@problem_id:3260144].", "problem": "Consider the complex-valued function $f(z) = z^4 + 1$ defined on the field of complex numbers. A root is any complex number $z$ such that $f(z) = 0$. Your task is to write a complete, runnable program that, for each provided set of three distinct initial complex guesses $(z_0, z_1, z_2)$, uses an open root-finding approach that operates without bracketing and allows complex-valued iterates to converge to a single root of $f(z) = 0$. For numerical termination, use an absolute step-size tolerance of $10^{-12}$ on the update in $z$ and an absolute function-value tolerance of $10^{-12}$ on $|f(z)|$, with a maximum of $100$ iterations per case. All computations are dimensionless. Angles, if any arise internally, must be treated in radians.\n\nTest suite of initial guesses (each case is an ordered triple $(z_0, z_1, z_2)$ of complex numbers):\n- Case $1$: $(0.4 + 0.6 i,\\; 0.9 + 1.1 i,\\; 0.7 + 0.5 i)$\n- Case $2$: $(-1.2 + 1.0 i,\\; -0.6 + 0.8 i,\\; -0.9 + 0.7 i)$\n- Case $3$: $(-0.9 - 1.1 i,\\; -0.6 - 0.5 i,\\; -0.8 - 0.7 i)$\n- Case $4$: $(1.0 - 1.0 i,\\; 0.6 - 0.8 i,\\; 0.9 - 0.6 i)$\n- Case $5$ (edge case near the real axis): $(0.1 + 10^{-12} i,\\; 0.5 + 10^{-12} i,\\; 0.9 + 10^{-12} i)$\n\nFor each case, your program must compute one root to which the method converges, and report it as its real and imaginary parts. The required final output is a single line containing a list of length $5$, where each element is a two-element list $[x,y]$ with $x$ the real part and $y$ the imaginary part of the computed root. Round each $x$ and $y$ to $10$ decimal places. The final line must therefore look like\n$[[x_1,y_1],[x_2,y_2],[x_3,y_3],[x_4,y_4],[x_5,y_5]]$\nwith no extra spaces and no additional text. All numbers must be reported as plain real-valued floats rounded to $10$ decimal places.", "solution": "The problem requires an open root-finding method using three initial points to find a complex root of $f(z) = z^4+1$. This specification points to **Muller's method**, a powerful algorithm that generalizes the secant method and is well-suited for finding complex roots.\n\n### Principle of Muller's Method\n\nWhile the secant method approximates a function with a line (a first-degree polynomial) passing through two points, Muller's method fits a parabola (a second-degree polynomial) through three points to find the next approximation of the root. This quadratic approximation generally leads to a faster convergence rate (order $\\approx 1.84$) than the secant method.\n\n### Derivation and Algorithm\n\nLet the three most recent estimates of the root be $z_{k-2}$, $z_{k-1}$, and $z_k$. Muller's method constructs a unique parabola that passes through the points $(z_{k-2}, f_{k-2})$, $(z_{k-1}, f_{k-1})$, and $(z_k, f_k)$, where $f_i = f(z_i)$. The next approximation, $z_{k+1}$, is then chosen as the root of this parabola that is closest to $z_k$.\n\nIt is convenient to write the parabola's equation centered at the latest point $z_k$:\n$$P(z) = a(z - z_k)^2 + b(z - z_k) + c$$\nThe coefficients can be found by solving a system of equations, but a more robust approach uses divided differences:\n-   $f_k = f(z_k)$\n-   $f[z_{k-1}, z_k] = \\frac{f_k - f_{k-1}}{z_k - z_{k-1}}$\n-   $f[z_{k-2}, z_{k-1}] = \\frac{f_{k-1} - f_{k-2}}{z_{k-1} - z_{k-2}}$\n-   $f[z_{k-2}, z_{k-1}, z_k] = \\frac{f[z_{k-1}, z_k] - f[z_{k-2}, z_{k-1}]}{z_k - z_{k-2}}$\n\nThe coefficients of the parabola are then:\n-   $c = f_k$\n-   $b = f[z_{k-1}, z_k] + (z_k - z_{k-1}) f[z_{k-2}, z_{k-1}, z_k]$\n-   $a = f[z_{k-2}, z_{k-1}, z_k]$\n\nThe next iterate $z_{k+1}$ is a root of $P(z)=0$. The change $\\Delta z = z_{k+1} - z_k$ is found by solving the quadratic equation $a(\\Delta z)^2 + b(\\Delta z) + c = 0$. To avoid loss of precision from subtraction of nearly equal numbers, which can occur with the standard quadratic formula, a stabilized formula is used:\n$$ \\Delta z = \\frac{-2c}{b \\pm \\sqrt{b^2 - 4ac}} $$\nThe sign in the denominator is chosen to maximize its magnitude. For complex numbers, this means we select the sign such that $|b \\pm \\sqrt{b^2 - 4ac}|$ is maximized. This choice provides the smallest update $|\\Delta z|$, guiding the iteration towards the root closest to $z_k$.\n\nThe iterative procedure is as follows:\n1.  Initialize with three distinct guesses $z_0, z_1, z_2$.\n2.  For each iteration, calculate the coefficients $a, b, c$ using the current three points.\n3.  Calculate the update $\\Delta z$ using the stabilized formula.\n4.  The new approximation is $z_{new} = z_2 + \\Delta z$.\n5.  Check for convergence using the criteria: $|\\Delta z|  10^{-12}$ and $|f(z_{new})|  10^{-12}$. If both are met, the iteration terminates.\n6.  If not converged, update the points for the next iteration: $z_0 \\gets z_1$, $z_1 \\gets z_2$, $z_2 \\gets z_{new}$.\n7.  Repeat from step 2, up to a maximum of 100 iterations.", "answer": "```python\n# The complete and runnable Python 3 code for solving the root-finding problem.\n# This program uses Muller's method to find roots of f(z) = z^4 + 1.\n\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    It defines the function, test cases, and calls the root-finding algorithm,\n    then formats and prints the final result.\n    \"\"\"\n    \n    def f(z: complex) - complex:\n        \"\"\"\n        The complex-valued function f(z) = z^4 + 1.\n        \"\"\"\n        return z**4 + 1\n\n    def muller(f_func, z0, z1, z2, tol=1e-12, max_iter=100):\n        \"\"\"\n        Implements Muller's method for finding a complex root.\n\n        Args:\n            f_func: The function for which to find a root.\n            z0, z1, z2: Three initial distinct complex guesses.\n            tol: The tolerance for termination.\n            max_iter: The maximum number of iterations.\n\n        Returns:\n            The complex root found, or the last estimate if max_iter is reached.\n        \"\"\"\n        p = [z0, z1, z2]\n        \n        for _ in range(max_iter):\n            p0, p1, p2 = p[0], p[1], p[2]\n            f0, f1, f2 = f_func(p0), f_func(p1), f_func(p2)\n\n            # To avoid division by zero in degenerate cases.\n            eps = np.finfo(complex).eps\n            if abs(p1 - p0)  eps or abs(p2 - p1)  eps or abs(p2 - p0)  eps:\n                return p2\n\n            # Coefficients of the interpolating parabola using divided differences.\n            # Polynomial P(z) = a(z - p2)^2 + b(z - p2) + c\n            d1 = (f1 - f0) / (p1 - p0)\n            d2 = (f2 - f1) / (p2 - p1)\n            \n            # Coefficient 'a'\n            a = (d2 - d1) / (p2 - p0)\n            \n            # Coefficient 'b'\n            b = d2 + (p2 - p1) * a\n            \n            # Coefficient 'c'\n            c = f2\n            \n            # Calculate the update step using the numerically stable formula.\n            discriminant = np.sqrt(b**2 - 4*a*c)\n            \n            # Choose the denominator with the largest magnitude.\n            den_plus = b + discriminant\n            den_minus = b - discriminant\n            \n            if abs(den_plus)  abs(den_minus):\n                denominator = den_plus\n            else:\n                denominator = den_minus\n            \n            # If denominator is too small, we might be at a root or stagnating.\n            if abs(denominator)  eps:\n                return p2\n\n            dz = -2 * c / denominator\n            p_new = p2 + dz\n            \n            # Termination criteria: step-size AND function-value tolerances met.\n            if abs(dz)  tol and abs(f_func(p_new))  tol:\n                return p_new\n\n            # Update points for the next iteration.\n            p = [p1, p2, p_new]\n            \n        return p[2]\n\n    # Test suite of initial guesses as specified in the problem statement.\n    test_cases = [\n        (0.4 + 0.6j, 0.9 + 1.1j, 0.7 + 0.5j),\n        (-1.2 + 1.0j, -0.6 + 0.8j, -0.9 + 0.7j),\n        (-0.9 - 1.1j, -0.6 - 0.5j, -0.8 - 0.7j),\n        (1.0 - 1.0j, 0.6 - 0.8j, 0.9 - 0.6j),\n        (0.1 + 1e-12j, 0.5 + 1e-12j, 0.9 + 1e-12j),\n    ]\n\n    results = []\n    for case in test_cases:\n        z0, z1, z2 = case\n        root = muller(f, z0, z1, z2)\n        \n        # Round the real and imaginary parts to 10 decimal places.\n        x = round(root.real, 10)\n        y = round(root.imag, 10)\n        \n        # Ensure -0.0 is represented as 0.0\n        if x == -0.0: x = 0.0\n        if y == -0.0: y = 0.0\n            \n        formatted_root = [x, y]\n        results.append(formatted_root)\n\n    # Convert the list of results to a string and remove spaces for exact output format.\n    final_output_string = str(results).replace(\" \", \"\")\n    print(final_output_string)\n\nsolve()\n```", "id": "2422736"}]}