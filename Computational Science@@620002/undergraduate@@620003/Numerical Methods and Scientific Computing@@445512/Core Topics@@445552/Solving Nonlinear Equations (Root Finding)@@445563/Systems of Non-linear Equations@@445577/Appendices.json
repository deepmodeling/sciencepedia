{"hands_on_practices": [{"introduction": "To truly master Newton's method, we must first build a solid foundation by understanding its core mechanics. Before letting a computer execute thousands of iterations, it is essential to be able to perform a single step by hand. This practice guides you through the fundamental process for a system involving both polynomial and transcendental functions, reinforcing the key skills of calculating the Jacobian, solving the resulting linear system for the correction vector, and updating the approximation [@problem_id:2207865].", "problem": "Consider the following system of two non-linear equations in two variables, $x$ and $y$:\n$$\n\\begin{cases}\nx^2 + 4y^2 = 4 \\\\\ny = \\ln(x)\n\\end{cases}\n$$\nYour task is to find an approximate solution to this system. Starting from the initial guess $(x_0, y_0) = (1.8, 0.6)$, perform a single iteration of the multivariate Newton's method to find the next approximation, $(x_1, y_1)$. Report your answer as a pair of numerical values for $x_1$ and $y_1$, with each value rounded to four significant figures.", "solution": "We solve the system\n$$\n\\begin{cases}\nf_{1}(x,y) = x^{2} + 4y^{2} - 4 = 0, \\\\\nf_{2}(x,y) = y - \\ln(x) = 0,\n\\end{cases}\n$$\nusing one iteration of the multivariate Newton method starting from $(x_{0},y_{0})=(1.8,0.6)$.\n\nNewton’s method for systems uses\n$$\n\\begin{pmatrix} x_{k+1} \\\\[4pt] y_{k+1} \\end{pmatrix}\n=\n\\begin{pmatrix} x_{k} \\\\[4pt] y_{k} \\end{pmatrix}\n+ \\mathbf{s}_{k}, \\quad \\text{where } J(x_{k},y_{k})\\,\\mathbf{s}_{k} = -\\mathbf{f}(x_{k},y_{k}),\n$$\nwith $\\mathbf{f}=(f_{1},f_{2})^{T}$ and the Jacobian\n$$\nJ(x,y) =\n\\begin{pmatrix}\n\\frac{\\partial f_{1}}{\\partial x}  \\frac{\\partial f_{1}}{\\partial y} \\\\\n\\frac{\\partial f_{2}}{\\partial x}  \\frac{\\partial f_{2}}{\\partial y}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2x  8y \\\\\n-\\frac{1}{x}  1\n\\end{pmatrix}.\n$$\n\nAt $(x_{0},y_{0})=(1.8,0.6)$,\n$$\n\\mathbf{f}(x_{0},y_{0}) =\n\\begin{pmatrix}\n1.8^{2} + 4(0.6)^{2} - 4 \\\\\n0.6 - \\ln(1.8)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0.68 \\\\\n0.6 - \\ln(1.8)\n\\end{pmatrix}\n\\approx\n\\begin{pmatrix}\n0.68 \\\\\n0.012213335097881\n\\end{pmatrix},\n$$\nand\n$$\nJ(x_{0},y_{0}) =\n\\begin{pmatrix}\n3.6  4.8 \\\\\n-\\frac{5}{9}  1\n\\end{pmatrix}.\n$$\n\nWe solve $J\\,\\mathbf{s}_{0}=-\\mathbf{f}(x_{0},y_{0})$ for $\\mathbf{s}_{0}=(\\Delta x,\\Delta y)^{T}$:\n$$\n\\begin{cases}\n3.6\\,\\Delta x + 4.8\\,\\Delta y = -0.68, \\\\\n-\\frac{5}{9}\\,\\Delta x + \\Delta y = -0.012213335097881.\n\\end{cases}\n$$\nFrom the second equation, $\\Delta y = -0.012213335097881 + \\frac{5}{9}\\Delta x$. Substituting into the first gives\n$$\n3.6\\,\\Delta x + 4.8\\left(-0.012213335097881 + \\frac{5}{9}\\Delta x\\right) = -0.68,\n$$\n$$\n\\left(3.6 + \\frac{24}{9}\\right)\\Delta x - 4.8\\cdot 0.012213335097881 = -0.68,\n$$\n$$\n\\frac{94}{15}\\,\\Delta x - 0.0586240084698288 = -0.68,\n$$\nhence\n$$\n\\Delta x = \\frac{-0.68 + 0.0586240084698288}{94/15} \\approx -0.099155745.\n$$\nThen\n$$\n\\Delta y = -0.012213335097881 + \\frac{5}{9}\\,\\Delta x \\approx -0.012213335097881 + \\frac{5}{9}(-0.099155745) \\approx -0.067299860.\n$$\n\nUpdate:\n$$\nx_{1} = x_{0} + \\Delta x \\approx 1.8 - 0.099155745 = 1.700844255,\n$$\n$$\ny_{1} = y_{0} + \\Delta y \\approx 0.6 - 0.067299860 = 0.532700140.\n$$\nRounding each to four significant figures gives $x_{1} \\approx 1.701$ and $y_{1} \\approx 0.5327$.", "answer": "$$\\boxed{\\begin{pmatrix} 1.701  0.5327 \\end{pmatrix}}$$", "id": "2207865"}, {"introduction": "A powerful algorithm is only as good as our understanding of its limitations. This exercise shifts our focus from \"how\" to \"why,\" exploring the conditions under which Newton's method can fail or perform poorly. By analyzing the behavior of the Jacobian matrix near a singularity, you will develop the critical insight needed to diagnose and avoid common pitfalls, a crucial skill for any practitioner of numerical methods [@problem_id:2207871].", "problem": "An engineer is tasked with finding a numerical solution to a system of non-linear equations modeling a steady-state physical system. The equations are given by:\n$$f_1(x, y) = x^2 - y^2 - 4 = 0$$\n$$f_2(x, y) = xy - 3 = 0$$\nThe engineer decides to use Newton's method for systems. The iterative formula for this method is given by $\\mathbf{x}_{k+1} = \\mathbf{x}_k - [J(\\mathbf{x}_k)]^{-1} \\mathbf{F}(\\mathbf{x}_k)$, where $\\mathbf{x} = (x, y)^T$, $\\mathbf{F} = (f_1, f_2)^T$, and $J(\\mathbf{x})$ is the Jacobian matrix of $\\mathbf{F}$.\n\nAfter some trials, the engineer observes that choosing an initial guess $\\mathbf{x}_0 = (x_0, y_0)$ that lies on either the x-axis (where $y_0 = 0$) or the y-axis (where $x_0 = 0$) is a poor strategy for this specific system. Which of the following statements provides the most accurate and fundamental reason for this poor performance?\n\nA. For any initial guess on an axis, the first iteration of Newton's method produces a point that is also on an axis, preventing the algorithm from ever moving towards a solution, as the solutions do not lie on an axis.\n\nB. For any initial guess on an axis, the function vector $\\mathbf{F}(\\mathbf{x}_0)$ is a zero vector, which incorrectly signals to the algorithm that a solution has been found.\n\nC. The Jacobian matrix is singular for any point on the x-axis or y-axis, causing the method to fail immediately due to an inability to compute the matrix inverse.\n\nD. The point $(0,0)$ is the only location where the Jacobian is singular. An initial guess on an axis that is close to the origin leads to a nearly-singular Jacobian, causing the subsequent iteration to produce a point extremely far from the true solution, indicating poor convergence behavior.\n\nE. The system of equations has no real solutions, so Newton's method will fail to converge regardless of the initial guess.", "solution": "We are given the system\n$$\nf_{1}(x,y)=x^{2}-y^{2}-4,\\qquad f_{2}(x,y)=xy-3,\n$$\nwith vector function $\\mathbf{F}(x,y)=(f_{1}(x,y),f_{2}(x,y))^{T}$ and Jacobian\n$$\nJ(x,y)=\\begin{pmatrix}\n\\frac{\\partial f_{1}}{\\partial x}  \\frac{\\partial f_{1}}{\\partial y}\\\\[4pt]\n\\frac{\\partial f_{2}}{\\partial x}  \\frac{\\partial f_{2}}{\\partial y}\n\\end{pmatrix}\n=\\begin{pmatrix}\n2x  -2y\\\\\ny  x\n\\end{pmatrix}.\n$$\nNewton’s method updates $\\mathbf{x}_{k+1}=\\mathbf{x}_{k}-J(\\mathbf{x}_{k})^{-1}\\mathbf{F}(\\mathbf{x}_{k})$, which requires $J$ to be invertible at the iterate.\n\nFirst, compute the determinant of the Jacobian:\n$$\n\\det J(x,y)=(2x)(x)-(-2y)(y)=2x^{2}+2y^{2}=2(x^{2}+y^{2}).\n$$\nThus $J(x,y)$ is singular if and only if $(x,y)=(0,0)$. In particular, $J$ is not singular at generic points on the axes (except at the origin). This immediately shows option C is false.\n\nNext, evaluate $\\mathbf{F}$ on the axes. On the $x$-axis with $y=0$,\n$$\n\\mathbf{F}(x,0)=\\bigl(x^{2}-4,\\,-3\\bigr),\n$$\nwhich is never the zero vector. On the $y$-axis with $x=0$,\n$$\n\\mathbf{F}(0,y)=\\bigl(-y^{2}-4,\\,-3\\bigr),\n$$\nwhich is also never the zero vector. Hence option B is false.\n\nNow check whether Newton’s method remains on an axis after one iteration. Use the Newton step $\\mathbf{s}$ defined by $J\\mathbf{s}=-\\mathbf{F}$. On the $x$-axis ($y=0$), we have\n$$\nJ(x,0)=\\begin{pmatrix}2x  0\\\\ 0  x\\end{pmatrix},\\qquad \\mathbf{F}(x,0)=\\begin{pmatrix}x^{2}-4\\\\ -3\\end{pmatrix}.\n$$\nThen\n$$\n\\mathbf{s}=-J^{-1}\\mathbf{F}=-\\begin{pmatrix}\\frac{1}{2x}  0\\\\[2pt] 0  \\frac{1}{x}\\end{pmatrix}\\begin{pmatrix}x^{2}-4\\\\ -3\\end{pmatrix}\n=\\begin{pmatrix}-\\frac{x^{2}-4}{2x}\\\\[4pt]\\frac{3}{x}\\end{pmatrix},\n$$\nso the next iterate is\n$$\nx_{1}=x-\\frac{x^{2}-4}{2x}=\\frac{x}{2}+\\frac{2}{x},\\qquad y_{1}=0+\\frac{3}{x}=\\frac{3}{x}.\n$$\nSince $y_{1}=\\frac{3}{x}\\neq 0$ for any finite $x$, the iterate immediately leaves the axis. A similar calculation on the $y$-axis ($x=0$) solves\n$$\n\\begin{pmatrix}0  -2y\\\\ y  0\\end{pmatrix}\\begin{pmatrix}s_{x}\\\\ s_{y}\\end{pmatrix}=-\\begin{pmatrix}-y^{2}-4\\\\ -3\\end{pmatrix}=\\begin{pmatrix}y^{2}+4\\\\ 3\\end{pmatrix},\n$$\nwhich yields $s_{x}=\\frac{3}{y}$ and $s_{y}=-\\frac{y^{2}+4}{2y}$, so\n$$\nx_{1}=0+\\frac{3}{y}=\\frac{3}{y},\\qquad y_{1}=y-\\frac{y^{2}+4}{2y}=\\frac{y}{2}-\\frac{2}{y}.\n$$\nAgain, the iterate leaves the axis immediately. Therefore option A is false.\n\nTo rule out option E, we check for real solutions. From $xy=3$ we have $y=\\frac{3}{x}$, and substituting into $x^{2}-y^{2}=4$ gives\n$$\nx^{2}-\\frac{9}{x^{2}}=4\\;\\;\\Longrightarrow\\;\\; x^{4}-4x^{2}-9=0.\n$$\nLet $t=x^{2}$. Then $t^{2}-4t-9=0$, so $t=2\\pm\\sqrt{13}$. The admissible root is $t=2+\\sqrt{13}0$, hence\n$$\nx=\\pm\\sqrt{2+\\sqrt{13}},\\qquad y=\\frac{3}{x}=\\pm\\frac{3}{\\sqrt{2+\\sqrt{13}}},\n$$\nwhich shows there are two real solutions. Therefore option E is false.\n\nIt remains to identify the fundamental reason axes are a poor choice for initial guesses. Since\n$$\n\\det J(x,y)=2(x^{2}+y^{2}),\n$$\nthe Jacobian is singular at $(0,0)$ and becomes ill-conditioned when $(x,y)$ is close to the origin. The explicit inverse is\n$$\nJ(x,y)^{-1}=\\frac{1}{2(x^{2}+y^{2})}\\begin{pmatrix}x  2y\\\\ -y  2x\\end{pmatrix},\n$$\nwhose entries scale like $\\frac{1}{x^{2}+y^{2}}$ times linear functions of $x$ and $y$. Near the origin this produces large Newton steps. On the axes in particular, the Newton updates derived above contain terms like $\\frac{3}{x}$ or $\\frac{3}{y}$, which become very large in magnitude when the initial guess is on an axis and close to the origin. This ill-conditioning explains the observed poor performance and is precisely captured by option D.\n\nTherefore, the most accurate and fundamental reason is that the Jacobian is singular at the origin and nearly singular for axis-aligned initial guesses near the origin, leading to instability and poor convergence.", "answer": "$$\\boxed{D}$$", "id": "2207871"}, {"introduction": "This advanced practice bridges the gap from manual calculation to computational science, challenging you to implement Newton's method and analyze its behavior in a system with famously rich dynamics. By exploring the fractal basins of attraction for the complex roots of unity, you will gain firsthand insight into the profound sensitivity of non-linear systems to initial conditions. This exercise not only tests your coding and algorithmic skills but also provides a window into the fascinating intersection of numerical analysis and chaos theory [@problem_id:3280962].", "problem": "Consider the system of non-linear equations in two real variables defined by the real and imaginary parts of the complex polynomial mapping $g(z)=z^3-1$, where $z=x+\\mathrm{i}y$ with $\\mathrm{i}^2=-1$. Define the vector-valued function $f:\\mathbb{R}^2\\to\\mathbb{R}^2$ by\n$$\nf(x,y)=\\begin{bmatrix}\nx^3-3xy^2-1\\\\\n3x^2y-y^3\n\\end{bmatrix}.\n$$\nThe zeros of $f$ correspond to the roots of $g(z)=0$, which are the three points\n$$\nr_1=\\begin{bmatrix}1\\\\0\\end{bmatrix},\\quad\nr_2=\\begin{bmatrix}-\\tfrac{1}{2}\\\\\\tfrac{\\sqrt{3}}{2}\\end{bmatrix},\\quad\nr_3=\\begin{bmatrix}-\\tfrac{1}{2}\\\\-\\tfrac{\\sqrt{3}}{2}\\end{bmatrix}.\n$$\nNewton's method applied to $f$ exhibits fractal basins of attraction in the plane, meaning that the set of initial guesses that converge to a given root has a boundary with fractal structure. Starting from the first principles of multivariate calculus and the definition of Newton's method for systems, derive the iteration scheme for solving $f(x,y)=\\mathbf{0}$ using the Jacobian matrix of $f$. Design and implement an algorithm that, for a given initial guess $(x_0,y_0)$, executes the Newton iteration until either a convergence criterion is met or a maximum number of iterations is reached.\n\nFor the analysis, quantify the computational cost of finding a root as a function of the initial guess precision. Let the initial guesses lie on a circle of radius $\\varepsilon$ around the root $r_1$, that is,\n$$\n(x_0(\\theta),y_0(\\theta))=r_1+\\varepsilon\\begin{bmatrix}\\cos\\theta\\\\ \\sin\\theta\\end{bmatrix},\n$$\nwhere $\\theta$ is an angle sampled uniformly as $\\theta_k=\\tfrac{2\\pi k}{K}$ for $k\\in\\{0,1,\\dots,K-1\\}$ and $K$ is a fixed positive integer. Angles must be interpreted in radians. For each $\\varepsilon$, run the algorithm from these $K$ initial guesses and compute:\n- The mean number of iterations taken among those initial guesses that converge within a fixed iteration budget.\n- The fraction of initial guesses that converge within the budget, expressed as a decimal in the interval $[0,1]$.\n\nDefine the convergence criteria as follows. An iterate $(x,y)$ is considered converged if either the residual norm satisfies $\\|f(x,y)\\|_2\\le \\tau_f$, or the distance to the nearest exact root $\\min_{j\\in\\{1,2,3\\}}\\|[x,y]^T-r_j\\|_2\\le \\tau_r$. If no initial guess at a given $\\varepsilon$ converges within the iteration budget, report the mean number of iterations for that $\\varepsilon$ as equal to the maximum iteration count $M$.\n\nStart your derivation only from fundamental definitions, namely the first-order multivariate Taylor expansion and the Jacobian matrix. Do not use pre-packaged formulas for Newton's method; derive the iteration rule explicitly. Ensure scientific realism by using sensible parameter values and robust stopping rules.\n\nUse the following test suite of precision radii:\n$$\n\\varepsilon\\in\\{0,\\;10^{-8},\\;0.2,\\;0.6,\\;1.2,\\;2.0\\}.\n$$\nUse $K=64$ angles, a maximum iteration count $M=50$, a residual tolerance $\\tau_f=10^{-12}$, a root-proximity tolerance $\\tau_r=10^{-8}$, and declare divergence if the iterate leaves the ball of radius $R_{\\max}=10$ centered at the origin. All angles must be processed in radians.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact format\n$$\n\\text{[[m}_1,\\text{m}_2,\\dots,\\text{m}_6],[p_1,p_2,\\dots,p_6]},\n$$\nwhere each $m_i$ is the mean iteration count (a floating-point number) for the corresponding $\\varepsilon$ and each $p_i$ is the fraction of convergent initial guesses (a floating-point number in $[0,1]$). The output must be computed for the six $\\varepsilon$ values given in the test suite, in the same order.", "solution": "The user wants to solve a system of non-linear equations, $f(x,y)=0$, using Newton's method. I will begin by deriving the iteration formula from first principles.\n\nLet $\\mathbf{x} = \\begin{bmatrix}x\\\\y\\end{bmatrix}$ be a vector in $\\mathbb{R}^2$. The system of non-linear equations is given by the vector-valued function $f(\\mathbf{x})=\\mathbf{0}$. Newton's method is an iterative algorithm that generates a sequence of approximations $\\mathbf{x}_0, \\mathbf{x}_1, \\mathbf{x}_2, \\dots$ that, under suitable conditions, converge to a root of $f$.\n\nThe derivation starts from the first-order multivariate Taylor expansion of $f$ around the current iterate $\\mathbf{x}_k$:\n$$\nf(\\mathbf{x}) \\approx f(\\mathbf{x}_k) + J_f(\\mathbf{x}_k)(\\mathbf{x} - \\mathbf{x}_k)\n$$\nwhere $J_f(\\mathbf{x}_k)$ is the Jacobian matrix of $f$ evaluated at $\\mathbf{x}_k$. We seek the next iterate, $\\mathbf{x}_{k+1}$, such that $f(\\mathbf{x}_{k+1}) = \\mathbf{0}$. By substituting $\\mathbf{x}_{k+1}$ for $\\mathbf{x}$ and setting the approximation to zero, we obtain:\n$$\n\\mathbf{0} \\approx f(\\mathbf{x}_k) + J_f(\\mathbf{x}_k)(\\mathbf{x}_{k+1} - \\mathbf{x}_k)\n$$\nThis equation can be rearranged to solve for the update step, $\\Delta\\mathbf{x}_k = \\mathbf{x}_{k+1} - \\mathbf{x}_k$:\n$$\nJ_f(\\mathbf{x}_k)\\Delta\\mathbf{x}_k = -f(\\mathbf{x}_k)\n$$\nAssuming the Jacobian matrix is invertible, we can solve for $\\Delta\\mathbf{x}_k$:\n$$\n\\Delta\\mathbf{x}_k = -[J_f(\\mathbf{x}_k)]^{-1}f(\\mathbf{x}_k)\n$$\nThe Newton iteration is then defined by the update rule:\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\Delta\\mathbf{x}_k = \\mathbf{x}_k - [J_f(\\mathbf{x}_k)]^{-1}f(\\mathbf{x}_k)\n$$\nNow, we apply this general formula to the specific function given in the problem:\n$$\nf(x,y) = \\begin{bmatrix} f_1(x,y) \\\\ f_2(x,y) \\end{bmatrix} = \\begin{bmatrix} x^3 - 3xy^2 - 1 \\\\ 3x^2y - y^3 \\end{bmatrix}\n$$\nThe Jacobian matrix $J_f(x,y)$ consists of the partial derivatives of $f$'s components:\n$$\nJ_f(x,y) = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x}  \\frac{\\partial f_1}{\\partial y} \\\\ \\frac{\\partial f_2}{\\partial x}  \\frac{\\partial f_2}{\\partial y} \\end{bmatrix} = \\begin{bmatrix} 3x^2 - 3y^2  -6xy \\\\ 6xy  3x^2 - 3y^2 \\end{bmatrix}\n$$\nTo find the inverse of the Jacobian, we first compute its determinant:\n$$\n\\det(J_f(x,y)) = (3x^2 - 3y^2)(3x^2 - 3y^2) - (-6xy)(6xy) = 9(x^2 - y^2)^2 + 36x^2y^2\n$$\n$$\n\\det(J_f(x,y)) = 9(x^4 - 2x^2y^2 + y^4) + 36x^2y^2 = 9(x^4 + 2x^2y^2 + y^4) = 9(x^2 + y^2)^2\n$$\nThe determinant is zero only when $x^2 + y^2 = 0$, which occurs only at the origin $(x,y)=(0,0)$. At this point, the Jacobian is singular, and the Newton step is undefined. This is consistent with $(0,0)$ being a critical point of the underlying complex map $g(z)=z^3-1$, as its derivative $g'(z)=3z^2$ is zero at $z=0$.\n\nFor any $(x,y) \\neq (0,0)$, the inverse of the a $2 \\times 2$ matrix is given by $[J_f]^{-1} = \\frac{1}{\\det(J_f)}\\begin{bmatrix} d  -b \\\\ -c  a \\end{bmatrix}$. Applying this to our Jacobian gives:\n$$\n[J_f(x,y)]^{-1} = \\frac{1}{9(x^2+y^2)^2} \\begin{bmatrix} 3x^2 - 3y^2  6xy \\\\ -6xy  3x^2 - 3y^2 \\end{bmatrix} = \\frac{1}{3(x^2+y^2)^2} \\begin{bmatrix} x^2 - y^2  2xy \\\\ -2xy  x^2 - y^2 \\end{bmatrix}\n$$\nThe Newton update step $\\Delta\\mathbf{x}_k = \\begin{bmatrix} \\Delta x_k \\\\ \\Delta y_k \\end{bmatrix}$ is computed by solving the linear system $J_f \\Delta\\mathbf{x} = -f$. Using the explicit inverse:\n$$\n\\begin{bmatrix} \\Delta x_k \\\\ \\Delta y_k \\end{bmatrix} = -\\frac{1}{3(x_k^2+y_k^2)^2} \\begin{bmatrix} x_k^2 - y_k^2  2x_ky_k \\\\ -2x_ky_k  x_k^2 - y_k^2 \\end{bmatrix} \\begin{bmatrix} x_k^3 - 3x_ky_k^2 - 1 \\\\ 3x_k^2y_k - y_k^3 \\end{bmatrix}\n$$\nPerforming the matrix-vector multiplication yields a simplified expression for the update:\n$$\nx_{k+1} = x_k + \\Delta x_k = \\frac{2}{3}x_k + \\frac{x_k^2 - y_k^2}{3(x_k^2+y_k^2)^2}\n$$\n$$\ny_{k+1} = y_k + \\Delta y_k = \\frac{2}{3}y_k - \\frac{2x_ky_k}{3(x_k^2+y_k^2)^2}\n$$\nThese are the explicit iteration formulas derived from first principles.\n\nThe algorithm to be implemented will perform the following steps for each specified value of $\\varepsilon$:\n1.  Initialize counters for the total number of iterations and the number of converged trajectories.\n2.  Generate $K=64$ initial points $(x_0, y_0)$ on a circle of radius $\\varepsilon$ centered at the root $r_1=(1,0)$ using the formula $(x_0,y_0) = (1+\\varepsilon\\cos\\theta, \\varepsilon\\sin\\theta)$, with $\\theta_k = \\frac{2\\pi k}{K}$ for $k \\in \\{0, 1, \\dots, K-1\\}$.\n3.  For each initial point, execute the Newton's method iteration up to a maximum of $M=50$ times.\n4.  In each iteration, check for convergence. An iterate $(x,y)$ is considered converged if the Euclidean norm of the residual, $\\|f(x,y)\\|_2$, is less than or equal to $\\tau_f=10^{-12}$, or if its distance to the nearest known root is less than or equal to $\\tau_r=10^{-8}$. If convergence occurs at iteration $i$, the number of iterations for this trajectory is recorded as $i$, and the counters are updated.\n5.  An iteration is considered divergent if the iterate's norm $\\|(x,y)\\|_2$ exceeds $R_{\\max}=10$. If an initial guess fails to converge within $M$ iterations, it is counted as non-convergent.\n6.  After processing all $K$ initial points for a given $\\varepsilon$, calculate the mean number of iterations for the successfully converged trajectories. If no trajectories converge, this mean is reported as $M$.\n7.  Calculate the fraction of trajectories that converged.\n8.  The final output will be two lists: one containing the mean iteration counts and the other containing the convergence fractions for the specified sequence of $\\varepsilon$ values.\nFor the case $\\varepsilon=0$, the initial guess is $(1,0)$, which is a root. The algorithm should correctly identify this as converged in $0$ iterations.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes Newton's method for the system of equations derived\n    from the complex polynomial g(z) = z^3 - 1.\n    \"\"\"\n    \n    # --- Problem Parameters ---\n    epsilons = [0.0, 1e-8, 0.2, 0.6, 1.2, 2.0]\n    K = 64\n    M = 50\n    tau_f = 1e-12\n    tau_r = 1e-8\n    R_max = 10.0\n\n    # --- Problem Definitions ---\n    # The three roots of z^3 - 1 = 0 in the complex plane, as (x,y) coordinates.\n    r1 = np.array([1.0, 0.0])\n    r2 = np.array([-0.5, np.sqrt(3.0) / 2.0])\n    r3 = np.array([-0.5, -np.sqrt(3.0) / 2.0])\n    roots = np.array([r1, r2, r3])\n\n    def f(p: np.ndarray) - np.ndarray:\n        \"\"\"\n        Calculates the vector-valued function f(x, y).\n        p is a numpy array [x, y].\n        \"\"\"\n        x, y = p\n        f1 = x**3 - 3.0 * x * y**2 - 1.0\n        f2 = 3.0 * x**2 * y - y**3\n        return np.array([f1, f2])\n\n    # Lists to store the final results\n    mean_iters_list = []\n    conv_frac_list = []\n\n    for eps in epsilons:\n        total_iters = 0\n        converged_count = 0\n        \n        # Generate K initial guesses on a circle of radius eps around root r1\n        angles = np.linspace(0.0, 2.0 * np.pi, K, endpoint=False)\n        \n        for theta in angles:\n            # Current initial guess\n            p0 = r1 + eps * np.array([np.cos(theta), np.sin(theta)])\n            p = p0.copy()\n            \n            is_converged = False\n            iterations_for_path = 0\n\n            # Newton's method loop\n            for i in range(M + 1):\n                # 1. Check for divergence (leaving a large ball around the origin)\n                if np.linalg.norm(p)  R_max:\n                    break\n\n                # 2. Check for convergence\n                # Criterion 2a: Residual norm is small\n                if np.linalg.norm(f(p)) = tau_f:\n                    iterations_for_path = i\n                    is_converged = True\n                    break\n                \n                # Criterion 2b: Proximity to an exact root is small\n                dist_to_roots = np.linalg.norm(p - roots, axis=1)\n                if np.min(dist_to_roots) = tau_r:\n                    iterations_for_path = i\n                    is_converged = True\n                    break\n\n                # 3. If max iterations reached, the path has not converged\n                if i == M:\n                    break\n                \n                # 4. Perform the Newton iteration step\n                x, y = p\n                \n                # Denominator corresponds to 3*|z|^4, where z=x+iy\n                denom_part = x**2 + y**2\n                \n                # Handle singularity at the origin (z=0)\n                if denom_part  1e-24: # Effectively zero, prevent division by zero\n                    break\n                    \n                denom = 3.0 * denom_part**2\n                \n                # Update formulas derived in the solution\n                x_new = (2.0 / 3.0) * x + (x**2 - y**2) / denom\n                y_new = (2.0 / 3.0) * y - (2.0 * x * y) / denom\n                \n                p = np.array([x_new, y_new])\n\n            if is_converged:\n                converged_count += 1\n                total_iters += iterations_for_path\n        \n        # Calculate the required metrics for the current epsilon\n        if converged_count  0:\n            mean_iters = float(total_iters) / converged_count\n        else:\n            # As per problem specification\n            mean_iters = float(M)\n            \n        conv_frac = float(converged_count) / K\n        \n        mean_iters_list.append(mean_iters)\n        conv_frac_list.append(conv_frac)\n\n    # Format the final output string exactly as required\n    mean_iters_str = ','.join(map(str, mean_iters_list))\n    conv_frac_str = ','.join(map(str, conv_frac_list))\n    \n    print(f\"[[{mean_iters_str}],[{conv_frac_str}]]\")\n\nsolve()\n```", "id": "3280962"}]}