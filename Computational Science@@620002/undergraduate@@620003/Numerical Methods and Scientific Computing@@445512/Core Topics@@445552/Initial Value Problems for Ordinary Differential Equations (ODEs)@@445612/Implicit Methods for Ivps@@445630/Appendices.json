{"hands_on_practices": [{"introduction": "To truly understand what makes an implicit method \"implicit,\" it's best to perform a step by hand. This exercise models the cooling of a CPU using the implicit Euler method, a foundational implicit scheme. By working through a single time step, you will see how the unknown future state, $T_{n+1}$, appears on both sides of the update equation, requiring you to solve for it algebraically before you can advance the solution [@problem_id:2178316].", "problem": "A simplified thermal model for a Central Processing Unit (CPU) under a variable load is described by the following first-order ordinary differential equation:\n$$ \\frac{dT}{dt} = -k(T - T_a) + P(t) $$\nwhere $T(t)$ is the temperature of the CPU in degrees Celsius ($^\\circ\\text{C}$) at time $t$ in minutes, $k$ is the heat dissipation constant, $T_a$ is the constant ambient temperature, and $P(t)$ represents the heat generated by the CPU's activity.\n\nSuppose the parameters are given as:\n- Heat dissipation constant, $k = 0.5 \\text{ min}^{-1}$\n- Ambient temperature, $T_a = 25^\\circ\\text{C}$\n- Heat generation function, $P(t) = P_0 \\cos(\\omega t)$, with $P_0 = 10^\\circ\\text{C}/\\text{min}$ and $\\omega = \\frac{\\pi}{6} \\text{ rad/min}$.\n- The initial temperature of the CPU is $T(0) = 85^\\circ\\text{C}$.\n\nTo predict the temperature evolution, a numerical method is employed. The temperature at the next time step, $T_{n+1}$ at time $t_{n+1}$, is estimated from the temperature at the current time step, $T_n$ at time $t_n$, using the following update rule:\n$$ T_{n+1} = T_n + h \\left( \\frac{dT}{dt} \\right)_{t=t_{n+1}, T=T_{n+1}} $$\nwhere $h = t_{n+1} - t_n$ is the time step.\n\nUsing a single step of this numerical method with a step size of $h = 0.5$ minutes, calculate the approximate temperature of the CPU at $t = 0.5$ minutes.\n\nExpress your answer in degrees Celsius, rounded to four significant figures.", "solution": "We are given the first-order ODE\n$$\n\\frac{dT}{dt}=-k\\left(T-T_{a}\\right)+P(t), \\quad P(t)=P_{0}\\cos(\\omega t),\n$$\nand we are to apply one step of the implicit (backward) Euler method with step size $h$ from $t_{0}=0$ to $t_{1}=h$. The backward Euler update is\n$$\nT_{1}=T_{0}+h\\left.\\frac{dT}{dt}\\right|_{t=t_{1},\\,T=T_{1}}=T_{0}+h\\big(-k(T_{1}-T_{a})+P_{0}\\cos(\\omega t_{1})\\big).\n$$\nSolve this equation for $T_{1}$. Expand the right-hand side and collect $T_{1}$ terms:\n$$\nT_{1}=T_{0}-hkT_{1}+hkT_{a}+hP_{0}\\cos(\\omega t_{1}).\n$$\nBring the $T_{1}$ terms to the left-hand side:\n$$\nT_{1}+hkT_{1}=T_{0}+hkT_{a}+hP_{0}\\cos(\\omega t_{1}).\n$$\nFactor the left-hand side and solve for $T_{1}$:\n$$\nT_{1}=\\frac{T_{0}+hkT_{a}+hP_{0}\\cos(\\omega t_{1})}{1+hk}.\n$$\nNow substitute the given data $T_{0}=85$, $k=0.5$, $T_{a}=25$, $P_{0}=10$, $\\omega=\\frac{\\pi}{6}$, $h=0.5$, and $t_{1}=0.5$:\n$$\n1+hk=1+0.5\\times 0.5=1.25,\n$$\n$$\nhkT_{a}=0.5\\times 0.5\\times 25=6.25,\n$$\n$$\nhP_{0}\\cos(\\omega t_{1})=0.5\\times 10\\times \\cos\\!\\left(\\frac{\\pi}{6}\\times 0.5\\right)=5\\cos\\!\\left(\\frac{\\pi}{12}\\right).\n$$\nThus\n$$\nT_{1}=\\frac{85+6.25+5\\cos\\!\\left(\\frac{\\pi}{12}\\right)}{1.25}.\n$$\nUsing $\\cos\\!\\left(\\frac{\\pi}{12}\\right)\\approx 0.9659258263$, we get\n$$\nT_{1}\\approx \\frac{85+6.25+4.829629131}{1.25}=\\frac{96.079629131}{1.25}\\approx 76.86370331.\n$$\nRounded to four significant figures, the approximate temperature at $t=0.5$ minutes is $76.86$ degrees Celsius.", "answer": "$$\\boxed{76.86}$$", "id": "2178316"}, {"introduction": "While simple linear problems can be solved algebraically, most real-world applications involve nonlinear ODEs where the implicit equation must be solved iteratively. This advanced practice guides you through implementing the implicit midpoint rule, a powerful second-order method, using Newton's method to handle the nonlinearity at each step. By testing your implementation against problems with known solutions, you will perform a convergence study to numerically verify the method's theoretical second-order accuracy, a crucial skill for validating any numerical code [@problem_id:3241497].", "problem": "You are to implement a program that numerically verifies the second-order convergence of an implicit time integration method applied to initial value problems (IVPs). The focus is on the implicit midpoint rule, a one-step method for ordinary differential equations (ODEs). Use the mathematical base consisting of the definition of an IVP, the integral form of the solution, and the concept of local and global truncation errors. Do not assume any formula for the method is given; derive and implement it based on these fundamental principles. All trigonometric functions must use radians.\n\nThe IVP is defined by the differential equation $y^{\\prime}(t)=f(t,y(t))$ with an initial condition $y(t_{0})=y_{0}$, where $y(t)$ may be scalar or vector-valued. Write a program that:\n- Implements the implicit midpoint rule for advancing from $t_{n}$ to $t_{n+1}=t_{n}+h$.\n- For nonlinear equations at each step, solves the resulting implicit nonlinear system to a tight tolerance using Newton’s method with a numerically approximated Jacobian and a reasonable initial guess.\n- Computes the global error at a final time $T$ for several step sizes $h$ and estimates the observed order of convergence based on error ratios when halving the step size.\n\nUse the following three test IVPs, each with a known analytic solution, to form a test suite. For all tests, measure the error at the final time $T$; for scalar problems use the absolute value, and for vector problems use the Euclidean norm. For each test, use three step sizes $(h_{1},h_{2},h_{3})$ with $h_{2}=h_{1}/2$ and $h_{3}=h_{2}/2$. For a given test, compute the errors $E(h_{1}),E(h_{2}),E(h_{3})$ at $T$ and estimate the observed order twice via\n$$\np_{12}=\\log_{2}\\left(\\frac{E(h_{1})}{E(h_{2})}\\right),\\quad\np_{23}=\\log_{2}\\left(\\frac{E(h_{2})}{E(h_{3})}\\right),\n$$\nand report the average $\\hat{p}=(p_{12}+p_{23})/2$ for that test.\n\nTest A (scalar, nonstiff, time-dependent forcing):\n- Differential equation: $y^{\\prime}(t)=-2\\,y(t)+\\sin(t)$, with $y(0)=1$.\n- Exact solution: $y(t)=\\frac{6}{5}e^{-2t}+\\frac{2\\sin(t)-\\cos(t)}{5}$.\n- Final time: $T=2$.\n- Step sizes: $(h_{1},h_{2},h_{3})=(0.2,0.1,0.05)$.\n\nTest B (two-dimensional rotation, constant coefficient, conservative):\n- Differential equation: $\\mathbf{y}^{\\prime}(t)=\\begin{bmatrix}-\\omega \\,y_{2}(t)\\\\ \\omega \\,y_{1}(t)\\end{bmatrix}$ with $\\omega=3$, initial condition $\\mathbf{y}(0)=\\begin{bmatrix}1\\\\ -1\\end{bmatrix}$.\n- Exact solution: $\\mathbf{y}(t)=R(\\omega t)\\,\\mathbf{y}_{0}$ with $R(\\theta)=\\begin{bmatrix}\\cos(\\theta)&-\\sin(\\theta)\\\\ \\sin(\\theta)&\\cos(\\theta)\\end{bmatrix}$.\n- Final time: $T=1.6$.\n- Step sizes: $(h_{1},h_{2},h_{3})=(0.2,0.1,0.05)$.\n\nTest C (scalar, moderately stiff, time-dependent forcing):\n- Differential equation: $y^{\\prime}(t)=-50\\,y(t)+50\\,\\sin(t)$ with $y(0)=0$.\n- Exact solution: $y(t)=\\frac{2500\\sin(t)-50\\cos(t)}{2501}+\\frac{50}{2501}e^{-50t}$.\n- Final time: $T=1$.\n- Step sizes: $(h_{1},h_{2},h_{3})=(0.1,0.05,0.025)$.\n\nAlgorithmic requirements:\n- Use Newton’s method at each implicit step with an absolute residual tolerance of $10^{-12}$ and a maximum of $20$ iterations per step.\n- Approximate the Jacobian of the implicit residual via finite differences.\n- Use a reasonable initial guess for Newton’s method at each step, such as an explicit Euler predictor.\n- Use uniform time steps so that $T/h$ is an integer for each $h$.\n\nAngle unit: all angles are in radians.\n\nFinal output:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the form \"[p1,p2,p3]\", where each entry is the average observed order $\\hat{p}$ for Tests A, B, and C respectively, rounded to three decimal places.\n- No other text should be printed.\n\nYour program must be a complete, runnable implementation that initializes and runs the above test suite without user input and produces the required single-line output. All numerical values must be handled in floating-point arithmetic and all trigonometric functions use radians, with the final observed orders reported as specified.", "solution": "The user has provided a valid problem statement. The following is a complete, reasoned solution.\n\nThe problem requires the derivation and implementation of the implicit midpoint rule for solving initial value problems (IVPs), followed by a numerical verification of its convergence order. The implementation must solve the implicit equation at each time step using Newton's method with a numerically approximated Jacobian.\n\n**1. Derivation of the Implicit Midpoint Rule**\n\nAn initial value problem is defined by a first-order ordinary differential equation and an initial condition:\n$$\ny^{\\prime}(t) = f(t, y(t)), \\quad y(t_0) = y_0\n$$\nIntegrating the differential equation over a single time step from $t_n$ to $t_{n+1} = t_n + h$ yields the exact relation:\n$$\ny(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(\\tau, y(\\tau)) \\, d\\tau\n$$\nThe implicit midpoint rule is derived by approximating the integral using the one-point rectangular quadrature rule evaluated at the midpoint of the interval, $t_{n+1/2} = t_n + h/2$. This approximation is:\n$$\n\\int_{t_n}^{t_{n+1}} f(\\tau, y(\\tau)) \\, d\\tau \\approx h \\cdot f\\left(t_n + \\frac{h}{2}, y\\left(t_n + \\frac{h}{2}\\right)\\right)\n$$\nSubstituting this into the integral form gives a one-step formula relating the numerical approximations $y_n \\approx y(t_n)$ and $y_{n+1} \\approx y(t_{n+1})$:\n$$\ny_{n+1} \\approx y_n + h \\cdot f\\left(t_n + \\frac{h}{2}, y\\left(t_n + \\frac{h}{2}\\right)\\right)\n$$\nThis formula is not yet computable because the value $y(t_n + h/2)$ is unknown. To form a practical scheme, we approximate this midpoint value using a centered average of the values at the beginning and end of the step, which is a second-order accurate approximation:\n$$\ny\\left(t_n + \\frac{h}{2}\\right) \\approx \\frac{y(t_n) + y(t_{n+1})}{2} \\approx \\frac{y_n + y_{n+1}}{2}\n$$\nSubstituting this into the scheme results in the **implicit midpoint rule**:\n$$\ny_{n+1} = y_n + h f\\left(t_n + \\frac{h}{2}, \\frac{y_n + y_{n+1}}{2}\\right)\n$$\nThis equation is implicit because the unknown value $y_{n+1}$ appears on both sides. For a general nonlinear function $f$, this represents a system of nonlinear algebraic equations that must be solved for $y_{n+1}$ at each time step.\n\n**2. Solving the Implicit Equation with Newton's Method**\n\nTo find $y_{n+1}$ at each step, we must solve a root-finding problem. Let $z$ be a candidate for the solution $y_{n+1}$. We define a residual function $G(z)$ whose root is the desired value:\n$$\nG(z) = z - y_n - h f\\left(t_n + \\frac{h}{2}, \\frac{y_n + z}{2}\\right) = 0\n$$\nThis nonlinear equation (or system of equations for vector-valued $y$) is solved iteratively using Newton's method. Starting with an initial guess $z^{(0)}$, successive approximations $z^{(k)}$ are generated by the formula:\n$$\nz^{(k+1)} = z^{(k)} - [J_G(z^{(k)})]^{-1} G(z^{(k)})\n$$\nwhere $J_G(z^{(k)})$ is the Jacobian matrix of $G$ with respect to $z$, evaluated at the current iterate $z^{(k)}$. The problem specifies an explicit Euler step as the initial guess: $z^{(0)} = y_n + h f(t_n, y_n)$.\n\nThe analytical Jacobian of $G(z)$ is found by differentiation with respect to $z$:\n$$\nJ_G(z) = \\frac{\\partial G}{\\partial z} = I - h \\cdot \\frac{\\partial}{\\partial z} \\left[ f\\left(t_n + \\frac{h}{2}, \\frac{y_n + z}{2}\\right) \\right]\n$$\nwhere $I$ is the identity matrix. Applying the chain rule, and letting $f_y$ denote the Jacobian of $f$ with respect to its second argument (the state $y$), we have:\n$$\nJ_G(z) = I - \\frac{h}{2} f_y\\left(t_n + \\frac{h}{2}, \\frac{y_n + z}{2}\\right)\n$$\nAs required, this Jacobian is approximated numerically. For a vector function $G(z)$ of dimension $d$, the $j$-th column of the Jacobian matrix $J_G$ is approximated using a forward finite difference:\n$$\n(J_G)_{:,j} \\approx \\frac{G(z + \\epsilon e_j) - G(z)}{\\epsilon} \\quad \\text{for } j=1, \\dots, d\n$$\nwhere $e_j$ is the $j$-th standard basis vector and $\\epsilon$ is a small perturbation (e.g., $\\epsilon \\approx 10^{-8}$). For a scalar problem ($d=1$), this simplifies to $G'(z) \\approx (G(z+\\epsilon) - G(z))/\\epsilon$.\n\nThe iteration at each time step proceeds until the Euclidean norm of the residual, $\\|G(z^{(k)})\\|$, falls below a specified tolerance of $10^{-12}$, or until a maximum of $20$ iterations is reached.\n\n**3. Verification of Convergence Order**\n\nFor a numerical method of order $p$, the global error $E(h)$ at a fixed final time $T$, defined as $E(h) = \\|y_{\\text{numerical}}(T) - y_{\\text{exact}}(T)\\|$, behaves as $E(h) \\approx C h^p$ for a constant $C$ and sufficiently small step size $h$. The error norm is the absolute value for scalar problems and the Euclidean norm for vector problems.\n\nGiven two simulations with step sizes $h_1$ and $h_2 = h_1/2$, the corresponding errors are $E(h_1) \\approx C h_1^p$ and $E(h_2) \\approx C (h_1/2)^p$. The ratio of these errors is:\n$$\n\\frac{E(h_1)}{E(h_2)} \\approx \\frac{C h_1^p}{C (h_1/2)^p} = 2^p\n$$\nSolving for $p$ gives the observed order of convergence:\n$$\np \\approx \\log_2\\left(\\frac{E(h_1)}{E(h_2)}\\right)\n$$\nThe problem specifies using three step sizes $(h_1, h_2, h_3)$ where $h_2=h_1/2$ and $h_3=h_2/2$. This allows for two independent estimates of the order, $p_{12}$ and $p_{23}$:\n$$\np_{12} = \\log_2\\left(\\frac{E(h_{1})}{E(h_{2})}\\right), \\quad p_{23} = \\log_2\\left(\\frac{E(h_{2})}{E(h_{3})}\\right)\n$$\nThe final reported estimate is the average of these two values, $\\hat{p} = (p_{12} + p_{23}) / 2$. The implicit midpoint rule is a symmetric one-step method, and its theoretical order of convergence is $p=2$. The computed values of $\\hat{p}$ are therefore expected to be close to $2$.\n\nThe implementation will consist of a general-purpose ODE solver for the implicit midpoint rule, which calls a Newton solver at each step. This framework is then applied to the three specified test cases to compute and report the respective average observed orders of convergence.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef newton_solver(G, z0, tol, max_iter, jac_eps):\n    \"\"\"\n    Solves the nonlinear system G(z) = 0 using Newton's method.\n\n    Args:\n        G (callable): The residual function G(z).\n        z0 (np.ndarray): The initial guess for the solution.\n        tol (float): The convergence tolerance for the norm of the residual.\n        max_iter (int): The maximum number of iterations.\n        jac_eps (float): Perturbation for finite difference Jacobian approximation.\n\n    Returns:\n        np.ndarray: The solution z.\n    \"\"\"\n    z = np.copy(z0)\n    dim = z.shape[0]\n\n    for _ in range(max_iter):\n        g_val = G(z)\n        \n        if np.linalg.norm(g_val) < tol:\n            return z\n\n        # Numerically approximate the Jacobian J_G(z)\n        jac = np.zeros((dim, dim))\n        for j in range(dim):\n            z_plus_eps = np.copy(z)\n            z_plus_eps[j] += jac_eps\n            g_plus_eps = G(z_plus_eps)\n            jac[:, j] = (g_plus_eps - g_val) / jac_eps\n\n        # Solve the linear system J * delta_z = -G\n        try:\n            delta_z = np.linalg.solve(jac, -g_val)\n        except np.linalg.LinAlgError:\n            # If Jacobian is singular, may indicate issues but try a small step.\n            # For this problem, this is unlikely.\n            raise RuntimeError(\"Newton's method failed due to singular Jacobian.\")\n\n        z += delta_z\n\n    raise RuntimeError(f\"Newton's method failed to converge within {max_iter} iterations.\")\n\ndef solve_ode_implicit_midpoint(f, y0, t_span, h, newton_tol, newton_max_iter):\n    \"\"\"\n    Solves an ODE IVP using the implicit midpoint rule.\n\n    Args:\n        f (callable): The ODE function f(t, y).\n        y0 (np.ndarray): The initial condition.\n        t_span (tuple): The start and end time (t0, T).\n        h (float): The step size.\n        newton_tol (float): Tolerance for the Newton solver.\n        newton_max_iter (int): Max iterations for the Newton solver.\n\n    Returns:\n        np.ndarray: The numerical solution at time T.\n    \"\"\"\n    t0, T = t_span\n    num_steps = round((T - t0) / h)\n    \n    y_n = np.copy(y0)\n    t_n = t0\n    jac_eps = np.sqrt(np.finfo(float).eps)\n\n    for _ in range(num_steps):\n        # Define the residual function for the current step\n        # G(z) = z - y_n - h * f(t_n + h/2, (y_n + z)/2) = 0\n        mid_t = t_n + h / 2.0\n        \n        # Use a closure to capture y_n, h, mid_t, f\n        def G(z):\n            y_mid = (y_n + z) / 2.0\n            return z - y_n - h * f(mid_t, y_mid)\n\n        # Initial guess for Newton's method (Explicit Euler predictor)\n        z0 = y_n + h * f(t_n, y_n)\n\n        # Solve for y_{n+1}\n        y_np1 = newton_solver(G, z0, newton_tol, newton_max_iter, jac_eps)\n        \n        y_n = y_np1\n        t_n += h\n    \n    return y_n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Algorithmic parameters from the problem statement\n    NEWTON_TOL = 1e-12\n    NEWTON_MAX_ITER = 20\n\n    # Test A: scalar, nonstiff\n    def f_A(t, y): return np.array([-2.0 * y[0] + np.sin(t)])\n    def y_exact_A(t, y0): return np.array([(6.0/5.0)*np.exp(-2.0*t) + (2.0*np.sin(t) - np.cos(t))/5.0])\n    \n    # Test B: 2D rotation\n    def f_B(t, y):\n        omega = 3.0\n        return np.array([-omega * y[1], omega * y[0]])\n    def y_exact_B(t, y0):\n        omega = 3.0\n        theta = omega * t\n        R = np.array([[np.cos(theta), -np.sin(theta)],\n                      [np.sin(theta),  np.cos(theta)]])\n        return R @ y0\n\n    # Test C: scalar, stiff\n    def f_C(t, y): return np.array([-50.0 * y[0] + 50.0 * np.sin(t)])\n    def y_exact_C(t, y0): return np.array([(2500.0 * np.sin(t) - 50.0 * np.cos(t)) / 2501.0 + (50.0 / 2501.0) * np.exp(-50.0 * t)])\n\n    test_cases = [\n        {\n            'name': 'A',\n            'f': f_A,\n            'y_exact': y_exact_A,\n            'y0': np.array([1.0]),\n            't_span': (0.0, 2.0),\n            'h_values': (0.2, 0.1, 0.05)\n        },\n        {\n            'name': 'B',\n            'f': f_B,\n            'y_exact': y_exact_B,\n            'y0': np.array([1.0, -1.0]),\n            't_span': (0.0, 1.6),\n            'h_values': (0.2, 0.1, 0.05)\n        },\n        {\n            'name': 'C',\n            'f': f_C,\n            'y_exact': y_exact_C,\n            'y0': np.array([0.0]),\n            't_span': (0.0, 1.0),\n            'h_values': (0.1, 0.05, 0.025)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        errors = []\n        T = case['t_span'][1]\n        \n        for h in case['h_values']:\n            y_num = solve_ode_implicit_midpoint(\n                case['f'], case['y0'], case['t_span'], h, NEWTON_TOL, NEWTON_MAX_ITER\n            )\n            y_true = case['y_exact'](T, case['y0'])\n            \n            # Error is Euclidean norm (which is absolute value for scalars)\n            error = np.linalg.norm(y_num - y_true)\n            errors.append(error)\n\n        # Calculate observed order of convergence\n        p12 = np.log2(errors[0] / errors[1])\n        p23 = np.log2(errors[1] / errors[2])\n        p_hat = (p12 + p23) / 2.0\n        results.append(p_hat)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{p:.3f}' for p in results)}]\")\n\nsolve()\n```", "id": "3241497"}, {"introduction": "Implicit methods are prized for their stability, especially for large systems of ODEs arising from the discretization of partial differential equations. This power comes at a computational cost: solving a large system of equations at every time step, where a naive approach using dense matrix solvers is often prohibitive, scaling as $\\mathcal{O}(n^3)$. This exercise demonstrates how to overcome this barrier by exploiting the special structure of the system's matrix—in this case, a tridiagonal form—to design a highly efficient solver that scales linearly ($\\mathcal{O}(n)$), making large-scale simulations feasible [@problem_id:3241515].", "problem": "You are to design and analyze an efficient implicit time-stepping method for an Initial Value Problem (IVP) in Ordinary Differential Equations (ODEs). Begin from the definition of an Ordinary Differential Equation (ODE) and the statement of an Initial Value Problem (IVP). Consider a linear system of the form $y'(t) = A y(t)$ for $t \\ge 0$, where $y(t) \\in \\mathbb{R}^n$ and $A \\in \\mathbb{R}^{n \\times n}$ is a tridiagonal matrix. The goal is to construct specific tridiagonal matrices $A$ and implement the Backward Euler method to advance the solution in time using an algorithm that avoids a full matrix inversion, while maintaining computational efficiency and numerical stability.\n\nYour program must:\n- Construct a tridiagonal matrix $A$ of specified size $n$ for each test case. A tridiagonal matrix has nonzero entries only on its main diagonal and the first sub- and super-diagonals. Denote the sub-diagonal by $a_i$ for $i=1,\\dots,n-1$, the diagonal by $d_i$ for $i=1,\\dots,n$, and the super-diagonal by $c_i$ for $i=1,\\dots,n-1$.\n- Derive and implement a single-step Backward Euler method (Backward Euler method (BE)) for the IVP $y'(t)=A y(t)$ without performing a dense inversion. At each time step from $t^n$ to $t^{n+1} = t^n + h$, set up and solve the linear system for $y^{n+1}$ using a tridiagonal solver based on Gaussian elimination specialized for tridiagonal matrices (often called the Thomas algorithm), which operates in $\\mathcal{O}(n)$ time and $\\mathcal{O}(n)$ memory.\n- Validate the tridiagonal solver by comparing its results to those obtained via a dense linear solver for the same linear system at each step, and report the maximum absolute difference.\n\nUse the following test suite. For each case, perform the specified operations and record a single floating-point number equal to the infinity-norm (maximum absolute value) of the difference between the solution computed by the tridiagonal solver and the solution computed by a dense solver. No physical units apply in this problem.\n\nTest suite:\n- Case $1$ (happy path, small $n$): Let $n=5$. Define $A$ by $d_i=-2$ for $i=1,\\dots,5$, $a_i=1$ for $i=1,\\dots,4$, and $c_i=1$ for $i=1,\\dots,4$. Use time step $h=0.05$. Take initial condition $y^0 = [1,0,0,0,0]^T$. Perform a single Backward Euler step and record the infinity-norm of the difference between the tridiagonal solution and the dense solution.\n- Case $2$ (diagonal-only, boundary condition coverage): Let $n=10$. Define $A$ diagonal by $d_i=-i$ for $i=1,\\dots,10$, and $a_i=0$, $c_i=0$. Use $h=0.2$. Take $y^0=[1,1,1,1,1,1,1,1,1,1]^T$. Perform a single Backward Euler step and record the difference.\n- Case $3$ (larger dimension, strong diagonal dominance): Let $n=50$. Define $A$ by $d_i=-5$ for $i=1,\\dots,50$, $a_i=-2$ for $i=1,\\dots,49$, $c_i=-1$ for $i=1,\\dots,49$. Use $h=0.1$. Take $y^0$ with components $y^0_j = j/50$ for $j=0,\\dots,49$. Perform a single Backward Euler step and record the difference.\n- Case $4$ (edge case $h=0$): Let $n=8$. Define $A$ by $d_i=-3$ for $i=1,\\dots,8$, $a_i=1$ for $i=1,\\dots,7$, $c_i=1$ for $i=1,\\dots,7$. Use $h=0$. Take $y^0$ with components $y^0_j = j$ for $j=0,\\dots,7$. Perform a single Backward Euler step and record the difference.\n- Case $5$ (multiple steps, cumulative stability): Let $n=20$. Define $A$ by $d_i=-1.6$ for $i=1,\\dots,20$, $a_i=0.8$ for $i=1,\\dots,19$, $c_i=0.9$ for $i=1,\\dots,19$. Use $h=0.05$. Take $y^0=[1,1,\\dots,1]^T \\in \\mathbb{R}^{20}$. Perform $100$ uniform Backward Euler steps to reach $t=5.0$, and record the difference between the tridiagonal-based result and the dense-based result.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3,r_4,r_5]$), where each $r_i$ is the floating-point difference specified for Case $i$.\n\nThe implementation must avoid any full matrix inversion and must use an algorithm that exploits the tridiagonal structure to achieve linear-time complexity per solve. All computations are in pure mathematics with no physical units. Angles are not involved. Percentages are not involved; any fraction should be represented as a decimal number. The program must be self-contained, require no input, and adhere to the specified execution environment.", "solution": "An Initial Value Problem (IVP) for a system of Ordinary Differential Equations (ODEs) is defined by the differential equation $y'(t) = f(t, y(t))$ for $t \\ge t_0$, along with an initial condition $y(t_0) = y_0$. The problem under consideration is a linear, time-invariant system of the form:\n$$\ny'(t) = A y(t), \\quad y(0) = y^0\n$$\nwhere $y(t) \\in \\mathbb{R}^n$ is the state vector and $A \\in \\mathbb{R}^{n \\times n}$ is a constant, tridiagonal matrix.\n\nTo solve this IVP numerically, we employ a time-stepping method. The problem specifies the use of the Backward Euler (BE) method, which is an implicit method known for its strong stability properties (A-stability). We discretize the time domain with a uniform time step $h$, such that $t^k = k h$ for $k=0, 1, 2, \\dots$. The solution at each time step is denoted by $y^k \\approx y(t^k)$.\n\nThe Backward Euler method approximates the derivative at the next time step, $t^{n+1}$, using a backward difference formula:\n$$\ny'(t^{n+1}) \\approx \\frac{y^{n+1} - y^n}{h}\n$$\nSubstituting this approximation into the ODE evaluated at $t=t^{n+1}$, we get:\n$$\n\\frac{y^{n+1} - y^n}{h} = A y^{n+1}\n$$\nThis equation is implicit because the unknown, $y^{n+1}$, appears on both sides. To solve for $y^{n+1}$, we rearrange the terms:\n$$\ny^{n+1} - h A y^{n+1} = y^n\n$$\nFactoring out $y^{n+1}$ yields a linear system of equations:\n$$\n(I - hA) y^{n+1} = y^n\n$$\nwhere $I$ is the $n \\times n$ identity matrix. At each time step, we must solve this linear system for the unknown vector $y^{n+1}$, given the known vector $y^n$ from the previous step.\n\nThe matrix $A$ is tridiagonal. Let its diagonals be represented by the vectors $d$ (main diagonal, length $n$), $a$ (sub-diagonal, length $n-1$), and $c$ (super-diagonal, length $n-1$). Using $1$-based indexing, the non-zero entries of $A$ are $A_{i,i} = d_i$ for $i=1,\\dots,n$, $A_{i+1,i} = a_i$ for $i=1,\\dots,n-1$, and $A_{i,i+1} = c_i$ for $i=1,\\dots,n-1$.\n\nThe matrix of the linear system to be solved, $M = I - hA$, inherits this tridiagonal structure. Its diagonals are:\n- Main diagonal: $\\tilde{d}_i = 1 - h d_i$ for $i=1,\\dots,n$.\n- Sub-diagonal: $\\tilde{a}_i = -h a_i$ for $i=1,\\dots,n-1$.\n- Super-diagonal: $\\tilde{c}_i = -h c_i$ for $i=1,\\dots,n-1$.\n\nSolving a tridiagonal system can be done much more efficiently than solving a general dense system. A direct inversion of the matrix $M$ would be an $\\mathcal{O}(n^3)$ operation and would destroy the sparse structure, requiring $\\mathcal{O}(n^2)$ memory. Instead, we use the Thomas algorithm (also known as the Tridiagonal Matrix Algorithm or TDMA), which is a specialized form of Gaussian elimination that solves the system in $\\mathcal{O}(n)$ time using $\\mathcal{O}(n)$ memory.\n\nThe Thomas algorithm consists of two phases: forward elimination and backward substitution. Consider the system $M x = y^n$, written component-wise using the diagonals of $M$: $\\tilde{a}_i$ (sub), $\\tilde{d}_i$ (main), and $\\tilde{c}_i$ (super).\n$$\n\\tilde{a}_{i-1} x_{i-1} + \\tilde{d}_i x_i + \\tilde{c}_i x_{i+1} = y^n_i \\quad (\\text{with } \\tilde{a}_0=0, \\tilde{c}_n=0)\n$$\n1.  **Forward Elimination**: The algorithm modifies the super-diagonal coefficients and the right-hand side vector. We compute new coefficients $c'_i$ and $y'_{i}$ (using $0$-based indexing for implementation clarity):\n    - For $i=0$:\n      $$\n      c'_0 = \\frac{\\tilde{c}_0}{\\tilde{d}_0}, \\quad y'_0 = \\frac{y^n_0}{\\tilde{d}_0}\n      $$\n    - For $i=1, \\dots, n-2$:\n      $$\n      c'_i = \\frac{\\tilde{c}_i}{\\tilde{d}_i - \\tilde{a}_{i-1} c'_{i-1}}, \\quad y'_i = \\frac{y^n_i - \\tilde{a}_{i-1} y'_{i-1}}{\\tilde{d}_i - \\tilde{a}_{i-1} c'_{i-1}}\n      $$\n    - For $i=n-1$:\n      $$\n      y'_{n-1} = \\frac{y^n_{n-1} - \\tilde{a}_{n-2} y'_{n-2}}{\\tilde{d}_{n-1} - \\tilde{a}_{n-2} c'_{n-2}}\n      $$\n    This transforms the system into an upper bidiagonal form.\n\n2.  **Backward Substitution**: The solution $x = y^{n+1}$ is found by substituting backwards:\n    - For $i=n-1$:\n      $$\n      x_{n-1} = y'_{n-1}\n      $$\n    - For $i=n-2, \\dots, 0$:\n      $$\n      x_i = y'_i - c'_i x_{i+1}\n      $$\n\nFor each test case, we perform one or more steps of the Backward Euler method. We construct the matrix $M=I-hA$ and its diagonals, and the right-hand side vector $y^n$. We then solve for $y^{n+1}$ using both the implemented Thomas algorithm ($y_{\\text{tri}}^{n+1}$) and a general-purpose dense linear solver ($y_{\\text{dense}}^{n+1}$) provided by a standard library for validation. The final result for each case is the infinity-norm of the difference vector, $\\max_i |(y_{\\text{tri}}^{n+1})_i - (y_{\\text{dense}}^{n+1})_i|$. For the multi-step case, this comparison is performed after the final time step. The matrices in the test cases are chosen such that $I-hA$ is diagonally dominant, ensuring the numerical stability of the Thomas algorithm without pivoting.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the Backward Euler method\n    with a tridiagonal solver.\n    \"\"\"\n\n    def construct_dense_A(n, d_vals, a_vals, c_vals):\n        \"\"\"Constructs the dense n x n tridiagonal matrix A.\"\"\"\n        A = np.zeros((n, n))\n        # Populate main diagonal\n        A += np.diag(d_vals)\n        # Populate sub-diagonal\n        if len(a_vals) > 0:\n            A += np.diag(a_vals, k=-1)\n        # Populate super-diagonal\n        if len(c_vals) > 0:\n            A += np.diag(c_vals, k=1)\n        return A\n\n    def thomas_algorithm(a, d, c, b):\n        \"\"\"\n        Solves a tridiagonal system of equations Ax=b using the Thomas algorithm.\n        a: sub-diagonal (length n-1)\n        d: main diagonal (length n)\n        c: super-diagonal (length n-1)\n        b: right-hand side vector (length n)\n        \n        The algorithm is not performed in-place to avoid side effects.\n        \"\"\"\n        n = len(d)\n        if n == 0:\n            return np.array([])\n        if n == 1:\n            return np.array([b[0] / d[0]])\n\n        # Create copies to avoid modifying original arrays\n        c_prime = np.zeros(n - 1)\n        d_prime = np.zeros(n)\n        x = np.zeros(n)\n\n        # Forward elimination\n        c_prime[0] = c[0] / d[0]\n        d_prime[0] = b[0] / d[0]\n\n        for i in range(1, n - 1):\n            denom = d[i] - a[i - 1] * c_prime[i - 1]\n            c_prime[i] = c[i] / denom\n            d_prime[i] = (b[i] - a[i - 1] * d_prime[i - 1]) / denom\n\n        denom_last = d[n - 1] - a[n - 2] * c_prime[n - 2]\n        d_prime[n - 1] = (b[n-1] - a[n-2] * d_prime[n-2]) / denom_last\n\n        # Backward substitution\n        x[n - 1] = d_prime[n - 1]\n        for i in range(n - 2, -1, -1):\n            x[i] = d_prime[i] - c_prime[i] * x[i + 1]\n\n        return x\n\n    # Define the test cases from the problem statement.\n    # Each tuple: (n, d_func, a_func, c_func, h, y0_func, steps)\n    test_cases = [\n        # Case 1\n        (5, lambda n: np.full(n, -2.0), lambda n: np.full(n - 1, 1.0), lambda n: np.full(n - 1, 1.0), 0.05, \n         lambda n: np.array([1.0] + [0.0]*(n-1)), 1),\n        # Case 2\n        (10, lambda n: -np.arange(1, n + 1, dtype=float), lambda n: np.full(n - 1, 0.0), lambda n: np.full(n - 1, 0.0), 0.2, \n         lambda n: np.ones(n), 1),\n        # Case 3\n        (50, lambda n: np.full(n, -5.0), lambda n: np.full(n - 1, -2.0), lambda n: np.full(n - 1, -1.0), 0.1, \n         lambda n: np.arange(n, dtype=float) / n, 1),\n        # Case 4\n        (8, lambda n: np.full(n, -3.0), lambda n: np.full(n - 1, 1.0), lambda n: np.full(n - 1, 1.0), 0.0,\n         lambda n: np.arange(n, dtype=float), 1),\n        # Case 5\n        (20, lambda n: np.full(n, -1.6), lambda n: np.full(n - 1, 0.8), lambda n: np.full(n - 1, 0.9), 0.05,\n         lambda n: np.ones(n), 100),\n    ]\n\n    results = []\n    for n, d_func, a_func, c_func, h, y0_func, steps in test_cases:\n        # Construct diagonals and initial condition\n        d_A = d_func(n)\n        a_A = a_func(n)\n        c_A = c_func(n)\n        y0 = y0_func(n)\n\n        # Construct dense matrix A for validation\n        A_dense = construct_dense_A(n, d_A, a_A, c_A)\n        \n        # System to solve is (I - hA)y_next = y_prev\n        M_dense = np.eye(n) - h * A_dense\n\n        # Diagonals of M = I - hA\n        # Sub-diagonal mapping: a_M[i] is for row i+1, corresponds to a_A[i]\n        # Super-diagonal mapping: c_M[i] is for row i, corresponds to c_A[i]\n        d_M = 1.0 - h * d_A\n        if n > 1:\n            a_M = -h * a_A\n            c_M = -h * c_A\n        else: # Handle n=1 case\n            a_M = np.array([])\n            c_M = np.array([])\n            \n        y_tri = y0.copy()\n        y_dense = y0.copy()\n\n        for _ in range(steps):\n            # Solve using Thomas algorithm\n            y_tri = thomas_algorithm(a_M, d_M, c_M, y_tri)\n            # Solve using dense solver for validation\n            y_dense = np.linalg.solve(M_dense, y_dense)\n\n        # Calculate the infinity norm of the difference\n        diff = np.max(np.abs(y_tri - y_dense))\n        results.append(diff)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3241515"}]}