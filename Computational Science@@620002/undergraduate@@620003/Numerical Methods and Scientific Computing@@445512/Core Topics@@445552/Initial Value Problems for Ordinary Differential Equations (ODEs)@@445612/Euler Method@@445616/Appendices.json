{"hands_on_practices": [{"introduction": "The best way to understand a new numerical technique is to apply it. This first practice exercise provides a straightforward application of the Euler method formula. By performing a single step, you will see how to use the known slope at a point to project an approximate value for the solution a short distance into the future. [@problem_id:2172233]", "problem": "Consider the initial value problem defined by the first-order ordinary differential equation:\n$$y'(t) = y - t^2$$\nwith the initial condition $y(0) = 1$.\n\nYour task is to approximate the value of the solution at $t = 0.2$ by performing a single step of Euler's method. Use a step size of $h = 0.2$. Present your final answer as a numerical value.", "solution": "We apply one step of the explicit Euler method for the initial value problem $y'(t)=f(t,y)=y-t^{2}$ with $y(0)=1$ and step size $h=0.2$. The Euler update is\n$$\ny_{n+1}=y_{n}+h\\,f(t_{n},y_{n}).\n$$\nWith $t_{0}=0$, $y_{0}=1$, and $h=0.2$, we compute\n$$\nf(t_{0},y_{0})=f(0,1)=1-0^{2}=1,\n$$\nso\n$$\ny_{1}=y_{0}+h\\,f(t_{0},y_{0})=1+0.2\\cdot 1=1.2.\n$$\nThus, the Euler approximation at $t=0.2$ is $y(0.2)\\approx 1.2$.", "answer": "$$\\boxed{1.2}$$", "id": "2172233"}, {"introduction": "The accuracy of the Euler method is highly dependent on the chosen step size, $h$. This exercise demonstrates this crucial concept by asking you to solve the same problem twice with different step sizes. Comparing the results will provide tangible insight into how smaller steps can improve accuracy and how larger steps can sometimes lead to significant errors or even numerical instability. [@problem_id:2172241]", "problem": "Consider the initial value problem (IVP) given by the ordinary differential equation $y'(t) = -2y(t)$ with the initial condition $y(0) = 1$. We wish to approximate the value of $y(1)$ using Euler's method.\n\nLet $A_1$ be the approximation of $y(1)$ obtained using Euler's method with a single step of size $h=1$.\nLet $A_{0.5}$ be the approximation of $y(1)$ obtained using Euler's method with two steps, each of size $h=0.5$.\n\nCalculate the value of the expression $3A_1 - 2A_{0.5}$.", "solution": "We apply explicit Euler's method, which for an IVP $y'(t)=f(t,y)$ with step size $h$ updates as\n$$\ny_{n+1}=y_{n}+h\\,f(t_{n},y_{n}).\n$$\nHere $f(t,y)=-2y$, so\n$$\ny_{n+1}=y_{n}+h(-2y_{n})=y_{n}(1-2h).\n$$\n\nSingle step with $h=1$ from $t_{0}=0$ to $t_{1}=1$:\n$$\ny_{1}=y_{0}(1-2\\cdot 1)=1\\cdot(-1)=-1,\n$$\nso $A_{1}=-1$.\n\nTwo steps with $h=0.5$ from $t_{0}=0$ to $t_{2}=1$:\nFirst step to $t_{1}=0.5$:\n$$\ny_{1}=y_{0}(1-2\\cdot 0.5)=1\\cdot(1-1)=0.\n$$\nSecond step to $t_{2}=1$:\n$$\ny_{2}=y_{1}(1-2\\cdot 0.5)=0\\cdot(1-1)=0,\n$$\nso $A_{0.5}=0$.\n\nTherefore,\n$$\n3A_{1}-2A_{0.5}=3(-1)-2(0)=-3.\n$$", "answer": "$$\\boxed{-3}$$", "id": "2172241"}, {"introduction": "While the Euler method is fundamental, its first-order accuracy, with error proportional to $h$, is often insufficient for practical applications. This advanced practice introduces a powerful idea: Richardson extrapolation, which combines results from two different step sizes to create a more accurate, second-order estimate. This coding challenge bridges the gap from manual calculation to computational problem-solving, showing how we can systematically improve upon a basic numerical method. [@problem_id:3226253]", "problem": "Consider the initial value problem (IVP) for an ordinary differential equation (ODE) given by $y'(t) = f(t, y(t))$ with initial condition $y(t_0) = y_0$. The forward Euler method arises from the fundamental definition of the derivative as the limit of a finite difference and the use of a first-order Taylor expansion. In practice, the forward Euler method updates an approximation $y_n$ to the exact solution $y(t_n)$ by advancing the solution in steps of size $h$ starting at $t_0$ and ending at a specified final time $T$, yielding an approximation at $T$ that we denote $y_h(T)$. The global discretization error of forward Euler is known to depend linearly on the step size $h$ under standard regularity assumptions on $f$ and $y(t)$.\n\nYour task is to:\n1. Implement a function that computes $y_h(T)$ using the forward Euler method for any given function $f(t,y)$, initial condition $y_0$, initial time $t_0$, final time $T$, and uniform step size $h$, assuming $T - t_0$ is an integer multiple of $h$.\n2. Assume the Euler methodâ€™s approximation at $T$ has an asymptotic error expansion of the form $y_h(T) = y(T) + C h + D h^2 + \\mathcal{O}(h^3)$ for constants $C$ and $D$ that depend on $f$ and the solution but not on $h$. Using the outputs $y_h(T)$ and $y_{h/2}(T)$, derive a linear combination of these two approximations, with constant weights independent of $h$, that cancels the leading $\\mathcal{O}(h)$ error term and yields an $\\mathcal{O}(h^2)$-accurate estimate of $y(T)$. Then implement this Richardson extrapolation estimator in code.\n3. For each test case below, compute the absolute error $\\lvert y_{\\text{extrap}}(T) - y(T) \\rvert$, where $y_{\\text{extrap}}(T)$ is your extrapolated estimate and $y(T)$ is the exact solution at time $T$.\n\nUse the following test suite. In all cases, take $t_0 = 0$ and use the provided $h$ such that $(T - t_0)/h$ is an integer:\n\n- Test 1 (happy path, linear homogeneous ODE): $f(t,y) = y$, $y_0 = 1$, $T = 1$, $h = 0.2$. The exact solution is $y(t) = e^{t}$ evaluated at $t = T$.\n- Test 2 (linear nonhomogeneous ODE): $f(t,y) = y + t$, $y_0 = 0$, $T = 2$, $h = 0.4$. The exact solution is $y(t) = e^{t} - t - 1$ evaluated at $t = T$.\n- Test 3 (nonlinear logistic growth): $f(t,y) = r y \\left(1 - \\frac{y}{K}\\right)$ with parameters $r = 1$ and $K = 10$, $y_0 = 1$, $T = 3$, $h = 0.5$. The exact solution is $y(t) = \\frac{K}{1 + A e^{-r t}}$ with $A = \\frac{K - y_0}{y_0}$, evaluated at $t = T$.\n- Test 4 (edge case, zero derivative): $f(t,y) = 0$, $y_0 = 3$, $T = 1$, $h = 0.5$. The exact solution is the constant function $y(t) = 3$ evaluated at $t = T$.\n\nYour program should:\n- Implement the forward Euler method to compute $y_h(T)$ and $y_{h/2}(T)$ for each test case.\n- Implement the derived Richardson extrapolation estimator using the two approximations $y_h(T)$ and $y_{h/2}(T)$ to obtain an $\\mathcal{O}(h^2)$ estimate at $T$.\n- Compute and record the absolute error for each test case, rounded to ten decimal places.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[0.0123456789,0.0000001234,0.0012340000,0.0000000000]\"), where the entries are the absolute errors for Test 1 through Test 4, each rounded to ten decimal places.", "solution": "The problem is assessed to be valid.\n\n### Step 1: Extract Givens\n- **Problem Type**: Initial Value Problem (IVP) for an Ordinary Differential Equation (ODE).\n- **ODE Form**: $y'(t) = f(t, y(t))$.\n- **Initial Condition**: $y(t_0) = y_0$.\n- **Numerical Method**: Forward Euler method, where $y_{n+1} = y_n + h f(t_n, y_n)$.\n- **Approximation at final time $T$**: $y_h(T)$.\n- **Asymptotic Error Expansion**: $y_h(T) = y(T) + C h + D h^2 + \\mathcal{O}(h^3)$.\n- **Constraint**: $T - t_0$ is an integer multiple of the step size $h$.\n- **Task 1**: Implement a function for the forward Euler method to compute $y_h(T)$.\n- **Task 2**: Derive and implement a Richardson extrapolation estimator for $y(T)$ that is $\\mathcal{O}(h^2)$-accurate, using $y_h(T)$ and $y_{h/2}(T)$.\n- **Task 3**: Compute the absolute error $\\lvert y_{\\text{extrap}}(T) - y(T) \\rvert$ for four test cases.\n- **Initial Time for all cases**: $t_0 = 0$.\n\n- **Test Case 1**:\n  - $f(t,y) = y$\n  - $y_0 = 1$\n  - $T = 1$\n  - $h = 0.2$\n  - Exact solution: $y(t) = e^{t}$\n\n- **Test Case 2**:\n  - $f(t,y) = y + t$\n  - $y_0 = 0$\n  - $T = 2$\n  - $h = 0.4$\n  - Exact solution: $y(t) = e^{t} - t - 1$\n\n- **Test Case 3**:\n  - $f(t,y) = r y \\left(1 - \\frac{y}{K}\\right)$ with $r = 1, K = 10$\n  - $y_0 = 1$\n  - $T = 3$\n  - $h = 0.5$\n  - Exact solution: $y(t) = \\frac{K}{1 + A e^{-r t}}$ with $A = \\frac{K - y_0}{y_0}$\n\n- **Test Case 4**:\n  - $f(t,y) = 0$\n  - $y_0 = 3$\n  - $T = 1$\n  - $h = 0.5$\n  - Exact solution: $y(t) = 3$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective.\n1.  **Scientific or Factual Soundness**: The problem is built upon fundamental concepts in numerical analysis for ODEs. The forward Euler method, its error analysis, and Richardson extrapolation are standard and mathematically sound techniques. The ODEs provided are classic examples used in teaching and research. The problem is free of any scientific or factual errors.\n2.  **Well-Posed**: The problem is well-posed. For each test case, the function $f(t,y)$ is sufficiently smooth (Lipschitz continuous in $y$), which guarantees the existence and uniqueness of a solution to the IVP. The task is clearly defined, and all necessary data (initial conditions, parameters, time intervals) are provided.\n3.  **Objective**: The language is precise and unbiased. The tasks are quantitative and require specific calculations, leaving no room for subjective interpretation.\n4.  **Completeness**: The problem is self-contained. It specifies the ODEs, initial conditions, step sizes, final times, and the exact solutions for error comparison. The constraint that $(T - t_0)/h$ is an integer simplifies implementation and avoids ambiguity.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A complete solution will be provided.\n\n### Principle-Based Solution\nThe solution is developed in three stages: first, the implementation of the forward Euler method; second, the derivation of the Richardson extrapolation formula; and third, the application of these methods to the specified test cases to compute the required errors.\n\n**1. The Forward Euler Method**\nThe forward Euler method is a first-order numerical procedure for solving an initial value problem of the form $y'(t) = f(t, y(t))$ with $y(t_0) = y_0$. It approximates the continuous solution $y(t)$ at discrete time points $t_n = t_0 + n h$, where $h$ is the step size. The method is derived from the first-order Taylor expansion of $y(t_{n+1})$ around $t_n$:\n$y(t_{n+1}) = y(t_n) + h y'(t_n) + \\mathcal{O}(h^2) = y(t_n) + h f(t_n, y(t_n)) + \\mathcal{O}(h^2)$.\nBy ignoring the $\\mathcal{O}(h^2)$ term, we obtain the iterative formula for the approximation $y_n \\approx y(t_n)$:\n$$y_{n+1} = y_n + h f(t_n, y_n)$$\nStarting with the initial condition $y_0$, we can apply this formula iteratively for $n = 0, 1, 2, \\dots, N-1$ where $N = (T-t_0)/h$, to find an approximation $y_N \\approx y(T)$. This defines the function $y_h(T)$.\n\n**2. Richardson Extrapolation**\nRichardson extrapolation is a general technique for improving the accuracy of a numerical approximation. We are given that the approximation $y_h(T)$ from the forward Euler method has an asymptotic error expansion:\n$$y_h(T) = y(T) + C h + D h^2 + \\mathcal{O}(h^3)$$\nHere, $y(T)$ is the exact solution, and $C$ and $D$ are constants that depend on the function $f$ and its derivatives but not on the step size $h$.\n\nIf we compute the approximation again with a halved step size, $h/2$, the formula becomes:\n$$y_{h/2}(T) = y(T) + C \\left(\\frac{h}{2}\\right) + D \\left(\\frac{h}{2}\\right)^2 + \\mathcal{O}(h^3)$$\n$$y_{h/2}(T) = y(T) + \\frac{1}{2} C h + \\frac{1}{4} D h^2 + \\mathcal{O}(h^3)$$\nOur goal is to find a linear combination of $y_h(T)$ and $y_{h/2}(T)$, which we denote $y_{\\text{extrap}}(T)$, that provides a more accurate estimate of $y(T)$. Let $y_{\\text{extrap}}(T) = \\alpha y_h(T) + \\beta y_{h/2}(T)$. Substituting the error expansions:\n$$y_{\\text{extrap}}(T) = \\alpha \\left(y(T) + C h + D h^2\\right) + \\beta \\left(y(T) + \\frac{1}{2} C h + \\frac{1}{4} D h^2\\right) + \\mathcal{O}(h^3)$$\n$$y_{\\text{extrap}}(T) = (\\alpha + \\beta) y(T) + \\left(\\alpha + \\frac{\\beta}{2}\\right) C h + \\left(\\alpha + \\frac{\\beta}{4}\\right) D h^2 + \\mathcal{O}(h^3)$$\nTo obtain an $\\mathcal{O}(h^2)$-accurate estimate for $y(T)$, we require the coefficient of $y(T)$ to be $1$ and the coefficient of the leading error term, $Ch$, to be $0$. This gives a system of two linear equations for $\\alpha$ and $\\beta$:\n1. $\\alpha + \\beta = 1$\n2. $\\alpha + \\frac{\\beta}{2} = 0$\nFrom equation (2), we find $\\alpha = -\\beta/2$. Substituting this into equation (1) yields $-\\beta/2 + \\beta = 1$, which simplifies to $\\beta/2 = 1$, so $\\beta = 2$. Consequently, $\\alpha = -1$.\nThe extrapolated estimator is therefore:\n$$y_{\\text{extrap}}(T) = 2 y_{h/2}(T) - y_h(T)$$\nLet's verify the error of this new estimate:\n$$y_{\\text{extrap}}(T) - y(T) = (2 y_{h/2}(T) - y_h(T)) - y(T)$$\n$$= \\left(2\\left(y(T) + \\frac{1}{2}Ch + \\frac{1}{4}Dh^2\\right) - \\left(y(T) + Ch + Dh^2\\right)\\right) - y(T) + \\mathcal{O}(h^3)$$\n$$= (2y(T) + Ch + \\frac{1}{2}Dh^2) - y(T) - Ch - Dh^2 - y(T) + \\mathcal{O}(h^3)$$\n$$= (2-1-1)y(T) + (1-1)Ch + (\\frac{1}{2}-1)Dh^2 + \\mathcal{O}(h^3) = -\\frac{1}{2} D h^2 + \\mathcal{O}(h^3)$$\nThe error is indeed of order $h^2$, so the method successfully eliminates the leading error term.\n\n**3. Computational Procedure**\nFor each of the four test cases, the following algorithm is applied:\n1.  Define the function $f(t,y)$, initial conditions $y_0, t_0$, final time $T$, and step size $h$.\n2.  Implement a function `forward_euler(f, y0, t0, T, h)` that performs the iterative Euler updates and returns the final approximation at time $T$. The number of steps, $N$, is calculated as the integer `(T - t0) / h`.\n3.  Compute the approximation with the given step size $h$: $A_h = \\text{forward_euler}(f, y_0, t_0, T, h)$.\n4.  Compute the approximation with the halved step size $h/2$: $A_{h/2} = \\text{forward_euler}(f, y_0, t_0, T, h/2)$.\n5.  Calculate the Richardson extrapolated value: $y_{\\text{extrap}}(T) = 2 A_{h/2} - A_h$.\n6.  Calculate the exact solution $y(T)$ using the provided formula for the specific test case.\n7.  Compute the absolute error: $E = \\lvert y_{\\text{extrap}}(T) - y(T) \\rvert$.\n8.  The final result for the test case is this error, rounded to ten decimal places. This process is repeated for all test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes absolute errors for Richardson-extrapolated Euler method solutions\n    for a suite of ODE test cases.\n    \"\"\"\n\n    def forward_euler(f, y0, t0, T, h):\n        \"\"\"\n        Computes the solution of an IVP y'(t) = f(t, y) with y(t0) = y0 at time T\n        using the forward Euler method with step size h.\n        \"\"\"\n        t = t0\n        y = y0\n        \n        # The problem statement guarantees (T - t0) / h is an integer.\n        # Using int() directly is safe, but rounding is more robust for floats.\n        num_steps = int(round((T - t0) / h))\n\n        for _ in range(num_steps):\n            y = y + h * f(t, y)\n            t = t + h\n        \n        return y\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"f\": lambda t, y: y,\n            \"y0\": 1.0,\n            \"t0\": 0.0,\n            \"T\": 1.0,\n            \"h\": 0.2,\n            \"exact_sol\": lambda t: np.exp(t)\n        },\n        {\n            \"f\": lambda t, y: y + t,\n            \"y0\": 0.0,\n            \"t0\": 0.0,\n            \"T\": 2.0,\n            \"h\": 0.4,\n            \"exact_sol\": lambda t: np.exp(t) - t - 1.0\n        },\n        {\n            \"f\": lambda t, y: 1.0 * y * (1.0 - y / 10.0), # r=1, K=10\n            \"y0\": 1.0,\n            \"t0\": 0.0,\n            \"T\": 3.0,\n            \"h\": 0.5,\n            \"exact_sol\": lambda t: 10.0 / (1.0 + ((10.0 - 1.0) / 1.0) * np.exp(-1.0 * t))\n        },\n        {\n            \"f\": lambda t, y: 0.0,\n            \"y0\": 3.0,\n            \"t0\": 0.0,\n            \"T\": 1.0,\n            \"h\": 0.5,\n            \"exact_sol\": lambda t: 3.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        f = case[\"f\"]\n        y0 = case[\"y0\"]\n        t0 = case[\"t0\"]\n        T = case[\"T\"]\n        h = case[\"h\"]\n        exact_sol_func = case[\"exact_sol\"]\n\n        # 1. Compute approximations with step sizes h and h/2\n        y_h = forward_euler(f, y0, t0, T, h)\n        y_h_half = forward_euler(f, y0, t0, T, h / 2.0)\n\n        # 2. Apply Richardson extrapolation\n        y_extrap = 2.0 * y_h_half - y_h\n\n        # 3. Compute the exact solution\n        y_exact = exact_sol_func(T)\n        \n        # 4. Compute the absolute error\n        abs_error = abs(y_extrap - y_exact)\n        \n        results.append(abs_error)\n\n    # Format the results as strings rounded to 10 decimal places\n    # The f-string formatting ensures trailing zeros as in the example.\n    # The rounding prior to formatting correctly handles cases near the rounding boundary.\n    results_str = [f\"{round(res, 10):.10f}\" for res in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```", "id": "3226253"}]}