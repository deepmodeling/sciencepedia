{"hands_on_practices": [{"introduction": "At the heart of any adaptive solver is the algorithm that decides whether to increase or decrease the step size, $h$. This decision is governed by the fundamental relationship between the local truncation error, $\\epsilon$, and the method's order, $p$: $\\epsilon \\propto h^{p+1}$. This exercise [@problem_id:1659045] provides a direct application of this principle, challenging you to calculate the optimal new step size to precisely meet a desired error tolerance.", "problem": "In the field of computational science, numerical methods are used to approximate solutions to Ordinary Differential Equations (ODEs). A crucial aspect of modern solvers is adaptive step-size control, which adjusts the integration step size, $h$, to maintain a desired level of accuracy while minimizing computational cost.\n\nConsider a numerical integration method of order $p$. The local truncation error, $\\epsilon$, incurred in a single step is known to be proportional to the step size raised to the power of $(p+1)$. This relationship can be expressed as $\\epsilon \\propto h^{p+1}$.\n\nAn engineer is using such a solver with a method of order $p=4$. The solver is configured with a constant target tolerance, $tol$, for the local error. After taking a step with size $h_{old}$, the error estimation module reports a local error of $\\epsilon_{old} = \\frac{1}{2} tol$. To prepare for the next integration step, the control algorithm must propose a new step size, $h_{new}$. The new step size is chosen such that the predicted error for the next step, $\\epsilon_{new}$, would be precisely equal to the target tolerance, $tol$.\n\nAssuming the proportionality constant relating the error to the step size does not change significantly between these two consecutive steps, determine the expression for the proposed new step size $h_{new}$ in terms of the old step size $h_{old}$.", "solution": "The local truncation error for a method of order $p$ satisfies $\\epsilon = C h^{p+1}$, where $C$ is an approximately constant proportionality factor between consecutive steps. For the old step,\n$$\n\\epsilon_{old} = C h_{old}^{p+1} = \\frac{1}{2} tol.\n$$\nFor the proposed new step, we require\n$$\n\\epsilon_{new} = C h_{new}^{p+1} = tol.\n$$\nTaking the ratio,\n$$\n\\frac{\\epsilon_{new}}{\\epsilon_{old}} = \\frac{C h_{new}^{p+1}}{C h_{old}^{p+1}} = \\left(\\frac{h_{new}}{h_{old}}\\right)^{p+1} = \\frac{tol}{\\frac{1}{2} tol} = 2.\n$$\nThus,\n$$\n\\left(\\frac{h_{new}}{h_{old}}\\right)^{p+1} = 2 \\quad \\Rightarrow \\quad \\frac{h_{new}}{h_{old}} = 2^{\\frac{1}{p+1}}.\n$$\nFor $p=4$, we obtain\n$$\nh_{new} = 2^{\\frac{1}{5}} h_{old}.\n$$", "answer": "$$\\boxed{2^{\\frac{1}{5}} h_{old}}$$", "id": "1659045"}, {"introduction": "While the principle of step-size adjustment is straightforward, a practical solver needs an efficient way to estimate the local error. Embedded Runge-Kutta methods, such as the Bogacki-Shampine pair, achieve this by generating two approximations of different orders from a shared set of computations. In this hands-on coding exercise [@problem_id:3203842], you will derive the error estimator for such a pair and build a complete adaptive solver, translating theoretical concepts into a functional numerical tool.", "problem": "You are asked to derive and implement an adaptive step-size control mechanism based on an embedded Runge–Kutta (RK) pair of orders $(p,p-1)$, and to test the resulting local error estimator on the scalar initial value problem $y'(t)=-10\\,y(t)$ with varying relative and absolute tolerances. The goal is to demonstrate, from first principles, how the local error estimator arises for an embedded pair and how it governs step acceptance and step-size selection.\n\nStart from the initial value problem $y'(t)=f(t,y(t))$ with $y(t_0)=y_0$, and the definition of explicit Runge–Kutta methods via the Butcher tableau and stage computations. An embedded pair shares the same stages but provides two distinct output formulas of different orders using two sets of weights. The local error estimator for the higher-order output is obtained by comparing the two outputs and analyzing the truncation error orders.\n\nYou must:\n\n- Derive, from the definitions of explicit Runge–Kutta methods and Taylor expansions of the exact solution, the local error estimator for an embedded RK pair $(p,p-1)$ as the difference between the two embedded outputs, showing its order and explaining why it controls step acceptance for the higher-order solution.\n- Specialize your implementation to a concrete embedded pair with $(p,p-1)=(3,2)$, namely the Bogacki–Shampine $3(2)$ method, using the following shared stages and weights:\n  - Stages:\n    - $k_1=f(t,y)$,\n    - $k_2=f(t+\\frac{1}{2}h,\\;y+\\frac{1}{2}h\\,k_1)$,\n    - $k_3=f(t+\\frac{3}{4}h,\\;y+\\frac{3}{4}h\\,k_2)$,\n    - $k_4=f(t+h,\\;y_{\\text{high}})$, where $y_{\\text{high}}$ is defined below.\n  - Higher-order ($p=3$) output:\n    $$y_{\\text{high}}=y+h\\left(\\frac{2}{9}k_1+\\frac{1}{3}k_2+\\frac{4}{9}k_3\\right).$$\n  - Lower-order ($p-1=2$) embedded output:\n    $$y_{\\text{low}}=y+h\\left(\\frac{7}{24}k_1+\\frac{1}{4}k_2+\\frac{1}{3}k_3+\\frac{1}{8}k_4\\right).$$\n- Use the scalar local error estimate $e=y_{\\text{high}}-y_{\\text{low}}$ and the standard scalar scaling with absolute and relative tolerances:\n  $$\\varepsilon=\\frac{|e|}{ATOL+RTOL\\cdot |y_{\\text{high}}|},$$\n  and accept a step if $\\varepsilon\\leq 1$.\n- Use the order-based step-size update for the accepted or rejected step, with $p=3$, safety factor $0.9$, minimum factor $0.2$, and maximum factor $5.0$:\n  $$h_{\\text{new}}=h\\cdot \\min\\!\\left(5.0,\\;\\max\\!\\left(0.2,\\;0.9\\cdot \\varepsilon^{-\\frac{1}{3}}\\right)\\right).$$\n- Integrate the scalar problem $y'(t)=-10\\,y(t)$, $y(0)=y_0$ on the interval $[0,1]$ using initial step size $h_0=10^{-2}$ and a minimum step size $h_{\\min}=10^{-12}$.\n\nTest Suite:\nImplement the above and run the solver for the following five test cases to probe different facets of the control mechanism:\n1. $RTOL=10^{-3}$, $ATOL=0$, $y_0=1$.\n2. $RTOL=0$, $ATOL=10^{-6}$, $y_0=1$.\n3. $RTOL=10^{-6}$, $ATOL=10^{-12}$, $y_0=1$.\n4. $RTOL=10^{-3}$, $ATOL=0$, $y_0=10^{-6}$.\n5. $RTOL=10^{-2}$, $ATOL=10^{-8}$, $y_0=1$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is the total number of accepted steps (an integer) needed to reach $t=1$ for the corresponding test case in the order listed above, for example:\n$$[n_1,n_2,n_3,n_4,n_5].$$\nNo other output is permitted.", "solution": "The problem requires the derivation and implementation of an adaptive step-size control algorithm for solving an initial value problem (IVP). The method is based on an embedded Runge-Kutta (RK) pair. We will first validate the problem statement and then provide a detailed derivation followed by the algorithmic implementation.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   **IVP**: $y'(t)=f(t,y(t))$ with initial condition $y(t_0)=y_0$.\n*   **Method**: Embedded Runge-Kutta pair of orders $(p, p-1)$.\n*   **Specific Method**: Bogacki–Shampine $3(2)$ pair, meaning $p=3$.\n*   **Stages**:\n    *   $k_1=f(t,y)$\n    *   $k_2=f(t+\\frac{1}{2}h, y+\\frac{1}{2}h k_1)$\n    *   $k_3=f(t+\\frac{3}{4}h, y+\\frac{3}{4}h k_2)$\n    *   $k_4=f(t+h, y_{\\text{high}})$\n*   **Outputs**:\n    *   Higher-order ($p=3$): $y_{\\text{high}}=y+h(\\frac{2}{9}k_1+\\frac{1}{3}k_2+\\frac{4}{9}k_3)$.\n    *   Lower-order ($p-1=2$): $y_{\\text{low}}=y+h(\\frac{7}{24}k_1+\\frac{1}{4}k_2+\\frac{1}{3}k_3+\\frac{1}{8}k_4)$.\n*   **Error Estimation**:\n    *   Local error estimate: $e=y_{\\text{high}}-y_{\\text{low}}$.\n    *   Scaled error: $\\varepsilon=\\frac{|e|}{ATOL+RTOL\\cdot |y_{\\text{high}}|}$.\n    *   Acceptance criterion: $\\varepsilon\\leq 1$.\n*   **Step-Size Control**:\n    *   Update formula: $h_{\\text{new}}=h\\cdot \\min(5.0, \\max(0.2, 0.9\\cdot \\varepsilon^{-\\frac{1}{3}}))$.\n    *   Safety factor: $0.9$.\n    *   Min/max factors: $0.2$, $5.0$.\n*   **Test Problem**:\n    *   ODE: $y'(t)=-10 y(t)$.\n    *   Initial condition: $y(0)=y_0$.\n    *   Integration interval: $[0,1]$.\n    *   Initial step size: $h_0=10^{-2}$.\n    *   Minimum step size: $h_{\\min}=10^{-12}$.\n*   **Test Suite**: Five cases with varying $RTOL$, $ATOL$, and $y_0$.\n    1.  $RTOL=10^{-3}$, $ATOL=0$, $y_0=1$.\n    2.  $RTOL=0$, $ATOL=10^{-6}$, $y_0=1$.\n    3.  $RTOL=10^{-6}$, $ATOL=10^{-12}$, $y_0=1$.\n    4.  $RTOL=10^{-3}$, $ATOL=0$, $y_0=10^{-6}$.\n    5.  $RTOL=10^{-2}$, $ATOL=10^{-8}$, $y_0=1$.\n*   **Required Output**: Integer array of the total number of accepted steps for each test case.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is scientifically grounded, a standard topic in numerical analysis. It is well-posed, with all necessary data and parameters provided for a deterministic algorithm. The language is objective and precise. The specified Bogacki-Shampine method and the adaptive control strategy are standard and consistent with established literature. The problem does not violate any of the invalidity criteria.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. A full solution will be provided.\n\n### Derivation and Algorithmic Design\n\n**1. Local Error Estimation for Embedded Runge-Kutta Methods**\n\nConsider an initial value problem $y'(t) = f(t, y(t))$ with $y(t_n) = y_n$. Let $y(t_n+h)$ be the exact solution at time $t_n+h$. A numerical one-step method of order $p$ produces an approximation $y_{n+1}$ such that its local truncation error (LTE), the error incurred in a single step, is of order $p+1$. That is,\n$$ LTE = y(t_n+h) - y_{n+1} = \\mathcal{O}(h^{p+1}) $$\nAn embedded Runge-Kutta pair consists of two methods that share the same function evaluations (stages) but have different orders of accuracy. Let the pair have orders $p$ and $p-1$. This provides two approximations to the solution at the end of a step of size $h$:\n1.  A higher-order approximation, $y_{\\text{high}}$, from a method of order $p$.\n2.  A lower-order approximation, $y_{\\text{low}}$, from a method of order $p-1$.\n\nLet's analyze their relationship to the exact solution, $y(t_n+h)$, assuming the step starts from the exact value $y_n = y(t_n)$. From the definition of order, we can write the local truncation errors as Taylor series in $h$:\n$$ y_{\\text{high}} = y(t_n+h) - C_{p+1}h^{p+1} - \\mathcal{O}(h^{p+2}) $$\n$$ y_{\\text{low}} = y(t_n+h) - C_{p}h^{p} - \\mathcal{O}(h^{p+1}) $$\nHere, $C_{p}$ and $C_{p+1}$ are coefficients related to the principal error terms and depend on the derivatives of $f$. We assume $C_p \\neq 0$.\n\nThe local error estimator, $e$, is defined as the difference between these two numerical solutions:\n$$ e = y_{\\text{high}} - y_{\\text{low}} $$\nSubstituting the expressions above:\n$$ e = (y(t_n+h) - C_{p+1}h^{p+1} - \\dots) - (y(t_n+h) - C_{p}h^{p} - \\dots) $$\n$$ e = C_{p}h^{p} - C_{p+1}h^{p+1} - \\mathcal{O}(h^{p+2}) $$\nFor a sufficiently small step size $h$, the term $C_{p}h^{p}$ dominates. Therefore,\n$$ e \\approx C_{p}h^{p} $$\nThis shows that the difference $e$ provides an estimate of the leading term of the local truncation error of the lower-order method, $y_{\\text{low}}$. Its magnitude is of order $p$, i.e., $|e| = \\mathcal{O}(h^p)$.\n\nIn a strategy called \"local extrapolation\", we use this error estimate to control the step size, but we advance the solution using the more accurate higher-order approximation, $y_{n+1} = y_{\\text{high}}$. The estimate $e$ serves as a computable proxy for the true, but unknown, local error of $y_{\\text{high}}$.\n\n**2. Step-Size Control Mechanism**\n\nThe goal of adaptive step-size control is to adjust the step size $h$ so that an estimate of the local error per step matches a prescribed tolerance. Let the desired error tolerance for a step be $S$. We have an estimate of the error for the current step $h$, which is $|e| \\approx |C_p|h^p$. We want to find a new step size, $h_{\\text{new}}$, such that the corresponding error, $|e_{\\text{new}}|$, satisfies $|e_{\\text{new}}| \\approx S$.\nAssuming the coefficient $C_p$ does not change significantly between steps, we have:\n$$ |e_{\\text{new}}| \\approx |C_p|h_{\\text{new}}^p $$\nTaking the ratio of the desired and current errors:\n$$ \\frac{|e_{\\text{new}}|}{|e|} \\approx \\frac{|C_p|h_{\\text{new}}^p}{|C_p|h^p} = \\left(\\frac{h_{\\text{new}}}{h}\\right)^p $$\nSetting $|e_{\\text{new}}| = S$, we get:\n$$ \\left(\\frac{h_{\\text{new}}}{h}\\right)^p \\approx \\frac{S}{|e|} \\implies h_{\\text{new}} \\approx h \\left(\\frac{S}{|e|}\\right)^{1/p} $$\nThe problem defines a mixed absolute and relative tolerance scheme. The tolerance for the step is scaled by the magnitude of the solution:\n$$ S = ATOL + RTOL \\cdot |y_{\\text{high}}| $$\nwhere $ATOL$ and $RTOL$ are the absolute and relative tolerances, respectively. The use of $|y_{\\text{high}}|$ is a standard choice for the scaling factor.\nThe scaled error, $\\varepsilon$, is defined as the ratio of the error estimate to the tolerance:\n$$ \\varepsilon = \\frac{|e|}{S} $$\nA step is considered successful if $\\varepsilon \\leq 1$. Substituting this into the step-size update formula gives:\n$$ h_{\\text{new}} \\approx h \\left(\\frac{1}{\\varepsilon}\\right)^{1/p} = h \\cdot \\varepsilon^{-1/p} $$\nFor reliable and smooth performance, this theoretical formula is modified with a safety factor and limiters on how much the step size can change. The problem specifies the formula:\n$$ h_{\\text{new}}=h\\cdot \\min(\\text{max\\_factor}, \\max(\\text{min\\_factor}, \\text{safety\\_factor} \\cdot \\varepsilon^{-1/p})) $$\nWith the given parameters for the Bogacki-Shampine 3(2) method ($p=3$, safety factor $0.9$, min factor $0.2$, max factor $5.0$), this becomes:\n$$ h_{\\text{new}}=h\\cdot \\min(5.0, \\max(0.2, 0.9 \\cdot \\varepsilon^{-1/3})) $$\nIf a step is accepted ($\\varepsilon \\leq 1$), the solution is advanced to $t_{n+1} = t_n+h$ with $y_{n+1} = y_{\\text{high}}$, and the next step is attempted with a size determined by $h_{\\text{new}}$. If a step is rejected ($\\varepsilon > 1$), the solution is not advanced, and the step is retried from the same point $(t_n, y_n)$ but with a smaller step size given by $h_{\\text{new}}$.\n\n**3. Implementation for the Bogacki–Shampine 3(2) Method**\n\nThe overall algorithm to integrate from $t=0$ to $t=1$ is as follows:\n1.  Initialize time $t=0$, solution $y=y_0$, and step size $h=h_0$. The problem specifies $h_0=10^{-2}$.\n2.  The Bogacki-Shampine method exhibits the First Same As Last (FSAL) property. The last stage computed in a step, $k_4=f(t_n+h, y_{\\text{high}})$, is identical to the first stage, $k_1 = f(t_{n+1}, y_{n+1})$, of the subsequent step if that step is accepted and starts from $t_{n+1}=t_n+h, y_{n+1}=y_{\\text{high}}$. We can exploit this to save one function evaluation per accepted step. We begin by computing an initial $k_1 = f(t,y)$.\n3.  Loop until $t \\geq 1$:\n    a. Check if the current step $h$ would overshoot the final time $t=1$. If so, set $h = 1-t$.\n    b. Using the current $(t, y, h)$ and the $k_1$ from the previous accepted step (or the initial one), compute the stages $k_2, k_3$.\n    c. Compute the higher-order solution $y_{\\text{high}}$.\n    d. Compute the stage $k_4$ using $y_{\\text{high}}$. This is the potential $k_1$ for the next step.\n    e. Compute the lower-order solution $y_{\\text{low}}$.\n    f. Calculate the error estimate $e = y_{\\text{high}} - y_{\\text{low}}$.\n    g. Calculate the scaled error $\\varepsilon = |e| / (ATOL + RTOL \\cdot |y_{\\text{high}}|)$.\n    h. If $\\varepsilon \\leq 1$ (step accepted):\n        i. Increment the accepted step counter.\n        ii. Advance time: $t \\leftarrow t+h$.\n        iii. Update solution: $y \\leftarrow y_{\\text{high}}$.\n        iv. For the next step, set its first stage $k_1 \\leftarrow k_4$ (FSAL).\n        v. Calculate the new step size $h_{\\text{new}}$ using the control formula, and set $h \\leftarrow \\max(h_{\\min}, h_{\\text{new}})$.\n    i. If $\\varepsilon > 1$ (step rejected):\n        i. Do not update $t$, $y$, or the step counter.\n        ii. The $k_1$ for the retry remains unchanged as it only depends on the unchanged $(t,y)$.\n        iii. Calculate a new, smaller step size $h_{\\text{new}}$ and set $h \\leftarrow \\max(h_{\\min}, h_{\\text{new}})$. The loop then retries the step from the same point with this smaller $h$.\n4.  Return the total count of accepted steps.\n\nThis algorithm provides a robust method for solving the specified test problem across various tolerance settings.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef adaptive_bs32_solver(f, t0, y0, t_final, h0, h_min, rtol, atol):\n    \"\"\"\n    Implements an adaptive step-size Runge-Kutta solver using the\n    Bogacki-Shampine 3(2) embedded pair.\n\n    Args:\n        f (callable): The function defining the ODE, y' = f(t, y).\n        t0 (float): Initial time.\n        y0 (float): Initial value.\n        t_final (float): Final time for integration.\n        h0 (float): Initial step size.\n        h_min (float): Minimum allowed step size.\n        rtol (float): Relative tolerance.\n        atol (float): Absolute tolerance.\n\n    Returns:\n        int: The total number of accepted steps.\n    \"\"\"\n    t = t0\n    y = y0\n    h = h0\n    \n    accepted_steps = 0\n    \n    # Coefficients for Bogacki-Shampine 3(2) method\n    c2, a21 = 1/2, 1/2\n    c3, a31, a32 = 3/4, 0, 3/4\n    \n    b_high = np.array([2/9, 1/3, 4/9, 0])\n    b_low = np.array([7/24, 1/4, 1/3, 1/8])\n    \n    # Safety factors and step-size limits\n    safety_factor = 0.9\n    min_factor = 0.2\n    max_factor = 5.0\n    p = 3 # Order of the higher-order method\n\n    # FSAL: k1 for the first step is computed here.\n    k1 = f(t, y)\n\n    while t < t_final:\n        is_last_step = False\n        if t + h > t_final:\n            h = t_final - t\n            is_last_step = True\n\n        # Compute stages using k1 from the previous step (or initial)\n        k2 = f(t + c2 * h, y + h * (a21 * k1))\n        k3 = f(t + c3 * h, y + h * (a31 * k1 + a32 * k2))\n        \n        y_high = y + h * (b_high[0] * k1 + b_high[1] * k2 + b_high[2] * k3)\n        \n        # k4 uses y_high. This is the stage for the FSAL property.\n        k4 = f(t + h, y_high)\n        \n        y_low = y + h * (b_low[0] * k1 + b_low[1] * k2 + b_low[2] * k3 + b_low[3] * k4)\n        \n        # Error estimation\n        error = abs(y_high - y_low)\n        \n        # Scale for tolerance checking\n        scale = atol + rtol * abs(y_high)\n        \n        if scale < 1e-30: # Avoid division by zero if tolerances and y are zero\n            epsilon = float('inf') if error > 0 else 0.0\n        else:\n            epsilon = error / scale\n        \n        # Step size control\n        if epsilon <= 1.0: # Step is accepted\n            accepted_steps += 1\n            t = t + h\n            y = y_high\n            \n            # Use k4 from this step as k1 for the next (FSAL)\n            k1 = k4\n            \n            if is_last_step:\n                break\n            \n            # Calculate optimal h for the next step\n            if epsilon == 0.0:\n                h_factor = max_factor\n            else:\n                h_factor = safety_factor * (epsilon**(-1.0 / p))\n            \n            h_new = h * min(max_factor, max(min_factor, h_factor))\n            h = max(h_min, h_new)\n\n        else: # Step is rejected\n            if is_last_step:\n                is_last_step = False # Allow for more steps with smaller h\n                \n            # Calculate a new, smaller h for the retry\n            h_factor = safety_factor * (epsilon**(-1.0 / p))\n            h_new = h * min(max_factor, max(min_factor, h_factor))\n            h = max(h_min, h_new)\n            \n            # Check for stagnation\n            if t + h <= t:\n                # Could happen if h shrinks below machine epsilon relative to t\n                raise RuntimeError(\"Step size has stagnated.\")\n\n    return accepted_steps\n\ndef solve():\n    # Define the ODE: y'(t) = -10*y(t)\n    def f(t, y):\n        return -10.0 * y\n\n    # Problem parameters\n    t0 = 0.0\n    t_final = 1.0\n    h0 = 1e-2\n    h_min = 1e-12\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'rtol': 1e-3, 'atol': 0.0, 'y0': 1.0},\n        {'rtol': 0.0, 'atol': 1e-6, 'y0': 1.0},\n        {'rtol': 1e-6, 'atol': 1e-12, 'y0': 1.0},\n        {'rtol': 1e-3, 'atol': 0.0, 'y0': 1e-6},\n        {'rtol': 1e-2, 'atol': 1e-8, 'y0': 1.0}\n    ]\n\n    results = []\n    for case in test_cases:\n        num_steps = adaptive_bs32_solver(f, t0, case['y0'], t_final, h0, h_min, case['rtol'], case['atol'])\n        results.append(num_steps)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3203842"}, {"introduction": "A solver's true mettle is tested by 'stiff' differential equations, which contain interacting phenomena at vastly different time scales. For these problems, an explicit adaptive solver must drastically reduce its step size to navigate regions of rapid change, revealing a crucial challenge in scientific computing. This advanced practice [@problem_id:3203792] uses the famous Van der Pol oscillator to let you investigate this behavior, providing valuable insight into the performance and limitations of adaptive algorithms when faced with stiffness.", "problem": "Consider the initial value problem arising from the Van der Pol oscillator given by the second-order ordinary differential equation $y'' - \\mu(1-y^2)y' + y = 0$. Introduce $y_1 = y$ and $y_2 = y'$ to obtain the equivalent first-order system\n$$\n\\begin{aligned}\ny_1' &= y_2, \\\\\ny_2' &= \\mu(1 - y_1^2)\\,y_2 - y_1,\n\\end{aligned}\n$$\nwhere $\\mu > 0$ controls stiffness and large $\\mu$ yields a stiff system. The task is to design and implement an adaptive step-size solver based on an embedded explicit Runge–Kutta method of order at least $4$ with a built-in local error estimator, and to investigate the behavior of the step size when solving this system for large $\\mu$.\n\nStart from the following foundational base:\n- The initial value problem for systems of ordinary differential equations is defined by $\\mathbf{y}'(t) = \\mathbf{f}(t,\\mathbf{y}(t))$ with initial condition $\\mathbf{y}(t_0) = \\mathbf{y}_0$.\n- For an embedded explicit Runge–Kutta pair of orders $p$ and $p+1$, the local truncation error estimator $\\mathbf{e}$ is computed as the difference between two stage-weighted updates at the same step, and the local error scales proportionally to $h^{p+1}$ for sufficiently smooth $\\mathbf{f}$, where $h$ is the step size.\n- Adaptive step-size control aims to enforce a user-specified error tolerance by increasing $h$ when the estimated local error is small and decreasing $h$ when it is large.\n\nYour program must implement an adaptive explicit embedded Runge–Kutta integrator with the following requirements:\n- Use a relative tolerance $rtol$ and an absolute tolerance $atol$ combined into a componentwise scale $s_i = atol + rtol \\cdot \\max(|y_i|, |y_i^{\\text{new}}|)$ for each component $i$, where $y_i^{\\text{new}}$ is the candidate updated value. Use the root mean square (RMS) norm of the scaled error vector $\\mathbf{e}/\\mathbf{s}$ to decide acceptance of a step.\n- If a trial step is accepted, advance time by $h$ and update the solution; otherwise, reject the step and retry with a smaller $h$.\n- Implement a step-size update rule consistent with the local error scaling with $h^{p+1}$ and include a safety factor and growth/shrink limits to avoid erratic changes.\n- Employ the First Same As Last (FSAL) property if available in the chosen method to reduce function evaluations, but ensure correctness regardless of rejections.\n- Enforce a minimum step size $h_{\\min}$; if the algorithm requires $h  h_{\\min}$ to proceed, terminate and report failure for that test case.\n- Limit the total number of attempted steps to a maximum $N_{\\max}$ to guarantee termination.\n\nUse the initial conditions $y(0) = 2$ and $y'(0) = 0$, so that $\\mathbf{y}_0 = (2, 0)^{\\top}$. No physical units are required for this problem. Angles are not involved.\n\nDesign a test suite that probes the solver’s behavior across non-stiff, moderately stiff, and very stiff regimes. Use the following three test cases:\n- Test case $1$: $\\mu = 1$, $T = 10$, $rtol = 10^{-6}$, $atol = 10^{-9}$, $h_0 = 0.1$, $h_{\\min} = 10^{-12}$, $N_{\\max} = 100000$.\n- Test case $2$: $\\mu = 50$, $T = 10$, $rtol = 10^{-5}$, $atol = 10^{-8}$, $h_0 = 0.1$, $h_{\\min} = 10^{-12}$, $N_{\\max} = 100000$.\n- Test case $3$: $\\mu = 200$, $T = 6$, $rtol = 10^{-4}$, $atol = 10^{-7}$, $h_0 = 0.05$, $h_{\\min} = 10^{-12}$, $N_{\\max} = 100000$.\n\nFor each test case, run the integrator on the interval $[0, T]$ and record the following metrics:\n- The total number of accepted steps (an integer).\n- The total number of rejected steps (an integer).\n- The minimum step size $h$ encountered during the run (a float).\n- A boolean indicating whether the final time $T$ was successfully reached (true) or not (false).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list in the order described above. For example, the output format must be\n$[ [\\text{accepted}_1,\\text{rejected}_1,\\text{min\\_h}_1,\\text{success}_1], [\\text{accepted}_2,\\text{rejected}_2,\\text{min\\_h}_2,\\text{success}_2], [\\text{accepted}_3,\\text{rejected}_3,\\text{min\\_h}_3,\\text{success}_3] ]$.", "solution": "The posed problem requires the design and implementation of an adaptive step-size integrator to solve the initial value problem defined by the Van der Pol oscillator. The governing system of first-order ordinary differential equations (ODEs), obtained by setting $y_1 = y$ and $y_2 = y'$, is:\n$$\n\\mathbf{y}'(t) = \\mathbf{f}(t, \\mathbf{y}(t)) = \\begin{pmatrix} y_2 \\\\ \\mu(1-y_1^2)y_2 - y_1 \\end{pmatrix}\n$$\nThe problem is defined on the interval $[0, T]$ with the initial condition $\\mathbf{y}(0) = (2, 0)^{\\top}$. The parameter $\\mu > 0$ determines the stiffness of the ODE system; large values of $\\mu$ correspond to a numerically stiff problem, which presents a challenge for many numerical methods. The objective is to implement a solver that automatically adjusts its step size, $h$, to satisfy a prescribed error tolerance while efficiently navigating both the slow and fast dynamics of the solution.\n\nThe problem specifies the use of an embedded explicit Runge-Kutta method of order at least $4$. A canonical and highly effective choice satisfying this requirement is the Dormand-Prince 5(4) pair, hereafter referred to as DP5(4). This method calculates a fifth-order accurate solution ($p+1=5$) and simultaneously provides a fourth-order accurate solution ($p=4$) used for estimating the local truncation error. A significant advantage of the DP5(4) method is its \"First Same As Last\" (FSAL) property. This property ensures that the derivative evaluation for the final stage of a successful step is identical to the derivative evaluation for the first stage of the subsequent step. This allows for the reuse of one function evaluation per accepted step, thereby reducing the computational cost.\n\nAn $s$-stage explicit Runge-Kutta method advances the solution from $t_n$ to $t_{n+1} = t_n + h$. For an autonomous system $\\mathbf{y}' = \\mathbf{f}(\\mathbf{y})$, this involves computing a set of intermediate stage derivatives:\n$$ \\mathbf{k}_i = \\mathbf{f}\\left(t_n + c_i h, \\mathbf{y}_n + h \\sum_{j=1}^{i-1} a_{ij} \\mathbf{k}_j\\right) \\quad \\text{for } i=1, \\dots, s $$\nThe DP5(4) method utilizes $s=7$ stages. The higher-order (fifth-order) approximation, which is used to advance the solution if the step is accepted, is given by:\n$$ \\mathbf{y}_{n+1} = \\mathbf{y}_n + h \\sum_{i=1}^7 b_i \\mathbf{k}_i $$\nSimultaneously, a lower-order (fourth-order) approximation is computed:\n$$ \\hat{\\mathbf{y}}_{n+1} = \\mathbf{y}_n + h \\sum_{i=1}^7 b_i^* \\mathbf{k}_i $$\nThe coefficients $c_i$, $a_{ij}$, $b_i$, and $b_i^*$ are constants defined by the method's Butcher tableau. The local error estimate for the step is defined as the difference between these two approximations:\n$$ \\mathbf{e}_{n+1} = \\mathbf{y}_{n+1} - \\hat{\\mathbf{y}}_{n+1} = h \\sum_{i=1}^7 (b_i - b_i^*) \\mathbf{k}_i $$\nThis estimate, $\\mathbf{e}_{n+1}$, approximates the local truncation error of the lower-order method and scales as $\\mathcal{O}(h^{p+1})$, which corresponds to $\\mathcal{O}(h^5)$ for this particular method pair.\n\nThe core of the adaptive algorithm is to control the step size $h$ to keep the magnitude of the local error within a specified tolerance. The control mechanism is implemented as follows:\n$1$. **Error Norm Calculation**: The problem requires a mixed error tolerance criterion combining a relative tolerance, $rtol$, and an absolute tolerance, $atol$. For each component $j$ of the solution vector $\\mathbf{y}$, a tolerance scale $s_j$ is defined:\n$$ s_j = atol + rtol \\cdot \\max(|\\mathbf{y}_{n,j}|, |\\mathbf{y}_{n+1,j}|) $$\nHere, $\\mathbf{y}_{n,j}$ is the solution component at the beginning of the step, and $\\mathbf{y}_{n+1,j}$ is the candidate solution from the higher-order formula. The total error is then measured using a scaled root-mean-square (RMS) norm:\n$$ E = \\sqrt{\\frac{1}{d} \\sum_{j=1}^d \\left( \\frac{\\mathbf{e}_{n+1,j}}{s_j} \\right)^2} $$\nwhere $d$ is the dimension of the ODE system (in this case, $d=2$).\n$2$. **Step Acceptance/Rejection**: A trial step is accepted if its error norm $E$ is less than or equal to $1$. Upon acceptance, the solution and time are advanced: $\\mathbf{y}_{n+1} \\leftarrow \\mathbf{y}_{n+1}$ and $t_{n+1} \\leftarrow t_n + h$. If $E > 1$, the step is rejected; the solution and time are not updated, and the step must be retried with a smaller step size.\n$3$. **Step Size Update**: The optimal step size is one for which $E \\approx 1$. Leveraging the known error scaling $E \\propto h^{p+1}$, the proposed new step size $h_{\\text{new}}$ is calculated from the current step size $h$ and the computed error norm $E$:\n$$ h_{\\text{new}} = h \\cdot S \\cdot \\left( \\frac{1}{E} \\right)^{1/(p+1)} $$\nwhere $p=4$ is the order of the error-estimating method. $S$ is a safety factor (a value such as $0.9$ is common) to promote conservative step size adjustments and reduce the frequency of rejections. To prevent erratic or overly aggressive changes, the new step size is bounded:\n$$ h \\cdot F_{\\min} \\le h_{\\text{new}} \\le h \\cdot F_{\\max} $$\nwhere $F_{\\min}$ and $F_{\\max}$ are minimum and maximum scaling factors (e.g., $0.2$ and $5.0$, respectively). This update formula is applied both to decrease the step size after a rejected step and to adjust it (potentially increasing it) after an accepted one.\n\nFor large values of $\\mu$, the Van der Pol oscillator exhibits stiff behavior, characterized by the presence of both very slow and very fast time scales in the solution. The trajectory rapidly converges to a limit cycle, which consists of long periods of slow change intercepted by short intervals of rapid transition. An explicit method like DP5(4) is forced to take extremely small steps to accurately and stably resolve these rapid transitions. As $\\mu$ increases, we expect the adaptive solver to require a significantly larger number of steps, and the minimum step size encountered will become substantially smaller. This behavior highlights a known limitation of explicit methods for stiff problems. The algorithm incorporates necessary safeguards, such as a maximum number of steps ($N_{\\max}$) and a minimum allowable step size ($h_{\\min}$), to ensure termination even when the problem's stiffness becomes computationally prohibitive for the chosen method.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the adaptive Runge-Kutta solver.\n    \"\"\"\n    \n    # Dormand-Prince 5(4) Butcher Tableau\n    C = np.array([0, 1/5, 3/10, 4/5, 8/9, 1, 1], dtype=np.float64)\n    A = np.array([\n        [0, 0, 0, 0, 0, 0, 0],\n        [1/5, 0, 0, 0, 0, 0, 0],\n        [3/40, 9/40, 0, 0, 0, 0, 0],\n        [44/45, -56/15, 32/9, 0, 0, 0, 0],\n        [19372/6561, -25360/2187, 64448/6561, -212/729, 0, 0, 0],\n        [9017/3168, -355/33, 46732/5247, 49/176, -5103/18656, 0, 0],\n        [35/384, 0, 500/1113, 125/192, -2187/6784, 11/84, 0]\n    ], dtype=np.float64)\n    # 5th-order solution coefficients\n    B_HIGH = np.array([35/384, 0, 500/1113, 125/192, -2187/6784, 11/84, 0], dtype=np.float64)\n    # 4th-order solution coefficients\n    B_LOW = np.array([5179/57600, 0, 7571/16695, 393/640, -92097/339200, 187/2100, 1/40], dtype=np.float64)\n    E_COEFFS = B_HIGH - B_LOW\n    \n    # Order of the error estimator\n    ORDER = 4\n\n    # --- Solver Parameters ---\n    # Safety factor for step-size update\n    SAFETY = 0.9\n    # Min/max step-size change factors\n    MIN_FACTOR = 0.2\n    MAX_FACTOR = 5.0\n\n    def vdp_ode(t, y, mu):\n        \"\"\"Van der Pol oscillator ODE system.\"\"\"\n        y1, y2 = y\n        return np.array([y2, mu * (1 - y1**2) * y2 - y1], dtype=np.float64)\n\n    def run_integrator(mu, T, rtol, atol, h0, h_min, N_max):\n        \"\"\"\n        Implements the adaptive explicit embedded Runge-Kutta integrator.\n        \"\"\"\n        f = lambda t, y: vdp_ode(t, y, mu)\n        y0 = np.array([2.0, 0.0], dtype=np.float64)\n        \n        t = 0.0\n        y = y0\n        h = h0\n        \n        accepted_steps = 0\n        rejected_steps = 0\n        min_h_val = h0\n        \n        num_stages = 7\n        k_stages = np.zeros((num_stages, len(y0)), dtype=np.float64)\n        \n        # Initial derivative evaluation\n        k_stages[0] = f(t, y)\n        total_steps = 0\n        \n        while t  T:\n            total_steps = accepted_steps + rejected_steps\n            if total_steps = N_max:\n                return [accepted_steps, rejected_steps, min_h_val, False]\n\n            # Ensure the final step lands exactly on T\n            if t + h  T:\n                h = T - t\n            \n            if h  h_min:\n                return [accepted_steps, rejected_steps, min_h_val, False]\n\n            step_accepted = False\n            while not step_accepted:\n                # Compute stages k2 to k7\n                for i in range(1, num_stages):\n                    dy = np.dot(A[i, :i], k_stages[:i])\n                    k_stages[i] = f(t + C[i] * h, y + h * dy)\n                \n                # Higher-order solution candidate\n                y_new = y + h * np.dot(B_HIGH, k_stages)\n                \n                # Compute error estimate\n                err_vec = h * np.dot(E_COEFFS, k_stages)\n                \n                # Calculate error norm based on mixed tolerance\n                scale = atol + rtol * np.maximum(np.abs(y), np.abs(y_new))\n                err_norm = np.sqrt(np.mean((err_vec / scale)**2))\n\n                if err_norm = 1.0:\n                    step_accepted = True\n                    # Accept the step\n                    t += h\n                    y = y_new\n                    accepted_steps += 1\n                    min_h_val = min(min_h_val, h)\n                    \n                    # Use FSAL: k1 for next step is k7 from this step\n                    k_stages[0] = k_stages[num_stages-1]\n                    \n                    # Update step size for the next step\n                    if err_norm == 0.0:\n                        h_new = h * MAX_FACTOR\n                    else:\n                        h_new = h * SAFETY * (err_norm**(-1.0 / (ORDER + 1)))\n                    h = min(h * MAX_FACTOR, max(h * MIN_FACTOR, h_new))\n                \n                else:\n                    # Reject the step\n                    rejected_steps += 1\n                    total_steps = accepted_steps + rejected_steps\n                    if total_steps = N_max:\n                        return [accepted_steps, rejected_steps, min_h_val, False]\n                    \n                    # Compute smaller step size for retry\n                    h_new = h * SAFETY * (err_norm**(-1.0 / (ORDER + 1)))\n                    h = max(h_new, h * MIN_FACTOR)\n                    \n                    if h  h_min:\n                        return [accepted_steps, rejected_steps, min_h_val, False]\n        \n        return [accepted_steps, rejected_steps, min_h_val, True]\n\n    # --- Test Suite ---\n    test_cases = [\n        # (mu, T, rtol, atol, h0, h_min, N_max)\n        (1, 10, 1e-6, 1e-9, 0.1, 1e-12, 100000),\n        (50, 10, 1e-5, 1e-8, 0.1, 1e-12, 100000),\n        (200, 6, 1e-4, 1e-7, 0.05, 1e-12, 100000)\n    ]\n    \n    results = []\n    for case in test_cases:\n        result = run_integrator(*case)\n        results.append(result)\n    \n    # Format the final output string exactly as required\n    result_strings = []\n    for res in results:\n        accepted, rejected, min_h, success = res\n        result_strings.append(f\"[{accepted},{rejected},{min_h},{str(success).lower()}]\")\n        \n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3203792"}]}