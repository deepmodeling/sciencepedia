{"hands_on_practices": [{"introduction": "The relationship between a numerical method's order, the step size $h$, and the resulting global truncation error is the cornerstone of error analysis. This first practice provides a direct test of your understanding of this fundamental scaling law [@problem_id:2185101]. By exploring how the error changes when the step size is adjusted, you can solidify your intuition for the predictable nature of error in well-behaved numerical methods.", "problem": "A numerical analyst is studying the accuracy of a stable numerical method for solving an ordinary differential equation over a fixed interval. The method is known to be a second-order method. In a first computational experiment, using a constant step size of $h$, the analyst computes the solution and finds that the global truncation error at the end of the interval is approximately $\\epsilon$.\n\nIn a second experiment, the analyst decides to change the step size to $2h$ while keeping all other parameters and the differential equation the same. Assuming the method remains stable and the relationship between global error and step size holds in this range, what is the estimated global truncation error for this second experiment?\n\nA. $\\frac{\\epsilon}{4}$\n\nB. $\\frac{\\epsilon}{2}$\n\nC. $\\epsilon$\n\nD. $2\\epsilon$\n\nE. $4\\epsilon$", "solution": "A stable method of order $p$ has a global truncation error at a fixed final time that scales as $E(h)=C h^{p}$ for some constant $C$ independent of $h$, provided the method remains stable and the asymptotic error relation holds.\n\nGiven that the method is second-order, $p=2$. For step size $h$, the observed global error is $\\epsilon$, so\n$$\n\\epsilon = C h^{2}.\n$$\nIf the step size is changed to $2h$, the new global error is\n$$\nE(2h) = C (2h)^{2} = 4 C h^{2} = 4 \\epsilon.\n$$\nTherefore, doubling the step size multiplies the global error by $4$, which corresponds to option E.", "answer": "$$\\boxed{E}$$", "id": "2185101"}, {"introduction": "While theoretical analysis tells us that a method like the fourth-order Runge-Kutta should have a global error that scales as $O(h^{4})$, verifying this behavior empirically is a crucial skill for any computational scientist. This hands-on coding exercise challenges you to act as a numerical analyst, running simulations and using data analysis to reconstruct the error law $E(h) \\approx C h^{p}$ [@problem_id:3236754]. Through log-log plotting and linear regression, you will experimentally determine both the order $p$ and the constant $C$, a powerful technique for validating numerical software.", "problem": "Consider the initial value problem defined by the ordinary differential equation $y^{\\prime}(t) = f(t, y)$ with initial condition $y(t_{0}) = y_{0}$. The explicit classical Runge-Kutta method of order four (RK4) is a four-stage one-step method that advances the numerical solution $y_{n}$ from time $t_{n}$ to $t_{n+1} = t_{n} + h$ using stage evaluations of $f$ at $t_{n}$, $t_{n} + \\frac{h}{2}$, and $t_{n} + h$. The global truncation error at a final time $T$ associated with a uniform step size $h$ is defined as $e(h) = |y_{N} - y_{\\mathrm{exact}}(T)|$, where $N = \\frac{T - t_{0}}{h}$ and $y_{N}$ is the numerical approximation produced by RK4 after $N$ steps.\n\nYour task is to empirically reconstruct the global truncation error law for RK4 by fitting the observed errors $e(h)$ across multiple step sizes $h$ and inferring both the empirical order $p$ and the leading constant $C$ through a linear fit on a log-log scale. Specifically, for each test case below, you must:\n- Implement a solver that, for each provided step size $h$, computes the RK4 numerical solution at the final time $T$ and evaluates the corresponding error $e(h)$.\n- Use natural logarithms to fit the model $\\log(e(h)) = \\log(C) + p \\log(h)$ via least squares over the provided set of step sizes, thereby inferring $p$ and $C$.\n- Aggregate the inferred $[p, C]$ across the test cases in the exact output format specified at the end of this statement.\n\nFoundational base to use and justify: the initial value problem definition $y^{\\prime}(t) = f(t, y)$ with $y(t_{0}) = y_{0}$, the definition of global truncation error $e(h)$ as the difference at $T$ between the numerical solution and the exact solution, and the empirical linearization of a power-law relationship on a log-log scale.\n\nTest suite to evaluate and fit the empirical error law:\n- Test case $1$: $f(t, y) = y$, $t_{0} = 0$, $y_{0} = 1$, $T = 1$, exact solution $y_{\\mathrm{exact}}(t) = e^{t}$, step sizes $h \\in \\{0.5, 0.25, 0.125, 0.0625\\}$.\n- Test case $2$: $f(t, y) = -3y$, $t_{0} = 0$, $y_{0} = 2$, $T = 2$, exact solution $y_{\\mathrm{exact}}(t) = 2e^{-3t}$, step sizes $h \\in \\{0.5, 0.25, 0.125, 0.0625\\}$.\n- Test case $3$: $f(t, y) = y + t$, $t_{0} = 0$, $y_{0} = 0$, $T = 0.8$, exact solution $y_{\\mathrm{exact}}(t) = e^{t} - t - 1$, step sizes $h \\in \\{0.4, 0.2, 0.1, 0.05\\}$.\n- Test case $4$: $f(t, y) = -5y$, $t_{0} = 0$, $y_{0} = 1$, $T = 1$, exact solution $y_{\\mathrm{exact}}(t) = e^{-5t}$, step sizes $h \\in \\{0.1, 0.05, 0.025, 0.0125\\}$.\n\nAll time quantities are dimensionless. No physical units are involved. Angles do not appear. Your program must:\n- Use natural logarithms for the least-squares fit of $(\\log(h), \\log(e(h)))$ pairs.\n- Produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test case contributes a two-element list $[p, C]$. For example, the required format is $[[p_{1}, C_{1}], [p_{2}, C_{2}], [p_{3}, C_{3}], [p_{4}, C_{4}]]$.\n- Report numerical values as floating-point numbers. No rounding or specific significant figures are required.\n\nYour final answer must be a complete, runnable program that performs these computations and prints only the specified output line.", "solution": "The problem requires the empirical determination of the global truncation error law for the fourth-order classical Runge-Kutta (RK4) method. For a numerical method of order $p$, the global error $e(h)$ at a fixed final time $T$, committed when using a step size $h$, is expected to follow the power law $e(h) \\approx C h^p$ for sufficiently small $h$. Here, $C$ is a constant that depends on the differential equation and the final time $T$, but not on $h$. The objective is to determine the empirical values of the order $p$ and the constant $C$ for several initial value problems by performing a linear regression on transformed error data.\n\nThe initial value problem is given by the ordinary differential equation (ODE) $y^{\\prime}(t) = f(t, y)$ with an initial condition $y(t_0) = y_0$.\n\nThe core of the numerical solution is the RK4 method, a one-step, four-stage explicit method. To advance the solution from $(t_n, y_n)$ to $(t_{n+1}, y_{n+1})$, where $t_{n+1} = t_n + h$, the method computes four intermediate slope estimates (stages) $k_1, k_2, k_3, k_4$:\n$$k_1 = f(t_n, y_n)$$\n$$k_2 = f\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2}k_1\\right)$$\n$$k_3 = f\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2}k_2\\right)$$\n$$k_4 = f(t_n + h, y_n + hk_3)$$\nThe solution is then updated using a weighted average of these slopes:\n$$y_{n+1} = y_n + \\frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4)$$\n\nTo determine the parameters $p$ and $C$ in the error model $e(h) \\approx C h^p$, we linearize the relationship by taking the natural logarithm of both sides:\n$$\n\\log(e(h)) \\approx \\log(C h^p) = \\log(C) + \\log(h^p) = \\log(C) + p \\log(h)\n$$\nThis equation has the form of a line, $Y = mX + b$, where $Y = \\log(e(h))$, the slope is $m = p$, the independent variable is $X = \\log(h)$, and the y-intercept is $b = \\log(C)$.\n\nThe overall algorithm for each test case is as follows:\n1.  For each given step size $h_i$ in the provided set, solve the ODE from $t_0$ to $T$ using the RK4 method. This involves taking $N_i = (T - t_0) / h_i$ steps to obtain the numerical solution $y_{N_i}$.\n2.  Calculate the exact solution at the final time, $y_{\\mathrm{exact}}(T)$.\n3.  Compute the absolute global error for each step size: $e(h_i) = |y_{N_i} - y_{\\mathrm{exact}}(T)|$.\n4.  This procedure yields a set of data points $(h_i, e(h_i))$.\n5.  Transform these data points to a logarithmic scale, creating a new set of points $(\\log(h_i), \\log(e(h_i)))$.\n6.  Perform a linear least-squares regression on these transformed points to find the best-fit line. The slope of this line is the empirical order $p$, and the intercept is $\\log(C)$.\n7.  The constant $C$ is then recovered by exponentiating the intercept: $C = \\exp(\\log(C))$.\n8.  The resulting pair $[p, C]$ is stored for the test case.\n\nThis process is repeated for all four test cases provided in the problem statement. The final output is an aggregation of the $[p, C]$ pairs for each case. The use of multiple step sizes for each regression improves the robustness of the inferred parameters, assuming the step sizes are in the asymptotic regime where the leading-order error term dominates.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Empirically reconstructs the global truncation error law for the RK4 method\n    by fitting observed errors to a power law e(h) = C*h^p.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"f\": lambda t, y: y,\n            \"t0\": 0.0,\n            \"y0\": 1.0,\n            \"T\": 1.0,\n            \"y_exact\": lambda t: np.exp(t),\n            \"h_values\": np.array([0.5, 0.25, 0.125, 0.0625])\n        },\n        {\n            \"f\": lambda t, y: -3.0 * y,\n            \"t0\": 0.0,\n            \"y0\": 2.0,\n            \"T\": 2.0,\n            \"y_exact\": lambda t: 2.0 * np.exp(-3.0 * t),\n            \"h_values\": np.array([0.5, 0.25, 0.125, 0.0625])\n        },\n        {\n            \"f\": lambda t, y: y + t,\n            \"t0\": 0.0,\n            \"y0\": 0.0,\n            \"T\": 0.8,\n            \"y_exact\": lambda t: np.exp(t) - t - 1.0,\n            \"h_values\": np.array([0.4, 0.2, 0.1, 0.05])\n        },\n        {\n            \"f\": lambda t, y: -5.0 * y,\n            \"t0\": 0.0,\n            \"y0\": 1.0,\n            \"T\": 1.0,\n            \"y_exact\": lambda t: np.exp(-5.0 * t),\n            \"h_values\": np.array([0.1, 0.05, 0.025, 0.0125])\n        }\n    ]\n\n    def run_rk4(f, y0, t0, T, h):\n        \"\"\"\n        Solves an ODE y'(t) = f(t,y) from t0 to T with step size h using RK4.\n        \n        Args:\n            f (callable): The function f(t, y).\n            y0 (float): The initial value y(t0).\n            t0 (float): The initial time.\n            T (float): The final time.\n            h (float): The step size.\n\n        Returns:\n            float: The numerical solution y(T).\n        \"\"\"\n        t = t0\n        y = y0\n        # Use rounding to ensure integer number of steps for the loop\n        num_steps = int(round((T - t0) / h))\n        \n        for _ in range(num_steps):\n            k1 = f(t, y)\n            k2 = f(t + 0.5 * h, y + 0.5 * h * k1)\n            k3 = f(t + 0.5 * h, y + 0.5 * h * k2)\n            k4 = f(t + h, y + h * k3)\n            y += (h / 6.0) * (k1 + 2.0 * k2 + 2.0 * k3 + k4)\n            t += h\n            \n        return y\n\n    results = []\n    for case in test_cases:\n        errors = []\n        h_vals = case[\"h_values\"]\n        \n        for h in h_vals:\n            y_numerical = run_rk4(case[\"f\"], case[\"y0\"], case[\"t0\"], case[\"T\"], h)\n            y_true = case[\"y_exact\"](case[\"T\"])\n            error = np.abs(y_numerical - y_true)\n            errors.append(error)\n        \n        # Transform data to log-log scale\n        log_h = np.log(h_vals)\n        log_e = np.log(np.array(errors))\n        \n        # Perform linear least-squares fit: log(e) = p*log(h) + log(C)\n        # np.polyfit returns [p, log(C)]\n        p, log_C = np.polyfit(log_h, log_e, 1)\n        \n        # Recover C from the intercept\n        C = np.exp(log_C)\n        \n        results.append([p, C])\n\n    # Convert results to a string with the specified format\n    # The default str() representation of a list is [item1, item2], which matches the requirement\n    # Example: str([4.0, 0.001]) -> '[4.0, 0.001]'\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3236754"}, {"introduction": "The ultimate goal of understanding global error is not just to analyze it, but to control it. This final practice bridges the gap between theory and practical application by tasking you with an essential engineering problem: selecting a step size $h$ to achieve a specific error tolerance $\\epsilon$ [@problem_id:3236664]. You will start with a theoretical estimate and then refine your choice using an empirically measured error constant, demonstrating an adaptive workflow for achieving computational accuracy efficiently.", "problem": "Consider initial value problems of the form $y^{\\prime}(t)=f(t,y(t))$ with a given initial condition $y(0)=y_{0}$ on a finite interval $[0,T]$. Assume $f$ is sufficiently smooth so that the exact solution $y(t)$ exists and is unique on $[0,T]$. For a one-step numerical method that is consistent and zero-stable, of order $p$ in the sense that the global truncation error behaves asymptotically like $O(h^{p})$ as the step size $h \\to 0$, one can relate the maximum nodal global error on a grid of step size $h$ to a method- and problem-dependent constant. Specifically, suppose a known bound constant $C_{0}$ is available such that, in the asymptotic regime for small $h$, the maximum nodal global error $E(h)$ satisfies $E(h) \\leq C_{0} h^{p}$.\n\nTask: Write a complete program that, for each provided test case, performs the following steps in a mathematically justified and computationally verifiable way.\n\n1. From the base principles of consistency, zero-stability, and the order definition (order $p$), derive the inequality needed to select a step size $h$ so that the maximum nodal global error $E(h)$ is guaranteed to be at most a user-specified target $\\epsilon$. Use the provided order $p$ and bound constant $C_{0}$, but do not assume any unproven shortcut formulas.\n2. Using the derived relationship, compute an initial step size $h_{\\text{pred}}$ and then choose an integer number of steps $N_{0}$ so that $h_{0}=T/N_{0}$ does not exceed $h_{\\text{pred}}$. This ensures the grid aligns with the interval $[0,T]$ and respects the target bound logic. Compute the numerical solution on this grid and measure the maximum nodal global error $E(h_{0})=\\max_{0\\leq n \\leq N_{0}} | y_{n}-y(t_{n}) |$, where $t_{n}=n h_{0}$ and $y_{n}$ is the numerical solution value.\n3. Halve the step size by doubling the number of steps $N_{1}=2N_{0}$, yielding $h_{1}=T/N_{1}$. Recompute the numerical solution and measure $E(h_{1})$.\n4. Estimate a measured asymptotic error constant $K_{\\text{meas}}$ using the two computed errors, acknowledging the asymptotic behavior $E(h) \\approx K h^{p}$ for sufficiently small $h$. Use a conservative estimate that does not underestimate the constant.\n5. Using the measured constant $K_{\\text{meas}}$, compute an adjusted step size $h_{\\text{adj}}$ to meet the target error $\\epsilon$. Choose an integer number of steps $N_{\\text{adj}}$ so that $h_{\\text{adj}}=T/N_{\\text{adj}}$ does not exceed the prediction from $K_{\\text{meas}}$. Verify by computation that the resulting maximum nodal global error $E(h_{\\text{adj}})$ meets the target $\\epsilon$.\n6. For each test case, return the list $[h_{0}, E(h_{0}), h_{1}, E(h_{1}), K_{\\text{meas}}, h_{\\text{adj}}, E(h_{\\text{adj}}), \\text{meets\\_target}]$, where $\\text{meets\\_target}$ is a boolean indicating whether $E(h_{\\text{adj}}) \\leq \\epsilon$.\n\nYou must implement two methods: the classical fourth-order Runge–Kutta method and the explicit Euler method, and measure the maximum nodal global error on the grid points. When trigonometric functions appear, treat angles in radians.\n\nTest Suite:\n- Case A (happy path, smooth autonomous problem):\n  - Differential equation: $y^{\\prime}(t)=-y(t)$, exact solution $y(t)=e^{-t}$.\n  - Interval: $T=1$.\n  - Initial condition: $y_{0}=1$.\n  - Method: classical fourth-order Runge–Kutta.\n  - Order: $p=4$.\n  - Error target: $\\epsilon=10^{-6}$.\n  - Known bound constant: $C_{0}=2 \\times 10^{-2}$.\n- Case B (nonautonomous problem with trigonometric coefficient, angles in radians):\n  - Differential equation: $y^{\\prime}(t)=y(t)\\cos(t)$, exact solution $y(t)=\\exp(\\sin(t))$.\n  - Interval: $T=\\pi$.\n  - Initial condition: $y_{0}=1$.\n  - Method: classical fourth-order Runge–Kutta.\n  - Order: $p=4$.\n  - Error target: $\\epsilon=5 \\times 10^{-7}$.\n  - Known bound constant: $C_{0}=2 \\times 10^{-1}$.\n- Case C (boundary case with a first-order method):\n  - Differential equation: $y^{\\prime}(t)=-y(t)$, exact solution $y(t)=e^{-t}$.\n  - Interval: $T=1$.\n  - Initial condition: $y_{0}=1$.\n  - Method: explicit Euler.\n  - Order: $p=1$.\n  - Error target: $\\epsilon=10^{-2}$.\n  - Known bound constant: $C_{0}=8 \\times 10^{-1}$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one entry per test case, where each entry is the list $[h_{0}, E(h_{0}), h_{1}, E(h_{1}), K_{\\text{meas}}, h_{\\text{adj}}, E(h_{\\text{adj}}), \\text{meets\\_target}]$. For example, the printed line should look like $[[\\ldots],[\\ldots],[\\ldots]]$. There are no physical units; all quantities are dimensionless. All angles are in radians.", "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of numerical analysis for ordinary differential equations, is well-posed with all necessary information provided, and is stated using objective, formal language. The tasks are computationally verifiable and directly address the topic of global truncation error and step size selection.\n\nThe solution proceeds by first deriving the necessary mathematical relationships and then applying them computationally to each specified test case.\n\n### 1. Theoretical Derivation\n\nThe problem is centered on solving an initial value problem (IVP) of the form:\n$$\ny^{\\prime}(t) = f(t, y(t)), \\quad y(0) = y_0\n$$\non a finite interval $[0, T]$. We are given that a one-step numerical method has order of convergence $p$, which implies that its maximum nodal global error, $E(h)$, behaves asymptotically for small step sizes $h$ as:\n$$\nE(h) = \\max_{0 \\le n \\le N} |y_n - y(t_n)| \\approx K h^p\n$$\nwhere $y_n$ is the numerical approximation to the exact solution $y(t_n)$ at grid point $t_n = n h$, and $K$ is an asymptotic error constant that depends on the method and the specific IVP. The problem provides a known upper bound constant $C_0$ such that $E(h) \\le C_0 h^p$ holds in the asymptotic regime.\n\n#### 1.1. Step Size Selection\nOur first objective is to derive a relationship for the step size $h$ that guarantees the global error $E(h)$ will be no more than a specified tolerance $\\epsilon$. Using the provided bound:\n$$\nE(h) \\le C h^p\n$$\nTo ensure $E(h) \\le \\epsilon$, we must impose the condition:\n$$\nC h^p \\le \\epsilon\n$$\nSolving this inequality for $h$ yields:\n$$\nh^p \\le \\frac{\\epsilon}{C} \\implies h \\le \\left(\\frac{\\epsilon}{C}\\right)^{1/p}\n$$\nThis inequality is the fundamental relationship for selecting the step size. Given a constant $C$ (either the initial bound $C_0$ or a measured constant $K_{\\text{meas}}$), the largest allowable predicted step size is $h_{\\text{pred}} = (\\epsilon/C)^{1/p}$. To ensure the grid points align with the interval $[0, T]$, we must choose an integer number of steps, $N$. The step size is then $h = T/N$. To satisfy the derived constraint, we need $T/N \\le h_{\\text{pred}}$, which implies $N \\ge T/h_{\\text{pred}}$. To use the largest possible step size (and thus the fewest steps) that respects the bound, we select the smallest integer $N$ satisfying this condition:\n$$\nN = \\left\\lceil \\frac{T}{h_{\\text{pred}}} \\right\\rceil\n$$\nwhere $\\lceil \\cdot \\rceil$ is the ceiling function. The actual step size used is then $h = T/N$.\n\n#### 1.2. Estimation of the Asymptotic Error Constant\nAfter performing two computations with step sizes $h_0$ and $h_1 = h_0/2$, yielding errors $E(h_0)$ and $E(h_1)$, we can estimate the true asymptotic constant $K$. Assuming both computations are in the asymptotic regime:\n$$\nE(h_0) \\approx K h_0^p\n$$\n$$\nE(h_1) \\approx K h_1^p\n$$\nWe can obtain an estimate for $K$ from each. For instance, from the second, more accurate computation: $K \\approx E(h_1) / h_1^p$. To be conservative and not underestimate the constant, as the problem suggests, it is prudent to calculate both estimates, $K_{\\text{est},0} = E(h_0)/h_0^p$ and $K_{\\text{est},1} = E(h_1)/h_1^p$, and take the larger of the two as our measured constant:\n$$\nK_{\\text{meas}} = \\max\\left(\\frac{E(h_0)}{h_0^p}, \\frac{E(h_1)}{h_1^p}\\right)\n$$\nThis measured constant, $K_{\\text{meas}}$, which is empirically derived from the actual performance of the method on the specific problem, is typically a much tighter estimate than the general theoretical bound $C_0$.\n\n### 2. Algorithmic Procedure\nThe overall process for each test case is as follows:\n\n1.  **Initial Step Size Calculation**:\n    *   Calculate the initial predicted step size using the provided bound $C_0$: $h_{\\text{pred},0} = (\\epsilon / C_0)^{1/p}$.\n    *   Determine the number of steps: $N_0 = \\lceil T / h_{\\text{pred},0} \\rceil$.\n    *   Set the initial step size: $h_0 = T/N_0$.\n\n2.  **First Numerical Solution and Error**:\n    *   Solve the ODE over $[0, T]$ using the specified numerical method (Explicit Euler or RK4) with $N_0$ steps.\n    *   Calculate the maximum nodal global error: $E(h_0) = \\max_{0 \\le n \\le N_0} |y_n - y(t_n)|$.\n\n3.  **Second Numerical Solution and Error**:\n    *   Set $N_1 = 2N_0$ and $h_1 = T/N_1 = h_0/2$.\n    *   Solve the ODE again with $N_1$ steps.\n    *   Calculate the corresponding error: $E(h_1) = \\max_{0 \\le n \\le N_1} |y_n - y(t_n)|$.\n\n4.  **Estimate Asymptotic Constant**:\n    *   Calculate the measured constant $K_{\\text{meas}}$ using the conservative formula derived above: $K_{\\text{meas}} = \\max(E(h_0)/h_0^p, E(h_1)/h_1^p)$.\n\n5.  **Adjusted Step Size and Final Verification**:\n    *   Calculate an adjusted predicted step size using $K_{\\text{meas}}$: $h_{\\text{pred,adj}} = (\\epsilon / K_{\\text{meas}})^{1/p}$.\n    *   Determine the adjusted number of steps: $N_{\\text{adj}} = \\lceil T / h_{\\text{pred,adj}} \\rceil$.\n    *   Set the adjusted step size: $h_{\\text{adj}} = T/N_{\\text{adj}}$.\n    *   Solve the ODE a final time with $N_{\\text{adj}}$ steps.\n    *   Calculate the final error: $E(h_{\\text{adj}}) = \\max_{0 \\le n \\le N_{\\text{adj}}} |y_n - y(t_n)|$.\n    *   Verify if the error target is met: Check if $E(h_{\\text{adj}}) \\le \\epsilon$.\n\n6.  **Report Results**: For each test case, the list $[h_0, E(h_0), h_1, E(h_1), K_{\\text{meas}}, h_{\\text{adj}}, E(h_{\\text{adj}}), \\text{meets\\_target}]$ is compiled.\n\n### 3. Numerical Methods\nThe two required one-step methods are defined by their update rules.\n\n*   **Explicit Euler Method (Order $p=1$)**:\n    $$\n    y_{n+1} = y_n + h f(t_n, y_n)\n    $$\n\n*   **Classical Fourth-Order Runge-Kutta Method (RK4, Order $p=4$)**:\n    \\begin{align*}\n    k_1 &= f(t_n, y_n) \\\\\n    k_2 &= f(t_n + h/2, y_n + (h/2) k_1) \\\\\n    k_3 &= f(t_n + h/2, y_n + (h/2) k_2) \\\\\n    k_4 &= f(t_n + h, y_n + h k_3) \\\\\n    y_{n+1} &= y_n + \\frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4)\n    \\end{align*}\n\nThese steps and formulas are implemented for each test case provided.", "answer": "```python\nimport numpy as np\n\ndef solve_ode(method_name, f, y0, T, N):\n    \"\"\"\n    Solves an ODE y'(t) = f(t, y) using a specified one-step method.\n\n    Args:\n        method_name (str): Name of the method ('euler' or 'rk4').\n        f (callable): Function f(t, y).\n        y0 (float): Initial condition y(0).\n        T (float): Final time.\n        N (int): Number of steps.\n\n    Returns:\n        tuple: (numpy.ndarray of time points, numpy.ndarray of solution values).\n    \"\"\"\n    h = T / N\n    t = np.linspace(0, T, N + 1)\n    y = np.zeros(N + 1)\n    y[0] = y0\n\n    if method_name == 'euler':\n        for n in range(N):\n            y[n + 1] = y[n] + h * f(t[n], y[n])\n    elif method_name == 'rk4':\n        for n in range(N):\n            k1 = f(t[n], y[n])\n            k2 = f(t[n] + h / 2, y[n] + h / 2 * k1)\n            k3 = f(t[n] + h / 2, y[n] + h / 2 * k2)\n            k4 = f(t[n] + h, y[n] + h * k3)\n            y[n + 1] = y[n] + h / 6 * (k1 + 2 * k2 + 2 * k3 + k4)\n    else:\n        raise ValueError(\"Unknown method specified.\")\n        \n    return t, y\n\ndef calculate_max_error(t, y_num, y_exact_func):\n    \"\"\"Calculates the maximum nodal global error.\"\"\"\n    y_exact_vals = y_exact_func(t)\n    return np.max(np.abs(y_num - y_exact_vals))\n\ndef solve():\n    \"\"\"\n    Main function to process test cases and generate the final output.\n    \"\"\"\n    # Test Suite Definition\n    test_cases = [\n        # Case A\n        {\n            \"f\": lambda t, y: -y,\n            \"y_exact\": lambda t: np.exp(-t),\n            \"T\": 1.0,\n            \"y0\": 1.0,\n            \"method\": \"rk4\",\n            \"p\": 4.0,\n            \"epsilon\": 1e-6,\n            \"C0\": 2e-2\n        },\n        # Case B\n        {\n            \"f\": lambda t, y: y * np.cos(t),\n            \"y_exact\": lambda t: np.exp(np.sin(t)),\n            \"T\": np.pi,\n            \"y0\": 1.0,\n            \"method\": \"rk4\",\n            \"p\": 4.0,\n            \"epsilon\": 5e-7,\n            \"C0\": 2e-1\n        },\n        # Case C\n        {\n            \"f\": lambda t, y: -y,\n            \"y_exact\": lambda t: np.exp(-t),\n            \"T\": 1.0,\n            \"y0\": 1.0,\n            \"method\": \"euler\",\n            \"p\": 1.0,\n            \"epsilon\": 1e-2,\n            \"C0\": 8e-1\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        f = case[\"f\"]\n        y_exact = case[\"y_exact\"]\n        T = case[\"T\"]\n        y0 = case[\"y0\"]\n        method = case[\"method\"]\n        p = case[\"p\"]\n        epsilon = case[\"epsilon\"]\n        C0 = case[\"C0\"]\n\n        # 1. Initial Step Size Calculation\n        h_pred_0 = (epsilon / C0)**(1 / p)\n        N0 = int(np.ceil(T / h_pred_0))\n        h0 = T / N0\n\n        # 2. First Numerical Solution and Error\n        t0, y0_num = solve_ode(method, f, y0, T, N0)\n        E_h0 = calculate_max_error(t0, y0_num, y_exact)\n\n        # 3. Second Numerical Solution and Error\n        N1 = 2 * N0\n        h1 = T / N1\n        t1, y1_num = solve_ode(method, f, y0, T, N1)\n        E_h1 = calculate_max_error(t1, y1_num, y_exact)\n\n        # 4. Estimate Asymptotic Constant\n        # Handle cases where error is zero to avoid division by zero\n        K_est_0 = E_h0 / (h0**p) if h0 > 0 else 0\n        K_est_1 = E_h1 / (h1**p) if h1 > 0 else 0\n        K_meas = max(K_est_0, K_est_1)\n        # Prevent K_meas from being zero if errors are zero\n        if K_meas == 0:\n            K_meas = 1e-12 # A small non-zero value\n\n        # 5. Adjusted Step Size and Final Verification\n        h_pred_adj = (epsilon / K_meas)**(1 / p)\n        N_adj = int(np.ceil(T / h_pred_adj))\n        h_adj = T / N_adj\n        \n        t_adj, y_adj_num = solve_ode(method, f, y0, T, N_adj)\n        E_h_adj = calculate_max_error(t_adj, y_adj_num, y_exact)\n        \n        meets_target = bool(E_h_adj <= epsilon)\n\n        # 6. Report Results\n        case_results = [h0, E_h0, h1, E_h1, K_meas, h_adj, E_h_adj, meets_target]\n        all_results.append(case_results)\n\n    # Format the final output string exactly as required.\n    # Convert boolean to lowercase string for printing.\n    # Format floats to a consistent representation.\n    def format_list(lst):\n        items = []\n        for item in lst:\n            if isinstance(item, bool):\n                items.append(str(item).lower())\n            else:\n                items.append(f\"{item:.10g}\")\n        return \"[\" + \",\".join(items) + \"]\"\n\n    output_str = \"[\" + \",\".join(map(format_list, all_results)) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3236664"}]}