{"hands_on_practices": [{"introduction": "To truly master a numerical method, one must move beyond simply memorizing its formula and verify its theoretical properties in practice. This first exercise guides you through implementing the classical fourth-order Runge-Kutta method for the fundamental initial value problem $y' = y$. By systematically reducing the step size and analyzing the resulting error, you will numerically confirm the method's celebrated fourth-order convergence, building a solid foundation and an intuition for how numerical error behaves [@problem_id:3213354].", "problem": "Consider the initial value problem defined by the autonomous ordinary differential equation (ODE) $y^{\\prime}(t) = y(t)$ with initial condition $y(0) = 1$. The exact solution is $y(t) = e^{t}$, so $y(1) = e$. You will approximate $y(1)$ using the classical fourth-order Runge-Kutta method (RK4). Your task is to design and implement a program that performs the following steps from first principles, using only the definition of the ODE initial value problem and the classical Runge-Kutta method structure.\n\nYou must:\n- Implement a function that advances the numerical solution of $y^{\\prime} = f(t,y)$ using the classical fourth-order Runge-Kutta method (RK4) over a single time step of size $h$. Then apply it repeatedly from $t=0$ to $t=1$ to compute the numerical approximation $y_h(1)$ using $N = 1/h$ steps, where $h$ is the step size.\n- For the specific function $f(t,y) = y$ and initial condition $y(0) = 1$, compute $y_h(1)$ for step sizes $h = 10^{-k}$ with $k \\in \\{1,2,3,4,5\\}$. For each $k$, define the numerical error $E_k = y_h(1) - e$ and the empirical asymptotic error constant $C_k = E_k / h^{4}$.\n- Additionally, for each consecutive pair $(k,k+1)$, compute the observed order estimate $p_{k,k+1} = \\log_{10}\\!\\left(\\lvert E_k \\rvert / \\lvert E_{k+1} \\rvert\\right)$, which is expected to approach $4$ for a fourth-order method when the asymptotic regime is reached.\n\nFundamental base you must rely on:\n- The definition of an initial value problem for an ODE $y^{\\prime}(t) = f(t,y)$ with $y(t_0)=y_0$.\n- The structure of a one-step explicit method and the concept of local and global truncation errors for consistent methods.\n- The classical fourth-order Runge-Kutta method (RK4) as a four-stage explicit method that evaluates $f$ at intermediate stages within each step and combines them to advance the solution.\n\nTest suite specification:\n- Use the parameter set $\\mathcal{K} = \\{k \\in \\mathbb{N} : k = 1,2,3,4,5\\}$, corresponding to step sizes $h_k = 10^{-k}$. This covers a general moderate step size (the \"happy path\") at $k = 1$, increasingly refined steps to assess convergence, and an edge of the asymptotic regime at $k = 5$ where roundoff and truncation effects interplay in double precision arithmetic.\n- For each $k \\in \\mathcal{K}$, compute $y_{h_k}(1)$, $E_k = y_{h_k}(1) - e$, and $C_k = E_k / h_k^{4}$.\n- For each consecutive pair $(k,k+1)$ with $k \\in \\{1,2,3,4\\}$, compute $p_{k,k+1} = \\log_{10}\\!\\left(\\lvert E_k \\rvert / \\lvert E_{k+1} \\rvert\\right)$.\n\nFinal output format:\n- Your program must print a single line containing a comma-separated list enclosed in square brackets. The list must contain first the five constants $[C_1,C_2,C_3,C_4,C_5]$ in order of increasing $k$, followed by the four observed orders $[p_{1,2},p_{2,3},p_{3,4},p_{4,5}]$, concatenated into one flat list:\n  - Output: $[C_1,C_2,C_3,C_4,C_5,p_{1,2},p_{2,3},p_{3,4},p_{4,5}]$.\n- Each floating-point number must be rounded to exactly $12$ significant digits in standard decimal or scientific notation.\n\nAngle units are not applicable. There are no physical units; all quantities are dimensionless.\n\nYour program must be completely self-contained and require no user input. It must use the precise value of $e$ available in your programming environment to compute the exact reference $y(1) = e$.", "solution": "The problem requires the numerical approximation of the solution to an initial value problem (IVP) using the classical fourth-order Runge-Kutta (RK4) method, followed by an analysis of the numerical error and order of convergence.\n\nThe IVP is defined by the autonomous ordinary differential equation (ODE):\n$$\ny^{\\prime}(t) = f(t, y(t)) = y(t)\n$$\nwith the initial condition:\n$$\ny(0) = 1\n$$\nThe integration is performed over the time interval $t \\in [0, 1]$. The exact solution to this IVP is known to be the exponential function, $y(t) = e^t$. Therefore, the exact value at the endpoint is $y(1) = e$.\n\nThe classical fourth-order Runge-Kutta (RK4) method is an explicit one-step numerical method used to approximate solutions of ODEs. To advance the solution from a point $(t_n, y_n)$ to $(t_{n+1}, y_{n+1})$ with a step size $h = t_{n+1} - t_n$, the RK4 a takes the form:\n$$\ny_{n+1} = y_n + \\frac{h}{6} (k_1 + 2k_2 + 2k_3 + k_4)\n$$\nThe intermediate slope evaluations, $k_i$, are calculated as follows:\n$$\n\\begin{aligned}\nk_1 = f(t_n, y_n) \\\\\nk_2 = f\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2} k_1\\right) \\\\\nk_3 = f\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2} k_2\\right) \\\\\nk_4 = f(t_n + h, y_n + h k_3)\n\\end{aligned}\n$$\nFor the specific ODE in this problem, $f(t, y) = y$, the function $f$ is independent of $t$. The stages simplify to:\n$$\n\\begin{aligned}\nk_1 = y_n \\\\\nk_2 = y_n + \\frac{h}{2} k_1 = y_n \\left(1 + \\frac{h}{2}\\right) \\\\\nk_3 = y_n + \\frac{h}{2} k_2 = y_n \\left(1 + \\frac{h}{2} + \\frac{h^2}{4}\\right) \\\\\nk_4 = y_n + h k_3 = y_n \\left(1 + h + \\frac{h^2}{2} + \\frac{h^3}{4}\\right)\n\\end{aligned}\n$$\nSubstituting these expressions into the main RK4 formula yields the update rule for a single step:\n$$\ny_{n+1} = y_n + \\frac{h}{6} \\left[ y_n + 2y_n\\left(1 + \\frac{h}{2}\\right) + 2y_n\\left(1 + \\frac{h}{2} + \\frac{h^2}{4}\\right) + y_n\\left(1 + h + \\frac{h^2}{2} + \\frac{h^3}{4}\\right) \\right]\n$$\nSimplifying the expression within the brackets:\n$$\ny_{n+1} = y_n \\left[ 1 + \\frac{h}{6} \\left( 1 + 2 + h + 2 + h + \\frac{h^2}{2} + 1 + h + \\frac{h^2}{2} + \\frac{h^3}{4} \\right) \\right] = y_n \\left( 1 + h + \\frac{h^2}{2} + \\frac{h^3}{6} + \\frac{h^4}{24} \\right)\n$$\nThis recurrence relation shows that each step multiplies the current value $y_n$ by a factor that corresponds to the first five terms of the Taylor series expansion of $e^h$.\n\nTo approximate $y(1)$, we start with $y_0 = 1$ at $t_0 = 0$. For a given step size $h$, we take $N = 1/h$ steps to reach $t_N=1$. The final value, $y_N$, is our numerical approximation, denoted $y_h(1)$. The problem specifies using step sizes $h_k = 10^{-k}$ for $k \\in \\{1, 2, 3, 4, 5\\}$.\n\nFor each $k$, we compute the following quantities:\n1.  **Numerical Error ($E_k$)**: The global error is the difference between the numerical approximation and the exact value.\n    $$\n    E_k = y_{h_k}(1) - e\n    $$\n2.  **Empirical Asymptotic Error Constant ($C_k$)**: The global truncation error of the RK4 method is of order $4$, meaning $E_k \\approx C h_k^4$ for some constant $C$ when $h_k$ is sufficiently small. We can estimate this constant for each step size:\n    $$\n    C_k = \\frac{E_k}{h_k^4}\n    $$\n    As $k$ increases, $h_k$ decreases, and $C_k$ is expected to converge to the true asymptotic error constant $C$. For the ODE $y'=y$, this constant is theoretically $C = -e/120 \\approx -0.02265234855$.\n\n3.  **Observed Order of Convergence ($p_{k,k+1}$)**: The order of convergence can be estimated by comparing the errors from two consecutive step sizes, $h_k$ and $h_{k+1}$. Under the assumption $E_h = C h^p$, the order $p$ can be found from the ratio of errors.\n    $$\n    \\frac{|E_k|}{|E_{k+1}|} \\approx \\frac{|C h_k^p|}{|C h_{k+1}^p|} = \\left(\\frac{h_k}{h_{k+1}}\\right)^p\n    $$\n    Given $h_k = 10^{-k}$ and $h_{k+1} = 10^{-(k+1)}$, the ratio $h_k/h_{k+1} = 10$. Taking the base-$10$ logarithm on both sides gives:\n    $$\n    \\log_{10}\\left(\\frac{|E_k|}{|E_{k+1}|}\\right) \\approx p \\log_{10}(10) = p\n    $$\n    Thus, we compute the observed order for each pair $(k, k+1)$ for $k \\in \\{1, 2, 3, 4\\}$ as:\n    $$\n    p_{k,k+1} = \\log_{10}\\left(\\frac{|E_k|}{|E_{k+1}|}\\right)\n    $$\n    For a fourth-order method, we expect $p_{k,k+1}$ to approach $4$.\n\nThe implementation will consist of a function for the RK4 step, which is applied iteratively inside a loop for each specified value of $k$. The resulting approximations will be used to calculate the lists of constants $[C_1, C_2, C_3, C_4, C_5]$ and observed orders $[p_{1,2}, p_{2,3}, p_{3,4}, p_{4,5}]$, which are then concatenated and formatted for the final output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the initial value problem y'(t) = y(t) with y(0) = 1 on [0, 1]\n    using the classical fourth-order Runge-Kutta method (RK4).\n    It then computes error metrics and convergence properties.\n    \"\"\"\n\n    # Define the differential equation f(t, y) = y'(t)\n    def f_ode(t: float, y: float) - float:\n        \"\"\"The right-hand side of the ODE y' = y.\"\"\"\n        return y\n\n    def rk4_step(f, t_n: float, y_n: float, h: float) - float:\n        \"\"\"\n        Performs a single step of the classical RK4 method.\n\n        Args:\n            f: The function f(t, y) defining the ODE y' = f(t, y).\n            t_n: The current time.\n            y_n: The current value of y.\n            h: The step size.\n\n        Returns:\n            The new value y_{n+1}.\n        \"\"\"\n        k1 = f(t_n, y_n)\n        k2 = f(t_n + 0.5 * h, y_n + 0.5 * h * k1)\n        k3 = f(t_n + 0.5 * h, y_n + 0.5 * h * k2)\n        k4 = f(t_n + h, y_n + h * k3)\n        y_next = y_n + (h / 6.0) * (k1 + 2.0 * k2 + 2.0 * k3 + k4)\n        return y_next\n\n    # Define the set of parameters k for step sizes h = 10^-k\n    k_values = [1, 2, 3, 4, 5]\n    \n    # Store numerical results and step sizes\n    y_approximations = []\n    h_values = []\n    \n    # The exact value of y(1) = e\n    e_exact = np.e\n\n    # Main simulation loop\n    for k in k_values:\n        h = 10.0**(-k)\n        h_values.append(h)\n        # Number of steps to reach t=1\n        num_steps = int(round(1.0 / h))\n        \n        y = 1.0  # Initial condition y(0) = 1\n        t = 0.0  # Initial time t = 0\n\n        for _ in range(num_steps):\n            y = rk4_step(f_ode, t, y, h)\n            t += h\n            \n        y_approximations.append(y)\n    \n    # Calculate the numerical errors E_k\n    errors_E = [y_approx - e_exact for y_approx in y_approximations]\n    \n    # Calculate the empirical asymptotic error constants C_k\n    constants_C = [E / (h**4) for E, h in zip(errors_E, h_values)]\n    \n    # Calculate the observed orders of convergence p_{k,k+1}\n    orders_p = []\n    for i in range(len(k_values) - 1):\n        # p_{k,k+1} = log10(|E_k| / |E_{k+1}|)\n        order = np.log10(abs(errors_E[i]) / abs(errors_E[i+1]))\n        orders_p.append(order)\n        \n    # Concatenate the results into a single list\n    final_results = constants_C + orders_p\n    \n    # Format each number to 12 significant digits\n    def format_to_12_sig_figs(value):\n        # The 'g' format specifier with a precision of 12 rounds to\n        # 12 significant digits and automatically chooses between\n        # standard decimal and scientific notation.\n        return f\"{value:.12g}\"\n\n    formatted_strings = [format_to_12_sig_figs(res) for res in final_results]\n    \n    # Print the final output in the specified format\n    print(f\"[{','.join(formatted_strings)}]\")\n\nsolve()\n```", "id": "3213354"}, {"introduction": "While RK4 performs beautifully on well-behaved problems, real-world systems often present significant challenges. This practice introduces the concept of \"stiffness\" using the famous van der Pol oscillator, a system where different solution components evolve on vastly different time scales. You will discover that for such stiff problems, the stability of an explicit method like RK4 can demand a much smaller step size than accuracy alone would require, a crucial distinction for any computational scientist [@problem_id:3213455].", "problem": "Consider the initial value problem for the van der Pol oscillator in first-order form:\n$$\n\\frac{dx}{dt} = y, \\qquad \\frac{dy}{dt} = \\mu \\bigl(1 - x^2\\bigr)\\,y - x,\n$$\nwith initial condition $x(0) = x_0$, $y(0) = y_0$ on a finite interval $t \\in [0,T]$. You must implement the classical fourth-order Runge–Kutta method (RK4) to approximate the solution. Begin from the definition of an initial value problem and construct the RK4 update by matching the Taylor expansion of the exact flow up to order $4$ to determine the weights and stages. Do not assume any pre-given formula for the RK4 method in your design of the algorithm; instead, justify the structure of the method from first principles of local error cancellation.\n\nYour program must:\n- Implement the RK4 method for systems $\\dot{\\mathbf{z}} = \\mathbf{f}(t,\\mathbf{z})$ where $\\mathbf{z} = [x,y]^T$ and $\\mathbf{f}$ is the van der Pol right-hand side.\n- For a given step size $h$, integrate on $[0,T]$ and determine a stability flag as follows. The integration is called “stable” if throughout the computation no component of the numerical solution becomes non-finite and the Euclidean norm remains bounded by $M_{\\max}$, where $M_{\\max} = 10^{6}$. Formally, at each accepted step $t \\mapsto t+h$, the new state $\\mathbf{z}_{n+1}$ must satisfy $\\|\\mathbf{z}_{n+1}\\|_2 \\le M_{\\max}$ and all entries are finite real numbers; otherwise, the run is deemed unstable.\n- Quantify accuracy at final time $T$ using a reference solution computed with the same RK4 method but a strictly smaller step size $h_{\\mathrm{ref}}$ that satisfies\n$$\nh_{\\mathrm{ref}} = \\min\\!\\bigl(h/5,\\;10^{-3}\\bigr).\n$$\nLet $\\mathbf{z}_h(T)$ and $\\mathbf{z}_{\\mathrm{ref}}(T)$ denote the respective numerical solutions at time $T$. Define the relative final-time discrepancy\n$$\nE = \\frac{\\|\\mathbf{z}_h(T) - \\mathbf{z}_{\\mathrm{ref}}(T)\\|_2}{\\max\\bigl(1,\\;\\|\\mathbf{z}_{\\mathrm{ref}}(T)\\|_2\\bigr)}.\n$$\nAn RK4 run at step size $h$ is called “accurate” if $E \\le \\tau$ with tolerance $\\tau = 10^{-1}$, provided that both the $h$-run and the reference run are stable. If the reference run is unstable, classify the case as unstable.\n\nClassify each test case into one of three integer labels:\n- $0$: unstable (the $h$-run is unstable, or the reference run is unstable),\n- $1$: stable but inaccurate (the $h$-run is stable, the reference run is stable, and $E  \\tau$),\n- $2$: stable and accurate (the $h$-run is stable, the reference run is stable, and $E \\le \\tau$).\n\nUse the following test suite, each specified by $(\\mu, T, h, x_0, y_0)$:\n- Test 1: $(\\mu, T, h, x_0, y_0) = (1.0, 10.0, 0.1, 2.0, 0.0)$.\n- Test 2: $(\\mu, T, h, x_0, y_0) = (10.0, 10.0, 0.05, 2.0, 0.0)$.\n- Test 3: $(\\mu, T, h, x_0, y_0) = (100.0, 2.0, 0.005, 2.0, 0.0)$.\n- Test 4: $(\\mu, T, h, x_0, y_0) = (100.0, 2.0, 0.0005, 2.0, 0.0)$.\n- Test 5: $(\\mu, T, h, x_0, y_0) = (100.0, 2.0, 0.0001, 2.0, 0.0)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., “[2,1,0,1,2]”). Each entry must be the integer label for the corresponding test in the order given above.\n\nNotes:\n- All computations are non-dimensional; there are no physical units.\n- Angles are not used.\n- The strict stability bound $M_{\\max} = 10^{6}$ and tolerance $\\tau = 10^{-1}$ must be used exactly as specified.", "solution": "The user-provided problem has been validated and is determined to be a valid, well-posed, and scientifically grounded task in the field of numerical analysis. All necessary data and definitions are provided, and there are no internal contradictions or ambiguities. The problem requires the implementation and analysis of the classical fourth-order Runge-Kutta method for a standard dynamical system, which is a canonical exercise in scientific computing. The derivation of the method from first principles is explicitly requested, which adds a layer of conceptual depth that is appropriate for the context.\n\n### Principle-Based Design of the RK4 Algorithm\n\nThe problem at hand is to find a numerical approximation to the solution of an initial value problem (IVP) for a system of ordinary differential equations (ODEs). The general form of such a problem is\n$$\n\\frac{d\\mathbf{z}}{dt} = \\mathbf{f}(t, \\mathbf{z}), \\qquad \\mathbf{z}(t_0) = \\mathbf{z}_0\n$$\nwhere $\\mathbf{z}(t)$ is a vector-valued function of time $t$. The van der Pol oscillator is an autonomous system, meaning $\\mathbf{f}$ does not explicitly depend on $t$, so we can write $\\frac{d\\mathbf{z}}{dt} = \\mathbf{f}(\\mathbf{z})$. The core idea of a numerical method is to approximate the state $\\mathbf{z}(t_{n+1})$ given the state $\\mathbf{z}_n \\approx \\mathbf{z}(t_n)$ at time $t_n$, where $t_{n+1} = t_n + h$ and $h$ is the step size.\n\nThe foundation for high-order methods like the Runge-Kutta method is the Taylor series expansion of the exact solution $\\mathbf{z}(t)$ around $t_n$:\n$$\n\\mathbf{z}(t_n + h) = \\mathbf{z}(t_n) + h \\mathbf{z}'(t_n) + \\frac{h^2}{2!} \\mathbf{z}''(t_n) + \\frac{h^3}{3!} \\mathbf{z}'''(t_n) + \\frac{h^4}{4!} \\mathbf{z}''''(t_n) + \\mathcal{O}(h^5)\n$$\nThe derivatives of $\\mathbf{z}$ can be expressed in terms of $\\mathbf{f}$ and its derivatives using the chain rule. This becomes algebraically complex very quickly, as it involves Jacobians and higher-order tensors. The genius of the Runge-Kutta approach is to approximate this expansion without explicitly computing derivatives of $\\mathbf{f}$. Instead, it uses multiple evaluations of $\\mathbf{f}$ within a single step.\n\nA general $s$-stage explicit Runge-Kutta method takes the form:\n$$\n\\mathbf{z}_{n+1} = \\mathbf{z}_n + h \\sum_{i=1}^s b_i \\mathbf{k}_i\n$$\nwhere the stages $\\mathbf{k}_i$ are intermediate estimates of the derivative:\n$$\n\\begin{aligned}\n\\mathbf{k}_1 = \\mathbf{f}(t_n, \\mathbf{z}_n) \\\\\n\\mathbf{k}_2 = \\mathbf{f}\\left(t_n + c_2 h, \\mathbf{z}_n + h a_{21} \\mathbf{k}_1\\right) \\\\\n\\mathbf{k}_3 = \\mathbf{f}\\left(t_n + c_3 h, \\mathbf{z}_n + h (a_{31} \\mathbf{k}_1 + a_{32} \\mathbf{k}_2)\\right) \\\\\n\\vdots \\\\\n\\mathbf{k}_s = \\mathbf{f}\\left(t_n + c_s h, \\mathbf{z}_n + h \\sum_{j=1}^{s-1} a_{sj} \\mathbf{k}_j\\right)\n\\end{aligned}\n$$\nThe coefficients $b_i$ (weights), $c_i$ (nodes), and $a_{ij}$ (the RK matrix) are chosen to make the Taylor expansion of $\\mathbf{z}_{n+1}$ match the expansion of the exact solution $\\mathbf{z}(t_n+h)$ to the highest possible order in $h$. The problem requires a fourth-order method ($p=4$), which means matching terms up to and including the $h^4$ term.\n\nTo achieve this, each stage $\\mathbf{k}_i$ is expanded in a multivariate Taylor series around $(t_n, \\mathbf{z}_n)$. These expansions are then substituted into the formula for $\\mathbf{z}_{n+1}$. The resulting polynomial in $h$ is compared term-by-term with the exact Taylor series expansion. This matching process yields a system of nonlinear algebraic equations for the coefficients, known as the \"order conditions\". For order $p=4$, the conditions are:\n\\begin{itemize}\n    \\item Order 1 (1 equation): $\\sum_{i=1}^s b_i = 1$\n    \\item Order 2 (1 equation): $\\sum_{i=1}^s b_i c_i = \\frac{1}{2}$\n    \\item Order 3 (2 equations): $\\sum_{i=1}^s b_i c_i^2 = \\frac{1}{3}$ and $\\sum_{i,j=1}^s b_i a_{ij} c_j = \\frac{1}{6}$\n    \\item Order 4 (4 equations): $\\sum_{i=1}^s b_i c_i^3 = \\frac{1}{4}$, $\\sum_{i,j,k=1}^s b_i a_{ij} a_{jk} c_k = \\frac{1}{24}$, $\\sum_{i,j=1}^s b_i c_i a_{ij} c_j = \\frac{1}{8}$, and $\\sum_{i,j=1}^s b_i a_{ij} c_j^2 = \\frac{1}{12}$\n\\end{itemize}\nA minimum of $s=4$ stages are needed to satisfy these 8 equations. This yields a family of possible solutions for the coefficients. The \"classical\" fourth-order Runge-Kutta method (RK4) corresponds to a specific, widely used choice of these coefficients derived under simplifying assumptions (e.g., $c_2 = a_{21}$, $c_3 = a_{31} + a_{32}$, etc.). This choice is:\n$$\n\\begin{alignedat}{4}\nc_2 = \\frac{1}{2},  \\qquad c_3 = \\frac{1}{2},  \\qquad c_4 = 1 \\\\\na_{21} = \\frac{1}{2} \\\\\na_{31} = 0,  \\qquad a_{32} = \\frac{1}{2} \\\\\na_{41} = 0,  \\qquad a_{42} = 0,  \\qquad a_{43} = 1 \\\\\nb_1 = \\frac{1}{6},  \\qquad b_2 = \\frac{1}{3},  \\qquad b_3 = \\frac{1}{3},  \\qquad b_4 = \\frac{1}{6}\n\\end{alignedat}\n$$\nSubstituting these coefficients into the general form yields the well-known algorithm for a single step from $\\mathbf{z}_n$ to $\\mathbf{z}_{n+1}$:\n$$\n\\begin{aligned}\n\\mathbf{k}_1 = \\mathbf{f}(t_n, \\mathbf{z}_n) \\\\\n\\mathbf{k}_2 = \\mathbf{f}\\left(t_n + \\frac{h}{2}, \\mathbf{z}_n + \\frac{h}{2} \\mathbf{k}_1\\right) \\\\\n\\mathbf{k}_3 = \\mathbf{f}\\left(t_n + \\frac{h}{2}, \\mathbf{z}_n + \\frac{h}{2} \\mathbf{k}_2\\right) \\\\\n\\mathbf{k}_4 = \\mathbf{f}\\left(t_n + h, \\mathbf{z}_n + h \\mathbf{k}_3\\right) \\\\\n\\mathbf{z}_{n+1} = \\mathbf{z}_n + \\frac{h}{6} (\\mathbf{k}_1 + 2\\mathbf{k}_2 + 2\\mathbf{k}_3 + \\mathbf{k}_4)\n\\end{aligned}\n$$\nThe local truncation error of this method is $\\mathcal{O}(h^5)$, and thus its global error is $\\mathcal{O}(h^4)$.\n\nFor the van der Pol oscillator, the state vector and the right-hand side function are:\n$$\n\\mathbf{z} = \\begin{bmatrix} x \\\\ y \\end{bmatrix}, \\qquad \\mathbf{f}(\\mathbf{z}, \\mu) = \\begin{bmatrix} y \\\\ \\mu(1-x^2)y - x \\end{bmatrix}\n$$\nThis $\\mathbf{f}$ is used in the RK4 stage calculations.\n\n### Implementation and Classification Strategy\n\nThe overall program is structured to execute the RK4 method for each test case and classify the result.\n1.  **RK4 Integrator**: A function `integrate_rk4` is implemented. It takes the parameters $(\\mu, T, h, \\mathbf{z}_0)$ and iteratively applies the RK4 update rule from $t=0$ to $t=T$. At each step, it calculates $\\mathbf{z}_{n+1}$ and performs a stability check. If $\\mathbf{z}_{n+1}$ contains non-finite numbers (NaN or Inf) or if its Euclidean norm $\\|\\mathbf{z}_{n+1}\\|_2$ exceeds the threshold $M_{\\max} = 10^6$, the integration is terminated, and an \"unstable\" flag is returned.\n2.  **Reference Solution**: For each test case with step size $h$, a high-fidelity reference solution is computed using the same RK4 integrator but with a much smaller step size, $h_{\\mathrm{ref}} = \\min(h/5, 10^{-3})$. The stability of this reference run is crucial; if the reference integration fails, the test case is immediately classified as unstable (label $0$).\n3.  **Accuracy Assessment**: If both the primary run (with step $h$) and the reference run (with step $h_{\\mathrm{ref}}$) complete successfully, their final states at time $T$, denoted $\\mathbf{z}_h(T)$ and $\\mathbf{z}_{\\mathrm{ref}}(T)$, are compared. The relative final-time discrepancy $E$ is calculated as:\n    $$\n    E = \\frac{\\|\\mathbf{z}_h(T) - \\mathbf{z}_{\\mathrm{ref}}(T)\\|_2}{\\max\\bigl(1,\\;\\|\\mathbf{z}_{\\mathrm{ref}}(T)\\|_2\\bigr)}\n    $$\n4.  **Classification**: Based on the outcomes, each case is assigned an integer label:\n    - `0`: If the $h$-run is unstable or the $h_{\\mathrm{ref}}$-run is unstable.\n    - `1`: If both runs are stable but the discrepancy $E$ is greater than the tolerance $\\tau = 10^{-1}$. This indicates the solution is stable but inaccurate.\n    - `2`: If both runs are stable and the discrepancy $E$ is less than or equal to $\\tau = 10^{-1}$. This indicates the solution is both stable and accurate.\n\nThis procedure is systematically applied to all provided test cases to generate the final list of classification labels.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the van der Pol oscillator problem for a suite of test cases,\n    classifying each based on numerical stability and accuracy.\n    \"\"\"\n    # Define constants from the problem statement.\n    M_MAX = 1.0e6\n    TAU = 1.0e-1\n\n    def van_der_pol_rhs(z, mu):\n        \"\"\"\n        Computes the right-hand side of the van der Pol oscillator system.\n        z is the state vector [x, y].\n        mu is the nonlinearity parameter.\n        \"\"\"\n        x, y = z\n        dxdt = y\n        dydt = mu * (1.0 - x**2) * y - x\n        return np.array([dxdt, dydt])\n\n    def integrate_rk4(mu, T, h, z0):\n        \"\"\"\n        Integrates the van der Pol system using the classical fourth-order Runge-Kutta method.\n        \n        Args:\n            mu (float): The nonlinearity parameter.\n            T (float): The final integration time.\n            h (float): The step size.\n            z0 (np.ndarray): The initial state vector [x0, y0].\n            \n        Returns:\n            A tuple (z_final, is_stable).\n            z_final is the state at time T, or the last state before instability.\n            is_stable is a boolean flag indicating if the integration completed stably.\n        \"\"\"\n        # The number of steps is guaranteed to be an integer for the given test cases.\n        num_steps = int(round(T / h))\n        z = np.copy(z0).astype(np.float64)\n\n        for _ in range(num_steps):\n            # RK4 stages\n            k1 = van_der_pol_rhs(z, mu)\n            k2 = van_der_pol_rhs(z + 0.5 * h * k1, mu)\n            k3 = van_der_pol_rhs(z + 0.5 * h * k2, mu)\n            k4 = van_der_pol_rhs(z + h * k3, mu)\n\n            # Update step\n            z_next = z + (h / 6.0) * (k1 + 2.0 * k2 + 2.0 * k3 + k4)\n\n            # Stability check as per problem definition\n            if not np.all(np.isfinite(z_next)) or np.linalg.norm(z_next)  M_MAX:\n                return z_next, False  # Unstable\n\n            z = z_next\n\n        return z, True  # Stable\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (mu, T, h, x0, y0)\n        (1.0, 10.0, 0.1, 2.0, 0.0),\n        (10.0, 10.0, 0.05, 2.0, 0.0),\n        (100.0, 2.0, 0.005, 2.0, 0.0),\n        (100.0, 2.0, 0.0005, 2.0, 0.0),\n        (100.0, 2.0, 0.0001, 2.0, 0.0),\n    ]\n\n    results = []\n    for mu, T, h, x0, y0 in test_cases:\n        z0 = np.array([x0, y0])\n\n        # --- Reference run ---\n        # Compute the reference step size h_ref.\n        h_ref = min(h / 5.0, 1.0e-3)\n        z_ref_T, is_ref_stable = integrate_rk4(mu, T, h_ref, z0)\n        \n        if not is_ref_stable:\n            results.append(0)  # Unstable if reference run is unstable\n            continue\n\n        # --- Main run ---\n        z_h_T, is_h_stable = integrate_rk4(mu, T, h, z0)\n        \n        if not is_h_stable:\n            results.append(0)  # Unstable if h-run is unstable\n            continue\n            \n        # --- Both runs are stable, perform accuracy check ---\n        norm_z_ref_T = np.linalg.norm(z_ref_T)\n        diff_norm = np.linalg.norm(z_h_T - z_ref_T)\n        \n        # Calculate relative final-time discrepancy E.\n        E = diff_norm / max(1.0, norm_z_ref_T)\n        \n        if E = TAU:\n            results.append(2)  # Stable and accurate\n        else:\n            results.append(1)  # Stable but inaccurate\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3213455"}, {"introduction": "The previous exercises demonstrate that a fixed step size is often inefficient; it may be too large for accuracy in some regions and wastefully small in others. This final practice introduces a powerful solution: estimating the error on the fly using \"step-doubling.\" This exercise [@problem_id:3213350] will not only show you how to estimate the local error of an RK4 step but also how to use that information via Richardson extrapolation to construct a more accurate, fifth-order approximation, paving the way towards adaptive step-size control.", "problem": "Consider the initial value problem (IVP) defined by the ordinary differential equation $y'(t) = f(t,y(t))$ with initial condition $y(t_0) = y_0$. The classical fourth-order Runge–Kutta method (RK4) provides a one-step approximation to $y(t_0 + h)$ by combining multiple evaluations of $f$ within the interval $\\left[t_0, t_0 + h\\right]$. The local truncation error of a one-step RK4 update scales predictably with the step size $h$. Step-doubling compares one step of size $h$ with two successive steps of size $h/2$ to estimate the local error and, using the scaling law, construct a linear combination that cancels the leading error term, yielding a fifth-order accurate approximation via Richardson extrapolation. Angles in trigonometric functions must be interpreted in radians.\n\nYour task is to implement the classical fourth-order Runge–Kutta method (without using any prebuilt ODE libraries), use step-doubling to estimate the local truncation error of the single step of size $h$, and derive a Richardson-extrapolated fifth-order approximation at $t_0 + h$. For each test case below, compute:\n1. The Richardson-extrapolated value at $t_0 + h$ (a fifth-order approximation).\n2. The magnitude of the estimated local truncation error for the single RK4 step of size $h$ obtained via step-doubling.\n3. The magnitude of the actual error of the Richardson-extrapolated value, computed against the exact solution at $t_0 + h$.\n\nUse the following test suite. All constants and step sizes are specified as pure numbers without physical units. For trigonometric functions, angles are in radians.\n\n- Test Case A (happy path): $f(t,y) = y$, $t_0 = 0$, $y_0 = 1$, $h = 0.1$. Exact solution: $y(t) = e^{t}$.\n- Test Case B (linear, nonhomogeneous): $f(t,y) = -2y + t$, $t_0 = 0$, $y_0 = 0$, $h = 0.2$. Exact solution: $y(t) = \\frac{1}{2}t - \\frac{1}{4} + \\frac{1}{4}e^{-2t}$.\n- Test Case C (trigonometric forcing, radians): $f(t,y) = \\cos(t)$, $t_0 = 0$, $y_0 = 0$, $h = 0.5$. Exact solution: $y(t) = \\sin(t)$.\n- Test Case D (nonlinear, rational exact solution): $f(t,y) = y^2$, $t_0 = 0$, $y_0 = \\frac{1}{2}$, $h = 0.05$. Exact solution: $y(t) = \\frac{1}{2 - t}$.\n\nYour program must:\n- Implement the classical fourth-order Runge–Kutta method for one step from $(t_0,y_0)$ to $(t_0+h)$.\n- Perform step-doubling (two successive RK4 steps of size $h/2$) to approximate $y(t_0 + h)$ a second way.\n- Use the step-doubling results to estimate the local truncation error of the single step of size $h$ and to construct a fifth-order accurate Richardson-extrapolated approximation at $t_0 + h$ by canceling the leading error term based on the scaling law.\n- Compute the exact value at $t_0 + h$ for each test case, and the magnitude of the actual error of the Richardson-extrapolated value.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- For each test case, append the following three values to the output list, in this exact order: the Richardson-extrapolated value at $t_0 + h$, the magnitude of the estimated local truncation error for the single step of size $h$, and the magnitude of the actual error of the Richardson-extrapolated value.\n- Thus, with $4$ test cases, the final output should be a single flat list of $12$ floating-point numbers: $\\left[ \\text{A}_1, \\text{A}_2, \\text{A}_3, \\text{B}_1, \\text{B}_2, \\text{B}_3, \\text{C}_1, \\text{C}_2, \\text{C}_3, \\text{D}_1, \\text{D}_2, \\text{D}_3 \\right]$, where for each case the indices $1,2,3$ correspond to the three quantities specified above.\n\nNo input should be read from standard input, and no files or networks should be accessed. All numerical answers must be reported as plain floating-point numbers.", "solution": "The problem as stated is valid. It is a well-posed problem in numerical analysis, providing all necessary definitions, initial conditions, and functions to carry out the required computations. The problem is scientifically grounded in the theory of numerical solutions to ordinary differential equations, specifically the Runge-Kutta methods and Richardson extrapolation.\n\nThe task is to analyze the initial value problem (IVP) given by $y'(t) = f(t,y(t))$ with an initial condition $y(t_0) = y_0$. We will use the classical fourth-order Runge-Kutta method (RK4) to approximate the solution at $t_0 + h$. Step-doubling will be employed to estimate the local truncation error and to construct a higher-order approximation via Richardson extrapolation.\n\n### The Classical Fourth-Order Runge-Kutta (RK4) Method\n\nThe RK4 method is a one-step numerical procedure for solving ODEs. To advance the solution from $(t_n, y_n)$ by a step size $h$, we compute the next value $y_{n+1} \\approx y(t_n+h)$ using a weighted average of four slope estimates within the interval $[t_n, t_n+h]$. The formula is:\n$$ y_{n+1} = y_n + \\frac{h}{6} (k_1 + 2k_2 + 2k_3 + k_4) $$\nwhere the slope estimates are defined as:\n\\begin{align*}\nk_1 = f(t_n, y_n) \\\\\nk_2 = f\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2}k_1\\right) \\\\\nk_3 = f\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2}k_2\\right) \\\\\nk_4 = f(t_n + h, y_n + hk_3)\n\\end{align*}\nThe RK4 method is known for its balance of accuracy and computational cost. The local truncation error (LTE), which is the error introduced in a single step assuming the starting point is exact, is of order $h^5$, i.e., $\\text{LTE} = O(h^5)$. This means the global error over an interval scales as $O(h^4)$, hence the name \"fourth-order\" method.\n\n### Step-Doubling and Richardson Extrapolation\n\nTo analyze and improve the accuracy of the RK4 approximation at $t_0+h$, we compute it in two different ways:\n1.  A single \"coarse\" step of size $h$, starting from $(t_0, y_0)$, yielding an approximation we denote as $y_1$.\n2.  Two successive \"fine\" steps, each of size $h/2$. The first step takes us from $(t_0, y_0)$ to $(t_0+h/2, y_{1/2})$, and the second step takes us from $(t_0+h/2, y_{1/2})$ to $t_0+h$. This two-step sequence yields a final approximation we denote as $y_2$.\n\nLet $Y(t_0+h)$ be the exact solution at $t_0+h$. The relationship between the exact solution and our numerical approximations can be written by expanding the local truncation error:\n$$ Y(t_0+h) = y_1 + C h^5 + O(h^6) \\quad (1) $$\nFor the two finer steps, the error accumulates. The total leading error is the sum of the errors from each step of size $h/2$:\n$$ Y(t_0+h) = y_2 + 2 \\cdot C \\left(\\frac{h}{2}\\right)^5 + O(h^6) = y_2 + \\frac{C h^5}{16} + O(h^6) \\quad (2) $$\nwhere $C$ is a constant that depends on the function $f$ and its derivatives at $t_0$, but is independent of $h$.\n\n### Derivation of Required Quantities\n\nWith equations $(1)$ and $(2)$, we can derive expressions for the three values requested by the problem.\n\n1.  **Richardson-Extrapolated Value ($y_{extrap}$)**:\n    Our goal is to find a linear combination of $y_1$ and $y_2$ that eliminates the leading error term, $C h^5$. We can algebraically manipulate equations $(1)$ and $(2)$. First, multiply equation $(2)$ by $16$:\n    $$ 16 Y(t_0+h) = 16 y_2 + C h^5 + O(h^6) $$\n    Now, subtract equation $(1)$ from this result:\n    $$ 15 Y(t_0+h) = (16 y_2 - y_1) - O(h^6) $$\n    Solving for $Y(t_0+h)$ gives a higher-order approximation:\n    $$ Y(t_0+h) = \\frac{16 y_2 - y_1}{15} + O(h^6) $$\n    The Richardson-extrapolated value, which is a fifth-order approximation, is therefore:\n    $$ y_{extrap} = \\frac{16 y_2 - y_1}{15} = y_2 + \\frac{y_2 - y_1}{15} $$\n\n2.  **Estimated Local Truncation Error of the Single Step**:\n    The problem asks for an estimate of the local truncation error for the single coarse step of size $h$. This error is $\\text{LTE}_h = Y(t_0+h) - y_1 \\approx C h^5$. To estimate this quantity, we subtract equation $(1)$ from equation $(2)$:\n    $$ 0 = (y_2 - y_1) + \\left(\\frac{C h^5}{16} - C h^5\\right) + O(h^6) $$\n    $$ y_1 - y_2 = -\\frac{15}{16} C h^5 + O(h^6) $$\n    Solving for the leading error term $C h^5$:\n    $$ C h^5 \\approx \\frac{16}{15} (y_2 - y_1) $$\n    The estimated local truncation error for the single step of size $h$ is $y_1 - Y(t_0+h) \\approx -C h^5$. Therefore, the estimate is:\n    $$ \\text{LTE}_h^{\\text{est}} = -\\frac{16}{15}(y_2-y_1) = \\frac{16}{15}(y_1-y_2) $$\n    The required quantity is the magnitude of this value: $|\\frac{16}{15}(y_1 - y_2)|$.\n\n3.  **Actual Error of the Extrapolated Value**:\n    This is a direct comparison of our best approximation, $y_{extrap}$, against the provided exact solution, $Y_{exact}(t_0+h)$. The magnitude of this error is:\n    $$ E_{actual} = |y_{extrap} - Y_{exact}(t_0+h)| $$\n\n### Computational Procedure\n\nFor each test case provided, the following algorithm is executed:\n1.  Define the function $f(t,y)$, the initial conditions $(t_0, y_0)$, the step size $h$, and the exact solution function $Y_{exact}(t)$.\n2.  Implement a function for a single RK4 step.\n3.  Calculate $y_1$ by performing one RK4 step of size $h$ from $(t_0, y_0)$.\n4.  Calculate $y_2$ by performing two successive RK4 steps of size $h/2$.\n5.  Compute the three required outputs using the formulas derived above: $y_{extrap}$, $|\\frac{16}{15}(y_1 - y_2)|$, and $|y_{extrap} - Y_{exact}(t_0+h)|$.\n6.  Collect these three results for each test case and format them into a single list for the final output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It implements the classical RK4 method, step-doubling, and Richardson extrapolation.\n    \"\"\"\n\n    def rk4_step(f, t, y, h):\n        \"\"\"\n        Performs a single step of the classical fourth-order Runge-Kutta method.\n\n        Args:\n            f: The function dy/dt = f(t, y).\n            t: The current time.\n            y: The current value of y.\n            h: The step size.\n\n        Returns:\n            The approximated value of y at t + h.\n        \"\"\"\n        k1 = f(t, y)\n        k2 = f(t + 0.5 * h, y + 0.5 * h * k1)\n        k3 = f(t + 0.5 * h, y + 0.5 * h * k2)\n        k4 = f(t + h, y + h * k3)\n        return y + (h / 6.0) * (k1 + 2.0 * k2 + 2.0 * k3 + k4)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"id\": \"A\",\n            \"f\": lambda t, y: y,\n            \"t0\": 0.0,\n            \"y0\": 1.0,\n            \"h\": 0.1,\n            \"y_exact\": lambda t: np.exp(t),\n        },\n        {\n            \"id\": \"B\",\n            \"f\": lambda t, y: -2.0 * y + t,\n            \"t0\": 0.0,\n            \"y0\": 0.0,\n            \"h\": 0.2,\n            \"y_exact\": lambda t: 0.5 * t - 0.25 + 0.25 * np.exp(-2.0 * t),\n        },\n        {\n            \"id\": \"C\",\n            \"f\": lambda t, y: np.cos(t),\n            \"t0\": 0.0,\n            \"y0\": 0.0,\n            \"h\": 0.5,\n            \"y_exact\": lambda t: np.sin(t),\n        },\n        {\n            \"id\": \"D\",\n            \"f\": lambda t, y: y**2,\n            \"t0\": 0.0,\n            \"y0\": 0.5,\n            \"h\": 0.05,\n            \"y_exact\": lambda t: 1.0 / (2.0 - t),\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        f = case[\"f\"]\n        t0 = case[\"t0\"]\n        y0 = case[\"y0\"]\n        h = case[\"h\"]\n        y_exact_func = case[\"y_exact\"]\n\n        # 1. One coarse step of size h\n        y1 = rk4_step(f, t0, y0, h)\n\n        # 2. Two fine steps of size h/2\n        h_half = h / 2.0\n        y_half = rk4_step(f, t0, y0, h_half)\n        y2 = rk4_step(f, t0 + h_half, y_half, h_half)\n\n        # 3. Compute the three required quantities\n        \n        # Quantity 1: Richardson-extrapolated value (5th order approximation)\n        # y_richardson = y2 + (y2 - y1) / 15.0\n        y_richardson = (16.0 * y2 - y1) / 15.0\n        \n        # Quantity 2: Magnitude of estimated LTE for the single step of size h\n        # Estimated error of the coarse step y1 is (16/15)*(y1 - y2)\n        est_err_mag_y1 = abs((16.0 / 15.0) * (y1 - y2))\n\n        # Quantity 3: Magnitude of actual error of the Richardson-extrapolated value\n        t_final = t0 + h\n        y_true_final = y_exact_func(t_final)\n        actual_err_mag_richardson = abs(y_richardson - y_true_final)\n        \n        results.append(y_richardson)\n        results.append(est_err_mag_y1)\n        results.append(actual_err_mag_richardson)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3213350"}]}