{"hands_on_practices": [{"introduction": "When solving differential equations numerically, our intuition might suggest that smaller step sizes always lead to more accurate results. However, the digital world of computers introduces a competing factor: floating-point round-off error. This first practice invites you to explore the fundamental trade-off between the mathematical truncation error of a method, which decreases with step size $h$, and the computational round-off error, which accumulates with the number of steps and thus tends to increase as $h$ gets smaller. By numerically solving an IVP with the RK4 method over a range of fixed step sizes [@problem_id:3259605], you will discover that there is often an optimal step size that minimizes the total error, providing a crucial insight into the practical limits of numerical computation.", "problem": "Consider the initial value problem (IVP) defined by the ordinary differential equation $y^{\\prime}(t) = f(t,y)$ with initial condition $y(t_{0}) = y_{0}$. A one-step method with fixed step size $h$ advances an approximation $y_{n}$ at $t_{n}$ to $y_{n+1}$ at $t_{n+1} = t_{n} + h$ using only information from the current step. For such methods, the total error at a fixed final time $T$ typically arises from two sources: truncation error due to the method’s finite order and floating-point round-off error from finite-precision arithmetic. The truncation error depends on $h$ through the method’s order and the smoothness of $f$, while round-off error depends on the number of steps $N = T/h$ and the arithmetic used. Your task is to numerically find, for specific IVPs and a single one-step method, the fixed step size $h$ that minimizes the absolute final error by balancing truncation and floating-point round-off error.\n\nFundamental base to rely on:\n- The IVP definition $y^{\\prime}(t) = f(t,y)$ with $y(t_{0}) = y_{0}$.\n- The definition of a one-step method, such as the classical fourth-order Runge–Kutta method, which constructs $y_{n+1}$ from $(t_{n}, y_{n})$ and $h$ using a fixed formula.\n- The well-tested fact that, for a method of order $p$ applied on a fixed interval, the global truncation error scales like a constant times $h^{p}$ when $h$ is sufficiently small and the solution is smooth.\n- The well-tested fact that floating-point round-off errors accumulate over many arithmetic operations, and the number of steps $N$ scales like $T/h$ for a fixed interval $[0,T]$.\n\nYour program must use the classical fourth-order Runge–Kutta (RK4) method as the one-step integrator for all test cases. You must not use any built-in ordinary differential equation solvers. For each specified test case, you must:\n- Integrate from $t_{0}$ to $T$ with a fixed step size $h = T/N$ for each $N$ in a provided candidate list.\n- Compute the absolute final error $\\left|y_{N} - y_{\\text{true}}(T)\\right|$ using the exact solution $y_{\\text{true}}(t)$ provided for that test case.\n- Choose the step size $h^{\\star}$ among the candidates that minimizes the absolute final error. If two or more candidates produce final errors that are numerically indistinguishable within an absolute tolerance of $10^{-18}$, select the largest $h$ among those tied candidates.\n\nAngle unit: whenever $\\sin(\\cdot)$ or $\\cos(\\cdot)$ appear, interpret their arguments in radians.\n\nUse double-precision floating-point arithmetic. No randomness is allowed.\n\nTest suite:\n- Case $1$ (growth, interior minimum expected):\n  - $f(t,y) = y$, $y(0) = 1$, $T = 50$. Exact solution $y_{\\text{true}}(t) = e^{t}$.\n- Case $2$ (decay, boundary behavior expected):\n  - $f(t,y) = -2\\,y$, $y(0) = 1$, $T = 50$. Exact solution $y_{\\text{true}}(t) = e^{-2t}$.\n- Case $3$ (forced linear, bounded solution):\n  - $f(t,y) = \\sin(t) - y$, $y(0) = 1$, $T = 30$. Exact solution $y_{\\text{true}}(t) = \\tfrac{3}{2} e^{-t} + \\tfrac{1}{2}(\\sin t - \\cos t)$.\n\nFor each case, evaluate the following candidate subdivisions $N \\in \\{50, 100, 200, 400, 800, 1600, 3200, 6400, 12800, 25600, 51200\\}$ and thus $h = T/N$. For each $N$, perform exactly $N$ fixed-size RK4 steps to reach time $T$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases above. Specifically, the output must be the list $[h^{\\star}_{1}, h^{\\star}_{2}, h^{\\star}_{3}]$, where each $h^{\\star}_{i}$ is the chosen optimal $h$ for Case $i$. The entries must be printed as decimal floating-point numbers. No units are involved.\n\nImportant constraints:\n- Implement the RK4 one-step method explicitly. Do not reuse any library time-steppers.\n- Ensure that every candidate $h$ produces exactly $N$ uniform steps so that the final time is exactly $T$.\n- Use the exact solutions given above to compute $\\left|y_{N} - y_{\\text{true}}(T)\\right|$ accurately.\n\nThe final output format must be exactly one line: for example, $[h^{\\star}_{1},h^{\\star}_{2},h^{\\star}_{3}]$.", "solution": "The user has provided a problem that requires finding the optimal fixed step size $h$ for solving several initial value problems (IVPs) using the classical fourth-order Runge-Kutta (RK4) method. The optimization criterion is the minimization of the absolute error at a fixed final time $T$, which involves balancing the method's truncation error against the accumulated floating-point round-off error.\n\n### Step 1: Extract Givens\n\n- **IVP Definition**: $y^{\\prime}(t) = f(t,y)$ with initial condition $y(t_{0}) = y_{0}$.\n- **Numerical Method**: The classical fourth-order Runge-Kutta (RK4) method is to be used.\n- **Task**: For a given list of candidate step counts $N$, find the step size $h = T/N$ that minimizes the absolute final error $\\left|y_{N} - y_{\\text{true}}(T)\\right|$.\n- **Candidate Subdivisions $N$**: The set of candidates for the number of steps is $N \\in \\{50, 100, 200, 400, 800, 1600, 3200, 6400, 12800, 25600, 51200\\}$.\n- **Tie-Breaking Rule**: If multiple candidates for $h$ yield a minimum error (values are within an absolute tolerance of $10^{-18}$), the largest such $h$ must be chosen.\n- **Arithmetic**: Double-precision floating-point arithmetic.\n- **Test Case 1**:\n  - $f(t,y) = y$\n  - $y(0) = 1$\n  - $T = 50$\n  - Exact solution: $y_{\\text{true}}(t) = e^{t}$\n- **Test Case 2**:\n  - $f(t,y) = -2y$\n  - $y(0) = 1$\n  - $T = 50$\n  - Exact solution: $y_{\\text{true}}(t) = e^{-2t}$\n- **Test Case 3**:\n  - $f(t,y) = \\sin(t) - y$\n  - $y(0) = 1$\n  - $T = 30$\n  - Exact solution: $y_{\\text{true}}(t) = \\tfrac{3}{2} e^{-t} + \\tfrac{1}{2}(\\sin t - \\cos t)$\n- **Angle Units**: Arguments of trigonometric functions are in radians.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is reviewed against the validation criteria:\n\n- **Scientifically Grounded**: The problem is fundamentally sound. It explores a classic concept in numerical analysis: the trade-off between truncation error, which decreases with smaller step sizes ($h$), and round-off error, which accumulates with the number of computational steps ($N = T/h$). The RK4 method is a standard, well-understood algorithm for solving ODEs. The chosen ODEs are canonical examples with known analytical solutions, suitable for error analysis.\n- **Well-Posed**: The problem is well-posed. It provides all necessary components: the differential equations, initial conditions, a specific numerical method, a discrete set of step sizes to test, and an unambiguous objective function (minimizing absolute final error). The explicit tie-breaking rule ensures a unique solution.\n- **Objective**: The problem is stated in objective, mathematical terms. The evaluation is based on quantitative error calculation, leaving no room for subjective interpretation.\n\nAll other checks (completeness, consistency, feasibility, etc.) are passed. The problem does not exhibit any of the defined flaws.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. A solution will be provided.\n\n### Principle-Based Design of the Solution\n\nThe core of the problem is to implement a numerical experiment. For each of the three IVPs, we must simulate the solution's evolution using the RK4 method over a range of step sizes and identify which step size yields the most accurate result at the final time $T$.\n\n**The Fourth-Order Runge-Kutta (RK4) Method**\nGiven an IVP $y'(t) = f(t,y)$, $y(t_n) = y_n$, the RK4 method computes the approximation $y_{n+1}$ at time $t_{n+1} = t_n + h$ using the following sequence of calculations:\n$$\nk_1 = f(t_n, y_n)\n$$\n$$\nk_2 = f\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2}k_1\\right)\n$$\n$$\nk_3 = f\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2}k_2\\right)\n$$\n$$\nk_4 = f\\left(t_n + h, y_n + hk_3\\right)\n$$\nThe next value $y_{n+1}$ is then given by a weighted average of these intermediate slopes:\n$$\ny_{n+1} = y_n + \\frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4)\n$$\nThis formula is applied iteratively $N$ times to advance the solution from the initial condition $(t_0, y_0)$ to the final time $T = t_0 + N \\cdot h$.\n\n**Error Dynamics**\nThe total absolute error at the final time $T$, denoted $E(h)$, is a combination of two primary error sources:\n1.  **Global Truncation Error ($E_T$)**: This error is inherent to the method's approximation of the true solution. For RK4, which is a fourth-order method, this error scales with the fourth power of the step size, i.e., $E_T \\propto h^4$. As $h \\to 0$, this error diminishes rapidly.\n2.  **Round-off Error ($E_R$)**: This error arises from the finite precision of floating-point arithmetic. Each step of the RK4 method introduces a small round-off error. These errors accumulate over the $N = (T-t_0)/h$ steps. A simplified model suggests that the total round-off error grows with the number of steps, so $E_R \\propto N = 1/h$.\n\nThe total error can be modeled as $E(h) \\approx C_1 h^4 + C_2/h$. This function has a characteristic shape: it is large for large $h$ (where truncation error dominates), decreases as $h$ is reduced, reaches a minimum at an optimal step size $h^{\\star}$, and then increases again for very small $h$ (where round-off error begins to dominate). Our task is to find this $h^{\\star}$ empirically from a discrete set of candidates.\n\n**Computational Procedure**\nFor each of the three test cases, the following algorithm is executed:\n1.  The true solution at the final time, $y_{\\text{true}}(T)$, is computed once for reference.\n2.  A loop iterates through each candidate number of steps $N$ from the provided list $\\{50, 100, \\dots, 51200\\}$.\n3.  Inside the loop, for each $N$:\n    a. The step size $h$ is calculated as $h = (T - t_0) / N$. Note that for all cases, $t_0=0$, so $h = T/N$.\n    b. The RK4 integration is performed, starting with $y_0$ at $t_0$ and executing exactly $N$ steps to obtain the numerical approximation $y_N$ at time $T$.\n    c. The absolute error is computed: $\\epsilon = |y_N - y_{\\text{true}}(T)|$.\n    d. The pair $(h, \\epsilon)$ is stored.\n4.  After testing all candidate values of $N$, the collected list of $(h, \\epsilon)$ pairs is analyzed to find the minimum error, $\\epsilon_{min}$.\n5.  In accordance with the tie-breaking rule, all candidates $(h, \\epsilon)$ where $|\\epsilon - \\epsilon_{min}| \\le 10^{-18}$ are identified.\n6.  Among these tied candidates, the one with the largest value of $h$ is selected as the optimal step size, $h^{\\star}$.\n7.  The three resulting optimal step sizes, one for each test case, are collected and formatted for the final output.\n\nThis procedure is systematically applied to determine the optimal step size for each IVP, revealing the practical effects of the error trade-off in numerical integration.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef rk4_solver(f, t0, y0, T, N):\n    \"\"\"\n    Solves an IVP y'(t) = f(t, y) using the classical RK4 method.\n\n    Args:\n        f (callable): The function f(t, y) defining the ODE.\n        t0 (float): The initial time.\n        y0 (float): The initial value y(t0).\n        T (float): The final time.\n        N (int): The number of steps to take.\n\n    Returns:\n        float: The numerical approximation of y(T).\n    \"\"\"\n    h = (T - t0) / float(N)\n    t = float(t0)\n    y = float(y0)\n\n    for _ in range(N):\n        k1 = f(t, y)\n        k2 = f(t + 0.5 * h, y + 0.5 * h * k1)\n        k3 = f(t + 0.5 * h, y + 0.5 * h * k2)\n        k4 = f(t + h, y + h * k3)\n        y += (h / 6.0) * (k1 + 2.0 * k2 + 2.0 * k3 + k4)\n        t += h\n        \n    return y\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and find the optimal step size h.\n    \"\"\"\n    test_cases = [\n        {\n            \"f\": lambda t, y: y,\n            \"y_true\": lambda t: np.exp(t),\n            \"t0\": 0.0,\n            \"y0\": 1.0,\n            \"T\": 50.0,\n        },\n        {\n            \"f\": lambda t, y: -2.0 * y,\n            \"y_true\": lambda t: np.exp(-2.0 * t),\n            \"t0\": 0.0,\n            \"y0\": 1.0,\n            \"T\": 50.0,\n        },\n        {\n            \"f\": lambda t, y: np.sin(t) - y,\n            \"y_true\": lambda t: 1.5 * np.exp(-t) + 0.5 * (np.sin(t) - np.cos(t)),\n            \"t0\": 0.0,\n            \"y0\": 1.0,\n            \"T\": 30.0,\n        },\n    ]\n\n    N_candidates = [50, 100, 200, 400, 800, 1600, 3200, 6400, 12800, 25600, 51200]\n    optimal_hs = []\n    TIE_TOLERANCE = 1e-18\n\n    for case in test_cases:\n        f = case[\"f\"]\n        y_true_func = case[\"y_true\"]\n        t0, y0, T = case[\"t0\"], case[\"y0\"], case[\"T\"]\n        \n        y_exact_at_T = y_true_func(T)\n        \n        results_for_case = []\n        for N in N_candidates:\n            h = (T - t0) / float(N)\n            y_N = rk4_solver(f, t0, y0, T, N)\n            error = np.abs(y_N - y_exact_at_T)\n            results_for_case.append({\"h\": h, \"error\": error})\n\n        # Find the minimum error among all candidates\n        if not results_for_case:\n            continue\n            \n        min_error = min(r['error'] for r in results_for_case)\n        \n        # Identify all candidates that are tied for the minimum error\n        # A tie is defined as being within an absolute tolerance of the minimum\n        tied_candidates = [\n            r for r in results_for_case \n            if np.isclose(r['error'], min_error, rtol=0, atol=TIE_TOLERANCE)\n        ]\n        \n        # From the tied candidates, select the one with the largest step size h\n        if not tied_candidates: # Should not happen if results_for_case is not empty\n             continue\n\n        optimal_h = max(r['h'] for r in tied_candidates)\n        optimal_hs.append(optimal_h)\n\n    # Format the final output string as specified\n    print(f\"[{','.join(map(str, optimal_hs))}]\")\n\nsolve()\n```", "id": "3259605"}, {"introduction": "The previous exercise revealed the limitations of a fixed step size. A robust solver should be able to adapt its step size, taking small steps when the solution changes rapidly and larger steps when it is smooth. The key to building such an adaptive algorithm is the ability to estimate the local error at each step *without* knowing the true solution. This practice introduces step-doubling, a powerful technique based on Richardson extrapolation, to achieve exactly that. By comparing the result of one large step with two smaller steps [@problem_id:3259715], you will derive and verify an estimator for the local truncation error, mastering the core mechanism that powers adaptive ODE solvers.", "problem": "Consider the Initial Value Problem (IVP) for an Ordinary Differential Equation (ODE): find a function $y(t)$ such that $y'(t) = f(t,y(t))$ for $t \\in [t_0, t_0 + h]$ with $y(t_0) = y_0$, where the exact solution is not available in closed form. You will apply a one-step numerical method to advance from $t_0$ to $t_0 + h$ and use step-doubling to estimate the local truncation error without using the exact solution. Your task is to derive the step-doubling error estimator from first principles and verify, numerically, the order at which this estimator scales with the step size.\n\nUse the following context-appropriate base:\n- The definition of a one-step method of order $p$: when started from the exact value $y(t_0)$, its single-step local truncation error from $t_0$ to $t_0 + h$ scales like $C h^{p+1}$ for some constant $C$ depending on $f$, $t_0$, and $y_0$, and not on $h$.\n- The definition of local truncation error: the difference between the one-step numerical advance and the exact value $y(t_0 + h)$, when starting from the exact initial value $y(t_0)$.\n- Step-doubling: compare a single step of size $h$ against two successive steps of size $h/2$, both starting from the same initial value $y(t_0)$.\n\nYou must implement the following one-step methods:\n- Forward Euler (order $p = 1$).\n- Explicit Midpoint (order $p = 2$).\n- Classical Runge–Kutta of order four (order $p = 4$).\n\nUse the ODE\n$$\nf(t,y) = e^{-t^2}\\sin(y) + t\\,y,\n$$\nwith the initial condition $y(t_0) = y_0$, where $t_0 = 0$ and $y_0 = 0.3$. The exact solution of this ODE is to be treated as unknown.\n\nFrom the one-step method definition, derive a step-doubling estimator for the local truncation error of the finer solution (the two half-steps) that depends only on computable quantities. Then, verify the scaling order of this estimator by computing it for two step sizes $h$ and $h/2$ and calculating the observed order\n$$\nq = \\frac{\\ln\\left(\\frac{E(h)}{E(h/2)}\\right)}{\\ln(2)},\n$$\nwhere $E(\\cdot)$ denotes the step-doubling local error estimate for the finer solution over a total advance of size $h$.\n\nTest Suite:\n- Case $1$: Forward Euler with $h = 0.2$ and $h/2 = 0.1$.\n- Case $2$: Explicit Midpoint with $h = 0.2$ and $h/2 = 0.1$.\n- Case $3$: Classical Runge–Kutta of order four with $h = 0.2$ and $h/2 = 0.1$.\n- Case $4$ (edge case with smaller step): Explicit Midpoint with $h = 0.05$ and $h/2 = 0.025$.\n\nFor each case, compute the observed order $q$ and compare it to the theoretical value $p+1$ using the tolerance $\\varepsilon = 0.3$; declare the case as verified if $\\lvert q - (p+1) \\rvert \\le \\varepsilon$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case in the test suite, include the observed order $q$ (a floating-point number) immediately followed by the verification boolean. The output should, therefore, contain eight entries ordered by the cases above, resulting in a list of the form $[q_1,\\text{pass}_1,q_2,\\text{pass}_2,q_3,\\text{pass}_3,q_4,\\text{pass}_4]$.", "solution": "The objective is to derive the step-doubling error estimator for a one-step numerical method and to verify its order of convergence numerically. This will be done for the Forward Euler, Explicit Midpoint, and Classical Runge-Kutta methods.\n\n### 1. Derivation of the Step-Doubling Error Estimator\n\nLet the initial value problem be defined by $y'(t) = f(t, y(t))$ with the initial condition $y(t_0) = y_0$. Let a one-step numerical method of order $p$ be employed to approximate the solution.\n\nThe local truncation error (LTE) is the error incurred in a single step of size $h$, starting from the exact solution $y(t_n)$. It is defined as the difference between the numerical approximation and the exact solution at the next time point. For a method of order $p$, the LTE over a step from $t_n$ to $t_n + h$ has the asymptotic expansion:\n$$\n\\text{LTE}(h) = y_{\\text{approx}}(t_n+h) - y(t_n+h) = C h^{p+1} + O(h^{p+2})\n$$\nwhere the constant $C$ depends on the function $f$ and its derivatives at $(t_n, y(t_n))$, but not on $h$.\n\nWe consider two numerical approximations to advance the solution from $t_0$ to $t_0 + h$:\n1. A single \"coarse\" step of size $h$. Let the resulting approximation be $y_1$.\n2. Two successive \"fine\" steps, each of size $h/2$. Let the final approximation be $y_2$.\n\nBoth computations start from the same initial value $y(t_0) = y_0$. Based on the definition of the local truncation error, we can write the error for each approximation, assuming we start from the exact value $y(t_0)$.\n\nFor the coarse step, the error is:\n$$\nLTE_1 = y_1 - y(t_0+h) = C h^{p+1} + O(h^{p+2})\n$$\n\nFor the fine steps, the total error is the sum of the local truncation error from each step. The error after two steps of size $h/2$ is approximately:\n$$\nLTE_2 = y_2 - y(t_0+h) \\approx 2 \\times \\left( C \\left(\\frac{h}{2}\\right)^{p+1} \\right) = \\frac{C h^{p+1}}{2^p} + O(h^{p+2})\n$$\nThis approximation assumes that the error propagated from the first half-step does not significantly alter the local error constant $C$ for the second half-step, which is a valid assumption for small $h$.\n\nWe now have a system of two equations with two unknowns, $y(t_0+h)$ and the error coefficient term $C h^{p+1}$. We wish to find an estimator for the error of the more accurate, fine solution, $LTE_2$. We can eliminate the unknown exact solution $y(t_0+h)$ by subtracting the two expressions for the approximations:\n$$\ny_2 - y_1 = (y(t_0+h) + LTE_2) - (y(t_0+h) + LTE_1) = LTE_2 - LTE_1\n$$\nSubstituting the asymptotic forms for the errors, we get:\n$$\ny_2 - y_1 \\approx \\left(\\frac{C h^{p+1}}{2^p}\\right) - (C h^{p+1}) = C h^{p+1} \\left(\\frac{1}{2^p} - 1\\right)\n$$\nWe can solve for the term $C h^{p+1}$:\n$$\nC h^{p+1} \\approx \\frac{y_2 - y_1}{\\frac{1}{2^p} - 1}\n$$\nThe problem asks for an estimator for the local truncation error of the finer solution, which we denote as $E(h)$. This estimator is an approximation of $LTE_2$:\n$$\nE(h) \\approx LTE_2 \\approx \\frac{C h^{p+1}}{2^p}\n$$\nSubstituting the expression for $C h^{p+1}$ yields the step-doubling error estimator for the fine-grid solution:\n$$\nE(h) \\approx \\frac{1}{2^p} \\left( \\frac{y_2 - y_1}{\\frac{1}{2^p} - 1} \\right) = \\frac{y_2 - y_1}{1 - 2^p}\n$$\nThis formula provides an estimate for the error $y_2 - y(t_0+h)$ using only computable quantities ($y_1$ and $y_2$) and the order of the method, $p$.\n\n### 2. Verification of the Estimator's Order\n\nThe derived error estimator is $E(h) = \\frac{y_2 - y_1}{1 - 2^p}$. As shown in its derivation, the leading term of this estimator is $\\frac{C h^{p+1}}{2^p}$. This implies that the estimator itself scales with the step size $h$ as:\n$$\nE(h) \\propto h^{p+1}\n$$\nTo numerically verify this, we compute the estimator for two different step sizes, $h$ and $h/2$. Let these be $E(h)$ and $E(h/2)$. We can then calculate the observed order of convergence, $q$, using the formula:\n$$\nq = \\frac{\\ln\\left(\\frac{|E(h)|}{|E(h/2)|}\\right)}{\\ln(2)}\n$$\nThe use of absolute values is a standard practice to ensure the argument of the logarithm is positive, especially when $h$ is not yet in the fully asymptotic regime. If the estimator scales as $E(h) \\approx K h^{p+1}$ for some constant $K$, then the ratio becomes:\n$$\n\\frac{|E(h)|}{|E(h/2)|} \\approx \\frac{|K h^{p+1}|}{|K (h/2)^{p+1}|} = 2^{p+1}\n$$\nSubstituting this into the formula for $q$ gives:\n$$\nq \\approx \\frac{\\ln(2^{p+1})}{\\ln(2)} = \\frac{(p+1)\\ln(2)}{\\ln(2)} = p+1\n$$\nThus, we expect the observed order $q$ to be approximately equal to $p+1$.\n\n### 3. Numerical Implementation Details\n\nThe numerical verification will be performed for the ODE $y' = f(t,y) = e^{-t^2}\\sin(y) + t\\,y$ with the initial condition $y(0) = 0.3$. The one-step methods to be implemented are:\n- **Forward Euler ($p=1$):** $y_{n+1} = y_n + h f(t_n, y_n)$\n- **Explicit Midpoint ($p=2$):** $k_1 = f(t_n, y_n)$; $k_2 = f(t_n + h/2, y_n + (h/2)k_1)$; $y_{n+1} = y_n + h k_2$\n- **Classical RK4 ($p=4$):** $k_1 = f(t_n, y_n)$; $k_2 = f(t_n + h/2, y_n + (h/2)k_1)$; $k_3 = f(t_n + h/2, y_n + (h/2)k_2)$; $k_4 = f(t_n + h, y_n + h k_3)$; $y_{n+1} = y_n + (h/6)(k_1 + 2k_2 + 2k_3 + k_4)$\n\nFor each test case, we perform the following steps:\n1.  Select a method (with its order $p$) and a primary step size $h_a$. The second step size for comparison will be $h_b = h_a/2$.\n2.  Compute $E(h_a)$:\n    a. Calculate $y_1$ using one step of size $h_a$.\n    b. Calculate $y_2$ using two steps of size $h_a/2$.\n    c. Compute $E(h_a) = (y_2 - y_1) / (1 - 2^p)$.\n3.  Compute $E(h_b)$:\n    a. Calculate $y'_1$ using one step of size $h_b$.\n    b. Calculate $y'_2$ using two steps of size $h_b/2$.\n    c. Compute $E(h_b) = (y'_2 - y'_1) / (1 - 2^p)$.\n4.  Calculate the observed order $q = \\ln(|E(h_a)|/|E(h_b)|) / \\ln(2)$.\n5.  Verify the result by checking if $|q - (p+1)| \\le \\varepsilon$, where $\\varepsilon = 0.3$.\nThe results from the specified test suite are generated by the provided Python code.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to derive, implement, and verify the step-doubling error estimator.\n    \"\"\"\n    # Define the ODE function f(t, y)\n    def f(t, y):\n        return np.exp(-t**2) * np.sin(y) + t * y\n\n    # Define the one-step methods\n    def forward_euler_step(y_start, t_start, h, f_func):\n        return y_start + h * f_func(t_start, y_start)\n\n    def explicit_midpoint_step(y_start, t_start, h, f_func):\n        k1 = f_func(t_start, y_start)\n        k2 = f_func(t_start + h / 2.0, y_start + (h / 2.0) * k1)\n        return y_start + h * k2\n\n    def rk4_step(y_start, t_start, h, f_func):\n        k1 = f_func(t_start, y_start)\n        k2 = f_func(t_start + h / 2.0, y_start + (h / 2.0) * k1)\n        k3 = f_func(t_start + h / 2.0, y_start + (h / 2.0) * k2)\n        k4 = f_func(t_start + h, y_start + h * k3)\n        return y_start + (h / 6.0) * (k1 + 2.0 * k2 + 2.0 * k3 + k4)\n\n    # Store methods and their orders in a dictionary\n    methods = {\n        'euler': {'func': forward_euler_step, 'p': 1},\n        'midpoint': {'func': explicit_midpoint_step, 'p': 2},\n        'rk4': {'func': rk4_step, 'p': 4}\n    }\n\n    # Test suite from the problem statement\n    # Format: (method_name, h_large)\n    test_cases = [\n        ('euler', 0.2),\n        ('midpoint', 0.2),\n        ('rk4', 0.2),\n        ('midpoint', 0.05),\n    ]\n\n    # Initial conditions\n    t0 = 0.0\n    y0 = 0.3\n    \n    # Verification tolerance\n    epsilon = 0.3\n\n    results = []\n\n    def compute_error_estimate(method_func, p, h, t_start, y_start, f_func):\n        \"\"\"\n        Computes the step-doubling error estimate for an advance of size h.\n        \"\"\"\n        # One coarse step of size h\n        y1 = method_func(y_start, t_start, h, f_func)\n        \n        # Two fine steps of size h/2\n        h_half = h / 2.0\n        y_mid = method_func(y_start, t_start, h_half, f_func)\n        y2 = method_func(y_mid, t_start + h_half, h_half, f_func)\n        \n        # Richardson extrapolation based error formula for the finer solution\n        error_est = (y2 - y1) / (1.0 - 2.0**p)\n        return error_est\n\n    for method_name, h_large in test_cases:\n        method = methods[method_name]['func']\n        p = methods[method_name]['p']\n        \n        h_small = h_large / 2.0\n        \n        # Calculate error estimate for step size h_large\n        E_h_large = compute_error_estimate(method, p, h_large, t0, y0, f)\n        \n        # Calculate error estimate for step size h_small\n        E_h_small = compute_error_estimate(method, p, h_small, t0, y0, f)\n        \n        # Calculate observed order of convergence of the estimator\n        # Using abs() is crucial for numerical stability\n        ratio = abs(E_h_large) / abs(E_h_small)\n        q = np.log(ratio) / np.log(2.0)\n        \n        # Verify if observed order is within tolerance of theoretical order p+1\n        verified = abs(q - (p + 1.0)) = epsilon\n        \n        results.extend([q, verified])\n\n    # Format the final output string\n    output_str = ','.join(map(str, results))\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "3259715"}, {"introduction": "Having learned how to estimate local error, we can now assemble a complete, modern adaptive integrator. This final practice guides you through the implementation of a solver driven by an embedded Runge-Kutta pair, the Bogacki-Shampine 2(3) method. This elegant construction provides two approximations of different orders using the same function evaluations, giving an efficient error estimate that can be used in a control loop to automatically adjust the step size. By implementing a step-size controller that responds to this error estimate to meet user-specified tolerances [@problem_id:3259704], you will build a sophisticated tool that embodies the principles of efficiency and reliability in scientific computing.", "problem": "Consider the Initial Value Problem (IVP) defined by an Ordinary Differential Equation (ODE) of the form $y'(t) = f(t,y)$ with initial condition $y(t_0) = y_0$. A one-step method advances the solution from $(t,y)$ to $(t+h, y_{\\text{new}})$ using only information from the current step. In an explicit Runge-Kutta (RK) method, the increment is constructed from a weighted combination of stage derivatives. An embedded RK pair provides two approximations of different orders computed from the same stages, enabling a local error estimate for adaptive step-size control.\n\nYour task is to implement an embedded Runge-Kutta pair that is consistent with the Bogacki–Shampine $2(3)$ construction and use it to drive an adaptive step-size selection algorithm. The algorithm must:\n\n- Compute stage derivatives and produce a higher-order approximation $y^{[p]}$ and a lower-order approximation $y^{[q]}$ at each step, with $pq$.\n- Estimate the local truncation error from the difference $y^{[p]} - y^{[q]}$.\n- Accept a step when a scaled error norm is less than or equal to $1$ and reject otherwise.\n- Adjust the step size $h$ using the asymptotic scaling relation between the error and the step size, ensuring numerical stability by applying a safety factor and bounding the growth and decay of $h$.\n- Respect the final time $t_f$ by reducing the last step as necessary so that $t$ reaches $t_f$ exactly.\n\nYou must implement the adaptive integrator for scalar ODEs with the following scaled error norm per step:\n$$\nE = \\frac{|y^{[p]} - y^{[q]}|}{\\mathrm{atol} + \\mathrm{rtol}\\,\\max(|y|,|y^{[p]}|)},\n$$\nwhere $\\mathrm{atol}$ is the absolute tolerance and $\\mathrm{rtol}$ is the relative tolerance. The step is accepted if $E \\leq 1$. The step-size controller must be based on the principle that an error of order $h^{m}$ implies $h_{\\text{new}} \\propto h\\,E^{-1/m}$ for some integer $m$ that matches the order of the error estimator. Include a multiplicative safety factor and cap the growth and decay by prescribed bounds.\n\nAngle quantities must be interpreted in radians whenever trigonometric functions appear.\n\nImplement the adaptive solver and apply it to the following test suite of IVPs. For each case, return the numerical approximation of $y(t_f)$ as a floating-point number.\n\n- Case $1$ (happy path, exponentially stable):\n  - $f(t,y) = -y$, $y(0) = 1$, $t_0 = 0$, $t_f = 5$.\n  - Tolerances: $\\mathrm{rtol} = 10^{-6}$, $\\mathrm{atol} = 10^{-9}$.\n  - Rationale: tests general performance and stability on a smooth problem.\n\n- Case $2$ (time-dependent growth with trigonometric forcing, radians):\n  - $f(t,y) = y\\sin(t)$, $y(0) = 1$, $t_0 = 0$, $t_f = 3$.\n  - Tolerances: $\\mathrm{rtol} = 10^{-7}$, $\\mathrm{atol} = 10^{-10}$.\n  - Rationale: tests handling of time-dependent coefficients and requires angle in radians.\n\n- Case $3$ (moderately stiff-like linear decay):\n  - $f(t,y) = -15\\,y$, $y(0) = 1$, $t_0 = 0$, $t_f = 1$.\n  - Tolerances: $\\mathrm{rtol} = 10^{-6}$, $\\mathrm{atol} = 10^{-9}$.\n  - Rationale: tests step-size adaptation under faster decay.\n\n- Case $4$ (nonlinear growth near a singularity):\n  - $f(t,y) = y^2$, $y(0) = 1$, $t_0 = 0$, $t_f = 0.9$.\n  - Tolerances: $\\mathrm{rtol} = 10^{-6}$, $\\mathrm{atol} = 10^{-9}$.\n  - Rationale: tests robustness near a blow-up at $t = 1$.\n\n- Case $5$ (zero dynamics, edge case):\n  - $f(t,y) = 0$, $y(0) = 5$, $t_0 = 0$, $t_f = 10$.\n  - Tolerances: $\\mathrm{rtol} = 10^{-8}$, $\\mathrm{atol} = 10^{-12}$.\n  - Rationale: tests the controller when the estimated error is identically zero.\n\nController parameters common to all cases:\n- Safety factor $s = 0.9$,\n- Minimum growth factor $g_{\\min} = 0.2$,\n- Maximum growth factor $g_{\\max} = 5.0$,\n- Initial step size $h_0 = 10^{-3}$,\n- Minimum step size $h_{\\min} = 10^{-12}$,\n- Maximum step size $h_{\\max} = (t_f - t_0)$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Cases $1$ through $5$, for example $[r_1,r_2,r_3,r_4,r_5]$, where each $r_i$ is the floating-point approximation of $y(t_f)$ for Case $i$.", "solution": "The core of the task is to construct an adaptive one-step integrator from first principles, grounded in the definitions of an Initial Value Problem (IVP), Runge-Kutta (RK) methods, and local truncation error.\n\nAn IVP specifies $y'(t) = f(t,y)$ with $y(t_0) = y_0$. A one-step method computes $y_{n+1}$ from $(t_n,y_n)$ using a single step size $h_n$ without reference to earlier history. In an explicit Runge-Kutta (RK) method, the new value is a weighted combination of stage derivatives. The general explicit RK construction is:\n$$\nk_1 = f(t_n, y_n),\\quad\nk_2 = f(t_n + c_2 h, y_n + h a_{21} k_1),\\quad \\dots,\\quad\nk_s = f(t_n + c_s h, y_n + h \\sum_{j=1}^{s-1} a_{sj} k_j),\n$$\nand an order-$p$ approximation is\n$$\ny^{[p]}_{n+1} = y_n + h \\sum_{j=1}^s b_j k_j.\n$$\nAn embedded pair provides two sets of weights $(b_j)$ and $(\\hat b_j)$ evaluated with the same stages to deliver $y^{[p]}$ and $y^{[q]}$ with $pq$. The difference\n$$\n\\Delta = y^{[p]}_{n+1} - y^{[q]}_{n+1}\n$$\nis computable with negligible extra cost and scales with a known power of $h$ determined by the construction.\n\nFor the Bogacki–Shampine $2(3)$ pair, the method uses $s=4$ stages with nodes $c_2 = \\tfrac{1}{2}$, $c_3 = \\tfrac{3}{4}$, $c_4 = 1$. The internal coefficients are $a_{21} = \\tfrac{1}{2}$, $a_{32} = \\tfrac{3}{4}$ with $a_{31} = 0$, and $a_{41} = \\tfrac{2}{9}$, $a_{42} = \\tfrac{1}{3}$, $a_{43} = \\tfrac{4}{9}$. The higher-order weights (order $p=3$) are\n$$\nb_1 = \\tfrac{2}{9},\\quad b_2 = \\tfrac{1}{3},\\quad b_3 = \\tfrac{4}{9},\\quad b_4 = 0,\n$$\nand the lower-order weights (order $q=2$) are\n$$\n\\hat b_1 = \\tfrac{7}{24},\\quad \\hat b_2 = \\tfrac{1}{4},\\quad \\hat b_3 = \\tfrac{1}{3},\\quad \\hat b_4 = \\tfrac{1}{8}.\n$$\nThe stages and approximations are computed as\n$$\n\\begin{aligned}\nk_1 = f(t, y),\\\\\nk_2 = f\\Big(t + \\tfrac{1}{2}h,\\, y + h\\,\\tfrac{1}{2}\\,k_1\\Big),\\\\\nk_3 = f\\Big(t + \\tfrac{3}{4}h,\\, y + h\\,\\tfrac{3}{4}\\,k_2\\Big),\\\\\ny^{[3]} = y + h\\Big(\\tfrac{2}{9}k_1 + \\tfrac{1}{3}k_2 + \\tfrac{4}{9}k_3\\Big),\\\\\nk_4 = f\\Big(t + h,\\, y^{[3]}\\Big),\\\\\ny^{[2]} = y + h\\Big(\\tfrac{7}{24}k_1 + \\tfrac{1}{4}k_2 + \\tfrac{1}{3}k_3 + \\tfrac{1}{8}k_4\\Big).\n\\end{aligned}\n$$\nThe embedded difference $\\Delta = y^{[3]} - y^{[2]}$ provides an error estimator with leading behavior $O(h^3)$; that is, it scales like $C h^3$ for smooth $f$. This scaling justifies a controller of the form\n$$\nh_{\\text{new}} = h \\cdot s \\cdot E^{-1/3},\n$$\nwhere $s$ is a safety factor and $E$ is a scaled error norm. To ensure numerical robustness, we bound the multiplicative growth/decay factor by $g_{\\min} \\leq s E^{-1/3} \\leq g_{\\max}$, and we also clip $h_{\\text{new}}$ to $[h_{\\min}, h_{\\max}]$. When $E \\leq 1$, the step is accepted and $y$ advances to $y^{[3]}$; otherwise, the step is rejected and recomputed with a smaller $h$.\n\nThe scaled error norm for scalar problems is chosen to balance absolute and relative tolerances:\n$$\nE = \\frac{|\\Delta|}{\\mathrm{atol} + \\mathrm{rtol}\\,\\max(|y|,|y^{[3]}|)}.\n$$\nThis ensures that the acceptance criterion $E \\leq 1$ controls the error relative to the magnitude of the solution while preventing division by very small numbers when $y$ is near zero.\n\nAlgorithmic steps for integrating from $t_0$ to $t_f$:\n- Initialize $t = t_0$, $y = y_0$, choose $h$ in $[h_{\\min}, h_{\\max}]$ (for example $h_0$ as given), and set controller parameters $s$, $g_{\\min}$, $g_{\\max}$.\n- While $t  t_f$:\n  - If $t + h  t_f$, set $h = t_f - t$ to land exactly at $t_f$.\n  - Compute $k_1$, $k_2$, $k_3$, $y^{[3]}$, $k_4$, and $y^{[2]}$.\n  - Compute $E$ from $y$, $y^{[3]}$, $y^{[2]}$, $\\mathrm{atol}$, and $\\mathrm{rtol}$.\n  - If $E \\leq 1$, accept the step: set $t \\gets t + h$, $y \\gets y^{[3]}$.\n  - Compute the candidate growth factor $g = s \\cdot E^{-1/3}$; if $E=0$, set $g = g_{\\max}$.\n  - Bound $g$ to $[g_{\\min}, g_{\\max}]$, then update $h \\gets \\mathrm{clip}(h \\cdot g, h_{\\min}, h_{\\max})$.\n  - If a step is rejected ($E  1$), update $h$ as above and recompute without advancing $t$ or $y$.\n- Return $y(t_f)$.\n\nWe now briefly analyze each test case and its expected behavior:\n- Case $1$: $y'(t) = -y$, exact solution $y(t) = e^{-t}$, so $y(5) = e^{-5}$. The method should take moderate steps and converge rapidly.\n- Case $2$: $y'(t) = y\\sin(t)$ in radians with exact solution $y(t) = \\exp(1 - \\cos t)$, giving $y(3) = \\exp(1 - \\cos 3)$. The algorithm must handle time-dependent forcing smoothly.\n- Case $3$: $y'(t) = -15y$ with exact $y(1) = e^{-15}$, requiring smaller steps initially due to rapid decay, but the controller will increase $h$ as $y$ shrinks.\n- Case $4$: $y'(t) = y^2$ with exact $y(t) = \\frac{1}{1 - t}$ for $t1$, so $y(0.9) = 10$. The method must adaptively reduce $h$ as $t$ approaches the blow-up at $t=1$.\n- Case $5$: $y'(t) = 0$ yields constant solution $y(t) = 5$; the error estimator is identically zero, and the controller will enlarge $h$ up to $h_{\\max}$.\n\nBy implementing the Bogacki–Shampine $2(3)$ stages and the controller derived from the $O(h^3)$ error estimate, the adaptive solver will provide $y(t_f)$ for all cases, printed in the specified format.", "answer": "```python\n# Python 3.12, numpy 1.23.5 allowed; no other libraries.\nimport numpy as np\n\ndef rk23_bogacki_shampine_step(f, t, y, h):\n    \"\"\"\n    Perform one Bogacki-Shampine 2(3) step for a scalar ODE y' = f(t,y).\n    Returns (y_high, y_low) where y_high is the 3rd-order solution, y_low is the 2nd-order embedded solution.\n    \"\"\"\n    k1 = f(t, y)\n    k2 = f(t + 0.5 * h, y + h * 0.5 * k1)\n    k3 = f(t + 0.75 * h, y + h * 0.75 * k2)\n    # 3rd-order solution\n    y3 = y + h * ( (2.0/9.0) * k1 + (1.0/3.0) * k2 + (4.0/9.0) * k3 )\n    # Stage 4 evaluated at t+h, y3\n    k4 = f(t + h, y3)\n    # 2nd-order embedded solution\n    y2 = y + h * ( (7.0/24.0) * k1 + (1.0/4.0) * k2 + (1.0/3.0) * k3 + (1.0/8.0) * k4 )\n    return y3, y2\n\ndef integrate_adaptive(f, t0, tf, y0, rtol, atol,\n                       h0=1e-3, hmin=1e-12, hmax=None,\n                       safety=0.9, growth_min=0.2, growth_max=5.0):\n    \"\"\"\n    Adaptive integrator using Bogacki-Shampine 2(3) pair for scalar ODEs.\n    \"\"\"\n    t = float(t0)\n    y = float(y0)\n    if hmax is None:\n        hmax = abs(tf - t0)\n    h = max(hmin, min(h0, hmax))\n    # Direction of integration\n    direction = 1.0 if tf >= t0 else -1.0\n    h *= direction\n    hmin_signed = hmin * direction\n    hmax_signed = hmax * direction\n\n    # Main integration loop\n    # Guard for max iterations to prevent infinite loops in pathological cases\n    max_steps = 10_000_000\n    steps = 0\n    while (direction > 0 and t  tf) or (direction  0 and t > tf):\n        steps += 1\n        if steps > max_steps:\n            # Fallback: give current y\n            break\n\n        # Adjust step to not overshoot tf\n        remaining = tf - t\n        if direction * h > direction * remaining:\n            h = remaining\n\n        # Take one RK23 step\n        y3, y2 = rk23_bogacki_shampine_step(f, t, y, h)\n\n        # Scaled error norm (scalar)\n        scale = atol + rtol * max(abs(y), abs(y3))\n        # Prevent zero scale\n        if scale == 0.0:\n            scale = atol\n        err = abs(y3 - y2) / scale\n\n        # Accept or reject\n        if err = 1.0:\n            # Accept the step\n            t = t + h\n            y = y3\n            # Compute growth factor; estimator scales ~ h^3\n            if err == 0.0:\n                g = growth_max\n            else:\n                g = safety * err ** (-1.0 / 3.0)\n            # Bound growth factor\n            g = max(growth_min, min(g, growth_max))\n            # Update h and clip\n            h = h * g\n            # Clip to [hmin, hmax] with sign\n            if direction > 0:\n                h = min(max(h, hmin_signed), hmax_signed)\n            else:\n                h = max(min(h, hmin_signed), hmax_signed)\n        else:\n            # Reject step; decrease h\n            g = safety * err ** (-1.0 / 3.0)\n            g = max(growth_min, min(g, growth_max))\n            h = h * g\n            # Ensure not below minimum\n            if direction > 0:\n                h = max(h, hmin_signed)\n            else:\n                h = min(h, hmin_signed)\n            # If h becomes too small, break to avoid infinite loop\n            if abs(h)  hmin:\n                # Cannot reduce further; accept and break\n                # This is a conservative fallback\n                t = t + h\n                y = y3\n                break\n\n    return y\n\ndef solve():\n    # Define the test cases\n    # Case 1: y' = -y, y(0) = 1, tf = 5\n    def f1(t, y): return -y\n\n    # Case 2: y' = y*sin(t), radians, y(0) = 1, tf = 3\n    def f2(t, y): return y * np.sin(t)\n\n    # Case 3: y' = -15 y, y(0) = 1, tf = 1\n    def f3(t, y): return -15.0 * y\n\n    # Case 4: y' = y^2, y(0) = 1, tf = 0.9\n    def f4(t, y): return y * y\n\n    # Case 5: y' = 0, y(0) = 5, tf = 10\n    def f5(t, y): return 0.0\n\n    test_cases = [\n        # (f, t0, tf, y0, rtol, atol)\n        (f1, 0.0, 5.0, 1.0, 1e-6, 1e-9),\n        (f2, 0.0, 3.0, 1.0, 1e-7, 1e-10),\n        (f3, 0.0, 1.0, 1.0, 1e-6, 1e-9),\n        (f4, 0.0, 0.9, 1.0, 1e-6, 1e-9),\n        (f5, 0.0, 10.0, 5.0, 1e-8, 1e-12),\n    ]\n\n    results = []\n    for f, t0, tf, y0, rtol, atol in test_cases:\n        ytf = integrate_adaptive(\n            f=f, t0=t0, tf=tf, y0=y0, rtol=rtol, atol=atol,\n            h0=1e-3, hmin=1e-12, hmax=abs(tf - t0),\n            safety=0.9, growth_min=0.2, growth_max=5.0\n        )\n        results.append(ytf)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3259704"}]}