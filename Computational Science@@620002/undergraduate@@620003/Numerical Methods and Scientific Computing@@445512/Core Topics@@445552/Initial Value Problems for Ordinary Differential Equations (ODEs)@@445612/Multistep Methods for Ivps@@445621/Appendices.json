{"hands_on_practices": [{"introduction": "The design of any numerical method begins with fundamental principles. This first practice guides you through the process of constructing a linear two-step method from the ground up, enforcing specific properties of accuracy and stability. By deriving the method's coefficients directly from order conditions and zero-stability requirements [@problem_id:3254441], you will gain a concrete understanding of how these theoretical concepts translate into a functional numerical scheme.", "problem": "Consider initial value problems (IVPs) of the form $y'(t)=f(t,y(t))$, $y(t_0)=y_0$, and linear multistep methods that approximate the solution on a uniform grid $t_n=t_0+n h$ by formulas of the form\n$$\ny_{n+1} \\;=\\; a_0\\,y_n \\;+\\; a_1\\,y_{n-1} \\;+\\; h\\bigl(b_{-1}\\,y'_{n+1} \\;+\\; b_0\\,y'_n\\bigr),\n$$\nwhere $h$ is the step size and $y'_k=f(t_k,y_k)$.\n\nDerive a specific two-step method within this family that is:\n- consistent and of order $2$ (that is, exact for $y(t)=1$, $y(t)=t$, and $y(t)=t^2$),\n- zero-stable with characteristic polynomial having simple roots at $\\xi=1$ and $\\xi=-1$.\n\nProceed as follows:\n1. Starting from the defining principle of linear multistep methods, enforce exactness on the polynomials $y(t)=1$, $y(t)=t$, and $y(t)=t^2$ to obtain the order conditions up to order $2$ and determine the coefficients $a_0$, $a_1$, $b_{-1}$, and $b_0$ under the additional zero-stability constraint that the characteristic polynomial has roots at $\\xi=1$ and $\\xi=-1$.\n2. Analyze the absolute stability of the resulting method by applying it to the linear test equation $y'(t)=\\lambda y(t)$ with $z=h\\lambda$, and derive the region of absolute stability along the imaginary axis {$z \\in \\mathbb{C}: z=\\mathrm{i}\\omega h,\\ \\omega\\in\\mathbb{R}$}.\n3. Using your stability condition on the imaginary axis, determine the largest admissible time step $h_{\\max}$ that ensures absolute stability for the oscillatory test equation $y'(t)=\\mathrm{i}\\,\\omega\\,y(t)$ with $\\omega=7.5\\,\\mathrm{s}^{-1}$. Round your answer to four significant figures and express the final step size in seconds. Your final answer must be a single number.", "solution": "The problem requires the derivation and analysis of a specific two-step linear multistep method. The process is divided into three parts: determining the method's coefficients, analyzing its absolute stability, and calculating the maximum stable step size for a given oscillatory problem.\n\nThe general form of the two-step method is given as:\n$$\ny_{n+1} \\;=\\; a_0\\,y_n \\;+\\; a_1\\,y_{n-1} \\;+\\; h\\bigl(b_{-1}\\,y'_{n+1} \\;+\\; b_0\\,y'_n\\bigr)\n$$\nTo analyze this method, we first express it in the standard form for a $k$-step linear multistep method, $\\sum_{j=0}^k \\alpha_j y_{n+j} = h \\sum_{j=0}^k \\beta_j f_{n+j}$, by shifting the index $n \\to n+1$. This gives:\n$$\ny_{n+2} \\;=\\; a_0\\,y_{n+1} \\;+\\; a_1\\,y_{n} \\;+\\; h\\bigl(b_{-1}\\,y'_{n+2} \\;+\\; b_0\\,y'_{n+1}\\bigr)\n$$\nRearranging into standard form:\n$$\ny_{n+2} - a_0\\,y_{n+1} - a_1\\,y_{n} = h\\bigl(b_{-1}\\,y'_{n+2} + b_0\\,y'_{n+1}\\bigr)\n$$\nFrom this, we identify the coefficients for the first and second characteristic polynomials:\n$\\alpha_2=1$, $\\alpha_1=-a_0$, $\\alpha_0=-a_1$.\n$\\beta_2=b_{-1}$, $\\beta_1=b_0$, and all other $\\beta_j=0$.\n\n### 1. Derivation of the Method's Coefficients\n\nThe problem specifies two constraints to determine the four coefficients $a_0, a_1, b_{-1}, b_0$.\n\nFirst, we use the zero-stability constraint. The first characteristic polynomial is $\\rho(\\xi) = \\sum_{j=0}^{2} \\alpha_j \\xi^j = \\alpha_2 \\xi^2 + \\alpha_1 \\xi + \\alpha_0 = \\xi^2 - a_0 \\xi - a_1$. The problem states that the roots of $\\rho(\\xi)$ are $\\xi_1=1$ and $\\xi_2=-1$. A polynomial with these roots must be of the form $C(\\xi-1)(\\xi+1) = C(\\xi^2-1)$. Since the leading coefficient of $\\rho(\\xi)$ is $\\alpha_2=1$, we have $C=1$, so $\\rho(\\xi) = \\xi^2 - 1$.\nComparing this with $\\rho(\\xi) = \\xi^2 - a_0 \\xi - a_1$, we deduce the coefficients:\n$-a_0 = 0 \\implies a_0 = 0$\n$-a_1 = -1 \\implies a_1 = 1$\n\nWith $a_0=0$ and $a_1=1$, the method simplifies to:\n$$\ny_{n+1} = y_{n-1} + h\\bigl(b_{-1}\\,y'_{n+1} + b_0\\,y'_n\\bigr)\n$$\nNext, we use the order conditions. The method must be exact for the polynomials $y(t)=1, y(t)=t,$ and $y(t)=t^2$. We can test this by substituting these functions into the method's equation. For simplicity, let's center the grid points around $t_n$, such that $t_{n+1}=t_n+h$ and $t_{n-1}=t_n-h$.\n\n- **Exactness for $y(t)=1$**:\nIf $y(t)=1$, then $y'(t)=0$. The formula becomes $1 = 1 + h(b_{-1} \\cdot 0 + b_0 \\cdot 0)$, which simplifies to $1=1$. This condition is satisfied for any choice of $b_{-1}, b_0$. This is equivalent to the consistency condition $\\rho(1)=0$, which is already fulfilled by our choice of roots.\n\n- **Exactness for $y(t)=t$**:\nIf $y(t)=t$, then $y'(t)=1$. The exact values at the grid points are $y(t_{n+1})=t_{n+1}$ and $y(t_{n-1})=t_{n-1}$. The formula becomes:\n$t_{n+1} = t_{n-1} + h(b_{-1} \\cdot 1 + b_0 \\cdot 1)$\n$(t_n+h) = (t_n-h) + h(b_{-1} + b_0)$\n$2h = h(b_{-1} + b_0)$\n$b_{-1} + b_0 = 2$\n\n- **Exactness for $y(t)=t^2$**:\nIf $y(t)=t^2$, then $y'(t)=2t$. The exact values are $y(t_{n+1})=t_{n+1}^2$, $y(t_{n-1})=t_{n-1}^2$, $y'(t_{n+1})=2t_{n+1}$, and $y'(t_n)=2t_n$. The formula becomes:\n$t_{n+1}^2 = t_{n-1}^2 + h\\bigl(b_{-1}(2t_{n+1}) + b_0(2t_n)\\bigr)$\n$(t_n+h)^2 = (t_n-h)^2 + 2h\\bigl(b_{-1}(t_n+h) + b_0 t_n\\bigr)$\n$t_n^2+2t_nh+h^2 = t_n^2-2t_nh+h^2 + 2h\\bigl((b_{-1}+b_0)t_n + b_{-1}h\\bigr)$\n$4t_nh = 2h\\bigl((b_{-1}+b_0)t_n + b_{-1}h\\bigr)$\n$2t_n = (b_{-1}+b_0)t_n + b_{-1}h$\nSubstituting the condition $b_{-1}+b_0=2$ from the previous step:\n$2t_n = 2t_n + b_{-1}h$\nThis implies $b_{-1}h = 0$. Since $h \\neq 0$, we must have $b_{-1}=0$.\n\nFrom $b_{-1}+b_0=2$ and $b_{-1}=0$, we find $b_0=2$.\nThe determined coefficients are $a_0=0, a_1=1, b_{-1}=0, b_0=2$. The resulting method is the explicit two-step midpoint rule:\n$$\ny_{n+1} = y_{n-1} + 2h y'_n\n$$\n\n### 2. Absolute Stability Analysis\n\nTo analyze the absolute stability, we apply the method to the Dahlquist test equation $y'(t)=\\lambda y(t)$, where $\\lambda \\in \\mathbb{C}$. Substituting $y'_n = \\lambda y_n$ into the method gives:\n$$\ny_{n+1} = y_{n-1} + 2h\\lambda y_n\n$$\nLet $z=h\\lambda$. The recurrence relation is:\n$$\ny_{n+1} - 2z y_n - y_{n-1} = 0\n$$\nWe seek solutions of the form $y_n = \\xi^n$. Substituting this ansatz into the recurrence yields the stability polynomial:\n$$\n\\xi^{n+1} - 2z\\xi^n - \\xi^{n-1} = 0\n$$\nDividing by $\\xi^{n-1}$ (for $\\xi \\neq 0$), we get:\n$$\n\\xi^2 - 2z\\xi - 1 = 0\n$$\nThe method is absolutely stable for a given $z$ if all roots $\\xi$ of this polynomial satisfy $|\\xi| \\le 1$, and any roots with $|\\xi|=1$ are simple.\n\nWe are asked to find the region of absolute stability along the imaginary axis, where $z=i\\alpha$ for $\\alpha = \\omega h \\in \\mathbb{R}$. The stability polynomial becomes:\n$$\n\\xi^2 - 2i\\alpha\\xi - 1 = 0\n$$\nThe roots are given by the quadratic formula:\n$$\n\\xi = \\frac{2i\\alpha \\pm \\sqrt{(-2i\\alpha)^2 - 4(1)(-1)}}{2} = \\frac{2i\\alpha \\pm \\sqrt{-4\\alpha^2 + 4}}{2} = i\\alpha \\pm \\sqrt{1-\\alpha^2}\n$$\nWe analyze the magnitude of the roots based on the value of $\\alpha$:\n\n- **Case 1: $|\\alpha|<1$**. In this case, $1-\\alpha^2 > 0$, and $\\sqrt{1-\\alpha^2}$ is real. The roots are $\\xi_{1,2} = i\\alpha \\pm \\sqrt{1-\\alpha^2}$. The magnitude squared of these complex roots is:\n$|\\xi|^2 = (\\pm\\sqrt{1-\\alpha^2})^2 + (\\alpha)^2 = (1-\\alpha^2) + \\alpha^2 = 1$.\nSo, $|\\xi|=1$. Since $\\alpha \\neq \\pm 1$, the term $\\sqrt{1-\\alpha^2}$ is non-zero, meaning the two roots are distinct. Thus, for $|\\alpha|<1$, we have two distinct roots on the unit circle. The method is stable.\n\n- **Case 2: $|\\alpha|=1$**. In this case, $1-\\alpha^2=0$. The roots are $\\xi = i\\alpha$. This is a double root on the unit circle ($|\\xi|=|i\\alpha|=|\\alpha|=1$). A method with a multiple root on the boundary of the unit disk is not absolutely stable.\n\n- **Case 3: $|\\alpha|>1$**. In this case, $1-\\alpha^2 < 0$. We can write $\\sqrt{1-\\alpha^2} = i\\sqrt{\\alpha^2-1}$. The roots are:\n$\\xi = i\\alpha \\pm i\\sqrt{\\alpha^2-1} = i(\\alpha \\pm \\sqrt{\\alpha^2-1})$.\nThe roots are purely imaginary. Their magnitudes are $|\\xi| = |\\alpha \\pm \\sqrt{\\alpha^2-1}|$.\nOne root has magnitude $|\\xi_1| = |\\alpha| + \\sqrt{\\alpha^2-1}$. Since $|\\alpha|>1$, $\\sqrt{\\alpha^2-1}>0$, so $|\\xi_1| > |\\alpha| > 1$.\nSince one root has magnitude greater than $1$, the method is unstable.\n\nCombining these findings, the region of absolute stability along the imaginary axis corresponds to the interval where $|\\alpha|<1$. The endpoints are excluded. The region is $\\{z \\in \\mathbb{C}: z=i\\alpha, -1 < \\alpha < 1 \\}$.\n\n### 3. Calculation of the Maximum Step Size $h_{\\max}$\n\nWe are given the oscillatory test equation $y'(t)=i\\omega y(t)$ with $\\omega=7.5\\,\\mathrm{s}^{-1}$. This corresponds to the Dahlquist test equation with $\\lambda = i\\omega$.\nTherefore, $z = h\\lambda = i\\omega h$. In the notation of the stability analysis, $\\alpha = \\omega h$.\nThe condition for absolute stability is $|\\alpha| < 1$, which translates to:\n$$\n|\\omega h| < 1\n$$\nGiven $\\omega=7.5\\,\\mathrm{s}^{-1}$ and that the step size $h$ must be positive, the inequality becomes:\n$$\n7.5 h < 1\n$$\n$$\nh < \\frac{1}{7.5} \\, \\mathrm{s}\n$$\nThe set of admissible time steps is the interval $(0, 1/7.5)$. The largest admissible time step, $h_{\\max}$, is the supremum of this set.\n$$\nh_{\\max} = \\frac{1}{7.5} \\, \\mathrm{s} = \\frac{1}{15/2} \\, \\mathrm{s} = \\frac{2}{15} \\, \\mathrm{s}\n$$\nTo obtain the numerical value, we compute the fraction:\n$$\nh_{\\max} = \\frac{2}{15} \\approx 0.133333... \\, \\mathrm{s}\n$$\nThe problem requires the answer to be rounded to four significant figures.\n$$\nh_{\\max} \\approx 0.1333 \\, \\mathrm{s}\n$$", "answer": "$$\\boxed{0.1333}$$", "id": "3254441"}, {"introduction": "Implicit methods, like the Adams-Moulton family, are prized for their excellent stability properties, especially for stiff problems. However, their implicit nature—where the unknown value $y_{n+1}$ appears on both sides of the equation—presents a practical challenge: each step requires solving a nonlinear algebraic equation. This hands-on coding exercise [@problem_id:3254452] delves into this crucial aspect by tasking you with implementing and comparing two powerful root-finding techniques, fixed-point iteration and Newton's method, to solve the implicit step and analyze their respective robustness.", "problem": "Consider the initial value problem $y' = f(y)$ with $f(y) = y^3 - y$ on a uniform time grid $t_n = t_0 + n h$ with step size $h > 0$. Starting from the fundamental identity for ordinary differential equations, namely $y(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(y(t)) \\, dt$, derive the one-step implicit Adams–Moulton method of order two by approximating the integral using the trapezoidal rule. This yields an implicit scalar equation for $y_{n+1}$ in terms of $y_n$ and $h$ that must be solved at every step. For the right-hand side $f(y) = y^3 - y$, this implicit equation is a cubic polynomial in $y_{n+1}$.\n\nYour task is to:\n- Derive the implicit scalar equation for $y_{n+1}$ starting from the integral form and the trapezoidal approximation. Do not assume any pre-given discrete formula; begin from $y(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(y(t)) \\, dt$ and proceed logically.\n- Construct two solvers for the implicit equation at a single step $n \\to n+1$:\n  1. A fixed-point iteration solver based on the natural iteration induced by the implicit form.\n  2. A Newton's method solver applied to the residual function derived from the implicit equation.\n- For each solver, define a convergence criterion and a maximum iteration cap. Convergence must be detected by the residual of the implicit equation falling below a prescribed tolerance and, for fixed-point iteration, also by the successive iterate difference falling below a prescribed tolerance. Treat divergence robustly by marking a failure when an iterate becomes non-finite, leaves a prescribed bound, or the method cannot progress due to near-zero derivative in Newton's method.\n\nTo compare basins of convergence, for each test case below:\n- Compute the exact real roots of the cubic implicit equation using its polynomial form and categorize the basins by running each solver from a set of initial guesses {$y^{(0)}$} uniformly spaced over a prescribed range. For each initial guess, run the solver and decide which root it converges to with a nearest-root assignment. If the method fails to converge, record it as a failure.\n- Use the following parameters for the basins scan:\n  - Initial guess grid: uniformly spaced over $[-2, 2]$ with $101$ points.\n  - Maximum iterations: $100$.\n  - Success thresholds: residual tolerance $10^{-12}$ for both methods; successive-iterate tolerance $10^{-10}$ for fixed-point iteration.\n  - Divergence bound: declare failure if $|y|$ exceeds $10^6$ or if Newton’s method encounters a derivative magnitude smaller than $10^{-14}$ at any iteration.\n\nTest suite (each item is a pair $(y_n, h)$):\n- Test A (happy path): $(y_n, h) = (0.2, 0.1)$.\n- Test B (moderate nonlinearity): $(y_n, h) = (0.8, 1.0)$.\n- Test C (strong nonlinearity, challenging fixed-point): $(y_n, h) = (-0.5, 2.0)$.\n\nDefine the final output format precisely as follows. For each test case, let $r_1 < r_2 < \\dots < r_k$ be the sorted real roots of the implicit cubic equation. Your program must produce, for that test case, a single list structured as\n$[c^{\\text{fp}}_1, c^{\\text{fp}}_2, \\dots, c^{\\text{fp}}_k, F^{\\text{fp}}, c^{\\text{N}}_1, c^{\\text{N}}_2, \\dots, c^{\\text{N}}_k, F^{\\text{N}}]$\nwhere $c^{\\text{fp}}_i$ is the number of initial guesses that converge to root $r_i$ under fixed-point iteration, $F^{\\text{fp}}$ is the number of fixed-point failures, $c^{\\text{N}}_i$ is the number of initial guesses that converge to root $r_i$ under Newton’s method, and $F^{\\text{N}}$ is the number of Newton failures. Across all test cases, aggregate these per-case lists into a single top-level list and print that top-level list on one line, with no spaces, as a comma-separated list enclosed in square brackets. The outputs are integers, and no physical units or angles are involved.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces. For example, for three test cases the output must look like:\n[[a1,a2,...,afp,b1,b2,...,bn],[c1,c2,...,cfp,d1,d2,...,dn],[e1,e2,...,efp,f1,f2,...,fn]]", "solution": "The problem requires the derivation and analysis of a numerical method for an initial value problem (IVP). I will first validate the problem statement.\n\n### Step 1: Extract Givens\n- **IVP**: $y' = f(y)$ where $f(y) = y^3 - y$.\n- **Time Grid**: $t_n = t_0 + n h$ for step size $h > 0$.\n- **Fundamental Identity**: $y(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(y(t)) \\, dt$.\n- **Numerical Method**: Implicit Adams-Moulton of order two, to be derived by applying the trapezoidal rule to the integral in the fundamental identity.\n- **Solvers**: Two root-finding methods are to be implemented for the resulting implicit equation: (1) Fixed-point iteration, and (2) Newton's method.\n- **Basin Analysis Parameters**:\n    - **Initial Guesses**: $101$ uniformly spaced points on the interval $[-2, 2]$.\n    - **Maximum Iterations**: $100$.\n    - **Tolerances**: Residual tolerance $\\epsilon_{\\text{res}} = 10^{-12}$ for both methods. Successive iterate tolerance $\\epsilon_{\\text{diff}} = 10^{-10}$ for fixed-point iteration only.\n    - **Divergence Criteria**: Failure if an iterate $y^{(k)}$ has magnitude $|y^{(k)}| > 10^6$, is non-finite, or if the derivative in Newton's method has magnitude less than $10^{-14}$.\n- **Test Cases**: $(y_n, h)$ pairs are given as A: $(0.2, 0.1)$, B: $(0.8, 1.0)$, and C: $(-0.5, 2.0)$.\n- **Output Format**: A specific nested list format representing the number of initial guesses converging to each real root of the implicit equation, plus failures, for each solver and each test case.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically sound. It deals with standard concepts in numerical analysis for ordinary differential equations: the derivation of an implicit integrator (the trapezoidal rule, which is the 2nd-order Adams-Moulton method) and the application of standard iterative root-finding techniques (fixed-point and Newton's methods) to solve the resulting nonlinear algebraic equation at each time step.\n\nAll parameters, conditions, and test cases are specified unambiguously. The task is well-posed, objective, and self-contained. It presents a standard, non-trivial numerical experiment to compare the basins of attraction for two different solvers, which is a core topic in scientific computing. There are no contradictions, factual errors, or violations of physical or mathematical principles.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full, reasoned solution will be provided.\n\n### Derivation of the Implicit Equation\n\nWe begin with the fundamental exact identity for the solution $y(t)$ of the ODE $y' = f(y)$:\n$$\ny(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(y(t)) \\, dt\n$$\nLet $y_n$ be the numerical approximation to $y(t_n)$. We seek an approximation for $y_{n+1} \\approx y(t_{n+1})$. The integral is approximated using the trapezoidal rule:\n$$\n\\int_{t_n}^{t_{n+1}} g(t) \\, dt \\approx \\frac{t_{n+1} - t_n}{2} [g(t_n) + g(t_{n+1})]\n$$\nApplying this to our integral with $g(t) = f(y(t))$ and $h = t_{n+1} - t_n$, we obtain:\n$$\n\\int_{t_n}^{t_{n+1}} f(y(t)) \\, dt \\approx \\frac{h}{2} [f(y(t_n)) + f(y(t_{n+1}))]\n$$\nSubstituting this into the discretized version of the fundamental identity yields the implicit one-step method:\n$$\ny_{n+1} = y_n + \\frac{h}{2} [f(y_n) + f(y_{n+1})]\n$$\nThis is the one-step Adams-Moulton method of order two, also known as the trapezoidal rule for ODEs. For the given function $f(y) = y^3 - y$, the equation becomes:\n$$\ny_{n+1} = y_n + \\frac{h}{2} [(y_n^3 - y_n) + (y_{n+1}^3 - y_{n+1})]\n$$\nTo analyze this as a root-finding problem for $y_{n+1}$, we rearrange the terms to form a residual function $R(z)$, where $z$ represents the unknown $y_{n+1}$. We seek $z$ such that $R(z) = 0$.\n$$\nR(z) = z - y_n - \\frac{h}{2} [f(y_n) + f(z)] = 0\n$$\nSubstituting $f(z) = z^3 - z$ yields:\n$$\nR(z) = z - y_n - \\frac{h}{2} [(y_n^3 - y_n) + (z^3 - z)] = 0\n$$\nGrouping terms by powers of $z$, we obtain a cubic polynomial equation for $y_{n+1}$:\n$$\n-\\frac{h}{2} z^3 + (1 + \\frac{h}{2})z - \\left( y_n + \\frac{h}{2}(y_n^3 - y_n) \\right) = 0\n$$\nThis is the implicit scalar equation that must be solved for $y_{n+1}$ at each time step.\n\n### Numerical Solvers\n\nFor a given $y_n$ and $h$, we must find the roots of the cubic equation $R(z) = 0$.\n\n**1. Fixed-Point Iteration**\n\nThe implicit equation $y_{n+1} = y_n + \\frac{h}{2} [f(y_n) + f(y_{n+1})]$ naturally suggests a fixed-point iteration scheme. Let $z$ be our unknown $y_{n+1}$. We define an iteration function $G(z)$ and the scheme $z^{(k+1)} = G(z^{(k)})$:\n$$\nG(z) = y_n + \\frac{h}{2} [f(y_n) + f(z)]\n$$\nFor the specific $f(y) = y^3 - y$, this becomes:\n$$\nz^{(k+1)} = y_n + \\frac{h}{2} [(y_n^3 - y_n) + ((z^{(k)})^3 - z^{(k)})]\n$$\nStarting with an initial guess $z^{(0)}$, we iterate until convergence is achieved. Convergence is determined by two conditions: the magnitude of the residual of the implicit equation, $|R(z^{(k+1)})|$, must be less than $\\epsilon_{\\text{res}} = 10^{-12}$, and the magnitude of the difference between successive iterates, $|z^{(k+1)} - z^{(k)}|$, must be less than $\\epsilon_{\\text{diff}} = 10^{-10}$.\n\n**2. Newton's Method**\n\nNewton's method finds a root of $R(z) = 0$ using the iteration:\n$$\nz^{(k+1)} = z^{(k)} - \\frac{R(z^{(k)})}{R'(z^{(k)})}\n$$\nWe have the residual function:\n$$\nR(z) = z - y_n - \\frac{h}{2} [f(y_n) + f(z)]\n$$\nIts derivative with respect to $z$ is:\n$$\nR'(z) = \\frac{d}{dz} \\left( z - \\frac{h}{2}f(z) - \\text{const} \\right) = 1 - \\frac{h}{2} f'(z)\n$$\nWith $f(y) = y^3 - y$, the derivative is $f'(y) = 3y^2 - 1$. So,\n$$\nR'(z) = 1 - \\frac{h}{2} (3z^2 - 1)\n$$\nThe iteration starts from a guess $z^{(0)}$ and continues until the residual magnitude $|R(z^{(k+1)})|$ is less than $\\epsilon_{\\text{res}} = 10^{-12}$.\n\n### Basin of Convergence Analysis\n\nFor each test case $(y_n, h)$, the analysis proceeds as follows:\n1.  **Identify Roots**: The coefficients of the cubic polynomial $R(z)=0$ are determined. The roots are found using a robust numerical polynomial root-finder. We filter for the real roots, which are sorted as $r_1  r_2  \\dots  r_k$. For a cubic polynomial, $k$ can be $1$ or $3$.\n2.  **Scan Initial Guesses**: A grid of $101$ initial guesses, $\\{z^{(0)}\\}$, is created uniformly over the interval $[-2, 2]$.\n3.  **Run and Classify**: For each $z^{(0)}$ on the grid:\n    - The fixed-point solver is executed. If it converges to a value $z_{\\text{sol}}$, the outcome is classified by finding the root $r_j$ closest to $z_{\\text{sol}}$. The counter for that root, $c^{\\text{fp}}_j$, is incremented. If the solver fails (due to reaching maximum iterations, divergence, or non-finite values), the failure counter $F^{\\text{fp}}$ is incremented.\n    - The Newton's method solver is executed. The outcome is similarly classified, incrementing either the root counter $c^{\\text{N}}_j$ or the failure counter $F^{\\text{N}}$.\n4.  **Aggregate Results**: For each test case, the counts are collected into a single list $[c^{\\text{fp}}_1, \\dots, c^{\\text{fp}}_k, F^{\\text{fp}}, c^{\\text{N}}_1, \\dots, c^{\\text{N}}_k, F^{\\text{N}}]$. These lists are then aggregated into a final list of lists for all test cases.\n\nThis procedure provides a quantitative comparison of the size of the basins of attraction for each solver.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives, solves, and analyzes an implicit ODE solver to determine basins of convergence.\n    \"\"\"\n    # Define test cases: (yn, h) pairs\n    test_cases = [\n        (0.2, 0.1),  # Test A\n        (0.8, 1.0),  # Test B\n        (-0.5, 2.0), # Test C\n    ]\n\n    # Parameters for basin scan\n    initial_guesses = np.linspace(-2.0, 2.0, 101)\n    max_iter = 100\n    res_tol = 1e-12\n    fp_diff_tol = 1e-10\n    div_bound = 1e6\n    newton_deriv_tol = 1e-14\n\n    all_results = []\n\n    for yn, h in test_cases:\n        # Define the functions for this (yn, h)\n        def f(y):\n            return y**3 - y\n\n        def G_fp(y): # Fixed-point iteration function\n            return yn + h / 2.0 * (f(yn) + f(y))\n\n        def R(y): # Residual function R(y) = 0\n            return y - G_fp(y)\n\n        def R_prime(y): # Derivative of the residual function\n            return 1.0 - h / 2.0 * (3.0 * y**2 - 1.0)\n        \n        # Find the real roots of the cubic polynomial R(z) = 0\n        # R(z) = -h/2 * z^3 + (1 + h/2) * z - (yn + h/2 * (yn^3-yn)) = 0\n        poly_coeffs = [\n            -h / 2.0,\n            0.0,\n            1.0 + h / 2.0,\n            -(yn + h / 2.0 * f(yn))\n        ]\n        roots = np.roots(poly_coeffs)\n        real_roots = np.sort(roots[np.isreal(roots)].real)\n        num_roots = len(real_roots)\n\n        # Initialize counters for this test case\n        # [c_fp_1, ..., c_fp_k, F_fp, c_N_1, ..., c_N_k, F_N]\n        counts = np.zeros(2 * num_roots + 2, dtype=int)\n        \n        fp_failure_idx = num_roots\n        newton_start_idx = num_roots + 1\n        newton_failure_idx = 2 * num_roots + 1\n\n        for y0 in initial_guesses:\n            # --- Fixed-Point Iteration Solver ---\n            y = y0\n            converged = False\n            for _ in range(max_iter):\n                y_prev = y\n                y = G_fp(y)\n                if not np.isfinite(y) or np.abs(y)  div_bound:\n                    break\n                \n                res = np.abs(R(y))\n                diff = np.abs(y - y_prev)\n                \n                if res  res_tol and diff  fp_diff_tol:\n                    converged = True\n                    break\n            \n            if converged:\n                root_idx = np.argmin(np.abs(real_roots - y))\n                counts[root_idx] += 1\n            else:\n                counts[fp_failure_idx] += 1\n\n            # --- Newton's Method Solver ---\n            y = y0\n            converged = False\n            for _ in range(max_iter):\n                deriv = R_prime(y)\n                if not np.isfinite(y) or np.abs(deriv)  newton_deriv_tol:\n                    break\n                \n                step = R(y) / deriv\n                y = y - step\n                \n                if not np.isfinite(y) or np.abs(y)  div_bound:\n                    break\n                \n                if np.abs(R(y))  res_tol:\n                    converged = True\n                    break\n\n            if converged:\n                root_idx = np.argmin(np.abs(real_roots - y))\n                counts[newton_start_idx + root_idx] += 1\n            else:\n                counts[newton_failure_idx] += 1\n        \n        all_results.append(counts.tolist())\n\n    # Format output as specified: [[...],[...]]; no spaces\n    output_str = f\"[{','.join(str(sublist).replace(' ', '') for sublist in all_results)}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3254452"}, {"introduction": "A key insight in scientific computing is that there is no \"one-size-fits-all\" numerical method; the choice of integrator must match the characteristics of the problem. This practice demonstrates this principle by applying a Backward Differentiation Formula (BDF), renowned for its performance on stiff problems, to a purely oscillatory system [@problem_id:3254493]. Through analysis and implementation, you will uncover why BDF methods introduce non-physical numerical damping and phase errors in this context, providing a critical lesson in method selection.", "problem": "Consider the initial value problem (IVP) given by the second-order ordinary differential equation (ODE) $y''(t) + 100\\,y(t) = 0$ with initial conditions $y(0) = 1$ and $y'(0) = 0$. This IVP describes a non-stiff but highly oscillatory system with angular frequency $\\omega = 10$. To apply a multistep method, first rewrite the IVP as a first-order system by introducing the state vector $Y(t) = \\begin{bmatrix} y(t) \\\\ v(t) \\end{bmatrix}$ where $v(t) = y'(t)$, leading to the system $Y'(t) = A\\,Y(t)$ with the constant matrix $A = \\begin{bmatrix} 0  1 \\\\ -100  0 \\end{bmatrix}$.\n\nStarting from the fundamental definition of a linear multistep method,\n$$\n\\sum_{j=0}^{k} \\alpha_j\\,y_{n-j} = h \\sum_{j=0}^{k} \\beta_j\\,f(t_{n-j}, y_{n-j}),\n$$\nderive the backward differentiation formula of order $2$ (BDF2) by ensuring second-order accuracy through Taylor series expansions about $t_n$, and the choice $\\beta_0 = 1$, $\\beta_j = 0$ for $j \\geq 1$. Then apply the resulting implicit method to the first-order system $Y'(t) = A\\,Y(t)$.\n\nExplain, using the scalar modal analysis on the complex test equation $y'(t) = i\\omega\\,y(t)$, why backward differentiation formulas (BDF) yield poor phase accuracy for oscillatory problems. Your explanation must be grounded in the mode-wise amplification factor analysis of the derived multistep recurrence, showing how the method’s step amplification factor $r(\\phi)$ (where $\\phi = \\omega h$) leads to a numerical angle per step $\\theta = \\arg(r)$ that deviates from the exact $\\phi$, and how this deviation accumulates over many steps to produce a significant total phase lag. Additionally, comment on the magnitude $|r|$ and its role in artificial damping of the oscillatory solution.\n\nImplement a program that, for each test case listed below, computes:\n- the accumulated phase lag over $N$ steps,\n$$\n\\Delta \\varphi_N = N\\left(\\theta - \\phi\\right),\n$$\nexpressed in radians, and\n- the amplitude ratio after $N$ steps,\n$$\n\\rho_N = |r|^N,\n$$\nwhich compares the numerical amplitude to the exact unit amplitude.\n\nAngle quantities must be expressed in radians. The final outputs must be floating-point numbers. Your program should not use any external input and should rely only on the provided test suite.\n\nUse the following test suite of step sizes $h$ and step counts $N$ (with $\\omega = 10$ fixed):\n- Test case 1 (happy path, moderate step): $h = 0.05$, $N = 400$.\n- Test case 2 (larger step): $h = 0.10$, $N = 200$.\n- Test case 3 (smaller step): $h = 0.02$, $N = 1000$.\n- Test case 4 (very small step, edge case approaching continuous limit): $h = 0.005$, $N = 4000$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a two-element list of the form $[\\Delta \\varphi_N, \\rho_N]$ corresponding to the above test cases in order. For example, the output format must be\n$$\n\\big[ [\\Delta \\varphi_1, \\rho_1], [\\Delta \\varphi_2, \\rho_2], [\\Delta \\varphi_3, \\rho_3], [\\Delta \\varphi_4, \\rho_4] \\big].\n$$", "solution": "The problem statement is assessed as valid. It is scientifically grounded in the established theory of numerical analysis for ordinary differential equations, specifically concerning linear multistep methods. The problem is well-posed, providing all necessary definitions, equations, and parameters to derive the required formulas, perform the analysis, and implement the specified calculations. The language is objective and precise. Therefore, a full solution is warranted.\n\n### Derivation of the BDF2 Method\n\nThe general form of a $k$-step linear multistep method (LMM) for the initial value problem $y'(t) = f(t, y(t))$ is given by:\n$$\n\\sum_{j=0}^{k} \\alpha_j\\,y_{n-j} = h \\sum_{j=0}^{k} \\beta_j\\,f(t_{n-j}, y_{n-j})\n$$\nwhere $y_{m}$ is the numerical approximation of $y(t_m)$ at time $t_m = m h$.\n\nThe family of Backward Differentiation Formulas (BDF) is characterized by setting $\\beta_0 \\neq 0$ and $\\beta_j = 0$ for $j \\geq 1$. The problem specifies $\\beta_0=1$. This makes the method implicit, as the unknown $y_n$ appears on both sides of the equation via $f_n = f(t_n, y_n)$. The general BDF form is thus:\n$$\n\\sum_{j=0}^{k} \\alpha_j\\,y_{n-j} = h\\,f(t_n, y_n)\n$$\nFor the BDF2 method, the number of steps is $k=2$. The formula involves approximations at three time levels: $y_n$, $y_{n-1}$, and $y_{n-2}$.\n$$\n\\alpha_0 y_n + \\alpha_1 y_{n-1} + \\alpha_2 y_{n-2} = h f_n\n$$\nTo determine the coefficients $\\alpha_0$, $\\alpha_1$, and $\\alpha_2$, we enforce accuracy by minimizing the local truncation error. The local truncation error, $\\tau_n$, is the residual obtained when the exact solution $y(t)$ is substituted into the formula. We expand $y(t_{n-1})$ and $y(t_{n-2})$ in Taylor series around $t_n$:\n\\begin{align*}\ny(t_{n-1}) = y(t_n - h) = y(t_n) - h y'(t_n) + \\frac{h^2}{2} y''(t_n) - \\frac{h^3}{6} y'''(t_n) + O(h^4) \\\\\ny(t_{n-2}) = y(t_n - 2h) = y(t_n) - 2h y'(t_n) + \\frac{(2h)^2}{2} y''(t_n) - \\frac{(2h)^3}{6} y'''(t_n) + O(h^4) \\\\\n= y(t_n) - 2h y'(t_n) + 2h^2 y''(t_n) - \\frac{4h^3}{3} y'''(t_n) + O(h^4)\n\\end{align*}\nTo achieve second-order accuracy, the method must be exact for polynomials of degree up to $2$. This is equivalent to making the first three terms in the Taylor expansion of the local truncation error vanish. Let the operator $\\mathcal{L}_h[y(t)]$ be defined as $\\mathcal{L}_h[y(t)] = \\alpha_0 y(t_n) + \\alpha_1 y(t_{n-1}) + \\alpha_2 y(t_{n-2}) - h y'(t_n)$. Substituting the Taylor series expansions:\n\\begin{align*}\n\\mathcal{L}_h[y(t)] = \\ (\\alpha_0 + \\alpha_1 + \\alpha_2) y(t_n) \\\\\n- h (\\alpha_1 + 2\\alpha_2 + 1) y'(t_n) \\\\\n+ h^2 \\left( \\frac{\\alpha_1}{2} + 2\\alpha_2 \\right) y''(t_n) + O(h^3)\n\\end{align*}\nGrouping terms by derivatives of $y(t_n)$:\n\\begin{align*}\n\\mathcal{L}_h[y(t)] = \\ (\\alpha_0 + \\alpha_1 + \\alpha_2) y(t_n) \\\\\n- h (\\alpha_1 + 2\\alpha_2 + 1) y'(t_n) \\\\\n+ h^2 \\left( \\frac{\\alpha_1}{2} + 2\\alpha_2 \\right) y''(t_n) + O(h^3)\n\\end{align*}\nFor the method to be second-order accurate, the coefficients of the terms up to $O(h^2)$ must be zero. This gives a system of linear equations for the $\\alpha_j$ coefficients:\n1. $y(t_n)$ term: $\\alpha_0 + \\alpha_1 + \\alpha_2 = 0$\n2. $y'(t_n)$ term: $\\alpha_1 + 2\\alpha_2 = -1$\n3. $y''(t_n)$ term: $\\frac{\\alpha_1}{2} + 2\\alpha_2 = 0$\n\nFrom equation (3), we get $\\alpha_1 = -4\\alpha_2$. Substituting this into equation (2) gives $-4\\alpha_2 + 2\\alpha_2 = -1$, which simplifies to $-2\\alpha_2 = -1$, so $\\alpha_2 = 1/2$.\nConsequently, $\\alpha_1 = -4(1/2) = -2$.\nFinally, from equation (1), $\\alpha_0 + (-2) + (1/2) = 0$, which gives $\\alpha_0 = 3/2$.\n\nThe resulting BDF2 formula is:\n$$\n\\frac{3}{2} y_n - 2 y_{n-1} + \\frac{1}{2} y_{n-2} = h f(t_n, y_n)\n$$\n\n### Scalar Modal Analysis and Phase Error\n\nTo analyze the performance of BDF2 on oscillatory problems, we apply it to the scalar complex test equation $y'(t) = \\lambda y(t)$, where $\\lambda = i\\omega$ for a purely oscillatory system with angular frequency $\\omega$. Here, $f(t_n, y_n) = \\lambda y_n$. The BDF2 formula becomes:\n$$\n\\frac{3}{2} y_n - 2 y_{n-1} + \\frac{1}{2} y_{n-2} = h \\lambda y_n\n$$\nThis is a linear homogeneous recurrence relation. We seek a solution of the form $y_n = r^n$, where $r$ is the amplification factor that maps the solution from one step to the next. Substituting this form into the equation and dividing by $r^{n-2}$ yields the characteristic polynomial for $r$:\n$$\n\\frac{3}{2} r^2 - 2 r + \\frac{1}{2} = h \\lambda r^2\n$$\nLet $z = h\\lambda = i\\omega h$. We introduce $\\phi = \\omega h$, which is the exact phase angle increment over one time step $h$. So, $z = i\\phi$. Rearranging the equation gives:\n$$\n\\left(\\frac{3}{2} - z\\right) r^2 - 2 r + \\frac{1}{2} = 0\n$$\nThe roots of this quadratic equation determine the stability and accuracy properties of the method. Using the quadratic formula, the roots are:\n$$\nr(z) = \\frac{2 \\pm \\sqrt{4 - 4\\left(\\frac{3}{2} - z\\right)\\left(\\frac{1}{2}\\right)}}{2\\left(\\frac{3}{2} - z\\right)} = \\frac{2 \\pm \\sqrt{4 - (3 - 2z)}}{3 - 2z} = \\frac{2 \\pm \\sqrt{1 + 2z}}{3 - 2z}\n$$\nThe principal root, which approximates $e^z$ for small $z$, corresponds to the '$+$' sign. Thus, the amplification factor for the BDF2 method is:\n$$\nr(\\phi) = \\frac{2 + \\sqrt{1 + 2i\\phi}}{3 - 2i\\phi}\n$$\n\nThe exact solution to $y' = i\\omega y$ evolves by multiplication with $e^{i\\omega h} = e^{i\\phi}$ at each step. This corresponds to a pure rotation in the complex plane by an angle $\\phi$, with no change in amplitude, since $|e^{i\\phi}| = 1$. The numerical method approximates this operation by multiplication with the complex number $r(\\phi)$.\n\n**Phase Error (Dispersion):** The numerical phase angle per step is $\\theta = \\arg(r(\\phi))$. The exact phase angle is $\\phi$. The phase error per step is $\\theta - \\phi$. For BDF methods and purely imaginary $z$, it can be shown that $\\theta  \\phi$, meaning the numerical solution's phase progression lags behind the true solution. This cumulative effect over many steps results in a significant total phase lag, $\\Delta\\varphi_N$:\n$$\n\\Delta \\varphi_N = N(\\theta - \\phi)\n$$\n\n**Amplitude Error (Artificial Damping):** The amplitude of the numerical solution is multiplied by $|r(\\phi)|$ at each step. For a conservative oscillatory system, the amplitude should remain constant, i.e., the amplification factor should have a magnitude of $1$. However, BDF methods are designed for their strong stability properties for stiff problems (where $\\Re(\\lambda) \\ll 0$), not for preserving oscillations. Their regions of absolute stability only touch the imaginary axis at the origin. For any $z=i\\phi$ with $\\phi \\neq 0$, the amplification factor satisfies $|r(\\phi)|  1$. This introduces artificial numerical damping, causing the amplitude of the numerical solution to decay exponentially. The ratio of the numerical amplitude to the exact amplitude after $N$ steps, $\\rho_N$, is:\n$$\n\\rho_N = |r(\\phi)|^N\n$$\nThis demonstrates why BDF methods, while excellent for stiff problems, are generally unsuitable for simulating non-stiff, undamped oscillatory systems, as they introduce substantial phase lag and artificial damping.\n\n### Calculation Plan\nFor each test case with parameters $h$ and $N$, and fixed $\\omega=10$:\n1. Calculate the exact phase step: $\\phi = \\omega h$.\n2. Define the complex variable: $z = i\\phi$.\n3. Compute the complex amplification factor: $r = \\frac{2 + \\sqrt{1 + 2z}}{3 - 2z}$.\n4. Extract the numerical phase angle: $\\theta = \\arg(r)$.\n5. Compute the total accumulated phase lag: $\\Delta\\varphi_N = N(\\theta - \\phi)$.\n6. Compute the final amplitude ratio: $\\rho_N = |r|^N$.\nAll angle calculations are performed in radians.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and analyzes the BDF2 method for an oscillatory IVP.\n\n    This function calculates the accumulated phase lag and amplitude ratio\n    for the BDF2 method applied to y''(t) + 100y(t) = 0.\n    \"\"\"\n    \n    # Define the fixed angular frequency from the problem statement.\n    omega = 10.0\n\n    # Define the test cases from the problem statement.\n    # Each tuple is (step_size_h, number_of_steps_N).\n    test_cases = [\n        (0.05, 400),   # Test case 1\n        (0.10, 200),   # Test case 2\n        (0.02, 1000),  # Test case 3\n        (0.005, 4000)  # Test case 4\n    ]\n\n    results = []\n    for h, N in test_cases:\n        # 1. Calculate the exact phase step (phi = omega * h)\n        phi = omega * h\n\n        # 2. Define the complex variable z = i*phi\n        # In Python, 1j is the imaginary unit.\n        z = 1j * phi\n\n        # 3. Compute the complex amplification factor r(z) for BDF2.\n        # The formula for the principal root is r = (2 + sqrt(1 + 2z)) / (3 - 2z).\n        # np.sqrt correctly handles the principal square root of a complex number.\n        r = (2 + np.sqrt(1 + 2 * z)) / (3 - 2 * z)\n\n        # 4. Extract the numerical phase angle theta = arg(r).\n        # np.angle returns the argument of a complex number in radians.\n        theta = np.angle(r)\n        \n        # 5. Compute the total accumulated phase lag after N steps.\n        # This is N * (numerical_angle - exact_angle).\n        accumulated_phase_lag = N * (theta - phi)\n        \n        # 6. Compute the final amplitude ratio after N steps.\n        # This is |r|^N. np.abs() calculates the magnitude of a complex number.\n        amplitude_ratio = np.abs(r)**N\n\n        # Store the pair of results for this test case.\n        results.append([accumulated_phase_lag, amplitude_ratio])\n\n    # Final print statement in the exact required format.\n    # Example format: [[lag1,ratio1],[lag2,ratio2],...]\n    # We use an f-string and a generator expression to build the string.\n    print(f\"[{','.join(f'[{lag},{ratio}]' for lag, ratio in results)}]\")\n\nsolve()\n```", "id": "3254493"}]}