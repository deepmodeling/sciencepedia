{"hands_on_practices": [{"introduction": "Understanding stability begins with mastering its fundamental definition. This first exercise provides a direct application of the absolute stability condition, $|R(z)| \\le 1$, where $R(z)$ is the stability function. By analyzing a given polynomial $R(z)$ for real, negative values of $z=h\\lambda$, you will determine the precise interval of stability, building a foundational intuition for how a method's properties translate into concrete constraints on the step size [@problem_id:2219443].", "problem": "In the numerical analysis of Ordinary Differential Equations (ODEs), the stability of a method is crucial for obtaining reliable solutions. This is often studied using the test equation $y'(t) = \\lambda y(t)$, where $\\lambda$ is a complex constant. When a one-step numerical method is applied to this equation with a step size $h$, the numerical solution follows a recurrence relation of the form $y_{n+1} = R(z) y_n$, where $z = h\\lambda$ and $R(z)$ is the stability function of the method.\n\nThe method is defined to be absolutely stable for a given $z$ if $|R(z)| \\le 1$. This condition ensures that the numerical solution does not grow in magnitude for decaying exact solutions.\n\nConsider a numerical method whose stability function is given by the polynomial:\n$$ R(z) = 1 + z + \\frac{z^2}{2} $$\n\nFor a real, negative $\\lambda$, the value of $z$ will be real and negative. The region of absolute stability on the negative real axis is an interval of the form $[a, 0]$. Determine the numerical value of the left endpoint, $a$.", "solution": "The absolute stability condition for a one-step method applied to the test equation $y'(t)=\\lambda y(t)$ with step size $h$ and stability function $R(z)$, where $z=h\\lambda$, is $|R(z)|\\leq 1$. Here $R(z)=1+z+\\frac{z^{2}}{2}$. For real, negative $\\lambda$, the parameter $z$ is real and negative.\n\nSince $z\\in \\mathbb{R}$, $R(z)\\in \\mathbb{R}$, and the stability condition becomes the pair of inequalities\n$$\n-1 \\leq R(z) \\leq 1.\n$$\nFirst, rewrite $R(z)$ by completing the square:\n$$\nR(z)=1+z+\\frac{z^{2}}{2}=\\frac{1}{2}\\left(z^{2}+2z+2\\right)=\\frac{1}{2}\\left((z+1)^{2}+1\\right).\n$$\nTherefore,\n$$\nR(z)\\geq \\frac{1}{2},\n$$\nwith equality at $z=-1$. Hence the lower bound $-1\\leq R(z)$ is automatically satisfied for all real $z$. The stability condition on the negative real axis thus reduces to\n$$\nR(z)\\leq 1.\n$$\nSolve the inequality:\n$$\n1+z+\\frac{z^{2}}{2}\\leq 1 \\quad \\Longleftrightarrow \\quad z+\\frac{z^{2}}{2}\\leq 0 \\quad \\Longleftrightarrow \\quad z\\left(1+\\frac{z}{2}\\right)\\leq 0.\n$$\nThe roots are $z=0$ and $z=-2$, and since the quadratic coefficient is positive, the solution set is the interval\n$$\nz\\in[-2,0].\n$$\nTherefore, the region of absolute stability on the negative real axis is $[a,0]$ with left endpoint $a=-2$.", "answer": "$$\\boxed{-2}$$", "id": "2219443"}, {"introduction": "Moving from abstract functions to practical applications, this problem demonstrates how to assess the stability of a numerical method for a physical system. We will take a second-order Ordinary Differential Equation (ODE) modeling a damped oscillator, convert it into a first-order system, and determine its characteristic eigenvalues. This exercise highlights a crucial concept in computational science: the stability of the entire system is dictated by its most rapidly changing component (the largest-magnitude eigenvalue), which imposes the strictest limit on the integration step size $h$ [@problem_id:2219436].", "problem": "In the study of control systems and mechanical vibrations, it is common to analyze the behavior of damped oscillators. Consider a simplified model of such a system where the dynamics are governed by the following second-order linear homogeneous Ordinary Differential Equation (ODE):\n$$ y''(t) + 101 y'(t) + 100 y(t) = 0 $$\nwhere $y(t)$ represents the displacement of the system at time $t$.\n\nTo approximate the solution numerically, one common technique is to first rewrite this second-order ODE as an equivalent system of two first-order ODEs. Subsequently, a numerical integration scheme is applied. For this problem, we will use the Forward Euler method with a constant step size $h  0$.\n\nA crucial aspect of numerical integration is stability. If the step size $h$ is too large, the numerical solution can grow without bound, even if the true solution decays to zero. For the Forward Euler method, there is a maximum step size beyond which the integration becomes unstable.\n\nDetermine the maximum possible step size, $h_{max}$, for which the Forward Euler method, when applied to the corresponding first-order system, is absolutely stable.", "solution": "Start by rewriting the second-order ODE as a first-order system. Define the state vector $x(t) = \\begin{pmatrix} y(t) \\\\ v(t) \\end{pmatrix}$ with $v(t) = y'(t)$. Then\n$$\n\\begin{cases}\ny'(t) = v(t), \\\\\nv'(t) = -101\\,v(t) - 100\\,y(t),\n\\end{cases}\n$$\nwhich can be written in matrix form as $x'(t) = A x(t)$ with\n$$\nA = \\begin{pmatrix} 0  1 \\\\ -100  -101 \\end{pmatrix}.\n$$\n\nThe eigenvalues of $A$ are the roots of the characteristic polynomial of the original ODE,\n$$\n\\lambda^{2} + 101 \\lambda + 100 = 0.\n$$\nCompute the discriminant:\n$$\n\\Delta = 101^{2} - 4 \\cdot 100 = 10201 - 400 = 9801,\n$$\nso $\\sqrt{\\Delta} = 99$. Therefore,\n$$\n\\lambda_{1,2} = \\frac{-101 \\pm 99}{2},\n$$\nwhich gives\n$$\n\\lambda_{1} = -1, \\quad \\lambda_{2} = -100.\n$$\n\nApply the Forward Euler method to $x'(t) = A x(t)$:\n$$\nx_{n+1} = x_{n} + h A x_{n} = (I + h A) x_{n}.\n$$\nThe method is absolutely stable if the spectral radius of the amplification matrix is less than or equal to $1$, which for a diagonalizable $A$ is ensured by requiring that for each eigenvalue $\\lambda$ of $A$,\n$$\n|1 + h \\lambda| \\le 1.\n$$\nFor $\\lambda_{1} = -1$, the condition is\n$$\n|1 - h| \\le 1 \\;\\;\\Longleftrightarrow\\;\\; -1 \\le 1 - h \\le 1 \\;\\;\\Longleftrightarrow\\;\\; 0 \\le h \\le 2.\n$$\nFor $\\lambda_{2} = -100$, the condition is\n$$\n|1 - 100 h| \\le 1 \\;\\;\\Longleftrightarrow\\;\\; -1 \\le 1 - 100 h \\le 1 \\;\\;\\Longleftrightarrow\\;\\; 0 \\le h \\le \\frac{2}{100} = \\frac{1}{50}.\n$$\nTo ensure absolute stability for the system, both conditions must hold simultaneously, so the allowable step sizes are\n$$\n0  h \\le \\min\\left\\{2, \\frac{1}{50}\\right\\} = \\frac{1}{50}.\n$$\nThus, the maximum possible step size for absolute stability is\n$$\nh_{\\max} = \\frac{1}{50}.\n$$", "answer": "$$\\boxed{\\frac{1}{50}}$$", "id": "2219436"}, {"introduction": "This final practice serves as a capstone, integrating theory with computational implementation to build a versatile stability analysis tool. You will start from the fundamental definition of a Runge-Kutta method, its Butcher tableau, and derive its stability function—in this case, for the widely used classical fourth-order method (RK4). The core of the task is to write a program that numerically finds the maximum stable step size for any given linear system, a skill essential for tackling complex real-world simulations where analytical solutions are intractable [@problem_id:3197733].", "problem": "You are given the initial value problem $y'(t)=A_{\\text{sys}}\\,y(t)$, where $A_{\\text{sys}}$ is a complex-valued matrix and $y(t)$ is a complex-valued vector function. Consider integrating this system using a fixed-step explicit Runge–Kutta method described by a Butcher tableau. The stability of the time integration is governed by the behavior of the method on the linear test equation $y'(t)=\\lambda\\,y(t)$, where $\\lambda\\in\\mathbb{C}$. For an explicit Runge–Kutta method, its stability region is the set of $z\\in\\mathbb{C}$ such that the corresponding scalar amplification factor applied to $y'(t)=\\lambda\\,y(t)$ satisfies $\\lvert R(z)\\rvert\\leq 1$, where $z=\\Delta t\\,\\lambda$ and $\\Delta t$ is the time step size. A method is called A-stable if its stability region includes the entire left half-plane $\\{z\\in\\mathbb{C}:\\Re(z)\\leq 0\\}$, and called L-stable if it is A-stable and additionally $R(z)\\to 0$ as $\\Re(z)\\to -\\infty$. Explicit Runge–Kutta methods are not A-stable, so the maximal stable $\\Delta t$ is generally finite, even when $\\Re(\\lambda)0$.\n\nStarting from the fundamental base of the linear test equation and the definition of the Runge–Kutta method through its Butcher tableau, derive the stability function $R(z)$ for the given method, justify the stability criterion $\\lvert R(\\Delta t\\,\\lambda)\\rvert\\leq 1$ for each eigenvalue $\\lambda$ of $A_{\\text{sys}}$, and construct a numerical search procedure that, for a fixed Runge–Kutta method and a given system matrix $A_{\\text{sys}}$, finds the maximal $\\Delta t0$ such that all scaled eigenvalues $z_i=\\Delta t\\,\\lambda_i$ lie within the stability region, i.e., $\\lvert R(z_i)\\rvert\\leq 1$ for all eigenvalues $\\lambda_i$ of $A_{\\text{sys}}$. Your program must not assume a pre-tabulated stability polynomial; it must obtain $R(z)$ directly from the Butcher tableau and the definition of the method applied to $y'(t)=\\lambda\\,y(t)$.\n\nUse the classical $4$-stage explicit Runge–Kutta method (often referred to as \"RK$4$\") with the following Butcher tableau:\n$$\nA_{\\text{RK}}=\\begin{bmatrix}\n0  0  0  0\\\\\n\\frac{1}{2}  0  0  0\\\\\n0  \\frac{1}{2}  0  0\\\\\n0  0  1  0\n\\end{bmatrix},\\quad\nb=\\begin{bmatrix}\n\\frac{1}{6}\\\\\n\\frac{1}{3}\\\\\n\\frac{1}{3}\\\\\n\\frac{1}{6}\n\\end{bmatrix},\\quad\nc=\\begin{bmatrix}\n0\\\\\n\\frac{1}{2}\\\\\n\\frac{1}{2}\\\\\n1\n\\end{bmatrix}.\n$$\n\nDesign your program to apply this single RK method to each of the following test matrices $A_{\\text{sys}}^{(k)}$ and for each matrix compute a single float value: the maximal $\\Delta t$ such that the stability condition $\\lvert R(\\Delta t\\,\\lambda_i)\\rvert\\leq 1$ holds simultaneously for all eigenvalues $\\lambda_i$ of $A_{\\text{sys}}^{(k)}$. The search must be performed along the rays $z=\\Delta t\\,\\lambda_i$ in $\\mathbb{C}$ for $\\Delta t\\geq 0$, and should return the largest $\\Delta t$ satisfying the constraint. If the constraint only holds for $\\Delta t=0$, your program must return $0.0$ for that case. All numerical outputs must be rounded to $8$ decimal places.\n\nTest suite:\n- Case $1$ (general negative real eigenvalues):\n$$\nA_{\\text{sys}}^{(1)}=\\begin{bmatrix}\n-2  0\\\\\n0  -5\n\\end{bmatrix}.\n$$\n- Case $2$ (positive real eigenvalues):\n$$\nA_{\\text{sys}}^{(2)}=\\begin{bmatrix}\n1  0\\\\\n0  \\frac{1}{2}\n\\end{bmatrix}.\n$$\n- Case $3$ (complex-conjugate pair with negative real part):\n$$\nA_{\\text{sys}}^{(3)}=\\begin{bmatrix}\n-1  -4\\\\\n1  -1\n\\end{bmatrix}.\n$$\n- Case $4$ (highly oscillatory with small damping):\n$$\nA_{\\text{sys}}^{(4)}=\\begin{bmatrix}\n-0.1  10\\\\\n-10  -0.1\n\\end{bmatrix}.\n$$\n\nFinal output format requirement:\n- Your program should produce a single line of output containing the results for the four cases as a comma-separated list enclosed in square brackets, with each float rounded to $8$ decimal places and no spaces. For example, the output must look like $[d_1,d_2,d_3,d_4]$ where each $d_k$ is a decimal string with exactly $8$ digits after the decimal point.", "solution": "The user-provided problem has been validated and is determined to be a well-posed, scientifically grounded problem in the field of numerical analysis for ordinary differential equations. All necessary information is provided, and the problem is free of contradictions or ambiguities. I will now proceed with a complete solution.\n\n### 1. The Explicit Runge-Kutta Method and the Stability Function\n\nAn $s$-stage explicit Runge–Kutta (ERK) method for solving the initial value problem $y'(t) = f(t, y(t))$ is defined by the equations:\n$$\ny_{n+1} = y_n + \\Delta t \\sum_{i=1}^s b_i k_i \\\\\nk_i = f\\left(t_n + c_i \\Delta t, y_n + \\Delta t \\sum_{j=1}^{i-1} a_{ij} k_j\\right)\n$$\nThe coefficients $a_{ij}$, $b_i$, and $c_i$ are given by a Butcher tableau, which for an explicit method has a strictly lower triangular matrix $A = (a_{ij})$.\n\nTo analyze the stability of the method, we apply it to the Dahlquist test equation, $y'(t) = \\lambda y(t)$, where $\\lambda \\in \\mathbb{C}$. In this case, $f(t, y) = \\lambda y$. The stage values $k_i$ become:\n$$\nk_i = \\lambda \\left(y_n + \\Delta t \\sum_{j=1}^{i-1} a_{ij} k_j\\right)\n$$\nLet us define the scaled time step $z = \\Delta t \\lambda$. We can observe that each stage vector $k_i$ must be proportional to $\\lambda y_n$. Let $k_i = K_i(z) \\lambda y_n$ for some function $K_i(z)$. Substituting this into the stage equation:\n$$\nK_i(z) \\lambda y_n = \\lambda \\left(y_n + \\Delta t \\sum_{j=1}^{i-1} a_{ij} (K_j(z) \\lambda y_n)\\right)\n$$\nDividing by $\\lambda y_n$ (assuming $\\lambda, y_n \\neq 0$), we obtain a recurrence relation for the stage polynomials $K_i(z)$:\n$$\nK_i(z) = 1 + z \\sum_{j=1}^{i-1} a_{ij} K_j(z)\n$$\nwith $K_1(z) = 1$ since the sum is empty for $i=1$. Because $A$ is strictly lower triangular, we can compute each $K_i(z)$ sequentially. $K_i(z)$ is a polynomial in $z$ of degree $i-1$.\n\nThe numerical solution is updated as:\n$$\ny_{n+1} = y_n + \\Delta t \\sum_{i=1}^s b_i (K_i(z) \\lambda y_n) = y_n \\left(1 + z \\sum_{i=1}^s b_i K_i(z)\\right)\n$$\nThe term in the parenthesis is the amplification factor, which maps $y_n$ to $y_{n+1}$. This is the stability function $R(z)$:\n$$\nR(z) = 1 + z \\sum_{i=1}^s b_i K_i(z)\n$$\nFor an $s$-stage ERK method, $R(z)$ is a polynomial in $z$ of degree at most $s$.\n\n### 2. Stability Function for the Classical RK4 Method\n\nThe problem provides the Butcher tableau for the classical $4$-stage Runge-Kutta method ($s=4$):\n$$\nA_{\\text{RK}}=\\begin{bmatrix}\n0  0  0  0\\\\\n\\frac{1}{2}  0  0  0\\\\\n0  \\frac{1}{2}  0  0\\\\\n0  0  1  0\n\\end{bmatrix},\\quad\nb=\\begin{bmatrix}\n\\frac{1}{6}\\\\\n\\frac{1}{3}\\\\\n\\frac{1}{3}\\\\\n\\frac{1}{6}\n\\end{bmatrix}\n$$\nWe derive the stage polynomials $K_i(z)$:\n\\begin{align*}\nK_1(z) = 1 \\\\\nK_2(z) = 1 + z a_{21} K_1(z) = 1 + z \\left(\\frac{1}{2}\\right)(1) = 1 + \\frac{z}{2} \\\\\nK_3(z) = 1 + z (a_{31} K_1(z) + a_{32} K_2(z)) = 1 + z \\left(0 + \\frac{1}{2}\\left(1 + \\frac{z}{2}\\right)\\right) = 1 + \\frac{z}{2} + \\frac{z^2}{4} \\\\\nK_4(z) = 1 + z (a_{41} K_1 + a_{42} K_2 + a_{43} K_3) = 1 + z \\left(0 + 0 + 1\\left(1 + \\frac{z}{2} + \\frac{z^2}{4}\\right)\\right) = 1 + z + \\frac{z^2}{2} + \\frac{z^3}{4}\n\\end{align*}\nNow, we construct the stability function $R(z)$:\n$$\nR(z) = 1 + z \\left( b_1 K_1(z) + b_2 K_2(z) + b_3 K_3(z) + b_4 K_4(z) \\right)\n$$\nSubstituting the values of $b_i$ and polynomials $K_i(z)$:\n$$\nR(z) = 1 + z \\left[ \\frac{1}{6}(1) + \\frac{1}{3}\\left(1+\\frac{z}{2}\\right) + \\frac{1}{3}\\left(1+\\frac{z}{2}+\\frac{z^2}{4}\\right) + \\frac{1}{6}\\left(1+z+\\frac{z^2}{2}+\\frac{z^3}{4}\\right) \\right]\n$$\nCombining terms by powers of $z$ inside the brackets:\n\\begin{itemize}\n    \\item Constant term: $\\frac{1}{6} + \\frac{1}{3} + \\frac{1}{3} + \\frac{1}{6} = 1$\n    \\item Term in $z$: $\\frac{1}{3}\\left(\\frac{1}{2}\\right) + \\frac{1}{3}\\left(\\frac{1}{2}\\right) + \\frac{1}{6}(1) = \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} = \\frac{1}{2}$\n    \\item Term in $z^2$: $\\frac{1}{3}\\left(\\frac{1}{4}\\right) + \\frac{1}{6}\\left(\\frac{1}{2}\\right) = \\frac{1}{12} + \\frac{1}{12} = \\frac{1}{6}$\n    \\item Term in $z^3$: $\\frac{1}{6}\\left(\\frac{1}{4}\\right) = \\frac{1}{24}$\n\\end{itemize}\nMultiplying the expression in brackets by $z$ and adding $1$, we get:\n$$\nR(z) = 1 + z\\left(1 + \\frac{1}{2}z + \\frac{1}{6}z^2 + \\frac{1}{24}z^3\\right) = 1 + z + \\frac{z^2}{2} + \\frac{z^3}{6} + \\frac{z^4}{24} = \\sum_{k=0}^{4} \\frac{z^k}{k!}\n$$\nThis is the Taylor series expansion of $e^z$ truncated to the 4th order.\n\n### 3. Stability Criterion for Linear Systems\n\nFor the system of ODEs $y'(t) = A_{\\text{sys}} y(t)$, assuming $A_{\\text{sys}}$ is diagonalizable, there exists an invertible matrix $P$ such that $A_{\\text{sys}} = P \\Lambda P^{-1}$, where $\\Lambda$ is the diagonal matrix of eigenvalues $\\lambda_i$ of $A_{\\text{sys}}$.\nBy the change of variables $u(t) = P^{-1} y(t)$, the system decouples into a set of independent scalar equations: $u_i'(t) = \\lambda_i u_i(t)$.\nApplying a Runge-Kutta method to the original system $y' = A_{\\text{sys}} y$ is equivalent to applying the same method to each of these scalar equations. The update rule for the transformed variables is:\n$u_{i, n+1} = R(\\Delta t \\lambda_i) u_{i, n}$.\nFor the numerical solution $y_n$ to remain bounded as $n \\to \\infty$, all components $u_{i,n}$ must remain bounded. This requires that the amplification factor for each component has a magnitude no greater than one:\n$$\n|R(\\Delta t \\lambda_i)| \\leq 1 \\quad \\text{for all eigenvalues } \\lambda_i \\text{ of } A_{\\text{sys}}\n$$\nThis is the condition for numerical stability.\n\n### 4. Algorithm for Maximal Stable Time Step $\\Delta t_{\\max}$\n\nWe seek the largest $\\Delta t \\geq 0$ that satisfies the stability condition for all eigenvalues simultaneously. This is $\\Delta t_{\\max} = \\sup\\{\\Delta t \\geq 0 \\mid |R(\\Delta t \\lambda_i)| \\leq 1 \\text{ for all } i\\}$.\n\nThe algorithm is as follows:\n1.  Compute the set of eigenvalues $\\{\\lambda_i\\}$ of the matrix $A_{\\text{sys}}$.\n2.  For each eigenvalue $\\lambda_i$:\n    a. If $\\Re(\\lambda_i)  0$, the physical system is unstable. The numerical method will also be unstable for any $\\Delta t  0$, because for small $z=\\Delta t \\lambda_i$, $|R(z)| \\approx |1+z| = \\sqrt{(1+\\Delta t \\Re(\\lambda_i))^2 + (\\Delta t \\Im(\\lambda_i))^2}  1$. Thus, $\\Delta t_{\\max} = 0$.\n    b. If $\\lambda_i=0$, $R(0)=1$, so this eigenvalue imposes no restriction on $\\Delta t$.\n    c. If $\\Re(\\lambda_i) \\leq 0$ and $\\lambda_i \\neq 0$, we must find the smallest positive $\\Delta t_i^*$ such that $|R(\\Delta t_i^* \\lambda_i)| = 1$. This value represents the boundary of the stability region along the ray defined by $\\lambda_i$.\n3.  The overall maximal stable time step is the minimum of these individual limits: $\\Delta t_{\\max} = \\min_{i} \\{\\Delta t_i^*\\}$.\n\nTo find $\\Delta t_i^*$, we solve the equation $|R(\\Delta t \\lambda_i)| - 1 = 0$ for the smallest positive root $\\Delta t$. This is a nonlinear equation that we can solve numerically. A robust approach is to first bracket the root and then use a root-finding algorithm like Brent's method.\n-   **Bracketing**: For a given $\\lambda_i$ with $\\Re(\\lambda_i) \\le 0$, the function $h(\\Delta t) = |R(\\Delta t \\lambda_i)| - 1$ is non-positive for small $\\Delta t0$. Since the stability region of any explicit RK method is bounded, we can find an upper bound $b$ where $h(b)  0$ by starting with a guess and increasing it (e.g., by doubling) until the condition is met. This provides an interval $[a, b]$ containing the root.\n-   **Root-finding**: With the root bracketed in $[a, b]$, `scipy.optimize.brentq` can efficiently find the precise value of $\\Delta t_i^*$.\n\nThe implementation will compute $R(z)$ from the Butcher tableau as derived, then execute this numerical search for each test matrix.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    It computes the maximum stable time step for the classical RK4 method\n    for several linear systems.\n    \"\"\"\n    # Butcher tableau for the classical 4-stage Runge-Kutta method (RK4)\n    A_rk4 = np.array([\n        [0.0, 0.0, 0.0, 0.0],\n        [0.5, 0.0, 0.0, 0.0],\n        [0.0, 0.5, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 0.0]\n    ], dtype=float)\n    b_rk4 = np.array([1/6, 1/3, 1/3, 1/6], dtype=float)\n\n    # Test suite of system matrices A_sys\n    test_cases = [\n        # Case 1: general negative real eigenvalues\n        np.array([[-2.0, 0.0], [0.0, -5.0]], dtype=float),\n        # Case 2: positive real eigenvalues\n        np.array([[1.0, 0.0], [0.0, 0.5]], dtype=float),\n        # Case 3: complex-conjugate pair with negative real part\n        np.array([[-1.0, -4.0], [1.0, -1.0]], dtype=float),\n        # Case 4: highly oscillatory with small damping\n        np.array([[-0.1, 10.0], [-10.0, -0.1]], dtype=float)\n    ]\n\n    results = []\n    for A_sys in test_cases:\n        max_dt = find_max_dt(A_sys, A_rk4, b_rk4)\n        results.append(f\"{max_dt:.8f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\ndef compute_R_from_tableau(z, A, b):\n    \"\"\"\n    Computes the value of the stability function R(z) for a given z\n    and a Runge-Kutta method specified by its Butcher tableau (A, b).\n    \"\"\"\n    s = len(b)\n    K = np.zeros(s, dtype=np.complex128)\n    for i in range(s):\n        # K_i(z) = 1 + z * sum(a[i,j] * K_j(z) for j  i)\n        stage_sum = np.dot(A[i, :i], K[:i])\n        K[i] = 1.0 + z * stage_sum\n    \n    # R(z) = 1 + z * sum(b_i * K_i(z))\n    return 1.0 + z * np.dot(b, K)\n\ndef find_max_dt(A_sys, butcher_A, butcher_b):\n    \"\"\"\n    Finds the maximum stable time step dt for a system y'(t) = A_sys*y(t)\n    using the stability function derived from the provided Butcher tableau.\n    \"\"\"\n    try:\n        eigenvalues = np.linalg.eigvals(A_sys)\n    except np.linalg.LinAlgError:\n        return 0.0\n\n    # If any eigenvalue has a positive real part, the system is unstable,\n    # and the time integration will be unstable for any dt > 0.\n    if any(lam.real > 1e-9 for lam in eigenvalues):\n        return 0.0\n        \n    min_dt_root = float('inf')\n    \n    for lam in eigenvalues:\n        if abs(lam)  1e-9:  # An eigenvalue of 0 imposes no stability constraint.\n            continue\n\n        def h(dt):\n            \"\"\"Target function for root finding: |R(dt*lam)| - 1.\"\"\"\n            z = dt * lam\n            R_val = compute_R_from_tableau(z, butcher_A, butcher_b)\n            return abs(R_val) - 1.0\n\n        # Search for the smallest positive root of h(dt) = 0.\n        # This determines the stability limit for this eigenvalue.\n        \n        # Step 1: Bracket the root. Find an interval [a, b] such that\n        # h(a) = 0 and h(b) > 0.\n        # For stable/neutral eigenvalues, h(dt) = 0 for small dt > 0.\n        a = 1e-9 # Small positive number to start the search interval.\n        b = 1e-3 # Initial guess for the upper bound.\n        \n        # Exponentially increase b until h(b) > 0.\n        while h(b) = 0:\n            b *= 2.0\n            if b > 1e6: # Safety break to avoid infinite loops\n                b = float('inf')\n                break\n        \n        if b == float('inf'):\n            # This eigenvalue does not seem to impose a stability constraint\n            # within a reasonable range. This shouldn't happen for explicit methods.\n            continue\n            \n        a = b / 2.0\n        if a == 0: a = 1e-9\n\n        # Step 2: Use Brent's method to find the root within the bracketed interval.\n        try:\n            root = brentq(h, a, b)\n            min_dt_root = min(min_dt_root, root)\n        except ValueError:\n            # Should not happen with the bracketing logic above.\n            # If it does, it implies immediate instability for this eigenvalue.\n            min_dt_root = 0.0\n            break\n\n    if min_dt_root == float('inf'):\n        # This case would occur if all eigenvalues were zero.\n        # Any dt would be stable, so there's no finite maximum.\n        # The problem cases avoid this scenario. Returning 0.0 as a safe default.\n        return 0.0\n        \n    return min_dt_root\n\nsolve()\n```", "id": "3197733"}]}