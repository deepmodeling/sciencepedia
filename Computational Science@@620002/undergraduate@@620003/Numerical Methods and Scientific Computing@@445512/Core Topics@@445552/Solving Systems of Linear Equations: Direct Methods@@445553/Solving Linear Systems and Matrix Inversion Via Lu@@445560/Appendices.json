{"hands_on_practices": [{"introduction": "General-purpose algorithms for dense matrices are powerful, but their computational cost can be prohibitive for large systems. This practice explores how we can achieve dramatic efficiency gains by exploiting special matrix structures. You will derive the specialized LU factorization algorithm for a tridiagonal matrix, a common structure in scientific computing, and see how its computational complexity reduces from $O(n^3)$ to a remarkable $O(n)$. [@problem_id:3275855]", "problem": "Consider an $n \\times n$ tridiagonal matrix $A$ with subdiagonal entries $\\{a_i\\}_{i=2}^{n}$, diagonal entries $\\{b_i\\}_{i=1}^{n}$, and superdiagonal entries $\\{c_i\\}_{i=1}^{n-1}$, so that $A_{i,i}=b_i$, $A_{i+1,i}=a_{i+1}$, and $A_{i,i+1}=c_i$. Starting only from the definition of Lower-Upper (LU) factorization and the mechanics of Gaussian elimination via elementary row operations, derive explicit scalar recurrences for the entries of a unit lower triangular matrix $L$ and an upper triangular matrix $U$ satisfying $A=LU$. Your derivation must show that each index update uses a constant number of arithmetic operations and thus yields an order-of $n$ (big-$O(n)$) algorithm. Assume $A$ is strictly diagonally dominant so that no row pivoting is needed.\n\nThen, apply your derived recurrences to solve the linear system $A \\boldsymbol{x}=\\boldsymbol{f}$ for the specific $5 \\times 5$ tridiagonal matrix\n$$\nA=\\begin{pmatrix}\n4  -1  0  0  0 \\\\\n-1  4  -1  0  0 \\\\\n0  -1  4  -1  0 \\\\\n0  0  -1  4  -1 \\\\\n0  0  0  -1  4\n\\end{pmatrix},\n$$\nwith right-hand side vector\n$$\n\\boldsymbol{f}=\\begin{pmatrix}1 \\\\ 2 \\\\ 3 \\\\ 2 \\\\ 1\\end{pmatrix}.\n$$\nReport the value of the fifth component $x_5$ of the solution $\\boldsymbol{x}$ in exact rational form. Do not round; provide the exact value as a rational number.", "solution": "The problem is valid as it is a well-posed, scientifically grounded problem in numerical linear algebra, with all necessary data and conditions provided.\n\nThe problem consists of two parts. First, deriving the general recursive formulas for the LU factorization of a tridiagonal matrix. Second, applying these formulas to solve a specific linear system.\n\n**Part 1: Derivation of the Recurrence Relations**\n\nLet $A$ be an $n \\times n$ tridiagonal matrix with subdiagonal entries $\\{a_i\\}_{i=2}^{n}$, diagonal entries $\\{b_i\\}_{i=1}^{n}$, and superdiagonal entries $\\{c_i\\}_{i=1}^{n-1}$.\n$$\nA = \\begin{pmatrix}\nb_1  c_1  0  \\dots  0 \\\\\na_2  b_2  c_2  \\ddots  \\vdots \\\\\n0  a_3  b_3  \\ddots  0 \\\\\n\\vdots  \\ddots  \\ddots  \\ddots  c_{n-1} \\\\\n0  \\dots  0  a_n  b_n\n\\end{pmatrix}\n$$\nWe seek an LU factorization $A = LU$, where $L$ is a unit lower triangular matrix and $U$ is an upper triangular matrix. The problem states that $A$ is strictly diagonally dominant, which ensures that an LU factorization exists without the need for pivoting. The tridiagonal structure of $A$ results in a bidiagonal structure for both $L$ and $U$ (beyond their main diagonals).\n\nLet $L$ be a unit lower bidiagonal matrix and $U$ be an upper bidiagonal matrix:\n$$\nL = \\begin{pmatrix}\n1  0  0  \\dots  0 \\\\\nl_2  1  0  \\ddots  \\vdots \\\\\n0  l_3  1  \\ddots  0 \\\\\n\\vdots  \\ddots  \\ddots  \\ddots  0 \\\\\n0  \\dots  0  l_n  1\n\\end{pmatrix}, \\quad\nU = \\begin{pmatrix}\nu_1  d_1  0  \\dots  0 \\\\\n0  u_2  d_2  \\ddots  \\vdots \\\\\n0  0  u_3  \\ddots  0 \\\\\n\\vdots  \\ddots  \\ddots  \\ddots  d_{n-1} \\\\\n0  \\dots  0  0  u_n\n\\end{pmatrix}\n$$\nHere, $L$ is unit lower triangular with non-zero entries $L_{i,i}=1$ and $L_{i,i-1}=l_i$. $U$ is upper triangular with non-zero entries $U_{i,i}=u_i$ and $U_{i,i+1}=d_i$.\n\nBy definition of LU factorization, we equate the entries of $A$ with the entries of the product $LU$:\n$$\nA = LU = \\begin{pmatrix}\n1  0  \\dots \\\\\nl_2  1  \\\\\n\\vdots  \\ddots  \\ddots\n\\end{pmatrix}\n\\begin{pmatrix}\nu_1  d_1  0  \\dots \\\\\n0  u_2  d_2  \\\\\n\\vdots   \\ddots  \\ddots\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nu_1  d_1  0  \\dots \\\\\nl_2 u_1  l_2 d_1 + u_2  d_2  \\\\\n0  l_3 u_2  l_3 d_2 + u_3  \\ddots \\\\\n\\vdots   \\ddots  \\ddots\n\\end{pmatrix}\n$$\nBy comparing the entries of $A$ and $LU$ row by row:\n\nFor the first row ($i=1$):\n- Diagonal: $A_{1,1} = b_1 = (LU)_{1,1} = u_1$. So, $u_1 = b_1$.\n- Superdiagonal: $A_{1,2} = c_1 = (LU)_{1,2} = d_1$. So, $d_1 = c_1$.\n\nFor any subsequent row $i=2, \\dots, n-1$:\n- Subdiagonal: $A_{i,i-1} = a_i = (LU)_{i,i-1} = l_i u_{i-1}$. This gives $l_i = \\frac{a_i}{u_{i-1}}$.\n- Diagonal: $A_{i,i} = b_i = (LU)_{i,i} = l_i d_{i-1} + u_i$. This gives $u_i = b_i - l_i d_{i-1}$.\n- Superdiagonal: $A_{i,i+1} = c_i = (LU)_{i,i+1} = d_i$. So, $d_i = c_i$.\n\nFor the last row ($i=n$):\n- Subdiagonal: $A_{n,n-1} = a_n = (LU)_{n,n-1} = l_n u_{n-1}$. This gives $l_n = \\frac{a_n}{u_{n-1}}$.\n- Diagonal: $A_{n,n} = b_n = (LU)_{n,n} = l_n d_{n-1} + u_n$. This gives $u_n = b_n - l_n d_{n-1}$.\n\nCombining these, we note that $d_i = c_i$ for all $i=1, \\dots, n-1$. We can write the complete set of recurrences:\n\n1. Initialize for $i=1$:\n   $u_1 = b_1$\n\n2. For $i=2, 3, \\dots, n$, compute:\n   $l_i = \\frac{a_i}{u_{i-1}}$\n   $u_i = b_i - l_i c_{i-1}$\n\nThis is known as the Crout factorization algorithm for a tridiagonal matrix. To find all the entries of $L$ and $U$, we iterate from $i=2$ to $n$. In each iteration, we compute one value $l_i$ (one division) and one value $u_i$ (one multiplication and one subtraction). The number of arithmetic operations per iteration is constant (independent of $n$). Since the loop runs $n-1$ times, the total number of operations is proportional to $n-1$. Therefore, the algorithm has a time complexity of $O(n)$.\n\n**Part 2: Application to the Specific System**\n\nWe need to solve $A \\boldsymbol{x}=\\boldsymbol{f}$ for the specific $5 \\times 5$ system.\nThe matrix $A$ gives the parameters: $n=5$, $b_i = 4$ for all $i$, $a_i = -1$ for $i \\geq 2$, and $c_i = -1$ for $i  5$. The vector $\\boldsymbol{f}$ has components $f_1=1, f_2=2, f_3=3, f_4=2, f_5=1$.\n\nFirst, we find the LU factorization of $A$ using the derived recurrences.\n- $i=1$:\n  $u_1 = b_1 = 4$\n- $i=2$:\n  $l_2 = \\frac{a_2}{u_1} = \\frac{-1}{4}$\n  $u_2 = b_2 - l_2 c_1 = 4 - (-\\frac{1}{4})(-1) = 4 - \\frac{1}{4} = \\frac{15}{4}$\n- $i=3$:\n  $l_3 = \\frac{a_3}{u_2} = \\frac{-1}{15/4} = -\\frac{4}{15}$\n  $u_3 = b_3 - l_3 c_2 = 4 - (-\\frac{4}{15})(-1) = 4 - \\frac{4}{15} = \\frac{56}{15}$\n- $i=4$:\n  $l_4 = \\frac{a_4}{u_3} = \\frac{-1}{56/15} = -\\frac{15}{56}$\n  $u_4 = b_4 - l_4 c_3 = 4 - (-\\frac{15}{56})(-1) = 4 - \\frac{15}{56} = \\frac{224-15}{56} = \\frac{209}{56}$\n- $i=5$:\n  $l_5 = \\frac{a_5}{u_4} = \\frac{-1}{209/56} = -\\frac{56}{209}$\n  $u_5 = b_5 - l_5 c_4 = 4 - (-\\frac{56}{209})(-1) = 4 - \\frac{56}{209} = \\frac{836-56}{209} = \\frac{780}{209}$\n\nThe equation $A\\boldsymbol{x}=\\boldsymbol{f}$ is equivalent to $LU\\boldsymbol{x}=\\boldsymbol{f}$. We solve this in two steps:\n1. Solve $L\\boldsymbol{y}=\\boldsymbol{f}$ for $\\boldsymbol{y}$ (forward substitution).\n2. Solve $U\\boldsymbol{x}=\\boldsymbol{y}$ for $\\boldsymbol{x}$ (backward substitution).\n\nStep 1: Forward substitution to find $\\boldsymbol{y}$.\nThe system $L\\boldsymbol{y}=\\boldsymbol{f}$ is:\n$y_1 = f_1$\n$l_i y_{i-1} + y_i = f_i$ for $i=2, \\dots, 5$, which implies $y_i = f_i - l_i y_{i-1}$.\n- $y_1 = f_1 = 1$\n- $y_2 = f_2 - l_2 y_1 = 2 - (-\\frac{1}{4})(1) = 2 + \\frac{1}{4} = \\frac{9}{4}$\n- $y_3 = f_3 - l_3 y_2 = 3 - (-\\frac{4}{15})(\\frac{9}{4}) = 3 + \\frac{9}{15} = 3 + \\frac{3}{5} = \\frac{18}{5}$\n- $y_4 = f_4 - l_4 y_3 = 2 - (-\\frac{15}{56})(\\frac{18}{5}) = 2 + \\frac{15 \\cdot 18}{56 \\cdot 5} = 2 + \\frac{3 \\cdot 18}{56} = 2 + \\frac{3 \\cdot 9}{28} = 2 + \\frac{27}{28} = \\frac{56+27}{28} = \\frac{83}{28}$\n- $y_5 = f_5 - l_5 y_4 = 1 - (-\\frac{56}{209})(\\frac{83}{28}) = 1 + \\frac{56 \\cdot 83}{209 \\cdot 28} = 1 + \\frac{2 \\cdot 83}{209} = 1 + \\frac{166}{209} = \\frac{209+166}{209} = \\frac{375}{209}$\n\nStep 2: Backward substitution to find $\\boldsymbol{x}$.\nThe system $U\\boldsymbol{x}=\\boldsymbol{y}$ only needs to be solved for $x_5$. The last equation of the system is $u_5 x_5 = y_5$.\n- $\\frac{780}{209} x_5 = \\frac{375}{209}$\n- $x_5 = \\frac{375/209}{780/209} = \\frac{375}{780}$\n\nFinally, we simplify the fraction for $x_5$:\n$$\nx_5 = \\frac{375}{780} = \\frac{375 \\div 5}{780 \\div 5} = \\frac{75}{156} = \\frac{75 \\div 3}{156 \\div 3} = \\frac{25}{52}\n$$\nThe numbers $25=5^2$ and $52=4 \\times 13$ share no common factors, so the fraction is fully reduced.", "answer": "$$\n\\boxed{\\frac{25}{52}}\n$$", "id": "3275855"}, {"introduction": "Efficiency in numerical methods often comes from avoiding redundant computations. This problem presents a choice between a direct, brute-force method and a more elegant, reuse-oriented approach for solving the system $A^k x = b$. By performing a careful floating-point operation count, you will quantitatively prove why computing the LU factorization just once is vastly superior to explicitly forming the matrix power $A^k$. [@problem_id:3275805]", "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be a dense, nonsingular matrix with no special structure, and let $k \\geq 2$ be an integer. You wish to compute the vector $x$ that satisfies $A^{k} x = b$ for a given right-hand-side $b \\in \\mathbb{R}^{n}$. Consider two strategies:\n\n(a) Explicitly form $A^{k}$ via sequential dense matrix multiplications, then solve the linear system $A^{k} x = b$ by computing a Lower-Upper (LU) factorization of $A^{k}$ followed by forward and backward substitution.\n\n(b) Compute a single Lower-Upper (LU) factorization of $A$, and then solve the sequence of $k$ linear systems $A y_{1} = b$, $A y_{2} = y_{1}$, $\\dots$, $A y_{k} = y_{k-1}$ to obtain $x = y_{k}$ by forward and backward substitution with the reused factors.\n\nUsing the following widely accepted operation counts for dense algorithms as foundational facts: a dense $n \\times n$ matrix-matrix multiplication costs $2 n^{3}$ floating-point operations, a dense Lower-Upper (LU) factorization with partial pivoting costs $\\frac{2}{3} n^{3}$ floating-point operations, and a triangular solve (either forward with $L$ or backward with $U$) for a single right-hand side costs $n^{2}$ floating-point operations, derive expressions for the total floating-point operation counts for strategies (a) and (b), and then compute the ratio of the total operation count of strategy (a) to that of strategy (b) as a closed-form expression in $n$ and $k$. Ignore all lower-order terms beyond those stated and treat permutation application as negligible.\n\nExpress your final answer as a single simplified analytic expression in $n$ and $k$.", "solution": "The problem requires a comparative analysis of the computational cost, measured in floating-point operations (flops), for two distinct strategies to solve the linear system $A^{k} x = b$. Let $C_a$ and $C_b$ denote the total flop counts for strategies (a) and (b), respectively. The analysis will be based on the provided operation counts for fundamental dense matrix operations.\n\nLet us first analyze strategy (a). This strategy consists of two main stages: explicitly forming the matrix $A^{k}$, and then solving the resulting linear system.\n\nStage 1(a): Formation of $A^{k}$.\nThe matrix $A^{k}$ is computed by performing $k-1$ sequential matrix-matrix multiplications: $A^2 = A \\cdot A$, $A^3 = A^2 \\cdot A$, ..., $A^k = A^{k-1} \\cdot A$. Each multiplication involves two dense $n \\times n$ matrices.\nThe cost of a single dense $n \\times n$ matrix-matrix multiplication is given as $2n^3$ flops.\nSince there are $k-1$ such multiplications, the total cost for this stage is:\n$$C_{1,a} = (k-1) \\times (2n^3) = (2k-2)n^3 \\text{ flops.}$$\n\nStage 2(a): Solving the system $A^{k} x = b$.\nLet $B = A^k$. The system to be solved is $B x = b$. The prescribed method is LU factorization followed by forward and backward substitution.\nFirst, the LU factorization of the dense $n \\times n$ matrix $B$ is computed. The cost for this is given as $\\frac{2}{3}n^3$ flops.\nNext, the system is solved in two steps:\n1.  Solve $L y = b$ for $y$ (forward substitution). The cost for a triangular solve on a single right-hand side is $n^2$ flops.\n2.  Solve $U x = y$ for $x$ (backward substitution). This also costs $n^2$ flops.\nThe total cost for this stage is the sum of the costs of factorization and the two substitutions:\n$$C_{2,a} = \\frac{2}{3}n^3 + n^2 + n^2 = \\frac{2}{3}n^3 + 2n^2 \\text{ flops.}$$\n\nThe total operation count for strategy (a) is the sum of the costs of these two stages:\n$$C_a(n,k) = C_{1,a} + C_{2,a} = (2k-2)n^3 + \\left(\\frac{2}{3}n^3 + 2n^2\\right)$$\n$$C_a(n,k) = \\left(2k - 2 + \\frac{2}{3}\\right)n^3 + 2n^2 = \\left(2k - \\frac{4}{3}\\right)n^3 + 2n^2.$$\n\nNext, let us analyze strategy (b). This strategy avoids forming $A^k$ and instead solves a sequence of $k$ linear systems. The equation $A^k x = b$ can be written as $A(A(...A x)) = b$. This suggests solving a sequence of systems. Let $y_1, y_2, \\dots, y_k$ be intermediate vectors.\n- Let $A y_1 = b$.\n- Let $A y_2 = y_1$. Substituting the first equation into this gives $A(A y_2) = A^2 y_2 = b$.\n- Continuing this process, we define $A y_i = y_{i-1}$ for $i=2, \\dots, k$.\nThe final system is $A y_k = y_{k-1}$, which implies $A^k y_k = b$. Thus, the solution is $x = y_k$.\n\nStage 1(b): LU factorization of $A$.\nThis is performed only once at the beginning. The cost is the standard cost for an LU factorization of a dense $n \\times n$ matrix:\n$$C_{1,b} = \\frac{2}{3}n^3 \\text{ flops.}$$\n\nStage 2(b): Solving $k$ sequential linear systems.\nThe $k$ systems to be solved are $A y_1 = b$, and $A y_i = y_{i-1}$ for $i=2, \\dots, k$.\nFor each system, the LU factors of $A$ are reused. Solving one such system $A z = w$ using existing factors $L$ and $U$ requires one forward substitution ($L v = w$, cost $n^2$) and one backward substitution ($U z = v$, cost $n^2$).\nThe cost to solve a single system is $n^2 + n^2 = 2n^2$ flops.\nSince there are $k$ such systems to solve, the total cost for this stage is:\n$$C_{2,b} = k \\times (2n^2) = 2kn^2 \\text{ flops.}$$\n\nThe total operation count for strategy (b) is the sum of the costs of the initial factorization and the $k$ solves:\n$$C_b(n,k) = C_{1,b} + C_{2,b} = \\frac{2}{3}n^3 + 2kn^2.$$\n\nFinally, we are asked to compute the ratio of the total operation count of strategy (a) to that of strategy (b).\n$$\\text{Ratio} = \\frac{C_a(n,k)}{C_b(n,k)} = \\frac{\\left(2k - \\frac{4}{3}\\right)n^3 + 2n^2}{\\frac{2}{3}n^3 + 2kn^2}.$$\nTo simplify this expression, we can factor out $n^2$ from the numerator and the denominator, as $n \\neq 0$:\n$$\\text{Ratio} = \\frac{n^2 \\left[ \\left(2k - \\frac{4}{3}\\right)n + 2 \\right]}{n^2 \\left[ \\frac{2}{3}n + 2k \\right]} = \\frac{\\left(2k - \\frac{4}{3}\\right)n + 2}{\\frac{2}{3}n + 2k}.$$\nTo eliminate the fractions, we multiply the numerator and the denominator by $3$:\n$$\\text{Ratio} = \\frac{3 \\left[ \\left(2k - \\frac{4}{3}\\right)n + 2 \\right]}{3 \\left[ \\frac{2}{3}n + 2k \\right]} = \\frac{(6k - 4)n + 6}{2n + 6k}.$$\nWe can factor out a common factor of $2$ from the numerator and the denominator:\n$$\\text{Ratio} = \\frac{2 \\left[ (3k - 2)n + 3 \\right]}{2 [n + 3k]} = \\frac{(3k - 2)n + 3}{n + 3k}.$$\nThis is the final simplified analytic expression for the ratio of the costs.", "answer": "$$\n\\boxed{\\frac{(3k - 2)n + 3}{n + 3k}}\n$$", "id": "3275805"}, {"introduction": "A common mantra in numerical linear algebra is to 'avoid forming the inverse.' This exercise puts that principle into practice by tackling the challenge of finding the diagonal entries of $A^{-1}$ without ever computing the full inverse. You will design an efficient algorithm that leverages the $PA=LU$ factorization to solve a series of targeted linear systems, demonstrating a powerful technique for extracting specific information with minimal computational effort. [@problem_id:3275751]", "problem": "You are given the mathematical task of computing only the diagonal entries of the inverse of a nonsingular square matrix using its lower-upper factorization. The fundamental base is that a square matrix $A$ admits a Lower-Upper (LU) factorization with partial pivoting, written as $PA = LU$, where $P$ is a permutation matrix arising from row interchanges, $L$ is a unit lower triangular matrix, and $U$ is an upper triangular matrix. Triangular systems $Lx = b$ and $Uy = x$ are solved by forward and backward substitution, respectively. The inverse $A^{-1}$, when it exists, can be expressed in terms of the LU factors and $P$, but computing $A^{-1}$ explicitly is not required, nor desired here.\n\nYour task is to derive and implement an algorithm that, given $L$, $U$, and a representation of the permutation $P$ from a partial-pivoting LU factorization $PA = LU$ of a square nonsingular matrix $A$, computes only the diagonal entries of $A^{-1}$ efficiently, using forward and backward substitution with carefully chosen right-hand sides. You must not form the full inverse of $A$. The algorithm must be justified from first principles based on $PA = LU$ and properties of triangular solves.\n\nImplement the following steps in a complete, runnable program:\n- Perform Gaussian elimination with partial pivoting to compute $P$, $L$, and $U$ for a given matrix $A$, with $L$ unit diagonal and $U$ upper triangular. Represent $P$ by a permutation vector $p$ where $p_i$ records which original row of $A$ appears in row $i$ of $PA$. Interpretations of $P$ must be handled consistently.\n- Using only $L$, $U$, and $p$, derive and implement a routine to compute the diagonal entries of $A^{-1}$ without forming $A^{-1}$ itself. Your routine must use forward substitution to solve $Lx = e_j$ and backward substitution to solve $Uy = x$, where $e_j$ is a standard basis vector selected according to $P$. The result for the $i$-th diagonal entry must be extracted correctly.\n- Implement numerically stable forward and backward substitution for general dense triangular matrices.\n- Return, for each test matrix, a list of floating-point numbers equal to the diagonal entries of $A^{-1}$. Do not include any physical units. There are no angle units involved. Express all numerical values as decimal floating-point numbers.\n\nUse the following test suite of matrices to exercise the algorithm:\n- Test case $1$ (general nonsingular $3 \\times 3$): \n$$\nA_1 = \\begin{bmatrix}\n4  1  -2 \\\\\n1  3  0 \\\\\n-2  0  5\n\\end{bmatrix}.\n$$\n- Test case $2$ (diagonal, boundary case, $3 \\times 3$):\n$$\nA_2 = \\begin{bmatrix}\n1  0  0 \\\\\n0  2  0 \\\\\n0  0  3\n\\end{bmatrix}.\n$$\n- Test case $3$ (requires pivoting, $3 \\times 3$):\n$$\nA_3 = \\begin{bmatrix}\n0  1  2 \\\\\n1  0  3 \\\\\n4  5  6\n\\end{bmatrix}.\n$$\n- Test case $4$ (boundary case, $1 \\times 1$):\n$$\nA_4 = \\begin{bmatrix}\n7\n\\end{bmatrix}.\n$$\n- Test case $5$ (ill-conditioned but invertible, $3 \\times 3$):\n$$\nA_5 = \\begin{bmatrix}\n10^{-8}  1  0 \\\\\n0  10^{-8}  1 \\\\\n1  0  10^{-8}\n\\end{bmatrix}.\n$$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of the diagonal entries of $A^{-1}$ for that case. For example, the output format should be $[[d_{11}^{(1)},d_{22}^{(1)},d_{33}^{(1)}],[d_{11}^{(2)},d_{22}^{(2)},d_{33}^{(2)}],\\dots]$, with the lists sized appropriately for each matrix. The outputs are lists of floating-point numbers.", "solution": "The subject of this problem is the efficient computation of the diagonal entries of a matrix inverse, $(A^{-1})_{ii}$, without explicitly computing the full inverse matrix $A^{-1}$. The methodology relies on the Lower-Upper (LU) factorization of the matrix $A$ with partial pivoting, represented as $PA = LU$.\n\nLet $A$ be a nonsingular square matrix of dimension $n \\times n$. Its inverse, $A^{-1}$, is the unique matrix such that $AA^{-1} = I$, where $I$ is the identity matrix. Let the columns of $A^{-1}$ be denoted by $X_j$ and the columns of $I$ by $e_j$ (the standard basis vectors), for $j \\in \\{0, 1, \\dots, n-1\\}$. The definition of matrix multiplication implies that the $j$-th column of the inverse, $X_j$, is the solution to the linear system:\n$$AX_j = e_j$$\nThe diagonal entries of $A^{-1}$ are the elements $(A^{-1})_{jj}$. The element $(A^{-1})_{jj}$ is the $j$-th component of the column vector $X_j$. Thus, to find $(A^{-1})_{jj}$, we must compute the $j$-th component of the solution to the system $AX_j = e_j$.\n\nThe problem provides the LU factorization with partial pivoting, $PA = LU$. Here, $P$ is a permutation matrix, $L$ is a unit lower triangular matrix (with ones on its diagonal), and $U$ is an upper triangular matrix. We can use this factorization to solve the system $AX_j = e_j$ efficiently.\n\nFirst, we pre-multiply by $P$:\n$$PAX_j = Pe_j$$\nSubstituting $PA = LU$:\n$$LUX_j = Pe_j$$\nThis matrix equation can be solved as a sequence of two simpler triangular systems. Let $Y_j = UX_j$. Then the first system is:\n$$LY_j = Pe_j$$\nThis is solved for the intermediate vector $Y_j$ using forward substitution. Once $Y_j$ is known, the second system is:\n$$UX_j = Y_j$$\nThis is solved for the desired vector $X_j$ using backward substitution.\n\nThe right-hand side of the first system, $Pe_j$, requires careful interpretation. The permutation matrix $P$ is represented by a permutation vector $p$ of length $n$, where $p_i$ is the index of the original row of $A$ that has been moved to row $i$ of the permuted matrix $PA$. That is, $(PA)_{i,:} = A_{p_i,:}$. The matrix $P$ is formally defined by $P_{ik} = 1$ if $k=p_i$ and $0$ otherwise. When $P$ acts on a column vector $v$, the result is a new vector $w=Pv$ where $w_i = v_{p_i}$.\n\nThe vector $Pe_j$ is the $j$-th column of the matrix $P$. A column $j$ of a matrix $P$ contains a $1$ at row $k$ if $P_{kj}=1$, and zeros elsewhere. According to the definition of our permutation vector $p$, the matrix entry $P_{ki}$ is non-zero (and equal to $1$) only if original row $i$ is moved to row $k$, which is not how we defined $p$. Let us rigorously define $P$. The action of $P$ on a matrix $A$ is such that the $i$-th row of $PA$ is the $p_i$-th row of $A$. Symbolically, $(PA)_{i,:} = A_{p_i,:}$. The permutation matrix $P$ for this operation can be written as $P = \\sum_{i=0}^{n-1} e_i e_{p_i}^T$.\nThe term $Pe_j$ is the $j$-th column of $P$. This is given by $(Pe_j)_k = P_{kj}$. From the sum representation, $P_{kj} = \\sum_{i=0}^{n-1} (e_i)_k (e_{p_i}^T)_j = \\sum_{i=0}^{n-1} \\delta_{ik} \\delta_{p_i, j} = \\delta_{p_k, j}$. This means $Pe_j$ is a vector that is zero everywhere except for a $1$ at the position $k$ for which $p_k = j$. This is the standard basis vector $e_k$. The index $k$ can be found by inverting the permutation map $p$.\n\nThe overall algorithm to compute the diagonal of $A^{-1}$ is as follows:\n\n1.  **Decomposition**: Compute the factorization $PA=LU$ using Gaussian elimination with partial pivoting. This yields the matrices $L$ and $U$, and the permutation vector $p$.\n2.  **Inverse Permutation**: Compute the inverse permutation vector, $p_{inv}$, such that $p_{inv}[j]$ gives the index $k$ for which $p[k]=j$.\n3.  **Iterate for each diagonal entry**: For each index $j$ from $0$ to $n-1$:\n    a.  **Identify Right-Hand Side**: Determine the row index $k = p_{inv}[j]$. The right-hand side for the system $LY_j = Pe_j$ is the basis vector $e_k$.\n    b.  **Forward Substitution**: Solve the unit lower triangular system $LY_j = e_k$ for the vector $Y_j$. Since $L$ has a unit diagonal, this is given by:\n        $$ (Y_j)_i = (e_k)_i - \\sum_{m=0}^{i-1} L_{im} (Y_j)_m $$\n    c.  **Backward Substitution**: Solve the upper triangular system $UX_j = Y_j$ for the vector $X_j$. This is given by:\n        $$ (X_j)_i = \\frac{1}{U_{ii}} \\left( (Y_j)_i - \\sum_{m=i+1}^{n-1} U_{im} (X_j)_m \\right) $$\n    d.  **Extract Diagonal Element**: The desired diagonal entry $(A^{-1})_{jj}$ is the $j$-th component of the computed solution vector, $(X_j)_j$.\n\nThis procedure is more efficient than computing the full inverse matrix, as it only calculates the necessary columns of $A^{-1}$ (and only the components of those columns that are needed for the subsequent calculation of the diagonal entry). Specifically, for each $j$, we perform one forward and one backward substitution, an operation of complexity $O(n^2)$. Repeating for all $n$ diagonal elements results in a total complexity of $O(n^3)$, but with a smaller prefactor than a full matrix inversion and with significantly reduced memory requirements, as the full inverse is never stored.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef forward_substitution(L, b):\n    \"\"\"\n    Solves the system Lx = b for x, where L is a unit lower triangular matrix.\n    Since L is unit triangular, L[i, i] is always 1.\n    \"\"\"\n    n = L.shape[0]\n    x = np.zeros(n, dtype=np.float64)\n    for i in range(n):\n        # x[i] = (b[i] - L[i, :i] @ x[:i]) / L[i, i]\n        # Since L is unit triangular, L[i, i] = 1\n        x[i] = b[i] - np.dot(L[i, :i], x[:i])\n    return x\n\ndef backward_substitution(U, y):\n    \"\"\"\n    Solves the system Ux = y for x, where U is an upper triangular matrix.\n    \"\"\"\n    n = U.shape[0]\n    x = np.zeros(n, dtype=np.float64)\n    for i in range(n - 1, -1, -1):\n        if U[i, i] == 0:\n            # Should not happen for a nonsingular matrix\n            raise ValueError(\"Matrix is singular or near-singular.\")\n        x[i] = (y[i] - np.dot(U[i, i+1:], x[i+1:])) / U[i, i]\n    return x\n\ndef lu_decomposition_pivoting(A):\n    \"\"\"\n    Computes the PA = LU factorization of a square matrix A.\n    \n    Args:\n        A (np.ndarray): The input square matrix.\n    \n    Returns:\n        L (np.ndarray): The unit lower triangular matrix.\n        U (np.ndarray): The upper triangular matrix.\n        p (np.ndarray): The permutation vector. p[i] gives the original row index\n                          that is in row i of the permuted matrix.\n    \"\"\"\n    n = A.shape[0]\n    LU = A.copy().astype(np.float64)\n    p = np.arange(n)\n    \n    for k in range(n - 1):\n        # Find pivot: row with the largest element in column k (from k downwards)\n        pivot_row = k + np.argmax(np.abs(LU[k:, k]))\n        \n        # Swap rows if necessary\n        if pivot_row != k:\n            LU[[k, pivot_row], :] = LU[[pivot_row, k], :]\n            p[[k, pivot_row]] = p[[pivot_row, k]]\n            \n        # Check for singularity\n        if LU[k, k] == 0:\n            # This indicates the matrix is singular, as even with pivoting, a zero pivot was found.\n            # For the given test cases, this should not be reached.\n            continue\n            \n        # Compute multipliers and store them in the lower part of LU\n        factors = LU[k+1:, k] / LU[k, k]\n        LU[k+1:, k] = factors\n        \n        # Update the trailing submatrix\n        LU[k+1:, k+1:] -= np.outer(factors, LU[k, k+1:])\n        \n    L = np.tril(LU, k=-1) + np.eye(n)\n    U = np.triu(LU)\n    \n    return L, U, p\n\ndef compute_diag_inverse(A):\n    \"\"\"\n    Computes the diagonal entries of the inverse of matrix A.\n    \"\"\"\n    n = A.shape[0]\n    \n    # Handle 1x1 case explicitly for simplicity\n    if n == 1:\n        return [1.0 / A[0, 0]]\n        \n    try:\n        L, U, p = lu_decomposition_pivoting(A)\n    except ValueError as e:\n        # Handle cases flagged as singular during factorization\n        print(f\"Error during LU decomposition: {e}\")\n        return []\n\n    # Create the inverse permutation vector p_inv\n    # p_inv[j] = k means original row j is now at row k, but we need the other way\n    # We need k such that p[k] = j.\n    p_inv = np.argsort(p)\n    \n    diag_entries = []\n    \n    for j in range(n):\n        # To find (A_inv)_jj, we solve A * X_j = e_j and take the j-th component of X_j.\n        # This is equivalent to solving LUX_j = P*e_j\n        # P*e_j is the basis vector e_k where p[k] = j.\n        \n        k = p_inv[j]\n        e_k = np.zeros(n)\n        e_k[k] = 1.0\n        \n        # 1. Solve L * y = e_k for y (forward substitution)\n        y = forward_substitution(L, e_k)\n        \n        # 2. Solve U * x = y for x (backward substitution)\n        x = backward_substitution(U, y)\n        \n        # x is the j-th column of A_inv. We need the j-th component.\n        diag_entries.append(x[j])\n        \n    return diag_entries\n\ndef solve():\n    \"\"\"\n    Main solver function that processes test cases.\n    \"\"\"\n    test_cases = [\n        np.array([[4, 1, -2], [1, 3, 0], [-2, 0, 5]]),\n        np.array([[1, 0, 0], [0, 2, 0], [0, 0, 3]]),\n        np.array([[0, 1, 2], [1, 0, 3], [4, 5, 6]]),\n        np.array([[7]]),\n        np.array([[1e-8, 1, 0], [0, 1e-8, 1], [1, 0, 1e-8]])\n    ]\n    \n    # Calculate the diagonal of the inverse for each test case\n    results = [compute_diag_inverse(A) for A in test_cases]\n    \n    # Format the output as specified\n    output_str = \"[\" + \",\".join([str(res) for res in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3275751"}]}