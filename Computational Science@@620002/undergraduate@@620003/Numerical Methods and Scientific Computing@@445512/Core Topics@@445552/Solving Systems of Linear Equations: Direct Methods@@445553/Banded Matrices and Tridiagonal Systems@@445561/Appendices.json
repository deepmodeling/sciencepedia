{"hands_on_practices": [{"introduction": "While the general Thomas algorithm is efficient, many real-world problems, such as those from discretizing diffusion equations, yield symmetric tridiagonal matrices. This exercise [@problem_id:3208596] challenges you to exploit this symmetry, deriving a specialized solver that reduces both storage and computational cost by relating the elimination process to an $L D L^{\\mathsf{T}}$ factorization.", "problem": "Consider a linear system $A x = f$ where $A$ is a symmetric tridiagonal matrix of dimension $n \\times n$. By definition, $A$ has nonzero entries only on its main diagonal and its first sub- and super-diagonals, and symmetry implies that the sub-diagonal equals the super-diagonal. Let the main diagonal be specified by $a_1,a_2,\\dots,a_n$ and the sub- (and super-) diagonal by $b_1,b_2,\\dots,b_{n-1}$, so that $A_{i,i} = a_i$ and $A_{i,i+1} = A_{i+1,i} = b_i$ for $i = 1,2,\\dots,n-1$. The vector $x$ is the unknown, and $f$ is a given right-hand side.\n\nStarting from the fundamental base of Gaussian elimination for banded systems and the definition of symmetry, derive a specialized elimination and substitution scheme that exploits symmetry to minimize stored quantities and arithmetic operations. Specifically:\n- Begin from Gaussian elimination written in terms of local operations on adjacent rows for tridiagonal matrices and use the symmetry of $A$ to reason about the structure preserved during elimination. Show that the elimination can be organized using a single sequence of multipliers and a single sequence of pivots without needing to store distinct sub- and super-diagonal arrays.\n- Justify, using properties of symmetric matrices and leading principal minors, that the elimination leads to a lower-triangular and diagonal structure that can be used for efficient forward and backward substitution on any single right-hand side $f$ for which the elimination does not encounter a zero pivot.\n- Provide a clear, stepwise algorithm that first performs a forward sweep computing the necessary scalar sequences and an intermediate vector, then a diagonal solve, then a backward sweep, all while using only the main diagonal array $a_i$, one off-diagonal array $b_i$, and the right-hand side $f$.\n- Analyze the storage requirements and operation count of your specialized scheme compared to a generic Thomas algorithm for a non-symmetric tridiagonal system, highlighting precisely what is reduced when symmetry is exploited.\n\nImplement your derived algorithm in a single, runnable program. The program must solve the system $A x = f$ for each of the following test cases, which together probe typical, boundary, and edge behaviors:\n\n- Test case $1$ (symmetric positive definite \"happy path\"): $n = 6$, $a_i = 2$ for $i = 1,\\dots,6$, $b_i = -1$ for $i = 1,\\dots,5$, and $f = [1, 1, 1, 1, 1, 1]$.\n- Test case $2$ (boundary size): $n = 1$, $a_1 = 3$, $b$ is empty, and $f = [6]$.\n- Test case $3$ (symmetric indefinite but nonsingular): $n = 3$, $a = [0.5, -1.2, 0.5]$, $b = [1.0, 1.0]$, and $f = [1.0, 2.0, 3.0]$.\n- Test case $4$ (diagonal matrix as a tridiagonal with zero off-diagonal): $n = 3$, $a = [5, 7, 9]$, $b = [0, 0]$, and $f = [10, 14, 27]$.\n\nAll inputs are dimensionless real numbers. You must express outputs as real numbers. For each test case, the output is the solution vector $x$ as a list of floats.\n\nFinal output format specification:\n- Your program should produce a single line of output containing the solutions for the four test cases as a comma-separated list of lists enclosed in square brackets, with no spaces. For example, if the four solution vectors were $[\\,x_1,x_2\\,]$, $[\\,y_1\\,]$, $[\\,z_1,z_2,z_3\\,]$, and $[\\,w_1,w_2,w_3\\,]$, the format would be exactly $[[x_1,x_2],[y_1],[z_1,z_2,z_3],[w_1,w_2,w_3]]$.", "solution": "We begin from Gaussian elimination on tridiagonal systems. A tridiagonal matrix has nonzero entries only on the main diagonal and the first sub- and super-diagonals. For a general tridiagonal system $A x = f$, the Thomas algorithm performs forward elimination to zero out the sub-diagonal entries step-by-step, producing an upper-triangular system, followed by backward substitution.\n\nThe specialized setting here is that $A$ is symmetric tridiagonal. Symmetry implies $A_{i+1,i} = A_{i,i+1}$, so only one off-diagonal array is needed. Let the main diagonal be $a_1,\\dots,a_n$ and the off-diagonal be $b_1,\\dots,b_{n-1}$. The local elimination at step $i$ (for $i = 2,\\dots,n$) uses the pivot at row $i-1$ to eliminate the sub-diagonal entry at row $i$. In a banded system, elimination affects only adjacent rows due to sparsity.\n\nFundamental base: Gaussian elimination for tridiagonal matrices expresses the elimination of $A_{i,i-1}$ by adding to row $i$ a multiple $\\ell_i$ of row $i-1$, where the multiple is chosen so that the new sub-diagonal entry becomes zero. The value of the multiple is\n$$\n\\ell_i = \\frac{A_{i,i-1}}{\\tilde{A}_{i-1,i-1}},\n$$\nwhere $\\tilde{A}$ denotes the running, modified coefficient at that step after previous eliminations. For tridiagonal matrices, after elimination at step $i-1$, the only entries in row $i-1$ that may affect row $i$ are the diagonal $\\tilde{A}_{i-1,i-1}$ and the super-diagonal $\\tilde{A}_{i-1,i}$. By symmetry and bandedness, $\\tilde{A}_{i-1,i}$ equals the off-diagonal $b_{i-1}$ because the structure preserved by adjacent-row elimination does not introduce fill beyond the first super-diagonal.\n\nThus the elimination at step $i$ updates the diagonal of row $i$ via a rank-one correction dependent on $\\ell_i$ and the off-diagonal $b_{i-1}$, and the right-hand side $f$ receives an analogous update for the forward substitution. Concretely, one maintains a sequence of pivots denoted by $d_i = \\tilde{A}_{i,i}$ and multipliers denoted by $\\ell_i = \\tilde{A}_{i,i-1}/d_{i-1}$, with $d_1 = a_1$ and $\\ell_1$ unused. The forward sweep obeys the recurrence\n$$\n\\ell_i = \\frac{b_{i-1}}{d_{i-1}}, \\quad d_i = a_i - \\ell_i\\,b_{i-1}, \\quad y_i = f_i - \\ell_i\\,y_{i-1},\n$$\nfor $i = 2,\\dots,n$, with initialization $y_1 = f_1$. These relations arise directly from Gaussian elimination: subtracting $\\ell_i$ times row $i-1$ from row $i$ eliminates the sub-diagonal, and because the only nonzero in row $i-1$ beyond its diagonal is the super-diagonal $b_{i-1}$, the diagonal of row $i$ is reduced by $\\ell_i b_{i-1}$; the right-hand side at row $i$ is reduced by $\\ell_i y_{i-1}$.\n\nThe structure can be interpreted as an $L D L^{\\mathsf{T}}$ factorization, a well-tested fact for symmetric matrices under elimination without pivot breakdown when all leading principal minors are nonzero. Specifically,\n$$\nA = L D L^{\\mathsf{T}},\n$$\nwhere $L$ is unit lower-triangular with sub-diagonal entries $\\ell_2,\\dots,\\ell_n$ and ones on the diagonal, and $D$ is diagonal with entries $d_1,\\dots,d_n$. This follows from the fact that each elimination step updates only the current row using a multiple of the previous row, preserving symmetry and the tridiagonal structure; the accumulated effect is equivalent to factorizing $A$ into a unit lower-triangular matrix $L$, a diagonal matrix $D$, and the transpose of $L$. The conditions for this factorization to hold without pivoting are that $d_i \\neq 0$ for all $i$, which is equivalent to nonzero leading principal minors.\n\nOnce $A = L D L^{\\mathsf{T}}$ is established, solving $A x = f$ proceeds in three principled steps rooted in the factorization:\n1. Forward substitution to solve $L y = f$. This uses the recurrence\n$$\ny_1 = f_1, \\quad y_i = f_i - \\ell_i y_{i-1} \\quad \\text{for } i = 2,\\dots,n.\n$$\n2. Diagonal solve to obtain $w$ from $D w = y$:\n$$\nw_i = \\frac{y_i}{d_i} \\quad \\text{for } i = 1,\\dots,n.\n$$\n3. Backward substitution to solve $L^{\\mathsf{T}} x = w$. Because $L^{\\mathsf{T}}$ is unit upper-triangular with super-diagonal entries $\\ell_2,\\dots,\\ell_n$, the recurrence is\n$$\nx_n = w_n, \\quad x_i = w_i - \\ell_{i+1} x_{i+1} \\quad \\text{for } i = n-1,n-2,\\dots,1.\n$$\n\nStorage reduction arises because symmetry eliminates the need to carry separate sub- and super-diagonal arrays; only one off-diagonal array is required. The factorization stores one scalar sequence of multipliers $\\ell_i$ and one scalar sequence of pivots $d_i$, totaling $n + (n-1)$ scalars for the matrix factors. In a generic Thomas algorithm for non-symmetric tridiagonal matrices, three diagonal arrays must be carried ($n$ main, $(n-1)$ sub-, $(n-1)$ super-diagonal), and the forward sweep typically computes and uses modified upper diagonal entries as well, leading to more memory traffic.\n\nOperation count analysis:\n- Forward sweep with symmetry and tridiagonality computes, at each step $i = 2,\\dots,n$, one division for $\\ell_i$, one multiplication and one subtraction to update $d_i$, and one multiplication and one subtraction to update $y_i$. This is one division and four floating-point operations per step, totaling $(n-1)$ divisions and $4(n-1)$ basic floating-point operations.\n- Diagonal solve performs $n$ divisions to compute $w_i$.\n- Backward substitution performs, for $i = n-1,\\dots,1$, one multiplication and one subtraction per step, totaling $2(n-1)$ floating-point operations.\n\nIn comparison, a generic Thomas algorithm for a non-symmetric tridiagonal matrix performs a forward sweep that, per step, computes a multiplier (one division), updates the main diagonal (one multiplication and one subtraction), and updates the right-hand side (one multiplication and one subtraction), and must also reference a distinct super-diagonal array. While the count of arithmetic operations per step can be similar in leading order, exploiting symmetry removes the need to store or access a separate super-diagonal array and enables a factorization form that is naturally reusable for multiple right-hand sides without recomputing eliminations, thereby reducing the effective operation count per solve when multiple $f$ are present.\n\nAlgorithm summary suitable for implementation:\n- Inputs: arrays $a$ of length $n$ (main diagonal), $b$ of length $n-1$ (off-diagonal), and $f$ of length $n$.\n- Forward factorization and substitution:\n  - Set $d_1 = a_1$, $y_1 = f_1$.\n  - For $i = 2$ to $n$:\n    - Compute $\\ell_i = b_{i-1} / d_{i-1}$.\n    - Compute $d_i = a_i - \\ell_i b_{i-1}$.\n    - Compute $y_i = f_i - \\ell_i y_{i-1}$.\n- Diagonal solve:\n  - For $i = 1$ to $n$, set $w_i = y_i / d_i$.\n- Backward substitution:\n  - Set $x_n = w_n$.\n  - For $i = n-1$ down to $1$, set $x_i = w_i - \\ell_{i+1} x_{i+1}$.\n- Output $x$.\n\nThe test suite consists of four cases:\n- Case $1$: $n = 6$, $a_i = 2$ for $i = 1,\\dots,6$, $b_i = -1$ for $i = 1,\\dots,5$, $f = [1, 1, 1, 1, 1, 1]$.\n- Case $2$: $n = 1$, $a_1 = 3$, $b$ empty, $f = [6]$.\n- Case $3$: $n = 3$, $a = [0.5, -1.2, 0.5]$, $b = [1.0, 1.0]$, $f = [1.0, 2.0, 3.0]$.\n- Case $4$: $n = 3$, $a = [5, 7, 9]$, $b = [0, 0]$, $f = [10, 14, 27]$.\n\nThe program must print a single line containing the four solution vectors in the specified nested-list format without spaces: $[[\\cdot],[\\cdot],[\\cdot],[\\cdot]]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef symmetric_tridiagonal_ldl_solve(a, b, f):\n    \"\"\"\n    Solve A x = f for symmetric tridiagonal A using LDL^T factorization.\n    A has main diagonal a (length n) and off-diagonal b (length n-1).\n    Returns x as a list of floats.\n    \"\"\"\n    n = len(a)\n    if len(f) != n:\n        raise ValueError(\"Length of f must equal length of a.\")\n    if n == 0:\n        return []\n    if len(b) not in (0, max(0, n - 1)):\n        raise ValueError(\"Length of b must be 0 or n-1.\")\n\n    # Allocate arrays\n    d = np.empty(n, dtype=float)         # diagonal of D\n    l = np.empty(n, dtype=float)         # subdiagonal of L (l[0] unused/zero)\n    y = np.empty(n, dtype=float)         # intermediate vector for Ly=f\n\n    # Forward factorization and forward substitution\n    d[0] = float(a[0])\n    l[0] = 0.0\n    y[0] = float(f[0])\n    for i in range(1, n):\n        bi_1 = float(b[i - 1]) if len(b) > 0 else 0.0\n        li = bi_1 / d[i - 1]\n        l[i] = li\n        d[i] = float(a[i]) - li * bi_1\n        y[i] = float(f[i]) - li * y[i - 1]\n\n    # Diagonal solve: D w = y\n    w = y / d\n\n    # Backward substitution: L^T x = w\n    x = np.empty(n, dtype=float)\n    x[n - 1] = w[n - 1]\n    for i in range(n - 2, -1, -1):\n        x[i] = w[i] - l[i + 1] * x[i + 1]\n\n    return x.tolist()\n\ndef format_nested_list_no_spaces(list_of_lists):\n    \"\"\"\n    Format a list of lists of numbers as a string without spaces, e.g.,\n    [[1.0,2.0],[3.0]] -> '[[1.0,2.0],[3.0]]' but with no spaces.\n    \"\"\"\n    def format_list(lst):\n        return \"[\" + \",\".join(f\"{num:.12g}\" if isinstance(num, float) else str(num) for num in lst) + \"]\"\n    return \"[\" + \",\".join(format_list(inner) for inner in list_of_lists) + \"]\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (a, b, f)\n    test_cases = [\n        # Case 1: n=6, a_i=2, b_i=-1, f=ones\n        ([2, 2, 2, 2, 2, 2], [-1, -1, -1, -1, -1], [1, 1, 1, 1, 1, 1]),\n        # Case 2: n=1, a=[3], b=[], f=[6]\n        ([3], [], [6]),\n        # Case 3: n=3, symmetric indefinite\n        ([0.5, -1.2, 0.5], [1.0, 1.0], [1.0, 2.0, 3.0]),\n        # Case 4: n=3, diagonal matrix (b zeros)\n        ([5, 7, 9], [0, 0], [10, 14, 27]),\n    ]\n\n    results = []\n    for a, b, f in test_cases:\n        x = symmetric_tridiagonal_ldl_solve(a, b, f)\n        results.append(x)\n\n    # Final print statement in the exact required format: no spaces.\n    print(format_nested_list_no_spaces(results))\n\nsolve()\n```", "id": "3208596"}, {"introduction": "Not all sparse systems fit a perfect pattern; sometimes, physical conditions like periodic boundaries introduce a few 'stray' non-zero entries that break the tridiagonal structure. This practice [@problem_id:3208725] demonstrates a powerful and general strategy for such 'almost-tridiagonal' systems, using the Sherman-Morrison formula to correct the solution from a purely tridiagonal solve. This method elegantly handles the perturbation by solving two simpler tridiagonal systems, showcasing a common theme in numerical computing: leveraging known solutions for related problems.", "problem": "Consider a linear system $A x = b$ of size $n \\times n$ where $A$ is an \"almost\" tridiagonal matrix obtained by adding a single non-zero entry in the top-right corner to a strictly tridiagonal matrix. Let $T$ denote the strictly tridiagonal part of $A$ and let the extra entry be at position $(1,n)$ with value $\\gamma \\in \\mathbb{R}$. The matrix $T$ has a non-zero subdiagonal $a$, diagonal $d$, and superdiagonal $c$. The matrix $A$ has the structure $A = T + \\gamma e_1 e_n^{\\top}$, where $e_1$ and $e_n$ are the first and last standard basis vectors in $\\mathbb{R}^n$.\n\nYour task is to design and implement a solver that:\n- Uses a stable tridiagonal solver for the system $T y = b$ based on forward elimination and backward substitution (commonly known as the Thomas algorithm).\n- Incorporates a principled rank-$1$ update to account for the extra entry at $(1,n)$, relying on foundational linear algebra identities about inverses of rank-$1$ perturbations, without assuming any shortcut formulas in advance.\n- Produces numerically robust solutions for all provided test cases.\n\nThe foundational starting points include:\n- The definition of tridiagonal and banded matrices.\n- The existence and uniqueness of solutions for strictly diagonally dominant systems.\n- The fact that solving a tridiagonal system by elimination can be done in $\\mathcal{O}(n)$ time.\n- The property that the inverse of a matrix perturbed by a rank-$1$ matrix can be expressed in terms of the inverse of the original matrix and the perturbation vector outer product.\n\nYou must implement the algorithm by deriving the necessary steps from these principles. The implementation must solve the following test suite of five cases. Each test case specifies $n$, the tridiagonal arrays $a$, $d$, $c$, the scalar $\\gamma$, and the right-hand side vector $b$. All numerical values are real numbers, and all vectors are of length $n$.\n\nTest suite:\n- Case $1$: $n = 5$, $a = [-1,-1,-1,-1]$, $d = [2,2,2,2,2]$, $c = [-1,-1,-1,-1]$, $\\gamma = 0.5$, $b = [1,1,1,1,1]$.\n- Case $2$: $n = 4$, $a = [-1,-1,-1]$, $d = [2,2,2,2]$, $c = [-1,-1,-1]$, $\\gamma = 0$, $b = [1,2,3,4]$.\n- Case $3$: $n = 6$, $a = [-1,-1,-1,-1,-1]$, $d = [2,2,2,2,2,2]$, $c = [-1,-1,-1,-1,-1]$, $\\gamma = -6.999$, $b = [0,1,0,1,0,1]$.\n- Case $4$: $n = 2$, $a = [-1]$, $d = [2,2]$, $c = [-1]$, $\\gamma = 0.3$, $b = [2,1]$.\n- Case $5$: $n = 5$, $a = [-1,-0.5,-1.5,-1.0]$, $d = [4,3.5,5,4.2,3.8]$, $c = [-1,-1.2,-0.7,-0.9]$, $\\gamma = 0.7$, $b = [1,0.5,-1,0.25,2]$.\n\nYour program must:\n- Implement a tridiagonal solver for $T y = b$ in $\\mathcal{O}(n)$ time using forward elimination and backward substitution.\n- Implement a rank-$1$ update using only fundamental linear algebra identities to obtain the solution $x$ to $A x = b$ from solutions of tridiagonal systems.\n- For each test case, compute the solution vector $x$ and round each component to $6$ decimal places.\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is itself a bracketed list representing the solution vector for one test case. For example, the format should be $[[$x_{1,1},\\dots,x_{1,n_1}$],[$x_{2,1},\\dots,x_{2,n_2}$],\\dots]$, with no spaces.\n- There are no physical units involved, and no angle units are required.\n- Each test case's answer is a list of floats rounded to $6$ decimal places.", "solution": "The user-provided problem has been validated and is determined to be a valid, well-posed problem in numerical linear algebra. The task is to solve a system of linear equations $A x = b$ where the matrix $A$ is a tridiagonal matrix $T$ perturbed by a rank-$1$ matrix.\n\nThe system is defined as:\n$$A x = b$$\nwhere $A$ is an $n \\times n$ matrix given by\n$$A = T + \\gamma e_1 e_n^{\\top}$$\nHere, $T$ is a strictly tridiagonal matrix with subdiagonal elements $a_i$ (for $i=2, \\dots, n$), diagonal elements $d_i$ (for $i=1, \\dots, n$), and superdiagonal elements $c_i$ (for $i=1, \\dots, n-1$). The term $\\gamma e_1 e_n^{\\top}$ represents a rank-$1$ update, where $\\gamma \\in \\mathbb{R}$ is a scalar, and $e_1$ and $e_n$ are the first and last standard basis vectors in $\\mathbb{R}^n$, respectively. This update adds a single non-zero entry, $\\gamma$, at position $(1, n)$ of the matrix $T$.\n\nThe solution $x$ can be formally written as $x = A^{-1} b$. To derive a computational algorithm, we rely on the Sherman-Morrison formula, a foundational identity for the inverse of a rank-$1$ perturbed matrix. The formula states that for an invertible matrix $B$ and vectors $u, v$, the inverse of $B + uv^{\\top}$ is:\n$$(B + uv^{\\top})^{-1} = B^{-1} - \\frac{B^{-1} u v^{\\top} B^{-1}}{1 + v^{\\top} B^{-1} u}$$\nThis formula is valid provided that $1 + v^{\\top} B^{-1} u \\neq 0$.\n\nIn our problem, we identify $B=T$, $u = \\gamma e_1$, and $v = e_n$. Substituting these into the formula, we get the inverse of $A$:\n$$A^{-1} = \\left(T + (\\gamma e_1)e_n^{\\top}\\right)^{-1} = T^{-1} - \\frac{T^{-1}(\\gamma e_1)e_n^{\\top}T^{-1}}{1 + e_n^{\\top}T^{-1}(\\gamma e_1)}$$\nThe solution vector $x$ is then obtained by multiplying $A^{-1}$ by $b$:\n$$x = A^{-1} b = T^{-1} b - \\frac{T^{-1}(\\gamma e_1)e_n^{\\top}T^{-1}b}{1 + \\gamma e_n^{\\top}T^{-1}e_1}$$\nTo make this expression computationally tractable, we introduce two auxiliary vectors, $y$ and $z$, which are solutions to tridiagonal systems involving the matrix $T$:\n1. Let $y = T^{-1} b$. This is equivalent to solving the tridiagonal system $T y = b$.\n2. Let $z = T^{-1} e_1$. This is equivalent to solving the tridiagonal system $T z = e_1$.\n\nUsing these definitions, we can simplify the expression for $x$.\n- The term $e_n^{\\top}T^{-1}b$ is the $n$-th component of the vector $y$, which we denote as $y_n$.\n- The term $e_n^{\\top}T^{-1}e_1$ is the $n$-th component of the vector $z$, which we denote as $z_n$.\n- The term $T^{-1}(\\gamma e_1)$ becomes $\\gamma (T^{-1}e_1) = \\gamma z$.\n\nSubstituting these into the equation for $x$, we arrive at the final expression:\n$$x = y - \\frac{\\gamma y_n}{1 + \\gamma z_n} z$$\nThis elegant formula reduces the problem of solving the almost-tridiagonal system to solving two strictly tridiagonal systems and performing a simple vector update. If $\\gamma = 0$, the formula correctly simplifies to $x=y$, as $A=T$. The singularity of $A$ depends on the denominator $1 + \\gamma z_n$ being non-zero.\n\nThe two tridiagonal systems, $T y = b$ and $T z = e_1$, can be efficiently solved in $\\mathcal{O}(n)$ time using the Thomas algorithm. We employ a stable variant of the algorithm based on LU decomposition. For a tridiagonal system $T u = f$, the algorithm proceeds in three stages:\n\n1.  **LU Factorization**: The matrix $T$ is decomposed into $T = LU$, where $L$ is a lower bidiagonal matrix and $U$ is an upper bidiagonal matrix with ones on its diagonal. The non-zero elements of $L$ (diagonal $\\alpha_i$) and $U$ (superdiagonal $\\delta_i$) are computed via the following recurrences (using $0$-based indexing for arrays $a, d, c$):\n    $$ \\alpha_0 = d_0 $$\n    $$ \\delta_0 = c_0 / \\alpha_0 $$\n    For $i = 1, \\dots, n-2$:\n    $$ \\alpha_i = d_i - a_{i-1} \\delta_{i-1} $$\n    $$ \\delta_i = c_i / \\alpha_i $$\n    $$ \\alpha_{n-1} = d_{n-1} - a_{n-2} \\delta_{n-2} $$\n\n2.  **Forward Substitution**: The system $T u = LU u = f$ is split into two simpler systems. First, we solve $L w = f$ for an intermediate vector $w$.\n    $$ w_0 = f_0 / \\alpha_0 $$\n    For $i = 1, \\dots, n-1$:\n    $$ w_i = (f_i - a_{i-1} w_{i-1}) / \\alpha_i $$\n\n3.  **Backward Substitution**: Finally, we solve $U u = w$ for the solution vector $u$.\n    $$ u_{n-1} = w_{n-1} $$\n    For $i = n-2, \\dots, 0$:\n    $$ u_i = w_i - \\delta_i u_{i+1} $$\n\nThe overall algorithm is as follows:\n1.  Handle the trivial case: if $\\gamma=0$, solve $Tx=b$ using the Thomas algorithm and return $x$.\n2.  Solve $Ty = b$ for $y$ using the Thomas algorithm.\n3.  Solve $Tz = e_1$ for $z$ using the Thomas algorithm.\n4.  Extract the last components, $y_{n-1}$ and $z_{n-1}$ (using $0$-based indexing).\n5.  Calculate the scalar factor $\\beta = \\frac{\\gamma y_{n-1}}{1 + \\gamma z_{n-1}}$.\n6.  Compute the final solution vector $x = y - \\beta z$.\n7.  Return the resulting vector $x$.\nThis procedure is numerically stable for diagonally dominant matrices $T$ and provides an efficient $\\mathcal{O}(n)$ solution to the problem.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, solve them, and print the results.\n    \"\"\"\n\n    def tridiagonal_solver(a, d, c, f):\n        \"\"\"\n        Solves a tridiagonal linear system Tu=f using an LU decomposition-based\n        Thomas algorithm.\n        \n        Args:\n            a: Subdiagonal elements (length n-1).\n            d: Diagonal elements (length n).\n            c: Superdiagonal elements (length n-1).\n            f: Right-hand side vector (length n).\n            \n        Returns:\n            The solution vector u (numpy array of length n).\n        \"\"\"\n        n = len(d)\n        \n        # Ensure inputs are numpy arrays and create copies to avoid mutation.\n        ac, dc, cc, fc = map(lambda v: np.array(v, dtype=float), (a, d, c, f))\n\n        # LU Factorization (T = LU)\n        # alpha is the diagonal of L, delta is the superdiagonal of U.\n        alpha = np.zeros(n)\n        delta = np.zeros(n - 1)\n\n        alpha[0] = dc[0]\n        if n > 1:\n            delta[0] = cc[0] / alpha[0]\n\n        for i in range(1, n - 1):\n            alpha[i] = dc[i] - ac[i-1] * delta[i-1]\n            delta[i] = cc[i] / alpha[i]\n        \n        if n > 1:\n            alpha[n-1] = dc[n-1] - ac[n-2] * delta[n-2]\n\n        # Forward Substitution: Solve Lw = f for w\n        w = np.zeros(n)\n        w[0] = fc[0] / alpha[0]\n        for i in range(1, n):\n            w[i] = (fc[i] - ac[i-1] * w[i-1]) / alpha[i]\n            \n        # Backward Substitution: Solve Uu = w for u\n        u = np.zeros(n)\n        u[n-1] = w[n-1]\n        for i in range(n-2, -1, -1):\n            u[i] = w[i] - delta[i] * u[i+1]\n            \n        return u\n\n    def solve_almost_tridiagonal(n, a, d, c, gamma, b):\n        \"\"\"\n        Solves the almost-tridiagonal system using the Sherman-Morrison formula.\n        \"\"\"\n        # If gamma is zero, the system is purely tridiagonal.\n        if gamma == 0.0:\n            return tridiagonal_solver(a, d, c, b)\n        \n        # Step 1: Solve Ty = b\n        y = tridiagonal_solver(a, d, c, b)\n        \n        # Step 2: Solve Tz = e_1\n        e1 = np.zeros(n)\n        e1[0] = 1.0\n        z = tridiagonal_solver(a, d, c, e1)\n        \n        # Step 3: Apply the Sherman-Morrison update formula\n        # x = y - (gamma * y_n / (1 + gamma * z_n)) * z\n        y_n = y[-1]\n        z_n = z[-1]\n        \n        denominator = 1.0 + gamma * z_n\n        \n        if abs(denominator) < 1e-15:\n            # This case should not occur with the given test data, but it's\n            # good practice to guard against singular matrices.\n            raise ValueError(\"Matrix A is singular or nearly singular.\")\n        \n        beta = (gamma * y_n) / denominator\n        x = y - beta * z\n        return x\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        (5, [-1,-1,-1,-1], [2,2,2,2,2], [-1,-1,-1,-1], 0.5, [1,1,1,1,1]),\n        # Case 2\n        (4, [-1,-1,-1], [2,2,2,2], [-1,-1,-1], 0.0, [1,2,3,4]),\n        # Case 3\n        (6, [-1,-1,-1,-1,-1], [2,2,2,2,2,2], [-1,-1,-1,-1,-1], -6.999, [0,1,0,1,0,1]),\n        # Case 4\n        (2, [-1], [2,2], [-1], 0.3, [2,1]),\n        # Case 5\n        (5, [-1,-0.5,-1.5,-1.0], [4,3.5,5,4.2,3.8], [-1,-1.2,-0.7,-0.9], 0.7, [1,0.5,-1,0.25,2]),\n    ]\n\n    all_solutions = []\n    for case_params in test_cases:\n        solution_vector = solve_almost_tridiagonal(*case_params)\n        \n        # Round each component to 6 decimal places.\n        rounded_solution = np.round(solution_vector, 6).tolist()\n        all_solutions.append(rounded_solution)\n\n    # Format the final output string as per the specification.\n    solution_strings = []\n    for sol in all_solutions:\n        # Create a comma-separated string for one solution vector without spaces.\n        sol_str = \",\".join(map(str, sol))\n        solution_strings.append(f\"[{sol_str}]\")\n    \n    # Join all solution strings into the final format.\n    final_output = f\"[{','.join(solution_strings)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "3208725"}, {"introduction": "Many complex physical models, particularly those involving coupled fields or higher-order discretizations, produce systems with a block tridiagonal structure where the elements are themselves small matrices. In this advanced exercise [@problem_id:3208590], you will generalize the Thomas algorithm from scalar entries to matrix blocks. This requires re-interpreting division as solving a small linear system (or matrix inversion) and multiplication as matrix-matrix products, providing deep insight into the structural essence of the algorithm and its applicability to a much broader class of problems.", "problem": "Consider a block tridiagonal Linear System of Equations (LSE) with $N$ blocks, where each block is a $2 \\times 2$ matrix. The global coefficient matrix $\\mathbf{A}$ has the following block structure: diagonal blocks $\\mathbf{D}_i \\in \\mathbb{R}^{2 \\times 2}$ for $i = 1, \\ldots, N$, upper off-diagonal blocks $\\mathbf{U}_i \\in \\mathbb{R}^{2 \\times 2}$ for $i = 1, \\ldots, N-1$, lower off-diagonal blocks $\\mathbf{L}_i \\in \\mathbb{R}^{2 \\times 2}$ for $i = 2, \\ldots, N$, and a right-hand side composed of $2$-vectors $\\mathbf{f}_i \\in \\mathbb{R}^{2}$ for $i = 1, \\ldots, N$. The global unknown vector is the concatenation of block unknowns $\\mathbf{x}_i \\in \\mathbb{R}^{2}$, $i = 1, \\ldots, N$, so that the overall system is of dimension $2N$.\n\nThe fundamental base for this problem is the definition of a linear system $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$ and the principle of Gaussian elimination applied at the block level: elimination operates row by row using block matrix operations that respect associativity and distributivity of matrix multiplication, and the solution is obtained by forward elimination followed by backward substitution, provided that all intermediate block pivots are nonsingular.\n\nYour task is to implement a block Thomas algorithm for this block tridiagonal system. The algorithm must:\n- Operate purely on the blocks $\\mathbf{D}_i$, $\\mathbf{U}_i$, $\\mathbf{L}_i$ and vectors $\\mathbf{f}_i$ without forming the full $2N \\times 2N$ matrix.\n- Use standard matrix and vector operations to perform forward elimination and backward substitution at the block level.\n- Assume that each required block inversion or block linear solve is well-defined (i.e., the relevant $2 \\times 2$ matrices are nonsingular).\n\nFor numerical output consistency, round each component of the solution vector to $8$ decimal places.\n\nImplement the program to solve the following test suite of block tridiagonal systems. For each test case, the data are given by the number of blocks $N$, the lists $\\{\\mathbf{D}_i\\}$, $\\{\\mathbf{U}_i\\}$, $\\{\\mathbf{L}_i\\}$, and $\\{\\mathbf{f}_i\\}$:\n\nTest Case $1$ (typical case):\n- $N = 3$.\n- Diagonal blocks:\n$$\n\\mathbf{D}_1 = \\begin{bmatrix} 4 & 1 \\\\ 1 & 3 \\end{bmatrix}, \\quad\n\\mathbf{D}_2 = \\begin{bmatrix} 3 & 0.5 \\\\ 0.5 & 4 \\end{bmatrix}, \\quad\n\\mathbf{D}_3 = \\begin{bmatrix} 2.5 & 0.4 \\\\ 0.4 & 2.0 \\end{bmatrix}.\n$$\n- Upper blocks:\n$$\n\\mathbf{U}_1 = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}, \\quad\n\\mathbf{U}_2 = \\begin{bmatrix} -1 & 0.2 \\\\ 0.4 & 0.5 \\end{bmatrix}.\n$$\n- Lower blocks:\n$$\n\\mathbf{L}_2 = \\begin{bmatrix} 0.5 & -0.2 \\\\ 0.1 & 0.3 \\end{bmatrix}, \\quad\n\\mathbf{L}_3 = \\begin{bmatrix} -0.3 & 0.4 \\\\ 0.2 & 0.1 \\end{bmatrix}.\n$$\n- Right-hand sides:\n$$\n\\mathbf{f}_1 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\quad\n\\mathbf{f}_2 = \\begin{bmatrix} 0 \\\\ -1 \\end{bmatrix}, \\quad\n\\mathbf{f}_3 = \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix}.\n$$\n\nTest Case $2$ (boundary case with a single block):\n- $N = 1$.\n- Diagonal block:\n$$\n\\mathbf{D}_1 = \\begin{bmatrix} 2 & -1 \\\\ -1 & 2 \\end{bmatrix}.\n$$\n- There are no upper or lower blocks.\n- Right-hand side:\n$$\n\\mathbf{f}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}.\n$$\n\nTest Case $3$ (longer chain with varied signs):\n- $N = 4$.\n- Diagonal blocks:\n$$\n\\mathbf{D}_1 = \\begin{bmatrix} 3 & 0.2 \\\\ 0.2 & 2.5 \\end{bmatrix}, \\quad\n\\mathbf{D}_2 = \\begin{bmatrix} 2.8 & -0.1 \\\\ -0.1 & 3.2 \\end{bmatrix}, \\quad\n\\mathbf{D}_3 = \\begin{bmatrix} 3.0 & 0.4 \\\\ 0.4 & 2.2 \\end{bmatrix}, \\quad\n\\mathbf{D}_4 = \\begin{bmatrix} 2.6 & -0.3 \\\\ -0.3 & 2.9 \\end{bmatrix}.\n$$\n- Upper blocks:\n$$\n\\mathbf{U}_1 = \\begin{bmatrix} 0.1 & -0.2 \\\\ 0.3 & 0.0 \\end{bmatrix}, \\quad\n\\mathbf{U}_2 = \\begin{bmatrix} 0.0 & 0.25 \\\\ -0.3 & 0.15 \\end{bmatrix}, \\quad\n\\mathbf{U}_3 = \\begin{bmatrix} -0.2 & 0.1 \\\\ 0.25 & -0.05 \\end{bmatrix}.\n$$\n- Lower blocks:\n$$\n\\mathbf{L}_2 = \\begin{bmatrix} -0.05 & 0.2 \\\\ 0.1 & -0.1 \\end{bmatrix}, \\quad\n\\mathbf{L}_3 = \\begin{bmatrix} 0.2 & -0.1 \\\\ 0.05 & 0.3 \\end{bmatrix}, \\quad\n\\mathbf{L}_4 = \\begin{bmatrix} -0.1 & 0.0 \\\\ 0.15 & -0.2 \\end{bmatrix}.\n$$\n- Right-hand sides:\n$$\n\\mathbf{f}_1 = \\begin{bmatrix} 0.5 \\\\ -1.0 \\end{bmatrix}, \\quad\n\\mathbf{f}_2 = \\begin{bmatrix} 1.5 \\\\ 2.0 \\end{bmatrix}, \\quad\n\\mathbf{f}_3 = \\begin{bmatrix} -0.5 \\\\ 0.25 \\end{bmatrix}, \\quad\n\\mathbf{f}_4 = \\begin{bmatrix} 0.0 \\\\ -1.0 \\end{bmatrix}.\n$$\n\nTest Case $4$ (near-singular diagonal blocks but still invertible):\n- $N = 3$.\n- Diagonal blocks:\n$$\n\\mathbf{D}_1 = \\begin{bmatrix} 10^{-3} & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix}, \\quad\n\\mathbf{D}_2 = \\begin{bmatrix} 1.0 & 0.9999 \\\\ 0.9999 & 1.0 \\end{bmatrix}, \\quad\n\\mathbf{D}_3 = \\begin{bmatrix} 10^{-2} & 0.0 \\\\ 0.0 & 2.0 \\end{bmatrix}.\n$$\n- Upper blocks:\n$$\n\\mathbf{U}_1 = \\begin{bmatrix} 0.2 & -0.1 \\\\ 0.1 & 0.0 \\end{bmatrix}, \\quad\n\\mathbf{U}_2 = \\begin{bmatrix} -0.1 & 0.0 \\\\ 0.0 & 0.1 \\end{bmatrix}.\n$$\n- Lower blocks:\n$$\n\\mathbf{L}_2 = \\begin{bmatrix} 0.0 & 0.1 \\\\ 0.05 & 0.0 \\end{bmatrix}, \\quad\n\\mathbf{L}_3 = \\begin{bmatrix} 0.0 & -0.05 \\\\ 0.02 & 0.0 \\end{bmatrix}.\n$$\n- Right-hand sides:\n$$\n\\mathbf{f}_1 = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}, \\quad\n\\mathbf{f}_2 = \\begin{bmatrix} -1.0 \\\\ 1.0 \\end{bmatrix}, \\quad\n\\mathbf{f}_3 = \\begin{bmatrix} 0.5 \\\\ -0.5 \\end{bmatrix}.\n$$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the solution for one test case represented as a list of length $2N$ from block $1$ to block $N$ and rounded to $8$ decimal places. For example, the overall output format must look like\n$$\n\\texttt{[[x_{1,1},x_{1,2},\\ldots,x_{N,1},x_{N,2}],\\;[\\ldots],\\;[\\ldots],\\;[\\ldots]]}\n$$\ncovering the four test cases in order.", "solution": "The problem posed is to solve a block tridiagonal linear system of equations of the form $\\mathbf{A}\\mathbf{x} = \\mathbf{f}$. This problem is well-defined, scientifically grounded in numerical linear algebra, and all necessary data for the test cases are provided. The assumption that all intermediate block pivots are nonsingular ensures the existence of a unique solution via the proposed block Gaussian elimination method. The problem is therefore deemed valid.\n\nThe system is defined by $N$ block equations. For $i=1, \\dots, N$, the $i$-th block equation is:\n$$ \\mathbf{L}_i \\mathbf{x}_{i-1} + \\mathbf{D}_i \\mathbf{x}_i + \\mathbf{U}_i \\mathbf{x}_{i+1} = \\mathbf{f}_i $$\nwhere the blocks are $\\mathbf{L}_i, \\mathbf{D}_i, \\mathbf{U}_i \\in \\mathbb{R}^{2 \\times 2}$ and the vectors are $\\mathbf{x}_i, \\mathbf{f}_i \\in \\mathbb{R}^2$. By convention, $\\mathbf{L}_1$ and $\\mathbf{U}_N$ are zero matrices of appropriate size. The algorithm to solve this system is a direct generalization of the Thomas algorithm for scalar tridiagonal systems, known as the block Thomas algorithm or block tridiagonal algorithm. It consists of two main phases: a forward elimination sweep and a backward substitution sweep.\n\nThe core principle is to perform a block-wise Gaussian elimination to transform the system into a block upper bidiagonal form, which can then be easily solved by backward substitution.\n\n**1. Forward Elimination Phase**\n\nThe objective of this phase is to transform each block equation into the form:\n$$ \\mathbf{x}_i + \\mathbf{U}'_i \\mathbf{x}_{i+1} = \\mathbf{f}'_i $$\nThis is achieved by deriving recurrence relations for the modified block coefficients $\\mathbf{U}'_i$ and right-hand side vectors $\\mathbf{f}'_i$.\n\nFor the first block row ($i=1$), the equation is:\n$$ \\mathbf{D}_1 \\mathbf{x}_1 + \\mathbf{U}_1 \\mathbf{x}_2 = \\mathbf{f}_1 $$\nAssuming $\\mathbf{D}_1$ is nonsingular (as per the problem statement's guarantee), we can pre-multiply by its inverse, $\\mathbf{D}_1^{-1}$:\n$$ \\mathbf{x}_1 + \\mathbf{D}_1^{-1} \\mathbf{U}_1 \\mathbf{x}_2 = \\mathbf{D}_1^{-1} \\mathbf{f}_1 $$\nThis gives the initial values for our recurrence:\n$$ \\mathbf{U}'_1 = \\mathbf{D}_1^{-1} \\mathbf{U}_1 $$\n$$ \\mathbf{f}'_1 = \\mathbf{D}_1^{-1} \\mathbf{f}_1 $$\n\nNow, consider the general block row for $i = 2, \\ldots, N-1$:\n$$ \\mathbf{L}_i \\mathbf{x}_{i-1} + \\mathbf{D}_i \\mathbf{x}_i + \\mathbf{U}_i \\mathbf{x}_{i+1} = \\mathbf{f}_i $$\nWe assume the relation for the previous block, $\\mathbf{x}_{i-1} = \\mathbf{f}'_{i-1} - \\mathbf{U}'_{i-1} \\mathbf{x}_i$, has already been established. Substituting this into the equation for block $i$:\n$$ \\mathbf{L}_i (\\mathbf{f}'_{i-1} - \\mathbf{U}'_{i-1} \\mathbf{x}_i) + \\mathbf{D}_i \\mathbf{x}_i + \\mathbf{U}_i \\mathbf{x}_{i+1} = \\mathbf{f}_i $$\nGrouping terms with $\\mathbf{x}_i$:\n$$ (\\mathbf{D}_i - \\mathbf{L}_i \\mathbf{U}'_{i-1}) \\mathbf{x}_i + \\mathbf{U}_i \\mathbf{x}_{i+1} = \\mathbf{f}_i - \\mathbf{L}_i \\mathbf{f}'_{i-1} $$\nLet's define the intermediate block pivot as $\\mathbf{D}'_i = \\mathbf{D}_i - \\mathbf{L}_i \\mathbf{U}'_{i-1}$. The problem statement guarantees that $\\mathbf{D}'_i$ is nonsingular. Pre-multiplying by $(\\mathbf{D}'_i)^{-1}$:\n$$ \\mathbf{x}_i + (\\mathbf{D}'_i)^{-1} \\mathbf{U}_i \\mathbf{x}_{i+1} = (\\mathbf{D}'_i)^{-1} (\\mathbf{f}_i - \\mathbf{L}_i \\mathbf{f}'_{i-1}) $$\nThis gives the recurrence relations for $i=2, \\ldots, N-1$:\n$$ \\mathbf{U}'_i = (\\mathbf{D}_i - \\mathbf{L}_i \\mathbf{U}'_{i-1})^{-1} \\mathbf{U}_i $$\n$$ \\mathbf{f}'_i = (\\mathbf{D}_i - \\mathbf{L}_i \\mathbf{U}'_{i-1})^{-1} (\\mathbf{f}_i - \\mathbf{L}_i \\mathbf{f}'_{i-1}) $$\nIn implementation, it is numerically preferable to solve linear systems rather than computing matrix inverses explicitly. Thus, we compute $\\mathbf{U}'_i$ by solving $(\\mathbf{D}_i - \\mathbf{L}_i \\mathbf{U}'_{i-1}) \\mathbf{X} = \\mathbf{U}_i$, and similarly for $\\mathbf{f}'_i$.\n\nFor the last block row ($i=N$), the equation is:\n$$ \\mathbf{L}_N \\mathbf{x}_{N-1} + \\mathbf{D}_N \\mathbf{x}_N = \\mathbf{f}_N $$\nSubstituting for $\\mathbf{x}_{N-1}$ using the relation from step $N-1$:\n$$ \\mathbf{L}_N (\\mathbf{f}'_{N-1} - \\mathbf{U}'_{N-1} \\mathbf{x}_N) + \\mathbf{D}_N \\mathbf{x}_N = \\mathbf{f}_N $$\nRearranging to solve for $\\mathbf{x}_N$:\n$$ (\\mathbf{D}_N - \\mathbf{L}_N \\mathbf{U}'_{N-1}) \\mathbf{x}_N = \\mathbf{f}_N - \\mathbf{L}_N \\mathbf{f}'_{N-1} $$\nLet the final pivot be $\\mathbf{D}'_N = \\mathbf{D}_N - \\mathbf{L}_N \\mathbf{U}'_{N-1}$. The last unknown block vector $\\mathbf{x}_N$ is found by solving:\n$$ \\mathbf{x}_N = (\\mathbf{D}'_N)^{-1} (\\mathbf{f}_N - \\mathbf{L}_N \\mathbf{f}'_{N-1}) $$\n\n**2. Backward Substitution Phase**\n\nWith the last block vector $\\mathbf{x}_N$ computed, we can find the remaining solution vectors by substituting backwards from $i=N-1$ down to $1$. The recurrence relation is taken directly from the transformed system:\n$$ \\mathbf{x}_i = \\mathbf{f}'_i - \\mathbf{U}'_i \\mathbf{x}_{i+1} \\quad \\text{for } i = N-1, N-2, \\ldots, 1 $$\nThis process computes all $\\mathbf{x}_i$, completing the solution.\n\n**Special Case: N=1**\nIf there is only one block ($N=1$), the system simplifies to $\\mathbf{D}_1 \\mathbf{x}_1 = \\mathbf{f}_1$. The solution is directly obtained by solving this single $2 \\times 2$ system: $\\mathbf{x}_1 = \\mathbf{D}_1^{-1} \\mathbf{f}_1$.\n\n**Algorithmic Summary**\n\n1.  **If $N=1$**: Solve $\\mathbf{D}_1 \\mathbf{x}_1 = \\mathbf{f}_1$ and return $\\mathbf{x}_1$.\n2.  **Forward Elimination ($i=1$ to $N$)**:\n    a. Initialize: Compute $\\mathbf{U}'_1$ by solving $\\mathbf{D}_1 \\mathbf{X} = \\mathbf{U}_1$, and $\\mathbf{f}'_1$ by solving $\\mathbf{D}_1 \\mathbf{y} = \\mathbf{f}_1$.\n    b. Iterate for $i=2, \\ldots, N-1$:\n       i.  Compute pivot $\\mathbf{D}'_i = \\mathbf{D}_i - \\mathbf{L}_i \\mathbf{U}'_{i-1}$.\n       ii. Compute $\\mathbf{U}'_i$ by solving $\\mathbf{D}'_i \\mathbf{X} = \\mathbf{U}_i$.\n       iii.Compute $\\mathbf{f}'_i$ by solving $\\mathbf{D}'_i \\mathbf{y} = (\\mathbf{f}_i - \\mathbf{L}_i \\mathbf{f}'_{i-1})$.\n    c. Final step: Compute the last block vector, $\\mathbf{x}_N$, by solving $(\\mathbf{D}_N - \\mathbf{L}_N \\mathbf{U}'_{N-1}) \\mathbf{x}_N = (\\mathbf{f}_N - \\mathbf{L}_N \\mathbf{f}'_{N-1})$.\n3.  **Backward Substitution ($i=N-1$ to $1$)**:\n    a. Iterate downwards: $\\mathbf{x}_i = \\mathbf{f}'_i - \\mathbf{U}'_i \\mathbf{x}_{i+1}$.\n4.  **Result**: The solution is the ordered collection of vectors $\\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_N\\}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define, solve, and format the results for the test cases.\n    \"\"\"\n\n    def solve_block_tridiagonal(N, D, U, L, f):\n        \"\"\"\n        Solves a block tridiagonal system of equations using the block Thomas algorithm.\n\n        Args:\n            N (int): The number of blocks.\n            D (list of np.ndarray): List of N diagonal 2x2 blocks.\n            U (list of np.ndarray): List of N-1 upper off-diagonal 2x2 blocks.\n            L (list of np.ndarray): List of N-1 lower off-diagonal 2x2 blocks.\n            f (list of np.ndarray): List of N right-hand side 2x1 vectors.\n\n        Returns:\n            list: The flattened solution vector x, with components rounded to 8 decimal places.\n        \"\"\"\n        # Handle the special case of a single block.\n        if N == 1:\n            x0 = np.linalg.solve(D[0], f[0])\n            solution_vecs = [x0]\n            flat_solution = np.concatenate(solution_vecs).ravel()\n            return [round(val, 8) for val in flat_solution]\n\n        # Storage for modified coefficients (forward sweep).\n        U_prime = [None] * (N - 1)\n        f_prime = [None] * (N - 1) # We only need up to N-1 for back-substitution.\n\n        # ----- Forward Elimination Phase -----\n\n        # Step 1: i = 1 (Python index 0)\n        # Solve D_1 * X = U_1 for U'_1 and D_1 * y = f_1 for f'_1\n        U_prime[0] = np.linalg.solve(D[0], U[0])\n        f_prime[0] = np.linalg.solve(D[0], f[0])\n        \n        # Step 2: i = 2 to N-1 (Python index 1 to N-2)\n        for i in range(1, N - 1):\n            D_i, U_i = D[i], U[i]\n            L_i = L[i-1] # L_i in math corresponds to L[i-1] in problem data\n            \n            # Compute intermediate pivot D'_i = D_i - L_i * U'_{i-1}\n            D_prime_i = D_i - L_i @ U_prime[i-1]\n            \n            # Solve D'_i * X = U_i for U'_i\n            U_prime[i] = np.linalg.solve(D_prime_i, U_i)\n            \n            # Solve D'_i * y = (f_i - L_i * f'_{i-1}) for f'_i\n            f_rhs = f[i] - L_i @ f_prime[i-1]\n            f_prime[i] = np.linalg.solve(D_prime_i, f_rhs)\n\n        # Step 3: Final step of forward sweep for i=N\n        # Solve D'_N * x_N = (f_N - L_N * f'_{N-1})\n        D_N = D[N-1]\n        L_N = L[N-2]\n        \n        D_prime_N = D_N - L_N @ U_prime[N-2]\n        f_rhs_N = f[N-1] - L_N @ f_prime[N-2]\n        \n        solution_vecs = [None] * N\n        solution_vecs[N-1] = np.linalg.solve(D_prime_N, f_rhs_N)\n\n        # ----- Backward Substitution Phase -----\n        # For i = N-1 down to 1 (Python indices N-2 down to 0)\n        for i in range(N - 2, -1, -1):\n            # x_i = f'_i - U'_i * x_{i+1}\n            solution_vecs[i] = f_prime[i] - U_prime[i] @ solution_vecs[i+1]\n\n        # Flatten the list of solution vectors and round each component\n        flat_solution = np.concatenate(solution_vecs).ravel()\n        rounded_solution = [round(val, 8) for val in flat_solution]\n        \n        return rounded_solution\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1\n        {\n            \"N\": 3,\n            \"D\": [[[4, 1], [1, 3]], [[3, 0.5], [0.5, 4]], [[2.5, 0.4], [0.4, 2.0]]],\n            \"U\": [[[1, 0], [0, 1]], [[-1, 0.2], [0.4, 0.5]]],\n            \"L\": [[[0.5, -0.2], [0.1, 0.3]], [[-0.3, 0.4], [0.2, 0.1]]],\n            \"f\": [[1, 2], [0, -1], [3, 1]]\n        },\n        # Test Case 2\n        {\n            \"N\": 1,\n            \"D\": [[[2, -1], [-1, 2]]],\n            \"U\": [],\n            \"L\": [],\n            \"f\": [[1, 0]]\n        },\n        # Test Case 3\n        {\n            \"N\": 4,\n            \"D\": [[[3, 0.2], [0.2, 2.5]], [[2.8, -0.1], [-0.1, 3.2]], [[3.0, 0.4], [0.4, 2.2]], [[2.6, -0.3], [-0.3, 2.9]]],\n            \"U\": [[[0.1, -0.2], [0.3, 0.0]], [[0.0, 0.25], [-0.3, 0.15]], [[-0.2, 0.1], [0.25, -0.05]]],\n            \"L\": [[[-0.05, 0.2], [0.1, -0.1]], [[0.2, -0.1], [0.05, 0.3]], [[-0.1, 0.0], [0.15, -0.2]]],\n            \"f\": [[0.5, -1.0], [1.5, 2.0], [-0.5, 0.25], [0.0, -1.0]]\n        },\n        # Test Case 4\n        {\n            \"N\": 3,\n            \"D\": [[[1e-3, 0.0], [0.0, 1.0]], [[1.0, 0.9999], [0.9999, 1.0]], [[1e-2, 0.0], [0.0, 2.0]]],\n            \"U\": [[[0.2, -0.1], [0.1, 0.0]], [[-0.1, 0.0], [0.0, 0.1]]],\n            \"L\": [[[0.0, 0.1], [0.05, 0.0]], [[0.0, -0.05], [0.02, 0.0]]],\n            \"f\": [[1.0, 0.0], [-1.0, 1.0], [0.5, -0.5]]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        N = case[\"N\"]\n        D_mats = [np.array(m) for m in case[\"D\"]]\n        U_mats = [np.array(m) for m in case[\"U\"]]\n        L_mats = [np.array(m) for m in case[\"L\"]]\n        f_vecs = [np.array(v) for v in case[\"f\"]]\n        \n        result = solve_block_tridiagonal(N, D_mats, U_mats, L_mats, f_vecs)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    formatted_results = ','.join([str(r).replace(' ', '') for r in results])\n    print(f\"[{formatted_results}]\")\n\nsolve()\n\n```", "id": "3208590"}]}