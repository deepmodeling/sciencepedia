{"hands_on_practices": [{"introduction": "A cornerstone of polynomial interpolation theory is that the process is \"exact\" when applied to polynomials themselves. Specifically, for any polynomial $p(x)$ of degree $d$, its divided differences of order $m \\gt d$ are theoretically zero. This exercise allows you to empirically verify this fundamental property by constructing divided difference tables for randomly generated polynomials [@problem_id:3254801]. By observing that higher-order differences are numerically close to zero, you will build confidence in the method and also appreciate the practical effects of floating-point arithmetic on this theoretical exactness.", "problem": "You are asked to write a complete, runnable program that empirically verifies, using the Newton divided difference construction, that interpolation preserves polynomials. The fundamental base for this task is the definition of the divided differences generated by their recursive construction from sample data and the linearity of polynomial evaluation. Concretely, for a function $f$ sampled at nodes $x_0, x_1, \\dots, x_n$ with values $f(x_0), f(x_1), \\dots, f(x_n)$, the divided differences are defined recursively so that the order-$m$ divided differences are built from order-$(m-1)$ differences and the sample nodes. A polynomial $p$ of degree $d$ is a linear combination of monomials of degrees up to $d$, and divided differences are linear in the data. Therefore, if the data come from a polynomial $p$ of degree $d$, the order-$m$ divided differences vanish for $m>d$ in exact arithmetic. Your program must implement this logic in a scientifically sound way and test robustness under rounding of function values.\n\nProgram requirements:\n- Generate a polynomial $p(x)$ of degree $d$ using pseudo-random coefficients drawn from the uniform distribution on $[-1,1]$ with a specified seed so the experiment is reproducible. The coefficient vector must be $(a_0, a_1, \\dots, a_d)$ with $a_k \\in [-1,1]$, and the polynomial is $p(x)=a_0 + a_1 x + \\dots + a_d x^d$.\n- Evaluate $p$ at the provided nodes to obtain $p(x_i)$ for each node $x_i$.\n- Construct the full divided difference table from the nodes $x_i$ and values $p(x_i)$. Then, for orders $m>d$, compute the maximum absolute magnitude across all entries of the order-$m$ column. Report, for each test case, the single float equal to this maximum absolute magnitude over all orders strictly greater than $d$. If there are no orders strictly greater than $d$ available (i.e., the number of nodes is at most $d+1$), report $0.0$.\n- To test robustness under rounding, in designated cases round the function values $p(x_i)$ to a specified number of decimal places before constructing the divided difference table. You must still report the maximum absolute magnitude across all divided differences of orders $m>d$.\n\nTest suite:\n- Case $1$ (happy path, overdetermined nodes):\n  - Degree $d=3$.\n  - Seed $12345$.\n  - Nodes $[-1.0,-0.5,0.0,0.75,1.5,2.0]$.\n  - No rounding of $p(x_i)$.\n  - Expected qualitative behavior: order-$m$ divided differences for $m>3$ are numerically near zero in floating-point arithmetic.\n- Case $2$ (boundary case: constant polynomial):\n  - Degree $d=0$.\n  - Seed $54321$.\n  - Nodes $[-2.0,-1.0,0.0,1.0,2.0]$.\n  - No rounding of $p(x_i)$.\n  - Expected qualitative behavior: all order-$m$ divided differences for $m\\geq 1$ are numerically near zero.\n- Case $3$ (higher degree, minimal excess nodes):\n  - Degree $d=5$.\n  - Seed $2024$.\n  - Nodes $[-1.5,-1.0,-0.25,0.1,0.8,1.3,2.2]$.\n  - No rounding of $p(x_i)$.\n  - Expected qualitative behavior: order-$6$ divided differences are numerically near zero.\n- Case $4$ (robustness under rounding of function values):\n  - Degree $d=3$.\n  - Seed $4242$.\n  - Nodes $[-1.0,-0.5,0.0,0.75,1.5,2.0]$.\n  - Round $p(x_i)$ to $4$ decimal places before constructing the table.\n  - Expected qualitative behavior: the maximum absolute magnitude of order-$m$ divided differences for $m>3$ reflects rounding-induced nonzero values, typically on the scale of the rounding unit.\n- Case $5$ (degree equals the next-to-top order with multiple nodes):\n  - Degree $d=4$.\n  - Seed $909$.\n  - Nodes $[-2.0,-1.0,-0.2,0.4,1.1,1.9]$.\n  - No rounding of $p(x_i)$.\n  - Expected qualitative behavior: the order-$5$ divided differences are numerically near zero.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each entry must be the single float returned for the corresponding test case in the above order, for example, $[r_1,r_2,r_3,r_4,r_5]$.\n- No physical units or angle units apply.", "solution": "The problem requires an empirical verification of a fundamental theorem in numerical analysis concerning polynomial interpolation. Specifically, it states that for a set of data points $(x_i, p(x_i))$ sampled from a polynomial $p(x)$ of degree $d$, the divided differences of order $m > d$ are identically zero. This property is a direct consequence of the recursive definition of divided differences and the structure of polynomials. The task is to implement a program to compute these higher-order divided differences and observe their behavior in finite-precision floating-point arithmetic, where they are expected to be numerically close to zero, and to analyze the effect of rounding errors on this property.\n\nThe mathematical foundation rests on the definition of divided differences. For a function $f$ sampled at a set of distinct nodes $\\{x_0, x_1, \\dots, x_n\\}$, the divided differences are defined as:\nThe zeroth-order divided difference is simply the function value:\n$$f[x_i] = f(x_i)$$\nHigher-order divided differences are defined recursively. The $k$-th order divided difference is:\n$$f[x_i, x_{i+1}, \\dots, x_{i+k}] = \\frac{f[x_{i+1}, \\dots, x_{i+k}] - f[x_i, \\dots, x_{i+k-1}]}{x_{i+k} - x_i}$$\nA key theorem states that if the function $f$ is a polynomial $p(x)$ of degree at most $d$, then for any set of $k+1$ distinct points, the $k$-th order divided difference $p[x_0, x_1, \\dots, x_k]$ is zero for all $k > d$. For $k=d$, the divided difference $p[x_0, \\dots, x_d]$ is a constant equal to the leading coefficient of the polynomial, regardless of the choice of nodes.\n\nOur program will verify this theorem by performing the following steps for each test case:\n$1$. **Polynomial Generation**: A polynomial $p(x) = \\sum_{k=0}^{d} a_k x^k$ of a specified degree $d$ is constructed. The coefficients $(a_0, a_1, \\dots, a_d)$ are pseudo-randomly generated from a uniform distribution on the interval $[-1, 1]$ using a specific seed for reproducibility.\n\n$2$. **Data Sampling**: The polynomial $p(x)$ is evaluated at a given set of $n_{\\text{pts}}$ distinct nodes $\\{x_0, x_1, \\dots, x_{n_{\\text{pts}}-1}\\}$ to generate the corresponding function values $\\{p(x_0), p(x_1), \\dots, p(x_{n_{\\text{pts}}-1})\\}$. For the test case involving rounding, these values are rounded to a specified number of decimal places. This simulates measurement or quantization error.\n\n$3$. **Divided Difference Table Computation**: A table is constructed to hold all necessary divided differences. Let this table be a two-dimensional array $T$, where $T[i, j]$ stores the $j$-th order divided difference $p[x_i, \\dots, x_{i+j}]$.\n- The first column ($j=0$) is initialized with the sampled function values: $T[i, 0] = p(x_i)$ for $i=0, \\dots, n_{\\text{pts}}-1$.\n- Subsequent columns ($j=1, \\dots, n_{\\text{pts}}-1$) are filled using the recursive formula:\n$$T[i, j] = \\frac{T[i+1, j-1] - T[i, j-1]}{x_{i+j} - x_i}$$\nThis calculation is performed for $i$ from $0$ to $n_{\\text{pts}}-1-j$.\n\n$4$. **Result Extraction**: The core of the verification is to examine the divided differences of orders $m > d$. This is only possible if the number of nodes $n_{\\text{pts}}$ is greater than $d+1$, which allows for the computation of at least one divided difference of order $d+1$. If $n_{\\text{pts}} \\le d+1$, no such differences can be computed, and the result is defined as $0.0$. Otherwise, we compute the maximum absolute value across all entries in the divided difference table corresponding to orders $m > d$:\n$$\\max \\{ |T[i, m]| : m \\in \\{d+1, \\dots, n_{\\text{pts}}-1\\}, i \\in \\{0, \\dots, n_{\\text{pts}}-1-m\\} \\}$$\nThis value is reported as the result for the test case. In exact arithmetic, this result would be $0$. Due to floating-point representation and arithmetic errors, we expect a very small non-zero value. When function values are intentionally rounded, the data no longer perfectly represent a polynomial of degree $d$, and we expect the resulting higher-order differences to be non-zero, with a magnitude related to the rounding precision.\n\nThe implementation will utilize the `numpy` library for efficient array manipulations, random number generation, and polynomial evaluation. The overall procedure is a direct and robust formalization of the problem statement, adhering to the principles of numerical experimentation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_max_dd_magnitude(d, seed, nodes, rounding_places=None):\n    \"\"\"\n    Computes the maximum absolute magnitude of divided differences of order m > d.\n\n    Args:\n        d (int): The degree of the polynomial.\n        seed (int): The seed for the random number generator.\n        nodes (list[float]): A list of x-coordinates for sampling.\n        rounding_places (int, optional): Number of decimal places to round the\n                                         function values. If None, no rounding.\n\n    Returns:\n        float: The maximum absolute magnitude of divided differences for orders > d.\n    \"\"\"\n    # 1. Generate the polynomial\n    rng = np.random.default_rng(seed)\n    # Coefficients for a_0 + a_1*x + ... + a_d*x^d\n    coeffs = rng.uniform(-1.0, 1.0, d + 1)\n    \n    # 2. Evaluate the polynomial at the nodes\n    x_nodes = np.array(nodes, dtype=float)\n    # np.polyval expects coefficients for highest power first, so reverse coeffs\n    y_values = np.polyval(coeffs[::-1], x_nodes)\n    \n    # 3. Round function values if specified\n    if rounding_places is not None:\n        y_values = np.round(y_values, rounding_places)\n        \n    n_pts = len(x_nodes)\n    \n    # Handle the case where no orders > d can be computed\n    if n_pts <= d + 1:\n        return 0.0\n\n    # 4. Construct the divided difference table\n    # dd_table[i, j] will store the j-th order difference starting at x_i\n    dd_table = np.zeros((n_pts, n_pts))\n    dd_table[:, 0] = y_values\n    \n    for j in range(1, n_pts):  # j is the order of the difference\n        for i in range(n_pts - j):\n            denominator = x_nodes[i + j] - x_nodes[i]\n            # Avoid division by zero, although problem implies distinct nodes\n            if denominator == 0:\n                # This case indicates an ill-posed problem (non-distinct nodes)\n                # and should not occur with the given test cases.\n                # In a general-purpose library, this would raise an error.\n                # For this problem, we can assign a large value or NaN.\n                # We assume nodes are distinct per the problem's context.\n                pass\n            dd_table[i, j] = (dd_table[i + 1, j - 1] - dd_table[i, j - 1]) / denominator\n\n    # 5. Find the maximum absolute magnitude for orders m > d\n    max_abs_mag = 0.0\n    \n    magnitudes = []\n    # Iterate through columns (orders) from d+1 to n_pts-1\n    for m in range(d + 1, n_pts):\n        # The m-th order column has n_pts - m valid entries\n        valid_entries = dd_table[0:n_pts - m, m]\n        if valid_entries.size > 0:\n            magnitudes.append(np.max(np.abs(valid_entries)))\n    \n    if magnitudes:\n        max_abs_mag = np.max(magnitudes)\n\n    return max_abs_mag\n\ndef solve():\n    \"\"\"\n    Solves the problem by running all specified test cases and printing the results.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path, overdetermined nodes)\n        {'d': 3, 'seed': 12345, 'nodes': [-1.0, -0.5, 0.0, 0.75, 1.5, 2.0], 'rounding': None},\n        # Case 2 (boundary case: constant polynomial)\n        {'d': 0, 'seed': 54321, 'nodes': [-2.0, -1.0, 0.0, 1.0, 2.0], 'rounding': None},\n        # Case 3 (higher degree, minimal excess nodes)\n        {'d': 5, 'seed': 2024, 'nodes': [-1.5, -1.0, -0.25, 0.1, 0.8, 1.3, 2.2], 'rounding': None},\n        # Case 4 (robustness under rounding)\n        {'d': 3, 'seed': 4242, 'nodes': [-1.0, -0.5, 0.0, 0.75, 1.5, 2.0], 'rounding': 4},\n        # Case 5 (degree equals next-to-top order)\n        {'d': 4, 'seed': 909, 'nodes': [-2.0, -1.0, -0.2, 0.4, 1.1, 1.9], 'rounding': None},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_max_dd_magnitude(\n            d=case['d'],\n            seed=case['seed'],\n            nodes=case['nodes'],\n            rounding_places=case['rounding']\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.17e}' for r in results)}]\")\n\nsolve()\n```", "id": "3254801"}, {"introduction": "Beyond simply approximating a function, polynomial interpolation serves as a powerful tool in solving more complex numerical problems. A common application is approximating the roots of a transcendental or otherwise complicated function by finding the exact roots of its simpler, polynomial interpolant. This practice guides you through the complete workflow: constructing a Newton polynomial for a given function, converting it to the standard monomial form, and implementing a root-finding procedure to approximate the function's zeros [@problem_id:3254701].", "problem": "You are given distinct interpolation nodes and a real-valued function defined on those nodes. The objective is to approximate the roots of the function by constructing the Newton interpolating polynomial of degree at most $n$ that matches the function at the provided nodes, and then finding the real roots of this polynomial within a specified interval. The approach must begin from foundational definitions: the unique existence of a polynomial of degree at most $n$ that interpolates $n+1$ distinct data points, and the recursive definition of divided differences. You must not assume any pre-derived closed-form \"shortcut\" expressions beyond these definitions.\n\nYour program must implement the following, for each test case:\n- Using the definition of divided differences, construct the Newton interpolating polynomial $P_n(x)$ of degree at most $n$ that satisfies $P_n(x_i) = f(x_i)$ for the given nodes $\\{x_0, x_1, \\dots, x_n\\}$.\n- Convert the Newton form to the standard monomial form with coefficients $\\{a_0, a_1, \\dots, a_n\\}$ so that $P_n(x) = a_0 + a_1 x + \\cdots + a_n x^n$.\n- Compute all roots of $P_n(x)$ and extract those that are real (imaginary part magnitude less than $10^{-10}$), lie within the specified interval, and represent distinct values under a de-duplication tolerance of $5 \\cdot 10^{-7}$. Sort the roots in ascending order.\n- Report the resulting list of real roots for each case, with every root rounded to $8$ decimal places.\n- Angles for any trigonometric functions must be interpreted in radians.\n\nTest suite specification:\n- Case $1$ (oscillatory minus linear, multiple roots expected):\n  - Function: $f(x) = \\sin(x) - 0.5\\,x$.\n  - Nodes: $\\{0,\\;0.5,\\;1.0,\\;1.5,\\;2.0,\\;3.0,\\;4.0\\}$.\n  - Interval: $[0,\\;4]$.\n- Case $2$ (polynomial with a multiple root, exact reproduction by interpolation expected):\n  - Function: $f(x) = x\\,(x-1)^2$.\n  - Nodes: $\\{-0.5,\\;0.0,\\;0.75,\\;1.5\\}$.\n  - Interval: $[-0.5,\\;1.5]$.\n- Case $3$ (monotone decaying, single root expected):\n  - Function: $f(x) = e^{-x} - 0.1$.\n  - Nodes: $\\{0.0,\\;0.5,\\;1.0,\\;1.5,\\;2.0,\\;2.5,\\;3.0\\}$.\n  - Interval: $[0,\\;3]$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case’s root list itself rendered as a bracketed, comma-separated list with no spaces. For example, if the three cases yield lists $L_1$, $L_2$, and $L_3$, the output must be exactly in the form $[L_1,L_2,L_3]$.\n- Each root must be rounded to $8$ decimal places.\n\nThe answers for all test cases are lists of floating-point numbers. No physical units are involved. All trigonometric evaluations must use angles measured in radians.", "solution": "The user-provided problem is valid. It is a well-posed problem in numerical analysis, grounded in established principles of polynomial interpolation and root-finding. All necessary data and constraints are provided, and the problem is free of scientific impossibilities, contradictions, or ambiguities. We may therefore proceed with the solution.\n\nThe problem requires us to find the real roots of a function $f(x)$ within a specified interval by first approximating the function with its Newton interpolating polynomial, $P_n(x)$, and then finding the roots of $P_n(x)$.\n\n**1. Newton Interpolating Polynomial**\n\nGiven a set of $n+1$ distinct data points $(x_0, y_0), (x_1, y_1), \\dots, (x_n, y_n)$, where $y_i = f(x_i)$, there exists a unique polynomial $P_n(x)$ of degree at most $n$ such that $P_n(x_i) = y_i$ for all $i = 0, \\dots, n$. The Newton form of this polynomial is given by:\n\n$$P_n(x) = c_0 + c_1(x-x_0) + c_2(x-x_0)(x-x_1) + \\dots + c_n(x-x_0)(x-x_1)\\dots(x-x_{n-1})$$\n\nThis can be expressed more compactly using product notation:\n\n$$P_n(x) = \\sum_{k=0}^{n} c_k \\prod_{j=0}^{k-1} (x-x_j)$$\n\nThe coefficients $c_k$ are the divided differences, defined recursively. The $k$-th order divided difference of $f$ with respect to the nodes $x_i, \\dots, x_{i+k}$ is denoted by $f[x_i, \\dots, x_{i+k}]$.\n\nThe recursive definition is as follows:\n- **Zeroth-order:**\n  $$f[x_i] = f(x_i) = y_i$$\n- **$k$-th order (for $k \\ge 1$):**\n  $$f[x_i, x_{i+1}, \\dots, x_{i+k}] = \\frac{f[x_{i+1}, \\dots, x_{i+k}] - f[x_i, \\dots, x_{i+k-1}]}{x_{i+k} - x_i}$$\n\nThe coefficients $c_k$ of the Newton polynomial are the divided differences along the top diagonal of the divided difference table:\n$$c_k = f[x_0, x_1, \\dots, x_k]$$\n\nFor a set of nodes $\\{x_0, \\dots, x_n\\}$, the divided difference table is constructed. The first column contains the function values $y_i = f(x_i)$. Each subsequent entry is calculated from two entries in the previous column. For example, for $n=3$:\n\n| $x_i$ | $f[x_i]$      | $f[x_i, x_{i+1}]$ | $f[x_i, x_{i+1}, x_{i+2}]$ | $f[x_0, x_1, x_2, x_3]$ |\n| :---- | :------------ | :---------------- | :------------------------- | :-------------------------- |\n| $x_0$ | $y_0 = c_0$   |                   |                            |                             |\n|       |               | $f[x_0, x_1]=c_1$ |                            |                             |\n| $x_1$ | $y_1$         |                   | $f[x_0, x_1, x_2]=c_2$     |                             |\n|       |               | $f[x_1, x_2]$     |                            | $f[x_0, x_1, x_2, x_3]=c_3$ |\n| $x_2$ | $y_2$         |                   | $f[x_1, x_2, x_3]$         |                             |\n|       |               | $f[x_2, x_3]$     |                            |                             |\n| $x_3$ | $y_3$         |                   |                            |                             |\n\nWe will implement a procedure to compute this table and extract the coefficients $c_0, c_1, \\dots, c_n$.\n\n**2. Conversion to Monomial Form**\n\nThe Newton form is suitable for evaluation but not for standard root-finding algorithms, which typically operate on polynomials in the monomial basis, $P_n(x) = \\sum_{i=0}^{n} a_i x^i$. We must convert $P_n(x)$ into this form.\n\nA stable and direct method for this conversion is to use a nested multiplication scheme. The Newton form can be rewritten as:\n$$P_n(x) = c_0 + (x-x_0)\\bigg(c_1 + (x-x_1)\\Big(c_2 + \\dots + (x-x_{n-1})c_n\\Big)\\dots\\bigg)$$\n\nThis structure suggests an iterative algorithm. Let $Q_k(x)$ be the polynomial inside the $k$-th level of nesting.\n- Start with the innermost polynomial: $Q_n(x) = c_n$ (a polynomial of degree $0$).\n- Iteratively compute the outer polynomials for $k = n-1, n-2, \\dots, 0$:\n  $Q_k(x) = c_k + (x-x_k)Q_{k+1}(x)$\n- The final polynomial is $P_n(x) = Q_0(x)$.\n\nWe can perform this iteration by manipulating arrays of polynomial coefficients in the monomial basis. Let `coeffs(P)` denote the array of coefficients of a polynomial $P$.\n1. Initialize with `coeffs(Q_n) = [c_n]`.\n2. For $k$ from $n-1$ down to $0$:\n   a. Let `p_coeffs = coeffs(Q_{k+1})`.\n   b. Compute `coeffs((x-x_k)Q_{k+1}(x))`. This involves multiplying `p_coeffs` by $x$ (shifting the coefficient array) and by $-x_k$ (scalar multiplication) and adding the results.\n   c. Add $c_k$ to the constant term (the first element) of the resulting coefficient array. This gives `coeffs(Q_k)`.\n3. The final array `coeffs(Q_0)` will contain the monomial coefficients $\\{a_0, a_1, \\dots, a_n\\}$.\n\n**3. Root-Finding and Filtering**\n\nOnce we have the monomial coefficients $\\{a_0, a_1, \\dots, a_n\\}$, we can find the roots of $P_n(x) = 0$. A standard numerical method, implemented in libraries like `NumPy`, is to find the eigenvalues of the companion matrix of the polynomial. For a monic polynomial $p(x) = x^n + p_{n-1}x^{n-1} + \\dots + p_0$, the companion matrix is:\n$$C(p) = \\begin{pmatrix}\n0 & 0 & \\dots & 0 & -p_0 \\\\\n1 & 0 & \\dots & 0 & -p_1 \\\\\n0 & 1 & \\dots & 0 & -p_2 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & \\dots & 1 & -p_{n-1}\n\\end{pmatrix}$$\nThe eigenvalues of $C(p)$ are precisely the roots of $p(x)$. `NumPy`'s `roots` function uses this principle.\n\nThe roots found will be complex numbers in general. We must filter them according to the problem's criteria:\n1.  **Real Roots**: A root $z$ is considered real if the absolute value of its imaginary part is negligible, i.e., $|\\text{Im}(z)| < 10^{-10}$.\n2.  **Interval Check**: The real roots must lie within the specified interval $[x_{\\text{min}}, x_{\\text{max}}]$.\n3.  **De-duplication**: Numerically found roots, especially multiple roots, may have small variations. To find the set of distinct roots, we sort the filtered roots and then iterate through them, keeping only those that differ from the previous one by more than a tolerance of $5 \\cdot 10^{-7}$.\n4.  **Sorting and Rounding**: The final list of unique real roots is sorted in ascending order and each root is rounded to $8$ decimal places for the final output.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n\n    def divided_differences(x_nodes, y_values):\n        \"\"\"\n        Computes the divided differences table and returns the Newton coefficients.\n        \n        Args:\n            x_nodes (np.array): The distinct x-coordinates of the data points.\n            y_values (np.array): The y-coordinates of the data points (f(x)).\n\n        Returns:\n            np.array: The coefficients c_k of the Newton polynomial.\n        \"\"\"\n        n = len(x_nodes)\n        if n == 0:\n            return np.array([])\n        \n        # The table of divided differences.\n        # table[i, j] will store f[x_i, ..., x_{i+j}]\n        table = np.zeros((n, n))\n        table[:, 0] = y_values\n        \n        for j in range(1, n):\n            for i in range(n - j):\n                numerator = table[i + 1, j - 1] - table[i, j - 1]\n                denominator = x_nodes[i + j] - x_nodes[i]\n                table[i, j] = numerator / denominator\n                \n        # The coefficients are the top diagonal of the table: f[x_0], f[x_0,x_1], ...\n        return table[0, :]\n\n    def newton_to_monomial(x_nodes, newton_coeffs):\n        \"\"\"\n        Converts a polynomial from Newton form to monomial form.\n\n        Args:\n            x_nodes (np.array): The interpolation nodes x_0, x_1, ...\n            newton_coeffs (np.array): The Newton coefficients c_0, c_1, ...\n\n        Returns:\n            np.array: The coefficients a_0, a_1, ... of the monomial form.\n        \"\"\"\n        n = len(newton_coeffs) - 1\n        if n < 0:\n            return np.array([])\n        \n        # Start with the highest-degree term: Q_n(x) = c_n\n        # poly_coeffs stores coeffs in increasing order of power [a_0, a_1, ...]\n        poly_coeffs = np.array([newton_coeffs[n]])\n        \n        # Iterate downwards: Q_k(x) = c_k + (x-x_k)Q_{k+1}(x)\n        for k in range(n - 1, -1, -1):\n            # Current poly_coeffs are for Q_{k+1}(x)\n            # Find coeffs for (x - x_k) * Q_{k+1}(x)\n            # Multiplying by x shifts coeffs up by one power. \n            # Pad with 0 at the start for the new constant term.\n            term_x_mult = np.pad(poly_coeffs, (1, 0), 'constant')\n\n            # Multiplying by -x_k is scalar multiplication. \n            # Pad with 0 at the end to match length.\n            term_xk_mult = -x_nodes[k] * np.pad(poly_coeffs, (0, 1), 'constant')\n\n            product_coeffs = term_x_mult + term_xk_mult\n\n            # Add the constant Newton coefficient c_k\n            product_coeffs[0] += newton_coeffs[k]\n            \n            poly_coeffs = product_coeffs\n\n        return poly_coeffs\n\n    def find_and_filter_roots(poly_coeffs, interval, imag_tol=1e-10, dedupe_tol=5e-7):\n        \"\"\"\n        Finds roots of a polynomial and filters them based on criteria.\n        \n        Args:\n            poly_coeffs (np.array): Monomial coefficients [a_0, a_1, ...].\n            interval (tuple): The interval [min, max] for filtering roots.\n            imag_tol (float): Tolerance for considering a root as real.\n            dedupe_tol (float): Tolerance for de-duplicating roots.\n\n        Returns:\n            list: A sorted list of unique, real roots within the interval.\n        \"\"\"\n        if len(poly_coeffs) < 2:\n            return [] # Constant polynomial has no roots unless it's zero\n            \n        # numpy.roots expects coefficients from highest power to lowest\n        roots = np.roots(np.flip(poly_coeffs))\n        \n        # Filter for real roots\n        real_roots = roots[np.abs(np.imag(roots)) < imag_tol].real\n        \n        # Filter for roots within the specified interval\n        x_min, x_max = interval\n        interval_roots = real_roots[(real_roots >= x_min) & (real_roots <= x_max)]\n        interval_roots.sort()\n        \n        # De-duplicate roots\n        if len(interval_roots) == 0:\n            return []\n            \n        unique_roots = [interval_roots[0]]\n        for i in range(1, len(interval_roots)):\n            if interval_roots[i] - unique_roots[-1] > dedupe_tol:\n                unique_roots.append(interval_roots[i])\n                \n        return unique_roots\n    \n    # Test suite specification\n    test_cases = [\n        {\n            \"func\": lambda x: np.sin(x) - 0.5 * x,\n            \"nodes\": np.array([0., 0.5, 1.0, 1.5, 2.0, 3.0, 4.0]),\n            \"interval\": [0., 4.]\n        },\n        {\n            \"func\": lambda x: x * (x - 1)**2,\n            \"nodes\": np.array([-0.5, 0.0, 0.75, 1.5]),\n            \"interval\": [-0.5, 1.5]\n        },\n        {\n            \"func\": lambda x: np.exp(-x) - 0.1,\n            \"nodes\": np.array([0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]),\n            \"interval\": [0., 3.]\n        }\n    ]\n    \n    final_results = []\n    \n    for case in test_cases:\n        func = case[\"func\"]\n        x_nodes = case[\"nodes\"]\n        interval = case[\"interval\"]\n        \n        y_values = func(x_nodes)\n        \n        # Step 1: Compute Newton coefficients\n        c_k = divided_differences(x_nodes, y_values)\n        \n        # Step 2: Convert to monomial form\n        a_i = newton_to_monomial(x_nodes, c_k)\n        \n        # Step 3: Find, filter, and process roots\n        roots_list = find_and_filter_roots(a_i, interval)\n        \n        # Step 4: Round roots to 8 decimal places\n        rounded_roots = [f\"{r:.8f}\" for r in roots_list]\n        \n        # Format for final output string\n        formatted_list = f\"[{','.join(rounded_roots)}]\"\n        final_results.append(formatted_list)\n        \n    print(f\"[{','.join(final_results)}]\")\n\nsolve()\n```", "id": "3254701"}, {"introduction": "High-degree polynomial interpolation is famously effective for smooth, well-behaved functions but can perform poorly for functions with sharp corners or discontinuities. This exercise explores this critical limitation by examining the canonical non-smooth function, $f(x)=|x|$, whose non-differentiability at $x=0$ poses a challenge for global polynomial approximation. By analyzing the divided difference coefficients when nodes straddle the origin, you will discover how the singularity manifests as explosive growth in the coefficients, leading to a poor, oscillating approximation, and contrast this with a more robust, local interpolation strategy [@problem_id:3254712].", "problem": "Consider the function $f(x)=|x|$ and the Newton form of polynomial interpolation, which uses divided differences. Starting from the fundamental definition of the first-order divided difference between two distinct nodes $x_0$ and $x_1$,\n$$\n[x_0,x_1] = \\frac{f(x_1)-f(x_0)}{x_1-x_0},\n$$\nand the recursive definition of higher-order divided differences,\n$$\n[x_0,x_1,\\dots,x_k] = \\frac{[x_1,\\dots,x_k]-[x_0,\\dots,x_{k-1}]}{x_k - x_0},\n$$\nconstruct the Newton interpolation polynomial using the divided differences as coefficients. Analyze how the non-differentiability of $f(x)=|x|$ at $x=0$ manifests in these higher-order divided differences when node sets straddle $x=0$. Then, propose and implement a robust interpolation strategy that avoids large oscillations, and evaluate its effectiveness compared to the global Newton polynomial.\n\nYou must write a complete program that:\n- For each provided node set, computes the divided difference table coefficients $c_k=[x_0,\\dots,x_k]$ for $f(x)=|x|$ and builds the Newton interpolation polynomial $P_n(x)$ of degree $n$.\n- Constructs a robust interpolant $L(x)$ that avoids oscillations. Use a piecewise linear interpolant on consecutive nodes $(x_i,f(x_i))$.\n- Evaluates $P_n(x)$ and $L(x)$ on a uniform grid of $m=1001$ points spanning $[x_{\\min},x_{\\max}]$, where $x_{\\min}$ and $x_{\\max}$ are the smallest and largest nodes in the set.\n- Reports, for each node set, the absolute value of the highest-order divided difference coefficient $|c_n|$, the maximum absolute interpolation error of the Newton polynomial,\n$$\nE_{\\text{poly}} = \\max_{x\\in\\text{grid}} |P_n(x)-|x||,\n$$\nand the maximum absolute interpolation error of the robust piecewise linear interpolant,\n$$\nE_{\\text{robust}} = \\max_{x\\in\\text{grid}} |L(x)-|x||.\n$$\n\nUse the following test suite of node sets:\n1. $x=\\left[-1,\\,0,\\,1\\right]$ (symmetric nodes straddling $x=0$; general case).\n2. $x=\\left[-1,\\,-\\frac{1}{2},\\,0,\\,\\frac{1}{2},\\,1\\right]$ (denser symmetric nodes; potential oscillations).\n3. $x=\\left[-10^{-3},\\,0,\\,10^{-3}\\right]$ (nodes extremely close to $x=0$; boundary behavior of higher-order differences).\n4. $x=\\left[0.1,\\,0.2,\\,0.4,\\,0.8\\right]$ (all nodes on the positive side; no crossing of the non-differentiability point).\n\nAll quantities are dimensionless. There are no angle units involved.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a sub-list of three decimal numbers in the order $[|c_n|, E_{\\text{poly}}, E_{\\text{robust}}]$. The format must be exactly:\n$$\n\\texttt{[[|c_n|,E_{\\text{poly}},E_{\\text{robust}}],[|c_n|,E_{\\text{poly}},E_{\\text{robust}},\\dots]]}\n$$\nwith no spaces anywhere in the line.", "solution": "The problem requires an analysis of Newton's divided difference interpolation for the non-smooth function $f(x) = |x|$. We must compute the interpolation polynomial and a robust piecewise linear alternative, then compare their maximum absolute errors for several sets of nodes.\n\nThe core of the analysis rests on the relationship between a function's smoothness and the behavior of its divided differences. For a function $f$ that is $k$ times continuously differentiable on an interval $I$, the $k$-th order divided difference based on distinct nodes $x_0, x_1, \\dots, x_k$ in $I$ is related to the $k$-th derivative of $f$ by the Mean Value Theorem for Divided Differences:\n$$\n[x_0, x_1, \\dots, x_k] = \\frac{f^{(k)}(\\xi)}{k!}\n$$\nfor some $\\xi$ in the interval spanned by the nodes. The coefficients $c_k = [x_0, \\dots, x_k]$ of the Newton interpolation polynomial,\n$$\nP_n(x) = c_0 + c_1(x-x_0) + c_2(x-x_0)(x-x_1) + \\dots + c_n \\prod_{i=0}^{n-1}(x-x_i),\n$$\ncan thus be seen as approximations of the scaled derivatives of the function. For an analytic function, the derivatives are well-behaved, and for high-degree interpolation, the higher-order coefficients $c_k$ tend to zero, leading to convergence of $P_n(x)$ to $f(x)$.\n\nThe function $f(x)=|x|$ presents a critical exception to this behavior. It is continuous everywhere but not differentiable at $x=0$. Its first derivative can be expressed as the signum function, $f'(x) = \\text{sgn}(x)$, which has a jump discontinuity at $x=0$. The second derivative does not exist in the classical sense; in the framework of distributions, it is the Dirac delta function, $f''(x) = 2\\delta(x)$, which is singular at $x=0$.\n\nThis non-smoothness has profound consequences for polynomial interpolation, which we can analyze based on the node configuration:\n\n1.  **Nodes on one side of the singularity:** If all interpolation nodes $\\{x_i\\}$ are on one side of $x=0$ (i.e., all $x_i > 0$ or all $x_i < 0$), then over the interval spanned by these nodes, $f(x)$ is equivalent to a simple polynomial, either $f(x)=x$ or $f(x)=-x$. For $f(x)=x$, the first divided difference $[x_i, x_j]$ is always $1$, and all higher-order divided differences $[x_i, \\dots, x_k]$ for $k \\ge 2$ are zero. The Newton polynomial will reconstruct the function exactly, resulting in zero interpolation error. This is demonstrated in test case 4.\n\n2.  **Nodes straddling the singularity:** When the set of nodes includes points on both sides of (and possibly at) $x=0$, the interval of interpolation contains the non-differentiable point. The Mean Value Theorem for Divided Differences is no longer applicable for derivatives of order $k \\ge 1$. The divided differences do not approximate a well-behaved derivative. Instead, they attempt to capture the singular nature of the function's derivatives at $x=0$. Specifically, the second-order divided difference $[x_0, x_1, x_2]$ for nodes bracketing the origin will be large. For the symmetric nodes $x_0 = -\\epsilon, x_1 = 0, x_2 = \\epsilon$ with $\\epsilon > 0$, we compute:\n    $$\n    \\begin{align*}\n    [x_0, x_1] &= \\frac{f(0) - f(-\\epsilon)}{0 - (-\\epsilon)} = \\frac{0 - \\epsilon}{\\epsilon} = -1 \\\\\n    [x_1, x_2] &= \\frac{f(\\epsilon) - f(0)}{\\epsilon - 0} = \\frac{\\epsilon - 0}{\\epsilon} = 1 \\\\\n    c_2 = [x_0, x_1, x_2] &= \\frac{[x_1, x_2] - [x_0, x_1]}{x_2 - x_0} = \\frac{1 - (-1)}{\\epsilon - (-\\epsilon)} = \\frac{2}{2\\epsilon} = \\frac{1}{\\epsilon}\n    \\end{align*}\n    $$\n    As $\\epsilon \\to 0$, this coefficient diverges, reflecting the behavior of the singular second derivative. This growth in higher-order coefficients causes the global polynomial $P_n(x)$ to exhibit spurious oscillations (a behavior related to Runge's phenomenon), leading to a large maximum error $E_{\\text{poly}}$.\n\nTo avoid this problem, a robust interpolation strategy must be local, not global. The proposed piecewise linear interpolant, $L(x)$, connects each pair of adjacent nodes $(x_i, f(x_i))$ and $(x_{i+1}, f(x_{i+1}))$ with a straight line. This method is insensitive to the global properties of the function and only depends on local data. Since the function $f(x)=|x|$ is itself composed of two linear pieces, the piecewise linear interpolant $L(x)$ that passes through nodes lying on $f(x)$ will be identical to $f(x)$ itself. Consequently, the interpolation error $E_{\\text{robust}} = \\max |L(x) - |x||$ is expected to be zero, or at the level of machine precision.\n\nThe algorithm proceeds as follows for each test case:\n1.  Given the node set $\\{x_i\\}_{i=0}^n$, calculate the corresponding values $\\{y_i = |x_i|\\}_{i=0}^n$.\n2.  Construct the divided difference table to find the Newton coefficients $c_k = [x_0, \\dots, x_k]$. The final coefficient is $c_n$.\n3.  Evaluate the Newton polynomial $P_n(x)$ using Horner's method on a dense grid of $m=1001$ points in $[x_{\\min}, x_{\\max}]$. Calculate the maximum absolute error $E_{\\text{poly}} = \\max |P_n(x) - |x||$.\n4.  Construct the piecewise linear interpolant $L(x)$ using the sorted nodes. For $f(x)=|x|$, this will be equivalent to $|x|$. Calculate the maximum absolute error $E_{\\text{robust}} = \\max |L(x) - |x||$.\n5.  Report the triplet $[|c_n|, E_{\\text{poly}}, E_{\\text{robust}}]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the interpolation problem for f(x)=|x| for multiple test cases.\n    \"\"\"\n\n    def compute_divided_differences(x_nodes, y_nodes):\n        \"\"\"\n        Computes the divided differences table and returns the coefficients for\n        the Newton polynomial (the top diagonal of the table).\n        \n        Args:\n            x_nodes (np.ndarray): The x-coordinates of the interpolation nodes.\n            y_nodes (np.ndarray): The y-coordinates of the interpolation nodes.\n\n        Returns:\n            np.ndarray: The Newton polynomial coefficients (c_0, c_1, ..., c_n).\n        \"\"\"\n        n = len(x_nodes)\n        coeffs = np.copy(y_nodes)\n        for j in range(1, n):\n            for i in range(n - 1, j - 1, -1):\n                coeffs[i] = (coeffs[i] - coeffs[i - 1]) / (x_nodes[i] - x_nodes[i - j])\n        return coeffs\n\n    def evaluate_newton_poly(x_eval, x_nodes, coeffs):\n        \"\"\"\n        Evaluates the Newton form of the interpolation polynomial using Horner's method.\n        \n        Args:\n            x_eval (np.ndarray): Points at which to evaluate the polynomial.\n            x_nodes (np.ndarray): The x-coordinates of the interpolation nodes.\n            coeffs (np.ndarray): The Newton polynomial coefficients.\n\n        Returns:\n            np.ndarray: The value of the polynomial at each point in x_eval.\n        \"\"\"\n        n = len(coeffs) - 1\n        y = np.full_like(x_eval, coeffs[n])\n        for i in range(n - 1, -1, -1):\n            y = coeffs[i] + (x_eval - x_nodes[i]) * y\n        return y\n    \n    def process_case(x_nodes_list):\n        \"\"\"\n        Processes a single test case: computes coefficients, polynomials, and errors.\n        \n        Args:\n            x_nodes_list (list): A list of floating-point numbers for the nodes.\n\n        Returns:\n            list: A list containing [|c_n|, E_poly, E_robust].\n        \"\"\"\n        x_nodes = np.array(x_nodes_list, dtype=np.float64)\n        y_nodes = np.abs(x_nodes)\n        n = len(x_nodes) - 1\n\n        # 1. Compute divided differences and get the highest-order coefficient c_n\n        # We need to reorder the coefficients from the typical output of a DD table\n        # function to match the Newton form P(x) = c0 + c1(x-x0) + ...\n        # The function below computes coeffs in place: F[j,j] becomes the coefficient c_j\n        F = np.zeros((n + 1, n + 1))\n        F[:, 0] = y_nodes\n\n        for j in range(1, n + 1):\n            for i in range(j, n + 1):\n                F[i, j] = (F[i, j - 1] - F[i - 1, j - 1]) / (x_nodes[i] - x_nodes[i - j])\n        \n        coeffs = np.diag(F)\n        abs_cn = np.abs(coeffs[n])\n\n        # 2. Set up evaluation grid\n        m = 1001\n        x_min, x_max = np.min(x_nodes), np.max(x_nodes)\n        x_eval = np.linspace(x_min, x_max, m)\n        y_true = np.abs(x_eval)\n\n        # 3. Evaluate Newton polynomial and its error\n        y_poly = evaluate_newton_poly(x_eval, x_nodes, coeffs)\n        e_poly = np.max(np.abs(y_poly - y_true))\n\n        # 4. Evaluate piecewise linear interpolant and its error\n        # np.interp requires sorted x-coordinates\n        sort_indices = np.argsort(x_nodes)\n        x_sorted = x_nodes[sort_indices]\n        y_sorted = y_nodes[sort_indices]\n        y_robust = np.interp(x_eval, x_sorted, y_sorted)\n        e_robust = np.max(np.abs(y_robust - y_true))\n\n        return [abs_cn, e_poly, e_robust]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        [-1.0, 0.0, 1.0],\n        [-1.0, -0.5, 0.0, 0.5, 1.0],\n        [-1e-3, 0.0, 1e-3],\n        [0.1, 0.2, 0.4, 0.8],\n    ]\n\n    results = []\n    for case in test_cases:\n        results.append(process_case(case))\n    \n    # Format the final output string exactly as required, with no spaces.\n    result_strings = []\n    for res_list in results:\n        # Format each number and join with commas, enclosed in brackets.\n        sublist_string = f\"[{res_list[0]},{res_list[1]},{res_list[2]}]\"\n        result_strings.append(sublist_string)\n\n    final_output = f\"[{','.join(result_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "3254712"}]}