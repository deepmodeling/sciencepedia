{"hands_on_practices": [{"introduction": "The Vandermonde matrix is the cornerstone of polynomial interpolation, guaranteeing a unique solution. This exercise moves beyond direct computation and challenges you to use the logic of interpolation itself to uncover a fundamental property of the matrix: its determinant. By knowing the expected outcome of interpolating a specific polynomial, you can ingeniously deduce the determinant of the associated Vandermonde matrix, forging a deeper link between abstract linear algebra and the concrete behavior of polynomials. [@problem_id:968114]", "problem": "Consider the interpolation of the function $f(x) = x^4$ at distinct nodes $x_0 = 0$, $x_1 = a$, $x_2 = 2a$, $x_3 = 3a$, $x_4 = 4a$, where $a$ is a non-zero real number. The interpolation system uses a Vandermonde matrix $V$ of size $5 \\times 5$. By Cramer's rule, the coefficient of $x^4$ in the interpolating polynomial is given by the ratio of determinants. Using the fact that the interpolating polynomial is exactly $x^4$, determine the determinant of $V$.", "solution": "The Vandermonde matrix $V$ for nodes $[x_0, x_1, x_2, x_3, x_4]$ is:\n$$\nV = \\begin{bmatrix}\n1  x_0  x_0^2  x_0^3  x_0^4 \\\\\n1  x_1  x_1^2  x_1^3  x_1^4 \\\\\n1  x_2  x_2^2  x_2^3  x_2^4 \\\\\n1  x_3  x_3^2  x_3^3  x_3^4 \\\\\n1  x_4  x_4^2  x_4^3  x_4^4\n\\end{bmatrix} = \\begin{bmatrix}\n1  0  0  0  0 \\\\\n1  a  a^2  a^3  a^4 \\\\\n1  2a  4a^2  8a^3  16a^4 \\\\\n1  3a  9a^2  27a^3  81a^4 \\\\\n1  4a  16a^2  64a^3  256a^4\n\\end{bmatrix}.\n$$\nThe vector of function values is $\\mathbf{y} = [f(x_0), f(x_1), f(x_2), f(x_3), f(x_4)]^\\top = [0, a^4, 16a^4, 81a^4, 256a^4]^\\top$. By Cramer's rule, the coefficient $c_4$ (for $x^4$) is:\n$$\nc_4 = \\frac{\\det(V_4)}{\\det(V)},\n$$\nwhere $V_4$ is $V$ with its fifth column replaced by $\\mathbf{y}$:\n$$\nV_4 = \\begin{bmatrix}\n1  0  0  0  0 \\\\\n1  a  a^2  a^3  a^4 \\\\\n1  2a  4a^2  8a^3  16a^4 \\\\\n1  3a  9a^2  27a^3  81a^4 \\\\\n1  4a  16a^2  64a^3  256a^4\n\\end{bmatrix}.\n$$\nExpand $\\det(V_4)$ along the first row. The only non-zero entry in the first row is the first element (1), so:\n$$\n\\det(V_4) = 1 \\cdot (-1)^{1+1} \\det(M_{11}) = \\det \\begin{bmatrix}\na  a^2  a^3  a^4 \\\\\n2a  4a^2  8a^3  16a^4 \\\\\n3a  9a^2  27a^3  81a^4 \\\\\n4a  16a^2  64a^3  256a^4\n\\end{bmatrix}.\n$$\nFactor $a$ from the first column, $a^2$ from the second, $a^3$ from the third, and $a^4$ from the fourth:\n$$\n= a \\cdot a^2 \\cdot a^3 \\cdot a^4 \\cdot \\det \\begin{bmatrix}\n1  1  1  1 \\\\\n2  4  8  16 \\\\\n3  9  27  81 \\\\\n4  16  64  256\n\\end{bmatrix} = a^{10} \\det(W), \\quad W = \\begin{bmatrix}\n1  1  1  1 \\\\\n2  4  8  16 \\\\\n3  9  27  81 \\\\\n4  16  64  256\n\\end{bmatrix}.\n$$\nFactor constants from each row of $W$:\n- Row 1: factor 1 → $1 \\cdot [1, 1, 1, 1]$\n- Row 2: factor 2 → $2 \\cdot [1, 2, 4, 8]$\n- Row 3: factor 3 → $3 \\cdot [1, 3, 9, 27]$\n- Row 4: factor 4 → $4 \\cdot [1, 4, 16, 64]$\nThus:\n$$\n\\det(W) = (1 \\cdot 2 \\cdot 3 \\cdot 4) \\det \\begin{bmatrix}\n1  1  1  1 \\\\\n1  2  4  8 \\\\\n1  3  9  27 \\\\\n1  4  16  64\n\\end{bmatrix} = 24 \\det(V_{\\text{std}}),\n$$\nwhere $V_{\\text{std}}$ is the Vandermonde matrix for nodes $[1, 2, 3, 4]$ (powers 0 to 3). The determinant is:\n$$\n\\det(V_{\\text{std}}) = \\prod_{1 \\le i  j \\le 4} (j - i) = (2-1)(3-1)(4-1)(3-2)(4-2)(4-3) = 1 \\cdot 2 \\cdot 3 \\cdot 1 \\cdot 2 \\cdot 1 = 12.\n$$\nSo $\\det(W) = 24 \\cdot 12 = 288$, and $\\det(V_4) = a^{10} \\cdot 288$. Since the interpolating polynomial is $x^4$, we know $c_4 = 1$. Solving for $\\det(V)$:\n$$\n1 = \\frac{288 a^{10}}{\\det(V)} \\implies \\det(V) = 288 a^{10}.\n$$", "answer": "$$ \\boxed{288 a^{10}} $$", "id": "968114"}, {"introduction": "While the Vandermonde matrix ensures a unique polynomial passes through a set of points, does this interpolant faithfully represent the underlying continuous function? This practice explores this critical question in the context of signal processing. Through a hands-on coding experiment, you will witness the phenomenon of aliasing, where sampling a high-frequency signal too slowly leads to an interpolating polynomial that incorrectly traces a lower-frequency imposter. [@problem_id:3285621] This demonstrates a crucial limitation of naive polynomial interpolation and highlights the importance of the Nyquist-Shannon sampling theorem.", "problem": "You will write a complete program that uses a Vandermonde system for polynomial interpolation to demonstrate aliasing of a sinusoidal signal. You must work from fundamental definitions of sampling and polynomial interpolation, and quantify how the resulting interpolant cannot disambiguate high-frequency signals that alias to lower frequencies when sampled too slowly.\n\nFundamental base:\n- Uniform sampling at rate $f_s$ means sampling times $t_n = n/f_s$ for integers $n$. For any real frequency $f$, the discrete-time complex exponential $e^{i 2\\pi f t_n}$ is periodic in $f$ with period $f_s$, so $e^{i 2\\pi f t_n} = e^{i 2\\pi (f + k f_s) t_n}$ for any integer $k$. Since $\\sin(\\cdot)$ and $\\cos(\\cdot)$ are the imaginary and real parts of such exponentials, discrete-time sinusoids are indistinguishable up to aliasing. A common baseband alias mapping is $f \\mapsto f_{\\text{alias}} \\in [0, f_s/2]$, defined by folding $f$ into the principal interval via the periodicity of the discrete-time frequency. This encapsulates the Shannon–Nyquist sampling theorem that continuous-time sinusoids cannot be uniquely reconstructed from uniform samples if $f \\ge f_s/2$.\n- For polynomial interpolation, given distinct nodes $x_0, \\dots, x_{N-1}$ and values $y_0, \\dots, y_{N-1}$, there exists a unique polynomial $p(x)$ of degree at most $N-1$ such that $p(x_n)=y_n$ for all $n$. Writing $p(x)=\\sum_{k=0}^{N-1} c_k x^k$, the coefficients $c_k$ solve the Vandermonde linear system $Vc = y$, where $V_{n,k} = x_n^k$.\n\nTask:\n- For each test case, consider a continuous-time signal $s(t) = \\sin(2\\pi f_{\\text{high}} t)$, sampled at times $t_n = n/f_s$ for $n=0, 1, \\dots, N-1$. Build the degree-$(N-1)$ interpolating polynomial $p(t)$ by solving the Vandermonde system with nodes $t_n$ and data $y_n = s(t_n)$.\n- Compute the baseband alias frequency $f_{\\text{alias}} \\in [0, f_s/2]$ obtained by folding $f_{\\text{high}}$ according to the discrete-time frequency periodicity described above.\n- Fit the sampled data $y_n$ by a sinusoid at frequency $f_{\\text{alias}}$ using a linear least squares model $y_n \\approx A \\sin(2\\pi f_{\\text{alias}} t_n) + B \\cos(2\\pi f_{\\text{alias}} t_n)$ to determine coefficients $A$ and $B$ that minimize the sum of squared residuals. This captures the fact that any sampled sinusoid at any frequency is representable (up to aliasing) as a linear combination of in-phase and quadrature sinusoids at its baseband alias.\n- On a dense grid of $M$ points over the interval spanned by the samples, evaluate:\n  1. The true continuous signal $s(t)$.\n  2. The fitted alias sinusoid $\\tilde{s}_{\\text{alias}}(t) = A \\sin(2\\pi f_{\\text{alias}} t) + B \\cos(2\\pi f_{\\text{alias}} t)$.\n  3. The interpolating polynomial $p(t)$ from the Vandermonde system.\n\nFor each test case, compute and report the following quantifiable metrics:\n- $E_1$: the maximum absolute mismatch at the sample times between the measured samples and the fitted alias sinusoid, $E_1 = \\max_n \\left| y_n - \\left(A \\sin(2\\pi f_{\\text{alias}} t_n) + B \\cos(2\\pi f_{\\text{alias}} t_n)\\right) \\right|$.\n- $E_2$: the maximum absolute interpolation error at the sample times, $E_2 = \\max_n | y_n - p(t_n) |$.\n- $\\kappa(V)$: the $2$-norm condition number of the Vandermonde matrix $V$ built on $\\{t_n\\}$.\n- $E_3$: the root-mean-square difference over the dense grid between the true continuous signal and the fitted alias sinusoid, $E_3 = \\sqrt{\\frac{1}{M} \\sum_{m=1}^{M} \\left( s(t_m) - \\tilde{s}_{\\text{alias}}(t_m) \\right)^2 }$.\n- $E_4$: the root-mean-square difference over the dense grid between the interpolating polynomial and the true continuous signal, $E_4 = \\sqrt{\\frac{1}{M} \\sum_{m=1}^{M} \\left( p(t_m) - s(t_m) \\right)^2 }$.\n\nTest suite:\nProvide three test cases to probe different regimes:\n- Case 1 (Aliasing): $f_s = 10.0$ Hertz, $N = 8$, $f_{\\text{high}} = 9.0$ Hertz. Here $f_{\\text{high}} > f_s/2$, so sampling is insufficient and should alias to a low-frequency sinusoid.\n- Case 2 (Adequate sampling): $f_s = 50.0$ Hertz, $N = 12$, $f_{\\text{high}} = 9.0$ Hertz. Here $f_{\\text{high}}  f_s/2$, so the fitted alias should coincide with the true frequency.\n- Case 3 (Nyquist boundary): $f_s = 20.0$ Hertz, $N = 11$, $f_{\\text{high}} = 10.0$ Hertz. This is the Nyquist limit $f_{\\text{high}} = f_s/2$, where a pure sine sampled at $t_n = n/f_s$ produces $y_n = \\sin(\\pi n) = 0$ for all integers $n$, highlighting extreme ambiguity.\n\nImplementation details:\n- Use seconds for time and Hertz for frequency throughout.\n- Build the Vandermonde system with $x_n = t_n$ and monomial basis $\\{1, x, x^2, \\dots, x^{N-1}\\}$; solve $V c = y$ for $c$.\n- Use a dense uniform grid of $M = 1001$ points from the minimum sample time to the maximum sample time.\n- Round all floating-point outputs to $10$ decimal places.\n\nFinal output format:\nYour program should produce a single line of output containing a list of results for all test cases. Each test case must contribute a list of five floats in the order $[E_1, E_2, \\kappa(V), E_3, E_4]$, rounded to $10$ decimal places. The overall output must be a single list of these per-case lists, printed as a single line with no extra text. For example, the printed structure should look like:\n[[e11,e12,e13,e14,e15],[e21,e22,e23,e24,e25],[e31,e32,e33,e34,e35]]", "solution": "The user-provided problem statement has been analyzed and validated.\n\n### Step 1: Extract Givens\n- **Signal**: The continuous-time signal is $s(t) = \\sin(2\\pi f_{\\text{high}} t)$.\n- **Sampling**: Samples are taken at a uniform rate $f_s$, at times $t_n = n/f_s$ for $n=0, 1, \\dots, N-1$. The sampled values are $y_n = s(t_n)$.\n- **Alias Frequency**: The baseband alias frequency $f_{\\text{alias}}$ is in the range $[0, f_s/2]$ and is determined by folding the high frequency $f_{\\text{high}}$ based on the periodicity of discrete-time signals, where $e^{i 2\\pi f t_n} = e^{i 2\\pi (f + k f_s) t_n}$ for any integer $k$.\n- **Polynomial Interpolation**: A unique polynomial $p(t)$ of degree at most $N-1$ is found such that $p(t_n) = y_n$. The coefficients $c_k$ of $p(t)=\\sum_{k=0}^{N-1} c_k t^k$ are found by solving the Vandermonde system $V c = y$, where the matrix $V$ has elements $V_{n,k} = t_n^k$.\n- **Alias Sinusoid Fit**: Coefficients $A$ and $B$ are determined to minimize the sum of squared residuals for the model $y_n \\approx A \\sin(2\\pi f_{\\text{alias}} t_n) + B \\cos(2\\pi f_{\\text{alias}} t_n)$. The resulting fitted sinusoid is $\\tilde{s}_{\\text{alias}}(t) = A \\sin(2\\pi f_{\\text{alias}} t) + B \\cos(2\\pi f_{\\text{alias}} t)$.\n- **Dense Grid**: An evaluation grid of $M = 1001$ uniformly spaced points is used, spanning the interval from $t_0$ to $t_{N-1}$.\n- **Metrics**:\n    1.  Max error of alias fit at sample points: $E_1 = \\max_n \\left| y_n - \\tilde{s}_{\\text{alias}}(t_n) \\right|$.\n    2.  Max error of polynomial interpolation at sample points: $E_2 = \\max_n | y_n - p(t_n) |$.\n    3.  Condition number of Vandermonde matrix: $\\kappa(V)$, using the $2$-norm.\n    4.  RMS error between true signal and alias fit on dense grid: $E_3 = \\sqrt{\\frac{1}{M} \\sum_{m=1}^{M} \\left( s(t_m) - \\tilde{s}_{\\text{alias}}(t_m) \\right)^2 }$.\n    5.  RMS error between true signal and polynomial interpolant on dense grid: $E_4 = \\sqrt{\\frac{1}{M} \\sum_{m=1}^{M} \\left( p(t_m) - s(t_m) \\right)^2 }$.\n- **Test Cases**:\n    - Case 1: $f_s = 10.0$ Hz, $N = 8$, $f_{\\text{high}} = 9.0$ Hz.\n    - Case 2: $f_s = 50.0$ Hz, $N = 12$, $f_{\\text{high}} = 9.0$ Hz.\n    - Case 3: $f_s = 20.0$ Hz, $N = 11$, $f_{\\text{high}} = 10.0$ Hz.\n- **Rounding**: All final floating-point metrics must be rounded to $10$ decimal places.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded (Critical)**: The problem is fundamentally sound. It is built upon core principles of digital signal processing (the Shannon-Nyquist sampling theorem, aliasing) and numerical analysis (polynomial interpolation via Vandermonde systems, linear least squares). All concepts are standard and factually correct.\n- **Well-Posed**: The problem is well-posed. For distinct sample times $t_n$, the Vandermonde matrix $V$ is non-singular, guaranteeing a unique interpolating polynomial exists. The linear least squares problem for the alias sinusoid fit is also well-defined, even in the special edge case (Case 3) where one of the basis functions becomes identically zero over the sample points. The computations are clearly specified and lead to a unique set of metrics.\n- **Objective (Critical)**: The problem is stated in precise, quantitative, and unbiased language. All tasks are deterministic computations based on the provided data.\n- **Other Flaws**: The problem is self-contained, consistent, and computationally feasible. It does not exhibit any of the flaws listed in the validation checklist, such as being non-formalizable, incomplete, unrealistic, ill-posed, or trivial.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n### Principle-Based Design\nThe solution will be constructed by implementing a computational procedure for each test case, following the principles of digital signal processing and numerical linear algebra.\n\n#### 1. Sampling and Aliasing\nFor each test case with parameters $f_s$, $N$, and $f_{\\text{high}}$, we first establish the discrete representation of the signal.\n- The sample times are generated over the interval $[0, (N-1)/f_s]$ as $t_n = n/f_s$ for $n \\in \\{0, 1, \\dots, N-1\\}$.\n- The continuous signal is $s(t) = \\sin(2\\pi f_{\\text{high}} t)$. The samples are computed as $y_n = s(t_n)$.\n- The alias frequency $f_{\\text{alias}}$ must be determined. For a continuous frequency $f$, its discrete-time alias equivalent is periodic with period $f_s$. We find the unique frequency $f' \\in [-f_s/2, f_s/2]$ such that $f \\equiv f' \\pmod{f_s}$. The baseband alias frequency is then $f_{\\text{alias}} = |f'|$. This mapping can be systematically computed as $| \\text{mod}(f_{\\text{high}} + f_s/2, f_s) - f_s/2 |$, where $\\text{mod}$ is the floating-point remainder operation.\n\n#### 2. Polynomial Interpolation using a Vandermonde System\nThe goal is to find the unique polynomial $p(t) = \\sum_{k=0}^{N-1} c_k t^k$ of degree at most $N-1$ that passes through the $N$ sample points $(t_n, y_n)$. This condition, $p(t_n) = y_n$ for all $n$, generates a system of $N$ linear equations for the $N$ unknown coefficients $c_k$:\n$$\n\\begin{pmatrix}\nt_0^0  t_0^1  \\dots  t_0^{N-1} \\\\\nt_1^0  t_1^1  \\dots  t_1^{N-1} \\\\\n\\vdots  \\vdots  \\ddots  \\vdots \\\\\nt_{N-1}^0  t_{N-1}^1  \\dots  t_{N-1}^{N-1}\n\\end{pmatrix}\n\\begin{pmatrix}\nc_0 \\\\ c_1 \\\\ \\vdots \\\\ c_{N-1}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\ny_0 \\\\ y_1 \\\\ \\vdots \\\\ y_{N-1}\n\\end{pmatrix}\n$$\nThis is the Vandermonde system $V c = y$. We will:\n- Construct the Vandermonde matrix $V$ where $V_{n,k} = t_n^k$.\n- Solve the system for the coefficient vector $c$ using a standard linear solver.\n- Compute the $2$-norm condition number $\\kappa(V)$ to assess the numerical stability of the anachronistically ill-conditioned Vandermonde matrix.\n\n#### 3. Alias Sinusoid Fitting via Least Squares\nThe samples $y_n$ are known to be indistinguishable from samples of a sinusoid at frequency $f_{\\text{alias}}$. We model this relationship as $y_n \\approx \\tilde{s}_{\\text{alias}}(t_n) = A \\sin(2\\pi f_{\\text{alias}} t_n) + B \\cos(2\\pi f_{\\text{alias}} t_n)$. We find the coefficients $A$ and $B$ that minimize the sum of squared errors, $\\sum_{n=0}^{N-1} (y_n - \\tilde{s}_{\\text{alias}}(t_n))^2$. This is a linear least squares problem of the form $M x \\approx y$, where:\n$$\nM = \\begin{pmatrix}\n\\sin(2\\pi f_{\\text{alias}} t_0)  \\cos(2\\pi f_{\\text{alias}} t_0) \\\\\n\\sin(2\\pi f_{\\text{alias}} t_1)  \\cos(2\\pi f_{\\text{alias}} t_1) \\\\\n\\vdots  \\vdots \\\\\n\\sin(2\\pi f_{\\text{alias}} t_{N-1})  \\cos(2\\pi f_{\\text{alias}} t_{N-1})\n\\end{pmatrix}, \\quad\nx = \\begin{pmatrix} A \\\\ B \\end{pmatrix}, \\quad\ny = \\begin{pmatrix} y_0 \\\\ y_1 \\\\ \\vdots \\\\ y_{N-1} \\end{pmatrix}\n$$\nThe solution $x$ is found by solving the normal equations $(M^T M) x = M^T y$.\n\n#### 4. Error Quantification\nFinally, we compute the five specified metrics to quantify the effects of aliasing and the behavior of the interpolant.\n- $E_1$: Measures how perfectly the samples $y_n$ match a sinusoid at the alias frequency. For true aliasing, these should be identical, so $E_1$ should be near machine precision.\n- $E_2$: Measures the accuracy of the polynomial interpolation at the sample nodes. By definition of interpolation, this error should be near machine precision, serving as a sanity check for the numerical solution of $Vc=y$.\n- $\\kappa(V)$: Characterizes the ill-conditioning of the Vandermonde matrix. This value is typically large, indicating that small errors in $y_n$ could lead to large errors in the coefficients $c_k$.\n- $E_3$: Measures the RMS difference between the true high-frequency signal and its low-frequency alias representation over the continuous interval. A large value indicates that although they match at sample points, they are distinct functions.\n- $E_4$: Measures the RMS difference between the true signal and the interpolating polynomial. This reveals how well the polynomial, which is forced to match the samples, approximates the underlying continuous function between the sample points. In cases of aliasing, this error is expected to be large, as the polynomial will tend to follow the low-frequency alias rather than the true high-frequency signal.\n\nThe procedure will be applied to each of the three test cases, which are designed to probe the phenomena of aliasing ($f_{\\text{high}} > f_s/2$), adequate sampling ($f_{\\text{high}}  f_s/2$), and the critical Nyquist boundary case ($f_{\\text{high}} = f_s/2$).", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef process_case(fs, N, f_high):\n    \"\"\"\n    Performs the full analysis for a single test case of signal aliasing.\n    \n    Args:\n        fs (float): Sampling frequency in Hertz.\n        N (int): Number of samples.\n        f_high (float): Frequency of the continuous-time signal in Hertz.\n        \n    Returns:\n        list: A list of five metrics [E1, E2, kappa_V, E3, E4], rounded.\n    \"\"\"\n    M = 1001 # Number of points for the dense grid\n\n    # 1. Sampling and Signal Generation\n    t_n = np.arange(N) / fs\n    y_n = np.sin(2 * np.pi * f_high * t_n)\n\n    # 2. Polynomial Interpolation via Vandermonde System\n    # The problem defines V_nk = t_n^k, which corresponds to increasing=True.\n    V = np.vander(t_n, N, increasing=True)\n    \n    # Solve Vc = y_n for polynomial coefficients c\n    # These coefficients are for the basis {1, t, t^2, ...}\n    try:\n        c = np.linalg.solve(V, y_n)\n    except np.linalg.LinAlgError:\n        # This should not happen for distinct t_n, but as a safeguard\n        return [np.nan] * 5\n\n    # Compute condition number of V\n    kappa_V = np.linalg.cond(V)\n\n    # 3. Alias Sinusoid Fitting\n    # Calculate alias frequency in the baseband [0, fs/2]\n    f_prime = np.mod(f_high + fs / 2.0, fs) - fs / 2.0\n    f_alias = np.abs(f_prime)\n    \n    # Set up and solve the linear least squares problem:\n    # y_n approx A*sin(...) + B*cos(...)\n    A_matrix = np.column_stack([\n        np.sin(2 * np.pi * f_alias * t_n),\n        np.cos(2 * np.pi * f_alias * t_n)\n    ])\n    \n    # lstsq returns (coeffs, residuals, rank, singular_values)\n    coeffs_ab, _, _, _ = np.linalg.lstsq(A_matrix, y_n, rcond=None)\n    A, B = coeffs_ab[0], coeffs_ab[1]\n    \n    # Fitted alias sinusoid function\n    s_alias_func = lambda t: A * np.sin(2 * np.pi * f_alias * t) + B * np.cos(2 * np.pi * f_alias * t)\n\n    # 4. Evaluation on Dense Grid\n    t_min, t_max = t_n[0], t_n[-1]\n    t_dense = np.linspace(t_min, t_max, M)\n    \n    # Evaluate signals on the dense grid\n    s_true_dense = np.sin(2 * np.pi * f_high * t_dense)\n    s_alias_dense = s_alias_func(t_dense)\n    # np.polyval expects coefficients in descending order of power\n    p_dense = np.polyval(c[::-1], t_dense)\n\n    # 5. Compute Metrics\n    # E1: Max absolute mismatch of alias fit at sample times\n    s_alias_at_samples = s_alias_func(t_n)\n    E1 = np.max(np.abs(y_n - s_alias_at_samples))\n\n    # E2: Max absolute interpolation error at sample times\n    # This should be close to machine epsilon by definition of interpolation\n    p_at_samples = np.polyval(c[::-1], t_n)\n    E2 = np.max(np.abs(y_n - p_at_samples))\n    \n    # E3: RMS difference between true signal and alias fit on dense grid\n    E3 = np.sqrt(np.mean((s_true_dense - s_alias_dense)**2))\n    \n    # E4: RMS difference between interpolating polynomial and true signal on dense grid\n    E4 = np.sqrt(np.mean((p_dense - s_true_dense)**2))\n\n    # Return rounded metrics\n    return [\n        np.round(E1, 10),\n        np.round(E2, 10),\n        np.round(kappa_V, 10),\n        np.round(E3, 10),\n        np.round(E4, 10)\n    ]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (Aliasing)\n        (10.0, 8, 9.0),\n        # Case 2 (Adequate sampling)\n        (50.0, 12, 9.0),\n        # Case 3 (Nyquist boundary)\n        (20.0, 11, 10.0)\n    ]\n\n    results = []\n    for i, case in enumerate(test_cases):\n        fs, N, f_high = case\n        case_results = process_case(fs, N, f_high)\n        results.append(case_results)\n\n    # Convert each inner list to its string representation\n    # and join them with commas for the final output format.\n    # e.g., [[1.0, 2.0], [3.0, 4.0]] - '[[1.0, 2.0],[3.0, 4.0]]'\n    final_output_str = f\"[{','.join(map(str, results))}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output_str)\n\nsolve()\n```", "id": "3285621"}, {"introduction": "The theoretical elegance of the Vandermonde matrix is often overshadowed by its practical numerical instability; for high-degree polynomials, it becomes severely ill-conditioned. This final exercise guides you through a numerical experiment to confront this problem and explore the solution. You will directly compare the condition number of the monomial basis Vandermonde matrix, $\\kappa_2(V)$, with that of a design matrix built from a stable orthogonal basis (Legendre polynomials), quantitatively demonstrating why orthogonal polynomials are the preferred tool for robust, high-degree polynomial fitting in scientific computing. [@problem_id:3285634]", "problem": "Design and implement a numerical experiment to compare the numerical conditioning of two polynomial bases used for data fitting on a real interval: the monomial basis and an orthogonal polynomial basis. The monomial basis yields the classical Vandermonde matrix. The orthogonal basis to be used is the Legendre polynomial basis on the closed interval $[-1,1]$. Your program must, for several prescribed test cases, construct the corresponding design matrices, compute their condition numbers in the spectral matrix norm, and report the ratio of condition numbers, thereby empirically demonstrating relative numerical stability.\n\nFundamental base:\n- A Vandermonde matrix for nodes $\\{x_i\\}_{i=0}^{n-1}$ and polynomial degree $m$ is the design matrix $V \\in \\mathbb{R}^{n \\times (m+1)}$ with entries $V_{i,k} = x_i^k$ for $k=0,1,\\dots,m$.\n- The Legendre polynomials $\\{P_k\\}_{k=0}^m$ on $[-1,1]$ are defined by orthogonality with respect to the standard inner product $\\langle f,g \\rangle = \\int_{-1}^{1} f(x) g(x) \\, dx$, with normalization $P_0(x)=1$, $P_1(x)=x$, and the well-tested three-term recurrence for $k \\ge 1$,\n$$\n(k+1) P_{k+1}(x) = (2k+1) x P_k(x) - k P_{k-1}(x).\n$$\nThe Legendre design matrix $L \\in \\mathbb{R}^{n \\times (m+1)}$ has entries $L_{i,k} = P_k(x_i)$ for $k=0,1,\\dots,m$.\n- The spectral (matrix $2$-norm) condition number of a matrix $A$ with full column rank is defined by $\\kappa_2(A) = \\sigma_{\\max}(A)/\\sigma_{\\min}(A)$, where $\\sigma_{\\max}(A)$ and $\\sigma_{\\min}(A)$ are the largest and smallest singular values of $A$, respectively. In finite precision arithmetic, larger $\\kappa_2(A)$ indicates greater sensitivity of least-squares solutions to perturbations in the data, hence worse numerical stability.\n\nTask:\n- For each test case specified below, with given degree $m$ and node set $\\{x_i\\}$, construct:\n  1. The Vandermonde matrix $V$ with columns $\\{1,x,x^2,\\dots,x^m\\}$ evaluated at the nodes.\n  2. The Legendre matrix $L$ with columns $\\{P_0(x),P_1(x),\\dots,P_m(x)\\}$ evaluated at the nodes using the three-term recurrence above.\n- Compute $\\kappa_2(V)$ and $\\kappa_2(L)$ for each case.\n- For each case, output the ratio $r = \\kappa_2(V) / \\kappa_2(L)$ as a floating-point number rounded to $6$ significant figures.\n\nAngle units: All trigonometric functions use arguments in radians.\n\nTest suite:\n- Case $1$ (square, equispaced, moderate degree): $m=10$, $n=m+1$, nodes $x_i = -1 + \\frac{2i}{m}$ for $i=0,1,\\dots,m$.\n- Case $2$ (square, equispaced, higher degree): $m=15$, $n=m+1$, nodes $x_i = -1 + \\frac{2i}{m}$ for $i=0,1,\\dots,m$.\n- Case $3$ (square, Chebyshev–Lobatto nodes): $m=15$, $n=m+1$, nodes $x_i = \\cos\\left(\\frac{\\pi i}{m}\\right)$ for $i=0,1,\\dots,m$.\n- Case $4$ (square, clustered near zero): $m=15$, $n=m+1$, nodes $x_i = 10^{-3}\\left(-1 + \\frac{2i}{m}\\right)$ for $i=0,1,\\dots,m$.\n- Case $5$ (overdetermined, Chebyshev–Lobatto nodes): $m=10$, $n=50$, nodes $x_i = \\cos\\left(\\frac{\\pi i}{n-1}\\right)$ for $i=0,1,\\dots,n-1$.\n\nOutput specification:\n- Your program should produce a single line of output containing the $5$ ratios in order for Cases $1$ through $5$, as a comma-separated list enclosed in square brackets. For example: $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5\\right]$.\n- Each $\\text{result}_k$ must be a floating-point number rounded to $6$ significant figures.\n\nScientific realism and coverage notes:\n- The equispaced cases test the known ill-conditioning of Vandermonde systems and the expected improvement from an orthogonal basis.\n- The Chebyshev–Lobatto cases test nodes that are theoretically better for polynomial approximation and reduce ill-conditioning.\n- The clustered near-zero case stresses the monomial basis by creating near-linear dependence among higher powers.\n- The overdetermined case illustrates conditioning in least-squares design matrices.", "solution": "The problem statement has been analyzed and is determined to be **valid**. It is a well-posed, scientifically grounded problem in numerical analysis, with all necessary data and definitions provided. The task is to conduct a numerical experiment comparing the conditioning of design matrices derived from a monomial basis (Vandermonde matrix) and an orthogonal polynomial basis (Legendre polynomials).\n\nThe solution proceeds by implementing a numerical procedure that, for each specified test case, calculates the required ratio of condition numbers. The methodology is as follows:\n\nFirst, for each test case, we are given a polynomial degree $m$, a number of sample points $n$, and a set of nodes $\\{x_i\\}_{i=0}^{n-1}$ on the interval $[-1, 1]$. We construct two design matrices of size $n \\times (m+1)$.\n\nThe first matrix is the Vandermonde matrix, $V$, which arises from using the monomial basis $\\{1, x, x^2, \\dots, x^m\\}$. The entry in the $i$-th row and $k$-th column (using 0-based indexing for rows and columns) of $V$ is given by:\n$$\nV_{i,k} = x_i^k \\quad \\text{for } i=0, \\dots, n-1 \\text{ and } k=0, \\dots, m.\n$$\nThis matrix is constructed for the given node set $\\{x_i\\}$.\n\nThe second matrix is the Legendre design matrix, $L$, which arises from using the Legendre polynomial basis $\\{P_0(x), P_1(x), \\dots, P_m(x)\\}$. The entries of $L$ are given by:\n$$\nL_{i,k} = P_k(x_i) \\quad \\text{for } i=0, \\dots, n-1 \\text{ and } k=0, \\dots, m.\n$$\nThe values of the Legendre polynomials $P_k(x_i)$ are computed numerically. The first two polynomials are defined as $P_0(x) = 1$ and $P_1(x) = x$. Subsequent polynomials for $k \\ge 1$ are generated using the specified three-term recurrence relation:\n$$\n(k+1) P_{k+1}(x) = (2k+1) x P_k(x) - k P_{k-1}(x).\n$$\nThis recurrence is applied iteratively for $k=1, 2, \\dots, m-1$ to compute the values of all necessary basis polynomials at each node $x_i$, thereby populating the columns of the matrix $L$.\n\nNext, the spectral condition number, $\\kappa_2$, is computed for both matrices $V$ and $L$. The spectral condition number of a matrix $A$ with full column rank is defined as the ratio of its largest singular value, $\\sigma_{\\max}(A)$, to its smallest singular value, $\\sigma_{\\min}(A)$:\n$$\n\\kappa_2(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)}.\n$$\nThis value quantifies the sensitivity of the solution of a linear system or a least-squares problem to perturbations in the input data. A larger condition number indicates greater numerical instability. The singular values are computed using a standard Singular Value Decomposition (SVD) algorithm, which is available in numerical libraries like NumPy.\n\nFinally, for each test case, the ratio $r$ of the two condition numbers is calculated:\n$$\nr = \\frac{\\kappa_2(V)}{\\kappa_2(L)}.\n$$\nThis ratio provides a quantitative measure of the improvement in numerical stability gained by using an orthogonal polynomial basis instead of the standard monomial basis for polynomial data fitting. The procedure is repeated for all five test cases, and the resulting ratios are collected. Each ratio is formatted to $6$ significant figures before being included in the final output. The experiment demonstrates the known poor conditioning of the Vandermonde matrix, especially for equispaced nodes and higher degrees, and the significant improvement afforded by the orthogonal Legendre basis.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the numerical experiment problem by comparing the conditioning of\n    Vandermonde and Legendre matrices for several test cases.\n    \"\"\"\n\n    # Helper function to evaluate Legendre polynomials P_k(x) for k=0...m\n    # at a given set of nodes x.\n    def evaluate_legendre_basis(x: np.ndarray, m: int) - np.ndarray:\n        \"\"\"\n        Computes the Legendre matrix L where L[i, k] = P_k(x[i]).\n\n        Args:\n            x: A 1D numpy array of nodes.\n            m: The maximum degree of the polynomial basis.\n\n        Returns:\n            A 2D numpy array of shape (len(x), m + 1) containing the\n            evaluations of Legendre polynomials.\n        \"\"\"\n        n = len(x)\n        L_matrix = np.zeros((n, m + 1))\n\n        # P_0(x) = 1\n        L_matrix[:, 0] = 1.0\n\n        if m  0:\n            # P_1(x) = x\n            L_matrix[:, 1] = x\n\n        # Recurrence relation for k = 1:\n        # (k+1) * P_{k+1}(x) = (2k+1) * x * P_k(x) - k * P_{k-1}(x)\n        for k in range(1, m):\n            p_k = L_matrix[:, k]\n            p_k_minus_1 = L_matrix[:, k-1]\n            p_k_plus_1 = ((2 * k + 1) * x * p_k - k * p_k_minus_1) / (k + 1)\n            L_matrix[:, k + 1] = p_k_plus_1\n        \n        return L_matrix\n\n    # Helper function to process a single test case\n    def compute_condition_number_ratio(m: int, nodes: np.ndarray) - float:\n        \"\"\"\n        Constructs Vandermonde and Legendre matrices and computes the ratio\n        of their condition numbers.\n\n        Args:\n            m: The maximum degree of the polynomial basis.\n            nodes: A 1D numpy array of nodes.\n\n        Returns:\n            The ratio kappa(V) / kappa(L).\n        \"\"\"\n        # 1. Construct the Vandermonde matrix (monomial basis)\n        # Columns are [1, x, x^2, ..., x^m]\n        V = np.vander(nodes, N=m + 1, increasing=True)\n        \n        # 2. Construct the Legendre matrix (orthogonal basis)\n        L = evaluate_legendre_basis(nodes, m)\n        \n        # 3. Compute the spectral (2-norm) condition numbers\n        cond_V = np.linalg.cond(V)\n        cond_L = np.linalg.cond(L)\n        \n        # 4. Return the ratio\n        if cond_L == 0:\n            # Avoid division by zero, though unlikely in these cases\n            return float('inf')\n        return cond_V / cond_L\n\n    # Define the test suite as specified in the problem\n    test_cases = [\n        # Case 1: m=10, n=11, equispaced\n        {'m': 10, 'n': 11, 'node_gen': lambda i, m, n: -1 + 2 * i / m},\n        # Case 2: m=15, n=16, equispaced\n        {'m': 15, 'n': 16, 'node_gen': lambda i, m, n: -1 + 2 * i / m},\n        # Case 3: m=15, n=16, Chebyshev-Lobatto\n        {'m': 15, 'n': 16, 'node_gen': lambda i, m, n: np.cos(np.pi * i / m)},\n        # Case 4: m=15, n=16, clustered near zero\n        {'m': 15, 'n': 16, 'node_gen': lambda i, m, n: 1e-3 * (-1 + 2 * i / m)},\n        # Case 5: m=10, n=50, overdetermined Chebyshev-Lobatto\n        {'m': 10, 'n': 50, 'node_gen': lambda i, m, n: np.cos(np.pi * i / (n - 1))}\n    ]\n\n    results = []\n    for case in test_cases:\n        m, n = case['m'], case['n']\n        node_gen = case['node_gen']\n        \n        indices = np.arange(n)\n        nodes = node_gen(indices, m, n)\n        \n        ratio = compute_condition_number_ratio(m, nodes)\n        \n        # Format the result to 6 significant figures\n        results.append(f\"{ratio:.6g}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```", "id": "3285634"}]}