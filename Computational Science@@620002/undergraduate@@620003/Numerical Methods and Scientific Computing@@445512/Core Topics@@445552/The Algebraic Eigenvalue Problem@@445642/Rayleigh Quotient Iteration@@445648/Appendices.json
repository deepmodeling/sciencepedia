{"hands_on_practices": [{"introduction": "To begin our hands-on exploration of the Rayleigh Quotient Iteration, let's first solidify our understanding of the Rayleigh quotient itself. In the ideal scenario where our vector is already an eigenvector of a matrix, the Rayleigh quotient provides a direct and elegant connection to its corresponding eigenvalue. This foundational exercise [@problem_id:2196888] demonstrates this principle with a simple diagonal matrix, where the eigenvectors and eigenvalues are immediately apparent.", "problem": "In numerical linear algebra, the Rayleigh quotient is a valuable tool for estimating eigenvalues of a symmetric matrix. For a given real symmetric square matrix $M$ and a non-zero vector $v$, the Rayleigh quotient is defined as:\n$$R(M, v) = \\frac{v^T M v}{v^T v}$$\nwhere $v^T$ is the transpose of the column vector $v$. When the vector $v$ is an eigenvector of $M$, the Rayleigh quotient is exactly equal to the corresponding eigenvalue.\n\nConsider the real, symmetric $4 \\times 4$ matrix $A$ defined as a diagonal matrix with entries $(2, 5, 11, -3)$:\n$$A = \\begin{pmatrix} 2 & 0 & 0 & 0 \\\\ 0 & 5 & 0 & 0 \\\\ 0 & 0 & 11 & 0 \\\\ 0 & 0 & 0 & -3 \\end{pmatrix}$$\nLet $e_3$ be the third standard basis vector in $\\mathbb{R}^4$, which is a column vector with a 1 in the third position and 0s elsewhere.\n\nCalculate the value of the Rayleigh quotient $R(A, e_3)$.", "solution": "The Rayleigh quotient for a real symmetric matrix $A$ and non-zero vector $v$ is defined by\n$$\nR(A,v)=\\frac{v^{T}A v}{v^{T}v}.\n$$\nFor the given diagonal matrix\n$$\nA=\\begin{pmatrix}2&0&0&0\\\\0&5&0&0\\\\0&0&11&0\\\\0&0&0&-3\\end{pmatrix}\n$$\nand the standard basis vector $e_3=(0,0,1,0)^{T}$, we compute\n$$\nA e_3=\\begin{pmatrix}0\\\\0\\\\11\\\\0\\end{pmatrix}=11 e_3.\n$$\nThus,\n$$\ne_3^{T}A e_3=e_3^{T}(11 e_3)=11\\,(e_3^{T}e_3).\n$$\nSince $e_3^{T}e_3=1$, the Rayleigh quotient is\n$$\nR(A,e_3)=\\frac{e_3^{T}A e_3}{e_3^{T}e_3}=\\frac{11\\,(e_3^{T}e_3)}{e_3^{T}e_3}=11.\n$$\nThis is consistent with the fact that $e_3$ is an eigenvector of $A$ with eigenvalue $11$, and the Rayleigh quotient returns that eigenvalue.", "answer": "$$\\boxed{11}$$", "id": "2196888"}, {"introduction": "Having established how the Rayleigh quotient relates to eigenvalues, we can now examine the iterative process that gives the method its name. A single step of the Rayleigh Quotient Iteration involves a sequence of three key operations: estimating the eigenvalue, solving a shifted linear system, and normalizing the result to get an improved eigenvector approximation. This practice [@problem_id:2196909] guides you through one complete cycle of this powerful algorithm, providing a concrete feel for its mechanics.", "problem": "The Rayleigh quotient iteration is a powerful numerical method used to find an eigenvalue and a corresponding eigenvector of a matrix. For a given symmetric matrix $A$ and an initial non-zero vector approximation $x_k$ (at iteration $k$), a single full iteration to compute the next approximation $x_{k+1}$ proceeds as follows:\n\n1.  Calculate the Rayleigh quotient, $\\mu_k$, which is an estimate for the eigenvalue:\n    $$ \\mu_k = \\frac{x_k^T A x_k}{x_k^T x_k} $$\n2.  Solve the linear system for an intermediate vector $w$:\n    $$ (A - \\mu_k I)w = x_k $$\n    where $I$ is the identity matrix.\n3.  Normalize the vector $w$ to find the new eigenvector approximation:\n    $$ x_{k+1} = \\frac{w}{||w||_2} $$\n    where $||w||_2$ denotes the Euclidean norm of $w$.\n\nYou are given the symmetric matrix $A = \\begin{pmatrix} 5 & 2 \\\\ 2 & 2 \\end{pmatrix}$ and an initial vector $x_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n\nPerform one full iteration of the Rayleigh quotient method to find the next eigenvector approximation, $x_1$. Express your answer as a row matrix containing the two components of $x_1$ in their exact analytical form.", "solution": "Given $A=\\begin{pmatrix}5 & 2 \\\\ 2 & 2\\end{pmatrix}$ and $x_{0}=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$.\n\nFirst compute the Rayleigh quotient:\n$$\n\\mu_{0}=\\frac{x_{0}^{T}Ax_{0}}{x_{0}^{T}x_{0}}=\\frac{\\begin{pmatrix}1 & 0\\end{pmatrix}\\begin{pmatrix}5 & 2 \\\\ 2 & 2\\end{pmatrix}\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}}{\\begin{pmatrix}1 & 0\\end{pmatrix}\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}}=\\frac{\\begin{pmatrix}1 & 0\\end{pmatrix}\\begin{pmatrix}5 \\\\ 2\\end{pmatrix}}{1}=\\frac{5}{1}=5.\n$$\n\nNext solve $(A-\\mu_{0}I)w=x_{0}$:\n$$\n\\left(\\begin{pmatrix}5 & 2 \\\\ 2 & 2\\end{pmatrix}-5\\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix}\\right)\\begin{pmatrix}w_{1} \\\\ w_{2}\\end{pmatrix}=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}\\;\\;\\Longrightarrow\\;\\;\\begin{pmatrix}0 & 2 \\\\ 2 & -3\\end{pmatrix}\\begin{pmatrix}w_{1} \\\\ w_{2}\\end{pmatrix}=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}.\n$$\nThis gives the system\n$$\n2w_{2}=1,\\qquad 2w_{1}-3w_{2}=0.\n$$\nFrom the first equation $w_{2}=\\frac{1}{2}$, and substituting into the second gives $2w_{1}-\\frac{3}{2}=0$, hence $w_{1}=\\frac{3}{4}$. Therefore\n$$\nw=\\begin{pmatrix}\\frac{3}{4} \\\\ \\frac{1}{2}\\end{pmatrix}.\n$$\n\nNormalize $w$ to obtain $x_{1}$:\n$$\n\\|w\\|_{2}=\\sqrt{\\left(\\frac{3}{4}\\right)^{2}+\\left(\\frac{1}{2}\\right)^{2}}=\\sqrt{\\frac{9}{16}+\\frac{4}{16}}=\\sqrt{\\frac{13}{16}}=\\frac{\\sqrt{13}}{4},\n$$\n$$\nx_{1}=\\frac{w}{\\|w\\|_{2}}=\\frac{4}{\\sqrt{13}}\\begin{pmatrix}\\frac{3}{4} \\\\ \\frac{1}{2}\\end{pmatrix}=\\begin{pmatrix}\\frac{3}{\\sqrt{13}} \\\\ \\frac{2}{\\sqrt{13}}\\end{pmatrix}.\n$$\n\nExpressed as a row matrix, the two components are $\\begin{pmatrix}\\frac{3}{\\sqrt{13}} & \\frac{2}{\\sqrt{13}}\\end{pmatrix}$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{3}{\\sqrt{13}} & \\frac{2}{\\sqrt{13}}\\end{pmatrix}}$$", "id": "2196909"}, {"introduction": "Real-world applications often present challenges that go beyond simple, distinct eigenvalues. This advanced practice delves into a more complex and common scenario: the behavior of RQI when a matrix possesses repeated eigenvalues, resulting in a multi-dimensional eigenspace. This computational experiment [@problem_id:3184156] reveals how the algorithm's convergence path is influenced by the initial guess and, more importantly, how we can strategically impose constraints to guide the iteration toward a specific, desired eigenvector within that space.", "problem": "Consider a real, symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ and the iterative method known as Rayleigh quotient iteration (RQI), where one seeks to compute an eigenpair $(\\lambda, v)$ by iteratively updating a direction vector $x$ and a scalar estimate $\\mu$ associated with the Rayleigh quotient. The foundational base for this problem is the definition of eigenvalues and eigenvectors, namely that $A v = \\lambda v$ for some scalar $\\lambda$ and nonzero vector $v$, and the observation that stationary points of the Rayleigh quotient over the unit sphere correspond to eigenvectors of $A$. In the presence of repeated eigenvalues, the associated eigenspace has dimension greater than $1$, and the direction of convergence under RQI is not uniquely determined by the eigenvalue alone. Instead, the limiting direction can be any unit vector in the eigenspace, depending on the initial iterate and any constraints imposed during the iterations.\n\nYour task is to implement an experiment that demonstrates and quantifies the behavior of Rayleigh quotient iteration when $A$ has a repeated eigenvalue. Use the following specific, universal setup:\n\n- Let $n = 3$, and let $A$ be the diagonal matrix $A = \\operatorname{diag}(5, 5, 1)$, where the eigenvalue $\\lambda = 5$ is repeated with multiplicity $2$. The associated eigenspace is the span of the standard basis vectors $e_1 = [1, 0, 0]^\\top$ and $e_2 = [0, 1, 0]^\\top$.\n- Implement any correct version of Rayleigh quotient iteration that, at each iteration, computes the current Rayleigh quotient estimate for $x$, solves the corresponding shifted linear system, normalizes the resulting direction, and checks for convergence using the residual norm. To address the case of near-singular or singular shifts, you should use a numerically sensible strategy (for example, a least-squares solve) whenever direct solving is ill-conditioned or fails. Convergence should be declared when the Euclidean norm of the residual $r = A x - \\mu x$ is less than $10^{-12}$, where $\\mu$ is the current Rayleigh quotient estimate and $x$ is the current normalized iterate. Use a maximum of $50$ iterations if the residual criterion is not met earlier.\n- To study selection under constraints, include an optional enforcement step after each update that projects the current iterate onto the null space of a linear constraint $g^\\top x = 0$ for a given constraint vector $g \\in \\mathbb{R}^n$. The projection should be the orthogonal projector onto $\\{x \\in \\mathbb{R}^n : g^\\top x = 0\\}$, which leaves vectors in the constraint set unchanged and removes the component along $g$ otherwise.\n\nDefine the selection rule for reporting the result as follows. Let $x_{\\text{final}}$ be the final normalized iterate returned by your implementation:\n- Compute the absolute inner products $a_1 = |\\langle x_{\\text{final}}, e_1 \\rangle|$ and $a_2 = |\\langle x_{\\text{final}}, e_2 \\rangle|$.\n- If $a_1 \\ge a_2$, report the integer $0$; otherwise report the integer $1$. This reports which basis direction in the eigenspace $\\operatorname{span}\\{e_1, e_2\\}$ the final iterate is closer to, according to the defined rule.\n\nTest Suite:\nUse the following five test cases, which together exercise a general case, a boundary condition, and constraint-enforced selection:\n1. Unconstrained RQI with initial vector $x_0 = [0.8, 0.2, 0.1]^\\top$ (normalize $x_0$ before iteration). Report the selection integer based on the rule above.\n2. Unconstrained RQI with initial vector $x_0 = [0.2, 0.8, 0.1]^\\top$ (normalize $x_0$ before iteration). Report the selection integer based on the rule above.\n3. Unconstrained RQI with initial vector $x_0 = e_1 = [1, 0, 0]^\\top$. This is a boundary case where the initial vector is already an eigenvector in the repeated eigenspace. Report the selection integer based on the rule above.\n4. Constrained RQI with constraint vector $g = e_2 = [0, 1, 0]^\\top$ and initial vector $x_0 = [0.2, 0.8, 0.1]^\\top$ (normalize $x_0$ before iteration). Enforce $g^\\top x = 0$ at every iteration via orthogonal projection. Report the selection integer based on the rule above.\n5. Constrained RQI with constraint vector $g = e_1 = [1, 0, 0]^\\top$ and initial vector $x_0 = [0.8, 0.2, 0.1]^\\top$ (normalize $x_0$ before iteration). Enforce $g^\\top x = 0$ at every iteration via orthogonal projection. Report the selection integer based on the rule above.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite above. For example, the output must look like $[r_1, r_2, r_3, r_4, r_5]$, where each $r_i$ is the integer selection result for test case $i$ as defined.", "solution": "This problem requires implementing Rayleigh quotient iteration (RQI) to investigate its convergence behavior for a matrix with a repeated eigenvalue. The core algorithm and its application to several test cases (both unconstrained and constrained) will demonstrate how the final converged eigenvector is selected from the multi-dimensional eigenspace.\n\nThe RQI algorithm proceeds iteratively:\n1.  **Estimate Eigenvalue**: For the current normalized vector iterate $x_k$, the Rayleigh quotient $\\mu_k = x_k^\\top A x_k$ provides an excellent estimate of the corresponding eigenvalue.\n2.  **Solve Shifted System**: The next iterate direction is found by solving the linear system $(A - \\mu_k I) y_k = x_k$. As $\\mu_k$ approaches an eigenvalue, the matrix $(A - \\mu_k I)$ becomes singular. To handle this robustly, we solve the system in a least-squares sense, which correctly identifies a direction in the target eigenspace.\n3.  **Normalize**: The new iterate is obtained by normalizing the direction vector: $x_{k+1} = y_k / \\|y_k\\|_2$.\n\nThe specified matrix is $A = \\operatorname{diag}(5, 5, 1)$, which has a repeated eigenvalue $\\lambda = 5$ with a two-dimensional eigenspace spanned by $e_1=[1,0,0]^\\top$ and $e_2=[0,1,0]^\\top$. Any vector in the span of $e_1$ and $e_2$ is an eigenvector.\n\n-   **Unconstrained RQI**: When RQI is started with an initial vector $x_0$ that has non-zero components in the directions of both $e_1$ and $e_2$, it will converge to an eigenvector in the $e_1$-$e_2$ plane. The specific direction of the final vector is determined by the projection of the initial vector onto this plane. The iteration preserves the ratio of the components within this eigenspace. For instance, an initial vector with a larger component along $e_1$ than $e_2$ will result in a final vector that is also \"closer\" to $e_1$.\n\n-   **Constrained RQI**: We can force convergence to a specific eigenvector within the degenerate eigenspace by applying a constraint. At each step, after finding the new vector $x_{k+1}$, we project it orthogonally onto the subspace defined by $g^\\top x = 0$. The projector is $P = I - \\frac{g g^\\top}{g^\\top g}$. This projection removes the component of the vector along $g$. For example, if we set the constraint vector $g = e_2$, the projection step will zero out the second component of the iterate at every step, forcing convergence towards an eigenvector orthogonal to $e_2$, which in this case is $e_1$.\n\n**Analysis of Test Cases:**\n1.  **Case 1**: $x_0 = [0.8, 0.2, 0.1]^\\top$. The initial vector's projection onto the eigenspace of $\\lambda=5$ is $[0.8, 0.2, 0]^\\top$. Since the first component (0.8) is larger than the second (0.2), the unconstrained iteration will converge to a vector closer to $e_1$. Result: **0**.\n2.  **Case 2**: $x_0 = [0.2, 0.8, 0.1]^\\top$. The projection onto the eigenspace is $[0.2, 0.8, 0]^\\top$. The second component is larger, so convergence will be towards a vector closer to $e_2$. Result: **1**.\n3.  **Case 3**: $x_0 = e_1$. The initial vector is already the desired eigenvector. The iteration will converge immediately (the residual is already zero). The final vector is $e_1$. Result: **0**.\n4.  **Case 4**: $x_0 = [0.2, 0.8, 0.1]^\\top$, constraint $g = e_2$. Even though the initial vector has a large $e_2$ component, the constraint $g^\\top x = 0$ (i.e., $x_2=0$) is enforced at every iteration. This systematically removes the $e_2$ component, forcing convergence to the only other direction in the eigenspace, which is along $e_1$. Result: **0**.\n5.  **Case 5**: $x_0 = [0.8, 0.2, 0.1]^\\top$, constraint $g = e_1$. Symmetrically to case 4, the constraint $x_1=0$ forces convergence towards the $e_2$ direction, despite the initial vector being closer to $e_1$. Result: **1**.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef rayleigh_quotient_iteration(A, x0, g=None, tol=1e-12, max_iter=50):\n    \"\"\"\n    Performs Rayleigh Quotient Iteration to find an eigenpair of matrix A.\n\n    Args:\n        A (np.ndarray): The real, symmetric matrix.\n        x0 (np.ndarray): The initial guess vector.\n        g (np.ndarray, optional): The constraint vector for projection. Defaults to None.\n        tol (float): The convergence tolerance for the residual norm.\n        max_iter (int): The maximum number of iterations.\n\n    Returns:\n        np.ndarray: The final normalized eigenvector estimate.\n    \"\"\"\n    n = A.shape[0]\n    I = np.identity(n)\n    \n    # Normalize the initial vector\n    if np.linalg.norm(x0) == 0:\n        x = np.random.rand(n)\n        x = x / np.linalg.norm(x)\n    else:\n        x = x0 / np.linalg.norm(x0)\n\n    # Pre-calculate for projection if g is provided and non-zero\n    g_norm_sq = 0.0\n    if g is not None:\n        g_norm_sq = np.dot(g, g)\n        if g_norm_sq == 0:\n            g = None # Treat zero vector constraint as no constraint\n\n    for _ in range(max_iter):\n        mu = np.dot(x, A @ x)\n        \n        residual = A @ x - mu * x\n        if np.linalg.norm(residual)  tol:\n            break\n            \n        # Solve the shifted system (A - mu*I)y = x using least squares\n        # This is robust against mu being close to an eigenvalue\n        B = A - mu * I\n        y, _, _, _ = np.linalg.lstsq(B, x, rcond=None)\n        \n        # Normalize the new direction\n        norm_y = np.linalg.norm(y)\n        if norm_y  np.finfo(float).eps:\n            break\n        x_new = y / norm_y\n        \n        # Apply optional constraint by orthogonal projection\n        if g is not None and g_norm_sq > 0:\n            # Project x_new onto the null space of g'x = 0\n            # P(x) = x - g * (g'x / g'g)\n            proj = x_new - g * (np.dot(g, x_new) / g_norm_sq)\n            \n            norm_proj = np.linalg.norm(proj)\n            if norm_proj  np.finfo(float).eps:\n                break\n            x = proj / norm_proj\n        else:\n            x = x_new\n            \n    return x\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define problem parameters\n    A = np.diag([5.0, 5.0, 1.0])\n    e1 = np.array([1.0, 0.0, 0.0])\n    e2 = np.array([0.0, 1.0, 0.0])\n    \n    # Define the test cases\n    test_cases = [\n        # Case 1: Unconstrained, x0 has larger e1 component\n        {'x0': np.array([0.8, 0.2, 0.1]), 'g': None},\n        # Case 2: Unconstrained, x0 has larger e2 component\n        {'x0': np.array([0.2, 0.8, 0.1]), 'g': None},\n        # Case 3: Unconstrained, x0 is an eigenvector\n        {'x0': np.array([1.0, 0.0, 0.0]), 'g': None},\n        # Case 4: Constrained to be orthogonal to e2\n        {'x0': np.array([0.2, 0.8, 0.1]), 'g': e2},\n        # Case 5: Constrained to be orthogonal to e1\n        {'x0': np.array([0.8, 0.2, 0.1]), 'g': e1},\n    ]\n\n    results = []\n    for case in test_cases:\n        x_final = rayleigh_quotient_iteration(A, case['x0'], g=case['g'])\n        \n        # Apply the selection rule\n        a1 = np.abs(np.dot(x_final, e1))\n        a2 = np.abs(np.dot(x_final, e2))\n        \n        if a1 >= a2:\n            results.append(0)\n        else:\n            results.append(1)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3184156"}]}