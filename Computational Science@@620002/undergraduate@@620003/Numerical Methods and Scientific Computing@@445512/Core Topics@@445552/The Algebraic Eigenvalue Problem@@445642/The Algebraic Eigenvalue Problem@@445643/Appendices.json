{"hands_on_practices": [{"introduction": "The power method is often the first iterative algorithm taught for approximating the dominant eigenvalue of a matrix, due to its conceptual simplicity. However, its practical performance can reveal deep insights into the matrix's underlying structure. This exercise guides you through implementing the power method for a nearly-defective matrix, a case where the eigenvectors are almost linearly dependent. By observing the algorithm's behavior, you will gain a practical understanding of how an ill-conditioned eigenbasis can dramatically slow convergence, a crucial lesson in numerical stability [@problem_id:3282259].", "problem": "You will investigate the behavior of the power method on a carefully constructed nearly-defective matrix in the algebraic eigenvalue problem. The power method attempts to approximate a dominant eigenpair by repeatedly applying a matrix to a vector and normalizing. The fundamental base for this investigation comprises the definitions of eigenvalues and eigenvectors, the notion of diagonalizability, and the definition of the power iteration. Specifically, if a matrix $A \\in \\mathbb{R}^{n \\times n}$ has an eigenvalue of largest magnitude (the spectral radius), and the initial vector has a nonzero component in the direction of the corresponding right eigenvector, then the power method tends to align with that eigenvector; otherwise, it fails to do so.\n\nConstruct a nearly-defective $2 \\times 2$ matrix as follows. Let $S = \\begin{bmatrix} 1  1 \\\\ 0  \\varepsilon \\end{bmatrix}$, with $\\varepsilon = 10^{-8}$, and let $J = \\operatorname{diag}(\\lambda_1,\\lambda_2)$ with $\\lambda_1 = 1 + \\delta$ and $\\lambda_2 = 1$, where $\\delta = 10^{-4}$. Define\n$$\nA \\;=\\; S J S^{-1}.\n$$\nBy construction, $A$ is diagonalizable with right eigenvectors given by the columns of $S$, which are $s_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ associated with $\\lambda_1$ and $s_2 = \\begin{bmatrix} 1 \\\\ \\varepsilon \\end{bmatrix}$ associated with $\\lambda_2$. Because $\\varepsilon$ is very small, the two eigenvectors are nearly parallel, and the eigenbasis is ill-conditioned, making $A$ nearly defective. The left eigenvectors (rows of $S^{-1}$) determine the decomposition coefficients of an initial vector when expressed in the right-eigenvector basis.\n\nImplement the power method iteration defined by\n$$\ny_{k+1} \\;=\\; A x_k,\\quad x_{k+1} \\;=\\; \\frac{y_{k+1}}{\\|y_{k+1}\\|_2},\n$$\nstarting from a given initial vector $x_0 \\in \\mathbb{R}^2 \\setminus \\{0\\}$. After a prescribed number of iterations $N$, report the Rayleigh quotient\n$$\n\\rho(x) \\;=\\; \\frac{x^\\top A x}{x^\\top x},\n$$\nas an estimate of the eigenvalue associated with the final direction $x$. Use this estimate to decide whether the iteration aligned numerically with the dominant eigenvalue $\\lambda_1$ or with the subdominant eigenvalue $\\lambda_2$ by comparing absolute differences $|\\rho(x) - \\lambda_1|$ and $|\\rho(x) - \\lambda_2|$.\n\nYour program must:\n- Construct $A$ from the specified $S$, $\\varepsilon = 10^{-8}$, and $J$ with $\\lambda_1 = 1 + 10^{-4}$ and $\\lambda_2 = 1$.\n- Implement the power method for exactly $N = 1000$ iterations (do not use an adaptive stopping criterion).\n- For each initial vector in the test suite below, compute the final iterate $x_N$ (normalized at each step) and its Rayleigh quotient $\\rho(x_N)$, then output a boolean that is true if the final Rayleigh quotient is closer to $\\lambda_1$ than to $\\lambda_2$, and false otherwise.\n\nTest suite of initial vectors:\n- Case $1$ (exact subdominant eigenvector): $x_0^{(1)} = s_2 = \\begin{bmatrix} 1 \\\\ \\varepsilon \\end{bmatrix}$.\n- Case $2$ (extremely close to subdominant eigenvector): $x_0^{(2)} = s_2 + 10^{-30} s_1$.\n- Case $3$ (exact dominant eigenvector): $x_0^{(3)} = s_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\n- Case $4$ (nearly orthogonal to the left dominant eigenvector): $x_0^{(4)} = \\begin{bmatrix} \\varepsilon^{-1} + 1 \\\\ 1 \\end{bmatrix}$.\n\nExpected qualitative behavior rooted in first principles:\n- In Case $1$, because $x_0$ is exactly an eigenvector for $\\lambda_2$, all iterates remain in the span of $s_2$, so the power method will produce a Rayleigh quotient equal (up to roundoff) to $\\lambda_2$, i.e., the wrong eigenvalue relative to the dominant.\n- In Case $2$, the coefficient of $s_1$ is astronomically small, and with $|\\lambda_1/\\lambda_2|$ barely greater than $1$, after $1000$ iterations the dominant component remains negligible; the method appears to converge to the wrong eigenvector.\n- In Case $3$, the method immediately yields the dominant eigenpair.\n- In Case $4$, the coefficient of $s_1$ is very small compared to that of $s_2$, so within $1000$ iterations the method will still be numerically aligned with the subdominant eigenvector.\n\nFinal output format:\n- Your program should produce a single line of output containing the four boolean results for the four cases above as a comma-separated list enclosed in square brackets, for example, \"[True,False,True,False]\".\n- No physical units or angles are involved; all numerical quantities are dimensionless real numbers.\n\nThe problem requires you to reason from the definitions of eigenvalues and eigenvectors, diagonalization, left and right eigenvectors, and the structure of the power method. You must avoid any shortcuts beyond these definitions. Ensure scientific realism by using the specified parameters and provide a robust implementation following the above instructions.", "solution": "The user has presented a problem from the field of numerical linear algebra, specifically concerning the behavior of the power method on a nearly-defective matrix. The problem requires implementing the power method and analyzing its convergence for a set of carefully chosen initial vectors.\n\n### Problem Validation\nThe problem statement has been validated and is deemed sound.\n- **Givens**: All parameters ($\\varepsilon = 10^{-8}$, $\\delta = 10^{-4}$), matrices ($S$, $J$, and the resulting $A$), algorithm specifications (power method with $N=1000$ iterations and $L_2$ normalization), initial conditions (four test vectors $x_0$), and output requirements (a boolean comparison of the final Rayleigh quotient against the known eigenvalues) are explicitly provided.\n- **Scientific Grounding**: The problem is based on fundamental and standard principles of the algebraic eigenvalue problem, including matrix diagonalization, left and right eigenvectors, and the power iteration method. The construction of a nearly-defective matrix to study numerical stability and convergence is a classic pedagogical tool in numerical analysis.\n- **Well-Posedness and Objectivity**: The problem is unambiguous, self-contained, and objective. It specifies a deterministic computational task that yields a unique, verifiable result for each case.\n\nThe problem is valid and a solution will be furnished.\n\n### Principle-Based Solution\nThe core task is to analyze the power method's performance for a matrix $A$ with an ill-conditioned eigenbasis. The power method is an iterative algorithm designed to find the dominant eigenvalue and a corresponding eigenvector of a matrix. Its behavior is dictated by the decomposition of the initial vector in the basis of the matrix's eigenvectors.\n\n**1. Matrix Construction and Properties**\n\nThe matrix $A$ is constructed as $A = S J S^{-1}$, where:\n$S = \\begin{bmatrix} 1  1 \\\\ 0  \\varepsilon \\end{bmatrix}$ with $\\varepsilon = 10^{-8}$.\n$J = \\operatorname{diag}(\\lambda_1, \\lambda_2)$ with $\\lambda_1 = 1 + \\delta = 1 + 10^{-4}$ and $\\lambda_2 = 1$.\n\nThe columns of $S$ are the right eigenvectors of $A$:\n- $s_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ corresponding to the dominant eigenvalue $\\lambda_1 = 1.0001$.\n- $s_2 = \\begin{bmatrix} 1 \\\\ \\varepsilon \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 10^{-8} \\end{bmatrix}$ corresponding to the subdominant eigenvalue $\\lambda_2 = 1$.\n\nSince $\\varepsilon$ is very small, the eigenvectors $s_1$ and $s_2$ are nearly parallel, making the eigenvector basis ill-conditioned. This means that $A$ is a nearly-defective matrix.\n\nThe inverse of $S$ is computed as $S^{-1} = \\frac{1}{\\det(S)} \\begin{bmatrix} \\varepsilon  -1 \\\\ 0  1 \\end{bmatrix} = \\frac{1}{\\varepsilon} \\begin{bmatrix} \\varepsilon  -1 \\\\ 0  1 \\end{bmatrix} = \\begin{bmatrix} 1  -1/\\varepsilon \\\\ 0  1/\\varepsilon \\end{bmatrix}$. The rows of $S^{-1}$ are the left eigenvectors of $A$.\n\nThe matrix $A$ can be computed explicitly:\n$$\nA = S J S^{-1} = \\begin{bmatrix} 1  1 \\\\ 0  \\varepsilon \\end{bmatrix} \\begin{bmatrix} 1+\\delta  0 \\\\ 0  1 \\end{bmatrix} \\begin{bmatrix} 1  -1/\\varepsilon \\\\ 0  1/\\varepsilon \\end{bmatrix}\n$$\n$$\nA = \\begin{bmatrix} 1  1 \\\\ 0  \\varepsilon \\end{bmatrix} \\begin{bmatrix} 1+\\delta  -(1+\\delta)/\\varepsilon \\\\ 0  1/\\varepsilon \\end{bmatrix} = \\begin{bmatrix} 1+\\delta  -(1+\\delta)/\\varepsilon + 1/\\varepsilon \\\\ 0  1 \\end{bmatrix} = \\begin{bmatrix} 1+\\delta  -\\delta/\\varepsilon \\\\ 0  1 \\end{bmatrix}\n$$\nSo, $A = \\begin{bmatrix} 1.0001  -10^{-4}/10^{-8} \\\\ 0  1 \\end{bmatrix} = \\begin{bmatrix} 1.0001  -10000 \\\\ 0  1 \\end{bmatrix}$.\n\n**2. Power Method Dynamics**\n\nThe power method iteration is $x_{k+1} = \\frac{A x_k}{\\|A x_k\\|_2}$. After $N$ iterations starting from $x_0$, the resulting vector is proportional to $A^N x_0$. Let the initial vector $x_0$ be decomposed in the eigenvector basis as $x_0 = c_1 s_1 + c_2 s_2$. Then:\n$$\nA^N x_0 = A^N (c_1 s_1 + c_2 s_2) = c_1 \\lambda_1^N s_1 + c_2 \\lambda_2^N s_2\n$$\nThe vector $A^N x_0$ can be rewritten as:\n$$\nA^N x_0 = \\lambda_2^N \\left( c_1 \\left(\\frac{\\lambda_1}{\\lambda_2}\\right)^N s_1 + c_2 s_2 \\right)\n$$\nThe direction of the iterated vector $x_N$ depends on the relative magnitudes of the two terms in the parenthesis. Convergence to the dominant eigenvector $s_1$ requires the first term to dominate the second. This occurs if the amplification factor $(\\lambda_1/\\lambda_2)^N$ is large enough to overcome a potentially small initial coefficient ratio $|c_1/c_2|$.\n\nIn this problem, the amplification factor is $(\\frac{1+\\delta}{1})^N = (1+10^{-4})^{1000}$. Using the approximation $(1+x)^n \\approx e^{nx}$ for small $x$, we have $(1+10^{-4})^{1000} \\approx e^{1000 \\cdot 10^{-4}} = e^{0.1} \\approx 1.10517$. This relatively small amplification factor means that the power method will converge very slowly.\n\n**3. Analysis of Test Cases**\n\nThe final output for each case is `True` if the computed Rayleigh quotient $\\rho(x_N) = x_N^\\top A x_N$ is closer to $\\lambda_1$ than to $\\lambda_2$. This is equivalent to checking if $\\rho(x_N) > (\\lambda_1 + \\lambda_2)/2 = 1 + \\delta/2$.\n\n- **Case 1**: $x_0^{(1)} = s_2$.\nThe initial vector is exactly the subdominant eigenvector. Here, $c_1=0$ and $c_2=1$. The power method iteration will remain in the eigenspace of $\\lambda_2$ for all steps: $x_k = s_2/\\|s_2\\|_2$ for all $k0$. Thus, $\\rho(x_N)$ will be equal to $\\lambda_2 = 1$ (up to floating point error). This is closer to $\\lambda_2$ than to $\\lambda_1$. The result is **False**.\n\n- **Case 2**: $x_0^{(2)} = s_2 + 10^{-30} s_1$.\nThe initial vector has coefficients $c_1 = 10^{-30}$ and $c_2 = 1$. The ratio of the component magnitudes after $N=1000$ iterations is:\n$$\n\\frac{\\|c_1 (\\lambda_1/\\lambda_2)^N s_1\\|}{\\|c_2 s_2\\|} \\approx \\frac{|c_1|}{|c_2|} \\left(\\frac{\\lambda_1}{\\lambda_2}\\right)^N \\approx 10^{-30} \\times 1.105 \\approx 1.105 \\times 10^{-30}\n$$\nThis ratio is extraordinarily small. The vector $x_{1000}$ will be numerically indistinguishable from $s_2$. The Rayleigh quotient will be extremely close to $\\lambda_2$. The result is **False**.\n\n- **Case 3**: $x_0^{(3)} = s_1$.\nThe initial vector is the dominant eigenvector, so $c_1=1$ and $c_2=0$. The iteration immediately yields the dominant eigenvector: $x_k = s_1/\\|s_1\\|_2$ for all $k$. The Rayleigh quotient $\\rho(x_N)$ will be equal to $\\lambda_1 = 1.0001$. This is closer to $\\lambda_1$. The result is **True**.\n\n- **Case 4**: $x_0^{(4)} = \\begin{bmatrix} \\varepsilon^{-1} + 1 \\\\ 1 \\end{bmatrix}$.\nTo find the coefficients $c_1, c_2$, we solve $x_0^{(4)} = c_1 s_1 + c_2 s_2$. This is equivalent to $\\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix} = S^{-1} x_0^{(4)}$.\n$$\n\\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix} = \\begin{bmatrix} 1  -1/\\varepsilon \\\\ 0  1/\\varepsilon \\end{bmatrix} \\begin{bmatrix} \\varepsilon^{-1} + 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} (\\varepsilon^{-1} + 1) - 1/\\varepsilon \\\\ 1/\\varepsilon \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1/\\varepsilon \\end{bmatrix}\n$$\nSo, $c_1 = 1$ and $c_2 = 1/\\varepsilon = 10^8$. The initial vector has a component along $s_2$ that is $10^8$ times larger than its component along $s_1$. The ratio of component magnitudes after $N=1000$ iterations is:\n$$\n\\frac{\\|c_1 (\\lambda_1/\\lambda_2)^N s_1\\|}{\\|c_2 s_2\\|} \\approx \\frac{|c_1|}{|c_2|} \\left(\\frac{\\lambda_1}{\\lambda_2}\\right)^N = \\varepsilon \\times (1+\\delta)^{1000} \\approx 10^{-8} \\times 1.105 \\approx 1.105 \\times 10^{-8}\n$$\nThis ratio is very small. Despite having a non-zero component in the dominant eigenvector direction, the iterate $x_{1000}$ is still overwhelmingly dominated by the subdominant eigenvector $s_2$ due to the slow convergence and the extreme initial imbalance. $\\rho(x_N)$ will be very close to $\\lambda_2$. The result is **False**.\n\nThe expected output is therefore `[False, False, True, False]`. This demonstrates that for nearly-defective matrices with eigenvalues close to each other, the power method can exhibit extremely slow convergence, appearing to converge to a subdominant eigenvector if the initial vector is not sufficiently aligned with the dominant one.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the power method problem for a nearly-defective matrix.\n    \"\"\"\n    # 1. Define constants and parameters from the problem statement.\n    epsilon = 1e-8\n    delta = 1e-4\n    lambda1 = 1.0 + delta\n    lambda2 = 1.0\n    N = 1000\n\n    # 2. Construct the matrix A.\n    # A = S J S^-1, where S = [[1, 1], [0, epsilon]] and J = diag(lambda1, lambda2).\n    # This simplifies to A = [[1+delta, -delta/epsilon], [0, 1]].\n    A = np.array([\n        [1.0 + delta, -delta / epsilon],\n        [0.0, 1.0]\n    ], dtype=np.float64)\n\n    # 3. Define the initial vectors for the test suite.\n    # Right eigenvectors s1 and s2\n    s1 = np.array([1.0, 0.0], dtype=np.float64)\n    s2 = np.array([1.0, epsilon], dtype=np.float64)\n\n    # Case 1: Exact subdominant eigenvector\n    x0_1 = s2\n    \n    # Case 2: Extremely close to subdominant eigenvector\n    x0_2 = s2 + 1e-30 * s1\n    \n    # Case 3: Exact dominant eigenvector\n    x0_3 = s1\n    \n    # Case 4: Nearly orthogonal to the left dominant eigenvector\n    x0_4 = np.array([1.0/epsilon + 1.0, 1.0], dtype=np.float64)\n\n    test_cases = [x0_1, x0_2, x0_3, x0_4]\n    \n    results = []\n\n    # 4. Iterate through each test case.\n    for x0 in test_cases:\n        # Check for zero vector, though not expected in these cases.\n        if np.linalg.norm(x0) == 0:\n            # Handle this unlikely edge case.\n            # A zero vector will remain zero, rho is undefined.\n            # The problem assumes x0 is non-zero.\n            # We can arbitrarily assign a result or raise an error.\n            # Let's assume it doesn't happen.\n            pass\n\n        x = x0.copy()\n\n        # 5. Implement the power method for N iterations.\n        # The normalization is part of the loop.\n        for _ in range(N):\n            y = A @ x\n            norm_y = np.linalg.norm(y)\n            if norm_y == 0: # Should not happen with this non-singular matrix A\n                x = np.zeros_like(x)\n                break\n            x = y / norm_y\n        \n        x_N = x\n\n        # 6. Compute the Rayleigh quotient for the final iterate x_N.\n        # rho(x) = (x.T * A * x) / (x.T * x)\n        # Since x_N is normalized, its L2 norm is 1, so x_N.T @ x_N = 1.\n        # rho(x_N) = x_N.T @ (A @ x_N)\n        rho = x_N.T @ A @ x_N\n        \n        # 7. Compare absolute differences to determine which eigenvalue is closer.\n        is_closer_to_lambda1 = abs(rho - lambda1)  abs(rho - lambda2)\n        results.append(is_closer_to_lambda1)\n\n    # 8. Format and print the final output as specified.\n    # str(bool) gives 'True' or 'False' with capital letters, as in the example.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3282259"}, {"introduction": "When dealing with large-scale eigenvalue problems, direct computation is often impossible, and simple iterations like the power method are inefficient. Krylov subspace methods provide a powerful alternative and form the bedrock of many modern numerical libraries. In this practice, you will implement the Arnoldi iteration, a fundamental Krylov method, to project a large matrix onto a smaller subspace. You will then compute the \"Ritz eigenvalues\" from this smaller system, gaining first-hand experience with the principles behind today's state-of-the-art eigenvalue solvers [@problem_id:3282272].", "problem": "Consider the algebraic eigenvalue problem: for a square matrix $A \\in \\mathbb{R}^{n \\times n}$, find scalars $\\lambda \\in \\mathbb{C}$ and nonzero vectors $x \\in \\mathbb{C}^n$ such that $A x = \\lambda x$. A projection-based approach to approximating eigenvalues proceeds by constructing a Krylov subspace of dimension $m$, defined as $K_m(A,b) = \\operatorname{span}\\{b, A b, A^2 b, \\dots, A^{m-1} b\\}$, where $b \\in \\mathbb{R}^n$ is a nonzero starting vector. Using Arnoldi iteration, one can build an orthonormal basis for $K_m(A,b)$ and then form a small upper Hessenberg matrix that represents the action of $A$ on this subspace. The eigenvalues of this small matrix, called Ritz eigenvalues, provide approximations to the true eigenvalues of $A$.\n\nTask: Starting from the definitions above, implement Arnoldi iteration to construct an orthonormal basis for $K_m(A,b)$ and the associated upper Hessenberg matrix, then compute the Ritz eigenvalues as approximations to the eigenvalues of $A$. For each test case below, measure the quality of approximation by computing the following quantity: for the set of Ritz eigenvalues $\\{r_i\\}$ and the set of true eigenvalues $\\{\\lambda_j\\}$ of $A$, return the single float\n$$\n\\max_i \\min_j \\left| r_i - \\lambda_j \\right|.\n$$\nThis quantity is the maximum, over all Ritz eigenvalues, of the nearest-eigenvalue absolute difference in the complex plane. Report this value for each test case.\n\nImplement the program with the following test suite of parameter values. Each test case is a triple $(A, b, m)$:\n\n- Test case $1$ (general non-symmetric, diagonalizable, \"happy path\"): $A$ is formed as $A = S D S^{-1}$ with\n$$\nS = \\begin{bmatrix}\n1  0.2  -0.1  0  0  0 \\\\\n0  1  0.3  0  0  0 \\\\\n0  0  1  0.4  0  0 \\\\\n0  0  0  1  0.5  0 \\\\\n0  0  0  0  1  0.6 \\\\\n0  0  0  0  0  1\n\\end{bmatrix},\\quad\nD = \\operatorname{diag}(5,4,3,2,1,-1),\n$$\n$b = [1,1,1,1,1,1]^T$, and $m = 4$.\n\n- Test case $2$ (boundary condition $m = 1$): $A$ is the tridiagonal matrix of size $5 \\times 5$ with $2$ on the diagonal and $-1$ on the sub- and super-diagonals,\n$$\nA = \\begin{bmatrix}\n2  -1  0  0  0 \\\\\n-1  2  -1  0  0 \\\\\n0  -1  2  -1  0 \\\\\n0  0  -1  2  -1 \\\\\n0  0  0  -1  2\n\\end{bmatrix},\n$$\n$b = [1,0,0,0,0]^T$, and $m = 1$.\n\n- Test case $3$ (defective matrix edge case): $A$ is the Jordan block of size $6 \\times 6$ with eigenvalue $3$, that is, $\\mathbf{A} = 3\\mathbf{I} + \\mathbf{J}$ where $J$ has $1$ on the superdiagonal and $0$ elsewhere,\n$$\nA = \\begin{bmatrix}\n3  1  0  0  0  0 \\\\\n0  3  1  0  0  0 \\\\\n0  0  3  1  0  0 \\\\\n0  0  0  3  1  0 \\\\\n0  0  0  0  3  1 \\\\\n0  0  0  0  0  3\n\\end{bmatrix},\n$$\n$b = [1,1,1,1,1,1]^T$, and $m = 3$.\n\n- Test case $4$ (full dimension $m = n$, symmetric case): $A$ is a symmetric $4 \\times 4$ matrix,\n$$\nA = \\begin{bmatrix}\n4  1  0  0 \\\\\n1  3  1  0 \\\\\n0  1  2  1 \\\\\n0  0  1  1\n\\end{bmatrix},\n$$\n$b = [1,1,1,1]^T$, and $m = 4$.\n\nYour program must compute the quantity $\\max_i \\min_j |r_i - \\lambda_j|$ for each test case and produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each value must be a float rounded to $8$ decimal places, with no additional text. For example, an output line has the format $[v_1,v_2,v_3,v_4]$ where each $v_k$ is the computed float for test case $k$ rounded to $8$ decimal places. No physical units or angles are involved, and all computations are purely numerical.", "solution": "The user has provided a valid problem statement from the field of numerical linear algebra. The task is to implement the Arnoldi iteration for approximating eigenvalues of a matrix and to evaluate the quality of these approximations for several test cases.\n\n### Introduction to Arnoldi Iteration\n\nThe algebraic eigenvalue problem seeks to find scalars $\\lambda$ and nonzero vectors $x$ for a given square matrix $A \\in \\mathbb{R}^{n \\times n}$ such that $A x = \\lambda x$. For large matrices, direct computation of all eigenvalues can be prohibitively expensive. Projection methods offer an efficient alternative by approximating the eigenvalues of $A$ with the eigenvalues of a much smaller matrix.\n\nThe Arnoldi iteration is a projection method that operates on a Krylov subspace. For a matrix $A$ and a starting vector $b$, the $m$-th Krylov subspace is defined as:\n$$\nK_m(A,b) = \\operatorname{span}\\{b, A b, A^2 b, \\dots, A^{m-1} b\\}\n$$\nThis subspace is of dimension at most $m$. The core idea is to find the best approximation to the eigenvalues of $A$ within this subspace. This is achieved by constructing an orthonormal basis for $K_m(A,b)$ and then representing the action of $A$ with respect to that basis.\n\n### The Algorithm\n\nThe Arnoldi iteration employs a modified Gram-Schmidt procedure to build an orthonormal basis $\\{q_1, q_2, \\dots, q_m\\}$ for the Krylov subspace $K_m(A,b)$. The basis vectors are stored as columns of a matrix $Q_m = [q_1, q_2, \\dots, q_m] \\in \\mathbb{R}^{n \\times m}$.\n\nThe algorithm proceeds as follows:\n\n1.  **Initialization**: Normalize the starting vector $b$ to obtain the first basis vector. Assuming $b \\neq 0$:\n    $$\n    q_1 = \\frac{b}{\\|b\\|_2}\n    $$\n2.  **Iteration**: For $k=1, 2, \\dots, m$:\n    a. Generate a new vector by applying $A$ to the latest basis vector:\n       $$\n       v = A q_k\n       $$\n    b. Orthogonalize $v$ against all previous basis vectors $q_1, \\dots, q_k$. The coefficients of this projection form the $k$-th column of the Hessenberg matrix $H_m$:\n       $$\n       \\text{for } j=1, \\dots, k: \\quad h_{j,k} = q_j^T v \\quad \\text{and} \\quad v \\leftarrow v - h_{j,k} q_j\n       $$\n    c. The norm of the resulting vector $v$ becomes the subdiagonal element of the Hessenberg matrix:\n       $$\n       h_{k+1, k} = \\|v\\|_2\n       $$\n    d. If $h_{k+1, k}$ is zero (or numerically close to zero), the algorithm has found an $A$-invariant subspace and terminates. This is known as \"breakdown\".\n    e. Normalize $v$ to get the next basis vector:\n       $$\n       q_{k+1} = \\frac{v}{h_{k+1, k}}\n       $$\n\nAfter $m$ steps (assuming no breakdown), this process yields the Arnoldi factorization:\n$$\nA Q_m = Q_{m+1} \\tilde{H}_m\n$$\nwhere $Q_m \\in \\mathbb{R}^{n \\times m}$ and $Q_{m+1} \\in \\mathbb{R}^{n \\times (m+1)}$ are matrices with orthonormal columns, and $\\tilde{H}_m \\in \\mathbb{R}^{(m+1) \\times m}$ is an upper Hessenberg matrix. The matrix $H_m \\in \\mathbb{R}^{m \\times m}$ is formed by the first $m$ rows of $\\tilde{H}_m$. This $H_m$ is the representation of $A$ projected onto the Krylov subspace, as $H_m = Q_m^T A Q_m$.\n\n### Ritz Eigenvalues and Error Metric\n\nThe eigenvalues of the small $m \\times m$ Hessenberg matrix $H_m$ are called the **Ritz eigenvalues**. These values, denoted $\\{r_i\\}_{i=1}^m$, serve as approximations to the true eigenvalues of $A$, which we denote $\\{\\lambda_j\\}_{j=1}^n$.\n\nTo measure the quality of this approximation, we compute the Hausdorff distance between the set of Ritz eigenvalues and the set of true eigenvalues. The specific metric is:\n$$\n\\max_i \\min_j \\left| r_i - \\lambda_j \\right|\n$$\nThis formula calculates, for each Ritz eigenvalue $r_i$, the absolute distance to the nearest true eigenvalue $\\lambda_j$. The final reported value is the maximum of these minimum distances.\n\n### Implementation Plan\n\nThe solution will be implemented as a Python program following these steps for each test case $(A, b, m)$:\n\n1.  **Construct** the matrix $A \\in \\mathbb{R}^{n \\times n}$ and the starting vector $b \\in \\mathbb{R}^n$ as specified.\n2.  **Compute True Eigenvalues**: Calculate the full set of eigenvalues $\\{\\lambda_j\\}$ of $A$ using a standard numerical library function.\n3.  **Perform Arnoldi Iteration**: Implement the algorithm described above to generate the $m \\times m$ upper Hessenberg matrix $H_m$. The implementation will handle potential breakdown by returning a smaller matrix if an invariant subspace is found before $m$ iterations are completed.\n4.  **Compute Ritz Eigenvalues**: Calculate the eigenvalues $\\{r_i\\}$ of the resulting matrix $H_m$.\n5.  **Calculate Metric**: Compute the error metric $\\max_i \\min_j |r_i - \\lambda_j|$ by finding the maximum of the minimum distances between the two sets of eigenvalues in the complex plane.\n6.  **Format Output**: Collect the metric for each test case and format the final output as a comma-separated list of floating-point numbers rounded to $8$ decimal places, enclosed in brackets.\n\nAll matrix and vector operations will be performed using the `numpy` library. Since the input matrices and vectors are real, the Arnoldi iteration will be performed using real arithmetic. The resulting eigenvalues may be complex, which `numpy`'s eigenvalue solvers handle automatically.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef arnoldi_iteration(A, b, m):\n    \"\"\"\n    Performs Arnoldi iteration to find the Hessenberg matrix H.\n\n    Args:\n        A (np.ndarray): The square matrix.\n        b (np.ndarray): The starting vector.\n        m (int): The dimension of the Krylov subspace.\n\n    Returns:\n        np.ndarray: The m x m Hessenberg matrix H_m.\n    \"\"\"\n    n = A.shape[0]\n    \n    # Use real types since A and b are real. Eigenvalues may be complex later.\n    H = np.zeros((m + 1, m), dtype=np.float64)\n    Q = np.zeros((n, m + 1), dtype=np.float64)\n\n    Q[:, 0] = b / np.linalg.norm(b)\n\n    for k in range(m):\n        v = A @ Q[:, k]\n        for j in range(k + 1):\n            H[j, k] = np.dot(Q[:, j].T, v)\n            v = v - H[j, k] * Q[:, j]\n\n        h_next_k = np.linalg.norm(v)\n\n        # Handle breakdown: invariant subspace found\n        if h_next_k  1e-12:\n            return H[:k + 1, :k + 1]\n\n        H[k + 1, k] = h_next_k\n        Q[:, k + 1] = v / h_next_k\n        \n    return H[:m, :m]\n\ndef calculate_approximation_error(ritz_vals, true_vals):\n    \"\"\"\n    Calculates the approximation error metric max_i min_j |r_i - lambda_j|.\n\n    Args:\n        ritz_vals (np.ndarray): Array of Ritz eigenvalues.\n        true_vals (np.ndarray): Array of true eigenvalues.\n\n    Returns:\n        float: The calculated error metric.\n    \"\"\"\n    if ritz_vals.size == 0:\n        return 0.0\n\n    # Reshape for broadcasting\n    ritz_vals_col = ritz_vals[:, np.newaxis]\n    \n    # Calculate the matrix of absolute differences\n    abs_diff_matrix = np.abs(ritz_vals_col - true_vals)\n    \n    # Find the minimum distance for each Ritz value to any true eigenvalue\n    min_dists = np.min(abs_diff_matrix, axis=1)\n    \n    # The error is the maximum of these minimum distances\n    error = np.max(min_dists)\n    \n    return error\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    # Test case 1\n    S1 = np.array([\n        [1, 0.2, -0.1, 0, 0, 0],\n        [0, 1, 0.3, 0, 0, 0],\n        [0, 0, 1, 0.4, 0, 0],\n        [0, 0, 0, 1, 0.5, 0],\n        [0, 0, 0, 0, 1, 0.6],\n        [0, 0, 0, 0, 0, 1]\n    ], dtype=np.float64)\n    D1 = np.diag([5, 4, 3, 2, 1, -1])\n    A1 = S1 @ D1 @ np.linalg.inv(S1)\n    b1 = np.ones(6)\n    m1 = 4\n\n    # Test case 2\n    n2 = 5\n    A2 = np.diag(2 * np.ones(n2)) + np.diag(-1 * np.ones(n2 - 1), k=1) + np.diag(-1 * np.ones(n2 - 1), k=-1)\n    b2 = np.zeros(n2)\n    b2[0] = 1\n    m2 = 1\n\n    # Test case 3\n    n3 = 6\n    A3 = 3 * np.eye(n3) + np.diag(np.ones(n3 - 1), k=1)\n    b3 = np.ones(n3)\n    m3 = 3\n\n    # Test case 4\n    A4 = np.array([\n        [4, 1, 0, 0],\n        [1, 3, 1, 0],\n        [0, 1, 2, 1],\n        [0, 0, 1, 1]\n    ], dtype=np.float64)\n    b4 = np.ones(4)\n    m4 = 4\n\n    test_cases = [\n        (A1, b1, m1),\n        (A2, b2, m2),\n        (A3, b3, m3),\n        (A4, b4, m4)\n    ]\n\n    results = []\n    for A, b, m in test_cases:\n        # Step 1: Compute true eigenvalues\n        true_eigenvalues = np.linalg.eigvals(A)\n        \n        # Step 2: Perform Arnoldi iteration\n        H_m = arnoldi_iteration(A, b, m)\n        \n        # Step 3: Compute Ritz eigenvalues\n        ritz_eigenvalues = np.linalg.eigvals(H_m)\n        \n        # Step 4: Calculate the error metric\n        error = calculate_approximation_error(ritz_eigenvalues, true_eigenvalues)\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.8f}' for r in results])}]\")\n\nsolve()\n```", "id": "3282272"}, {"introduction": "One of the key themes in numerical analysis is that exploiting special matrix structures can lead to exceptionally efficient and robust algorithms. This exercise focuses on symmetric tridiagonal matrices, which appear frequently in scientific and engineering applications. You will implement a powerful solver that combines the bisection method with the Sturm sequence property, an elegant theoretical result that allows you to count the number of eigenvalues in any interval. This practice demonstrates how a deep theoretical understanding can be translated into a highly practical and reliable computational tool [@problem_id:3282393].", "problem": "Implement a complete, runnable program that solves the following task from first principles in the algebraic eigenvalue problem for symmetric tridiagonal matrices. The algebraic eigenvalue problem asks for the scalars $\\lambda$ such that a square matrix $A$ satisfies $A v = \\lambda v$ for some nonzero vector $v$. For a real symmetric tridiagonal matrix $A \\in \\mathbb{R}^{n \\times n}$ with diagonal entries $\\{d_i\\}_{i=1}^n$ and sub/super-diagonal entries $\\{e_i\\}_{i=1}^{n-1}$, the eigenvalues are real. The fundamental base for this problem is the definition of eigenvalues as the roots of the characteristic equation $\\det(A - x I) = 0$, the properties of leading principal minors, and the inertia interpretation via symmetric elimination leading to an $L D L^\\top$ factorization without pivoting for $A - x I$. The Sturm sequence property states that the number of eigenvalues less than or equal to a scalar $x$ equals the number of sign changes in a sequence derived from the leading principal minors (or, equivalently, the number of negative diagonal pivots in the $L D L^\\top$ factorization of $A - x I$). Use these foundations to design your algorithm logically.\n\nYour program must:\n- Construct, for each test case, the symmetric tridiagonal matrix $A$ from given $\\{d_i\\}$ and $\\{e_i\\}$ (implicitly; do not form dense matrices if not needed).\n- Implement a function that, given a real scalar $x$, returns the count of eigenvalues of $A$ that are less than or equal to $x$ by evaluating the Sturm sequence count implied by the $L D L^\\top$ factorization of $A - x I$ without pivoting.\n- Use the monotonicity of the count function in $x$ together with the bisection method to approximate all eigenvalues of $A$ in a given closed interval $[a,b]$, inclusive, to within an absolute tolerance $\\tau$. The bisection routine must locate each eigenvalue individually by comparing target indices against the count at midpoints.\n- Handle multiplicities correctly: if an eigenvalue has algebraic multiplicity greater than $1$, it must appear that many times in the output list for the corresponding test case.\n- Do not use any dense eigenvalue solver or black-box root finders.\n\nNumerical requirements:\n- Absolute tolerance must be $\\tau = 10^{-10}$.\n- The interval $[a,b]$ is closed: include any eigenvalue $\\lambda$ satisfying $a \\le \\lambda \\le b$.\n- Angles are not involved; no angle units are required.\n- There are no physical units involved.\n\nTest suite:\nFor each test case below, use the given sequences $\\{d_i\\}$ and $\\{e_i\\}$ and interval $[a,b]$. The program must compute, for each case, the sorted list (in nondecreasing order) of all eigenvalues of $A$ in $[a,b]$, each approximated to absolute error at most $\\tau$.\n\n- Test case $1$ (happy path, multiple distinct interior eigenvalues):\n  - $n = 3$, $d = [2,2,2]$, $e = [-1,-1]$, interval $[a,b] = [0.5, 3.5]$.\n- Test case $2$ (boundary inclusion and multiple eigenvalues in the interval):\n  - $n = 5$, $d = [2,2,2,2,2]$, $e = [-1,-1,-1,-1]$, interval $[a,b] = [0.0, 1.0]$.\n- Test case $3$ (interval with no eigenvalues):\n  - $n = 4$, $d = [0.0, 1.0, 0.0, 1.0]$, $e = [0.1, 0.1, 0.1]$, interval $[a,b] = [10.0, 11.0]$.\n- Test case $4$ (minimal dimension):\n  - $n = 1$, $d = [3.14159]$, $e = []$, interval $[a,b] = [3.14159, 3.14159]$.\n- Test case $5$ (multiplicity due to zero off-diagonal, closed interval at a repeated eigenvalue):\n  - $n = 3$, $d = [1.0, 1.0, 2.0]$, $e = [0.0, 0.0]$, interval $[a,b] = [1.0, 1.0]$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of lists enclosed in square brackets, where each inner list corresponds to a test case and contains the eigenvalues in $[a,b]$ in nondecreasing order. Represent each eigenvalue rounded to exactly $10$ decimal places. For example, a valid output looks like $[[x_{1,1},x_{1,2}],[x_{2,1}],[]]$ with each $x_{i,j}$ shown to $10$ decimal places.\n\nDo not read any input; all data are specified above and must be embedded in the program. The program must be complete and runnable as given.", "solution": "The problem requires the implementation of an algorithm to find all eigenvalues of a real symmetric tridiagonal matrix $A \\in \\mathbb{R}^{n \\times n}$ within a given closed interval $[a,b]$. The algorithm must be designed from first principles, based on the Sturm sequence property and the bisection method, without recourse to black-box eigenvalue solvers.\n\nA real symmetric tridiagonal matrix $A$ is defined by its diagonal entries, $\\{d_i\\}_{i=1}^n$, and its off-diagonal entries, $\\{e_i\\}_{i=1}^{n-1}$:\n$$\nA = \\begin{pmatrix}\nd_1  e_1    \\\\\ne_1  d_2  e_2   \\\\\n \\ddots  \\ddots  \\ddots  \\\\\n  e_{n-2}  d_{n-1}  e_{n-1} \\\\\n   e_{n-1}  d_n\n\\end{pmatrix}\n$$\nThe eigenvalues $\\lambda$ are the roots of the characteristic equation $\\det(A - xI) = 0$. The core of the specified method is to determine, for any real scalar $x$, the number of eigenvalues of $A$ that are less than or equal to $x$. Let this count be denoted by $\\nu_{\\le}(x)$. The problem statement posits that this count can be found via the Sturm sequence property, which relates $\\nu_{\\le}(x)$ to the pivots of the $LDL^\\top$ factorization of the shifted matrix $A - xI$.\n\nLet us formalize this. Sylvester's Law of Inertia states that the number of positive, negative, and zero eigenvalues of a symmetric matrix $B$ is invariant under a congruence transformation $C B C^\\top$. Gaussian elimination without pivoting on $B = A-xI$ produces an $LDL^\\top$ factorization, where $L$ is unit lower-bidiagonal and $D$ is a diagonal matrix of pivots, $\\{\\delta_k\\}_{k=1}^n$. The law of inertia implies that the number of negative eigenvalues of $A-xI$ is equal to the number of negative pivots $\\delta_k$ in $D$. An eigenvalue of $A-xI$ is of the form $\\lambda_j - x$. Thus, the number of negative pivots equals the number of eigenvalues $\\lambda_j$ of $A$ such that $\\lambda_j - x  0$, which is the count of eigenvalues strictly less than $x$, denoted $\\nu_{}(x)$.\n\nThe problem statement specifies using a function that counts eigenvalues \"less than or equal to\" $x$. To align with this, we interpret the \"number of negative pivots\" criterion to mean the number of non-positive pivots ($\\delta_k \\le 0$). This will give a count $\\nu_{\\le}(x)$, which includes any eigenvalue exactly equal to $x$. The pivots are computed via the recurrence relation:\n$$\n\\delta_1 = d_1 - x\n$$\n$$\n\\delta_k = (d_k - x) - \\frac{e_{k-1}^2}{\\delta_{k-1}} \\quad \\text{for } k = 2, \\dots, n\n$$\nThis recurrence fails if a pivot $\\delta_{k-1}$ is zero. This occurs when $x$ is an eigenvalue of the leading principal submatrix $A_{k-1}$. In such a case, the term $e_{k-1}^2/\\delta_{k-1}$ becomes infinite. For an irreducible tridiagonal matrix ($e_i \\neq 0$ for all $i$), if $\\delta_{k-1} \\to 0$, then $\\delta_k$ tends to $-\\infty$ or $+\\infty$. A robust numerical implementation treats a zero pivot $\\delta_{k-1}$ as a very small number, which forces $\\delta_k$ to be a very large negative number (since $e_{k-1}^20$). Thus, a zero pivot $\\delta_{k-1}$ leads to $\\delta_k$ being counted as non-positive. This handling makes the counting function robust.\n\nIf any off-diagonal entry $e_i$ is zero, the matrix $A$ is block-diagonal. The set of eigenvalues of $A$ is the union of the eigenvalues of its diagonal blocks. The counting function $\\nu_{\\le}(x)$ can thus be computed by summing the counts for each irreducible tridiagonal sub-block.\n\nWith a robust function for $\\nu_{\\le}(x)$, we can find any specified eigenvalue $\\lambda_k$ (where $\\lambda_1 \\le \\lambda_2 \\le \\dots \\le \\lambda_n$) using the bisection method. The $k$-th eigenvalue, $\\lambda_k$, is the smallest value $x$ for which $\\nu_{\\le}(x) \\ge k$. The bisection algorithm searches for $\\lambda_k$ within a search interval $[low, high]$ by repeatedly testing the midpoint `mid`:\n- If $\\nu_{\\le}(\\text{mid}) \\ge k$, then $\\lambda_k \\le \\text{mid}$, so we update `high = mid`.\n- If $\\nu_{\\le}(\\text{mid})  k$, then $\\lambda_k  \\text{mid}$, so we update `low = mid`.\nThis process continues until `high - low` is less than the desired tolerance $\\tau$.\n\nTo find all eigenvalues, we first establish an interval $[g_{\\min}, g_{\\max}]$ that is guaranteed to contain all eigenvalues using Gerschgorin's Circle Theorem. For a symmetric tridiagonal matrix, this interval is the union of $[d_i - r_i, d_i + r_i]$ for $i=1,\\dots,n$, where $r_1 = |e_1|$, $r_n = |e_{n-1}|$, and $r_i = |e_{i-1}| + |e_i|$ for $i=2,\\dots,n-1$.\n\nThe overall algorithm is as follows:\n1.  For a given matrix defined by $\\{d_i\\}$ and $\\{e_i\\}$, compute the Gerschgorin bounds $[g_{\\min}, g_{\\max}]$.\n2.  Iterate from $k=1$ to $n$ to find each eigenvalue $\\lambda_k$.\n3.  For each $k$, use the bisection method on the interval $[g_{\\min}, g_{\\max}]$ (or a progressively tightened interval, e.g., $[\\lambda_{k-1}, g_{\\max}]$) to find an approximation of $\\lambda_k$ to within tolerance $\\tau$.\n4.  After computing all $n$ eigenvalues, filter the resulting list to retain only those that fall within the specified problem interval $[a,b]$.\n5.  Collect and format the results for all test cases as required.\n\nThis strategy correctly handles eigenvalue multiplicities because the bisection search for $\\lambda_k$ and $\\lambda_{k+1}$ will both converge to the same value if they are equal. It is robust and builds directly upon the specified first principles.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function that processes all test cases specified in the problem.\n    \"\"\"\n    \n    # --- Core Algorithmic Components ---\n\n    def _count_le_irreducible(x, d, e):\n        \"\"\"\n        Counts eigenvalues = x for an irreducible symmetric tridiagonal matrix.\n        This is the core of the Sturm sequence method, using the pivot recurrence\n        from LDL^T factorization of (A - xI).\n        \"\"\"\n        n = len(d)\n        if n == 0:\n            return 0\n        \n        count = 0\n        delta = 1.0  # Initial value, will be updated in the first iteration.\n        \n        for i in range(n):\n            diag_shifted = d[i] - x\n            if i == 0:\n                delta = diag_shifted\n            else:\n                # This check handles the case where a pivot is zero.\n                # If delta_{k-1} is 0, the next pivot delta_k tends to -infinity,\n                # ensuring it's counted as non-positive.\n                # We use a large negative number to simulate this robustly.\n                if delta == 0.0:\n                    delta = -1e30 \n                else:\n                    delta = diag_shifted - (e[i - 1] ** 2) / delta\n            \n            if delta = 0.0:\n                count += 1\n        return count\n\n    def count_le(x, d, e):\n        \"\"\"\n        Counts eigenvalues = x for a general symmetric tridiagonal matrix.\n        Handles block-diagonal structure by splitting at zero off-diagonal elements.\n        \"\"\"\n        n = len(d)\n        if n == 0:\n            return 0\n        \n        total_count = 0\n        start_idx = 0\n        for i in range(n - 1):\n            if e[i] == 0.0:\n                # Process the block from start_idx up to i.\n                total_count += _count_le_irreducible(x, d[start_idx:i + 1], e[start_idx:i])\n                start_idx = i + 1\n        \n        # Process the final block.\n        total_count += _count_le_irreducible(x, d[start_idx:], e[start_idx:n - 1])\n        return total_count\n\n    def get_gerschgorin_bounds(d, e):\n        \"\"\"\n        Computes an interval [g_min, g_max] that contains all eigenvalues.\n        \"\"\"\n        n = len(d)\n        if n == 0:\n            return 0.0, 0.0\n        if n == 1:\n            return d[0], d[0]\n\n        g_min = float('inf')\n        g_max = float('-inf')\n\n        for i in range(n):\n            radius = 0.0\n            if i  0:\n                radius += abs(e[i - 1])\n            if i  n - 1:\n                radius += abs(e[i])\n            \n            g_min = min(g_min, d[i] - radius)\n            g_max = max(g_max, d[i] + radius)\n            \n        return g_min, g_max\n\n    def find_all_eigenvalues(d, e, tol=1e-10):\n        \"\"\"\n        Finds all eigenvalues of the matrix using bisection and the count_le function.\n        \"\"\"\n        n = len(d)\n        if n == 0:\n            return []\n\n        g_min, g_max = get_gerschgorin_bounds(d, e)\n        eigenvalues = []\n        \n        search_low = g_min\n        for k in range(1, n + 1):\n            low = search_low\n            high = g_max\n\n            # Bisection to find the k-th eigenvalue\n            while high - low  tol:\n                mid = low + (high - low) / 2\n                if mid == low or mid == high: # Reached precision limit\n                    break\n                \n                # count_le(mid) = k means lambda_k is in [low, mid]\n                if count_le(mid, d, e) = k:\n                    high = mid\n                else:\n                    low = mid\n            \n            eig_k = (low + high) / 2\n            eigenvalues.append(eig_k)\n            # Optimization: The next eigenvalue must be = the current one.\n            search_low = eig_k\n\n        return eigenvalues\n\n    # --- Test Suite and Main Execution Logic ---\n    \n    test_cases = [\n        # Test case 1\n        {\"d\": [2.0, 2.0, 2.0], \"e\": [-1.0, -1.0], \"interval\": [0.5, 3.5]},\n        # Test case 2\n        {\"d\": [2.0, 2.0, 2.0, 2.0, 2.0], \"e\": [-1.0, -1.0, -1.0, -1.0], \"interval\": [0.0, 1.0]},\n        # Test case 3\n        {\"d\": [0.0, 1.0, 0.0, 1.0], \"e\": [0.1, 0.1, 0.1], \"interval\": [10.0, 11.0]},\n        # Test case 4\n        {\"d\": [3.14159], \"e\": [], \"interval\": [3.14159, 3.14159]},\n        # Test case 5\n        {\"d\": [1.0, 1.0, 2.0], \"e\": [0.0, 0.0], \"interval\": [1.0, 1.0]},\n    ]\n    \n    TOLERANCE = 1e-10\n    all_results = []\n\n    for case in test_cases:\n        d_vec = case[\"d\"]\n        e_vec = case[\"e\"]\n        a, b = case[\"interval\"]\n\n        all_eigs = find_all_eigenvalues(d_vec, e_vec, TOLERANCE)\n        \n        # Filter eigenvalues to be within the specified interval [a, b].\n        # A small margin related to tolerance is used for robust endpoint inclusion.\n        filtered_eigs = [eig for eig in all_eigs if a - TOLERANCE = eig = b + TOLERANCE]\n        \n        all_results.append(filtered_eigs)\n\n    # --- Final Output Formatting ---\n    \n    output_str = \"[\"\n    for i, res_list in enumerate(all_results):\n        formatted_list = [f\"{val:.10f}\" for val in res_list]\n        output_str += f\"[{','.join(formatted_list)}]\"\n        if i  len(all_results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    print(output_str)\n\nsolve()\n```", "id": "3282393"}]}