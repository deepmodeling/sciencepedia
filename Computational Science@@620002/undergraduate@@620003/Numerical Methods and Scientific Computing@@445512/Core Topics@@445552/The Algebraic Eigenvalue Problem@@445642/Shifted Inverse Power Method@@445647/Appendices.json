{"hands_on_practices": [{"introduction": "The power of the shifted inverse power method lies in its ability to target specific eigenvalues. However, its behavior is governed by a simple, rigid rule: for a given shift $\\sigma$, the iteration converges to the eigenpair whose eigenvalue $\\lambda$ minimizes the distance $|\\lambda - \\sigma|$. This practice [@problem_id:3273240] is a purely analytical exercise designed to drive this principle home, demonstrating how an intuitively chosen shift, perhaps based on a diagonal entry, can sometimes lead to convergence towards an unexpected and seemingly distant eigenvalue.", "problem": "Consider the real symmetric matrix\n$$\nA=\\begin{pmatrix}\n-9  0  0\\\\\n0  \\tfrac{137}{50}  \\tfrac{42}{25}\\\\\n0  \\tfrac{42}{25}  \\tfrac{44}{25}\n\\end{pmatrix}.\n$$\nAn engineer, intending to target an interior eigenvalue near the middle of the spectrum, chooses the shift $\\sigma=1.8$ based on inspecting the third diagonal entry, $1.76$, as a plausible choice. Starting from a generic initial vector with nonzero components along all eigenvectors, the shifted inverse power method applies repeated solves with $(A-\\sigma I)$ followed by normalization.\n\nUsing only foundational definitions and properties of eigenvalues and eigenvectors, determine the single eigenvalue of $A$ to which the shifted inverse power method will converge for almost all initial vectors under this choice of $\\sigma$. Give your answer as a single real number. No numerical iteration is to be performed in your reasoning, and no rounding of the final answer is required.", "solution": "The problem asks to identify the eigenvalue of the matrix $A$ to which the shifted inverse power method will converge, given the shift $\\sigma=1.8$.\n\nThe shifted inverse power method is an iterative algorithm for finding an eigenvalue-eigenvector pair $(\\lambda, v)$ of a matrix $A$. The iteration is given by $x_{k+1} = (A - \\sigma I)^{-1} x_k$, where $\\sigma$ is a chosen shift. This is equivalent to applying the power method to the matrix $M = (A - \\sigma I)^{-1}$. The power method converges to the eigenvector corresponding to the eigenvalue of $M$ with the largest magnitude.\n\nLet the eigenvalues of $A$ be $\\lambda_i$. The eigenvalues of $M = (A - \\sigma I)^{-1}$ are $\\mu_i = \\frac{1}{\\lambda_i - \\sigma}$. The algorithm will converge to the eigenvector associated with the eigenvalue $\\mu_j$ such that $|\\mu_j| = \\max_i |\\mu_i|$. This condition is equivalent to finding the eigenvalue $\\lambda_j$ of $A$ that minimizes the distance to the shift $\\sigma$ in the complex plane, i.e., $|\\lambda_j - \\sigma| = \\min_i |\\lambda_i - \\sigma|$.\n\nFor a generic initial vector (one with non-zero components in the direction of all eigenvectors), the method converges to the eigenvalue of $A$ that is closest to the shift $\\sigma$. Therefore, the task reduces to:\n1. Finding all eigenvalues of the matrix $A$.\n2. Calculating the distance $|\\lambda_i - \\sigma|$ for each eigenvalue $\\lambda_i$ and the given shift $\\sigma=1.8$.\n3. Identifying the eigenvalue $\\lambda_j$ for which this distance is minimized.\n\nFirst, we find the eigenvalues of the matrix $A$:\n$$\nA=\\begin{pmatrix}\n-9  0  0\\\\\n0  \\tfrac{137}{50}  \\tfrac{42}{25}\\\\\n0  \\tfrac{42}{25}  \\tfrac{44}{25}\n\\end{pmatrix}\n$$\nThe matrix $A$ is block-diagonal. Its eigenvalues are the eigenvalues of its diagonal blocks. The first block is the $1 \\times 1$ matrix $(-9)$, so one eigenvalue is immediately identified as $\\lambda_1 = -9$.\n\nThe remaining two eigenvalues are the eigenvalues of the $2 \\times 2$ submatrix:\n$$\nB = \\begin{pmatrix}\n\\tfrac{137}{50}  \\tfrac{42}{25}\\\\\n\\tfrac{42}{25}  \\tfrac{44}{25}\n\\end{pmatrix}\n$$\nTo simplify the calculation, we can factor out $\\frac{1}{50}$ by rewriting the matrix with a common denominator:\n$$\nB = \\begin{pmatrix}\n\\tfrac{137}{50}  \\tfrac{84}{50}\\\\\n\\tfrac{84}{50}  \\tfrac{88}{50}\n\\end{pmatrix} = \\frac{1}{50} \\begin{pmatrix}\n137  84\\\\\n84  88\n\\end{pmatrix}\n$$\nLet the matrix be $B' = \\begin{pmatrix} 137  84 \\\\ 84  88 \\end{pmatrix}$. If $\\mu$ is an eigenvalue of $B'$, then $\\lambda = \\frac{\\mu}{50}$ is an eigenvalue of $B$. The eigenvalues of $B'$ are the roots of its characteristic equation $\\det(B' - \\mu I) = 0$, which is $\\mu^2 - \\text{tr}(B')\\mu + \\det(B') = 0$.\n\nThe trace of $B'$ is:\n$$\n\\text{tr}(B') = 137 + 88 = 225\n$$\nThe determinant of $B'$ is:\n$$\n\\det(B') = (137)(88) - (84)(84) = 12056 - 7056 = 5000\n$$\nThe characteristic equation for $\\mu$ is:\n$$\n\\mu^2 - 225\\mu + 5000 = 0\n$$\nWe solve this quadratic equation for $\\mu$:\n$$\n\\mu = \\frac{225 \\pm \\sqrt{(-225)^2 - 4(1)(5000)}}{2} = \\frac{225 \\pm \\sqrt{50625 - 20000}}{2} = \\frac{225 \\pm \\sqrt{30625}}{2}\n$$\nThe square root is $\\sqrt{30625} = 175$. Thus, the eigenvalues of $B'$ are:\n$$\n\\mu_2 = \\frac{225 + 175}{2} = \\frac{400}{2} = 200\n$$\n$$\n\\mu_3 = \\frac{225 - 175}{2} = \\frac{50}{2} = 25\n$$\nThe eigenvalues of the submatrix $B$ are then:\n$$\n\\lambda_2 = \\frac{\\mu_2}{50} = \\frac{200}{50} = 4\n$$\n$$\n\\lambda_3 = \\frac{\\mu_3}{50} = \\frac{25}{50} = \\frac{1}{2} = 0.5\n$$\nSo, the set of eigenvalues of the matrix $A$ is $\\{\\lambda_1, \\lambda_2, \\lambda_3\\} = \\{-9, 4, 0.5\\}$.\n\nNext, we calculate the distance from each eigenvalue to the shift $\\sigma = 1.8$.\n1. For $\\lambda_1 = -9$:\n   $$\n   |\\lambda_1 - \\sigma| = |-9 - 1.8| = |-10.8| = 10.8\n   $$\n2. For $\\lambda_2 = 4$:\n   $$\n   |\\lambda_2 - \\sigma| = |4 - 1.8| = |2.2| = 2.2\n   $$\n3. For $\\lambda_3 = 0.5$:\n   $$\n   |\\lambda_3 - \\sigma| = |0.5 - 1.8| = |-1.3| = 1.3\n   $$\nComparing these distances, we find that $1.3  2.2  10.8$. The smallest distance is $1.3$, which corresponds to the eigenvalue $\\lambda_3 = 0.5$.\n\nTherefore, for almost all initial vectors, the shifted inverse power method with the shift $\\sigma = 1.8$ will converge to the eigenvalue $\\lambda_3 = 0.5.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "3273240"}, {"introduction": "Moving from theory to practical application, a common use case for the shifted inverse power method is to verify a specific eigenvalue produced by another algorithm. The natural choice for the shift $\\sigma$ is the claimed eigenvalue itself, but this presents a critical challenge: the matrix $(A - \\sigma I)$ becomes singular. This hands-on coding exercise [@problem_id:3273273] guides you through implementing a numerically robust solution using techniques like Tikhonov regularization, a crucial skill for any computational scientist building reliable numerical tools.", "problem": "You are given the task of verifying whether a single claimed eigenvalue from a full-spectrum solver is truly an eigenvalue of a given real matrix by using the shifted inverse power method with the shift equal to the claimed value. The foundational base consists of the following: by definition, a scalar $\\lambda$ is an eigenvalue of a square matrix $A \\in \\mathbb{R}^{n \\times n}$ if and only if there exists a nonzero vector $x \\in \\mathbb{R}^{n}$ such that $A x = \\lambda x$. The verification will be based on constructing an iterative process that, when the shift equals the true eigenvalue, produces a vector whose residual satisfies $A x - \\lambda x \\approx 0$ to within a prescribed tolerance. If the shift equals the true eigenvalue, the operator $A - \\lambda I$ is singular, so a numerically sound scheme must be used to define and solve the linear systems that appear.\n\nDesign and implement a program using the shifted inverse power method with shift $\\sigma = \\lambda_{\\text{test}}$ to verify $\\lambda_{\\text{test}}$ for several matrices. Your program must follow these specifications:\n\n- For each test case, start from a deterministic initial vector $x_{0}$ constructed by using a fixed pseudorandom number generator seed and normalizing the resulting vector to unit $2$-norm.\n- At iteration index $k \\ge 1$, solve a linear system of the form $(A - \\sigma I) y_{k} = x_{k-1}$ for $y_{k}$, then set $x_{k} = y_{k} / \\lVert y_{k} \\rVert_{2}$. When $(A - \\sigma I)$ is singular or numerically unstable, you must still define $y_{k}$ in a stable way by using any of the following admissible mechanisms: (i) Tikhonov-type regularization, i.e., solve $\\big((A - \\sigma I) + \\mu I\\big) y_{k} = x_{k-1}$ for a small $\\mu  0$, or (ii) a least-squares solution computed via a numerically stable method such as the Singular Value Decomposition (SVD). Your implementation may choose adaptively between these mechanisms to ensure progress.\n- At each iteration, compute the Rayleigh quotient $\\rho_{k} = \\dfrac{x_{k}^{\\top} A x_{k}}{x_{k}^{\\top} x_{k}}$ and the residuals $r_{\\sigma,k} = \\lVert A x_{k} - \\sigma x_{k} \\rVert_{2}$ and $r_{\\rho,k} = \\lVert A x_{k} - \\rho_{k} x_{k} \\rVert_{2}$.\n- Terminate when either $r_{\\sigma,k} \\le \\tau_{\\text{res}}$ or a maximum of $k_{\\max}$ iterations is reached. Additionally, report the final $r_{\\sigma,k}$ and the last Rayleigh quotient $\\rho_{k}$.\n\nUse the following constants across all tests: $\\tau_{\\text{res}} = 10^{-8}$, an absolute Rayleigh-quotient proximity tolerance $\\tau_{\\text{rq}} = 10^{-8}$ for optional cross-checking, and $k_{\\max} = 30$. When regularization is needed, you may choose a baseline $\\mu = 10^{-8} \\max(1,\\lVert A \\rVert_{F})$ and adjust it if necessary to achieve a successful solve. The initial vector must be created by drawing entries from a standard normal distribution with a fixed seed and normalized to have unit $2$-norm.\n\nVerification rule: declare $\\lambda_{\\text{test}}$ as verified if and only if the final $r_{\\sigma,k}$ at termination is less than or equal to $\\tau_{\\text{res}}$. You may optionally require that $\\lvert \\rho_{k} - \\sigma \\rvert \\le \\tau_{\\text{rq}}$ as a secondary check; if you use this secondary check, both conditions must hold.\n\nTest suite. For each test case, you are provided $(A, \\lambda_{\\text{test}})$:\n\n- Test $1$: $A_{1} = \\begin{bmatrix} 2  1  0 \\\\ 1  2  1 \\\\ 0  1  2 \\end{bmatrix}$, $\\lambda_{\\text{test},1} = 2$.\n- Test $2$: $A_{2} = \\begin{bmatrix} 1  2  3 \\\\ 0  4  5 \\\\ 0  0  6 \\end{bmatrix}$, $\\lambda_{\\text{test},2} = 4$.\n- Test $3$: $A_{3} = A_{1}$ from Test $1$, $\\lambda_{\\text{test},3} = 2.5$.\n- Test $4$: $A_{4} = \\operatorname{diag}(1.0, 1.001, 10.0)$, $\\lambda_{\\text{test},4} = 1.001$.\n\nFor each test case, your program must run the described verification process and produce one result per test case containing:\n- A boolean stating whether $\\lambda_{\\text{test}}$ is verified according to the rule above.\n- The final Rayleigh quotient $\\rho_{k}$ as a floating-point number.\n- The final residual $r_{\\sigma,k}$ as a floating-point number.\n- The number of iterations performed as an integer.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each individual test result appears as a list in the form $[\\text{boolean}, \\rho_{k}, r_{\\sigma,k}, k]$. For example, an output with two hypothetical results would look like $[[\\text{True},1.0,1\\mathrm{e}{-9},7],[\\text{False},0.0,1.23,30]]$. There are no physical units and no angles in this problem; all numerical values are real numbers in standard floating-point format. The program must not read any input and must execute deterministically.", "solution": "The user wants to solve a problem. The user wants me to act as a meticulous and exacting professor in the STEM fields. I need to validate the problem statement first.\n\n## Problem Validation\n\n### Step 1: Extract Givens\n- **Task**: Verify a claimed eigenvalue $\\lambda_{\\text{test}}$ of a real matrix $A$ using the shifted inverse power method with shift $\\sigma = \\lambda_{\\text{test}}$.\n- **Initial Vector**: A deterministic vector $x_0$ created from a standard normal distribution with a fixed seed, normalized to unit $2$-norm.\n- **Iteration ($k \\ge 1$):**\n    1. Solve $(A - \\sigma I) y_k = x_{k-1}$ for $y_k$.\n    2. Normalize: $x_k = y_k / \\lVert y_k \\rVert_2$.\n- **Handling Singularity**: For singular or unstable $(A - \\sigma I)$, use one of:\n    1. Tikhonov regularization: solve $\\big((A - \\sigma I) + \\mu I\\big) y_{k} = x_{k-1}$ for small $\\mu  0$. Suggested baseline: $\\mu = 10^{-8} \\max(1, \\lVert A \\rVert_F)$.\n    2. Least-squares solution via a stable method like SVD.\n- **Metrics**: At each iteration, compute:\n    - Rayleigh quotient: $\\rho_k = \\frac{x_k^\\top A x_k}{x_k^\\top x_k}$.\n    - Residuals: $r_{\\sigma,k} = \\lVert A x_k - \\sigma x_k \\rVert_2$ and $r_{\\rho,k} = \\lVert A x_k - \\rho_k x_k \\rVert_2$.\n- **Termination**: Stop when $r_{\\sigma,k} \\le \\tau_{\\text{res}}$ or $k = k_{\\max}$.\n- **Constants**:\n    - $\\tau_{\\text{res}} = 10^{-8}$\n    - $\\tau_{\\text{rq}} = 10^{-8}$ (for optional check)\n    - $k_{\\max} = 30$\n- **Verification Rule**: $\\lambda_{\\text{test}}$ is verified if and only if the final $r_{\\sigma,k} \\le \\tau_{\\text{res}}$. An optional secondary check on $\\lvert \\rho_k - \\sigma \\rvert$ is mentioned but not required.\n- **Test Suite**:\n    - Test 1: $A_1 = \\begin{bmatrix} 2  1  0 \\\\ 1  2  1 \\\\ 0  1  2 \\end{bmatrix}$, $\\lambda_{\\text{test},1} = 2$.\n    - Test 2: $A_2 = \\begin{bmatrix} 1  2  3 \\\\ 0  4  5 \\\\ 0  0  6 \\end{bmatrix}$, $\\lambda_{\\text{test},2} = 4$.\n    - Test 3: $A_3 = A_1$, $\\lambda_{\\text{test},3} = 2.5$.\n    - Test 4: $A_4 = \\operatorname{diag}(1.0, 1.001, 10.0)$, $\\lambda_{\\text{test},4} = 1.001$.\n- **Output**: For each test, a list `[verified (bool), final_rho_k (float), final_r_sigma_k (float), iterations (int)]`. The final output is a list of these lists.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in numerical linear algebra, specifically eigenvalue algorithms. The shifted inverse power method is a standard technique. The challenge of a singular system when the shift equals an eigenvalue is a key practical aspect of the method, and the suggested remedies (regularization, least-squares) are standard numerical practices. The problem is well-posed, with all parameters, conditions, and test cases clearly defined, allowing for a deterministic and verifiable solution. The language is objective and precise. The problem statement passes all validation criteria.\n\n### Step 3: Verdict and Action\nThe problem is valid. A reasoned solution will be provided.\n\n## Solution\n\nThe problem requires the implementation of the shifted inverse power method to verify if a given value $\\sigma = \\lambda_{\\text{test}}$ is an eigenvalue of a matrix $A$. The core of the algorithm is the iterative application of the resolvent operator, $(A-\\sigma I)^{-1}$, to a vector.\n\n### Principle of the Shifted Inverse Power Method\nThe shifted inverse power method is an algorithm for finding an eigenvector of a matrix $A$ corresponding to an eigenvalue $\\lambda_j$ that is closest to a chosen shift $\\sigma$. The method relies on the property that if the eigenvalues of $A$ are $\\{\\lambda_i\\}$, then the eigenvalues of the matrix $(A-\\sigma I)^{-1}$ are $\\{(\\lambda_i - \\sigma)^{-1}\\}$. The power method, when applied to $(A-\\sigma I)^{-1}$, converges to the eigenvector associated with its eigenvalue of largest magnitude. This corresponds to the value $(\\lambda_j - \\sigma)^{-1}$ where $|\\lambda_j - \\sigma|$ is minimized. The iterative step is defined as solving $(A-\\sigma I)y_k = x_{k-1}$ for $y_k$, and then normalizing to obtain the next iterate, $x_k = y_k / \\lVert y_k \\rVert_2$.\n\n### The Singularity Issue\nA critical complication arises when the shift $\\sigma$ is an actual eigenvalue of $A$. In this case, the matrix $A - \\sigma I$ is singular, and its inverse is undefined. The linear system $(A-\\sigma I)y_k = x_{k-1}$ becomes ill-posed. A robust numerical method must be employed to handle this scenario. The problem statement proposes two such mechanisms: Tikhonov regularization and a least-squares solution.\n\n### Analysis of Proposed Mechanisms\n1.  **Tikhonov Regularization**: This approach modifies the system to $((A - \\sigma I) + \\mu I) y_k = x_{k-1}$, where $\\mu$ is a small positive parameter. This is equivalent to solving $(A - (\\sigma - \\mu) I) y_k = x_{k-1}$, which is the standard inverse power iteration with a slightly perturbed shift $\\sigma_{\\text{eff}} = \\sigma - \\mu$. Since $\\sigma$ is an eigenvalue $\\lambda_j$, and $\\mu$ is small, $\\sigma_{\\text{eff}}$ is very close to $\\lambda_j$. The resolvent $(A - \\sigma_{\\text{eff}} I)^{-1}$ is well-defined (assuming $\\sigma - \\mu$ is not another eigenvalue, which is highly unlikely for a small, fixed $\\mu$) and has a very large eigenvalue $(\\lambda_j - (\\sigma - \\mu))^{-1} = \\mu^{-1}$. The method will therefore converge rapidly to the desired eigenvector corresponding to $\\lambda_j$. This approach is mathematically sound and robust.\n\n2.  **Least-Squares Solution**: This involves finding a vector $y_k$ that minimizes the residual norm $\\lVert (A - \\sigma I) y_k - x_{k-1} \\rVert_2$. Standard numerical solvers for this problem, such as those based on the Singular Value Decomposition (SVD), typically find the unique solution of minimum Euclidean norm. This solution is orthogonal to the null space of $(A - \\sigma I)^T$. If $A$ is symmetric, this means the solution is orthogonal to the very eigenvector we are trying to find, which is counterproductive. For non-symmetric matrices, the situation is more complex, but the method is not guaranteed to amplify the component of the desired eigenvector.\n\n### Chosen Implementation Strategy\nGiven the role of a meticulous professor, the most correct and reliable method must be chosen. The Tikhonov regularization approach is demonstrably superior for this application. We will adopt a unified strategy that applies regularization in all cases. By solving $(A - \\sigma I + \\mu I) y_k = x_{k-1}$ with a fixed, small $\\mu$, the algorithm robustly handles both cases:\n- If $\\sigma$ is an eigenvalue, the regularization makes the system well-posed and ensures convergence to the correct eigenvector.\n- If $\\sigma$ is not an eigenvalue, the term $\\mu I$ is a small perturbation to an already non-singular system, and the iteration proceeds essentially as standard shifted inverse iteration.\n\nThis strategy is simple, elegant, and avoids the pitfalls of the minimum-norm least-squares approach. We will use the suggested baseline for the regularization parameter, $\\mu = 10^{-8} \\max(1, \\lVert A \\rVert_F)$.\n\nThe verification process proceeds as follows:\nFor each test case $(A, \\lambda_{\\text{test}})$, we set $\\sigma = \\lambda_{\\text{test}}$ and generate a deterministic initial unit vector $x_0$. We then iterate up to $k_{\\max}$ times. In each iteration $k$, we solve the regularized linear system for $y_k$, normalize it to get $x_k$, and compute the residual $r_{\\sigma,k} = \\lVert A x_k - \\sigma x_k \\rVert_2$. If $r_{\\sigma,k}$ drops below the tolerance $\\tau_{\\text{res}} = 10^{-8}$, the iteration terminates, and $\\lambda_{\\text{test}}$ is declared \"verified\". Otherwise, the process continues until $k=k_{\\max}$, at which point the final state is reported. The Rayleigh quotient $\\rho_k = x_k^{\\top} A x_k$ is also computed at each step and its final value is reported.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_verification(A: np.ndarray, lambda_test: float, k_max: int, tau_res: float, seed: int) - list:\n    \"\"\"\n    Verifies a claimed eigenvalue using the shifted inverse power method.\n\n    Args:\n        A: The square matrix.\n        lambda_test: The claimed eigenvalue to test (used as the shift).\n        k_max: Maximum number of iterations.\n        tau_res: Residual tolerance for convergence.\n        seed: Seed for the pseudorandom number generator for the initial vector.\n\n    Returns:\n        A list containing [verified, final_rho, final_r_sigma, iterations].\n    \"\"\"\n    n = A.shape[0]\n    sigma = lambda_test\n\n    # 1. Initialization\n    # Set seed for a deterministic initial vector\n    np.random.seed(seed)\n    # Create and normalize initial vector from a standard normal distribution\n    x_prev = np.random.randn(n)\n    x_prev = x_prev / np.linalg.norm(x_prev)\n\n    # Use Tikhonov regularization a priori for a unified, robust approach.\n    # This is equivalent to a slightly perturbed shift sigma_eff = sigma - mu.\n    mu = 1.0e-8 * max(1.0, np.linalg.norm(A, 'fro'))\n    M = A - sigma * np.identity(n) + mu * np.identity(n)\n\n    # Initialize variables to be reported\n    x_k = np.copy(x_prev)\n    rho_k = 0.0\n    r_sigma_k = np.inf\n    k = 0\n\n    # 2. Iteration Loop\n    for k_iter in range(1, k_max + 1):\n        k = k_iter\n\n        try:\n            # Solve the regularized system: (A - sigma*I + mu*I) * y_k = x_{k-1}\n            y_k = np.linalg.solve(M, x_prev)\n        except np.linalg.LinAlgError:\n            # This case is extremely unlikely if mu is chosen properly, but as a safeguard,\n            # we report failure by breaking the loop with a high residual.\n            r_sigma_k = np.inf\n            break\n        \n        y_norm = np.linalg.norm(y_k)\n\n        # If y_k is a zero vector, iteration cannot proceed.\n        if y_norm  np.finfo(float).eps:\n            break\n\n        # Normalize to get the next iterate\n        x_k = y_k / y_norm\n\n        # 3. Compute Metrics\n        Ax_k = A @ x_k\n        # Rayleigh quotient (since x_k is unit norm, x_k.T @ x_k = 1)\n        rho_k = x_k.T @ Ax_k\n        # Residual with respect to the shift sigma\n        r_sigma_k = np.linalg.norm(Ax_k - sigma * x_k)\n\n        # Update for the next iteration\n        x_prev = x_k\n\n        # 4. Termination Condition\n        if r_sigma_k = tau_res:\n            break\n\n    # 5. Finalization\n    # The verification rule is based on the final residual\n    verified = r_sigma_k = tau_res\n\n    return [verified, rho_k, r_sigma_k, k]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define constants from the problem statement\n    TAU_RES = 1.0e-8\n    K_MAX = 30\n    # A single fixed seed is used to generate the initial vector for all tests,\n    # ensuring full reproducibility of the entire script.\n    SEED = 42\n\n    # Define the test cases from the problem statement\n    A1 = np.array([[2.0, 1.0, 0.0], [1.0, 2.0, 1.0], [0.0, 1.0, 2.0]])\n    lambda1 = 2.0\n\n    A2 = np.array([[1.0, 2.0, 3.0], [0.0, 4.0, 5.0], [0.0, 0.0, 6.0]])\n    lambda2 = 4.0\n\n    A3 = A1\n    lambda3 = 2.5\n\n    A4 = np.diag([1.0, 1.001, 10.0])\n    lambda4 = 1.001\n\n    test_cases = [\n        (A1, lambda1),\n        (A2, lambda2),\n        (A3, lambda3),\n        (A4, lambda4),\n    ]\n\n    results = []\n    for A, lambda_test in test_cases:\n        result = run_verification(A, lambda_test, K_MAX, TAU_RES, SEED)\n        results.append(result)\n\n    # Final print statement in the exact required format\n    result_strings = []\n    for res in results:\n        verified_str = str(res[0])\n        rho_k_str = f\"{res[1]:.15g}\"\n        r_sigma_k_str = f\"{res[2]:.15g}\"\n        k_str = str(res[3])\n        result_strings.append(f\"[{verified_str},{rho_k_str},{r_sigma_k_str},{k_str}]\")\n\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n\n```", "id": "3273273"}, {"introduction": "While many problems involve real symmetric matrices with real eigenvalues, a vast number of applications in physics and engineering feature non-symmetric systems with complex eigenpairs that describe oscillations or instabilities. The shifted inverse power method extends elegantly to this domain by allowing the shift $\\sigma$ to be a complex number, thereby targeting eigenvalues in the complex plane. This practice [@problem_id:3273174] will challenge you to implement the method using complex arithmetic to isolate and compute these important complex eigenpairs.", "problem": "Implement a program that, for a given real, non-symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ and a complex shift $\\sigma \\in \\mathbb{C}$, computes an approximation to a complex eigenpair $(\\lambda, x)$ of $A$ using the shifted inverse power method. The derivation and algorithm must start from the following fundamental base: the eigenpair definition $A x = \\lambda x$, the fact that for any shift $\\sigma \\in \\mathbb{C}$ the resolvent $(A - \\sigma I)^{-1}$ exists when $\\sigma$ is not an eigenvalue, and that if $A$ is diagonalizable with $A = V \\Lambda V^{-1}$ then $(A - \\sigma I)^{-1} = V (\\Lambda - \\sigma I)^{-1} V^{-1}$. From this base, it follows that repeatedly applying $(A - \\sigma I)^{-1}$ to a nonzero vector and normalizing will amplify the component in the direction of the eigenvector corresponding to the eigenvalue with smallest $|\\lambda_j - \\sigma|$, thereby steering the iteration toward that eigenvector. The method is required to handle $\\sigma \\in \\mathbb{C}$ to target complex eigenpairs of a real, non-symmetric $A$.\n\nYour program must:\n- Implement the shifted inverse power iteration with a fixed complex shift $\\sigma \\in \\mathbb{C}$.\n- Use an initial vector $x_0 \\neq 0$ in $\\mathbb{C}^n$; choose $x_0$ deterministically (for example, all entries equal to $1$) and normalize it in the $\\ell^2$-norm.\n- At each iteration, solve $(A - \\sigma I) y_k = x_{k-1}$ exactly using a direct linear solve, then normalize $x_k = y_k / \\|y_k\\|_2$.\n- At iteration $k$, estimate the eigenvalue by the Rayleigh quotient $\\lambda_k = \\dfrac{x_k^* A x_k}{x_k^* x_k}$, where $x_k^*$ denotes conjugate transpose, and compute the residual $r_k = A x_k - \\lambda_k x_k$.\n- Terminate when $\\|r_k\\|_2 \\le \\tau$ or after a maximum of $m$ iterations, where $\\tau$ and $m$ are user-chosen tolerances. If $(A - \\sigma I)$ is singular or nearly singular numerically, perturb the shift by adding a tiny imaginary part $\\mathrm{i} \\,\\varepsilon$ with $\\varepsilon  0$ small (for example, $\\varepsilon = 10^{-12}$) and proceed.\n- Return the last $(\\lambda_k, x_k)$ along with the residual norm $\\|r_k\\|_2$ and the number of iterations performed.\n\nUse the following test suite. For each test, use maximum iterations $m = 100$ and tolerance $\\tau = 10^{-10}$.\n- Test $1$ (complex pair in a $2 \\times 2$ real block, embedded in $3 \\times 3$): \n  - $A_1 = \\begin{pmatrix} 0  -1  0 \\\\ 1  0  0 \\\\ 0  0  2 \\end{pmatrix}$, \n  - $\\sigma_1 = 0 + 1.05 \\,\\mathrm{i}$.\n  - Targeted eigenvalue is near $\\lambda \\approx 0 + 1 \\,\\mathrm{i}$.\n- Test $2$ (the conjugate pair in the same matrix): \n  - $A_2 = A_1$, \n  - $\\sigma_2 = 0 - 1.03 \\,\\mathrm{i}$.\n  - Targeted eigenvalue is near $\\lambda \\approx 0 - 1 \\,\\mathrm{i}$.\n- Test $3$ (non-symmetric $2 \\times 2$ rotation-scaling): \n  - $A_3 = \\begin{pmatrix} 1.1  -0.4 \\\\ 0.4  1.1 \\end{pmatrix}$, \n  - $\\sigma_3 = 1.06 + 0.39 \\,\\mathrm{i}$.\n  - Targeted eigenvalue is near $\\lambda \\approx 1.1 + 0.4 \\,\\mathrm{i}$.\n- Test $4$ (block diagonal with two distinct complex conjugate pairs): \n  - $A_4 = \\mathrm{blkdiag}\\!\\left(\\begin{pmatrix} 1.1  -0.3 \\\\ 0.3  1.1 \\end{pmatrix}, \\begin{pmatrix} 0.5  -1.2 \\\\ 1.2  0.5 \\end{pmatrix}\\right)$,\n  - $\\sigma_4 = 0.48 + 1.25 \\,\\mathrm{i}$.\n  - Targeted eigenvalue is near $\\lambda \\approx 0.5 + 1.2 \\,\\mathrm{i}$.\n\nFor each test case, your program must output a list $[\\Re(\\lambda), \\Im(\\lambda), \\|r\\|_2, k]$, where $\\Re(\\lambda)$ and $\\Im(\\lambda)$ denote the real and imaginary parts of the final eigenvalue estimate, $\\|r\\|_2$ is the final residual norm, and $k$ is the number of iterations used. Round $\\Re(\\lambda)$, $\\Im(\\lambda)$, and $\\|r\\|_2$ to eight decimal places, and output $k$ as an integer.\n\nFinal output format: Your program should produce a single line of output containing the results for the four tests as a comma-separated list enclosed in square brackets, where each entry is itself a bracketed comma-separated list with no spaces, for example \n$[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],[a_3,b_3,c_3,d_3],[a_4,b_4,c_4,d_4]]$, \nwith all $a_j$, $b_j$, $c_j$ rounded to eight decimal places and $d_j$ integers. There are no physical units involved in this problem.", "solution": "The problem requires the implementation of the shifted inverse power method to find a complex eigenpair $(\\lambda, x)$ of a real, non-symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ for a given complex shift $\\sigma \\in \\mathbb{C}$.\n\n### Theoretical Foundation\n\nThe shifted inverse power method is derived from the standard eigenvalue problem, $A x = \\lambda x$, where $\\lambda$ is an eigenvalue and $x$ is the corresponding eigenvector.\n\n1.  **Introducing the Shift**: For a given complex shift $\\sigma \\in \\mathbb{C}$, which is not an eigenvalue of $A$, we can manipulate the eigenvalue equation:\n    $$\n    A x - \\sigma x = \\lambda x - \\sigma x\n    $$\n    $$\n    (A - \\sigma I) x = (\\lambda - \\sigma) x\n    $$\n    where $I$ is the identity matrix.\n\n2.  **The Resolvent Matrix**: Since $\\sigma$ is not an eigenvalue, the matrix $(A - \\sigma I)$ is invertible. This inverse, $(A - \\sigma I)^{-1}$, is known as the resolvent of $A$ at $\\sigma$. Applying the resolvent to both sides of the equation yields:\n    $$\n    x = (\\lambda - \\sigma) (A - \\sigma I)^{-1} x\n    $$\n    Rearranging this gives:\n    $$\n    (A - \\sigma I)^{-1} x = \\frac{1}{\\lambda - \\sigma} x\n    $$\n    This equation reveals a crucial property: if $x$ is an eigenvector of $A$ with eigenvalue $\\lambda$, then $x$ is also an eigenvector of the matrix $(A - \\sigma I)^{-1}$ with eigenvalue $\\mu = \\frac{1}{\\lambda - \\sigma}$.\n\n3.  **Convergence Mechanism**: The power method is an iterative algorithm that finds the eigenvector corresponding to the eigenvalue with the largest magnitude (the dominant eigenvalue). When we apply the power method to the matrix $B = (A - \\sigma I)^{-1}$, it converges to the eigenvector associated with the dominant eigenvalue of $B$. Let the eigenvalues of $A$ be $\\lambda_1, \\lambda_2, \\ldots, \\lambda_n$. The eigenvalues of $B$ are $\\mu_j = \\frac{1}{\\lambda_j - \\sigma}$. The power method on $B$ will find the eigenvector corresponding to the $\\mu_j$ that maximizes $|\\mu_j|$.\n    $$\n    \\max_j |\\mu_j| = \\max_j \\left| \\frac{1}{\\lambda_j - \\sigma} \\right| = \\frac{1}{\\min_j |\\lambda_j - \\sigma|}\n    $$\n    Therefore, the power method applied to $(A - \\sigma I)^{-1}$ converges to the eigenvector of $A$ corresponding to the eigenvalue $\\lambda_j$ that is closest to the shift $\\sigma$. This is the principle of the shifted inverse power method.\n\n### Algorithmic Formulation\n\nThe algorithm iteratively refines an approximation of the eigenvector. Since the matrix $A$ is real but the shift $\\sigma$ and the target eigenvalues may be complex, all calculations must be performed using complex arithmetic.\n\n1.  **Initialization**:\n    - Select a random or deterministic initial vector $x_0 \\in \\mathbb{C}^n$ such that $x_0 \\neq 0$. As specified, we choose a vector of ones.\n    - Normalize the initial vector in the $\\ell^2$-norm: $x_0 \\leftarrow \\frac{x_0}{\\|x_0\\|_2}$.\n\n2.  **Iteration**: For $k = 1, 2, \\ldots, m$ (where $m$ is the maximum number of iterations):\n    - The core step of the power method on $(A - \\sigma I)^{-1}$ is computing $y_k = (A - \\sigma I)^{-1} x_{k-1}$. Instead of computing the inverse matrix explicitly, which is computationally expensive and numerically unstable, we solve the equivalent linear system for $y_k$:\n      $$\n      (A - \\sigma I) y_k = x_{k-1}\n      $$\n    - Normalize the resulting vector to prevent its magnitude from diverging or vanishing:\n      $$\n      x_k = \\frac{y_k}{\\|y_k\\|_2}\n      $$\n\n3.  **Eigenvalue Estimation and Convergence Check**:\n    - At each iteration $k$, the corresponding eigenvalue $\\lambda_k$ can be estimated from the current eigenvector approximation $x_k$ using the Rayleigh quotient. For a complex vector $x_k$, the Rayleigh quotient is defined as:\n      $$\n      \\lambda_k = \\frac{x_k^* A x_k}{x_k^* x_k}\n      $$\n      where $x_k^*$ is the conjugate transpose of $x_k$. Since $x_k$ is normalized to have unit $\\ell^2$-norm ($x_k^* x_k = \\|x_k\\|_2^2 = 1$), the expression simplifies to $\\lambda_k = x_k^* A x_k$.\n    - The quality of the approximate eigenpair $(\\lambda_k, x_k)$ is measured by the norm of the residual vector, $r_k = A x_k - \\lambda_k x_k$.\n    - The iteration terminates if the residual norm falls below a specified tolerance $\\tau$, i.e., $\\|r_k\\|_2 \\le \\tau$, or if the maximum number of iterations $m$ is reached.\n\n4.  **Numerical Stability**:\n    - If the shift $\\sigma$ is very close to an actual eigenvalue $\\lambda_j$, the matrix $(A - \\sigma I)$ becomes singular or nearly singular (ill-conditioned). This can cause the linear solve to fail or produce large numerical errors.\n    - As specified, if $(A - \\sigma I)$ is found to be numerically singular, the shift $\\sigma$ is perturbed by adding a small imaginary component, $\\sigma \\leftarrow \\sigma + \\mathrm{i}\\varepsilon$ for a small $\\varepsilon  0$ (e.g., $\\varepsilon = 10^{-12}$). This moves the shift slightly away from the real axis and typically resolves the singularity for real matrices, allowing the iteration to proceed. This check is performed once before the main iterative loop.\n\nThe complete algorithm proceeds as follows:\n- Given: Matrix $A$, shift $\\sigma$, tolerance $\\tau$, max iterations $m$.\n- Initialize $x_0$ as a normalized vector of ones.\n- Construct $M = A - \\sigma I$. Check for singularity; if singular, perturb $\\sigma$ by adding $\\mathrm{i}\\varepsilon$ and reconstruct $M$.\n- For $k = 1, \\dots, m$:\n    1.  Solve $M y_k = x_{k-1}$ for $y_k$.\n    2.  Normalize $x_k = y_k / \\|y_k\\|_2$.\n    3.  Compute $\\lambda_k = x_k^* A x_k$.\n    4.  Compute $\\|r_k\\|_2 = \\|A x_k - \\lambda_k x_k\\|_2$.\n    5.  If $\\|r_k\\|_2 \\le \\tau$, break the loop and return $(\\lambda_k, x_k, \\|r_k\\|_2, k)$.\n    6.  Set $x_{k-1} = x_k$.\n- If the loop completes, return the final $(\\lambda_m, x_m, \\|r_m\\|_2, m)$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef shifted_inverse_power_method(A, sigma, max_iter=100, tol=1e-10):\n    \"\"\"\n    Computes an eigenpair of a matrix A using the shifted inverse power method.\n\n    Args:\n        A (np.ndarray): The real, non-symmetric matrix.\n        sigma (complex): The complex shift.\n        max_iter (int): Maximum number of iterations.\n        tol (float): Tolerance for the residual norm to determine convergence.\n\n    Returns:\n        A tuple (re_lambda, im_lambda, residual_norm, iterations) containing the\n        real and imaginary parts of the eigenvalue estimate, the final residual\n        norm, and the number of iterations performed.\n    \"\"\"\n    n = A.shape[0]\n    A_complex = A.astype(np.complex128)\n\n    # 1. Initialization\n    x_prev = np.ones(n, dtype=np.complex128)\n    x_prev = x_prev / np.linalg.norm(x_prev)\n\n    current_sigma = complex(sigma)\n    \n    # 2. Construct M and handle potential singularity\n    M = A_complex - current_sigma * np.identity(n, dtype=np.complex128)\n    try:\n        # Test if M is invertible by attempting to solve a system.\n        # This is more robust than checking the determinant.\n        np.linalg.solve(M, x_prev)\n    except np.linalg.LinAlgError:\n        # Perturb sigma if M is singular\n        perturbation = 1e-12j\n        current_sigma += perturbation\n        M = A_complex - current_sigma * np.identity(n, dtype=np.complex128)\n\n    lmbda = 0.0 + 0.0j\n    x_curr = x_prev\n    residual_norm = np.inf\n\n    # 3. Iteration\n    for k in range(1, max_iter + 1):\n        # Solve (A - sigma*I)y_k = x_{k-1}\n        y_k = np.linalg.solve(M, x_prev)\n        \n        # Normalize to get x_k\n        norm_y = np.linalg.norm(y_k)\n        if norm_y == 0:\n            # This case occurs if x_prev is in the null space of M, which is highly unlikely.\n            # We exit gracefully if it happens.\n            break\n        x_curr = y_k / norm_y\n        \n        # 4. Eigenvalue estimation and residual calculation\n        # Rayleigh quotient: lmbda = (x_curr* @ A @ x_curr) / (x_curr* @ x_curr)\n        # Denominator is 1 due to normalization.\n        lmbda = x_curr.conj().T @ A_complex @ x_curr\n        \n        residual_norm = np.linalg.norm(A_complex @ x_curr - lmbda * x_curr)\n        \n        # 5. Check for convergence\n        if residual_norm  tol:\n            return lmbda.real, lmbda.imag, residual_norm, k\n        \n        x_prev = x_curr\n\n    # Return the last computed values if max_iter is reached\n    return lmbda.real, lmbda.imag, residual_norm, max_iter\n\ndef solve():\n    \"\"\"\n    Sets up and runs the test cases for the shifted inverse power method.\n    \"\"\"\n    m = 100\n    tau = 1e-10\n\n    # Test Case 1\n    A1 = np.array([[0, -1, 0], [1, 0, 0], [0, 0, 2]], dtype=float)\n    sigma1 = 0 + 1.05j\n\n    # Test Case 2\n    A2 = A1\n    sigma2 = 0 - 1.03j\n\n    # Test Case 3\n    A3 = np.array([[1.1, -0.4], [0.4, 1.1]], dtype=float)\n    sigma3 = 1.06 + 0.39j\n\n    # Test Case 4\n    A4 = np.array([\n        [1.1, -0.3, 0.0, 0.0],\n        [0.3, 1.1, 0.0, 0.0],\n        [0.0, 0.0, 0.5, -1.2],\n        [0.0, 0.0, 1.2, 0.5]\n    ], dtype=float)\n    sigma4 = 0.48 + 1.25j\n\n    test_cases = [\n        (A1, sigma1),\n        (A2, sigma2),\n        (A3, sigma3),\n        (A4, sigma4),\n    ]\n\n    results_list = []\n    \n    for A, sigma in test_cases:\n        re_l, im_l, res_norm, iters = shifted_inverse_power_method(A, sigma, m, tau)\n        \n        # Format the output as per requirement: [Re(l), Im(l), ||r||, k]\n        # Floats rounded to 8 decimal places.\n        formatted_result = f\"[{re_l:.8f},{im_l:.8f},{res_norm:.8f},{iters}]\"\n        results_list.append(formatted_result)\n        \n    # The final output must be a single line in the format [[...],[...],...]\n    final_output = f\"[{','.join(results_list)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3273174"}]}