{"hands_on_practices": [{"introduction": "This exercise solidifies your understanding of the Householder tridiagonalization as a sequence of reversible similarity transformations. By working backward from the tridiagonal form to reconstruct the original matrix, you will gain a deeper appreciation for the structure and properties of Householder reflectors, particularly their self-inverse nature ($H = H^{-1}$). This \"reverse engineering\" challenge [@problem_id:3239697] reinforces the foundational mechanics of the algorithm in a practical, puzzle-like context.", "problem": "A real symmetric matrix $A \\in \\mathbb{R}^{4 \\times 4}$ is reduced to a real symmetric tridiagonal matrix $T$ by a sequence of two Householder reflections constructed from unit vectors $v_1$ and $v_2$. A Householder reflector $H(v)$ associated with a unit vector $v \\in \\mathbb{R}^n$ is defined by $H(v) = I - 2 v v^{\\top}$, where $I$ denotes the identity matrix. Orthogonal similarity transformations preserve symmetry. You are given\n$$\nT = \\begin{pmatrix}\n4 & 1 & 0 & 0 \\\\\n1 & 3 & 2 & 0 \\\\\n0 & 2 & 5 & -1 \\\\\n0 & 0 & -1 & 2\n\\end{pmatrix}, \\quad\nv_1 = \\begin{pmatrix} 0 \\\\ \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix}, \\quad\nv_2 = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix}.\n$$\nReconstruct the original symmetric matrix $A$ consistent with this reduction and then compute the determinant of $A$. Express your final answer as an exact real number (no rounding).", "solution": "The problem requires the reconstruction of a real symmetric matrix $A \\in \\mathbb{R}^{4 \\times 4}$ from its tridiagonal form $T$, which was obtained through a sequence of two Householder reflections. Subsequently, the determinant of $A$ must be computed.\n\nThe process of tridiagonalization of a symmetric matrix $A$ using Householder transformations is a sequence of similarity transformations. For a $4 \\times 4$ matrix, two steps are required. The process can be written as:\n$A^{(1)} = A$\n$A^{(2)} = H_1 A^{(1)} H_1$\n$T = A^{(3)} = H_2 A^{(2)} H_2 = H_2 (H_1 A H_1) H_2 = H_2 H_1 A H_1 H_2$\nHere, $H_1$ and $H_2$ are Householder matrices constructed from the given unit vectors $v_1$ and $v_2$, respectively. A Householder matrix $H(v) = I - 2vv^\\top$ for a unit vector $v$ is both symmetric ($H^\\top = H$) and orthogonal ($H^{-1} = H^\\top = H$).\n\nTo reconstruct the original matrix $A$, we can reverse the transformation sequence:\n$A = H_1 H_2 T H_2 H_1$.\n\nFirst, we construct the Householder matrices $H_1$ and $H_2$.\nThe first unit vector is $v_1 = \\begin{pmatrix} 0 \\\\ \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix}$.\nThe outer product $v_1 v_1^\\top$ is:\n$$v_1 v_1^\\top = \\begin{pmatrix} 0 \\\\ \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 0 & \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ 0 & \\frac{1}{2} & \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{2} & \\frac{1}{2} & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}$$\nThe Householder matrix $H_1$ is:\n$$H_1 = I - 2v_1v_1^\\top = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix} - 2\\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ 0 & \\frac{1}{2} & \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{2} & \\frac{1}{2} & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 0 & -1 & 0 \\\\ 0 & -1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix}$$\nThe second unit vector is $v_2 = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix}$.\nThe outer product $v_2 v_2^\\top$ is:\n$$v_2 v_2^\\top = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\begin{pmatrix} 0 & 0 & \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & \\frac{1}{2} & \\frac{1}{2} \\\\ 0 & 0 & \\frac{1}{2} & \\frac{1}{2} \\end{pmatrix}$$\nThe Householder matrix $H_2$ is:\n$$H_2 = I - 2v_2v_2^\\top = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix} - 2\\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & \\frac{1}{2} & \\frac{1}{2} \\\\ 0 & 0 & \\frac{1}{2} & \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & -1 \\\\ 0 & 0 & -1 & 0 \\end{pmatrix}$$\nNow we compute $A = H_1 H_2 T H_2 H_1$ with the given matrix $T$:\n$$T = \\begin{pmatrix} 4 & 1 & 0 & 0 \\\\ 1 & 3 & 2 & 0 \\\\ 0 & 2 & 5 & -1 \\\\ 0 & 0 & -1 & 2 \\end{pmatrix}$$\nLet's first compute the intermediate matrix $T' = H_2 T H_2$:\n$$H_2 T = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & -1 \\\\ 0 & 0 & -1 & 0 \\end{pmatrix} \\begin{pmatrix} 4 & 1 & 0 & 0 \\\\ 1 & 3 & 2 & 0 \\\\ 0 & 2 & 5 & -1 \\\\ 0 & 0 & -1 & 2 \\end{pmatrix} = \\begin{pmatrix} 4 & 1 & 0 & 0 \\\\ 1 & 3 & 2 & 0 \\\\ 0 & 0 & 1 & -2 \\\\ 0 & -2 & -5 & 1 \\end{pmatrix}$$\n$$T' = (H_2 T) H_2 = \\begin{pmatrix} 4 & 1 & 0 & 0 \\\\ 1 & 3 & 2 & 0 \\\\ 0 & 0 & 1 & -2 \\\\ 0 & -2 & -5 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & -1 \\\\ 0 & 0 & -1 & 0 \\end{pmatrix} = \\begin{pmatrix} 4 & 1 & 0 & 0 \\\\ 1 & 3 & 0 & -2 \\\\ 0 & 0 & 2 & -1 \\\\ 0 & -2 & -1 & 5 \\end{pmatrix}$$\nNow, we compute $A = H_1 T' H_1$:\n$$H_1 T' = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 0 & -1 & 0 \\\\ 0 & -1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 4 & 1 & 0 & 0 \\\\ 1 & 3 & 0 & -2 \\\\ 0 & 0 & 2 & -1 \\\\ 0 & -2 & -1 & 5 \\end{pmatrix} = \\begin{pmatrix} 4 & 1 & 0 & 0 \\\\ 0 & 0 & -2 & 1 \\\\ -1 & -3 & 0 & 2 \\\\ 0 & -2 & -1 & 5 \\end{pmatrix}$$\n$$A = (H_1 T') H_1 = \\begin{pmatrix} 4 & 1 & 0 & 0 \\\\ 0 & 0 & -2 & 1 \\\\ -1 & -3 & 0 & 2 \\\\ 0 & -2 & -1 & 5 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 0 & -1 & 0 \\\\ 0 & -1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 4 & 0 & -1 & 0 \\\\ 0 & 2 & 0 & 1 \\\\ -1 & 0 & 3 & 2 \\\\ 0 & 1 & 2 & 5 \\end{pmatrix}$$\nThis is the reconstructed original symmetric matrix $A$.\n\nThe final step is to compute the determinant of $A$. Since $A$ and $T$ are related by a similarity transformation (because $H_1$ and $H_2$ are orthogonal), their determinants are equal.\n$\\det(A) = \\det(H_1 H_2 T H_2 H_1)$. Let $Q = H_1 H_2$. $Q$ is orthogonal, so $Q^{-1} = Q^\\top = (H_1 H_2)^\\top = H_2^\\top H_1^\\top = H_2 H_1$. Thus $A = Q T Q^\\top$.\n$\\det(A) = \\det(Q T Q^\\top) = \\det(Q) \\det(T) \\det(Q^\\top) = \\det(T)$ since $\\det(Q) = \\det(Q^\\top) = \\pm 1$.\nThis allows us to compute the determinant from the simpler tridiagonal matrix $T$.\n\nCalculating $\\det(T)$:\n$$\\det(T) = \\det \\begin{pmatrix} 4 & 1 & 0 & 0 \\\\ 1 & 3 & 2 & 0 \\\\ 0 & 2 & 5 & -1 \\\\ 0 & 0 & -1 & 2 \\end{pmatrix}$$\nWe use cofactor expansion along the first row:\n$$\\det(T) = 4 \\cdot \\det \\begin{pmatrix} 3 & 2 & 0 \\\\ 2 & 5 & -1 \\\\ 0 & -1 & 2 \\end{pmatrix} - 1 \\cdot \\det \\begin{pmatrix} 1 & 2 & 0 \\\\ 0 & 5 & -1 \\\\ 0 & -1 & 2 \\end{pmatrix}$$\nThe first $3 \\times 3$ determinant is:\n$$\\det \\begin{pmatrix} 3 & 2 & 0 \\\\ 2 & 5 & -1 \\\\ 0 & -1 & 2 \\end{pmatrix} = 3(5 \\cdot 2 - (-1)(-1)) - 2(2 \\cdot 2 - 0) = 3(10 - 1) - 2(4) = 3(9) - 8 = 27 - 8 = 19$$\nThe second $3 \\times 3$ determinant is:\n$$\\det \\begin{pmatrix} 1 & 2 & 0 \\\\ 0 & 5 & -1 \\\\ 0 & -1 & 2 \\end{pmatrix} = 1(5 \\cdot 2 - (-1)(-1)) = 10 - 1 = 9$$\nTherefore, the determinant of $T$ is:\n$$\\det(T) = 4(19) - 1(9) = 76 - 9 = 67$$\nThus, $\\det(A) = 67$.\n\nAs a verification, we can compute the determinant of the reconstructed matrix $A$ directly:\n$$\\det(A) = \\det \\begin{pmatrix} 4 & 0 & -1 & 0 \\\\ 0 & 2 & 0 & 1 \\\\ -1 & 0 & 3 & 2 \\\\ 0 & 1 & 2 & 5 \\end{pmatrix}$$\nUsing cofactor expansion along the first row:\n$$\\det(A) = 4 \\cdot \\det \\begin{pmatrix} 2 & 0 & 1 \\\\ 0 & 3 & 2 \\\\ 1 & 2 & 5 \\end{pmatrix} + (-1) \\cdot \\det \\begin{pmatrix} 0 & 2 & 1 \\\\ -1 & 0 & 2 \\\\ 0 & 1 & 5 \\end{pmatrix}$$\nThe first $3 \\times 3$ determinant is:\n$$\\det \\begin{pmatrix} 2 & 0 & 1 \\\\ 0 & 3 & 2 \\\\ 1 & 2 & 5 \\end{pmatrix} = 2(3 \\cdot 5 - 2 \\cdot 2) + 1(0 \\cdot 2 - 3 \\cdot 1) = 2(15 - 4) - 3 = 2(11) - 3 = 22 - 3 = 19$$\nThe second $3 \\times 3$ determinant (the minor corresponding to the $A_{13}$ entry) is:\nWe use cofactor expansion along the first column for the second minor:\n$$\\det \\begin{pmatrix} 0 & 2 & 1 \\\\ -1 & 0 & 2 \\\\ 0 & 1 & 5 \\end{pmatrix} = -(-1) \\det \\begin{pmatrix} 2 & 1 \\\\ 1 & 5 \\end{pmatrix} = 10 - 1 = 9$$\nSo, the determinant of $A$ is:\n$$\\det(A) = 4(19) + (-1)(9) = 76 - 9 = 67$$\nThe results match, confirming the correctness of the calculations.", "answer": "$$\\boxed{67}$$", "id": "3239697"}, {"introduction": "This thought experiment challenges you to look beyond the standard case of a dense matrix and consider the structural consequences of the Householder transformation. It demonstrates that the algorithm's effect is not always as simple as just creating zeros, introducing the crucial concept of \"fill-in\" where an operation destroys existing sparsity [@problem_id:3239644]. Answering this question provides valuable insight for applications involving matrices with special structures, a common scenario in physics and engineering.", "problem": "Consider a real, symmetric, $n \\times n$ arrowhead matrix $A$ with the structure\n$$\nA \\;=\\; \\begin{bmatrix}\n\\alpha & z^{\\top} \\\\\nz & D\n\\end{bmatrix},\n$$\nwhere $z \\in \\mathbb{R}^{n-1}$, $D \\in \\mathbb{R}^{(n-1)\\times (n-1)}$ is diagonal, and all entries of $A$ outside the first row, first column, and main diagonal are zero. The Householder tridiagonalization algorithm applies a sequence of orthogonal Householder reflectors to $A$ to produce a tridiagonal matrix via orthogonal similarity. A Householder reflector $H$ has the form $H = I - 2 v v^{\\top}$ for some $v \\in \\mathbb{R}^{n}$ with $\\|v\\|_2 = 1$, and satisfies $H^{\\top} H = I$ and $H = H^{\\top}$.\n\nFrom first principles about orthogonal similarity and reflections in $\\mathbb{R}^n$, reason about the effect of the first Householder step that acts only on indices $2,\\dots,n$ so as to annihilate all but the first component of $z$, and then about the overall progression of the tridiagonalization process. Which statement is correct?\n\nA. After one Householder step, the matrix is exactly tridiagonal for all $n \\ge 4$.\n\nB. The matrix is already tridiagonal before any transformations.\n\nC. One Householder step annihilates all but one entry in the first column (and the symmetric first row), but the matrix is not yet tridiagonal; in general, the standard algorithm still requires $n-2$ Householder reflectors to complete tridiagonalization.\n\nD. Householder reflectors preserve the arrowhead structure at each step, so the matrix remains arrowhead throughout.\n\nE. The first Householder step produces dense fill in the trailing block that makes tridiagonalization impossible.", "solution": "The problem statement has been validated and found to be scientifically grounded, well-posed, objective, and internally consistent. It does not violate any of the specified criteria for invalidity. Therefore, a full analysis will be performed.\n\nThe problem asks for a correct statement regarding the application of the Householder tridiagonalization algorithm to a real, symmetric, $n \\times n$ arrowhead matrix $A$. The structure of $A$ is given by\n$$\nA \\;=\\; \\begin{bmatrix}\n\\alpha & z^{\\top} \\\\\nz & D\n\\end{bmatrix},\n$$\nwhere $\\alpha \\in \\mathbb{R}$, $z \\in \\mathbb{R}^{n-1}$, and $D \\in \\mathbb{R}^{(n-1)\\times (n-1)}$ is a diagonal matrix. The entries of $A$ are zero everywhere except for the first row, the first column, and the main diagonal. Let the entries of $z$ be $z = (z_1, z_2, \\dots, z_{n-1})^{\\top}$ and the diagonal entries of $D$ be $d_1, d_2, \\dots, d_{n-1}$. The matrix $A$ can be written explicitly as:\n$$\nA = \\begin{pmatrix}\n\\alpha & z_1 & z_2 & \\dots & z_{n-1} \\\\\nz_1 & d_1 & 0 & \\dots & 0 \\\\\nz_2 & 0 & d_2 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\nz_{n-1} & 0 & 0 & \\dots & d_{n-1}\n\\end{pmatrix}\n$$\nThe standard Householder tridiagonalization algorithm for a symmetric $n \\times n$ matrix proceeds in $n-2$ steps. At the first step ($k=1$), the algorithm creates zeros in the first column at positions $3, \\dots, n$. This is achieved by applying a Householder reflector that operates on the vector of entries $(A_{21}, A_{31}, \\dots, A_{n1})^{\\top}$. In this problem, this vector is precisely $z=(z_1, \\dots, z_{n-1})^\\top$.\n\nThe problem states that the first Householder step acts on indices $2, \\dots, n$. This corresponds to a transformation matrix $H_1$ of the block form:\n$$\nH_1 = \\begin{pmatrix}\n1 & 0 \\\\\n0 & \\hat{H}_1\n\\end{pmatrix}\n$$\nwhere $\\hat{H}_1$ is an $(n-1) \\times (n-1)$ Householder reflector. $\\hat{H}_1$ is designed to zero out all but the first component of the vector $z$. That is, $\\hat{H}_1 z = \\sigma e_1$, where $e_1 = (1, 0, \\dots, 0)^{\\top} \\in \\mathbb{R}^{n-1}$ and $\\sigma = \\pm \\|z\\|_2$.\n\nThe similarity transformation updates the matrix $A$ to $A^{(1)} = H_1 A H_1$. Since $H_1$ is symmetric ($H_1 = H_1^{\\top}$), we have:\n$$\nA^{(1)} = H_1 A H_1 = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\hat{H}_1 \\end{pmatrix} \\begin{pmatrix} \\alpha & z^{\\top} \\\\ z & D \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & \\hat{H}_1 \\end{pmatrix} = \\begin{pmatrix} \\alpha & z^{\\top}\\hat{H}_1 \\\\ \\hat{H}_1 z & \\hat{H}_1 D \\hat{H}_1 \\end{pmatrix}\n$$\nUsing the properties $\\hat{H}_1 z = \\sigma e_1$ and $\\hat{H}_1 = \\hat{H}_1^{\\top}$, the term $z^{\\top}\\hat{H}_1$ becomes $(\\hat{H}_1 z)^{\\top} = (\\sigma e_1)^{\\top}$. Thus, the transformed matrix $A^{(1)}$ has the structure:\n$$\nA^{(1)} = \\begin{pmatrix}\n\\alpha & \\sigma & 0 & \\dots & 0 \\\\\n\\sigma & \\multicolumn{4}{c}{\\multirow{4}{*}{$\\hat{H}_1 D \\hat{H}_1$}} \\\\\n0 & \\\\\n\\vdots & \\\\\n0 &\n\\end{pmatrix}\n$$\nThe first row and column of $A^{(1)}$ now have the desired tridiagonal form. To determine if the entire matrix is tridiagonal, we must inspect the structure of the trailing $(n-1) \\times (n-1)$ submatrix, which we denote as $B = \\hat{H}_1 D \\hat{H}_1$.\n\nThe reflector $\\hat{H}_1$ has the form $\\hat{H}_1 = I - 2vv^{\\top}$ for a unit vector $v \\in \\mathbb{R}^{n-1}$. In general, if $z$ is not already proportional to $e_1$, the vector $v$ will be dense (i.e., have no zero entries). The submatrix $B$ is then:\n$$\nB = (I - 2vv^{\\top})D(I - 2vv^{\\top}) = D - 2vv^{\\top}D - 2Dvv^{\\top} + 4(v^{\\top}Dv)vv^{\\top}\n$$\nThis expression represents a symmetric rank-$2$ update to the diagonal matrix $D$. Let's examine the off-diagonal entries of $B$. For $i \\neq j$, the $(i,j)$-th entry is:\n$$\nB_{ij} = D_{ij} - 2(vv^{\\top}D)_{ij} - 2(Dvv^{\\top})_{ij} + 4(v^{\\top}Dv)(vv^{\\top})_{ij}\n$$\nSince $D$ is diagonal, $D_{ij} = 0$ for $i \\neq j$. The other terms are:\n$(vv^{\\top}D)_{ij} = v_i \\sum_{k=1}^{n-1} v_k D_{kj} = v_i v_j d_j$.\n$(Dvv^\\top)_{ij} = \\sum_{k=1}^{n-1} D_{ik}(vv^\\top)_{kj} = D_{ii}(vv^\\top)_{ij} = d_i v_i v_j$.\n$(vv^{\\top})_{ij} = v_i v_j$.\nSubstituting these gives:\n$$\nB_{ij} = -2v_i v_j d_j - 2d_i v_i v_j + 4(v^{\\top}Dv) v_i v_j = v_iv_j \\left( -2(d_i+d_j) + 4\\sum_{k=1}^{n-1} v_k^2 d_k \\right)\n$$\nSince $v$ is dense in general, its components $v_i$ and $v_j$ are non-zero. The term in the parentheses is also generally non-zero (unless there is a very specific relationship between the entries of $z$ and $D$). Therefore, $B_{ij} \\neq 0$ for $i \\neq j$. This means the submatrix $B = \\hat{H}_1 D \\hat{H}_1$ is, in general, a dense symmetric matrix. This phenomenon is known as \"fill-in\".\n\nFor $A^{(1)}$ to be tridiagonal, the submatrix $B$ would also need to be tridiagonal. Since $B$ is dense, $A^{(1)}$ is not tridiagonal for $n \\ge 4$. For example, the entry $A^{(1)}_{24}$ (which equals $B_{13}$) will be non-zero.\n\nThe Householder tridiagonalization must now proceed on the dense $(n-1) \\times (n-1)$ submatrix $B$. Tridiagonalizing a general symmetric matrix of size $m \\times m$ requires $m-2$ Householder steps. For the submatrix $B$ of size $(n-1) \\times (n-1)$, this requires $(n-1)-2 = n-3$ additional steps. The total number of Householder steps is the one step applied to $A$ plus the $n-3$ steps applied to the subproblem, giving a total of $1 + (n-3) = n-2$ steps. This is the same number of steps required for a general dense symmetric $n \\times n$ matrix.\n\nNow, we evaluate each option:\n\nA. After one Householder step, the matrix is exactly tridiagonal for all $n \\ge 4$.\nThis is **Incorrect**. As shown, the transformation introduces dense fill-in in the trailing $(n-1) \\times (n-1)$ submatrix. For $n \\ge 4$, this submatrix is at least $3 \\times 3$ and will not be tridiagonal, so the full matrix $A^{(1)}$ is not tridiagonal.\n\nB. The matrix is already tridiagonal before any transformations.\nThis is **Incorrect**. A tridiagonal matrix $T$ satisfies $T_{ij}=0$ for $|i-j|>1$. The arrowhead matrix $A$ has non-zero entries $A_{1,j+1} = z_j$ for $j=1, \\dots, n-1$. If $n \\ge 4$ and $z_3 \\neq 0$, then $A_{14} \\neq 0$, which violates the tridiagonal condition since $|1-4|=3 > 1$. This is not true in general.\n\nC. One Householder step annihilates all but one entry in the first column (and the symmetric first row), but the matrix is not yet tridiagonal; in general, the standard algorithm still requires $n-2$ Householder reflectors to complete tridiagonalization.\nThis is **Correct**. The analysis above has shown that:\n1. The first step creates zeros in the first column (and row) as desired.\n2. The matrix is not yet tridiagonal due to the creation of a dense submatrix.\n3. The total number of steps required is $n-2$, which is the standard number for an $n \\times n$ matrix.\nThis statement accurately describes the process.\n\nD. Householder reflectors preserve the arrowhead structure at each step, so the matrix remains arrowhead throughout.\nThis is **Incorrect**. The original arrowhead matrix $A$ has a diagonal submatrix $D$. After one step, the corresponding submatrix $\\hat{H}_1 D \\hat{H}_1$ is dense. A dense matrix is not an arrowhead matrix (which has a diagonal trailing block), so the arrowhead structure is destroyed.\n\nE. The first Householder step produces dense fill in the trailing block that makes tridiagonalization impossible.\nThis is **Incorrect**. The first part of the statement is correct: the step produces dense fill-in. However, the conclusion that this makes tridiagonalization \"impossible\" is false. The Householder algorithm is designed to work on dense symmetric matrices. The process simply continues by applying the standard algorithm to the now-dense subproblem. The tridiagonalization is perfectly possible and is completed in the subsequent $n-3$ steps.", "answer": "$$\\boxed{C}$$", "id": "3239644"}, {"introduction": "Moving from theory to practice, this coding challenge puts you in the role of a developer debugging a common but subtle error in the Householder algorithm. The core of the method is a similarity transformation, $A \\to Q A Q^\\top$, which must be applied completely to preserve the matrix's eigenvalues. By implementing both a correct and a faulty procedure [@problem_id:3239695], you will see firsthand why every part of the similarity update is essential for the algorithm to work as intended.", "problem": "Consider a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$. A sequence of orthogonal similarity transformations using Householder reflectors can reduce $A$ to a symmetric tridiagonal matrix $T$ without altering its eigenvalues. A Householder reflector acting on a vector $x \\in \\mathbb{R}^{m}$ is defined by choosing $u = x - \\alpha e_1$ where $e_1$ is the first standard basis vector, and $\\alpha = -\\operatorname{sign}(x_1)\\lVert x \\rVert_2$. The vector $v = \\dfrac{u}{\\lVert u \\rVert_2}$ defines the reflector $H = I_m - 2 v v^\\top$, which satisfies $H x = \\alpha e_1$, $H^\\top = H$, and $H$ is orthogonal. The symmetric tridiagonalization applies a similarity at each step $k$ by a block-diagonal orthogonal matrix $Q_k = I_{k+1} \\oplus H_k$, where $H_k$ acts on the trailing subvector of the $k$-th column of $A$ below the diagonal. The exact similarity is $A \\leftarrow Q_k A Q_k^\\top$. When correctly implemented for $k = 0, 1, \\dots, n-3$, this produces a matrix $T$ where all entries $T_{i,j}$ with $\\lvert i - j \\rvert > 1$ are exactly zero in exact arithmetic and are numerically negligible in floating-point arithmetic.\n\nA common implementation bug is to update only the trailing principal submatrix $A_{22}$ via $A_{22} \\leftarrow H_k A_{22} H_k$ while neglecting the off-diagonal coupling blocks $A_{12}$ and $A_{21}$. To see why this is incorrect, write the block partitioning at step $k$ as\n$$\nA = \\begin{bmatrix}\nA_{11} & A_{12} \\\\\nA_{21} & A_{22}\n\\end{bmatrix}, \\quad Q_k = \\begin{bmatrix}\nI_{k+1} & 0 \\\\\n0 & H_k\n\\end{bmatrix}.\n$$\nThen the mathematically correct similarity gives\n$$\nQ_k A Q_k^\\top = \n\\begin{bmatrix}\nA_{11} & A_{12} H_k \\\\\nH_k A_{21} & H_k A_{22} H_k\n\\end{bmatrix}.\n$$\nIf one updates only $A_{22}$ and leaves $A_{12}$ and $A_{21}$ unchanged, the global similarity is not performed, and fill-in outside the tridiagonal band persists. The resulting matrix can remain symmetric but not be perfectly tridiagonal.\n\nTask: Implement both the faulty and the corrected Householder tridiagonalization procedures. The faulty procedure must perform only the trailing principal submatrix update at each step, while the corrected procedure must apply the full block similarity to all involved blocks. Implement a function to count how many entries of the final matrix lie outside the tridiagonal band with magnitude exceeding a threshold $\\varepsilon$. Use the threshold $\\varepsilon = 10^{-10}$.\n\nYour program must process the following test suite of symmetric matrices:\n- Test case $1$: $A_1 \\in \\mathbb{R}^{5 \\times 5}$,\n$$\nA_1 =\n\\begin{bmatrix}\n6 & -2 & 3 & 0 & 1 \\\\\n-2 & 5 & 2 & -1 & 4 \\\\\n3 & 2 & 4 & 2 & 0 \\\\\n0 & -1 & 2 & 3 & -2 \\\\\n1 & 4 & 0 & -2 & 7\n\\end{bmatrix}.\n$$\n- Test case $2$: $A_2 \\in \\mathbb{R}^{2 \\times 2}$,\n$$\nA_2 =\n\\begin{bmatrix}\n2 & -1 \\\\\n-1 & 3\n\\end{bmatrix}.\n$$\n- Test case $3$: $A_3 \\in \\mathbb{R}^{1 \\times 1}$,\n$$\nA_3 =\n\\begin{bmatrix}\n5\n\\end{bmatrix}.\n$$\n- Test case $4$: $A_4 \\in \\mathbb{R}^{6 \\times 6}$ already tridiagonal,\n$$\nA_4 =\n\\begin{bmatrix}\n4 & 1 & 0 & 0 & 0 & 0 \\\\\n1 & 5 & -2 & 0 & 0 & 0 \\\\\n0 & -2 & 6 & 3 & 0 & 0 \\\\\n0 & 0 & 3 & 7 & -4 & 0 \\\\\n0 & 0 & 0 & -4 & 8 & 5 \\\\\n0 & 0 & 0 & 0 & 5 & 9\n\\end{bmatrix}.\n$$\n- Test case $5$: $A_5 \\in \\mathbb{R}^{5 \\times 5}$ tridiagonal with tiny off-tridiagonal perturbations of size $10^{-12}$,\n$$\nA_5 =\n\\begin{bmatrix}\n10 & 2 & 0 & 10^{-12} & 0 \\\\\n2 & 9 & -1 & 0 & 10^{-12} \\\\\n0 & -1 & 8 & 3 & 0 \\\\\n10^{-12} & 0 & 3 & 7 & 2 \\\\\n0 & 10^{-12} & 0 & 2 & 6\n\\end{bmatrix}.\n$$\n\nDefine a function that, for a given matrix $T$ and threshold $\\varepsilon$, counts the number of entries $T_{i,j}$ with $\\lvert i - j \\rvert > 1$ and $\\lvert T_{i,j} \\rvert > \\varepsilon$. For each test case, produce two integers: the count from the faulty implementation and the count from the corrected implementation.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a two-integer list for a test case. For example, the output should look like $[[b_1,f_1],[b_2,f_2],\\dots]$, where $b_k$ is the faulty count and $f_k$ is the corrected count for test case $k$. No physical units or angle units are involved in this problem.", "solution": "The user's request is to implement and compare two versions of the Householder tridiagonalization algorithm for a real symmetric matrix $A$: a mathematically correct version and a common faulty version.\n\n### 1. Problem Validation\n\nThe problem statement has been meticulously validated.\n\n- **Extraction of Givens**: All definitions, constants, matrices, and procedural descriptions were extracted verbatim.\n- **Scientific Grounding**: The problem is well-grounded in the theory of numerical linear algebra. The description of Householder reflectors and the tridiagonalization process is standard and mathematically sound. The \"faulty\" algorithm represents a plausible and illustrative implementation error.\n- **Well-Posedness**: The task is clearly defined. The inputs are specific matrices, and the output is a quantitative comparison (counts of off-tridiagonal elements) based on a precise threshold. The problem is self-contained and free of contradictions or ambiguities.\n- **Verdict**: The problem is valid and can be solved as stated.\n\n### 2. Householder Transformation\n\nThe core of the algorithm is the Householder transformation. For a given vector $x \\in \\mathbb{R}^m$, a Householder reflector $H$ is an orthogonal matrix that transforms $x$ into a multiple of the first standard basis vector $e_1$. The construction, as defined in the problem, is as follows:\n1.  Calculate a scalar $\\alpha = -\\operatorname{sign}(x_1) \\lVert x \\rVert_2$. To handle the case $x_1=0$ robustly and maintain numerical stability (avoiding subtractive cancellation), we define $\\operatorname{sign}(0)=+1$.\n2.  Form the vector $u = x - \\alpha e_1$.\n3.  Normalize $u$ to get the direction vector $v = \\frac{u}{\\lVert u \\rVert_2}$.\n4.  The reflector is given by the matrix $H = I_m - 2 v v^\\top$.\n\nThis matrix $H$ is symmetric ($H=H^\\top$) and orthogonal ($H^\\top H = I_m$). When applied to $x$, it yields $H x = \\alpha e_1$.\n\n### 3. Tridiagonalization Algorithm\n\nThe process reduces a symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ to a symmetric tridiagonal matrix $T$ by applying a sequence of similarity transformations. The algorithm proceeds in steps, $k=0, 1, \\dots, n-3$. At each step $k$, the goal is to introduce zeros in the $k$-th column below the first subdiagonal element, and symmetrically in the $k$-th row.\n\nLet $A^{(k)}$ be the matrix at the beginning of step $k$ (with $A^{(0)}=A$).\n1.  Extract the vector $x = A^{(k)}[k+1:n, k]$. This vector has dimension $m=n-(k+1)$.\n2.  Construct the $m \\times m$ Householder reflector $H_k$ for this vector $x$.\n3.  Form the full transformation matrix $Q_k = \\begin{bmatrix} I_{k+1} & 0 \\\\ 0 & H_k \\end{bmatrix}$.\n4.  Apply the similarity transformation: $A^{(k+1)} = Q_k A^{(k)} Q_k^{\\top}$.\n\nSince $H_k$ is symmetric, $Q_k$ is also symmetric, so $A^{(k+1)} = Q_k A^{(k)} Q_k$.\n\n### 4. Correct vs. Faulty Implementation\n\nThe crucial difference between the two implementations lies in how the similarity transformation $A \\leftarrow Q_k A Q_k$ is applied. Using the block-matrix form from the problem statement:\n$$\nA = \\begin{bmatrix}\nA_{11} & A_{12} \\\\\nA_{21} & A_{22}\n\\end{bmatrix}, \\quad Q_k = \\begin{bmatrix}\nI_{k+1} & 0 \\\\\n0 & H_k\n\\end{bmatrix}\n$$\nwhere the partition is after row/column $k$.\n\n**Correct Procedure:** The full similarity transformation must be applied.\n$$\nA \\leftarrow Q_k A Q_k^\\top =\n\\begin{bmatrix}\nA_{11} & A_{12} H_k \\\\\nH_k A_{21} & H_k A_{22} H_k\n\\end{bmatrix}\n$$\nThis involves three updates:\n-   $A_{21} \\leftarrow H_k A_{21}$: This is the key step that zeros out the target elements in the $k$-th column.\n-   $A_{12} \\leftarrow A_{12} H_k$: Symmetrically zeros out elements in the $k$-th row.\n-   $A_{22} \\leftarrow H_k A_{22} H_k$: A similarity transformation on the trailing principal submatrix to maintain the global similarity and preserve eigenvalues.\n\n**Faulty Procedure:** The problem describes a bug where the updates to the off-diagonal blocks $A_{12}$ and $A_{21}$ are neglected.\n$$\nA \\leftarrow \\begin{bmatrix}\nA_{11} & A_{12} \\\\\nA_{21} & H_k A_{22} H_k\n\\end{bmatrix}\n$$\nOnly the trailing submatrix $A_{22}$ is updated. This fails to zero out the elements in column $k$, and since the full similarity transformation is not performed, the resulting matrix is not guaranteed to be tridiagonal. It does, however, remain symmetric.\n\n### 5. Implementation Strategy\n\nFor each test case matrix $A$:\n1.  Two copies, `A_correct` and `A_faulty`, are created.\n2.  The correct and faulty tridiagonalization procedures are applied to their respective matrices. The algorithms iterate from $k=0$ to $n-3$. For matrices of size $n<3$, no transformations are performed.\n3.  Inside the loop for step $k$, the vector $x = A[k+1:n, k]$ is used to construct the Householder reflector $H_k$.\n4.  For `A_correct`, the transformation $H_k$ is applied to the block of rows `A_correct[k+1:n, k:]` from the left, and then to the block of columns `A_correct[:, k+1:n]` from the right. This correctly implements $A \\leftarrow Q_k A Q_k$.\n5.  For `A_faulty`, only the trailing submatrix `A_faulty[k+1:n, k+1:n]` is updated via `Hk @ submatrix @ Hk`.\n6.  Finally, a counting function inspects both resulting matrices. It tallies the number of elements $T_{i,j}$ for which $|i-j| > 1$ and $|T_{i,j}| > \\varepsilon = 10^{-10}$.\n\nThe correct algorithm is expected to produce a count of $0$ for all test cases, as all elements outside the tridiagonal band should be numerically zero. The faulty algorithm is expected to leave non-zero elements in these positions for matrices that are not already tridiagonal.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef count_off_tridiagonal(T, eps):\n    \"\"\"\n    Counts the number of entries T_ij with |i-j| > 1 and |T_ij| > eps.\n    \"\"\"\n    n = T.shape[0]\n    count = 0\n    for i in range(n):\n        for j in range(n):\n            if abs(i - j) > 1 and abs(T[i, j]) > eps:\n                count += 1\n    return count\n\ndef correct_tridiagonalization(A_in):\n    \"\"\"\n    Performs Householder tridiagonalization using the correct similarity transformation.\n    \"\"\"\n    A = A_in.copy()\n    n = A.shape[0]\n\n    for k in range(n - 2):\n        # 1. Define the vector x\n        x = A[k+1:n, k].copy()\n        m = x.shape[0]\n        if m == 0:\n            continue\n        \n        norm_x = np.linalg.norm(x)\n        # If the sub-column is already zero, reflector is identity.\n        if norm_x < 1e-15:\n            continue\n        \n        # 2. Construct the Householder reflector H_k for x\n        # Use copysign for sign(0)=1 behavior, ensuring stability\n        sign_x0 = np.copysign(1.0, x[0]) if x[0] != 0 else 1.0\n        alpha = -sign_x0 * norm_x\n        \n        u = x.copy()\n        u[0] -= alpha\n        \n        norm_u = np.linalg.norm(u)\n        if norm_u < 1e-15:\n            continue\n        v = u / norm_u\n        \n        Hk = np.eye(m) - 2 * np.outer(v, v)\n        \n        # 3. Apply the full similarity transformation A <-- Q_k * A * Q_k^T\n        # This is done by applying H_k to the relevant sub-blocks of A.\n        \n        # Apply H_k from the left: A[k+1:n, :] <-- Hk @ A[k+1:n, :]\n        sub_block_rows = A[k+1:n, k:]\n        A[k+1:n, k:] = Hk @ sub_block_rows\n        \n        # Apply H_k from the right: A[:, k+1:n] <-- A[:, k+1:n] @ Hk\n        sub_block_cols = A[:, k+1:n]\n        A[:, k+1:n] = sub_block_cols @ Hk\n        \n    return A\n\ndef faulty_tridiagonalization(A_in):\n    \"\"\"\n    Performs a faulty Householder tridiagonalization, updating only A_22.\n    \"\"\"\n    A = A_in.copy()\n    n = A.shape[0]\n\n    for k in range(n - 2):\n        # 1. Define the vector x\n        x = A[k+1:n, k].copy()\n        m = x.shape[0]\n        if m == 0:\n            continue\n        \n        norm_x = np.linalg.norm(x)\n        if norm_x < 1e-15:\n            continue\n        \n        # 2. Construct the Householder reflector H_k for x\n        sign_x0 = np.copysign(1.0, x[0]) if x[0] != 0 else 1.0\n        alpha = -sign_x0 * norm_x\n        \n        u = x.copy()\n        u[0] -= alpha\n        \n        norm_u = np.linalg.norm(u)\n        if norm_u < 1e-15:\n            continue\n        v = u / norm_u\n        \n        Hk = np.eye(m) - 2 * np.outer(v, v)\n        \n        # 3. Apply faulty update: only the trailing submatrix A_22 is updated.\n        sub_A22 = A[k+1:n, k+1:n]\n        A[k+1:n, k+1:n] = Hk @ sub_A22 @ Hk\n    \n    return A\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        np.array([\n            [6, -2, 3, 0, 1],\n            [-2, 5, 2, -1, 4],\n            [3, 2, 4, 2, 0],\n            [0, -1, 2, 3, -2],\n            [1, 4, 0, -2, 7]\n        ], dtype=float),\n        np.array([\n            [2, -1],\n            [-1, 3]\n        ], dtype=float),\n        np.array([\n            [5]\n        ], dtype=float),\n        np.array([\n            [4, 1, 0, 0, 0, 0],\n            [1, 5, -2, 0, 0, 0],\n            [0, -2, 6, 3, 0, 0],\n            [0, 0, 3, 7, -4, 0],\n            [0, 0, 0, -4, 8, 5],\n            [0, 0, 0, 0, 5, 9]\n        ], dtype=float),\n        np.array([\n            [10, 2, 0, 1e-12, 0],\n            [2, 9, -1, 0, 1e-12],\n            [0, -1, 8, 3, 0],\n            [1e-12, 0, 3, 7, 2],\n            [0, 1e-12, 0, 2, 6]\n        ], dtype=float)\n    ]\n\n    results = []\n    eps = 1e-10\n\n    for A in test_cases:\n        # Run faulty implementation\n        A_faulty_result = faulty_tridiagonalization(A)\n        count_faulty = count_off_tridiagonal(A_faulty_result, eps)\n\n        # Run corrected implementation\n        A_correct_result = correct_tridiagonalization(A)\n        count_correct = count_off_tridiagonal(A_correct_result, eps)\n        \n        results.append(f\"[{count_faulty},{count_correct}]\")\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3239695"}]}