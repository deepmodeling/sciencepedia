{"hands_on_practices": [{"introduction": "Numerical differentiation embodies a classic engineering trade-off. While the definition of a derivative suggests using an infinitesimally small step size $h$, the finite precision of computer arithmetic introduces round-off errors that become catastrophic as $h$ shrinks. This exercise guides you through the fundamental process of balancing these two competing error sources—truncation error from the approximation formula and round-off error from the machine—to derive an analytical expression for the optimal step size $h_{opt}$ that minimizes the total error [@problem_id:2167887].", "problem": "In the field of numerical analysis, a fundamental challenge is to balance truncation error (from mathematical approximation) and round-off error (from finite-precision hardware). Consider the task of computing the derivative of the function $f(x) = \\sin(x)$ at an arbitrary point $x=a$ using the central difference formula.\n\nThe total error, $E_{total}(h)$, as a function of the step size $h$, is modeled as the sum of the magnitudes of these two error types.\nThe magnitude of the truncation error for the central difference method is given by the expression:\n$$|E_{trunc}(h)| = \\frac{h^2}{6} |f'''(a)|$$\nThe magnitude of the round-off error, which results from the limitations of floating-point arithmetic, is modeled as:\n$$|E_{round}(h)| = \\frac{\\epsilon}{h}$$\nHere, $\\epsilon$ is a small, positive constant that represents the upper bound on the absolute error of a single function evaluation.\n\nThe total error is thus approximated by the sum $E_{total}(h) = |E_{trunc}(h)| + |E_{round}(h)|$. Your task is to find the optimal step size, $h_{opt}$, that minimizes this total error.\n\nProvide your answer as a closed-form analytic expression in terms of the parameters $a$ and $\\epsilon$. You may assume that $\\cos(a) \\neq 0$.", "solution": "We are given the function $f(x) = \\sin(x)$ and the central difference total error model\n$$E_{total}(h) = |E_{trunc}(h)| + |E_{round}(h)| = \\frac{h^{2}}{6}\\,|f'''(a)| + \\frac{\\epsilon}{h}.$$\nFor $f(x) = \\sin(x)$, the third derivative is $f'''(x) = -\\cos(x)$, hence $|f'''(a)| = |\\cos(a)|$. Therefore,\n$$E_{total}(h) = \\frac{h^{2}}{6}\\,|\\cos(a)| + \\frac{\\epsilon}{h}, \\quad h>0.$$\nTo find the minimizing step size, differentiate with respect to $h$ and set the derivative to zero:\n$$\\frac{dE_{total}}{dh} = \\frac{2h}{6}\\,|\\cos(a)| - \\frac{\\epsilon}{h^{2}} = \\frac{h}{3}\\,|\\cos(a)| - \\frac{\\epsilon}{h^{2}}.$$\nSetting $\\frac{dE_{total}}{dh}=0$ gives\n$$\\frac{h}{3}\\,|\\cos(a)| = \\frac{\\epsilon}{h^{2}} \\quad \\Longrightarrow \\quad \\frac{|\\cos(a)|}{3}\\,h^{3} = \\epsilon \\quad \\Longrightarrow \\quad h^{3} = \\frac{3\\epsilon}{|\\cos(a)|}.$$\nThus,\n$$h_{opt} = \\left(\\frac{3\\epsilon}{|\\cos(a)|}\\right)^{\\frac{1}{3}}.$$\nTo verify this is a minimum, compute the second derivative:\n$$\\frac{d^{2}E_{total}}{dh^{2}} = \\frac{1}{3}\\,|\\cos(a)| + \\frac{2\\epsilon}{h^{3}},$$\nwhich is strictly positive for $h>0$, confirming that the critical point is a minimizer. Therefore, the optimal step size is\n$$h_{opt} = \\left(\\frac{3\\epsilon}{|\\cos(a)|}\\right)^{\\frac{1}{3}}.$$", "answer": "$$\\boxed{\\left(\\frac{3\\epsilon}{|\\cos(a)|}\\right)^{\\frac{1}{3}}}$$", "id": "2167887"}, {"introduction": "Having established the general trade-off between truncation and round-off errors, we now focus on a primary culprit of numerical instability: subtractive cancellation. This practice [@problem_id:3269410] demonstrates how a seemingly benign formula for approximating the derivative of $f(x)=\\sqrt{x}$ can lead to a disastrous loss of precision for small step sizes. By algebraically reformulating the difference quotient, you will see how to construct a mathematically equivalent expression that is far more robust in floating-point arithmetic, transforming a numerically unstable calculation into a stable one.", "problem": "Consider the numerical approximation of the derivative of the function $f(x)=\\sqrt{x}$ at the point $x=1$ using a forward difference. Let the floating-point arithmetic obey the standard model $\\,\\mathrm{fl}(a \\,\\circ\\, b) = (a \\,\\circ\\, b)(1+\\delta)$ with $|\\delta|\\leq u$ for each elementary operation $\\,\\circ\\,$ (addition, subtraction, multiplication, division), and let square root satisfy $\\mathrm{fl}(\\sqrt{z})=\\sqrt{z}\\,(1+\\delta)$ with $|\\delta|\\leq u$. Here $u$ denotes the unit roundoff. Assume computations are performed in Institute of Electrical and Electronics Engineers (IEEE) 754 double precision, for which $u=2^{-53}$ and the spacing at $x=1$ is $2^{-52}$, so that $\\mathrm{fl}(1+h)>1$ if and only if $h\\geq 2^{-52}$.\n\nYou will analyze two algebraically equivalent implementations of the forward difference quotient for $f$ at $x=1$:\n- the naive difference quotient, computed as $(\\sqrt{1+h}-\\sqrt{1})/h$, and\n- an algebraically transformed quotient that avoids subtracting nearly equal numbers.\n\nStarting only from the definition of the derivative, basic Taylor expansion facts for smooth functions, and the floating-point model above, do the following:\n- By purely algebraic manipulation, derive an equivalent form of the forward difference quotient that eliminates the direct subtraction of $\\sqrt{1+h}$ and $\\sqrt{1}$.\n- Determine the leading-order truncation error in $h$ of the forward difference approximation to $f'(1)$.\n- Using first-order rounding-error propagation under the floating-point model, obtain leading-order absolute round-off error models for both implementations as functions of $h$ and $u$. Your models should make clear which implementation suffers an $O(u/h)$ amplification and which yields an $O(u)$ contribution.\n- Combine truncation and round-off terms to form a leading-order total absolute error model for the naive implementation, and determine the step size $h_{\\ast}$ that minimizes this total error in terms of $u$.\n- Finally, specialize to IEEE 754 double precision with $u=2^{-53}$ and report the numerical value of $h_{\\ast}$ in scientific notation. Round your answer to four significant figures. Do not include units.", "solution": "The task is to analyze the forward difference approximation of the derivative of $f(x)=\\sqrt{x}$ at $x=1$. The exact derivative is $f'(x) = \\frac{1}{2\\sqrt{x}}$, so at $x=1$, the exact value is $f'(1) = \\frac{1}{2}$.\n\n### Equivalent Form of the Difference Quotient\nThe naive forward difference quotient is given by\n$$\nD_h = \\frac{f(1+h) - f(1)}{h} = \\frac{\\sqrt{1+h} - \\sqrt{1}}{h} = \\frac{\\sqrt{1+h} - 1}{h}\n$$\nFor small positive $h$, the numerator involves the subtraction of two nearly equal numbers, $\\sqrt{1+h} \\approx 1$, a situation known as subtractive cancellation which leads to a loss of relative precision in floating-point arithmetic. To derive an algebraically equivalent form that avoids this, we multiply the numerator and denominator by the conjugate of the numerator, which is $\\sqrt{1+h} + 1$:\n$$\nD_h = \\frac{\\sqrt{1+h} - 1}{h} \\times \\frac{\\sqrt{1+h} + 1}{\\sqrt{1+h} + 1} = \\frac{(\\sqrt{1+h})^2 - 1^2}{h(\\sqrt{1+h} + 1)} = \\frac{(1+h) - 1}{h(\\sqrt{1+h} + 1)} = \\frac{h}{h(\\sqrt{1+h} + 1)}\n$$\nFor $h \\neq 0$, we can cancel $h$ from the numerator and denominator to obtain the stabilized formula:\n$$\nD_h = \\frac{1}{\\sqrt{1+h} + 1}\n$$\nThis form is equivalent to the original for exact arithmetic. In floating-point arithmetic, it is numerically stable because it only involves additions and avoids the subtraction of nearly equal quantities.\n\n### Truncation Error\nThe truncation error, $E_{\\text{trunc}}$, is the error inherent in the approximation formula, assuming exact arithmetic. It is defined as $E_{\\text{trunc}}(h) = D_h - f'(1)$. We use the Taylor expansion of $f(1+h)$ around $h=0$. For $f(x) = \\sqrt{x}$, the derivatives are $f'(x) = \\frac{1}{2}x^{-1/2}$ and $f''(x) = -\\frac{1}{4}x^{-3/2}$.\nAt $x=1$, we have $f(1)=1$, $f'(1)=\\frac{1}{2}$, and $f''(1)=-\\frac{1}{4}$.\nThe Taylor expansion of $f(1+h)$ is:\n$$\nf(1+h) = f(1) + f'(1)h + \\frac{f''(1)}{2!}h^2 + O(h^3) = 1 + \\frac{1}{2}h - \\frac{1}{8}h^2 + O(h^3)\n$$\nSubstituting this into the naive difference quotient formula:\n$$\nD_h = \\frac{(1 + \\frac{1}{2}h - \\frac{1}{8}h^2 + O(h^3)) - 1}{h} = \\frac{\\frac{1}{2}h - \\frac{1}{8}h^2 + O(h^3)}{h} = \\frac{1}{2} - \\frac{1}{8}h + O(h^2)\n$$\nThe truncation error is then:\n$$\nE_{\\text{trunc}}(h) = D_h - f'(1) = \\left(\\frac{1}{2} - \\frac{1}{8}h + O(h^2)\\right) - \\frac{1}{2} = -\\frac{1}{8}h + O(h^2)\n$$\nThe leading-order truncation error is $-\\frac{1}{8}h$.\n\n### Round-off Error Models\nWe analyze the propagation of round-off errors for both implementations under the given floating-point model. Let $u$ be the unit roundoff.\n\n**Naive Implementation:** $D_h = (\\sqrt{1+h}-1)/h$.\nThe computed approximation, $\\hat{D}_{\\text{naive}}$, is approximately\n$$\n\\hat{D}_{\\text{naive}} \\approx \\frac{( \\sqrt{1+h}(1+\\delta_1) - 1 )(1+\\delta_2)}{h} (1+\\delta_3)\n$$\nwhere $|\\delta_i| \\le u$ represent the errors from square root, subtraction, and division, respectively. A more detailed first-order error analysis shows the error in computing the numerator, $N = \\sqrt{1+h}-1$, is dominated by the initial evaluation of the function values. The computed numerator $\\hat{N}$ has an absolute error $|\\hat{N}-N|$ which is roughly proportional to the magnitude of the numbers being subtracted, not their difference. Since $\\sqrt{1+h} \\approx 1$, the absolute error in the numerator is on the order of $u \\cdot |1| = u$. A more careful analysis bounds the numerator error by approximately $2u$. When this error is divided by the small step size $h$, it gets magnified. The absolute round-off error, $E_{\\text{round, naive}}$, is therefore dominated by this term:\n$$\n|E_{\\text{round, naive}}| \\approx \\frac{C \\cdot u}{h}\n$$\nfor some constant $C$ of order $1$. This is the characteristic $O(u/h)$ error amplification due to subtractive cancellation.\n\n**Stable Implementation:** $D_h = 1/(\\sqrt{1+h}+1)$.\nThe computed approximation, $\\hat{D}_{\\text{stable}}$, is\n$$\n\\hat{D}_{\\text{stable}} \\approx \\frac{1}{(\\sqrt{1+h}(1+\\delta_1) + 1)(1+\\delta_2)} (1+\\delta_3)\n$$\nHere, the denominator is approximately $2$. The operations are addition of positive numbers and square root, which are numerically benign. The absolute error in computing the denominator, $Den = \\sqrt{1+h}+1$, is on the order of $u \\cdot |Den| \\approx 2u$. When we take the reciprocal, the relative error is preserved. Thus, the absolute error in the final result is approximately $u \\cdot |D_h| \\approx u/2$. The leading-order absolute round-off error is:\n$$\n|E_{\\text{round, stable}}| \\approx C' \\cdot u\n$$\nfor some constant $C'$ of order $1$. This implementation avoids error amplification, yielding an error contribution of order $O(u)$.\n\n### Optimal Step Size for Naive Implementation\nThe total absolute error for the naive implementation, $|E_{\\text{total}}|$, is the sum of the magnitudes of the truncation and round-off errors.\n$$\n|E_{\\text{total}}(h)| \\approx |E_{\\text{trunc}}(h)| + |E_{\\text{round, naive}}(h)|\n$$\nUsing the leading-order terms derived previously, we have:\n$$\n|E_{\\text{trunc}}(h)| = |-\\frac{1}{8}h| = \\frac{h}{8}\n$$\nFor the round-off error, a standard model uses $|E_{\\text{round}}| \\approx \\frac{\\epsilon(|f(x+h)|+|f(x)|)}{h}$, where $\\epsilon$ is the machine epsilon, which is $2u$. For this problem, it is more direct to say the error in the numerator $f(1+h)-f(1)$ is bounded by $u|f(1+h)|+u|f(1)| \\approx u(1+1) = 2u$. Thus,\n$$\n|E_{\\text{round, naive}}(h)| \\approx \\frac{2u}{h}\n$$\nThe total error function to be minimized is:\n$$\nE(h) = \\frac{h}{8} + \\frac{2u}{h}\n$$\nTo find the step size $h_{\\ast}$ that minimizes this error, we differentiate $E(h)$ with respect to $h$ and set the derivative to zero:\n$$\n\\frac{dE}{dh} = \\frac{1}{8} - \\frac{2u}{h^2} = 0\n$$\nSolving for $h$:\n$$\n\\frac{1}{8} = \\frac{2u}{h^2} \\implies h^2 = 16u \\implies h = \\sqrt{16u} = 4\\sqrt{u}\n$$\nSo, the optimal step size is $h_{\\ast} = 4\\sqrt{u}$.\n\n### Numerical Value of $h_{\\ast}$\nFor IEEE 754 double precision, the unit roundoff is $u = 2^{-53}$. We substitute this value into our expression for $h_{\\ast}$:\n$$\nh_{\\ast} = 4\\sqrt{2^{-53}} = 2^2 \\times (2^{-53})^{1/2} = 2^2 \\times 2^{-53/2} = 2^{2 - 26.5} = 2^{-24.5}\n$$\nTo find the numerical value, we compute:\n$$\nh_{\\ast} = 2^{-24.5} = \\frac{1}{2^{24.5}} = \\frac{1}{2^{24}\\sqrt{2}} = \\frac{1}{16777216\\sqrt{2}}\n$$\nUsing a calculator, $h_{\\ast} \\approx 4.2146301... \\times 10^{-8}$.\nRounding to four significant figures, we get:\n$$\nh_{\\ast} \\approx 4.215 \\times 10^{-8}\n$$\nThis value is much larger than the machine epsilon relative to $1$, which the problem notes is $2^{-52} \\approx 2.22 \\times 10^{-16}$, so the floating point number $\\mathrm{fl}(1+h_{\\ast})$ is indeed greater than $1$, validating a key assumption of the analysis.", "answer": "$$\n\\boxed{4.215 \\times 10^{-8}}\n$$", "id": "3269410"}, {"introduction": "This final practice synthesizes theory and computation to tackle a more complex and realistic scenario. Here, not only does the differentiation formula suffer from round-off error, but the function being differentiated, $f(x) = (\\exp(x) - 1)/x$, is itself prone to catastrophic cancellation near $x=0$. You will implement and compare a naive evaluator with a stabilized version that leverages specialized functions, empirically discovering the optimal step size and validating the theoretical error models you've learned [@problem_id:3269491]. This exercise highlights the critical importance of both choosing stable algorithms and using high-quality numerical libraries in scientific computing.", "problem": "Consider the function $f(x) = \\frac{\\exp(x) - 1}{x}$ and the task of numerically approximating its derivative near $x = 0$ using a symmetric central difference. The function $f(x)$ is smooth but susceptible to catastrophic cancellation when $x$ is small, because subtracting $1$ from $\\exp(x)$ eliminates leading digits that are relevant for accuracy. This numerical vulnerability interacts with the derivative approximation, which itself involves subtracting two nearly equal values. You must determine the step size $h$ that minimizes the total error in the numerical differentiation, where the total error arises from a combination of truncation error due to Taylor series approximation and round-off error due to finite precision arithmetic governed by the Institute of Electrical and Electronics Engineers (IEEE) 754 standard.\n\nStarting from core definitions and well-tested facts, derive the exact value of $f'(0)$ using a power series argument. Then, using Taylor expansions around $x = 0$, derive the leading-order truncation error term for the symmetric central difference applied to $f$ at $x = 0$. Model the effect of floating-point rounding in the IEEE 754 sense with a unit roundoff $\\epsilon_{\\text{mach}}$ so that evaluating $f(h)$ and $f(-h)$ and then subtracting them contributes a round-off term that scales like $\\epsilon_{\\text{mach}}$ divided by $h$, up to a method-dependent constant factor. Combine these into a single error model that depends on $h$ and minimize it to produce a theoretical optimal step size $h^\\star$ for binary64 precision.\n\nYour program must implement and compare two evaluators for $f$:\n1. A naive evaluator $f_{\\text{naive}}(x) = \\frac{\\exp(x) - 1}{x}$ using the standard exponential function.\n2. A stabilized evaluator $f_{\\text{stab}}(x) = \\frac{\\mathrm{expm1}(x)}{x}$, where $\\mathrm{expm1}(x)$ computes $\\exp(x) - 1$ with high relative accuracy for small $x$.\n\nUse the symmetric central difference $D(h) = \\frac{f(h) - f(-h)}{2h}$ to approximate $f'(0)$. For each evaluator, search over the candidate set of step sizes $H = \\{10^{-1}, 10^{-2}, \\ldots, 10^{-16}\\}$ in binary64 arithmetic to find the $h$ that minimizes the absolute error $|D(h) - f'(0)|$. Also, compute the theoretical optimal step size $h^\\star$ in binary64 using your derived error model with a method-dependent constant factor taken to be unity. Finally, as a boundary condition exploring precision effects, repeat the naive evaluation using binary32 arithmetic over the same candidate set $H$ (cast to binary32) and find the minimizing $h$.\n\nTest Suite:\n- Case 1: Binary64, naive evaluator, search over $H$ to find the $h$ that minimizes $|D(h) - f'(0)|$.\n- Case 2: Binary64, stabilized evaluator, search over $H$ to find the $h$ that minimizes $|D(h) - f'(0)|$.\n- Case 3: Binary64, theoretical $h^\\star$ from your derived model using the binary64 unit roundoff $\\epsilon_{\\text{mach}}$.\n- Case 4: Binary32, naive evaluator, search over $H$ (cast to binary32) to find the $h$ that minimizes $|D(h) - f'(0)|$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the above cases. Each result must be a floating-point number. For example: \"[h_case1,h_case2,h_case3,h_case4]\". No physical units or angles are involved in this problem, so no unit conversion is required. The program must be self-contained and require no user input.", "solution": "### Theoretical Analysis\n\n#### 2.1: Exact Derivative $f'(0)$\n\nThe function $f(x) = \\frac{\\exp(x) - 1}{x}$ has a removable singularity at $x=0$. To find its behavior at this point, we use the Maclaurin series expansion for $\\exp(x)$:\n$$\n\\exp(x) = \\sum_{k=0}^{\\infty} \\frac{x^k}{k!} = 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\dots\n$$\nSubtracting $1$ and dividing by $x$ gives the series for $f(x)$:\n$$\nf(x) = \\frac{1}{x} \\left( \\sum_{k=1}^{\\infty} \\frac{x^k}{k!} \\right) = \\sum_{k=1}^{\\infty} \\frac{x^{k-1}}{k!} = 1 + \\frac{x}{2!} + \\frac{x^2}{3!} + \\frac{x^3}{4!} + \\dots\n$$\nThis series representation is valid for all $x$, and we can define $f(0) = 1$ to make the function continuous and infinitely differentiable everywhere. To find the derivative $f'(x)$, we differentiate the series term-by-term:\n$$\nf'(x) = \\frac{d}{dx} \\left( \\sum_{k=0}^{\\infty} \\frac{x^k}{(k+1)!} \\right) = \\sum_{k=1}^{\\infty} \\frac{k x^{k-1}}{(k+1)!} = \\frac{1}{2!} + \\frac{2x}{3!} + \\frac{3x^2}{4!} + \\dots\n$$\nEvaluating this at $x=0$, all terms except the first vanish, yielding the exact value of the derivative:\n$$\nf'(0) = \\frac{1}{2!} = \\frac{1}{2}\n$$\n\n#### 2.2: Truncation Error\n\nThe symmetric central difference approximation for $f'(x)$ is $D(h) = \\frac{f(x+h) - f(x-h)}{2h}$. The error of this approximation is called the truncation error. It can be derived from the Taylor expansion of $f(x+h)$ and $f(x-h)$ around $x$:\n$$\nf(x+h) = f(x) + hf'(x) + \\frac{h^2}{2}f''(x) + \\frac{h^3}{6}f'''(x) + O(h^4)\n$$\n$$\nf(x-h) = f(x) - hf'(x) + \\frac{h^2}{2}f''(x) - \\frac{h^3}{6}f'''(x) + O(h^4)\n$$\nSubtracting the second from the first gives:\n$$\nf(x+h) - f(x-h) = 2hf'(x) + \\frac{h^3}{3}f'''(x) + O(h^5)\n$$\nDividing by $2h$ and rearranging, we find the approximation and its error term:\n$$\n\\frac{f(x+h) - f(x-h)}{2h} = f'(x) + \\frac{h^2}{6}f'''(x) + O(h^4)\n$$\nThe leading-order truncation error at $x=0$ is $E_{\\text{trunc}}(h) = \\frac{h^2}{6}f'''(0)$. We find $f'''(0)$ from the series for $f(x)$:\n$$\nf(x) = 1 + \\frac{1}{2}x + \\frac{1}{6}x^2 + \\frac{1}{24}x^3 + \\dots = \\sum_{k=0}^{\\infty} \\frac{f^{(k)}(0)}{k!} x^k\n$$\nBy comparing coefficients, we see $\\frac{f'''(0)}{3!} = \\frac{1}{4!}$, which gives $f'''(0) = \\frac{3!}{4!} = \\frac{1}{4}$.\nTherefore, the truncation error is:\n$$\nE_{\\text{trunc}}(h) = \\frac{h^2}{6} \\cdot \\frac{1}{4} = \\frac{h^2}{24}\n$$\n\n#### 2.3: Round-off Error and Optimal Step Size $h^\\star$\n\nRound-off error arises from the finite precision of floating-point arithmetic. The total error $E(h)$ is a sum of truncation error, which decreases with $h$, and round-off error, which increases as $h$ decreases.\n\nThe problem asks to model the round-off error by assuming the subtraction $f(h) - f(-h)$ is the dominant source. Let $\\tilde{f}(x)$ be the value of $f(x)$ computed in floating-point arithmetic. The error can be modeled as $\\tilde{f}(x) = f(x) + \\delta(x)$, where $|\\delta(x)|$ is bounded. For a stable evaluation method (like using `expm1`), the relative error is small, so $|\\delta(x)| \\approx \\epsilon_{\\text{mach}}|f(x)|$. The computed difference is $(\\tilde{f}(h) - \\tilde{f}(-h))$, which introduces a round-off error of magnitude approximately $|\\delta(h)| + |\\delta(-h)|$. For small $h$, $f(h) \\approx f(0) = 1$ and $f(-h) \\approx f(0) = 1$. The error in the numerator is thus $\\approx 2 \\epsilon_{\\text{mach}}$. The round-off error in the derivative approximation is this numerator error divided by $2h$:\n$$\n|E_{\\text{round}}(h)| \\approx \\frac{2 \\epsilon_{\\text{mach}}}{2h} = \\frac{\\epsilon_{\\text{mach}}}{h}\n$$\nThis matches the problem's specified model, where the method-dependent constant is $1$.\n\nThe total error is the sum of the magnitudes of the truncation and round-off errors:\n$$\nE(h) = |E_{\\text{trunc}}(h)| + |E_{\\text{round}}(h)| = \\frac{h^2}{24} + \\frac{\\epsilon_{\\text{mach}}}{h}\n$$\nTo find the step size $h^\\star$ that minimizes this total error, we differentiate $E(h)$ with respect to $h$ and set the result to zero:\n$$\n\\frac{dE}{dh} = \\frac{2h}{24} - \\frac{\\epsilon_{\\text{mach}}}{h^2} = \\frac{h}{12} - \\frac{\\epsilon_{\\text{mach}}}{h^2}\n$$\nSetting $\\frac{dE}{dh} = 0$ gives:\n$$\n\\frac{h^\\star}{12} = \\frac{\\epsilon_{\\text{mach}}}{(h^\\star)^2} \\implies (h^\\star)^3 = 12 \\epsilon_{\\text{mach}}\n$$\nThus, the theoretical optimal step size is:\n$$\nh^\\star = \\sqrt[3]{12 \\epsilon_{\\text{mach}}}\n$$\nThis model is most appropriate for the stabilized evaluator, where the evaluation of $f(x)$ itself is accurate. For the naive evaluator, the catastrophic cancellation in $\\exp(x)-1$ for small $x$ introduces a much larger round-off error, which scales as $\\epsilon_{\\text{mach}}/h^2$, leading to a different optimal $h$. The problem asks for a single theoretical $h^\\star$, which corresponds to this \"best case\" scenario. For binary64, $\\epsilon_{\\text{mach}} = 2^{-52}$.\n\n### Computational Implementation\n\nThe following steps are implemented in the Python code:\n1.  **Exact Value and Search Space**: The exact value $f'(0)=0.5$ is defined. The search space $H = \\{10^{-1}, 10^{-2}, \\ldots, 10^{-16}\\}$ is created.\n2.  **Evaluators**: Two functions for $f(x)$, $f_{\\text{naive}}(x) = (\\exp(x) - 1)/x$ and $f_{\\text{stab}}(x) = \\mathrm{expm1}(x)/x$, are defined.\n3.  **Numerical Search**:\n    *   **Case 1 (binary64, naive)**: The derivative is computed for each $h \\in H$ using $f_{\\text{naive}}$ in `float64` precision. The $h$ that minimizes the absolute error $|D(h) - f'(0)|$ is found.\n    *   **Case 2 (binary64, stabilized)**: The process is repeated using $f_{\\text{stab}}$ in `float64` precision.\n    *   **Case 4 (binary32, naive)**: The process is repeated using $f_{\\text{naive}}$ but with all calculations and data types cast to `float32`. This demonstrates the effect of lower precision.\n4.  **Theoretical Calculation**:\n    *   **Case 3 (binary64, theoretical)**: $h^\\star$ is calculated using the derived formula $h^\\star = \\sqrt[3]{12 \\epsilon_{\\text{mach}}}$ with the binary64 machine epsilon.\n\nThese four results are then collected and printed in the specified format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the numerical differentiation problem as specified.\n    Derives the optimal step size h for approximating f'(0) where f(x) = (exp(x)-1)/x.\n    Compares theoretical and empirical results for naive and stabilized evaluators in\n    binary64 and binary32 precision.\n    \"\"\"\n\n    # --- Setup ---\n    # The exact value of the derivative f'(0) is 1/2.\n    f_prime_0_exact_64 = np.float64(0.5)\n    f_prime_0_exact_32 = np.float32(0.5)\n\n    # Candidate set of step sizes H.\n    h_values = np.logspace(-1, -16, 16, dtype=np.float64)\n\n    # Machine epsilon values for binary64 (double) and binary32 (single).\n    eps_64 = np.finfo(np.float64).eps\n    eps_32 = np.finfo(np.float32).eps\n\n    # --- Evaluator Functions ---\n    def f_naive(x):\n        # Naive implementation susceptible to catastrophic cancellation.\n        # Ensure input is float64 for this case.\n        x = np.float64(x)\n        return (np.exp(x) - np.float64(1)) / x\n\n    def f_stab(x):\n        # Stabilized implementation using expm1 for high accuracy at small x.\n        # Ensure input is float64 for this case.\n        x = np.float64(x)\n        return np.expm1(x) / x\n\n    def f_naive_32(x):\n        # Naive implementation explicitly using float32 for all operations.\n        x = np.float32(x)\n        one = np.float32(1)\n        return (np.exp(x) - one) / x\n\n    def central_difference(f_eval, h_vals, dtype):\n        # Symmetric central difference D(h) = (f(h) - f(-h)) / (2h)\n        h = np.array(h_vals, dtype=dtype)\n        two = dtype(2)\n        return (f_eval(h) - f_eval(-h)) / (two * h)\n\n    results = []\n\n    # --- Case 1: Binary64, naive evaluator ---\n    d_approx_naive_64 = central_difference(f_naive, h_values, np.float64)\n    errors_naive_64 = np.abs(d_approx_naive_64 - f_prime_0_exact_64)\n    min_error_idx = np.argmin(errors_naive_64)\n    h_case1 = h_values[min_error_idx]\n    results.append(h_case1)\n\n    # --- Case 2: Binary64, stabilized evaluator ---\n    d_approx_stab_64 = central_difference(f_stab, h_values, np.float64)\n    errors_stab_64 = np.abs(d_approx_stab_64 - f_prime_0_exact_64)\n    min_error_idx = np.argmin(errors_stab_64)\n    h_case2 = h_values[min_error_idx]\n    results.append(h_case2)\n\n    # --- Case 3: Binary64, theoretical h* ---\n    # Derived optimal h* = (12 * eps_mach)^(1/3)\n    h_case3 = (12 * eps_64)**(1/3)\n    results.append(h_case3)\n\n    # --- Case 4: Binary32, naive evaluator ---\n    h_values_32 = h_values.astype(np.float32)\n    d_approx_naive_32 = central_difference(f_naive_32, h_values_32, np.float32)\n    # Some h values might become 0 in float32, leading to NaN.\n    # We should ignore these results when finding the minimum error.\n    valid_indices = ~np.isnan(d_approx_naive_32)\n    errors_naive_32 = np.abs(d_approx_naive_32[valid_indices] - f_prime_0_exact_32)\n    min_error_idx_32 = np.argmin(errors_naive_32)\n    h_case4 = h_values_32[valid_indices][min_error_idx_32]\n    results.append(h_case4)\n\n    # --- Final Output ---\n    # The final print statement must match the required format exactly.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3269491"}]}