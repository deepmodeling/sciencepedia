{"hands_on_practices": [{"introduction": "This exercise goes to the heart of Richardson extrapolation by tasking you with deriving a crucial formula. You will see how two approximations, generated with different step sizes, can be combined not only to produce a more accurate result but also to estimate the error in your best initial approximation. This principle is the theoretical engine that powers the entire technique [@problem_id:2197911].", "problem": "A computational scientist is using a numerical method to estimate a physical quantity, whose true but unknown value is $A_{\\text{true}}$. The numerical method is known to have a convergence order of $p$, where $p$ is a positive integer. This means that for a sufficiently small step size $h$, the computed approximation $A(h)$ is related to the true value by the expression:\n\n$A(h) = A_{\\text{true}} + K h^p + O(h^{q})$\n\nwhere $K$ is an unknown constant independent of $h$, and $O(h^{q})$ represents higher-order error terms with $q > p$.\n\nTo improve accuracy and estimate the error, the scientist computes two approximations:\n1. An approximation with a step size $h$, which yields the value $A(h)$.\n2. A more refined approximation with a step size $h/2$, which yields the value $A(h/2)$.\n\nAssuming the higher-order terms $O(h^q)$ are negligible, derive an expression for the error in the more accurate approximation, $A(h/2)$. The error is defined as $E(h/2) = A_{\\text{true}} - A(h/2)$. Your final expression should be in terms of the two computed values, $A(h)$ and $A(h/2)$, and the order of the method, $p$.", "solution": "Given the convergence model and neglecting higher-order terms, write the two approximations as\n$$\nA(h) = A_{\\text{true}} + K h^{p}, \\qquad A\\!\\left(\\frac{h}{2}\\right) = A_{\\text{true}} + K \\left(\\frac{h}{2}\\right)^{p} = A_{\\text{true}} + K h^{p} 2^{-p}.\n$$\nSubtract the second from the first to eliminate $A_{\\text{true}}$:\n$$\nA(h) - A\\!\\left(\\frac{h}{2}\\right) = K h^{p} \\left(1 - 2^{-p}\\right).\n$$\nSolve for $K h^{p}$:\n$$\nK h^{p} = \\frac{A(h) - A\\!\\left(\\frac{h}{2}\\right)}{1 - 2^{-p}}.\n$$\nThe error in the refined approximation is\n$$\nE\\!\\left(\\frac{h}{2}\\right) = A_{\\text{true}} - A\\!\\left(\\frac{h}{2}\\right) = -K h^{p} 2^{-p}.\n$$\nSubstitute the expression for $K h^{p}$ and simplify:\n$$\nE\\!\\left(\\frac{h}{2}\\right) = -2^{-p}\\,\\frac{A(h) - A\\!\\left(\\frac{h}{2}\\right)}{1 - 2^{-p}} = -\\frac{A(h) - A\\!\\left(\\frac{h}{2}\\right)}{2^{p} - 1} = \\frac{A\\!\\left(\\frac{h}{2}\\right) - A(h)}{2^{p} - 1}.\n$$\nThus, expressed solely in terms of $A(h)$, $A(h/2)$, and $p$, the error in the more accurate approximation is given by the stated formula.", "answer": "$$\\boxed{\\frac{A(h/2)-A(h)}{2^{p}-1}}$$", "id": "2197911"}, {"introduction": "With the theoretical formula in hand, let's apply it to a practical scenario involving a first-order numerical method. This problem uses data from a hypothetical aerospace simulation to demonstrate the straightforward application of Richardson extrapolation for the common case where the error is of order $O(h)$. By working through this example, you'll gain confidence in the mechanics of canceling the leading error term to achieve a more precise result [@problem_id:2197893].", "problem": "A team of aerospace engineers is developing a computer simulation to predict the final landing velocity of a probe on a newly discovered exoplanet. The simulation employs a numerical integration algorithm to solve the equations of motion. It is known that the primary numerical method used has a global truncation error of the first order, commonly denoted as $O(h)$, where $h$ is the time step of the simulation.\n\nThe team conducts two simulation runs with different time steps to assess the convergence of their results.\n1.  With a time step of $h_1 = 0.20$ seconds, the simulation predicts a final landing velocity of $V(h_1) = 15.60$ m/s.\n2.  With a reduced time step of $h_2 = 0.10$ seconds, the simulation yields a more refined landing velocity of $V(h_2) = 15.85$ m/s.\n\nTo obtain a more accurate prediction without the computational expense of running another simulation with an even smaller time step, the team decides to use Richardson extrapolation. Apply Richardson extrapolation to the two available velocity estimates to compute an improved estimate of the true landing velocity.\n\nExpress your final answer for the improved velocity in m/s, rounded to four significant figures.", "solution": "Let the true landing velocity be $V$ and the numerical estimate with time step $h$ be $V(h)$. For a first-order method, the global truncation error model is\n$$\nV(h) = V + C h + O(h^{2}),\n$$\nwhere $C$ is an $h$-independent constant. For two step sizes $h_{1}$ and $h_{2}$, we have\n$$\nV(h_{1}) = V + C h_{1} + O(h_{1}^{2}), \\quad V(h_{2}) = V + C h_{2} + O(h_{2}^{2}).\n$$\nNeglecting $O(h^{2})$ terms and eliminating $C$ gives\n$$\nC \\approx \\frac{V(h_{2}) - V(h_{1})}{h_{2} - h_{1}}, \\quad V \\approx V(h_{2}) - C h_{2}.\n$$\nSubstituting $C$ yields the Richardson extrapolation formula for a first-order method:\n$$\nV \\approx V(h_{2}) - h_{2}\\,\\frac{V(h_{2}) - V(h_{1})}{h_{2} - h_{1}} = V(h_{2}) + \\frac{V(h_{2}) - V(h_{1})}{\\frac{h_{1}}{h_{2}} - 1}.\n$$\nWith $h_{1} = 0.20$, $h_{2} = 0.10$, $V(h_{1}) = 15.60$, and $V(h_{2}) = 15.85$, the step ratio is $r = \\frac{h_{1}}{h_{2}} = 2$, so\n$$\nV \\approx 15.85 + \\frac{15.85 - 15.60}{2 - 1} = 15.85 + 0.25 = 16.10.\n$$\nRounded to four significant figures, the improved estimate of the true landing velocity is $16.10$.", "answer": "$$\\boxed{16.10}$$", "id": "2197893"}, {"introduction": "The ultimate test of understanding is to translate a concept into working code. This practice challenges you to implement the full pipeline: first, code the well-known forward Euler method, an $O(h)$ numerical solver for differential equations. Then, you will apply the Richardson extrapolation formula you've practiced to the outputs of your Euler solver, effectively creating a new, more accurate $O(h^2)$ solver from a simpler one [@problem_id:3226253].", "problem": "Consider the initial value problem (IVP) for an ordinary differential equation (ODE) given by $y'(t) = f(t, y(t))$ with initial condition $y(t_0) = y_0$. The forward Euler method arises from the fundamental definition of the derivative as the limit of a finite difference and the use of a first-order Taylor expansion. In practice, the forward Euler method updates an approximation $y_n$ to the exact solution $y(t_n)$ by advancing the solution in steps of size $h$ starting at $t_0$ and ending at a specified final time $T$, yielding an approximation at $T$ that we denote $y_h(T)$. The global discretization error of forward Euler is known to depend linearly on the step size $h$ under standard regularity assumptions on $f$ and $y(t)$.\n\nYour task is to:\n1. Implement a function that computes $y_h(T)$ using the forward Euler method for any given function $f(t,y)$, initial condition $y_0$, initial time $t_0$, final time $T$, and uniform step size $h$, assuming $T - t_0$ is an integer multiple of $h$.\n2. Assume the Euler methodâ€™s approximation at $T$ has an asymptotic error expansion of the form $y_h(T) = y(T) + C h + D h^2 + \\mathcal{O}(h^3)$ for constants $C$ and $D$ that depend on $f$ and the solution but not on $h$. Using the outputs $y_h(T)$ and $y_{h/2}(T)$, derive a linear combination of these two approximations, with constant weights independent of $h$, that cancels the leading $\\mathcal{O}(h)$ error term and yields an $\\mathcal{O}(h^2)$-accurate estimate of $y(T)$. Then implement this Richardson extrapolation estimator in code.\n3. For each test case below, compute the absolute error $\\lvert y_{\\text{extrap}}(T) - y(T) \\rvert$, where $y_{\\text{extrap}}(T)$ is your extrapolated estimate and $y(T)$ is the exact solution at time $T$.\n\nUse the following test suite. In all cases, take $t_0 = 0$ and use the provided $h$ such that $(T - t_0)/h$ is an integer:\n\n- Test 1 (happy path, linear homogeneous ODE): $f(t,y) = y$, $y_0 = 1$, $T = 1$, $h = 0.2$. The exact solution is $y(t) = e^{t}$ evaluated at $t = T$.\n- Test 2 (linear nonhomogeneous ODE): $f(t,y) = y + t$, $y_0 = 0$, $T = 2$, $h = 0.4$. The exact solution is $y(t) = e^{t} - t - 1$ evaluated at $t = T$.\n- Test 3 (nonlinear logistic growth): $f(t,y) = r y \\left(1 - \\frac{y}{K}\\right)$ with parameters $r = 1$ and $K = 10$, $y_0 = 1$, $T = 3$, $h = 0.5$. The exact solution is $y(t) = \\frac{K}{1 + A e^{-r t}}$ with $A = \\frac{K - y_0}{y_0}$, evaluated at $t = T$.\n- Test 4 (edge case, zero derivative): $f(t,y) = 0$, $y_0 = 3$, $T = 1$, $h = 0.5$. The exact solution is the constant function $y(t) = 3$ evaluated at $t = T$.\n\nYour program should:\n- Implement the forward Euler method to compute $y_h(T)$ and $y_{h/2}(T)$ for each test case.\n- Implement the derived Richardson extrapolation estimator using the two approximations $y_h(T)$ and $y_{h/2}(T)$ to obtain an $\\mathcal{O}(h^2)$ estimate at $T$.\n- Compute and record the absolute error for each test case, rounded to ten decimal places.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[0.0123456789,0.0000001234,0.0012340000,0.0000000000]\"), where the entries are the absolute errors for Test 1 through Test 4, each rounded to ten decimal places.", "solution": "The problem is assessed to be valid.\n\n### Step 1: Extract Givens\n- **Problem Type**: Initial Value Problem (IVP) for an Ordinary Differential Equation (ODE).\n- **ODE Form**: $y'(t) = f(t, y(t))$.\n- **Initial Condition**: $y(t_0) = y_0$.\n- **Numerical Method**: Forward Euler method, where $y_{n+1} = y_n + h f(t_n, y_n)$.\n- **Approximation at final time $T$**: $y_h(T)$.\n- **Asymptotic Error Expansion**: $y_h(T) = y(T) + C h + D h^2 + \\mathcal{O}(h^3)$.\n- **Constraint**: $T - t_0$ is an integer multiple of the step size $h$.\n- **Task 1**: Implement a function for the forward Euler method to compute $y_h(T)$.\n- **Task 2**: Derive and implement a Richardson extrapolation estimator for $y(T)$ that is $\\mathcal{O}(h^2)$-accurate, using $y_h(T)$ and $y_{h/2}(T)$.\n- **Task 3**: Compute the absolute error $\\lvert y_{\\text{extrap}}(T) - y(T) \\rvert$ for four test cases.\n- **Initial Time for all cases**: $t_0 = 0$.\n\n- **Test Case 1**:\n  - $f(t,y) = y$\n  - $y_0 = 1$\n  - $T = 1$\n  - $h = 0.2$\n  - Exact solution: $y(t) = e^{t}$\n\n- **Test Case 2**:\n  - $f(t,y) = y + t$\n  - $y_0 = 0$\n  - $T = 2$\n  - $h = 0.4$\n  - Exact solution: $y(t) = e^{t} - t - 1$\n\n- **Test Case 3**:\n  - $f(t,y) = r y \\left(1 - \\frac{y}{K}\\right)$ with $r = 1, K = 10$\n  - $y_0 = 1$\n  - $T = 3$\n  - $h = 0.5$\n  - Exact solution: $y(t) = \\frac{K}{1 + A e^{-r t}}$ with $A = \\frac{K - y_0}{y_0}$\n\n- **Test Case 4**:\n  - $f(t,y) = 0$\n  - $y_0 = 3$\n  - $T = 1$\n  - $h = 0.5$\n  - Exact solution: $y(t) = 3$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective.\n1.  **Scientific or Factual Soundness**: The problem is built upon fundamental concepts in numerical analysis for ODEs. The forward Euler method, its error analysis, and Richardson extrapolation are standard and mathematically sound techniques. The ODEs provided are classic examples used in teaching and research. The problem is free of any scientific or factual errors.\n2.  **Well-Posed**: The problem is well-posed. For each test case, the function $f(t,y)$ is sufficiently smooth (Lipschitz continuous in $y$), which guarantees the existence and uniqueness of a solution to the IVP. The task is clearly defined, and all necessary data (initial conditions, parameters, time intervals) are provided.\n3.  **Objective**: The language is precise and unbiased. The tasks are quantitative and require specific calculations, leaving no room for subjective interpretation.\n4.  **Completeness**: The problem is self-contained. It specifies the ODEs, initial conditions, step sizes, final times, and the exact solutions for error comparison. The constraint that $(T - t_0)/h$ is an integer simplifies implementation and avoids ambiguity.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A complete solution will be provided.\n\n### Principle-Based Solution\nThe solution is developed in three stages: first, the implementation of the forward Euler method; second, the derivation of the Richardson extrapolation formula; and third, the application of these methods to the specified test cases to compute the required errors.\n\n**1. The Forward Euler Method**\nThe forward Euler method is a first-order numerical procedure for solving an initial value problem of the form $y'(t) = f(t, y(t))$ with $y(t_0) = y_0$. It approximates the continuous solution $y(t)$ at discrete time points $t_n = t_0 + n h$, where $h$ is the step size. The method is derived from the first-order Taylor expansion of $y(t_{n+1})$ around $t_n$:\n$y(t_{n+1}) = y(t_n) + h y'(t_n) + \\mathcal{O}(h^2) = y(t_n) + h f(t_n, y(t_n)) + \\mathcal{O}(h^2)$.\nBy ignoring the $\\mathcal{O}(h^2)$ term, we obtain the iterative formula for the approximation $y_n \\approx y(t_n)$:\n$$y_{n+1} = y_n + h f(t_n, y_n)$$\nStarting with the initial condition $y_0$, we can apply this formula iteratively for $n = 0, 1, 2, \\dots, N-1$ where $N = (T-t_0)/h$, to find an approximation $y_N \\approx y(T)$. This defines the function $y_h(T)$.\n\n**2. Richardson Extrapolation**\nRichardson extrapolation is a general technique for improving the accuracy of a numerical approximation. We are given that the approximation $y_h(T)$ from the forward Euler method has an asymptotic error expansion:\n$$y_h(T) = y(T) + C h + D h^2 + \\mathcal{O}(h^3)$$\nHere, $y(T)$ is the exact solution, and $C$ and $D$ are constants that depend on the function $f$ and its derivatives but not on the step size $h$.\n\nIf we compute the approximation again with a halved step size, $h/2$, the formula becomes:\n$$y_{h/2}(T) = y(T) + C \\left(\\frac{h}{2}\\right) + D \\left(\\frac{h}{2}\\right)^2 + \\mathcal{O}(h^3)$$\n$$y_{h/2}(T) = y(T) + \\frac{1}{2} C h + \\frac{1}{4} D h^2 + \\mathcal{O}(h^3)$$\nOur goal is to find a linear combination of $y_h(T)$ and $y_{h/2}(T)$, which we denote $y_{\\text{extrap}}(T)$, that provides a more accurate estimate of $y(T)$. Let $y_{\\text{extrap}}(T) = \\alpha y_h(T) + \\beta y_{h/2}(T)$. Substituting the error expansions:\n$$y_{\\text{extrap}}(T) = \\alpha \\left(y(T) + C h + D h^2\\right) + \\beta \\left(y(T) + \\frac{1}{2} C h + \\frac{1}{4} D h^2\\right) + \\mathcal{O}(h^3)$$\n$$y_{\\text{extrap}}(T) = (\\alpha + \\beta) y(T) + \\left(\\alpha + \\frac{\\beta}{2}\\right) C h + \\left(\\alpha + \\frac{\\beta}{4}\\right) D h^2 + \\mathcal{O}(h^3)$$\nTo obtain an $\\mathcal{O}(h^2)$-accurate estimate for $y(T)$, we require the coefficient of $y(T)$ to be $1$ and the coefficient of the leading error term, $Ch$, to be $0$. This gives a system of two linear equations for $\\alpha$ and $\\beta$:\n1. $\\alpha + \\beta = 1$\n2. $\\alpha + \\frac{\\beta}{2} = 0$\nFrom equation (2), we find $\\alpha = -\\beta/2$. Substituting this into equation (1) yields $-\\beta/2 + \\beta = 1$, which simplifies to $\\beta/2 = 1$, so $\\beta = 2$. Consequently, $\\alpha = -1$.\nThe extrapolated estimator is therefore:\n$$y_{\\text{extrap}}(T) = 2 y_{h/2}(T) - y_h(T)$$\nLet's verify the error of this new estimate:\n$$y_{\\text{extrap}}(T) - y(T) = (2 y_{h/2}(T) - y_h(T)) - y(T)$$\n$$= \\left(2\\left(y(T) + \\frac{1}{2}Ch + \\frac{1}{4}Dh^2\\right) - \\left(y(T) + Ch + Dh^2\\right)\\right) - y(T) + \\mathcal{O}(h^3)$$\n$$= (2y(T) + Ch + \\frac{1}{2}Dh^2) - y(T) - Ch - Dh^2 - y(T) + \\mathcal{O}(h^3)$$\n$$= (2-1-1)y(T) + (1-1)Ch + \\left(\\frac{1}{2}-1\\right)Dh^2 + \\mathcal{O}(h^3) = -\\frac{1}{2} D h^2 + \\mathcal{O}(h^3)$$\nThe error is indeed of order $h^2$, so the method successfully eliminates the leading error term.\n\n**3. Computational Procedure**\nFor each of the four test cases, the following algorithm is applied:\n1.  Define the function $f(t,y)$, initial conditions $y_0, t_0$, final time $T$, and step size $h$.\n2.  Implement a function `forward_euler(f, y0, t0, T, h)` that performs the iterative Euler updates and returns the final approximation at time $T$. The number of steps, $N$, is calculated as the integer `(T - t0) / h`.\n3.  Compute the approximation with the given step size $h$: $A_h = \\text{forward_euler}(f, y_0, t_0, T, h)$.\n4.  Compute the approximation with the halved step size $h/2$: $A_{h/2} = \\text{forward_euler}(f, y_0, t_0, T, h/2)$.\n5.  Calculate the Richardson extrapolated value: $y_{\\text{extrap}}(T) = 2 A_{h/2} - A_h$.\n6.  Calculate the exact solution $y(T)$ using the provided formula for the specific test case.\n7.  Compute the absolute error: $E = \\lvert y_{\\text{extrap}}(T) - y(T) \\rvert$.\n8.  The final result for the test case is this error, rounded to ten decimal places. This process is repeated for all test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes absolute errors for Richardson-extrapolated Euler method solutions\n    for a suite of ODE test cases.\n    \"\"\"\n\n    def forward_euler(f, y0, t0, T, h):\n        \"\"\"\n        Computes the solution of an IVP y'(t) = f(t, y) with y(t0) = y0 at time T\n        using the forward Euler method with step size h.\n        \"\"\"\n        t = t0\n        y = y0\n        \n        # The problem statement guarantees (T - t0) / h is an integer.\n        # Using int() directly is safe, but rounding is more robust for floats.\n        num_steps = int(round((T - t0) / h))\n\n        for _ in range(num_steps):\n            y = y + h * f(t, y)\n            t = t + h\n        \n        return y\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"f\": lambda t, y: y,\n            \"y0\": 1.0,\n            \"t0\": 0.0,\n            \"T\": 1.0,\n            \"h\": 0.2,\n            \"exact_sol\": lambda t: np.exp(t)\n        },\n        {\n            \"f\": lambda t, y: y + t,\n            \"y0\": 0.0,\n            \"t0\": 0.0,\n            \"T\": 2.0,\n            \"h\": 0.4,\n            \"exact_sol\": lambda t: np.exp(t) - t - 1.0\n        },\n        {\n            \"f\": lambda t, y: 1.0 * y * (1.0 - y / 10.0), # r=1, K=10\n            \"y0\": 1.0,\n            \"t0\": 0.0,\n            \"T\": 3.0,\n            \"h\": 0.5,\n            \"exact_sol\": lambda t: 10.0 / (1.0 + ((10.0 - 1.0) / 1.0) * np.exp(-1.0 * t))\n        },\n        {\n            \"f\": lambda t, y: 0.0,\n            \"y0\": 3.0,\n            \"t0\": 0.0,\n            \"T\": 1.0,\n            \"h\": 0.5,\n            \"exact_sol\": lambda t: 3.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        f = case[\"f\"]\n        y0 = case[\"y0\"]\n        t0 = case[\"t0\"]\n        T = case[\"T\"]\n        h = case[\"h\"]\n        exact_sol_func = case[\"exact_sol\"]\n\n        # 1. Compute approximations with step sizes h and h/2\n        y_h = forward_euler(f, y0, t0, T, h)\n        y_h_half = forward_euler(f, y0, t0, T, h / 2.0)\n\n        # 2. Apply Richardson extrapolation\n        y_extrap = 2.0 * y_h_half - y_h\n\n        # 3. Compute the exact solution\n        y_exact = exact_sol_func(T)\n        \n        # 4. Compute the absolute error\n        abs_error = abs(y_extrap - y_exact)\n        \n        results.append(abs_error)\n\n    # Format the results as strings rounded to 10 decimal places\n    # The f-string formatting ensures trailing zeros as in the example.\n    # The rounding prior to formatting correctly handles cases near the rounding boundary.\n    results_str = [f\"{round(res, 10):.10f}\" for res in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```", "id": "3226253"}]}