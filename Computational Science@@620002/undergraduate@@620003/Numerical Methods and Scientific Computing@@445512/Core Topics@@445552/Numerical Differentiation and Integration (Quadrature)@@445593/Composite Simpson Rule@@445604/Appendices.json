{"hands_on_practices": [{"introduction": "This first exercise provides a foundational opportunity to apply the Composite Simpson's 1/3 rule directly. By calculating the integral of a simple trigonometric function, for which the exact value is easily found, you will gain hands-on experience with the mechanics of the formula. This practice is crucial for understanding how the rule works step-by-step and for developing an initial intuition about its remarkable accuracy by computing the absolute error [@problem_id:2210199].", "problem": "The rate of change of an observable quantity $Q$ in a physical system is described by the function $f(t) = V_0 \\cos(\\omega t)$, where $t$ is time. The total change in this quantity over an interval $[a, b]$ is given by the definite integral $\\Delta Q = \\int_{a}^{b} f(t) \\, dt$.\n\nConsider a system with constants $V_0 = 1.0$ unit/s and $\\omega = 1.0$ rad/s. Your task is to analyze the total change in $Q$, denoted $\\Delta Q$ (in units), over the time interval from $t=0$ to $t=\\frac{\\pi}{2}$ seconds.\n\nFirst, calculate the exact value for the total change, $\\Delta Q_{\\text{exact}}$.\nSecond, calculate an approximation for the total change, $\\Delta Q_{\\text{approx}}$, by applying the composite Simpson's 1/3 rule using $n=4$ subintervals. The formula for the composite Simpson's 1/3 rule approximation of an integral $\\int_a^b g(x) \\, dx$ with an even number of subintervals $n$ is given by\n$$S_n = \\frac{h}{3} \\left[ g(x_0) + 4\\sum_{i=1}^{n/2} g(x_{2i-1}) + 2\\sum_{i=1}^{n/2-1} g(x_{2i}) + g(x_n) \\right]$$\nwhere $h = \\frac{b-a}{n}$ and $x_i = a+ih$ for $i=0, 1, \\dots, n$.\n\nFinally, compute the magnitude of the absolute error of the approximation, which is defined as $|\\Delta Q_{\\text{exact}} - \\Delta Q_{\\text{approx}}|$. Report this absolute error as your final answer. Round your final answer to five significant figures.", "solution": "We are given the rate $f(t) = V_{0}\\cos(\\omega t)$ and the total change over $[a,b]$:\n$$\n\\Delta Q=\\int_{a}^{b} f(t)\\,dt.\n$$\nWith $V_{0}=1$ and $\\omega=1$, we have $f(t)=\\cos t$, $a=0$, and $b=\\frac{\\pi}{2}$.\n\nExact value:\nUsing the antiderivative $\\int \\cos t\\,dt=\\sin t$, the Fundamental Theorem of Calculus gives\n$$\n\\Delta Q_{\\text{exact}}=\\int_{0}^{\\frac{\\pi}{2}} \\cos t\\,dt=\\left[\\sin t\\right]_{0}^{\\frac{\\pi}{2}}=\\sin\\!\\left(\\frac{\\pi}{2}\\right)-\\sin(0)=1-0=1.\n$$\n\nComposite Simpsonâ€™s $1/3$ rule with $n=4$:\nHere $h=\\frac{b-a}{n}=\\frac{\\frac{\\pi}{2}-0}{4}=\\frac{\\pi}{8}$ and $x_{i}=a+ih=0+i\\frac{\\pi}{8}$ for $i=0,1,2,3,4$. Thus\n$$\nx_{0}=0,\\quad x_{1}=\\frac{\\pi}{8},\\quad x_{2}=\\frac{\\pi}{4},\\quad x_{3}=\\frac{3\\pi}{8},\\quad x_{4}=\\frac{\\pi}{2}.\n$$\nUsing the composite Simpson formula\n$$\nS_{4}=\\frac{h}{3}\\left[g(x_{0})+4\\sum_{i=1}^{2}g(x_{2i-1})+2\\sum_{i=1}^{1}g(x_{2i})+g(x_{4})\\right],\n$$\nwith $g(t)=\\cos t$, we obtain\n$$\nS_{4}=\\frac{\\pi}{24}\\left[\\cos 0+4\\left(\\cos\\!\\left(\\frac{\\pi}{8}\\right)+\\cos\\!\\left(\\frac{3\\pi}{8}\\right)\\right)+2\\cos\\!\\left(\\frac{\\pi}{4}\\right)+\\cos\\!\\left(\\frac{\\pi}{2}\\right)\\right].\n$$\nUsing exact special values $\\cos 0=1$, $\\cos\\!\\left(\\frac{\\pi}{4}\\right)=\\frac{\\sqrt{2}}{2}$, $\\cos\\!\\left(\\frac{\\pi}{2}\\right)=0$, and numerical approximations $\\cos\\!\\left(\\frac{\\pi}{8}\\right)\\approx 0.9238795325$, $\\cos\\!\\left(\\frac{3\\pi}{8}\\right)\\approx 0.3826834324$, we compute\n$$\nS_{4}\\approx \\frac{\\pi}{24}\\left[1+4(0.9238795325+0.3826834324)+2\\cdot 0.7071067812+0\\right]\\approx 1.000134584974194.\n$$\n\nAbsolute error:\n$$\n\\left|\\Delta Q_{\\text{exact}}-\\Delta Q_{\\text{approx}}\\right|=\\left|1-1.000134584974194\\right|\\approx 0.000134584974194.\n$$\nRounding to five significant figures gives $1.3458\\times 10^{-4}$.", "answer": "$$\\boxed{1.3458 \\times 10^{-4}}$$", "id": "2210199"}, {"introduction": "Moving beyond direct application, this practice delves into the predictive power of numerical analysis. Instead of just calculating an approximation, you will use the error bound formula for the Composite Simpson's rule to determine the necessary number of subintervals, $n$, to guarantee a specific level of accuracy. This skill is essential for planning efficient and reliable numerical computations, ensuring that the result meets a required tolerance without unnecessary computational cost [@problem_id:2210241].", "problem": "In numerical analysis, the definite integral of a function is often approximated using quadrature rules when an analytical solution is difficult or impossible to find. One such method is the composite Simpson's 1/3 rule.\n\nConsider the task of approximating the value of the integral $I = \\int_1^2 \\frac{1}{x} \\, dx$.\n\nThe absolute error, $E_S$, of the composite Simpson's 1/3 rule for approximating $\\int_a^b f(x) \\, dx$ with an even number of subintervals, $n$, is bounded by the inequality:\n$$|E_S| \\leq \\frac{(b-a)^5}{180n^4} M_4$$\nwhere $M_4$ is an upper bound for the absolute value of the fourth derivative of the function $f(x)$ on the interval $[a, b]$; that is, $|f^{(4)}(x)| \\leq M_4$ for all $x \\in [a, b]$.\n\nDetermine the minimum even integer value of $n$ that is required to guarantee that the approximation of the integral $I$ using the composite Simpson's 1/3 rule has an absolute error less than $10^{-5}$.", "solution": "We approximate $I=\\int_{1}^{2}\\frac{1}{x}\\,dx$ using the composite Simpson's $1/3$ rule with an even number of subintervals, $n$. The error bound is\n$$\n|E_{S}|\\leq \\frac{(b-a)^{5}}{180\\,n^{4}}\\,M_{4},\n$$\nwhere $M_{4}$ satisfies $|f^{(4)}(x)|\\leq M_{4}$ for $x\\in[1,2]$ and $f(x)=\\frac{1}{x}$.\n\nCompute derivatives:\n$$\nf(x)=x^{-1},\\quad f'(x)=-x^{-2},\\quad f''(x)=2x^{-3},\\quad f^{(3)}(x)=-6x^{-4},\\quad f^{(4)}(x)=24x^{-5}.\n$$\nThus $|f^{(4)}(x)|=24x^{-5}$, which is decreasing on $[1,2]$, so its maximum occurs at $x=1$. Hence\n$$\nM_{4}=24.\n$$\n\nWith $a=1$ and $b=2$, we have $b-a=1$, so the error bound becomes\n$$\n|E_{S}|\\leq \\frac{1^{5}}{180\\,n^{4}}\\cdot 24=\\frac{24}{180\\,n^{4}}=\\frac{2}{15\\,n^{4}}.\n$$\nTo guarantee $|E_{S}|<10^{-5}$, it suffices to require\n$$\n\\frac{2}{15\\,n^{4}}\\leq 10^{-5}\\quad\\Longrightarrow\\quad n^{4}\\geq \\frac{2}{15}\\times 10^{5}=\\frac{40000}{3}.\n$$\nTaking fourth roots yields $n\\geq \\left(\\frac{40000}{3}\\right)^{1/4}$. Since $n$ must be an even integer, we test values.\n$$10^{4}=10000  \\frac{40000}{3}$$\n$$12^{4}=20736 > \\frac{40000}{3}$$\nTherefore, the minimum even integer $n$ that ensures $|E_{S}|10^{-5}$ is $n=12$.", "answer": "$$\\boxed{12}$$", "id": "2210241"}, {"introduction": "This final challenge synthesizes the concepts of approximation and error control into a powerful, automated tool. You will design and implement an adaptive quadrature algorithm, which represents a significant step from static calculations to dynamic problem-solving in scientific computing. This practice moves beyond a fixed number of intervals, instead building a method that intelligently refines its approach to meet a given tolerance, a technique frequently used in professional software packages [@problem_id:3215250].", "problem": "You are asked to design and implement a complete, runnable program that approximates definite integrals using an adaptive refinement based on the composite Simpson's rule. The goal is to compute $$\\int_a^b f(x)\\,dx$$ to within a user-specified absolute tolerance $\\epsilon$ by recursively subdividing the interval and applying Simpson-type approximations on subintervals. Your implementation must be grounded in first principles: start from the definition of the definite integral as the limit of Riemann sums and the idea of local polynomial interpolation, then formalize a recursive acceptance criterion derived from a principled local error estimate. Do not use any external libraries beyond the Python standard library and the given numerical library.\n\nYour program must implement Adaptive Simpson's Quadrature (ASQ), defined here as a recursive algorithm that:\n- Employs quadratic interpolation of $f(x)$ over subintervals to produce Simpson-type approximations.\n- Computes a local error indicator by comparing approximations obtained at one level of refinement and at a more refined level.\n- Accepts a subinterval if the local error indicator guarantees the absolute error on that subinterval is less than a prescribed local tolerance, otherwise recursively subdivides the subinterval and repeats the process.\n- Allocates the prescribed global tolerance $\\epsilon$ across subintervals in a manner that ensures the overall error does not exceed $\\epsilon$.\n- Enforces a maximum recursion depth to avoid infinite recursion; when the maximum depth is reached, the current best refined estimate must be returned.\n- Counts and minimizes function evaluations by reusing already computed values at shared nodes whenever possible.\n\nScientific and numerical requirements:\n- All angles must be treated in radians.\n- If $a=b$, the algorithm must return $0$.\n- The acceptance criterion must be rooted in the behavior of the Simpson-type truncation error under interval bisection and must include a correction to the accepted value based on the local error indicator so that the returned estimate is more accurate than the raw composite sum.\n- The implementation must guarantee that the sum of accepted local errors does not exceed the global tolerance $\\epsilon$.\n- You must ensure numerical stability by using double-precision floating-point arithmetic throughout.\n\nTest suite:\nImplement your program to evaluate the following six test cases. For each case, your program must compute the integral approximation and aggregate the results into the final output as specified below. All angles are in radians.\n\n1. $f(x)=\\sin(x)$ on $[0,\\pi]$ with $\\epsilon=10^{-12}$. This is a smooth \"happy path\" case with a known exact value.\n2. $f(x)=e^{-x^2}$ on $[-1,1]$ with $\\epsilon=10^{-10}$. This tests a smooth, non-polynomial function with rapidly decaying tails.\n3. $f(x)=\\dfrac{1}{1+x^2}$ on $[-5,5]$ with $\\epsilon=10^{-12}$. This tests behavior on a wider interval with a rational function.\n4. $f(x)=\\sqrt{x}$ on $[0,1]$ with $\\epsilon=10^{-12}$. This tests endpoint behavior where the derivative is unbounded at $x=0$ but the integral is well-defined.\n5. $f(x)=x^3$ on $[-1,1]$ with $\\epsilon=10^{-12}$. This tests exactness properties for odd polynomials.\n6. $f(x)=e^{x}$ on $[2,2]$ with $\\epsilon=10^{-12}$. This is a boundary condition where the integral should be $0$ because the interval length is $0$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain the six integral approximations, in the order of the test cases above, each represented as a floating-point number. For example, output must look like\n$$[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5,\\text{result}_6]$$\nwith no additional text printed before or after the list.", "solution": "The problem of computing a definite integral to a specified tolerance is a classic task in numerical analysis. The proposed method, Adaptive Simpson's Quadrature (ASQ), is a robust and efficient algorithm for this purpose. The problem statement is valid as it is scientifically grounded in the principles of numerical integration and error analysis, is well-posed with a clear objective and constraints, and provides a set of verifiable test cases. It is a formalizable problem within the domain of numerical methods.\n\nThe foundation of the method is the approximation of a definite integral, $\\int_a^b f(x)\\,dx$, which geometrically represents the area under the curve of the function $f(x)$ from $x=a$ to $x=b$. Numerical quadrature methods approximate this area by replacing the function $f(x)$ with a simpler function that can be integrated exactly, such as a polynomial.\n\nSimpson's $1/3$ rule is derived by approximating $f(x)$ on an interval $[a, b]$ with a quadratic polynomial $P_2(x)$ that interpolates $f(x)$ at three equally spaced points: the endpoints $a$ and $b$, and the midpoint $m = (a+b)/2$. The integral of this polynomial provides the approximation:\n$$\n\\int_a^b f(x) \\,dx \\approx \\int_a^b P_2(x) \\,dx = \\frac{h}{3} \\left( f(a) + 4f(m) + f(b) \\right)\n$$\nwhere $h = (b-a)/2$ is the step size between the interpolation points. It is more common to define the approximation in terms of the total interval width, $b-a$, which gives:\n$$\nS(a, b) = \\frac{b-a}{6} \\left( f(a) + 4f\\left(\\frac{a+b}{2}\\right) + f(b) \\right)\n$$\nThe truncation error of this approximation, $E_S = \\int_a^b f(x) \\,dx - S(a, b)$, can be shown to be:\n$$\nE_S = -\\frac{(b-a)^5}{2880} f^{(4)}(\\xi)\n$$\nfor some $\\xi \\in (a, b)$, provided $f(x)$ is four times continuously differentiable. The error is of order $O((b-a)^5)$. This high order of accuracy makes Simpson's rule very effective for smooth functions.\n\nThe core of an *adaptive* method is the ability to estimate the local error of the approximation on a given subinterval and to subdivide the interval only where necessary to meet a desired tolerance. This is more efficient than using a composite rule with a fixed, small step size over the entire domain, as it concentrates computational effort on regions where the function is less well-behaved.\n\nTo estimate the error without knowledge of $f^{(4)}(x)$, we compare two approximations over the same interval $[a, b]$. First, we compute the coarse approximation, $S_1$, using the entire interval:\n$$\nS_1 = \\frac{b-a}{6} \\left( f(a) + 4f(m) + f(b) \\right)\n$$\nwhere $m=(a+b)/2$. Next, we compute a more refined approximation, $S_2$, by bisecting the interval into $[a, m]$ and $[m, b]$ and applying Simpson's rule to each subinterval. Let $c=(a+m)/2$ and $d=(m+b)/2$.\n$$\nS_2 = S(a, m) + S(m, b) = \\frac{m-a}{6}\\left(f(a)+4f(c)+f(m)\\right) + \\frac{b-m}{6}\\left(f(m)+4f(d)+f(b)\\right)\n$$\nSince $m-a = b-m = (b-a)/2$, we can write $S_2$ as:\n$$\nS_2 = \\frac{b-a}{12} \\left( f(a) + 4f(c) + 2f(m) + 4f(d) + f(b) \\right)\n$$\nLet $I$ be the exact value of the integral. The error of $S_1$ is $I - S_1 \\approx C(b-a)^5$. The error of $S_2$ is the sum of errors over two intervals of half the width, so $I - S_2 \\approx 2 \\times C((b-a)/2)^5 = C(b-a)^5/16$.\nThus, the error of the coarse approximation is approximately $16$ times larger than the error of the refined approximation:\n$$\nI - S_1 \\approx 16(I - S_2)\n$$\nBy rearranging the terms, we can solve for the error of the more accurate approximation, $S_2$:\n$$\nS_2 - S_1 \\approx 15(I - S_2) \\implies E_2 = I - S_2 \\approx \\frac{S_2 - S_1}{15}\n$$\nThe quantity $|\\frac{S_2 - S_1}{15}|$ serves as a computable local error indicator for the approximation $S_2$.\n\nThe adaptive algorithm proceeds recursively. Given an interval $[a, b]$ and a local tolerance $\\tau$:\n1.  Calculate the five necessary function values: $f(a)$, $f(c)$, $f(m)$, $f(d)$, $f(b)$.\n2.  Compute the approximations $S_1$ and $S_2$.\n3.  Compute the error estimate $\\Delta = |S_2 - S_1|/15$.\n4.  If $\\Delta \\le \\tau$, the approximation is accepted. To further improve accuracy, we use Richardson extrapolation. The improved value is $S_{extrapolated} = S_2 + (S_2 - S_1)/15$. This value is returned. The error of this extrapolated value is $O((b-a)^7)$, making it significantly more accurate.\n5.  If $\\Delta  \\tau$, the interval must be refined. The algorithm calls itself on the two subintervals, $[a, m]$ and $[m, b]$. The tolerance is distributed between them, so each subproblem is solved with a tolerance of $\\tau/2$. The result is the sum of the results from the two recursive calls.\n\nThe process begins with an initial call over the full interval $[a, b]$ with the global tolerance $\\epsilon$. To prevent infinite recursion, especially for functions with singularities, a maximum recursion depth is enforced. If this depth is reached, the process is terminated for that branch, and the current best estimate ($S_{extrapolated}$) is returned. To minimize function evaluations, function values computed at one level are passed to the next, as the endpoints and midpoints of a parent interval become reused points in its children. For the initial interval $[a,b]$, the points $a$, $m$, $b$ are shared with the subintervals $[a,m]$ and $[m,b]$. The function must only be evaluated at the new points $c$ and $d$. Finally, the case where $a=b$ is handled explicitly, returning $0$ as the value of the integral.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the adaptive Simpson's quadrature problem for the given test suite.\n    \"\"\"\n\n    def _recursive_asq(f, a, b, tol, fa, fm, fb, depth, max_depth):\n        \"\"\"Recursive helper for adaptive Simpson's quadrature.\"\"\"\n        m = (a + b) / 2.0\n        h = b - a\n\n        # Calculate new interpolation points and function values\n        c = (a + m) / 2.0\n        d = (m + b) / 2.0\n        fl = f(c)\n        fr = f(d)\n\n        # Coarse (1-panel) and refined (2-panel) Simpson's rule approximations\n        s1 = (h / 6.0) * (fa + 4.0 * fm + fb)\n        s2 = (h / 12.0) * (fa + 4.0 * fl + 2.0 * fm + 4.0 * fr + fb)\n        \n        # Error estimate based on the difference between the two approximations\n        error_estimate = abs(s2 - s1) / 15.0\n        \n        # Use Richardson extrapolation for a more accurate result\n        extrapolated_value = s2 + (s2 - s1) / 15.0\n\n        # Acceptance criterion: if error is within tolerance or max depth is reached\n        if depth = max_depth or error_estimate = tol:\n            return extrapolated_value\n        \n        # If not accepted, recurse on subintervals with halved tolerance\n        left_integral = _recursive_asq(f, a, m, tol / 2.0, fa, fl, fm, depth + 1, max_depth)\n        right_integral = _recursive_asq(f, m, b, tol / 2.0, fm, fr, fb, depth + 1, max_depth)\n        \n        return left_integral + right_integral\n\n    def adaptive_simpson_quadrature(f, a, b, epsilon, max_depth=50):\n        \"\"\"\n        Computes the definite integral of f from a to b using adaptive Simpson's rule.\n\n        Args:\n            f: The function to integrate.\n            a: The start of the integration interval.\n            b: The end of the integration interval.\n            epsilon: The desired absolute tolerance.\n            max_depth: The maximum recursion depth to prevent infinite loops.\n\n        Returns:\n            The approximate value of the integral.\n        \"\"\"\n        # Handle the zero-width interval case\n        if a == b:\n            return 0.0\n\n        # Initial function evaluations for the first level\n        fa = f(a)\n        fb = f(b)\n        m = (a + b) / 2.0\n        fm = f(m)\n        \n        return _recursive_asq(f, a, b, epsilon, fa, fm, fb, 1, max_depth)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'func': np.sin, 'a': 0, 'b': np.pi, 'eps': 1e-12},\n        {'func': lambda x: np.exp(-x**2), 'a': -1, 'b': 1, 'eps': 1e-10},\n        {'func': lambda x: 1 / (1 + x**2), 'a': -5, 'b': 5, 'eps': 1e-12},\n        {'func': np.sqrt, 'a': 0, 'b': 1, 'eps': 1e-12},\n        {'func': lambda x: x**3, 'a': -1, 'b': 1, 'eps': 1e-12},\n        {'func': np.exp, 'a': 2, 'b': 2, 'eps': 1e-12},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = adaptive_simpson_quadrature(\n            f=case['func'], \n            a=float(case['a']), \n            b=float(case['b']), \n            epsilon=float(case['eps'])\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.15g}' for r in results)}]\")\n\nsolve()\n```", "id": "3215250"}]}