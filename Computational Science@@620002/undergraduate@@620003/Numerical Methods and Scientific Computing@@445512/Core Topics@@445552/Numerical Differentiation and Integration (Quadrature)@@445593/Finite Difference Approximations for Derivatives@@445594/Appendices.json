{"hands_on_practices": [{"introduction": "A core challenge in numerical methods is balancing the error from the mathematical approximation (truncation error) with the error from finite-precision computer arithmetic (round-off error). This practice explores this fundamental trade-off by asking you to build an error model for a central difference approximation. By minimizing this model, you will discover the non-intuitive but crucial concept that an optimal step size $h$ exists, which is not infinitesimally small, providing a deep insight into the practical limits of numerical differentiation [@problem_id:2391199].", "problem": "Consider approximating the first derivative of the function $f(x)=\\exp(x)$ at the point $x=1$ using the symmetric three-point central difference\n$$\nD(h)\\equiv \\frac{f(1+h)-f(1-h)}{2h}.\n$$\nAssume that the function evaluations are performed in floating-point arithmetic that rounds to nearest with unit roundoff $u=2^{-53}$, so that each computed function value satisfies $\\operatorname{fl}(f(t))=f(t)\\,(1+\\delta)$ with $|\\delta|\\le u$. Assume that the subtraction and division in $D(h)$ are exact for the purposes of this analysis. Using these assumptions and first principles, construct an upper bound model for the total absolute error at $x=1$ by combining truncation and round-off errors, and determine the value of the step size $h>0$ that minimizes this bound. Provide the optimal $h$ as a single real number. Round your answer to $3$ significant figures.", "solution": "The problem requires the determination of an optimal step size $h$ for a finite difference approximation by modeling and minimizing the total error. The total error is a combination of truncation error inherent in the approximation formula and round-off error from finite-precision arithmetic.\n\nFirst, we validate the problem statement.\nGivens are:\n- Function: $f(x) = \\exp(x)$.\n- Point of evaluation: $x=1$.\n- Approximation formula: $D(h) = \\frac{f(1+h)-f(1-h)}{2h}$.\n- Floating-point error model: $\\operatorname{fl}(f(t)) = f(t)(1+\\delta)$ with $|\\delta| \\le u$.\n- Unit roundoff: $u=2^{-53}$.\n- Arithmetic assumption: Subtraction and division are exact.\n- Objective: Find the optimal step size $h_{\\text{opt}}$ that minimizes an upper bound for the total absolute error.\n\nThe problem is scientifically grounded in numerical analysis, well-posed, and objective. There are no identifiable flaws. The problem is valid. We proceed with the solution.\n\nLet $\\tilde{D}(h)$ denote the value of the central difference formula as computed in floating-point arithmetic. The total absolute error $E_{\\text{total}}$ is given by the magnitude of the difference between the exact derivative $f'(1)$ and the computed approximation $\\tilde{D}(h)$:\n$$ E_{\\text{total}} = |f'(1) - \\tilde{D}(h)| $$\nWe can decompose this error into two parts using the triangle inequality, by introducing the exact-arithmetic approximation $D(h)$:\n$$ E_{\\text{total}} = |f'(1) - D(h) + D(h) - \\tilde{D}(h)| \\le |f'(1) - D(h)| + |D(h) - \\tilde{D}(h)| $$\nThe term $E_{\\text{trunc}}(h) = |f'(1) - D(h)|$ is the truncation error, which arises because the finite difference formula is an approximation of the derivative. The term $E_{\\text{round}}(h) = |D(h) - \\tilde{D}(h)|$ is the round-off error, which arises from the finite precision of computer arithmetic.\n\nWe will now derive an upper bound for each of these error components to construct our error model.\n\n**Truncation Error Analysis**\nWe use Taylor's theorem to expand the function $f(x)$ around the point $x=1$. For the function $f(x) = \\exp(x)$, all its derivatives are also $\\exp(x)$. Thus, $f^{(n)}(1) = \\exp(1) = e$ for all $n \\ge 0$.\nThe Taylor series for $f(1+h)$ and $f(1-h)$ are:\n$$ f(1+h) = f(1) + f'(1)h + \\frac{f''(1)}{2!}h^2 + \\frac{f'''(1)}{3!}h^3 + O(h^4) $$\n$$ f(1-h) = f(1) - f'(1)h + \\frac{f''(1)}{2!}h^2 - \\frac{f'''(1)}{3!}h^3 + O(h^4) $$\nSubtracting the second expansion from the first:\n$$ f(1+h) - f(1-h) = 2f'(1)h + \\frac{2f'''(1)}{6}h^3 + O(h^5) $$\nSubstituting this into the formula for $D(h)$:\n$$ D(h) = \\frac{2f'(1)h + \\frac{1}{3}f'''(1)h^3 + O(h^5)}{2h} = f'(1) + \\frac{f'''(1)}{6}h^2 + O(h^4) $$\nThe truncation error is therefore:\n$$ E_{\\text{trunc}}(h) = |D(h) - f'(1)| = \\left| \\frac{f'''(1)}{6}h^2 + O(h^4) \\right| $$\nFor small $h$, the dominant term is proportional to $h^2$. To establish an upper bound for our model, we use the leading term. For $f(x) = \\exp(x)$ at $x=1$, we have $f'''(1)=e$. Thus, a simple model for the truncation error bound is:\n$$ E_{\\text{trunc}}(h) \\le \\frac{e}{6}h^2 $$\n\n**Round-off Error Analysis**\nThe computed approximation $\\tilde{D}(h)$ uses floating-point values for the function evaluations:\n$$ \\tilde{D}(h) = \\frac{\\operatorname{fl}(f(1+h)) - \\operatorname{fl}(f(1-h))}{2h} $$\nUsing the given floating-point error model, $\\operatorname{fl}(f(t)) = f(t)(1+\\delta)$ with $|\\delta| \\le u$, we have:\n$$ \\tilde{D}(h) = \\frac{f(1+h)(1+\\delta_1) - f(1-h)(1+\\delta_2)}{2h} $$\nwhere $|\\delta_1| \\le u$ and $|\\delta_2| \\le u$. The round-off error is:\n$$ E_{\\text{round}}(h) = |D(h) - \\tilde{D}(h)| = \\left| \\frac{f(1+h)-f(1-h)}{2h} - \\frac{f(1+h)(1+\\delta_1) - f(1-h)(1+\\delta_2)}{2h} \\right| $$\n$$ E_{\\text{round}}(h) = \\left| \\frac{-f(1+h)\\delta_1 + f(1-h)\\delta_2}{2h} \\right| \\le \\frac{|f(1+h)||\\delta_1| + |f(1-h)||\\delta_2|}{2h} $$\nUsing the bounds on $\\delta_i$:\n$$ E_{\\text{round}}(h) \\le \\frac{(|f(1+h)| + |f(1-h)|)u}{2h} $$\nFor constructing the model, we approximate the function values for small $h$. As $h \\to 0$, both $f(1+h)$ and $f(1-h)$ approach $f(1)=e$. Since $\\exp(x)$ is always positive, the absolute values are not needed. We model the numerator as approximately $e+e=2e$.\n$$ E_{\\text{round}}(h) \\le \\frac{2eu}{2h} = \\frac{eu}{h} $$\n\n**Minimization of the Total Error Bound**\nWe construct the total error bound model, $E(h)$, by summing the bounds for the truncation and round-off errors:\n$$ E(h) = \\frac{e}{6}h^2 + \\frac{eu}{h} $$\nTo find the value of $h$ that minimizes this function, we compute its derivative with respect to $h$ and set it to zero:\n$$ \\frac{dE}{dh} = \\frac{d}{dh} \\left( \\frac{e}{6}h^2 + eu h^{-1} \\right) = \\frac{2e}{6}h - eu h^{-2} = \\frac{e}{3}h - \\frac{eu}{h^2} $$\nSetting the derivative to zero gives the optimal $h$:\n$$ \\frac{e}{3}h = \\frac{eu}{h^2} \\implies h^3 = \\frac{3eu}{e} = 3u $$\n$$ h_{\\text{opt}} = (3u)^{1/3} $$\nThe second derivative, $\\frac{d^2E}{dh^2} = \\frac{e}{3} + \\frac{2eu}{h^3}$, is always positive for $h>0$, confirming that this value of $h$ indeed corresponds to a minimum.\n\n**Numerical Calculation**\nWe are given $u=2^{-53}$. Substituting this into our expression for $h_{\\text{opt}}$:\n$$ h_{\\text{opt}} = (3 \\times 2^{-53})^{1/3} $$\nThe numerical value is:\n$$ h_{\\text{opt}} \\approx (3 \\times 1.1102230246 \\times 10^{-16})^{1/3} \\approx (3.3306690738 \\times 10^{-16})^{1/3} \\approx 6.931833 \\times 10^{-6} $$\nRounding this result to $3$ significant figures as requested gives $6.93 \\times 10^{-6}$.", "answer": "$$\\boxed{6.93 \\times 10^{-6}}$$", "id": "2391199"}, {"introduction": "Moving from theoretical error to practical measurement challenges, this exercise simulates the common task of estimating velocity and acceleration from noisy position data. Numerical differentiation acts as a high-frequency filter, meaning it tends to amplify noise, a problem that becomes more severe for higher-order derivatives. This hands-on coding practice allows you to quantitatively investigate this noise amplification, comparing your empirical results with a theoretical model derived from the finite difference stencil weights [@problem_id:2392343].", "problem": "You are given a time series model for one-dimensional position as a function of time with additive measurement noise. The position is a smooth, known function of time with superimposed zero-mean, independent noise at each sample. Your task is to design and implement a program that, for a set of test cases, constructs the noisy position data, estimates the velocity and acceleration using finite differences, and quantitatively analyzes the amplification of measurement noise by the differentiation operators.\n\nBase your reasoning on the following fundamental definitions and facts only: the derivative of a function is the limit of the difference quotient, linear operators acting on independent random variables produce outputs whose variances add according to the squares of the operator weights, and a Taylor series expansion can be used to derive the local truncation error order of a finite difference stencil. Do not use any other pre-packaged formulas beyond these principles.\n\nUse the following signal model. The noiseless position is\n$$\nx(t) = A \\sin(2\\pi f_1 t) + C \\sin(2\\pi f_2 t) + D t^2,\n$$\nwith parameters\n$$\nA = 1.0\\ \\text{m},\\quad C = 0.5\\ \\text{m},\\quad f_1 = 1.0\\ \\text{Hz},\\quad f_2 = 3.0\\ \\text{Hz},\\quad D = 0.05\\ \\text{m/s}^2.\n$$\nAngles in trigonometric functions are in radians. The exact velocity and acceleration are, respectively,\n$$\nv(t) = \\frac{dx}{dt} = 2\\pi f_1 A \\cos(2\\pi f_1 t) + 2\\pi f_2 C \\cos(2\\pi f_2 t) + 2 D t,\n$$\n$$\na(t) = \\frac{d^2 x}{dt^2} = - (2\\pi f_1)^2 A \\sin(2\\pi f_1 t) - (2\\pi f_2)^2 C \\sin(2\\pi f_2 t) + 2 D.\n$$\n\nSampling and noise model. For each test case, sample uniformly at times\n$$\nt_n = n \\,\\Delta t,\\quad n=0,1,\\dots,N-1,\\quad N = \\left\\lfloor \\frac{T}{\\Delta t}\\right\\rfloor + 1,\\quad T = 10\\ \\text{s},\n$$\nand form noisy measurements\n$$\nx_n^{\\text{noisy}} = x(t_n) + \\eta_n,\\quad \\eta_n \\sim \\mathcal{N}(0,\\sigma_x^2)\\ \\text{i.i.d.},\n$$\nwith a fixed random seed equal to $12345$ for reproducibility. All distances are in meters and time in seconds.\n\nFinite difference requirements. From first principles, derive and implement second-order accurate finite difference schemes for the first and second derivatives as follows:\n- For interior points, use a centered, second-order accurate stencil.\n- At the two boundaries, use one-sided, second-order accurate stencils.\nYour implementation must produce arrays $v_n^{\\text{FD}}$ and $a_n^{\\text{FD}}$ of the same length as the input $x_n$.\n\nNoise amplification analysis. Let a finite difference derivative at index $i$ be the linear combination\n$$\ny_i = \\sum_{j} w_{i,j} x_j,\n$$\nwhere $y_i$ represents either the first or second derivative estimate, and $w_{i,j}$ are the finite difference weights divided by the appropriate power of $\\Delta t$. Using only linearity of expectation and independence of the noise samples, derive and compute:\n- The empirical Root Mean Square (RMS) noise amplification for the first derivative,\n$$\ng_v^{\\text{emp}} = \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left( v_i^{\\text{FD}}[x^{\\text{noisy}}] - v_i^{\\text{FD}}[x^{\\text{clean}}]\\right)^2 },\n$$\nand for the second derivative,\n$$\ng_a^{\\text{emp}} = \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left( a_i^{\\text{FD}}[x^{\\text{noisy}}] - a_i^{\\text{FD}}[x^{\\text{clean}}]\\right)^2 }.\n$$\n- The theoretical RMS noise amplification predicted by the weights,\n$$\ng^{\\text{theory}} = \\sigma_x \\sqrt{ \\frac{1}{N} \\sum_{i=0}^{N-1} \\left( \\sum_{j} w_{i,j}^2 \\right) }.\n$$\nCompute $g_v^{\\text{theory}}$ and $g_a^{\\text{theory}}$ using the actual weights used at each index, including boundary stencils.\n\nPerformance metrics. For each test case, compute:\n- The RMS error of the velocity estimate relative to the exact velocity,\n$$\nE_v = \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left( v_i^{\\text{FD}}[x^{\\text{noisy}}] - v(t_i) \\right)^2 } \\ \\text{in m/s}.\n$$\n- The RMS error of the acceleration estimate relative to the exact acceleration,\n$$\nE_a = \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left( a_i^{\\text{FD}}[x^{\\text{noisy}}] - a(t_i) \\right)^2 } \\ \\text{in m/s}^2.\n$$\n- The ratios\n$$\nR_v = \\frac{ g_v^{\\text{emp}} }{ g_v^{\\text{theory}} },\\qquad R_a = \\frac{ g_a^{\\text{emp}} }{ g_a^{\\text{theory}} },\n$$\nwhich indicate how well the empirical noise amplification matches the theoretical prediction.\n\nTest suite. Run your program on the following four test cases, which vary sampling interval and noise level:\n- Case $1$: $\\Delta t = 0.01\\ \\text{s}$, $\\sigma_x = 0.001\\ \\text{m}$.\n- Case $2$: $\\Delta t = 0.1\\ \\text{s}$, $\\sigma_x = 0.001\\ \\text{m}$.\n- Case $3$: $\\Delta t = 0.01\\ \\text{s}$, $\\sigma_x = 0.01\\ \\text{m}$.\n- Case $4$: $\\Delta t = 0.001\\ \\text{s}$, $\\sigma_x = 0.001\\ \\text{m}$.\n\nFinal output format. Your program should produce a single line of output containing the results aggregated as a comma-separated list enclosed in square brackets. For each case in the order $1$ through $4$, append the four floating-point numbers in the following order: $E_v$ (in m/s), $E_a$ (in m/s$^2$), $R_v$ (dimensionless), $R_a$ (dimensionless). Each number must be printed in scientific notation with exactly six significant figures. For example, the overall output format is\n$$\n[\\ E_{v,1},\\ E_{a,1},\\ R_{v,1},\\ R_{a,1},\\ E_{v,2},\\ E_{a,2},\\ R_{v,2},\\ R_{a,2},\\ E_{v,3},\\ E_{a,3},\\ R_{v,3},\\ R_{a,3},\\ E_{v,4},\\ E_{a,4},\\ R_{v,4},\\ R_{a,4}\\ ].\n$$", "solution": "We begin from first principles. The first derivative of a function $x(t)$ at time $t$ is defined by the limit\n$$\n\\frac{dx}{dt}(t) = \\lim_{\\Delta t \\to 0} \\frac{x(t+\\Delta t) - x(t-\\Delta t)}{2\\Delta t},\n$$\nwhich suggests symmetric (centered) difference quotients for finite but small $\\Delta t$. The second derivative is defined by\n$$\n\\frac{d^2x}{dt^2}(t) = \\lim_{\\Delta t \\to 0} \\frac{x(t+\\Delta t) - 2x(t) + x(t-\\Delta t)}{\\Delta t^2}.\n$$\nUsing Taylor series expansions around $t_i = i \\Delta t$,\n$$\nx(t_{i\\pm 1}) = x(t_i) \\pm \\Delta t\\, x'(t_i) + \\frac{\\Delta t^2}{2} x''(t_i) \\pm \\frac{\\Delta t^3}{6} x^{(3)}(t_i) + \\frac{\\Delta t^4}{24} x^{(4)}(t_i) + \\mathcal{O}(\\Delta t^5),\n$$\nwe can derive second-order accurate centered stencils for interior points:\n$$\nx'(t_i) = \\frac{x_{i+1} - x_{i-1}}{2\\Delta t} + \\mathcal{O}(\\Delta t^2), \\quad\nx''(t_i) = \\frac{x_{i+1} - 2 x_i + x_{i-1}}{\\Delta t^2} + \\mathcal{O}(\\Delta t^2),\n$$\nwhere $x_i = x(t_i)$. At the boundaries, one cannot form symmetric differences; instead, a Taylor expansion using forward points yields one-sided, second-order accurate stencils. For the first derivative at the left boundary $i=0$,\n$$\nx'(t_0) = \\frac{-3 x_0 + 4 x_1 - x_2}{2\\Delta t} + \\mathcal{O}(\\Delta t^2),\n$$\nand analogously for the right boundary $i=N-1$,\n$$\nx'(t_{N-1}) = \\frac{3 x_{N-1} - 4 x_{N-2} + x_{N-3}}{2\\Delta t} + \\mathcal{O}(\\Delta t^2).\n$$\nFor the second derivative, forward and backward second-order accurate one-sided stencils are\n$$\nx''(t_0) = \\frac{2 x_0 - 5 x_1 + 4 x_2 - x_3}{\\Delta t^2} + \\mathcal{O}(\\Delta t^2), \\quad\nx''(t_{N-1}) = \\frac{2 x_{N-1} - 5 x_{N-2} + 4 x_{N-3} - x_{N-4}}{\\Delta t^2} + \\mathcal{O}(\\Delta t^2).\n$$\nThese formulas are obtained by solving systems of equations that match the Taylor expansions term-by-term to eliminate lower-order error terms, ensuring second-order accuracy.\n\nNoise amplification analysis rests on linearity. Let the finite difference operator mapping $\\{x_j\\}$ to a derivative estimate $\\{y_i\\}$ be linear:\n$$\ny_i = \\sum_{j} w_{i,j} x_j,\n$$\nwhere $w_{i,j}$ are the operator weights, including normalization by the appropriate power of $\\Delta t$ dictated by the derivative order. Suppose the measurements include additive zero-mean, independent noise $\\eta_j$ with variance $\\mathbb{V}[\\eta_j] = \\sigma_x^2$. Due to linearity and independence,\n$$\n\\mathbb{E}[y_i] = \\sum_{j} w_{i,j} \\mathbb{E}[x_j], \\quad\n\\mathbb{V}[y_i] = \\sum_{j} w_{i,j}^2 \\,\\mathbb{V}[\\eta_j] = \\sigma_x^2 \\sum_{j} w_{i,j}^2.\n$$\nTherefore, the Root Mean Square (RMS) of the noise component at index $i$ equals\n$$\n\\sqrt{\\mathbb{V}[y_i]} = \\sigma_x \\sqrt{\\sum_{j} w_{i,j}^2}.\n$$\nA global RMS across all indices, consistent with the empirical RMS used in practice, is\n$$\ng^{\\text{theory}} = \\sigma_x \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left(\\sum_{j} w_{i,j}^2\\right)}.\n$$\nThis expression includes boundary effects naturally via index-dependent weights.\n\nTwo consequences follow:\n- For the first derivative, the weights scale like $1/\\Delta t$, so $g_v^{\\text{theory}} \\propto \\sigma_x/\\Delta t$; that is, reducing $\\Delta t$ increases the noise amplification in the velocity estimate if the position noise level per sample is fixed.\n- For the second derivative, the weights scale like $1/\\Delta t^2$, so $g_a^{\\text{theory}} \\propto \\sigma_x/\\Delta t^2$, implying even stronger amplification of noise.\n\nThe empirical Root Mean Square (RMS) noise amplification can be isolated by comparing the derivative operator applied to noisy data and to clean data, which cancels deterministic truncation error:\n$$\ng^{\\text{emp}} = \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left( y_i[x^{\\text{noisy}}] - y_i[x^{\\text{clean}}] \\right)^2 }.\n$$\nBecause $y_i[\\cdot]$ is linear and $x^{\\text{noisy}} = x^{\\text{clean}} + \\eta$, the difference equals $y_i[\\eta]$, whose RMS matches the theoretical expression derived above in the limit of many samples. Hence, the ratios\n$$\nR_v = \\frac{g_v^{\\text{emp}}}{g_v^{\\text{theory}}},\\qquad R_a = \\frac{g_a^{\\text{emp}}}{g_a^{\\text{theory}}},\n$$\nshould be close to $1$ when the empirical averages are representative.\n\nAlgorithmic design:\n- Construct time samples $t_i = i \\Delta t$ for $i=0,\\dots,N-1$ with $T=10$ s.\n- Compute the clean position $x(t_i)$, and the exact velocity $v(t_i)$ and acceleration $a(t_i)$ from the given analytic expressions.\n- Generate additive noise $\\eta_i \\sim \\mathcal{N}(0,\\sigma_x^2)$ using a fixed seed to ensure reproducibility, and form $x^{\\text{noisy}}_i = x(t_i) + \\eta_i$.\n- Implement second-order accurate finite difference estimators for the first and second derivatives that apply centered stencils in the interior and one-sided stencils at the boundaries, yielding $v^{\\text{FD}}$ and $a^{\\text{FD}}$ for both clean and noisy inputs.\n- Compute performance metrics: $E_v$ and $E_a$ as RMS errors against the exact derivatives. Compute $g_v^{\\text{emp}}$ and $g_a^{\\text{emp}}$ as RMS differences between the finite difference outputs on noisy and clean data. Compute $g_v^{\\text{theory}}$ and $g_a^{\\text{theory}}$ via the weights by summing squared weights at each index and averaging, then multiply by $\\sigma_x$ and take a square root. Finally, compute $R_v$ and $R_a$ as ratios of empirical to theoretical amplification.\n- Repeat for the four specified test cases. Print the consolidated results in the required single-line, bracketed, comma-separated list, with scientific notation and six significant figures.\n\nExpected trends:\n- Increasing $\\sigma_x$ by a factor of $10$ should increase $g^{\\text{emp}}$ and the noise-dominated components of $E_v$ and $E_a$ by a factor of $10$.\n- Increasing $\\Delta t$ should reduce $g_v^{\\text{theory}}$ roughly like $1/\\Delta t$ and $g_a^{\\text{theory}}$ roughly like $1/\\Delta t^2$; however, truncation error grows like $\\mathcal{O}(\\Delta t^2)$, so $E_v$ and $E_a$ may not decrease monotonically with $\\Delta t$ because of the trade-off between truncation error and noise amplification.\n- The ratios $R_v$ and $R_a$ should be close to $1$, validating the linear noise amplification analysis.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef position(t, A=1.0, C=0.5, f1=1.0, f2=3.0, D=0.05):\n    # x(t) in meters\n    return A * np.sin(2*np.pi*f1*t) + C * np.sin(2*np.pi*f2*t) + D * t**2\n\ndef velocity_true(t, A=1.0, C=0.5, f1=1.0, f2=3.0, D=0.05):\n    # v(t) in m/s\n    return 2*np.pi*f1*A * np.cos(2*np.pi*f1*t) + 2*np.pi*f2*C * np.cos(2*np.pi*f2*t) + 2*D*t\n\ndef acceleration_true(t, A=1.0, C=0.5, f1=1.0, f2=3.0, D=0.05):\n    # a(t) in m/s^2\n    return -(2*np.pi*f1)**2 * A * np.sin(2*np.pi*f1*t) - (2*np.pi*f2)**2 * C * np.sin(2*np.pi*f2*t) + 2*D\n\ndef fd_first_derivative(x, dt):\n    \"\"\"\n    Second-order accurate finite difference for first derivative.\n    - One-sided 3-point at boundaries (order 2).\n    - Centered 3-point in interior (order 2).\n    Returns y of same length as x.\n    \"\"\"\n    n = x.size\n    y = np.empty_like(x, dtype=float)\n    if n  3:\n        raise ValueError(\"Need at least 3 points for second-order first derivative\")\n    # Left boundary: (-3 x0 + 4 x1 - x2)/(2 dt)\n    y[0] = (-3.0*x[0] + 4.0*x[1] - 1.0*x[2]) / (2.0*dt)\n    # Interior\n    y[1:-1] = (x[2:] - x[:-2]) / (2.0*dt)\n    # Right boundary: (3 xN-1 - 4 xN-2 + xN-3)/(2 dt)\n    y[-1] = (3.0*x[-1] - 4.0*x[-2] + 1.0*x[-3]) / (2.0*dt)\n    return y\n\ndef fd_second_derivative(x, dt):\n    \"\"\"\n    Second-order accurate finite difference for second derivative.\n    - One-sided 4-point at boundaries (order 2).\n    - Centered 3-point in interior (order 2).\n    Returns y of same length as x.\n    \"\"\"\n    n = x.size\n    y = np.empty_like(x, dtype=float)\n    if n  4:\n        raise ValueError(\"Need at least 4 points for second derivative with second-order accuracy\")\n    # Left boundary: (2 x0 - 5 x1 + 4 x2 - x3)/dt^2\n    y[0] = (2.0*x[0] - 5.0*x[1] + 4.0*x[2] - 1.0*x[3]) / (dt*dt)\n    # Interior\n    y[1:-1] = (x[2:] - 2.0*x[1:-1] + x[:-2]) / (dt*dt)\n    # Right boundary: (2 xN-1 - 5 xN-2 + 4 xN-3 - xN-4)/dt^2\n    y[-1] = (2.0*x[-1] - 5.0*x[-2] + 4.0*x[-3] - 1.0*x[-4]) / (dt*dt)\n    return y\n\ndef weights_sqsum_first(n, dt):\n    \"\"\"\n    For each index i, compute sum_j w_{i,j}^2 for the first derivative operator.\n    Returns an array s of length n with s[i] = sum_j w_{i,j}^2.\n    \"\"\"\n    s = np.zeros(n, dtype=float)\n    inv = 1.0 / (2.0*dt)\n    # Left boundary: (-3, 4, -1)/(2 dt)\n    coeffs = np.array([-3.0, 4.0, -1.0]) * inv\n    s[0] = np.sum(coeffs**2)\n    # Interior: [-1, +1] at i-1 and i+1\n    c = np.array([-1.0, 1.0]) * inv\n    val = np.sum(c**2)\n    s[1:-1] = val\n    # Right boundary: (1, -4, 3)/(2 dt) applied to (i-2, i-1, i)\n    coeffs = np.array([1.0, -4.0, 3.0]) * inv\n    s[-1] = np.sum(coeffs**2)\n    return s\n\ndef weights_sqsum_second(n, dt):\n    \"\"\"\n    For each index i, compute sum_j w_{i,j}^2 for the second derivative operator.\n    Returns an array s of length n with s[i] = sum_j w_{i,j}^2.\n    \"\"\"\n    s = np.zeros(n, dtype=float)\n    inv2 = 1.0 / (dt*dt)\n    # Left boundary: (2, -5, 4, -1)/dt^2\n    coeffs = np.array([2.0, -5.0, 4.0, -1.0]) * inv2\n    s[0] = np.sum(coeffs**2)\n    # Interior: (1, -2, 1)/dt^2\n    c = np.array([1.0, -2.0, 1.0]) * inv2\n    val = np.sum(c**2)\n    s[1:-1] = val\n    # Right boundary: (-1, 4, -5, 2)/dt^2 applied to (i-3, i-2, i-1, i)\n    coeffs = np.array([-1.0, 4.0, -5.0, 2.0]) * inv2\n    s[-1] = np.sum(coeffs**2)\n    return s\n\ndef rms(x):\n    return np.sqrt(np.mean(np.square(x)))\n\ndef format_float(x):\n    # Scientific notation with exactly six significant figures\n    return f\"{x:.6e}\"\n\ndef run_case(dt, sigma_x, rng):\n    T = 10.0\n    N = int(np.floor(T/dt)) + 1\n    t = np.linspace(0.0, dt*(N-1), N)\n    x_clean = position(t)\n    # Generate noise with given sigma\n    noise = rng.normal(loc=0.0, scale=sigma_x, size=N)\n    x_noisy = x_clean + noise\n\n    # True derivatives\n    v_true = velocity_true(t)\n    a_true = acceleration_true(t)\n\n    # Finite difference estimates\n    v_fd_clean = fd_first_derivative(x_clean, dt)\n    a_fd_clean = fd_second_derivative(x_clean, dt)\n    v_fd_noisy = fd_first_derivative(x_noisy, dt)\n    a_fd_noisy = fd_second_derivative(x_noisy, dt)\n\n    # RMS errors against exact\n    E_v = rms(v_fd_noisy - v_true)\n    E_a = rms(a_fd_noisy - a_true)\n\n    # Empirical noise amplification (difference noisy-clean)\n    g_v_emp = rms(v_fd_noisy - v_fd_clean)\n    g_a_emp = rms(a_fd_noisy - a_fd_clean)\n\n    # Theoretical noise amplification from weights\n    s1 = weights_sqsum_first(N, dt)\n    s2 = weights_sqsum_second(N, dt)\n    g_v_theory = sigma_x * np.sqrt(np.mean(s1))\n    g_a_theory = sigma_x * np.sqrt(np.mean(s2))\n\n    # Ratios (avoid division by zero, though here not zero)\n    R_v = g_v_emp / g_v_theory if g_v_theory > 0 else np.nan\n    R_a = g_a_emp / g_a_theory if g_a_theory > 0 else np.nan\n\n    return E_v, E_a, R_v, R_a\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple: (dt [s], sigma_x [m])\n    test_cases = [\n        (0.01, 0.001),   # Case 1\n        (0.1, 0.001),    # Case 2\n        (0.01, 0.01),    # Case 3\n        (0.001, 0.001),  # Case 4\n    ]\n\n    rng = np.random.default_rng(12345)\n\n    results = []\n    for dt, sigma_x in test_cases:\n        E_v, E_a, R_v, R_a = run_case(dt, sigma_x, rng)\n        results.extend([E_v, E_a, R_v, R_a])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(format_float(x) for x in results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2392343"}, {"introduction": "The accuracy and convergence rates of finite difference formulas rely on the smoothness of the function being differentiated, an assumption rooted in the Taylor series expansion. This practice challenges that assumption by applying standard difference schemes to a function with a cusp, $f(x)=|x|^{3/2}$, which is not sufficiently smooth at the origin. By analyzing the resulting errors, you will see firsthand how the convergence rate can degrade when the underlying mathematical requirements are not met, highlighting the importance of understanding the theoretical limitations of a numerical method [@problem_id:2392345].", "problem": "Write a program that quantitatively evaluates the effect of low regularity at a cusp on the accuracy of finite difference approximations for the first derivative at the point $x=0$ for the function $f(x)=|x|^{3/2}$. The exact derivative is defined by the limit definition of the derivative, and the value at $x=0$ is the value of that limit. Consider three finite difference approximations for the first derivative at $x=0$ using a step size $h0$:\n1. The forward difference $D^{+}_{h}f(0)=\\dfrac{f(h)-f(0)}{h}$.\n2. The backward difference $D^{-}_{h}f(0)=\\dfrac{f(0)-f(-h)}{h}$.\n3. The central difference $D^{0}_{h}f(0)=\\dfrac{f(h)-f(-h)}{2h}$.\nFor a set of step sizes $\\mathcal{H}=\\{10^{-1},10^{-2},10^{-3},10^{-4}\\}$, define the absolute error for a scheme $S\\in\\{+,-,0\\}$ at step size $h$ by $e_{S}(h)=\\left|D^{S}_{h}f(0)-f^{\\prime}(0)\\right|$, where $f^{\\prime}(0)$ denotes the exact derivative at $x=0$. For each scheme $S$, define the observed pairwise convergence rates on consecutive step sizes by\n$$p_{S}(h_{k},h_{k+1})=\\frac{\\log\\left(e_{S}(h_{k})/e_{S}(h_{k+1})\\right)}{\\log\\left(h_{k}/h_{k+1}\\right)},$$\nfor each consecutive pair $(h_{k},h_{k+1})$ in $\\mathcal{H}$ in descending order of $h$. If a pair has $e_{S}(h_{k})=0$ or $e_{S}(h_{k+1})=0$, that pair is excluded from the rate computation for scheme $S$. Define the reported convergence rate for scheme $S$ as the median of the available $p_{S}(h_{k},h_{k+1})$ over all included consecutive pairs. If no pairs are included for a scheme (for example, if all errors are zero), report the convergence rate for that scheme as $+\\infty$.\nYour program must compute and report, in the order specified, the following six quantities:\n- The reported convergence rate for the forward difference scheme (a float).\n- The reported convergence rate for the backward difference scheme (a float).\n- The reported convergence rate for the central difference scheme (a float, use $+\\infty$ if applicable).\n- The absolute error for the forward difference at the smallest step size in $\\mathcal{H}$ (a float).\n- The absolute error for the backward difference at the smallest step size in $\\mathcal{H}$ (a float).\n- The absolute error for the central difference at the smallest step size in $\\mathcal{H}$ (a float).\nAll quantities are unitless. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_{+},r_{-},r_{0},e_{+}^{\\min},e_{-}^{\\min},e_{0}^{\\min}]$), where $r_{S}$ denotes the reported convergence rate for scheme $S$ and $e_{S}^{\\min}$ denotes the absolute error at $h=10^{-4}$ for scheme $S$. This specification constitutes the complete test suite and the exact required output format.", "solution": "The problem is subjected to validation and is deemed valid. It is a well-posed problem in numerical analysis, free of scientific or logical inconsistencies.\n\nThe task is to evaluate the convergence of three finite difference schemes for the first derivative of the function $f(x) = |x|^{3/2}$ at the point $x=0$. The function exhibits a cusp at this point, which implies limited regularity. Specifically, the function is continuously differentiable ($C^1$) but not twice continuously differentiable ($C^2$) at $x=0$. This lack of smoothness is expected to degrade the convergence rates of the numerical schemes from their theoretical orders on smooth functions.\n\nFirst, we must determine the exact value of the derivative, $f'(0)$. Using the limit definition of the derivative:\n$$ f'(0) = \\lim_{h \\to 0} \\frac{f(0+h) - f(0)}{h} = \\lim_{h \\to 0} \\frac{|h|^{3/2} - 0}{h} = \\lim_{h \\to 0} \\frac{|h|^{3/2}}{h} $$\nWe evaluate the left-hand and right-hand limits.\nFor $h \\to 0^{+}$:\n$$ \\lim_{h \\to 0^{+}} \\frac{h^{3/2}}{h} = \\lim_{h \\to 0^{+}} h^{1/2} = 0 $$\nFor $h \\to 0^{-}$:\n$$ \\lim_{h \\to 0^{-}} \\frac{|h|^{3/2}}{h} = \\lim_{h \\to 0^{-}} \\frac{(-h)^{3/2}}{h} = \\lim_{h \\to 0^{-}} \\frac{-(-h)^{3/2}}{-h} = \\lim_{h \\to 0^{-}} -(-h)^{1/2} = 0 $$\nSince both limits are equal to $0$, the derivative exists at $x=0$ and its value is $f'(0) = 0$.\n\nNext, we analyze the behavior of the second derivative. For $x > 0$, $f(x)=x^{3/2}$, so $f'(x) = \\frac{3}{2}x^{1/2}$ and $f''(x) = \\frac{3}{4}x^{-1/2}$. For $x  0$, $f(x)=(-x)^{3/2}$, so $f'(x) = -\\frac{3}{2}(-x)^{1/2}$ and $f''(x) = \\frac{3}{4}(-x)^{-1/2}$. In both cases, as $x \\to 0$, $|f''(x)| \\to \\infty$. Thus, the second derivative is undefined at $x=0$, and the standard Taylor series expansion error analysis, which relies on the existence and boundedness of higher-order derivatives, is not directly applicable.\n\nWe now derive the analytical form of each finite difference approximation and its absolute error $e_S(h) = |D^S_h f(0) - f'(0)|$. For $h > 0$, we have $f(h) = h^{3/2}$ and $f(-h) = |-h|^{3/2} = h^{3/2}$. Since $f(0)=0$ and $f'(0)=0$, the error calculation is simplified.\n\n1.  **Forward Difference ($D^{+}$):**\n    $$ D^{+}_{h}f(0) = \\frac{f(h) - f(0)}{h} = \\frac{h^{3/2} - 0}{h} = h^{1/2} $$\n    The absolute error is:\n    $$ e_{+}(h) = |h^{1/2} - 0| = h^{1/2} $$\n    The error is of order $O(h^{0.5})$. The theoretical convergence rate is $p = 0.5$.\n\n2.  **Backward Difference ($D^{-}$):**\n    $$ D^{-}_{h}f(0) = \\frac{f(0) - f(-h)}{h} = \\frac{0 - h^{3/2}}{h} = -h^{1/2} $$\n    The absolute error is:\n    $$ e_{-}(h) = |-h^{1/2} - 0| = h^{1/2} $$\n    The error is also of order $O(h^{0.5})$, giving a theoretical convergence rate of $p = 0.5$.\n\n3.  **Central Difference ($D^{0}$):**\n    $$ D^{0}_{h}f(0) = \\frac{f(h) - f(-h)}{2h} = \\frac{h^{3/2} - h^{3/2}}{2h} = 0 $$\n    The absolute error is:\n    $$ e_{0}(h) = |0 - 0| = 0 $$\n    For this specific function, the central difference approximation is exactly zero for any $h > 0$ due to the even symmetry of the function ($f(x) = f(-x)$). This implies the error is identically zero.\n\nWith these analytical expressions for the errors, we can compute the required quantities. The set of step sizes is $\\mathcal{H} = \\{10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}\\}$. Let $h_k = 10^{-k}$ for $k \\in \\{1, 2, 3, 4\\}$. The pairwise convergence rate is defined as $p_{S}(h_{k},h_{k+1})=\\frac{\\log\\left(e_{S}(h_{k})/e_{S}(h_{k+1})\\right)}{\\log\\left(h_{k}/h_{k+1}\\right)}$. The denominator is always $\\log(10^{-k}/10^{-(k+1)}) = \\log(10)$.\n\nFor the forward difference scheme:\n$e_{+}(h_k) = (10^{-k})^{1/2} = 10^{-k/2}$.\nThe ratio of errors for any consecutive pair is $e_{+}(h_k)/e_{+}(h_{k+1}) = 10^{-k/2} / 10^{-(k+1)/2} = 10^{1/2}$.\nThus, the pairwise rate is constant:\n$$ p_{+}(h_k, h_{k+1}) = \\frac{\\log(10^{1/2})}{\\log(10)} = \\frac{0.5 \\log(10)}{\\log(10)} = 0.5 $$\nThe set of available rates is $\\{0.5, 0.5, 0.5\\}$. The median, which is the reported convergence rate $r_{+}$, is $0.5$. The error at the smallest step size, $h=10^{-4}$, is $e_{+}(10^{-4}) = (10^{-4})^{1/2} = 10^{-2} = 0.01$.\n\nFor the backward difference scheme:\nThe error $e_{-}(h_k) = h_k^{1/2}$ is identical to the forward difference case. Therefore, the reported convergence rate $r_{-}$ is also $0.5$, and the error at $h=10^{-4}$ is $e_{-}(10^{-4}) = 0.01$.\n\nFor the central difference scheme:\nThe error $e_{0}(h)$ is identically $0$ for all $h > 0$. According to the problem statement, if either $e_S(h_k)=0$ or $e_S(h_{k+1})=0$, the pair is excluded from the rate computation. Since all errors are zero, no pairwise rates can be computed. The problem specifies that in this case, the reported convergence rate $r_{0}$ must be $+\\infty$. The error at the smallest step size, $e_{0}(10^{-4})$, is $0$.\n\nThe six quantities to be reported are:\n1.  Reported convergence rate for forward difference ($r_{+}$): $0.5$.\n2.  Reported convergence rate for backward difference ($r_{-}$): $0.5$.\n3.  Reported convergence rate for central difference ($r_{0}$): $+\\infty$.\n4.  Absolute error for forward difference at $h=10^{-4}$ ($e_{+}^{\\min}$): $0.01$.\n5.  Absolute error for backward difference at $h=10^{-4}$ ($e_{-}^{\\min}$): $0.01$.\n6.  Absolute error for central difference at $h=10^{-4}$ ($e_{0}^{\\min}$): $0.0$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes convergence rates and errors for finite difference approximations\n    of the derivative of f(x) = |x|^(3/2) at x=0.\n    \"\"\"\n    \n    # 1. Define the problem parameters\n    \n    # The function f(x) = |x|^(3/2)\n    def f(x):\n        return np.abs(x)**(3/2)\n    \n    # The exact first derivative at x=0 is 0.\n    f_prime_0 = 0.0\n    \n    # Set of step sizes in descending order\n    step_sizes = np.array([1e-1, 1e-2, 1e-3, 1e-4])\n\n    # 2. Define the finite difference schemes\n    def forward_diff(h):\n        return (f(h) - f(0)) / h\n\n    def backward_diff(h):\n        return (f(0) - f(-h)) / h\n\n    def central_diff(h):\n        return (f(h) - f(-h)) / (2 * h)\n\n    schemes = {\n        'forward': forward_diff,\n        'backward': backward_diff,\n        'central': central_diff\n    }\n    \n    # 3. Compute errors and convergence rates for each scheme\n    \n    results = {}\n    \n    for name, scheme_func in schemes.items():\n        # Calculate absolute errors for all step sizes\n        approximations = np.array([scheme_func(h) for h in step_sizes])\n        errors = np.abs(approximations - f_prime_0)\n        \n        # Calculate pairwise convergence rates\n        pairwise_rates = []\n        for k in range(len(step_sizes) - 1):\n            h_k = step_sizes[k]\n            h_k_plus_1 = step_sizes[k+1]\n            e_k = errors[k]\n            e_k_plus_1 = errors[k+1]\n            \n            # Exclude pairs where error is zero, as per problem statement\n            if e_k == 0.0 or e_k_plus_1 == 0.0:\n                continue\n                \n            rate = np.log(e_k / e_k_plus_1) / np.log(h_k / h_k_plus_1)\n            pairwise_rates.append(rate)\n            \n        # Determine the reported convergence rate\n        if not pairwise_rates:\n            # If no rates were computed, report as +infinity\n            reported_rate = float('inf')\n        else:\n            # Otherwise, the reported rate is the median\n            reported_rate = np.median(pairwise_rates)\n            \n        # Store the results for this scheme\n        results[name] = {\n            'rate': reported_rate,\n            'error_min_h': errors[-1] # Error at the smallest h\n        }\n        \n    # 4. Assemble the final output list in the specified order\n    \n    r_plus = results['forward']['rate']\n    r_minus = results['backward']['rate']\n    r_zero = results['central']['rate']\n    \n    e_plus_min = results['forward']['error_min_h']\n    e_minus_min = results['backward']['error_min_h']\n    e_zero_min = results['central']['error_min_h']\n    \n    final_output = [\n        r_plus,\n        r_minus,\n        r_zero,\n        e_plus_min,\n        e_minus_min,\n        e_zero_min\n    ]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, final_output))}]\")\n\nsolve()\n```", "id": "2392345"}]}