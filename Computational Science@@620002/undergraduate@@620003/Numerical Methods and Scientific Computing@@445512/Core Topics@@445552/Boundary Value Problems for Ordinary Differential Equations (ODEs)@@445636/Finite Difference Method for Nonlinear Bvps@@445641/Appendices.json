{"hands_on_practices": [{"introduction": "This first practice serves as our foundational exercise in building a robust solver from the ground up. We will tackle a nonlinear boundary value problem presented in conservation form, a structure frequently encountered when modeling physical phenomena like heat transfer or fluid dynamics where a quantity (the 'flux') is conserved. By discretizing the divergence of a nonlinear flux, you will gain hands-on experience with a powerful and physically intuitive finite difference technique and implement a complete Newton's method solver with a line search for globalization [@problem_id:3228466].", "problem": "Consider the nonlinear second-order Ordinary Differential Equation (ODE) Boundary Value Problem (BVP) given by\n$$\\frac{d}{dx}\\Big((1+u(x)^2)\\,u'(x)\\Big) = \\sin(x), \\quad x \\in [a,b],$$\nwith Dirichlet boundary conditions\n$$u(a) = \\alpha,\\qquad u(b) = \\beta.$$\nAngles in the forcing function $\\sin(x)$ must be interpreted in radians.\n\nYour task is to construct a second-order accurate finite difference method based on the following first-principles foundation and implement a robust nonlinear solver:\n- Begin from the conservation form by introducing the flux $q(x) = (1+u(x)^2)u'(x)$ so that $q'(x) = \\sin(x)$.\n- Use a uniform grid with nodes $x_i = a + i h$ for $i=0,1,\\dots,N$ and mesh width $h = (b-a)/N$.\n- Approximate the flux divergence at interior nodes using centered differences of half-node fluxes, and evaluate the coefficient that depends on $u$ at half-nodes using the arithmetic average of adjacent nodal values.\n- Impose the Dirichlet boundary conditions directly at the end nodes.\n- Assemble the resulting nonlinear residual equations for the interior unknowns and solve them using Newton’s method for systems of nonlinear equations, with an analytically derived Jacobian matrix based on the linearization of the flux at half-nodes.\n- Use a line search globalization strategy in Newton’s method that guarantees residual decrease.\n\nDesign your method step-by-step from the fundamental definitions of derivative and flux balance. Do not use any pre-packaged discretization formula beyond those definitions. The implementation must terminate when a norm of the nonlinear residual over interior nodes is below a user-chosen tolerance or a maximum number of Newton iterations is reached.\n\nTest suite and required outputs:\n- Implement a program that solves the BVP for each parameter set below. For each case, report the numerical approximation of $u(x^\\star)$ at the specified grid-aligned location $x^\\star$.\n- The grid size $N$ in each case is chosen so that $x^\\star$ coincides with a grid node (that is, $x^\\star = a + i^\\star h$ for an integer $i^\\star$). Your code should identify the corresponding node index by arithmetic using $a$, $h$, and $x^\\star$.\n\nProvide results for the following four cases:\n- Case 1 (happy path):\n  - $a = 0$, $b = 1$, $\\alpha = 0$, $\\beta = 0$, $N = 100$, $x^\\star = 0.5$.\n- Case 2 (different domain length with symmetric bounds):\n  - $a = 0$, $b = \\pi$, $\\alpha = 0$, $\\beta = 0$, $N = 200$, $x^\\star = \\pi/2$.\n- Case 3 (nonzero Dirichlet data):\n  - $a = 0$, $b = 1$, $\\alpha = 1$, $\\beta = -1$, $N = 120$, $x^\\star = 0.25$.\n- Case 4 (shifted domain including the origin):\n  - $a = -1$, $b = 2$, $\\alpha = 0$, $\\beta = 0$, $N = 150$, $x^\\star = 0$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases above, for example, $[r_1,r_2,r_3,r_4]$.\n- Each entry must be a floating-point number equal to the computed $u(x^\\star)$ for that case, rounded to exactly six digits after the decimal point.\n- No extra whitespace or text should be printed.\n\nAngle unit requirement:\n- All evaluations of $\\sin(x)$ must use $x$ expressed in radians.\n\nYour solution must be fully self-contained and must not read any input. It must compile and run as-is in a modern programming environment.", "solution": "The user-provided problem is a nonlinear second-order ordinary differential equation (ODE) formulated as a boundary value problem (BVP).\n\n### Step 1: Extract Givens\n- **Governing Equation**: $\\frac{d}{dx}\\Big((1+u(x)^2)\\,u'(x)\\Big) = \\sin(x)$ for $x \\in [a,b]$.\n- **Boundary Conditions**: $u(a) = \\alpha$, $u(b) = \\beta$.\n- **Discretization Grid**: Uniform grid $x_i = a + i h$ for $i=0,1,\\dots,N$, with mesh width $h = (b-a)/N$.\n- **Discretization Method**:\n    - The ODE is expressed in conservation form $q'(x) = \\sin(x)$ with flux $q(x) = (1+u(x)^2)u'(x)$.\n    - The term $q'(x)$ is approximated at interior nodes $x_i$ using a centered difference of fluxes evaluated at half-nodes: $q'(x_i) \\approx (q_{i+1/2} - q_{i-1/2})/h$.\n    - The flux $q_{i+1/2}$ is approximated using a centered difference for the derivative $u'(x_{i+1/2})$ and an arithmetic average for the nonlinear coefficient term involving $u(x_{i+1/2})$.\n- **Nonlinear Solver**: Newton's method with an analytically derived Jacobian and a line search globalization strategy.\n- **Test Cases**: Four specific sets of parameters $(a, b, \\alpha, \\beta, N, x^\\star)$ are provided for testing.\n- **Output Requirement**: For each case, report the value of the numerical solution $u(x^\\star)$, rounded to six decimal places.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is subjected to a rigorous validation check.\n\n- **Scientifically Grounded (Critical)**: The problem is a standard nonlinear Poisson-type equation, a common model in various fields of physics and engineering (e.g., heat transfer with temperature-dependent conductivity, nonlinear diffusion). The formulation is mathematically and physically sound. It does not violate any scientific principles.\n- **Well-Posed**: The problem is a Dirichlet BVP for a second-order elliptic ODE. The operator is monotone, which typically ensures the existence and uniqueness of a solution. The specified discretization method (a cell-centered finite volume/difference approach) is standard and known to be stable and convergent. Thus, a meaningful solution is expected.\n- **Objective (Critical)**: The problem statement is precise, quantitative, and free of any subjective or ambiguous language. All parameters, equations, and methods are clearly defined.\n- **Completeness**: All necessary information is provided: the ODE, boundary conditions, domain, discretization strategy, solution algorithm, and specific test parameters. The problem is self-contained.\n- **Consistency**: The givens are internally consistent. The requirement that $x^\\star$ falls on a grid node is ensured by the choice of parameters in each test case.\n- **Other Flaws**: The problem is not metaphorical, trivial, or outside the scope of scientific verifiability.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A step-by-step reasoned solution will be provided, adhering to all specified constraints.\n\n### Solution Derivation\n\nThe solution process involves discretizing the BVP to form a system of nonlinear algebraic equations and then solving this system using Newton's method.\n\n**1. Finite Difference Discretization**\nThe governing ODE is given in conservation form, $q'(x) = \\sin(x)$, with the flux defined as $q(x) = D(u)u'(x)$, where $D(u) = 1+u(x)^2$. We define a uniform grid $x_i = a + i h$ for $i=0, 1, \\dots, N$, where $h = (b-a)/N$. Let $U_i$ be the numerical approximation of $u(x_i)$. The boundary conditions are imposed directly: $U_0 = \\alpha$ and $U_N = \\beta$.\n\nFollowing the problem's instructions, we discretize the divergence of the flux at each interior grid node $x_i$ for $i=1, 2, \\dots, N-1$ using a centered difference scheme involving fluxes at half-nodes $x_{i\\pm1/2}$:\n$$\n\\frac{q_{i+1/2} - q_{i-1/2}}{h} = \\sin(x_i)\n$$\nHere, $q_{i+1/2}$ is the numerical approximation of the flux at the midpoint $x_{i+1/2} = x_i + h/2$. This flux is discretized as follows:\n- The derivative $u'(x_{i+1/2})$ is approximated by a second-order centered difference:\n$$ u'(x_{i+1/2}) \\approx \\frac{U_{i+1} - U_i}{h} $$\n- The nonlinear coefficient $D(u(x_{i+1/2}))$ is evaluated using the arithmetic mean of the solution at adjacent nodes:\n$$ D(u(x_{i+1/2})) \\approx 1 + \\left(\\frac{U_i + U_{i+1}}{2}\\right)^2 $$\nCombining these, the numerical flux at the half-node $x_{i+1/2}$ is:\n$$ q_{i+1/2} = \\left(1 + \\left(\\frac{U_i + U_{i+1}}{2}\\right)^2\\right) \\frac{U_{i+1} - U_i}{h} $$\nSimilarly, the flux at $x_{i-1/2}$ is:\n$$ q_{i-1/2} = \\left(1 + \\left(\\frac{U_{i-1} + U_i}{2}\\right)^2\\right) \\frac{U_i - U_{i-1}}{h} $$\n\n**2. Nonlinear Residual Equations**\nSubstituting the numerical flux expressions into the discretized conservation law gives a system of $N-1$ nonlinear equations for the $N-1$ interior unknowns $\\{U_1, U_2, \\dots, U_{N-1}\\}$. For each interior node $i=1, \\dots, N-1$, we define a residual function $F_i$:\n$$ F_i(\\vec{U}) = \\frac{1}{h^2} \\left[ \\left(1 + \\left(\\frac{U_i + U_{i+1}}{2}\\right)^2\\right) (U_{i+1} - U_i) - \\left(1 + \\left(\\frac{U_{i-1} + U_i}{2}\\right)^2\\right) (U_i - U_{i-1}) \\right] - \\sin(x_i) = 0 $$\nwhere $\\vec{U} = [U_1, U_2, \\dots, U_{N-1}]^T$. For $i=1$, $U_0$ is replaced by $\\alpha$. For $i=N-1$, $U_N$ is replaced by $\\beta$. Our goal is to find the vector $\\vec{U}$ that solves the system $\\vec{F}(\\vec{U}) = \\vec{0}$.\n\n**3. Newton's Method**\nNewton's method is an iterative procedure for solving systems of nonlinear equations. Starting with an initial guess $\\vec{U}^{(0)}$, we generate a sequence of approximations $\\vec{U}^{(k)}$ that converge to the solution. Each iteration involves solving the linear system:\n$$ J(\\vec{U}^{(k)}) \\Delta\\vec{U}^{(k)} = -\\vec{F}(\\vec{U}^{(k)}) $$\nwhere $J(\\vec{U}^{(k)})$ is the Jacobian matrix of $\\vec{F}$ evaluated at $\\vec{U}^{(k)}$, and $\\Delta\\vec{U}^{(k)}$ is the update step. The next iterate is then found using a line search:\n$$ \\vec{U}^{(k+1)} = \\vec{U}^{(k)} + \\lambda \\Delta\\vec{U}^{(k)} $$\nThe parameter $\\lambda \\in (0, 1]$ is chosen to ensure a sufficient decrease in the norm of the residual.\n\n**4. Jacobian Matrix Derivation**\nThe Jacobian matrix $J$ has entries $J_{ij} = \\frac{\\partial F_i}{\\partial U_j}$. Since the residual $F_i$ depends only on $U_{i-1}$, $U_i$, and $U_{i+1}$, the Jacobian is a tridiagonal matrix.\nTo simplify the derivation, we define the scaled numerical flux term:\n$$ g(u_L, u_R) = \\left(1 + \\left(\\frac{u_L + u_R}{2}\\right)^2\\right) (u_R - u_L) $$\nThe residual equation (multiplied by $h^2$) is $h^2 F_i = g(U_i, U_{i+1}) - g(U_{i-1}, U_i) - h^2 \\sin(x_i)$. The entries of the Jacobian are $J_{ij} = \\frac{1}{h^2}\\frac{\\partial(h^2 F_i)}{\\partial U_j}$. The required partial derivatives of $g$ are:\n$$ \\frac{\\partial g}{\\partial u_L} = -1 + \\frac{1}{4}(u_R + u_L)(u_R - 3u_L) = \\frac{1}{4}(u_R^2 - 2u_L u_R - 3u_L^2 - 4) $$\n$$ \\frac{\\partial g}{\\partial u_R} = 1 + \\frac{1}{4}(u_R + u_L)(3u_R - u_L) = \\frac{1}{4}(3u_R^2 + 2u_L u_R - u_L^2 + 4) $$\nThe non-zero entries of the $i$-th row of the Jacobian are:\n- **Sub-diagonal ($j = i-1$):**\n$$ J_{i, i-1} = -\\frac{1}{h^2} \\frac{\\partial g}{\\partial u_L}(U_{i-1}, U_i) = -\\frac{1}{4h^2}(U_i^2 - 2U_{i-1}U_i - 3U_{i-1}^2 - 4) = \\frac{1}{4h^2}(4 - U_i^2 + 2U_{i-1}U_i + 3U_{i-1}^2) $$\n- **Main-diagonal ($j = i$):**\n$$ J_{i, i} = \\frac{1}{h^2} \\left[ \\frac{\\partial g}{\\partial u_L}(U_i, U_{i+1}) - \\frac{\\partial g}{\\partial u_R}(U_{i-1}, U_i) \\right] = \\frac{1}{4h^2} [ (U_{i+1}^2 - 2U_iU_{i+1} - 3U_i^2 - 4) - (3U_i^2 + 2U_{i-1}U_i - U_{i-1}^2 + 4) ] $$\n$$ J_{i, i} = \\frac{1}{4h^2} (U_{i+1}^2 + U_{i-1}^2 - 6U_i^2 - 2U_i(U_{i+1} + U_{i-1}) - 8) $$\n- **Super-diagonal ($j = i+1$):**\n$$ J_{i, i+1} = \\frac{1}{h^2} \\frac{\\partial g}{\\partial u_R}(U_i, U_{i+1}) = \\frac{1}{4h^2}(3U_{i+1}^2 + 2U_i U_{i+1} - U_i^2 + 4) $$\nThese expressions are used to construct the Jacobian matrix at each Newton iteration.\n\n**5. Line Search Algorithm**\nA backtracking line search is employed to globalize the convergence of Newton's method. After computing the Newton step $\\Delta\\vec{U}^{(k)}$, we find a damping factor $\\lambda$.\n1.  Start with $\\lambda = 1$.\n2.  Compute the candidate next iterate $\\vec{U}_{cand} = \\vec{U}^{(k)} + \\lambda \\Delta\\vec{U}^{(k)}$.\n3.  Evaluate the residual norm at the candidate point, $\\|\\vec{F}(\\vec{U}_{cand})\\|$.\n4.  If $\\|\\vec{F}(\\vec{U}_{cand})\\|  \\|\\vec{F}(\\vec{U}^{(k)})\\|$, accept the step and set $\\vec{U}^{(k+1)} = \\vec{U}_{cand}$.\n5.  Otherwise, reduce $\\lambda$ (e.g., $\\lambda \\leftarrow \\lambda / 2$) and repeat from step 2. The process is terminated if $\\lambda$ becomes smaller than a predefined minimum, indicating a failure to find a suitable descent direction.\n\nThis combination of a second-order finite difference scheme and a robust Newton-Krylov solver provides an effective method for solving the given nonlinear BVP.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the BVP for the specified test cases and print results.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1 (happy path)\n        {'a': 0, 'b': 1, 'alpha': 0, 'beta': 0, 'N': 100, 'x_star': 0.5},\n        # Case 2 (different domain length with symmetric bounds)\n        {'a': 0, 'b': np.pi, 'alpha': 0, 'beta': 0, 'N': 200, 'x_star': np.pi/2},\n        # Case 3 (nonzero Dirichlet data)\n        {'a': 0, 'b': 1, 'alpha': 1, 'beta': -1, 'N': 120, 'x_star': 0.25},\n        # Case 4 (shifted domain including the origin)\n        {'a': -1, 'b': 2, 'alpha': 0, 'beta': 0, 'N': 150, 'x_star': 0},\n    ]\n\n    results = []\n    for case in test_cases:\n        solution = solve_bvp(case['a'], case['b'], case['alpha'], case['beta'], case['N'])\n        \n        h = (case['b'] - case['a']) / case['N']\n        # Find index i_star such that x_star = a + i_star * h\n        i_star = int(round((case['x_star'] - case['a']) / h))\n        \n        result_val = solution[i_star]\n        results.append(f\"{result_val:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\n\ndef solve_bvp(a, b, alpha, beta, N, tol=1e-9, max_iter=50):\n    \"\"\"\n    Solves the nonlinear BVP using Newton's method.\n    \"\"\"\n    h = (b - a) / N\n    x = np.linspace(a, b, N + 1)\n    \n    # Interior grid points and unknowns\n    x_int = x[1:-1]\n    num_unknowns = N - 1\n    \n    # Initial guess for interior nodes: linear interpolation\n    U_int = alpha + (beta - alpha) * (x_int - a) / (b - a)\n    \n    # --- Newton's Method Loop ---\n    for k in range(max_iter):\n        # Full solution vector including boundary conditions\n        U_full = np.concatenate(([alpha], U_int, [beta]))\n        \n        # --- 1. Assemble Residual Vector F ---\n        F = np.zeros(num_unknowns)\n        \n        # Flux term g(uL, uR)\n        def g_flux(uL, uR):\n            return (1 + ((uL + uR) / 2)**2) * (uR - uL)\n\n        for i in range(1, N): # Corresponds to F[i-1]\n            u_left = U_full[i-1]\n            u_mid = U_full[i]\n            u_right = U_full[i+1]\n            \n            q_iph = g_flux(u_mid, u_right) / h\n            q_imh = g_flux(u_left, u_mid) / h\n            \n            F[i-1] = (q_iph - q_imh) / h - np.sin(x[i])\n\n        # Check for convergence\n        res_norm = np.linalg.norm(F)\n        if res_norm  tol:\n            break\n            \n        # --- 2. Assemble Jacobian Matrix J ---\n        J = np.zeros((num_unknowns, num_unknowns))\n        h2 = h * h\n\n        # Partial derivatives of scaled flux g\n        def dg_duL(uL, uR):\n            return 0.25 * (uR**2 - 2*uL*uR - 3*uL**2 - 4)\n\n        def dg_duR(uL, uR):\n            return 0.25 * (3*uR**2 + 2*uL*uR - uL**2 + 4)\n\n        # Diagonal entries\n        for i in range(1, N): # Row i-1 of Jacobian\n            u_left = U_full[i-1]\n            u_mid = U_full[i]\n            u_right = U_full[i+1]\n            \n            diag_val = (dg_duL(u_mid, u_right) - dg_duR(u_left, u_mid)) / h2\n            J[i-1, i-1] = diag_val\n        \n        # Super-diagonal entries\n        for i in range(1, N-1): # Row i-1, Col i\n             J[i-1, i] = dg_duR(U_full[i], U_full[i+1]) / h2\n             \n        # Sub-diagonal entries\n        for i in range(2, N): # Row i-1, Col i-2\n            J[i-1, i-2] = -dg_duL(U_full[i-1], U_full[i]) / h2\n\n        # --- 3. Solve linear system for update step ---\n        delta_U = np.linalg.solve(J, -F)\n        \n        # --- 4. Line Search ---\n        lambda_val = 1.0\n        min_lambda = 1e-8\n        \n        while lambda_val  min_lambda:\n            U_new = U_int + lambda_val * delta_U\n            U_full_new = np.concatenate(([alpha], U_new, [beta]))\n            \n            F_new = np.zeros(num_unknowns)\n            for i in range(1, N):\n                u_left = U_full_new[i-1]\n                u_mid = U_full_new[i]\n                u_right = U_full_new[i+1]\n                q_iph = g_flux(u_mid, u_right) / h\n                q_imh = g_flux(u_left, u_mid) / h\n                F_new[i-1] = (q_iph - q_imh) / h - np.sin(x[i])\n            \n            if np.linalg.norm(F_new)  res_norm:\n                break\n            \n            lambda_val /= 2.0\n            \n        if lambda_val = min_lambda:\n             # If line search fails, maybe it already converged\n             if res_norm  tol:\n                 # In a real application, one might raise an error here.\n                 # For this problem, we assume convergence will be achieved.\n                 pass\n\n        U_int += lambda_val * delta_U\n\n    return np.concatenate(([alpha], U_int, [beta]))\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3228466"}, {"introduction": "Now that you have developed a working nonlinear solver, we can explore one of the most interesting features of nonlinear systems: the existence of multiple distinct solutions. We will investigate the BVP for the physical pendulum, $u''(x) + \\sin(u(x)) = 0$, which admits more than one solution under the same boundary conditions. This exercise demonstrates how the choice of initial guess for Newton's method determines which solution is found, providing a practical look at the concept of basins of attraction [@problem_id:3228545].", "problem": "Consider the two-point boundary value problem (BVP) for a twice continuously differentiable function $u(x)$ on the interval $[0,L]$ given by the nonlinear ordinary differential equation $u''(x)+\\sin(u(x))=0$ with Dirichlet boundary conditions $u(0)=0$ and $u(L)=0$. The task is to construct a finite difference method and a Newton–Raphson method (Newton’s method) to approximate solutions, and then to demonstrate how different initial guesses can lead to distinct solutions when $L=2\\pi$, using angles in radians. Your solution must proceed from fundamental bases as follows.\n\n1. Begin with the definition of the second derivative and use Taylor expansions to obtain a second-order accurate centered finite difference approximation for $u''(x)$ on a uniform grid with $N$ subintervals and grid spacing $h=L/N$. Assemble the discrete nonlinear system for the interior unknowns in vector form, where each interior equation is formed by replacing $u''(x)$ with its centered finite difference approximation and $u(x)$ with the corresponding grid value. Use the boundary conditions $u(0)=0$ and $u(L)=0$ exactly.\n\n2. Derive Newton’s method for the resulting nonlinear algebraic system $\\mathbf{F}(\\mathbf{u})=\\mathbf{0}$ by linearizing $\\mathbf{F}$ through its Jacobian matrix $\\mathbf{J}(\\mathbf{u})$ and solving, at each iteration, the linear system $\\mathbf{J}(\\mathbf{u}^{(k)})\\Delta\\mathbf{u}^{(k)}=-\\mathbf{F}(\\mathbf{u}^{(k)})$ to update $\\mathbf{u}^{(k+1)}=\\mathbf{u}^{(k)}+\\alpha^{(k)}\\Delta\\mathbf{u}^{(k)}$, where $\\alpha^{(k)}$ is a damping factor chosen to ensure a decrease in a suitable norm of the residual. Justify the form of the Jacobian using the chain rule and the derivative of $\\sin(\\cdot)$.\n\n3. Implement the Jacobian as a tridiagonal matrix that captures nearest-neighbor coupling of the centered difference and the local derivative of $\\sin(u)$ evaluated at each iterate. Explain how to efficiently solve the tridiagonal linear systems produced at each Newton iteration.\n\n4. Provide clear stopping criteria based on the residual norm and/or the Newton step norm to ensure convergence to a solution with a well-defined numerical tolerance.\n\n5. Use angles in radians for all trigonometric evaluations. There are no dimensional physical units in this problem beyond the interval length $L$.\n\nYour program must implement the above method generically for any $L0$ and any integer $N\\ge 2$, and then run the following test suite. For each test case, apply Newton’s method with the specified initial guess function $u_0(x)$ mapped to the grid. After convergence, report the $L^\\infty$-norm (maximum absolute value over the discrete grid including endpoints) of the computed solution, rounded to six decimal places.\n\nTest suite (all angles in radians):\n- Case 1 (happy path, trivial solution on the long interval): $L=2\\pi$, $N=400$, initial guess $u_0(x)\\equiv 0$.\n- Case 2 (happy path, positive nontrivial solution on the long interval): $L=2\\pi$, $N=400$, initial guess $u_0(x)=2.5\\sin(\\pi x/L)$.\n- Case 3 (symmetry check, negative nontrivial solution on the long interval): $L=2\\pi$, $N=400$, initial guess $u_0(x)=-2.5\\sin(\\pi x/L)$.\n- Case 4 (edge case, interval too short for nontrivial solution consistent with the small-amplitude period): $L=\\pi$, $N=400$, initial guess $u_0(x)=2.5\\sin(\\pi x/L)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the above cases, for example, $[r_1,r_2,r_3,r_4]$, where each $r_i$ is the $L^\\infty$-norm of the computed solution for the $i$-th case, rounded to six decimal places (e.g., $[0.000000,2.800000,2.800000,0.000000]$). Do not print any additional text.", "solution": "The user-provided problem is assessed to be **valid**. It is a well-posed, scientifically grounded problem in numerical analysis, specifically concerning the solution of a nonlinear two-point boundary value problem using the finite difference method and Newton's method. All necessary data, equations, and conditions are provided, and there are no internal contradictions, ambiguities, or factual unsoundness.\n\n### 1. Discretization of the Boundary Value Problem\n\nThe problem is to solve the nonlinear ordinary differential equation (ODE)\n$$\nu''(x) + \\sin(u(x)) = 0\n$$\non the interval $x \\in [0, L]$ with homogeneous Dirichlet boundary conditions $u(0)=0$ and $u(L)=0$.\n\nWe discretize the domain $[0, L]$ using a uniform grid with $N$ subintervals. The grid points are $x_i = i h$ for $i = 0, 1, \\dots, N$, where the grid spacing is $h = L/N$. We seek to find approximations $u_i \\approx u(x_i)$ at these grid points. The boundary conditions give $u_0 = 0$ and $u_N = 0$. The unknown values are the interior grid points $u_1, u_2, \\dots, u_{N-1}$. We denote the vector of these unknowns as $\\mathbf{u} = [u_1, u_2, \\dots, u_{N-1}]^T \\in \\mathbb{R}^{N-1}$.\n\nTo discretize the second derivative term $u''(x)$, we use a second-order accurate centered finite difference approximation. This is derived from Taylor series expansions of $u(x)$ around a point $x_i$:\n$$\nu(x_i+h) = u(x_i) + h u'(x_i) + \\frac{h^2}{2} u''(x_i) + \\frac{h^3}{6} u'''(x_i) + O(h^4)\n$$\n$$\nu(x_i-h) = u(x_i) - h u'(x_i) + \\frac{h^2}{2} u''(x_i) - \\frac{h^3}{6} u'''(x_i) + O(h^4)\n$$\nAdding these two expansions eliminates the odd-derivative terms:\n$$\nu(x_i+h) + u(x_i-h) = 2u(x_i) + h^2 u''(x_i) + O(h^4)\n$$\nSolving for $u''(x_i)$ yields the approximation:\n$$\nu''(x_i) = \\frac{u(x_i+h) - 2u(x_i) + u(x_i-h)}{h^2} + O(h^2)\n$$\nIn terms of our discrete variables $u_i$, this is $u''(x_i) \\approx \\frac{u_{i+1} - 2u_i + u_{i-1}}{h^2}$.\n\nBy replacing the continuous derivatives and functions with their discrete counterparts in the ODE at each interior grid point $x_i$ for $i=1, \\dots, N-1$, we obtain a system of $N-1$ nonlinear algebraic equations:\n$$\n\\frac{u_{i+1} - 2u_i + u_{i-1}}{h^2} + \\sin(u_i) = 0, \\quad \\text{for } i = 1, 2, \\dots, N-1\n$$\nWe can write this system in vector form $\\mathbf{F}(\\mathbf{u}) = \\mathbf{0}$, where $\\mathbf{F}: \\mathbb{R}^{N-1} \\to \\mathbb{R}^{N-1}$ and its $i$-th component $F_i(\\mathbf{u})$ is:\n$$\nF_i(\\mathbf{u}) = \\frac{u_{i+1} - 2u_i + u_{i-1}}{h^2} + \\sin(u_i)\n$$\nThe boundary conditions $u_0=0$ and $u_N=0$ are incorporated at the endpoints of the system. For $i=1$, the equation is $\\frac{u_2 - 2u_1 + u_0}{h^2} + \\sin(u_1) = 0$, which becomes $\\frac{u_2 - 2u_1}{h^2} + \\sin(u_1) = 0$. For $i=N-1$, the equation is $\\frac{u_N - 2u_{N-1} + u_{N-2}}{h^2} + \\sin(u_{N-1}) = 0$, which becomes $\\frac{-2u_{N-1} + u_{N-2}}{h^2} + \\sin(u_{N-1}) = 0$.\n\n### 2. Newton's Method for the Nonlinear System\n\nNewton's method is an iterative procedure for finding roots of a system of nonlinear equations $\\mathbf{F}(\\mathbf{u}) = \\mathbf{0}$. Starting from an initial guess $\\mathbf{u}^{(0)}$, the method generates a sequence of approximations $\\mathbf{u}^{(k)}$ that ideally converge to a solution. The update rule is derived by linearizing $\\mathbf{F}(\\mathbf{u})$ around the current iterate $\\mathbf{u}^{(k)}$:\n$$\n\\mathbf{F}(\\mathbf{u}) \\approx \\mathbf{F}(\\mathbf{u}^{(k)}) + \\mathbf{J}(\\mathbf{u}^{(k)})(\\mathbf{u} - \\mathbf{u}^{(k)})\n$$\nwhere $\\mathbf{J}(\\mathbf{u}^{(k)})$ is the Jacobian matrix of $\\mathbf{F}$ evaluated at $\\mathbf{u}^{(k)}$. Setting $\\mathbf{F}(\\mathbf{u}) = \\mathbf{0}$ to find the next iterate $\\mathbf{u}^{(k+1)}$, we solve for the update step $\\Delta\\mathbf{u}^{(k)} = \\mathbf{u}^{(k+1)} - \\mathbf{u}^{(k)}$:\n$$\n\\mathbf{J}(\\mathbf{u}^{(k)}) \\Delta\\mathbf{u}^{(k)} = -\\mathbf{F}(\\mathbf{u}^{(k)})\n$$\nThe next iterate is then found as $\\mathbf{u}^{(k+1)} = \\mathbf{u}^{(k)} + \\Delta\\mathbf{u}^{(k)}$. For improved convergence and robustness, a damping factor (or step size) $\\alpha^{(k)} \\in (0, 1]$ is introduced:\n$$\n\\mathbf{u}^{(k+1)} = \\mathbf{u}^{(k)} + \\alpha^{(k)} \\Delta\\mathbf{u}^{(k)}\n$$\nThe value of $\\alpha^{(k)}$ is typically chosen using a line search algorithm to ensure that the norm of the residual decreases at each step, e.g., $\\|\\mathbf{F}(\\mathbf{u}^{(k+1)})\\|  \\|\\mathbf{F}(\\mathbf{u}^{(k)})\\|$.\n\nThe Jacobian matrix $\\mathbf{J}$ is an $(N-1) \\times (N-1)$ matrix with entries $J_{ij} = \\frac{\\partial F_i}{\\partial u_j}$. We compute these partial derivatives from the expression for $F_i(\\mathbf{u})$:\n$$\nJ_{ij} = \\frac{\\partial}{\\partial u_j} \\left( \\frac{u_{i+1} - 2u_i + u_{i-1}}{h^2} + \\sin(u_i) \\right)\n$$\nThe derivative of the finite difference term is nonzero only when $j$ is $i-1$, $i$, or $i+1$:\n- For $j = i-1$: $J_{i,i-1} = \\frac{\\partial F_i}{\\partial u_{i-1}} = \\frac{1}{h^2}$\n- For $j = i+1$: $J_{i,i+1} = \\frac{\\partial F_i}{\\partial u_{i+1}} = \\frac{1}{h^2}$\nThe derivative of the nonlinear term $\\sin(u_i)$, by the chain rule, contributes only to the diagonal entry: $\\frac{\\partial}{\\partial u_j} \\sin(u_i) = \\cos(u_i) \\frac{\\partial u_i}{\\partial u_j} = \\cos(u_i) \\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta.\n- For $j = i$: $J_{ii} = \\frac{\\partial F_i}{\\partial u_i} = -\\frac{2}{h^2} + \\cos(u_i)$\n- For $|i-j|  1$: $J_{ij} = 0$\n\n### 3. The Tridiagonal Jacobian System\n\nThe resulting Jacobian matrix is a symmetric, tridiagonal matrix:\n$$\n\\mathbf{J}(\\mathbf{u}) = \\begin{pmatrix}\n-\\frac{2}{h^2}+\\cos(u_1)  \\frac{1}{h^2}  0  \\dots  0 \\\\\n\\frac{1}{h^2}  -\\frac{2}{h^2}+\\cos(u_2)  \\frac{1}{h^2}  \\dots  0 \\\\\n0  \\ddots  \\ddots  \\ddots  0 \\\\\n\\vdots   \\frac{1}{h^2}  -\\frac{2}{h^2}+\\cos(u_{N-2})  \\frac{1}{h^2} \\\\\n0  \\dots  0  \\frac{1}{h^2}  -\\frac{2}{h^2}+\\cos(u_{N-1})\n\\end{pmatrix}\n$$\nAt each iteration of Newton's method, we must solve the linear system $\\mathbf{J}(\\mathbf{u}^{(k)}) \\Delta\\mathbf{u}^{(k)} = -\\mathbf{F}(\\mathbf{u}^{(k)})$. Since $\\mathbf{J}$ is tridiagonal, this system can be solved very efficiently using the Thomas algorithm (also known as the Tridiagonal Matrix Algorithm, TDMA). The Thomas algorithm is a specialized form of Gaussian elimination with a computational complexity of $O(N)$, which is significantly faster than the $O(N^3)$ complexity required for general dense matrices. This efficiency is crucial for problems with a large number of grid points $N$.\n\n### 4. Stopping Criteria\n\nThe iterative process is terminated when the solution has converged to a desired precision. A common and robust stopping criterion is to check the norm of the residual vector $\\mathbf{F}(\\mathbf{u}^{(k)})$. We stop the iterations when the infinity norm (maximum absolute component) of the residual falls below a small tolerance $\\epsilon$:\n$$\n\\|\\mathbf{F}(\\mathbf{u}^{(k)})\\|_\\infty  \\epsilon\n$$\nA typical value for $\\epsilon$ is $10^{-10}$ or smaller. Additionally, a maximum number of iterations is set to prevent infinite loops in case of non-convergence.\n\n### 5. Final Solution Evaluation\n\nAfter the Newton's method converges to a discrete solution vector $\\mathbf{u}_{\\text{sol}} = [u_1, \\dots, u_{N-1}]^T$, the full solution on the grid is constructed by including the boundary points:\n$$\n\\mathbf{u}_{\\text{full}} = [u_0, u_1, \\dots, u_{N-1}, u_N]^T = [0, u_1, \\dots, u_{N-1}, 0]^T\n$$\nThe $L^\\infty$-norm of the computed solution is then the maximum absolute value over this complete set of grid points:\n$$\n\\|\\mathbf{u}_{\\text{full}}\\|_\\infty = \\max_{i=0, \\dots, N} |u_i|\n$$\nThis value is reported for each test case. All trigonometric evaluations must use radians, which is the standard for scientific computing libraries.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef thomas_solver(a, b, c, d):\n    \"\"\"\n    Solves a tridiagonal system of equations Ax = d.\n    a: sub-diagonal (size m-1)\n    b: main diagonal (size m)\n    c: super-diagonal (size m-1)\n    d: right-hand side (size m)\n    \"\"\"\n    m = len(b)\n    # Create copies to avoid modifying the original arrays\n    ac, bc, cc, dc = map(np.copy, (a, b, c, d))\n    \n    # Forward elimination\n    for i in range(1, m):\n        w = ac[i-1] / bc[i-1]\n        bc[i] = bc[i] - w * cc[i-1]\n        dc[i] = dc[i] - w * dc[i-1]\n        \n    # Backward substitution\n    x = np.zeros(m)\n    x[m-1] = dc[m-1] / bc[m-1]\n    for i in range(m-2, -1, -1):\n        x[i] = (dc[i] - cc[i] * x[i+1]) / bc[i]\n        \n    return x\n\ndef newton_bvp_solver(L, N, initial_guess_func, tol=1e-10, max_iter=50):\n    \"\"\"\n    Solves the BVP u'' + sin(u) = 0 with u(0)=u(L)=0 using Newton's method.\n    \n    Args:\n        L (float): Length of the interval [0, L].\n        N (int): Number of subintervals.\n        initial_guess_func (callable): A function u_0(x) for the initial guess.\n        tol (float): Convergence tolerance for the residual norm.\n        max_iter (int): Maximum number of Newton iterations.\n    \n    Returns:\n        numpy.ndarray: The solution vector for the interior grid points.\n    \"\"\"\n    h = L / N\n    m = N - 1  # Number of interior points (unknowns)\n    x_interior = np.linspace(0, L, N + 1)[1:-1]\n    \n    u = initial_guess_func(x_interior)\n    \n    h2_inv = 1.0 / (h * h)\n\n    def compute_residual(u_vec):\n        # Create a padded vector with boundary conditions u(0)=0, u(L)=0\n        u_padded = np.concatenate(([0.0], u_vec, [0.0]))\n        # Compute the finite difference approximation of u''\n        u_xx = (u_padded[2:] - 2 * u_padded[1:-1] + u_padded[:-2]) * h2_inv\n        # Compute the residual F(u) = u'' + sin(u)\n        return u_xx + np.sin(u_vec)\n\n    for k in range(max_iter):\n        # 1. Compute residual F(u)\n        F = compute_residual(u)\n        \n        # 2. Check for convergence\n        res_norm = np.max(np.abs(F))\n        if res_norm  tol:\n            return u\n        \n        # 3. Compute Jacobian diagonals\n        diag_b = -2.0 * h2_inv + np.cos(u)\n        diag_a = np.full(m - 1, h2_inv)\n        diag_c = np.full(m - 1, h2_inv)\n        \n        # 4. Solve J * delta_u = -F for the Newton step delta_u\n        # The Jacobian is symmetric, so sub-diagonal and super-diagonal are identical\n        delta_u = thomas_solver(diag_a, diag_b, diag_c, -F)\n        \n        # 5. Damped update using a simple backtracking line search\n        alpha = 1.0\n        u_new = u + alpha * delta_u\n        res_norm_new = np.max(np.abs(compute_residual(u_new)))\n        \n        # Reduce alpha until the residual norm decreases\n        while res_norm_new = res_norm and alpha  1e-4:\n            alpha /= 2.0\n            u_new = u + alpha * delta_u\n            res_norm_new = np.max(np.abs(compute_residual(u_new)))\n\n        # 6. Update solution\n        u = u_new\n        \n        # Check if the update step is too small\n        if np.max(np.abs(alpha * delta_u))  tol:\n            return u\n\n    raise RuntimeError(f\"Newton's method failed to converge for L={L}, N={N} within {max_iter} iterations.\")\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        {'L': 2 * np.pi, 'N': 400, 'u0_func': lambda x: np.zeros_like(x)},\n        {'L': 2 * np.pi, 'N': 400, 'u0_func': lambda x: 2.5 * np.sin(np.pi * x / (2 * np.pi))},\n        {'L': 2 * np.pi, 'N': 400, 'u0_func': lambda x: -2.5 * np.sin(np.pi * x / (2 * np.pi))},\n        {'L': np.pi,     'N': 400, 'u0_func': lambda x: 2.5 * np.sin(np.pi * x / np.pi)}\n    ]\n\n    results = []\n    for case in test_cases:\n        L = case['L']\n        N = case['N']\n        u0_func = case['u0_func']\n        \n        # Compute the numerical solution for the interior points\n        u_solution = newton_bvp_solver(L, N, u0_func)\n        \n        # Construct full solution including boundaries to calculate L-infinity norm\n        u_full = np.concatenate(([0.0], u_solution, [0.0]))\n        \n        # Calculate the L-infinity norm of the full solution\n        L_inf_norm = np.max(np.abs(u_full))\n        results.append(L_inf_norm)\n\n    # Format the final output string exactly as specified\n    # The f-string formatting automatically rounds to the specified number of decimal places.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "3228545"}, {"introduction": "Writing code is one thing; knowing it is correct is another. This final practice introduces a cornerstone of scientific software development: code verification using the Method of Manufactured Solutions (MMS). Instead of solving a problem with an unknown answer, we will invent a problem by first choosing a desired exact solution, $u_{\\text{exact}}(x)$, and then deriving the forcing function $f(x)$ that makes it a valid solution. By running your solver on this 'manufactured' problem, you can precisely measure the error and empirically verify that your implementation achieves the expected theoretical order of accuracy, a critical step in building confidence in any numerical tool [@problem_id:3228560].", "problem": "You will implement the method of manufactured solutions for a nonlinear boundary value problem (BVP) on a one-dimensional domain to verify the correctness and empirical order of accuracy of a finite difference solver that uses Newton’s method for nonlinear systems. The governing equation is the second-order ordinary differential equation on the interval $[0,1]$:\n$$\nu''(x) + g(u(x)) = f(x), \\quad x \\in (0,1),\n$$\nwith Dirichlet boundary conditions\n$$\nu(0) = \\beta_0, \\quad u(1) = \\beta_1.\n$$\nAll trigonometric functions must use angles in radians. There are no physical units in this problem. The foundation for the computational method must start from the following well-tested facts: (i) central finite differences provide approximations to derivatives on uniform grids, (ii) Newton’s method solves nonlinear algebraic systems by iteratively linearizing the residual, and (iii) the method of manufactured solutions constructs a consistent right-hand side $f(x)$ by substituting an exact, smooth function into the differential equation and evaluating the left-hand side.\n\nYou will design three manufactured test cases by selecting a smooth exact solution $u_{\\text{exact}}(x)$ and a nonlinear function $g(u)$, then defining the forcing $f(x)$ to make $u_{\\text{exact}}$ satisfy the equation exactly. Your solver must not use any knowledge of $u_{\\text{exact}}(x)$ except for setting the Dirichlet boundary values; it must compute an approximate numerical solution $u_{\\text{num}}(x)$ by solving the discretized nonlinear system on a uniform grid with $N$ nodes. For each test case, compute the maximum norm error\n$$\nE(N) = \\max_{0 \\le i \\le N-1} \\left| u_{\\text{num}}(x_i) - u_{\\text{exact}}(x_i) \\right|,\n$$\nwhere $x_i$ are the grid nodes. Evaluate $E(N)$ on a refinement sequence $N \\in \\{33, 65, 129, 257\\}$ and then report the observed order of accuracy using the last two refinements:\n$$\np = \\frac{\\log\\left(E(N_3)/E(N_4)\\right)}{\\log\\left(h_3/h_4\\right)},\n$$\nwhere $N_3 = 129$, $N_4 = 257$, $h_k = 1/(N_k-1)$, and $\\log$ denotes the natural logarithm.\n\nImplement the following test suite of manufactured solutions:\n\n- Test case $1$ (happy path, nonzero Dirichlet boundaries): \n  - Exact solution $u_{\\text{exact}}(x) = 1 + 0.1 \\sin(2\\pi x)$.\n  - Nonlinearity $g(u) = u^3$.\n  - Define $f(x) = u_{\\text{exact}}''(x) + \\big(u_{\\text{exact}}(x)\\big)^3$.\n  - Boundary values $\\beta_0 = u_{\\text{exact}}(0)$ and $\\beta_1 = u_{\\text{exact}}(1)$.\n\n- Test case $2$ (nonlinearity with exponential term, zero Dirichlet boundaries):\n  - Exact solution $u_{\\text{exact}}(x) = 0.1 \\sin(3\\pi x)$.\n  - Nonlinearity $g(u) = e^{u} - 1$.\n  - Define $f(x) = u_{\\text{exact}}''(x) + e^{u_{\\text{exact}}(x)} - 1$.\n  - Boundary values $\\beta_0 = u_{\\text{exact}}(0)$ and $\\beta_1 = u_{\\text{exact}}(1)$.\n\n- Test case $3$ (stronger smooth nonlinearity, zero Dirichlet boundaries):\n  - Exact solution $u_{\\text{exact}}(x) = 0.15 \\sin(\\pi x) + 0.05 \\sin(4\\pi x)$.\n  - Nonlinearity $g(u) = \\alpha \\sin(u)$ with $\\alpha = 2.5$.\n  - Define $f(x) = u_{\\text{exact}}''(x) + \\alpha \\sin\\big(u_{\\text{exact}}(x)\\big)$.\n  - Boundary values $\\beta_0 = u_{\\text{exact}}(0)$ and $\\beta_1 = u_{\\text{exact}}(1)$.\n\nAlgorithmic requirements:\n\n- Use a uniform grid on $[0,1]$ with $N$ nodes and spacing $h = 1/(N-1)$.\n- Discretize the second derivative using the central difference approximation.\n- Form the nonlinear residual at interior nodes by combining the discrete second derivative, $g(u)$ evaluated at the node, and the forcing $f(x)$.\n- Solve the resulting nonlinear system with Newton’s method. Construct the Jacobian using the derivative $g'(u)$ and the tridiagonal stencil from the second derivative.\n- Initialize the interior unknowns with a simple guess (for example, the linear interpolation between boundary values or zeros) and iterate until the residual infinity norm is below a small tolerance.\n\nOutput specification:\n\n- For each of the three test cases, compute the observed order $p$ using $N \\in \\{33, 65, 129, 257\\}$ as described above.\n- Your program should produce a single line of output containing the three observed orders as a comma-separated list enclosed in square brackets (for example, $[p_1,p_2,p_3]$). The entries must be floating-point numbers.\n\nYour implementation must be a complete, runnable program in a modern programming language, and it must not require any user input. All trigonometric calculations must use angles in radians.", "solution": "The user has provided a well-posed problem in numerical analysis, tasking me with verifying a finite difference solver for a nonlinear boundary value problem (BVP) using the method of manufactured solutions. The problem is scientifically grounded, formally specified, and internally consistent. I will now proceed with a detailed solution.\n\n### 1. Problem Formulation and Discretization\n\nThe governing nonlinear second-order ordinary differential equation (ODE) is defined on the domain $x \\in [0,1]$:\n$$\nu''(x) + g(u(x)) = f(x)\n$$\nsubject to Dirichlet boundary conditions:\n$$\nu(0) = \\beta_0, \\quad u(1) = \\beta_1\n$$\nTo solve this BVP numerically, we discretize the continuous domain $[0,1]$ into a uniform grid with $N$ nodes. The grid points are denoted by $x_i = i h$ for $i=0, 1, \\dots, N-1$, where the grid spacing is $h = 1/(N-1)$. Let $u_i$ be the numerical approximation to the exact solution $u(x_i)$ at each grid node. The values at the boundaries are known: $u_0 = \\beta_0$ and $u_{N-1} = \\beta_1$. The remaining $N-2$ interior values, $u_1, u_2, \\dots, u_{N-2}$, are the unknowns to be determined.\n\nThe second derivative $u''(x)$ at an interior node $x_i$ (where $1 \\le i \\le N-2$) is approximated using a second-order accurate central finite difference formula:\n$$\nu''(x_i) \\approx \\frac{u_{i-1} - 2u_i + u_{i+1}}{h^2}\n$$\nSubstituting this approximation into the ODE yields a system of $N-2$ coupled nonlinear algebraic equations for the interior unknowns:\n$$\n\\frac{u_{i-1} - 2u_i + u_{i+1}}{h^2} + g(u_i) = f(x_i), \\quad \\text{for } i = 1, 2, \\dots, N-2.\n$$\nThese equations, along with the given boundary values, form the discrete problem.\n\n### 2. Newton's Method for the Nonlinear System\n\nThe system of discrete equations can be written in the form of a root-finding problem. Let $\\mathbf{U} = [u_1, u_2, \\dots, u_{N-2}]^T$ be the vector of $M = N-2$ interior unknowns. We define a residual vector function $\\mathbf{R}(\\mathbf{U})$ of size $M$, whose components $R_i$ correspond to the $i$-th interior grid point (using indices $i=1, \\dots, M$ for the vector components, which map to grid indices $i, \\dots, i+M-1$). For consistency with programming array indices, let's use $j = 0, \\dots, M-1$ for the vector index, where $u_j^{\\text{vec}} = u_{j+1}^{\\text{grid}}$. The $j$-th component of the residual is:\n$$\nR_j(\\mathbf{U}) = \\frac{u_{j} - 2u_{j+1} + u_{j+2}}{h^2} + g(u_{j+1}) - f(x_{j+1}), \\quad \\text{for } j = 0, \\dots, M-1.\n$$\nIn this notation, $u_0$ and $u_{N-1}$ are the known boundary values $\\beta_0$ and $\\beta_1$. The problem is to find the vector $\\mathbf{U}$ such that $\\mathbf{R}(\\mathbf{U}) = \\mathbf{0}$.\n\nWe solve this nonlinear system using Newton's method. Starting with an initial guess $\\mathbf{U}^{(0)}$, the method iteratively refines the solution using the update rule:\n$$\n\\mathbf{U}^{(k+1)} = \\mathbf{U}^{(k)} + \\Delta \\mathbf{U}^{(k)}\n$$\nwhere the update vector $\\Delta \\mathbf{U}^{(k)}$ is the solution to the linear system:\n$$\nJ(\\mathbf{U}^{(k)}) \\Delta \\mathbf{U}^{(k)} = -\\mathbf{R}(\\mathbf{U}^{(k)})\n$$\nHere, $J(\\mathbf{U}^{(k)})$ is the Jacobian matrix of the residual function $\\mathbf{R}$ evaluated at the current iterate $\\mathbf{U}^{(k)}$. The iteration continues until the norm of the residual, $\\|\\mathbf{R}(\\mathbf{U}^{(k)})\\|_{\\infty}$, falls below a specified tolerance.\n\n### 3. Jacobian Matrix Derivation\n\nThe Jacobian matrix $J$ is an $M \\times M$ matrix with elements $J_{ij} = \\frac{\\partial R_i}{\\partial U_j}$, where $R_i$ is the $i$-th component of the residual vector and $U_j$ is the $j$-th component of the unknown vector $\\mathbf{U}$ (mapping to grid unknowns $u_{i+1}$ and $u_{j+1}$). The $i$-th residual equation (for grid point $i$, $1 \\le i \\le N-2$) is:\n$$\nR_i(u_1, \\dots, u_{N-2}) = \\frac{u_{i-1} - 2u_i + u_{i+1}}{h^2} + g(u_i) - f_i\n$$\nThe partial derivatives with respect to the unknown $u_j$ are non-zero only if $j$ is $i-1$, $i$, or $i+1$.\n- Diagonal elements ($j=i$):\n  $$\n  \\frac{\\partial R_i}{\\partial u_i} = -\\frac{2}{h^2} + g'(u_i)\n  $$\n- Sub-diagonal elements ($j=i-1$, for $i1$):\n  $$\n  \\frac{\\partial R_i}{\\partial u_{i-1}} = \\frac{1}{h^2}\n  $$\n- Super-diagonal elements ($j=i+1$, for $iN-2$):\n  $$\n  \\frac{\\partial R_i}{\\partial u_{i+1}} = \\frac{1}{h^2}\n  $$\nAll other partial derivatives are zero. Thus, the Jacobian is a tridiagonal matrix, which allows the linear system $J \\Delta \\mathbf{U} = -\\mathbf{R}$ to be solved efficiently, typically in $O(M)$ operations.\n\n### 4. Method of Manufactured Solutions and Verification\n\nThe method of manufactured solutions is used to verify the correctness and accuracy of the numerical solver. The process involves a few steps:\n1.  Choose a smooth, non-trivial function $u_{\\text{exact}}(x)$ that will serve as the exact solution.\n2.  Choose a nonlinear function $g(u)$.\n3.  Substitute $u_{\\text{exact}}(x)$ into the left-hand side of the differential equation, $u''(x) + g(u(x))$, to define the forcing function $f(x)$. By construction, $f(x) = u_{\\text{exact}}''(x) + g(u_{\\text{exact}}(x))$.\n4.  The boundary conditions are determined from the manufactured solution: $\\beta_0 = u_{\\text{exact}}(0)$ and $\\beta_1 = u_{\\text{exact}}(1)$.\n\nThe numerical solver is then used to solve the BVP $u'' + g(u) = f$ with these boundary conditions. The solver itself has no knowledge of $u_{\\text{exact}}(x)$, it only uses $g(u)$, $f(x)$, and the boundary values. The resulting numerical solution $u_{\\text{num}}(x_i)$ can then be compared against the known exact solution $u_{\\text{exact}}(x_i)$ to compute the error.\n\n### 5. Error Analysis and Order of Accuracy\n\nThe error of the numerical solution is quantified using the maximum norm (or infinity norm) over all grid points:\n$$\nE(N) = \\max_{0 \\le i \\le N-1} \\left| u_{\\text{num}}(x_i) - u_{\\text{exact}}(x_i) \\right|\n$$\nThe theoretical order of accuracy, $p$, describes how the error decreases as the grid spacing $h$ is reduced: $E \\approx C h^p$ for some constant $C$. To measure this empirically, we compute the error on a sequence of refined grids. Given the errors $E_1$ and $E_2$ on two grids with respective spacings $h_1$ and $h_2$, the observed order of accuracy is calculated as:\n$$\np = \\frac{\\log(E_1 / E_2)}{\\log(h_1 / h_2)}\n$$\nFor this problem, we use the results from the last two refinements, $N_3=129$ and $N_4=257$. The corresponding grid spacings are $h_3 = 1/(N_3-1) = 1/128$ and $h_4 = 1/(N_4-1) = 1/256$. The ratio $h_3/h_4 = 2$, so the formula simplifies to:\n$$\np = \\frac{\\log(E(129) / E(257))}{\\log(2)}\n$$\nSince the central difference scheme for $u''$ is second-order accurate ($O(h^2)$), the expected value for $p$ is approximately $2$.\n\nThe provided Python code implements this entire procedure for the three specified test cases. It defines the manufactured solutions, constructs and solves the nonlinear systems using Newton's method on a sequence of grids, and computes the final observed order of accuracy for each case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_bvp_newton(N, u_exact, u_pp_exact, g, g_prime, tol=1e-12, max_iter=50):\n    \"\"\"\n    Solves the nonlinear BVP u'' + g(u) = f using a finite difference\n    method with Newton's method for the resulting nonlinear system.\n\n    Args:\n        N (int): Number of grid points.\n        u_exact (callable): The exact solution function u(x).\n        u_pp_exact (callable): The second derivative of the exact solution u''(x).\n        g (callable): The nonlinear function g(u).\n        g_prime (callable): The derivative of the nonlinear function g'(u).\n        tol (float): Tolerance for Newton's method convergence.\n        max_iter (int): Maximum number of Newton iterations.\n\n    Returns:\n        float: The maximum norm error between the numerical and exact solutions.\n    \"\"\"\n    # 1. Grid and problem setup\n    h = 1.0 / (N - 1)\n    x = np.linspace(0, 1, N)\n    \n    # Boundary conditions from the exact solution\n    beta0 = u_exact(x[0])\n    beta1 = u_exact(x[-1])\n    \n    # Forcing function f(x) is manufactured from the exact solution\n    f_vals = u_pp_exact(x) + g(u_exact(x))\n    \n    # Interior grid points and corresponding forcing function values\n    x_int = x[1:-1]\n    f_int = f_vals[1:-1]\n    M = N - 2\n    \n    if M = 0:\n        # Trivial case with no interior points\n        u_num = np.array([beta0, beta1]) if N == 2 else np.array([beta0])\n        u_ex_vals = u_exact(x)\n        return np.linalg.norm(u_num - u_ex_vals, np.inf)\n\n    # 2. Initial guess for interior unknowns\n    # Use linear interpolation between boundary values\n    u_int = beta0 + (beta1 - beta0) * x_int\n    \n    # 3. Newton's method iterations\n    h_sq = h * h\n    \n    for k in range(max_iter):\n        # Build residual vector R\n        R = np.zeros(M)\n        \n        # First interior point (i=1)\n        R[0] = (beta0 - 2 * u_int[0] + u_int[1]) / h_sq + g(u_int[0]) - f_int[0]\n        \n        # Central interior points (1  i  N-2)\n        if M  2:\n            R[1:-1] = (u_int[:-2] - 2 * u_int[1:-1] + u_int[2:]) / h_sq + g(u_int[1:-1]) - f_int[1:-1]\n\n        # Last interior point (i=N-2)\n        if M  1:\n            R[-1] = (u_int[-2] - 2 * u_int[-1] + beta1) / h_sq + g(u_int[-1]) - f_int[-1]\n\n        residual_norm = np.linalg.norm(R, np.inf)\n        if residual_norm  tol:\n            break\n            \n        # Build Jacobian matrix J (tridiagonal)\n        diag = -2.0 / h_sq + g_prime(u_int)\n        sup_diag = np.ones(M - 1) / h_sq\n        sub_diag = np.ones(M - 1) / h_sq\n        \n        J = np.diag(diag) + np.diag(sup_diag, k=1) + np.diag(sub_diag, k=-1)\n        \n        # Solve the linear system J * delta_u = -R\n        delta_u = np.linalg.solve(J, -R)\n        \n        # Update solution for interior points\n        u_int += delta_u\n        \n    # 4. Construct full numerical solution and compute error\n    u_num = np.concatenate(([beta0], u_int, [beta1]))\n    u_ex_vals = u_exact(x)\n    error = np.linalg.norm(u_num - u_ex_vals, np.inf)\n    \n    return error\n\ndef solve():\n    \"\"\"\n    Main function to run the verification for all test cases and print the results.\n    \"\"\"\n    pi = np.pi\n    alpha = 2.5\n\n    test_cases = [\n        {\n            \"name\": \"Case 1\",\n            \"u_exact\": lambda x: 1 + 0.1 * np.sin(2 * pi * x),\n            \"u_pp_exact\": lambda x: -0.1 * (2*pi)**2 * np.sin(2 * pi * x),\n            \"g\": lambda u: u**3,\n            \"g_prime\": lambda u: 3 * u**2\n        },\n        {\n            \"name\": \"Case 2\",\n            \"u_exact\": lambda x: 0.1 * np.sin(3 * pi * x),\n            \"u_pp_exact\": lambda x: -0.1 * (3*pi)**2 * np.sin(3 * pi * x),\n            \"g\": lambda u: np.exp(u) - 1.0,\n            \"g_prime\": lambda u: np.exp(u)\n        },\n        {\n            \"name\": \"Case 3\",\n            \"u_exact\": lambda x: 0.15 * np.sin(pi * x) + 0.05 * np.sin(4 * pi * x),\n            \"u_pp_exact\": lambda x: -0.15 * pi**2 * np.sin(pi * x) - 0.05 * (4*pi)**2 * np.sin(4 * pi * x),\n            \"g\": lambda u: alpha * np.sin(u),\n            \"g_prime\": lambda u: alpha * np.cos(u)\n        }\n    ]\n    \n    N_values = [33, 65, 129, 257]\n    observed_orders = []\n\n    for case in test_cases:\n        errors = []\n        for N in N_values:\n            error = solve_bvp_newton(N, \n                                     case[\"u_exact\"], \n                                     case[\"u_pp_exact\"], \n                                     case[\"g\"], \n                                     case[\"g_prime\"])\n            errors.append(error)\n        \n        # Compute the observed order of accuracy 'p' using the last two refinements\n        # p = log(E_3/E_4) / log(h_3/h_4)\n        E3 = errors[2]  # Error for N=129\n        E4 = errors[3]  # Error for N=257\n        N3 = N_values[2]\n        N4 = N_values[3]\n        h3 = 1.0 / (N3 - 1)\n        h4 = 1.0 / (N4 - 1)\n        \n        # Note: log(h3/h4) = log((1/128)/(1/256)) = log(2)\n        p = np.log(E3 / E4) / np.log(h3 / h4)\n        observed_orders.append(p)\n        \n    print(f\"[{','.join(map(str, observed_orders))}]\")\n\nsolve()\n\n```", "id": "3228560"}]}