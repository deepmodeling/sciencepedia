{"hands_on_practices": [{"introduction": "Mastering induced matrix norms begins with applying their fundamental definitions. This exercise focuses on the Givens rotation matrix, a fundamental tool in numerical algorithms used for tasks like QR factorization. By deriving its $1$-, $2$-, and $\\infty$-norms directly from the supremum definition, you will build a strong intuition for how a matrix's structure determines its amplification effect on vectors measured in different ways [@problem_id:3242259].", "problem": "In many algorithms for numerical linear algebra within numerical methods and scientific computing, a Givens rotation is used to introduce zeros while preserving lengths. Let $n \\geq 2$ and let $G \\in \\mathbb{R}^{n \\times n}$ be a Givens rotation that acts on coordinates $i$ and $j$ (with $1 \\leq i  j \\leq n$) by the planar rotation angle $\\theta \\in \\mathbb{R}$, so that, in the $2 \\times 2$ submatrix on rows and columns $i$ and $j$, $G$ has entries $G_{ii} = \\cos \\theta$, $G_{ij} = \\sin \\theta$, $G_{ji} = -\\sin \\theta$, $G_{jj} = \\cos \\theta$, and $G$ equals the identity matrix elsewhere. Using only the definition of the induced matrix norm\n$$\n\\|A\\|_{p} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{p}}{\\|x\\|_{p}},\n$$\nderive the induced $1$-, $2$-, and $\\infty$-norms of $G$ as explicit expressions in terms of $\\cos \\theta$ and $\\sin \\theta$. Provide exact symbolic expressions; no rounding is required. Your final answer must be a single composite expression listing the three norms in the order $1$-, $2$-, $\\infty$-norms.", "solution": "The problem requires the derivation of the induced $1$-, $2$-, and $\\infty$-norms of a Givens rotation matrix $G \\in \\mathbb{R}^{n \\times n}$, using only the definition of an induced matrix norm. The matrix $G$ operates on coordinates $i$ and $j$, where $1 \\leq i  j \\leq n$. Its non-trivial entries form a $2 \\times 2$ submatrix on rows and columns $i$ and $j$:\n$$\n\\begin{pmatrix} G_{ii}  G_{ij} \\\\ G_{ji}  G_{jj} \\end{pmatrix} = \\begin{pmatrix} \\cos \\theta  \\sin \\theta \\\\ -\\sin \\theta  \\cos \\theta \\end{pmatrix}\n$$\nwhere $\\theta \\in \\mathbb{R}$ is the angle of rotation. Elsewhere, $G$ is the identity matrix, meaning $G_{kk} = 1$ for $k \\notin \\{i, j\\}$ and all other off-diagonal entries are $0$. For notational convenience, let $c = \\cos \\theta$ and $s = \\sin \\theta$.\n\nThe induced $p$-norm of a matrix $A$ is defined as\n$$\n\\|A\\|_{p} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{p}}{\\|x\\|_{p}} = \\sup_{\\|x\\|_{p}=1} \\|A x\\|_{p}\n$$\nWe will apply this definition for $p=1$, $p=2$, and $p=\\infty$.\n\nLet $x \\in \\mathbb{R}^n$ be an arbitrary vector, and let $y = Gx$. The components of $y$ are given by:\n$y_k = x_k$ for $k \\notin \\{i, j\\}$\n$y_i = c x_i + s x_j$\n$y_j = -s x_i + c x_j$\n\n**Derivation of the Induced $1$-Norm, $\\|G\\|_{1}$**\n\nThe induced $1$-norm is defined as $\\|G\\|_{1} = \\sup_{\\|x\\|_{1}=1} \\|Gx\\|_{1}$.\nLet $x \\in \\mathbb{R}^n$ such that $\\|x\\|_{1} = \\sum_{k=1}^n |x_k| = 1$. The $1$-norm of $y=Gx$ is:\n$$\n\\|Gx\\|_{1} = \\sum_{k=1}^n |y_k| = \\sum_{k \\notin \\{i,j\\}} |y_k| + |y_i| + |y_j| = \\sum_{k \\notin \\{i,j\\}} |x_k| + |c x_i + s x_j| + |-s x_i + c x_j|\n$$\nUsing the triangle inequality on the last two terms:\n$|c x_i + s x_j| \\leq |c||x_i| + |s||x_j|$\n$|-s x_i + c x_j| \\leq |-s||x_i| + |c||x_j| = |s||x_i| + |c||x_j|$\nSumming these inequalities, we get:\n$|c x_i + s x_j| + |-s x_i + c x_j| \\leq (|c|+|s|)|x_i| + (|s|+|c|)|x_j| = (|c|+|s|)(|x_i|+|x_j|)$.\nSubstituting this into the expression for $\\|Gx\\|_{1}$:\n$$\n\\|Gx\\|_{1} \\leq \\sum_{k \\notin \\{i,j\\}} |x_k| + (|c|+|s|)(|x_i|+|x_j|)\n$$\nLet $M = |c|+|s| = |\\cos\\theta| + |\\sin\\theta|$. Note that $M \\geq 1$, since $M^2 = (|c|+|s|)^2 = |c|^2+|s|^2+2|c||s| = c^2+s^2+2|cs| = 1+2|\\cos\\theta \\sin\\theta| \\geq 1$.\nLet $\\alpha = |x_i| + |x_j|$. Since $\\|x\\|_1=1$, we have $\\sum_{k \\notin \\{i,j\\}} |x_k| = 1-\\alpha$, where $0 \\leq \\alpha \\leq 1$. The inequality becomes:\n$$\n\\|Gx\\|_{1} \\leq (1-\\alpha) + M\\alpha = 1 + (M-1)\\alpha\n$$\nSince $M \\geq 1$, the expression $1 + (M-1)\\alpha$ is maximized when $\\alpha$ is maximized. The maximum value of $\\alpha$ is $1$, which occurs when $x_k = 0$ for all $k \\notin \\{i,j\\}$, i.e., when the support of $x$ is restricted to the indices $\\{i, j\\}$. For such vectors, the inequality implies $\\|Gx\\|_{1} \\leq M$.\nThis establishes an upper bound: $\\|G\\|_{1} \\leq |\\cos\\theta| + |\\sin\\theta|$.\n\nTo show that this supremum is attained, we must find a vector $x$ with $\\|x\\|_{1}=1$ for which $\\|Gx\\|_{1} = |\\cos\\theta| + |\\sin\\theta|$. Consider the standard basis vector $x = e_i$. Then $\\|e_i\\|_{1}=1$. The vector $Ge_i$ corresponds to the $i$-th column of $G$, which is $c e_i - s e_j$.\nThe $1$-norm of this vector is:\n$$\n\\|Ge_i\\|_{1} = \\|c e_i - s e_j\\|_{1} = |c| + |-s| = |\\cos\\theta| + |\\sin\\theta|\n$$\nSince we have found a vector $x$ for which $\\|Gx\\|_{1}$ equals the upper bound, the supremum must be this value.\n$$\n\\|G\\|_{1} = |\\cos\\theta| + |\\sin\\theta|\n$$\n\n**Derivation of the Induced $\\infty$-Norm, $\\|G\\|_{\\infty}$**\n\nThe induced $\\infty$-norm is defined as $\\|G\\|_{\\infty} = \\sup_{\\|x\\|_{\\infty}=1} \\|Gx\\|_{\\infty}$.\nLet $x \\in \\mathbb{R}^n$ such that $\\|x\\|_{\\infty} = \\max_k |x_k| = 1$. The $\\infty$-norm of $y=Gx$ is:\n$$\n\\|Gx\\|_{\\infty} = \\max_{k} |y_k| = \\max\\left(\\max_{k \\notin \\{i,j\\}} |x_k|, |c x_i + s x_j|, |-s x_i + c x_j|\\right)\n$$\nSince $\\|x\\|_{\\infty}=1$, we know that $|x_k| \\leq 1$ for all $k$. Thus, $\\max_{k \\notin \\{i,j\\}} |x_k| \\leq 1$.\nUsing the triangle inequality and the fact that $|x_i| \\leq 1$ and $|x_j| \\leq 1$:\n$|c x_i + s x_j| \\leq |c||x_i| + |s||x_j| \\leq |c|(1) + |s|(1) = |\\cos\\theta| + |\\sin\\theta|$.\n$|-s x_i + c x_j| \\leq |s||x_i| + |c||x_j| \\leq |s|(1) + |c|(1) = |\\cos\\theta| + |\\sin\\theta|$.\nThus, we have an upper bound for $\\|Gx\\|_{\\infty}$:\n$$\n\\|Gx\\|_{\\infty} \\leq \\max(1, |\\cos\\theta| + |\\sin\\theta|)\n$$\nAs established earlier, $|\\cos\\theta| + |\\sin\\theta| \\geq 1$. Therefore, the upper bound is $\\|G\\|_{\\infty} \\leq |\\cos\\theta| + |\\sin\\theta|$.\n\nTo show this bound is attained, we construct a vector $x$ with $\\|x\\|_{\\infty}=1$. Let $x_k = 0$ for $k \\notin \\{i,j\\}$. We choose $x_i$ and $x_j$ to maximize $|y_i| = |c x_i + s x_j|$. Let $x_i = \\mathrm{sgn}(c)$ and $x_j = \\mathrm{sgn}(s)$, where $\\mathrm{sgn}$ is the sign function. Since $c^2+s^2=1$, at least one of them is non-zero, ensuring $\\|x\\|_\\infty = \\max(|\\mathrm{sgn}(c)|, |\\mathrm{sgn}(s)|, 0) = 1$.\nFor this choice of $x$, the $i$-th component of $y=Gx$ is:\n$$\ny_i = c x_i + s x_j = c(\\mathrm{sgn}(c)) + s(\\mathrm{sgn}(s)) = |c| + |s| = |\\cos\\theta| + |\\sin\\theta|\n$$\nThe $\\infty$-norm of the resulting vector $y$ is $\\|y\\|_{\\infty} = \\max_k |y_k|$. Since $|y_i|$ is one of the components, $\\|y\\|_{\\infty} \\geq |y_i| = |\\cos\\theta| + |\\sin\\theta|$.\nCombining this with the upper bound, we must have equality.\n$$\n\\|G\\|_{\\infty} = |\\cos\\theta| + |\\sin\\theta|\n$$\n\n**Derivation of the Induced $2$-Norm, $\\|G\\|_{2}$**\n\nThe induced $2$-norm is defined as $\\|G\\|_{2} = \\sup_{\\|x\\|_{2}=1} \\|Gx\\|_{2}$.\nLet $x \\in \\mathbb{R}^n$. We compute the squared Euclidean norm of $y=Gx$:\n$$\n\\|Gx\\|_{2}^2 = \\|y\\|_{2}^2 = \\sum_{k=1}^n y_k^2 = \\sum_{k \\notin \\{i,j\\}} y_k^2 + y_i^2 + y_j^2\n$$\nSubstituting the expressions for the components of $y$:\n$$\n\\|Gx\\|_{2}^2 = \\sum_{k \\notin \\{i,j\\}} x_k^2 + (c x_i + s x_j)^2 + (-s x_i + c x_j)^2\n$$\nExpanding the squared terms:\n$$\n(c x_i + s x_j)^2 = c^2 x_i^2 + 2cs x_i x_j + s^2 x_j^2\n$$\n$$\n(-s x_i + c x_j)^2 = s^2 x_i^2 - 2cs x_i x_j + c^2 x_j^2\n$$\nThe sum of these two terms is:\n$$\n(c^2 x_i^2 + 2cs x_i x_j + s^2 x_j^2) + (s^2 x_i^2 - 2cs x_i x_j + c^2 x_j^2) = (c^2+s^2)x_i^2 + (s^2+c^2)x_j^2\n$$\nSince $c^2+s^2 = \\cos^2\\theta + \\sin^2\\theta = 1$, this simplifies to $x_i^2+x_j^2$.\nSubstituting this back into the expression for $\\|Gx\\|_{2}^2$:\n$$\n\\|Gx\\|_{2}^2 = \\sum_{k \\notin \\{i,j\\}} x_k^2 + (x_i^2 + x_j^2) = \\sum_{k=1}^n x_k^2 = \\|x\\|_{2}^2\n$$\nThis shows that $\\|Gx\\|_{2} = \\|x\\|_{2}$ for any vector $x \\in \\mathbb{R}^n$. The Givens rotation matrix $G$ is an isometry with respect to the $2$-norm; it preserves the length of vectors.\nUsing this property in the definition of the induced $2$-norm:\n$$\n\\|G\\|_{2} = \\sup_{x \\neq 0} \\frac{\\|Gx\\|_{2}}{\\|x\\|_{2}} = \\sup_{x \\neq 0} \\frac{\\|x\\|_{2}}{\\|x\\|_{2}} = \\sup_{x \\neq 0} 1 = 1\n$$\nThus, the induced $2$-norm is $1$. This might be written as $\\cos^2\\theta+\\sin^2\\theta$, but $1$ is the canonical simplified expression.\n\nIn summary, the induced norms are:\n$\\|G\\|_{1} = |\\cos\\theta| + |\\sin\\theta|$\n$\\|G\\|_{2} = 1$\n$\\|G\\|_{\\infty} = |\\cos\\theta| + |\\sin\\theta|$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n|\\cos\\theta| + |\\sin\\theta|  1  |\\cos\\theta| + |\\sin\\theta|\n\\end{pmatrix}\n}\n$$", "id": "3242259"}, {"introduction": "Induced norms are not just abstract concepts; they are essential for analyzing real-world systems, such as the one that powers web search. This practice explores the Google matrix, the mathematical foundation of the PageRank algorithm, which is a convex combination of a stochastic matrix $S$ and a uniform matrix $E$. Calculating the $1$- and $\\infty$-norms of this matrix demonstrates how its specific structure provides insight into the behavior and stability of the ranking process [@problem_id:3242242].", "problem": "Let $n \\in \\mathbb{N}$ and let $S \\in \\mathbb{R}^{n \\times n}$ be a column-stochastic matrix, meaning $S$ has nonnegative entries and each of its columns sums to $1$. Denote $S = [s_{ij}]_{i,j=1}^{n}$. Let $E \\in \\mathbb{R}^{n \\times n}$ be the matrix whose entries are all $1$. For a fixed parameter $\\alpha \\in (0,1)$, define the Google matrix\n$$\nG \\;=\\; \\alpha S \\;+\\; (1-\\alpha)\\,\\frac{1}{n}\\,E.\n$$\nUsing only the definitions of the operator norms induced by the vector $1$-norm and the vector $\\infty$-norm, namely\n$$\n\\|A\\|_{1} \\;=\\; \\sup_{x \\neq 0} \\frac{\\|Ax\\|_{1}}{\\|x\\|_{1}}, \n\\qquad\n\\|A\\|_{\\infty} \\;=\\; \\sup_{x \\neq 0} \\frac{\\|Ax\\|_{\\infty}}{\\|x\\|_{\\infty}},\n$$\ndetermine the exact values of $\\|G\\|_{1}$ and $\\|G\\|_{\\infty}$ in closed form in terms of $\\alpha$, $n$, and the entries of $S$. Express your final answer as a single row matrix $\\bigl(\\|G\\|_{1},\\,\\|G\\|_{\\infty}\\bigr)$. No rounding is required, and no units are involved.", "solution": "The problem is to determine the operator norms $\\|G\\|_{1}$ and $\\|G\\|_{\\infty}$ for the Google matrix $G$, defined as $G = \\alpha S + (1-\\alpha)\\frac{1}{n}E$, where $S$ is a column-stochastic matrix, $E$ is the matrix of all ones, $\\alpha \\in (0,1)$, and $n \\in \\mathbb{N}$. We are required to use the fundamental definitions of these norms.\n\nFirst, let us establish the properties of the matrix $G$. Let $S = [s_{ij}]$ and $G = [g_{ij}]$. The entries of $E$ are all $1$. The entries of $G$ are given by\n$$\ng_{ij} \\;=\\; \\alpha s_{ij} + (1-\\alpha)\\frac{1}{n}(1) \\;=\\; \\alpha s_{ij} + \\frac{1-\\alpha}{n}.\n$$\nSince $S$ is column-stochastic, its entries are non-negative, $s_{ij} \\ge 0$. The parameter $\\alpha$ is in the interval $(0,1)$, which implies $1-\\alpha  0$. Therefore, all entries of $G$ are non-negative:\n$$\ng_{ij} \\;=\\; \\alpha s_{ij} + \\frac{1-\\alpha}{n} \\;\\ge\\; 0 \\quad \\text{for all } i,j \\in \\{1, 2, \\dots, n\\}.\n$$\nThis property, $g_{ij} \\ge 0$, implies that $|g_{ij}| = g_{ij}$ for all entries, which will simplify our calculations.\n\nNext, we examine the column sums of $G$. For any column $j \\in \\{1, 2, \\dots, n\\}$, the sum of its entries is\n$$\n\\sum_{i=1}^{n} g_{ij} \\;=\\; \\sum_{i=1}^{n} \\left(\\alpha s_{ij} + \\frac{1-\\alpha}{n}\\right) \\;=\\; \\alpha \\left(\\sum_{i=1}^{n} s_{ij}\\right) + \\sum_{i=1}^{n} \\frac{1-\\alpha}{n}.\n$$\nBy definition of a column-stochastic matrix $S$, we have $\\sum_{i=1}^{n} s_{ij} = 1$ for every column $j$. Thus,\n$$\n\\sum_{i=1}^{n} g_{ij} \\;=\\; \\alpha(1) + n\\left(\\frac{1-\\alpha}{n}\\right) \\;=\\; \\alpha + (1-\\alpha) \\;=\\; 1.\n$$\nThis shows that $G$ is also a column-stochastic matrix.\n\nWe are now prepared to calculate the norms.\n\nDetermination of $\\|G\\|_{1}$:\nThe operator $1$-norm is defined as $\\|G\\|_{1} = \\sup_{x \\neq 0} \\frac{\\|Gx\\|_{1}}{\\|x\\|_{1}}$, which is equivalent to $\\sup_{\\|x\\|_{1}=1} \\|Gx\\|_{1}$.\nLet $x \\in \\mathbb{R}^n$ be an arbitrary vector. The $1$-norm of the vector $y = Gx$ is given by $\\|Gx\\|_{1} = \\sum_{i=1}^{n} |(Gx)_i| = \\sum_{i=1}^{n} \\left| \\sum_{j=1}^{n} g_{ij} x_j \\right|$.\nUsing the triangle inequality, we have\n$$\n\\|Gx\\|_{1} \\;=\\; \\sum_{i=1}^{n} \\left| \\sum_{j=1}^{n} g_{ij} x_j \\right| \\;\\le\\; \\sum_{i=1}^{n} \\sum_{j=1}^{n} |g_{ij}| |x_j|.\n$$\nSince we established that $g_{ij} \\ge 0$, we have $|g_{ij}| = g_{ij}$. So,\n$$\n\\|Gx\\|_{1} \\;\\le\\; \\sum_{i=1}^{n} \\sum_{j=1}^{n} g_{ij} |x_j|.\n$$\nBy changing the order of summation, we get\n$$\n\\|Gx\\|_{1} \\;\\le\\; \\sum_{j=1}^{n} \\left( \\sum_{i=1}^{n} g_{ij} \\right) |x_j|.\n$$\nAs previously shown, the column sums of $G$ are all equal to $1$, i.e., $\\sum_{i=1}^{n} g_{ij} = 1$ for all $j$. Substituting this into the inequality gives\n$$\n\\|Gx\\|_{1} \\;\\le\\; \\sum_{j=1}^{n} (1) |x_j| \\;=\\; \\sum_{j=1}^{n} |x_j| \\;=\\; \\|x\\|_{1}.\n$$\nThis inequality, $\\|Gx\\|_{1} \\le \\|x\\|_{1}$, holds for all vectors $x \\in \\mathbb{R}^n$. It implies that $\\frac{\\|Gx\\|_{1}}{\\|x\\|_{1}} \\le 1$ for any $x \\neq 0$, and therefore $\\|G\\|_{1} \\le 1$.\n\nTo show that the supremum is exactly $1$, we must find a non-zero vector $x$ for which the equality $\\|Gx\\|_{1} = \\|x\\|_{1}$ is attained. Let us consider any standard basis vector $e_k$ for $k \\in \\{1, 2, \\dots, n\\}$. The vector $e_k$ has a $1$ in the $k$-th position and $0$s elsewhere.\nIts $1$-norm is $\\|e_k\\|_{1} = 1$.\nThe product $Ge_k$ is the $k$-th column of the matrix $G$. The entries of this column vector are $g_{ik}$ for $i=1, \\dots, n$.\nThe $1$-norm of $Ge_k$ is\n$$\n\\|Ge_k\\|_{1} \\;=\\; \\sum_{i=1}^{n} |g_{ik}|.\n$$\nSince $g_{ik} \\ge 0$, this simplifies to\n$$\n\\|Ge_k\\|_{1} \\;=\\; \\sum_{i=1}^{n} g_{ik}.\n$$\nWe have already shown that every column of $G$ sums to $1$. Therefore, $\\|Ge_k\\|_{1} = 1$.\nFor this specific choice of vector $x=e_k$, we have $\\frac{\\|Ge_k\\|_{1}}{\\|e_k\\|_{1}} = \\frac{1}{1} = 1$.\nSince we have shown that $\\frac{\\|Gx\\|_{1}}{\\|x\\|_{1}} \\le 1$ for all $x \\neq 0$ and we have found a vector for which the ratio is exactly $1$, the supremum must be $1$.\nThus, $\\|G\\|_{1} = 1$.\n\nDetermination of $\\|G\\|_{\\infty}$:\nThe operator $\\infty$-norm is defined as $\\|G\\|_{\\infty} = \\sup_{x \\neq 0} \\frac{\\|Gx\\|_{\\infty}}{\\|x\\|_{\\infty}}$, which is equivalent to $\\sup_{\\|x\\|_{\\infty}=1} \\|Gx\\|_{\\infty}$.\nLet $x \\in \\mathbb{R}^n$ be an arbitrary vector with $\\|x\\|_{\\infty} = 1$, which means $\\max_{j} |x_j| = 1$.\nThe $\\infty$-norm of the vector $y = Gx$ is given by $\\|Gx\\|_{\\infty} = \\max_{1 \\le i \\le n} |(Gx)_i| = \\max_{1 \\le i \\le n} \\left| \\sum_{j=1}^{n} g_{ij} x_j \\right|$.\nFor any row $i$, using the triangle inequality:\n$$\n\\left| \\sum_{j=1}^{n} g_{ij} x_j \\right| \\;\\le\\; \\sum_{j=1}^{n} |g_{ij}| |x_j|.\n$$\nSince $g_{ij} \\ge 0$, we have $|g_{ij}| = g_{ij}$. Also, since $\\|x\\|_{\\infty} = 1$, we have $|x_j| \\le 1$ for all $j$. This leads to\n$$\n\\left| \\sum_{j=1}^{n} g_{ij} x_j \\right| \\;\\le\\; \\sum_{j=1}^{n} g_{ij} |x_j| \\;\\le\\; \\sum_{j=1}^{n} g_{ij} (1) \\;=\\; \\sum_{j=1}^{n} g_{ij}.\n$$\nThis inequality holds for each row $i$. Therefore, taking the maximum over all rows, we get\n$$\n\\|Gx\\|_{\\infty} \\;=\\; \\max_{1 \\le i \\le n} \\left| \\sum_{j=1}^{n} g_{ij} x_j \\right| \\;\\le\\; \\max_{1 \\le i \\le n} \\sum_{j=1}^{n} g_{ij}.\n$$\nThis demonstrates that $\\|G\\|_{\\infty} \\le \\max_{1 \\le i \\le n} \\sum_{j=1}^{n} g_{ij}$.\n\nTo show that this upper bound is attained, we must construct a specific vector $x$ with $\\|x\\|_{\\infty}=1$. Let's choose the vector $x$ whose entries are all $1$, i.e., $x = [1, 1, \\dots, 1]^T$. For this vector, $\\|x\\|_{\\infty} = \\max_j |1| = 1$.\nFor this choice of $x$, the $i$-th component of the vector $Gx$ is\n$$\n(Gx)_i \\;=\\; \\sum_{j=1}^{n} g_{ij} (1) \\;=\\; \\sum_{j=1}^{n} g_{ij}.\n$$\nThe $\\infty$-norm of $Gx$ is then\n$$\n\\|Gx\\|_{\\infty} \\;=\\; \\max_{1 \\le i \\le n} |(Gx)_i| \\;=\\; \\max_{1 \\le i \\le n} \\left| \\sum_{j=1}^{n} g_{ij} \\right|.\n$$\nSince all $g_{ij} \\ge 0$, the sums are non-negative, and the absolute value is redundant.\n$$\n\\|Gx\\|_{\\infty} \\;=\\; \\max_{1 \\le i \\le n} \\sum_{j=1}^{n} g_{ij}.\n$$\nFor this vector $x$, we have found that $\\|Gx\\|_{\\infty}$ is exactly equal to the upper bound we derived. This means the supremum is indeed this value.\nThus, $\\|G\\|_{\\infty} = \\max_{1 \\le i \\le n} \\sum_{j=1}^{n} g_{ij}$.\n\nFinally, we express this result in terms of the given parameters. The sum of the entries in the $i$-th row of $G$ is\n$$\n\\sum_{j=1}^{n} g_{ij} \\;=\\; \\sum_{j=1}^{n} \\left(\\alpha s_{ij} + \\frac{1-\\alpha}{n}\\right) \\;=\\; \\alpha \\left(\\sum_{j=1}^{n} s_{ij}\\right) + n\\left(\\frac{1-\\alpha}{n}\\right) \\;=\\; \\alpha \\left(\\sum_{j=1}^{n} s_{ij}\\right) + (1-\\alpha).\n$$\nSubstituting this into our expression for the norm gives\n$$\n\\|G\\|_{\\infty} \\;=\\; \\max_{1 \\le i \\le n} \\left( \\alpha \\sum_{j=1}^{n} s_{ij} + 1-\\alpha \\right).\n$$\nThe results are $\\|G\\|_{1} = 1$ and $\\|G\\|_{\\infty} = \\max_{1 \\le i \\le n} \\left( \\alpha \\sum_{j=1}^{n} s_{ij} + 1-\\alpha \\right)$.\nThe final answer is to be presented as a row matrix.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1  \\max_{1 \\le i \\le n} \\left( \\alpha \\sum_{j=1}^{n} s_{ij} + 1-\\alpha \\right) \\end{pmatrix}}\n$$", "id": "3242242"}, {"introduction": "In scientific computing, the choice of a mathematical tool can dramatically influence your conclusions. This exercise presents a cautionary tale from the field of numerical stability analysis, where different induced norms can lead to vastly different assessments of an algorithm's behavior. By comparing the $1$-norm and $2$-norm of a time-stepping matrix, you will quantify how one norm might suggest instability while another suggests stability, highlighting the critical importance of selecting the appropriate norm for the problem at hand [@problem_id:3242244].", "problem": "Consider the forward Euler method applied to a linear Ordinary Differential Equation (ODE) of the form $\\frac{d\\mathbf{y}}{dt} = A \\mathbf{y}$, where $A \\in \\mathbb{R}^{n \\times n}$. An analyst proposes a crude norm-based stability check that bounds the per-step amplification by the induced matrix norm of $I + hA$, where $h  0$ is the time step. To study how different induced norms can lead to different conclusions about stability, let $n$ be a power of $2$, and define the Sylvester-type Hadamard matrix $H \\in \\mathbb{R}^{n \\times n}$ with entries in $\\{+1,-1\\}$ satisfying $H H^{\\top} = n I$ and $H = H^{\\top}$. Let $A = \\frac{1}{\\sqrt{n}} H$. Two analysts estimate amplification using the induced matrix $1$-norm $\\|I + hA\\|_{1}$ and the induced matrix $2$-norm $\\|I + hA\\|_{2}$, respectively.\n\nUsing only the definitions of the induced matrix $p$-norm, the vector $p$-norms, and basic spectral facts for symmetric orthogonal matrices, derive the exact closed-form expression (in terms of $n$ and $h$) for the ratio\n$$\nR(n,h) = \\frac{\\|I + hA\\|_{1}}{\\|I + hA\\|_{2}}.\n$$\nProvide the final expression for $R(n,h)$ as your answer. No rounding is required.", "solution": "The problem requires the derivation of the ratio $R(n,h) = \\frac{\\|I + hA\\|_{1}}{\\|I + hA\\|_{2}}$, where $A = \\frac{1}{\\sqrt{n}} H$. The matrix $H \\in \\mathbb{R}^{n \\times n}$ is a symmetric Sylvester-type Hadamard matrix with entries in $\\{+1, -1\\}$ satisfying $H H^{\\top} = nI$, $n$ is a power of $2$, and $h0$.\n\nLet the matrix of interest be denoted by $M = I + hA = I + \\frac{h}{\\sqrt{n}} H$. Since the identity matrix $I$ and the Hadamard matrix $H$ are symmetric, the matrix $M$ is also symmetric.\n\nFirst, we compute the induced matrix $2$-norm, $\\|M\\|_{2}$. For a symmetric matrix, the induced $2$-norm is equal to its spectral radius, which is the maximum absolute value of its eigenvalues.\n$$\n\\|M\\|_{2} = \\rho(M) = \\max_i |\\lambda_i(M)|\n$$\nwhere $\\lambda_i(M)$ are the eigenvalues of $M$.\n\nThe eigenvalues of $M$ are related to the eigenvalues of $H$. Let $\\lambda$ be an eigenvalue of $H$ with a corresponding eigenvector $\\mathbf{v} \\neq \\mathbf{0}$, such that $H\\mathbf{v} = \\lambda\\mathbf{v}$.\nThe action of $M$ on $\\mathbf{v}$ is:\n$$\nM\\mathbf{v} = \\left(I + \\frac{h}{\\sqrt{n}} H\\right)\\mathbf{v} = I\\mathbf{v} + \\frac{h}{\\sqrt{n}} (H\\mathbf{v}) = \\mathbf{v} + \\frac{h}{\\sqrt{n}} (\\lambda\\mathbf{v}) = \\left(1 + \\frac{h\\lambda}{\\sqrt{n}}\\right)\\mathbf{v}\n$$\nThus, the eigenvalues of $M$ are of the form $1 + \\frac{h\\lambda}{\\sqrt{n}}$, where $\\lambda$ are the eigenvalues of $H$.\n\nTo find the eigenvalues of $H$, we use the given property $H H^{\\top} = nI$. Since $H$ is symmetric ($H=H^{\\top}$), this simplifies to $H^2 = nI$.\nLet $\\lambda$ be an eigenvalue of $H$. Then $H^2\\mathbf{v} = \\lambda^2\\mathbf{v}$. Also, $(nI)\\mathbf{v} = n\\mathbf{v}$.\nFrom $H^2 = nI$, we must have $\\lambda^2 = n$. This implies that the only possible eigenvalues of $H$ are $\\lambda = \\sqrt{n}$ and $\\lambda = -\\sqrt{n}$.\n\nThe matrix $A = \\frac{1}{\\sqrt{n}}H$ has eigenvalues $\\mu = \\frac{\\lambda}{\\sqrt{n}}$, which are therefore $\\mu=+1$ and $\\mu=-1$.\nConsequently, the eigenvalues of $M = I+hA$ are $1+h(+1) = 1+h$ and $1+h(-1) = 1-h$.\n\nThe spectral radius of $M$ is the maximum of the absolute values of these eigenvalues:\n$$\n\\rho(M) = \\max(|1+h|, |1-h|)\n$$\nSince $h0$, $1+h$ is always positive and greater than $1$. For any $h0$, the inequality $1+h  |1-h|$ holds:\n- If $0  h \\le 1$, then $1-h \\ge 0$, and we compare $1+h$ with $1-h$. $1+h  1-h$ because $2h0$.\n- If $h  1$, then $1-h  0$, and we compare $1+h$ with $-(1-h)=h-1$. $1+h  h-1$ because $20$.\nTherefore, for all $h0$, the maximum is $1+h$.\nSo, the induced matrix $2$-norm is:\n$$\n\\|I + hA\\|_{2} = 1+h\n$$\n\nNext, we compute the induced matrix $1$-norm, $\\|M\\|_{1}$. This norm is defined as the maximum absolute column sum:\n$$\n\\|M\\|_{1} = \\max_{1 \\le j \\le n} \\sum_{i=1}^{n} |M_{ij}|\n$$\nThe entries of $M = I + \\frac{h}{\\sqrt{n}} H$ are given by $M_{ij} = \\delta_{ij} + \\frac{h}{\\sqrt{n}} H_{ij}$, where $\\delta_{ij}$ is the Kronecker delta.\n\nFor an arbitrary column $j$, the sum of the absolute values of its entries is:\n$$\n\\sum_{i=1}^{n} |M_{ij}| = |M_{jj}| + \\sum_{i=1, i \\ne j}^{n} |M_{ij}|\n$$\nSubstituting the expressions for the entries:\n- The diagonal entry is $M_{jj} = \\delta_{jj} + \\frac{h}{\\sqrt{n}} H_{jj} = 1 + \\frac{h}{\\sqrt{n}} H_{jj}$.\n- The off-diagonal entries ($i \\ne j$) are $M_{ij} = \\delta_{ij} + \\frac{h}{\\sqrt{n}} H_{ij} = \\frac{h}{\\sqrt{n}} H_{ij}$.\n\nThe sum for column $j$ becomes:\n$$\n\\sum_{i=1}^{n} |M_{ij}| = \\left|1 + \\frac{h}{\\sqrt{n}} H_{jj}\\right| + \\sum_{i=1, i \\ne j}^{n} \\left|\\frac{h}{\\sqrt{n}} H_{ij}\\right|\n$$\nThe entries of $H$ are $H_{ij} \\in \\{+1,-1\\}$, so $|H_{ij}|=1$. The sum simplifies to:\n$$\n\\sum_{i=1}^{n} |M_{ij}| = \\left|1 + \\frac{h}{\\sqrt{n}} H_{jj}\\right| + (n-1) \\frac{h}{\\sqrt{n}}\n$$\nThis column sum depends on the value of the diagonal entry $H_{jj}$, which can be either $+1$ or $-1$. Sylvester-type Hadamard matrices of order $n=2^k$ for $k \\ge 1$ have both $+1$ and $-1$ on their diagonals. For $n=1=2^0$, $H=[1]$ and $H_{11}=1$. In any case, a column with $H_{jj}=1$ always exists. To find the maximum column sum, we must consider both possibilities for $H_{jj}$.\n\nCase 1: $H_{jj} = +1$. The column sum is:\n$$\nS_{+} = \\left|1 + \\frac{h}{\\sqrt{n}}\\right| + (n-1) \\frac{h}{\\sqrt{n}} = 1 + \\frac{h}{\\sqrt{n}} + (n-1) \\frac{h}{\\sqrt{n}} = 1 + n\\frac{h}{\\sqrt{n}} = 1 + h\\sqrt{n}\n$$\n(since $h0$, $1+\\frac{h}{\\sqrt{n}}  0$).\n\nCase 2: $H_{jj} = -1$. The column sum is:\n$$\nS_{-} = \\left|1 - \\frac{h}{\\sqrt{n}}\\right| + (n-1) \\frac{h}{\\sqrt{n}}\n$$\n\nTo find $\\|M\\|_1$, we need to find $\\max(S_{+}, S_{-})$. Let us compare them.\n$S_{+} - S_{-} = (1 + h\\sqrt{n}) - \\left(\\left|1 - \\frac{h}{\\sqrt{n}}\\right| + (n-1) \\frac{h}{\\sqrt{n}}\\right)$\n$S_{+} - S_{-} = \\left(1 + \\frac{h}{\\sqrt{n}}\\right) - \\left|1 - \\frac{h}{\\sqrt{n}}\\right|$\nLet $x = \\frac{h}{\\sqrt{n}}$. Since $h0$ and $n \\ge 1$, we have $x0$. We need to compare $1+x$ with $|1-x|$. As established earlier when computing the spectral radius, for any $x0$, the inequality $1+x  |1-x|$ holds.\nThis implies that $S_{+}  S_{-}$.\n\nThe maximum column sum is therefore $S_{+}$. Thus, the induced matrix $1$-norm is:\n$$\n\\|I + hA\\|_{1} = 1 + h\\sqrt{n}\n$$\n\nFinally, we can compute the ratio $R(n,h)$:\n$$\nR(n,h) = \\frac{\\|I + hA\\|_{1}}{\\|I + hA\\|_{2}} = \\frac{1 + h\\sqrt{n}}{1+h}\n$$\nThis is the final closed-form expression for the ratio.", "answer": "$$\n\\boxed{\\frac{1 + h\\sqrt{n}}{1+h}}\n$$", "id": "3242244"}]}