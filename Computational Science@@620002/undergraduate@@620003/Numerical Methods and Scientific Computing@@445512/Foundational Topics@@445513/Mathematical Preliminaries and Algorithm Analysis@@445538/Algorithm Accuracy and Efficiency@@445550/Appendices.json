{"hands_on_practices": [{"introduction": "In numerical analysis, a key measure of an algorithm's quality is its order of accuracy, which tells us how fast the error decreases as we refine our computation, for example by shrinking a step size $h$. This first practice provides a direct way to see this principle in action by comparing a simple numerical differentiation formula with a more sophisticated one derived from Richardson extrapolation. By empirically measuring the convergence order, you will gain a concrete understanding of how algorithmic design directly impacts accuracy [@problem_id:3204708].", "problem": "Consider the task of numerically approximating the derivative of a smooth function $f$ at a point $x_0$. Two algorithms are to be compared in terms of algorithm accuracy and efficiency: a simple forward finite difference and a Richardson extrapolation constructed from forward finite differences. The comparison will be based on the empirical dependence of the absolute error on the step size $h$, expressed as a power law $E(h) \\approx C h^p$ for sufficiently small $h$, where $E(h)$ is the absolute error, $C$ is a constant depending on $f$ and $x_0$, and $p$ is the convergence order. The investigation is to be grounded in the definition of the derivative and the Taylor series expansion for smooth functions.\n\nStarting from the fundamental definition of the derivative $f'(x_0) = \\lim_{h \\to 0} \\frac{f(x_0+h) - f(x_0)}{h}$ and the Taylor series expansion of $f$ around $x_0$, derive the leading-order dependence of the truncation error on $h$ for:\n- The forward finite difference $D_{\\mathrm{fd}}(x_0,h) = \\frac{f(x_0+h) - f(x_0)}{h}$.\n- The Richardson extrapolation constructed from forward differences $D_{\\mathrm{rich}}(x_0,h) = 2 D_{\\mathrm{fd}}(x_0,\\frac{h}{2}) - D_{\\mathrm{fd}}(x_0,h)$.\n\nThen implement a program that, for a given set of smooth test functions with known exact derivatives, computes the empirical convergence order $p$ and a local error ratio $r$ defined by $r(h) = \\frac{E(h)}{E(h/2)}$ for each method. Use linear least squares regression on the data $(\\log h, \\log E(h))$ to estimate $p$ over a prescribed range of $h$ values where truncation error dominates roundoff error.\n\nAssume angles are measured in radians. No physical units are involved. All mathematical quantities must be handled in a unitless manner.\n\nThe test suite consists of the following function-point pairs and step schedules:\n- $f_1(x) = \\sin(x)$ at $x_0 = 1.3$ with $h_k = 2^{-k}$ for $k \\in \\{3,4,5,\\dots,20\\}$.\n- $f_2(x) = e^{x}$ at $x_0 = 1.0$ with $h_k = 2^{-k}$ for $k \\in \\{3,4,5,\\dots,20\\}$.\n- $f_3(x) = \\frac{1}{1+x^2}$ at $x_0 = 0.5$ with $h_k = 2^{-k}$ for $k \\in \\{3,4,5,\\dots,20\\}$.\n\nFor each test case:\n- Compute the absolute errors $E_{\\mathrm{fd}}(h_k) = \\left|D_{\\mathrm{fd}}(x_0,h_k) - f'(x_0)\\right|$ and $E_{\\mathrm{rich}}(h_k) = \\left|D_{\\mathrm{rich}}(x_0,h_k) - f'(x_0)\\right|$.\n- Estimate the empirical convergence orders $p_{\\mathrm{fd}}$ and $p_{\\mathrm{rich}}$ by linear least squares regression of $\\log E$ versus $\\log h$ using the subset $k \\in \\{3,4,5,6,7,8,9,10\\}$.\n- Compute the local error ratios $r_{\\mathrm{fd}} = \\frac{E_{\\mathrm{fd}}(h_8)}{E_{\\mathrm{fd}}(h_9)}$ and $r_{\\mathrm{rich}} = \\frac{E_{\\mathrm{rich}}(h_8)}{E_{\\mathrm{rich}}(h_9)}$, where $h_8 = 2^{-8}$ and $h_9 = 2^{-9}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets and with no spaces. Each test case result must be formatted as a list $[p_{\\mathrm{fd}},p_{\\mathrm{rich}},r_{\\mathrm{fd}},r_{\\mathrm{rich}}]$, in the order of the test cases listed above. The final output therefore has the form $[[p_{\\mathrm{fd},1},p_{\\mathrm{rich},1},r_{\\mathrm{fd},1},r_{\\mathrm{rich},1}],[p_{\\mathrm{fd},2},p_{\\mathrm{rich},2},r_{\\mathrm{fd},2},r_{\\mathrm{rich},2}],[p_{\\mathrm{fd},3},p_{\\mathrm{rich},3},r_{\\mathrm{fd},3},r_{\\mathrm{rich},3}]]$, where all entries are decimal floats.", "solution": "The user-provided problem is a well-defined exercise in numerical analysis, concerning the comparison of two algorithms for numerical differentiation: the forward finite difference and a Richardson extrapolation scheme. The problem is scientifically sound, self-contained, and all data and procedures are specified unambiguously. Therefore, the problem is valid, and I will proceed with a full solution.\n\nThe solution consists of two parts: first, a theoretical derivation of the truncation error for both methods, and second, an explanation of the numerical implementation designed to verify these theoretical findings.\n\n### Theoretical Analysis of Truncation Error\n\nThe core of the analysis rests on the Taylor series expansion of a smooth function $f(x)$ around a point $x_0$. For a small displacement $h$, the series is:\n$$\nf(x_0+h) = f(x_0) + hf'(x_0) + \\frac{h^2}{2!}f''(x_0) + \\frac{h^3}{3!}f'''(x_0) + \\dots + \\frac{h^n}{n!}f^{(n)}(x_0) + \\mathcal{O}(h^{n+1})\n$$\nwhere $f^{(n)}(x_0)$ is the $n$-th derivative of $f$ evaluated at $x_0$, and $\\mathcal{O}(h^{n+1})$ represents terms of order $h^{n+1}$ and higher.\n\n#### 1. Forward Finite Difference ($D_{\\mathrm{fd}}$)\n\nThe forward finite difference formula is defined as:\n$$\nD_{\\mathrm{fd}}(x_0,h) = \\frac{f(x_0+h) - f(x_0)}{h}\n$$\nTo analyze its accuracy, we substitute the Taylor series for $f(x_0+h)$:\n$$\nD_{\\mathrm{fd}}(x_0,h) = \\frac{1}{h} \\left[ \\left( f(x_0) + hf'(x_0) + \\frac{h^2}{2}f''(x_0) + \\mathcal{O}(h^3) \\right) - f(x_0) \\right]\n$$\nSimplifying the expression by canceling $f(x_0)$ and dividing by $h$:\n$$\nD_{\\mathrm{fd}}(x_0,h) = \\frac{1}{h} \\left[ hf'(x_0) + \\frac{h^2}{2}f''(x_0) + \\mathcal{O}(h^3) \\right] = f'(x_0) + \\frac{h}{2}f''(x_0) + \\mathcal{O}(h^2)\n$$\nThe truncation error, $E_{\\mathrm{trunc}}(h) = D_{\\mathrm{fd}}(x_0,h) - f'(x_0)$, is therefore:\n$$\nE_{\\mathrm{trunc}}(h) = \\frac{h}{2}f''(x_0) + \\mathcal{O}(h^2)\n$$\nThe leading-order term of the error is proportional to $h^1$. Thus, the forward finite difference method is first-order accurate, and its convergence order is $p=1$. The absolute error $E(h)$ for small $h$ is approximated by $E(h) \\approx |\\frac{f''(x_0)}{2}| h$.\n\n#### 2. Richardson Extrapolation ($D_{\\mathrm{rich}}$)\nRichardson extrapolation is a technique to improve the order of accuracy of a numerical method. The specific formula provided is constructed to cancel the leading-order error term of the forward finite difference method.\n$$\nD_{\\mathrm{rich}}(x_0,h) = 2 D_{\\mathrm{fd}}(x_0,\\frac{h}{2}) - D_{\\mathrm{fd}}(x_0,h)\n$$\nTo find its truncation error, we need the error expansion of $D_{\\mathrm{fd}}$ to a higher order. Let $A(h)$ be the approximation $D_{\\mathrm{fd}}(x_0, h)$. We have established that $A(h)$ has an error expansion of the form:\n$$\nA(h) = f'(x_0) + C_1 h + C_2 h^2 + C_3 h^3 + \\dots\n$$\nwhere $C_k = \\frac{f^{(k+1)}(x_0)}{(k+1)!}$. Specifically, $C_1 = \\frac{f''(x_0)}{2}$ and $C_2 = \\frac{f'''(x_0)}{6}$.\n\nThe approximation with step size $h/2$ is:\n$$\nA(\\frac{h}{2}) = f'(x_0) + C_1 \\left(\\frac{h}{2}\\right) + C_2 \\left(\\frac{h}{2}\\right)^2 + \\mathcal{O}(h^3) = f'(x_0) + \\frac{C_1}{2}h + \\frac{C_2}{4}h^2 + \\mathcal{O}(h^3)\n$$\nNow, substitute these into the formula for $D_{\\mathrm{rich}}(x_0,h)$:\n$$\nD_{\\mathrm{rich}}(x_0,h) = 2 \\left( f'(x_0) + \\frac{C_1}{2}h + \\frac{C_2}{4}h^2 + \\mathcal{O}(h^3) \\right) - \\left( f'(x_0) + C_1 h + C_2 h^2 + \\mathcal{O}(h^3) \\right)\n$$\nDistributing the terms yields:\n$$\nD_{\\mathrm{rich}}(x_0,h) = (2f'(x_0) - f'(x_0)) + (C_1 - C_1)h + \\left(\\frac{C_2}{2} - C_2\\right)h^2 + \\mathcal{O}(h^3)\n$$\n$$\nD_{\\mathrm{rich}}(x_0,h) = f'(x_0) - \\frac{C_2}{2}h^2 + \\mathcal{O}(h^3)\n$$\nSubstituting back the expression for $C_2$:\n$$\nD_{\\mathrm{rich}}(x_0,h) = f'(x_0) - \\frac{f'''(x_0)}{12}h^2 + \\mathcal{O}(h^3)\n$$\nThe truncation error for the Richardson extrapolation is:\n$$\nE_{\\mathrm{trunc}}(h) = D_{\\mathrm{rich}}(x_0,h) - f'(x_0) = - \\frac{f'''(x_0)}{12}h^2 + \\mathcal{O}(h^3)\n$$\nThe leading-order error term is proportional to $h^2$. Thus, this method is second-order accurate, and its convergence order is $p=2$. The absolute error $E(h)$ for small $h$ is approximated by $E(h) \\approx |\\frac{f'''(x_0)}{12}| h^2$.\n\n### Local Error Ratio and Numerical Verification\n\nThe theoretical relationship $E(h) \\approx C h^p$ provides a way to verify the convergence order empirically. The local error ratio $r(h)$ is defined as:\n$$\nr(h) = \\frac{E(h)}{E(h/2)} \\approx \\frac{C h^p}{C (h/2)^p} = \\frac{h^p}{h^p / 2^p} = 2^p\n$$\nBased on our derivations, we expect:\n- For the forward difference ($p=1$): $r(h) \\approx 2^1 = 2$.\n- For Richardson extrapolation ($p=2$): $r(h) \\approx 2^2 = 4$.\n\nThe empirical convergence order $p$ is estimated from the equation $\\log E(h) \\approx \\log C + p \\log h$. This shows a linear relationship between $\\log E$ and $\\log h$, where the slope is the convergence order $p$. Linear least squares regression on a set of $(\\log h, \\log E)$ data points provides an estimate for this slope.\n\nThe implementation will compute the absolute errors $E_{\\mathrm{fd}}(h_k)$ and $E_{\\mathrm{rich}}(h_k)$ for each function over the specified range of step sizes $h_k = 2^{-k}$. It will then perform the regression on the specified subset of data ($k \\in \\{3, \\dots, 10\\}$) to find $p_{\\mathrm{fd}}$ and $p_{\\mathrm{rich}}$, and compute the local ratios $r_{\\mathrm{fd}}$ and $r_{\\mathrm{rich}}$ at $h=h_8=2^{-8}$. The results will be collated and printed in the specified format. The numerical results are expected to closely match the theoretical predictions of $p=1$, $r \\approx 2$ for the forward difference and $p=2$, $r \\approx 4$ for Richardson extrapolation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes theoretical and empirical convergence properties for numerical differentiation.\n    \"\"\"\n\n    # Define the test functions and their analytical derivatives\n    def f1(x):\n        return np.sin(x)\n\n    def df1(x):\n        return np.cos(x)\n\n    def f2(x):\n        return np.exp(x)\n\n    def df2(x):\n        return np.exp(x)\n\n    def f3(x):\n        return 1.0 / (1.0 + x**2)\n\n    def df3(x):\n        return -2.0 * x / (1.0 + x**2)**2\n\n    # Define the numerical differentiation algorithms\n    def forward_diff(f, x0, h):\n        \"\"\"Computes the forward finite difference approximation of the derivative.\"\"\"\n        return (f(x0 + h) - f(x0)) / h\n\n    def richardson_extrap(f, x0, h):\n        \"\"\"Computes the Richardson extrapolation from forward differences.\"\"\"\n        # This formula is equivalent to the central difference approximation\n        # (f(x0+h/2) - f(x0-h/2))/h but derived from Richardson's method\n        # D_rich = 2 * D_fd(h/2) - D_fd(h)\n        d_h = forward_diff(f, x0, h)\n        d_h_half = forward_diff(f, x0, h / 2.0)\n        return 2.0 * d_h_half - d_h\n\n    # Define the test cases as specified in the problem statement\n    test_cases = [\n        {'f': f1, 'df': df1, 'x0': 1.3},\n        {'f': f2, 'df': df2, 'x0': 1.0},\n        {'f': f3, 'df': df3, 'x0': 0.5}\n    ]\n\n    all_results = []\n\n    # Define the range of step sizes\n    k_values = np.arange(3, 21)\n    h_values = 2.0**(-k_values)\n\n    # Define the slice of data to be used for regression (k=3 to 10)\n    k_min_reg, k_max_reg = 3, 10\n    reg_start_index = k_min_reg - k_values[0]\n    reg_end_index = k_max_reg - k_values[0] + 1\n    reg_slice = slice(reg_start_index, reg_end_index)\n\n    # Define indices for calculating the local error ratio (k=8, k=9)\n    k8_index = 8 - k_values[0]\n    k9_index = 9 - k_values[0]\n\n\n    for case in test_cases:\n        f = case['f']\n        df = case['df']\n        x0 = case['x0']\n\n        exact_derivative = df(x0)\n\n        # Compute numerical approximations for all h values\n        d_fd = np.array([forward_diff(f, x0, h) for h in h_values])\n        d_rich = np.array([richardson_extrap(f, x0, h) for h in h_values])\n        \n        # Compute absolute errors\n        e_fd = np.abs(d_fd - exact_derivative)\n        e_rich = np.abs(d_rich - exact_derivative)\n\n        # To prevent log(0) errors, replace any zero errors with a very small number.\n        # This is a safeguard; for the given functions, non-zero error is expected.\n        e_fd[e_fd == 0] = np.finfo(float).tiny\n        e_rich[e_rich == 0] = np.finfo(float).tiny\n        \n        # --- Empirical Convergence Order (p) ---\n        # Select the data for the regression analysis\n        h_reg = h_values[reg_slice]\n        e_fd_reg = e_fd[reg_slice]\n        e_rich_reg = e_rich[reg_slice]\n\n        # Prepare data for log-log regression\n        log_h = np.log(h_reg)\n        log_e_fd = np.log(e_fd_reg)\n        log_e_rich = np.log(e_rich_reg)\n\n        # Perform linear least squares regression (polyfit of degree 1)\n        # The slope of the line in the log-log plot is the order of convergence p.\n        p_fd = np.polyfit(log_h, log_e_fd, 1)[0]\n        p_rich = np.polyfit(log_h, log_e_rich, 1)[0]\n\n        # --- Local Error Ratio (r) ---\n        # Compute the ratio of errors for h_8 and h_9\n        r_fd = e_fd[k8_index] / e_fd[k9_index]\n        r_rich = e_rich[k8_index] / e_rich[k9_index]\n\n        all_results.append([p_fd, p_rich, r_fd, r_rich])\n\n    # Format the final output string as a list of lists of floats\n    inner_lists_str = []\n    for sublist in all_results:\n        inner_lists_str.append(f\"[{','.join(map(str, sublist))}]\")\n    \n    final_output_str = f\"[{','.join(inner_lists_str)}]\"\n    \n    print(final_output_str)\n\nsolve()\n```", "id": "3204708"}, {"introduction": "While improving accuracy is crucial, it's not the only concern. In simulating dynamic systems, such as the famous Lotka-Volterra predator-prey model, we must also ensure our numerical solution is stable and physically plausible. This exercise demonstrates how the pursuit of efficiency—by using a large time step $h$ in the simple Forward Euler method—can lead to numerical instability and nonsensical results like negative populations, highlighting the critical trade-off between speed and stability [@problem_id:3204844].", "problem": "You are to study the numerical accuracy and efficiency of the Forward Euler method when applied to the classical Lotka–Volterra predator–prey model. The continuous model consists of two coupled ordinary differential equations (ODEs), which represent the time evolution of a prey population $x(t)$ and a predator population $y(t)$:\n$$\n\\frac{dx}{dt} = \\alpha x - \\beta x y,\\quad\n\\frac{dy}{dt} = \\delta x y - \\gamma y,\n$$\nwith parameters $\\alpha > 0$, $\\beta > 0$, $\\gamma > 0$, and $\\delta > 0$, and initial conditions $x(0) = x_0 > 0$, $y(0) = y_0 > 0$. The exact continuous-time dynamics preserve non-negativity of the populations under these conditions. The Forward Euler method is a first-order explicit time-stepping scheme that approximates the solution by advancing in steps of size $h > 0$ from $t_n = n h$ to $t_{n+1} = (n+1) h$.\n\nYour tasks are:\n- Starting from the definition of the derivative as a limit and the notion of a time discretization at uniform step size $h$, derive the Forward Euler update equations for the above ODE system. Explain why, despite the true solution remaining non-negative, a large step size $h$ can lead to non-physical negative values in the numerical solution.\n- Implement a program that, given $(\\alpha,\\beta,\\gamma,\\delta)$, $(x_0,y_0)$, a step size $h$, and a final time $T$, applies the Forward Euler method for $N = T/h$ steps (assume $T/h$ is an integer for all test cases). In each simulation, detect whether either population becomes negative at any step, track the minimum values attained by $x_n$ and $y_n$, and return the final values $x_N$ and $y_N$ along with the number of steps $N$ as a proxy for computational effort.\n- Discuss how the choice of $h$ affects accuracy (via truncation error) and efficiency (via the number of steps), and relate this to the observed presence or absence of non-physical negative populations.\n\nUse the following test suite, each specified by $(\\alpha,\\beta,\\gamma,\\delta,x_0,y_0,h,T)$, with all quantities dimensionless:\n- Test $1$: $(\\alpha,\\beta,\\gamma,\\delta) = (\\,1.1,\\,0.4,\\,0.4,\\,0.1\\,)$, $(x_0,y_0) = (\\,10.0,\\,5.0\\,)$, $h = 0.01$, $T = 10.0$.\n- Test $2$: $(\\alpha,\\beta,\\gamma,\\delta) = (\\,1.1,\\,0.4,\\,0.4,\\,0.1\\,)$, $(x_0,y_0) = (\\,10.0,\\,5.0\\,)$, $h = 0.1$, $T = 10.0$.\n- Test $3$: $(\\alpha,\\beta,\\gamma,\\delta) = (\\,1.1,\\,0.4,\\,0.4,\\,0.1\\,)$, $(x_0,y_0) = (\\,10.0,\\,5.0\\,)$, $h = 1.2$, $T = 6.0$.\n- Test $4$: $(\\alpha,\\beta,\\gamma,\\delta) = (\\,1.0,\\,0.5,\\,1.0,\\,0.05\\,)$, $(x_0,y_0) = (\\,5.0,\\,20.0\\,)$, $h = 0.12$, $T = 1.2$.\n- Edge Test $5$: $(\\alpha,\\beta,\\gamma,\\delta) = (\\,1.0,\\,1.0,\\,1.0,\\,0.5\\,)$, $(x_0,y_0) = (\\,1.0,\\,2.0\\,)$, $h = 1.0$, $T = 3.0$.\n\nProgram output format:\n- For each test, produce a list `[neg, min x, min y, x_final, y_final, N]`, where `neg` is a boolean indicating whether any non-physical negative population occurred at any step (including the final step), `min x` and `min y` are the minimum values of $x_n$ and $y_n$ observed over the trajectory (including initial and final values), `x_final` and `y_final` are the final values after $N$ steps, and $N$ is the total number of steps taken.\n- Aggregate the results for all tests into a single line as a comma-separated list enclosed in square brackets. That is, your program should print a single line of the form\n`[result_1,result_2,result_3,result_4,result_5]`\nwhere each result is the list described above, with booleans, integers, and floating-point numbers in standard decimal notation.", "solution": "The problem as stated is scientifically sound, well-posed, and objective. It is grounded in the fundamental principles of mathematical biology (Lotka-Volterra model) and numerical analysis (Forward Euler method), which are well-established fields of study. The problem provides all necessary data and defines a clear, verifiable task. It is a canonical example used to study the properties of numerical methods, specifically the trade-off between accuracy, efficiency, and stability. Therefore, the problem is valid and I will proceed with a full solution.\n\nThe Lotka-Volterra predator-prey model is a system of two coupled, first-order, non-linear ordinary differential equations (ODEs). Let $x(t)$ be the population of the prey and $y(t)$ be the population of the predator at time $t$. The system is given by:\n$$\n\\frac{dx}{dt} = \\alpha x - \\beta x y = f(x, y)\n$$\n$$\n\\frac{dy}{dt} = \\delta x y - \\gamma y = g(x, y)\n$$\nThe parameters $\\alpha, \\beta, \\gamma, \\delta$ are positive real constants representing the interaction rates. The term $\\alpha x$ models the exponential growth of prey in the absence of predators. The term $-\\beta x y$ represents the rate of prey being consumed by predators. The term $\\delta x y$ represents the growth of the predator population due to consumption of prey. The term $-\\gamma y$ represents the natural death of predators. The initial conditions are given as $x(0) = x_0 > 0$ and $y(0) = y_0 > 0$. An important property of the exact solution is that if the populations start positive, they remain positive for all time $t > 0$.\n\n### Derivation of the Forward Euler Method\n\nThe Forward Euler method is a numerical procedure for approximating the solution of an initial value problem. It is derived from the definition of the derivative. For a function $x(t)$, the derivative is defined as:\n$$\n\\frac{dx}{dt} = \\lim_{h \\to 0} \\frac{x(t+h) - x(t)}{h}\n$$\nFor a small, finite time step $h > 0$, we can approximate the derivative by dropping the limit, which is a first-order forward difference approximation:\n$$\n\\frac{dx}{dt} \\approx \\frac{x(t+h) - x(t)}{h}\n$$\nWe discretize time into steps of size $h$, such that $t_n = n h$ for $n = 0, 1, 2, \\ldots$. Let $x_n$ and $y_n$ be the numerical approximations of the true solution $x(t_n)$ and $y(t_n)$, respectively. So, $x_n \\approx x(t_n)$ and $x_{n+1} \\approx x(t_{n+1}) = x(t_n + h)$.\n\nSubstituting this approximation into the ODE system at time $t_n$, we get:\n$$\n\\frac{x_{n+1} - x_n}{h} \\approx \\left.\\frac{dx}{dt}\\right|_{t=t_n} = \\alpha x(t_n) - \\beta x(t_n) y(t_n)\n$$\n$$\n\\frac{y_{n+1} - y_n}{h} \\approx \\left.\\frac{dy}{dt}\\right|_{t=t_n} = \\delta x(t_n) y(t_n) - \\gamma y(t_n)\n$$\nThe Forward Euler method evaluates the right-hand side (the derivatives) at the current time step $t_n$, using the known values $x_n$ and $y_n$. This yields the following explicit update rules:\n$$\nx_{n+1} = x_n + h \\cdot f(x_n, y_n) = x_n + h(\\alpha x_n - \\beta x_n y_n)\n$$\n$$\ny_{n+1} = y_n + h \\cdot g(x_n, y_n) = y_n + h(\\delta x_n y_n - \\gamma y_n)\n$$\nThese equations allow us to compute the state $(x_{n+1}, y_{n+1})$ at the next time step from the state $(x_n, y_n)$ at the current time step, starting from the initial condition $(x_0, y_0)$.\n\n### The Problem of Non-Physical Negative Populations\n\nAlthough the exact solution of the Lotka-Volterra equations remains non-negative, the numerical solution produced by the Forward Euler method can become negative. This is a manifestation of numerical instability. We can analyze the update equations to see how this can happen.\n\nConsider the update for the predator population $y$:\n$$\ny_{n+1} = y_n + h(\\delta x_n y_n - \\gamma y_n) = y_n (1 + h(\\delta x_n - \\gamma))\n$$\nIf we start with a positive population $y_n > 0$, the updated population $y_{n+1}$ will be negative if and only if the term in the parenthesis is negative:\n$$\n1 + h(\\delta x_n - \\gamma)  0 \\implies h(\\delta x_n - \\gamma)  -1 \\implies h(\\gamma - \\delta x_n) > 1\n$$\nThis inequality is more likely to be satisfied when the step size $h$ is large. It also depends on the state variable $x_n$. If the prey population $x_n$ is very small, such that $\\delta x_n$ is much smaller than $\\gamma$, the predator population's per-capita growth rate $(\\delta x_n - \\gamma)$ becomes a large negative number. A large step $h$ can cause the update to \"overshoot\" the value $y=0$, resulting in a non-physical negative population density.\n\nA similar analysis for the prey population $x$ shows:\n$$\nx_{n+1} = x_n + h(\\alpha x_n - \\beta x_n y_n) = x_n(1 + h(\\alpha - \\beta y_n))\n$$\nStarting with $x_n > 0$, $x_{n+1}$ can become negative if:\n$$\n1 + h(\\alpha - \\beta y_n)  0 \\implies h(\\beta y_n - \\alpha) > 1\n$$\nThis can occur if the step size $h$ is large and the predator population $y_n$ is significantly high, causing a large negative growth rate for the prey.\n\n### Accuracy, Efficiency, and Stability\n\nThe choice of the step size $h$ involves a critical trade-off between accuracy, efficiency, and stability.\n- **Accuracy**: The Forward Euler method is a first-order method, meaning its global truncation error (the difference between the numerical and true solutions after a fixed time $T$) is proportional to the step size, i.e., Error $\\propto O(h)$. A smaller $h$ reduces the error and yields a more accurate approximation of the true dynamics.\n- **Efficiency**: The computational effort is determined by the total number of steps $N$ required to reach the final time $T$. Since $N = T/h$, the cost is inversely proportional to $h$. A smaller $h$ requires more steps, increasing the computation time and making the simulation less efficient.\n- **Stability**: As demonstrated, a large step size $h$ can lead to numerical instability, producing physically meaningless results such as negative populations. The requirement for a stable and physically plausible solution imposes an upper bound on the step size $h$. This upper bound, known as the stability condition, often depends on the system parameters and the state variables themselves, making it complex for non-linear systems.\n\nIn summary, choosing $h$ requires a balance. It must be small enough to ensure the solution is stable (e.g., remains non-negative) and accurate to the desired degree, but large enough to keep the computational cost manageable. The provided test cases are designed to illustrate this trade-off: small $h$ values (Test 1) should be stable and accurate at a high computational cost ($N=1000$), while large $h$ values (Tests 3, 4, 5) risk instability and significant inaccuracy for the sake of high efficiency (low $N$).", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Applies the Forward Euler method to the Lotka-Volterra model for a suite of test cases\n    and analyzes the numerical behavior.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is: (alpha, beta, gamma, delta, x0, y0, h, T)\n    test_cases = [\n        (1.1, 0.4, 0.4, 0.1, 10.0, 5.0, 0.01, 10.0),\n        (1.1, 0.4, 0.4, 0.1, 10.0, 5.0, 0.1, 10.0),\n        (1.1, 0.4, 0.4, 0.1, 10.0, 5.0, 1.2, 6.0),\n        (1.0, 0.5, 1.0, 0.05, 5.0, 20.0, 0.12, 1.2),\n        (1.0, 1.0, 1.0, 0.5, 1.0, 2.0, 1.0, 3.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        alpha, beta, gamma, delta, x0, y0, h, T = case\n\n        # The problem statement guarantees T/h is an integer.\n        N = int(round(T / h))\n\n        # Initialize state variables\n        x = x0\n        y = y0\n\n        # Initialize tracking variables\n        min_x = x0\n        min_y = y0\n        neg_occurred = False\n\n        # Time-stepping loop\n        for _ in range(N):\n            # Calculate the next state using Forward Euler update rules\n            x_next = x + h * (alpha * x - beta * x * y)\n            y_next = y + h * (delta * x * y - gamma * y)\n            \n            # Update state\n            x = x_next\n            y = y_next\n\n            # Check for non-physical negative populations\n            if x  0.0 or y  0.0:\n                neg_occurred = True\n\n            # Track the minimum values encountered\n            if x  min_x:\n                min_x = x\n            if y  min_y:\n                min_y = y\n        \n        # In case the initial conditions were the minimums and a negative value\n        # occurred, we must ensure the minimum reflects the negative value.\n        # This is already handled by the loop logic correctly.\n\n        # Store the results for the current test case.\n        # Format: [neg_occurred, min_x, min_y, x_final, y_final, N]\n        result_for_case = [neg_occurred, min_x, min_y, x, y, N]\n        results.append(result_for_case)\n\n    # The final print statement must follow this exact format, with each sublist\n    # represented by its standard string form, joined by commas.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3204844"}, {"introduction": "Our final practice delves into a sophisticated trade-off common in large-scale scientific computing: solving systems of linear equations $A u = b$. Here, the fastest algorithm isn't always the one with the fewest iterations. This exercise compares two \"preconditioning\" strategies for the Conjugate Gradient method, forcing a decision between a cheap-but-simple method and a complex-but-powerful one that has a high initial setup cost. You will use a principled cost model to determine the most efficient overall strategy, balancing the one-time setup investment against the recurring cost of iterations [@problem_id:3204865].", "problem": "You are given the task of evaluating the trade-off between algorithmic accuracy and efficiency when solving the linear system $A u = b$ for symmetric positive definite (SPD) matrices $A$ using the Conjugate Gradient (CG) method with two different preconditioners: Jacobi and incomplete Cholesky with zero fill-in (IC$(0)$). Your goal is to implement a program that, for a set of specified test cases, computes the number of CG iterations required to achieve a target absolute residual norm and combines this with a simple yet principled operation-count model to estimate the total work. You must then decide which preconditioner minimizes the total cost for each test case.\n\nFundamental base to use:\n- Conjugate Gradient (CG) is appropriate for SPD systems and generates iterates $u_k$ in a Krylov subspace that reduce the residual $r_k = b - A u_k$.\n- Preconditioning transforms the system into $M^{-1} A u = M^{-1} b$, where $M$ is SPD and chosen so that the transformed system is better conditioned. The Jacobi preconditioner uses $M = \\operatorname{diag}(A)$, and incomplete Cholesky with zero fill-in (IC$(0)$) uses $M = L L^\\top$ with $L$ lower triangular constructed from the sparsity pattern of $A$ without introducing fill beyond that pattern.\n- Algorithmic efficiency is assessed here using floating-point operation (flop) counts that scale with structural quantities such as the number of nonzeros.\n\nImplementation requirements:\n1. Construct the following SPD test matrices $A$ and right-hand side vectors $b$:\n   - Case $\\#1$: One-dimensional Poisson tridiagonal matrix of size $n = 50$ with stencil entries $2$ on the diagonal and $-1$ on the first sub- and super-diagonals, with $b = \\mathbf{1}$ and target absolute residual tolerance $\\varepsilon = 10^{-8}$.\n   - Case $\\#2$: Two-dimensional Poisson five-point stencil on a $10 \\times 10$ grid (so $n = 100$) with $4$ on the diagonal and $-1$ for each interior neighbor, with $b = \\mathbf{1}$ and $\\varepsilon = 10^{-8}$.\n   - Case $\\#3$: Diagonal SPD matrix of size $n = 80$ with diagonal entries geometrically spaced from $10^{-2}$ to $10^{2}$, i.e., $A = \\operatorname{diag}\\left(10^{-2}, 10^{-2 + \\frac{4}{79}}, \\dots, 10^{2}\\right)$, with $b = \\mathbf{1}$ and $\\varepsilon = 10^{-10}$.\n   - Case $\\#4$: Scalar SPD matrix $A = [2]$ (so $n = 1$), with $b = [1]$ and $\\varepsilon = 10^{-12}$.\n   - Case $\\#5$: One-dimensional Poisson tridiagonal matrix of size $n = 200$ with the same stencil as in Case $\\#1$, with $b = \\mathbf{1}$ and $\\varepsilon = 10^{-6}$.\n   In all cases, use the absolute $2$-norm residual stopping rule $\\lVert r_k \\rVert_2 \\le \\varepsilon$ and initialize with $u_0 = 0$. The angle unit is not applicable.\n\n2. Implement Preconditioned Conjugate Gradient (PCG) with the following components:\n   - Jacobi preconditioner: $M = \\operatorname{diag}(A)$, so $M^{-1} r$ is computed by element-wise division by the diagonal of $A$.\n   - Incomplete Cholesky IC$(0)$ preconditioner: compute a lower-triangular matrix $L$ such that $M = L L^\\top \\approx A$ using the sparsity pattern of $A$ without fill-in beyond positions where $A_{ij} \\ne 0$ in the strict lower triangle. That is, $L_{ij}$ may be nonzero only if $i \\ge j$ and $A_{ij} \\ne 0$. Apply $M^{-1}$ by solving $L y = r$ (forward substitution) followed by $L^\\top z = y$ (backward substitution).\n   - Ensure numerical robustness by handling tiny or negative diagonal pivots in the IC$(0)$ factorization with a small positive regularization if needed.\n\n3. Define and use the following operation-count model to estimate total computational cost:\n   - Let $\\operatorname{nnz}(A)$ denote the number of nonzero entries in $A$ and $\\operatorname{nnz}(L)$ denote the number of nonzero entries in the lower-triangular factor $L$ (including its diagonal).\n   - Setup cost:\n     - Jacobi: $C_{\\text{setup}}^{\\text{Jac}} = n$.\n     - IC$(0)$: $C_{\\text{setup}}^{\\text{IC}} = \\sum_{i=1}^{n} d_i^2$, where $d_i$ is the count of structurally allowed nonzeros in row $i$ of $L$ (including the diagonal), i.e., the number of indices $j \\le i$ with $A_{ij} \\ne 0$.\n   - Per-iteration cost:\n     - Cost baseline per PCG iteration (common to both): $C_{\\text{base}} = 2 \\cdot \\operatorname{nnz}(A) + 12 n$ capturing one sparse matrix-vector product, inner products, and vector updates.\n     - Additional cost for preconditioner application:\n       - Jacobi: $C_{\\text{prec}}^{\\text{Jac}} = n$.\n       - IC$(0)$: $C_{\\text{prec}}^{\\text{IC}} = 2 \\cdot \\operatorname{nnz}(L)$ (forward plus backward triangular solves).\n     - Therefore, per-iteration costs are $C_{\\text{iter}}^{\\text{Jac}} = 2 \\cdot \\operatorname{nnz}(A) + 13 n$ and $C_{\\text{iter}}^{\\text{IC}} = 2 \\cdot \\operatorname{nnz}(A) + 12 n + 2 \\cdot \\operatorname{nnz}(L)$.\n   - Total cost to achieve the tolerance $\\varepsilon$ is $C_{\\text{total}} = C_{\\text{setup}} + k \\cdot C_{\\text{iter}}$, where $k$ is the number of iterations required by PCG to reach $\\lVert r_k \\rVert_2 \\le \\varepsilon$.\n\n4. For each test case, compute:\n   - $k_{\\text{Jac}}$: number of iterations with Jacobi preconditioning.\n   - $k_{\\text{IC}}$: number of iterations with IC$(0)$ preconditioning.\n   - $C_{\\text{total}}^{\\text{Jac}}$ and $C_{\\text{total}}^{\\text{IC}}$ using the model above.\n   - A winner code $w$ defined as $w = 0$ if Jacobi yields strictly smaller total cost, $w = 1$ if IC$(0)$ yields strictly smaller total cost, and $w = -1$ if the total costs are equal.\n\n5. Final output format:\n   - Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result must itself be a list of five integers in the form $[w, k_{\\text{Jac}}, k_{\\text{IC}}, C_{\\text{total}}^{\\text{Jac}}, C_{\\text{total}}^{\\text{IC}}]$.\n   - For example, the overall structure must be $[[w_1, k_{\\text{Jac},1}, k_{\\text{IC},1}, C_{\\text{total},1}^{\\text{Jac}}, C_{\\text{total},1}^{\\text{IC}}], \\dots, [w_5, k_{\\text{Jac},5}, k_{\\text{IC},5}, C_{\\text{total},5}^{\\text{Jac}}, C_{\\text{total},5}^{\\text{IC}}]]$ on a single line.\n\nTest suite to implement exactly:\n- Case $\\#1$: $n = 50$, one-dimensional Poisson, $\\varepsilon = 10^{-8}$.\n- Case $\\#2$: $10 \\times 10$ two-dimensional Poisson, $\\varepsilon = 10^{-8}$.\n- Case $\\#3$: $n = 80$, diagonal entries geometrically spaced from $10^{-2}$ to $10^{2}$, $\\varepsilon = 10^{-10}$.\n- Case $\\#4$: $n = 1$, $A = [2]$, $\\varepsilon = 10^{-12}$.\n- Case $\\#5$: $n = 200$, one-dimensional Poisson, $\\varepsilon = 10^{-6}$.\n\nAll numeric answers must be integers as specified, and no physical units or angle units are involved in this problem. The algorithmic residual norm must be the absolute $2$-norm and iterations must be counted as integers. The program must be fully self-contained and must not require any user input or external data. The output must respect the exact format specified above.", "solution": "The problem requires an evaluation of the trade-off between algorithmic accuracy and efficiency for two preconditioned Conjugate Gradient (PCG) methods applied to Symmetric Positive Definite (SPD) linear systems, $Au=b$. The two preconditioners are Jacobi and Incomplete Cholesky with zero fill-in (IC($0$)). The evaluation is based on a specified floating-point operation (flop) count model. The solution is structured into four principal components: matrix construction, preconditioner implementation, the PCG solver, and the cost analysis logic, each designed to meet the problem's specifications.\n\n### 1. System and Algorithm Implementation\n\n**Matrix Construction:**\nThe five test cases involve three types of SPD matrices, which are constructed as follows:\n- **1D Poisson Matrix:** For an $n \\times n$ matrix, this is a tridiagonal matrix with $2$ on the main diagonal and $-1$ on the first sub-diagonal and super-diagonal. This is constructed using `numpy.diag`.\n- **2D Poisson Matrix:** For an $m \\times m$ grid, this results in an $n \\times n$ matrix where $n=m^2$. The matrix has $4$ on the main diagonal and $-1$ on off-diagonals corresponding to grid neighbors. This structure is built using `numpy.diag` for the main diagonal (value $4$), the first sub/super-diagonals (value $-1$), and the $m$-th sub/super-diagonals (value $-1$). Connections corresponding to grid edges (e.g., between row $m-1$ and $m$) are then explicitly set to $0$ to enforce the correct 2D topology.\n- **Diagonal Matrix:** For Case #3, a diagonal matrix of size $n=80$ is created with its diagonal entries spaced geometrically from $10^{-2}$ to $10^{2}$. This is achieved using `numpy.geomspace` to generate the diagonal elements.\n- **Scalar Matrix:** Case #4 is a trivial $1 \\times 1$ matrix $A=[2]$.\n\nThe right-hand side vector $b$ is a vector of ones, $\\mathbf{1}$, in all cases except Case #4 where $b=[1]$. The initial guess for the solution is the zero vector, $u_0 = 0$, in all cases.\n\n**Preconditioned Conjugate Gradient (PCG) Solver:**\nA standard implementation of the PCG algorithm is employed. The algorithm iteratively refines the solution $u_k$ to minimize the $A$-norm of the error. The process starts with $u_0 = 0$, $r_0 = b$, and proceeds as follows for $k=0, 1, 2, \\ldots$:\n1. Solve $M z_k = r_k$ for $z_k$.\n2. Compute $\\rho_k = r_k^\\top z_k$.\n3. Update the search direction: $p_k = z_k$ if $k=0$, otherwise $p_k = z_k + (\\rho_k / \\rho_{k-1}) p_{k-1}$.\n4. Compute $q_k = A p_k$.\n5. Compute the step size: $\\alpha_k = \\rho_k / (p_k^\\top q_k)$.\n6. Update the solution: $u_{k+1} = u_k + \\alpha_k p_k$.\n7. Update the residual: $r_{k+1} = r_k - \\alpha_k q_k$.\nThe loop terminates when the absolute $2$-norm of the residual, $\\lVert r_{k+1} \\rVert_2$, falls below the specified tolerance $\\varepsilon$. The number of iterations, $k$, is returned.\n\n### 2. Preconditioner Implementation\n\nA preconditioner $M$ approximates $A$ and is used to transform the system into a better-conditioned one, $M^{-1} A u = M^{-1} b$, accelerating the convergence of CG. The action of $M^{-1}$ on a vector is required in each PCG iteration.\n\n**Jacobi Preconditioner:**\nThe Jacobi preconditioner is defined as $M = \\operatorname{diag}(A)$. Since $A$ is SPD, its diagonal entries are positive. The application of $M^{-1}$ to a residual vector $r$ is a simple and computationally inexpensive element-wise division: $(M^{-1}r)_i = r_i / A_{ii}$. Its setup cost is minimal, involving only the extraction of the diagonal.\n\n**Incomplete Cholesky (IC(0)) Preconditioner:**\nThe IC($0$) preconditioner constructs an approximation $M = L L^\\top \\approx A$, where $L$ is a lower-triangular matrix. The key constraint of IC($0$) is that the sparsity pattern of $L$ is restricted to that of the lower-triangular part of $A$. That is, $L_{ij}$ can be nonzero only if $A_{ij}$ is nonzero for $i \\ge j$.\n\nThe factorization is implemented by iterating through the matrix elements $(i, j)$ with $i \\ge j$ and computing $L_{ij}$ based on the standard Cholesky update formula, but with sums restricted to the allowed sparsity pattern:\n$$ L_{ij} = \\frac{1}{L_{jj}} \\left( A_{ij} - \\sum_{k=0}^{j-1, L_{ik}\\ne 0, L_{jk}\\ne 0} L_{ik} L_{jk} \\right) \\quad (i > j) $$\n$$ L_{ii} = \\sqrt{A_{ii} - \\sum_{k=0}^{i-1, L_{ik}\\ne 0} L_{ik}^2} $$\nTo ensure numerical stability as requested, if the argument of the square root for $L_{ii}$ is non-positive, it is replaced with a small positive value ($10^{-12}$).\nThe application of $M^{-1}$ involves solving $L y = r$ (forward substitution) followed by $L^\\top z = y$ (backward substitution). These triangular solves are implemented with loops that respect the sparse structure of $L$.\n\n### 3. Cost Analysis and Comparison\n\nThe decision of which preconditioner is more \"efficient\" is based on the total computational cost model provided. The total cost is the sum of a one-time setup cost and the cumulative cost of iterations.\n\n**Cost Model Calculation:**\nFor each test case and preconditioner, the following quantities are computed:\n- The number of iterations, $k$, from the PCG solver.\n- **Setup Cost ($C_{\\text{setup}}$):**\n  - Jacobi: $C_{\\text{setup}}^{\\text{Jac}} = n$.\n  - IC($0$): $C_{\\text{setup}}^{\\text{IC}} = \\sum_{i=1}^{n} d_i^2$, where $d_i$ is the number of non-zero elements in the lower triangle of row $i$ of $A$ (i.e., `np.count_nonzero(A[i, :i+1])`).\n- **Per-Iteration Cost ($C_{\\text{iter}}$):**\n  - Jacobi: $C_{\\text{iter}}^{\\text{Jac}} = 2 \\cdot \\operatorname{nnz}(A) + 13 n$.\n  - IC($0$): $C_{\\text{iter}}^{\\text{IC}} = 2 \\cdot \\operatorname{nnz}(A) + 12 n + 2 \\cdot \\operatorname{nnz}(L)$.\n  Here, $\\operatorname{nnz}(A)$ and $\\operatorname{nnz}(L)$ are the number of nonzero entries in matrices $A$ and the computed factor $L$, respectively.\n- **Total Cost ($C_{\\text{total}}$):**\n  - $C_{\\text{total}} = C_{\\text{setup}} + k \\cdot C_{\\text{iter}}$.\n\nAll cost calculations are performed using integer arithmetic, and final results are cast to integers as required.\n\n**Winner Determination:**\nFor each test case, the total costs $C_{\\text{total}}^{\\text{Jac}}$ and $C_{\\text{total}}^{\\text{IC}}$ are compared. A winner code $w$ is assigned: $w=0$ if Jacobi is strictly cheaper, $w=1$ if IC($0$) is strictly cheaper, and $w=-1$ if costs are identical. The final output for each case is a list containing $[w, k_{\\text{Jac}}, k_{\\text{IC}}, C_{\\text{total}}^{\\text{Jac}}, C_{\\text{total}}^{\\text{IC}}]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem, encapsulating all logic.\n    \"\"\"\n\n    def build_poisson_1d(n):\n        \"\"\"Builds a 1D Poisson matrix of size n x n.\"\"\"\n        A = np.diag(np.full(n, 2.0))\n        A += np.diag(np.full(n - 1, -1.0), k=1)\n        A += np.diag(np.full(n - 1, -1.0), k=-1)\n        return A\n\n    def build_poisson_2d(m):\n        \"\"\"Builds a 2D Poisson matrix for an m x m grid.\"\"\"\n        n = m * m\n        A = np.diag(np.full(n, 4.0))\n        off_diag1 = np.full(n - 1, -1.0)\n        off_diag_m = np.full(n - m, -1.0)\n        \n        A += np.diag(off_diag1, k=1)\n        A += np.diag(off_diag1, k=-1)\n        A += np.diag(off_diag_m, k=m)\n        A += np.diag(off_diag_m, k=-m)\n        \n        # Remove connections at grid boundaries\n        for i in range(1, m):\n            idx = i * m\n            A[idx - 1, idx] = 0\n            A[idx, idx - 1] = 0\n            \n        return A\n\n    def build_diag_geometric(n, v_start, v_end):\n        \"\"\"Builds a diagonal matrix with geometrically spaced entries.\"\"\"\n        return np.diag(np.geomspace(v_start, v_end, n))\n\n    class JacobiPreconditioner:\n        \"\"\"Jacobi Preconditioner: M = diag(A).\"\"\"\n        def __init__(self, A):\n            self.inv_diag = 1.0 / np.diag(A)\n        \n        def apply(self, r):\n            return self.inv_diag * r\n\n    class IC0Preconditioner:\n        \"\"\"Incomplete Cholesky (0) Preconditioner.\"\"\"\n        def __init__(self, A):\n            self.L = self._factor(A)\n            self.Lt = self.L.T\n\n        def _factor(self, A):\n            n = A.shape[0]\n            L = np.zeros_like(A, dtype=float)\n            \n            for i in range(n):\n                for j in range(i + 1):\n                    if A[i, j] != 0 or i == j:\n                        s = A[i, j]\n                        for k in range(j):\n                            if L[i,k] != 0 and L[j,k] != 0:\n                                s -= L[i, k] * L[j, k]\n                        \n                        if i == j:\n                            if s = 1e-12: s = 1e-12\n                            L[i, i] = np.sqrt(s)\n                        else:\n                            L[i, j] = s / L[j, j]\n            return L\n\n        def apply(self, r):\n            n = self.L.shape[0]\n            # Forward substitution: Solve Ly = r\n            y = np.zeros_like(r)\n            for i in range(n):\n                s = r[i]\n                for j in range(i):\n                    s -= self.L[i, j] * y[j]\n                y[i] = s / self.L[i, i]\n            \n            # Backward substitution: Solve L^T z = y\n            z = np.zeros_like(y)\n            for i in range(n - 1, -1, -1):\n                s = y[i]\n                for j in range(i + 1, n):\n                    s -= self.Lt[i, j] * z[j]\n                z[i] = s / self.Lt[i, i]\n            return z\n\n    def pcg(A, b, precon, tol, max_iter=5000):\n        \"\"\"Preconditioned Conjugate Gradient solver.\"\"\"\n        n = A.shape[0]\n        u = np.zeros(n)\n        r = b - A @ u\n        \n        r_norm = np.linalg.norm(r)\n        if r_norm  tol:\n            return 0\n        \n        z = precon.apply(r)\n        p = z\n        rho_old = np.dot(r, z)\n        \n        for k in range(1, max_iter + 1):\n            q = A @ p\n            alpha = rho_old / np.dot(p, q)\n            u += alpha * p\n            r -= alpha * q\n            \n            r_norm = np.linalg.norm(r)\n            if r_norm  tol:\n                return k\n            \n            z = precon.apply(r)\n            rho_new = np.dot(r, z)\n            beta = rho_new / rho_old\n            p = z + beta * p\n            rho_old = rho_new\n            \n        return max_iter\n\n    def calculate_total_cost(A, k, precon_name, L=None):\n        \"\"\"Calculates total cost based on the provided model.\"\"\"\n        n = A.shape[0]\n        nnz_A = np.count_nonzero(A)\n        \n        if precon_name == 'jacobi':\n            c_setup = n\n            c_iter = 2 * nnz_A + 13 * n\n        elif precon_name == 'ic0':\n            d_i_sq_sum = sum(np.count_nonzero(A[i, :i+1])**2 for i in range(n))\n            c_setup = d_i_sq_sum\n            nnz_L = np.count_nonzero(L)\n            c_iter = 2 * nnz_A + 12 * n + 2 * nnz_L\n        else:\n            raise ValueError(\"Unknown preconditioner name\")\n            \n        return c_setup + k * c_iter\n\n    test_cases = [\n        {'id': 1, 'type': 'poisson_1d', 'n': 50, 'tol': 1e-8},\n        {'id': 2, 'type': 'poisson_2d', 'm': 10, 'tol': 1e-8},\n        {'id': 3, 'type': 'diag_geom', 'n': 80, 'start': 1e-2, 'end': 1e2, 'tol': 1e-10},\n        {'id': 4, 'type': 'scalar', 'A': np.array([[2.0]]), 'b': np.array([1.0]), 'tol': 1e-12},\n        {'id': 5, 'type': 'poisson_1d', 'n': 200, 'tol': 1e-6},\n    ]\n\n    results = []\n    for case in test_cases:\n        if case['type'] == 'poisson_1d':\n            A = build_poisson_1d(case['n'])\n            b = np.ones(case['n'])\n        elif case['type'] == 'poisson_2d':\n            n = case['m']**2\n            A = build_poisson_2d(case['m'])\n            b = np.ones(n)\n        elif case['type'] == 'diag_geom':\n            A = build_diag_geometric(case['n'], case['start'], case['end'])\n            b = np.ones(case['n'])\n        elif case['type'] == 'scalar':\n            A = case['A']\n            b = case['b']\n        \n        tol = case['tol']\n        \n        # Jacobi\n        precon_jac = JacobiPreconditioner(A)\n        k_jac = pcg(A, b, precon_jac, tol)\n        cost_jac = calculate_total_cost(A, k_jac, 'jacobi')\n        \n        # IC(0)\n        precon_ic0 = IC0Preconditioner(A)\n        k_ic0 = pcg(A, b, precon_ic0, tol)\n        cost_ic0 = calculate_total_cost(A, k_ic0, 'ic0', L=precon_ic0.L)\n        \n        # Determine winner\n        if cost_jac  cost_ic0:\n            w = 0\n        elif cost_ic0  cost_jac:\n            w = 1\n        else:\n            w = -1\n        \n        results.append([w, k_jac, k_ic0, int(cost_jac), int(cost_ic0)])\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3204865"}]}