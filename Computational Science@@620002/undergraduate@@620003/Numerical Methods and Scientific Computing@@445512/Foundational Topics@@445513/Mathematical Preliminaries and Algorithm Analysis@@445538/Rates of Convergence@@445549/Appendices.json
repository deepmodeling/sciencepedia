{"hands_on_practices": [{"introduction": "Newton's method is celebrated for its rapid, quadratic convergence when finding simple roots. However, its performance changes dramatically when encountering a root with multiplicity greater than one. This exercise provides a crucial hands-on opportunity to observe this degradation firsthand, requiring you to implement the method and numerically analyze its output to confirm the theoretical prediction of linear convergence [@problem_id:3265302]. Mastering this concept is key to understanding the robustness and limitations of fundamental numerical algorithms.", "problem": "You are asked to construct and analyze a case where Newton’s method exhibits only linear convergence due to root multiplicity. Work from foundational definitions of convergence rates and Newton’s method. You must implement the algorithm and quantify the observed behavior using precise numerical metrics.\n\nDefinitions to use as the fundamental base:\n- A point $\\alpha \\in \\mathbb{R}$ is a root of multiplicity $m \\in \\mathbb{N}$ of a function $f:\\mathbb{R}\\to\\mathbb{R}$ if $f(\\alpha)=0$, $f'(\\alpha)=0$, $\\dots$, $f^{(m-1)}(\\alpha)=0$, and $f^{(m)}(\\alpha)\\neq 0$.\n- Newton’s method for approximating a simple root of a differentiable function $f$ is defined by the iteration $x_{k+1} = x_k - \\dfrac{f(x_k)}{f'(x_k)}$ for $k \\ge 0$.\n- Given a sequence $\\{x_k\\}$ converging to $\\alpha$, define the error $e_k = |x_k - \\alpha|$. The method is said to converge linearly if there exists a constant $c \\in (0,1)$ such that $\\displaystyle \\lim_{k\\to\\infty} \\dfrac{e_{k+1}}{e_k} = c$, and to converge with order $p>0$ if $\\displaystyle \\lim_{k\\to\\infty} \\dfrac{e_{k+1}}{e_k^p} = \\lambda$ for some $\\lambda \\in (0,\\infty)$. An empirical estimator of the order is\n$$\np_k = \\dfrac{\\ln\\!\\left(\\dfrac{e_{k+1}}{e_k}\\right)}{\\ln\\!\\left(\\dfrac{e_k}{e_{k-1}}\\right)} \\quad \\text{for } k \\ge 1,\n$$\nwhere $\\ln$ denotes the natural logarithm.\n\nTasks:\n1) Provide one explicit function $f$ with a root of multiplicity $3$. Justify multiplicity by the above definition.\n2) Implement Newton’s method for $f$ and compute the sequence of errors $e_k$ for several initial guesses. Use these errors to estimate:\n   - The asymptotic error ratio $r_k = \\dfrac{e_{k+1}}{e_k}$ as $k$ increases.\n   - The empirical order $p_k$ as defined above.\n3) Demonstrate numerically that Newton’s method converges only linearly for a triple root by showing that $r_k$ stabilizes near a constant strictly between $0$ and $1$, and that $p_k$ stabilizes near $1$.\n\nFor reproducibility and assessment, use the following test suite of parameter sets. In each case, you are given $f$, $f'$, the true root $\\alpha$, an initial guess $x_0$, and a fixed iteration count $N$:\n- Test A (exact factorization case): $f(x) = (x-1)^3$, $f'(x) = 3(x-1)^2$, $\\alpha = 1$, $x_0 = 2$, $N = 6$.\n- Test B (nontrivial factor case): $f(x) = (x-1)^3 (x^2+1)$, $f'(x) = 3(x-1)^2 (x^2+1) + (x-1)^3 (2x)$, $\\alpha = 1$, $x_0 = 1.7$, $N = 10$.\n- Test C (analytic nonpolynomial factor case): $f(x) = (x+0.5)^3 e^x$, $f'(x) = e^x\\left(3(x+0.5)^2 + (x+0.5)^3\\right)$, $\\alpha = -0.5$, $x_0 = 0$, $N = 12$.\n\nComputation and output requirements:\n- For each test case, run exactly $N$ iterations of Newton’s method starting from $x_0$, collecting the error sequence $e_k = |x_k - \\alpha|$ for $k=0,1,\\dots,N$. Compute the last ratio $r_{N-1} = \\dfrac{e_N}{e_{N-1}}$ and the last order estimate $p_{N-1} = \\dfrac{\\ln(e_N/e_{N-1})}{\\ln(e_{N-1}/e_{N-2})}$.\n- Your program should output a single line containing a comma-separated flat list with the values $[r_A, p_A, r_B, p_B, r_C, p_C]$, where $r_{\\cdot}$ and $p_{\\cdot}$ are the final ratio and final order estimate for each test case, respectively.\n- Output all numbers as floating-point values. For clarity and reproducibility, round each to $6$ decimal places before printing.\n- No physical units are involved, and no angles are used.\n\nYour program must be a complete, runnable program that performs all computations with the specified test suite and produces the required single-line output format: a single list on one line like $[r_A,p_A,r_B,p_B,r_C,p_C]$.", "solution": "The problem requires a theoretical and numerical analysis of Newton's method when applied to a function with a root of multiplicity greater than one, specifically for a multiplicity of $m=3$. We will first establish the theoretical basis for the expected linear convergence, then verify the multiplicity of a sample function, and finally design and execute a numerical experiment to quantify the convergence behavior for the provided test cases.\n\n**1. Theoretical Analysis of Convergence**\n\nNewton's method is an iterative scheme defined by $x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}$. This can be expressed as a fixed-point iteration $x_{k+1} = g(x_k)$, where the iteration function is $g(x) = x - \\frac{f(x)}{f'(x)}$. The convergence behavior near a root $\\alpha$ is determined by the derivative of $g(x)$ at $\\alpha$. If $|g'(\\alpha)| < 1$, the iteration converges locally to $\\alpha$. The rate of convergence is linear if $g'(\\alpha) \\neq 0$ and at least quadratic if $g'(\\alpha) = 0$.\n\nLet $\\alpha$ be a root of multiplicity $m \\in \\mathbb{N}$ with $m > 1$. By definition, the function $f(x)$ can be written in the form $f(x) = (x-\\alpha)^m h(x)$, where $h(x)$ is a function such that $h(\\alpha) \\neq 0$. Taking the derivative of $f(x)$ gives:\n$$\nf'(x) = m(x-\\alpha)^{m-1}h(x) + (x-\\alpha)^m h'(x)\n$$\nSubstituting these into the expression for $g(x)$:\n$$\ng(x) = x - \\frac{(x-\\alpha)^m h(x)}{m(x-\\alpha)^{m-1}h(x) + (x-\\alpha)^m h'(x)} = x - \\frac{(x-\\alpha)h(x)}{m h(x) + (x-\\alpha)h'(x)}\n$$\nTo find the convergence rate, we compute $g'(x)$:\n$$\ng'(x) = 1 - \\frac{[h(x) + (x-\\alpha)h'(x)][m h(x) + (x-\\alpha)h'(x)] - [(x-\\alpha)h(x)][m h'(x) + h'(x) + (x-\\alpha)h''(x)]}{[m h(x) + (x-\\alpha)h'(x)]^2}\n$$\nEvaluating the limit as $x \\to \\alpha$:\n$$\n\\lim_{x\\to\\alpha} g'(x) = g'(\\alpha) = 1 - \\frac{[h(\\alpha) + 0][m h(\\alpha) + 0] - [0]}{[m h(\\alpha) + 0]^2} = 1 - \\frac{m h(\\alpha)^2}{m^2 h(\\alpha)^2} = 1 - \\frac{1}{m}\n$$\nThus, for a root of multiplicity $m > 1$, Newton's method converges linearly with an asymptotic error ratio of $c = |g'(\\alpha)| = |1 - 1/m|$. Since $m > 1$, we have $0 < c < 1$.\n\nIn this problem, we are specifically interested in a root of multiplicity $m=3$. The theoretical asymptotic error ratio is therefore:\n$$\nc = 1 - \\frac{1}{3} = \\frac{2}{3}\n$$\nThe convergence order is $p=1$, indicating linear convergence.\n\n**2. Justification of Root Multiplicity**\n\nThe problem asks to provide and justify one function with a root of multiplicity $3$. We will use the function from Test A: $f(x) = (x-1)^3$. The root is $\\alpha=1$. According to the provided definition, we must verify that $f(\\alpha)=f'(\\alpha)=f''(\\alpha)=0$ and $f'''(\\alpha)\\neq 0$.\n\nThe function and its successive derivatives are:\n- $f(x) = (x-1)^3$\n- $f'(x) = 3(x-1)^2$\n- $f''(x) = 6(x-1)$\n- $f'''(x) = 6$\n\nEvaluating these at the root $\\alpha=1$:\n- $f(1) = (1-1)^3 = 0$\n- $f'(1) = 3(1-1)^2 = 0$\n- $f''(1) = 6(1-1) = 0$\n- $f'''(1) = 6 \\neq 0$\n\nSince the first two derivatives are zero at $\\alpha=1$ and the third derivative is non-zero, this confirms that $\\alpha=1$ is a root of $f(x)$ with multiplicity $m=3$. The other provided functions can be verified similarly, as they are constructed in the form $(x-\\alpha)^3 h(x)$ with $h(\\alpha) \\neq 0$.\n\n**3. Numerical Experiment and Expected Results**\n\nWe will implement Newton's method for each of the three test cases (A, B, C). For each case, we start with an initial guess $x_0$ and iterate $N$ times.\nThe process is as follows:\n1. Initialize $k=0$ with $x_k = x_0$. An array will store the errors $e_k = |x_k - \\alpha|$ for $k=0, 1, \\dots, N$.\n2. For $k$ from $0$ to $N-1$, compute the next iterate: $x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}$.\n3. Calculate and store the corresponding error $e_{k+1} = |x_{k+1} - \\alpha|$.\n4. After $N$ iterations, the error sequence $e_0, e_1, \\dots, e_N$ is available.\n5. Compute the final asymptotic error ratio estimate: $r_{N-1} = \\frac{e_N}{e_{N-1}}$.\n6. Compute the final empirical order of convergence: $p_{N-1} = \\frac{\\ln(e_N/e_{N-1})}{\\ln(e_{N-1}/e_{N-2})}$.\n\nBased on our theoretical analysis, the numerical results should show that for each test case, the value of $r_{N-1}$ approaches the theoretical limit of $2/3 \\approx 0.666667$, and the value of $p_{N-1}$ approaches $1$. This will numerically demonstrate that Newton's method exhibits linear convergence for a triple root, as required. The final implementation will perform these calculations and format the output as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and analyzes cases where Newton’s method exhibits linear convergence\n    due to a root of multiplicity 3. The implementation runs three test cases\n    and computes the final asymptotic error ratio and empirical order of convergence.\n    \"\"\"\n    \n    # Test A (exact factorization case)\n    # f(x) = (x-1)^3\n    # f'(x) = 3(x-1)^2\n    # alpha = 1, x_0 = 2, N = 6\n    case_A = {\n        \"f\": lambda x: (x - 1.0)**3,\n        \"fp\": lambda x: 3.0 * (x - 1.0)**2,\n        \"alpha\": 1.0,\n        \"x0\": 2.0,\n        \"N\": 6,\n    }\n\n    # Test B (nontrivial factor case)\n    # f(x) = (x-1)^3 * (x^2+1)\n    # f'(x) = 3(x-1)^2 * (x^2+1) + (x-1)^3 * (2x)\n    # alpha = 1, x_0 = 1.7, N = 10\n    case_B = {\n        \"f\": lambda x: (x - 1.0)**3 * (x**2 + 1.0),\n        \"fp\": lambda x: 3.0 * (x - 1.0)**2 * (x**2 + 1.0) + (x - 1.0)**3 * (2.0 * x),\n        \"alpha\": 1.0,\n        \"x0\": 1.7,\n        \"N\": 10,\n    }\n\n    # Test C (analytic nonpolynomial factor case)\n    # f(x) = (x+0.5)^3 * e^x\n    # f'(x) = e^x * (3(x+0.5)^2 + (x+0.5)^3)\n    # alpha = -0.5, x_0 = 0, N = 12\n    case_C = {\n        \"f\": lambda x: (x + 0.5)**3 * np.exp(x),\n        \"fp\": lambda x: np.exp(x) * (3.0 * (x + 0.5)**2 + (x + 0.5)**3),\n        \"alpha\": -0.5,\n        \"x0\": 0.0,\n        \"N\": 12,\n    }\n\n    test_cases = [case_A, case_B, case_C]\n    results = []\n\n    for case in test_cases:\n        f, fp = case[\"f\"], case[\"fp\"]\n        alpha, x0, N = case[\"alpha\"], case[\"x0\"], case[\"N\"]\n        \n        # Array to store the sequence of errors e_k = |x_k - alpha|\n        # Size is N+1 for e_0, e_1, ..., e_N\n        errors = np.zeros(N + 1, dtype=float)\n        \n        x_k = float(x0)\n        errors[0] = np.abs(x_k - alpha)\n        \n        # Perform N iterations of Newton's method\n        for k in range(N):\n            f_val = f(x_k)\n            fp_val = fp(x_k)\n            \n            # Newton's iteration step\n            # Note: Given problem setup ensures fp_val is not zero away from the root.\n            x_k = x_k - f_val / fp_val\n            \n            # Store the error of the new iterate\n            errors[k + 1] = np.abs(x_k - alpha)\n            \n        # The last three errors needed for calculations are e_{N-2}, e_{N-1}, e_N\n        e_N = errors[N]\n        e_N_minus_1 = errors[N - 1]\n        e_N_minus_2 = errors[N - 2]\n\n        # Calculate the final asymptotic error ratio r_{N-1}\n        # r_{N-1} = e_N / e_{N-1}\n        r_final = e_N / e_N_minus_1\n        \n        # Calculate the final empirical order of convergence p_{N-1}\n        # p_{N-1} = ln(e_N / e_{N-1}) / ln(e_{N-1} / e_{N-2})\n        # numpy.log is the natural logarithm (ln)\n        p_final = np.log(e_N / e_N_minus_1) / np.log(e_N_minus_1 / e_N_minus_2)\n        \n        results.extend([r_final, p_final])\n\n    # Final print statement in the exact required format.\n    # Output is a flat list [r_A, p_A, r_B, p_B, r_C, p_C]\n    # with each value rounded to 6 decimal places.\n    print(f\"[{','.join([f'{val:.6f}' for val in results])}]\")\n\nsolve()\n```", "id": "3265302"}, {"introduction": "While the previous practice explored how a fast method can slow down, this exercise tackles the opposite challenge: how can we accelerate a method that is inherently slow? You will start with the classic fixed-point iteration for $x = \\cos(x)$, which converges linearly, and apply Steffensen's method to transform it into a quadratically convergent process. This practice demonstrates the power of convergence acceleration techniques, a vital tool in the numerical analyst's arsenal for developing efficient computational solutions [@problem_id:3265247].", "problem": "Write a complete program that studies and compares the rates of convergence of a basic fixed-point iteration and its Steffensen-accelerated variant for the function $\\cos(x)$, using angles in radians. You must base your reasoning on the core definition of order of convergence: a sequence $\\{x_k\\}$ converges to $x^\\star$ with order $p \\ge 1$ if there exists a constant $C \\in (0,\\infty)$ such that\n$$\n\\lim_{k \\to \\infty} \\frac{|x_{k+1} - x^\\star|}{|x_k - x^\\star|^p} = C.\n$$\nThe problem is to implement the following, starting from this definition and without using any external data sources.\n\nTask:\n- Consider the fixed-point mapping $g(x) = \\cos(x)$ and the fixed-point iteration $x_{k+1} = g(x_k)$. This is known to converge linearly to the unique fixed point $x^\\star$ of $g$, that is, the solution of $x = \\cos(x)$.\n- Apply Steffensen’s method to the same mapping $g$, which, at a current iterate $x_k$, uses only function values of $g$ and is given by the algebraic update\n$$\nx_{k+1} = x_k - \\frac{\\left(g(x_k) - x_k\\right)^2}{g(g(x_k)) - 2 g(x_k) + x_k},\n$$\nwhenever the denominator is nonzero. If the denominator equals $0$ or is numerically indistinguishable from $0$ within machine precision, then for that step you must fall back to the plain fixed-point update $x_{k+1} = g(x_k)$.\n- Use the observed-order estimator computed from three consecutive errors with respect to a high-accuracy reference $x^\\star$. For errors $e_k = |x_k - x^\\star|$, define\n$$\np_k = \\frac{\\ln\\left(e_{k} / e_{k-1}\\right)}{\\ln\\left(e_{k-1} / e_{k-2}\\right)},\n$$\nwhenever the logarithms are defined. Use the latest available $p_k$ that is computed from errors not too close to floating-point underflow; specifically, prefer the largest index $k$ such that $e_{k-2} \\ge 10^{-10}$; if no such triple exists, use the largest valid $p_k$ available.\n- Use a stopping criterion based on the absolute error with respect to $x^\\star$ falling below a tolerance, or a maximum iteration cap, whichever occurs first.\n\nReference solution $x^\\star$:\n- Internally compute a high-accuracy reference for $x^\\star$ as the solution to $f(x) = \\cos(x) - x = 0$ by applying Newton’s method\n$$\nx_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}, \\quad f'(x) = -\\sin(x) - 1,\n$$\nstarting from $x_0 = 0.7$ and iterating until the Newton update magnitude is less than $10^{-16}$ or a maximum of $100$ iterations is reached. Use this internally computed $x^\\star$ for all error calculations. Angles are in radians.\n\nImplementation and numerical details:\n- Implement two solvers that generate sequences $\\{x_k\\}$:\n  1. The plain fixed-point iteration $x_{k+1} = \\cos(x_k)$.\n  2. The Steffensen-accelerated iteration defined above.\n- For both solvers, stop when $|x_k - x^\\star| < 10^{-14}$ or after $200$ iterations, whichever happens first.\n- For Steffensen’s method, if the denominator $g(g(x_k)) - 2 g(x_k) + x_k$ equals $0$ in exact arithmetic or has absolute value below $10^{-14}$ in floating-point arithmetic, use the fallback $x_{k+1} = g(x_k)$ for that step.\n- For each produced sequence, compute the observed order using the $p_k$ estimator defined above, and report a single estimate per sequence as specified earlier.\n\nTest suite:\n- Use the following four initial guesses for both methods: $x_0 \\in \\{0.1, 1.0, 2.0, -1.0\\}$.\n- For each $x_0$, run both the plain fixed-point iteration and Steffensen’s method. For each sequence, return the observed order estimate rounded to $2$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the $8$ results as a comma-separated list enclosed in square brackets, ordered as\n$$\n[\\text{plain}(0.1),\\ \\text{steff}(0.1),\\ \\text{plain}(1.0),\\ \\text{steff}(1.0),\\ \\text{plain}(2.0),\\ \\text{steff}(2.0),\\ \\text{plain}(-1.0),\\ \\text{steff}(-1.0)].\n$$\n- Each entry is the observed order estimate rounded to $2$ decimal places and must be printed as a floating-point number.\n- No user input is required. No physical units are involved. Angles are in radians. The single-line output must exactly follow the format above, for example: $[1.00,2.00,1.00,2.00,1.00,2.00,1.00,2.00]$.", "solution": "The problem requires a comparative study of the convergence rates for a standard fixed-point iteration and its Steffensen-accelerated counterpart. The analysis will be performed on the function $g(x) = \\cos(x)$, with all calculations using angles in radians. The core of the task is to implement both methods, generate sequences of iterates, and then numerically estimate the order of convergence for each sequence.\n\n### Theoretical Framework\n\nA fixed point of a function $g(x)$ is a value $x^\\star$ such that $x^\\star = g(x^\\star)$. For the given function $g(x) = \\cos(x)$, the fixed point is the unique solution to the equation $x = \\cos(x)$, also known as the Dottie number.\n\n**1. Plain Fixed-Point Iteration**\n\nThe basic fixed-point iteration is defined by the sequence $x_{k+1} = g(x_k)$, for $k=0, 1, 2, \\dots$. The convergence of this method is governed by the properties of the function $g(x)$ in the neighborhood of the fixed point $x^\\star$. According to the Fixed-Point Theorem, if $|g'(x^\\star)| < 1$, the iteration will converge linearly for any starting guess $x_0$ sufficiently close to $x^\\star$.\n\nFor $g(x) = \\cos(x)$, the derivative is $g'(x) = -\\sin(x)$. The fixed point $x^\\star$ is approximately $0.739085$. At this point, the derivative is $g'(x^\\star) = -\\sin(x^\\star) \\approx -0.674$. Since $|g'(x^\\star)| \\approx 0.674 < 1$, the iteration is guaranteed to converge.\n\nThe convergence is linear, which means the error $e_k = |x_k - x^\\star|$ decreases by a roughly constant factor at each step. Formally, a sequence $\\{x_k\\}$ converges to $x^\\star$ with order $p=1$ (linearly) if\n$$\n\\lim_{k \\to \\infty} \\frac{|x_{k+1} - x^\\star|}{|x_k - x^\\star|^1} = |g'(x^\\star)| = C,\n$$\nwhere $C \\in (0, 1)$ is the asymptotic error constant. For this problem, we expect the estimated order of convergence to be close to $p=1$.\n\n**2. Steffensen's Method**\n\nSteffensen's method is an acceleration technique that transforms a linearly convergent fixed-point iteration into a quadratically convergent one, without requiring the computation of derivatives. For a given mapping $g(x)$, the update rule is:\n$$\nx_{k+1} = x_k - \\frac{\\left(g(x_k) - x_k\\right)^2}{g(g(x_k)) - 2 g(x_k) + x_k}.\n$$\nThis can be viewed as applying Newton's method to the function $f(x) = g(x) - x = 0$, where the derivative $f'(x) = g'(x) - 1$ is approximated using a finite difference. Under suitable conditions (specifically, $g'(x^\\star) \\neq 1$), Steffensen's method exhibits quadratic convergence.\n\nQuadratic convergence (order $p=2$) means the number of correct significant digits roughly doubles at each iteration. Formally,\n$$\n\\lim_{k \\to \\infty} \\frac{|x_{k+1} - x^\\star|}{|x_k - x^\\star|^2} = C,\n$$\nfor some constant $C \\in (0, \\infty)$. We therefore expect the estimated order of convergence for Steffensen's method to be close to $p=2$. A fallback to the plain fixed-point step $x_{k+1}=g(x_k)$ is necessary if the denominator of the Steffensen update is numerically close to zero, which can occur if $g(x_k)-x_k$ is very small, i.e., when we are already very close to a solution.\n\n### Implementation Strategy\n\n**1. High-Accuracy Reference Solution, $x^\\star$**\n\nTo calculate errors $e_k = |x_k - x^\\star|$, a highly accurate value for $x^\\star$ is required. This will be computed by applying Newton's method to the equation $f(x) = \\cos(x) - x = 0$. The Newton iteration is\n$$\nx_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)} = x_n - \\frac{\\cos(x_n) - x_n}{-\\sin(x_n) - 1}.\n$$\nStarting with $x_0 = 0.7$, we iterate until the update step magnitude $|x_{n+1} - x_n|$ falls below a stringent tolerance of $10^{-16}$.\n\n**2. Iterative Solvers**\n\nTwo solvers will be implemented: one for the plain fixed-point iteration $x_{k+1} = \\cos(x_k)$ and one for Steffensen's method. Both solvers will start from a given initial guess $x_0$ and generate a sequence of iterates $\\{x_k\\}$. Iterations will terminate when the absolute error $|x_k - x^\\star|$ is less than $10^{-14}$ or a maximum of $200$ iterations is reached. Steffensen's solver will incorporate the specified fallback mechanism where if $|g(g(x_k)) - 2g(x_k) + x_k| < 10^{-14}$, the simpler update $x_{k+1} = \\cos(x_k)$ is used.\n\n**3. Observed Order of Convergence, $p_k$**\n\nThe order of convergence is estimated numerically from the sequence of errors $\\{e_k\\}$. The formula provided is:\n$$\np_k = \\frac{\\ln\\left(e_{k} / e_{k-1}\\right)}{\\ln\\left(e_{k-1} / e_{k-2}\\right)}.\n$$\nThis formula is derived from the theoretical definition $e_k \\approx C e_{k-1}^p$, which implies $\\ln(e_k) - \\ln(e_{k-1}) \\approx p (\\ln(e_{k-1}) - \\ln(e_{k-2}))$. After generating a sequence, we compute $p_k$ for all possible indices $k$. To obtain a single representative order estimate, we select the value of $p_k$ for the largest index $k$ such that the error $e_{k-2}$ is not excessively small (specifically, $e_{k-2} \\ge 10^{-10}$). This ensures the estimate is taken from the asymptotic regime of convergence but before floating-point precision limits the accuracy of the error ratios. If no such index exists, the last computed value of $p_k$ is used.\n\nThe final program will execute this process for each initial guess $x_0 \\in \\{0.1, 1.0, 2.0, -1.0\\}$, producing eight order estimates in total, which will be formatted and printed as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the numerical experiment and print the final results.\n    \"\"\"\n\n    def compute_reference_solution():\n        \"\"\"\n        Computes a high-accuracy reference solution x* for x = cos(x) using Newton's method.\n        The equation is f(x) = cos(x) - x = 0.\n        The derivative is f'(x) = -sin(x) - 1.\n        \"\"\"\n        x = 0.7  # Initial guess\n        max_iter = 100\n        tolerance = 1e-16\n        \n        for _ in range(max_iter):\n            f_x = np.cos(x) - x\n            fp_x = -np.sin(x) - 1\n            if fp_x == 0:  # Avoid division by zero\n                break\n            update = -f_x / fp_x\n            x += update\n            if np.abs(update) < tolerance:\n                break\n        return x\n\n    def run_iteration(method, x0, x_star):\n        \"\"\"\n        Generates a sequence of iterates for a given method.\n\n        Args:\n            method (str): 'plain' for fixed-point, 'steffensen' for Steffensen's method.\n            x0 (float): The initial guess.\n            x_star (float): The high-accuracy reference solution.\n\n        Returns:\n            list: The history of iterates [x0, x1, ...].\n        \"\"\"\n        x_k = float(x0)\n        history = [x_k]\n        max_iter = 200\n        stop_tol = 1e-14\n        fallback_tol = 1e-14\n\n        for _ in range(max_iter):\n            if np.abs(x_k - x_star) < stop_tol:\n                break\n            \n            if method == 'plain':\n                x_k_plus_1 = np.cos(x_k)\n            elif method == 'steffensen':\n                g_xk = np.cos(x_k)\n                g_g_xk = np.cos(g_xk)\n                denominator = g_g_xk - 2 * g_xk + x_k\n                \n                if np.abs(denominator) < fallback_tol:\n                    # Fallback to plain fixed-point iteration\n                    x_k_plus_1 = g_xk\n                else:\n                    numerator = (g_xk - x_k)**2\n                    x_k_plus_1 = x_k - numerator / denominator\n            else:\n                raise ValueError(\"Unknown method specified.\")\n            \n            x_k = x_k_plus_1\n            history.append(x_k)\n            \n        return history\n\n    def estimate_order(x_history, x_star):\n        \"\"\"\n        Estimates the order of convergence from a sequence of iterates.\n\n        Args:\n            x_history (list): The sequence of iterates.\n            x_star (float): The high-accuracy reference solution.\n\n        Returns:\n            float: The estimated order of convergence.\n        \"\"\"\n        if len(x_history) < 3:\n            # Not enough data points to compute even one p_k value.\n            # Based on the problem, this scenario is not expected.\n            return np.nan\n\n        errors = np.abs(np.array(x_history) - x_star)\n        \n        p_k_values = []\n        for k in range(2, len(errors)):\n            e_k2, e_k1, e_k = errors[k-2], errors[k-1], errors[k]\n            \n            # Avoid log(0) and division by zero from the ratios\n            if e_k1 == 0 or e_k2 == 0 or e_k1 == e_k2:\n                continue\n            \n            # Ratios for the logarithms\n            ratio1 = e_k / e_k1\n            ratio2 = e_k1 / e_k2\n\n            # Ensure arguments to log are positive\n            if ratio1 <= 0 or ratio2 <= 0:\n                continue\n            \n            log_numerator = np.log(ratio1)\n            log_denominator = np.log(ratio2)\n            \n            if log_denominator == 0:\n                continue\n            \n            p_k = log_numerator / log_denominator\n            p_k_values.append((k, p_k))\n\n        if not p_k_values:\n            # Could not compute any p_k.\n            return np.nan\n        \n        # Selection rule: Find the latest p_k where e_{k-2} is not too small.\n        for k, p_k in reversed(p_k_values):\n            if errors[k-2] >= 1e-10:\n                return p_k\n        \n        # Fallback: if no such p_k exists, use the latest one available.\n        return p_k_values[-1][1]\n\n    # Compute the reference solution once\n    x_star = compute_reference_solution()\n\n    test_cases = [0.1, 1.0, 2.0, -1.0]\n    final_results = []\n\n    for x0_val in test_cases:\n        # Plain fixed-point iteration\n        history_plain = run_iteration('plain', x0_val, x_star)\n        order_plain = estimate_order(history_plain, x_star)\n        final_results.append(order_plain)\n\n        # Steffensen-accelerated iteration\n        history_steff = run_iteration('steffensen', x0_val, x_star)\n        order_steff = estimate_order(history_steff, x_star)\n        final_results.append(order_steff)\n\n    # Format the results to 2 decimal places and print\n    formatted_output = [f\"{res:.2f}\" for res in final_results]\n    print(f\"[{','.join(formatted_output)}]\")\n\nsolve()\n```", "id": "3265247"}, {"introduction": "We have seen that some methods converge linearly while others converge quadratically, but what is the practical consequence of this distinction? This final practice moves from observation to quantification, directly addressing the computational cost associated with a \"slow\" linear convergence rate. By calculating the number of iterations required to achieve a desired precision when the convergence constant is very close to 1, you will gain a tangible appreciation for why achieving a higher order of convergence is often a critical goal in scientific computing [@problem_id:3265283].", "problem": "Consider a fixed-point iteration defined by $x_{k+1} = g(x_k)$ that converges to a fixed point $x^{\\ast}$, and suppose there exists a neighborhood $\\mathcal{N}$ of $x^{\\ast}$ such that for all $x, y \\in \\mathcal{N}$ the mapping $g$ satisfies the contraction property $\\lvert g(x) - g(y) \\rvert \\leq C \\lvert x - y \\rvert$ with a constant $C$ strictly less than $1$. Assume the iteration is initialized at $x_0 \\in \\mathcal{N}$ with an a priori bound on the initial absolute error $\\lvert x_0 - x^{\\ast} \\rvert \\leq 10^{-2}$. In a particular implementation, the contraction constant is empirically estimated to be $C = 0.999$ near $x^{\\ast}$. Starting only from the definition of a contraction mapping and the notion of linear convergence, derive an explicit bound on the absolute error after $k$ iterations and determine the smallest integer number of iterations $N$ that guarantees $\\lvert x_N - x^{\\ast} \\rvert \\leq 10^{-10}$. Your final answer must be the single integer $N$.", "solution": "The problem is validated as follows:\nFirst, the given information is extracted.\n- Fixed-point iteration: $x_{k+1} = g(x_k)$ converging to a fixed point $x^{\\ast}$.\n- Contraction property: $\\lvert g(x) - g(y) \\rvert \\leq C \\lvert x - y \\rvert$ for all $x, y$ in a neighborhood $\\mathcal{N}$ of $x^{\\ast}$.\n- Contraction constant: $C < 1$.\n- Initial condition: $x_0 \\in \\mathcal{N}$.\n- Initial error bound: $\\lvert x_0 - x^{\\ast} \\rvert \\leq 10^{-2}$.\n- Specific contraction constant: $C = 0.999$.\n- Objective: Find the smallest integer number of iterations $N$ that guarantees $\\lvert x_N - x^{\\ast} \\rvert \\leq 10^{-10}$.\n\nThe problem is scientifically grounded, being a direct application of the Banach Fixed-Point Theorem and its convergence properties, which are fundamental concepts in numerical analysis. The problem is well-posed, providing all necessary information ($C$, initial error bound, target precision) to determine a unique integer solution. The language is objective and mathematically precise. Therefore, the problem is deemed valid and a solution can be derived.\n\nThe core of the problem is to establish a bound on the absolute error after $k$ iterations, denoted by $e_k = \\lvert x_k - x^{\\ast} \\rvert$. We are given the fixed-point iteration $x_{k+1} = g(x_k)$. Since $x^{\\ast}$ is a fixed point, it satisfies the equation $x^{\\ast} = g(x^{\\ast})$.\n\nThe absolute error at step $k+1$ can be written as:\n$$e_{k+1} = \\lvert x_{k+1} - x^{\\ast} \\rvert = \\lvert g(x_k) - g(x^{\\ast}) \\rvert$$\nAssuming the iterates $x_k$ remain in the neighborhood $\\mathcal{N}$ where the contraction property holds, we can apply the given inequality with $x = x_k$ and $y = x^{\\ast}$:\n$$\\lvert g(x_k) - g(x^{\\ast}) \\rvert \\leq C \\lvert x_k - x^{\\ast} \\rvert$$\nThis directly gives us a recursive relationship for the error:\n$$e_{k+1} \\leq C e_k$$\nThis relationship shows that the error at each step is reduced by a factor of at least $C$. We can apply this relation recursively to establish a bound on $e_k$ in terms of the initial error $e_0 = \\lvert x_0 - x^{\\ast} \\rvert$.\nFor $k=1$:\n$$e_1 \\leq C e_0$$\nFor $k=2$:\n$$e_2 \\leq C e_1 \\leq C (C e_0) = C^2 e_0$$\nBy induction, for any integer $k \\geq 0$, the error after $k$ iterations is bounded by:\n$$e_k = \\lvert x_k - x^{\\ast} \\rvert \\leq C^k e_0 = C^k \\lvert x_0 - x^{\\ast} \\rvert$$\nThis is the explicit bound on the absolute error after $k$ iterations. This type of convergence, where the error is reduced by a constant factor at each step, is known as linear convergence.\n\nThe problem requires us to find the smallest integer $N$ that guarantees the absolute error is no more than $10^{-10}$. That is, we must find the smallest integer $N$ such that $\\lvert x_N - x^{\\ast} \\rvert \\leq 10^{-10}$.\nUsing the error bound we derived, this condition is guaranteed if:\n$$C^N \\lvert x_0 - x^{\\ast} \\rvert \\leq 10^{-10}$$\nWe are given an a priori bound on the initial error, $\\lvert x_0 - x^{\\ast} \\rvert \\leq 10^{-2}$. To guarantee the condition for any starting point satisfying this bound, we must consider the worst-case scenario for the initial error, which is its maximum possible value, $\\lvert x_0 - x^{\\ast} \\rvert = 10^{-2}$.\nSubstituting the given values $C = 0.999$ and the initial error bound into the inequality, we get:\n$$(0.999)^N (10^{-2}) \\leq 10^{-10}$$\nTo solve for $N$, we first isolate the term containing $N$:\n$$(0.999)^N \\leq \\frac{10^{-10}}{10^{-2}} = 10^{-8}$$\nTo solve for the exponent $N$, we take the natural logarithm of both sides of the inequality. Since the natural logarithm function, $\\ln(x)$, is monotonically increasing for $x > 0$, the direction of the inequality is preserved:\n$$\\ln((0.999)^N) \\leq \\ln(10^{-8})$$\nUsing the logarithm property $\\ln(a^b) = b \\ln(a)$, we have:\n$$N \\ln(0.999) \\leq -8 \\ln(10)$$\nNow, we must solve for $N$. It is crucial to note that since $0 < 0.999 < 1$, its natural logarithm $\\ln(0.999)$ is a negative number. Therefore, when we divide both sides of the inequality by $\\ln(0.999)$, we must reverse the direction of the inequality sign:\n$$N \\geq \\frac{-8 \\ln(10)}{\\ln(0.999)}$$\nThe right-hand side of the inequality is a positive value, as it is a ratio of two negative numbers. We can calculate this value:\n$$N \\geq \\frac{-8 \\ln(10)}{\\ln(1 - 0.001)}$$\nUsing computational tools, we find $\\ln(10) \\approx 2.302585$ and $\\ln(0.999) \\approx -0.0010005$.\n$$N \\geq \\frac{-8 \\times 2.30258509...}{-0.0010005003...} \\approx 18411.4709...$$\nThe problem asks for the smallest integer number of iterations $N$. Since $N$ must be an integer and satisfy the condition $N \\geq 18411.4709...$, the smallest such integer is the ceiling of this value.\n$$N = \\lceil 18411.4709... \\rceil = 18412$$\nThus, a minimum of $18412$ iterations are required to guarantee that the absolute error is reduced from an initial bound of $10^{-2}$ to a final bound of $10^{-10}$ with a contraction constant of $0.999$.", "answer": "$$\\boxed{18412}$$", "id": "3265283"}]}