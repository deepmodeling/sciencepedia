{"hands_on_practices": [{"introduction": "The first step in mastering matrix norms is to become fluent in their calculation. This exercise provides direct practice with the three most common induced norms—the $1$-norm, $2$-norm, and $\\infty$-norm—using a simple diagonal matrix. By working with a matrix that represents a straightforward geometric scaling, you can build intuition for how these different norms quantify the \"size\" or maximum stretching effect of a linear transformation.", "problem": "In two-dimensional computer graphics, a scaling transformation is often represented by a matrix. Consider a non-uniform scaling transformation in a 2D Cartesian coordinate system represented by the matrix $S$ given by:\n$$\nS = \\begin{pmatrix} 4.5  0 \\\\ 0  2.1 \\end{pmatrix}\n$$\nThis matrix scales the x-component of a vector by a factor of $4.5$ and the y-component by a factor of $2.1$.\n\nFor this scaling matrix $S$, compute its induced 1-norm $\\|S\\|_1$, its induced 2-norm (also known as the spectral norm) $\\|S\\|_2$, and its induced $\\infty$-norm $\\|S\\|_\\infty$. Select the option that correctly lists all three values.\n\nA. $\\|S\\|_1 = 6.6$, $\\|S\\|_2 = 4.966$, $\\|S\\|_\\infty = 6.6$\n\nB. $\\|S\\|_1 = 4.5$, $\\|S\\|_2 = 4.966$, $\\|S\\|_\\infty = 4.5$\n\nC. $\\|S\\|_1 = 4.5$, $\\|S\\|_2 = 4.5$, $\\|S\\|_\\infty = 4.5$\n\nD. $\\|S\\|_1 = 2.1$, $\\|S\\|_2 = 2.1$, $\\|S\\|_\\infty = 2.1$\n\nE. $\\|S\\|_1 = 4.5$, $\\|S\\|_2 = 2.1$, $\\|S\\|_\\infty = 4.5$", "solution": "We are given the diagonal matrix\n$$\nS=\\begin{pmatrix}4.5  0 \\\\ 0  2.1\\end{pmatrix}.\n$$\nFor the induced 1-norm, by definition,\n$$\n\\|S\\|_{1}=\\max_{j}\\sum_{i}|a_{ij}|,\n$$\nwhich is the maximum absolute column sum. The column sums are\n$$\n|4.5|+|0|=4.5,\\quad |0|+|2.1|=2.1,\n$$\nso\n$$\n\\|S\\|_{1}=4.5.\n$$\n\nFor the induced infinity-norm, by definition,\n$$\n\\|S\\|_{\\infty}=\\max_{i}\\sum_{j}|a_{ij}|,\n$$\nwhich is the maximum absolute row sum. The row sums are\n$$\n|4.5|+|0|=4.5,\\quad |0|+|2.1|=2.1,\n$$\nso\n$$\n\\|S\\|_{\\infty}=4.5.\n$$\n\nFor the induced 2-norm (spectral norm), by definition,\n$$\n\\|S\\|_{2}=\\sqrt{\\lambda_{\\max}(S^{\\mathsf{T}}S)}.\n$$\nSince $S$ is diagonal, we have\n$$\nS^{\\mathsf{T}}S=\\begin{pmatrix}4.5^{2}  0 \\\\ 0  2.1^{2}\\end{pmatrix},\n$$\nwhose eigenvalues are $4.5^{2}$ and $2.1^{2}$. Therefore,\n$$\n\\|S\\|_{2}=\\sqrt{\\max\\{4.5^{2},\\,2.1^{2}\\}}=4.5.\n$$\n\nThus,\n$$\n\\|S\\|_{1}=4.5,\\quad \\|S\\|_{2}=4.5,\\quad \\|S\\|_{\\infty}=4.5,\n$$\nwhich corresponds to option C.", "answer": "$$\\boxed{C}$$", "id": "1376555"}, {"introduction": "Beyond simple computation, a deeper understanding of matrix norms comes from grasping their role as bounds in linear transformations. The inequality $\\|Ax\\|_p \\le \\|A\\|_p \\|x\\|_p$ is a cornerstone of numerical analysis, guaranteeing that a matrix cannot amplify a vector's norm by more than a factor of $\\|A\\|_p$. This practice [@problem_id:2186740] challenges you to explore the conditions for equality, investigating the specific circumstances under which a matrix transformation achieves this maximum possible amplification.", "problem": "In the study of numerical linear algebra, matrix norms are fundamental. For any matrix $A \\in \\mathbb{R}^{m \\times n}$ and vector $x \\in \\mathbb{R}^n$, the induced matrix norm $\\|A\\|_p$ is defined in a way that is compatible with the vector norm $\\|x\\|_p$, satisfying the submultiplicative property $\\|Ax\\|_p \\le \\|A\\|_p \\|x\\|_p$.\n\nWe will focus on the $L_1$-norm. For a vector $x = (x_1, x_2, \\dots, x_n)^T$, its $L_1$-norm is defined as $\\|x\\|_1 = \\sum_{i=1}^n |x_i|$. The corresponding induced matrix norm, often called the maximum absolute column sum norm, is given by $\\|A\\|_1 = \\max_{1 \\le j \\le n} \\sum_{i=1}^m |A_{ij}|$.\n\nWhile the inequality $\\|Ax\\|_1 \\le \\|A\\|_1 \\|x\\|_1$ always holds, the case of equality, $\\|Ax\\|_1 = \\|A\\|_1 \\|x\\|_1$, is of special interest as it characterizes the maximum possible \"amplification\" of a vector's norm by the matrix transformation.\n\nConsider the matrix $A = \\begin{pmatrix} 2  -3 \\\\ 5  \\alpha \\end{pmatrix}$ and the vector $x = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$, where $\\alpha$ is a real parameter.\n\nDetermine the unique real value of $\\alpha$ for which the equality $\\|Ax\\|_1 = \\|A\\|_1 \\|x\\|_1$ holds.", "solution": "We are given $A=\\begin{pmatrix}2  -3 \\\\ 5  \\alpha\\end{pmatrix}$ and $x=\\begin{pmatrix}1 \\\\ -1\\end{pmatrix}$. First compute the norms and the product $Ax$:\n$$\n\\|x\\|_{1}=|1|+|-1|=2,\n$$\n$$\nAx=\\begin{pmatrix}2\\cdot 1+(-3)\\cdot(-1)\\\\ 5\\cdot 1+\\alpha\\cdot(-1)\\end{pmatrix}=\\begin{pmatrix}5\\\\ 5-\\alpha\\end{pmatrix},\n$$\n$$\n\\|Ax\\|_{1}=|5|+|5-\\alpha|=5+|5-\\alpha|.\n$$\nThe induced $L_{1}$-matrix norm is the maximum absolute column sum:\n$$\n\\|A\\|_{1}=\\max\\left\\{|2|+|5|,\\;|-3|+|\\alpha|\\right\\}=\\max\\left\\{7,\\;3+|\\alpha|\\right\\}.\n$$\nThe equality condition $\\|Ax\\|_{1}=\\|A\\|_{1}\\|x\\|_{1}$ becomes\n$$\n5+|5-\\alpha|=2\\max\\left\\{7,\\;3+|\\alpha|\\right\\}.\n$$\nWe solve this piecewise.\n\nIf $|\\alpha|\\leq 4$, then $\\|A\\|_{1}=7$ and\n$$\n5+|5-\\alpha|=14\\quad\\Longrightarrow\\quad |5-\\alpha|=9\\quad\\Longrightarrow\\quad \\alpha=-4\\ \\text{ or }\\ \\alpha=14.\n$$\nThe case assumption $|\\alpha|\\leq 4$ retains only $\\alpha=-4$.\n\nIf $|\\alpha|4$, then $\\|A\\|_{1}=3+|\\alpha|$ and\n$$\n5+|5-\\alpha|=2(3+|\\alpha|)\\quad\\Longrightarrow\\quad |5-\\alpha|=1+2|\\alpha|.\n$$\nBut $|5-\\alpha|\\leq 5+|\\alpha|$ implies $1+2|\\alpha|\\leq 5+|\\alpha|$, hence $|\\alpha|\\leq 4$, which contradicts $|\\alpha|4$. Thus there is no solution in this case.\n\nTherefore the only solution from the norm equality is $\\alpha=-4$. This also matches the structural equality conditions for the induced $L_{1}$-norm: since $x$ has both entries nonzero, both columns must have maximal column sums, giving $7=3+|\\alpha|$ so $|\\alpha|=4$, and the triangle inequality must be tight row-wise; with $x_{1}=1$ and $x_{2}=-1$, row $2$ requires $5$ and $-\\alpha$ to have the same sign, which forces $\\alpha0$. Together these yield $\\alpha=-4$.\n\nVerification at $\\alpha=-4$:\n$$\n\\|Ax\\|_{1}=5+|5-(-4)|=5+9=14,\\quad \\|A\\|_{1}=\\max\\{7,3+4\\}=7,\\quad \\|x\\|_{1}=2,\n$$\nso indeed $\\|Ax\\|_{1}=14=\\|A\\|_{1}\\|x\\|_{1}$.", "answer": "$$\\boxed{-4}$$", "id": "2186740"}, {"introduction": "Matrix norms are not merely tools for analysis; they are essential for synthesis and optimization in modern scientific computing. This advanced exercise [@problem_id:3250797] presents a classic problem: finding the \"smallest\" matrix that satisfies a set of linear constraints, where size is measured by the Frobenius norm. This principle of minimum-norm solutions is a powerful concept that underpins methods in fields ranging from control systems engineering to machine learning.", "problem": "Consider the real vector space of matrices $\\mathbb{R}^{m \\times n}$ endowed with the Frobenius inner product $\\langle A, B \\rangle_{\\mathrm{F}} = \\mathrm{trace}(A^{\\top} B)$ and its induced Frobenius norm $\\|A\\|_{\\mathrm{F}} = \\sqrt{\\langle A, A \\rangle_{\\mathrm{F}}}$. Let $m = 2$, $n = 3$, and suppose we are given two input vectors $u_{1}, u_{2} \\in \\mathbb{R}^{3}$ and two target vectors $v_{1}, v_{2} \\in \\mathbb{R}^{2}$ defined by\n$$\nu_{1} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\quad u_{2} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad\nv_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad v_{2} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}.\n$$\nAmong all matrices $A \\in \\mathbb{R}^{2 \\times 3}$ satisfying the linear constraints $A u_{1} = v_{1}$ and $A u_{2} = v_{2}$, determine the minimal possible Frobenius norm $\\|A\\|_{\\mathrm{F}}$. Your derivation must start from foundational definitions (inner product, norm, linear constraints) and standard tools of constrained optimization or orthogonal projections in Euclidean spaces. Do not assume any specialized formulas for the answer a priori. Provide the exact value of the minimal norm as a single closed-form expression. No rounding is required.", "solution": "The problem asks for the minimum Frobenius norm of a matrix $A \\in \\mathbb{R}^{2 \\times 3}$ that satisfies two linear constraints, $A u_{1} = v_{1}$ and $A u_{2} = v_{2}$. The space of matrices $\\mathbb{R}^{m \\times n}$ is a real Hilbert space when endowed with the Frobenius inner product $\\langle A, B \\rangle_{\\mathrm{F}} = \\mathrm{trace}(A^{\\top} B)$. The Frobenius norm is induced by this inner product, $\\|A\\|_{\\mathrm{F}} = \\sqrt{\\langle A, A \\rangle_{\\mathrm{F}}}$.\n\nMinimizing $\\|A\\|_{\\mathrm{F}}$ is equivalent to minimizing its square, $\\|A\\|_{\\mathrm{F}}^2$. The problem can thus be formulated as a constrained optimization problem:\n$$\n\\text{minimize} \\quad f(A) = \\|A\\|_{\\mathrm{F}}^2 = \\langle A, A \\rangle_{\\mathrm{F}} \\\\\n\\text{subject to} \\quad A u_{1} = v_{1} \\quad \\text{and} \\quad A u_{2} = v_{2}\n$$\nThe set of matrices satisfying the linear constraints forms a non-empty affine subspace of $\\mathbb{R}^{2 \\times 3}$. By the Hilbert projection theorem, there exists a unique matrix $A$ in this affine subspace with the minimum norm. We can find this matrix using the method of Lagrange multipliers.\n\nLet the Lagrangian function $\\mathcal{L}$ be defined for $A \\in \\mathbb{R}^{2 \\times 3}$ and Lagrange multiplier vectors $\\lambda_1, \\lambda_2 \\in \\mathbb{R}^{2}$:\n$$\n\\mathcal{L}(A, \\lambda_1, \\lambda_2) = \\frac{1}{2} \\|A\\|_{\\mathrm{F}}^2 - \\lambda_1^{\\top}(A u_1 - v_1) - \\lambda_2^{\\top}(A u_2 - v_2)\n$$\nWe use $\\frac{1}{2}\\|A\\|_{\\mathrm{F}}^2$ for algebraic convenience, as it does not change the minimizer. To find the gradient of $\\mathcal{L}$ with respect to $A$, we first express the constraint terms using the Frobenius inner product. For a vector $\\lambda \\in \\mathbb{R}^{m}$ and a vector $u \\in \\mathbb{R}^{n}$, the scalar product $\\lambda^{\\top} A u$ can be written as:\n$$\n\\lambda^{\\top} A u = \\mathrm{trace}(\\lambda^{\\top} A u) = \\mathrm{trace}(u \\lambda^{\\top} A) = \\langle (u \\lambda^{\\top})^{\\top}, A \\rangle_{\\mathrm{F}} = \\langle \\lambda u^{\\top}, A \\rangle_{\\mathrm{F}}\n$$\nwhere we used the cyclic property of the trace. The matrix $\\lambda u^{\\top}$ is an element of $\\mathbb{R}^{m \\times n}$.\n\nApplying this to our Lagrangian, we get:\n$$\n\\mathcal{L}(A, \\lambda_1, \\lambda_2) = \\frac{1}{2} \\langle A, A \\rangle_{\\mathrm{F}} - \\langle \\lambda_1 u_1^{\\top}, A \\rangle_{\\mathrm{F}} + \\lambda_1^{\\top} v_1 - \\langle \\lambda_2 u_2^{\\top}, A \\rangle_{\\mathrm{F}} + \\lambda_2^{\\top} v_2\n$$\nThe Fréchet derivative of $\\mathcal{L}$ with respect to $A$ at the optimal point must be zero. The derivative is:\n$$\n\\nabla_A \\mathcal{L} = A - \\lambda_1 u_1^{\\top} - \\lambda_2 u_2^{\\top}\n$$\nSetting the derivative to the zero matrix gives the form of the optimal matrix, which we denote as $A^*$:\n$$\nA^* = \\lambda_1 u_1^{\\top} + \\lambda_2 u_2^{\\top}\n$$\nThis demonstrates that the minimum-norm solution is a linear combination of the outer products formed by the Lagrange multipliers and the input vectors.\n\nWe now determine the multiplier vectors $\\lambda_1, \\lambda_2 \\in \\mathbb{R}^2$ by substituting the form of $A^*$ back into the constraints:\n$A^* u_1 = (\\lambda_1 u_1^{\\top} + \\lambda_2 u_2^{\\top}) u_1 = \\lambda_1(u_1^{\\top} u_1) + \\lambda_2(u_2^{\\top} u_1) = v_1$\n$A^* u_2 = (\\lambda_1 u_1^{\\top} + \\lambda_2 u_2^{\\top}) u_2 = \\lambda_1(u_1^{\\top} u_2) + \\lambda_2(u_2^{\\top} u_2) = v_2$\n\nLet's compute the scalar products of the given vectors $u_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$ and $u_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}$:\n$u_1^{\\top} u_1 = 1^2 + 1^2 + 0^2 = 2$\n$u_2^{\\top} u_2 = 0^2 + 1^2 + 1^2 = 2$\n$u_1^{\\top} u_2 = u_2^{\\top} u_1 = 1 \\cdot 0 + 1 \\cdot 1 + 0 \\cdot 1 = 1$\n\nThe system of equations for $\\lambda_1$ and $\\lambda_2$ becomes:\n$$\n2 \\lambda_1 + \\lambda_2 = v_1 \\\\\n\\lambda_1 + 2 \\lambda_2 = v_2\n$$\nThis is a system of two linear equations for the vectors $\\lambda_1, \\lambda_2$. We can solve for them. Multiply the second equation by $2$: $2\\lambda_1 + 4\\lambda_2 = 2v_2$. Subtracting the first equation from this yields:\n$3 \\lambda_2 = 2v_2 - v_1 \\implies \\lambda_2 = \\frac{1}{3}(2v_2 - v_1)$\nSimilarly, multiplying the first equation by $2$ and subtracting the second gives:\n$3 \\lambda_1 = 2v_1 - v_2 \\implies \\lambda_1 = \\frac{1}{3}(2v_1 - v_2)$\n\nNow, we substitute the given target vectors $v_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $v_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$:\n$$\n\\lambda_1 = \\frac{1}{3} \\left( 2 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\right) = \\frac{1}{3} \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}\n$$\n$$\n\\lambda_2 = \\frac{1}{3} \\left( 2 \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\right) = \\frac{1}{3} \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix}\n$$\nWe have found the Lagrange multipliers. Now we can find the Frobenius norm of the solution $A^*$. A particularly elegant way to compute this norm is to use the properties of the inner product and the form of $A^*$:\n$$\n\\|A^*\\|_{\\mathrm{F}}^2 = \\langle A^*, A^* \\rangle_{\\mathrm{F}} = \\langle A^*, \\lambda_1 u_1^{\\top} + \\lambda_2 u_2^{\\top} \\rangle_{\\mathrm{F}}\n$$\nBy linearity of the inner product:\n$$\n\\|A^*\\|_{\\mathrm{F}}^2 = \\langle A^*, \\lambda_1 u_1^{\\top} \\rangle_{\\mathrm{F}} + \\langle A^*, \\lambda_2 u_2^{\\top} \\rangle_{\\mathrm{F}}\n$$\nUsing the identity $\\langle C, \\lambda u^{\\top} \\rangle_{\\mathrm{F}} = \\lambda^{\\top} C u$, this becomes:\n$$\n\\|A^*\\|_{\\mathrm{F}}^2 = \\lambda_1^{\\top} (A^* u_1) + \\lambda_2^{\\top} (A^* u_2)\n$$\nSince $A^*$ satisfies the constraints, $A^* u_1 = v_1$ and $A^* u_2 = v_2$. Therefore:\n$$\n\\|A^*\\|_{\\mathrm{F}}^2 = \\lambda_1^{\\top} v_1 + \\lambda_2^{\\top} v_2\n$$\nSubstituting the expressions for $\\lambda_1$, $\\lambda_2$, $v_1$, and $v_2$:\n$$\n\\lambda_1^{\\top} v_1 = \\frac{1}{3} \\begin{pmatrix} 2  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\frac{2}{3}\n$$\n$$\n\\lambda_2^{\\top} v_2 = \\frac{1}{3} \\begin{pmatrix} -1  2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\frac{2}{3}\n$$\nThe squared minimal norm is:\n$$\n\\|A^*\\|_{\\mathrm{F}}^2 = \\frac{2}{3} + \\frac{2}{3} = \\frac{4}{3}\n$$\nThe minimal Frobenius norm is the square root of this value:\n$$\n\\|A^*\\|_{\\mathrm{F}} = \\sqrt{\\frac{4}{3}} = \\frac{2}{\\sqrt{3}} = \\frac{2\\sqrt{3}}{3}\n$$\nThis is the exact minimal norm.", "answer": "$$\\boxed{\\frac{2\\sqrt{3}}{3}}$$", "id": "3250797"}]}