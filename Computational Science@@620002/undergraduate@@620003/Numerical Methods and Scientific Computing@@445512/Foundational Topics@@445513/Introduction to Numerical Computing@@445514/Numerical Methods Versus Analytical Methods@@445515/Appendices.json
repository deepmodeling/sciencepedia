{"hands_on_practices": [{"introduction": "Many problems in science and engineering boil down to finding the root of an equation. While polynomial equations have systematic solutions, transcendental equations like $x = \\cos(x)$ often lack a closed-form analytical solution. This practice guides you through the quintessential numerical approach: first, we use the Contraction Mapping Theorem to rigorously prove that a unique solution exists, and then we implement a simple iterative method to approximate it, giving us a tangible answer where analytical methods cannot. [@problem_id:3259287]", "problem": "Consider the fixed point problem of finding a real number $x^{\\ast}$ such that $x^{\\ast} = \\cos(x^{\\ast})$, where $\\cos(\\cdot)$ denotes the cosine function with its argument measured in radians. Your tasks are to analyze the limitations of a symbolic approach, justify existence and uniqueness of the fixed point using first principles, and implement a numerical method to approximate it with a rigorous a posteriori error bound, all in a way that is testable by a program.\n\n1. Analytical limitations. Explain why a symbolic elementary closed-form expression for solving $x = \\cos(x)$ is not expected and is not used here. You should make clear that this is a transcendental fixed point equation and justify, at a high level appropriate for an advanced undergraduate, why algebraic manipulations within elementary functions do not yield an explicit formula for $x^{\\ast}$.\n\n2. Existence and uniqueness via contraction. Let $g(x) = \\cos(x)$. Using only fundamental definitions and well-tested facts, show that $g$ has a unique fixed point in a closed interval that is invariant under $g$. Specifically:\n   - Identify a closed interval $I \\subset \\mathbb{R}$ such that $g(I) \\subset I$.\n   - Bound the derivative $g'(x)$ on $I$ to obtain a Lipschitz constant $L$ with $0 \\le L < 1$.\n   - Conclude from the Banach Fixed Point Theorem (also known as the Contraction Mapping Theorem) that there exists a unique fixed point $x^{\\ast} \\in I$.\n\n3. Numerical iteration and stopping rule. Consider the simple fixed-point iteration defined by $x_{n+1} = g(x_n) = \\cos(x_n)$ with an initial guess $x_0 \\in \\mathbb{R}$. Design a stopping rule based solely on successive iterates:\n   - Stop at the first index $n \\ge 0$ such that $|x_{n+1} - x_n| \\le \\text{tol}$, where $\\text{tol} > 0$ is a user-specified tolerance.\n   - Impose a hard iteration cap $\\text{max\\_iter} \\in \\mathbb{N}$; if the stopping condition has not been met by $n = \\text{max\\_iter}$, report non-convergence.\n\n4. A posteriori error bound. Assume your iterates lie in an interval $I$ on which $g$ is a contraction with Lipschitz constant $L$ satisfying $0 \\le L < 1$. Derive and use the standard a posteriori bound\n   $$|x_{n+1} - x^{\\ast}| \\le \\frac{L}{1 - L} |x_{n+1} - x_n|,$$\n   which must be computed at the last available iteration, regardless of whether the stopping rule has been met. Your program should use $L = \\sin(1)$, corresponding to a contraction analysis on the invariant interval $I = [-1, 1]$.\n\n5. Units. Angles must be in radians. No other physical units are involved.\n\n6. Test suite. Your program must run the method on the following test cases, each specified by a triple $(x_0, \\text{tol}, \\text{max\\_iter})$:\n   - Case A (happy path): $(0, 10^{-12}, 100)$, that is $x_0 = 0$, $\\text{tol} = 10^{-12}$, $\\text{max\\_iter} = 100$.\n   - Case B (different start in invariant set): $(1, 10^{-12}, 100)$.\n   - Case C (start far from invariant set, rapid entry): $(10, 10^{-12}, 100)$.\n   - Case D (boundary condition: insufficient iterations): $(0.5, 10^{-16}, 5)$.\n\n7. Required final output format. For each test case, output a list of four items:\n   - The numerical approximation to $x^{\\ast}$ at termination (either when the stopping rule is satisfied or when the iteration cap is reached).\n   - The number of iterations performed.\n   - A boolean indicating whether the stopping rule was satisfied within the iteration cap.\n   - The a posteriori error bound $\\dfrac{L}{1-L} |x_{n+1} - x_n|$ computed at the last available iteration, using $L = \\sin(1)$.\n\n   Your program should produce a single line of output containing the results for all four cases as a comma-separated list enclosed in square brackets, for example\n   $[ \\text{caseA\\_result}, \\text{caseB\\_result}, \\text{caseC\\_result}, \\text{caseD\\_result} ]$,\n   where each $\\text{caseX\\_result}$ is itself a list $[x\\_{\\text{approx}}, \\text{iterations}, \\text{converged}, \\text{bound}]$.\n\n8. Numerical reporting. The two floating-point outputs in each case, namely the approximation to $x^{\\ast}$ and the a posteriori bound, must be rounded to $12$ decimal places before printing. The iteration count must be an integer, and the convergence flag must be a boolean.\n\nAll angles must be in radians, and all numerical values should be dimensionless. No percentage signs are to be used anywhere; any ratio should be expressed as a decimal number if needed. The final output must be exactly one line in the specified format.", "solution": "The problem statement is a valid exercise in numerical analysis. It asks for the theoretical justification and numerical approximation of the fixed point of the cosine function, $x^{\\ast} = \\cos(x^{\\ast})$. The problem is scientifically grounded, well-posed, objective, and contains all necessary information for a complete solution.\n\n### 1. Analytical Limitations\n\nThe equation to be solved is $x = \\cos(x)$. This is a transcendental equation because it equates a polynomial function, $f(x)=x$, with a transcendental function, $g(x) = \\cos(x)$. A solution in \"elementary closed form\" would mean expressing the fixed point $x^{\\ast}$ using a finite combination of arithmetic operations, roots, exponentials, logarithms, and trigonometric functions.\n\nThere is no general algebraic procedure to solve such equations. Algebraic manipulations, which are effective for polynomial equations, rely on inverting operations (e.g., inverting $x^2$ by taking a square root). The cosine function does not have an elementary inverse that can be used to isolate $x$ from the expression $\\cos(x)$. While its inverse, $\\arccos(y)$, exists, applying it to the equation $x = \\cos(x)$ yields $\\arccos(x) = \\arccos(\\cos(x))$, which leads to identities like $\\arccos(x) = x$ or $\\arccos(x) = -x + 2k\\pi$ for integer $k$, which are themselves new transcendental equations and do not isolate $x$.\n\nThe inability to find a closed-form solution is a deep property related to the algebraic structure of the field of elementary functions. Proving that no such solution exists is a non-trivial task (related to results like the Lindemann-Weierstrass theorem or differential Galois theory, which are beyond the scope here), but it is a standard result that equations mixing algebraic and transcendental terms, like $x = \\cos(x)$, generally do not have solutions expressible in terms of standard elementary functions. Consequently, one must resort to numerical methods to obtain an approximation of the solution.\n\n### 2. Existence and Uniqueness via the Banach Fixed Point Theorem\n\nThe Banach Fixed Point Theorem (or Contraction Mapping Theorem) provides a powerful tool to prove the existence and uniqueness of fixed points. The theorem states that if a function $g$ maps a complete metric space $(X, d)$ into itself and is a contraction mapping, then $g$ has a unique fixed point in $X$.\n\nLet the function be $g(x) = \\cos(x)$ and the space be a closed interval $I \\subset \\mathbb{R}$ with the standard Euclidean metric $d(x, y) = |x - y|$. A closed interval in $\\mathbb{R}$ is a complete metric space. We must perform two steps:\n\n**Step 2a: Identify an Invariant Interval $I$**\n\nWe need to find a closed interval $I$ such that for any $x \\in I$, the value $g(x)$ is also in $I$; that is, $g(I) \\subset I$. The range of the cosine function is $[-1, 1]$ for any real input. Therefore, for any $x \\in \\mathbb{R}$, $g(x) = \\cos(x) \\in [-1, 1]$. If we choose the interval $I = [-1, 1]$, then for any $x \\in I$, $g(x) = \\cos(x)$ is guaranteed to be in $[-1, 1]$. Thus, $g([-1, 1]) \\subset [-1, 1]$, and $I = [-1, 1]$ is an invariant set for $g$.\n\n**Step 2b: Show $g$ is a Contraction on $I$**\n\nA function $g$ is a contraction on $I$ if there exists a Lipschitz constant $L$ such that $0 \\le L < 1$ and for all $x, y \\in I$, $|g(x) - g(y)| \\le L|x - y|$.\n\nBy the Mean Value Theorem, for any $x, y \\in I$, there exists a point $c$ between $x$ and $y$ such that $|g(x) - g(y)| = |g'(c)||x - y|$. To find the smallest possible Lipschitz constant, we must find the maximum value of $|g'(x)|$ on $I$.\n\nThe derivative of $g(x)$ is $g'(x) = -\\sin(x)$. On the interval $I = [-1, 1]$, we need to bound $|g'(x)| = |-\\sin(x)| = |\\sin(x)|$. The function $|\\sin(x)|$ is even and is increasing on $[0, 1]$. Therefore, its maximum value on $[-1, 1]$ occurs at the endpoints $x = -1$ and $x = 1$. The maximum value is $L = \\sup_{x \\in [-1, 1]} |\\sin(x)| = \\sin(1)$.\n\nNumerically, $L = \\sin(1) \\approx 0.84147$. Since $0 < 1 < \\pi/2$, we know that $0 < \\sin(1) < \\sin(\\pi/2) = 1$. Thus, we have found a Lipschitz constant $L = \\sin(1)$ such that $0 \\le L < 1$.\n\n**Step 2c: Conclusion**\n\nThe function $g(x) = \\cos(x)$ maps the complete metric space $I = [-1, 1]$ to itself and is a contraction mapping on $I$ with constant $L = \\sin(1) < 1$. Therefore, by the Banach Fixed Point Theorem, there exists a unique fixed point $x^{\\ast} \\in [-1, 1]$ such that $x^{\\ast} = \\cos(x^{\\ast})$.\n\n### 3. Numerical Iteration and Stopping Rule\n\nThe constructive proof of the Banach Fixed Point Theorem provides the basis for a numerical algorithm. Starting from an initial guess $x_0$, the sequence $x_{n+1} = g(x_n) = \\cos(x_n)$ for $n \\ge 0$ is guaranteed to converge to the unique fixed point $x^{\\ast}$, for any $x_0 \\in \\mathbb{R}$. This is because the first iteration $x_1 = \\cos(x_0)$ will place the iterate inside the invariant interval $I = [-1, 1]$, and all subsequent iterates will remain in $I$, where convergence is guaranteed.\n\nIn practice, the iteration must terminate after a finite number of steps. The algorithm will be:\n1. Initialize $x_0$, a tolerance $\\text{tol} > 0$, and a maximum number of iterations $\\text{max\\_iter}$.\n2. For $n = 0, 1, 2, \\dots, \\text{max\\_iter}-1$:\n   a. Compute the next iterate: $x_{n+1} = \\cos(x_n)$.\n   b. Check the stopping condition: if $|x_{n+1} - x_n| \\le \\text{tol}$, the sequence is deemed to have converged. The process terminates, and $x_{n+1}$ is taken as the approximation to $x^{\\ast}$.\n   c. If the condition is not met, continue to the next iteration.\n3. If the loop completes without the stopping condition being met, the method has failed to converge within the given iteration budget. The final iterate $x_{\\text{max\\_iter}}$ is reported.\n\n### 4. A Posteriori Error Bound\n\nAn essential part of numerical computation is to estimate the error of the approximation. The a posteriori error bound provides an estimate for the distance from the current iterate to the true fixed point, $|x_{n+1} - x^{\\ast}|$, using only quantities available from the iteration itself.\n\nThe derivation is as follows, for iterates within an interval $I$ where $g$ is a contraction with constant $L$:\nThe fixed point property is $x^{\\ast} = g(x^{\\ast})$. The distance to the fixed point is:\n$$|x_{n+1} - x^{\\ast}| = |g(x_n) - g(x^{\\ast})|$$\nBy the contraction property, we have:\n$$|g(x_n) - g(x^{\\ast})| \\le L |x_n - x^{\\ast}|$$\nSo, $|x_{n+1} - x^{\\ast}| \\le L |x_n - x^{\\ast}|$. To relate this to known quantities, we use the triangle inequality on $|x_n - x^{\\ast}|$:\n$$|x_n - x^{\\ast}| = |x_n - x_{n+1} + x_{n+1} - x^{\\ast}| \\le |x_n - x_{n+1}| + |x_{n+1} - x^{\\ast}|$$\nSubstituting this back into the inequality for $|x_{n+1} - x^{\\ast}|$:\n$$|x_{n+1} - x^{\\ast}| \\le L (|x_n - x_{n+1}| + |x_{n+1} - x^{\\ast}|)$$\nRearranging the terms to isolate $|x_{n+1} - x^{\\ast}|$:\n$$|x_{n+1} - x^{\\ast}| - L|x_{n+1} - x^{\\ast}| \\le L|x_n - x_{n+1}|$$\n$$(1 - L)|x_{n+1} - x^{\\ast}| \\le L|x_{n+1} - x_n|$$\nSince $L < 1$, $(1-L)$ is positive, so we can divide by it without changing the inequality's direction:\n$$|x_{n+1} - x^{\\ast}| \\le \\frac{L}{1 - L} |x_{n+1} - x_n|$$\nThis is the required a posteriori error bound. The implementation will compute this value at the final iteration step, using the specified constant $L = \\sin(1)$, regardless of whether convergence was achieved.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the fixed point problem x = cos(x) for a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (x0, tol, max_iter).\n    test_cases = [\n        (0.0, 1e-12, 100),  # Case A\n        (1.0, 1e-12, 100),  # Case B\n        (10.0, 1e-12, 100), # Case C\n        (0.5, 1e-16, 5),   # Case D\n    ]\n\n    # The Lipschitz constant for g(x) = cos(x) on the interval [-1, 1].\n    L = np.sin(1)\n    \n    # List to store the results for each test case.\n    results = []\n    \n    for x0, tol, max_iter in test_cases:\n        x_current = float(x0)\n        n_iters = 0\n        converged = False\n        diff = 0.0\n\n        for i in range(max_iter):\n            n_iters = i + 1\n            x_next = np.cos(x_current)\n            diff = np.abs(x_next - x_current)\n            \n            if diff <= tol:\n                converged = True\n                x_current = x_next\n                break\n            \n            x_current = x_next\n\n        # At this point, the loop has terminated.\n        # x_current holds the final approximation.\n        # n_iters holds the total number of iterations performed.\n        # converged indicates if the tolerance was met.\n        # diff holds the last computed |x_{n+1} - x_n|.\n        \n        # Calculate the a posteriori error bound\n        # The formula is |x_n+1 - x*| <= (L / (1 - L)) * |x_n+1 - x_n|\n        # This is computed at the last available iteration step.\n        bound = (L / (1.0 - L)) * diff\n\n        # Round the floating-point results to 12 decimal places as required.\n        x_approx_rounded = round(x_current, 12)\n        bound_rounded = round(bound, 12)\n        \n        # Assemble the result for this test case.\n        case_result = [x_approx_rounded, n_iters, converged, bound_rounded]\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The format is a string representation of a list of lists.\n    # Using str() on the list of lists naturally produces the desired format,\n    # including spaces after commas inside the sublist representations, which\n    # aligns with the visual layout of a standard Python list representation.\n    # Example: [[0.739..., 58, True, 0.0], [...]]\n    print(str(results))\n\nsolve()\n```", "id": "3259287"}, {"introduction": "The quadratic formula is a celebrated achievement of classical algebra, providing an exact analytical solution for the roots of any quadratic polynomial. However, an analytical solution's perfection can be deceptive when translated into finite-precision computation. This practice demonstrates how the direct implementation of this famous formula can lead to a drastic loss of accuracy due to \"catastrophic cancellation,\" a subtle but critical flaw in numerical calculations. By implementing a mathematically equivalent but numerically robust version of the formula, you will gain first-hand experience in identifying and mitigating numerical instability. [@problem_id:3259338]", "problem": "You are to write a complete, runnable program that contrasts the numerical evaluation of the analytical quadratic formula with a numerically stable method, in the regime where catastrophic cancellation occurs. Consider the quadratic polynomial $a x^2 + b x + c = 0$ with real coefficients $a$, $b$, and $c$. The analytical solution (quadratic formula) expresses the roots in terms of $a$, $b$, and $c$, but evaluating it in finite precision arithmetic can suffer catastrophic cancellation when $b^2 \\gg 4 a c$. The goal is to demonstrate, quantify, and mitigate this loss of significance.\n\nFundamental base and facts to use:\n- The quadratic polynomial $a x^2 + b x + c$ has formal analytical roots given by the quadratic formula.\n- In Institute of Electrical and Electronics Engineers (IEEE) 754 double-precision (64-bit binary floating-point), arithmetic obeys rounding to nearest with a machine epsilon on the order of $2.22 \\times 10^{-16}$, and the absolute rounding scale for a value of magnitude $|y|$ is $O(\\varepsilon |y|)$.\n- Vieta’s formulas: For the roots $x_1$ and $x_2$ of $a x^2 + b x + c = 0$, one has $x_1 + x_2 = -b/a$ and $x_1 x_2 = c/a$.\n\nTask specification:\n- For each test case, compute the numerically cancellation-prone small-magnitude root by choosing the sign that subtracts nearly equal numbers. Define $s = \\mathrm{sign}(b)$ where $s = +1$ if $b > 0$ and $s = -1$ if $b < 0$. The cancellation-prone root to be evaluated in floating point is\n$$\nx_{\\text{small,naive}} = \\frac{-b + s \\sqrt{b^2 - 4 a c}}{2 a}.\n$$\n- Derive and implement a numerically stable expression for the same small root by rationalizing the numerator, yielding\n$$\nx_{\\text{small,stable}} = \\frac{2 c}{-b - s \\sqrt{b^2 - 4 a c}}.\n$$\n- Obtain a high-precision reference value for the small root $x_{\\text{small,true}}$ using arbitrary-precision arithmetic (at least $60$ significant digits) to evaluate the exact analytical expression\n$$\nx_{\\text{small,true}} = \\frac{-b + s \\sqrt{b^2 - 4 a c}}{2 a}.\n$$\n- Quantify the numerical error by computing the relative error for each method as a decimal number:\n$$\n\\text{rel\\_err\\_naive} = \\frac{\\left|x_{\\text{small,naive}} - x_{\\text{small,true}}\\right|}{\\left|x_{\\text{small,true}}\\right|}, \\quad\n\\text{rel\\_err\\_stable} = \\frac{\\left|x_{\\text{small,stable}} - x_{\\text{small,true}}\\right|}{\\left|x_{\\text{small,true}}\\right|}.\n$$\n\nTest suite:\n- Case $1$: $a = 1$, $b = 10^{10}$, $c = 1$ (severe cancellation for $b > 0$).\n- Case $2$: $a = 1$, $b = -10^{10}$, $c = 1$ (severe cancellation for $b < 0$).\n- Case $3$: $a = 1$, $b = 2$, $c = 1$ (discriminant zero boundary, $b^2 = 4 a c$).\n- Case $4$: $a = 1$, $b = 100$, $c = 1$ (moderate values, $b^2 \\gg 4 a c$ but limited cancellation).\n\nOutput specification:\n- For each test case, compute $\\text{rel\\_err\\_naive}$ and $\\text{rel\\_err\\_stable}$, each rounded to $10$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[\\text{rel\\_err\\_naive}^{(1)}, \\text{rel\\_err\\_stable}^{(1)}, \\text{rel\\_err\\_naive}^{(2)}, \\text{rel\\_err\\_stable}^{(2)}, \\text{rel\\_err\\_naive}^{(3)}, \\text{rel\\_err\\_stable}^{(3)}, \\text{rel\\_err\\_naive}^{(4)}, \\text{rel\\_err\\_stable}^{(4)}]$ where the superscript indicates the test case index.\n\nAll answers are pure numbers and must be floats. No physical units are involved. Angles are not present. Percentages must not be used; express errors as decimal fractions. The program must be self-contained, require no input, and adhere to the specified execution environment.", "solution": "The user wants to analyze the numerical stability of two different formulas for computing the roots of a quadratic equation, $a x^2 + b x + c = 0$.\n\n### Step 1: Extract Givens\n- **Polynomial**: $a x^2 + b x + c = 0$ with real coefficients $a$, $b$, and $c$.\n- **Condition for Cancellation**: $b^2 \\gg 4 a c$.\n- **Definition of $s$**: $s = \\mathrm{sign}(b)$, where $s = +1$ if $b > 0$ and $s = -1$ if $b < 0$.\n- **Cancellation-Prone Formula (\"naive\")**: \n$$\nx_{\\text{small,naive}} = \\frac{-b + s \\sqrt{b^2 - 4 a c}}{2 a}\n$$\n- **Numerically Stable Formula**:\n$$\nx_{\\text{small,stable}} = \\frac{2 c}{-b - s \\sqrt{b^2 - 4 a c}}\n$$\n- **High-Precision \"True\" Value**: The naive formula evaluated using arbitrary-precision arithmetic (at least $60$ significant digits).\n$$\nx_{\\text{small,true}} = \\frac{-b + s \\sqrt{b^2 - 4 a c}}{2 a}\n$$\n- **Relative Error Formulas**:\n$$\n\\text{rel\\_err\\_naive} = \\frac{\\left|x_{\\text{small,naive}} - x_{\\text{small,true}}\\right|}{\\left|x_{\\text{small,true}}\\right|}\n$$\n$$\n\\text{rel\\_err\\_stable} = \\frac{\\left|x_{\\text{small,stable}} - x_{\\text{small,true}}\\right|}{\\left|x_{\\text{small,true}}\\right|}\n$$\n- **Test Cases**:\n    1.  $a = 1$, $b = 10^{10}$, $c = 1$\n    2.  $a = 1$, $b = -10^{10}$, $c = 1$\n    3.  $a = 1$, $b = 2$, $c = 1$\n    4.  $a = 1$, $b = 100$, $c = 1$\n- **Output Specification**: For each case, compute $\\text{rel\\_err\\_naive}$ and $\\text{rel\\_err\\_stable}$, rounded to $10$ decimal places. Output is a single-line string of a comma-separated list of these values.\n- **Arithmetic Context**: Standard calculations are to be performed in IEEE $754$ double-precision ($64$-bit) floating-point arithmetic.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is a classic example in numerical analysis, demonstrating the phenomenon of catastrophic cancellation in floating-point arithmetic. The analysis is based on established principles of numerical computing and error analysis. The formulas provided are correct. The \"stable\" formula is derived from the \"naive\" one by rationalizing the numerator, a standard technique.\n2.  **Well-Posed**: The problem is well-posed. For each set of coefficients $(a, b, c)$, there is a unique set of roots. The method for calculating the \"true\" value (using high-precision arithmetic) is clearly defined, providing a solid baseline for error comparison. The error metrics are standard and unambiguous.\n3.  **Objective**: The problem is stated using precise mathematical language and objective criteria. There are no subjective or opinion-based elements.\n4.  **Completeness**: All necessary data (coefficients for test cases), formulas, and definitions are provided. The required output format is explicitly specified.\n5.  **Consistency**: There are no contradictions in the problem statement. The use of the naive formula for the high-precision reference is consistent with the goal of evaluating the analytical expression without floating-point artifacts.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a standard, well-defined problem in numerical methods. I will now proceed with the solution.\n\n### Principle-Based Design\nThe solution will be implemented in Python, leveraging the `numpy` library for standard double-precision floating-point arithmetic and the `decimal` standard library for high-precision reference calculations.\n\n1.  **High-Precision Reference (`x_small,true`)**: The core of the validation process relies on a \"true\" value, against which the floating-point results are compared. Since an analytical solution can only be approximated in finite-precision arithmetic, we use a much higher precision to simulate the \"true\" value. The `decimal` module in Python allows setting arbitrary precision. For this problem, a precision of $100$ decimal digits is chosen, which is well above the requested minimum of $60$ and vastly exceeds the approximately $16$ digits of IEEE $754$ double precision. For each test case $(a, b, c)$, the coefficients are converted to `Decimal` objects. The formula for $x_{\\text{small,true}}$ is then evaluated entirely using `Decimal` arithmetic, including the square root, which is handled by the `Decimal.sqrt()` method. This minimizes rounding errors to a level that is negligible for the purpose of this analysis.\n\n2.  **Standard-Precision Computation (`x_small,naive`)**: The first method to be tested is the direct evaluation of the quadratic formula. The formula is specifically chosen to cause subtraction of nearly equal numbers when $b^2 \\gg 4ac$.\n    - The term $\\sqrt{b^2 - 4ac}$ can be approximated by a Taylor series: $|b|\\sqrt{1 - 4ac/b^2} \\approx |b|(1 - 2ac/b^2) = |b| - 2ac/|b|$.\n    - The numerator becomes $-b + s\\sqrt{b^2 - 4ac}$. Since $s = \\mathrm{sign}(b)$, this is equivalent to $-b + |b|\\sqrt{1 - 4ac/b^2}$.\n    - If $b > 0$, $s=+1$, and the numerator is approximately $-b + (b - 2ac/b) = -2ac/b$.\n    - If $b < 0$, $s=-1$, and the numerator is approximately $-b - ((-b) - 2ac/(-b)) = -b + b - 2ac/b = -2ac/b$. This is incorrect. Let's re-evaluate. If $b<0$, $|b|=-b$. The term is $-b - \\sqrt{b^2-4ac} \\approx -b - (-b-2ac/(-b)) = -b+b+2ac/b$. Ah, the problem states the cancellation-prone root is $-b + s\\sqrt{...}$. If $b<0, s=-1$, this is $-b - \\sqrt{...}$. This is the large root. The small root is $-b + \\sqrt{...}$. Let's follow the problem's formula as written: $-b + s\\sqrt{...}$.\n        - If $b>0, s=+1$: $-b+\\sqrt{b^2-4ac}$. This subtracts two nearly equal positive numbers. This is the small-magnitude root.\n        - If $b<0, s=-1$: $-b-\\sqrt{b^2-4ac}$. This adds two large-magnitude negative numbers. This is the large-magnitude root, and *not* subject to cancellation.\n    - My derivation of the small root was slightly off. The problem *defines* which root to compute. Let's strictly follow the definition: $x_{\\text{small,naive}} = \\frac{-b + s \\sqrt{b^2 - 4 a c}}{2 a}$.\n        - For $b > 0$, $s=+1$, this is $\\frac{-b + \\sqrt{b^2-4ac}}{2a}$. This is the small-magnitude root, and subject to cancellation.\n        - For $b < 0$, $s=-1$, this is $\\frac{-b - \\sqrt{b^2-4ac}}{2a}$. This is the large-magnitude root, and *not* subject to cancellation.\n    - Let's re-read the problem very carefully: \"compute the numerically cancellation-prone small-magnitude root by choosing the sign that subtracts nearly equal numbers\". It then gives the formula $x_{\\text{small,naive}} = \\frac{-b + s \\sqrt{b^2 - 4 a c}}{2 a}$. This formula is correct for the small root when $b>0$, but it gives the large root when $b<0$. There is an ambiguity or a subtle error in the problem statement's formal definition versus its textual description. A more robust definition for the cancellation-prone root would be $\\frac{-b - \\mathrm{sign}(b)\\sqrt{b^2-4ac}}{2a}$ (the one that is always small). However, I am required to follow the problem statement literally. The provided formula for $x_{\\text{small,naive}}$ *sometimes* calculates the small root and *sometimes* calculates the large root, depending on the sign of $b$. The critical part is that the numerator is of the form \"large - almost large\", which is where cancellation happens.\n    - For $b>0, s=+1$, the numerator is $-b + \\sqrt{b^2-4ac}$. This is a catastrophic subtraction.\n    - For $b<0, s=-1$, the numerator is $-b - \\sqrt{b^2-4ac}$. This should be $-b + (-1)\\sqrt{...}$. This is a subtraction of a negative from a positive, i.e., an addition. This suggests my analysis of the sign for $b<0$ is faulty. Let's re-verify. Let $b = -10^{10}$. $s = -1$. The numerator is $-(-10^{10}) + (-1)\\sqrt{(-10^{10})^2-4} = 10^{10} - \\sqrt{10^{20}-4}$. This is again a subtraction of two nearly equal numbers. The problem statement is indeed correct and self-consistent. The formula as given always computes the root whose calculation is unstable. Let's call this root $x_{unstable}$.\n    - This calculation is implemented using `numpy.float64` types to ensure standard behavior.\n\n3.  **Stable-Precision Computation (`x_small,stable`)**: The second formula is derived by multiplying the numerator and denominator of the naive formula by its conjugate, $(-b - s\\sqrt{b^2-4ac})$:\n$$\nx = \\frac{-b + s\\sqrt{D}}{2a} \\times \\frac{-b - s\\sqrt{D}}{-b - s\\sqrt{D}} = \\frac{(-b)^2 - (s\\sqrt{D})^2}{2a(-b - s\\sqrt{D})} = \\frac{b^2 - s^2 D}{2a(-b - s\\sqrt{D})}\n$$\nwhere $D = b^2 - 4ac$. Since $s^2 = (\\pm 1)^2 = 1$, this simplifies to:\n$$\nx = \\frac{b^2 - (b^2 - 4ac)}{2a(-b - s\\sqrt{D})} = \\frac{4ac}{2a(-b - s\\sqrt{D})} = \\frac{2c}{-b - s\\sqrt{D}}\n$$\nThis is the provided stable formula. The denominator involves the sum of two numbers with the same sign ($-b$ and $-s\\sqrt{D}$ have the same sign), thus avoiding catastrophic cancellation. This is also implemented using `numpy.float64`.\n\n4.  **Error Calculation**: The relative errors are computed for both the naive and stable methods with respect to the high-precision reference value. The absolute value of the difference between the computed value (e.g., $x_{\\text{small,naive}}$) and the true value ($x_{\\text{small,true}}$) is divided by the absolute value of the true value. The `Decimal` type is used for this calculation to maintain precision, and the final result is converted to a float and rounded to $10$ decimal places as specified.\n\nThe program will iterate through the provided test cases, performing these four steps for each, and will collate the resulting relative errors into a single list for the final formatted output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom decimal import Decimal, getcontext\n\ndef solve():\n    \"\"\"\n    Computes and compares the relative errors of a naive and a stable formula\n    for a quadratic root subject to catastrophic cancellation.\n    \"\"\"\n    \n    # Set precision for high-accuracy reference calculations.\n    # 60 digits are required, 100 provides a safe margin.\n    getcontext().prec = 100\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (a, b, c).\n    test_cases = [\n        (1, 10**10, 1),      # Case 1: Severe cancellation, b > 0\n        (1, -10**10, 1),     # Case 2: Severe cancellation, b < 0\n        (1, 2, 1),           # Case 3: Discriminant is zero\n        (1, 100, 1)          # Case 4: Moderate cancellation\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        a, b, c = case\n\n        # --- High-precision reference calculation (x_small_true) ---\n        a_d = Decimal(a)\n        b_d = Decimal(b)\n        c_d = Decimal(c)\n\n        s_d = Decimal(1) if b_d > 0 else Decimal(-1)\n        \n        discriminant_d = b_d**2 - 4 * a_d * c_d\n        \n        # Handle the case of zero discriminant separately for sqrt\n        # although Decimal.sqrt() handles it correctly. This is for clarity.\n        if discriminant_d < 0:\n            # Problem assumes real coefficients leading to real roots.\n            # This branch should not be taken by the test cases.\n            continue\n        \n        sqrt_discriminant_d = discriminant_d.sqrt()\n        \n        # x_small_true is computed using the high-precision Decimal objects\n        # following the 'naive' formula structure, as specified.\n        x_small_true = (-b_d + s_d * sqrt_discriminant_d) / (2 * a_d)\n\n        # --- Standard double-precision calculations (numpy.float64) ---\n        a_f = np.float64(a)\n        b_f = np.float64(b)\n        c_f = np.float64(c)\n\n        s_f = np.sign(b_f)\n        \n        discriminant_f = b_f**2 - 4 * a_f * c_f\n        sqrt_discriminant_f = np.sqrt(discriminant_f)\n        \n        # 1. Naive (cancellation-prone) formula\n        x_small_naive = (-b_f + s_f * sqrt_discriminant_f) / (2 * a_f)\n        \n        # 2. Stable (rationalized) formula\n        denominator_stable = -b_f - s_f * sqrt_discriminant_f\n        # Avoid division by zero if discriminant is zero and b is also zero,\n        # which is not the case in the test suite.\n        if denominator_stable == 0:\n            # This can happen if b^2=4ac and b=0, meaning a=0 or c=0 with b=0.\n            # In our case 3, b=2 so denominator is -2-2=-4.\n            # If a=0, it's not a quadratic. So this case is unlikely in valid problems.\n            # Based on the formula, if D=0, root is -b/2a. stable formula becomes 2c/-b.\n            # Vieta's formulas: x1*x2 = c/a and x1+x2 = -b/a.\n            # If D=0, x1=x2=-b/2a. So (-b/2a)^2 = c/a => b^2/4a^2 = c/a => b^2 = 4ac.\n            # And x1*x1 = (2c/-b)*(2c/-b) = 4c^2/b^2 = 4c^2/(4ac) = c/a. Correct.\n            x_small_stable = -b_f / (2 * a_f)\n        else:\n            x_small_stable = (2 * c_f) / denominator_stable\n\n        # --- Relative Error Calculation ---\n        # Convert float results to Decimal for accurate error computation\n        x_naive_d = Decimal(x_small_naive)\n        x_stable_d = Decimal(x_small_stable)\n        \n        # Avoid division by zero for relative error if true value is zero.\n        # None of the test cases produce a true root of zero.\n        if x_small_true == 0:\n            rel_err_naive = Decimal(0) if x_naive_d == 0 else Decimal('inf')\n            rel_err_stable = Decimal(0) if x_stable_d == 0 else Decimal('inf')\n        else:\n            rel_err_naive = abs((x_naive_d - x_small_true) / x_small_true)\n            rel_err_stable = abs((x_stable_d - x_small_true) / x_small_true)\n            \n        # Round the final error values to 10 decimal places as specified\n        rounded_err_naive = round(float(rel_err_naive), 10)\n        rounded_err_stable = round(float(rel_err_stable), 10)\n        \n        results.append(rounded_err_naive)\n        results.append(rounded_err_stable)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3259338"}, {"introduction": "Approximating a function from a discrete set of data points is a cornerstone of data analysis and scientific modeling. A natural analytical approach is to construct a single polynomial of high degree that passes through every point exactly. This exercise explores the surprising pitfalls of that strategy, a phenomenon first observed by Carl Runge, where such polynomials can oscillate wildly and deviate significantly from the true function between the nodes. By contrasting this behavior with the stability of a piecewise cubic spline, you will learn why local, numerically-constructed approximations are often superior to a single, global analytical fit. [@problem_id:3259327]", "problem": "You will investigate the trade-off between a single global high-degree polynomial interpolant and a piecewise cubic spline interpolant for data sampled from a smooth function on a closed interval, focusing on when global interpolation exhibits large oscillations near the interval endpoints (Runge's phenomenon) but a numerically constructed spline remains well-behaved.\n\nFundamental base to use:\n- Definition of the unique polynomial interpolant: for $n$ distinct nodes $\\{x_i\\}_{i=0}^{n-1}$ and data $\\{y_i\\}_{i=0}^{n-1}$, there exists a unique polynomial $p$ of degree at most $n-1$ such that $p(x_i)=y_i$ for all $i$.\n- Definition of a natural cubic spline: a twice continuously differentiable piecewise cubic function $S$ on an interval $[a,b]$ that interpolates given data at nodes, has continuous first and second derivatives at interior nodes, and satisfies natural boundary conditions $S''(a)=0$ and $S''(b)=0$.\n- Runge's observation: for the analytic function $f(x)=\\frac{1}{1+25x^2}$ on the interval $[-1,1]$, the error of the degree-$(n-1)$ polynomial interpolant at equally spaced nodes can grow near the endpoints as $n$ increases, while a cubic spline interpolant typically remains stable.\n\nTask:\n- Consider the function $f(x)=\\dfrac{1}{1+25x^2}$ on the interval $\\left[-1,1\\right]$.\n- For each specified number of equally spaced nodes $n$, construct:\n  1. The unique degree-$(n-1)$ interpolating polynomial $p_n$ that exactly matches the sampled data $\\left\\{(x_i,f(x_i))\\right\\}_{i=0}^{n-1}$ at equally spaced nodes $x_i=-1+\\dfrac{2i}{n-1}$ for $i=0,1,\\dots,n-1$.\n  2. The natural cubic spline $S_n$ that interpolates the same data with $S_n''(-1)=0$ and $S_n''(1)=0$.\n- On a uniform evaluation grid with $m=1001$ points $\\{t_j\\}_{j=0}^{m-1}$ in $\\left[-1,1\\right]$, compute the following for each $n$:\n  - The maximum absolute error of the polynomial interpolant, $E_p(n)=\\max_{0\\le j<m}\\left|p_n(t_j)-f(t_j)\\right|$.\n  - The maximum absolute error of the spline interpolant, $E_s(n)=\\max_{0\\le j<m}\\left|S_n(t_j)-f(t_j)\\right|$.\n  - A boolean indicator $B(n)$ which is true if and only if $E_p(n)>E_s(n)$.\n\nImplementation requirements:\n- The polynomial interpolant must be evaluated from first principles, starting from the interpolation definition. You may use any mathematically equivalent formulation that does not call a black-box interpolation library. For instance, you may use the Lagrange form, the barycentric form, or the Newton divided-difference form.\n- The cubic spline must be implemented from its defining conditions: piecewise cubic polynomials with $C^2$ continuity at interior nodes and natural boundary conditions. Set up and solve the tridiagonal linear system that determines the second derivatives at the nodes from the continuity conditions and the data, then evaluate the spline on each subinterval. Do not call any black-box spline library.\n- Use floating point arithmetic. No physical units are involved in this problem. Angles are not used.\n\nTest suite:\n- Use the equally spaced node counts $n\\in\\{5,11,21\\}$ on the interval $\\left[-1,1\\right]$.\n- Use the uniform evaluation grid size $m=1001$.\n\nRequired final output format:\n- The program must output a single line containing a list with one entry per test case, in the order $n=5$, $n=11$, $n=21$.\n- Each entry must be a list of three items in the order $[E_p(n),E_s(n),B(n)]$, where $E_p(n)$ and $E_s(n)$ are floats rounded to eight decimal places, and $B(n)$ is a boolean written as either True or False.\n- There must be no spaces in the output. For example, a valid output format is:\n  - $[[0.01234567,0.00123456,True],[0.00012345,0.00001234,True],[0.12345678,0.00123456,True]]$", "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard exercise in numerical analysis to compare the performance of global polynomial interpolation against piecewise spline interpolation.\n\nThe core of the problem lies in constructing and evaluating two different types of interpolants for data sampled from the function $f(x) = \\dfrac{1}{1+25x^2}$ on the interval $[-1, 1]$. For a given number of nodes $n$, the interpolation points are $\\{(x_i, y_i)\\}_{i=0}^{n-1}$, where the nodes $x_i$ are equally spaced and $y_i = f(x_i)$.\n\nThe nodes are defined as $x_i = -1 + \\frac{2i}{n-1}$ for $i=0, 1, \\dots, n-1$.\n\n### 1. Global Polynomial Interpolant ($p_n$)\n\nFor a set of $n$ distinct data points, there exists a unique interpolating polynomial $p_n(x)$ of degree at most $n-1$ such that $p_n(x_i) = y_i$ for all $i$. While several formulations exist (e.g., Lagrange, Newton), the barycentric form is chosen for its superior numerical stability and computational efficiency, especially when evaluating the polynomial at multiple points.\n\nThe first barycentric interpolation formula is given by:\n$$\np_n(x) = \\frac{\\sum_{i=0}^{n-1} \\frac{w_i}{x-x_i} y_i}{\\sum_{i=0}^{n-1} \\frac{w_i}{x-x_i}}\n$$\nThis formula is valid for any $x$ that is not a node. If $x=x_k$ for some node $k$, the value is simply $p_n(x_k)=y_k$.\n\nThe barycentric weights $w_i$ are defined as $w_i = \\prod_{j \\neq i} \\frac{1}{x_i - x_j}$. For the specific case of equally spaced nodes, these weights simplify significantly. Up to a constant factor that cancels in the formula, the weights can be computed as:\n$$\nw_i = (-1)^i \\binom{n-1}{i}\n$$\nThese weights are computed once for a given set of nodes. The polynomial is then evaluated on a fine grid of points $\\{t_j\\}$ to determine its shape and error.\n\n### 2. Natural Cubic Spline Interpolant ($S_n$)\n\nA cubic spline $S_n(x)$ is a piecewise cubic function that interpolates the data points $\\{(x_i, y_i)\\}$. It is defined such that the function itself, its first derivative, and its second derivative are continuous across all interior nodes. This property is known as $C^2$ continuity.\n\nThe construction of the spline involves determining the second derivatives at each node, denoted $M_i = S_n''(x_i)$. The continuity of the first derivative $S_n'(x)$ at each interior node $x_i$ (for $i=1, \\dots, n-2$) provides a system of linear equations relating the unknown values of $M_i$. For equally spaced nodes with step size $h = x_{i+1} - x_i$, this relationship is:\n$$\nM_{i-1} + 4 M_i + M_{i+1} = \\frac{6}{h^2}(y_{i+1} - 2y_i + y_{i-1})\n$$\nThis provides $n-2$ equations for the $n$ unknowns $\\{M_0, M_1, \\dots, M_{n-1}\\}$. Two additional constraints are required, which are provided by the boundary conditions. The problem specifies a \"natural\" cubic spline, which implies zero second derivatives at the endpoints of the interval:\n$$\nS_n''(-1) = M_0 = 0 \\quad \\text{and} \\quad S_n''(1) = M_{n-1} = 0\n$$\nSubstituting these into the system leaves a solvable $(n-2) \\times (n-2)$ tridiagonal system of linear equations for the interior second derivatives $\\{M_1, \\dots, M_{n-2}\\}$. This system is strictly diagonally dominant and thus has a unique solution, which can be found using a standard linear solver.\n\nOnce all second derivatives $M_i$ are known, the spline can be evaluated at any point $t$ within a subinterval $[x_i, x_{i+1}]$ using the following formula:\n$$\nS_n(t) = \\frac{M_i}{6h}(x_{i+1}-t)^3 + \\frac{M_{i+1}}{6h}(t-x_i)^3 + \\left(\\frac{y_i}{h} - \\frac{M_i h}{6}\\right)(x_{i+1}-t) + \\left(\\frac{y_{i+1}}{h} - \\frac{M_{i+1} h}{6}\\right)(t-x_i)\n$$\n\n### 3. Error Analysis\n\nTo compare the two interpolation methods, both the polynomial $p_n(x)$ and the spline $S_n(x)$ are evaluated on a fine uniform grid of $m=1001$ points $\\{t_j\\}_{j=0}^{m-1}$ in the interval $[-1, 1]$. The maximum absolute error is then computed for each interpolant against the true function $f(x)$:\n\n- Polynomial error: $E_p(n) = \\max_{0 \\le j < m} |p_n(t_j) - f(t_j)|$\n- Spline error: $E_s(n) = \\max_{0 \\le j < m} |S_n(t_j) - f(t_j)|$\n\nA boolean indicator, $B(n)$, is then determined such that $B(n)$ is true if and only if $E_p(n) > E_s(n)$. This analysis is performed for each value of $n$ in the test suite $\\{5, 11, 21\\}$, demonstrating the trade-offs between the two methods. The results are expected to show that as $n$ increases, the error of the global polynomial interpolant with equally spaced nodes grows significantly near the endpoints (Runge's phenomenon), while the cubic spline error remains small and decreases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef runge_function(x):\n    \"\"\"The function f(x) = 1 / (1 + 25x^2).\"\"\"\n    return 1.0 / (1.0 + 25.0 * x**2)\n\ndef barycentric_poly_interp(nodes_x, nodes_y, eval_t):\n    \"\"\"\n    Evaluates the interpolating polynomial using the barycentric formula.\n    This implementation is for equally spaced nodes.\n    \"\"\"\n    n = len(nodes_x)\n    \n    # Pre-compute barycentric weights for equally spaced nodes.\n    # w_i = (-1)^i * C(n-1, i)\n    weights = np.zeros(n)\n    for i in range(n):\n        weights[i] = ((-1)**i) * math.comb(n - 1, i)\n\n    p_vals = np.zeros_like(eval_t, dtype=np.float64)\n\n    for j, t in enumerate(eval_t):\n        # Check if t is one of the nodes (within a small tolerance)\n        # Using np.isclose is robust for floating point comparison.\n        is_node = np.isclose(t, nodes_x)\n        if np.any(is_node):\n            p_vals[j] = nodes_y[np.where(is_node)[0][0]]\n        else:\n            # Barycentric formula\n            terms = weights / (t - nodes_x)\n            numerator = np.sum(terms * nodes_y)\n            denominator = np.sum(terms)\n            p_vals[j] = numerator / denominator\n            \n    return p_vals\n\ndef natural_cubic_spline_interp(nodes_x, nodes_y, eval_t):\n    \"\"\"\n    Constructs and evaluates a natural cubic spline interpolant.\n    \"\"\"\n    n = len(nodes_x)\n    h = nodes_x[1] - nodes_x[0]  # Assuming equally spaced nodes\n\n    # Set up the tridiagonal system for the second derivatives M_i = S''(x_i)\n    # The system is A*M_interior = b for interior M_i (i=1..n-2)\n    # Natural boundary conditions: M_0 = 0, M_{n-1} = 0\n    \n    num_interior_nodes = n - 2\n    if num_interior_nodes <= 0: # M vector is just [0, 0] for n=2.\n        M = np.zeros(n)\n    else:\n        # A is a (n-2)x(n-2) tridiagonal matrix with 4 on diag, 1 on off-diags\n        A = np.diag(np.full(num_interior_nodes, 4.0)) + \\\n            np.diag(np.ones(num_interior_nodes - 1), 1) + \\\n            np.diag(np.ones(num_interior_nodes - 1), -1)\n\n        # b is the right-hand side vector\n        # b_i = (6/h^2) * (y_{i+2} - 2y_{i+1} + y_i)\n        # Indices are shifted because we solve for M_1 to M_{n-2}.\n        y = nodes_y\n        b = (6.0 / h**2) * (y[0:n-2] - 2.0 * y[1:n-1] + y[2:n])\n        \n        # Solve for the interior second derivatives\n        M_interior = np.linalg.solve(A, b)\n        \n        # Form the full vector of second derivatives\n        M = np.concatenate(([0.0], M_interior, [0.0]))\n\n    # Evaluate the spline at points in eval_t\n    # Find the interval index for each evaluation point\n    indices = np.searchsorted(nodes_x, eval_t, side='right')\n    indices = np.clip(indices, 1, n-1) # Ensure indices are valid\n    indices = indices - 1\n    \n    # Extract relevant values for vectorized calculation\n    x_i = nodes_x[indices]\n    x_i1 = nodes_x[indices + 1]\n    y_i = nodes_y[indices]\n    y_i1 = nodes_y[indices + 1]\n    M_i = M[indices]\n    M_i1 = M[indices + 1]\n    \n    # Terms in the spline evaluation formula\n    term_A = x_i1 - eval_t\n    term_B = eval_t - x_i\n    \n    # Spline evaluation formula S(t) on [x_i, x_{i+1}]\n    s_vals = (M_i / (6.0 * h)) * term_A**3 + \\\n             (M_i1 / (6.0 * h)) * term_B**3 + \\\n             (y_i / h - M_i * h / 6.0) * term_A + \\\n             (y_i1 / h - M_i1 * h / 6.0) * term_B\n\n    return s_vals\n\ndef solve():\n    \"\"\"\n    Main solver function that orchestrates the interpolation and error analysis.\n    \"\"\"\n    test_cases = [5, 11, 21]\n    m = 1001\n    interval = [-1.0, 1.0]\n\n    results = []\n    \n    # Uniform evaluation grid\n    eval_t = np.linspace(interval[0], interval[1], m)\n    true_f_vals = runge_function(eval_t)\n\n    for n in test_cases:\n        # Generate equally spaced nodes and corresponding function values\n        nodes_x = np.linspace(interval[0], interval[1], n)\n        nodes_y = runge_function(nodes_x)\n        \n        # 1. Polynomial Interpolation\n        p_vals = barycentric_poly_interp(nodes_x, nodes_y, eval_t)\n        E_p = np.max(np.abs(p_vals - true_f_vals))\n        \n        # 2. Cubic Spline Interpolation\n        s_vals = natural_cubic_spline_interp(nodes_x, nodes_y, eval_t)\n        E_s = np.max(np.abs(s_vals - true_f_vals))\n        \n        # 3. Boolean Indicator\n        B = E_p > E_s\n        \n        results.append([E_p, E_s, B])\n\n    # Format the final output string exactly as required\n    result_strings = []\n    for res in results:\n        e_p, e_s, b = res\n        # Format floats to 8 decimal places, bool as True/False string\n        result_strings.append(f\"[{e_p:.8f},{e_s:.8f},{str(b)}]\")\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3259327"}]}