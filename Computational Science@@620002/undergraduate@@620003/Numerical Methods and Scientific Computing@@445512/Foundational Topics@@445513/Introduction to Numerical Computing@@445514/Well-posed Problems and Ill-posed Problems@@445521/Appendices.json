{"hands_on_practices": [{"introduction": "Many problems in science and engineering boil down to solving a linear system of equations, $Ax=b$. This exercise demonstrates a core challenge in numerical analysis: ill-conditioning, where the solution's stability is compromised. Through a carefully chosen $2 \\times 2$ matrix, you will see firsthand how a minuscule perturbation in the data vector $b$ can cause a massive change in the solution vector $x$, a classic example of an ill-posed problem [@problem_id:2197153].", "problem": "In many scientific and engineering applications, we encounter systems of linear equations of the form $Ax = b$, where we need to find the vector $x$ given a matrix $A$ and a vector $b$. The vector $b$ often represents measurements, which are subject to small errors. An ill-conditioned system is one where small errors in $b$ can lead to large errors in the solution $x$.\n\nConsider the following ill-conditioned linear system:\n$$\nA = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1.001 \\end{pmatrix}\n$$\nLet's first consider the case where the measurement vector is exactly:\n$$\nb_{orig} = \\begin{pmatrix} 2 \\\\ 2.001 \\end{pmatrix}\n$$\nAnd the corresponding solution vector is $x_{orig}$, satisfying $A x_{orig} = b_{orig}$.\n\nNow, suppose a small measurement error occurs, leading to a new, perturbed measurement vector:\n$$\nb_{pert} = \\begin{pmatrix} 2 \\\\ 2.002 \\end{pmatrix}\n$$\nThe new solution vector is $x_{pert}$, which satisfies $A x_{pert} = b_{pert}$.\n\nYour task is to quantify the effect of this small perturbation in the measurement vector. Calculate the magnitude of the change in the solution vector, which is given by the Euclidean norm of the difference, $\\| x_{pert} - x_{orig} \\|_2$. Round your final answer to three significant figures.", "solution": "We solve both linear systems using $x = A^{-1} b$. For a $2 \\times 2$ matrix $A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$, the inverse is $A^{-1} = \\frac{1}{ad - bc} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}$. Here,\n$$\nA = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1.001 \\end{pmatrix},\n\\quad \\det(A) = 1 \\cdot 1.001 - 1 \\cdot 1 = 0.001.\n$$\nThus,\n$$\nA^{-1} = \\frac{1}{0.001} \\begin{pmatrix} 1.001 & -1 \\\\ -1 & 1 \\end{pmatrix} = 1000 \\begin{pmatrix} 1.001 & -1 \\\\ -1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1001 & -1000 \\\\ -1000 & 1000 \\end{pmatrix}.\n$$\nFor $b_{orig} = \\begin{pmatrix} 2 \\\\ 2.001 \\end{pmatrix}$,\n$$\nx_{orig} = A^{-1} b_{orig} = \\begin{pmatrix} 1001 & -1000 \\\\ -1000 & 1000 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 2.001 \\end{pmatrix}\n= \\begin{pmatrix} 1001 \\cdot 2 - 1000 \\cdot 2.001 \\\\ -1000 \\cdot 2 + 1000 \\cdot 2.001 \\end{pmatrix}\n= \\begin{pmatrix} 2002 - 2001 \\\\ -2000 + 2001 \\end{pmatrix}\n= \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\nFor $b_{pert} = \\begin{pmatrix} 2 \\\\ 2.002 \\end{pmatrix}$,\n$$\nx_{pert} = A^{-1} b_{pert} = \\begin{pmatrix} 1001 & -1000 \\\\ -1000 & 1000 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 2.002 \\end{pmatrix}\n= \\begin{pmatrix} 1001 \\cdot 2 - 1000 \\cdot 2.002 \\\\ -1000 \\cdot 2 + 1000 \\cdot 2.002 \\end{pmatrix}\n= \\begin{pmatrix} 2002 - 2002 \\\\ -2000 + 2002 \\end{pmatrix}\n= \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}.\n$$\nThe change in the solution is\n$$\nx_{pert} - x_{orig} = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix},\n$$\nand its Euclidean norm is\n$$\n\\|x_{pert} - x_{orig}\\|_{2} = \\sqrt{(-1)^{2} + 1^{2}} = \\sqrt{2}.\n$$\nRounding $\\sqrt{2}$ to three significant figures gives $1.41$.", "answer": "$$\\boxed{1.41}$$", "id": "2197153"}, {"introduction": "The concept of well-posedness is more nuanced than it might first appear, often depending on how we define the space of possible inputs. This problem delves into this subtlety by examining a linear system that is not solvable for arbitrary data, but becomes well-posed when the inputs are restricted to a specific subspace [@problem_id:3286696]. This exercise challenges you to apply the criteria of existence and uniqueness with precision, revealing that a problem's classification can change by carefully defining its domain.", "problem": "Consider a linear system with a side constraint. Let $A \\in \\mathbb{R}^{n \\times n}$ have rank $n - 1$. Let $\\mathcal{N}(A)$ and $\\mathcal{N}(A^\\top)$ denote the nullspaces of $A$ and $A^\\top$, respectively, and suppose $\\mathcal{N}(A) = \\operatorname{span}\\{v\\}$ and $\\mathcal{N}(A^\\top) = \\operatorname{span}\\{w\\}$, where $v, w \\in \\mathbb{R}^n$ are nonzero vectors. Consider the constrained problem: find $x \\in \\mathbb{R}^n$ such that $A x = b$ and $x^\\top v = 0$. It is known that a solution can exist only if $b^\\top w = 0$, and that under the side constraint $x^\\top v = 0$, any solution (if it exists) is unique.\n\nIn the sense of well-posedness introduced by Jacques Hadamard, which requires existence, uniqueness, and continuous dependence on the input data, select all correct statements about this problem, taking care to specify the data space being considered.\n\nA. If the data space is the full $\\mathbb{R}^n$, the problem is ill-posed, because arbitrarily small perturbations of $b$ can destroy the compatibility condition $b^\\top w = 0$, causing the problem to lose existence.\n\nB. If the data space is restricted to $\\operatorname{range}(A) = \\{A y : y \\in \\mathbb{R}^n\\}$, the problem is well-posed: for every $b \\in \\operatorname{range}(A)$ there is a unique solution satisfying $x^\\top v = 0$, and the solution depends continuously on $b$.\n\nC. The problem is well-posed regardless of the data space, because uniqueness holds under the side constraint $x^\\top v = 0$.\n\nD. Even when the data space is restricted to $\\operatorname{range}(A)$, the problem is ill-posed, because the inverse mapping from $b$ to $x$ is unbounded in finite-dimensional spaces.", "solution": "We analyze well-posedness in the sense of Jacques Hadamard: a problem is well-posed if (i) a solution exists for all admissible data, (ii) the solution is unique, and (iii) the solution depends continuously on the data within the specified data space.\n\nWe start from fundamental linear algebra facts. If $A \\in \\mathbb{R}^{n \\times n}$ has rank $n - 1$, then $\\dim \\mathcal{N}(A) = 1$ and $\\dim \\mathcal{N}(A^\\top) = 1$. The range of $A$ is a subspace of codimension $1$, and can be characterized as\n$$\n\\operatorname{range}(A) = \\{b \\in \\mathbb{R}^n : b^\\top w = 0\\},\n$$\nwhere $w$ spans $\\mathcal{N}(A^\\top)$, and the compatibility condition $b^\\top w = 0$ is necessary and sufficient for solvability of $A x = b$.\n\nWe consider the constrained problem: find $x \\in \\mathbb{R}^n$ such that $A x = b$ and $x^\\top v = 0$, where $v$ spans $\\mathcal{N}(A)$. Define the subspace\n$$\nS := \\{x \\in \\mathbb{R}^n : x^\\top v = 0\\}.\n$$\nWe analyze the mapping $T : \\operatorname{range}(A) \\to S$ which assigns to each compatible $b$ the unique constrained solution $x$.\n\nExistence and uniqueness on the restricted domain. Take $b \\in \\operatorname{range}(A)$, so there exists some $x_0 \\in \\mathbb{R}^n$ with $A x_0 = b$. Because $v \\in \\mathcal{N}(A)$, for any $\\alpha \\in \\mathbb{R}$ we have $A(x_0 + \\alpha v) = b$. Choose\n$$\n\\alpha^\\star := -\\frac{x_0^\\top v}{v^\\top v},\n$$\nso that $(x_0 + \\alpha^\\star v)^\\top v = 0$. Hence, there exists a solution in $S$. For uniqueness, suppose $x_1, x_2 \\in S$ both satisfy $A x_1 = A x_2 = b$. Then $A(x_1 - x_2) = 0$, so $x_1 - x_2 \\in \\mathcal{N}(A) = \\operatorname{span}\\{v\\}$. Write $x_1 - x_2 = \\beta v$ for some $\\beta \\in \\mathbb{R}$. Because both $x_1$ and $x_2$ lie in $S$, we have $(x_1 - x_2)^\\top v = 0$, hence $\\beta \\, v^\\top v = 0$, implying $\\beta = 0$ and $x_1 = x_2$. Thus $A$ restricted to $S$ is injective and surjective onto $\\operatorname{range}(A)$, so $T$ is a bijection.\n\nContinuity on the restricted domain. In finite-dimensional normed spaces, any linear bijection has a continuous inverse. More concretely, because $A$ is linear and $A|_S$ has trivial kernel, the function $f : S \\to \\mathbb{R}$ defined by $f(x) = \\|A x\\|$ has $f(x) > 0$ for all $x \\in S \\setminus \\{0\\}$. The unit sphere in $S$,\n$$\n\\{x \\in S : \\|x\\| = 1\\},\n$$\nis compact, and $f$ is continuous, so\n$$\nm := \\min_{\\substack{x \\in S \\\\ \\|x\\| = 1}} \\|A x\\| > 0.\n$$\nTherefore, for all $x \\in S$,\n$$\n\\|A x\\| \\ge m \\|x\\|,\n$$\nwhich yields the bound\n$$\n\\|x\\| \\le \\frac{1}{m} \\|A x\\|.\n$$\nHence the inverse mapping $T^{-1} : S \\to \\operatorname{range}(A)$ is continuous, and the solution mapping $T : \\operatorname{range}(A) \\to S$ is continuous as well. Consequently, on the restricted data space $\\operatorname{range}(A)$, the problem is well-posed.\n\nIll-posedness on the full space. If the data space is the full $\\mathbb{R}^n$, existence fails for inputs $b$ with $b^\\top w \\ne 0$. Moreover, the problem is not robust to perturbations: given any $b \\in \\operatorname{range}(A)$, for any $\\varepsilon > 0$ there exists $\\delta b \\in \\mathbb{R}^n$ with $\\|\\delta b\\| < \\varepsilon$ and $(b + \\delta b)^\\top w \\ne 0$, so the perturbed system has no solution. In the Hadamard sense, if the data space is $\\mathbb{R}^n$, the problem fails the existence requirement and also fails continuous dependence because arbitrarily small perturbations can lead to non-existence.\n\nOption-by-option analysis:\n\nA. If the data space is the full $\\mathbb{R}^n$, the compatibility condition $b^\\top w = 0$ is not guaranteed. Existence fails for many $b$, and arbitrarily small perturbations can break compatibility, making the problem unsolvable. This violates Hadamardâ€™s criteria. Verdict: Correct.\n\nB. Restricting the data space to $\\operatorname{range}(A)$ ensures $b^\\top w = 0$ for all admissible $b$. We proved existence and uniqueness under the side constraint $x^\\top v = 0$, and we showed the solution depends continuously on $b$ via the bounded inverse argument on $S$. Verdict: Correct.\n\nC. Uniqueness alone does not guarantee well-posedness; existence must hold for all admissible data and continuity must be satisfied. On the full $\\mathbb{R}^n$ data space, existence fails for many $b$. Verdict: Incorrect.\n\nD. In finite-dimensional spaces, a linear bijection has a bounded inverse. We proved $A|_S$ is a bijection from $S$ to $\\operatorname{range}(A)$, hence its inverse is bounded and the solution mapping is continuous. Verdict: Incorrect.", "answer": "$$\\boxed{AB}$$", "id": "3286696"}, {"introduction": "While identifying an ill-posed problem is crucial, the ultimate goal is often to find a way to obtain a meaningful solution. This hands-on exercise introduces regularization, a fundamental strategy for stabilizing ill-posed inverse problems like numerical differentiation from noisy data [@problem_id:3286744]. By deriving and implementing an optimal smoothing width, you will explore the essential trade-off between bias and variance and learn how to transform an unstable problem into a solvable one.", "problem": "You are given the task of estimating the derivative $f'(x_0)$ from uniformly sampled and noisy observations $y_i = f(x_i) + \\epsilon_i$ on a grid $x_i = x_0 + i \\,\\Delta x$, where the noise $\\epsilon_i$ are independent and identically distributed with zero mean and variance $\\sigma^2$. Using the definitions of well-posedness due to Jacques Hadamard (existence, uniqueness, and continuous dependence on data), frame numerical differentiation as an inverse problem and show why it is ill-posed by demonstrating that high-frequency noise is amplified by differentiation. Then, to stabilize the estimation, consider the following two-step linear estimator for $f'(x_0)$:\n- First smooth the data with a symmetric boxcar (uniform) kernel of width $h$ (so the smoothed function is $f * K_h$, with $K_h$ even and normalized).\n- Then apply the central difference formula to the smoothed signal with step $\\Delta x$.\n\nStarting only from the following foundational facts:\n- The derivative of a convolution satisfies $(f * K_h)' = f' * K_h$.\n- A symmetric, normalized boxcar kernel has second moment $\\mu_2(K_h) = h^2/12$.\n- A moving average of $m \\approx h/\\Delta x$ independent samples reduces variance by a factor of $m$, and differencing two independent samples adds variances.\nderive the leading-order mean squared error at a point,\n$$\n\\operatorname{MSE}(h) \\approx \\text{bias}^2(h) + \\text{var}(h),\n$$\nwhere the bias is governed by the local third derivative $f^{(3)}(x_0)$ and the variance is governed by $\\sigma^2$, $\\Delta x$, and $h$. Show that minimizing this asymptotic expression leads to an optimal width\n$$\nh_\\star \\;=\\; \\left(\\frac{72\\,\\sigma^2}{\\Delta x\\,\\big(f^{(3)}(x_0)\\big)^2}\\right)^{1/5}.\n$$\nIn practice, $h$ cannot be smaller than $\\Delta x$ and should not exceed a user-chosen maximum $H_{\\max}$ determined by domain or trend-scale considerations. Also, when $\\lvert f^{(3)}(x_0)\\rvert$ is extremely small, the bias is negligible and the optimal width should saturate at $H_{\\max}$.\n\nYour program must implement the mapping\n$$\n(\\Delta x,\\;\\sigma,\\;H_{\\max},\\;x_0,\\;f)\\;\\longmapsto\\; h,\n$$\nusing the rule:\n- If $\\lvert f^{(3)}(x_0)\\rvert \\le \\tau$ for a small threshold $\\tau$, set $h = H_{\\max}$.\n- Otherwise compute $h_\\star = \\left(\\frac{72\\,\\sigma^2}{\\Delta x\\,[f^{(3)}(x_0)]^2}\\right)^{1/5}$ and clip to $[\\Delta x,\\;H_{\\max}]$.\n\nAngles are to be interpreted in radians. There are no physical units in this problem. For numerical reporting, express each $h$ as a decimal rounded to six digits after the decimal point.\n\nTest suite. For each case below, compute $h$ using the rule above. Each function $f$ is smooth and its third derivative $f^{(3)}$ is provided analytically:\n- Case A: $f(x) = \\sin(x)$, so $f^{(3)}(x) = -\\cos(x)$. Parameters: $x_0 = 1.0$, $\\Delta x = 0.01$, $\\sigma = 0.02$, $H_{\\max} = 2.0$.\n- Case B: $f(x) = e^{x}$, so $f^{(3)}(x) = e^{x}$. Parameters: $x_0 = 0.0$, $\\Delta x = 0.005$, $\\sigma = 0.05$, $H_{\\max} = 2.0$.\n- Case C: $f(x) = x^3$, so $f^{(3)}(x) = 6$. Parameters: $x_0 = 0.2$, $\\Delta x = 0.1$, $\\sigma = 0.0001$, $H_{\\max} = 2.0$.\n- Case D: $f(x) = \\cos(5x)$, so $f^{(3)}(x) = 125\\,\\sin(5x)$. Parameters: $x_0 = 0.0$, $\\Delta x = 0.01$, $\\sigma = 0.1$, $H_{\\max} = 1.0$.\n\nFinal output format. Your program should produce a single line of output containing a comma-separated list of the four computed $h$ values, in the order A, B, C, D, rounded to six decimal places, and enclosed in square brackets, for example, $[h_A,h_B,h_C,h_D]$.", "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the principles of numerical analysis and the study of inverse problems, specifically the regularization of ill-posed problems. The problem is well-posed, self-contained, and all data, definitions, and constraints are provided and are mutually consistent. The task is to first provide a theoretical justification for the methods used and then to implement a specific algorithm based on the derived results.\n\n### The Ill-Posed Nature of Numerical Differentiation\n\nThe problem of finding the derivative $f'$ of a function $f$ from a set of data points can be framed as an inverse problem. Let $D$ be the differentiation operator, such that $D(f) = f'$. Our goal is to recover $f'$ given noisy data $y = f + \\epsilon$. Applying the operator $D$ to the data yields $D(y) = D(f) + D(\\epsilon) = f' + \\epsilon'$. The crux of the problem lies in the behavior of the noise term $D(\\epsilon) = \\epsilon'$.\n\nAccording to Jacques Hadamard, a problem is well-posed if a solution exists, is unique, and depends continuously on the input data. Numerical differentiation violates the third criterion. To see this, consider a high-frequency noise component in the data, of the form $\\epsilon(x) = A \\sin(\\omega x)$ for some small amplitude $A$ and large frequency $\\omega$. The magnitude of this perturbation to the function $f$ is at most $A$. However, the perturbation to the derivative $f'$ is $\\epsilon'(x) = A \\omega \\cos(\\omega x)$, which has a magnitude of $A\\omega$. By choosing a sufficiently large frequency $\\omega$, the error in the derivative, $A\\omega$, can be made arbitrarily large, even for an infinitesimally small error $A$ in the data.\n\nThis shows that the solution (the derivative) does not depend continuously on the input data. Small, high-frequency perturbations in the input can be amplified to arbitrarily large perturbations in the output. Therefore, numerical differentiation is an ill-posed problem. To obtain a stable and meaningful solution, a regularization technique is required, which typically involves smoothing the data before differentiation.\n\n### Analysis of the Regularized Estimator\n\nThe proposed estimator regularizes the problem by first smoothing the data and then applying a finite difference formula. The two steps are:\n1.  Smooth the noisy data $y$ using a symmetric boxcar (uniform) kernel $K_h$ of width $h$. The smoothed signal is $\\tilde{y} = y * K_h$.\n2.  Apply the central difference formula with step size $\\Delta x$ to the smoothed signal $\\tilde{y}$ to approximate the derivative at $x_0$.\n\nThe estimator for $f'(x_0)$ is thus given by:\n$$\n\\hat{f}'(x_0) = \\frac{\\tilde{y}(x_0 + \\Delta x) - \\tilde{y}(x_0 - \\Delta x)}{2 \\Delta x} = \\frac{(y * K_h)(x_0 + \\Delta x) - (y * K_h)(x_0 - \\Delta x)}{2 \\Delta x}\n$$\nThe performance of this estimator is characterized by its Mean Squared Error ($\\operatorname{MSE}$), which can be decomposed into the sum of the squared bias and the variance:\n$$\n\\operatorname{MSE}(h) = \\text{bias}^2(h) + \\text{var}(h) = \\left( E[\\hat{f}'(x_0)] - f'(x_0) \\right)^2 + \\operatorname{Var}(\\hat{f}'(x_0))\n$$\n\n#### Bias Derivation\nThe bias is the systematic error of the estimator. Since the noise $\\epsilon_i$ has zero mean ($E[\\epsilon_i]=0$), the expected value of the estimator is found by applying it to the noise-free function $f(x)$:\n$$\nE[\\hat{f}'(x_0)] = \\frac{(f * K_h)(x_0 + \\Delta x) - (f * K_h)(x_0 - \\Delta x)}{2 \\Delta x}\n$$\nThis is the central difference approximation of the derivative of the function $g(x) = (f * K_h)(x)$ at $x=x_0$. For a sufficiently small $\\Delta x$, this expression is a good approximation of $g'(x_0)$. Using the given fact that $(f * K_h)' = f' * K_h$, we have $g'(x_0) = (f' * K_h)(x_0)$. The error introduced by the central difference formula itself is of order $O((\\Delta x)^2)$ and is considered of higher order compared to the smoothing bias, so for the leading-order bias we can approximate $E[\\hat{f}'(x_0)] \\approx (f' * K_h)(x_0)$.\n\nThe bias is then the difference between the expected smoothed derivative and the true derivative:\n$$\n\\text{bias}(h) = E[\\hat{f}'(x_0)] - f'(x_0) \\approx (f' * K_h)(x_0) - f'(x_0)\n$$\nTo analyze this, we expand $f'$ in a Taylor series around $x_0$ inside the convolution integral:\n$$\n(f' * K_h)(x_0) = \\int_{-\\infty}^{\\infty} f'(x_0 - u) K_h(u) du \\approx \\int_{-\\infty}^{\\infty} \\left[ f'(x_0) - u f''(x_0) + \\frac{u^2}{2} f^{(3)}(x_0) \\right] K_h(u) du\n$$\nSince the kernel $K_h$ is normalized ($\\int K_h(u) du = 1$) and symmetric ($\\int u K_h(u) du = 0$), this simplifies to:\n$$\n(f' * K_h)(x_0) \\approx f'(x_0) \\int K_h(u) du - f''(x_0) \\int u K_h(u) du + \\frac{f^{(3)}(x_0)}{2} \\int u^2 K_h(u) du\n$$\n$$\n(f' * K_h)(x_0) \\approx f'(x_0) + \\frac{f^{(3)}(x_0)}{2} \\mu_2(K_h)\n$$\nwhere $\\mu_2(K_h)$ is the second moment of the kernel. Using the provided fact $\\mu_2(K_h) = h^2/12$, the leading-order bias is:\n$$\n\\text{bias}(h) \\approx \\frac{h^2}{24} f^{(3)}(x_0)\n$$\nThe squared bias is therefore:\n$$\n\\text{bias}^2(h) \\approx \\frac{h^4}{576} \\left(f^{(3)}(x_0)\\right)^2\n$$\n\n#### Variance Derivation\nThe variance of the estimator captures the effect of the random noise $\\epsilon_i$.\n$$\n\\operatorname{Var}(\\hat{f}'(x_0)) = \\operatorname{Var}\\left( \\frac{(\\epsilon * K_h)(x_0 + \\Delta x) - (\\epsilon * K_h)(x_0 - \\Delta x)}{2 \\Delta x} \\right)\n$$\nLet $\\tilde{\\epsilon}(x) = (\\epsilon * K_h)(x)$ be the smoothed noise. The variance is:\n$$\n\\operatorname{Var}(\\hat{f}'(x_0)) = \\frac{1}{4(\\Delta x)^2} \\operatorname{Var}\\left( \\tilde{\\epsilon}(x_0 + \\Delta x) - \\tilde{\\epsilon}(x_0 - \\Delta x) \\right)\n$$\nUsing the provided hint, smoothing with a moving average over $m \\approx h/\\Delta x$ samples reduces the variance of the noise from $\\sigma^2$ to $\\sigma^2/m = \\sigma^2 \\Delta x / h$.\nThus, $\\operatorname{Var}(\\tilde{\\epsilon}(x)) \\approx \\sigma^2 \\Delta x / h$.\nThe second hint states that differencing two independent samples adds their variances. For the purpose of a leading-order analysis, we assume the smoothed noise values at $x_0+\\Delta x$ and $x_0-\\Delta x$ are approximately independent, which is reasonable if $h \\gg \\Delta x$. The variance of their difference is then the sum of their variances:\n$$\n\\operatorname{Var}\\left( \\tilde{\\epsilon}(x_0 + \\Delta x) - \\tilde{\\epsilon}(x_0 - \\Delta x) \\right) \\approx \\operatorname{Var}(\\tilde{\\epsilon}(x_0+\\Delta x)) + \\operatorname{Var}(\\tilde{\\epsilon}(x_0-\\Delta x)) \\approx 2 \\frac{\\sigma^2 \\Delta x}{h}\n$$\nSubstituting this back into the expression for the variance of the estimator:\n$$\n\\operatorname{var}(h) \\approx \\frac{1}{4(\\Delta x)^2} \\left( \\frac{2 \\sigma^2 \\Delta x}{h} \\right) = \\frac{\\sigma^2}{2 h \\Delta x}\n$$\n\n#### MSE Minimization and Optimal Width\nCombining the squared bias and variance gives the asymptotic Mean Squared Error:\n$$\n\\operatorname{MSE}(h) \\approx \\frac{h^4}{576} \\left(f^{(3)}(x_0)\\right)^2 + \\frac{\\sigma^2}{2 h \\Delta x}\n$$\nTo find the optimal kernel width $h_\\star$ that minimizes this $\\operatorname{MSE}$, we differentiate with respect to $h$ and set the result to zero:\n$$\n\\frac{d}{dh}\\operatorname{MSE}(h) = \\frac{4h^3}{576} \\left(f^{(3)}(x_0)\\right)^2 - \\frac{\\sigma^2}{2 h^2 \\Delta x} = 0\n$$\n$$\n\\frac{h^3}{144} \\left(f^{(3)}(x_0)\\right)^2 = \\frac{\\sigma^2}{2 h^2 \\Delta x}\n$$\nSolving for $h^5$:\n$$\nh^5 = \\frac{144 \\sigma^2}{2 \\Delta x \\left(f^{(3)}(x_0)\\right)^2} = \\frac{72 \\sigma^2}{\\Delta x \\left(f^{(3)}(x_0)\\right)^2}\n$$\nThis gives the optimal width $h_\\star$:\n$$\nh_\\star = \\left(\\frac{72\\,\\sigma^2}{\\Delta x\\,\\big(f^{(3)}(x_0)\\big)^2}\\right)^{1/5}\n$$\nThis derivation confirms the formula provided in the problem statement. The algorithm for implementation will compute this value, handling the case where $f^{(3)}(x_0)$ is near zero and ensuring the resulting $h$ is within the physical and practical bounds $[\\Delta x, H_{\\max}]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates the optimal smoothing width h for numerical differentiation\n    based on a bias-variance trade-off analysis.\n    \"\"\"\n    \n    # A small threshold to handle cases where the third derivative is zero or very close to it.\n    # This prevents division by zero and implements the saturation logic described.\n    tau = 1e-9\n\n    # Define the test cases from the problem statement.\n    # Each case is a dictionary for clarity.\n    test_cases = [\n        # Case A\n        {\n            \"f_name\": \"sin(x)\",\n            \"f3\": lambda x: -np.cos(x),\n            \"x0\": 1.0,\n            \"dx\": 0.01,\n            \"sigma\": 0.02,\n            \"Hmax\": 2.0,\n        },\n        # Case B\n        {\n            \"f_name\": \"exp(x)\",\n            \"f3\": lambda x: np.exp(x),\n            \"x0\": 0.0,\n            \"dx\": 0.005,\n            \"sigma\": 0.05,\n            \"Hmax\": 2.0,\n        },\n        # Case C\n        {\n            \"f_name\": \"x^3\",\n            \"f3\": lambda x: 6.0,\n            \"x0\": 0.2,\n            \"dx\": 0.1,\n            \"sigma\": 0.0001,\n            \"Hmax\": 2.0,\n        },\n        # Case D\n        {\n            \"f_name\": \"cos(5x)\",\n            \"f3\": lambda x: 125 * np.sin(5 * x),\n            \"x0\": 0.0,\n            \"dx\": 0.01,\n            \"sigma\": 0.1,\n            \"Hmax\": 1.0,\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        dx = case[\"dx\"]\n        sigma = case[\"sigma\"]\n        Hmax = case[\"Hmax\"]\n        \n        # Calculate the third derivative at x0\n        f3_val = case[\"f3\"](case[\"x0\"])\n        \n        # Apply the rule specified in the problem statement\n        if abs(f3_val) = tau:\n            # If the third derivative is negligible, bias is negligible.\n            # We choose the maximum smoothing to minimize variance.\n            h = Hmax\n        else:\n            # Calculate the optimal width h_star based on the derived formula\n            numerator = 72 * sigma**2\n            denominator = dx * f3_val**2\n            h_star = (numerator / denominator)**(1/5)\n            \n            # Clip the result to the valid range [dx, Hmax]\n            # h cannot be smaller than the grid spacing, and should not exceed Hmax.\n            h = np.clip(h_star, dx, Hmax)\n            \n        results.append(h)\n\n    # Final print statement in the exact required format.\n    # Each h value is formatted to six decimal places.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "3286744"}]}