{"hands_on_practices": [{"introduction": "Computers represent real numbers using a finite number of bits, a system known as floating-point arithmetic. This inherent limitation is the ultimate source of round-off error. To quantify the precision of a floating-point system, we use a key metric called machine epsilon, denoted $ \\epsilon_m $, which represents the smallest positive number that, when added to 1, results in a value that the computer can distinguish from 1. This exercise [@problem_id:2204331] demystifies this fundamental concept by guiding you through its calculation for a simplified, hypothetical floating-point system, building a concrete understanding of how precision is determined by its binary architecture.", "problem": "Consider a hypothetical 8-bit floating-point number system designed for a low-power embedded controller. In this system, each number is represented using 1 bit for the sign ($S$), 4 bits for a biased exponent ($E$), and 3 bits for the fraction part of the mantissa ($M$).\n\nThe value of a number is given by the formula:\n$$V = (-1)^S \\times (1.M)_2 \\times 2^{E - \\text{bias}}$$\n\nThe mantissa is normalized, meaning it is of the form $(1.M)_2$, which represents the binary number $1$ followed by the 3-bit fraction $M$. The exponent bias is defined as $\\text{bias} = 2^{k-1} - 1$, where $k$ is the number of bits in the exponent field.\n\nIn any floating-point system, there is a fundamental limitation on precision known as machine epsilon ($\\epsilon_m$). It is defined as the smallest positive number that, when added to 1, results in a value greater than 1 in that system's representation.\n\nCalculate the value of the machine epsilon for this 8-bit floating-point system. Express your answer as a single real number in decimal form.", "solution": "We have an 8-bit floating-point format with 1 sign bit, 4 exponent bits, and 3 fraction bits in the mantissa. The value is given by\n$$\nV = (-1)^{S} (1.M)_{2} 2^{E - \\text{bias}},\n$$\nwith normalized mantissa and bias defined by $\\text{bias} = 2^{k-1} - 1$ for $k$ exponent bits.\n\nFirst compute the bias for $k=4$:\n$$\n\\text{bias} = 2^{4-1} - 1 = 2^{3} - 1 = 7.\n$$\nThe number $1$ is represented by choosing $S=0$, $E=\\text{bias}$, and $M=000$, giving\n$$\nV_{1} = (1.000)_{2} \\times 2^{7-7} = 1 \\times 2^{0} = 1.\n$$\nThe next representable number greater than $1$ at the same exponent has the smallest positive mantissa increment, namely $M=001$. Its value is\n$$\nV_{\\text{next}} = (1.001)_{2} \\times 2^{7-7} = \\left(1 + 2^{-3}\\right) \\times 2^{0} = 1 + 2^{-3}.\n$$\nBy the definition of machine epsilon $\\epsilon_{m}$ as the smallest positive number such that $1 + \\epsilon_{m} > 1$ in this system, we have\n$$\n\\epsilon_{m} = V_{\\text{next}} - V_{1} = \\left(1 + 2^{-3}\\right) - 1 = 2^{-3} = \\frac{1}{8} = 0.125.\n$$\nEquivalently, since the precision is $p=1+3=4$ significant bits (including the implicit leading $1$), the general formula $\\epsilon_{m} = 2^{1-p}$ yields\n$$\n\\epsilon_{m} = 2^{1-4} = 2^{-3} = 0.125,\n$$\nconsistent with the direct construction.", "answer": "$$\\boxed{0.125}$$", "id": "2204331"}, {"introduction": "Beyond the errors inherent in hardware precision, another significant source of computational error arises from our mathematical methods. Many essential functions, like logarithms and trigonometric functions, are computed not through exact evaluation but through approximations, such as a finite Taylor series. The error introduced by using a partial sum instead of an infinite one is known as truncation error. This practice [@problem_id:2204306] gives you hands-on experience in managing this trade-off, challenging you to determine how many series terms are required to guarantee a specified level of accuracy for the natural logarithm function, a crucial skill in designing efficient and reliable numerical code.", "problem": "A programmer is tasked with implementing a high-precision natural logarithm function, `log(y)`, for a specialized computational library. To handle inputs `y` that are very close to 1, the function uses the Maclaurin series expansion for $f(x) = \\ln(1+x)$, where $x = y-1$. The Maclaurin series is given by:\n$$ \\ln(1+x) = \\sum_{n=1}^{\\infty} (-1)^{n-1} \\frac{x^n}{n} = x - \\frac{x^2}{2} + \\frac{x^3}{3} - \\frac{x^4}{4} + \\dots $$\nFor a specific benchmark test, the value of $\\ln(1.01)$ must be calculated. The requirement is that the absolute value of the truncation error—the difference between the true value of $\\ln(1.01)$ and the partial sum from the series—must be strictly less than $10^{-8}$.\n\nWhat is the minimum number of terms of the series that must be summed to guarantee that the approximation meets this error tolerance? Your answer should be a single integer.", "solution": "We approximate $\\ln(1+x)$ with $x=y-1$ using its alternating Maclaurin series $\\sum_{n=1}^{\\infty}(-1)^{n-1}\\frac{x^{n}}{n}$. For $y=1.01$ we have $x=0.01=10^{-2}$. The $N$-term partial sum $S_{N}=\\sum_{n=1}^{N}(-1)^{n-1}\\frac{x^{n}}{n}$ has remainder $R_{N}=\\ln(1+x)-S_{N}$. Since $x\\in(0,1)$, the term magnitudes $a_{n}=\\frac{x^{n}}{n}$ decrease to zero because\n$$\n\\frac{a_{n+1}}{a_{n}}=\\frac{x^{n+1}}{n+1}\\cdot\\frac{n}{x^{n}}=x\\cdot\\frac{n}{n+1}<1,\n$$\nso the alternating series remainder satisfies the Leibniz bound\n$$\n|R_{N}|\\leq a_{N+1}=\\frac{x^{N+1}}{N+1}.\n$$\nTo guarantee $|R_{N}|<10^{-8}$, it suffices to require\n$$\n\\frac{x^{N+1}}{N+1}<10^{-8}.\n$$\nSubstituting $x=10^{-2}$ gives\n$$\n\\frac{10^{-2(N+1)}}{N+1}<10^{-8}\\quad\\Longleftrightarrow\\quad 10^{6-2N}<N+1.\n$$\nTesting the smallest integers:\n- For $N=2$: $10^{6-4}=10^{2}=100\\not<3$, so not sufficient.\n- For $N=3$: $10^{6-6}=10^{0}=1<4$, so the condition holds.\n\nSince $10^{6-2N}$ decreases strictly with increasing $N$, all $N\\geq 3$ satisfy the inequality, and the minimal such $N$ is $N=3$.", "answer": "$$\\boxed{3}$$", "id": "2204306"}, {"introduction": "The interaction between a computational algorithm and a machine's finite precision can lead to surprisingly large and disastrous errors. A particularly notorious pitfall is catastrophic cancellation, which occurs when subtracting two nearly identical floating-point numbers, leading to a dramatic loss of significant digits. This demonstrates that algebraically equivalent formulas are not always numerically equivalent. By working through the calculation of variance using two different formulas on a hypothetical limited-precision calculator [@problem_id:2204336], you will directly observe how a mathematically correct but poorly chosen algorithm can fail spectacularly, reinforcing the critical importance of selecting numerically stable algorithms in scientific computing.", "problem": "A quality control engineer is measuring the length of three high-precision steel rods from a single production batch. The measurements are recorded as $x_1 = 10000.001$ mm, $x_2 = 10000.002$ mm, and $x_3 = 10000.003$ mm. The engineer needs to calculate the population variance of these three lengths.\n\nThe engineer uses a calculator with a limited floating-point precision. This calculator performs every arithmetic operation and stores every intermediate result by rounding to 8 significant figures.\n\nThe population variance, $\\sigma^2$, can be computed using two different formulas:\n1.  The \"one-pass\" computational formula: $\\sigma^2 = \\left(\\frac{1}{N}\\sum_{i=1}^{N} x_i^2\\right) - \\mu^2$, where $\\mu = \\frac{1}{N}\\sum_{i=1}^{N} x_i$.\n2.  The \"two-pass\" definitional formula: $\\sigma^2 = \\frac{1}{N}\\sum_{i=1}^{N} (x_i - \\mu)^2$.\n\nLet $V_1$ be the variance calculated using the one-pass formula and $V_2$ be the variance calculated using the two-pass formula, both subject to the calculator's rounding constraint. The two-pass formula is known to be more numerically stable for data with a small standard deviation compared to its mean. Therefore, we will treat $V_2$ as the more accurate value for the purpose of this problem.\n\nCalculate the relative error, $E_{rel}$, of the one-pass formula's result with respect to the two-pass formula's result, defined as $E_{rel} = \\frac{|V_2 - V_1|}{|V_2|}$. Report your final answer for the relative error rounded to four significant figures.", "solution": "We are given three measurements $x_{1}=10000.001$, $x_{2}=10000.002$, $x_{3}=10000.003$ with $N=3$. Every operation and intermediate storage is rounded to 8 significant figures.\n\nFirst compute the mean used in both methods. Sum:\n$$x_{1}+x_{2}=20000.003 \\quad(\\text{already 8 s.f.}),$$\n$$x_{1}+x_{2}+x_{3}=30000.006 \\quad(\\text{already 8 s.f.}).$$\nThen\n$$\\mu=\\frac{1}{3}(30000.006)=10000.002 \\quad(\\text{rounded to 8 s.f.}).$$\n\nOne-pass (computational) variance $V_{1}=\\left(\\frac{1}{N}\\sum x_{i}^{2}\\right)-\\mu^{2}$ with 8 s.f. rounding after each operation:\n- Square each $x_{i}$ and round to 8 s.f.:\n$$x_{1}^{2}=100000020.000001 \\to 1.0000002\\times 10^{8},$$\n$$x_{2}^{2}=100000040.000004 \\to 1.0000004\\times 10^{8},$$\n$$x_{3}^{2}=100000060.000009 \\to 1.0000006\\times 10^{8}.$$\n- Sum and average (rounding preserved at 8 s.f.):\n$$\\sum x_{i}^{2}=3.0000012\\times 10^{8},\\qquad \\frac{1}{3}\\sum x_{i}^{2}=1.0000004\\times 10^{8}.$$\n- Square the rounded mean and round to 8 s.f.:\n$$\\mu^{2}=(10000.002)^{2}=1.00000040000004\\times 10^{8}\\to 1.0000004\\times 10^{8}.$$\n- Subtract:\n$$V_{1}=\\left(1.0000004\\times 10^{8}\\right)-\\left(1.0000004\\times 10^{8}\\right)=0.$$\n\nTwo-pass (definitional) variance $V_{2}=\\frac{1}{N}\\sum (x_{i}-\\mu)^{2}$ with 8 s.f. rounding at each step:\n- Deviations (rounded to 8 s.f.):\n$$x_{1}-\\mu=-0.001= -1.0000000\\times 10^{-3},\\quad x_{2}-\\mu=0,\\quad x_{3}-\\mu=+0.001=1.0000000\\times 10^{-3}.$$\n- Squares (rounded to 8 s.f.):\n$$(x_{1}-\\mu)^{2}=1.0000000\\times 10^{-6},\\quad (x_{2}-\\mu)^{2}=0,\\quad (x_{3}-\\mu)^{2}=1.0000000\\times 10^{-6}.$$\n- Sum and average:\n$$\\sum (x_{i}-\\mu)^{2}=2.0000000\\times 10^{-6},\\qquad V_{2}=\\frac{1}{3}\\left(2.0000000\\times 10^{-6}\\right)=6.6666667\\times 10^{-7}\\quad(\\text{8 s.f.}).$$\n\nRelative error using these rounded results:\n$$E_{\\text{rel}}=\\frac{|V_{2}-V_{1}|}{|V_{2}|}=\\frac{6.6666667\\times 10^{-7}-0}{6.6666667\\times 10^{-7}}=1.$$\nRounded to four significant figures, this is $1.000$.", "answer": "$$\\boxed{1.000}$$", "id": "2204336"}]}