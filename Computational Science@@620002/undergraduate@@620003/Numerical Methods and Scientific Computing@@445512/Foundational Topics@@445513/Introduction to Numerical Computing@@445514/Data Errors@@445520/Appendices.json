{"hands_on_practices": [{"introduction": "Our journey into hands-on practice begins with a foundational concept: error propagation. In scientific computing, we often use approximations of mathematical constants like $\\pi$. This exercise demonstrates how a seemingly small truncation error in an input value can propagate through a formula, leading to a significant error in the final result, especially as other parameters in the calculation become large. By analyzing the volume of a sphere, you will derive an exact expression for this error, providing a clear and tangible understanding of forward error analysis [@problem_id:3221388].", "problem": "In numerical methods and scientific computing, truncation of mathematical constants introduces data errors that propagate through computations. Consider the volume of a sphere of radius $R$, where $R$ is a positive real number and is known exactly. The true volume is determined by a well-tested geometric formula. In practice, suppose the constant $\\pi$ is truncated to the decimal value $3.14$ before computing the sphere's volume. Using the foundational definition of forward (output) error as the difference between the computed quantity and the true quantity, and using only well-established geometric facts, derive the exact closed-form expression for the signed absolute error in the computed volume due solely to truncating $\\pi$ to $3.14$. Express your final answer as a single analytic expression in terms of $R$. No rounding is required.", "solution": "The problem asks for the exact closed-form expression for the signed absolute error in the computed volume of a sphere, where the error arises from truncating $\\pi$ to $3.14$.\n\nLet $V_{\\text{true}}$ be the true volume and $V_{\\text{computed}}$ be the computed volume. The radius of the sphere, $R$, is known exactly.\n\nThe formula for the true volume of a sphere is:\n$$V_{\\text{true}} = \\frac{4}{3}\\pi R^3$$\n\nThe computed volume uses the approximation $\\pi_{\\text{approx}} = 3.14$:\n$$V_{\\text{computed}} = \\frac{4}{3}\\pi_{\\text{approx}} R^3 = \\frac{4}{3}(3.14)R^3$$\n\nThe signed absolute error, $E$, is defined as the difference between the computed and true quantities:\n$$E = V_{\\text{computed}} - V_{\\text{true}}$$\n\nSubstituting the expressions for the volumes:\n$$E = \\frac{4}{3}(3.14)R^3 - \\frac{4}{3}\\pi R^3$$\n\nFactoring out the common term $\\frac{4}{3}R^3$ gives the final expression for the error:\n$$E = \\frac{4}{3}R^3 (3.14 - \\pi)$$", "answer": "$$\\boxed{\\frac{4}{3}(3.14 - \\pi)R^3}$$", "id": "3221388"}, {"introduction": "Computers perform calculations using finite-precision floating-point arithmetic, which introduces small rounding errors at nearly every step. While individual errors are tiny, they can accumulate into a significant discrepancy, especially when summing a long series of numbers. This practice explores the non-intuitive but critical fact that the order of operations can drastically affect the accuracy of a sum, and introduces the Kahan summation algorithm, a powerful compensated summation technique designed to preserve precision [@problem_id:3221372].", "problem": "Consider floating-point summation in double precision. Assume rounding to nearest with unit roundoff $u$, modeled by the standard floating-point rounding relation $ \\operatorname{fl}(a \\circ b) = (a \\circ b)(1 + \\theta) $ for a basic operation $ \\circ \\in \\{ +, -, \\times, \\div \\} $, with $|\\theta| \\leq u$ and $u = 2^{-53}$ for IEEE 754 double precision. Let $x_i = \\frac{1}{i} + \\delta_i$ where $i \\in \\{1,2,\\dots,n\\}$ and $\\delta_i$ is small additive data noise. The goal is to study how summation order affects rounding error by computing $ \\sum_{i=1}^{n} x_i $ in ascending versus descending $i$, and to compare a naive sequential summation to a compensated summation method (Kahan Summation Algorithm) in the presence of additive data noise $\\delta_i$.\n\nTasks:\n- For each test case, construct the sequence $x_i = \\frac{1}{i} + \\delta_i$ for $i = 1,\\dots,n$. Generate $\\delta_i$ as independent samples from a normal distribution with mean $0$ and standard deviation $\\sigma$, using a fixed pseudorandom seed $0$ so that the noise realization is deterministic. Use double precision throughout.\n- Compute four sums for each test case: naive ascending, naive descending, compensated ascending, compensated descending. Here \"naive\" means a straightforward left-to-right accumulation without any compensation, and \"compensated\" means a numerically stable summation technique designed to reduce rounding error.\n- For each test case, report the following four float metrics:\n    1. $d_{\\text{naive}} = \\left| S_{\\text{naive}}^{\\uparrow} - S_{\\text{naive}}^{\\downarrow} \\right|$, the absolute difference between naive ascending and descending sums.\n    2. $d_{\\text{comp}} = \\left| S_{\\text{comp}}^{\\uparrow} - S_{\\text{comp}}^{\\downarrow} \\right|$, the absolute difference between compensated ascending and descending sums.\n    3. $I_{\\uparrow} = \\left| S_{\\text{naive}}^{\\uparrow} - S_{\\text{comp}}^{\\uparrow} \\right|$, the absolute difference between naive and compensated summation in ascending order.\n    4. $I_{\\downarrow} = \\left| S_{\\text{naive}}^{\\downarrow} - S_{\\text{comp}}^{\\downarrow} \\right|$, the absolute difference between naive and compensated summation in descending order.\n\nTest suite:\n- Case A: $n = 1$, $\\sigma = 0$.\n- Case B: $n = 10^3$, $\\sigma = 0$.\n- Case C: $n = 10^5$, $\\sigma = 0$.\n- Case D: $n = 10^5$, $\\sigma = 10^{-12}$.\n\nImplementation details:\n- Use a fixed pseudorandom seed $0$ in each test case to generate the entire vector $\\delta_i$ of length $n$. The noise is independent across $i$.\n- All computations must be in double precision.\n- There are no physical units; all quantities are dimensionless.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a comma-separated list of the four metrics in the order specified above. For example, the output format should be like $[[d_{\\text{naive}},d_{\\text{comp}},I_{\\uparrow},I_{\\downarrow}],\\dots]$ with no spaces.", "solution": "The core of this problem lies in understanding how floating-point arithmetic deviates from exact real arithmetic, particularly in the context of summing a long sequence of numbers.\n\nA standard floating-point addition operation is modeled by $\\operatorname{fl}(a + b) = (a + b)(1 + \\theta)$, where $|\\theta| \\leq u$. The term $u$ is the unit roundoff, which is $u = 2^{-53}$ for IEEE 754 double precision. Each addition can introduce a small relative error. When summing $n$ numbers, $S_n = \\sum_{i=1}^n x_i$, a naive sequential summation computes $s_1 = x_1$, $s_2 = \\operatorname{fl}(s_1 + x_2)$, $s_3 = \\operatorname{fl}(s_2 + x_3)$, and so on. The errors from each step accumulate.\n\nA well-known issue arises when adding numbers of vastly different magnitudes. If we add a small number to a large one, i.e., $|x_i| \\ll |s_{i-1}|$, the smaller number may be partially or completely lost due to rounding. For the sequence $x_i = \\frac{1}{i}$, the terms are strictly decreasing in magnitude: $x_1 > x_2 > \\dots > x_n$.\n\n- **Naive Ascending Summation ($S_{\\text{naive}}^{\\uparrow}$):** This involves summing from $i=1$ to $n$. We start with the largest term, $x_1=1$, and successively add smaller and smaller terms. As the running sum $s_k = \\sum_{i=1}^k x_i$ grows, the newly added terms $x_{k+1}$ become increasingly smaller relative to $s_k$. This leads to significant loss of precision, a phenomenon known as absorption error.\n\n- **Naive Descending Summation ($S_{\\text{naive}}^{\\downarrow}$):** This involves summing from $i=n$ to $1$. We start with the smallest terms and gradually add larger ones. The running sum grows more slowly, and consecutive terms added are of more similar magnitude. This strategy generally reduces the accumulation of rounding error. Thus, we expect $S_{\\text{naive}}^{\\downarrow}$ to be more accurate than $S_{\\text{naive}}^{\\uparrow}$. The difference $d_{\\text{naive}} = |S_{\\text{naive}}^{\\uparrow} - S_{\\text{naive}}^{\\downarrow}|$ is expected to be significant for large $n$.\n\nTo mitigate the accumulation of rounding errors, numerically stable algorithms have been developed. The problem specifies using a compensated summation method, for which the Kahan Summation Algorithm is the canonical choice. It cleverly tracks the roundoff error from each addition and incorporates it into the subsequent step.\n\nThe algorithm to compute $S = \\sum_{i=1}^n x_i$ proceeds as follows:\n1. Initialize sum $s = 0.0$ and a compensation variable $c = 0.0$.\n2. For $i=1$ to $n$:\n   a. $y = x_i - c$  (Subtract the error from the previous step)\n   b. $t = s + y$    (Add the corrected term to the sum)\n   c. $c = (t - s) - y$  (The new error is the part of $y$ that was lost when adding to $s$)\n   d. $s = t$          (Update the sum)\n3. The final result is $s$.\n\nThe key is step $2c$. In exact arithmetic, $c$ would be $0$. In floating-point arithmetic, $(t - s)$ is the part of $y$ that was actually assimilated into $s$. Subtracting $y$ from this value gives the negative of the part of $y$ that was lost. This \"lost low-part\" is then subtracted from the next term $x_{i+1}$ in step $2a$ of the next iteration, effectively carrying it forward to be included in the sum.\n\nThe error of Kahan summation is bounded by approximately $(2u + O(nu^2))\\sum_{i=1}^n |x_i|$. The first-order term is independent of $n$, making the algorithm extremely accurate even for very large sums. As a result, both ascending ($S_{\\text{comp}}^{\\uparrow}$) and descending ($S_{\\text{comp}}^{\\downarrow}$) compensated sums should be very close to the true mathematical sum, and thus very close to each other. We expect their difference, $d_{\\text{comp}} = |S_{\\text{comp}}^{\\uparrow} - S_{\\text{comp}}^{\\downarrow}|$, to be close to zero.\n\nThe compensated sum serves as a high-accuracy benchmark. Therefore, the metrics $I_{\\uparrow} = |S_{\\text{naive}}^{\\uparrow} - S_{\\text{comp}}^{\\uparrow}|$ and $I_{\\downarrow} = |S_{\\text{naive}}^{\\downarrow} - S_{\\text{comp}}^{\\downarrow}|$ effectively measure the absolute error of the naive ascending and descending summations, respectively. We expect $I_{\\uparrow} > I_{\\downarrow}$ for large $n$.\n\nCase D introduces additive data noise $\\delta_i$ sampled from a normal distribution with mean $0$ and standard deviation $\\sigma = 10^{-12}$. The unit roundoff $u$ is approximately $2.22 \\times 10^{-16}$. Since $\\sigma \\gg u$, the data noise is significantly larger than the rounding error of a single operation. The sum of these noise terms, $\\sum \\delta_i$, will contribute an error to the total sum. The standard deviation of this total noise error is $\\sqrt{n}\\sigma$. For $n=10^5$, this is $\\sqrt{10^5} \\times 10^{-12} \\approx 3.16 \\times 10^{-10}$.\n\nKahan summation accurately sums the given noisy data $x_i = 1/i + \\delta_i$. The difference between ascending and descending Kahan sums should remain negligible. Naive summation, however, will be afflicted by both its inherent rounding error accumulation and the data noise. The metrics $I_{\\uparrow}$ and $I_{\\downarrow}$ will still primarily reflect the large rounding errors of the naive methods, as these errors for $n=10^5$ are expected to be much larger than the total data error.\n\nThe algorithmic implementation follows these steps:\n- For each test case $(n, \\sigma)$:\n  1. Generate the vector of terms $x_i = \\frac{1}{i} + \\delta_i$ for $i=1, \\dots, n$. A new pseudorandom number generator seeded with $0$ is used for each case to generate the noise vector $\\delta$ of length $n$. For cases with $\\sigma = 0$, $\\delta_i = 0$. All computations use double precision (`np.float64`).\n  2. Create two versions of the data vector: one in ascending order of $i$ and one in descending order.\n  3. Implement two summation functions: `naive_sum` (a simple loop accumulator) and `kahan_sum` (implementing the algorithm described above).\n  4. Compute the four sums: $S_{\\text{naive}}^{\\uparrow}$, $S_{\\text{naive}}^{\\downarrow}$, $S_{\\text{comp}}^{\\uparrow}$, and $S_{\\text{comp}}^{\\downarrow}$ by applying the appropriate function to the appropriate data vector.\n  5. Calculate the four required metrics ($d_{\\text{naive}}, d_{\\text{comp}}, I_{\\uparrow}, I_{\\downarrow}$) from these sums.\n  6. Store and format the results as specified.", "answer": "```python\nimport numpy as np\n\ndef naive_sum(arr: np.ndarray) -> np.float64:\n    \"\"\"\n    Computes the sum of array elements using a naive, left-to-right\n    iterative approach.\n    \"\"\"\n    s = np.float64(0.0)\n    for x_i in arr:\n        s += x_i\n    return s\n\ndef kahan_sum(arr: np.ndarray) -> np.float64:\n    \"\"\"\n    Computes the sum of array elements using the Kahan summation algorithm\n    to minimize rounding error.\n    \"\"\"\n    s = np.float64(0.0)\n    c = np.float64(0.0)  # A running compensation for lost low-order bits.\n    for x_i in arr:\n        y = x_i - c\n        t = s + y\n        # (t - s) is the high-order part of y; ((t - s) - y) is the low-order part.\n        c = (t - s) - y\n        s = t\n    return s\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (1, 0.0),            # Case A\n        (10**3, 0.0),        # Case B\n        (10**5, 0.0),        # Case C\n        (10**5, 10**-12)     # Case D\n    ]\n\n    all_results = []\n    for n, sigma in test_cases:\n        # 1. Construct the sequence x_i = 1/i + delta_i\n        # As per problem, re-seed for each test case for deterministic noise.\n        rng = np.random.default_rng(0)\n        \n        i_vals = np.arange(1, n + 1, dtype=np.float64)\n        x = 1.0 / i_vals\n        \n        if sigma > 0:\n            delta = rng.normal(loc=0.0, scale=sigma, size=n)\n            x += delta.astype(np.float64)\n\n        # Ensure final array is of the correct type\n        x = x.astype(np.float64)\n\n        # 2. Get ascending and descending views of the sequence\n        x_asc = x\n        x_desc = x[::-1].copy() # Use copy to ensure it's a new continuous array\n\n        # 3. Compute the four required sums\n        S_naive_up = naive_sum(x_asc)\n        S_naive_down = naive_sum(x_desc)\n        S_comp_up = kahan_sum(x_asc)\n        S_comp_down = kahan_sum(x_desc)\n\n        # 4. Compute the four metrics\n        d_naive = abs(S_naive_up - S_naive_down)\n        d_comp = abs(S_comp_up - S_comp_down)\n        I_up = abs(S_naive_up - S_comp_up)\n        I_down = abs(S_naive_down - S_comp_down)\n        \n        case_results = [d_naive, d_comp, I_up, I_down]\n        all_results.append(case_results)\n\n    # 5. Format the final output string as per requirements\n    # Format: [[d_naive_A,d_comp_A,I_up_A,I_down_A],[...]]\n    output_str = \",\".join([f\"[{','.join(map(str, res))}]\" for res in all_results])\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "3221372"}, {"introduction": "Real-world data is rarely perfect; it is often contaminated with noise from measurement processes. This final practice explores the crucial interaction between numerical algorithms and noisy data. By analyzing three different finite difference formulas for numerical differentiation, you will discover how the choice of algorithm can either amplify or suppress high-frequency noise, a key factor in determining the stability and reliability of a numerical method in practical applications [@problem_id:3221398].", "problem": "A uniformly spaced spatial grid is given by $x_i = x_0 + i h$ for integer $i$ and fixed spacing $h > 0$. A physical field $f(x)$ is sampled on this grid, but the measurements $u_i$ are contaminated by additive high-frequency noise so that $u_i = f(x_i) + \\epsilon \\cos(k x_i)$, where $\\epsilon > 0$ is small and $k$ is the wavenumber of the noise component. Consider numerical differentiation at $x_i$ using the following finite difference approximations of the derivative based on the samples $u_i$:\n- Forward difference: $(u_{i+1} - u_i)/h$,\n- Backward difference: $(u_i - u_{i-1})/h$,\n- Central difference: $(u_{i+1} - u_{i-1})/(2 h)$.\nAssume the notion of stability under high-frequency noise is defined by the magnitude of the amplification of the noise component by the differentiation operator: the smaller the amplification for a given high-frequency $k$, the more stable the method. Starting from core definitions of these difference operators and basic properties of complex exponentials $\\exp(i \\theta)$, analyze the response of each operator to the pure noise component and compare their stability at the highest resolvable wavenumber for this sampling, namely $k h = \\pi$.\nWhich statement correctly compares the stability of the three formulas in the presence of such high-frequency noise?\n\nA. The central difference suppresses the Nyquist-frequency noise (i.e., yields $0$ contribution from the noise), while the forward and backward differences amplify it maximally and equally; therefore, the central difference is most stable and the forward and backward differences are equally least stable.\n\nB. The forward difference suppresses the Nyquist-frequency noise, while the backward and central differences amplify it maximally; therefore, the forward difference is most stable.\n\nC. The backward difference suppresses the Nyquist-frequency noise, while the forward and central differences amplify it maximally; therefore, the backward difference is most stable.\n\nD. All three formulas have identical amplification of the Nyquist-frequency noise and thus identical stability.", "solution": "The problem asks to compare the stability of three finite difference formulas (forward, backward, central) when applied to data contaminated with high-frequency noise. Stability is defined by how much the operator amplifies the noise. The analysis is to be performed at the highest resolvable frequency on the grid, the Nyquist frequency, where the wavenumber $k$ and grid spacing $h$ satisfy $k h = \\pi$.\n\nThe noise component at a grid point $x_i$ is $n(x_i) = \\epsilon \\cos(k x_i)$. To analyze the operator's response, it is convenient to use the complex exponential form, $N_i = N(x_i) = \\epsilon e^{\\mathrm{j} k x_i}$, where $\\mathrm{j} = \\sqrt{-1}$. The amplification of the magnitude will be the same, and the algebra is simpler.\n\nAt the Nyquist frequency ($k h = \\pi$), the noise signal at adjacent grid points is related. Since $x_{i+1} = x_i + h$, we have:\n$$N_{i+1} = \\epsilon e^{\\mathrm{j} k (x_i + h)} = \\epsilon e^{\\mathrm{j} k x_i} e^{\\mathrm{j} k h} = N_i e^{\\mathrm{j} \\pi} = -N_i$$\nThis means the noise signal at the Nyquist frequency alternates in sign at every grid point, e.g., $A, -A, A, -A, \\dots$ for some amplitude $A$. Let's analyze the output of each operator applied to this alternating signal $N_i$.\n\n**1. Forward Difference (FD)**\nThe operator is applied to the noise component $N_i$:\n$$D_{FD}[N_i] = \\frac{N_{i+1} - N_i}{h} = \\frac{-N_i - N_i}{h} = \\frac{-2N_i}{h}$$\nThe magnitude of the noise is amplified by a factor of $2/h$.\n\n**2. Backward Difference (BD)**\nWe need the value at $N_{i-1}$. Since $N_i = -N_{i-1}$, we have $N_{i-1} = -N_i$.\n$$D_{BD}[N_i] = \\frac{N_i - N_{i-1}}{h} = \\frac{N_i - (-N_i)}{h} = \\frac{2N_i}{h}$$\nThe magnitude of the noise is also amplified by a factor of $2/h$.\n\n**3. Central Difference (CD)**\n$$D_{CD}[N_i] = \\frac{N_{i+1} - N_{i-1}}{2h} = \\frac{-N_i - (-N_i)}{2h} = \\frac{-N_i + N_i}{2h} = 0$$\nThe central difference operator completely eliminates the noise signal at the Nyquist frequency. The amplification factor is 0.\n\n**Conclusion:**\n- **Central Difference:** Amplification is 0 (most stable, suppresses noise).\n- **Forward & Backward Difference:** Amplification is $2/h$ (least stable, maximally amplify noise).\n\nThis analysis shows that the central difference formula is the most stable as it completely suppresses noise at the Nyquist frequency. The forward and backward difference formulas are equally unstable, both maximally amplifying this noise. Therefore, statement A is the correct description.", "answer": "$$\\boxed{A}$$", "id": "3221398"}]}