{"hands_on_practices": [{"introduction": "In numerical computing, appearances can be deceiving. An expression that is algebraically simple can be a source of significant error. This practice explores a canonical example of \"catastrophic cancellation,\" a phenomenon where subtracting two nearly equal floating-point numbers obliterates significant digits. By working with the function $f(x) = \\sqrt{x+1} - \\sqrt{x}$ for large $x$, you will learn a fundamental algebraic technique to transform an unstable expression into a numerically robust one, a crucial skill for any scientific programmer. [@problem_id:3273448]", "problem": "In standard floating-point arithmetic as described by the widely accepted model in numerical analysis, each basic operation on a real quantity $y$ produces a floating-point result $\\operatorname{fl}(y)$ satisfying $\\operatorname{fl}(y) = y(1+\\delta)$ with $|\\delta| \\leq u$, where $u$ denotes the unit roundoff. Loss of significance occurs when subtracting nearly equal quantities, because the relative error introduced by independent rounding in each term can dominate the small true difference.\n\nConsider the function $f(x) = \\sqrt{x+1} - \\sqrt{x}$, evaluated for large $x > 0$. Using only fundamental algebraic identities and the above error model as the foundational base, derive an algebraically equivalent expression for $f(x)$ that avoids loss of significance caused by subtractive cancellation when $x$ is large. Your derivation must make clear why the new expression is less sensitive to rounding. Provide the final expression as a single closed-form analytic function of $x$. No numerical approximation or rounding is required. Your final answer should be a single analytic expression in $x$.", "solution": "The starting point is the floating-point error model $\\operatorname{fl}(y) = y(1+\\delta)$ with $|\\delta| \\leq u$, and the observation that subtracting nearly equal quantities amplifies relative error. For the function $f(x) = \\sqrt{x+1} - \\sqrt{x}$, when $x$ is large, both $\\sqrt{x+1}$ and $\\sqrt{x}$ are close in magnitude: specifically, for large $x$,\n$$\n\\sqrt{x+1} \\approx \\sqrt{x} + \\frac{1}{2\\sqrt{x}},\n$$\nso $f(x)$ is small, of order $\\frac{1}{2\\sqrt{x}}$, while each term $\\sqrt{x+1}$ and $\\sqrt{x}$ is of order $\\sqrt{x}$. Subtracting two large, nearly equal terms produces a small result, which is susceptible to large relative error due to rounding in each term.\n\nTo avoid subtractive cancellation, we seek an algebraically equivalent form that does not require subtracting two almost-equal quantities. A fundamental identity suitable for this transformation is the difference-of-squares identity:\n$$\n(a - b)(a + b) = a^{2} - b^{2}.\n$$\nSet $a = \\sqrt{x+1}$ and $b = \\sqrt{x}$. Then\n$$\n(\\sqrt{x+1} - \\sqrt{x})(\\sqrt{x+1} + \\sqrt{x}) = (x+1) - x = 1.\n$$\nAssuming $x > 0$ so that all square roots and sums are positive and nonzero, we can divide both sides by $\\sqrt{x+1} + \\sqrt{x}$ to obtain\n$$\n\\sqrt{x+1} - \\sqrt{x} = \\frac{1}{\\sqrt{x+1} + \\sqrt{x}}.\n$$\nThis expression is algebraically equivalent to the original but avoids the subtraction of nearly equal quantities. In floating-point arithmetic, computing $\\sqrt{x+1}$ and $\\sqrt{x}$ still introduces small relative errors of at most $u$, but forming $\\sqrt{x+1} + \\sqrt{x}$ adds two positive terms to produce a result of magnitude about $2\\sqrt{x}$, which is well conditioned. The subsequent reciprocal does not reintroduce cancellation.\n\nTo justify the improved numerical behavior, consider the error propagation under the floating-point model. Let $\\widehat{a} = \\operatorname{fl}(\\sqrt{x+1}) = \\sqrt{x+1}(1+\\delta_{1})$ and $\\widehat{b} = \\operatorname{fl}(\\sqrt{x}) = \\sqrt{x}(1+\\delta_{2})$ with $|\\delta_{1}|, |\\delta_{2}| \\leq u$. In the subtractive form, the computed difference is\n$$\n\\operatorname{fl}(\\widehat{a} - \\widehat{b}) \\approx (\\sqrt{x+1} - \\sqrt{x}) + \\sqrt{x+1}\\,\\delta_{1} - \\sqrt{x}\\,\\delta_{2},\n$$\nso the absolute error is of order $\\sqrt{x}\\,u$, while the true value is of order $\\frac{1}{2\\sqrt{x}}$, yielding a relative error on the order of $2x u$, which grows with $x$ and can be very large for large $x$. In contrast, in the rewritten form,\n$$\nf(x) = \\frac{1}{\\sqrt{x+1} + \\sqrt{x}},\n$$\nthe computed denominator $\\operatorname{fl}(\\widehat{a} + \\widehat{b})$ has a relative error bounded by a small multiple of $u$, and the denominator itself is of order $2\\sqrt{x}$, so the absolute error in the denominator is of order $\\sqrt{x}\\,u$. The relative error in the final reciprocal is then of order $u$, independent of $x$, which is numerically stable for large $x$.\n\nTherefore, the numerically stable, algebraically equivalent expression is\n$$\n\\frac{1}{\\sqrt{x+1} + \\sqrt{x}}.\n$$", "answer": "$$\\boxed{\\frac{1}{\\sqrt{x+1}+\\sqrt{x}}}$$", "id": "3273448"}, {"introduction": "Building on the principle of avoiding subtractive cancellation, this exercise examines a formula familiar to every student of mathematics: the quadratic formula. While algebraically perfect, its direct implementation in floating-point arithmetic can lead to disastrously inaccurate results under certain conditions, specifically when $b^2 \\gg 4ac$. This practice challenges you to diagnose the source of this instability and use a clever algebraic reformulation to derive a stable alternative for computing the roots of a quadratic equation. [@problem_id:3273565]", "problem": "Consider computing the roots of the quadratic equation $a x^{2} + b x + c = 0$ in floating-point arithmetic when $b^{2} \\gg 4 a c$. Assume a rounding-to-nearest floating-point model in which any basic arithmetic operation or elementary function evaluation returns $\\operatorname{fl}(z) = z (1 + \\delta)$ with $|\\delta| \\leq u$, where $u$ (the unit roundoff) is small. Work with exact symbols until the final numerical evaluation is requested.\n\n1. Starting from the quadratic formula and the floating-point model above, explain why directly evaluating the root\n$$\nx_{\\mathrm{small}} = \\frac{-b + \\sqrt{b^{2} - 4 a c}}{2 a}\n$$\nwhen $b > 0$ is susceptible to catastrophic cancellation if $b^{2} \\gg 4 a c$. Quantify, to leading order in $u$ and $4ac/b^{2}$, how the subtraction in the numerator amplifies relative errors.\n\n2. Propose a change of variables $y = 1/x$, derive the quadratic equation that $y$ satisfies, and identify which choice of sign in the quadratic formula for $y$ avoids catastrophic cancellation when $b > 0$ and $b^{2} \\gg 4 a c$. Use this to obtain an algebraic expression for $x_{\\mathrm{small}}$ that mitigates cancellation, expressed in terms of $a$, $b$, and $c$ only.\n\n3. Apply your expression to the specific coefficients $a = 1$, $b = 2 \\times 10^{8}$, and $c = 3$ to compute the smaller-magnitude root. Round your final answer to $8$ significant figures. Provide only the numerical value of the smaller-magnitude root as your final answer.", "solution": "1. Analysis of Catastrophic Cancellation\n\nWe are asked to analyze the evaluation of the root $x_{\\mathrm{small}} = \\frac{-b + \\sqrt{b^{2} - 4 a c}}{2 a}$ for a quadratic equation $a x^{2} + b x + c = 0$ under the conditions $b > 0$ and $b^{2} \\gg 4 a c$. The condition $b^{2} \\gg 4 a c$ implies that the term $4ac$ is very small compared to $b^2$.\n\nLet's analyze the term $\\sqrt{b^{2} - 4 a c}$ using a Taylor expansion. We can factor out $b^2$ from the square root:\n$$\n\\sqrt{b^{2} - 4 a c} = \\sqrt{b^2 \\left(1 - \\frac{4ac}{b^2}\\right)} = |b|\\left(1 - \\frac{4ac}{b^2}\\right)^{1/2}\n$$\nSince $b>0$, $|b|=b$. Let $\\epsilon = \\frac{4ac}{b^2}$. The condition $b^2 \\gg 4ac$ means $|\\epsilon| \\ll 1$. Using the binomial approximation $(1-z)^{1/2} \\approx 1 - \\frac{1}{2}z$ for small $z$, we get:\n$$\n\\sqrt{b^{2} - 4 a c} \\approx b\\left(1 - \\frac{1}{2}\\frac{4ac}{b^2}\\right) = b\\left(1 - \\frac{2ac}{b^2}\\right) = b - \\frac{2ac}{b}\n$$\nThe numerator of $x_{\\mathrm{small}}$ is $-b + \\sqrt{b^{2} - 4ac}$. Substituting the approximation, we see:\n$$\n-b + \\sqrt{b^{2} - 4ac} \\approx -b + \\left(b - \\frac{2ac}{b}\\right) = -\\frac{2ac}{b}\n$$\nThis shows that the numerator is the result of subtracting two nearly equal quantities: $-b$ and $\\sqrt{b^{2} - 4ac} \\approx b$. This situation is known as catastrophic cancellation. When two nearly equal numbers are subtracted in floating-point arithmetic, the result can have a large relative error because the leading significant digits cancel, leaving a result dominated by the less significant, and potentially erroneous, digits from the original numbers.\n\nTo quantify the error amplification, we use the given floating-point model $\\operatorname{fl}(z) = z(1+\\delta)$ with $|\\delta| \\leq u$. Let $d = \\sqrt{b^2-4ac}$. The computed value of the square root is $\\hat{d} = \\operatorname{fl}(d) = d(1+\\delta_1)$ for some $|\\delta_1| \\leq u$. The subsequent subtraction is computed as $\\hat{N} = \\operatorname{fl}(-b + \\hat{d}) = (-b + \\hat{d})(1+\\delta_2)$ for some $|\\delta_2| \\leq u$. We assume $b$ is represented exactly.\n\nThe exact numerator is $N = -b+d$. The computed numerator is:\n$$\n\\hat{N} = (-b + d(1+\\delta_1))(1+\\delta_2) = (-b+d + d\\delta_1)(1+\\delta_2) = (N + d\\delta_1)(1+\\delta_2)\n$$\nExpanding and keeping only first-order terms in $\\delta_i$:\n$$\n\\hat{N} \\approx N + d\\delta_1 + N\\delta_2\n$$\nThe absolute error in the numerator is $\\hat{N}-N \\approx d\\delta_1 + N\\delta_2$. The relative error is:\n$$\n\\frac{\\hat{N}-N}{N} \\approx \\frac{d\\delta_1}{N} + \\delta_2\n$$\nWe have already established that $d \\approx b$ and $N \\approx -2ac/b$. Substituting these approximations into the relative error expression:\n$$\n\\frac{\\hat{N}-N}{N} \\approx \\frac{b}{-2ac/b}\\delta_1 + \\delta_2 = -\\frac{b^2}{2ac}\\delta_1 + \\delta_2\n$$\nThe relative error $\\delta_1$ from the square root computation is amplified by a factor of $-\\frac{b^2}{2ac}$. Letting $\\epsilon = \\frac{4ac}{b^2}$, this factor is $-\\frac{2}{\\epsilon}$. Since $\\epsilon$ is small, the amplification factor is very large. This confirms that the direct evaluation of the formula for $x_{\\mathrm{small}}$ is numerically unstable under the given conditions.\n\n2. Derivation of a Stable Formula\n\nTo find a stable formula, we perform a change of variables $y = 1/x$. Substituting $x=1/y$ into the quadratic equation $ax^2 + bx + c = 0$ (assuming $x\\neq 0$, which is true if $c\\neq 0$):\n$$\na\\left(\\frac{1}{y}\\right)^2 + b\\left(\\frac{1}{y}\\right) + c = 0\n$$\nMultiplying the entire equation by $y^2$ (assuming $y \\ne 0$) yields the quadratic equation for $y$:\n$$\nc y^{2} + b y + a = 0\n$$\nThe roots of this equation for $y$ are given by the quadratic formula:\n$$\ny = \\frac{-b \\pm \\sqrt{b^{2} - 4ca}}{2c}\n$$\nThe roots of the $y$ equation, say $y_1$ and $y_2$, are the reciprocals of the roots of the original equation for $x$, $x_1$ and $x_2$. The root we are interested in is $x_{\\mathrm{small}}$, which is the root smaller in magnitude. Its reciprocal, $y_{\\mathrm{large}} = 1/x_{\\mathrm{small}}$, will be the root larger in magnitude.\n\nWe need to choose the sign in the formula for $y$ that corresponds to the larger-magnitude root and is numerically stable. The two choices for the numerator are $-b + \\sqrt{b^2-4ac}$ and $-b - \\sqrt{b^2-4ac}$. Given $b>0$ and $\\sqrt{b^2-4ac} \\approx b > 0$, the first choice involves subtraction of nearly equal numbers (unstable), while the second choice, $-b - \\sqrt{b^2-4ac}$, involves the sum of two negative numbers. This sum is numerically stable as there is no cancellation of leading digits.\n\nTherefore, we choose the minus sign to stably compute one of the roots for $y$:\n$$\ny_{\\mathrm{stable}} = \\frac{-b - \\sqrt{b^{2} - 4ac}}{2c}\n$$\nSince this is a sum of two large negative terms in the numerator (for $b>0$), this will be the root with the larger magnitude. Thus, $y_{\\mathrm{stable}} = y_{\\mathrm{large}} = 1/x_{\\mathrm{small}}$.\n\nWe can now find the desired stable expression for $x_{\\mathrm{small}}$ by taking the reciprocal:\n$$\nx_{\\mathrm{small}} = \\frac{1}{y_{\\mathrm{stable}}} = \\frac{2c}{-b - \\sqrt{b^{2} - 4ac}}\n$$\nThis formula avoids catastrophic cancellation because the denominator involves an addition of two numbers of the same sign. This is algebraically equivalent to the original formula for $x_{\\mathrm{small}}$, which can be shown by multiplying the numerator and denominator of the original formula by its conjugate, $-b - \\sqrt{b^2 - 4ac}$.\n\n3. Numerical Application\n\nWe are given the coefficients $a=1$, $b=2 \\times 10^8$, and $c=3$. The condition $b>0$ is met. We check $b^2 \\gg 4ac$: $b^2 = (2 \\times 10^8)^2 = 4 \\times 10^{16}$ and $4ac = 4(1)(3)=12$. Indeed, $4 \\times 10^{16} \\gg 12$.\n\nWe must use the numerically stable formula for the smaller-magnitude root, $x_{\\mathrm{small}}$:\n$$\nx_{\\mathrm{small}} = \\frac{2c}{-b - \\sqrt{b^{2} - 4ac}}\n$$\nSubstituting the given values:\n$$\nx_{\\mathrm{small}} = \\frac{2(3)}{-(2 \\times 10^{8}) - \\sqrt{(2 \\times 10^{8})^{2} - 4(1)(3)}} = \\frac{6}{-2 \\times 10^8 - \\sqrt{4 \\times 10^{16} - 12}}\n$$\nA direct calculation with a standard double-precision calculator might fail, as $\\operatorname{fl}(4 \\times 10^{16} - 12)$ could evaluate to $4 \\times 10^{16}$, leading to a numerator of $-b+b=0$. However, using the stable formula:\nThe denominator is approximately $-2 \\times 10^8 - \\sqrt{4 \\times 10^{16}} = -2 \\times 10^8 - 2 \\times 10^8 = -4 \\times 10^8$.\nSo, $x_{\\mathrm{small}} \\approx \\frac{6}{-4 \\times 10^8} = -1.5 \\times 10^{-8}$.\n\nTo obtain the result to $8$ significant figures, a high-precision calculation is necessary.\n$$\nx_{\\mathrm{small}} = \\frac{6}{-200000000 - \\sqrt{39999999999999988}}\n$$\nEvaluating this expression yields:\n$$\nx_{\\mathrm{small}} \\approx -1.500000000000000075 \\times 10^{-8}\n$$\nRounding this value to $8$ significant figures gives:\n$$\nx_{\\mathrm{small}} \\approx -1.5000000 \\times 10^{-8}\n$$\nThe first significant figure is $1$, the second is $5$, and the next six are all $0$. The ninth significant figure is $0$, so no rounding up is required.", "answer": "$$\\boxed{-1.5000000 \\times 10^{-8}}$$", "id": "3273565"}, {"introduction": "Catastrophic cancellation extends beyond simple formulas to entire algorithms, with profound implications for data analysis. This practice investigates the computation of statistical variance, a cornerstone of data science. You will empirically demonstrate how the common \"one-pass\" algorithm, while efficient, can fail spectacularly when the standard deviation of a dataset is small compared to its mean. Through a hands-on coding exercise, you will contrast this naive approach with the more robust two-pass algorithm, gaining a practical understanding of how algorithmic choices are critical for achieving reliable results in scientific computing. [@problem_id:3273473]", "problem": "Consider the computation of the population variance from a finite dataset in floating-point arithmetic. The population variance is defined from first principles as $$\\mathrm{Var}(x) = \\frac{1}{n}\\sum_{i=1}^{n}\\left(x_i - \\mu\\right)^2,\\quad \\mu = \\frac{1}{n}\\sum_{i=1}^{n}x_i,$$ where $n$ is the number of data points. In floating-point arithmetic obeying the IEEE 754 standard (Institute of Electrical and Electronics Engineers 754) for binary64 format (commonly called double precision), each elementary operation is modeled as $$\\mathrm{fl}(a \\circ b) = (a \\circ b)(1+\\delta),\\quad |\\delta| \\le u,$$ where $u$ is the unit roundoff. For round-to-nearest, $u = 2^{-53}$.\n\nYour task is to write a complete program that empirically analyzes loss of significance for variance when the dataset has a large location offset and a small spread. Specifically:\n\n1) Implement two variance computations in binary64 arithmetic:\n   - A two-pass “definition-based” computation that directly evaluates $$\\mathrm{Var}(x) = \\frac{1}{n}\\sum_{i=1}^{n}\\left(x_i - \\mu\\right)^2,$$ where $\\mu$ is computed in a first pass, and the sum of squared deviations in a second pass.\n   - A single-pass computation that accumulates only the sums of $x_i$ and $x_i^2$ in a single traversal and then forms the algebraically equivalent population variance from these two sums using standard real-number algebra. Do not use compensated summation or any multi-pass correction beyond this single traversal and final algebraic combination.\n\n2) Use the following deterministic test suite of datasets. For each test, construct $x_i$ by choosing an integer pattern $k_i \\in \\{-1,0,+1\\}$ with equal counts so that $\\sum_{i=1}^{n} k_i = 0$ and then setting $$x_i = M + d\\,k_i.$$ Concretely, take the first $n/3$ entries with $k_i = -1$, the next $n/3$ with $k_i = 0$, and the last $n/3$ with $k_i = +1$. This yields an exact population variance in real arithmetic equal to $$V_{\\mathrm{true}} = \\mathbb{E}[d^2 k^2] = \\frac{2}{3}\\,d^2.$$\n\n   Use these four test cases (each with $n$ divisible by $3$):\n   - Test A: $n = 6000$, $M = 0$, $d = 10^{-3}$.\n   - Test B: $n = 6000$, $M = 10^{6}$, $d = 1$.\n   - Test C: $n = 6000$, $M = 10^{12}$, $d = 1$.\n   - Test D: $n = 6000$, $M = 10^{16}$, $d = 1$.\n\n3) For each test case, compute:\n   - The naive single-pass variance value $V_{\\mathrm{naive}}$ as described above.\n   - The two-pass variance value $V_{\\mathrm{2pass}}$ as described above.\n   - The relative error of the naive method with respect to the exact real-value variance $V_{\\mathrm{true}}$: $$\\varepsilon_{\\mathrm{rel}} = \\frac{|V_{\\mathrm{naive}} - V_{\\mathrm{true}}|}{V_{\\mathrm{true}}}.$$\n   - The achieved correct significant digits for the naive method, defined as $$D_{\\mathrm{ach}} = \\max\\left(0,\\;\\min\\left(-\\log_{10}(\\varepsilon_{\\mathrm{rel}}),\\;-\\log_{10}(u)\\right)\\right),$$ where the cap by $-\\log_{10}(u)$ reflects the best-possible digits in binary64 arithmetic.\n   - A first-principles prediction of the correct significant digits retained by the naive method based on cancellation analysis. When $|M| \\gg |d|$, the naive formula subtracts two large, nearly equal sums of order $\\Theta(n M^2)$ to obtain a result of order $\\Theta(n d^2)$, so a rough model gives $$\\varepsilon_{\\mathrm{rel}} \\approx C\\,u\\left(\\frac{M}{d}\\right)^2,$$ for some problem-dependent constant $C$ of order $1$. Translate this to a digit prediction by ignoring $C$ and capping by $-\\log_{10}(u)$: $$D_{\\mathrm{pred}} = \\max\\left(0,\\;\\min\\left(-\\log_{10}(u) - 2\\log_{10}\\left(\\frac{|M|}{|d|}\\right),\\;-\\log_{10}(u)\\right)\\right),$$ with the convention that if $M = 0$ then $D_{\\mathrm{pred}} = -\\log_{10}(u)$.\n\n4) Final output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a bracketed 3-tuple in the order $[\\varepsilon_{\\mathrm{rel}}, D_{\\mathrm{ach}}, D_{\\mathrm{pred}}]$. Thus the full output is a single line of the form\n   $$[[e_A, dA_{\\mathrm{ach}}, dA_{\\mathrm{pred}}],[e_B, dB_{\\mathrm{ach}}, dB_{\\mathrm{pred}}],[e_C, dC_{\\mathrm{ach}}, dC_{\\mathrm{pred}}],[e_D, dD_{\\mathrm{ach}}, dD_{\\mathrm{pred}}]].$$\n\nConstraints and notes:\n- Use only binary64 arithmetic; do not employ extended precision, arbitrary precision, compensated summation, or stochastic rounding.\n- All answers are pure numbers with no physical units.\n- Angles are not involved.\n- The output must be exactly one line in the specified bracketed list format with no spaces.", "solution": "The objective is to empirically analyze the numerical stability of two different algorithms for computing the population variance of a dataset, particularly in the presence of a large mean offset relative to the data's spread. The analysis will be conducted using binary64 floating-point arithmetic.\n\nThe population variance of a dataset $\\{x_i\\}_{i=1}^{n}$ is defined as the mean of the squared deviations from the population mean $\\mu$:\n$$\n\\mathrm{Var}(x) = \\frac{1}{n}\\sum_{i=1}^{n}\\left(x_i - \\mu\\right)^2, \\quad \\text{where} \\quad \\mu = \\frac{1}{n}\\sum_{i=1}^{n}x_i\n$$\n\nWe are asked to implement and compare two computational methods:\n\n$1$. **Two-Pass Algorithm:** This method directly implements the definition.\n    - **Pass 1:** Compute the mean $\\mu = \\frac{1}{n}\\sum_{i=1}^{n}x_i$.\n    - **Pass 2:** Compute the sum of squared differences using the pre-computed mean, yielding $\\mathrm{Var}_{\\mathrm{2pass}}(x) = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\mu)^2$.\n    This algorithm is generally numerically stable. By first computing the mean and then the deviations, it centers the data around $0$ before squaring. This prevents the intermediate quantities $(x_i - \\mu)^2$ from becoming excessively large if the original data $x_i$ is far from the origin, thereby preserving precision.\n\n$2$. **Naive Single-Pass Algorithm:** This method uses an algebraically equivalent formula derived as follows:\n$$\n\\mathrm{Var}(x) = \\mathbb{E}[(x-\\mu)^2] = \\mathbb{E}[x^2 - 2x\\mu + \\mu^2] = \\mathbb{E}[x^2] - 2\\mu\\mathbb{E}[x] + \\mu^2 = \\mathbb{E}[x^2] - \\mu^2\n$$\nIn terms of sums, this is:\n$$\n\\mathrm{Var}_{\\mathrm{naive}}(x) = \\left(\\frac{1}{n}\\sum_{i=1}^{n}x_i^2\\right) - \\left(\\frac{1}{n}\\sum_{i=1}^{n}x_i\\right)^2\n$$\nThis allows for the computation of variance in a single pass by accumulating $\\sum x_i$ and $\\sum x_i^2$ simultaneously. However, this method is numerically unstable if the standard deviation is much smaller than the mean. If the data points $x_i$ are all clustered far from the origin, i.e., $x_i \\approx M$ for a large $M$, then $\\mathbb{E}[x^2] \\approx M^2$ and $\\mu^2 \\approx M^2$. The formula involves the subtraction of two large, nearly equal numbers, an operation known as **catastrophic cancellation**, which leads to a significant loss of precision.\n\nThe problem defines a specific test suite to demonstrate this effect. The datasets are constructed as $x_i = M + d\\,k_i$, where $n$ is the total number of points, and we have $n/3$ points for each $k_i \\in \\{-1, 0, +1\\}$.\nFor this dataset structure, the exact mean in real arithmetic is:\n$$\n\\mu = \\mathbb{E}[M+d k] = M + d\\,\\mathbb{E}[k] = M + d\\left(\\frac{1}{3}(-1) + \\frac{1}{3}(0) + \\frac{1}{3}(+1)\\right) = M\n$$\nThe exact population variance $V_{\\mathrm{true}}$ in real arithmetic is:\n$$\nV_{\\mathrm{true}} = \\mathrm{Var}(x) = \\mathbb{E}[(x-\\mu)^2] = \\mathbb{E}[((M+dk) - M)^2] = \\mathbb{E}[(dk)^2] = d^2\\mathbb{E}[k^2]\n$$\nThe expectation of $k^2$ is:\n$$\n\\mathbb{E}[k^2] = \\frac{1}{3}(-1)^2 + \\frac{1}{3}(0)^2 + \\frac{1}{3}(+1)^2 = \\frac{1}{3} + 0 + \\frac{1}{3} = \\frac{2}{3}\n$$\nThus, the exact variance for all test cases is $V_{\\mathrm{true}} = \\frac{2}{3}d^2$.\n\nThe four test cases are designed to vary the ratio $|M/d|$:\n-   Test A: $n = 6000$, $M = 0$, $d = 10^{-3}$. Here $|M/d| = 0$.\n-   Test B: $n = 6000$, $M = 10^{6}$, $d = 1$. Here $|M/d| = 10^6$.\n-   Test C: $n = 6000$, $M = 10^{12}$, $d = 1$. Here $|M/d| = 10^{12}$.\n-   Test D: $n = 6000$, $M = 10^{16}$, $d = 1$. Here $|M/d| = 10^{16}$.\n\nFor each test, we will compute the following quantities for the naive method:\n$1$. The relative error $\\varepsilon_{\\mathrm{rel}} = \\frac{|V_{\\mathrm{naive}} - V_{\\mathrm{true}}|}{V_{\\mathrm{true}}}$.\n$2$. The achieved significant digits $D_{\\mathrm{ach}} = \\max\\left(0,\\;\\min\\left(-\\log_{10}(\\varepsilon_{\\mathrm{rel}}),\\;-\\log_{10}(u)\\right)\\right)$. This quantifies the actual accuracy, capped by the theoretical maximum precision of binary64 arithmetic, where $u = 2^{-53}$ is the unit roundoff, and $-\\log_{10}(u) \\approx 15.95$.\n$3$. The predicted significant digits $D_{\\mathrm{pred}}$. This is based on a first-order error analysis of the catastrophic cancellation, which suggests the relative error grows as $\\varepsilon_{\\mathrm{rel}} \\propto u(M/d)^2$. Ignoring the constant of proportionality, the number of lost digits is approximately $2\\log_{10}(|M/d|)$. The prediction is thus given by $D_{\\mathrm{pred}} = \\max\\left(0,\\;\\min\\left(-\\log_{10}(u) - 2\\log_{10}\\left(\\frac{|M|}{|d|}\\right),\\;-\\log_{10}(u)\\right)\\right)$. For $M=0$, the cancellation does not occur, and the predicted digits are maximal, $-\\log_{10}(u)$.\n\nThe program will iterate through the four test cases, construct the dataset for each, compute $V_{\\mathrm{naive}}$, and then calculate the tuple $(\\varepsilon_{\\mathrm{rel}}, D_{\\mathrm{ach}}, D_{\\mathrm{pred}})$. The final output will be a list of these tuples.\nAs expected, the results will show that for Test A ($M=0$), the naive method is highly accurate. For Tests B, C, and D, as the ratio $|M/d|$ increases, the catastrophic cancellation becomes more severe, leading to a dramatic increase in relative error and a corresponding decrease in the number of correct significant digits, with the achieved digits closely matching the theoretical prediction. test C and D will likely result in a complete loss of significant digits.", "answer": "```\n[[0.0,15.954589770195254,15.954589770195254],[2.2107391972073315e-05,4.655410229804745,3.9545897701952544],[1.5,0.0,0.0],[1.0,0.0,0.0]]\n```", "id": "3273473"}]}