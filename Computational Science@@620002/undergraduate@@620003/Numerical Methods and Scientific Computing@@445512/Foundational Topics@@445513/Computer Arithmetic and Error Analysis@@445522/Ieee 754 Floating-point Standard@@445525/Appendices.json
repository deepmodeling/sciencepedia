{"hands_on_practices": [{"introduction": "A deep understanding of the IEEE 754 standard begins with the ability to interpret any raw bit pattern. This foundational practice challenges you to build a classifier from first principles, using bitwise operations to dissect the sign, exponent, and fraction fields. Mastering this skill is essential for debugging numerical issues and truly grasping how floating-point numbers are encoded [@problem_id:3240373].", "problem": "You are to implement, from first principles, a classifier for the Institute of Electrical and Electronics Engineers (IEEE) 754 binary64 encoding (commonly called double precision). The classifier will take as input a set of fixed unsigned $64$-bit patterns and, for each one, determine whether the pattern represents a normal finite number, a subnormal number, a signed zero, a signed infinity, or a Not-a-Number (NaN). You must justify the classification logic by starting from the field-level definition of binary64.\n\nStart from the following core definitions that constitute the fundamental base:\n- A binary64 datum consists of a sign bit $s$ ($1$ bit), an exponent field $E$ ($11$ bits), and a fraction field $F$ ($52$ bits). The stored exponent uses a bias of $1023$.\n- The encoding defines the numerical interpretation by combining $s$, $E$, and $F$ according to the standard. Your classification must be derived by reasoning from these field definitions and the meaning of the exponent extremes $E = 0$ and $E = 2^{11}-1$.\n- The classification must account for edge cases where the exponent field is all zeros or all ones, and it must rigorously distinguish the roles of $s$, $E$, and $F$.\n\nYour program must:\n- Implement a function that, given an unsigned $64$-bit integer pattern, extracts $s$, $E$, and $F$ and classifies the datum into exactly one of the following categories, encoded as integers:\n  - $0$: normal finite\n  - $1$: subnormal\n  - $2$: $+0$\n  - $3$: $-0$\n  - $4$: $+\\infty$\n  - $5$: $-\\infty$\n  - $6$: NaN\n- Use only bit-level manipulation of the $64$-bit pattern; do not convert to native floating-point for classification.\n\nExplain in your solution how each classification follows from the binary64 field definitions. In particular, explain the edge cases for $E = 0$ and $E = 2^{11}-1$ and how the sign bit $s$ affects only certain categories.\n\nTest suite:\nClassify the following ten unsigned $64$-bit patterns, given in hexadecimal. Treat each as a raw bit pattern according to the binary64 encoding.\n- $\\texttt{0x3FF0000000000000}$\n- $\\texttt{0x0000000000000001}$\n- $\\texttt{0x8000000000000001}$\n- $\\texttt{0x0000000000000000}$\n- $\\texttt{0x8000000000000000}$\n- $\\texttt{0x7FF0000000000000}$\n- $\\texttt{0xFFF0000000000000}$\n- $\\texttt{0x7FF8000000000001}$\n- $\\texttt{0x7FEFFFFFFFFFFFFF}$\n- $\\texttt{0x0010000000000000}$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of the integer codes, in the same order as the test suite, enclosed in square brackets. For example, if there were three test cases and their codes were $0$, $1$, and $6$, the output would be $\\texttt{[0,1,6]}$.", "solution": "The problem of classifying a given $64$-bit pattern according to the IEEE 754 binary64 standard is a well-defined task that relies on the partitioning of the bit pattern into three distinct fields: the sign bit ($s$), the exponent field ($E$), and the fraction field ($F$). The interpretation of the bit pattern is determined by a set of rules based on the values of these fields. The problem is valid as it is scientifically grounded in the universally accepted IEEE 754 standard, is self-contained, and provides all necessary information for a deterministic classification.\n\nA $64$-bit binary64 pattern is structured as follows:\n- **Sign bit ($s$)**: $1$ bit (bit $63$)\n- **Exponent field ($E$)**: $11$ bits (bits $62$ through $52$)\n- **Fraction field ($F$)**: $52$ bits (bits $51$ through $0$)\n\nThe numerical value $v$ represented by a given pattern is a function of $s$, $E$, and $F$. The classification of the number into categories such as normal, subnormal, zero, infinity, or Not-a-Number (NaN) depends primarily on the special values of the exponent field, $E$. The two critical cases for $E$ are when its bits are all zeros ($E=0$) or all ones ($E=2^{11}-1 = 2047$).\n\nTo implement the classifier, we must first extract these three fields from a given $64$-bit unsigned integer pattern, which we denote as $p$. This is achieved using bitwise operations: masks and right shifts.\n- The sign bit $s$ is the most significant bit (MSB). It can be extracted by shifting the pattern $63$ bits to the right:\n  $$s = (p \\gg 63) \\;\\\\; 1$$\n- The exponent field $E$ consists of the next $11$ bits. We can isolate these bits by first shifting the pattern $52$ bits to the right to remove the fraction, and then applying a mask to keep only the lowermost $11$ bits of the result. The mask for $11$ bits is $2^{11}-1$, which is $2047$ in decimal or $\\texttt{0x7FF}$ in hexadecimal.\n  $$E = (p \\gg 52) \\;\\\\; \\texttt{0x7FF}$$\n- The fraction field $F$ comprises the least significant $52$ bits. We can extract it by applying a mask that keeps only these $52$ bits. The mask is $2^{52}-1$, or $\\texttt{0x000FFFFFFFFFFFFF}$ in hexadecimal.\n  $$F = p \\;\\\\; \\texttt{0x000FFFFFFFFFFFFF}$$\n\nWith $s$, $E$, and $F$ extracted, the classification proceeds according to the rules defined by the IEEE 754 standard, which form a decision tree based on the values of $E$ and $F$.\n\n**Case 1: $E$ is all ones ($E = 2047$)**\nThis case represents the special values of infinity and NaN. The choice between them depends on the fraction field, $F$.\n- If $F = 0$: The pattern represents **infinity**. The sign of the infinity is determined by the sign bit, $s$.\n    - If $s=0$, it is positive infinity ($+\\infty$). This corresponds to code $4$.\n    - If $s=1$, it is negative infinity ($-\\infty$). This corresponds to code $5$.\n    The value is given by $v = (-1)^s \\times \\infty$.\n- If $F \\neq 0$: The pattern represents a **Not-a-Number (NaN)**. The standard distinguishes between quiet NaNs (qNaN) and signaling NaNs (sNaN) based on the most significant bit of $F$, but for this problem, all such patterns are classified simply as NaN. The sign bit $s$ is part of the NaN's payload but does not affect its classification as a NaN. All patterns with $E=2047$ and $F \\neq 0$ are classified as NaN. This corresponds to code $6$.\n\n**Case 2: $E$ is all zeros ($E = 0$)**\nThis case represents zero and subnormal (or denormalized) numbers. The distinction is made based on the fraction field, $F$. These numbers have a different interpretation for their value, with an effective exponent fixed at the minimum value and no implicit leading $1$ in the significand.\n- If $F = 0$: The pattern represents **zero**. The IEEE 754 standard includes both positive and negative zero, distinguished by the sign bit $s$.\n    - If $s=0$, it is positive zero ($+0$). This corresponds to code $2$.\n    - If $s=1$, it is negative zero ($-0$). This corresponds to code $3$.\n    The value is $v = (-1)^s \\times 0$.\n- If $F \\neq 0$: The pattern represents a **subnormal number**. These numbers allow for gradual underflow by sacrificing precision. The value is given by $v = (-1)^s \\times 2^{1-\\text{bias}} \\times (0.F)_2$, where $\\text{bias} = 1023$ and $(0.F)_2$ is the fraction $F$ interpreted as the fractional part of a binary number. The sign is determined by $s$, but the problem specifies a single category for all subnormal numbers. This corresponds to code $1$.\n\n**Case 3: $E$ is neither all zeros nor all ones ($0  E  2047$)**\nThis is the case for all **normal finite numbers**. For these numbers, the exponent is calculated by subtracting a bias of $1023$ from the field value $E$. The significand (the combination of the fraction field and an implicit leading bit) is $1.F$. The value of a normal number is given by:\n$$v = (-1)^s \\times 2^{E-1023} \\times (1.F)_2$$\nThe sign bit $s$ determines if the number is positive or negative, and the fields $E$ and $F$ determine its magnitude. However, all numbers in this range, regardless of their sign or magnitude, fall into the single category of \"normal finite\". This corresponds to code $0$.\n\nThis set of rules is complete and unambiguous, providing a unique classification for any of the $2^{64}$ possible bit patterns. The implementation will follow this logic directly.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Classifies a set of IEEE 754 binary64 patterns and prints the results.\n    \"\"\"\n\n    def classify_binary64(pattern: np.uint64) - int:\n        \"\"\"\n        Classifies a 64-bit pattern according to IEEE 754 binary64 rules.\n\n        The classification logic is based on the field definitions:\n        - s: sign (1 bit)\n        - E: exponent (11 bits)\n        - F: fraction (52 bits)\n\n        The classification codes are:\n        - 0: normal finite\n        - 1: subnormal\n        - 2: +0\n        - 3: -0\n        - 4: +infinity\n        - 5: -infinity\n        - 6: NaN\n\n        Args:\n            pattern: An unsigned 64-bit integer representing the bit pattern.\n\n        Returns:\n            An integer code representing the classification.\n        \"\"\"\n        # Define masks to extract the fields from the 64-bit pattern.\n        # These are defined as uint64 to match the pattern type.\n        sign_mask = np.uint64(0x8000000000000000)\n        exponent_mask = np.uint64(0x7FF0000000000000)\n        fraction_mask = np.uint64(0x000FFFFFFFFFFFFF)\n\n        # Extract the sign bit 's'.\n        # s = 0 for positive, s = 1 for negative.\n        s = 1 if (pattern  sign_mask) != 0 else 0\n\n        # Extract the 11-bit exponent field 'E'.\n        # The exponent is bits 62-52. Shift right by 52.\n        E = np.uint64((pattern  exponent_mask) >> np.uint64(52))\n\n        # Extract the 52-bit fraction field 'F'.\n        F = pattern  fraction_mask\n\n        # Maximum possible value for the 11-bit exponent field.\n        E_max = np.uint64(0x7FF)  # 2^11 - 1 = 2047\n\n        # Classification logic based on the values of E and F.\n\n        # Case 1: E is all ones (E = 2047). Special values: infinity or NaN.\n        if E == E_max:\n            if F == 0:\n                # Infinity\n                return 4 if s == 0 else 5  # +inf or -inf\n            else:\n                # Not-a-Number (NaN)\n                return 6\n\n        # Case 2: E is all zeros (E = 0). Special values: zero or subnormal.\n        elif E == 0:\n            if F == 0:\n                # Zero\n                return 2 if s == 0 else 3  # +0 or -0\n            else:\n                # Subnormal (denormalized) number\n                return 1\n\n        # Case 3: 0  E  2047. Normal finite numbers.\n        else:\n            return 0  # Normal finite number\n\n    # The test suite provided in the problem statement.\n    test_cases_hex = [\n        \"0x3FF0000000000000\", # 1.0 (normal)\n        \"0x0000000000000001\", # Smallest positive subnormal\n        \"0x8000000000000001\", # Smallest negative subnormal\n        \"0x0000000000000000\", # +0\n        \"0x8000000000000000\", # -0\n        \"0x7FF0000000000000\", # +infinity\n        \"0xFFF0000000000000\", # -infinity\n        \"0x7FF8000000000001\", # Quiet NaN\n        \"0x7FEFFFFFFFFFFFFF\", # Largest normal finite number\n        \"0x0010000000000000\", # Smallest positive normal number\n    ]\n\n    # Convert hex strings to numpy.uint64 integers.\n    test_cases = [np.uint64(int(h, 16)) for h in test_cases_hex]\n\n    # Classify each test case.\n    results = [classify_binary64(case) for case in test_cases]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3240373"}, {"introduction": "After learning to decode the structure of floating-point numbers, we now explore the practical limits of their finite precision. This exercise guides you to discover the largest number $x$ for which the computation $1.0 + x$ still rounds back to $1.0$ under IEEE 754 rules [@problem_id:3240413]. This value, known as the unit roundoff, is a fundamental quantity in numerical analysis that quantifies the resolution of the floating-point system.", "problem": "Consider addition under the Institute of Electrical and Electronics Engineers (IEEE) 754 binary32 format using the default rounding mode, rounding to nearest with ties to even. A normalized binary32 number has the form $m \\times 2^{E}$ with $m \\in [1,2)$ and a $23$-bit fractional part, and $E$ an integer exponent with bias $127$. Let $1.0$ denote the binary32 number with $m = 1$ and $E = 0$. Define $x$ to be a positive, finite binary32 number. Using only the structural properties of the binary32 representation and the rounding rule described above, determine the largest such $x$ for which evaluating $1.0 + x$ in binary32 arithmetic yields $1.0$ as the final rounded result. Express your answer as an exact power of two. Also, give the standard numerical analysis term by which this value is known. The final numerical value must be written as a single exact expression; no rounding is required.", "solution": "The user wants to find the largest positive, finite IEEE 754 binary32 number $x$ such that the operation $1.0 + x$, when performed using binary32 arithmetic with the round-to-nearest, ties-to-even mode, results in $1.0$.\n\nFirst, we analyze the representation of numbers in the IEEE 754 binary32 format. A normalized number is of the form $(-1)^s \\times m \\times 2^E$, where $s$ is the sign bit, $m$ is the significand (or mantissa), and $E$ is the exponent.\nFor binary32:\n- The significand $m$ has a precision of $p=24$ bits (1 implicit leading bit and 23 explicit fractional bits). For a normalized number, $m$ is of the form $1.f$ where $f$ is the $23$-bit fractional part, so $m \\in [1, 2)$.\n- The exponent $E$ is stored in a biased form, with a bias of $127$. The valid range for the true exponent $E$ for normalized numbers is $[-126, 127]$.\n\nThe number $1.0$ is represented in this format. Its value is $1.0 \\times 2^0$.\n- The sign is positive, so $s=0$.\n- The exponent is $E=0$. The stored biased exponent is $0+127=127$.\n- The significand is $m=1.0$. In binary, this is $1.000...0_2$. The implicit bit is $1$, and the $23$ fractional bits are all $0$. Let's denote the significand of $1.0$ as $m_1$.\n$m_1 = 1.\\underbrace{000...000}_{23 \\text{ bits}}$\n\nThe problem requires that the computed sum of $1.0$ and $x$ rounds back to $1.0$. Let this computed sum be denoted by $\\text{fl}(1.0+x)$. We are looking for the largest representable number $x > 0$ such that $\\text{fl}(1.0+x) = 1.0$.\n\nThe rounding mode is \"round to nearest, ties to even\". This means that an exact result is rounded to the nearest representable floating-point number. If the exact result is exactly halfway between two representable numbers, it is rounded to the one whose significand has a least significant bit (LSB) of $0$ (the \"even\" one).\n\nLet's identify the two representable numbers relevant to this problem:\n1.  The number $y_1 = 1.0$.\n2.  The next representable number greater than $1.0$, which we'll call $y_2$.\n\nTo find $y_2$, we increment the LSB of the significand of $y_1$. The LSB of the $23$-bit fractional part corresponds to a value of $2^{-23}$.\nSo, $y_2 = 1.0 + 2^{-23}$.\nIts significand is $m_2 = 1.\\underbrace{000...001}_{23 \\text{ bits}}$.\n\nFor the result of an operation to round to $y_1=1.0$, the exact result must be closer to $y_1$ than to $y_2$, or be exactly at the midpoint. Let's find the midpoint $M$ between $y_1$ and $y_2$:\n$$M = \\frac{y_1 + y_2}{2} = \\frac{1.0 + (1.0 + 2^{-23})}{2} = \\frac{2.0 + 2^{-23}}{2} = 1.0 + 2^{-24}$$\n\nAccording to the rounding rule:\n- If the exact sum $1.0+x  M$, it rounds down to $y_1=1.0$.\n- If the exact sum $1.0+x > M$, it rounds up to $y_2=1.0+2^{-23}$.\n- If the exact sum $1.0+x = M$, we have a tie. In this case, we round to the \"even\" number. The significand of $y_1=1.0$ ends in $0$, while the significand of $y_2=1.0+2^{-23}$ ends in $1$. Thus, $y_1$ is the \"even\" number, and the result rounds down to $1.0$.\n\nCombining these conditions, for $\\text{fl}(1.0+x)$ to be $1.0$, the exact sum must satisfy:\n$$1.0 + x \\le M$$\n$$1.0 + x \\le 1.0 + 2^{-24}$$\nThis simplifies to:\n$$x \\le 2^{-24}$$\n\nThe problem asks for the largest *representable binary32 number* $x$ that fulfills this condition. We must check if the value $2^{-24}$ is itself a representable binary32 number.\nWe can write $2^{-24}$ in the normalized form $m \\times 2^E$:\n$$2^{-24} = 1.0 \\times 2^{-24}$$\n- The significand is $m=1.0$. This is a valid significand for a normalized number (implicit bit $1$, fractional part all zeros).\n- The exponent is $E=-24$. The biased exponent is $E_{biased} = -24 + 127 = 103$. This value is within the valid range for biased exponents of normalized numbers, which is $[1, 254]$.\n\nSince $2^{-24}$ is a valid normalized binary32 number, and we require $x \\le 2^{-24}$ where $x$ must also be a representable number, the largest possible value for $x$ is exactly $2^{-24}$.\n\nThe problem also asks for the standard numerical analysis term for this value. This value, representing half the gap between $1.0$ and the next representable number, is known as the **unit roundoff**. It is often denoted by $u$. For a floating-point system with base $\\beta$ and precision $p$ using round-to-nearest, the unit roundoff is defined as $u = \\frac{1}{2}\\beta^{1-p}$. For binary32, $\\beta=2$ and $p=24$, so $u = \\frac{1}{2}(2)^{1-24} = 2^{-1} \\cdot 2^{-23} = 2^{-24}$. This value represents the maximum relative error when rounding a number to the nearest floating-point representation. It is half of the machine epsilon, $\\epsilon = 2^{-23}$.\n\nTherefore, the largest such $x$ is $2^{-24}$.", "answer": "$$\\boxed{2^{-24}}$$", "id": "3240413"}, {"introduction": "Building on our understanding of representation and precision, this final practice traces a seemingly simple calculation, $5 \\times 0.2$, from start to finish. You will see firsthand how a common decimal number like $0.2$ is represented inexactly in binary, and then calculate how this initial rounding error propagates through arithmetic according to the standard's rules [@problem_id:3240494]. This case study reveals the subtle and sometimes counter-intuitive nature of floating-point computation.", "problem": "An engineer is analyzing how decimal fractions are represented and used in arithmetic on the Institute of Electrical and Electronics Engineers (IEEE) 754 binary64 format. In binary64, a normalized finite number is encoded by a sign bit $s \\in \\{0,1\\}$, an $11$-bit biased exponent $E_{\\text{bias}} \\in \\{1,\\dots,2046\\}$ with bias $1023$, and a $52$-bit fraction field $F \\in \\{0,1,\\dots,2^{52}-1\\}$ that represents the significand $1 + F/2^{52}$. The value is\n$$\n(-1)^{s} \\left(1 + \\frac{F}{2^{52}}\\right) 2^{E_{\\text{bias}} - 1023},\n$$\nand basic operations are rounded to nearest, ties to even.\n\nUsing only these definitions and first principles of base conversion and rounding, do the following:\n\n1. Determine the exact binary64 encoding of the decimal number $0.2$ (that is, $1/5$). Identify $s$, the $11$ exponent bits, and the $52$ fraction bits, and also give the consolidated $16$-hex-digit encoding of the $64$-bit word.\n\n2. Now consider evaluating $5 \\times 0.2$ in binary64 arithmetic. Model the computation as: the decimal literal $0.2$ is first converted to its exact binary64 value, then multiplied by the exactly represented integer $5$, and finally rounded once to binary64 according to round-to-nearest, ties-to-even. Let the exact real result be $1$. What is the absolute error, defined as $\\left|\\operatorname{fl}(5 \\times 0.2) - 1\\right|$, of this binary64 computation?\n\nProvide all derivations. As your final answer, report only the absolute error as a single real number. No rounding instruction is needed for the final answer because it is exact.", "solution": "The problem is assessed to be valid as it is scientifically grounded in the IEEE 754 standard, is well-posed with sufficient information, and uses objective, formal language. We may proceed with the solution.\n\nThe problem consists of two parts. First, we must determine the IEEE 754 binary64 representation of the decimal number $0.2$. Second, we must calculate the absolute error of the floating-point computation $5 \\times 0.2$.\n\n### Part 1: Binary64 Encoding of $0.2$\n\nThe value of a normalized binary64 number is given by $v = (-1)^{s} \\left(1 + \\frac{F}{2^{52}}\\right) 2^{e}$, where $e = E_{\\text{bias}} - 1023$.\n\n1.  **Convert to Binary:**\n    The decimal number $0.2$ is equivalent to the fraction $\\frac{1}{5}$. To convert this to binary, we repeatedly multiply by $2$ and record the integer part:\n    $0.2 \\times 2 = 0.4 \\implies 0$\n    $0.4 \\times 2 = 0.8 \\implies 0$\n    $0.8 \\times 2 = 1.6 \\implies 1$\n    $0.6 \\times 2 = 1.2 \\implies 1$\n    The fractional part $0.2$ reappears, so the binary representation is a repeating fraction:\n    $$0.2_{10} = 0.00110011..._{2} = 0.\\overline{0011}_{2}$$\n\n2.  **Normalize:**\n    To fit the IEEE 754 format, we normalize the binary number to the form $1.M \\times 2^e$:\n    $$0.\\overline{0011}_{2} = 1.\\overline{1001}_{2} \\times 2^{-3}$$\n\n3.  **Determine Encoding Fields:**\n    -   **Sign bit ($s$):** The number is positive, so $s=0$.\n    -   **Exponent ($e$):** The true exponent is $e = -3$.\n    -   **Biased Exponent ($E_{\\text{bias}}$):** With a bias of $1023$, the biased exponent is $E_{\\text{bias}} = e + 1023 = -3 + 1023 = 1020$. In $11$-bit binary, this is $1020_{10} = 01111111100_{2}$.\n    -   **Fraction ($F$):** The significand is $1.\\overline{1001}_{2}$. The fraction field $F$ stores the $52$ bits following the implicit leading $1$. The unrounded fraction bits are the first $52$ bits of the repeating pattern `1001...`, which consists of thirteen repetitions of `1001`.\n        $$1.\\underbrace{10011001...1001}_{52 \\text{ bits}} \\vert 1001...$$\n        The first truncated bit (the guard bit, at position $53$) is $1$. The subsequent bits (sticky bits) are $001...$, which are not all zero. According to the \"round-to-nearest, ties-to-even\" rule, when the guard bit is $1$ and the sticky bits are not all zero, we must round up. This involves adding $1$ to the $52$-bit fraction.\n        The unrounded $52$-bit fraction is $(\\underbrace{1001...1001}_{13 \\text{ times}})_2$. Adding $1$ to this binary number gives:\n        $$(\\underbrace{1001...1001}_{12 \\text{ times}}1001)_2 + 1_2 = (\\underbrace{1001...1001}_{12 \\text{ times}}1010)_2$$\n        So, the final $52$ bits for the fraction field $F$ are twelve repetitions of `1001` followed by `1010`.\n\n4.  **Assemble the 64-bit Hexadecimal Representation:**\n    We concatenate the sign, exponent, and fraction bits:\n    -   Sign bit ($1$ bit): $0$\n    -   Exponent bits ($11$ bits): $01111111100$\n    -   Fraction bits ($52$ bits): $(1001)^{12}1010_2$\n\n    The full $64$-bit word is:\n    $$0 \\underbrace{01111111100}_{\\text{Exponent}} \\underbrace{10011001...10011010}_{\\text{Fraction}}$$\n    Grouping these bits into $4$-bit nibbles from left to right:\n    -   `0011` $\\rightarrow$ $3_{16}$\n    -   `1111` $\\rightarrow$ $F_{16}$\n    -   `1100` $\\rightarrow$ $C_{16}$\n    -   The fraction starts with `1001` $\\rightarrow$ $9_{16}$. This pattern repeats $12$ times.\n    -   The final $4$ bits of the fraction are `1010` $\\rightarrow$ $A_{16}$.\n\n    The consolidated $16$-hex-digit encoding is `3FC999999999999A`.\n\n### Part 2: Absolute Error of $5 \\times 0.2$\n\n1.  **Value of $\\operatorname{fl}(0.2)$:**\n    Let $x_{64} = \\operatorname{fl}(0.2)$ be the binary64 representation of $0.2$. Its value is determined by the fields we found. The integer value of the fraction field $F$ must be calculated. One way is to realize that we rounded up from the exact value. The exact significand needed is $1.6 = 1 + \\frac{3}{5}$. We needed to approximate $\\frac{3}{5}$ by $\\frac{F}{2^{52}}$.\n    $$F \\approx \\frac{3}{5} \\times 2^{52}$$\n    The exact value $\\frac{3}{5} \\times 2^{52}$ is $k+0.6$ for some integer $k$. Rounding to the nearest integer gives $k+1$.\n    $$F = \\text{round}\\left(\\frac{3}{5} \\times 2^{52}\\right) = \\left\\lfloor \\frac{3}{5} \\times 2^{52} \\right\\rfloor + 1 = \\frac{3 \\cdot 2^{52}-3}{5} + 1 = \\frac{3 \\cdot 2^{52}+2}{5}$$\n    The value of $x_{64}$ is:\n    $$x_{64} = \\left(1 + \\frac{F}{2^{52}}\\right) \\times 2^{-3} = \\left(1 + \\frac{(3 \\cdot 2^{52}+2)/5}{2^{52}}\\right) \\times 2^{-3}$$\n    $$x_{64} = \\left(1 + \\frac{3}{5} + \\frac{2}{5 \\cdot 2^{52}}\\right) \\times 2^{-3} = \\left(\\frac{8}{5} + \\frac{2}{5 \\cdot 2^{52}}\\right) \\times 2^{-3}$$\n    $$x_{64} = \\frac{8}{5 \\cdot 2^3} + \\frac{2}{5 \\cdot 2^{52} \\cdot 2^3} = \\frac{1}{5} + \\frac{2}{5 \\cdot 2^{55}} = \\frac{1}{5} + \\frac{1}{5 \\cdot 2^{54}}$$\n    So, the stored value is slightly larger than the true value of $0.2$.\n\n2.  **Value of $\\operatorname{fl}(5)$:**\n    The integer $5$ is represented as $5_{10} = 101_2 = 1.01_2 \\times 2^2$. This can be represented exactly in binary64 since the significand requires only $3$ bits. Thus, $\\operatorname{fl}(5) = 5$.\n\n3.  **Perform the Multiplication:**\n    The computation is modeled as rounding the exact product of $\\operatorname{fl}(5)$ and $\\operatorname{fl}(0.2)$.\n    Let $P$ be the exact product:\n    $$P = 5 \\times x_{64} = 5 \\times \\left(\\frac{1}{5} + \\frac{1}{5 \\cdot 2^{54}}\\right) = 1 + \\frac{5}{5 \\cdot 2^{54}} = 1 + 2^{-54}$$\n\n4.  **Round the Product:**\n    We must round the result $P = 1 + 2^{-54}$ to the nearest binary64 number. The number $1$ is exactly representable. The next larger representable number has the same exponent ($e=0$) and its fraction field is the smallest possible non-zero value, which is $1$. The significand is $1+2^{-52}$.\n    The two binary64 numbers bracketing $P$ are:\n    -   $N_1 = 1$, with significand $1.0$ and exponent $0$.\n    -   $N_2 = 1+2^{-52}$, with significand $1.0...01$ and exponent $0$.\n\n    To use the round-to-nearest rule, we compare $P$ with the midpoint $M$ between $N_1$ and $N_2$:\n    $$M = \\frac{N_1 + N_2}{2} = \\frac{1 + (1+2^{-52})}{2} = 1 + \\frac{2^{-52}}{2} = 1 + 2^{-53}$$\n    We compare our product $P$ to this midpoint:\n    $$P = 1 + 2^{-54}$$\n    Since $2^{-54}  2^{-53}$, we have $P  M$. The value $P$ is closer to $N_1 = 1$ than to $N_2 = 1+2^{-52}$.\n    Therefore, the final rounded result of the computation is:\n    $$\\operatorname{fl}(5 \\times 0.2) = \\operatorname{round}(1 + 2^{-54}) = 1$$\n\n5.  **Calculate Absolute Error:**\n    The problem asks for the absolute error between the computed result and the exact mathematical result, which is $1$.\n    $$\\text{Absolute Error} = |\\operatorname{fl}(5 \\times 0.2) - 1| = |1 - 1| = 0$$", "answer": "$$\\boxed{0}$$", "id": "3240494"}]}