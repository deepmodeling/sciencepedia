{"hands_on_practices": [{"introduction": "Understanding machine epsilon begins with a startling realization: there's a limit to how high you can count. This exercise explores the fundamental boundary where floating-point representation can no longer distinguish between an integer $N$ and $N+1$. By deriving the smallest such integer $N$, you will build a concrete intuition for how the gap between representable numbers grows with magnitude, directly connecting this phenomenon to the value of machine epsilon, $\\epsilon_{mach}$ [@problem_id:3250012].", "problem": "A high-performance computing system uses normalized binary floating-point numbers with precision $p$ bits in the significand (including the implicit leading bit), and rounding is to nearest with ties going to the even significand. A software engineer decides to store an integer counter in this floating-point format for speed. Let $\\epsilon_{mach}$ denote the machine epsilon, defined as the smallest positive floating-point number $e$ such that $1 + e$ is representable and strictly greater than $1$.\n\nAssume the counter is incremented by $1$ in exact arithmetic and then rounded to the nearest representable floating-point value before storage. Determine the smallest positive integer $N$ such that if the counter currently stores $N$, then storing $N+1$ as a floating-point number results in the same stored value as $N$. Express your answer as a closed-form analytic expression in terms of $\\epsilon_{mach}$ only. No numerical evaluation is required, and no rounding instructions apply since the final answer is symbolic.", "solution": "A normalized binary floating-point number representation with a precision of $p$ bits in the significand (including the implicit leading bit) can represent a number $x$ as $x = \\sigma \\cdot (1.f) \\cdot 2^E$, where $\\sigma$ is the sign bit, $1.f$ is the $p$-bit significand with a hidden bit, and $E$ is the exponent. The fractional part $f$ consists of $p-1$ bits.\n\nThe machine epsilon, $\\epsilon_{mach}$, is defined as the smallest positive floating-point number such that $1 + \\epsilon_{mach}  1$. The number $1$ is represented as $1.00...0 \\times 2^0$. The next larger representable number is $1.00...01 \\times 2^0$, where the final $1$ is in the $(p-1)$-th fractional position. This number has a value of $1 + 2^{-(p-1)}$. Therefore, the machine epsilon is given by:\n$$ \\epsilon_{mach} = 2^{-(p-1)} $$\nThis relationship allows us to connect the precision $p$ with $\\epsilon_{mach}$.\n\nThe problem asks for the smallest positive integer $N$ such that storing $N+1$ as a floating-point number results in the same value as storing $N$. Let $fl(x)$ denote the operation of rounding a real number $x$ to the nearest representable floating-point value. The condition is $fl(N+1) = fl(N)$.\n\nFirst, let's determine which integers are exactly representable in this format. An integer $I$ is exactly representable if it can be written in the form $I = M \\cdot 2^k$ for some integers $M$ and $k$, where $M$ is the significand integer and can be represented with at most $p$ bits. This means $M  2^p$. We can write any integer $I$ as $I = m \\cdot 2^j$, where $m$ is an odd integer. The integer $I$ is exactly representable if and only if $m  2^p$.\n\nLet's examine integers $I \\le 2^p$. Let $I = m \\cdot 2^j$ where $m$ is odd.\nIf $j \\ge 1$, then $m = I/2^j \\le 2^p/2^j \\le 2^{p-1}$. So $m  2^p$.\nIf $j=0$, then $I$ must be odd. The largest odd integer $\\le 2^p$ is $2^p-1$. So $m = I \\le 2^p-1$, which means $m  2^p$.\nThe integer $2^p$ itself can be written as $1 \\cdot 2^p$, so its odd part is $m=1$, which is less than $2^p$.\nTherefore, all integers $I$ in the range $0 \\le I \\le 2^p$ are exactly representable as floating-point numbers.\n\nNow, consider any positive integer $N  2^p$. For such an $N$, the integer $N+1$ is also less than or equal to $2^p$. Since both $N$ and $N+1$ are exactly representable, their floating-point representations are themselves:\n$$ fl(N) = N $$\n$$ fl(N+1) = N+1 $$\nClearly, $N \\neq N+1$, so $fl(N) \\neq fl(N+1)$. This means the condition is not met for any $N  2^p$. The smallest integer $N$ satisfying the condition must be at least $2^p$.\n\nLet's test $N=2^p$.\nSince $2^p$ is an exactly representable integer, we have:\n$$ fl(N) = fl(2^p) = 2^p $$\nNow consider $N+1 = 2^p+1$. We check if $2^p+1$ is exactly representable. The odd part of $2^p+1$ is $m=2^p+1$. Since $m  2^p$, the integer $2^p+1$ is not exactly representable. We must round it.\n\nTo round $2^p+1$, we must identify the two nearest representable floating-point numbers. The gap between representable numbers, or the unit in the last place (ulp), depends on the magnitude of the number. For numbers $x$ in the range $[2^p, 2^{p+1})$, the exponent is $E=p$. The ulp is given by:\n$$ \\text{ulp}(x) = \\epsilon_{mach} \\cdot 2^p = 2^{-(p-1)} \\cdot 2^p = 2 $$\nThis means that for numbers with magnitude around $2^p$, the representable values are consecutive even integers.\nThe number $2^p$ is representable. The next representable number is $2^p + \\text{ulp}(2^p) = 2^p+2$.\nThe number to be rounded, $2^p+1$, lies exactly at the midpoint between the two representable numbers $2^p$ and $2^p+2$.\n$$ 2^p+1 = \\frac{(2^p) + (2^p+2)}{2} $$\nThis is a tie. The rounding rule is \"ties to even\". We must round to the neighbor whose significand is even.\nLet's examine the significands.\nFor the number $2^p$: It is represented as $1.00...0 \\times 2^p$. The integer value of its $p$-bit significand is $2^{p-1}$, which is an even number for $p1$.\nFor the number $2^p+2$:\n$$ 2^p+2 = (1 + \\frac{2}{2^p}) \\cdot 2^p = (1 + 2^{-(p-1)}) \\cdot 2^p $$\nIts significand is $1.00...01$ in binary, corresponding to the integer $2^{p-1}+1$, which is odd.\nAccording to the round-ties-to-even rule, $2^p+1$ is rounded to the value with the even significand, which is $2^p$.\n$$ fl(2^p+1) = 2^p $$\nThus for $N=2^p$, we have $fl(N)=2^p$ and $fl(N+1)=2^p$. The condition $fl(N+1) = fl(N)$ is met.\nSince this condition fails for all $N  2^p$, the smallest integer for which it holds is $N=2^p$.\n\nThe final step is to express this result in terms of $\\epsilon_{mach}$.\nFrom $\\epsilon_{mach} = 2^{-(p-1)}$, we have $2^{p-1} = \\frac{1}{\\epsilon_{mach}}$.\nMultiplying by $2$ gives the desired expression for $N$:\n$$ N = 2^p = 2 \\cdot 2^{p-1} = \\frac{2}{\\epsilon_{mach}} $$", "answer": "$$\\boxed{\\frac{2}{\\epsilon_{mach}}}$$", "id": "3250012"}, {"introduction": "The theoretical limit explored previously has direct and often surprising consequences in programming. One of the most classic bugs in numerical code is an infinite loop caused by a variable that stops changing because the increment is too small to be registered [@problem_id:3250049]. This hands-on practice challenges you to create and diagnose such a non-terminating loop, demonstrating how an update step like $x \\leftarrow x + \\delta$ can stagnate when $\\delta$ becomes smaller than the local unit in the last place (ULP). By experimenting with different magnitudes and increments, you will see firsthand why floating-point equality checks are notoriously dangerous.", "problem": "You must write a complete, runnable program that demonstrates and detects non-termination in a loop whose condition uses floating-point equality, due to the effect of machine epsilon near equality. Work in the standard binary floating-point model consistent with the Institute of Electrical and Electronics Engineers (IEEE) 754 standard, using rounding to nearest with ties to even. Start from the following foundational base:\n\n- The definition of machine epsilon: the smallest positive real number $\\varepsilon_{\\mathrm{mach}}$ such that, in floating-point arithmetic, $\\mathrm{fl}(1 + \\varepsilon_{\\mathrm{mach}}) \\neq 1$.\n- The unit in the last place (ULP): for a given floating-point number $a$, the distance to the next larger representable floating-point number, denoted $\\mathrm{ulp}(a)$, is $\\mathrm{nextafter}(a,+\\infty) - a$, which is a widely tested fact in floating-point analysis.\n- Rounding to nearest with ties to even: given a real number $z$ lying exactly halfway between two representable floating-point numbers $z_{-}$ and $z_{+}$, the rounded value is the one whose significand has an even least significant bit.\n\nYour program will simulate a family of simple update loops of the form\n- initialize $x \\leftarrow x_{0}$,\n- fix a target $y$ and a constant increment $\\delta  0$,\n- iterate $x \\leftarrow \\mathrm{fl}(x + \\delta)$ until the condition $x = y$ is satisfied.\n\nIn exact real arithmetic, if $x_{0}  y$ and $\\delta  0$, then there exists an integer $k$ such that $x_{0} + k \\delta = y$. However, in floating-point arithmetic, if $\\delta$ is smaller than $\\tfrac{1}{2}\\,\\mathrm{ulp}(x)$ at the current $x$, then $\\mathrm{fl}(x + \\delta) = x$, and the loop makes no progress. In particular, if $x \\neq y$ and $\\mathrm{fl}(x + \\delta) = x$, then the loop body cannot change $x$ and the loop condition $x \\neq y$ remains true forever, so the loop does not terminate.\n\nTasks your program must perform:\n\n1) Compute the machine epsilon $\\varepsilon_{\\mathrm{mach}}$ for the working floating-point type by a first-principles halving procedure. Do not hard-code a constant for $\\varepsilon_{\\mathrm{mach}}$.\n2) Implement a function to compute $\\mathrm{ulp}(a)$ for any finite $a$, defined as $\\left|\\mathrm{nextafter}(a,+\\infty) - a\\right|$.\n3) Implement a simulator for the loop “while $x \\neq y$ do $x \\leftarrow \\mathrm{fl}(x + \\delta)$” with the following behavior:\n   - If $x = y$ at the top of an iteration, declare that the loop terminates.\n   - Otherwise, compute $x_{\\text{new}} \\leftarrow \\mathrm{fl}(x + \\delta)$.\n   - If $x_{\\text{new}} = x$ and $x \\neq y$, declare that the loop cannot terminate because the state is stagnant and the loop condition remains true.\n   - Also guard against pathological cases by stopping after a large fixed number of iterations $N_{\\max}$ and declaring non-termination in that case. You may take $N_{\\max} = 10^{7}$.\n4) For each of the test cases below, construct the specified $x_{0}$, $y$, and $\\delta$ and report a boolean indicating whether the loop terminates (true) or is detected as non-terminating by the stagnation rule or the iteration cap (false).\n\nTest suite specification:\n\nLet $\\mathrm{prev}(a)$ denote the largest representable floating-point number strictly less than $a$, that is, $\\mathrm{prev}(a) = \\mathrm{nextafter}(a,0)$ for $a  0$.\n\nProvide results for the following five cases:\n\n- Case A (non-terminating by stagnation near one): $y = 1$, $x_{0} = \\mathrm{prev}(y)$, and $\\delta = \\tfrac{1}{4}\\,\\mathrm{ulp}(y)$. Expected behavior: since $\\delta  \\tfrac{1}{2}\\,\\mathrm{ulp}(x)$ at $x = x_{0}$, rounding yields $\\mathrm{fl}(x + \\delta) = x$, so no progress is possible when $x \\neq y$.\n- Case B (terminating in one iteration by midpoint rounding): $y = 1$, $x_{0} = \\mathrm{prev}(y)$, and $\\delta = \\tfrac{1}{2}\\,\\mathrm{ulp}(y)$. Expected behavior: $\\mathrm{fl}(x + \\delta)$ rounds to $y$ due to ties to even, so the loop terminates on the next condition check.\n- Case C (immediate termination): $y = 1$, $x_{0} = 1$, and $\\delta = \\tfrac{1}{4}\\,\\mathrm{ulp}(y)$. Expected behavior: the loop does not enter because $x = y$ initially.\n- Case D (non-terminating at large magnitude with too-small increment): $y = 2^{30}$, $x_{0} = \\mathrm{prev}(y)$, and $\\delta = \\tfrac{1}{4}\\,\\mathrm{ulp}(1)$. Expected behavior: since $\\mathrm{ulp}(y) \\gg \\mathrm{ulp}(1)$, one has $\\delta \\ll \\tfrac{1}{2}\\,\\mathrm{ulp}(x)$ at $x \\approx y$, hence $\\mathrm{fl}(x + \\delta) = x$ and the loop cannot make progress.\n- Case E (terminating at large magnitude by adequate increment): $y = 2^{30}$, $x_{0} = \\mathrm{prev}(y)$, and $\\delta = \\tfrac{1}{2}\\,\\mathrm{ulp}(y)$. Expected behavior: as in Case B, one step moves $x$ to $y$.\n\nFinal output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For the five cases A through E in order, output a list of booleans, for example, “[True,False,True,False,True]”. There must be no additional text in the output line.", "solution": "The problem requires an investigation into the properties of floating-point arithmetic, specifically how its discrete nature can lead to the non-termination of loops that rely on equality checks. We will construct a program to demonstrate this phenomenon by implementing a first-principles calculation of machine epsilon, a function to compute the unit in the last place (ULP), and a simulator for a simple iterative loop. The analysis rests on the IEEE 754 standard for binary floating-point arithmetic, using rounding to the nearest representable value, with ties rounded to the value with an even least significant bit in its significand.\n\nThe core of the problem lies in the update step $x \\leftarrow \\mathrm{fl}(x + \\delta)$, where $\\mathrm{fl}(\\cdot)$ denotes the floating-point representation of a real number. Due to the finite precision of floating-point numbers, there is a minimum gap between any number $x$ and the next larger representable number. This gap is known as the unit in the last place, $\\mathrm{ulp}(x)$. If the increment $\\delta$ is too small relative to this gap, the sum $x + \\delta$ may be rounded back to $x$, causing the loop to stagnate.\n\nThe step-by-step solution is as follows:\n\nFirst, we must compute the machine epsilon, $\\varepsilon_{\\mathrm{mach}}$, for the working floating-point type (which we take as 64-bit double precision). $\\varepsilon_{\\mathrm{mach}}$ is defined as the smallest positive number such that $\\mathrm{fl}(1 + \\varepsilon_{\\mathrm{mach}}) \\neq 1$. It is equivalent to $\\mathrm{ulp}(1)$. We can determine $\\varepsilon_{\\mathrm{mach}}$ algorithmically by starting with a value, say $\\varepsilon = 1$, and repeatedly halving it. The loop `while (1.0 + epsilon) > 1.0` continues as long as `epsilon` is large enough to be recognized when added to $1$. The loop terminates when `1.0 + epsilon` is rounded down to $1.0$. If the final value of `epsilon` upon loop termination is $\\varepsilon_{f}$, this means that $\\mathrm{fl}(1 + \\varepsilon_{f}) = 1$, whereas the previous value, $2\\varepsilon_{f}$, satisfied $\\mathrm{fl}(1 + 2\\varepsilon_{f}) > 1$. Therefore, $\\varepsilon_{\\mathrm{mach}} = 2\\varepsilon_{f}$.\n\nSecond, we implement a function to compute $\\mathrm{ulp}(a)$ for a floating-point number $a$. As specified, this is the distance to the next larger representable floating-point number. This is given by the expression $\\mathrm{ulp}(a) = |\\mathrm{nextafter}(a, +\\infty) - a|$. The value of $\\mathrm{ulp}(a)$ is not constant; it scales with the magnitude of $a$. Specifically, for a number $a$ in the range $[2^k, 2^{k+1})$, its ULP is given by $\\mathrm{ulp}(a) = 2^k \\varepsilon_{\\mathrm{mach}}$.\n\nThird, we analyze the condition for loop stagnation. The sum $x + \\delta$ is a real number that must be rounded to a representable floating-point value. The representable numbers nearest to $x$ are $x$ itself and $\\mathrm{nextafter}(x, +\\infty) = x + \\mathrm{ulp}(x)$. The midpoint between these two is $x + \\frac{1}{2}\\mathrm{ulp}(x)$. The \"round to nearest\" rule dictates:\n- If $x + \\delta  x + \\frac{1}{2}\\mathrm{ulp}(x)$, the sum is rounded down to $x$. This is equivalent to $\\delta  \\frac{1}{2}\\mathrm{ulp}(x)$.\n- If $x + \\delta > x + \\frac{1}{2}\\mathrm{ulp}(x)$, the sum is rounded up to $\\mathrm{nextafter}(x, +\\infty)$. This is equivalent to $\\delta > \\frac{1}{2}\\mathrm{ulp}(x)$.\n- If $x + \\delta = x + \\frac{1}{2}\\mathrm{ulp}(x)$, the sum is exactly halfway. The \"ties to even\" rule applies, rounding to the neighbor whose significand has a least significant bit of $0$.\n\nIf $\\delta  \\frac{1}{2}\\mathrm{ulp}(x)$, then $\\mathrm{fl}(x + \\delta) = x$. If at this point $x \\neq y$, the loop variable ceases to change, and the termination condition $x=y$ is never met, leading to an infinite loop. Our simulator must detect this stagnation. It will also include a safeguard limit of $N_{\\max} = 10^7$ iterations.\n\nFourth, we design the loop simulator function. It takes initial values $x_0$, $y$, and $\\delta$. It iterates, updating $x \\leftarrow \\mathrm{fl}(x + \\delta)$. In each iteration, it first checks if $x=y$ (termination), then computes the new value $x_{\\text{new}}$, and finally checks if $x_{\\text{new}} = x$ (stagnation). If the loop completes $N_{\\max}$ iterations without either condition being met, it is also declared as non-terminating.\n\nFinally, we apply this framework to the five specified test cases. We denote $\\mathrm{prev}(a) = \\mathrm{nextafter}(a, 0)$ for $a0$.\n\nCase A: $y = 1$, $x_{0} = \\mathrm{prev}(y)$, and $\\delta = \\frac{1}{4}\\,\\mathrm{ulp}(y)$.\nHere, $y=1$, so $\\mathrm{ulp}(y) = \\mathrm{ulp}(1) = \\varepsilon_{\\mathrm{mach}}$. The increment is $\\delta = \\frac{1}{4}\\varepsilon_{\\mathrm{mach}}$. The initial value is $x_0 = \\mathrm{prev}(1)$. For double precision, $\\mathrm{prev}(1) = 1 - \\varepsilon_{\\mathrm{mach}}/2$. The unit in the last place for $x_0$ (which is in the range $[0.5, 1)$) is $\\mathrm{ulp}(x_0) = \\varepsilon_{\\mathrm{mach}}/2$. The value to be rounded is $x_0 + \\delta = (1 - \\varepsilon_{\\mathrm{mach}}/2) + \\frac{1}{4}\\varepsilon_{\\mathrm{mach}} = 1 - \\frac{1}{4}\\varepsilon_{\\mathrm{mach}}$. This value is exactly halfway between the representable numbers $x_0 = 1 - \\varepsilon_{\\mathrm{mach}}/2$ and $y=1$. This constitutes a tie. The \"round-ties-to-even\" rule requires rounding to the neighbor with an even significand. The significand for $y=1$ is even, while for $x_0$ it is odd. Therefore, the result is rounded to $y$. The loop terminates in one step. The result is termination (True). Note that this contradicts the \"expected behavior\" in the problem description, which is based on a faulty premise.\n\nCase B: $y = 1$, $x_{0} = \\mathrm{prev}(y)$, and $\\delta = \\frac{1}{2}\\,\\mathrm{ulp}(y)$.\nHere, $\\delta = \\frac{1}{2}\\varepsilon_{\\mathrm{mach}}$. The value to round is $x_0 + \\delta = (1 - \\varepsilon_{\\mathrm{mach}}/2) + \\frac{1}{2}\\varepsilon_{\\mathrm{mach}} = 1$. This is exactly representable as $y=1$. Thus, $\\mathrm{fl}(x_0 + \\delta) = y$. The loop terminates after one iteration. The result is termination (True).\n\nCase C: $y = 1$, $x_{0} = 1$, and $\\delta = \\frac{1}{4}\\,\\mathrm{ulp}(y)$.\nThe initial condition is $x_0 = y$. The loop condition `while x != y` is false from the beginning. The loop body never executes. The result is immediate termination (True).\n\nCase D: $y = 2^{30}$, $x_{0} = \\mathrm{prev}(y)$, and $\\delta = \\frac{1}{4}\\,\\mathrm{ulp}(1)$.\nThe target $y = 2^{30}$ is a large number. Its ULP is significantly larger than for $1$: $\\mathrm{ulp}(y) = \\mathrm{ulp}(2^{30}) = 2^{30}\\mathrm{ulp}(1) = 2^{30}\\varepsilon_{\\mathrm{mach}}$. The increment is $\\delta = \\frac{1}{4}\\mathrm{ulp}(1) = \\frac{1}{4}\\varepsilon_{\\mathrm{mach}}$. The current value is $x_0 \\approx y$, so $\\mathrm{ulp}(x_0) \\approx \\mathrm{ulp}(y)$. The stagnation condition $\\delta  \\frac{1}{2}\\mathrm{ulp}(x_0)$ becomes $\\frac{1}{4}\\varepsilon_{\\mathrm{mach}}  \\frac{1}{2}(2^{30}\\varepsilon_{\\mathrm{mach}})$, or $\\frac{1}{4}  2^{29}$. This is clearly true. The increment is far too small to have any effect at this magnitude. The loop stagnates. The result is non-termination (False).\n\nCase E: $y = 2^{30}$, $x_{0} = \\mathrm{prev}(y)$, and $\\delta = \\frac{1}{2}\\,\\mathrm{ulp}(y)$.\nThis is analogous to Case B but at a larger magnitude. The increment $\\delta = \\frac{1}{2}\\mathrm{ulp}(y)$ places the sum $x_0 + \\delta$ exactly halfway between $x_0$ and $y$. The number $y=2^{30}$ is a power of two, and its significand has an even least significant bit. The number $x_0 = \\mathrm{prev}(y)$ has an odd least significant bit. Rounding is to the even neighbor, $y$. The loop terminates in one step. The result is termination (True).\n\nThe expected outcomes are [True, True, True, False, True]. The following program formalizes this analysis.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by setting up and running simulations for the five\n    specified test cases and printing the results.\n    \"\"\"\n\n    def compute_machine_epsilon():\n        \"\"\"\n        Computes machine epsilon for float64 using a first-principles halving procedure.\n        Machine epsilon is the smallest positive number eps such that 1.0 + eps != 1.0.\n        \"\"\"\n        # Start with a value that is guaranteed to be significant when added to 1.\n        epsilon = np.float64(1.0)\n        # Repeatedly halve epsilon until 1.0 + epsilon is indistinguishable from 1.0.\n        while np.float64(1.0) + epsilon  np.float64(1.0):\n            epsilon /= np.float64(2.0)\n        # The loop terminates when epsilon is too small. The machine epsilon is the\n        # previous value of epsilon, which is the current value multiplied by 2.\n        return epsilon * np.float64(2.0)\n\n    def compute_ulp(a: np.float64) - np.float64:\n        \"\"\"\n        Computes the Unit in the Last Place (ULP) for a given float64 number 'a'.\n        Defined as the distance to the next larger representable floating-point number.\n        \"\"\"\n        # Ensure input is a float64 for consistent calculations.\n        a = np.float64(a)\n        # Use np.inf to find the next representable number in the positive direction.\n        # np.nextafter handles negative numbers and special cases correctly.\n        return np.abs(np.nextafter(a, np.inf) - a)\n\n    def simulate_loop(x0: np.float64, y: np.float64, delta: np.float64) - bool:\n        \"\"\"\n        Simulates the loop 'while x != y: x = x + delta' and detects non-termination.\n\n        Returns:\n            bool: True if the loop terminates, False otherwise.\n        \"\"\"\n        N_MAX = 10**7\n        x = np.float64(x0)\n        y = np.float64(y)\n        delta = np.float64(delta)\n\n        for _ in range(N_MAX):\n            # Condition 1: Loop terminates if x reaches the target y.\n            if x == y:\n                return True\n\n            # Perform one step of the iteration.\n            x_new = x + delta\n\n            # Condition 2: Non-termination due to stagnation.\n            # If the value of x does not change, and x is not at the target,\n            # the loop will never terminate.\n            if x_new == x:\n                return False\n\n            x = x_new\n\n        # Condition 3: Non-termination due to exceeding max iterations.\n        return False\n\n    # Define a helper for prev(a) as specified in the problem\n    def prev(a: np.float64) - np.float64:\n        return np.nextafter(a, np.float64(0.0))\n\n    # --- Test Case Construction ---\n    \n    # Common helper values\n    y1 = np.float64(1.0)\n    ulp1 = compute_ulp(y1)\n    \n    y2 = np.float64(2**30)\n    ulp2 = compute_ulp(y2)\n\n    test_cases = [\n        # Case A: Non-terminating by stagnation near one\n        (prev(y1), y1, np.float64(0.25) * ulp1),\n        \n        # Case B: Terminating in one iteration by midpoint rounding\n        (prev(y1), y1, np.float64(0.5) * ulp1),\n        \n        # Case C: Immediate termination\n        (y1, y1, np.float64(0.25) * ulp1),\n        \n        # Case D: Non-terminating at large magnitude with too-small increment\n        (prev(y2), y2, np.float64(0.25) * ulp1),\n        \n        # Case E: Terminating at large magnitude by adequate increment\n        (prev(y2), y2, np.float64(0.5) * ulp2),\n    ]\n\n    results = []\n    for x0, y, delta in test_cases:\n        terminates = simulate_loop(x0, y, delta)\n        results.append(terminates)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3250049"}, {"introduction": "Beyond causing loops to stagnate, the finite precision of floating-point numbers can also lead to a drastic loss of accuracy through catastrophic cancellation. This occurs when subtracting two nearly-equal numbers, effectively erasing significant digits and leaving a result dominated by rounding noise [@problem_id:3250061]. This coding exercise guides you to find the critical point where the simple expression $e^x - 1$ collapses to zero, and it contrasts this fragile computation with robust library functions like `expm1` designed to prevent such accuracy loss. This practice highlights the importance of numerical awareness and using the right tools for stable and reliable scientific software.", "problem": "You are given the task of analyzing the numerical stability of computing the function value of the real-valued map $x \\mapsto e^x - 1$ in floating-point arithmetic. The objective is to detect when computing $e^x - 1$ naively as $\\exp(x) - 1$ in floating-point arithmetic suffers from catastrophic cancellation, and to compare this with a numerically stable alternative that evaluates the same function without cancellation.\n\nBase your reasoning on the following fundamental concepts.\n- Floating-point rounding to nearest, ties-to-even for the standard double-precision format, and the monotonicity of the exponential function $x \\mapsto e^x$ for real $x$.\n- The definition of machine epsilon: the smallest positive floating-point number $\\varepsilon$ such that $1 + \\varepsilon  1$ in the target format.\n- The fact that for small $x$, the real number $e^x$ equals $1 + x + \\frac{x^2}{2} + \\cdots$ and therefore satisfies $e^x - 1 \\approx x$.\n- The phenomenon of catastrophic cancellation: subtracting nearly equal floating-point numbers can erase leading significant digits, causing large relative error.\n\nYour program must implement the following, using double-precision floating point arithmetic (that is, the language’s standard $64$-bit float, as provided by the Numerical Python library).\n1. Compute the machine epsilon $\\varepsilon$ for the floating-point type by an iterative halving method. Denote the result by $\\hat{\\varepsilon}$.\n2. Determine the smallest positive real number $x$ (in the sense of the largest $x$ for which the computed naive difference remains zero) such that the naive floating-point computation of $e^x - 1$ using $\\exp(x) - 1$ returns exactly $0$ in floating-point arithmetic. Formally, find the supremum $x_{\\mathrm{crit}}  0$ such that, in the chosen floating-point type, $\\exp(x) - 1 = 0$ holds. Use monotonicity and a bisection search to approximate $x_{\\mathrm{crit}}$ to within the resolution of the floating-point type.\n3. For the same $x_{\\mathrm{crit}}$, compute the value produced by a cancellation-free implementation of $e^x - 1$ that is designed for small $x$. Denote this value by $y_{\\mathrm{alt}}(x_{\\mathrm{crit}})$. This function should be the language or library-provided implementation that avoids cancellation in the region near zero.\n4. For a well-chosen set of inputs, compare the relative errors of the naive difference and the cancellation-free implementation against a high-precision reference. Use a high-precision real arithmetic facility to compute the reference value $\\operatorname{true}(x)$ for each $x$, and then compute the relative error\n$$\n\\operatorname{relerr}(y;\\operatorname{true}) = \\frac{|y - \\operatorname{true}|}{|\\operatorname{true}|}.\n$$\nThe test suite of inputs to evaluate must be the list\n$$\n\\left[ x_{\\mathrm{crit}}, \\frac{x_{\\mathrm{crit}}}{2}, \\hat{\\varepsilon}, \\frac{\\hat{\\varepsilon}}{2}, \\sqrt{\\hat{\\varepsilon}}, -x_{\\mathrm{crit}}, -\\frac{x_{\\mathrm{crit}}}{2}, -\\hat{\\varepsilon}, -\\frac{\\hat{\\varepsilon}}{2}, -\\sqrt{\\hat{\\varepsilon}} \\right].\n$$\nFor each input $x$ in this list, compute the boolean\n$$\nb(x) = \\left( \\operatorname{relerr}\\big(\\exp(x) - 1; \\operatorname{true}(x)\\big)  \\operatorname{relerr}\\big(\\text{alt}(x); \\operatorname{true}(x)\\big) \\right),\n$$\nwhich answers whether the naive difference has strictly larger relative error than the cancellation-free implementation on that input.\nAssume no physical units are involved. Angles are not relevant. All numerical results must be reported as floating-point numbers or booleans as specified below.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the following exact order:\n- First, $\\hat{\\varepsilon}$.\n- Second, $x_{\\mathrm{crit}}$.\n- Third, $y_{\\mathrm{alt}}(x_{\\mathrm{crit}})$.\n- Then the sequence of $10$ booleans $b(x)$ corresponding to the inputs in the test suite listed above, in that exact order.\n\nFor instance, the output must look like\n$[r_1,r_2,r_3,b_1,b_2,\\dots,b_{10}]$\nwith $r_1, r_2, r_3$ being floating-point numbers and $b_1,\\dots,b_{10}$ being booleans. No additional text should be printed.", "solution": "The objective is to analyze the numerical instability of the function $f(x) = e^x - 1$ when computed naively in double-precision floating-point arithmetic for values of $x$ near zero. This instability, known as catastrophic cancellation, is compared against a stable alternative implementation. The analysis involves four main steps: computing the machine epsilon, determining a critical threshold for cancellation, evaluating a stable alternative, and comparing the relative errors of both methods on a specified set of test inputs.\n\n### Part 1: Computation of Machine Epsilon ($\\hat{\\varepsilon}$)\n\nMachine epsilon, denoted by $\\varepsilon$, is the smallest positive floating-point number such that the addition of $1$ and $\\varepsilon$ results in a value strictly greater than $1$. Formally, in floating-point arithmetic, $\\varepsilon = \\min \\{ y  0 \\mid 1 + y  1 \\}$. It quantifies the gap between $1$ and the next larger representable floating-point number.\n\nTo compute $\\hat{\\varepsilon}$ for the standard $64$-bit double-precision floating-point type, we employ an iterative halving method as stipulated. We start with a value, for instance $x_0 = 1$, and repeatedly divide it by $2$. For each new value $x_k = x_{k-1}/2$, we test the condition $1 + x_k  1$. The loop terminates when this condition fails. The last value of $x_{k-1}$ for which the condition held true is our computed machine epsilon, $\\hat{\\varepsilon}$. Using NumPy's `float64` type ensures the operations are performed in double precision.\n\n### Part 2: Determination of the Critical Value ($x_{\\mathrm{crit}}$)\n\nCatastrophic cancellation occurs in the expression $e^x - 1$ when $x$ is close to $0$. In this regime, $e^x$ is close to $1$. The floating-point representation of $e^x$, denoted $\\mathrm{fl}(e^x)$, may lose precision. When we then subtract $1$, the leading bits of $\\mathrm{fl}(e^x)$ and $1$ are identical, and their cancellation leads to a result dominated by rounding errors.\n\nThe problem asks to find the supremum $x_{\\mathrm{crit}}  0$ such that the naive floating-point computation yields exactly zero:\n$$\n\\mathrm{fl}(\\mathrm{fl}(e^x) - 1) = 0\n$$\nIn standard floating-point arithmetic, this equality holds if and only if $\\mathrm{fl}(e^x)$ is exactly $1.0$. According to the IEEE 754 standard (round-to-nearest, ties-to-even), a real number $z$ is rounded to $1.0$ if it falls within the interval $[1.0 - \\varepsilon/4, 1.0 + \\varepsilon/2)$. Since we consider $x  0$, we have $e^x  1$. Thus, $\\mathrm{fl}(e^x) = 1.0$ if $e^x  1.0 + \\varepsilon/2$. The threshold is reached when $e^x = 1.0 + \\varepsilon/2$, which implies $x = \\ln(1.0 + \\varepsilon/2) \\approx \\varepsilon/2$. The supremum of the set of all $x$ for which the condition holds is therefore $x_{\\mathrm{crit}} = \\ln(1 + \\varepsilon/2) \\approx \\varepsilon/2$.\n\nA bisection search is used to find this value numerically. We search for the largest positive floating-point number $x$ for which the boolean expression `np.exp(x) - 1.0 == 0.0` is true. The search interval is initialized, for example, to $[0, \\hat{\\varepsilon}]$, and iteratively refined. If for a midpoint `mid`, the condition `np.exp(mid) - 1.0 == 0.0` holds, we know $x_{\\mathrm{crit}}$ is at least `mid`, so we set the lower bound of our search to `mid`. Otherwise, `mid` is too large, and we set the upper bound to `mid`. After a sufficient number of iterations, the lower-bound of the search interval converges to the machine-representable value that is our best approximation of $x_{\\mathrm{crit}}$.\n\n### Part 3: Stable Alternative Implementation ($y_{\\mathrm{alt}}(x_{\\mathrm{crit}})$)\n\nSpecialized library functions exist to compute $e^x - 1$ accurately for small $|x|$. These functions, such as `numpy.expm1`, typically use a Taylor series expansion for $e^x - 1 = x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\cdots$ when $|x|$ is small, and fall back to the direct computation $\\exp(x) - 1$ when $|x|$ is large enough that cancellation is not a concern. We are asked to compute the value of this stable function at our determined critical point, $y_{\\mathrm{alt}}(x_{\\mathrm{crit}}) = \\mathrm{expm1}(x_{\\mathrm{crit}})$.\n\n### Part 4: Relative Error Comparison\n\nTo quantitatively assess the performance of the naive versus the stable method, we compute their relative errors against a high-precision reference value. The relative error of an approximation $y$ with respect to a true value $\\operatorname{true}$ is given by:\n$$\n\\operatorname{relerr}(y; \\operatorname{true}) = \\frac{|y - \\operatorname{true}|}{|\\operatorname{true}|}\n$$\nThe \"true\" value for $e^x - 1$ is computed using Python's `decimal` module, configured with a high precision (e.g., $50$ digits) to serve as a reliable ground truth.\n\nThe comparison is performed for a test suite of $10$ inputs: $[ x_{\\mathrm{crit}}, \\frac{x_{\\mathrm{crit}}}{2}, \\hat{\\varepsilon}, \\frac{\\hat{\\varepsilon}}{2}, \\sqrt{\\hat{\\varepsilon}}, -x_{\\mathrm{crit}}, -\\frac{x_{\\mathrm{crit}}}{2}, -\\hat{\\varepsilon}, -\\frac{\\hat{\\varepsilon}}{2}, -\\sqrt{\\hat{\\varepsilon}} ]$. These inputs are chosen to lie within the region where catastrophic cancellation is expected to be significant. For each input $x$, we compute the boolean indicator:\n$$\nb(x) = \\left( \\operatorname{relerr}\\big(\\mathrm{fl}(\\exp(x) - 1); \\operatorname{true}(x)\\big)  \\operatorname{relerr}\\big(\\mathrm{expm1}(x); \\operatorname{true}(x)\\big) \\right)\n$$\nThis boolean is `True` if the naive computation is strictly less accurate than the stable one. For all specified test inputs, which are small in magnitude, the naive method is expected to suffer from severe cancellation, yielding much larger relative errors than the `expm1` function. Thus, we anticipate all $10$ boolean results to be `True`.\n\nThe final output is an aggregation of these computed values: $\\hat{\\varepsilon}$, $x_{\\mathrm{crit}}$, $y_{\\mathrm{alt}}(x_{\\mathrm{crit}})$, and the $10$ boolean results $b(x_i)$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom decimal import Decimal, getcontext\n\ndef solve():\n    \"\"\"\n    Solves the numerical analysis problem as specified.\n    1. Computes machine epsilon.\n    2. Finds the critical value x_crit where exp(x)-1 underflows to 0.\n    3. Computes the stable alternative expm1(x_crit).\n    4. Compares relative errors for a test suite of inputs.\n    \"\"\"\n    \n    # Task 1: Compute machine epsilon (eps_hat) for float64 by iterative halving.\n    # We are looking for the smallest positive float64 'eps' such that 1.0 + eps  1.0.\n    eps = np.float64(1.0)\n    # The loop finds the largest power-of-two smaller than or equal to the true machine epsilon.\n    # The true epsilon is the *first* value that fails 1+eps  1, so the last one that passes is what we want.\n    # A more precise way is to check 1.0 + eps/2.0.\n    # Let `one = np.float64(1.0)`. Find eps s.t. `one + eps  one` and `one + eps/2 == one`.\n    current_eps = np.float64(1.0)\n    while np.float64(1.0) + current_eps / np.float64(2.0)  np.float64(1.0):\n        current_eps /= np.float64(2.0)\n    eps_hat = current_eps\n\n    # Task 2: Determine the critical value x_crit using a bisection search.\n    # We are looking for the supremum x  0 such that naive exp(x) - 1 == 0.\n    # This happens when exp(x) is rounded to 1.0.\n    # The search space is bounded by 0 and a value known to be too large, like eps_hat.\n    low = np.float64(0.0)\n    high = eps_hat\n    # 100 iterations are sufficient for double precision convergence.\n    for _ in range(100):\n        mid = low + (high - low) / np.float64(2.0)\n        # Check if the naive computation results in zero\n        if np.exp(mid) - np.float64(1.0) == np.float64(0.0):\n            # If so, mid is a candidate; try larger values.\n            low = mid\n        else:\n            # If not, mid is too large.\n            high = mid\n    x_crit = low\n\n    # Task 3: Compute the value using the cancellation-free implementation.\n    y_alt_x_crit = np.expm1(x_crit)\n\n    # Task 4: Compare relative errors for the test suite.\n    \n    # Set precision for the high-precision reference calculation.\n    getcontext().prec = 50\n\n    # Define the test suite of inputs.\n    test_inputs = [\n        x_crit,\n        x_crit / np.float64(2.0),\n        eps_hat,\n        eps_hat / np.float64(2.0),\n        np.sqrt(eps_hat),\n        -x_crit,\n        -x_crit / np.float64(2.0),\n        -eps_hat,\n        -eps_hat / np.float64(2.0),\n        -np.sqrt(eps_hat)\n    ]\n\n    boolean_results = []\n    for x_float in test_inputs:\n        x_dec = Decimal(x_float)\n        \n        # Compute high-precision \"true\" value\n        true_val = x_dec.exp() - Decimal(1)\n\n        # Compute values using naive and stable methods in float64\n        naive_val = np.exp(x_float) - np.float64(1.0)\n        alt_val = np.expm1(x_float)\n        \n        # Convert to Decimal for high-precision error calculation\n        naive_val_dec = Decimal(naive_val)\n        alt_val_dec = Decimal(alt_val)\n\n        # Avoid division by zero if true_val is zero (not possible for these inputs)\n        if true_val == Decimal(0):\n            # This case shouldn't be reached with the given test inputs.\n            # If it were, relative error is ill-defined. We could define error as absolute.\n            # For this problem, we assume it's non-zero.\n            rel_err_naive = Decimal('inf') if naive_val_dec != 0 else Decimal(0)\n            rel_err_alt = Decimal('inf') if alt_val_dec != 0 else Decimal(0)\n        else:\n            rel_err_naive = abs(naive_val_dec - true_val) / abs(true_val)\n            rel_err_alt = abs(alt_val_dec - true_val) / abs(true_val)\n        \n        # Compare relative errors\n        is_naive_worse = rel_err_naive  rel_err_alt\n        boolean_results.append(is_naive_worse)\n\n    # Assemble the final list of results.\n    final_results = [eps_hat, x_crit, y_alt_x_crit] + boolean_results\n    \n    # Format the final list as a string per problem specification.\n    # Booleans must be lowercase 'true' or 'false'.\n    str_results = []\n    for item in final_results:\n        if isinstance(item, bool):\n            str_results.append(str(item).lower())\n        else:\n            str_results.append(str(item))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str_results)}]\")\n\nsolve()\n```", "id": "3250061"}]}