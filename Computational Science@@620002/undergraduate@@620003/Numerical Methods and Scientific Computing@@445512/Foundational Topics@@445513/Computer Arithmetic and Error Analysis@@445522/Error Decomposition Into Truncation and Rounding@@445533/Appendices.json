{"hands_on_practices": [{"introduction": "In numerical computing, mathematical formulas that are identical in exact arithmetic can have vastly different behaviors when evaluated using finite-precision floating-point numbers. This exercise explores one of the most classic examples of this phenomenon, a situation known as \"catastrophic cancellation.\" This practice will challenge you to analyze why a straightforward computation can fail dramatically and how a simple algebraic reformulation can restore accuracy, forcing a sharp distinction between rounding error and the concept of truncation error. [@problem_id:3225142]", "problem": "Consider evaluating the function $f(x)=\\sqrt{x+1}-\\sqrt{x}$ for large positive $x$ in a floating-point system that adheres to the Institute of Electrical and Electronics Engineers (IEEE) 754 floating-point standard (IEEE 754) with rounding to nearest. Let the unit roundoff be $u$, and adopt the standard floating-point model: each elementary operation (including $\\sqrt{\\cdot}$, $+$, $-$, and $/$) produces a result equal to the exact result times $(1+\\delta)$, with $|\\delta|\\le u$, and independent instances of $\\delta$ are bounded but may differ. Assume $\\sqrt{\\cdot}$ is correctly rounded as specified by IEEE 754, so no algorithmic iteration is used and any error is purely rounding. You are asked to examine truncation versus rounding errors and the effect of algebraic reformulation on numerical stability.\n\nTwo algorithms are considered:\n- Algorithm N (naive): compute $a=\\sqrt{x+1}$ and $b=\\sqrt{x}$, then return $a-b$.\n- Algorithm R (reformulated): use an algebraic expression that avoids subtracting nearly equal quantities and then evaluate it with floating-point operations.\n\nAnswer the following by selecting all options that are correct. Your reasoning should start from the floating-point rounding model and the definition of truncation error (error from approximating the mathematical problem by a finite process) versus rounding error (error from finite precision arithmetic).\n\nA. Under the stated model, Algorithm N has zero truncation error, and its relative rounding error grows approximately like $2ux$ for large $x$ due to cancellation in $a-b$.\n\nB. Under the stated model, Algorithm R has zero truncation error, and its relative rounding error is bounded by a constant times $u$, independent of $x$.\n\nC. Rewriting $\\sqrt{x+1}-\\sqrt{x}$ as $\\dfrac{1}{\\sqrt{x+1}+\\sqrt{x}}$ changes the mathematical value and thus introduces truncation error.\n\nD. For $x$ such that $ux\\gtrsim 1$, Algorithm N’s relative error becomes $\\mathcal{O}(1)$ (or worse) and the result may lose most significant digits, whereas Algorithm R avoids this breakdown.\n\nE. In Algorithm N, because the exact value $f(x)$ is $\\mathcal{O}\\!\\left(\\dfrac{1}{\\sqrt{x}}\\right)$, the absolute rounding error in $a-b$ is $\\mathcal{O}\\!\\left(\\dfrac{u}{\\sqrt{x}}\\right)$.", "solution": "The user wants a critical analysis of a numerical methods problem concerning the evaluation of $f(x)=\\sqrt{x+1}-\\sqrt{x}$ for large $x$.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n-   Function to evaluate: $f(x)=\\sqrt{x+1}-\\sqrt{x}$ for large positive $x$.\n-   Floating-point system: Conforms to IEEE 754 with rounding to nearest.\n-   Unit roundoff: $u$.\n-   Floating-point model: For an elementary operation `op`, the floating-point result is $fl(\\text{result}) = (\\text{exact result}) \\times (1+\\delta)$, where $|\\delta| \\le u$. This applies to $\\sqrt{\\cdot}$, $+$, $-$, and $/$.\n-   Assumption on $\\sqrt{\\cdot}$: It is correctly rounded according to IEEE 754.\n-   Algorithm N (naive): Compute $a=\\sqrt{x+1}$ and $b=\\sqrt{x}$, then return $a-b$.\n-   Algorithm R (reformulated): Use an algebraic expression that avoids subtracting nearly equal quantities. The standard reformulation is $f(x) = \\dfrac{1}{\\sqrt{x+1}+\\sqrt{x}}$.\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientific Grounding**: The problem is a canonical example in numerical analysis used to demonstrate catastrophic cancellation (loss of significance) and the benefits of algorithmic reformulation. The floating-point model is a standard for first-order forward error analysis. The concepts of truncation error and rounding error are central to numerical methods. The problem is scientifically sound.\n2.  **Well-Posed**: The problem is well-posed. It provides two algorithms and a clear computational model, asking for an analysis of the resulting errors. The questions posed in the options are specific and can be answered through rigorous derivation based on the provided model.\n3.  **Objective**: The problem statement uses precise, unambiguous terminology from the field of numerical analysis. There are no subjective or opinion-based elements.\n4.  **Completeness**: The problem provides all necessary information to perform the error analysis.\n5.  **Consistency**: There are no contradictions in the provided information.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is valid. It is a standard, well-posed problem in numerical analysis. The solution process will proceed.\n\n### Solution Derivation\n\nFirst, let's analyze the exact mathematical function $f(x)=\\sqrt{x+1}-\\sqrt{x}$ for large positive $x$.\nFor large $x$, $\\sqrt{x+1}$ and $\\sqrt{x}$ are very close in value.\nUsing a Taylor expansion for $\\sqrt{1+z}$ around $z=0$, where $z=1/x$:\n$\\sqrt{1+z} = 1 + \\frac{1}{2}z - \\frac{1}{8}z^2 + \\mathcal{O}(z^3)$.\nSo, $f(x) = \\sqrt{x}\\left(\\sqrt{1+\\frac{1}{x}}-1\\right) = \\sqrt{x}\\left(\\left(1 + \\frac{1}{2x} - \\frac{1}{8x^2} + \\mathcal{O}(x^{-3})\\right) - 1\\right) = \\frac{1}{2\\sqrt{x}} - \\frac{1}{8x^{3/2}} + \\mathcal{O}(x^{-5/2})$.\nFor large $x$, the true value is $f(x) \\approx \\frac{1}{2\\sqrt{x}}$, which is of order $\\mathcal{O}(x^{-1/2})$.\n\nThe reformulated expression is obtained by multiplying by the conjugate:\n$f(x) = (\\sqrt{x+1}-\\sqrt{x}) \\frac{\\sqrt{x+1}+\\sqrt{x}}{\\sqrt{x+1}+\\sqrt{x}} = \\frac{(x+1)-x}{\\sqrt{x+1}+\\sqrt{x}} = \\frac{1}{\\sqrt{x+1}+\\sqrt{x}}$.\nThis is an exact algebraic identity.\n\n**Analysis of Algorithm N (Naive)**\n\nAlgorithm N computes $f_N(x) = fl(fl(\\sqrt{x+1}) - fl(\\sqrt{x}))$.\nLet's analyze the rounding errors.\nThe computation proceeds in three steps:\n1.  $a_c = fl(\\sqrt{x+1}) = \\sqrt{x+1}(1+\\delta_1)$, with $|\\delta_1| \\le u$.\n2.  $b_c = fl(\\sqrt{x}) = \\sqrt{x}(1+\\delta_2)$, with $|\\delta_2| \\le u$.\n3.  $f_N(x) = fl(a_c - b_c) = (a_c - b_c)(1+\\delta_3)$, with $|\\delta_3| \\le u$.\n\nSubstituting the first two steps into the third:\n$f_N(x) = (\\sqrt{x+1}(1+\\delta_1) - \\sqrt{x}(1+\\delta_2))(1+\\delta_3)$\n$f_N(x) = ((\\sqrt{x+1}-\\sqrt{x}) + \\sqrt{x+1}\\delta_1 - \\sqrt{x}\\delta_2)(1+\\delta_3)$\nLet $f(x) = \\sqrt{x+1}-\\sqrt{x}$.\n$f_N(x) = (f(x) + \\sqrt{x+1}\\delta_1 - \\sqrt{x}\\delta_2)(1+\\delta_3)$\nTo first order in $\\delta_i$, the absolute error $E_{abs} = f_N(x) - f(x)$ is:\n$E_{abs} \\approx f(x)\\delta_3 + \\sqrt{x+1}\\delta_1 - \\sqrt{x}\\delta_2$.\nFor large $x$, $f(x) \\approx \\frac{1}{2\\sqrt{x}}$ and $\\sqrt{x+1} \\approx \\sqrt{x}$.\n$E_{abs} \\approx \\frac{\\delta_3}{2\\sqrt{x}} + \\sqrt{x}(\\delta_1 - \\delta_2)$.\nThe term $\\sqrt{x}(\\delta_1 - \\delta_2)$ dominates for large $x$. The maximum magnitude of this error is approximately $2u\\sqrt{x}$.\n\nThe relative error is $E_{rel} = \\frac{E_{abs}}{f(x)}$.\n$E_{rel} \\approx \\frac{\\sqrt{x}(\\delta_1 - \\delta_2)}{1/(2\\sqrt{x})} = 2x(\\delta_1 - \\delta_2)$.\nThe magnitude of the relative rounding error is bounded by approximately $|E_{rel}| \\le 2x(|\\delta_1|+|\\delta_2|) \\le 4ux$. Thus, the relative error grows proportionally to $x$.\n\nTruncation error is the error introduced by approximating a mathematical procedure, such as replacing an infinite series with a finite sum. Algorithm N directly implements the exact formula for $f(x)$. No mathematical approximation is made. Therefore, Algorithm N has zero truncation error.\n\n**Analysis of Algorithm R (Reformulated)**\n\nAlgorithm R computes $f_R(x) = fl\\left(\\frac{1}{fl(fl(\\sqrt{x+1}) + fl(\\sqrt{x}))}\\right)$.\nLet's analyze the propagation of errors.\n1.  $a_c = fl(\\sqrt{x+1}) = \\sqrt{x+1}(1+\\delta_1)$.\n2.  $b_c = fl(\\sqrt{x}) = \\sqrt{x}(1+\\delta_2)$.\n3.  $s_c = fl(a_c+b_c) = (a_c+b_c)(1+\\delta_3) = (\\sqrt{x+1}(1+\\delta_1) + \\sqrt{x}(1+\\delta_2))(1+\\delta_3)$.\n4.  $f_R(x) = fl(1/s_c) = (1/s_c)(1+\\delta_4)$.\n\nAll operations in Algorithm R are benign. The addition $\\sqrt{x+1}+\\sqrt{x}$ involves two positive, non-close numbers, so it is well-conditioned. The division by this sum is also well-conditioned.\nLet's find the total relative error:\n$f_R(x) \\approx \\frac{1+\\delta_4}{(\\sqrt{x+1}+\\sqrt{x})(1+\\delta_3)(1+\\frac{\\sqrt{x+1}\\delta_1+\\sqrt{x}\\delta_2}{\\sqrt{x+1}+\\sqrt{x}})}$\n$f_R(x) \\approx f(x) \\frac{1+\\delta_4}{(1+\\delta_3)(1+\\epsilon)}$, where $\\epsilon = \\frac{\\sqrt{x+1}\\delta_1+\\sqrt{x}\\delta_2}{\\sqrt{x+1}+\\sqrt{x}}$.\nFor large $x$, $\\frac{\\sqrt{x+1}}{\\sqrt{x+1}+\\sqrt{x}} \\approx \\frac{1}{2}$ and $\\frac{\\sqrt{x}}{\\sqrt{x+1}+\\sqrt{x}} \\approx \\frac{1}{2}$. Thus $|\\epsilon| \\le \\frac{1}{2}|\\delta_1| + \\frac{1}{2}|\\delta_2| \\le u$.\nTo first order, the relative error is $E_{rel} = \\frac{f_R(x)-f(x)}{f(x)} \\approx \\delta_4 - \\delta_3 - \\epsilon \\approx \\delta_4 - \\delta_3 - \\frac{1}{2}(\\delta_1+\\delta_2)$.\nThe magnitude of the relative error is bounded by $|E_{rel}| \\le |\\delta_4| + |\\delta_3| + \\frac{1}{2}|\\delta_1| + \\frac{1}{2}|\\delta_2| \\le u + u + \\frac{1}{2}u + \\frac{1}{2}u = 3u$.\nThe relative rounding error is bounded by a small constant multiple of $u$ and is independent of $x$.\n\nThe reformulation $f(x) = \\frac{1}{\\sqrt{x+1}+\\sqrt{x}}$ is an exact mathematical identity. Algorithm R implements this exact formula. Therefore, Algorithm R also has zero truncation error.\n\n### Option-by-Option Analysis\n\n**A. Under the stated model, Algorithm N has zero truncation error, and its relative rounding error grows approximately like $2ux$ for large $x$ due to cancellation in $a-b$.**\n-   **Truncation Error**: As established, Algorithm N computes the exact function $f(x)$ with floating-point numbers. It does not rely on a mathematical approximation (like a truncated series). Hence, its truncation error is zero. This part is correct.\n-   **Rounding Error**: Our analysis shows the relative rounding error is approximately $2x(\\delta_1 - \\delta_2)$, with a magnitude bounded by $4ux$. The statement says it \"grows approximately like $2ux$\". This correctly captures the linear dependence on both $u$ and $x$. The factor of $2$ versus $4$ is a minor detail of the specific error bounding method, and \"approximately like\" is sufficiently general. The cause is correctly identified as cancellation.\n-   **Verdict**: Correct.\n\n**B. Under the stated model, Algorithm R has zero truncation error, and its relative rounding error is bounded by a constant times $u$, independent of $x$.**\n-   **Truncation Error**: The reformulation is an exact algebraic identity. Like Algorithm N, Algorithm R makes no mathematical approximation, so its truncation error is zero. This part is correct.\n-   **Rounding Error**: Our analysis showed the relative rounding error for Algorithm R is bounded by approximately $3u$. This is a constant multiple of $u$ and does not depend on $x$. This part is correct. The algorithm is numerically stable.\n-   **Verdict**: Correct.\n\n**C. Rewriting $\\sqrt{x+1}-\\sqrt{x}$ as $\\dfrac{1}{\\sqrt{x+1}+\\sqrt{x}}$ changes the mathematical value and thus introduces truncation error.**\n-   The statement claims the reformulation changes the mathematical value. This is false. For $x \\ge 0$, $\\sqrt{x+1}-\\sqrt{x} = \\dfrac{1}{\\sqrt{x+1}+\\sqrt{x}}$ is a strict mathematical identity.\n-   Since the transformation is exact, it does not introduce any truncation error. Truncation error stems from mathematical approximation, not exact algebra.\n-   **Verdict**: Incorrect.\n\n**D. For $x$ such that $ux\\gtrsim 1$, Algorithm N’s relative error becomes $\\mathcal{O}(1)$ (or worse) and the result may lose most significant digits, whereas Algorithm R avoids this breakdown.**\n-   From the analysis of Algorithm N, the relative error is $\\approx C \\cdot ux$ for some small constant $C$ (e.g., $C \\le 4$).\n-   If $ux \\gtrsim 1$, then the relative error is $\\gtrsim C$, which is of order $\\mathcal{O}(1)$. A relative error of this magnitude means the computed answer has few or no correct significant figures compared to the true answer (e.g., computing $0.1$ when the answer is $1.0$). This is the definition of loss of significance.\n-   Algorithm R's relative error is bounded by $\\approx 3u$, which remains small regardless of $x$. Thus, it avoids this catastrophic breakdown.\n-   **Verdict**: Correct.\n\n**E. In Algorithm N, because the exact value $f(x)$ is $\\mathcal{O}\\!\\left(\\dfrac{1}{\\sqrt{x}}\\right)$, the absolute rounding error in $a-b$ is $\\mathcal{O}\\!\\left(\\dfrac{u}{\\sqrt{x}}\\right)$.**\n-   The exact value $f(x)$ is indeed $\\mathcal{O}(x^{-1/2})$.\n-   However, our analysis of Algorithm N's absolute error $E_{abs}$ showed that $E_{abs} \\approx \\sqrt{x}(\\delta_1 - \\delta_2)$.\n-   Therefore, the absolute error is of order $\\mathcal{O}(u\\sqrt{x})$.\n-   The statement claims the absolute error is $\\mathcal{O}(u/\\sqrt{x})$. This would be true if the *relative* error were bounded by a constant. $E_{abs} = E_{rel} \\times f(x)$. If $E_{rel} = \\mathcal{O}(u)$, then $E_{abs} = \\mathcal{O}(u) \\times \\mathcal{O}(x^{-1/2}) = \\mathcal{O}(ux^{-1/2})$. But for Algorithm N, the relative error is $\\mathcal{O}(ux)$, leading to $E_{abs} = \\mathcal{O}(ux) \\times \\mathcal{O}(x^{-1/2}) = \\mathcal{O}(ux^{1/2})$.\n-   **Verdict**: Incorrect.", "answer": "$$\\boxed{ABD}$$", "id": "3225142"}, {"introduction": "While some numerical instabilities can be removed by reformulation, many numerical methods involve an inherent trade-off between two opposing sources of error. In numerical differentiation, for instance, there is a fundamental conflict: decreasing the step size $h$ reduces the *truncation error* (the error from approximating a derivative with a finite difference), but it simultaneously amplifies the impact of *rounding error* due to catastrophic cancellation. This practice provides a quintessential hands-on experience in modeling both error types analytically to find the \"sweet spot\"—the optimal step size $h$ that minimizes the total error by balancing these two competing effects. [@problem_id:3225238]", "problem": "You are given the symmetric $5$-point stencil for approximating the fourth derivative of a sufficiently smooth function $f$ at a point $x$:\n$$\nD^{(4)}_h f(x) \\;=\\; \\frac{f(x-2h) \\;-\\; 4 f(x-h) \\;+\\; 6 f(x) \\;-\\; 4 f(x+h) \\;+\\; f(x+2h)}{h^4}.\n$$\nConsider $f(x) = \\cos(x)$ and $x = 0$, with angles measured in radians. Assume function evaluations are exact and roundoff occurs only in the floating-point arithmetic used to form the linear combination in the numerator and the division by $h^4$. Adopt the following standard floating-point model: each elementary floating-point operation satisfies $\\operatorname{fl}(a \\circ b) = (a \\circ b)(1+\\delta)$ for $\\circ \\in \\{+,-,\\times,/\\}$ with $|\\delta| \\le u$, where $u$ is the unit roundoff of the arithmetic. Work in Institute of Electrical and Electronics Engineers (IEEE) 754 binary64 (double precision), taking $u = 2^{-53}$.\n\nUsing Taylor’s theorem about $x=0$, derive the leading-order truncation error of $D^{(4)}_h f(0)$ in powers of $h$ up to and including the first nonzero term beyond $f^{(4)}(0)$. Then, using the floating-point model and the bound $|f(t)| \\le 1$ for all real $t$, derive a first-order (in $u$) upper bound for the magnitude of the rounding error in $D^{(4)}_h f(0)$ in terms of $h$ and $u$. By equating the magnitudes of the truncation and rounding errors, determine the step size $h$ that balances these two error sources (the “sweet spot”). Round your final numerical value of $h$ to three significant figures.", "solution": "The user wants to analyze the error properties of a finite difference formula for the fourth derivative. The process involves three main steps: deriving the truncation error, deriving the rounding error bound, and then finding the optimal step size `h` that balances these two error sources.\n\n### Step 1: Problem Validation\n\n**1.1. Extract Givens:**\n- Finite difference formula: $D^{(4)}_h f(x) = \\frac{f(x-2h) - 4 f(x-h) + 6 f(x) - 4 f(x+h) + f(x+2h)}{h^4}$\n- Function and point: $f(x) = \\cos(x)$ at $x=0$.\n- Floating-point model: $\\operatorname{fl}(a \\circ b) = (a \\circ b)(1+\\delta)$ with $|\\delta| \\le u$.\n- Unit roundoff: $u = 2^{-53}$ (IEEE 754 binary64).\n- Function bound: $|f(t)| \\le 1$.\n- Assumption: Function evaluations are exact; roundoff only occurs during arithmetic operations.\n\n**1.2. Validate Using Extracted Givens:**\n- **Scientifically Grounded:** The problem is a standard exercise in numerical analysis, involving Taylor series for error analysis and a standard model for floating-point arithmetic. The finite difference stencil is a well-known centered difference formula for the fourth derivative. All concepts are firmly based on established principles of mathematics and computer science.\n- **Well-Posed:** The problem is clearly stated and asks for specific, derivable quantities: a truncation error term, a rounding error bound, and an optimal step size. The provided information is sufficient to find a unique and meaningful solution.\n- **Objective:** The problem is phrased using precise, objective mathematical language, free from any subjective or biased statements.\n\n**1.3. Verdict and Action:**\nThe problem is valid. We proceed to the solution.\n\n### Step 2: Derivation of the Truncation Error\n\nThe truncation error $T_h$ is defined as the difference between the approximation and the true value:\n$$T_h = D^{(4)}_h f(x) - f^{(4)}(x)$$\nTo find the leading-order term of $T_h$, we use Taylor's theorem to expand each function evaluation in the numerator of $D^{(4)}_h f(x)$ around the point $x$. For a sufficiently smooth function $f$, the expansions are:\n$$f(x \\pm ch) = \\sum_{k=0}^{\\infty} \\frac{(\\pm ch)^k}{k!} f^{(k)}(x)$$\nWe expand the terms in the numerator, $N(h) = f(x-2h) - 4 f(x-h) + 6 f(x) - 4 f(x+h) + f(x+2h)$, and collect terms by the order of the derivative $f^{(k)}(x)$.\n\n- For $f^{(0)}(x) = f(x)$: The coefficient is $1 - 4 + 6 - 4 + 1 = 0$.\n- For $f^{(1)}(x)$: The coefficient is $\\frac{h}{1!}(-2 - 4(-1) - 4(1) + 2) = h(-2+4-4+2) = 0$.\n- For $f^{(2)}(x)$: The coefficient is $\\frac{h^2}{2!}((-2)^2 - 4(-1)^2 - 4(1)^2 + (2)^2) = \\frac{h^2}{2}(4-4-4+4) = 0$.\n- For $f^{(3)}(x)$: The coefficient is $\\frac{h^3}{3!}((-2)^3 - 4(-1)^3 - 4(1)^3 + (2)^3) = \\frac{h^3}{6}(-8+4-4+8) = 0$.\n- For $f^{(4)}(x)$: The coefficient is $\\frac{h^4}{4!}((-2)^4 - 4(-1)^4 - 4(1)^4 + (2)^4) = \\frac{h^4}{24}(16-4-4+16) = \\frac{24h^4}{24} = h^4$.\n- For $f^{(5)}(x)$: The coefficient is $\\frac{h^5}{5!}((-2)^5 - 4(-1)^5 - 4(1)^5 + (2)^5) = \\frac{h^5}{120}(-32+4-4+32) = 0$.\n- For $f^{(6)}(x)$: The coefficient is $\\frac{h^6}{6!}((-2)^6 - 4(-1)^6 - 4(1)^6 + (2)^6) = \\frac{h^6}{720}(64-4-4+64) = \\frac{120h^6}{720} = \\frac{h^6}{6}$.\n\nThe numerator can thus be expressed as:\n$$N(h) = h^4 f^{(4)}(x) + \\frac{h^6}{6} f^{(6)}(x) + O(h^8)$$\nDividing by $h^4$ gives the expression for the approximation:\n$$D^{(4)}_h f(x) = \\frac{N(h)}{h^4} = f^{(4)}(x) + \\frac{h^2}{6} f^{(6)}(x) + O(h^4)$$\nThe leading-order truncation error is therefore:\n$$T_h = D^{(4)}_h f(x) - f^{(4)}(x) = \\frac{h^2}{6} f^{(6)}(x) + O(h^4)$$\nFor the specific function $f(x) = \\cos(x)$ at $x=0$, we evaluate the required derivatives:\n- $f^{(4)}(x) = \\cos(x) \\implies f^{(4)}(0) = \\cos(0) = 1$.\n- $f^{(5)}(x) = -\\sin(x)$.\n- $f^{(6)}(x) = -\\cos(x) \\implies f^{(6)}(0) = -\\cos(0) = -1$.\nSubstituting $f^{(6)}(0) = -1$ into the expression for $T_h$, the leading-order truncation error is:\n$$T_h = -\\frac{h^2}{6}$$\n\n### Step 3: Derivation of the Rounding Error Bound\n\nThe rounding error arises from the floating-point arithmetic used to compute the value of $D^{(4)}_h f(0)$. Let $\\tilde{D}^{(4)}_h f(0)$ be the computed value. The total rounding error is $R_h = \\tilde{D}^{(4)}_h f(0) - D^{(4)}_h f(0)$.\n\nThe computation involves forming a linear combination in the numerator and then a division. A dominant source of error comes from the evaluation of the numerator, especially because of catastrophic cancellation: the sum involves terms of order $O(1)$ which cancel out to produce a result of order $O(h^4)$.\n\nLet the exact numerator be $N = f(-2h) - 4f(-h) + 6f(0) - 4f(h) + f(2h)$.\nLet the computed numerator be $\\tilde{N}$. The error in computing this sum, $\\epsilon_N = \\tilde{N} - N$, is bounded by a multiple of the unit roundoff $u$ and the sum of the magnitudes of the terms being added. Let the terms be $y_1=f(-2h)$, $y_2=-4f(-h)$, $y_3=6f(0)$, $y_4=-4f(h)$, and $y_5=f(2h)$.\nA first-order bound on the error in computing the sum is:\n$$|\\epsilon_N| \\le u \\sum_{i=1}^5 |c_i f(x_i)|$$\nwhere the coefficients are $c_i \\in \\{1, -4, 6\\}$ and $x_i$ are the evaluation points. We also use the given bound $|f(t)| = |\\cos(t)| \\le 1$.\n$$|\\epsilon_N| \\le u \\left( |1 \\cdot f(-2h)| + |-4 \\cdot f(-h)| + |6 \\cdot f(0)| + |-4 \\cdot f(h)| + |1 \\cdot f(2h)| \\right)$$\n$$|\\epsilon_N| \\le u \\left( |f(-2h)| + 4|f(-h)| + 6|f(0)| + 4|f(h)| + |f(2h)| \\right)$$\nUsing $|f(t)| \\le 1$:\n$$|\\epsilon_N| \\le u (1 + 4 + 6 + 4 + 1) = 16u$$\nThe final computed value is $\\tilde{D}^{(4)}_h f(0) = \\operatorname{fl}(\\tilde{N}/h^4) = (\\tilde{N}/h^4)(1+\\delta)$ with $|\\delta| \\le u$.\nThe total rounding error is:\n$$R_h = \\frac{\\tilde{N}(1+\\delta)}{h^4} - \\frac{N}{h^4} = \\frac{(\\tilde{N}-N) + \\tilde{N}\\delta}{h^4} = \\frac{\\epsilon_N + \\tilde{N}\\delta}{h^4}$$\nTaking the magnitude:\n$$|R_h| \\le \\frac{|\\epsilon_N| + |\\tilde{N}||\\delta|}{h^4}$$\nWe have $|\\epsilon_N| \\le 16u$ and $|\\delta| \\le u$. For small $h$, $\\tilde{N} \\approx N \\approx h^4 f^{(4)}(0) = h^4$.\n$$|R_h| \\le \\frac{16u + |h^4|u}{h^4} = \\frac{16u}{h^4} + u$$\nFor small $h$, the first term dominates. Thus, a suitable upper bound for the magnitude of the rounding error is:\n$$|R_h| \\le \\frac{16u}{h^4}$$\n\n### Step 4: Determination of the Optimal Step Size\n\nThe total error is the sum of the truncation and rounding errors, $E_h = T_h + R_h$. The optimal step size $h$ is found where the magnitudes of these two error components are approximately equal, minimizing their sum.\n$$|T_h| \\approx |R_h|$$\nUsing the leading-order truncation error and the rounding error bound:\n$$\\left|-\\frac{h^2}{6}\\right| = \\frac{16u}{h^4}$$\n$$\\frac{h^2}{6} = \\frac{16u}{h^4}$$\nWe solve for $h$:\n$$h^6 = 6 \\times 16u = 96u$$\n$$h = (96u)^{1/6}$$\nSubstituting the given value for the unit roundoff, $u = 2^{-53}$:\n$$h = (96 \\cdot 2^{-53})^{1/6}$$\nWe can simplify the expression: $96 = 32 \\times 3 = 2^5 \\times 3$.\n$$h = (2^5 \\cdot 3 \\cdot 2^{-53})^{1/6} = (3 \\cdot 2^{5-53})^{1/6} = (3 \\cdot 2^{-48})^{1/6}$$\n$$h = 3^{1/6} \\cdot (2^{-48})^{1/6} = 3^{1/6} \\cdot 2^{-8}$$\nNow, we compute the numerical value:\n$$h = \\frac{3^{1/6}}{2^8} = \\frac{3^{1/6}}{256}$$\nUsing a calculator, $3^{1/6} \\approx 1.20093695$.\n$$h \\approx \\frac{1.20093695}{256} \\approx 0.00469115996$$\nRounding the result to three significant figures, we get:\n$$h \\approx 0.00469$$\nThis can be written in scientific notation as $4.69 \\times 10^{-3}$.", "answer": "$$\\boxed{4.69 \\times 10^{-3}}$$", "id": "3225238"}, {"introduction": "Beyond analytical models, a crucial skill in scientific computing is the ability to empirically diagnose and quantify errors in complex, multi-step simulations like solving differential equations. This practice introduces a powerful computational technique for \"error forensics\": using a high-precision reference calculation to numerically decompose the total error of a result into its truncation and rounding components. By implementing and comparing a high-order method in low precision (RK4 in single precision) with a low-order method in high precision (Forward Euler in double precision), you will gain practical insight into the important trade-off between algorithmic sophistication and arithmetic precision. [@problem_id:3225287]", "problem": "Consider the initial value problem $y' = -y$ with $y(0) = 1$. The exact solution is $y(t) = e^{-t}$. You will investigate error decomposition into truncation and rounding for two numerical time-stepping methods applied to this problem over a finite time interval $[0,T]$, with a uniform partition of $N$ steps of size $h = T/N$.\n\nYour task is to implement and compare:\n- A high-order method in single precision: the classical Fourth Order Runge-Kutta method (Fourth Order Runge-Kutta (RK4)) computed in binary $32$-bit floating point arithmetic (single precision).\n- A low-order method in double precision: the Forward Euler method computed in binary $64$-bit floating point arithmetic (double precision).\n\nUse the following foundational definitions and facts as the basis for your derivation and implementation:\n- The global truncation error of a one-step method of order $p$ is $O(h^p)$ under standard smoothness assumptions. The Forward Euler method has $p=1$, and Fourth Order Runge-Kutta has $p=4$.\n- Finite precision arithmetic introduces rounding error at each elementary operation due to the finite mantissa. The machine epsilon for binary $32$-bit is approximately $\\epsilon_{\\text{mach}} \\approx 2^{-24}$, and for binary $64$-bit is approximately $\\epsilon_{\\text{mach}} \\approx 2^{-53}$.\n- For a stable linear problem such as $y'=-y$, rounding errors introduced at each step propagate through the numerical recurrence but do not grow unbounded.\n\nTo decompose the total error into truncation and rounding components for a given method and $h$, use the following procedure, motivated by the conceptual separation of discretization and arithmetic:\n1. Compute a high-precision reference solution for the same discrete method and step size using arbitrary precision decimal arithmetic with sufficiently high precision (e.g., $80$ significant digits). This approximates the rounding-free discrete solution for that method. Denote this by $y_{\\text{ref}}$.\n2. Compute the floating-point numerical solution $y_{\\text{fp}}$ with the designated precision (single precision for RK4, double precision for Forward Euler).\n3. Compute the exact solution $y_{\\text{exact}} = e^{-T}$ using double precision arithmetic.\n4. Define the absolute total error as $E_{\\text{total}} = |y_{\\text{fp}} - y_{\\text{exact}}|$.\n5. Define the absolute truncation error as $E_{\\text{trunc}} = |y_{\\text{ref}} - y_{\\text{exact}}|$.\n6. Define the absolute rounding error as $E_{\\text{round}} = |y_{\\text{fp}} - y_{\\text{ref}}|$.\n\nImplement the RK4 method as follows for step size $h$ and current value $y_n$:\n1. $k_1 = h \\cdot f(y_n)$,\n2. $k_2 = h \\cdot f(y_n + \\tfrac{1}{2}k_1)$,\n3. $k_3 = h \\cdot f(y_n + \\tfrac{1}{2}k_2)$,\n4. $k_4 = h \\cdot f(y_n + k_3)$,\n5. $y_{n+1} = y_n + \\tfrac{1}{6}(k_1 + 2k_2 + 2k_3 + k_4)$,\nwhere $f(y) = -y$.\n\nImplement the Forward Euler method as follows:\n1. $y_{n+1} = y_n + h \\cdot f(y_n)$,\nwhere $f(y) = -y$.\n\nAll computations for the single precision RK4 must be carried out using binary $32$-bit floating point arithmetic, and all computations for the double precision Forward Euler must be carried out using binary $64$-bit floating point arithmetic. The high-precision reference computations for each method must be carried out using arbitrary precision decimal arithmetic with at least $80$ significant digits. The exact solution $y_{\\text{exact}}$ must be computed using double precision arithmetic.\n\nFor each test case, report a list of six floats in the order:\n$[E_{\\text{total}}^{\\text{RK4-single}}, E_{\\text{trunc}}^{\\text{RK4}}, E_{\\text{round}}^{\\text{RK4-single}}, E_{\\text{total}}^{\\text{Euler-double}}, E_{\\text{trunc}}^{\\text{Euler}}, E_{\\text{round}}^{\\text{Euler-double}}]$.\n\nUse absolute errors (no units are involved). Angles are not used. Do not express any quantity with a percentage sign.\n\nTest suite:\n- Case $1$: $(T, N_{\\text{RK4}}, N_{\\text{Euler}}) = (1, 10, 100)$.\n- Case $2$: $(T, N_{\\text{RK4}}, N_{\\text{Euler}}) = (1, 1000, 1000)$.\n- Case $3$: $(T, N_{\\text{RK4}}, N_{\\text{Euler}}) = (10, 1000, 10000)$.\n- Case $4$: $(T, N_{\\text{RK4}}, N_{\\text{Euler}}) = (1, 1, 1)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of six floats as specified (for example, $[[\\cdots],[\\cdots],[\\cdots],[\\cdots]]$).", "solution": "The user wants me to solve a numerical analysis problem by providing a runnable Python script. The problem involves implementing two numerical methods (RK4 and Forward Euler) for solving a simple ordinary differential equation ($y'=-y, y(0)=1$) and analyzing the results in terms of truncation and rounding errors.\n\n### Principle-Based Design\n\nThe core of the problem is to numerically decompose the total error of a computation into its constituent parts: truncation error (from the algorithm's approximation) and rounding error (from the machine's finite precision). The problem provides a well-defined methodology for this decomposition:\n\n1.  **Exact Solution ($y_{\\text{exact}}$):** The true mathematical solution, $y(t) = e^{-t}$.\n2.  **Floating-Point Solution ($y_{\\text{fp}}$):** The result obtained by running a given numerical method (e.g., RK4) with a specific step size $h$ and a specific finite precision (e.g., single precision). This result contains both truncation and rounding errors.\n3.  **High-Precision Reference ($y_{\\text{ref}}$):** The result obtained by running the *same* numerical method with the *same* step size $h$, but using extremely high arithmetic precision (e.g., 80 decimal digits). This result is effectively free of rounding error and thus represents the \"pure\" output of the discrete algorithm. Its deviation from the exact solution is therefore a direct measure of the truncation error.\n\nBased on these three quantities, the errors are defined as:\n-   **Total Error ($E_{\\text{total}} = |y_{\\text{fp}} - y_{\\text{exact}}|$):** The overall difference between what the computer produces and the true answer.\n-   **Truncation Error ($E_{\\text{trunc}} = |y_{\\text{ref}} - y_{\\text{exact}}|$):** The error inherent to the algorithm's approximation of the continuous problem.\n-   **Rounding Error ($E_{\\text{round}} = |y_{\\text{fp}} - y_{\\text{ref}}|$):** The error caused by performing the algorithm's steps with finite-precision arithmetic.\n\n### Implementation Strategy\n\nThe solution will be a Python script that faithfully implements this procedure for the given test cases.\n\n1.  **Solvers:** Four distinct solver functions will be created:\n    *   `rk4_fp`: Implements the RK4 method using `numpy` to enforce a specific floating-point precision (`np.float32`).\n    *   `forward_euler_fp`: Implements the Forward Euler method using `numpy` to enforce a specific precision (`np.float64`).\n    *   `rk4_high_precision`: Implements the RK4 method using Python's `decimal` library for the rounding-free reference calculation.\n    *   `forward_euler_high_precision`: Implements the Forward Euler method using the `decimal` library.\n2.  **Error Calculation:** For each test case, the script will:\n    *   Call the appropriate solvers to get $y_{\\text{fp}}$ and $y_{\\text{ref}}$ for both the RK4-single and Euler-double scenarios.\n    *   Calculate $y_{\\text{exact}}$ using `numpy.exp` in double precision, as specified.\n    *   Compute the six required absolute error values according to the definitions above. For subtractions involving different data types (e.g., `np.float32` and `np.float64`, or `Decimal` and `float`), values are promoted to Python's standard `float` (double precision) to ensure the comparison itself does not lose precision.\n3.  **Output:** The script will collect the list of six error values for each test case and print them in a single line, formatted exactly as specified: `[[...],[...],[...],[...]]`.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport decimal\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem, calculating and reporting error decompositions.\n    \"\"\"\n    \n    # Set precision for high-precision reference calculations.\n    # At least 80 is requested; 82 provides a safe margin.\n    decimal.getcontext().prec = 82\n\n    # --- Solver Implementations ---\n\n    def rk4_fp(T, N, y0, dtype):\n        \"\"\"\n        Solves y' = -y using RK4 method with specified numpy floating-point type.\n        \"\"\"\n        # Perform division in the target precision to model floating-point behavior accurately.\n        h = dtype(T) / dtype(N)\n        y = dtype(y0)\n        \n        # Pre-cast constants to the target data type.\n        c1_6 = dtype(1.0 / 6.0)\n        c1_2 = dtype(1.0 / 2.0)\n        c2 = dtype(2.0)\n        \n        for _ in range(N):\n            # All intermediate calculations are performed in the specified precision\n            # because y and h are of that type.\n            k1 = h * (-y)\n            k2 = h * (-(y + c1_2 * k1))\n            k3 = h * (-(y + c1_2 * k2))\n            k4 = h * (-(y + k3))\n            y = y + c1_6 * (k1 + c2 * k2 + c2 * k3 + k4)\n        return y\n\n    def forward_euler_fp(T, N, y0, dtype):\n        \"\"\"\n        Solves y' = -y using Forward Euler method with specified numpy floating-point type.\n        \"\"\"\n        h = dtype(T) / dtype(N)\n        y = dtype(y0)\n        \n        for _ in range(N):\n            y = y * (dtype(1.0) - h)\n        return y\n\n    def rk4_high_precision(T, N, y0_dec):\n        \"\"\"\n        Solves y' = -y using RK4 method with high-precision decimal arithmetic.\n        \"\"\"\n        T_dec = decimal.Decimal(T)\n        N_dec = decimal.Decimal(N)\n        h_dec = T_dec / N_dec\n        y_dec = y0_dec\n\n        # High-precision constants\n        c1_6 = decimal.Decimal(1) / decimal.Decimal(6)\n        c1_2 = decimal.Decimal(1) / decimal.Decimal(2)\n        c2 = decimal.Decimal(2)\n        \n        for _ in range(N):\n            k1 = h_dec * (-y_dec)\n            k2 = h_dec * (-(y_dec + c1_2 * k1))\n            k3 = h_dec * (-(y_dec + c1_2 * k2))\n            k4 = h_dec * (-(y_dec + k3))\n            y_dec = y_dec + c1_6 * (k1 + c2 * k2 + c2 * k3 + k4)\n        return y_dec\n\n    def forward_euler_high_precision(T, N, y0_dec):\n        \"\"\"\n        Solves y' = -y using Forward Euler method with high-precision decimal arithmetic.\n        \"\"\"\n        T_dec = decimal.Decimal(T)\n        N_dec = decimal.Decimal(N)\n        h_dec = T_dec / N_dec\n        y_dec = y0_dec\n        one = decimal.Decimal(1)\n        \n        for _ in range(N):\n            y_dec = y_dec * (one - h_dec)\n        return y_dec\n\n    # --- Test Cases and Main Loop ---\n\n    test_cases = [\n        (1, 10, 100),\n        (1, 1000, 1000),\n        (10, 1000, 10000),\n        (1, 1, 1),\n    ]\n\n    results = []\n    y0 = 1.0\n    y0_dec = decimal.Decimal(y0)\n\n    for case in test_cases:\n        T, N_rk4, N_euler = case\n        \n        # Calculate the exact solution using double precision as specified.\n        y_exact = np.exp(-np.float64(T))\n        \n        # --- RK4 with single precision (binary32) ---\n        y_fp_rk4 = rk4_fp(T, N_rk4, y0, np.float32)\n        y_ref_rk4 = rk4_high_precision(T, N_rk4, y0_dec)\n        \n        # Promote to Python's float (double precision) for error calculation\n        # to preserve significance.\n        E_total_rk4 = abs(float(y_fp_rk4) - y_exact)\n        E_trunc_rk4 = abs(float(y_ref_rk4) - y_exact)\n        E_round_rk4 = abs(float(y_fp_rk4) - float(y_ref_rk4))\n\n        # --- Forward Euler with double precision (binary64) ---\n        y_fp_euler = forward_euler_fp(T, N_euler, y0, np.float64)\n        y_ref_euler = forward_euler_high_precision(T, N_euler, y0_dec)\n        \n        E_total_euler = abs(y_fp_euler - y_exact)\n        E_trunc_euler = abs(float(y_ref_euler) - y_exact)\n        E_round_euler = abs(y_fp_euler - float(y_ref_euler))\n\n        case_results = [\n            E_total_rk4, E_trunc_rk4, E_round_rk4,\n            E_total_euler, E_trunc_euler, E_round_euler\n        ]\n        results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # The str() of a list produces '[...]', so joining these with commas and\n    # wrapping with brackets produces the required '[[...],[...]]' format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3225287"}]}