{"hands_on_practices": [{"introduction": "Catastrophic cancellation is a fundamental concept in numerical analysis, where subtracting two nearly equal numbers results in a dramatic loss of precision. This exercise [@problem_id:3231943] provides a concrete example of this phenomenon, allowing you to quantify the error. By calculating the backward error, you will see that the algorithm (subtraction) is stable, yet the forward error is large, demonstrating how a well-behaved algorithm can yield a poor result for an ill-conditioned problem.", "problem": "Let $x$ and $y$ be real numbers stored and subtracted in a decimal floating-point system that rounds to nearest with precision $p=7$ significant digits. Let the true inputs be $x=1.0000004$ and $y=1.0000001$. The stored values are $\\mathrm{fl}(x)$ and $\\mathrm{fl}(y)$, each obtained by rounding the true value to $7$ significant digits, and the computed subtraction is $z_{\\mathrm{comp}}=\\mathrm{fl}(\\mathrm{fl}(x)-\\mathrm{fl}(y))$. Use the core definitions of forward error and backward error, together with the fact that rounding to $p$ significant digits is a well-tested model of floating-point storage, to analyze the subtraction of nearly equal numbers. Specifically, determine the relative backward error in $x$, defined as $\\frac{|\\Delta x|}{|x|}$ where $\\Delta x$ is the smallest perturbation to $x$ such that the exact subtraction $(x+\\Delta x)-y$ equals $0$. Round your final numerical answer to $4$ significant figures.", "solution": "The objective is to determine the relative backward error in the input $x$ for the operation $x-y$, where the backward error is defined with respect to a target result of $0$. The given true inputs are $x=1.0000004$ and $y=1.0000001$. The relative backward error in $x$ is given by the formula $\\frac{|\\Delta x|}{|x|}$, where $\\Delta x$ is the smallest perturbation to $x$ that causes the exact subtraction to yield $0$.\n\nFirst, let us analyze the context provided by the floating-point system. The system is a decimal floating-point system with precision $p=7$ and rounds to the nearest representable number.\nThe true value of $x$ is $1.0000004$. To store this value with $7$ significant digits, we must round it. The number can be written as $1.0000004 \\times 10^0$. The $8^{th}$ significant digit is $4$. Since $4 < 5$, we round down.\nThus, the stored value of $x$ is $\\mathrm{fl}(x) = 1.000000$.\nThe true value of $y$ is $1.0000001$. Similarly, this is $1.0000001 \\times 10^0$. The $8^{th}$ significant digit is $1$. Since $1 < 5$, we round down.\nThus, the stored value of $y$ is $\\mathrm{fl}(y) = 1.000000$.\n\nThe computed subtraction is given by $z_{\\mathrm{comp}}=\\mathrm{fl}(\\mathrm{fl}(x)-\\mathrm{fl}(y))$.\nSubstituting the stored values:\n$z_{\\mathrm{comp}} = \\mathrm{fl}(1.000000 - 1.000000) = \\mathrm{fl}(0) = 0$.\nThis confirms that the computed result of the subtraction in the specified floating-point system is indeed $0$. The problem is therefore asking for the backward error of the actual computation performed, where the computed result is $0$. This phenomenon, where the subtraction of two nearly equal numbers results in a loss of precision (in this case, total loss), is known as catastrophic cancellation.\n\nThe core task is to find the perturbation $\\Delta x$ as defined in the problem. The problem states that $\\Delta x$ is the smallest perturbation to $x$ such that the exact subtraction $(x+\\Delta x)-y$ equals $0$.\nWe set up the equation based on this definition:\n$$ (x + \\Delta x) - y = 0 $$\nThis is a linear equation in $\\Delta x$. We can solve for $\\Delta x$ by rearranging the terms:\n$$ \\Delta x = y - x $$\nNow, we substitute the given true values for $x$ and $y$:\n$$ x = 1.0000004 $$\n$$ y = 1.0000001 $$\nSo, the perturbation $\\Delta x$ is:\n$$ \\Delta x = 1.0000001 - 1.0000004 = -0.0000003 $$\nThe problem specifies the \"smallest perturbation\". Since the equation for $\\Delta x$ has a unique solution, this solution is trivially the smallest in magnitude. The magnitude of the perturbation is:\n$$ |\\Delta x| = |-0.0000003| = 0.0000003 = 3 \\times 10^{-7} $$\n\nNext, we compute the relative backward error in $x$, which is defined as $\\frac{|\\Delta x|}{|x|}$.\nThe magnitude of $x$ is $|x| = |1.0000004| = 1.0000004$.\nThe relative backward error is therefore:\n$$ \\frac{|\\Delta x|}{|x|} = \\frac{3 \\times 10^{-7}}{1.0000004} $$\nNow, we perform the numerical division:\n$$ \\frac{|\\Delta x|}{|x|} \\approx 2.999998800000479... \\times 10^{-7} $$\nThe problem requires this final numerical answer to be rounded to $4$ significant figures. Let the value be $V = 2.9999988... \\times 10^{-7}$.\nThe first four significant digits are $2$, $9$, $9$, $9$. The fifth significant digit is $9$. Since $9 \\ge 5$, we must round up the fourth digit. Rounding $2.999$ up results in $3.000$. The trailing zeros are significant and must be included to indicate the precision of the rounded result.\nSo, the relative backward error, rounded to $4$ significant figures, is $3.000 \\times 10^{-7}$.\n\nThis small backward error indicates that the algorithm (subtraction) is backward stable for this instance, as a very small relative change in the input $x$ is sufficient to explain the computed output of $0$. However, it is important to note that the forward error is large. The true result is $x-y = 0.0000003$, while the computed result is $0$. The relative forward error is $\\frac{|(x-y) - z_{\\mathrm{comp}}|}{|x-y|} = \\frac{|0.0000003 - 0|}{|0.0000003|} = 1$, which represents a $100\\%$ error. This highlights the distinction between backward stability and forward error amplification.", "answer": "$$\\boxed{3.000 \\times 10^{-7}}$$", "id": "3231943"}, {"introduction": "The concept of ill-conditioning can be vividly illustrated through geometry, such as finding the intersection of two nearly parallel lines. This exercise [@problem_id:3232092] frames this familiar problem as a $2 \\times 2$ linear system, showing how small perturbations in the input data can lead to massive changes in the solution. By calculating the forward and backward errors, you will quantify the problem's sensitivity and compute an amplification factor that reveals the inherent numerical difficulty.", "problem": "You are given two lines in the plane, described in slope-intercept form by $y = m_1 x + d_1$ and $y = m_2 x + d_2$, with $m_1 = 1$, $d_1 = 0$, $m_2 = 1 + 10^{-6}$, and $d_2 = 1$. Their intersection can be framed as a $2 \\times 2$ linear system $A \\mathbf{x} = \\mathbf{c}$ where $A = \\begin{pmatrix} -m_1 & 1 \\\\ -m_2 & 1 \\end{pmatrix}$, $\\mathbf{x} = \\begin{pmatrix} x \\\\ y \\end{pmatrix}$, and $\\mathbf{c} = \\begin{pmatrix} d_1 \\\\ d_2 \\end{pmatrix}$. Because the lines are nearly parallel, the matrix $A$ is nearly singular. Suppose the coefficients are measured with small relative errors, yielding perturbed values $\\tilde{m}_2 = m_2 (1 + \\varepsilon_m)$ and $\\tilde{d}_2 = d_2 (1 + \\varepsilon_d)$ with $\\varepsilon_m = 10^{-8}$ and $\\varepsilon_d = -10^{-8}$, while $m_1$ and $d_1$ are unchanged. Treat $\\tilde{m}_2$ and $\\tilde{d}_2$ as the only perturbed data. \n\nStarting from the core definitions of forward error, backward error, and the geometric interpretation of line intersections, complete the following tasks:\n\n- Compute the exact intersection $\\mathbf{x}^{\\ast} = \\begin{pmatrix} x^{\\ast} \\\\ y^{\\ast} \\end{pmatrix}$ of the unperturbed lines.\n- Compute the intersection $\\tilde{\\mathbf{x}} = \\begin{pmatrix} \\tilde{x} \\\\ \\tilde{y} \\end{pmatrix}$ of the perturbed lines using the perturbed coefficients $\\tilde{m}_2$ and $\\tilde{d}_2$.\n- Using the Euclidean norm (also called the $2$-norm), evaluate the normwise relative forward error $\\frac{\\|\\tilde{\\mathbf{x}} - \\mathbf{x}^{\\ast}\\|_{2}}{\\|\\mathbf{x}^{\\ast}\\|_{2}}$.\n- Define the coefficientwise relative backward error as $\\max\\!\\left\\{\\frac{|\\tilde{m}_2 - m_2|}{|m_2|}, \\frac{|\\tilde{d}_2 - d_2|}{|d_2|}\\right\\}$ and evaluate it for the given perturbations.\n- Compute the amplification factor, defined as the ratio of the normwise relative forward error to the coefficientwise relative backward error, and provide its numerical value.\n\nRound your final amplification factor to four significant figures. No units are required.", "solution": "The problem asks for the analysis of the intersection of two lines, which is equivalent to solving a $2 \\times 2$ linear system. We are given the unperturbed system and a perturbed version and tasked with calculating the forward error, backward error, and their ratio (the amplification factor).\n\nThe two lines are described by:\nLine 1: $y = m_1 x + d_1$\nLine 2: $y = m_2 x + d_2$\n\nGiven unperturbed coefficients are $m_1 = 1$, $d_1 = 0$, $m_2 = 1 + 10^{-6}$, and $d_2 = 1$.\n\n**Step 1: Compute the exact intersection $\\mathbf{x}^{\\ast}$**\nThe intersection $(x^{\\ast}, y^{\\ast})$ is found by setting the expressions for $y$ equal:\n$$m_1 x^{\\ast} + d_1 = m_2 x^{\\ast} + d_2$$\nSolving for $x^{\\ast}$:\n$$(m_1 - m_2) x^{\\ast} = d_2 - d_1$$\n$$x^{\\ast} = \\frac{d_2 - d_1}{m_1 - m_2}$$\nSubstituting the given values:\n$$x^{\\ast} = \\frac{1 - 0}{1 - (1 + 10^{-6})} = \\frac{1}{-10^{-6}} = -10^6$$\nNow, we find $y^{\\ast}$ using the equation for Line 1:\n$$y^{\\ast} = m_1 x^{\\ast} + d_1 = (1)(-10^6) + 0 = -10^6$$\nThus, the exact intersection is $\\mathbf{x}^{\\ast} = \\begin{pmatrix} -10^6 \\\\ -10^6 \\end{pmatrix}$.\n\n**Step 2: Compute the perturbed intersection $\\tilde{\\mathbf{x}}$**\nThe coefficients $m_2$ and $d_2$ are perturbed. The new coefficients are $\\tilde{m}_2$ and $\\tilde{d}_2$.\nThe perturbations are given by $\\varepsilon_m = 10^{-8}$ and $\\varepsilon_d = -10^{-8}$.\n$$\\tilde{m}_2 = m_2(1 + \\varepsilon_m) = (1 + 10^{-6})(1 + 10^{-8}) = 1 + 10^{-6} + 10^{-8} + 10^{-14}$$\n$$\\tilde{d}_2 = d_2(1 + \\varepsilon_d) = 1(1 - 10^{-8}) = 1 - 10^{-8}$$\nThe coefficients $m_1$ and $d_1$ remain unchanged. The perturbed intersection $(\\tilde{x}, \\tilde{y})$ is found by solving the system with the perturbed coefficients:\n$$m_1 \\tilde{x} + d_1 = \\tilde{m}_2 \\tilde{x} + \\tilde{d}_2$$\n$$\\tilde{x} = \\frac{\\tilde{d}_2 - d_1}{m_1 - \\tilde{m}_2}$$\nSubstituting the values:\n$$\\tilde{x} = \\frac{(1 - 10^{-8}) - 0}{1 - (1 + 10^{-6} + 10^{-8} + 10^{-14})} = \\frac{1 - 10^{-8}}{-10^{-6} - 10^{-8} - 10^{-14}}$$\nFor the new $y$-coordinate, using the unperturbed Line 1 equation:\n$$\\tilde{y} = m_1 \\tilde{x} + d_1 = (1)\\tilde{x} + 0 = \\tilde{x}$$\nSo, the perturbed intersection is $\\tilde{\\mathbf{x}} = \\begin{pmatrix} \\tilde{x} \\\\ \\tilde{x} \\end{pmatrix}$.\n\n**Step 3: Evaluate the normwise relative forward error**\nThe normwise relative forward error is defined as $\\frac{\\|\\tilde{\\mathbf{x}} - \\mathbf{x}^{\\ast}\\|_{2}}{\\|\\mathbf{x}^{\\ast}\\|_{2}}$.\nFirst, calculate the norms and the difference vector.\n$$\\|\\mathbf{x}^{\\ast}\\|_{2} = \\sqrt{(x^{\\ast})^2 + (y^{\\ast})^2} = \\sqrt{(-10^6)^2 + (-10^6)^2} = \\sqrt{2 \\cdot 10^{12}} = \\sqrt{2} \\cdot 10^6$$\nThe difference vector is:\n$$\\tilde{\\mathbf{x}} - \\mathbf{x}^{\\ast} = \\begin{pmatrix} \\tilde{x} - (-10^6) \\\\ \\tilde{x} - (-10^6) \\end{pmatrix} = \\begin{pmatrix} \\tilde{x} + 10^6 \\\\ \\tilde{x} + 10^6 \\end{pmatrix}$$\nThe norm of the difference vector is:\n$$\\|\\tilde{\\mathbf{x}} - \\mathbf{x}^{\\ast}\\|_{2} = \\sqrt{(\\tilde{x} + 10^6)^2 + (\\tilde{x} + 10^6)^2} = \\sqrt{2(\\tilde{x} + 10^6)^2} = \\sqrt{2} |\\tilde{x} + 10^6|$$\nThe relative forward error, $E_{\\mathrm{fwd}}$, is the ratio of these norms:\n$$E_{\\mathrm{fwd}} = \\frac{\\sqrt{2} |\\tilde{x} + 10^6|}{\\sqrt{2} \\cdot 10^6} = \\frac{|\\tilde{x} + 10^6|}{10^6}$$\nLet's compute the value of $\\tilde{x} + 10^6$:\n$$ \\tilde{x} + 10^6 = \\frac{1 - 10^{-8}}{-(10^{-6} + 10^{-8} + 10^{-14})} + 10^6 $$\n$$ = \\frac{(1 - 10^{-8}) - 10^6(10^{-6} + 10^{-8} + 10^{-14})}{-(10^{-6} + 10^{-8} + 10^{-14})} $$\n$$ = \\frac{1 - 10^{-8} - (1 + 10^{-2} + 10^{-8})}{-(10^{-6} + 10^{-8} + 10^{-14})} $$\n$$ = \\frac{-10^{-2} - 2 \\cdot 10^{-8}}{-(10^{-6} + 10^{-8} + 10^{-14})} = \\frac{10^{-2} + 2 \\cdot 10^{-8}}{10^{-6} + 10^{-8} + 10^{-14}} $$\nTherefore, the relative forward error is:\n$$ E_{\\mathrm{fwd}} = \\frac{1}{10^6} \\left( \\frac{10^{-2} + 2 \\cdot 10^{-8}}{10^{-6} + 10^{-8} + 10^{-14}} \\right) = \\frac{10^{-8} + 2 \\cdot 10^{-14}}{10^{-6} + 10^{-8} + 10^{-14}} $$\nWe can factor out common terms to simplify:\n$$ E_{\\mathrm{fwd}} = \\frac{10^{-2}(1 + 2 \\cdot 10^{-6})}{10^6 \\cdot 10^{-6}(1 + 10^{-2} + 10^{-8})} = 10^{-2} \\frac{1 + 2 \\cdot 10^{-6}}{1 + 10^{-2} + 10^{-8}} $$\n\n**Step 4: Evaluate the coefficientwise relative backward error**\nThe coefficientwise relative backward error, $E_{\\mathrm{bwd}}$, is defined as:\n$$E_{\\mathrm{bwd}} = \\max\\!\\left\\{\\frac{|\\tilde{m}_2 - m_2|}{|m_2|}, \\frac{|\\tilde{d}_2 - d_2|}{|d_2|}\\right\\}$$\nUsing the definitions $\\tilde{m}_2 = m_2(1+\\varepsilon_m)$ and $\\tilde{d}_2 = d_2(1+\\varepsilon_d)$:\n$$\\frac{|\\tilde{m}_2 - m_2|}{|m_2|} = \\frac{|m_2(1+\\varepsilon_m) - m_2|}{|m_2|} = \\frac{|m_2 \\varepsilon_m|}{|m_2|} = |\\varepsilon_m|$$\n$$\\frac{|\\tilde{d}_2 - d_2|}{|d_2|} = \\frac{|d_2(1+\\varepsilon_d) - d_2|}{|d_2|} = \\frac{|d_2 \\varepsilon_d|}{|d_2|} = |\\varepsilon_d|$$\nGiven $\\varepsilon_m = 10^{-8}$ and $\\varepsilon_d = -10^{-8}$:\n$$E_{\\mathrm{bwd}} = \\max\\{|10^{-8}|, |-10^{-8}|\\} = \\max\\{10^{-8}, 10^{-8}\\} = 10^{-8}$$\n\n**Step 5: Compute the amplification factor**\nThe amplification factor is the ratio of the relative forward error to the relative backward error:\n$$\\text{Amplification Factor} = \\frac{E_{\\mathrm{fwd}}}{E_{\\mathrm{bwd}}}$$\n$$\\text{Amplification Factor} = \\frac{10^{-2} \\frac{1 + 2 \\cdot 10^{-6}}{1 + 10^{-2} + 10^{-8}}}{10^{-8}} = 10^6 \\frac{1 + 2 \\cdot 10^{-6}}{1 + 10^{-2} + 10^{-8}}$$\nNow, we compute the numerical value:\n$$\\text{Amplification Factor} = 10^6 \\frac{1.000002}{1.01000001} \\approx 10^6(0.990100980297)$$\n$$\\text{Amplification Factor} \\approx 990100.980297$$\nThe problem requires rounding the final answer to four significant figures. The first four significant digits are $9, 9, 0, 1$. The fifth digit is $0$, so we round down.\n$$\\text{Amplification Factor} \\approx 990100$$\nIn scientific notation, this is $9.901 \\times 10^5$.", "answer": "$$\\boxed{9.901 \\times 10^5}$$", "id": "3232092"}, {"introduction": "Not all algorithms are created equal, even if they are mathematically equivalent in exact arithmetic. This practice [@problem_id:3232097] explores the critical difference in numerical stability between two methods for QR factorization: Classical Gram–Schmidt (CGS) and Modified Gram–Schmidt (MGS). By measuring the loss of orthogonality as a natural form of backward error, you will directly observe why MGS is preferred in practice and learn to distinguish an algorithm's instability from a problem's inherent ill-conditioning.", "problem": "You are given the task of comparing forward error and backward error for two widely used orthonormalization procedures, classical Gram–Schmidt and modified Gram–Schmidt, implemented in standard floating-point arithmetic. Use the definitions of forward error and backward error grounded in the model of floating-point arithmetic: for any basic operation applied to real numbers, the computed result can be represented as $\\mathrm{fl}(x \\,\\mathrm{op}\\, y) = (x \\,\\mathrm{op}\\, y)(1+\\delta)$ where $\\mathrm{op} \\in \\{+,-,\\times,\\div\\}$ and $|\\delta| \\le u$, with $u$ being the unit roundoff of the arithmetic in use. In exact arithmetic, the Gram–Schmidt method produces a factorization $A = Q R$ with $Q^\\top Q = I$ and $R$ upper triangular with nonnegative diagonal. In floating-point arithmetic, the computed factors $\\widehat{Q}$ and $\\widehat{R}$ satisfy $A \\approx \\widehat{Q}\\widehat{R}$ and $\\widehat{Q}^\\top \\widehat{Q} \\approx I$.\n\nYour program must:\n- Implement two functions that, given a full-rank or rank-deficient real matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m \\ge n$, return $(Q,R)$ using:\n  1. Classical Gram–Schmidt (single pass, no reorthogonalization).\n  2. Modified Gram–Schmidt (single pass, no reorthogonalization).\n- For each $(Q,R)$ pair and each input matrix $A$, compute:\n  1. The forward error of the factorization as the relative spectral-norm residual\n     $$\\eta_{\\mathrm{fwd}} = \\frac{\\lVert A - Q R \\rVert_2}{\\lVert A \\rVert_2}.$$\n  2. The backward error expressed as loss of orthogonality via\n     $$F = Q^\\top Q - I,\\quad \\eta_{\\mathrm{orth}} = \\lVert F \\rVert_2.$$\n- Use the spectral matrix norm $\\lVert \\cdot \\rVert_2$ for all matrix norms.\n- Round each reported scalar to $12$ decimal places.\n\nTest suite:\n- Case $1$ (well-conditioned tall matrix):\n  $$A_1 = \\begin{bmatrix}\n  1 & 2 & 3 & 4 \\\\\n  2 & 1 & 0 & 1 \\\\\n  0 & 1 & 2 & 3 \\\\\n  1 & 0 & 1 & 0 \\\\\n  2 & 2 & 2 & 2 \\\\\n  3 & 1 & 4 & 1\n  \\end{bmatrix} \\in \\mathbb{R}^{6 \\times 4}.$$\n- Case $2$ (nearly dependent columns): Let $\\varepsilon = 10^{-8}$ and\n  $$c_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{bmatrix},\\quad\n  w = \\begin{bmatrix} 1 \\\\ -1 \\\\ 1 \\\\ -1 \\\\ 1 \\end{bmatrix},\\quad\n  c_2 = c_1 + \\varepsilon w,\\quad\n  c_3 = \\begin{bmatrix} 2 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 2 \\end{bmatrix},\\quad\n  A_2 = [\\, c_1\\ \\ c_2\\ \\ c_3 \\,] \\in \\mathbb{R}^{5 \\times 3}.$$\n- Case $3$ (Hilbert-type tall matrix, ill-conditioned columns): For $m = 8$, $n = 5$, define\n  $$\\left(A_3\\right)_{i,j} = \\frac{1}{i + j + 1},\\quad 0 \\le i \\le 7,\\ 0 \\le j \\le 4,$$\n  so that $A_3 \\in \\mathbb{R}^{8 \\times 5}$.\n- Case $4$ (already orthonormal columns): Let $m = 6$, $n = 3$, and\n  $$A_4 = \\begin{bmatrix}\n  1 & 0 & 0 \\\\\n  0 & 0 & 0 \\\\\n  0 & 1 & 0 \\\\\n  0 & 0 & 0 \\\\\n  0 & 0 & 0 \\\\\n  0 & 0 & 1\n  \\end{bmatrix} \\in \\mathbb{R}^{6 \\times 3}.$$\n\nFor each test case $A_k$, compute four scalars:\n- $\\eta_{\\mathrm{fwd}}^{\\mathrm{CGS}}$,\n- $\\eta_{\\mathrm{fwd}}^{\\mathrm{MGS}}$,\n- $\\eta_{\\mathrm{orth}}^{\\mathrm{CGS}}$,\n- $\\eta_{\\mathrm{orth}}^{\\mathrm{MGS}}$,\neach rounded to $12$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for all four cases as a comma-separated list of lists, each inner list ordered as\n  $$[\\, \\eta_{\\mathrm{fwd}}^{\\mathrm{CGS}},\\ \\eta_{\\mathrm{fwd}}^{\\mathrm{MGS}},\\ \\eta_{\\mathrm{orth}}^{\\mathrm{CGS}},\\ \\eta_{\\mathrm{orth}}^{\\mathrm{MGS}} \\,],$$\n  enclosed in square brackets. For example: \n  $$[\\ [a_1,b_1,c_1,d_1],\\ [a_2,b_2,c_2,d_2],\\ [a_3,b_3,c_3,d_3],\\ [a_4,b_4,c_4,d_4]\\ ].$$\n\nAll computations must be performed in standard double-precision floating-point arithmetic. No user input is allowed; the program must be self-contained and must print exactly one line in the specified format. All reported values are unitless real numbers rounded to $12$ decimal places.", "solution": "This problem requires the implementation and comparison of two standard algorithms, Classical Gram-Schmidt (CGS) and Modified Gram-Schmidt (MGS), for computing the QR factorization of a matrix. The task is scientifically grounded, providing precise mathematical definitions for the algorithms and the error metrics to be computed. The goal is to highlight the numerical properties of CGS and MGS.\n\nThe core of the problem is to compute the QR factorization of a given matrix $A \\in \\mathbb{R}^{m \\times n}$ (where $m \\ge n$), which is a decomposition $A = QR$, where $Q \\in \\mathbb{R}^{m \\times n}$ has orthonormal columns ($Q^\\top Q = I_n$) and $R \\in \\mathbb{R}^{n \\times n}$ is an upper triangular matrix. In floating-point arithmetic, the computed factors, denoted $\\widehat{Q}$ and $\\widehat{R}$, will only approximate these properties.\n\nThe Gram-Schmidt process is a fundamental method for constructing such a factorization. It generates a sequence of orthonormal vectors $q_0, q_1, \\dots, q_{n-1}$ that form an orthonormal basis for the column space of $A$. The two variants to be implemented are:\n\n$1$. **Classical Gram-Schmidt (CGS)**: This algorithm computes each vector $q_j$ by explicitly subtracting its projections onto all previously computed orthonormal vectors $q_0, \\dots, q_{j-1}$ from the corresponding column $a_j$ of $A$. The vector $a_j$ is used for all projection computations for column $j$. The steps for column $j$ are:\n$$v_j = a_j - \\sum_{i=0}^{j-1} (q_i^\\top a_j) q_i$$\n$$r_{jj} = \\lVert v_j \\rVert_2, \\quad q_j = v_j / r_{jj}$$\nThe coefficients $r_{ij} = q_i^\\top a_j$ for $i < j$ are also computed.\nWhile mathematically sound, CGS is known to be numerically unstable. Small rounding errors in the computation of $q_i^\\top a_j$ can lead to a significant loss of orthogonality in the computed columns of $\\widehat{Q}$.\n\n$2$. **Modified Gram-Schmidt (MGS)**: This is an algebraically equivalent but numerically more stable rearrangement of CGS. Instead of projecting $a_j$ onto each $q_i$, MGS updates the remaining columns of $A$ at each step. After computing $q_i$, its component is removed from all subsequent columns $a_{i+1}, \\dots, a_{n-1}$. This can be expressed as:\nLet $v^{(0)}_j = a_j$ for all $j$. For $i = 0, \\dots, n-1$:\n$$r_{ii} = \\lVert v^{(i)}_i \\rVert_2, \\quad q_i = v^{(i)}_i / r_{ii}$$\nFor $j = i+1, \\dots, n-1$:\n$$r_{ij} = q_i^\\top v^{(i)}_j, \\quad v^{(i+1)}_j = v^{(i)}_j - r_{ij} q_i$$\nThis process ensures that each new vector is orthogonalized against already-orthogonalized vectors, reducing the accumulation of errors and resulting in a matrix $\\widehat{Q}$ with columns that are much closer to being perfectly orthogonal.\n\nIn cases of rank deficiency, a vector $v_j$ (in CGS) or $v_i^{(i)}$ (in MGS) may become zero or numerically close to zero. The implementation will handle this by checking if the norm is below a small tolerance ($10^{-12}$). If so, the corresponding column $q_j$ is set to a zero vector, and its diagonal entry $r_{jj}$ in $R$ is set to $0$.\n\nThe stability and accuracy of these algorithms are evaluated using two metrics:\n\n$1$. **Forward Error**: $\\eta_{\\mathrm{fwd}} = \\frac{\\lVert A - \\widehat{Q} \\widehat{R} \\rVert_2}{\\lVert A \\rVert_2}$. This measures the relative residual of the factorization. A smaller value indicates that the computed factors $\\widehat{Q}$ and $\\widehat{R}$ more accurately reconstruct the original matrix $A$. This can be interpreted as a measure of backward stability of the factorization problem itself; the computed factors are the exact factors for a nearby matrix $A+E$, and this error measures the size of $E$.\n\n$2$. **Loss of Orthogonality**: $\\eta_{\\mathrm{orth}} = \\lVert \\widehat{Q}^\\top \\widehat{Q} - I \\rVert_2$. This is a direct measure of the backward error of the algorithm with respect to the property of producing an orthonormal $Q$. A value close to $0$ indicates that the columns of $\\widehat{Q}$ are nearly orthonormal, while a value close to $1$ indicates a significant loss of orthogonality. This metric is expected to clearly distinguish the stability of MGS from the instability of CGS, especially for ill-conditioned matrices.\n\nThe program will implement both CGS and MGS, apply them to the four provided test matrices, and compute both error metrics for each factorization. The spectral norm $\\lVert \\cdot \\rVert_2$ is used for all matrix norm computations. The final results for each of the four test cases, consisting of the two errors for both algorithms, are rounded to $12$ decimal places and presented in the specified list-of-lists format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef classical_gram_schmidt(A):\n    \"\"\"\n    Computes the QR factorization of a matrix A using Classical Gram-Schmidt.\n    \"\"\"\n    m, n = A.shape\n    Q = np.zeros((m, n), dtype=float)\n    R = np.zeros((n, n), dtype=float)\n    \n    for j in range(n):\n        v = A[:, j].copy()\n        for i in range(j):\n            R[i, j] = Q[:, i].T @ A[:, j] # Project a_j onto q_i\n            v -= R[i, j] * Q[:, i]\n            \n        norm_v = np.linalg.norm(v)\n        \n        if norm_v > 1e-12:\n            R[j, j] = norm_v\n            Q[:, j] = v / R[j, j]\n        else:\n            R[j, j] = 0.0\n            # Q[:, j] remains a zero vector\n            \n    return Q, R\n\ndef modified_gram_schmidt(A):\n    \"\"\"\n    Computes the QR factorization of a matrix A using Modified Gram-Schmidt.\n    \"\"\"\n    m, n = A.shape\n    V = A.copy()\n    Q = np.zeros((m, n), dtype=float)\n    R = np.zeros((n, n), dtype=float)\n    \n    for i in range(n):\n        norm_v_i = np.linalg.norm(V[:, i])\n        R[i, i] = norm_v_i\n        \n        if R[i, i] > 1e-12:\n            Q[:, i] = V[:, i] / R[i, i]\n            for j in range(i + 1, n):\n                R[i, j] = Q[:, i].T @ V[:, j]\n                V[:, j] -= R[i, j] * Q[:, i]\n        else:\n            # R[i, i] is already set to the small norm (or 0)\n            # Q[:, i] remains a zero vector.\n            # Projections of V[:, j] onto a zero Q[:, i] are zero, so R[i, j] for j>i are zero\n            # and subsequent V columns are not modified.\n            pass\n            \n    return Q, R\n\ndef compute_errors(A, Q, R):\n    \"\"\"\n    Computes the forward error and orthogonality loss for a given QR factorization.\n    \"\"\"\n    m, n = A.shape\n    \n    # Forward error: ||A - QR||_2 / ||A||_2\n    norm_A = np.linalg.norm(A, 2)\n    if norm_A == 0:\n        fwd_error = 0.0\n    else:\n        fwd_error = np.linalg.norm(A - Q @ R, 2) / norm_A\n\n    # Orthogonality loss: ||Q^T Q - I||_2\n    if n > 0:\n        orth_error = np.linalg.norm(Q.T @ Q - np.eye(n), 2)\n    else:\n        orth_error = 0.0\n        \n    return fwd_error, orth_error\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis on the defined test cases.\n    \"\"\"\n    # Case 1: Well-conditioned tall matrix\n    A1 = np.array([\n        [1, 2, 3, 4],\n        [2, 1, 0, 1],\n        [0, 1, 2, 3],\n        [1, 0, 1, 0],\n        [2, 2, 2, 2],\n        [3, 1, 4, 1]\n    ], dtype=float)\n\n    # Case 2: Nearly dependent columns\n    epsilon = 1e-8\n    c1 = np.array([1, 2, 3, 4, 5], dtype=float).reshape(-1, 1)\n    w = np.array([1, -1, 1, -1, 1], dtype=float).reshape(-1, 1)\n    c2 = c1 + epsilon * w\n    c3 = np.array([2, 0, 1, 0, 2], dtype=float).reshape(-1, 1)\n    A2 = np.hstack([c1, c2, c3])\n\n    # Case 3: Hilbert-type tall matrix\n    m3, n3 = 8, 5\n    A3 = np.fromfunction(lambda i, j: 1 / (i + j + 1), (m3, n3), dtype=float)\n\n    # Case 4: Already orthonormal columns\n    A4 = np.array([\n        [1, 0, 0],\n        [0, 0, 0],\n        [0, 1, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 1]\n    ], dtype=float)\n\n    test_cases = [A1, A2, A3, A4]\n    all_results = []\n\n    for A in test_cases:\n        # Classical Gram-Schmidt\n        Q_cgs, R_cgs = classical_gram_schmidt(A)\n        fwd_cgs, orth_cgs = compute_errors(A, Q_cgs, R_cgs)\n\n        # Modified Gram-Schmidt\n        Q_mgs, R_mgs = modified_gram_schmidt(A)\n        fwd_mgs, orth_mgs = compute_errors(A, Q_mgs, R_mgs)\n\n        case_results = [\n            round(fwd_cgs, 12),\n            round(fwd_mgs, 12),\n            round(orth_cgs, 12),\n            round(orth_mgs, 12)\n        ]\n        all_results.append(case_results)\n\n    # Format the output as a string representation of a list of lists, with spaces\n    result_strings = [str(res) for res in all_results]\n    print(f\"[{', '.join(result_strings)}]\")\n\nsolve()\n```", "id": "3232097"}]}