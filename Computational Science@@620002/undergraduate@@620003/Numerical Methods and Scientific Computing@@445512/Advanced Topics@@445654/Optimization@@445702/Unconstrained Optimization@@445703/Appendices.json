{"hands_on_practices": [{"introduction": "The steepest descent algorithm is a cornerstone of unconstrained optimization, serving as the simplest example of a first-order, iterative method. Its performance, however, is critically dependent on the choice of step size, $\\alpha$. In this practice [@problem_id:3285028], you will explore this relationship by analyzing the convergence behavior of steepest descent on a quadratic objective, a fundamental model in optimization theory. By examining how the eigenvalues of the problem's Hessian matrix and the step size interact, you will gain a deep, analytical understanding of what separates rapid convergence from slow progress or even outright divergence.", "problem": "Consider unconstrained minimization of a twice continuously differentiable objective over $\\mathbb{R}^n$. A fundamental case in numerical methods is the strictly convex quadratic objective, \n$$\nf(x) = \\tfrac{1}{2} x^{\\top} Q x - b^{\\top} x,\n$$\nwhere $Q \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite and $b \\in \\mathbb{R}^{n}$. The unique minimizer $x^{\\star}$ satisfies $Q x^{\\star} = b$. The Steepest Descent (SD) method with a fixed step size $\\alpha > 0$ updates \n$$\nx_{k+1} = x_k - \\alpha \\nabla f(x_k),\n$$\nwith $\\nabla f(x) = Q x - b$.\n\nThe analytical behavior of SD with a fixed step size $\\alpha$ can exhibit fast convergence, slow convergence, or divergence. The classification in this problem must be grounded in the asymptotic linear rate of the error sequence. Define the error $e_k = x_k - x^{\\star}$ and the asymptotic linear rate \n$$\nr_{\\mathrm{asym}}(\\alpha) = \\limsup_{k \\to \\infty} \\frac{\\|e_{k+1}\\|_2}{\\|e_k\\|_2}.\n$$\nClassify the behavior for each test case using the following rule:\n- If $r_{\\mathrm{asym}}(\\alpha) \\ge 1$ (including the boundary case $r_{\\mathrm{asym}}(\\alpha) = 1$), return the integer $0$ to indicate divergence or non-convergence.\n- If $0.95 \\le r_{\\mathrm{asym}}(\\alpha) < 1$, return the integer $1$ to indicate slow convergence.\n- If $0 \\le r_{\\mathrm{asym}}(\\alpha) < 0.95$, return the integer $2$ to indicate acceptable convergence speed.\n\nYour program must:\n- Implement the SD iteration with fixed $\\alpha$ for the given quadratic instances, starting at the specified initial point $x_0$, and run for $N$ iterations.\n- Compute $x^{\\star}$ by solving $Q x^{\\star} = b$ and empirically observe the error norms $\\|e_k\\|_2$.\n- Justify the classification of each test case using a mathematically principled criterion consistent with the asymptotic linear rate definition (for example, by analyzing the induced linear iteration on the error and its spectral properties).\n- Produce a single line of output containing the classification results for all test cases as a comma-separated list enclosed in square brackets (e.g., $[0,1,2]$). Each list element must be an integer as defined above.\n\nThere are no physical units and no angles in this problem. All numerical thresholds (such as $0.95$) must be treated as decimals.\n\nTest suite (each test provides $(Q, b, x_0, \\alpha, N)$):\n1. $Q = \\operatorname{diag}(1, 100)$, $b = (1, 1)^{\\top}$, $x_0 = (10, -10)^{\\top}$, $\\alpha = 0.05$, $N = 50$.\n2. $Q = \\operatorname{diag}(1, 100)$, $b = (1, 1)^{\\top}$, $x_0 = (10, -10)^{\\top}$, $\\alpha = 0.02$, $N = 50$.\n3. $Q = \\operatorname{diag}(1, 100)$, $b = (1, 1)^{\\top}$, $x_0 = (10, -10)^{\\top}$, $\\alpha = 0.019$, $N = 200$.\n4. $Q = \\operatorname{diag}(2, 2)$, $b = (1, 1)^{\\top}$, $x_0 = (10, -10)^{\\top}$, $\\alpha = 0.2$, $N = 50$.\n5. $Q = \\operatorname{diag}(0.2, 0.1)$, $b = (1, 1)^{\\top}$, $x_0 = (10, -10)^{\\top}$, $\\alpha = 12$, $N = 50$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3,result4,result5]$).", "solution": "The problem requires the classification of the convergence behavior of the Steepest Descent (SD) method. The method is applied to an unconstrained minimization problem with a strictly convex quadratic objective function $f(x) = \\tfrac{1}{2} x^{\\top} Q x - b^{\\top} x$, where $x \\in \\mathbb{R}^n$, $Q \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite (SPD), and $b \\in \\mathbb{R}^{n}$. The classification must be based on the asymptotic linear rate of convergence, $r_{\\mathrm{asym}}(\\alpha)$, for a given fixed step size $\\alpha > 0$.\n\nThe SD iteration is defined by the update rule $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$. For the given quadratic objective, the gradient is $\\nabla f(x) = Qx - b$. Substituting this into the iteration formula yields:\n$$\nx_{k+1} = x_k - \\alpha(Qx_k - b) = (I - \\alpha Q)x_k + \\alpha b\n$$\nwhere $I$ is the identity matrix of appropriate dimension.\n\nThe unique minimizer of $f(x)$, denoted $x^{\\star}$, is the solution to the linear system $\\nabla f(x^\\star) = Qx^{\\star} - b = 0$, which implies $Qx^{\\star} = b$. The error at iteration $k$ is defined as the vector $e_k = x_k - x^{\\star}$. We can establish a recursive relationship for the error vector:\n$$\ne_{k+1} = x_{k+1} - x^{\\star} = \\left( (I - \\alpha Q)x_k + \\alpha b \\right) - x^{\\star}\n$$\nSubstituting $b = Qx^{\\star}$, we can write $\\alpha b = \\alpha Qx^{\\star}$. This allows us to express the error recurrence as:\n$$\ne_{k+1} = (I - \\alpha Q)x_k + \\alpha Qx^{\\star} - x^{\\star} = (I - \\alpha Q)x_k - (I - \\alpha Q)x^{\\star}\n$$\nBy factoring out the matrix $(I - \\alpha Q)$, we obtain the linear error dynamics:\n$$\ne_{k+1} = (I - \\alpha Q) e_k\n$$\nThis equation shows that the error at each step is obtained by multiplying the previous error by the fixed iteration matrix $G = I - \\alpha Q$. The long-term behavior of the magnitude of the error, $\\|e_k\\|_2$, is determined by the spectral radius of this iteration matrix, $\\rho(G)$, which is the maximum absolute value among its eigenvalues. The asymptotic linear rate of convergence is precisely this spectral radius:\n$$\nr_{\\mathrm{asym}}(\\alpha) = \\limsup_{k \\to \\infty} \\frac{\\|e_{k+1}\\|_2}{\\|e_k\\|_2} = \\rho(G) = \\rho(I - \\alpha Q)\n$$\nSince the matrix $Q$ is symmetric, it is diagonalizable and its eigenvalues, denoted $\\lambda_i$, are all real. As $Q$ is also positive definite, all its eigenvalues are positive, i.e., $\\lambda_i > 0$. The eigenvalues of the matrix $G = I - \\alpha Q$ are given by $\\mu_i = 1 - \\alpha \\lambda_i$. The spectral radius of $G$ is therefore:\n$$\n\\rho(G) = \\max_i |\\mu_i| = \\max_i |1 - \\alpha \\lambda_i|\n$$\nFor a fixed $\\alpha > 0$, the function $g(\\lambda) = |1 - \\alpha \\lambda|$ is a V-shaped function whose value depends on the distance of $\\lambda$ from $1/\\alpha$. The maximum of this function over the set of eigenvalues of $Q$ will be attained at either the smallest eigenvalue, $\\lambda_{\\min}$, or the largest eigenvalue, $\\lambda_{\\max}$. Thus, the asymptotic rate can be computed directly using only the extremal eigenvalues of $Q$:\n$$\nr_{\\mathrm{asym}}(\\alpha) = \\max \\left( |1 - \\alpha \\lambda_{\\min}|, |1 - \\alpha \\lambda_{\\max}| \\right)\n$$\nThis formula provides an exact, analytical value for the asymptotic rate, which is superior to empirical estimation from a finite number of iterations ($N$). The convergence classification relies solely on $Q$ and $\\alpha$; the initial point $x_0$ and number of iterations $N$ are irrelevant for determining the asymptotic rate. The condition for convergence is $r_{\\mathrm{asym}}(\\alpha) < 1$, which holds if and only if $0 < \\alpha < 2/\\lambda_{\\max}$.\n\nWe now apply this analytical result to each test case.\n\nCase 1: $Q = \\operatorname{diag}(1, 100)$, $\\alpha = 0.05$.\nThe eigenvalues of the diagonal matrix $Q$ are its diagonal entries. Thus, $\\lambda_{\\min} = 1$ and $\\lambda_{\\max} = 100$.\n$r_{\\mathrm{asym}}(0.05) = \\max(|1 - (0.05)(1)|, |1 - (0.05)(100)|) = \\max(|1 - 0.05|, |1 - 5|) = \\max(0.95, 4.0) = 4.0$.\nSince $r_{\\mathrm{asym}}(\\alpha) = 4.0 \\ge 1$, the method diverges. The classification is $0$.\n\nCase 2: $Q = \\operatorname{diag}(1, 100)$, $\\alpha = 0.02$.\nThe eigenvalues are $\\lambda_{\\min} = 1$ and $\\lambda_{\\max} = 100$. The step size $\\alpha=0.02$ is exactly at the stability boundary, $\\alpha = 2/\\lambda_{\\max} = 2/100 = 0.02$.\n$r_{\\mathrm{asym}}(0.02) = \\max(|1 - (0.02)(1)|, |1 - (0.02)(100)|) = \\max(|1 - 0.02|, |1 - 2|) = \\max(0.98, 1.0) = 1.0$.\nSince $r_{\\mathrm{asym}}(\\alpha) = 1.0 \\ge 1$, the method fails to converge. The classification is $0$.\n\nCase 3: $Q = \\operatorname{diag}(1, 100)$, $\\alpha = 0.019$.\nThe eigenvalues are $\\lambda_{\\min} = 1$ and $\\lambda_{\\max} = 100$.\n$r_{\\mathrm{asym}}(0.019) = \\max(|1 - (0.019)(1)|, |1 - (0.019)(100)|) = \\max(|1 - 0.019|, |1 - 1.9|) = \\max(0.981, 0.9) = 0.981$.\nSince $0.95 \\le 0.981 < 1$, the convergence is classified as slow. The classification is $1$.\n\nCase 4: $Q = \\operatorname{diag}(2, 2)$, $\\alpha = 0.2$.\nThe eigenvalues are identical: $\\lambda_{\\min} = \\lambda_{\\max} = 2$.\n$r_{\\mathrm{asym}}(0.2) = |1 - (0.2)(2)| = |1 - 0.4| = 0.6$.\nSince $0 \\le 0.6 < 0.95$, the convergence is classified as acceptable. The classification is $2$.\n\nCase 5: $Q = \\operatorname{diag}(0.2, 0.1)$, $\\alpha = 12$.\nThe eigenvalues are $\\lambda_{\\min} = 0.1$ and $\\lambda_{\\max} = 0.2$. The stability boundary is $2/\\lambda_{\\max} = 2/0.2 = 10$. The step size $\\alpha = 12$ is outside this range.\n$r_{\\mathrm{asym}}(12) = \\max(|1 - (12)(0.1)|, |1 - (12)(0.2)|) = \\max(|1 - 1.2|, |1 - 2.4|) = \\max(0.2, 1.4) = 1.4$.\nSince $r_{\\mathrm{asym}}(\\alpha) = 1.4 \\ge 1$, the method diverges. The classification is $0$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the convergence classification for several Steepest Descent scenarios\n    on a quadratic objective function. The classification is based on the analytical\n    asymptotic convergence rate derived from the spectral properties of the iteration matrix.\n    \"\"\"\n    \n    # Test suite format: (Q, b, x0, alpha, N)\n    test_cases = [\n        (np.diag([1.0, 100.0]), np.array([1.0, 1.0]), np.array([10.0, -10.0]), 0.05, 50),\n        (np.diag([1.0, 100.0]), np.array([1.0, 1.0]), np.array([10.0, -10.0]), 0.02, 50),\n        (np.diag([1.0, 100.0]), np.array([1.0, 1.0]), np.array([10.0, -10.0]), 0.019, 200),\n        (np.diag([2.0, 2.0]), np.array([1.0, 1.0]), np.array([10.0, -10.0]), 0.2, 50),\n        (np.diag([0.2, 0.1]), np.array([1.0, 1.0]), np.array([10.0, -10.0]), 12.0, 50),\n    ]\n\n    results = []\n    for Q, b, x0, alpha, N in test_cases:\n        # The convergence classification for a quadratic objective depends only on the \n        # matrix Q and the step size alpha. The parameters b, x0, and N do not affect\n        # the asymptotic rate.\n        \n        # The analytical asymptotic rate, r_asym, is the spectral radius of the \n        # iteration matrix G = I - alpha*Q. For a symmetric matrix Q, this is:\n        # r_asym = max(|1 - alpha*lambda_min|, |1 - alpha*lambda_max|)\n        # where lambda_min and lambda_max are the minimum and maximum eigenvalues of Q.\n        \n        # For the diagonal matrices in the test cases, the eigenvalues are simply the\n        # diagonal elements.\n        eigenvalues = np.diag(Q)\n        lambda_min = np.min(eigenvalues)\n        lambda_max = np.max(eigenvalues)\n        \n        # Calculate the asymptotic convergence rate\n        rate = max(abs(1.0 - alpha * lambda_min), abs(1.0 - alpha * lambda_max))\n        \n        # Classify the behavior based on the problem's rules for the rate.\n        classification = 0 # Default: divergence or non-convergence\n        if rate < 1.0:\n            if rate < 0.95:\n                classification = 2 # Acceptable convergence\n            else: # 0.95 <= rate < 1.0\n                classification = 1 # Slow convergence\n        \n        results.append(classification)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3285028"}, {"introduction": "While steepest descent uses only first-order information, Newton's method leverages second-order curvature information via the Hessian matrix to propose more direct and typically faster steps toward a solution. A crucial insight, however, is that Newton's method is designed to find stationary points where the gradient vanishes, without distinguishing between minima, maxima, or saddle points. This practice [@problem_id:3285127] allows you to investigate this behavior directly by implementing the method for an objective function that contains both local minima and a saddle point, demonstrating how the starting point can determine whether the algorithm converges to a desired minimizer or gets trapped at an undesirable saddle.", "problem": "Consider unconstrained optimization in two dimensions. A point $x^\\star \\in \\mathbb{R}^2$ is a stationary point of a twice continuously differentiable objective $f:\\mathbb{R}^2 \\to \\mathbb{R}$ if the gradient $\\nabla f(x^\\star)$ equals $0$. A stationary point is classified by the eigenvalues of the Hessian matrix $\\nabla^2 f(x^\\star)$: positive definiteness yields a local minimum, negative definiteness yields a local maximum, and mixed signs (indefiniteness) yield a saddle point. Newton's method for unconstrained optimization arises by forming the second-order Taylor model of $f$ at a current iterate $x_k$, then choosing the step $p_k$ that makes the gradient of that quadratic model vanish at $x_k+p_k$, and updating $x_{k+1}=x_k+p_k$. In practice, one may include a backtracking line search on the norm of the gradient to encourage stable progress when the Hessian is indefinite.\n\nDesign the following two-dimensional objective function:\n$$\nf(x,y) = x^2 - y^2 + (x^2 + y^2)^2,\n$$\nwhich is twice continuously differentiable on $\\mathbb{R}^2$. Starting from the fundamental definition of a stationary point and the second-order Taylor model for $f$, derive the Newton iteration for this problem and implement it as a program that:\n- Computes the gradient $\\nabla f(x,y)$ and the Hessian $\\nabla^2 f(x,y)$ symbolically and evaluates them numerically at any $(x,y)$.\n- Applies Newton's method by solving the linear system for the step $p_k$ that nulls the gradient of the quadratic model at $x_k+p_k$ and updates the iterate with a backtracking line search on the gradient norm to avoid unstable steps when the Hessian is indefinite.\n- Terminates when the gradient norm is below a small tolerance or when the step norm is extremely small, or when a specified maximum number of iterations is reached.\n\nUsing the eigenvalues of $\\nabla^2 f$ at the converged point, classify the stationary point into one of the following categories:\n- $0$ for a saddle point (eigenvalues have mixed signs).\n- $1$ for a local minimum (both eigenvalues strictly positive).\n- $2$ for a local maximum (both eigenvalues strictly negative).\n- $3$ for failure to converge to a stationary point within the iteration limit.\n\nYour program must run the Newton method on the following test suite of starting points (each is an ordered pair $(x_0,y_0)$):\n- $(0.05,\\,0.05)$\n- $(0.05,\\,0.9)$\n- $(0.05,\\,-0.9)$\n- $(0.8,\\,0.1)$\n- $(2.0,\\,2.0)$\n- $(0.0,\\,0.0)$\n\nFor each test case, return a list containing the classification integer, the number of iterations used, and the final point coordinates $(x^\\star,y^\\star)$ rounded to six decimal places. The final output format must be a single line containing a comma-separated list of these per-case lists, enclosed in square brackets, for example:\n$$\n[\\,[c_1,n_1,x^\\star_1,y^\\star_1],\\,[c_2,n_2,x^\\star_2,y^\\star_2],\\,\\ldots\\,]\n$$\nThere are no physical units or angles in this problem. Use a gradient norm tolerance of $10^{-8}$, a step norm tolerance of $10^{-12}$, and a maximum of $100$ iterations. If the Hessian is singular or ill-conditioned at any iteration, stabilize the linear solve with a small diagonal regularization added to the Hessian, and use backtracking to reduce the gradient norm monotonically.", "solution": "The objective $f(x,y) = x^2 - y^2 + (x^2 + y^2)^2$ is a polynomial, hence twice continuously differentiable. We begin with the foundational definitions for unconstrained optimization. A stationary point $x^\\star$ satisfies $\\nabla f(x^\\star)=0$. The second-order Taylor expansion of $f$ around $x_k$ is\n$$\nm_k(p) = f(x_k) + \\nabla f(x_k)^\\top p + \\frac{1}{2} p^\\top \\nabla^2 f(x_k) p,\n$$\nwhere $p \\in \\mathbb{R}^2$ is the step from $x_k$. Newton's method is obtained by choosing $p_k$ so that the gradient of the model vanishes at $p$, namely\n$$\n\\nabla m_k(p) = \\nabla f(x_k) + \\nabla^2 f(x_k) p = 0,\n$$\nwhich leads to the linear system\n$$\n\\nabla^2 f(x_k) \\, p_k = -\\nabla f(x_k).\n$$\nThe iterate is updated by $x_{k+1}=x_k+p_k$. When $\\nabla^2 f(x_k)$ is indefinite or ill-conditioned, a pure Newton step can be unstable. A widely used stabilization is to add a small diagonal regularization $\\lambda I$ and a backtracking line search; here we monitor the norm of the gradient to ensure monotone decrease:\n$$\n\\|\\nabla f(x_k + \\alpha p_k)\\| < \\|\\nabla f(x_k)\\|, \\quad \\text{with backtracking on } \\alpha \\in (0,1].\n$$\n\nWe now derive the gradient and Hessian of $f$. Define $r^2 = x^2 + y^2$. Then\n$$\nf(x,y) = x^2 - y^2 + r^4.\n$$\nThe gradient components are\n$$\n\\frac{\\partial f}{\\partial x} = 2x + 4x r^2 = 2x + 4x(x^2 + y^2), \\quad\n\\frac{\\partial f}{\\partial y} = -2y + 4y r^2 = -2y + 4y(x^2 + y^2).\n$$\nHence\n$$\n\\nabla f(x,y) = \\begin{bmatrix} 2x + 4x(x^2 + y^2) \\\\ -2y + 4y(x^2 + y^2) \\end{bmatrix}.\n$$\nFor the Hessian, differentiate the gradient:\n$$\n\\frac{\\partial^2 f}{\\partial x^2} = 2 + 12x^2 + 4y^2, \\quad\n\\frac{\\partial^2 f}{\\partial y^2} = -2 + 4x^2 + 12y^2, \\quad\n\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{\\partial^2 f}{\\partial y \\partial x} = 8xy.\n$$\nTherefore\n$$\n\\nabla^2 f(x,y) = \\begin{bmatrix}\n2 + 12x^2 + 4y^2 & 8xy \\\\\n8xy & -2 + 4x^2 + 12y^2\n\\end{bmatrix}.\n$$\n\nStationary points are obtained by solving $\\nabla f(x,y)=0$. The $x$-component equation is\n$$\n2x + 4x(x^2+y^2) = 2x\\left(1 + 2(x^2+y^2)\\right) = 0,\n$$\nwhich implies either $x=0$ or $1 + 2(x^2+y^2)=0$, but the latter has no real solution, so $x=0$. The $y$-component equation is\n$$\n-2y + 4y(x^2+y^2) = 2y\\left(-1 + 2(x^2+y^2)\\right) = 0,\n$$\nwhich implies either $y=0$ or $-1 + 2(x^2+y^2) = 0$. With $x=0$, the second alternative gives $2y^2 = 1$, hence $y = \\pm \\frac{1}{\\sqrt{2}}$. Thus the stationary points are $(0,0)$, $\\left(0, \\frac{1}{\\sqrt{2}}\\right)$, and $\\left(0, -\\frac{1}{\\sqrt{2}}\\right)$.\n\nTo classify the stationary points, examine the Hessian. At $(0,0)$,\n$$\n\\nabla^2 f(0,0) = \\begin{bmatrix} 2 & 0 \\\\ 0 & -2 \\end{bmatrix},\n$$\nwhich is indefinite (one positive and one negative eigenvalue). Therefore $(0,0)$ is a saddle point. At $\\left(0, \\pm \\frac{1}{\\sqrt{2}}\\right)$, using $x=0$ and $y^2 = \\frac{1}{2}$,\n$$\n\\nabla^2 f\\left(0, \\pm \\tfrac{1}{\\sqrt{2}}\\right) = \\begin{bmatrix} 2 + 4\\cdot \\tfrac{1}{2} & 0 \\\\ 0 & -2 + 12\\cdot \\tfrac{1}{2} \\end{bmatrix} = \\begin{bmatrix} 4 & 0 \\\\ 0 & 4 \\end{bmatrix},\n$$\nwhich is positive definite. Thus $\\left(0, \\pm \\frac{1}{\\sqrt{2}}\\right)$ are strict local minima.\n\nNewton's method behavior near $(0,0)$ can be analyzed via the local linearization. For small $(x,y)$, the gradient is approximately $\\nabla f(x,y) \\approx \\begin{bmatrix} 2x \\\\ -2y \\end{bmatrix}$ and the Hessian is approximately $\\nabla^2 f(x,y) \\approx \\begin{bmatrix} 2 & 0 \\\\ 0 & -2 \\end{bmatrix}$. The Newton step $p$ solves\n$$\n\\begin{bmatrix} 2 & 0 \\\\ 0 & -2 \\end{bmatrix} p = - \\begin{bmatrix} 2x \\\\ -2y \\end{bmatrix},\n$$\nwhich gives $p \\approx \\begin{bmatrix} -x \\\\ y \\end{bmatrix}$, hence $x_{k+1} \\approx \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$ in one iteration. This shows attraction to the saddle when both coordinates are sufficiently small. Conversely, when $y$ is moderate so that $-1 + 2(x^2 + y^2)$ becomes positive, the curvature in the $y$-direction becomes strongly positive, and the Newton step combined with a backtracking line search tends to move toward the minima at $y=\\pm \\frac{1}{\\sqrt{2}}$, hence repulsion from the saddle along the unstable direction depending on the starting point. This dichotomy illustrates how Newton's method for unconstrained optimization solves $\\nabla f(x)=0$ without regard to the stationary point's type: it can be attracted to a saddle if the initial guess lies in its basin of attraction, or repelled toward minima otherwise.\n\nAlgorithmic design for the implementation:\n- Compute $\\nabla f(x,y)$ and $\\nabla^2 f(x,y)$ exactly as derived.\n- At each iteration, attempt to solve $\\nabla^2 f(x_k)\\, p_k = -\\nabla f(x_k)$. If the Hessian is singular or nearly singular, stabilize with a small diagonal $\\lambda I$ before solving.\n- Use backtracking on the step length $\\alpha$ to decrease $\\|\\nabla f(x_k + \\alpha p_k)\\|$ monotonically.\n- Terminate when $\\|\\nabla f(x_k)\\| \\le 10^{-8}$ or when $\\| \\alpha p_k \\| \\le 10^{-12}$, or after $100$ iterations.\n- Classify the converged point by the signs of the Hessian eigenvalues.\n\nExpected outcomes for the specified test suite based on the analysis:\n- Starting at $(0.05,0.05)$: attraction to the saddle $(0,0)$, classification $0$.\n- Starting at $(0.05,0.9)$: repulsion from the saddle along $y$, convergence to $\\left(0,\\tfrac{1}{\\sqrt{2}}\\right)$, classification $1$.\n- Starting at $(0.05,-0.9)$: symmetric to the previous case, convergence to $\\left(0,-\\tfrac{1}{\\sqrt{2}}\\right)$, classification $1$.\n- Starting at $(0.8,0.1)$: attraction to the saddle via reduction of both coordinates, classification $0$.\n- Starting at $(2.0,2.0)$: repulsion from the saddle along $y$, convergence to a local minimum near $\\left(0,\\tfrac{1}{\\sqrt{2}}\\right)$, classification $1$.\n- Starting at $(0.0,0.0)$: immediate recognition of a stationary saddle, classification $0$.\n\nThe program will report, for each case, the classification integer, the iteration count, and the final $(x^\\star,y^\\star)$ with six decimal places, in the single-line format prescribed.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef f(xy: np.ndarray) -> float:\n    x, y = xy\n    r2 = x*x + y*y\n    return x*x - y*y + r2*r2\n\ndef grad(xy: np.ndarray) -> np.ndarray:\n    x, y = xy\n    r2 = x*x + y*y\n    return np.array([2.0*x + 4.0*x*r2, -2.0*y + 4.0*y*r2], dtype=float)\n\ndef hess(xy: np.ndarray) -> np.ndarray:\n    x, y = xy\n    h11 = 2.0 + 12.0*x*x + 4.0*y*y\n    h22 = -2.0 + 4.0*x*x + 12.0*y*y\n    h12 = 8.0*x*y\n    return np.array([[h11, h12], [h12, h22]], dtype=float)\n\ndef newton_optimize(x0: tuple,\n                    tol_grad: float = 1e-8,\n                    tol_step: float = 1e-12,\n                    max_iter: int = 100) -> tuple[np.ndarray, int, bool]:\n    x = np.array([float(x0[0]), float(x0[1])], dtype=float)\n    iters = 0\n    for k in range(max_iter):\n        g = grad(x)\n        normg = float(np.linalg.norm(g))\n        if normg <= tol_grad:\n            return x, k, True\n        H = hess(x)\n\n        # Stabilize Hessian if singular or ill-conditioned\n        # Try small diagonal regularization if needed.\n        lam = 0.0\n        p = None\n        for attempt in range(6):\n            try:\n                if lam > 0.0:\n                    Hp = H + lam * np.eye(2)\n                else:\n                    Hp = H\n                p = np.linalg.solve(Hp, -g)\n                break\n            except np.linalg.LinAlgError:\n                lam = 1e-8 if lam == 0.0 else lam * 10.0\n                continue\n        if p is None:\n            return x, k, False\n\n        # Backtracking line search to reduce gradient norm\n        alpha = 1.0\n        accepted = False\n        for _ in range(20):\n            x_new = x + alpha * p\n            if np.linalg.norm(grad(x_new)) < normg:\n                x = x_new\n                accepted = True\n                break\n            alpha *= 0.5\n        if not accepted:\n            # Even if not strictly decreasing, take a very small step to avoid stagnation\n            x = x + alpha * p\n\n        step_norm = float(np.linalg.norm(alpha * p))\n        iters = k + 1\n        if step_norm <= tol_step:\n            # If step is tiny, check convergence by gradient\n            if float(np.linalg.norm(grad(x))) <= tol_grad:\n                return x, iters, True\n            else:\n                return x, iters, False\n\n    # Max iterations reached\n    return x, max_iter, float(np.linalg.norm(grad(x))) <= tol_grad\n\ndef classify_stationary_point(xy: np.ndarray) -> int:\n    # 0: saddle (mixed signs), 1: minimum (all positive), 2: maximum (all negative), 3: failure (handled outside)\n    H = hess(xy)\n    eigs = np.linalg.eigvals(H)\n    tol = 1e-7\n    if np.all(eigs > tol):\n        return 1\n    elif np.all(eigs < -tol):\n        return 2\n    else:\n        return 0\n\ndef format_result(code: int, iters: int, xy: np.ndarray) -> str:\n    # Produce no-space list representation: [code,iters,x,y] with six decimal places\n    return f\"[{code},{iters},{xy[0]:.6f},{xy[1]:.6f}]\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.05, 0.05),\n        (0.05, 0.9),\n        (0.05, -0.9),\n        (0.8, 0.1),\n        (2.0, 2.0),\n        (0.0, 0.0),\n    ]\n\n    results_strs = []\n    for case in test_cases:\n        xy_star, iters, success = newton_optimize(case, tol_grad=1e-8, tol_step=1e-12, max_iter=100)\n        if success:\n            code = classify_stationary_point(xy_star)\n        else:\n            code = 3\n        results_strs.append(format_result(code, iters, xy_star))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_strs)}]\")\n\nsolve()\n```", "id": "3285127"}, {"introduction": "The pure Newton's method, while powerful, has a significant practical weakness: it can falter or fail when the Hessian matrix is not positive definite, which is common when optimizing general non-convex functions. This final practice [@problem_id:3255780] guides you in transforming the basic algorithm into a robust solver capable of handling such cases. You will implement a key strategy used in modern optimization software: checking for positive definiteness using a Cholesky factorization and systematically modifying the Hessian when the check fails, thereby ensuring the algorithm always computes a productive descent direction.", "problem": "You are to implement a Newton-type method for unconstrained optimization that checks the positive definiteness of the Hessian matrix via Cholesky factorization and modifies the search direction when this factorization fails. The algorithmic design must be derived from the second-order Taylor model of a smooth objective function and must use a line search that enforces a sufficient decrease condition. The implementation must be fully deterministic, use only fixed parameter values provided below, and produce a single-line output in a precisely specified format.\n\nStarting point for derivation. Let $f:\\mathbb{R}^n\\to\\mathbb{R}$ be twice continuously differentiable. Around a point $\\mathbf{x}_k\\in\\mathbb{R}^n$, the second-order Taylor model of $f$ is\n$$\nm_k(\\mathbf{s}) \\equiv f(\\mathbf{x}_k) + \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{s} + \\tfrac{1}{2}\\,\\mathbf{s}^\\top \\nabla^2 f(\\mathbf{x}_k)\\,\\mathbf{s},\n$$\nwhere $\\nabla f(\\mathbf{x}_k)$ is the gradient and $\\nabla^2 f(\\mathbf{x}_k)$ is the Hessian. The Newton search direction is obtained by minimizing $m_k(\\mathbf{s})$, which requires solving the stationarity condition for the model. When the Hessian at $\\mathbf{x}_k$ is symmetric positive definite (SPD), this produces a unique search direction that is a local minimizer of the quadratic model.\n\nPositive definiteness check and modification. To decide whether the Hessian is SPD, attempt a Cholesky factorization. If $\\nabla^2 f(\\mathbf{x}_k)$ admits a Cholesky factorization, it is SPD. If the factorization fails, modify the quadratic model by adding a diagonal shift $\\tau_k \\mathbf{I}$ with $\\tau_k>0$, increasing $\\tau_k$ geometrically until the Cholesky factorization of $\\nabla^2 f(\\mathbf{x}_k)+\\tau_k \\mathbf{I}$ succeeds. Use the resulting shifted system to define a descent direction. If, due to numerical issues, the computed direction does not satisfy a strict descent condition, fall back to the steepest descent direction.\n\nGlobalization by backtracking line search. Use backtracking line search to enforce the Armijo condition. Start from the full step and reduce the step length geometrically until the sufficient decrease condition holds.\n\nTermination. Stop when the Euclidean norm of the gradient is below a given tolerance or a step size measure falls below a given tolerance, or when a maximum number of iterations is reached.\n\nImplementation requirements.\n- Use the following fixed parameters for all test cases: initial step length $\\alpha_0=1$, Armijo parameter $c=10^{-4}$, backtracking reduction factor $\\rho=0.5$, initial diagonal shift $\\tau_0=10^{-6}$ with geometric growth by a factor of $10$ when needed, gradient norm tolerance $\\varepsilon_g=10^{-8}$, step tolerance $\\varepsilon_s=10^{-12}$, and maximum iterations $N_{\\max}=200$.\n- The Hessian positive definiteness check must use Cholesky factorization. If it fails on $\\nabla^2 f(\\mathbf{x}_k)$, use $\\nabla^2 f(\\mathbf{x}_k)+\\tau_k \\mathbf{I}$ with $\\tau_k$ grown geometrically from $\\tau_0$ until Cholesky succeeds.\n- Use an Armijo backtracking line search that accepts $\\alpha$ if\n$$\nf(\\mathbf{x}_k+\\alpha \\mathbf{d}_k) \\le f(\\mathbf{x}_k) + c\\,\\alpha\\,\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{d}_k.\n$$\n- Terminate if $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le \\varepsilon_g$ or $\\alpha\\|\\mathbf{d}_k\\|_2 \\le \\varepsilon_s$, or after $N_{\\max}$ iterations.\n\nTest suite. Implement your solver and run it on the following four test cases. Each function is defined on $\\mathbb{R}^2$ with the specified starting point.\n- Case A (strictly convex quadratic; happy path): $f_1(\\mathbf{x})=\\tfrac{1}{2}\\mathbf{x}^\\top Q \\mathbf{x} + \\mathbf{b}^\\top \\mathbf{x}$ with $Q=\\begin{bmatrix}4 & 1\\\\ 1 & 3\\end{bmatrix}$ and $\\mathbf{b}=\\begin{bmatrix}-1\\\\ 2\\end{bmatrix}$, starting at $\\mathbf{x}^{(0)}=\\begin{bmatrix}2\\\\ -1\\end{bmatrix}$. Use the exact gradient $\\nabla f_1(\\mathbf{x})=Q\\mathbf{x}+\\mathbf{b}$ and Hessian $\\nabla^2 f_1(\\mathbf{x})=Q$.\n- Case B (nonconvex, indefinite Hessian at start): Himmelblau’s function $f_2(x,y)=(x^2+y-11)^2+(x+y^2-7)^2$, starting at $\\mathbf{x}^{(0)}=\\begin{bmatrix}0\\\\ 0\\end{bmatrix}$. Use $\\nabla f_2(x,y)=\\begin{bmatrix}4x(x^2+y-11)+2(x+y^2-7)\\\\ 2(x^2+y-11)+4y(x+y^2-7)\\end{bmatrix}$ and\n$$\n\\nabla^2 f_2(x,y)=\\begin{bmatrix}\n8x^2+4(x^2+y-11)+2 & 4x+4y\\\\\n4x+4y & 8y^2+4(x+y^2-7)+2\n\\end{bmatrix}.\n$$\n- Case C (saddle near origin, Hessian indefinite): $f_3(x,y)=x^2-y^2+0.1\\,x^4+0.1\\,y^4$, starting at $\\mathbf{x}^{(0)}=\\begin{bmatrix}0.1\\\\ 0.1\\end{bmatrix}$. Use $\\nabla f_3(x,y)=\\begin{bmatrix}2x+0.4\\,x^3\\\\ -2y+0.4\\,y^3\\end{bmatrix}$ and $\\nabla^2 f_3(x,y)=\\begin{bmatrix}2+1.2\\,x^2 & 0\\\\ 0 & -2+1.2\\,y^2\\end{bmatrix}$.\n- Case D (flat, singular Hessian at minimizer): $f_4(x,y)=(x^2+y^2)^2$, starting at $\\mathbf{x}^{(0)}=\\begin{bmatrix}10^{-3}\\\\ -10^{-3}\\end{bmatrix}$. Use $\\nabla f_4(x,y)=\\begin{bmatrix}4x(x^2+y^2)\\\\ 4y(x^2+y^2)\\end{bmatrix}$ and $\\nabla^2 f_4(x,y)=\\begin{bmatrix}12x^2+4y^2 & 8xy\\\\ 8xy & 12y^2+4x^2\\end{bmatrix}$.\n\nProgram output. For each case, return a list of five values: the two coordinates of the final iterate $\\mathbf{x}_\\star$ in order, the final objective value $f(\\mathbf{x}_\\star)$, the number of iterations taken, and an integer flag equal to $1$ if a diagonal shift $\\tau_k>0$ was used at least once (that is, the original Hessian failed a Cholesky factorization in any iteration), and $0$ otherwise. Aggregate the four case results into a single list in the order A, B, C, D. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces, for example\n$[ [\\dots],[\\dots],[\\dots],[\\dots] ]$\nbut with no spaces anywhere, e.g.,\n$[[a_1,a_2,a_3,a_4,a_5],[b_1,b_2,b_3,b_4,b_5],[c_1,c_2,c_3,c_4,c_5],[d_1,d_2,d_3,d_4,d_5]]$,\nwhere each $a_i,b_i,c_i,d_i$ is a real number or an integer as specified above. No other output should be printed.", "solution": "The problem requires the implementation of a Newton-type method for unconstrained optimization. The algorithm must incorporate a mechanism to handle non-positive definite Hessian matrices and a line search strategy to ensure global convergence. The exposition below details the derivation of the algorithm from first principles, culminating in a complete procedural description.\n\nAn unconstrained optimization problem seeks to find a minimizer $\\mathbf{x}_\\star$ for a given objective function $f: \\mathbb{R}^n \\to \\mathbb{R}$. Iterative methods generate a sequence of points $\\{\\mathbf{x}_k\\}$ that are intended to converge to $\\mathbf{x}_\\star$. The update from $\\mathbf{x}_k$ to $\\mathbf{x}_{k+1}$ is given by\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{d}_k,\n$$\nwhere $\\mathbf{d}_k$ is a search direction and $\\alpha_k > 0$ is a step length.\n\nThe core of Newton's method lies in approximating the objective function $f$ at the current iterate $\\mathbf{x}_k$ with a quadratic model. Assuming $f$ is twice continuously differentiable, its second-order Taylor expansion around $\\mathbf{x}_k$ is\n$$\nf(\\mathbf{x}_k + \\mathbf{s}) \\approx m_k(\\mathbf{s}) \\equiv f(\\mathbf{x}_k) + \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{s} + \\frac{1}{2}\\,\\mathbf{s}^\\top \\nabla^2 f(\\mathbf{x}_k)\\,\\mathbf{s},\n$$\nwhere $\\mathbf{s} = \\mathbf{x} - \\mathbf{x}_k$ is the step. We denote the gradient as $\\mathbf{g}_k = \\nabla f(\\mathbf{x}_k)$ and the Hessian as $H_k = \\nabla^2 f(\\mathbf{x}_k)$. The model is then\n$$\nm_k(\\mathbf{s}) = f_k + \\mathbf{g}_k^\\top \\mathbf{s} + \\frac{1}{2}\\,\\mathbf{s}^\\top H_k\\,\\mathbf{s}.\n$$\nThe Newton search direction $\\mathbf{d}_k$ is the step $\\mathbf{s}$ that minimizes this quadratic model. The necessary condition for a minimum is that the gradient of the model with respect to $\\mathbf{s}$ is zero:\n$$\n\\nabla m_k(\\mathbf{s}) = \\mathbf{g}_k + H_k \\mathbf{s} = \\mathbf{0}.\n$$\nThis yields the Newton system of linear equations:\n$$\nH_k \\mathbf{s} = -\\mathbf{g}_k.\n$$\nIf the Hessian $H_k$ is symmetric positive definite (SPD), it is invertible, and the model $m_k(\\mathbf{s})$ is strictly convex with a unique minimizer. The solution to this system, $\\mathbf{d}_k = -H_k^{-1} \\mathbf{g}_k$, is the pure Newton direction. Furthermore, if $H_k$ is SPD, $\\mathbf{d}_k$ is guaranteed to be a descent direction, meaning it makes an angle greater than $90$ degrees with the gradient:\n$$\n\\mathbf{g}_k^\\top \\mathbf{d}_k = \\mathbf{g}_k^\\top (-H_k^{-1} \\mathbf{g}_k) = -\\mathbf{g}_k^\\top H_k^{-1} \\mathbf{g}_k < 0,\n$$\nsince the inverse of an SPD matrix is also SPD.\n\nA significant challenge arises when the Hessian $H_k$ is not positive definite. In this case, the quadratic model $m_k(\\mathbf{s})$ may not have a unique minimizer (it could be unbounded below), and the computed Newton direction might not be a descent direction. The specified algorithm addresses this by modifying the Hessian. The positive definiteness of $H_k$ is tested by attempting a Cholesky factorization, $H_k = LL^\\top$, where $L$ is a lower triangular matrix. This factorization succeeds if and only if $H_k$ is SPD.\n\nIf the Cholesky factorization of $H_k$ fails, the Hessian is modified by adding a scaled identity matrix, a technique related to the Levenberg-Marquardt method. We solve a modified system:\n$$\n(H_k + \\tau_k \\mathbf{I}) \\mathbf{d}_k = -\\mathbf{g}_k,\n$$\nwhere $\\mathbf{I}$ is the identity matrix and $\\tau_k > 0$ is a regularization parameter. Adding $\\tau_k \\mathbf{I}$ increases the eigenvalues of the Hessian by $\\tau_k$. For a sufficiently large $\\tau_k$, the matrix $H_k + \\tau_k \\mathbf{I}$ is guaranteed to be positive definite. The algorithm finds a suitable $\\tau_k$ adaptively. It starts with a small value, $\\tau_0 = 10^{-6}$, and attempts the Cholesky factorization of $H_k + \\tau_k \\mathbf{I}$. If it fails, $\\tau_k$ is increased geometrically by a factor of $10$ until the factorization succeeds. Let $\\hat{H}_k = H_k + \\tau_k \\mathbf{I}$ be the final, SPD modified Hessian. The search direction is then computed by solving $\\hat{H}_k \\mathbf{d}_k = -\\mathbf{g}_k$. As a safeguard against numerical instability, the algorithm must verify that the computed $\\mathbf{d}_k$ is indeed a descent direction by checking if $\\mathbf{g}_k^\\top \\mathbf{d}_k < 0$. If this condition fails, the algorithm reverts to the most basic descent direction, the steepest descent direction, $\\mathbf{d}_k = -\\mathbf{g}_k$.\n\nOnce a suitable descent direction $\\mathbf{d}_k$ is obtained, a step length $\\alpha_k > 0$ must be chosen to ensure a sufficient decrease in the objective function and guarantee convergence. This is achieved through a backtracking line search that enforces the Armijo condition:\n$$\nf(\\mathbf{x}_k + \\alpha \\mathbf{d}_k) \\le f(\\mathbf{x}_k) + c \\alpha \\mathbf{g}_k^\\top \\mathbf{d}_k,\n$$\nfor a constant $c \\in (0,1)$, specified as $c=10^{-4}$. The line search starts with a full step $\\alpha = \\alpha_0 = 1$. If the Armijo condition is not satisfied, the step length is reduced by a factor $\\rho = 0.5$, i.e., $\\alpha \\leftarrow \\rho \\alpha$, and the check is repeated until the condition holds.\n\nThe overall iterative process is terminated when one of the following criteria is met:\n1.  The gradient norm is sufficiently small: $\\|\\mathbf{g}_k\\|_2 \\le \\varepsilon_g$, where $\\varepsilon_g = 10^{-8}$. This indicates that $\\mathbf{x}_k$ is near a stationary point.\n2.  The step size becomes negligible: $\\alpha_k \\|\\mathbf{d}_k\\|_2 \\le \\varepsilon_s$, where $\\varepsilon_s=10^{-12}$. This suggests that further progress is minimal.\n3.  A maximum number of iterations is reached: $k \\ge N_{\\max}$, with $N_{\\max} = 200$. This prevents infinite loops.\n\nIn summary, the algorithm proceeds as follows for each iteration $k$:\n1.  Compute the gradient $\\mathbf{g}_k = \\nabla f(\\mathbf{x}_k)$ and the Hessian $H_k = \\nabla^2 f(\\mathbf{x}_k)$.\n2.  Check for termination based on $\\|\\mathbf{g}_k\\|_2$.\n3.  Find a suitable search direction $\\mathbf{d}_k$:\n    a. Initialize $\\tau = 0$.\n    b. Loop: Attempt Cholesky factorization of $H_k + \\tau \\mathbf{I}$.\n    c. If successful, solve $(H_k + \\tau \\mathbf{I}) \\mathbf{d}_k = -\\mathbf{g}_k$ and break the loop.\n    d. If unsuccessful, record that a modification was needed. Set $\\tau = \\tau_0$ if $\\tau=0$, otherwise set $\\tau = 10\\tau$. Repeat the loop.\n    e. As a safeguard, if $\\mathbf{g}_k^\\top \\mathbf{d}_k \\ge 0$, set $\\mathbf{d}_k = -\\mathbf{g}_k$.\n4.  Perform a backtracking line search to find a step length $\\alpha_k$ satisfying the Armijo condition.\n5.  Check for termination based on the step size $\\alpha_k \\|\\mathbf{d}_k\\|_2$.\n6.  Update the iterate: $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{d}_k$.\n7.  Check for termination based on the maximum iteration count.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test cases through the modified Newton solver.\n    \"\"\"\n\n    # --- Algorithmic Parameters ---\n    PARAMS = {\n        'alpha0': 1.0,\n        'c': 1e-4,\n        'rho': 0.5,\n        'tau0': 1e-6,\n        'tau_factor': 10.0,\n        'eps_g': 1e-8,\n        'eps_s': 1e-12,\n        'N_max': 200,\n    }\n\n    def newton_modified(f, grad, hess, x0, params):\n        \"\"\"\n        Implements a modified Newton method for unconstrained optimization.\n        \"\"\"\n        x = np.array(x0, dtype=float)\n        hessian_modified_flag = 0\n\n        for k in range(params['N_max']):\n            fk = f(x)\n            gk = grad(x)\n\n            # Termination condition on gradient norm\n            if np.linalg.norm(gk) <= params['eps_g']:\n                return x.tolist(), fk, k, hessian_modified_flag\n\n            Hk = hess(x)\n            tau = 0.0\n            \n            while True:\n                try:\n                    # Attempt Cholesky factorization\n                    H_mod = Hk\n                    if tau > 0:\n                        H_mod = Hk + tau * np.identity(len(x))\n                    \n                    L = np.linalg.cholesky(H_mod)\n                    \n                    # Solve (Hk + tau*I)d = -gk\n                    # 1. Solve L y = -gk\n                    y = np.linalg.solve(L, -gk)\n                    # 2. Solve L^T d = y\n                    dk = np.linalg.solve(L.T, y)\n                    break \n                except np.linalg.LinAlgError:\n                    # Cholesky factorization failed, matrix is not SPD\n                    hessian_modified_flag = 1\n                    if tau == 0.0:\n                        tau = params['tau0']\n                    else:\n                        tau *= params['tau_factor']\n\n            # Safeguard: ensure descent direction\n            if gk @ dk >= 0:\n                dk = -gk\n\n            # Backtracking line search with Armijo condition\n            alpha = params['alpha0']\n            while True:\n                x_new = x + alpha * dk\n                if f(x_new) <= fk + params['c'] * alpha * (gk @ dk):\n                    break\n                alpha *= params['rho']\n                # Failsafe for alpha becoming too small, though step condition should handle it\n                if alpha < 1e-16:\n                   break\n\n            # Termination condition on step size\n            if alpha * np.linalg.norm(dk) <= params['eps_s']:\n                return x_new.tolist(), f(x_new), k + 1, hessian_modified_flag\n            \n            x = x_new\n        \n        # Reached max iterations\n        return x.tolist(), f(x), params['N_max'], hessian_modified_flag\n\n    # --- Test Cases ---\n    \n    # Case A: Strictly convex quadratic\n    Q_A = np.array([[4.0, 1.0], [1.0, 3.0]])\n    b_A = np.array([-1.0, 2.0])\n    f_A = lambda x: 0.5 * x.T @ Q_A @ x + b_A.T @ x\n    grad_A = lambda x: Q_A @ x + b_A\n    hess_A = lambda x: Q_A\n    x0_A = [2.0, -1.0]\n\n    # Case B: Himmelblau's function\n    f_B = lambda x: (x[0]**2 + x[1] - 11)**2 + (x[0] + x[1]**2 - 7)**2\n    grad_B = lambda x: np.array([\n        4*x[0]*(x[0]**2 + x[1] - 11) + 2*(x[0] + x[1]**2 - 7),\n        2*(x[0]**2 + x[1] - 11) + 4*x[1]*(x[0] + x[1]**2 - 7)\n    ])\n    hess_B = lambda x: np.array([\n        [12*x[0]**2 + 4*x[1] - 42, 4*x[0] + 4*x[1]],\n        [4*x[0] + 4*x[1], 12*x[1]**2 + 4*x[0] - 26]\n    ])\n    x0_B = [0.0, 0.0]\n\n    # Case C: Saddle point function\n    f_C = lambda x: x[0]**2 - x[1]**2 + 0.1*x[0]**4 + 0.1*x[1]**4\n    grad_C = lambda x: np.array([2*x[0] + 0.4*x[0]**3, -2*x[1] + 0.4*x[1]**3])\n    hess_C = lambda x: np.array([\n        [2 + 1.2*x[0]**2, 0.0],\n        [0.0, -2 + 1.2*x[1]**2]\n    ])\n    x0_C = [0.1, 0.1]\n\n    # Case D: Flat function with singular Hessian at minimizer\n    f_D = lambda x: (x[0]**2 + x[1]**2)**2\n    grad_D = lambda x: np.array([\n        4*x[0]*(x[0]**2 + x[1]**2),\n        4*x[1]*(x[0]**2 + x[1]**2)\n    ])\n    hess_D = lambda x: np.array([\n        [12*x[0]**2 + 4*x[1]**2, 8*x[0]*x[1]],\n        [8*x[0]*x[1], 12*x[1]**2 + 4*x[0]**2]\n    ])\n    x0_D = [1e-3, -1e-3]\n    \n    test_cases = [\n        (f_A, grad_A, hess_A, x0_A),\n        (f_B, grad_B, hess_B, x0_B),\n        (f_C, grad_C, hess_C, x0_C),\n        (f_D, grad_D, hess_D, x0_D),\n    ]\n\n    all_results = []\n    for f, grad, hess, x0 in test_cases:\n        x_star_coords, f_star, iters, mod_flag = newton_modified(f, grad, hess, x0, PARAMS)\n        result_list = x_star_coords + [f_star, iters, mod_flag]\n        all_results.append(result_list)\n        \n    # Format the output string without any spaces.\n    def format_list(lst):\n        s = []\n        for item in lst:\n            if isinstance(item, list):\n                s.append(format_list(item))\n            elif isinstance(item, float):\n                # Use a format that avoids scientific notation for small numbers if possible\n                # and prints enough precision, but string conversion handles this well.\n                s.append(str(item))\n            else:\n                s.append(str(item))\n        return '[' + ','.join(s) + ']'\n        \n    output_str = format_list(all_results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```", "id": "3255780"}]}