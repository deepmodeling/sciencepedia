{"hands_on_practices": [{"introduction": "The journey of minimizing a function begins with a single step in the right direction. The steepest descent method defines this \"right direction\" as the one where the function's value decreases most rapidly, which corresponds to the negative of the function's gradient. This practice grounds us in this fundamental principle by asking us to compute the initial search direction for the famous Rosenbrock function, a classic and challenging benchmark in numerical optimization [@problem_id:2221567].", "problem": "In the field of numerical optimization, the performance of new algorithms is often benchmarked using a set of standard test functions. One such function is the Rosenbrock function, which is challenging to minimize due to its narrow, parabolic valley.\n\nConsider a two-dimensional version of the Rosenbrock function given by\n$$f(x,y) = (a-x)^2 + b(y-x^2)^2$$\nwhere $a$ and $b$ are positive real constants.\n\nAn iterative minimization algorithm is initiated at the point $(x_0, y_0) = (0, 0)$. The first step of this algorithm involves determining the initial search direction. This direction is defined as the vector along which the function's value decreases most rapidly from the starting point.\n\nDetermine this initial search direction vector, $\\mathbf{d}_0$. Express your answer as a column vector in terms of the constants $a$ and $b$.", "solution": "The direction along which $f$ decreases most rapidly at a point is the negative gradient, so the initial search direction from $(x_{0},y_{0})=(0,0)$ is $\\mathbf{d}_{0}=-\\nabla f(0,0)$.\n\nCompute the gradient of $f(x,y)=(a-x)^{2}+b(y-x^{2})^{2}$:\n- The partial derivative with respect to $x$ is\n$$\n\\frac{\\partial f}{\\partial x}=2(x-a)-4bx(y-x^{2}).\n$$\n- The partial derivative with respect to $y$ is\n$$\n\\frac{\\partial f}{\\partial y}=2b(y-x^{2}).\n$$\n\nEvaluate these at $(0,0)$:\n$$\n\\frac{\\partial f}{\\partial x}(0,0)=-2a,\\qquad \\frac{\\partial f}{\\partial y}(0,0)=0.\n$$\nHence,\n$$\n\\nabla f(0,0)=\\begin{pmatrix}-2a\\\\0\\end{pmatrix},\n\\quad\n\\mathbf{d}_{0}=-\\nabla f(0,0)=\\begin{pmatrix}2a\\\\0\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}2a\\\\0\\end{pmatrix}}$$", "id": "2221567"}, {"introduction": "Once a descent direction is chosen, the next question is how far to travel along it. This exercise demonstrates the \"exact line search\" strategy, where we find the optimal step size that minimizes the function along the chosen direction. By manually performing two iterations on a simple quadratic function, you will observe the method's characteristic \"zigzagging\" behavior, which provides crucial insight into its performance on ill-conditioned problems [@problem_id:2221576].", "problem": "Consider the unconstrained optimization problem of minimizing the function $f(x, y) = 10x^2 + y^2$. The optimization process is initiated at the point $\\mathbf{x}_0 = (x_0, y_0) = (1, 1)$.\n\nThe steepest descent algorithm is to be used. For each iteration $k$, the step size, denoted by $\\alpha_k > 0$, is determined via an exact line search. This means that for a given point $\\mathbf{x}_k$ and descent direction $\\mathbf{p}_k$, the step size $\\alpha_k$ is chosen to find the global minimum of the single-variable function $g(\\alpha) = f(\\mathbf{x}_k + \\alpha \\mathbf{p}_k)$.\n\nCalculate the coordinates of the point $\\mathbf{x}_2 = (x_2, y_2)$ after two complete iterations of this method. The coordinates in your final answer should be expressed as exact fractions in their simplest form.", "solution": "We minimize $f(x,y)=10x^{2}+y^{2}$ starting at $\\mathbf{x}_{0}=(1,1)$ using steepest descent with exact line search. The gradient and Hessian are\n$$\n\\nabla f(x,y)=\\begin{pmatrix}20x\\\\2y\\end{pmatrix},\\qquad H=\\begin{pmatrix}20&0\\\\0&2\\end{pmatrix}.\n$$\nAt iteration $k$, with $\\mathbf{g}_{k}=\\nabla f(\\mathbf{x}_{k})$ and direction $\\mathbf{p}_{k}=-\\mathbf{g}_{k}$, the exact line search minimizes $g(\\alpha)=f(\\mathbf{x}_{k}+\\alpha\\mathbf{p}_{k})$. Since $H$ is constant (quadratic function), the derivative is\n$$\ng'(\\alpha)=\\nabla f(\\mathbf{x}_{k}-\\alpha \\mathbf{g}_{k})^{\\mathsf{T}}(-\\mathbf{g}_{k})=\\left(\\mathbf{g}_{k}-\\alpha H\\mathbf{g}_{k}\\right)^{\\mathsf{T}}(-\\mathbf{g}_{k})=-(\\mathbf{g}_{k}^{\\mathsf{T}}\\mathbf{g}_{k})+\\alpha\\,\\mathbf{g}_{k}^{\\mathsf{T}}H\\mathbf{g}_{k}.\n$$\nSetting $g'(\\alpha)=0$ yields the exact step size\n$$\n\\alpha_{k}=\\frac{\\mathbf{g}_{k}^{\\mathsf{T}}\\mathbf{g}_{k}}{\\mathbf{g}_{k}^{\\mathsf{T}}H\\mathbf{g}_{k}}.\n$$\n\nIteration $0$: $\\mathbf{x}_{0}=(1,1)$ gives\n$$\n\\mathbf{g}_{0}=\\begin{pmatrix}20\\\\2\\end{pmatrix},\\quad \\mathbf{g}_{0}^{\\mathsf{T}}\\mathbf{g}_{0}=404,\\quad H\\mathbf{g}_{0}=\\begin{pmatrix}400\\\\4\\end{pmatrix},\\quad \\mathbf{g}_{0}^{\\mathsf{T}}H\\mathbf{g}_{0}=8008,\n$$\nso\n$$\n\\alpha_{0}=\\frac{404}{8008}=\\frac{101}{2002}.\n$$\nThen\n$$\n\\mathbf{x}_{1}=\\mathbf{x}_{0}-\\alpha_{0}\\mathbf{g}_{0}=\\begin{pmatrix}1-20\\alpha_{0}\\\\1-2\\alpha_{0}\\end{pmatrix}=\\begin{pmatrix}1-\\frac{1010}{1001}\\\\1-\\frac{101}{1001}\\end{pmatrix}=\\begin{pmatrix}-\\frac{9}{1001}\\\\\\frac{900}{1001}\\end{pmatrix}.\n$$\n\nIteration $1$: $\\mathbf{x}_{1}=\\left(-\\frac{9}{1001},\\,\\frac{900}{1001}\\right)$ gives\n$$\n\\mathbf{g}_{1}=\\begin{pmatrix}20x_{1}\\\\2y_{1}\\end{pmatrix}=\\begin{pmatrix}-\\frac{180}{1001}\\\\\\frac{1800}{1001}\\end{pmatrix}.\n$$\nCompute\n$$\n\\mathbf{g}_{1}^{\\mathsf{T}}\\mathbf{g}_{1}=\\frac{180^{2}+1800^{2}}{1001^{2}}=\\frac{180^{2}\\cdot 101}{1001^{2}},\\qquad\nH\\mathbf{g}_{1}=\\begin{pmatrix}-\\frac{3600}{1001}\\\\\\frac{3600}{1001}\\end{pmatrix},\\qquad\n\\mathbf{g}_{1}^{\\mathsf{T}}H\\mathbf{g}_{1}=\\frac{180\\cdot 3600}{1001^{2}}(1+10)=\\frac{7{,}128{,}000}{1001^{2}}.\n$$\nThus\n$$\n\\alpha_{1}=\\frac{\\mathbf{g}_{1}^{\\mathsf{T}}\\mathbf{g}_{1}}{\\mathbf{g}_{1}^{\\mathsf{T}}H\\mathbf{g}_{1}}=\\frac{3{,}272{,}400}{7{,}128{,}000}=\\frac{101}{220}.\n$$\nUpdate\n$$\n\\mathbf{x}_{2}=\\mathbf{x}_{1}-\\alpha_{1}\\mathbf{g}_{1}=\\begin{pmatrix}-\\frac{9}{1001}-\\frac{101}{220}\\left(-\\frac{180}{1001}\\right)\\\\\\frac{900}{1001}-\\frac{101}{220}\\left(\\frac{1800}{1001}\\right)\\end{pmatrix}=\\begin{pmatrix}-\\frac{9}{1001}+\\frac{909}{11011}\\\\\\frac{900}{1001}-\\frac{9090}{11011}\\end{pmatrix}=\\begin{pmatrix}\\frac{810}{11011}\\\\\\frac{810}{11011}\\end{pmatrix}.\n$$\nThe fractions are already in simplest terms because $\\gcd(810,11011)=1$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{810}{11011} & \\frac{810}{11011} \\end{pmatrix}}$$", "id": "2221576"}, {"introduction": "The zigzagging path observed in the previous practice is a symptom of slow convergence, a problem that becomes severe for \"ill-conditioned\" functions with long, narrow valleys. This computational exercise moves beyond manual calculation to systematically investigate this phenomenon. By constructing quadratic functions with varying degrees of ill-conditioning, you will quantify how the condition number of the Hessian matrix directly impacts the convergence rate of the steepest descent method [@problem_id:3279006].", "problem": "You are to investigate the Steepest Descent Method (also called Gradient Descent) with exact line search on a strictly convex quadratic in two variables and to exhibit slow convergence caused by ill-conditioning.\n\nConsider a quadratic objective of the form\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\,(x - x^\\star)^\\top A (x - x^\\star),\n$$\nwhere $x \\in \\mathbb{R}^2$, $x^\\star \\in \\mathbb{R}^2$ is the unique minimizer, and $A \\in \\mathbb{R}^{2 \\times 2}$ is symmetric positive definite (SPD). In all cases below, take $x^\\star = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$ so that $f(x) = \\tfrac{1}{2}\\,x^\\top A x$. The gradient is $\\nabla f(x)$, and the steepest descent iteration is\n$$\nx_{k+1} \\;=\\; x_k \\;-\\; \\alpha_k \\,\\nabla f(x_k),\n$$\nwhere the step length $\\alpha_k$ is chosen by exact line search that minimizes $f(x_k - \\alpha\\,\\nabla f(x_k))$ with respect to $\\alpha \\in \\mathbb{R}$.\n\nYou will design a family of SPD matrices with a prescribed condition number using a rotation and eigenvalues. Let\n$$\nR(\\theta) \\;=\\; \\begin{bmatrix}\n\\cos\\theta & -\\sin\\theta \\\\\n\\sin\\theta & \\cos\\theta\n\\end{bmatrix}, \\qquad\n\\Lambda \\;=\\; \\operatorname{diag}(\\lambda_{\\min}, \\lambda_{\\max}), \\qquad\nA \\;=\\; R(\\theta)\\,\\Lambda\\,R(\\theta)^\\top,\n$$\nso that the spectrum of $A$ is $\\{\\lambda_{\\min}, \\lambda_{\\max}\\}$ and the condition number in the $2$-norm is $\\kappa(A) = \\lambda_{\\max}/\\lambda_{\\min}$. Angles must be specified in radians.\n\nTasks:\n1. From first principles, derive the exact line-search step length rule for steepest descent applied to a strictly convex quadratic $f(x)$ of the form above by minimizing $f(x_k - \\alpha\\,\\nabla f(x_k))$ with respect to $\\alpha$.\n2. Implement steepest descent with this exact line-search rule for the quadratic defined by $A$ as above, starting from the initial point $x_0 = \\begin{bmatrix} 3 \\\\ -1 \\end{bmatrix}$. Stop after exactly $T$ iterations unless the gradient norm becomes exactly zero earlier, in which case you may stop immediately.\n3. For each case in the test suite below, compute the reduction factor\n$$\nr_T \\;=\\; \\frac{f(x_T)}{f(x_0)},\n$$\nwhere $x_T$ is the iterate after $T$ steps (or the final iterate if the method terminates early because the gradient is zero). This ratio is dimensionless.\n\nTest suite (angles in radians):\n- Case S (spherical, boundary condition): $\\lambda_{\\min} = 10$, $\\lambda_{\\max} = 10$, $\\theta = 0.7$, $T = 10$.\n- Case M (moderately ill-conditioned): $\\lambda_{\\min} = 1$, $\\lambda_{\\max} = 100$, $\\theta = \\pi/6$, $T = 10$.\n- Case H (highly ill-conditioned): $\\lambda_{\\min} = 1$, $\\lambda_{\\max} = 10^4$, $\\theta = \\pi/4$, $T = 10$.\n\nYour program must:\n- Construct $A$ for each case using the given $\\lambda_{\\min}$, $\\lambda_{\\max}$, and $\\theta$.\n- Run steepest descent with exact line search for $T$ iterations from $x_0 = \\begin{bmatrix} 3 \\\\ -1 \\end{bmatrix}$.\n- Compute and record $r_T$ for each case.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"), where the three entries are the three values of $r_T$ (in the order S, M, H) as floating-point numbers. No other text should be printed.", "solution": "The problem is assessed to be valid as it is scientifically grounded, well-posed, objective, and contains sufficient information for a unique solution. It represents a standard numerical experiment in optimization to demonstrate the effect of ill-conditioning on the convergence of the steepest descent method.\n\nThe required tasks are:\n1.  Derive the formula for the exact line search step length for a strictly convex quadratic function.\n2.  Implement the steepest descent method using this step length.\n3.  Compute the function value reduction factor $r_T = f(x_T)/f(x_0)$ for three specific cases.\n\n**1. Derivation of the Exact Line Search Step Length**\n\nThe steepest descent algorithm generates a sequence of iterates using the update rule:\n$$\nx_{k+1} = x_k - \\alpha_k \\nabla f(x_k)\n$$\nwhere $\\alpha_k > 0$ is the step length. An exact line search chooses $\\alpha_k$ to minimize the objective function along the search direction $d_k = -\\nabla f(x_k)$. We define a one-dimensional function $\\phi(\\alpha) = f(x_k + \\alpha d_k) = f(x_k - \\alpha g_k)$, where $g_k = \\nabla f(x_k)$.\n\nThe objective function is the strictly convex quadratic form $f(x) = \\frac{1}{2}x^\\top A x$, where $A$ is a symmetric positive definite (SPD) matrix.\nThe gradient of this function is $\\nabla f(x) = \\frac{1}{2}(A + A^\\top)x$. Since $A$ is symmetric ($A=A^\\top$), the gradient is simply:\n$$\ng_k = \\nabla f(x_k) = A x_k\n$$\nWe substitute the update rule into the objective function to get $\\phi(\\alpha)$:\n$$\n\\phi(\\alpha) = f(x_k - \\alpha g_k) = \\frac{1}{2} (x_k - \\alpha g_k)^\\top A (x_k - \\alpha g_k)\n$$\nExpanding this expression, we get:\n$$\n\\phi(\\alpha) = \\frac{1}{2} (x_k^\\top A x_k - \\alpha x_k^\\top A g_k - \\alpha g_k^\\top A x_k + \\alpha^2 g_k^\\top A g_k)\n$$\nSince $g_k^\\top A x_k$ is a scalar, it equals its transpose, $(g_k^\\top A x_k)^\\top = x_k^\\top A^\\top g_k$. Because $A$ is symmetric, this is $x_k^\\top A g_k$. The two linear terms in $\\alpha$ are therefore identical.\n$$\n\\phi(\\alpha) = \\frac{1}{2} x_k^\\top A x_k - \\alpha g_k^\\top A x_k + \\frac{1}{2} \\alpha^2 g_k^\\top A g_k\n$$\nThis is a scalar quadratic function in $\\alpha$. To find the value of $\\alpha$ that minimizes $\\phi(\\alpha)$, we compute its derivative with respect to $\\alpha$ and set it to zero.\n$$\n\\frac{d\\phi}{d\\alpha} = -g_k^\\top A x_k + \\alpha g_k^\\top A g_k = 0\n$$\nSolving for $\\alpha$, we obtain the optimal step length, which we denote as $\\alpha_k$:\n$$\n\\alpha_k = \\frac{g_k^\\top A x_k}{g_k^\\top A g_k}\n$$\nWe can simplify this formula by using the relation $g_k = A x_k$. The numerator becomes $g_k^\\top (A x_k) = g_k^\\top g_k$. Thus, the step length is:\n$$\n\\alpha_k = \\frac{g_k^\\top g_k}{g_k^\\top A g_k}\n$$\nThis is the celebrated formula for the exact line search step length for a quadratic objective. The second derivative, $\\frac{d^2\\phi}{d\\alpha^2} = g_k^\\top A g_k$, is positive because $A$ is positive definite (and assuming $g_k \\neq 0$), which confirms that $\\alpha_k$ yields a minimum. If $g_k = 0$, the iterate $x_k$ is already the minimizer of $f(x)$, and the iteration terminates.\n\n**2. Algorithm Implementation and Execution**\n\nThe numerical experiment is conducted by implementing the steepest descent method with the derived exact line search. For each case (S, M, H), the following steps are performed:\n1.  The specified parameters $\\lambda_{\\min}$, $\\lambda_{\\max}$, and $\\theta$ are used to construct the SPD matrix $A = R(\\theta)\\Lambda R(\\theta)^\\top$.\n2.  The initial point is set to $x_0 = \\begin{bmatrix} 3 \\\\ -1 \\end{bmatrix}$. The initial function value $f(x_0)$ is computed.\n3.  The steepest descent iteration $x_{k+1} = x_k - \\alpha_k g_k$ is run for exactly $T$ steps, unless the gradient norm becomes exactly zero. At each step $k$, the gradient $g_k = A x_k$ and the step length $\\alpha_k = (g_k^\\top g_k) / (g_k^\\top A g_k)$ are computed.\n4.  After $T$ iterations (or early termination), the final iterate is $x_T$.\n5.  The final function value $f(x_T)$ is computed.\n6.  The reduction factor $r_T = f(x_T) / f(x_0)$ is calculated and stored.\n\nFor Case S, $\\lambda_{\\min}=\\lambda_{\\max}$, so the condition number $\\kappa(A)=1$. The matrix $A$ is a scalar multiple of the identity matrix, $A=10I$. The level sets of $f(x)$ are circles, and steepest descent converges in a single step to the minimizer $x^\\star=0$. Thus, $x_1=x_T=0$, $f(x_T)=0$, and $r_T=0$.\n\nFor Cases M and H, the condition numbers are $\\kappa(A)=100$ and $\\kappa(A)=10^4$, respectively. The level sets are elongated ellipses. The steepest descent path will exhibit a characteristic zigzagging behavior, leading to slow convergence. The reduction factor $r_T$ is expected to be significantly larger than $0$, and larger for Case H than for Case M, demonstrating the degradation of performance with increasing ill-conditioning. The provided Python code implements this procedure to compute the required values.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_steepest_descent(lambda_min: float, lambda_max: float, theta: float, T: int, x0: np.ndarray) -> float:\n    \"\"\"\n    Performs steepest descent with exact line search for a quadratic function.\n\n    Args:\n        lambda_min: The minimum eigenvalue of the matrix A.\n        lambda_max: The maximum eigenvalue of the matrix A.\n        theta: The rotation angle in radians for constructing A.\n        T: The number of iterations.\n        x0: The starting point as a numpy array.\n\n    Returns:\n        The reduction factor r_T = f(x_T) / f(x_0).\n    \"\"\"\n    # 1. Construct the matrix A\n    c, s = np.cos(theta), np.sin(theta)\n    R = np.array([[c, -s], [s, c]])\n    Lambda = np.diag([lambda_min, lambda_max])\n    A = R @ Lambda @ R.T\n\n    # Define the objective function\n    def f(x: np.ndarray) -> float:\n        return 0.5 * x.T @ A @ x\n\n    # 2. Calculate initial function value\n    f_x0 = f(x0)\n    \n    # If starting at the minimum, reduction is not well-defined,\n    # but we can consider f(x_T) = f(x_0) = 0, so r_T would be 1.\n    # A more sensible interpretation for f(x_0)=0 is r_T=0, as there's 0 to reduce.\n    if f_x0 == 0.0:\n        return 0.0\n\n    # 3. Run steepest descent iterations\n    x = x0.copy()\n    for _ in range(T):\n        # Calculate gradient\n        g = A @ x\n        \n        # Calculate squared norm of the gradient\n        g_dot_g = g.T @ g\n        \n        # If gradient is zero, we have reached the minimum. Stop early.\n        if g_dot_g == 0.0:\n            x = np.zeros_like(x0)  # The minimum is at x=0\n            break\n            \n        # Calculate exact step length alpha\n        # Note: g.T @ A @ g cannot be zero if A is SPD and g is non-zero.\n        alpha = g_dot_g / (g.T @ A @ g)\n        \n        # Update iterate x\n        x = x - alpha * g\n        \n    # 4. Compute final function value and reduction factor\n    f_xT = f(x)\n    r_T = f_xT / f_x0\n    \n    return r_T\n\ndef solve():\n    \"\"\"\n    Defines test cases, runs the steepest descent for each, and prints results.\n    \"\"\"\n    # Initial point for all cases\n    x0 = np.array([3.0, -1.0])\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case S (spherical)\n        {'name': 'S', 'params': {'lambda_min': 10.0, 'lambda_max': 10.0, 'theta': 0.7, 'T': 10}},\n        # Case M (moderately ill-conditioned)\n        {'name': 'M', 'params': {'lambda_min': 1.0, 'lambda_max': 100.0, 'theta': np.pi/6, 'T': 10}},\n        # Case H (highly ill-conditioned)\n        {'name': 'H', 'params': {'lambda_min': 1.0, 'lambda_max': 10**4, 'theta': np.pi/4, 'T': 10}},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_steepest_descent(\n            lambda_min=case['params']['lambda_min'],\n            lambda_max=case['params']['lambda_max'],\n            theta=case['params']['theta'],\n            T=case['params']['T'],\n            x0=x0\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3279006"}]}