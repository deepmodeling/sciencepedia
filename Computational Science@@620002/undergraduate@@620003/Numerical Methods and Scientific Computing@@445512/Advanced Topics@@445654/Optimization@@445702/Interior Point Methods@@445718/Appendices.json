{"hands_on_practices": [{"introduction": "At the heart of every interior-point method is the iterative process of calculating a search direction and taking a step towards the optimum. This exercise demystifies that core process by guiding you through a single, concrete iteration for a semidefinite program derived from an eigenvalue minimization problem [@problem_id:3242579]. By manually computing a Nesterov–Todd scaled step, you will gain a tangible understanding of the algebraic machinery that enables IPMs to navigate the interior of the feasible set.", "problem": "You will work through a two-part task that connects constrained eigenvalue minimization to semidefinite programming (SDP) and then carries out a single iteration of a primal-dual interior-point method using Nesterov–Todd (NT) scaling.\n\nPart I (Formulation). Consider the constrained eigenvalue minimization problem\n- minimize the largest eigenvalue of a symmetric matrix $Z \\in \\mathbb{S}^{2}$,\n- subject to the linear constraint $\\operatorname{tr}(Z) = 2$.\n\nStarting from the definition of the largest eigenvalue $\\lambda_{\\max}(Z)$ as the minimum real number $t$ such that $t I - Z \\succeq 0$, derive an equivalent linear matrix inequality (LMI) formulation as an SDP with decision variables $(t, Z)$, and state the resulting conic form constraints explicitly.\n\nPart II (One NT-scaled primal-dual iteration and centrality symmetry). Consider the standard-form SDP associated to the conic block $X \\in \\mathbb{S}^{2}$ defined by\n- minimize $C \\bullet X$,\n- subject to $A \\bullet X = b$,\n- and $X \\succ 0$,\nwhere $C = 0 \\cdot I \\in \\mathbb{S}^{2}$, $A = I \\in \\mathbb{S}^{2}$, $b = 2 \\in \\mathbb{R}$, and $\\bullet$ denotes the Frobenius inner product. Its dual is\n- maximize $b y$,\n- subject to $A^{*}(y) + S = C$,\n- and $S \\succ 0$,\nwith $A^{*}(y) = y I$ and $S \\in \\mathbb{S}^{2}$.\n\nStart from the strictly feasible primal-dual point $(X_{0}, y_{0}, S_{0})$ given by $X_{0} = I$, $y_{0} = -2$, $S_{0} = 2 I$. Set the centering parameter to $\\sigma = \\tfrac{1}{2}$. Using the Nesterov–Todd (NT) scaled primal-dual direction, compute a single iteration that uses the full step length if it preserves positive definiteness, and otherwise the largest step that maintains $X \\succ 0$ and $S \\succ 0$. Then:\n- compute the updated duality measure $\\mu_{1} = \\tfrac{1}{n} \\operatorname{tr}(X_{1} S_{1})$ with $n = 2$,\n- and verify the centrality symmetry condition $X_{1} S_{1} = \\mu_{1} I$.\n\nReport the single scalar value of $\\mu_{1}$ as your final answer. No rounding is required; provide the exact value.", "solution": "The problem statement is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n**Part I:**\n- Objective: Minimize the largest eigenvalue, $\\lambda_{\\max}(Z)$, of a symmetric matrix $Z \\in \\mathbb{S}^{2}$.\n- Constraint: $\\operatorname{tr}(Z) = 2$.\n- Definition: $\\lambda_{\\max}(Z)$ is the minimum real number $t$ such that $t I - Z \\succeq 0$.\n- Task: Derive an equivalent semidefinite program (SDP) with decision variables $(t, Z)$ and state the conic form constraints.\n\n**Part II:**\n- Primal SDP: Minimize $C \\bullet X$, subject to $A \\bullet X = b$ and $X \\succ 0$.\n- Primal Data: $X \\in \\mathbb{S}^{2}$, $C = 0 \\cdot I \\in \\mathbb{S}^{2}$, $A = I \\in \\mathbb{S}^{2}$, $b = 2 \\in \\mathbb{R}$.\n- Dual SDP: Maximize $b y$, subject to $A^{*}(y) + S = C$ and $S \\succ 0$.\n- Dual Data: $S \\in \\mathbb{S}^{2}$, $A^{*}(y) = y I$.\n- Initial Point: $(X_{0}, y_{0}, S_{0})$ where $X_{0} = I$, $y_{0} = -2$, and $S_{0} = 2 I$.\n- Centering Parameter: $\\sigma = \\frac{1}{2}$.\n- Method: Nesterov–Todd (NT) scaled primal-dual direction.\n- Task: \n  1. Compute one iteration to find $(X_1, S_1, y_1)$ using a full step length if feasible, otherwise the largest feasible step.\n  2. Compute the updated duality measure $\\mu_{1} = \\frac{1}{n} \\operatorname{tr}(X_{1} S_{1})$ with $n = 2$.\n  3. Verify the centrality symmetry condition $X_{1} S_{1} = \\mu_{1} I$.\n  4. Report the value of $\\mu_1$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-defined and consists of two parts.\n\nPart I is a standard reformulation task in the field of convex optimization, converting an eigenvalue minimization problem into an SDP. The provided definition is correct.\n\nPart II sets up a specific SDP and asks for one iteration of a primal-dual interior-point method. Let's verify the feasibility of the initial point $(X_0, y_0, S_0)$.\n- Primal strict feasibility ($X_0 \\succ 0$): $X_0 = I$, the identity matrix in $\\mathbb{S}^2$, is positive definite. This holds.\n- Primal constraint satisfaction ($A \\bullet X_0 = b$): $A \\bullet X_0 = I \\bullet I = \\operatorname{tr}(I^T I) = \\operatorname{tr}(I) = 2$. The given $b=2$. The constraint is satisfied.\n- Dual strict feasibility ($S_0 \\succ 0$): $S_0 = 2I$ is positive definite. This holds.\n- Dual constraint satisfaction ($A^{*}(y_0) + S_0 = C$): $A^{*}(y_0) + S_0 = y_0 I + S_0 = (-2)I + 2I = 0$. The given $C = 0 \\cdot I = 0$. The constraint is satisfied.\n\nAll data are self-contained and consistent. The problem is scientifically grounded in numerical optimization, is objective, and well-posed. No flaws are detected.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n### Part I: Formulation\n\nThe problem is to minimize the largest eigenvalue of a symmetric matrix $Z \\in \\mathbb{S}^{2}$, subject to a linear constraint on its trace.\n$$\n\\begin{array}{ll}\n\\text{minimize} & \\lambda_{\\max}(Z) \\\\\n\\text{subject to} & \\operatorname{tr}(Z) = 2, \\\\\n& Z \\in \\mathbb{S}^{2}.\n\\end{array}\n$$\nWe introduce an auxiliary scalar variable $t \\in \\mathbb{R}$. The objective can be reformulated as minimizing $t$ subject to the additional constraint $t \\ge \\lambda_{\\max}(Z)$. The problem becomes:\n$$\n\\begin{array}{ll}\n\\text{minimize} & t \\\\\n\\text{subject to} & \\lambda_{\\max}(Z) \\le t, \\\\\n& \\operatorname{tr}(Z) = 2, \\\\\n& Z \\in \\mathbb{S}^{2}, t \\in \\mathbb{R}.\n\\end{array}\n$$\nThe problem statement provides the key equivalence: $\\lambda_{\\max}(Z) \\le t$ is equivalent to the linear matrix inequality (LMI) $t I - Z \\succeq 0$, where $\\succeq 0$ denotes that the matrix is positive semidefinite and $I$ is the $2 \\times 2$ identity matrix.\n\nSubstituting this equivalence yields the SDP formulation in the decision variables $(t, Z)$:\n$$\n\\begin{array}{ll}\n\\text{minimize} & t \\\\\n\\text{subject to} & \\operatorname{tr}(Z) = 2, \\\\\n& t I - Z \\succeq 0.\n\\end{array}\n$$\nThe resulting constraints are explicitly:\n1.  A linear equality constraint: $\\operatorname{tr}(Z) = 2$.\n2.  A conic constraint (specifically, an LMI): The matrix $t I - Z$ must be positive semidefinite.\n\n### Part II: One NT-scaled primal-dual iteration\n\nWe are given the primal-dual pair of SDPs and an initial strictly feasible point $(X_0, y_0, S_0) = (I, -2, 2I)$. The matrix dimension is $n=2$.\n\nFirst, we compute the initial duality measure $\\mu_0$:\n$$ \\mu_0 = \\frac{1}{n} \\operatorname{tr}(X_0 S_0) = \\frac{1}{2} \\operatorname{tr}(I \\cdot 2I) = \\frac{1}{2} \\operatorname{tr}(2I) = \\frac{1}{2}(2+2) = 2. $$\nWe observe that $X_0 S_0 = I (2I) = 2I$ and $\\mu_0 I = 2I$. Thus, $X_0 S_0 = \\mu_0 I$, which means the initial point lies on the central path.\n\nThe Nesterov-Todd (NT) direction $(\\Delta X, \\Delta y, \\Delta S)$ is found by solving the linearized KKT system for the perturbed barrier problem. Since the initial point is feasible, the feasibility residuals are zero. The system to solve for the direction is:\n1.  Primal feasibility: $A \\bullet \\Delta X = 0 \\implies \\operatorname{tr}(\\Delta X) = 0$.\n2.  Dual feasibility: $A^{*}(\\Delta y) + \\Delta S = 0 \\implies (\\Delta y)I + \\Delta S = 0$.\n3.  Centrality condition: $\\Delta X S_0 + X_0 \\Delta S = \\sigma \\mu_0 I - X_0 S_0$.\n\nFor a starting point on the central path where $X_0$ and $S_0$ are multiples of the identity, they commute, so $X_0 S_0 = S_0 X_0$. In this case, the NT direction system simplifies and is equivalent to other common primal-dual directions.\n\nLet's solve this system.\nFrom (2), we have $\\Delta S = -(\\Delta y)I$.\n\nNow, we compute the right-hand side of equation (3):\n$$ \\sigma \\mu_0 I - X_0 S_0 = \\frac{1}{2}(2)I - 2I = I - 2I = -I. $$\nSubstitute $X_0 = I$, $S_0 = 2I$, and the expression for the right-hand side into (3):\n$$ \\Delta X (2I) + I (\\Delta S) = -I \\implies 2\\Delta X + \\Delta S = -I. $$\nNow substitute $\\Delta S = -(\\Delta y)I$:\n$$ 2\\Delta X - (\\Delta y)I = -I. $$\nTo solve for $\\Delta y$, we take the trace of this equation and use the condition $\\operatorname{tr}(\\Delta X)=0$ from (1):\n$$ \\operatorname{tr}(2\\Delta X - (\\Delta y)I) = \\operatorname{tr}(-I) $$\n$$ 2\\operatorname{tr}(\\Delta X) - \\Delta y \\operatorname{tr}(I) = -2 $$\n$$ 2(0) - \\Delta y(2) = -2 $$\n$$ -2\\Delta y = -2 \\implies \\Delta y = 1. $$\nWith $\\Delta y = 1$, we find $\\Delta S$:\n$$ \\Delta S = -(\\Delta y)I = -I. $$\nAnd we find $\\Delta X$:\n$$ 2\\Delta X - (1)I = -I \\implies 2\\Delta X = 0 \\implies \\Delta X = 0. $$\nThe computed primal-dual direction is $(\\Delta X, \\Delta y, \\Delta S) = (0, 1, -I)$.\n\nNext, we find the step length $\\alpha$. The problem asks to use a full step $\\alpha=1$ if it preserves positive definiteness ($X_1 \\succ 0, S_1 \\succ 0$), and otherwise the largest feasible step. Let's check feasibility for $\\alpha > 0$.\nThe next iterates are given by:\n$X(\\alpha) = X_0 + \\alpha \\Delta X = I + \\alpha(0) = I$.\n$S(\\alpha) = S_0 + \\alpha \\Delta S = 2I + \\alpha(-I) = (2-\\alpha)I$.\n\nFor $X(\\alpha) \\succ 0$: Since $X(\\alpha) = I$, which is positive definite, this condition holds for any $\\alpha$.\nFor $S(\\alpha) \\succ 0$: We need $(2-\\alpha)I \\succ 0$, which requires $2-\\alpha > 0$, or $\\alpha < 2$.\n\nSince the maximum feasible step length is $2$, the full step length $\\alpha = 1$ is feasible. We use $\\alpha=1$.\n\nWe compute the new iterate $(X_1, y_1, S_1)$:\n$y_1 = y_0 + \\alpha \\Delta y = -2 + 1(1) = -1$.\n$X_1 = X_0 + \\alpha \\Delta X = I + 1(0) = I$.\n$S_1 = S_0 + \\alpha \\Delta S = 2I + 1(-I) = I$.\n\nNow, we compute the updated duality measure $\\mu_1$:\n$$ \\mu_1 = \\frac{1}{n} \\operatorname{tr}(X_1 S_1) = \\frac{1}{2} \\operatorname{tr}(I \\cdot I) = \\frac{1}{2} \\operatorname{tr}(I) = \\frac{1}{2}(2) = 1. $$\nFinally, we verify the centrality symmetry condition $X_1 S_1 = \\mu_1 I$:\n$X_1 S_1 = I \\cdot I = I$.\n$\\mu_1 I = 1 \\cdot I = I$.\nThe condition $X_1 S_1 = \\mu_1 I$ is satisfied.\n\nThe final requested value is $\\mu_1$.", "answer": "$$\n\\boxed{1}\n$$", "id": "3242579"}, {"introduction": "The way an optimization problem is formulated can significantly impact algorithmic performance, even if the feasible region remains unchanged. This practice explores the subtle but important effects of redundant constraints on an Interior Point Method [@problem_id:3242722]. You will analyze how duplication of a constraint alters the logarithmic barrier function, the conditioning of the Newton system, and the structure of the dual solution, providing critical insight for practical modeling.", "problem": "Consider the linear program with decision vector $x \\in \\mathbb{R}^2$:\nminimize $c^{\\top} x$ with $c = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$, subject to the inequality constraints\n$x_1 \\ge 0$, $2 x_1 \\ge 0$, and $x_2 \\ge 1$.\nThe inequality $2 x_1 \\ge 0$ is redundant with $x_1 \\ge 0$. An Interior Point Method (IPM) is implemented using the logarithmic barrier for the inequalities, forming the barrier subproblem with parameter $\\mu > 0$:\nminimize $c^{\\top} x - \\mu \\sum_{i=1}^m \\log(g_i(x))$, where each $g_i(x) > 0$ represents an affine function corresponding to a constraint, and the Karush-Kuhn-Tucker (KKT) conditions of the barrier subproblem define the central path.\n\nAnalyze the effect of including redundant constraints on the formulation and behavior of the Interior Point Method (IPM). Which of the following statements are true? Select all that apply.\n\nA. The duplicate constraints $x_1 \\ge 0$ and $2 x_1 \\ge 0$ do not change the primal feasible region, but in a logarithmic barrier formulation they scale the curvature contributed by the boundary $x_1 = 0$ by a factor of $2$, which can worsen the conditioning of the Newton system solved within IPM iterations.\n\nB. For any fixed barrier parameter $\\mu > 0$, the primal point on the central path with the duplicated model coincides with that of the non-duplicated model (with only $x_1 \\ge 0$), because the additive constant $-\\mu \\log(2)$ from the redundant constraint does not affect first-order optimality.\n\nC. The maximum step length allowed by positivity of slacks in a primal-dual IPM line search is strictly smaller when the redundant inequality $2 x_1 \\ge 0$ is present, because the additional slack unavoidably hits zero sooner along any search direction.\n\nD. At optimality, the Lagrange multipliers associated with the two duplicate constraints can be non-unique, and only their combined effect in the stationarity equation must match the objective gradient; thus, redundancy can cause dual degeneracy without affecting the primal optimum or optimal value.\n\nE. Redundant constraints render the logarithmic barrier non-strictly convex and can destroy the existence of a unique central path.\n\nF. Removing redundant constraints reduces the dimension of the KKT linear system solved at each IPM iteration and can decrease per-iteration computational cost, while the limiting primal solution as $\\mu \\to 0$ remains unchanged under standard regularity assumptions.", "solution": "The user wants me to validate the problem statement and, if valid, solve the problem by analyzing the given options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   The problem is a linear program (LP).\n-   The decision vector is $x \\in \\mathbb{R}^2$, with components $x_1$ and $x_2$.\n-   The objective is to minimize $c^{\\top} x$, where $c = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\n-   The constraints are $x_1 \\ge 0$, $2 x_1 \\ge 0$, and $x_2 \\ge 1$.\n-   It is noted that the constraint $2 x_1 \\ge 0$ is redundant with $x_1 \\ge 0$.\n-   The solution method is an Interior Point Method (IPM) using a logarithmic barrier.\n-   The barrier subproblem for a barrier parameter $\\mu > 0$ is: minimize $c^{\\top} x - \\mu \\sum_{i=1}^m \\log(g_i(x))$, where $g_i(x) > 0$ are the affine functions from the inequality constraints.\n-   The central path is defined by the Karush-Kuhn-Tucker (KKT) conditions of the barrier subproblem.\n-   The task is to analyze the effect of the redundant constraint on the IPM and determine which of the provided statements are true.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem is firmly situated in the field of convex optimization and numerical methods. Linear programming, interior point methods, logarithmic barriers, the central path, and the effects of redundant constraints are all standard, well-established concepts.\n-   **Well-Posed:** The problem is clearly stated. It presents a specific LP and asks for an analysis of the consequences of a redundant constraint within the framework of a standard algorithm (IPM). This is a well-defined question with a definite answer based on the principles of optimization theory.\n-   **Objective:** The language is technical and precise. There are no subjective, ambiguous, or opinion-based statements in the problem's setup.\n\n**Flaw Checklist:**\n1.  **Scientific or Factual Unsoundness:** None. The premises and concepts are standard in mathematics and computer science.\n2.  **Non-Formalizable or Irrelevant:** None. The problem is a formal mathematical one, directly relevant to the topic of interior point methods.\n3.  **Incomplete or Contradictory Setup:** None. The problem provides all necessary information. The noted redundancy is the subject of the analysis, not a flaw in the problem statement.\n4.  **Unrealistic or Infeasible:** None. The LP is simple and has a clear solution. Redundant constraints are a common practical issue in optimization.\n5.  **Ill-Posed or Poorly Structured:** None. The questions posed in the options are specific and can be rigorously evaluated.\n6.  **Trivial or Tautological:** None. The effects of redundancy in IPMs involve non-trivial interactions between the primal and dual variables, the Hessian of the barrier function, and computational complexity.\n7.  **Outside Scientific Verifiability:** None. All claims can be verified through mathematical derivation.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. I will proceed with the solution.\n\n### Solution Derivation\n\nFirst, we formalize the problem with and without the redundant constraint to analyze the differences.\n\n**Model 1: Non-Redundant Formulation**\nThe problem is to minimize $x_1 + x_2$ subject to $x_1 \\ge 0$ and $x_2 \\ge 1$.\nThe constraints are $g_1(x) = x_1 > 0$ and $g_2(x) = x_2 - 1 > 0$.\nThe logarithmic barrier subproblem is:\n$$ \\text{minimize} \\quad f_{NR}(x, \\mu) = (x_1 + x_2) - \\mu (\\log(x_1) + \\log(x_2 - 1)) $$\nThe central path is defined by the points $x(\\mu)$ that satisfy the first-order optimality condition $\\nabla_x f_{NR}(x, \\mu) = 0$:\n$$ \\nabla_x f_{NR}(x, \\mu) = \\begin{bmatrix} 1 - \\frac{\\mu}{x_1} \\\\ 1 - \\frac{\\mu}{x_2 - 1} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} $$\nSolving for $x_1$ and $x_2$ gives the point on the central path for the non-redundant model:\n$$ x_{1,NR}(\\mu) = \\mu $$\n$$ x_{2,NR}(\\mu) = 1 + \\mu $$\nThe Hessian of the barrier objective function is:\n$$ \\nabla^2 f_{NR}(x, \\mu) = \\begin{bmatrix} \\frac{\\mu}{x_1^2} & 0 \\\\ 0 & \\frac{\\mu}{(x_2-1)^2} \\end{bmatrix} $$\n\n**Model 2: Redundant Formulation**\nThe problem is to minimize $x_1 + x_2$ subject to $x_1 \\ge 0$, $2x_1 \\ge 0$, and $x_2 \\ge 1$.\nThe constraints are $g_1(x) = x_1 > 0$, $g_2(x) = 2x_1 > 0$, and $g_3(x) = x_2 - 1 > 0$.\nThe logarithmic barrier subproblem is:\n$$ \\text{minimize} \\quad f_{R}(x, \\mu) = (x_1 + x_2) - \\mu (\\log(x_1) + \\log(2x_1) + \\log(x_2 - 1)) $$\nUsing the property $\\log(2x_1) = \\log(2) + \\log(x_1)$, we can rewrite the objective:\n$$ f_{R}(x, \\mu) = (x_1 + x_2) - \\mu (2\\log(x_1) + \\log(x_2 - 1)) - \\mu \\log(2) $$\nThe constant term $-\\mu\\log(2)$ does not affect the location of the minimum. The central path is defined by $\\nabla_x f_{R}(x, \\mu) = 0$:\n$$ \\nabla_x f_{R}(x, \\mu) = \\begin{bmatrix} 1 - \\frac{2\\mu}{x_1} \\\\ 1 - \\frac{\\mu}{x_2 - 1} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} $$\nSolving for $x_1$ and $x_2$ gives the point on the central path for the redundant model:\n$$ x_{1,R}(\\mu) = 2\\mu $$\n$$ x_{2,R}(\\mu) = 1 + \\mu $$\nThe Hessian of this objective function is:\n$$ \\nabla^2 f_{R}(x, \\mu) = \\begin{bmatrix} \\frac{2\\mu}{x_1^2} & 0 \\\\ 0 & \\frac{\\mu}{(x_2-1)^2} \\end{bmatrix} $$\n\nNow, we evaluate each option.\n\n**A. The duplicate constraints $x_1 \\ge 0$ and $2 x_1 \\ge 0$ do not change the primal feasible region, but in a logarithmic barrier formulation they scale the curvature contributed by the boundary $x_1 = 0$ by a factor of $2$, which can worsen the conditioning of the Newton system solved within IPM iterations.**\n- The feasible region is $\\{x \\in \\mathbb{R}^2 \\mid x_1 \\ge 0, x_2 \\ge 1\\}$. The constraint $2x_1 \\ge 0$ is equivalent to $x_1 \\ge 0$, so the feasible region is unchanged. This part is true.\n- The curvature of the objective function is given by its Hessian. The barrier part of the objective is $-\\mu \\sum \\log(g_i(x))$. For the non-redundant case, the term related to the boundary $x_1=0$ is $-\\mu \\log(x_1)$, and its contribution to the Hessian is the $(1,1)$ entry $\\frac{\\mu}{x_1^2}$. In the redundant case, the term is $-\\mu(\\log(x_1) + \\log(2x_1)) = -2\\mu\\log(x_1) - \\mu\\log(2)$. Its contribution to the Hessian's $(1,1)$ entry is $\\frac{2\\mu}{x_1^2}$.\n- This is exactly twice the curvature contribution from the non-redundant formulation. The statement is correct that it is scaled by a factor of $2$.\n- The Newton system solved in IPM is based on this Hessian. Increasing one eigenvalue of the Hessian relative to others generally increases the condition number of the matrix. For example, the condition number of $\\nabla^2 f_R$ is $\\frac{\\max(2\\mu/x_1^2, \\mu/(x_2-1)^2)}{\\min(2\\mu/x_1^2, \\mu/(x_2-1)^2)}$. If $x_1 \\approx x_2-1$, the condition number is approximately $2$, whereas for the non-redundant case it would be approximately $1$. A higher condition number means the system is worse conditioned. This part is also true.\nVerdict: **Correct**.\n\n**B. For any fixed barrier parameter $\\mu > 0$, the primal point on the central path with the duplicated model coincides with that of the non-duplicated model (with only $x_1 \\ge 0$), because the additive constant $-\\mu \\log(2)$ from the redundant constraint does not affect first-order optimality.**\n- It is true that the additive constant $-\\mu\\log(2)$ does not affect the first-order optimality conditions (the gradient).\n- However, the conclusion is false. As derived above, the central path points are different:\n    - Non-redundant: $x_{NR}(\\mu) = (\\mu, 1+\\mu)$.\n    - Redundant: $x_{R}(\\mu) = (2\\mu, 1+\\mu)$.\n- For any $\\mu > 0$, these points do not coincide. The redundant constraint fundamentally alters the shape of the barrier function, not just by shifting it by a constant.\nVerdict: **Incorrect**.\n\n**C. The maximum step length allowed by positivity of slacks in a primal-dual IPM line search is strictly smaller when the redundant inequality $2 x_1 \\ge 0$ is present, because the additional slack unavoidably hits zero sooner along any search direction.**\n- Let the slack variables for the constraints $x_1 \\ge 0$ and $2x_1 \\ge 0$ be $s_a = x_1$ and $s_b = 2x_1$, respectively. Note that $s_b=2s_a$.\n- A primal-dual IPM takes a step $\\alpha$ in direction $(\\Delta x, \\Delta s)$. The step must maintain positivity: $s + \\alpha \\Delta s > 0$.\n- The conditions for our two related slacks are:\n    1. $s_a + \\alpha \\Delta s_a > 0$\n    2. $s_b + \\alpha \\Delta s_b > 0$\n- Since $s_b = 2s_a$, the change in slacks is also related by $\\Delta s_b = 2 \\Delta s_a$. Substituting this into the second condition gives:\n    $2s_a + \\alpha (2\\Delta s_a) > 0 \\implies 2(s_a + \\alpha \\Delta s_a) > 0$\n- This is the exact same condition as the first one. The redundant constraint does not impose any new or stricter limit on the step size $\\alpha$. The two slacks approach their zero boundary at the same proportional rate.\nVerdict: **Incorrect**.\n\n**D. At optimality, the Lagrange multipliers associated with the two duplicate constraints can be non-unique, and only their combined effect in the stationarity equation must match the objective gradient; thus, redundancy can cause dual degeneracy without affecting the primal optimum or optimal value.**\n- The primal optimum is found by minimizing $x_1+x_2$ over $x_1\\ge0, x_2\\ge1$. The solution is trivially $x^*=(0,1)$ with optimal value $1$. This is unaffected by the redundant constraint.\n- Let's write the KKT conditions for the redundant LP. The Lagrangian is $L(x, \\lambda) = (x_1+x_2) - \\lambda_1 x_1 - \\lambda'_1 (2x_1) - \\lambda_2 (x_2-1)$, with dual variables $\\lambda_1, \\lambda'_1, \\lambda_2 \\ge 0$.\n- The stationarity conditions are $\\nabla_x L = 0$:\n    - $\\frac{\\partial L}{\\partial x_1}: 1 - \\lambda_1 - 2\\lambda'_1 = 0 \\implies \\lambda_1 + 2\\lambda'_1 = 1$.\n    - $\\frac{\\partial L}{\\partial x_2}: 1 - \\lambda_2 = 0 \\implies \\lambda_2 = 1$.\n- Any set of non-negative multipliers $(\\lambda_1, \\lambda'_1, \\lambda_2)$ satisfying these equations is a valid dual solution. We have $\\lambda_2=1$. For the others, we need $\\lambda_1 + 2\\lambda'_1 = 1$ and $\\lambda_1, \\lambda'_1 \\ge 0$. This equation defines a line segment in the $(\\lambda_1, \\lambda'_1)$-plane, with endpoints $(1,0)$ and $(0, 1/2)$. There are infinitely many solutions. For example, $(\\lambda_1, \\lambda'_1) = (1, 0)$ and $(\\lambda_1, \\lambda'_1) = (0, 1/2)$ are both valid.\n- This non-uniqueness of the dual optimal solution is known as dual degeneracy. Primal redundancy is a common cause of dual degeneracy. So, the statement is entirely correct.\nVerdict: **Correct**.\n\n**E. Redundant constraints render the logarithmic barrier non-strictly convex and can destroy the existence of a unique central path.**\n- The strict convexity of the barrier objective function $f(x,\\mu)$ depends on its Hessian, $\\nabla^2 f(x, \\mu)$, being positive definite.\n- The Hessian of the barrier component is $\\sum_i \\mu \\frac{\\nabla g_i(x) (\\nabla g_i(x))^{\\top}}{(g_i(x))^2}$. This sum of rank-1 positive semi-definite matrices is positive definite if the constraint gradients $\\{\\nabla g_i(x)\\}$ span the entire space $\\mathbb{R}^n$.\n- In our problem, $n=2$. The constraint gradients are $\\nabla g_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, $\\nabla g_2 = \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}$, and $\\nabla g_3 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$. The set of vectors $\\{\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\}$ is a subset of these gradients and already spans $\\mathbb{R}^2$.\n- Therefore, the Hessian $\\nabla^2 f_R(x,\\mu)$ is positive definite for any strictly feasible $x$. As calculated before, $\\nabla^2 f_R(x, \\mu) = \\text{diag}(\\frac{2\\mu}{x_1^2}, \\frac{\\mu}{(x_2-1)^2})$, which has positive eigenvalues.\n- Since the objective function is strictly convex (on the interior of the feasible set), it has a unique minimizer for each $\\mu > 0$. This ensures the central path is well-defined and unique. The statement is false.\nVerdict: **Incorrect**.\n\n**F. Removing redundant constraints reduces the dimension of the KKT linear system solved at each IPM iteration and can decrease per-iteration computational cost, while the limiting primal solution as $\\mu \\to 0$ remains unchanged under standard regularity assumptions.**\n- The KKT system in a primal-dual IPM involves variables for $x \\in \\mathbb{R}^n$, duals $\\lambda \\in \\mathbb{R}^m$, and slacks $s \\in \\mathbb{R}^m$, where $m$ is the number of inequalities. The size of the linear system to be solved at each step is related to $n+m$.\n- In our case, $n=2$. The redundant model has $m=3$ constraints. The non-redundant model has $m=2$. Removing the redundant constraint reduces $m$ by $1$, thereby reducing the dimension of the KKT system. Solving a smaller linear system is computationally cheaper. This part is true.\n- The limiting point of the central path, $\\lim_{\\mu \\to 0} x(\\mu)$, corresponds to the optimal solution of the LP. Since the redundant constraint does not change the feasible set or the objective function, the optimal solution to the LP remains the same.\n- We verified this with our specific paths: $\\lim_{\\mu \\to 0} x_{NR}(\\mu) = \\lim_{\\mu \\to 0} (\\mu, 1+\\mu) = (0,1)$ and $\\lim_{\\mu \\to 0} x_{R}(\\mu) = \\lim_{\\mu \\to 0} (2\\mu, 1+\\mu) = (0,1)$. The limit is indeed the same. The statement is true.\nVerdict: **Correct**.", "answer": "$$\\boxed{ADF}$$", "id": "3242722"}, {"introduction": "Moving beyond single steps and theoretical analysis, this practice challenges you to build a complete, robust solver for linear programs. You will implement the Homogeneous Self-Dual Embedding (HSDE), a powerful IPM variant that requires no feasible starting point and can definitively classify a problem as feasible-optimal, primal infeasible, or dual infeasible [@problem_id:3242648]. This capstone exercise integrates the fundamental theory of IPMs into a practical, working algorithm, solidifying your understanding of how these methods are engineered for real-world use.", "problem": "Implement a complete, runnable program that constructs and solves the Homogeneous Self-Dual Embedding (HSDE) of a small family of linear programs using an interior-point method, and classifies each problem as either feasible-optimal, primal infeasible, or dual infeasible (unbounded primal). The program must produce a single line of output, a comma-separated list of integers enclosed in square brackets, where each integer corresponds to the classification of one test case: $0$ for feasible-optimal (solution found), $1$ for primal infeasible, and $2$ for dual infeasible (primal unbounded).\n\nThe linear programming (LP) primal-dual pair in standard form is defined as follows. The primal problem is\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; c^\\top x \\quad \\text{subject to} \\quad A x = b, \\; x \\ge 0,\n$$\nand the dual problem is\n$$\n\\max_{y \\in \\mathbb{R}^m} \\; b^\\top y \\quad \\text{subject to} \\quad A^\\top y + s = c, \\; s \\ge 0,\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, $c \\in \\mathbb{R}^n$, $x \\in \\mathbb{R}^n$ are primal variables, $y \\in \\mathbb{R}^m$ are dual variables, and $s \\in \\mathbb{R}^n$ are dual slack variables. The Homogeneous Self-Dual Embedding (HSDE) introduces two additional nonnegative scalars $\\tau \\ge 0$ and $\\kappa \\ge 0$ to create a single feasibility problem whose solutions encode all outcomes: feasible-optimality, primal infeasibility, or dual infeasibility.\n\nStarting from core definitions in convex optimization and the Karush–Kuhn–Tucker (KKT) conditions, use the HSDE to form a system that enforces the primal and dual feasibility with a scalar complementarity condition and centrality. Specifically, include variables $(x, s, y, \\tau, \\kappa)$ obeying the homogeneous constraints\n$$\nA x - b \\tau = 0, \\quad A^\\top y + s - c \\tau = 0, \\quad c^\\top x - b^\\top y + \\kappa = 0,\n$$\nwith nonnegativity constraints $x \\ge 0$, $s \\ge 0$, $\\tau \\ge 0$, $\\kappa \\ge 0$, together with a complementarity condition coupling the primal and dual slacks that defines an interior-point central path. Derive a Newton step for a barrier-augmented system that linearizes these equations and the complementarity relations for $(x, s)$ and $(\\tau, \\kappa)$, and use backtracking to maintain strict positivity.\n\nYour implementation must:\n- Initialize $(x, s, y, \\tau, \\kappa)$ at strictly positive values, enforce centrality through a barrier parameter, and iteratively solve the linearized KKT system until a termination criterion is met.\n- Use residuals\n$$\nr_p = A x - b \\tau,\\quad r_d = A^\\top y + s - c \\tau,\\quad r_g = c^\\top x - b^\\top y + \\kappa\n$$\nand an averaged complementarity\n$$\n\\mu = \\frac{x^\\top s + \\tau \\kappa}{n+1}\n$$\nto guide progress. Maintain strict positivity of $x$, $s$, $\\tau$, and $\\kappa$ at all times.\n- Implement classification rules based on HSDE outcomes:\n    - Feasible-optimal ($0$): when $\\tau$ is bounded away from $0$, $\\kappa/\\tau$ is small, and the scaled residuals $\\|r_p\\|/\\tau$ and $\\|r_d\\|/\\tau$ are small.\n    - Primal infeasible ($1$): when $\\tau$ is close to $0$, $\\kappa$ is positive, and a Farkas certificate for primal infeasibility is detected via the dual variable $y$, namely $A^\\top y \\ge 0$ and $b^\\top y < 0$.\n    - Dual infeasible ($2$): when $\\tau$ is close to $0$, $\\kappa$ is positive, and a certificate $x \\ge 0$, $A x = 0$, $c^\\top x < 0$ is detected (primal unbounded).\n\nTest Suite:\nProvide three LP instances $(A, b, c)$ that collectively test feasibility, primal infeasibility, and dual infeasibility. Use the following concrete cases:\n1. Case F (feasible-optimal):\n   $$\n   A = \\begin{bmatrix} 1 & 2 \\\\ 1 & -1 \\end{bmatrix},\\quad b = \\begin{bmatrix} 4 \\\\ 1 \\end{bmatrix},\\quad c = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}.\n   $$\n   This system admits a strictly feasible solution with $x \\ge 0$.\n2. Case PI (primal infeasible):\n   $$\n   A = \\begin{bmatrix} 1 & 0 \\end{bmatrix},\\quad b = \\begin{bmatrix} -1 \\end{bmatrix},\\quad c = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}.\n   $$\n   The equality $x_1 = -1$ contradicts $x \\ge 0$, so no primal feasible point exists.\n3. Case DI (dual infeasible, primal unbounded):\n   $$\n   A = \\begin{bmatrix} 1 & -1 \\end{bmatrix},\\quad b = \\begin{bmatrix} 0 \\end{bmatrix},\\quad c = \\begin{bmatrix} -1 \\\\ -1 \\end{bmatrix}.\n   $$\n   The constraint $A x = 0$ with $x \\ge 0$ admits nontrivial directions with $c^\\top x < 0$, implying primal unboundedness.\n\nYour program should run these three cases in order and produce a single output line containing a list of three integers $[r_F, r_{PI}, r_{DI}]$, where each $r$ is one of $\\{0,1,2\\}$ according to the classification described above.\n\nFinal Output Format:\nYour program should produce a single line containing the results as a comma-separated list enclosed in square brackets (e.g., \"[0,1,2]\"). No other text should be printed.", "solution": "The user has requested the implementation of a Homogeneous Self-Dual Embedding (HSDE) interior-point method for solving a family of linear programs (LPs). The solution must be capable of classifying each LP as feasible-optimal, primal infeasible, or dual infeasible.\n\n### Step 1: Extract Givens\n\n-   **Primal LP**: $\\min_{x \\in \\mathbb{R}^n} \\; c^\\top x \\quad \\text{subject to} \\quad A x = b, \\; x \\ge 0$.\n-   **Dual LP**: $\\max_{y \\in \\mathbb{R}^m} \\; b^\\top y \\quad \\text{subject to} \\quad A^\\top y + s = c, \\; s \\ge 0$.\n-   **HSDE Variables**: $(x, s, y, \\tau, \\kappa)$, where $x \\in \\mathbb{R}^n, s \\in \\mathbb{R}^n, y \\in \\mathbb{R}^m, \\tau \\in \\mathbb{R}, \\kappa \\in \\mathbb{R}$.\n-   **HSDE Homogeneous Constraints**:\n    1.  $A x - b \\tau = 0$\n    2.  $A^\\top y + s - c \\tau = 0$\n    3.  $c^\\top x - b^\\top y + \\kappa = 0$\n-   **Non-negativity Constraints**: $x \\ge 0, s \\ge 0, \\tau \\ge 0, \\kappa \\ge 0$.\n-   **Residuals Definitions**:\n    -   $r_p = A x - b \\tau$\n    -   $r_d = A^\\top y + s - c \\tau$\n    -   $r_g = c^\\top x - b^\\top y + \\kappa$\n-   **Averaged Complementarity**: $\\mu = \\frac{x^\\top s + \\tau \\kappa}{n+1}$.\n-   **Classification Rules**:\n    1.  **Feasible-optimal (0)**: $\\tau$ bounded away from $0$, $\\kappa/\\tau$ small, scaled residuals small.\n    2.  **Primal infeasible (1)**: $\\tau$ close to $0$, $\\kappa > 0$, and certificate $A^\\top y \\ge 0, b^\\top y < 0$.\n    3.  **Dual infeasible (2)**: $\\tau$ close to $0$, $\\kappa > 0$, and certificate $x \\ge 0, A x = 0, c^\\top x < 0$.\n-   **Test Cases**:\n    1.  **Case F (feasible-optimal)**: $A = \\begin{bmatrix} 1 & 2 \\\\ 1 & -1 \\end{bmatrix}, b = \\begin{bmatrix} 4 \\\\ 1 \\end{bmatrix}, c = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$.\n    2.  **Case PI (primal infeasible)**: $A = \\begin{bmatrix} 1 & 0 \\end{bmatrix}, b = \\begin{bmatrix} -1 \\end{bmatrix}, c = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\n    3.  **Case DI (dual infeasible)**: $A = \\begin{bmatrix} 1 & -1 \\end{bmatrix}, b = \\begin{bmatrix} 0 \\end{bmatrix}, c = \\begin{bmatrix} -1 \\\\ -1 \\end{bmatrix}$.\n-   **Final Output Format**: A comma-separated list of integers (e.g., `[0,1,2]`).\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem statement is scientifically sound, well-posed, and objective.\n-   **Scientifically Grounded**: The problem describes the Homogeneous Self-Dual Embedding, a standard and rigorously proven technique in the field of convex optimization and numerical methods for solving linear programs. All mathematical formulations are correct and based on fundamental principles of optimization theory, specifically interior-point methods and KKT conditions.\n-   **Well-Posed**: The problem is well-posed. The HSDE framework is specifically designed to have a strictly feasible starting point and to guarantee a solution that allows for unambiguous classification of the original LP's status. The provided test cases are constructed to demonstrate each of the three possible outcomes (feasible-optimal, primal infeasible, dual infeasible).\n-   **Objective**: The language is precise and formal. All terms are standard in the field. The objectives are quantifiable and the classification rules are based on verifiable mathematical conditions.\n-   **Completeness**: The problem is self-contained. It provides the full mathematical specification of the model, the algorithm's core components (Newton step, backtracking), the termination and classification logic, and a complete set of test data.\n\n### Step 3: Verdict and Action\n\nThe problem is valid. We will proceed with providing a solution.\n\n### Algorithmic Derivation and Implementation Plan\n\nThe core of an interior-point method is to iteratively solve a linearized version of the perturbed Karush-Kuhn-Tucker (KKT) conditions, which define a \"central path\" that converges to the solution. For the HSDE, the KKT conditions are augmented with a barrier term parametrized by $\\mu$. The goal is to find a Newton step $(\\Delta x, \\Delta y, \\Delta s, \\Delta \\tau, \\Delta \\kappa)$ that moves the current iterate closer to a point on the central path with a smaller barrier parameter.\n\nThe system of equations to be solved at each iteration is a linearization of the HSDE feasibility and centrality conditions:\n1.  $A(x + \\Delta x) - b(\\tau + \\Delta \\tau) = 0 \\implies A\\Delta x - b\\Delta\\tau = -r_p$\n2.  $A^\\top(y + \\Delta y) + (s + \\Delta s) - c(\\tau + \\Delta \\tau) = 0 \\implies A^\\top\\Delta y + \\Delta s - c\\Delta\\tau = -r_d$\n3.  $c^\\top(x + \\Delta x) - b^\\top(y + \\Delta y) + (\\kappa + \\Delta \\kappa) = 0 \\implies c^\\top\\Delta x - b^\\top\\Delta y + \\Delta \\kappa = -r_g$\n4.  $(X + \\Delta X)(S + \\Delta S)e = \\sigma\\mu e \\implies S\\Delta x + X\\Delta s = \\sigma\\mu e - XSe$\n5.  $(\\tau + \\Delta \\tau)(\\kappa + \\Delta \\kappa) = \\sigma\\mu \\implies \\kappa\\Delta\\tau + \\tau\\Delta\\kappa = \\sigma\\mu - \\tau\\kappa$\n\nHere, $X$ and $S$ are diagonal matrices with elements of $x$ and $s$ respectively, $e$ is a vector of ones, and $\\sigma \\in [0, 1]$ is a centering parameter that balances progress towards feasibility/optimality (`\\sigma=0`, affine-scaling step) and centrality (`\\sigma=1`, centering step).\n\nTo solve this system efficiently, we eliminate $\\Delta s$ and $\\Delta \\kappa$:\nFrom (4): $\\Delta s = X^{-1}(\\sigma\\mu e - XSe - S\\Delta x)$\nFrom (5): $\\Delta\\kappa = \\tau^{-1}(\\sigma\\mu - \\tau\\kappa - \\kappa\\Delta\\tau)$\n\nSubstituting these into the remaining equations (1, 2, 3) yields a reduced linear system for $(\\Delta x, \\Delta y, \\Delta \\tau)$. Let's form this system:\nSubstituting $\\Delta s$ into (2):\n$A^\\top\\Delta y + X^{-1}(\\sigma\\mu e - XSe - S\\Delta x) - c\\Delta\\tau = -r_d$\n$\\implies -X^{-1}S\\Delta x + A^\\top\\Delta y - c\\Delta\\tau = -r_d - X^{-1}(\\sigma\\mu e - XSe) = -r_d + s - \\sigma\\mu X^{-1}e$\n\nSubstituting $\\Delta \\kappa$ into (3):\n$c^\\top\\Delta x - b^\\top\\Delta y + \\tau^{-1}(\\sigma\\mu - \\tau\\kappa - \\kappa\\Delta\\tau) = -r_g$\n$\\implies c^\\top\\Delta x - b^\\top\\Delta y - (\\kappa/\\tau)\\Delta\\tau = -r_g - \\tau^{-1}(\\sigma\\mu - \\tau\\kappa) = -r_g - \\sigma\\mu/\\tau + \\kappa$\n\nThis gives the following $(n+m+1) \\times (n+m+1)$ linear system:\n$$\n\\begin{pmatrix}\n    -X^{-1}S & A^\\top & -c \\\\\n    A & 0 & -b \\\\\n    c^\\top & -b^\\top & -\\kappa/\\tau\n\\end{pmatrix}\n\\begin{pmatrix}\n    \\Delta x \\\\ \\Delta y \\\\ \\Delta \\tau\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n    -r_d + s - \\sigma \\mu X^{-1} e \\\\\n    -r_p \\\\\n    -r_g - \\sigma \\mu/\\tau + \\kappa\n\\end{pmatrix}\n$$\nThis system can be solved using a standard linear solver. Once $(\\Delta x, \\Delta y, \\Delta \\tau)$ are found, $(\\Delta s, \\Delta \\kappa)$ are computed from the substitution formulas.\n\nA backtracking line search is used to find a step size $\\alpha \\in (0, 1]$ that ensures the non-negativity constraints ($x, s, \\tau, \\kappa > 0$) are strictly maintained. The new iterate is then $(x, y, s, \\tau, \\kappa) \\leftarrow (x, y, s, \\tau, \\kappa) + \\alpha (\\Delta x, \\Delta y, \\Delta s, \\Delta \\tau, \\Delta \\kappa)$.\n\nThe algorithm proceeds as follows:\n1.  Initialize with a strictly positive point, e.g., $x=e, s=e, y=0, \\tau=1, \\kappa=1$.\n2.  Iterate until the complementarity gap $\\mu < \\text{TOL}$:\n    a. Calculate residuals $r_p, r_d, r_g$.\n    b. Calculate the gap $\\mu$.\n    c. Form and solve the reduced Newton system for $(\\Delta x, \\Delta y, \\Delta \\tau)$.\n    d. Compute $\\Delta s$ and $\\Delta \\kappa$.\n    e. Find the maximum step size $\\alpha$ that maintains positivity, scaled by a factor $\\eta < 1$.\n    f. Update all variables.\n3.  Upon termination, classify the result based on the final values of $\\tau$ and $\\kappa$:\n    - If $\\tau > \\kappa$ (approximating $\\tau > 0, \\kappa \\approx 0$), the original LP is feasible-optimal (code 0).\n    - If $\\tau \\le \\kappa$ (approximating $\\tau \\approx 0, \\kappa > 0$), the original LP is infeasible.\n        - If $b^\\top y < 0$, it is primal infeasible (code 1).\n        - Otherwise, it is dual infeasible (primal unbounded, code 2).\n\nThis procedure is implemented below for the given test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define and solve the test cases using the HSDE-IPM.\n    \"\"\"\n\n    def _solve_lp_hsde(A, b, c):\n        \"\"\"\n        Solves a linear program standard form using a Homogeneous Self-Dual\n        Embedding Interior-Point Method.\n        \n        Args:\n            A (np.ndarray): The m x n constraint matrix.\n            b (np.ndarray): The m x 1 constraint vector.\n            c (np.ndarray): The n x 1 objective vector.\n            \n        Returns:\n            int: 0 for feasible-optimal, 1 for primal infeasible, \n                 2 for dual infeasible.\n        \"\"\"\n        m, n = A.shape\n\n        # --- Parameters ---\n        MAX_ITER = 100\n        TOL = 1e-9  # Tolerance for complementarity gap\n        SIGMA = 0.1  # Centering parameter\n        ETA = 0.9995 # Backtracking step-size factor\n\n        # --- Initialization ---\n        x = np.ones(n)\n        s = np.ones(n)\n        y = np.zeros(m)\n        tau = 1.0\n        kappa = 1.0\n\n        for _ in range(MAX_ITER):\n            # --- Calculate Residuals and Gap ---\n            r_p = A @ x - b * tau\n            r_d = A.T @ y + s - c * tau\n            r_g = c @ x - b @ y + kappa\n            \n            mu = (x @ s + tau * kappa) / (n + 1)\n\n            if mu < TOL:\n                break\n\n            # --- Form the Newton System M*dz = R ---\n            # Matrix M\n            M_size = n + m + 1\n            M = np.zeros((M_size, M_size))\n            \n            D_inv_sq = -s / x\n            \n            M[:n, :n] = np.diag(D_inv_sq)\n            M[:n, n:n+m] = A.T\n            M[:n, n+m] = -c\n            \n            M[n:n+m, :n] = A\n            # M[n:n+m, n:n+m] is already zero\n            M[n:n+m, n+m] = -b\n            \n            M[n+m, :n] = c\n            M[n+m, n:n+m] = -b\n            M[n+m, n+m] = -kappa / tau\n\n            # RHS vector R\n            R = np.zeros(M_size)\n            R[:n] = -r_d + s - SIGMA * mu / x\n            R[n:n+m] = -r_p\n            R[n+m] = -r_g - (SIGMA * mu / tau) + kappa\n\n            # --- Solve the linear system for the step direction ---\n            try:\n                # Solve for primary step components\n                sol = np.linalg.solve(M, R)\n                dx = sol[:n]\n                dy = sol[n:n+m]\n                dtau = sol[n+m]\n                \n                # Compute remaining step components\n                ds = (SIGMA * mu - x * s - s * dx) / x\n                dkappa = (SIGMA * mu - tau * kappa - kappa * dtau) / tau\n            except np.linalg.LinAlgError:\n                # If matrix is singular, likely numerical issues. Stop iterating.\n                break\n\n            # --- Backtracking Line Search ---\n            alpha = 1.0\n            \n            neg_dx_indices = dx < -1e-12\n            if np.any(neg_dx_indices):\n                alpha = min(alpha, np.min(-x[neg_dx_indices] / dx[neg_dx_indices]))\n            \n            neg_ds_indices = ds < -1e-12\n            if np.any(neg_ds_indices):\n                alpha = min(alpha, np.min(-s[neg_ds_indices] / ds[neg_ds_indices]))\n\n            if dtau < -1e-12:\n                alpha = min(alpha, -tau / dtau)\n            \n            if dkappa < -1e-12:\n                alpha = min(alpha, -kappa / dkappa)\n\n            alpha *= ETA\n            \n            # --- Update variables ---\n            x += alpha * dx\n            y += alpha * dy\n            s += alpha * ds\n            tau += alpha * dtau\n            kappa += alpha * dkappa\n\n        # --- Classification ---\n        if tau > kappa:\n            # Feasible-Optimal case: tau > 0, kappa ~ 0\n            return 0\n        else:\n            # Infeasible case: tau ~ 0, kappa > 0\n            # A certificate for primal infeasibility is y s.t. A'y >= 0 and b'y < 0\n            if b @ y < 0:\n                return 1 # Primal Infeasible\n            else:\n                # Otherwise, it must be dual infeasible (primal unbounded)\n                # A certificate is x s.t. Ax = 0, x >= 0, c'x < 0\n                return 2 # Dual Infeasible\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Feasible-optimal\n        (\n            np.array([[1.0, 2.0], [1.0, -1.0]]),\n            np.array([4.0, 1.0]),\n            np.array([1.0, 2.0])\n        ),\n        # Case 2: Primal infeasible\n        (\n            np.array([[1.0, 0.0]]),\n            np.array([-1.0]),\n            np.array([1.0, 1.0])\n        ),\n        # Case 3: Dual infeasible (primal unbounded)\n        (\n            np.array([[1.0, -1.0]]),\n            np.array([0.0]),\n            np.array([-1.0, -1.0])\n        )\n    ]\n\n    results = []\n    for case in test_cases:\n        A, b, c = case\n        result = _solve_lp_hsde(A, b, c)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3242648"}]}