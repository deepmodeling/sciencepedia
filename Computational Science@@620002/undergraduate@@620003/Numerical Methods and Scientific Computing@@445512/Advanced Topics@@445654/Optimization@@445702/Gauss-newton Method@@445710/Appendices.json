{"hands_on_practices": [{"introduction": "Let's begin by putting the theory into practice with a single, concrete step of the Gauss-Newton method. This exercise is designed to solidify your understanding of the core components of the algorithm: calculating the Jacobian matrix and the residual vector to compute the parameter update. By working through one iteration by hand for a simple one-parameter model, you will gain a tangible feel for how the method linearizes the problem to find a search direction [@problem_id:2214282].", "problem": "In an experimental study, a certain physical process is modeled by the function $y(x) = \\frac{x}{1+ax}$, where $a$ is an unknown parameter to be determined. A researcher has collected two data points $(x_i, y_i)$: the first point is $(1, 0.5)$ and the second point is $(2, 0.8)$.\n\nTo find the optimal value of the parameter $a$ that best fits the data in a least-squares sense, the researcher decides to use the Gauss-Newton method. Starting with an initial guess of $a_0 = 1$, perform exactly one iteration of the Gauss-Newton method to find the updated estimate for the parameter, denoted as $a_1$.\n\nExpress your answer for $a_1$ as an exact fraction in simplest form.", "solution": "We use the Gauss-Newton method to find an update step $\\Delta a$ that moves our parameter estimate from the initial guess $a_0$ to a new estimate $a_1 = a_0 + \\Delta a$. The update step is found by solving the normal equations:\n$$\n(J^{\\top}J)\\Delta a = J^{\\top}r\n$$\nHere, $r$ is the vector of residuals, $r_i = y_i - f(x_i; a)$, and $J$ is the Jacobian of the model function $f(x;a)$ with respect to the parameter $a$. All quantities are evaluated at the current guess, $a_0=1$.\n\nThe model function is $f(x;a)=\\dfrac{x}{1+a x}$. First, we compute its derivative with respect to $a$:\n$$\n\\frac{\\partial f}{\\partial a} = \\frac{\\partial}{\\partial a} \\left[x(1+a x)^{-1}\\right] = x \\cdot (-1)(1+a x)^{-2} \\cdot x = -x^{2}(1+a x)^{-2}\n$$\nThe Jacobian $J$ is a column vector whose entries are these derivatives evaluated at each data point $x_i$ and at $a_0=1$. With data points $(x_{1},y_{1})=(1, 1/2)$ and $(x_{2},y_{2})=(2, 4/5)$:\n$$\nJ_1 = \\left. \\frac{\\partial f}{\\partial a} \\right|_{x_1=1, a_0=1} = -\\frac{1^2}{(1+1\\cdot 1)^2} = -\\frac{1}{4}\n$$\n$$\nJ_2 = \\left. \\frac{\\partial f}{\\partial a} \\right|_{x_2=2, a_0=1} = -\\frac{2^2}{(1+1\\cdot 2)^2} = -\\frac{4}{9}\n$$\nSo, the Jacobian vector is $J = \\begin{pmatrix} -1/4 \\\\ -4/9 \\end{pmatrix}$.\n\nNext, we compute the residuals at $a_0=1$:\n$$\nr_1 = y_1 - f(1;1) = \\frac{1}{2} - \\frac{1}{1+1} = \\frac{1}{2} - \\frac{1}{2} = 0\n$$\n$$\nr_2 = y_2 - f(2;1) = \\frac{4}{5} - \\frac{2}{1+2} = \\frac{4}{5} - \\frac{2}{3} = \\frac{12-10}{15} = \\frac{2}{15}\n$$\nSo, the residual vector is $r = \\begin{pmatrix} 0 \\\\ 2/15 \\end{pmatrix}$.\n\nNow we compute the scalar quantities $J^{\\top}J$ and $J^{\\top}r$:\n$$\nJ^{\\top}J = J_1^2 + J_2^2 = \\left(-\\frac{1}{4}\\right)^2 + \\left(-\\frac{4}{9}\\right)^2 = \\frac{1}{16} + \\frac{16}{81} = \\frac{81+256}{1296} = \\frac{337}{1296}\n$$\n$$\nJ^{\\top}r = J_1 r_1 + J_2 r_2 = \\left(-\\frac{1}{4}\\right)(0) + \\left(-\\frac{4}{9}\\right)\\left(\\frac{2}{15}\\right) = -\\frac{8}{135}\n$$\nWe solve the normal equation $\\frac{337}{1296} \\Delta a = -\\frac{8}{135}$ for $\\Delta a$:\n$$\n\\Delta a = \\left(-\\frac{8}{135}\\right) \\cdot \\left(\\frac{1296}{337}\\right) = -\\frac{8 \\cdot (1296/135)}{337} = -\\frac{8 \\cdot (48/5)}{337} = -\\frac{384}{1685}\n$$\nFinally, the updated estimate is:\n$$\na_1 = a_0 + \\Delta a = 1 - \\frac{384}{1685} = \\frac{1685-384}{1685} = \\frac{1301}{1685}\n$$", "answer": "$$\\boxed{\\frac{1301}{1685}}$$", "id": "2214282"}, {"introduction": "While the Gauss-Newton method is powerful, it can become unstable when the model parameters are difficult to distinguish from one another, a problem known as ill-conditioning. This practice explores a classic scenario where the Jacobian matrix, $J$, is nearly rank-deficient, causing the standard algorithm to produce erratic updates [@problem_id:3232838]. By analyzing the system using the Singular Value Decomposition (SVD), you will learn to diagnose this instability and understand how small amounts of noise can be dramatically amplified in certain parameter directions.", "problem": "Consider a nonlinear least-squares data-fitting problem with measurements $\\{(x_i, y_i)\\}_{i=1}^{m}$, where each $x_i \\in \\mathbb{R}$ and each $y_i \\in \\mathbb{R}$ is noisy. The model is\n$$\nf(x; \\theta) = a\\,e^{-x} + b\\,e^{-(1+\\epsilon)x},\n$$\nwith parameters $\\theta = (a,b)^\\top$, and a small constant $\\epsilon > 0$. Let the residual vector at a current iterate $\\theta^{(k)}$ be $r(\\theta^{(k)}) \\in \\mathbb{R}^m$, with entries $r_i(\\theta^{(k)}) = y_i - f(x_i; \\theta^{(k)})$. The Gauss-Newton method constructs a local linear approximation to the new residuals by $r(\\theta^{(k)} + \\Delta \\theta) \\approx r(\\theta^{(k)}) - J(\\theta^{(k)})\\,\\Delta \\theta$, where $J(\\theta^{(k)}) \\in \\mathbb{R}^{m\\times 2}$ is the Jacobian matrix of the model function $f$ with respect to $\\theta$. Specifically, the $i$th row of $J(\\theta^{(k)})$ is\n$$\n\\left[\\frac{\\partial f(x_i;\\theta)}{\\partial a}\\ \\ \\frac{\\partial f(x_i;\\theta)}{\\partial b}\\right]_{\\theta=\\theta^{(k)}} = \\left[e^{-x_i}\\ \\ e^{-(1+\\epsilon)x_i}\\right].\n$$\nThe Gauss-Newton update $\\Delta \\theta^{(k)}$ is defined as the minimizer of the linearized least-squares problem $\\min_{\\Delta \\theta}\\ \\|r(\\theta^{(k)}) - J(\\theta^{(k)})\\,\\Delta \\theta\\|_2^2$, which, under full column rank of $J(\\theta^{(k)})$, is equivalently given by the normal equations $J(\\theta^{(k)})^\\top J(\\theta^{(k)})\\,\\Delta \\theta^{(k)} = J(\\theta^{(k)})^\\top r(\\theta^{(k)})$.\n\nAssume that $\\epsilon$ is small (for instance, $\\epsilon \\ll 1$), and that the abscissas $\\{x_i\\}$ are spread over a moderate interval of positive values (for instance, $x_i \\in [0, X]$ with $X > 0$), so that the two columns of $J(\\theta^{(k)})$ are nearly collinear. Use a first-order Taylor expansion to explain why near-collinearity arises. Then, to analyze the Gauss-Newton update under near rank-deficiency, consider the Singular Value Decomposition (SVD), namely $J(\\theta^{(k)}) = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{m\\times m}$ and $V \\in \\mathbb{R}^{2\\times 2}$ are orthogonal matrices and $\\Sigma \\in \\mathbb{R}^{m\\times 2}$ contains the singular values $\\sigma_1 \\ge \\sigma_2 \\ge 0$. Express the Gauss-Newton update in the basis of the right singular vectors of $J(\\theta^{(k)})$, and elucidate the role of $\\sigma_2$ when $\\sigma_2 \\ll \\sigma_1$.\n\nWhich of the following statements accurately characterize the effect of near-collinearity (near rank-deficiency) of $J(\\theta^{(k)})$ on the Gauss-Newton update?\n\nSelect all that apply.\n\nA. If the residual $r(\\theta^{(k)})$ has a nonzero component along the left singular vector associated with the small singular value $\\sigma_2$, then the component of $\\Delta \\theta^{(k)}$ along the corresponding right singular vector can be very large, making the update highly sensitive to noise; damping or Tikhonov regularization can reduce this amplification.\n\nB. Near collinearity guarantees that the Gauss-Newton step is small in all directions, thereby improving stability and ensuring rapid convergence.\n\nC. The near rank-deficiency primarily affects only the data space; the parameter update remains well-conditioned because $V$ is orthonormal.\n\nD. Reparameterizing the problem using $\\theta_+ = a + b$ and $\\theta_- = a - b$ can improve conditioning by aligning the well-determined and ill-determined directions with the dominant and subdominant singular vectors, respectively, thereby mitigating amplification in the update.\n\nE. Using the Moore-Penrose pseudoinverse eliminates large coefficients in ill-conditioned directions without any need for damping, because it produces the minimal-norm solution.", "solution": "The problem describes ill-conditioning in a Gauss-Newton context. The two columns of the Jacobian matrix $J$ are $j_1$ with entries $e^{-x_i}$ and $j_2$ with entries $e^{-(1+\\epsilon)x_i}$. For a small $\\epsilon$, a first-order Taylor expansion gives $e^{-(1+\\epsilon)x_i} = e^{-x_i}e^{-\\epsilon x_i} \\approx e^{-x_i}(1-\\epsilon x_i)$. This shows that the second column $j_2$ is nearly a multiple of the first column $j_1$, meaning they are nearly collinear. This near-collinearity makes the Jacobian matrix $J$ nearly rank-deficient, which means its second singular value $\\sigma_2$ will be much smaller than its first, $\\sigma_2 \\ll \\sigma_1$.\n\nThe Gauss-Newton update $\\Delta\\theta$ is the solution to the normal equations $J^\\top J \\Delta\\theta = J^\\top r$. This can be expressed using the Moore-Penrose pseudoinverse as $\\Delta\\theta = J^\\dagger r$. Using the SVD, $J=U\\Sigma V^\\top$, the pseudoinverse is $J^\\dagger = V\\Sigma^\\dagger U^\\top$. The update becomes:\n$$ \\Delta\\theta = V \\Sigma^\\dagger U^\\top r $$\nExpanding this in terms of the singular vectors ($v_1, v_2$ for $V$ and $u_1, u_2, \\dots$ for $U$) and singular values ($\\sigma_1, \\sigma_2$):\n$$ \\Delta\\theta = V \\begin{pmatrix} (u_1^\\top r) / \\sigma_1 \\\\ (u_2^\\top r) / \\sigma_2 \\end{pmatrix} = \\frac{u_1^\\top r}{\\sigma_1} v_1 + \\frac{u_2^\\top r}{\\sigma_2} v_2 $$\nThis equation is key to evaluating the options.\n\n**Option A: Correct.** The equation shows that the component of the update $\\Delta\\theta$ in the direction of the right singular vector $v_2$ is $\\frac{u_2^\\top r}{\\sigma_2}$. Since $\\sigma_2$ is very small, even a small projection of the residual vector $r$ onto the corresponding left singular vector $u_2$ (i.e., $u_2^\\top r \\neq 0$) will be massively amplified. Because the residual $r$ contains noise from the data, this makes the update step extremely sensitive to that noise. Tikhonov regularization (damping) modifies the update to effectively replace the $1/\\sigma_i$ terms with $\\sigma_i/(\\sigma_i^2+\\lambda^2)$, which remains small for small $\\sigma_i$, thus taming the amplification.\n\n**Option B: Incorrect.** Near-collinearity leads to a very large step component in the ill-conditioned direction ($v_2$), causing instability and divergence, not stability and convergence.\n\n**Option C: Incorrect.** The near rank-deficiency of $J$ is precisely what causes the parameter update to become ill-conditioned. The effect is directly transmitted from the data space (via $J$) to the parameter space. The orthonormality of $V$ is what allows for this clear analysis, but it does not prevent the problem.\n\n**Option D: Correct.** This reparameterization is a standard technique. The new parameters $\\theta_+$ and $\\theta_-$ are associated with new basis functions $B_+ \\approx e^{-x}$ and $B_- \\approx e^{-x}(\\epsilon x/2)$, respectively. The new Jacobian columns are now nearly orthogonal, which decouples the estimation problem. The ill-conditioning is now isolated to the estimation of the small parameter $\\theta_-$, while the estimate for the well-determined parameter $\\theta_+$ remains stable. This mitigates the problem of instability corrupting the entire solution vector.\n\n**Option E: Incorrect.** The standard Gauss-Newton update, solved via the normal equations, *is* the Moore-Penrose pseudoinverse solution. As the analysis shows, this is precisely the solution that suffers from the amplification of noise. To fix this, one must modify the pseudoinverse, for example, by using Tikhonov regularization or a truncated SVD, which are no longer the minimal-norm least-squares solution to the original linearized problem.", "answer": "$$\\boxed{AD}$$", "id": "3232838"}, {"introduction": "Having learned to diagnose ill-conditioning, the next logical step is to fix it. This practice introduces Tikhonov regularization, a fundamental technique for stabilizing the solution of ill-posed problems. You will derive the modified Gauss-Newton update that incorporates a penalty term and then implement the algorithm to see firsthand how it tames the instability observed in problems with nearly collinear model parameters [@problem_id:3232783].", "problem": "Consider the unconstrained nonlinear least squares problem with a parameter vector $\\theta \\in \\mathbb{R}^p$ and residual vector $r(\\theta) \\in \\mathbb{R}^m$, where each component $r_i(\\theta)$ is a smooth function of $\\theta$. Augment the objective with Tikhonov regularization to obtain the penalized objective $\\phi(\\theta) = \\tfrac{1}{2}\\|r(\\theta)\\|_2^2 + \\tfrac{1}{2}\\lambda\\|\\theta\\|_2^2$, where $\\lambda \\ge 0$ is a fixed regularization parameter. Starting from the definitions of nonlinear least squares and Tikhonov regularization, and using first-order Taylor linearization of the residual $r(\\theta + \\Delta)$ around a current iterate $\\theta$, derive the Gauss-Newton (GN) step for $\\Delta$ that results from minimizing the quadratic approximation of $\\phi(\\theta + \\Delta)$ with respect to $\\Delta$. Explain each step of the derivation, including how the Jacobian matrix $J(\\theta) = \\frac{\\partial r(\\theta)}{\\partial \\theta}$ enters, and justify why the resulting linear system is better conditioned when $\\lambda > 0$ for ill-posed problems.\n\nThen, implement a GN algorithm that uses this regularized step to update $\\theta \\leftarrow \\theta + \\Delta$ until convergence, where convergence is declared when the step norm $\\|\\Delta\\|_2$ is below a tolerance. For concreteness, adopt the parametric model $F(x;\\theta) = \\theta_1 \\exp(\\theta_2 x)$, residuals $r_i(\\theta) = F(x_i;\\theta) - y_i$, and the Jacobian of the model function $F(\\theta)$ whose $i$-th row contains the partial derivatives with respect to $\\theta_1$ and $\\theta_2$, corresponding to the given $x_i$ values.\n\nYour program must implement this algorithm and evaluate it on the following test suite of parameter sets. In all cases, angles do not appear, and no physical units are involved. The program must compute, for each test case, the Euclidean norm $\\|r(\\theta^\\star)\\|_2$ of the final residual vector at convergence, expressed as a floating-point number.\n\nTest Suite:\n- Case 1 (well-posed, boundary $\\lambda = 0$): $x = [0.0, 0.11111111, 0.22222222, 0.33333333, 0.44444444, 0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.0]$, $\\theta_{\\text{true}} = [2.0, -0.5]$, $y_i = \\theta_{1,\\text{true}} \\exp(\\theta_{2,\\text{true}} x_i)$, initial guess $\\theta^{(0)} = [1.0, 0.0]$, regularization parameter $\\lambda = 0.0$.\n- Case 2 (ill-posed, identifiability failure when $x=0$): $x = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]$, $\\theta_{\\text{true}} = [1.5, 0.3]$, $y_i = \\theta_{1,\\text{true}} \\exp(\\theta_{2,\\text{true}} x_i)$ (constant values), initial guess $\\theta^{(0)} = [1.0, 1.0]$, regularization parameter $\\lambda = 0.5$.\n- Case 3 (ill-conditioned due to nearly collinear Jacobian columns): $x = [0.0, 0.004, 0.008, 0.012, 0.016, 0.02]$, $\\theta_{\\text{true}} = [2.0, 5.0]$, $y_i = \\theta_{1,\\text{true}} \\exp(\\theta_{2,\\text{true}} x_i)$, initial guess $\\theta^{(0)} = [1.0, 0.0]$, regularization parameter $\\lambda = 0.001$.\n\nAlgorithmic Details:\n- Use the residual $r(\\theta)$ and Jacobian $J(\\theta)$ corresponding to the model $F(x;\\theta) = \\theta_1 \\exp(\\theta_2 x)$. The problem states that the residual is $r_i(\\theta) = F(x_i;\\theta) - y_i$ and the Jacobian $J$ is of the model function $F$, not the residual.\n- At each iteration, update $\\theta$ by solving a linear system for the GN step $\\Delta$ derived from the quadratic approximation and move to $\\theta \\leftarrow \\theta + \\Delta$.\n- Terminate when $\\|\\Delta\\|_2 \\le \\text{tol}$ or when a maximum number of iterations is reached. Use a tolerance $\\text{tol} = 10^{-10}$ and a maximum of $50$ iterations.\n\nRequired Final Output Format:\nYour program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets, in the order Case 1, Case 2, Case 3. For example, the format must be exactly like $[r_1,r_2,r_3]$, where each $r_k$ is the floating-point value of $\\|r(\\theta^\\star)\\|_2$ for the $k$-th case.", "solution": "### Derivation of the Regularized Gauss-Newton Step\n\nThe objective is to find a parameter vector $\\theta \\in \\mathbb{R}^p$ that minimizes the penalized objective function $\\phi(\\theta)$, which consists of a sum-of-squares error term and a Tikhonov regularization term:\n$$ \\phi(\\theta) = \\frac{1}{2}\\|r(\\theta)\\|_2^2 + \\frac{1}{2}\\lambda\\|\\theta\\|_2^2 $$\nHere, $r(\\theta) \\in \\mathbb{R}^m$ is the residual vector, and $\\lambda \\ge 0$ is the regularization parameter. The Gauss-Newton method is an iterative algorithm that, at each step $k$, finds an update $\\Delta$ to move from the current estimate $\\theta^{(k)}$ to a new estimate $\\theta^{(k+1)} = \\theta^{(k)} + \\Delta$. For simplicity, we will drop the superscript $(k)$ and denote the current estimate as $\\theta$.\n\nThe step $\\Delta$ is found by minimizing a quadratic approximation of the objective function at $\\theta + \\Delta$. The problem defines the residuals as $r_i(\\theta) = F(x_i; \\theta) - y_i$. To obtain the Gauss-Newton approximation, we linearize the model function $F(\\theta + \\Delta)$ (or equivalently the residual function $r(\\theta + \\Delta)$) using its first-order Taylor expansion around $\\theta$:\n$$ r(\\theta + \\Delta) \\approx r(\\theta) + J(\\theta)\\Delta $$\nwhere $J(\\theta)$ is the $m \\times p$ Jacobian matrix of the model function $F$, with entries $J_{ij} = \\frac{\\partial F_i}{\\partial \\theta_j}$.\n\nWe substitute this approximation into the penalized objective function $\\phi(\\theta + \\Delta)$. This yields a quadratic model for $\\phi(\\theta+\\Delta)$, which we denote as $\\Phi(\\Delta)$:\n$$ \\Phi(\\Delta) = \\frac{1}{2} \\|r(\\theta) + J(\\theta)\\Delta\\|_2^2 + \\frac{1}{2}\\lambda\\|\\theta + \\Delta\\|_2^2 $$\nTo find the step $\\Delta$ that minimizes this function, we expand the terms. Using vector notation where $r \\equiv r(\\theta)$ and $J \\equiv J(\\theta)$:\n$$ \\Phi(\\Delta) = \\frac{1}{2} (r + J\\Delta)^T(r + J\\Delta) + \\frac{1}{2}\\lambda (\\theta + \\Delta)^T(\\theta + \\Delta) $$\nExpanding the products gives:\n$$ \\Phi(\\Delta) = \\frac{1}{2} (r^T r + 2r^T J\\Delta + \\Delta^T J^T J\\Delta) + \\frac{1}{2}\\lambda (\\theta^T\\theta + 2\\theta^T\\Delta + \\Delta^T\\Delta) $$\nThis expression is a quadratic function of $\\Delta$. To find its minimum, we compute its gradient with respect to $\\Delta$ and set it to zero. Using the matrix calculus identities $\\nabla_x(a^Tx) = a$ and $\\nabla_x(x^TAx) = (A+A^T)x$:\n$$ \\nabla_{\\Delta} \\Phi(\\Delta) = (J^T r + J^T J\\Delta) + \\lambda(\\theta + I\\Delta) $$\nSetting the gradient to zero, $\\nabla_{\\Delta} \\Phi(\\Delta) = 0$:\n$$ J^T J\\Delta + J^T r + \\lambda\\theta + \\lambda I\\Delta = 0 $$\nRearranging the terms to solve for $\\Delta$, we group the terms involving $\\Delta$:\n$$ (J^T J + \\lambda I)\\Delta = -J^T r - \\lambda\\theta $$\nThis is the linear system for the regularized Gauss-Newton step $\\Delta$.\n\n### Justification for Regularization in Ill-Posed Problems\n\nThe standard, unregularized Gauss-Newton step is found by solving the normal equations, which are obtained by setting $\\lambda=0$ in the derived system. However, for many problems, the matrix $J^T J$ is singular or ill-conditioned.\n\n1.  **Singularity**: If the columns of the Jacobian matrix $J$ are linearly dependent, $J$ is rank-deficient. This implies that $J^T J$ is singular (not invertible). In this case, the linear system for $\\Delta$ does not have a unique solution.\n\n2.  **Ill-Conditioning**: If the columns of $J$ are nearly linearly dependent, the matrix $J^T J$ is ill-conditioned. Its condition number is very large, making the solution for $\\Delta$ extremely sensitive to small perturbations, leading to unstable updates.\n\nThe Tikhonov regularization term $\\lambda I$ mitigates these issues. The matrix to be inverted is $A_\\lambda = J^T J + \\lambda I$.\nThe matrix $J^T J$ is always symmetric and positive semi-definite, so its eigenvalues, $\\mu_j$, are all non-negative ($\\mu_j \\ge 0$). If $\\lambda > 0$, the matrix $A_\\lambda = J^T J + \\lambda I$ has eigenvalues $\\nu_j = \\mu_j + \\lambda$. Since $\\lambda > 0$, all $\\nu_j > 0$. This guarantees that $A_\\lambda$ is strictly positive definite and therefore always invertible, ensuring a unique solution for $\\Delta$ exists. Furthermore, the condition number of $A_\\lambda$ is $\\kappa(A_\\lambda) = (\\mu_{\\max}+\\lambda)/(\\mu_{\\min}+\\lambda)$. Even if $\\mu_{\\min}$ is zero or very close to zero, the denominator is bounded below by $\\lambda$. This prevents the condition number from becoming excessively large, stabilizing the solution.\n\n### Algorithm and Model Specification\n\nThe algorithm iteratively solves for $\\Delta$ and updates $\\theta \\leftarrow \\theta + \\Delta$.\n- **Model**: $F(x;\\theta) = \\theta_1 \\exp(\\theta_2 x)$ with $\\theta = [\\theta_1, \\theta_2]^T$.\n- **Residuals**: $r_i(\\theta) = F(x_i; \\theta) - y_i = \\theta_1 \\exp(\\theta_2 x_i) - y_i$.\n- **Jacobian**: The $i$-th row of the model Jacobian $J(\\theta)$ is $[\\frac{\\partial F_i}{\\partial \\theta_1}, \\frac{\\partial F_i}{\\partial \\theta_2}]$.\n  $$ \\frac{\\partial F_i}{\\partial \\theta_1} = \\exp(\\theta_2 x_i) $$\n  $$ \\frac{\\partial F_i}{\\partial \\theta_2} = \\theta_1 x_i \\exp(\\theta_2 x_i) $$\n  So, the $i$-th row is $[\\exp(\\theta_2 x_i), \\theta_1 x_i \\exp(\\theta_2 x_i)]$.\n- **Termination**: The iteration stops when $\\|\\Delta\\|_2 \\le 10^{-10}$ or after $50$ iterations.", "answer": "```python\nimport numpy as np\n\ndef run_gauss_newton(x, y, theta0, lam, tol, max_iter):\n    \"\"\"\n    Implements the regularized Gauss-Newton algorithm for the model F(theta) = theta1 * exp(theta2 * x).\n    The linear system solved is (J^T J + lambda*I) * delta = -(J^T r + lambda*theta),\n    where r = F(theta) - y.\n    \"\"\"\n    theta = np.array(theta0, dtype=float)\n    p = len(theta0) # Number of parameters\n\n    for _ in range(max_iter):\n        # Unpack parameters for clarity\n        theta1, theta2 = theta\n\n        # 1. Calculate model output and residual vector r = F(theta) - y\n        model_y = theta1 * np.exp(theta2 * x)\n        r = model_y - y\n\n        # 2. Calculate Jacobian matrix J of the model function F\n        J1 = np.exp(theta2 * x) # dF/d(theta1)\n        J2 = theta1 * x * np.exp(theta2 * x) # dF/d(theta2)\n        J = np.stack((J1, J2), axis=1)\n\n        # 3. Form and solve the linear system for the step delta\n        JtJ = J.T @ J\n        I = np.identity(p)\n        A = JtJ + lam * I\n\n        Jtr = J.T @ r\n        b = -(Jtr + lam * theta)\n        \n        try:\n            delta = np.linalg.solve(A, b)\n        except np.linalg.LinAlgError:\n            # If the matrix is singular, terminate.\n            # This can happen in the unregularized case (lambda=0).\n            break\n            \n        # 4. Update the parameters\n        theta = theta + delta\n\n        # 5. Check for convergence\n        step_norm = np.linalg.norm(delta)\n        if step_norm = tol:\n            break\n\n    # After loop, calculate final residual norm\n    final_model_y = theta[0] * np.exp(theta[1] * x)\n    final_r = final_model_y - y\n    final_residual_norm = np.linalg.norm(final_r)\n    \n    return final_residual_norm\n\ndef solve():\n    \"\"\"\n    Sets up and runs the test cases for the regularized Gauss-Newton algorithm.\n    \"\"\"\n    # Algorithmic parameters\n    tol = 1e-10\n    max_iter = 50\n\n    # Test Case 1: well-posed, lambda = 0\n    x1 = np.linspace(0.0, 1.0, 10)\n    theta_true1 = np.array([2.0, -0.5])\n    y1 = theta_true1[0] * np.exp(theta_true1[1] * x1)\n    theta0_1 = [1.0, 0.0]\n    lam1 = 0.0\n    result1 = run_gauss_newton(x1, y1, theta0_1, lam1, tol, max_iter)\n\n    # Test Case 2: ill-posed, non-identifiability\n    x2 = np.zeros(6)\n    theta_true2 = np.array([1.5, 0.3])\n    y2 = theta_true2[0] * np.exp(theta_true2[1] * x2)\n    theta0_2 = [1.0, 1.0]\n    lam2 = 0.5\n    result2 = run_gauss_newton(x2, y2, theta0_2, lam2, tol, max_iter)\n\n    # Test Case 3: ill-conditioned, near-collinear Jacobian\n    x3 = np.arange(6, dtype=float) * 0.004\n    theta_true3 = np.array([2.0, 5.0])\n    y3 = theta_true3[0] * np.exp(theta_true3[1] * x3)\n    theta0_3 = [1.0, 0.0]\n    lam3 = 0.001\n    result3 = run_gauss_newton(x3, y3, theta0_3, lam3, tol, max_iter)\n\n    results = [result1, result2, result3]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3232783"}]}