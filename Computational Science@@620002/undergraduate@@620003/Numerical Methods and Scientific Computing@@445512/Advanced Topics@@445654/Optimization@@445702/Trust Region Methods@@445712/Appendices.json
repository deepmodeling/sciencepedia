{"hands_on_practices": [{"introduction": "This first exercise goes to the core of the trust-region subproblem for the case where the model Hessian, $H_k$, is positive definite. You will use foundational principles of constrained optimization to derive the \"secular equation,\" a key result that characterizes the optimal step when it lies on the boundary of the trust region. This practice reveals the elegant mathematical structure that transforms the multi-dimensional subproblem into a tractable one-dimensional root-finding problem, a cornerstone of many practical TR solvers. [@problem_id:3284949]", "problem": "Consider the trust-region subproblem with a symmetric positive definite (SPD) matrix $H \\in \\mathbb{R}^{n \\times n}$, gradient $g \\in \\mathbb{R}^{n}$, and trust-region radius $\\Delta  0$: minimize the quadratic model $m(p) = g^{\\mathsf{T}} p + \\tfrac{1}{2} p^{\\mathsf{T}} H p$ subject to the Euclidean norm constraint $\\|p\\| \\le \\Delta$. Starting from first principles (namely, the definition of the optimization problem and the Karush–Kuhn–Tucker conditions for constrained optimization), derive a scalar equation in a single variable that characterizes boundary solutions where $\\|p\\| = \\Delta$ in the SPD case. Then, using only fundamental linear algebra facts and properties of SPD matrices, justify why this scalar equation admits exactly one solution for $\\lambda \\ge 0$ and explain why the corresponding residual function is strictly decreasing.\n\nNext, outline an efficient root-finding strategy for computing the unique Lagrange multiplier $\\lambda \\ge 0$ that solves the scalar equation. Your outline should specify:\n- how to evaluate the residual and its derivative using linear solves with $H + \\lambda I$,\n- a safe globalization strategy to guarantee convergence, and\n- how to obtain an initial interval or iterate that leads to rapid convergence in practice.\n\nFinally, for the specific instance with\n- $H = \\mathrm{diag}(3, 1)$,\n- $g = \\begin{pmatrix} 10 \\\\ 0 \\end{pmatrix}$,\n- $\\Delta = 2$,\ncompute the exact value of the unique $\\lambda \\ge 0$ that satisfies the boundary condition. Provide the exact value of $\\lambda$ as your final answer. Do not round.", "solution": "The problem is subjected to validation and is found to be valid. It is a well-posed, scientifically grounded problem from the field of numerical optimization, with all necessary data and definitions provided.\n\nThe trust-region subproblem is to find a step $p \\in \\mathbb{R}^{n}$ that minimizes a quadratic model of a function, subject to a constraint on the step length. The problem is formulated as:\n$$\n\\min_{p \\in \\mathbb{R}^{n}} m(p) = g^{\\mathsf{T}} p + \\frac{1}{2} p^{\\mathsf{T}} H p \\quad \\text{subject to} \\quad \\|p\\| \\le \\Delta\n$$\nwhere $H \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite (SPD) matrix, $g \\in \\mathbb{R}^{n}$ is a gradient vector, and $\\Delta  0$ is the trust-region radius. The constraint is on the Euclidean norm, $\\|p\\| = \\sqrt{p^{\\mathsf{T}}p}$.\n\nTo derive the characterizing equation for a solution on the boundary of the trust region, we employ the Karush–Kuhn–Tucker (KKT) conditions for constrained optimization. The inequality constraint is $g_c(p) = \\|p\\|^2 - \\Delta^2 \\le 0$. The Lagrangian function is:\n$$\nL(p, \\lambda) = m(p) + \\frac{\\lambda}{2} g_c(p) = g^{\\mathsf{T}} p + \\frac{1}{2} p^{\\mathsf{T}} H p + \\frac{\\lambda}{2} (p^{\\mathsf{T}}p - \\Delta^2)\n$$\nwhere $\\lambda$ is the Lagrange multiplier. The factor of $\\frac{1}{2}$ is introduced for convenience. The KKT conditions for a minimizer $p^*$ are:\n1.  Stationarity: $\\nabla_p L(p^*, \\lambda) = g + H p^* + \\lambda p^* = 0$.\n2.  Primal Feasibility: $\\|p^*\\|^2 \\le \\Delta^2$.\n3.  Dual Feasibility: $\\lambda \\ge 0$.\n4.  Complementary Slackness: $\\lambda (\\|p^*\\|^2 - \\Delta^2) = 0$.\n\nThe problem asks to characterize solutions on the boundary, where $\\|p^*\\| = \\Delta$. From the complementary slackness condition, this implies that $\\lambda$ can be non-negative, $\\lambda \\ge 0$. The stationarity condition can be rearranged as:\n$$\n(H + \\lambda I) p^* = -g\n$$\nwhere $I$ is the identity matrix. Since $H$ is SPD, its eigenvalues $\\mu_i$ are all strictly positive, $\\mu_i  0$. For any $\\lambda \\ge 0$, the eigenvalues of the matrix $(H + \\lambda I)$ are $\\mu_i + \\lambda$, which are also strictly positive. Therefore, $(H + \\lambda I)$ is SPD and, consequently, invertible for all $\\lambda \\ge 0$. We can thus uniquely express the solution $p^*$ as a function of $\\lambda$:\n$$\np(\\lambda) = -(H + \\lambda I)^{-1} g\n$$\nFor a boundary solution, this step $p(\\lambda)$ must satisfy the constraint $\\|p(\\lambda)\\| = \\Delta$. Substituting the expression for $p(\\lambda)$ gives the scalar equation in the single variable $\\lambda$:\n$$\n\\|-(H + \\lambda I)^{-1} g\\| = \\Delta\n$$\nThis is the fundamental equation, often called the secular equation, that characterizes the Lagrange multiplier $\\lambda$ for a solution on the boundary of the trust region.\n\nNext, we justify that for a boundary solution, this equation admits exactly one solution for $\\lambda \\ge 0$. Let us analyze the function $\\phi(\\lambda) = \\|p(\\lambda)\\| = \\|(H + \\lambda I)^{-1} g\\|$ for $\\lambda \\ge 0$. It is more convenient to analyze its square, $\\psi(\\lambda) = \\phi(\\lambda)^2 = \\|p(\\lambda)\\|^2$. Let $H = Q \\Lambda Q^{\\mathsf{T}}$ be the spectral decomposition of $H$, where $Q$ is an orthogonal matrix and $\\Lambda = \\mathrm{diag}(\\mu_1, \\ldots, \\mu_n)$ is the diagonal matrix of positive eigenvalues of $H$. We can write $\\psi(\\lambda)$ as:\n$$\n\\psi(\\lambda) = g^{\\mathsf{T}} (H + \\lambda I)^{-2} g = g^{\\mathsf{T}} (Q (\\Lambda + \\lambda I) Q^{\\mathsf{T}})^{-2} g = g^{\\mathsf{T}} Q (\\Lambda + \\lambda I)^{-2} Q^{\\mathsf{T}} g\n$$\nLet $\\hat{g} = Q^{\\mathsf{T}} g$. Then $\\psi(\\lambda)$ becomes:\n$$\n\\psi(\\lambda) = \\hat{g}^{\\mathsf{T}} (\\Lambda + \\lambda I)^{-2} \\hat{g} = \\sum_{i=1}^{n} \\frac{\\hat{g}_i^2}{(\\mu_i + \\lambda)^2}\n$$\nTo analyze the monotonicity of this function, we compute its derivative with respect to $\\lambda$:\n$$\n\\psi'(\\lambda) = \\sum_{i=1}^{n} \\hat{g}_i^2 \\frac{d}{d\\lambda} (\\mu_i + \\lambda)^{-2} = \\sum_{i=1}^{n} \\hat{g}_i^2 (-2)(\\mu_i + \\lambda)^{-3} (1) = -2 \\sum_{i=1}^{n} \\frac{\\hat{g}_i^2}{(\\mu_i + \\lambda)^3}\n$$\nSince $\\mu_i  0$ and we consider $\\lambda \\ge 0$, the denominator $(\\mu_i + \\lambda)^3$ is always positive. The numerators $\\hat{g}_i^2$ are non-negative. If $g \\neq 0$, then $\\hat{g} \\neq 0$, and at least one $\\hat{g}_i^2$ is positive. Thus, for $g \\neq 0$, $\\psi'(\\lambda)  0$ for all $\\lambda \\ge 0$. This proves that $\\psi(\\lambda)$ is a strictly decreasing function of $\\lambda$ on $[0, \\infty)$. Since $\\phi(\\lambda)=\\sqrt{\\psi(\\lambda)}$ and the square root function is strictly increasing for positive arguments, $\\phi(\\lambda)$ is also a strictly decreasing function of $\\lambda$ on $[0, \\infty)$.\n\nThe existence and uniqueness of a solution $\\lambda \\ge 0$ to $\\phi(\\lambda) = \\Delta$ depends on the value of $\\phi(0) = \\|-H^{-1}g\\|$, which is the norm of the unconstrained minimizer (the full Newton step).\n- If $\\| -H^{-1}g \\| \\le \\Delta$, the unconstrained solution is feasible. The KKT conditions are satisfied with $p^* = -H^{-1}g$ and $\\lambda = 0$. This is the interior solution.\n- If $\\| -H^{-1}g \\|  \\Delta$, the solution must lie on the boundary. We analyze the function $\\phi(\\lambda)$ on $[0, \\infty)$. We have $\\phi(0) = \\|-H^{-1}g\\|  \\Delta$.\nIn the limit as $\\lambda \\to \\infty$, we have $\\phi(\\lambda) = \\|-(H+\\lambda I)^{-1}g\\| \\approx \\|\\frac{-1}{\\lambda}I g\\| = \\frac{\\|g\\|}{\\lambda} \\to 0$.\nSince $\\phi(\\lambda)$ is a continuous and strictly decreasing function on $[0, \\infty)$ with $\\phi(0)  \\Delta$ and $\\lim_{\\lambda \\to \\infty} \\phi(\\lambda) = 0$, by the Intermediate Value Theorem, there must exist exactly one value $\\lambda^*  0$ such that $\\phi(\\lambda^*) = \\Delta$. Thus, in the boundary case, there is a unique positive Lagrange multiplier.\n\nAn efficient root-finding strategy for the secular equation typically uses Newton's method on a related function, for instance $r(\\lambda) = \\|p(\\lambda)\\| - \\Delta = 0$.\nThe Newton iteration is $\\lambda_{k+1} = \\lambda_k - r(\\lambda_k)/r'(\\lambda_k)$.\nTo evaluate $r(\\lambda)$ and its derivative $r'(\\lambda)$:\n1.  **Evaluation of $r(\\lambda)$**: For a given $\\lambda$, we must compute $p(\\lambda) = -(H+\\lambda I)^{-1}g$. This is done by solving the linear system $(H+\\lambda I)p = -g$. Since $(H+\\lambda I)$ is SPD, this system is efficiently solved via Cholesky decomposition. After computing $p$, we evaluate $r(\\lambda) = \\|p\\| - \\Delta$.\n2.  **Evaluation of $r'(\\lambda)$**: We must compute $\\frac{d}{d\\lambda}\\|p(\\lambda)\\|$.\n    $r'(\\lambda) = \\frac{p(\\lambda)^{\\mathsf{T}} p'(\\lambda)}{\\|p(\\lambda)\\|}$, where $p'(\\lambda) = \\frac{d p(\\lambda)}{d\\lambda}$.\n    By differentiating the equation $(H + \\lambda I) p(\\lambda) = -g$ with respect to $\\lambda$, we get:\n    $I p(\\lambda) + (H + \\lambda I) p'(\\lambda) = 0 \\implies p'(\\lambda) = -(H + \\lambda I)^{-1} p(\\lambda)$.\n    Let $q = p'(\\lambda)$. To compute $q$, we solve another linear system $(H + \\lambda I)q = -p(\\lambda)$. Note that the Cholesky factorization of $(H+\\lambda I)$ from the computation of $p(\\lambda)$ can be reused.\n    Then, $r'(\\lambda) = \\frac{p(\\lambda)^{\\mathsf{T}} q}{\\|p(\\lambda)\\|}$.\nA Newton step consists of two linear solves with the same matrix.\n- **Globalization Strategy**: Newton's method is locally quadratically convergent but may not converge globally. A safeguarding strategy is necessary. Since we know the root $\\lambda^*$ is bracketed by $[0, \\infty)$ and the function $r(\\lambda)$ is convex and monotonic, simple safeguards are effective. For example, if a Newton step $\\lambda_{k+1}$ is negative, it can be replaced by a bisection step on a known bracket, such as $[\\lambda_k/2, \\lambda_k]$.\n- **Initial Iterate**: We only need to solve the secular equation if $\\|-H^{-1}g\\|  \\Delta$. In this case, we know $\\lambda^*  0$. A common and simple starting guess is $\\lambda_0 = 0$. From there, the Newton iteration will produce a positive $\\lambda_1$. More sophisticated initial guesses can be derived from eigenvalue bounds, such as $\\lambda_0 = \\frac{\\|g\\|}{\\Delta} - \\mu_{\\min}$, though computing $\\mu_{\\min}$ can be costly. Starting with $\\lambda_0=0$ in a safeguarded scheme is robust.\n\nFinally, we compute the exact value of $\\lambda$ for the given specific instance:\n- $H = \\mathrm{diag}(3, 1)$\n- $g = \\begin{pmatrix} 10 \\\\ 0 \\end{pmatrix}$\n- $\\Delta = 2$\n\nFirst, we check if the solution is on the boundary. We compute the unconstrained minimizer $p_{unc} = -H^{-1}g$.\n$H^{-1} = \\mathrm{diag}(1/3, 1)$.\n$p_{unc} = -\\begin{pmatrix} 1/3  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 10 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -10/3 \\\\ 0 \\end{pmatrix}$.\nThe norm of this step is $\\|p_{unc}\\| = \\sqrt{(-10/3)^2 + 0^2} = 10/3$.\nSince $10/3 \\approx 3.33  \\Delta = 2$, the solution lies on the boundary, and we must find a $\\lambda  0$.\n\nWe solve the secular equation $\\|p(\\lambda)\\| = \\Delta$. The step $p(\\lambda)$ is given by:\n$p(\\lambda) = -(H + \\lambda I)^{-1} g$.\n$H + \\lambda I = \\begin{pmatrix} 3  0 \\\\ 0  1 \\end{pmatrix} + \\lambda \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 3+\\lambda  0 \\\\ 0  1+\\lambda \\end{pmatrix}$.\n$(H + \\lambda I)^{-1} = \\begin{pmatrix} \\frac{1}{3+\\lambda}  0 \\\\ 0  \\frac{1}{1+\\lambda} \\end{pmatrix}$.\n$p(\\lambda) = -\\begin{pmatrix} \\frac{1}{3+\\lambda}  0 \\\\ 0  \\frac{1}{1+\\lambda} \\end{pmatrix} \\begin{pmatrix} 10 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -\\frac{10}{3+\\lambda} \\\\ 0 \\end{pmatrix}$.\n\nNow, we enforce the boundary condition $\\|p(\\lambda)\\| = 2$:\n$\\left\\| \\begin{pmatrix} -\\frac{10}{3+\\lambda} \\\\ 0 \\end{pmatrix} \\right\\| = 2$.\n$\\sqrt{\\left(-\\frac{10}{3+\\lambda}\\right)^2 + 0^2} = 2$.\nAs we seek $\\lambda  0$, the term $3+\\lambda$ is positive, so we can write:\n$\\frac{10}{3+\\lambda} = 2$.\n$10 = 2(3+\\lambda)$.\n$10 = 6 + 2\\lambda$.\n$4 = 2\\lambda$.\n$\\lambda = 2$.\n\nThis is the unique positive Lagrange multiplier that satisfies the boundary condition.", "answer": "$$\n\\boxed{2}\n$$", "id": "3284949"}, {"introduction": "While a simple step along the negative gradient, $-g_k$, (the Cauchy point) guarantees some progress, is it always a good choice? This exercise challenges you to analyze a carefully constructed scenario where the quadratic model has negative curvature. By comparing the progress offered by the Cauchy point to that of the true optimal step, you will see why simply following the gradient can be highly inefficient and why more sophisticated methods are needed to fully exploit the model. [@problem_id:3284825]", "problem": "Consider a quadratic model arising in a Trust Region (TR) method for unconstrained optimization. At a current iterate, the local second-order model is\n$$\nm(p) \\;=\\; f(x_k) \\;+\\; g^{\\top}p \\;+\\; \\frac{1}{2}p^{\\top}B\\,p,\n$$\nwhere $p \\in \\mathbb{R}^{2}$ is the step, $g \\in \\mathbb{R}^{2}$ is the gradient at $x_k$, and $B \\in \\mathbb{R}^{2 \\times 2}$ is a symmetric Hessian approximation. The TR subproblem is to minimize $m(p)$ subject to the trust-region constraint $\\|p\\| \\leq \\Delta$. The Cauchy point is defined as the minimizer of $m(p)$ along the ray $p = -\\alpha g$ subject to $\\|p\\| \\leq \\Delta$.\n\nConstruct the specific two-dimensional instance with\n$$\ng \\;=\\; \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \n\\qquad\nB \\;=\\; \\begin{pmatrix} 1  0 \\\\ 0  -1000 \\end{pmatrix},\n\\qquad\n\\Delta \\;=\\; 1,\n$$\nand consider the corresponding TR subproblem. Using only fundamental definitions and principles of TR methods and quadratic models, determine the ratio $\\rho$ between the predicted reduction at the Cauchy point and the predicted reduction at the global solution of the TR subproblem. The predicted reduction at a step $p$ is defined as $m(0) - m(p)$.\n\nProvide your final numerical value for $\\rho$ in scientific notation rounded to four significant figures.", "solution": "The Trust Region (TR) subproblem seeks\n$$\n\\min_{p \\in \\mathbb{R}^{2}} \\;\\; m(p) \\;=\\; g^{\\top}p + \\frac{1}{2}p^{\\top}B\\,p \n\\quad \\text{subject to} \\quad \\|p\\| \\leq \\Delta,\n$$\nwhere $f(x_k)$ is constant with respect to $p$ and thus omitted from the minimization. The predicted reduction at a step $p$ is\n$$\n\\text{predicted reduction}(p) \\;=\\; m(0) - m(p) \\;=\\; -\\,g^{\\top}p \\;-\\; \\frac{1}{2}p^{\\top}B\\,p.\n$$\n\nWe are given\n$$\ng \\;=\\; \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix},\n\\qquad\nB \\;=\\; \\begin{pmatrix} 1  0 \\\\ 0  -1000 \\end{pmatrix},\n\\qquad\n\\Delta \\;=\\; 1.\n$$\nWe first compute the Cauchy point, which minimizes $m(p)$ on the ray $p=-\\alpha g$ subject to $\\|p\\| \\leq \\Delta$. Parameterize $p$ as $p(\\alpha) = -\\alpha g = \\begin{pmatrix} -\\alpha \\\\ 0 \\end{pmatrix}$ with $\\alpha \\geq 0$ and the constraint $\\|p(\\alpha)\\| = \\alpha\\|g\\| \\leq \\Delta$. Since $\\|g\\| = 1$, this is $\\alpha \\leq \\Delta = 1$.\n\nRestricting $m$ to this ray,\n$$\nm(-\\alpha g) \\;=\\; g^{\\top}(-\\alpha g) + \\frac{1}{2}(-\\alpha g)^{\\top}B(-\\alpha g).\n$$\nWe compute $g^{\\top}(-\\alpha g) = -\\alpha \\|g\\|^{2} = -\\alpha$ and\n$$\n(-\\alpha g)^{\\top}B(-\\alpha g) \\;=\\; \\alpha^{2} g^{\\top}B g \\;=\\; \\alpha^{2} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}^{\\top} \\begin{pmatrix} 1  0 \\\\ 0  -1000 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\;=\\; \\alpha^{2} \\cdot 1 \\;=\\; \\alpha^{2}.\n$$\nTherefore,\n$$\nm(-\\alpha g) \\;=\\; -\\alpha + \\frac{1}{2}\\alpha^{2}.\n$$\nThis is a one-dimensional quadratic in $\\alpha$; the unconstrained minimizer satisfies\n$$\n\\frac{d}{d\\alpha}\\left(-\\alpha + \\frac{1}{2}\\alpha^{2}\\right) \\;=\\; -1 + \\alpha \\;=\\; 0\n\\quad \\Rightarrow \\quad \\alpha^{\\star} \\;=\\; 1.\n$$\nSince the trust-region bound is $\\alpha \\leq 1$, the unconstrained minimizer lies on the boundary and is feasible. Thus, the Cauchy point is\n$$\np_{\\mathrm{C}} \\;=\\; -\\alpha^{\\star} g \\;=\\; \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}.\n$$\nThe predicted reduction at the Cauchy point is\n$$\n\\text{PR}_{\\mathrm{C}} \n\\;=\\; -\\,g^{\\top}p_{\\mathrm{C}} - \\frac{1}{2}p_{\\mathrm{C}}^{\\top}B\\,p_{\\mathrm{C}}\n\\;=\\; -\\,\\begin{pmatrix} 1  0 \\end{pmatrix}\\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}\n\\;-\\; \\frac{1}{2}\\begin{pmatrix} -1  0 \\end{pmatrix}\\begin{pmatrix} 1  0 \\\\ 0  -1000 \\end{pmatrix}\\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}.\n$$\nCompute the terms:\n$$\n-\\,g^{\\top}p_{\\mathrm{C}} \\;=\\; -(-1) \\;=\\; 1, \n\\qquad\np_{\\mathrm{C}}^{\\top}B\\,p_{\\mathrm{C}} \\;=\\; (-1)^{2}\\cdot 1 + 0^{2}\\cdot (-1000) \\;=\\; 1.\n$$\nHence,\n$$\n\\text{PR}_{\\mathrm{C}} \\;=\\; 1 - \\frac{1}{2}\\cdot 1 \\;=\\; \\frac{1}{2}.\n$$\n\nNext, we determine the global TR solution. The matrix $B$ is indefinite with eigenvalues $1$ and $-1000$. The model along the second coordinate direction has strong negative curvature. The TR subproblem minimizes\n$$\nm(p) \\;=\\; x + \\frac{1}{2}\\left(x^{2} - 1000\\,y^{2}\\right)\n$$\nover $(x,y)$ with $x^{2}+y^{2} \\leq 1$. For any feasible $(x,y)$, the term $-\\frac{1}{2}\\cdot 1000\\,y^{2}$ strictly decreases with increasing $|y|$, suggesting that the minimizer will saturate the trust-region boundary in $y$. On the boundary $x^{2}+y^{2}=1$, the term $x + \\frac{1}{2}x^{2}$ is minimized by $x=0$ because, with $y$ fixed at maximal $|y|$, the constraint forces $x=0$. Thus, a global minimizer is\n$$\np^{\\star} \\;=\\; \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n\\quad \\text{or} \\quad\np^{\\star} \\;=\\; \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix},\n$$\nboth yielding the same model value due to symmetry. We select $p^{\\star} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ for concreteness. The predicted reduction at the global TR solution is\n$$\n\\text{PR}_{\\star} \n\\;=\\; -\\,g^{\\top}p^{\\star} - \\frac{1}{2}(p^{\\star})^{\\top}B\\,p^{\\star}\n\\;=\\; -\\,\\begin{pmatrix} 1  0 \\end{pmatrix}\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n\\;-\\; \\frac{1}{2}\\begin{pmatrix} 0  1 \\end{pmatrix}\\begin{pmatrix} 1  0 \\\\ 0  -1000 \\end{pmatrix}\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}.\n$$\nCompute the terms:\n$$\n-\\,g^{\\top}p^{\\star} \\;=\\; -0 \\;=\\; 0,\n\\qquad\n(p^{\\star})^{\\top}B\\,p^{\\star} \\;=\\; 0^{2}\\cdot 1 + 1^{2}\\cdot (-1000) \\;=\\; -1000.\n$$\nTherefore,\n$$\n\\text{PR}_{\\star} \\;=\\; 0 - \\frac{1}{2}\\cdot(-1000) \\;=\\; 500.\n$$\n\nThe requested ratio $\\rho$ between the predicted reduction at the Cauchy point and at the global TR solution is\n$$\n\\rho \\;=\\; \\frac{\\text{PR}_{\\mathrm{C}}}{\\text{PR}_{\\star}} \\;=\\; \\frac{\\frac{1}{2}}{500} \\;=\\; \\frac{1}{1000} \\;=\\; 10^{-3}.\n$$\nExpressed in scientific notation rounded to four significant figures, this is\n$$\n\\rho \\;=\\; 1.000 \\times 10^{-3}.\n$$\nThis demonstrates that, in this two-dimensional instance, the TR global solution lies on the boundary and the Cauchy point yields an exceptionally poor predicted reduction relative to the optimal boundary step.", "answer": "$$\\boxed{1.000 \\times 10^{-3}}$$", "id": "3284825"}, {"introduction": "Theory is best understood when put into practice. This final exercise guides you through implementing a complete trust-region algorithm from first principles, featuring the popular dogleg method to solve the subproblem. By applying your code to the classic Rosenbrock function—a function notorious for its narrow, curved valley—and comparing its performance against a standard line-search method, you will gain firsthand experience with the practical behavior and robustness of the trust-region framework. [@problem_id:3284806]", "problem": "Consider minimizing a twice continuously differentiable function with a curved narrow valley by applying two classical algorithms from numerical optimization: a trust region method and Newton's method with line search. Use the canonical Rosenbrock function, a standard test function with a narrow curved valley, defined for $x = (x_1,x_2) \\in \\mathbb{R}^2$ by\n$$\nf(x) = (1 - x_1)^2 + 100\\,(x_2 - x_1^2)^2.\n$$\nConstruct and compare both methods from first principles using a second-order Taylor model and an Armijo backtracking condition, respectively. Track the trust region radius sequence and agreement ratio, and report summary statistics for convergence. Implement the following specifications:\n\n1. Definitions and base facts to use:\n   - The second-order Taylor model of $f$ at $x_k$ is\n     $$\n     m_k(p) = f(x_k) + g_k^\\top p + \\tfrac{1}{2} p^\\top H_k p,\n     $$\n     where $g_k = \\nabla f(x_k)$ and $H_k = \\nabla^2 f(x_k)$.\n   - The trust region subproblem is\n     $$\n     \\min_{p \\in \\mathbb{R}^2} \\ m_k(p) \\quad \\text{subject to} \\quad \\|p\\|_2 \\le \\Delta_k,\n     $$\n     where $\\Delta_k  0$ is the trust region radius. Use the dogleg strategy when $H_k$ is symmetric positive definite, and fall back to the Cauchy point on the boundary when $H_k$ is not symmetric positive definite.\n   - The agreement ratio is\n     $$\n     \\rho_k = \\frac{f(x_k) - f(x_k + p_k)}{m_k(0) - m_k(p_k)}.\n     $$\n     A step $p_k$ is accepted if $\\rho_k \\ge \\eta$, where $\\eta \\in (0,1)$ is a fixed threshold.\n   - Use the following trust region radius update rule with parameters $\\gamma_1 \\in (0,1)$ and $\\gamma_2  1$:\n     - If $\\rho_k  0.25$, set $\\Delta_{k+1} = \\gamma_1\\,\\Delta_k$.\n     - If $\\rho_k  0.75$ and $\\|p_k\\|_2 \\ge 0.99\\,\\Delta_k$, set $\\Delta_{k+1} = \\min(\\gamma_2\\,\\Delta_k,\\Delta_{\\max})$.\n     - Otherwise set $\\Delta_{k+1} = \\Delta_k$.\n   - Newton's method with line search computes a search direction $d_k$ by solving\n     $$\n     H_k d_k = -g_k,\n     $$\n     and uses backtracking to find a step length $\\alpha_k$ satisfying the Armijo condition\n     $$\n     f(x_k + \\alpha_k d_k) \\le f(x_k) + c_1\\,\\alpha_k\\,g_k^\\top d_k,\n     $$\n     where $c_1 \\in (0,1)$ is a fixed constant. If $d_k$ fails to be a descent direction (i.e., $g_k^\\top d_k \\ge 0$), replace $d_k$ by the steepest descent direction $d_k = -g_k$ before line search. Perform backtracking using $\\alpha_k \\leftarrow \\beta\\,\\alpha_k$ with $\\beta \\in (0,1)$ until the Armijo condition is met or a prescribed maximum number of backtracking steps is reached.\n\n2. Derive and implement the required components:\n   - For the Rosenbrock function $f$, derive and implement its gradient $g(x)$ and Hessian $H(x)$:\n     $$\n     g(x) = \\begin{bmatrix}\n     -2(1 - x_1) - 400 x_1 (x_2 - x_1^2) \\\\\n     200 (x_2 - x_1^2)\n     \\end{bmatrix}, \\quad\n     H(x) = \\begin{bmatrix}\n     2 - 400 x_2 + 1200 x_1^2  -400 x_1 \\\\\n     -400 x_1  200\n     \\end{bmatrix}.\n     $$\n   - Implement the Cauchy point $p_{\\mathrm{C}}$ along the steepest descent direction when needed:\n     $$\n     p_{\\mathrm{C}} = -\\alpha_{\\mathrm{C}}\\,g_k, \\quad \\alpha_{\\mathrm{C}} = \\begin{cases}\n     \\dfrac{\\|g_k\\|_2^2}{g_k^\\top H_k g_k},  \\text{if } g_k^\\top H_k g_k  0, \\\\\n     \\dfrac{\\Delta_k}{\\|g_k\\|_2},  \\text{otherwise},\n     \\end{cases}\n     $$\n     and project to the boundary if $\\|p_{\\mathrm{C}}\\|_2  \\Delta_k$ via $p = -\\Delta_k\\,g_k/\\|g_k\\|_2$.\n   - Implement the dogleg step $p_k$ using the segment from the Cauchy point to the Newton step $p_{\\mathrm{B}} = -H_k^{-1} g_k$ when $H_k$ is symmetric positive definite. Choose $p_k = p_{\\mathrm{B}}$ if $\\|p_{\\mathrm{B}}\\|_2 \\le \\Delta_k$, choose $p_k = -\\Delta_k\\,g_k/\\|g_k\\|_2$ if $\\|p_{\\mathrm{C}}\\|_2 \\ge \\Delta_k$, and otherwise choose the unique point on the segment joining $p_{\\mathrm{C}}$ to $p_{\\mathrm{B}}$ that satisfies $\\|p_k\\|_2 = \\Delta_k$.\n   - Track the trust region radius sequence $(\\Delta_k)$ and agreement ratios $(\\rho_k)$, and count the number of radius contractions (instances with $\\Delta_{k+1}  \\Delta_k$) and expansions (instances with $\\Delta_{k+1}  \\Delta_k$). Also count accepted and rejected steps according to the threshold $\\eta$.\n\n3. Stopping criterion for both methods:\n   - Stop when the Euclidean norm of the gradient satisfies $\\|\\nabla f(x_k)\\|_2 \\le \\text{tol}$ or when a maximum number of iterations is reached.\n\n4. Test suite:\n   - Use the following test cases with fixed parameters. For all cases, use the termination tolerance $\\text{tol} = 10^{-6}$ and the maximum number of iterations $K_{\\max} = 200$.\n   - Trust region parameters: $\\eta = 0.1$, $\\gamma_1 = 0.25$, $\\gamma_2 = 2$, and $\\Delta_{\\max} = 100$.\n   - Line search parameters: $c_1 = 10^{-4}$, $\\beta = 0.5$, and a maximum of $M = 50$ backtracking steps per iteration.\n   - Test cases are:\n     - Case 1 (happy path): $x_0 = (-1.2,\\,1.0)$, $\\Delta_0 = 1$.\n     - Case 2 (small initial radius boundary): $x_0 = (0.0,\\,0.0)$, $\\Delta_0 = 10^{-3}$.\n     - Case 3 (near optimum edge case): $x_0 = (1.0,\\,1.0)$, $\\Delta_0 = 1$.\n     - Case 4 (curved valley off-axis): $x_0 = (-1.5,\\,2.0)$, $\\Delta_0 = 0.5$.\n\n5. Program output specification:\n   - For each test case, compute and return the following list of values:\n     - Number of trust region iterations $n_{\\mathrm{TR}}$ (integer).\n     - Number of Newton line search iterations $n_{\\mathrm{NT}}$ (integer).\n     - Final objective value for trust region $f_{\\mathrm{TR}}$ (float, rounded to six decimal places).\n     - Final objective value for Newton line search $f_{\\mathrm{NT}}$ (float, rounded to six decimal places).\n     - Total trust region radius contractions (integer).\n     - Total trust region radius expansions (integer).\n     - Mean agreement ratio over accepted trust region steps $\\bar{\\rho}$ (float, rounded to six decimal places; use $0$ if no step was accepted).\n     - Total number of backtracking reductions performed by Newton line search across all iterations (integer).\n     - Final gradient norm for trust region $\\|g_{\\mathrm{TR}}\\|_2$ (float, rounded to six decimal places).\n     - Final gradient norm for Newton line search $\\|g_{\\mathrm{NT}}\\|_2$ (float, rounded to six decimal places).\n     - A boolean indicating whether trust region converged in fewer iterations than Newton line search ($n_{\\mathrm{TR}}  n_{\\mathrm{NT}}$).\n   - Your program should produce a single line of output containing the results for all four test cases as a comma-separated list enclosed in square brackets, where each test case result is itself a list in the order above. For example:\n     $$\n     [\\,[n_{\\mathrm{TR}},n_{\\mathrm{NT}},f_{\\mathrm{TR}},f_{\\mathrm{NT}},\\text{contractions},\\text{expansions},\\bar{\\rho},\\text{backtracks},\\|g_{\\mathrm{TR}}\\|_2,\\|g_{\\mathrm{NT}}\\|_2,\\text{TR\\_faster}],\\ldots\\,]\n     $$\n   - No physical units are involved; all quantities are dimensionless real numbers.", "solution": "The goal is to compare a trust region method and Newton's method with backtracking line search on a function with a curved narrow valley. The Rosenbrock function is a prototypical choice because it is twice continuously differentiable, has a unique minimizer at $x^\\star = (1,1)$, and exhibits strong curvature along one direction and a narrow curved valley that makes pure Newton or pure gradient methods delicate without globalization.\n\nPrinciple-based design:\n\n1. Start from the second-order Taylor model. For a twice differentiable function $f$ at $x_k$, the local quadratic approximation in step $p$ is\n$$\nm_k(p) = f(x_k) + g_k^\\top p + \\tfrac{1}{2} p^\\top H_k p,\n$$\nwhere $g_k = \\nabla f(x_k)$ and $H_k = \\nabla^2 f(x_k)$. This model is justified by Taylor's theorem and is the basis for both Newton updates (unconstrained minimizer of $m_k$) and trust region updates (constrained minimizer within a ball).\n\n2. Trust region method: The constrained problem\n$$\n\\min_{\\|p\\|_2 \\le \\Delta_k} m_k(p)\n$$\nis used to ensure robust steps that do not rely solely on the fidelity of the quadratic model far from $x_k$. When $H_k$ is symmetric positive definite, the unconstrained minimizer $p_{\\mathrm{B}} = -H_k^{-1} g_k$ lies inside the ball if $\\|p_{\\mathrm{B}}\\|_2 \\le \\Delta_k$, in which case the trust region step equals the Newton step. Otherwise, one can use the dogleg strategy: first move along the steepest descent direction to the Cauchy point $p_{\\mathrm{C}}$ that minimizes $m_k$ along $-g_k$, and then along the segment from $p_{\\mathrm{C}}$ to $p_{\\mathrm{B}}$ to reach the boundary $\\|p\\|_2 = \\Delta_k$. If $H_k$ is not symmetric positive definite (for instance, indefinite), the quadratic model may not be reliable in directions of negative curvature. In that case, choosing the Cauchy point projected to the boundary is a principled fallback:\n$$\n\\alpha_{\\mathrm{C}} = \\begin{cases}\n\\dfrac{\\|g_k\\|_2^2}{g_k^\\top H_k g_k},  \\text{if } g_k^\\top H_k g_k  0, \\\\\n\\dfrac{\\Delta_k}{\\|g_k\\|_2},  \\text{otherwise},\n\\end{cases}\n\\quad p_{\\mathrm{C}} = -\\alpha_{\\mathrm{C}}\\,g_k,\n\\quad p = \\begin{cases}\np_{\\mathrm{C}},  \\|p_{\\mathrm{C}}\\|_2 \\le \\Delta_k, \\\\\n-\\Delta_k\\,\\dfrac{g_k}{\\|g_k\\|_2},  \\text{otherwise}.\n\\end{cases}\n$$\n\n3. Agreement ratio and acceptance: To quantify how well the quadratic model predicts actual function reduction, define\n$$\n\\rho_k = \\frac{f(x_k) - f(x_k + p_k)}{m_k(0) - m_k(p_k)} = \\frac{f(x_k) - f(x_k + p_k)}{-\\left(g_k^\\top p_k + \\tfrac{1}{2} p_k^\\top H_k p_k\\right)}.\n$$\nWhen $m_k(0) - m_k(p_k)  0$, this ratio measures agreement between the model and actual decrease. Using a threshold $\\eta \\in (0,1)$, accept the step if $\\rho_k \\ge \\eta$ and update $x_{k+1} = x_k + p_k$; otherwise reject and retain $x_{k+1} = x_k$. Update the radius $\\Delta_k$ according to\n- If $\\rho_k  0.25$, contract the radius: $\\Delta_{k+1} = \\gamma_1\\,\\Delta_k$ with $\\gamma_1 \\in (0,1)$.\n- If $\\rho_k  0.75$ and the step nearly lies on the boundary $\\|p_k\\|_2 \\ge 0.99\\,\\Delta_k$, expand the radius: $\\Delta_{k+1} = \\min(\\gamma_2\\,\\Delta_k,\\Delta_{\\max})$ with $\\gamma_2  1$ and a cap $\\Delta_{\\max}$.\n- Otherwise keep the radius: $\\Delta_{k+1} = \\Delta_k$.\n\nThese rules balance caution and aggressiveness, growing the trust region when the model is predictive and shrinking it when it is not.\n\n4. Newton's method with Armijo backtracking: The Newton direction $d_k$ solves\n$$\nH_k d_k = -g_k.\n$$\nIf $H_k$ is not suitable (e.g., indefinite or near-singular), solving may yield a non-descent direction with $g_k^\\top d_k \\ge 0$. In that case, replace $d_k$ by the steepest descent direction $d_k = -g_k$. Use an Armijo backtracking line search: starting with $\\alpha_k = 1$, reduce $\\alpha_k \\leftarrow \\beta\\,\\alpha_k$ with $\\beta \\in (0,1)$ until\n$$\nf(x_k + \\alpha_k d_k) \\le f(x_k) + c_1\\,\\alpha_k\\,g_k^\\top d_k,\n$$\nwith $c_1 \\in (0,1)$. The Armijo condition guarantees sufficient decrease proportional to the directional derivative. Limit the number of backtracking steps per iteration to a fixed maximum to ensure termination.\n\n5. Stopping criterion: For both methods, use\n$$\n\\|\\nabla f(x_k)\\|_2 \\le \\text{tol}\n$$\nor reaching a maximum iteration budget $K_{\\max}$.\n\n6. Implementation of the Rosenbrock function: Compute $f(x)$, $g(x)$, and $H(x)$ exactly using the derived formulas,\n$$\ng(x) = \\begin{bmatrix}\n-2(1 - x_1) - 400 x_1 (x_2 - x_1^2) \\\\\n200 (x_2 - x_1^2)\n\\end{bmatrix}, \\quad\nH(x) = \\begin{bmatrix}\n2 - 400 x_2 + 1200 x_1^2  -400 x_1 \\\\\n-400 x_1  200\n\\end{bmatrix}.\n$$\nDetect whether $H(x)$ is symmetric positive definite via a Cholesky factorization; if it fails, treat $H(x)$ as unsuitable for the dogleg Newton step and use the Cauchy point strategy.\n\n7. Metrics to collect:\n- For trust region: count iterations $n_{\\mathrm{TR}}$, accepted steps, rejected steps, contractions and expansions of radius, and track $(\\Delta_k)$ and $(\\rho_k)$. Report the mean agreement ratio over accepted steps\n$$\n\\bar{\\rho} = \\frac{1}{N_{\\mathrm{acc}}}\\sum_{k \\in \\mathcal{A}} \\rho_k,\n$$\nwith the convention $\\bar{\\rho} = 0$ if there were no accepted steps. Also report final $f_{\\mathrm{TR}}$ and $\\|g_{\\mathrm{TR}}\\|_2$.\n- For Newton line search: count iterations $n_{\\mathrm{NT}}$, total backtracking reductions, and report final $f_{\\mathrm{NT}}$ and $\\|g_{\\mathrm{NT}}\\|_2$.\n\n8. Test suite and output: Run the specified four test cases with the provided parameters and termination criteria. For each case, output the list\n$$\n\\left[n_{\\mathrm{TR}},\\,n_{\\mathrm{NT}},\\,f_{\\mathrm{TR}},\\,f_{\\mathrm{NT}},\\,\\text{contractions},\\,\\text{expansions},\\,\\bar{\\rho},\\,\\text{backtracks},\\,\\|g_{\\mathrm{TR}}\\|_2,\\,\\|g_{\\mathrm{NT}}\\|_2,\\,\\text{TR\\_faster}\\right],\n$$\nusing floats rounded to six decimal places and the final boolean indicating whether trust region used fewer iterations than Newton line search. Aggregate the four lists into a single line as a comma-separated list enclosed in square brackets.\n\nThis design derives directly from the Taylor model and sufficient decrease conditions and translates into explicit algorithms robust on narrow valleys like those of the Rosenbrock function, enabling a principled comparison between trust region globalization and line search globalization.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef rosenbrock(x):\n    x1, x2 = x[0], x[1]\n    f = (1 - x1)**2 + 100.0 * (x2 - x1**2)**2\n    g = np.array([\n        -2.0 * (1.0 - x1) - 400.0 * x1 * (x2 - x1**2),\n        200.0 * (x2 - x1**2)\n    ])\n    H = np.array([\n        [2.0 - 400.0 * x2 + 1200.0 * x1**2, -400.0 * x1],\n        [-400.0 * x1, 200.0]\n    ])\n    return f, g, H\n\ndef is_spd(H):\n    try:\n        np.linalg.cholesky(H)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\ndef cauchy_point(g, H, Delta):\n    g_norm = np.linalg.norm(g)\n    gHg = float(g @ (H @ g))\n    if gHg  0:\n        alpha_c = (g_norm**2) / gHg\n        p = -alpha_c * g\n        if np.linalg.norm(p)  Delta:\n            p = -Delta * g / g_norm\n    else:\n        # Nonpositive curvature: take boundary step along -g\n        p = -Delta * g / (g_norm if g_norm  0 else 1.0)\n    return p\n\ndef dogleg_step(g, H, Delta):\n    # If Hessian is SPD, attempt dogleg between Cauchy and Newton steps\n    if is_spd(H):\n        # Newton step\n        try:\n            pB = -np.linalg.solve(H, g)\n        except np.linalg.LinAlgError:\n            # Fallback: Cauchy to boundary\n            return cauchy_point(g, H, Delta)\n        # Cauchy step\n        pU = cauchy_point(g, H, Delta * 1.0e6)  # unconstrained cauchy magnitude\n        # If Cauchy exceeds Delta, move to boundary along -g\n        if np.linalg.norm(pU) = Delta:\n            return -Delta * g / (np.linalg.norm(g) if np.linalg.norm(g)  0 else 1.0)\n        # If Newton inside TR, take it\n        if np.linalg.norm(pB) = Delta:\n            return pB\n        # Else, find tau in (0,1) such that ||pU + tau*(pB - pU)|| = Delta\n        a = pB - pU\n        pu_dot_a = float(pU @ a)\n        a_norm2 = float(a @ a)\n        pu_norm2 = float(pU @ pU)\n        # Solve quadratic: (pu_norm2) + 2*tau*(pu_dot_a) + tau^2*(a_norm2) = Delta^2\n        # tau = [-pu_dot_a + sqrt(pu_dot_a^2 + a_norm2*(Delta^2 - pu_norm2))] / a_norm2\n        rad = pu_dot_a**2 + a_norm2 * (Delta**2 - pu_norm2)\n        rad = max(rad, 0.0)\n        tau = (-pu_dot_a + np.sqrt(rad)) / (a_norm2 if a_norm2  0 else 1.0)\n        return pU + tau * a\n    else:\n        # Fallback: Cauchy point projected to boundary\n        return cauchy_point(g, H, Delta)\n\ndef trust_region_method(x0, Delta0, tol=1e-6, max_iters=200, eta=0.1, gamma1=0.25, gamma2=2.0, Delta_max=100.0):\n    x = np.array(x0, dtype=float)\n    Delta = float(Delta0)\n    n_iters = 0\n    contractions = 0\n    expansions = 0\n    accepted = 0\n    rejected = 0\n    rho_acc_sum = 0.0\n    # histories (not printed, but tracked)\n    Delta_hist = []\n    rho_hist = []\n    for k in range(max_iters):\n        f, g, H = rosenbrock(x)\n        gnorm = np.linalg.norm(g)\n        if gnorm = tol:\n            break\n        p = dogleg_step(g, H, Delta)\n        # Predicted reduction\n        gTp = float(g @ p)\n        pTHp = float(p @ (H @ p))\n        pred_red = -(gTp + 0.5 * pTHp)\n        # Actual reduction\n        f_new, _, _ = rosenbrock(x + p)\n        act_red = f - f_new\n        if pred_red  0:\n            rho = act_red / pred_red\n        else:\n            rho = -np.inf\n        rho_hist.append(rho)\n        # Acceptance\n        if rho = eta and f_new  f:\n            x = x + p\n            accepted += 1\n            if np.isfinite(rho):\n                rho_acc_sum += rho\n        else:\n            rejected += 1\n            # keep x\n        # Radius update rules\n        if rho  0.25 or not np.isfinite(rho):\n            Delta = max(gamma1 * Delta, 1e-12)\n            contractions += 1\n        elif rho  0.75 and np.linalg.norm(p) = 0.99 * Delta:\n            new_Delta = min(gamma2 * Delta, Delta_max)\n            if new_Delta  Delta + 1e-15:\n                expansions += 1\n            Delta = new_Delta\n        # Track radius\n        Delta_hist.append(Delta)\n        n_iters += 1\n    f_final, g_final, _ = rosenbrock(x)\n    gnorm_final = float(np.linalg.norm(g_final))\n    avg_rho = (rho_acc_sum / accepted) if accepted  0 else 0.0\n    return {\n        \"x\": x,\n        \"n_iters\": n_iters,\n        \"f\": float(f_final),\n        \"gnorm\": gnorm_final,\n        \"contractions\": contractions,\n        \"expansions\": expansions,\n        \"accepted\": accepted,\n        \"rejected\": rejected,\n        \"avg_rho\": float(avg_rho),\n        \"Delta_hist\": Delta_hist,\n        \"rho_hist\": rho_hist\n    }\n\ndef newton_line_search(x0, tol=1e-6, max_iters=200, c1=1e-4, beta=0.5, max_backtracks=50):\n    x = np.array(x0, dtype=float)\n    n_iters = 0\n    total_backtracks = 0\n    for k in range(max_iters):\n        f, g, H = rosenbrock(x)\n        gnorm = np.linalg.norm(g)\n        if gnorm = tol:\n            break\n        # Compute Newton direction\n        try:\n            d = -np.linalg.solve(H, g)\n        except np.linalg.LinAlgError:\n            d = -g\n        # Ensure descent\n        if float(g @ d) = 0.0:\n            d = -g\n        # Backtracking Armijo\n        alpha = 1.0\n        backtracks = 0\n        gTd = float(g @ d)\n        f_curr = f\n        while backtracks  max_backtracks:\n            x_trial = x + alpha * d\n            f_trial, _, _ = rosenbrock(x_trial)\n            if f_trial = f_curr + c1 * alpha * gTd:\n                break\n            alpha *= beta\n            backtracks += 1\n        total_backtracks += backtracks\n        # If no acceptable alpha found, stop\n        if backtracks == max_backtracks and f_trial  f_curr + c1 * alpha * gTd:\n            # Unable to find sufficient decrease; terminate to avoid stagnation\n            break\n        x = x + alpha * d\n        n_iters += 1\n    f_final, g_final, _ = rosenbrock(x)\n    gnorm_final = float(np.linalg.norm(g_final))\n    return {\n        \"x\": x,\n        \"n_iters\": n_iters,\n        \"f\": float(f_final),\n        \"gnorm\": gnorm_final,\n        \"backtracks\": total_backtracks\n    }\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (x0, Delta0)\n    test_cases = [\n        (np.array([-1.2, 1.0]), 1.0),       # Case 1\n        (np.array([0.0, 0.0]), 1e-3),       # Case 2\n        (np.array([1.0, 1.0]), 1.0),        # Case 3\n        (np.array([-1.5, 2.0]), 0.5),       # Case 4\n    ]\n\n    # Common parameters\n    tol = 1e-6\n    max_iters = 200\n    # Trust region parameters\n    eta = 0.1\n    gamma1 = 0.25\n    gamma2 = 2.0\n    Delta_max = 100.0\n    # Line search parameters\n    c1 = 1e-4\n    beta = 0.5\n    max_backtracks = 50\n\n    results = []\n    for x0, Delta0 in test_cases:\n        # Trust region\n        tr = trust_region_method(\n            x0=x0, Delta0=Delta0, tol=tol, max_iters=max_iters,\n            eta=eta, gamma1=gamma1, gamma2=gamma2, Delta_max=Delta_max\n        )\n        # Newton with line search\n        nt = newton_line_search(\n            x0=x0, tol=tol, max_iters=max_iters,\n            c1=c1, beta=beta, max_backtracks=max_backtracks\n        )\n        # Build result list for this case\n        n_tr = tr[\"n_iters\"]\n        n_nt = nt[\"n_iters\"]\n        f_tr = round(tr[\"f\"], 6)\n        f_nt = round(nt[\"f\"], 6)\n        contractions = tr[\"contractions\"]\n        expansions = tr[\"expansions\"]\n        avg_rho = round(tr[\"avg_rho\"], 6)\n        backtracks_nt = nt[\"backtracks\"]\n        g_tr = round(tr[\"gnorm\"], 6)\n        g_nt = round(nt[\"gnorm\"], 6)\n        tr_faster = n_tr  n_nt\n        case_result = [n_tr, n_nt, f_tr, f_nt, contractions, expansions, avg_rho, backtracks_nt, g_tr, g_nt, tr_faster]\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # Print a single line: list of per-case lists.\n    def format_item(item):\n        if isinstance(item, bool):\n            return \"True\" if item else \"False\"\n        elif isinstance(item, (int, np.integer)):\n            return str(int(item))\n        elif isinstance(item, float):\n            # Ensure standard Python float formatting (already rounded).\n            return str(item)\n        elif isinstance(item, list):\n            return \"[\" + \",\".join(format_item(elem) for elem in item) + \"]\"\n        else:\n            return str(item)\n\n    print(\"[\" + \",\".join(format_item(case) for case in results) + \"]\")\n\nsolve()\n```", "id": "3284806"}]}