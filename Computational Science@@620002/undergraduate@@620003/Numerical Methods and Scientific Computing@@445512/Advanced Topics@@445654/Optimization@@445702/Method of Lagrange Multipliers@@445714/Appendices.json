{"hands_on_practices": [{"introduction": "To truly master the method of Lagrange multipliers, we must move from theoretical understanding to practical application. This first exercise challenges you to translate a tangible geometric puzzle into the language of constrained optimization. By determining the largest possible ellipse that fits inside a unit square, you will practice the crucial skills of defining an objective function, formulating inequality constraints, and systematically applying the Karush-Kuhn-Tucker (KKT) conditions to find a solution [@problem_id:3251729]. This process is fundamental in fields like engineering and design, where optimal shapes and configurations must be found within given physical boundaries.", "problem": "Consider the family of axis-aligned ellipses parameterized by semi-axes and center,\n$$\n\\mathcal{E}(a,b,h,k) \\;=\\; \\left\\{ (x,y) \\in \\mathbb{R}^{2} \\;:\\; \\frac{(x-h)^{2}}{a^{2}} + \\frac{(y-k)^{2}}{b^{2}} \\le 1 \\right\\},\n$$\nwith $a0$, $b0$, $h \\in \\mathbb{R}$, and $k \\in \\mathbb{R}$. Let the unit square be\n$$\n\\mathcal{S} \\;=\\; [0,1] \\times [0,1].\n$$\nAmong all such ellipses that are entirely contained in $\\mathcal{S}$, determine the maximal possible area. Formulate the problem as a constrained optimization using the method of Lagrange multipliers for inequality constraints (Karush–Kuhn–Tucker conditions), starting only from the defining containment requirements for $\\mathcal{E}(a,b,h,k) \\subset \\mathcal{S}$ and the geometric formula for the area of an ellipse. Derive the necessary optimality conditions and solve them to obtain the maximal area. Provide your final answer as an exact expression, with no rounding.", "solution": "The problem asks for the maximal area of an axis-aligned ellipse that is fully contained within the unit square $\\mathcal{S} = [0,1] \\times [0,1]$.\n\nFirst, we must formulate this as a constrained optimization problem. The variables are the semi-axes $a$ and $b$, and the center coordinates $(h,k)$ of the ellipse $\\mathcal{E}(a,b,h,k)$. The problem statement specifies $a0$ and $b0$.\n\nThe objective function to be maximized is the area of the ellipse, given by the formula $A(a,b) = \\pi ab$. Maximizing $\\pi ab$ is equivalent to maximizing the product $ab$, since $\\pi$ is a positive constant. Let our objective function be $f(a,b,h,k) = ab$.\n\nNext, we must translate the geometric containment constraint $\\mathcal{E}(a,b,h,k) \\subset \\mathcal{S}$ into a set of algebraic inequalities. An ellipse is defined by $\\frac{(x-h)^2}{a^2} + \\frac{(y-k)^2}{b^2} \\le 1$. The horizontal extent of the ellipse is given by the interval $[h-a, h+a]$, and its vertical extent is given by $[k-b, k+b]$. For the ellipse to be contained in the unit square $\\mathcal{S} = [0,1] \\times [0,1]$, these intervals must be subsets of $[0,1]$.\n\nThe containment condition for the x-axis is $[h-a, h+a] \\subseteq [0,1]$, which yields two inequalities:\n$1.$ $h-a \\ge 0$\n$2.$ $h+a \\le 1$\n\nThe containment condition for the y-axis is $[k-b, k+b] \\subseteq [0,1]$, which yields another two inequalities:\n$3.$ $k-b \\ge 0$\n$4.$ $k+b \\le 1$\n\nWe will use the Karush-Kuhn-Tucker (KKT) conditions to solve this problem. The standard formulation is for a minimization problem with constraints of the form $g_i(\\mathbf{x}) \\le 0$. Maximizing $ab$ is equivalent to minimizing $-ab$. The constraints can be written as:\n$c_1(a,h) = a-h \\le 0$\n$c_2(a,h) = a+h-1 \\le 0$\n$c_3(b,k) = b-k \\le 0$\n$c_4(b,k) = b+k-1 \\le 0$\n\nThe optimization problem is thus:\nMinimize $F(a,b,h,k) = -ab$\nsubject to $c_1, c_2, c_3, c_4 \\le 0$, and implicit constraints $a0, b0$.\n\nThe Lagrangian function is:\n$$ \\mathcal{L}(a,b,h,k, \\mu_1, \\mu_2, \\mu_3, \\mu_4) = -ab + \\mu_1(a-h) + \\mu_2(a+h-1) + \\mu_3(b-k) + \\mu_4(b+k-1) $$\nwhere $\\mu_1, \\mu_2, \\mu_3, \\mu_4$ are the Lagrange multipliers.\n\nThe KKT conditions for an optimal solution $(a^*,b^*,h^*,k^*)$ are:\n1.  **Stationarity**: The gradient of the Lagrangian with respect to the variables $a,b,h,k$ must be zero.\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial a} = -b + \\mu_1 + \\mu_2 = 0 \\implies b = \\mu_1 + \\mu_2 $$\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial b} = -a + \\mu_3 + \\mu_4 = 0 \\implies a = \\mu_3 + \\mu_4 $$\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial h} = -\\mu_1 + \\mu_2 = 0 \\implies \\mu_1 = \\mu_2 $$\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial k} = -\\mu_3 + \\mu_4 = 0 \\implies \\mu_3 = \\mu_4 $$\n\n2.  **Primal Feasibility**: The constraints must be satisfied.\n    $$ a-h \\le 0 $$\n    $$ a+h-1 \\le 0 $$\n    $$ b-k \\le 0 $$\n    $$ b+k-1 \\le 0 $$\n\n3.  **Dual Feasibility**: The Lagrange multipliers for inequality constraints must be non-negative.\n    $$ \\mu_1, \\mu_2, \\mu_3, \\mu_4 \\ge 0 $$\n\n4.  **Complementary Slackness**: The product of each multiplier and its corresponding constraint must be zero.\n    $$ \\mu_1(a-h) = 0 $$\n    $$ \\mu_2(a+h-1) = 0 $$\n    $$ \\mu_3(b-k) = 0 $$\n    $$ \\mu_4(b+k-1) = 0 $$\n\nWe now solve this system of equations and inequalities.\nFrom the stationarity conditions, we have $\\mu_1 = \\mu_2$ and $\\mu_3 = \\mu_4$. Substituting these into the first two stationarity equations gives:\n$$ b = 2\\mu_1 $$\n$$ a = 2\\mu_3 $$\n\nWe seek to maximize the area, so we are not interested in the trivial solution where $a=0$ or $b=0$, as this yields an area of $0$. We must have $a  0$ and $b  0$.\nIf $a  0$, then $a = 2\\mu_3  0$, which implies $\\mu_3  0$. Since $\\mu_3 = \\mu_4$, we have $\\mu_4  0$.\nIf $b  0$, then $b = 2\\mu_1  0$, which implies $\\mu_1  0$. Since $\\mu_1 = \\mu_2$, we have $\\mu_2  0$.\nThus, for a solution with non-zero area, all Lagrange multipliers must be strictly positive: $\\mu_1, \\mu_2, \\mu_3, \\mu_4  0$.\n\nNow we apply the complementary slackness conditions. Since all multipliers are non-zero, their corresponding constraint functions must be equal to zero. This means all four inequality constraints must be active (i.e., hold with equality):\n1.  $a-h = 0 \\implies a = h$\n2.  $a+h-1 = 0$\n3.  $b-k = 0 \\implies b = k$\n4.  $b+k-1 = 0$\n\nWe now have a system of four linear equations for the four variables $a,b,h,k$.\nFrom the first pair of equations, substitute $h=a$ into $a+h-1=0$:\n$$ a+a-1 = 0 \\implies 2a = 1 \\implies a = \\frac{1}{2} $$\nSince $h=a$, we have $h = \\frac{1}{2}$.\n\nFrom the second pair of equations, substitute $k=b$ into $b+k-1=0$:\n$$ b+b-1 = 0 \\implies 2b = 1 \\implies b = \\frac{1}{2} $$\nSince $k=b$, we have $k = \\frac{1}{2}$.\n\nThe unique solution satisfying the KKT conditions for a non-zero area is $(a,b,h,k) = (\\frac{1}{2}, \\frac{1}{2}, \\frac{1}{2}, \\frac{1}{2})$. This represents a circle of radius $\\frac{1}{2}$ centered at $(\\frac{1}{2}, \\frac{1}{2})$.\nLet's verify this solution. The parameters $a=\\frac{1}{2}, b=\\frac{1}{2}$ are positive. The constraints are all met with equality, so primal feasibility is satisfied. The multipliers are $b = 2\\mu_1 \\implies \\frac{1}{2} = 2\\mu_1 \\implies \\mu_1 = \\frac{1}{4}$, and $a = 2\\mu_3 \\implies \\frac{1}{2} = 2\\mu_3 \\implies \\mu_3 = \\frac{1}{4}$. Thus, $\\mu_1=\\mu_2=\\mu_3=\\mu_4=\\frac{1}{4}$, which are all non-negative, satisfying dual feasibility. All KKT conditions are met.\n\nThe feasible region for the variables is convex, but the objective function $F=-ab$ is not convex. However, we can argue for global optimality by examining the constraints. The inequalities $a-h \\le 0$ and $a+h-1 \\le 0$ imply $a \\le h$ and $h \\le 1-a$. This requires $a \\le 1-a$, which simplifies to $2a \\le 1$ or $a \\le \\frac{1}{2}$. Similarly, the inequalities for $b$ and $k$ imply $b \\le \\frac{1}{2}$. Since we wish to maximize $ab$ for $a0, b0$, and the function $ab$ is strictly increasing in $a$ and $b$ on this domain, the maximum value must occur at the upper bounds of the feasible region for $a$ and $b$, i.e., at $a=\\frac{1}{2}$ and $b=\\frac{1}{2}$. This confirms our KKT solution is indeed the global maximum.\n\nThe maximal possible area is obtained using these optimal parameter values:\n$$ A_{max} = \\pi a b = \\pi \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{\\pi}{4} $$", "answer": "$$\\boxed{\\frac{\\pi}{4}}$$", "id": "3251729"}, {"introduction": "Beyond simply calculating a solution, it is vital to understand what the Lagrange multipliers represent. This next practice is a conceptual deep dive into the behavior and meaning of the multipliers themselves. By analyzing a simple optimization problem with overlapping or redundant constraints, you will explore how multipliers react when a constraint is active, inactive, or part of a dependent set [@problem_id:3251896]. This exercise builds crucial intuition about how the optimal value is sensitive to changes in the constraints and reveals that multipliers are not just computational artifacts, but carriers of valuable information.", "problem": "Consider the constrained optimization problem of minimizing the Euclidean norm subject to two scalar inequality constraints with potentially redundant bounds. Let the objective be $f(x,y) = x^2 + y^2$, and consider the constraint pair $x \\le \\alpha$ and $x \\le \\beta$, where $\\alpha  0$ is a fixed constant and $\\beta \\in \\mathbb{R}$ is a parameter. Assume standard regularity conditions (for example, Slater’s condition) hold so that first-order optimality applies. Using the method of Lagrange multipliers for inequality constraints, also known as the Karush-Kuhn-Tucker (KKT) conditions, analyze the optimal solution and the associated Lagrange multipliers as the parameter $\\beta$ varies, with particular attention to the case where one constraint becomes redundant because it is strictly looser than the other.\n\nLet $\\lambda_1 \\ge 0$ and $\\lambda_2 \\ge 0$ denote the KKT multipliers associated with the constraints $x \\le \\alpha$ and $x \\le \\beta$, respectively. Which of the following statements are correct?\n\nA. If $\\beta  \\alpha$, then the constraint $x \\le \\beta$ is redundant at the optimizer, and the multipliers satisfy $\\lambda_2 = 0$ and $\\lambda_1 = -2\\alpha$.\n\nB. If $\\beta  \\alpha$, then the constraint $x \\le \\alpha$ is redundant at the optimizer, and the multipliers satisfy $\\lambda_1 = 0$ and $\\lambda_2 = -2\\beta$.\n\nC. If $\\beta = \\alpha$, then the multipliers are unique and satisfy $\\lambda_1 = \\lambda_2 = -\\alpha$.\n\nD. For all $\\beta \\in \\mathbb{R}$, the sum of the multipliers satisfies $\\lambda_1 + \\lambda_2 = -2\\min(\\alpha,\\beta)$, and this sum depends continuously on $\\beta$.\n\nE. For some $\\beta \\neq \\alpha$, the multiplier associated with a redundant constraint is strictly positive.", "solution": "The user wants me to solve and analyze a constrained optimization problem using the Karush-Kuhn-Tucker (KKT) conditions.\n\n### Problem Validation\n\nI will first validate the problem statement.\n\n**Step 1: Extract Givens**\n- **Objective function**: $f(x,y) = x^2 + y^2$.\n- **Constraints**:\n    1. $x \\le \\alpha$\n    2. $x \\le \\beta$\n- **Constants and Parameters**:\n    - $\\alpha  0$ is a fixed constant.\n    - $\\beta \\in \\mathbb{R}$ is a parameter.\n- **Lagrange Multipliers**:\n    - $\\lambda_1 \\ge 0$ for the constraint $x \\le \\alpha$.\n    - $\\lambda_2 \\ge 0$ for the constraint $x \\le \\beta$.\n- **Assumptions**: Standard regularity conditions hold, implying the KKT conditions are necessary for optimality.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is mathematically and scientifically sound. It is a standard convex optimization problem: minimizing a strictly convex quadratic function over a convex set defined by linear inequalities.\n- **Scientifically Grounded**: The problem is a textbook example of applying KKT theory, a fundamental tool in mathematical optimization.\n- **Well-Posed**: The objective function $f(x,y)$ is strictly convex, and the feasible set is non-empty and convex. This guarantees the existence of a unique optimal solution $(x^*, y^*)$.\n- **Objective**: The problem is stated using precise mathematical language, with no ambiguity or subjectivity.\n- **Completeness and Consistency**: The problem provides all necessary information: an objective function, a complete set of constraints, and the relevant assumptions for the chosen method (KKT). The given information is internally consistent.\n\n**Step 3: Verdict and Action**\nThe problem statement is **valid**. I will proceed with the solution.\n\n### Derivation of the Solution\n\nThe optimization problem is:\n$$\n\\text{minimize} \\quad f(x,y) = x^2 + y^2 \\\\\n\\text{subject to} \\quad g_1(x,y) = x - \\alpha \\le 0 \\\\\n\\quad \\quad \\quad \\quad g_2(x,y) = x - \\beta \\le 0\n$$\nThe feasible region is the set of points $(x,y)$ such that $x \\le \\alpha$ and $x \\le \\beta$. This is equivalent to the single constraint $x \\le \\min(\\alpha, \\beta)$. Let $c = \\min(\\alpha, \\beta)$. Since $\\alpha  0$, we have $c \\le \\alpha  0$.\n\nThe objective function $f(x,y)$ represents the squared distance from the origin $(0,0)$. To minimize this function subject to $x \\le c$, we need to find the point in the feasible half-plane $\\{ (x,y) \\in \\mathbb{R}^2 \\mid x \\le c \\}$ that is closest to the origin. The value of $y$ is unconstrained, so its optimal value is $y^*=0$. To minimize $x^2$ for $x \\in (-\\infty, c]$ where $c0$, we must choose the value of $x$ with the smallest absolute value, which is $x=c$.\nThus, the unique optimal solution is $(x^*, y^*) = (c, 0) = (\\min(\\alpha, \\beta), 0)$.\n\nNow, we apply the KKT conditions. The Lagrangian is:\n$$\n\\mathcal{L}(x, y, \\lambda_1, \\lambda_2) = x^2 + y^2 + \\lambda_1(x - \\alpha) + \\lambda_2(x - \\beta)\n$$\nThe KKT conditions are:\n1.  **Stationarity**: $\\nabla_{x,y} \\mathcal{L} = (0,0)$\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial x} = 2x + \\lambda_1 + \\lambda_2 = 0 $$\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial y} = 2y = 0 $$\n2.  **Primal Feasibility**:\n    $$ x - \\alpha \\le 0 $$\n    $$ x - \\beta \\le 0 $$\n3.  **Dual Feasibility**:\n    $$ \\lambda_1 \\ge 0 $$\n    $$ \\lambda_2 \\ge 0 $$\n4.  **Complementary Slackness**:\n    $$ \\lambda_1(x - \\alpha) = 0 $$\n    $$ \\lambda_2(x - \\beta) = 0 $$\n\nFrom stationarity, $2y=0$ implies $y^*=0$. Plugging the optimal solution $x^* = \\min(\\alpha, \\beta)$ into the first stationarity condition gives:\n$$ 2\\min(\\alpha, \\beta) + \\lambda_1 + \\lambda_2 = 0 \\implies \\lambda_1 + \\lambda_2 = -2\\min(\\alpha, \\beta) $$\nThis relationship holds for any valid set of multipliers. We now analyze the specific values of the multipliers by considering three cases for the parameter $\\beta$.\n\n**Case 1: $\\beta  \\alpha$**\nIn this case, $\\min(\\alpha, \\beta) = \\alpha$. The optimal solution is $(x^*, y^*) = (\\alpha, 0)$.\n- The first constraint is active: $x^* - \\alpha = \\alpha - \\alpha = 0$.\n- The second constraint is inactive: $x^* - \\beta = \\alpha - \\beta  0$ (since $\\beta  \\alpha$).\n- By complementary slackness for the inactive second constraint, $\\lambda_2(x^* - \\beta) = 0 \\implies \\lambda_2 = 0$.\n- Using the sum of multipliers: $\\lambda_1 + \\lambda_2 = -2\\min(\\alpha, \\beta) \\implies \\lambda_1 + 0 = -2\\alpha$. Thus, $\\lambda_1 = -2\\alpha$.\n- We check dual feasibility: $\\lambda_2 = 0 \\ge 0$. Since $\\alpha  0$, $\\lambda_1 = -2\\alpha  0$. Both are valid.\n- The constraint $x \\le \\beta$ is redundant, as the feasible set $x \\le \\alpha$ is unaffected by its removal.\n\n**Case 2: $\\beta  \\alpha$**\nIn this case, $\\min(\\alpha, \\beta) = \\beta$. The optimal solution is $(x^*, y^*) = (\\beta, 0)$.\n- The first constraint is inactive: $x^* - \\alpha = \\beta - \\alpha  0$ (since $\\beta  \\alpha$).\n- The second constraint is active: $x^* - \\beta = \\beta - \\beta = 0$.\n- By complementary slackness for the inactive first constraint, $\\lambda_1(x^* - \\alpha) = 0 \\implies \\lambda_1 = 0$.\n- Using the sum of multipliers: $\\lambda_1 + \\lambda_2 = -2\\min(\\alpha, \\beta) \\implies 0 + \\lambda_2 = -2\\beta$. Thus, $\\lambda_2 = -2\\beta$.\n- We check dual feasibility: $\\lambda_1 = 0 \\ge 0$. We are given $\\alpha  0$, and here $\\beta  \\alpha$, so $\\beta  0$. This implies $\\lambda_2 = -2\\beta  0$. Both are valid.\n- The constraint $x \\le \\alpha$ is redundant.\n\n**Case 3: $\\beta = \\alpha$**\nIn this case, $\\min(\\alpha, \\beta) = \\alpha = \\beta$. The optimal solution is $(x^*, y^*) = (\\alpha, 0)$.\n- Both constraints are active: $x^* - \\alpha = 0$ and $x^* - \\beta = 0$.\n- The complementary slackness conditions $\\lambda_1(0)=0$ and $\\lambda_2(0)=0$ provide no information to determine $\\lambda_1$ and $\\lambda_2$ individually.\n- The stationarity condition requires $\\lambda_1 + \\lambda_2 = -2\\alpha$.\n- Any pair $(\\lambda_1, \\lambda_2)$ that satisfies $\\lambda_1 + \\lambda_2 = -2\\alpha$ along with $\\lambda_1 \\ge 0$ and $\\lambda_2 \\ge 0$ is a valid set of KKT multipliers. For example, $(\\lambda_1, \\lambda_2) = (-2\\alpha, 0)$, $(0, -2\\alpha)$, or $(-\\alpha, -\\alpha)$ are all valid solutions (since $\\alpha  0$, $-\\alpha  0$ and $-2\\alpha  0$).\n- Therefore, the KKT multipliers are **not unique** in this case. This occurs because the gradients of the active constraints, $\\nabla g_1 = (1, 0)$ and $\\nabla g_2 = (1, 0)$, are linearly dependent, so the Linear Independence Constraint Qualification (LICQ) is not satisfied.\n\n### Evaluation of Options\n\n**A. If $\\beta  \\alpha$, then the constraint $x \\le \\beta$ is redundant at the optimizer, and the multipliers satisfy $\\lambda_2 = 0$ and $\\lambda_1 = -2\\alpha$.**\nOur analysis for Case 1 ($\\beta  \\alpha$) showed that the constraint $x \\le \\beta$ is redundant (and inactive, since $x^*=\\alpha  \\beta$), which implies $\\lambda_2=0$ from complementary slackness. The stationarity condition then gives $\\lambda_1 = -2\\alpha$. This statement is consistent with our findings.\n**Verdict: Correct.**\n\n**B. If $\\beta  \\alpha$, then the constraint $x \\le \\alpha$ is redundant at the optimizer, and the multipliers satisfy $\\lambda_1 = 0$ and $\\lambda_2 = -2\\beta$.**\nOur analysis for Case 2 ($\\beta  \\alpha$) showed that the constraint $x \\le \\alpha$ is redundant (and inactive, since $x^*=\\beta  \\alpha$), which implies $\\lambda_1=0$. The stationarity condition then gives $\\lambda_2 = -2\\beta$. This statement is consistent with our findings.\n**Verdict: Correct.**\n\n**C. If $\\beta = \\alpha$, then the multipliers are unique and satisfy $\\lambda_1 = \\lambda_2 = -\\alpha$.**\nOur analysis for Case 3 ($\\beta = \\alpha$) showed that the multipliers are not unique. They must satisfy $\\lambda_1 + \\lambda_2 = -2\\alpha$ and be non-negative. While $(\\lambda_1, \\lambda_2) = (-\\alpha, -\\alpha)$ is one possible solution, it is not the only one. The claim of uniqueness is false.\n**Verdict: Incorrect.**\n\n**D. For all $\\beta \\in \\mathbb{R}$, the sum of the multipliers satisfies $\\lambda_1 + \\lambda_2 = -2\\min(\\alpha,\\beta)$, and this sum depends continuously on $\\beta$.**\nFrom the stationarity condition, $2x^* + \\lambda_1 + \\lambda_2 = 0$. Since we established $x^* = \\min(\\alpha, \\beta)$, it follows directly that $\\lambda_1 + \\lambda_2 = -2x^* = -2\\min(\\alpha, \\beta)$. The function $g(\\beta) = \\min(\\alpha, \\beta)$ is a continuous function of $\\beta$. The composition of a continuous function with a scaling (multiplication by $-2$) is also continuous. Thus, the sum of the multipliers is a continuous function of $\\beta$.\n**Verdict: Correct.**\n\n**E. For some $\\beta \\neq \\alpha$, the multiplier associated with a redundant constraint is strictly positive.**\nFor $\\beta \\neq \\alpha$, exactly one constraint is redundant, and it is strictly inactive (i.e., $g_i(x^*)  0$).\n- If $\\beta  \\alpha$, the redundant constraint is $x \\le \\beta$. Its multiplier is $\\lambda_2=0$.\n- If $\\beta  \\alpha$, the redundant constraint is $x \\le \\alpha$. Its multiplier is $\\lambda_1=0$.\nIn both cases where $\\beta \\neq \\alpha$, the multiplier associated with the redundant constraint is exactly zero, not strictly positive. This is a direct consequence of the KKT complementary slackness condition.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{ABD}$$", "id": "3251896"}, {"introduction": "In modern science and engineering, many optimization problems are solved numerically. This final exercise bridges the gap between abstract theory and computational practice by having you implement a program to solve a quadratic optimization problem and verify the KKT conditions. You will codify the logic for handling both active and inactive constraints and numerically check the two key conditions that lie at the heart of the theory: dual feasibility ($\\lambda \\ge 0$) and complementary slackness ($\\lambda g(x^\\star) = 0$) [@problem_id:3251733]. This hands-on coding experience reinforces the core principles of constrained optimization and demonstrates how they are applied in a practical, algorithmic context.", "problem": "Consider the smooth, strictly convex quadratic optimization problem in $\\mathbb{R}^2$ with a single linear inequality constraint. Let the objective be $f(x) = \\tfrac{1}{2} x^\\top Q x + c^\\top x$ and the constraint be $g(x) = a^\\top x - b \\le 0$, where $Q \\in \\mathbb{R}^{2 \\times 2}$ is symmetric positive definite, $c \\in \\mathbb{R}^2$, $a \\in \\mathbb{R}^2$, and $b \\in \\mathbb{R}$. The Karush-Kuhn-Tucker (KKT) conditions for a minimizer $x^\\star$ with a single multiplier $\\lambda^\\star$ assert primal feasibility, dual feasibility, stationarity, and complementary slackness. Your task is to numerically verify two of these conditions, namely dual feasibility and complementary slackness, without assuming any special-case shortcuts.\n\nStarting from the fundamental definition that an unconstrained minimizer of a smooth function satisfies $\\nabla f(x) = 0$, and that an active inequality constraint becomes an equality with a Lagrange multiplier according to the method of Lagrange multipliers, design a procedure that:\n- Computes the unconstrained minimizer $x_{\\mathrm{un}}$ by solving $\\nabla f(x) = 0$.\n- If $g(x_{\\mathrm{un}}) \\le 0$, declares the constraint inactive and sets $\\lambda = 0$, with $x^\\star = x_{\\mathrm{un}}$.\n- If $g(x_{\\mathrm{un}})  0$, enforces the constraint as active and solves simultaneously for $x^\\star$ and $\\lambda$ using the stationarity condition and the active constraint equation.\n\nAfter computing $(x^\\star, \\lambda)$, numerically check:\n1. Dual feasibility: $\\lambda \\ge 0$.\n2. Complementary slackness: $\\lambda \\, g(x^\\star) = 0$.\n\nUse a numerical tolerance of $\\varepsilon = 10^{-9}$ for comparisons. That is, treat a real number $r$ as nonnegative if $r \\ge -\\varepsilon$, and treat a product $p$ as zero if $|p| \\le \\varepsilon$.\n\nImplement a program that applies this verification to the following test suite of five cases, each specified by $(Q, c, a, b)$:\n- Case A (active constraint, general feasibility): \n  $$Q = \\begin{bmatrix}1  0\\\\ 0  1\\end{bmatrix}, \\quad c = \\begin{bmatrix}-2\\\\ -3\\end{bmatrix}, \\quad a = \\begin{bmatrix}1\\\\ 0\\end{bmatrix}, \\quad b = 1.$$\n- Case B (inactive constraint, unconstrained minimum feasible):\n  $$Q = \\begin{bmatrix}1  0\\\\ 0  1\\end{bmatrix}, \\quad c = \\begin{bmatrix}-0.5\\\\ -0.5\\end{bmatrix}, \\quad a = \\begin{bmatrix}1\\\\ 0\\end{bmatrix}, \\quad b = 1.$$\n- Case C (active constraint, non-identity Hessian):\n  $$Q = \\begin{bmatrix}2  0\\\\ 0  1\\end{bmatrix}, \\quad c = \\begin{bmatrix}-1\\\\ 0\\end{bmatrix}, \\quad a = \\begin{bmatrix}1\\\\ 1\\end{bmatrix}, \\quad b = 0.2.$$\n- Case D (near-boundary with very small multiplier):\n  $$Q = \\begin{bmatrix}1  0\\\\ 0  1\\end{bmatrix}, \\quad c = \\begin{bmatrix}-0.6\\\\ -0.6\\end{bmatrix}, \\quad a = \\begin{bmatrix}1\\\\ 1\\end{bmatrix}, \\quad b = 1.2 - 10^{-9}.$$\n- Case E (boundary equality satisfied by unconstrained minimum):\n  $$Q = \\begin{bmatrix}1  0\\\\ 0  1\\end{bmatrix}, \\quad c = \\begin{bmatrix}-1\\\\ 0\\end{bmatrix}, \\quad a = \\begin{bmatrix}1\\\\ 0\\end{bmatrix}, \\quad b = 1.$$\n\nYour program must output, for each case, a pair of booleans $[\\text{dual\\_feasible}, \\text{comp\\_slack}]$, where $\\text{dual\\_feasible}$ is true if $\\lambda \\ge 0$ up to tolerance, and $\\text{comp\\_slack}$ is true if $\\lambda \\, g(x^\\star) = 0$ up to tolerance. The final output must aggregate the results for all cases as a single line: a comma-separated list enclosed in square brackets, where each element is itself a two-element list of booleans. For example, the output format must look like\n$$[\\,[\\text{bool},\\text{bool}],\\,[\\text{bool},\\text{bool}],\\,\\dots\\,]$$\nwith no additional text.\n\nAngles or physical units do not apply. All computations are purely mathematical and dimensionless.", "solution": "The user-provided problem has been validated and is sound. It is a well-posed numerical problem in the field of constrained optimization, grounded in established mathematical principles. All necessary data are provided, and the task is clear and formally specified.\n\nThe problem requires the numerical verification of the dual feasibility and complementary slackness Karush-Kuhn-Tucker (KKT) conditions for a series of quadratic programming problems. Each problem involves minimizing a strictly convex quadratic objective function $f(x)$ subject to a single linear inequality constraint $g(x) \\le 0$. The functions are defined as:\n$$\nf(x) = \\frac{1}{2} x^\\top Q x + c^\\top x\n$$\n$$\ng(x) = a^\\top x - b \\le 0\n$$\nwhere $x, c, a \\in \\mathbb{R}^2$, $b \\in \\mathbb{R}$, and $Q \\in \\mathbb{R}^{2 \\times 2}$ is a symmetric positive definite matrix.\n\nThe solution methodology follows the standard approach for solving such problems, which first determines if the constraint is active at the optimal solution.\n\nFirst, we compute the unconstrained minimizer of $f(x)$, denoted $x_{\\mathrm{un}}$. This point is found by setting the gradient of $f(x)$ to zero. The gradient is:\n$$\n\\nabla f(x) = Qx + c\n$$\nSetting $\\nabla f(x) = 0$ gives $Qx + c = 0$. Since $Q$ is symmetric positive definite, it is invertible. Therefore, the unique unconstrained minimizer is:\n$$\nx_{\\mathrm{un}} = -Q^{-1}c\n$$\n\nNext, we evaluate the constraint function $g(x)$ at the unconstrained minimizer $x_{\\mathrm{un}}$. This determines whether the unconstrained solution is feasible. We use the provided numerical tolerance $\\varepsilon = 10^{-9}$ to handle floating-point comparisons, treating the condition $g(x_{\\mathrm{un}}) \\le 0$ as $g(x_{\\mathrm{un}}) \\le \\varepsilon$.\n\nCase 1: The constraint is inactive or satisfied at the boundary ($g(x_{\\mathrm{un}}) \\le \\varepsilon$).\nIf the unconstrained minimizer lies within or on the boundary of the feasible region, it is the optimal solution to the constrained problem.\n$$\nx^\\star = x_{\\mathrm{un}}\n$$\nIn this case, the Lagrange multiplier $\\lambda$ associated with the inequality constraint is zero.\n$$\n\\lambda = 0\n$$\n\nCase 2: The constraint is active ($g(x_{\\mathrm{un}})  \\varepsilon$).\nIf the unconstrained minimizer is outside the feasible region, the solution to the constrained problem must lie on the boundary of the feasible region, meaning the constraint is active: $g(x^\\star) = 0$. The solution $(x^\\star, \\lambda)$ is found by solving the KKT system of equations for stationarity and the active constraint:\n1. Stationarity: $\\nabla f(x^\\star) + \\lambda \\nabla g(x^\\star) = 0$\n2. Active Constraint: $g(x^\\star) = 0$\n\nThe gradient of the constraint is $\\nabla g(x) = a$. Substituting the gradients into the stationarity condition yields:\n$$\nQx^\\star + c + \\lambda a = 0\n$$\nFrom this, we can express $x^\\star$ in terms of $\\lambda$:\n$$\nx^\\star = -Q^{-1}(c + \\lambda a) = -Q^{-1}c - \\lambda Q^{-1}a = x_{\\mathrm{un}} - \\lambda Q^{-1}a\n$$\nWe substitute this expression for $x^\\star$ into the active constraint equation $a^\\top x^\\star - b = 0$:\n$$\na^\\top (x_{\\mathrm{un}} - \\lambda Q^{-1}a) - b = 0\n$$\n$$\na^\\top x_{\\mathrm{un}} - b = \\lambda (a^\\top Q^{-1} a)\n$$\nSolving for $\\lambda$, we get:\n$$\n\\lambda = \\frac{a^\\top x_{\\mathrmun} - b}{a^\\top Q^{-1} a}\n$$\nNote that since we are in the active case, the numerator $a^\\top x_{\\mathrmun} - b = g(x_{\\mathrmun})$ is positive. The denominator $a^\\top Q^{-1} a$ is also positive because $Q$ and therefore $Q^{-1}$ are positive definite and $a \\neq 0$. Thus, $\\lambda$ will be positive.\n\nAfter computing the solution pair $(x^\\star, \\lambda)$ for each case, we perform the two specified numerical checks using the tolerance $\\varepsilon = 10^{-9}$.\n1.  Dual Feasibility: $\\lambda \\ge 0$. This is checked as $\\lambda \\ge -\\varepsilon$.\n2.  Complementary Slackness: $\\lambda g(x^\\star) = 0$. This is checked as $|\\lambda g(x^\\star)| \\le \\varepsilon$. Note that $g(x^\\star) = a^\\top x^\\star - b$.\n\nThis procedure is applied to each of the five test cases provided. The results of the two checks for each case, represented as a pair of booleans, are then aggregated into a final list.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a series of quadratic optimization problems with a single linear inequality constraint\n    and verifies the KKT conditions of dual feasibility and complementary slackness.\n    \"\"\"\n    epsilon = 1e-9\n\n    test_cases = [\n        # Case A: active constraint\n        {'Q': np.array([[1.0, 0.0], [0.0, 1.0]]), 'c': np.array([-2.0, -3.0]), 'a': np.array([1.0, 0.0]), 'b': 1.0},\n        # Case B: inactive constraint\n        {'Q': np.array([[1.0, 0.0], [0.0, 1.0]]), 'c': np.array([-0.5, -0.5]), 'a': np.array([1.0, 0.0]), 'b': 1.0},\n        # Case C: active constraint, non-identity Hessian\n        {'Q': np.array([[2.0, 0.0], [0.0, 1.0]]), 'c': np.array([-1.0, 0.0]), 'a': np.array([1.0, 1.0]), 'b': 0.2},\n        # Case D: near-boundary case\n        {'Q': np.array([[1.0, 0.0], [0.0, 1.0]]), 'c': np.array([-0.6, -0.6]), 'a': np.array([1.0, 1.0]), 'b': 1.2 - 1e-9},\n        # Case E: boundary equality satisfied by unconstrained minimum\n        {'Q': np.array([[1.0, 0.0], [0.0, 1.0]]), 'c': np.array([-1.0, 0.0]), 'a': np.array([1.0, 0.0]), 'b': 1.0},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        Q, c, a, b = case['Q'], case['c'], case['a'], case['b']\n\n        # Step 1: Compute the unconstrained minimizer x_un by solving Qx + c = 0\n        Q_inv = np.linalg.inv(Q)\n        x_un = -Q_inv @ c\n\n        # Step 2: Evaluate the constraint at the unconstrained minimizer\n        g_x_un = a.T @ x_un - b\n\n        # Step 3: Determine if the constraint is active and find (x*, lambda)\n        # We check g(x_un) = epsilon to handle floating-point arithmetic.\n        # If g(x_un) is a very small positive number (= epsilon), we treat it as 0,\n        # meaning the unconstrained minimum is on the boundary and thus feasible.\n        if g_x_un = epsilon:\n            # Case 1: Constraint is inactive (or on the boundary).\n            # The unconstrained minimizer is the solution.\n            x_star = x_un\n            lambda_val = 0.0\n        else:\n            # Case 2: Constraint is active.\n            # The solution lies on the constraint boundary g(x*) = 0.\n            # Solve for lambda from the KKT system.\n            lambda_numerator = g_x_un\n            lambda_denominator = a.T @ Q_inv @ a\n            lambda_val = lambda_numerator / lambda_denominator\n            \n            # Compute the optimal x* using lambda.\n            x_star = x_un - lambda_val * (Q_inv @ a)\n\n        # Step 4: Numerically verify dual feasibility and complementary slackness\n        \n        # 1. Dual feasibility: lambda = 0\n        dual_feasible = lambda_val = -epsilon\n\n        # 2. Complementary slackness: lambda * g(x*) = 0\n        g_x_star = a.T @ x_star - b\n        comp_slack_product = lambda_val * g_x_star\n        comp_slack = abs(comp_slack_product) = epsilon\n\n        results.append([dual_feasible, comp_slack])\n\n    # Format the final output string as a list of lists of booleans,\n    # e.g., [[true,true],[true,true],...]\n    str_results = []\n    for res_pair in results:\n        str_results.append(f\"[{str(res_pair[0]).lower()},{str(res_pair[1]).lower()}]\")\n    print(f\"[{','.join(str_results)}]\")\n\nsolve()\n```", "id": "3251733"}]}