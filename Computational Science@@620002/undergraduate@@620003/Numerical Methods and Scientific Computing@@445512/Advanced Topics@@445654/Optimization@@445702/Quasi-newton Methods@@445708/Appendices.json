{"hands_on_practices": [{"introduction": "The Broyden–Fletcher–Goldfarb–Shanno (BFGS) update formula lies at the heart of one of the most effective quasi-Newton methods. Understanding its structure is the first step toward mastering the algorithm. This exercise isolates one of the two rank-one correction terms in the formula, allowing you to focus on the fundamental matrix and vector algebra involved in updating the Hessian approximation. By performing this calculation, you will build foundational skills for implementing and analyzing the full BFGS iteration. [@problem_id:2195881]", "problem": "In the field of numerical optimization, the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm is a popular quasi-Newton method for solving unconstrained nonlinear optimization problems. At each iteration, the algorithm approximates the Hessian matrix of the objective function. Starting with an initial approximation $B_k$, the next approximation $B_{k+1}$ is computed using the update formula:\n$$B_{k+1} = B_k + \\frac{y_k y_k^T}{y_k^T s_k} - \\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k}$$\nwhere $s_k$ is the step taken and $y_k$ is the change in the gradient. The update consists of adding two rank-one matrices to the current approximation $B_k$.\n\nConsider a single iteration of the BFGS algorithm where the current Hessian approximation is the $2 \\times 2$ identity matrix, $B_k = I$. The step vector is given by $s_k = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and the change in the gradient vector is $y_k = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$.\n\nCalculate the negative rank-one update term for $B_{k+1}$, which is given by the expression $-\\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k}$. Your final answer should be a $2 \\times 2$ matrix.", "solution": "We are asked to compute the negative rank-one update term\n$$-\\frac{B_{k}s_{k}s_{k}^{T}B_{k}}{s_{k}^{T}B_{k}s_{k}}$$\nfor $B_{k}=I$, $s_{k}=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$, and $y_{k}$ given but not needed for this term.\n\nFirst, use the fact that $B_{k}=I$ to simplify the numerator:\n$$B_{k}s_{k} = Is_{k} = s_{k}, \\quad \\text{and} \\quad B_{k}s_{k}s_{k}^{T}B_{k} = s_{k}s_{k}^{T}.$$\n\nNext, compute the denominator:\n$$s_{k}^{T}B_{k}s_{k} = s_{k}^{T}Is_{k} = s_{k}^{T}s_{k}.$$\nWith $s_{k}=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$, we have\n$$s_{k}^{T}s_{k} = 1^{2} + 0^{2} = 1.$$\n\nTherefore, the negative rank-one update term becomes\n$$-\\frac{B_{k}s_{k}s_{k}^{T}B_{k}}{s_{k}^{T}B_{k}s_{k}} = -\\frac{s_{k}s_{k}^{T}}{s_{k}^{T}s_{k}} = -s_{k}s_{k}^{T}.$$\n\nCompute $s_{k}s_{k}^{T}$ explicitly:\n$$s_{k}s_{k}^{T} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}\\begin{pmatrix}1 & 0\\end{pmatrix} = \\begin{pmatrix}1 & 0 \\\\ 0 & 0\\end{pmatrix}.$$\n\nHence, the negative rank-one update term is\n$$-\\begin{pmatrix}1 & 0 \\\\ 0 & 0\\end{pmatrix} = \\begin{pmatrix}-1 & 0 \\\\ 0 & 0\\end{pmatrix}.$$", "answer": "$$\\boxed{\\begin{pmatrix}-1 & 0 \\\\ 0 & 0\\end{pmatrix}}$$", "id": "2195881"}, {"introduction": "A key requirement for any reliable minimization algorithm is that each step moves towards a lower function value. In quasi-Newton methods, this is achieved by generating a \"descent direction.\" This property is guaranteed if the Hessian approximation, $B_k$, is positive definite. This practice presents a hypothetical scenario where this condition is violated. By calculating the search direction with a non-positive-definite $B_k$, you will see firsthand why this theoretical property is not just an academic detail, but a practical necessity for ensuring the algorithm makes consistent progress toward a solution. [@problem_id:2195908]", "problem": "In an iterative optimization algorithm using a quasi-Newton method, the search direction $p_k$ at iteration $k$ is determined by solving the linear system $B_k p_k = -\\nabla f_k$, where $B_k$ is the current approximation of the Hessian matrix and $\\nabla f_k$ is the gradient of the objective function $f$ at the iterate $x_k$. A direction $p_k$ is considered a descent direction if taking a small step along it decreases the function value, which mathematically corresponds to the condition $p_k^T \\nabla f_k < 0$.\n\nSuppose that at a particular iteration $k$, the gradient is given by\n$$\n\\nabla f_k = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}\n$$\nand the symmetric Hessian approximation is\n$$\nB_k = \\begin{pmatrix} 1 & 2 \\\\ 2 & 1 \\end{pmatrix}\n$$\nBased on these values, which of the following statements correctly describes the calculated search direction $p_k$?\n\nA. The search direction is an ascent direction.\n\nB. The search direction is a descent direction.\n\nC. The search direction is orthogonal to the gradient.\n\nD. The quasi-Newton equation has no solution for the search direction.\n\nE. The search direction is the zero vector, indicating the algorithm should stop.", "solution": "We use the quasi-Newton search direction definition: solve the linear system $B_{k} p_{k} = -\\nabla f_{k}$ and then test the descent condition $p_{k}^{T} \\nabla f_{k} < 0$.\n\nGiven $\\nabla f_{k} = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$ and $B_{k} = \\begin{pmatrix} 1 & 2 \\\\ 2 & 1 \\end{pmatrix}$, we solve\n$$\n\\begin{pmatrix} 1 & 2 \\\\ 2 & 1 \\end{pmatrix} \\begin{pmatrix} p_{1} \\\\ p_{2} \\end{pmatrix} = - \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix}.\n$$\nThis yields the linear system\n$$\np_{1} + 2 p_{2} = -1, \\quad 2 p_{1} + p_{2} = 2.\n$$\nFrom the first equation, $p_{1} = -1 - 2 p_{2}$. Substituting into the second gives\n$$\n2(-1 - 2 p_{2}) + p_{2} = 2 \\;\\Rightarrow\\; -2 - 4 p_{2} + p_{2} = 2 \\;\\Rightarrow\\; -3 p_{2} = 4 \\;\\Rightarrow\\; p_{2} = -\\frac{4}{3}.\n$$\nThen\n$$\np_{1} = -1 - 2\\left(-\\frac{4}{3}\\right) = -1 + \\frac{8}{3} = \\frac{5}{3}.\n$$\nHence\n$$\np_{k} = \\begin{pmatrix} \\frac{5}{3} \\\\ -\\frac{4}{3} \\end{pmatrix}.\n$$\nTo classify the direction, compute the directional derivative:\n$$\np_{k}^{T} \\nabla f_{k} = \\begin{pmatrix} \\frac{5}{3} & -\\frac{4}{3} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} = \\frac{5}{3} + \\frac{8}{3} = \\frac{13}{3} > 0.\n$$\nSince $p_{k}^{T} \\nabla f_{k} > 0$, moving a small positive step along $p_{k}$ increases $f$, so $p_{k}$ is an ascent direction. The system has a unique solution because $\\det(B_{k}) = 1 \\cdot 1 - 2 \\cdot 2 = -3 \\neq 0$, and $p_{k} \\neq 0$ and is not orthogonal to the gradient.", "answer": "$$\\boxed{A}$$", "id": "2195908"}, {"introduction": "Theory comes to life through implementation. This hands-on coding challenge invites you to build the BFGS algorithm from the ground up and test it on a function that is convex but not strictly convex—a function with a \"valley\" of optimal solutions. Such functions possess a Hessian matrix that is positive semidefinite but has a nullspace, posing a unique challenge for optimization algorithms. By observing how your implementation navigates this landscape, you will gain invaluable insight into the algorithm's behavior in directions of zero curvature, connecting the abstract mathematics of eigenvalues and definiteness to the concrete performance of the code. [@problem_id:3264863]", "problem": "Consider the task of minimizing a twice continuously differentiable convex function that is not strictly convex. Construct such a function as follows: let $f:\\mathbb{R}^2\\to\\mathbb{R}$ be defined by $f(x_1,x_2)=x_1^2$, which is convex but not strictly convex because its Hessian is positive semidefinite with a nontrivial nullspace. Implement the Broyden–Fletcher–Goldfarb–Shanno (BFGS) quasi-Newton method to minimize $f$ over $\\mathbb{R}^2$, using an inverse Hessian approximation and a backtracking line search satisfying the Armijo sufficient decrease condition. The algorithm should stop when $\\|\\nabla f(x_k)\\|_2 \\le \\varepsilon$ or after a maximum number of iterations $N_{\\max}$.\n\nBegin from the following fundamental basis:\n- The definition of convexity via the second derivative characterization: a function $f$ is convex if its Hessian $\\nabla^2 f(x)$ is positive semidefinite for all $x$ in its domain.\n- The gradient-based descent paradigm: iterative updates $x_{k+1}=x_k+\\alpha_k p_k$ where $p_k$ is a descent direction and $\\alpha_k>0$ satisfies a sufficient decrease condition.\n- The quasi-Newton approach: use an iterative approximation $H_k$ to the inverse Hessian to define the search direction $p_k=-H_k\\nabla f(x_k)$, where $H_k$ is updated to satisfy the secant condition and maintain positive definiteness.\n\nYour program must:\n- Implement the function $f(x_1,x_2)=x_1^2$ and its gradient $\\nabla f(x_1,x_2)=(2x_1,0)$.\n- Initialize $H_0=\\gamma I$, where $I$ is the identity matrix and $\\gamma>0$ is a scalar provided per test case.\n- Use a backtracking line search with Armijo condition $f(x_k+\\alpha p_k)\\le f(x_k)+c_1\\alpha\\nabla f(x_k)^\\top p_k$, with fixed $c_1$ and backtracking factor $\\beta\\in(0,1)$, both chosen by you but remaining constant across test cases.\n- Update $H_k$ according to the standard BFGS inverse update derived from the quasi-Newton secant condition, provided that the curvature condition $s_k^\\top y_k>0$ holds, where $s_k=x_{k+1}-x_k$ and $y_k=\\nabla f(x_{k+1})-\\nabla f(x_k)$.\n\nDesign a test suite with the following parameter sets, each specified as an initial point $(x_1^{(0)},x_2^{(0)})$, tolerance $\\varepsilon$, maximum iterations $N_{\\max}$, and initial scaling $\\gamma$:\n- Test $1$: $(x_1^{(0)},x_2^{(0)})=(5,7)$, $\\varepsilon=10^{-9}$, $N_{\\max}=50$, $\\gamma=1$.\n- Test $2$: $(x_1^{(0)},x_2^{(0)})=(0,3)$, $\\varepsilon=10^{-9}$, $N_{\\max}=50$, $\\gamma=1$.\n- Test $3$: $(x_1^{(0)},x_2^{(0)})=(-10,-2)$, $\\varepsilon=10^{-9}$, $N_{\\max}=50$, $\\gamma=1$.\n- Test $4$: $(x_1^{(0)},x_2^{(0)})=(10^6,10^6)$, $\\varepsilon=10^{-12}$, $N_{\\max}=100$, $\\gamma=1$.\n- Test $5$: $(x_1^{(0)},x_2^{(0)})=(5,7)$, $\\varepsilon=10^{-9}$, $N_{\\max}=50$, $\\gamma=10$.\n\nFor each test, also compute whether the second coordinate remains unchanged within a user-defined tolerance $\\tau$ by checking $|x_{2,\\text{final}}-x_{2,\\text{initial}}|\\le\\tau$, where you should set $\\tau=\\varepsilon$ for that test.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test’s result must be a list in the form $[x_{1,\\text{final}},x_{2,\\text{final}},f(x_{\\text{final}}),\\text{iterations},\\text{x2\\_unchanged}]$, where $x_{1,\\text{final}}$ and $x_{2,\\text{final}}$ are floating-point numbers, $f(x_{\\text{final}})$ is a floating-point number, $\\text{iterations}$ is an integer, and $\\text{x2\\_unchanged}$ is a boolean indicating whether $x_2$ remained unchanged within tolerance. The final output aggregates the results of all provided test cases into a single list, for example $[[x_{1,\\text{final}}^{(1)},x_{2,\\text{final}}^{(1)},\\dots],[x_{1,\\text{final}}^{(2)},x_{2,\\text{final}}^{(2)},\\dots],\\dots]$.", "solution": "The problem requires the implementation and analysis of the Broyden–Fletcher–Goldfarb–Shanno (BFGS) quasi-Newton optimization algorithm for minimizing a non-strictly convex function. The validity of the problem statement is confirmed as it is scientifically grounded in numerical optimization theory, well-posed, objective, and provides a complete set of specifications for implementation and testing.\n\nThe core of the task is to apply an iterative descent method of the form $x_{k+1} = x_k + \\alpha_k p_k$, where $x_k \\in \\mathbb{R}^2$ is the current estimate of the minimum, $p_k$ is a search direction, and $\\alpha_k > 0$ is a step length.\n\n**1. The Objective Function and its Properties**\nThe function to be minimized is $f:\\mathbb{R}^2 \\to \\mathbb{R}$ defined by:\n$$ f(x_1, x_2) = x_1^2 $$\nThe gradient of $f$ is a vector of its partial derivatives:\n$$ \\nabla f(x_1, x_2) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 2x_1 \\\\ 0 \\end{pmatrix} $$\nThe Hessian matrix of $f$ contains its second-order partial derivatives:\n$$ \\nabla^2 f(x_1, x_2) = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} \\end{pmatrix} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 0 \\end{pmatrix} $$\nA twice-differentiable function is convex if and only if its Hessian matrix is positive semidefinite for all points in its domain. The eigenvalues of $\\nabla^2 f$ are $\\lambda_1 = 2$ and $\\lambda_2 = 0$. Since all eigenvalues are non-negative, the Hessian is positive semidefinite, and thus $f$ is convex. However, for a function to be strictly convex, its Hessian must be positive definite (all eigenvalues strictly positive). The presence of a zero eigenvalue means $f$ is not strictly convex. The set of minimizers for $f$ is the entire line where $x_1=0$, i.e., $\\{(0, c) \\mid c \\in \\mathbb{R}\\}$, since $f(0, c) = 0$ is the global minimum value.\n\n**2. The BFGS Algorithm**\nThe BFGS method is a quasi-Newton method that avoids the explicit computation of the Hessian matrix and its inverse. Instead, it maintains an approximation to the inverse Hessian, denoted $H_k$, and updates it at each iteration.\n\n**a. Initialization**\nThe process starts at an initial point $x_0$. The initial inverse Hessian approximation, $H_0$, is typically set to a scaled identity matrix, $H_0 = \\gamma I$, where $\\gamma > 0$ is a scaling factor and $I$ is the identity matrix. A common choice is $\\gamma=1$.\n\n**b. Iterative Steps**\nFor each iteration $k = 0, 1, 2, \\dots$:\n1.  **Compute Search Direction**: The search direction $p_k$ is computed using the current inverse Hessian approximation and the gradient:\n    $$ p_k = -H_k \\nabla f(x_k) $$\n    This direction is analogous to the Newton direction $p_k = -[\\nabla^2 f(x_k)]^{-1} \\nabla f(x_k)$.\n\n2.  **Determine Step Length with Line Search**: The step length $\\alpha_k > 0$ is found to ensure sufficient decrease in the function value. A backtracking line search is employed. Starting with an initial guess (e.g., $\\alpha = 1$), the step length is successively reduced by a factor $\\beta \\in (0,1)$ until the Armijo condition is met:\n    $$ f(x_k + \\alpha p_k) \\le f(x_k) + c_1 \\alpha \\nabla f(x_k)^\\top p_k $$\n    Here, $c_1$ is a small constant, typically $c_1 = 10^{-4}$. We will use $c_1 = 10^{-4}$ and $\\beta = 0.5$. Once a suitable $\\alpha$ is found, we set $\\alpha_k = \\alpha$.\n\n3.  **Update Position**: The next iterate is found by taking the step:\n    $$ x_{k+1} = x_k + \\alpha_k p_k $$\n\n4.  **Update Inverse Hessian**: $H_k$ is updated to $H_{k+1}$ to incorporate information about the function's curvature gathered from the most recent step. We define:\n    -   $s_k = x_{k+1} - x_k = \\alpha_k p_k$ (step in position)\n    -   $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$ (change in gradient)\n    The BFGS update is performed only if the curvature condition $y_k^\\top s_k > 0$ holds. This condition ensures that the updated matrix $H_{k+1}$ remains positive definite if $H_k$ was. The update formula for the inverse Hessian is:\n    $$ H_{k+1} = \\left(I - \\frac{s_k y_k^\\top}{y_k^\\top s_k}\\right) H_k \\left(I - \\frac{y_k s_k^\\top}{y_k^\\top s_k}\\right) + \\frac{s_k s_k^\\top}{y_k^\\top s_k} $$\n    If the curvature condition is not met, the update is skipped, and we set $H_{k+1} = H_k$.\n\n**c. Termination**\nThe algorithm terminates when the norm of the gradient is below a specified tolerance $\\varepsilon$, i.e., $\\|\\nabla f(x_k)\\|_2 \\le \\varepsilon$, or when a maximum number of iterations, $N_{\\max}$, is reached.\n\n**3. Application to the Specific Function**\nA key observation for this problem is the structure of the gradient $\\nabla f(x) = (2x_1, 0)^\\top$. The second component is always zero. This has a profound effect on the behavior of the BFGS algorithm.\n\nLet us assume the initial inverse Hessian is diagonal, $H_0 = \\gamma I = \\begin{pmatrix} \\gamma & 0 \\\\ 0 & \\gamma \\end{pmatrix}$.\nThe initial search direction is $p_0 = -H_0 \\nabla f(x_0) = -\\begin{pmatrix} \\gamma & 0 \\\\ 0 & \\gamma \\end{pmatrix} \\begin{pmatrix} 2x_{1,0} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -2\\gamma x_{1,0} \\\\ 0 \\end{pmatrix}$.\nThe second component of the search direction is zero. Consequently, the step $x_1 = x_0 + \\alpha_0 p_0$ will only change the first coordinate of $x_0$. The second coordinate, $x_{2,1}$, will be equal to $x_{2,0}$.\n\nNow consider the vectors for the BFGS update:\n$s_0 = x_1 - x_0 = \\alpha_0 p_0 = (-2\\alpha_0\\gamma x_{1,0}, 0)^\\top$.\n$y_0 = \\nabla f(x_1) - \\nabla f(x_0) = (2x_{1,1} - 2x_{1,0}, 0)^\\top$.\nBoth $s_0$ and $y_0$ have their second components equal to zero. Let's denote them as $s_0 = (s_{1,0}, 0)^\\top$ and $y_0 = (y_{1,0}, 0)^\\top$.\n\nWhen we compute the matrices needed for the update, we find that they are structured such that they do not introduce any off-diagonal elements into the new Hessian approximation:\n$s_0 y_0^\\top = \\begin{pmatrix} s_{1,0}y_{1,0} & 0 \\\\ 0 & 0 \\end{pmatrix}$\n$s_0 s_0^\\top = \\begin{pmatrix} s_{1,0}^2 & 0 \\\\ 0 & 0 \\end{pmatrix}$\nGiven that $H_0$ is diagonal, and all matrices in the update formula have a structure that preserves diagonality ($V H V^\\top$ where $H$ and $V$ are diagonal), the resulting matrix $H_1$ will also be diagonal.\n\nBy induction, if $H_k$ is diagonal, then $p_k = -H_k \\nabla f(x_k)$ will have a zero in its second component. This implies $s_k$ will have a zero second component. $y_k$ will also have a zero second component. Therefore, $H_{k+1}$ will remain diagonal.\n\nThis proves a critical property for this problem: **if the initial inverse Hessian approximation $H_0$ is diagonal, all subsequent approximations $H_k$ will be diagonal, and the algorithm will never alter the $x_2$ coordinate of the iterates.** The optimization proceeds entirely within the subspace defined by the $x_1$ axis, effectively reducing the problem to a one-dimensional search for the minimum of $g(x_1) = x_1^2$.\n\nThe provided test cases all use $H_0 = \\gamma I$, which is a diagonal matrix. Therefore, we expect the $x_2$ coordinate to remain unchanged in all tests. The algorithm will converge on the point $(0, x_{2,0})$, which is one of the points in the set of global minimizers. For a quadratic function, BFGS with an exact line search is known to converge in at most $n$ iterations, where $n$ is the dimension of the space. Here, due to the problem's specific structure, we observe even faster convergence (1 or 2 steps).", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef bfgs_minimize(f, grad_f, x0, epsilon, n_max, gamma):\n    \"\"\"\n    Minimizes a function using the BFGS quasi-Newton method.\n\n    Args:\n        f: The objective function.\n        grad_f: The gradient of the objective function.\n        x0: The initial point.\n        epsilon: The tolerance for the gradient norm stopping criterion.\n        n_max: The maximum number of iterations.\n        gamma: The scaling factor for the initial inverse Hessian approximation.\n    \n    Returns:\n        A tuple (x_final, f_final, k) containing the final point,\n        final function value, and number of iterations.\n    \"\"\"\n    # Line search parameters\n    c1 = 1e-4\n    beta = 0.5\n    \n    x = np.array(x0, dtype=float)\n    n = len(x)\n    H = gamma * np.identity(n)\n    \n    for k in range(n_max):\n        g = grad_f(x)\n        \n        # Check stopping criterion\n        if np.linalg.norm(g) <= epsilon:\n            return x, f(x), k\n            \n        # Compute search direction\n        p = -H @ g\n        \n        # Backtracking line search (Armijo condition)\n        alpha = 1.0\n        f_x = f(x)\n        g_dot_p = g.T @ p\n        \n        while True:\n            x_new = x + alpha * p\n            if f(x_new) <= f_x + c1 * alpha * g_dot_p:\n                break\n            alpha *= beta\n        \n        s = x_new - x\n        x = x_new\n        \n        g_new = grad_f(x)\n        y = g_new - g\n        \n        # BFGS inverse Hessian update\n        # Check curvature condition\n        y_dot_s = y.T @ s\n        if y_dot_s > 1e-12: # Use a small threshold for numerical stability\n            rho = 1.0 / y_dot_s\n            I = np.identity(n)\n            \n            term1 = I - rho * np.outer(s, y)\n            term2 = I - rho * np.outer(y, s)\n            \n            H = term1 @ H @ term2 + rho * np.outer(s, s)\n            \n    return x, f(x), n_max\n\ndef solve():\n    \"\"\"\n    Main function to run the BFGS algorithm on the specified test cases.\n    \"\"\"\n    # The objective function f(x1, x2) = x1^2\n    def f(x):\n        return x[0]**2\n\n    # The gradient of the objective function\n    def grad_f(x):\n        return np.array([2 * x[0], 0.0])\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'x0': (5, 7), 'eps': 1e-9, 'n_max': 50, 'gamma': 1},\n        {'x0': (0, 3), 'eps': 1e-9, 'n_max': 50, 'gamma': 1},\n        {'x0': (-10, -2), 'eps': 1e-9, 'n_max': 50, 'gamma': 1},\n        {'x0': (10**6, 10**6), 'eps': 1e-12, 'n_max': 100, 'gamma': 1},\n        {'x0': (5, 7), 'eps': 1e-9, 'n_max': 50, 'gamma': 10},\n    ]\n\n    results = []\n    for case in test_cases:\n        x0 = case['x0']\n        eps = case['eps']\n        \n        x_final, f_final, iterations = bfgs_minimize(\n            f, grad_f, x0, eps, case['n_max'], case['gamma']\n        )\n        \n        x1_final, x2_final = x_final[0], x_final[1]\n        \n        # Check if the second coordinate remained unchanged within tolerance\n        tau = eps\n        x2_unchanged = abs(x2_final - x0[1]) <= tau\n        \n        results.append([x1_final, x2_final, f_final, iterations, x2_unchanged])\n\n    # Format the final output string as specified a list of lists.\n    sub_results_str = []\n    for r in results:\n        # Manually format each sub-list to control spacing and boolean representation\n        sub_results_str.append(f\"[{r[0]},{r[1]},{r[2]},{r[3]},{str(r[4]).lower()}]\")\n    \n    final_output_str = f\"[{','.join(sub_results_str)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output_str)\n\n# The following code is for local testing and will be commented out in the final submission.\n# if __name__ == '__main__':\n#     solve()\n```", "id": "3264863"}]}