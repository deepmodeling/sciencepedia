{"hands_on_practices": [{"introduction": "Before writing complex code, it's crucial to understand the mechanics of the core algorithm. This exercise [@problem_id:2154925] has you perform a backtracking line search by hand, using the fundamental Armijo condition to find an acceptable step size. By working through the simple iterative process of testing and reducing the step size $\\alpha$, you will gain a concrete intuition for how line search methods guarantee progress towards a minimum.", "problem": "In the field of numerical optimization, line search algorithms are fundamental for determining an appropriate step size during an iterative process to minimize a function. One of the most common methods is the backtracking line search, which aims to find a step size $\\alpha > 0$ that provides a sufficient decrease in the objective function. This is often enforced by the Armijo condition.\n\nThe Armijo condition is given by the inequality:\n$$f(x_k + \\alpha p_k) \\le f(x_k) + c_1 \\alpha \\nabla f(x_k)^T p_k$$\nHere, $f$ is the objective function, $x_k$ is the current iterate, $p_k$ is the search direction, $\\nabla f(x_k)$ is the gradient of the function at $x_k$, and $c_1$ is a constant controlling the required decrease, typically chosen in the range $(0, 1)$.\n\nThe backtracking algorithm operates as follows:\n1.  Begin with an initial trial step size, $\\alpha = \\bar{\\alpha}$.\n2.  Check if the Armijo condition is satisfied for the current $\\alpha$.\n3.  If the condition is not met, reduce the step size by a backtracking factor $\\rho \\in (0, 1)$ such that the new step size becomes $\\alpha \\leftarrow \\rho \\alpha$.\n4.  Repeat from step 2 until an acceptable $\\alpha$ is found.\n\nConsider the optimization of the one-dimensional objective function $f(x) = x^4$. We are at the current iterate $x_k = 1$ and have chosen a descent direction $p_k = -1$. The parameters for the backtracking line search are configured as follows:\n- Initial trial step size: $\\bar{\\alpha} = 1$\n- Sufficient decrease constant: $c_1 = 0.8$\n- Backtracking factor: $\\rho = 0.5$\n\nYour task is to execute the backtracking line search procedure and determine the first accepted step size $\\alpha$.", "solution": "We are given the objective function $f(x) = x^{4}$, current iterate $x_{k} = 1$, and search direction $p_{k} = -1$. The Armijo condition for a trial step size $\\alpha > 0$ is\n$$\nf(x_{k} + \\alpha p_{k}) \\le f(x_{k}) + c_{1}\\alpha \\nabla f(x_{k})^{T} p_{k}.\n$$\nCompute the gradient: in one dimension, $\\nabla f(x) = f'(x) = 4x^{3}$. At $x_{k} = 1$, we have\n$$\nf(1) = 1, \\qquad \\nabla f(1) = 4, \\qquad \\nabla f(1)^{T} p_{k} = 4 \\cdot (-1) = -4.\n$$\nWith $c_{1} = \\frac{4}{5}$, the Armijo condition becomes\n$$\nf(1 + \\alpha(-1)) \\le 1 + \\frac{4}{5}\\alpha(-4) \\quad \\Longleftrightarrow \\quad (1 - \\alpha)^{4} \\le 1 - \\frac{16}{5}\\alpha.\n$$\nBacktracking starts with $\\alpha = \\bar{\\alpha} = 1$ and reduces $\\alpha \\leftarrow \\rho \\alpha$ with $\\rho = \\frac{1}{2}$ until the inequality holds.\n\nTest $\\alpha = 1$:\n$$\n\\text{LHS} = (1 - 1)^{4} = 0, \\qquad \\text{RHS} = 1 - \\frac{16}{5} = -\\frac{11}{5}.\n$$\nCheck $0 \\le -\\frac{11}{5}$: false. Reject $\\alpha = 1$.\n\nTest $\\alpha = \\frac{1}{2}$:\n$$\n\\text{LHS} = \\left(1 - \\frac{1}{2}\\right)^{4} = \\left(\\frac{1}{2}\\right)^{4} = \\frac{1}{16}, \\qquad \\text{RHS} = 1 - \\frac{16}{5}\\cdot \\frac{1}{2} = 1 - \\frac{8}{5} = -\\frac{3}{5}.\n$$\nCheck $\\frac{1}{16} \\le -\\frac{3}{5}$: false. Reject $\\alpha = \\frac{1}{2}$.\n\nTest $\\alpha = \\frac{1}{4}$:\n$$\n\\text{LHS} = \\left(1 - \\frac{1}{4}\\right)^{4} = \\left(\\frac{3}{4}\\right)^{4} = \\frac{81}{256}, \\qquad \\text{RHS} = 1 - \\frac{16}{5}\\cdot \\frac{1}{4} = 1 - \\frac{4}{5} = \\frac{1}{5}.\n$$\nCheck $\\frac{81}{256} \\le \\frac{1}{5}$, i.e., $81 \\cdot 5 \\le 256 \\cdot 1 \\iff 405 \\le 256$: false. Reject $\\alpha = \\frac{1}{4}$.\n\nTest $\\alpha = \\frac{1}{8}$:\n$$\n\\text{LHS} = \\left(1 - \\frac{1}{8}\\right)^{4} = \\left(\\frac{7}{8}\\right)^{4} = \\frac{2401}{4096}, \\qquad \\text{RHS} = 1 - \\frac{16}{5}\\cdot \\frac{1}{8} = 1 - \\frac{2}{5} = \\frac{3}{5}.\n$$\nCheck $\\frac{2401}{4096} \\le \\frac{3}{5}$, i.e., $2401 \\cdot 5 \\le 4096 \\cdot 3 \\iff 12005 \\le 12288$: true. Accept $\\alpha = \\frac{1}{8}$ as the first step size satisfying the Armijo condition.", "answer": "$$\\boxed{\\frac{1}{8}}$$", "id": "2154925"}, {"introduction": "While finding an *acceptable* step is good, finding the *optimal* step along a search direction seems even better. This exercise [@problem_id:2184823] explores the concept of an exact line search, where the goal is to find the precise step size $\\alpha$ that minimizes the objective function along the chosen direction. By comparing the result of an exact line search with that of a simple fixed step size, you will quantify the benefit of a more deliberate step-size selection strategy.", "problem": "Consider the unconstrained optimization problem of minimizing the function $f(x_1, x_2) = x_1^2 + 4x_2^2$. We are interested in finding the minimum of this function using an iterative method. Starting from the point $\\mathbf{x}_0 = (4, 1)^T$, a new iterate $\\mathbf{x}_1$ is generated according to the update rule $\\mathbf{x}_1 = \\mathbf{x}_0 - \\alpha \\nabla f(\\mathbf{x}_0)$, where $\\nabla f(\\mathbf{x}_0)$ is the gradient of $f$ evaluated at $\\mathbf{x}_0$, and $\\alpha  0$ is a scalar known as the step size.\n\nTwo different strategies for choosing the step size are to be compared.\n\nStrategy A: A fixed step size of $\\alpha_A = 0.1$ is used. Let the resulting iterate be $\\mathbf{x}_{1,A}$.\nStrategy B: An exact line search is performed to find the optimal step size, $\\alpha_B$, which is the value of $\\alpha$ that minimizes the one-dimensional function $g(\\alpha) = f(\\mathbf{x}_0 - \\alpha \\nabla f(\\mathbf{x}_0))$. Let the resulting iterate be $\\mathbf{x}_{1,B}$.\n\nCalculate the ratio $R = \\frac{f(\\mathbf{x}_{1,A})}{f(\\mathbf{x}_{1,B})}$. Report your answer for $R$ rounded to three significant figures.", "solution": "We are minimizing $f(x_{1},x_{2})=x_{1}^{2}+4x_{2}^{2}$ with gradient $\\nabla f(x_{1},x_{2})=(2x_{1},8x_{2})^{T}$. At $\\mathbf{x}_{0}=(4,1)^{T}$, the gradient is\n$$\n\\nabla f(\\mathbf{x}_{0})=(2\\cdot 4,\\,8\\cdot 1)^{T}=(8,8)^{T}.\n$$\nThe update rule is $\\mathbf{x}_{1}=\\mathbf{x}_{0}-\\alpha \\nabla f(\\mathbf{x}_{0})$.\n\nStrategy A: With $\\alpha_{A}=0.1=\\frac{1}{10}$,\n$$\n\\mathbf{x}_{1,A}=(4,1)^{T}-\\tfrac{1}{10}(8,8)^{T}=\\left(4-\\tfrac{8}{10},\\,1-\\tfrac{8}{10}\\right)^{T}=\\left(\\tfrac{16}{5},\\,\\tfrac{1}{5}\\right)^{T}.\n$$\nThen\n$$\nf(\\mathbf{x}_{1,A})=\\left(\\tfrac{16}{5}\\right)^{2}+4\\left(\\tfrac{1}{5}\\right)^{2}=\\tfrac{256}{25}+\\tfrac{4}{25}=\\tfrac{260}{25}=\\tfrac{52}{5}.\n$$\n\nStrategy B: Perform exact line search on $g(\\alpha)=f(\\mathbf{x}_{0}-\\alpha \\nabla f(\\mathbf{x}_{0}))$. Using $\\mathbf{x}_{0}=(4,1)$ and $\\nabla f(\\mathbf{x}_{0})=(8,8)$,\n$$\ng(\\alpha)=f(4-8\\alpha,\\,1-8\\alpha)=(4-8\\alpha)^{2}+4(1-8\\alpha)^{2}.\n$$\nExpand:\n$$\ng(\\alpha)=(16-64\\alpha+64\\alpha^{2})+4(1-16\\alpha+64\\alpha^{2})=20-128\\alpha+320\\alpha^{2}.\n$$\nDifferentiate and set to zero:\n$$\ng'(\\alpha)=-128+640\\alpha=0 \\;\\Rightarrow\\; \\alpha_{B}=\\tfrac{128}{640}=\\tfrac{1}{5}.\n$$\nThus\n$$\n\\mathbf{x}_{1,B}=(4,1)^{T}-\\tfrac{1}{5}(8,8)^{T}=\\left(4-\\tfrac{8}{5},\\,1-\\tfrac{8}{5}\\right)^{T}=\\left(\\tfrac{12}{5},\\,-\\tfrac{3}{5}\\right)^{T},\n$$\nand\n$$\nf(\\mathbf{x}_{1,B})=\\left(\\tfrac{12}{5}\\right)^{2}+4\\left(-\\tfrac{3}{5}\\right)^{2}=\\tfrac{144}{25}+\\tfrac{36}{25}=\\tfrac{180}{25}=\\tfrac{36}{5}.\n$$\n\nCompute the ratio\n$$\nR=\\frac{f(\\mathbf{x}_{1,A})}{f(\\mathbf{x}_{1,B})}=\\frac{\\tfrac{52}{5}}{\\tfrac{36}{5}}=\\frac{52}{36}=\\frac{13}{9}\\approx 1.444\\ldots\n$$\nRounded to three significant figures, $R=1.44$.", "answer": "$$\\boxed{1.44}$$", "id": "2184823"}, {"introduction": "The final step in mastering line searches is to translate theory into robust code. This exercise [@problem_id:3247710] challenges you to implement a steepest-descent method that uses the strong Wolfe conditions, which pair the sufficient decrease condition with a curvature condition to ensure a high-quality step. By applying your algorithm to the notoriously difficult Rosenbrock function and building in a robust fallback strategy, you will bridge the gap between simple examples and practical numerical optimization software.", "problem": "Implement a complete, runnable program that performs a line search within a steepest-descent method on the Rosenbrock function. The objective function is the two-dimensional Rosenbrock function defined for a vector $x = (x_1, x_2)$ as $f(x) = (1 - x_1)^2 + 100 (x_2 - x_1^2)^2$. The algorithm must use a principled line search based on conditions that ensure sufficient decrease and appropriate curvature along a descent direction, and it must be capable of falling back to a simple backtracking strategy when the primary line search fails to find a step length. The program should compute the sequence of step lengths $ \\alpha_k $ chosen at each iteration until convergence or until a maximum number of iterations is reached. Instead of plotting, the program must output these sequences numerically.\n\nThe program must implement the following requirements:\n\n- Use the steepest-descent direction $p_k = - \\nabla f(x_k)$ at each iteration $k$.\n- Implement a strong Wolfe line search that seeks a step length $ \\alpha $ satisfying a sufficient decrease condition and a curvature condition along the line $x_k + \\alpha p_k$.\n- If the strong Wolfe line search fails to return a valid $ \\alpha $, fall back to Armijo backtracking with a reduction factor and a minimum step length threshold.\n- Terminate the steepest-descent iterations when the Euclidean norm of the gradient $ \\lVert \\nabla f(x_k) \\rVert_2 $ is less than a specified tolerance or when the iteration count reaches a specified maximum.\n\nAssume all computations are unitless. Angles are not used. Percentages are not used.\n\nYour implementation must be tested on the following suite of cases, which together exercise typical, boundary, and edge behaviors:\n\n- Test case $1$ (typical difficult start): $x_0 = (-1.2, 1.0)$, $c_1 = 10^{-4}$, $c_2 = 0.9$, initial step guess $ \\alpha_0 = 1.0$, maximum iterations $30$, tolerance $10^{-8}$.\n- Test case $2$ (origin start): $x_0 = (0.0, 0.0)$, $c_1 = 10^{-4}$, $c_2 = 0.9$, initial step guess $ \\alpha_0 = 1.0$, maximum iterations $30$, tolerance $10^{-8}$.\n- Test case $3$ (at minimizer boundary case): $x_0 = (1.0, 1.0)$, $c_1 = 10^{-4}$, $c_2 = 0.9$, initial step guess $ \\alpha_0 = 1.0$, maximum iterations $10$, tolerance $10^{-12}$. This should produce an empty sequence because the starting point is already optimal.\n- Test case $4$ (alternative curvature parameter and initial guess): $x_0 = (-1.2, 1.0)$, $c_1 = 10^{-4}$, $c_2 = 0.1$, initial step guess $ \\alpha_0 = 0.5$, maximum iterations $20$, tolerance $10^{-8}$.\n\nFor the Armijo backtracking fallback, use a reduction factor $ \\tau = 0.5 $ and a minimum step threshold $ \\alpha_{\\min} = 10^{-8} $.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces, where each result is itself the list of the selected step lengths $ \\alpha_k $ for one test case, in order. For example, the final output must have the format $[R_1,R_2,R_3,R_4]$, where $R_i$ is the list of floats of $ \\alpha_k $ for test case $i$ (e.g., $[[0.5,0.25],[0.75],[\\,], [0.5,0.5,0.25]]$ but without any spaces).", "solution": "The problem requires the implementation of a steepest-descent optimization algorithm for the two-dimensional Rosenbrock function, incorporating a sophisticated line search procedure. The solution is presented by first detailing the mathematical principles and then describing the algorithmic components.\n\n### 1. Mathematical Formulation\n\nThe objective function is the Rosenbrock function, a standard benchmark for optimization algorithms known for its narrow, parabolic valley. For a vector $x = (x_1, x_2)$, it is defined as:\n$$\nf(x) = (1 - x_1)^2 + 100(x_2 - x_1^2)^2\n$$\nTo implement a gradient-based method like steepest descent, we must compute the gradient of $f(x)$, denoted by $\\nabla f(x)$. The partial derivatives are:\n$$\n\\frac{\\partial f}{\\partial x_1} = -2(1 - x_1) + 100 \\cdot 2(x_2 - x_1^2) \\cdot (-2x_1) = -2(1 - x_1) - 400x_1(x_2 - x_1^2)\n$$\n$$\n\\frac{\\partial f}{\\partial x_2} = 100 \\cdot 2(x_2 - x_1^2) = 200(x_2 - x_1^2)\n$$\nThus, the gradient vector is:\n$$\n\\nabla f(x) = \\begin{pmatrix} -2(1 - x_1) - 400x_1(x_2 - x_1^2) \\\\ 200(x_2 - x_1^2) \\end{pmatrix}\n$$\n\n### 2. Steepest-Descent Algorithm\n\nThe steepest-descent method is an iterative algorithm that seeks a local minimum of a function. Starting from an initial point $x_0$, it generates a sequence of points $\\{x_k\\}$ using the update rule:\n$$\nx_{k+1} = x_k + \\alpha_k p_k\n$$\nwhere $k$ is the iteration index, $p_k$ is the search direction, and $\\alpha_k  0$ is the step length. For the steepest-descent method, the search direction is chosen to be the direction of most rapid decrease, which is the negative of the gradient:\n$$\np_k = - \\nabla f(x_k)\n$$\nThe iterations continue until a termination criterion is met. In this problem, the process halts if the Euclidean norm of the gradient falls below a specified tolerance, $\\lVert \\nabla f(x_k) \\rVert_2  \\text{tolerance}$, or if the number of iterations reaches a maximum value, $k \\ge \\text{max\\_iterations}$.\n\n### 3. The Line Search Subproblem and Wolfe Conditions\n\nA crucial part of the algorithm is determining the step length $\\alpha_k$ at each iteration. This is a one-dimensional optimization problem: find an $\\alpha_k$ that minimizes the function along the search direction, $\\phi(\\alpha) = f(x_k + \\alpha p_k)$. Instead of finding the exact minimum, it is more efficient to find an $\\alpha_k$ that provides sufficient progress. The strong Wolfe conditions provide a standard and effective way to ensure this.\n\nThe **strong Wolfe conditions** consist of two inequalities that the step length $\\alpha$ must satisfy:\n1.  **Sufficient Decrease Condition (Armijo condition):**\n    $$\n    f(x_k + \\alpha p_k) \\le f(x_k) + c_1 \\alpha \\nabla f(x_k)^T p_k\n    $$\n    This condition ensures that the step length $\\alpha$ achieves a sufficient decrease in the objective function $f$. Here, $c_1$ is a constant, typically small, e.g., $c_1 = 10^{-4}$.\n\n2.  **Curvature Condition:**\n    $$\n    |\\nabla f(x_k + \\alpha p_k)^T p_k| \\le c_2 |\\nabla f(x_k)^T p_k|\n    $$\n    This condition ensures that the slope of the function at the new point is sufficiently reduced, which prevents the algorithm from taking excessively small steps. The parameter $c_2$ satisfies $c_1  c_2  1$. For descent directions, $\\nabla f(x_k)^T p_k  0$, so the condition can be written as $|\\nabla f(x_k + \\alpha p_k)^T p_k| \\le -c_2 \\nabla f(x_k)^T p_k$.\n\n### 4. Line Search Implementation\n\nA robust line search algorithm that finds a step length satisfying the strong Wolfe conditions typically involves a two-stage process: a bracketing stage followed by a zooming stage.\n\n1.  **Bracketing**: Starting with an initial guess for $\\alpha$, the algorithm generates a sequence of trial step lengths, expanding a search interval until it brackets one or more points satisfying the Wolfe conditions.\n2.  **Zooming**: Once an interval $[\\alpha_{\\text{lo}}, \\alpha_{\\text{hi}}]$ is identified that is known to contain a suitable step length, a procedure (e.g., bisection or interpolation) is used to zoom in on a point within this interval that satisfies the strong Wolfe conditions.\n\nThe implementation will include an internal iteration limit for this search. If a suitable $\\alpha$ is not found within this limit, the line search is considered to have \"failed,\" triggering the fallback mechanism.\n\n### 5. Fallback Strategy: Armijo Backtracking\n\nIf the primary strong Wolfe line search fails, the algorithm falls back to a simpler but more robust method: Armijo backtracking. This method guarantees that only the sufficient decrease condition is met. The algorithm is as follows:\n\n1.  Start with an initial step length, e.g., $\\alpha = \\alpha_0$.\n2.  While the sufficient decrease condition is not satisfied:\n    $$\n    f(x_k + \\alpha p_k)  f(x_k) + c_1 \\alpha \\nabla f(x_k)^T p_k\n    $$\n    reduce the step length by a factor $\\tau \\in (0, 1)$:\n    $$\n    \\alpha \\leftarrow \\tau \\alpha\n    $$\n3.  The process stops when the condition is met or if $\\alpha$ becomes smaller than a minimum threshold $\\alpha_{\\min}$.\n\n### 6. Summary of the Complete Algorithm\n\nThe final program integrates these components for each specified test case:\n\n1.  Initialize the starting point $x_0$ and parameters ($c_1, c_2, \\alpha_0, \\text{max\\_iterations}, \\text{tolerance}$).\n2.  Begin the main loop for $k = 0, 1, \\dots, \\text{max\\_iterations}-1$.\n3.  At each iteration $k$, compute the gradient $\\nabla f(x_k)$ and its norm. If the norm is below the tolerance, terminate and return the sequence of step lengths found so far.\n4.  Set the search direction $p_k = - \\nabla f(x_k)$.\n5.  Call the strong Wolfe line search function with an initial guess $\\alpha_0$ to find the step length $\\alpha_k$.\n6.  If the Wolfe search fails (returns a failure signal), call the Armijo backtracking function to find $\\alpha_k$. The starting $\\alpha$ for backtracking is the initial guess for the current iteration.\n7.  Store the computed step length $\\alpha_k$.\n8.  Update the position: $x_{k+1} = x_k + \\alpha_k p_k$.\n9.  After the loop terminates (either by convergence or max iterations), output the lists of all stored $\\alpha_k$ values for each test case in the specified format. The starting point $x_0 = (1.0, 1.0)$ in Test Case 3 is the global minimizer, so the gradient norm is $0$, leading to immediate termination and an empty list of step lengths.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef rosenbrock(x):\n    \"\"\"\n    Computes the Rosenbrock function value.\n    f(x) = (1 - x_1)^2 + 100 * (x_2 - x_1^2)^2\n    \"\"\"\n    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n\ndef rosenbrock_grad(x):\n    \"\"\"\n    Computes the gradient of the Rosenbrock function.\n    \"\"\"\n    grad_x1 = -2 * (1 - x[0]) - 400 * x[0] * (x[1] - x[0]**2)\n    grad_x2 = 200 * (x[1] - x[0]**2)\n    return np.array([grad_x1, grad_x2])\n\ndef _zoom(phi, phi_prime, alpha_lo, alpha_hi, phi_0, phi_prime_0, c1, c2, max_iter=20):\n    \"\"\"\n    Helper function for strong_wolfe_linesearch.\n    Narrows down an interval known to contain a point satisfying the Wolfe conditions.\n    Based on Algorithm 3.6 in Nocedal  Wright, \"Numerical Optimization\".\n    \"\"\"\n    for _ in range(max_iter):\n        # Use bisection to find a trial step length.\n        alpha_j = (alpha_lo + alpha_hi) / 2.0\n        \n        phi_j = phi(alpha_j)\n\n        if phi_j > phi_0 + c1 * alpha_j * phi_prime_0 or phi_j >= phi(alpha_lo):\n            alpha_hi = alpha_j\n        else:\n            phi_prime_j = phi_prime(alpha_j)\n            if abs(phi_prime_j) = -c2 * phi_prime_0:\n                return alpha_j\n            \n            if phi_prime_j * (alpha_hi - alpha_lo) >= 0:\n                alpha_hi = alpha_lo\n            \n            alpha_lo = alpha_j\n        \n        # Terminate if interval becomes too small to prevent precision issues\n        if abs(alpha_hi - alpha_lo)  1e-12:\n            break\n            \n    # If the loop finishes without finding a point, it failed.\n    # Return the last valid point satisfying sufficient decrease, if any.\n    if phi(alpha_lo) = phi_0 + c1 * alpha_lo * phi_prime_0 and abs(phi_prime(alpha_lo)) = -c2 * phi_prime_0:\n        return alpha_lo\n\n    return None\n\ndef strong_wolfe_linesearch(f, grad, x, p, alpha_init, c1, c2, max_iter=20):\n    \"\"\"\n    Performs a line search to find a step length satisfying the strong Wolfe conditions.\n    Based on Algorithm 3.5 in Nocedal  Wright.\n    Returns the step length alpha, or None if it fails.\n    \"\"\"\n    phi = lambda alpha: f(x + alpha * p)\n    phi_prime = lambda alpha: np.dot(grad(x + alpha * p), p)\n\n    phi_0 = phi(0)\n    phi_prime_0 = phi_prime(0)\n\n    alpha_prev = 0.0\n    alpha_i = alpha_init\n\n    for i in range(max_iter):\n        phi_i = phi(alpha_i)\n\n        if (phi_i > phi_0 + c1 * alpha_i * phi_prime_0) or (i > 0 and phi_i >= phi(alpha_prev)):\n            return _zoom(phi, phi_prime, alpha_prev, alpha_i, phi_0, phi_prime_0, c1, c2)\n\n        phi_prime_i = phi_prime(alpha_i)\n\n        if abs(phi_prime_i) = -c2 * phi_prime_0:\n            return alpha_i\n\n        if phi_prime_i >= 0:\n            return _zoom(phi, phi_prime, alpha_i, alpha_prev, phi_0, phi_prime_0, c1, c2)\n        \n        # If we reach here, expand the search. A simple expansion is used.\n        alpha_prev = alpha_i\n        alpha_i = alpha_i * 2.0\n\n    return None\n\ndef armijo_backtracking(f, grad, x, p, alpha_init, c1, tau, alpha_min):\n    \"\"\"\n    Fallback line search using Armijo backtracking.\n    \"\"\"\n    phi_0 = f(x)\n    phi_prime_0 = np.dot(grad(x), p)\n    \n    alpha = alpha_init\n\n    while f(x + alpha * p) > phi_0 + c1 * alpha * phi_prime_0:\n        alpha = tau * alpha\n        if alpha  alpha_min:\n            return alpha_min\n    \n    return alpha\n\ndef steepest_descent(x0, c1, c2, alpha_init_guess, max_iter, tol, tau, alpha_min):\n    \"\"\"\n    Main steepest descent optimization loop.\n    \"\"\"\n    x = np.array(x0, dtype=float)\n    alphas = []\n\n    for k in range(max_iter):\n        grad_val = rosenbrock_grad(x)\n        grad_norm = np.linalg.norm(grad_val)\n\n        if grad_norm  tol:\n            break\n\n        p = -grad_val\n        \n        # Primary line search: strong Wolfe\n        alpha = strong_wolfe_linesearch(rosenbrock, rosenbrock_grad, x, p, alpha_init_guess, c1, c2)\n        \n        # Fallback line search: Armijo\n        if alpha is None:\n            alpha = armijo_backtracking(rosenbrock, rosenbrock_grad, x, p, alpha_init_guess, c1, tau, alpha_min)\n            \n        alphas.append(alpha)\n        x = x + alpha * p\n\n    return alphas\n\ndef solve():\n    \"\"\"\n    Main execution function to run all test cases.\n    \"\"\"\n    test_cases = [\n        # x0, c1, c2, alpha_init_guess, max_iter, tol\n        {'x0': [-1.2, 1.0], 'c1': 1e-4, 'c2': 0.9, 'alpha_init': 1.0, 'max_iter': 30, 'tol': 1e-8},\n        {'x0': [0.0, 0.0], 'c1': 1e-4, 'c2': 0.9, 'alpha_init': 1.0, 'max_iter': 30, 'tol': 1e-8},\n        {'x0': [1.0, 1.0], 'c1': 1e-4, 'c2': 0.9, 'alpha_init': 1.0, 'max_iter': 10, 'tol': 1e-12},\n        {'x0': [-1.2, 1.0], 'c1': 1e-4, 'c2': 0.1, 'alpha_init': 0.5, 'max_iter': 20, 'tol': 1e-8},\n    ]\n\n    # Armijo fallback parameters\n    tau = 0.5\n    alpha_min = 1e-8\n\n    all_results = []\n    for case in test_cases:\n        alphas = steepest_descent(\n            case['x0'], case['c1'], case['c2'], case['alpha_init'],\n            case['max_iter'], case['tol'], tau, alpha_min\n        )\n        all_results.append(alphas)\n\n    # Format the output string to match the required format exactly.\n    result_strings = [str(r).replace(' ', '') for r in all_results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3247710"}]}