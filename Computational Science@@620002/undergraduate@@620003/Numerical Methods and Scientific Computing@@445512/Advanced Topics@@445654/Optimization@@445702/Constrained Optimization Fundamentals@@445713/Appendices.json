{"hands_on_practices": [{"introduction": "The Karush-Kuhn-Tucker (KKT) conditions are the cornerstone of constrained optimization theory. This exercise demonstrates their power by asking you to solve for the projection of a vector onto the probability simplex—a fundamental task in machine learning and statistics [@problem_id:3217521]. By carefully setting up and solving the KKT system, you will uncover a surprisingly elegant structure for the optimal solution, revealing its relationship to a simple thresholding operation.", "problem": "Consider the Euclidean projection of a vector $v \\in \\mathbb{R}^{n}$ onto the probability simplex, defined as the set $\\Delta^{n} = \\{ x \\in \\mathbb{R}^{n} : x_{i} \\ge 0 \\text{ for all } i, \\ \\sum_{i=1}^{n} x_{i} = 1 \\}$. The projection is the solution of the optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\frac{1}{2} \\| x - v \\|^{2} \\quad \\text{subject to} \\quad x_{i} \\ge 0 \\ \\text{for all } i, \\ \\sum_{i=1}^{n} x_{i} = 1.\n$$\nStarting from first principles of convex optimization, use the Karush–Kuhn–Tucker (KKT) conditions to derive the necessary and sufficient optimality conditions for this problem. In particular, introduce a single Lagrange multiplier for the equality constraint and appropriate multipliers for the nonnegativity constraints, and then eliminate the inequality multipliers using complementary slackness to obtain an explicit structural form of the optimizer in terms of $v$ and a scalar threshold. \n\nThen, apply your derivation to the specific case $n = 6$ with \n$$\nv = \\begin{pmatrix} 1.2 \\\\ 0.9 \\\\ 0.7 \\\\ 0.4 \\\\ 0.05 \\\\ -0.2 \\end{pmatrix}.\n$$\nCompute the value of the equality-constraint multiplier (the scalar threshold) that enforces the simplex constraint in your explicit form. Provide the exact value. Your final answer must be a single real number.", "solution": "We begin from the foundational principles of convex optimization. The objective function $f(x) = \\frac{1}{2} \\| x - v \\|^{2}$ is strictly convex because its Hessian is the identity matrix, which is positive definite. The constraints $x_{i} \\ge 0$ for all $i$ and $\\sum_{i=1}^{n} x_{i} = 1$ are affine, hence convex. Therefore, the problem is a convex optimization problem with a strictly convex objective and affine constraints, and the Karush–Kuhn–Tucker (KKT) conditions are necessary and sufficient for optimality.\n\nForm the Lagrangian with a single multiplier $\\nu \\in \\mathbb{R}$ for the equality constraint and multipliers $\\lambda_{i} \\ge 0$ for the nonnegativity constraints:\n$$\n\\mathcal{L}(x,\\nu,\\lambda) = \\frac{1}{2} \\sum_{i=1}^{n} (x_{i} - v_{i})^{2} + \\nu \\left( \\sum_{i=1}^{n} x_{i} - 1 \\right) - \\sum_{i=1}^{n} \\lambda_{i} x_{i}.\n$$\nThe KKT conditions are:\n- Primal feasibility: $x_{i} \\ge 0$ for all $i$, and $\\sum_{i=1}^{n} x_{i} = 1$.\n- Dual feasibility: $\\lambda_{i} \\ge 0$ for all $i$.\n- Complementary slackness: $\\lambda_{i} x_{i} = 0$ for all $i$.\n- Stationarity: $\\nabla_{x} \\mathcal{L} = 0$, which componentwise yields\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_{i}} = x_{i} - v_{i} + \\nu - \\lambda_{i} = 0 \\quad \\Longrightarrow \\quad x_{i} = v_{i} - \\nu + \\lambda_{i}.\n$$\n\nUse complementary slackness to eliminate $\\lambda_{i}$. There are two cases for each index $i$:\n- If $x_{i}  0$, then complementary slackness implies $\\lambda_{i} = 0$, hence $x_{i} = v_{i} - \\nu$ and thus $v_{i}  \\nu$.\n- If $x_{i} = 0$, then the stationarity condition gives $\\lambda_{i} = \\nu - v_{i}$, and dual feasibility implies $\\nu - v_{i} \\ge 0$, hence $v_{i} \\le \\nu$.\n\nCombining these, the optimizer has the explicit form\n$$\nx_{i}^{\\star} = \\max(v_{i} - \\nu, 0).\n$$\nLet $\\tau := \\nu$ denote the scalar threshold. Enforcing the equality constraint $\\sum_{i=1}^{n} x_{i}^{\\star} = 1$ yields the scalar equation\n$$\n\\sum_{i=1}^{n} \\max(v_{i} - \\tau, 0) = 1.\n$$\nEquivalently, if $A = \\{ i : v_{i}  \\tau \\}$ is the active set of indices with $x_{i}^{\\star}  0$, then\n$$\n\\sum_{i \\in A} (v_{i} - \\tau) = 1 \\quad \\Longrightarrow \\quad \\tau = \\frac{\\sum_{i \\in A} v_{i} - 1}{|A|},\n$$\nwith the consistency conditions $v_{i}  \\tau$ for $i \\in A$ and $v_{j} \\le \\tau$ for $j \\notin A$.\n\nWe now apply this to the given instance with $n = 6$ and\n$$\nv = \\begin{pmatrix} 1.2 \\\\ 0.9 \\\\ 0.7 \\\\ 0.4 \\\\ 0.05 \\\\ -0.2 \\end{pmatrix}.\n$$\nSort the entries of $v$ in nonincreasing order to test possible active set sizes $k \\in \\{1,2,\\dots,6\\}$:\n$$\nv_{(1)} = 1.2, \\quad v_{(2)} = 0.9, \\quad v_{(3)} = 0.7, \\quad v_{(4)} = 0.4, \\quad v_{(5)} = 0.05, \\quad v_{(6)} = -0.2.\n$$\nFor each $k$, define\n$$\n\\tau_{k} = \\frac{\\sum_{i=1}^{k} v_{(i)} - 1}{k},\n$$\nand check the consistency condition $v_{(k)}  \\tau_{k} \\ge v_{(k+1)}$ (with the convention $v_{(7)} = -\\infty$).\n\nCompute candidates:\n- For $k = 1$: $\\tau_{1} = \\frac{1.2 - 1}{1} = 0.2$. Check $v_{(1)}  \\tau_{1}$ is $1.2  0.2$ (true), but $\\tau_{1} \\ge v_{(2)}$ requires $0.2 \\ge 0.9$ (false). So $k = 1$ is invalid.\n- For $k = 2$: $\\tau_{2} = \\frac{1.2 + 0.9 - 1}{2} = \\frac{1.1}{2} = 0.55$. Check $v_{(2)}  \\tau_{2}$ is $0.9  0.55$ (true), but $\\tau_{2} \\ge v_{(3)}$ requires $0.55 \\ge 0.7$ (false). So $k = 2$ is invalid.\n- For $k = 3$: $\\tau_{3} = \\frac{1.2 + 0.9 + 0.7 - 1}{3} = \\frac{2.8 - 1}{3} = \\frac{1.8}{3} = 0.6$. Check $v_{(3)}  \\tau_{3}$ is $0.7  0.6$ (true), and $\\tau_{3} \\ge v_{(4)}$ is $0.6 \\ge 0.4$ (true). Thus $k = 3$ is consistent.\n\nTherefore, the equality-constraint multiplier (threshold) is $\\tau = \\tau_{3} = 0.6$. This uniquely determines the projection via $x_{i}^{\\star} = \\max(v_{i} - 0.6, 0)$, and the sum of the positive parts is indeed $1$:\n$$\n(1.2 - 0.6) + (0.9 - 0.6) + (0.7 - 0.6) = 0.6 + 0.3 + 0.1 = 1.\n$$\nHence, the required scalar threshold is exactly $0.6$.", "answer": "$$\\boxed{0.6}$$", "id": "3217521"}, {"introduction": "Understanding KKT conditions is one thing; seeing how an algorithm uses them to navigate a feasible region is another. This practice places you in the middle of an active-set method, one of the classical algorithms for quadratic programming [@problem_id:3217327]. Your task is to analyze the current state, calculate the Lagrange multipliers, and use their signs to make a critical decision: have you reached the optimum, or must you drop a constraint to find a path toward a better solution?", "problem": "Consider the convex quadratic program\nminimize over $x \\in \\mathbb{R}^2$: $f(x) = \\dfrac{1}{2} x^\\top x$\nsubject to three linear inequality constraints:\n$1)$ $x_1 \\ge 0$,\n$2)$ $x_2 \\ge 0$,\n$3)$ $x_1 + x_2 \\ge 1$.\nAssume an active-set method for quadratic programming is applied. The initial feasible point is $x^{(0)} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$. The initial working set is chosen as $\\mathcal{W}^{(0)} = \\{1, 3\\}$, that is, the constraints $x_1 \\ge 0$ and $x_1 + x_2 \\ge 1$ are enforced as equalities at $x^{(0)}$.\n\nUsing only fundamental definitions for convex quadratic programming and the Karush–Kuhn–Tucker (KKT) conditions, determine the correct next action of a classical feasible active-set iteration starting from $x^{(0)}$ with $\\mathcal{W}^{(0)}$. In particular, decide whether a constraint must be dropped from the working set to strictly improve the objective, and if so, which one.\n\nChoose exactly one option:\n\nA. Declare optimality at $x^{(0)}$ because all Lagrange multipliers associated with the active constraints in $\\mathcal{W}^{(0)}$ are nonnegative; no constraint should be dropped.\n\nB. Drop constraint $1$ (the constraint $x_1 \\ge 0$), then compute the equality-constrained search direction that maintains $x_1 + x_2 = 1$ and move along it to strictly reduce $f(x)$, reaching $x = \\begin{bmatrix} \\tfrac{1}{2} \\\\ \\tfrac{1}{2} \\end{bmatrix}$.\n\nC. Drop constraint $3$ (the constraint $x_1 + x_2 \\ge 1$), then compute the equality-constrained search direction that maintains $x_1 = 0$ and move along it to strictly reduce $f(x)$.\n\nD. Keep both constraints in the working set and take a nonzero descent step along the one-dimensional null space of the active constraints to strictly reduce $f(x)$ without modifying the working set.", "solution": "The current iterate is $x^{(0)} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$ and the working set is $\\mathcal{W}^{(0)} = \\{1, 3\\}$, corresponding to the constraints $x_1 = 0$ and $x_1 + x_2 = 1$. The point $x^{(0)}$ is the unique solution to this system. In an active-set method, when the iterate lies at the intersection of the active constraints, we must check the signs of the Lagrange multipliers to determine if the point is optimal or if a constraint must be dropped.\n\nThe stationarity condition of the Karush-Kuhn-Tucker (KKT) system for the subproblem defined by the working set states that the gradient of the objective function must be a linear combination of the gradients of the active constraints. The objective gradient is $\\nabla f(x) = x$. The constraint gradients are $\\nabla c_1(x) = [1, 0]^\\top$ and $\\nabla c_3(x) = [1, 1]^\\top$. At $x^{(0)}$, the stationarity condition is $\\nabla f(x^{(0)}) = \\lambda_1 \\nabla c_1 + \\lambda_3 \\nabla c_3$:\n$$\n\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\lambda_1 \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} + \\lambda_3 \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n$$\nThis yields a system of linear equations:\n1. $0 = \\lambda_1 + \\lambda_3$\n2. $1 = \\lambda_3$\n\nSolving this system gives $\\lambda_3 = 1$ and $\\lambda_1 = -1$.\n\nSince $\\lambda_1 = -1  0$, the KKT conditions for the original inequality-constrained problem are not met, and the current point is not optimal. The active-set method requires dropping a constraint with a negative multiplier. Here, we must drop constraint 1 ($x_1 \\ge 0$). This defines the action in option B.\n\nThe next step, as described in option B, is to find a better point by minimizing $f(x)$ subject only to the remaining constraint in the working set, $x_1 + x_2 = 1$. This new subproblem is to find the point on the line $x_1 + x_2 = 1$ closest to the origin. The solution is $x = \\begin{bmatrix} 1/2 \\\\ 1/2 \\end{bmatrix}$. Moving from $x^{(0)}$ to this point is a valid descent step that reduces the objective function from $f(x^{(0)}) = 1/2$ to $f(x) = 1/4$.\n\nOption A is incorrect because a Lagrange multiplier is negative. Option B is correct because $\\lambda_1$ is negative, so constraint 1 is dropped, and the subsequent move to the subproblem minimizer $\\begin{bmatrix} 1/2 \\\\ 1/2 \\end{bmatrix}$ is the correct procedure. Option C is incorrect because $\\lambda_3$ is positive, so constraint 3 should not be dropped. Option D is incorrect because the two active constraint gradients are linearly independent, so their null space is trivial (only the zero vector), meaning no move is possible without changing the working set.", "answer": "$$\\boxed{B}$$", "id": "3217327"}, {"introduction": "Not all optimization algorithms are created equal, and their performance in practice often hinges on subtle numerical properties. This exercise contrasts the intuitive quadratic penalty method with the more powerful augmented Lagrangian method for a simple equality-constrained problem [@problem_id:3217528]. By analyzing the Hessian matrices of the unconstrained subproblems that each method solves, you will discover the critical issue of ill-conditioning and see firsthand why the augmented Lagrangian approach offers a more numerically stable and robust path to convergence.", "problem": "Consider the equality-constrained optimization problem of minimizing the quadratic function subject to a linear constraint:\nMinimize $f(x) = \\tfrac{1}{2} x_1^2 + \\tfrac{1}{2} x_2^2$ subject to $c(x) = x_1 - 1 = 0$, where $x = (x_1, x_2) \\in \\mathbb{R}^2$. You will examine two classical approaches: the standard quadratic penalty method and the augmented Lagrangian method.\n\nBy first principles, the constrained minimizer $(x^\\star, \\lambda^\\star)$ satisfies the Karush-Kuhn-Tucker (KKT) conditions, which for an equality constraint are the stationarity and feasibility conditions\n$\\nabla f(x^\\star) + \\lambda^\\star \\nabla c(x^\\star) = 0$ and $c(x^\\star) = 0$,\nwhere $\\lambda^\\star$ is the Lagrange multiplier.\n\nThe quadratic penalty method solves a sequence of unconstrained problems of the form\n$\\min_x \\, \\phi_\\rho(x) = f(x) + \\tfrac{\\rho}{2} \\, c(x)^2$ with penalty parameter $\\rho  0$.\nThe augmented Lagrangian method solves subproblems of the form\n$\\min_x \\, \\mathcal{L}_\\rho(x, \\lambda) = f(x) + \\lambda \\, c(x) + \\tfrac{\\rho}{2} \\, c(x)^2$ with a fixed moderate $\\rho  0$ while updating the multiplier by $\\lambda_{k+1} = \\lambda_k + \\rho \\, c(x_k)$.\n\nAssume you want the constraint violation $|c(x)| = |x_1 - 1|$ to be at most a tolerance $t = 10^{-8}$. For the penalty method, let $x_\\rho$ denote the exact minimizer of $\\phi_\\rho$. For the augmented Lagrangian method, consider a fixed $\\rho = 10$ and the exact minimizer $x(\\lambda)$ of $\\mathcal{L}_\\rho(\\cdot, \\lambda)$ at each iteration with multiplier update $\\lambda_{k+1} = \\lambda_k + \\rho \\, c(x(\\lambda_k))$.\n\nWhich of the following statements is correct?\n\nA. In the penalty method, achieving $|x_{1,\\rho} - 1| \\le t$ requires $\\rho \\ge 10^8$, which makes the Hessian of $\\phi_\\rho$ equal to $\\nabla^2 \\phi_\\rho(x) = \\begin{pmatrix} 1 + \\rho  0 \\\\ 0  1 \\end{pmatrix}$ and yields a condition number $\\kappa(\\nabla^2 \\phi_\\rho) \\approx 10^8$, causing ill-conditioning; in contrast, the augmented Lagrangian method with $\\rho = 10$ has subproblem Hessian $\\nabla^2_x \\mathcal{L}_\\rho = \\begin{pmatrix} 1 + \\rho  0 \\\\ 0  1 \\end{pmatrix}$ with condition number $\\kappa \\approx 11$ and the multiplier update contracts the error by a factor $1/(1+\\rho) = 1/11$ per iteration, so it converges robustly.\n\nB. Both the penalty and augmented Lagrangian methods require taking $\\rho \\to \\infty$ to drive $|x_1 - 1| \\to 0$, so both Hessians necessarily become severely ill-conditioned.\n\nC. The quadratic penalty method yields a Hessian independent of $\\rho$, so it does not suffer ill-conditioning; meanwhile, the augmented Lagrangian method’s Hessian becomes ill-conditioned as $\\rho$ grows, making it less robust.\n\nD. The augmented Lagrangian method cannot handle linear equality constraints; only the penalty method applies in this case.\n\nSelect the correct option.", "solution": "First, we find the exact solution $(x^\\star, \\lambda^\\star)$ using the KKT conditions. Feasibility $c(x^\\star) = x_1^\\star - 1 = 0$ implies $x_1^\\star = 1$. Stationarity $\\nabla f(x^\\star) + \\lambda^\\star \\nabla c(x^\\star) = 0$ gives $(x_1^\\star, x_2^\\star) + \\lambda^\\star (1, 0) = (0, 0)$, which yields $x_2^\\star = 0$ and $x_1^\\star + \\lambda^\\star = 0$. Thus, $x^\\star = (1, 0)$ and $\\lambda^\\star = -1$.\n\n**Quadratic Penalty Method:** The unconstrained objective is $\\phi_\\rho(x) = \\tfrac{1}{2} x_1^2 + \\tfrac{1}{2} x_2^2 + \\tfrac{\\rho}{2} (x_1 - 1)^2$. Its minimizer $x_\\rho$ is found by setting $\\nabla \\phi_\\rho(x) = 0$, which yields $x_\\rho = (\\frac{\\rho}{1+\\rho}, 0)$. The constraint violation is $c(x_\\rho) = x_{1,\\rho} - 1 = -\\frac{1}{1+\\rho}$. To achieve $|c(x_\\rho)| \\le 10^{-8}$, we need $\\frac{1}{1+\\rho} \\le 10^{-8}$, which implies $\\rho \\ge 10^8 - 1$. The Hessian of the subproblem is $\\nabla^2 \\phi_\\rho(x) = \\begin{pmatrix} 1+\\rho  0 \\\\ 0  1 \\end{pmatrix}$. Its condition number is $\\kappa = 1+\\rho \\approx 10^8$, indicating severe ill-conditioning.\n\n**Augmented Lagrangian Method:** The subproblem is to minimize $\\mathcal{L}_\\rho(x, \\lambda) = f(x) + \\lambda c(x) + \\tfrac{\\rho}{2} c(x)^2$. For a fixed, moderate $\\rho > 0$, the minimizer $x(\\lambda)$ is found by setting $\\nabla_x \\mathcal{L}_\\rho = 0$, giving $x(\\lambda) = (\\frac{\\rho-\\lambda}{1+\\rho}, 0)$. The multiplier is updated via $\\lambda_{k+1} = \\lambda_k + \\rho c(x(\\lambda_k))$. The error $(\\lambda_{k+1} - \\lambda^\\star)$ can be shown to be $(\\lambda_k - \\lambda^\\star)/(1+\\rho)$. With $\\rho=10$, the error contracts by a factor of $1/11$ each iteration, so the method converges without increasing $\\rho$. The Hessian of the subproblem is $\\nabla^2_x \\mathcal{L}_\\rho = \\begin{pmatrix} 1+\\rho  0 \\\\ 0  1 \\end{pmatrix}$. For $\\rho=10$, this Hessian is $\\begin{pmatrix} 11  0 \\\\ 0  1 \\end{pmatrix}$, with a benign condition number of $\\kappa = 11$.\n\nThis analysis shows that the penalty method requires an enormous penalty parameter $\\rho$, leading to an ill-conditioned subproblem, while the augmented Lagrangian method converges robustly with a moderate $\\rho$ and well-conditioned subproblems. Statement A correctly summarizes these facts. B is wrong because the augmented Lagrangian method does not require $\\rho \\to \\infty$. C is wrong as it reverses the conditioning properties. D is wrong because the augmented Lagrangian method is well-suited for this problem.", "answer": "$$\\boxed{A}$$", "id": "3217528"}]}