{"hands_on_practices": [{"introduction": "The heart of the simplex method is the pivot operation, a systematic procedure that moves from one basic feasible solution to an adjacent one with a better objective value. This exercise focuses on executing a single, complete pivot operation for a standard maximization problem. Mastering these fundamental mechanics—identifying the entering and leaving variables and updating the tableau—is the essential first step toward solving any linear program with the simplex algorithm. [@problem_id:2221017]", "problem": "A marketing analytics firm is using linear programming to determine the optimal allocation of a client's advertising budget. The goal is to maximize customer reach. The decision variables are $x_1$, the number of ad units purchased on social media, and $x_2$, the number of ad units purchased on search engines. The problem has been formulated as a standard maximization problem, and the initial simplex tableau is given below. The variables $s_1$ and $s_2$ are slack variables representing unused budget and staff hours, respectively. The variable $Z$ represents the total customer reach to be maximized.\n\nThe rows of the tableau correspond to the objective function $Z$, and the two constraints involving $s_1$ and $s_2$ as basic variables. The columns correspond to the variables $Z, x_1, x_2, s_1, s_2$, and the Right-Hand Side (RHS) values.\n\nThe initial tableau is:\n$$\n\\begin{array}{c|cccccc}\n\\text{Basic Variable} & Z & x_1 & x_2 & s_1 & s_2 & \\text{RHS} \\\\\n\\hline\nZ & 1 & -5 & -4 & 0 & 0 & 0 \\\\\ns_1 & 0 & 10 & 8 & 1 & 0 & 80 \\\\\ns_2 & 0 & 2 & 3 & 0 & 1 & 24 \\\\\n\\end{array}\n$$\n\nPerform exactly one complete pivot operation according to the standard simplex algorithm for maximization problems. Determine the updated values for the coefficient of $x_2$ in the Z-row, the coefficient of $s_1$ in the Z-row, and the new value of the objective function $Z$ (the RHS value of the Z-row).\n\nYour answer should be a set of three numerical values presented in a row matrix in the specified order: (coefficient of $x_2$, coefficient of $s_1$, value of $Z$).", "solution": "We apply the standard simplex pivot rules for a maximization problem.\n\n1) Identify the entering variable: in the Z-row, the most negative coefficient among decision variables is $-5$ for $x_1$, so $x_1$ enters.\n\n2) Perform the minimum ratio test on the $x_1$ column to choose the leaving variable:\n$$\n\\frac{80}{10}=8 \\quad \\text{for } s_1, \\qquad \\frac{24}{2}=12 \\quad \\text{for } s_2.\n$$\nThe minimum ratio is $8$, so $s_1$ leaves. The pivot element is $10$ in the $s_1$ row and $x_1$ column.\n\n3) Pivot to make the pivot element unity and eliminate $x_1$ from other rows.\n\nNormalize the pivot row ($s_1$ row) by dividing by $10$:\n$$\ns_1\\text{-row}: \\quad [0,\\;10,\\;8,\\;1,\\;0\\;|\\;80] \\;\\to\\; [0,\\;1,\\;\\tfrac{4}{5},\\;\\tfrac{1}{10},\\;0\\;|\\;8].\n$$\n\nEliminate $x_1$ from the Z-row by adding $5$ times the new $s_1$ row to the Z-row:\n$$\nZ\\text{-row}: \\quad [1,\\;-5,\\;-4,\\;0,\\;0\\;|\\;0] + 5\\cdot[0,\\;1,\\;\\tfrac{4}{5},\\;\\tfrac{1}{10},\\;0\\;|\\;8]\n= [1,\\;0,\\;0,\\;\\tfrac{1}{2},\\;0\\;|\\;40].\n$$\n\n(Eliminating $x_1$ from the $s_2$ row is part of the complete pivot but is not needed for the requested Z-row coefficients.)\n\nTherefore, after this single pivot, the coefficient of $x_2$ in the Z-row is $0$, the coefficient of $s_1$ in the Z-row is $\\tfrac{1}{2}$, and the new objective value $Z$ (RHS of the Z-row) is $40$.", "answer": "$$\\boxed{\\begin{pmatrix}0 & \\frac{1}{2} & 40\\end{pmatrix}}$$", "id": "2221017"}, {"introduction": "While the standard simplex method starts with a feasible solution and seeks optimality, the dual simplex method works from the other direction. It begins with a dual-feasible (but primal-infeasible) solution and works to restore primal feasibility, making it an incredibly powerful tool for sensitivity analysis. This practice will guide you through a single iteration of the dual simplex method, highlighting its unique rules for choosing pivot elements when the current solution is optimal but not feasible. [@problem_id:2212987]", "problem": "Consider the following simplex tableau for a standard maximization problem, where $z$ is the objective function and the variables $x_3, x_4, x_5$ are currently the basic variables.\n\n$$\n\\begin{array}{c|cccccc|c}\n\\text{Basis} & z & x_1 & x_2 & x_3 & x_4 & x_5 & \\text{RHS} \\\\\n\\hline\nz & 1 & 3 & 2 & 0 & 0 & 0 & 50 \\\\\n\\hline\nx_3 & 0 & 1 & 3 & 1 & 0 & 0 & 10 \\\\\nx_4 & 0 & -1 & 2 & 0 & 1 & 0 & -4 \\\\\nx_5 & 0 & -2 & -1 & 0 & 0 & 1 & -5 \\\\\n\\end{array}\n$$\n\nPerform exactly one pivot operation using the dual simplex method. After this single iteration, identify the new set of basic variables. Report the values of these new basic variables and the value of the objective function $z$. Arrange the values of the basic variables in increasing order of their variable index. Your answer should be a single row matrix containing these values, followed by the value of $z$. For example, if the new basic variables are $x_a, x_b, x_c$ with $a<b<c$, you should report your answer in the format $[x_a, x_b, x_c, z]$.", "solution": "We apply the dual simplex method because the current basis $\\{x_3,x_4,x_5\\}$ yields negative right-hand sides for $x_4$ and $x_5$, indicating primal infeasibility, while the reduced costs in the $z$-row for nonbasic variables $x_1,x_2$ are nonnegative, indicating dual feasibility under the $z_j-c_j$ convention.\n\nStep 1: Choose the leaving variable as the basic variable with the most negative RHS. From the tableau, the RHS values for basic rows are $10$ (for $x_3$), $-4$ (for $x_4$), and $-5$ (for $x_5$). The most negative is $-5$, so $x_5$ leaves.\n\nStep 2: Choose the entering variable using the dual simplex ratio test. For the pivot row (row of $x_5$), consider columns $j$ with $a_{ij}<0$. In the $x_5$-row, $a_{i1}=-2$ and $a_{i2}=-1$, so candidates are $x_1$ and $x_2$. With the $z$-row entries interpreted as $z_j-c_j$, we compute\n$$\n\\frac{z_1-c_1}{-a_{i1}}=\\frac{3}{2},\\qquad \\frac{z_2-c_2}{-a_{i2}}=\\frac{2}{1}=2.\n$$\nThe minimum is $\\frac{3}{2}$, so $x_1$ enters. The pivot element is $-2$ at the intersection of row $x_5$ and column $x_1$.\n\nStep 3: Perform the pivot. First scale the pivot row by $-\\frac{1}{2}$ to make the pivot $1$:\n$$\nR_{5}'=\\left[\\,0,\\;1,\\;\\frac{1}{2},\\;0,\\;0,\\;-\\frac{1}{2}\\;\\middle|\\;\\frac{5}{2}\\right].\n$$\nEliminate $x_1$ from the other rows:\n\n- New $z$-row: $R_{z}'=R_{z}-3R_{5}'$ gives\n$$\n[\\,1,\\;0,\\;\\tfrac{1}{2},\\;0,\\;0,\\;\\tfrac{3}{2}\\;|\\;\\tfrac{85}{2}\\,].\n$$\n\n- New $x_3$-row: $R_{3}'=R_{3}-R_{5}'$ gives\n$$\n[\\,0,\\;0,\\;\\tfrac{5}{2},\\;1,\\;0,\\;\\tfrac{1}{2}\\;|\\;\\tfrac{15}{2}\\,].\n$$\n\n- New $x_4$-row: $R_{4}'=R_{4}+R_{5}'$ gives\n$$\n[\\,0,\\;0,\\;\\tfrac{5}{2},\\;0,\\;1,\\;-\\tfrac{1}{2}\\;|\\;-\\tfrac{3}{2}\\,].\n$$\n\nAfter this single dual simplex iteration, the basic variables are $\\{x_1,x_3,x_4\\}$, and the nonbasic variables are $\\{x_2,x_5\\}$. Setting nonbasic variables to $0$, the basic variable values are read from the RHS of their rows:\n$$\nx_1=\\frac{5}{2},\\quad x_3=\\frac{15}{2},\\quad x_4=-\\frac{3}{2}.\n$$\nThe objective value is the RHS of the $z$-row:\n$$\nz=\\frac{85}{2}.\n$$\nArranged in increasing order of indices for the basic variables $(x_1,x_3,x_4)$, the requested row is $\\left[\\frac{5}{2},\\,\\frac{15}{2},\\, -\\frac{3}{2},\\, \\frac{85}{2}\\right]$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{5}{2} & \\frac{15}{2} & -\\frac{3}{2} & \\frac{85}{2}\\end{pmatrix}}$$", "id": "2212987"}, {"introduction": "A deep understanding of linear programming requires moving beyond mechanical calculations to grasp the underlying geometry. A common point of confusion is the relationship between an unbounded feasible region and an unbounded objective function; it is tempting to assume one implies the other. This exercise challenges you to build a counterexample from first principles, proving that an unbounded feasible set does not necessarily lead to an unbounded solution, thereby clarifying the conditions for optimality. [@problem_id:2443959]", "problem": "In linear programming, a common misconception is that an unbounded feasible region forces the objective function to be unbounded. Using only first principles, construct and analyze a two-dimensional linear program that serves as a counterexample to this claim.\n\nStart from the following foundational definitions:\n- A feasible region is the set of all points that satisfy the linear constraints of the problem.\n- A set is unbounded if there exists a direction vector along which one can move arbitrarily far while remaining in the set.\n- A linear objective function is a linear form $c^{\\top}x$ in the decision vector $x$.\n\nConsider the linear program in two variables $x_{1}$ and $x_{2}$:\n- Decision variables: $x_{1} \\geq 0$, $x_{2} \\geq 0$.\n- Constraint: $x_{1} \\leq 1$.\n- Objective: maximize $z = x_{1}$.\n\nTasks:\n1. Using the definitions above, argue directly from the constraint set that the feasible region is unbounded.\n2. Using only linearity of the objective and the constraint $x_{1} \\leq 1$, determine the optimal value of the objective function.\n3. Explain why the unboundedness of the feasible region does not translate into unboundedness of the objective in this instance, referring to the existence of a feasible direction in which the objective does not increase.\n\nProvide the maximal objective value as your final answer. Express your answer as an exact number with no rounding.", "solution": "The problem presents a linear program and asks for an analysis to demonstrate that an unbounded feasible region does not necessarily imply an unbounded objective function. We shall proceed by systematically addressing each of the posed tasks, adhering strictly to first principles and the provided definitions.\n\nThe linear program is defined over the decision variables $x_1$ and $x_2$ as follows:\nMaximize $z = x_1$\nSubject to:\n$x_1 \\leq 1$\n$x_1 \\geq 0$\n$x_2 \\geq 0$\n\nLet the feasible region be denoted by the set $S$. A point $p = (x_1, x_2)$ is in $S$ if and only if its coordinates satisfy all three constraints simultaneously. Thus, $S = \\{ (x_1, x_2) \\in \\mathbb{R}^2 \\mid 0 \\leq x_1 \\leq 1, x_2 \\geq 0 \\}$. This set represents a semi-infinite strip in the first quadrant of the Cartesian plane.\n\n**Task 1: Unboundedness of the Feasible Region**\n\nTo prove that the feasible region $S$ is unbounded, we must show, according to the provided definition, that there exists a direction vector along which one can move arbitrarily far while remaining in the set.\n\nLet us choose a point $p_0 \\in S$. A simple choice is $p_0 = (0, 0)$, which clearly satisfies $0 \\leq 0 \\leq 1$ and $0 \\geq 0$.\nNow, let us define a direction vector $d = (0, 1)^T$. We construct a ray starting from $p_0$ in the direction of $d$. Any point on this ray can be parameterized by $\\lambda \\geq 0$ as:\n$$ p(\\lambda) = p_0 + \\lambda d = (0, 0)^T + \\lambda (0, 1)^T = (0, \\lambda)^T $$\nThe coordinates of any such point are $(x_1, x_2) = (0, \\lambda)$. We must verify if $p(\\lambda)$ remains in the feasible region $S$ for any non-negative value of $\\lambda$. We check the constraints:\n1.  $x_1 \\leq 1$: The condition is $0 \\leq 1$, which is true.\n2.  $x_1 \\geq 0$: The condition is $0 \\geq 0$, which is true.\n3.  $x_2 \\geq 0$: The condition is $\\lambda \\geq 0$, which is true by our definition of the parameter $\\lambda$.\n\nSince all constraints are satisfied for any $\\lambda \\geq 0$, any point on the ray $(0, \\lambda)$ is in the feasible set $S$. As we can choose $\\lambda$ to be arbitrarily large, we can move an arbitrary distance from the origin along the direction $d=(0,1)^T$ and remain within $S$. Therefore, by the given definition, the feasible region $S$ is unbounded.\n\n**Task 2: Optimal Value of the Objective Function**\n\nThe objective is to maximize the function $z = x_1$ for all points $(x_1, x_2)$ in the feasible region $S$. The definition of $S$ includes the constraint $x_1 \\leq 1$. This constraint, by its very nature, imposes an upper bound on the possible values of $x_1$.\n\nThe objective function $z$ is a linear function that depends solely on the variable $x_1$. To maximize $z$, we must find the maximum possible value for $x_1$ that is permitted within the feasible region $S$. The constraints defining $S$ are $0 \\leq x_1 \\leq 1$ and $x_2 \\geq 0$. From the constraint $x_1 \\leq 1$, it is clear that the value of $x_1$ can never exceed $1$.\n\nDoes this maximum value exist within the feasible region? Yes. Consider the point $p^* = (1, 0)$.\n- Is $p^*$ feasible? We check the constraints: $x_1 = 1$ satisfies $0 \\leq 1 \\leq 1$, and $x_2 = 0$ satisfies $0 \\geq 0$. Thus, $p^* \\in S$.\n- At this point, the objective function value is $z = x_1 = 1$.\n\nSince for any point $(x_1, x_2) \\in S$, we have $x_1 \\leq 1$, it follows that the objective function $z = x_1$ is bounded above by $1$. Because we have found a feasible point where this value is achieved, the maximal value of the objective function is precisely $1$.\n\n**Task 3: Explanation of Bounded Objective with Unbounded Region**\n\nThe unboundedness of the feasible region does not translate to the unboundedness of the objective function in this case. The reason lies in the relationship between the gradient of the objective function and the direction(s) of unboundedness of the feasible region.\n\nThe objective function is $z = x_1$. This can be written in vector form as $z = c^T x$, where $x = (x_1, x_2)^T$ and the cost vector (which is the gradient of $z$) is $c = (1, 0)^T$.\n\nIn Task 1, we identified a direction of unboundedness, $d = (0, 1)^T$. This vector indicates that we can move infinitely in the positive $x_2$ direction. For the objective function to be unbounded, its value must increase indefinitely as we move along such a feasible direction. Let's analyze the change in the objective function along this direction $d$.\n\nThe rate of change of the objective function $z$ in the direction $d$ is given by the directional derivative, which for a linear function is simply the dot product $c^T d$. Let us compute this value:\n$$ c^T d = (1, 0) \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = (1)(0) + (0)(1) = 0 $$\nThe result $c^T d = 0$ shows that moving along the direction of unboundedness $d$ does not change the value of the objective function at all. The direction of unboundedness is orthogonal to the direction of increase of the objective function.\n\nTo illustrate, consider any feasible point $p_0 = (x_{1,0}, x_{2,0}) \\in S$. Let's move from this point along the direction $d$. A new point is $p(\\lambda) = p_0 + \\lambda d = (x_{1,0}, x_{2,0} + \\lambda)$ for $\\lambda \\geq 0$. As long as $0 \\leq x_{1,0} \\leq 1$, this new point remains feasible for all $\\lambda \\geq 0$. The objective value at this new point is:\n$$ z(p(\\lambda)) = c^T p(\\lambda) = c^T(p_0 + \\lambda d) = c^T p_0 + \\lambda(c^T d) $$\nSince we found $c^T d = 0$, this simplifies to:\n$$ z(p(\\lambda)) = c^T p_0 = x_{1,0} $$\nThis confirms that the objective function value remains constant along any ray parallel to the $x_2$-axis within the feasible set. The unboundedness of the region exists in a direction in which the objective function does not increase. The objective function is bounded because its value depends only on $x_1$, and $x_1$ itself is bounded by the constraints.\n\nIn summary, for a linear program with an unbounded feasible region to have an unbounded objective, there must exist a feasible direction of unboundedness $d$ for which $c^T d > 0$. In this counterexample, no such direction exists; for the only direction of unboundedness, we have $c^T d = 0$.", "answer": "$$\\boxed{1}$$", "id": "2443959"}]}