{"hands_on_practices": [{"introduction": "To truly master an algorithm, we must look under the hood. This first exercise moves beyond simply applying formulas and challenges you to derive the core mechanics of the golden section search from first principles. By working with the simple, non-differentiable function $f(x) = |x - m|$, you will see precisely how the interval is reduced at each step and prove that the convergence rate is a geometric property of the method itself, independent of the specific unimodal function being minimized [@problem_id:3237350].", "problem": "Consider a minimization on a closed interval using the golden section search. Define a piecewise linear unimodal function on the interval $[0,1]$ by\n$$\nf(x) = |x - m|, \\quad m = \\frac{3}{7}.\n$$\nThis $f(x)$ has a unique global minimum at $x=m$. Let the golden ratio constant $\\varphi$ be defined by the algebraic identity $\\varphi^{2}=\\varphi+1$ with $\\varphi>0$, and let $\\tau=\\varphi^{-1}$. The golden section search initializes a bracket $[a_{0},b_{0}]=[0,1]$ and, at iteration $k$, evaluates $f$ at two internal points chosen so that the ratio of the current bracket length to the larger of the two subintervals equals $\\varphi$. At each iteration, one endpoint of the bracket is discarded and the other is kept, with $m$ remaining inside the new bracket.\n\nTasks:\n1. Starting from the defining property of the golden ratio constant (without using any shortcut formulas for the interior points), derive exact closed-form expressions for the internal points $c_{k}$ and $d_{k}$ at an arbitrary iteration $k$ in terms of $a_{k}$, $b_{k}$, and $\\tau$. Then derive the exact update rule for the bracket endpoints $[a_{k+1},b_{k+1}]$ based solely on comparisons of $f(c_{k})$ and $f(d_{k})$.\n2. Using the function $f(x)=|x-m|$ with $m=3/7$, determine the ordering of $m$, $c_{0}$, and $d_{0}$ and justify which endpoint is discarded at the first iteration. Show the updated bracket $[a_{1},b_{1}]$ and the new internal points $c_{1}$ and $d_{1}$ in exact symbolic form (expressions involving $\\tau$ only).\n3. From first principles and the definition of $\\tau$, prove that the bracket length after $n$ iterations is a geometric progression in $n$ and give its exact closed-form. Using this, derive the minimal integer number of iterations $n$ required so that the bracket length is less than or equal to $\\varepsilon = 1.0 \\times 10^{-8}$.\n4. Compare the analytically predicted iteration count in Task 3 with the observed iteration count implied by the deterministic bracket updates for the specific function $f(x)=|x-3/7|$ starting from $[0,1]$. State the difference between these two iteration counts and explain why this difference arises or does not arise.\n\nExpress your final answer as the single integer $n$ found in Task 3. No rounding instruction is necessary because the minimal integer $n$ is exact.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It provides a complete and consistent setup for analyzing the golden section search algorithm applied to a specific unimodal function. The problem is valid and can be solved.\n\nThe golden ratio constant $\\varphi$ is defined by $\\varphi^2 = \\varphi + 1$ with $\\varphi > 0$. The solution is $\\varphi = \\frac{1+\\sqrt{5}}{2}$. The constant $\\tau$ is defined as $\\tau = \\varphi^{-1} = \\frac{\\sqrt{5}-1}{2}$. The identity $\\varphi^2 = \\varphi + 1$ implies, after dividing by $\\varphi^2$, that $1 = \\frac{1}{\\varphi} + \\frac{1}{\\varphi^2}$, which translates to $1 = \\tau + \\tau^2$. This identity will be used throughout the solution.\n\n### Task 1: Derivation of Internal Points and Update Rule\n\nLet the search bracket at iteration $k$ be $[a_k, b_k]$, with length $L_k = b_k - a_k$. The algorithm evaluates the function at two internal points, $c_k$ and $d_k$, such that $a_k < c_k < d_k < b_k$.\n\nThe defining property of the golden section search is that the ratio of the length of the whole interval to the length of the larger of the two sub-intervals created by an interior point is $\\varphi$. This leads to a symmetric placement of the two interior points. Let the points $c_k$ and $d_k$ be placed such that the ratio of the full interval length $L_k$ to the length of the sub-intervals $[a_k, d_k]$ and $[c_k, b_k]$ is $\\varphi$.\n$$\n\\frac{b_k - a_k}{d_k - a_k} = \\varphi \\quad \\text{and} \\quad \\frac{b_k - a_k}{b_k - c_k} = \\varphi\n$$\nFrom these relations, we derive the positions of $c_k$ and $d_k$:\n$$\nd_k - a_k = \\frac{b_k - a_k}{\\varphi} = \\tau(b_k - a_k) \\implies d_k = a_k + \\tau(b_k - a_k)\n$$\n$$\nb_k - c_k = \\frac{b_k - a_k}{\\varphi} = \\tau(b_k - a_k) \\implies c_k = b_k - \\tau(b_k - a_k)\n$$\nThese are the closed-form expressions for the internal points $c_k$ and $d_k$ in terms of $a_k$, $b_k$, and $\\tau$.\n\nThe update rule for the bracket endpoints is based on the comparison of the function values $f(c_k)$ and $f(d_k)$. Since the function $f(x)$ is unimodal, the minimum must lie in the sub-interval that contains the point with the lower function value.\n1. If $f(c_k) \\le f(d_k)$: The minimum is located in the interval $[a_k, d_k]$ (since $c_k < d_k$ and the function is unimodal). The new bracket becomes $[a_{k+1}, b_{k+1}] = [a_k, d_k]$.\n2. If $f(c_k) > f(d_k)$: The minimum is located in the interval $[c_k, b_k]$. The new bracket becomes $[a_{k+1}, b_{k+1}] = [c_k, b_k]$.\n\nThis pair of rules defines the update for the bracket endpoints based solely on the comparison of $f(c_k)$ and $f(d_k)$.\n\n### Task 2: First Iteration Analysis\n\nThe given function is $f(x) = |x - m|$ with $m = \\frac{3}{7}$, and the initial bracket is $[a_0, b_0] = [0, 1]$.\n\nFirst, we calculate the initial internal points $c_0$ and $d_0$ using the formulas from Task 1:\n$$\nc_0 = b_0 - \\tau(b_0 - a_0) = 1 - \\tau(1 - 0) = 1 - \\tau\n$$\n$$\nd_0 = a_0 + \\tau(b_0 - a_0) = 0 + \\tau(1 - 0) = \\tau\n$$\nTo determine the ordering of $m$, $c_0$, and $d_0$, we compare their values:\n$m = \\frac{3}{7} \\approx 0.42857$\n$\\tau = \\frac{\\sqrt{5}-1}{2} \\approx 0.61803$\n$c_0 = 1 - \\tau = 1 - \\frac{\\sqrt{5}-1}{2} = \\frac{3-\\sqrt{5}}{2} \\approx 0.38197$. Note that $1-\\tau = \\tau^2$.\nSo, we have $c_0 < m < d_0$. The order is $\\frac{3-\\sqrt{5}}{2} < \\frac{3}{7} < \\frac{\\sqrt{5}-1}{2}$.\n\nNext, we compare the function values at these points:\n$f(c_0) = |c_0 - m| = |(1-\\tau) - \\frac{3}{7}|$. Since $c_0 < m$, this is $m - c_0 = \\frac{3}{7} - (1-\\tau) = \\tau - \\frac{4}{7}$.\n$f(d_0) = |d_0 - m| = |\\tau - \\frac{3}{7}|$. Since $d_0 > m$, this is $d_0 - m = \\tau - \\frac{3}{7}$.\n\nComparing the two values:\nSince $-\\frac{4}{7} < -\\frac{3}{7}$, we have $\\tau - \\frac{4}{7} < \\tau - \\frac{3}{7}$.\nTherefore, $f(c_0) < f(d_0)$.\n\nAccording to the update rule, since $f(c_0) < f(d_0)$, we discard the right endpoint $b_0$. The updated bracket is $[a_1, b_1] = [a_0, d_0] = [0, \\tau]$.\n\nThe new internal points, $c_1$ and $d_1$, for the bracket $[a_1, b_1] = [0, \\tau]$ are:\n$$\nc_1 = b_1 - \\tau(b_1 - a_1) = \\tau - \\tau(\\tau - 0) = \\tau - \\tau^2\n$$\n$$\nd_1 = a_1 + \\tau(b_1 - a_1) = 0 + \\tau(\\tau - 0) = \\tau^2\n$$\nSo, the updated bracket is $[a_1, b_1] = [0, \\tau]$, and the new internal points are $c_1 = \\tau - \\tau^2$ and $d_1 = \\tau^2$.\n\n### Task 3: Bracket Length and Number of Iterations\n\nLet $L_k = b_k - a_k$ be the length of the bracket at iteration $k$.\nAt each step $k$, the new bracket length $L_{k+1}$ is the length of either $[a_k, d_k]$ or $[c_k, b_k]$.\nLength of $[a_k, d_k]$ is $d_k - a_k = (a_k + \\tau(b_k-a_k)) - a_k = \\tau(b_k-a_k) = \\tau L_k$.\nLength of $[c_k, b_k]$ is $b_k - c_k = b_k - (b_k - \\tau(b_k-a_k)) = \\tau(b_k-a_k) = \\tau L_k$.\nIn both cases, the length of the bracket is reduced by the same factor $\\tau$:\n$$\nL_{k+1} = \\tau L_k\n$$\nThis demonstrates that the sequence of bracket lengths $\\{L_k\\}$ is a geometric progression with a common ratio of $\\tau$. The closed-form expression for the length after $n$ iterations is $L_n = \\tau^n L_0$.\nGiven the initial bracket $[0, 1]$, the initial length is $L_0 = 1 - 0 = 1$.\nThus, the bracket length after $n$ iterations is $L_n = \\tau^n$.\n\nWe want to find the minimal integer number of iterations $n$ such that the bracket length is less than or equal to $\\varepsilon = 1.0 \\times 10^{-8}$.\n$$\nL_n \\le \\varepsilon \\implies \\tau^n \\le 10^{-8}\n$$\nTaking the natural logarithm of both sides:\n$$\n\\ln(\\tau^n) \\le \\ln(10^{-8})\n$$\n$$\nn \\ln(\\tau) \\le -8 \\ln(10)\n$$\nSince $\\tau = \\frac{\\sqrt{5}-1}{2} \\approx 0.618$, it is between $0$ and $1$, which means $\\ln(\\tau)$ is negative. Dividing by $\\ln(\\tau)$ reverses the inequality sign:\n$$\nn \\ge \\frac{-8 \\ln(10)}{\\ln(\\tau)}\n$$\nWe can write $\\ln(\\tau) = \\ln(\\varphi^{-1}) = -\\ln(\\varphi)$.\n$$\nn \\ge \\frac{-8 \\ln(10)}{-\\ln(\\varphi)} = \\frac{8 \\ln(10)}{\\ln(\\varphi)}\n$$\nNow, we can substitute the values and compute:\n$$\nn \\ge \\frac{8 \\ln(10)}{\\ln\\left(\\frac{1+\\sqrt{5}}{2}\\right)} \\approx \\frac{8 \\times 2.302585}{0.481212} \\approx \\frac{18.42068}{0.481212} \\approx 38.27904\n$$\nSince $n$ must be an integer, the minimal number of iterations required is the smallest integer greater than or equal to this value, which is $\\lceil 38.27904 \\rceil = 39$.\n\n### Task 4: Predicted vs. Observed Iteration Count\n\nThe analytically predicted iteration count from Task 3 is $n=39$. This is the number of iterations required to guarantee that the bracket length $L_n$ is no larger than $\\varepsilon = 1.0 \\times 10^{-8}$.\n\nThe \"observed\" iteration count for the specific function $f(x)=|x-3/7|$ is the actual number of iterations after which the bracket length becomes less than or equal to $\\varepsilon$.\n\nAs derived in Task 3, the length of the bracket at each iteration $k$ follows the deterministic rule $L_{k+1} = \\tau L_k$, which simplifies to $L_k = \\tau^k$ for $L_0=1$. This reduction in length is a geometric property of the golden section search algorithm itself and is independent of the unimodal function being minimized. The function $f(x)$ determines *which* sub-interval is chosen (i.e., the location of the next bracket), but not its *length*.\n\nTherefore, the sequence of bracket lengths is fixed for any application of the golden section search on a given initial interval. The number of iterations required to achieve a certain length tolerance $\\varepsilon$ depends only on the initial length $L_0$, $\\varepsilon$, and the constant factor $\\tau$. It does not depend on the specific choices made at each step based on the function values.\n\nConsequently, the analytically predicted iteration count is not a 'worst-case' or 'average-case' scenario for the length reduction; it is the exact number of iterations required for the length to shrink to the specified tolerance. The observed iteration count for the specific function $f(x)=|x-3/7|$ will be identical to the one calculated.\n\nThe difference between the two iteration counts is $39 - 39 = 0$. This absence of a difference arises because the convergence rate of the bracket length in the golden section search is a constant $\\tau$ at every iteration, irrespective of the function's behavior (provided it remains unimodal).", "answer": "$$\\boxed{39}$$", "id": "3237350"}, {"introduction": "The best way to solidify theoretical knowledge is to translate it into a working tool. This practice guides you through implementing the golden section search algorithm in code, a crucial step for any student of scientific computing. You will test your implementation on a specially designed function, $f(x) = |x - \\mu| + \\beta(x - \\mu)^2$, which is unimodal but has a non-differentiable minimum, deliberately highlighting the power of this derivative-free approach where gradient-based methods would fail [@problem_id:3237407].", "problem": "Consider designing and analyzing a derivative-free search algorithm to locate the minimum of a unimodal function. A function is called unimodal on an interval if it has exactly one local minimum on that interval. In this task, you will construct a specific unimodal function whose minimum occurs at a point where the first derivative does not exist, and then implement a bracketing method based on the golden ratio to locate the minimum.\n\nConstruct the function\n$$\nf(x) = \\lvert x - \\mu \\rvert + \\beta \\,(x - \\mu)^2,\n$$\nwhere $\\mu \\in \\mathbb{R}$ and $\\beta \\in [0,\\infty)$ are parameters. The point $x = \\mu$ is the unique minimizer, and the function is unimodal because it is strictly decreasing on $(-\\infty,\\mu)$ and strictly increasing on $(\\mu,\\infty)$; moreover, $f$ is not differentiable at $x = \\mu$ due to the absolute value. This setup ensures that any derivative-based method would encounter a non-differentiable point, while a derivative-free method such as the golden section search remains applicable.\n\nStarting from the fundamental property of unimodal functions—namely, if $f$ is unimodal on $[a,b]$, then for any two interior points $c$ and $d$ with $a < c < d < b$, the sign of $f(c) - f(d)$ indicates which subinterval contains the minimum—you must implement a golden section search. The golden section search chooses $c$ and $d$ symmetrically so that one function evaluation can be reused at the next iteration after narrowing the interval. The algorithm should:\n- Start from a bracketing interval $[a,b]$ with $a < \\mu < b$.\n- Use the golden ratio to place points $c$ and $d$ in $(a,b)$ and iteratively narrow the interval containing the minimizer based on comparing $f(c)$ and $f(d)$.\n- Terminate when the interval width $b-a$ is less than a prescribed tolerance or a maximum number of iterations is reached.\n- Return the approximate minimizer as $(a+b)/2$.\n\nImplement the algorithm and test it on the following test suite of parameter sets, where each test case is a tuple $(\\mu,\\beta,a,b,\\text{tol})$:\n1. $(\\mu,\\beta,a,b,\\text{tol}) = (1.23456789, 0.1, -2.0, 5.0, 10^{-8})$.\n2. $(\\mu,\\beta,a,b,\\text{tol}) = (-3.5, 0.0, -10.0, -1.0, 10^{-7})$.\n3. $(\\mu,\\beta,a,b,\\text{tol}) = (0.0, 5.0, -0.001, 0.002, 10^{-12})$.\n4. $(\\mu,\\beta,a,b,\\text{tol}) = (2.0, 0.2, 1.5, 6.0, 10^{-10})$.\n5. $(\\mu,\\beta,a,b,\\text{tol}) = (1000.0, 0.02, 900.0, 1100.0, 10^{-6})$.\n\nAssume a maximum iteration count of $1000$ for all cases. There are no physical units and no angles involved in this problem.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,\\dots,r_5]$), where each $r_i$ is the approximate minimizer returned by the golden section search for the $i$-th test case. The outputs must be floating-point numbers.", "solution": "The user-provided problem has been analyzed and validated.\n\n### Step 1: Extract Givens\n- **Function to Minimize:** $f(x) = \\lvert x - \\mu \\rvert + \\beta \\,(x - \\mu)^2$\n- **Parameters:** $\\mu \\in \\mathbb{R}$ and $\\beta \\in [0,\\infty)$.\n- **Function Properties:** Unimodal on $\\mathbb{R}$ with a unique minimum at $x = \\mu$. The function is not differentiable at $x=\\mu$.\n- **Algorithm:** Golden section search.\n- **Initial Conditions:** A bracketing interval $[a, b]$ such that $a < \\mu < b$.\n- **Termination Criteria:**\n    1.  The interval width $b - a$ is less than a tolerance $\\text{tol}$.\n    2.  A maximum of $1000$ iterations is reached.\n- **Output of Algorithm:** The approximate minimizer is given by the midpoint of the final interval, $(a+b)/2$.\n- **Test Cases:** A suite of five test cases, each defined by a tuple $(\\mu, \\beta, a, b, \\text{tol})$:\n    1.  $(1.23456789, 0.1, -2.0, 5.0, 10^{-8})$\n    2.  $(-3.5, 0.0, -10.0, -1.0, 10^{-7})$\n    3.  $(0.0, 5.0, -0.001, 0.002, 10^{-12})$\n    4.  $(2.0, 0.2, 1.5, 6.0, 10^{-10})$\n    5.  $(1000.0, 0.02, 900.0, 1100.0, 10^{-6})$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is subjected to a rigorous validation process.\n- **Scientific Grounding:** The problem is scientifically and mathematically sound. The function $f(x) = \\lvert x - \\mu \\rvert + \\beta \\,(x - \\mu)^2$ is a valid mathematical construct. Its claimed properties are correct:\n    - For $x > \\mu$, $f(x) = (x-\\mu) + \\beta(x-\\mu)^2$. The derivative is $f'(x) = 1 + 2\\beta(x-\\mu)$. Since $\\beta \\ge 0$ and $x > \\mu$, $f'(x) > 0$, so the function is strictly increasing.\n    - For $x < \\mu$, $f(x) = -(x-\\mu) + \\beta(x-\\mu)^2$. The derivative is $f'(x) = -1 + 2\\beta(x-\\mu)$. Since $x < \\mu$, $f'(x) < 0$, so the function is strictly decreasing.\n    - This confirms the function is unimodal with a minimum at $x=\\mu$.\n    - The left-hand derivative at $\\mu$ is $-1$ and the right-hand derivative is $1$, confirming non-differentiability at the minimum.\n- **Well-Posedness:** The problem is well-posed. The golden section search is a convergent algorithm for unimodal functions, guaranteeing a unique solution exists. The initial conditions ($a < \\mu < b$) and termination criteria are clearly specified and consistent.\n- **Objectivity:** The problem is stated using precise, objective mathematical language, with no ambiguity or subjective elements.\n- **Completeness and Consistency:** All necessary parameters ($\\mu, \\beta$), initial intervals $[a, b]$, and tolerances (`tol`) are provided for each test case. The condition $a < \\mu < b$ holds for all cases. The setup is self-contained and free of contradictions.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a well-defined task in numerical optimization that can be solved algorithmically.\n\n### Solution Derivation\nThe solution involves implementing the golden section search algorithm to find the minimum of the given function $f(x)$.\n\n**Principle of Golden Section Search**\nThe golden section search is a derivative-free optimization technique used to find the minimum of a unimodal function over a closed interval. It operates by iteratively narrowing the interval that brackets the minimum. The method is named for its use of the golden ratio, which ensures that the reduction in interval size at each step is optimal and allows for the reuse of a function evaluation from the previous iteration, thereby improving efficiency.\n\n**Algorithm Design**\nLet the interval containing the minimum be $[a, b]$. The algorithm introduces two interior points, $c$ and $d$, such that $a < c < d < b$. Their positions are determined by the golden ratio conjugate, $\\tau = (\\sqrt{5}-1)/2 \\approx 0.618034$. The points are placed symmetrically:\n$$\nc = b - \\tau(b-a)\n$$\n$$\nd = a + \\tau(b-a)\n$$\nThis arrangement ensures that the ratio of the length of the larger subinterval (e.g., $[a, d]$) to the full interval $[a, b]$ is equal to $\\tau$.\n\nThe iterative procedure is as follows:\n1.  Initialize the interval brackets $[a, b]$ and calculate the initial interior points $c$ and $d$. Evaluate the function at these points to get $f(c)$ and $f(d)$.\n2.  Compare the function values:\n    - If $f(c) < f(d)$, the minimum must lie in the interval $(a, d)$ because the function is unimodal. The search interval is updated to $[a', b'] \\leftarrow [a, d]$. The key property of the golden section search is that the old point $c$ becomes the new point $d'$ relative to the new interval, so we only need to compute one new point $c'$ and one new function value. Specifically, the new assignments are:\n      - $b \\leftarrow d$\n      - $d \\leftarrow c$\n      - $f_d \\leftarrow f_c$\n      - $c \\leftarrow b - \\tau(b-a)$\n      - $f_c \\leftarrow f(c)$\n    - If $f(c) \\ge f(d)$, the minimum must lie in the interval $(c, b)$. The search interval is updated to $[a', b'] \\leftarrow [c, b]$. The old point $d$ becomes the new point $c'$. The new assignments are:\n      - $a \\leftarrow c$\n      - $c \\leftarrow d$\n      - $f_c \\leftarrow f_d$\n      - $d \\leftarrow a + \\tau(b-a)$\n      - $f_d \\leftarrow f(d)$\n3.  The process repeats until the interval width $(b-a)$ is less than the specified tolerance $\\text{tol}$ or the maximum number of iterations ($1000$) is exceeded.\n4.  The final estimate for the minimizer is the midpoint of the final interval, $(a+b)/2$.\n\nThis algorithm will be implemented and applied to the five test cases provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and tests the golden section search algorithm for a specified\n    unimodal function with a non-differentiable minimum.\n    \"\"\"\n\n    def f(x, mu, beta):\n        \"\"\"\n        The unimodal function to be minimized.\n        f(x) = |x - mu| + beta * (x - mu)^2\n        \"\"\"\n        return np.abs(x - mu) + beta * (x - mu)**2\n\n    def golden_section_search(func, a, b, tol, max_iter=1000):\n        \"\"\"\n        Finds the minimum of a unimodal function 'func' on the interval [a, b]\n        using the golden section search method.\n\n        Args:\n            func: The unimodal function to minimize.\n            a: The lower bound of the initial interval.\n            b: The upper bound of the initial interval.\n            tol: The desired tolerance for the interval width.\n            max_iter: The maximum number of iterations.\n\n        Returns:\n            The approximate location of the minimum.\n        \"\"\"\n        # Define the golden ratio conjugate\n        tau = (np.sqrt(5.0) - 1.0) / 2.0\n\n        # Set up the initial interior points\n        c = b - tau * (b - a)\n        d = a + tau * (b - a)\n\n        # Evaluate the function at the interior points\n        fc = func(c)\n        fd = func(d)\n\n        for _ in range(max_iter):\n            # Check for termination\n            if (b - a) < tol:\n                break\n            \n            # Compare function values and shrink the interval\n            if fc < fd:\n                # The minimum is in the interval [a, d]\n                b = d\n                d = c\n                fd = fc\n                c = b - tau * (b - a)\n                fc = func(c)\n            else:\n                # The minimum is in the interval [c, b]\n                a = c\n                c = d\n                fc = fd\n                d = a + tau * (b - a)\n                fd = func(d)\n        \n        # Return the midpoint of the final interval\n        return (a + b) / 2.0\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1.23456789, 0.1, -2.0, 5.0, 1e-8),\n        (-3.5, 0.0, -10.0, -1.0, 1e-7),\n        (0.0, 5.0, -0.001, 0.002, 1e-12),\n        (2.0, 0.2, 1.5, 6.0, 1e-10),\n        (1000.0, 0.02, 900.0, 1100.0, 1e-6),\n    ]\n\n    results = []\n    for mu, beta, a, b, tol in test_cases:\n        # Create a callable function instance for the current set of parameters\n        current_f = lambda x: f(x, mu, beta)\n        \n        # Calculate the result for the current case\n        approximated_minimizer = golden_section_search(current_f, a, b, tol, max_iter=1000)\n        results.append(approximated_minimizer)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.15g}' for r in results)}]\")\n\nsolve()\n```", "id": "3237407"}, {"introduction": "While the golden section search is exceptionally robust, its linear convergence can be slow. This final practice elevates your skills by guiding you to build a hybrid algorithm, a simplified version of the renowned Brent's method, that is both fast and reliable. You will combine the rapid, superlinear convergence of parabolic interpolation with the guaranteed progress of the golden section search as a fallback, learning a key design principle in numerical analysis: creating powerful routines by intelligently merging the strengths of different methods [@problem_id:3237355].", "problem": "You are asked to design and implement a robust one-dimensional unconstrained minimization routine that combines parabolic interpolation with a golden section fallback. The routine must identify an approximate minimizer of a real-valued function on a closed interval under the assumption of unimodality, and must be precise enough to meet a specified absolute tolerance.\n\nThe fundamental base for your design is as follows. A function $f:\\mathbb{R}\\to\\mathbb{R}$ is unimodal on a closed interval $[a,b]$ if there exists a point $x^\\star\\in[a,b]$ such that $f$ is nonincreasing on $[a,x^\\star]$ and nondecreasing on $[x^\\star,b]$. For such functions, a bracketing search that maintains a shrinking interval containing $x^\\star$ is guaranteed to converge if a sufficient decrease in interval length is enforced at each step. The golden ratio $\\varphi=(1+\\sqrt{5})/2$ satisfies the property that a golden section step reduces the interval length by a fixed factor $1-1/\\varphi=1/\\varphi^2$ in the worst case, ensuring monotonic progress. Quadratic (parabolic) interpolation provides a locally superlinear step by fitting the quadratic polynomial that interpolates three points and moving to its stationary point, but this step must be accepted only when it is consistent with the current bracket and provides a stable improvement.\n\nYour task is to implement a variant of the golden section search that attempts a parabolic interpolation step whenever three distinct abscissae with known function values are available, but falls back to a golden section step whenever the interpolation is not admissible. The algorithm must satisfy the following requirements.\n\n- Inputs: A function $f(x)$, a closed bracketing interval $[a,b]$ with $a<b$, an absolute tolerance $\\varepsilon>0$, and a maximum iteration count $N_{\\max}\\in\\mathbb{N}$. Angles, when present in $f(x)$, must be interpreted in radians.\n- State and step selection:\n  - Maintain a current best point $x\\in(a,b)$ with the smallest known value $f(x)$ inside $[a,b]$, and two additional distinct points $w,v\\in(a,b)$ with their values $f(w),f(v)$ for constructing a quadratic interpolant. At each iteration, attempt a parabolic interpolation step by constructing the unique quadratic polynomial that interpolates $\\{(x,f(x)),(w,f(w)),(v,f(v))\\}$ and proposing its stationary point $u$ as the next evaluation.\n  - Accept the parabolic step if and only if the three abscissae are distinct, their function values do not make the quadratic degenerate, the proposed $u$ lies strictly inside $(a,b)$, and the step magnitude from the current best $|u-x|$ is modest relative to the current bracket so as to ensure stability. If any of these conditions fail, take a golden section step from $x$ toward the center of $[a,b]$ that reduces the longer subinterval by a factor $1/\\varphi^2$, where $\\varphi=(1+\\sqrt{5})/2$.\n  - After computing $u$, evaluate $f(u)$ and update the bracketing interval $[a,b]$ so that it continues to contain a minimizer of $f$, update the triplet $(x,w,v)$ and their values to reflect the best, second-best, and third-best distinct abscissae encountered, and repeat.\n- Termination: Stop when the current bracket length is at most $\\varepsilon$, that is, when $|b-a|\\le\\varepsilon$, or when the iteration count reaches $N_{\\max}$. Return the best abscissa found $x$ as the approximate minimizer. Your program must round each reported minimizer to $8$ decimal places.\n- Output: For a given suite of test cases specified below, produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,r_3]$.\n\nTest suite. Implement your routine and apply it to the following test cases. In each case, the answer to be produced is the approximate minimizer $x^\\star$ rounded to $8$ decimal places.\n\n- Case A (strictly convex quadratic): $f(x)=(x-2)^2$ on $[0,5]$, with $\\varepsilon=10^{-10}$ and $N_{\\max}=1000$. The true minimizer is at $x^\\star=2$.\n- Case B (smooth non-symmetric convex): $f(x)=\\exp(x)-4x$ on $[-2,3]$, with $\\varepsilon=10^{-10}$ and $N_{\\max}=1000$. The true minimizer is at $x^\\star=\\log 4$.\n- Case C (smooth strongly convex with small oscillation; angles in radians): $f(x)=(x+1)^2+0.001\\sin(50x)$ on $[-3,1]$, with $\\varepsilon=10^{-10}$ and $N_{\\max}=2000$. The true minimizer is near $x^\\star=-1$.\n- Case D (nondifferentiable at minimizer): $f(x)=|x-0.3|$ on $[-1,2]$, with $\\varepsilon=10^{-10}$ and $N_{\\max}=1000$. The true minimizer is at $x^\\star=0.3$.\n- Case E (boundary minimizer): $f(x)=\\exp(5x)$ on $[-1,1]$, with $\\varepsilon=10^{-10}$ and $N_{\\max}=2000$. The true minimizer is at the left boundary $x^\\star=-1$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases A through E, for example $[r_A,r_B,r_C,r_D,r_E]$, where each $r_\\cdot$ is the rounded minimizer as a decimal string with exactly $8$ digits after the decimal point.", "solution": "The user has requested the design and implementation of a one-dimensional unconstrained minimization algorithm. The method specified is a hybrid approach, combining the rapid convergence of parabolic interpolation with the guaranteed progress of the golden section search. The algorithm must be robust and applicable to a range of test functions on a given interval, assuming the function is unimodal.\n\n### Problem Validation\n\nFirst, a critical validation of the problem statement is performed.\n\n*   **Step 1: Extract Givens**\n    *   **Algorithm:** One-dimensional minimization on a closed interval $[a,b]$ for a function $f(x)$.\n    *   **Method:** A hybrid of parabolic interpolation and golden section search.\n    *   **Assumption:** $f(x)$ is unimodal on $[a,b]$.\n    *   **Inputs:** Function $f$, interval $[a,b]$ with $a<b$, absolute tolerance $\\varepsilon > 0$, maximum iterations $N_{\\max}$. Angles in radians.\n    *   **State:** Maintain three distinct points $(x,w,v)$ and their function values $(f(x), f(w), f(v))$, where $x$ is the current best point.\n    *   **Step Logic:**\n        1.  **Attempt Parabolic Step:** Interpolate a quadratic through $(x, f(x)), (w, f(w)), (v, f(v))$ and propose its stationary point, $u$.\n        2.  **Acceptance Criteria:** The parabolic step is accepted only if the abscissae are distinct, the function values are not degenerate for interpolation, the proposed point $u$ is strictly within $(a,b)$, and the step $|u-x|$ is \"modest\".\n        3.  **Fallback Step:** If the parabolic step is not accepted, a golden section step is performed on the longer of the two sub-intervals $(a,x)$ or $(x,b)$.\n    *   **Update:** After evaluating $f(u)$, the bracket $[a,b]$ and the points $(x,w,v)$ must be updated.\n    *   **Termination:** Stop when $|b-a| \\le \\varepsilon$ or after $N_{\\max}$ iterations.\n    *   **Output:** Return the best abscissa $x$ found, rounded to $8$ decimal places, for each test case in a specific list format.\n    *   **Test Suite:**\n        *   A: $f(x)=(x-2)^2$ on $[0,5]$, $\\varepsilon=10^{-10}$, $N_{\\max}=1000$.\n        *   B: $f(x)=\\exp(x)-4x$ on $[-2,3]$, $\\varepsilon=10^{-10}$, $N_{\\max}=1000$.\n        *   C: $f(x)=(x+1)^2+0.001\\sin(50x)$ on $[-3,1]$, $\\varepsilon=10^{-10}$, $N_{\\max}=2000$.\n        *   D: $f(x)=|x-0.3|$ on $[-1,2]$, $\\varepsilon=10^{-10}$, $N_{\\max}=1000$.\n        *   E: $f(x)=\\exp(5x)$ on $[-1,1]$, $\\varepsilon=10^{-10}$, $N_{\\max}=2000$.\n\n*   **Step 2: Validate Using Extracted Givens**\n    *   **Scientific Grounding:** The problem describes a well-established and scientifically sound numerical method, often known as Brent's method. The underlying principles of quadratic interpolation and golden section search are fundamental in numerical optimization. The test functions are standard for evaluating such algorithms. The problem is scientifically grounded.\n    *   **Well-Posedness:** The objective is clear, inputs are defined, and termination conditions are specified. The problem is well-posed. Certain phrases like \"modest relative to the current bracket\" are slightly qualitative but can be made precise by adopting standard robust practices from the numerical analysis literature (e.g., comparing the proposed step to previous step sizes), which is an expected part of the implementation design.\n    *   **Objectivity:** The problem is stated in precise, objective mathematical and algorithmic language.\n    *   **Consistency:** A minor inconsistency exists in Test Case C. The function $f(x)=(x+1)^2+0.001\\sin(50x)$ is not strictly unimodal on $[-3,1]$ due to the oscillatory term, which technically violates the global assumption of unimodality. However, the function's large-scale structure is dominated by the convex $(x+1)^2$ term, and its global minimum is located near $x=-1$. In the context of numerical methods, applying an algorithm to a function that slightly deviates from its ideal assumptions is a common and valid way to test its robustness. Therefore, this is interpreted as a \"stress test\" rather than a fatal flaw invalidating the problem. The other test cases, including the non-differentiable and boundary-minimum cases, are valid and test different aspects of the algorithm's robustness.\n\n*   **Step 3: Verdict and Action**\n    *   **Verdict:** The problem is **valid**.\n    *   **Action:** Proceed with providing a complete solution.\n\n### Algorithmic Design and Solution\n\nThe requested algorithm will be implemented as a Python function. This method is a variant of Brent's algorithm for one-dimensional minimization.\n\n**Constants:**\nThe golden ratio $\\varphi = (1+\\sqrt{5})/2$ is a key constant. The algorithm uses the complementary golden ratio, $C = 1 - 1/\\varphi \\approx 0.381966$, for the golden section steps. A small machine epsilon-like value, `ZEPS`, is used to guard against floating-point comparisons and divisions by near-zero numbers.\n\n**Initialization:**\nThe state of the algorithm is defined by the interval bracket $[a,b]$ and three interior points:\n*   $x$: The point with the lowest function value found so far.\n*   $w$: The point with the second-lowest function value.\n*   $v$: The previous value of $w$.\nInitially, we lack three distinct points. We start by placing $x, w, v$ at a point determined by a single golden section step into the interval $[a,b]$. This forces the first few iterations to use the golden section method, which generates the distinct points required for the first parabolic interpolation attempt. We also initialize two variables, $d$ and $e$, to track the step sizes of the last two iterations, which are used for the \"modest step\" admissibility test.\n\n**Iteration Loop:**\nThe main loop continues until the bracket width $|b-a|$ is smaller than the specified tolerance $\\varepsilon$, or the maximum number of iterations $N_{\\max}$ is reached.\n\n1.  **Parabolic Interpolation Step:**\n    An attempt is made to fit a parabola through the three points $(x, f_x), (w, f_w), (v, f_v)$. The stationary point of this parabola, $u$, is calculated. The vertex of a parabola passing through $(x_1, y_1), (x_2, y_2), (x_3, y_3)$ is given by a standard formula. To ensure numerical stability and correctness, we use a robust formulation. The step is accepted only if:\n    a. The three points are sufficiently distinct.\n    b. The interpolating parabola is convex (i.e., it opens upwards, having a minimum), which is checked via the sign of the denominator in the vertex formula.\n    c. The proposed point $u$ lies strictly within the current bracket $[a, b]$ and is not excessively close to the boundaries.\n    d. The step size $|u-x|$ is \"modest,\" which we implement as being smaller than half the step size from two iterations ago. This prevents large, unstable jumps.\n\n2.  **Golden Section Fallback:**\n    If any of the acceptance criteria for the parabolic step fail, the algorithm falls back to the reliable golden section method. A new point $u$ is chosen in the larger of the two sub-intervals, $(a,x)$ or $(x,b)$, reducing its length by the golden ratio factor $C$. This guarantees a reduction in the bracket size and ensures convergence.\n\n3.  **State Update:**\n    After computing the new point $u$ (either via parabola or golden section) and its value $f(u)$, the algorithm updates its state:\n    *   If $f(u) \\le f(x)$, we have found a new best point. The bracket $[a,b]$ is tightened around the new minimum, and the points $x, w, v$ are updated by \"promoting\" the previous best points: the old $x$ becomes the new $w$, the old $w$ becomes the new $v$, and $u$ becomes the new $x$.\n    *   If $f(u) > f(x)$, the best point $x$ remains unchanged. The bracket is tightened by replacing the endpoint on the same side of $x$ as $u$. The points $w$ and $v$ are updated if $u$ provides a better second- or third-best point.\n\nThis hybrid strategy leverages the superlinear convergence of parabolic interpolation when the function is well-behaved near the minimum, while retaining the robust linear convergence of the golden section search in all other cases.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the minimization problems for the defined test suite and prints the results.\n    \"\"\"\n\n    # Golden ratio constant related value\n    CGOLD = 0.5 * (3.0 - np.sqrt(5.0))\n    # A small number for floating point comparisons to avoid division by zero\n    ZEPS = 1.0e-12\n\n    def minimize_hybrid(f, a, b, tol, max_iter):\n        \"\"\"\n        Finds the minimum of a unimodal function f on the interval [a, b]\n        using a combination of golden section search and parabolic interpolation.\n\n        This implementation is a variant of Brent's method.\n        \"\"\"\n        # Initial setup of points\n        x = w = v = a + CGOLD * (b - a)\n        fx = fw = fv = f(x)\n        \n        # d: current step size, e: previous step size\n        d = e = 0.0\n\n        for _ in range(max_iter):\n            # Midpoint of the bracket\n            xm = 0.5 * (a + b)\n            \n            # Termination check\n            # The tolerance check must be robust.\n            # abs(x-xm) is the distance from the best point to the center.\n            # (tol - 0.5*(b-a)) is a relative tolerance measure.\n            if abs(x - xm) = (tol - 0.5 * (b - a)) or abs(b-a) = tol:\n                return x\n\n            parabolic_step_taken = False\n            # Attempt a parabolic interpolation step if we have moved enough in the past\n            if abs(e) > tol:\n                r = (x - w) * (fx - fv)\n                q = (x - v) * (fx - fw)\n                p = (x - v) * q - (x - w) * r\n                q = 2.0 * (q - r)\n                \n                if q > 0.0:\n                    p = -p\n                q = abs(q)\n                \n                # Check acceptance criteria for parabolic step\n                # Step must be modest, and within the bracket\n                if abs(p)  abs(0.5 * q * e) and p > q * (a - x) and p  q * (b - x):\n                    e_old = e\n                    e = d\n                    d = p / q\n                    u = x + d\n                    \n                    # Security check: if step is too close to boundaries, take a minimum step\n                    if (u - a)  tol or (b - u)  tol:\n                        d = np.sign(xm - x) * tol if xm != x else tol\n                    parabolic_step_taken = True\n                \n            if not parabolic_step_taken:\n                # Fallback to golden section search\n                e = (a - x) if x >= xm else (b - x)\n                d = CGOLD * e\n\n            # Guard against minimal step size being too small\n            u = x + (d if abs(d) >= tol else np.sign(d) * tol)\n            fu = f(u)\n\n            # Update state (a, b, x, w, v)\n            if fu = fx:\n                if u >= x: a = x\n                else: b = x\n                v, fv = w, fw\n                w, fw = x, fx\n                x, fx = u, fu\n            else:\n                if u  x: a = u\n                else: b = u\n                if fu = fw or w == x:\n                    v, fv = w, fw\n                    w, fw = u, fu\n                elif fu = fv or v == x or v == w:\n                    v, fv = u, fu\n        \n        return x\n\n    # Define the test cases from the problem statement\n    test_cases = [\n        {'func': lambda x: (x - 2.0)**2, 'bracket': [0.0, 5.0], 'tol': 1e-10, 'max_iter': 1000},\n        {'func': lambda x: np.exp(x) - 4.0 * x, 'bracket': [-2.0, 3.0], 'tol': 1e-10, 'max_iter': 1000},\n        {'func': lambda x: (x + 1.0)**2 + 0.001 * np.sin(50.0 * x), 'bracket': [-3.0, 1.0], 'tol': 1e-10, 'max_iter': 2000},\n        {'func': lambda x: abs(x - 0.3), 'bracket': [-1.0, 2.0], 'tol': 1e-10, 'max_iter': 1000},\n        {'func': lambda x: np.exp(5.0 * x), 'bracket': [-1.0, 1.0], 'tol': 1e-10, 'max_iter': 2000},\n    ]\n\n    results = []\n    for case in test_cases:\n        minimizer = minimize_hybrid(case['func'], case['bracket'][0], case['bracket'][1], case['tol'], case['max_iter'])\n        # Round the result to 8 decimal places and format as a string\n        results.append(f\"{minimizer:.8f}\")\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3237355"}]}