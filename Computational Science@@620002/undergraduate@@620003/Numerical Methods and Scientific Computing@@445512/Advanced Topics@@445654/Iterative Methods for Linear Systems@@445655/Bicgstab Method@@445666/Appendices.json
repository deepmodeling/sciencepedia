{"hands_on_practices": [{"introduction": "To truly understand an iterative method, we must look beyond its procedural steps and grasp the underlying mathematical principles that guarantee its success. In ideal, exact arithmetic, Krylov subspace methods like BiCGSTAB are guaranteed to find the exact solution for an $N \\times N$ system in at most $N$ iterations. This exercise illuminates this fundamental property by connecting the algorithm's behavior to the minimal polynomial of the system matrix $A$, a core concept in linear algebra. By working through this for a small $2 \\times 2$ system, you will see precisely how the BiCGSTAB residual is constructed from a polynomial in $A$ and why this ensures finite termination [@problem_id:3210293].", "problem": "Consider the Bi-Conjugate Gradient Stabilized (BiCGSTAB) method applied to a linear system with a real, nonsingular, nonsymmetric matrix of dimension two. Let\n$$\nA \\;=\\; \\begin{pmatrix} 2 & 1 \\\\ 0 & 3 \\end{pmatrix},\n$$\nand suppose the initial guess is $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ with right-hand side $b \\in \\mathbb{R}^{2}$ such that $b \\neq 0$, and choose the shadow residual $\\hat{r}_{0} = r_{0} := b - A x_{0} = b$. Assume no breakdowns occur in the BiCGSTAB recurrences (that is, all division operations defined by the method are well-defined for this input).\n\nUsing only the following foundational principles:\n- the definition of the Krylov subspace $ \\mathcal{K}_{k}(A, r_{0}) = \\operatorname{span}\\{ r_{0}, A r_{0}, \\dots, A^{k-1} r_{0} \\}$,\n- the definition of the minimal polynomial $\\mu_{A}(t)$ of $A$ as the unique monic polynomial of least degree such that $\\mu_{A}(A) = 0$,\n- and the update relations of the Bi-Conjugate Gradient Stabilized method that establish each BiCGSTAB residual $r_{k}$ as $r_{k} = \\phi_{k}(A)\\,r_{0}$ for some polynomial $\\phi_{k}$,\n\ndo the following:\n1) Derive, from the Bi-Conjugate Gradient Stabilized update equations, that after $k$ outer iterations the residual has the form $r_{k} = \\phi_{k}(A)\\,r_{0}$ with $\\deg(\\phi_{k}) \\leq 2k$, and that $\\phi_{k}(t)$ factors as the product of a degree-$k$ “stabilizer” polynomial and a degree-$k$ Bi-Conjugate Gradient residual polynomial. Use this structure, together with the definition of the minimal polynomial, to show that in dimension two the method finitely terminates in at most two outer iterations (i.e., $r_{2} = 0$ in exact arithmetic) whenever breakdowns do not occur.\n2) For the specific matrix $A$ above, compute the unique monic minimal polynomial $\\mu_{A}(t)$ explicitly.\n\nYour final answer must be the explicit analytic expression of the polynomial $\\mu_{A}(t)$. No numerical rounding is required. Do not include units.", "solution": "The problem is divided into two parts. The first part is a theoretical derivation concerning the termination property of the Bi-Conjugate Gradient Stabilized (BiCGSTAB) method. The second part is a specific computation of the minimal polynomial for a given matrix.\n\nPart 1: BiCGSTAB Residual Structure and Finite Termination\n\nThe BiCGSTAB method is an iterative method for solving non-symmetric linear systems $A x = b$. The method generates a sequence of iterates $x_k$ such that the corresponding residuals $r_k = b - A x_k$ are progressively reduced. A key property of Krylov subspace methods like BiCGSTAB is that the residual after $k$ iterations can be expressed as a polynomial in the matrix $A$ applied to the initial residual $r_0$.\n\nAs stated in the problem, the residual of the BiCGSTAB method after $k$ outer iterations has the form $r_k = \\phi_k(A) r_0$, where $\\phi_k(t)$ is a polynomial. The problem further specifies the structure of this polynomial. A detailed derivation from the algorithm's recurrence relations confirms that $\\phi_k(t)$ is the product of two polynomials, each of degree $k$:\n$$ \\phi_k(t) = \\sigma_k(t) \\psi_k(t) $$\nHere, $\\psi_k(t)$ is a polynomial of degree $k$ that arises from the underlying Bi-Conjugate Gradient (BiCG) structure of the algorithm. It is referred to as the BiCG residual polynomial. The second polynomial, $\\sigma_k(t) = \\prod_{j=1}^{k} (1 - \\omega_j t)$, is a degree-$k$ polynomial that arises from the \"stabilizing\" steps, which are akin to one-step Generalized Minimal Residual (GMRES) updates. The coefficients $\\omega_j$ are chosen at each step to locally minimize the norm of the residual. The total degree of the BiCGSTAB residual polynomial $\\phi_k(t)$ is therefore $\\deg(\\phi_k) = \\deg(\\sigma_k) + \\deg(\\psi_k) = k + k = 2k$. This establishes the first requested property.\n\nNow, we use this structure to demonstrate the finite termination of BiCGSTAB for a $2 \\times 2$ matrix. The argument relies on the properties of the minimal polynomial of a matrix.\nLet $A$ be an $N \\times N$ matrix. The minimal polynomial of $A$, denoted $\\mu_A(t)$, is the unique monic polynomial of least degree $m$ such that $\\mu_A(A) = 0$. By the Cayley-Hamilton theorem, $m \\le N$.\nFor any vector $v$, the minimal polynomial of $v$ with respect to $A$, denoted $\\mu_{A,v}(t)$, is the unique monic polynomial of least degree $m'$ such that $\\mu_{A,v}(A)v=0$. This polynomial divides $\\mu_A(t)$, so its degree satisfies $m' \\le m \\le N$.\n\nThe BiCG method is guaranteed to find the exact solution in at most $m'$ steps, assuming no breakdowns occur, where $m'$ is the degree of the minimal polynomial of the initial residual $r_0$ with respect to $A$. This means that the BiCG residual polynomial $\\psi_k(t)$ for $k=m'$ must be a scalar multiple of $\\mu_{A, r_0}(t)$. Since both $\\psi_{m'}(t)$ and $\\mu_{A, r_0}(t)/\\mu_{A, r_0}(0)$ are polynomials of degree $m'$ that equal $1$ at $t=0$, they must be identical. Consequently, the BiCG residual at step $m'$ is zero:\n$$ r_{m'}^{\\text{BCG}} = \\psi_{m'}(A) r_0 = \\frac{\\mu_{A, r_0}(A)}{\\mu_{A, r_0}(0)} r_0 = 0 $$\nThe problem states that the BiCGSTAB residual polynomial $\\phi_k(t)$ factors into a stabilizer part and a BiCG residual polynomial part $\\psi_k(t)$. Therefore, the BiCGSTAB residual at step $k=m'$ is given by:\n$$ r_{m'} = \\sigma_{m'}(A) \\psi_{m'}(A) r_0 $$\nSubstituting the result that $\\psi_{m'}(A) r_0 = 0$, we get:\n$$ r_{m'} = \\sigma_{m'}(A) \\cdot 0 = 0 $$\nThis shows that BiCGSTAB must also terminate in at most $m'$ steps.\n\nFor the specific case given in the problem, the matrix $A$ has dimension $N=2$. The degree of its minimal polynomial $\\mu_A(t)$ is $m \\le 2$. The degree of the minimal polynomial of $r_0$ with respect to $A$ is $m' \\le m \\le 2$. Therefore, the BiCGSTAB method is guaranteed to terminate with $r_k=0$ for some $k \\le m' \\le 2$. This means the method finds the exact solution in at most $2$ outer iterations.\n\nPart 2: Computation of the Minimal Polynomial\n\nWe are given the matrix:\n$$ A = \\begin{pmatrix} 2 & 1 \\\\ 0 & 3 \\end{pmatrix} $$\nTo find the minimal polynomial $\\mu_A(t)$, we first compute the characteristic polynomial $\\chi_A(t)$.\n$$ \\chi_A(t) = \\det(A - tI) = \\det\\begin{pmatrix} 2-t & 1 \\\\ 0 & 3-t \\end{pmatrix} $$\n$$ \\chi_A(t) = (2-t)(3-t) = t^2 - 5t + 6 $$\nThe minimal polynomial $\\mu_A(t)$ must divide the characteristic polynomial $\\chi_A(t)$. The roots of $\\chi_A(t)$ are the eigenvalues of $A$, which are $\\lambda_1 = 2$ and $\\lambda_2 = 3$. Since the eigenvalues are distinct, the matrix $A$ is diagonalizable. For a diagonalizable matrix, the minimal polynomial is the product of linear factors corresponding to each distinct eigenvalue.\nThus, the minimal polynomial must be:\n$$ \\mu_A(t) = (t-2)(t-3) = t^2 - 5t + 6 $$\nThe minimal polynomial is required to be monic, which this is. Its degree is $2$. The only other monic divisors of $\\chi_A(t)$ are $(t-2)$ and $(t-3)$, which are of degree $1$. We check if they annihilate $A$:\n$$ A - 2I = \\begin{pmatrix} 2 & 1 \\\\ 0 & 3 \\end{pmatrix} - \\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\\\ 0 & 1 \\end{pmatrix} \\neq 0 $$\n$$ A - 3I = \\begin{pmatrix} 2 & 1 \\\\ 0 & 3 \\end{pmatrix} - \\begin{pmatrix} 3 & 0 \\\\ 0 & 3 \\end{pmatrix} = \\begin{pmatrix} -1 & 1 \\\\ 0 & 0 \\end{pmatrix} \\neq 0 $$\nSince no polynomial of degree $1$ annihilates $A$, the minimal polynomial must have degree $2$. As it must divide and be of the same degree as the characteristic polynomial (and both are monic), they must be equal.\n\nTherefore, the unique monic minimal polynomial of $A$ is $\\mu_A(t) = t^2 - 5t + 6$.", "answer": "$$\\boxed{t^2 - 5t + 6}$$", "id": "3210293"}, {"introduction": "While BiCGSTAB offers significant improvements in stability over the Bi-Conjugate Gradient (BiCG) method, it is not entirely immune to breakdown or stagnation. This practice problem constructs a specific scenario where the \"stabilizing\" part of the algorithm falters. You will investigate a case that leads to a \"pivot breakdown,\" where the crucial smoothing parameter $\\omega_1$ becomes exactly zero [@problem_id:2376337]. This demonstrates that if the intermediate residual $s_1$ is orthogonal to its image under the matrix $A$, the stabilization step contributes nothing, and the algorithm can stall.", "problem": "A linear system arises in a non-symmetric discretization context: solve $A x = b$ with the Biconjugate Gradient Stabilized (BiCGSTAB) method. Consider\n$$\nA=\\begin{pmatrix}\n1 & 1\\\\\n-1 & 0\n\\end{pmatrix}, \\quad\nb=\\begin{pmatrix}\n1\\\\\n0\n\\end{pmatrix}, \\quad\nx_{0}=\\begin{pmatrix}\n0\\\\\n0\n\\end{pmatrix}.\n$$\nLet the initial residual be $r_{0}=b-A x_{0}$ and choose the shadow residual $\\hat{r}=r_{0}$. Use the standard Euclidean inner product. In the first outer iteration of BiCGSTAB (with $p_{0}=r_{0}$), define\n- $v_{0}=A p_{0}$,\n- $\\alpha_{1}=\\dfrac{\\hat{r}^{T} r_{0}}{\\hat{r}^{T} v_{0}}$,\n- $s_{1}=r_{0}-\\alpha_{1} v_{0}$,\n- $t_{1}=A s_{1}$,\n- $\\omega_{1}=\\dfrac{t_{1}^{T} s_{1}}{t_{1}^{T} t_{1}}$.\nCompute the exact value of $\\omega_{1}$. Provide your answer as a single real number. No rounding is required.", "solution": "The problem statement is critically examined and found to be valid. It is a well-posed problem in computational linear algebra, specifically concerning the application of the Biconjugate Gradient Stabilized (BiCGSTAB) method. All required data and definitions are provided, the context is scientifically sound, and the objective is clear. We may proceed with the solution.\n\nThe task is to compute the value of $\\omega_{1}$ in the first iteration of the BiCGSTAB method for the given linear system $A x = b$. The steps are provided in the problem statement. We will follow them systematically.\n\nThe given matrix and vectors are:\n$$\nA=\\begin{pmatrix}\n1 & 1\\\\\n-1 & 0\n\\end{pmatrix}, \\quad\nb=\\begin{pmatrix}\n1\\\\\n0\n\\end{pmatrix}, \\quad\nx_{0}=\\begin{pmatrix}\n0\\\\\n0\n\\end{pmatrix}\n$$\n\n**Step 1: Compute the initial residual $r_{0}$**\nThe initial residual $r_{0}$ is defined as $r_{0} = b - A x_{0}$. First, we compute the product $A x_{0}$:\n$$\nA x_{0} = \\begin{pmatrix} 1 & 1\\\\ -1 & 0 \\end{pmatrix} \\begin{pmatrix} 0\\\\ 0 \\end{pmatrix} = \\begin{pmatrix} (1)(0) + (1)(0) \\\\ (-1)(0) + (0)(0) \\end{pmatrix} = \\begin{pmatrix} 0\\\\ 0 \\end{pmatrix}\n$$\nNow, we can find $r_{0}$:\n$$\nr_{0} = b - A x_{0} = \\begin{pmatrix} 1\\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0\\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1\\\\ 0 \\end{pmatrix}\n$$\n\n**Step 2: Initialize shadow residual $\\hat{r}$ and search direction $p_{0}$**\nAccording to the problem, the shadow residual is chosen as $\\hat{r} = r_{0}$, and the initial search direction is $p_{0} = r_{0}$.\n$$\n\\hat{r} = \\begin{pmatrix} 1\\\\ 0 \\end{pmatrix}, \\quad p_{0} = \\begin{pmatrix} 1\\\\ 0 \\end{pmatrix}\n$$\n\n**Step 3: Compute the vector $v_{0}$**\nThe vector $v_{0}$ is defined as $v_{0} = A p_{0}$:\n$$\nv_{0} = A p_{0} = \\begin{pmatrix} 1 & 1\\\\ -1 & 0 \\end{pmatrix} \\begin{pmatrix} 1\\\\ 0 \\end{pmatrix} = \\begin{pmatrix} (1)(1) + (1)(0) \\\\ (-1)(1) + (0)(0) \\end{pmatrix} = \\begin{pmatrix} 1\\\\ -1 \\end{pmatrix}\n$$\n\n**Step 4: Compute the scalar $\\alpha_{1}$**\nThe scalar $\\alpha_{1}$ is given by the formula $\\alpha_{1}=\\dfrac{\\hat{r}^{T} r_{0}}{\\hat{r}^{T} v_{0}}$. We need to compute the two inner products in the numerator and denominator. The inner product is the standard Euclidean one, equivalent to a vector transpose multiplication.\nNumerator:\n$$\n\\hat{r}^{T} r_{0} = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1\\\\ 0 \\end{pmatrix} = (1)(1) + (0)(0) = 1\n$$\nDenominator:\n$$\n\\hat{r}^{T} v_{0} = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1\\\\ -1 \\end{pmatrix} = (1)(1) + (0)(-1) = 1\n$$\nThus, $\\alpha_{1}$ is:\n$$\n\\alpha_{1} = \\frac{1}{1} = 1\n$$\n\n**Step 5: Compute the vector $s_{1}$**\nThe vector $s_{1}$ is defined as $s_{1} = r_{0} - \\alpha_{1} v_{0}$:\n$$\ns_{1} = \\begin{pmatrix} 1\\\\ 0 \\end{pmatrix} - (1) \\begin{pmatrix} 1\\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1 - 1\\\\ 0 - (-1) \\end{pmatrix} = \\begin{pmatrix} 0\\\\ 1 \\end{pmatrix}\n$$\n\n**Step 6: Compute the vector $t_{1}$**\nThe vector $t_{1}$ is defined as $t_{1} = A s_{1}$:\n$$\nt_{1} = A s_{1} = \\begin{pmatrix} 1 & 1\\\\ -1 & 0 \\end{pmatrix} \\begin{pmatrix} 0\\\\ 1 \\end{pmatrix} = \\begin{pmatrix} (1)(0) + (1)(1) \\\\ (-1)(0) + (0)(1) \\end{pmatrix} = \\begin{pmatrix} 1\\\\ 0 \\end{pmatrix}\n$$\n\n**Step 7: Compute the scalar $\\omega_{1}$**\nFinally, we compute $\\omega_{1}$ using the formula $\\omega_{1}=\\dfrac{t_{1}^{T} s_{1}}{t_{1}^{T} t_{1}}$. We compute the required inner products.\nNumerator:\n$$\nt_{1}^{T} s_{1} = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0\\\\ 1 \\end{pmatrix} = (1)(0) + (0)(1) = 0\n$$\nDenominator:\n$$\nt_{1}^{T} t_{1} = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1\\\\ 0 \\end{pmatrix} = (1)(1) + (0)(0) = 1\n$$\nWith these values, $\\omega_{1}$ becomes:\n$$\n\\omega_{1} = \\frac{0}{1} = 0\n$$\nThe value of $\\omega_{1}$ is exactly zero. This occurs because the vector $s_{1}$ is orthogonal to $t_{1} = A s_{1}$. In this case, the stabilizing step of the BiCGSTAB algorithm contributes nothing to the solution update.", "answer": "$$\\boxed{0}$$", "id": "2376337"}, {"introduction": "Moving from theoretical analysis to practical implementation, this final exercise challenges you to build a truly robust BiCGSTAB solver. In the world of floating-point arithmetic, the theoretical breakdowns we've studied manifest as divisions by numbers that are nearly, but not exactly, zero. This hands-on coding problem requires you to implement a full BiCGSTAB algorithm complete with a deterministic \"breakdown repair\" mechanism to anticipate and handle these numerical instabilities [@problem_id:3210263]. By programming a strategy to recover from near-zero denominators, you will gain an appreciation for the engineering required to turn a numerical algorithm into a reliable scientific computing tool.", "problem": "You are to implement a robust solver for the linear system $A x = b$ using the Bi-Conjugate Gradient Stabilized (BiCGSTAB) method with a \"breakdown repair\" mechanism for the near-breakdown case when the bi-orthogonality scalar $\\rho_k = \\langle \\hat{r}, r_k \\rangle$ becomes numerically close to zero. The implementation must be fully deterministic, reproducible, and self-contained.\n\nStart from the following fundamental base:\n- For a nonsymmetric, nonsingular matrix $A \\in \\mathbb{R}^{n \\times n}$, Krylov subspace methods construct approximations $x_k$ from the affine space $x_0 + \\mathcal{K}_k(A, r_0)$ where $r_0 = b - A x_0$ and $\\mathcal{K}_k(A, r_0) = \\text{span}\\{r_0, A r_0, \\dots, A^{k-1} r_0\\}$.\n- The Bi-Conjugate Gradient (BiCG) method seeks two sequences with bi-orthogonality constraints defined by a shadow residual $\\hat{r}$, requiring at each step the scalar products $\\langle \\hat{r}, r_k \\rangle$ and $\\langle \\hat{r}, v_k \\rangle$ to be well-defined and nonzero.\n- The Bi-Conjugate Gradient Stabilized (BiCGSTAB) method applies a local minimal residual stabilization to BiCG to improve robustness for nonsymmetric problems.\n\nDefine a near-breakdown event when $|\\rho_k| \\le \\tau \\,\\|\\hat{r}\\|_2 \\,\\|r_k\\|_2$, for a small threshold $\\tau > 0$, with $\\rho_k = \\langle \\hat{r}, r_k \\rangle$. Upon such an event, perform a deterministic \"breakdown repair\" consisting of:\n- Choose a random direction $z$ of unit norm generated by a fixed pseudorandom number generator with a specified seed.\n- Update the shadow residual by $\\hat{r} \\leftarrow r_k + \\varepsilon z$ for a small $\\varepsilon > 0$.\n- Take a small random step on the iterate $x_k \\leftarrow x_k + \\gamma \\|r_k\\|_2 z$ for a small $\\gamma > 0$.\n- Restart the BiCGSTAB recurrence by resetting the auxiliary vectors and scalars to their initial values.\n- Limit the total number of repairs to a small integer $R_{\\max}$ and declare failure if exceeded.\n\nYour program must implement the following:\n- A function that attempts to solve $A x = b$ by BiCGSTAB with the above near-breakdown detection and repair. Use relative residual tolerance $\\|b - A x_k\\|_2 / \\|b\\|_2 \\le \\text{tol}$ as the success criterion. Use a maximum iteration count $\\text{max\\_iter}$.\n- The near-breakdown detector must use the inequality $|\\rho_k| \\le \\tau \\,\\|\\hat{r}\\|_2 \\,\\|r_k\\|_2$ with a given $\\tau$ as above. You must also treat other divisions that can break down (such as $\\langle \\hat{r}, v_k \\rangle$ and $\\langle t, t \\rangle$) with the same repair strategy when their absolute value is $\\le \\tau$ times a relevant product of norms.\n- The repair parameters must be fixed as follows: $\\tau = 10^{-14}$, $\\varepsilon = 10^{-16}$, $\\gamma = 10^{-8}$, $R_{\\max} = 5$. All random vectors $z$ must be generated deterministically from a provided seed by a pseudorandom number generator and then normalized to unit $2$-norm.\n\nInput data and determinism:\n- You must not read any input. Construct all test matrices and vectors internally as specified below. All random draws must be made from a standard normal distribution using a pseudorandom number generator initialized with the given seed for each test case. The dimension is $n$ in each case, and the initial guess is $x_0 = 0$.\n\nTest suite. For each case, produce a boolean indicating whether the solver returned an $x$ such that the relative residual is $\\le \\text{tol}$.\n\n- Case $1$ (happy path, diagonally dominant nonsymmetric system):\n  - Dimension $n = 10$.\n  - Seed $20231107$.\n  - Draw $M \\in \\mathbb{R}^{n \\times n}$ with independent standard normal entries.\n  - Set $A \\leftarrow M$, then enforce strict diagonal dominance row-wise by updating $A_{ii} \\leftarrow A_{ii} + \\sum_{j=1}^{n} |A_{ij}| + 0.5$ for each $i$.\n  - Draw $x^\\star \\in \\mathbb{R}^{n}$ with standard normal entries and set $b \\leftarrow A x^\\star$.\n  - Use $x_0 = 0$, no shadow-residual override.\n  - Use $\\text{tol} = 10^{-10}$, $\\text{max\\_iter} = 1000$, $\\tau = 10^{-14}$.\n\n- Case $2$ (forced near-breakdown at the first step via nearly orthogonal shadow residual):\n  - Dimension $n = 12$.\n  - Seed $20231108$.\n  - Construct $A$ as in Case $1$ (with diagonal dominance).\n  - Draw $x^\\star$ as standard normal and set $b \\leftarrow A x^\\star$.\n  - Use $x_0 = 0$. Let $r_0 = b$.\n  - Draw $z \\in \\mathbb{R}^{n}$ standard normal, form $z_\\perp \\leftarrow z - \\frac{\\langle z, r_0 \\rangle}{\\langle r_0, r_0 \\rangle} r_0$, and set the initial shadow residual to $\\hat{r}_0 \\leftarrow z_\\perp + 10^{-16} r_0$.\n  - Use $\\text{tol} = 10^{-10}$, $\\text{max\\_iter} = 1000$, $\\tau = 10^{-14}$.\n  - The algorithm must detect the near-breakdown at the start and repair deterministically using the rule above.\n\n- Case $3$ (mildly nonsymmetric, well-conditioned tridiagonal with small upper-triangular perturbation):\n  - Dimension $n = 30$.\n  - Seed $20231109$.\n  - Let $T \\in \\mathbb{R}^{n \\times n}$ be tridiagonal with $T_{ii} = 2$, $T_{i,i+1} = -1$, $T_{i+1,i} = -1$ for $i = 1,\\dots,n-1$.\n  - Let $U$ be strictly upper triangular with $U_{ij} = 10^{-2} \\,\\xi_{ij}$ for $i < j$, where each $\\xi_{ij}$ is standard normal drawn from the seeded generator; $U_{ij} = 0$ for $i \\ge j$.\n  - Set $A \\leftarrow T + U$.\n  - Draw $x^\\star$ as standard normal and set $b \\leftarrow A x^\\star$.\n  - Use $x_0 = 0$, no shadow-residual override.\n  - Use $\\text{tol} = 10^{-10}$, $\\text{max\\_iter} = 2000$, $\\tau = 10^{-14}$.\n\nFor all cases, use the same breakdown repair constants $\\varepsilon = 10^{-16}$, $\\gamma = 10^{-8}$, and $R_{\\max} = 5$. The success condition is the relative residual $\\|b - A x\\|_2 / \\|b\\|_2 \\le \\text{tol}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of boolean literals enclosed in square brackets, for the cases in order $1,2,3$. For example: `[True, False, True]`.", "solution": "The problem requires the implementation of the Bi-Conjugate Gradient Stabilized (BiCGSTAB) method to solve a nonsymmetric linear system $A x = b$. A crucial part of the task is to incorporate a specific \"breakdown repair\" mechanism to handle numerical instabilities that arise when certain scalar quantities in the algorithm become close to zero. The entire implementation must be deterministic, with random components controlled by a seeded pseudorandom number generator (PRNG).\n\nFirst, let us formalize the BiCGSTAB algorithm with the specified repair mechanism. The algorithm iteratively computes an approximate solution $x_k$ to the linear system.\n\nThe standard BiCGSTAB algorithm proceeds as follows, starting with an initial guess $x_0$ (here, $x_0=0$):\n\n1.  **Initialization**:\n    $r \\leftarrow b - A x$\n    $\\hat{r} \\leftarrow r$ (or a specified initial shadow residual)\n    $p \\leftarrow 0$, $v \\leftarrow 0$\n    $\\rho_{prev} \\leftarrow 1$, $\\alpha \\leftarrow 1$, $\\omega \\leftarrow 1$\n    \n2.  **Iteration**: For $k=1, 2, \\dots, \\text{max\\_iter}$:\n    a.  $\\rho \\leftarrow \\langle \\hat{r}, r \\rangle$\n    b.  $\\beta \\leftarrow (\\rho / \\rho_{prev}) (\\alpha / \\omega)$\n    c.  $p \\leftarrow r + \\beta (p - \\omega v)$\n    d.  $v \\leftarrow A p$\n    e.  $\\alpha \\leftarrow \\rho / \\langle \\hat{r}, v \\rangle$\n    f.  $s \\leftarrow r - \\alpha v$\n    g.  $t \\leftarrow A s$\n    h.  $\\omega \\leftarrow \\langle t, s \\rangle / \\langle t, t \\rangle$\n    i.  $x \\leftarrow x + \\alpha p + \\omega s$\n    j.  $r \\leftarrow s - \\omega t$\n    k.  $\\rho_{prev} \\leftarrow \\rho$\n    l.  Check for convergence.\n\nThis algorithm is susceptible to breakdowns if any of the denominators in the calculations of $\\beta$, $\\alpha$, or $\\omega$ are zero or numerically very small. Specifically, these are $\\rho_{prev}$, $\\langle \\hat{r}, v \\rangle$, and $\\langle t, t \\rangle$. The problem defines a \"near-breakdown\" and a corresponding repair strategy.\n\nA near-breakdown event is detected if a denominator's absolute value is smaller than a threshold defined by a tolerance $\\tau$ and the norms of the involved vectors. The three specific checks are:\n\n1.  For $\\rho = \\langle \\hat{r}, r \\rangle$: A breakdown is flagged if $|\\rho| \\le \\tau \\|\\hat{r}\\|_2 \\|r\\|_2$. This check is performed before computing $\\beta$. Note that $\\rho$ from the current iteration becomes $\\rho_{prev}$ for the next, so this check effectively safeguards the denominator of $\\beta$.\n2.  For $\\delta = \\langle \\hat{r}, v \\rangle$: A breakdown is flagged if $|\\delta| \\le \\tau \\|\\hat{r}\\|_2 \\|v\\|_2$. This is the denominator of $\\alpha$.\n3.  For $\\eta = \\langle t, t \\rangle = \\|t\\|_2^2$: A breakdown is flagged if $|\\eta| \\le \\tau \\|t\\|_2 \\|t\\|_2$, which simplifies to $\\|t\\|_2^2 \\le \\tau \\|t\\|_2^2$. For $\\tau < 1$, this condition is only met if $\\|t\\|_2 = 0$. This correctly identifies the breakdown when $t$ is the zero vector.\n\nUpon detection of any of these near-breakdown conditions, the following deterministic repair procedure is executed:\n\n1.  Increment a repair counter. If the counter exceeds a maximum $R_{\\max}$, the solution process fails.\n2.  Generate a random vector $z \\in \\mathbb{R}^n$ from a standard normal distribution using the seeded PRNG, and normalize it to have a unit $L_2$-norm: $z \\leftarrow z / \\|z\\|_2$.\n3.  Perturb the shadow residual: $\\hat{r} \\leftarrow r + \\varepsilon z$, where $r$ is the current residual and $\\varepsilon$ is a small positive constant.\n4.  Take a small random step in the solution space: $x \\leftarrow x + \\gamma \\|r\\|_2 z$, where $\\gamma$ is another small positive constant.\n5.  Recompute the true residual based on the new iterate: $r \\leftarrow b - A x$.\n6.  Restart the BiCGSTAB recurrence by resetting the auxiliary vectors and scalars to their initial state: $p \\leftarrow 0$, $v \\leftarrow 0$, $\\rho_{prev} \\leftarrow 1$, $\\alpha \\leftarrow 1$, $\\omega \\leftarrow 1$. The algorithm then continues to the next iteration with the updated $x$, $r$, and $\\hat{r}$.\n\nThe overall algorithm is implemented as a single function that takes the matrix $A$, vector $b$, and other parameters. It returns a boolean indicating whether convergence was achieved within the given tolerance $\\text{tol}$, maximum iterations $\\text{max\\_iter}$, and maximum repairs $R_{\\max}$. Convergence is defined by the relative residual norm: $\\|b - A x\\|_2 / \\|b\\|_2 \\le \\text{tol}$.\n\nThe three test cases specified are constructed as follows:\n- **Case 1**: A diagonally dominant $10 \\times 10$ matrix, which should be well-behaved for BiCGSTAB. This serves as a \"happy path\" test. The matrix $A$ is generated from a random matrix $M$ by updating each diagonal element $A_{ii}$ as $A_{ii} \\leftarrow M_{ii} + \\sum_{j=1}^{n} |M_{ij}| + 0.5$.\n- **Case 2**: A $12 \\times 12$ system designed to trigger the breakdown repair mechanism at the first iteration. This is achieved by setting the initial shadow residual $\\hat{r}_0$ to be nearly orthogonal to the initial residual $r_0=b$. The dot product $\\rho_1 = \\langle \\hat{r}_0, r_0 \\rangle$ will thus be very small, triggering the condition $|\\rho_1| \\le \\tau \\|\\hat{r}_0\\|_2 \\|r_0\\|_2$.\n- **Case 3**: A $30 \\times 30$ system based on a discrete Laplacian operator (a symmetric positive definite tridiagonal matrix) perturbed by a small, strictly upper-triangular random matrix. This creates a mildly nonsymmetric, well-conditioned system.\n\nFor each case, the right-hand side vector $b$ is constructed as $b = A x^\\star$, where $x^\\star$ is a known \"true\" solution with entries drawn from a standard normal distribution. This ensures the system is consistent and allows for verification if needed, although the convergence criterion relies only on the residual. The solver starts with an initial guess $x_0=0$. All pseudorandom numbers for matrix construction and breakdown repair are generated deterministically using a per-case seed value, ensuring reproducibility. The implemented solver will be applied to each case, and the boolean result (success or failure) will be reported.", "answer": "```python\nimport numpy as np\n\ndef bicgstab_with_repair(A, b, x0, tol, max_iter, seed, r_hat0=None, tau=1e-14, epsilon=1e-16, gamma=1e-8, R_max=5):\n    \"\"\"\n    Solves the linear system Ax = b using the Bi-Conjugate Gradient Stabilized (BiCGSTAB)\n    method with a breakdown repair mechanism.\n\n    Args:\n        A (np.ndarray): The n x n coefficient matrix.\n        b (np.ndarray): The n-dimensional right-hand side vector.\n        x0 (np.ndarray): The initial guess for the solution.\n        tol (float): The relative residual tolerance for convergence.\n        max_iter (int): The maximum number of iterations.\n        seed (int): Seed for the pseudorandom number generator for repairs.\n        r_hat0 (np.ndarray, optional): The initial shadow residual. If None, set to r0.\n        tau (float): Threshold for breakdown detection.\n        epsilon (float): Perturbation factor for shadow residual in repair.\n        gamma (float): Step size factor for solution update in repair.\n        R_max (int): Maximum number of repairs allowed.\n\n    Returns:\n        tuple: (bool indicating success, final solution vector x)\n    \"\"\"\n    n = A.shape[0]\n    x = x0.copy()\n    \n    norm_b = np.linalg.norm(b)\n    if norm_b == 0:\n        return True, np.zeros(n)\n\n    r = b - A @ x\n    \n    if r_hat0 is None:\n        hat_r = r.copy()\n    else:\n        hat_r = r_hat0.copy()\n\n    # Auxiliary vectors and scalars\n    p = np.zeros(n)\n    v = np.zeros(n)\n    rho_prev = 1.0\n    alpha = 1.0\n    omega = 1.0\n    \n    num_repairs = 0\n    rng = np.random.default_rng(seed)\n\n    # Initial residual check\n    if np.linalg.norm(r) / norm_b = tol:\n        return True, x\n\n    for _ in range(max_iter):\n        rho = np.dot(hat_r, r)\n        \n        # --- Breakdown Detection and Repair ---\n        do_repair = False\n        \n        # Check 1: Breakdown in rho\n        norm_hat_r = np.linalg.norm(hat_r)\n        norm_r = np.linalg.norm(r)\n        if abs(rho) = tau * norm_hat_r * norm_r:\n            do_repair = True\n        \n        if not do_repair:\n            if abs(rho_prev) == 0.0 and abs(omega) == 0.0:  # Avoid division by zero\n                 do_repair = True\n            else:\n                 beta = (rho / rho_prev) * (alpha / omega) if rho_prev != 0 and omega != 0 else 0.0\n                 p = r + beta * (p - omega * v)\n                 v = A @ p\n                 \n                 # Check 2: Breakdown in alpha's denominator\n                 denom_alpha = np.dot(hat_r, v)\n                 norm_v = np.linalg.norm(v)\n                 if abs(denom_alpha) = tau * norm_hat_r * norm_v:\n                     do_repair = True\n\n        if do_repair:\n            if num_repairs >= R_max:\n                return False, x\n            num_repairs += 1\n            \n            z = rng.standard_normal(n)\n            z /= np.linalg.norm(z)\n            \n            hat_r = r + epsilon * z\n            x += gamma * norm_r * z\n            r = b - A @ x\n            \n            # Restart recurrence\n            p = np.zeros(n)\n            v = np.zeros(n)\n            rho_prev = 1.0\n            alpha = 1.0\n            omega = 1.0\n            continue\n\n        # --- Standard BiCGSTAB Step ---\n        alpha = rho / denom_alpha\n        s = r - alpha * v\n\n        # Early exit if s is small\n        if np.linalg.norm(s) / norm_b = tol:\n            x += alpha * p\n            # Recompute final residual for accurate check\n            final_r = b - A @ x\n            if np.linalg.norm(final_r) / norm_b = tol:\n                return True, x\n        \n        t = A @ s\n        \n        # Check 3: Breakdown in omega's denominator\n        norm_t_sq = np.dot(t, t)\n        norm_t = np.sqrt(norm_t_sq)\n        if norm_t_sq = tau * norm_t * norm_t: # Effectively checks if t is zero\n            # This is a breakdown, as omega would be undefined. Per the problem statement,\n            # apply the full repair strategy.\n            if num_repairs >= R_max:\n                return False, x\n            num_repairs += 1\n            z = rng.standard_normal(n)\n            z /= np.linalg.norm(z)\n            hat_r = r + epsilon * z\n            x += gamma * norm_r * z\n            r = b - A @ x\n            # Restart recurrence\n            p = np.zeros(n); v = np.zeros(n); rho_prev = 1.0; alpha = 1.0; omega = 1.0\n            continue\n\n        omega = np.dot(t, s) / norm_t_sq\n        x += alpha * p + omega * s\n        r = s - omega * t\n        \n        rho_prev = rho\n\n        # --- Convergence Check ---\n        final_r = b - A @ x\n        if np.linalg.norm(final_r) / norm_b = tol:\n            return True, x\n\n    return False, x\n\ndef solve():\n    # Global Parameters\n    params = {\n        'tau': 1e-14,\n        'epsilon': 1e-16,\n        'gamma': 1e-8,\n        'R_max': 5\n    }\n\n    results = []\n\n    # --- Case 1 ---\n    n1 = 10\n    seed1 = 20231107\n    tol1 = 1e-10\n    max_iter1 = 1000\n    rng1 = np.random.default_rng(seed1)\n    \n    M1 = rng1.standard_normal((n1, n1))\n    A1 = M1.copy()\n    diag_updates = np.array([np.sum(np.abs(M1[i, :])) for i in range(n1)])\n    np.fill_diagonal(A1, M1.diagonal() + diag_updates + 0.5)\n    \n    x_star1 = rng1.standard_normal(n1)\n    b1 = A1 @ x_star1\n    x0_1 = np.zeros(n1)\n    \n    success1, _ = bicgstab_with_repair(A1, b1, x0_1, tol1, max_iter1, seed1, **params)\n    results.append(success1)\n\n    # --- Case 2 ---\n    n2 = 12\n    seed2 = 20231108\n    tol2 = 1e-10\n    max_iter2 = 1000\n    rng2 = np.random.default_rng(seed2)\n\n    M2 = rng2.standard_normal((n2, n2))\n    A2 = M2.copy()\n    diag_updates_2 = np.array([np.sum(np.abs(M2[i, :])) for i in range(n2)])\n    np.fill_diagonal(A2, M2.diagonal() + diag_updates_2 + 0.5)\n\n    x_star2 = rng2.standard_normal(n2)\n    b2 = A2 @ x_star2\n    x0_2 = np.zeros(n2)\n    r0_2 = b2 - A2 @ x0_2\n\n    z2 = rng2.standard_normal(n2)\n    z_perp = z2 - (np.dot(z2, r0_2) / np.dot(r0_2, r0_2)) * r0_2\n    r_hat0_2 = z_perp + 1e-16 * r0_2\n    \n    success2, _ = bicgstab_with_repair(A2, b2, x0_2, tol2, max_iter2, seed2, r_hat0=r_hat0_2, **params)\n    results.append(success2)\n\n    # --- Case 3 ---\n    n3 = 30\n    seed3 = 20231109\n    tol3 = 1e-10\n    max_iter3 = 2000\n    rng3 = np.random.default_rng(seed3)\n    \n    T3 = np.diag(2 * np.ones(n3)) - np.diag(np.ones(n3 - 1), k=1) - np.diag(np.ones(n3 - 1), k=-1)\n    \n    U3 = np.zeros((n3, n3))\n    xi3 = rng3.standard_normal((n3, n3))\n    for i in range(n3):\n        for j in range(i + 1, n3):\n            U3[i, j] = 1e-2 * xi3[i, j]\n            \n    A3 = T3 + U3\n    \n    x_star3 = rng3.standard_normal(n3)\n    b3 = A3 @ x_star3\n    x0_3 = np.zeros(n3)\n\n    success3, _ = bicgstab_with_repair(A3, b3, x0_3, tol3, max_iter3, seed3, **params)\n    results.append(success3)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3210263"}]}