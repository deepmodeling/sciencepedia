{"hands_on_practices": [{"introduction": "The efficiency of many iterative solvers hinges on a carefully chosen relaxation parameter, $\\omega$. This first practice explores this principle for the weighted Jacobi method, where the optimal parameter $\\omega_{\\text{opt}}$ can be determined precisely if the spectral bounds of the system matrix are known. By first deriving the expression for $\\omega_{\\text{opt}}$ and then comparing its exact value against approximations derived from computationally inexpensive spectral bounds, you will build a foundational understanding of the interplay between spectral properties and convergence optimization [@problem_id:3266515].", "problem": "You are to implement a program that, for a class of symmetric positive definite matrices, selects the relaxation parameter $\\,\\omega\\,$ for the weighted Jacobi method using three different sources of spectral information, and then quantitatively compares their accuracy. The weighted Jacobi method for solving a linear system $\\,A x = b\\,$ uses the iteration\n$$\nx^{(k+1)} = x^{(k)} + \\omega D^{-1} \\left(b - A x^{(k)}\\right),\n$$\nwhere $\\,A \\in \\mathbb{R}^{n \\times n}\\,$ is symmetric positive definite, and $\\,D = \\mathrm{diag}(A)\\,$ is the diagonal of $\\,A\\,$. The iteration matrix is $\\,B(\\omega) = I - \\omega D^{-1} A\\,$.\n\nStarting from the definition of the iteration matrix and the fact that $\\,A\\,$ is symmetric positive definite, derive from first principles an explicit expression for the optimal choice of $\\,\\omega\\,$ that minimizes the spectral radius of $\\,B(\\omega)\\,$ when the spectrum of $\\,D^{-1/2} A D^{-1/2}\\,$ is known to lie in an interval $[\\lambda_{\\min}, \\lambda_{\\max}]$ with $\\,0  \\lambda_{\\min} \\le \\lambda_{\\max}\\,$. Then, when only bounds for $\\,\\lambda_{\\min}\\,$ and $\\,\\lambda_{\\max}\\,$ are available, define the corresponding approximate choice by consistently replacing the exact interval endpoints with the bounds.\n\nYour program must do the following, for each matrix $\\,A\\,$ in the test suite:\n1. Construct $\\,S = D^{-1/2} A D^{-1/2}\\,$, where $\\,D = \\mathrm{diag}(A)\\,$.\n2. Compute the exact endpoints $\\,\\lambda_{\\min}\\,$ and $\\,\\lambda_{\\max}\\,$ of the spectrum of $\\,S\\,$ by eigenvalue computation.\n3. Using only $\\,S\\,$, compute the Gershgorin disc bounds for the spectrum of $\\,S\\,$:\n   - For each row $\\,i\\,$, define $\\,c_i = s_{ii}\\,$ and $\\,R_i = \\sum_{j \\neq i} |s_{ij}|\\,$, and form the interval $[c_i - R_i, c_i + R_i]$.\n   - Set $\\,L_G = \\min_i (c_i - R_i)\\,$ and $\\,U_G = \\max_i (c_i + R_i)\\,$.\n4. Using only $\\,S\\,$, compute the Brauer oval of Cassini bounds for the spectrum of $\\,S\\,$:\n   - For each unordered pair $\\,i  j\\,$, define $\\,c_i = s_{ii}\\,$, $\\,c_j = s_{jj}\\,$, $\\,R_i = \\sum_{k \\neq i} |s_{ik}|\\,$, and $\\,R_j = \\sum_{k \\neq j} |s_{jk}|\\,$.\n   - Consider the real set given by $\\,|z - c_i| \\, |z - c_j| \\le R_i R_j\\,$. The extreme real intersection points for this pair are the roots of\n     $$\n     z^2 - (c_i + c_j) z + (c_i c_j - R_i R_j) = 0.\n     $$\n     Let the smaller and larger roots be $\\,\\ell_{ij}\\,$ and $\\,u_{ij}\\,$, respectively.\n   - Set $\\,L_B = \\min_{ij} \\ell_{ij}\\,$ and $\\,U_B = \\max_{ij} u_{ij}\\,$.\n5. From $\\,\\lambda_{\\min}\\,$ and $\\,\\lambda_{\\max}\\,$, compute the optimal relaxation parameter $\\,\\omega_{\\mathrm{exact}}\\,$ that minimizes the spectral radius of the weighted Jacobi iteration matrix $\\,B(\\omega)\\,$.\n6. From the Gershgorin bounds $[L_G, U_G]$ and the Brauer bounds $[L_B, U_B]$, compute corresponding approximate choices $\\,\\omega_G\\,$ and $\\,\\omega_B\\,$ by replacing the exact endpoints with these bounds in your expression from step $\\,5$.\n7. Using the exact spectral endpoints $\\,\\lambda_{\\min}\\,$ and $\\,\\lambda_{\\max}\\,$ of $\\,S\\,$, evaluate the spectral radius of $\\,B(\\omega)\\,$ at $\\,\\omega_{\\mathrm{exact}}\\,$, $\\,\\omega_G\\,$, and $\\,\\omega_B\\,$. Report the nonnegative excesses\n   $$\n   \\Delta_G = \\rho\\!\\left(B(\\omega_G)\\right) - \\rho\\!\\left(B(\\omega_{\\mathrm{exact}})\\right), \\quad\n   \\Delta_B = \\rho\\!\\left(B(\\omega_B)\\right) - \\rho\\!\\left(B(\\omega_{\\mathrm{exact}})\\right).\n   $$\n8. Also report the absolute parameter errors $\\,|\\omega_G - \\omega_{\\mathrm{exact}}|\\,$ and $\\,|\\omega_B - \\omega_{\\mathrm{exact}}|\\,$.\n\nTest suite:\n- Case $\\,1\\,$ (banded tridiagonal): $\\,A \\in \\mathbb{R}^{n \\times n}\\,$ with $\\,n = 50\\,$, $\\,a_{ii} = 2\\,$, $\\,a_{i,i+1} = a_{i+1,i} = -1\\,$ for all valid indices, and zeros elsewhere.\n- Case $\\,2\\,$ (banded pentadiagonal): $\\,A \\in \\mathbb{R}^{n \\times n}\\,$ with $\\,n = 60\\,$, $\\,a_{ii} = 4\\,$, $\\,a_{i,i\\pm 1} = -1\\,$, $\\,a_{i,i\\pm 2} = -0.5\\,$ for all valid indices, and zeros elsewhere.\n- Case $\\,3\\,$ (sparse random symmetric strictly diagonally dominant): $\\,A \\in \\mathbb{R}^{n \\times n}\\,$ with $\\,n = 40\\,$. Initialize $\\,A\\,$ as the zero matrix. For each unordered pair $\\,i  j\\,$, independently with probability $\\,p = 0.06\\,$, draw $\\,v \\sim \\mathrm{Uniform}([-0.2, 0.2])\\,$ and set $\\,a_{ij} = a_{ji} = v\\,$; otherwise leave it zero. Finally, set each diagonal entry to enforce strict diagonal dominance: $\\,a_{ii} \\leftarrow \\sum_{j \\neq i} |a_{ij}| + \\delta\\,$ with $\\,\\delta = 0.5\\,$. Use a fixed pseudorandom seed $\\,7\\,$ for reproducibility.\n- Case $\\,4\\,$ (identity edge case): $\\,A = I \\in \\mathbb{R}^{n \\times n}\\,$ with $\\,n = 25\\,$.\n\nEvaluation and output specification:\n- For each case, compute the $\\,4\\,$ floats in the order\n  $$\n  \\left|\\omega_G - \\omega_{\\mathrm{exact}}\\right|,\\;\n  \\left|\\omega_B - \\omega_{\\mathrm{exact}}\\right|,\\;\n  \\Delta_G,\\;\n  \\Delta_B.\n  $$\n- Your program should produce a single line of output containing all results, concatenated for the $\\,4\\,$ cases in order, as a comma-separated list enclosed in square brackets, for a total of $\\,16\\,$ comma-separated floats. For example, a valid output format is\n  $$\n  [r_1,r_2,\\dots,r_{16}],\n  $$\n  where each $\\,r_k\\,$ is a float in standard decimal notation with no units.", "solution": "The user-provided problem is assessed to be **valid**. It is scientifically grounded in the principles of numerical linear algebra, specifically concerning iterative methods for solving linear systems. The problem is well-posed, objective, self-contained, and formalizable. All terms are standard and clearly defined. The steps to follow are explicit and logically sequential. Therefore, I will proceed with a full solution.\n\nThe core of the problem is to derive and apply a formula for the optimal relaxation parameter $\\omega$ for the weighted Jacobi method and compare it with approximations derived from spectral bounds.\n\n### Part 1: Derivation of the Optimal Relaxation Parameter $\\omega_{\\mathrm{opt}}$\n\nThe weighted Jacobi iteration for solving the linear system $A x = b$ is given by\n$$\nx^{(k+1)} = x^{(k)} + \\omega D^{-1} \\left(b - A x^{(k)}\\right)\n$$\nwhere $A \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite (SPD) matrix and $D = \\mathrm{diag}(A)$.\n\nThe error $e^{(k)} = x^{(k)} - x$ (where $Ax=b$) evolves according to\n$$\ne^{(k+1)} = x^{(k+1)} - x = \\left(x^{(k)} - x\\right) + \\omega D^{-1} \\left(b - A x^{(k)}\\right) = e^{(k)} + \\omega D^{-1} \\left(A x - A x^{(k)}\\right) = e^{(k)} - \\omega D^{-1} A e^{(k)}\n$$\nThis leads to the error propagation equation $e^{(k+1)} = (I - \\omega D^{-1} A) e^{(k)}$. The iteration matrix is therefore $B(\\omega) = I - \\omega D^{-1} A$. The iterative method converges if and only if the spectral radius $\\rho(B(\\omega))$ is less than $1$. The rate of convergence is maximized by minimizing $\\rho(B(\\omega))$.\n\nThe eigenvalues of $B(\\omega)$ are related to the eigenvalues of the matrix $D^{-1}A$. Let $\\lambda$ be an eigenvalue of $D^{-1}A$. Then the corresponding eigenvalue $\\mu$ of $B(\\omega)$ is $\\mu = 1 - \\omega \\lambda$.\nSince $A$ is SPD, its diagonal entries $a_{ii}$ are positive, so $D$ is invertible and $D^{-1/2}$ is a real diagonal matrix. The matrix $D^{-1}A$ is not generally symmetric but is similar to the symmetric matrix $S = D^{-1/2} A D^{-1/2}$:\n$$\nD^{-1}A = D^{-1/2} (D^{-1/2} A D^{-1/2}) D^{1/2} = D^{-1/2} S D^{1/2}\n$$\nSince they are similar, $D^{-1}A$ and $S$ share the same set of eigenvalues. As $A$ is SPD, $S$ is also SPD, and its eigenvalues are real and positive. Let the spectrum of $S$ (and thus of $D^{-1}A$) be denoted by $\\{\\lambda_i\\}_{i=1}^n$, where $0  \\lambda_i$. The problem states that this spectrum is known to lie in an interval $[\\lambda_{\\min}, \\lambda_{\\max}]$ where $0  \\lambda_{\\min} \\le \\lambda_{\\max}$.\n\nThe eigenvalues of the iteration matrix $B(\\omega)$ are $\\mu_i = 1 - \\omega \\lambda_i$, where $\\lambda_i \\in [\\lambda_{\\min}, \\lambda_{\\max}]$. The spectral radius is\n$$\n\\rho(B(\\omega)) = \\max_i |1 - \\omega \\lambda_i|\n$$\nSince $\\lambda \\mapsto 1-\\omega\\lambda$ is a linear function, its maximum absolute value over the interval $[\\lambda_{\\min}, \\lambda_{\\max}]$ must occur at one of the endpoints. Therefore,\n$$\n\\rho(B(\\omega)) = \\max\\left(|1 - \\omega \\lambda_{\\min}|, |1 - \\omega \\lambda_{\\max}|\\right)\n$$\nTo find the optimal $\\omega$, we must solve the min-max problem: $\\min_{\\omega} \\max\\left(|1 - \\omega \\lambda_{\\min}|, |1 - \\omega \\lambda_{\\max}|\\right)$. The minimum value of the maximum of two functions is typically found where they are equal. For optimal convergence, we expect the extreme eigenvalues of $B(\\omega)$ to have the same magnitude, i.e., $|1 - \\omega \\lambda_{\\min}| = |1 - \\omega \\lambda_{\\max}|$. For non-trivial convergence, the eigenvalues $1 - \\omega\\lambda$ should lie in $(-1, 1)$, implying they have opposite signs at the spectral bounds $\\lambda_{\\min}$ and $\\lambda_{\\max}$. This leads to the condition:\n$$\n1 - \\omega \\lambda_{\\min} = -(1 - \\omega \\lambda_{\\max}) = -1 + \\omega \\lambda_{\\max}\n$$\nSolving this equation for $\\omega$ yields:\n$$\n2 = \\omega \\lambda_{\\min} + \\omega \\lambda_{\\max} = \\omega (\\lambda_{\\min} + \\lambda_{\\max})\n$$\nThus, the optimal relaxation parameter is\n$$\n\\omega_{\\mathrm{opt}} = \\frac{2}{\\lambda_{\\min} + \\lambda_{\\max}}\n$$\nThis is the required expression. When only bounds $[L, U]$ for the spectrum are available, the corresponding approximate choice is obtained by substitution: $\\omega_{\\mathrm{approx}} = \\frac{2}{L + U}$.\n\n### Part 2: Implementation Plan\n\nThe program will be structured to handle each test case by implementing the sequence of steps specified in the problem statement.\n\n1.  **Construct $S$**: For a given matrix $A$, compute $D = \\mathrm{diag}(A)$ and then $S = D^{-1/2} A D^{-1/2}$. Note that the diagonal entries of $S$ will always be $s_{ii} = a_{ii} / \\sqrt{a_{ii}^2} = 1$.\n\n2.  **Exact Spectrum**: Compute the eigenvalues of the symmetric matrix $S$ using a numerically stable method (e.g., `numpy.linalg.eigvalsh`) and find the minimum ($\\lambda_{\\min}$) and maximum ($\\lambda_{\\max}$) eigenvalues.\n\n3.  **Gershgorin Bounds**: For the matrix $S$, the Gershgorin discs are centered at $c_i = s_{ii} = 1$. The radii are $R_i = \\sum_{j \\neq i} |s_{ij}|$. The interval containing the spectrum is $[L_G, U_G]$, where $L_G = \\min_i(1 - R_i) = 1 - \\max_i R_i$ and $U_G = \\max_i(1 + R_i) = 1 + \\max_i R_i$.\n\n4.  **Brauer Bounds**: For the matrix $S$, the ovals of Cassini are defined by $|z-s_{ii}||z-s_{jj}| \\le R_i R_j$. With $s_{ii}=s_{jj}=1$, this is $|z-1|^2 \\le R_i R_j$. For real $z$, this is $|z-1| \\le \\sqrt{R_i R_j}$, giving the interval $[1-\\sqrt{R_i R_j}, 1+\\sqrt{R_i R_j}]$. The union of these intervals over all pairs $i  j$ gives the bounds $[L_B, U_B]$, where $L_B = \\min_{ij}(1 - \\sqrt{R_i R_j}) = 1 - \\max_{ij}\\sqrt{R_i R_j}$ and $U_B = \\max_{ij}(1 + \\sqrt{R_i R_j}) = 1 + \\max_{ij}\\sqrt{R_i R_j}$.\n\n5.  **Compute $\\omega_{\\mathrm{exact}}$**: Use the derived formula $\\omega_{\\mathrm{exact}} = 2 / (\\lambda_{\\min} + \\lambda_{\\max})$.\n\n6.  **Compute $\\omega_G$ and $\\omega_B$**: Apply the formula to the bounds.\n    $\\omega_G = 2 / (L_G + U_G) = 2 / ((1-\\max R_i) + (1+\\max R_i)) = 2 / 2 = 1$.\n    $\\omega_B = 2 / (L_B + U_B) = 2 / ((1-\\max \\sqrt{R_i R_j}) + (1+\\max \\sqrt{R_i R_j})) = 2 / 2 = 1$.\n    The estimates from these standard symmetric bounds result in $\\omega=1$, which corresponds to the unweighted Jacobi method.\n\n7.  **Evaluate Spectral Radii and Excesses**:\n    The spectral radius for a given $\\omega$ is $\\rho(B(\\omega)) = \\max(|1 - \\omega \\lambda_{\\min}|, |1 - \\omega \\lambda_{\\max}|)$.\n    The optimal spectral radius is $\\rho(B(\\omega_{\\mathrm{exact}})) = (\\lambda_{\\max} - \\lambda_{\\min}) / (\\lambda_{\\max} + \\lambda_{\\min})$.\n    We will compute $\\rho(B(\\omega_G))$ and $\\rho(B(\\omega_B))$ and find the excesses $\\Delta_G$ and $\\Delta_B$.\n\n8.  **Evaluate Parameter Errors**: Compute $|\\omega_G - \\omega_{\\mathrm{exact}}|$ and $|\\omega_B - \\omega_{\\mathrm{exact}}|$.\n\nThese steps will be coded for each of the four test matrices described in the problem. The final output will be a flat list of the 16 computed floating-point values.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef process_case(A):\n    \"\"\"\n    Processes a single matrix A according to the problem description.\n    \"\"\"\n    n = A.shape[0]\n\n    # Step 1: Construct S = D^(-1/2) * A * D^(-1/2)\n    diag_A = np.diag(A)\n    if np.any(diag_A = 0):\n        raise ValueError(\"Matrix A must have positive diagonal entries.\")\n    D_inv_sqrt_vals = 1.0 / np.sqrt(diag_A)\n    D_inv_sqrt = np.diag(D_inv_sqrt_vals)\n    S = D_inv_sqrt @ A @ D_inv_sqrt\n\n    # Step 2: Compute exact eigenvalues of S\n    eigvals_S = np.linalg.eigvalsh(S)\n    lambda_min = np.min(eigvals_S)\n    lambda_max = np.max(eigvals_S)\n\n    # Step 3: Compute Gershgorin bounds for S\n    # s_ii is always 1. c_i = 1 for all i.\n    # R_i = sum over j != i of |s_ij|\n    R = np.sum(np.abs(S), axis=1) - np.abs(np.diag(S))\n    max_R = np.max(R) if R.size > 0 else 0.0\n    L_G = 1.0 - max_R\n    U_G = 1.0 + max_R\n    \n    # Step 4: Compute Brauer oval of Cassini bounds for S\n    # s_ii = s_jj = 1. The roots of z^2 - 2z + (1-R_i*R_j)=0 are 1 +/- sqrt(R_i*R_j)\n    max_sqrt_RiRj = 0.0\n    if n > 1:\n        for i in range(n):\n            for j in range(i + 1, n):\n                sqrt_RiRj = np.sqrt(R[i] * R[j])\n                if sqrt_RiRj > max_sqrt_RiRj:\n                    max_sqrt_RiRj = sqrt_RiRj\n    L_B = 1.0 - max_sqrt_RiRj\n    U_B = 1.0 + max_sqrt_RiRj\n\n    # Step 5: Compute optimal omega\n    # Handle the case lambda_min + lambda_max = 0, though not expected for SPD\n    if lambda_min + lambda_max == 0:\n        omega_exact = 1.0 # Or some other reasonable default\n    else:\n        omega_exact = 2.0 / (lambda_min + lambda_max)\n    \n    # Step 6: Compute approximate omegas\n    omega_G = 2.0 / (L_G + U_G)\n    omega_B = 2.0 / (L_B + U_B)\n\n    # Step 7: Evaluate spectral radii and excesses\n    def get_rho_B(omega, lmin, lmax):\n        return max(abs(1.0 - omega * lmin), abs(1.0 - omega * lmax))\n        \n    rho_exact = get_rho_B(omega_exact, lambda_min, lambda_max)\n    rho_G = get_rho_B(omega_G, lambda_min, lambda_max)\n    rho_B = get_rho_B(omega_B, lambda_min, lambda_max)\n\n    Delta_G = rho_G - rho_exact\n    Delta_B = rho_B - rho_exact\n\n    # Step 8: Compute absolute parameter errors\n    err_omega_G = abs(omega_G - omega_exact)\n    err_omega_B = abs(omega_B - omega_exact)\n\n    return (err_omega_G, err_omega_B, Delta_G, Delta_B)\n\ndef generate_matrices():\n    \"\"\"\n    Generates the matrices for all test cases.\n    \"\"\"\n    matrices = []\n\n    # Case 1: Tridiagonal matrix\n    n1 = 50\n    A1 = np.zeros((n1, n1))\n    np.fill_diagonal(A1, 2.0)\n    A1 += np.diag(-1.0 * np.ones(n1 - 1), k=1)\n    A1 += np.diag(-1.0 * np.ones(n1 - 1), k=-1)\n    matrices.append(A1)\n\n    # Case 2: Pentadiagonal matrix\n    n2 = 60\n    A2 = np.zeros((n2, n2))\n    np.fill_diagonal(A2, 4.0)\n    A2 += np.diag(-1.0 * np.ones(n2 - 1), k=1)\n    A2 += np.diag(-1.0 * np.ones(n2 - 1), k=-1)\n    A2 += np.diag(-0.5 * np.ones(n2 - 2), k=2)\n    A2 += np.diag(-0.5 * np.ones(n2 - 2), k=-2)\n    matrices.append(A2)\n\n    # Case 3: Sparse random SDD matrix\n    n3 = 40\n    p = 0.06\n    delta = 0.5\n    seed = 7\n    rng = np.random.default_rng(seed)\n    A3 = np.zeros((n3, n3))\n    for i in range(n3):\n        for j in range(i + 1, n3):\n            if rng.random()  p:\n                v = rng.uniform(-0.2, 0.2)\n                A3[i, j] = v\n                A3[j, i] = v\n    row_sums = np.sum(np.abs(A3), axis=1)\n    np.fill_diagonal(A3, row_sums + delta)\n    matrices.append(A3)\n\n    # Case 4: Identity matrix\n    n4 = 25\n    A4 = np.identity(n4)\n    matrices.append(A4)\n\n    return matrices\n\ndef solve():\n    \"\"\"\n    Main solver function.\n    \"\"\"\n    test_cases = generate_matrices()\n    results = []\n    \n    for A in test_cases:\n        case_results = process_case(A)\n        results.extend(case_results)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3266515"}, {"introduction": "While the weighted Jacobi method's performance is invariant to the ordering of equations, other powerful methods like Successive Over-Relaxation (SOR) behave quite differently. The SOR iteration depends on a splitting of the matrix $A$ into its diagonal, lower, and upper triangular parts, a decomposition that is fundamentally altered by permutation. This exercise guides you to discover the profound impact of equation ordering on the SOR convergence rate and its optimal relaxation parameter, $\\omega_{\\text{opt}}$, revealing a critical subtlety in the design of iterative solvers [@problem_id:3266531].", "problem": "Consider the linear system $A x = b$ where $A \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite (SPD) matrix. Iterative methods for solving $A x = b$ often begin with the decomposition $A = D - L - U$, where $D$ is the diagonal of $A$, $L$ is the strictly lower-triangular part of $A$ with sign chosen so that $A = D - L - U$, and $U$ is the strictly upper-triangular part with the same sign convention. The Successive Over-Relaxation (SOR) method updates the iterate by weighting the Gauss–Seidel step with a relaxation parameter $\\omega \\in (0,2)$, and its linear error-propagation operator depends on $\\omega$ and on the splitting $A = D - L - U$. A permutation of the unknowns by a permutation matrix $P$ reorders the system to $\\tilde{A} = P A P^{\\top}$, and induces a new splitting $\\tilde{A} = \\tilde{D} - \\tilde{L} - \\tilde{U}$ defined relative to the new ordering.\n\nStarting from the decomposition $A = D - L - U$ and the definition of the SOR method as a weighted Gauss–Seidel iteration, derive the expression for the error-propagation operator as a function $T(\\omega)$ that depends on the splitting and the relaxation parameter. Then, by computing its spectral radius $\\rho(T(\\omega))$ for $\\omega \\in (0,2)$, determine a numerically optimal relaxation parameter $\\omega^\\star$ that minimizes $\\rho(T(\\omega))$.\n\nYou must construct and analyze the following test suite composed of matrices and permutations. For each case, you must:\n1. Form the matrix $A$.\n2. Apply the specified permutation vector $p$ to obtain $\\tilde{A}$ via $\\tilde{A}_{ij} = A_{p(i), p(j)}$.\n3. Construct $\\tilde{D}$, $\\tilde{L}$, and $\\tilde{U}$ from $\\tilde{A}$ so that $\\tilde{A} = \\tilde{D} - \\tilde{L} - \\tilde{U}$.\n4. Derive $T(\\omega)$ from first principles and compute $\\rho(T(\\omega))$ for $\\omega$ on a uniform grid over the open interval $(0,2)$ with step size $0.01$, and choose $\\omega^\\star$ as the grid value that minimizes $\\rho(T(\\omega))$.\n5. Report both $\\omega^\\star$ and the minimal spectral radius $\\min_{\\omega} \\rho(T(\\omega))$ as decimal floats rounded to six decimal places.\n\nTest suite:\n- Case 1 (two-dimensional Poisson, lexicographic ordering): $A$ is the $N^2 \\times N^2$ SPD matrix from the five-point finite-difference Laplacian on an $N \\times N$ interior grid with Dirichlet boundary conditions, with $N = 6$. Diagonal entries are $4$ and each interior point couples to its four grid neighbors with $-1$. The permutation is the identity (lexicographic row-major).\n- Case 2 (two-dimensional Poisson, red–black ordering): Same $A$ as Case 1. The permutation first lists all grid points with $(i+j)$ even, then all with $(i+j)$ odd, where $i$ and $j$ are zero-based row and column indices.\n- Case 3 (one-dimensional Poisson, lexicographic ordering): $A$ is the $n \\times n$ SPD tridiagonal matrix from the one-dimensional finite-difference Laplacian with Dirichlet boundary conditions, with $n = 20$, having $2$ on the diagonal and $-1$ on the first sub- and super-diagonals. The permutation is the identity.\n- Case 4 (one-dimensional Poisson, even–odd ordering): Same $A$ as Case 3. The permutation lists all even indices first, then all odd indices (zero-based).\n- Case 5 (diagonal SPD, identity ordering): $A$ is a diagonal matrix $\\operatorname{diag}(1,2,\\dots,10)$ of size $10 \\times 10$. The permutation is the identity.\n- Case 6 (diagonal SPD, random ordering): Same $A$ as Case 5. The permutation is any fixed nontrivial permutation of $\\{0,1,\\dots,9\\}$.\n\nYour program must implement the above steps and produce a single line containing the results as a comma-separated list of floats enclosed in square brackets, in the following order:\n$[\\omega^\\star_{1}, \\rho_{\\min,1}, \\omega^\\star_{2}, \\rho_{\\min,2}, \\omega^\\star_{3}, \\rho_{\\min,3}, \\omega^\\star_{4}, \\rho_{\\min,4}, \\omega^\\star_{5}, \\rho_{\\min,5}, \\omega^\\star_{6}, \\rho_{\\min,6}]$.\n\nAll numbers must be reported as decimal floats rounded to six decimal places. No physical units or angles are involved in this problem; do not include any units in your output. Your implementation must be a complete, runnable program that requires no input and uses only the specified libraries.\n\nThe objective is to construct an example where the optimal relaxation parameter depends strongly on the permutation of unknowns and to explain this behavior by the non-commutativity of taking strictly lower/upper triangular parts with respect to reordering, which changes the SOR iteration operator and its spectrum.", "solution": "The problem presented is a valid and well-posed exercise in numerical linear algebra. It is scientifically grounded, objective, and contains all necessary information to derive a unique solution for each test case. The objective is to analyze the Successive Over-Relaxation (SOR) iterative method for solving a linear system $A x = b$, with a focus on how the choice of an optimal relaxation parameter $\\omega^\\star$ is influenced by the ordering of the unknowns.\n\nWe begin by deriving the SOR iteration matrix, or error-propagation operator, $T(\\omega)$, from first principles. The starting point is the linear system $A x = b$, where $A \\in \\mathbb{R}^{n \\times n}$ is a given matrix. The SOR method is based on a splitting of the matrix $A$ into its diagonal, strictly lower-triangular, and strictly upper-triangular parts. As per the problem statement, we define this splitting as $A = D - L - U$, where $D$ is a diagonal matrix containing the diagonal entries of $A$, $-L$ is the strictly lower-triangular part of $A$, and $-U$ is the strictly upper-triangular part of $A$.\n\nThe SOR update rule for the $i$-th component of the solution vector at iteration $k+1$, denoted $x_i^{(k+1)}$, is a weighted average of the previous iterate $x_i^{(k)}$ and the Gauss-Seidel update:\n$$\nx_i^{(k+1)} = (1-\\omega) x_i^{(k)} + \\frac{\\omega}{a_{ii}} \\left( b_i - \\sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} - \\sum_{j=i+1}^{n} a_{ij} x_j^{(k)} \\right)\n$$\nwhere $\\omega \\in (0,2)$ is the relaxation parameter. To express this in matrix form, we can rearrange the equation for all components $i=1, \\dots, n$:\n$$\na_{ii} x_i^{(k+1)} + \\omega \\sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} = (1-\\omega) a_{ii} x_i^{(k)} - \\omega \\sum_{j=i+1}^{n} a_{ij} x_j^{(k)} + \\omega b_i\n$$\nUsing the matrix splitting definitions, the summations can be written in terms of the matrices $L$ and $U$. The left-hand side is $(D - \\omega L) x^{(k+1)}$, and the right-hand side is $((1-\\omega)D + \\omega U) x^{(k)} + \\omega b$. This gives the matrix form of the SOR iteration:\n$$\n(D - \\omega L) x^{(k+1)} = ((1-\\omega)D + \\omega U) x^{(k)} + \\omega b\n$$\nThe next iterate $x^{(k+1)}$ is thus:\n$$\nx^{(k+1)} = (D - \\omega L)^{-1} ((1-\\omega)D + \\omega U) x^{(k)} + \\omega (D - \\omega L)^{-1} b\n$$\nThis is a stationary iterative method of the form $x^{(k+1)} = T(\\omega) x^{(k)} + c$. The matrix $T(\\omega)$ is the error-propagation operator, given by:\n$$\nT(\\omega) = (D - \\omega L)^{-1} ((1-\\omega)D + \\omega U)\n$$\nThe convergence of the method is determined by the spectral radius of this operator, $\\rho(T(\\omega)) = \\max_i |\\lambda_i(T(\\omega))|$, where $\\{\\lambda_i\\}$ are the eigenvalues of $T(\\omega)$. The method converges if and only if $\\rho(T(\\omega))  1$. The optimal relaxation parameter, $\\omega^\\star$, is the value of $\\omega$ in the interval $(0,2)$ that minimizes this spectral radius.\n\nA permutation of the unknowns, represented by a permutation vector $p$, transforms the system $A x = b$ into $\\tilde{A} \\tilde{x} = \\tilde{b}$, where $\\tilde{A} = P A P^{\\top}$, $\\tilde{x} = Px$, and $\\tilde{b} = Pb$. The problem defines the permutation using a vector $p$ such that the new matrix elements are $\\tilde{A}_{ij} = A_{p(i), p(j)}$. This reordering induces a new splitting of the permuted matrix, $\\tilde{A} = \\tilde{D} - \\tilde{L} - \\tilde{U}$. Crucially, the operation of taking the strictly lower (or upper) triangular part of a matrix does not generally commute with permutation. That is, $\\tilde{L}$ is not necessarily equal to $P L P^{\\top}$. Consequently, the SOR iteration operator for the permuted system,\n$$\n\\tilde{T}(\\omega) = (\\tilde{D} - \\omega \\tilde{L})^{-1} ((1-\\omega)\\tilde{D} + \\omega \\tilde{U})\n$$\nis different from the operator for the original system, and its spectrum $\\rho(\\tilde{T}(\\omega))$ will be different. This implies that the optimal relaxation parameter $\\omega^\\star$ is dependent on the ordering of the equations.\n\nThe problem requires a numerical search for $\\omega^\\star$ for six distinct cases. For each case, the permuted matrix $\\tilde{A}$ is formed. Then, its splitting $\\tilde{A} = \\tilde{D} - \\tilde{L} - \\tilde{U}$ is constructed. The spectral radius $\\rho(\\tilde{T}(\\omega))$ is computed for values of $\\omega$ on a uniform grid over $(0,2)$ with a step size of $0.01$. The value $\\omega^\\star$ is chosen as the grid point that yields the minimum spectral radius.\n\n- **Cases 1  2** and **Cases 3  4** compare standard lexicographic ordering with reorderings (red-black, even-odd) that are known to be \"consistently ordered\" for the given Laplacian matrices. For such orderings, there exists an analytical formula for $\\omega^\\star$, providing a check for the numerical results. The change from an arbitrary ordering to a consistently ordered one significantly alters the structure of $\\tilde{L}$ and $\\tilde{U}$ and thus the optimal parameter.\n- **Cases 5  6** provide a counterexample. For a diagonal matrix $A$, any permutation $P$ results in another diagonal matrix $\\tilde{A} = P A P^{\\top}$. For any diagonal matrix, the strictly triangular parts $L$ and $U$ are zero matrices. Thus, $\\tilde{L} = \\tilde{U} = 0$, just as $L=U=0$. In this special case, the splitting operation commutes with permutation. The SOR operator simplifies to $T(\\omega) = (1-\\omega)I$, whose spectral radius is $|1-\\omega|$. This is minimized at $\\omega=1$, regardless of the permutation. These cases demonstrate a scenario where reordering has no effect on $\\omega^\\star$.\n\nThe numerical implementation will proceed by constructing the matrices and permutations for each case, performing the grid search for $\\omega^\\star$, and reporting the required values.", "answer": "```python\nimport numpy as np\n\ndef solve_case(A, p):\n    \"\"\"\n    Solves for the optimal SOR parameter for a given matrix and permutation.\n\n    Args:\n        A (np.ndarray): The original matrix.\n        p (np.ndarray): The permutation vector.\n\n    Returns:\n        tuple: A tuple containing the optimal omega and the minimum spectral radius.\n    \"\"\"\n    # Apply permutation: A_perm[i, j] = A[p[i], p[j]]\n    A_perm = A[np.ix_(p, p)]\n    \n    # Decompose the permuted matrix\n    D_tilde = np.diag(np.diag(A_perm))\n    # Note on sign convention: A = D - L - U, so -L is the strictly lower part.\n    L_tilde = -np.tril(A_perm, k=-1)\n    U_tilde = -np.triu(A_perm, k=1)\n\n    omegas = np.arange(0.01, 2.0, 0.01)\n    min_rho = float('inf')\n    omega_star = -1.0\n\n    for omega in omegas:\n        # Construct the SOR iteration matrix T_omega\n        # T_omega = inv(D - omega*L) * ((1-omega)*D + omega*U)\n        M = D_tilde - omega * L_tilde\n        N_mat = (1 - omega) * D_tilde + omega * U_tilde\n        \n        # Using linalg.solve is more stable than computing the inverse explicitly\n        try:\n            T_omega = np.linalg.solve(M, N_mat)\n        except np.linalg.LinAlgError:\n            continue # Matrix M is singular for this omega, skip\n\n        # Compute spectral radius\n        eigenvalues = np.linalg.eigvals(T_omega)\n        rho = np.max(np.abs(eigenvalues))\n\n        if rho  min_rho:\n            min_rho = rho\n            omega_star = omega\n            \n    return omega_star, min_rho\n\ndef make_laplacian_2d(N):\n    \"\"\"Creates the 2D finite-difference Laplacian matrix.\"\"\"\n    n = N * N\n    A = np.zeros((n, n))\n    for i in range(N):\n        for j in range(N):\n            k = i * N + j\n            A[k, k] = 4\n            if i > 0: A[k, (i-1)*N + j] = -1 # Up\n            if i  N-1: A[k, (i+1)*N + j] = -1 # Down\n            if j > 0: A[k, i*N + (j-1)] = -1 # Left\n            if j  N-1: A[k, i*N + (j+1)] = -1 # Right\n    return A\n\ndef make_laplacian_1d(n):\n    \"\"\"Creates the 1D finite-difference Laplacian matrix.\"\"\"\n    A = np.zeros((n, n))\n    for i in range(n):\n        A[i, i] = 2\n        if i > 0: A[i, i-1] = -1\n        if i  n-1: A[i, i+1] = -1\n    return A\n\ndef get_red_black_perm(N):\n    \"\"\"Generates the red-black ordering permutation for an N x N grid.\"\"\"\n    n = N * N\n    red_indices = []\n    black_indices = []\n    for i in range(N):\n        for j in range(N):\n            k = i * N + j\n            if (i + j) % 2 == 0:\n                red_indices.append(k)\n            else:\n                black_indices.append(k)\n    return np.array(red_indices + black_indices, dtype=int)\n\ndef get_even_odd_perm(n):\n    \"\"\"Generates the even-odd ordering permutation for a 1D vector of size n.\"\"\"\n    even_indices = [i for i in range(n) if i % 2 == 0]\n    odd_indices = [i for i in range(n) if i % 2 != 0]\n    return np.array(even_indices + odd_indices, dtype=int)\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    \n    # Case 1: 2D Poisson, N=6, lexicographic\n    N1 = 6\n    n1 = N1 * N1\n    A1 = make_laplacian_2d(N1)\n    p1 = np.arange(n1)\n    \n    # Case 2: 2D Poisson, N=6, red-black\n    A2 = A1\n    p2 = get_red_black_perm(N1)\n    \n    # Case 3: 1D Poisson, n=20, lexicographic\n    n3 = 20\n    A3 = make_laplacian_1d(n3)\n    p3 = np.arange(n3)\n    \n    # Case 4: 1D Poisson, n=20, even-odd\n    A4 = A3\n    p4 = get_even_odd_perm(n3)\n    \n    # Case 5: Diagonal SPD, identity ordering\n    n5 = 10\n    A5 = np.diag(np.arange(1, n5 + 1))\n    p5 = np.arange(n5)\n    \n    # Case 6: Diagonal SPD, random (reversed) ordering\n    A6 = A5\n    p6 = np.arange(n5 - 1, -1, -1)\n\n    test_cases = [\n        (A1, p1),\n        (A2, p2),\n        (A3, p3),\n        (A4, p4),\n        (A5, p5),\n        (A6, p6),\n    ]\n\n    results = []\n    for A, p in test_cases:\n        omega_star, min_rho = solve_case(A, p)\n        results.append(round(omega_star, 6))\n        results.append(round(min_rho, 6))\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3266531"}, {"introduction": "Finding a globally optimal, static relaxation parameter can be difficult or computationally expensive. An alternative approach is to adapt the parameter $\\omega$ dynamically during the iteration based on observed performance. This practice introduces an adaptive Richardson method where $\\omega$ is adjusted based on a simple backtracking rule, ensuring monotonic reduction of the residual norm. By implementing this strategy and analyzing its theoretical guarantees, you will explore a powerful technique that forms the basis for more advanced optimization algorithms [@problem_id:3266497].", "problem": "Consider solving a linear system $A x = b$ with a stationary Richardson iteration using a relaxation parameter $\\omega$ that is chosen adaptively. Let $A \\in \\mathbb{R}^{n \\times n}$ be a symmetric positive definite (SPD) matrix and $b \\in \\mathbb{R}^{n}$. Define the residual at iteration $k$ as $r_k = b - A x_k$. The Richardson update with relaxation is $x_{k+1} = x_k + \\omega r_k$. You are asked to implement an adaptive selection of $\\omega$ based on the following rule.\n\nAlgorithmic rule for adaptive relaxation parameter:\n- Given a current iterate $x_k$, its residual $r_k$, and a current relaxation parameter $\\omega_k  0$, attempt the trial update $x_{k+1}^{\\mathrm{trial}} = x_k + \\omega_k r_k$ and compute $r_{k+1}^{\\mathrm{trial}} = b - A x_{k+1}^{\\mathrm{trial}}$.\n- If $\\lVert r_{k+1}^{\\mathrm{trial}} \\rVert_2 \\le \\lVert r_k \\rVert_2$, accept the step: set $x_{k+1} \\leftarrow x_{k+1}^{\\mathrm{trial}}$, $r_{k+1} \\leftarrow r_{k+1}^{\\mathrm{trial}}$, and keep $\\omega_{k+1} \\leftarrow \\omega_k$ for the next iteration.\n- If $\\lVert r_{k+1}^{\\mathrm{trial}} \\rVert_2  \\lVert r_k \\rVert_2$, reject the step and backtrack the relaxation parameter by halving: set $\\omega_k \\leftarrow \\omega_k / 2$ and retry from the same $x_k$ and $r_k$; repeat the halving until a trial step yields $\\lVert r_{k+1}^{\\mathrm{trial}} \\rVert_2 \\le \\lVert r_k \\rVert_2$, then accept as above.\n\nTheoretical tasks:\n- Starting only from the definition of the residual and the update rule, and from the spectral theorem for real symmetric matrices, derive a sufficient condition on $\\omega$ (in terms of the eigenvalues of $A$) under which the residual contracts for every residual vector $r_k$. Explain why the backtracking halving must terminate in finitely many halvings for any SPD matrix $A$ and any initial $\\omega_k  0$, and justify that the accepted step is a contraction in the Euclidean norm. State a bound on the maximum number of halvings needed at any iteration in terms of the spectrum of $A$ and the current $\\omega_k$.\n- Explain why the accepted iterates yield a nonincreasing sequence of residual norms $\\lVert r_k \\rVert_2$, and discuss under what additional condition on the sequence $\\{\\omega_k\\}$ you obtain a linear convergence rate.\n\nImplementation task:\n- Implement the above adaptive Richardson method. For each test case below, run exactly $N$ accepted iterations, where $N$ is specified for the test case. The current $\\omega$ persists across accepted steps and is only decreased by the backtracking rule when a trial step increases the residual norm. If $\\lVert r_k \\rVert_2 = 0$ at any point, you may terminate early and keep the current $x_k$ and $\\omega_k$ for the remaining count of accepted steps.\n\nTest suite:\n- Test case $1$:\n  - $A = \\mathrm{diag}([1, 10, 50])$\n  - $b = [1, 1, 1]^\\top$\n  - $x_0 = [0, 0, 0]^\\top$\n  - $\\omega_0 = 1.0$\n  - $N = 10$\n- Test case $2$:\n  - $A = \\begin{bmatrix} 4  1  0 \\\\ 1  3  0.5 \\\\ 0  0.5  2 \\end{bmatrix}$\n  - $b = [1, 2, 3]^\\top$\n  - $x_0 = [0, 0, 0]^\\top$\n  - $\\omega_0 = 0.3$\n  - $N = 10$\n- Test case $3$:\n  - $A = \\mathrm{diag}([0.01, 1, 2])$\n  - $b = [1, 1, 1]^\\top$\n  - $x_0 = [0, 0, 0]^\\top$\n  - $\\omega_0 = 1.2$\n  - $N = 12$\n- Test case $4$:\n  - $A \\in \\mathbb{R}^{5 \\times 5}$ is the tridiagonal matrix with $2$ on the diagonal and $-1$ on the first sub- and super-diagonals:\n    $A = \\begin{bmatrix}\n    2  -1  0  0  0\\\\\n    -1  2  -1  0  0\\\\\n    0  -1  2  -1  0\\\\\n    0  0  -1  2  -1\\\\\n    0  0  0  -1  2\n    \\end{bmatrix}$\n  - $b = [1, 0, 0, 0, 1]^\\top$\n  - $x_0 = [0, 0, 0, 0, 0]^\\top$\n  - $\\omega_0 = 0.8$\n  - $N = 8$\n\nFinal output specification:\n- For each test case, compute the Euclidean norm of the final residual $\\lVert r_{\\mathrm{final}} \\rVert_2$ after $N$ accepted iterations and the final relaxation parameter $\\omega_{\\mathrm{final}}$. Your program should produce a single line of output containing all results aggregated in order as a comma-separated list enclosed in square brackets:\n  - $[\\lVert r_{\\mathrm{final}}^{(1)} \\rVert_2, \\omega_{\\mathrm{final}}^{(1)}, \\lVert r_{\\mathrm{final}}^{(2)} \\rVert_2, \\omega_{\\mathrm{final}}^{(2)}, \\lVert r_{\\mathrm{final}}^{(3)} \\rVert_2, \\omega_{\\mathrm{final}}^{(3)}, \\lVert r_{\\mathrm{final}}^{(4)} \\rVert_2, \\omega_{\\mathrm{final}}^{(4)}]$\n- Round each floating-point number in the output to exactly $6$ decimal places.\n- No physical units are involved. Angles are not used.\n\nYour submission must be a complete, runnable program that performs these computations exactly as specified and prints only the required single line. No user input is permitted.", "solution": "The problem is valid. It presents a well-defined numerical algorithm, the adaptive Richardson iteration for solving linear systems, and poses clear theoretical and implementation tasks based on established principles of numerical linear algebra. All necessary data and conditions for the test cases are provided, and the problem is scientifically sound and objective.\n\n### Theoretical Analysis\n\nThe problem asks for a theoretical analysis of the adaptive Richardson method for solving the linear system $A x = b$, where $A \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite (SPD) matrix. The iteration is given by $x_{k+1} = x_k + \\omega r_k$, with the residual $r_k = b - A x_k$.\n\n#### 1. Sufficient Condition for Contraction and Termination of Backtracking\n\nFirst, we derive the update rule for the residual.\n$$\nr_{k+1} = b - A x_{k+1} = b - A(x_k + \\omega r_k) = (b - A x_k) - \\omega A r_k = r_k - \\omega A r_k\n$$\nThis can be written as $r_{k+1} = (I - \\omega A) r_k$, where $I$ is the identity matrix. The matrix $G(\\omega) = I - \\omega A$ is the iteration matrix.\n\nFor the residual norm to contract, we require $\\lVert r_{k+1} \\rVert_2  \\lVert r_k \\rVert_2$ for any non-zero residual $r_k$. This is guaranteed if the induced $2$-norm of the iteration matrix is less than $1$, i.e., $\\lVert G(\\omega) \\rVert_2  1$.\n\nSince $A$ is a real symmetric matrix, the spectral theorem states that it has a full set of orthonormal eigenvectors, $v_1, \\dots, v_n$, with corresponding real eigenvalues $\\lambda_1, \\dots, \\lambda_n$. As $A$ is positive definite, all its eigenvalues are positive: $\\lambda_i  0$ for all $i$. Let $\\lambda_{\\min} = \\min_i \\lambda_i$ and $\\lambda_{\\max} = \\max_i \\lambda_i$ be the smallest and largest eigenvalues of $A$, respectively.\n\nThe iteration matrix $G(\\omega) = I - \\omega A$ is also symmetric. The eigenvalues of $G(\\omega)$ are $\\mu_i = 1 - \\omega \\lambda_i$. For a symmetric matrix, the induced $2$-norm is equal to its spectral radius (the maximum absolute value of its eigenvalues):\n$$\n\\lVert G(\\omega) \\rVert_2 = \\rho(G(\\omega)) = \\max_{i} |\\mu_i| = \\max_{i} |1 - \\omega \\lambda_i|\n$$\nThe condition for guaranteed contraction, $\\lVert G(\\omega) \\rVert_2  1$, becomes $\\max_{i} |1 - \\omega \\lambda_i|  1$. This is equivalent to requiring $-1  1 - \\omega \\lambda_i  1$ for all $i=1, \\dots, n$.\n\nLet's analyze the two inequalities:\n1.  $1 - \\omega \\lambda_i  1 \\implies -\\omega \\lambda_i  0$. Since $\\omega  0$ and $\\lambda_i  0$, this inequality is always satisfied.\n2.  $-1  1 - \\omega \\lambda_i \\implies \\omega \\lambda_i  2 \\implies \\omega  2 / \\lambda_i$. This must hold for all eigenvalues $\\lambda_i$. The most restrictive condition is for the largest eigenvalue, $\\lambda_{\\max}$.\n\nThus, a sufficient condition on $\\omega$ for the residual norm to contract for any non-zero residual is $0  \\omega  2 / \\lambda_{\\max}$.\n\nThe backtracking algorithm accepts a step if $\\lVert r_{k+1}^{\\mathrm{trial}} \\rVert_2 \\le \\lVert r_k \\rVert_2$. This is guaranteed to happen if $\\lVert G(\\omega) \\rVert_2 \\le 1$, which is equivalent to $\\max_i |1 - \\omega \\lambda_i| \\le 1$. From the derivation above, this holds if $\\omega \\le 2 / \\lambda_{\\max}$.\nThe backtracking procedure starts with a given $\\omega_k  0$ and, upon rejection, replaces it with $\\omega_k/2$. After $m$ halvings, the new parameter is $\\omega_k' = \\omega_k / 2^m$. A step will be accepted once $\\omega_k' \\le 2/\\lambda_{\\max}$. This inequality can be written as $\\omega_k / 2^m \\le 2/\\lambda_{\\max}$, which implies $2^m \\ge \\omega_k \\lambda_{\\max} / 2$. Taking the base-$2$ logarithm, we get $m \\ge \\log_2(\\omega_k \\lambda_{\\max} / 2)$. Since $\\omega_k  0$ and $\\lambda_{\\max}  0$, the argument of the logarithm is positive, and a finite integer $m$ satisfying this condition always exists. This proves that the halving process must terminate. The maximum number of halvings at any iteration $k$ is bounded by $\\max(0, \\lceil \\log_2(\\omega_k \\lambda_{\\max} / 2) \\rceil)$, where the ceiling function ensures an integer result and the $\\max$ handles the case where the initial $\\omega_k$ already satisfies the condition. The accepted step is a non-expansion, $\\lVert r_{k+1} \\rVert_2 \\le \\lVert r_k \\rVert_2$. It is a strict contraction unless $\\lVert G(\\omega) \\rVert_2 = 1$, which can happen if $\\omega = 2/\\lambda_i$ for some $i$ and $r_k$ lies in the corresponding eigenspace. The halving ensures $\\omega$ will eventually enter the open interval $(0, 2/\\lambda_{\\max})$, guaranteeing a strict contraction for any non-zero residual.\n\n#### 2. Non-increasing Residual Norm and Linear Convergence\n\nThe sequence of residual norms $\\{\\lVert r_k \\rVert_2\\}$ resulting from the accepted iterates is non-increasing by the very definition of the acceptance criterion: $\\lVert r_{k+1} \\rVert_2 \\le \\lVert r_k \\rVert_2$.\n\nLinear convergence is achieved if there exists a constant $C \\in [0, 1)$ such that $\\lVert r_{k+1} \\rVert_2 \\le C \\lVert r_k \\rVert_2$ for all $k$ sufficiently large. The contraction factor at step $k$, using the accepted parameter $\\omega_k$, is $c_k = \\lVert I - \\omega_k A \\rVert_2$. For linear convergence, we require that the sequence of contraction factors be bounded away from $1$, i.e., there exists a constant $C  1$ such that $c_k \\le C$ for all $k$.\n\nThe contraction factor is $c_k = \\max(|1 - \\omega_k \\lambda_{\\min}|, |1 - \\omega_k \\lambda_{\\max}|)$. The adaptive algorithm as described only decreases $\\omega$ via backtracking. While this ensures that if the initial $\\omega_0$ is too large, it will be reduced to the stable range $\\omega \\le 2/\\lambda_{\\max}$, it does not prevent $\\omega$ from becoming arbitrarily small. If, for some reason, the algorithm must repeatedly backtrack, the sequence of accepted parameters $\\{\\omega_k\\}$ could converge to $0$. If $\\omega_k \\to 0$, then $c_k \\to \\max(|1-0|, |1-0|) = 1$. This would cause convergence to stall, losing the linear rate.\n\nTherefore, an additional condition to guarantee a linear convergence rate is that the sequence of accepted relaxation parameters $\\{\\omega_k\\}$ must be bounded away from zero. That is, there must exist some $\\omega_{\\min}  0$ such that $\\omega_k \\ge \\omega_{\\min}$ for all $k$. If this holds, and we also have an upper bound $\\omega_k \\le \\omega_{\\max}  2/\\lambda_{\\max}$, then the convergence factor $c_k$ is uniformly bounded by $C = \\max(|1 - \\omega_{\\min} \\lambda_{\\min}|, |1 - \\omega_{\\max} \\lambda_{\\max}|)  1$, ensuring linear convergence.\n\n### Implementation\n\nThe implementation will follow the algorithm described. For each of the $N$ required accepted steps, we perform a trial-and-error process. An inner loop attempts an update with the current $\\omega_k$. If the residual norm increases, $\\omega_k$ is halved and the trial is repeated with the same $x_k$ and $r_k$. Once a trial step is accepted, the state $(x, r)$ is updated, and the successful $\\omega$ is carried over to the next iteration.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef adaptive_richardson(A, b, x0, omega0, N):\n    \"\"\"\n    Solves Ax = b using an adaptive Richardson iteration.\n\n    Args:\n        A (np.ndarray): The matrix A of the system.\n        b (np.ndarray): The vector b of the system.\n        x0 (np.ndarray): The initial guess for x.\n        omega0 (float): The initial relaxation parameter.\n        N (int): The number of accepted iterations to perform.\n\n    Returns:\n        tuple[float, float]: A tuple containing the final residual norm\n                             and the final relaxation parameter.\n    \"\"\"\n    x = np.copy(x0).astype(np.float64)\n    omega = float(omega0)\n    \n    # Initial residual\n    r = b - A @ x\n    \n    for _ in range(N):\n        norm_r_k = np.linalg.norm(r)\n        \n        # Terminate early if the solution is found\n        if np.isclose(norm_r_k, 0.0):\n            break\n\n        Ar = A @ r # Expensive product is computed once per accepted step\n        omega_trial = omega\n        \n        # Backtracking loop to find an acceptable omega\n        while True:\n            # Compute trial residual using the current omega_trial\n            r_trial = r - omega_trial * Ar\n            norm_r_trial = np.linalg.norm(r_trial)\n            \n            # Acceptance condition\n            if norm_r_trial = norm_r_k:\n                # Accept the step\n                x = x + omega_trial * r\n                r = r_trial\n                omega = omega_trial  # Keep this omega for the next iteration\n                break  # Exit backtracking and move to the next of N steps\n            else:\n                # Reject the step, halve omega and retry\n                omega_trial /= 2.0\n                \n    final_norm_r = np.linalg.norm(r)\n    final_omega = omega\n    \n    return final_norm_r, final_omega\n\ndef solve():\n    \"\"\"\n    Defines test cases and runs the adaptive Richardson solver,\n    then prints the results in the specified format.\n    \"\"\"\n    \n    # Test case 1\n    A1 = np.diag([1.0, 10.0, 50.0])\n    b1 = np.array([1.0, 1.0, 1.0])\n    x01 = np.array([0.0, 0.0, 0.0])\n    omega01 = 1.0\n    N1 = 10\n    \n    # Test case 2\n    A2 = np.array([\n        [4.0, 1.0, 0.0],\n        [1.0, 3.0, 0.5],\n        [0.0, 0.5, 2.0]\n    ])\n    b2 = np.array([1.0, 2.0, 3.0])\n    x02 = np.array([0.0, 0.0, 0.0])\n    omega02 = 0.3\n    N2 = 10\n    \n    # Test case 3\n    A3 = np.diag([0.01, 1.0, 2.0])\n    b3 = np.array([1.0, 1.0, 1.0])\n    x03 = np.array([0.0, 0.0, 0.0])\n    omega03 = 1.2\n    N3 = 12\n\n    # Test case 4\n    diag4 = 2.0 * np.ones(5)\n    off_diag4 = -1.0 * np.ones(4)\n    A4 = np.diag(diag4) + np.diag(off_diag4, k=1) + np.diag(off_diag4, k=-1)\n    b4 = np.array([1.0, 0.0, 0.0, 0.0, 1.0])\n    x04 = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n    omega04 = 0.8\n    N4 = 8\n\n    test_cases = [\n        (A1, b1, x01, omega01, N1),\n        (A2, b2, x02, omega02, N2),\n        (A3, b3, x03, omega03, N3),\n        (A4, b4, x04, omega04, N4),\n    ]\n\n    results = []\n    for A, b, x0, omega0, N in test_cases:\n        final_norm_r, final_omega = adaptive_richardson(A, b, x0, omega0, N)\n        results.append(final_norm_r)\n        results.append(final_omega)\n\n    # Format output as a comma-separated list of numbers rounded to 6 decimal places.\n    output_str = \",\".join([f\"{val:.6f}\" for val in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "3266497"}]}