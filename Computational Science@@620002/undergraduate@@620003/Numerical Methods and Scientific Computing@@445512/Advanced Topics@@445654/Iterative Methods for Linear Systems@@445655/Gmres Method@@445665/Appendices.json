{"hands_on_practices": [{"introduction": "The Generalized Minimal Residual (GMRES) method is an iterative process that begins with an initial guess and systematically improves upon it. The very first step is to quantify how far this initial guess is from yielding the correct right-hand side by calculating the initial residual. This exercise provides practice in computing this fundamental quantity, which forms the starting point for the entire GMRES algorithm and the basis for the Krylov subspace it constructs [@problem_id:2214791].", "problem": "The Generalized Minimal Residual (GMRES) method is a popular iterative algorithm for solving large, sparse, non-symmetric linear systems of equations of the form $Ax=b$. A crucial first step in the algorithm is to compute the initial residual based on an initial guess for the solution vector $x$.\n\nConsider the linear system $Ax=b$ where the matrix $A$ and the vector $b$ are given by:\n$$\nA = \\begin{pmatrix} 4 & -1 \\\\ 5 & 2 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 10 \\\\ 3 \\end{pmatrix}\n$$\nSuppose we start the GMRES algorithm with an initial guess $x_0$ given by:\n$$\nx_0 = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}\n$$\nCalculate the initial residual vector, $r_0$. Express your answer as a row matrix with two components.", "solution": "In GMRES, the initial residual is defined as the difference between the right-hand side vector and the product of the matrix with the initial guess:\n$$\nr_{0} = b - A x_{0}.\n$$\nGiven\n$$\nA = \\begin{pmatrix} 4 & -1 \\\\ 5 & 2 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 10 \\\\ 3 \\end{pmatrix}, \\quad x_{0} = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix},\n$$\nfirst compute the matrix-vector product $A x_{0}$:\n$$\nA x_{0} = \\begin{pmatrix} 4 & -1 \\\\ 5 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}\n= \\begin{pmatrix} 4 \\cdot 1 + (-1) \\cdot (-2) \\\\ 5 \\cdot 1 + 2 \\cdot (-2) \\end{pmatrix}\n= \\begin{pmatrix} 6 \\\\ 1 \\end{pmatrix}.\n$$\nThen compute the residual:\n$$\nr_{0} = b - A x_{0} = \\begin{pmatrix} 10 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 6 \\\\ 1 \\end{pmatrix}\n= \\begin{pmatrix} 4 \\\\ 2 \\end{pmatrix}.\n$$\nExpressed as a row matrix with two components, this is\n$$\n\\begin{pmatrix} 4 & 2 \\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix} 4 & 2 \\end{pmatrix}}$$", "id": "2214791"}, {"introduction": "With the initial residual in hand, GMRES builds a sequence of approximations within a special subspace, called a Krylov subspace, to find the best possible correction to the solution at each step. This exercise demystifies the process by guiding you through the first two complete iterations for a simple $2 \\times 2$ system [@problem_id:3237128]. By performing the Arnoldi process to construct an orthonormal basis and solving the small-scale minimization problem by hand, you will gain a concrete understanding of the algorithm's inner machinery.", "problem": "Consider the linear system $A x = b$ with \n$$A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.$$\nStarting from the initial guess $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, and using the standard Euclidean inner product, perform the Generalized Minimal Residual (GMRES) method for $k = 1$ and $k = 2$ iterations, without preconditioning. \n\nUse only the foundational characterization that at iteration $k$, GMRES chooses $x_k$ in $x_0 + \\mathcal{K}_k(A, r_0)$, where $\\mathcal{K}_k(A, r_0)$ is the $k$-th Krylov subspace generated by $A$ and $r_0 = b - A x_0$, so as to minimize the Euclidean norm of the residual $r_k = b - A x_k$. Construct the necessary orthonormal basis of the Krylov subspace by the Arnoldi process, and from first principles of least-squares residual minimization on the projected problem, compute the first two GMRES iterates $x_1$ and $x_2$ exactly.\n\nProvide your final answer as a single row matrix $\\begin{pmatrix} x_{1,1} & x_{1,2} & x_{2,1} & x_{2,2} \\end{pmatrix}$ with exact rational entries. No rounding is required.", "solution": "The problem is to perform the first two iterations ($k=1$ and $k=2$) of the Generalized Minimal Residual (GMRES) method for the linear system $Ax=b$, starting from a zero initial guess. The problem is well-defined and all necessary data are provided.\n\nThe given system is defined by:\n$$A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\n\nThe GMRES method constructs an approximate solution $x_k$ at iteration $k$ of the form $x_k = x_0 + z_k$, where $z_k$ is chosen from the $k$-th Krylov subspace $\\mathcal{K}_k(A, r_0) = \\text{span}\\{r_0, Ar_0, \\dots, A^{k-1}r_0\\}$. The vector $z_k$ is chosen to minimize the Euclidean norm of the residual $r_k = b - Ax_k$.\n\nFirst, we calculate the initial residual $r_0$:\n$$r_0 = b - A x_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$$\n\nThe Arnoldi process is used to generate an orthonormal basis $V_k = [v_1, v_2, \\dots, v_k]$ for the Krylov subspace $\\mathcal{K}_k(A, r_0)$. The process starts by normalizing the initial residual. Let $\\beta = \\|r_0\\|_2$.\n$$\\beta = \\left\\| \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\right\\|_2 = \\sqrt{1^2 + 0^2} = 1$$\nThe first basis vector $v_1$ is:\n$$v_1 = \\frac{r_0}{\\beta} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$$\n\nAt iteration $k$, we seek $x_k = x_0 + V_k y_k$ for some vector $y_k \\in \\mathbb{R}^k$ that minimizes $\\|b - A(x_0 + V_k y_k)\\|_2 = \\|r_0 - A V_k y_k\\|_2$.\nFrom the Arnoldi process, we have the relation $A V_k = V_{k+1} \\tilde{H}_k$, where $\\tilde{H}_k$ is a $(k+1) \\times k$ upper Hessenberg matrix.\nSubstituting this into the minimization problem, and using $r_0 = \\beta v_1 = \\beta V_{k+1} e_1$ (where $e_1$ is the first standard basis vector in $\\mathbb{R}^{k+1}$), we obtain:\n$$\\| \\beta V_{k+1} e_1 - V_{k+1} \\tilde{H}_k y_k \\|_2 = \\| V_{k+1} (\\beta e_1 - \\tilde{H}_k y_k) \\|_2$$\nSince $V_{k+1}$ has orthonormal columns, this simplifies to finding $y_k$ that solves the least-squares problem:\n$$y_k = \\arg\\min_{y \\in \\mathbb{R}^k} \\| \\beta e_1 - \\tilde{H}_k y \\|_2$$\n\n**Iteration $k=1$**\n\nWe first execute one step of the Arnoldi iteration to find $v_2$ and the matrix $\\tilde{H}_1$.\nCompute $w_1 = A v_1$:\n$$w_1 = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$$\nThe entry $h_{1,1}$ is the projection of $w_1$ onto $v_1$:\n$$h_{1,1} = v_1^T w_1 = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 2$$\nThe unnormalized next vector is $\\tilde{w}_1 = w_1 - h_{1,1}v_1$:\n$$\\tilde{w}_1 = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} - 2 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$$\nIts norm is $h_{2,1} = \\|\\tilde{w}_1\\|_2 = \\sqrt{0^2+1^2} = 1$.\nThe second basis vector is $v_2 = \\tilde{w}_1 / h_{2,1}$:\n$$v_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$$\nThe matrix $\\tilde{H}_1$ is a $2 \\times 1$ matrix:\n$$\\tilde{H}_1 = \\begin{pmatrix} h_{1,1} \\\\ h_{2,1} \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$$\nWe now solve the least-squares problem for $y_1 \\in \\mathbb{R}$:\n$$y_1 = \\arg\\min_{y \\in \\mathbb{R}} \\left\\| \\beta e_1 - \\tilde{H}_1 y \\right\\|_2 = \\arg\\min_{y_1 \\in \\mathbb{R}} \\left\\| \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} y_1 \\right\\|_2$$\nThe solution is given by the normal equations $(\\tilde{H}_1^T \\tilde{H}_1) y_1 = \\tilde{H}_1^T (\\beta e_1)$.\n$$\\tilde{H}_1^T \\tilde{H}_1 = \\begin{pmatrix} 2 & 1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 2^2 + 1^2 = 5$$\n$$\\tilde{H}_1^T (\\beta e_1) = \\begin{pmatrix} 2 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 2$$\nSo, $5 y_1 = 2$, which gives $y_1 = \\frac{2}{5}$.\nThe first GMRES iterate $x_1$ is:\n$$x_1 = x_0 + V_1 y_1 = x_0 + v_1 y_1 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\frac{2}{5} = \\begin{pmatrix} \\frac{2}{5} \\\\ 0 \\end{pmatrix}$$\n\n**Iteration $k=2$**\n\nWe continue the Arnoldi process to find the second column of $\\tilde{H}_2$.\nCompute $w_2 = A v_2$:\n$$w_2 = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$$\nThe entries $h_{1,2}$ and $h_{2,2}$ are the projections of $w_2$ onto $v_1$ and $v_2$:\n$$h_{1,2} = v_1^T w_2 = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = 1$$\n$$h_{2,2} = v_2^T w_2 = \\begin{pmatrix} 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = 2$$\nThe unnormalized next vector $\\tilde{w}_2$ is:\n$$\\tilde{w}_2 = w_2 - h_{1,2} v_1 - h_{2,2} v_2 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} - 1 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - 2 \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\nThe norm is $h_{3,2} = \\|\\tilde{w}_2\\|_2 = 0$. This indicates that the Krylov subspace $\\mathcal{K}_2(A, r_0)$ is invariant under $A$, and the Arnoldi process terminates. For a $2 \\times 2$ system, this means GMRES(2) will find the exact solution.\nThe matrix $\\tilde{H}_2$ is a $3 \\times 2$ matrix:\n$$\\tilde{H}_2 = \\begin{pmatrix} h_{1,1} & h_{1,2} \\\\ h_{2,1} & h_{2,2} \\\\ 0 & h_{3,2} \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\\\ 0 & 0 \\end{pmatrix}$$\nWe solve the least-squares problem for $y_2 = \\begin{pmatrix} y_{2,1} \\\\ y_{2,2} \\end{pmatrix} \\in \\mathbb{R}^2$:\n$$y_2 = \\arg\\min_{y \\in \\mathbb{R}^2} \\| \\beta e_1 - \\tilde{H}_2 y \\|_2 = \\arg\\min_{y_2 \\in \\mathbb{R}^2} \\left\\| \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\\\ 0 & 0 \\end{pmatrix} y_2 \\right\\|_2$$\nSince the last row of $\\tilde{H}_2$ is zero, the residual for this problem will be zero if we can solve the upper $2 \\times 2$ system exactly. We need to solve $H_2 y_2 = \\beta e_1^{(2)}$, where $H_2$ is the upper $2 \\times 2$ part of $\\tilde{H}_2$ and $e_1^{(2)}$ is the first standard basis vector in $\\mathbb{R}^2$.\n$$\\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} y_{2,1} \\\\ y_{2,2} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$$\nFrom the second equation, $y_{2,1} + 2y_{2,2} = 0$, so $y_{2,1} = -2y_{2,2}$.\nSubstituting into the first equation: $2(-2y_{2,2}) + y_{2,2} = 1$, which simplifies to $-3y_{2,2} = 1$, so $y_{2,2} = -\\frac{1}{3}$.\nThis gives $y_{2,1} = -2(-\\frac{1}{3}) = \\frac{2}{3}$.\nThus, the coefficient vector is $y_2 = \\begin{pmatrix} \\frac{2}{3} \\\\ -\\frac{1}{3} \\end{pmatrix}$.\nThe second GMRES iterate $x_2$ is:\n$$x_2 = x_0 + V_2 y_2 = x_0 + v_1 y_{2,1} + v_2 y_{2,2} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\frac{2}{3} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\left(-\\frac{1}{3}\\right) = \\begin{pmatrix} \\frac{2}{3} \\\\ -\\frac{1}{3} \\end{pmatrix}$$\nAs expected, $x_2$ is the exact solution to the system $Ax=b$.\n\nThe computed iterates are $x_1 = \\begin{pmatrix} \\frac{2}{5} \\\\ 0 \\end{pmatrix}$ and $x_2 = \\begin{pmatrix} \\frac{2}{3} \\\\ -\\frac{1}{3} \\end{pmatrix}$. The required components are $x_{1,1} = \\frac{2}{5}$, $x_{1,2} = 0$, $x_{2,1} = \\frac{2}{3}$, and $x_{2,2} = -\\frac{1}{3}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{2}{5} & 0 & \\frac{2}{3} & -\\frac{1}{3}\n\\end{pmatrix}\n}\n$$", "id": "3237128"}, {"introduction": "A common misconception is that an algorithm designed to find a better solution must reduce the error at every single step. GMRES, however, is designed to monotonically decrease the norm of the residual, which is not equivalent to the norm of the true error. This practice problem presents a scenario with a highly non-normal matrix where, counter-intuitively, the error to the true solution actually increases after the first iteration, highlighting a critical and subtle aspect of the method's behavior [@problem_id:2398705].", "problem": "Consider the linear system $A x = b$ with the $2 \\times 2$ matrix\n$$\nA = \\begin{pmatrix}\n1 & 10 \\\\\n0 & 1\n\\end{pmatrix},\n$$\nthe right-hand side $b = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, and the initial guess $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. Let $\\|\\cdot\\|$ denote the Euclidean norm. The residual for any $x$ is $r(x) = b - A x$. The Generalized Minimal Residual (GMRES) method defines the first iterate $x_{1}$ as the unique vector in the affine space $x_{0} + \\mathcal{K}_{1}(A, r_{0})$, where $r_{0} = r(x_{0})$, that minimizes $\\|r(x)\\|$.\n\nCompute the ratio\n$$\n\\frac{\\|x - x_{1}\\|}{\\|x - x_{0}\\|},\n$$\nwhere $x$ is the exact solution of $A x = b$, using the Euclidean norm. Round your answer to four significant figures.", "solution": "The problem statement is scientifically grounded, well-posed, and complete. It describes a standard application of the Generalized Minimal Residual (GMRES) method. We will proceed to solve it.\n\nThe objective is to compute the ratio $\\frac{\\|x - x_{1}\\|}{\\|x - x_{0}\\|}$, where $x$ is the exact solution of the linear system $A x = b$, $x_{0}$ is the given initial guess, and $x_{1}$ is the first iterate generated by the GMRES method. The norm is the Euclidean norm.\n\nFirst, we determine the exact solution $x$. The linear system is given by:\n$$\nA x = b \\implies \\begin{pmatrix} 1 & 10 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} x_a \\\\ x_b \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\nUsing back substitution, the second row gives $0 \\cdot x_a + 1 \\cdot x_b = 1$, which implies $x_b = 1$.\nSubstituting $x_b=1$ into the first row gives $1 \\cdot x_a + 10 \\cdot (1) = 1$, which yields $x_a = 1 - 10 = -9$.\nThus, the exact solution is $x = \\begin{pmatrix} -9 \\\\ 1 \\end{pmatrix}$.\n\nNext, we calculate the initial residual, $r_0$. The initial guess is $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\nThe residual is defined as $r(x) = b - A x$. The initial residual $r_0$ is:\n$$\nr_0 = r(x_0) = b - A x_0 = b - A \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = b = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\nThe first GMRES iterate, $x_1$, is defined as the vector in the affine space $x_0 + \\mathcal{K}_1(A, r_0)$ that minimizes the norm of the residual $\\|r(x)\\|$. The first Krylov subspace, $\\mathcal{K}_1(A, r_0)$, is the space spanned by the initial residual vector, $\\mathcal{K}_1(A, r_0) = \\text{span}\\{r_0\\}$.\nTherefore, $x_1$ can be expressed as:\n$$\nx_1 = x_0 + \\alpha r_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\alpha \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\alpha \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\nfor some scalar $\\alpha \\in \\mathbb{R}$.\n\nTo find the optimal $\\alpha$, we must minimize the norm of the corresponding residual, $r(x_1) = b - A x_1$.\nSubstituting the expression for $x_1$ and using $b=r_0$, we get:\n$$\nr(x_1) = b - A (\\alpha r_0) = r_0 - \\alpha (A r_0)\n$$\nWe need to find $\\alpha$ that minimizes the squared Euclidean norm $\\|r_0 - \\alpha A r_0\\|^2$. This is a standard linear least-squares problem, whose solution is the orthogonal projection of $r_0$ onto $A r_0$. The coefficient $\\alpha$ is given by:\n$$\n\\alpha = \\frac{\\langle r_0, A r_0 \\rangle}{\\|A r_0\\|^2}\n$$\nWe compute the vector $A r_0$:\n$$\nA r_0 = \\begin{pmatrix} 1 & 10 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 10 \\cdot 1 \\\\ 0 \\cdot 1 + 1 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 11 \\\\ 1 \\end{pmatrix}\n$$\nNow, we compute the inner product and the squared norm:\n$$\n\\langle r_0, A r_0 \\rangle = (1)(11) + (1)(1) = 12\n$$\n$$\n\\|A r_0\\|^2 = 11^2 + 1^2 = 121 + 1 = 122\n$$\nSubstituting these values, we find $\\alpha$:\n$$\n\\alpha = \\frac{12}{122} = \\frac{6}{61}\n$$\nWith this value of $\\alpha$, the first GMRES iterate is:\n$$\nx_1 = \\frac{6}{61} r_0 = \\frac{6}{61} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 6/61 \\\\ 6/61 \\end{pmatrix}\n$$\nNow we compute the error vectors $e_0 = x - x_0$ and $e_1 = x - x_1$, and their norms.\nThe initial error is:\n$$\ne_0 = x - x_0 = \\begin{pmatrix} -9 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -9 \\\\ 1 \\end{pmatrix}\n$$\nThe norm of the initial error is:\n$$\n\\|e_0\\| = \\|x - x_0\\| = \\sqrt{(-9)^2 + 1^2} = \\sqrt{81 + 1} = \\sqrt{82}\n$$\nThe error after the first iteration is:\n$$\ne_1 = x - x_1 = \\begin{pmatrix} -9 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 6/61 \\\\ 6/61 \\end{pmatrix} = \\begin{pmatrix} -9 - \\frac{6}{61} \\\\ 1 - \\frac{6}{61} \\end{pmatrix} = \\begin{pmatrix} -\\frac{549+6}{61} \\\\ \\frac{61-6}{61} \\end{pmatrix} = \\begin{pmatrix} -555/61 \\\\ 55/61 \\end{pmatrix}\n$$\nThe norm of this error vector is:\n$$\n\\|e_1\\| = \\|x - x_1\\| = \\sqrt{\\left(-\\frac{555}{61}\\right)^2 + \\left(\\frac{55}{61}\\right)^2} = \\frac{1}{61}\\sqrt{(-555)^2 + 55^2} = \\frac{1}{61}\\sqrt{308025 + 3025} = \\frac{\\sqrt{311050}}{61}\n$$\nFinally, we compute the required ratio:\n$$\n\\frac{\\|x - x_1\\|}{\\|x - x_0\\|} = \\frac{\\sqrt{311050}/61}{\\sqrt{82}} = \\sqrt{\\frac{311050}{61^2 \\cdot 82}} = \\sqrt{\\frac{311050}{3721 \\cdot 82}} = \\sqrt{\\frac{311050}{305122}}\n$$\nWe now evaluate this expression numerically:\n$$\n\\frac{\\|x - x_1\\|}{\\|x - x_0\\|} \\approx \\sqrt{1.0194280} \\approx 1.00966727\n$$\nThe problem asks for the answer to be rounded to four significant figures. Rounding $1.00966727$ gives $1.010$. The fact that the error norm increases is a known phenomenon for GMRES when applied to highly non-normal matrices, such as the one given in this problem. While the residual norm is guaranteed to be monotonically non-increasing, the error norm is not.", "answer": "$$\\boxed{1.010}$$", "id": "2398705"}]}