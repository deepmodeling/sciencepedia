{"hands_on_practices": [{"introduction": "A crucial first step in preconditioning is often to address systems where equations have vastly different scales. This simple but powerful technique, known as diagonal scaling, aims to balance the matrix rows or columns to improve its numerical properties. This exercise [@problem_id:2194457] provides a hands-on introduction to this idea by having you construct a diagonal preconditioner that performs row equilibration, a fundamental practice for stabilizing iterative solvers.", "problem": "Consider the linear system of equations $Ax = b$, where the matrix $A$ and vector $b$ are given by:\n$$\nA = \\begin{pmatrix} 2 & -1 & 0 \\\\ 1000 & 2000 & -1000 \\\\ 0 & -1 & 2 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\nIt is observed that the convergence of many iterative solvers can be improved by transforming the system into an equivalent one, $A'x = b'$, where the rows of the new matrix $A'$ have a more uniform \"size\". This transformation is achieved using an invertible matrix $P$ such that $A' = PA$ and $b' = Pb$.\n\nYour task is to find a diagonal matrix $P$ of the form\n$$\nP = \\begin{pmatrix} p_1 & 0 & 0 \\\\ 0 & p_2 & 0 \\\\ 0 & 0 & p_3 \\end{pmatrix}\n$$\nthat performs row equilibration on $A$. Specifically, determine the values of $p_1, p_2,$ and $p_3$ such that the infinity norm of each row vector in the resulting matrix $A'$ is equal to 1. The infinity norm of a vector $v = (v_1, v_2, \\dots, v_n)$ is defined as $\\|v\\|_{\\infty} = \\max_{i} |v_i|$. For simplicity, you may assume the diagonal entries of $P$ are positive.\n\nExpress your final answer as the 3x3 matrix $P$.", "solution": "We seek a diagonal scaling matrix $P=\\operatorname{diag}(p_{1},p_{2},p_{3})$ such that $A' = PA$ has each row with infinity norm equal to $1$. Let $a_{i}^{T}$ denote the $i$-th row of $A$. Then the $i$-th row of $A'$ is $p_{i}a_{i}^{T}$. Since $p_{i} > 0$, the infinity norm scales as\n$$\n\\|p_{i}a_{i}^{T}\\|_{\\infty} = p_{i}\\|a_{i}^{T}\\|_{\\infty}.\n$$\nImposing $\\|p_{i}a_{i}^{T}\\|_{\\infty} = 1$ gives\n$$\np_{i} = \\frac{1}{\\|a_{i}^{T}\\|_{\\infty}} \\quad \\text{for } i=1,2,3.\n$$\nCompute the row infinity norms of $A$:\n- Row $1$: $a_{1}^{T}=(2,-1,0)$, so $\\|a_{1}^{T}\\|_{\\infty}=\\max\\{|2|,|{-1}|,|0|\\}=2$, hence $p_{1}=\\frac{1}{2}$.\n- Row $2$: $a_{2}^{T}=(1000,2000,-1000)$, so $\\|a_{2}^{T}\\|_{\\infty}=\\max\\{1000,2000,1000\\}=2000$, hence $p_{2}=\\frac{1}{2000}$.\n- Row $3$: $a_{3}^{T}=(0,-1,2)$, so $\\|a_{3}^{T}\\|_{\\infty}=\\max\\{|0|,|{-1}|,|2|\\}=2$, hence $p_{3}=\\frac{1}{2}$.\n\nTherefore,\n$$\nP=\\begin{pmatrix}\n\\frac{1}{2} & 0 & 0 \\\\\n0 & \\frac{1}{2000} & 0 \\\\\n0 & 0 & \\frac{1}{2}\n\\end{pmatrix}.\n$$\nThis choice ensures each row of $A' = PA$ has infinity norm equal to $1$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{2} & 0 & 0 \\\\ 0 & \\frac{1}{2000} & 0 \\\\ 0 & 0 & \\frac{1}{2}\\end{pmatrix}}$$", "id": "2194457"}, {"introduction": "The ultimate goal of preconditioning is to transform a difficult linear system into an easy one. In an ideal scenario, the preconditioned matrix would have a condition number of $1$, allowing an iterative method to converge in a single step. This problem [@problem_id:2194479] challenges you to achieve this ideal outcome in a simplified setting, finding a diagonal preconditioner that makes the condition number of a $2 \\times 2$ system exactly $1$, thereby providing a concrete look at the theoretical target of all preconditioning strategies.", "problem": "In numerical linear algebra, iterative methods are often used to find approximate solutions to large systems of linear equations of the form $A\\mathbf{x} = \\mathbf{b}$. The efficiency of many such methods is highly dependent on the properties of the coefficient matrix $A$, particularly its condition number, $\\kappa(A)$. To improve convergence, a technique called preconditioning is used. The original system is transformed into an equivalent one, $M\\mathbf{x} = \\mathbf{c}$, where $M = P^{-1}A$ and $\\mathbf{c} = P^{-1}\\mathbf{b}$. The matrix $P$ is called a preconditioner, and it is chosen to make the condition number of the preconditioned matrix $M$, $\\kappa(M)$, as close to 1 as possible, while ensuring that $P^{-1}$ is easy to compute.\n\nConsider the matrix $A$ given by:\n$$A = \\begin{pmatrix} 1 & 2 \\\\ -4 & 2 \\end{pmatrix}$$\nWe are looking for a diagonal preconditioner $P$ of the form:\n$$P = \\begin{pmatrix} p_1 & 0 \\\\ 0 & p_2 \\end{pmatrix}$$\nwhere $p_1$ and $p_2$ are positive real numbers.\n\nFind the value of the ratio $p_2/p_1$ such that the condition number of the preconditioned matrix $P^{-1}A$ with respect to the matrix 2-norm, $\\kappa_2(P^{-1}A)$, is exactly 1.", "solution": "We want $\\kappa_{2}(P^{-1}A)=1$. For the matrix 2-norm, $\\kappa_{2}(M)=1$ if and only if there exists $\\alpha>0$ such that $M^{T}M=\\alpha I$. Let $M=P^{-1}A$ with $P=\\mathrm{diag}(p_{1},p_{2})$ and $p_{1},p_{2}>0$. Then\n$$\nM^{T}M=A^{T}P^{-T}P^{-1}A=A^{T}P^{-2}A,\n$$\nsince $P$ is diagonal with positive entries. Write $D=P^{-2}=\\mathrm{diag}(d_{1},d_{2})$ where $d_{1}=p_{1}^{-2}$ and $d_{2}=p_{2}^{-2}$. We seek $d_{1},d_{2}>0$ and $\\alpha>0$ such that\n$$\nA^{T}DA=\\alpha I.\n$$\nWith $A=\\begin{pmatrix}1&2\\\\ -4&2\\end{pmatrix}$, first compute\n$$\nDA=\\begin{pmatrix}d_{1}&2d_{1}\\\\ -4d_{2}&2d_{2}\\end{pmatrix},\n$$\nand then\n$$\nA^{T}DA=\\begin{pmatrix}1&-4\\\\ 2&2\\end{pmatrix}\\begin{pmatrix}d_{1}&2d_{1}\\\\ -4d_{2}&2d_{2}\\end{pmatrix}\n=\\begin{pmatrix}d_{1}+16d_{2}&2d_{1}-8d_{2}\\\\ 2d_{1}-8d_{2}&4d_{1}+4d_{2}\\end{pmatrix}.\n$$\nFor this to equal $\\alpha I$, the off-diagonal entries must vanish and the diagonal entries must be equal. The off-diagonal condition gives\n$$\n2d_{1}-8d_{2}=0 \\quad \\Longrightarrow \\quad d_{1}=4d_{2}.\n$$\nWith this relation, the diagonals match automatically:\n$$\nd_{1}+16d_{2}=4d_{2}+16d_{2}=20d_{2}, \\quad 4d_{1}+4d_{2}=16d_{2}+4d_{2}=20d_{2}.\n$$\nTherefore $D$ must satisfy $d_{1}=4d_{2}$. In terms of $p_{1},p_{2}$, this is\n$$\np_{1}^{-2}=4p_{2}^{-2} \\quad \\Longrightarrow \\quad \\frac{p_{1}^{-2}}{p_{2}^{-2}}=4 \\quad \\Longrightarrow \\quad \\frac{p_{2}^{2}}{p_{1}^{2}}=4 \\quad \\Longrightarrow \\quad \\frac{p_{2}}{p_{1}}=2,\n$$\nusing $p_{1},p_{2}>0$. Hence $\\kappa_{2}(P^{-1}A)=1$ precisely when $p_{2}/p_{1}=2$.", "answer": "$$\\boxed{2}$$", "id": "2194479"}, {"introduction": "The most effective preconditioning strategies are often those that incorporate knowledge of the underlying physical problem. In systems derived from partial differential equations, like the convection-diffusion equation, information propagates in a specific direction. This advanced, code-based exercise [@problem_id:3263468] explores how the performance of a Gauss-Seidel preconditioner is deeply connected to this physical flow, demonstrating that aligning the ordering of the solver with the direction of convection can dramatically accelerate convergence.", "problem": "Consider the one-dimensional steady convection-diffusion boundary value problem on the interval $[0,1]$ with homogeneous Dirichlet boundary conditions: find a function $u(x)$ such that\n$$\n-\\epsilon \\, \\frac{d^2 u}{dx^2} + \\beta \\, \\frac{d u}{dx} = f(x), \\quad x \\in (0,1), \\quad u(0) = 0, \\quad u(1) = 0,\n$$\nwhere $\\epsilon > 0$ is the diffusion coefficient and $\\beta$ is the convection velocity. Discretize this problem using a uniform grid with $n$ interior points, grid spacing $h = \\frac{1}{n+1}$, and construct the linear system $A \\, x = b$ with $x \\in \\mathbb{R}^n$. Use the standard second-order central difference for the diffusion term and the first-order upwind difference for the convection term aligned with the sign of $\\beta$:\n- If $\\beta \\ge 0$, approximate $\\frac{d u}{dx}$ by $\\frac{u_i - u_{i-1}}{h}$.\n- If $\\beta < 0$, approximate $\\frac{d u}{dx}$ by $\\frac{u_{i+1} - u_i}{h}$.\n\nWith $f(x)$ discretized as a constant forcing, set $b$ to the vector of all ones in $\\mathbb{R}^n$. The resulting matrix $A \\in \\mathbb{R}^{n \\times n}$ is tridiagonal with entries determined by $\\epsilon$, $\\beta$, and $h$.\n\nUse left-preconditioned Generalized Minimal Residual method (GMRES) to solve the linear system, where the left preconditioner $M^{-1}$ is chosen to be the Gauss-Seidel preconditioner. Define the Gauss-Seidel preconditioner based on a permutation (ordering) $\\pi$ of unknowns as follows:\n- Let $P_\\pi$ be the permutation matrix associated with ordering $\\pi$, so that $P_\\pi^\\top A P_\\pi$ is the matrix reordered according to $\\pi$.\n- Define $M_\\pi$ to be the lower triangular part (including the diagonal) of $P_\\pi^\\top A P_\\pi$.\n- The preconditioning operation applies $M_\\pi^{-1}$ to a vector in the permuted coordinates and then maps the result back via $P_\\pi$.\n\nImplement left-preconditioned GMRES to solve the system $M_\\pi^{-1} A x = M_\\pi^{-1} b$ using the Krylov subspace generated by $M_\\pi^{-1} A$ and initial residual $M_\\pi^{-1} b$, starting from the zero initial guess. At each iteration $k$, form the approximation $x_k$ from the Krylov basis and compute the relative residual\n$$\n\\rho_k = \\frac{\\|b - A x_k\\|_2}{\\|b\\|_2}.\n$$\nTerminate when $\\rho_k \\le 10^{-8}$ or when the iteration count reaches $200$, whichever occurs first. The measure of effectiveness of the preconditioner for a given ordering $\\pi$ is the number of iterations required to reach the tolerance $10^{-8}$; if convergence is not achieved within $200$ iterations, report the iteration count as $200$.\n\nDemonstrate how the effectiveness of the Gauss-Seidel preconditioner depends on the ordering of unknowns relative to the convection direction by running the following test suite. Fix $n = 80$. For each test case, use the specified $(\\epsilon,\\beta,\\pi)$:\n- Test case $1$: $(\\epsilon, \\beta, \\pi) = (0.01, 20, \\text{natural})$, where $\\pi$ is the natural left-to-right ordering.\n- Test case $2$: $(\\epsilon, \\beta, \\pi) = (0.01, 20, \\text{reversed})$, where $\\pi$ is the right-to-left ordering.\n- Test case $3$: $(\\epsilon, \\beta, \\pi) = (0.01, -20, \\text{natural})$.\n- Test case $4$: $(\\epsilon, \\beta, \\pi) = (0.01, -20, \\text{reversed})$.\n- Test case $5$: $(\\epsilon, \\beta, \\pi) = (1.0, 2, \\text{natural})$.\n- Test case $6$: $(\\epsilon, \\beta, \\pi) = (1.0, 2, \\text{reversed})$.\n\nYour program must:\n- Construct $A$ according to the above discretization rules for each $(\\epsilon,\\beta)$.\n- Build the Gauss-Seidel preconditioner based on the specified ordering $\\pi$.\n- Run left-preconditioned GMRES and record the number of iterations to reach $\\rho_k \\le 10^{-8}$ (or $200$ if not reached).\n- Produce a single line of output containing the six iteration counts as a comma-separated list enclosed in square brackets in the order of the test cases $1$ through $6$ (for example, $[i_1,i_2,i_3,i_4,i_5,i_6]$), where each $i_j$ is an integer.", "solution": "The user wants to solve a one-dimensional convection-diffusion problem and analyze the performance of a Gauss-Seidel preconditioner with different orderings.\n\n### 1. Problem Validation\n\nThe provided problem is a standard exercise in numerical methods for partial differential equations.\n-   **Givens**: All parameters ($\\epsilon$, $\\beta$), discretization rules (central for diffusion, upwind for convection), linear system setup ($A x=b$ with $b$ as a vector of ones), solver (left-preconditioned GMRES), preconditioner definition (ordering-dependent Gauss-Seidel), termination criteria ($\\rho_k \\le 10^{-8}$ or $200$ iterations), and test cases are explicitly and unambiguously defined.\n-   **Scientific Grounding**: The problem is rooted in fundamental principles of numerical analysis and scientific computing. The convection-diffusion equation is a canonical model in physics and engineering. The discretization schemes, GMRES algorithm, and Gauss-Seidel preconditioning are all well-established, standard techniques.\n-   **Well-Posedness and Objectivity**: The problem is mathematically well-posed. The discretization leads to a non-singular M-matrix $A$, guaranteeing a unique solution to the linear system. The GMRES algorithm is a formal procedure that will produce a deterministic result (iteration count) for the given inputs. The problem statement is objective and free of ambiguity.\n\nThe problem is valid. We proceed to the solution.\n\n### 2. Discretization and Matrix Assembly\n\nThe convection-diffusion equation is given by:\n$$\n-\\epsilon \\, \\frac{d^2 u}{dx^2} + \\beta \\, \\frac{d u}{dx} = f(x)\n$$\non the interval $[0,1]$ with homogeneous Dirichlet boundary conditions $u(0)=u(1)=0$. We use a uniform grid with $n$ interior points $x_i = i h$ for $i=1, \\dots, n$, where the grid spacing is $h = 1/(n+1)$. Let $u_i \\approx u(x_i)$.\n\nThe second-order central difference for the diffusion term at point $x_i$ is:\n$$\n-\\epsilon \\frac{d^2 u}{dx^2}\\bigg|_{x_i} \\approx -\\epsilon \\frac{u_{i+1} - 2u_i + u_{i-1}}{h^2} = \\frac{-\\epsilon}{h^2}u_{i-1} + \\frac{2\\epsilon}{h^2}u_i - \\frac{\\epsilon}{h^2}u_{i+1}\n$$\n\nThe first-order upwind difference for the convection term depends on the sign of the velocity $\\beta$.\n-   If $\\beta \\ge 0$ (flow from left to right), the scheme uses the \"upwind\" point $x_{i-1}$:\n    $$\n    \\beta \\frac{d u}{dx}\\bigg|_{x_i} \\approx \\beta \\frac{u_i - u_{i-1}}{h} = -\\frac{\\beta}{h}u_{i-1} + \\frac{\\beta}{h}u_i\n    $$\n-   If $\\beta < 0$ (flow from right to left), the scheme uses the \"upwind\" point $x_{i+1}$:\n    $$\n    \\beta \\frac{d u}{dx}\\bigg|_{x_i} \\approx \\beta \\frac{u_{i+1} - u_i}{h} = -\\frac{\\beta}{h}u_i + \\frac{\\beta}{h}u_{i+1}\n    $$\n\nCombining these discretizations yields the $i$-th row of the linear system $A x = b$.\n\n**Case 1: $\\beta \\ge 0$**\nThe equation for row $i$ is:\n$$\n\\left(-\\frac{\\epsilon}{h^2} - \\frac{\\beta}{h}\\right)u_{i-1} + \\left(\\frac{2\\epsilon}{h^2} + \\frac{\\beta}{h}\\right)u_i + \\left(-\\frac{\\epsilon}{h^2}\\right)u_{i+1} = b_i\n$$\nThe resulting matrix $A$ is tridiagonal with diagonals:\n-   Subdiagonal: $A_{i, i-1} = -\\frac{\\epsilon}{h^2} - \\frac{\\beta}{h}$\n-   Main diagonal: $A_{i, i} = \\frac{2\\epsilon}{h^2} + \\frac{\\beta}{h}$\n-   Superdiagonal: $A_{i, i+1} = -\\frac{\\epsilon}{h^2}$\n\n**Case 2: $\\beta < 0$**\nThe equation for row $i$ is:\n$$\n\\left(-\\frac{\\epsilon}{h^2}\\right)u_{i-1} + \\left(\\frac{2\\epsilon}{h^2} - \\frac{\\beta}{h}\\right)u_i + \\left(-\\frac{\\epsilon}{h^2} + \\frac{\\beta}{h}\\right)u_{i+1} = b_i\n$$\nThe matrix $A$ is tridiagonal with diagonals:\n-   Subdiagonal: $A_{i, i-1} = -\\frac{\\epsilon}{h^2}$\n-   Main diagonal: $A_{i, i} = \\frac{2\\epsilon}{h^2} - \\frac{\\beta}{h} = \\frac{2\\epsilon}{h^2} + \\frac{|\\beta|}{h}$\n-   Superdiagonal: $A_{i, i+1} = -\\frac{\\epsilon}{h^2} + \\frac{\\beta}{h} = -\\frac{\\epsilon}{h^2} - \\frac{|\\beta|}{h}$\n\nIn both cases, this structure applies to interior rows $i=2, \\dots, n-1$. For $i=1$ and $i=n$, the terms $u_0$ and $u_{n+1}$ are zero due to the boundary conditions, so the structure holds. The vector $b$ is set to a vector of all ones.\n\n### 3. Preconditioned GMRES Solver\n\nWe implement the Generalized Minimal Residual (GMRES) method to solve the linear system. The method is left-preconditioned, meaning we solve the equivalent system $M^{-1}Ax = M^{-1}b$.\n\n**GMRES Algorithm:**\nStarting with an initial guess $x_0=0$, the algorithm iteratively builds an orthonormal basis $\\{v_0, v_1, \\dots, v_k\\}$ for the Krylov subspace $\\mathcal{K}_{k+1}(M^{-1}A, M^{-1}r_0)$, where $r_0=b-Ax_0=b$. At each iteration $k$, it finds an approximate solution $x_k$ in the affine space $x_0 + \\text{span}\\{v_0, \\dots, v_{k-1}\\}$ that minimizes the preconditioned residual norm $\\|M^{-1}(b-Ax_k)\\|_2$. The specific steps are:\n1.  Initialize: $x_0=0$, compute $r_0=b$, $r_{tilde_0}=M^{-1}r_0$, $\\gamma = \\|r_{tilde_0}\\|_2$, $v_0 = r_{tilde_0}/\\gamma$.\n2.  **Arnoldi Iteration**: For $k=0, 1, \\dots$:\n    -   Compute $w = M^{-1}A v_k$.\n    -   Orthogonalize $w$ against $\\{v_0, \\dots, v_k\\}$ via Gram-Schmidt: for $j=0, \\dots, k$, set $h_{j,k} = w^T v_j$ and $w \\leftarrow w-h_{j,k}v_j$.\n    -   Set $h_{k+1,k} = \\|w\\|_2$ and $v_{k+1} = w/h_{k+1,k}$. This populates the $(k+1) \\times k$ Hessenberg matrix $H$.\n3.  **Solve Least-Squares Problem**: Find $y_k \\in \\mathbb{R}^{k+1}$ that minimizes $\\|\\gamma e_1 - H_{k+1, k} y\\|_2$, where $e_1=[1,0,\\dots,0]^T$.\n4.  **Update Solution**: Form the solution $x_k = x_0 + [v_0, \\dots, v_k] y_k$.\n5.  **Check Convergence**: Compute the true relative residual $\\rho_k = \\|b - A x_k\\|_2 / \\|b\\|_2$. If $\\rho_k \\le 10^{-8}$, terminate.\n6.  If the iteration count reaches $200$, terminate.\n\n**Gauss-Seidel Preconditioner:**\nThe preconditioner $M$ depends on a permutation (ordering) $\\pi$ of the unknowns, $\\{0, 1, \\dots, n-1\\}$.\n1.  Let $P_\\pi$ be the permutation matrix associated with $\\pi$. The interpretation used here is that for a vector $v$, $v[\\pi]$ is the permuted vector. The inverse permutation is $v[\\pi_{inv}]$, where $\\pi_{inv}$ is the inverse permutation array.\n2.  The matrix $A$ is reordered to $A_\\pi = P_\\pi^T A P_\\pi$. In `numpy`, this is implemented as `A[np.ix_(pi, pi)]`.\n3.  The preconditioner matrix in the permuted space is $M_\\pi = \\text{tril}(A_\\pi)$, the lower triangular part of $A_\\pi$, including its diagonal.\n4.  The preconditioning operation $y = M^{-1}z$ is defined as applying the inverse of the Gauss-Seidel sweep in the permuted coordinates. This is implemented as:\n    a. Permute the input vector: $z_\\pi = z[\\pi]$.\n    b. Solve the lower triangular system $M_\\pi y_\\pi = z_\\pi$ for $y_\\pi$ using forward substitution.\n    c. Permute the result back to the original ordering: $y[\\pi] = y_\\pi$.\n\nThe effectiveness of this preconditioner hinges on the choice of $\\pi$. For convection-dominated problems, information propagates along the direction of flow $\\beta$. A Gauss-Seidel sweep also propagates information according to its ordering. If the ordering $\\pi$ aligns with the physical direction of flow, the preconditioner acts as a good approximate solver, leading to rapid convergence. If the ordering is opposite to the flow, its effectiveness is greatly diminished.\n\n-   $\\beta > 0$: Flow is left-to-right (from index $i$ to $i+1$). The **natural ordering** `(0, 1, ...)` aligns with the flow.\n-   $\\beta < 0$: Flow is right-to-left (from index $i$ to $i-1$). The **reversed ordering** `(n-1, n-2, ...)` aligns with the flow.\nWe expect a low iteration count when the ordering and flow align, and a high count when they are opposed.\n\nThe implementation will follow this logic, constructing the appropriate matrix $A$ and preconditioner $M_\\pi$ for each test case, then running the preconditioned GMRES algorithm to find the number of iterations required for convergence.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the convection-diffusion problem for a suite of test cases\n    using left-preconditioned GMRES with an ordering-dependent\n    Gauss-Seidel preconditioner.\n    \"\"\"\n\n    def _solve_lower_triangular(L, b):\n        \"\"\"\n        Solves the lower triangular system L*x = b via forward substitution.\n        \"\"\"\n        n = len(b)\n        x = np.zeros(n, dtype=float)\n        for i in range(n):\n            s = np.dot(L[i, :i], x[:i])\n            if abs(L[i, i]) < 1e-16:\n                # Should not happen as the diagonal is positive definite\n                raise ValueError(\"Matrix is singular or near-singular.\")\n            x[i] = (b[i] - s) / L[i, i]\n        return x\n\n    def _run_preconditioned_gmres(A, b, precon_M_pi, precon_pi, tol, max_iter):\n        \"\"\"\n        Implementation of the left-preconditioned GMRES algorithm.\n        \"\"\"\n        n = A.shape[0]\n\n        def apply_preconditioner(z):\n            \"\"\"\n            Applies the Gauss-Seidel preconditioner M_inv.\n            y = M_inv * z\n            \"\"\"\n            # 1. Permute z to the new coordinate system\n            z_perm = z[precon_pi]\n            \n            # 2. Solve the lower triangular system in the permuted system\n            y_perm = _solve_lower_triangular(precon_M_pi, z_perm)\n            \n            # 3. Permute the solution back to the original coordinate system\n            y = np.zeros_like(y_perm)\n            y[precon_pi] = y_perm\n            return y\n\n        # Initial state for GMRES\n        x = np.zeros(n, dtype=float)\n        norm_b = np.linalg.norm(b)\n\n        if norm_b == 0:\n            return 0\n        \n        r0 = b - A @ x\n        initial_rel_res = np.linalg.norm(r0) / norm_b\n        if initial_rel_res <= tol:\n            return 0\n\n        # Starting vector for Arnoldi iteration\n        r_tilde = apply_preconditioner(r0)\n        beta_gmres = np.linalg.norm(r_tilde)\n        \n        if beta_gmres < 1e-16:\n            return 0\n\n        V = np.zeros((n, max_iter + 1), dtype=float)\n        H = np.zeros((max_iter + 1, max_iter), dtype=float)\n        V[:, 0] = r_tilde / beta_gmres\n\n        for k in range(max_iter):\n            # Arnoldi process (Modified Gram-Schmidt)\n            w = A @ V[:, k]\n            w_tilde = apply_preconditioner(w)\n            \n            for j in range(k + 1):\n                H[j, k] = np.dot(w_tilde, V[:, j])\n                w_tilde -= H[j, k] * V[:, j]\n            \n            H[k + 1, k] = np.linalg.norm(w_tilde)\n            \n            # Check for breakdown\n            if H[k + 1, k] < 1e-16:\n                k_eff = k + 1\n                break\n            \n            V[:, k + 1] = w_tilde / H[k + 1, k]\n            \n            # Solve least squares and check for convergence on the true residual\n            k_eff = k + 1\n            H_k_sub = H[:k_eff + 1, :k_eff]\n            g0 = np.zeros(k_eff + 1)\n            g0[0] = beta_gmres\n            \n            y, _, _, _ = np.linalg.lstsq(H_k_sub, g0, rcond=None)\n            \n            x = V[:, :k_eff] @ y\n            rel_res = np.linalg.norm(b - A @ x) / norm_b\n            \n            if rel_res <= tol:\n                return k_eff\n        else: # Loop completed without convergence\n          k_eff = max_iter\n\n        # Final check if loop was broken by breakdown or finished\n        H_k_sub = H[:k_eff, :k_eff]\n        g0 = np.zeros(k_eff)\n        g0[0] = beta_gmres\n        y, _, _, _ = np.linalg.lstsq(H_k_sub, g0, rcond=None)\n        x = V[:, :k_eff] @ y\n        rel_res = np.linalg.norm(b - A @ x) / norm_b\n        if rel_res <= tol:\n            return k_eff\n\n        return max_iter\n\n    test_cases = [\n        (0.01, 20.0, \"natural\"),\n        (0.01, 20.0, \"reversed\"),\n        (0.01, -20.0, \"natural\"),\n        (0.01, -20.0, \"reversed\"),\n        (1.0, 2.0, \"natural\"),\n        (1.0, 2.0, \"reversed\"),\n    ]\n\n    n = 80\n    tol = 1e-8\n    max_iter = 200\n    h = 1.0 / (n + 1)\n    b = np.ones(n, dtype=float)\n    \n    results = []\n\n    for epsilon, beta, ordering_str in test_cases:\n        A = np.zeros((n, n), dtype=float)\n        \n        # Assemble the matrix A based on upwind discretization\n        if beta >= 0:\n            diag_val = 2 * epsilon / h**2 + beta / h\n            sub_diag_val = -epsilon / h**2 - beta / h\n            super_diag_val = -epsilon / h**2\n        else:  # beta < 0\n            diag_val = 2 * epsilon / h**2 - beta / h\n            sub_diag_val = -epsilon / h**2\n            super_diag_val = -epsilon / h**2 + beta / h\n        \n        np.fill_diagonal(A, diag_val)\n        np.fill_diagonal(A[1:], sub_diag_val)\n        np.fill_diagonal(A[:, 1:], super_diag_val)\n        \n        # Define the permutation for the Gauss-Seidel preconditioner\n        if ordering_str == \"natural\":\n            pi = np.arange(n)\n        else:  # \"reversed\"\n            pi = np.arange(n - 1, -1, -1)\n            \n        # Construct the preconditioner matrix M_pi\n        A_perm = A[np.ix_(pi, pi)]\n        M_pi = np.tril(A_perm)\n\n        iterations = _run_preconditioned_gmres(A, b, M_pi, pi, tol, max_iter)\n        results.append(iterations)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3263468"}]}