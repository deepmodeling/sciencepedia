{"hands_on_practices": [{"introduction": "To begin our hands-on exploration of the Successive Over-relaxation (SOR) method, we will start with a concrete application. This first practice grounds the abstract iterative formula in a physical context—the steady-state heat distribution along a rod. By calculating the temperature at an interior point for the first iteration, you will gain direct experience with the mechanics of the SOR update rule, seeing how it combines the previous state, neighboring values, and the relaxation parameter $\\omega$ to produce a new estimate [@problem_id:1369790].", "problem": "A simplified steady-state heat distribution model for a thin, insulated rod is being analyzed. The rod is discretized into five equally spaced points, $P_0, P_1, P_2, P_3, P_4$. The temperatures at the two endpoints are held constant at $T_0 = 25.0^\\circ\\text{C}$ and $T_4 = 100.0^\\circ\\text{C}$. The temperatures at the three interior points, denoted by $T_1, T_2,$ and $T_3$, are unknown.\n\nIn the steady state, the temperature at each interior point is the arithmetic mean of the temperatures of its two immediate neighbors. This physical principle leads to a system of linear equations for the unknown temperatures $T_1, T_2,$ and $T_3$.\n\nTo solve this system, the Successive Over-Relaxation (SOR) iterative method is employed. The relaxation parameter is chosen as $\\omega = 1.15$. The iteration starts with an initial guess of zero for all unknown temperatures, i.e., $T_1^{(0)} = T_2^{(0)} = T_3^{(0)} = 0$.\n\nCalculate the value of the temperature at the first interior point, $T_1$, after the first full iteration, denoted as $T_1^{(1)}$. Express your answer in degrees Celsius, rounded to three significant figures.", "solution": "The steady-state condition on the 1D grid imposes, for each interior node, the mean relation\n$$\nT_{i}=\\frac{T_{i-1}+T_{i+1}}{2}.\n$$\nFor the three unknowns, this yields the linear system\n$$\n\\begin{aligned}\n2T_{1}-T_{2}=T_{0},\\\\\n-T_{1}+2T_{2}-T_{3}=0,\\\\\n-T_{2}+2T_{3}=T_{4}.\n\\end{aligned}\n$$\nUsing the SOR iteration for a system $A\\mathbf{T}=\\mathbf{b}$,\n$$\nT_{i}^{(k+1)}=(1-\\omega)T_{i}^{(k)}+\\frac{\\omega}{a_{ii}}\\left(b_{i}-\\sum_{ji}a_{ij}T_{j}^{(k+1)}-\\sum_{ji}a_{ij}T_{j}^{(k)}\\right),\n$$\nthe update for $T_{1}$ (with $a_{11}=2$, $a_{12}=-1$, $b_{1}=T_{0}$) is\n$$\nT_{1}^{(1)}=(1-\\omega)T_{1}^{(0)}+\\frac{\\omega}{2}\\left(T_{0}-(-1)T_{2}^{(0)}\\right).\n$$\nWith the initial guess $T_{1}^{(0)}=T_{2}^{(0)}=0$,\n$$\nT_{1}^{(1)}=\\frac{\\omega}{2}T_{0}.\n$$\nSubstituting $\\omega=1.15$ and $T_{0}=25.0$ gives\n$$\nT_{1}^{(1)}=\\frac{1.15}{2}\\times 25.0=14.375.\n$$\nRounded to three significant figures, this is $14.4$ in degrees Celsius.", "answer": "$$\\boxed{14.4}$$", "id": "1369790"}, {"introduction": "Having familiarized ourselves with the basic SOR update, we now investigate its primary advantage over the simpler Gauss-Seidel method. This exercise challenges you to construct a linear system where the Gauss-Seidel method ($\\omega=1$) fails to converge, yet the SOR method succeeds for a specific range of $\\omega$ values greater than 1. By analyzing the spectral radius of the iteration matrix, you will discover firsthand how over-relaxation can fundamentally alter the convergence properties of an iterative solver, turning a divergent process into a convergent one [@problem_id:3280357].", "problem": "Construct a purely mathematical and logically self-contained example of a linear system of equations of dimension $3 \\times 3$ in which the Gauss–Seidel method diverges while the Successive Over-Relaxation (SOR) method converges for a nonempty subset of relaxation parameters $\\omega \\in (1,2)$. Begin from the standard splitting of a matrix $A$ into $A = D - L - U$, where $D$ is the diagonal of $A$, $L$ is the strictly lower triangular part, and $U$ is the strictly upper triangular part. Use this splitting to derive, from first principles, the iteration matrices associated with Gauss–Seidel and Successive Over-Relaxation, based solely on the update rules implied by the splitting and the relaxation parameter.\n\nTo make the construction explicit and testable, consider the parametric matrix\n$$\nA(\\alpha,\\beta) = I - \\alpha L_0 - \\beta U_0,\n$$\nwhere $I$ is the $3 \\times 3$ identity, $L_0$ is the strictly lower triangular matrix with ones below the diagonal,\n$$\nL_0 = \\begin{bmatrix}\n0  0  0 \\\\\n1  0  0 \\\\\n1  1  0\n\\end{bmatrix},\n$$\nand $U_0$ is the strictly upper triangular matrix with ones above the diagonal,\n$$\nU_0 = \\begin{bmatrix}\n0  1  1 \\\\\n0  0  1 \\\\\n0  0  0\n\\end{bmatrix}.\n$$\nFix the parameters to $\\alpha = 1$ and $\\beta = \\frac{1}{4}$, and take the right-hand side vector to be $b = (1,1,1)^\\top$. Using the definitions of $D$, $L$, and $U$ for $A(\\alpha,\\beta)$, derive the iteration matrix for Gauss–Seidel solely from the rearranged form of the method’s update equation, and similarly derive the iteration matrix for Successive Over-Relaxation with relaxation parameter $\\omega \\in (1,2)$ from its update equation. You must not use pre-memorized formulas; instead, derive the iteration matrices from the fundamental splitting and update rules.\n\nUse the spectral radius criterion for convergence: A stationary linear iteration $x^{(k+1)} = T x^{(k)} + c$ converges for any initial vector if and only if the spectral radius $\\rho(T)$ satisfies $\\rho(T)  1$. This is a well-tested, foundational fact in numerical analysis. Based on this criterion, verify divergence of Gauss–Seidel for the constructed system, and determine for which relaxation parameters $\\omega \\in (1,2)$ the SOR iteration converges.\n\nYour program must carry out the following tasks:\n- Construct $A$ and $b$ with $\\alpha = 1$ and $\\beta = \\frac{1}{4}$.\n- Compute the Gauss–Seidel iteration matrix and its spectral radius to determine whether Gauss–Seidel converges; output a boolean for convergence of Gauss–Seidel (expected to be false if $\\rho \\ge 1$).\n- For the SOR method, compute its iteration matrix for each provided $\\omega$, compute its spectral radius, and output a boolean for convergence for each $\\omega$ tested.\n\nTest suite:\n- Evaluate and output convergence booleans for the following relaxation parameters for SOR: $\\omega = 1.1$, $\\omega = 1.3$, $\\omega = 1.5$, and $\\omega = 1.9$.\n- Additionally, scan the set $\\{\\omega_k \\mid \\omega_k = 1.05 + 0.05 k, \\; k = 0,1,\\dots,18\\}$, i.e., $\\omega \\in \\{1.05, 1.10, 1.15, \\dots, 1.95\\}$, and return the sublist of those $\\omega$ values for which SOR converges, formatted to two decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain:\n  1. A boolean indicating Gauss–Seidel convergence (using $\\omega = 1$ implicitly for Gauss–Seidel).\n  2. Four booleans indicating SOR convergence for $\\omega \\in \\{1.1, 1.3, 1.5, 1.9\\}$, in that order.\n  3. A list of floats containing the subset of scanned $\\omega$ values in $\\{1.05, 1.10, \\dots, 1.95\\}$ for which SOR converges, each float printed with two decimal places.\n- For example, the output must have the form\n$$\n[\\text{GS\\_flag}, \\text{SOR\\_1.1}, \\text{SOR\\_1.3}, \\text{SOR\\_1.5}, \\text{SOR\\_1.9}, [\\omega_1,\\omega_2,\\dots]].\n$$\nAll mathematical symbols and numbers in this statement have been expressed in LaTeX as required. No physical units, angle units, or percentage formatting are involved in this problem.", "solution": "The user wants to construct and analyze a $3 \\times 3$ linear system for which the Gauss-Seidel method diverges, but the Successive Over-Relaxation (SOR) method converges for some relaxation parameters $\\omega \\in (1,2)$. The analysis will be based on the spectral radius of the corresponding iteration matrices.\n\nThe solution proceeds in four steps:\n1.  Construction of the matrix $A$ for the linear system $A\\mathbf{x} = \\mathbf{b}$ based on the provided parametric form.\n2.  Derivation of the Gauss-Seidel iteration matrix $T_{GS}$ and calculation of its spectral radius $\\rho(T_{GS})$ to verify divergence.\n3.  Derivation of the SOR iteration matrix $T_{SOR}(\\omega)$ and establishing the computational procedure to find its spectral radius $\\rho(T_{SOR}(\\omega))$.\n4.  Implementation of a program to perform the required numerical computations and generate the output in the specified format.\n\n**1. System Construction**\n\nThe problem specifies the matrix $A$ as\n$$\nA(\\alpha,\\beta) = I - \\alpha L_0 - \\beta U_0\n$$\nwhere $I$ is the $3 \\times 3$ identity matrix, and\n$$\nL_0 = \\begin{bmatrix}\n0  0  0 \\\\\n1  0  0 \\\\\n1  1  0\n\\end{bmatrix}, \\quad\nU_0 = \\begin{bmatrix}\n0  1  1 \\\\\n0  0  1 \\\\\n0  0  0\n\\end{bmatrix}\n$$\nThe parameters are fixed at $\\alpha = 1$ and $\\beta = \\frac{1}{4}$. Substituting these values, we construct the matrix $A$:\n$$\nA = I - 1 \\cdot L_0 - \\frac{1}{4} \\cdot U_0 = \\begin{bmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{bmatrix} - \\begin{bmatrix} 0  0  0 \\\\ 1  0  0 \\\\ 1  1  0 \\end{bmatrix} - \\frac{1}{4}\\begin{bmatrix} 0  1  1 \\\\ 0  0  1 \\\\ 0  0  0 \\end{bmatrix}\n$$\n$$\nA = \\begin{bmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{bmatrix} - \\begin{bmatrix} 0  0  0 \\\\ 1  0  0 \\\\ 1  1  0 \\end{bmatrix} - \\begin{bmatrix} 0  1/4  1/4 \\\\ 0  0  1/4 \\\\ 0  0  0 \\end{bmatrix} = \\begin{bmatrix} 1  -1/4  -1/4 \\\\ -1  1  -1/4 \\\\ -1  -1  1 \\end{bmatrix}\n$$\nFor the iterative methods, we use the standard splitting $A = D - L - U$, where $D$ is the diagonal part of $A$, $-L$ is the strictly lower-triangular part of $A$, and $-U$ is the strictly upper-triangular part of $A$.\nFrom the constructed matrix $A$, we have:\n$$\nD = \\begin{bmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{bmatrix} = I\n$$\n$$\nL = -\\begin{bmatrix} 0  0  0 \\\\ -1  0  0 \\\\ -1  -1  0 \\end{bmatrix} = \\begin{bmatrix} 0  0  0 \\\\ 1  0  0 \\\\ 1  1  0 \\end{bmatrix} = L_0\n$$\n$$\nU = -\\begin{bmatrix} 0  -1/4  -1/4 \\\\ 0  0  -1/4 \\\\ 0  0  0 \\end{bmatrix} = \\begin{bmatrix} 0  1/4  1/4 \\\\ 0  0  1/4 \\\\ 0  0  0 \\end{bmatrix} = \\frac{1}{4} U_0 = \\beta U_0\n$$\nThe right-hand side vector is given as $\\mathbf{b} = (1,1,1)^\\top$, although it is not needed for the convergence analysis, which depends only on the iteration matrix.\n\n**2. Gauss-Seidel Method**\n\nThe Gauss-Seidel method is derived from the splitting $A\\mathbf{x}=\\mathbf{b}$ by rewriting it as $(D-L)\\mathbf{x} = U\\mathbf{x} + \\mathbf{b}$. This suggests the iterative scheme:\n$$\n(D-L)\\mathbf{x}^{(k+1)} = U\\mathbf{x}^{(k)} + \\mathbf{b}\n$$\nSolving for $\\mathbf{x}^{(k+1)}$, we obtain the update rule:\n$$\n\\mathbf{x}^{(k+1)} = (D-L)^{-1}U\\mathbf{x}^{(k)} + (D-L)^{-1}\\mathbf{b}\n$$\nThis is a stationary linear iteration of the form $\\mathbf{x}^{(k+1)} = T\\mathbf{x}^{(k)} + \\mathbf{c}$, where the Gauss-Seidel iteration matrix $T_{GS}$ is:\n$$\nT_{GS} = (D-L)^{-1}U\n$$\nUsing the matrices $D$, $L$, and $U$ derived above:\n$$\nD-L = \\begin{bmatrix} 1  0  0 \\\\ -1  1  0 \\\\ -1  -1  1 \\end{bmatrix}\n$$\nThe inverse, $(D-L)^{-1}$, is found to be:\n$$\n(D-L)^{-1} = \\begin{bmatrix} 1  0  0 \\\\ 1  1  0 \\\\ 2  1  1 \\end{bmatrix}\n$$\nNow we compute $T_{GS}$:\n$$\nT_{GS} = \\begin{bmatrix} 1  0  0 \\\\ 1  1  0 \\\\ 2  1  1 \\end{bmatrix} \\begin{bmatrix} 0  1/4  1/4 \\\\ 0  0  1/4 \\\\ 0  0  0 \\end{bmatrix} = \\begin{bmatrix} 0  1/4  1/4 \\\\ 0  1/4  1/2 \\\\ 0  1/2  3/4 \\end{bmatrix}\n$$\nTo determine convergence, we find the spectral radius $\\rho(T_{GS})$, which is the maximum of the absolute values of its eigenvalues. The eigenvalues $\\lambda$ are the roots of the characteristic equation $\\det(T_{GS} - \\lambda I) = 0$:\n$$\n\\det\\begin{bmatrix} -\\lambda  1/4  1/4 \\\\ 0  1/4-\\lambda  1/2 \\\\ 0  1/2  3/4-\\lambda \\end{bmatrix} = -\\lambda \\left[ \\left(\\frac{1}{4}-\\lambda\\right)\\left(\\frac{3}{4}-\\lambda\\right) - \\frac{1}{4} \\right] = 0\n$$\nThis gives one eigenvalue $\\lambda_1 = 0$. The other two are roots of the quadratic factor:\n$$\n\\lambda^2 - \\lambda + \\frac{3}{16} - \\frac{1}{4} = 0 \\implies \\lambda^2 - \\lambda - \\frac{1}{16} = 0\n$$\nUsing the quadratic formula, $\\lambda = \\frac{1 \\pm \\sqrt{1 - 4(-1/16)}}{2} = \\frac{1 \\pm \\sqrt{1+1/4}}{2} = \\frac{1 \\pm \\sqrt{5}/2}{2}$.\nThe eigenvalues are $\\lambda_1=0$, $\\lambda_{2,3} = \\frac{1}{2} \\pm \\frac{\\sqrt{5}}{4}$.\nThe spectral radius is the largest absolute value:\n$$\n\\rho(T_{GS}) = \\max\\left(|0|, \\left|\\frac{1}{2} + \\frac{\\sqrt{5}}{4}\\right|, \\left|\\frac{1}{2} - \\frac{\\sqrt{5}}{4}\\right|\\right) = \\frac{1}{2} + \\frac{\\sqrt{5}}{4} \\approx 1.059\n$$\nSince $\\rho(T_{GS})  1$, the Gauss-Seidel method diverges for this system.\n\n**3. Successive Over-Relaxation (SOR) Method**\n\nThe SOR update is a weighted average of the previous iterate $\\mathbf{x}^{(k)}$ and the Gauss-Seidel update. The component-wise update is:\n$$\nx_i^{(k+1)} = (1-\\omega) x_i^{(k)} + \\frac{\\omega}{a_{ii}} \\left( b_i - \\sum_{ji} a_{ij} x_j^{(k+1)} - \\sum_{ji} a_{ij} x_j^{(k)} \\right)\n$$\nRearranging to group terms with $\\mathbf{x}^{(k+1)}$ and $\\mathbf{x}^{(k)}$:\n$$\na_{ii}x_i^{(k+1)} + \\omega \\sum_{ji} a_{ij} x_j^{(k+1)} = (1-\\omega)a_{ii}x_i^{(k)} - \\omega \\sum_{ji} a_{ij} x_j^{(k)} + \\omega b_i\n$$\nIn matrix form, using the relations $D_{ii}=a_{ii}$, $(-L)_{ij} = a_{ij}$ for $ji$, and $(-U)_{ij} = a_{ij}$ for $ji$:\n$$\nD\\mathbf{x}^{(k+1)} - \\omega L \\mathbf{x}^{(k+1)} = (1-\\omega)D\\mathbf{x}^{(k)} + \\omega U \\mathbf{x}^{(k)} + \\omega \\mathbf{b}\n$$\n$$\n(D - \\omega L)\\mathbf{x}^{(k+1)} = ((1-\\omega)D + \\omega U)\\mathbf{x}^{(k)} + \\omega \\mathbf{b}\n$$\nSolving for $\\mathbf{x}^{(k+1)}$ yields the iteration:\n$$\n\\mathbf{x}^{(k+1)} = (D - \\omega L)^{-1}((1-\\omega)D + \\omega U)\\mathbf{x}^{(k)} + \\omega (D - \\omega L)^{-1}\\mathbf{b}\n$$\nThe SOR iteration matrix $T_{SOR}(\\omega)$ is therefore:\n$$\nT_{SOR}(\\omega) = (D - \\omega L)^{-1}((1-\\omega)D + \\omega U)\n$$\nFor our specific problem, $D=I$, $L=L_0$, and $U=\\beta U_0$. The matrix becomes:\n$$\nT_{SOR}(\\omega) = (I - \\omega L_0)^{-1}((1-\\omega)I + \\omega \\beta U_0)\n$$\nThe analytical calculation of the eigenvalues of this matrix is complex. Instead, we will compute this matrix numerically for each required value of $\\omega$ and then determine its spectral radius using a numerical eigenvalue solver. Convergence occurs if and only if $\\rho(T_{SOR}(\\omega))  1$.\n\n**4. Numerical Implementation**\n\nThe program will implement the formulas derived above. It will construct the matrices $D$, $L$, and $U$, compute $T_{GS}$ and $T_{SOR}(\\omega)$ for various $\\omega$, and then calculate their spectral radii using `numpy.linalg.eigvals` to check the convergence condition. The results will be collected and formatted into the required output string.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs a 3x3 linear system where Gauss-Seidel diverges but SOR\n    converges for a range of omega, and reports on convergence.\n    \"\"\"\n    \n    # 1. Setup the matrices as per the problem statement\n    alpha = 1.0\n    beta = 0.25\n    \n    I = np.identity(3, dtype=np.float64)\n    L0 = np.array([[0., 0., 0.], [1., 0., 0.], [1., 1., 0.]], dtype=np.float64)\n    U0 = np.array([[0., 1., 1.], [0., 0., 1.], [0., 0., 0.]], dtype=np.float64)\n    \n    # Construct the matrix A = I - alpha*L0 - beta*U0\n    A = I - alpha * L0 - beta * U0\n    \n    # Decompose A into D - L - U, where D is the diagonal part,\n    # L is the negative of the strictly lower triangular part,\n    # and U is the negative of the strictly upper triangular part.\n    D = np.diag(np.diag(A))\n    L = -np.tril(A, k=-1)\n    U = -np.triu(A, k=1)\n\n    # List to hold the final results for printing\n    final_results = []\n    \n    # 2. Analyze Gauss-Seidel convergence\n    # The GS iteration matrix is T_gs = (D-L)^-1 * U\n    gs_converges = False\n    try:\n        D_minus_L_inv = np.linalg.inv(D - L)\n        T_gs = D_minus_L_inv @ U\n        eigenvalues_gs = np.linalg.eigvals(T_gs)\n        rho_gs = np.max(np.abs(eigenvalues_gs))\n        gs_converges = rho_gs  1.0\n    except np.linalg.LinAlgError:\n        # If D-L is singular, the method is not well-defined.\n        gs_converges = False\n    \n    final_results.append(gs_converges)\n\n    def check_sor_convergence(omega, D, L, U):\n        \"\"\"\n        Calculates the spectral radius of the SOR iteration matrix for a given omega.\n        Returns True if the method converges, False otherwise.\n        \"\"\"\n        try:\n            # The SOR iteration matrix is T_sor = (D - omega*L)^-1 * ((1-omega)*D + omega*U)\n            D_minus_omega_L = D - omega * L\n            D_minus_omega_L_inv = np.linalg.inv(D_minus_omega_L)\n            \n            rhs_matrix = (1.0 - omega) * D + omega * U\n            \n            T_sor = D_minus_omega_L_inv @ rhs_matrix\n            \n            eigenvalues_sor = np.linalg.eigvals(T_sor)\n            rho_sor = np.max(np.abs(eigenvalues_sor))\n            \n            return rho_sor  1.0\n        except np.linalg.LinAlgError:\n            return False\n\n    # 3. Analyze SOR convergence for the specified test suite\n    sor_test_omegas = [1.1, 1.3, 1.5, 1.9]\n    for omega in sor_test_omegas:\n        final_results.append(check_sor_convergence(omega, D, L, U))\n\n    # 4. Scan for all convergent omega values in the specified range\n    # omega_k = 1.05 + 0.05 * k, for k = 0, 1, ..., 18\n    # Using np.linspace is more robust for generating floating point ranges\n    scan_omegas = np.linspace(1.05, 1.95, 19)\n    convergent_omegas = []\n    for omega in scan_omegas:\n        if check_sor_convergence(omega, D, L, U):\n            convergent_omegas.append(omega)\n    \n    # Python booleans stringify to \"True\" and \"False\".\n    # The problem example implies a specific string format. We will create it manually.\n    str_parts = [str(r).lower() for r in final_results]\n    \n    # Format the list of convergent omegas to two decimal places\n    omega_list_str = \"[\" + \",\".join([f\"{w:.2f}\" for w in convergent_omegas]) + \"]\"\n    str_parts.append(omega_list_str)\n\n    # 5. Final print statement in the exact required format.\n    print(f\"[{','.join(str_parts)}]\")\n\nsolve()\n```", "id": "3280357"}, {"introduction": "After seeing the power of over-relaxation, a natural question arises: is a larger $\\omega$ always better? This final practice serves as a crucial cautionary tale, demonstrating that the choice of the relaxation parameter is a delicate balancing act. Here, you will analyze a system where the Gauss-Seidel method converges, but applying SOR with a relaxation parameter $\\omega$ approaching 2 leads to divergence. This exercise highlights the sensitivity of convergence to $\\omega$ and reinforces the idea that there is often an optimal, not maximal, value for the relaxation parameter [@problem_id:3280240].", "problem": "Consider the linear system $A x = b$ with a real, nonsymmetric $2 \\times 2$ coefficient matrix $A$. The Successive Over-Relaxation (SOR) method is defined by a stationary iteration $x^{(k+1)} = G_{\\omega} x^{(k)} + c_{\\omega}$, where $G_{\\omega}$ is the SOR iteration matrix depending on the relaxation parameter $\\omega \\in \\mathbb{R}$ and the matrix $A$, and $c_{\\omega}$ is an affine term. The Gauss–Seidel method is the special case with $\\omega = 1$. Convergence of any stationary iteration is determined by the spectral radius $\\rho(G_{\\omega})$, which must satisfy $\\rho(G_{\\omega})  1$.\n\nStart from the standard matrix splitting $A = D - L - U$, where $D$ is the diagonal part of $A$, $L$ is the strictly lower part of $A$ with sign chosen so that $A = D - L - U$, and $U$ is the strictly upper part of $A$ similarly defined. From this base, derive the SOR iteration matrix $G_{\\omega}$ and the Gauss–Seidel iteration matrix $G_{1}$ in terms of $D$, $L$, $U$, and $\\omega$, without using any pre-given formulas. Then analyze the eigenstructure of $G_{\\omega}$ in the special $2 \\times 2$ case where\n$$\nA = \\begin{bmatrix}\n1  \\alpha \\\\\n\\beta  1\n\\end{bmatrix}\n$$\nwith real parameters $\\alpha$ and $\\beta$. Use this analysis to explain the sensitivity of $\\rho(G_{\\omega})$ with respect to $\\omega$ and the sign of the product $\\alpha \\beta$.\n\nChoose specific values $\\alpha = 0.5$ and $\\beta = -1.0$, so that $A = \\begin{bmatrix} 1  0.5 \\\\ -1.0  1 \\end{bmatrix}$. Using your derived expressions and reasoning from the fundamental base described above, justify theoretically why the Gauss–Seidel method ($\\omega = 1$) converges for this $A$ but the SOR method diverges for sufficiently large $\\omega$ close to $2$, and relate this to the sensitivity of $\\rho(G_{\\omega})$ with respect to $\\omega$.\n\nYour program must:\n- Construct $A = \\begin{bmatrix} 1  0.5 \\\\ -1.0  1 \\end{bmatrix}$.\n- Implement the construction of $G_{\\omega}$ from the base splitting $A = D - L - U$.\n- Compute the spectral radius $\\rho(G_{\\omega})$ for the following test suite of relaxation parameters:\n    1. $\\omega = 1.0$ (Gauss–Seidel baseline; \"happy path\" case).\n    2. $\\omega = 1.10$ (moderate over-relaxation).\n    3. $\\omega = 1.95$ (near the upper stability limit).\n    4. $\\omega = 1.99$ (very close to the upper stability limit).\n- For each test case, output a boolean indicating the appropriate property:\n    - For $\\omega \\in \\{1.0, 1.10\\}$, output whether the method converges, i.e., whether $\\rho(G_{\\omega})  1$.\n    - For $\\omega \\in \\{1.95, 1.99\\}$, output whether the method diverges, i.e., whether $\\rho(G_{\\omega})  1$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"), where each result is a boolean corresponding to the test cases in the order listed above. No physical units or angle units are involved in this problem; all quantities are dimensionless real numbers.", "solution": "The problem is valid as it is a well-posed, scientifically grounded, and objective question within the domain of numerical linear algebra. We will proceed with a solution.\n\nThe solution is presented in three parts: first, a general derivation of the Successive Over-Relaxation (SOR) iteration matrix $G_{\\omega}$; second, an analysis of the eigenstructure of $G_{\\omega}$ for the specified $2 \\times 2$ matrix form; and third, a specific analysis for the given numerical parameters to justify the observed convergence and divergence behaviors.\n\n**1. Derivation of the SOR Iteration Matrix $G_{\\omega}$**\n\nThe goal of an iterative method for solving the linear system $Ax=b$ is to generate a sequence of vectors $x^{(k)}$ that converges to the true solution $x$. The SOR method is a stationary iterative method of the form $x^{(k+1)} = G_{\\omega} x^{(k)} + c_{\\omega}$.\n\nWe begin with the component-wise update rule for SOR. For the $i$-th component of the solution vector at iteration $k+1$, the update is a weighted average of the previous value, $x_i^{(k)}$, and a new value computed using the Gauss-Seidel approach. The new value for $x_i$ is computed to satisfy the $i$-th equation, using the most recently computed values for components $x_j$ where $j  i$.\n\nThe $i$-th equation of the system $Ax=b$ is $\\sum_{j=1}^{n} a_{ij} x_j = b_i$. We can write this as $a_{ii}x_i = b_i - \\sum_{ji} a_{ij} x_j - \\sum_{ji} a_{ij} x_j$.\nThe Gauss-Seidel update for $x_i^{(k+1)}$ would use $x_j^{(k+1)}$ for $ji$ and $x_j^{(k)}$ for $ji$:\n$$x_{i, GS}^{(k+1)} = \\frac{1}{a_{ii}} \\left( b_i - \\sum_{ji} a_{ij} x_j^{(k+1)} - \\sum_{ji} a_{ij} x_j^{(k)} \\right)$$\nThe SOR method introduces a relaxation parameter $\\omega$ to extrapolate from the previous iterate $x_i^{(k)}$ in the direction of the Gauss-Seidel update:\n$$x_i^{(k+1)} = x_i^{(k)} + \\omega \\left( x_{i, GS}^{(k+1)} - x_i^{(k)} \\right) = (1-\\omega)x_i^{(k)} + \\omega x_{i, GS}^{(k+1)}$$\nSubstituting the expression for $x_{i, GS}^{(k+1)}$ gives:\n$$x_i^{(k+1)} = (1-\\omega)x_i^{(k)} + \\frac{\\omega}{a_{ii}} \\left( b_i - \\sum_{ji} a_{ij} x_j^{(k+1)} - \\sum_{ji} a_{ij} x_j^{(k)} \\right)$$\nTo derive the matrix form, we rearrange the equation to group terms involving $x^{(k+1)}$:\n$$a_{ii} x_i^{(k+1)} + \\omega \\sum_{ji} a_{ij} x_j^{(k+1)} = a_{ii}(1-\\omega)x_i^{(k)} - \\omega \\sum_{ji} a_{ij} x_j^{(k)} + \\omega b_i$$\nThis must hold for all $i=1, \\dots, n$. We now use the matrix splitting $A = D - L - U$, where $D$ is the diagonal of $A$, $-L$ is the strictly lower-triangular part of $A$, and $-U$ is the strictly upper-triangular part of $A$.\nThe terms in the component-wise equation correspond to matrix-vector products:\n- $a_{ii} x_i^{(k+1)}$ corresponds to the diagonal part: $D x^{(k+1)}$.\n- $\\sum_{ji} a_{ij} x_j^{(k+1)}$ corresponds to the strictly lower-triangular part: $-L x^{(k+1)}$.\n- $\\sum_{ji} a_{ij} x_j^{(k)}$ corresponds to the strictly upper-triangular part: $-U x^{(k)}$.\n\nSubstituting these into the rearranged equation gives the matrix form:\n$$D x^{(k+1)} - \\omega L x^{(k+1)} = (1-\\omega) D x^{(k)} + \\omega U x^{(k)} + \\omega b$$\nGrouping terms with $x^{(k+1)}$ on the left side:\n$$(D - \\omega L) x^{(k+1)} = ((1-\\omega)D + \\omega U) x^{(k)} + \\omega b$$\nSince $D$ is diagonal and $L$ is strictly lower triangular, the matrix $(D-\\omega L)$ is lower triangular with non-zero diagonal entries (assuming $a_{ii} \\ne 0$), and thus invertible. We can solve for $x^{(k+1)}$:\n$$x^{(k+1)} = (D - \\omega L)^{-1} ((1-\\omega)D + \\omega U) x^{(k)} + \\omega(D - \\omega L)^{-1} b$$\nBy comparing this to the standard form $x^{(k+1)} = G_{\\omega} x^{(k)} + c_{\\omega}$, we identify the SOR iteration matrix as:\n$$G_{\\omega} = (D - \\omega L)^{-1} ((1-\\omega)D + \\omega U)$$\nFor the special case $\\omega=1$, the method becomes the Gauss-Seidel method. The iteration matrix is:\n$$G_{1} = (D - 1 \\cdot L)^{-1} ((1-1)D + 1 \\cdot U) = (D - L)^{-1} U$$\n\n**2. Analysis for the $2 \\times 2$ Case**\n\nWe consider the specific matrix $A = \\begin{bmatrix} 1  \\alpha \\\\ \\beta  1 \\end{bmatrix}$. The matrix splitting $A = D-L-U$ yields:\n$$D = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix} = I, \\quad L = \\begin{bmatrix} 0  0 \\\\ -\\beta  0 \\end{bmatrix}, \\quad U = \\begin{bmatrix} 0  -\\alpha \\\\ 0  0 \\end{bmatrix}$$\nWe compute the components of $G_{\\omega}$:\n$$D - \\omega L = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix} - \\omega \\begin{bmatrix} 0  0 \\\\ -\\beta  0 \\end{bmatrix} = \\begin{bmatrix} 1  0 \\\\ \\omega\\beta  1 \\end{bmatrix}$$\n$$(D - \\omega L)^{-1} = \\begin{bmatrix} 1  0 \\\\ -\\omega\\beta  1 \\end{bmatrix}$$\n$$(1-\\omega)D + \\omega U = (1-\\omega)\\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix} + \\omega\\begin{bmatrix} 0  -\\alpha \\\\ 0  0 \\end{bmatrix} = \\begin{bmatrix} 1-\\omega  -\\omega\\alpha \\\\ 0  1-\\omega \\end{bmatrix}$$\nNow, we form $G_{\\omega}$:\n$$G_{\\omega} = (D - \\omega L)^{-1}((1-\\omega)D + \\omega U) = \\begin{bmatrix} 1  0 \\\\ -\\omega\\beta  1 \\end{bmatrix} \\begin{bmatrix} 1-\\omega  -\\omega\\alpha \\\\ 0  1-\\omega \\end{bmatrix} = \\begin{bmatrix} 1-\\omega  -\\omega\\alpha \\\\ -(1-\\omega)\\omega\\beta  \\omega^2\\alpha\\beta + 1-\\omega \\end{bmatrix}$$\nTo analyze convergence, we study the eigenvalues $\\lambda$ of $G_{\\omega}$, given by the characteristic equation $\\det(G_{\\omega} - \\lambda I) = 0$:\n$$\\det \\begin{bmatrix} 1-\\omega-\\lambda  -\\omega\\alpha \\\\ -(1-\\omega)\\omega\\beta  \\omega^2\\alpha\\beta + 1-\\omega - \\lambda \\end{bmatrix} = 0$$\n$$(1-\\omega-\\lambda)(\\omega^2\\alpha\\beta + 1-\\omega - \\lambda) - (-(1-\\omega)\\omega\\beta)(-\\omega\\alpha) = 0$$\n$$(\\lambda - (1-\\omega))^2 - \\omega^2\\alpha\\beta(\\lambda-(1-\\omega)) - (1-\\omega)\\omega^2\\alpha\\beta = 0$$\nThis simplifies to a well-known relationship for consistently ordered matrices. A more direct expansion yields:\n$$(1-\\omega-\\lambda)^2 + \\omega^2\\alpha\\beta(1-\\omega-\\lambda) - (1-\\omega)\\omega^2\\alpha\\beta = 0$$\n$$(\\lambda+\\omega-1)^2 - \\lambda\\omega^2\\alpha\\beta - (\\omega-1)\\omega^2\\alpha\\beta - (1-\\omega)\\omega^2\\alpha\\beta = 0$$\n$$(\\lambda+\\omega-1)^2 - \\lambda\\omega^2\\alpha\\beta = 0$$\nThis equation determines the eigenvalues $\\lambda$ of $G_{\\omega}$. It is a quadratic equation in $\\lambda$:\n$$\\lambda^2 + 2\\lambda(\\omega-1) + (\\omega-1)^2 - \\lambda\\omega^2\\alpha\\beta = 0$$\n$$\\lambda^2 + \\lambda(2(\\omega-1) - \\omega^2\\alpha\\beta) + (\\omega-1)^2 = 0$$\nThe nature of the eigenvalues depends on the sign of the term $\\alpha\\beta$. When $\\alpha\\beta  0$, the coefficient of $\\lambda$ is $2(\\omega-1) - \\omega^2\\alpha\\beta = 2(\\omega-1) + \\omega^2|\\alpha\\beta|$. The discriminant of this quadratic is $D = (2(\\omega-1) - \\omega^2\\alpha\\beta)^2 - 4(\\omega-1)^2 = \\omega^2\\alpha\\beta(\\omega^2\\alpha\\beta - 4(\\omega-1))$. When $\\alpha\\beta  0$ and $\\omega \\ge 1$, the term $4(\\omega-1)$ is non-negative, so the discriminant is positive and the eigenvalues $\\lambda$ of $G_{\\omega}$ are real.\n\n**3. Specific Analysis for $\\alpha = 0.5$ and $\\beta = -1.0$**\n\nFor $A = \\begin{bmatrix} 1  0.5 \\\\ -1.0  1 \\end{bmatrix}$, we have $\\alpha=0.5$ and $\\beta=-1.0$, so $\\alpha\\beta = -0.5$. The eigenvalues are real for $\\omega \\ge 1$. The characteristic equation becomes:\n$$\\lambda^2 + \\lambda(2(\\omega-1) + 0.5\\omega^2) + (\\omega-1)^2 = 0$$\nLet the two real eigenvalues be $\\lambda_1$ and $\\lambda_2$. From Vieta's formulas, their sum and product are:\n$$\\lambda_1 + \\lambda_2 = -(2(\\omega-1) + 0.5\\omega^2)$$\n$$\\lambda_1 \\lambda_2 = (\\omega-1)^2$$\nFor $\\omega  1$, the product is positive, so $\\lambda_1$ and $\\lambda_2$ have the same sign. The sum is clearly negative, so both eigenvalues are negative. The spectral radius is $\\rho(G_\\omega) = \\max(|\\lambda_1|, |\\lambda_2|)$.\n\n**Justification for Gauss-Seidel Convergence ($\\omega=1.0$)**\nFor $\\omega=1$, the characteristic equation is $\\lambda^2 + \\lambda(0 + 0.5(1)^2) + 0 = 0$, which is $\\lambda^2 + 0.5\\lambda = 0$.\nThe eigenvalues are $\\lambda(\\lambda+0.5)=0 \\implies \\lambda_1=0, \\lambda_2=-0.5$.\nThe spectral radius is $\\rho(G_1) = \\max(|0|, |-0.5|) = 0.5$.\nSince $\\rho(G_1) = 0.5  1$, the Gauss-Seidel method converges.\n\n**Justification for SOR Divergence for $\\omega$ near $2$**\nLet's analyze the behavior as $\\omega$ approaches $2$.\nThe sum of eigenvalues is $\\lambda_1 + \\lambda_2 = -2\\omega + 2 - 0.5\\omega^2$.\nThe product is $\\lambda_1 \\lambda_2 = (\\omega-1)^2$.\nAs $\\omega \\to 2$, the sum approaches $\\lambda_1 + \\lambda_2 \\to -2(2)+2-0.5(2^2) = -4+2-2 = -4$.\nThe product approaches $\\lambda_1 \\lambda_2 \\to (2-1)^2 = 1$.\nThe eigenvalues at $\\omega=2$ are the roots of $\\lambda^2 + 4\\lambda + 1 = 0$.\nThe roots are $\\lambda = \\frac{-4 \\pm \\sqrt{16-4}}{2} = -2 \\pm \\sqrt{3}$.\nSo, $\\lambda_1 = -2+\\sqrt{3} \\approx -0.268$ and $\\lambda_2 = -2-\\sqrt{3} \\approx -3.732$.\nThe spectral radius at $\\omega=2$ is $\\rho(G_2) = |-2-\\sqrt{3}| = 2+\\sqrt{3} \\approx 3.732$.\nSince $\\rho(G_2)  1$ and the eigenvalues are continuous functions of $\\omega$, for $\\omega$ sufficiently close to $2$, the spectral radius $\\rho(G_\\omega)$ must also be greater than $1$. This demonstrates that the SOR method diverges for such $\\omega$.\n\nThe sensitivity of $\\rho(G_\\omega)$ to $\\omega$ when $\\alpha\\beta0$ is explained by the behavior of the real eigenvalues. The product $\\lambda_1\\lambda_2 = (\\omega-1)^2$ grows moderately from $0$ to $1$ as $\\omega$ goes from $1$ to $2$. However, the sum $\\lambda_1+\\lambda_2 = -(2(\\omega-1) + 0.5\\omega^2)$ grows in magnitude quadratically. To satisfy both conditions, one negative eigenvalue must become very large in magnitude while the other stays small. This rapid growth of the magnitude of one eigenvalue causes the spectral radius to increase sharply as $\\omega$ moves away from $1$, leading to a small convergence interval (if any) for $\\omega1$ and eventual divergence. This contrasts with the typical case for symmetric positive-definite matrices where $\\alpha\\beta0$, and over-relaxation up to an optimal $\\omega_{opt} \\in (1, 2)$ is beneficial.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes the convergence of the SOR method for a specific 2x2 matrix\n    and a suite of relaxation parameters.\n    \"\"\"\n    \n    # Define problem parameters\n    alpha = 0.5\n    beta = -1.0\n    A = np.array([\n        [1.0, alpha],\n        [beta, 1.0]\n    ])\n    \n    # Define the test suite for the relaxation parameter omega\n    test_cases = [\n        # (omega, expected property: 'converge' or 'diverge')\n        (1.0, 'converge'),\n        (1.10, 'converge'),\n        (1.95, 'diverge'),\n        (1.99, 'diverge')\n    ]\n\n    results = []\n    \n    # Extract matrix components D, L, U based on the splitting A = D - L - U\n    D = np.diag(np.diag(A))\n    # Note: L is the strictly lower part of A with sign flipped\n    L = -np.tril(A, k=-1)\n    # Note: U is the strictly upper part of A with sign flipped\n    U = -np.triu(A, k=1)\n\n    for omega, property_to_test in test_cases:\n        # Construct the SOR iteration matrix G_omega\n        # G_omega = inv(D - omega*L) * ((1-omega)*D + omega*U)\n        \n        # Calculate the first term: (D - omega*L)^-1\n        D_minus_omega_L = D - omega * L\n        inv_D_minus_omega_L = np.linalg.inv(D_minus_omega_L)\n        \n        # Calculate the second term: (1-omega)*D + omega*U\n        second_term = (1 - omega) * D + omega * U\n        \n        # Compute G_omega\n        G_omega = inv_D_minus_omega_L @ second_term\n        \n        # Compute the eigenvalues of G_omega\n        eigenvalues = np.linalg.eigvals(G_omega)\n        \n        # Compute the spectral radius (max of absolute values of eigenvalues)\n        spectral_radius = np.max(np.abs(eigenvalues))\n        \n        # Check the specified property for the test case\n        if property_to_test == 'converge':\n            # Convergence requires spectral radius  1\n            result = spectral_radius  1.0\n        elif property_to_test == 'diverge':\n            # Divergence requires spectral radius  1\n            result = spectral_radius  1.0\n        else:\n            # Fallback for unexpected property strings\n            result = False\n            \n        results.append(str(result).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```", "id": "3280240"}]}