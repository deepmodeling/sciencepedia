{"hands_on_practices": [{"introduction": "At its core, Proper Orthogonal Decomposition (POD) is mathematically equivalent to the Singular Value Decomposition (SVD) of the snapshot data matrix. This first exercise provides a conceptual check of this fundamental link, stripping away the complexity of real-world data. By constructing a matrix directly from its constituent SVD components—the singular values and vectors—you will see firsthand how POD recovers the exact number of underlying energetic modes, reinforcing the connection between POD, SVD, and matrix rank. [@problem_id:3265930]", "problem": "Consider a snapshot matrix $X \\in \\mathbb{R}^{100 \\times 50}$ used in reduced-order modeling via Proper Orthogonal Decomposition (POD). Construct $X$ by selecting three orthonormal spatial vectors $u_1, u_2, u_3 \\in \\mathbb{R}^{100}$, three orthonormal temporal vectors $v_1, v_2, v_3 \\in \\mathbb{R}^{50}$, and three positive scalars $\\sigma_1, \\sigma_2, \\sigma_3$. Let the spatial vectors be the first three standard basis vectors, $u_1 = e_1$, $u_2 = e_2$, $u_3 = e_3$, where $e_i \\in \\mathbb{R}^{100}$ has a $1$ in the $i$-th position and $0$ elsewhere. Let the temporal vectors be the first three standard basis vectors, $v_1 = e_1$, $v_2 = e_2$, $v_3 = e_3$, where $e_i \\in \\mathbb{R}^{50}$ has a $1$ in the $i$-th position and $0$ elsewhere. Choose the singular values $\\sigma_1 = 6$, $\\sigma_2 = 3$, and $\\sigma_3 = 1$. Define the snapshot matrix as the rank-$3$ sum of outer products\n$$\nX \\;=\\; \\sigma_1 \\, u_1 v_1^{\\top} \\;+\\; \\sigma_2 \\, u_2 v_2^{\\top} \\;+\\; \\sigma_3 \\, u_3 v_3^{\\top}.\n$$\nUsing only the foundational definitions of singular value decomposition and matrix rank, determine the number of non-zero singular values recovered by Proper Orthogonal Decomposition (POD) applied to $X$. Express your final answer as a single integer.", "solution": "The problem requires determining the number of non-zero singular values of a given matrix $X$, which is constructed in a specific manner relevant to Proper Orthogonal Decomposition (POD).\n\nFirst, let us establish the foundational principles. For any real matrix $X \\in \\mathbb{R}^{m \\times n}$, the Singular Value Decomposition (SVD) provides a factorization of the form $X = U \\Sigma V^{\\top}$. Here, $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a rectangular diagonal matrix. The diagonal entries of $\\Sigma$, denoted by $\\sigma_i$, are the singular values of $X$. They are non-negative and conventionally ordered such that $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_p \\ge 0$, where $p = \\min(m, n)$.\n\nThe SVD can also be expressed as a sum of rank-$1$ outer products:\n$$\nX = \\sum_{i=1}^{r} \\sigma_i u_i v_i^{\\top}\n$$\nwhere $r$ is the rank of the matrix $X$. The vectors $u_i \\in \\mathbb{R}^m$ are the columns of $U$ (the left singular vectors) and form an orthonormal set, i.e., $u_i^{\\top} u_j = \\delta_{ij}$. The vectors $v_i \\in \\mathbb{R}^n$ are the columns of $V$ (the right singular vectors) and also form an orthonormal set, i.e., $v_i^{\\top} v_j = \\delta_{ij}$. The scalars $\\sigma_i > 0$ for $i=1, \\dots, r$ are the non-zero singular values of $X$. The number of non-zero singular values is, by definition, the rank of the matrix.\n\nThe problem states that Proper Orthogonal Decomposition (POD) is applied to the snapshot matrix $X$. Within the framework of numerical linear algebra, POD (specifically, using the method of snapshots) is mathematically equivalent to the SVD of the snapshot matrix. The process recovers the left singular vectors $u_i$ (the POD modes), the right singular vectors $v_i$ (the temporal modes), and the singular values $\\sigma_i$. The number of non-zero singular values recovered is therefore the rank of $X$.\n\nThe snapshot matrix $X \\in \\mathbb{R}^{100 \\times 50}$ is defined as:\n$$\nX = \\sigma_1 u_1 v_1^{\\top} + \\sigma_2 u_2 v_2^{\\top} + \\sigma_3 u_3 v_3^{\\top}\n$$\nWe are given the following definitions:\n1.  The scalars are $\\sigma_1 = 6$, $\\sigma_2 = 3$, and $\\sigma_3 = 1$. These are all positive.\n2.  The spatial vectors are $u_1 = e_1$, $u_2 = e_2$, $u_3 = e_3$, where $e_i \\in \\mathbb{R}^{100}$ are the standard basis vectors. The set $\\{u_1, u_2, u_3\\}$ is an orthonormal set, as $e_i^{\\top}e_j = \\delta_{ij}$ for $i, j \\in \\{1, 2, 3\\}$.\n3.  The temporal vectors are $v_1 = e_1$, $v_2 = e_2$, $v_3 = e_3$, where $e_i \\in \\mathbb{R}^{50}$ are the standard basis vectors. The set $\\{v_1, v_2, v_3\\}$ is also an orthonormal set.\n\nThe given expression for $X$ perfectly matches the structure of the SVD outer product expansion. We have a sum of three terms. Each term is the product of a positive scalar $\\sigma_i$, a unit vector $u_i$, and the transpose of a unit vector $v_i$. The sets of vectors $\\{u_1, u_2, u_3\\}$ and $\\{v_1, v_2, v_3\\}$ are both orthonormal.\n\nThis means that the provided expression is, in fact, the singular value decomposition of $X$. The non-zero singular values of $X$ are precisely the scalars given: $\\sigma_1 = 6$, $\\sigma_2 = 3$, and $\\sigma_3 = 1$. All other singular values, $\\sigma_i$ for $i > 3$, are zero.\n\nThe problem asks for the *number* of non-zero singular values recovered by applying POD to $X$. Based on our analysis, there are exactly three positive singular values. The number of non-zero singular values is therefore $3$. This is also the rank of the matrix $X$.\n\nThe question is a direct test of the foundational definition of singular value decomposition and its relation to matrix rank. The construction of the matrix $X$ is a deliberate formulation of a rank-$3$ matrix via its SVD expansion. No further computation is necessary. The number of terms in the sum, given that the scalars are non-zero and the vector sets are orthonormal, directly provides the answer.\nThe number of non-zero singular values is $3$.", "answer": "$$\n\\boxed{3}\n$$", "id": "3265930"}, {"introduction": "In practical applications, we are often more interested in the dynamic fluctuations of a system than its static, average state. This hands-on coding exercise explores the critical preprocessing step of mean-subtraction, which focuses the POD on capturing variance. You will discover how a dominant, non-zero mean can mask the underlying dynamic modes and see how centering the data is essential for revealing the system's most significant variations. [@problem_id:3265876]", "problem": "You are given a family of synthetic snapshot matrices representing temperature fields measured on a one-dimensional spatial grid over time. The goal is to analyze how subtracting the temporal mean affects the Proper Orthogonal Decomposition (POD) basis. Proper Orthogonal Decomposition (POD) is defined as the orthonormal basis obtained from the left singular vectors of the data matrix via the Singular Value Decomposition (SVD), which optimally captures the variance (energy) of the data in a least-squares sense. Specifically, if a data matrix is denoted by $X \\in \\mathbb{R}^{n \\times m}$ with columns as snapshots, the SVD is $X = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{n \\times n}$ has orthonormal columns (spatial modes), $\\Sigma \\in \\mathbb{R}^{n \\times m}$ is diagonal with nonnegative entries (singular values), and $V \\in \\mathbb{R}^{m \\times m}$ has orthonormal columns (temporal coefficients). The energy captured by the first mode is quantified by $\\sigma_{1}^{2} / \\sum_{i} \\sigma_{i}^{2}$, where $\\sigma_{i}$ are the singular values.\n\nYou will construct data matrices from first principles as follows. Consider a spatial grid with $n$ points $x_{j}$ uniformly spaced over $[0,1)$ and $m$ time snapshots indexed by $k = 0, 1, \\dots, m-1$. Define two spatial patterns: a mean pattern $\\phi_{\\mathrm{mean}}(x) \\equiv 1$ and an oscillatory pattern $\\phi_{\\mathrm{osc}}(x) \\equiv \\sin(2 \\pi x)$. Normalize each spatial pattern to unit Euclidean norm in $\\mathbb{R}^{n}$. Let the phase be $\\theta_{k} = 2 \\pi k / m$. Define each snapshot (a column vector in $\\mathbb{R}^{n}$) as\n$$\nT_{k} = A \\,\\phi_{\\mathrm{mean}} + B \\cos(\\theta_{k}) \\,\\phi_{\\mathrm{osc}},\n$$\nand assemble the snapshot matrix $X = [T_{0}\\; T_{1}\\; \\cdots\\; T_{m-1}] \\in \\mathbb{R}^{n \\times m}$.\n\nTwo versions of the POD basis are to be computed for each $X$:\n- The uncentered version uses $X$ directly.\n- The mean-centered version uses $X_{\\mathrm{c}} = X - \\mu \\mathbf{1}^{\\top}$, where $\\mu = \\frac{1}{m} \\sum_{k=0}^{m-1} T_{k}$ is the temporal mean field and $\\mathbf{1} \\in \\mathbb{R}^{m}$ is the vector of ones.\n\nFrom each SVD, extract:\n- The first left singular vector $u_{1}$ (unit norm) and the singular values $\\{\\sigma_{i}\\}$.\n- Define the absolute correlation between $u_{1}$ and the mean field $\\mu$ as\n$$\nr \\equiv \\begin{cases}\n\\left| \\dfrac{u_{1}^{\\top} \\mu}{\\| \\mu \\|_{2}} \\right|, & \\text{if } \\| \\mu \\|_{2} > 0,\\\n$$6pt]\n0, & \\text{if } \\| \\mu \\|_{2} = 0,\n\\end{cases}\n$$\nwhere $\\| \\cdot \\|_{2}$ denotes the Euclidean norm. If the data matrix (centered or uncentered) is numerically zero so that $\\sum_{i} \\sigma_{i}^{2} = 0$, define its first-mode energy fraction as $0$ and set the correlation $r$ to $0$ by convention.\n\nFor each test case below, compute four quantities:\n- $r_{\\mathrm{unc}}$: absolute correlation between the uncentered first mode and $\\mu$.\n- $r_{\\mathrm{cen}}$: absolute correlation between the centered first mode and $\\mu$.\n- $e_{\\mathrm{unc}}$: uncentered first-mode energy fraction, equal to $\\sigma_{1}^{2} / \\sum_{i} \\sigma_{i}^{2}$ for $X$.\n- $e_{\\mathrm{cen}}$: centered first-mode energy fraction, equal to $\\sigma_{1}^{2} / \\sum_{i} \\sigma_{i}^{2}$ for $X_{\\mathrm{c}}$.\n\nUse the following fixed parameters (no randomness is allowed):\n- Spatial grid size $n = 128$ with $x_{j}$ uniformly spaced in $[0,1)$.\n- Number of snapshots $m = 80$ with $\\theta_{k} = 2 \\pi k / m$ for $k = 0, 1, \\dots, m-1$.\n\nTest suite of parameter values $(A, B)$:\n- Case $1$ (non-zero-mean dominant): $(A, B) = (3, 1)$.\n- Case $2$ (zero mean): $(A, B) = (0, 1)$.\n- Case $3$ (constant field): $(A, B) = (2, 0)$.\n- Case $4$ (oscillation dominant with small mean): $(A, B) = (0.1, 2)$.\n\nYour program must:\n- Construct $X$ for each case exactly as specified.\n- Compute $(r_{\\mathrm{unc}}, r_{\\mathrm{cen}}, e_{\\mathrm{unc}}, e_{\\mathrm{cen}})$ for each case.\n- Round each reported value to $6$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of results, one per test case, where each test case result is the list $[r_{\\mathrm{unc}}, r_{\\mathrm{cen}}, e_{\\mathrm{unc}}, e_{\\mathrm{cen}}]$.\n- The entire output must be printed as a single line with no spaces, using square brackets and comma separators, for example, $[[a_{11},a_{12},a_{13},a_{14}],[a_{21},a_{22},a_{23},a_{24}],\\dots]$, where each $a_{ij}$ is a floating-point number rounded to $6$ decimal places.", "solution": "The user-provided problem has been analyzed and is determined to be valid. It is scientifically grounded in the principles of numerical linear algebra, specifically the Singular Value Decomposition (SVD) and its application to data analysis via Proper Orthogonal Decomposition (POD). The problem is well-posed, with all parameters, definitions, and procedures explicitly and unambiguously stated. There are no internal contradictions or missing information. I will now provide a complete solution.\n\nThe problem requires a comparison of the POD basis for a given synthetic dataset, $X$, and its mean-centered version, $X_c$. The analysis focuses on the first POD mode, its alignment with the temporal mean field, and the fraction of total variance it captures.\n\nFirst, I shall establish the theoretical framework based on the problem's definitions.\n\n**1. Spatial Basis construction**\nThe spatial domain is a one-dimensional grid with $n=128$ points, $x_j = j/n$ for $j=0, 1, \\dots, n-1$. Two spatial patterns are defined:\n- A constant pattern: $\\phi_{\\mathrm{mean}}(x) = 1$. The discrete vector is an $n$-vector of ones.\n- An oscillatory pattern: $\\phi_{\\mathrm{osc}}(x) = \\sin(2\\pi x)$. The discrete vector is $[\\sin(2\\pi x_0), \\dots, \\sin(2\\pi x_{n-1})]^{\\top}$.\n\nThese vectors are normalized to have a unit Euclidean norm in $\\mathbb{R}^n$. Let the unnormalized vectors be $v_{\\mathrm{mean}} = [1, \\dots, 1]^{\\top}$ and $v_{\\mathrm{osc}} = [\\sin(2\\pi x_0), \\dots, \\sin(2\\pi x_{n-1})]^{\\top}$. Their squared norms are $\\|v_{\\mathrm{mean}}\\|_2^2 = \\sum_{j=0}^{n-1} 1^2 = n$ and $\\|v_{\\mathrm{osc}}\\|_2^2 = \\sum_{j=0}^{n-1} \\sin^2(2\\pi j/n) = \\sum_{j=0}^{n-1} \\frac{1 - \\cos(4\\pi j/n)}{2} = \\frac{n}{2} - \\frac{1}{2}\\sum_{j=0}^{n-1}\\cos(4\\pi j/n)$. For $n>2$, the cosine sum is zero, so $\\|v_{\\mathrm{osc}}\\|_2^2 = n/2$.\nThe normalized vectors are $\\phi_{\\mathrm{mean}} = v_{\\mathrm{mean}}/\\sqrt{n}$ and $\\phi_{\\mathrm{osc}} = v_{\\mathrm{osc}}/\\sqrt{n/2}$.\nThese two vectors are orthogonal, as their dot product is $\\sum_{j=0}^{n-1} \\sin(2\\pi j/n)$, which is the imaginary part of $\\sum_{j=0}^{n-1} \\exp(i 2\\pi j/n) = 0$ for $n>1$.\n\n**2. Temporal Basis Construction**\nThe temporal behavior is governed by $m=80$ snapshots. For each snapshot $k \\in \\{0, \\dots, m-1\\}$, the phase is $\\theta_k = 2\\pi k/m$. The temporal vectors associated with the mean and oscillatory spatial patterns are:\n- A constant vector: $\\mathbf{1} = [1, \\dots, 1]^{\\top} \\in \\mathbb{R}^m$.\n- A cosine vector: $c = [\\cos(\\theta_0), \\dots, \\cos(\\theta_{m-1})]^{\\top} \\in \\mathbb{R}^m$.\n\nThese vectors are also orthogonal, as their dot product is $\\mathbf{1}^\\top c = \\sum_{k=0}^{m-1} \\cos(2\\pi k/m)$, which is the real part of $\\sum_{k=0}^{m-1} \\exp(i 2\\pi k/m) = 0$ for $m>1$. Their squared norms are $\\|\\mathbf{1}\\|_2^2=m$ and $\\|c\\|_2^2 = \\sum_{k=0}^{m-1} \\cos^2(2\\pi k/m) = m/2$ for $m>2$.\n\n**3. Snapshot Matrix and its SVD**\nThe $k$-th snapshot is $T_k = A \\phi_{\\mathrm{mean}} + B \\cos(\\theta_k) \\phi_{\\mathrm{osc}}$. The snapshot matrix $X = [T_0 \\dots T_{m-1}]$ can be written as an outer product sum:\n$$\nX = A \\phi_{\\mathrm{mean}}\\mathbf{1}^{\\top} + B \\phi_{\\mathrm{osc}}c^{\\top}\n$$\nLet $X_{\\mathrm{mean}} = A \\phi_{\\mathrm{mean}}\\mathbf{1}^{\\top}$ and $X_{\\mathrm{osc}} = B \\phi_{\\mathrm{osc}}c^{\\top}$. As the spatial vectors $(\\phi_{\\mathrm{mean}}, \\phi_{\\mathrm{osc}})$ and temporal vectors $(\\mathbf{1}, c)$ form orthogonal pairs, the SVD of $X$ is composed of the SVDs of $X_{\\mathrm{mean}}$ and $X_{\\mathrm{osc}}$.\nThe SVD of $X_{\\mathrm{mean}}$ is $u_1 = \\phi_{\\mathrm{mean}}$, $v_1 = \\mathbf{1}/\\sqrt{m}$, and $\\sigma_1 = A\\sqrt{m}$.\nThe SVD of $X_{\\mathrm{osc}}$ is $u_1' = \\phi_{\\mathrm{osc}}$, $v_1' = c/\\sqrt{m/2}$, and $\\sigma_1' = B\\sqrt{m/2}$.\nThus, the SVD of $X$ consists of two non-zero singular values, $\\sigma_{\\mathrm{mean}} = A\\sqrt{m}$ and $\\sigma_{\\mathrm{osc}} = B\\sqrt{m/2}$, with corresponding left singular vectors $\\phi_{\\mathrm{mean}}$ and $\\phi_{\\mathrm{osc}}$. The first POD mode $u_1$ will be the one associated with the larger singular value.\n\n**4. Mean Field and Centered Data**\nThe temporal mean field is $\\mu = \\frac{1}{m}\\sum_k T_k$.\n$$\n\\mu = \\frac{1}{m} \\sum_{k=0}^{m-1} (A \\phi_{\\mathrm{mean}} + B \\cos(\\theta_k) \\phi_{\\mathrm{osc}}) = A \\phi_{\\mathrm{mean}} + B \\phi_{\\mathrm{osc}} \\left(\\frac{1}{m} \\sum_{k=0}^{m-1} \\cos(\\theta_k)\\right)\n$$\nSince $\\sum \\cos(\\theta_k) = 0$, we have $\\mu = A \\phi_{\\mathrm{mean}}$.\nThe mean-centered data matrix is:\n$$\nX_c = X - \\mu \\mathbf{1}^{\\top} = (A \\phi_{\\mathrm{mean}}\\mathbf{1}^{\\top} + B \\phi_{\\mathrm{osc}}c^{\\top}) - (A \\phi_{\\mathrm{mean}})\\mathbf{1}^{\\top} = B \\phi_{\\mathrm{osc}}c^{\\top} = X_{\\mathrm{osc}}\n$$\nThus, $X_c$ is a rank-1 matrix, whose only POD mode is $\\phi_{\\mathrm{osc}}$.\n\nWith this theoretical foundation, I can predict the outcomes for each test case.\n- $r_{\\mathrm{unc}} = |u_1^{\\mathrm{unc}\\top}\\mu / \\|\\mu\\|_2|$\n- $e_{\\mathrm{unc}} = \\sigma_1^2 / (\\sigma_{\\mathrm{mean}}^2 + \\sigma_{\\mathrm{osc}}^2)$\n- $r_{\\mathrm{cen}} = |u_1^{\\mathrm{cen}\\top}\\mu / \\|\\mu\\|_2|$\n- $e_{\\mathrm{cen}} = \\sigma_{1,c}^2 / \\sum \\sigma_{i,c}^2$\n\n**Case 1: $(A, B) = (3, 1)$**\n$\\sigma_{\\mathrm{mean}} = 3\\sqrt{80} \\approx 26.83$, $\\sigma_{\\mathrm{osc}} = 1\\sqrt{40} \\approx 6.32$. $\\sigma_{\\mathrm{mean}} > \\sigma_{\\mathrm{osc}}$, so $u_1^{\\mathrm{unc}} = \\phi_{\\mathrm{mean}}$.\n$\\mu = 3\\phi_{\\mathrm{mean}} \\neq 0$.\n$r_{\\mathrm{unc}} = |\\phi_{\\mathrm{mean}}^{\\top}(3\\phi_{\\mathrm{mean}}) / \\|3\\phi_{\\mathrm{mean}}\\|_2| = 1$.\n$e_{\\mathrm{unc}} = \\frac{(3\\sqrt{80})^2}{(3\\sqrt{80})^2 + (1\\sqrt{40})^2} = \\frac{9 \\cdot 80}{9 \\cdot 80 + 1 \\cdot 40} = \\frac{720}{760} = \\frac{18}{19} \\approx 0.947368$.\n$X_c = X_{\\mathrm{osc}}$, rank-1. $u_1^{\\mathrm{cen}} = \\phi_{\\mathrm{osc}}$. $e_{\\mathrm{cen}} = 1$.\n$r_{\\mathrm{cen}} = |\\phi_{\\mathrm{osc}}^{\\top}(3\\phi_{\\mathrm{mean}}) / \\|3\\phi_{\\mathrm{mean}}\\|_2| = 0$ by orthogonality.\nExpected: $[1.0, 0.0, 0.947368, 1.0]$.\n\n**Case 2: $(A, B) = (0, 1)$**\n$\\mu = 0 \\cdot \\phi_{\\mathrm{mean}} = 0$. $X = X_{\\mathrm{osc}}$. Rank-1. $u_1^{\\mathrm{unc}}=\\phi_{\\mathrm{osc}}$.\n$r_{\\mathrm{unc}} = 0$ by definition since $\\mu=0$.\n$e_{\\mathrm{unc}} = 1$ since $X$ is rank-1.\n$X_c = X - 0 = X$. So centered results are identical.\n$r_{\\mathrm{cen}} = 0$, $e_{\\mathrm{cen}} = 1$.\nExpected: $[0.0, 0.0, 1.0, 1.0]$.\n\n**Case 3: $(A, B) = (2, 0)$**\n$X = 2\\phi_{\\mathrm{mean}}\\mathbf{1}^{\\top} = X_{\\mathrm{mean}}$. Rank-1. $u_1^{\\mathrm{unc}} = \\phi_{\\mathrm{mean}}$.\n$\\mu = 2\\phi_{\\mathrm{mean}} \\neq 0$.\n$r_{\\mathrm{unc}} = |\\phi_{\\mathrm{mean}}^{\\top}(2\\phi_{\\mathrm{mean}}) / \\|2\\phi_{\\mathrm{mean}}\\|_2| = 1$.\n$e_{\\mathrm{unc}} = 1$.\n$X_c = X - \\mu\\mathbf{1}^{\\top} = X_{\\mathrm{mean}} - X_{\\mathrm{mean}} = 0$.\nFor a zero matrix, $r_{\\mathrm{cen}}=0$ and $e_{\\mathrm{cen}}=0$ by definition.\nExpected: $[1.0, 0.0, 1.0, 0.0]$.\n\n**Case 4: $(A, B) = (0.1, 2)$**\n$\\sigma_{\\mathrm{mean}} = 0.1\\sqrt{80} \\approx 0.894$, $\\sigma_{\\mathrm{osc}} = 2\\sqrt{40} \\approx 12.65$. $\\sigma_{\\mathrm{osc}} > \\sigma_{\\mathrm{mean}}$, so $u_1^{\\mathrm{unc}} = \\phi_{\\mathrm{osc}}$.\n$\\mu = 0.1\\phi_{\\mathrm{mean}} \\neq 0$.\n$r_{\\mathrm{unc}} = |\\phi_{\\mathrm{osc}}^{\\top}(0.1\\phi_{\\mathrm{mean}}) / \\|0.1\\phi_{\\mathrm{mean}}\\|_2| = 0$.\n$e_{\\mathrm{unc}} = \\frac{(2\\sqrt{40})^2}{(0.1\\sqrt{80})^2 + (2\\sqrt{40})^2} = \\frac{4 \\cdot 40}{0.01 \\cdot 80 + 4 \\cdot 40} = \\frac{160}{0.8 + 160} = \\frac{160}{160.8} = \\frac{200}{201} \\approx 0.995025$.\n$X_c = X_{\\mathrm{osc}}$, rank-1. $u_1^{\\mathrm{cen}} = \\phi_{\\mathrm{osc}}$. $e_{\\mathrm{cen}} = 1$.\n$r_{\\mathrm{cen}} = |\\phi_{\\mathrm{osc}}^{\\top}(0.1\\phi_{\\mathrm{mean}}) / \\|0.1\\phi_{\\mathrm{mean}}\\|_2| = 0$.\nExpected: $[0.0, 0.0, 0.995025, 1.0]$.\n\nThe following program implements these calculations numerically.", "answer": "```python\nimport numpy as np\n\ndef compute_pod_metrics(A, B, n, m):\n    \"\"\"\n    Computes POD metrics for a given set of parameters.\n\n    Args:\n        A (float): Amplitude of the mean pattern.\n        B (float): Amplitude of the oscillatory pattern.\n        n (int): Number of spatial grid points.\n        m (int): Number of time snapshots.\n\n    Returns:\n        tuple: A tuple containing (r_unc, r_cen, e_unc, e_cen).\n    \"\"\"\n    # 1. Define grid and spatial patterns\n    x = np.linspace(0, 1, n, endpoint=False)\n    phi_mean_raw = np.ones(n)\n    phi_osc_raw = np.sin(2 * np.pi * x)\n\n    # 2. Normalize spatial patterns to unit Euclidean norm\n    norm_mean = np.linalg.norm(phi_mean_raw)\n    norm_osc = np.linalg.norm(phi_osc_raw)\n    \n    # Avoid division by zero if a pattern is somehow zero (e.g., n=1)\n    phi_mean = phi_mean_raw / norm_mean if norm_mean > 0 else phi_mean_raw\n    phi_osc = phi_osc_raw / norm_osc if norm_osc > 0 else phi_osc_raw\n\n    # 3. Construct the snapshot matrix X\n    k_vals = np.arange(m)\n    theta_k = 2 * np.pi * k_vals / m\n    cos_theta = np.cos(theta_k)\n\n    # Use broadcasting via np.outer for efficient construction\n    X = A * np.outer(phi_mean, np.ones(m)) + B * np.outer(phi_osc, cos_theta)\n    \n    # 4. Calculate the temporal mean field mu\n    mu = np.mean(X, axis=1)\n    mu_norm = np.linalg.norm(mu)\n\n    # 5. Compute metrics for the uncentered data X\n    U_unc, s_unc, _ = np.linalg.svd(X, full_matrices=False)\n    total_energy_sq_unc = np.sum(s_unc**2)\n\n    if total_energy_sq_unc > 1e-14:\n        e_unc = s_unc[0]**2 / total_energy_sq_unc\n        u1_unc = U_unc[:, 0]\n        if mu_norm > 1e-14:\n            r_unc = np.abs(np.dot(u1_unc, mu) / mu_norm)\n        else:\n            r_unc = 0.0\n    else: # This case corresponds to a numerically zero matrix\n        e_unc = 0.0\n        r_unc = 0.0\n\n    # 6. Compute metrics for the mean-centered data X_c\n    # X_c = X - mu * 1^T. Broadcasting handles this efficiently.\n    X_c = X - mu.reshape(-1, 1)\n    \n    U_cen, s_cen, _ = np.linalg.svd(X_c, full_matrices=False)\n    total_energy_sq_cen = np.sum(s_cen**2)\n\n    if total_energy_sq_cen > 1e-14:\n        e_cen = s_cen[0]**2 / total_energy_sq_cen\n        u1_cen = U_cen[:, 0]\n        if mu_norm > 1e-14:\n            r_cen = np.abs(np.dot(u1_cen, mu) / mu_norm)\n        else:\n            r_cen = 0.0\n    else: # This case corresponds to a numerically zero centered matrix\n        e_cen = 0.0\n        r_cen = 0.0\n        \n    return r_unc, r_cen, e_unc, e_cen\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n    # Fixed parameters from the problem statement\n    n = 128\n    m = 80\n\n    # Test suite of parameter values (A, B)\n    test_cases = [\n        (3.0, 1.0),   # Case 1: non-zero-mean dominant\n        (0.0, 1.0),   # Case 2: zero mean\n        (2.0, 0.0),   # Case 3: constant field\n        (0.1, 2.0)    # Case 4: oscillation dominant with small mean\n    ]\n\n    all_results = []\n    for A, B in test_cases:\n        # Compute the four required quantities for the current case\n        results_tuple = compute_pod_metrics(A, B, n, m)\n        \n        # Format the results as strings rounded to 6 decimal places\n        formatted_results = [f\"{val:.6f}\" for val in results_tuple]\n        \n        # Create the inner list string, e.g., \"[1.000000,0.000000,...]\"\n        all_results.append(f\"[{','.join(formatted_results)}]\")\n\n    # Join all case results into a single string in the required final format\n    final_output = f\"[{','.join(all_results)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "3265876"}, {"introduction": "A powerful tool is best used with an understanding of its limitations. This final practice investigates a classic scenario where POD performs poorly: the pure translation of a shape, like a moving wave packet. By attempting to reconstruct a simple translating Gaussian pulse, you will find that it requires a surprisingly large number of POD modes, revealing that the method's efficiency depends on the nature of the underlying dynamics and is better suited for stationary oscillations than for advection-dominated phenomena. [@problem_id:3265968]", "problem": "Consider the family of snapshot functions of a translating Gaussian pulse defined by $u(x,t) = \\exp\\!\\left(-\\big(x - c t\\big)^{2}\\right)$ on the spatial interval $x \\in [-L,L]$ and discrete times $t \\in \\{t_{0}, t_{1}, \\dots, t_{m-1}\\}$. From first principles, Proper Orthogonal Decomposition (POD) is the procedure that, for a given rank $r$, selects an $r$-dimensional orthonormal basis in space that minimizes the total squared projection error of the snapshot set. Equivalently, it produces the best rank-$r$ approximation of the snapshot data (in the Euclidean least-squares sense across all grid points and times).\n\nYour task is to implement a program that:\n- Constructs the snapshot matrix $X \\in \\mathbb{R}^{N_x \\times m}$ whose $k$-th column is the sampled snapshot $u(x,t_k)$ at $N_x$ uniformly spaced grid points in $[-L,L]$, for a given speed $c$, number of snapshots $m$, and final time $T$ with $t_k$ equally spaced in $[0,T]$.\n- Computes, for ranks $r \\in \\{1,2,5,10\\}$, the best rank-$r$ approximation $X_r$ (as defined by POD) and the corresponding relative reconstruction error\n$$E_r = \\frac{\\lVert X - X_r \\rVert_F}{\\lVert X \\rVert_F},$$\nwhere $\\lVert \\cdot \\rVert_F$ denotes the Frobenius norm.\n- Reports the errors $E_r$ for each test case as floating-point numbers rounded to six decimal places.\n\nFundamental base to use:\n- Definitions of Euclidean inner product and Frobenius norm.\n- The defining optimization property of Proper Orthogonal Decomposition (POD): among all $r$-dimensional orthonormal bases, POD minimizes the total squared projection error of the snapshots. This yields the best rank-$r$ approximation of the snapshot matrix in the least-squares sense.\n\nTest suite:\nUse $L = 10$ and $N_x = 401$ for all cases. The four test cases are:\n1. Case A (stationary pulse): $c = 0$, $T = 5$, $m = 50$.\n2. Case B (slow translation): $c = 0.5$, $T = 10$, $m = 100$.\n3. Case C (fast translation): $c = 2.0$, $T = 4$, $m = 80$.\n4. Case D (few snapshots): $c = 0.5$, $T = 10$, $m = 5$.\n\nAnswer specification:\n- For each test case, output a list $[E_{1},E_{2},E_{5},E_{10}]$ of four floats rounded to six decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the bracketed list of four errors for one test case, in the order of Cases A, B, C, D. For example, an output of the correct format would look like `[[e_{A,1},e_{A,2},e_{A,5},e_{A,10}],[e_{B,1},e_{B,2},e_{B,5},e_{B,10}],[e_{C,1},e_{C,2},e_{C,5},e_{C,10}],[e_{D,1},e_{D,2},e_{D,5},e_{D,10}]]`, with no spaces anywhere in the line.\n\nUnits:\n- There are no physical units required in this problem.\n\nAngle units:\n- Not applicable.\n\nPercentages:\n- Not applicable; express all quantities as decimals.\n\nYour implementation must be self-contained and require no user input, external files, or network access. It must run in a modern programming language and produce the exact final output format described above in a single line.", "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n- **Snapshot function**: $u(x,t) = \\exp(-(x - c t)^2)$\n- **Spatial domain**: $x \\in [-L,L]$\n- **Time domain**: $t \\in \\{t_{0}, t_{1}, \\dots, t_{m-1}\\}$, which are $m$ equally spaced points in $[0,T]$.\n- **Spatial discretization**: $N_x$ uniformly spaced grid points in $[-L,L]$.\n- **Snapshot matrix**: $X \\in \\mathbb{R}^{N_x \\times m}$, where the $k$-th column is the sampled snapshot $u(x,t_k)$.\n- **Task**: Compute the best rank-$r$ approximation $X_r$ using Proper Orthogonal Decomposition (POD) for ranks $r \\in \\{1, 2, 5, 10\\}$.\n- **Error metric**: Relative reconstruction error $E_r = \\frac{\\lVert X - X_r \\rVert_F}{\\lVert X \\rVert_F}$, where $\\lVert \\cdot \\rVert_F$ is the Frobenius norm.\n- **Constants**: $L = 10$, $N_x = 401$.\n- **Test cases**:\n    1. Case A: $c = 0$, $T = 5$, $m = 50$.\n    2. Case B: $c = 0.5$, $T = 10$, $m = 100$.\n    3. Case C: $c = 2.0$, $T = 4$, $m = 80$.\n    4. Case D: $c = 0.5$, $T = 10$, $m = 5$.\n- **Output specification**: A single-line comma-separated list of lists, e.g., `[[e_{A,1},...,e_{A,10}],[e_{B,1},...,e_{B,10}],...]`, with numbers rounded to six decimal places and no spaces.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is a standard application of Proper Orthogonal Decomposition (POD), a cornerstone of reduced-order modeling in scientific computing. The mathematical foundation is the Singular Value Decomposition (SVD), which is a principal result in linear algebra. The physics is a simple translating Gaussian pulse, which is a common and valid test case. The problem is scientifically sound.\n- **Well-Posed**: All necessary parameters ($L, N_x, c, T, m$) are provided for each case. The function $u(x,t)$, the procedure for constructing the snapshot matrix $X$, and the error metric $E_r$ are all defined unambiguously. The SVD provides a unique construction for the best rank-$r$ approximation, ensuring a unique solution exists.\n- **Objective**: The problem is expressed in precise mathematical terms, devoid of any subjectivity, ambiguity, or opinion-based statements.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a well-defined, self-contained, and scientifically sound problem in the field of numerical methods. A complete solution will be provided.\n\n### Principle-Based Solution\nThe objective is to compute the relative reconstruction error for a rank-$r$ approximation of a set of data snapshots. The foundational principle is that the optimal rank-$r$ approximation, in the least-squares sense defined by the Frobenius norm, is obtained via the Singular Value Decomposition (SVD). This result is formally stated by the Eckart-Young-Mirsky theorem.\n\n**1. Snapshot Matrix Construction**\nFirst, we discretize the problem domain. The spatial domain $x \\in [-L, L]$ is sampled at $N_x$ uniformly spaced points, forming the grid $\\{x_j\\}_{j=0}^{N_x-1}$. The time interval $t \\in [0, T]$ is sampled at $m$ discrete, equally spaced points $\\{t_k\\}_{k=0}^{m-1}$. The snapshot data at each time point $t_k$ is a vector in $\\mathbb{R}^{N_x}$ whose entries are given by the function $u(x_j, t_k)$. The collection of these snapshots forms the columns of the snapshot matrix $X \\in \\mathbb{R}^{N_x \\times m}$. An element $X_{jk}$ of this matrix is given by:\n$$X_{jk} = u(x_j, t_k) = \\exp\\!\\left(-\\big(x_j - c t_k\\big)^{2}\\right)$$\n\n**2. Singular Value Decomposition and Optimal Approximation**\nThe SVD of the snapshot matrix $X$ is given by:\n$$X = U \\Sigma V^T$$\nwhere $U \\in \\mathbb{R}^{N_x \\times N_x}$ is an orthogonal matrix whose columns $u_i$ are the left-singular vectors (POD modes), $V \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix whose columns $v_i$ are the right-singular vectors, and $\\Sigma \\in \\mathbb{R}^{N_x \\times m}$ is a rectangular diagonal matrix containing the singular values $\\sigma_i$. The singular values are non-negative and ordered by convention: $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_k \\ge 0$, where $k = \\min(N_x, m)$ is the rank of the matrix.\n\nThe Eckart-Young-Mirsky theorem states that the best rank-$r$ approximation of $X$ that minimizes the Frobenius norm of the difference, $\\lVert X - X_r \\rVert_F$, is the truncated SVD:\n$$X_r = \\sum_{i=1}^{r} \\sigma_i u_i v_i^T$$\nThis approximation is constructed using the first $r$ singular values and their corresponding left and right singular vectors.\n\n**3. Error Calculation**\nThe relative reconstruction error $E_r$ is defined as the ratio of the Frobenius norm of the error matrix $(X - X_r)$ to the Frobenius norm of the original matrix $X$. The Frobenius norm is related to the singular values by the identity $\\lVert A \\rVert_F^2 = \\sum_{i=1}^{\\text{rank}(A)} \\sigma_i(A)^2$.\nApplying this property, the squared norm of the original matrix is the sum of the squares of all its singular values:\n$$\\lVert X \\rVert_F^2 = \\sum_{i=1}^{k} \\sigma_i^2$$\nThe error matrix is $X - X_r = \\sum_{i=r+1}^{k} \\sigma_i u_i v_i^T$. Due to the orthogonality of the singular vectors, the squared Frobenius norm of the error matrix is the sum of the squares of the discarded singular values:\n$$\\lVert X - X_r \\rVert_F^2 = \\sum_{i=r+1}^{k} \\sigma_i^2$$\nCombining these results, the relative reconstruction error is given by:\n$$E_r = \\frac{\\lVert X - X_r \\rVert_F}{\\lVert X \\rVert_F} = \\frac{\\sqrt{\\sum_{i=r+1}^{k} \\sigma_i^2}}{\\sqrt{\\sum_{i=1}^{k} \\sigma_i^2}}$$\nNote that if the requested rank $r$ is greater than or equal to the actual rank of the matrix, $k$, the sum in the numerator is empty and evaluates to $0$, correctly yielding an error $E_r = 0$.\n\n**4. Computational Procedure**\nThe algorithm proceeds as follows for each test case:\n1.  Define the parameters $c$, $T$, and $m$, along with the fixed constants $L=10$ and $N_x=401$.\n2.  Construct the spatial grid $x$ and temporal grid $t$.\n3.  Assemble the $N_x \\times m$ snapshot matrix $X$ using the given function $u(x,t)$.\n4.  Compute the singular values $\\sigma_i$ of $X$ using a standard numerical library function for SVD. It is most efficient to compute only the singular values, not the full $U$ and $V$ matrices.\n5.  Calculate the total energy, represented by the squared Frobenius norm, $S_{total} = \\sum_{i=1}^{k} \\sigma_i^2$.\n6.  For each required rank $r \\in \\{1, 2, 5, 10\\}$, calculate the error energy, $S_{error} = \\sum_{i=r+1}^{k} \\sigma_i^2$.\n7.  The relative error is then $E_r = \\sqrt{S_{error} / S_{total}}$.\n8.  The calculated errors for each test case are collected and formatted according to the output specification.\nThis procedure provides a direct and numerically stable method for determining the required reconstruction errors based on fundamental principles of linear algebra.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Proper Orthogonal Decomposition problem for a translating Gaussian pulse.\n    \"\"\"\n    # Global parameters for all test cases\n    L = 10.0\n    Nx = 401\n    ranks_to_compute = [1, 2, 5, 10]\n\n    # Test suite: (c, T, m)\n    # c: speed, T: final time, m: number of snapshots\n    test_cases = [\n        (0.0, 5.0, 50),   # Case A: stationary pulse\n        (0.5, 10.0, 100), # Case B: slow translation\n        (2.0, 4.0, 80),   # Case C: fast translation\n        (0.5, 10.0, 5),   # Case D: few snapshots\n    ]\n\n    all_results = []\n\n    for c, T, m in test_cases:\n        # 1. Create spatial and temporal grids\n        x = np.linspace(-L, L, Nx)\n        t = np.linspace(0.0, T, m)\n\n        # 2. Construct the snapshot matrix X using broadcasting\n        # x_col has shape (Nx, 1) and t_row has shape (1, m)\n        # Broadcasting expands them to (Nx, m) for element-wise operations\n        x_col = x[:, np.newaxis]\n        t_row = t[np.newaxis, :]\n        X = np.exp(-((x_col - c * t_row) ** 2))\n\n        # 3. Compute the singular values of X\n        # We only need the singular values, so compute_uv=False is most efficient.\n        s = np.linalg.svd(X, compute_uv=False)\n        num_singular_values = s.shape[0]\n\n        # 4. Calculate the total energy (squared Frobenius norm of X)\n        # This is the sum of the squares of all singular values.\n        norm_X_sq = np.sum(s**2)\n\n        case_errors = []\n        for r in ranks_to_compute:\n            # 5. Calculate the reconstruction error for rank r\n            \n            # If norm_X_sq is zero, all errors are zero.\n            if norm_X_sq == 0.0:\n                 error = 0.0\n            # If rank r is >= number of singular values, the approximation is perfect.\n            elif r >= num_singular_values:\n                error = 0.0\n            else:\n                # The error norm is based on the truncated singular values (from r to end).\n                # s[r:] corresponds to sigma_{r+1}, sigma_{r+2}, ...\n                norm_err_sq = np.sum(s[r:]**2)\n                error = np.sqrt(norm_err_sq / norm_X_sq)\n            \n            case_errors.append(error)\n\n        all_results.append(case_errors)\n\n    # 6. Format the output string exactly as specified.\n    # e.g., [[err1,err2,...],[err1,err2,...]] with no spaces.\n    formatted_sublists = []\n    for res_list in all_results:\n        # Format each number to 6 decimal places.\n        formatted_numbers = [f\"{err:.6f}\" for err in res_list]\n        # Join numbers with commas and enclose in brackets.\n        formatted_sublists.append(f\"[{','.join(formatted_numbers)}]\")\n    \n    # Join the sublists with commas and enclose in outer brackets.\n    final_output = f\"[{','.join(formatted_sublists)}]\"\n\n    print(final_output)\n\nsolve()\n```", "id": "3265968"}]}