{"hands_on_practices": [{"introduction": "To truly understand the Canonical Polyadic (CP) decomposition, we must move from its abstract definition to a concrete implementation. This first practice focuses on the fundamentals by tasking you with constructing a tensor directly from its constituent rank-1 components. By building a tensor with a known CP rank and then analyzing the matrix ranks of its unfoldings, you will gain hands-on intuition for the core structure of CP models and the crucial relationship between a tensor and its various matricizations.", "problem": "Given positive integers $I$, $J$, $K$, and $R$, construct a third-order tensor $X \\in \\mathbb{R}^{I \\times J \\times K}$ that has a Canonical Polyadic (CP) rank of exactly $R$ by definition, starting from first principles. The Canonical Polyadic (CP) decomposition represents a tensor as a finite sum of rank-$1$ outer products. By definition, a rank-$1$ tensor in three modes is the outer product of three nonzero vectors. Specifically, if $a_r \\in \\mathbb{R}^{I}$, $b_r \\in \\mathbb{R}^{J}$, and $c_r \\in \\mathbb{R}^{K}$ for $r \\in \\{1,\\dots,R\\}$, then the tensor\n$$\nX \\;=\\; \\sum_{r=1}^{R} \\; a_r \\circ b_r \\circ c_r\n$$\nis a CP representation with at most $R$ terms, where the outer product $a_r \\circ b_r \\circ c_r$ has entries\n$$\n\\left(a_r \\circ b_r \\circ c_r\\right)_{i,j,k} \\;=\\; a_{r,i}\\, b_{r,j}\\, c_{r,k}, \\quad \\text{for } i \\in \\{1,\\dots,I\\}, \\; j \\in \\{1,\\dots,J\\}, \\; k \\in \\{1,\\dots,K\\}.\n$$\nYour task is to write a complete program that, given a small collection of parameter sets $(I,J,K,R)$ and seeds for reproducibility, does the following for each parameter set:\n\n1) Generate factor matrices $A \\in \\mathbb{R}^{I \\times R}$, $B \\in \\mathbb{R}^{J \\times R}$, and $C \\in \\mathbb{R}^{K \\times R}$ with independent and identically distributed entries drawn from a standard normal distribution, using the provided random seed to ensure reproducibility. Then form the tensor $X$ by summing the $R$ rank-$1$ outer products implied by the columns of $A$, $B$, and $C$, strictly following the definition above.\n\n2) Using only core definitions of matricization (also called unfolding) of a tensor along a mode, implement the mode-$1$, mode-$2$, and mode-$3$ unfoldings of $X$ into matrices $X_{(1)} \\in \\mathbb{R}^{I \\times (JK)}$, $X_{(2)} \\in \\mathbb{R}^{J \\times (IK)}$, and $X_{(3)} \\in \\mathbb{R}^{K \\times (IJ)}$, respectively. The unfolding should be constructed by arranging the entries of $X$ in lexicographic order consistent with the usual definition where the chosen mode index is kept as the row index and the remaining indices are combined into the column index. The exact map from $(i,j,k)$ to a linear column index must be self-consistent across your unfold and fold operations.\n\n3) From first principles, derive an algebraically equivalent matricized expression of $X$ in mode-$1$ using $A$, $B$, and $C$ that is implied by the outer-product definition, and use it to reconstruct a tensor $\\widehat{X}$ by folding the resulting matrix back into a three-way array consistent with your chosen unfolding definition. Compute the Frobenius norm of the residual\n$$\n\\| X - \\widehat{X} \\|_F \\;=\\; \\sqrt{ \\sum_{i=1}^{I} \\sum_{j=1}^{J} \\sum_{k=1}^{K} \\left( X_{i,j,k} - \\widehat{X}_{i,j,k} \\right)^2 }.\n$$\nReport this residual as a floating-point number. A correctly implemented derivation must yield a residual close to $0$, up to numerical rounding.\n\n4) Compute the numerical matrix rank (using Singular Value Decomposition (SVD), a well-tested numerical method) of each unfolding $X_{(1)}$, $X_{(2)}$, and $X_{(3)}$. Use the standard numerical rank criterion: if the singular values of a matrix $M$ are $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots$, then the numerical rank is the count of singular values larger than a tolerance, where a safe tolerance is\n$$\n\\tau \\;=\\; \\max\\{m,n\\} \\,\\varepsilon \\,\\sigma_1,\n$$\nfor an $m \\times n$ matrix $M$ and machine precision $\\varepsilon$ of double-precision arithmetic. Report each rank as an integer.\n\nTest suite. Run your program on the following parameter sets, in this exact order. For each case, set the random seed as specified. In the fourth case only, after sampling the random factors, replace the second column of each of $A$, $B$, and $C$ by the first column (thereby creating a dependent-column edge case where two rank-$1$ terms become identical):\n- Case $1$: $(I,J,K,R) = (4,5,6,3)$ with seed $101$.\n- Case $2$: $(I,J,K,R) = (3,3,3,1)$ with seed $202$.\n- Case $3$: $(I,J,K,R) = (5,4,4,6)$ with seed $303$.\n- Case $4$: $(I,J,K,R) = (3,4,2,2)$ with seed $404$ and the dependent-column modification described above.\n\nFor each case, produce a four-tuple of outputs:\n- The Frobenius residual $\\|X - \\widehat{X}\\|_F$ as a floating-point number.\n- The numerical rank of $X_{(1)}$ as an integer.\n- The numerical rank of $X_{(2)}$ as an integer.\n- The numerical rank of $X_{(3)}$ as an integer.\n\nFinal output format. Your program should produce a single line of output containing the results for the four cases as a comma-separated list of four-element lists, with no spaces, enclosed in square brackets, for example:\n$$\n\\text{print } [[r_{1},r_{2},r_{3},r_{4}],\\,[r_{1},r_{2},r_{3},r_{4}],\\,[r_{1},r_{2},r_{3},r_{4}],\\,[r_{1},r_{2},r_{3},r_{4}]].\n$$\nYour program must not read any input. There are no physical units in this problem. All angles, if any are used, must be in radians. All fractions must be reported in decimal form, not as percentages.", "solution": "The problem is assessed to be **valid**. It is a well-posed, scientifically sound, and objective problem from the field of numerical methods for tensor decompositions. All definitions and parameters are clearly stated, allowing for a unique and reproducible solution. The tasks require a fundamental understanding of the Canonical Polyadic (CP) decomposition, tensor unfolding (matricization), the Khatri-Rao product, and numerical rank computation, all of which are standard concepts in numerical and scientific computing.\n\nThe solution proceeds in four stages as outlined in the problem:\n1.  Construction of a third-order tensor from its CP factors.\n2.  Implementation of tensor unfolding for all three modes.\n3.  Reconstruction of the tensor from a matricized representation to verify the algebraic equivalence.\n4.  Computation of the numerical matrix ranks of the unfolded tensors.\n\n**1. Tensor Construction from CP Decomposition**\n\nA third-order tensor $X \\in \\mathbb{R}^{I \\times J \\times K}$ is constructed based on a given Canonical Polyadic (CP) rank, $R$. The tensor is defined as the sum of $R$ rank-$1$ tensors:\n$$\nX = \\sum_{r=1}^{R} a_r \\circ b_r \\circ c_r\n$$\nThe vectors $a_r \\in \\mathbb{R}^{I}$, $b_r \\in \\mathbb{R}^{J}$, and $c_r \\in \\mathbb{R}^{K}$ are the columns of three factor matrices, $A \\in \\mathbb{R}^{I \\times R}$, $B \\in \\mathbb{R}^{J \\times R}$, and $C \\in \\mathbb{R}^{K \\times R}$, respectively. The entries of these matrices are generated from a standard normal distribution for reproducibility using a specified seed. The element-wise definition of the tensor $X$ is:\n$$\nX_{i,j,k} = \\sum_{r=1}^{R} A_{i,r} B_{j,r} C_{k,r}\n$$\nfor $i \\in \\{0, \\dots, I-1\\}$, $j \\in \\{0, \\dots, J-1\\}$, and $k \\in \\{0, \\dots, K-1\\}$ (using $0$-based indexing for consistency with implementation). This summation can be implemented efficiently using Einstein summation convention, for instance via `numpy.einsum('ir,jr,kr->ijk', A, B, C)`.\n\n**2. Tensor Unfolding (Matricization)**\n\nUnfolding, or matricization, is the process of re-arranging the elements of a tensor into a matrix. The mode-$n$ unfolding of a tensor arranges the fibers along mode $n$ as the columns of the resulting matrix. However, the problem specifies keeping the chosen mode index as the row index. For a third-order tensor $X \\in \\mathbb{R}^{I \\times J \\times K}$:\n\n-   **Mode-1 Unfolding**: $X_{(1)} \\in \\mathbb{R}^{I \\times (JK)}$. The element $X_{i,j,k}$ is mapped to the entry $(X_{(1)})_{i,p}$, where $p$ is a linear index corresponding to the pair $(j,k)$. We adopt the standard lexicographical ordering where the last index varies fastest (C-style ordering), such that $p = j \\cdot K + k$. This is implemented by reshaping the tensor.\n-   **Mode-2 Unfolding**: $X_{(2)} \\in \\mathbb{R}^{J \\times (IK)}$. The element $X_{i,j,k}$ is mapped to $(X_{(2)})_{j,p}$, where $p = i \\cdot K + k$. This is implemented by first transposing the tensor dimensions from $(I,J,K)$ to $(J,I,K)$ and then reshaping.\n-   **Mode-3 Unfolding**: $X_{(3)} \\in \\mathbb{R}^{K \\times (IJ)}$. The element $X_{i,j,k}$ is mapped to $(X_{(3)})_{k,p}$, where $p = i \\cdot J + j$. This is implemented by transposing dimensions to $(K,I,J)$ and reshaping.\n\n**3. Matricized CP Representation and Reconstruction**\n\nA fundamental property of the CP decomposition is its concise representation in matricized form. The mode-$1$ unfolding of $X$ can be expressed as:\n$$\nX_{(1)} = A (B \\odot C)^T\n$$\nHere, $\\odot$ denotes the Khatri-Rao product, which is a column-wise Kronecker product. The matrix $B \\odot C \\in \\mathbb{R}^{(JK) \\times R}$ has as its $r$-th column the Kronecker product of the $r$-th columns of $B$ and $C$, i.e., $(B \\odot C)_{:,r} = b_r \\otimes c_r$. This formula is consistent with our chosen unfolding convention where the column index is $p=jK+k$.\n\nTo verify our implementation, we first compute the matricized form $\\widehat{X}_{(1)} = A (B \\odot C)^T$. Then, we \"fold\" this matrix back into a tensor $\\widehat{X}$ of size $I \\times J \\times K$. This folding operation is the inverse of the unfolding, accomplished by reshaping $\\widehat{X}_{(1)}$ into the dimensions $(I, J, K)$. Finally, we compute the Frobenius norm of the residual, $\\|X - \\widehat{X}\\|_F$. A correctly implemented process will result in a residual that is close to machine precision, confirming the algebraic equivalence.\n\n**4. Numerical Rank Calculation**\n\nThe numerical rank of a matrix is determined by counting its singular values that are greater than a specified tolerance. The singular values are obtained via Singular Value Decomposition (SVD). For an $m \\times n$ matrix $M$ with singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots$, the tolerance is given as $\\tau = \\max\\{m,n\\} \\cdot \\varepsilon \\cdot \\sigma_1$, where $\\varepsilon$ is the machine precision for double-precision floating-point numbers. The numerical rank is the number of singular values $\\sigma_i$ satisfying $\\sigma_i > \\tau$.\n\nFor a tensor $X$ constructed from a CP decomposition with $R$ linearly independent components (which is true with probability $1$ for randomly generated factors), the rank of its mode-$n$ unfolding is given by $\\text{rank}(X_{(n)}) = \\min(\\text{dim}_n, R)$, where $\\text{dim}_n$ is the dimension of mode $n$. Thus, we expect $\\text{rank}(X_{(1)}) = \\min(I, R)$, $\\text{rank}(X_{(2)}) = \\min(J, R)$, and $\\text{rank}(X_{(3)}) = \\min(K, R)$. In Case $4$, two components are deliberately made identical, reducing the effective rank of the tensor to $1$. Consequently, the numerical ranks of the unfoldings are all expected to be $1$.", "answer": "```python\nimport numpy as np\n\ndef construct_tensor(A, B, C):\n    \"\"\"\n    Constructs a 3rd-order tensor from its CP factor matrices.\n    X_ijk = sum_r A_ir * B_jr * C_kr\n    \"\"\"\n    return np.einsum('ir,jr,kr->ijk', A, B, C)\n\ndef unfold(tensor, mode):\n    \"\"\"\n    Unfolds a 3rd-order tensor into a matrix.\n    mode 0: I x (JK)\n    mode 1: J x (IK)\n    mode 2: K x (IJ)\n    \"\"\"\n    if mode == 0:\n        return tensor.reshape(tensor.shape[0], -1)\n    if mode == 1:\n        return np.transpose(tensor, (1, 0, 2)).reshape(tensor.shape[1], -1)\n    if mode == 2:\n        return np.transpose(tensor, (2, 0, 1)).reshape(tensor.shape[2], -1)\n    raise ValueError(\"Mode must be 0, 1, or 2.\")\n\ndef fold(matrix, mode, shape):\n    \"\"\"\n    Folds a matrix back into a 3rd-order tensor.\n    This is the inverse of the unfold operation.\n    \"\"\"\n    I, J, K = shape\n    if mode == 0:\n        return matrix.reshape((I, J, K))\n    if mode == 1:\n        return np.transpose(matrix.reshape((J, I, K)), (1, 0, 2))\n    if mode == 2:\n        return np.transpose(matrix.reshape((K, I, J)), (2, 0, 1))\n    raise ValueError(\"Mode must be 0, 1, or 2.\")\n\ndef khatri_rao(A, B):\n    \"\"\"\n    Computes the Khatri-Rao product of two matrices A and B.\n    This is a column-wise Kronecker product.\n    If A is (I, R) and B is (J, R), the result is (I*J, R).\n    The r-th column of the result is kron(A[:,r], B[:,r]).\n    \"\"\"\n    if A.shape[1] != B.shape[1]:\n        raise ValueError(\"Matrices must have the same number of columns.\")\n    \n    # Efficient implementation using Einstein summation and reshaping.\n    # einsum produces a tensor T of shape (I, J, R) where T[i,j,r] = A[i,r] * B[j,r].\n    # Reshaping to (I*J, R) flattens the first two dimensions, with the second\n    # index (j) varying fastest, which corresponds to the Kronecker product.\n    return np.einsum('ir,jr->ijr', A, B).reshape(A.shape[0] * B.shape[0], A.shape[1])\n\ndef numerical_rank(matrix):\n    \"\"\"\n    Computes the numerical rank of a matrix using SVD.\n    \"\"\"\n    if matrix.size == 0 or np.all(matrix==0):\n        return 0\n        \n    m, n = matrix.shape\n    s = np.linalg.svd(matrix, compute_uv=False)\n    \n    if s[0] == 0:\n        return 0\n\n    eps = np.finfo(float).eps\n    tolerance = max(m, n) * eps * s[0]\n    \n    return np.sum(s > tolerance)\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and produce the final output.\n    \"\"\"\n    test_cases = [\n        {'I': 4, 'J': 5, 'K': 6, 'R': 3, 'seed': 101, 'mod': False},\n        {'I': 3, 'J': 3, 'K': 3, 'R': 1, 'seed': 202, 'mod': False},\n        {'I': 5, 'J': 4, 'K': 4, 'R': 6, 'seed': 303, 'mod': False},\n        {'I': 3, 'J': 4, 'K': 2, 'R': 2, 'seed': 404, 'mod': True},\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        I, J, K, R, seed, mod = case['I'], case['J'], case['K'], case['R'], case['seed'], case['mod']\n        \n        # 1. Generate factor matrices and construct the tensor X\n        np.random.seed(seed)\n        A = np.random.randn(I, R)\n        B = np.random.randn(J, R)\n        C = np.random.randn(K, R)\n        \n        if mod:\n            # Special modification for Case 4\n            A[:, 1] = A[:, 0]\n            B[:, 1] = B[:, 0]\n            C[:, 1] = C[:, 0]\n            \n        X = construct_tensor(A, B, C)\n\n        # 2. Unfold X along all three modes\n        X1 = unfold(X, 0)\n        X2 = unfold(X, 1)\n        X3 = unfold(X, 2)\n\n        # 3. Reconstruct X from matricized form and compute residual\n        # The unfolding convention used (last index fastest) corresponds to X_1 = A (B \\odot C)^T\n        KR_BC = khatri_rao(B, C)\n        X1_hat_mat = A @ KR_BC.T\n        X_hat = fold(X1_hat_mat, 0, (I, J, K))\n        residual = np.linalg.norm(X - X_hat)\n\n        # 4. Compute numerical ranks of unfoldings\n        rank1 = numerical_rank(X1)\n        rank2 = numerical_rank(X2)\n        rank3 = numerical_rank(X3)\n        \n        all_results.append((residual, rank1, rank2, rank3))\n\n    # Format the final output string as specified\n    formatted_results = []\n    for res_tuple in all_results:\n        # Use repr() for the float to get a standard, high-precision representation\n        s = f\"[{repr(res_tuple[0])},{res_tuple[1]},{res_tuple[2]},{res_tuple[3]}]\"\n        formatted_results.append(s)\n        \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3282221"}, {"introduction": "While our intuition from linear algebra is a helpful starting point, tensors possess unique and sometimes surprising properties. This exercise delves into one of the most famous examples: the dependence of tensor rank on the underlying number field. You will demonstrate that a specific real-valued tensor has a rank of $3$ over the real numbers ($\\mathbb{R}$) but a rank of $2$ over the complex numbers ($\\mathbb{C}$), a phenomenon with no equivalent in matrix theory that highlights the richness and complexity of multilinear algebra.", "problem": "Let a third-order tensor $\\mathcal{T} \\in \\mathbb{F}^{2 \\times 2 \\times 2}$ be represented by its frontal slices along the third mode, denoted $\\mathbf{A}$ and $\\mathbf{B}$, so that for a Canonical Polyadic (CP) representation $\\mathcal{T} = \\sum_{p=1}^{r} \\mathbf{u}_{p} \\otimes \\mathbf{v}_{p} \\otimes \\mathbf{w}_{p}$, the slices satisfy $\\mathbf{A} = \\sum_{p=1}^{r} w_{p,1}\\,\\mathbf{u}_{p}\\mathbf{v}_{p}^{\\top}$ and $\\mathbf{B} = \\sum_{p=1}^{r} w_{p,2}\\,\\mathbf{u}_{p}\\mathbf{v}_{p}^{\\top}$, where $\\mathbf{u}_{p}, \\mathbf{v}_{p} \\in \\mathbb{F}^{2}$ and $\\mathbf{w}_{p} = \\begin{pmatrix} w_{p,1} \\\\ w_{p,2} \\end{pmatrix} \\in \\mathbb{F}^{2}$, and $\\mathbb{F}$ denotes a field. The Canonical Polyadic (CP) rank of $\\mathcal{T}$ over a field $\\mathbb{F}$, denoted $\\operatorname{rank}_{\\mathbb{F}}(\\mathcal{T})$, is the smallest integer $r$ for which such a representation exists with entries in $\\mathbb{F}$.\n\nConsider the specific tensor $\\mathcal{T} \\in \\mathbb{R}^{2 \\times 2 \\times 2}$ whose frontal slices are\n$$\n\\mathbf{A} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}, \n\\qquad\n\\mathbf{B} = \\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix}.\n$$\nStarting only from the definition of CP rank, properties of rank-$1$ matrices, and basic facts about $2 \\times 2$ matrix pencils, do the following:\n\n- Determine the minimal number $r_{\\mathbb{R}}$ of rank-$1$ outer products needed to represent $\\mathcal{T}$ over $\\mathbb{R}$, and the minimal number $r_{\\mathbb{C}}$ needed over $\\mathbb{C}$.\n- Construct explicitly a CP decomposition of $\\mathcal{T}$ over $\\mathbb{C}$ achieving $r_{\\mathbb{C}}$, and a real CP decomposition that demonstrates $r_{\\mathbb{R}} \\leq 3$.\n- Justify, by analyzing the pencil $\\mathbf{A} + x \\mathbf{B}$ and the occurrence of rank drops as $x$ varies in the base field, that $r_{\\mathbb{R}} \\neq 2$.\n\nFinally, report the single numerical value\n$$\n\\Delta \\;=\\; r_{\\mathbb{R}} - r_{\\mathbb{C}}.\n$$\nNo rounding is required. Provide $\\Delta$ as a single number.", "solution": "The problem requires the determination of the Canonical Polyadic (CP) rank of a specific third-order tensor $\\mathcal{T} \\in \\mathbb{R}^{2 \\times 2 \\times 2}$ over the fields of real numbers ($\\mathbb{R}$) and complex numbers ($\\mathbb{C}$). The tensor $\\mathcal{T}$ is defined by its frontal slices $\\mathbf{A} = \\mathcal{T}(:,:,1)$ and $\\mathbf{B} = \\mathcal{T}(:,:,2)$, given by:\n$$\n\\mathbf{A} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\qquad \\mathbf{B} = \\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix}\n$$\nThe CP rank over a field $\\mathbb{F}$, denoted $\\operatorname{rank}_{\\mathbb{F}}(\\mathcal{T})$, is the smallest integer $r$ such that $\\mathcal{T}$ can be written as a sum of $r$ rank-$1$ tensors: $\\mathcal{T} = \\sum_{p=1}^{r} \\mathbf{u}_{p} \\otimes \\mathbf{v}_{p} \\otimes \\mathbf{w}_{p}$, where $\\mathbf{u}_{p}, \\mathbf{v}_{p}, \\mathbf{w}_{p}$ have entries in $\\mathbb{F}$. The slices are then linear combinations of the rank-$1$ matrices $\\mathbf{M}_p = \\mathbf{u}_{p}\\mathbf{v}_{p}^{\\top}$:\n$$\n\\mathbf{A} = \\sum_{p=1}^{r} w_{p,1}\\,\\mathbf{M}_p, \\qquad \\mathbf{B} = \\sum_{p=1}^{r} w_{p,2}\\,\\mathbf{M}_p\n$$\nwhere $\\mathbf{w}_p = \\begin{pmatrix} w_{p,1} \\\\ w_{p,2} \\end{pmatrix}$.\n\nA fundamental tool for analyzing the CP rank of a $3$-way tensor of size $I_1 \\times I_2 \\times 2$ is the matrix pencil, which is a family of matrices parameterized by a scalar $x$. For the given tensor, we form the pencil $\\mathbf{P}(x) = \\mathbf{A} + x\\mathbf{B}$:\n$$\n\\mathbf{P}(x) = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + x\\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & -x \\\\ x & 1 \\end{pmatrix}\n$$\nIf $\\mathcal{T}$ has a CP rank of $r$, then $\\mathbf{P}(x) = \\sum_{p=1}^{r} (w_{p,1} + x w_{p,2}) \\mathbf{M}_p$. The rank of the pencil is at most $r$. A drop in the rank of $\\mathbf{P}(x)$ occurs for values of $x$ that make it singular. For a $2 \\times 2$ matrix, this occurs when its determinant is zero.\n$$\n\\det(\\mathbf{P}(x)) = \\det\\begin{pmatrix} 1 & -x \\\\ x & 1 \\end{pmatrix} = (1)(1) - (-x)(x) = 1 + x^2\n$$\nThe values of $x$ for which the rank drops are the roots of the characteristic equation $\\det(\\mathbf{P}(x))=0$.\n\nFirst, we can establish a lower bound on the rank. A rank-$1$ tensor $\\mathcal{T} = \\mathbf{u} \\otimes \\mathbf{v} \\otimes \\mathbf{w}$ would imply that both slices $\\mathbf{A} = w_1 \\mathbf{u}\\mathbf{v}^\\top$ and $\\mathbf{B} = w_2 \\mathbf{u}\\mathbf{v}^\\top$ are rank-$1$ matrices (or the zero matrix). However, the slice $\\mathbf{A}$ is the identity matrix $\\mathbf{I}$, which has rank $2$. Thus, $\\operatorname{rank}_{\\mathbb{F}}(\\mathcal{T}) \\geq 2$ for any field $\\mathbb{F}$.\n\n**Analysis over the Complex Field $\\mathbb{C}$**\nWe seek to find the minimal number of rank-$1$ terms, $r_{\\mathbb{C}}$, to represent $\\mathcal{T}$ over $\\mathbb{C}$. The characteristic equation is $1+x^2=0$. In the field of complex numbers, this equation has two distinct roots:\n$$\nx_1 = i, \\quad x_2 = -i\n$$\nThe existence of two distinct roots for which the pencil's rank drops to $1$ is a necessary and, for $2 \\times 2 \\times 2$ tensors, sufficient condition for the rank to be $2$. Since we know the rank is at least $2$, we conclude that $r_{\\mathbb{C}} = \\operatorname{rank}_{\\mathbb{C}}(\\mathcal{T}) = 2$.\n\nTo construct the decomposition, we use the matrices formed at these rank-dropping values of $x$:\n$\\mathbf{P}(i) = \\mathbf{A} + i\\mathbf{B} = \\begin{pmatrix} 1 & -i \\\\ i & 1 \\end{pmatrix}$\n$\\mathbf{P}(-i) = \\mathbf{A} - i\\mathbf{B} = \\begin{pmatrix} 1 & i \\\\ -i & 1 \\end{pmatrix}$\nThese matrices are rank-$1$. We can express $\\mathbf{A}$ and $\\mathbf{B}$ as linear combinations of them:\n$$\n\\mathbf{A} = \\frac{1}{2}(\\mathbf{P}(i) + \\mathbf{P}(-i)), \\qquad \\mathbf{B} = \\frac{1}{2i}(\\mathbf{P}(i) - \\mathbf{P}(-i)) = -\\frac{i}{2}\\mathbf{P}(i) + \\frac{i}{2}\\mathbf{P}(-i)\n$$\nLet $\\mathbf{M}_1 = \\mathbf{P}(i)$ and $\\mathbf{M}_2 = \\mathbf{P}(-i)$. We factor them into outer products:\n$\\mathbf{M}_1 = \\begin{pmatrix} 1 & -i \\\\ i & 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ i \\end{pmatrix}\\begin{pmatrix} 1 & -i \\end{pmatrix}$. Let $\\mathbf{u}_1 = \\begin{pmatrix} 1 \\\\ i \\end{pmatrix}$ and $\\mathbf{v}_1 = \\begin{pmatrix} 1 \\\\ -i \\end{pmatrix}$.\n$\\mathbf{M}_2 = \\begin{pmatrix} 1 & i \\\\ -i & 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -i \\end{pmatrix}\\begin{pmatrix} 1 & i \\end{pmatrix}$. Let $\\mathbf{u}_2 = \\begin{pmatrix} 1 \\\\ -i \\end{pmatrix}$ and $\\mathbf{v}_2 = \\begin{pmatrix} 1 \\\\ i \\end{pmatrix}$.\n\nComparing the expressions for $\\mathbf{A}$ and $\\mathbf{B}$ with the general form, we identify the coefficients $w_{p,k}$:\n$\\mathbf{A} = \\frac{1}{2}\\mathbf{M}_1 + \\frac{1}{2}\\mathbf{M}_2 \\implies w_{1,1} = \\frac{1}{2}, w_{2,1} = \\frac{1}{2}$.\n$\\mathbf{B} = -\\frac{i}{2}\\mathbf{M}_1 + \\frac{i}{2}\\mathbf{M}_2 \\implies w_{1,2} = -\\frac{i}{2}, w_{2,2} = \\frac{i}{2}$.\nThe factor vectors for the rank-$2$ complex CP decomposition are:\n$$\n\\mathbf{u}_1 = \\begin{pmatrix} 1 \\\\ i \\end{pmatrix}, \\mathbf{v}_1 = \\begin{pmatrix} 1 \\\\ -i \\end{pmatrix}, \\mathbf{w}_1 = \\begin{pmatrix} 1/2 \\\\ -i/2 \\end{pmatrix}\n$$\n$$\n\\mathbf{u}_2 = \\begin{pmatrix} 1 \\\\ -i \\end{pmatrix}, \\mathbf{v}_2 = \\begin{pmatrix} 1 \\\\ i \\end{pmatrix}, \\mathbf{w}_2 = \\begin{pmatrix} 1/2 \\\\ i/2 \\end{pmatrix}\n$$\n\n**Analysis over the Real Field $\\mathbb{R}$**\nWe now determine the rank $r_{\\mathbb{R}}$ over $\\mathbb{R}$. The characteristic equation $1+x^2=0$ has no solutions in $\\mathbb{R}$. This means that for any $x \\in \\mathbb{R}$, $1+x^2 \\ge 1$, so $\\det(\\mathbf{A}+x\\mathbf{B})$ is never zero. Consequently, the matrix pencil $\\mathbf{A}+x\\mathbf{B}$ has rank $2$ for all $x \\in \\mathbb{R}$.\n\nLet us justify that $r_{\\mathbb{R}} \\neq 2$. Assume, for contradiction, that a rank-$2$ real decomposition exists: $\\mathcal{T} = \\sum_{p=1}^2 \\mathbf{u}_p \\otimes \\mathbf{v}_p \\otimes \\mathbf{w}_p$ with real factor vectors. This implies $\\mathbf{A} + x\\mathbf{B} = (w_{1,1}+xw_{1,2})\\mathbf{u}_1\\mathbf{v}_1^\\top + (w_{2,1}+xw_{2,2})\\mathbf{u}_2\\mathbf{v}_2^\\top$.\nThe vectors $\\mathbf{w}_1$ and $\\mathbf{w}_2$ must be linearly independent, otherwise $\\mathbf{A}$ and $\\mathbf{B}$ would be linearly dependent, which is false ( $\\mathbf{A}=\\mathbf{I}$ is symmetric, $\\mathbf{B}$ is skew-symmetric). If $\\mathbf{w}_1, \\mathbf{w}_2$ are linearly independent, then so are the polynomials $(w_{1,1}+xw_{1,2})$ and $(w_{2,1}+xw_{2,2})$. There must exist a real $x$ that makes at least one of these coefficients zero. For example, if $w_{1,2} \\neq 0$, then $x = -w_{1,1}/w_{1,2}$ is a real root that makes the first coefficient zero. At this value of $x$, the pencil $\\mathbf{A}+x\\mathbf{B}$ would become a multiple of the rank-$1$ matrix $\\mathbf{u}_2\\mathbf{v}_2^\\top$, thus having rank at most $1$. This contradicts our finding that $\\mathbf{A}+x\\mathbf{B}$ always has rank $2$ for all real $x$. Therefore, the assumption of a rank-$2$ real decomposition is false, and $r_{\\mathbb{R}} \\neq 2$.\n\nSince $r_{\\mathbb{R}} \\geq 2$ and $r_{\\mathbb{R}} \\neq 2$, we must have $r_{\\mathbb{R}} \\geq 3$. We now show that a rank-$3$ decomposition over $\\mathbb{R}$ exists, which will establish that $r_{\\mathbb{R}} = 3$. We need to find three real rank-$1$ matrices $\\mathbf{M}_1, \\mathbf{M}_2, \\mathbf{M}_3$ such that both $\\mathbf{A}$ and $\\mathbf{B}$ lie in their span. Let us choose two simple rank-$1$ matrices whose sum is $\\mathbf{A}$:\n$$\n\\mathbf{M}_1 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}, \\quad \\mathbf{M}_2 = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nThen $\\mathbf{A} = \\mathbf{M}_1 + \\mathbf{M}_2$. We can write the decomposition coefficients for $\\mathbf{A}$ as $w_{1,1}=1, w_{2,1}=1, w_{3,1}=0$. The decomposition for $\\mathbf{B}$ will be $\\mathbf{B} = w_{1,2}\\mathbf{M}_1 + w_{2,2}\\mathbf{M}_2 + w_{3,2}\\mathbf{M}_3$. This implies that the matrix $\\mathbf{B} - w_{1,2}\\mathbf{M}_1 - w_{2,2}\\mathbf{M}_2$ must be a rank-$1$ matrix (a multiple of $\\mathbf{M}_3$). Let's compute this matrix:\n$$\n\\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix} - w_{1,2}\\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} - w_{2,2}\\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} -w_{1,2} & -1 \\\\ 1 & -w_{2,2} \\end{pmatrix}\n$$\nFor this matrix to be rank-$1$, its determinant must be zero:\n$(-w_{1,2})(-w_{2,2}) - (-1)(1) = 0 \\implies w_{1,2}w_{2,2} + 1 = 0$.\nWe can choose any real values satisfying this condition, for instance, $w_{1,2}=1$ and $w_{2,2}=-1$. With this choice, the rank-$1$ matrix becomes:\n$$\n\\mathbf{M}'_3 = \\begin{pmatrix} -1 & -1 \\\\ 1 & 1 \\end{pmatrix}\n$$\nWe set this to be our third rank-$1$ component, scaled by $w_{3,2}$, so let $w_{3,2}=1$ and $\\mathbf{M}_3 = \\mathbf{M}'_3$.\nThe linear combinations are:\n$\\mathbf{A} = 1 \\cdot \\mathbf{M}_1 + 1 \\cdot \\mathbf{M}_2 + 0 \\cdot \\mathbf{M}_3$\n$\\mathbf{B} = 1 \\cdot \\mathbf{M}_1 - 1 \\cdot \\mathbf{M}_2 + 1 \\cdot \\mathbf{M}_3$\nWe can now write the factor vectors for this rank-$3$ real CP decomposition:\n$\\mathbf{M}_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\implies \\mathbf{u}_1=\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\mathbf{v}_1=\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n$\\mathbf{M}_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\end{pmatrix} \\implies \\mathbf{u}_2=\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\mathbf{v}_2=\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\n$\\mathbf{M}_3 = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\implies \\mathbf{u}_3=\\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}, \\mathbf{v}_3=\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\nThe factors $\\mathbf{w}_p$ are given by the coefficients:\n$\\mathbf{w}_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, $\\mathbf{w}_2 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$, $\\mathbf{w}_3 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\nThis explicit construction demonstrates that $r_{\\mathbb{R}} \\leq 3$. Since we already proved $r_{\\mathbb{R}} \\geq 3$, we conclude that $r_{\\mathbb{R}} = 3$.\n\nFinally, we compute the requested difference $\\Delta$:\n$$\n\\Delta = r_{\\mathbb{R}} - r_{\\mathbb{C}} = 3 - 2 = 1.\n$$", "answer": "$$\n\\boxed{1}\n$$", "id": "3282140"}, {"introduction": "In most real-world applications, we don't start with factors; we start with incomplete and noisy data and aim to discover the underlying low-rank structure. This capstone practice brings you to the forefront of practical tensor methods by guiding you to implement a CP decomposition algorithm for tensors with missing values. By developing a solver based on a weighted least squares objective, you will learn how to perform tensor completion, a powerful technique for data imputation in fields ranging from recommender systems to bioinformatics.", "problem": "You must implement a complete, runnable program in Python that computes a Canonical Polyadic (CP) decomposition under missing data by minimizing a Weighted Least Squares (WLS) objective. The mathematical foundation starts from the core definition of the CP decomposition and the weighted least squares objective. Let a tensor be an order-$N$ array $ \\mathcal{X} \\in \\mathbb{R}^{I_0 \\times I_1 \\times \\cdots \\times I_{N-1}} $, and the CP decomposition approximates $ \\mathcal{X} $ as a sum of $R$ rank-$1$ components. The approximation is given by\n$$\n\\mathcal{\\hat{X}}[i_0,i_1,\\dots,i_{N-1}] = \\sum_{r=1}^{R} \\prod_{n=0}^{N-1} A^{(n)}[i_n, r],\n$$\nwhere $ A^{(n)} \\in \\mathbb{R}^{I_n \\times R} $ are the factor matrices. The weighted least squares objective, with weights $ W[i_0,\\dots,i_{N-1}] \\in \\{0,1\\} $ indicating missing ($0$) or observed ($1$) entries, is\n$$\n\\min_{\\{A^{(n)}\\}} \\; \\sum_{i_0=0}^{I_0-1} \\cdots \\sum_{i_{N-1}=0}^{I_{N-1}-1} W[i_0,\\dots,i_{N-1}] \\, \\left( \\mathcal{X}[i_0,\\dots,i_{N-1}] - \\mathcal{\\hat{X}}[i_0,\\dots,i_{N-1}] \\right)^2.\n$$\nYou must design an optimization method from first principles that is scientifically sound and numerically justified. Use alternating block-coordinate descent over the factor matrices, where each subproblem is solved as a weighted least squares with an $ \\ell_2 $-regularization term of $ \\lambda = 10^{-6} $ to ensure numerical stability. You must avoid any shortcut formulas in the problem statement; all update rules that you use should be derived consistently from the stated objective. Implement robust handling when the weights for an entire row in a mode are zero by leaving that row unchanged or setting it to zero, ensuring a well-defined behavior in such degenerate cases.\n\nDefinitions required for the implementation:\n- Let the Khatri-Rao product of two matrices $ U \\in \\mathbb{R}^{I \\times R} $ and $ V \\in \\mathbb{R}^{J \\times R} $ be the column-wise Kronecker product $ U \\odot V \\in \\mathbb{R}^{(I J) \\times R} $.\n- Let the mode-$n$ unfolding of $ \\mathcal{X} $ be the matrix $ X_{(n)} \\in \\mathbb{R}^{I_n \\times \\prod_{m \\neq n} I_m} $ obtained by permuting axes so that mode $n$ becomes the first dimension and flattening the remaining axes in a specific, consistent order. You must use a consistent unfolding scheme across all modes and match it with the corresponding Khatri-Rao ordering of the other factor matrices.\n\nYour program must implement the above optimization method for the following test suite. Each test case constructs a synthetic tensor by summing outer products of given factor matrices and adding Gaussian noise with a specified standard deviation $ \\sigma $ and random seed. Then, a binary weight tensor $ W $ is formed with zeros at specified missing indices and ones elsewhere. The CP-WLS algorithm must be run with rank $ R $ equal to the generating rank for $ T = 200 $ iterations, $ \\lambda = 10^{-6} $, and a fixed random initialization for the factor matrices using a specified seed. After fitting, compute the observed Root Mean Squared Error (RMSE) defined as\n$$\n\\mathrm{RMSE} = \\sqrt{ \\frac{ \\sum_{i_0,\\dots,i_{N-1}} W[i_0,\\dots,i_{N-1}] \\, \\left( \\mathcal{X}[i_0,\\dots,i_{N-1}] - \\mathcal{\\hat{X}}[i_0,\\dots,i_{N-1}] \\right)^2 }{ \\sum_{i_0,\\dots,i_{N-1}} W[i_0,\\dots,i_{N-1}] } }.\n$$\nReturn the RMSE for each test case.\n\nTest suite specification:\n\n- Case $1$ (happy path with partial missing entries):\n  - Order $N = 3$, dimensions $ (I_0,I_1,I_2) = (4,3,2) $, rank $ R = 2 $.\n  - Generating factor matrices:\n    $$\n    A^{(0)} = \\begin{bmatrix}\n    1.0 & 0.5 \\\\\n    0.8 & -0.2 \\\\\n    0.3 & 1.2 \\\\\n    1.1 & 0.7\n    \\end{bmatrix}, \\quad\n    A^{(1)} = \\begin{bmatrix}\n    0.9 & -0.4 \\\\\n    0.1 & 0.5 \\\\\n    1.0 & 0.3\n    \\end{bmatrix}, \\quad\n    A^{(2)} = \\begin{bmatrix}\n    1.0 & 0.2 \\\\\n    0.5 & -1.0\n    \\end{bmatrix}.\n    $$\n  - Noise standard deviation $ \\sigma = 0.01 $ with seed $ 42 $.\n  - Missing entries (set weights to zero at these indices):\n    $ (0,0,1), (1,2,0), (2,1,1), (3,0,0) $.\n  - Initialization seed for the algorithm: $ 202 $.\n\n- Case $2$ (all entries observed, higher rank):\n  - Order $N = 3$, dimensions $ (I_0,I_1,I_2) = (3,3,3) $, rank $ R = 3 $.\n  - Generating factor matrices:\n    $$\n    A^{(0)} = \\begin{bmatrix}\n    0.5 & 1.0 & -0.5 \\\\\n    0.2 & -0.3 & 0.7 \\\\\n    1.2 & 0.1 & 0.4\n    \\end{bmatrix}, \\quad\n    A^{(1)} = \\begin{bmatrix}\n    -0.6 & 0.9 & 0.3 \\\\\n    0.8 & 0.1 & -0.2 \\\\\n    0.5 & -1.1 & 0.6\n    \\end{bmatrix}, \\quad\n    A^{(2)} = \\begin{bmatrix}\n    0.7 & 0.4 & -0.9 \\\\\n    1.0 & -0.5 & 0.2 \\\\\n    0.3 & 0.8 & 0.1\n    \\end{bmatrix}.\n    $$\n  - Noise standard deviation $ \\sigma = 0.02 $ with seed $ 0 $.\n  - All entries observed: weights $ W[i_0,i_1,i_2] = 1 $ for all indices.\n  - Initialization seed for the algorithm: $ 202 $.\n\n- Case $3$ (edge case with an entirely missing slice in one mode and additional missing entries):\n  - Order $N = 3$, dimensions $ (I_0,I_1,I_2) = (5,4,3) $, rank $ R = 2 $.\n  - Generating factor matrices:\n    $$\n    A^{(0)} = \\begin{bmatrix}\n    0.9 & -0.1 \\\\\n    0.4 & 0.2 \\\\\n    -0.3 & 0.8 \\\\\n    1.0 & 0.5 \\\\\n    0.7 & -0.6\n    \\end{bmatrix}, \\quad\n    A^{(1)} = \\begin{bmatrix}\n    0.5 & 1.1 \\\\\n    -0.2 & 0.3 \\\\\n    0.9 & -0.4 \\\\\n    0.1 & 0.6\n    \\end{bmatrix}, \\quad\n    A^{(2)} = \\begin{bmatrix}\n    1.0 & 0.2 \\\\\n    -0.5 & 0.7 \\\\\n    0.3 & -0.9\n    \\end{bmatrix}.\n    $$\n  - Noise standard deviation $ \\sigma = 0.015 $ with seed $ 123 $.\n  - Missing entries: all indices with $ i_0 = 0 $ are missing, plus $ (2,1,2) $ and $ (4,3,1) $.\n  - Initialization seed for the algorithm: $ 202 $.\n\nImplementation requirements:\n- Use Python $3.12$ and the NumPy library $1.23.5$ only.\n- Implement a general $N$-mode CP-WLS algorithm with alternating updates over modes, a consistent unfolding scheme, and Khatri-Rao products aligned with that scheme.\n- Use $T = 200$ iterations and $ \\lambda = 10^{-6} $ regularization.\n- After each full sweep across all modes, perform column normalization by dividing the columns of all factor matrices except $ A^{(0)} $ by their $ \\ell_2 $-norms and absorbing the product of norms into $ A^{(0)} $ for each component, to maintain scaling stability.\n- Compute the final observed RMSE as defined above.\n\nFinal output format:\n- Your program should produce a single line of output containing the RMSE values for the three test cases as a comma-separated list enclosed in square brackets, for example $ [r_1,r_2,r_3] $. Each $ r_k $ must be a floating-point value.", "solution": "The problem requires the implementation of a Canonical Polyadic (CP) decomposition for tensors with missing entries by minimizing a Weighted Least Squares (WLS) objective. The solution is developed from first principles using an alternating optimization scheme.\n\n### Mathematical Formulation\n\nThe CP decomposition approximates a given order-$N$ tensor $ \\mathcal{X} \\in \\mathbb{R}^{I_0 \\times \\cdots \\times I_{N-1}} $ by a model tensor $ \\mathcal{\\hat{X}} $ composed of $R$ rank-1 components. The elements of $ \\mathcal{\\hat{X}} $ are given by:\n$$\n\\mathcal{\\hat{X}}[i_0,i_1,\\dots,i_{N-1}] = \\sum_{r=1}^{R} \\prod_{n=0}^{N-1} A^{(n)}[i_n, r]\n$$\nwhere $ A^{(n)} \\in \\mathbb{R}^{I_n \\times R} $ are the factor matrices.\n\nFor a tensor with missing data, we are given a weight tensor $ W \\in \\{0, 1\\}^{I_0 \\times \\cdots \\times I_{N-1}} $, where $W[\\dots] = 1$ for observed entries and $W[\\dots] = 0$ for missing entries. The optimization problem is to find the factor matrices that minimize the $\\ell_2$-regularized weighted sum of squared errors:\n$$\n\\min_{\\{A^{(n)}\\}} \\; \\frac{1}{2} \\sum_{i_0=0}^{I_0-1} \\cdots \\sum_{i_{N-1}=0}^{I_{N-1}-1} W[\\dots] \\left( \\mathcal{X}[\\dots] - \\mathcal{\\hat{X}}[\\dots] \\right)^2 + \\frac{\\lambda}{2} \\sum_{n=0}^{N-1} \\|A^{(n)}\\|_F^2\n$$\nwhere $ \\| \\cdot \\|_F $ is the Frobenius norm and $ \\lambda $ is the regularization parameter.\n\n### Optimization via Alternating Block-Coordinate Descent\n\nThis objective function is non-convex, making a simultaneous global solution for all factor matrices intractable. A standard and effective heuristic is to use Alternating Block-Coordinate Descent, often called Alternating Least Squares (ALS). In this method, we optimize for one factor matrix $ A^{(k)} $ at a time while keeping all other factor matrices $ \\{A^{(n)}\\}_{n \\neq k} $ fixed. This process is repeated iteratively for all modes $k = 0, \\dots, N-1$.\n\n### Derivation of the Update Rule\n\nLet's derive the update rule for a specific factor matrix $ A^{(k)} $. When all other factors are fixed, the objective function can be rewritten using the mode-$k$ unfolding of the tensors. The mode-$k$ unfolding, denoted $X_{(k)}$, is a matrix of size $I_k \\times (\\prod_{m \\neq k} I_m)$ where the rows correspond to the indices of mode $k$.\n\nThe CP model can be expressed in matricized form as:\n$$\nX_{(k)} \\approx A^{(k)} \\left( \\mathbf{C}^{(k)} \\right)^T\n$$\nHere, $\\mathbf{C}^{(k)}$ is the Khatri-Rao product of all factor matrices except $A^{(k)}$. The specific ordering of matrices in this product must be consistent with the column ordering of the unfolded tensor. If we unfold $\\mathcal{X}$ to $X_{(k)}$ such that the column index $j$ corresponds to the multi-index $(i_m)_{m \\neq k}$ with the lowest mode index varying fastest (`order='F'` in NumPy), then $\\mathbf{C}^{(k)}$ must be constructed as:\n$$\n\\mathbf{C}^{(k)} = A^{(m_p)} \\odot A^{(m_{p-1})} \\odot \\cdots \\odot A^{(m_1)}\n$$\nwhere $m_1 < m_2 < \\dots < m_p$ are the sorted indices of the modes other than $k$. In the implementation, this is achieved by iteratively applying the Khatri-Rao product to the sorted list of other factor matrices.\n\nWith this formulation, the objective function, as a function of only $A^{(k)}$, decouples row-wise. For each row $i_k$ of $A^{(k)}$, denoted by the row vector $a_{i_k}^{(k)}$, the subproblem is an independent regularized weighted least squares problem:\n$$\n\\min_{a_{i_k}^{(k)}} \\frac{1}{2} \\| \\sqrt{w_{i_k}} \\odot (x_{i_k} - a_{i_k}^{(k)} (\\mathbf{C}^{(k)})^T) \\|_2^2 + \\frac{\\lambda'}{2} \\|a_{i_k}^{(k)}\\|_2^2\n$$\nwhere $x_{i_k}$ and $w_{i_k}$ are the $i_k$-th rows of the unfolded matrices $X_{(k)}$ and $W_{(k)}$, respectively, and $\\odot$ is the element-wise product. The original regularization term is distributed among the subproblems; we solve this with parameter $\\lambda$.\n\nThis is a standard ridge regression problem. The solution is found by setting the gradient with respect to $a_{i_k}^{(k)}$ to zero, which yields the following $R \\times R$ linear system for the transpose of the row vector, $(a_{i_k}^{(k)})^T$:\n$$\n\\left( (\\mathbf{C}^{(k)})^T \\mathrm{diag}(w_{i_k}) \\mathbf{C}^{(k)} + \\lambda I_R \\right) (a_{i_k}^{(k)})^T = (\\mathbf{C}^{(k)})^T \\mathrm{diag}(w_{i_k}) (x_{i_k})^T\n$$\nwhere $I_R$ is the $R \\times R$ identity matrix.\n\n### Algorithmic Design and Implementation\n\nThe algorithm proceeds as follows:\n\n1.  **Initialization**: Factor matrices $\\{A^{(n)}\\}$ are initialized with values drawn from a standard normal distribution, using the specified random seed.\n2.  **Iterative Updates**: For a fixed number of iterations ($T=200$):\n    a.  **Mode Iteration**: For each mode $n = 0, \\dots, N-1$:\n        i.  **Form $\\mathbf{C}^{(n)}$**: The Khatri-Rao product of factor matrices $\\{A^{(m)}\\}_{m \\neq n}$ is computed. The matrices are sorted by mode index to match the unfolding convention.\n        ii. **Unfold Tensors**: The data tensor $\\mathcal{X}$ and weight tensor $W$ are unfolded to $X_{(n)}$ and $W_{(n)}$.\n        iii. **Row-wise Updates**: For each row $i_n = 0, \\dots, I_n-1$ of $A^{(n)}$:\n            - The weight vector $w_{i_n}$ is extracted from $W_{(n)}$.\n            - **Edge Case Handling**: If all weights in $w_{i_n}$ are zero (i.e., an entire slice is missing), the matrix $(\\mathbf{C}^{(n)})^T \\mathrm{diag}(w_{i_n}) \\mathbf{C}^{(n)}$ and the vector on the right-hand side of the linear system both become zero. The system reduces to $\\lambda I_R (a_{i_n}^{(n)})^T = 0$, which implies $(a_{i_n}^{(n)})^T=0$. Thus, the row is set to a zero vector.\n            - **Solve Linear System**: For non-degenerate rows, the $R \\times R$ system is formed and solved for $(a_{i_n}^{(n)})^T$. To avoid explicitly forming the large diagonal matrix $\\mathrm{diag}(w_{i_n})$, the products are calculated efficiently using broadcasting: `(C_n.T * w_i) @ C_n` and `(C_n.T * w_i) @ x_i.T`.\n    b.  **Normalization**: After a full sweep through all modes, the columns of the factor matrices are normalized to have unit $\\ell_2$-norm. This is a crucial step for numerical stability and managing the inherent scaling ambiguity of the CP decomposition (where one can scale a column of $A^{(n)}$ by $\\alpha$ and a column of $A^{(m)}$ by $1/\\alpha$ without changing the product). The norms are accumulated and absorbed into the columns of the first factor matrix, $A^{(0)}$.\n3.  **Reconstruction and Evaluation**: After the final iteration, the model tensor $\\hat{\\mathcal{X}}$ is reconstructed from the fitted factor matrices. The final observed Root Mean Squared Error (RMSE) is calculated using the specified formula:\n$$\n\\mathrm{RMSE} = \\sqrt{ \\frac{ \\sum_{i_0,\\dots,i_{N-1}} W[\\dots] \\, \\left( \\mathcal{X}[\\dots] - \\mathcal{\\hat{X}}[\\dots] \\right)^2 }{ \\sum_{i_0,\\dots,i_{N-1}} W[\\dots] } }\n$$\n\nThis principled approach ensures a robust and correct implementation of the CP-WLS algorithm, directly derived from its mathematical definition.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef khatri_rao(matrices):\n    \"\"\"\n    Computes the Khatri-Rao product of a list of matrices.\n    The product is computed iteratively: M_1 @ M_2 @ ... @ M_k.\n    \"\"\"\n    if not matrices:\n        return None\n    if len(matrices) == 1:\n        return matrices[0]\n\n    result = matrices[0]\n    for i in range(1, len(matrices)):\n        mat = matrices[i]\n        I_res, R = result.shape\n        I_mat, _ = mat.shape\n        \n        # Use einsum for column-wise outer products, then reshape.\n        # 'ir,jr->ijr' computes the outer product of each corresponding column r.\n        # reshape with 'F' order ensures the first index (from `result`) varies fastest,\n        # matching the unfolding convention.\n        result = np.einsum('ir,jr->ijr', result, mat).reshape(I_res * I_mat, R, order='F')\n    return result\n\ndef reconstruct(factors):\n    \"\"\"\n    Reconstructs a full tensor from its CP factor matrices.\n    \"\"\"\n    if not factors:\n        return None\n    \n    R = factors[0].shape[1]\n    N = len(factors)\n    shape = tuple(f.shape[0] for f in factors)\n    \n    full_tensor = np.zeros(shape)\n    for r in range(R):\n        # Start with the first factor's r-th column\n        rank1_tensor = factors[0][:, r]\n        # Successively compute outer products with other factors' r-th columns\n        for n in range(1, N):\n            rank1_tensor = np.multiply.outer(rank1_tensor, factors[n][:, r])\n        full_tensor += rank1_tensor\n        \n    return full_tensor\n\ndef cp_wls(X, W, R, T, lmbda, init_seed):\n    \"\"\"\n    Computes the CP decomposition for a tensor with missing data using\n    Weighted Alternating Least Squares (WALS).\n    \"\"\"\n    N = X.ndim\n    dims = X.shape\n    \n    rng = np.random.default_rng(init_seed)\n    factors = [rng.standard_normal((dims[n], R)) for n in range(N)]\n\n    for _ in range(T):\n        for n in range(N):\n            # 1. Form the Khatri-Rao product of all other factor matrices.\n            modes_other = sorted([m for m in range(N) if m != n])\n            kr_matrices = [factors[m] for m in modes_other]\n            C_n = khatri_rao(kr_matrices)\n            \n            # 2. Unfold data and weight tensors along mode n with Fortran ordering.\n            Xn = np.moveaxis(X, n, 0).reshape((dims[n], -1), order='F')\n            Wn = np.moveaxis(W, n, 0).reshape((dims[n], -1), order='F')\n            \n            # 3. Update each row of the factor matrix A^{(n)}.\n            new_An = np.zeros_like(factors[n])\n            for i in range(dims[n]):\n                wi = Wn[i, :]\n                \n                if np.sum(wi) < 1e-12:\n                    continue  # Row remains zero\n                \n                # Setup and solve the regularized weighted linear least squares for the row.\n                M = (C_n.T * wi) @ C_n + lmbda * np.eye(R)\n                v = (C_n.T * wi) @ Xn[i, :].T\n                \n                try:\n                    solved_row = np.linalg.solve(M, v)\n                    new_An[i, :] = solved_row\n                except np.linalg.LinAlgError:\n                    # Failsafe for singular matrix, unlikely with regularization.\n                    new_An[i, :] = 0.0\n\n            factors[n] = new_An\n\n        # 4. Column normalization after a full sweep.\n        col_norms_product = np.ones(R)\n        for mode_idx in range(1, N):\n            norms = np.linalg.norm(factors[mode_idx], axis=0)\n            non_zero_indices = norms > 1e-12\n            factors[mode_idx][:, non_zero_indices] /= norms[non_zero_indices]\n            norms[norms < 1e-12] = 1.0\n            col_norms_product *= norms\n        \n        factors[0] *= col_norms_product\n    \n    return factors\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Global algorithm parameters\n    T = 200\n    lmbda = 1e-6\n    init_seed = 202\n\n    test_cases = [\n        # Case 1: Happy path\n        {\n            \"dims\": (4, 3, 2), \"R\": 2, \"sigma\": 0.01, \"noise_seed\": 42,\n            \"gen_factors\": [\n                np.array([[1.0, 0.5], [0.8, -0.2], [0.3, 1.2], [1.1, 0.7]]),\n                np.array([[0.9, -0.4], [0.1, 0.5], [1.0, 0.3]]),\n                np.array([[1.0, 0.2], [0.5, -1.0]])\n            ],\n            \"missing_indices\": [(0, 0, 1), (1, 2, 0), (2, 1, 1), (3, 0, 0)]\n        },\n        # Case 2: Fully observed\n        {\n            \"dims\": (3, 3, 3), \"R\": 3, \"sigma\": 0.02, \"noise_seed\": 0,\n            \"gen_factors\": [\n                np.array([[0.5, 1.0, -0.5], [0.2, -0.3, 0.7], [1.2, 0.1, 0.4]]),\n                np.array([[-0.6, 0.9, 0.3], [0.8, 0.1, -0.2], [0.5, -1.1, 0.6]]),\n                np.array([[0.7, 0.4, -0.9], [1.0, -0.5, 0.2], [0.3, 0.8, 0.1]])\n            ],\n            \"missing_indices\": []\n        },\n        # Case 3: Edge case with a missing slice\n        {\n            \"dims\": (5, 4, 3), \"R\": 2, \"sigma\": 0.015, \"noise_seed\": 123,\n            \"gen_factors\": [\n                np.array([[0.9, -0.1], [0.4, 0.2], [-0.3, 0.8], [1.0, 0.5], [0.7, -0.6]]),\n                np.array([[0.5, 1.1], [-0.2, 0.3], [0.9, -0.4], [0.1, 0.6]]),\n                np.array([[1.0, 0.2], [-0.5, 0.7], [0.3, -0.9]])\n            ],\n            \"missing_indices\": [\n                (0, i1, i2) for i1 in range(4) for i2 in range(3)\n            ] + [(2, 1, 2), (4, 3, 1)]\n        }\n    ]\n\n    rmse_results = []\n    \n    for case in test_cases:\n        # 1. Generate synthetic data\n        X_true = reconstruct(case[\"gen_factors\"])\n        rng_noise = np.random.default_rng(case[\"noise_seed\"])\n        noise = rng_noise.standard_normal(size=case[\"dims\"]) * case[\"sigma\"]\n        X_noisy = X_true + noise\n        \n        # 2. Create weight tensor W\n        W = np.ones(case[\"dims\"])\n        if case[\"missing_indices\"]:\n            # Need to handle combined list and generator\n            indices_to_set = list(case[\"missing_indices\"])\n            for idx in indices_to_set:\n                W[idx] = 0\n            \n        # 3. Run CP-WLS algorithm\n        fitted_factors = cp_wls(X_noisy, W, case[\"R\"], T, lmbda, init_seed)\n        \n        # 4. Reconstruct tensor from fitted factors\n        X_hat = reconstruct(fitted_factors)\n        \n        # 5. Compute observed RMSE\n        squared_error = W * (X_noisy - X_hat)**2\n        sum_sq_err = np.sum(squared_error)\n        num_observed = np.sum(W)\n        \n        rmse = np.sqrt(sum_sq_err / num_observed) if num_observed > 0 else 0.0\n        rmse_results.append(rmse)\n\n    print(f\"[{','.join(f'{r:.8f}' for r in rmse_results)}]\")\n\nsolve()\n```", "id": "3282166"}]}