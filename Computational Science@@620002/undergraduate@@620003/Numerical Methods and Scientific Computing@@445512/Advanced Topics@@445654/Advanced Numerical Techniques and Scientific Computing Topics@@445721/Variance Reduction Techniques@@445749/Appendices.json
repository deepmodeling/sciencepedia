{"hands_on_practices": [{"introduction": "The control variates method is a powerful technique for increasing the precision of Monte Carlo estimates by leveraging a correlated variable with a known expected value. This first exercise [@problem_id:1348989] provides a fundamental hands-on application of this principle. By working through the calculation of the optimal coefficient and the resulting variance reduction, you will gain a concrete understanding of how and why this method improves simulation efficiency.", "problem": "An analyst is trying to estimate the value of $\\theta = E[(U+1)^2]$, where $U$ is a random variable uniformly distributed on the interval $[0, 1]$. The standard approach is to use a simple Monte Carlo estimator, where one generates samples of $X = (U+1)^2$ and averages them. The precision of this estimator is determined by the variance of a single sample, $\\text{Var}(X)$.\n\nTo improve the efficiency of the estimation, a control variate technique is proposed. A new estimator is constructed using a related random variable $C = U$, for which the expected value is known. The new estimator for a single sample is given by $X(b) = X - b(C - E[C])$, where $b$ is a real-valued constant coefficient to be optimized.\n\nYour task is to determine the theoretical improvement offered by this technique. First, find the optimal coefficient $b^*$ that minimizes the variance of the new estimator, $\\text{Var}(X(b))$. Then, calculate the variance reduction factor, which is the ratio of the variance of a single optimized control variate sample to the variance of a single simple sample.\n\nExpress this variance reduction factor, $\\frac{\\text{Var}(X(b^*))}{\\text{Var}(X)}$, as an exact fraction.", "solution": "Let $U \\sim \\text{Unif}[0,1]$, $X=(U+1)^{2}$, and $C=U$. For $k \\geq 0$, $E[U^{k}]=\\frac{1}{k+1}$. Hence $E[U]=\\frac{1}{2}$, $E[U^{2}]=\\frac{1}{3}$, $E[U^{3}]=\\frac{1}{4}$, $E[U^{4}]=\\frac{1}{5}$, and $\\text{Var}(U)=\\frac{1}{12}$.\n\nExpand $X$ as $X=U^{2}+2U+1$. Then\n$$\nE[X]=E[U^{2}]+2E[U]+1=\\frac{1}{3}+1+1=\\frac{7}{3}.\n$$\nAlso\n$$\nX^{2}=(U^{2}+2U+1)^{2}=U^{4}+4U^{3}+6U^{2}+4U+1,\n$$\nso\n$$\nE[X^{2}]=\\frac{1}{5}+4\\cdot\\frac{1}{4}+6\\cdot\\frac{1}{3}+4\\cdot\\frac{1}{2}+1=\\frac{31}{5}.\n$$\nTherefore\n$$\n\\text{Var}(X)=E[X^{2}]-(E[X])^{2}=\\frac{31}{5}-\\left(\\frac{7}{3}\\right)^{2}=\\frac{34}{45}.\n$$\n\nCompute the covariance with $C=U$:\n$$\n\\text{Cov}(U^{2},U)=E[U^{3}]-E[U^{2}]E[U]=\\frac{1}{4}-\\frac{1}{3}\\cdot\\frac{1}{2}=\\frac{1}{12},\n$$\nso\n$$\n\\text{Cov}(X,C)=\\text{Cov}(U^{2}+2U+1,U)=\\frac{1}{12}+2\\cdot\\text{Var}(U)=\\frac{1}{12}+2\\cdot\\frac{1}{12}=\\frac{1}{4},\n$$\nand $\\text{Var}(C)=\\text{Var}(U)=\\frac{1}{12}$.\n\nFor the control variate estimator $X(b)=X-b(C-E[C])$, the variance is\n$$\n\\text{Var}(X(b))=\\text{Var}(X)-2b\\,\\text{Cov}(X,C)+b^{2}\\text{Var}(C).\n$$\nMinimizing the quadratic in $b$ yields\n$$\nb^{*}=\\frac{\\text{Cov}(X,C)}{\\text{Var}(C)}=\\frac{\\frac{1}{4}}{\\frac{1}{12}}=3,\n$$\nand the minimized variance\n$$\n\\text{Var}(X(b^{*}))=\\text{Var}(X)-\\frac{\\text{Cov}(X,C)^{2}}{\\text{Var}(C)}=\\frac{34}{45}-\\frac{\\left(\\frac{1}{4}\\right)^{2}}{\\frac{1}{12}}=\\frac{34}{45}-\\frac{3}{4}=\\frac{1}{180}.\n$$\n\nHence the variance reduction factor is\n$$\n\\frac{\\text{Var}(X(b^{*}))}{\\text{Var}(X)}=\\frac{\\frac{1}{180}}{\\frac{34}{45}}=\\frac{1}{136}.\n$$", "answer": "$$\\boxed{\\frac{1}{136}}$$", "id": "1348989"}, {"introduction": "Antithetic variates offer an elegant way to reduce variance by generating negatively correlated pairs of samples from a single random draw. This practice [@problem_id:1349000] applies this concept to a tangible physical system, estimating the expected maximum height of a projectile with a random launch speed. You will see how this method effectively reduces estimator variance when the function of interest is monotonic, a common scenario in many models.", "problem": "A team of engineers is designing a novel micro-catapult system. In this system, projectiles are launched at a fixed angle $\\theta$ with respect to the horizontal. Due to fluctuations in the energy source, the initial launch speed, $v_0$, is a random variable. It is known to be uniformly distributed between a minimum speed $v_{min}$ and a maximum speed $v_{max}$. The maximum height $H$ reached by a projectile is given by the formula $H(v_0) = \\frac{(v_0 \\sin\\theta)^2}{2g}$, where $g$ is the acceleration due to gravity.\n\nTo estimate the expected maximum height, $E[H]$, the team employs the antithetic variates method, a variance reduction technique. A specific launch speed $v$ is generated from a standard uniform random variable $u \\sim U(0,1)$ using the inverse transform sampling formula: $v = v_{min} + (v_{max} - v_{min})u$. An antithetic pair of speeds, $(v_1, v'_1)$, is then generated using a single draw $u_1$ and its antithetic counterpart $u'_1 = 1 - u_1$.\n\nYou are given the following parameters for the system:\n- Launch angle: $\\theta = 30^{\\circ}$\n- Minimum speed: $v_{min} = 20.0$ m/s\n- Maximum speed: $v_{max} = 50.0$ m/s\n- Acceleration due to gravity: $g = 9.81 \\, \\text{m/s}^2$\n\nBased on a single random draw $u_1 = 0.300$ from the standard uniform distribution $U(0,1)$, calculate the antithetic variate estimate for the expected maximum height. Express your answer in meters, rounded to three significant figures.", "solution": "We seek the antithetic variate estimate of the expected maximum height using one antithetic pair. The maximum height as a function of launch speed is given by\n$$\nH(v_{0})=\\frac{(v_{0}\\sin\\theta)^{2}}{2g}.\n$$\nThe inverse transform sampling for a uniform speed on $[v_{\\min},v_{\\max}]$ is\n$$\nv(u)=v_{\\min}+(v_{\\max}-v_{\\min})u,\n$$\nand the antithetic pair uses $u_{1}$ and $u_{1}'=1-u_{1}$, giving speeds\n$$\nv_{1}=v(u_{1}),\\qquad v_{1}'=v(u_{1}').\n$$\nThe antithetic variate estimator based on one pair is the average\n$$\n\\widehat{E}_{\\text{AV}}=\\frac{H(v_{1})+H(v_{1}')}{2}.\n$$\n\nWith $\\theta=30^{\\circ}$, we use $\\sin(30^{\\circ})=\\frac{1}{2}$, hence\n$$\nH(v)=\\frac{v^{2}\\sin^{2}(30^{\\circ})}{2g}=\\frac{v^{2}}{8g}.\n$$\nGiven $v_{\\min}=20.0$, $v_{\\max}=50.0$, and $u_{1}=0.300$,\n$$\nv_{1}=20.0+(50.0-20.0)\\cdot 0.300=20.0+30.0\\cdot 0.300=29.0,\n$$\n$$\nu_{1}'=1-0.300=0.700,\\qquad v_{1}'=20.0+30.0\\cdot 0.700=41.0.\n$$\nThus\n$$\nH(v_{1})=\\frac{29.0^{2}}{8g}=\\frac{841}{8g},\\qquad H(v_{1}')=\\frac{41.0^{2}}{8g}=\\frac{1681}{8g}.\n$$\nThe antithetic estimate is\n$$\n\\widehat{E}_{\\text{AV}}=\\frac{1}{2}\\left(\\frac{841}{8g}+\\frac{1681}{8g}\\right)=\\frac{841+1681}{16g}=\\frac{2522}{16g}.\n$$\nWith $g=9.81$, this yields\n$$\n\\widehat{E}_{\\text{AV}}=\\frac{2522}{16\\times 9.81}=\\frac{2522}{156.96}\\approx 16.0678.\n$$\nRounded to three significant figures, the estimate is $16.1$ meters.", "answer": "$$\\boxed{16.1}$$", "id": "1349000"}, {"introduction": "Stratified sampling improves estimation by guaranteeing a representative sample across the entire domain, effectively removing the \"between-strata\" component of variance. This advanced challenge [@problem_id:3285833] pushes beyond simple application and asks you to design a function that exposes the core mechanism of this technique. By constructing a function where stratification is highly effective along one axis but completely ineffective along another, you will gain a profound insight into the conditions that govern the success of stratified sampling.", "problem": "Consider independent random variables $X$ and $Y$ uniformly distributed on $[0,1]$, and the problem of estimating the integral $I = \\mathbb{E}[f(X,Y)]$ using Monte Carlo (MC) sampling. Two variance reduction strategies are considered: stratified sampling along the $x$-axis and stratified sampling along the $y$-axis. In both strategies, the axis is divided into $L$ equal-width strata, and a total of $n$ samples are drawn with equal allocation across strata, so each stratum contains $m = n/L$ samples. The stratified estimator is formed as the probability-weighted sum of the mean of $f(X,Y)$ within each stratum, with equal stratum probabilities.\n\nDesign an explicit two-variable function $f(x,y)$ defined on $[0,1]^{2}$ such that stratification along the $x$-axis is highly effective, meaning the variance of the stratified estimator along the $x$-axis is strictly smaller than that of the plain MC estimator by a factor that scales like $1/L^{2}$, while stratification along the $y$-axis provides no variance reduction relative to plain MC. Your derivation must start from the core definitions of variance, conditional variance, and the construction of the stratified estimator, without invoking pre-packaged formulas for the variance of stratified sampling.\n\nProvide the explicit analytic expression of the function $f(x,y)$ that satisfies these properties as your final answer. No rounding is required, and no units are involved. Express the final answer as a single closed-form expression.", "solution": "The problem asks us to design a two-variable function $f(x,y)$ on $[0,1]^2$ such that stratified sampling is highly effective along the $x$-axis but provides no benefit along the $y$-axis, relative to a plain Monte Carlo (MC) estimator. We will derive the necessary properties of $f(x,y)$ from the fundamental definitions of variance and the estimators, and then construct an explicit function that satisfies these properties.\n\nLet $X$ and $Y$ be independent random variables uniformly distributed on $[0,1]$. We wish to estimate $I = \\mathbb{E}[f(X,Y)] = \\int_0^1 \\int_0^1 f(x,y) dx dy$.\n\nThe plain MC estimator based on $n$ independent samples $(X_i, Y_i)$ is $\\hat{I}_{MC} = \\frac{1}{n} \\sum_{i=1}^n f(X_i, Y_i)$. Its variance is $\\text{Var}(\\hat{I}_{MC}) = \\frac{1}{n} \\text{Var}(f(X,Y))$.\n\nThe variance of any random variable $Z$ can be decomposed using the law of total variance with respect to another random variable $W$: $\\text{Var}(Z) = \\mathbb{E}[\\text{Var}(Z|W)] + \\text{Var}(\\mathbb{E}[Z|W])$.\n\nFor stratified sampling, we partition the domain. Let $W$ be a discrete random variable indicating the stratum to which a sample $(X,Y)$ belongs. The variance of the stratified estimator, with $n$ total samples and proportional allocation, is given by $\\text{Var}(\\hat{I}_{S}) = \\frac{1}{n} \\mathbb{E}[\\text{Var}(f(X,Y)|W)]$. The variance reduction compared to the plain MC estimator is therefore $\\text{Var}(\\hat{I}_{MC}) - \\text{Var}(\\hat{I}_{S}) = \\frac{1}{n} \\text{Var}(\\mathbb{E}[f(X,Y)|W])$.\n\n**Condition 1: No Variance Reduction for Stratification Along the y-axis**\n\nFor stratification along the $y$-axis, the domain $[0,1]^2$ is divided into $L$ strata $S_{y,j} = [0,1] \\times [\\frac{j-1}{L}, \\frac{j}{L}]$ for $j=1, \\dots, L$. Let $W_y$ be the random variable indicating the y-stratum. The condition of no variance reduction means the variance reduction term is zero:\n$$\n\\text{Var}(\\mathbb{E}[f(X,Y)|W_y]) = 0\n$$\nThis implies that the conditional expectation $\\mathbb{E}[f(X,Y)|W_y=j]$ must be a constant for all strata $j=1, \\dots, L$. The conditional expectation in stratum $j$ is the average value of $f$ over that stratum:\n$$\n\\mathbb{E}[f(X,Y)|W_y=j] = \\frac{1}{\\text{Area}(S_{y,j})} \\iint_{S_{y,j}} f(x,y) dx dy = L \\int_{\\frac{j-1}{L}}^{\\frac{j}{L}} \\left( \\int_0^1 f(x,y) dx \\right) dy\n$$\nLet $g(y) = \\int_0^1 f(x,y) dx$. The condition is that $L \\int_{\\frac{j-1}{L}}^{\\frac{j}{L}} g(y) dy$ is constant for all $j$. Since this must hold for any number of strata $L$, it implies that $g(y)$ must be a constant for almost every $y \\in [0,1]$.\n$$\n\\int_0^1 f(x,y) dx = C \\quad (\\text{for some constant } C)\n$$\n\n**Condition 2: High Effectiveness for Stratification Along the x-axis**\n\nFor stratification along the $x$-axis, the strata are $S_{x,j} = [\\frac{j-1}{L}, \\frac{j}{L}] \\times [0,1]$. The variance of the x-stratified estimator is:\n$$\n\\text{Var}(\\hat{I}_{S,x}) = \\frac{1}{n} \\mathbb{E}[\\text{Var}(f(X,Y)|W_x)] = \\frac{1}{n} \\sum_{j=1}^L P(W_x=j) \\text{Var}(f(X,Y)|W_x=j)\n$$\nWith equal-width strata, $P(W_x=j) = 1/L$. Let $h=1/L$ be the width of a stratum.\n$$\n\\text{Var}(\\hat{I}_{S,x}) = \\frac{1}{nL} \\sum_{j=1}^L \\text{Var}_{S_{x,j}}(f(X,Y))\n$$\nHere, $\\text{Var}_{S_{x,j}}(f(X,Y))$ denotes the variance of $f(X,Y)$ where $X \\sim U[\\frac{j-1}{L}, \\frac{j}{L}]$ and $Y \\sim U[0,1]$.\nThe problem states that this variance should be smaller than $\\text{Var}(\\hat{I}_{MC})$ by a factor that scales like $1/L^2$. Since $\\text{Var}(\\hat{I}_{MC})$ is a constant with respect to $L$ (for a non-constant $f$), we require $\\text{Var}(\\hat{I}_{S,x}) = O(1/L^2)$.\n\nLet's analyze the within-stratum variance, $\\text{Var}_{S_{x,j}}(f)$, by applying the law of total variance, conditioning on $X$:\n$$\n\\text{Var}_{S_{x,j}}(f(X,Y)) = \\mathbb{E}_{S_{x,j}}[\\text{Var}_Y(f(X,Y)|X)] + \\text{Var}_{S_{x,j}}(\\mathbb{E}_Y[f(X,Y)|X])\n$$\nThe second term, $\\text{Var}_{S_{x,j}}(\\mathbb{E}_Y[f(X,Y)|X])$, represents the variance due to the change in the function's $y$-average as $x$ varies within the small stratum. If $\\mathbb{E}_Y[f(x,Y)]$ is a smooth function of $x$, say $G(x)$, then its variance over an interval of width $h=1/L$ is approximately $\\frac{h^2}{12} (G'(x_j))^2$, which scales as $O(1/L^2)$. This term contributes to the desired scaling.\n\nThe first term is $\\mathbb{E}_{S_{x,j}}[\\text{Var}_Y(f(X,Y)|X)] = L \\int_{\\frac{j-1}{L}}^{\\frac{j}{L}} \\text{Var}_Y(f(x,Y)) dx$. Summing over strata and dividing by $nL$:\n$$\n\\frac{1}{nL} \\sum_{j=1}^L \\left( L \\int_{\\frac{j-1}{L}}^{\\frac{j}{L}} \\text{Var}_Y(f(x,Y)) dx \\right) = \\frac{1}{n} \\int_0^1 \\text{Var}_Y(f(x,Y)) dx\n$$\nThis part of the total variance does not depend on $L$. For $\\text{Var}(\\hat{I}_{S,x})$ to have the overall scaling of $O(1/L^2)$, this term must be zero.\n$$\n\\int_0^1 \\text{Var}_Y(f(x,Y)) dx = 0\n$$\nSince variance is non-negative, this implies $\\text{Var}_Y(f(x,Y))=0$ for almost every $x \\in [0,1]$. This means that for a fixed $x$, $f(x,y)$ must be constant with respect to $y$. Therefore, $f(x,y)$ must be a function of $x$ only, i.e., $f(x,y) = g(x)$.\n\n**Constructing and Verifying the Function**\n\nThe simplest non-trivial smooth function of $x$ is $g(x)=x$. Let us propose the function:\n$$\nf(x,y) = x\n$$\nWe must verify this function satisfies both conditions from first principles.\n\n**Verification of Condition 1 (y-stratification):**\nWe require no variance reduction. The reduction is $\\frac{1}{n}\\text{Var}(\\mathbb{E}[f(X,Y)|W_y])$. Let's calculate the conditional expectation:\n$$\n\\mathbb{E}[f(X,Y)|W_y=j] = \\mathbb{E}[X|Y \\in [\\frac{j-1}{L}, \\frac{j}{L}]]\n$$\nSince $X$ and $Y$ are independent, the condition on $Y$ does not affect the expectation of $X$.\n$$\n\\mathbb{E}[X|Y \\in [\\frac{j-1}{L}, \\frac{j}{L}]] = \\mathbb{E}[X] = \\int_0^1 x dx = \\frac{1}{2}\n$$\nThe conditional expectation is $1/2$ for all strata $j$. Since it is a constant, its variance is zero:\n$$\n\\text{Var}(\\mathbb{E}[f(X,Y)|W_y]) = \\text{Var}\\left(\\frac{1}{2}\\right) = 0\n$$\nThus, stratification along the $y$-axis provides exactly zero variance reduction. Condition 1 is satisfied.\n\n**Verification of Condition 2 (x-stratification):**\nWe require the variance of the x-stratified estimator to be smaller than the plain MC variance by a factor scaling as $1/L^2$.\nThe variance of the plain MC estimator is:\n$$\n\\text{Var}(\\hat{I}_{MC}) = \\frac{1}{n}\\text{Var}(f(X,Y)) = \\frac{1}{n}\\text{Var}(X) = \\frac{1}{n} \\left( \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 \\right) = \\frac{1}{n} \\left( \\frac{1}{3} - \\left(\\frac{1}{2}\\right)^2 \\right) = \\frac{1}{12n}\n$$\nThe variance of the x-stratified estimator is $\\text{Var}(\\hat{I}_{S,x}) = \\frac{1}{nL} \\sum_{j=1}^L \\text{Var}_{S_{x,j}}(f(X,Y))$.\nThe variance within stratum $S_{x,j} = [\\frac{j-1}{L}, \\frac{j}{L}] \\times [0,1]$ is:\n$$\n\\text{Var}_{S_{x,j}}(f(X,Y)) = \\text{Var}(X | X \\in [\\frac{j-1}{L}, \\frac{j}{L}])\n$$\nThis is the variance of a uniform random variable on an interval of width $h=1/L$. The variance of $U[a,b]$ is $(b-a)^2/12$.\n$$\n\\text{Var}(X | X \\in [\\frac{j-1}{L}, \\frac{j}{L}]) = \\frac{1}{12} \\left( \\frac{j}{L} - \\frac{j-1}{L} \\right)^2 = \\frac{1}{12L^2}\n$$\nThis within-stratum variance is constant for all strata $j$. The variance of the stratified estimator is:\n$$\n\\text{Var}(\\hat{I}_{S,x}) = \\frac{1}{nL} \\sum_{j=1}^L \\left(\\frac{1}{12L^2}\\right) = \\frac{1}{nL} \\cdot L \\cdot \\frac{1}{12L^2} = \\frac{1}{12nL^2}\n$$\nNow, compare the variances:\n$$\n\\frac{\\text{Var}(\\hat{I}_{S,x})}{\\text{Var}(\\hat{I}_{MC})} = \\frac{1/(12nL^2)}{1/(12n)} = \\frac{1}{L^2}\n$$\nThe variance is reduced by a factor of precisely $1/L^2$, which scales as $1/L^2$. Thus, condition 2 is strictly satisfied.\n\nThe function $f(x,y)=x$ fulfills all the requirements of the problem.", "answer": "$$\\boxed{x}$$", "id": "3285833"}]}