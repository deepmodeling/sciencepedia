{"hands_on_practices": [{"introduction": "Mastering any complex algorithm begins with understanding its fundamental building blocks. This first practice focuses on the core mechanics of a single Arnoldi step [@problem_id:2154392]. By manually computing the first basis vector $v_2$ and the first entry $h_{1,1}$ of the Hessenberg matrix, you will solidify your understanding of the normalization and orthogonalization process that lies at the heart of the iteration.", "problem": "In numerical linear algebra, the Arnoldi iteration is an algorithm for building an orthonormal basis of the Krylov subspace generated by a matrix $A$ and a vector $b$. Consider the square matrix $A$ and the initial vector $b$ given by:\n$$ A = \\begin{pmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\end{pmatrix} $$\nThe first step in the Arnoldi process is to normalize the initial vector to obtain the first basis vector, $v_1 = b / \\|b\\|_2$, where $\\|\\cdot\\|_2$ denotes the standard Euclidean norm. The iteration then proceeds to generate subsequent vectors and the entries of an upper Hessenberg matrix $H$.\n\nPerform one complete step of the Arnoldi iteration to compute the Hessenberg matrix entry $h_{1,1}$ and the second Arnoldi basis vector $v_2$.\n\nYour final answer should be given as a single row matrix containing four exact, symbolic values in the following order: the value of $h_{1,1}$, followed by the first, second, and third components of the vector $v_2$.", "solution": "We apply one step of the Arnoldi iteration.\n\nFirst, normalize $b$ to obtain $v_{1}$. The Euclidean norm is\n$$\n\\|b\\|_{2}=\\sqrt{1^{2}+2^{2}+2^{2}}=\\sqrt{9}=3,\n$$\nso\n$$\nv_{1}=\\frac{b}{\\|b\\|_{2}}=\\begin{pmatrix}\\frac{1}{3} \\\\ \\frac{2}{3} \\\\ \\frac{2}{3}\\end{pmatrix}.\n$$\nCompute $w=A v_{1}$:\n$$\nw=\\begin{pmatrix}1 & 1 & 0\\\\ 1 & 1 & 0\\\\ 0 & 0 & 1\\end{pmatrix}\\begin{pmatrix}\\frac{1}{3} \\\\ \\frac{2}{3} \\\\ \\frac{2}{3}\\end{pmatrix}=\\begin{pmatrix}1 \\\\ 1 \\\\ \\frac{2}{3}\\end{pmatrix}.\n$$\nThe Hessenberg entry is\n$$\nh_{1,1}=v_{1}^{\\top}w=\\frac{1}{3}\\cdot 1+\\frac{2}{3}\\cdot 1+\\frac{2}{3}\\cdot\\frac{2}{3}= \\frac{13}{9}.\n$$\nOrthogonalize and normalize to get $v_{2}$. Define\n$$\nr=w-h_{1,1}v_{1}=\\begin{pmatrix}1 \\\\ 1 \\\\ \\frac{2}{3}\\end{pmatrix}-\\frac{13}{9}\\begin{pmatrix}\\frac{1}{3} \\\\ \\frac{2}{3} \\\\ \\frac{2}{3}\\end{pmatrix}=\\begin{pmatrix}\\frac{14}{27} \\\\ \\frac{1}{27} \\\\ -\\frac{8}{27}\\end{pmatrix}.\n$$\nIts norm is\n$$\n\\|r\\|_{2}=\\sqrt{\\left(\\frac{14}{27}\\right)^{2}+\\left(\\frac{1}{27}\\right)^{2}+\\left(-\\frac{8}{27}\\right)^{2}}=\\frac{1}{27}\\sqrt{261}=\\frac{\\sqrt{29}}{9}.\n$$\nThus\n$$\nv_{2}=\\frac{r}{\\|r\\|_{2}}=\\frac{9}{\\sqrt{29}}\\begin{pmatrix}\\frac{14}{27} \\\\ \\frac{1}{27} \\\\ -\\frac{8}{27}\\end{pmatrix}=\\begin{pmatrix}\\frac{14}{3\\sqrt{29}} \\\\ \\frac{1}{3\\sqrt{29}} \\\\ -\\frac{8}{3\\sqrt{29}}\\end{pmatrix}.\n$$\nTherefore, $h_{1,1}=\\frac{13}{9}$ and $v_{2}=\\left(\\frac{14}{3\\sqrt{29}},\\,\\frac{1}{3\\sqrt{29}},\\,-\\frac{8}{3\\sqrt{29}}\\right)^{\\top}$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{13}{9} & \\frac{14}{3\\sqrt{29}} & \\frac{1}{3\\sqrt{29}} & -\\frac{8}{3\\sqrt{29}}\\end{pmatrix}}$$", "id": "2154392"}, {"introduction": "What happens when the Arnoldi iteration stops unexpectedly early? This scenario, known as a 'breakdown,' is not an error but a valuable insight into the structure of the matrix and the Krylov subspace. This exercise [@problem_id:2154381] challenges you to think critically about the conditions for a breakdown, revealing a fundamental connection between the Arnoldi process and the concept of eigenvectors.", "problem": "The Arnoldi iteration is a numerical algorithm for approximating eigenvalues. For a given square matrix $A$ and a starting vector $v_1$, the first step involves normalizing the vector to $q_1 = v_1 / \\|v_1\\|_2$, computing $w = A q_1$, and then calculating a residual vector $r_1 = w - (q_1^T w) q_1$. The algorithm terminates after this first step if the residual vector $r_1$ is the zero vector.\n\nConsider a $3 \\times 3$ matrix $A$ with elements defined as $A_{11}=1$, $A_{12}=0$, $A_{13}=1$, $A_{21}=1$, $A_{22}=1$, $A_{23}=1$, $A_{31}=1$, $A_{32}=1$, and $A_{33}=-1$. Which of the following column vectors, when chosen as the starting vector $v_1$, will cause the Arnoldi iteration to terminate after just one step?\n\nA. A vector with components (1, 1, 1).\n\nB. A vector with components (1, 2, 1).\n\nC. A vector with components (1, -1, 0).\n\nD. A vector with components (0, 1, 1).\n\nE. A vector with components (1, 0, -1).", "solution": "The problem asks to identify which starting vector $v_1$ causes the Arnoldi iteration to terminate after the first step.\nAccording to the problem description, the first step of the Arnoldi iteration terminates if the residual vector $r_1$ is the zero vector.\nThe residual vector is defined as $r_1 = w - h_{11} q_1$, where $q_1 = v_1 / \\|v_1\\|_2$, $w = A q_1$, and $h_{11} = q_1^T w = q_1^T A q_1$.\n\nSetting $r_1 = 0$, we get:\n$$\nw - h_{11} q_1 = 0 \\implies w = h_{11} q_1\n$$\nSubstituting the expression for $w$:\n$$\nA q_1 = h_{11} q_1\n$$\nThis is the definition of an eigenvector and eigenvalue. The equation shows that the vector $q_1$ must be an eigenvector of the matrix $A$, and the scalar $h_{11}$ must be the corresponding eigenvalue.\n\nSince $q_1$ is just a normalized version of the starting vector $v_1$ (i.e., $q_1$ is $v_1$ scaled by $1/\\|v_1\\|_2$), if $q_1$ is an eigenvector, then $v_1$ must also be an eigenvector of $A$. Specifically, if $A q_1 = \\lambda q_1$, then:\n$$\nA \\left(\\frac{v_1}{\\|v_1\\|_2}\\right) = \\lambda \\left(\\frac{v_1}{\\|v_1\\|_2}\\right)\n$$\nMultiplying both sides by the scalar $\\|v_1\\|_2$ gives:\n$$\nA v_1 = \\lambda v_1\n$$\nThus, the problem reduces to finding which of the given vectors is an eigenvector of the matrix $A$.\n\nThe matrix $A$ is given by its components:\n$$\nA = \\begin{pmatrix} 1 & 0 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & -1 \\end{pmatrix}\n$$\nWe now test each option by multiplying the matrix $A$ by the vector $v_1$ from that option and checking if the resulting vector is a scalar multiple of the original vector.\n\nA. For $v_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$:\n$$\nA v_1 = \\begin{pmatrix} 1 & 0 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1(1) + 0(1) + 1(1) \\\\ 1(1) + 1(1) + 1(1) \\\\ 1(1) + 1(1) - 1(1) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 3 \\\\ 1 \\end{pmatrix}\n$$\nIs $\\begin{pmatrix} 2 \\\\ 3 \\\\ 1 \\end{pmatrix} = \\lambda \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$ for some scalar $\\lambda$? No, because $2/1 \\neq 3/1$.\n\nB. For $v_1 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}$:\n$$\nA v_1 = \\begin{pmatrix} 1 & 0 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1(1) + 0(2) + 1(1) \\\\ 1(1) + 1(2) + 1(1) \\\\ 1(1) + 1(2) - 1(1) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 2 \\end{pmatrix}\n$$\nIs $\\begin{pmatrix} 2 \\\\ 4 \\\\ 2 \\end{pmatrix} = \\lambda \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}$ for some scalar $\\lambda$? Yes, if we let $\\lambda=2$, we have $2 \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 2 \\end{pmatrix}$. This vector is an eigenvector of $A$.\n\nC. For $v_1 = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}$:\n$$\nA v_1 = \\begin{pmatrix} 1 & 0 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1(1) + 0(-1) + 1(0) \\\\ 1(1) + 1(-1) + 1(0) \\\\ 1(1) + 1(-1) - 1(0) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nIs $\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\lambda \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}$ for some scalar $\\lambda$? No. For example, the second component is $0 = \\lambda(-1)$, which means $\\lambda=0$, but the first component is $1 = \\lambda(1)$, which would mean $\\lambda=1$. This is a contradiction.\n\nD. For $v_1 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}$:\n$$\nA v_1 = \\begin{pmatrix} 1 & 0 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & -1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1(0) + 0(1) + 1(1) \\\\ 1(0) + 1(1) + 1(1) \\\\ 1(0) + 1(1) - 1(1) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix}\n$$\nIs $\\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix} = \\lambda \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}$ for some scalar $\\lambda$? No. The first component requires $1 = \\lambda(0)$, which is impossible.\n\nE. For $v_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}$:\n$$\nA v_1 = \\begin{pmatrix} 1 & 0 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1(1) + 0(0) + 1(-1) \\\\ 1(1) + 1(0) + 1(-1) \\\\ 1(1) + 1(0) - 1(-1) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 2 \\end{pmatrix}\n$$\nIs $\\begin{pmatrix} 0 \\\\ 0 \\\\ 2 \\end{pmatrix} = \\lambda \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}$ for some scalar $\\lambda$? No. The first component requires $0 = \\lambda(1)$, which means $\\lambda=0$. But the third component requires $2 = \\lambda(-1)$, which would mean $\\lambda=-2$. This is a contradiction.\n\nOnly the vector in option B is an eigenvector of $A$. Therefore, choosing this vector as the starting vector $v_1$ will cause the Arnoldi iteration to terminate after the first step.", "answer": "$$\\boxed{B}$$", "id": "2154381"}, {"introduction": "An algorithm that is perfect in theory can fail in practice due to the limitations of finite-precision computer arithmetic. This coding exercise [@problem_id:3206345] moves from pen-and-paper to the computer, asking you to implement the Arnoldi iteration and compare two different orthogonalization schemes: Classical Gram-Schmidt (CGS) and Modified Gram-Schmidt (MGS). By observing the loss of orthogonality on a specially chosen matrix, you will gain practical insight into the critical importance of numerical stability in scientific computing.", "problem": "You are asked to implement the Arnoldi iteration using two orthogonalization strategies: classical Gram-Schmidt (CGS) and modified Gram-Schmidt (MGS). Starting from the fundamental definition of a Krylov subspace for a given square matrix, the Arnoldi process constructs an orthonormal basis and a corresponding upper Hessenberg matrix. Your goal is to implement both variants, quantify the loss of orthogonality in finite precision arithmetic, and identify a matrix for which CGS fails due to loss of orthogonality while MGS succeeds.\n\nBegin from the following fundamental bases:\n- For a given matrix $A \\in \\mathbb{R}^{n \\times n}$ and a nonzero vector $b \\in \\mathbb{R}^{n}$, the $k$-th Krylov subspace is $K_k(A,b) = \\text{span}\\{b, Ab, A^2 b, \\dots, A^{k-1} b\\}$.\n- The Arnoldi iteration constructs an orthonormal basis $\\{q_1,\\dots,q_k\\}$ of $K_k(A,b)$ and an upper Hessenberg matrix $H_{k+1,k}$ such that $A Q_k = Q_{k+1} H_{k+1,k}$, where $Q_k = [q_1,\\dots,q_k]$ and $Q_{k+1} = [q_1,\\dots,q_{k+1}]$. The orthogonality of $Q_k$ is crucial for numerical stability and accuracy.\n\nTasks to implement and verify:\n1. Implement two Arnoldi procedures:\n   - Classical Gram-Schmidt (CGS): at each Arnoldi step $j$, compute $v = A q_j$, then compute the vector of coefficients $h = Q_j^\\top v$ in one matrix-vector step, and update $v \\leftarrow v - Q_j h$. Finally compute $h_{j+1,j} = \\|v\\|_2$ and, if nonzero, set $q_{j+1} = v / h_{j+1,j}$. Detect breakdown when $h_{j+1,j} = 0$ and stop.\n   - Modified Gram-Schmidt (MGS): at each Arnoldi step $j$, compute $v = A q_j$, then for $i = 1,\\dots,j$ sequentially compute $h_{i,j} = q_i^\\top v$ and update $v \\leftarrow v - h_{i,j} q_i$. Finally compute $h_{j+1,j} = \\|v\\|_2$ and proceed as above, with breakdown detection.\n2. For each run, compute the orthogonality error\n   $$E_k = \\left\\| Q_k^\\top Q_k - I_k \\right\\|_F,$$\n   where $I_k$ is the identity matrix of size $k$ and $\\|\\cdot\\|_F$ denotes the Frobenius norm. Report $E_k$ for CGS and MGS as floating-point numbers.\n3. In the event of breakdown, report the effective Krylov dimension produced, denoted $k_{\\text{eff}}$, which equals the number of columns actually constructed in $Q_k$ up to breakdown (inclusive of the initial vector $q_1$).\n\nYour program must run the following test suite. For each test case, output the four values $(E_k^{\\text{CGS}}, E_k^{\\text{MGS}}, k_{\\text{eff}}^{\\text{CGS}}, k_{\\text{eff}}^{\\text{MGS}})$ in this order. Aggregate all test cases into a single flat list in the final output as specified below.\n\nTest Suite:\n- Test case $1$ (well-conditioned, happy path):\n  - Matrix $A \\in \\mathbb{R}^{n \\times n}$ is the tridiagonal Toeplitz matrix with $A_{i,i} = -2$, $A_{i,i+1} = 1$, $A_{i+1,i} = 1$ for $i = 1,\\dots,n-1$, and zeros elsewhere. Use $n = 40$.\n  - Starting vector $b \\in \\mathbb{R}^{n}$ defined componentwise by $b_i = i$ for $i = 1,\\dots,n$, then normalized to unit $2$-norm.\n  - Arnoldi steps $k = 20$.\n- Test case $2$ (non-normal Jordan block, CGS expected to lose orthogonality, MGS to succeed):\n  - Matrix $A \\in \\mathbb{R}^{n \\times n}$ is the Jordan block with ones on the diagonal and ones on the superdiagonal: $A_{i,i} = 1$, $A_{i,i+1} = 1$ for $i = 1,\\dots,n-1$, and zeros elsewhere. Use $n = 60$.\n  - Starting vector $b = e_1 \\in \\mathbb{R}^{n}$, the first canonical basis vector.\n  - Arnoldi steps $k = 40$.\n- Test case $3$ (breakdown boundary case):\n  - Matrix $A = 2 I \\in \\mathbb{R}^{n \\times n}$ with $n = 20$ and $I$ the identity.\n  - Starting vector $b \\in \\mathbb{R}^{n}$ with $b_i = 1$ for $i = 1,\\dots,n$, then normalized to unit $2$-norm.\n  - Arnoldi steps $k = 5$.\n  - In exact arithmetic, breakdown occurs after the first step because $A q_1$ is colinear with $q_1$; your implementation must detect this and report the effective dimensions accordingly.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order, the concatenation over the test cases of:\n  $[E_k^{\\text{CGS}}, E_k^{\\text{MGS}}, k_{\\text{eff}}^{\\text{CGS}}, k_{\\text{eff}}^{\\text{MGS}}]$.\n- For the three specified test cases, the final output must therefore contain $12$ entries: $[E_1^{\\text{CGS}}, E_1^{\\text{MGS}}, k_{1,\\text{eff}}^{\\text{CGS}}, k_{1,\\text{eff}}^{\\text{MGS}}, E_2^{\\text{CGS}}, E_2^{\\text{MGS}}, k_{2,\\text{eff}}^{\\text{CGS}}, k_{2,\\text{eff}}^{\\text{MGS}}, E_3^{\\text{CGS}}, E_3^{\\text{MGS}}, k_{3,\\text{eff}}^{\\text{CGS}}, k_{3,\\text{eff}}^{\\text{MGS}}]$.\n- All angles, if any appear, must be in radians. There are no physical units in this problem. All reported numbers must be real numbers or integers. No percentage signs are allowed; use decimals for any fractional values.", "solution": "The present task requires the implementation and comparison of two variants of the Arnoldi iteration algorithm for constructing an orthonormal basis for a Krylov subspace. The two variants differ in their choice of orthogonalization procedure: the Classical Gram-Schmidt (CGS) process and the Modified Gram-Schmidt (MGS) process. The objective is to analyze their numerical stability by quantifying the loss of orthogonality for specific test cases, including one designed to highlight the superior stability of MGS in finite-precision arithmetic.\n\n### The Arnoldi Iteration and Krylov Subspaces\n\nGiven a square matrix $A \\in \\mathbb{R}^{n \\times n}$ and a non-zero starting vector $b \\in \\mathbb{R}^{n}$, the $k$-th Krylov subspace, denoted $K_k(A, b)$, is the vector space spanned by the first $k$ vectors in the sequence of matrix-vector products:\n$$\nK_k(A, b) = \\text{span}\\{b, Ab, A^2b, \\dots, A^{k-1}b\\}\n$$\nThe Arnoldi iteration is an algorithm that generates an orthonormal basis $\\{q_1, q_2, \\dots, q_k\\}$ for this subspace. The process is iterative. Assuming we have constructed an orthonormal set $\\{q_1, \\dots, q_j\\}$, the next vector $q_{j+1}$ is found by taking the vector $A q_j$, making it orthogonal to all preceding basis vectors $q_1, \\dots, q_j$, and then normalizing it.\n\nThis procedure constructs a matrix $Q_k = [q_1, q_2, \\dots, q_k]$ whose columns form the orthonormal basis. It also generates an upper Hessenberg matrix $H_{k+1,k} \\in \\mathbb{R}^{(k+1) \\times k}$ that satisfies the fundamental Arnoldi relation:\n$$\nA Q_k = Q_{k+1} H_{k+1,k}\n$$\nHere, $Q_{k+1} = [Q_k, q_{k+1}]$, and the entries of $H_{k+1,k}$, denoted $h_{i,j}$, are the coefficients from the orthogonalization steps.\n\nThe core of the algorithm lies in the orthogonalization step. At step $j$ (for $j=1, \\dots, k$), we compute $v = A q_j$ and then orthogonalize $v$ against the existing basis $\\{q_1, \\dots, q_j\\}$.\n\n### Orthogonalization Methods: CGS vs. MGS\n\n**1. Classical Gram-Schmidt (CGS) based Arnoldi:**\nIn the CGS variant, the vector $v = A q_j$ is orthogonalized against all previous basis vectors $\\{q_1, \\dots, q_j\\}$ simultaneously. The projection coefficients $h_{i,j}$ are computed first, and then their combined projection is subtracted from $v$ in a single step.\n\nThe procedure at step $j$ is:\n1.  Compute the new candidate vector: $v = A q_j$.\n2.  Compute all projection coefficients: $h_{i,j} = q_i^\\top v$ for $i=1, \\dots, j$. In matrix form, this is $h_j = Q_j^\\top v$, where $h_j$ is the $j$-th column of $H_{k,k}$.\n3.  Compute the orthogonal vector: $w_j = v - \\sum_{i=1}^j h_{i,j} q_i$. In matrix form, $w_j = v - Q_j h_j$.\n4.  Compute the normalization factor: $h_{j+1,j} = \\|w_j\\|_2$.\n5.  Normalize to get the next basis vector: $q_{j+1} = w_j / h_{j+1,j}$.\n\nWhile mathematically sound, CGS is known to be numerically unstable. Rounding errors in the computation of $h_{i,j}$ and the subtraction can lead to a computed $w_j$ that is not fully orthogonal to the basis $\\{q_1, \\dots, q_j\\}$, resulting in a rapid loss of orthogonality in the matrix $Q_k$.\n\n**2. Modified Gram-Schmidt (MGS) based Arnoldi:**\nThe MGS variant addresses the instability of CGS by reordering the orthogonalization operations. Instead of subtracting a sum of projections, it subtracts each projection sequentially.\n\nThe procedure at step $j$ is:\n1.  Initialize the vector to be orthogonalized: $v^{(0)} = A q_j$.\n2.  Sequentially remove components along $q_i$: For $i = 1, \\dots, j$:\n    a. Compute projection coefficient: $h_{i,j} = q_i^\\top v^{(i-1)}$.\n    b. Update the vector: $v^{(i)} = v^{(i-1)} - h_{i,j} q_i$.\n3.  The final orthogonal vector is $w_j = v^{(j)}$.\n4.  Compute the normalization factor: $h_{j+1,j} = \\|w_j\\|_2$.\n5.  Normalize to get the next basis vector: $q_{j+1} = w_j / h_{j+1,j}$.\n\nThis sequential process is numerically more robust. By updating the vector $v$ at each sub-step, MGS reduces the components along previous basis vectors more effectively, mitigating the accumulation of rounding errors.\n\n### Breakdown and Error Measurement\n\n**Breakdown Condition:**\nThe Arnoldi iteration can terminate prematurely if, at step $j$, the vector $w_j$ becomes zero (or numerically, its norm $h_{j+1,j}$ is smaller than a given tolerance). This event is known as a breakdown. It occurs when the vector $A q_j$ is already contained within the subspace $K_j(A, b) = \\text{span}\\{q_1, \\dots, q_j\\}$. This implies that the Krylov subspace is an invariant subspace of $A$. In this case, the process stops, and the effective dimension of the constructed subspace is $k_{\\text{eff}} = j$.\n\n**Orthogonality Error:**\nTo quantify the numerical stability of CGS and MGS, we measure the loss of orthogonality of the generated basis matrix $Q_k$. A perfectly orthonormal matrix $Q_k$ satisfies $Q_k^\\top Q_k = I_k$, where $I_k$ is the $k \\times k$ identity matrix. Any deviation indicates a loss of orthogonality. We use the Frobenius norm of the difference to measure this error:\n$$\nE_k = \\| Q_k^\\top Q_k - I_k \\|_F\n$$\nA smaller value of $E_k$ signifies better numerical performance. If a breakdown occurs at dimension $k_{\\text{eff}}$, the error is computed for the generated matrix $Q_{k_{\\text{eff}}}$.\n\nThe provided test cases are designed to verify the implementation and demonstrate these numerical properties. Test case 1 uses a well-behaved symmetric matrix where both methods should perform well. Test case 2 uses a non-normal Jordan block, which is a classic example where CGS is expected to perform poorly compared to MGS. Test case 3 is a simple case designed to test the algorithm's handling of an early breakdown.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef arnoldi_iteration(A, b, k_max, method='mgs'):\n    \"\"\"\n    Performs Arnoldi iteration to find an orthonormal basis of the Krylov subspace.\n\n    Args:\n        A (np.ndarray): The square matrix of size n x n.\n        b (np.ndarray): The starting vector of size n.\n        k_max (int): The number of Arnoldi steps to perform.\n        method (str): The orthogonalization method, 'cgs' or 'mgs'.\n\n    Returns:\n        tuple: A tuple containing:\n            - Q_k_eff (np.ndarray): Matrix with orthonormal columns spanning the\n              Krylov subspace of effective dimension k_eff.\n            - H (np.ndarray): The upper Hessenberg matrix.\n            - k_eff (int): The effective dimension of the Krylov subspace.\n    \"\"\"\n    n = A.shape[0]\n    # Use float64 for higher precision, crucial for observing stability differences\n    Q = np.zeros((n, k_max + 1), dtype=np.float64)\n    H = np.zeros((k_max + 1, k_max), dtype=np.float64)\n\n    # Normalize the starting vector b\n    b_norm = np.linalg.norm(b)\n    if b_norm == 0:\n        # If b is a zero vector, the subspace is trivial.\n        return Q[:, :1], H[:1, :0], 0\n    Q[:, 0] = b / b_norm\n\n    k_eff = k_max\n    # A small tolerance to detect numerical breakdown\n    breakdown_tol = 1e-12\n\n    for j in range(k_max):\n        # v = A * q_j\n        v = A @ Q[:, j]\n\n        if method == 'cgs':\n            # Classical Gram-Schmidt\n            # h = Q_j^T * v\n            h_col = Q[:, :j + 1].T @ v\n            H[:j + 1, j] = h_col\n            # v_ortho = v - Q_j * h\n            v_ortho = v - Q[:, :j + 1] @ h_col\n        elif method == 'mgs':\n            # Modified Gram-Schmidt\n            v_ortho = v.copy()\n            for i in range(j + 1):\n                # h_ij = q_i^T * v_ortho\n                h_ij = Q[:, i].T @ v_ortho\n                H[i, j] = h_ij\n                # v_ortho = v_ortho - h_ij * q_i\n                v_ortho -= h_ij * Q[:, i]\n        else:\n            raise ValueError(\"Method must be 'cgs' or 'mgs'\")\n\n        h_next = np.linalg.norm(v_ortho)\n        H[j + 1, j] = h_next\n\n        if h_next  breakdown_tol:\n            # Breakdown occurred, the subspace is invariant\n            k_eff = j + 1\n            break\n\n        # q_{j+1} = v_ortho / h_{j+1, j}\n        Q[:, j + 1] = v_ortho / h_next\n\n    # Truncate Q to the effective dimension\n    Q_k_eff = Q[:, :k_eff]\n\n    return Q_k_eff, H, k_eff\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases\n    test_cases = []\n\n    # Test case 1 (well-conditioned)\n    n1 = 40\n    k1 = 20\n    A1 = np.diag(-2 * np.ones(n1)) + np.diag(np.ones(n1 - 1), 1) + np.diag(np.ones(n1 - 1), -1)\n    b1 = np.arange(1, n1 + 1, dtype=np.float64)\n    b1 /= np.linalg.norm(b1)\n    test_cases.append({'A': A1, 'b': b1, 'k': k1})\n\n    # Test case 2 (non-normal Jordan block)\n    n2 = 60\n    k2 = 40\n    A2 = np.eye(n2) + np.diag(np.ones(n2 - 1), 1)\n    b2 = np.zeros(n2, dtype=np.float64)\n    b2[0] = 1.0\n    test_cases.append({'A': A2, 'b': b2, 'k': k2})\n    \n    # Test case 3 (breakdown)\n    n3 = 20\n    k3 = 5\n    A3 = 2 * np.eye(n3)\n    b3 = np.ones(n3, dtype=np.float64)\n    b3 /= np.linalg.norm(b3)\n    test_cases.append({'A': A3, 'b': b3, 'k': k3})\n\n    results = []\n    for case in test_cases:\n        A, b, k = case['A'], case['b'], case['k']\n        \n        # Run CGS\n        Q_cgs, _, k_eff_cgs = arnoldi_iteration(A, b, k, method='cgs')\n        I_cgs = np.eye(k_eff_cgs)\n        error_cgs = np.linalg.norm(Q_cgs.T @ Q_cgs - I_cgs, 'fro')\n        \n        # Run MGS\n        Q_mgs, _, k_eff_mgs = arnoldi_iteration(A, b, k, method='mgs')\n        I_mgs = np.eye(k_eff_mgs)\n        error_mgs = np.linalg.norm(Q_mgs.T @ Q_mgs - I_mgs, 'fro')\n\n        results.extend([error_cgs, error_mgs, k_eff_cgs, k_eff_mgs])\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3206345"}]}