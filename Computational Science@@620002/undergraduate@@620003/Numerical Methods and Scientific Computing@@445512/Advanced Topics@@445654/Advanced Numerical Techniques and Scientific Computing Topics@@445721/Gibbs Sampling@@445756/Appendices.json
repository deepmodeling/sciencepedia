{"hands_on_practices": [{"introduction": "To truly understand an algorithm, we must walk through its steps. This first practice exercise takes you through a single, complete iteration of a Gibbs sampler. By using the inverse transform sampling method to generate samples from given conditional distributions, you'll gain a concrete understanding of the mechanics that drive this powerful simulation tool. [@problem_id:1920320]", "problem": "Consider a two-dimensional random vector $(X, Y)$ whose joint probability distribution is defined by the following full conditional distributions:\n- The conditional distribution of $X$ given $Y=y$ is an Exponential distribution with a rate parameter of $y$. The probability density function is given by $p(x|y) = y \\exp(-yx)$ for $x > 0$.\n- The conditional distribution of $Y$ given $X=x$ is a Poisson distribution with a mean parameter of $x$. The probability mass function is given by $p(y=k|x) = \\frac{x^k \\exp(-x)}{k!}$ for $k \\in \\{0, 1, 2, \\dots\\}$.\n\nYou are tasked with performing one full iteration of a Gibbs sampler. Starting from the initial state $(x^{(0)}, y^{(0)}) = (2, 3)$, you will generate a new state $(x^{(1)}, y^{(1)})$. The procedure for the iteration is as follows: first, draw a sample for $x^{(1)}$ from the distribution $p(x|y^{(0)})$, and then, using this new value $x^{(1)}$, draw a sample for $y^{(1)}$ from the distribution $p(y|x^{(1)})$.\n\nTo generate the necessary random variates, you must use the inverse transform sampling method. Use the following random numbers, which are drawn from a Uniform(0,1) distribution:\n- For generating $x^{(1)}$, use the uniform random number $u_x = 0.600$.\n- For generating $y^{(1)}$, use the uniform random number $u_y = 0.750$.\n\nWhat are the numerical values for the new state $(x^{(1)}, y^{(1)})$? The value for $x^{(1)}$ must be rounded to four significant figures.", "solution": "We perform one Gibbs update using inverse transform sampling.\n\n1) Sample $x^{(1)}$ from $p(x \\mid y^{(0)}=3)$.\nFor an Exponential rate $y$, the conditional CDF is\n$$\nF(x \\mid y)=1-\\exp(-yx), \\quad x>0.\n$$\nInverse transform uses $u_{x}=F(x \\mid y)$, hence\n$$\nx^{(1)}=F^{-1}(u_{x})=-\\frac{1}{y^{(0)}}\\ln\\!\\bigl(1-u_{x}\\bigr).\n$$\nWith $y^{(0)}=3$ and $u_{x}=0.600$,\n$$\nx^{(1)}=-\\frac{1}{3}\\ln(1-0.600)=-\\frac{1}{3}\\ln(0.4)=\\frac{1}{3}\\ln(2.5)\\approx 0.3054302439.\n$$\nRounded to four significant figures: $x^{(1)}=0.3054$.\n\n2) Sample $y^{(1)}$ from $p(y \\mid x^{(1)})$ using $u_{y}=0.750$.\nFor a Poisson mean $x$, the pmf is\n$$\np(y=k \\mid x)=\\frac{x^{k}\\exp(-x)}{k!}, \\quad k\\in\\{0,1,2,\\dots\\}.\n$$\nInverse transform for a discrete distribution selects the smallest $k$ such that $F(k \\mid x)=\\sum_{j=0}^{k}p(j \\mid x)\\ge u_{y}$.\n\nWith $x=x^{(1)}=\\frac{1}{3}\\ln(2.5)$, compute\n$$\np(0 \\mid x)=\\exp(-x)=\\exp\\!\\Bigl(-\\tfrac{1}{3}\\ln(2.5)\\Bigr)=2.5^{-1/3}\\approx 0.7368.\n$$\nSince $p(0 \\mid x)=0.7368<0.750$, continue to $k=1$:\n$$\np(1 \\mid x)=x\\exp(-x)=x\\,2.5^{-1/3}\\approx 0.30543\\times 0.7368\\approx 0.2250.\n$$\nThen\n$$\nF(1 \\mid x)=p(0 \\mid x)+p(1 \\mid x)\\approx 0.7368+0.2250=0.9618>0.750,\n$$\nso the smallest $k$ with $F(k \\mid x)\\ge 0.750$ is $k=1$. Therefore $y^{(1)}=1$.\n\nThus, the new state is $(x^{(1)},y^{(1)})=(0.3054,1)$ with $x^{(1)}$ rounded to four significant figures.", "answer": "$$\\boxed{\\begin{pmatrix}0.3054 & 1\\end{pmatrix}}$$", "id": "1920320"}, {"introduction": "An algorithm can be correct yet inefficient. This exercise explores a critical challenge in Gibbs sampling: slow convergence when variables are highly correlated. By simulating a sampler on a hypothetical bivariate normal distribution with strong correlation, you will observe how the sampler makes only incremental progress, a phenomenon known as slow mixing. To isolate this effect, this problem uses a thought experiment where each 'random' draw is replaced by the mean of its conditional distribution, providing a clear, deterministic view of this common MCMC pitfall. [@problem_id:1363745]", "problem": "In a high-precision manufacturing process for optical components, two geometric parameters of a lens, $x_1$ and $x_2$, are critical for its performance. These parameters represent normalized deviations from the ideal design specifications. Due to the physics of the fabrication process, these parameters are not independent. An analysis of production data reveals that their joint probability distribution can be modeled by an unnormalized density function $f(x_1, x_2)$ given by:\n$$f(x_1, x_2) \\propto \\exp \\left( -\\frac{1}{2(1-\\rho^2)} (x_1^2 - 2\\rho x_1 x_2 + x_2^2) \\right)$$\nFor this specific process, the correlation coefficient is found to be $\\rho = 0.99$. The mode of the distribution is at $(0, 0)$, which corresponds to a perfect component.\n\nTo simulate the process variations, you are asked to use a Gibbs sampler. You start from an initial state $(x_1^{(0)}, x_2^{(0)}) = (-4.0, -4.1)$, which represents a component at the edge of the acceptable quality range.\n\nYour task is to determine the state of the sampler, $(x_1^{(2)}, x_2^{(2)})$, after two full iterations. A full iteration consists of updating $x_1$ first, and then updating $x_2$. To make the calculation deterministic, you must assume that at each sampling step, the new value drawn for a variable is equal to the mean of its conditional distribution.\n\nCalculate the coordinates of the state $(x_1^{(2)}, x_2^{(2)})$. Report both coordinates in your final answer, rounded to four significant figures.", "solution": "We recognize the given unnormalized joint density\n$$\nf(x_{1},x_{2}) \\propto \\exp\\left(-\\frac{1}{2(1-\\rho^{2})}\\left(x_{1}^{2}-2\\rho x_{1}x_{2}+x_{2}^{2}\\right)\\right)\n$$\nas the kernel of a bivariate normal distribution with mean vector $(0,0)$, unit variances, and correlation coefficient $\\rho$. To derive the full conditional distributions, complete the square in $x_{1}$:\n$$\nx_{1}^{2}-2\\rho x_{1}x_{2}+x_{2}^{2}=(x_{1}-\\rho x_{2})^{2}+(1-\\rho^{2})x_{2}^{2}.\n$$\nHence, conditional on $x_{2}$,\n$$\nf(x_{1}\\mid x_{2}) \\propto \\exp\\left(-\\frac{1}{2(1-\\rho^{2})}(x_{1}-\\rho x_{2})^{2}\\right),\n$$\nwhich is the kernel of a normal distribution with\n$$\nx_{1}\\mid x_{2} \\sim \\mathcal{N}\\!\\left(\\rho x_{2},\\,1-\\rho^{2}\\right).\n$$\nBy symmetry, we also have\n$$\nx_{2}\\mid x_{1} \\sim \\mathcal{N}\\!\\left(\\rho x_{1},\\,1-\\rho^{2}\\right).\n$$\n\nThe Gibbs sampler updates $x_{1}$ first using $x_{2}$, then updates $x_{2}$ using the new $x_{1}$. Under the deterministic rule that each draw equals the conditional mean, the updates are\n$$\nx_{1}^{(t+1)}=\\rho\\,x_{2}^{(t)},\\qquad x_{2}^{(t+1)}=\\rho\\,x_{1}^{(t+1)}.\n$$\nCombining these,\n$$\nx_{2}^{(t+1)}=\\rho^{2}\\,x_{2}^{(t)},\\qquad x_{1}^{(t+1)}=\\rho\\,x_{2}^{(t)}.\n$$\nStarting from $(x_{1}^{(0)},x_{2}^{(0)})=(-4.0,-4.1)$ and using $\\rho=0.99$, the first full iteration yields\n$$\nx_{1}^{(1)}=\\rho\\,x_{2}^{(0)}=0.99\\times(-4.1)=-4.059,\\qquad\nx_{2}^{(1)}=\\rho\\,x_{1}^{(1)}=0.99\\times(-4.059)=-4.01841.\n$$\nThe second full iteration then gives\n$$\nx_{1}^{(2)}=\\rho\\,x_{2}^{(1)}=0.99\\times(-4.01841)=-3.9782259,\\qquad\nx_{2}^{(2)}=\\rho\\,x_{1}^{(2)}=0.99\\times(-3.9782259)=-3.938443641.\n$$\nRounding each coordinate to four significant figures:\n$$\nx_{1}^{(2)}\\approx -3.978,\\qquad x_{2}^{(2)}\\approx -3.938.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}-3.978 & -3.938\\end{pmatrix}}$$", "id": "1363745"}, {"introduction": "The slow mixing we observed visually in the previous exercise can be described with mathematical precision. This advanced practice asks you to derive the theoretical autocorrelation of the sample chain produced by a Gibbs sampler for a bivariate normal distribution. By establishing the direct relationship between the target distribution's correlation $\\rho$ and the sampler's autocorrelation, you will formalize your intuition and gain a deeper understanding of how to analyze the performance of MCMC algorithms. [@problem_id:3235784]", "problem": "Consider the bivariate normal target distribution with zero mean and covariance matrix\n$$\\Sigma=\\begin{pmatrix}1 & \\rho\\\\ \\rho & 1\\end{pmatrix},$$\nwhere $|\\rho|<1$. A deterministic-scan Gibbs sampler for this target alternates between drawing $x$ given the current $y$ and drawing $y$ given the current $x$, producing a Markov chain $(x,y)$ with stationary marginal distributions $x\\sim \\mathcal{N}(0,1)$ and $y\\sim \\mathcal{N}(0,1)$. Let the $x$-subchain be the sequence $\\{x_t\\}_{t\\geq 0}$ obtained by recording the value of $x$ immediately after each $x$-update.\n\nStarting from the core definitions of the Gibbs sampler and conditional distributions of the multivariate normal, and from the definition of autocorrelation, derive the closed-form expression for the lag-$k$ autocorrelation of the stationary $x$-subchain, defined by\n$$\\mathrm{Corr}(x_t,x_{t+k})=\\frac{\\mathrm{Cov}(x_t,x_{t+k})}{\\mathrm{Var}(x_t)},$$\nas a function of $\\rho$ and $k\\in\\{0,1,2,\\dots\\}$. Express your final answer as a single analytic expression in terms of $\\rho$ and $k$. No rounding is required.", "solution": "The problem requires the derivation of the lag-$k$ autocorrelation for the stationary $x$-subchain generated by a deterministic-scan Gibbs sampler on a bivariate normal distribution.\n\nFirst, we must establish the conditional distributions required for the Gibbs sampler. The target distribution is a bivariate normal distribution for a random vector $(X, Y)^T$ with mean vector $\\mu = (0, 0)^T$ and covariance matrix:\n$$\n\\Sigma = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}\n$$\nThe marginal variances are $\\mathrm{Var}(X) = \\sigma_x^2 = 1$ and $\\mathrm{Var}(Y) = \\sigma_y^2 = 1$. The covariance is $\\mathrm{Cov}(X, Y) = \\rho \\sigma_x \\sigma_y = \\rho$.\n\nFor a general bivariate normal distribution $(X, Y)^T$, the conditional distribution of $X$ given $Y=y$ is a normal distribution with mean $E[X|Y=y]$ and variance $\\mathrm{Var}(X|Y=y)$ given by:\n$$\nE[X|Y=y] = E[X] + \\frac{\\mathrm{Cov}(X,Y)}{\\mathrm{Var}(Y)}(y - E[Y])\n$$\n$$\n\\mathrm{Var}(X|Y=y) = \\mathrm{Var}(X) \\left(1 - \\frac{\\mathrm{Cov}(X,Y)^2}{\\mathrm{Var}(X)\\mathrm{Var}(Y)}\\right) = \\mathrm{Var}(X)(1-\\rho_{XY}^2)\n$$\nwhere $\\rho_{XY}$ is the correlation coefficient between $X$ and $Y$.\n\nIn this specific problem, $E[X]=0$, $E[Y]=0$, $\\mathrm{Var}(X)=1$, $\\mathrm{Var}(Y)=1$, and $\\mathrm{Cov}(X,Y)=\\rho$. Substituting these values gives:\n$$\nE[X|Y=y] = 0 + \\frac{\\rho}{1}(y - 0) = \\rho y\n$$\n$$\n\\mathrm{Var}(X|Y=y) = 1(1 - \\rho^2) = 1-\\rho^2\n$$\nThus, the conditional distribution of $X$ given $Y=y$ is $X|Y=y \\sim \\mathcal{N}(\\rho y, 1-\\rho^2)$.\nBy symmetry of the problem, the conditional distribution of $Y$ given $X=x$ is $Y|X=x \\sim \\mathcal{N}(\\rho x, 1-\\rho^2)$.\n\nThe Gibbs sampler generates a sequence of states. Let the state of the Markov chain at completion of iteration $t-1$ be $(x_{t-1}, y_{t-1})$. A deterministic-scan Gibbs sampler updates each component in a fixed order. Let's define one full iteration (from $t-1$ to $t$) as updating $y$ first, then $x$.\n1.  Draw a new value for $y$, let's call it $y'_t$, from its conditional distribution given the current value of $x$, which is $x_{t-1}$:\n    $y'_t \\sim p(y | x = x_{t-1})$, i.e., $y'_t \\sim \\mathcal{N}(\\rho x_{t-1}, 1-\\rho^2)$.\n2.  Draw a new value for $x$, which is $x_t$, from its conditional distribution given the newly drawn value of $y$, which is $y'_t$:\n    $x_t \\sim p(x | y = y'_t)$, i.e., $x_t \\sim \\mathcal{N}(\\rho y'_t, 1-\\rho^2)$.\n\nThe problem asks for the autocorrelation of the $x$-subchain $\\{x_t\\}_{t \\ge 0}$. We need to find a relationship between $x_t$ and $x_{t-1}$. We can express the sampling steps as stochastic equations. Let $\\epsilon_t$ and $\\delta_t$ be independent standard normal random variables, i.e., $\\epsilon_t, \\delta_t \\sim \\mathcal{N}(0,1)$, drawn at iteration $t$.\nFrom step 1:\n$$\ny'_t = \\rho x_{t-1} + \\sqrt{1-\\rho^2} \\epsilon_t\n$$\nFrom step 2:\n$$\nx_t = \\rho y'_t + \\sqrt{1-\\rho^2} \\delta_t\n$$\nSubstituting the expression for $y'_t$ into the equation for $x_t$:\n$$\nx_t = \\rho (\\rho x_{t-1} + \\sqrt{1-\\rho^2} \\epsilon_t) + \\sqrt{1-\\rho^2} \\delta_t\n$$\n$$\nx_t = \\rho^2 x_{t-1} + \\rho\\sqrt{1-\\rho^2} \\epsilon_t + \\sqrt{1-\\rho^2} \\delta_t\n$$\nThis equation describes the evolution of the $x$-subchain. It is an autoregressive process of order $1$, or AR($1$). Let's define the noise term $u_t = \\rho\\sqrt{1-\\rho^2} \\epsilon_t + \\sqrt{1-\\rho^2} \\delta_t$. Since $\\epsilon_t$ and $\\delta_t$ are independent of each other and of $x_{t-1}$, the noise term $u_t$ is independent of $x_{t-1}$. The mean of $u_t$ is $E[u_t] = 0$. The variance of $u_t$ is:\n$$\n\\mathrm{Var}(u_t) = \\mathrm{Var}(\\rho\\sqrt{1-\\rho^2} \\epsilon_t) + \\mathrm{Var}(\\sqrt{1-\\rho^2} \\delta_t)\n$$\n$$\n\\mathrm{Var}(u_t) = (\\rho\\sqrt{1-\\rho^2})^2 \\mathrm{Var}(\\epsilon_t) + (1-\\rho^2) \\mathrm{Var}(\\delta_t)\n$$\n$$\n\\mathrm{Var}(u_t) = \\rho^2(1-\\rho^2)(1) + (1-\\rho^2)(1) = (1-\\rho^2)(\\rho^2+1) = 1-\\rho^4\n$$\nSo the $x$-subchain follows the AR($1$) process:\n$$\nx_t = \\rho^2 x_{t-1} + u_t, \\quad \\text{where } u_t \\sim \\mathcal{N}(0, 1-\\rho^4)\n$$\nThe problem considers the stationary $x$-subchain. The stationary marginal distribution for $X$ is $\\mathcal{N}(0,1)$, thus $\\mathrm{Var}(x_t)=1$ for all $t$ in the stationary regime. We can verify this is consistent with the AR($1$) process: $\\mathrm{Var}(x_t) = \\mathrm{Var}(\\rho^2 x_{t-1} + u_t) = (\\rho^2)^2 \\mathrm{Var}(x_{t-1}) + \\mathrm{Var}(u_t) = \\rho^4 \\mathrm{Var}(x_{t-1}) + 1-\\rho^4$. If $\\mathrm{Var}(x_{t-1})=1$, then $\\mathrm{Var}(x_t) = \\rho^4(1) + 1-\\rho^4 = 1$. This confirms the stationary variance.\n\nWe need to find the lag-$k$ autocorrelation, $\\mathrm{Corr}(x_t, x_{t+k})$. By definition:\n$$\n\\mathrm{Corr}(x_t, x_{t+k}) = \\frac{\\mathrm{Cov}(x_t, x_{t+k})}{\\sqrt{\\mathrm{Var}(x_t)\\mathrm{Var}(x_{t+k})}}\n$$\nIn the stationary state, $\\mathrm{Var}(x_t) = \\mathrm{Var}(x_{t+k}) = 1$. Therefore, the autocorrelation is equal to the autocovariance:\n$$\n\\mathrm{Corr}(x_t, x_{t+k}) = \\mathrm{Cov}(x_t, x_{t+k})\n$$\nWe can find a recurrence relation for the autocovariance. For $k \\ge 1$:\n$$\n\\mathrm{Cov}(x_t, x_{t+k}) = \\mathrm{Cov}(x_t, \\rho^2 x_{t+k-1} + u_{t+k})\n$$\nUsing the bilinearity of covariance:\n$$\n\\mathrm{Cov}(x_t, x_{t+k}) = \\mathrm{Cov}(x_t, \\rho^2 x_{t+k-1}) + \\mathrm{Cov}(x_t, u_{t+k})\n$$\nThe noise term $u_{t+k}$ is generated from random variables $\\epsilon_{t+k}$ and $\\delta_{t+k}$ drawn at iteration $t+k$. It is therefore independent of $x_t$, which depends only on noise terms up to iteration $t$. Thus, $\\mathrm{Cov}(x_t, u_{t+k})=0$.\nThis simplifies the expression to:\n$$\n\\mathrm{Cov}(x_t, x_{t+k}) = \\rho^2 \\mathrm{Cov}(x_t, x_{t+k-1})\n$$\nThis is a geometric recurrence relation. Let $C(k) = \\mathrm{Cov}(x_t, x_{t+k})$. The relation is $C(k) = \\rho^2 C(k-1)$.\nThe base case for the recurrence is at lag $k=0$:\n$$\nC(0) = \\mathrm{Cov}(x_t, x_t) = \\mathrm{Var}(x_t) = 1\n$$\nSolving the recurrence:\n$$\nC(k) = (\\rho^2)^k C(0) = (\\rho^2)^k \\cdot 1 = \\rho^{2k}\n$$\nThis result is valid for any non-negative integer $k \\in \\{0, 1, 2, \\dots\\}$. For $k=0$, it gives $\\rho^0=1$, which is correct as $\\mathrm{Corr}(x_t, x_t)=1$.\n\nThus, the lag-$k$ autocorrelation of the stationary $x$-subchain is $\\rho^{2k}$. Note that if the order of updates within the Gibbs scan were reversed (update $x$ then $y$), the resulting AR($1$) process equation for the component subchain and its autocorrelation would remain identical.", "answer": "$$\\boxed{\\rho^{2k}}$$", "id": "3235784"}]}