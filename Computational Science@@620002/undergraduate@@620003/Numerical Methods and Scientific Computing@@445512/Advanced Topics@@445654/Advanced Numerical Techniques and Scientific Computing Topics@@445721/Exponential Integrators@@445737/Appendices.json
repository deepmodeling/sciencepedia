{"hands_on_practices": [{"introduction": "The journey into exponential integrators begins with understanding how to derive the most fundamental schemes. This practice guides you through the construction of two classic first-order methods, the Lawson and Exponential Euler integrators, directly from the variation-of-constants formula [@problem_id:3227424]. By implementing and comparing these methods on a solvable test problem, you will gain hands-on experience with their structure, performance, and the crucial process of numerical order verification.", "problem": "You are to derive, implement, and test two first-order exponential integrators for a semilinear ordinary differential equation of the form $y' = A y + N(y)$, based on fundamental definitions from the variation-of-constants formula and the exponential of a linear operator. Work in the scalar case where $A$ is represented by the real scalar parameter $-\\lambda$ with $\\lambda > 0$, and the nonlinearity is $N(y) = \\mu y - \\mu y^2$. The initial value is $y(0) = y_0$.\n\n1. Derivation task. Starting from the variation-of-constants formula and the definition of the exponential of a linear operator, derive:\n   - A first-order Lawson method obtained by a time-dependent change of variables that removes the linear part and then applies the explicit Euler method in the transformed coordinates.\n   - A first-order exponential Euler method obtained by applying a first-order quadrature to the variation-of-constants integral.\n\n   Your derivations must clearly indicate the discrete-time update formulas in terms of the step size $h$, the matrix exponential of $A$ (in the scalar case the exponential of a number), and the nonlinearity $N(\\cdot)$.\n\n2. Exact reference solution. For the scalar problem $y' = -\\lambda y + \\mu y - \\mu y^2$ with $y(0) = y_0$, obtain a closed-form exact solution $y(t)$ that you will use as a reference for error computation. Express $y(t)$ in terms of $\\lambda$, $\\mu$, $y_0$, and $t$. Ensure your expression covers both the generic case $\\mu - \\lambda \\neq 0$ and the limiting case $\\mu - \\lambda = 0$.\n\n3. Implementation task. Implement a program that:\n   - Integrates the scalar problem using the two derived schemes over a uniform grid with step size $h = T/m$ for a specified positive integer $m$, up to final time $T$, and returns the numerical solution at $t = T$.\n   - Computes the absolute global error at $t = T$ by comparing to the exact solution from part $2$.\n   - Computes an observed order estimate using two successive refinements with a halved step size: if $e(h)$ and $e(h/2)$ are the errors at step sizes $h$ and $h/2$, respectively, estimate the observed order $p$ as $p \\approx \\log\\left(e(h)/e(h/2)\\right) / \\log(2)$.\n\n4. Test suite. Use the following three parameter sets and step counts:\n   - Case A (happy path, moderately stiff): $(\\lambda, \\mu, y_0, T) = (50, 1, 0.1, 1)$ with step counts $m \\in \\{50, 100, 200, 400\\}$.\n   - Case B (boundary case with vanishing net linear growth): $(\\lambda, \\mu, y_0, T) = (1, 1, 0.5, 1)$ with step counts $m \\in \\{50, 100, 200, 400\\}$.\n   - Case C (high stiffness, short horizon): $(\\lambda, \\mu, y_0, T) = (200, 2, 0.9, 0.5)$ with step counts $m \\in \\{50, 100, 200, 400\\}$.\n\n   For each case:\n   - Compute the absolute global errors at $t = T$ for both methods at $m = 200$ and $m = 400$, and use these to estimate the observed order $p$ for each method as described above.\n   - Compute the ratio of the Lawson error to the exponential Euler error at the finest step count $m = 400$.\n\n5. Output specification. Your program must produce a single line of output containing a flat list of $9$ floating-point numbers in the following order:\n   - For Case A: the Lawson observed order using $(m=200, m=400)$, the exponential Euler observed order using $(m=200, m=400)$, and the Lawson-to-exponential-Euler error ratio at $m=400$.\n   - For Case B: the Lawson observed order using $(m=200, m=400)$, the exponential Euler observed order using $(m=200, m=400)$, and the Lawson-to-exponential-Euler error ratio at $m=400$.\n   - For Case C: the Lawson observed order using $(m=200, m=400)$, the exponential Euler observed order using $(m=200, m=400)$, and the Lawson-to-exponential-Euler error ratio at $m=400$.\n\n   The final output line must therefore be of the form $[p_{L,A}, p_{E,A}, r_A, p_{L,B}, p_{E,B}, r_B, p_{L,C}, p_{E,C}, r_C]$, where each entry is a floating-point number. No physical units are involved. Angles are not used.\n\nThe program must be complete and runnable as-is, and must not require any user input or external files. It must implement the algorithms you derived and use only the libraries specified in the execution environment. The final line printed by the program must be exactly the list described above, with comma-separated entries enclosed in square brackets, with no additional text.", "solution": "The problem statement is scrutinized and found to be valid. It is scientifically grounded in the theory of numerical methods for ordinary differential equations, is well-posed, objective, and provides a complete and consistent set of requirements for derivation, implementation, and testing.\n\nThe problem centers on the scalar semilinear ordinary differential equation (ODE):\n$$\ny'(t) = -\\lambda y(t) + (\\mu y(t) - \\mu y(t)^2)\n$$\nwith initial condition $y(0) = y_0$, where $\\lambda > 0$, $\\mu$, and $y_0$ are real parameters. This equation is of the form $y' = Ay + N(y)$ with the linear operator $A = -\\lambda$ and the nonlinear part $N(y) = \\mu y - \\mu y^2$.\n\n### 1. Derivation of Numerical Schemes\n\nThe basis for exponential integrators is the variation-of-constants formula, which gives the exact solution to $y' = Ay + N(y)$ over a time interval $[t_n, t_{n+1}]$ of length $h = t_{n+1} - t_n$:\n$$\ny(t_{n+1}) = e^{Ah} y(t_n) + \\int_{0}^{h} e^{A(h-s)} N(y(t_n+s)) ds\n$$\nwhere $y_n$ is the approximation to $y(t_n)$. Different approximations of the integral term lead to different exponential integration schemes.\n\n#### 1.1 First-Order Lawson Method\nThe Lawson method introduces a change of variables to remove the stiff linear part. Let $v(t) = e^{-At} y(t)$, which implies $y(t) = e^{At} v(t)$. Differentiating $v(t)$ with respect to time gives:\n$$\nv'(t) = -A e^{-At} y(t) + e^{-At} y'(t)\n$$\nSubstituting $y'(t) = Ay(t) + N(y(t))$:\n$$\nv'(t) = -A e^{-At} y(t) + e^{-At} (Ay(t) + N(y(t))) = e^{-At} N(y(t))\n$$\nSubstituting $y(t) = e^{At} v(t)$:\n$$\nv'(t) = e^{-At} N(e^{At} v(t))\n$$\nThe first-order Lawson method applies the explicit (forward) Euler method to this transformed equation for $v(t)$:\n$$\nv_{n+1} = v_n + h v'(t_n) = v_n + h e^{-At_n} N(e^{At_n}v_n)\n$$\nRecognizing that $y_n = e^{At_n} v_n$, we have $N(e^{At_n}v_n) = N(y_n)$. Substituting $v_n = e^{-At_n} y_n$ and $v_{n+1} = e^{-At_{n+1}} y_{n+1}$:\n$$\ne^{-At_{n+1}} y_{n+1} = e^{-At_n} y_n + h e^{-At_n} N(y_n)\n$$\nMultiplying by $e^{At_n}$ gives:\n$$\ne^{A(t_n - t_{n+1})} y_{n+1} = y_n + h N(y_n)\n$$\nSince $t_n - t_{n+1} = -h$, we get $e^{-Ah} y_{n+1} = y_n + h N(y_n)$. The final update formula for the first-order Lawson method is:\n$$\ny_{n+1} = e^{Ah} (y_n + h N(y_n))\n$$\nFor the given scalar problem, $A = -\\lambda$ and $N(y_n) = \\mu y_n - \\mu y_n^2$. The update formula becomes:\n$$\ny_{n+1} = e^{-\\lambda h} (y_n + h (\\mu y_n - \\mu y_n^2))\n$$\n\n#### 1.2 First-Order Exponential Euler Method\nThis method is derived by applying a simple first-order quadrature to the integral in the variation-of-constants formula. We approximate the integrand by assuming the nonlinear term $N(y(t_n+s))$ is constant over the interval $[0, h]$ and equal to its value at the start, $N(y_n)$.\n$$\ny_{n+1} \\approx e^{Ah} y_n + \\int_{0}^{h} e^{A(h-s)} N(y_n) ds\n$$\nSince $N(y_n)$ is constant with respect to the integration variable $s$, we can pull it out of the integral:\n$$\ny_{n+1} = e^{Ah} y_n + \\left( \\int_{0}^{h} e^{A(h-s)} ds \\right) N(y_n)\n$$\nThe integral can be evaluated exactly. Let $u=h-s$, so $du = -ds$. The limits change from $(0, h)$ for $s$ to $(h, 0)$ for $u$.\n$$\n\\int_{0}^{h} e^{A(h-s)} ds = \\int_{h}^{0} e^{Au} (-du) = \\int_{0}^{h} e^{Au} du = A^{-1}(e^{Ah} - I)\n$$\nThis requires $A$ to be invertible, which is true since $A = -\\lambda$ and $\\lambda > 0$. This integral is often expressed using the $\\phi_1$ function, $\\int_0^h e^{A(h-s)}ds = h \\phi_1(Ah)$, where $\\phi_1(z) = (e^z-1)/z$.\nThe update formula for the exponential Euler method (also known as ETD1) is:\n$$\ny_{n+1} = e^{Ah} y_n + A^{-1}(e^{Ah} - I) N(y_n)\n$$\nFor the scalar problem, $e^{Ah} = e^{-\\lambda h}$ and $A^{-1}(e^{Ah}-I) = (-\\lambda)^{-1}(e^{-\\lambda h}-1) = \\frac{1-e^{-\\lambda h}}{\\lambda}$. The update formula is:\n$$\ny_{n+1} = e^{-\\lambda h} y_n + \\frac{1-e^{-\\lambda h}}{\\lambda} (\\mu y_n - \\mu y_n^2)\n$$\n\n### 2. Exact Reference Solution\nThe given ODE is $y' = (\\mu - \\lambda)y - \\mu y^2$. This is a Bernoulli differential equation of the form $y' + P(t)y = Q(t)y^n$, with $P(t) = \\lambda - \\mu$, $Q(t) = -\\mu$, and $n=2$. We solve it by the substitution $u(t) = y(t)^{1-n} = y(t)^{-1}$. This gives $u' = -y^{-2}y'$.\nThe original equation can be written as $y' + (\\lambda-\\mu)y = -\\mu y^2$. Dividing by $y^2$ gives:\n$$\ny^{-2}y' + (\\lambda-\\mu)y^{-1} = -\\mu\n$$\nSubstituting $u$ and $u'$:\n$$\n-u' + (\\lambda-\\mu)u = -\\mu \\implies u' - (\\lambda-\\mu)u = \\mu\n$$\nThis is a first-order linear ODE for $u(t)$.\n\nCase 1: $\\lambda \\neq \\mu$.\nThe integrating factor is $I(t) = \\exp\\left(-\\int (\\lambda-\\mu) dt\\right) = e^{-(\\lambda-\\mu)t}$.\nMultiplying the ODE for $u$ by $I(t)$ gives $(u e^{-(\\lambda-\\mu)t})' = \\mu e^{-(\\lambda-\\mu)t}$. Integrating from $0$ to $t$:\n$$\nu(t)e^{-(\\lambda-\\mu)t} - u(0) = \\int_0^t \\mu e^{-(\\lambda-\\mu)\\tau} d\\tau = \\mu \\left[ \\frac{e^{-(\\lambda-\\mu)\\tau}}{-(\\lambda-\\mu)} \\right]_0^t = \\frac{\\mu}{-(\\lambda-\\mu)} (e^{-(\\lambda-\\mu)t} - 1)\n$$\nWith $u(0) = y_0^{-1}$, solving for $u(t)$ gives:\n$$\nu(t) = u(0)e^{(\\lambda-\\mu)t} + \\frac{\\mu}{\\lambda-\\mu}(e^{(\\lambda-\\mu)t} - 1) = \\left(\\frac{1}{y_0} + \\frac{\\mu}{\\lambda-\\mu}\\right)e^{(\\lambda-\\mu)t} - \\frac{\\mu}{\\lambda-\\mu}\n$$\nSince $y(t) = u(t)^{-1}$, the exact solution is:\n$$\ny(t) = \\left( \\left(\\frac{1}{y_0} + \\frac{\\mu}{\\lambda-\\mu}\\right)e^{(\\lambda-\\mu)t} - \\frac{\\mu}{\\lambda-\\mu} \\right)^{-1}\n$$\n\nCase 2: $\\lambda = \\mu$.\nThe ODE for $u$ simplifies to $u' = \\mu$.\nIntegrating from $0$ to $t$ gives $u(t) - u(0) = \\mu t$.\nWith $u(0) = y_0^{-1}$, we have $u(t) = y_0^{-1} + \\mu t$.\nThe exact solution for this case is:\n$$\ny(t) = \\frac{1}{y_0^{-1} + \\mu t} = \\frac{y_0}{1 + \\mu t y_0}\n$$\n\n### 3. Implementation and Analysis\nThe implementation will consist of functions for each numerical method and for the exact solution. The main procedure will iterate through the provided test cases. For each case, it will run the simulations for step counts $m=200$ and $m=400$ to compute the final time errors. The observed order of convergence, $p$, is estimated using the errors from two successive refinements, $e(h)$ and $e(h/2)$, as:\n$$\np \\approx \\frac{\\log(e(h) / e(h/2))}{\\log(2)}\n$$\nHere, $h$ corresponds to $m=200$ and $h/2$ to $m=400$. The ratio of errors between the Lawson and exponential Euler methods will be computed at the finest resolution ($m=400$).", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives, implements, and tests two first-order exponential integrators.\n    \"\"\"\n\n    def exact_solution(lam, mu, y0, t):\n        \"\"\"Computes the exact solution y(t).\"\"\"\n        if lam == mu:\n            if 1.0 + mu * t * y0 == 0:\n                return np.inf  # or handle as a singularity\n            return y0 / (1.0 + mu * t * y0)\n        else:\n            c = lam - mu\n            term1 = (1.0 / y0 + mu / c) * np.exp(c * t)\n            term2 = mu / c\n            denominator = term1 - term2\n            if denominator == 0:\n                return np.inf\n            return 1.0 / denominator\n\n    def integrate_lawson(lam, mu, y0, T, m):\n        \"\"\"Integrates the ODE using the first-order Lawson method.\"\"\"\n        h = T / m\n        y = y0\n        exp_term = np.exp(-lam * h)\n        \n        for _ in range(m):\n            N_y = mu * y - mu * y**2\n            y = exp_term * (y + h * N_y)\n        return y\n\n    def integrate_exp_euler(lam, mu, y0, T, m):\n        \"\"\"Integrates the ODE using the first-order exponential Euler method.\"\"\"\n        h = T / m\n        y = y0\n        exp_term = np.exp(-lam * h)\n        \n        # The phi_1 function term: (e^(Ah) - I) / A\n        # h * phi_1(Ah) = h * (e^(Ah)-1)/(Ah) = (e^(-lam*h)-1)/(-lam) = (1-e^(-lam*h))/lam\n        # Use np.expm1 for numerical stability for small lam*h\n        if lam == 0: # Should not happen based on problem spec (lam > 0)\n            phi1_term = h\n        else:\n            phi1_term = -np.expm1(-lam * h) / lam\n\n        for _ in range(m):\n            N_y = mu * y - mu * y**2\n            y = exp_term * y + phi1_term * N_y\n        return y\n\n    def calculate_metrics(params, m1, m2):\n        \"\"\"Calculates errors, order, and ratio for a given parameter set.\"\"\"\n        lam, mu, y0, T = params\n        \n        y_exact_T = exact_solution(lam, mu, y0, T)\n\n        # Lawson method calculations\n        y_L1 = integrate_lawson(lam, mu, y0, T, m1)\n        y_L2 = integrate_lawson(lam, mu, y0, T, m2)\n        error_L1 = np.abs(y_L1 - y_exact_T)\n        error_L2 = np.abs(y_L2 - y_exact_T)\n        \n        # Exponential Euler method calculations\n        y_E1 = integrate_exp_euler(lam, mu, y0, T, m1)\n        y_E2 = integrate_exp_euler(lam, mu, y0, T, m2)\n        error_E1 = np.abs(y_E1 - y_exact_T)\n        error_E2 = np.abs(y_E2 - y_exact_T)\n\n        # Observed order calculation\n        # p = log(e(h)/e(h/2)) / log(2)\n        p_L = np.log(error_L1 / error_L2) / np.log(2) if error_L2 > 0 else 0.0\n        p_E = np.log(error_E1 / error_E2) / np.log(2) if error_E2 > 0 else 0.0\n        \n        # Error ratio at finest grid (m2)\n        ratio = error_L2 / error_E2 if error_E2 > 0 else np.inf\n        \n        return p_L, p_E, ratio\n\n    # Define test cases from the problem statement\n    test_cases = [\n        # Case A: (lambda, mu, y0, T) = (50, 1, 0.1, 1)\n        (50.0, 1.0, 0.1, 1.0),\n        # Case B: (lambda, mu, y0, T) = (1, 1, 0.5, 1)\n        (1.0, 1.0, 0.5, 1.0),\n        # Case C: (lambda, mu, y0, T) = (200, 2, 0.9, 0.5)\n        (200.0, 2.0, 0.9, 0.5),\n    ]\n    \n    m_coarse = 200\n    m_fine = 400\n\n    results = []\n    for case in test_cases:\n        p_L, p_E, r = calculate_metrics(case, m_coarse, m_fine)\n        results.extend([p_L, p_E, r])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3227424"}, {"introduction": "A powerful numerical formula is only as good as its implementation, and exponential integrators rely heavily on the stable computation of the $\\varphi$-functions. This exercise addresses a critical practical challenge: the loss of precision when evaluating $\\varphi_1(z) = (e^z - 1)/z$ for small $|z|$ due to subtractive cancellation [@problem_id:3227524]. You will explore and compare two powerful remedies—Taylor series expansions and Padé approximants—to understand how to ensure your code is both accurate and robust.", "problem": "Consider the function that is central to Exponential Time Differencing (ETD) integrators, defined for any complex argument $z$ by $\\varphi_1(z) = \\frac{e^z - 1}{z}$ for $z \\neq 0$, and by continuity $\\varphi_1(0) = 1$. In exponential integrators, accurate and stable evaluation of $\\varphi_1(z)$ for small $|z|$ is essential. When $|z| \\ll 1$, direct computation of $\\frac{e^z - 1}{z}$ suffers from subtractive cancellation, which can amplify floating-point roundoff. Two common strategies to mitigate this are: a truncated Taylor series for $\\varphi_1(z)$ derived from the power series of the exponential function, and rational Padé approximants that match the series of $\\varphi_1(z)$ to a specified order.\n\nStarting from fundamental definitions and well-tested formulas:\n- The exponential function has the power series $e^z = \\sum_{n=0}^{\\infty} \\frac{z^n}{n!}$, valid for all complex $z$.\n- The function $\\varphi_1(z)$ can be expressed as a power series by dividing $\\sum_{n=1}^{\\infty} \\frac{z^n}{n!}$ by $z$.\n- A Padé approximant of type $[m/n]$ is a rational function whose series matches a given function up to a prescribed order.\n\nYour tasks:\n1. Derive, from first principles, a truncated Taylor series representation for $\\varphi_1(z)$ by using the definition of $e^z$ above. Express $\\varphi_1(z)$ as a series $\\sum_{k=0}^{K} c_k z^k$ by identifying the coefficients $c_k$ in terms of factorials, without using any pre-derived shortcut formulas.\n2. Construct a rational Padé approximant of type $[2/2]$ for $\\varphi_1(z)$ by equating the power series of an unknown rational function $\\frac{a_0 + a_1 z + a_2 z^2}{1 + b_1 z + b_2 z^2}$ to the series of $\\varphi_1(z)$ up to and including terms of order $z^4$, and solve for the coefficients $a_0$, $a_1$, $a_2$, $b_1$, and $b_2$ using only algebraic matching of series coefficients.\n3. Analyze roundoff amplification by defining, for a given computational method $M$ and test value $z$, the amplification factor\n$$A_M(z) = \\frac{\\left|\\varphi_{1,M}(z) - \\varphi_{1,\\text{ref}}(z)\\right|}{\\left|\\varphi_{1,\\text{ref}}(z)\\right|} \\cdot \\frac{1}{\\varepsilon},$$\nwhere $\\varphi_{1,M}(z)$ is the value computed by method $M$, $\\varphi_{1,\\text{ref}}(z)$ is a high-accuracy reference computed by summing the Taylor series to a sufficiently large number of terms, and $\\varepsilon$ is the machine epsilon for IEEE-754 double precision. In this computation, all quantities are dimensionless, and no physical units are involved.\n4. Implement a complete program that:\n   - Uses a truncated series with $N_{\\text{series}} = 12$ terms to compute $\\varphi_{1,\\text{series}}(z)$.\n   - Uses the derived $[2/2]$ Padé approximant to compute $\\varphi_{1,\\text{Padé}}(z)$.\n   - Uses a high-accuracy reference series with $N_{\\text{ref}} = 60$ terms to compute $\\varphi_{1,\\text{ref}}(z)$.\n   - Computes $A_{\\text{series}}(z)$ and $A_{\\text{Padé}}(z)$ for each test case.\n   - Produces, for each test case, a three-item list $[A_{\\text{series}}(z), A_{\\text{Padé}}(z), B(z)]$, where $B(z)$ is a boolean that is true if $A_{\\text{series}}(z) < A_{\\text{Padé}}(z)$ and false otherwise.\n\nTest Suite:\nEvaluate the program on the following six test inputs, chosen to exercise different facets:\n- General small real: $z = 10^{-2}$.\n- Very small real (happy path for cancellation analysis): $z = 10^{-8}$.\n- Extremely small real (near the double precision scale): $z = 10^{-12}$.\n- Very small negative real: $z = -10^{-12}$.\n- Very small purely imaginary: $z = i \\cdot 10^{-8}$.\n- Extremely small complex with equal real and imaginary parts: $z = \\frac{10^{-16}}{\\sqrt{2}} (1 + i)$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must have six entries, each corresponding to a test case in the order given above. Each entry must itself be a list of the form $[A_{\\text{series}}(z), A_{\\text{Padé}}(z), B(z)]$, where the first two entries are floating-point numbers and the third is a boolean. For example, the output should have the form\n$[[a_1,p_1,b_1],[a_2,p_2,b_2],[a_3,p_3,b_3],[a_4,p_4,b_4],[a_5,p_5,b_5],[a_6,p_6,b_6]]$.", "solution": "The problem is valid. It is scientifically grounded in the principles of numerical analysis, specifically the study of exponential integrators and the mitigation of roundoff error. The problem is well-posed, objective, and self-contained, with all necessary definitions, constants, and test cases provided for a unique and verifiable solution.\n\nThe core task is to evaluate the function $\\varphi_1(z) = \\frac{e^z - 1}{z}$ for small $|z|$. Direct computation suffers from catastrophic cancellation because for $z \\approx 0$, we have $e^z \\approx 1$, leading to a subtraction of nearly equal numbers. To circumvent this, we can use approximations based on the function's power series. We will derive and compare two such approximations: a truncated Taylor series and a Padé approximant.\n\n**1. Derivation of the Taylor Series for $\\varphi_1(z)$**\n\nWe begin with the fundamental power series representation of the exponential function, which converges for all complex $z$:\n$$e^z = \\sum_{n=0}^{\\infty} \\frac{z^n}{n!} = 1 + z + \\frac{z^2}{2!} + \\frac{z^3}{3!} + \\dots$$\nFrom this, we find the series for $e^z - 1$:\n$$e^z - 1 = \\left(\\sum_{n=0}^{\\infty} \\frac{z^n}{n!}\\right) - 1 = \\left(1 + \\sum_{n=1}^{\\infty} \\frac{z^n}{n!}\\right) - 1 = \\sum_{n=1}^{\\infty} \\frac{z^n}{n!}$$\nNow, we can derive the series for $\\varphi_1(z)$ by dividing by $z$:\n$$\\varphi_1(z) = \\frac{e^z - 1}{z} = \\frac{1}{z} \\sum_{n=1}^{\\infty} \\frac{z^n}{n!} = \\sum_{n=1}^{\\infty} \\frac{z^{n-1}}{n!}$$\nTo express this in the standard form $\\sum_{k=0}^{\\infty} c_k z^k$, we perform a change of index by setting $k = n-1$. As $n$ goes from $1$ to $\\infty$, $k$ goes from $0$ to $\\infty$. Substituting $n = k+1$ into the series gives:\n$$\\varphi_1(z) = \\sum_{k=0}^{\\infty} \\frac{z^k}{(k+1)!}$$\nThus, the truncated Taylor series representation is $\\sum_{k=0}^{K} c_k z^k$ with coefficients $c_k = \\frac{1}{(k+1)!}$. The first few terms are:\n$$\\varphi_1(z) = \\frac{1}{1!} + \\frac{z}{2!} + \\frac{z^2}{3!} + \\frac{z^3}{4!} + \\dots = 1 + \\frac{z}{2} + \\frac{z^2}{6} + \\frac{z^3}{24} + \\dots$$\n\n**2. Construction of the $[2/2]$ Padé Approximant**\n\nA Padé approximant of type $[m/n]$ for a function $f(z)$ is a rational function $R(z) = \\frac{P_m(z)}{Q_n(z)}$, where $P_m(z)$ and $Q_n(z)$ are polynomials of degree $m$ and $n$ respectively, whose Maclaurin series agrees with that of $f(z)$ to the highest possible order, which is $m+n$.\n\nWe seek the $[2/2]$ Padé approximant for $\\varphi_1(z)$, which has the form:\n$$R(z) = \\frac{a_0 + a_1 z + a_2 z^2}{1 + b_1 z + b_2 z^2}$$\nThe coefficient of $z^0$ in the denominator is normalized to $1$. We need to match the series expansion of $R(z)$ with the Taylor series of $\\varphi_1(z)$ up to the term $z^{m+n} = z^4$. This is equivalent to the condition:\n$$(1 + b_1 z + b_2 z^2) \\varphi_1(z) - (a_0 + a_1 z + a_2 z^2) = O(z^5)$$\nSubstituting the series for $\\varphi_1(z)$:\n$$(1 + b_1 z + b_2 z^2) \\left(1 + \\frac{z}{2} + \\frac{z^2}{6} + \\frac{z^3}{24} + \\frac{z^4}{120} + \\dots\\right) = a_0 + a_1 z + a_2 z^2$$\nWe expand the left-hand side and equate coefficients of powers of $z$ on both sides.\n\n- Coefficient of $z^0$: $1 \\cdot 1 = a_0 \\implies a_0 = 1$.\n- Coefficient of $z^1$: $1 \\cdot \\frac{1}{2} + b_1 \\cdot 1 = a_1 \\implies \\frac{1}{2} + b_1 = a_1$.\n- Coefficient of $z^2$: $1 \\cdot \\frac{1}{6} + b_1 \\cdot \\frac{1}{2} + b_2 \\cdot 1 = a_2 \\implies \\frac{1}{6} + \\frac{b_1}{2} + b_2 = a_2$.\n- Coefficient of $z^3$: $1 \\cdot \\frac{1}{24} + b_1 \\cdot \\frac{1}{6} + b_2 \\cdot \\frac{1}{2} = 0$.\n- Coefficient of $z^4$: $1 \\cdot \\frac{1}{120} + b_1 \\cdot \\frac{1}{24} + b_2 \\cdot \\frac{1}{6} = 0$.\n\nThe equations for the coefficients of $z^3$ and $z^4$ form a linear system for $b_1$ and $b_2$:\n1. $\\frac{b_1}{6} + \\frac{b_2}{2} = -\\frac{1}{24} \\implies 4b_1 + 12b_2 = -1$.\n2. $\\frac{b_1}{24} + \\frac{b_2}{6} = -\\frac{1}{120} \\implies 5b_1 + 20b_2 = -1$.\n\nMultiplying the first equation by $5$ and the second by $4$ gives:\n$20b_1 + 60b_2 = -5$\n$20b_1 + 80b_2 = -4$\nSubtracting the first from the second yields $20b_2 = 1$, so $b_2 = \\frac{1}{20}$.\nSubstituting $b_2$ back into $4b_1 + 12b_2 = -1$:\n$4b_1 + 12\\left(\\frac{1}{20}\\right) = -1 \\implies 4b_1 + \\frac{3}{5} = -1 \\implies 4b_1 = -\\frac{8}{5} \\implies b_1 = -\\frac{2}{5}$.\n\nNow we solve for the numerator coefficients:\n- $a_0 = 1$.\n- $a_1 = \\frac{1}{2} + b_1 = \\frac{1}{2} - \\frac{2}{5} = \\frac{5 - 4}{10} = \\frac{1}{10}$.\n- $a_2 = \\frac{1}{6} + \\frac{b_1}{2} + b_2 = \\frac{1}{6} + \\frac{1}{2}\\left(-\\frac{2}{5}\\right) + \\frac{1}{20} = \\frac{1}{6} - \\frac{1}{5} + \\frac{1}{20} = \\frac{10 - 12 + 3}{60} = \\frac{1}{60}$.\n\nThe resulting $[2/2]$ Padé approximant for $\\varphi_1(z)$ is:\n$$R(z) = \\frac{1 + \\frac{1}{10}z + \\frac{1}{60}z^2}{1 - \\frac{2}{5}z + \\frac{1}{20}z^2}$$\n\n**3. Numerical Implementation and Analysis**\n\nWe will now implement the numerical comparison. The goal is to compute the roundoff amplification factor,\n$$A_M(z) = \\frac{\\left|\\varphi_{1,M}(z) - \\varphi_{1,\\text{ref}}(z)\\right|}{\\left|\\varphi_{1,\\text{ref}}(z)\\right|} \\cdot \\frac{1}{\\varepsilon}$$\nfor two methods $M$: a truncated Taylor series and the derived Padé approximant.\n- $\\varphi_{1,\\text{series}}(z)$ will be computed using the Taylor series truncated to $N_{\\text{series}} = 12$ terms.\n- $\\varphi_{1,\\text{Padé}}(z)$ will be computed using the $[2/2]$ approximant derived above.\n- The reference value $\\varphi_{1,\\text{ref}}(z)$ will be computed using the Taylor series with a high number of terms, $N_{\\text{ref}} = 60$, to ensure high fidelity. For the small values of $|z|$ in the test suite, this sum is a highly accurate representation of the true function value within the limits of double-precision arithmetic.\n- The machine epsilon $\\varepsilon$ is for IEEE-754 double precision, approximately $2.22 \\times 10^{-16}$.\n\nThe implementation will be provided as a complete Python program. This program will define functions for each computation method, iterate through the specified test cases, and calculate the amplification factors $A_{\\text{series}}(z)$ and $A_{\\text{Padé}}(z)$, along with a boolean $B(z)$ indicating if the series method is more accurate than the Padé method for that input.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef phi1_series(z: complex, N: int) -> complex:\n    \"\"\"\n    Computes phi_1(z) using its Taylor series expansion up to N terms.\n    The series is phi_1(z) = sum_{k=0}^{N-1} z^k / (k+1)!.\n    This is implemented using an efficient and stable iterative method.\n    \"\"\"\n    total = complex(0.0, 0.0)\n    # The term for k=0 is z^0 / 1! = 1\n    term = complex(1.0, 0.0)\n    for k in range(N):\n        # At the start of iteration k, 'term' holds z^k / (k+1)!\n        total += term\n        # Update term for the next iteration (k+1).\n        # T_{k+1} = z^(k+1) / (k+2)!\n        # T_{k+1} = T_k * z * (k+1)! / (k+2)! = T_k * z / (k+2)\n        term *= z / (k + 2)\n    return total\n\ndef phi1_pade_2_2(z: complex) -> complex:\n    \"\"\"\n    Computes phi_1(z) using the derived [2/2] Padé approximant.\n    R(z) = (1 + (1/10)z + (1/60)z^2) / (1 - (2/5)z + (1/20)z^2).\n    \"\"\"\n    # Numerator coefficients derived as a_0=1, a_1=1/10, a_2=1/60\n    num = 1.0 + z / 10.0 + (z**2) / 60.0\n    # Denominator coefficients derived as b_1=-2/5, b_2=1/20\n    den = 1.0 - (2.0 * z) / 5.0 + (z**2) / 20.0\n    return num / den\n\ndef solve():\n    \"\"\"\n    Main function to execute the problem tasks.\n    It computes and compares the accuracy of the Taylor series and Padé\n    approximants for phi_1(z) for a suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        1e-2 + 0j,\n        1e-8 + 0j,\n        1e-12 + 0j,\n        -1e-12 + 0j,\n        1j * 1e-8,\n        (1e-16 / np.sqrt(2)) * (1 + 1j)\n    ]\n\n    N_series = 12\n    N_ref = 60\n    # Machine epsilon for IEEE-754 double precision float.\n    eps = np.finfo(float).eps\n\n    results = []\n    for z in test_cases:\n        # 1. Compute high-accuracy reference value\n        phi_ref = phi1_series(z, N_ref)\n\n        # 2. Compute values using the two methods under test\n        phi_series = phi1_series(z, N_series)\n        phi_pade = phi1_pade_2_2(z)\n\n        # 3. Compute the amplification factors\n        # A = (relative error) / (machine epsilon)\n        # Use np.abs for complex numbers. The denominator abs(phi_ref) is a\n        # safe operation as it will be very close to 1 for small z.\n        err_series = np.abs(phi_series - phi_ref)\n        err_pade = np.abs(phi_pade - phi_ref)\n        \n        # Handle cases where the error is numerically zero to avoid division by zero\n        # if phi_ref also happens to be zero, although unlikely here.\n        abs_phi_ref = np.abs(phi_ref)\n        if abs_phi_ref == 0:\n            # This case is not expected for the given test values.\n            # If both error and ref are 0, amplification is 0. If error is non-zero, it's infinite.\n            A_series = 0.0 if err_series == 0 else np.inf\n            A_pade = 0.0 if err_pade == 0 else np.inf\n        else:\n            A_series = (err_series / abs_phi_ref) / eps\n            A_pade = (err_pade / abs_phi_ref) / eps\n        \n        # 4. Perform the boolean comparison\n        is_series_better = A_series < A_pade\n\n        # 5. Format the result list for this test case\n        # The boolean must be formatted as lowercase \"true\" or \"false\".\n        results.append(f\"[{A_series},{A_pade},{str(is_series_better).lower()}]\")\n\n    # Final print statement in the exact required format.\n    # e.g., [[a1,p1,b1],[a2,p2,b2],...]\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3227524"}, {"introduction": "Efficient computation requires not just accurate formulas, but also intelligent control over the simulation process. This practice elevates our understanding from fixed-step methods to the design of an adaptive time-stepping algorithm, a core feature of modern ODE solvers [@problem_id:3227510]. By deriving an embedded Exponential Runge-Kutta pair, you will construct a step-size controller that automatically balances computational effort and accuracy, providing a powerful tool for tackling complex problems.", "problem": "Consider the semilinear initial value problem $y'(t) = A y(t) + g(y(t), t)$ with $y(t_0) = y_0$, where $A$ is a constant matrix and $g$ is a sufficiently smooth nonlinear function. The fundamental base is the variation-of-constants formula for linear-nonlinear splits, which states that for any step $h > 0$,\n$$\ny(t_0 + h) = e^{h A} y(t_0) + \\int_{0}^{h} e^{(h - \\tau) A} \\, g\\!\\big(y(t_0 + \\tau), t_0 + \\tau\\big) \\, d\\tau.\n$$\nDefine the $\\varphi$-functions by\n$$\n\\varphi_0(z) = e^{z}, \\quad\n\\varphi_1(z) = \\frac{e^{z} - 1}{z}, \\quad\n\\varphi_2(z) = \\frac{e^{z} - z - 1}{z^2},\n$$\nwith the convention that the limiting values are used if $z = 0$ (the corresponding limits exist and equal the first terms of their Taylor expansions).\n\nYour tasks are:\n\n1. Starting from the variation-of-constants formula, derive an embedded pair of Exponential Runge–Kutta (ERK) methods of orders $1/2$: a first-order method and a second-order method that share function evaluations. Both methods must be expressed only using evaluations of the $\\varphi_k$ functions applied to $h A$ and the nonlinear function $g$. Provide clear reasoning that the higher-order method achieves order $2$ and the embedded method achieves order $1$.\n\n2. Design an adaptive step-size controller that uses the difference between the second-order and first-order updates as an error estimator. Your controller must be explicit in how it uses the estimated local truncation error, must specify the acceptance and rejection criteria for a step, and must provide a formula for updating the step size $h$. Use a norm-based error measure with absolute tolerance $\\mathrm{atol}$ and relative tolerance $\\mathrm{rtol}$. The exponent in the step-size update must be justified from first principles using the local error order. Use a safety factor. The controller must rely on $\\varphi_k$ evaluations required by the embedded ERK pair and must avoid additional, unnecessary function evaluations. Angles appearing in trigonometric functions must be treated in radians.\n\n3. Implement a complete, runnable program that integrates the following test suite using your derived embedded ERK pair and adaptive controller. In all cases, $A$ is diagonal, so the matrix functions $e^{h A}$, $\\varphi_1(h A)$, and $\\varphi_2(h A)$ act element-wise on vectors. For numerical stability, when a diagonal entry $z$ of $h A$ is small, use the first few terms of the Taylor series to evaluate $\\varphi_1(z)$ and $\\varphi_2(z)$.\n\n   Test cases (angles in radians):\n   - Case $1$ (happy path, stiff linear part): $A = [-10]$ (a $1 \\times 1$ diagonal matrix), $g(y,t) = \\sin(t)$, $t_0 = 0$, $t_{\\text{end}} = 1$, $y_0 = [1]$, $\\mathrm{rtol} = 10^{-6}$, $\\mathrm{atol} = 10^{-12}$.\n   - Case $2$ (boundary, zero linear part and nonlinear growth): $A = [0]$, $g(y,t) = y^2$, $t_0 = 0$, $t_{\\text{end}} = 0.5$, $y_0 = [0.1]$, $\\mathrm{rtol} = 10^{-7}$, $\\mathrm{atol} = 10^{-12}$.\n   - Case $3$ (edge, mixed stiff/nonstiff components): $A = \\mathrm{diag}([-5, 0])$, $g(y,t) = [0, \\cos(t)]$, $t_0 = 0$, $t_{\\text{end}} = 2$, $y_0 = [2, 0]$, $\\mathrm{rtol} = 10^{-7}$, $\\mathrm{atol} = 10^{-12}$.\n\n   For verification, compare the numerical solution $y(t_{\\text{end}})$ with the exact solution at $t_{\\text{end}}$:\n   - Case $1$: Solve $y'(t) = -10 y(t) + \\sin(t)$ with $y(0) = 1$. The exact solution is\n     $$\n     y(t) = e^{-10 t}\\left(y_0 + \\frac{1}{101}\\right) + \\frac{10 \\sin(t) - \\cos(t)}{101}.\n     $$\n   - Case $2$: Solve $y'(t) = y(t)^2$ with $y(0) = 0.1$. The exact solution is\n     $$\n     y(t) = \\frac{y_0}{1 - y_0 t}.\n     $$\n   - Case $3$: Solve $y_1'(t) = -5 y_1(t)$ with $y_1(0) = 2$ and $y_2'(t) = \\cos(t)$ with $y_2(0) = 0$. The exact solution is\n     $$\n     y_1(t) = 2 e^{-5 t}, \\quad y_2(t) = \\sin(t).\n     $$\n\n   Your program must compute the absolute error in the infinity norm at $t_{\\text{end}}$ for each case. The final output format must be a single line containing the results as a comma-separated list enclosed in square brackets, for example, $[e_1,e_2,e_3]$, where $e_i$ is a floating-point number.\n\nEnsure scientific realism and self-consistency throughout. Do not use or cite shortcut formulas in the problem statement; start from the variation-of-constants formula and core definitions. The final program must be self-contained, require no user input or external files, and use angles in radians for trigonometric functions.", "solution": "The problem requires the derivation, analysis, and implementation of an embedded Exponential Runge-Kutta (ERK) method for solving semilinear initial value problems of the form $y'(t) = A y(t) + g(y(t), t)$ with the initial condition $y(t_0) = y_0$. The derivation must begin from the variation-of-constants formula.\n\nThe variation-of-constants formula gives the exact solution over a time step $h > 0$ from $t_n$ to $t_{n+1} = t_n + h$:\n$$\ny(t_{n+1}) = e^{h A} y(t_n) + \\int_{0}^{h} e^{(h - \\tau) A} \\, g(y(t_n + \\tau), t_n + \\tau) \\, d\\tau.\n$$\nBy using the substitution $s = \\tau/h$, the integral can be rewritten as:\n$$\ny(t_{n+1}) = e^{h A} y_n + h \\int_{0}^{1} e^{(1-s)h A} \\, g(y(t_n + sh), t_n + sh) \\, ds,\n$$\nwhere $y_n$ is the numerical approximation to $y(t_n)$. Exponential integrators are constructed by approximating the integral term. The accuracy of the method depends on the accuracy of the approximation of the integrand $g(y(t_n + sh), t_n + sh)$. The $\\varphi$-functions, defined as\n$$\n\\varphi_k(z) = \\int_0^1 e^{(1-s)z} \\frac{s^{k-1}}{(k-1)!} ds,\n$$\nnaturally arise from this formulation. For $k=0, 1, 2$, their common forms are:\n$$\n\\varphi_0(z) = e^z, \\quad \\varphi_1(z) = \\frac{e^z - 1}{z}, \\quad \\varphi_2(z) = \\frac{e^z - z - 1}{z^2}.\n$$\nTheir limiting values for $z \\to 0$ are given by their Taylor series expansions: $\\varphi_1(0) = 1$ and $\\varphi_2(0) = 1/2$.\n\n### Part 1: Derivation of the Embedded 1(2) ERK Pair\n\nAn embedded pair consists of two methods of different orders that share function evaluations, allowing for efficient error estimation. We will derive a first-order method and a second-order method.\n\n**First-Order Method ($y_1^{(1)}$)**\n\nThe simplest approximation for the integrand is to assume it is constant over the interval $[t_n, t_{n+1}]$:\n$$\ng(y(t_n + sh), t_n + sh) \\approx g(y_n, t_n) \\equiv g_n.\n$$\nSubstituting this into the integral form gives:\n$$\ny_1^{(1)} = \\varphi_0(hA) y_n + h \\int_{0}^{1} e^{(1-s)hA} g_n \\, ds.\n$$\nThe integral is $g_n$ (which is constant with respect to $s$) times $\\int_0^1 e^{(1-s)hA} ds$. By definition, this integral is $\\varphi_1(hA)$. Thus, the first-order ERK method, also known as the Exponential Euler or ETD1 method, is:\n$$\ny_1^{(1)} = \\varphi_0(hA) y_n + h \\varphi_1(hA) g_n.\n$$\nTo verify its order, we compare the Taylor expansion of $y_1^{(1)}$ with the exact solution $y(t_n+h)$. Let $z = hA$.\n$$\n\\varphi_0(z) = I + z + \\frac{z^2}{2} + O(z^3) \\implies \\varphi_0(hA) = I + hA + \\frac{h^2 A^2}{2} + O(h^3).\n$$\n$$\n\\varphi_1(z) = I + \\frac{z}{2} + \\frac{z^2}{6} + O(z^3) \\implies h \\varphi_1(hA) = hI + \\frac{h^2 A}{2} + \\frac{h^3 A^2}{6} + O(h^4).\n$$\nSubstituting these into the formula for $y_1^{(1)}$:\n$$\ny_1^{(1)} = \\left(I + hA + \\frac{h^2A^2}{2}\\right)y_n + \\left(hI + \\frac{h^2A}{2}\\right)g_n + O(h^3)\n$$\n$$\ny_1^{(1)} = y_n + h(Ay_n + g_n) + \\frac{h^2}{2}(A(Ay_n+g_n)) + O(h^3) = y_n + h y'(t_n) + \\frac{h^2}{2} A y'(t_n) + O(h^3).\n$$\nThe exact solution expands as:\n$$\ny(t_n+h) = y_n + h y'(t_n) + \\frac{h^2}{2} y''(t_n) + O(h^3).\n$$\nwhere $y''(t) = \\frac{d}{dt}(Ay+g) = A y' + \\frac{dg}{dt}$. At $t_n$, $y''(t_n) = A y'(t_n) + \\frac{dg}{dt}|_n$.\nThe local truncation error is:\n$$\nLTE_1 = y(t_n+h) - y_1^{(1)} = \\frac{h^2}{2} (y''(t_n) - A y'(t_n)) + O(h^3) = \\frac{h^2}{2}\\frac{dg}{dt}\\Big|_n + O(h^3).\n$$\nThe local error is $O(h^2)$, so the method is of order $1$.\n\n**Second-Order Method ($y_1^{(2)}$)**\n\nTo achieve second-order accuracy, we need a better approximation for the integrand $g$. We can use a predictor-corrector approach, in the spirit of Heun's method. We use the first-order solution $y_1^{(1)}$ as a predictor for the state at $t_{n+1}$:\n$$\ng_1 = g(y_1^{(1)}, t_n+h).\n$$\nWe then approximate the integrand $g(y(t_n + sh), t_n + sh)$ by a linear interpolant between $g_n = g(y_n, t_n)$ and our approximation $g_1$:\n$$\ng(y(t_n + sh), t_n + sh) \\approx (1-s)g_n + s g_1.\n$$\nSubstituting this into the integral form yields the second-order update $y_1^{(2)}$:\n$$\ny_1^{(2)} = \\varphi_0(hA) y_n + h \\int_{0}^{1} e^{(1-s)hA} ((1-s)g_n + s g_1) \\, ds\n$$\n$$\ny_1^{(2)} = \\varphi_0(hA) y_n + h g_n \\int_{0}^{1} (1-s)e^{(1-s)hA} ds + h g_1 \\int_{0}^{1} s e^{(1-s)hA} ds.\n$$\nUsing the integral definitions of the $\\varphi$-functions, the second integral is identified as $\\varphi_2(hA)$. For the first integral, we use the identity $\\int_0^1 (1-s) e^{(1-s)z} ds = \\varphi_1(z) - \\varphi_2(z)$. Thus, the method becomes:\n$$\ny_1^{(2)} = \\varphi_0(hA) y_n + h (\\varphi_1(hA) - \\varphi_2(hA)) g_n + h \\varphi_2(hA) g_1.\n$$\nWe can rewrite this by reusing the first-order result.\n$$\ny_1^{(1)} = \\varphi_0(hA) y_n + h \\varphi_1(hA) g_n.\n$$\nSo,\n$$\ny_1^{(2)} = (\\varphi_0(hA) y_n + h \\varphi_1(hA) g_n) - h \\varphi_2(hA) g_n + h \\varphi_2(hA) g_1\n$$\n$$\ny_1^{(2)} = y_1^{(1)} + h \\varphi_2(hA) (g_1 - g_n).\n$$\nThis is an elegant form that highlights the \"correction\" term. This method is a second-order ERK method, often called ETD2-RK. The linear approximation of $g$ has an error of order $O(h^2)$, which, when integrated, leads to a local truncation error of $O(h^3)$. Thus, the method is of order $2$.\n\nThe embedded pair is therefore:\n1. Compute $g_n = g(y_n, t_n)$.\n2. Compute the first-order solution: $y_1^{(1)} = \\varphi_0(hA) y_n + h \\varphi_1(hA) g_n$.\n3. Compute $g_1 = g(y_1^{(1)}, t_n+h)$.\n4. Compute the second-order solution: $y_1^{(2)} = y_1^{(1)} + h \\varphi_2(hA) (g_1 - g_n)$.\n\n### Part 2: Adaptive Step-Size Controller\n\nAn adaptive step-size controller adjusts the step size $h$ to keep the local error within a specified tolerance, ensuring both efficiency and accuracy.\n\n**Error Estimator:** The difference between the higher-order and lower-order solutions provides an estimate of the local truncation error of the lower-order method.\n$$\nE = y_1^{(2)} - y_1^{(1)} = h \\varphi_2(hA) (g(y_1^{(1)}, t_n+h) - g(y_n, t_n)).\n$$\nThe local truncation error of the first-order method is $LTE_1 = y(t_n+h) - y_1^{(1)} = O(h^2)$. The local truncation error of the second-order method is $LTE_2 = y(t_n+h) - y_1^{(2)} = O(h^3)$. The error estimate is $E = LTE_1 - LTE_2 = O(h^2)$. Thus, $E$ is an asymptotically correct estimator for the error of the first-order method.\n\n**Error Measure and Tolerance:** We use a mixed absolute-relative tolerance criterion. The scalar error measure $err$ is the infinity norm of the vector $E$:\n$$\nerr = ||E||_\\infty = ||y_1^{(2)} - y_1^{(1)}||_\\infty.\n$$\nThe acceptable tolerance $Tol$ is defined as:\n$$\nTol = \\mathrm{atol} + \\mathrm{rtol} \\cdot \\max(||y_n||_\\infty, ||y_1^{(2)}||_\\infty),\n$$\nwhere $\\mathrm{atol}$ is the absolute tolerance and $\\mathrm{rtol}$ is the relative tolerance.\n\n**Acceptance and Rejection Criteria:**\n*   If $err \\le Tol$, the step is accepted. We advance the solution: $y_{n+1} = y_1^{(2)}$ and $t_{n+1} = t_n + h$.\n*   If $err > Tol$, the step is rejected. We keep the current solution $y_n$ at time $t_n$ and retry the step with a smaller step size $h$.\n\n**Step-Size Update Formula:** The goal is to choose a new step size $h_{new}$ such that the estimated error for the new step, $err_{new}$, will be approximately equal to the desired tolerance $Tol$. The local error of the embedded method is of order $p+1$, where $p=1$ is the order of the lower-order method. So, $err \\approx C h^{p+1} = C h^2$ for some constant $C$.\nFor the current step: $err \\approx C h^2$.\nFor the new step: $Tol \\approx C h_{new}^2$.\nDividing the two equations gives $\\frac{h_{new}^2}{h^2} \\approx \\frac{Tol}{err}$, which leads to:\n$$\nh_{new} = h \\left( \\frac{Tol}{err} \\right)^{1/(p+1)} = h \\left( \\frac{Tol}{err} \\right)^{1/2}.\n$$\nThe exponent $1/2$ is justified because the error estimator has an order of $p+1=2$. To prevent overly aggressive step changes and to ensure reliability, a safety factor $S < 1$ (e.g., $S=0.9$) is introduced. Additionally, step size changes are typically bounded by minimum and maximum factors (e.g., $0.2$ and $5.0$). The final update formula is:\n$$\nh_{new} = S \\cdot h \\cdot \\left( \\frac{Tol}{err} \\right)^{1/2}.\n$$\n\n### Part 3: Implementation Details\n\nThe implementation follows the derived methods and controller. As specified, for a diagonal matrix $A=\\mathrm{diag}(\\lambda_i)$, the matrix functions $\\varphi_k(hA)$ are also diagonal, $\\varphi_k(hA) = \\mathrm{diag}(\\varphi_k(h\\lambda_i))$. Operations like $\\varphi_k(hA)v$ become element-wise vector products.\n\nFor numerical stability, the $\\varphi_k(z)$ functions are evaluated using their Taylor series expansions when $|z|$ is small (e.g., $|z| < 10^{-8}$), to avoid subtractive cancellation and division by a small number.\n$$\n\\varphi_1(z) = 1 + \\frac{z}{2} + \\frac{z^2}{6} + \\frac{z^3}{24} + \\dots\n$$\n$$\n\\varphi_2(z) = \\frac{1}{2} + \\frac{z}{6} + \\frac{z^2}{24} + \\frac{z^3}{120} + \\dots\n$$\nThe controller integrates the test cases until $t_{\\text{end}}$, and the final numerical solution is compared against the provided exact solution to compute the absolute error in the infinity norm.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef phi(z, k):\n    \"\"\"\n    Computes the phi_k functions for a vector of arguments z.\n    Handles small z using Taylor series expansion to avoid numerical issues.\n    \"\"\"\n    # Using a threshold for small |z|\n    small_z_threshold = 1e-8\n    \n    # Initialize results array\n    res = np.zeros_like(z, dtype=float)\n    \n    # Identify small and large z values\n    is_small = np.abs(z) < small_z_threshold\n    is_large = ~is_small\n\n    z_large = z[is_large]\n\n    if k == 0:\n        res[is_large] = np.exp(z_large)\n        # Taylor for small z: 1 + z + z^2/2 + ...\n        res[is_small] = 1.0 + z[is_small] * (1.0 + z[is_small] / 2.0)\n    elif k == 1:\n        res[is_large] = (np.exp(z_large) - 1.0) / z_large\n        # Taylor for small z: 1 + z/2 + z^2/6 + z^3/24 + ...\n        z_s = z[is_small]\n        res[is_small] = 1.0 + z_s / 2.0 * (1.0 + z_s / 3.0 * (1.0 + z_s / 4.0))\n    elif k == 2:\n        res[is_large] = (np.exp(z_large) - z_large - 1.0) / (z_large**2)\n        # Taylor for small z: 1/2 + z/6 + z^2/24 + z^3/120 + ...\n        z_s = z[is_small]\n        res[is_small] = 0.5 + z_s / 6.0 * (1.0 + z_s / 4.0 * (1.0 + z_s / 5.0))\n    else:\n        raise ValueError(\"k must be 0, 1, or 2.\")\n        \n    return res\n\ndef solve_one_case(A_diag, g_func, t0, tend, y0, rtol, atol):\n    \"\"\"\n    Solves one instance of the semilinear IVP using the adaptive ERK 1(2) method.\n    \"\"\"\n    t = t0\n    y = np.array(y0, dtype=float)\n    \n    # Controller parameters\n    h = 0.01  # Initial step size\n    safety_factor = 0.9\n    min_factor = 0.2\n    max_factor = 5.0\n    \n    while t < tend:\n        # Ensure the last step does not overshoot tend\n        if t + h > tend:\n            h = tend - t\n\n        # Perform one step of the embedded ERK 1(2) method\n        z = h * A_diag\n        \n        phi0_hA = phi(z, 0)\n        phi1_hA = phi(z, 1)\n        phi2_hA = phi(z, 2)\n        \n        g_n = g_func(y, t)\n        \n        # Lower-order solution (order 1)\n        y_low = phi0_hA * y + h * phi1_hA * g_n\n\n        # Higher-order solution (order 2)\n        g_1 = g_func(y_low, t + h)\n        y_high = y_low + h * phi2_hA * (g_1 - g_n)\n        \n        # Error estimation\n        err_est_vec = y_high - y_low\n        err_norm = np.linalg.norm(err_est_vec, ord=np.inf)\n        \n        # Tolerance calculation\n        y_norm = np.linalg.norm(y, ord=np.inf)\n        y_high_norm = np.linalg.norm(y_high, ord=np.inf)\n        tol = atol + rtol * max(y_norm, y_high_norm)\n        \n        if err_norm <= tol:\n            # Step is accepted\n            y = y_high\n            t += h\n            # Update step size for the next step (can increase)\n            if err_norm == 0.0:\n                 q = max_factor\n            else:\n                 q = safety_factor * (tol / err_norm)**0.5\n            h *= min(max_factor, max(min_factor, q))\n        else:\n            # Step is rejected\n            # Update step size (must decrease) and retry\n            q = safety_factor * (tol / err_norm)**0.5\n            h *= min(max_factor, max(min_factor, q))\n            # loop continues with new h, same t and y\n\n    return y\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the solver.\n    \"\"\"\n    test_cases = [\n        {\n            \"A_diag\": np.array([-10.0]),\n            \"g_func\": lambda y, t: np.array([math.sin(t)]),\n            \"t0\": 0.0, \"tend\": 1.0, \"y0\": np.array([1.0]),\n            \"rtol\": 1e-6, \"atol\": 1e-12,\n            \"exact_sol\": lambda t, y0: np.array([(y0[0] + 1.0/101.0) * math.exp(-10.0*t) + (10.0*math.sin(t) - math.cos(t))/101.0]),\n        },\n        {\n            \"A_diag\": np.array([0.0]),\n            \"g_func\": lambda y, t: y**2,\n            \"t0\": 0.0, \"tend\": 0.5, \"y0\": np.array([0.1]),\n            \"rtol\": 1e-7, \"atol\": 1e-12,\n            \"exact_sol\": lambda t, y0: np.array([y0[0] / (1.0 - y0[0]*t)]),\n        },\n        {\n            \"A_diag\": np.array([-5.0, 0.0]),\n            \"g_func\": lambda y, t: np.array([0.0, math.cos(t)]),\n            \"t0\": 0.0, \"tend\": 2.0, \"y0\": np.array([2.0, 0.0]),\n            \"rtol\": 1e-7, \"atol\": 1e-12,\n            \"exact_sol\": lambda t, y0: np.array([y0[0] * math.exp(-5.0*t), math.sin(t)]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        y_final = solve_one_case(\n            case[\"A_diag\"], case[\"g_func\"], case[\"t0\"], \n            case[\"tend\"], case[\"y0\"], case[\"rtol\"], case[\"atol\"]\n        )\n        \n        y_exact = case[\"exact_sol\"](case[\"tend\"], case[\"y0\"])\n        \n        error = np.linalg.norm(y_final - y_exact, ord=np.inf)\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3227510"}]}