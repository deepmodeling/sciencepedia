{"hands_on_practices": [{"introduction": "This first practice is a fundamental exercise designed to build your core understanding of how control variates work from the ground up. By estimating a simple definite integral, you will derive the optimal control variate coefficient by directly minimizing the estimator's variance. This foundational problem ([@problem_id:3218918]) will solidify your grasp of the underlying statistical principles and demonstrate the potential for significant variance reduction.", "problem": "Consider estimating the definite integral $I = \\int_{0}^{1} x^{2} \\, dx$ using the Monte Carlo (MC) method with $n$ independent and identically distributed samples $U_{1}, \\dots, U_{n}$ drawn from the $\\text{Uniform}[0,1]$ distribution. Define the plain MC estimator $\\hat{I}_{\\text{MC}} = \\frac{1}{n} \\sum_{i=1}^{n} X_{i}$ where $X_{i} = U_{i}^{2}$. Now apply the control variate (CV) technique using the control variate $Y = U$ with known mean $\\mathbb{E}[Y] = \\frac{1}{2}$, and define the adjusted sample $Z_{c} = X - c\\,(Y - \\mathbb{E}[Y])$ and the corresponding estimator $\\hat{I}_{\\text{CV}}(c) = \\frac{1}{n} \\sum_{i=1}^{n} Z_{c,i}$. \n\nStarting from the definitions of expectation, variance, and covariance, and using fundamental calculus and algebra, derive the value $c^{\\star}$ that minimizes $\\operatorname{Var}(Z_{c})$, then compute the variance reduction factor\n$$\nR = \\frac{\\operatorname{Var}\\!\\big(\\hat{I}_{\\text{CV}}(c^{\\star})\\big)}{\\operatorname{Var}\\!\\big(\\hat{I}_{\\text{MC}}\\big)}.\n$$\nProvide the exact value of $R$ as a single rational number. No rounding is required; express the final answer as a rational number.", "solution": "The user wants to find the optimal control variate coefficient and the resulting variance reduction for a Monte Carlo integration problem.\n\n### Step 1: Extract Givens\n- The integral to estimate is $I = \\int_{0}^{1} x^{2} \\, dx$.\n- The sampling distribution is $U_i \\sim \\text{Uniform}[0,1]$ for $i = 1, \\dots, n$.\n- The plain Monte Carlo estimator is $\\hat{I}_{\\text{MC}} = \\frac{1}{n} \\sum_{i=1}^{n} X_{i}$, with $X_i = U_i^2$.\n- The control variate is $Y = U$, with known mean $\\mathbb{E}[Y] = \\frac{1}{2}$.\n- The control variate random variable is $Z_c = X - c(Y - \\mathbb{E}[Y])$.\n- The control variate estimator is $\\hat{I}_{\\text{CV}}(c) = \\frac{1}{n} \\sum_{i=1}^{n} Z_{c,i}$.\n- The objective is to find the value $c^{\\star}$ that minimizes $\\operatorname{Var}(Z_c)$ and then compute the variance reduction factor $R = \\frac{\\operatorname{Var}\\!\\big(\\hat{I}_{\\text{CV}}(c^{\\star})\\big)}{\\operatorname{Var}\\!\\big(\\hat{I}_{\\text{MC}}\\big)}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a well-defined, standard exercise in the field of numerical methods, specifically Monte Carlo techniques.\n- **Scientifically Grounded:** The problem is based on established principles of probability theory, statistics, and calculus. All concepts (Monte Carlo integration, control variates, variance, covariance) are standard.\n- **Well-Posed:** The problem provides all necessary information to derive a unique solution for $c^{\\star}$ and $R$. The objective is clear and unambiguous.\n- **Objective:** The problem is stated in precise mathematical language, free from subjective or qualitative claims.\n\nThe problem does not violate any of the invalidity criteria. It is a complete, consistent, and solvable problem within the scope of scientific computing.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n### Solution Derivation\n\nThe goal is to find the optimal coefficient $c^{\\star}$ that minimizes the variance of the control variate estimator, and then to compute the variance reduction factor.\n\nThe random variable for the control variate estimator is $Z_c = X - c(Y - \\mathbb{E}[Y])$. Its variance is a function of $c$. Since $\\mathbb{E}[Y]$ is a constant, it does not affect the variance.\n$$\n\\operatorname{Var}(Z_c) = \\operatorname{Var}(X - cY)\n$$\nUsing the properties of variance, we can expand this expression:\n$$\n\\operatorname{Var}(Z_c) = \\operatorname{Var}(X) + \\operatorname{Var}(cY) - 2\\operatorname{Cov}(X, cY)\n$$\n$$\n\\operatorname{Var}(Z_c) = \\operatorname{Var}(X) + c^2\\operatorname{Var}(Y) - 2c\\operatorname{Cov}(X, Y)\n$$\nThis expression is a quadratic function of $c$. To find the value $c^{\\star}$ that minimizes this variance, we take the derivative with respect to $c$ and set it to $0$:\n$$\n\\frac{d}{dc}\\operatorname{Var}(Z_c) = 2c\\operatorname{Var}(Y) - 2\\operatorname{Cov}(X, Y)\n$$\nSetting the derivative to zero yields:\n$$\n2c^{\\star}\\operatorname{Var}(Y) - 2\\operatorname{Cov}(X, Y) = 0\n$$\n$$\nc^{\\star} = \\frac{\\operatorname{Cov}(X, Y)}{\\operatorname{Var}(Y)}\n$$\nTo compute $c^{\\star}$, we must calculate $\\operatorname{Var}(Y)$ and $\\operatorname{Cov}(X, Y)$. This requires computing moments of the random variables $X = U^2$ and $Y = U$, where $U \\sim \\text{Uniform}[0,1]$.\n\nThe $k$-th moment of a random variable $U \\sim \\text{Uniform}[0,1]$ is given by:\n$$\n\\mathbb{E}[U^k] = \\int_{0}^{1} u^k \\cdot 1 \\, du = \\left[ \\frac{u^{k+1}}{k+1} \\right]_{0}^{1} = \\frac{1}{k+1}\n$$\nFirst, let's compute the moments and variance of $Y=U$.\n$$\n\\mathbb{E}[Y] = \\mathbb{E}[U] = \\frac{1}{1+1} = \\frac{1}{2}\n$$\n$$\n\\mathbb{E}[Y^2] = \\mathbb{E}[U^2] = \\frac{1}{2+1} = \\frac{1}{3}\n$$\nThe variance of $Y$ is:\n$$\n\\operatorname{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2 = \\frac{1}{3} - \\left(\\frac{1}{2}\\right)^2 = \\frac{1}{3} - \\frac{1}{4} = \\frac{4-3}{12} = \\frac{1}{12}\n$$\nNext, we compute the moments and variance of $X=U^2$.\n$$\n\\mathbb{E}[X] = \\mathbb{E}[U^2] = \\frac{1}{2+1} = \\frac{1}{3}\n$$\nThis is the true value of the integral $I = \\int_0^1 x^2 dx$.\n$$\n\\mathbb{E}[X^2] = \\mathbb{E}[(U^2)^2] = \\mathbb{E}[U^4] = \\frac{1}{4+1} = \\frac{1}{5}\n$$\nThe variance of $X$ is:\n$$\n\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 = \\frac{1}{5} - \\left(\\frac{1}{3}\\right)^2 = \\frac{1}{5} - \\frac{1}{9} = \\frac{9-5}{45} = \\frac{4}{45}\n$$\nNow, we compute the covariance between $X$ and $Y$.\n$$\n\\operatorname{Cov}(X, Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]\n$$\nThe term $\\mathbb{E}[XY]$ is:\n$$\n\\mathbb{E}[XY] = \\mathbb{E}[U^2 \\cdot U] = \\mathbb{E}[U^3] = \\frac{1}{3+1} = \\frac{1}{4}\n$$\nSo, the covariance is:\n$$\n\\operatorname{Cov}(X, Y) = \\frac{1}{4} - \\left(\\frac{1}{3}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4} - \\frac{1}{6} = \\frac{3-2}{12} = \\frac{1}{12}\n$$\nWe can now calculate the optimal coefficient $c^{\\star}$:\n$$\nc^{\\star} = \\frac{\\operatorname{Cov}(X, Y)}{\\operatorname{Var}(Y)} = \\frac{1/12}{1/12} = 1\n$$\nThe next step is to compute the variance reduction factor $R$.\nThe estimators $\\hat{I}_{\\text{MC}}$ and $\\hat{I}_{\\text{CV}}(c)$ are averages of $n$ i.i.d. samples. Therefore, their variances are related to the variances of the individual samples as follows:\n$$\n\\operatorname{Var}(\\hat{I}_{\\text{MC}}) = \\operatorname{Var}\\left(\\frac{1}{n}\\sum_{i=1}^n X_i\\right) = \\frac{1}{n^2} \\sum_{i=1}^n \\operatorname{Var}(X_i) = \\frac{n \\operatorname{Var}(X)}{n^2} = \\frac{\\operatorname{Var}(X)}{n}\n$$\n$$\n\\operatorname{Var}(\\hat{I}_{\\text{CV}}(c^{\\star})) = \\operatorname{Var}\\left(\\frac{1}{n}\\sum_{i=1}^n Z_{c^{\\star},i}\\right) = \\frac{\\operatorname{Var}(Z_{c^{\\star}})}{n}\n$$\nThe variance reduction factor $R$ is the ratio of these two variances:\n$$\nR = \\frac{\\operatorname{Var}(\\hat{I}_{\\text{CV}}(c^{\\star}))}{\\operatorname{Var}(\\hat{I}_{\\text{MC}})} = \\frac{\\operatorname{Var}(Z_{c^{\\star}})/n}{\\operatorname{Var}(X)/n} = \\frac{\\operatorname{Var}(Z_{c^{\\star}})}{\\operatorname{Var}(X)}\n$$\nWe need to find the minimized variance $\\operatorname{Var}(Z_{c^{\\star}})$. We substitute $c^{\\star}$ back into the expression for $\\operatorname{Var}(Z_c)$:\n$$\n\\operatorname{Var}(Z_{c^{\\star}}) = \\operatorname{Var}(X) + (c^{\\star})^2\\operatorname{Var}(Y) - 2c^{\\star}\\operatorname{Cov}(X, Y)\n$$\nSubstituting $c^{\\star} = \\frac{\\operatorname{Cov}(X, Y)}{\\operatorname{Var}(Y)}$:\n$$\n\\operatorname{Var}(Z_{c^{\\star}}) = \\operatorname{Var}(X) + \\left(\\frac{\\operatorname{Cov}(X, Y)}{\\operatorname{Var}(Y)}\\right)^2 \\operatorname{Var}(Y) - 2\\frac{\\operatorname{Cov}(X, Y)}{\\operatorname{Var}(Y)}\\operatorname{Cov}(X, Y)\n$$\n$$\n\\operatorname{Var}(Z_{c^{\\star}}) = \\operatorname{Var}(X) + \\frac{\\operatorname{Cov}(X, Y)^2}{\\operatorname{Var}(Y)} - 2\\frac{\\operatorname{Cov}(X, Y)^2}{\\operatorname{Var}(Y)} = \\operatorname{Var}(X) - \\frac{\\operatorname{Cov}(X, Y)^2}{\\operatorname{Var}(Y)}\n$$\nNow, we substitute the numerical values we calculated:\n$$\n\\operatorname{Var}(Z_{c^{\\star}}) = \\frac{4}{45} - \\frac{(1/12)^2}{1/12} = \\frac{4}{45} - \\frac{1}{12}\n$$\nTo subtract the fractions, we find a common denominator, which is $180$:\n$$\n\\operatorname{Var}(Z_{c^{\\star}}) = \\frac{4 \\cdot 4}{180} - \\frac{1 \\cdot 15}{180} = \\frac{16 - 15}{180} = \\frac{1}{180}\n$$\nFinally, we compute the variance reduction factor $R$:\n$$\nR = \\frac{\\operatorname{Var}(Z_{c^{\\star}})}{\\operatorname{Var}(X)} = \\frac{1/180}{4/45} = \\frac{1}{180} \\cdot \\frac{45}{4} = \\frac{45}{720}\n$$\nSimplifying the fraction:\n$$\nR = \\frac{1 \\cdot 45}{16 \\cdot 45} = \\frac{1}{16}\n$$\nThe variance has been reduced by a factor of $16$.", "answer": "$$\n\\boxed{\\frac{1}{16}}\n$$", "id": "3218918"}, {"introduction": "Moving beyond basic applications, this exercise challenges you to think more deeply about the relationship between a function and its control. You will construct a scenario where the standard linear correlation is zero, suggesting a poor control variate, yet a clever non-linear transformation yields a massive improvement in efficiency. This practice ([@problem_id:3218904]) is crucial for developing the intuition needed to select or engineer highly effective control variates, especially in complex, non-linear problems.", "problem": "Consider a Monte Carlo estimation problem for the mean $\\mu = \\mathbb{E}[X]$ where you have access to samples of two random variables $X$ and $Y$. You will construct a joint distribution of $(X,Y)$ for which the Pearson correlation coefficient $\\rho(X,Y)$ is zero, yet a non-linear control variate $Z = g(Y)$ leads to a very large variance reduction. Use only foundational definitions of expectation, variance, covariance, and properties of the normal distribution.\n\nLet $Y \\sim \\mathcal{N}(0,1)$ and $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2})$, independent of $Y$, with $\\sigma^{2} = 0.01$. Define\n$$\nX = (Y^{2} - 1) + \\varepsilon\n$$\nand consider the non-linear control variate\n$$\nZ = g(Y) = Y^{2} - 1,\n$$\nwhose mean $\\mathbb{E}[Z]$ is known from the distribution of $Y$.\n\nTasks:\n- Using the core definitions of covariance and properties of moments of the standard normal distribution, verify that the covariance $\\operatorname{Cov}(X,Y)$ equals $0$, and hence the correlation $\\rho(X,Y)$ equals $0$.\n- Starting from the definition of variance and covariance and without assuming any particular formula, derive the value of the coefficient $\\beta$ that minimizes the variance of the adjusted estimator for $\\mu$ given by $X - \\beta(Z - \\mathbb{E}[Z])$.\n- Compute the per-sample variance of the adjusted estimator at the optimal choice of $\\beta$, and the variance reduction factor\n$$\nR = \\frac{\\operatorname{Var}(X)}{\\operatorname{Var}\\big(X - \\beta^{\\star}(Z - \\mathbb{E}[Z])\\big)}.\n$$\n\nProvide your final answer as the ordered pair $(\\beta^{\\star}, R)$ in exact form. No rounding is required. The final answer must be a calculation as specified.", "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It constitutes a formal exercise in probability theory and numerical methods. All necessary information is provided, and the tasks are well-defined. We may proceed with the solution.\n\nThe problem asks for three tasks related to a Monte Carlo estimation scenario. We are given the random variables $Y \\sim \\mathcal{N}(0,1)$ and $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2})$ with $\\sigma^{2} = 0.01$. The random variables $Y$ and $\\varepsilon$ are independent. The variable of interest is $X = (Y^{2} - 1) + \\varepsilon$, and the control variate is $Z = g(Y) = Y^{2} - 1$.\n\nFirst, we establish the necessary moments of the standard normal variable $Y$.\nThe probability density function of $Y$ is symmetric about $0$. Therefore, all odd moments of $Y$ are zero.\n$\\mathbb{E}[Y] = 0$\n$\\mathbb{E}[Y^3] = 0$\nThe even moments are well-known. The second moment is the variance, since the mean is zero:\n$\\mathbb{E}[Y^2] = \\operatorname{Var}(Y) = 1$\nThe fourth moment of a standard normal distribution is:\n$\\mathbb{E}[Y^4] = 3$\n\nWith these, we can determine the properties of $X$ and $Z$.\nThe mean of $Z$ is:\n$\\mathbb{E}[Z] = \\mathbb{E}[Y^2 - 1] = \\mathbb{E}[Y^2] - 1 = 1 - 1 = 0$.\nThe mean of $X$, which is the quantity $\\mu$ to be estimated, is:\n$\\mu = \\mathbb{E}[X] = \\mathbb{E}[(Y^2 - 1) + \\varepsilon] = \\mathbb{E}[Y^2 - 1] + \\mathbb{E}[\\varepsilon] = \\mathbb{E}[Z] + 0 = 0$.\n\n**Task 1: Verify $\\operatorname{Cov}(X,Y) = 0$**\n\nThe covariance is defined as $\\operatorname{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]$.\nWe have $\\mathbb{E}[Y] = 0$, so the second term vanishes:\n$\\operatorname{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X] \\cdot 0 = \\mathbb{E}[XY]$.\nWe compute $\\mathbb{E}[XY]$ by substituting the definition of $X$:\n$\\mathbb{E}[XY] = \\mathbb{E}[((Y^2 - 1) + \\varepsilon)Y] = \\mathbb{E}[Y^3 - Y + \\varepsilon Y]$.\nBy linearity of expectation:\n$\\mathbb{E}[XY] = \\mathbb{E}[Y^3] - \\mathbb{E}[Y] + \\mathbb{E}[\\varepsilon Y]$.\nAs established, $\\mathbb{E}[Y^3] = 0$ and $\\mathbb{E}[Y] = 0$. For the last term, since $\\varepsilon$ and $Y$ are independent, $\\mathbb{E}[\\varepsilon Y] = \\mathbb{E}[\\varepsilon]\\mathbb{E}[Y]$. We know $\\mathbb{E}[\\varepsilon]=0$ and $\\mathbb{E}[Y]=0$, so $\\mathbb{E}[\\varepsilon Y] = 0 \\cdot 0 = 0$.\nTherefore, $\\mathbb{E}[XY] = 0 - 0 + 0 = 0$.\nThis confirms that $\\operatorname{Cov}(X,Y) = 0$.\nThe Pearson correlation coefficient is $\\rho(X,Y) = \\frac{\\operatorname{Cov}(X,Y)}{\\sqrt{\\operatorname{Var}(X)\\operatorname{Var}(Y)}}$. Since the numerator is $0$ and the variances are non-zero (as will be shown), $\\rho(X,Y) = 0$.\n\n**Task 2: Derive the optimal coefficient $\\beta^{\\star}$**\n\nWe are tasked to find the value of $\\beta$, denoted $\\beta^{\\star}$, that minimizes the variance of the adjusted estimator $X_{\\beta} = X - \\beta(Z - \\mathbb{E}[Z])$. Let $V(\\beta) = \\operatorname{Var}(X_{\\beta})$.\nSince $\\mathbb{E}[Z] = 0$, the estimator is $X_{\\beta} = X - \\beta Z$.\nThe variance to be minimized is:\n$V(\\beta) = \\operatorname{Var}(X - \\beta Z)$.\nUsing the properties of variance, where $\\beta$ is a constant:\n$V(\\beta) = \\operatorname{Var}(X) + \\operatorname{Var}(-\\beta Z) + 2\\operatorname{Cov}(X, -\\beta Z)$.\n$V(\\beta) = \\operatorname{Var}(X) + \\beta^{2}\\operatorname{Var}(Z) - 2\\beta\\operatorname{Cov}(X, Z)$.\nThis is a quadratic function of $\\beta$. To find the minimum, we compute the derivative with respect to $\\beta$ and set it to zero:\n$\\frac{d V}{d\\beta} = \\frac{d}{d\\beta} \\left( \\operatorname{Var}(X) + \\beta^{2}\\operatorname{Var}(Z) - 2\\beta\\operatorname{Cov}(X, Z) \\right) = 2\\beta\\operatorname{Var}(Z) - 2\\operatorname{Cov}(X, Z)$.\nSetting the derivative to zero:\n$2\\beta\\operatorname{Var}(Z) - 2\\operatorname{Cov}(X, Z) = 0$.\nSolving for the optimal coefficient $\\beta^{\\star}$:\n$\\beta^{\\star} = \\frac{\\operatorname{Cov}(X, Z)}{\\operatorname{Var}(Z)}$.\nThe second derivative $\\frac{d^2 V}{d\\beta^2} = 2\\operatorname{Var}(Z)$ is positive since variance is non-negative (and non-zero in this case), confirming that this is a minimum.\n\n**Task 3: Compute $\\beta^{\\star}$ and the variance reduction factor $R$**\n\nTo compute $\\beta^{\\star}$, we need to calculate $\\operatorname{Var}(Z)$ and $\\operatorname{Cov}(X,Z)$.\nFirst, we find the variance of the control variate $Z$:\n$\\operatorname{Var}(Z) = \\mathbb{E}[Z^2] - (\\mathbb{E}[Z])^2$. Since $\\mathbb{E}[Z]=0$, $\\operatorname{Var}(Z) = \\mathbb{E}[Z^2]$.\n$\\operatorname{Var}(Z) = \\mathbb{E}[(Y^2-1)^2] = \\mathbb{E}[Y^4 - 2Y^2 + 1]$.\nUsing linearity of expectation and the moments of $Y$:\n$\\operatorname{Var}(Z) = \\mathbb{E}[Y^4] - 2\\mathbb{E}[Y^2] + 1 = 3 - 2(1) + 1 = 2$.\n\nNext, we calculate the covariance between $X$ and $Z$:\n$\\operatorname{Cov}(X,Z) = \\mathbb{E}[XZ] - \\mathbb{E}[X]\\mathbb{E}[Z]$.\nSince $\\mathbb{E}[X] = 0$ and $\\mathbb{E}[Z] = 0$, we have $\\operatorname{Cov}(X,Z) = \\mathbb{E}[XZ]$.\nSubstitute $X = Z + \\varepsilon$:\n$\\operatorname{Cov}(X,Z) = \\mathbb{E}[(Z+\\varepsilon)Z] = \\mathbb{E}[Z^2 + \\varepsilon Z] = \\mathbb{E}[Z^2] + \\mathbb{E}[\\varepsilon Z]$.\nWe know $\\mathbb{E}[Z^2] = \\operatorname{Var}(Z) = 2$.\nFor the term $\\mathbb{E}[\\varepsilon Z]$, we use the independence of $\\varepsilon$ and $Y$. Since $Z = Y^2-1$ is a function of $Y$, $Z$ and $\\varepsilon$ are also independent.\nTherefore, $\\mathbb{E}[\\varepsilon Z] = \\mathbb{E}[\\varepsilon]\\mathbb{E}[Z] = 0 \\cdot 0 = 0$.\nSo, $\\operatorname{Cov}(X,Z) = 2 + 0 = 2$.\n\nNow we can compute the optimal coefficient $\\beta^{\\star}$:\n$\\beta^{\\star} = \\frac{\\operatorname{Cov}(X, Z)}{\\operatorname{Var}(Z)} = \\frac{2}{2} = 1$.\n\nFinally, we compute the variance reduction factor $R = \\frac{\\operatorname{Var}(X)}{\\operatorname{Var}\\big(X - \\beta^{\\star}(Z - \\mathbb{E}[Z])\\big)}$.\nFirst, the numerator $\\operatorname{Var}(X)$:\n$X = Z + \\varepsilon$. Since $Z$ and $\\varepsilon$ are independent, the variance of their sum is the sum of their variances:\n$\\operatorname{Var}(X) = \\operatorname{Var}(Z) + \\operatorname{Var}(\\varepsilon) = 2 + \\sigma^2 = 2 + 0.01 = 2.01$.\n\nNext, the denominator, which is the variance of the adjusted estimator with the optimal $\\beta^{\\star}=1$ and $\\mathbb{E}[Z]=0$:\n$\\operatorname{Var}\\big(X - \\beta^{\\star}(Z - \\mathbb{E}[Z])\\big) = \\operatorname{Var}(X - 1 \\cdot (Z - 0)) = \\operatorname{Var}(X-Z)$.\nSubstitute $X = Z + \\varepsilon$:\n$\\operatorname{Var}(X-Z) = \\operatorname{Var}((Z+\\varepsilon) - Z) = \\operatorname{Var}(\\varepsilon)$.\nWe are given $\\operatorname{Var}(\\varepsilon) = \\sigma^2 = 0.01$.\n\nNow, we can compute the variance reduction factor $R$:\n$R = \\frac{\\operatorname{Var}(X)}{\\operatorname{Var}(X-Z)} = \\frac{2.01}{0.01} = 201$.\n\nThe final answer is the ordered pair $(\\beta^{\\star}, R)$.\n$\\beta^{\\star} = 1$\n$R = 201$\nThe final pair is $(1, 201)$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1  201 \\end{pmatrix}}\n$$", "id": "3218904"}, {"introduction": "This final practice demonstrates a powerful and common application of control variates in sensitivity analysis and computational science. You will estimate the change in an expected value, $E[f(X; \\theta_1)] - E[f(X; \\theta_0)]$, by using the baseline calculation at $\\theta_0$ as a control for the perturbed calculation at $\\theta_1$. This technique ([@problem_id:3218894]) is widely used in fields like finance and physics to efficiently compute derivatives or differences, showcasing the method's practical utility beyond simple integration.", "problem": "You are tasked with estimating the change in an expected value using Monte Carlo simulation with the method of control variates. The objective is to estimate the difference $E[f(X; \\theta_1)] - E[f(X; \\theta_0)]$ efficiently by using $f(X; \\theta_0)$ as a control variate for estimating $E[f(X; \\theta_1)]$. Work in the following mathematically defined setting.\n\nFundamental base:\n- Let $X$ be a random variable with the standard normal distribution, that is $X \\sim \\mathcal{N}(0, 1)$.\n- Let $f(x; \\theta) = \\exp(\\theta x)$ for a real parameter $\\theta$.\n- The expectation $E[f(X; \\theta)]$ is well-defined and finite for any real $\\theta$.\n- The expectation, variance, and covariance are defined as $E[Y] = \\int y \\, dP$, $\\operatorname{Var}(Y) = E\\left[(Y - E[Y])^2\\right]$, and $\\operatorname{Cov}(Y, Z) = E\\left[(Y - E[Y])(Z - E[Z])\\right]$ where $P$ is the underlying probability measure.\n\nTasks:\n1. Starting only from the linearity of expectation and the definitions of variance and covariance, derive the coefficient for a linear control variate estimator that uses $f(X; \\theta_0)$ as the control to estimate $E[f(X; \\theta_1)]$. Consider estimators of the form\n   $$\\widehat{\\mu}(\\beta) = \\frac{1}{N} \\sum_{i=1}^{N} \\left(Y_i - \\beta \\left(G_i - \\mu_0\\right)\\right),$$\n   where $Y_i = f(X_i; \\theta_1)$, $G_i = f(X_i; \\theta_0)$, $\\mu_0 = E[f(X; \\theta_0)]$, and $X_i$ are independent and identically distributed draws from $X$. Derive the choice of $\\beta$ that minimizes the variance of $\\widehat{\\mu}(\\beta)$ and justify that $\\widehat{\\mu}(\\beta)$ is unbiased for $E[f(X; \\theta_1)]$.\n2. Implement a Monte Carlo algorithm that:\n   - Generates $N$ independent samples $X_i$ from $X$.\n   - Computes the naive difference estimator $\\widehat{\\Delta}_{\\mathrm{naive}} = \\overline{Y} - \\overline{G}$, where $\\overline{Y} = \\frac{1}{N}\\sum_{i=1}^{N} Y_i$ and $\\overline{G} = \\frac{1}{N}\\sum_{i=1}^{N} G_i$.\n   - Computes the control variate estimate of $E[f(X; \\theta_1)]$ using the optimal coefficient derived in Task $1$, and then computes the difference estimate $\\widehat{\\Delta}_{\\mathrm{cv}} = \\widehat{\\mu}(\\widehat{\\beta}) - \\mu_0$, where $\\widehat{\\beta}$ is the sample-based estimate of the optimal coefficient.\n   - Uses the exact value of $\\mu_0$ and the exact difference $E[f(X; \\theta_1)] - E[f(X; \\theta_0)]$ whenever these quantities are available from well-tested formulas and facts. For $X \\sim \\mathcal{N}(0, 1)$ and $f(x; \\theta) = \\exp(\\theta x)$, use the known moment generating function of the normal distribution to provide exact values for comparison.\n3. Use the following test suite of parameter values to evaluate your algorithm. Each test case is a tuple $(\\theta_0, \\theta_1, N, \\text{seed})$:\n   - Case $1$ (happy path, close parameters): $(0.50, 0.60, 200000, 12345)$.\n   - Case $2$ (boundary, no change): $(0.75, 0.75, 50000, 54321)$.\n   - Case $3$ (less correlated, larger change): $(0.00, 1.50, 150000, 2023)$.\n   - Case $4$ (edge case, minimal samples): $(0.25, 0.30, 1, 7)$.\n   For each case, $N$ is the Monte Carlo sample size and $\\text{seed}$ is the pseudorandom number generator seed.\n4. Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case should produce a list of three floating-point numbers $[\\widehat{\\Delta}_{\\mathrm{naive}}, \\widehat{\\Delta}_{\\mathrm{cv}}, \\Delta_{\\mathrm{exact}}]$, and the final output should be the list of these lists in the same order as the test suite. For example, the output should look like\n   $$[[a_1, b_1, c_1], [a_2, b_2, c_2], [a_3, b_3, c_3], [a_4, b_4, c_4]],$$\n   where each $a_i$, $b_i$, and $c_i$ are floats. If the variance estimate for the control variate is zero due to $N$ being too small, set the control variate coefficient to $\\beta = 0$.\n\nNotes:\n- Angles are not involved; no angle unit is required.\n- No physical quantities are involved; no physical units are required.\n- The algorithm must be implemented so that all results are deterministic under the given seeds.", "solution": "The objective is to estimate the change in an expectation, $E[f(X; \\theta_1)] - E[f(X; \\theta_0)]$, using the method of control variates to improve the efficiency of a Monte Carlo simulation. We are given the function $f(x; \\theta) = \\exp(\\theta x)$ where $X$ is a standard normal random variable, $X \\sim \\mathcal{N}(0, 1)$. We will first derive the optimal coefficient for the control variate estimator and prove its properties, then detail the algorithm for its numerical implementation.\n\n### Task 1: Derivation of the Optimal Control Variate Coefficient\n\nWe are given a control variate estimator for $\\mu_1 = E[f(X; \\theta_1)]$:\n$$\n\\widehat{\\mu}(\\beta) = \\frac{1}{N} \\sum_{i=1}^{N} \\left(Y_i - \\beta \\left(G_i - \\mu_0\\right)\\right)\n$$\nwhere $Y_i = f(X_i; \\theta_1)$, $G_i = f(X_i; \\theta_0)$, and $\\mu_0 = E[G_i] = E[f(X; \\theta_0)]$. The samples $X_i$ are independent and identically distributed (i.i.d.).\n\n#### Unbiasedness of the Estimator\nFirst, we demonstrate that $\\widehat{\\mu}(\\beta)$ is an unbiased estimator of $\\mu_1$ for any constant coefficient $\\beta$. We compute the expectation of $\\widehat{\\mu}(\\beta)$:\n$$\nE[\\widehat{\\mu}(\\beta)] = E\\left[\\frac{1}{N} \\sum_{i=1}^{N} \\left(Y_i - \\beta \\left(G_i - \\mu_0\\right)\\right)\\right]\n$$\nBy the linearity of expectation, we can move the expectation operator inside the sum:\n$$\nE[\\widehat{\\mu}(\\beta)] = \\frac{1}{N} \\sum_{i=1}^{N} E\\left[Y_i - \\beta \\left(G_i - \\mu_0\\right)\\right]\n$$\nApplying linearity again to the term inside the summation:\n$$\nE\\left[Y_i - \\beta \\left(G_i - \\mu_0\\right)\\right] = E[Y_i] - \\beta E[G_i - \\mu_0]\n$$\nBy definition, $E[Y_i] = \\mu_1$ and $E[G_i] = \\mu_0$. Therefore:\n$$\nE[G_i - \\mu_0] = E[G_i] - E[\\mu_0] = \\mu_0 - \\mu_0 = 0\n$$\nSubstituting this back, we get:\n$$\nE\\left[Y_i - \\beta \\left(G_i - \\mu_0\\right)\\right] = \\mu_1 - \\beta(0) = \\mu_1\n$$\nSince this holds for each sample $i=1, \\dots, N$, the expectation of the estimator is:\n$$\nE[\\widehat{\\mu}(\\beta)] = \\frac{1}{N} \\sum_{i=1}^{N} \\mu_1 = \\frac{1}{N} (N \\mu_1) = \\mu_1\n$$\nThus, $\\widehat{\\mu}(\\beta)$ is an unbiased estimator of $\\mu_1$ regardless of the choice of $\\beta$.\n\n#### Variance Minimization\nNext, we find the value of $\\beta$ that minimizes the variance of $\\widehat{\\mu}(\\beta)$. The variance is:\n$$\n\\operatorname{Var}(\\widehat{\\mu}(\\beta)) = \\operatorname{Var}\\left(\\frac{1}{N} \\sum_{i=1}^{N} \\left(Y_i - \\beta \\left(G_i - \\mu_0\\right)\\right)\\right)\n$$\nSince the terms in the sum are i.i.d., the variance of the sample mean is $\\frac{1}{N}$ times the variance of a single term:\n$$\n\\operatorname{Var}(\\widehat{\\mu}(\\beta)) = \\frac{1}{N} \\operatorname{Var}\\left(Y_1 - \\beta \\left(G_1 - \\mu_0\\right)\\right)\n$$\nThe constant term $-\\beta \\mu_0$ does not affect the variance, so we can simplify this to:\n$$\n\\operatorname{Var}(\\widehat{\\mu}(\\beta)) = \\frac{1}{N} \\operatorname{Var}\\left(Y - \\beta G\\right)\n$$\nwhere $Y = f(X; \\theta_1)$ and $G = f(X; \\theta_0)$. Using the standard formula for the variance of a linear combination of random variables:\n$$\n\\operatorname{Var}(Y - \\beta G) = \\operatorname{Var}(Y) + \\operatorname{Var}(-\\beta G) + 2 \\operatorname{Cov}(Y, -\\beta G) = \\operatorname{Var}(Y) + \\beta^2 \\operatorname{Var}(G) - 2\\beta \\operatorname{Cov}(Y, G)\n$$\nTo minimize this variance with respect to $\\beta$, we take the derivative of this quadratic function of $\\beta$ and set it to zero:\n$$\n\\frac{d}{d\\beta} \\operatorname{Var}(Y - \\beta G) = \\frac{d}{d\\beta} \\left( \\operatorname{Var}(Y) + \\beta^2 \\operatorname{Var}(G) - 2\\beta \\operatorname{Cov}(Y, G) \\right) = 2\\beta \\operatorname{Var}(G) - 2 \\operatorname{Cov}(Y, G)\n$$\nSetting the derivative to zero gives the optimal coefficient $\\beta^*$:\n$$\n2\\beta^* \\operatorname{Var}(G) - 2 \\operatorname{Cov}(Y, G) = 0 \\implies \\beta^* = \\frac{\\operatorname{Cov}(Y, G)}{\\operatorname{Var}(G)}\n$$\nThe second derivative, $2\\operatorname{Var}(G)$, is non-negative, confirming that this value of $\\beta$ corresponds to a minimum (provided $\\operatorname{Var}(G)  0$).\n\n### Task 2: Algorithm and Implementation Details\n\nThe implementation requires computing three quantities for each test case: the naive difference estimate $\\widehat{\\Delta}_{\\mathrm{naive}}$, the control variate difference estimate $\\widehat{\\Delta}_{\\mathrm{cv}}$, and the exact difference $\\Delta_{\\mathrm{exact}}$.\n\n#### Exact Values from Moment Generating Function\nFor a standard normal random variable $X \\sim \\mathcal{N}(0, 1)$, the moment generating function (MGF) is $M_X(t) = E[\\exp(tX)] = \\exp(t^2/2)$. The expectation of our target function $f(x; \\theta) = \\exp(\\theta x)$ is therefore:\n$$\n\\mu_\\theta = E[f(X; \\theta)] = E[\\exp(\\theta X)] = M_X(\\theta) = \\exp(\\theta^2/2)\n$$\nUsing this, the required exact values are:\n- $\\mu_0 = E[f(X; \\theta_0)] = \\exp(\\theta_0^2/2)$\n- $\\mu_1 = E[f(X; \\theta_1)] = \\exp(\\theta_1^2/2)$\n- $\\Delta_{\\mathrm{exact}} = \\mu_1 - \\mu_0 = \\exp(\\theta_1^2/2) - \\exp(\\theta_0^2/2)$\n\n#### Estimators to be Computed\n1.  **Naive Difference Estimator**: This is the difference of the sample means of $Y$ and $G$:\n    $$\n    \\widehat{\\Delta}_{\\mathrm{naive}} = \\overline{Y} - \\overline{G} = \\left(\\frac{1}{N}\\sum_{i=1}^{N} Y_i\\right) - \\left(\\frac{1}{N}\\sum_{i=1}^{N} G_i\\right)\n    $$\n2.  **Control Variate Difference Estimator**: This estimator uses the control variate estimate of $\\mu_1$ and subtracts the exact value of $\\mu_0$. The optimal coefficient $\\beta^*$ is unknown and must be estimated from the samples. The sample-based estimate $\\widehat{\\beta}$ is:\n    $$\n    \\widehat{\\beta} = \\frac{\\widehat{\\operatorname{Cov}}(Y, G)}{\\widehat{\\operatorname{Var}}(G)} = \\frac{\\frac{1}{N-1}\\sum_{i=1}^N (Y_i - \\overline{Y})(G_i - \\overline{G})}{\\frac{1}{N-1}\\sum_{i=1}^N (G_i - \\overline{G})^2}\n    $$\n    The control variate estimate for $\\mu_1$ is $\\widehat{\\mu}_1(\\widehat{\\beta}) = \\overline{Y} - \\widehat{\\beta}(\\overline{G} - \\mu_0)$. The final difference estimator is:\n    $$\n    \\widehat{\\Delta}_{\\mathrm{cv}} = \\widehat{\\mu}_1(\\widehat{\\beta}) - \\mu_0 = \\overline{Y} - \\widehat{\\beta}(\\overline{G} - \\mu_0) - \\mu_0\n    $$\n\n#### Handling Edge Cases\nAs per the instructions, if the sample variance of the control variate, $\\widehat{\\operatorname{Var}}(G)$, is zero, the coefficient $\\widehat{\\beta}$ is set to $0$. This occurs if $N \\le 1$ or if all generated samples $G_i$ are identical (e.g., when $\\theta_0=0$). In this scenario, the control variate term vanishes, and the estimator $\\widehat{\\mu}_1(\\widehat{\\beta})$ reduces to the simple sample mean $\\overline{Y}$.\n\nThe procedure for each test case $(\\theta_0, \\theta_1, N, \\text{seed})$ is as follows:\n1.  Seed the random number generator with `seed`.\n2.  Generate $N$ samples $X_i \\sim \\mathcal{N}(0, 1)$.\n3.  Compute the vectors $Y_i = \\exp(\\theta_1 X_i)$ and $G_i = \\exp(\\theta_0 X_i)$.\n4.  Calculate sample means $\\overline{Y}$ and $\\overline{G}$.\n5.  Calculate $\\widehat{\\Delta}_{\\mathrm{naive}} = \\overline{Y} - \\overline{G}$.\n6.  Calculate $\\widehat{\\beta}$. If $N  1$ and $\\widehat{\\operatorname{Var}}(G)  0$, use the sample-based formula. Otherwise, set $\\widehat{\\beta} = 0$.\n7.  Calculate the exact value $\\mu_0 = \\exp(\\theta_0^2/2)$.\n8.  Calculate $\\widehat{\\Delta}_{\\mathrm{cv}} = \\overline{Y} - \\widehat{\\beta}(\\overline{G} - \\mu_0) - \\mu_0$.\n9.  Calculate $\\Delta_{\\mathrm{exact}} = \\exp(\\theta_1^2/2) - \\exp(\\theta_0^2/2)$.\n10. Collect the triplet $[\\widehat{\\Delta}_{\\mathrm{naive}}, \\widehat{\\Delta}_{\\mathrm{cv}}, \\Delta_{\\mathrm{exact}}]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the Monte Carlo simulation to estimate the change in an expected value\n    using both a naive estimator and a control variate estimator, and compares them\n    to the exact analytical solution.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (theta_0, theta_1, N, seed)\n    test_cases = [\n        (0.50, 0.60, 200000, 12345),\n        (0.75, 0.75, 50000, 54321),\n        (0.00, 1.50, 150000, 2023),\n        (0.25, 0.30, 1, 7),\n    ]\n\n    results = []\n    \n    for theta_0, theta_1, N, seed in test_cases:\n        # Set the seed for the pseudorandom number generator for reproducibility.\n        rng = np.random.default_rng(seed)\n        \n        # 1. Generate N independent samples from the standard normal distribution.\n        X_samples = rng.normal(loc=0.0, scale=1.0, size=N)\n        \n        # 2. Compute Y and G values.\n        # Y_i = f(X_i; theta_1)\n        # G_i = f(X_i; theta_0)\n        Y = np.exp(theta_1 * X_samples)\n        G = np.exp(theta_0 * X_samples)\n        \n        # 3. Compute sample means.\n        Y_bar = np.mean(Y)\n        G_bar = np.mean(G)\n        \n        # 4. Compute the naive difference estimator.\n        # delta_naive = E[Y] - E[G] estimated by mean(Y) - mean(G).\n        delta_naive = Y_bar - G_bar\n        \n        # 5. Compute the control variate estimator.\n        # First, estimate the optimal coefficient beta.\n        # beta_hat = Cov(Y, G) / Var(G).\n        beta_hat = 0.0\n        # The sample variance and covariance are undefined for N = 1.\n        # The problem also specifies to set beta=0 if the variance is zero.\n        if N > 1:\n            var_G = np.var(G, ddof=1)\n            # Avoid division by zero if Var(G) is zero (e.g., if theta_0 = 0).\n            if var_G > 0:\n                # np.cov with 1D arrays Y, G returns a 2x2 covariance matrix.\n                # cov_matrix[0, 1] is Cov(Y, G).\n                cov_YG = np.cov(Y, G, ddof=1)[0, 1]\n                beta_hat = cov_YG / var_G\n\n        # 6. Compute exact value for mu_0 = E[G]\n        # For X ~ N(0, 1), E[exp(t*X)] = exp(t^2 / 2).\n        mu_0_exact = np.exp(theta_0**2 / 2.0)\n        \n        # 7. Compute the control variate estimate for mu_1 = E[Y].\n        # mu1_cv = mean(Y) - beta_hat * (mean(G) - mu_0)\n        mu_1_cv = Y_bar - beta_hat * (G_bar - mu_0_exact)\n        \n        # 8. Compute the control variate difference estimator.\n        # delta_cv = mu1_cv - mu_0\n        delta_cv = mu_1_cv - mu_0_exact\n        \n        # 9. Compute the exact difference.\n        mu_1_exact = np.exp(theta_1**2 / 2.0)\n        delta_exact = mu_1_exact - mu_0_exact\n        \n        # 10. Store the triplet of results for the current test case.\n        results.append([delta_naive, delta_cv, delta_exact])\n\n    # Final print statement in the exact required format.\n    # The format is a string representation of a list of lists of floats.\n    output_str = '[' + ','.join([f\"[{a},{b},{c}]\" for a, b, c in results]) + ']'\n    print(output_str)\n\nsolve()\n```", "id": "3218894"}]}