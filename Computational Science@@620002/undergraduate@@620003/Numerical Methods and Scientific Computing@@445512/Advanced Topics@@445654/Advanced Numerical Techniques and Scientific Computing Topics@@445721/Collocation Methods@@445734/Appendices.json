{"hands_on_practices": [{"introduction": "This first exercise provides a hands-on introduction to the core principles of the collocation method. By working through a simple second-order boundary value problem, you will learn how to construct a trial function that satisfies the boundary conditions, define the residual, and use a single collocation point to determine the unknown parameter in your approximate solution. This practice is essential for understanding how collocation transforms a differential equation into a solvable algebraic equation. [@problem_id:2159883]", "problem": "An engineer is modeling the steady-state temperature distribution $y(x)$ along a thin, laterally insulated rod of unit length. The rod is subject to an internal heat source proportional to the square of the position $x$, and its temperature is fixed at both ends. The governing differential equation for this system is a second-order linear non-homogeneous Boundary Value Problem (BVP):\n\n$$y''(x) - y(x) = -x^2, \\quad \\text{for } x \\in [0, 1]$$\n\nwith boundary conditions:\n\n$$y(0) = 0, \\quad y(1) = 2$$\n\nTo find an approximate solution, the collocation method is employed. A single-parameter trial function $\\tilde{y}(x)$ is proposed, which is constructed to automatically satisfy the boundary conditions:\n\n$$\\tilde{y}(x) = 2x + c \\cdot x(1-x)$$\n\nwhere $c$ is an unknown constant coefficient to be determined.\n\nYour task is to use a single collocation point at $x_c = 1/2$ to determine the value of the parameter $c$. Then, with this value of $c$, calculate the maximum value of the absolute residual, $\\max_{x \\in [0, 1]} |R(x)|$, where the residual function $R(x)$ is defined as $R(x) = \\tilde{y}''(x) - \\tilde{y}(x) + x^2$.\n\nExpress your final answer as a single real number, rounded to three significant figures.", "solution": "The trial function is $\\tilde{y}(x) = 2x + c x(1-x) = (2+c)x - c x^{2}$, which satisfies the boundary conditions $\\tilde{y}(0)=0$ and $\\tilde{y}(1)=2$. Its derivatives are\n$$\n\\tilde{y}'(x) = (2+c) - 2c x, \\qquad \\tilde{y}''(x) = -2c.\n$$\nThe residual is defined by $R(x) = \\tilde{y}''(x) - \\tilde{y}(x) + x^{2}$, hence\n$$\nR(x) = -2c - \\big[(2+c)x - c x^{2}\\big] + x^{2} = -2c - (2+c)x + (c+1)x^{2}.\n$$\nUsing the collocation condition at $x_{c}=\\frac{1}{2}$, we impose $R(\\frac{1}{2})=0$:\n$$\n-2c - \\frac{2+c}{2} + \\frac{c+1}{4} = 0.\n$$\nWith a common denominator of $4$, this is\n$$\n\\frac{-8c - 2(2+c) + (c+1)}{4} = \\frac{-8c - 4 - 2c + c + 1}{4} = \\frac{-9c - 3}{4} = 0,\n$$\nso $-9c - 3 = 0$ and therefore $c = -\\frac{1}{3}$.\n\nSubstitute $c=-\\frac{1}{3}$ into $R(x)$:\n$$\nR(x) = -2(-\\frac{1}{3}) - (2 - \\frac{1}{3})x + (1 - \\frac{1}{3})x^{2} = \\frac{2}{3} - \\frac{5}{3}x + \\frac{2}{3}x^{2} = \\frac{1}{3}(2 - 5x + 2x^{2}).\n$$\nLet $q(x) = 2 - 5x + 2x^{2}$. Then $R(x) = \\frac{1}{3}q(x)$. The function $q$ is a quadratic with $q'(x) = -5 + 4x$, which is negative on $[0,1]$, so $q$ decreases monotonically on $[0,1]$. Moreover, $q(\\frac{1}{2})=0$ (consistent with the collocation). Thus on $[0,\\frac{1}{2}]$, $q \\ge 0$ and $|q|=q$ decreasing from $q(0)=2$ to $0$, while on $[\\frac{1}{2},1]$, $q \\le 0$ and $|q|=-q$ increasing from $0$ to $|q(1)|=1$. Hence\n$$\n\\max_{x \\in [0,1]} |q(x)| = 2 \\quad \\Rightarrow \\quad \\max_{x \\in [0,1]} |R(x)| = \\frac{1}{3} \\cdot 2 = \\frac{2}{3}.\n$$\nRounded to three significant figures, this is $0.667$.", "answer": "$$\\boxed{0.667}$$", "id": "2159883"}, {"introduction": "Moving beyond a single, arbitrary point, this practice explores the power of pseudospectral collocation methods, where a systematic choice of nodes can lead to highly accurate solutions. You will investigate how different families of Chebyshev nodes—specifically Gauss versus Gauss-Lobatto—affect the ability to resolve sharp boundary layers in a solution. This computational exercise highlights the crucial link between node distribution and approximation accuracy, a key concept in advanced numerical analysis. [@problem_id:3179398]", "problem": "You will implement a pseudospectral collocation method to study how endpoint clustering affects the resolution of boundary layers when approximating the solution of the ordinary differential equation (ODE) $u'(x)=\\alpha u(x)$ on the interval $[-1,1]$ with the boundary condition $u(1)=1$ (unitless). The exact solution is $u(x)=\\exp(\\alpha(x-1))$. You must contrast Chebyshev–Gauss nodes $x_k=\\cos(\\frac{(2k-1)\\pi}{2N})$ for $k=1,2,\\dots,N$ and Chebyshev–Gauss–Lobatto nodes $x_k=\\cos(\\frac{k\\pi}{N})$ for $k=0,1,\\dots,N$, where all angles are in radians. The goal is to assess how endpoint inclusion and clustering at $x=\\pm 1$ influences resolution of a thin boundary layer near $x=1$ when $\\alpha$ is large and positive.\n\nStart from the following fundamental base:\n- Polynomial interpolation and collocation: approximate $u(x)$ by a polynomial $p(x)$ that interpolates values at distinct nodes $\\{x_k\\}$; enforce the ODE at selected collocation nodes and the boundary condition at $x=1$.\n- The barycentric interpolation formula: for distinct nodes $\\{x_k\\}$ with barycentric weights $\\{w_k\\}$, the interpolant evaluated at $x$ is $$p(x)=\\frac{\\sum_{k=0}^{M-1}\\frac{w_k u_k}{x-x_k}}{\\sum_{k=0}^{M-1}\\frac{w_k}{x-x_k}},$$ where $u_k=p(x_k)$ and $M$ is the number of nodes.\n- The barycentric differentiation matrix $D\\in\\mathbb{R}^{M\\times M}$ defined by $$D_{ij}=\\begin{cases}\\frac{w_j}{w_i(x_i-x_j)},  i\\neq j \\\\ -\\sum_{j\\neq i}D_{ij},  i=j\\end{cases}$$ This yields $p'(x_i)=\\sum_{j=0}^{M-1}D_{ij} u_j$.\n\nImplement the following pseudospectral collocation designs:\n- For Chebyshev–Gauss–Lobatto nodes with $N+1$ nodes ($k=0,\\dots,N$), enforce $u(1)=1$ directly at the node $x_0=1$; enforce the ODE $u'(x)=\\alpha u(x)$ at the interior nodes $x_k$ for $k=1,\\dots,N$. This yields a linear system for the nodal values $\\{u_k\\}_{k=0}^N$ with $N+1$ equations.\n- For Chebyshev–Gauss nodes with $N$ nodes ($k=1,\\dots,N$) that do not include the endpoint, construct a square system by enforcing the ODE at $N-1$ nodes and replacing one ODE row with the boundary condition $u(1)=1$ expressed in terms of nodal values via barycentric evaluation. Specifically, for $x^\\ast=1$, define $\\phi_k=\\frac{w_k}{x^\\ast-x_k}$, $\\beta_k=\\frac{\\phi_k}{\\sum_{j=1}^N \\phi_j}$, and enforce $\\sum_{k=1}^N \\beta_k u_k=1$ as the boundary condition. Enforce the ODE at the remaining $N-1$ nodes via $p'(x_i)-\\alpha p(x_i)=0$. Choose the replaced ODE row to be the node with maximal $x_k$ to align the boundary constraint with the nearest node.\n\nUse known barycentric weights for the Chebyshev families:\n- For Chebyshev–Gauss nodes $x_k=\\cos(\\frac{(2k-1)\\pi}{2N})$ ($k=1,\\dots,N$), let $w_k=(-1)^{k-1}$.\n- For Chebyshev–Gauss–Lobatto nodes $x_k=\\cos(\\frac{k\\pi}{N})$ ($k=0,\\dots,N$), let $w_k=(-1)^k c_k$, where $c_0=c_N=\\tfrac{1}{2}$ and $c_k=1$ for $k=1,\\dots,N-1$.\n\nTo quantify resolution of the boundary layer near $x=1$, define a layer-focused evaluation grid of $m$ points with $\\delta=\\min(0.5,\\frac{8}{\\alpha})$, $x_j=1-\\delta+\\frac{j}{m-1}\\delta, \\quad j=0,1,\\dots,m-1,$ using $m=400$. Evaluate the polynomial approximants $p_{\\text{Gauss}}(x)$ and $p_{\\text{Lobatto}}(x)$ on this grid via the barycentric formula, and compute the maximum absolute errors $E_{\\text{Gauss}}=\\max_j\\left|p_{\\text{Gauss}}(x_j)-\\exp(\\alpha(x_j-1))\\right|$, $E_{\\text{Lobatto}}=\\max_j\\left|p_{\\text{Lobatto}}(x_j)-\\exp(\\alpha(x_j-1))\\right|$. Report the base-$10$ logarithm of the error ratio $\\log_{10}(E_{\\text{Gauss}}/E_{\\text{Lobatto}})$ for each test case.\n\nTest suite:\n- Case $1$: $(N,\\alpha)=(16,20)$.\n- Case $2$: $(N,\\alpha)=(16,100)$.\n- Case $3$: $(N,\\alpha)=(8,50)$.\n- Case $4$: $(N,\\alpha)=(32,100)$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each float in scientific notation with six digits after the decimal (e.g., $[1.234567\\mathrm{e}{-2},-3.210000\\mathrm{e}{0}]$). The line must aggregate the four test-case results in the order listed above.", "solution": "The user has provided a valid problem statement. I will proceed with a full solution.\n\nThe problem requires a comparative analysis of two pseudospectral collocation methods for solving the ordinary differential equation (ODE) $u'(x) = \\alpha u(x)$ on the domain $x \\in [-1, 1]$ with a boundary condition $u(1) = 1$. The exact solution is $u(x) = \\exp(\\alpha(x-1))$. For large positive $\\alpha$, this solution features a thin boundary layer near $x=1$, where the function value changes rapidly. The objective is to assess how two different choices of collocation nodes—Chebyshev–Gauss and Chebyshev–Gauss–Lobatto—affect the numerical accuracy in resolving this layer.\n\nThe core of a pseudospectral method is to approximate the unknown function $u(x)$ by a global polynomial $p(x)$ of degree $M-1$, where $M$ is the number of collocation nodes. The polynomial is defined by its values $\\{u_k = p(x_k)\\}$ at a set of distinct nodes $\\{x_k\\}$. The derivatives of the polynomial at these nodes, $p'(x_i)$, can be computed as a linear combination of the nodal values, $p'(x_i) = \\sum_{j} D_{ij} u_j$, where $D$ is the spectral differentiation matrix. By enforcing the ODE and boundary conditions at these nodes, we can construct a linear system for the unknown nodal values $\\{u_k\\}$.\n\nThe problem specifies using the barycentric interpolation formula to represent the polynomial and its derivative. The differentiation matrix $D$ is defined based on the nodes $\\{x_k\\}$ and their corresponding barycentric weights $\\{w_k\\}$ as:\n$$\nD_{ij} = \\begin{cases}\n\\dfrac{w_j}{w_i(x_i - x_j)},  i \\neq j \\\\\n-\\sum_{j \\neq i} D_{ij},  i=j\n\\end{cases}\n$$\nOnce the nodal values $\\{u_k\\}$ are found by solving the linear system, the polynomial approximant can be evaluated at any point $x$ using the barycentric formula:\n$$\np(x) = \\frac{\\sum_{k} \\frac{w_k u_k}{x - x_k}}{\\sum_{k} \\frac{w_k}{x - x_k}}\n$$\n\nWe will now detail the implementation for each set of nodes.\n\n### 1. Chebyshev–Gauss–Lobatto Method\n\nThis method uses $M = N+1$ nodes, including the endpoints $x=\\pm 1$.\n- **Nodes**: $x_k = \\cos\\left(\\frac{k\\pi}{N}\\right)$ for $k = 0, 1, \\dots, N$. Note that $x_0 = 1$ and $x_N = -1$.\n- **Barycentric Weights**: $w_k = (-1)^k c_k$, where $c_0 = c_N = \\frac{1}{2}$ and $c_k = 1$ for $k=1, \\dots, N-1$.\n\nA linear system of $N+1$ equations for the $N+1$ unknown nodal values $\\{u_k\\}_{k=0}^N$ is constructed as follows:\n- **Boundary Condition**: The condition $u(1)=1$ is enforced directly at the node $x_0 = 1$. This provides the first equation: $u_0 = 1$.\n- **ODE Collocation**: The ODE, in the form $u'(x) - \\alpha u(x) = 0$, is enforced at the nodes $x_k$ for $k = 1, \\dots, N$. For each such node $x_k$, we get an equation:\n$$\np'(x_k) - \\alpha p(x_k) = 0 \\implies \\sum_{j=0}^{N} D_{kj} u_j - \\alpha u_k = 0\n$$\nThis gives $N$ equations. Combined with the boundary condition, we have a square linear system $A_L u = b_L$, where $u = [u_0, u_1, \\dots, u_N]^T$. The matrix $A_L$ has $[1, 0, \\dots, 0]$ as its first row, and the subsequent rows are derived from the differentiation matrix $(D - \\alpha I)$. The right-hand side is $b_L = [1, 0, \\dots, 0]^T$. Solving this system yields the nodal values for the Lobatto approximant, $p_{\\text{Lobatto}}(x)$.\n\n### 2. Chebyshev–Gauss Method\n\nThis method uses $M=N$ nodes, which are interior to the interval $[-1, 1]$.\n- **Nodes**: $x_k = \\cos\\left(\\frac{(2k-1)\\pi}{2N}\\right)$ for $k = 1, \\dots, N$. For a 0-indexed implementation ($k=0, \\dots, N-1$), the formula becomes $x_k = \\cos\\left(\\frac{(2k+1)\\pi}{2N}\\right)$.\n- **Barycentric Weights**: $w_k = (-1)^{k-1}$ for $k=1, \\dots, N$. For a 0-indexed implementation, $w_k = (-1)^k$.\n\nA linear system of $N$ equations for the $N$ unknown nodal values $\\{u_k\\}_{k=1}^N$ is constructed. Since the boundary point $x=1$ is not a node, the condition $u(1)=1$ must be enforced via interpolation.\n- **Boundary Condition via Interpolation**: We enforce $p(1) = 1$ using the barycentric formula:\n$$\np(1) = \\frac{\\sum_{k=1}^{N} \\frac{w_k u_k}{1 - x_k}}{\\sum_{j=1}^{N} \\frac{w_j}{1 - x_j}} = 1 \\implies \\sum_{k=1}^{N} \\beta_k u_k = 1\n$$\nwhere $\\beta_k = \\frac{\\phi_k}{\\sum_{j=1}^N \\phi_j}$ and $\\phi_k = \\frac{w_k}{1 - x_k}$. This constitutes one linear equation.\n- **ODE Collocation**: To maintain a square system, we enforce the ODE $p'(x_k) - \\alpha p(x_k) = 0$ at $N-1$ of the $N$ nodes. The problem specifies that the ODE equation corresponding to the node with the maximal $x_k$ (i.e., the node closest to $x=1$) should be replaced by the boundary condition equation. This node is $x_1 = \\cos(\\pi/(2N))$ (or $x_0$ in a 0-indexed scheme).\n\nThis setup gives a square linear system $A_G u = b_G$ for the nodal values $u=[u_1, \\dots, u_N]^T$. Solving this system yields the nodal values for the Gauss approximant, $p_{\\text{Gauss}}(x)$.\n\n### 3. Error Analysis\n\nTo quantify and compare the accuracy of the two methods, we evaluate the maximum absolute error for each approximation within the boundary layer region. The evaluation grid is concentrated near $x=1$, defined by:\n- Layer width parameter: $\\delta = \\min(0.5, 8/\\alpha)$.\n- Evaluation points: $x_j = 1 - \\delta + \\frac{j}{m-1}\\delta$ for $j=0, \\dots, m-1$, with $m=400$.\n\nThe maximum errors are computed as:\n$$\nE_{\\text{Gauss}} = \\max_{j} |p_{\\text{Gauss}}(x_j) - u(x_j)|\n$$\n$$\nE_{\\text{Lobatto}} = \\max_{j} |p_{\\text{Lobatto}}(x_j) - u(x_j)|\n$$\nThe final metric for comparison is the base-10 logarithm of the ratio of these errors, $\\log_{10}(E_{\\text{Gauss}}/E_{\\text{Lobatto}})$. A large positive value indicates that the Lobatto method is significantly more accurate for the given problem. This is expected, as the inclusion of the boundary node $x=1$ allows for direct enforcement of the boundary condition and provides higher node density in the boundary layer.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# The problem allows for scipy, but numpy.linalg.solve is sufficient.\n# from scipy.linalg import solve as solve_linear_system\n\ndef bary_eval(x_eval, nodes, u_nodal, weights):\n    \"\"\"\n    Evaluates a polynomial given by nodal values using the barycentric formula.\n    Handles singularities where an evaluation point coincides with a node.\n    \"\"\"\n    p_eval = np.zeros_like(x_eval, dtype=float)\n    for i, x in enumerate(x_eval):\n        # Check if the evaluation point x is very close to one of the nodes.\n        match_indices = np.where(np.isclose(x, nodes))[0]\n        if len(match_indices)  0:\n            # If so, the interpolated value is simply the nodal value.\n            p_eval[i] = u_nodal[match_indices[0]]\n            continue\n\n        # Otherwise, use the standard barycentric formula.\n        diffs = x - nodes\n        numer_terms = weights * u_nodal / diffs\n        denom_terms = weights / diffs\n        \n        numer = np.sum(numer_terms)\n        denom = np.sum(denom_terms)\n\n        # The ratio gives the interpolated value.\n        # Add a small epsilon to avoid division by zero in pathological cases.\n        if abs(denom)  1e-100:\n           # This case is unlikely if x is not a node but handled for robustness.\n           closest_node_idx = np.argmin(np.abs(diffs))\n           p_eval[i] = u_nodal[closest_node_idx]\n        else:\n           p_eval[i] = numer / denom\n    return p_eval\n\ndef solve_case(N, alpha):\n    \"\"\"\n    Solves the ODE for a given (N, alpha) pair and returns the log10 error ratio.\n    \"\"\"\n    # ====== Part 1: Chebyshev-Gauss-Lobatto Method ======\n    M_L = N + 1\n    # Nodes and weights (0-indexed: k=0, ..., N)\n    k_L = np.arange(M_L)\n    nodes_L = np.cos(k_L * np.pi / N)\n    weights_L = (-1.0)**k_L\n    weights_L[0] *= 0.5\n    weights_L[N] *= 0.5\n\n    # Differentiation matrix D_L using a direct loop for clarity\n    D_L = np.zeros((M_L, M_L))\n    for i in range(M_L):\n        for j in range(M_L):\n            if i != j:\n                D_L[i, j] = (weights_L[j] / weights_L[i]) / (nodes_L[i] - nodes_L[j])\n    np.fill_diagonal(D_L, -np.sum(D_L, axis=1))\n\n    # Construct linear system A_L u_L = b_L\n    A_L = np.identity(M_L)\n    b_L = np.zeros(M_L)\n    \n    ode_matrix_L = D_L - alpha * np.identity(M_L)\n    \n    # First row: Boundary condition u(1)=1 = u_0 = 1\n    A_L[0, :] = 0.0\n    A_L[0, 0] = 1.0\n    b_L[0] = 1.0\n    \n    # Rows 1 to N: ODE u' - alpha*u = 0 at nodes x_1, ..., x_N\n    A_L[1:, :] = ode_matrix_L[1:, :]\n\n    u_L = np.linalg.solve(A_L, b_L)\n\n    # ====== Part 2: Chebyshev-Gauss Method ======\n    M_G = N\n    # Nodes and weights (0-indexed: k=0, ..., N-1)\n    k_G = np.arange(M_G)\n    nodes_G = np.cos((2 * k_G + 1) * np.pi / (2 * N))\n    weights_G = (-1.0)**k_G\n\n    # Differentiation matrix D_G\n    D_G = np.zeros((M_G, M_G))\n    for i in range(M_G):\n        for j in range(M_G):\n            if i != j:\n                D_G[i, j] = (weights_G[j] / weights_G[i]) / (nodes_G[i] - nodes_G[j])\n    np.fill_diagonal(D_G, -np.sum(D_G, axis=1))\n\n    # Construct linear system A_G u_G = b_G\n    A_G = np.identity(M_G)\n    b_G = np.zeros(M_G)\n    \n    ode_matrix_G = D_G - alpha * np.identity(M_G)\n\n    # First row: BC u(1)=1 implemented via barycentric interpolation.\n    # This replaces the ODE at the node closest to x=1 (k=0).\n    x_star = 1.0\n    phi_G = weights_G / (x_star - nodes_G)\n    beta_G = phi_G / np.sum(phi_G)\n    A_G[0, :] = beta_G\n    b_G[0] = 1.0\n    \n    # Rows 1 to N-1: ODE at remaining nodes\n    A_G[1:, :] = ode_matrix_G[1:, :]\n    \n    u_G = np.linalg.solve(A_G, b_G)\n\n    # ====== Part 3: Error Analysis ======\n    m = 400\n    delta = min(0.5, 8.0 / alpha)\n    x_eval = np.linspace(1.0 - delta, 1.0, m)\n\n    # Evaluate numerical and exact solutions on the fine grid\n    p_L_eval = bary_eval(x_eval, nodes_L, u_L, weights_L)\n    p_G_eval = bary_eval(x_eval, nodes_G, u_G, weights_G)\n    u_exact_eval = np.exp(alpha * (x_eval - 1.0))\n\n    # Compute maximum absolute errors\n    E_Gauss = np.max(np.abs(p_G_eval - u_exact_eval))\n    E_Lobatto = np.max(np.abs(p_L_eval - u_exact_eval))\n\n    # Return log10 of the error ratio, handling E_Lobatto=0\n    if E_Lobatto  1e-100:\n        return np.inf if E_Gauss  0 else -np.inf\n    \n    return np.log10(E_Gauss / E_Lobatto)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (16, 20),\n        (16, 100),\n        (8, 50),\n        (32, 100),\n    ]\n\n    results = []\n    for N, alpha in test_cases:\n        result = solve_case(N, alpha)\n        results.append(result)\n\n    # Format results to scientific notation with 6 decimal places.\n    formatted_results = [\"{:.6e}\".format(r) for r in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3179398"}, {"introduction": "This final practice demonstrates the remarkable versatility of collocation methods by applying them to a sophisticated problem in optimal control theory. You will discretize the nonlinear Hamilton-Jacobi-Bellman (HJB) equation, which is fundamental to finding optimal strategies, turning a complex functional equation into a system of nonlinear algebraic equations. By implementing a numerical solver, you will gain insight into how these powerful approximation techniques are used to tackle challenging problems far beyond simple boundary value problems. [@problem_id:3214244]", "problem": "Consider a deterministic optimal control system where the scalar state $x(t)$ evolves according to the control $u(t)$ via the dynamics $\\frac{dx}{dt} = u(t)$. The performance index to be minimized is the infinite-horizon integral $$J = \\int_0^{\\infty} \\left( \\frac{1}{2} x(t)^2 + \\frac{1}{2} \\alpha u(t)^2 \\right) dt$$, where $\\alpha > 0$ is a given scalar penalizing control effort. This setup serves as a simplified proxy for vertical ascent control, where $x(t)$ could represent a velocity-like state and $u(t)$ a thrust-like control; the quadratic penalty on $u(t)$ plays the role of a smooth surrogate for fuel usage appropriate for numerical analysis.\n\nStarting from the dynamic programming principle, the stationary Hamilton-Jacobi-Bellman (HJB) equation for the value function $V(x)$ can be formulated by equating the minimum instantaneous cost plus the directional derivative of $V$ along the dynamics to zero. The problem is to approximate the value function $V(x)$ on a finite symmetric domain $[-L,L]$ using a global collocation method based on Chebyshev-Gauss-Lobatto points, and to compute the resulting approximation by solving the arising nonlinear algebraic system. The approximation should enforce the HJB equation pointwise at collocation points, together with consistency conditions ensuring a unique solution, without introducing any shortcut formulas in the problem statement itself.\n\nYou must derive the stationary HJB equation from first principles and then construct a collocation approximation for $V(x)$ on $[-L,L]$, using $N+1$ Chebyshev-Gauss-Lobatto nodes mapped to this domain. Use nodal values of $V$ at these collocation points as the unknowns. Enforce the HJB equation at all collocation points except the center point, and impose $V(0)=0$ at the center to fix the additive constant in $V(x)$. Solve the resulting nonlinear system for the nodal unknowns by a suitable iterative method that does not rely on external libraries beyond the specified ones. After computing the nodal solution, evaluate the interpolation polynomial defined by the nodal values on a fine verification grid of $200$ equispaced points in $[-L,L]$ to estimate the maximum absolute approximation error with respect to the exact value function obtained from the HJB equation’s first-principles derivation.\n\nYour program must implement the following steps in a mathematically sound way:\n1. Derive the stationary Hamilton-Jacobi-Bellman (HJB) equation for $V(x)$ from the dynamic programming principle, and use it to characterize the optimal control $u^*(x)$ and the resulting equation that $V$ must satisfy.\n2. Construct Chebyshev-Gauss-Lobatto nodes $s_k$ on $[-1,1]$, map them to the physical domain $x_k = L s_k$, and form the spectral differentiation matrix $D$ associated to these nodes in the $s$-coordinate. Use the chain rule to convert derivatives with respect to $s$ into derivatives with respect to $x$.\n3. Pose the nonlinear collocation equations that enforce the HJB equation at all nodes except the central node $s=0$, and enforce $V(0)=0$ at the central node to fix the additive constant.\n4. Solve the nonlinear system for the nodal values of $V$ using a Newton-type iteration with a properly constructed Jacobian based on the residual’s dependence on the nodal values via the differentiation matrix.\n5. Using barycentric interpolation constructed from Chebyshev-Gauss-Lobatto nodes and their associated weights, evaluate the approximated $V(x)$ on a verification grid and compute the maximum absolute error with respect to the exact $V(x)$ derived by solving the HJB equation analytically.\n6. Output the maximum absolute error for each test case as specified below.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The result for each test case must be a single floating-point number equal to the maximum absolute error between the collocation approximation and the exact value function on the verification grid.\n\nUse the following test suite covering different parameter regimes:\n- Test case 1 (happy path): $\\alpha = 1$, $L = 1$, $N = 10$.\n- Test case 2 (changed control penalty): $\\alpha = 4$, $L = 1$, $N = 8$.\n- Test case 3 (larger domain): $\\alpha = 9$, $L = 2$, $N = 12$.\n\nNo physical units are required because all quantities are dimensionless. Angles are not involved. Percentages are not involved. Your program should produce exactly one line of output in the format \"[r1,r2,r3]\" where $r_i$ are the maximum absolute errors (as floating-point numbers) for the three test cases, in the order listed above.", "solution": "The problem is to find an approximate solution for the value function $V(x)$ of a deterministic optimal control system using a global collocation method. The system is governed by the state dynamics $\\frac{dx}{dt} = u(t)$, and the objective is to minimize the infinite-horizon performance index $J = \\int_0^{\\infty} \\left( \\tfrac{1}{2} x(t)^2 + \\tfrac{1}{2} \\alpha u(t)^2 \\right) dt$ with $\\alpha  0$.\n\nThe solution process involves deriving the stationary Hamilton-Jacobi-Bellman (HJB) equation, discretizing it using Chebyshev collocation, solving the resulting nonlinear algebraic system, and evaluating the accuracy of the numerical solution against the exact analytical solution.\n\n### Step 1: Derivation of the Hamilton-Jacobi-Bellman (HJB) Equation and Analytical Solution\n\nThe value function, $V(x)$, represents the minimum possible cost starting from state $x$. It is defined as:\n$$\nV(x) = \\min_{u(t)} \\int_{0}^{\\infty} \\left( \\tfrac{1}{2} x(t)^2 + \\tfrac{1}{2} \\alpha u(t)^2 \\right) dt, \\quad \\text{subject to } x(0) = x \\text{ and } \\frac{dx}{dt} = u(t).\n$$\nFor an infinite-horizon, time-invariant problem, the dynamic programming principle leads to the stationary Hamilton-Jacobi-Bellman (HJB) equation. The principle states that for any small time interval $\\Delta t$, the optimal cost from state $x$ is the minimum of the cost incurred over $\\Delta t$ plus the optimal cost from the subsequent state $x(t+\\Delta t)$. In the limit $\\Delta t \\to 0$, this yields:\n$$\n0 = \\min_{u} \\left[ L(x, u) + \\nabla V(x) \\cdot \\frac{dx}{dt} \\right]\n$$\nwhere $L(x, u) = \\tfrac{1}{2} x^2 + \\tfrac{1}{2} \\alpha u^2$ is the Lagrangian, $\\nabla V(x)$ is the gradient of the value function (which is simply the derivative $V'(x)$ for a scalar state), and $\\frac{dx}{dt} = u$. Substituting these into the HJB equation gives:\n$$\n0 = \\min_{u} \\left[ \\tfrac{1}{2} x^2 + \\tfrac{1}{2} \\alpha u^2 + V'(x)u \\right]\n$$\nTo find the control $u$ that minimizes the expression in the brackets (the Hamiltonian), we take the partial derivative with respect to $u$ and set it to zero:\n$$\n\\frac{\\partial}{\\partial u} \\left[ \\tfrac{1}{2} x^2 + \\tfrac{1}{2} \\alpha u^2 + V'(x)u \\right] = \\alpha u + V'(x) = 0\n$$\nThis gives the optimal control law, $u^*(x)$, as a function of the state and the value function's derivative:\n$$\nu^*(x) = -\\frac{1}{\\alpha} V'(x)\n$$\nSubstituting this optimal control back into the HJB equation yields a nonlinear ordinary differential equation for $V(x)$:\n$$\n0 = \\tfrac{1}{2} x^2 + \\tfrac{1}{2} \\alpha \\left(-\\frac{1}{\\alpha} V'(x)\\right)^2 + V'(x) \\left(-\\frac{1}{\\alpha} V'(x)\\right)\n$$\n$$\n0 = \\tfrac{1}{2} x^2 + \\frac{1}{2\\alpha} (V'(x))^2 - \\frac{1}{\\alpha} (V'(x))^2\n$$\n$$\n0 = \\tfrac{1}{2} x^2 - \\frac{1}{2\\alpha} (V'(x))^2\n$$\nRearranging gives the final form of the stationary HJB equation for this problem:\n$$\n(V'(x))^2 = \\alpha x^2\n$$\nThis is a standard Linear-Quadratic Regulator (LQR) problem, for which the value function is known to be a quadratic form. Let us seek a solution of the form $V(x) = \\frac{1}{2} P x^2$, where $P$ is a positive constant. The condition $V(0)=0$ is naturally satisfied. The derivative is $V'(x) = Px$. Substituting this into the HJB equation gives:\n$$\n(Px)^2 = \\alpha x^2 \\implies P^2x^2 = \\alpha x^2\n$$\nThis must hold for all $x$, so we must have $P^2 = \\alpha$. Since the cost must be non-negative, the value function $V(x)$ must be positive definite, which means $P  0$. Therefore, $P = \\sqrt{\\alpha}$. The exact analytical solution for the value function is:\n$$\nV_{\\text{exact}}(x) = \\frac{\\sqrt{\\alpha}}{2} x^2\n$$\n\n### Step 2: Chebyshev Collocation Discretization\n\nWe approximate $V(x)$ on the domain $[-L, L]$ with a degree-$N$ polynomial, $V_N(x)$, which interpolates the function at $N+1$ collocation points. We use the Chebyshev-Gauss-Lobatto (CGL) nodes, which are the extrema of the $N$-th degree Chebyshev polynomial.\nThe CGL nodes in the canonical interval $s \\in [-1, 1]$ are given by:\n$$\ns_k = \\cos\\left(\\frac{k\\pi}{N}\\right), \\quad k = 0, 1, \\dots, N\n$$\nThese nodes are mapped to the physical domain $x \\in [-L, L]$ via the affine transformation $x = Ls$:\n$$\nx_k = Ls_k = L\\cos\\left(\\frac{k\\pi}{N}\\right)\n$$\nLet $\\vec{V} = [V_0, V_1, \\dots, V_N]^T$ be the vector of unknown values of the function at these nodes, $V_k = V_N(x_k)$. The derivative of the interpolating polynomial $V_N(s)$ at the nodes $s_k$ can be computed via matrix-vector multiplication, $\\vec{V}'_s = D \\vec{V}$, where $D$ is the $(N+1) \\times (N+1)$ Chebyshev spectral differentiation matrix. Its entries are:\n$$\nD_{ij} = \\begin{cases} \\frac{c_i}{c_j} \\frac{(-1)^{i+j}}{s_i - s_j},  i \\neq j \\\\ -\\frac{s_j}{2(1-s_j^2)},  i=j \\in \\{1,\\dots,N-1\\} \\\\ \\frac{2N^2+1}{6},  i=j=0 \\\\ -\\frac{2N^2+1}{6},  i=j=N \\end{cases}\n$$\nwhere $c_k = 2$ for $k=0, N$ and $c_k=1$ otherwise.\nUsing the chain rule, the derivative with respect to $x$ is related to the derivative with respect to $s$ by $\\frac{dV_N}{dx} = \\frac{dV_N}{ds} \\frac{ds}{dx} = \\frac{1}{L} \\frac{dV_N}{ds}$. Thus, the vector of derivatives at the physical nodes $x_k$, denoted $\\vec{V}'$, is:\n$$\n\\vec{V}' = \\frac{1}{L} D \\vec{V}\n$$\n\n### Step 3: Formulation of the Nonlinear Algebraic System\n\nWe enforce the discretized HJB equation at the collocation points. The problem specifies that for $N$ even, we use the central node $x_{N/2} = L\\cos(\\frac{(N/2)\\pi}{N}) = L\\cos(\\pi/2) = 0$ to enforce the condition $V(0)=0$. At all other nodes, we enforce the HJB equation. This gives $N+1$ equations for the $N+1$ unknowns $V_k$.\n\nThe system of equations is defined by a residual vector $\\vec{R}(\\vec{V})$ of size $N+1$:\n$$\nR_k(\\vec{V}) = \\begin{cases} (V'_k)^2 - \\alpha x_k^2 = 0,  k \\in \\{0, \\dots, N\\} \\setminus \\{N/2\\} \\\\ V_k - 0 = 0,  k=N/2 \\end{cases}\n$$\nwhere $V'_k$ is the $k$-th component of the vector $\\vec{V}' = \\frac{1}{L}D\\vec{V}$.\n\n### Step 4: Solving the System with Newton's Method\n\nThe nonlinear system $\\vec{R}(\\vec{V}) = \\vec{0}$ is solved using Newton's method. Starting with an initial guess $\\vec{V}^{(0)}$, we iteratively refine the solution using:\n$$\n\\vec{V}^{(m+1)} = \\vec{V}^{(m)} + \\Delta\\vec{V}\n$$\nwhere the update $\\Delta\\vec{V}$ is found by solving the linear system:\n$$\nJ(\\vec{V}^{(m)}) \\Delta\\vec{V} = -\\vec{R}(\\vec{V}^{(m)})\n$$\nHere, $J$ is the Jacobian matrix of the residual vector $\\vec{R}$ with respect to the unknowns $\\vec{V}$, with entries $J_{ij} = \\frac{\\partial R_i}{\\partial V_j}$.\nFor the rows $i \\neq N/2$ where $R_i = (V'_i)^2 - \\alpha x_i^2$:\n$$\nJ_{ij} = \\frac{\\partial}{\\partial V_j} \\left( \\left(\\frac{1}{L}\\sum_{l=0}^N D_{il}V_l\\right)^2 \\right) = 2 V'_i \\cdot \\frac{\\partial V'_i}{\\partial V_j} = 2 V'_i \\cdot \\frac{1}{L}D_{ij} = \\frac{2}{L} V'_i D_{ij}\n$$\nFor the row $i = N/2$ where $R_{N/2} = V_{N/2}$:\n$$\nJ_{N/2, j} = \\frac{\\partial V_{N/2}}{\\partial V_j} = \\delta_{j, N/2} \\quad (\\text{Kronecker delta})\n$$\nThe iteration is started with a good initial guess, for which we can use the nodal values of the exact solution: $V^{(0)}_k = \\frac{\\sqrt{\\alpha}}{2} x_k^2$. The iteration continues until the norm of the residual vector `||\\vec{R}||` falls below a specified tolerance.\n\n### Step 5: Error Evaluation and Interpolation\n\nOnce the nodal values $\\vec{V}$ are computed, the polynomial approximation $V_N(x)$ is evaluated on a fine verification grid of $200$ equispaced points in $[-L, L]$. For numerical stability, this evaluation is performed using the barycentric interpolation formula (second form):\n$$\nV_N(x) = \\frac{\\displaystyle\\sum_{k=0}^{N} \\frac{w_k}{x - x_k} V_k}{\\displaystyle\\sum_{k=0}^{N} \\frac{w_k}{x - x_k}}\n$$\nwhere the barycentric weights $w_k$ for CGL nodes are given by $w_k = (-1)^k \\delta_k$, with $\\delta_0 = \\delta_N = 1/2$ and $\\delta_k = 1$ for $k=1, \\dots, N-1$.\nThe maximum absolute error is then computed by comparing the interpolated values $V_N(x)$ with the exact values $V_{\\text{exact}}(x)$ at each point on the verification grid:\n$$\n\\text{Error} = \\max_{x \\in \\text{grid}} |V_N(x) - V_{\\text{exact}}(x)|\n$$", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the optimal control problem for all test cases.\n    \"\"\"\n    test_cases = [\n        # (alpha, L, N)\n        (1.0, 1.0, 10),\n        (4.0, 1.0, 8),\n        (9.0, 2.0, 12),\n    ]\n\n    results = []\n    for alpha, L, N in test_cases:\n        error = solve_hjb_collocation(alpha, L, N)\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef solve_hjb_collocation(alpha, L, N):\n    \"\"\"\n    Solves the HJB equation using Chebyshev collocation for a given set of parameters.\n\n    Args:\n        alpha (float): Control penalty parameter.\n        L (float): Half-width of the symmetric domain [-L, L].\n        N (int): Degree of the Chebyshev polynomial, N+1 nodes. N must be even.\n\n    Returns:\n        float: The maximum absolute error on the verification grid.\n    \"\"\"\n    if N % 2 != 0:\n        raise ValueError(\"N must be even for a central node at 0.\")\n\n    # 1. Setup grid and differentiation matrix\n    k = np.arange(N + 1)\n    s_nodes = np.cos(k * np.pi / N)  # CGL nodes in [-1, 1], ordered from 1 down to -1\n    x_nodes = L * s_nodes  # Physical nodes in [-L, L]\n\n    # Chebyshev differentiation matrix D in s-space\n    D = np.zeros((N + 1, N + 1))\n    c = np.ones(N + 1)\n    c[0], c[N] = 2.0, 2.0\n\n    for i in range(N + 1):\n        for j in range(N + 1):\n            if i == j:\n                if i == 0:\n                    D[i, j] = (2 * N**2 + 1) / 6.0\n                elif i == N:\n                    D[i, j] = -(2 * N**2 + 1) / 6.0\n                else:\n                    D[i, j] = -s_nodes[j] / (2.0 * (1.0 - s_nodes[j]**2))\n            else:\n                D[i, j] = (c[i] / c[j]) * ((-1)**(i + j)) / (s_nodes[i] - s_nodes[j])\n\n    # 2. Newton's method to solve for the nodal values V\n    # Initial guess for V from the analytical solution form\n    V = (np.sqrt(alpha) / 2.0) * x_nodes**2\n\n    k_center = N // 2\n    \n    max_iter = 15\n    tolerance = 1e-13\n\n    for _ in range(max_iter):\n        V_prime = (1.0 / L) * (D @ V)\n\n        # Build residual vector R\n        R = np.zeros(N + 1)\n        non_center_indices = np.arange(N + 1) != k_center\n        R[non_center_indices] = V_prime[non_center_indices]**2 - alpha * x_nodes[non_center_indices]**2\n        R[k_center] = V[k_center] - 0.0\n\n        if np.linalg.norm(R)  tolerance:\n            break\n\n        # Build Jacobian matrix J\n        J = np.zeros((N + 1, N + 1))\n        for i in range(N + 1):\n            if i == k_center:\n                J[i, k_center] = 1.0\n            else:\n                J[i, :] = (2.0 / L) * V_prime[i] * D[i, :]\n        \n        # Solve linear system for the update step dV and update V\n        try:\n            dV = np.linalg.solve(J, -R)\n            V += dV\n        except np.linalg.LinAlgError:\n            # If Jacobian is singular, break. The solution might be good enough.\n            break\n\n    # 3. Evaluate error on a fine grid\n    V_exact_func = lambda x, a: (np.sqrt(a) / 2.0) * x**2\n\n    x_verif = np.linspace(-L, L, 200)\n\n    # Barycentric interpolation weights\n    w = (-1.0)**np.arange(N + 1)\n    w[0] /= 2.0\n    w[N] /= 2.0\n\n    # Evaluate numerical solution on verification grid using barycentric formula\n    V_num = np.zeros_like(x_verif)\n    for i, xv in enumerate(x_verif):\n        # Check if xv is very close to a node to avoid division by zero\n        match_indices = np.where(np.isclose(xv, x_nodes))[0]\n        if len(match_indices)  0:\n            V_num[i] = V[match_indices[0]]\n        else:\n            terms = w / (xv - x_nodes)\n            V_num[i] = np.sum(terms * V) / np.sum(terms)\n\n    # Compute V_exact on verification grid and calculate max absolute error\n    V_exact_vals = V_exact_func(x_verif, alpha)\n    max_error = np.max(np.abs(V_num - V_exact_vals))\n\n    return max_error\n\nsolve()\n```", "id": "3214244"}]}