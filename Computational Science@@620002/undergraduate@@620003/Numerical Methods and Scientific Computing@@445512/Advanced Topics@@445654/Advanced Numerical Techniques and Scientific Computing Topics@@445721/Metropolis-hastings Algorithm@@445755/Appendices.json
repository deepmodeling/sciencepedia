{"hands_on_practices": [{"introduction": "To master the Metropolis-Hastings algorithm, one must first become proficient in its core mechanic: the acceptance probability calculation. This exercise [@problem_id:1401721] provides a tangible scenario involving a robot on a grid, which serves as an ideal discrete state space to practice the direct application of the acceptance formula. By working through this problem, you will build a foundational understanding of how the target distribution $\\pi(x)$ and the proposal distribution $q(x'|x)$ together determine the behavior of the Markov chain.", "problem": "A cleaning robot operates on a small 2x2 grid of tiles. The state of the robot is given by its coordinate $(i, j)$, where the row index $i$ and column index $j$ can each be either 1 or 2. The robot is being programmed to sample from a target stationary distribution $P(i, j)$ which is known to be proportional to the sum of the coordinates, that is, $P(i, j) \\propto i+j$.\n\nThe robot's movement is determined by a simple proposal mechanism. From its current state, the robot identifies all adjacent (non-diagonal) grid tiles. It then proposes to move to one of these neighboring tiles, with each neighbor having an equal probability of being chosen.\n\nUsing the Metropolis-Hastings algorithm, calculate the acceptance probability for a proposed move from the initial state $(1,2)$ to the new state $(1,1)$.\n\nThe final answer must be a single closed-form analytic expression.", "solution": "We want the Metropolis-Hastings acceptance probability for a proposed move from the current state $x=(1,2)$ to the proposed state $y=(1,1)$ when the target distribution is $\\pi(i,j)\\propto i+j$ and the proposal selects uniformly among adjacent non-diagonal neighbors.\n\nBy the Metropolis-Hastings rule, the acceptance probability is\n$$\n\\alpha(x\\to y)=\\min\\left(1,\\frac{\\pi(y)\\,q(y\\to x)}{\\pi(x)\\,q(x\\to y)}\\right).\n$$\nSince $\\pi(i,j)\\propto i+j$, the ratio of target probabilities is\n$$\n\\frac{\\pi(1,1)}{\\pi(1,2)}=\\frac{1+1}{1+2}=\\frac{2}{3}.\n$$\nThe proposal $q(\\cdot\\to\\cdot)$ chooses uniformly among valid adjacent neighbors.\n\nFrom $x=(1,2)$, the adjacent non-diagonal neighbors are $(1,1)$ and $(2,2)$, so there are $2$ neighbors and\n$$\nq\\big((1,2)\\to(1,1)\\big)=\\frac{1}{2}.\n$$\nFrom $y=(1,1)$, the adjacent non-diagonal neighbors are $(1,2)$ and $(2,1)$, so there are $2$ neighbors and\n$$\nq\\big((1,1)\\to(1,2)\\big)=\\frac{1}{2}.\n$$\nThus the proposal ratio is\n$$\n\\frac{q\\big((1,1)\\to(1,2)\\big)}{q\\big((1,2)\\to(1,1)\\big)}=\\frac{\\frac{1}{2}}{\\frac{1}{2}}=1.\n$$\nPutting these together,\n$$\n\\alpha\\big((1,2)\\to(1,1)\\big)=\\min\\left(1,\\frac{2}{3}\\cdot 1\\right)=\\frac{2}{3}.\n$$", "answer": "$$\\boxed{\\frac{2}{3}}$$", "id": "1401721"}, {"introduction": "Beyond single-step calculations, a crucial property of a reliable MCMC sampler is ergodicityâ€”its ability to eventually explore the entire state space of the target distribution. This problem [@problem_id:1401742] presents a thought-provoking scenario where the state space is disconnected and the proposal mechanism is too 'local' to bridge the gap. Analyzing this case demonstrates a common failure mode in MCMC and highlights the critical importance of choosing a proposal distribution that ensures the chain doesn't get permanently trapped.", "problem": "A statistician is using a Markov Chain Monte Carlo (MCMC) algorithm to sample from a one-dimensional target probability density $p(x)$. The state space $\\mathcal{X}$ for the random variable $x$ is defined as the union of two disjoint intervals: $\\mathcal{X} = [-2, -1] \\cup [1, 2]$.\n\nThe unnormalized target density, $f(x)$, is defined piecewise as:\n$$\nf(x) = \\begin{cases}\n1 & \\text{if } x \\in [-2, -1] \\\\\n3 & \\text{if } x \\in [1, 2] \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nThe normalized target density is $p(x) = f(x) / Z$, where $Z = \\int_{\\mathcal{X}} f(x) dx$.\n\nThe sampler uses the Metropolis-Hastings algorithm. At each step, given the current state $x_t$, a candidate state $x'$ is proposed from a symmetric proposal distribution $q(x'|x_t)$ which is a uniform distribution over the interval $[x_t - 0.5, x_t + 0.5]$. If the proposed state $x'$ falls outside the state space $\\mathcal{X}$, it is automatically rejected.\n\nThe sampler is initialized at the state $x_0 = 1.5$. It is then run for a very large number of iterations. According to the theory of MCMC, the long-run average of the samples, $\\frac{1}{N}\\sum_{i=1}^N x_i$, should converge to the expected value of $x$ under the stationary distribution of the chain.\n\nDetermine the numerical value to which the sample mean of $x$ will converge. Express your answer as an exact fraction.", "solution": "The unnormalized target density is piecewise constant: $f(x)=1$ on $[-2,-1]$, $f(x)=3$ on $[1,2]$, and $0$ otherwise. The normalizing constant is\n$$\nZ=\\int_{\\mathcal{X}} f(x)\\,dx=\\int_{-2}^{-1} 1\\,dx+\\int_{1}^{2} 3\\,dx=(x)\\big|_{-2}^{-1}+3(x)\\big|_{1}^{2}=1+3=4.\n$$\nThus the full-space normalized target density is $p(x)=f(x)/Z$, which places total mass $\\int_{-2}^{-1} p(x)\\,dx=\\frac{1}{4}$ on $[-2,-1]$ and $\\int_{1}^{2} p(x)\\,dx=\\frac{3}{4}$ on $[1,2]$.\n\nThe Metropolis-Hastings proposal is symmetric: $q(x'|x)$ is uniform on $[x-0.5,x+0.5]$. Proposals falling outside $\\mathcal{X}$ are rejected, which is equivalent to the standard MH rule with target density zero outside $\\mathcal{X}$. Crucially, because the maximum proposal step size is $0.5$ while the gap between the two intervals is $2$, there is zero probability to move from one interval to the other in any number of accepted steps: from any $x\\in[1,2]$, the proposal support $[x-0.5,x+0.5]\\subset[0.5,2.5]$, so any proposed $x'<1$ or $x'>2$ lies outside $\\mathcal{X}$ and is rejected. Therefore the chain has two closed communicating classes, and starting at $x_{0}=1.5\\in[1,2]$ it remains in $[1,2]$ forever.\n\nWithin $[1,2]$, $f(x)\\equiv 3$ is constant, so for any proposed $x'\\in[1,2]$ the Metropolis-Hastings acceptance probability is $\\min\\{1,f(x')/f(x)\\}=1$. The invariant distribution of the chain restricted to this class is proportional to $f$ on $[1,2]$ and hence is uniform on $[1,2]$:\n$$\ng(x)=\\frac{f(x)}{\\int_{1}^{2} f(u)\\,du}=\\frac{3}{\\int_{1}^{2}3\\,du}=\\frac{3}{3}=1\\quad\\text{for }x\\in[1,2],\n$$\nand $g(x)=0$ otherwise. Consequently, the long-run sample mean converges to the expectation of $X$ under $g$, namely\n$$\n\\mathbb{E}_{g}[X]=\\int_{1}^{2} x\\,g(x)\\,dx=\\int_{1}^{2} x\\,dx=\\frac{1}{2}\\left(x^{2}\\right)\\bigg|_{1}^{2}=\\frac{1}{2}\\left(4-1\\right)=\\frac{3}{2}.\n$$\nBy the ergodic theorem for Markov chains within a closed communicating class, the empirical mean $\\frac{1}{N}\\sum_{i=1}^{N} x_{i}$ converges almost surely to $\\mathbb{E}_{g}[X]=\\frac{3}{2}$.", "answer": "$$\\boxed{\\frac{3}{2}}$$", "id": "1401742"}, {"introduction": "Translating theoretical algorithms into robust computer code requires confronting the practical limits of numerical precision. In many real-world applications, probabilities can be vanishingly small, leading to underflow errors. This final practice [@problem_id:1401715] introduces the indispensable technique of working with log-probabilities to ensure numerical stability. By calculating the log-acceptance ratio, you will learn a standard method used in professional scientific computing to make MCMC samplers both efficient and reliable.", "problem": "A computational biologist is developing a statistical model to analyze the conformational states of a flexible biomolecule. The state of the molecule is described by a single generalized coordinate $\\theta$. The unnormalized posterior probability density of the state, $\\pi(\\theta)$, is computationally challenging to work with directly due to extremely small values. Instead, the model is defined by its unnormalized log-posterior density, $\\mathcal{L}(\\theta) = \\ln(\\pi(\\theta))$, which is given by the expression:\n$$\n\\mathcal{L}(\\theta) = -A (\\theta - \\theta_0)^4 - B \\sin^2(k \\theta)\n$$\nTo explore the landscape of possible states, a Metropolis-Hastings Markov Chain Monte Carlo (MCMC) algorithm is used. The algorithm generates a proposed new state $\\theta'$ from a current state $\\theta$ using a symmetric proposal distribution, which means the probability of proposing $\\theta'$ from $\\theta$ is the same as proposing $\\theta$ from $\\theta'$.\n\nTo maintain numerical stability and avoid underflow errors, the algorithm's acceptance criterion is evaluated in the log domain. Instead of calculating the acceptance probability $A$, the algorithm directly computes a quantity known as the log-acceptance ratio, $\\alpha_{\\log}$. A proposed move is accepted if a random number $u$ drawn from a uniform distribution on $[0, 1]$ satisfies $\\ln(u) < \\alpha_{\\log}$ (for cases where $\\alpha_{\\log}$ is negative).\n\nConsider a single step in the simulation where the current state is $\\theta = 0.25$ and the proposed new state is $\\theta' = 0.35$. The model parameters are set as:\n- $A = 120.0$\n- $\\theta_0 = 0.10$\n- $B = 20.0$\n- $k = 2\\pi$\n\nCalculate the numerical value of the log-acceptance ratio, $\\alpha_{\\log}$, for this specific proposed move. All arguments for trigonometric functions are in radians. Round your final answer to four significant figures.", "solution": "The core of the Metropolis-Hastings algorithm is the acceptance probability, $A$, for a transition from a state $\\theta$ to a proposed state $\\theta'$. It is given by:\n$$\nA(\\theta'|\\theta) = \\min \\left( 1, \\frac{\\pi(\\theta') Q(\\theta|\\theta')}{\\pi(\\theta) Q(\\theta'|\\theta)} \\right)\n$$\nwhere $\\pi(\\theta)$ is the target probability density and $Q(x'|x)$ is the proposal distribution.\n\nThe problem states that the proposal distribution is symmetric, which means $Q(\\theta'|\\theta) = Q(\\theta|\\theta')$. This simplifies the acceptance probability to:\n$$\nA(\\theta'|\\theta) = \\min \\left( 1, \\frac{\\pi(\\theta')}{\\pi(\\theta)} \\right)\n$$\nThe problem specifies that to avoid numerical underflow, the algorithm computes the log-acceptance ratio, $\\alpha_{\\log}$. This is the natural logarithm of the ratio term inside the $\\min$ function:\n$$\n\\alpha_{\\log} = \\ln \\left( \\frac{\\pi(\\theta')}{\\pi(\\theta)} \\right)\n$$\nUsing the properties of logarithms, this can be expressed as the difference of the log-probabilities:\n$$\n\\alpha_{\\log} = \\ln(\\pi(\\theta')) - \\ln(\\pi(\\theta))\n$$\nThe problem defines the unnormalized log-posterior density as $\\mathcal{L}(\\theta) = \\ln(\\pi(\\theta))$. Therefore, we can write the log-acceptance ratio in terms of $\\mathcal{L}(\\theta)$:\n$$\n\\alpha_{\\log} = \\mathcal{L}(\\theta') - \\mathcal{L}(\\theta)\n$$\nThis formulation is numerically stable because it involves differences of moderately-sized numbers ($\\mathcal{L}(\\theta)$ values) rather than ratios of extremely small numbers ($\\pi(\\theta)$ values).\n\nNow, we substitute the given expression for $\\mathcal{L}(\\theta)$:\n$$\n\\mathcal{L}(\\theta) = -A (\\theta - \\theta_0)^4 - B \\sin^2(k \\theta)\n$$\nSo, the expression for $\\alpha_{\\log}$ becomes:\n$$\n\\alpha_{\\log} = \\left[ -A (\\theta' - \\theta_0)^4 - B \\sin^2(k \\theta') \\right] - \\left[ -A (\\theta - \\theta_0)^4 - B \\sin^2(k \\theta) \\right]\n$$\nRearranging the terms, we get:\n$$\n\\alpha_{\\log} = -A \\left[ (\\theta' - \\theta_0)^4 - (\\theta - \\theta_0)^4 \\right] - B \\left[ \\sin^2(k \\theta') - \\sin^2(k \\theta) \\right]\n$$\nNow, we substitute the specified numerical values:\n- $A = 120.0$\n- $\\theta_0 = 0.10$\n- $B = 20.0$\n- $k = 2\\pi$\n- $\\theta = 0.25$\n- $\\theta' = 0.35$\n\nFirst, let's calculate the components of the first term:\n- $\\theta - \\theta_0 = 0.25 - 0.10 = 0.15$\n- $\\theta' - \\theta_0 = 0.35 - 0.10 = 0.25$\n- $(\\theta' - \\theta_0)^4 - (\\theta - \\theta_0)^4 = (0.25)^4 - (0.15)^4 = 0.00390625 - 0.00050625 = 0.0034$\n\nNext, let's calculate the components of the second term. The arguments to the sine function are in radians.\n- $k \\theta = 2\\pi \\times 0.25 = \\frac{\\pi}{2}$\n- $k \\theta' = 2\\pi \\times 0.35 = 0.7\\pi$\n- $\\sin(k \\theta) = \\sin(\\frac{\\pi}{2}) = 1$\n- $\\sin(k \\theta') = \\sin(0.7\\pi)$\n- $\\sin^2(k \\theta') - \\sin^2(k \\theta) = \\sin^2(0.7\\pi) - 1^2$\nWe can use the identity $\\sin(\\pi - x) = \\sin(x)$ to evaluate $\\sin(0.7\\pi) = \\sin(\\pi - 0.3\\pi) = \\sin(0.3\\pi)$.\n- $\\sin^2(0.7\\pi) - 1 = (\\sin(0.7\\pi))^2 - 1 \\approx (0.809017)^2 - 1 \\approx 0.654508 - 1 = -0.345492$\n\nNow, we can assemble the final value for $\\alpha_{\\log}$:\n$$\n\\alpha_{\\log} = -120.0 \\times (0.0034) - 20.0 \\times (\\sin^2(0.7\\pi) - 1)\n$$\n$$\n\\alpha_{\\log} \\approx -120.0 \\times 0.0034 - 20.0 \\times (-0.345492)\n$$\n$$\n\\alpha_{\\log} \\approx -0.408 + 6.90984\n$$\n$$\n\\alpha_{\\log} \\approx 6.50184\n$$\nRounding the result to four significant figures, we get:\n$$\n\\alpha_{\\log} = 6.502\n$$", "answer": "$$\\boxed{6.502}$$", "id": "1401715"}]}