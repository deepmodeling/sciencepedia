{"hands_on_practices": [{"introduction": "Mastering a new computational technique begins with a foundational example. This first practice uses the triangular distribution, a simple yet non-trivial case, to walk through the entire inverse transform sampling workflow. By deriving the probability density function (PDF), integrating it to find the cumulative distribution function (CDF), and then inverting the piecewise CDF, you will build the core skills needed to implement samplers from first principles. This exercise serves as the perfect building block for tackling more complex probability distributions [@problem_id:2403851].", "problem": "You are asked to implement Inverse Transform Sampling for a triangular probability distribution with finite support. The triangular distribution is defined by three real parameters $a$, $c$, and $b$ with $a  b$ and $a \\le c \\le b$, where $a$ is the left endpoint, $b$ is the right endpoint, and $c$ is the mode (location of the peak). The probability density function is continuous, piecewise linear on $[a, c]$ and $[c, b]$, and zero outside $[a, b]$. Your task is to derive the sampler from first principles and implement it as a program.\n\nFundamental base:\n- Probability density function (PDF): A nonnegative function $f(x)$ integrating to $1$ over its support.\n- Cumulative distribution function (CDF): $F(x) = \\mathbb{P}(X \\le x) = \\int_{-\\infty}^{x} f(t)\\, dt$.\n- Inverse Transform Sampling: If $U$ is a random variable uniformly distributed on $[0,1]$, and $F$ is a continuous, strictly increasing CDF, then $X = F^{-1}(U)$ has CDF $F$.\n\nRequirement:\n1) Starting only from the definitions above, derive the piecewise CDF $F(x)$ for the triangular distribution on $[a, c, b]$, and analytically invert it to obtain the quantile function $F^{-1}(u)$ for $u \\in [0,1]$. Ensure that your derivation covers the general case $a  c  b$ and also correctly handles the limiting cases $c = a$ and $c = b$.\n\n2) Implement a program that, given $(a, c, b)$ and a list of $u$-values with each $u \\in [0,1]$, returns the corresponding samples $x = F^{-1}(u)$ using your derived inverse function. You must handle the following details:\n- For numerical robustness, clamp each $u$ into $[0,1]$ before applying the inverse.\n- Your implementation must work when $c = a$ and when $c = b$ without division-by-zero or invalid-branch issues.\n- For each computed sample, round the result to $10$ decimal places.\n\n3) You must not rely on any prepackaged triangular distribution from any library. You may use basic numerical operations from allowed libraries, but the sampler must directly implement your derived inverse function.\n\nInput and output for this challenge are fixed; there is no user input. Your program must evaluate the inverse at prescribed parameter sets and $u$-values (the test suite below) and print a single line containing all results.\n\nTest suite:\n- Case $1$ (general “happy path”): $(a, c, b) = (0, 2, 5)$ with $u$-values $[0, 0.1, 0.4, 0.5, 0.9, 1]$.\n- Case $2$ (edge case $c = a$): $(a, c, b) = (3, 3, 7)$ with $u$-values $[0, 0.25, 0.999999, 1]$.\n- Case $3$ (edge case $c = b$): $(a, c, b) = (-2, 1, 1)$ with $u$-values $[0, 0.25, 0.8, 1]$.\n- Case $4$ (boundary robustness): $(a, c, b) = (0, 1, 2)$ with $u$-values $[0, 10^{-12}, 0.5, 1 - 10^{-12}, 1]$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes one list of rounded samples. Concretely, the output must be a single line string of the form\n$[[x_{1,1},x_{1,2},\\dots],[x_{2,1},\\dots],[x_{3,1},\\dots],[x_{4,1},\\dots]]$\nwith no spaces. Each $x_{i,j}$ must be a floating-point number rounded to $10$ decimal places.\n- No physical units are involved in this problem; all quantities are dimensionless real numbers.", "solution": "The user has provided a problem statement. I will first validate it.\n\n### Step 1: Extract Givens\n- **Distribution**: Triangular probability distribution.\n- **Parameters**: Real numbers $a$, $c$, $b$ such that $a  b$ and $a \\le c \\le b$. $a$ is the minimum, $b$ is the maximum, and $c$ is the mode.\n- **Probability Density Function (PDF)**: $f(x)$ is continuous, piecewise linear, supported on $[a, b]$.\n- **Cumulative Distribution Function (CDF)**: $F(x) = \\mathbb{P}(X \\le x) = \\int_{-\\infty}^{x} f(t)\\, dt$.\n- **Core Task**: Use Inverse Transform Sampling, where a sample $X$ is generated as $X = F^{-1}(U)$ from a uniform random variable $U \\sim \\text{Uniform}[0,1]$.\n- **Derivation Requirement**: Derive the piecewise CDF $F(x)$ and its inverse, the quantile function $F^{-1}(u)$, from first principles. The derivation must cover the general case $a  c  b$ and the limiting cases $c = a$ and $c = b$.\n- **Implementation Requirements**:\n    1. Implement the derived $F^{-1}(u)$ function.\n    2. Do not use pre-packaged library functions for the triangular distribution.\n    3. Input values of $u$ must be clamped to the interval $[0,1]$.\n    4. The implementation must correctly handle the cases $c=a$ and $c=b$.\n    5. Output samples must be rounded to $10$ decimal places.\n- **Test Suite**:\n    - Case 1: $(a, c, b) = (0, 2, 5)$, $u = [0, 0.1, 0.4, 0.5, 0.9, 1]$.\n    - Case 2: $(a, c, b) = (3, 3, 7)$, $u = [0, 0.25, 0.999999, 1]$.\n    - Case 3: $(a, c, b) = (-2, 1, 1)$, $u = [0, 0.25, 0.8, 1]$.\n    - Case 4: $(a, c, b) = (0, 1, 2)$, $u = [0, 10^{-12}, 0.5, 1 - 10^{-12}, 1]$.\n- **Output Format**: A single-line string representing a list of lists of results, with no spaces: `[[...],[...],[...],[...]]`.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is based on fundamental, well-established principles of probability theory (PDF, CDF) and computational statistics (Inverse Transform Sampling). The triangular distribution is a standard, non-controversial probability distribution. This criterion is met.\n- **Well-Posed**: The problem provides all necessary definitions, constraints ($a  b$, $a \\le c \\le b$), and a clear objective. The existence and uniqueness of the CDF and its inverse (for a continuous, strictly increasing function) are guaranteed by mathematical principles. This criterion is met.\n- **Objective**: The problem is stated in precise mathematical language, free from any subjectivity or ambiguity. This criterion is met.\n- **Flaw Check**: The problem does not violate any scientific laws, is formalizable, complete, and verifiable. The constraints are consistent and the required task is well-defined.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. I will proceed with the derivation and solution.\n\nThe task is to derive the quantile function for a triangular distribution and implement it. This proceeds in three stages: first, defining the probability density function (PDF); second, integrating the PDF to find the cumulative distribution function (CDF); and third, inverting the CDF to obtain the quantile function, $F^{-1}(u)$.\n\n**1. Probability Density Function (PDF), $f(x)$**\n\nThe PDF $f(x)$ describes a triangle with vertices at $(a, 0)$, $(c, h)$, and $(b, 0)$, where $h$ is the height of the triangle. The total area under the PDF must be $1$. The area of the triangle is given by $\\frac{1}{2} \\times \\text{base} \\times \\text{height}$.\n$$\n\\text{Area} = \\frac{1}{2} (b-a) h = 1 \\implies h = \\frac{2}{b-a}\n$$\nThe PDF is a piecewise linear function.\nFor $a \\le x \\le c$, the line passes through $(a, 0)$ and $(c, h)$. The equation is:\n$$\nf(x) = \\frac{h-0}{c-a} (x-a) = \\frac{2(x-a)}{(b-a)(c-a)}\n$$\nFor $c  x \\le b$, the line passes through $(c, h)$ and $(b, 0)$. The equation is:\n$$\nf(x) = h + \\frac{0-h}{b-c} (x-c) = \\frac{2}{b-a} - \\frac{2(x-c)}{(b-a)(b-c)} = \\frac{2(b-c) - 2(x-c)}{(b-a)(b-c)} = \\frac{2(b-x)}{(b-a)(b-c)}\n$$\nCombining these, the PDF is:\n$$\nf(x) = \\begin{cases}\n\\frac{2(x-a)}{(b-a)(c-a)}  \\text{if } a \\le x \\le c \\\\\n\\frac{2(b-x)}{(b-a)(b-c)}  \\text{if } c  x \\le b \\\\\n0  \\text{otherwise}\n\\end{cases}\n$$\nThese expressions are defined assuming $a  c  b$. We shall see that the final quantile function correctly handles the edge cases $c=a$ and $c=b$.\n\n**2. Cumulative Distribution Function (CDF), $F(x)$**\n\nThe CDF is the integral of the PDF, $F(x) = \\int_{a}^{x} f(t) \\,dt$.\nFor $x  a$, $F(x)=0$. For $x  b$, $F(x)=1$.\n\nFor $a \\le x \\le c$:\n$$\nF(x) = \\int_{a}^{x} \\frac{2(t-a)}{(b-a)(c-a)} \\,dt = \\frac{2}{(b-a)(c-a)} \\left[ \\frac{(t-a)^2}{2} \\right]_{a}^{x} = \\frac{(x-a)^2}{(b-a)(c-a)}\n$$\nAt the mode $c$, the CDF value is $F(c) = \\frac{(c-a)^2}{(b-a)(c-a)} = \\frac{c-a}{b-a}$. This value is the total probability mass in the interval $[a, c]$.\n\nFor $c  x \\le b$:\n$$\nF(x) = F(c) + \\int_{c}^{x} \\frac{2(b-t)}{(b-a)(b-c)} \\,dt \\\\\nF(x) = \\frac{c-a}{b-a} + \\frac{2}{(b-a)(b-c)} \\left[ -\\frac{(b-t)^2}{2} \\right]_{c}^{x} \\\\\nF(x) = \\frac{c-a}{b-a} + \\frac{-(b-x)^2 - (-(b-c)^2)}{(b-a)(b-c)} \\\\\nF(x) = \\frac{c-a}{b-a} + \\frac{(b-c)^2 - (b-x)^2}{(b-a)(b-c)}\n$$\nA more elegant form is found by simplifying:\n$$\nF(x) = \\frac{(c-a)(b-c) + (b-c)^2 - (b-x)^2}{(b-a)(b-c)} = \\frac{(b-c)(c-a+b-c) - (b-x)^2}{(b-a)(b-c)} \\\\\nF(x) = \\frac{(b-c)(b-a) - (b-x)^2}{(b-a)(b-c)} = 1 - \\frac{(b-x)^2}{(b-a)(b-c)}\n$$\nSo, the complete CDF is:\n$$\nF(x) = \\begin{cases}\n0  \\text{if } x  a \\\\\n\\frac{(x-a)^2}{(b-a)(c-a)}  \\text{if } a \\le x \\le c \\\\\n1 - \\frac{(b-x)^2}{(b-a)(b-c)}  \\text{if } c  x \\le b \\\\\n1  \\text{if } x  b\n\\end{cases}\n$$\n\n**3. Inverse CDF (Quantile Function), $F^{-1}(u)$**\n\nTo implement inverse transform sampling, we must solve for $x$ from the equation $u = F(x)$, where $u \\in [0, 1]$.\nThe form of the inverse depends on whether $u$ falls into the range corresponding to $x \\le c$ or $x  c$. The critical value of $u$ that separates these two regimes is $u_c = F(c) = \\frac{c-a}{b-a}$.\n\nCase 1: $0 \\le u \\le u_c$ (corresponds to $a \\le x \\le c$)\n$$\nu = \\frac{(x-a)^2}{(b-a)(c-a)} \\\\\n(x-a)^2 = u(b-a)(c-a) \\\\\nx-a = \\sqrt{u(b-a)(c-a)} \\quad (\\text{positive root since } x \\ge a) \\\\\nx = a + \\sqrt{u(b-a)(c-a)}\n$$\n\nCase 2: $u_c  u \\le 1$ (corresponds to $c  x \\le b$)\n$$\nu = 1 - \\frac{(b-x)^2}{(b-a)(b-c)} \\\\\n(b-x)^2 = (1-u)(b-a)(b-c) \\\\\nb-x = \\sqrt{(1-u)(b-a)(b-c)} \\quad (\\text{positive root since } x \\le b) \\\\\nx = b - \\sqrt{(1-u)(b-a)(b-c)}\n$$\n\nThe resulting quantile function $F^{-1}(u)$ is:\n$$\nF^{-1}(u) = \\begin{cases}\na + \\sqrt{u (b-a) (c-a)}  \\text{if } 0 \\le u \\le \\frac{c-a}{b-a} \\\\\nb - \\sqrt{(1-u) (b-a) (b-c)}  \\text{if } \\frac{c-a}{b-a}  u \\le 1\n\\end{cases}\n$$\nThis single, unified formulation is robust and correctly handles the edge cases.\n- If $c=a$, then $u_c = \\frac{c-a}{b-a} = 0$. The condition $0 \\le u \\le 0$ is only true for $u=0$. For any $u  0$, the second branch is used: $x = b - \\sqrt{(1-u) (b-a) (b-a)} = b - (b-a)\\sqrt{1-u}$. This corresponds to a right triangle with its peak at $a$.\n- If $c=b$, then $u_c = \\frac{c-a}{b-a} = 1$. The condition $0 \\le u \\le 1$ holds for all $u$, so the first branch is always used: $x = a + \\sqrt{u (b-a) (b-a)} = a + (b-a)\\sqrt{u}$. This corresponds to a right triangle with its peak at $b$.\n\nThe derivation is complete and sound. The implementation will follow this final formula.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and implements the inverse transform sampling for a triangular distribution.\n    The final output is printed in the exact required format.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: General case\n        {'params': (0, 2, 5), 'u_values': [0, 0.1, 0.4, 0.5, 0.9, 1]},\n        # Case 2: Edge case c = a\n        {'params': (3, 3, 7), 'u_values': [0, 0.25, 0.999999, 1]},\n        # Case 3: Edge case c = b\n        {'params': (-2, 1, 1), 'u_values': [0, 0.25, 0.8, 1]},\n        # Case 4: Boundary robustness\n        {'params': (0, 1, 2), 'u_values': [0, 10**-12, 0.5, 1 - 10**-12, 1]},\n    ]\n    \n    all_results = []\n\n    for case in test_cases:\n        a, c, b = case['params']\n        u_values = case['u_values']\n        \n        case_results = []\n        \n        # Pre-calculate terms for efficiency and clarity\n        # The problem states a  b, so b - a  0 is guaranteed.\n        range_ab = b - a\n        range_ac = c - a\n        range_cb = b - c\n        \n        # Calculate the CDF value at the mode, which is the threshold for u\n        # This handles the case where b-a might equal 0 if the problem constraints were different,\n        # but here it's safe.\n        if range_ab == 0:\n            # Degenerate case, not covered by problem statement a  b\n            # but good practice for robustness. All samples are at 'a'.\n            fc = 0.5 # A reasonable default.\n        else:\n            fc = range_ac / range_ab\n        \n        for u in u_values:\n            # Per requirement, clamp u into [0, 1] for numerical robustness.\n            u_clamped = max(0.0, min(1.0, u))\n            \n            x = 0.0\n            # The logic directly implements the derived piecewise inverse CDF.\n            # The comparison u_clamped = fc correctly selects the formula branch,\n            # naturally handling the edge cases c=a (fc=0) and c=b (fc=1).\n            if u_clamped = fc:\n                # This branch corresponds to the interval [a, c]\n                # x = a + sqrt(u * (b-a) * (c-a))\n                term = u_clamped * range_ab * range_ac\n                x = a + np.sqrt(term)\n            else:\n                # This branch corresponds to the interval (c, b]\n                # x = b - sqrt((1-u) * (b-a) * (b-c))\n                term = (1 - u_clamped) * range_ab * range_cb\n                x = b - np.sqrt(term)\n            \n            # Round the result to 10 decimal places as required.\n            case_results.append(round(x, 10))\n            \n        all_results.append(case_results)\n\n    # Format the final output string to the exact required format.\n    # e.g., [[x11,x12,...],[x21,...],[x31,...],[x41,...]] with no spaces.\n    # Convert list of lists to string and remove whitespace.\n    output_string = str(all_results).replace(\" \", \"\")\n    \n    print(output_string)\n\nsolve()\n```", "id": "2403851"}, {"introduction": "Real-world phenomena are not always purely continuous. This exercise advances your skills by introducing a mixed distribution, which combines a discrete point mass with a continuous probability density. To create a sampler for this scenario, you must grapple with a cumulative distribution function (CDF) that has a jump discontinuity [@problem_id:2403899]. Deriving the correct sampling algorithm from the generalized inverse CDF will deepen your understanding of the theory and result in an elegant, practical method that is essential for modeling systems with both discrete events and continuous outcomes.", "problem": "You are to design and implement an inverse transform sampler for a probability distribution consisting of a point mass (Dirac delta) mixed with a continuous component. The target probability density is\n$$\np(x) = w\\,\\delta(x - x_0) + (1 - w)\\,f(x),\n$$\nwhere $w \\in [0,1]$, $x_0 \\in \\mathbb{R}$, and $f(x)$ is a proper probability density function on its domain. Your derivation must start from the foundational definition that if $U$ is a random variable uniformly distributed on $[0,1]$, then $X = F^{-1}(U)$ has cumulative distribution function $F(x)$, where $F^{-1}$ is the generalized inverse of $F$. You must not assume any shortcut formulas; derive what the generalized inverse sampling rule implies when the cumulative distribution function has a jump discontinuity due to a Dirac delta term, and show how this leads to a practical algorithm that uses a single uniform variate per sample.\n\nImplement a single program that:\n- Derives, implements, and uses a sampler that draws samples from $p(x)$ by transforming a single uniform random number $U \\sim \\mathrm{Uniform}(0,1)$ per sample.\n- Supports the following choices for the continuous component $f(x)$ (you must derive and use the corresponding inverse cumulative distribution functions from first principles):\n  1. Exponential distribution with rate parameter $\\lambda  0$ on $x \\ge 0$.\n  2. Uniform distribution on $[a,b]$ with $a  b$.\n  3. Logistic distribution on $x \\in \\mathbb{R}$ with location $\\mu$ and scale $s  0$.\n\nFor each supported continuous distribution, derive and use analytic expressions for the first and second moments of $f(x)$, and then derive the mixture’s mean and variance using the law of the unconscious statistician and basic properties of mixtures.\n\nYour program must hard-code and execute the following test suite. For each test case, it must:\n- Generate $N$ samples from the specified $p(x)$ using only one independent uniform variate per generated sample.\n- Compute the empirical point mass, empirical mean, and empirical variance, defined respectively as:\n  - The empirical point mass at $x_0$, equal to the fraction of generated samples exactly equal to $x_0$.\n  - The empirical mean $\\hat{\\mu} = \\frac{1}{N}\\sum_{i=1}^N x_i$.\n  - The empirical variance $\\hat{\\sigma}^2 = \\frac{1}{N}\\sum_{i=1}^N (x_i - \\hat{\\mu})^2$.\n- Compute the theoretical point mass $w$, the theoretical mixture mean, and the theoretical mixture variance using your derived formulas.\n- For each metric (point mass, mean, variance), output a boolean indicating whether the empirical value is within a given absolute tolerance of the theoretical value.\n\nUse the following test suite with fixed seeds for reproducibility:\n- Case A (general mixture on $\\mathbb{R}_{\\ge 0}$): $w = 0.3$, $x_0 = 1.5$, $f$ is exponential with rate $\\lambda = 2.0$, $N = 80000$, seed $314159$.\n- Case B (boundary condition $w = 0$): $w = 0$, $x_0 = -100.0$, $f$ is uniform on $[a,b]$ with $a = 2.0$, $b = 5.0$, $N = 60000$, seed $271828$.\n- Case C (boundary condition $w = 1$): $w = 1$, $x_0 = -2.0$, $f$ is logistic with $\\mu = 0.0$, $s = 1.5$, $N = 40000$, seed $161803$.\n- Case D (mixture with dominant point mass): $w = 0.95$, $x_0 = 0.0$, $f$ is logistic with $\\mu = -1.0$, $s = 0.8$, $N = 80000$, seed $141421$.\n\nUse the following absolute tolerances:\n- Point mass tolerance $\\tau_p = 0.01$.\n- Mean tolerance $\\tau_m = 0.02$.\n- Variance tolerance $\\tau_v = 0.05$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output six items in the following order: empirical point mass (float), empirical mean (float), empirical variance (float), then three booleans indicating the pass/fail of the point mass, mean, and variance tolerance checks. Concatenate the outputs of all test cases in order A, B, C, D into one flat list. For example, the overall structure is\n$$\n[\\hat{p}_A, \\hat{\\mu}_A, \\hat{\\sigma}^2_A, b^p_A, b^\\mu_A, b^\\sigma_A,\\ \\hat{p}_B, \\hat{\\mu}_B, \\hat{\\sigma}^2_B, b^p_B, b^\\mu_B, b^\\sigma_B,\\ \\ldots ].\n$$\nNo user input is required and no physical units are involved. Angles are not used. The output must be fully determined by the fixed seeds given above.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- **Target Probability Density Function (PDF):** $p(x) = w\\,\\delta(x - x_0) + (1 - w)\\,f(x)$, where $w \\in [0,1]$, $x_0 \\in \\mathbb{R}$, and $f(x)$ is a proper PDF.\n- **Fundamental Principle:** The sampler must be derived from the inverse transform sampling principle, where $X = F^{-1}(U)$ for a uniform variate $U \\sim \\mathrm{Uniform}(0,1)$ and a cumulative distribution function (CDF) $F(x)$. $F^{-1}$ is the generalized inverse CDF. The sampler must use a single uniform variate per sample.\n- **Continuous Components $f(x)$:**\n  1. Exponential distribution with rate $\\lambda  0$ on $x \\ge 0$.\n  2. Uniform distribution on $[a,b]$ with $a  b$.\n  3. Logistic distribution on $x \\in \\mathbb{R}$ with location $\\mu$ and scale $s  0$.\n- **Required Derivations:**\n  1. The sampling algorithm itself, from first principles.\n  2. Inverse CDFs for each specified $f(x)$.\n  3. First and second moments for each $f(x)$.\n  4. Mean and variance of the mixture distribution $p(x)$.\n- **Test Suite:**\n  - **Case A:** $w = 0.3$, $x_0 = 1.5$, $f(x)$ is Exponential($\\lambda = 2.0$), $N = 80000$, seed $314159$.\n  - **Case B:** $w = 0$, $x_0 = -100.0$, $f(x)$ is Uniform($a = 2.0, b = 5.0$), $N = 60000$, seed $271828$.\n  - **Case C:** $w = 1$, $x_0 = -2.0$, $f(x)$ is Logistic($\\mu = 0.0, s = 1.5$), $N = 40000$, seed $161803$.\n  - **Case D:** $w = 0.95$, $x_0 = 0.0$, $f(x)$ is Logistic($\\mu = -1.0, s = 0.8$), $N = 80000$, seed $141421$.\n- **Tolerances:** Point mass $\\tau_p = 0.01$, mean $\\tau_m = 0.02$, variance $\\tau_v = 0.05$.\n- **Output Format:** A single flat list of floats and booleans: $[\\hat{p}_A, \\hat{\\mu}_A, \\hat{\\sigma}^2_A, b^p_A, b^\\mu_A, b^\\sigma_A, \\ldots]$.\n\n**Step 2: Validate Using Extracted Givens**\n1.  **Scientifically Grounded:** The problem is based on the well-established statistical methods of mixture models and inverse transform sampling. The specified distributions (Dirac delta, Exponential, Uniform, Logistic) are standard in probability theory. The problem is fundamentally sound.\n2.  **Well-Posed:** The problem defines a clear objective (implement and test a sampler) with all necessary parameters, constraints, and test cases provided. The existence of a solution is guaranteed by the theory of probability. The use of fixed seeds ensures a unique, reproducible output.\n3.  **Objective:** The problem is stated in precise mathematical and computational terms, free of subjectivity or ambiguity.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. It is scientifically sound, well-posed, objective, and complete. A full solution will be provided.\n\n**Derivation and Solution**\n\nThe objective is to construct a sampling algorithm for the mixed probability distribution with density $p(x) = w\\,\\delta(x - x_0) + (1 - w)\\,f(x)$, where $w$ is the weight of the point mass at $x_0$ and $f(x)$ is a continuous probability density. The derivation must adhere to the principle of inverse transform sampling.\n\n**1. General Sampling Algorithm from First Principles**\n\nThe cumulative distribution function (CDF) $P(x)$ for the density $p(x)$ is given by the integral:\n$$\nP(x) = \\int_{-\\infty}^{x} p(t) \\,dt = \\int_{-\\infty}^{x} \\left[ w\\,\\delta(t - x_0) + (1 - w)\\,f(t) \\right] dt\n$$\nBy linearity of the integral, we have:\n$$\nP(x) = w \\int_{-\\infty}^{x} \\delta(t - x_0) \\,dt + (1 - w) \\int_{-\\infty}^{x} f(t) \\,dt\n$$\nThe integral of the Dirac delta function is the Heaviside step function, $H(z)$, which is $1$ for $z \\ge 0$ and $0$ for $z  0$. The integral of the density $f(t)$ is its own CDF, which we denote as $F_f(x)$. Therefore, the mixture CDF is:\n$$\nP(x) = w\\,H(x - x_0) + (1-w)\\,F_f(x)\n$$\nThis CDF has a jump discontinuity of size $w$ at $x = x_0$. The principle of inverse transform sampling states that if $U$ is a random variable uniformly distributed on $[0,1]$, then the random variable $X = P^{-1}(U)$ has the CDF $P(x)$. Direct algebraic inversion to find $P^{-1}$ is complicated due to the step function and the piecewise nature of $P(x)$.\n\nHowever, we can construct an algorithm and then prove that it correctly generates samples according to $P(x)$, thereby demonstrating it as a valid implementation of the inverse transform method. This approach is known as the composition method. The mixture can be interpreted as a two-step process: first, choose a component (either the Dirac delta or the continuous distribution $f(x)$) with probabilities $w$ and $1-w$ respectively; second, draw a sample from the chosen component. This can be implemented with a single uniform variate.\n\nThe proposed algorithm is as follows:\n1.  Generate a single uniform random number $U \\sim \\mathrm{Uniform}(0,1)$.\n2.  If $U \\le w$, set the sample $X = x_0$. This branch is chosen with probability $w$.\n3.  If $U  w$, draw a sample from the density $f(x)$. This branch is chosen with probability $1-w$. To do this with the same variate $U$, we must transform it to a new uniform variate on $[0,1]$. Let $U' = \\frac{U-w}{1-w}$. If $U \\sim \\mathrm{Uniform}(w, 1)$, then $U' \\sim \\mathrm{Uniform}(0,1)$. We then set the sample $X = F_f^{-1}(U')$, where $F_f^{-1}$ is the inverse CDF of $f(x)$.\n\nTo prove this algorithm is correct, we derive the CDF of the random variable $X$ it produces. Let this computed CDF be $P_X(z) = \\mathrm{Pr}(X \\le z)$. Using the law of total probability, conditioning on the value of $U$:\n$$\nP_X(z) = \\mathrm{Pr}(X \\le z | U \\le w)\\mathrm{Pr}(U \\le w) + \\mathrm{Pr}(X \\le z | U  w)\\mathrm{Pr}(U  w)\n$$\nThe probabilities of the conditions are $\\mathrm{Pr}(U \\le w) = w$ and $\\mathrm{Pr}(U  w) = 1-w$.\n- If $U \\le w$, the algorithm sets $X = x_0$. Thus, $\\mathrm{Pr}(X \\le z | U \\le w) = \\mathrm{Pr}(x_0 \\le z) = H(z - x_0)$.\n- If $U  w$, the algorithm sets $X = F_f^{-1}(U')$. The variable $U' = \\frac{U-w}{1-w}$ is uniform on $(0,1)$. By the inverse transform theorem, the variable $X$ generated in this branch has the CDF $F_f(z)$. Thus, $\\mathrm{Pr}(X \\le z | U  w) = F_f(z)$.\n\nSubstituting these back into the equation for $P_X(z)$:\n$$\nP_X(z) = H(z - x_0) \\cdot w + F_f(z) \\cdot (1-w)\n$$\nThis is identical to the target CDF $P(x)$. The algorithm is therefore a correct implementation of sampling from $p(x)$ and is derived from the principles of inverse transform sampling.\n\n**2. Analysis of Continuous Components $f(x)$**\n\n**2.1. Exponential Distribution**\n- **PDF:** $f(x) = \\lambda e^{-\\lambda x}$ for $x \\ge 0$, and $0$ otherwise.\n- **CDF:** $F_f(x) = \\int_0^x \\lambda e^{-\\lambda t} dt = [-e^{-\\lambda t}]_0^x = 1 - e^{-\\lambda x}$ for $x \\ge 0$.\n- **Inverse CDF:** Let $u = F_f(x)$. Then $u = 1 - e^{-\\lambda x} \\implies e^{-\\lambda x} = 1 - u \\implies -\\lambda x = \\ln(1-u) \\implies x = -\\frac{\\ln(1-u)}{\\lambda}$. This is $F_f^{-1}(u)$.\n- **Moments:**\n  - Mean: $E_f[X] = \\int_0^\\infty x \\lambda e^{-\\lambda x} dx = \\frac{1}{\\lambda}$.\n  - Second Moment: $E_f[X^2] = \\int_0^\\infty x^2 \\lambda e^{-\\lambda x} dx = \\frac{2}{\\lambda^2}$.\n\n**2.2. Uniform Distribution**\n- **PDF:** $f(x) = \\frac{1}{b-a}$ for $x \\in [a,b]$, and $0$ otherwise.\n- **CDF:** $F_f(x) = \\int_a^x \\frac{1}{b-a} dt = \\frac{x-a}{b-a}$ for $x \\in [a,b]$.\n- **Inverse CDF:** Let $u = F_f(x)$. Then $u = \\frac{x-a}{b-a} \\implies u(b-a) = x-a \\implies x = a + u(b-a)$. This is $F_f^{-1}(u)$.\n- **Moments:**\n  - Mean: $E_f[X] = \\int_a^b x \\frac{1}{b-a} dx = \\frac{1}{b-a} [\\frac{x^2}{2}]_a^b = \\frac{b^2-a^2}{2(b-a)} = \\frac{a+b}{2}$.\n  - Second Moment: $E_f[X^2] = \\int_a^b x^2 \\frac{1}{b-a} dx = \\frac{1}{b-a} [\\frac{x^3}{3}]_a^b = \\frac{b^3-a^3}{3(b-a)} = \\frac{a^2+ab+b^2}{3}$.\n\n**2.3. Logistic Distribution**\n- **PDF:** $f(x) = \\frac{e^{-(x-\\mu)/s}}{s(1+e^{-(x-\\mu)/s})^2}$ for $x \\in \\mathbb{R}$.\n- **CDF:** $F_f(x) = \\frac{1}{1+e^{-(x-\\mu)/s}}$.\n- **Inverse CDF:** Let $u = F_f(x)$. Then $u = \\frac{1}{1+e^{-(x-\\mu)/s}} \\implies \\frac{1}{u} = 1+e^{-(x-\\mu)/s} \\implies \\frac{1-u}{u} = e^{-(x-\\mu)/s} \\implies \\ln(\\frac{1-u}{u}) = -\\frac{x-\\mu}{s} \\implies x = \\mu - s\\ln(\\frac{1-u}{u}) = \\mu + s\\ln(\\frac{u}{1-u})$. This is $F_f^{-1}(u)$.\n- **Moments:**\n  - Mean: $E_f[X] = \\mu$.\n  - Variance: $\\mathrm{Var}_f(X) = \\frac{\\pi^2 s^2}{3}$.\n  - Second Moment: $E_f[X^2] = \\mathrm{Var}_f(X) + (E_f[X])^2 = \\frac{\\pi^2 s^2}{3} + \\mu^2$.\n\n**3. Mixture Moments**\n\nThe moments of the mixture distribution $p(x)$ are derived using the law of the unconscious statistician.\n\n- **Mean of the Mixture ($E[X]$):**\n$$\nE[X] = \\int_{-\\infty}^{\\infty} x\\, p(x) \\,dx = \\int_{-\\infty}^{\\infty} x [w\\,\\delta(x - x_0) + (1 - w)\\,f(x)] \\,dx\n$$\n$$\nE[X] = w \\int_{-\\infty}^{\\infty} x\\,\\delta(x - x_0)\\,dx + (1-w) \\int_{-\\infty}^{\\infty} x\\,f(x)\\,dx\n$$\nBy the sifting property of the Dirac delta, the first integral is $x_0$. The second integral is the mean of $f(x)$, $E_f[X]$.\n$$\nE[X] = w\\,x_0 + (1-w)\\,E_f[X]\n$$\n\n- **Second Moment of the Mixture ($E[X^2]$):**\n$$\nE[X^2] = \\int_{-\\infty}^{\\infty} x^2 p(x) \\,dx = w \\int_{-\\infty}^{\\infty} x^2 \\delta(x - x_0) \\,dx + (1-w) \\int_{-\\infty}^{\\infty} x^2 f(x)\\,dx\n$$\nThe first integral is $x_0^2$. The second integral is the second moment of $f(x)$, $E_f[X^2]$.\n$$\nE[X^2] = w\\,x_0^2 + (1-w)\\,E_f[X^2]\n$$\n\n- **Variance of the Mixture ($\\mathrm{Var}(X)$):**\nThe variance is given by the standard formula, $\\mathrm{Var}(X) = E[X^2] - (E[X])^2$.\n$$\n\\mathrm{Var}(X) = \\left( w\\,x_0^2 + (1-w)\\,E_f[X^2] \\right) - \\left( w\\,x_0 + (1-w)\\,E_f[X] \\right)^2\n$$\nThese formulas allow for the calculation of the theoretical mean and variance for each test case.\nThe theoretical point mass probability at $x_0$ is simply $w$.\n\nThe implementation will now follow from these derivations.", "answer": "```python\nimport numpy as np\nimport math\n\n# Use a fixed version of the default generator for reproducibility across numpy versions.\n# PCG64 is available in numpy 1.23.5.\nGenerator = np.random.PCG64\n\nclass Distribution:\n    \"\"\"Abstract base class for continuous probability distributions.\"\"\"\n    def inverse_cdf(self, u: np.ndarray) - np.ndarray:\n        raise NotImplementedError\n\n    def mean(self) - float:\n        raise NotImplementedError\n\n    def second_moment(self) - float:\n        raise NotImplementedError\n\nclass Exponential(Distribution):\n    \"\"\"Exponential distribution f(x) = lambda * exp(-lambda * x).\"\"\"\n    def __init__(self, lam: float):\n        if not lam  0:\n            raise ValueError(\"Rate parameter lambda must be positive.\")\n        self.lam = float(lam)\n\n    def inverse_cdf(self, u: np.ndarray) - np.ndarray:\n        # Derived from u = 1 - exp(-lambda * x)\n        # Using 1-u. For a uniform variate u, 1-u is also uniform.\n        # This form is common, but log(u) is numerically more stable if u can be 1.\n        # Since u is U', it's from (0,1), so no risk of u=1 for inverse_cdf.\n        return -np.log(1.0 - u) / self.lam\n\n    def mean(self) - float:\n        return 1.0 / self.lam\n\n    def second_moment(self) - float:\n        return 2.0 / self.lam**2\n\nclass Uniform(Distribution):\n    \"\"\"Uniform distribution f(x) = 1/(b-a) on [a, b].\"\"\"\n    def __init__(self, a: float, b: float):\n        if not a  b:\n            raise ValueError(\"Parameter 'a' must be less than 'b'.\")\n        self.a = float(a)\n        self.b = float(b)\n\n    def inverse_cdf(self, u: np.ndarray) - np.ndarray:\n        # Derived from u = (x - a) / (b - a)\n        return self.a + u * (self.b - self.a)\n\n    def mean(self) - float:\n        return (self.a + self.b) / 2.0\n\n    def second_moment(self) - float:\n        # E[X^2] = Var(X) + E[X]^2\n        var = ((self.b - self.a)**2) / 12.0\n        mean = self.mean()\n        return var + mean**2\n\nclass Logistic(Distribution):\n    \"\"\"Logistic distribution.\"\"\"\n    def __init__(self, mu: float, s: float):\n        if not s  0:\n            raise ValueError(\"Scale parameter 's' must be positive.\")\n        self.mu = float(mu)\n        self.s = float(s)\n\n    def inverse_cdf(self, u: np.ndarray) - np.ndarray:\n        # Derived from u = 1 / (1 + exp(-(x-mu)/s))\n        # This is x = mu + s * log(u / (1-u))\n        # The input u will be in (0, 1), so no division by zero or log(0).\n        return self.mu + self.s * np.log(u / (1.0 - u))\n\n    def mean(self) - float:\n        return self.mu\n\n    def second_moment(self) - float:\n        # E[X^2] = Var(X) + E[X]^2\n        var = (math.pi**2 * self.s**2) / 3.0\n        return var + self.mu**2\n\ndef solve():\n    \"\"\"\n    Main function to execute the test suite for the mixed distribution sampler.\n    \"\"\"\n    test_cases = [\n        {'name': 'A', 'w': 0.3, 'x0': 1.5, 'f_type': 'exp', 'f_params': {'lam': 2.0}, 'N': 80000, 'seed': 314159},\n        {'name': 'B', 'w': 0.0, 'x0': -100.0, 'f_type': 'unif', 'f_params': {'a': 2.0, 'b': 5.0}, 'N': 60000, 'seed': 271828},\n        {'name': 'C', 'w': 1.0, 'x0': -2.0, 'f_type': 'logis', 'f_params': {'mu': 0.0, 's': 1.5}, 'N': 40000, 'seed': 161803},\n        {'name': 'D', 'w': 0.95, 'x0': 0.0, 'f_type': 'logis', 'f_params': {'mu': -1.0, 's': 0.8}, 'N': 80000, 'seed': 141421},\n    ]\n\n    tolerances = {'pm': 0.01, 'mean': 0.02, 'var': 0.05}\n    \n    dist_map = {\n        'exp': Exponential,\n        'unif': Uniform,\n        'logis': Logistic,\n    }\n\n    final_results = []\n\n    for case in test_cases:\n        w = case['w']\n        x0 = case['x0']\n        N = case['N']\n        seed = case['seed']\n        \n        f_dist = dist_map[case['f_type']](**case['f_params'])\n        \n        # --- Generate Samples ---\n        rng = np.random.Generator(Generator(seed))\n        U = rng.random(size=N)\n        samples = np.zeros(N)\n\n        # Boolean mask for the point mass component\n        is_point_mass = U = w\n        \n        # Assign point mass samples\n        samples[is_point_mass] = x0\n        \n        # Handle continuous component\n        # The condition (1-w)  0 avoids division by zero when w=1\n        num_continuous = np.sum(~is_point_mass)\n        if num_continuous  0:\n            # Rescale uniform variates for the continuous part\n            U_cont = U[~is_point_mass]\n            U_prime = (U_cont - w) / (1.0 - w)\n            samples[~is_point_mass] = f_dist.inverse_cdf(U_prime)\n\n        # --- Empirical Metrics ---\n        # The fraction of samples exactly equal to x0\n        emp_point_mass = np.sum(samples == x0) / N\n        emp_mean = np.mean(samples)\n        # np.var with ddof=0 calculates population variance (1/N), as required\n        emp_var = np.var(samples)\n\n        # --- Theoretical Metrics ---\n        theo_point_mass = w\n        \n        E_f_X = f_dist.mean()\n        E_f_X2 = f_dist.second_moment()\n        \n        theo_mean = w * x0 + (1.0 - w) * E_f_X\n        E_mix_X2 = w * x0**2 + (1.0 - w) * E_f_X2\n        theo_var = E_mix_X2 - theo_mean**2\n        \n        # --- Tolerance Checks ---\n        pm_ok = abs(emp_point_mass - theo_point_mass)  tolerances['pm']\n        mean_ok = abs(emp_mean - theo_mean)  tolerances['mean']\n        var_ok = abs(emp_var - theo_var)  tolerances['var']\n        \n        # Collect results for this case\n        case_results = [\n            emp_point_mass,\n            emp_mean,\n            emp_var,\n            pm_ok,\n            mean_ok,\n            var_ok\n        ]\n        final_results.extend(case_results)\n\n    # Format output as a single flat list\n    # The map converts booleans to 'True'/'False' strings; problem implies booleans in output list. OK.\n    # Python's `str()` on booleans is 'True' or 'False' (capitalized).\n    # To be precise, output `true`/`false`.\n    formatted_results = []\n    for item in final_results:\n        if isinstance(item, bool):\n            formatted_results.append(str(item).lower())\n        else:\n            # Rounding for clean output, not required but good practice.\n            formatted_results.append(f\"{item:.6f}\")\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2403899"}, {"introduction": "This final practice serves as a practical capstone, generalizing the piecewise approach to a powerful and flexible tool. Here, you will develop a sampler for any distribution defined by a histogram with non-uniform bin widths, a common task when working with empirical data [@problem_id:3244347]. This exercise demonstrates how the fundamental principles of inverse transform sampling can be scaled to create a robust sampler for arbitrarily shaped distributions, moving from textbook examples to a technique directly applicable in scientific computing and data analysis.", "problem": "Consider a probability distribution on a real interval specified by a histogram with non-uniform bin widths. The histogram is given by a strictly increasing sequence of bin edges $E = [e_0, e_1, \\dots, e_m]$ and positive bin heights $H = [h_0, h_1, \\dots, h_{m-1}]$. The interpretation is that the probability density function $f(x)$ is piecewise constant on each bin $[e_i, e_{i+1})$, with $f(x)$ proportional to $h_i$ on that bin, and zero outside $[e_0, e_m]$. The cumulative distribution function $F(x)$ is defined by the fundamental relationship $F(x) = \\int_{-\\infty}^{x} f(t) \\, dt$, and the inverse transform sampling method uses the definition of a cumulative distribution function to generate samples: if $U$ is a uniform random variable on $[0,1)$, then a sample from the target distribution can be obtained by solving $F(x) = U$ for $x$.\n\nYour tasks are to:\n\n- Derive from first principles and then implement a numerically stable sampler that uses inverse transform sampling to generate independent samples from the distribution described by $(E, H)$. Use that $f(x)$ is piecewise constant on each bin $[e_i, e_{i+1})$, and recall that the probability density function must integrate to $1$, which determines a normalization constant.\n- Construct a function to evaluate the cumulative distribution function $F(x)$ at arbitrary query points $x$, consistent with the piecewise constant $f(x)$ and the fundamental definition $F(x) = \\int_{-\\infty}^x f(t)\\,dt$.\n- Generate random samples using inverse transform sampling. For reproducibility, you must use a fixed pseudo-random number generator seed provided for each test case. Angles do not appear in this problem, and no physical units are involved.\n\nImplementation requirements:\n\n- Treat the bin heights $h_i$ as relative density heights, meaning the unnormalized density on bin $i$ is $h_i$ and the normalization factor ensures $\\int f(x)\\,dx = 1$.\n- Ensure that bins with $h_i = 0$ contribute no probability mass; these bins should not be selected by the sampler.\n- Handle boundary conditions for $F(x)$ so that $F(x) = 0$ for $x \\le e_0$ and $F(x) = 1$ for $x \\ge e_m$.\n\nTest suite:\n\nProvide results for the following three test cases. For each case, compute and output in order:\n\n$1.$ The normalization constant $c$ such that $f(x) = c \\cdot h_i$ on bin $i$.\n\n$2.$ The cumulative distribution function value $F(q_1)$ at the first query point $q_1$.\n\n$3.$ The cumulative distribution function value $F(q_2)$ at the second query point $q_2$.\n\n$4.$ The sample mean $\\bar{x}$ of $N$ independent samples generated by your inverse transform sampler with the specified seed. The sample mean must be a real-valued float.\n\nThe three cases are:\n\n- Case $1$: $E = [0,1,3,4]$, $H = [2,1,3]$, query points $Q = [0.5, 3.2]$, sample size $N = 1000$, seed $S = 314159$.\n- Case $2$: $E = [-2,0,0.5,2.5]$, $H = [0,4,1]$, query points $Q = [-1.0, 2.0]$, sample size $N = 800$, seed $S = 271828$.\n- Case $3$: $E = [0,0.1,1.0]$, $H = [10,0.1]$, query points $Q = [0.05, 0.5]$, sample size $N = 1200$, seed $S = 1618033$.\n\nFinal output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain $12$ floats in the order described above, aggregating across the three cases, for example, $[c_1,F_1(q_{1}),F_1(q_{2}),\\bar{x}_1,c_2,F_2(q_{1}),F_2(q_{2}),\\bar{x}_2,c_3,F_3(q_{1}),F_3(q_{2}),\\bar{x}_3]$ with no additional text. All numbers must be printed as standard decimal floats. No physical units or angle units are required, and the results should not be expressed with percentage signs. Each element of the output list must be a float.", "solution": "The construction of an inverse transform sampler for a piecewise constant probability density function begins with core definitions from probability theory. A probability density function $f(x)$ on the real line must satisfy $\\int_{-\\infty}^{\\infty} f(x)\\,dx = 1$ and the cumulative distribution function is defined by $F(x) = \\int_{-\\infty}^{x} f(t)\\,dt$. The inverse transform sampling method uses the principle that if $U$ is a random variable uniformly distributed on $[0,1)$ and if $F$ is a continuous, non-decreasing function with $F(e_0) = 0$ and $\\lim_{x\\to\\infty}F(x) = 1$, then solving $F(x) = U$ yields a random sample $x$ distributed according to $f$.\n\nWe are given a histogram with non-uniform bin widths described by bin edges $E = [e_0, e_1, \\dots, e_m]$ with strictly increasing $e_i$ and bin heights $H = [h_0, h_1, \\dots, h_{m-1}]$ with $h_i \\ge 0$. The interpretation is that the unnormalized probability density function is piecewise constant: for $x \\in [e_i, e_{i+1})$ we have $f(x) \\propto h_i$ and outside $[e_0, e_m]$ we have $f(x) = 0$. Let $w_i = e_{i+1} - e_i$ denote the width of bin $i$. The normalization constant $c$ is determined by the requirement that the integral of $f(x)$ over its support equals $1$:\n$$\n\\int_{-\\infty}^{\\infty} f(x)\\,dx = \\sum_{i=0}^{m-1} \\int_{e_i}^{e_{i+1}} c \\cdot h_i \\, dx = c \\sum_{i=0}^{m-1} h_i \\cdot (e_{i+1} - e_i) = 1.\n$$\nThus,\n$$\nc = \\left( \\sum_{i=0}^{m-1} h_i \\cdot (e_{i+1} - e_i) \\right)^{-1}.\n$$\nBins with $h_i = 0$ contribute zero mass and do not affect the sampler except through the edges.\n\nThe cumulative distribution function $F(x)$ follows from the definition $F(x) = \\int_{-\\infty}^{x} f(t)\\,dt$. For $x \\le e_0$, we have $F(x) = 0$. For $x \\ge e_m$, we have $F(x) = 1$. For $x \\in [e_i, e_{i+1})$,\n$$\nF(x) = \\sum_{j=0}^{i-1} \\int_{e_j}^{e_{j+1}} c h_j \\, dt + \\int_{e_i}^{x} c h_i \\, dt = \\sum_{j=0}^{i-1} c h_j (e_{j+1} - e_j) + c h_i (x - e_i).\n$$\nIt is therefore a continuous, piecewise linear function increasing from $0$ to $1$ across the support.\n\nTo construct the inverse transform sampler, we first precompute the cumulative end-of-bin masses:\n$$\nM_i^{\\text{end}} = \\sum_{j=0}^{i} c h_j (e_{j+1} - e_j), \\quad i = 0,1,\\dots,m-1,\n$$\nwith $M_{m-1}^{\\text{end}} = 1$ by normalization. Let $M_i^{\\text{start}} = \\sum_{j=0}^{i-1} c h_j (e_{j+1} - e_j)$ denote the cumulative mass up to the start of bin $i$ (with $M_0^{\\text{start}} = 0$). Given a uniform random variate $U \\in [0,1)$, we identify the unique bin index $i$ such that $M_i^{\\text{start}} \\le U  M_i^{\\text{end}}$. This selection step can be implemented efficiently via a binary search on the array of $M_i^{\\text{end}}$. Within bin $i$, the equation $F(x) = U$ becomes\n$$\nM_i^{\\text{start}} + c h_i (x - e_i) = U,\n$$\nwhich can be solved directly for $x$:\n$$\nx = e_i + \\frac{U - M_i^{\\text{start}}}{c h_i}.\n$$\nThis formula is valid provided $h_i  0$, which is guaranteed because bins with $h_i = 0$ have zero mass and therefore cannot be chosen by the search step. Numerical stability considerations include ensuring $U$ lies in $[0,1)$ (not equal to $1$ due to floating-point rounding), and using robust search procedures that account for repeated cumulative mass values where bins have zero mass.\n\nThe analytical mean of the distribution is useful for validation and derived directly from the definition of expected value:\n$$\n\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x f(x)\\,dx = \\sum_{i=0}^{m-1} \\int_{e_i}^{e_{i+1}} x \\cdot c h_i \\, dx = c \\sum_{i=0}^{m-1} h_i \\left( \\frac{e_{i+1}^2 - e_i^2}{2} \\right).\n$$\nAlthough the program is only required to output the normalization constant, selected cumulative distribution function values, and the sample mean, this expression underpins the correctness of the implementation and is used internally for verification.\n\nAlgorithmic steps:\n\n- Validate that $E$ is strictly increasing and that all $h_i \\ge 0$.\n- Compute bin widths $w_i = e_{i+1} - e_i$ and the normalization constant $c = \\left( \\sum_i h_i w_i \\right)^{-1}$.\n- Compute end-of-bin cumulative masses $M_i^{\\text{end}} = \\sum_{j=0}^{i} c h_j w_j$.\n- Implement $F(x)$ by:\n  - Returning $0$ if $x \\le e_0$,\n  - Returning $1$ if $x \\ge e_m$,\n  - Otherwise, locating bin $i$ with $e_i \\le x  e_{i+1}$ and computing $F(x) = M_i^{\\text{start}} + c h_i (x - e_i)$.\n- Implement inverse transform sampling by:\n  - Drawing $U$ from a Uniform$(0,1)$ generator,\n  - Clipping $U$ to a value strictly less than $1$ if necessary,\n  - Locating bin $i$ via a binary search on $M_i^{\\text{end}}$ such that $U  M_i^{\\text{end}}$,\n  - Recovering the start mass $M_i^{\\text{start}}$ and computing $x = e_i + \\frac{U - M_i^{\\text{start}}}{c h_i}$.\n- For each test case, compute $c$, evaluate $F$ at the two query points, and compute the sample mean of $N$ samples using the specified seed for reproducibility.\n- Output the $12$ floats in the specified order as a single bracketed comma-separated list.\n\nThis approach fully adheres to the inverse transform sampling method derived from the foundational definitions of the probability density function and cumulative distribution function, and manages non-uniform bin widths and zero-height bins correctly.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_normalization_constant(edges, heights):\n    # edges: list of bin edges, strictly increasing\n    # heights: list of nonnegative bin heights (relative densities)\n    widths = np.diff(edges)\n    total = np.dot(heights, widths)\n    if total = 0:\n        raise ValueError(\"Total unnormalized mass must be positive.\")\n    return 1.0 / total\n\ndef compute_cumulative_end_masses(edges, heights, c):\n    widths = np.diff(edges)\n    bin_masses = c * heights * widths\n    cum_end = np.cumsum(bin_masses)\n    return cum_end  # end-of-bin cumulative masses\n\ndef cdf_from_histogram(edges, heights, c, x):\n    # Piecewise linear CDF consistent with piecewise constant PDF\n    e = np.array(edges)\n    h = np.array(heights)\n    if x = e[0]:\n        return 0.0\n    if x = e[-1]:\n        return 1.0\n    # Find bin i such that e[i] = x  e[i+1]\n    i = np.searchsorted(e, x, side='right') - 1\n    # Compute cumulative mass up to start of bin i\n    widths = np.diff(e)\n    bin_masses = c * h * widths\n    if i  0:\n        start_mass = float(np.sum(bin_masses[:i]))\n    else:\n        start_mass = 0.0\n    # Contribution within bin i\n    return start_mass + c * h[i] * (x - e[i])\n\ndef inverse_transform_sample(edges, heights, size, seed):\n    # Build cumulative end-of-bin masses to locate bins\n    e = np.array(edges, dtype=float)\n    h = np.array(heights, dtype=float)\n    if np.any(np.diff(e) = 0):\n        raise ValueError(\"Edges must be strictly increasing.\")\n    if np.any(h  0):\n        raise ValueError(\"Heights must be nonnegative.\")\n    c = compute_normalization_constant(e, h)\n    cum_end = compute_cumulative_end_masses(e, h, c)\n    widths = np.diff(e)\n    bin_masses = c * h * widths\n    rng = np.random.default_rng(seed)\n    # Draw uniforms in [0,1)\n    u = rng.random(size=size)\n    # Clip any values that are numerically 1.0 (extremely unlikely) to just below 1.0\n    u = np.minimum(u, np.nextafter(1.0, 0.0))\n    # Locate bin index for each u: find first cum_end  u\n    idx = np.searchsorted(cum_end, u, side='right')\n    # Compute start mass for each selected bin\n    # For idx == 0, start_mass = 0; else start_mass = cum_end[idx-1]\n    start_mass = np.where(idx  0, cum_end[idx - 1], 0.0)\n    # Compute x for each sample: e[idx] + (u - start_mass)/(c * h[idx])\n    # Ensure heights for selected bins are positive; bins with zero height have zero mass and won't be selected.\n    selected_heights = h[idx]\n    # To be safe, handle any potential zero height (shouldn't occur) by leaving x at left edge\n    denom = c * selected_heights\n    # Avoid divide-by-zero by assigning any zero denom to 1 (and zero numerator), though should not happen.\n    safe_denom = np.where(denom == 0.0, 1.0, denom)\n    offset = (u - start_mass) / safe_denom\n    x = e[idx] + offset\n    return x\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"edges\": [0.0, 1.0, 3.0, 4.0],\n            \"heights\": [2.0, 1.0, 3.0],\n            \"queries\": [0.5, 3.2],\n            \"N\": 1000,\n            \"seed\": 314159\n        },\n        {\n            \"edges\": [-2.0, 0.0, 0.5, 2.5],\n            \"heights\": [0.0, 4.0, 1.0],\n            \"queries\": [-1.0, 2.0],\n            \"N\": 800,\n            \"seed\": 271828\n        },\n        {\n            \"edges\": [0.0, 0.1, 1.0],\n            \"heights\": [10.0, 0.1],\n            \"queries\": [0.05, 0.5],\n            \"N\": 1200,\n            \"seed\": 1618033\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        edges = case[\"edges\"]\n        heights = case[\"heights\"]\n        queries = case[\"queries\"]\n        N = case[\"N\"]\n        seed = case[\"seed\"]\n\n        c = compute_normalization_constant(edges, heights)\n        # Compute CDF values at queries\n        Fq1 = cdf_from_histogram(edges, heights, c, queries[0])\n        Fq2 = cdf_from_histogram(edges, heights, c, queries[1])\n        # Generate samples and compute sample mean\n        samples = inverse_transform_sample(edges, heights, N, seed)\n        sample_mean = float(np.mean(samples))\n\n        results.extend([float(c), float(Fq1), float(Fq2), sample_mean])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3244347"}]}