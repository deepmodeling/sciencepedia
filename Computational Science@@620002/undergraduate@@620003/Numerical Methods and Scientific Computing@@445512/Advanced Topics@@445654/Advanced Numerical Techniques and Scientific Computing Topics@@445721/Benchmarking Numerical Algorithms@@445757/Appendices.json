{"hands_on_practices": [{"introduction": "In computational science, theoretical complexity does not always translate directly into practical speed. This exercise explores the classic trade-off between an asymptotically faster algorithm, Strassen's method for matrix multiplication with complexity $\\mathcal{O}(N^{\\log_2 7})$, and the standard cubic-time algorithm, $\\mathcal{O}(N^3)$. You will implement a benchmark to empirically identify the \"crossover point\"—the problem size $N$ at which the higher overhead of the more complex algorithm begins to pay off [@problem_id:3209812]. This practice is fundamental for making informed decisions when selecting algorithms for real-world applications.", "problem": "You are to implement a reproducible benchmarking program to empirically identify a finite-size crossover, within a prescribed finite test grid, between the classical cubic-time matrix multiplication and the Strassen algorithm when run on dense square matrices of real numbers. The goal is to quantify where the classical algorithm is no slower than Strassen because of implementation overheads that dominate at small sizes. The benchmarking must be done in seconds, using the median of multiple runs for robustness, and without any user input.\n\nStart from these fundamental bases:\n- The classical dense matrix multiplication of two square matrices of size $N \\times N$ has time complexity $\\mathcal{O}(N^3)$ under the standard arithmetic cost model where addition and multiplication of real numbers have constant cost.\n- The Strassen algorithm reduces the number of sub-multiplications by using $7$ block multiplications at each recursion, along with block additions and subtractions whose combined cost per level is bounded by a constant times $N^2$ under the same model.\n- Empirical benchmarking measures actual runtime $t(N)$ in seconds, which can be modeled as a positive function that depends on implementation constants and overheads not captured by asymptotic notation.\n\nDefinitions and requirements:\n- Implement two algorithms:\n  1. A classical multiplication that computes $C = A \\times B$ for $A, B \\in \\mathbb{R}^{N \\times N}$, with time complexity $\\mathcal{O}(N^3)$.\n  2. A Strassen multiplication that uses the standard $7$-multiplication recursion on square blocks, with a base-case threshold parameter $b$ such that if $N \\le b$, the multiplication is delegated to the classical algorithm. For general $N$, pad inputs with zeros to the next power of two so that block partitioning is always valid; return the output cropped back to size $N \\times N$.\n- For each candidate size $N$, generate inputs $A, B$ with independent entries drawn from a standard normal distribution $\\mathcal{N}(0,1)$ using a fixed random seed to ensure reproducibility. Perform one untimed warm-up call per algorithm and per size to mitigate cold-start artifacts.\n- For each $N$ and each algorithm, measure the runtime in seconds using a monotonic high-resolution clock and record the median over $r$ repeated timed runs. Denote the medians by $t_{\\mathrm{classical}}(N)$ and $t_{\\mathrm{Strassen}}(N)$.\n- Define the finite-grid crossover decision for a given parameter set as follows: among the provided candidate sizes, return the largest $N$ such that $t_{\\mathrm{classical}}(N) \\le t_{\\mathrm{Strassen}}(N)$. If no such $N$ exists in the candidate list, return $-1$.\n- Verify numerical correctness once per $N$ by checking that the maximum absolute entrywise difference between the two algorithms’ outputs is at most $\\tau = 10^{-8}$; do not include this check in any timed run. If a violation occurs, treat this as an implementation error and do not alter timing behavior; but your program should still complete and produce outputs for the test suite.\n\nUnits and measurement:\n- All times must be measured and reported internally in seconds.\n- Angles are not involved.\n- Any ratio, if used for internal decisions, must be treated as a pure number (dimensionless) and not printed.\n\nTest suite:\nRun the benchmark for the following five parameter sets; each set is a tuple $(\\text{sizes}, b, r, s)$:\n- Case $1$: sizes $=[\\,16,32,64\\,]$, base-case threshold $b=16$, repeats $r=3$, seed $s=0$.\n- Case $2$: sizes $=[\\,24,32,48\\,]$, base-case threshold $b=8$, repeats $r=3$, seed $s=1$.\n- Case $3$: sizes $=[\\,8,16,32\\,]$, base-case threshold $b=64$, repeats $r=5$, seed $s=2$.\n- Case $4$: sizes $=[\\,32,64\\,]$, base-case threshold $b=32$, repeats $r=2$, seed $s=3$.\n- Case $5$: sizes $=[\\,16\\,]$, base-case threshold $b=8$, repeats $r=5$, seed $s=4$.\n\nFinal output format:\n- Your program should produce a single line of output containing the crossover decisions for the five cases as a comma-separated list enclosed in square brackets, for example, $[n_1,n_2,n_3,n_4,n_5]$, where each $n_i$ is an integer equal to the largest $N$ in the corresponding candidate list for which $t_{\\mathrm{classical}}(N) \\le t_{\\mathrm{Strassen}}(N)$, or $-1$ if no such $N$ exists for that case.\n- Print nothing else.", "solution": "The problem requires the implementation and empirical benchmarking of classical and Strassen matrix multiplication algorithms to identify the crossover point in performance. The crossover point is defined as the largest matrix size $N$ within a given finite set for which the classical algorithm, benefiting from lower overheads, is no slower than the asymptotically superior Strassen algorithm.\n\nFirst, the theoretical foundations of the two algorithms are considered. The classical algorithm for multiplying two matrices $A, B \\in \\mathbb{R}^{N \\times N}$ to compute $C = A \\times B$ is defined by the element-wise formula $C_{ij} = \\sum_{k=1}^{N} A_{ik} B_{kj}$. This requires $N^3$ scalar multiplications and $N^2(N-1)$ scalar additions, resulting in a time complexity of $\\mathcal{O}(N^3)$. In contrast, Strassen's algorithm is a recursive, divide-and-conquer method. It partitions the $N \\times N$ matrices into four $N/2 \\times N/2$ sub-blocks and computes the product matrix using only $7$ recursive multiplications and a fixed number of matrix additions and subtractions. The computational cost is described by the recurrence relation $T(N) = 7 T(N/2) + \\mathcal{O}(N^2)$, which solves to a time complexity of $\\mathcal{O}(N^{\\log_2 7}) \\approx \\mathcal{O}(N^{2.807})$. Despite its better asymptotic scaling, Strassen's algorithm incurs significant overhead from recursive function calls and the additional matrix-add-subtract operations, making it less efficient for small $N$.\n\nThe implementation consists of two main functions, one for each algorithm, and a benchmarking framework to execute the comparison.\n\nThe classical matrix multiplication is implemented as a function `classical_matmul(A, B)`. To provide a realistic and high-performance baseline, this function leverages `numpy.dot`. Since the high-level language `Python` is used with the `numpy` library, this is the canonical implementation of classical matrix multiplication, typically linking to optimized Basic Linear Algebra Subprograms (BLAS) libraries.\n\nThe Strassen matrix multiplication is implemented in a recursive function `strassen_matmul(A, B, b)`. This function is designed according to the problem's specifications:\n1.  **Base Case**: The recursion terminates when the matrix size $N$ satisfies $N \\le b$, where $b$ is the base-case threshold. In this scenario, the computation is delegated to the `classical_matmul` function. This creates a hybrid algorithm that avoids the overhead of recursion for small matrices where it is not beneficial.\n2.  **Padding and Cropping**: The recursive partitioning of Strassen's algorithm requires matrices with dimensions that are powers of two. The implementation handles a general input matrix of size $N \\times N$ by first checking if $N$ is a power of two. If it is not, the input matrices $A$ and $B$ are padded with zeros to create new square matrices of size $m \\times m$, where $m$ is the smallest power of two greater than or equal to $N$. The recursive algorithm is then called on these padded matrices. The resulting $m \\times m$ product matrix is then cropped to the original size $N \\times N$ before being returned. This padding-and-cropping wrapper ensures the core recursive logic can operate exclusively on dimensions that are powers of two.\n3.  **Recursive Step**: If $N > b$ and $N$ is a power of two, the matrices $A$ and $B$ are each partitioned into four sub-blocks of size $N/2 \\times N/2$. The algorithm then proceeds to compute the $7$ intermediate products $M_1, \\dots, M_7$ via recursive calls to `strassen_matmul`. For example, $M_1$ is calculated as `strassen_matmul(A11 + A22, B11 + B22, b)`. Finally, the sub-blocks of the result matrix $C$ are computed by combining these $7$ products according to Strassen's formulas (e.g., $C_{11} = M_1 + M_4 - M_5 + M_7$).\n\nThe benchmarking framework is encapsulated within a main `solve` function that processes a suite of test cases. For each test case, defined by a tuple of parameters $(\\text{sizes}, b, r, s)$:\n1.  **Reproducibility**: The random number generator is initialized with the seed $s$ using `numpy.random.seed(s)`. The input matrices $A$ and $B$ of size $N \\times N$ are then generated with entries drawn from a standard normal distribution, $\\mathcal{N}(0,1)$.\n2.  **Timing Measurement**: Prior to timed runs, one untimed \"warm-up\" call is made for each algorithm to mitigate cold-start effects. For an accurate performance measurement, each algorithm is executed $r$ times. The wall-clock time for each execution is measured in seconds using the high-resolution monotonic clock `time.perf_counter()`. The median of these $r$ time measurements is taken as the representative runtime, denoted $t_{\\mathrm{classical}}(N)$ and $t_{\\mathrm{Strassen}}(N)$ respectively. The median is used for its robustness to system-related timing noise.\n3.  **Numerical Validation**: As a sanity check, the correctness of the Strassen implementation is verified once per size $N$ by computing the maximum absolute entrywise difference between its output and the classical algorithm's output, ensuring it does not exceed a tolerance of $\\tau = 10^{-8}$. This check is not included in the timed portion of the benchmark.\n4.  **Crossover Decision**: After obtaining the median runtimes for all specified sizes $N$ in a test case, the program identifies the set of sizes for which $t_{\\mathrm{classical}}(N) \\le t_{\\mathrm{Strassen}}(N)$. The final result for the test case is the largest $N$ in this set. If the set is empty (i.e., Strassen's algorithm is faster for all test sizes), the result is $-1$.\n\nThe program systematically executes this procedure for all five test cases and collates the resulting crossover values. The final output is a single line containing these five integer results, formatted as a comma-separated list enclosed in square brackets, e.g., $[n_1,n_2,n_3,n_4,n_5]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport time\n\ndef classical_matmul(A, B):\n    \"\"\"\n    Computes the matrix product C = A @ B using NumPy's optimized dot product.\n    This serves as the baseline classical O(N^3) algorithm.\n    \"\"\"\n    return np.dot(A, B)\n\ndef strassen_matmul(A, B, b):\n    \"\"\"\n    Computes the matrix product C = A @ B using Strassen's algorithm.\n    - b: The base-case threshold. If N <= b, switches to classical_matmul.\n    - Handles non-power-of-two matrices by padding and cropping.\n    \"\"\"\n    N = A.shape[0]\n\n    # Base case: If the matrix size is below or at the threshold,\n    # delegate to the classical algorithm.\n    if N <= b:\n        return classical_matmul(A, B)\n\n    # If N is not a power of two, pad it to the next power of two.\n    is_power_of_two = (N > 0) and (N & (N - 1) == 0)\n    if not is_power_of_two:\n        # Find the next power of two\n        m = 1 << (N - 1).bit_length()\n        \n        # Create padded matrices\n        A_pad = np.zeros((m, m))\n        A_pad[:N, :N] = A\n        B_pad = np.zeros((m, m))\n        B_pad[:N, :N] = B\n        \n        # Perform Strassen multiplication on the padded matrices\n        C_pad = strassen_matmul(A_pad, B_pad, b)\n        \n        # Crop the result back to the original size\n        return C_pad[:N, :N]\n\n    # --- Recursive Step ---\n    # At this point, N is guaranteed to be a power of two and N > b.\n    mid = N // 2\n    \n    # Partition matrices into four sub-blocks of size mid x mid\n    A11, A12 = A[:mid, :mid], A[:mid, mid:]\n    A21, A22 = A[mid:, :mid], A[mid:, mid:]\n    B11, B12 = B[:mid, :mid], B[:mid, mid:]\n    B21, B22 = B[mid:, :mid], B[mid:, mid:]\n\n    # 7 recursive calls as per Strassen's algorithm\n    M1 = strassen_matmul(A11 + A22, B11 + B22, b)\n    M2 = strassen_matmul(A21 + A22, B11, b)\n    M3 = strassen_matmul(A11, B12 - B22, b)\n    M4 = strassen_matmul(A22, B21 - B11, b)\n    M5 = strassen_matmul(A11 + A12, B22, b)\n    M6 = strassen_matmul(A21 - A11, B11 + B12, b)\n    M7 = strassen_matmul(A12 - A22, B21 + B22, b)\n\n    # Combine the 7 products to form the sub-blocks of the result matrix C\n    C11 = M1 + M4 - M5 + M7\n    C12 = M3 + M5\n    C21 = M2 + M4\n    C22 = M1 - M2 + M3 + M6\n\n    # Assemble the final result matrix C from its sub-blocks\n    C = np.empty((N, N), dtype=A.dtype)\n    C[:mid, :mid] = C11\n    C[:mid, mid:] = C12\n    C[mid:, :mid] = C21\n    C[mid:, mid:] = C22\n\n    return C\n\ndef solve():\n    \"\"\"\n    Main function to run the benchmarking suite and find crossover points.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (sizes, base_case_threshold, repeats, seed)\n        ([16, 32, 64], 16, 3, 0),\n        ([24, 32, 48], 8, 3, 1),\n        ([8, 16, 32], 64, 5, 2),\n        ([32, 64], 32, 2, 3),\n        ([16], 8, 5, 4),\n    ]\n\n    results = []\n    tau = 1e-8\n\n    for sizes, b, r, s in test_cases:\n        np.random.seed(s)\n        \n        crossover_candidates = []\n        \n        for N in sizes:\n            # Generate reproducible random matrices\n            A = np.random.randn(N, N)\n            B = np.random.randn(N, N)\n            \n            # Untimed warm-up calls to mitigate cold-start artifacts\n            _ = classical_matmul(A, B)\n            _ = strassen_matmul(A, B, b)\n            \n            # Benchmark classical algorithm\n            classical_times = []\n            for _ in range(r):\n                start = time.perf_counter()\n                C_classical = classical_matmul(A, B)\n                end = time.perf_counter()\n                classical_times.append(end - start)\n            t_classical = np.median(classical_times)\n            \n            # Benchmark Strassen algorithm\n            strassen_times = []\n            for _ in range(r):\n                start = time.perf_counter()\n                C_strassen = strassen_matmul(A, B, b)\n                end = time.perf_counter()\n                strassen_times.append(end - start)\n            t_strassen = np.median(strassen_times)\n            \n            # Verify numerical correctness (not part of timing)\n            max_abs_diff = np.max(np.abs(C_classical - C_strassen))\n            if max_abs_diff > tau:\n                # Per problem spec, this is an implementation error but should not\n                # alter program completion or output format.\n                pass \n\n            # Check if classical is no slower than Strassen\n            if t_classical <= t_strassen:\n                crossover_candidates.append(N)\n        \n        # Determine the crossover decision for the current case\n        if not crossover_candidates:\n            case_result = -1\n        else:\n            case_result = max(crossover_candidates)\n            \n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3209812"}, {"introduction": "An algorithm's performance is not only determined by its abstract steps but also by the data structures used to implement it. This practice investigates how the choice between an adjacency list and an adjacency matrix impacts the efficiency of the Breadth-First Search (BFS) algorithm, particularly in relation to the graph's density. Instead of measuring wall-clock time, this benchmark [@problem_id:3209872] introduces a more rigorous, machine-independent metric: counting the number of \"adjacency examinations.\" This approach provides deeper insight into the fundamental data access patterns that govern algorithmic complexity.", "problem": "You are asked to design and implement a principled benchmark that compares Breadth-First Search (BFS) on two common graph representations: an adjacency matrix and an adjacency list. The benchmark must be rooted in a fundamental operations model rather than wall-clock time to ensure reproducibility and numerical determinism across environments. Specifically, measure the number of adjacency examinations performed by BFS in each representation under well-defined rules, and report these counts for a provided test suite of graphs.\n\nFundamental base and definitions:\n- A graph has $V$ vertices and $E$ edges. All graphs in this problem are undirected, simple (no self-loops and no parallel edges).\n- Breadth-First Search (BFS) explores a graph level by level from a specified source vertex.\n- Adjacency list representation: each vertex $v$ stores a list of its neighbors. The degree of vertex $v$ is $\\deg(v)$.\n- Adjacency matrix representation: a binary matrix $A \\in \\{0,1\\}^{V \\times V}$ where $A_{ij} = 1$ if and only if there is an edge between vertex $i$ and vertex $j$.\n- Performance metric to count:\n  1. In the adjacency list BFS, count one adjacency examination for each neighbor encountered when iterating the list of each dequeued vertex. Formally, when BFS dequeues a vertex $u$, increment the count by $\\lvert \\text{Adj}(u) \\rvert$, where $\\text{Adj}(u)$ is the neighbor list of $u$.\n  2. In the adjacency matrix BFS, count one adjacency examination for each matrix entry scanned in the row of the dequeued vertex. Formally, when BFS dequeues a vertex $u$, increment the count by $V$ as it scans all $V$ entries $A_{u,0},A_{u,1},\\ldots,A_{u,V-1}$.\n\nScientific realism and rationale:\n- For the adjacency list, the total number of adjacency examinations performed by BFS equals $\\sum_{v \\in R} \\deg(v)$, where $R$ is the set of vertices dequeued (reachable from the source). For undirected graphs, this sum equals $2\\lvert E_R \\rvert$, where $E_R$ are the edges internal to the reachable component.\n- For the adjacency matrix, BFS scans one full row per dequeued vertex, yielding exactly $V \\cdot \\lvert R \\rvert$ examinations, independent of sparsity.\n\nProgram requirements:\n- Implement BFS for both representations exactly once per graph and source. Maintain a visited set to prevent revisiting vertices.\n- Count adjacency examinations as defined above; do not count other operations.\n- The program must be self-contained and produce results deterministically without randomness or external input.\n\nTest suite (all graphs undirected):\n- Test case $1$ (sparse path):\n  - $V = 10$.\n  - Edges: $\\{(0,1),(1,2),(2,3),(3,4),(4,5),(5,6),(6,7),(7,8),(8,9)\\}$.\n  - Source: $0$.\n- Test case $2$ (dense complete graph):\n  - $V = 8$.\n  - Edges: all pairs $(i,j)$ with $0 \\le i < j \\le 7$.\n  - Source: $0$.\n- Test case $3$ (star graph):\n  - $V = 10$.\n  - Edges: $\\{(0,1),(0,2),(0,3),(0,4),(0,5),(0,6),(0,7),(0,8),(0,9)\\}$.\n  - Source: $0$.\n- Test case $4$ (disconnected graph: a path and a triangle with isolates):\n  - $V = 12$.\n  - Edges: path $\\{(0,1),(1,2),(2,3)\\}$ and triangle $\\{(4,5),(5,6),(4,6)\\}$; vertices $\\{7,8,9,10,11\\}$ are isolated.\n  - Source: $4$.\n- Test case $5$ (edgeless graph):\n  - $V = 12$.\n  - Edges: $\\varnothing$.\n  - Source: $3$.\n\nFinal output specification:\n- For each test case, compute the pair $[\\text{list\\_count}, \\text{matrix\\_count}]$, where $\\text{list\\_count}$ is the total number of adjacency examinations in the adjacency list BFS and $\\text{matrix\\_count}$ is the total number in the adjacency matrix BFS.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case represented as a two-element list. For example, the output format must be like $\\texttt{[[l_1,m_1],[l_2,m_2],\\ldots]}$ with no additional text.\n\nThe task assesses understanding of how representation affects algorithmic complexity and data access patterns, emphasizing deterministic benchmarking grounded in counting fundamental operations rather than time measurements.", "solution": "The problem statement has been critically validated and is deemed **valid**. It is scientifically grounded, well-posed, objective, and self-contained. The problem provides a rigorous and formalizable framework for benchmarking Breadth-First Search (BFS) performance on adjacency list versus adjacency matrix representations by defining a deterministic, machine-independent performance metric: the count of \"adjacency examinations\". All definitions, constraints, and test cases are clear, consistent, and sufficient for deriving a unique, verifiable solution.\n\nThe solution is designed based on the fundamental principles of graph theory and algorithm analysis. We will implement BFS for both representations and count the specified operations for a suite of test graphs.\n\n### Algorithmic Design and Principles\n\nThe core of the task is to compare two implementations of Breadth-First Search, distinguished by the underlying graph data structure. The performance is measured not by time, but by counting a fundamental operation: examining adjacencies.\n\n**1. Breadth-First Search (BFS) Framework**\n\nBFS systematically explores a graph from a given source vertex $s$. It uses a queue to manage vertices to be visited, ensuring that it explores the graph level by level. A `visited` set is maintained to prevent cycles and redundant processing. The general algorithm is as follows:\n1. Initialize a queue and add the source vertex $s$ to it.\n2. Initialize a `visited` set containing only $s$.\n3. Initialize an examination counter to $0$.\n4. While the queue is not empty:\n   a. Dequeue a vertex $u$.\n   b. **Increment the examination counter** based on the representation-specific rule.\n   c. For each neighbor $v$ of $u$:\n      i. If $v$ has not been visited:\n         - Mark $v$ as visited.\n         - Enqueue $v$.\n5. Return the final examination count.\n\n**2. Adjacency List Implementation**\n\n- **Representation**: A graph with $V$ vertices is represented as an array of lists. The element at index $u$ of the array is a list containing all vertices adjacent to $u$. For an undirected graph, if an edge $(u,v)$ exists, $v$ appears in $u$'s list and $u$ appears in $v$'s list. The length of the list for vertex $u$ is its degree, $\\deg(u)$.\n\n- **Cost Model**: According to the problem definition, finding the neighbors of a dequeued vertex $u$ involves iterating through its adjacency list. The cost of this operation is defined as the length of this list. Therefore, upon dequeuing $u$, the examination count is incremented by $\\deg(u)$.\n\n- **Total Examinations**: The BFS algorithm processes each vertex in the reachable component from the source exactly once. Let $R$ be the set of vertices reachable from the source. The total number of examinations is the sum of the degrees of all vertices in this set:\n$$\n\\text{list\\_count} = \\sum_{u \\in R} \\deg(u)\n$$\nFor an undirected graph, this sum is equal to $2|E_R|$, where $E_R$ is the set of edges with both endpoints in $R$. This highlights that the performance is directly proportional to the number of edges in the explored component, making it efficient for sparse graphs. The time complexity is $O(|R| + |E_R|)$.\n\n**3. Adjacency Matrix Implementation**\n\n- **Representation**: A graph with $V$ vertices is represented by a $V \\times V$ binary matrix $A$, where $A_{ij} = 1$ if there is an edge between vertex $i$ and vertex $j$, and $A_{ij} = 0$ otherwise. For an undirected graph, this matrix is symmetric ($A_{ij} = A_{ji}$).\n\n- **Cost Model**: To find the neighbors of a dequeued vertex $u$, one must scan the entire $u$-th row of the matrix $A$. The problem formalizes the cost of this scan as $V$ examinations, regardless of how many neighbors are actually found.\n\n- **Total Examinations**: The BFS algorithm dequeues each of the $|R|$ reachable vertices once. For each such vertex, it incurs a cost of $V$ examinations. The total number of examinations is therefore:\n$$\n\\text{matrix\\_count} = |R| \\cdot V\n$$\nThis model demonstrates that the performance is dependent on the total number of vertices in the graph, not the number of edges. This can be inefficient for sparse graphs where $|E|$ is much smaller than $V^2$, as the algorithm must still perform work proportional to $V$ for each processed vertex. The time complexity is $O(|R| \\cdot V)$ or, if considering the full component, $O(V^2)$.\n\n**4. Implementation Details**\n\nThe provided solution implements two functions, `bfs_adj_list` and `bfs_adj_matrix`, which execute the BFS algorithm according to the respective cost models described above. The `collections.deque` object is used for an efficient queue implementation. A main `solve` function defines the test suite, calls the BFS functions for each case, and formats the output as a list of pairs $[\\text{list\\_count}, \\text{matrix\\_count}]$, precisely adhering to the specified output format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom collections import deque\n\ndef bfs_adj_list(V, edges, source):\n    \"\"\"\n    Performs BFS on an adjacency list representation and counts adjacency examinations.\n    \n    An examination is counted for each neighbor of a dequeued vertex, effectively summing\n    the degrees of all reachable vertices.\n    \"\"\"\n    if V == 0:\n        return 0\n        \n    adj = [[] for _ in range(V)]\n    for u, v in edges:\n        adj[u].append(v)\n        adj[v].append(u)\n\n    queue = deque([source])\n    # A set is used for O(1) average time complexity for additions and checks.\n    visited = {source}\n    list_count = 0\n\n    while queue:\n        u = queue.popleft()\n        \n        # Per the problem definition, increment count by degree of the dequeued vertex.\n        list_count += len(adj[u])\n\n        for v in adj[u]:\n            if v not in visited:\n                visited.add(v)\n                queue.append(v)\n                \n    return list_count\n\ndef bfs_adj_matrix(V, edges, source):\n    \"\"\"\n    Performs BFS on an adjacency matrix representation and counts adjacency examinations.\n    \n    An examination of V entries is counted for each dequeued vertex, reflecting the\n    cost of scanning an entire matrix row.\n    \"\"\"\n    if V == 0:\n        return 0\n\n    matrix = np.zeros((V, V), dtype=int)\n    for u, v in edges:\n        matrix[u, v] = 1\n        matrix[v, u] = 1\n\n    queue = deque([source])\n    visited = {source}\n    matrix_count = 0\n\n    while queue:\n        u = queue.popleft()\n\n        # Per the problem definition, increment count by V for scanning the row.\n        matrix_count += V\n\n        for v in range(V):\n            if matrix[u, v] == 1:\n                if v not in visited:\n                    visited.add(v)\n                    queue.append(v)\n                    \n    return matrix_count\n\ndef solve():\n    \"\"\"\n    Defines the test suite, runs the benchmarks, and prints the results in the\n    specified format.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Sparse path graph\n        (10, [(i, i + 1) for i in range(9)], 0),\n        # Case 2: Dense complete graph\n        (8, [(i, j) for i in range(8) for j in range(i + 1, 8)], 0),\n        # Case 3: Star graph\n        (10, [(0, i) for i in range(1, 10)], 0),\n        # Case 4: Disconnected graph\n        (12, [(0, 1), (1, 2), (2, 3), (4, 5), (5, 6), (4, 6)], 4),\n        # Case 5: Edgeless graph\n        (12, [], 3)\n    ]\n\n    results = []\n    for V, edges, source in test_cases:\n        list_count = bfs_adj_list(V, edges, source)\n        matrix_count = bfs_adj_matrix(V, edges, source)\n        results.append([list_count, matrix_count])\n\n    # Format each result pair as '[l,m]' without spaces.\n    formatted_results = [f\"[{r[0]},{r[1]}]\" for r in results]\n    \n    # Final print statement in the exact required format '[[l1,m1],[l2,m2],...]'.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3209872"}, {"introduction": "The efficiency of iterative solvers, a cornerstone of large-scale scientific computing, is highly dependent on the mathematical properties of the problem itself. This exercise demonstrates the relationship between a matrix's condition number, $\\kappa_2(A)$, and the convergence rate of the Conjugate Gradient method. By systematically constructing a series of matrices with increasing condition numbers and measuring the corresponding number of iterations to convergence [@problem_id:3209858], you will gain direct, hands-on experience with how ill-conditioning can severely degrade solver performance. Understanding this connection is critical for diagnosing and solving challenging numerical problems.", "problem": "You will study how the iteration count of an iterative solver degrades as the condition number of the system matrix increases. Work entirely in exact, dimensionless, purely mathematical terms.\n\nStart from the following fundamental base:\n- For a real, symmetric positive definite matrix $A \\in \\mathbb{R}^{n \\times n}$, the spectral condition number in the spectral norm is $\\kappa_2(A) = \\lambda_{\\max}(A)/\\lambda_{\\min}(A)$, where $\\lambda_{\\max}(A)$ and $\\lambda_{\\min}(A)$ denote the largest and smallest eigenvalues of $A$, respectively.\n- The Conjugate Gradient method (abbreviated as CG) for solving $A x = b$ with $A$ symmetric positive definite iteratively minimizes the error in the $A$-norm over a Krylov subspace and terminates when the residual $r_k = b - A x_k$ satisfies a specified stopping criterion.\n\nTask. Implement a complete, runnable program that:\n1. Constructs, for each specified target condition number $\\kappa$, a family member $A(\\kappa)$ of real, symmetric positive definite matrices of size $n \\times n$ with $\\kappa_2(A(\\kappa)) = \\kappa$, by prescribing the eigenvalues and mixing with a fixed orthogonal matrix:\n   - Use $n = 64$.\n   - Let the eigenvalues be $\\lambda_i$ linearly spaced between $1$ and $\\kappa$ for $i = 1, \\dots, n$.\n   - Form $A(\\kappa) = Q^\\top \\operatorname{diag}(\\lambda_1,\\dots,\\lambda_n) Q$, where $Q$ is a fixed orthogonal matrix, generated reproducibly as follows: construct a dense matrix $M \\in \\mathbb{R}^{n \\times n}$ with independent, identically distributed standard normal entries using a fixed seed $0$; compute a reduced $Q R$ factorization $M = \\widehat{Q} R$; define $D = \\operatorname{diag}(\\operatorname{sign}(R_{11}), \\dots, \\operatorname{sign}(R_{nn}))$ with the convention $\\operatorname{sign}(0)=1$, and set $Q = \\widehat{Q} D$. This guarantees $R$ with positive diagonal and fixes the column signs of $Q$ deterministically.\n2. Solves $A(\\kappa) x = b$ using the Conjugate Gradient method with:\n   - Right-hand side $b = \\mathbf{1} \\in \\mathbb{R}^n$ (all ones).\n   - Initial guess $x_0 = 0$.\n   - Stopping criterion $\\lVert r_k \\rVert_2 \\le \\text{tol} \\cdot \\lVert b \\rVert_2$ with $\\text{tol} = 10^{-8}$, where $\\lVert \\cdot \\rVert_2$ is the Euclidean norm.\n   - Maximum number of iterations equal to $n$.\n3. Reports, for each test case, the number of iterations $k$ taken by the Conjugate Gradient method to satisfy the stopping criterion, or the maximum $n$ if the stopping criterion is not met within $n$ iterations.\n\nTest suite. Run the program for the following parameter values:\n- Case $1$: $(n, \\kappa, \\text{tol}) = (64, 1, 10^{-8})$.\n- Case $2$: $(n, \\kappa, \\text{tol}) = (64, 10, 10^{-8})$.\n- Case $3$: $(n, \\kappa, \\text{tol}) = (64, 100, 10^{-8})$.\n- Case $4$: $(n, \\kappa, \\text{tol}) = (64, 1000, 10^{-8})$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the same order as the test suite, for example, $[k_1,k_2,k_3,k_4]$, where each $k_i$ is an integer iteration count for Case $i$.\n\nYour implementation must not read any input and must not use any external files or network access. All computations must use double-precision arithmetic. No physical units are involved, and no angles are used. The reported quantities are integers.", "solution": "The problem is valid. It presents a well-posed and scientifically sound numerical experiment to investigate the effect of a matrix's condition number on the convergence of the Conjugate Gradient method. All parameters and procedures are specified precisely and unambiguously.\n\nThe objective is to quantify the relationship between the spectral condition number, $\\kappa_2(A)$, of a real, symmetric positive definite (SPD) matrix $A \\in \\mathbb{R}^{n \\times n}$ and the number of iterations required by the Conjugate Gradient (CG) method to solve the linear system $A x = b$. The experiment is designed to be fully reproducible and is conducted for a series of matrices with progressively increasing condition numbers.\n\nFirst, we must construct a family of matrices $A(\\kappa)$ of size $n \\times n$ with a precisely controlled condition number $\\kappa$. The problem specifies $n=64$. A standard method to achieve this is to define the matrix through its eigendecomposition, $A(\\kappa) = Q \\Lambda(\\kappa) Q^\\top$, or equivalently, $A(\\kappa) = Q^\\top \\Lambda(\\kappa) Q$ since $Q$ is orthogonal. The matrix $\\Lambda(\\kappa) = \\operatorname{diag}(\\lambda_1, \\dots, \\lambda_n)$ is a diagonal matrix containing the eigenvalues of $A(\\kappa)$. The condition number is defined as $\\kappa_2(A) = \\lambda_{\\max}(A)/\\lambda_{\\min}(A)$. To achieve a target condition number $\\kappa$, we can set $\\lambda_{\\min} = 1$ and $\\lambda_{\\max} = \\kappa$. The problem specifies that the $n$ eigenvalues, $\\lambda_i$, are to be linearly spaced between $1$ and $\\kappa$. Thus, the set of eigenvalues is $\\{\\lambda_i\\}_{i=1}^n$ where $\\lambda_i = 1 + (i-1)\\frac{\\kappa-1}{n-1}$. Since all $\\kappa \\ge 1$, all eigenvalues $\\lambda_i$ are greater than or equal to $1$, ensuring that the resulting matrix $A(\\kappa)$ is positive definite. The construction $A(\\kappa) = Q^\\top \\Lambda(\\kappa) Q$ guarantees that $A(\\kappa)$ is symmetric.\n\nThe orthogonal matrix $Q$ is fixed for all test cases to ensure that a consistent rotational transformation is applied to the eigensystem. This isolates the effect of the eigenvalues' distribution. The matrix $Q$ is generated deterministically to ensure reproducibility. This is accomplished by first creating a dense matrix $M \\in \\mathbb{R}^{n \\times n}$ with entries drawn from a standard normal distribution, using a fixed random seed of $0$. A QR factorization of $M$ is then computed, $M = \\widehat{Q}R$. The matrix $\\widehat{Q}$ is orthogonal, but the QR decomposition is not unique; the signs of the columns of $\\widehat{Q}$ and corresponding rows of $R$ can be flipped. To obtain a unique, deterministic $Q$, we enforce a convention that the diagonal elements of $R$ must be non-negative. This is achieved by creating a diagonal matrix $D = \\operatorname{diag}(\\operatorname{sign}(R_{11}), \\dots, \\operatorname{sign}(R_{nn}))$, with the specified convention $\\operatorname{sign}(0)=1$. The final, fixed orthogonal matrix is then $Q = \\widehat{Q}D$.\n\nWith the matrix $A(\\kappa)$ constructed, we solve the linear system $A(\\kappa)x = b$ using the Conjugate Gradient method. The parameters for the solver are: a right-hand side vector $b = \\mathbf{1} \\in \\mathbb{R}^n$ (a vector of all ones), an initial guess $x_0 = 0 \\in \\mathbb{R}^n$, and a maximum of $n=64$ iterations. The CG algorithm iteratively generates a sequence of solution approximations. The process starts with the initial residual $r_0 = b - Ax_0 = b$ and the initial search direction $p_0 = r_0$. For each iteration $k = 0, 1, 2, \\dots$, the algorithm computes:\n$$ \\alpha_k = \\frac{r_k^\\top r_k}{p_k^\\top A p_k} $$\n$$ x_{k+1} = x_k + \\alpha_k p_k $$\n$$ r_{k+1} = r_k - \\alpha_k A p_k $$\nThe iteration is terminated when the Euclidean norm of the residual, $\\lVert r_{k+1} \\rVert_2$, satisfies the stopping criterion $\\lVert r_{k+1} \\rVert_2 \\le \\text{tol} \\cdot \\lVert b \\rVert_2$, where the tolerance is $\\text{tol} = 10^{-8}$. If convergence is achieved at step $k+1$, the number of iterations is recorded as $k+1$. To continue the iteration, a new search direction is computed:\n$$ \\beta_k = \\frac{r_{k+1}^\\top r_{k+1}}{r_k^\\top r_k} $$\n$$ p_{k+1} = r_{k+1} + \\beta_k p_k $$\n\nThis entire procedure is executed for each of the specified test cases, corresponding to $\\kappa \\in \\{1, 10, 100, 1000\\}$. The number of iterations for each case is collected and reported. For $\\kappa=1$, $A(1)=I$, and the CG method is expected to converge in a single iteration. For increasing $\\kappa$, the problem becomes more ill-conditioned, and the number of iterations is expected to increase, potentially reaching the maximum limit of $n=64$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef conjugate_gradient(A, b, tol, max_iter):\n    \"\"\"\n    Solves the system Ax=b using the Conjugate Gradient method.\n\n    Args:\n        A (np.ndarray): The symmetric positive definite matrix of the system.\n        b (np.ndarray): The right-hand side vector.\n        tol (float): The relative tolerance for the stopping criterion.\n        max_iter (int): The maximum number of iterations.\n\n    Returns:\n        int: The number of iterations performed.\n    \"\"\"\n    n = A.shape[0]\n    x = np.zeros(n, dtype=np.float64)\n\n    r = b - A @ x  # Since x is zero, r = b\n    p = r.copy()\n    rs_old = np.dot(r, r)\n\n    norm_b = np.linalg.norm(b)\n    # Handle the trivial case where b is the zero vector.\n    if norm_b == 0.0:\n        return 0\n\n    stop_threshold = tol * norm_b\n\n    # The initial residual r_0 is b. If it already meets the criterion,\n    # 0 iterations are needed. This is unlikely for the given problem.\n    if np.linalg.norm(r) <= stop_threshold:\n        return 0\n\n    for k in range(1, max_iter + 1):\n        Ap = A @ p\n        \n        # Numerator for alpha is r_k^T * r_k\n        # Denominator is p_k^T * A * p_k\n        alpha = rs_old / np.dot(p, Ap)\n        \n        # Update solution and residual\n        x = x + alpha * p\n        r = r - alpha * Ap\n        \n        # Check for convergence using the L2-norm of the residual\n        if np.linalg.norm(r) <= stop_threshold:\n            return k\n            \n        rs_new = np.dot(r, r)\n        \n        # Update search direction\n        beta = rs_new / rs_old\n        p = r + beta * p\n        \n        rs_old = rs_new\n        \n    # If the loop completes, the method did not converge within max_iter.\n    return max_iter\n\ndef solve():\n    \"\"\"\n    Main function to run the numerical experiment and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, kappa, tol)\n        (64, 1.0, 1e-8),\n        (64, 10.0, 1e-8),\n        (64, 100.0, 1e-8),\n        (64, 1000.0, 1e-8),\n    ]\n\n    results = []\n    \n    # Common parameters\n    n = test_cases[0][0]\n    seed = 0\n\n    # 1. Construct the fixed orthogonal matrix Q, as per the problem description.\n    # Use the modern, preferred way of seeding for reproducibility.\n    rng = np.random.default_rng(seed)\n    M = rng.normal(size=(n, n))\n    \n    # Compute the QR factorization. For a square matrix, 'reduced' is default.\n    Q_hat, R = np.linalg.qr(M)\n    \n    # Create the sign-correction diagonal matrix D to ensure Q is unique.\n    # The problem specifies sign(0) = 1. numpy.sign(0) is 0, so we correct for it.\n    signs = np.sign(np.diag(R))\n    signs[signs == 0] = 1.0\n    # The final fixed orthogonal matrix Q.\n    Q = Q_hat @ np.diag(signs)\n\n    for n_case, kappa, tol in test_cases:\n        # 2. Construct the SPD matrix A(kappa) for the current case.\n        # Eigenvalues are linearly spaced between 1 and kappa.\n        eigenvalues = np.linspace(1.0, kappa, n, dtype=np.float64)\n        Lambda = np.diag(eigenvalues)\n        \n        # Form A = Q^T * Lambda * Q\n        A = Q.T @ Lambda @ Q\n\n        # 3. Define the linear system and solve it.\n        # Right-hand side is a vector of all ones.\n        b = np.ones(n, dtype=np.float64)\n        \n        # The maximum number of iterations is n.\n        max_iterations = n_case\n        \n        # Run the Conjugate Gradient solver.\n        iterations_count = conjugate_gradient(A, b, tol, max_iterations)\n        results.append(iterations_count)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3209858"}]}