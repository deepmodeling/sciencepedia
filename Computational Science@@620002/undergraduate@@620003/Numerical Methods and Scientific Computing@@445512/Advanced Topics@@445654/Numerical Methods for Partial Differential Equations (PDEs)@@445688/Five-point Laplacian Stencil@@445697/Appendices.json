{"hands_on_practices": [{"introduction": "After deriving the five-point stencil from Taylor series, a crucial step is to verify its theoretical accuracy through numerical experiments. This practice guides you through the process of a grid refinement study, a fundamental technique in computational science for confirming that an implementation behaves as expected. By applying the discrete Laplacian to known analytical functions and measuring the error as the grid becomes finer, you will numerically confirm the stencil's second-order convergence, bridging the gap between mathematical theory and practical code. [@problem_id:3230823]", "problem": "You are asked to work from first principles to analyze and implement the five-point discrete Laplacian operator for a manufactured solution, and then perform a grid refinement study to confirm second-order convergence. Begin from the definition of the Laplacian and the Taylor series of smooth functions. Do not use any pre-derived discrete formula; instead, derive it using the fundamental base described below.\n\nFundamental base:\n- Definition of the Laplacian: for a sufficiently smooth scalar field $u(x,y)$, the Laplacian is $\\Delta u(x,y) = \\dfrac{\\partial^2 u}{\\partial x^2}(x,y) + \\dfrac{\\partial^2 u}{\\partial y^2}(x,y)$.\n- Taylor series for a smooth function in one variable $x$ about a point $x_0$ is $u(x_0 + h) = u(x_0) + h\\,u_x(x_0) + \\dfrac{h^2}{2}\\,u_{xx}(x_0) + \\dfrac{h^3}{6}\\,u_{xxx}(x_0) + \\dfrac{h^4}{24}\\,u_{xxxx}(x_0) + \\mathcal{O}(h^5)$, and similarly for $u(x_0 - h)$.\n- The central finite difference approximation for a second derivative in one dimension can be derived from the Taylor series expansions above.\n\nTask:\n1. Using the Taylor series expansions around a grid point $(x_i,y_j)$ with spacings $h_x$ in the $x$ direction and $h_y$ in the $y$ direction, derive the five-point discrete Laplacian stencil that approximates $\\Delta u(x_i,y_j)$ by a linear combination of $u(x_{i\\pm 1},y_j)$ and $u(x_i,y_{j\\pm 1})$ and $u(x_i,y_j)$. Your derivation must clearly identify the leading-order truncation error term and show that the local truncation error is of order $\\mathcal{O}(h_x^2 + h_y^2)$ for sufficiently smooth $u$.\n2. Implement a program that, for each manufactured solution $u(x,y)$ and its exact Laplacian $\\Delta u(x,y)$, computes the discrete Laplacian at interior grid points on a rectangular domain using Dirichlet boundary values from the exact $u(x,y)$. Compute the error as the maximum absolute difference between the discrete Laplacian and the exact Laplacian over all interior grid points.\n3. Perform a grid refinement study: for a sequence of meshes where the spacing in each direction is halved successively, compute the errors and the observed convergence orders $p_k = \\log_2\\!\\left(\\dfrac{E(h_k)}{E(h_{k+1})}\\right)$, where $E(h)$ is the chosen error norm and $h$ denotes the grid spacing in the $x$ direction. In anisotropic cases, keep the ratio $h_y/h_x$ constant while refining both $h_x$ and $h_y$ by a factor of $2$.\n4. For trigonometric functions, angles must be interpreted in radians.\n5. Round each observed order $p_k$ to $3$ decimal places.\n\nError norm:\n- Use the maximum norm over interior nodes: $E(h_x,h_y) = \\max_{i,j} \\left| \\Delta_h u(x_i,y_j) - \\Delta u(x_i,y_j) \\right|$, where $\\Delta_h$ is your derived five-point discrete Laplacian and $(x_i,y_j)$ ranges over interior grid points only.\n\nTest suite:\n- Domain for all tests: $[0,1]\\times[0,1]$.\n- Boundary values: use exact $u(x,y)$ on $\\partial([0,1]\\times[0,1])$.\n- For each test case, use successive refinements by halving $h_x$ (and $h_y$ accordingly) to obtain a sequence of $4$ meshes, yielding $3$ observed orders $p_k$.\n\nProvide the following four test cases:\n- Case $1$ (happy path, isotropic):\n  - $u_1(x,y) = \\sin(\\pi x)\\sin(\\pi y)$, with $\\Delta u_1(x,y) = -2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$.\n  - Initial spacings: $h_x = h_y = 1/8$.\n- Case $2$ (smooth exponential, isotropic):\n  - $u_2(x,y) = e^{x+y}$, with $\\Delta u_2(x,y) = 2 e^{x+y}$.\n  - Initial spacings: $h_x = h_y = 1/8$.\n- Case $3$ (anisotropic grid, fixed ratio $h_y/h_x = 2$):\n  - $u_3(x,y) = \\sin(\\pi x)\\sin(\\pi y)$, with $\\Delta u_3(x,y) = -2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$.\n  - Initial spacings: $h_x = 1/16$, $h_y = 1/8$.\n- Case $4$ (coarser initial grid, polynomial solution):\n  - $u_4(x,y) = x^4 + y^4$, with $\\Delta u_4(x,y) = 12 x^2 + 12 y^2$.\n  - Initial spacings: $h_x = h_y = 1/4$.\n\nOutput specification:\n- Your program should produce a single line of output containing one top-level list of four sublists, one per test case. Each sublist must contain the three rounded observed orders $p_k$ for successive refinement pairs, formatted as a comma-separated list with no spaces and enclosed in square brackets. The four sublists must be combined as a comma-separated list with no spaces and enclosed in square brackets. For example: $[[2.000,2.000,2.000],[2.000,2.000,2.000],[2.000,2.000,2.000],[2.000,2.000,2.000]]$.", "solution": "The problem is valid as it is scientifically sound, well-posed, and based on established principles of numerical analysis. It requires the derivation of the five-point finite difference stencil for the Laplacian and a numerical verification of its convergence order, which is a standard and well-defined task.\n\n**Part 1: Derivation of the Five-Point Laplacian Stencil and Truncation Error**\n\nWe seek an approximation for the Laplacian operator, $\\Delta u(x,y) = \\dfrac{\\partial^2 u}{\\partial x^2}(x,y) + \\dfrac{\\partial^2 u}{\\partial y^2}(x,y)$, at a grid point $(x_i, y_j)$. We begin by approximating the second partial derivatives $\\dfrac{\\partial^2 u}{\\partial x^2}$ and $\\dfrac{\\partial^2 u}{\\partial y^2}$ using central differences derived from Taylor series expansions.\n\nLet's consider the function $u(x,y)$'s behavior along the line $y=y_j$. The Taylor series for $u(x, y_j)$ expanded around $x=x_i$ with a step size of $h_x$ are given by:\n$$ u(x_i + h_x, y_j) = u(x_i, y_j) + h_x \\frac{\\partial u}{\\partial x}(x_i, y_j) + \\frac{h_x^2}{2} \\frac{\\partial^2 u}{\\partial x^2}(x_i, y_j) + \\frac{h_x^3}{6} \\frac{\\partial^3 u}{\\partial x^3}(x_i, y_j) + \\frac{h_x^4}{24} \\frac{\\partial^4 u}{\\partial x^4}(x_i, y_j) + \\mathcal{O}(h_x^5) $$\n$$ u(x_i - h_x, y_j) = u(x_i, y_j) - h_x \\frac{\\partial u}{\\partial x}(x_i, y_j) + \\frac{h_x^2}{2} \\frac{\\partial^2 u}{\\partial x^2}(x_i, y_j) - \\frac{h_x^3}{6} \\frac{\\partial^3 u}{\\partial x^3}(x_i, y_j) + \\frac{h_x^4}{24} \\frac{\\partial^4 u}{\\partial x^4}(x_i, y_j) + \\mathcal{O}(h_x^5) $$\n\nUsing the compact index notation $u_{i,j} = u(x_i, y_j)$, $u_{i+1, j} = u(x_i+h_x, y_j)$, etc., and adding the two expansions, we eliminate the odd-order derivative terms:\n$$ u_{i+1,j} + u_{i-1,j} = 2u_{i,j} + h_x^2 \\frac{\\partial^2 u}{\\partial x^2}(x_i, y_j) + \\frac{h_x^4}{12} \\frac{\\partial^4 u}{\\partial x^4}(x_i, y_j) + \\mathcal{O}(h_x^6) $$\nNote that the error term becomes $\\mathcal{O}(h_x^6)$ because the $\\mathcal{O}(h_x^5)$ terms from the two series cancel out.\n\nRearranging this equation to solve for the second partial derivative with respect to $x$:\n$$ \\frac{\\partial^2 u}{\\partial x^2}(x_i, y_j) = \\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{h_x^2} - \\frac{h_x^2}{12} \\frac{\\partial^4 u}{\\partial x^4}(x_i, y_j) + \\mathcal{O}(h_x^4) $$\nThis gives us the central difference approximation for $\\dfrac{\\partial^2 u}{\\partial x^2}$:\n$$ \\delta_{xx}u_{i,j} = \\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{h_x^2} $$\nThe local truncation error for this approximation is the difference between the exact derivative and its approximation:\n$$ \\tau_{xx} = \\frac{\\partial^2 u}{\\partial x^2}(x_i, y_j) - \\delta_{xx}u_{i,j} = -\\frac{h_x^2}{12} \\frac{\\partial^4 u}{\\partial x^4}(x_i, y_j) + \\mathcal{O}(h_x^4) $$\nThe leading-order term shows that this approximation is second-order accurate in $h_x$.\n\nBy an identical argument for the partial derivative with respect to $y$ with step size $h_y$, we obtain:\n$$ \\frac{\\partial^2 u}{\\partial y^2}(x_i, y_j) = \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{h_y^2} - \\frac{h_y^2}{12} \\frac{\\partial^4 u}{\\partial y^4}(x_i, y_j) + \\mathcal{O}(h_y^4) $$\nThe corresponding approximation and truncation error are:\n$$ \\delta_{yy}u_{i,j} = \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{h_y^2} $$\n$$ \\tau_{yy} = \\frac{\\partial^2 u}{\\partial y^2}(x_i, y_j) - \\delta_{yy}u_{i,j} = -\\frac{h_y^2}{12} \\frac{\\partial^4 u}{\\partial y^4}(x_i, y_j) + \\mathcal{O}(h_y^4) $$\n\nThe five-point discrete Laplacian, denoted $\\Delta_h u(x_i, y_j)$, is the sum of these two central difference operators:\n$$ \\Delta_h u(x_i, y_j) = \\delta_{xx}u_{i,j} + \\delta_{yy}u_{i,j} = \\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{h_x^2} + \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{h_y^2} $$\nThis can be rewritten as a linear combination of the value at the center point $(i,j)$ and its four nearest neighbors:\n$$ \\Delta_h u_{i,j} = \\frac{1}{h_x^2} u_{i+1,j} + \\frac{1}{h_x^2} u_{i-1,j} + \\frac{1}{h_y^2} u_{i,j+1} + \\frac{1}{h_y^2} u_{i,j-1} - 2\\left(\\frac{1}{h_x^2} + \\frac{1}{h_y^2}\\right)u_{i,j} $$\nThis completes the derivation of the five-point stencil.\n\nThe local truncation error $\\tau$ for the Laplacian approximation is the sum of the truncation errors for the second derivatives:\n$$ \\tau(x_i, y_j) = \\Delta u(x_i, y_j) - \\Delta_h u(x_i, y_j) = \\tau_{xx} + \\tau_{yy} $$\n$$ \\tau(x_i, y_j) = \\left(-\\frac{h_x^2}{12} \\frac{\\partial^4 u}{\\partial x^4} + \\mathcal{O}(h_x^4)\\right) + \\left(-\\frac{h_y^2}{12} \\frac{\\partial^4 u}{\\partial y^4} + \\mathcal{O}(h_y^4)\\right) $$\nAssuming the function $u(x,y)$ is sufficiently smooth (i.e., its fourth partial derivatives are continuous, $u \\in C^4$), the leading-order truncation error is:\n$$ \\tau(x_i,y_j) = -\\frac{1}{12} \\left( h_x^2 \\frac{\\partial^4 u}{\\partial x^4}(x_i, y_j) + h_y^2 \\frac{\\partial^4 u}{\\partial y^4}(x_i, y_j) \\right) + \\text{H.O.T.} $$\nThe order of the local truncation error is therefore $\\mathcal{O}(h_x^2 + h_y^2)$. For an isotropic grid where $h_x = h_y = h$, the error is $\\mathcal{O}(h^2)$.\n\n**Part 2: Grid Refinement and Convergence Order**\n\nThe total error, measured by the maximum norm $E(h_x, h_y) = \\max_{i,j} \\left| \\Delta_h u(x_i,y_j) - \\Delta u(x_i,y_j) \\right|$, is expected to behave proportionally to the local truncation error for stable schemes. Thus, we expect $E \\approx C(h_x^2 + h_y^2)$ for some constant $C$ that depends on the higher derivatives of $u$.\n\nIn our grid refinement study, we successively halve the grid spacings, so $h_k \\to h_{k+1} = h_k/2$. Considering an isotropic grid where $h_x = h_y = h$, we have $E(h) \\approx C h^2$. The error at the next refinement level is $E(h/2) \\approx C (h/2)^2 = (C h^2)/4 = E(h)/4$. The theoretical convergence rate $p$ is calculated as:\n$$ p = \\log_2\\left(\\frac{E(h)}{E(h/2)}\\right) \\approx \\log_2\\left(\\frac{C h^2}{C (h/2)^2}\\right) = \\log_2(4) = 2 $$\nThis confirms the expected second-order convergence of the five-point Laplacian stencil. The same reasoning applies to the anisotropic case where the ratio $h_y/h_x$ is held constant, as the error term $h_x^2 + h_y^2$ will still be proportional to $h_x^2$ (or $h_y^2$), leading to an observed order of $2$. The program will compute this observed order $p_k = \\log_2(E(h_k)/E(h_{k+1}))$ for a sequence of errors $E_k$ corresponding to successively refined meshes. We expect $p_k \\approx 2$ for all test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_refinement_study(u_func, lap_u_func, hx_initial, hy_initial):\n    \"\"\"\n    Performs a grid refinement study for a given manufactured solution.\n    Computes errors on 4 successively refined meshes and returns the 3\n    observed convergence orders.\n    \"\"\"\n    errors = []\n    num_meshes = 4\n\n    for k in range(num_meshes):\n        hx = hx_initial / (2**k)\n        hy = hy_initial / (2**k)\n\n        # Determine number of grid points from spacings for the [0,1] domain.\n        # Number of intervals is 1/h. Number of points is num_intervals + 1.\n        # Use rounding to handle potential floating point inaccuracies.\n        nx_intervals = int(round(1.0 / hx))\n        ny_intervals = int(round(1.0 / hy))\n        \n        nx = nx_intervals + 1\n        ny = ny_intervals + 1\n\n        # Create grid coordinates.\n        x = np.linspace(0.0, 1.0, nx)\n        y = np.linspace(0.0, 1.0, ny)\n        \n        # Create 2D grid using 'ij' indexing to align with matrix (row, col) convention.\n        xx, yy = np.meshgrid(x, y, indexing='ij')\n\n        # Evaluate exact solution on the entire grid to get Dirichlet boundary values.\n        U = u_func(xx, yy)\n\n        # Compute the discrete Laplacian on the interior grid points using slicing.\n        # The equation is: (U(i+1,j) - 2U(i,j) + U(i-1,j))/hx^2 + (U(i,j+1) - 2U(i,j) + U(i,j-1))/hy^2\n        # Slices correspond to i from 1 to nx-2 and j from 1 to ny-2.\n        U_center = U[1:-1, 1:-1]\n        term_x = (U[2:, 1:-1] - 2 * U_center + U[:-2, 1:-1]) / (hx**2)\n        term_y = (U[1:-1, 2:] - 2 * U_center + U[1:-1, :-2]) / (hy**2)\n        lap_h_U = term_x + term_y\n\n        # Compute the exact Laplacian on the interior grid points.\n        xx_interior = xx[1:-1, 1:-1]\n        yy_interior = yy[1:-1, 1:-1]\n        lap_U_exact = lap_u_func(xx_interior, yy_interior)\n\n        # Compute the maximum norm of the error over the interior grid.\n        error = np.max(np.abs(lap_h_U - lap_U_exact))\n        errors.append(error)\n\n    # Compute observed orders of convergence, p_k = log2(E_k / E_{k+1}).\n    orders = []\n    for k in range(num_meshes - 1):\n        if errors[k] > 0 and errors[k+1] > 0:\n            order = np.log2(errors[k] / errors[k+1])\n            orders.append(order)\n        else:\n            # This case (zero error) is not expected with the given functions,\n            # but is included for robustness.\n            orders.append(float('nan')) \n    \n    return orders\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the convergence studies.\n    \"\"\"\n    # Define manufactured solutions and their exact Laplacians.\n    u1 = lambda x, y: np.sin(np.pi * x) * np.sin(np.pi * y)\n    lap_u1 = lambda x, y: -2 * np.pi**2 * np.sin(np.pi * x) * np.sin(np.pi * y)\n    \n    u2 = lambda x, y: np.exp(x + y)\n    lap_u2 = lambda x, y: 2 * np.exp(x + y)\n    \n    u4 = lambda x, y: x**4 + y**4\n    lap_u4 = lambda x, y: 12 * x**2 + 12 * y**2\n\n    # Define the test cases as a list of tuples.\n    # Each tuple: (u_function, laplacian_function, initial_hx, initial_hy)\n    test_cases = [\n        # Case 1: Isotropic grid, trigonometric solution\n        (u1, lap_u1, 1/8, 1/8),\n        # Case 2: Isotropic grid, exponential solution\n        (u2, lap_u2, 1/8, 1/8),\n        # Case 3: Anisotropic grid, trigonometric solution\n        (u1, lap_u1, 1/16, 1/8),\n        # Case 4: Isotropic coarse grid, polynomial solution\n        (u4, lap_u4, 1/4, 1/4),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        u_func, lap_u_func, hx0, hy0 = case\n        orders = run_refinement_study(u_func, lap_u_func, hx0, hy0)\n        # Round orders to 3 decimal places as required.\n        rounded_orders = [round(p, 3) for p in orders]\n        all_results.append(rounded_orders)\n\n    # Format the final output string to match the exact specification.\n    # Example: [[2.000,2.000,2.000],[...]]\n    sublist_strings = []\n    for res in all_results:\n        # Format each number to 3 decimal places to ensure consistent output.\n        formatted_numbers = [f\"{x:.3f}\" for x in res]\n        sublist_strings.append(f\"[{','.join(formatted_numbers)}]\")\n    \n    final_output = f\"[{','.join(sublist_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "3230823"}, {"introduction": "While the five-point stencil exhibits second-order accuracy for smooth solutions on simple domains, its performance can degrade in more complex scenarios. This exercise explores such a case by applying the stencil to solve the Laplace equation on an L-shaped domain, which features a 're-entrant' corner known to introduce a singularity into the solution. You will observe that the global convergence rate is reduced below the ideal second order, providing a critical insight: the accuracy of a numerical method is intrinsically linked to the regularity of the problem being solved. [@problem_id:3230895]", "problem": "Consider the Dirichlet boundary value problem for the Laplace equation on an L-shaped domain. Let the computational box be the square $[-1,1]\\times[-1,1]$ and define the L-shaped domain $\\Omega$ by removing the first quadrant, that is $\\Omega = \\{(x,y)\\in[-1,1]\\times[-1,1] : \\neg(x0 \\wedge y0)\\}$. The governing equation is the Laplace equation $\\Delta u = 0$ in $\\Omega$, with Dirichlet boundary condition $u = g$ prescribed on $\\partial\\Omega$. The boundary data is chosen by restricting an analytic, harmonic function that exhibits a singular gradient at the re-entrant corner of $\\Omega$. Using polar coordinates $(r,\\theta)$ centered at $(0,0)$ with $\\theta$ measured in radians, define the shifted angle $\\theta' = \\theta - \\frac{\\pi}{2}$, adjusted to the interval $[0,2\\pi)$ by adding $2\\pi$ when necessary. Let the exact harmonic function be\n$$\nu_{\\text{exact}}(r,\\theta) = r^{\\frac{2}{3}} \\sin\\!\\left(\\frac{2}{3}\\,\\theta'\\right),\n$$\nwhich satisfies $\\Delta u_{\\text{exact}}=0$ away from $r=0$ and is compatible with zero boundary values on the two rays forming the re-entrant corner with interior angle $3\\pi/2$. The Dirichlet boundary condition is set as $g = u_{\\text{exact}}$ on the entire boundary $\\partial\\Omega$.\n\nStarting from the Taylor expansion of a sufficiently smooth function on a uniform Cartesian grid, derive the standard $5$-point discrete approximation to the Laplace operator $\\Delta u$ and use it to assemble and solve the discrete Dirichlet problem on the grid restricted to $\\Omega$. The uniform grid spacing is $h = \\frac{1}{m}$, with grid nodes at coordinates $x_i = -1 + i h$ and $y_j = -1 + j h$ for integer indices $i,j \\in \\{0,1,\\dots,2m\\}$. A grid node $(x_i,y_j)$ is in $\\Omega$ if it is not in the removed first quadrant, that is $i \\le m$ or $j \\le m$ (or both). A grid node is a Dirichlet boundary node if it lies on the outer box boundary (any of $i=0$, $i=2m$, $j=0$, $j=2m$) or on one of the interior boundaries aligned with the coordinate axes forming the re-entrant corner (that is $(i=m$ and $j\\ge m)$ or $(j=m$ and $i\\ge m)$). All other nodes in $\\Omega$ are interior unknowns. Angles must be treated in radians.\n\nFor error analysis, let $u_h$ denote the numerical solution interpolated at grid nodes, and measure the error on the set of interior unknowns using the discrete uniform norm\n$$\nE_{\\infty}(h) = \\max_{(x_i,y_j)\\in\\Omega_{\\text{int}}} \\left|u_h(x_i,y_j) - u_{\\text{exact}}(x_i,y_j)\\right|,\n$$\nwhere $\\Omega_{\\text{int}}$ denotes the set of interior unknown grid nodes. Define the observed order of accuracy between two grid spacings $h_1$ and $h_2$ by\n$$\np(h_1\\rightarrow h_2) = \\frac{\\log\\!\\left(E_{\\infty}(h_2)/E_{\\infty}(h_1)\\right)}{\\log\\!\\left(h_2/h_1\\right)}.\n$$\nDue to the singularity at the re-entrant corner, the global convergence rate is expected to be reduced below the nominal second order associated with smooth solutions.\n\nImplement a complete, runnable program that:\n- Constructs the discrete system using the derived $5$-point stencil and imposes Dirichlet boundary conditions from $u_{\\text{exact}}$.\n- Solves the resulting linear system for the interior unknowns.\n- Computes $E_{\\infty}(h)$ for each test grid.\n\nTest suite:\n- Use three grid resolutions with $m\\in\\{4,8,16\\}$, corresponding to $h\\in\\{\\frac{1}{4},\\frac{1}{8},\\frac{1}{16}\\}$.\n- For coverage: $m=4$ is a coarse resolution (edge case near the singular corner), $m=8$ is an intermediate case (happy path), and $m=16$ is a finer resolution.\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- The list must contain the following entries in order:\n$$\n\\left[E_{\\infty}\\!\\left(\\tfrac{1}{4}\\right),\\;E_{\\infty}\\!\\left(\\tfrac{1}{8}\\right),\\;E_{\\infty}\\!\\left(\\tfrac{1}{16}\\right),\\;p\\!\\left(\\tfrac{1}{4}\\rightarrow\\tfrac{1}{8}\\right),\\;p\\!\\left(\\tfrac{1}{8}\\rightarrow\\tfrac{1}{16}\\right),\\;\\text{flag}_{4\\rightarrow 8},\\;\\text{flag}_{8\\rightarrow 16}\\right],\n$$\nwhere each $E_{\\infty}(h)$ and $p(\\cdot)$ is a floating-point number, and each $\\text{flag}$ is a boolean equal to $\\text{True}$ if the observed order is strictly less than $2$ and $\\text{False}$ otherwise. Angles used in computing $u_{\\text{exact}}$ must be handled in radians.", "solution": "The user wants to solve the Laplace equation on an L-shaped domain with a singular solution, and analyze the convergence of the 5-point finite difference method.\n\n### Step 1: Derivation of the 5-Point Laplacian Stencil\n\nThe foundation of the finite difference method is the approximation of derivatives using Taylor series expansions of a function at nearby points. Let $u(x,y)$ be a sufficiently smooth function defined on a domain. We consider a uniform Cartesian grid with spacing $h$ in both the $x$ and $y$ directions.\n\nThe Taylor expansion of $u(x,y)$ around a point $(x,y)$ for its neighbors along the x-axis are:\n$$\nu(x+h, y) = u(x,y) + h \\frac{\\partial u}{\\partial x}(x,y) + \\frac{h^2}{2!} \\frac{\\partial^2 u}{\\partial x^2}(x,y) + \\frac{h^3}{3!} \\frac{\\partial^3 u}{\\partial x^3}(x,y) + O(h^4)\n$$\n$$\nu(x-h, y) = u(x,y) - h \\frac{\\partial u}{\\partial x}(x,y) + \\frac{h^2}{2!} \\frac{\\partial^2 u}{\\partial x^2}(x,y) - \\frac{h^3}{3!} \\frac{\\partial^3 u}{\\partial x^3}(x,y) + O(h^4)\n$$\n\nAdding these two equations eliminates the odd-order derivative terms:\n$$\nu(x+h, y) + u(x-h, y) = 2u(x,y) + h^2 \\frac{\\partial^2 u}{\\partial x^2}(x,y) + O(h^4)\n$$\n\nSolving for the second partial derivative with respect to $x$ gives the second-order accurate central difference approximation:\n$$\n\\frac{\\partial^2 u}{\\partial x^2}(x,y) = \\frac{u(x+h, y) - 2u(x,y) + u(x-h, y)}{h^2} + O(h^2)\n$$\n\nBy an identical argument for the $y$ direction, we obtain:\n$$\n\\frac{\\partial^2 u}{\\partial y^2}(x,y) = \\frac{u(x, y+h) - 2u(x,y) + u(x, y-h)}{h^2} + O(h^2)\n$$\n\nThe Laplace operator $\\Delta u$ is defined as $\\Delta u = \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}$. Substituting the central difference approximations, we get:\n$$\n\\Delta u(x,y) \\approx \\frac{u(x+h, y) - 2u(x,y) + u(x-h, y)}{h^2} + \\frac{u(x, y+h) - 2u(x,y) + u(x, y-h)}{h^2}\n$$\n$$\n\\Delta u(x,y) \\approx \\frac{u(x+h, y) + u(x-h, y) + u(x, y+h) + u(x, y-h) - 4u(x,y)}{h^2}\n$$\nThe truncation error of this approximation is $O(h^2)$, assuming sufficient smoothness of $u$.\n\nThe problem is to solve the Laplace equation, $\\Delta u = 0$. Using the discrete approximation, for each interior grid node $(x_i, y_j)$, we have the equation:\n$$\n\\frac{u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1} - 4u_{i,j}}{h^2} = 0\n$$\nwhere $u_{i,j}$ denotes the numerical approximation $u_h(x_i, y_j)$. This simplifies to:\n$$\n4u_{i,j} - u_{i+1,j} - u_{i-1,j} - u_{i,j+1} - u_{i,j-1} = 0\n$$\nThis linear equation relates the value at a node to the values at its four cardinal neighbors, forming the well-known 5-point stencil for the Laplacian.\n\n### Step 2: Assembling and Solving the Discrete System\n\nThe problem is defined on an L-shaped domain $\\Omega$, discretized by a uniform grid. The grid nodes are classified into three types:\n1.  **Interior nodes**: Nodes inside $\\Omega$ where the value of $u$ is unknown.\n2.  **Boundary nodes**: Nodes on the boundary $\\partial\\Omega$ where the value of $u$ is prescribed by the Dirichlet condition $u=g$.\n3.  **Exterior nodes**: Nodes outside the domain $\\Omega$ (in the removed first quadrant), which are not part of the computation.\n\nFor each interior node, we write one instance of the 5-point stencil equation. This results in a system of linear equations, which can be written in matrix form as $A\\mathbf{U} = \\mathbf{b}$, where:\n-   $\\mathbf{U}$ is a vector of the unknown values of $u$ at all interior nodes.\n-   $A$ is the coefficient matrix derived from the stencil.\n-   $\\mathbf{b}$ is the right-hand side vector, which incorporates the known values from the boundary nodes.\n\nLet there be $N_{\\text{int}}$ interior nodes. We map each interior node's 2D grid index $(i,j)$ to a unique 1D index $k \\in \\{0, 1, ..., N_{\\text{int}}-1\\}$. The matrix $A$ will be an $N_{\\text{int}} \\times N_{\\text{int}}$ sparse matrix. For the equation corresponding to the $k$-th interior node (at grid location $(i,j)$):\n-   The diagonal element of the matrix is $A_{k,k} = 4$.\n-   For each neighbor $(i', j')$ of $(i,j)$:\n    -   If the neighbor is also an interior node with 1D index $k'$, the corresponding off-diagonal element is $A_{k, k'} = -1$.\n    -   If the neighbor is a boundary node, its value $u_{i',j'}$ is known from the boundary condition $g(x_{i'}, y_{j'})$. The term $-u_{i',j'}$ is moved to the right side of the equation. So, the value $u_{i',j'}$ is added to the $k$-th element of the vector $\\mathbf{b}$.\n\nAfter assembling the matrix $A$ and vector $\\mathbf{b}$ for all interior nodes, the linear system $A\\mathbf{U} = \\mathbf{b}$ is solved for the vector of unknowns $\\mathbf{U}$. The matrix $A$ is diagonally dominant and non-singular, guaranteeing a unique solution.\n\n### Step 3: Error Analysis\n\nThe analytical solution $u_{\\text{exact}}$ is given in polar coordinates $(r,\\theta)$:\n$$\nu_{\\text{exact}}(r,\\theta) = r^{\\frac{2}{3}} \\sin\\!\\left(\\frac{2}{3}\\,\\theta'\\right)\n$$\nwhere $\\theta' = (\\theta - \\pi/2) \\pmod{2\\pi}$. This function has a singularity at the origin $(r=0)$, where its gradient is unbounded. Standard finite difference theory predicts second-order convergence ($O(h^2)$) for smooth solutions. However, the presence of the singularity at the re-entrant corner reduces the global order of accuracy. The expected order is $\\alpha = 2/3$, corresponding to the exponent of $r$ in the leading singular term of the solution.\n\nThe implementation will carry out the following:\n1.  For each grid resolution $m \\in \\{4, 8, 16\\}$, construct the grid and classify nodes.\n2.  Map interior nodes to indices and determine the size of the linear system, $N_{\\text{int}}$.\n3.  Assemble the $N_{\\text{int}} \\times N_{\\text{int}}$ matrix $A$ and the $N_{\\text{int}}$-dimensional vector $\\mathbf{b}$. Boundary values are calculated using the provided $u_{\\text{exact}}$ function.\n4.  Solve the linear system $A\\mathbf{U} = \\mathbf{b}$ using a direct solver.\n5.  Compute the maximum absolute error $E_{\\infty}(h) = \\max |u_h - u_{\\text{exact}}|$ over all interior nodes.\n6.  Use the computed errors for different $h$ values to calculate the observed order of accuracy, $p$.\n7.  Check if the observed order $p$ is strictly less than $2$.\n\nThis procedure is implemented for the specified test suite to produce the required output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef u_exact_func(x: float, y: float) -> float:\n    \"\"\"\n    Computes the value of the exact analytical solution at a point (x, y).\n\n    Args:\n        x: The x-coordinate.\n        y: The y-coordinate.\n\n    Returns:\n        The value of the exact solution u(x, y).\n    \"\"\"\n    r = np.sqrt(x**2 + y**2)\n    if r == 0.0:\n        return 0.0\n    \n    # atan2(y, x) gives angle in (-pi, pi]. We need it in [0, 2pi).\n    theta = np.arctan2(y, x)\n    \n    # Shifted angle theta' = theta - pi/2, adjusted to [0, 2pi)\n    theta_prime = theta - np.pi / 2.0\n    if theta_prime  0:\n        theta_prime += 2 * np.pi\n\n    alpha = 2.0 / 3.0\n    return r**alpha * np.sin(alpha * theta_prime)\n\ndef compute_error_for_m(m: int) -> float:\n    \"\"\"\n    Solves the Laplace equation on an L-shaped domain for a given grid parameter m\n    and returns the maximum error on the interior nodes.\n\n    Args:\n        m: The grid resolution parameter.\n\n    Returns:\n        The discrete uniform norm of the error, E_infinity(h).\n    \"\"\"\n    h = 1.0 / m\n    dim = 2 * m + 1\n    \n    # Create coordinate vectors for the grid\n    coords = -1.0 + np.arange(dim) * h\n    \n    # Classify nodes and map interior nodes to a 1D index\n    interior_nodes_map = {}  # (i, j) -> k\n    interior_nodes_list = []  # k -> (i, j)\n    k = 0\n    \n    # Use boolean arrays for efficient node type checking\n    is_boundary_node = np.zeros((dim, dim), dtype=bool)\n    is_domain_node = np.zeros((dim, dim), dtype=bool)\n\n    for j in range(dim):\n        for i in range(dim):\n            # Node is in the computational domain if not in the removed quadrant.\n            # Removed quadrant is x > 0 and y > 0, which corresponds to i > m and j > m.\n            if not(i > m and j > m):\n                is_domain_node[i, j] = True\n    \n    for j in range(dim):\n        for i in range(dim):\n            if not is_domain_node[i, j]:\n                continue\n            \n            # A node is boundary if on outer box or on the re-entrant corner boundary\n            if (i == 0 or i == 2*m or j == 0 or j == 2*m or\n                (i == m and j >= m) or (j == m and i >= m)):\n                is_boundary_node[i, j] = True\n            else:\n                # It's an interior node\n                interior_nodes_map[(i, j)] = k\n                interior_nodes_list.append((i, j))\n                k += 1\n\n    num_interior = len(interior_nodes_list)\n    A = np.zeros((num_interior, num_interior))\n    b = np.zeros(num_interior)\n\n    # Assemble matrix A and vector b\n    for current_k, (i, j) in enumerate(interior_nodes_list):\n        A[current_k, current_k] = 4.0\n        \n        # Consider the four neighbors\n        neighbors = [(i + 1, j), (i - 1, j), (i, j + 1), (i, j - 1)]\n        for ni, nj in neighbors:\n            if not is_boundary_node[ni, nj]:\n                # Neighbor is an interior node\n                neighbor_k = interior_nodes_map[(ni, nj)]\n                A[current_k, neighbor_k] = -1.0\n            else:\n                # Neighbor is a boundary node, so its value is known\n                x_n, y_n = coords[ni], coords[nj]\n                b[current_k] += u_exact_func(x_n, y_n)\n\n    # Solve the linear system for interior node values\n    if num_interior > 0:\n        U_interior_vec = np.linalg.solve(A, b)\n    else:\n        U_interior_vec = np.array([])\n\n    # Calculate the maximum error on the interior nodes\n    max_error = 0.0\n    for k in range(num_interior):\n        i, j = interior_nodes_list[k]\n        x_i, y_j = coords[i], coords[j]\n        \n        numerical_val = U_interior_vec[k]\n        exact_val = u_exact_func(x_i, y_j)\n        \n        error = np.abs(numerical_val - exact_val)\n        if error > max_error:\n            max_error = error\n            \n    return max_error\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the final results.\n    \"\"\"\n    test_cases = [4, 8, 16]\n    \n    errors = [compute_error_for_m(m) for m in test_cases]\n    \n    h_values = [1.0/m for m in test_cases]\n\n    # Calculate observed orders of accuracy\n    p1 = np.log(errors[1] / errors[0]) / np.log(h_values[1] / h_values[0])\n    p2 = np.log(errors[2] / errors[1]) / np.log(h_values[2] / h_values[1])\n    \n    # Check if a convergence order is strictly less than 2\n    flag1 = p1  2.0\n    flag2 = p2  2.0\n    \n    results = [\n        errors[0],\n        errors[1],\n        errors[2],\n        p1,\n        p2,\n        flag1,\n        flag2\n    ]\n    \n    # Format the final output string\n    # str() on a boolean produces 'True' or 'False' as required.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3230895"}, {"introduction": "Discretizing a partial differential equation like the Poisson equation transforms it into a large system of linear algebraic equations, $A \\mathbf{u} = \\mathbf{f}$. For fine grids, solving this system efficiently is paramount. This advanced practice challenges you to implement and compare two iterative solvers: the classical Gauss-Seidel method and a powerful geometric multigrid V-cycle, demonstrating the dramatic difference in performance and why methods like multigrid are essential for large-scale scientific computing. [@problem_id:3230858]", "problem": "You are asked to implement, from first principles, a geometric multigrid solver for the linear system that arises from the standard five-point discretization of the Laplace operator on a uniform two-dimensional grid with Dirichlet boundary conditions, and to compare its convergence behavior against the Gauss–Seidel iterative method.\n\nThe fundamental base to use is the partial differential equation of the Poisson problem on the unit square, the definition of central finite differences, and the concept of iterative relaxation and multilevel correction. Specifically, consider the model problem on the unit square domain $[0,1] \\times [0,1]$ with homogeneous Dirichlet boundary conditions and a given source function $f(x,y)$:\n$$\n- \\Delta u(x,y) = f(x,y), \\quad u(x,y) = 0 \\text{ on the boundary}.\n$$\nDiscretize the interior of the unit square using a uniform grid with $n$ interior points per coordinate direction and grid spacing $h = \\frac{1}{n+1}$, where $n$ is a positive integer. Approximate the Laplacian using second-order central differences to obtain the five-point stencil for the discrete operator acting on interior unknowns $u_{i,j}$:\n$$\n\\left( -\\Delta_h u \\right)_{i,j} = \\frac{4 u_{i,j} - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1}}{h^2},\n$$\nwhich yields the linear system $A \\mathbf{u} = \\mathbf{f}$ for the vector of interior values. All boundary values are prescribed to be zero. Angles used in any trigonometric functions must be in radians.\n\nYou must implement two solvers:\n- The Gauss–Seidel method using red–black ordering. One complete sweep consists of updating the red points (those with $(i+j)$ even), then the black points (those with $(i+j)$ odd), using the discrete equation solved for $u_{i,j}$ in terms of its nearest neighbors and the local right-hand side. Start from the zero initial guess and iterate until the relative residual norm satisfies\n$$\n\\frac{\\lVert \\mathbf{f} - A \\mathbf{u} \\rVert_2}{\\lVert \\mathbf{f} \\rVert_2} \\leq 10^{-6},\n$$\nor a maximum of $2000$ complete sweeps is reached, whichever occurs first. Here, $\\lVert \\cdot \\rVert_2$ denotes the Euclidean norm.\n\n- A geometric multigrid solver using a V-cycle with pre-smoothing and post-smoothing performed by the same red–black Gauss–Seidel smoother. Use $2$ pre-smoothing sweeps and $2$ post-smoothing sweeps per level. Use full-weighting restriction and bilinear interpolation prolongation between levels. On the coarsest grid, defined by $n \\leq 3$, solve the linear system exactly by dense linear algebra. Start from the zero initial guess and perform V-cycles until the same relative residual tolerance $10^{-6}$ is satisfied or a maximum of $50$ V-cycles is reached.\n\nDefine the right-hand side $\\mathbf{f}$ by sampling $f(x,y)$ at the interior grid points $(x_i, y_j)$, where $x_i = i h$ and $y_j = j h$ with $i,j \\in \\{1,2,\\ldots,n\\}$. Construct the following test suite of parameter values:\n- Test case $1$: $n = 15$, $f(x,y) = 1$ for all $(x,y)$.\n- Test case $2$: $n = 31$, $f(x,y) = \\sin(\\pi x)\\sin(\\pi y)$.\n- Test case $3$: $n = 7$, $f(x,y) = 1$ for all $(x,y)$.\n\nFor each test case, measure:\n- The number of complete Gauss–Seidel sweeps required to reach the tolerance (or the cap $2000$ if not achieved).\n- The number of multigrid V-cycles required to reach the tolerance (or the cap $50$ if not achieved).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is the ratio given by\n$$\n\\text{ratio} = \\frac{\\text{number of Gauss–Seidel sweeps}}{\\text{number of multigrid V-cycles}},\n$$\nfor the three test cases in order. Each ratio must be a floating-point number. This format must be exactly\n\"[result1,result2,result3]\".\n\nNo external inputs are required. All computations are unitless numeric values; no physical units are involved. Angles in trigonometric functions must be interpreted in radians. The implementation must be self-contained and runnable as-is in the specified environment.", "solution": "The user has provided a well-defined computational problem in the field of numerical analysis. The problem is scientifically grounded, self-contained, and objective. It asks for the implementation and comparison of two standard iterative methods for solving the linear system arising from the finite difference discretization of the Poisson equation. All parameters, algorithms, and test cases are specified with sufficient precision to permit a unique and verifiable solution. Therefore, the problem is deemed valid.\n\nThe problem to be solved is the Poisson equation on the unit square domain $\\Omega = [0,1] \\times [0,1]$ with homogeneous Dirichlet boundary conditions:\n$$\n-\\Delta u(x,y) = f(x,y) \\quad \\text{for } (x,y) \\in \\Omega\n$$\n$$\nu(x,y) = 0 \\quad \\text{for } (x,y) \\in \\partial\\Omega\n$$\nwhere $\\Delta = \\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}$ is the Laplace operator and $f(x,y)$ is a given source function.\n\n**1. Discretization**\n\nWe introduce a uniform grid with $n$ interior points in each coordinate direction. The grid spacing is $h = \\frac{1}{n+1}$. The grid points are $(x_i, y_j) = (ih, jh)$ for $i,j \\in \\{0, 1, \\dots, n+1\\}$. The values $u_{i,j} = u(x_i, y_j)$ are the unknowns at the interior points, where $i,j \\in \\{1, \\dots, n\\}$. The boundary conditions dictate that $u_{i,j} = 0$ if $i \\in \\{0, n+1\\}$ or $j \\in \\{0, n+1\\}$.\n\nThe Laplacian $\\Delta u$ at an interior point $(x_i, y_j)$ is approximated using a second-order central finite difference scheme:\n$$\n\\Delta u(x_i, y_j) \\approx \\frac{u_{i-1,j} - 2u_{i,j} + u_{i+1,j}}{h^2} + \\frac{u_{i,j-1} - 2u_{i,j} + u_{i,j+1}}{h^2}\n$$\nSubstituting this into the Poisson equation, we obtain the discrete equation for each interior point $(i,j)$:\n$$\n- \\left( \\frac{u_{i-1,j} + u_{i+1,j} + u_{i,j-1} + u_{i,j+1} - 4u_{i,j}}{h^2} \\right) = f_{i,j}\n$$\nwhere $f_{i,j} = f(x_i, y_j)$. This can be rearranged to form the standard five-point stencil equation:\n$$\n\\frac{4u_{i,j} - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1}}{h^2} = f_{i,j}\n$$\nThis set of $n^2$ linear equations for the $n^2$ unknowns $u_{i,j}$ can be written in matrix form as $A \\mathbf{u} = \\mathbf{f}$, where $\\mathbf{u}$ is the vector of unknowns and $\\mathbf{f}$ is the vector of source term values.\n\n**2. Gauss–Seidel Method with Red–Black Ordering**\n\nThe Gauss–Seidel method is an iterative relaxation technique. To derive the update rule, we solve the discrete equation for $u_{i,j}$:\n$$\nu_{i,j} = \\frac{1}{4} \\left( u_{i-1,j} + u_{i+1,j} + u_{i,j-1} + u_{i,j+1} + h^2 f_{i,j} \\right)\n$$\nIn a standard lexicographic sweep, we would update $u_{i,j}$ for $i=1,\\dots,n$ and $j=1,\\dots,n$, using the most recently computed values.\n\nFor improved parallel properties and as a robust smoother for multigrid, a red–black ordering is used. The grid points are colored like a chessboard: a point $(i,j)$ is \"red\" if $(i+j)$ is even, and \"black\" if $(i+j)$ is odd. A full sweep consists of two stages:\n1.  **Red Update:** Update all $u_{i,j}$ at red points using the current values at their four (black) neighbors. Since all red points are only coupled to black points, these updates can be computed independently.\n2.  **Black Update:** Update all $u_{i,j}$ at black points using the newly computed values at their four (red) neighbors.\n\nThe iteration starts with an initial guess, $\\mathbf{u}^{(0)} = \\mathbf{0}$, and continues until the relative residual norm is below a tolerance $\\epsilon = 10^{-6}$:\n$$\n\\frac{\\lVert \\mathbf{r}^{(k)} \\rVert_2}{\\lVert \\mathbf{f} \\rVert_2} = \\frac{\\lVert \\mathbf{f} - A \\mathbf{u}^{(k)} \\rVert_2}{\\lVert \\mathbf{f} \\rVert_2} \\leq \\epsilon\n$$\nor a maximum of $2000$ sweeps is reached. The residual vector components are $r_{i,j} = f_{i,j} - (4u_{i,j} - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1})/h^2$.\n\n**3. Geometric Multigrid Method (V-Cycle)**\n\nThe geometric multigrid method accelerates convergence by using a hierarchy of grids. High-frequency (oscillatory) components of the error are efficiently reduced by a relaxation method (the \"smoother\") on a fine grid, while low-frequency (smooth) components are resolved on a coarser grid where they appear more oscillatory and can be treated efficiently.\n\nA V-cycle is a recursive algorithm that traverses down the grid hierarchy and back up. For a given level with grid spacing $h$ and equation $A_h u_h = f_h$:\n\n1.  **Pre-smoothing:** Apply $\\nu_1 = 2$ sweeps of the red–black Gauss–Seidel smoother to the current approximation of $u_h$. This damps high-frequency errors.\n    $u_h \\leftarrow \\text{Smooth}^{\\nu_1}(u_h, f_h)$\n\n2.  **Compute Residual:** Calculate the residual of the smoothed approximation:\n    $r_h = f_h - A_h u_h$\n\n3.  **Restriction:** Transfer the residual to the next coarser grid (with spacing $2h$). The coarse-grid right-hand side is $f_{2h} = I_{h}^{2h} r_h$. We use full-weighting restriction, where a coarse-grid value is a weighted average of its $9$ corresponding fine-grid neighbors:\n    $$\n    (I_{h}^{2h} r_h)_{i_c,j_c} = \\frac{1}{16} \\sum_{k,l \\in \\{-1,0,1\\}} w_{k,l} (r_h)_{2i_c+k, 2j_c+l}\n    $$\n    with weights given by the stencil $\\begin{pmatrix} 1  2  1 \\\\ 2  4  2 \\\\ 1  2  1 \\end{pmatrix}$.\n\n4.  **Coarse-Grid Problem:** Solve the residual equation on the coarse grid $A_{2h} e_{2h} = f_{2h}$ to find the coarse-grid correction $e_{2h}$.\n    *   If the current grid is the coarsest level (defined by $n \\leq 3$), solve this system exactly using a direct dense solver (e.g., LU decomposition via `numpy.linalg.solve`).\n    *   If not the coarsest level, recursively call the V-cycle algorithm to approximate the solution: $e_{2h} \\leftarrow \\text{V-cycle}(0, f_{2h})$.\n\n5.  **Prolongation (Interpolation):** Transfer the coarse-grid correction $e_{2h}$ back to the fine grid. The fine-grid correction is $e_h = I_{2h}^h e_{2h}$. We use bilinear interpolation. For a coarse-grid point $(i_c, j_c)$, its value is directly injected to the corresponding fine-grid point $(2i_c, 2j_c)$. Values at other fine-grid points are interpolated from the surrounding coarse-grid points.\n\n6.  **Correction:** Add the interpolated correction to the fine-grid solution:\n    $u_h \\leftarrow u_h + e_h$\n\n7.  **Post-smoothing:** Apply $\\nu_2 = 2$ sweeps of the smoother to remove high-frequency errors introduced by the interpolation step.\n    $u_h \\leftarrow \\text{Smooth}^{\\nu_2}(u_h, f_h)$\n\nThe process starts with an initial guess $\\mathbf{u}^{(0)} = \\mathbf{0}$ on the finest grid, and V-cycles are performed until the same relative residual tolerance $\\epsilon=10^{-6}$ is met, or a maximum of $50$ cycles is reached.\n\n**4. Implementation Strategy**\n\n-   **Grids:** Grid functions like $u$ and $f$ are represented as $(n+2) \\times (n+2)$ NumPy arrays, which naturally incorporates the zero boundary conditions by leaving the border elements as $0$.\n-   **Right-hand Side (f):** For a given $n$ and $f(x,y)$, a meshgrid of coordinates is created to evaluate $f(x_i, y_j) = f(ih, jh)$ for $i,j \\in \\{1,\\dots,n\\}$.\n-   **Residual:** The residual $r_{i,j} = f_{i,j} - (4u_{i,j} - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1})/h^2$ is computed efficiently using array slicing.\n-   **Coarsest Grid Solver:** For $n=3$ (a $3 \\times 3$ interior grid), the $9 \\times 9$ matrix $A$ is explicitly constructed and the system $A \\mathbf{e} = \\mathbf{f}$ is solved using `numpy.linalg.solve`. The solution vector is then reshaped back into a $3 \\times 3$ grid.\n-   **Recursive V-Cycle:** The main `v_cycle` function is implemented recursively. The base case for the recursion is when the grid size $n \\leq 3$, at which point it calls the direct solver.\n-   **Main Loop:** Two driver functions, one for Gauss–Seidel and one for Multigrid, implement the iterative loop, call the respective core routines (sweep or V-cycle), check the convergence criterion, and count the iterations.\n\nThe final output is the ratio of Gauss-Seidel sweeps to Multigrid V-cycles required for convergence for each of the three test cases. This ratio highlights the superior efficiency of the multigrid method, which is expected to exhibit convergence rates that are nearly independent of the grid size $n$, in contrast to Gauss-Seidel, whose convergence rate degrades significantly as $n$ increases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef apply_laplacian(u, h):\n    \"\"\"Applies the 5-point discrete Laplacian operator to a grid u.\"\"\"\n    lap = (\n        4 * u[1:-1, 1:-1]\n        - u[:-2, 1:-1]\n        - u[2:, 1:-1]\n        - u[1:-1, :-2]\n        - u[1:-1, 2:]\n    ) / h**2\n    return lap\n\ndef calculate_relative_residual_norm(u, f, h):\n    \"\"\"Calculates the L2 norm of the relative residual.\"\"\"\n    f_norm = np.linalg.norm(f[1:-1, 1:-1])\n    if f_norm == 0:\n        return 0.0\n    \n    residual_interior = f[1:-1, 1:-1] - apply_laplacian(u, h)\n    residual_norm = np.linalg.norm(residual_interior)\n    \n    return residual_norm / f_norm\n\ndef gauss_seidel_sweep(u, f, h):\n    \"\"\"Performs one red-black Gauss-Seidel sweep.\"\"\"\n    n = u.shape[0] - 2\n    h2 = h**2\n    \n    # Red points update (i+j is even)\n    for i in range(1, n + 1):\n        for j in range(1, n + 1):\n            if (i + j) % 2 == 0:\n                u[i, j] = 0.25 * (\n                    u[i - 1, j] + u[i + 1, j] +\n                    u[i, j - 1] + u[i, j + 1] +\n                    h2 * f[i, j]\n                )\n\n    # Black points update (i+j is odd)\n    for i in range(1, n + 1):\n        for j in range(1, n + 1):\n            if (i + j) % 2 != 0:\n                u[i, j] = 0.25 * (\n                    u[i - 1, j] + u[i + 1, j] +\n                    u[i, j - 1] + u[i, j + 1] +\n                    h2 * f[i, j]\n                )\n\ndef solve_gauss_seidel(n, f_func):\n    \"\"\"Solves the Poisson problem using Gauss-Seidel.\"\"\"\n    h = 1.0 / (n + 1)\n    x = np.linspace(0, 1, n + 2)\n    y = np.linspace(0, 1, n + 2)\n    xv, yv = np.meshgrid(x, y, indexing='ij')\n\n    f = np.zeros((n + 2, n + 2))\n    f[1:-1, 1:-1] = f_func(xv[1:-1, 1:-1], yv[1:-1, 1:-1])\n    u = np.zeros((n + 2, n + 2))\n\n    max_sweeps = 2000\n    tolerance = 1e-6\n\n    for sweep in range(1, max_sweeps + 1):\n        gauss_seidel_sweep(u, f, h)\n        if calculate_relative_residual_norm(u, f, h) = tolerance:\n            return sweep\n    return max_sweeps\n\ndef restrict(r_fine):\n    \"\"\"Full-weighting restriction operator.\"\"\"\n    n_fine = r_fine.shape[0] - 2\n    n_coarse = (n_fine - 1) // 2\n    r_coarse = np.zeros((n_coarse + 2, n_coarse + 2))\n    \n    r_coarse[1:-1, 1:-1] = (\n        0.25 * r_fine[2:-2:2, 2:-2:2] +          # Center (4/16)\n        0.125 * (r_fine[1:-3:2, 2:-2:2] +        # Neighbors (2/16)\n                 r_fine[3::2, 2:-2:2] +\n                 r_fine[2:-2:2, 1:-3:2] +\n                 r_fine[2:-2:2, 3::2]) +\n        0.0625 * (r_fine[1:-3:2, 1:-3:2] +       # Diagonals (1/16)\n                  r_fine[1:-3:2, 3::2] +\n                  r_fine[3::2, 1:-3:2] +\n                  r_fine[3::2, 3::2])\n    )\n    return r_coarse\n\ndef prolong(e_coarse):\n    \"\"\"Bilinear interpolation prolongation operator.\"\"\"\n    n_coarse = e_coarse.shape[0] - 2\n    n_fine = 2 * n_coarse + 1\n    e_fine = np.zeros((n_fine + 2, n_fine + 2))\n\n    # Inject coarse grid points\n    e_fine[2:-2:2, 2:-2:2] = e_coarse[1:-1, 1:-1]\n    \n    # Interpolate along columns\n    e_fine[1:-1:2, 2:-2:2] = 0.5 * (e_coarse[:-1, 1:-1] + e_coarse[1:, 1:-1])\n    \n    # Interpolate along rows\n    e_fine[2:-2:2, 1:-1:2] = 0.5 * (e_coarse[1:-1, :-1] + e_coarse[1:-1, 1:])\n    \n    # Interpolate at center points\n    e_fine[1:-1:2, 1:-1:2] = 0.25 * (e_coarse[:-1, :-1] + e_coarse[1:, :-1] +\n                                     e_coarse[:-1, 1:] + e_coarse[1:, 1:])\n    return e_fine\n\ndef solve_coarsest_grid(f_coarse, h_coarse):\n    \"\"\"Direct solver for the coarsest grid.\"\"\"\n    n = f_coarse.shape[0] - 2\n    size = n * n\n    if size == 0:\n        return np.zeros_like(f_coarse)\n\n    A = np.zeros((size, size))\n    I = np.eye(n)\n    T = np.diag(4 * np.ones(n)) - np.diag(np.ones(n - 1), 1) - np.diag(np.ones(n - 1), -1)\n\n    for i in range(n):\n        A[i*n:(i+1)*n, i*n:(i+1)*n] = T\n        if i > 0:\n            A[i*n:(i+1)*n, (i-1)*n:i*n] = -I\n        if i  n - 1:\n            A[i*n:(i+1)*n, (i+1)*n:(i+2)*n] = -I\n    \n    A /= h_coarse**2\n    \n    f_vec = f_coarse[1:-1, 1:-1].flatten()\n    u_vec = np.linalg.solve(A, f_vec)\n    \n    u_coarse = np.zeros_like(f_coarse)\n    u_coarse[1:-1, 1:-1] = u_vec.reshape((n, n))\n    return u_coarse\n\nmg_levels = {}\ndef v_cycle(u, f, h):\n    \"\"\"Performs one multigrid V-cycle.\"\"\"\n    n = u.shape[0] - 2\n    if n not in mg_levels:\n        mg_levels[n] = {'u': u, 'f': f, 'h': h}\n\n    # 1. Pre-smoothing\n    for _ in range(2):\n        gauss_seidel_sweep(u, f, h)\n\n    # 2. Compute residual\n    res_interior = f[1:-1, 1:-1] - apply_laplacian(u, h)\n    r = np.zeros_like(u)\n    r[1:-1, 1:-1] = res_interior\n\n    # 3. Restrict residual\n    r_coarse = restrict(r)\n    n_coarse = r_coarse.shape[0] - 2\n    h_coarse = 2 * h\n    \n    e_coarse = np.zeros_like(r_coarse)\n\n    # 4. Solve on coarse grid\n    if n_coarse = 3:\n        e_coarse = solve_coarsest_grid(r_coarse, h_coarse)\n    else:\n        v_cycle(e_coarse, r_coarse, h_coarse)\n\n    # 5. Prolong and 6. Correct\n    u += prolong(e_coarse)\n\n    # 7. Post-smoothing\n    for _ in range(2):\n        gauss_seidel_sweep(u, f, h)\n    return u\n\ndef solve_multigrid(n, f_func):\n    \"\"\"Solves the Poisson problem using multigrid V-cycles.\"\"\"\n    h = 1.0 / (n + 1)\n    x = np.linspace(0, 1, n + 2)\n    y = np.linspace(0, 1, n + 2)\n    xv, yv = np.meshgrid(x, y, indexing='ij')\n\n    f = np.zeros((n + 2, n + 2))\n    f[1:-1, 1:-1] = f_func(xv[1:-1, 1:-1], yv[1:-1, 1:-1])\n    u = np.zeros((n + 2, n + 2))\n    \n    max_cycles = 50\n    tolerance = 1e-6\n\n    for cycle in range(1, max_cycles + 1):\n        mg_levels.clear()\n        v_cycle(u, f, h)\n        if calculate_relative_residual_norm(u, f, h) = tolerance:\n            return cycle\n    return max_cycles\n\n\ndef solve():\n    \"\"\"Main function to run test cases and print results.\"\"\"\n    test_cases = [\n        (15, lambda x, y: 1.0),\n        (31, lambda x, y: np.sin(np.pi * x) * np.sin(np.pi * y)),\n        (7, lambda x, y: 1.0),\n    ]\n\n    results = []\n    for n, f_func in test_cases:\n        gs_sweeps = solve_gauss_seidel(n, f_func)\n        mg_cycles = solve_multigrid(n, f_func)\n        \n        ratio = float(gs_sweeps) / float(mg_cycles)\n        results.append(ratio)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3230858"}]}