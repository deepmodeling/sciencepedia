{"hands_on_practices": [{"introduction": "The heart of the discrete least squares method is an optimization problem rooted in calculus. To find the \"best-fit\" curve, we define a function for the total squared error and minimize it by finding where its partial derivatives with respect to the model's coefficients are zero. This foundational exercise [@problem_id:3223314] will guide you through this process from first principles, demonstrating how to derive the normal equations for a polynomial model with a specific constraint.", "problem": "Consider the discrete data set consisting of $7$ points $(x_i,y_i)$ with\n$(x_1,y_1)=(-3,18)$, $(x_2,y_2)=(-2,9)$, $(x_3,y_3)=(-1,4)$, $(x_4,y_4)=(0,3)$, $(x_5,y_5)=(1,6)$, $(x_6,y_6)=(2,13)$, $(x_7,y_7)=(3,24)$. These values arise from the underlying relation $y=2x^2+x+3$ sampled at the listed $x$-values. Let $p(x)$ be a quadratic polynomial constrained to pass through the origin, i.e., $p(0)=0$, and write it as $p(x)=a x^2 + b x$.\n\nUsing only the fundamental definition of a discrete least squares approximation—namely, that $p(x)$ minimizes the sum of squared residuals $\\sum_{i=1}^{7} \\big(p(x_i)-y_i\\big)^2$ over the given data—determine the coefficients $a$ and $b$ and provide the explicit analytic expression for the best-fit quadratic $p(x)$ that satisfies $p(0)=0$. Express your final answer in exact form; no rounding is required.", "solution": "The problem is to find the coefficients $a$ and $b$ of the quadratic polynomial $p(x) = ax^2 + bx$ that provides the best discrete least squares approximation to a given set of $7$ data points. The polynomial is constrained to pass through the origin, which is satisfied by its form $p(0) = a(0)^2 + b(0) = 0$.\n\nThe fundamental principle of the method of least squares is to minimize the sum of the squared residuals, $E$, which is the sum of the squared differences between the model's predictions $p(x_i)$ and the observed data values $y_i$. The error function $E(a,b)$ is given by:\n$$E(a,b) = \\sum_{i=1}^{7} \\left(p(x_i) - y_i\\right)^2 = \\sum_{i=1}^{7} \\left(ax_i^2 + bx_i - y_i\\right)^2$$\nTo find the values of $a$ and $b$ that minimize this function, we must find the critical point where the partial derivatives of $E$ with respect to $a$ and $b$ are both equal to zero.\n$$ \\frac{\\partial E}{\\partial a} = 0 \\quad \\text{and} \\quad \\frac{\\partial E}{\\partial b} = 0 $$\nCalculating the partial derivative with respect to $a$:\n$$ \\frac{\\partial E}{\\partial a} = \\sum_{i=1}^{7} 2(ax_i^2 + bx_i - y_i) \\cdot \\frac{\\partial}{\\partial a}(ax_i^2 + bx_i - y_i) = \\sum_{i=1}^{7} 2(ax_i^2 + bx_i - y_i)(x_i^2) $$\nSetting this to zero and dividing by $2$:\n$$ \\sum_{i=1}^{7} (ax_i^4 + bx_i^3 - y_i x_i^2) = 0 $$\n$$ a\\left(\\sum_{i=1}^{7} x_i^4\\right) + b\\left(\\sum_{i=1}^{7} x_i^3\\right) = \\sum_{i=1}^{7} y_i x_i^2 $$\nThis is the first of the two normal equations.\n\nCalculating the partial derivative with respect to $b$:\n$$ \\frac{\\partial E}{\\partial b} = \\sum_{i=1}^{7} 2(ax_i^2 + bx_i - y_i) \\cdot \\frac{\\partial}{\\partial b}(ax_i^2 + bx_i - y_i) = \\sum_{i=1}^{7} 2(ax_i^2 + bx_i - y_i)(x_i) $$\nSetting this to zero and dividing by $2$:\n$$ \\sum_{i=1}^{7} (ax_i^3 + bx_i^2 - y_i x_i) = 0 $$\n$$ a\\left(\\sum_{i=1}^{7} x_i^3\\right) + b\\left(\\sum_{i=1}^{7} x_i^2\\right) = \\sum_{i=1}^{7} y_i x_i $$\nThis is the second normal equation.\n\nThe next step is to compute the required sums from the given data set:\n$(x_1, y_1)=(-3, 18)$, $(x_2, y_2)=(-2, 9)$, $(x_3, y_3)=(-1, 4)$, $(x_4, y_4)=(0, 3)$, $(x_5, y_5)=(1, 6)$, $(x_6, y_6)=(2, 13)$, $(x_7, y_7)=(3, 24)$.\nThe set of $x$-values is $\\{ -3, -2, -1, 0, 1, 2, 3 \\}$. This set is symmetric about $x=0$, which simplifies the sums of odd powers of $x_i$ to zero.\n\nWe calculate the sums:\n$$ \\sum_{i=1}^{7} x_i^2 = (-3)^2 + (-2)^2 + (-1)^2 + 0^2 + 1^2 + 2^2 + 3^2 = 9 + 4 + 1 + 0 + 1 + 4 + 9 = 28 $$\n$$ \\sum_{i=1}^{7} x_i^3 = (-3)^3 + (-2)^3 + (-1)^3 + 0^3 + 1^3 + 2^3 + 3^3 = -27 - 8 - 1 + 0 + 1 + 8 + 27 = 0 $$\n$$ \\sum_{i=1}^{7} x_i^4 = (-3)^4 + (-2)^4 + (-1)^4 + 0^4 + 1^4 + 2^4 + 3^4 = 81 + 16 + 1 + 0 + 1 + 16 + 81 = 196 $$\nNext, we compute the sums involving $y_i$:\n$$ \\sum_{i=1}^{7} y_i x_i = (18)(-3) + (9)(-2) + (4)(-1) + (3)(0) + (6)(1) + (13)(2) + (24)(3) \\\\ = -54 - 18 - 4 + 0 + 6 + 26 + 72 = -76 + 104 = 28 $$\n$$ \\sum_{i=1}^{7} y_i x_i^2 = (18)(-3)^2 + (9)(-2)^2 + (4)(-1)^2 + (3)(0)^2 + (6)(1)^2 + (13)(2)^2 + (24)(3)^2 \\\\ = 18(9) + 9(4) + 4(1) + 3(0) + 6(1) + 13(4) + 24(9) \\\\ = 162 + 36 + 4 + 0 + 6 + 52 + 216 = 476 $$\nNow, we substitute these numerical values into the normal equations:\n1. $a(196) + b(0) = 476$\n2. $a(0) + b(28) = 28$\n\nThis forms a simple, uncoupled system of linear equations for $a$ and $b$.\nFrom the first equation:\n$$ 196a = 476 \\implies a = \\frac{476}{196} $$\nTo simplify the fraction, we can divide the numerator and denominator by common factors. Both are divisible by $4$:\n$$ a = \\frac{476 \\div 4}{196 \\div 4} = \\frac{119}{49} $$\nNow we recognize that $119 = 7 \\times 17$ and $49 = 7 \\times 7$:\n$$ a = \\frac{7 \\times 17}{7 \\times 7} = \\frac{17}{7} $$\nFrom the second equation:\n$$ 28b = 28 \\implies b = 1 $$\nThe coefficients are $a = \\frac{17}{7}$ and $b = 1$. The resulting best-fit quadratic polynomial is:\n$$ p(x) = \\frac{17}{7}x^2 + x $$\nThe problem states that the data arise from the relation $y = 2x^2 + x + 3$. The least squares fit retrieves the coefficient $b=1$ exactly. This is because for a symmetric set of $x_i$ values and data generated by $y_i=f(x_i)$, the cross-correlation terms between even and odd parts of the function vanish. The coefficient $a$ is $\\frac{17}{7} \\approx 2.428$, which differs from the original coefficient $2$ because the model $p(x)$ is forced to be $0$ at $x=0$, while the data point is $(0,3)$. The least squares method adjusts the curvature parameter $a$ to minimize the total squared error over all points, accommodating the large residual at the origin.", "answer": "$$\\boxed{p(x) = \\frac{17}{7}x^2 + x}$$", "id": "3223314"}, {"introduction": "While \"linear least squares\" sounds restrictive, its power extends to many non-linear relationships commonly found in scientific data, such as power laws of the form $y = ax^k$. By applying a mathematical transformation like the natural logarithm, we can linearize the model, allowing us to use the standard least squares machinery. This hands-on coding practice [@problem_id:3223297] teaches this versatile and essential data analysis technique, showing you how to estimate the parameters of a non-linear model by first converting it into a linear one.", "problem": "You are given a collection of discrete data points that are assumed to follow a power-law relationship of the form $y = a x^k$ with $x > 0$ and $y > 0$. The task is to estimate the parameters $a$ and $k$ using the method of discrete least squares approximation by linearizing the model via the natural logarithm to obtain $\\log(y) = \\log(a) + k \\log(x)$. The fundamental base for this derivation is the principle that the discrete least squares approximation finds parameters that minimize the sum of squared residuals, defined for a linear model $m(u) = \\beta_0 + \\beta_1 u$ by the objective function $\\sum_{i=1}^n (v_i - m(u_i))^2$, where $(u_i,v_i)$ are transformed data. You must:\n- Formulate the discrete least squares problem in the transformed space using the natural logarithm (base $e$).\n- Derive, from first principles of minimizing the sum of squared residuals, the conditions characterizing the minimizer for the linearized model, and then implement an algorithm to compute the estimates $\\hat{a}$ and $\\hat{k}$ for given datasets.\n- Use the following test suite of data, each specified as a list of $x$ values and a corresponding list of $y$ values. All $x$ and $y$ are strictly positive real numbers.\n\nTest Suite:\n1. Happy path (exact power-law with integer exponent):\n   - $x$: $[1,2,3,4,5]$\n   - $y$: $[3,12,27,48,75]$\n2. Noisy measurements around a sub-linear power-law:\n   - $x$: $[0.2,0.5,0.8,1.1,1.4,1.7,2.0,2.3]$\n   - $y$: $[0.4145670020285,0.6236681800069,0.8130343166190,0.9439279633550,1.0489213395436,1.1969255615947,1.2600642840744,1.3990405194275]$\n3. Negative exponent with small $x$ (wide dynamic range, exact values):\n   - $x$: $[0.01,0.02,0.05,0.1,0.2,0.5]$\n   - $y$: $[199.0535852767,114.2585902281,54.9185573277,31.5478672240,18.1194915919,8.70550563296]$\n4. Repeated $x$ values with large dynamic range and mild measurement noise:\n   - $x$: $[1,1,10,10,100,1000]$\n   - $y$: $[1.25,1.2575,10.0283949630875,9.87945741908125,80.4460614212,620.21920059]$\n\nYour program must:\n- For each test case, transform the data via the natural logarithm, set up the linear least squares problem, solve for the parameters of the linear model in the transformed space, and then map the solution back to estimates of $a$ and $k$ in the original space.\n- Produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test case result is itself a two-element list in the form $[\\hat{a},\\hat{k}]$. Each $\\hat{a}$ and $\\hat{k}$ must be printed as a floating-point number rounded to six decimal places.\n\nThe final output format must be exactly:\n- One line, no extra spaces other than those needed to separate numbers within commas, presenting $[[\\hat{a}_1,\\hat{k}_1],[\\hat{a}_2,\\hat{k}_2],[\\hat{a}_3,\\hat{k}_3],[\\hat{a}_4,\\hat{k}_4]]$.\n\nUse the natural logarithm throughout (i.e., base $e$). No physical units, angles, or percentages are involved; all quantities are dimensionless real numbers. The program must be self-contained, require no input, and use only numerical operations consistent with the discrete least squares approximation via linearization.", "solution": "The problem is to determine the parameters $a$ and $k$ of a power-law model $y = a x^k$ that best fit a given set of data points $(x_i, y_i)$, for $i=1, \\dots, n$. The problem specifies that the data satisfy $x_i > 0$ and $y_i > 0$. The method to be used is a discrete least squares approximation applied to a linearized version of the model.\n\n**Step 1: Problem Formulation and Linearization**\nThe given power-law relationship is:\n$$y = a x^k$$\nTo linearize this model, we apply the natural logarithm (base $e$) to both sides. This is permissible because $x$ and $y$ are strictly positive.\n$$\\ln(y) = \\ln(a x^k)$$\nUsing the properties of logarithms, $\\ln(AB) = \\ln(A) + \\ln(B)$ and $\\ln(A^p) = p \\ln(A)$, we get:\n$$\\ln(y) = \\ln(a) + \\ln(x^k)$$\n$$\\ln(y) = \\ln(a) + k \\ln(x)$$\nThis equation is in the form of a linear relationship. To make this explicit, we introduce a change of variables. Let:\n- $v = \\ln(y)$\n- $u = \\ln(x)$\n- $\\beta_0 = \\ln(a)$\n- $\\beta_1 = k$\n\nSubstituting these into the transformed equation yields the linear model:\n$$v = \\beta_0 + \\beta_1 u$$\nGiven a set of $n$ data points $(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)$, we can transform them into a new set of points $(u_1, v_1), (u_2, v_2), \\dots, (u_n, v_n)$ where $u_i = \\ln(x_i)$ and $v_i = \\ln(y_i)$. Our objective is to find the parameters $\\beta_0$ and $\\beta_1$ that define the line of best fit for these transformed points.\n\n**Step 2: The Discrete Least Squares Principle**\nThe principle of discrete least squares states that the best-fit line is the one that minimizes the sum of the squared residuals. The residual for the $i$-th point is the difference between the observed value $v_i$ and the value predicted by the model, $\\beta_0 + \\beta_1 u_i$. The sum of squared residuals, $S$, is a function of the parameters $\\beta_0$ and $\\beta_1$:\n$$S(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} (v_i - (\\beta_0 + \\beta_1 u_i))^2$$\n\n**Step 3: Derivation of the Normal Equations**\nTo find the values of $\\beta_0$ and $\\beta_1$ that minimize $S$, we apply a standard method from calculus: we find the partial derivatives of $S$ with respect to $\\beta_0$ and $\\beta_1$ and set them equal to zero. This identifies the critical points of the function $S$.\n\nFirst, the partial derivative with respect to $\\beta_0$:\n$$ \\frac{\\partial S}{\\partial \\beta_0} = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\beta_0} (v_i - \\beta_0 - \\beta_1 u_i)^2 = \\sum_{i=1}^{n} 2(v_i - \\beta_0 - \\beta_1 u_i)(-1) $$\nSetting this to zero:\n$$ -2 \\sum_{i=1}^{n} (v_i - \\beta_0 - \\beta_1 u_i) = 0 $$\n$$ \\sum_{i=1}^{n} v_i - \\sum_{i=1}^{n} \\beta_0 - \\sum_{i=1}^{n} \\beta_1 u_i = 0 $$\n$$ \\sum_{i=1}^{n} v_i - n\\beta_0 - \\beta_1 \\sum_{i=1}^{n} u_i = 0 $$\nRearranging gives the first normal equation:\n$$ n\\beta_0 + \\left(\\sum_{i=1}^{n} u_i\\right) \\beta_1 = \\sum_{i=1}^{n} v_i $$\n\nSecond, the partial derivative with respect to $\\beta_1$:\n$$ \\frac{\\partial S}{\\partial \\beta_1} = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\beta_1} (v_i - \\beta_0 - \\beta_1 u_i)^2 = \\sum_{i=1}^{n} 2(v_i - \\beta_0 - \\beta_1 u_i)(-u_i) $$\nSetting this to zero:\n$$ -2 \\sum_{i=1}^{n} u_i(v_i - \\beta_0 - \\beta_1 u_i) = 0 $$\n$$ \\sum_{i=1}^{n} (u_i v_i - \\beta_0 u_i - \\beta_1 u_i^2) = 0 $$\n$$ \\sum_{i=1}^{n} u_i v_i - \\beta_0 \\sum_{i=1}^{n} u_i - \\beta_1 \\sum_{i=1}^{n} u_i^2 = 0 $$\nRearranging gives the second normal equation:\n$$ \\left(\\sum_{i=1}^{n} u_i\\right) \\beta_0 + \\left(\\sum_{i=1}^{n} u_i^2\\right) \\beta_1 = \\sum_{i=1}^{n} u_i v_i $$\n\n**Step 4: Solving the Normal Equations**\nWe now have a system of two linear equations for the two unknowns, $\\beta_0$ and $\\beta_1$:\n$$\n\\begin{cases}\nn\\beta_0 + (\\sum u_i) \\beta_1 = \\sum v_i \\\\\n(\\sum u_i) \\beta_0 + (\\sum u_i^2) \\beta_1 = \\sum u_i v_i\n\\end{cases}\n$$\nSolving this system (e.g., using Cramer's rule or substitution) yields the solutions for $\\beta_0$ and $\\beta_1$. The denominator of the solution is the determinant of the coefficient matrix, $D = n(\\sum u_i^2) - (\\sum u_i)^2$. The solution for $\\beta_1$ is:\n$$ \\beta_1 = \\frac{n(\\sum u_i v_i) - (\\sum u_i)(\\sum v_i)}{n(\\sum u_i^2) - (\\sum u_i)^2} $$\nOnce $\\beta_1$ is known, we can find $\\beta_0$ from the first normal equation:\n$$ \\beta_0 = \\frac{1}{n} \\left( \\sum v_i - \\beta_1 \\sum u_i \\right) = \\bar{v} - \\beta_1 \\bar{u} $$\nwhere $\\bar{u} = \\frac{1}{n}\\sum u_i$ and $\\bar{v} = \\frac{1}{n}\\sum v_i$ are the arithmetic means.\n\n**Step 5: Mapping Back to Original Parameters**\nThe estimates for the original parameters, denoted $\\hat{a}$ and $\\hat{k}$, are found by reversing the variable substitutions:\n$$ \\hat{k} = \\beta_1 $$\nfrom $\\beta_0 = \\ln(a)$, we have:\n$$ \\hat{a} = e^{\\beta_0} $$\nThis procedure provides the estimates for the parameters of the power-law model that minimize the sum of squared errors in the log-transformed space.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for power-law parameters a and k for given datasets using\n    discrete least squares on the linearized model.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # 1. Happy path (exact power-law with integer exponent)\n        (\n            [1, 2, 3, 4, 5],\n            [3, 12, 27, 48, 75]\n        ),\n        # 2. Noisy measurements around a sub-linear power-law\n        (\n            [0.2, 0.5, 0.8, 1.1, 1.4, 1.7, 2.0, 2.3],\n            [0.4145670020285, 0.6236681800069, 0.8130343166190, 0.9439279633550,\n             1.0489213395436, 1.1969255615947, 1.2600642840744, 1.3990405194275]\n        ),\n        # 3. Negative exponent with small x (wide dynamic range, exact values)\n        (\n            [0.01, 0.02, 0.05, 0.1, 0.2, 0.5],\n            [199.0535852767, 114.2585902281, 54.9185573277, 31.5478672240,\n             18.1194915919, 8.70550563296]\n        ),\n        # 4. Repeated x values with large dynamic range and mild measurement noise\n        (\n            [1, 1, 10, 10, 100, 1000],\n            [1.25, 1.2575, 10.0283949630875, 9.87945741908125,\n             80.4460614212, 620.21920059]\n        )\n    ]\n\n    results = []\n    for x_vals, y_vals in test_cases:\n        # Convert data to numpy arrays for vectorized operations\n        x = np.array(x_vals, dtype=np.float64)\n        y = np.array(y_vals, dtype=np.float64)\n\n        # Linearize the model: log(y) = log(a) + k*log(x)\n        # Transformed variables: v = log(y), u = log(x)\n        # Linear model: v = beta0 + beta1*u, where beta0 = log(a), beta1 = k\n        u = np.log(x)\n        v = np.log(y)\n\n        # Number of data points\n        n = len(u)\n\n        # Calculate the necessary sums for the normal equations\n        sum_u = np.sum(u)\n        sum_v = np.sum(v)\n        sum_uv = np.sum(u * v)\n        sum_u_sq = np.sum(u**2)\n\n        # Solve the normal equations for beta0 and beta1\n        # beta1 = (n * sum_uv - sum_u * sum_v) / (n * sum_u_sq - sum_u**2)\n        # beta0 = (sum_v - beta1 * sum_u) / n\n        \n        denominator = n * sum_u_sq - sum_u**2\n        \n        # This condition is met if not all x_i are the same\n        if denominator == 0:\n            # Should not happen with the given test data\n            raise ValueError(\"All x values are the same; cannot solve.\")\n\n        beta1 = (n * sum_uv - sum_u * sum_v) / denominator\n        beta0 = (sum_v - beta1 * sum_u) / n\n\n        # Map beta0 and beta1 back to the original power-law parameters a and k\n        # k = beta1\n        # a = exp(beta0)\n        k_hat = beta1\n        a_hat = np.exp(beta0)\n        \n        # Round the results to six decimal places as required\n        a_rounded = round(a_hat, 6)\n        k_rounded = round(k_hat, 6)\n        \n        results.append([a_rounded, k_rounded])\n    \n    # Format the final output string exactly as specified, without extra spaces\n    # Example format: [[a1,k1],[a2,k2],[a3,k3],[a4,k4]]\n    formatted_results = [f\"[{res[0]},{res[1]}]\" for res in results]\n    final_string = f\"[{','.join(formatted_results)}]\"\n\n    print(final_string)\n\nsolve()\n```", "id": "3223297"}, {"introduction": "A \"best-fit\" line is only as good as the data it's based on, and the least squares method is notoriously sensitive to outliers. Because the algorithm minimizes the *sum of squared residuals*, a single data point with a large error can have a disproportionate influence, pulling the entire fit towards it. Through this computational experiment [@problem_id:3223340], you will investigate this sensitivity firsthand, building critical intuition about how outliers and high-leverage points can affect a model and why diagnosing their presence is a crucial step in any real-world data analysis.", "problem": "You are tasked with analyzing the sensitivity of a discrete least squares polynomial fit to the presence of a single, massive outlier. Work within the following purely mathematical and computational framework.\n\nGiven a set of data points $\\{(x_i, y_i)\\}_{i=1}^n$, the discrete least squares polynomial of degree $m$ is the polynomial $p_m(x) = \\sum_{k=0}^m c_k x^k$ whose coefficients $\\boldsymbol{c} = [c_0,\\dots,c_m]^\\top$ minimize the sum of squared residuals $\\sum_{i=1}^n (y_i - p_m(x_i))^2$. In matrix form, if $\\boldsymbol{A} \\in \\mathbb{R}^{n \\times (m+1)}$ has entries $A_{i,k} = x_i^k$ and $\\boldsymbol{y} \\in \\mathbb{R}^n$ has entries $y_i$, the coefficients solve the minimization problem $\\min_{\\boldsymbol{c} \\in \\mathbb{R}^{m+1}} \\|\\boldsymbol{A}\\boldsymbol{c} - \\boldsymbol{y}\\|_2^2$. Assume the matrix $\\boldsymbol{A}$ has full column rank so that the minimizer is unique.\n\nStart from the following base dataset corresponding to a noise-free quadratic:\n- Degree $m = 2$.\n- Baseline abscissae $\\boldsymbol{x}_{\\text{base}} = [-2, -1, 0, 1, 2]$.\n- Baseline ordinates $\\boldsymbol{y}_{\\text{base}}$ given by $y_i = x_i^2$ for each $x_i \\in \\boldsymbol{x}_{\\text{base}}$.\n\nLet the baseline least squares coefficients be $\\boldsymbol{c}^{(\\text{base})} \\in \\mathbb{R}^{3}$. Consider augmenting the dataset by a single additional point $(x_{\\text{out}}, y_{\\text{out}})$ to form a new fit with coefficients $\\boldsymbol{c}^{(\\text{aug})} \\in \\mathbb{R}^{3}$. For each specified outlier, quantify the effect of the outlier on the fit by computing the Euclidean norm of the coefficient change $\\|\\boldsymbol{c}^{(\\text{aug})} - \\boldsymbol{c}^{(\\text{base})}\\|_2$ and the new coefficients $\\boldsymbol{c}^{(\\text{aug})}$.\n\nBase your reasoning on fundamental definitions of discrete least squares approximation, normal equations, and linear algebraic properties of full column rank matrices. Do not assume any formula for the effect of rank-one updates; derive any needed relations from first principles.\n\nYour program must:\n- Construct the baseline least squares fit for the given baseline dataset with degree $m = 2$.\n- For each test case below, form the augmented dataset by appending the single given outlier $(x_{\\text{out}}, y_{\\text{out}})$ and recompute the least squares coefficients.\n- For each test case, output a list of four floating point numbers: $[\\|\\boldsymbol{c}^{(\\text{aug})} - \\boldsymbol{c}^{(\\text{base})}\\|_2, c_0^{(\\text{aug})}, c_1^{(\\text{aug})}, c_2^{(\\text{aug})}]$, each rounded to six decimal places.\n\nTest suite to evaluate different leverage and residual regimes:\n- Case $1$ (low-leverage, massive residual at center): $(x_{\\text{out}}, y_{\\text{out}}) = (0, 1000)$.\n- Case $2$ (moderate high-leverage, massive residual outside range): $(x_{\\text{out}}, y_{\\text{out}}) = (3, 1000)$.\n- Case $3$ (edge leverage, massive negative residual at boundary): $(x_{\\text{out}}, y_{\\text{out}}) = (2, -1000)$.\n- Case $4$ (extreme leverage, massive residual far outside range): $(x_{\\text{out}}, y_{\\text{out}}) = (-10, 1000)$.\n- Case $5$ (control, no residual outlier; consistent with the model): $(x_{\\text{out}}, y_{\\text{out}}) = (3, 9)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces. Each element corresponds to one test case in the order listed above and is itself a list of four rounded floats. For example, the format must be exactly like\n- $[[0.123456,0.000001,0.000002,0.999997],[\\dots],\\dots]$\nwith no additional text and no spaces.", "solution": "The problem requires finding a polynomial $p_m(x) = \\sum_{k=0}^m c_k x^k$ that best fits a set of data points $\\{(x_i, y_i)\\}_{i=1}^n$ in the least squares sense. This is achieved by minimizing the sum of squared residuals, $S(\\boldsymbol{c})$, defined as:\n$$S(\\boldsymbol{c}) = \\sum_{i=1}^n (y_i - p_m(x_i))^2$$\nThe vector of coefficients is $\\boldsymbol{c} = [c_0, c_1, \\dots, c_m]^\\top \\in \\mathbb{R}^{m+1}$. This minimization problem can be expressed in matrix form. Let $\\boldsymbol{y} \\in \\mathbb{R}^n$ be the vector of ordinates $y_i$, and let $\\boldsymbol{A} \\in \\mathbb{R}^{n \\times (m+1)}$ be the Vandermonde-like matrix with entries $A_{i,k} = x_i^k$ for $i=1, \\dots, n$ and $k=0, \\dots, m$. The product $\\boldsymbol{A}\\boldsymbol{c}$ is a vector whose $i$-th entry is $p_m(x_i)$. The minimization problem is then equivalent to:\n$$\\min_{\\boldsymbol{c} \\in \\mathbb{R}^{m+1}} \\|\\boldsymbol{A}\\boldsymbol{c} - \\boldsymbol{y}\\|_2^2$$\nTo find the minimum, we set the gradient of the squared norm with respect to $\\boldsymbol{c}$ to zero. This yields the **Normal Equations**:\n$$(\\boldsymbol{A}^\\top \\boldsymbol{A})\\boldsymbol{c} = \\boldsymbol{A}^\\top \\boldsymbol{y}$$\nThe problem states that the matrix $\\boldsymbol{A}$ has full column rank. This implies that the Gram matrix $\\boldsymbol{A}^\\top \\boldsymbol{A}$ is symmetric, positive definite, and therefore invertible. This guarantees the existence of a unique solution for the coefficient vector $\\boldsymbol{c}$, which can be found by solving this system of linear equations.\n\nThe procedure to solve the problem is as follows:\n\nFirst, we compute the baseline coefficients $\\boldsymbol{c}^{(\\text{base})}$. The problem specifies a polynomial degree $m=2$. The baseline dataset has $n=5$ points, with abscissae $\\boldsymbol{x}_{\\text{base}} = [-2, -1, 0, 1, 2]$ and ordinates $\\boldsymbol{y}_{\\text{base}}$ given by $y_i = x_i^2$, which are $[4, 1, 0, 1, 4]$. We construct the $5 \\times 3$ matrix $\\boldsymbol{A}_{\\text{base}}$ and the vector $\\boldsymbol{y}_{\\text{base}}$, and solve the linear system $(\\boldsymbol{A}_{\\text{base}}^\\top \\boldsymbol{A}_{\\text{base}})\\boldsymbol{c}^{(\\text{base})} = \\boldsymbol{A}_{\\text{base}}^\\top \\boldsymbol{y}_{\\text{base}}$ for $\\boldsymbol{c}^{(\\text{base})}$. Since the data perfectly fit the polynomial $p(x) = 0 \\cdot x^0 + 0 \\cdot x^1 + 1 \\cdot x^2$, we expect to find $\\boldsymbol{c}^{(\\text{base})} = [0, 0, 1]^\\top$.\n\nSecond, for each test case, we augment the baseline dataset with the specified outlier point $(x_{\\text{out}}, y_{\\text{out}})$. This creates a new dataset with $n=6$ points. We form the new $6 \\times 3$ matrix $\\boldsymbol{A}_{\\text{aug}}$ and the new vector $\\boldsymbol{y}_{\\text{aug}}$. We then solve the corresponding normal equations, $(\\boldsymbol{A}_{\\text{aug}}^\\top \\boldsymbol{A}_{\\text{aug}})\\boldsymbol{c}^{(\\text{aug})} = \\boldsymbol{A}_{\\text{aug}}^\\top \\boldsymbol{y}_{\\text{aug}}$, for the new coefficient vector $\\boldsymbol{c}^{(\\text{aug})}$.\n\nFinally, for each case, we quantify the effect of the outlier by computing the Euclidean norm of the difference between the augmented and baseline coefficient vectors, $\\|\\boldsymbol{c}^{(\\text{aug})} - \\boldsymbol{c}^{(\\text{base})}\\|_2$. The required output for each case is a list containing this norm and the three components of the new coefficient vector $\\boldsymbol{c}^{(\\text{aug})} = [c_0^{(\\text{aug})}, c_1^{(\\text{aug})}, c_2^{(\\text{aug})}]^\\top$. This will be done for all five specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes the sensitivity of a discrete least squares polynomial fit to a single outlier.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0, 1000),      # Case 1: low-leverage, massive residual at center\n        (3, 1000),      # Case 2: moderate high-leverage, massive residual outside range\n        (2, -1000),     # Case 3: edge leverage, massive negative residual at boundary\n        (-10, 1000),    # Case 4: extreme leverage, massive residual far outside range\n        (3, 9)          # Case 5: control, no residual outlier\n    ]\n\n    # Degree of the polynomial\n    m = 2\n    \n    # Baseline dataset\n    x_base = np.array([-2, -1, 0, 1, 2], dtype=float)\n    y_base = x_base**2\n    \n    # Construct the baseline Vandermonde matrix A_base\n    # A_{i,k} = x_i^k, so we use increasing=True\n    A_base = np.vander(x_base, m + 1, increasing=True)\n    \n    # Solve the normal equations for the baseline coefficients c_base\n    # (A.T @ A) c = A.T @ y\n    # np.linalg.solve is numerically preferable to inv()\n    c_base = np.linalg.solve(A_base.T @ A_base, A_base.T @ y_base)\n\n    results = []\n    \n    # Iterate through each test case (outlier)\n    for x_out, y_out in test_cases:\n        # Form the augmented dataset\n        x_aug = np.append(x_base, x_out)\n        y_aug = np.append(y_base, y_out)\n        \n        # Construct the augmented Vandermonde matrix A_aug\n        A_aug = np.vander(x_aug, m + 1, increasing=True)\n        \n        # Solve the normal equations for the augmented coefficients c_aug\n        c_aug = np.linalg.solve(A_aug.T @ A_aug, A_aug.T @ y_aug)\n        \n        # Calculate the Euclidean norm of the change in coefficients\n        norm_diff = np.linalg.norm(c_aug - c_base)\n        \n        # Collect the results for this case\n        case_result = [norm_diff, c_aug[0], c_aug[1], c_aug[2]]\n        results.append(case_result)\n\n    # Format the final output string as specified\n    # Each sublist is formatted as \"[d,c0,c1,c2]\" with 6 decimal places\n    # The outer list is a comma-separated join of the sublist strings\n    inner_strings = []\n    for R in results:\n        # Format each number to 6 decimal places and join with commas\n        formatted_r = [f\"{val:.6f}\" for val in R]\n        inner_strings.append(f\"[{','.join(formatted_r)}]\")\n        \n    final_output = f\"[{','.join(inner_strings)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "3223340"}]}