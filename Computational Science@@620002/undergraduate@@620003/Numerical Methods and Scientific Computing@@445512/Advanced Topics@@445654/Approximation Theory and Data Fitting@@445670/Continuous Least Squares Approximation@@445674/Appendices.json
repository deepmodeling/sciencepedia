{"hands_on_practices": [{"introduction": "This first exercise goes back to the basics by asking for the simplest possible approximation: representing a function with a single constant value. By starting from the fundamental principle of minimizing the integrated squared error, you will derive the optimal constant from scratch. This practice builds intuition and introduces the important concept of a weight function, which allows us to control the approximation's accuracy across different parts of the interval. [@problem_id:3218137]", "problem": "Consider a function $f:[a,b]\\to\\mathbb{R}$ and a strictly positive weight function $w:[a,b]\\to\\mathbb{R}$ that is integrable on the interval $[a,b]$. The task is to find a constant $c\\in\\mathbb{R}$ that best approximates $f$ in the continuous least squares sense with respect to $w$, meaning $c$ minimizes the weighted mean-square error\n$$\n\\int_{a}^{b} w(x)\\,\\big(f(x)-c\\big)^{2}\\,dx.\n$$\nStarting only from the fundamental definition of the weighted mean-square error, derive the condition that characterizes the minimizing constant $c$, and express $c$ in terms of integrals involving $f$ and $w$ over $[a,b]$. Then, apply your derivation to the specific case $a=1$, $b=e$, $f(x)=\\ln(x)$, and $w(x)=x$ to compute the exact minimizing constant. Express your final answer as an exact symbolic expression; do not round.", "solution": "The problem requires finding the constant $c \\in \\mathbb{R}$ that best approximates a function $f:[a,b]\\to\\mathbb{R}$ in the continuous least squares sense with respect to a weight function $w:[a,b]\\to\\mathbb{R}$. This is achieved by minimizing the weighted mean-square error, given by the functional $E(c)$:\n$$\nE(c) = \\int_{a}^{b} w(x)\\,\\big(f(x)-c\\big)^{2}\\,dx.\n$$\nTo find the value of $c$ that minimizes $E(c)$, we employ standard methods from calculus. A necessary condition for a minimum is that the derivative of $E(c)$ with respect to $c$ must be zero. Since the integration is with respect to $x$, we can apply the Leibniz integral rule (differentiation under the integral sign) to compute the derivative of $E(c)$ with respect to $c$:\n$$\n\\frac{dE}{dc} = \\frac{d}{dc} \\int_{a}^{b} w(x)\\,\\big(f(x)-c\\big)^{2}\\,dx = \\int_{a}^{b} \\frac{\\partial}{\\partial c} \\left[ w(x)\\,\\big(f(x)-c\\big)^{2} \\right] \\,dx.\n$$\nUsing the chain rule for the integrand, we find the partial derivative:\n$$\n\\frac{\\partial}{\\partial c} \\left[ w(x)\\,(f(x)-c)^{2} \\right] = w(x) \\cdot 2(f(x)-c) \\cdot \\frac{\\partial}{\\partial c}(f(x)-c) = w(x) \\cdot 2(f(x)-c) \\cdot (-1) = -2w(x)(f(x)-c).\n$$\nSubstituting this back into the expression for the derivative of $E(c)$:\n$$\n\\frac{dE}{dc} = \\int_{a}^{b} -2w(x)(f(x)-c) \\,dx = -2 \\int_{a}^{b} \\big(w(x)f(x) - cw(x)\\big) \\,dx.\n$$\nSetting this derivative to zero provides the condition for the optimal $c$:\n$$\n-2 \\int_{a}^{b} \\big(w(x)f(x) - cw(x)\\big) \\,dx = 0.\n$$\nSince $-2 \\neq 0$, the integral itself must be zero:\n$$\n\\int_{a}^{b} \\big(w(x)f(x) - cw(x)\\big) \\,dx = 0.\n$$\nBy the linearity of the integral operator, we can separate the terms:\n$$\n\\int_{a}^{b} w(x)f(x) \\,dx - \\int_{a}^{b} cw(x) \\,dx = 0.\n$$\nSince $c$ is a constant with respect to the integration variable $x$, we can factor it out of the second integral:\n$$\n\\int_{a}^{b} w(x)f(x) \\,dx - c \\int_{a}^{b} w(x) \\,dx = 0.\n$$\nSolving for $c$ yields:\n$$\nc \\int_{a}^{b} w(x) \\,dx = \\int_{a}^{b} w(x)f(x) \\,dx.\n$$\nThe problem states that $w(x)$ is a strictly positive weight function, so for $a < b$, the integral $\\int_{a}^{b} w(x) \\,dx$ is positive and non-zero. Thus, we can divide by it to obtain the general expression for the minimizing constant $c$:\n$$\nc = \\frac{\\int_{a}^{b} w(x)f(x) \\,dx}{\\int_{a}^{b} w(x) \\,dx}.\n$$\nTo verify that this value of $c$ corresponds to a minimum, we check the second derivative, $\\frac{d^{2}E}{dc^{2}}$. Differentiating $\\frac{dE}{dc} = -2\\int_{a}^{b} w(x)f(x) \\,dx + 2c\\int_{a}^{b} w(x) \\,dx$ with respect to $c$:\n$$\n\\frac{d^{2}E}{dc^{2}} = \\frac{d}{dc} \\left( -2\\int_{a}^{b} w(x)f(x) \\,dx + 2c\\int_{a}^{b} w(x) \\,dx \\right) = 2 \\int_{a}^{b} w(x) \\,dx.\n$$\nAs established, since $w(x) > 0$ on $[a,b]$, the integral $\\int_{a}^{b} w(x) \\,dx$ is positive. Consequently, $\\frac{d^{2}E}{dc^{2}} > 0$, confirming that the critical point is a minimum. Since $E(c)$ is a quadratic in $c$ with a positive leading coefficient, this is a global minimum.\n\nWe now apply this result to the specific case given: $a=1$, $b=e$, $f(x)=\\ln(x)$, and $w(x)=x$. The formula for $c$ is:\n$$\nc = \\frac{\\int_{1}^{e} x \\ln(x) \\,dx}{\\int_{1}^{e} x \\,dx}.\n$$\nWe compute the numerator integral, $\\int_{1}^{e} x \\ln(x) \\,dx$, using integration by parts, $\\int u \\,dv = uv - \\int v \\,du$. Let $u=\\ln(x)$ and $dv=x\\,dx$, which gives $du=\\frac{1}{x}\\,dx$ and $v=\\frac{x^{2}}{2}$.\n$$\n\\int_{1}^{e} x \\ln(x) \\,dx = \\left[ \\frac{x^{2}}{2}\\ln(x) \\right]_{1}^{e} - \\int_{1}^{e} \\frac{x^{2}}{2} \\cdot \\frac{1}{x} \\,dx = \\left( \\frac{e^{2}}{2}\\ln(e) - \\frac{1^{2}}{2}\\ln(1) \\right) - \\int_{1}^{e} \\frac{x}{2} \\,dx.\n$$\nSince $\\ln(e)=1$ and $\\ln(1)=0$, this simplifies to:\n$$\n= \\left( \\frac{e^{2}}{2} - 0 \\right) - \\left[ \\frac{x^{2}}{4} \\right]_{1}^{e} = \\frac{e^{2}}{2} - \\left( \\frac{e^{2}}{4} - \\frac{1^{2}}{4} \\right) = \\frac{e^{2}}{2} - \\frac{e^{2}}{4} + \\frac{1}{4} = \\frac{2e^{2}-e^{2}+1}{4} = \\frac{e^{2}+1}{4}.\n$$\nNext, we compute the denominator integral, $\\int_{1}^{e} x \\,dx$:\n$$\n\\int_{1}^{e} x \\,dx = \\left[ \\frac{x^{2}}{2} \\right]_{1}^{e} = \\frac{e^{2}}{2} - \\frac{1^{2}}{2} = \\frac{e^{2}-1}{2}.\n$$\nFinally, we substitute these results back into the expression for $c$:\n$$\nc = \\frac{\\frac{e^{2}+1}{4}}{\\frac{e^{2}-1}{2}} = \\frac{e^{2}+1}{4} \\cdot \\frac{2}{e^{2}-1} = \\frac{e^{2}+1}{2(e^{2}-1)}.\n$$", "answer": "$$\n\\boxed{\\frac{\\exp(2)+1}{2(\\exp(2)-1)}}\n$$", "id": "3218137"}, {"introduction": "Having mastered constant approximations, we now move to a more flexible polynomial fit. This practice frames the continuous least squares problem as an orthogonal projection in a function space, a key geometric insight in numerical analysis. You will discover how the abstract concept of orthogonality provides a concrete method for finding the best approximation and how exploiting symmetries, such as function parity, can dramatically simplify the required calculations. [@problem_id:3218261]", "problem": "Let $C([-1,1])$ denote the space of continuous real-valued functions on $[-1,1]$, and equip it with the standard inner product $\\langle g,h\\rangle=\\int_{-1}^{1} g(x)\\,h(x)\\,dx$ and induced norm $\\|g\\|=\\sqrt{\\langle g,g\\rangle}$. Consider the function $f(x)=x^{3}$ and the subspace $V=\\operatorname{span}\\{1,x,x^{2}\\}$.\n\nUsing only the definitions of inner product, norm, and orthogonal projection in this inner product space, determine the unique function $p^{\\ast}\\in V$ that minimizes $\\|f-p\\|$ over $p\\in V$ by enforcing that the residual $r=f-p^{\\ast}$ is orthogonal to $V$. Then, compute the minimal squared error $\\|f-p^{\\ast}\\|^{2}$, and explain how the parity of $f$ and the basis functions in $V$ affects orthogonality on the symmetric interval $[-1,1]$.\n\nProvide your final answer as the exact value of the minimal squared error $\\|f-p^{\\ast}\\|^{2}$, expressed as a reduced fraction. Do not round.", "solution": "The problem is to find the function $p^{\\ast}(x)$ in the subspace $V = \\operatorname{span}\\{1, x, x^2\\}$ that best approximates the function $f(x) = x^3$ on the interval $[-1, 1]$ in the least squares sense. The space of functions $C([-1,1])$ is equipped with the inner product $\\langle g, h \\rangle = \\int_{-1}^{1} g(x)h(x)dx$. The best approximation $p^{\\ast}(x)$ is the unique function in $V$ that minimizes the error norm $\\|f - p\\|$, where $\\|g\\| = \\sqrt{\\langle g, g \\rangle}$. This minimum occurs when the residual vector, $r(x) = f(x) - p^{\\ast}(x)$, is orthogonal to the subspace $V$.\n\nAny function $p(x) \\in V$ can be written as a linear combination of the basis functions: $p(x) = c_0 \\cdot 1 + c_1 \\cdot x + c_2 \\cdot x^2$. Let the basis functions be denoted as $\\phi_0(x) = 1$, $\\phi_1(x) = x$, and $\\phi_2(x) = x^2$. The condition that the residual $r(x) = f(x) - p^{\\ast}(x)$ is orthogonal to $V$ means that $r(x)$ must be orthogonal to every basis function of $V$. This gives rise to a system of three linear equations, known as the normal equations:\n$$ \\langle f - p^{\\ast}, \\phi_i \\rangle = 0 \\quad \\text{for } i = 0, 1, 2 $$\n$$ \\langle f, \\phi_i \\rangle = \\langle p^{\\ast}, \\phi_i \\rangle \\quad \\text{for } i = 0, 1, 2 $$\nSubstituting $p^{\\ast}(x) = c_0^{\\ast}\\phi_0(x) + c_1^{\\ast}\\phi_1(x) + c_2^{\\ast}\\phi_2(x)$, we get:\n$$ \\langle f, \\phi_i \\rangle = c_0^{\\ast}\\langle \\phi_0, \\phi_i \\rangle + c_1^{\\ast}\\langle \\phi_1, \\phi_i \\rangle + c_2^{\\ast}\\langle \\phi_2, \\phi_i \\rangle \\quad \\text{for } i = 0, 1, 2 $$\n\nBefore computing the integrals, we can simplify the problem by considering the parity of the functions involved over the symmetric interval $[-1, 1]$.\nA function $g(x)$ is even if $g(-x) = g(x)$ and odd if $g(-x) = -g(x)$. The integral of an odd function over a symmetric interval $[-a, a]$ is always zero. The product of two even functions or two odd functions is even. The product of an even and an odd function is odd.\nOur function to approximate is $f(x) = x^3$, which is odd.\nThe basis for $V$ consists of $\\phi_0(x) = 1$ (even), $\\phi_1(x) = x$ (odd), and $\\phi_2(x) = x^2$ (even).\n\nThis parity structure affects orthogonality directly. The inner product $\\langle g, h \\rangle = \\int_{-1}^{1} g(x)h(x)dx$ will be zero if the integrand $g(x)h(x)$ is an odd function. This occurs when one of the functions is even and the other is odd. This simplifies the system of normal equations significantly.\n\nLet's compute the required inner products:\nRight-hand side of the normal equations, $\\langle f, \\phi_i \\rangle$:\n$f(x) = x^3$ is odd.\n$\\langle f, \\phi_0 \\rangle = \\langle x^3, 1 \\rangle = \\int_{-1}^{1} x^3 dx = 0$ (integrand is odd).\n$\\langle f, \\phi_1 \\rangle = \\langle x^3, x \\rangle = \\int_{-1}^{1} x^4 dx = \\left[\\frac{x^5}{5}\\right]_{-1}^{1} = \\frac{1}{5} - (-\\frac{1}{5}) = \\frac{2}{5}$.\n$\\langle f, \\phi_2 \\rangle = \\langle x^3, x^2 \\rangle = \\int_{-1}^{1} x^5 dx = 0$ (integrand is odd).\n\nLeft-hand side matrix elements (Gram matrix), $\\langle \\phi_j, \\phi_i \\rangle$:\n$\\langle \\phi_0, \\phi_0 \\rangle = \\langle 1, 1 \\rangle = \\int_{-1}^{1} 1 dx = 2$.\n$\\langle \\phi_0, \\phi_1 \\rangle = \\langle 1, x \\rangle = \\int_{-1}^{1} x dx = 0$.\n$\\langle \\phi_0, \\phi_2 \\rangle = \\langle 1, x^2 \\rangle = \\int_{-1}^{1} x^2 dx = \\frac{2}{3}$.\n$\\langle \\phi_1, \\phi_1 \\rangle = \\langle x, x \\rangle = \\int_{-1}^{1} x^2 dx = \\frac{2}{3}$.\n$\\langle \\phi_1, \\phi_2 \\rangle = \\langle x, x^2 \\rangle = \\int_{-1}^{1} x^3 dx = 0$.\n$\\langle \\phi_2, \\phi_2 \\rangle = \\langle x^2, x^2 \\rangle = \\int_{-1}^{1} x^4 dx = \\frac{2}{5}$.\n\nThe system of equations for the coefficients $(c_0^{\\ast}, c_1^{\\ast}, c_2^{\\ast})$ becomes:\n1. $c_0^{\\ast}\\langle \\phi_0, \\phi_0 \\rangle + c_1^{\\ast}\\langle \\phi_1, \\phi_0 \\rangle + c_2^{\\ast}\\langle \\phi_2, \\phi_0 \\rangle = \\langle f, \\phi_0 \\rangle \\implies c_0^{\\ast}(2) + c_1^{\\ast}(0) + c_2^{\\ast}(\\frac{2}{3}) = 0$.\n2. $c_0^{\\ast}\\langle \\phi_0, \\phi_1 \\rangle + c_1^{\\ast}\\langle \\phi_1, \\phi_1 \\rangle + c_2^{\\ast}\\langle \\phi_2, \\phi_1 \\rangle = \\langle f, \\phi_1 \\rangle \\implies c_0^{\\ast}(0) + c_1^{\\ast}(\\frac{2}{3}) + c_2^{\\ast}(0) = \\frac{2}{5}$.\n3. $c_0^{\\ast}\\langle \\phi_0, \\phi_2 \\rangle + c_1^{\\ast}\\langle \\phi_1, \\phi_2 \\rangle + c_2^{\\ast}\\langle \\phi_2, \\phi_2 \\rangle = \\langle f, \\phi_2 \\rangle \\implies c_0^{\\ast}(\\frac{2}{3}) + c_1^{\\ast}(0) + c_2^{\\ast}(\\frac{2}{5}) = 0$.\n\nThe system decouples based on parity. The second equation, involving only the odd basis function $\\phi_1$, can be solved independently:\n$\\frac{2}{3} c_1^{\\ast} = \\frac{2}{5} \\implies c_1^{\\ast} = \\frac{3}{5}$.\n\nThe first and third equations form a system for the coefficients of the even basis functions:\n$$ 2c_0^{\\ast} + \\frac{2}{3}c_2^{\\ast} = 0 $$\n$$ \\frac{2}{3}c_0^{\\ast} + \\frac{2}{5}c_2^{\\ast} = 0 $$\nThis is a homogeneous linear system. The determinant of the coefficient matrix is $(2)(\\frac{2}{5}) - (\\frac{2}{3})(\\frac{2}{3}) = \\frac{4}{5} - \\frac{4}{9} = \\frac{36-20}{45} = \\frac{16}{45} \\neq 0$. Since the matrix is non-singular, the only solution is the trivial one: $c_0^{\\ast} = 0$ and $c_2^{\\ast} = 0$.\n\nThis illustrates a general principle: the best approximation of an odd function $f$ from a basis consisting of even and odd functions will be composed solely of the odd basis functions. The projection of $f$ onto the subspace of even functions is zero.\nThe coefficients are $c_0^{\\ast}=0$, $c_1^{\\ast}=\\frac{3}{5}$, $c_2^{\\ast}=0$. Thus, the best approximating polynomial is $p^{\\ast}(x) = \\frac{3}{5}x$.\n\nNext, we compute the minimal squared error, $\\|f - p^{\\ast}\\|^2$.\n$\\|f - p^{\\ast}\\|^2 = \\langle f - p^{\\ast}, f - p^{\\ast} \\rangle$.\nBy the construction of $p^{\\ast}$, the residual $f-p^{\\ast}$ is orthogonal to any function in $V$. Since $p^{\\ast} \\in V$, we have $\\langle f - p^{\\ast}, p^{\\ast} \\rangle = 0$.\nTherefore, we can expand the squared norm as:\n$\\|f - p^{\\ast}\\|^2 = \\langle f - p^{\\ast}, f \\rangle - \\langle f - p^{\\ast}, p^{\\ast} \\rangle = \\langle f, f \\rangle - \\langle p^{\\ast}, f \\rangle$.\n\nWe compute the two required terms:\n$\\|f\\|^2 = \\langle f, f \\rangle = \\langle x^3, x^3 \\rangle = \\int_{-1}^{1} x^6 dx = \\left[\\frac{x^7}{7}\\right]_{-1}^{1} = \\frac{1}{7} - (-\\frac{1}{7}) = \\frac{2}{7}$.\n$\\langle p^{\\ast}, f \\rangle = \\langle \\frac{3}{5}x, x^3 \\rangle = \\frac{3}{5} \\langle x, x^3 \\rangle = \\frac{3}{5} \\int_{-1}^{1} x^4 dx = \\frac{3}{5} \\left( \\frac{2}{5} \\right) = \\frac{6}{25}$.\n\nThe minimal squared error is:\n$\\|f - p^{\\ast}\\|^2 = \\frac{2}{7} - \\frac{6}{25} = \\frac{2 \\cdot 25}{7 \\cdot 25} - \\frac{6 \\cdot 7}{25 \\cdot 7} = \\frac{50}{175} - \\frac{42}{175} = \\frac{8}{175}$.\nThis fraction is in reduced form since $8=2^3$ and $175=5^2 \\cdot 7$.\nAlternatively, one could directly compute the integral of the squared residual:\n$\\|f - p^{\\ast}\\|^2 = \\int_{-1}^{1} \\left(x^3 - \\frac{3}{5}x\\right)^2 dx = \\int_{-1}^{1} \\left(x^6 - \\frac{6}{5}x^4 + \\frac{9}{25}x^2\\right) dx$.\nSince the integrand is an even function:\n$= 2 \\left[\\frac{x^7}{7} - \\frac{6}{5}\\frac{x^5}{5} + \\frac{9}{25}\\frac{x^3}{3}\\right]_{0}^{1} = 2 \\left(\\frac{1}{7} - \\frac{6}{25} + \\frac{3}{25}\\right) = 2 \\left(\\frac{1}{7} - \\frac{3}{25}\\right) = 2 \\left(\\frac{25-21}{175}\\right) = 2\\left(\\frac{4}{175}\\right) = \\frac{8}{175}$.\nBoth methods yield the same result. The minimal squared error is $\\frac{8}{175}$.", "answer": "$$\n\\boxed{\\frac{8}{175}}\n$$", "id": "3218261"}, {"introduction": "The previous exercises demonstrated *how* to compute least squares approximations. This final practice challenges you to think about *why* certain methods are superior. By comparing two different polynomial bases—Legendre and Chebyshev—you will uncover the profound computational advantages of choosing a basis that is orthogonal with respect to the problem's specific inner product, a decision that directly impacts both efficiency and numerical stability. [@problem_id:3218278]", "problem": "You are approximating a square-integrable function $f(x)$ on the interval $[-1,1]$ by a polynomial of degree at most $N$ using continuous least squares with the weight $w(x)=1$. The approximation seeks $p_N(x)$ in a chosen basis $\\{\\varphi_0(x),\\varphi_1(x),\\dots,\\varphi_N(x)\\}$ that minimizes the objective functional $\\int_{-1}^{1} w(x)\\,|f(x)-p_N(x)|^2\\,dx$. In this setting, the coefficients of $p_N(x)$ solve the normal equations formed by the Gram matrix of the basis with respect to the inner product $\\langle g,h\\rangle=\\int_{-1}^{1} w(x)\\,g(x)\\,h(x)\\,dx$. You are deciding between the Legendre polynomial basis $\\{P_k(x)\\}_{k=0}^N$ and the Chebyshev polynomial basis of the first kind $\\{T_k(x)\\}_{k=0}^N$.\n\nWhich option best characterizes the choice between these bases for this continuous least squares problem on $[-1,1]$ with $w(x)=1$, in terms of both the ease of computing coefficients and the conditioning of the resulting linear system?\n\nA. The Legendre basis $\\{P_k(x)\\}$ is orthogonal on $[-1,1]$ under $w(x)=1$, so the Gram matrix is diagonal (and can be made the identity by normalizing), making coefficients simple to compute by projection integrals and yielding the best conditioning among the two choices.\n\nB. The Chebyshev basis $\\{T_k(x)\\}$ is orthogonal on $[-1,1]$ for any weight function, so its Gram matrix is diagonal for $w(x)=1$, which both simplifies coefficient computation and improves conditioning compared to the Legendre basis.\n\nC. Both Legendre and Chebyshev bases are equally orthogonal under $w(x)=1$ and therefore yield identical Gram matrices, identical ease of coefficient computation, and identical conditioning.\n\nD. The Chebyshev basis $\\{T_k(x)\\}$ should be preferred because, even if it is not orthogonal under $w(x)=1$, its near-minimax approximation property ensures the normal equations matrix has better conditioning and makes coefficients easier to compute than with Legendre polynomials.", "solution": "The user wants a critical evaluation of two polynomial bases, Legendre and Chebyshev, for a continuous least squares approximation problem.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Function to approximate:** A square-integrable function $f(x)$ on the interval $[-1, 1]$.\n- **Approximating function:** A polynomial $p_N(x)$ of degree at most $N$.\n- **Method:** Continuous least squares.\n- **Weight function:** $w(x) = 1$.\n- **Objective functional to minimize:** $J(p_N) = \\int_{-1}^{1} w(x)\\,|f(x)-p_N(x)|^2\\,dx$.\n- **Basis:** $p_N(x)$ is expressed in a basis $\\{\\varphi_0(x), \\varphi_1(x), \\dots, \\varphi_N(x)\\}$.\n- **Inner product:** $\\langle g,h\\rangle = \\int_{-1}^{1} w(x)\\,g(x)\\,h(x)\\,dx = \\int_{-1}^{1} g(x)\\,h(x)\\,dx$.\n- **Normal Equations:** The coefficients of $p_N(x)$ are determined by solving the normal equations, which involve the Gram matrix of the basis with respect to the defined inner product.\n- **Bases for comparison:**\n    1. Legendre polynomial basis $\\{P_k(x)\\}_{k=0}^N$.\n    2. Chebyshev polynomial basis of the first kind $\\{T_k(x)\\}_{k=0}^N$.\n- **Criteria for comparison:** Ease of computing coefficients and the conditioning of the resulting linear system.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding:** The problem is firmly rooted in the mathematical field of numerical analysis, specifically in approximation theory. The concepts of least squares approximation, inner product spaces, orthogonal polynomials, Gram matrices, and condition numbers are standard and rigorously defined.\n- **Well-Posed:** The problem asks for a qualitative comparison of two well-defined methods for a standard problem. It is a well-posed conceptual question.\n- **Objectivity:** The problem is stated using precise, objective mathematical language.\n- **Completeness and Consistency:** The problem provides all necessary information (interval, weight function, inner product, bases to compare) to perform the required analysis. There are no contradictions.\n- **Other Flaws:** The problem is not trivial, metaphorical, or scientifically unsound. It is a standard and important question in the practice of numerical approximation.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. I will proceed with the detailed derivation and analysis.\n\n### Derivation and Analysis\n\nThe continuous least squares approximation seeks to find the polynomial $p_N(x) = \\sum_{k=0}^N c_k \\varphi_k(x)$ that minimizes the squared error functional:\n$$ E = \\int_{-1}^{1} (f(x) - p_N(x))^2 dx = \\left\\langle f - \\sum_{k=0}^N c_k \\varphi_k, f - \\sum_{k=0}^N c_k \\varphi_k \\right\\rangle $$\nThe minimum is found by setting the partial derivatives of $E$ with respect to each coefficient $c_j$ to zero, for $j=0, 1, \\dots, N$. This leads to the normal equations:\n$$ \\sum_{k=0}^N c_k \\langle \\varphi_k, \\varphi_j \\rangle = \\langle f, \\varphi_j \\rangle \\quad \\text{for } j=0, 1, \\dots, N $$\nThis is a system of linear equations, which can be written in matrix form as $\\mathbf{G} \\mathbf{c} = \\mathbf{b}$, where:\n- $\\mathbf{c} = [c_0, c_1, \\dots, c_N]^T$ is the vector of unknown coefficients.\n- $\\mathbf{G}$ is the Gram matrix with entries $G_{jk} = \\langle \\varphi_j, \\varphi_k \\rangle = \\int_{-1}^{1} \\varphi_j(x) \\varphi_k(x) dx$.\n- $\\mathbf{b}$ is a vector with entries $b_j = \\langle f, \\varphi_j \\rangle = \\int_{-1}^{1} f(x) \\varphi_j(x) dx$.\n\nThe ease of computing the coefficients $\\mathbf{c}$ and the numerical stability of the computation depend directly on the structure and conditioning of the Gram matrix $\\mathbf{G}$.\n\nAn ideal basis $\\{\\varphi_k(x)\\}$ would be one that is **orthogonal** with respect to the given inner product. Orthogonality means $\\langle \\varphi_j, \\varphi_k \\rangle = 0$ for $j \\neq k$. If the basis is orthogonal, the Gram matrix $\\mathbf{G}$ becomes a diagonal matrix:\n$$ G_{jk} = \\begin{cases} \\langle \\varphi_j, \\varphi_j \\rangle & \\text{if } j=k \\\\ 0 & \\text{if } j \\neq k \\end{cases} $$\nIn this case, the linear system $\\mathbf{G} \\mathbf{c} = \\mathbf{b}$ decouples into $N+1$ independent equations:\n$$ c_j \\langle \\varphi_j, \\varphi_j \\rangle = \\langle f, \\varphi_j \\rangle $$\nThe solution for each coefficient is then trivially computed by a projection integral:\n$$ c_j = \\frac{\\langle f, \\varphi_j \\rangle}{\\langle \\varphi_j, \\varphi_j \\rangle} = \\frac{\\int_{-1}^{1} f(x) \\varphi_j(x) dx}{\\int_{-1}^{1} (\\varphi_j(x))^2 dx} $$\nFurthermore, a diagonal Gram matrix (with positive diagonal entries) is perfectly conditioned in a practical sense. If the basis is normalized such that $\\langle \\varphi_j, \\varphi_j \\rangle = 1$ (an orthonormal basis), the Gram matrix becomes the identity matrix $\\mathbf{I}$, which has a condition number of $\\kappa(\\mathbf{I})=1$, the best possible value.\n\nNow, we analyze the two proposed bases in the context of the inner product $\\langle g, h \\rangle = \\int_{-1}^{1} g(x) h(x) dx$.\n\n**1. Legendre Polynomial Basis $\\{P_k(x)\\}$**\nBy their standard definition, Legendre polynomials $P_k(x)$ are orthogonal on the interval $[-1, 1]$ with respect to the constant weight function $w(x)=1$. Their orthogonality relation is:\n$$ \\langle P_j, P_k \\rangle = \\int_{-1}^{1} P_j(x) P_k(x) dx = \\frac{2}{2k+1} \\delta_{kj} $$\nwhere $\\delta_{kj}$ is the Kronecker delta.\n- **Gram Matrix:** Since the basis is orthogonal with respect to the specified inner product, the Gram matrix $\\mathbf{G}$ is diagonal.\n- **Ease of Computation:** The coefficients are computed directly via projection integrals: $c_k = \\frac{2k+1}{2} \\int_{-1}^{1} f(x) P_k(x) dx$. No linear system needs to be solved. This is computationally very simple.\n- **Conditioning:** The resulting linear system is perfectly conditioned. If we use the orthonormal Legendre polynomials $\\tilde{P}_k(x) = \\sqrt{\\frac{2k+1}{2}} P_k(x)$, the Gram matrix is the identity matrix $\\mathbf{I}$, and its condition number is $1$.\n\n**2. Chebyshev Polynomial Basis of the First Kind $\\{T_k(x)\\}$**\nChebyshev polynomials $T_k(x)$ are orthogonal on the interval $[-1, 1]$, but with respect to the weight function $w(x) = (1-x^2)^{-1/2}$. Their orthogonality relation is:\n$$ \\int_{-1}^{1} T_j(x) T_k(x) \\frac{1}{\\sqrt{1-x^2}} dx = \\begin{cases} 0 & j \\neq k \\\\ \\pi & j=k=0 \\\\ \\pi/2 & j=k \\neq 0 \\end{cases} $$\nThe problem, however, specifies the weight function $w(x)=1$. We must check for orthogonality with respect to this weight. The inner product is $\\langle T_j, T_k \\rangle = \\int_{-1}^{1} T_j(x) T_k(x) dx$. Let's test a simple case, e.g., $j=0$ and $k=2$. We have $T_0(x)=1$ and $T_2(x)=2x^2-1$.\n$$ \\langle T_0, T_2 \\rangle = \\int_{-1}^{1} (1)(2x^2-1) dx = \\left[ \\frac{2x^3}{3} - x \\right]_{-1}^{1} = \\left(\\frac{2}{3}-1\\right) - \\left(-\\frac{2}{3}+1\\right) = -\\frac{1}{3} - \\frac{1}{3} = -\\frac{2}{3} \\neq 0 $$\nSince the inner product is non-zero for $j \\neq k$, the Chebyshev basis $\\{T_k(x)\\}$ is **not** orthogonal with respect to the weight function $w(x)=1$.\n- **Gram Matrix:** The Gram matrix is **not** diagonal. It will be a full (or at least non-diagonal) matrix, requiring a general-purpose linear solver.\n- **Ease of Computation:** The coefficients $c_k$ must be found by solving the full linear system $\\mathbf{G} \\mathbf{c} = \\mathbf{b}$. This is significantly more complex and computationally expensive than the direct projection for the Legendre basis.\n- **Conditioning:** Because the basis is not orthogonal, the Gram matrix will be more ill-conditioned than the diagonal matrix obtained with the Legendre basis. While better conditioned than the monomial basis $\\{x^k\\}$, it is suboptimal for this specific problem.\n\n**Conclusion:** For the continuous least squares problem on $[-1, 1]$ with $w(x)=1$, the Legendre basis is the superior choice. It is orthogonal with respect to the problem's inner product, leading to a diagonal Gram matrix, trivial coefficient computation, and optimal conditioning. The Chebyshev basis lacks this orthogonality, making it computationally more difficult and numerically less stable for this specific task.\n\n### Option-by-Option Analysis\n\n**A. The Legendre basis $\\{P_k(x)\\}$ is orthogonal on $[-1,1]$ under $w(x)=1$, so the Gram matrix is diagonal (and can be made the identity by normalizing), making coefficients simple to compute by projection integrals and yielding the best conditioning among the two choices.**\nThis statement is entirely consistent with our derivation. The Legendre basis is orthogonal for $w(x)=1$, which makes the Gram matrix diagonal. This simplifies coefficient calculation to direct projection and provides the best possible conditioning.\n**Verdict: Correct**\n\n**B. The Chebyshev basis $\\{T_k(x)\\}$ is orthogonal on $[-1,1]$ for any weight function, so its Gram matrix is diagonal for $w(x)=1$, which both simplifies coefficient computation and improves conditioning compared to the Legendre basis.**\nThis statement is incorrect on multiple grounds. First, no basis is orthogonal for \"any\" weight function. Second, the Chebyshev basis is not orthogonal for $w(x)=1$. Consequently, its Gram matrix is not diagonal, coefficient computation is not simplified, and its conditioning is worse than that of the Legendre basis for this problem.\n**Verdict: Incorrect**\n\n**C. Both Legendre and Chebyshev bases are equally orthogonal under $w(x)=1$ and therefore yield identical Gram matrices, identical ease of coefficient computation, and identical conditioning.**\nThis statement is false. Only the Legendre basis is orthogonal under $w(x)=1$. The Chebyshev basis is not. Their properties in this context are distinctly different.\n**Verdict: Incorrect**\n\n**D. The Chebyshev basis $\\{T_k(x)\\}$ should be preferred because, even if it is not orthogonal under $w(x)=1$, its near-minimax approximation property ensures the normal equations matrix has better conditioning and makes coefficients easier to compute than with Legendre polynomials.**\nThis statement is incorrect and misleading. The near-minimax property of Chebyshev polynomials relates to the $L^\\infty$ norm, which is not the norm being minimized in this $L^2$ (least squares) problem. For the $L^2$ norm with $w(x)=1$, the key property is orthogonality with respect to $w(x)=1$. Since the Chebyshev basis lacks this property, it makes the coefficients *harder* to compute (requiring a linear system solve) and leads to *worse* conditioning compared to the orthogonal Legendre basis.\n**Verdict: Incorrect**", "answer": "$$\\boxed{A}$$", "id": "3218278"}]}