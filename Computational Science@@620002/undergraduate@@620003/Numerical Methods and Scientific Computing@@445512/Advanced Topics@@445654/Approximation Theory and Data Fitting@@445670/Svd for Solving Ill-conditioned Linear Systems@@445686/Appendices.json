{"hands_on_practices": [{"introduction": "To master ill-conditioned systems, we must first understand their fundamental structure. This exercise demystifies the concept of the condition number by tasking you with building an ill-conditioned matrix from its core components as defined by the Singular Value Decomposition (SVD). By manually specifying singular values to create a massive gap in the spectrum, you will gain a concrete understanding of how the ratio $\\sigma_{\\max}/\\sigma_{\\min}$ dictates matrix stability and directly compute the resulting high condition number from first principles [@problem_id:3280607].", "problem": "Let $A \\in \\mathbb{R}^{5 \\times 5}$ be constructed using the Singular Value Decomposition (SVD), that is, $A = U \\Sigma V^{\\mathsf{T}}$, where $U \\in \\mathbb{R}^{5 \\times 5}$ and $V \\in \\mathbb{R}^{5 \\times 5}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{5 \\times 5}$ is diagonal with nonnegative diagonal entries $\\sigma_{1} \\ge \\sigma_{2} \\ge \\cdots \\ge \\sigma_{5} \\ge 0$. \n\nYour task is to design $A$ so that its singular values consist of two well-separated clusters: three singular values equal to $1$ and two singular values equal to $10^{-16}$. Then analyze the conditioning of the corresponding linear system.\n\nTasks:\n1. Using only the definition of the SVD and orthogonal matrices, explicitly construct one such matrix $A$ by specifying valid choices of $U$, $\\Sigma$, and $V$ that achieve the requested singular value clustering. Explain why your construction indeed has the desired singular values.\n2. Starting from the definition of the vector $2$-norm and the induced matrix $2$-norm, and using only properties of orthogonal transformations, derive an expression for the $2$-norm condition number $\\kappa_{2}(A)$ of a nonsingular matrix in terms of the singular values of $A$.\n3. Evaluate $\\kappa_{2}(A)$ exactly for your constructed matrix. State your final result as a single number. No rounding is required.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. All necessary information is provided, and the tasks are logically structured and solvable using established principles of linear algebra and numerical analysis. The problem is therefore deemed valid.\n\nThe solution is presented in three parts, corresponding to the three tasks in the problem statement.\n\n### Part 1: Construction of the Matrix $A$\n\nThe problem requires the construction of a matrix $A \\in \\mathbb{R}^{5 \\times 5}$ of the form $A = U \\Sigma V^{\\mathsf{T}}$, where $U$ and $V$ are orthogonal matrices and $\\Sigma$ is a diagonal matrix of singular values. The specified singular values are three instances of $1$ and two instances of $10^{-16}$. By convention, the singular values are ordered non-increasingly along the diagonal of $\\Sigma$.\n\nLet the singular values be $\\sigma_1, \\sigma_2, \\sigma_3, \\sigma_4, \\sigma_5$. According to the problem statement, we must have:\n$$\n\\sigma_1 = \\sigma_2 = \\sigma_3 = 1\n$$\n$$\n\\sigma_4 = \\sigma_5 = 10^{-16}\n$$\nThese values are all non-negative, satisfying the definition of singular values. The matrix $\\Sigma \\in \\mathbb{R}^{5 \\times 5}$ is therefore defined as:\n$$\n\\Sigma = \\text{diag}(1, 1, 1, 10^{-16}, 10^{-16}) =\n\\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 10^{-16} & 0 \\\\\n0 & 0 & 0 & 0 & 10^{-16}\n\\end{pmatrix}\n$$\nThe problem requires specifying valid choices for the orthogonal matrices $U$ and $V$. The simplest choice for an orthogonal matrix is the identity matrix, $I_5 \\in \\mathbb{R}^{5 \\times 5}$. A matrix $Q$ is orthogonal if $Q^{\\mathsf{T}}Q = I$. For the identity matrix, $I_5^{\\mathsf{T}}I_5 = I_5 I_5 = I_5$, so it is indeed orthogonal.\n\nWe choose $U = I_5$ and $V = I_5$.\n\nWith these choices, the matrix $A$ is constructed as:\n$$\nA = U \\Sigma V^{\\mathsf{T}} = I_5 \\Sigma I_5^{\\mathsf{T}} = \\Sigma\n$$\nThus, one such matrix $A$ that satisfies the given conditions is:\n$$\nA =\n\\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 10^{-16} & 0 \\\\\n0 & 0 & 0 & 0 & 10^{-16}\n\\end{pmatrix}\n$$\nBy definition, the singular values of a matrix are the diagonal entries of the $\\Sigma$ matrix in its Singular Value Decomposition. Our construction $A = I_5 \\Sigma I_5^{\\mathsf{T}}$ is a valid SVD, and the diagonal entries of $\\Sigma$ are precisely the values required by the problem. Therefore, this construction is valid and achieves the desired singular value clustering.\n\n### Part 2: Derivation of the Condition Number Expression\n\nThe $2$-norm condition number of a nonsingular matrix $A \\in \\mathbb{R}^{n \\times n}$ is defined as $\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2$. The induced matrix $2$-norm is defined as $\\|A\\|_2 = \\max_{\\|x\\|_2=1} \\|Ax\\|_2$, where $\\|x\\|_2 = \\sqrt{x^{\\mathsf{T}}x}$ is the Euclidean vector norm.\n\nLet the Singular Value Decomposition of $A$ be $A = U \\Sigma V^{\\mathsf{T}}$, where $U, V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices and $\\Sigma = \\text{diag}(\\sigma_1, \\sigma_2, \\dots, \\sigma_n)$ with $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_n \\ge 0$.\n\nFirst, we derive an expression for $\\|A\\|_2$. Consider the squared norm $\\|Ax\\|_2^2$ for a vector $x$ such that $\\|x\\|_2 = 1$:\n$$\n\\|Ax\\|_2^2 = \\|U \\Sigma V^{\\mathsf{T}} x\\|_2^2\n$$\nSince $U$ is an orthogonal matrix, it preserves the $2$-norm, meaning $\\|Uy\\|_2 = \\|y\\|_2$ for any vector $y$. Applying this property, we get:\n$$\n\\|Ax\\|_2^2 = \\|\\Sigma V^{\\mathsf{T}} x\\|_2^2\n$$\nLet us define a new vector $y = V^{\\mathsf{T}} x$. Since $V$ is orthogonal, $V^{\\mathsf{T}}$ is also orthogonal. Thus, $V^{\\mathsf{T}}$ also preserves the $2$-norm: $\\|y\\|_2 = \\|V^{\\mathsf{T}} x\\|_2 = \\|x\\|_2 = 1$. As $x$ varies over all vectors of unit length, the corresponding vector $y$ also varies over all vectors of unit length. Therefore, the problem of finding $\\|A\\|_2$ is equivalent to finding the maximum of $\\|\\Sigma y\\|_2$ for $\\|y\\|_2=1$.\n\nLet's expand $\\|\\Sigma y\\|_2^2$:\n$$\n\\|\\Sigma y\\|_2^2 = (\\Sigma y)^{\\mathsf{T}}(\\Sigma y) = y^{\\mathsf{T}}\\Sigma^{\\mathsf{T}}\\Sigma y = y^{\\mathsf{T}}\\Sigma^2 y\n$$\nSince $\\Sigma$ is diagonal, $\\Sigma^2 = \\text{diag}(\\sigma_1^2, \\sigma_2^2, \\dots, \\sigma_n^2)$. The expression becomes a weighted sum:\n$$\n\\|\\Sigma y\\|_2^2 = \\sum_{i=1}^{n} \\sigma_i^2 y_i^2\n$$\nWe want to maximize this quantity subject to the constraint $\\|y\\|_2^2 = \\sum_{i=1}^{n} y_i^2 = 1$. Since the singular values are ordered $\\sigma_1^2 \\ge \\sigma_2^2 \\ge \\dots \\ge \\sigma_n^2$, we can establish an upper bound:\n$$\n\\sum_{i=1}^{n} \\sigma_i^2 y_i^2 \\le \\sigma_1^2 \\sum_{i=1}^{n} y_i^2 = \\sigma_1^2 \\cdot 1 = \\sigma_1^2\n$$\nThis maximum value is achieved when $y$ is the first standard basis vector $e_1 = (1, 0, \\dots, 0)^{\\mathsf{T}}$.\nTherefore, $\\max_{\\|y\\|_2=1} \\|\\Sigma y\\|_2^2 = \\sigma_1^2$, which implies that $\\|A\\|_2 = \\sqrt{\\sigma_1^2} = \\sigma_1$. This is the largest singular value of $A$, denoted $\\sigma_{\\max}(A)$. So, $\\|A\\|_2 = \\sigma_{\\max}(A)$.\n\nNext, we derive an expression for $\\|A^{-1}\\|_2$. Since $A$ is nonsingular, all its singular values must be strictly positive: $\\sigma_n > 0$. The inverse of $A$ is:\n$$\nA^{-1} = (U \\Sigma V^{\\mathsf{T}})^{-1} = (V^{\\mathsf{T}})^{-1} \\Sigma^{-1} U^{-1} = V \\Sigma^{-1} U^{\\mathsf{T}}\n$$\nThis expression is a valid SVD for $A^{-1}$, where the orthogonal matrices are $V$ and $U$, and the diagonal matrix is $\\Sigma^{-1} = \\text{diag}(1/\\sigma_1, 1/\\sigma_2, \\dots, 1/\\sigma_n)$. The singular values of $A^{-1}$ are the diagonal entries of $\\Sigma^{-1}$. To find the largest singular value of $A^{-1}$, we must identify the largest element among $\\{1/\\sigma_i\\}$. Since $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_n > 0$, it follows that $1/\\sigma_n \\ge 1/\\sigma_{n-1} \\ge \\dots \\ge 1/\\sigma_1 > 0$.\nThe largest singular value of $A^{-1}$ is $\\sigma_{\\max}(A^{-1}) = 1/\\sigma_n$, which is the reciprocal of the smallest singular value of $A$, $\\sigma_{\\min}(A)$.\n\nUsing the result we derived for the $2$-norm of a matrix, we have:\n$$\n\\|A^{-1}\\|_2 = \\sigma_{\\max}(A^{-1}) = \\frac{1}{\\sigma_{\\min}(A)}\n$$\nFinally, we combine these results to find the condition number $\\kappa_2(A)$:\n$$\n\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2 = \\sigma_{\\max}(A) \\cdot \\frac{1}{\\sigma_{\\min}(A)} = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)}\n$$\nThis is the desired expression for the $2$-norm condition number in terms of the singular values of $A$.\n\n### Part 3: Evaluation of $\\kappa_2(A)$\n\nFor the matrix $A$ constructed in Part 1, the singular values are given as:\n$$\n\\sigma_1 = 1, \\quad \\sigma_2 = 1, \\quad \\sigma_3 = 1, \\quad \\sigma_4 = 10^{-16}, \\quad \\sigma_5 = 10^{-16}\n$$\nThe largest singular value is $\\sigma_{\\max}(A) = \\sigma_1 = 1$.\nThe smallest singular value is $\\sigma_{\\min}(A) = \\sigma_5 = 10^{-16}$.\nThe matrix $A$ is nonsingular because its smallest singular value is greater than $0$.\n\nUsing the formula derived in Part 2, the $2$-norm condition number of our constructed matrix $A$ is:\n$$\n\\kappa_2(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)} = \\frac{1}{10^{-16}} = 10^{16}\n$$\nThe condition number is extremely large, indicating that the linear system $Ax=b$ with this matrix $A$ is severely ill-conditioned. Small perturbations in $b$ can lead to very large changes in the solution $x$, a characteristic behavior explained by the vast ratio between its largest and smallest singular values.", "answer": "$$\\boxed{10^{16}}$$", "id": "3280607"}, {"introduction": "Diagnosing ill-conditioning is the first step; treating it is the next. This practice introduces a central idea in numerical regularization: the concept of a *numerical rank*, where singular values below a given threshold are deemed negligible and treated as zero. You will apply this truncation rule to a numerically rank-deficient system and explore the profound consequences for the solution space of the corresponding least-squares problem [@problem_id:3280528].", "problem": "Consider the linear system $A x = b$ with \n$$\nA \\in \\mathbb{R}^{3 \\times 2}, \\quad A = \\begin{pmatrix}\n1 & 0 \\\\\n0 & \\varepsilon \\\\\n0 & 0\n\\end{pmatrix}, \\qquad b = \\begin{pmatrix} 2 \\\\ 1 \\\\ 3 \\end{pmatrix},\n$$\nwhere $\\varepsilon = 10^{-4}$. You are told that $A$ is numerically rank-deficient with respect to the truncation threshold $\\tau = 10^{-3}$, in the sense that singular values smaller than $\\tau$ are treated as zero for model construction.\n\nUsing only the definitions of the least-squares problem $\\min_{x \\in \\mathbb{R}^{2}} \\|A x - b\\|_{2}$, the singular value decomposition (SVD), and orthogonal projections, proceed as follows:\n\n1. Starting from the definition of the singular value decomposition, explain why $A$ is numerically rank-deficient at the threshold $\\tau$, and construct the truncated matrix $A_{\\tau}$ obtained by discarding singular directions associated with singular values smaller than $\\tau$.\n\n2. For the truncated problem $\\min_{x \\in \\mathbb{R}^{2}} \\|A_{\\tau} x - b\\|_{2}$, derive the complete set of least-squares minimizers and demonstrate that it forms an affine subspace. Identify explicitly its minimum Euclidean norm element.\n\n3. Compute the minimal achievable residual norm $\\min_{x \\in \\mathbb{R}^{2}} \\|A_{\\tau} x - b\\|_{2}$ for this truncated problem. Provide your final numerical value rounded to four significant figures. Do not include units in your final answer.", "solution": "The user has provided a well-posed problem in numerical linear algebra. The validation process is completed, and the problem is deemed valid.\n\nThe problem asks for a three-part analysis of a linear system using the Singular Value Decomposition (SVD) and a truncation threshold. Let the given matrix, vector, constant, and threshold be:\n$$\nA = \\begin{pmatrix}\n1 & 0 \\\\\n0 & \\varepsilon \\\\\n0 & 0\n\\end{pmatrix}, \\quad b = \\begin{pmatrix} 2 \\\\ 1 \\\\ 3 \\end{pmatrix}, \\quad \\varepsilon = 10^{-4}, \\quad \\tau = 10^{-3}\n$$\n\n**Part 1: Numerical Rank-Deficiency and Truncated Matrix $A_{\\tau}$**\n\nFirst, we compute the Singular Value Decomposition (SVD) of $A$, which is of the form $A = U \\Sigma V^{\\mathsf{T}}$.\nThe singular values $\\sigma_i$ are the square roots of the eigenvalues of $A^{\\mathsf{T}} A$.\n$$\nA^{\\mathsf{T}} A = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\varepsilon & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & \\varepsilon \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1^2 & 0 \\\\ 0 & \\varepsilon^2 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & (10^{-4})^2 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 10^{-8} \\end{pmatrix}\n$$\nThe eigenvalues of $A^{\\mathsf{T}} A$ are $\\lambda_1 = 1$ and $\\lambda_2 = 10^{-8}$. The singular values of $A$ are:\n$$\n\\sigma_1 = \\sqrt{\\lambda_1} = 1\n$$\n$$\n\\sigma_2 = \\sqrt{\\lambda_2} = \\sqrt{10^{-8}} = 10^{-4}\n$$\nThe matrix of singular values $\\Sigma \\in \\mathbb{R}^{3 \\times 2}$ is:\n$$\n\\Sigma = \\begin{pmatrix} \\sigma_1 & 0 \\\\ 0 & \\sigma_2 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 10^{-4} \\\\ 0 & 0 \\end{pmatrix}\n$$\nThe right singular vectors $v_i$ are the orthonormal eigenvectors of $A^{\\mathsf{T}} A$. Since $A^{\\mathsf{T}} A$ is diagonal, the eigenvectors are the standard basis vectors:\n$$\nv_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad v_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n$$\nThus, the matrix $V \\in \\mathbb{R}^{2 \\times 2}$ is the identity matrix $V = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I_2$.\n\nThe left singular vectors $u_i$ are found using the relation $u_i = \\frac{1}{\\sigma_i} A v_i$ for $\\sigma_i > 0$.\n$$\nu_1 = \\frac{1}{\\sigma_1} A v_1 = \\frac{1}{1} \\begin{pmatrix} 1 & 0 \\\\ 0 & \\varepsilon \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n$$\nu_2 = \\frac{1}{\\sigma_2} A v_2 = \\frac{1}{\\varepsilon} \\begin{pmatrix} 1 & 0 \\\\ 0 & \\varepsilon \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\varepsilon} \\begin{pmatrix} 0 \\\\ \\varepsilon \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nTo complete the orthonormal basis for $\\mathbb{R}^3$, we need a vector $u_3$ orthogonal to $u_1$ and $u_2$. The clear choice is $u_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$. Thus, the matrix $U \\in \\mathbb{R}^{3 \\times 3}$ is the identity matrix $U = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = I_3$.\nThe SVD of $A$ is $A = I_3 \\Sigma I_2^{\\mathsf{T}}$, which is consistent since $A$ is already in the form of a singular value matrix.\n\nNow, we assess the numerical rank with respect to the threshold $\\tau = 10^{-3}$.\n$$\n\\sigma_1 = 1 > 10^{-3} = \\tau\n$$\n$$\n\\sigma_2 = 10^{-4} < 10^{-3} = \\tau\n$$\nSince $\\sigma_2$ is smaller than the threshold $\\tau$, it is treated as zero. The numerical rank is the number of singular values greater than or equal to $\\tau$, which is $1$. The full rank of $A$ is $2$. As the numerical rank ($1$) is less than the full rank ($2$), $A$ is numerically rank-deficient with respect to $\\tau$.\n\nThe truncated matrix $A_{\\tau}$ is constructed by setting all singular values smaller than $\\tau$ to zero. This corresponds to creating a truncated singular value matrix $\\Sigma_{\\tau}$:\n$$\n\\Sigma_{\\tau} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\nThen, $A_{\\tau}$ is computed as $A_{\\tau} = U \\Sigma_{\\tau} V^{\\mathsf{T}}$.\n$$\nA_{\\tau} = I_3 \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix} I_2^{\\mathsf{T}} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\n\n**Part 2: Least-Squares Minimizers for the Truncated Problem**\n\nWe seek to find the set of vectors $x \\in \\mathbb{R}^2$ that minimize $\\|A_{\\tau} x - b \\|_2$. The minimizers are the solutions to the normal equations $A_{\\tau}^{\\mathsf{T}} A_{\\tau} x = A_{\\tau}^{\\mathsf{T}} b$.\nWe first compute the matrices involved:\n$$\nA_{\\tau}^{\\mathsf{T}} A_{\\tau} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\n$$\nA_{\\tau}^{\\mathsf{T}} b = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}\n$$\nThe normal equations are:\n$$\n\\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}\n$$\nThis system of equations yields $1 \\cdot x_1 + 0 \\cdot x_2 = 2$ and $0 \\cdot x_1 + 0 \\cdot x_2 = 0$. The first equation implies $x_1 = 2$. The second equation is always true and provides no constraint on $x_2$. Thus, $x_2$ can be any real number.\nThe complete set of least-squares minimizers can be written in parametric form. Let $x_2 = c$ for any $c \\in \\mathbb{R}$.\n$$\nx = \\begin{pmatrix} 2 \\\\ c \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + c \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n$$\nThis set of solutions represents a line in $\\mathbb{R}^2$. It is an affine subspace, formed by a particular solution $\\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}$ and the null space of $A_{\\tau}$, which is $\\text{span}\\left\\{\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\right\\}$.\n\nTo find the minimum Euclidean norm element, we minimize $\\|x\\|_2^2$ over the set of solutions.\n$$\n\\|x\\|_2^2 = x_1^2 + x_2^2 = 2^2 + c^2 = 4 + c^2\n$$\nThis quadratic function of $c$ is minimized when $c=0$. The minimum norm solution $x_{\\text{LS}}$ is therefore:\n$$\nx_{\\text{LS}} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}\n$$\n\n**Part 3: Minimal Achievable Residual Norm**\n\nThe minimal residual norm is $\\|A_{\\tau} x - b\\|_2$ for any $x$ in the solution set found in Part 2. We can use the minimum norm solution $x_{\\text{LS}} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}$ for this calculation.\nFirst, compute the vector $A_{\\tau} x_{\\text{LS}}$:\n$$\nA_{\\tau} x_{\\text{LS}} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThis vector is the orthogonal projection of $b$ onto the column space of $A_{\\tau}$. The residual vector $r$ is the difference between this projection and $b$:\n$$\nr = A_{\\tau} x_{\\text{LS}} - b = \\begin{pmatrix} 2 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -1 \\\\ -3 \\end{pmatrix}\n$$\nThe minimal achievable residual norm is the Euclidean norm of this residual vector:\n$$\n\\min_{x \\in \\mathbb{R}^{2}} \\|A_{\\tau} x - b\\|_{2} = \\|r\\|_2 = \\sqrt{0^2 + (-1)^2 + (-3)^2} = \\sqrt{1+9} = \\sqrt{10}\n$$\nThe problem requires this value rounded to four significant figures.\n$$\n\\sqrt{10} \\approx 3.16227766...\n$$\nRounding to four significant figures gives $3.162$.", "answer": "$$\\boxed{3.162}$$", "id": "3280528"}, {"introduction": "This final practice synthesizes theory and application in a hands-on numerical experiment. Having learned to diagnose ill-conditioning and apply truncation, you will now see precisely why these techniques are indispensable in scientific computing, where data is invariably contaminated with noise. By implementing a Truncated SVD (TSVD) solver, you will generate quantitative evidence showing how regularization safeguards a solution from being corrupted by noise that is amplified by small singular values, demonstrating the practical power of this method [@problem_id:3280674].", "problem": "You are given a square, ill-conditioned linear system represented by a real matrix $A \\in \\mathbb{R}^{n \\times n}$ and a right-hand-side vector $b \\in \\mathbb{R}^{n}$. The goal is to investigate, for a fixed truncation level $k$, how the quality of the Truncated Singular Value Decomposition (TSVD) solution to $A x = b$ deteriorates as the noise level in $b$ increases. Your program must be a complete and runnable implementation that constructs a specific ill-conditioned matrix, generates controlled noisy right-hand-sides, computes TSVD solutions, and reports a quantitative measure of solution quality across a predefined test suite.\n\nUse the following fundamental base and setup:\n- Singular Value Decomposition (SVD) is defined for any real matrix $A$ as $A = U \\Sigma V^{\\mathsf{T}}$, where $U$ and $V$ are orthogonal and $\\Sigma$ is diagonal with nonnegative entries.\n- For a fixed truncation level $k$, the TSVD solution is defined as the least-squares solution obtained by restricting to the dominant $k$ singular components of $A$.\n- The matrix $A$ is constructed to be ill-conditioned by prescribing rapidly decaying singular values.\n\nFollow this deterministic construction:\n1. Choose the problem size $n$ as $n = 30$ and fix the truncation level as $k = 10$.\n2. Construct orthogonal matrices $U \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{n \\times n}$ as the $Q$ factors from the QR decomposition of two independent matrices with independently and identically distributed standard normal entries, each of size $n \\times n$. Use a fixed pseudorandom seed to ensure determinism for both matrices, with seed $0$ for $U$ and seed $1$ for $V$.\n3. Define singular values $\\sigma_i$ for $i = 1, 2, \\dots, n$ as $\\sigma_i = 10^{-\\frac{i-1}{2}}$, and form the diagonal matrix $\\Sigma = \\operatorname{diag}(\\sigma_1, \\dots, \\sigma_n)$.\n4. Set $A = U \\Sigma V^{\\mathsf{T}}$.\n5. Define the true solution vector $x_{\\text{true}} \\in \\mathbb{R}^{n}$ by $x_{\\text{true},i} = \\frac{1}{i}$ for $i = 1, 2, \\dots, n$, and compute the clean right-hand-side $b_{\\text{clean}} = A x_{\\text{true}}$.\n6. Fix a deterministic noise direction $z \\in \\mathbb{R}^{n}$ generated as a standard normal vector with seed $7$, normalized to unit Euclidean norm, i.e., $z \\leftarrow z / \\|z\\|_2$.\n7. For each noise fraction $\\eta$, define the noisy right-hand-side $b(\\eta) = b_{\\text{clean}} + \\eta \\|b_{\\text{clean}}\\|_2 \\, z$.\n\nImplement the TSVD solution at rank $k$ for each $b(\\eta)$ and quantify solution quality using the relative solution error:\n$$\ne(\\eta) = \\frac{\\|x_k(\\eta) - x_{\\text{true}}\\|_2}{\\|x_{\\text{true}}\\|_2},\n$$\nwhere $x_k(\\eta)$ denotes the TSVD solution obtained from $b(\\eta)$ at truncation level $k$.\n\nTest Suite:\n- Use the following set of noise fractions $\\eta$ to probe different regimes, including a boundary case and increasing noise magnitudes:\n$$\n\\{\\eta_1, \\eta_2, \\eta_3, \\eta_4, \\eta_5, \\eta_6\\} = \\{0.0, 10^{-8}, 10^{-4}, 10^{-2}, 10^{-1}, 1.0\\}.\n$$\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered according to the test suite noise fractions:\n$$\n[e(\\eta_1), e(\\eta_2), e(\\eta_3), e(\\eta_4), e(\\eta_5), e(\\eta_6)].\n$$\n- The outputs are dimensionless floats. No physical units are involved.", "solution": "The problem presented is a well-posed and deterministic numerical experiment designed to investigate the effect of noise on the solution of an ill-conditioned linear system using Truncated SingularValue Decomposition (TSVD). The validation is successful as the problem is scientifically grounded in numerical linear algebra, provides a complete and consistent set of instructions, and is formulated objectively. We may therefore proceed with a complete solution.\n\nThe core of the problem lies in solving the linear system $A x = b$, where $A \\in \\mathbb{R}^{n \\times n}$ is an ill-conditioned matrix, $x \\in \\mathbb{R}^{n}$ is the vector of unknowns, and $b \\in \\mathbb{R}^{n}$ is the vector of observations.\n\nThe Singular Value Decomposition (SVD) of the matrix $A$ provides the necessary framework for analysis and solution. The SVD is given by $A = U \\Sigma V^{\\mathsf{T}}$, where $U, V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices ($U^{\\mathsf{T}}U = I$, $V^{\\mathsf{T}}V = I$) and $\\Sigma \\in \\mathbb{R}^{n \\times n}$ is a diagonal matrix with non-negative, non-increasing entries $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_n \\ge 0$ on its diagonal. These entries are the singular values of $A$. The columns of $U$ are the left singular vectors, and the columns of $V$ are the right singular vectors.\n\nSubstituting the SVD into the linear system yields $U \\Sigma V^{\\mathsf{T}} x = b$. A formal solution can be derived by isolating $x$:\n$$\nx = V \\Sigma^{-1} U^{\\mathsf{T}} b\n$$\nThis can be expressed as a summation:\n$$\nx = \\sum_{i=1}^{n} \\frac{u_i^{\\mathsf{T}} b}{\\sigma_i} v_i\n$$\nwhere $u_i$ and $v_i$ are the $i$-th columns of $U$ and $V$, respectively.\n\nA matrix is ill-conditioned if its condition number, $\\kappa(A) = \\sigma_1 / \\sigma_n$, is very large. This occurs when the singular values decay rapidly, causing $\\sigma_n$ to be close to zero. From the summation form of the solution, it is evident that any component of the right-hand-side $b$ that projects onto a left singular vector $u_i$ is amplified by the factor $1/\\sigma_i$. If $\\sigma_i$ is small, this amplification is enormous. In practical applications, the vector $b$ is often contaminated with noise, i.e., $b = b_{\\text{clean}} + \\delta b$. The noise term $\\delta b$ is invariably amplified by the small singular values, overwhelming the true solution.\n\nThe problem specifies the singular values as $\\sigma_i = 10^{-\\frac{i-1}{2}}$ for $i=1, \\dots, n$. With $n=30$, the condition number is $\\kappa(A) = \\sigma_1 / \\sigma_{30} = 10^0 / 10^{-29/2} = 10^{14.5}$, which indicates severe ill-conditioning.\n\nTruncated SVD (TSVD) is a regularization method to combat this noise amplification. Instead of using all $n$ singular components, the sum is truncated at a level $k < n$. The TSVD solution, denoted $x_k$, is defined as:\n$$\nx_k = \\sum_{i=1}^{k} \\frac{u_i^{\\mathsf{T}} b}{\\sigma_i} v_i\n$$\nThis is equivalent to using a truncated pseudo-inverse of $A$, $A_k^{\\dagger} = V_k \\Sigma_k^{-1} U_k^{\\mathsf{T}}$, where $U_k$ and $V_k$ contain the first $k$ columns of $U$ and $V$, and $\\Sigma_k$ is the leading $k \\times k$ submatrix of $\\Sigma$. The solution is then $x_k = A_k^{\\dagger} b$. By discarding terms for $i > k$, we filter out the unstable components associated with small singular values $\\sigma_{k+1}, \\dots, \\sigma_n$. The choice of $k$ is critical: it must be small enough to ensure stability but large enough to capture the essential features of the solution.\n\nThe problem constructs a specific, reproducible scenario to observe this behavior.\n1.  Set problem dimensions $n=30$ and truncation level $k=10$.\n2.  Construct deterministic orthogonal matrices $U$ and $V$ from the QR decomposition of two specific random matrices, ensuring reproducibility via fixed seeds ($0$ and $1$).\n3.  Construct the matrix $A = U \\Sigma V^{\\mathsf{T}}$ using the prescribed singular values $\\sigma_i = 10^{-\\frac{i-1}{2}}$.\n4.  Define a true solution $x_{\\text{true}}$ with components $x_{\\text{true},i} = 1/i$.\n5.  Compute the corresponding \"clean\" right-hand-side $b_{\\text{clean}} = A x_{\\text{true}}$.\n6.  Generate a deterministic noise vector $z$ (using seed $7$) and normalize it.\n7.  Create a series of noisy right-hand sides $b(\\eta) = b_{\\text{clean}} + \\eta \\|b_{\\text{clean}}\\|_2 \\, z$ for a set of noise fractions $\\eta \\in \\{0.0, 10^{-8}, 10^{-4}, 10^{-2}, 10^{-1}, 1.0\\}$.\n\nFor each $\\eta$, we compute the TSVD solution $x_k(\\eta)$ using the fixed truncation level $k=10$:\n$$\nx_k(\\eta) = \\sum_{i=1}^{k} \\frac{u_i^{\\mathsf{T}} b(\\eta)}{\\sigma_i} v_i\n$$\nThe quality of this solution is measured by the relative solution error:\n$$\ne(\\eta) = \\frac{\\|x_k(\\eta) - x_{\\text{true}}\\|_2}{\\|x_{\\text{true}}\\|_2}\n$$\nThe error $e(\\eta)$ arises from two sources:\n-   **Truncation (or Regularization) Error**: The error present even with no noise ($\\eta=0$). It is caused by discarding the components of $x_{\\text{true}}$ corresponding to singular values $\\sigma_{k+1}, \\dots, \\sigma_n$. This error is $e(0)=\\|x_k(0) - x_{\\text{true}}\\|_2 / \\|x_{\\text{true}}\\|_2$.\n-   **Noise Propagation Error**: The error resulting from the amplification of the noise term $\\eta \\|b_{\\text{clean}}\\|_2 \\, z$. This part of the error grows with $\\eta$.\n\nThe implementation will systematically follow these steps. For each value of $\\eta$ in the test suite, we will construct $b(\\eta)$, calculate $x_k(\\eta)$ using the TSVD formula, and compute the relative error $e(\\eta)$. The final output will be the list of these errors, demonstrating how the solution quality degrades as the noise level increases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes TSVD solutions and their errors for an ill-conditioned system\n    with varying noise levels in the right-hand-side vector.\n    \"\"\"\n    # 1. Choose problem size n and truncation level k.\n    n = 30\n    k = 10\n\n    # 2. Construct orthogonal matrices U and V.\n    # Use fixed seeds for determinism.\n    rng_u = np.random.default_rng(seed=0)\n    matrix_for_u = rng_u.standard_normal(size=(n, n))\n    U, _ = np.linalg.qr(matrix_for_u)\n\n    rng_v = np.random.default_rng(seed=1)\n    matrix_for_v = rng_v.standard_normal(size=(n, n))\n    V, _ = np.linalg.qr(matrix_for_v)\n\n    # 3. Define singular values sigma_i and form diagonal matrix Sigma.\n    # sigma_i = 10^(-(i-1)/2) for i = 1, ..., n\n    indices = np.arange(n)\n    sigmas = 10.0**(-0.5 * indices)\n    Sigma = np.diag(sigmas)\n\n    # 4. Set A = U * Sigma * V.T.\n    A = U @ Sigma @ V.T\n\n    # 5. Define the true solution vector x_true and compute b_clean.\n    # x_true_i = 1/i for i = 1, ..., n\n    x_true = 1.0 / np.arange(1, n + 1)\n    b_clean = A @ x_true\n\n    # 6. Fix a deterministic noise direction z.\n    rng_z = np.random.default_rng(seed=7)\n    z = rng_z.standard_normal(size=n)\n    z /= np.linalg.norm(z)\n\n    # Test Suite: noise fractions.\n    noise_fractions = [0.0, 1e-8, 1e-4, 1e-2, 1e-1, 1.0]\n    \n    results = []\n    \n    # Pre-calculate components for TSVD solution for efficiency.\n    U_k = U[:, :k]\n    V_k = V[:, :k]\n    sigmas_k_inv = 1.0 / sigmas[:k]\n    norm_x_true = np.linalg.norm(x_true)\n    norm_b_clean = np.linalg.norm(b_clean)\n\n    for eta in noise_fractions:\n        # 7. Define the noisy right-hand-side b(eta).\n        b_eta = b_clean + eta * norm_b_clean * z\n\n        # Implement the TSVD solution at rank k.\n        # x_k(eta) = V_k @ (diag(1/sigma_i) @ U_k.T @ b_eta)\n        # The inner part is a component-wise multiplication.\n        x_k_eta = V_k @ (sigmas_k_inv * (U_k.T @ b_eta))\n        \n        # Quantify solution quality using relative solution error.\n        error = np.linalg.norm(x_k_eta - x_true) / norm_x_true\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3280674"}]}