{"hands_on_practices": [{"introduction": "While least squares is often synonymous with fitting models to noisy data, its core engine, the QR factorization, is fundamentally a tool for revealing the geometric structure of linear systems. This practice explores a compelling application from chemistry, where we balance a chemical equation not by trial and error, but by formulating it as a homogeneous system $Ax=0$ [@problem_id:3275534]. You will discover how the QR factorization provides a systematic way to find a basis for the null space of the constraint matrix $A$, yielding the precise integer coefficients that satisfy the laws of conservation.", "problem": "A chemist is analyzing the oxidation of oxalate by permanganate in acidic aqueous solution. The unbalanced reaction is\n$$\\mathrm{MnO_{4}^{-}} + \\mathrm{C_{2}O_{4}^{2-}} + \\mathrm{H^{+}} \\rightarrow \\mathrm{Mn^{2+}} + \\mathrm{CO_{2}} + \\mathrm{H_{2}O}.$$\nUsing conservation of atoms and charge, encode this as a homogeneous linear system $A x = 0$, where $x$ is the vector of unknown stoichiometric coefficients ordered as\n$$x = \\begin{pmatrix}x_{1} & x_{2} & x_{3} & x_{4} & x_{5} & x_{6}\\end{pmatrix}^{\\mathsf{T}} = \\begin{pmatrix}\\mathrm{MnO_{4}^{-}} & \\mathrm{C_{2}O_{4}^{2-}} & \\mathrm{H^{+}} & \\mathrm{Mn^{2+}} & \\mathrm{CO_{2}} & \\mathrm{H_{2}O}\\end{pmatrix}^{\\mathsf{T}}.$$\nModel the products by negative contributions in $A$ so that element and charge conservation are written as linear constraints that sum to zero.\n\nStarting from the foundational principle of conservation laws for each element and for net charge, and from the definition of the QR factorization of a matrix, explain how the QR factorization reveals a basis for the null space $\\mathcal{N}(A)$ of the constraint matrix and thereby yields the stoichiometric coefficients. Then determine the smallest positive integer stoichiometric coefficient for $\\mathrm{H^{+}}$ in the balanced equation.\n\nYour final answer must be a single real number. No rounding is required. Express the final answer as a pure number with no units.", "solution": "The problem is well-posed and scientifically grounded, presenting a standard application of linear algebra to stoichiometry. All necessary information is provided.\n\nThe problem requires balancing the chemical reaction\n$$x_{1}\\,\\mathrm{MnO_{4}^{-}} + x_{2}\\,\\mathrm{C_{2}O_{4}^{2-}} + x_{3}\\,\\mathrm{H^{+}} \\rightarrow x_{4}\\,\\mathrm{Mn^{2+}} + x_{5}\\,\\mathrm{CO_{2}} + x_{6}\\,\\mathrm{H_{2}O}$$\nby finding the smallest positive integer coefficients $x_i$. This is equivalent to finding a basis vector with the smallest integer components for the null space of a constraint matrix $A$. The vector of coefficients is $x = \\begin{pmatrix}x_{1} & x_{2} & x_{3} & x_{4} & x_{5} & x_{6}\\end{pmatrix}^{\\mathsf{T}}$. The constraints arise from the conservation of each element and of the total charge. Products are modeled with negative coefficients.\n\n1.  **Conservation of Manganese (Mn):** The number of Mn atoms must be equal on both sides.\n    $$1 \\cdot x_{1} - 1 \\cdot x_{4} = 0$$\n2.  **Conservation of Oxygen (O):**\n    $$4 \\cdot x_{1} + 4 \\cdot x_{2} - 2 \\cdot x_{5} - 1 \\cdot x_{6} = 0$$\n3.  **Conservation of Carbon (C):**\n    $$2 \\cdot x_{2} - 1 \\cdot x_{5} = 0$$\n4.  **Conservation of Hydrogen (H):**\n    $$1 \\cdot x_{3} - 2 \\cdot x_{6} = 0$$\n5.  **Conservation of Charge:** The net charge must be equal on both sides.\n    $$(-1) \\cdot x_{1} + (-2) \\cdot x_{2} + (1) \\cdot x_{3} - (2) \\cdot x_{4} = 0$$\n\nThese five linear equations form a homogeneous system $Ax = 0$, where $A$ is the $5 \\times 6$ matrix of coefficients and $x \\in \\mathbb{R}^6$ is the vector of stoichiometric coefficients.\n\n$$A = \\begin{pmatrix} 1 & 0 & 0 & -1 & 0 & 0 \\\\ 4 & 4 & 0 & 0 & -2 & -1 \\\\ 0 & 2 & 0 & 0 & -1 & 0 \\\\ 0 & 0 & 1 & 0 & 0 & -2 \\\\ -1 & -2 & 1 & -2 & 0 & 0 \\end{pmatrix}$$\n\nThe solution to this chemical balancing problem is a vector $x$ in the null space of $A$, $\\mathcal{N}(A)$.\n\nThe problem requires an explanation of how the QR factorization can be used to find a basis for $\\mathcal{N}(A)$. Let $A$ be an $m \\times n$ matrix. The QR factorization of the *transpose* of $A$, denoted $A^{\\mathsf{T}}$, is given by $A^{\\mathsf{T}} = QR$, where $Q$ is an $n \\times n$ orthogonal matrix ($Q^{\\mathsf{T}}Q = I$) and $R$ is an $n \\times m$ upper trapezoidal matrix. The columns of $Q$ form an orthonormal basis for $\\mathbb{R}^n$.\n\nLet $r = \\mathrm{rank}(A) = \\mathrm{rank}(A^{\\mathsf{T}})$. The first $r$ columns of $Q$ form an orthonormal basis for the column space of $A^{\\mathsf{T}}$, which is precisely the row space of $A$, $\\mathrm{Row}(A)$. Let us partition $Q$ as $Q = \\begin{pmatrix} Q_1 & Q_2 \\end{pmatrix}$, where $Q_1$ consists of the first $r$ columns and $Q_2$ consists of the remaining $n-r$ columns.\n\nAccording to the fundamental theorem of linear algebra, the null space of $A$ is the orthogonal complement of the row space of $A$: $\\mathcal{N}(A) = (\\mathrm{Row}(A))^{\\perp}$. Since the columns of $Q_1$ form a basis for $\\mathrm{Row}(A)$ and the columns of $Q$ form an orthonormal basis for the entire space $\\mathbb{R}^n$, the remaining columns in $Q_2$ must form an orthonormal basis for the orthogonal complement of $\\mathrm{Row}(A)$. Therefore, the columns of $Q_2$ form an orthonormal basis for the null space of $A$.\n\nIn this specific problem, $A$ is a $5 \\times 6$ matrix. We expect the rank of $A$ to be $r=5$, resulting in a null space of dimension $n-r = 6-5=1$. Thus, the QR factorization of the $6 \\times 5$ matrix $A^{\\mathsf{T}}$ would yield a $6 \\times 6$ orthogonal matrix $Q$. The last column of $Q$, $q_6$, would form a basis for the one-dimensional null space of $A$. Any valid vector of stoichiometric coefficients $x$ must be a scalar multiple of this basis vector $q_6$.\n\nTo find this basis vector, we can solve the system $Ax=0$ using Gaussian elimination, which is operationally equivalent to solving $Ux=0$ where $U$ is the upper triangular matrix resulting from the elimination process.\n\nThe augmented matrix for the system is $[A|0]$. We perform row reduction on $A$:\n$$ \\begin{pmatrix} 1 & 0 & 0 & -1 & 0 & 0 \\\\ 4 & 4 & 0 & 0 & -2 & -1 \\\\ 0 & 2 & 0 & 0 & -1 & 0 \\\\ 0 & 0 & 1 & 0 & 0 & -2 \\\\ -1 & -2 & 1 & -2 & 0 & 0 \\end{pmatrix} \\xrightarrow{R_2 \\leftarrow R_2-4R_1, R_5 \\leftarrow R_5+R_1} \\begin{pmatrix} 1 & 0 & 0 & -1 & 0 & 0 \\\\ 0 & 4 & 0 & 4 & -2 & -1 \\\\ 0 & 2 & 0 & 0 & -1 & 0 \\\\ 0 & 0 & 1 & 0 & 0 & -2 \\\\ 0 & -2 & 1 & -3 & 0 & 0 \\end{pmatrix} $$\n$$ \\xrightarrow{R_3 \\leftarrow R_2-2R_3, R_5 \\leftarrow R_2+2R_5} \\begin{pmatrix} 1 & 0 & 0 & -1 & 0 & 0 \\\\ 0 & 4 & 0 & 4 & -2 & -1 \\\\ 0 & 0 & 0 & 4 & 0 & -1 \\\\ 0 & 0 & 1 & 0 & 0 & -2 \\\\ 0 & 0 & 2 & -2 & -2 & -1 \\end{pmatrix} \\xrightarrow{\\text{Swap } R_3, R_4} \\begin{pmatrix} 1 & 0 & 0 & -1 & 0 & 0 \\\\ 0 & 4 & 0 & 4 & -2 & -1 \\\\ 0 & 0 & 1 & 0 & 0 & -2 \\\\ 0 & 0 & 0 & 4 & 0 & -1 \\\\ 0 & 0 & 2 & -2 & -2 & -1 \\end{pmatrix} $$\n$$ \\xrightarrow{R_5 \\leftarrow R_5-2R_3} \\begin{pmatrix} 1 & 0 & 0 & -1 & 0 & 0 \\\\ 0 & 4 & 0 & 4 & -2 & -1 \\\\ 0 & 0 & 1 & 0 & 0 & -2 \\\\ 0 & 0 & 0 & 4 & 0 & -1 \\\\ 0 & 0 & 0 & -2 & -2 & 3 \\end{pmatrix} \\xrightarrow{R_5 \\leftarrow R_4+2R_5} \\begin{pmatrix} 1 & 0 & 0 & -1 & 0 & 0 \\\\ 0 & 4 & 0 & 4 & -2 & -1 \\\\ 0 & 0 & 1 & 0 & 0 & -2 \\\\ 0 & 0 & 0 & 4 & 0 & -1 \\\\ 0 & 0 & 0 & 0 & -4 & 5 \\end{pmatrix} $$\nThe system is now in row echelon form. We have $5$ pivots and $1$ free variable, $x_6$. Let $x_6 = t$, where $t$ is a parameter. We solve for the other variables by back substitution.\n\nFrom the 5th row: $-4x_5 + 5x_6 = 0 \\implies -4x_5 + 5t = 0 \\implies x_5 = \\frac{5}{4}t$.\nFrom the 4th row: $4x_4 - x_6 = 0 \\implies 4x_4 - t = 0 \\implies x_4 = \\frac{1}{4}t$.\nFrom the 3rd row: $x_3 - 2x_6 = 0 \\implies x_3 - 2t = 0 \\implies x_3 = 2t$.\nFrom the 2nd row: $4x_2 + 4x_4 - 2x_5 - x_6 = 0 \\implies 4x_2 + 4(\\frac{1}{4}t) - 2(\\frac{5}{4}t) - t = 0 \\implies 4x_2 + t - \\frac{5}{2}t - t = 0 \\implies 4x_2 = \\frac{5}{2}t \\implies x_2 = \\frac{5}{8}t$.\nFrom the 1st row: $x_1 - x_4 = 0 \\implies x_1 = x_4 = \\frac{1}{4}t$.\n\nThe general solution is $x = t \\begin{pmatrix} 1/4 & 5/8 & 2 & 1/4 & 5/4 & 1 \\end{pmatrix}^{\\mathsf{T}}$.\nTo find the smallest positive integer coefficients, we must choose the smallest positive value of $t$ that makes all components of $x$ integers. The denominators are $4$, $8$, $1$, $4$, $4$, $1$. The least common multiple is $\\mathrm{lcm}(4, 8) = 8$. We set $t=8$.\n\n$x_1 = \\frac{1}{4}(8) = 2$\n$x_2 = \\frac{5}{8}(8) = 5$\n$x_3 = 2(8) = 16$\n$x_4 = \\frac{1}{4}(8) = 2$\n$x_5 = \\frac{5}{4}(8) = 10$\n$x_6 = 1(8) = 8$\n\nThe vector of smallest integer coefficients is $x = \\begin{pmatrix} 2 & 5 & 16 & 2 & 10 & 8 \\end{pmatrix}^{\\mathsf{T}}$. The balanced equation is:\n$$2\\,\\mathrm{MnO_{4}^{-}} + 5\\,\\mathrm{C_{2}O_{4}^{2-}} + 16\\,\\mathrm{H^{+}} \\rightarrow 2\\,\\mathrm{Mn^{2+}} + 10\\,\\mathrm{CO_{2}} + 8\\,\\mathrm{H_{2}O}$$\nThe coefficient for $\\mathrm{H^{+}}$ is $x_3$. The smallest positive integer value for this coefficient is $16$.", "answer": "$$\\boxed{16}$$", "id": "3275534"}, {"introduction": "Moving from theory to practical implementation, we confront a challenge ubiquitous in data science and engineering: badly scaled data. This exercise tasks you with building a least squares solver using Householder QR and applying it to a problem where features have mismatched physical units, a common source of numerical instability [@problem_id:3275437]. By experimenting with different scaling strategies, you will see firsthand how preconditioning a matrix can dramatically improve its condition number and the reliability of the computed solution, a crucial lesson for any computational practitioner.", "problem": "You are asked to design and implement a program that solves overdetermined linear least squares problems in the presence of mixed units across feature columns by using a numerically stable orthogonal-factorization approach. The program must explicitly construct and use a Householder-based decomposition to factor a real matrix with more rows than columns and then solve the associated upper-triangular system, rather than relying on black-box solvers.\n\nStart from the following foundational base: the least squares problem minimizes the squared Euclidean norm $\\lVert A x - b \\rVert_2$ for a real matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m \\ge n$ and a real vector $b \\in \\mathbb{R}^m$; orthogonal transformations preserve the Euclidean norm; and any real matrix $A$ admits a factorization by successive orthogonal transformations that zero out subdiagonal entries to produce an upper-triangular factor. Your implementation must use these facts to design the algorithm.\n\nYou must address mixed units explicitly. Suppose a column of $A$ is measured in kilometers while another is in meters. To achieve consistent physical interpretation of the coefficients $x$, you should construct a diagonal scaling matrix $D_{\\text{phys}}$ that converts the kilometer-measured columns to meters (for instance, multiply a kilometer column by $1000$). In addition, to examine numerical stability independent of physical units, you should construct a column normalization scaling matrix $S_{\\text{norm}}$ that scales each column to unit $\\ell_2$ norm. Given a scaled matrix $A S$ and a solution $y$ that minimizes $\\lVert A S y - b \\rVert_2$, the coefficient vector in the original mixed-unit coordinates is $x = S y$. Your program must evaluate the solution found under three scenarios: unscaled mixed units, physically consistent units, and column-normalized units, and compare the residual norms and conditioning across scenarios while mapping coefficients back to the same original coordinate system for interpretability.\n\nImplement the following:\n- Construct a Householder-based orthogonal factorization for a given $A$, apply the associated orthogonal transformations to $b$ to obtain $Q^\\top b$, and solve the resulting upper-triangular system $R x = Q^\\top b$ for the least squares solution $x$ where $A = Q R$ with $Q$ orthogonal and $R$ upper-triangular.\n- For physical scaling, define $D_{\\text{phys}}$ by multiplying each kilometer column by $1000$ to convert it to meters, and leave meter and bias columns unchanged. For normalization scaling, define $S_{\\text{norm}} = \\operatorname{diag}(1/\\lVert a_1 \\rVert_2, \\dots, 1/\\lVert a_n \\rVert_2)$, where $a_j$ is the $j$-th column of $A$. The mapped coefficients in the original mixed-unit coordinates are $x_{\\text{phys}} = D_{\\text{phys}} y_{\\text{phys}}$ and $x_{\\text{norm}} = S_{\\text{norm}} y_{\\text{norm}}$.\n- Compute the residual norms $\\lVert A x_{\\text{mix}} - b \\rVert_2$, $\\lVert A x_{\\text{phys}} - b \\rVert_2$, and $\\lVert A x_{\\text{norm}} - b \\rVert_2$ for the three scenarios. Also compute the $2$-norm condition numbers $\\kappa_2(A)$, $\\kappa_2(A D_{\\text{phys}})$, and $\\kappa_2(A S_{\\text{norm}})$. Finally, compute the relative differences $\\delta_{\\text{phys}} = \\lVert x_{\\text{mix}} - x_{\\text{phys}} \\rVert_2 / \\lVert x_{\\text{mix}} \\rVert_2$ and $\\delta_{\\text{norm}} = \\lVert x_{\\text{mix}} - x_{\\text{norm}} \\rVert_2 / \\lVert x_{\\text{mix}} \\rVert_2$.\n\nAll outputs must be dimensionless decimal numbers. Round all reported floats to 6 decimal places.\n\nTest suite. Use the following three test cases. In each, the matrix $A$ has three columns: the first measured in meters, the second measured in kilometers, and the third is a bias column of ones. The vector $b$ is explicitly given.\n\nTest case $1$ (moderate conditioning, mixed units):\n$$\nA_1 = \\begin{bmatrix}\n120 & 0.2 & 1 \\\\\n340 & 0.5 & 1 \\\\\n560 & 0.9 & 1 \\\\\n780 & 1.3 & 1 \\\\\n910 & 1.6 & 1 \\\\\n1050 & 2.0 & 1\n\\end{bmatrix},\\quad\nb_1 = \\begin{bmatrix}\n356 \\\\\n778 \\\\\n1325 \\\\\n1859 \\\\\n2253.5 \\\\\n2751.5\n\\end{bmatrix}.\n$$\n\nTest case $2$ (extreme scale disparity across columns):\n$$\nA_2 = \\begin{bmatrix}\n0.4 & 150 & 1 \\\\\n0.6 & 300 & 1 \\\\\n0.9 & 450 & 1 \\\\\n1.2 & 600 & 1 \\\\\n1.5 & 750 & 1\n\\end{bmatrix},\\quad\nb_2 = \\begin{bmatrix}\n75999.6 \\\\\n151002.9 \\\\\n225999.35 \\\\\n301002.8 \\\\\n376002.25\n\\end{bmatrix}.\n$$\n\nTest case $3$ (near-collinearity of meter and kilometer columns, but full rank):\n$$\nA_3 = \\begin{bmatrix}\n960 & 0.95 & 1 \\\\\n1035 & 1.05 & 1 \\\\\n1220 & 1.20 & 1 \\\\\n1295 & 1.30 & 1 \\\\\n1500 & 1.50 & 1 \\\\\n1790 & 1.80 & 1 \\\\\n2105 & 2.10 & 1\n\\end{bmatrix},\\quad\nb_3 = \\begin{bmatrix}\n396 \\\\\n401 \\\\\n480 \\\\\n486 \\\\\n549 \\\\\n634 \\\\\n731\n\\end{bmatrix}.\n$$\n\nFor each test case $i \\in \\{1,2,3\\}$, let $A_i$ and $b_i$ be as above. Your program must, for each $i$:\n- Solve the least squares problem in mixed units to obtain $x_{\\text{mix}}$ via Householder-based orthogonal factorization.\n- Solve the least squares problem after physical scaling to obtain $y_{\\text{phys}}$ on $A_i D_{\\text{phys}}$ and then map $x_{\\text{phys}} = D_{\\text{phys}} y_{\\text{phys}}$.\n- Solve the least squares problem after column normalization to obtain $y_{\\text{norm}}$ on $A_i S_{\\text{norm}}$ and then map $x_{\\text{norm}} = S_{\\text{norm}} y_{\\text{norm}}$.\n- Compute and report the eight floats for that case in the order\n$$\n\\big[\n\\lVert A_i x_{\\text{mix}} - b_i \\rVert_2,\n\\lVert A_i x_{\\text{phys}} - b_i \\rVert_2,\n\\lVert A_i x_{\\text{norm}} - b_i \\rVert_2,\n\\kappa_2(A_i),\n\\kappa_2(A_i D_{\\text{phys}}),\n\\kappa_2(A_i S_{\\text{norm}}),\n\\delta_{\\text{phys}},\n\\delta_{\\text{norm}}\n\\big].\n$$\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces. The top-level list should contain three sublists corresponding to the three test cases, each sublist containing the eight rounded floats in the order specified. For example, the output must look like\n$$\n\\big[\\,[f_{11},f_{12},\\dots,f_{18}],\\,[f_{21},f_{22},\\dots,f_{28}],\\,[f_{31},f_{32},\\dots,f_{38}]\\,\\big],\n$$\nwhere each $f_{jk}$ is a decimal number formatted to 6 decimal places. No other text may be printed.", "solution": "The problem requires the design and implementation of a numerically stable solver for overdetermined linear least squares problems, $\\min_{x} \\lVert A x - b \\rVert_2$, where $A \\in \\mathbb{R}^{m \\times n}$ with $m \\ge n$, and $b \\in \\mathbb{R}^m$. The specified method is QR factorization using Householder transformations. A key aspect of the problem is to investigate the effects of preconditioning the matrix $A$ through scaling to address issues arising from mixed physical units and disparate column norms.\n\nThe foundational principle is that the Euclidean norm is invariant under orthogonal transformations. If $Q \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix (i.e., $Q^\\top Q = I$), then for any vector $z \\in \\mathbb{R}^m$, $\\lVert Q z \\rVert_2 = \\lVert z \\rVert_2$. We can leverage this property by factorizing the matrix $A$ into the product of an orthogonal matrix $Q$ and an upper-triangular matrix $R$, such that $A=QR$. The least squares problem can then be rewritten as:\n$$\n\\lVert A x - b \\rVert_2^2 = \\lVert Q R x - b \\rVert_2^2 = \\lVert Q^\\top (Q R x - b) \\rVert_2^2 = \\lVert R x - Q^\\top b \\rVert_2^2\n$$\nThe matrix $R \\in \\mathbb{R}^{m \\times n}$ has an upper-triangular structure. Since $m \\ge n$, it can be partitioned as:\n$$\nR = \\begin{bmatrix} R_1 \\\\ 0 \\end{bmatrix}, \\quad \\text{where } R_1 \\in \\mathbb{R}^{n \\times n} \\text{ is upper-triangular.}\n$$\nSimilarly, the transformed vector $Q^\\top b$ can be partitioned as:\n$$\nQ^\\top b = \\begin{bmatrix} c_1 \\\\ c_2 \\end{bmatrix}, \\quad \\text{where } c_1 \\in \\mathbb{R}^n \\text{ and } c_2 \\in \\mathbb{R}^{m-n}.\n$$\nThe minimization problem is thus transformed into:\n$$\n\\min_{x} \\left\\lVert \\begin{bmatrix} R_1 \\\\ 0 \\end{bmatrix} x - \\begin{bmatrix} c_1 \\\\ c_2 \\end{bmatrix} \\right\\rVert_2^2 = \\min_{x} \\left( \\lVert R_1 x - c_1 \\rVert_2^2 + \\lVert c_2 \\rVert_2^2 \\right)\n$$\nThe term $\\lVert c_2 \\rVert_2^2$ is independent of $x$. The minimum of the expression is achieved when $\\lVert R_1 x - c_1 \\rVert_2^2 = 0$. Assuming the columns of $A$ are linearly independent, $A$ has full rank, and $R_1$ is invertible. The unique least squares solution $x$ is therefore found by solving the square upper-triangular system:\n$$\nR_1 x = c_1\n$$\nThis system can be efficiently solved using back substitution. The squared norm of the final residual is $\\lVert A x - b \\rVert_2^2 = \\lVert c_2 \\rVert_2^2$.\n\nThe QR factorization is constructed using a sequence of Householder transformations. A Householder transformation is a reflection across a hyperplane and is represented by a matrix $H = I - 2 \\frac{v v^\\top}{v^\\top v}$ for a non-zero vector $v \\in \\mathbb{R}^k$. For any vector $z \\in \\mathbb{R}^k$, we can choose a vector $v$ such that $H z$ is a multiple of the standard basis vector $e_1 = [1, 0, \\dots, 0]^\\top$. Specifically, the vector $v$ is chosen as $v = z + \\alpha e_1$, where $\\alpha = \\text{sgn}(z_1) \\lVert z \\rVert_2$. The sign is chosen to avoid catastrophic cancellation when $z$ is nearly parallel to $e_1$.\n\nThe algorithm for QR factorization of $A$ proceeds column by column. For each column $j$ from $1$ to $n$:\n$1$. Consider the vector $z$ comprising the elements of column $j$ from the diagonal downwards, i.e., $A_{j:m, j}$.\n$2$. Construct the corresponding Householder vector $v_j$ and the transformation matrix $H_j$.\n$3$. Apply this transformation to the submatrix of $A$ from row $j$ to $m$ and column $j$ to $n$. This introduces zeros below the diagonal in column $j$. The same transformation must be applied to the corresponding elements of vector $b$.\nAfter $n$ steps, the matrix $A$ is transformed into the upper-triangular matrix $R$, and the vector $b$ is transformed into $Q^\\top b$. Formally, $R = H_n \\cdots H_2 H_1 A$ and $Q^\\top b = H_n \\cdots H_2 H_1 b$, where $Q = H_1 H_2 \\cdots H_n$. The algorithm does not require forming $Q$ explicitly.\n\nThe problem investigates three scenarios based on matrix scaling:\n$1$. **Mixed Units**: The problem is solved using the given matrix $A$. The solution is $x_{\\text{mix}}$.\n$2$. **Physical Scaling**: The matrix $A$ is pre-multiplied by a diagonal scaling matrix $D_{\\text{phys}}$. For the given problem, the second column is in kilometers while others are in meters or are dimensionless. To convert kilometers to meters, we set $D_{\\text{phys}} = \\text{diag}(1, 1000, 1)$. The least squares problem is solved for the scaled matrix $\\hat{A}_{\\text{phys}} = A D_{\\text{phys}}$ to find a solution $y_{\\text{phys}}$. The solution in the original, unscaled coordinates is recovered by $x_{\\text{phys}} = D_{\\text{phys}} y_{\\text{phys}}$.\n$3$. **Column Normalization**: The matrix $A$ is pre-multiplied by a diagonal matrix $S_{\\text{norm}} = \\text{diag}(1/\\lVert a_1 \\rVert_2, \\dots, 1/\\lVert a_n \\rVert_2)$, where $a_j$ is the $j$-th column of $A$. This scaling makes each column of the new matrix $\\hat{A}_{\\text{norm}} = A S_{\\text{norm}}$ have a unit $\\ell_2$-norm. The least squares problem is solved for $\\hat{A}_{\\text{norm}}$ to find $y_{\\text{norm}}$, and the original solution is recovered via $x_{\\text{norm}} = S_{\\text{norm}} y_{\\text{norm}}$.\n\nThe analysis requires computing several metrics for each scenario:\n- **Residual Norms**: $\\lVert A x - b \\rVert_2$ for $x \\in \\{x_{\\text{mix}}, x_{\\text{phys}}, x_{\\text{norm}}\\}$. In exact arithmetic, these would be identical, but in floating-point arithmetic, they may differ due to the varied numerical stability of the underlying problems.\n- **Condition Numbers**: The $2$-norm condition number, $\\kappa_2(M) = \\sigma_{\\max}(M) / \\sigma_{\\min}(M)$, measures the sensitivity of the solution to perturbations in the data. We compute $\\kappa_2(A)$, $\\kappa_2(A D_{\\text{phys}})$, and $\\kappa_2(A S_{\\text{norm}})$. A lower condition number generally implies a more numerically stable problem.\n- **Relative Differences**: $\\delta_{\\text{phys}} = \\lVert x_{\\text{mix}} - x_{\\text{phys}} \\rVert_2 / \\lVert x_{\\text{mix}} \\rVert_2$ and $\\delta_{\\text{norm}} = \\lVert x_{\\text{mix}} - x_{\\text{norm}} \\rVert_2 / \\lVert x_{\\text{mix}} \\rVert_2$. These quantify the numerical deviation of the solutions obtained from scaled matrices compared to the unscaled one.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the least squares problems for the given test cases.\n    \"\"\"\n\n    def householder_qr_solve(A, b):\n        \"\"\"\n        Solves the least squares problem min ||Ax - b||_2 using Householder QR factorization.\n\n        Args:\n            A (np.ndarray): The matrix A (m x n).\n            b (np.ndarray): The vector b (m,).\n\n        Returns:\n            np.ndarray: The solution vector x (n,).\n        \"\"\"\n        m, n = A.shape\n        R = A.copy()\n        c = b.copy().flatten()\n\n        for j in range(n):\n            # Extract the j-th column from the diagonal down\n            z = R[j:, j].copy()\n            \n            # Compute the Householder vector v\n            # Use `np.copysign` to handle the sign of 0 correctly\n            norm_z = np.linalg.norm(z)\n            alpha = -np.copysign(norm_z, z[0]) if norm_z != 0 else 0.0\n            \n            v = z.copy()\n            v[0] -= alpha\n            \n            norm_v = np.linalg.norm(v)\n            if norm_v > 1e-12: # Check to avoid division by zero\n                v = v / norm_v  # Normalize v to a unit vector u\n\n                # Apply reflection to the remaining submatrix of R\n                sub_matrix_R = R[j:, j:]\n                R[j:, j:] -= 2 * np.outer(v, v.T @ sub_matrix_R)\n\n                # Apply reflection to the corresponding part of b\n                sub_c = c[j:]\n                c[j:] -= 2 * v * (v.T @ sub_c)\n\n        # Extract the upper triangular matrix R1 and the vector c1\n        R1 = R[:n, :n]\n        c1 = c[:n]\n\n        # Solve the upper triangular system R1 @ x = c1 using back substitution\n        x = np.zeros(n)\n        for i in range(n - 1, -1, -1):\n            s = c1[i] - np.dot(R1[i, i + 1:], x[i + 1:])\n            x[i] = s / R1[i, i]\n            \n        return x\n\n    test_cases = [\n        (\n            np.array([\n                [120., 0.2, 1.], [340., 0.5, 1.], [560., 0.9, 1.],\n                [780., 1.3, 1.], [910., 1.6, 1.], [1050., 2.0, 1.]\n            ]),\n            np.array([356., 778., 1325., 1859., 2253.5, 2751.5])\n        ),\n        (\n            np.array([\n                [0.4, 150., 1.], [0.6, 300., 1.], [0.9, 450., 1.],\n                [1.2, 600., 1.], [1.5, 750., 1.]\n            ]),\n            np.array([75999.6, 151002.9, 225999.35, 301002.8, 376002.25])\n        ),\n        (\n            np.array([\n                [960., 0.95, 1.], [1035., 1.05, 1.], [1220., 1.20, 1.],\n                [1295., 1.30, 1.], [1500., 1.50, 1.], [1790., 1.80, 1.],\n                [2105., 2.10, 1.]\n            ]),\n            np.array([396., 401., 480., 486., 549., 634., 731.])\n        )\n    ]\n\n    all_results = []\n    \n    for A, b in test_cases:\n        # Scenario 1: Mixed Units (unscaled)\n        x_mix = householder_qr_solve(A, b)\n        res_norm_mix = np.linalg.norm(A @ x_mix - b.flatten())\n        cond_mix = np.linalg.cond(A, 2)\n\n        # Scenario 2: Physical Scaling\n        # Column 2 is in km, convert to m by multiplying by 1000.\n        D_phys = np.diag([1.0, 1000.0, 1.0])\n        A_phys = A @ D_phys\n        y_phys = householder_qr_solve(A_phys, b)\n        x_phys = D_phys @ y_phys\n        res_norm_phys = np.linalg.norm(A @ x_phys - b.flatten())\n        cond_phys = np.linalg.cond(A_phys, 2)\n        \n        # Scenario 3: Column Normalization Scaling\n        col_norms = np.linalg.norm(A, axis=0)\n        S_norm = np.diag(1.0 / col_norms)\n        A_norm = A @ S_norm\n        y_norm = householder_qr_solve(A_norm, b)\n        x_norm = S_norm @ y_norm\n        res_norm_norm = np.linalg.norm(A @ x_norm - b.flatten())\n        cond_norm = np.linalg.cond(A_norm, 2)\n\n        # Relative differences in solutions\n        delta_phys = np.linalg.norm(x_mix - x_phys) / np.linalg.norm(x_mix)\n        delta_norm = np.linalg.norm(x_mix - x_norm) / np.linalg.norm(x_mix)\n\n        case_results = [\n            res_norm_mix, res_norm_phys, res_norm_norm,\n            cond_mix, cond_phys, cond_norm,\n            delta_phys, delta_norm\n        ]\n        \n        all_results.append(case_results)\n\n    # Format the final output string exactly as specified.\n    formatted_cases = []\n    for case_res in all_results:\n        formatted_floats = [f\"{val:.6f}\" for val in case_res]\n        formatted_cases.append(f\"[{','.join(formatted_floats)}]\")\n    \n    print(f\"[{','.join(formatted_cases)}]\")\n\nsolve()\n```", "id": "3275437"}, {"introduction": "Many real-world optimization problems are not simple unconstrained minimization tasks; they must satisfy certain exact conditions. This practice demonstrates how to extend the least squares framework to handle such scenarios by solving an equality-constrained least squares (ECLS) problem [@problem_id:3275428]. You will implement the powerful null-space method, which uses a first QR factorization to parameterize the feasible solution space and a second QR factorization to solve a reduced, unconstrained problem, showcasing the modularity and elegance of orthogonal transformations in complex optimizations.", "problem": "You are asked to design and implement a complete, runnable program that solves equality-constrained Least Squares (LS) problems by orthogonal factorization and null-space reduction. The problem is to minimize the Euclidean norm objective $\\|A x - b\\|_2$ subject to the linear constraints $C x = d$. The program must construct a principled numerical solution using the following fundamental base:\n- The definition of the Euclidean norm $\\|y\\|_2 = \\sqrt{y^\\top y}$.\n- The definition of LS: for a given matrix $A$ and vector $b$, find $x$ that minimizes $\\|A x - b\\|_2$.\n- The definition of an orthogonal matrix $Q$ satisfying $Q^\\top Q = I$.\n- The existence of a QR factorization of a real matrix $M$ into an orthogonal factor and an upper-triangular factor; apply this factorization to the transpose $C^\\top$ of the constraint matrix to derive an orthonormal basis for both the column space and its orthogonal complement.\n- The concept of the null space of $C$, denoted $\\mathcal{N}(C) = \\{x : C x = 0\\}$.\n\nYour program must:\n- Construct an algorithmic solution based on obtaining an orthonormal decomposition from a QR factorization of $C^\\top$, use it to parameterize the feasible set for $C x = d$, reduce the constrained LS problem to an unconstrained LS problem with a smaller number of variables, and solve that reduced problem using QR factorization again.\n- Avoid ad hoc formulas; the design must be grounded in the above fundamental definitions and properties.\n\nInput is fixed within the program; do not read from files or standard input. The program must run as-is and produce a single line of output.\n\nTest Suite:\nImplement the solver and evaluate it on the following four test cases. Each case defines $A$, $b$, $C$, and $d$ explicitly.\n\nCase 1 (happy path, overdetermined $A$, partially constrained):\nLet $A \\in \\mathbb{R}^{6 \\times 4}$, $b \\in \\mathbb{R}^{6}$, $C \\in \\mathbb{R}^{2 \\times 4}$, $d \\in \\mathbb{R}^{2}$ be\n$$\nA =\n\\begin{bmatrix}\n2 & -1 & 0 & 0 \\\\\n1 & 3 & -2 & 1 \\\\\n0 & 1 & 4 & -1 \\\\\n3 & 0 & 1 & 2 \\\\\n0 & 2 & 0 & 1 \\\\\n1 & 0 & -1 & 1\n\\end{bmatrix},\\quad\nb =\n\\begin{bmatrix}\n1 \\\\ 2 \\\\ 3 \\\\ 0 \\\\ -1 \\\\ 4\n\\end{bmatrix},\\quad\nC =\n\\begin{bmatrix}\n1 & 0 & 1 & 0 \\\\\n0 & 1 & -1 & 2\n\\end{bmatrix},\\quad\nd =\n\\begin{bmatrix}\n1 \\\\ 0.5\n\\end{bmatrix}.\n$$\n\nCase 2 (no constraints, unconstrained LS):\nLet $A \\in \\mathbb{R}^{5 \\times 3}$, $b \\in \\mathbb{R}^{5}$, $C \\in \\mathbb{R}^{0 \\times 3}$, $d \\in \\mathbb{R}^{0}$ be\n$$\nA =\n\\begin{bmatrix}\n1 & 2 & 0 \\\\\n0 & 1 & 1 \\\\\n3 & -1 & 2 \\\\\n0 & 0 & 1 \\\\\n2 & 1 & 0\n\\end{bmatrix},\\quad\nb =\n\\begin{bmatrix}\n1 \\\\ 0 \\\\ 4 \\\\ -1 \\\\ 2\n\\end{bmatrix},\\quad\nC = \\text{an empty matrix with } 0 \\text{ rows and } 3 \\text{ columns},\\quad\nd = \\text{an empty vector}.\n$$\n\nCase 3 (fully constrained, $m = n$):\nLet $A \\in \\mathbb{R}^{4 \\times 3}$, $b \\in \\mathbb{R}^{4}$, $C \\in \\mathbb{R}^{3 \\times 3}$, $d \\in \\mathbb{R}^{3}$ be\n$$\nA =\n\\begin{bmatrix}\n1 & 0 & 2 \\\\\n0 & 1 & -1 \\\\\n2 & -1 & 0 \\\\\n1 & 1 & 1\n\\end{bmatrix},\\quad\nb =\n\\begin{bmatrix}\n3 \\\\ -1 \\\\ 2 \\\\ 0\n\\end{bmatrix},\\quad\nC =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix},\\quad\nd =\n\\begin{bmatrix}\n0.5 \\\\ -1.0 \\\\ 2.0\n\\end{bmatrix}.\n$$\n\nCase 4 (single constraint, larger $n$):\nLet $A \\in \\mathbb{R}^{7 \\times 5}$, $b \\in \\mathbb{R}^{7}$, $C \\in \\mathbb{R}^{1 \\times 5}$, $d \\in \\mathbb{R}^{1}$ be\n$$\nA =\n\\begin{bmatrix}\n1 & 0 & 2 & -1 & 0 \\\\\n0 & 1 & 0 & 2 & -1 \\\\\n2 & -1 & 1 & 0 & 1 \\\\\n0 & 0 & 1 & 1 & 0 \\\\\n1 & 2 & -1 & 0 & 2 \\\\\n3 & 0 & 0 & -2 & 1 \\\\\n0 & 1 & 1 & 0 & 0\n\\end{bmatrix},\\quad\nb =\n\\begin{bmatrix}\n1 \\\\ 2 \\\\ 0 \\\\ -1 \\\\ 3 \\\\ 4 \\\\ 0.5\n\\end{bmatrix},\\quad\nC =\n\\begin{bmatrix}\n1 & 0 & -1 & 0 & 2\n\\end{bmatrix},\\quad\nd =\n\\begin{bmatrix}\n1.0\n\\end{bmatrix}.\n$$\n\nOutput specification:\nFor each test case, compute the solution vector $x^\\star$, the residual norm $r = \\|A x^\\star - b\\|_2$, and the constraint violation $v = \\|C x^\\star - d\\|_2$. Round all floats to six decimal places. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case’s result is itself a list of floats ordered as [$x_1, \\dots, x_n, r, v$], for the corresponding case dimension $n$.\n\nFor example, the final output format must look like\n`[[x_1,x_2,...,x_n,r,v],[...],[...],[...]]`\nwith all floats rounded to six decimal places.\n\nAngle units and physical units are not applicable to this purely mathematical problem; do not include any units in the output.", "solution": "The problem is to find the vector $x \\in \\mathbb{R}^n$ that minimizes the Euclidean norm of the residual $\\|A x - b\\|_2$ subject to a set of linear equality constraints $C x = d$. Here, $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, $C \\in \\mathbb{R}^{p \\times n}$, and $d \\in \\mathbb{R}^p$. This is an equality-constrained least squares (ECLS) problem. A robust and principled method to solve this is the null-space method, which transforms the constrained problem into a smaller, unconstrained one. The derivation proceeds as follows, based on the fundamental principles outlined in the problem statement.\n\nFirst, we characterize the set of all vectors $x$ that satisfy the constraint $C x = d$. This set is an affine subspace of $\\mathbb{R}^n$. Any solution $x$ can be expressed as the sum of a particular solution $x_p$ (which satisfies $C x_p = d$) and a homogeneous solution $x_h$ (which satisfies $C x_h = 0$). The homogeneous solution $x_h$ must lie in the null space of $C$, denoted $\\mathcal{N}(C)$.\n$$\nx = x_p + x_h, \\quad \\text{where } x_h \\in \\mathcal{N}(C)\n$$\n\nThe core of the method is to find an orthonormal basis for the null space $\\mathcal{N}(C)$ and a suitable particular solution $x_p$. The problem specifies using the QR factorization of the transpose of the constraint matrix, $C^\\top$. Let $C^\\top \\in \\mathbb{R}^{n \\times p}$ have the QR factorization:\n$$\nC^\\top = QR\n$$\nwhere $Q \\in \\mathbb{R}^{n \\times n}$ is an orthogonal matrix ($Q^\\top Q = I_n$) and $R \\in \\mathbb{R}^{n \\times p}$ is an upper trapezoidal matrix. We assume that the constraint matrix $C$ has full row rank, meaning $\\text{rank}(C)=p$. This implies that $C^\\top$ has full column rank.\n\nWe partition the orthogonal matrix $Q$ into two blocks:\n$$\nQ = \\begin{bmatrix} Q_1 & Q_2 \\end{bmatrix}\n$$\nwhere $Q_1 \\in \\mathbb{R}^{n \\times p}$ and $Q_2 \\in \\mathbb{R}^{n \\times (n-p)}$. The columns of $Q_1$ form an orthonormal basis for the range of $C^\\top$, $\\mathcal{R}(C^\\top)$. The columns of $Q_2$ form an orthonormal basis for the orthogonal complement of $\\mathcal{R}(C^\\top)$, which is precisely the null space of $C$, $\\mathcal{N}(C)$. Therefore, any homogeneous solution $x_h$ can be written as a linear combination of the columns of $Q_2$:\n$$\nx_h = Q_2 z\n$$\nfor some vector of free parameters $z \\in \\mathbb{R}^{n-p}$.\n\nNext, we find a particular solution $x_p$. A convenient choice is the minimum-norm solution to $C x = d$, which lies entirely in the range of $C^\\top$. Thus, we can express it as $x_p = Q_1 y$ for some vector $y \\in \\mathbb{R}^p$. To find $y$, we substitute this into the constraint equation:\n$$\nC x_p = d \\implies C (Q_1 y) = d\n$$\nFrom the QR factorization, we have $C = (QR)^\\top = R^\\top Q^\\top$. The matrix $R$ can be partitioned as $R = \\begin{bmatrix} R_1 \\\\ 0 \\end{bmatrix}$, where $R_1 \\in \\mathbb{R}^{p \\times p}$ is upper triangular and invertible (since $\\text{rank}(C)=p$). Substituting this into the previous equation gives:\n$$\nR^\\top Q^\\top (Q_1 y) = \\begin{bmatrix} R_1^\\top & 0 \\end{bmatrix} \\begin{bmatrix} Q_1^\\top \\\\ Q_2^\\top \\end{bmatrix} (Q_1 y) = R_1^\\top Q_1^\\top Q_1 y = d\n$$\nSince $Q_1^\\top Q_1 = I_p$, the equation simplifies to a lower-triangular system for $y$:\n$$\nR_1^\\top y = d\n$$\nThis system can be solved for $y$ using forward substitution. The particular solution is then $x_p = Q_1 y$.\n\nWith the complete parameterization $x = x_p + Q_2 z$, we substitute it back into the original least squares objective:\n$$\n\\min_{x} \\|A x - b\\|_2 \\quad \\implies \\quad \\min_{z} \\|A (x_p + Q_2 z) - b\\|_2\n$$\nRearranging the terms, we get:\n$$\n\\min_{z} \\| (A Q_2) z - (b - A x_p) \\|_2\n$$\nThis is an unconstrained least squares problem for the unknown vector $z \\in \\mathbb{R}^{n-p}$. Let $\\hat{A} = A Q_2$ and $\\hat{b} = b - A x_p$. The problem is now to find $z^\\star$ that minimizes $\\|\\hat{A} z - \\hat{b}\\|_2$.\n\nThis standard least squares problem is solved using its own QR factorization. Let the factorization of $\\hat{A}$ be $\\hat{A} = \\hat{Q} \\hat{R}$, where $\\hat{Q}$ is orthogonal and $\\hat{R}$ is upper trapezoidal. The objective function becomes:\n$$\n\\|\\hat{Q} \\hat{R} z - \\hat{b}\\|_2 = \\|\\hat{Q}^\\top(\\hat{Q} \\hat{R} z - \\hat{b})\\|_2 = \\|\\hat{R} z - \\hat{Q}^\\top \\hat{b}\\|_2\n$$\nLet $\\hat{R}_1$ be the upper triangular part of $\\hat{R}$ and let $c_1$ be the corresponding part of the vector $\\hat{c} = \\hat{Q}^\\top \\hat{b}$. The solution $z^\\star$ is found by solving the upper-triangular system $\\hat{R}_1 z = c_1$ using back substitution.\n\nFinally, the optimal solution $x^\\star$ for the original ECLS problem is reconstructed by combining the particular and homogeneous parts:\n$$\nx^\\star = x_p + Q_2 z^\\star\n$$\nThe residual norm is calculated as $r = \\|A x^\\star - b\\|_2$, and the constraint violation norm is $v = \\|C x^\\star - d\\|_2$. By construction, $v$ should be close to machine precision.\n\nThis principled, step-by-step reduction using orthogonal factorizations provides a numerically stable and accurate method for solving equality-constrained least squares problems. The algorithm gracefully handles edge cases, such as problems with no constraints ($p=0$) or problems where the solution is fully determined by the constraints ($p=n$).", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_ecls(A, b, C, d):\n    \"\"\"\n    Solves an equality-constrained least squares problem using the null-space method.\n\n    Problem: minimize ||A*x - b||_2 subject to C*x = d.\n\n    Args:\n        A (np.ndarray): Matrix A of size m x n.\n        b (np.ndarray): Vector b of size m.\n        C (np.ndarray): Constraint matrix C of size p x n.\n        d (np.ndarray): Constraint vector d of size p.\n\n    Returns:\n        tuple: A tuple containing:\n            - x_star (np.ndarray): The solution vector of size n.\n            - residual_norm (float): The final residual norm ||A*x_star - b||_2.\n            - constraint_violation (float): The constraint violation ||C*x_star - d||_2.\n    \"\"\"\n    m, n = A.shape\n    p = C.shape[0]\n\n    if p == 0:\n        # Case with no constraints: reduces to a standard least squares problem.\n        # This branch aligns with the general logic by setting up identity transformations.\n        xp = np.zeros(n)\n        Q2 = np.eye(n)\n        A_hat = A\n        b_hat = b\n    else:\n        # Perform QR factorization on the transpose of the constraint matrix C\n        # C is p x n, so C.T is n x p.\n        Q, R = np.linalg.qr(C.T, mode='complete')\n\n        # Partition Q and R\n        # Q is n x n, R is n x p\n        Q1 = Q[:, :p]\n        Q2 = Q[:, p:]\n        R1 = R[:p, :]\n\n        # Find the particular solution xp to C*x = d\n        # Solve the lower triangular system R1.T * y = d\n        y = np.linalg.solve(R1.T, d)\n        xp = Q1 @ y\n\n        # Form the reduced unconstrained least squares problem\n        # min ||(A*Q2)*z - (b - A*xp)||_2\n        A_hat = A @ Q2\n        b_hat = b - A @ xp\n\n    # Solve the reduced unconstrained LS problem for the coefficients z of the null space\n    k = A_hat.shape[1]  # This is n-p\n    if k > 0:\n        # Use QR factorization of A_hat to solve for z\n        Q_hat, R_hat = np.linalg.qr(A_hat, mode='reduced')\n\n        # Solve R_hat * z = Q_hat.T * b_hat\n        c_hat = Q_hat.T @ b_hat\n        z_star = np.linalg.solve(R_hat, c_hat)\n    else:\n        # Trivial case where the null space is empty (n=p)\n        z_star = np.array([])\n    \n    # Reconstruct the final solution\n    # Q2 @ z_star produces a vector of zeros if z_star is empty\n    x_star = xp + Q2 @ z_star\n\n    # Calculate final norms for verification and output\n    residual_norm = np.linalg.norm(A @ x_star - b)\n    if p > 0:\n        constraint_violation = np.linalg.norm(C @ x_star - d)\n    else:\n        constraint_violation = 0.0\n\n    return x_star, residual_norm, constraint_violation\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and print results in the specified format.\n    \"\"\"\n    # Case 1\n    A1 = np.array([\n        [2., -1., 0., 0.], [1., 3., -2., 1.], [0., 1., 4., -1.],\n        [3., 0., 1., 2.], [0., 2., 0., 1.], [1., 0., -1., 1.]\n    ])\n    b1 = np.array([1., 2., 3., 0., -1., 4.])\n    C1 = np.array([[1., 0., 1., 0.], [0., 1., -1., 2.]])\n    d1 = np.array([1., 0.5])\n\n    # Case 2\n    A2 = np.array([\n        [1., 2., 0.], [0., 1., 1.], [3., -1., 2.],\n        [0., 0., 1.], [2., 1., 0.]\n    ])\n    b2 = np.array([1., 0., 4., -1., 2.])\n    C2 = np.empty((0, 3))\n    d2 = np.empty(0)\n\n    # Case 3\n    A3 = np.array([\n        [1., 0., 2.], [0., 1., -1.], [2., -1., 0.], [1., 1., 1.]\n    ])\n    b3 = np.array([3., -1., 2., 0.])\n    C3 = np.array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])\n    d3 = np.array([0.5, -1.0, 2.0])\n\n    # Case 4\n    A4 = np.array([\n        [1., 0., 2., -1., 0.], [0., 1., 0., 2., -1.], [2., -1., 1., 0., 1.],\n        [0., 0., 1., 1., 0.], [1., 2., -1., 0., 2.], [3., 0., 0., -2., 1.],\n        [0., 1., 1., 0., 0.]\n    ])\n    b4 = np.array([1., 2., 0., -1., 3., 4., 0.5])\n    C4 = np.array([[1., 0., -1., 0., 2.]])\n    d4 = np.array([1.0])\n\n    test_cases = [\n        (A1, b1, C1, d1),\n        (A2, b2, C2, d2),\n        (A3, b3, C3, d3),\n        (A4, b4, C4, d4),\n    ]\n\n    all_results = []\n    for A, b, C, d in test_cases:\n        x_star, r, v = solve_ecls(A, b, C, d)\n        case_result = list(x_star) + [r, v]\n        all_results.append(case_result)\n    \n    # Format the final output string exactly as specified.\n    list_strs = []\n    for case_res in all_results:\n        num_strs = [f\"{x:.6f}\" for x in case_res]\n        list_strs.append(f\"[{','.join(num_strs)}]\")\n    final_output = f\"[{','.join(list_strs)}]\"\n\n    print(final_output)\n\nsolve()\n```", "id": "3275428"}]}