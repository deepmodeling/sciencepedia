{"hands_on_practices": [{"introduction": "Many fundamental relationships in physics, such as the connection between a wave's frequency and its wavelength, follow an inverse law. This exercise [@problem_id:3221525] demonstrates how to analyze such nonlinear data by transforming variables to create a linear model. By recasting the inverse relationship $f = v / \\lambda$ into a linear form, you will use the method of least squares from first principles to estimate the wave speed $v$.", "problem": "You are given a set of noisy measurements of wavelength and frequency for waves that obey the fundamental dispersion relation $f = v / \\lambda$, where $f$ is frequency in hertz, $\\lambda$ is wavelength in meters, and $v$ is the wave speed in meters per second. Your task is to linearize this inverse relationship and estimate $v$ from data using a principle-based numerical method.\n\nFundamental base: Use the core physical model $f = v / \\lambda$ and the definition of least squares as the minimizer of the sum of squared residuals. Do not use any prepackaged or shortcut fitting formulas. Your program must compute a linearized fit by transforming the variables so that the model becomes linear in the unknown, and then estimating the unknown by minimizing the sum of squared errors in that linearized space. For physical realism, constrain the fitted line to pass through the origin, consistent with $f = 0$ when $\\lambda \\to \\infty$ for constant $v$ and no offset in $f$.\n\nAlgorithmic task specification:\n- Linearize the model to a form that is linear in $v$.\n- Using ordinary least squares (defined as minimizing the sum of squared residuals), compute the estimate $\\hat{v}$ from the linearized data under the constraint that the fitted line passes through the origin.\n- Implement this computation directly from the definition of least squares without relying on any library fitting routines.\n- All estimates must be expressed in meters per second and rounded to exactly six digits after the decimal point.\n\nTest suite:\nCompute $\\hat{v}$ for each of the following five independent datasets. Each dataset provides wavelength values in meters and frequency values in hertz. There is small multiplicative measurement noise to ensure a realistic scenario.\n\n- Case A (happy path, moderate wavelengths):\n  - Wavelengths (meters): $[0.5, 0.8, 1.2, 1.5, 2.0]$\n  - Frequencies (hertz): $[689.1456, 427.713, 286.572, 227.656, 171.7716]$\n\n- Case B (large wavelengths, low frequencies):\n  - Wavelengths (meters): $[2.5, 5.0, 7.5, 10.0, 12.5]$\n  - Frequencies (hertz): $[592.8, 296.9928, 197.4024, 148.6446, 118.32288]$\n\n- Case C (very small wavelengths, very high frequencies):\n  - Wavelengths (meters): $[4.0\\times 10^{-7}, 5.0\\times 10^{-7}, 6.0\\times 10^{-7}, 7.0\\times 10^{-7}]$\n  - Frequencies (hertz): $[7.5\\times 10^{14}, 5.9994\\times 10^{14}, 5.001\\times 10^{14}, 4.285071428571429\\times 10^{14}]$\n\n- Case D (boundary case with two points):\n  - Wavelengths (meters): $[3.0, 4.0]$\n  - Frequencies (hertz): $[40.4, 29.85]$\n\n- Case E (influence of an outlier):\n  - Wavelengths (meters): $[1.0, 2.0, 3.0, 4.0, 5.0]$\n  - Frequencies (hertz): $[1000.0, 500.0, 366.6666666666667, 250.0, 200.0]$\n\nOutput format:\n- Your program must produce a single line containing a Python-style list of the five estimated speeds, each in meters per second, rounded to exactly six digits after the decimal point, in the order of the cases A, B, C, D, E. For example, the output must look like `[a,b,c,d,e]` with each of $a,b,c,d,e$ a floating-point number with exactly six digits after the decimal point.\n\nNo input should be read from standard input. All constants and datasets are provided above. Angles are not involved. All outputs must be in meters per second and must be rounded to exactly six digits after the decimal point. The final output must be a single line with the list as specified.", "solution": "The problem requires the estimation of wave speed, $v$, from sets of noisy measurements of wavelength, $\\lambda$, and frequency, $f$. The governing physical model is the fundamental dispersion relation:\n$$\nf = \\frac{v}{\\lambda}\n$$\nThis relationship is non-linear, specifically an inverse relationship between $f$ and $\\lambda$. To apply a linear estimation method, we must first linearize the model.\n\nLet us define a new set of variables. Let the independent variable be $x = 1/\\lambda$ and the dependent variable be $y = f$. Substituting these into the physical model yields:\n$$\ny = v \\cdot x\n$$\nThis transformed equation represents a linear relationship between $y$ and $x$. It describes a straight line in the $(x, y)$ plane that passes through the origin $(0, 0)$. The slope of this line is the wave speed, $v$, which is the parameter we wish to estimate. The constraint that the fitted line must pass through the origin is physically justified, as for a finite wave speed $v$, the frequency $f$ approaches $0$ as the wavelength $\\lambda$ approaches infinity (i.e., as $x = 1/\\lambda$ approaches $0$).\n\nThe task is to estimate $v$ using the principle of ordinary least squares (OLS) for a regression through the origin. Given a set of $N$ data points $(\\lambda_i, f_i)$, we first transform them into $(x_i, y_i) = (1/\\lambda_i, f_i)$. The model predicts that for a given $x_i$, the corresponding $y$ value should be $\\hat{y}_i = v x_i$. The difference between the observed value $y_i$ and the predicted value $\\hat{y}_i$ is the residual, $r_i$:\n$$\nr_i = y_i - \\hat{y}_i = y_i - v x_i\n$$\nThe OLS method finds the value of the parameter $v$ that minimizes the sum of the squares of these residuals, denoted by $S(v)$:\n$$\nS(v) = \\sum_{i=1}^{N} r_i^2 = \\sum_{i=1}^{N} (y_i - v x_i)^2\n$$\nTo find the minimum, we compute the derivative of $S(v)$ with respect to $v$ and set it to zero.\n$$\n\\frac{dS}{dv} = \\frac{d}{dv} \\sum_{i=1}^{N} (y_i - v x_i)^2\n$$\nBy linearity of differentiation, we can move the derivative inside the summation:\n$$\n\\frac{dS}{dv} = \\sum_{i=1}^{N} \\frac{d}{dv} (y_i - v x_i)^2\n$$\nUsing the chain rule, where the inner function is $g(v) = y_i - v x_i$ and the outer function is $h(g) = g^2$:\n$$\n\\frac{d}{dv} (y_i - v x_i)^2 = 2(y_i - v x_i) \\cdot \\frac{d}{dv}(y_i - v x_i) = 2(y_i - v x_i)(-x_i) = -2(y_i x_i - v x_i^2)\n$$\nSubstituting this back into the expression for the derivative of $S(v)$:\n$$\n\\frac{dS}{dv} = \\sum_{i=1}^{N} -2(y_i x_i - v x_i^2) = -2 \\left( \\sum_{i=1}^{N} y_i x_i - v \\sum_{i=1}^{N} x_i^2 \\right)\n$$\nSetting the derivative to zero to find the value of $v$ (which we call the estimate $\\hat{v}$) that minimizes $S(v)$:\n$$\n-2 \\left( \\sum_{i=1}^{N} y_i x_i - \\hat{v} \\sum_{i=1}^{N} x_i^2 \\right) = 0\n$$\n$$\n\\sum_{i=1}^{N} y_i x_i - \\hat{v} \\sum_{i=1}^{N} x_i^2 = 0\n$$\nSolving for $\\hat{v}$:\n$$\n\\hat{v} \\sum_{i=1}^{N} x_i^2 = \\sum_{i=1}^{N} y_i x_i\n$$\n$$\n\\hat{v} = \\frac{\\sum_{i=1}^{N} x_i y_i}{\\sum_{i=1}^{N} x_i^2}\n$$\nThis is the OLS estimator for the slope of a regression line forced through the origin. Note that the second derivative is $\\frac{d^2S}{dv^2} = \\sum_{i=1}^{N} 2x_i^2$, which is positive as long as not all $x_i$ are zero, ensuring that this critical point is indeed a minimum.\n\nSubstituting back the original variables $x_i = 1/\\lambda_i$ and $y_i = f_i$, the formula for the estimated wave speed becomes:\n$$\n\\hat{v} = \\frac{\\sum_{i=1}^{N} (1/\\lambda_i) f_i}{\\sum_{i=1}^{N} (1/\\lambda_i)^2}\n$$\nThis formula is now applied to each of the five provided datasets. For each case, the wavelengths $\\lambda_i$ and frequencies $f_i$ are used to calculate the numerator, $\\sum (f_i/\\lambda_i)$, and the denominator, $\\sum (1/\\lambda_i^2)$, to find the estimate $\\hat{v}$. The calculations will be performed using floating-point arithmetic, and the final results will be rounded to six decimal places as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the wave speed v from noisy (wavelength, frequency) data.\n\n    The method involves linearizing the model f = v / lambda to y = v*x,\n    where y = f and x = 1/lambda. The estimate for v is then found by\n    ordinary least squares for a line forced through the origin, which\n    minimizes the sum of squared residuals. The derived formula is:\n    v_hat = sum(x*y) / sum(x*x).\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A (happy path, moderate wavelengths)\n        (np.array([0.5, 0.8, 1.2, 1.5, 2.0]), \n         np.array([689.1456, 427.713, 286.572, 227.656, 171.7716])),\n        \n        # Case B (large wavelengths, low frequencies)\n        (np.array([2.5, 5.0, 7.5, 10.0, 12.5]), \n         np.array([592.8, 296.9928, 197.4024, 148.6446, 118.32288])),\n        \n        # Case C (very small wavelengths, very high frequencies)\n        (np.array([4.0e-7, 5.0e-7, 6.0e-7, 7.0e-7]), \n         np.array([7.5e14, 5.9994e14, 5.001e14, 4.285071428571429e14])),\n        \n        # Case D (boundary case with two points)\n        (np.array([3.0, 4.0]), \n         np.array([40.4, 29.85])),\n        \n        # Case E (influence of an outlier)\n        (np.array([1.0, 2.0, 3.0, 4.0, 5.0]), \n         np.array([1000.0, 500.0, 366.6666666666667, 250.0, 200.0]))\n    ]\n\n    results = []\n    for wavelengths, frequencies in test_cases:\n        # Transform variables for linearization: x = 1/lambda, y = f\n        x = 1.0 / wavelengths\n        y = frequencies\n\n        # Calculate the terms for the OLS estimator\n        # v_hat = sum(x_i * y_i) / sum(x_i^2)\n        sum_xy = np.sum(x * y)\n        sum_x_squared = np.sum(x**2)\n\n        # Compute the estimated wave speed\n        v_hat = sum_xy / sum_x_squared\n        \n        results.append(v_hat)\n\n    # Format the results to exactly six decimal places\n    formatted_results = [f\"{res:.6f}\" for res in results]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3221525"}, {"introduction": "Exponential processes are fundamental to modeling phenomena like radioactive decay, population dynamics, and heat transfer. This practice [@problem_id:3221524] explores how to analyze data from such a process using Newton's law of cooling. You will see how a logarithmic transformation can \"straighten out\" an exponential curve, allowing you to estimate the cooling constant $k$ by fitting a simple straight line to the transformed data.", "problem": "A cup of coffee cools in a room with a constant ambient temperature due to heat exchange with its environment. The phenomenological law governing this process is Newton's law of cooling, which states that the instantaneous rate of change of the coffee temperature is proportional to the difference between the coffee temperature and the ambient temperature. Mathematically, the governing ordinary differential equation is $\\dfrac{dT}{dt} = -k(T - T_{\\mathrm{env}})$ where $T$ is the coffee temperature, $t$ is time, $T_{\\mathrm{env}}$ is the ambient temperature, and $k > 0$ is the decay constant. Your task is to use data linearization to estimate the decay constant $k$ from experimental measurements.\n\nImplement a complete, runnable program that:\n- Starts from the governing ordinary differential equation $\\dfrac{dT}{dt} = -k(T - T_{\\mathrm{env}})$ as the fundamental base.\n- Derives and applies a linearization strategy appropriate for exponential decay so that the estimation of $k$ reduces to fitting a straight line using Ordinary Least Squares (OLS).\n- Handles the domain restriction of the natural logarithm by excluding any data points where $T - T_{\\mathrm{env}} \\le 0$.\n- Computes the OLS slope from the linearized data and returns the corresponding estimate of $k$ for each test case.\n\nAll time values must be treated in minutes, all temperatures in degrees Celsius, and the final estimates of $k$ must be expressed in inverse minutes (i.e., $\\text{min}^{-1}$). There are no angles or percentages in this task.\n\nUse the following test suite of measured time-temperature data sets, each paired with a known ambient temperature. Each tuple is of the form `(times, temperatures, $T_{\\mathrm{env}}$)` where lists are ordered and time is in minutes:\n- Test case 1 (general case with mild noise): `([0, 2, 4, 6, 8, 10, 12], [85.0, 71.86, 61.48, 52.86, 45.666, 41.063, 36.724], 22.0)`.\n- Test case 2 (edge case including a reading equal to ambient; exclude nonpositive differences): `([0, 5, 10, 15, 20], [90.0, 66.52, 51.65, 41.07, 20.0], 20.0)`.\n- Test case 3 (boundary case with the minimum number of usable points for a line fit): `([0, 7], [80.0, 52.6], 25.0)`.\n- Test case 4 (late-time readings where $T - T_{\\mathrm{env}}$ is small but positive): `([0, 12, 24, 36], [70.0, 49.145, 37.946, 31.399], 24.0)`.\n\nYour program should:\n- For each test case, construct the linearized dataset based on the governing ordinary differential equation and apply Ordinary Least Squares to estimate the slope.\n- Map the estimated slope to the decay constant $k$ in $\\text{min}^{-1}$.\n- Exclude any data point where $T - T_{\\mathrm{env}} \\le 0$ from the linearization; if fewer than $2$ data points remain after exclusion, return a floating-point Not-a-Number (`NaN`) for that test case.\n\nFinal output format specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, `[k_1, k_2, k_3, k_4]`, where each $k_i$ is a floating-point estimate in $\\text{min}^{-1}$ computed from the corresponding test case.\n\nDesign for coverage:\n- Test case 1 validates the general \"happy path\" with multiple points and mild measurement noise.\n- Test case 2 checks correct exclusion of invalid logarithm inputs by including a value equal to ambient temperature.\n- Test case 3 ensures proper behavior with exactly two usable points, which is the minimal requirement for a line fit.\n- Test case 4 probes stability when the temperature difference is small at late times but still positive.\n\nEnsure the program is self-contained, produces the output in the specified format, and requires no user input. All computed $k$ values must be expressed in $\\text{min}^{-1}$.", "solution": "The problem requires the estimation of the decay constant, $k$, in Newton's law of cooling using experimental data. The approach specified is data linearization followed by Ordinary Least Squares (OLS) regression. The solution is developed by first deriving the theoretical model, then formalizing the data linearization procedure, and finally designing an algorithm to compute the estimate of $k$.\n\n**1. Theoretical Model and Linearization**\n\nThe physical process is described by the first-order ordinary differential equation (ODE) for Newton's law of cooling:\n$$\n\\frac{dT}{dt} = -k(T - T_{\\mathrm{env}})\n$$\nwhere $T(t)$ is the temperature of the object at time $t$, $T_{\\mathrm{env}}$ is the constant ambient temperature, and $k > 0$ is the decay constant to be determined.\n\nTo linearize this model, we first solve the ODE. Let $\\Delta T(t) = T(t) - T_{\\mathrm{env}}$. Since $T_{\\mathrm{env}}$ is constant, $\\frac{d(\\Delta T)}{dt} = \\frac{dT}{dt}$. The ODE can be rewritten as:\n$$\n\\frac{d(\\Delta T)}{dt} = -k(\\Delta T)\n$$\nThis is a separable differential equation. We can separate the variables and integrate:\n$$\n\\int \\frac{d(\\Delta T)}{\\Delta T} = \\int -k \\, dt\n$$\n$$\n\\ln|\\Delta T| = -kt + C_1\n$$\nwhere $C_1$ is the constant of integration. Since the object is cooling, $T(t) \\ge T_{\\mathrm{env}}$, so $\\Delta T \\ge 0$. We can drop the absolute value, noting that the logarithm is only defined for $\\Delta T > 0$. Exponentiating both sides gives:\n$$\n\\Delta T = e^{-kt + C_1} = e^{C_1}e^{-kt}\n$$\nLet the constant $A = e^{C_1}$. The solution is $\\Delta T(t) = A e^{-kt}$. If we consider the initial condition $T(0) = T_0$, then $\\Delta T(0) = T_0 - T_{\\mathrm{env}} = A e^0 = A$. So, the specific solution is:\n$$\nT(t) - T_{\\mathrm{env}} = (T_0 - T_{\\mathrm{env}}) e^{-kt}\n$$\nTo linearize this exponential relationship, we take the natural logarithm of both sides. This step is only valid for data points where $T(t) > T_{\\mathrm{env}}$, or $T(t) - T_{\\mathrm{env}} > 0$.\n$$\n\\ln(T(t) - T_{\\mathrm{env}}) = \\ln\\left((T_0 - T_{\\mathrm{env}}) e^{-kt}\\right)\n$$\nUsing the properties of logarithms, $\\ln(ab) = \\ln(a) + \\ln(b)$ and $\\ln(e^x) = x$, we get:\n$$\n\\ln(T(t) - T_{\\mathrm{env}}) = \\ln(T_0 - T_{\\mathrm{env}}) - kt\n$$\nThis equation is now in the form of a straight line, $Y = mX + c$:\n- Let the dependent variable be $Y = \\ln(T - T_{\\mathrm{env}})$.\n- Let the independent variable be $X = t$.\n- The slope of the line is $m = -k$.\n- The y-intercept is $c = \\ln(T_0 - T_{\\mathrm{env}})$.\n\nThe crucial insight is that the decay constant $k$ can be estimated from the slope of a linear fit to the transformed data: $k = -m$.\n\n**2. Algorithmic Design for Parameter Estimation**\n\nGiven a set of $N$ experimental data points $(t_i, T_i)$ and a known $T_{\\mathrm{env}}$, we can estimate $k$ by following these steps:\n\n**Step 2.1: Data Transformation and Filtering**\nFirst, we transform the raw data into the linearized coordinate system $(X_i, Y_i)$.\n1.  For each measured pair $(t_i, T_i)$, calculate the temperature difference $\\Delta T_i = T_i - T_{\\mathrm{env}}$.\n2.  The problem mandates handling the domain of the natural logarithm. We must filter the dataset to include only points where $\\Delta T_i > 0$. Any point with $\\Delta T_i \\le 0$ is discarded.\n3.  A linear fit requires at least two points. If, after filtering, fewer than two data points remain, it is impossible to estimate a slope. In this case, the problem is ill-defined for the given dataset, and the result should be Not-a-Number (NaN).\n4.  For the remaining valid points, construct the linearized dataset:\n    $$\n    X_i = t_i\n    $$\n    $$\n    Y_i = \\ln(\\Delta T_i) = \\ln(T_i - T_{\\mathrm{env}})\n    $$\n\n**Step 2.2: Ordinary Least Squares (OLS) Regression**\nWe apply OLS to the filtered, transformed data points $(X_i, Y_i)$ to find the slope $m$ of the best-fit line. The formula for the OLS slope that minimizes the sum of squared residuals is:\n$$\nm = \\frac{\\sum_{i=1}^{N'} (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^{N'} (X_i - \\bar{X})^2}\n$$\nwhere $N'$ is the number of valid data points after filtering, and $\\bar{X}$ and $\\bar{Y}$ are the sample means of the $X_i$ and $Y_i$ values, respectively.\n\n**Step 2.3: Parameter Recovery**\nFinally, we recover the estimate for the decay constant $k$ from the estimated slope $m$:\n$$\nk = -m\n$$\nSince the temperature $T$ decreases over time, the transformed variable $Y = \\ln(T - T_{\\mathrm{env}})$ will also decrease with time $X=t$. Therefore, the slope $m$ will be negative, ensuring that the estimated $k = -m$ is positive, consistent with its physical definition as a decay constant.\n\nThis complete procedure provides a robust method for estimating $k$ from experimental data, grounded in the fundamental physics of the system and standard numerical techniques.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef estimate_k_from_data(times, temperatures, T_env):\n    \"\"\"\n    Estimates the decay constant k from time-temperature data using linearization.\n\n    The function linearizes the solution to Newton's law of cooling,\n    T(t) - T_env = (T0 - T_env) * exp(-k*t), by taking the natural log:\n    ln(T - T_env) = -k*t + ln(T0 - T_env).\n    This is a linear equation y = m*x + c, where y = ln(T - T_env), \n    x = t, and the slope m = -k.\n\n    Args:\n        times (list or np.ndarray): A list of time points in minutes.\n        temperatures (list or np.ndarray): A list of temperature readings in Celsius.\n        T_env (float): The constant ambient temperature in Celsius.\n\n    Returns:\n        float: The estimated decay constant k in min^-1, or np.nan if a fit is not possible.\n    \"\"\"\n    # Convert inputs to numpy arrays for vectorized operations\n    times = np.array(times, dtype=float)\n    temperatures = np.array(temperatures, dtype=float)\n\n    # Calculate the temperature difference\n    delta_T = temperatures - T_env\n\n    # Filter out data points where delta_T is not positive, as ln(x) is defined for x > 0.\n    # The problem specifies T - T_env = 0 should be excluded.\n    valid_indices = np.where(delta_T > 0)\n    \n    filtered_times = times[valid_indices]\n    filtered_delta_T = delta_T[valid_indices]\n\n    # A line fit requires at least 2 points.\n    if len(filtered_times)  2:\n        return np.nan\n\n    # Create the variables for the linear regression\n    # X = t\n    # Y = ln(T - T_env)\n    x_data = filtered_times\n    y_data = np.log(filtered_delta_T)\n\n    # Perform Ordinary Least Squares (OLS) to find the slope m.\n    # m = Cov(x, y) / Var(x)\n    # Using the direct summation formula for clarity:\n    # m = sum((x - x_bar)(y - y_bar)) / sum((x - x_bar)^2)\n    x_mean = np.mean(x_data)\n    y_mean = np.mean(y_data)\n    \n    numerator = np.sum((x_data - x_mean) * (y_data - y_mean))\n    denominator = np.sum((x_data - x_mean)**2)\n\n    # Avoid division by zero, although not expected if len(x_data) >= 2 and times are not identical\n    if denominator == 0:\n        return np.nan\n\n    slope_m = numerator / denominator\n\n    # The decay constant k is the negative of the slope\n    k = -slope_m\n    \n    return k\n\ndef solve():\n    \"\"\"\n    Runs the estimation for all test cases and prints the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple: (times, temperatures, T_env)\n    test_cases = [\n        # Test case 1 (general case with mild noise)\n        ([0, 2, 4, 6, 8, 10, 12], [85.0, 71.86, 61.48, 52.86, 45.666, 41.063, 36.724], 22.0),\n        # Test case 2 (edge case with T = T_env)\n        ([0, 5, 10, 15, 20], [90.0, 66.52, 51.65, 41.07, 20.0], 20.0),\n        # Test case 3 (boundary case with minimum points)\n        ([0, 7], [80.0, 52.6], 25.0),\n        # Test case 4 (late-time readings with small but positive delta_T)\n        ([0, 12, 24, 36], [70.0, 49.145, 37.946, 31.399], 24.0)\n    ]\n\n    results = []\n    for case in test_cases:\n        times, temperatures, T_env = case\n        k_estimate = estimate_k_from_data(times, temperatures, T_env)\n        results.append(k_estimate)\n\n    # Format output as a comma-separated list in brackets\n    # Use a custom formatter to handle floating point representation\n    formatted_results = [f'{r:.7f}' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3221524"}, {"introduction": "While linearization is a powerful tool, its application is not always straightforward or optimal. This exercise [@problem_id:3221622] provides a crucial lesson in understanding the assumptions behind your models, particularly the structure of experimental noise. You will compare a direct cubic fit to a log-linearized fit for a power-law model with additive noise, allowing you to discover for yourself when and why a seemingly convenient transformation might lead to a less accurate result.", "problem": "You are given a synthetic data generation process defined by the equation $y = a x^3 + \\epsilon$, where $a$ is a positive scalar parameter, $x$ is a positive input, and $\\epsilon$ is additive noise drawn independently for each sample from a normal distribution with mean $0$ and variance $\\sigma^2$. The goal is to estimate the parameter $a$ using two different approaches: a direct fit that respects the original model and a linearized fit based on a logarithmic transformation. From first principles, use the least squares criterion to design estimators for both approaches, and compare their accuracy in estimating $a$.\n\nFundamental base:\n- The data are generated by the model $y_i = a x_i^3 + \\epsilon_i$ with $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ independently.\n- Estimation is based on minimizing the sum of squared residuals (Ordinary Least Squares (OLS)) under each modeling approach.\n\nApproach 1 (direct cubic fit):\n- Treat the regression as $y_i = b z_i + \\eta_i$ with $z_i = x_i^3$ and enforce the intercept to be $0$. Use OLS to obtain an estimator $\\hat{a}_{\\text{direct}}$ by minimizing $\\sum_i (y_i - b z_i)^2$ with respect to $b$.\n\nApproach 2 (log-linearization):\n- Use the natural logarithm to transform the data as $u_i = \\log(x_i)$ and $v_i = \\log(y_i)$, and fit the linear model $v_i = \\beta_0 + \\beta_1 u_i + \\xi_i$ using OLS with a free intercept. The estimate of $a$ is then $\\hat{a}_{\\log} = \\exp(\\hat{\\beta}_0)$.\n- Since $\\log(y_i)$ is only defined for $y_i  0$, discard any samples with $y_i \\le 0$ from the log-linearized fit. Discarding must be applied only to the log-linearized approach; the direct approach must use all generated samples.\n- Use the natural logarithm $\\log(\\cdot)$.\n\nTie-breaking rule:\n- Define the absolute estimation error for each method as $e_{\\text{direct}} = |\\hat{a}_{\\text{direct}} - a|$ and $e_{\\log} = |\\hat{a}_{\\log} - a|$.\n- Declare the log-linearized approach “better” for a test case if and only if $e_{\\log}  e_{\\text{direct}}$. If $e_{\\log} = e_{\\text{direct}}$ or $e_{\\log}  e_{\\text{direct}}$, declare it “worse”.\n\nData generation details for the test suite:\n- For each test case, draw $n$ independent samples $x_i$ uniformly from $[x_{\\min}, x_{\\max}]$, draw $\\epsilon_i$ independently from $\\mathcal{N}(0,\\sigma^2)$, and compute $y_i = a x_i^3 + \\epsilon_i$.\n- Use the specified pseudo-random number generator seed to make the results reproducible.\n- All $x_i$ are strictly positive by construction.\n\nTest suite (each tuple is $(a,\\sigma,n,x_{\\min},x_{\\max},\\text{seed})$):\n- Case $1$: $(2.0, 0.1, 100, 1.0, 5.0, 1)$\n- Case $2$: $(0.2, 0.5, 100, 0.1, 5.0, 2)$\n- Case $3$: $(5.0, 5.0, 100, 1.0, 2.0, 42)$\n- Case $4$ (boundary, no noise): $(3.0, 0.0, 50, 0.5, 3.0, 3)$\n- Case $5$ (edge, many small $x$ with large noise): $(0.1, 1.0, 100, 0.01, 1.0, 4)$\n\nRequired program behavior:\n- Implement both estimators precisely as described.\n- For the direct fit, compute $\\hat{a}_{\\text{direct}}$ by OLS with zero intercept on $z_i = x_i^3$.\n- For the log-linearized fit, compute $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ by OLS with free intercept on the filtered set with $y_i  0$, and set $\\hat{a}_{\\log} = \\exp(\\hat{\\beta}_0)$. If fewer than $2$ samples remain after filtering (which would make the OLS estimation ill-posed), treat the log-linearized approach as “worse” by setting $e_{\\log}$ to a value considered larger than $e_{\\text{direct}}$.\n- For each test case, output a boolean indicating whether the log-linearized approach is better according to the strict inequality $e_{\\log}  e_{\\text{direct}}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite cases, for example $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5]$, where each $\\text{result}_k$ is a boolean.", "solution": "We begin from the least squares principle, which states that for a linear model the Ordinary Least Squares (OLS) estimator minimizes the sum of squared residuals. This principle yields closed-form estimators under linear relationships. We apply it separately to the direct cubic fit and the log-linearized fit.\n\nDirect cubic fit derivation:\nConsider the regression $y_i = b z_i + \\eta_i$ with $z_i = x_i^3$ and intercept fixed at $0$. The OLS estimator $\\hat{b}$ minimizes\n$$\nS(b) = \\sum_{i=1}^n (y_i - b z_i)^2.\n$$\nDifferentiating with respect to $b$ and setting the derivative to zero yields\n$$\n\\frac{dS}{db} = -2 \\sum_{i=1}^n z_i (y_i - b z_i) = 0 \\quad \\Rightarrow \\quad \\sum_{i=1}^n z_i y_i = b \\sum_{i=1}^n z_i^2,\n$$\nwhich gives\n$$\n\\hat{b} = \\frac{\\sum_{i=1}^n z_i y_i}{\\sum_{i=1}^n z_i^2}.\n$$\nSince the model is $y_i = a x_i^3 + \\epsilon_i$, and $z_i = x_i^3$, the parameter of interest is $a$. Therefore the estimator from this direct fit is\n$$\n\\hat{a}_{\\text{direct}} = \\frac{\\sum_{i=1}^n x_i^3 y_i}{\\sum_{i=1}^n x_i^6}.\n$$\nThis is the OLS solution under the correct additive noise model with zero intercept.\n\nLog-linearization derivation:\nDefine $u_i = \\log(x_i)$ and $v_i = \\log(y_i)$. Under the transformation (for $y_i  0$),\n$$\nv_i = \\log(a x_i^3 + \\epsilon_i) \\approx \\log(a x_i^3) + \\text{transformed noise},\n$$\nand if the transformed relationship were exactly linear, we would have\n$$\nv_i = \\beta_0 + \\beta_1 u_i + \\xi_i, \\quad \\text{with} \\quad \\beta_0 = \\log(a), \\quad \\beta_1 = 3.\n$$\nWe fit the linear model $v_i = \\beta_0 + \\beta_1 u_i + \\xi_i$ by OLS with an intercept. The OLS normal equations yield\n$$\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^m (u_i - \\bar{u})(v_i - \\bar{v})}{\\sum_{i=1}^m (u_i - \\bar{u})^2}, \\quad \\hat{\\beta}_0 = \\bar{v} - \\hat{\\beta}_1 \\bar{u},\n$$\nwhere $m$ is the number of retained samples with $y_i  0$, $\\bar{u} = \\frac{1}{m}\\sum_{i=1}^m u_i$, and $\\bar{v} = \\frac{1}{m}\\sum_{i=1}^m v_i$. The estimate of $a$ from the log-linearization is\n$$\n\\hat{a}_{\\log} = \\exp(\\hat{\\beta}_0).\n$$\n\nBias under additive noise and log transform:\nWhen the original noise is additive, the logarithm introduces a nonlinear transformation of the noise. A second-order Taylor expansion of the logarithm around $a x_i^3$ gives, for $y_i = a x_i^3 + \\epsilon_i$ with small $\\epsilon_i$,\n$$\n\\log(y_i) = \\log(a x_i^3 + \\epsilon_i) \\approx \\log(a x_i^3) + \\frac{\\epsilon_i}{a x_i^3} - \\frac{\\epsilon_i^2}{2 (a x_i^3)^2} + \\cdots.\n$$\nTaking expectations, using $\\mathbb{E}[\\epsilon_i] = 0$ and $\\mathbb{E}[\\epsilon_i^2] = \\sigma^2$, we obtain\n$$\n\\mathbb{E}[\\log(y_i)] \\approx \\log(a x_i^3) - \\frac{\\sigma^2}{2 (a x_i^3)^2}.\n$$\nThis shows a negative bias that is stronger for small $x_i$ and large $\\sigma$. Hence, under additive noise, the log-linearized approach is generally misspecified and can be biased, especially when $a x_i^3$ is small relative to $\\sigma$. Conversely, the direct cubic fit aligns with the true additive noise structure and yields an unbiased OLS estimator of $a$ under classical assumptions.\n\nAlgorithmic design:\n- For each test case, generate $n$ samples $x_i \\sim \\text{Uniform}(x_{\\min}, x_{\\max})$ and $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ using the given seed. Compute $y_i = a x_i^3 + \\epsilon_i$.\n- Compute $\\hat{a}_{\\text{direct}}$ by the formula $\\hat{a}_{\\text{direct}} = \\frac{\\sum x_i^3 y_i}{\\sum x_i^6}$ using all samples.\n- For the log-linearized approach, filter to indices with $y_i  0$ and compute $u_i = \\log(x_i)$ and $v_i = \\log(y_i)$. If fewer than $2$ samples remain, mark the log-linearized approach as “worse” for this case. Otherwise, compute $\\hat{\\beta}_1$ and $\\hat{\\beta}_0$ via OLS with intercept, and set $\\hat{a}_{\\log} = \\exp(\\hat{\\beta}_0)$.\n- Compute absolute errors $e_{\\text{direct}} = |\\hat{a}_{\\text{direct}} - a|$ and $e_{\\log} = |\\hat{a}_{\\log} - a|$.\n- Output a boolean for each test case indicating whether $e_{\\log}  e_{\\text{direct}}$ holds.\n\nEdge cases in the provided suite:\n- Case $4$ has $\\sigma = 0$, yielding a perfect linear relationship on the log scale and correct alignment on the original scale. Both methods should recover $a$ exactly in ideal arithmetic, so the comparison uses strict inequality and will not mark the log-linearized as “better” unless numerical rounding yields a strictly smaller error.\n- Cases with small $a$ and large $\\sigma$ may produce $y_i \\le 0$ for some samples, causing the log-linearized fit to discard data and potentially degrade estimation. This is handled explicitly.\n\nThe final program implements this logic and produces a single line in the specified format: a list of five booleans corresponding to the five test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef estimate_a_direct(x, y):\n    \"\"\"\n    Estimate 'a' in y = a*x^3 + epsilon via OLS through the origin:\n    a_hat = sum(x^3 * y) / sum(x^6)\n    \"\"\"\n    z = x**3\n    denom = np.dot(z, z)\n    if denom == 0.0:\n        return np.nan\n    a_hat = np.dot(z, y) / denom\n    return a_hat\n\ndef estimate_a_log(x, y):\n    \"\"\"\n    Estimate 'a' via log-linearization:\n    Fit log(y) = beta0 + beta1 * log(x) on samples with y>0\n    Return a_hat = exp(beta0).\n    If fewer than 2 samples remain or degenerate variance, return np.nan.\n    \"\"\"\n    mask = (y > 0)  (x > 0)\n    if np.count_nonzero(mask)  2:\n        return np.nan\n    u = np.log(x[mask])\n    v = np.log(y[mask])\n    u_mean = u.mean()\n    v_mean = v.mean()\n    du = u - u_mean\n    dv = v - v_mean\n    denom = np.sum(du**2)\n    if denom == 0.0:\n        return np.nan\n    beta1 = np.sum(du * dv) / denom\n    beta0 = v_mean - beta1 * u_mean\n    a_hat = np.exp(beta0)\n    return a_hat\n\ndef run_case(a, sigma, n, xmin, xmax, seed):\n    rng = np.random.default_rng(seed)\n    x = rng.uniform(xmin, xmax, size=n)\n    eps = rng.normal(loc=0.0, scale=sigma, size=n)\n    y = a * x**3 + eps\n\n    a_hat_direct = estimate_a_direct(x, y)\n    a_hat_log = estimate_a_log(x, y)\n\n    err_direct = abs(a_hat_direct - a) if np.isfinite(a_hat_direct) else float('inf')\n    err_log = abs(a_hat_log - a) if np.isfinite(a_hat_log) else float('inf')\n\n    return err_log  err_direct\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is (a, sigma, n, xmin, xmax, seed)\n    test_cases = [\n        (2.0, 0.1, 100, 1.0, 5.0, 1),\n        (0.2, 0.5, 100, 0.1, 5.0, 2),\n        (5.0, 5.0, 100, 1.0, 2.0, 42),\n        (3.0, 0.0, 50, 0.5, 3.0, 3),\n        (0.1, 1.0, 100, 0.01, 1.0, 4),\n    ]\n\n    results = []\n    for case in test_cases:\n        a, sigma, n, xmin, xmax, seed = case\n        better = run_case(a, sigma, n, xmin, xmax, seed)\n        results.append(better)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3221622"}]}