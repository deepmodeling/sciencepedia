{"hands_on_practices": [{"introduction": "To truly understand Singular Value Decomposition, we must start with its fundamental definition as a matrix product, $A = U\\Sigma V^T$. This first exercise is a direct application of this formula, tasking you with reconstructing a matrix from its constituent parts: two rotation matrices ($U$ and $V$) and a scaling matrix ($\\Sigma$). By manually computing the product, you will gain a concrete understanding of how these components interact to define the linear transformation represented by the matrix $A$ [@problem_id:16521].", "problem": "The Singular Value Decomposition (SVD) of a real $2 \\times 2$ matrix $A$ provides a factorization of the form $A = U\\Sigma V^T$, where $U$ and $V$ are $2 \\times 2$ orthogonal matrices and $\\Sigma$ is a $2 \\times 2$ diagonal matrix.\n\nConsider a matrix $A$ whose SVD components are given as follows:\n- $U$ is a rotation matrix defined by an angle $\\theta$:\n$$\nU = \\begin{pmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{pmatrix}\n$$\n- $\\Sigma$ is a diagonal matrix containing the singular values $\\sigma_1$ and $\\sigma_2$:\n$$\n\\Sigma = \\begin{pmatrix} \\sigma_1 & 0 \\\\ 0 & \\sigma_2 \\end{pmatrix}\n$$\n- $V$ is a rotation matrix defined by an angle $\\phi$:\n$$\nV = \\begin{pmatrix} \\cos\\phi & -\\sin\\phi \\\\ \\sin\\phi & \\cos\\phi \\end{pmatrix}\n$$\n\nDerive an expression for the element $A_{21}$ (the entry in the second row and first column) of the matrix $A$ in terms of the parameters $\\theta$, $\\phi$, $\\sigma_1$, and $\\sigma_2$.", "solution": "We have \n$$A \\;=\\; U\\,\\Sigma\\,V^T.$$\nFirst compute \n$$V^T=\\begin{pmatrix}\\cos\\phi&\\sin\\phi\\\\-\\sin\\phi&\\cos\\phi\\end{pmatrix}.$$\nThen \n$$\\Sigma\\,V^T\n=\\begin{pmatrix}\\sigma_1&0\\\\0&\\sigma_2\\end{pmatrix}\n\\begin{pmatrix}\\cos\\phi&\\sin\\phi\\\\-\\sin\\phi&\\cos\\phi\\end{pmatrix}\n=\\begin{pmatrix}\\sigma_1\\cos\\phi&\\sigma_1\\sin\\phi\\\\-\\,\\sigma_2\\sin\\phi&\\sigma_2\\cos\\phi\\end{pmatrix}.$$\nNext multiply by \n$$U=\\begin{pmatrix}\\cos\\theta&-\\sin\\theta\\\\\\sin\\theta&\\cos\\theta\\end{pmatrix}$$\nto get\n$$A=U(\\Sigma V^T)\n=\\begin{pmatrix}\\cos\\theta&-\\sin\\theta\\\\\\sin\\theta&\\cos\\theta\\end{pmatrix}\n\\begin{pmatrix}\\sigma_1\\cos\\phi&\\sigma_1\\sin\\phi\\\\-\\,\\sigma_2\\sin\\phi&\\sigma_2\\cos\\phi\\end{pmatrix}.$$\nThe $(2,1)$â€“entry is the dot product of the second row of $U$ with the first column of $\\Sigma V^T$:\n$$A_{21}\n=\\bigl(\\sin\\theta,\\;\\cos\\theta\\bigr)\\cdot\\begin{pmatrix}\\sigma_1\\cos\\phi\\\\-\\,\\sigma_2\\sin\\phi\\end{pmatrix}\n=\\sigma_1\\sin\\theta\\cos\\phi-\\sigma_2\\cos\\theta\\sin\\phi.$$", "answer": "$$\\boxed{\\sigma_1\\sin\\theta\\cos\\phi-\\sigma_2\\cos\\theta\\sin\\phi}$$", "id": "16521"}, {"introduction": "Beyond simple reconstruction, the true power of SVD lies in its elegant algebraic properties. This practice moves from numerical computation to a more abstract derivation, asking you to find the SVD of a matrix's inverse, $A^{-1}$, using only the SVD of the original matrix $A$. This exercise will deepen your appreciation for the properties of orthogonal matrices and reveal a beautifully symmetric relationship between the singular values of $A$ and $A^{-1}$ [@problem_id:3205922].", "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be an invertible matrix with a singular value decomposition (SVD) $A = U \\Sigma V^{T}$, where $U \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices and $\\Sigma \\in \\mathbb{R}^{n \\times n}$ is diagonal with strictly positive diagonal entries $\\sigma_{1}, \\dots, \\sigma_{n}$. Using only the core definitions and facts that characterize the singular value decomposition (namely, the definition of orthogonal matrices, the product inverse rule, and the property that the inverse of an orthogonal matrix is its transpose), derive the singular value decomposition of the inverse matrix $A^{-1}$. Your final answer should be a single closed-form analytic expression for the decomposition of $A^{-1}$ written as a product of an orthogonal matrix, a diagonal matrix with nonnegative entries, and the transpose of an orthogonal matrix. If you wish to enforce a particular ordering of singular values, you may assume the standard convention that $\\sigma_{1} \\geq \\cdots \\geq \\sigma_{n} > 0$, but your final expression must remain in a single compact form. No numerical rounding or units are required.", "solution": "The problem requires the derivation of the singular value decomposition (SVD) for the inverse of an invertible matrix $A$, given the SVD of $A$ itself. The derivation must rely solely on fundamental definitions and properties of matrix operations.\n\nFirst, we perform a validation of the problem statement.\n\n**Step 1: Extract Givens**\n-   $A \\in \\mathbb{R}^{n \\times n}$ is an invertible matrix.\n-   The SVD of $A$ is given by $A = U \\Sigma V^{T}$.\n-   $U \\in \\mathbb{R}^{n \\times n}$ is an orthogonal matrix.\n-   $V \\in \\mathbb{R}^{n \\times n}$ is an orthogonal matrix.\n-   $\\Sigma \\in \\mathbb{R}^{n \\times n}$ is a diagonal matrix with diagonal entries $\\sigma_{1}, \\dots, \\sigma_{n}$.\n-   The diagonal entries are strictly positive: $\\sigma_{i} > 0$ for $i = 1, \\dots, n$.\n-   Constraints on the derivation: use only core definitions, including the definition of orthogonal matrices ($Q^T Q = Q Q^T = I$), the product inverse rule ($(XY)^{-1} = Y^{-1}X^{-1}$), and the property that the inverse of an orthogonal matrix is its transpose ($Q^{-1} = Q^T$).\n-   The requested output is a single closed-form expression for the SVD of $A^{-1}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is mathematically well-defined and internally consistent. The premise that $A$ is invertible is consistent with the condition that all its singular values $\\sigma_i$ are strictly positive. A square matrix is invertible if and only if it has no zero singular values. The problem is grounded in established principles of linear algebra and does not violate any scientific or logical rules. It is well-posed, objective, and complete. All necessary information is provided.\n\n**Verdict:** The problem is valid. We may proceed with the solution.\n\nThe objective is to find the SVD of $A^{-1}$. We begin with the given SVD of $A$:\n$$A = U \\Sigma V^{T}$$\nSince $A$ is invertible, its inverse $A^{-1}$ exists. We can find an expression for $A^{-1}$ by taking the inverse of the SVD expression for $A$:\n$$A^{-1} = (U \\Sigma V^{T})^{-1}$$\nAccording to the rule for the inverse of a product of matrices, which states that for invertible matrices $X$, $Y$, and $Z$, the inverse of their product is $(XYZ)^{-1} = Z^{-1}Y^{-1}X^{-1}$, we can reverse the order of the terms:\n$$A^{-1} = (V^{T})^{-1} \\Sigma^{-1} U^{-1}$$\nNow, we must simplify the terms $(V^{T})^{-1}$ and $U^{-1}$. The problem states that $U$ and $V$ are orthogonal matrices. A defining property of an orthogonal matrix $Q$ is that its inverse is equal to its transpose, i.e., $Q^{-1} = Q^{T}$.\n\nApplying this property to the matrix $U$, we directly obtain:\n$$U^{-1} = U^{T}$$\nTo simplify $(V^{T})^{-1}$, we first note that if $V$ is an orthogonal matrix, then its transpose $V^{T}$ is also an orthogonal matrix. This is because $(V^{T})^{T}(V^{T}) = V V^{T} = I$ and $(V^{T})(V^{T})^{T} = V^{T}V = I$. Therefore, the inverse of $V^{T}$ is its transpose:\n$$(V^{T})^{-1} = (V^{T})^{T} = V$$\nNext, we consider the matrix $\\Sigma^{-1}$. The matrix $\\Sigma$ is a diagonal matrix with strictly positive diagonal entries $\\sigma_1, \\dots, \\sigma_n$:\n$$\\Sigma = \\begin{pmatrix} \\sigma_1 & 0 & \\cdots & 0 \\\\ 0 & \\sigma_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\sigma_n \\end{pmatrix}$$\nSince all $\\sigma_i > 0$, $\\Sigma$ is invertible. Its inverse, $\\Sigma^{-1}$, is also a diagonal matrix whose diagonal entries are the reciprocals of the original entries:\n$$\\Sigma^{-1} = \\begin{pmatrix} 1/\\sigma_1 & 0 & \\cdots & 0 \\\\ 0 & 1/\\sigma_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 1/\\sigma_n \\end{pmatrix}$$\nSubstituting the simplified expressions for $U^{-1}$, $(V^{T})^{-1}$, and $\\Sigma^{-1}$ back into the equation for $A^{-1}$, we get:\n$$A^{-1} = V \\Sigma^{-1} U^{T}$$\nWe must verify that this expression is a valid SVD. A singular value decomposition of a matrix $M$ has the form $P \\Delta Q^{T}$, where $P$ and $Q$ are orthogonal matrices and $\\Delta$ is a diagonal matrix with non-negative entries on the diagonal.\nLet's examine our result $A^{-1} = V \\Sigma^{-1} U^{T}$:\n1.  The first matrix is $V$. As given in the problem statement, $V$ is an orthogonal matrix.\n2.  The second matrix is $\\Sigma^{-1}$. As shown above, $\\Sigma^{-1}$ is a diagonal matrix. Its diagonal entries are $1/\\sigma_i$. Since we are given that $\\sigma_i > 0$ for all $i$, it follows that $1/\\sigma_i > 0$. Thus, $\\Sigma^{-1}$ is a diagonal matrix with non-negative (in fact, strictly positive) entries.\n3.  The third matrix is $U^{T}$. We are given that $U$ is an orthogonal matrix. The expression requires the transpose of an orthogonal matrix, which is precisely $U^T$.\n\nTherefore, the expression $V \\Sigma^{-1} U^{T}$ satisfies all the requirements of a singular value decomposition for the matrix $A^{-1}$. The singular values of $A^{-1}$ are the reciprocals of the singular values of $A$. If the singular values of $A$ are ordered as $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_n > 0$, then the singular values of $A^{-1}$ are $1/\\sigma_1 \\le 1/\\sigma_2 \\le \\dots \\le 1/\\sigma_n$. While standard convention often lists singular values in non-increasing order, the expression $V \\Sigma^{-1} U^{T}$ is a structurally valid SVD regardless of the ordering of the diagonal elements of $\\Sigma^{-1}$. The problem asks for a single compact expression, which this is.", "answer": "$$\\boxed{V \\Sigma^{-1} U^{T}}$$", "id": "3205922"}, {"introduction": "We now transition from theory to a powerful real-world application: digital image compression. This hands-on coding problem demonstrates how truncated SVD can create low-rank approximations of an image, drastically reducing storage requirements while preserving essential visual information. By implementing the compression algorithm on a synthetic image and quantifying the trade-off between reconstruction error and compression ratio, you will see firsthand why SVD is a cornerstone of modern data science and scientific computing [@problem_id:3206004].", "problem": "You are given the task of investigating the compression of a color image using singular value decomposition (SVD), applied separately to each of the red (R), green (G), and blue (B) channels. The goal is to quantify how truncated SVD approximations affect reconstruction quality and storage, starting from the formal definition of singular value decomposition (SVD) and the properties of matrix norms.\n\nConstruct a synthetic color image of height $H$ and width $W$ by defining each channel as a sum of outer products of deterministic one-dimensional functions over the normalized spatial grids $x \\in [0,1]$ (rows) and $y \\in [0,1]$ (columns). Specifically, for any given pair $(H,W)$, let $x$ be the $H$-vector $x_i = \\frac{i}{H-1}$ for $i = 0,1,\\dots,H-1$, and let $y$ be the $W$-vector $y_j = \\frac{j}{W-1}$ for $j = 0,1,\\dots,W-1$. Define the three channels by the following outer-product constructions:\n- Red channel $R \\in \\mathbb{R}^{H \\times W}$:\n$$\nR = 0.5 \\cdot (x \\, (1-y)^\\top) \\;+\\; 0.35 \\cdot (\\sin(2\\pi x) \\, \\cos(2\\pi y)^\\top) \\;+\\; 0.15 \\cdot \\left(\\exp\\!\\left(-\\frac{(x-0.3)^2}{0.05}\\right) \\, (y^{1/2})^\\top\\right).\n$$\n- Green channel $G \\in \\mathbb{R}^{H \\times W}$:\n$$\nG = 0.4 \\cdot (\\sin(\\pi x) \\, \\sin(\\pi y)^\\top) \\;+\\; 0.4 \\cdot (\\mathbf{1}_H \\, \\exp\\!\\left(-\\frac{(y-0.5)^2}{0.08}\\right)^\\top) \\;+\\; 0.2 \\cdot (\\cos(3\\pi x) \\, y^\\top),\n$$\nwhere $\\mathbf{1}_H$ denotes the $H$-vector of all ones.\n- Blue channel $B \\in \\mathbb{R}^{H \\times W}$:\n$$\nB = 0.45 \\cdot (\\cos(2\\pi x) \\, (y^2)^\\top) \\;+\\; 0.35 \\cdot (x^{1/2} \\, (1-y)^{0.3}^\\top) \\;+\\; 0.2 \\cdot (\\sin(4\\pi x) \\, \\cos(2\\pi y)^\\top).\n$$\n\nAfter constructing each channel, rescale it linearly to the interval $[0,1]$ by\n$$\n\\widetilde{C} = \\frac{C - \\min(C)}{\\max(C) - \\min(C) + 10^{-12}},\n$$\napplied elementwise to $C \\in \\{R,G,B\\}$, with the convention that if $\\max(C) = \\min(C)$ then $\\widetilde{C}$ is set to the zero matrix of the same size. This ensures the synthetic image intensities are well-defined and bounded.\n\nFor a given nonnegative integer $k$, perform truncated singular value decomposition (SVD) on each channel $\\widetilde{C}$ separately and reconstruct its best rank-$k$ approximation $\\widetilde{C}_k$. If $k > \\min(H,W)$, treat $k$ as $k = \\min(H,W)$. Use the Frobenius norm to define the relative reconstruction error across the three channels:\n$$\ne(H,W,k) \\;=\\; \\frac{\\sqrt{\\lVert \\widetilde{R} - \\widetilde{R}_k \\rVert_F^2 + \\lVert \\widetilde{G} - \\widetilde{G}_k \\rVert_F^2 + \\lVert \\widetilde{B} - \\widetilde{B}_k \\rVert_F^2}}{\\sqrt{\\lVert \\widetilde{R} \\rVert_F^2 + \\lVert \\widetilde{G} \\rVert_F^2 + \\lVert \\widetilde{B} \\rVert_F^2}}.\n$$\nAlso compute the per-channel storage compression ratio for truncated SVD (assuming storage of the truncated factors for each channel), defined as\n$$\nr(H,W,k) \\;=\\; \\frac{Hk + Wk + k}{HW} \\;=\\; \\frac{(H+W+1)k}{HW},\n$$\nand use the same $r(H,W,k)$ for all three channels, since the dimensions are identical.\n\nImplement a program that:\n- Constructs the synthetic image $(\\widetilde{R}, \\widetilde{G}, \\widetilde{B})$ for each test case.\n- Computes the truncated SVD reconstructions $(\\widetilde{R}_k, \\widetilde{G}_k, \\widetilde{B}_k)$ with the given $k$.\n- Outputs, for each test case, the pair $[e(H,W,k), r(H,W,k)]$ as floats rounded to $6$ decimal places.\n\nTest Suite:\nUse the following test cases to evaluate different regimes:\n- Case $1$: $(H,W,k) = (48,64,5)$, a general moderate-rank approximation.\n- Case $2$: $(H,W,k) = (48,64,0)$, the boundary case of rank-$0$ (all-zero reconstruction).\n- Case $3$: $(H,W,k) = (48,64,48)$, the full-rank case $k=\\min(H,W)$ for a tall matrix per channel.\n- Case $4$: $(H,W,k) = (32,20,3)$, a rectangular image with a small rank.\n- Case $5$: $(H,W,k) = (2,2,1)$, an extreme small image with rank-$1$ approximation.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a two-element list $[e,r]$ for the corresponding test case, with both values rounded to $6$ decimal places, for example $[[e_1,r_1],[e_2,r_2],\\dots]$.", "solution": "The user's request is a valid and well-posed problem in numerical linear algebra, specifically concerning the application of Singular Value Decomposition (SVD) for image compression. All provided formulas and parameters are scientifically sound and mathematically consistent. The problem will be solved by implementing the specified procedure.\n\nThe core of the solution lies in the Singular Value Decomposition (SVD) of a matrix. For any real matrix $A \\in \\mathbb{R}^{H \\times W}$, its SVD is given by $A = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{H \\times H}$ and $V \\in \\mathbb{R}^{W \\times W}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{H \\times W}$ is a rectangular diagonal matrix with non-negative real numbers on the diagonal. The diagonal entries of $\\Sigma$, denoted by $\\sigma_i$, are the singular values of $A$, conventionally sorted in descending order: $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_{\\min(H,W)} \\ge 0$. The columns of $U$ are the left singular vectors and the columns of $V$ are the right singular vectors.\n\nThe SVD provides an optimal low-rank approximation of a matrix. According to the Eckart-Young-Mirsky theorem, the best rank-$k$ approximation of $A$ in the Frobenius norm, denoted $A_k$, is obtained by truncating the SVD. Specifically, $A_k = U_k \\Sigma_k V_k^\\top$, where $U_k$ and $V_k$ contain the first $k$ columns of $U$ and $V$ respectively, and $\\Sigma_k$ is the $k \\times k$ diagonal matrix with the first $k$ singular values $\\sigma_1, \\dots, \\sigma_k$.\n\nA crucial property for this problem is the relationship between the Frobenius norm and the singular values. The squared Frobenius norm of a matrix is the sum of the squares of its singular values:\n$$\n\\lVert A \\rVert_F^2 = \\sum_{i=1}^{\\min(H,W)} \\sigma_i^2.\n$$\nThe error of the rank-$k$ approximation can also be expressed directly in terms of singular values, which is computationally more efficient than constructing the matrix difference:\n$$\n\\lVert A - A_k \\rVert_F^2 = \\sum_{i=k+1}^{\\min(H,W)} \\sigma_i^2.\n$$\nThis allows for a direct calculation of the relative reconstruction error $e(H,W,k)$ without explicitly forming the approximated matrices $\\widetilde{R}_k, \\widetilde{G}_k, \\widetilde{B}_k$. Let $s_{R,i}, s_{G,i}, s_{B,i}$ be the singular values of the rescaled channels $\\widetilde{R}, \\widetilde{G}, \\widetilde{B}$. The squared numerator of the error formula is $\\sum_{i=k'+1} s_{R,i}^2 + \\sum_{i=k'+1} s_{G,i}^2 + \\sum_{i=k'+1} s_{B,i}^2$, and the squared denominator is $\\sum_{i=1} s_{R,i}^2 + \\sum_{i=1} s_{G,i}^2 + \\sum_{i=1} s_{B,i}^2$, where $k' = \\min(k, \\min(H,W))$.\n\nThe storage compression ratio $r(H,W,k)$ is derived from comparing the storage required for the original matrix versus its rank-$k'$ approximation. The original $H \\times W$ matrix requires storing $HW$ values. The rank-$k'$ approximation requires storing the truncated $U_k$ (an $H \\times k'$ matrix), the singular values $\\sigma_1, \\dots, \\sigma_{k'}$ (a $k'$-vector), and the truncated $V_k$ (a $W \\times k'$ matrix). This totals $Hk' + k' + Wk' = (H+W+1)k'$ values. The ratio is therefore $\\frac{(H+W+1)k'}{HW}$.\n\nThe solution algorithm proceeds as follows for each test case $(H,W,k)$:\n1.  Define the effective rank $k' = \\min(k, \\min(H,W))$ as per the problem's clamping rule.\n2.  Generate the normalized spatial grids $x$ (size $H$) and $y$ (size $W$).\n3.  Construct the matrices for the red ($R$), green ($G$), and blue ($B$) channels using the provided formulas, leveraging vectorized outer product operations for efficiency.\n4.  Rescale each channel matrix $C \\in \\{R,G,B\\}$ to the range $[0,1]$ to obtain $\\widetilde{C}$, applying the specified formula $\\widetilde{C} = \\frac{C - \\min(C)}{\\max(C) - \\min(C) + 10^{-12}}$. The special case where a channel is constant ($\\max(C) = \\min(C)$) is handled by setting its rescaled version to a zero matrix.\n5.  For each rescaled channel $\\widetilde{R}, \\widetilde{G}, \\widetilde{B}$, compute its singular values using SVD. Let the vectors of singular values be $s_R, s_G, s_B$.\n6.  Calculate the relative error $e(H,W,k)$ by summing the squares of the singular values as described above. The numerator is the sum of squares of singular values from index $k'$ to the end, and the denominator is the sum of squares of all singular values. If the total norm is zero, the error is $0$.\n7.  Calculate the compression ratio $r(H,W,k)$ using the formula $r = \\frac{(H+W+1)k'}{HW}$.\n8.  Store the resulting pair $[e, r]$ after rounding both values to $6$ decimal places.\nThis procedure is repeated for all test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the synthetic image compression problem using SVD for a suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (H, W, k)\n        (48, 64, 5),\n        (48, 64, 0),\n        (48, 64, 48),\n        (32, 20, 3),\n        (2, 2, 1),\n    ]\n\n    results = []\n    for H, W, k in test_cases:\n        # 1. Define effective rank k'\n        k_eff = min(k, H, W)\n\n        # 2. Generate spatial grids\n        x = np.linspace(0, 1, H)\n        y = np.linspace(0, 1, W)\n\n        # 3. Construct raw channel matrices R, G, B\n        # Red channel\n        R = (\n            0.5 * np.outer(x, 1 - y) +\n            0.35 * np.outer(np.sin(2 * np.pi * x), np.cos(2 * np.pi * y)) +\n            0.15 * np.outer(np.exp(-((x - 0.3)**2) / 0.05), y**0.5)\n        )\n\n        # Green channel\n        one_H = np.ones(H)\n        G = (\n            0.4 * np.outer(np.sin(np.pi * x), np.sin(np.pi * y)) +\n            0.4 * np.outer(one_H, np.exp(-((y - 0.5)**2) / 0.08)) +\n            0.2 * np.outer(np.cos(3 * np.pi * x), y)\n        )\n\n        # Blue channel\n        B = (\n            0.45 * np.outer(np.cos(2 * np.pi * x), y**2) +\n            0.35 * np.outer(x**0.5, (1 - y)**0.3) +\n            0.2 * np.outer(np.sin(4 * np.pi * x), np.cos(2 * np.pi * y))\n        )\n\n        # 4. Rescale channels to [0, 1]\n        def rescale(C):\n            c_min, c_max = np.min(C), np.max(C)\n            if np.isclose(c_max, c_min):\n                return np.zeros_like(C, dtype=float)\n            return (C - c_min) / (c_max - c_min + 1e-12)\n\n        Rt = rescale(R)\n        Gt = rescale(G)\n        Bt = rescale(B)\n\n        # 5. Compute singular values for each channel\n        s_r = np.linalg.svd(Rt, compute_uv=False)\n        s_g = np.linalg.svd(Gt, compute_uv=False)\n        s_b = np.linalg.svd(Bt, compute_uv=False)\n\n        # 6. Calculate relative reconstruction error e(H,W,k)\n        num_sq_err = (\n            np.sum(s_r[k_eff:]**2) +\n            np.sum(s_g[k_eff:]**2) +\n            np.sum(s_b[k_eff:]**2)\n        )\n        den_sq_err = (\n            np.sum(s_r**2) +\n            np.sum(s_g**2) +\n            np.sum(s_b**2)\n        )\n\n        if den_sq_err < 1e-12: # Handles case of all-zero rescaled image\n            e = 0.0\n        else:\n            e = np.sqrt(num_sq_err / den_sq_err)\n\n        # 7. Calculate compression ratio r(H,W,k)\n        if H * W == 0:\n            r = 0.0\n        else:\n            r = (H + W + 1) * k_eff / (H * W)\n\n        # 8. Append rounded results\n        results.append([round(e, 6), round(r, 6)])\n\n    # Final print statement in the exact required format.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3206004"}]}