{"hands_on_practices": [{"introduction": "The method of least squares is a cornerstone of experimental science, providing a robust way to fit models to noisy data. This first practice problem places you in the role of an engineer calibrating a sensor. By setting up and solving the normal equations for a simple linear model, you will gain hands-on experience in translating real-world measurements into a solvable matrix system and finding the best-fit parameters that minimize the approximation error [@problem_id:2218047].", "problem": "An engineer is calibrating a novel thermal sensor. The sensor's output voltage, $V$, is assumed to be a linear function of the ambient temperature, $T$. The relationship is modeled by the equation $V(T) = c_0 + c_1 T$, where $c_0$ and $c_1$ are the calibration constants to be determined. To find these constants, four measurements are taken in a controlled environment:\n\n*   At a temperature of $T=10$ degrees Celsius, the measured voltage is $V=2.6$ volts.\n*   At a temperature of $T=20$ degrees Celsius, the measured voltage is $V=3.4$ volts.\n*   At a temperature of $T=30$ degrees Celsius, the measured voltage is $V=4.7$ volts.\n*   At a temperature of $T=40$ degrees Celsius, the measured voltage is $V=5.4$ volts.\n\nThe parameters $c_0$ and $c_1$ are to be determined such that the sum of the squared differences between the measured voltages and the voltages predicted by the linear model is minimized. Let the resulting best-fit line be $\\hat{V}(T) = \\hat{c}_0 + \\hat{c}_1 T$.\n\nYour task is to calculate the Euclidean norm of the residual vector, where the components of the residual vector are the differences between the individually measured voltages and the corresponding voltages predicted by this best-fit line.\n\nExpress your final answer in volts, rounded to three significant figures.", "solution": "We model the voltage as a linear function of temperature, $V(T)=c_{0}+c_{1}T$, and determine $(\\hat{c}_{0},\\hat{c}_{1})$ by least squares using the four measurements $(T_{i},V_{i})=(10,2.6),(20,3.4),(30,4.7),(40,5.4)$. Let the design matrix be $X=\\begin{pmatrix}1 & 10 \\\\ 1 & 20 \\\\ 1 & 30 \\\\ 1 & 40\\end{pmatrix}$ and the observation vector be $\\boldsymbol{V}=\\begin{pmatrix}2.6 \\\\ 3.4 \\\\ 4.7 \\\\ 5.4\\end{pmatrix}$. The least-squares estimate satisfies\n$$\n\\begin{pmatrix}\\hat{c}_{0} \\\\ \\hat{c}_{1}\\end{pmatrix}=(X^{T}X)^{-1}X^{T}\\boldsymbol{V},\n$$\nequivalently the normal equations\n$$\n\\begin{pmatrix}n & \\sum T_{i} \\\\ \\sum T_{i} & \\sum T_{i}^{2}\\end{pmatrix}\\begin{pmatrix}\\hat{c}_{0} \\\\ \\hat{c}_{1}\\end{pmatrix}=\\begin{pmatrix}\\sum V_{i} \\\\ \\sum T_{i}V_{i}\\end{pmatrix}.\n$$\nCompute the sums: $n=4$, $\\sum T_{i}=10+20+30+40=100$, $\\sum T_{i}^{2}=10^{2}+20^{2}+30^{2}+40^{2}=3000$, $\\sum V_{i}=2.6+3.4+4.7+5.4=16.1$, and $\\sum T_{i}V_{i}=10\\cdot 2.6+20\\cdot 3.4+30\\cdot 4.7+40\\cdot 5.4=451$. Thus we solve\n$$\n\\begin{pmatrix}4 & 100 \\\\ 100 & 3000\\end{pmatrix}\\begin{pmatrix}\\hat{c}_{0} \\\\ \\hat{c}_{1}\\end{pmatrix}=\\begin{pmatrix}16.1 \\\\ 451\\end{pmatrix}.\n$$\nThe determinant is $4\\cdot 3000-100\\cdot 100=2000$, so\n$$\n\\hat{c}_{0}=\\frac{3000\\cdot 16.1-100\\cdot 451}{2000}=1.6,\\quad \\hat{c}_{1}=\\frac{-100\\cdot 16.1+4\\cdot 451}{2000}=0.097.\n$$\nHence the best-fit line is $\\hat{V}(T)=1.6+0.097\\,T$.\n\nCompute the residuals $r_{i}=V_{i}-\\hat{V}(T_{i})$ at the four temperatures:\n$$\n\\hat{V}(10)=1.6+0.097\\cdot 10=2.57,\\quad r_{1}=2.6-2.57=0.03,\n$$\n$$\n\\hat{V}(20)=1.6+0.097\\cdot 20=3.54,\\quad r_{2}=3.4-3.54=-0.14,\n$$\n$$\n\\hat{V}(30)=1.6+0.097\\cdot 30=4.51,\\quad r_{3}=4.7-4.51=0.19,\n$$\n$$\n\\hat{V}(40)=1.6+0.097\\cdot 40=5.48,\\quad r_{4}=5.4-5.48=-0.08.\n$$\nThe Euclidean norm of the residual vector $\\boldsymbol{r}$ is\n$$\n\\|\\boldsymbol{r}\\|_{2}=\\sqrt{\\sum_{i=1}^{4}r_{i}^{2}}=\\sqrt{(0.03)^{2}+(-0.14)^{2}+(0.19)^{2}+(-0.08)^{2}}=\\sqrt{0.063}.\n$$\nEvaluating the square root and rounding to three significant figures gives\n$$\n\\|\\boldsymbol{r}\\|_{2}\\approx 0.251.\n$$\nThis value is in volts because each residual is a voltage difference.", "answer": "$$\\boxed{0.251}$$", "id": "2218047"}, {"introduction": "Beyond simply fitting a line to data points, the least squares solution has a profound geometric meaning. This exercise re-frames the problem as finding the closest point in a given subspace—a robot's reachable plane—to a target point outside of it. By solving this problem, you will see how the normal equations provide the coordinates for the orthogonal projection of a vector onto a subspace, offering a powerful visual intuition for why the least squares solution is \"optimal\" [@problem_id:2218040].", "problem": "You are a control systems engineer working on a simple 2-Degree-Of-Freedom (2-DOF) robotic manipulator. The robot's end-effector moves in 3D space, and its position relative to its base is described by a coordinate vector $\\mathbf{p} = [x, y, z]^T$. The robot's motion is generated by two independent actuators. The first actuator moves the end-effector along the direction vector $\\mathbf{u}_1 = [1, 0, 1]^T$, and the second actuator moves it along the direction vector $\\mathbf{u}_2 = [0, 1, 1]^T$. Consequently, any reachable position $\\mathbf{p}_{reach}$ must be a linear combination of these two vectors: $\\mathbf{p}_{reach} = c_1 \\mathbf{u}_1 + c_2 \\mathbf{u}_2$, for some scalar coefficients $c_1$ and $c_2$ representing the actuator displacements. This means the set of all reachable points forms a plane passing through the origin.\n\nA task requires the robot's end-effector to move to a target position $\\mathbf{b} = [1, 2, 2]^T$. This point may not lie within the robot's reachable plane. The control system's objective is to find the point $\\mathbf{p}^*$ within the reachable plane that is geometrically closest to the target point $\\mathbf{b}$. This optimal point is the orthogonal projection of $\\mathbf{b}$ onto the subspace spanned by the actuator direction vectors.\n\nUsing the method of normal equations, determine the coordinate vector of this optimal reachable point $\\mathbf{p}^*$. Express your answer as a column vector with exact fractional components.", "solution": "The problem asks for the orthogonal projection of the vector $\\mathbf{b}$ onto the subspace spanned by the vectors $\\mathbf{u}_1$ and $\\mathbf{u}_2$. This subspace is the column space of the matrix $A$ whose columns are $\\mathbf{u}_1$ and $\\mathbf{u}_2$. The projection $\\mathbf{p}^*$ is the vector in the column space of $A$ that is closest to $\\mathbf{b}$.\n\nFirst, we define the matrix $A$ and the vector $\\mathbf{b}$:\n$$ A = \\begin{bmatrix} \\mathbf{u}_1 & \\mathbf{u}_2 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 2 \\end{bmatrix} $$\n\nThe projection $\\mathbf{p}^*$ can be expressed as a linear combination of the columns of $A$, i.e., $\\mathbf{p}^* = A\\mathbf{c}^*$, where $\\mathbf{c}^* = \\begin{bmatrix} c_1 \\\\ c_2 \\end{bmatrix}$ is the vector of coefficients that minimizes the squared Euclidean distance $\\|A\\mathbf{c} - \\mathbf{b}\\|^2$. This is a linear least squares problem. The solution $\\mathbf{c}^*$ is found by solving the normal equations:\n$$ A^T A \\mathbf{c}^* = A^T \\mathbf{b} $$\n\nWe proceed by computing the components of this equation.\n\nStep 1: Compute the matrix $A^T A$.\nThe transpose of $A$ is:\n$$ A^T = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} $$\nNow, we compute the product $A^T A$:\n$$ A^T A = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} (1)(1)+(0)(0)+(1)(1) & (1)(0)+(0)(1)+(1)(1) \\\\ (0)(1)+(1)(0)+(1)(1) & (0)(0)+(1)(1)+(1)(1) \\end{bmatrix} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} $$\n\nStep 2: Compute the vector $A^T \\mathbf{b}$.\n$$ A^T \\mathbf{b} = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} (1)(1)+(0)(2)+(1)(2) \\\\ (0)(1)+(1)(2)+(1)(2) \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} $$\n\nStep 3: Solve the normal equations for $\\mathbf{c}^*$.\nThe system of equations is:\n$$ \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} \\begin{bmatrix} c_1 \\\\ c_2 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} $$\nThis corresponds to the two linear equations:\n1) $2c_1 + c_2 = 3$\n2) $c_1 + 2c_2 = 4$\n\nFrom equation (1), we can express $c_2$ in terms of $c_1$:\n$c_2 = 3 - 2c_1$\n\nSubstitute this into equation (2):\n$c_1 + 2(3 - 2c_1) = 4$\n$c_1 + 6 - 4c_1 = 4$\n$-3c_1 = -2$\n$c_1 = \\frac{2}{3}$\n\nNow, substitute the value of $c_1$ back to find $c_2$:\n$c_2 = 3 - 2\\left(\\frac{2}{3}\\right) = 3 - \\frac{4}{3} = \\frac{9}{3} - \\frac{4}{3} = \\frac{5}{3}$\nSo, the coefficient vector is $\\mathbf{c}^* = \\begin{bmatrix} 2/3 \\\\ 5/3 \\end{bmatrix}$.\n\nStep 4: Compute the projection vector $\\mathbf{p}^*$.\nThe optimal reachable point $\\mathbf{p}^*$ is given by $A\\mathbf{c}^*$:\n$$ \\mathbf{p}^* = A\\mathbf{c}^* = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} \\frac{2}{3} \\\\ \\frac{5}{3} \\end{bmatrix} = \\begin{bmatrix} (1)\\left(\\frac{2}{3}\\right) + (0)\\left(\\frac{5}{3}\\right) \\\\ (0)\\left(\\frac{2}{3}\\right) + (1)\\left(\\frac{5}{3}\\right) \\\\ (1)\\left(\\frac{2}{3}\\right) + (1)\\left(\\frac{5}{3}\\right) \\end{bmatrix} = \\begin{bmatrix} \\frac{2}{3} \\\\ \\frac{5}{3} \\\\ \\frac{2}{3} + \\frac{5}{3} \\end{bmatrix} = \\begin{bmatrix} \\frac{2}{3} \\\\ \\frac{5}{3} \\\\ \\frac{7}{3} \\end{bmatrix} $$\n\nThe coordinate vector of the optimal reachable point is $\\mathbf{p}^* = [2/3, 5/3, 7/3]^T$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{2}{3} \\\\\n\\frac{5}{3} \\\\\n\\frac{7}{3}\n\\end{pmatrix}\n}\n$$", "id": "2218040"}, {"introduction": "A powerful numerical method is only as good as the model it is applied to. This final practice explores a critical pitfall in linear regression: the use of linearly dependent basis functions in the model. You will investigate why a seemingly reasonable model leads to a singular system in the normal equations, which does not have a unique solution. This diagnostic exercise emphasizes the importance of ensuring your model's components are mathematically independent before attempting to find a least squares solution [@problem_id:2218021].", "problem": "An engineer is attempting to model a physical process using a linear combination of two basis functions, $f_1(t) = t$ and $f_2(t) = -2t$. The model is given by $y(t) = c_1 f_1(t) + c_2 f_2(t)$, where $c_1$ and $c_2$ are unknown coefficients. The engineer has collected three data points $(t_i, y_i)$: $(1, 2)$, $(2, 3)$, and $(3, 5)$.\n\nTo find the coefficients $c_1$ and $c_2$ that best fit the data in a least-squares sense, one must find the vector $\\mathbf{x} = \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}$ that minimizes the squared error $\\|A\\mathbf{x} - \\mathbf{b}\\|^2$. This is equivalent to solving the system of linear equations known as the normal equations:\n$$A^T A \\mathbf{x} = A^T \\mathbf{b}$$\nHere, $A$ is the design matrix constructed from the basis functions evaluated at the data points $t_i$, and $\\mathbf{b}$ is the vector of observed values $y_i$.\n\nWhich of the following statements correctly describes the solution set for the vector of coefficients $\\mathbf{x} = \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}$ for this specific problem?\n\nA. There is a unique solution for $\\mathbf{x}$.\n\nB. There are no solutions for $\\mathbf{x}$.\n\nC. There are infinitely many solutions for $\\mathbf{x}$.\n\nD. The only solution is the trivial solution, $\\mathbf{x} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n\nE. The matrix $A^T A$ is invertible, but the system is inconsistent.", "solution": "The model is $y(t)=c_{1}f_{1}(t)+c_{2}f_{2}(t)$ with $f_{1}(t)=t$ and $f_{2}(t)=-2t$. Since $f_{2}(t)=-2f_{1}(t)$ for all $t$, the two basis functions are linearly dependent. Therefore, in the design matrix $A$ with rows $\\bigl[f_{1}(t_{i})\\;\\;f_{2}(t_{i})\\bigr]=\\bigl[t_{i}\\;\\;-2t_{i}\\bigr]$, the second column is $-2$ times the first column. Hence $\\mathrm{rank}(A)=1<2$.\n\nThe least-squares solution(s) are obtained from the normal equations\n$$\nA^{T}A\\,\\mathbf{x}=A^{T}\\mathbf{b}.\n$$\nBecause $\\mathrm{rank}(A)=1$, we have $\\mathrm{rank}(A^{T}A)=\\mathrm{rank}(A)=1$, so $A^{T}A$ is singular and therefore not invertible. Consequently, a unique solution cannot exist.\n\nTo determine existence and possible multiplicity of solutions, note that $\\mathrm{col}(A^{T}A)=\\mathrm{col}(A^{T})$. Indeed, $A^{T}A w\\in \\mathrm{col}(A^{T})$ for every $w$, so $\\mathrm{col}(A^{T}A)\\subseteq \\mathrm{col}(A^{T})$. Conversely, if $u\\in \\mathrm{col}(A^{T})$, then $u=A^{T}v$ for some $v$, and decomposing $v$ orthogonally as $v=Az+v_{\\perp}$ with $v_{\\perp}\\perp \\mathrm{col}(A)$ yields $u=A^{T}Az\\in \\mathrm{col}(A^{T}A)$. Hence equality holds. Since the right-hand side of the normal equations is $A^{T}\\mathbf{b}\\in \\mathrm{col}(A^{T})=\\mathrm{col}(A^{T}A)$, the system $A^{T}A\\,\\mathbf{x}=A^{T}\\mathbf{b}$ is always consistent.\n\nBecause $A^{T}A$ is singular and the system is consistent, there are infinitely many solutions. Equivalently, the null space is nontrivial: if $\\mathbf{w}\\in \\mathrm{null}(A)$, then $A^{T}A\\mathbf{w}=0$, so if $\\mathbf{x}_{p}$ is one solution, every $\\mathbf{x}_{p}+\\mathbf{w}$ is also a solution. Here $\\mathrm{null}(A)$ is nontrivial since the columns are dependent (explicitly, $A\\begin{pmatrix}2\\\\1\\end{pmatrix}=0$), confirming the infinite family.\n\nTherefore, the correct statement is that there are infinitely many solutions for $\\mathbf{x}$.", "answer": "$$\\boxed{C}$$", "id": "2218021"}]}