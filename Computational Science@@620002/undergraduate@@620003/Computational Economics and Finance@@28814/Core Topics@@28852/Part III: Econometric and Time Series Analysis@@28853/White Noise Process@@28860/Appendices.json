{"hands_on_practices": [{"introduction": "In computational finance and economics, simulating economic scenarios or financial instrument behavior is a fundamental task. These simulations often require a source of randomness to model unpredictable market shocks or measurement errors. This exercise provides a concrete look at how to generate a realization of a Gaussian white noise process, a common model for such randomness, starting from the basic output of a standard random number generator. By applying the Box-Muller transform, you will bridge the gap between the theoretical definition of white noise and its practical implementation in a simulation environment [@problem_id:1350034].", "problem": "An engineer is developing a simulation for a digital communication system and needs to model channel noise. The noise is modeled as a discrete-time Gaussian white noise process, $\\{W[n]\\}_{n=1,2,...}$, which is a sequence of independent and identically distributed random variables, each following a normal distribution with a mean of zero and a specific variance.\n\nThe engineer uses a standard pseudo-random number generator that produces a sequence of numbers $\\{U_n\\}$ drawn from a uniform distribution on the interval $[0, 1)$. To convert these into standard normal random variables $\\{Z_n\\}$ (with mean $\\mu=0$ and variance $\\sigma^2=1$), the engineer employs the Box-Muller transform. This method takes two independent uniform random variables, $U_1$ and $U_2$, and produces two independent standard normal random variables, $Z_1$ and $Z_2$, using the following equations:\n$$Z_1 = \\sqrt{-2 \\ln(U_1)} \\cos(2\\pi U_2)$$\n$$Z_2 = \\sqrt{-2 \\ln(U_1)} \\sin(2\\pi U_2)$$\nThe Gaussian white noise process $\\{W[n]\\}$ is then generated by appropriately scaling the sequence of standard normal variables $\\{Z[n]\\}$ to achieve a desired variance of $\\sigma_W^2 = 7.5$.\n\nSuppose the first two numbers produced by the uniform random number generator are $U_1 = \\exp(-4.5)$ and $U_2 = 1/3$. Using the first standard normal variable $Z_1$ generated by these two uniform numbers, calculate the corresponding first sample of the white noise process, $W[1]$.\n\nExpress your answer as a single real number rounded to three significant figures.", "solution": "We use the Box–Muller transform. Given $U_{1}=\\exp(-4.5)$ and $U_{2}=\\frac{1}{3}$, the first standard normal variate is\n$$\nZ_{1}=\\sqrt{-2\\ln(U_{1})}\\,\\cos(2\\pi U_{2}).\n$$\nSince $\\ln(U_{1})=\\ln(\\exp(-4.5))=-4.5$, we have\n$$\n-2\\ln(U_{1})=-2(-4.5)=9,\\quad \\sqrt{-2\\ln(U_{1})}=\\sqrt{9}=3.\n$$\nAlso,\n$$\n\\cos(2\\pi U_{2})=\\cos\\!\\left(2\\pi\\cdot \\frac{1}{3}\\right)=\\cos\\!\\left(\\frac{2\\pi}{3}\\right)=-\\frac{1}{2}.\n$$\nTherefore,\n$$\nZ_{1}=3\\left(-\\frac{1}{2}\\right)=-\\frac{3}{2}.\n$$\nTo obtain the white noise sample with variance $\\sigma_{W}^{2}=7.5$, scale the standard normal by $\\sigma_{W}=\\sqrt{7.5}$:\n$$\nW[1]=\\sqrt{\\sigma_{W}^{2}}\\,Z_{1}=\\sqrt{7.5}\\left(-\\frac{3}{2}\\right)=-\\frac{3}{2}\\sqrt{7.5}.\n$$\nNumerically, $\\sqrt{7.5}\\approx 2.738612787$, hence\n$$\nW[1]\\approx -1.5\\times 2.738612787\\approx -4.107919181,\n$$\nwhich rounded to three significant figures is $-4.11$.", "answer": "$$\\boxed{-4.11}$$", "id": "1350034"}, {"introduction": "White noise is rarely a final model for economic data, which often exhibits trends and correlations. Instead, its power lies in its role as a fundamental building block for more sophisticated time series models. This practice demonstrates this key principle by constructing a Moving Average (MA) process, where the current value is a combination of a current and a past white noise shock. By calculating the autocovariance of this new process, you will see precisely how a simple, uncorrelated process can give rise to a more complex process with predictable short-term dependence, a cornerstone concept in time series analysis [@problem_id:1350040].", "problem": "A digital signal processing engineer is analyzing the output of a newly designed filter. The output signal at discrete time points $t$, denoted as $X_t$, is modeled by a stochastic process. This model describes how the output is influenced by a stream of unpredictable, random electronic fluctuations. The relationship is given by the equation:\n\n$$X_t = W_t + \\theta W_{t-1}$$\n\nHere, the sequence $\\{W_t\\}$ for all integer times $t$ represents a discrete-time white noise process. This means that each $W_t$ is a random variable with the following properties:\n1.  The mean (expected value) is zero for all $t$: $E[W_t] = 0$.\n2.  The variance is a constant value $\\sigma_W^2$ for all $t$: $\\text{Var}(W_t) = \\sigma_W^2$.\n3.  The values at different time points are uncorrelated: $\\text{Cov}(W_t, W_s) = 0$ for any $t \\neq s$.\n\nThe parameter $\\theta$ is a fixed, real-valued constant that characterizes the filter's design. To understand how a random fluctuation at one moment affects the signal at the next, the engineer needs to compute the covariance between the signal's value at time $t$ and its value at the immediately succeeding time step, $t+1$.\n\nDetermine the covariance $\\text{Cov}(X_t, X_{t+1})$. Your final answer should be a symbolic expression in terms of $\\theta$ and $\\sigma_W^2$.", "solution": "We are given the moving-average process of order one:\n$$X_{t} = W_{t} + \\theta W_{t-1}, \\quad X_{t+1} = W_{t+1} + \\theta W_{t}.$$\nThe covariance at lag one is defined by\n$$\\text{Cov}(X_{t}, X_{t+1}) = \\mathbb{E}[X_{t}X_{t+1}] - \\mathbb{E}[X_{t}]\\,\\mathbb{E}[X_{t+1}].$$\nSince $\\mathbb{E}[W_{t}] = 0$ for all $t$, we have\n$$\\mathbb{E}[X_{t}] = \\mathbb{E}[W_{t} + \\theta W_{t-1}] = 0 + \\theta \\cdot 0 = 0,$$\nand similarly $\\mathbb{E}[X_{t+1}] = 0$. Therefore,\n$$\\text{Cov}(X_{t}, X_{t+1}) = \\mathbb{E}[X_{t}X_{t+1}].$$\nCompute the product:\n$$X_{t}X_{t+1} = (W_{t} + \\theta W_{t-1})(W_{t+1} + \\theta W_{t})\n= W_{t}W_{t+1} + \\theta W_{t}^{2} + \\theta W_{t-1}W_{t+1} + \\theta^{2} W_{t-1}W_{t}.$$\nTaking expectations and using the white-noise properties $\\mathbb{E}[W_{s}W_{t}] = 0$ for $s \\neq t$ and $\\mathbb{E}[W_{t}^{2}] = \\text{Var}(W_{t}) = \\sigma_{W}^{2}$, we obtain\n$$\\mathbb{E}[X_{t}X_{t+1}] = 0 + \\theta\\,\\mathbb{E}[W_{t}^{2}] + 0 + 0 = \\theta \\sigma_{W}^{2}.$$\nThus,\n$$\\text{Cov}(X_{t}, X_{t+1}) = \\theta \\sigma_{W}^{2}.$$", "answer": "$$\\boxed{\\theta \\sigma_{W}^{2}}$$", "id": "1350040"}, {"introduction": "After developing and fitting a time series model to data—for example, to explain stock returns or forecast sales—how do we know if the model is any good? A crucial diagnostic step is to examine the model's residuals, which are the parts of the data the model couldn't explain. If the model has successfully captured the predictable patterns, the residuals should be unpredictable, resembling white noise. This advanced practice guides you through implementing the formal statistical tests—the Student's $t$-test for the mean and the Ljung-Box test for autocorrelation—that are used daily by quantitative analysts to determine if a residual series is consistent with white noise, thereby validating the fit of their models [@problem_id:2448045].", "problem": "You are analyzing whether seasonality-adjusted daily sales residuals from a firm behave like weak white noise in the sense of zero mean and no linear autocorrelation. Use the following fundamental base: by definition, a weak white noise process $\\{e_t\\}$ satisfies $\\mathbb{E}[e_t] = 0$ and $\\operatorname{Cov}(e_t, e_{t-k}) = 0$ for all integers $k \\neq 0$. To decide if a finite observed sequence is consistent with weak white noise at a given significance level, proceed from first principles as follows: test the mean using a two-sided Student's $t$-test for $\\mathbb{E}[e_t]=0$, and test the joint null of zero autocorrelation up to lag $h$ using the Ljung-Box portmanteau statistic.\n\nImplementation details to be followed by your program:\n- Mean-zero test. Let the sample size be $n$, the sample mean be $\\bar{e} = \\frac{1}{n}\\sum_{t=1}^{n} e_t$, and the unbiased sample standard deviation be $s = \\sqrt{\\frac{1}{n-1}\\sum_{t=1}^{n}(e_t - \\bar{e})^2}$. Under the null hypothesis $\\mathbb{E}[e_t] = 0$, the statistic\n$$\nT = \\frac{\\bar{e}}{s/\\sqrt{n}}\n$$\nhas a Student's $t$ distribution with $n-1$ degrees of freedom for independent, identically distributed data with finite second moment. Compute the two-sided $p$-value and compare to the given significance level $\\alpha$.\n- Autocorrelation test. For each lag $k \\in \\{1,\\dots,h^\\ast\\}$ where $h^\\ast = \\min(h, n-1)$, compute the sample autocorrelation\n$$\n\\hat{r}_k = \\frac{\\sum_{t=k+1}^{n}(e_t - \\bar{e})(e_{t-k} - \\bar{e})}{\\sum_{t=1}^{n}(e_t - \\bar{e})^2}.\n$$\nThen compute the Ljung-Box statistic\n$$\nQ = n(n+2)\\sum_{k=1}^{h^\\ast} \\frac{\\hat{r}_k^2}{n-k}.\n$$\nUnder the joint null hypothesis that $\\hat{r}_k = 0$ for all $k=1,\\dots,h^\\ast$, $Q$ is approximately distributed as chi-square with $h^\\ast$ degrees of freedom for large $n$. Compute the corresponding $p$-value and compare to $\\alpha$.\n- Decision rule. Declare a residual series \"weak white noise\" if and only if both tests fail to reject at level $\\alpha$, that is, both $p_{\\text{mean}} \\ge \\alpha$ and $p_{\\text{LB}} \\ge \\alpha$ hold.\n- Degenerate variance handling. If the unbiased sample variance is exactly zero, then all $e_t$ are equal. In that case, declare the series \"not white noise\" unless all $e_t$ are exactly zero, in which case treat it as white noise by convention for this exercise. This rule prevents undefined divisions in the test statistics.\n\nAngle unit requirement: any angles used below are in radians.\n\nTest suite. Apply the above decision rule to the following four seasonality-adjusted residual sequences, each with its specified significance level $\\alpha$ and maximum autocorrelation lag $h$:\n\n- Case 1 (happy path: sparse uncorrelated residuals): $n=60$, $h=10$, $\\alpha=0.05$. The residuals are $e_t = 0$ for all $t$ except $e_{6} = 1.0$ and $e_{51} = -1.0$.\n- Case 2 (autocorrelated leftovers from weekly pattern): $n=56$, $h=10$, $\\alpha=0.05$. The residuals are $e_t = \\sin\\left(\\frac{2\\pi t}{7}\\right)$ for $t=0,1,\\dots,55$, where $\\pi$ is the mathematical constant and the angle is in radians.\n- Case 3 (misadjusted constant bias; boundary case with zero variance): $n=30$, $h=5$, $\\alpha=0.05$. The residuals are constant $e_t = 0.3$ for all $t=1,\\dots,30$.\n- Case 4 (small sample, large $h$ close to $n$; boundary on $h$): $n=12$, $h=10$, $\\alpha=0.05$. The residuals are $e_t = 0$ for all $t$ except $e_{1} = 1.0$ and $e_{12} = -1.0$.\n\nYour program must compute, for each case, a boolean indicating whether the residuals are weak white noise at level $\\alpha$ according to the above rule. The final output format must be exactly one line containing the four boolean results in order as a comma-separated list enclosed in square brackets, for example, \"[True,False,True,False]\". No additional text should be printed.", "solution": "Here is the logical breakdown for the result of each test case. A series is classified as weak white noise if and only if it passes both the mean-zero test and the no-autocorrelation test at the given significance level $\\alpha$.\n\n- **Case 1: Result is `True`**\n    - **Mean Test:** The series consists of two non-zero values, $1.0$ and $-1.0$, with all other 58 values being zero. The sample mean is $\\bar{e} = (1.0 - 1.0) / 60 = 0$. Since the mean is exactly zero, the $t$-statistic is 0, and the two-sided $p$-value is 1.0. As $1.0 \\ge 0.05$, the mean test passes.\n    - **Autocorrelation Test:** The two non-zero values are at indices 6 and 51, separated by a lag of $45$. The Ljung-Box test considers autocorrelations up to a maximum lag of $h=10$. For any lag $k \\in \\{1, \\dots, 10\\}$, the lagged product terms in the autocorrelation calculation, $(e_t - \\bar{e})(e_{t-k} - \\bar{e})$, are always zero because the non-zero values are too far apart. Thus, all sample autocorrelations $\\hat{r}_k$ are zero. This makes the Ljung-Box statistic $Q=0$, yielding a $p$-value of 1.0. As $1.0 \\ge 0.05$, the autocorrelation test passes.\n    - **Conclusion:** Both tests pass, so the series is classified as weak white noise.\n\n- **Case 2: Result is `False`**\n    - **Mean Test:** The series is $e_t = \\sin(2\\pi t/7)$ for $t=0, \\dots, 55$. The sample size $n=56$ corresponds to exactly $56/7=8$ full cycles of the sine wave. The sum of the series is therefore zero (or extremely close due to floating point precision), resulting in a sample mean $\\bar{e} \\approx 0$. The $p$-value for the mean test will be approximately 1.0, which is greater than $0.05$. The mean test passes.\n    - **Autocorrelation Test:** The series is perfectly periodic with a period of 7. The sample autocorrelation at lag 7, $\\hat{r}_7$, will be very close to 1. This large autocorrelation term will make the Ljung-Box statistic $Q$ very large, causing the corresponding $p$-value to be extremely small (much less than $0.05$). The autocorrelation test fails.\n    - **Conclusion:** Since the autocorrelation test fails, the series is not weak white noise.\n\n- **Case 3: Result is `False`**\n    - **Degenerate Variance Rule:** The series consists of a constant value $e_t = 0.3$. The sample variance is exactly zero. The problem specifies a special rule for this case: \"declare the series 'not white noise' unless all $e_t$ are exactly zero.\" Since the values are $0.3$ (not zero), the series is immediately classified as not weak white noise, and the statistical tests are not performed.\n    - **Conclusion:** The series is not weak white noise.\n\n- **Case 4: Result is `True`**\n    - **Mean Test:** The series of size $n=12$ has two non-zero values, $1.0$ and $-1.0$. The sample mean is $\\bar{e} = (1.0 - 1.0) / 12 = 0$. The $p$-value for the mean test is 1.0, which is greater than $0.05$. The mean test passes.\n    - **Autocorrelation Test:** The maximum lag for testing is $h=10$. The effective maximum lag is $h^\\ast = \\min(10, 12-1) = 10$. The two non-zero values are at indices 1 and 12, separated by a lag of $11$. Since the separation (11) is greater than the maximum lag tested (10), all sample autocorrelations $\\hat{r}_k$ for $k \\in \\{1, \\dots, 10\\}$ will be zero. Therefore, $Q=0$ and the $p$-value is 1.0. As $1.0 \\ge 0.05$, the autocorrelation test passes.\n    - **Conclusion:** Both tests pass, so the series is classified as weak white noise.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t, chi2\n\ndef is_weak_white_noise(e: np.ndarray, h: int, alpha: float) -> bool:\n    \"\"\"\n    Tests if a time series behaves like weak white noise.\n\n    Args:\n        e: A numpy array representing the time series of residuals.\n        h: The maximum lag for the Ljung-Box test.\n        alpha: The significance level for the tests.\n\n    Returns:\n        True if the series is classified as weak white noise, False otherwise.\n    \"\"\"\n    n = len(e)\n    if n == 0:\n        return True # Convention for empty series\n\n    e_bar = np.mean(e)\n    sum_sq_dev = np.sum((e - e_bar)**2)\n\n    # Degenerate variance handling rule\n    if np.isclose(sum_sq_dev, 0):\n        # Variance is zero, so all elements are equal to the mean.\n        # It's white noise by convention only if the mean is also zero.\n        return np.isclose(e_bar, 0)\n\n    # 1. Mean-zero test (Student's t-test)\n    # Unbiased sample standard deviation\n    s = np.sqrt(sum_sq_dev / (n - 1))\n    \n    # Check for s=0 shouldn't be needed due to sum_sq_dev check, but for robustness:\n    if np.isclose(s, 0):\n        # This case is already covered by the sum_sq_dev check, but as a safeguard.\n        return np.isclose(e_bar, 0)\n        \n    t_statistic = e_bar / (s / np.sqrt(n))\n    df_t = n - 1\n    p_mean = t.sf(np.abs(t_statistic), df=df_t) * 2\n\n    # 2. Autocorrelation test (Ljung-Box)\n    h_star = min(h, n - 1)\n    \n    lb_sum = 0.0\n    for k in range(1, h_star + 1):\n        # Numerator: sum_{i=k to n-1} (e[i] - e_bar) * (e[i-k] - e_bar)\n        # using 0-indexed array 'e'\n        numerator = np.dot(e[k:] - e_bar, e[:-k] - e_bar)\n        r_k = numerator / sum_sq_dev\n        lb_sum += (r_k**2) / (n - k)\n\n    q_statistic = n * (n + 2) * lb_sum\n    df_q = h_star\n    \n    # Handle df_q=0 case (e.g., n=1, so h_star=0)\n    if df_q == 0:\n        p_lb = 1.0 # No autocorrelations to test\n    else:\n        p_lb = chi2.sf(q_statistic, df=df_q)\n\n    # 3. Decision rule\n    return p_mean >= alpha and p_lb >= alpha\n\ndef solve():\n    \"\"\"\n    Solves the problem by applying the white noise test to four specified cases.\n    \"\"\"\n    \n    # Case 1 (happy path: sparse uncorrelated residuals)\n    n1, h1, alpha1 = 60, 10, 0.05\n    e1 = np.zeros(n1)\n    e1[5] = 1.0   # e_6 in 1-based indexing\n    e1[50] = -1.0 # e_51 in 1-based indexing\n    \n    # Case 2 (autocorrelated leftovers from weekly pattern)\n    n2, h2, alpha2 = 56, 10, 0.05\n    t2 = np.arange(n2)\n    e2 = np.sin(2 * np.pi * t2 / 7)\n    \n    # Case 3 (misadjusted constant bias; boundary case with zero variance)\n    n3, h3, alpha3 = 30, 5, 0.05\n    e3 = np.full(n3, 0.3)\n    \n    # Case 4 (small sample, large h close to n; boundary on h)\n    n4, h4, alpha4 = 12, 10, 0.05\n    e4 = np.zeros(n4)\n    e4[0] = 1.0   # e_1 in 1-based indexing\n    e4[11] = -1.0 # e_12 in 1-based indexing\n\n    test_cases = [\n        (e1, h1, alpha1),\n        (e2, h2, alpha2),\n        (e3, h3, alpha3),\n        (e4, h4, alpha4),\n    ]\n\n    results = []\n    for e, h, alpha in test_cases:\n        result = is_weak_white_noise(e, h, alpha)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2448045"}]}