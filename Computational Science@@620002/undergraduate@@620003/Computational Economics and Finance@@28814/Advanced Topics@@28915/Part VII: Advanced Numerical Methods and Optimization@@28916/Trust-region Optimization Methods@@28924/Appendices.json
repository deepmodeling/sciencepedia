{"hands_on_practices": [{"introduction": "Trust-region methods work by solving a sequence of simpler subproblems. This first exercise breaks down the most fundamental of these: the one-dimensional trust-region subproblem. Here, we will explore the core decision-making process of the algorithm by minimizing a simple quadratic model within a constrained interval, which is our \"trust region\" [@problem_id:2224504]. This practice will help you build intuition for when the optimal step is the unconstrained minimum of the model versus when it is limited by the trust-region boundary.", "problem": "In the context of unconstrained optimization, trust-region methods iteratively approximate a complex function with a simpler model function $m(p)$ around the current point. The next step $p$ is then determined by solving the trust-region subproblem, which involves minimizing this model within a \"trust region\" of radius $\\Delta > 0$, where the model is believed to be a reliable approximation of the original function. The subproblem is formally stated as:\n$$\n\\min_{p} m(p) \\quad \\text{subject to} \\quad \\|p\\| \\le \\Delta\n$$\nConsider a one-dimensional optimization scenario where the model function for a step $p \\in \\mathbb{R}$ is a quadratic given by:\n$$\nm(p) = g p + \\frac{1}{2} H p^2 + c\n$$\nwith a gradient term $g = 2$, a Hessian term $H = 6$, and an arbitrary constant $c$. The step is constrained by a trust-region radius of $\\Delta = 0.1$.\n\nDetermine the optimal step $p$ that solves this one-dimensional trust-region subproblem. Provide your answer as an exact decimal number.", "solution": "We must minimize the quadratic model $m(p) = g p + \\frac{1}{2} H p^{2} + c$ subject to the trust-region constraint $|p| \\le \\Delta$, with given values $g=2$, $H=6$, and $\\Delta=0.1$. The constant $c$ does not affect the minimizer and can be ignored.\n\nFirst, we find the unconstrained minimizer of the model by setting the derivative to zero:\n$$\nm'(p) = g + H p = 0 \\quad \\Rightarrow \\quad p_{u} = -\\frac{g}{H}.\n$$\nFor $g=2$ and $H=6$, this gives the unconstrained step $p_{u} = -\\frac{2}{6} = -\\frac{1}{3}$.\n\nNext, we check if this step is within the trust region: $|p_{u}| = \\frac{1}{3} \\approx 0.333$, which is greater than the trust-region radius $\\Delta = 0.1$. The unconstrained solution is therefore not feasible.\n\nSince the unconstrained minimizer lies outside the trust region, the solution to the constrained problem must lie on the boundary of the region, i.e., $|p| = \\Delta$. There are two possibilities: $p = \\Delta$ or $p = -\\Delta$.\nThe model is convex ($H=6 > 0$), so the minimum on the interval $[-\\Delta, \\Delta]$ must be the boundary point closest to the unconstrained minimizer $p_u = -1/3$. This point is $p = -\\Delta$.\nAlternatively, the optimal step should be in the opposite direction of the gradient ($g=2 > 0$), so the step should be negative.\n\nThus, the optimal trust-region step is:\n$$\np^{\\star} = -\\Delta = -0.1.\n$$\nWe can verify this using the Karush-Kuhn-Tucker (KKT) conditions. The solution $p = -0.1$ requires a Lagrange multiplier $\\lambda \\ge 0$ such that $(H + 2 \\lambda) p = -g$.\n$$\n(6 + 2 \\lambda)(-0.1) = -2 \\quad \\Rightarrow \\quad 6 + 2 \\lambda = 20 \\quad \\Rightarrow \\quad 2 \\lambda = 14 \\quad \\Rightarrow \\quad \\lambda = 7.\n$$\nSince $\\lambda = 7 \\ge 0$, the KKT conditions are satisfied, confirming that $p=-0.1$ is the optimal step.", "answer": "$$\\boxed{-0.1}$$", "id": "2224504"}, {"introduction": "Having grasped the basic concept in one dimension, we now move to a more practical scenario in computational finance: portfolio optimization in two dimensions. This exercise introduces the dogleg method, an elegant and efficient way to approximate the solution to the trust-region subproblem by tracing a path between the steepest-descent direction (the Cauchy point) and the full Newton step [@problem_id:2444773]. By working through a concrete financial example, you will gain a deeper, more visual understanding of how this popular method navigates the optimization landscape to find a robust step.", "problem": "Consider a two-asset meanâ€“variance objective used in computational finance. Let the unconstrained objective in weights $w \\in \\mathbb{R}^{2}$ be\n$$\nf(w) \\;=\\; \\frac{\\gamma}{2}\\, w^{\\top} \\Sigma \\, w \\;-\\; \\mu^{\\top} w,\n$$\nwith risk-aversion parameter $\\gamma = 2$, expected excess returns $\\mu = \\begin{pmatrix}0.5 \\\\ 0.2\\end{pmatrix}$, and a symmetric positive definite matrix\n$$\n\\Sigma \\;=\\; \\begin{pmatrix}2 & 1 \\\\ 1 & 3\\end{pmatrix}.\n$$\nAt the current iterate $w^{0} = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$, consider the second-order model\n$$\nm(s) \\;=\\; f(w^{0}) \\;+\\; \\nabla f(w^{0})^{\\top} s \\;+\\; \\frac{1}{2}\\, s^{\\top} \\nabla^{2} f(w^{0}) \\, s,\n$$\nand an Euclidean trust region $\\{\\, s \\in \\mathbb{R}^{2} : \\|s\\|_{2} \\le \\Delta \\,\\}$ with radius $\\Delta = 0.11$. Using the dogleg trust-region strategy applied to $m(s)$, determine the unique scalar $\\tau \\in [0,1]$ such that the dogleg step lies exactly on the trust-region boundary when the Cauchy point is strictly inside the trust region and the full Newton step is strictly outside it.\n\nGive your final value of $\\tau$ as a decimal number rounded to four significant figures.", "solution": "The dogleg method constructs a path to approximate the minimizer of the quadratic model $m(s)$. This requires the gradient $g$ and Hessian $B$ of the objective function $f(w)$ at the current iterate $w^0$.\n\nThe gradient of the objective function is $\\nabla f(w) = \\gamma \\Sigma w - \\mu$, and the Hessian is $\\nabla^2 f(w) = \\gamma \\Sigma$. At the iterate $w^0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, the gradient $g$ and Hessian $B$ of the model are:\n$$\ng = \\nabla f(w^0) = -\\mu = \\begin{pmatrix} -0.5 \\\\ -0.2 \\end{pmatrix}\n$$\n$$\nB = \\nabla^2 f(w^0) = \\gamma \\Sigma = 2 \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} = \\begin{pmatrix} 4 & 2 \\\\ 2 & 6 \\end{pmatrix}\n$$\nThe dogleg path is constructed using the Cauchy point $s_C$ and the Newton step $s_N$.\n\nFirst, we find the unconstrained Cauchy step, $s_{UC}$, which minimizes $m(s)$ along the steepest descent direction $-g$:\n$$\ns_{UC} = -\\alpha g, \\quad \\text{where} \\quad \\alpha = \\frac{g^{\\top}g}{g^{\\top}Bg}\n$$\nWe compute the necessary scalars:\n$g^{\\top}g = (-0.5)^2 + (-0.2)^2 = 0.29$\n$g^{\\top}Bg = \\begin{pmatrix} -0.5 & -0.2 \\end{pmatrix} \\begin{pmatrix} 4 & 2 \\\\ 2 & 6 \\end{pmatrix} \\begin{pmatrix} -0.5 \\\\ -0.2 \\end{pmatrix} = 1.64$\nSo, $\\alpha = \\frac{0.29}{1.64} = \\frac{29}{164}$. The norm of the unconstrained Cauchy step is $\\|s_{UC}\\|_2 = \\alpha \\|g\\|_2 = \\frac{29}{164}\\sqrt{0.29} \\approx 0.09523$. Since this is less than $\\Delta = 0.11$, the Cauchy point is inside the trust region, so $s_C = s_{UC}$.\n\nNext, we compute the Newton step, $s_N = -B^{-1}g$:\n$$\nB^{-1} = \\frac{1}{4 \\cdot 6 - 2 \\cdot 2} \\begin{pmatrix} 6 & -2 \\\\ -2 & 4 \\end{pmatrix} = \\frac{1}{20} \\begin{pmatrix} 6 & -2 \\\\ -2 & 4 \\end{pmatrix}\n$$\n$$\ns_N = -\\frac{1}{20} \\begin{pmatrix} 6 & -2 \\\\ -2 & 4 \\end{pmatrix} \\begin{pmatrix} -0.5 \\\\ -0.2 \\end{pmatrix} = \\begin{pmatrix} 0.13 \\\\ -0.01 \\end{pmatrix}\n$$\nThe norm is $\\|s_N\\|_2 = \\sqrt{0.13^2 + (-0.01)^2} = \\sqrt{0.017} \\approx 0.13038$. Since this is greater than $\\Delta$, the Newton step is outside the trust region.\n\nThe dogleg step $s_D$ must lie on the line segment connecting $s_C$ and $s_N$. We parameterize a point on this segment as $s(\\tau) = s_C + \\tau(s_N - s_C)$ for $\\tau \\in [0,1]$ and find $\\tau$ such that $\\|s(\\tau)\\|_2 = \\Delta$. This leads to the quadratic equation in $\\tau$:\n$$\n\\|s_N - s_C\\|_2^2 \\tau^2 + 2 s_C^{\\top}(s_N - s_C) \\tau + (\\|s_C\\|_2^2 - \\Delta^2) = 0\n$$\nLet the coefficients be $a, b, c$. Using high-precision values:\n$\\|s_C\\|_2^2 = (\\frac{29}{164})^2 (0.29) \\approx 0.0090679655$\n$s_C^{\\top}s_N = \\left(\\frac{29}{164}\\begin{pmatrix} 0.5 \\\\ 0.2 \\end{pmatrix}\\right)^{\\top} \\begin{pmatrix} 0.13 \\\\ -0.01 \\end{pmatrix} = \\frac{29}{164}(0.063) \\approx 0.0111402439$\n\nThe coefficients of the quadratic equation $a\\tau^2+b\\tau+c=0$ are:\n$a = \\|s_N - s_C\\|_2^2 = \\|s_N\\|_2^2 - 2 s_C^{\\top}s_N + \\|s_C\\|_2^2 \\approx 0.017 - 2(0.0111402439) + 0.0090679655 \\approx 0.0037874777$\n$b = 2 s_C^{\\top}(s_N - s_C) = 2(s_C^{\\top}s_N - \\|s_C\\|_2^2) \\approx 2(0.0111402439 - 0.0090679655) \\approx 0.0041445568$\n$c = \\|s_C\\|_2^2 - \\Delta^2 \\approx 0.0090679655 - 0.11^2 = -0.0030320345$\n\nWe solve for $\\tau$ using the quadratic formula $\\tau = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$ and take the positive root to ensure $\\tau \\in [0,1]$:\n$b^2 - 4ac \\approx (0.0041445568)^2 - 4(0.0037874777)(-0.0030320345) \\approx 6.311455 \\times 10^{-5}$\n$\\sqrt{b^2 - 4ac} \\approx 0.007944466$\n$$\n\\tau = \\frac{-0.0041445568 + 0.007944466}{2(0.0037874777)} = \\frac{0.0037999092}{0.0075749554} \\approx 0.501641\n$$\nRounding to four significant figures gives $\\tau \\approx 0.5016$.", "answer": "$$\n\\boxed{0.5016}\n$$", "id": "2444773"}, {"introduction": "Solving the subproblem is only one part of the trust-region algorithm; the other crucial part is adapting the size of the trust region itself. This final practice focuses on this adaptive mechanism, which is the key to the method's robustness [@problem_id:2444749]. You will evaluate how well our quadratic model predicted the actual function's behavior by calculating the acceptance ratio $\\rho_k$, and based on this \"reality check,\" decide whether to shrink or expand the trust-region radius $\\Delta_k$ for the next iteration. This process prevents the algorithm from taking poor steps when the model is not a good local approximation to the true objective function.", "problem": "Consider a calibration subproblem that arises in Maximum Likelihood Estimation (MLE) for a binary choice model in computational finance. Let the per-observation negative log-likelihood for a Bernoulli outcome be modeled as a function of a single scalar index $x$ by\n$$\nf(x) \\;=\\; \\ln\\!\\big(1+\\exp(x)\\big) \\;-\\; \\bar{y}\\,x,\n$$\nwhere $\\bar{y}\\in(0,1)$ is the sample mean of observed outcomes. At trust-region iteration $k$, suppose the current iterate is $x_k = 0$, and the sample mean is $\\bar{y} = 0.51$. The trust-region quadratic model about $x_k$ is\n$$\nm_k(p) \\;=\\; f(x_k) \\;+\\; g_k\\,p \\;+\\; \\tfrac{1}{2}\\,B_k\\,p^2,\n$$\nwith gradient $g_k = \\nabla f(x_k)$ and a symmetric Hessian approximation $B_k$. Assume $B_k = 0.01$, which severely underestimates the true curvature at $x_k$. The trust-region radius is $\\Delta_k = 1$, and the step used is the Cauchy point along the negative gradient direction, namely in one dimension\n$$\np_k \\;=\\; \\Delta_k\\,\\mathrm{sign}(-g_k).\n$$\nDefine the trust-region acceptance ratio\n$$\n\\rho_k \\;=\\; \\frac{f(x_k) - f(x_k + p_k)}{m_k(0) - m_k(p_k)}.\n$$\nThe trust-region radius update policy is\n- if $\\rho_k < 0$, set $\\Delta_{k+1} = 0.1\\,\\Delta_k$,\n- if $0 \\le \\rho_k < 0.25$, set $\\Delta_{k+1} = 0.5\\,\\Delta_k$,\n- if $\\rho_k \\ge 0.25$, set $\\Delta_{k+1} = 2\\,\\Delta_k$.\n\nCompute $\\Delta_{k+1}$. Express your final answer as a pure number. No rounding is required.", "solution": "To find the next trust-region radius, $\\Delta_{k+1}$, we must first compute the acceptance ratio $\\rho_k$. This ratio compares the actual reduction in the objective function to the predicted reduction from the model.\n$$\n\\rho_k \\;=\\; \\frac{\\text{Actual Reduction}}{\\text{Predicted Reduction}} = \\frac{f(x_k) - f(x_k + p_k)}{m_k(0) - m_k(p_k)}\n$$\nThe objective function is $f(x) = \\ln(1+\\exp(x)) - 0.51x$. At the current iterate $x_k=0$, the function value is:\n$$\nf(x_k) = f(0) = \\ln(1+\\exp(0)) - 0.51(0) = \\ln(2)\n$$\nTo find the step $p_k$, we compute the gradient $g_k = f'(x_k)$. The derivative of $f(x)$ is:\n$$\nf'(x) = \\frac{\\exp(x)}{1+\\exp(x)} - 0.51\n$$\nAt $x_k=0$, the gradient is:\n$$\ng_k = f'(0) = \\frac{\\exp(0)}{1+\\exp(0)} - 0.51 = 0.5 - 0.51 = -0.01\n$$\nThe step is given as $p_k = \\Delta_k\\,\\mathrm{sign}(-g_k)$. With $\\Delta_k=1$:\n$$\np_k = 1 \\cdot \\mathrm{sign}(-(-0.01)) = \\mathrm{sign}(0.01) = 1\n$$\nThe new point is $x_k + p_k = 0 + 1 = 1$. The actual reduction is the change in the function value:\n$$\n\\text{Actual Reduction} = f(0) - f(1) = \\ln(2) - \\left( \\ln(1+e) - 0.51 \\right) = \\ln(2) - \\ln(1+e) + 0.51\n$$\nNext, we compute the predicted reduction from the quadratic model. Since $m_k(0) = f(x_k)$, the predicted reduction is:\n$$\n\\text{Predicted Reduction} = m_k(0) - m_k(p_k) = -(g_k p_k + \\frac{1}{2} B_k p_k^2)\n$$\nUsing the values $g_k = -0.01$, $p_k=1$, and $B_k=0.01$:\n$$\n\\text{Predicted Reduction} = -(-0.01)(1) - \\frac{1}{2}(0.01)(1)^2 = 0.01 - 0.005 = 0.005\n$$\nThe acceptance ratio is $\\rho_k = \\frac{\\ln(2) - \\ln(1+e) + 0.51}{0.005}$. The denominator is positive, so the sign of $\\rho_k$ is determined by the numerator. We know $e \\approx 2.718$, $\\ln(2) \\approx 0.693$, and $\\ln(1+e) \\approx \\ln(3.718) \\approx 1.313$.\nThe numerator is approximately $0.693 - 1.313 + 0.51 = -0.11$.\nSince the numerator is negative and the denominator is positive, the ratio $\\rho_k$ is negative.\n\nNow we apply the trust-region radius update policy:\n- if $\\rho_k < 0$, set $\\Delta_{k+1} = 0.1\\,\\Delta_k$,\n- if $0 \\le \\rho_k < 0.25$, set $\\Delta_{k+1} = 0.5\\,\\Delta_k$,\n- if $\\rho_k \\ge 0.25$, set $\\Delta_{k+1} = 2\\,\\Delta_k$.\n\nAs we established that $\\rho_k  0$, the first rule applies. Given the current radius $\\Delta_k = 1$, the new radius is:\n$$\n\\Delta_{k+1} = 0.1 \\cdot \\Delta_k = 0.1 \\cdot 1 = 0.1\n$$", "answer": "$$\n\\boxed{0.1}\n$$", "id": "2444749"}]}