{"hands_on_practices": [{"introduction": "二阶 Møller-Plesset 微扰理论 ($MP2$) 是一个广泛使用的电子相关方法，但它在某些情况下存在系统性偏差。特别是在处理具有显著自旋污染的开壳层体系时，非限制性 $MP2$ ($UMP2$) 往往会高估同自旋电子对的相关能。本实践旨在通过一个简化的数值实验，清晰地揭示这一问题，并展示自旋分量缩放 ($SCS-MP2$) 方法如何通过对同自旋和异自旋相关能分量进行经验性调整来有效减少误差。通过这个练习，你将亲手验证 $SCS-MP2$ 的核心思想及其在修正 $UMP2$ 缺陷方面的效用。[@problem_id:2926383]", "problem": "构建一个最小化的、自洽的数值实验，以阐明非限制二阶 Møller–Plesset 微扰理论 (UMP2) 在参考行列式破坏自旋对称性时会如何过高估计同自旋相关，以及自旋分量标度 Møller–Plesset 微扰理论 (SCS-MP2) 如何减少由此产生的误差。您的程序必须从第一性原理出发实现以下内容。\n\n从用于二阶相关能的 Rayleigh–Schrödinger 微扰理论开始。在自旋轨道基组中，二阶相关能是关于参考行列式双激发的求和，每一项是耦合矩阵元的平方除以激发能差。在一种适合编程的数据驱动抽象中，我们将其建模为两组不相交的贡献：\n- 同自旋组，其中两个激发电子具有相同的自旋，以及\n- 异自旋组，其中激发电子具有相反的自旋。\n\n对于每个激发类别，您会得到：\n- 一个正激发能差列表，记为 $\\Delta_k$，以及\n- 一个非负耦合权重平方列表，记为 $w_k = \\lvert \\langle \\Phi_k \\lvert \\hat{V} \\rvert \\Phi_0 \\rangle \\rvert^2$。\n\n使用二阶微扰能量定义，并将 $\\Delta_k$ 视为正激发能隙，来自类别 $\\mathcal{C}$ 的相关能贡献为\n$$\nE_{\\mathcal{C}} = - \\sum_{k \\in \\mathcal{C}} \\frac{w_k}{\\Delta_k}.\n$$\n因此，非限制二阶相关能为 $E_{\\text{UMP2}} = E_{\\text{os}} + E_{\\text{ss}}$，其中 $E_{\\text{os}}$ 和 $E_{\\text{ss}}$ 分别表示异自旋和同自旋的总和。\n\n自旋分量标度 Møller–Plesset 微扰理论 (SCS-MP2) 用自旋分辨分量的线性组合取代了未标度的总和，\n$$\nE_{\\text{SCS}} = c_{\\text{os}} \\, E_{\\text{os}} + c_{\\text{ss}} \\, E_{\\text{ss}},\n$$\n其中使用固定的经验系数 $c_{\\text{os}}$ 和 $c_{\\text{ss}}$。使用标准值 $c_{\\text{os}} = 1.20$ 和 $c_{\\text{ss}} = 0.33$。\n\n为您提供一个包含三个案例的测试套件。每个案例指定：\n- 异自旋的权重 $w_k$ 和分母 $\\Delta_k$ 列表，\n- 同自旋的权重 $w_k$ 和分母 $\\Delta_k$ 列表，以及\n- 代表高级基准（例如，最小空间中的全组态相互作用）的参考相关能 $E_{\\text{ref}}$。\n\n所有能量都以 hartree 为单位进行计算和报告。对于每个案例，您的程序必须计算：\n- 非限制二阶相关能 $E_{\\text{UMP2}}$，\n- 自旋分量标度相关能 $E_{\\text{SCS}}$，\n- UMP2 的绝对误差 $\\lvert E_{\\text{UMP2}} - E_{\\text{ref}} \\rvert$，\n- SCS-MP2 的绝对误差 $\\lvert E_{\\text{SCS}} - E_{\\text{ref}} \\rvert$，以及\n- 一个布尔标志，指示与 UMP2 相比，SCS-MP2 是否严格减小了绝对误差。\n\n测试套件（除特别注明外，所示各数值均为无量纲；所有能量均以 hartree 报告）：\n- 案例 A（行为良好，类似近平衡态）：\n  - 异自旋：权重 $[0.010, 0.006]$，分母 $[0.50, 0.60]$。\n  - 同自旋：权重 $[0.0005]$，分母 $[0.55]$。\n  - 参考相关能 $E_{\\text{ref}} = -0.0315$ hartree。\n- 案例 B（自旋破缺，同自旋振幅虚高）：\n  - 异自旋：权重 $[0.018, 0.014]$，分母 $[0.30, 0.35]$。\n  - 同自旋：权重 $[0.012, 0.010]$，分母 $[0.32, 0.36]$。\n  - 参考相关能 $E_{\\text{ref}} = -0.1100$ hartree。\n- 案例 C（边缘案例，同自旋通道中存在近简并）：\n  - 异自旋：权重 $[0.010, 0.008]$，分母 $[0.25, 0.30]$。\n  - 同自旋：权重 $[0.0020, 0.0015]$，分母 $[0.050, 0.060]$。\n  - 参考相关能 $E_{\\text{ref}} = -0.0850$ hartree。\n\n角度单位不适用。不使用百分比。\n\n您的程序必须生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个案例贡献一个形如 $[e_u, e_s, b]$ 的子列表，其中 $e_u$ 等于四舍五入到六位小数的 UMP2 绝对误差，$e_s$ 等于四舍五入到六位小数的 SCS-MP2 绝对误差，而 $b$ 等于布尔值 true 当且仅当 SCS-MP2 严格减小了绝对误差。例如，整体格式必须为\n$[[e_{u,A}, e_{s,A}, b_A],[e_{u,B}, e_{s,B}, b_B],[e_{u,C}, e_{s,C}, b_C]]$，\n打印在一行上，不含任何附加文本。", "solution": "所述问题是有效的。它在科学上基于量子力学微扰理论的原理，特别是二阶 Møller–Plesset 理论 (MP2) 及其经验变体，如自旋分量标度 MP2 (SCS-MP2)。该问题是适定的，为获得唯一的数值解提供了完整且一致的定义、公式和数据。它避免了主观性和模糊性。我们将着手求解。\n\n此分析的基础是 Rayleigh–Schrödinger 微扰理论，它为能量的二阶校正提供了表达式。在电子结构理论的框架内，这个被称为相关能的校正，解释了超出单一参考行列式 $\\Phi_0$ 的平均场近似的电子-电子相互作用。对于一个由非限制 Hartree-Fock (UHF) 描述的体系，二阶相关能 $E_{\\text{UMP2}}$ 由从占据自旋轨道 $i, j$ 到空自旋轨道 $a, b$ 的所有双激发行列式 $\\Phi_{ij}^{ab}$ 的求和给出：\n$$\nE_{\\text{UMP2}} = \\sum_{i<j} \\sum_{a<b} \\frac{\\lvert \\langle \\Phi_0 \\lvert \\hat{H} \\rvert \\Phi_{ij}^{ab} \\rangle \\rvert^2}{\\epsilon_i + \\epsilon_j - \\epsilon_a - \\epsilon_b}\n$$\n其中 $\\epsilon_p$ 是自旋轨道 $p$ 的能量。\n\n问题提供了此公式的一个抽象。我们将正激发能分母定义为 $\\Delta_k = \\epsilon_a + \\epsilon_b - \\epsilon_i - \\epsilon_j > 0$，将耦合权重平方定义为 $w_k = \\lvert \\langle \\Phi_0 \\lvert \\hat{H} \\rvert \\Phi_{ij}^{ab} \\rangle \\rvert^2$。那么，来自一组激发 $\\mathcal{C}$ 的相关能贡献为：\n$$\nE_{\\mathcal{C}} = - \\sum_{k \\in \\mathcal{C}} \\frac{w_k}{\\Delta_k}\n$$\n电子结构理论中的一个关键见解是，异自旋电子与同自旋电子的相关效应不同。因此，我们可以将总相关能划分为异自旋 (os) 和同自旋 (ss) 分量：\n$$\nE_{\\text{UMP2}} = E_{\\text{os}} + E_{\\text{ss}}\n$$\n其中 $E_{\\text{os}}$ 是对涉及一个 α 和一个 β 电子的激发的求和，而 $E_{\\text{ss}}$ 是对涉及两个 α 电子或两个 β 电子的激发的求和。\n\nUMP2 方法的一个已知缺陷是，在具有显著自旋污染的体系中——即 UHF 参考行列式不是总自旋算符 $\\hat{S}^2$ 的本征函数——同自旋分量 $E_{\\text{ss}}$ 通常被系统性地过高估计。这导致总相关能过负。\n\n自旋分量标度 Møller–Plesset 微扰理论 (SCS-MP2) 通过为各自的自旋分量引入经验性确定的标度因子 $c_{\\text{os}}$ 和 $c_{\\text{ss}}$ 来解决这个问题：\n$$\nE_{\\text{SCS}} = c_{\\text{os}} E_{\\text{os}} + c_{\\text{ss}} E_{\\text{ss}}\n$$\n我们被要求使用的标准参数是 $c_{\\text{os}} = 1.20$ 和 $c_{\\text{ss}} = 0.33$。MP2 倾向于低估的异自旋分量被放大。MP2 倾向于高估的同自旋分量（尤其是在自旋污染的情况下）被缩小。\n\n每个测试案例的计算步骤如下：\n1.  对于给定的异自旋权重集 $\\{w_{k, \\text{os}}\\}$ 和分母集 $\\{\\Delta_{k, \\text{os}}\\}$，计算异自旋相关能分量：\n    $$\n    E_{\\text{os}} = - \\sum_{k} \\frac{w_{k, \\text{os}}}{\\Delta_{k, \\text{os}}}\n    $$\n2.  同样地，对于同自旋数据 $\\{w_{k, \\text{ss}}\\}$ 和 $\\{\\Delta_{k, \\text{ss}}\\}$，计算同自旋分量：\n    $$\n    E_{\\text{ss}} = - \\sum_{k} \\frac{w_{k, \\text{ss}}}{\\Delta_{k, \\text{ss}}}\n    $$\n3.  计算总 UMP2 相关能：\n    $$\n    E_{\\text{UMP2}} = E_{\\text{os}} + E_{\\text{ss}}\n    $$\n4.  使用给定的标度参数计算 SCS-MP2 相关能：\n    $$\n    E_{\\text{SCS}} = (1.20) E_{\\text{os}} + (0.33) E_{\\text{ss}}\n    $$\n5.  使用提供的参考相关能 $E_{\\text{ref}}$，计算两种方法的绝对误差：\n    $$\n    \\epsilon_{\\text{UMP2}} = \\lvert E_{\\text{UMP2}} - E_{\\text{ref}} \\rvert\n    $$\n    $$\n    \\epsilon_{\\text{SCS}} = \\lvert E_{\\text{SCS}} - E_{\\text{ref}} \\rvert\n    $$\n6.  最后，评估布尔条件 $\\epsilon_{\\text{SCS}} < \\epsilon_{\\text{UMP2}}$，以确定对于给定案例，SCS-MP2 是否比 UMP2 提供了严格的改进。\n\n此程序将应用于所提供的三个不同案例，它们旨在模拟一个行为良好的体系（案例 A）、一个具有人为夸大的同自旋振幅的自旋污染体系（案例 B），以及一个由于近简并导致标准微扰理论失效的体系（案例 C）。所构建的数值实验正确地检验了这样一个假设：在 UMP2 失效的有问题的案例中，降低同自旋相关的权重可以提高准确性。案例 A 作为对照，表明这种经验标度并非万能药，在原始 MP2 公式更适用的行为良好体系中可能会降低准确性。案例 B 和 C 展示了 SCS-MP2 设计用来表现优异的场景。实现必须精确遵循这些步骤。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy is not needed for this problem.\n\ndef solve():\n    \"\"\"\n    Solves the computational chemistry problem comparing UMP2 and SCS-MP2.\n    \"\"\"\n    \n    # Define the scaling coefficients for SCS-MP2.\n    c_os = 1.20\n    c_ss = 0.33\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"os_weights\": [0.010, 0.006],\n            \"os_denominators\": [0.50, 0.60],\n            \"ss_weights\": [0.0005],\n            \"ss_denominators\": [0.55],\n            \"e_ref\": -0.0315\n        },\n        {\n            \"name\": \"Case B\",\n            \"os_weights\": [0.018, 0.014],\n            \"os_denominators\": [0.30, 0.35],\n            \"ss_weights\": [0.012, 0.010],\n            \"ss_denominators\": [0.32, 0.36],\n            \"e_ref\": -0.1100\n        },\n        {\n            \"name\": \"Case C\",\n            \"os_weights\": [0.010, 0.008],\n            \"os_denominators\": [0.25, 0.30],\n            \"ss_weights\": [0.0020, 0.0015],\n            \"ss_denominators\": [0.050, 0.060],\n            \"e_ref\": -0.0850\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Convert lists to numpy arrays for vectorized operations.\n        os_weights = np.array(case[\"os_weights\"])\n        os_denominators = np.array(case[\"os_denominators\"])\n        ss_weights = np.array(case[\"ss_weights\"])\n        ss_denominators = np.array(case[\"ss_denominators\"])\n        e_ref = case[\"e_ref\"]\n\n        # Calculate the opposite-spin and same-spin correlation energy components.\n        # E_C = - sum(w_k / Delta_k)\n        e_os = -np.sum(os_weights / os_denominators)\n        e_ss = -np.sum(ss_weights / ss_denominators)\n        \n        # Calculate the UMP2 correlation energy.\n        # E_UMP2 = E_os + E_ss\n        e_ump2 = e_os + e_ss\n        \n        # Calculate the SCS-MP2 correlation energy.\n        # E_SCS = c_os * E_os + c_ss * E_ss\n        e_scs = c_os * e_os + c_ss * e_ss\n        \n        # Calculate the absolute errors with respect to the reference energy.\n        error_ump2 = abs(e_ump2 - e_ref)\n        error_scs = abs(e_scs - e_ref)\n        \n        # Determine if SCS-MP2 strictly reduces the error.\n        is_scs_better = error_scs < error_ump2\n        \n        # Format the results for this case.\n        # Errors are rounded to six decimal places.\n        # Boolean is converted to lowercase string 'true' or 'false'.\n        case_result = [\n            round(error_ump2, 6),\n            round(error_scs, 6),\n            is_scs_better\n        ]\n        results.append(case_result)\n\n    # Format the final output string as specified in the problem.\n    # e.g., [[e_u,A, e_s,A, b_A],[e_u,B, e_s,B, b_B],...]\n    # The string representation of a list of lists in Python is very close to the target.\n    # The only change needed is converting boolean 'True'/'False' to 'true'/'false'.\n    result_strings = []\n    for res in results:\n        # res[0] is e_u, res[1] is e_s, res[2] is boolean b\n        result_strings.append(f\"[{res[0]}, {res[1]}, {str(res[2]).lower()}]\")\n\n    # Join the sublist strings with a comma and enclose in brackets.\n    final_output = f\"[{','.join(result_strings)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "2926383"}, {"introduction": "在理解了自旋分量缩放的基本原理后，一个自然的问题是：这些缩放系数 $c_{\\mathrm{OS}}$ 和 $c_{\\mathrm{SS}}$ 是如何确定的？$SCS-MP2$ 及许多现代量子化学方法都依赖于经验参数，这些参数是通过在可靠的参考数据集上进行拟合得到的。本实践将引导你模拟这一参数化过程，你将使用包含不同化学问题（非共价相互作用、反应能垒和热化学）的数据集，通过岭回归方法来优化一个包含 $D3$ 色散校正的自旋分量缩放模型的系数。这个练习将让你深入了解这些经验校正模型的构建过程及其对训练数据类型的敏感性。[@problem_id:2926415]", "problem": "本题要求您研究，在一个带有经验色散项的自旋分量标度二阶 Møller–Plesset 相关模型中，将一个仅包含非共价相互作用的校准集扩展为一个同时包含能垒高度和热化学数据的更广泛的数据集时，最优系数会如何变化。该模型对所关注的能量量的相关贡献使用以下线性拟设 (linear ansatz)：\n$$\n\\hat{y} = c_{\\mathrm{OS}}\\,x_{\\mathrm{OS}} + c_{\\mathrm{SS}}\\,x_{\\mathrm{SS}} + s_{6}\\,x_{\\mathrm{D3}} + c_{0},\n$$\n其中，$x_{\\mathrm{OS}}$ 是反向自旋二阶 Møller–Plesset (MP2) 相关分量，$x_{\\mathrm{SS}}$ 是同向自旋 MP2 相关分量，$x_{\\mathrm{D3}}$ 是一个预先计算的 Grimme 第三代色散校正 (D3)，$c_{\\mathrm{OS}}$ 和 $c_{\\mathrm{SS}}$ 是自旋分量标度系数，$s_{6}$ 是一个经验色散标度，$c_{0}$ 是一个截距。以下所有类能量输入的单位均为千卡/摩尔 (kcal/mol)。您必须以无单位的浮点值形式报告最终的数值输出。\n\n请从第一性原理出发，通过最小化带有对非截距项系数施加的 $\\ell_{2}$ 型（岭）惩罚项的残差平方和来确定最优系数。具体而言，对于一个给定的非负惩罚参数 $\\lambda$，最小化\n$$\nJ(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N}\\left(\\hat{y}_{i} - y_{i}\\right)^{2} + \\lambda\\left(c_{\\mathrm{OS}}^{2} + c_{\\mathrm{SS}}^{2} + s_{6}^{2}\\right),\n$$\n在上述模型的约束下进行，其中 $y_{i}$ 是所提供的参考目标值，且 $\\boldsymbol{\\theta} = \\left[c_{\\mathrm{OS}},\\,c_{\\mathrm{SS}},\\,s_{6},\\,c_{0}\\right]^{\\top}$。截距 $c_{0}$ 不得被惩罚。当 $\\lambda=0$ 时，使用普通最小二乘法。\n\n提供了三类数据：非共价相互作用 (NC)、能垒高度 (BH) 和热化学 (TC)。每个数据点都是一个四元组 $(x_{\\mathrm{OS}},x_{\\mathrm{SS}},x_{\\mathrm{D3}},y)$，单位均为 kcal/mol。请使用以下数值：\n\n- NC1: $\\left(-3.2,\\,-1.1,\\,-0.8,\\,-5.92\\right)$\n- NC2: $\\left(-2.5,\\,-0.9,\\,-0.6,\\,-4.62\\right)$\n- NC3: $\\left(-4.1,\\,-1.5,\\,-1.1,\\,-7.64\\right)$\n- NC4: $\\left(-1.8,\\,-0.6,\\,-0.4,\\,-3.31\\right)$\n- NC5: $\\left(-5.0,\\,-1.8,\\,-1.3,\\,-9.27\\right)$\n- NC6: $\\left(-2.9,\\,-1.0,\\,-0.7,\\,-5.35\\right)$\n\n- BH1: $\\left(-12.0,\\,-4.5,\\,-0.1,\\,-15.375\\right)$\n- BH2: $\\left(-9.0,\\,-3.2,\\,0.0,\\,-11.47\\right)$\n- BH3: $\\left(-15.0,\\,-5.6,\\,0.2,\\,-19.21\\right)$\n- BH4: $\\left(-7.5,\\,-2.7,\\,0.0,\\,-9.57\\right)$\n\n- TC1: $\\left(-25.0,\\,-9.0,\\,0.0,\\,-30.30\\right)$\n- TC2: $\\left(-18.0,\\,-6.5,\\,0.0,\\,-21.825\\right)$\n- TC3: $\\left(-30.0,\\,-11.0,\\,0.1,\\,-36.45\\right)$\n- TC4: $\\left(-22.0,\\,-8.0,\\,0.0,\\,-26.70\\right)$\n\n对于下面给出的测试集中的每个惩罚参数，计算两组最优系数：\n1) 仅使用 NC 数据 (NC1–NC6) 进行训练。\n2) 使用包括 NC、BH 和 TC 的组合数据集（全部十四个条目）进行训练。\n\n然后，对于每个惩罚参数，计算系数偏移向量\n$$\n\\Delta(\\lambda) = \\boldsymbol{\\theta}_{\\mathrm{all}}(\\lambda) - \\boldsymbol{\\theta}_{\\mathrm{NC}}(\\lambda),\n$$\n其中 $\\boldsymbol{\\theta}_{\\mathrm{all}}(\\lambda)$ 是在组合数据集上训练得到的，而 $\\boldsymbol{\\theta}_{\\mathrm{NC}}(\\lambda)$ 是在仅 NC 数据集上训练得到的。请按 $\\left[\\Delta c_{\\mathrm{OS}},\\,\\Delta c_{\\mathrm{SS}},\\,\\Delta s_{6},\\,\\Delta c_{0}\\right]$ 的顺序报告 $\\Delta(\\lambda)$ 的四个分量，并同时报告其欧几里得范数\n$$\n\\left\\|\\Delta(\\lambda)\\right\\|_{2} = \\sqrt{\\left(\\Delta c_{\\mathrm{OS}}\\right)^{2} + \\left(\\Delta c_{\\mathrm{SS}}\\right)^{2} + \\left(\\Delta s_{6}\\right)^{2} + \\left(\\Delta c_{0}\\right)^{2}}.\n$$\n\n岭惩罚参数测试集：\n- $\\lambda = 0$\n- $\\lambda = 10^{-4}$\n- $\\lambda = 10^{-2}$\n- $\\lambda = 1$\n- $\\lambda = 10^{3}$\n\n您的程序必须：\n- 构建设计矩阵，并为截距项添加一列全为 1 的列。\n- 求解带惩罚的最小二乘问题，其中只有前三个系数被惩罚。\n- 对于测试集中的每个 $\\lambda$，计算并收集 $\\left[\\Delta c_{\\mathrm{OS}},\\,\\Delta c_{\\mathrm{SS}},\\,\\Delta s_{6},\\,\\Delta c_{0},\\,\\left\\|\\Delta\\right\\|_{2}\\right]$。\n\n最终输出格式：\n- 生成单行输出，其中包含一个以逗号分隔的列表的列表，每个内部列表对应一个惩罚值，并按测试集的顺序排列。每个内部列表必须按 $\\left[\\Delta c_{\\mathrm{OS}},\\,\\Delta c_{\\mathrm{SS}},\\,\\Delta s_{6},\\,\\Delta c_{0},\\,\\left\\|\\Delta\\right\\|_{2}\\right]$ 的顺序包含五个浮点数。\n- 输出中的所有浮点数必须四舍五入到六位小数。\n- 格式示例（仅为说明）：$[[a_{1},b_{1},c_{1},d_{1},e_{1}],[a_{2},b_{2},c_{2},d_{2},e_{2}],\\dots]$。", "solution": "所呈现的问题陈述是有效的。它描述了计算化学中的一个明确定义的任务，具体是使用岭回归对一个半经验能量模型进行参数化。该模型具有物理动机，其目标函数在数学上是合理的，并且所有必要的数据和约束都已提供。此问题是自洽的、有科学依据的、并且是客观的。它是带惩罚的线性回归的一个标准应用，无需任何无效假设。\n\n该任务是通过最小化一个包含 $\\ell_{2}$ 惩罚项（岭回归）的残差平方和目标函数，来确定一个线性模型的最佳系数。此分析将针对两种不同的训练集和一系列惩罚参数 $\\lambda$ 进行。\n\n用于预测能量 $\\hat{y}$ 的线性模型由以下拟设 (ansatz) 给出：\n$$\n\\hat{y} = c_{\\mathrm{OS}}\\,x_{\\mathrm{OS}} + c_{\\mathrm{SS}}\\,x_{\\mathrm{SS}} + s_{6}\\,x_{\\mathrm{D3}} + c_{0}\n$$\n此处，$\\boldsymbol{\\theta} = \\begin{bmatrix} c_{\\mathrm{OS}} & c_{\\mathrm{SS}} & s_{6} & c_{0} \\end{bmatrix}^{\\top}$ 是待确定的参数向量。对于一组 $N$ 个数据点，这可以表示为矩阵形式：\n$$\n\\mathbf{\\hat{y}} = \\mathbf{X}\\boldsymbol{\\theta}\n$$\n其中 $\\mathbf{\\hat{y}} \\in \\mathbb{R}^{N}$ 是预测值向量，$\\mathbf{X} \\in \\mathbb{R}^{N \\times 4}$ 是设计矩阵，$\\boldsymbol{\\theta} \\in \\mathbb{R}^{4}$ 是参数向量。$\\mathbf{X}$ 的每一行对应一个数据点 $(x_{\\mathrm{OS},i}, x_{\\mathrm{SS},i}, x_{\\mathrm{D3},i})$，并附带第四列为 1 以对应截距项 $c_0$ 。向量 $\\mathbf{y} \\in \\mathbb{R}^{N}$ 包含相应的参考目标值。\n\n目标是找到参数向量 $\\boldsymbol{\\theta}$ 以最小化代价函数 $J(\\boldsymbol{\\theta})$：\n$$\nJ(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N}\\left(\\hat{y}_{i} - y_{i}\\right)^{2} + \\lambda\\left(c_{\\mathrm{OS}}^{2} + c_{\\mathrm{SS}}^{2} + s_{6}^{2}\\right)\n$$\n截距 $c_{0}$ 不被惩罚。该代价函数可以用矩阵表示法写为：\n$$\nJ(\\boldsymbol{\\theta}) = (\\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y})^{\\top}(\\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y}) + \\lambda \\boldsymbol{\\theta}^{\\top}\\mathbf{\\Gamma}\\boldsymbol{\\theta}\n$$\n其中 $\\mathbf{y}$ 是参考值向量，$\\mathbf{\\Gamma}$ 是一个 $4 \\times 4$ 的对角矩阵，用于选择要惩罚的系数。由于只有 $c_{\\mathrm{OS}}$、$c_{\\mathrm{SS}}$ 和 $s_{6}$ 被惩罚，$\\mathbf{\\Gamma}$ 为：\n$$\n\\mathbf{\\Gamma} = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}\n$$\n\n为求 $J(\\boldsymbol{\\theta})$ 的最小值，我们计算其关于 $\\boldsymbol{\\theta}$ 的梯度并使其为零向量：\n$$\n\\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) = \\nabla_{\\boldsymbol{\\theta}} \\left( (\\mathbf{X}\\boldsymbol{\\theta})^{\\top}(\\mathbf{X}\\boldsymbol{\\theta}) - 2(\\mathbf{X}\\boldsymbol{\\theta})^{\\top}\\mathbf{y} + \\mathbf{y}^{\\top}\\mathbf{y} + \\lambda\\boldsymbol{\\theta}^{\\top}\\mathbf{\\Gamma}\\boldsymbol{\\theta} \\right) = \\mathbf{0}\n$$\n$$\n\\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) = \\nabla_{\\boldsymbol{\\theta}} \\left( \\boldsymbol{\\theta}^{\\top}\\mathbf{X}^{\\top}\\mathbf{X}\\boldsymbol{\\theta} - 2\\boldsymbol{\\theta}^{\\top}\\mathbf{X}^{\\top}\\mathbf{y} + \\mathbf{y}^{\\top}\\mathbf{y} + \\lambda\\boldsymbol{\\theta}^{\\top}\\mathbf{\\Gamma}\\boldsymbol{\\theta} \\right) = \\mathbf{0}\n$$\n使用标准向量微积分法则，梯度为：\n$$\n2\\mathbf{X}^{\\top}\\mathbf{X}\\boldsymbol{\\theta} - 2\\mathbf{X}^{\\top}\\mathbf{y} + 2\\lambda\\mathbf{\\Gamma}\\boldsymbol{\\theta} = \\mathbf{0}\n$$\n整理各项，得到该岭回归问题的正规方程组：\n$$\n(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{\\Gamma})\\boldsymbol{\\theta} = \\mathbf{X}^{\\top}\\mathbf{y}\n$$\n这是一个 $\\mathbf{A}\\boldsymbol{\\theta} = \\mathbf{b}$ 形式的线性方程组，其中 $\\mathbf{A} = (\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{\\Gamma})$ 且 $\\mathbf{b} = \\mathbf{X}^{\\top}\\mathbf{y}$。最优参数向量 $\\boldsymbol{\\theta}$ 可以通过求解该方程组得到。对于 $\\lambda > 0$，矩阵 $\\mathbf{A}$ 保证是可逆的，从而提供唯一解。对于 $\\lambda = 0$，问题简化为普通最小二乘法，如果 $\\mathbf{X}^{\\top}\\mathbf{X}$ 可逆（即 $\\mathbf{X}$ 的列是线性无关的），则有唯一解。\n\n计算流程如下：\n1.  将提供的数据划分为两个集合：包含 $N_{\\mathrm{NC}} = 6$ 个点的非共价 (NC) 集，以及包含 $N_{\\mathrm{all}} = 14$ 个点的组合 (all) 集。\n2.  对每个数据集，构建设计矩阵 $\\mathbf{X}$ 和目标向量 $\\mathbf{y}$。对于 NC 集，得到 $\\mathbf{X}_{\\mathrm{NC}}$ ($6 \\times 4$) 和 $\\mathbf{y}_{\\mathrm{NC}}$ ($6 \\times 1$)。对于组合集，我们有 $\\mathbf{X}_{\\mathrm{all}}$ ($14 \\times 4$) 和 $\\mathbf{y}_{\\mathrm{all}}$ ($14 \\times 1$)。\n3.  对于测试集 $\\{0, 10^{-4}, 10^{-2}, 1, 10^{3}\\}$ 中的每个指定 $\\lambda$ 值：\n    a.  求解 $(\\mathbf{X}_{\\mathrm{NC}}^{\\top}\\mathbf{X}_{\\mathrm{NC}} + \\lambda\\mathbf{\\Gamma})\\boldsymbol{\\theta}_{\\mathrm{NC}} = \\mathbf{X}_{\\mathrm{NC}}^{\\top}\\mathbf{y}_{\\mathrm{NC}}$ 以找到仅 NC 数据集的最优参数 $\\boldsymbol{\\theta}_{\\mathrm{NC}}(\\lambda)$。\n    b.  求解 $(\\mathbf{X}_{\\mathrm{all}}^{\\top}\\mathbf{X}_{\\mathrm{all}} + \\lambda\\mathbf{\\Gamma})\\boldsymbol{\\theta}_{\\mathrm{all}} = \\mathbf{X}_{\\mathrm{all}}^{\\top}\\mathbf{y}_{\\mathrm{all}}$ 以找到组合数据集的最优参数 $\\boldsymbol{\\theta}_{\\mathrm{all}}(\\lambda)$。\n    c.  计算系数偏移向量 $\\Delta(\\lambda) = \\boldsymbol{\\theta}_{\\mathrm{all}}(\\lambda) - \\boldsymbol{\\theta}_{\\mathrm{NC}}(\\lambda)$。\n    d.  计算偏移向量的欧几里得范数 $\\|\\Delta(\\lambda)\\|_{2}$。\n4.  对于每个 $\\lambda$，将 $\\Delta(\\lambda)$ 的分量及其范数四舍五入到六位小数后收集起来，并格式化为最终输出。以下程序实现了此过程。", "answer": "```python\nimport numpy as np\n\ndef solve_ridge(X, y, lambda_val):\n    \"\"\"\n    Solves a ridge regression problem with a custom penalty.\n    The penalty is applied only to the first three coefficients.\n    \n    The problem is to find theta that minimizes:\n    ||X @ theta - y||^2 + lambda_val * (theta[0]^2 + theta[1]^2 + theta[2]^2)\n    \n    This corresponds to the normal equations:\n    (X.T @ X + lambda_val * Gamma) @ theta = X.T @ y\n    where Gamma = diag([1, 1, 1, 0]).\n    \"\"\"\n    num_params = X.shape[1]\n    \n    # Penalty matrix Gamma\n    Gamma = np.zeros((num_params, num_params))\n    Gamma[0, 0] = 1.0\n    Gamma[1, 1] = 1.0\n    Gamma[2, 2] = 1.0\n    \n    # Left-hand side of the normal equations\n    A = X.T @ X + lambda_val * Gamma\n    \n    # Right-hand side of the normal equations\n    b = X.T @ y\n    \n    # Solve the linear system A * theta = b\n    theta = np.linalg.solve(A, b)\n    \n    return theta\n\ndef solve():\n    \"\"\"\n    Main function to perform the analysis and print the results.\n    \"\"\"\n    # Data provided in the problem statement (x_OS, x_SS, x_D3, y)\n    nc_data_tuples = [\n        (-3.2, -1.1, -0.8, -5.92),\n        (-2.5, -0.9, -0.6, -4.62),\n        (-4.1, -1.5, -1.1, -7.64),\n        (-1.8, -0.6, -0.4, -3.31),\n        (-5.0, -1.8, -1.3, -9.27),\n        (-2.9, -1.0, -0.7, -5.35),\n    ]\n\n    bh_data_tuples = [\n        (-12.0, -4.5, -0.1, -15.375),\n        (-9.0, -3.2, 0.0, -11.47),\n        (-15.0, -5.6, 0.2, -19.21),\n        (-7.5, -2.7, 0.0, -9.57),\n    ]\n\n    tc_data_tuples = [\n        (-25.0, -9.0, 0.0, -30.30),\n        (-18.0, -6.5, 0.0, -21.825),\n        (-30.0, -11.0, 0.1, -36.45),\n        (-22.0, -8.0, 0.0, -26.70),\n    ]\n\n    all_data_tuples = nc_data_tuples + bh_data_tuples + tc_data_tuples\n\n    # Convert tuple data to numpy arrays\n    nc_data = np.array(nc_data_tuples)\n    all_data = np.array(all_data_tuples)\n\n    # Construct design matrices (X) and target vectors (y)\n    # The last column of X is for the intercept c0, so it's a column of ones.\n    X_nc = np.c_[nc_data[:, :3], np.ones(nc_data.shape[0])]\n    y_nc = nc_data[:, 3]\n    \n    X_all = np.c_[all_data[:, :3], np.ones(all_data.shape[0])]\n    y_all = all_data[:, 3]\n\n    # Test suite of ridge penalties\n    test_lambdas = [0.0, 1e-4, 1e-2, 1.0, 1e3]\n    \n    results = []\n    \n    for lambda_val in test_lambdas:\n        # 1) Train using only the NC data\n        theta_nc = solve_ridge(X_nc, y_nc, lambda_val)\n        \n        # 2) Train using the combined dataset\n        theta_all = solve_ridge(X_all, y_all, lambda_val)\n        \n        # 3) Compute the coefficient shift vector Delta\n        delta = theta_all - theta_nc\n        \n        # 4) Compute the Euclidean norm of Delta\n        norm_delta = np.linalg.norm(delta)\n        \n        # Collect the results: [delta_c_OS, delta_c_SS, delta_s6, delta_c0, norm_delta]\n        # Round to six decimal places as required.\n        current_result = [round(val, 6) for val in list(delta) + [norm_delta]]\n        results.append(current_result)\n        \n    # Format the final output string to match the required format exactly.\n    # The str() representation of a list of lists with spaces removed is correct.\n    output_string = str(results).replace(\" \", \"\")\n    \n    print(output_string)\n\nsolve()\n```", "id": "2926415"}, {"introduction": "经验模型的效果在很大程度上取决于用于校准它的训练数据，因此，评估模型参数及其预测的可靠性至关重要。仅仅给出一个预测值是不够的；理解该预测值的不确定性范围，即认知不确定性（epistemic uncertainty），对于科学研究中的严谨决策至关重要。本实践将介绍一种强大的统计工具——非参数自助法（nonparametric bootstrap），用于量化 $SCS-MP2$ 模型中的不确定性。你将通过对训练集进行重采样来估计模型参数的分布，并最终为测试分子的预测能量提供置信区间，从而学会如何严格评估数据驱动模型的预测可靠性。[@problem_id:2926422]", "problem": "给定一个数据集，用于校准一个自旋分量标度二阶 Møller–Plesset 微扰理论 (SCS-MP2) 经验模型，并通过非参数自助法量化其预测中的认知不确定性。设 SCS-MP2 相关能模型为一个带截距的线性预测器：\n$$\nE_{\\mathrm{pred}} = c_0 + c_{\\mathrm{OS}} E_{\\mathrm{OS}} + c_{\\mathrm{SS}} E_{\\mathrm{SS}},\n$$\n其中，$E_{\\mathrm{OS}}$ 和 $E_{\\mathrm{SS}}$ 分别是相反自旋和相同自旋的二阶 Møller–Plesset 微扰理论 (MP2) 分量，$E_{\\mathrm{pred}}$ 是预测的相关能。您将获得一个包含 $N$ 个分子的训练集，其中包含已知的参考相关能 $E_{\\mathrm{ref}}$ 及相应的 MP2 分量，以及一个包含 $T$ 个分子的测试集，其中只有 MP2 分量。所有能量单位均为 Hartree，且您必须以 Hartree 为单位报告所有输出。相关量如下：\n- 训练集大小 $N = 10$。\n- 测试集大小 $T = 4$。\n\n训练集数组（长度为 $N$，单位为 Hartree）：\n- $E_{\\mathrm{OS}}^{\\mathrm{train}} = [-0.040000,-0.120000,-0.220000,-0.350000,-0.500000,-0.180000,-0.270000,-0.080000,-0.420000,-0.300000]$,\n- $E_{\\mathrm{SS}}^{\\mathrm{train}} = [-0.010000,-0.030000,-0.060000,-0.090000,-0.130000,-0.050000,-0.070000,-0.020000,-0.110000,-0.080000]$,\n- $E_{\\mathrm{ref}}^{\\mathrm{train}} = [-0.051600,-0.154500,-0.284000,-0.450600,-0.643300,-0.233000,-0.347800,-0.102800,-0.540900,-0.386700]$。\n\n测试集数组（长度为 $T$，单位为 Hartree）：\n- $E_{\\mathrm{OS}}^{\\mathrm{test}} = [-0.260000,-0.150000,-0.520000,-0.020000]$,\n- $E_{\\mathrm{SS}}^{\\mathrm{test}} = [-0.070000,-0.040000,-0.140000,-0.005000]$。\n\n您的任务是：\n1. 通过在训练集上最小化残差平方和来校准线性模型参数 $c_0$、$c_{\\mathrm{OS}}$ 和 $c_{\\mathrm{SS}}$，并仅对斜率参数应用 Tikhonov (岭) 正则化以确保数值稳定性。具体来说，通过求解一个正则化最小二乘问题来估计参数，该问题惩罚 $c_{\\mathrm{OS}}$ 和 $c_{\\mathrm{SS}}$ 但不惩罚 $c_0$。使用正则化强度 $\\lambda = 10^{-8}$（作用于斜率参数平方 $L_2$ 范数的无量纲乘数）。\n2. 使用 $B = 2000$ 次自助法复制，通过对训练集进行有放回的非参数自助法重抽样，来量化已校准模型中的认知不确定性。在每次复制中，有放回地重抽样 $N$ 个训练对 $\\left(E_{\\mathrm{OS}}^{(i)}, E_{\\mathrm{SS}}^{(i)}, E_{\\mathrm{ref}}^{(i)}\\right)$，重新拟合正则化模型以获得参数 $\\left(c_0^{*}, c_{\\mathrm{OS}}^{*}, c_{\\mathrm{SS}}^{*}\\right)$，并为所有 $T$ 个测试分子计算预测能量 $E_{\\mathrm{pred}}^{*}$。\n3. 对每个测试分子 $j \\in \\{1,\\dots,T\\}$，计算：\n   - 使用在原始（非重抽样）训练集上拟合的参数计算一个点估计值 $\\hat{E}_{\\mathrm{pred},j}$。\n   - 从自助法分布中为 $E_{\\mathrm{pred},j}$ 计算一个双边 $95$ 百分比等尾置信区间，即经验分布在 $2.5$ 百分位和 $97.5$ 百分位的分位数。对分位数使用标准的线性插值规则。\n4. 为保证可复现性，自助法重抽样使用固定的伪随机数生成器种子 $12345$。\n\n您可以假设的基础理论：\n- 线性回归建模和最小二乘法原理是电子结构模型中小的经验校正的有效近似。\n- Tikhonov (岭) 正则化通过对系数大小施加一个小的惩罚来稳定最小二乘估计。\n- 非参数自助法通过从数据的经验分布中进行有放回重抽样来近似估计量的抽样分布。\n\n重要的科学和数值要求：\n- 完全按照所述的纯数学术语处理问题，不引用任何外部数据或超出上述定义的领域特定启发式方法。\n- 所有能量必须以 Hartree 为单位报告。\n- 置信区间的界限必须按所述方法计算，并以 Hartree 为单位报告。\n\n测试套件和输出规范：\n- 使用上面给出的精确的训练和测试数组。\n- 按规定使用 $\\lambda = 10^{-8}$，$B = 2000$ 和种子 $12345$。\n- 您的程序必须生成单行输出，其中包含一个由 $T$ 个子列表组成的列表。每个子列表按给定顺序对应一个测试分子，并包含三个四舍五入到小数点后恰好 $6$ 位的浮点数：$[\\hat{E}_{\\mathrm{pred},j}, \\mathrm{CI}_{\\mathrm{low},j}, \\mathrm{CI}_{\\mathrm{high},j}]$，其中所有条目都以 Hartree 为单位。例如，整体格式必须是：\n$[[x_{1},\\ell_{1},u_{1}],[x_{2},\\ell_{2},u_{2}],\\dots,[x_{T},\\ell_{T},u_{T}]]$\n，中间不插入任何空格。\n- 通过自助法需要覆盖的边缘情况包括对同一训练项的重复抽样；为确保任何重抽样样本的正规方程组的可逆性，请使用所述的岭正则化并且不要惩罚截距项。\n\n您的程序必须是完全确定性的，并且不得读取任何输入。它必须仅使用执行环境指定的库。打印的最后一行必须是唯一的输出，并且必须严格遵守上述格式要求。", "solution": "该问题要求校准一个自旋分量标度 Møller–Plesset 微扰理论 (SCS-MP2) 经验模型，并随后量化其预测中的认知不确定性。该问题是适定的、有科学依据的，并为获得唯一、可验证的解提供了所有必要信息。我们将继续进行推导和计算。\n\n给定的预测相关能 $E_{\\mathrm{pred}}$ 的线性模型是：\n$$\nE_{\\mathrm{pred}} = c_0 + c_{\\mathrm{OS}} E_{\\mathrm{OS}} + c_{\\mathrm{SS}} E_{\\mathrm{SS}}\n$$\n其中 $c_0$、$c_{\\mathrm{OS}}$ 和 $c_{\\mathrm{SS}}$ 是待定参数。$E_{\\mathrm{OS}}$ 和 $E_{\\mathrm{SS}}$ 分别是相反自旋和相同自旋的 MP2 相关能分量。\n\n给定一个包含 $N=10$ 个观测值的训练集。我们可以用矩阵形式表示整个训练集的关系：\n$$\n\\mathbf{y} = \\mathbf{X}\\mathbf{c} + \\boldsymbol{\\epsilon}\n$$\n这里，$\\mathbf{y}$ 是参考相关能 $E_{\\mathrm{ref}}^{\\mathrm{train}}$ 的 $N \\times 1$ 列向量。$\\mathbf{c}$ 是参数 $[c_0, c_{\\mathrm{OS}}, c_{\\mathrm{SS}}]^T$ 的 $3 \\times 1$ 列向量。$\\mathbf{X}$ 是 $N \\times 3$ 的设计矩阵，其中第 $i$ 行对应第 $i$ 个训练数据点，由 $[1, E_{\\mathrm{OS}, i}^{\\mathrm{train}}, E_{\\mathrm{SS}, i}^{\\mathrm{train}}]$ 给出。向量 $\\boldsymbol{\\epsilon}$ 代表残差。\n\n参数 $\\mathbf{c}$ 通过最小化残差平方和来估计，并辅以 Tikhonov (岭) 正则化项。问题规定惩罚仅应用于斜率参数 $c_{\\mathrm{OS}}$ 和 $c_{\\mathrm{SS}}$，而不应用于截距 $c_0$。要最小化的目标函数是：\n$$\nL(\\mathbf{c}) = \\sum_{i=1}^{N} (y_i - (\\mathbf{X}\\mathbf{c})_i)^2 + \\lambda (c_{\\mathrm{OS}}^2 + c_{\\mathrm{SS}}^2)\n$$\n这可以写成矩阵形式：\n$$\nL(\\mathbf{c}) = (\\mathbf{y} - \\mathbf{X}\\mathbf{c})^T (\\mathbf{y} - \\mathbf{X}\\mathbf{c}) + \\mathbf{c}^T \\mathbf{\\Lambda} \\mathbf{c}\n$$\n其中 $\\lambda=10^{-8}$ 是正则化强度，$\\mathbf{\\Lambda}$ 是一个选择性施加惩罚的 $3 \\times 3$ 矩阵：\n$$\n\\mathbf{\\Lambda} = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & \\lambda & 0 \\\\ 0 & 0 & \\lambda \\end{pmatrix}\n$$\n该目标函数的最小值通过将其关于 $\\mathbf{c}$ 的梯度设为零来找到，从而得到修正的正规方程组形式的解：\n$$\n(\\mathbf{X}^T \\mathbf{X} + \\mathbf{\\Lambda}) \\hat{\\mathbf{c}} = \\mathbf{X}^T \\mathbf{y}\n$$\n因此，最优参数向量 $\\hat{\\mathbf{c}}$ 为：\n$$\n\\hat{\\mathbf{c}} = (\\mathbf{X}^T \\mathbf{X} + \\mathbf{\\Lambda})^{-1} \\mathbf{X}^T \\mathbf{y}\n$$\n正则化项 $\\mathbf{\\Lambda}$ 确保矩阵 $(\\mathbf{X}^T \\mathbf{X} + \\mathbf{\\Lambda})$ 是可逆的，即使 $\\mathbf{X}^T \\mathbf{X}$ 是奇异或病态的（这在某些自助法重抽样样本中可能发生）。\n\n一旦从完整训练集中确定了参数 $\\hat{\\mathbf{c}}$，对 $T=4$ 个测试分子的预测能量点估计值计算如下：\n$$\n\\hat{\\mathbf{y}}_{\\mathrm{pred}} = \\mathbf{X}_{\\mathrm{test}} \\hat{\\mathbf{c}}\n$$\n其中 $\\mathbf{X}_{\\mathrm{test}}$ 是测试集的 $T \\times 3$ 设计矩阵，其构造方式与 $\\mathbf{X}$ 类似。\n\n为了量化这些预测中的认知不确定性，我们采用具有 $B=2000$ 次复制的非参数自助法程序。对于每次复制 $b \\in \\{1, \\dots, B\\}$：\n1. 从原始训练集中有放回地抽取一个大小为 $N$ 的自助法样本。这会得到一个新的设计矩阵 $\\mathbf{X}_b$ 和一个新的参考能量向量 $\\mathbf{y}_b$。\n2. 对该自助法样本求解正则化最小二乘问题，以获得一组新的参数 $\\hat{\\mathbf{c}}_b$：\n$$\n\\hat{\\mathbf{c}}_b = (\\mathbf{X}_b^T \\mathbf{X}_b + \\mathbf{\\Lambda})^{-1} \\mathbf{X}_b^T \\mathbf{y}_b\n$$\n3. 使用这些自助法参数为测试分子生成一组预测：\n$$\n\\hat{\\mathbf{y}}_{\\mathrm{pred}, b} = \\mathbf{X}_{\\mathrm{test}} \\hat{\\mathbf{c}}_b\n$$\n完成所有 $B$ 次复制后，对于每个测试分子 $j \\in \\{1, \\dots, T\\}$，我们得到了一个包含 $B$ 个预测能量的分布。通过找到该分布的经验 $2.5$ 百分位数和 $97.5$ 百分位数来构建 $95\\%$ 等尾置信区间。这些分位数是通过使用排序后分布中相邻值之间的线性插值计算的。使用固定的伪随机数生成器种子可确保自助法重抽样过程的可复现性，从而保证最终结果的可复现性。\n\n每个测试分子 $j$ 的最终输出将包括来自原始拟合的点估计值 $\\hat{E}_{\\mathrm{pred},j}$，以及其 $95\\%$ 置信区间的下界和上界 $[\\mathrm{CI}_{\\mathrm{low},j}, \\mathrm{CI}_{\\mathrm{high},j}]$。所有值均以 Hartree 为单位报告，并四舍五入到六位小数。\n\n实现过程将首先根据所提供的数据构造必要的矩阵。然后，计算原始数据的参数以找到点估计值。接下来，执行自助法循环，存储每次复制的预测值。最后，处理自助法预测的集合以计算指定的置信区间。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calibrates an SCS-MP2 model, predicts energies for a test set, and quantifies\n    uncertainty using nonparametric bootstrap.\n    \"\"\"\n    #\n    # Step 0: Define givens from the problem statement\n    #\n    N = 10  # Training set size\n    T = 4   # Test set size\n    B = 2000 # Number of bootstrap replicates\n    LAMBDA = 1e-8 # Regularization strength\n    SEED = 12345  # PRNG seed\n\n    # Training set data (in Hartree)\n    E_os_train = np.array([-0.040000,-0.120000,-0.220000,-0.350000,-0.500000,-0.180000,-0.270000,-0.080000,-0.420000,-0.300000])\n    E_ss_train = np.array([-0.010000,-0.030000,-0.060000,-0.090000,-0.130000,-0.050000,-0.070000,-0.020000,-0.110000,-0.080000])\n    E_ref_train = np.array([-0.051600,-0.154500,-0.284000,-0.450600,-0.643300,-0.233000,-0.347800,-0.102800,-0.540900,-0.386700])\n\n    # Test set data (in Hartree)\n    E_os_test = np.array([-0.260000,-0.150000,-0.520000,-0.020000])\n    E_ss_test = np.array([-0.070000,-0.040000,-0.140000,-0.005000])\n\n    #\n    # Step 1: Prepare matrices for linear regression\n    #\n    \n    # Design matrix X for training data (N x 3)\n    X_train = np.vstack([np.ones(N), E_os_train, E_ss_train]).T\n    y_train = E_ref_train\n\n    # Design matrix X_test for test data (T x 3)\n    X_test = np.vstack([np.ones(T), E_os_test, E_ss_test]).T\n    \n    # Regularization matrix Lambda (3 x 3)\n    # Penalizes c_os and c_ss, but not the intercept c_0\n    lambda_mat = np.diag([0, LAMBDA, LAMBDA])\n\n    def fit_model(X, y, reg_mat):\n        \"\"\"\n        Solves the regularized least-squares problem to find model parameters.\n        c = (X^T X + Lambda)^(-1) X^T y\n        \"\"\"\n        XTX = X.T @ X\n        XTy = X.T @ y\n        \n        # Add regularization term\n        regularized_XTX = XTX + reg_mat\n        \n        # Solve for coefficients\n        try:\n            inv_matrix = np.linalg.inv(regularized_XTX)\n            coeffs = inv_matrix @ XTy\n        except np.linalg.LinAlgError:\n            # Fallback to pseudoinverse if inversion fails, though ridge should prevent this\n            inv_matrix = np.linalg.pinv(regularized_XTX)\n            coeffs = inv_matrix @ XTy\n            \n        return coeffs\n\n    #\n    # Step 2: Calibrate on original training set and get point estimates\n    #\n\n    # Fit model to the original training data\n    c_hat = fit_model(X_train, y_train, lambda_mat)\n\n    # Compute point predictions for the test set\n    point_predictions = X_test @ c_hat\n\n    #\n    # Step 3: Perform nonparametric bootstrap\n    #\n    \n    # Initialize pseudo-random number generator\n    rng = np.random.default_rng(SEED)\n\n    # Store bootstrap predictions for each test molecule\n    bootstrap_predictions = np.zeros((B, T))\n\n    for i in range(B):\n        # Resample N indices with replacement\n        indices = rng.choice(N, size=N, replace=True)\n\n        # Create bootstrap sample\n        X_boot = X_train[indices]\n        y_boot = y_train[indices]\n\n        # Fit model on bootstrap sample\n        c_boot = fit_model(X_boot, y_boot, lambda_mat)\n\n        # Predict on test set and store results\n        bootstrap_predictions[i, :] = X_test @ c_boot\n\n    #\n    # Step 4: Compute confidence intervals and format the final output\n    #\n\n    results = []\n    for j in range(T):\n        # Get predictions for the j-th test molecule from all bootstrap runs\n        pred_dist = bootstrap_predictions[:, j]\n        \n        # Compute 2.5% and 97.5% quantiles for a 95% CI\n        # Using linear interpolation as specified\n        ci_low, ci_high = np.quantile(pred_dist, [0.025, 0.975], interpolation='linear')\n        \n        # Get the point estimate\n        point_est = point_predictions[j]\n        \n        results.append([point_est, ci_low, ci_high])\n\n    # Format the output string exactly as specified: no spaces, 6 decimal places.\n    # e.g., [[x1,l1,u1],[x2,l2,u2]]\n    sublist_strings = []\n    for res in results:\n        sublist_strings.append(f\"[{res[0]:.6f},{res[1]:.6f},{res[2]:.6f}]\")\n    final_output_string = f\"[{','.join(sublist_strings)}]\"\n    \n    print(final_output_string)\n\nsolve()\n```", "id": "2926422"}]}