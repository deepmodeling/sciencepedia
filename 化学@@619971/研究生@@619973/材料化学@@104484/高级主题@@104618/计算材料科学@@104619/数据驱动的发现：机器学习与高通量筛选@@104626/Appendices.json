{"hands_on_practices": [{"introduction": "在任何高通量筛选项目中，首要步骤之一是定义要探索的材料空间。本练习将理论与实践相结合，要求您为一个三元合金体系设计一个成分网格 [@problem_id:2479781]。通过从第一性原理出发，您将推导出所需计算点的数量，并学习如何量化离散化分辨率与代理模型插值精度之间的权衡关系，这是数据驱动材料发现中资源规划和实验设计的核心技能。", "problem": "在一个三元合金系统的高通量筛选活动中，您计划对组分单纯形进行离散化，以构建一个光滑属性场的数据驱动代理模型。设三元组分单纯形定义为 $S = \\{(x_1,x_2,x_3) \\in \\mathbb{R}^3 : x_i \\ge 0,\\ \\sum_{i=1}^{3} x_i = 1\\}$，并通过 $(x_1,x_2) \\in \\mathbb{R}^2$ 对 $S$ 进行参数化，其中 $x_3 = 1 - x_1 - x_2$。考虑一个分辨率为 $m \\in \\mathbb{N}$ 的重心网格，该网格由所有重心坐标为 $\\left(\\frac{i}{m}, \\frac{j}{m}, \\frac{k}{m}\\right)$ 的组分构成，其中 $i,j,k \\in \\mathbb{Z}_{\\ge 0}$ 且 $i+j+k = m$。该网格在 $(x_1,x_2)$ 平面中引入了一个标准的分段线性三角剖分，该剖分由全等的等腰直角三角形组成，其直角边长度为 $\\frac{1}{m}$。\n\n假设目标属性是一个实值函数 $f:S \\to \\mathbb{R}$，它在 $S$ 上是二次连续可微的，并且其（关于 $(x_1,x_2)$ 的）Hessian 矩阵具有一致的算子范数界 $\\|\\nabla^2 f(x)\\|_2 \\le M$ 对所有 $x \\in S$ 成立，其中 $M$ 是一个已知常数。您将使用在网格节点上的值构建的分段线性插值来逼近 $f$。\n\n从第一性原理出发，完成以下任务：\n\n1) 使用计算 $i+j+k=m$ 的非负整数解个数的组合学论证，推导唯一网格组分数量 $N(m)$ 的闭式表达式。\n\n2) 使用带有积分或拉格朗日形式余项的多维泰勒定理，并结合网格中等腰直角三角形的几何性质，获得以 $M$、$m$ 和三角形边长表示的最坏情况逐点插值误差的一个充分界。据此，推导出一个关于 $m$ 的充分条件，以保证一致插值误差最多为预设的容差 $\\varepsilon > 0$。\n\n3) 对于 $M = 500$ 和 $\\varepsilon = 3.0 \\times 10^{-3}$，计算满足您推导的充分条件的最小整数分辨率 $m$，然后计算 $N(m)$。\n\n将您的最终答案表示为您找到的最小 $m$ 所对应的 $N(m)$ 的精确整数值。不包括单位，不要四舍五入；报告精确的整数。", "solution": "该问题提出了三个关于三元组分单纯形上属性函数的数值逼近的任务。我们将从基本原理出发，依次解决每个任务。\n\n首先，我们必须确定在给定分辨率 $m$ 下的网格点数 $N(m)$。网格点由重心坐标 $(\\frac{i}{m}, \\frac{j}{m}, \\frac{k}{m})$ 定义，其中 $i, j, k$ 是非负整数，满足 $i+j+k=m$。因此，这种唯一组分的数量 $N(m)$ 等价于方程 $i+j+k=m$ 的非负整数解的数量。这是一个经典的组合问题，可以用“隔板法”来解决。我们正在将 $m$ 个相同的物品（星）分配到 3 个不同的箱子（由 $i, j, k$ 索引）中。这等价于将 $m$ 个星和 $3-1=2$ 个隔板排成一列。序列中的总位置数为 $m+2$。不同排列的数量是从 $m+2$ 个可用位置中为 2 个隔板选择位置的方式数量。这由二项式系数 $\\binom{m+2}{2}$ 给出。\n因此，网格点的数量为：\n$$N(m) = \\binom{m+2}{2} = \\frac{(m+2)!}{2!(m+2-2)!} = \\frac{(m+2)(m+1)}{2}$$\n\n第二，我们必须推导一个关于分辨率 $m$ 的充分条件，以确保一致插值误差不超过容差 $\\varepsilon$。该逼近是在定义域的三角剖分上对函数 $f(x)$ 进行的分段线性插值 $L(x)$。该插值的误差取决于函数 $f$ 的光滑度和网格中三角形的大小。\n设 $x=(x_1, x_2)$ 是平面中的坐标。函数为 $f(x)$。误差为 $E(x) = f(x) - L(x)$。根据问题陈述，$f$ 是二次连续可微的（$C^2$），并且其 Hessian 矩阵 $\\nabla^2 f(x)$ 具有一致的算子范数界 $\\|\\nabla^2 f(x)\\|_2 \\le M$。\n对于网格中的单个三角形 $T$，一个源自泰勒定理的有限元逼近理论的标准结果提供了逐点插值误差的上界。对于三角形 $T$ 上的线性插值，此界由下式给出：\n$$\\max_{x \\in T} |f(x) - L(x)| \\le C h_T^2 \\max_{y \\in T} \\|\\nabla^2 f(y)\\|_2$$\n其中 $h_T$ 是三角形的直径（其最长边的长度），$C$ 是一个取决于三角形几何形状的常数。对于一般的凸域，一个广泛使用的常数是 $C = \\frac{1}{8}$，它源于一维情况。\n问题指出，该网格引入了一个由全等等腰直角三角形组成的三角剖分，其直角边长度为 $h = \\frac{1}{m}$。这样一个三角形的直径 $h_T$ 是其斜边的长度：\n$$h_T = \\sqrt{h^2 + h^2} = \\sqrt{2h^2} = h\\sqrt{2} = \\frac{\\sqrt{2}}{m}$$\n将此结果以及给定的 Hessian 范数界代入误差公式，我们得到单个三角形内的误差界：\n$$\\max_{x \\in T} |f(x) - L(x)| \\le \\frac{1}{8} h_T^2 M = \\frac{1}{8} \\left(\\frac{\\sqrt{2}}{m}\\right)^2 M = \\frac{1}{8} \\left(\\frac{2}{m^2}\\right) M = \\frac{M}{4m^2}$$\n由于网格中所有三角形都是全等的，这个界在整个定义域 $S$ 上是一致的。一致插值误差为 $\\sup_{x \\in S} |f(x) - L(x)|$。因此，我们可以陈述该误差最多为 $\\varepsilon$ 的充分条件：\n$$\\frac{M}{4m^2} \\le \\varepsilon$$\n对 $m$ 求解，我们得到对分辨率的要求条件：\n$$m^2 \\ge \\frac{M}{4\\varepsilon} \\implies m \\ge \\sqrt{\\frac{M}{4\\varepsilon}} = \\frac{1}{2}\\sqrt{\\frac{M}{\\varepsilon}}$$\n由于 $m$ 必须是整数，保证所需精度的最小整数分辨率是 $m_{min} = \\left\\lceil \\frac{1}{2}\\sqrt{\\frac{M}{\\varepsilon}} \\right\\rceil$。\n\n第三，我们为给定的值 $M=500$ 和 $\\varepsilon = 3.0 \\times 10^{-3}$ 计算具体的最小整数分辨率 $m$ 以及相应的网格点数 $N(m)$。\n使用上面推导的条件：\n$$m \\ge \\frac{1}{2}\\sqrt{\\frac{500}{3.0 \\times 10^{-3}}} = \\frac{1}{2}\\sqrt{\\frac{5 \\times 10^2}{3 \\times 10^{-3}}} = \\frac{1}{2}\\sqrt{\\frac{5}{3} \\times 10^5}$$\n进行数值计算：\n$$m \\ge \\frac{1}{2}\\sqrt{166666.66...} \\approx \\frac{1}{2}(408.248...) \\approx 204.124$$\n由于 $m$ 必须是整数，满足此条件的最小整数值是 $m = 205$。\n\n现在，我们使用第一部分推导的公式计算该分辨率下唯一网格组分的数量 $N(m)$：\n$$N(m) = \\frac{(m+1)(m+2)}{2}$$\n代入 $m=205$:\n$$N(205) = \\frac{(205+1)(205+2)}{2} = \\frac{206 \\times 207}{2} = 103 \\times 207 = 21321$$\n因此，必须筛选最少 $21321$ 个组分，以确保插值误差低于指定的容差。", "answer": "$$\\boxed{21321}$$", "id": "2479781"}, {"introduction": "一个机器学习模型的真正价值在于其预测全新材料（即分布外样本）的能力，而非仅仅在与训练数据相似的样本上表现良好。本练习旨在培养您对模型评估的批判性思维，指导您设计能够严格测试模型外推能力的验证方案 [@problem_id:2479777]。通过实践“留一法”策略，例如“留一元素法”（LOEO）或“留一原型法”（LOPO），您将学会如何揭示标准交叉验证方法可能掩盖的模型泛化能力的缺陷。", "problem": "一个材料信息学团队正在构建一个监督回归模型，用于预测无机晶体化合物的单位原子生成能。每个样本由一个输入向量 $x$ 表示，该向量包括成分描述符、元素级嵌入和晶体结构原型标签。目标值为 $y \\in \\mathbb{R}$。设输入和目标的数据生成分布为 $P(X,Y)$，并设 $f_{\\theta}$ 是一个通过经验风险最小化（ERM）在独立同分布（i.i.d.）假设下使用绝对误差损失 $\\ell(\\hat{y},y) = |\\hat{y}-y|$ 训练的模型。\n\n在大小为 $|S|$ 的样本集 $S$ 上的经验风险定义为\n$$\n\\hat{R}_{S}(f) = \\frac{1}{|S|} \\sum_{(x_i,y_i)\\in S} \\ell\\big(f(x_i),y_i\\big),\n$$\n在分布 $Q$ 下的总体风险定义为\n$$\nR_{Q}(f) = \\mathbb{E}_{(X,Y)\\sim Q}\\big[\\ell\\big(f(X),Y\\big)\\big].\n$$\n在从同一批化合物中抽取的随机训练/测试集划分中，独立同分布假设意味着 $P_{\\text{train}}(X,Y) \\approx P_{\\text{test}}(X,Y)$，因此对于足够大的测试集，$R_{P_{\\text{test}}}(f)$ 可以由 $\\hat{R}_{\\text{test}}(f)$ 近似。\n\n然而，该团队最终关心的是对以下两种情况的分布外外推能力：(i) 包含训练集中未见过的元素的成分，以及 (ii) 属于训练集中未见过的原型的结构。令 $\\mathcal{E}(x)$ 表示 $x$ 中存在的化学元素集合，$\\mathrm{proto}(x) \\in \\mathcal{P}$ 表示 $x$ 的晶体原型标签。该团队怀疑，在随机划分下出现的低误差部分是由于训练集和测试集之间共享的元素和原型导致的目标泄漏，这使得模型能够记住特定于元素或原型的相关性。\n\n哪个选项指定了评估划分方案及其附带的理由，能够正确测试对新元素和新原型的外推能力，并解释这些划分如何揭示随机划分可能隐藏的失效模式？\n\nA. 定义“留一元素法”（LOEO）评估：对于数据集中观察到的元素周期表子集中的每个元素 $e$，将测试折（test fold）设为 $\\mathcal{D}^{\\text{test}}_{e} = \\{(x,y): e \\in \\mathcal{E}(x)\\}$，训练折为其补集，并计算平均绝对误差 $\\mathrm{MAE} = \\hat{R}_{\\text{test}}(f)$ 等指标。定义“留一原型法”（LOPO）：对于每个原型 $p \\in \\mathcal{P}$，将测试折设为 $\\mathcal{D}^{\\text{test}}_{p} = \\{(x,y): \\mathrm{proto}(x)=p\\}$，并使用其补集进行训练。作为随机划分的基线，首先按精确的简化化学式或结构标识符进行分组，并保持分组完整，以避免划分间的近重复泄漏。如果模型依赖于在 $P_{\\text{train}}$ 下学到的特定于元素或原型的相关性，那么在 LOEO 和 LOPO 下，测试分布 $P_{\\text{test}}$ 的支撑集在元素或原型维度上与 $P_{\\text{train}}$ 的支撑集不相交，因此与分组随机划分相比，$R_{P_{\\text{test}}}(f)$ 可能会显著增加，从而揭示随机独立同分布划分可能隐藏的失效模式（例如，词汇表外元素嵌入或原型记忆）。\n\nB. 使用 k 折随机划分，并通过重要性加权在损失函数中增加稀有元素的权重，即最小化 $\\sum_{(x,y)\\in S} w(x)\\,\\ell(f(x),y)$，其中 $w(x) \\propto 1/\\pi(\\mathcal{E}(x))$，$\\pi$ 是经验元素频率。因为稀有元素被强调，所以此方案下的性能可以测试对未见元素的外推能力；类似地，原型稀有度加权可以测试对未见原型的外推能力。\n\nC. 定义“留一元素法”为：将所有包含选定元素 $e$ 的化合物同时保留在训练集和测试集中，但在测试时屏蔽元素级嵌入中 $e$ 的条目，使模型无法看到它。因为模型必须在没有被屏蔽的元素通道的情况下进行推断，所以这可以测试对新颖元素的外推能力。对于原型，在测试时屏蔽原型特征，同时将所有原型保留在训练和测试中。\n\nD. 对于原型，执行分层 k 折交叉验证，以使每个折的训练集和测试集都保持相同的原型分布 $\\mathrm{proto}(x)$。这确保了公平性，并且因为原型的比例在不同划分中保持恒定，所以可以测试原型的外推能力。对于元素，执行随机划分，因为原型分层已经控制了结构相关性。\n\n选择唯一的最佳选项。", "solution": "首先将对问题陈述的科学性和逻辑完整性进行验证。\n\n### 步骤 1：提取已知条件\n- **任务：** 构建一个监督回归模型 $f_{\\theta}$，用于预测无机晶体化合物的单位原子生成能 $y \\in \\mathbb{R}$。\n- **输入特征：** 一个输入向量 $x$，包括成分描述符、元素级嵌入和晶体结构原型标签 $\\mathrm{proto}(x) \\in \\mathcal{P}$。\n- **数据分布：** 一个数据生成分布 $P(X,Y)$。\n- **训练：** 在从 $P(X,Y)$ 中独立同分布（i.i.d.）抽取的训练集上进行经验风险最小化（ERM）。\n- **损失函数：** 绝对误差，$\\ell(\\hat{y},y) = |\\hat{y}-y|$。\n- **经验风险：** 在集合 $S$ 上，$\\hat{R}_{S}(f) = \\frac{1}{|S|} \\sum_{(x_i,y_i)\\in S} \\ell\\big(f(x_i),y_i\\big)$。\n- **总体风险：** 在分布 $Q$ 下， $R_{Q}(f) = \\mathbb{E}_{(X,Y)\\sim Q}\\big[\\ell\\big(f(X),Y\\big)\\big]$。\n- **标准评估：** 对于随机划分，$P_{\\text{train}}(X,Y) \\approx P_{\\text{test}}(X,Y)$，且 $\\hat{R}_{\\text{test}}(f)$ 近似于 $R_{P_{\\text{test}}}(f)$。\n- **主要目标：** 评估分布外（OOD）外推性能，具体针对：\n    1.  包含训练期间未见过的元素的成分。令 $\\mathcal{E}(x)$ 为 $x$ 中的元素集合。\n    2.  属于训练期间未见过的原型的结构。\n- **假设：** 随机划分下的低误差可能是由于模型记住了训练集和测试集中都存在的特定于元素或原型的相关性（目标泄漏）。\n- **问题：** 确定能够正确测试这些外推能力，并揭示随机划分可能隐藏的失效模式的评估策略。\n\n### 步骤 2：使用提取的已知条件进行验证\n根据既定标准对问题陈述进行评估：\n- **科学上合理：** 该问题是材料信息学中的一个典型且关键的任务。预测生成能、使用指定特征以及训练回归模型都是标准做法。分布内（i.i.d.）泛化和分布外（OOD）泛化之间的区别是机器学习中的一个基础且前沿的课题，这里将其应用于一个现实的科学背景。\n- **提法明确：** 问题阐述清晰。它定义了目标（测试外推能力）、背景（材料属性预测）以及具体的 OOD 挑战（新元素、新原型）。它要求一种评估方法，对此，机器学习文献中存在一个正确的、标准的解决方案。\n- **客观性：** 语言正式且无歧义。所有术语，如 ERM、风险和 i.i.d.，都是统计学习理论中的标准术语。问题不含主观论断。\n\n该问题未表现出任何无效性标志。它并非科学上不合理、无法形式化、不完整、不切实际、提法不当或平凡。它提出了一个关于科学领域中鲁棒模型评估的有意义且不平凡的问题。\n\n### 步骤 3：结论与行动\n问题陈述是**有效**的。将推导一个解决方案。\n\n### 推导过程\n问题的核心在于机器学习背景下内插（interpolation）和外推（extrapolation）的区别。\n在独立同分布（i.i.d.）数据集上使用随机划分进行标准交叉验证，测试的是模型的内插能力。在这种设置下，训练分布 $P_{\\text{train}}$ 和测试分布 $P_{\\text{test}}$ 被假定为相同的（$P_{\\text{train}} = P_{\\text{test}} = P$）。一个测试样本 $(x, y)$ 是新的，但其特征（例如 $x$ 中的元素和结构原型）很可能已经在训练集中的其他样本中出现过。模型的任务本质上是为熟悉组件的新组合预测一个值。\n\n相比之下，外推要求模型能够泛化到与训练数据系统性不同的输入。这构成了一个分布外（OOD）挑战，其中 $P_{\\text{test}} \\neq P_{\\text{train}}$。问题指明了两个这样的 OOD 挑战：\n1.  **对新元素的泛化：** 模型必须预测一个包含训练集中完全没有出现过的元素的化合物的生成能。\n2.  **对新原型的泛化：** 模型必须预测一个其晶体结构原型在训练集中完全没有出现过的化合物的生成能。\n\n要设计一个能够测试这些能力的评估方法，必须构建训练-测试集划分，使得测试集包含特定元素（或原型）的实例，而训练集保证不包含该元素（或原型）的任何实例。这在感兴趣的特征维度上强制产生了一个分布偏移，$P_{\\text{test}} \\neq P_{\\text{train}}$。\n\n对此，标准方法是**留一分组交叉验证（leave-one-group-out cross-validation）**。\n- **对于元素：** 这被称为留一元素法（LOEO）交叉验证。对于完整数据集中存在的每个元素 $e$，创建一个折（fold），其中测试集包含所有含元素 $e$ 的化合物，而训练集包含所有*不*含元素 $e$ 的化合物。模型在后者上训练，在前者上评估。与随机划分评估相比，这种设置下的高误差表明模型严重依赖于在训练中见过元素 $e$，并且无法泛化其关于元素周期表的知识来推断 $e$ 的行为。\n- **对于原型：** 这被称为留一原型法（LOPO）交叉验证。逻辑是相同的。对于每个原型 $p$，测试集由所有具有该原型的结构组成，而训练集则不包含任何该原型。高误差表明模型仅仅记住了标签 $p$ 和目标值之间的相关性，而不是从定义该原型的几何或拓扑特征中学习潜在的结构-属性关系。\n\n一个合适的比较基线是一个鲁棒的 i.i.d. 评估。简单的随机划分容易受到“近重复”泄漏的影响（例如，非常相似的成分最终分别进入训练集和测试集）。一个更好的基线是**分组随机划分**，即首先按化合物或结构对数据进行分组，并将属于同一组的所有样本一起保留在训练集或测试集中。这可以防止模型在对其训练数据的平凡变体上进行测试。\n\n通过比较 LOEO/LOPO 划分与分组随机划分的性能，可以量化模型的外推能力。性能下降很小表明泛化能力好，而性能大幅下降则表明外推失败。\n\n### 逐项分析\n- **A. 定义“留一元素法”（LOEO）评估...**\n  该选项精确地描述了正确的留一分组法。对于元素，它定义了 LOEO 测试折 $\\mathcal{D}^{\\text{test}}_{e} = \\{(x,y): e \\in \\mathcal{E}(x)\\}$ 并在其补集上训练。对于原型，它定义了 LOPO 测试折 $\\mathcal{D}^{\\text{test}}_{p} = \\{(x,y): \\mathrm{proto}(x)=p\\}$ 并在其补集上训练。它正确地提出使用分组随机划分作为强有力的基线，以避免近重复泄漏。其理由也完全合理：它指出这些划分在感兴趣的维度上造成了 $P_{\\text{train}}$ 和 $P_{\\text{test}}$ 之间不相交的支撑集，并且误差（$R_{P_{\\text{test}}}(f)$）的大幅增加会揭示外推失败，如词汇表外元素嵌入或原型记忆。这与推导过程完全一致。\n  **结论：正确。**\n\n- **B. 使用 k 折随机划分，并通过重要性加权...**\n  该选项建议修改训练目标，通过最小化 $\\sum w(x)\\,\\ell(f(x),y)$ 来为稀有元素加权。此技术解决的是 i.i.d. 框架内的不平衡数据问题。它帮助模型更好地从它*确实*看到的少数稀有元素样本中学习，但它不测试对模型*从未*见过的元素的外推能力。训练集仍然包含所有元素，因此任务不是问题陈述所要求的那种 OOD 任务。该选项混淆了从稀疏数据中学习与外推到新的数据类别。\n  **结论：错误。**\n\n- **C. 定义“留一元素法”为：将所有包含选定元素 e 的化合物同时保留在训练集和测试集中...**\n  该选项存在根本性缺陷。将测试数据包含在训练集中构成了严重的数据泄漏，并使任何由此产生的性能指标无效。模型只需记住测试集样本即可实现零误差。此外，在测试时屏蔽特征评估的是模型对缺失数据的鲁棒性，这与外推到全新的、未见的特征值（例如一个新元素）是不同的任务。这种方法没有正确地测试问题所定义的外推能力。\n  **结论：错误。**\n\n- **D. 对于原型，执行分层 k 折交叉验证...**\n  该选项提出的方法与要求完全相反。分层交叉验证确保了特征（在此例中是原型）的分布在训练集和测试集中是*相同*的。这是一种*强制*执行 i.i.d. 假设并获得模型内插性能稳定估计的技术。它被明确设计用来*防止*测试外推能力所需的那种分布偏移。因此，所提供的理由是完全错误的。同样，对元素使用随机划分也只测试 i.i.d. 性能。\n  **结论：错误。**", "answer": "$$\\boxed{A}$$", "id": "2479777"}, {"introduction": "在真实的材料发现中，我们通常需要同时优化多个相互冲突的性能指标，例如在追求高稳定性的同时又要保证低成本。本练习将向您介绍一个关键工具——超体积（HV）指标，它用于量化和比较多目标优化所得到的最优解集（即帕累托前沿）的质量 [@problem_id:2479712]。您将从基本定义出发，推导其计算方法，并将其应用于一个实际场景，以量化一次迭代式高通量筛选所带来的性能提升，从而掌握评估多目标发现策略有效性的核心技能。", "problem": "在用于材料发现的多目标高通量筛选（HTS）中，机器学习模型通过平衡相互冲突的目标来指导候选材料的选择。考虑两个归一化、无量纲的目标 $f_1$ 和 $f_2$ 的双目标最小化问题，例如预测的合成难度和预测的环境负担。对于一个由模型建议的候选者组成的有限集 $S$，其全局质量可以通过超体积（HV）指标来评估。该指标定义为目标空间中某个区域的二维 Lebesgue 测度，该区域需要满足：(i) 在最小化条件下被 $S$ 中至少一个点弱支配；(ii) 由一个选定的参考点 $r=(r_1,r_2)$ 界定，该参考点的每个分量都不优于 $S$ 中所有点的对应分量。\n\n仅从最小化条件下的帕累托支配和 $\\mathbb{R}^2$ 中集合的 Lebesgue 测度的核心定义出发，完成以下任务：\n\n1) 在双目标和有限非支配集 $S=\\{(x_i,y_i)\\}_{i=1}^m$ 的特殊情况下，推导超体积（HV）指标的闭式表达式。其中，集合 $S$ 中的点按 $x_i$ 严格递增排序时，对应的 $y_i$ 严格递减，且参考点 $r=(r_1,r_2)$ 对所有 $i$ 均满足 $x_i \\le r_1$ 和 $y_i \\le r_2$。你的推导必须从基本原理开始：弱帕累托支配的定义以及 $\\mathbb{R}^2$ 中轴对齐矩形并集的面积（Lebesgue 测度）。\n\n2) 使用你推导出的表达式，计算两个非支配点集的超体积（HV）。这两个点集总结了针对同一参考点 $r=(3.0,3.0)$ 的两次连续的模型引导的高通量筛选活动：\n- 基线集 $S_0=\\{(1.0,2.0),\\,(1.6,1.2),\\,(2.3,0.95)\\}$。\n- 改进集 $S_1=\\{(0.9,1.9),\\,(1.4,1.1),\\,(2.0,0.9)\\}$。\n\n3) 将超体积（HV）提升量，定义为 $HV(S_1;r)-HV(S_0;r)$，以单个实数形式报告。将你的最终答案表示为一个无量纲数，并将其四舍五入到四位有效数字。", "solution": "根据既定标准对问题陈述进行验证。\n\n**第1步：提取已知条件**\n- **主题**：数据驱动的发现：材料化学中的机器学习和高通量筛选。\n- **目标**：两个归一化、无量纲的目标 $f_1$ 和 $f_2$ 的双目标最小化。\n- **候选集**：一个由模型建议的候选者组成的有限集 $S$。\n- **超体积（HV）指标定义**：目标空间中某个区域的二维 Lebesgue 测度，该区域需满足：(i) 在最小化条件下被 $S$ 中至少一个点弱支配；(ii) 由一个选定的参考点 $r=(r_1,r_2)$ 界定，该参考点的每个分量都不优于 $S$ 中所有点的对应分量。对于点 $(x,y) \\in S$，这意味着 $x \\le r_1$ 和 $y \\le r_2$。\n- **任务1**：为双目标空间中的一个非支配集 $S=\\{(x_i,y_i)\\}_{i=1}^m$ 推导HV指标的闭式表达式。这些点经过排序，使得 $x_i$ 严格递增而 $y_i$ 严格递减。推导必须从弱帕累托支配和 Lebesgue 测度的定义开始。\n- **任务2**：计算两个集合的HV：\n  - $S_0=\\{(1.0,2.0),\\,(1.6,1.2),\\,(2.3,0.95)\\}$。\n  - $S_1=\\{(0.9,1.9),\\,(1.4,1.1),\\,(2.0,0.9)\\}$。\n  - 两者的参考点均为 $r=(3.0,3.0)$。\n- **任务3**：将HV提升量 $HV(S_1;r)-HV(S_0;r)$ 作为单个实数报告，并四舍五入到四位有效数字。\n\n**第2步：使用提取的已知条件进行验证**\n- **科学依据**：该问题基于多目标优化这一成熟领域。超体积指标是一个标准的性能度量。帕累托支配和 Lebesgue 测度的定义是基本的数学概念。该问题在科学和数学上是合理的。\n- **适定性**：该问题是适定的。它要求根据明确给出的数据和条件进行特定的推导和计算。集合 $S$ 的性质（非支配、有序）确保了可以推导出并计算出唯一且有意义的解。\n- **客观性**：语言客观、精确，不含主观或推测性内容。\n- **缺陷分析**：\n  1.  不存在科学或逻辑上的谬误。\n  2.  该问题是可形式化的，并与指定主题直接相关。\n  3.  问题设定完整且一致。关于集合 $S_0$ 和 $S_1$ 的条件（$x$ 严格递增，$y$ 严格递减）与非支配集的性质相符。参考点定义得当。\n  4.  所提供的数据是无量纲的，代表归一化的目标，这使其在上下文中具有物理上的合理性。没有施加不可行的条件。\n  5.  问题结构良好，可以导出一个唯一的、稳定的解。\n  6.  推导过程并非易事，需要正确应用几何和测度论原理。\n  7.  该问题可通过数学证明和计算进行完全验证。\n\n**第3步：结论与行动**\n问题有效。将提供完整解答。\n\n**第1部分：超体积（HV）表达式的推导**\n\n问题要求计算一组点 $S = \\{(x_i, y_i)\\}_{i=1}^m$ 相对于参考点 $r = (r_1, r_2)$ 的超体积。目标是最小化。\n\n根据定义，如果 $x' \\le x$ 且 $y' \\le y$，则点 $p'=(x', y')$ 弱支配点 $p=(x, y)$。被点 $s_i = (x_i, y_i)$ 弱支配的区域是集合 $[x_i, \\infty) \\times [y_i, \\infty)$。\n\n超体积是在目标空间中，被 $S$ 中至少一个点弱支配且由参考点 $r$ 界定的区域的二维 Lebesgue 测度（面积）。我们将该区域表示为 $\\mathcal{A}$，它是 $S$ 中每个点与参考点 $r$ 形成的矩形的并集。鉴于对所有 $i$ 都有 $x_i \\le r_1$ 和 $y_i \\le r_2$，该区域在数学上表示为：\n$$ \\mathcal{A} = \\bigcup_{i=1}^m [x_i, r_1] \\times [y_i, r_2] $$\n超体积是该集合的测度，即 $HV(S;r) = \\mu(\\mathcal{A})$。\n\n为了推导该测度的闭式表达式，我们利用集合 $S$ 的给定属性。这些点已排序，使得 $x_1 < x_2 < \\dots < x_m$。由于集合 $S$ 是非支配的，这也意味着 $y$ 坐标同样存在严格排序：$y_1 > y_2 > \\dots > y_m$。\n\n我们可以将总面积 $\\mathcal{A}$ 分解为不相交矩形的并集，从而可以简单地对它们的面积求和。我们定义一个额外的坐标 $x_{m+1} = r_1$。我们使用点 $x_1, x_2, \\dots, x_m, x_{m+1}$ 沿x轴划分积分域。这将创建 $m$ 个相邻的垂直条带。\n\n考虑第 $i$ 个条带（对于 $i \\in \\{1, 2, \\dots, m\\}$）。该条带覆盖 x 区间 $[x_i, x_{i+1}]$。对于此区间中的任何 $x$，如果存在某个 $s_j = (x_j, y_j) \\in S$ 使得 $x \\ge x_j$ 且 $y \\ge y_j$，则点 $(x,y)$ 位于被支配区域 $\\mathcal{A}$ 中。\n\n对于 $x \\in [x_i, x_{i+1}]$，条件 $x \\ge x_j$ 对所有 $j \\le i$ 均成立，因为 $x_j \\le x_i \\le x$。而对任何 $j > i$ 都不成立，因为 $x < x_{i+1} \\le x_j$。\n因此，对于此垂直条带中的点 $(x,y)$，其 y 坐标必须满足至少对于一个 $j \\in \\{1, 2, \\dots, i\\}$ 有 $y \\ge y_j$。这等价于满足 $y \\ge \\min\\{y_1, y_2, \\dots, y_i\\}$。\n由于排序属性 $y_1 > y_2 > \\dots > y_i$，最小值为 $y_i$。因此，对 $y$ 的条件简化为 $y \\ge y_i$。\n\n从 $x_i$ 到 $x_{i+1}$ 的垂直条带内，被支配区域的部分是矩形 $[x_i, x_{i+1}] \\times [y_i, r_2]$。\n我们将这些不相交的矩形区域定义为 $C_i$：\n$$ C_i = [x_i, x_{i+1}] \\times [y_i, r_2], \\quad \\text{对于 } i=1, \\dots, m-1 $$\n$$ C_m = [x_m, r_1] \\times [y_m, r_2] $$\n总区域是这些区域的不相交并集 $\\mathcal{A} = \\bigcup_{i=1}^m C_i$。总面积是这些矩形面积的总和：\n$$ HV(S;r) = \\mu(\\mathcal{A}) = \\sum_{i=1}^m \\mu(C_i) $$\n每个矩形 $C_i$ 的面积是其宽度乘以高度：对于 $i<m$，$\\mu(C_i) = (x_{i+1} - x_i)(r_2 - y_i)$，而 $\\mu(C_m) = (r_1 - x_m)(r_2 - y_m)$。\n使用我们的定义 $x_{m+1} = r_1$，我们可以将其写成一个单一的求和式：\n$$ HV(S;r) = \\sum_{i=1}^{m} (x_{i+1} - x_i)(r_2 - y_i) $$\n这就是所求的闭式表达式。\n\n**第2部分：计算 $S_0$ 和 $S_1$ 的HV**\n\n参考点是 $r=(r_1, r_2)=(3.0, 3.0)$。\n\n对于基线集 $S_0=\\{(1.0,2.0),\\,(1.6,1.2),\\,(2.3,0.95)\\}$，我们有 $m=3$。这些点的顺序是正确的。\n- $s_1 = (x_1, y_1) = (1.0, 2.0)$\n- $s_2 = (x_2, y_2) = (1.6, 1.2)$\n- $s_3 = (x_3, y_3) = (2.3, 0.95)$\n我们定义 $x_4 = r_1 = 3.0$。\n应用推导出的公式：\n$$ HV(S_0;r) = (x_2 - x_1)(r_2 - y_1) + (x_3 - x_2)(r_2 - y_2) + (x_4 - x_3)(r_2 - y_3) $$\n$$ HV(S_0;r) = (1.6 - 1.0)(3.0 - 2.0) + (2.3 - 1.6)(3.0 - 1.2) + (3.0 - 2.3)(3.0 - 0.95) $$\n$$ HV(S_0;r) = (0.6)(1.0) + (0.7)(1.8) + (0.7)(2.05) $$\n$$ HV(S_0;r) = 0.6 + 1.26 + 1.435 = 3.295 $$\n\n对于改进集 $S_1=\\{(0.9,1.9),\\,(1.4,1.1),\\,(2.0,0.9)\\}$，我们有 $m=3$。这些点的顺序也是正确的。\n- $s'_1 = (x'_1, y'_1) = (0.9, 1.9)$\n- $s'_2 = (x'_2, y'_2) = (1.4, 1.1)$\n- $s'_3 = (x'_3, y'_3) = (2.0, 0.9)$\n我们定义 $x'_4 = r_1 = 3.0$。\n应用公式：\n$$ HV(S_1;r) = (x'_2 - x'_1)(r_2 - y'_1) + (x'_3 - x'_2)(r_2 - y'_2) + (x'_4 - x'_3)(r_2 - y'_3) $$\n$$ HV(S_1;r) = (1.4 - 0.9)(3.0 - 1.9) + (2.0 - 1.4)(3.0 - 1.1) + (3.0 - 2.0)(3.0 - 0.9) $$\n$$ HV(S_1;r) = (0.5)(1.1) + (0.6)(1.9) + (1.0)(2.1) $$\n$$ HV(S_1;r) = 0.55 + 1.14 + 2.1 = 3.79 $$\n\n**第3部分：超体积提升量**\n\n提升量是两个超体积之差：\n$$ \\Delta HV = HV(S_1;r) - HV(S_0;r) $$\n$$ \\Delta HV = 3.79 - 3.295 = 0.495 $$\n问题要求答案四舍五入到四位有效数字。因此，$0.495$ 变为 $0.4950$。", "answer": "$$\n\\boxed{0.4950}\n$$", "id": "2479712"}]}