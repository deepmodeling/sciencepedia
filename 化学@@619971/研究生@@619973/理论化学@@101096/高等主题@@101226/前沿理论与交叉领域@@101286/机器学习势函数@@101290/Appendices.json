{"hands_on_practices": [{"introduction": "构建任何机器学习势的第一步都是将原子位置编码为一组尊重物理学基本对称性的数值“描述符”。这项实践提供了实现Behler-Parrinello类型对称函数的直接动手经验，这是许多现代势的基石。通过将描述符的数学定义转化为代码，您将对如何强制实现平移、旋转和置换不变性有一个具体的理解 [@problem_id:2457438]。", "problem": "实现一个程序，用于推导和计算一个针对氩 (Ar) 原子的基本双体 Behler–Parrinello 型对称函数，并在一个小的几何结构测试集上对其进行评估。其目的在于，将机器学习原子间势的不变性要求与一个具体的描述符联系起来，并展示其在不同参数选择下的数值行为。总体背景是，一个系统的总势能面可以近似为原子贡献的总和，其中每个原子贡献都取决于其邻近环境的一个局域化、对称不变的表示，正如高维神经网络势 (HDNNP) 中那样。你的任务是，从不变性原理出发，推导出一个双体径向对称函数，并将其实现。\n\n从以下基本原则出发：\n- 标量势能的平移和旋转不变性意味着，原子的局域描述符必须由内部坐标（例如原子间距）构建。\n- 对于有限范围的相互作用以及学习映射中的局域性，在一个有限半径处施加一个平滑的截断，以便截断半径以外的远处原子没有贡献，并且力保持良好性。\n- 为了解析原子周围的径向分布，使用一个具有可调宽度和中心参数的径向基，从而使该表示能够区分不同长度尺度下的环境。\n\n根据这些原理，推导并实现一个为选定中心原子 $i$ 设计的、具有以下形式的双体径向对称函数 $G^2$：\n- 对邻近原子 $j \\neq i$ 的求和，\n- 一个平滑的、有限范围的截断函数，该函数在截断半径处 $C^1$ 连续且值为零，\n- 以及一个可以在选定距离周围移动和锐化的局域径向权重。\n\n在你的推导和实现中，具体指定并使用以下形式：\n- 使用余弦截断函数\n$$\nf_c(r; R_c) = \n\\begin{cases}\n\\dfrac{1}{2}\\left[\\cos\\!\\left(\\dfrac{\\pi r}{R_c}\\right) + 1\\right], & r \\le R_c,\\\\\n0, & r > R_c,\n\\end{cases}\n$$\n其中余弦函数的参数以弧度为单位。\n- 使用类高斯径向基\n$$\n\\exp\\!\\left[-\\eta\\,(r - R_s)^2\\right],\n$$\n其中包含宽度参数 $\\eta$ 和位移 $R_s$。\n- 将它们组合成双体对称函数\n$$\nG_i^{2}(\\eta, R_s, R_c) = \\sum_{j \\ne i} \\exp\\!\\left[-\\eta\\,(r_{ij} - R_s)^2\\right]\\, f_c(r_{ij}; R_c),\n$$\n其中 $r_{ij}$ 是原子 $i$ 和 $j$ 之间的欧几里得距离。\n\n所有原子均为氩 (Ar)，被视为单一化学物种，因此不需要依赖于物种的权重。距离 $r_{ij}$、截断半径 $R_c$ 和位移 $R_s$ 必须以埃 (Ångström) 为单位表示，而 $\\eta$ 以 $\\text{Å}^{-2}$ 为单位表示。余弦函数必须以弧度作为其参数。\n\n程序要求：\n- 实现一个函数，在给定一组笛卡尔坐标（单位为埃）、中心原子的索引 $i$ 和参数 $(\\eta, R_s, R_c)$ 的情况下，使用上述公式计算 $G_i^{2}(\\eta, R_s, R_c)$。\n- 使用标准三维欧几里得距离。不要应用周期性边界条件。\n- 数值稳定性：排除自相互作用 ($j = i$)。距离 $r_{ij}$ 严格非负；除了排除自相互作用外，不要对 $r_{ij} = 0$ 进行特殊处理。\n\n测试集：\n对以下五种情况分别评估 $G_i^{2}$。每种情况都指定了 $(\\text{positions}, i, R_c, \\eta, R_s)$，其中所有距离单位为埃，$\\eta$ 的单位为 $\\text{Å}^{-2}$：\n- 情况 A:\n  - 位置: $\\big[(0,0,0),(2.0,0,0),(0,3.0,0),(0,0,4.0)\\big]$\n  - $i = 0$\n  - $R_c = 5.0$\n  - $\\eta = 0.5$\n  - $R_s = 0.0$\n- 情况 B:\n  - 位置: $\\big[(0,0,0)\\big]$\n  - $i = 0$\n  - $R_c = 3.0$\n  - $\\eta = 1.0$\n  - $R_s = 0.0$\n- 情况 C:\n  - 位置: $\\big[(0,0,0),(5.0,0,0),(-5.0,0,0)\\big]$\n  - $i = 0$\n  - $R_c = 5.0$\n  - $\\eta = 1.0$\n  - $R_s = 0.0$\n- 情况 D:\n  - 位置: $\\big[(0,0,0),(2.0,0,0),(0,3.0,0),(0,0,4.0)\\big]$\n  - $i = 0$\n  - $R_c = 5.0$\n  - $\\eta = 2.0$\n  - $R_s = 2.5$\n- 情况 E:\n  - 位置: $\\big[(0,0,0),(2.0,0,0),(0,3.0,0),(0,0,4.0)\\big]$\n  - $i = 1$\n  - $R_c = 5.0$\n  - $\\eta = 0.5$\n  - $R_s = 0.0$\n\n输出规格：\n- 对于每种情况，计算一个单一的浮点数值 $G_i^{2}$。\n- 使用标准四舍五入将每个结果精确到 $6$ 位小数。\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，结果按 A、B、C、D、E 的顺序排列。例如，使用通用占位符的输出应如下所示：\"[0.123456,0.000000,0.000000,1.234567,0.654321]\"。", "solution": "所提出的问题是有效的、科学上合理的且定义明确的。它要求推导和实现一个双体径向对称函数，这是现代机器学习原子间势（如高维神经网络势 HDNNP）的一个基本组成部分。我们将首先从第一性原理推导出该函数的形式，然后详细说明其计算算法。\n\n原子系统的势能 $E$ 是一个标量。为了使其具有物理意义，它必须在整个系统的平移和旋转以及相同原子的置换下保持不变。在 HDNNP 方案中，总能量被分解为原子贡献 $E_i$，其中 $E = \\sum_i E_i$。每个原子能量 $E_i$ 是原子 $i$ 局域环境的函数，由一组描述符或“对称函数”$\\{G_i\\}$ 来表征。因此，这些对称函数本身必须对上述变换保持不变。\n\n1.  **平移和旋转不变性**：这些对称性决定了原子 i 的描述符必须仅依赖于其局域环境的内部坐标，而不是全局坐标系中原子的绝对笛卡尔坐标。最简单的内部坐标集由中心原子 $i$ 与其邻居 $j$ 之间的标量距离 $r_{ij}$ 组成。任何关于这些距离的函数 $G_i = F(\\{r_{ij}\\}_{j \\neq i})$，对于原子集合的刚性平移和旋转都是自动不变的。\n\n2.  **置换不变性**：原子 i 的能量贡献不能依赖于其相同邻居的任意标记。如果原子 j 和 k 属于同一物种，交换它们不能改变描述符的值。满足此条件的最简单的数学结构是对所有邻居求和。因此，我们提出一个形式为 $G_i = \\sum_{j \\neq i} g(r_{ij})$ 的描述符，其中 g 是原子间距的某个函数。这种形式是双体对称函数的基础。\n\n3.  **局域性和平滑性**：物理相互作用本质上是局域的；非常遥远的原子的影响可以忽略不计。为了对此进行建模，我们引入了一个平滑的截断函数 $f_c(r_{ij}; R_c)$，它乘以每个邻居的贡献。这个函数在小距离处必须等于 1，并随着距离 $r_{ij}$ 接近截断半径 $R_c$ 而平滑地趋于 0。对于距离 $r_{ij} > R_c$，贡献恰好为零。对平滑性的要求，特别是 $C^1$ 连续性（一阶导数连续），至关重要。原子上的力计算为势能的负梯度，$\\mathbf{F}_k = -\\nabla_{\\mathbf{r}_k} E$。能量一阶导数的不连续性将导致不合物理的、无限大的力。提供的余弦截断函数是：\n    $$\n    f_c(r; R_c) = \n    \\begin{cases}\n    \\frac{1}{2}\\left[\\cos\\left(\\frac{\\pi r}{R_c}\\right) + 1\\right], & r \\le R_c,\\\\\n    0, & r > R_c.\n    \\end{cases}\n    $$\n    在截断半径 $r = R_c$ 处，函数值为 $f_c(R_c; R_c) = \\frac{1}{2}[\\cos(\\pi) + 1] = \\frac{1}{2}[-1 + 1] = 0$，确保了连续性。其导数为 $f'_c(r; R_c) = -\\frac{\\pi}{2R_c}\\sin(\\frac{\\pi r}{R_c})$。在 $r = R_c$ 处，导数为 $f'_c(R_c; R_c) = -\\frac{\\pi}{2R_c}\\sin(\\pi) = 0$，这与 $r > R_c$ 时零函数的导数相匹配。因此，该函数按要求是 $C^1$ 连续的。\n\n4.  **径向解析度**：截断函数的简单求和只能提供截断球内邻居的加权计数。为了创建一个能够区分不同径向结构的描述符，我们引入了一个径向基函数。指定的高斯形式 $\\exp[-\\eta(r_{ij} - R_s)^2]$ 即为此目的。该函数以距离 $R_s$ 为中心，并具有由参数 $\\eta$ 控制的特征宽度。较大的 $\\eta$ 对应于一个更窄、更尖锐的高斯峰。通过使用一组具有不同参数 $(\\eta, R_s)$ 的此类函数，可以解析中心原子 $i$ 周围邻居的径向分布。\n\n结合这四个原理——使用距离带来的不变性、求和带来的置换对称性、平滑截断带来的局域性以及径向基带来的解析度——我们得到了指定的双体径向对称函数，记为 $G_i^2$：\n$$\nG_i^{2}(\\eta, R_s, R_c) = \\sum_{j \\ne i} \\exp\\!\\left[-\\eta\\,(r_{ij} - R_s)^2\\right]\\, f_c(r_{ij}; R_c)\n$$\n求和遍及系统中的所有原子 $j$，不包括中心原子 $i$。对于每个邻居 $j$，我们仅在其与原子 $i$ 的距离 $r_{ij}$ 小于或等于截断半径 $R_c$ 时才计算其贡献。\n\n计算过程如下：\n给定 $N$ 个原子的笛卡尔坐标集 $\\{\\mathbf{r}_k\\}_{k=0,..,N-1}$、一个中心原子索引 $i$ 以及参数 $\\eta$、$R_s$ 和 $R_c$：\n1.  将对称函数值 $G_i^2$ 初始化为 $0$。\n2.  确定中心原子的坐标向量 $\\mathbf{r}_i$。\n3.  遍历所有其他原子 $j$，其中 $j \\in \\{0, 1, ..., N-1\\}$ 且 $j \\neq i$。\n4.  对于每个邻居 $j$，计算欧几里得距离 $r_{ij} = ||\\mathbf{r}_j - \\mathbf{r}_i|| = \\sqrt{(x_j-x_i)^2 + (y_j-y_i)^2 + (z_j-z_i)^2}$。\n5.  检查是否 $r_{ij} \\le R_c$。如果不是，原子 $j$ 的贡献为 $0$，我们继续处理下一个邻居。\n6.  如果 $r_{ij} \\le R_c$，计算该项的两个分量：\n    -   径向基项：$T_{\\text{rad}} = \\exp[-\\eta(r_{ij} - R_s)^2]$。\n    -   截断函数项：$T_{\\text{cut}} = \\frac{1}{2}[\\cos(\\frac{\\pi r_{ij}}{R_c}) + 1]$。\n7.  将这些项的乘积 $T_{\\text{rad}} \\times T_{\\text{cut}}$ 加到 $G_i^2$ 的运行总和中。\n8.  遍历所有邻居 $j$ 后，最终的总和就是原子 $i$ 的对称函数值。\n\n现在将实现此过程并应用于五个指定的测试用例。所有单位必须一致；距离 ($r_{ij}$, $R_s$, $R_c$) 以埃 ($\\text{Å}$) 为单位，参数 $\\eta$ 以 $\\text{Å}^{-2}$ 为单位，从而确保指数的参数是无量纲的。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing the Behler-Parrinello G2 symmetry function\n    for a series of test cases.\n    \"\"\"\n\n    def compute_g2(positions, i, R_c, eta, R_s):\n        \"\"\"\n        Computes the G2 symmetry function for a central atom i.\n        \n        Args:\n            positions (np.ndarray): Array of shape (N, 3) with Cartesian coordinates.\n            i (int): Index of the central atom.\n            R_c (float): Cutoff radius in Angstrom.\n            eta (float): Width parameter in Angstrom^-2.\n            R_s (float): Shift parameter in Angstrom.\n        \n        Returns:\n            float: The computed value of the G2 symmetry function.\n        \"\"\"\n        if positions.shape[0] <= 1:\n            return 0.0\n\n        central_atom_pos = positions[i]\n        g2_value = 0.0\n\n        for j in range(positions.shape[0]):\n            if i == j:\n                continue\n\n            neighbor_pos = positions[j]\n            # Calculate Euclidean distance\n            r_ij = np.linalg.norm(central_atom_pos - neighbor_pos)\n\n            # Apply the cutoff condition\n            if r_ij <= R_c:\n                # Cosine cutoff function\n                fc = 0.5 * (np.cos(np.pi * r_ij / R_c) + 1.0)\n                \n                # Gaussian-like radial basis function\n                radial_term = np.exp(-eta * (r_ij - R_s)**2)\n                \n                # Add contribution to the sum\n                g2_value += radial_term * fc\n        \n        return g2_value\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {'positions': np.array([[0.0, 0.0, 0.0], [2.0, 0.0, 0.0], [0.0, 3.0, 0.0], [0.0, 0.0, 4.0]]),\n         'i': 0, 'R_c': 5.0, 'eta': 0.5, 'R_s': 0.0},\n        # Case B\n        {'positions': np.array([[0.0, 0.0, 0.0]]),\n         'i': 0, 'R_c': 3.0, 'eta': 1.0, 'R_s': 0.0},\n        # Case C\n        {'positions': np.array([[0.0, 0.0, 0.0], [5.0, 0.0, 0.0], [-5.0, 0.0, 0.0]]),\n         'i': 0, 'R_c': 5.0, 'eta': 1.0, 'R_s': 0.0},\n        # Case D\n        {'positions': np.array([[0.0, 0.0, 0.0], [2.0, 0.0, 0.0], [0.0, 3.0, 0.0], [0.0, 0.0, 4.0]]),\n         'i': 0, 'R_c': 5.0, 'eta': 2.0, 'R_s': 2.5},\n        # Case E\n        {'positions': np.array([[0.0, 0.0, 0.0], [2.0, 0.0, 0.0], [0.0, 3.0, 0.0], [0.0, 0.0, 4.0]]),\n         'i': 1, 'R_c': 5.0, 'eta': 0.5, 'R_s': 0.0},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_g2(\n            positions=case['positions'],\n            i=case['i'],\n            R_c=case['R_c'],\n            eta=case['eta'],\n            R_s=case['R_s']\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The format string \"{:.6f}\" handles rounding to 6 decimal places.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "2457438"}, {"introduction": "一个机器学习势只有在它不仅能提供能量，还能提供作用在每个原子上的力时，才对分子模拟有用。这些力是势能的负梯度。这项实践要求您通过应用链式法则（这一过程也称为反向传播），从神经网络势中推导出原子力的解析表达式 [@problem_id:2784660]。掌握这一推导是理解机器学习势如何被高效地集成到分子动力学引擎中的关键。", "problem": "考虑一个由 $N$ 个原子组成的分子系统，其笛卡尔坐标集合为 $\\mathbf{R}=\\{R_{k\\alpha}\\}$，其中 $k\\in\\{1,\\dots,N\\}$ 为原子索引，$\\alpha\\in\\{x,y,z\\}$ 为笛卡尔分量索引。在 Born–Oppenheimer 势能面上，势能 $E(\\mathbf{R};\\Theta)$ 由一个逐原子的神经网络 (ANN) 势建模为\n$$\nE(\\mathbf{R};\\Theta)=\\sum_{i=1}^{N}\\varepsilon\\!\\left(\\mathbf{G}_{i}(\\mathbf{R});\\Theta\\right),\n$$\n其中 $\\mathbf{G}_{i}(\\mathbf{R})\\in\\mathbb{R}^{d}$ 是对原子 $i$ 的局域环境的一个可微、保持对称性的描述符，$\\Theta$ 表示所有网络参数。标量原子能量 $\\varepsilon(\\mathbf{G}_{i};\\Theta)$ 由一个具有 $L$ 个隐藏层的前馈网络生成：\n- 输入：$\\mathbf{h}_{i}^{(0)}=\\mathbf{G}_{i}\\in\\mathbb{R}^{d}$。\n- 对每一层 $\\ell\\in\\{1,\\dots,L\\}$：预激活 $\\mathbf{a}_{i}^{(\\ell)}=\\mathbf{W}^{(\\ell)}\\mathbf{h}_{i}^{(\\ell-1)}+\\mathbf{b}^{(\\ell)}$，激活 $\\mathbf{h}_{i}^{(\\ell)}=\\boldsymbol{\\phi}^{(\\ell)}\\!\\left(\\mathbf{a}_{i}^{(\\ell)}\\right)$，其中 $\\mathbf{W}^{(\\ell)}\\in\\mathbb{R}^{n_{\\ell}\\times n_{\\ell-1}}$，$\\mathbf{b}^{(\\ell)}\\in\\mathbb{R}^{n_{\\ell}}$，$n_{0}=d$，$n_{L}$ 是最后一个隐藏层的宽度。激活函数 $\\boldsymbol{\\phi}^{(\\ell)}$ 逐元素作用且可微。\n- 输出层（线性）：$\\varepsilon(\\mathbf{G}_{i};\\Theta)=\\mathbf{w}^{(L+1)\\top}\\mathbf{h}_{i}^{(L)}+b^{(L+1)}$，其中 $\\mathbf{w}^{(L+1)}\\in\\mathbb{R}^{n_{L}}$ 且 $b^{(L+1)}\\in\\mathbb{R}$。\n\n为每个 $\\ell\\in\\{1,\\dots,L\\}$ 定义对角矩阵 $\\mathbf{D}_{i}^{(\\ell)}=\\mathrm{diag}\\!\\left(\\boldsymbol{\\phi}^{(\\ell)\\prime}\\!\\left(\\mathbf{a}_{i}^{(\\ell)}\\right)\\right)\\in\\mathbb{R}^{n_{\\ell}\\times n_{\\ell}}$，其中 $\\boldsymbol{\\phi}^{(\\ell)\\prime}$ 是 $\\boldsymbol{\\phi}^{(\\ell)}$ 的逐元素导数，并在 $\\mathbf{a}_{i}^{(\\ell)}$ 处求值。\n\n仅使用多元微积分的链式法则和上述定义，推导笛卡尔导数 $\\partial E/\\partial R_{k\\alpha}$ 的一个紧凑的、封闭形式的表达式，该表达式应以下列各项表示：\n- 描述符的雅可比矩阵 $\\partial \\mathbf{G}_{i}/\\partial R_{k\\alpha}\\in\\mathbb{R}^{d}$，以及\n- 由 $\\mathbf{W}^{(\\ell)}$、$\\mathbf{w}^{(L+1)}$ 和激活导数矩阵 $\\mathbf{D}_{i}^{(\\ell)}$ 构建的、依赖于神经网络参数的因子。\n\n你的最终结果必须是单个标量解析表达式，它将一个行向量（依赖于 $\\Theta$ 和激活值）与 $\\partial \\mathbf{G}_{i}/\\partial R_{k\\alpha}$ 进行缩并，然后对 $i\\in\\{1,\\dots,N\\}$ 求和。在最终答案中，只需提供最终的表达式（不带等号）。不需要进行数值计算。如果你选择提及力，请记住物理力分量是 $-\\,\\partial E/\\partial R_{k\\alpha}$，但你的任务是提供 $\\partial E/\\partial R_{k\\alpha}$ 本身的表达式。", "solution": "该问题陈述是计算化学领域机器学习势中一个有效、适定的理论推导。任务是推导由逐原子神经网络建模的总势能，关于某个笛卡尔原子坐标的解析梯度。该推导仅依赖于多元微积分的链式法则。\n\n总势能 $E$ 被给出为原子能量贡献 $\\varepsilon_i$ 的和：\n$$\nE(\\mathbf{R};\\Theta)=\\sum_{i=1}^{N}\\varepsilon_{i}(\\mathbf{G}_{i}(\\mathbf{R});\\Theta)\n$$\n为了求关于特定笛卡尔坐标 $R_{k\\alpha}$ 的导数，我们将微分算子应用于这个和。根据微分的线性性质，和的导数是导数的和：\n$$\n\\frac{\\partial E}{\\partial R_{k\\alpha}} = \\frac{\\partial}{\\partial R_{k\\alpha}} \\sum_{i=1}^{N} \\varepsilon_{i} = \\sum_{i=1}^{N} \\frac{\\partial \\varepsilon_{i}}{\\partial R_{k\\alpha}}\n$$\n每个原子能量 $\\varepsilon_{i}$ 通过其对应的描述符向量 $\\mathbf{G}_{i}(\\mathbf{R}) \\in \\mathbb{R}^{d}$ 隐式地依赖于笛卡尔坐标 $\\mathbf{R}$。应用多元链式法则求 $\\frac{\\partial \\varepsilon_{i}}{\\partial R_{k\\alpha}}$，得到：\n$$\n\\frac{\\partial \\varepsilon_{i}}{\\partial R_{k\\alpha}} = \\sum_{j=1}^{d} \\frac{\\partial \\varepsilon_{i}}{\\partial G_{ij}} \\frac{\\partial G_{ij}}{\\partial R_{k\\alpha}}\n$$\n其中 $G_{ij}$ 是向量 $\\mathbf{G}_{i}$ 的第 $j$ 个分量。这个和可以紧凑地写成一个行向量和一个列向量的矩阵乘积（或点积）：\n$$\n\\frac{\\partial \\varepsilon_{i}}{\\partial R_{k\\alpha}} = \\left(\\frac{\\partial \\varepsilon_{i}}{\\partial \\mathbf{G}_{i}}\\right)^{\\top} \\frac{\\partial \\mathbf{G}_{i}}{\\partial R_{k\\alpha}}\n$$\n这里，$\\frac{\\partial \\varepsilon_{i}}{\\partial \\mathbf{G}_{i}}$ 是标量 $\\varepsilon_{i}$ 关于向量 $\\mathbf{G}_{i}$ 的梯度，它是一个大小为 $d$ 的列向量。项 $\\frac{\\partial \\mathbf{G}_{i}}{\\partial R_{k\\alpha}}$ 是描述符向量关于标量坐标 $R_{k\\alpha}$ 的雅可比矩阵，它也是一个大小为 $d$ 的列向量。\n\n核心任务是确定神经网络输出 $\\varepsilon_i$ 关于其输入 $\\mathbf{G}_i$ 的导数。这可以通过在网络各层中顺序应用链式法则来实现，这个过程被称为反向传播。我们将 $\\varepsilon_i$ 关于第 $\\ell$ 层激活向量的梯度定义为 $\\boldsymbol{\\delta}_{i}^{(\\ell)} \\equiv \\frac{\\partial \\varepsilon_{i}}{\\partial \\mathbf{h}_{i}^{(\\ell)}}$，它是一个维度为 $n_{\\ell}$ 的列向量。\n\n推导从输出层 $(L+1)$ 向后进行到输入层 $(0)$。\n\n1.  **输出层**：原子能量由 $\\varepsilon_{i} = \\mathbf{w}^{(L+1)\\top}\\mathbf{h}_{i}^{(L)} + b^{(L+1)}$ 给出。关于最后一个隐藏层激活 $\\mathbf{h}_{i}^{(L)}$ 的梯度是：\n    $$\n    \\boldsymbol{\\delta}_{i}^{(L)} = \\frac{\\partial \\varepsilon_{i}}{\\partial \\mathbf{h}_{i}^{(L)}} = \\mathbf{w}^{(L+1)}\n    $$\n\n2.  **隐藏层**：对于任何层 $\\ell \\in \\{1, \\dots, L\\}$，我们必须将 $\\boldsymbol{\\delta}_{i}^{(\\ell-1)}$ 与 $\\boldsymbol{\\delta}_{i}^{(\\ell)}$ 联系起来。依赖链为 $\\mathbf{h}_{i}^{(\\ell-1)} \\to \\mathbf{a}_{i}^{(\\ell)} \\to \\mathbf{h}_{i}^{(\\ell)}$。使用链式法则：\n    $$\n    \\boldsymbol{\\delta}_{i}^{(\\ell-1)} = \\frac{\\partial \\varepsilon_{i}}{\\partial \\mathbf{h}_{i}^{(\\ell-1)}} = \\left(\\frac{\\partial \\mathbf{h}_{i}^{(\\ell)}}{\\partial \\mathbf{h}_{i}^{(\\ell-1)}}\\right)^{\\top} \\frac{\\partial \\varepsilon_{i}}{\\partial \\mathbf{h}_{i}^{(\\ell)}} = \\left(\\frac{\\partial \\mathbf{h}_{i}^{(\\ell)}}{\\partial \\mathbf{h}_{i}^{(\\ell-1)}}\\right)^{\\top} \\boldsymbol{\\delta}_{i}^{(\\ell)}\n    $$\n    雅可比矩阵 $\\frac{\\partial \\mathbf{h}_{i}^{(\\ell)}}{\\partial \\mathbf{h}_{i}^{(\\ell-1)}}$ 通过组合线性和激活步骤得到：\n    $$\n    \\frac{\\partial \\mathbf{h}_{i}^{(\\ell)}}{\\partial \\mathbf{h}_{i}^{(\\ell-1)}} = \\frac{\\partial \\mathbf{h}_{i}^{(\\ell)}}{\\partial \\mathbf{a}_{i}^{(\\ell)}} \\frac{\\partial \\mathbf{a}_{i}^{(\\ell)}}{\\partial \\mathbf{h}_{i}^{(\\ell-1)}}\n    $$\n    根据问题定义，$\\frac{\\partial \\mathbf{a}_{i}^{(\\ell)}}{\\partial \\mathbf{h}_{i}^{(\\ell-1)}} = \\mathbf{W}^{(\\ell)}$ 且 $\\frac{\\partial \\mathbf{h}_{i}^{(\\ell)}}{\\partial \\mathbf{a}_{i}^{(\\ell)}} = \\mathbf{D}_{i}^{(\\ell)}$。因此，雅可比矩阵为 $\\mathbf{D}_{i}^{(\\ell)}\\mathbf{W}^{(\\ell)}$。将此代入 $\\boldsymbol{\\delta}_{i}^{(\\ell-1)}$ 的递推关系中：\n    $$\n    \\boldsymbol{\\delta}_{i}^{(\\ell-1)} = \\left(\\mathbf{D}_{i}^{(\\ell)}\\mathbf{W}^{(\\ell)}\\right)^{\\top} \\boldsymbol{\\delta}_{i}^{(\\ell)} = (\\mathbf{W}^{(\\ell)})^{\\top}(\\mathbf{D}_{i}^{(\\ell)})^{\\top}\\boldsymbol{\\delta}_{i}^{(\\ell)}\n    $$\n    由于 $\\mathbf{D}_{i}^{(\\ell)}$ 是对角矩阵，它是对称的：$(\\mathbf{D}_{i}^{(\\ell)})^{\\top} = \\mathbf{D}_{i}^{(\\ell)}$。递推关系简化为：\n    $$\n    \\boldsymbol{\\delta}_{i}^{(\\ell-1)} = (\\mathbf{W}^{(\\ell)})^{\\top}\\mathbf{D}_{i}^{(\\ell)}\\boldsymbol{\\delta}_{i}^{(\\ell)}\n    $$\n\n3.  **输入层**：我们从 $\\ell=L$ 向下展开此递推关系至 $\\ell=1$，以求得关于输入层激活 $\\mathbf{h}_{i}^{(0)} = \\mathbf{G}_{i}$ 的梯度：\n    $$\n    \\frac{\\partial \\varepsilon_{i}}{\\partial \\mathbf{G}_{i}} = \\boldsymbol{\\delta}_{i}^{(0)} = (\\mathbf{W}^{(1)})^{\\top}\\mathbf{D}_{i}^{(1)} \\boldsymbol{\\delta}_{i}^{(1)} = (\\mathbf{W}^{(1)})^{\\top}\\mathbf{D}_{i}^{(1)} \\left( (\\mathbf{W}^{(2)})^{\\top}\\mathbf{D}_{i}^{(2)}\\boldsymbol{\\delta}_{i}^{(2)} \\right) = \\dots\n    $$\n    这种递归代入得出完整表达式：\n    $$\n    \\frac{\\partial \\varepsilon_{i}}{\\partial \\mathbf{G}_{i}} = \\left( (\\mathbf{W}^{(1)})^{\\top}\\mathbf{D}_{i}^{(1)} \\right) \\left( (\\mathbf{W}^{(2)})^{\\top}\\mathbf{D}_{i}^{(2)} \\right) \\cdots \\left( (\\mathbf{W}^{(L)})^{\\top}\\mathbf{D}_{i}^{(L)} \\right) \\mathbf{w}^{(L+1)}\n    $$\n\n为了构成缩并所需的行向量，我们转置这个梯度向量：\n$$\n\\left(\\frac{\\partial \\varepsilon_{i}}{\\partial \\mathbf{G}_{i}}\\right)^{\\top} = \\left( \\left( (\\mathbf{W}^{(1)})^{\\top}\\mathbf{D}_{i}^{(1)} \\right) \\cdots \\left( (\\mathbf{W}^{(L)})^{\\top}\\mathbf{D}_{i}^{(L)} \\right) \\mathbf{w}^{(L+1)} \\right)^{\\top}\n$$\n利用性质 $(ABC)^\\top = C^\\top B^\\top A^\\top$ 并回顾 $(\\mathbf{M}^\\top)^\\top = \\mathbf{M}$ 和 $\\mathbf{D}_{i}^{(\\ell)\\top} = \\mathbf{D}_{i}^{(\\ell)}$，我们得到：\n$$\n\\left(\\frac{\\partial \\varepsilon_{i}}{\\partial \\mathbf{G}_{i}}\\right)^{\\top} = (\\mathbf{w}^{(L+1)})^{\\top} \\left( \\mathbf{D}_{i}^{(L)}\\mathbf{W}^{(L)} \\right) \\left( \\mathbf{D}_{i}^{(L-1)}\\mathbf{W}^{(L-1)} \\right) \\cdots \\left( \\mathbf{D}_{i}^{(1)}\\mathbf{W}^{(1)} \\right)\n$$\n这个表达式可以使用连乘符号紧凑地写出，其中乘积索引按降序排列：\n$$\n\\left(\\frac{\\partial \\varepsilon_{i}}{\\partial \\mathbf{G}_{i}}\\right)^{\\top} = (\\mathbf{w}^{(L+1)})^{\\top} \\prod_{\\ell=L}^{1} \\left( \\mathbf{D}_{i}^{(\\ell)} \\mathbf{W}^{(\\ell)} \\right)\n$$\n\n最后，将此行向量代回总能量导数的表达式中，我们得到最终的封闭形式表达式：\n$$\n\\frac{\\partial E}{\\partial R_{k\\alpha}} = \\sum_{i=1}^{N} \\left( (\\mathbf{w}^{(L+1)})^{\\top} \\prod_{\\ell=L}^{1} \\left( \\mathbf{D}_{i}^{(\\ell)} \\mathbf{W}^{(\\ell)} \\right) \\right) \\frac{\\partial \\mathbf{G}_i}{\\partial R_{k\\alpha}}\n$$\n此表达式将总笛卡尔导数表示为原子贡献的总和。每个贡献是（由原子 $i$ 的网络参数和激活值决定的）一个行向量与（原子 $i$ 的描述符关于给定笛卡尔坐标的）雅可比矩阵的缩并。", "answer": "$$\n\\boxed{\\sum_{i=1}^{N} \\left( (\\mathbf{w}^{(L+1)})^{\\top} \\prod_{\\ell=L}^{1} \\left( \\mathbf{D}_{i}^{(\\ell)} \\mathbf{W}^{(\\ell)} \\right) \\right) \\frac{\\partial \\mathbf{G}_i}{\\partial R_{k\\alpha}}}\n$$", "id": "2784660"}, {"introduction": "现实世界的训练数据很少是完美的，可能包含错误。机器学习势的性能和可靠性在很大程度上取决于其对这类噪声的鲁棒性以及其超参数的选择。在这个练习中，您将研究单个错误数据点如何影响用核岭回归训练的势，并探索局部和全局准确性之间的权衡 [@problem_id:2457470]。这项实践将阐明核长度尺度（$\\ell$）和正则化（$\\lambda$）在控制模型灵活性和防止对噪声数据过拟合方面的关键作用。", "problem": "您将实现并分析一个一维机器学习势。该势使用核岭回归在双原子分子的键伸缩坐标上进行训练，并分别在包含和不含一个故意“投毒”的训练点的情况下进行。本任务的重点是量化单个极端错误的能量标签如何对学习到的势产生局部（在投毒构型附近）与全局（在整个定义域上）的扰动。您的程序必须是一个完整、可运行的程序，能够为固定的测试套件计算指定的度量指标，并以要求的格式输出结果。\n\n背景与基本原理：\n- 双原子势能曲线是核间距 $r$（单位为埃）的函数。一个科学上合理的模型是 Morse 势，\n$$\nE_{\\mathrm{true}}(r) = D_e\\left(1 - e^{-a(r-r_e)}\\right)^2,\n$$\n其中势阱深度 $D_e$ 的单位是电子伏特，刚度参数 $a$ 的单位是埃的倒数，平衡距离 $r_e$ 的单位是埃。该势是键伸缩的一个平滑且经过充分检验的模型，在此用作基准真相。\n- 在监督回归中，核岭回归（KRR）是通过最小化经验预测误差平方加上一个 Tikhonov 正则化项而产生的。给定输入 $x_i$ 和输出 $y_i$，由正定核 $k(\\cdot,\\cdot)$ 导出的假设空间会产生一个解，该解是核函数的线性组合，在点 $x$ 处的预测形式为 $f(x) = \\sum_{i=1}^N \\alpha_i k(x, x_i)$，其中系数 $\\alpha_i$ 通过最小化正则化经验风险来确定。\n- 带有长度尺度 $\\ell$ 的高斯核，\n$$\nk(x,z) = \\exp\\left(-\\frac{(x - z)^2}{2\\ell^2}\\right),\n$$\n是一个标准的、平滑的径向基函数。其局部性由 $\\ell$ 控制：小的 $\\ell$ 使影响集中在局部；大的 $\\ell$ 使影响更广泛地扩散。\n- 正则化参数 $\\lambda > 0$ 控制数据拟合和平滑度之间的权衡，抑制任何单个数据点的影响。\n\n训练与评估设置：\n- 使用 Morse 势，参数为 $D_e = 0.2\\ \\mathrm{eV}$，$a = 2.5\\ \\mathrm{\\AA^{-1}}$，以及 $r_e = 0.74\\ \\mathrm{\\AA}$。\n- 构建一个包含 $N=13$ 个键长 $r_i$ 的训练集，这些键长在 $[0.6, 1.2]\\ \\mathrm{\\AA}$ 区间内均匀分布（包含端点）。\n- 定义“干净”的训练标签 $y_i^{\\mathrm{clean}} = E_{\\mathrm{true}}(r_i)$，单位为电子伏特。\n- 通过从训练网格中选择一个训练坐标 $r_p$，并将其标签替换为 $y_p^{\\mathrm{poison}} = y_p^{\\mathrm{clean}} + \\Delta$（其中 $\\Delta = 1.0$ 电子伏特）来定义一个“投毒”的训练集，而所有其他标签保持与干净集中的一致。这个单一的损坏标签模拟了一个数据投毒事件。\n- 使用相同的核函数和相同的正则化，训练两个 KRR 模型（一个在干净标签上训练，一个在投毒标签上训练）。\n- 在一个包含 $M=1001$ 个点的均匀测试网格上进行评估，该网格在 $[0.5, 1.5]\\ \\mathrm{\\AA}$ 区间内（包含端点）。\n\n对于下述每个测试用例，计算以下三个度量指标：\n1. 投毒构型处的局部影响：\n$$\nI_{\\mathrm{local}} = \\left| f_{\\mathrm{poison}}(r_p) - f_{\\mathrm{clean}}(r_p) \\right| \\quad \\text{单位为电子伏特}。\n$$\n2. 在测试域上的全局平均绝对偏差：\n$$\nI_{\\mathrm{global}} = \\frac{1}{M}\\sum_{j=1}^{M} \\left| f_{\\mathrm{poison}}(r_j^{\\mathrm{test}}) - f_{\\mathrm{clean}}(r_j^{\\mathrm{test}}) \\right| \\quad \\text{单位为电子伏特}。\n$$\n3. 精度相对下降（无单位），通过在测试网格上相对于基准真相的均方根误差（RMSEs）之比来衡量：\n$$\nR_{\\mathrm{RMSE}} = \\frac{\\sqrt{\\frac{1}{M}\\sum_{j=1}^{M}\\left(f_{\\mathrm{poison}}(r_j^{\\mathrm{test}}) - E_{\\mathrm{true}}(r_j^{\\mathrm{test}})\\right)^2}}{\\sqrt{\\frac{1}{M}\\sum_{j=1}^{M}\\left(f_{\\mathrm{clean}}(r_j^{\\mathrm{test}}) - E_{\\mathrm{true}}(r_j^{\\mathrm{test}})\\right)^2}}。\n$$\n\n核函数与正则化参数说明：\n- 使用高斯核，其长度尺度 $\\ell$ 按每个测试用例指定。\n- 使用 Tikhonov 正则化参数 $\\lambda$，按每个测试用例指定。\n\n测试套件：\n为以下四个案例中的每一个计算三元组 $\\left(I_{\\mathrm{local}}, I_{\\mathrm{global}}, R_{\\mathrm{RMSE}}\\right)$。在所有案例中，投毒幅度为 $\\Delta = 1.0\\ \\mathrm{eV}$，且训练点 $r_p$ 从训练网格中选取。\n- 案例1（理想情况，中等局部性）：$\\ell = 0.05\\ \\mathrm{\\AA}$，$\\lambda = 1\\times 10^{-6}$，$r_p = 0.90\\ \\mathrm{\\AA}$。\n- 案例2（更强的全局影响）：$\\ell = 0.20\\ \\mathrm{\\AA}$，$\\lambda = 1\\times 10^{-6}$，$r_p = 0.90\\ \\mathrm{\\AA}$。\n- 案例3（更强的正则化）：$\\ell = 0.05\\ \\mathrm{\\AA}$，$\\lambda = 1\\times 10^{-2}$，$r_p = 0.90\\ \\mathrm{\\AA}$。\n- 案例4（边界点投毒）：$\\ell = 0.05\\ \\mathrm{\\AA}$，$\\lambda = 1\\times 10^{-6}$，$r_p = 0.60\\ \\mathrm{\\AA}$。\n\n实现要求：\n- 通过求解最小化正则化经验风险（在高斯核诱导的再生核希尔伯特空间上）所隐含的正规方程，来实现一维的核岭回归。\n- 仅使用上面给出的指定参数和网格。所有能量单位必须是电子伏特，所有长度单位必须是埃，不使用角度。报告 $I_{\\mathrm{local}}$ 和 $I_{\\mathrm{global}}$ 的值（单位为电子伏特），以及 $R_{\\mathrm{RMSE}}$ 的值（作为无单位的比率）。\n- 将每个报告的浮点数四舍五入到6位小数。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。具体来说，输出一个包含四个列表的列表，每个测试用例对应一个，每个内部列表包含三个浮点数 $\\left(I_{\\mathrm{local}}, I_{\\mathrm{global}}, R_{\\mathrm{RMSE}}\\right)$，四舍五入到6位小数。例如，打印输出必须具有以下形式\n$$\n\\left[\\left[i_{1,1},i_{1,2},i_{1,3}\\right],\\left[i_{2,1},i_{2,2},i_{2,3}\\right],\\left[i_{3,1},i_{3,2},i_{3,3}\\right],\\left[i_{4,1},i_{4,2},i_{4,3}\\right]\\right],\n$$\n其中所有 $i_{k,m}$ 都是四舍五入到6位的小数。输出必须只有一行，且不包含任何额外文本。", "solution": "所述问题是有效的。它在计算化学和机器学习领域具有科学依据，是一个适定的问题，具有清晰且确定性的过程，并使用客观、明确的语言进行阐述。所有必要的参数和定义都已提供。因此，我将继续提供完整的解决方案。\n\n问题的核心是量化单个错误数据点（一种数据投毒形式）对核岭回归（KRR）模型的影响。该模型被训练用于学习双原子分子的势能面，其基准真相由 Morse 势给出：\n$$E_{\\mathrm{true}}(r) = D_e\\left(1 - e^{-a(r-r_e)}\\right)^2$$\n参数指定为 $D_e = 0.2\\ \\mathrm{eV}$，$a = 2.5\\ \\mathrm{\\AA^{-1}}$，以及 $r_e = 0.74\\ \\mathrm{\\AA}$。\n\nKRR 模型预测在给定核间距 $r$ 处的能量，其形式为以训练数据点 $\\{r_i\\}_{i=1}^N$ 为中心的核函数的线性组合：\n$$f(r) = \\sum_{i=1}^{N} \\alpha_i k(r, r_i)$$\n核函数是具有特征长度尺度 $\\ell$ 的高斯核：\n$$k(r, r') = \\exp\\left(-\\frac{(r - r')^2}{2\\ell^2}\\right)$$\n系数 $\\alpha = (\\alpha_1, \\dots, \\alpha_N)^T$ 通过最小化正则化最小二乘代价函数来确定。这导出了以下线性方程组：\n$$(\\mathbf{K} + \\lambda \\mathbf{I}) \\alpha = \\mathbf{y}$$\n此处，$\\mathbf{K}$ 是 $N \\times N$ 的核矩阵，其元素为 $K_{ij} = k(r_i, r_j)$；$\\mathbf{y}$ 是训练能量标签的向量；$\\lambda > 0$ 是 Tikhonov 正则化参数；$\\mathbf{I}$ 是 $N \\times N$ 的单位矩阵。该线性系统对 $\\alpha$ 有唯一解，此解随后用于预测。\n\n步骤如下：\n1. 构建一个包含 $N=13$ 个点的训练网格 $\\mathbf{r}_{\\mathrm{train}}$，这些点在 $[0.6, 1.2]\\ \\mathrm{\\AA}$ 区间内均匀分布。\n2. 构建一个包含 $M=1001$ 个点的测试网格 $\\mathbf{r}_{\\mathrm{test}}$，这些点在 $[0.5, 1.5]\\ \\mathrm{\\AA}$ 区间内均匀分布。\n3. 通过在 $\\mathbf{r}_{\\mathrm{train}}$ 中的每个点上评估 $E_{\\mathrm{true}}(r)$ 来生成“干净”的训练标签 $\\mathbf{y}_{\\mathrm{clean}}$。\n4. 对于每个由参数 $(\\ell, \\lambda, r_p)$ 指定的测试用例：\n    a. 通过获取 $\\mathbf{y}_{\\mathrm{clean}}$ 并对与坐标 $r_p$ 对应的标签加上一个扰动 $\\Delta = 1.0\\ \\mathrm{eV}$ 来创建“投毒”的训练标签 $\\mathbf{y}_{\\mathrm{poison}}$。\n    b. 使用 $(\\mathbf{r}_{\\mathrm{train}}, \\mathbf{y}_{\\mathrm{clean}})$ 求解系数向量 $\\alpha_{\\mathrm{clean}}$。\n    c. 使用 $(\\mathbf{r}_{\\mathrm{train}}, \\mathbf{y}_{\\mathrm{poison}})$ 求解系数向量 $\\alpha_{\\mathrm{poison}}$。\n    d. 使用这些系数生成两个势能函数：$f_{\\mathrm{clean}}(r)$ 和 $f_{\\mathrm{poison}}(r)$。\n    e. 在测试网格 $\\mathbf{r}_{\\mathrm{test}}$ 和特定点 $r_p$ 上评估这两个函数。\n    f. 计算所需的三个度量指标：\n        i. 局部影响，$I_{\\mathrm{local}} = | f_{\\mathrm{poison}}(r_p) - f_{\\mathrm{clean}}(r_p) |$。\n        ii. 全局平均绝对偏差，$I_{\\mathrm{global}} = \\frac{1}{M}\\sum_{j=1}^{M} | f_{\\mathrm{poison}}(r_j^{\\mathrm{test}}) - f_{\\mathrm{clean}}(r_j^{\\mathrm{test}}) |$。\n        iii. 相对RMSE下降，$R_{\\mathrm{RMSE}}$，定义为在测试网格上，$f_{\\mathrm{poison}}$ 相对于基准真相 $E_{\\mathrm{true}}$ 的均方根误差与 $f_{\\mathrm{clean}}$ 的均方根误差之比。\n\n将对四个指定的测试用例中的每一个进行这整个计算实验。结果将被汇编并以所需格式呈现。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the impact of a poisoned data point on a Kernel Ridge Regression\n    model of a diatomic molecule's potential energy curve.\n    \"\"\"\n    # Define constants and problem setup from the statement.\n    DE = 0.2     # eV\n    A = 2.5      # 1/Angstrom\n    RE = 0.74    # Angstrom\n    DELTA = 1.0  # eV\n\n    N_TRAIN = 13\n    N_TEST = 1001\n    \n    # Define training and testing grids.\n    r_train = np.linspace(0.6, 1.2, N_TRAIN)\n    r_test = np.linspace(0.5, 1.5, N_TEST)\n\n    def morse_potential(r, De, a, re):\n        \"\"\"Calculates the Morse potential energy.\"\"\"\n        return De * (1 - np.exp(-a * (r - re)))**2\n\n    def gaussian_kernel(x, z, ell):\n        \"\"\"\n        Computes the Gaussian kernel matrix between two sets of points.\n        x and z are 1D arrays.\n        \"\"\"\n        dist_sq = np.subtract.outer(x, z)**2\n        return np.exp(-dist_sq / (2 * ell**2))\n\n    def train_krr(r_train, y_train, ell, lam):\n        \"\"\"Trains a KRR model and returns the coefficients alpha.\"\"\"\n        K = gaussian_kernel(r_train, r_train, ell)\n        # Solve the linear system (K + lambda*I) * alpha = y\n        A = K + lam * np.identity(K.shape[0])\n        alpha = np.linalg.solve(A, y_train)\n        return alpha\n\n    def predict_krr(r_predict, r_train, alpha, ell):\n        \"\"\"Makes predictions using a trained KRR model.\"\"\"\n        K_pred = gaussian_kernel(r_predict, r_train, ell)\n        return K_pred @ alpha\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (ell, lambda, r_p)\n        (0.05, 1e-6, 0.90), # Case 1: Moderate locality\n        (0.20, 1e-6, 0.90), # Case 2: More global influence\n        (0.05, 1e-2, 0.90), # Case 3: Stronger regularization\n        (0.05, 1e-6, 0.60), # Case 4: Boundary poison\n    ]\n\n    # Generate ground truth data.\n    y_true_train = morse_potential(r_train, DE, A, RE)\n    y_true_test = morse_potential(r_test, DE, A, RE)\n    \n    all_results = []\n    for ell, lam, r_p in test_cases:\n        # Find the index of the poisoned point in the training grid.\n        p_idx = np.argmin(np.abs(r_train - r_p))\n        \n        # Create clean and poisoned training labels.\n        y_clean = np.copy(y_true_train)\n        y_poison = np.copy(y_true_train)\n        y_poison[p_idx] += DELTA\n\n        # Train both clean and poisoned models.\n        alpha_clean = train_krr(r_train, y_clean, ell, lam)\n        alpha_poison = train_krr(r_train, y_poison, ell, lam)\n\n        # Make predictions on the test grid.\n        f_clean_test = predict_krr(r_test, r_train, alpha_clean, ell)\n        f_poison_test = predict_krr(r_test, r_train, alpha_poison, ell)\n\n        # Make predictions at the poisoned point r_p for I_local.\n        f_clean_rp = predict_krr(np.array([r_p]), r_train, alpha_clean, ell)[0]\n        f_poison_rp = predict_krr(np.array([r_p]), r_train, alpha_poison, ell)[0]\n\n        # Metric 1: Local impact I_local\n        i_local = np.abs(f_poison_rp - f_clean_rp)\n\n        # Metric 2: Global mean absolute deviation I_global\n        i_global = np.mean(np.abs(f_poison_test - f_clean_test))\n\n        # Metric 3: Relative degradation of accuracy R_RMSE\n        rmse_clean = np.sqrt(np.mean((f_clean_test - y_true_test)**2))\n        rmse_poison = np.sqrt(np.mean((f_poison_test - y_true_test)**2))\n        \n        if np.isclose(rmse_clean, 0.0):\n            r_rmse = np.inf\n        else:\n            r_rmse = rmse_poison / rmse_clean\n            \n        all_results.append([i_local, i_global, r_rmse])\n\n    # Final print statement in the exact required format.\n    inner_lists_str = []\n    for res in all_results:\n        formatted_res = [f\"{x:.6f}\" for x in res]\n        inner_lists_str.append(f\"[{','.join(formatted_res)}]\")\n    print(f\"[{','.join(inner_lists_str)}]\")\n\nsolve()\n```", "id": "2457470"}]}