{"hands_on_practices": [{"introduction": "要真正掌握广义线性模型，我们必须首先理解其内部工作原理。这项练习将引导您完成逻辑回归（医学结果建模的主力）的基础推导。通过从第一性原理推导对数似然、得分向量和 Hessian 矩阵，您将揭示统计软件用于寻找最大似然估计的数学引擎。[@problem_id:4988404]", "problem": "一个临床数据科学团队正在开发一个广义线性模型 (GLM)，以根据包括年龄、手术时长、基线白细胞计数和合并症评分在内的协变量，估计术后患者在 $48$ 小时内发生脓毒症的概率。对于患者 $i$，设其二元结果为 $y_{i} \\in \\{0,1\\}$，其中 $y_{i}=1$ 表示发生脓毒症，$y_{i}=0$ 表示未发生脓毒症。设协变量向量为 $x_{i} \\in \\mathbb{R}^{p}$，回归参数为 $\\beta \\in \\mathbb{R}^{p}$，线性预测器定义为 $\\eta_{i} = x_{i}^{\\top}\\beta$。假设在给定协变量和参数的情况下，结果是条件独立的，并且对于每个 $i$，$Y_{i}$ 服从均值为 $\\mu_{i} = \\mathbb{E}[Y_{i} \\mid x_{i}]$ 的伯努利分布。\n\n从以下基本依据出发：\n- 对于一个独立观测值，伯努利概率质量函数 (pmf) 为 $p(y_{i} \\mid \\mu_{i}) = \\mu_{i}^{y_{i}} (1-\\mu_{i})^{1-y_{i}}$，其中 $y_{i} \\in \\{0,1\\}$。\n- 在广义线性模型 (GLM) 中，伯努利分布的典范链接是 logit 链接，定义为 $\\ln\\left(\\frac{\\mu_{i}}{1-\\mu_{i}}\\right) = \\eta_{i} = x_{i}^{\\top}\\beta$。\n\n仅使用这些基本依据和指数族的定义，推导在 logit 链接下参数向量 $\\beta$ 的对数似然函数 $\\ell(\\beta)$。然后，计算 $\\ell(\\beta)$ 关于 $\\beta$ 的梯度（得分）向量和 Hessian（观测信息）矩阵，并用 $\\{(y_{i}, x_{i})\\}_{i=1}^{n}$ 和 $\\beta$ 来表示它们。提供简化的闭式表达式；无需数值近似。使用 $\\ln$ 表示任何对数，使用 $\\exp$ 表示任何指数。您的最终答案必须是这三个推导出的表达式。", "solution": "该问题要求为一个逻辑斯蒂回归模型推导其对数似然函数 $\\ell(\\beta)$、其梯度向量（得分向量）$\\nabla_{\\beta} \\ell(\\beta)$ 及其 Hessian 矩阵 $\\nabla_{\\beta}^2 \\ell(\\beta)$。\n\n**1. 对数似然函数 $\\ell(\\beta)$ 的推导**\n\n首先，我们从单个伯努利观测值 $y_i$ 的概率质量函数 (pmf) 开始：\n$$\np(y_i \\mid \\mu_i) = \\mu_i^{y_i} (1-\\mu_i)^{1-y_i}\n$$\n单个观测值的对数似然 $\\ell_i$ 是其自然对数：\n$$\n\\ell_i(\\mu_i) = y_i \\ln(\\mu_i) + (1-y_i) \\ln(1-\\mu_i)\n$$\n接下来，我们需要将 $\\ell_i$ 表示为参数向量 $\\beta$ 的函数。该模型通过 logit 链接函数将均值 $\\mu_i$ 与线性预测器 $\\eta_i = x_i^\\top\\beta$ 联系起来：\n$$\n\\eta_i = \\ln\\left(\\frac{\\mu_i}{1-\\mu_i}\\right)\n$$\n为了用 $\\eta_i$ 表示 $\\mu_i$，我们对链接函数求逆：\n$$\n\\exp(\\eta_i) = \\frac{\\mu_i}{1-\\mu_i} \\implies \\exp(\\eta_i)(1-\\mu_i) = \\mu_i \\implies \\mu_i = \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)} = \\frac{1}{1+\\exp(-\\eta_i)}\n$$\n相应地，$1-\\mu_i$ 为：\n$$\n1-\\mu_i = 1 - \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)} = \\frac{1}{1+\\exp(\\eta_i)}\n$$\n将这些代入单个观测值的对数似然 $\\ell_i$ 并化简：\n$$\n\\ell_i(\\eta_i) = y_i \\ln\\left(\\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)}\\right) + (1-y_i) \\ln\\left(\\frac{1}{1+\\exp(\\eta_i)}\\right)\n$$\n$$\n\\ell_i(\\eta_i) = y_i (\\eta_i - \\ln(1+\\exp(\\eta_i))) - (1-y_i)\\ln(1+\\exp(\\eta_i)) = y_i \\eta_i - \\ln(1+\\exp(\\eta_i))\n$$\n代入 $\\eta_i = x_i^\\top\\beta$，我们得到 $\\ell_i(\\beta) = y_i x_i^\\top\\beta - \\ln(1+\\exp(x_i^\\top\\beta))$。\n假设 $n$ 个观测值是条件独立的，总对数似然 $\\ell(\\beta)$ 是各个对数似然之和：\n$$\n\\ell(\\beta) = \\sum_{i=1}^{n} \\ell_i(\\beta) = \\sum_{i=1}^{n} \\left( y_i x_i^\\top\\beta - \\ln(1+\\exp(x_i^\\top\\beta)) \\right)\n$$\n\n**2. 梯度（得分）向量 $\\nabla_{\\beta} \\ell(\\beta)$ 的推导**\n\n总对数似然的梯度是各个对数似然梯度之和。我们对 $\\ell_i(\\beta)$ 使用链式法则：\n$$\n\\nabla_{\\beta} \\ell_i(\\beta) = \\frac{\\partial \\ell_i}{\\partial \\eta_i} \\nabla_{\\beta} \\eta_i\n$$\n其中，$\\nabla_{\\beta} \\eta_i = \\nabla_{\\beta} (x_i^\\top\\beta) = x_i$。$\\ell_i$ 关于 $\\eta_i$ 的导数为：\n$$\n\\frac{\\partial \\ell_i}{\\partial \\eta_i} = \\frac{\\partial}{\\partial \\eta_i} \\left( y_i \\eta_i - \\ln(1+\\exp(\\eta_i)) \\right) = y_i - \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)} = y_i - \\mu_i\n$$\n因此，单个观测值的梯度为 $\\nabla_{\\beta} \\ell_i(\\beta) = (y_i - \\mu_i)x_i$。总梯度为：\n$$\n\\nabla_{\\beta} \\ell(\\beta) = \\sum_{i=1}^{n} (y_i - \\mu_i)x_i = \\sum_{i=1}^{n} \\left( y_i - \\frac{\\exp(x_i^\\top\\beta)}{1+\\exp(x_i^\\top\\beta)} \\right) x_i\n$$\n\n**3. Hessian 矩阵 $\\nabla_{\\beta}^2 \\ell(\\beta)$ 的推导**\n\nHessian 矩阵是梯度的导数，同样可以对每个观测值求和。我们对 $\\nabla_{\\beta} \\ell_i(\\beta)$ 关于 $\\beta^\\top$ 求导：\n$$\n\\nabla_{\\beta}^2 \\ell_i(\\beta) = \\nabla_{\\beta} \\left[ (y_i - \\mu_i(\\beta))x_i \\right]^\\top = \\nabla_{\\beta} (y_i - \\mu_i) x_i^\\top = -(\\nabla_{\\beta} \\mu_i) x_i^\\top\n$$\n我们再次应用链式法则来求 $\\nabla_{\\beta} \\mu_i$：\n$$\n\\nabla_{\\beta} \\mu_i = \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\nabla_{\\beta} \\eta_i = \\frac{\\partial \\mu_i}{\\partial \\eta_i} x_i\n$$\n$\\mu_i$ 关于 $\\eta_i$ 的导数为：\n$$\n\\frac{\\partial \\mu_i}{\\partial \\eta_i} = \\frac{\\partial}{\\partial \\eta_i} \\left( \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)} \\right) = \\frac{\\exp(\\eta_i)(1+\\exp(\\eta_i)) - \\exp(\\eta_i)^2}{(1+\\exp(\\eta_i))^2} = \\frac{\\exp(\\eta_i)}{(1+\\exp(\\eta_i))^2} = \\mu_i(1-\\mu_i)\n$$\n代回后，单个观测值的 Hessian 矩阵为：\n$$\n\\nabla_{\\beta}^2 \\ell_i(\\beta) = -(\\mu_i(1-\\mu_i)x_i)x_i^\\top = -\\mu_i(1-\\mu_i)x_i x_i^\\top\n$$\n总 Hessian 矩阵是所有观测值的总和：\n$$\n\\nabla_{\\beta}^2 \\ell(\\beta) = - \\sum_{i=1}^{n} \\mu_i(1-\\mu_i)x_i x_i^\\top = - \\sum_{i=1}^{n} \\frac{\\exp(x_i^\\top\\beta)}{(1+\\exp(x_i^\\top\\beta))^2} x_i x_i^\\top\n$$\n这便完成了三个表达式的推导。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sum_{i=1}^{n} \\left( y_i x_i^\\top \\beta - \\ln(1 + \\exp(x_i^\\top \\beta)) \\right) & \\sum_{i=1}^{n} \\left( y_i - \\frac{\\exp(x_i^\\top \\beta)}{1 + \\exp(x_i^\\top \\beta)} \\right) x_i & - \\sum_{i=1}^{n} \\frac{\\exp(x_i^\\top \\beta)}{(1 + \\exp(x_i^\\top \\beta))^2} x_i x_i^\\top\n\\end{pmatrix}\n}\n$$", "id": "4988404"}, {"introduction": "一个设定良好的模型与一个正确的拟合算法同等重要。本练习探讨了统计建模中一个常见且关键的陷阱：完全多重共线性。通过一个关于患者性别指标的简单直观的例子，您将看到协变量中的线性依赖关系如何导致 Fisher 信息矩阵不可逆，从而使参数估计无法识别，标准误变为无穷大。[@problem_id:4988423]", "problem": "考虑一项医学研究，该研究使用广义线性模型 (GLM) 框架、二项分布和典则 logit 链接来模拟院内急性肾损伤的概率。对于每位患者 $i \\in \\{1,\\dots,n\\}$，令 $Y_{i} \\in \\{0,1\\}$ 表示二元结果，线性预测变量为\n$$\n\\eta_{i} \\equiv \\beta_{0} + \\beta_{\\mathrm{M}} M_{i} + \\beta_{\\mathrm{F}} F_{i},\n$$\n其中 $M_{i} \\in \\{0,1\\}$ 是男性性别的指示变量，$F_{i} \\in \\{0,1\\}$ 是女性性别的指示变量。协变量满足对所有患者都成立的医学上正确的编码规则 $M_{i} + F_{i} = 1$（每位患者要么是男性，要么是女性，且没有患者既是男性又是女性）。平均响应为 $\\mu_{i} \\equiv \\mathbb{E}[Y_{i} \\mid M_{i},F_{i}]$，并通过 logit 链接 $\\mathrm{logit}(\\mu_{i}) = \\eta_{i}$ 与线性预测变量相关联。假设观测是独立的。\n\n给定以下 $n=6$ 位患者的设计：\n$$\n(M_{1}, F_{1}) = (1,0),\\quad (M_{2}, F_{2}) = (1,0),\\quad (M_{3}, F_{3}) = (0,1),\\quad (M_{4}, F_{4}) = (0,1),\\quad (M_{5}, F_{5}) = (1,0),\\quad (M_{6}, F_{6}) = (0,1).\n$$\n从二项 GLM 及其典则链接的第一性原理出发，根据设计矩阵和二项分布族的均值-方差关系，推导参数向量 $(\\beta_{0}, \\beta_{\\mathrm{M}}, \\beta_{\\mathrm{F}})$ 的观测 Fisher 信息矩阵，并用它来解释协变量之间的完全多重共线性对系数可识别性和方差估计的影响。然后，计算该设计的观测 Fisher 信息矩阵的行列式，并将其精确地表示为典则权重 $w_{i} \\equiv \\mu_{i}(1-\\mu_{i})$ 的函数。\n\n将行列式的精确值作为最终答案。无需四舍五入。", "solution": "该问题是有效的。它提出了一个在统计建模中定义明确的场景，特别是在广义线性模型（GLM）框架内。该设置旨在阐明完全多重共线性、参数可识别性和 Fisher 信息矩阵奇异性等基本统计概念。该问题具有科学依据，是客观的，并包含进行完整求解所需的所有必要信息。\n\n该问题要求完成三个主要任务：首先，推导具有典则链接的二项 GLM 的观测 Fisher 信息矩阵。其次，利用此框架解释完全多重共线性的后果。第三，计算给定设计的该矩阵的行列式。\n\n对于来自指数族且具有典则链接函数的 GLM 中的单个观测值 $Y_i$，其一般对数似然函数由下式给出\n$$l_{i}(\\theta_i, \\phi; y_i) = \\frac{y_i \\theta_i - b(\\theta_i)}{a_i(\\phi)} + c(y_i, \\phi)$$\n其中 $\\theta_i$ 是典则参数。对于二项分布，典则参数是对数优势比，$\\theta_i = \\mathrm{logit}(\\mu_i) = \\ln(\\frac{\\mu_i}{1-\\mu_i})$。问题指定了典则 logit 链接，这意味着线性预测变量 $\\eta_i$ 等于典则参数，即 $\\eta_i = \\theta_i$。函数 $b(\\theta_i)$ 由 $b(\\theta_i) = \\ln(1 + \\exp(\\theta_i))$ 给出。均值为 $\\mathbb{E}[Y_i] = \\mu_i = b'(\\theta_i)$，方差为 $\\mathrm{Var}(Y_i) = a_i(\\phi)b''(\\theta_i)$。对于只有 1 次试验的二项分布（伯努利分布），离散参数为 $\\phi=1$，一个常见的先验权重是 $a_i(\\phi)=1$，方差为 $\\mathrm{Var}(Y_i) = b''(\\theta_i) = \\mu_i(1-\\mu_i)$。\n\n对于 $n$ 个独立观测的整个样本，对数似然函数为 $L(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} l_i$，其中 $\\boldsymbol{\\beta} = (\\beta_0, \\beta_M, \\beta_F)^T$ 是回归系数向量。得分向量是 $L(\\boldsymbol{\\beta})$ 关于 $\\boldsymbol{\\beta}$ 的一阶导数向量：\n$$\n\\frac{\\partial L}{\\partial \\beta_j} = \\sum_{i=1}^{n} \\frac{\\partial l_i}{\\partial \\beta_j} = \\sum_{i=1}^{n} \\frac{\\partial l_i}{\\partial \\theta_i} \\frac{\\partial \\theta_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_j}\n$$\n从对数似然函数的典则形式可知，$\\frac{\\partial l_i}{\\partial \\theta_i} = y_i - b'(\\theta_i) = y_i - \\mu_i$。对于典则链接，有 $\\theta_i = \\eta_i$，所以 $\\frac{\\partial \\theta_i}{\\partial \\eta_i} = 1$。线性预测变量为 $\\eta_i = \\beta_0 + \\beta_M M_i + \\beta_F F_i = \\mathbf{x}_i^T \\boldsymbol{\\beta}$，其中 $\\mathbf{x}_i^T = (1, M_i, F_i)$。因此，$\\frac{\\partial \\eta_i}{\\partial \\beta_j} = x_{ij}$。第 $j$ 个参数的得分为：\n$$\nU_j = \\frac{\\partial L}{\\partial \\beta_j} = \\sum_{i=1}^{n} (y_i - \\mu_i) x_{ij} \\implies \\mathbf{U}(\\boldsymbol{\\beta}) = \\mathbf{X}^T (\\mathbf{y} - \\boldsymbol{\\mu})\n$$\nHessian 矩阵 $\\mathbf{H}(\\boldsymbol{\\beta})$ 由二阶偏导数组成，$H_{jk} = \\frac{\\partial^2 L}{\\partial \\beta_j \\partial \\beta_k}$。\n$$\n\\frac{\\partial^2 L}{\\partial \\beta_j \\partial \\beta_k} = \\frac{\\partial}{\\partial \\beta_k} \\sum_{i=1}^{n} (y_i - \\mu_i) x_{ij} = \\sum_{i=1}^{n} -x_{ij} \\frac{\\partial \\mu_i}{\\partial \\beta_k}\n$$\n再次使用链式法则，$\\frac{\\partial \\mu_i}{\\partial \\beta_k} = \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_k}$。对于 GLM，$\\frac{\\partial \\mu_i}{\\partial \\eta_i}$ 是反链接函数的导数。对于 logit 链接，$\\mu_i = \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)}$，且 $\\frac{d\\mu_i}{d\\eta_i} = \\mu_i(1-\\mu_i)$。这也是方差函数 $V(\\mu_i)$。\n所以，$\\frac{\\partial \\mu_i}{\\partial \\beta_k} = \\mu_i(1-\\mu_i) x_{ik}$。\n将其代回，我们得到：\n$$\nH_{jk} = \\sum_{i=1}^{n} -x_{ij} (\\mu_i(1-\\mu_i)) x_{ik}\n$$\n以矩阵形式表示，$\\mathbf{H}(\\boldsymbol{\\beta}) = -\\mathbf{X}^T \\mathbf{W} \\mathbf{X}$，其中 $\\mathbf{W}$ 是一个对角矩阵，其对角元素为 $w_i = \\mu_i(1-\\mu_i)$。观测 Fisher 信息矩阵 $\\mathbf{I}(\\boldsymbol{\\beta})$ 定义为 $\\mathbf{I}(\\boldsymbol{\\beta}) = -\\mathbb{E}[\\mathbf{H}(\\boldsymbol{\\beta})]$。对于典则链接，Hessian 矩阵不依赖于数据 $\\mathbf{y}$，因此观测信息等于期望 Fisher 信息：\n$$\n\\mathbf{I}(\\boldsymbol{\\beta}) = -\\mathbf{H}(\\boldsymbol{\\beta}) = \\mathbf{X}^T \\mathbf{W} \\mathbf{X}\n$$\n这完成了问题的第一部分。\n\n对于第二部分，我们分析多重共线性的影响。对于 $n=6$ 位患者，设计矩阵 $\\mathbf{X}$ 由给定的协变量构建而成：\n$$\n\\mathbf{X} = \\begin{pmatrix}\n1 & M_1 & F_1 \\\\\n1 & M_2 & F_2 \\\\\n1 & M_3 & F_3 \\\\\n1 & M_4 & F_4 \\\\\n1 & M_5 & F_5 \\\\\n1 & M_6 & F_6\n\\end{pmatrix} = \\begin{pmatrix}\n1 & 1 & 0 \\\\\n1 & 1 & 0 \\\\\n1 & 0 & 1 \\\\\n1 & 0 & 1 \\\\\n1 & 1 & 0 \\\\\n1 & 0 & 1\n\\end{pmatrix}\n$$\n该矩阵的列向量为 $\\mathbf{c}_0 = (1,1,1,1,1,1)^T$，$\\mathbf{c}_M = (1,1,0,0,1,0)^T$ 和 $\\mathbf{c}_F = (0,0,1,1,0,1)^T$。问题陈述了对所有患者都成立的约束条件 $M_i + F_i = 1$。就设计矩阵的列而言，这意味着 $\\mathbf{c}_M + \\mathbf{c}_F = \\mathbf{c}_0$，或 $\\mathbf{c}_0 - \\mathbf{c}_M - \\mathbf{c}_F = \\mathbf{0}$。这是 $\\mathbf{X}$ 的列之间的完全线性相关性，即所谓的完全多重共线性。\n\n这种相关性对参数的可识别性有深远影响。让我们考虑另一组参数 $\\boldsymbol{\\beta}^* = (\\beta_0^*, \\beta_M^*, \\beta_F^*)^T$，定义为 $\\beta_0^* = \\beta_0 + c$，$\\beta_M^* = \\beta_M - c$ 和 $\\beta_F^* = \\beta_F - c$，其中 $c \\in \\mathbb{R}$ 是任意常数。这组新参数的线性预测变量为：\n$$\n\\eta_i^* = \\beta_0^* + \\beta_M^* M_i + \\beta_F^* F_i = (\\beta_0 + c) + (\\beta_M - c)M_i + (\\beta_F - c)F_i = (\\beta_0 + \\beta_M M_i + \\beta_F F_i) + c(1 - M_i - F_i)\n$$\n在约束条件 $M_i + F_i = 1$ 下，对所有 $i$ 都有 $1 - M_i - F_i = 0$。因此，$\\eta_i^* = \\eta_i$。这意味着存在无限多个参数向量 $\\boldsymbol{\\beta}^*$ 能产生完全相同的预测概率 $\\mu_i$，从而得到相同的似然函数值。参数是不可识别的，因此 $\\boldsymbol{\\beta}$ 不存在唯一的最大似然估计。\n\n对方法差估计的影响也至关重要。最大似然估计 $\\hat{\\boldsymbol{\\beta}}$ 的渐近协方差矩阵由 Fisher 信息矩阵的逆给出，即 $\\mathrm{Cov}(\\hat{\\boldsymbol{\\beta}}) \\approx \\mathbf{I}(\\hat{\\boldsymbol{\\beta}})^{-1} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1}$。$\\mathbf{X}$ 列向量中的线性相关性意味着矩阵 $\\mathbf{X}^T \\mathbf{W} \\mathbf{X}$ 是奇异的。为了证明这一点，令 $\\mathbf{v} = (1, -1, -1)^T$。线性相关性为 $\\mathbf{Xv} = \\mathbf{0}$。那么，\n$$\n(\\mathbf{X}^T \\mathbf{W} \\mathbf{X})\\mathbf{v} = \\mathbf{X}^T \\mathbf{W} (\\mathbf{Xv}) = \\mathbf{X}^T \\mathbf{W} \\mathbf{0} = \\mathbf{0}\n$$\n因为存在一个非零向量 $\\mathbf{v}$ 使得 $(\\mathbf{X}^T \\mathbf{W} \\mathbf{X})\\mathbf{v} = \\mathbf{0}$，所以根据定义，矩阵 $\\mathbf{X}^T \\mathbf{W} \\mathbf{X}$ 是奇异的。奇异矩阵是不可逆的，这意味着估计量的协方差矩阵是未定义的。这对应于参数估计的方差为无穷大，使其变得无用。\n\n对于第三部分，我们计算 $\\mathbf{I}(\\boldsymbol{\\beta})$ 的行列式。对于具有相同协变量值的所有患者，权重 $w_i = \\mu_i(1-\\mu_i)$ 是恒定的。有 $n_M=3$ 位男性患者（$i=1,2,5$）和 $n_F=3$ 位女性患者（$i=3,4,6$）。\n对于男性患者，$\\eta_i = \\beta_0 + \\beta_M$。令 $w_M = \\mu_M(1-\\mu_M)$，其中 $\\mu_M = (1+\\exp(-(\\beta_0+\\beta_M)))^{-1}$。\n对于女性患者，$\\eta_i = \\beta_0 + \\beta_F$。令 $w_F = \\mu_F(1-\\mu_F)$，其中 $\\mu_F = (1+\\exp(-(\\beta_0+\\beta_F)))^{-1}$。\n权重对角矩阵为 $\\mathbf{W} = \\mathrm{diag}(w_M, w_M, w_F, w_F, w_M, w_F)$。\nFisher 信息矩阵为 $\\mathbf{I} = \\mathbf{X}^T \\mathbf{W} \\mathbf{X}$。其元素 $I_{jk} = \\sum_{i=1}^6 x_{ij} w_i x_{ik}$ 如下：\n$I_{00} = \\sum w_i = 3w_M + 3w_F$\n$I_{0M} = \\sum w_i M_i = 3w_M$\n$I_{0F} = \\sum w_i F_i = 3w_F$\n$I_{MM} = \\sum w_i M_i^2 = 3w_M$\n$I_{FF} = \\sum w_i F_i^2 = 3w_F$\n$I_{MF} = \\sum w_i M_i F_i = 0$\n完整的矩阵是：\n$$\n\\mathbf{I} = \\begin{pmatrix}\n3w_M + 3w_F & 3w_M & 3w_F \\\\\n3w_M & 3w_M & 0 \\\\\n3w_F & 0 & 3w_F\n\\end{pmatrix}\n$$\n为了求行列式，我们可以沿第一行进行余子式展开：\n\\begin{align*}\n\\det(\\mathbf{I}) &= (3w_M + 3w_F) \\begin{vmatrix} 3w_M & 0 \\\\ 0 & 3w_F \\end{vmatrix} - (3w_M) \\begin{vmatrix} 3w_M & 0 \\\\ 3w_F & 3w_F \\end{vmatrix} + (3w_F) \\begin{vmatrix} 3w_M & 3w_M \\\\ 3w_F & 0 \\end{vmatrix} \\\\\n&= (3w_M + 3w_F)(9w_M w_F) - (3w_M)(9w_M w_F - 0) + (3w_F)(0 - 9w_M w_F) \\\\\n&= (27w_M^2 w_F + 27w_M w_F^2) - 27w_M^2 w_F - 27w_M w_F^2 \\\\\n&= 0\n\\end{align*}\n另外，我们观察到 $\\mathbf{I}$ 的第一列是第二列和第三列的和。一个具有线性相关的列（或行）的矩阵的行列式为零。这证实了我们的理论预期。无论权重 $w_i$ 的值是多少，行列式都为 $0$。", "answer": "$$\n\\boxed{0}\n$$", "id": "4988423"}, {"introduction": "掌握广义线性模型的最后一步是将理论转化为实践，并学会处理现实世界数据的复杂性。这个编码练习要求您为泊松模型实现一个拟合算法，并处理计数数据中常见的过度离散问题。您将计算并比较标准的基于模型的推断与更稳健的拟似然和“三明治”估计量，揭示标准误的选择如何可能彻底改变科学结论。[@problem_id:4988393]", "problem": "考虑一项医学研究中的计数结果，该结果在广义线性模型 (GLM) 框架下进行建模。设 $y_i \\in \\{0,1,2,\\dots\\}$ 表示受试者 $i$ 的计数值，其协变量为 $x_i \\in \\mathbb{R}^p$，线性预测器为 $\\eta_i = x_i^\\top \\beta$。假设使用标准对数连接函数 $g(\\mu_i) = \\log(\\mu_i)$，其中 $\\mu_i = \\mathbb{E}[y_i \\mid x_i] = \\exp(\\eta_i)$。采用拟似然方法，方差函数为 $V(\\mu) = \\mu$，因此 $\\operatorname{Var}(y_i \\mid x_i)$ 与 $\\mu_i$ 成正比。\n\n从泊松模型的指数族表示和拟似然的定义出发，推导并实现一个 Newton–Raphson 算法来计算泊松 GLM 下的最大似然估计 $\\widehat{\\beta}$。然后，从得分函数及其期望曲率出发，为治疗效应系数构建三种类型的标准误：\n- 在泊松方差假设下的基于模型的标准误，\n- 使用估计的离散度参数 $\\widehat{\\phi}$ 和 $V(\\mu) = \\mu$ 的拟似然标准误，以及\n- 不依赖于正确方差模型的稳健 (Huber–White 三明治) 标准误。\n\n使用这些方法，在基于模型与稳健标准误下，比较关于治疗效应的推断（在 $0.05$ 水平下的双边检验）。\n\n数据生成机制：对于每个测试案例，生成独立的受试者 $i=1,\\dots,n$，包含一个用于治疗的二元协变量 $t_i \\in \\{0,1\\}$ 和一个用于年龄的连续协变量 $a_i$。设 $t_i \\sim \\text{Bernoulli}(0.5)$ 和 $a_i \\sim \\text{Uniform}(20,80)$。定义一个缩放后的年龄协变量 $z_i = (a_i - 50)/10$。平均计数值 $\\mu_i$ 遵循\n$$\n\\log(\\mu_i) = \\beta_0 + \\beta_1 t_i + \\beta_2 z_i,\n$$\n其中固定 $\\beta_0 = \\log(2.0)$ 和 $\\beta_2 = 0.02$。为引入超出泊松模型的过度离散，独立地抽取 $g_i \\sim \\text{Gamma}(k, \\text{scale}=1/k)$，其形状参数 $k > 0$，并设置\n$$\ny_i \\mid x_i, g_i \\sim \\text{Poisson}(\\mu_i g_i),\n$$\n这得到 $\\mathbb{E}[y_i \\mid x_i] = \\mu_i$ 和 $\\operatorname{Var}(y_i \\mid x_i) = \\mu_i \\left(1 + \\frac{\\mu_i}{k}\\right)$，产生的过度离散通常违反纯泊松方差模型，但适用于 $V(\\mu)=\\mu$ 的拟似然方法。\n\n基于基本原理的算法要求：\n- 使用泊松模型的对数似然函数推导得分函数和 Hessian 矩阵，并实现 Newton–Raphson 更新以获得 $\\widehat{\\beta}$。\n- 从泊松模型下的期望 Fisher 信息中，获得 $\\widehat{\\beta}$ 的基于模型的协方差。\n- 在 $V(\\mu)=\\mu$ 的条件下，通过 Pearson 方法估计离散度参数 $\\widehat{\\phi}$，并用它来缩放基于模型的协方差以获得拟似然标准误。\n- 通过将观测到的曲率与得分贡献的经验协方差相结合，构建稳健 (Huber–White 三明治) 协方差，并提取稳健标准误。\n\n假设检验：对于治疗效应系数 $\\beta_1$，计算基于模型和稳健标准误下的双边 $p$ 值，并在 $0.05$ 的水平上宣告拒绝 $H_0\\!:\\,\\beta_1 = 0$。\n\n测试套件：\n- 案例 1 (接近泊松分布)：$n=400$, $k=100$, $\\beta_1=0.25$，随机种子 $12345$。\n- 案例 2 (中度过度离散)：$n=400$, $k=5$, $\\beta_1=0.25$，随机种子 $12345$。\n- 案例 3 (小样本，严重过度离散)：$n=80$, $k=2$, $\\beta_1=0.15$，随机种子 $24680$。\n\n实现约束：\n- 拟合一个具有标准对数连接函数且无偏移量的泊松 GLM。\n- 使用 Newton–Raphson 方法，并设置合理的收敛容差和步长减半法，以确保对数似然函数单调递增。\n- 计算：\n  1. $\\widehat{\\beta}_1$ (治疗效应估计值)，\n  2. $\\widehat{\\beta}_1$ 的基于模型的标准误，\n  3. 使用 Pearson 方法在 $V(\\mu)=\\mu$ 下估计的 $\\widehat{\\phi}$ 计算 $\\widehat{\\beta}_1$ 的拟似然标准误，\n  4. $\\widehat{\\beta}_1$ 的稳健标准误，\n  5. 用于检验 $H_0\\!:\\,\\beta_1=0$ 的基于模型的双边 $p$ 值，\n  6. 用于检验 $H_0\\!:\\,\\beta_1=0$ 的稳健双边 $p$ 值，以及\n  7. 一个布尔值，指示基于模型和稳健推断的拒绝决策在 $0.05$ 水平上是否不同。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个测试案例应贡献一个形如\n$$\n[\\widehat{\\beta}_1, \\text{SE}_{\\text{model}}, \\text{SE}_{\\text{quasi}}, \\text{SE}_{\\text{robust}}, p_{\\text{model}}, p_{\\text{robust}}, \\text{changed}],\n$$\n的嵌套列表，其中 $\\text{changed}$ 是一个布尔值，指示拒绝决策是否不同。因此，最终输出必须看起来像\n$$\n[[\\dots], [\\dots], [\\dots]]\n$$\n在一行上。不涉及物理单位，所有实数值输出都应以机器可读的十进制数形式打印。", "solution": "该问题已经过验证。\n\n### 第一步：提取已知信息\n- **模型框架**：用于计数结果 $y_i \\in \\{0, 1, 2, \\dots\\}$ 的广义线性模型 (GLM)。\n- **协变量**：$x_i \\in \\mathbb{R}^p$，用于受试者 $i=1, \\dots, n$。\n- **线性预测器**：$\\eta_i = x_i^\\top \\beta$。\n- **连接函数**：标准对数连接函数，$g(\\mu_i) = \\log(\\mu_i)$，其中 $\\mu_i = \\mathbb{E}[y_i \\mid x_i]$。因此，$\\mu_i = \\exp(\\eta_i)$。\n- **方差函数**：采用拟似然方法，方差函数为 $V(\\mu) = \\mu$。\n- **任务**：\n    1. 为泊松 GLM 推导并实现一个 Newton–Raphson 算法，以找到最大似然估计 $\\widehat{\\beta}$。\n    2. 为 $\\widehat{\\beta}_1$ 构建三种类型的标准误：基于模型的、拟似然的和稳健的 (Huber–White 三明治)。\n    3. 使用 $0.05$ 水平的双边检验比较关于治疗效应系数 $\\beta_1$ 的推断。\n- **数据生成机制**：\n    - 治疗：$t_i \\sim \\text{Bernoulli}(0.5)$。\n    - 年龄：$a_i \\sim \\text{Uniform}(20, 80)$。\n    - 缩放后的年龄：$z_i = (a_i - 50)/10$。\n    - 协变量向量：$x_i = [1, t_i, z_i]^\\top$。所以 $p=3$。\n    - 均值模型：$\\log(\\mu_i) = \\beta_0 + \\beta_1 t_i + \\beta_2 z_i$。\n    - 真实参数：$\\beta_0 = \\log(2.0)$，$\\beta_2 = 0.02$。$\\beta_1$ 根据每个测试案例指定。\n    - 过度离散机制：$g_i \\sim \\text{Gamma}(k, \\text{scale}=1/k)$，以及 $y_i \\mid x_i, g_i \\sim \\text{Poisson}(\\mu_i g_i)$。得到的边际方差为 $\\operatorname{Var}(y_i \\mid x_i) = \\mu_i + \\mu_i^2/k$。\n- **算法要求**：\n    - 从泊松对数似然函数进行推导（得分函数，Hessian 矩阵）。\n    - 实现 Newton–Raphson 方法，并使用步长减半法以确保对数似然函数单调递增。\n    - 使用 Pearson 方法在 $V(\\mu)=\\mu$ 下估计离散度参数 $\\widehat{\\phi}$。\n- **假设检验**：在 $\\alpha = 0.05$ 水平下对 $H_0: \\beta_1 = 0$ 进行双边检验。\n- **测试案例**：\n    1. $n=400, k=100, \\beta_1=0.25$, 随机种子 $12345$。\n    2. $n=400, k=5, \\beta_1=0.25$, 随机种子 $12345$。\n    3. $n=80, k=2, \\beta_1=0.15$, 随机种子 $24680$。\n- **要求输出（每个案例）**：$[\\widehat{\\beta}_1, \\text{SE}_{\\text{model}}, \\text{SE}_{\\text{quasi}}, \\text{SE}_{\\text{robust}}, p_{\\text{model}}, p_{\\text{robust}}, \\text{changed}]$。`changed` 是一个布尔值，指示在 $\\alpha=0.05$ 水平下拒绝决策是否不同。\n\n### 第二步：使用提取的已知信息进行验证\n- **科学基础**：该问题位于统计理论的核心领域，具体涉及广义线性模型、最大似然估计和稳健推断。过度离散、拟似然和三明治估计量等概念是公认的，并且是现代应用统计学的基础。\n- **定义明确**：该问题指定了清晰的数据生成过程、一个明确定义的待拟合模型，以及用于估计和推断的具体数值方法。目标是可量化的，并且对于每个测试案例都会导出一组唯一的数值结果。\n- **客观性**：该问题以精确的数学和统计语言陈述，没有歧义或主观性陈述。\n\n该问题没有违反任何无效标准。这是一个标准的、尽管全面的统计计算与理论练习。\n\n### 第三步：结论与行动\n该问题是**有效的**。将提供一个完整、合理的解决方案。\n\n### 基于原理的解决方案\n\n目标是估计泊松广义线性模型 (GLM) 的参数，并使用三种不同的标准误计算方法来比较特定系数的推断。\n\n**1. 模型设定与似然函数**\n\n为构建似然函数，响应变量 $y_i$ 被假定服从泊松分布。其概率质量函数为 $P(Y_i=y_i) = \\frac{e^{-\\mu_i}\\mu_i^{y_i}}{y_i!}$。单个观测值 $i$ 的对数似然函数为：\n$$\n\\ell_i(\\beta) = \\log(P(Y_i=y_i)) = y_i \\log(\\mu_i) - \\mu_i - \\log(y_i!)\n$$\n该模型使用对数连接函数，因此均值 $\\mu_i$ 通过 $\\mu_i = \\exp(\\eta_i)$ 与线性预测器 $\\eta_i = x_i^\\top \\beta$ 相关。将此代入对数似然函数得到：\n$$\n\\ell_i(\\beta) = y_i (x_i^\\top \\beta) - \\exp(x_i^\\top \\beta) - \\log(y_i!)\n$$\n对于 $n$ 个独立观测值的总对数似然函数为 $L(\\beta) = \\sum_{i=1}^n \\ell_i(\\beta)$。\n\n**2. 通过 Newton–Raphson 方法进行最大似然估计**\n\n为找到最大似然估计 (MLE) $\\widehat{\\beta}$，我们求解 $\\frac{\\partial L(\\beta)}{\\partial \\beta} = 0$。Newton–Raphson 算法为此提供了一种迭代方法，其更新规则为：\n$$\n\\beta^{(t+1)} = \\beta^{(t)} - [H(\\beta^{(t)})]^{-1} U(\\beta^{(t)})\n$$\n其中 $U(\\beta)$ 是得分向量（$L(\\beta)$ 的梯度），$H(\\beta)$ 是 Hessian 矩阵（$L(\\beta)$ 的二阶导数）。\n\n来自观测值 $i$ 的得分贡献为：\n$$\nU_i(\\beta) = \\frac{\\partial \\ell_i}{\\partial \\beta} = \\frac{\\partial \\ell_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta} = (y_i - e^{\\eta_i}) x_i = (y_i - \\mu_i) x_i\n$$\n总得分向量为 $U(\\beta) = \\sum_{i=1}^n (y_i - \\mu_i) x_i = \\mathbf{X}^\\top (y - \\mu)$，其中 $\\mathbf{X}$ 是 $n \\times p$ 的设计矩阵，$y$ 是 $n \\times 1$ 的结果向量，$\\mu$ 是 $n \\times 1$ 的均值向量。\n\n来自观测值 $i$ 的 Hessian 贡献为：\n$$\nH_i(\\beta) = \\frac{\\partial^2 \\ell_i}{\\partial \\beta \\partial \\beta^\\top} = \\frac{\\partial}{\\partial \\beta^\\top} [(y_i - e^{x_i^\\top \\beta}) x_i] = -e^{x_i^\\top \\beta} x_i x_i^\\top = -\\mu_i x_i x_i^\\top\n$$\n总 Hessian 矩阵为 $H(\\beta) = \\sum_{i=1}^n H_i(\\beta) = -\\sum_{i=1}^n \\mu_i x_i x_i^\\top = -\\mathbf{X}^\\top \\mathbf{W} \\mathbf{X}$，其中 $\\mathbf{W}$ 是一个对角矩阵，其元素为 $W_{ii} = \\mu_i$。\n\n期望 Fisher 信息矩阵 $I(\\beta)$ 定义为 $I(\\beta) = -\\mathbb{E}[H(\\beta)]$。对于标准连接函数的泊松模型，观测信息 $-H(\\beta)$ 和期望信息 $I(\\beta)$ 是相同的：\n$$\nI(\\beta) = -H(\\beta) = \\mathbf{X}^\\top \\mathbf{W} \\mathbf{X}\n$$\nNewton–Raphson 更新变为：\n$$\n\\beta^{(t+1)} = \\beta^{(t)} + (\\mathbf{X}^\\top \\mathbf{W}^{(t)} \\mathbf{X})^{-1} \\mathbf{X}^\\top (y - \\mu^{(t)})\n$$\n这是迭代重加权最小二乘法 (IRLS) 算法的核心迭代步骤。为确保收敛，使用步长减半法来保证每一步对数似然函数都增加。\n\n**3. 标准误估计**\n\n一旦找到 MLE $\\widehat{\\beta}$，我们估计其协方差矩阵。\n\n**a. 基于模型的标准误**\n在泊松模型被正确指定的假设下（即 $\\operatorname{Var}(y_i) = \\mu_i$），$\\widehat{\\beta}$ 的渐近协方差矩阵是在 MLE 处评估的 Fisher 信息矩阵的逆：\n$$\n\\text{Cov}_{\\text{model}}(\\widehat{\\beta}) = [I(\\widehat{\\beta})]^{-1} = (\\mathbf{X}^\\top \\widehat{\\mathbf{W}} \\mathbf{X})^{-1}\n$$\n其中 $\\widehat{W}_{ii} = \\widehat{\\mu}_i = \\exp(x_i^\\top \\widehat{\\beta})$。系数 $\\widehat{\\beta}_j$ 的基于模型的标准误是该矩阵第 $j$ 个对角元素的平方根。\n\n**b. 拟似然标准误**\n拟似然方法承认方差函数可能未被正确指定。它假设 $\\operatorname{Var}(y_i) = \\phi V(\\mu_i)$，其中 $\\phi$ 是一个常数离散度参数。对于本问题，我们使用泊松方差函数 $V(\\mu_i) = \\mu_i$。参数 $\\phi$ 使用 Pearson 卡方统计量进行估计：\n$$\n\\widehat{\\phi} = \\frac{1}{n-p} \\sum_{i=1}^n \\frac{(y_i - \\widehat{\\mu}_i)^2}{V(\\widehat{\\mu}_i)} = \\frac{1}{n-p} \\sum_{i=1}^n \\frac{(y_i - \\widehat{\\mu}_i)^2}{\\widehat{\\mu}_i}\n$$\n然后，拟似然协方差矩阵是基于模型的协方差矩阵的缩放版本：\n$$\n\\text{Cov}_{\\text{quasi}}(\\widehat{\\beta}) = \\widehat{\\phi} \\cdot \\text{Cov}_{\\text{model}}(\\widehat{\\beta}) = \\widehat{\\phi} (\\mathbf{X}^\\top \\widehat{\\mathbf{W}} \\mathbf{X})^{-1}\n$$\n如果数据存在过度离散（$\\widehat{\\phi} > 1$），这些标准误将大于基于模型的标准误。\n\n**c. 稳健（三明治）标准误**\nHuber–White 稳健估计量提供了一个一致的协方差矩阵估计，即使方差函数 $V(\\mu)$ 被错误指定。其“三明治”形式由以下公式给出：\n$$\n\\text{Cov}_{\\text{robust}}(\\widehat{\\beta}) = \\mathcal{I}^{-1} \\mathcal{J} \\mathcal{I}^{-1}\n$$\n其中 $\\mathcal{I}$ 是 Fisher 信息（“面包”），$\\mathcal{J}$ 是得分函数的方差（“肉”）。它们的估计如下：\n- $\\widehat{\\mathcal{I}} = \\mathbf{X}^\\top \\widehat{\\mathbf{W}} \\mathbf{X}$\n- $\\widehat{\\mathcal{J}} = \\sum_{i=1}^n U_i(\\widehat{\\beta}) U_i(\\widehat{\\beta})^\\top = \\sum_{i=1}^n (y_i - \\widehat{\\mu}_i)^2 x_i x_i^\\top$\n因此，稳健协方差估计量为：\n$$\n\\text{Cov}_{\\text{robust}}(\\widehat{\\beta}) = (\\mathbf{X}^\\top \\widehat{\\mathbf{W}} \\mathbf{X})^{-1} \\left( \\sum_{i=1}^n (y_i - \\widehat{\\mu}_i)^2 x_i x_i^\\top \\right) (\\mathbf{X}^\\top \\widehat{\\mathbf{W}} \\mathbf{X})^{-1}\n$$\n该估计量是稳健的，因为它使用经验残差 $(y_i - \\widehat{\\mu}_i)$ 来估计得分贡献的真实方差，而不是依赖于模型假设 $\\operatorname{Var}(y_i) = \\mu_i$。\n\n**4. 假设检验**\n\n对于每种类型的标准误 (SE)，都对原假设 $H_0: \\beta_1 = 0$ 进行双边 Wald 检验。检验统计量为 $Z = \\frac{\\widehat{\\beta}_1}{\\text{SE}(\\widehat{\\beta}_1)}$。在原假设下，$Z$ 渐近服从标准正态分布 $\\mathcal{N}(0,1)$。$p$ 值计算为 $p = 2 \\cdot (1 - \\Phi(|Z|))$，其中 $\\Phi$ 是标准正态分布的累积分布函数。如果在显著性水平 $\\alpha=0.05$ 下 $p  0.05$，则拒绝原假设。实现将比较基于模型与稳健标准误的拒绝决策。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1 (near-Poisson)\n        {\"n\": 400, \"k\": 100, \"beta1\": 0.25, \"seed\": 12345},\n        # Case 2 (moderate overdispersion)\n        {\"n\": 400, \"k\": 5, \"beta1\": 0.25, \"seed\": 12345},\n        # Case 3 (small sample, severe overdispersion)\n        {\"n\": 80, \"k\": 2, \"beta1\": 0.15, \"seed\": 24680},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = _solve_single_case(**case)\n        all_results.append(result)\n        \n    # Format and print the final output as a single-line string\n    # that represents a list of lists.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\n\ndef _generate_data(n, k, beta1, seed):\n    \"\"\"\n    Generates data according to the problem specification.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # True coefficients\n    beta_true = np.array([np.log(2.0), beta1, 0.02])\n    \n    # Generate covariates\n    t = rng.binomial(1, 0.5, size=n)\n    a = rng.uniform(20, 80, size=n)\n    z = (a - 50) / 10\n    \n    # Design matrix (intercept, treatment, scaled age)\n    X = np.c_[np.ones(n), t, z]\n    \n    # Generate outcome y\n    eta = X @ beta_true\n    mu = np.exp(eta)\n    \n    # Induce overdispersion via Gamma-Poisson mixture\n    g = rng.gamma(k, scale=1.0/k, size=n)\n    y = rng.poisson(mu * g)\n    \n    return X, y\n\n\ndef _fit_poisson_glm(X, y, tol=1e-8, max_iter=100, step_halving_max=10):\n    \"\"\"\n    Fits a Poisson GLM with log link using Newton-Raphson (IRLS).\n    \"\"\"\n    n_obs, n_params = X.shape\n    beta = np.zeros(n_params)\n    \n    # Initial log-likelihood (ignoring constant term)\n    eta = X @ beta\n    mu = np.exp(eta)\n    ll_old = np.sum(y * eta - mu)\n\n    for i in range(max_iter):\n        eta = X @ beta\n        mu = np.exp(eta)\n\n        # Score vector and Fisher information matrix\n        score = X.T @ (y - mu)\n        W = np.diag(mu)\n        fisher_info = X.T @ W @ X\n\n        try:\n            # Fisher info should be invertible if X is full rank\n            inv_fisher_info = np.linalg.inv(fisher_info)\n            delta_beta = inv_fisher_info @ score\n        except np.linalg.LinAlgError:\n            # Fallback to pseudo-inverse if singular\n            inv_fisher_info = np.linalg.pinv(fisher_info)\n            delta_beta = inv_fisher_info @ score\n\n        # Step halving to ensure monotone increase in log-likelihood\n        alpha = 1.0\n        beta_new = beta\n        ll_new = -np.inf\n        for _ in range(step_halving_max):\n            beta_try = beta + alpha * delta_beta\n            eta_new = X @ beta_try\n            \n            # Check for numerical overflow before exponentiating\n            if np.any(eta_new > 50):\n                alpha /= 2\n                continue\n                \n            mu_new = np.exp(eta_new)\n            ll_try = np.sum(y * eta_new - mu_new)\n\n            if ll_try > ll_old:\n                ll_new = ll_try\n                beta_new = beta_try\n                break\n            alpha /= 2\n        \n        if ll_new == -np.inf: # No improvement found\n            break\n\n        # Check for convergence\n        if np.linalg.norm(beta_new - beta)  tol:\n            beta = beta_new\n            break\n        \n        beta = beta_new\n        ll_old = ll_new\n    \n    # Recalculate final quantities at converged beta\n    eta_hat = X @ beta\n    mu_hat = np.exp(eta_hat)\n    W_hat = np.diag(mu_hat)\n    final_fisher_info = X.T @ W_hat @ X\n    \n    return beta, mu_hat, final_fisher_info\n\n\ndef _solve_single_case(n, k, beta1, seed):\n    \"\"\"\n    Generates data, fits model, computes results for one test case.\n    \"\"\"\n    # 1. Generate data\n    X, y = _generate_data(n, k, beta1, seed)\n    n_obs, n_params = X.shape\n\n    # 2. Fit Poisson GLM to get beta_hat and ancillary quantities\n    beta_hat, mu_hat, fisher_info = _fit_poisson_glm(X, y)\n\n    # 3. Calculate all three types of standard errors for beta_1\n    \n    # 3a. Model-based SE\n    try:\n        cov_model = np.linalg.inv(fisher_info)\n        se_model_vec = np.sqrt(np.diag(cov_model))\n        se_model = se_model_vec[1]\n    except np.linalg.LinAlgError:\n        # Should not happen with well-behaved data, but as a fallback\n        cov_model = np.linalg.pinv(fisher_info)\n        se_model_vec = np.sqrt(np.diag(cov_model))\n        se_model = se_model_vec[1]\n\n    # 3b. Quasi-likelihood SE\n    pearson_chi2 = np.sum((y - mu_hat)**2 / mu_hat)\n    phi_hat = pearson_chi2 / (n_obs - n_params)\n    cov_quasi = phi_hat * cov_model\n    se_quasi = np.sqrt(cov_quasi[1, 1])\n\n    # 3c. Robust (Sandwich) SE\n    # Bread\n    bread = cov_model\n    # Meat\n    meat = np.zeros((n_params, n_params))\n    residuals = y - mu_hat\n    for i in range(n_obs):\n        meat += np.outer(X[i, :], X[i, :]) * residuals[i]**2\n    \n    cov_robust = bread @ meat @ bread\n    se_robust = np.sqrt(cov_robust[1, 1])\n\n    # 4. Perform hypothesis tests\n    beta1_hat = beta_hat[1]\n    \n    # Model-based test\n    z_model = beta1_hat / se_model\n    p_model = 2 * norm.sf(np.abs(z_model))\n    \n    # Robust test\n    z_robust = beta1_hat / se_robust\n    p_robust = 2 * norm.sf(np.abs(z_robust))\n\n    # 5. Compare rejection decisions\n    reject_model = p_model  0.05\n    reject_robust = p_robust  0.05\n    changed = reject_model != reject_robust\n\n    # 6. Format results for this case\n    return [\n        beta1_hat,\n        se_model,\n        se_quasi,\n        se_robust,\n        p_model,\n        p_robust,\n        bool(changed) # Ensure it's a Python boolean for proper str() conversion\n    ]\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "4988393"}]}