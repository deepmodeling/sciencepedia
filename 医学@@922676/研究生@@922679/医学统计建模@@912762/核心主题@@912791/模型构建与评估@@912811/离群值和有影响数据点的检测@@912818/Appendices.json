{"hands_on_practices": [{"introduction": "在医学统计建模中，我们经常遇到各种诊断指标似乎给出相互矛盾建议的复杂情况。本练习 [@problem_id:4959084] 旨在通过一个看似悖论的场景来磨练您的统计推理能力：一个数据点在全局上提高了模型参数的估计精度（表现为 $\\mathrm{COVRATIO}_i > 1$），但同时又对某个特定系数产生了巨大影响（表现为较大的 $|\\mathrm{DFBETAS}_{i,j}|$）。通过深入分析这一现象，您将学会辨别影响力的全局性与局部性，并理解数据点在多维预测变量空间中的几何位置如何决定其具体影响，这是超越公式记忆、迈向高级应用分析的关键一步。", "problem": "一项前瞻性队列研究使用普通最小二乘（OLS）线性回归对一个连续的炎症标志物结果 $y$（例如，$\\log$ C-反应蛋白）进行建模，使用的预测变量为 $x_1$（年龄）、$x_2$（身体质量指数）、$x_3$（当前吸烟指示变量）和 $x_4$（治疗分配），以及一个截距项。设设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，其中 $p=5$（包括截距项）。考虑一个特定患者 $i$，其协变量向量 $x_i \\in \\mathbb{R}^p$ 位于观测到的预测变量空间的边缘（年龄非常大且身体质量指数非常低），因此来自帽子矩阵 $H = X (X^\\top X)^{-1} X^\\top$ 的杠杆值 $h_{ii}$ 很高。假设原始残差 $r_i = y_i - x_i^\\top \\hat{\\beta}$ 的绝对值很小，并且个案删除精确度诊断指标 $\\mathrm{COVRATIO}_i$ 满足 $\\mathrm{COVRATIO}_i > 1$，其中 $\\mathrm{COVRATIO}_i$ 定义为删除个案后的估计量 $\\hat{\\beta}_{(-i)}$ 的方差-协方差矩阵的行列式与全样本 OLS 估计量 $\\hat{\\beta}$ 的方差-协方差矩阵的行列式之比。经验上观察到，该患者对年龄系数有很大的系数特定个案删除效应，其 $|\\mathrm{DFBETAS}_{i,\\text{age}}|$ 超过了常规阈值，而对吸烟和治疗系数的影响可以忽略不计。\n\n从医学统计建模的角度出发，从 OLS 估计量及其方差的基本定义开始，推断包含一个极端的协变量向量 $x_i$ 如何改变 $\\hat{\\beta}$ 的全局精确度以及某些系数的参数特定敏感性。然后，选择正确解释为什么一个高杠杆点可以同时导致 $\\mathrm{COVRATIO}_i > 1$（表明全局精确度提高）但又对特定系数具有影响力的选项，并推荐在临床回归建模的背景下，能够解决这一明显矛盾的、科学上一致的适当诊断方法。\n\n选项：\nA. 如果 $h_{ii}$ 很高但 $|r_i|$ 很小，那么该观测值对任何系数都不可能有影响力，因为产生影响力需要大的残差；因此，$\\mathrm{COVRATIO}_i > 1$ 的发现证明该个案完全是良性的，不需要进行系数水平的诊断。\nB. 包含一个高杠杆的 $x_i$ 会将 $x_i x_i^\\top$ 添加到 $X^\\top X$ 中，这会减小 $\\det\\{(X^\\top X)^{-1}\\}$，并可能导致 $\\mathrm{COVRATIO}_i > 1$（提高全局精确度），而 $x_i$ 在预测变量空间中的几何方向仍然可以以一种改变特定系数的方式重新加权信息；为了解决这个问题，应检查系数特定的个案删除效应，如 $\\mathrm{DFBETAS}_{i,j}$、分量式库克距离、针对 $x_j$ 的增量变量图，并使用稳健回归进行敏感性分析。\nC. 因为 $\\mathrm{COVRATIO}_i$ 是全局比较全样本与个案删除后的精确度，所以值 > 1 保证了该观测值改善了模型的所有方面；因此，不可能出现系数影响力，任何大的 $\\mathrm{DFBETAS}$ 值都必定是虚假的。\nD. 参数影响是方向性的：即使 $|r_i| \\approx 0$，一个大的 $h_{ii}$ 加上 $x_i$ 与 $X$ 的一个信息不足的轴对齐，也可能通过个案删除引起的 $(X^\\top X)^{-1}$ 的重新定向，导致某些系数发生显著变化；适当的诊断方法包括对每个系数计算 $\\mathrm{DFBETAS}_{i,j}$、检查条件指数和方差分解比例以评估共线性、进行局部影响分析，以及通过目标性的留一法重新拟合来量化敏感性。", "solution": "该问题陈述描述了普通最小二乘（OLS）回归中一个在应用统计学中常见的情景，尤其是在医学研究等领域。它提出了一个明显的悖论：单个数据点在提高估计系数向量 $\\hat{\\beta}$ 的*全局*精确度的同时，又对某个特定系数具有高度*影响力*。我们必须验证该问题的前提，然后利用线性回归分析的基本原理来解决这个矛盾。\n\n### 问题验证\n\n**第一步：提取已知条件**\n- **模型：** OLS 线性回归，$y = X\\beta + \\epsilon$。\n- **数据：** 一项前瞻性队列研究。\n- **结果 $y$：** 一个连续的炎症标志物。\n- **预测变量：** $x_1$（年龄）、$x_2$（身体质量指数）、$x_3$（吸烟指示变量）、$x_4$（治疗分配），外加一个截距项。设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，其中 $p=5$。\n- **观测值 $i$：** 一个特定患者，其协变量向量为 $x_i$。\n- **观测值 $i$ 的属性：**\n    1. 协变量向量 $x_i$ 位于预测变量空间的边缘（例如，年龄非常大，BMI 非常低）。\n    2. 杠杆值 $h_{ii}$ 很高。杠杆值是帽子矩阵 $H = X(X^\\top X)^{-1}X^\\top$ 的第 $i$ 个对角元素。因此，$h_{ii} = x_i^\\top(X^\\top X)^{-1}x_i$。\n    3. 原始残差 $r_i = y_i - x_i^\\top \\hat{\\beta}$ 的绝对值很小。\n    4. 个案删除精确度诊断指标 $\\mathrm{COVRATIO}_i > 1$。其定义为删除个案后的估计量 $\\hat{\\beta}_{(-i)}$ 的方差-协方差矩阵的行列式与全样本 OLS 估计量 $\\hat{\\beta}$ 的方差-协方差矩阵的行列式之比。\n    5. 该观测值对年龄系数有很大的影响力，即 $|\\mathrm{DFBETAS}_{i,\\text{age}}|$ 很大，但对其他系数的影响可以忽略不计。\n\n**第二步：使用提取的已知条件进行验证**\n该问题在科学上和统计学上都是合理的。所描述的情景是回归诊断中的一个经典案例研究。\n- **科学基础：** OLS、杠杆值、残差以及诸如 $\\mathrm{COVRATIO}$ 和 $\\mathrm{DFBETAS}$ 之类的影响诊断概念是统计建模的基础。将其应用于包含合理变量的医学研究是现实的。\n- **良态性：** 该问题要求基于一个明确定义的统计情境，给出概念性解释并推荐进一步的诊断方法。其结构旨在得出一组正确、非唯一但一致的、基于统计理论的解释和建议。\n- **客观性：** 该问题使用精确、客观和标准的统计学术语进行陈述。\n\n**第三步：结论与行动**\n问题陈述是有效的。它描述了应用回归分析中一个重要而非凡的现象。我们将着手推导解决方案。\n\n### 从第一性原理推导\n\n设 $\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y$ 是针对大小为 $n$ 的完整数据集的 OLS 估计量。$\\hat{\\beta}$ 的估计方差-协方差矩阵是 $\\widehat{\\mathrm{Var}}(\\hat{\\beta}) = s^2(X^\\top X)^{-1}$，其中 $s^2$ 是估计的误差方差。设 $\\hat{\\beta}_{(-i)}$ 和 $s_{(-i)}^2$ 是删除观测值 $i$ 时的相应量。不包含第 $i$ 行的设计矩阵是 $X_{(-i)}$。\n\n**1. 对 $\\mathrm{COVRATIO}_i$ 的分析**\n\n根据标准的定义，$\\mathrm{COVRATIO}_i$ 定义为 $\\frac{\\det(\\widehat{\\mathrm{Var}}(\\hat{\\beta}_{(-i)}))}{\\det(\\widehat{\\mathrm{Var}}(\\hat{\\beta}))}$。代入公式：\n$$ \\mathrm{COVRATIO}_i = \\frac{\\det(s_{(-i)}^2(X_{(-i)}^\\top X_{(-i)})^{-1})}{\\det(s^2(X^\\top X)^{-1})} = \\left(\\frac{s_{(-i)}}{s}\\right)^{2p} \\frac{\\det(X^\\top X)}{\\det(X_{(-i)}^\\top X_{(-i)})} $$\n全样本和删除个案后的信息矩阵之间的关系是 $X^\\top X = X_{(-i)}^\\top X_{(-i)} + x_i x_i^\\top$。使用矩阵行列式引理 $\\det(A + uv^\\top) = \\det(A)(1 + v^\\top A^{-1} u)$，以及恒等式 $1-h_{ii} = 1/(1+x_i^\\top(X_{(-i)}^\\top X_{(-i)})^{-1}x_i)$，可以推导出：\n$$ \\frac{\\det(X^\\top X)}{\\det(X_{(-i)}^\\top X_{(-i)})} = \\frac{1}{1-h_{ii}} $$\n将此代回 $\\mathrm{COVRATIO}_i$ 的表达式中：\n$$ \\mathrm{COVRATIO}_i = \\left(\\frac{s_{(-i)}}{s}\\right)^{2p} \\frac{1}{1 - h_{ii}} $$\n问题陈述中提到杠杆值 $h_{ii}$ 很高，所以 $h_{ii}$ 接近于 $1$。这使得分母 $(1 - h_{ii})$ 项成为一个很小的正数。此外，残差 $r_i$ 很小。小残差意味着观测值 $i$ 很好地拟合了全样本模型，删除它不太可能显著改变估计的误差方差。因此，比率 $s_{(-i)}/s$ 预期接近于 $1$。主导项是 $1/(1-h_{ii})$，它将 $\\mathrm{COVRATIO}_i$ 的值推向一个很大的正数（大于 $1$）。\n\n其根本原因是 $\\hat{\\beta}$ 的精确度与信息矩阵 $X^\\top X$ 的“大小”有关。添加一个高杠杆点 $x_i$（它远离数据中心）会将矩阵 $x_i x_i^\\top$ 添加到 $X_{(-i)}^\\top X_{(-i)}$ 中，有效地扩展了数据点的凸包。这通常会增加信息矩阵的“大小”（具体来说，$\\det(X^\\top X) > \\det(X_{(-i)}^\\top X_{(-i)})$），这反过来又减小了方差-协方差矩阵 $(X^\\top X)^{-1}$ 的“大小”。方差-协方差矩阵的行列式，称为广义方差，是 $\\hat{\\beta}$ 置信椭球体积的度量。较小的体积意味着较高的*全局*精确度。因此，一个（良性的）高杠杆点可以提高估计的整体精确度，导致 $\\mathrm{COVRATIO}_i > 1$。\n\n**2. 对 $\\mathrm{DFBETAS}_{i,j}$ 的分析**\n\n$\\mathrm{DFBETAS}_{i,j}$ 衡量删除观测值 $i$ 对第 $j$ 个系数 $\\hat{\\beta}_j$ 的影响。系数向量的变化由下式给出：\n$$ \\hat{\\beta} - \\hat{\\beta}_{(-i)} = \\frac{(X^\\top X)^{-1} x_i r_i}{1 - h_{ii}} $$\n$\\mathrm{DFBETAS}_{i,j}$ 的分子是该向量的第 $j$ 个分量，即 $(\\hat{\\beta} - \\hat{\\beta}_{(-i)})_j$。完整的表达式为：\n$$ \\mathrm{DFBETAS}_{i,j} = \\frac{(\\hat{\\beta}_j - \\hat{\\beta}_{j,(-i)})}{s_{(-i)}\\sqrt{((X^\\top X)^{-1})_{jj}}} = \\frac{((X^\\top X)^{-1} x_i)_j}{s_{(-i)}\\sqrt{((X^\\top X)^{-1})_{jj}}} \\frac{r_i}{1-h_{ii}} $$\n在这里，我们看到了杠杆值和残差的关键组合。尽管原始残差 $r_i$ 很小，但高杠杆值 $h_{ii}$ 使得分母 $(1-h_{ii})$ 也很小。它们的比率 $r_i / (1-h_{ii})$，即点 $i$ 相对于删除个案后回归的残差，可能很大。这个被放大的残差效应通过向量 $((X^\\top X)^{-1} x_i)_j$ 分配到各个系数上。\n\n影响是*方向性的*。观测值 $x_i$ 是 $p$ 维预测变量空间中的一个向量。它对系数的影响取决于其相对于由预测变量及其相关性定义的轴的方向。对于给定的年龄非常大、BMI 非常低的患者，向量 $x_i$ 在“年龄”和“BMI”维度上是极端的。如果“年龄”预测变量是设计矩阵 $X$ 的一个“信息不足的轴”（例如，由于共线性，或者仅仅因为在那个极端年龄上几乎没有其他数据点），那么信息矩阵 $X^\\top X$ 在那个方向上相对较弱。包含/排除点 $i$ 可能会导致逆矩阵 $(X^\\top X)^{-1}$ 发生显著的“重新定向”，从而导致相应系数 $\\hat{\\beta}_{\\text{age}}$ 发生巨大变化。而对“吸烟”或“治疗”等其他系数的影响可能可以忽略不计，如果 $x_i$ 的极端性在 $X$ 所张成的空间中与这些预测变量维度大致正交的话。\n\n**推导结论：** 这个明显的矛盾通过认识到标量的、全局的精确度度量（$\\det(\\widehat{\\mathrm{Var}}(\\hat{\\beta}))$）与向量值的、分量特定的影响力度量（$\\mathrm{DFBETAS}_i$）之间的区别而得以解决。一个高杠杆点可以在整体上稳定回归平面（减小联合置信椭球的体积），同时以一种显著改变特定斜率的方式使其发生转动。\n\n### 逐项分析\n\n**A. 如果 $h_{ii}$ 很高但 $|r_i|$ 很小，那么该观测值对任何系数都不可能有影响力，因为产生影响力需要大的残差；因此，$\\mathrm{COVRATIO}_i > 1$ 的发现证明该个案完全是良性的，不需要进行系数水平的诊断。**\n这个陈述根本上是错误的。影响力是杠杆值和残差大小的共同函数。系数变化的公式 $\\hat{\\beta} - \\hat{\\beta}_{(-i)}$ 包含了 $r_i / (1 - h_{ii})$ 这一项。对于高杠杆值（$h_{ii} \\to 1$），即使 $r_i$ 很小，这一项也可能很大。声称高杠杆点因为其残差小而不可能有影响力，是一种危险的误解。结论中建议忽略系数水平的诊断是错误的，并且在统计学上是幼稚的。\n**结论：错误。**\n\n**B. 包含一个高杠杆的 $x_i$ 会将 $x_i x_i^\\top$ 添加到 $X^\\top X$ 中，这会减小 $\\det\\{(X^\\top X)^{-1}\\}$，并可能导致 $\\mathrm{COVRATIO}_i > 1$（提高全局精确度），而 $x_i$ 在预测变量空间中的几何方向仍然可以以一种改变特定系数的方式重新加权信息；为了解决这个问题，应检查系数特定的个案删除效应，如 $\\mathrm{DFBETAS}_{i,j}$、分量式库克距离、针对 $x_j$ 的增量变量图，并使用稳健回归进行敏感性分析。**\n该选项正确地提供了基于将 $x_i x_i^\\top$ 添加到信息矩阵及其逆矩阵行列式减小从而提高全局精确度（$\\mathrm{COVRATIO}_i > 1$）的机制。它也正确地认识到 $x_i$ 的几何方向可能导致对特定系数的影响。推荐的诊断方法列表——$\\mathrm{DFBETAS}$、分量式影响度量、增量变量图和稳健回归——是全面且完全适当的，可用于调查和解决该问题。\n**结论：正确。**\n\n**C. 因为 $\\mathrm{COVRATIO}_i$ 是全局比较全样本与个案删除后的精确度，所以值 > 1 保证了该观测值改善了模型的所有方面；因此，不可能出现系数影响力，任何大的 $\\mathrm{DFBETAS}$ 值都必定是虚假的。**\n这个陈述做出了一个无效的推广。一个全局的精确度度量并不能保证模型所有方面的改善，特别是单个参数估计的稳定性。问题陈述本身就是对这一主张的直接反例。基于一个全局指标就宣称观测到的大 $\\mathrm{DFBETAS}$ 值是“虚假的”，是统计推理中的严重错误。\n**结论：错误。**\n\n**D. 参数影响是方向性的：即使 $|r_i| \\approx 0$，一个大的 $h_{ii}$ 加上 $x_i$ 与 $X$ 的一个信息不足的轴对齐，也可能通过个案删除引起的 $(X^\\top X)^{-1}$ 的重新定向，导致某些系数发生显著变化；适当的诊断方法包括对每个系数计算 $\\mathrm{DFBETAS}_{i,j}$、检查条件指数和方差分解比例以评估共线性、进行局部影响分析，以及通过目标性的留一法重新拟合来量化敏感性。**\n该选项为参数特定的影响力提供了极好且深刻的解释。它正确地指出了影响的方向性，并精确地描述了其内在机制：极端点与设计矩阵的“信息不足的轴”对齐，导致 $(X^\\top X)^{-1}$ 的“重新定向”。推荐的诊断方法也非常合适，并与选项B中的方法互为补充，侧重于诊断设计矩阵的结构（共线性诊断）和形式化的敏感性（局部影响、留一法）。其解释富有洞察力且技术上精确。\n**结论：正确。**\n\n由于选项 B 和 D 都提供了正确的解释并推荐了适当的行动，它们都是有效的答案。选项 B 对 $\\mathrm{COVRATIO}$ 和 $\\mathrm{DFBETAS}$ 现象给出了更均衡的解释，而选项 D 对方向性影响提供了更深入、更专业的解释。两者共同构成了一幅全面的图景。", "answer": "$$\\boxed{BD}$$", "id": "4959084"}, {"introduction": "传统的离群点诊断方法常常依赖于普适性的“经验法则”阈值，但这些规则在面对具体数据集的特定结构时可能并不可靠。本实践 [@problem_id:4959122] 将引导您实现一种更为严谨和现代的方法：使用自助法（bootstrap）为诊断统计量（如学生化残差和库克距离）生成经验零分布。通过这种数据自适应的方式，您可以为您的特定模型推导出量身定制的阈值，从而在控制族错误率（family-wise error rate）方面比固定规则更为精确，这对于确保医学研究结论的稳健性至关重要。", "problem": "考虑一个用于连续医学结果的标准线性模型，其中响应向量 $y \\in \\mathbb{R}^{n}$ 被建模为 $y = X\\beta + \\varepsilon$。这里，$X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵（包含一个截距列），$\\beta \\in \\mathbb{R}^{p}$ 是系数向量，$\\varepsilon \\in \\mathbb{R}^{n}$ 是噪声向量。普通最小二乘 (OLS) 估计量 $\\hat{\\beta}$ 通过最小化残差平方和得到，并产生拟合值 $\\hat{y} = X\\hat{\\beta}$ 和残差 $e = y - \\hat{y}$。OLS 的帽子矩阵为 $H = X(X^{\\top}X)^{-1}X^{\\top}$，杠杆值为 $H$ 的对角元素 $h_{ii}$。残差方差估计量为 $s^{2} = \\frac{1}{n - p}\\sum_{i=1}^{n} e_{i}^{2}$。\n\n将观测值 $i$ 的外学生化残差定义为\n$$\nr_{i} = \\frac{e_{i}}{s_{(i)}\\sqrt{1 - h_{ii}}}, \n$$\n其中 $s_{(i)}^{2}$ 是在移除观测值 $i$ 后重新拟合模型计算得到的残差方差估计量。同样，将观测值 $i$ 的 Cook 距离定义为\n$$\nD_{i} = \\frac{e_{i}^{2}}{p\\,s^{2}} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^{2}}.\n$$\n\n你需要实现一种基于原理的阈值方法，用于检测离群点和强影响数据点。该方法使用通过带重拟合的残差自助法获得的经验零分布。经验零分布是在模型 $y = X\\hat{\\beta} + \\varepsilon^{*}$ 下定义的，其中 $\\varepsilon^{*}$ 是通过对中心化后的残差进行重抽样生成的。用于离群点检测的检验统计量族是外学生化残差的绝对值 $\\{|r_{i}|\\}_{i=1}^{n}$，用于影响点检测的检验统计量族是 Cook 距离 $\\{D_{i}\\}_{i=1}^{n}$。为了将每个统计量族的族错误率 (FWER) 控制在水平 $\\alpha$，请使用在自助法重复中所有观测值 $i$ 上最大统计量的经验分布：\n- 对于 $b = 1, \\dots, B$，构建自助法响应 $y^{*(b)} = X\\hat{\\beta} + e^{*(b)}$，其中 $e^{*(b)}$ 是通过从 $\\{e_{i} - \\bar{e}\\}_{i=1}^{n}$（$\\bar{e}$ 为平均残差）中有放回地抽样得到的。\n- 将模型重新拟合到 $y^{*(b)}$ 上以计算两个统计量族，并记录 $M_{r}^{(b)} = \\max_{1 \\le i \\le n} |r_{i}^{*(b)}|$ 和 $M_{D}^{(b)} = \\max_{1 \\le i \\le n} D_{i}^{*(b)}$。\n- 将阈值 $t_{r}$ 和 $t_{D}$ 分别定义为 $\\{M_{r}^{(b)}\\}_{b=1}^{B}$ 和 $\\{M_{D}^{(b)}\\}_{b=1}^{B}$ 的经验 $(1 - \\alpha)$-分位数。\n- 如果 $|r_{i}| > t_{r}$，则将观测值 $i$ 标记为离群点；如果 $D_{i} > t_{D}$，则标记为强影响点。\n\n你的程序必须实现上述过程，并为每个测试用例返回两个已排序的零基索引列表：检测出的离群点集合和检测出的强影响点集合。\n\n不涉及物理单位，因此答案中无需提供。不涉及角度。如果 $\\alpha$ 需要小数或分数，请以小数形式表示（例如，$0.05$）。最终输出必须是整数列表。\n\n测试套件：\n实现以下 4 个测试用例。在每个用例中，通过将一个截距列（全为 1）与一个指定的预测变量列 $x$ 连接来构造 $X$。对于所有用例，根据给定的公式确定性地定义 $y$，不进行任何随机抽取。\n\n- 案例 1（基线，理想情况）：\n  - 样本量 $n = 20$，参数数量 $p = 2$。\n  - 预测变量值 $x_{i} = \\frac{i}{19}$，对于 $i = 0, 1, \\dots, 19$。\n  - 噪声偏移 $\\varepsilon_{i}$ 由列表给出\n    $[0.3, -0.4, 0.8, -0.2, 0.1, -0.7, 0.9, -0.6, 0.2, -1.1, 0.3, 0.4, -0.2, 0.7, -0.5, 0.6, -0.8, 0.2, -0.4, 0.1]$。\n  - 响应值 $y_{i} = 100 + 5 x_{i} + \\varepsilon_{i}$。\n  - 自助法重复次数 $B = 300$，FWER 水平 $\\alpha = 0.05$。\n\n- 案例 2（单个垂直离群点）：\n  - 使用与案例 1 相同的 $x_{i}$ 和 $\\varepsilon_{i}$。\n  - 如案例 1 中构建基线 $y_{i}$，然后在索引 $i = 10$ 处添加一个大小为 $+8.0$ 的单个偏差；即，设置 $y_{10} \\leftarrow y_{10} + 8.0$。\n  - 自助法重复次数 $B = 300$，FWER 水平 $\\alpha = 0.05$。\n\n- 案例 3（高杠杆强影响点）：\n  - 从案例 1 的数据开始，并追加一个额外的观测值，得到 $n = 21$。\n  - 追加 $x_{20} = 5.0$。\n  - 定义追加的响应值为 $y_{20} = 100 + 5 \\cdot x_{20} + 8.5 = 133.5$（即，一个高杠杆 $x$ 处的适度正偏差）。\n  - 自助法重复次数 $B = 300$，FWER 水平 $\\alpha = 0.05$。\n\n- 案例 4（小样本，较温和的偏差，不同水平）：\n  - 样本量 $n = 8$，参数数量 $p = 2$。\n  - 预测变量值 $x_{i} = \\frac{i}{7}$，对于 $i = 0, 1, \\dots, 7$。\n  - 噪声偏移 $\\varepsilon_{i}$ 由列表给出 $[0.1, -0.2, 0.0, 0.3, -0.4, 0.2, -0.1, 0.0]$。\n  - 响应值 $y_{i} = 100 + 5 x_{i} + \\varepsilon_{i}$，然后在索引 $i = 3$ 处添加一个较温和的大小为 $+3.0$ 的偏差：设置 $y_{3} \\leftarrow y_{3} + 3.0$。\n  - 自助法重复次数 $B = 300$，FWER 水平 $\\alpha = 0.10$。\n\n算法要求：\n- 通过矩阵代数实现 OLS 以获得 $\\hat{\\beta}$、$\\hat{y}$、$e$、$H$ 和 $h_{ii}$。\n- 使用 OLS 下有效的代数恒等式计算 $s^{2}$ 和 $s_{(i)}^{2}$，避免进行 $n$ 次重拟合。通过在分母中应用安全保护措施，确保在 $1 - h_{ii}$ 非常小的情况下的数值稳定性。\n- 对 $B$ 次重复实现带重拟合的残差自助法，以获得经验最大值 $M_{r}^{(b)}$ 和 $M_{D}^{(b)}$ 以及在所需 $\\alpha$ 水平下的阈值 $t_{r}$ 和 $t_{D}$。\n- 对每个测试用例，返回两个已排序的列表：满足 $|r_{i}| > t_{r}$ 的索引 $i$ 和满足 $D_{i} > t_{D}$ 的索引 $i$。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含一个方括号括起来的、以逗号分隔的列表。此列表的每个元素对应一个测试用例，并且本身必须是一个双元素列表，其中第一个元素是检测到的离群点索引的已排序列表，第二个元素是检测到的强影响点索引的已排序列表。例如，输出必须类似于\n$[[[i\\_1,i\\_2], [j\\_1]], [[],[k\\_1,k\\_2]], \\dots]$\n，所有索引都表示为整数，且列表按升序排列。不得打印任何额外文本。", "solution": "在线性建模中，用于离群点和强影响观测值检测的基于原理的方法的基础始于普通最小二乘法 (OLS) 和帽子矩阵的几何学。给定 $y = X\\beta + \\varepsilon$（其中 $X \\in \\mathbb{R}^{n \\times p}$），OLS 估计量为 $\\hat{\\beta} = (X^{\\top}X)^{-1} X^{\\top} y$，拟合值为 $\\hat{y} = X \\hat{\\beta}$，残差为 $e = y - \\hat{y}$。帽子矩阵 $H = X (X^{\\top}X)^{-1} X^{\\top}$ 将观测响应投影到 $X$ 的列空间上，其对角线元素 $h_{ii}$ 量化了杠杆作用，即第 $i$ 个协变量模式对其自身拟合值的影响。\n\n为了量化相对于模型在响应方向上的离群性，外学生化残差对特定于观测值的方差和留一法不确定性进行了调整。观测值 $i$ 的外学生化残差为\n$$\nr_{i} = \\frac{e_{i}}{s_{(i)}\\sqrt{1 - h_{ii}}},\n$$\n其中 $s_{(i)}^{2}$ 是从移除观测值 $i$ 后重新拟合的模型中得到的残差方差估计量。恒等式\n$$\ns_{(i)}^{2} = \\frac{(n - p)s^{2} - \\frac{e_{i}^{2}}{1 - h_{ii}}}{n - p - 1}\n$$\n可以从 Sherman–Morrison–Woodbury 引理和残差平方和的留一法分解中获得。这个恒等式使得可以在不显式重拟合 $n$ 个模型的情况下高效地计算 $s_{(i)}^{2}$。它要求 $n - p - 1 > 0$，并且在 $1 - h_{ii}$ 接近于零时需要数值保护措施。\n\n对拟合模型的影响由 Cook 距离量化，\n$$\nD_{i} = \\frac{e_{i}^{2}}{p\\,s^{2}} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^{2}},\n$$\n它衡量了当删除观测值 $i$ 时 $\\hat{\\beta}$ 的总体变化，并由残差方差和维度进行归一化。较大的 $D_{i}$ 值表示强影响点，这通常是显著的杠杆作用 $h_{ii}$ 和不可忽略的残差大小的结合。\n\n针对 $|r_{i}|$ 或 $D_{i}$ 的经典参数阈值依赖于精确的高斯假设和多重比较校正。然而，医学数据常常表现出对理想化假设的偏离。为了以一种模型无关的方式将所有观测值的族错误率 (FWER) 控制在水平 $\\alpha$，我们通过带重拟合的残差自助法使用经验零分布：\n- 拟合模型以获得 $\\hat{\\beta}$、残差 $e$ 和拟合值 $\\hat{y}$。\n- 将残差中心化以强制其均值为零，$\\tilde{e}_{i} = e_{i} - \\bar{e}$，其中 $\\bar{e} = \\frac{1}{n}\\sum_{i=1}^{n} e_{i}$。在带有截距的标准 OLS 下，$\\bar{e} = 0$，但中心化可以提高数值稳定性并保持零假设下的对称性。\n- 对于 $b = 1, \\dots, B$：\n  1. 从 $\\{\\tilde{e}_{i}\\}_{i=1}^{n}$ 中有放回地抽样得到 $\\{e_{i}^{*(b)}\\}_{i=1}^{n}$。\n  2. 形成自助法响应 $y^{*(b)} = X\\hat{\\beta} + e^{*(b)}$。\n  3. 将 OLS 重新拟合到 $(X, y^{*(b)})$ 上，以获得 $e^{*(b)}$、$h_{ii}^{*(b)}$、$s^{2*(b)}$，并计算外学生化残差 $\\{r_{i}^{*(b)}\\}$ 和 Cook 距离 $\\{D_{i}^{*(b)}\\}$。\n  4. 记录最大值 $M_{r}^{(b)} = \\max_{i} |r_{i}^{*(b)}|$ 和 $M_{D}^{(b)} = \\max_{i} D_{i}^{*(b)}$。\n- 经过 $B$ 次重复后，经验阈值是 $(1 - \\alpha)$-分位数 $t_{r} = Q_{1 - \\alpha}\\left(\\{M_{r}^{(b)}\\}_{b=1}^{B}\\right)$ 和 $t_{D} = Q_{1 - \\alpha}\\left(\\{M_{D}^{(b)}\\}_{b=1}^{B}\\right)$。\n\n这种方法可以控制所有观测值的 FWER，因为阈值是在将设计矩阵 $X$ 视为固定的情况下，根据拟合模型的残差结构下最大统计量的分布推导出来的。这些阈值能自适应由杠杆作用引起的相关性以及残差变异的尺度。如果观测到的统计量超过这些阈值，则标记该观测值：如果 $|r_{i}| > t_{r}$，则观测值 $i$ 被视为离群点；如果 $D_{i} > t_{D}$，则被视为强影响点。\n\n实现的算法步骤：\n1. 使用截距和指定的预测向量 $x$ 构建 $X$。\n2. 通过 OLS 恒等式计算 $\\hat{\\beta}$、$\\hat{y}$、$e$、$H$、$h_{ii}$ 和 $s^{2}$。\n3. 使用留一法恒等式计算 $s_{(i)}^{2}$，应用非负性和小分母保护措施：当 $(1 - h_{ii})$ 非常小时，使用一个最小下限（如 $10^{-12}$）以避免除以零；类似地，将 $s_{(i)}^{2}$ 的值限制在至少 $10^{-12}$ 以避免数值不稳定。\n4. 计算观测数据的 $|r_{i}|$ 和 $D_{i}$。\n5. 执行 $B$ 次带重拟合的残差自助法重复，并记录两个统计量族的最大值；计算经验 $(1 - \\alpha)$-分位数以得到 $t_{r}$ 和 $t_{D}$。\n6. 为每个测试用例返回两个已排序的索引列表：满足 $|r_{i}| > t_{r}$ 的索引 $i$ 和满足 $D_{i} > t_{D}$ 的索引 $i$。\n\n测试套件覆盖范围讨论：\n- 案例 1 提供了一个具有中等噪声且没有注入异常的基线；经验阈值应能适应残差尺度和设计几何，通常不会检测到任何点。\n- 案例 2 注入了一个单一的垂直离群点（在典型 $x$ 处 $y$ 有大的偏差），这应该会导致一个大的外学生化残差，并被 $|r_{i}|$ 标准检测到；如果杠杆作用保持典型水平，Cook 距离可能不太敏感。\n- 案例 3 追加了一个高杠杆点，其 $y$ 值有中等偏差；Cook 距离应该能检测到其影响，因为 $D_{i}$ 同时与 $h_{ii}$ 和 $e_{i}^{2}$ 成比例，特别是通过 $(1 - h_{ii})^{-2}$ 项惩罚大的杠杆作用。\n- 案例 4 使用小样本和较温和的偏差，在不同的 FWER 水平 $\\alpha = 0.10$ 下测试敏感性，并确保算法在较小的 $n$ 值（其中 $n - p - 1$ 仍为正）下行为正确。\n\n最终程序必须精确实现这些步骤，并打印单行内容，该行包含一个列表，列表内是四个案例对应的成对的已排序索引列表，使用零基索引，且无额外文本。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef add_intercept(x):\n    \"\"\"\n    Construct design matrix X with intercept and single predictor column.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    ones = np.ones_like(x)\n    X = np.column_stack([ones, x])\n    return X\n\ndef ols_fit(X, y):\n    \"\"\"\n    Compute OLS estimates, fitted values, residuals, residual variance, and hat matrix diagonal.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float)\n    XtX = X.T @ X\n    XtX_inv = np.linalg.inv(XtX)\n    beta = XtX_inv @ (X.T @ y)\n    fitted = X @ beta\n    resid = y - fitted\n    n, p = X.shape\n    rss = float(resid.T @ resid)\n    s2 = rss / max(n - p, 1)  # guard against division by zero, though test cases ensure n  p\n    # Efficient computation of hat matrix diagonal: diag(X @ XtX_inv @ X^T)\n    # Each diagonal element h_i = x_i^T (X^T X)^{-1} x_i\n    H_diag = np.sum(X * (X @ XtX_inv), axis=1)\n    return beta, fitted, resid, s2, H_diag\n\ndef externally_studentized_residuals_abs(X, y):\n    \"\"\"\n    Compute absolute externally studentized residuals |r_i|.\n    \"\"\"\n    beta, fitted, resid, s2, h = ols_fit(X, y)\n    n, p = X.shape\n    # Safeguards for numerical stability\n    one_minus_h = np.maximum(1.0 - h, 1e-12)\n    # Leave-one-out residual variance using OLS identity:\n    # s_(i)^2 = ((n - p) * s^2 - e_i^2 / (1 - h_ii)) / (n - p - 1)\n    denom_df = max(n - p - 1, 1)  # guard; test cases ensure n - p - 1  0\n    s_loo_sq = ((n - p) * s2 - (resid ** 2) / one_minus_h) / denom_df\n    s_loo_sq = np.maximum(s_loo_sq, 1e-12)\n    r_ext = resid / (np.sqrt(one_minus_h) * np.sqrt(s_loo_sq))\n    return np.abs(r_ext)\n\ndef cooks_distance(X, y):\n    \"\"\"\n    Compute Cook's distance D_i for each observation.\n    \"\"\"\n    beta, fitted, resid, s2, h = ols_fit(X, y)\n    n, p = X.shape\n    one_minus_h = np.maximum(1.0 - h, 1e-12)\n    denom = p * s2 * (one_minus_h ** 2)\n    denom = np.maximum(denom, 1e-15)\n    D = (resid ** 2) * h / denom\n    return D\n\ndef bootstrap_thresholds(X, y, B=300, alpha=0.05, rng_seed=12345):\n    \"\"\"\n    Residual bootstrap with refitting to compute empirical (1 - alpha)-quantile thresholds\n    for the maxima of |r_i| and D_i across observations.\n    \"\"\"\n    # Fit on the original data\n    beta, fitted, resid, s2, h = ols_fit(X, y)\n    # Center residuals\n    resid_centered = resid - np.mean(resid)\n    n = len(y)\n    rng = np.random.default_rng(rng_seed)\n    max_r_values = np.empty(B, dtype=float)\n    max_D_values = np.empty(B, dtype=float)\n    for b in range(B):\n        resampled = rng.choice(resid_centered, size=n, replace=True)\n        y_star = fitted + resampled\n        # Recompute statistics on bootstrap sample\n        r_abs_star = externally_studentized_residuals_abs(X, y_star)\n        D_star = cooks_distance(X, y_star)\n        max_r_values[b] = np.max(r_abs_star)\n        max_D_values[b] = np.max(D_star)\n    thr_r = float(np.quantile(max_r_values, 1.0 - alpha))\n    thr_D = float(np.quantile(max_D_values, 1.0 - alpha))\n    return thr_r, thr_D\n\ndef case_1():\n    # n = 20, x_i = i/19\n    n = 20\n    x = np.linspace(0.0, 1.0, n)\n    eps = np.array([0.3, -0.4, 0.8, -0.2, 0.1, -0.7, 0.9, -0.6, 0.2, -1.1,\n                    0.3, 0.4, -0.2, 0.7, -0.5, 0.6, -0.8, 0.2, -0.4, 0.1], dtype=float)\n    y = 100.0 + 5.0 * x + eps\n    B = 300\n    alpha = 0.05\n    return x, y, B, alpha\n\ndef case_2():\n    # Same as case 1, but add +8 at index 10\n    x, y, B, alpha = case_1()\n    y2 = y.copy()\n    y2[10] = y2[10] + 8.0\n    return x, y2, B, alpha\n\ndef case_3():\n    # Case 1 appended with high-leverage x=5.0 and y=133.5\n    x, y, B, alpha = case_1()\n    x3 = np.concatenate([x, np.array([5.0])])\n    y3 = np.concatenate([y, np.array([133.5])])\n    return x3, y3, B, alpha\n\ndef case_4():\n    # n = 8, x_i = i/7, mild deviation +3 at index 3, alpha = 0.10\n    n = 8\n    x = np.linspace(0.0, 1.0, n)\n    eps = np.array([0.1, -0.2, 0.0, 0.3, -0.4, 0.2, -0.1, 0.0], dtype=float)\n    y = 100.0 + 5.0 * x + eps\n    y[3] = y[3] + 3.0\n    B = 300\n    alpha = 0.10\n    return x, y, B, alpha\n\ndef analyze_case(x, y, B, alpha):\n    \"\"\"\n    Perform bootstrap-based thresholding and return lists of outlier and influential indices.\n    \"\"\"\n    X = add_intercept(x)\n    thr_r, thr_D = bootstrap_thresholds(X, y, B=B, alpha=alpha, rng_seed=12345)\n    r_abs = externally_studentized_residuals_abs(X, y)\n    D = cooks_distance(X, y)\n    outliers = np.where(r_abs  thr_r)[0]\n    influentials = np.where(D  thr_D)[0]\n    return sorted(outliers.tolist()), sorted(influentials.tolist())\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        case_1(),\n        case_2(),\n        case_3(),\n        case_4(),\n    ]\n\n    results = []\n    for x, y, B, alpha in test_cases:\n        out_idx, inf_idx = analyze_case(x, y, B, alpha)\n        results.append([out_idx, inf_idx])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4959122"}, {"introduction": "识别出离群点和强影响点后的传统做法往往是将其删除并重新拟合模型，但这种硬性决策可能导致信息损失或引入偏倚。本练习 [@problem_id:4959208] 将介绍一种更为优雅的替代策略：构建一个本身就能抵抗离群点影响的稳健回归模型。您将通过实现一个使用重尾误差分布（具体为学生t分布）的贝叶斯模型，学习如何让模型通过自适应地降低异常观测值的权重来“包容”它们，而不是简单地排除。此练习不仅能加深您对贝叶斯方法的理解，也为您提供了一个强大的工具，以获得在数据存在污染时更为可靠和稳定的参数估计。", "problem": "考虑医学中患者级别测量的线性回归模型：对于每个患者索引 $i \\in \\{1,\\dots,n\\}$，标量响应 $y_i$ 与标量协变量 $x_i$ 通过 $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$ 关联。假设在参数给定的条件下，$\\varepsilon_i$ 是独立的。我们比较两种贝叶斯设定，它们唯一的区别在于如何对观测特定的偏差进行正则化。\n\n设定 $S_{\\mathrm{N}}$ (轻尾)：$\\varepsilon_i \\mid \\sigma^2 \\sim \\mathcal{N}(0,\\sigma^2)$，其中 $\\sigma^2$ 有一个未指定的正常先验，系数向量 $(\\beta_0,\\beta_1)$ 也有一个未指定的正常先验；这些选择在标准条件下保证了正常的后验分布，但在其他方面并无规定性。\n\n设定 $S_{\\mathrm{HT}}$ (通过尺度混合实现的重尾)：引入潜在尺度 $\\lambda_i$ 并设置 $\\varepsilon_i \\mid \\lambda_i,\\sigma^2 \\sim \\mathcal{N}(0,\\lambda_i \\sigma^2)$，其中潜在尺度 $\\lambda_i$ 独立地服从 $\\mathrm{Inv\\text{-}Gamma}(\\alpha,\\beta)$ 分布，$(\\alpha,\\beta)$ 是固定的超参数，$(\\beta_0,\\beta_1,\\sigma^2)$ 具有未指定的正常先验。这种分层先验为 $y_i \\mid x_i$ 引入了重尾的边际误差，并通过允许大的 $\\lambda_i$ 值来容纳大的残差，而不是通过极端的参数偏移，从而广泛用于减轻离群值的影响。\n\n使用贝叶斯定理 $p(\\theta \\mid y) \\propto p(y \\mid \\theta)\\,p(\\theta)$（其中 $\\theta$ 集合了所有未知量）和尺度混合表示 $\\mathcal{N}(0,\\lambda_i \\sigma^2)$（其中 $\\lambda_i \\sim \\mathrm{Inv\\text{-}Gamma}(\\alpha,\\beta)$），推导遵循最大后验 (MAP) 原则的算法估计器，从条件独立性和正态线性模型的基本定义出发，不假设任何可以简化推导的闭式后验。\n\n你的程序必须为每种设定实现以下内容：\n- 一个有原则的迭代方案，对于 $S_{\\mathrm{HT}}$ 基于期望最大化 (EM) 算法，对于 $S_{\\mathrm{N}}$ 基于标准正规方程，得到 $(\\beta_0,\\beta_1)$ 和 $\\sigma^2$ 的与 MAP 等价的定点估计，以及一组反映局部降权的观测特定权重 $w_i$。对于 $S_{\\mathrm{N}}$，权重是平坦的，即对所有 $i$ 都有 $w_i = 1$；对于 $S_{\\mathrm{HT}}$，EM 算法生成的 $w_i$ 是残差的函数，当 $|y_i - \\beta_0 - \\beta_1 x_i|$ 较大时该函数值减小。\n- 从拟合的加权最小二乘法的帽子矩阵 $H$ 推导出的影响诊断。令 $X$ 表示 $n \\times 2$ 的设计矩阵，其第一列为 $1$，第二列为 $x_i$，$W$ 为对角元素为 $w_i$ 的对角矩阵。定义加权帽子矩阵 $H = W^{1/2} X (X^\\top W X)^{-1} X^\\top W^{1/2}$，残差 $r_i = y_i - \\hat{y}_i$（其中 $\\hat{y} = X \\hat{\\beta}$），并使用留一法 (LOO) 恒等式 $r_i^{(-i)} = r_i / (1 - h_{ii})$，其中 $h_{ii}$ 是 $H$ 的第 $i$ 个对角元素。对于设定 $S_{\\mathrm{N}}$，将 LOO 预测方差近似为 $\\hat{\\sigma}^2/(1 - h_{ii})$。对于设定 $S_{\\mathrm{HT}}$，将 LOO 预测方差近似为 $(\\hat{\\sigma}^2 / w_i)/(1 - h_{ii})$。这些近似必须从加权最小二乘法中的线性投影和条件高斯性的第一性原理推导出来。\n- 后验预测检查 (PPC)：对于每个 $i$，在近似的 LOO 预测分布下，计算 $|Y_i - \\mu_i^{(-i)}| \\ge |y_i - \\mu_i^{(-i)}|$ 的双边尾部概率，其中 $\\mu_i^{(-i)} = y_i - r_i^{(-i)}$，方差如上所述。在方差为 $v_i^{(-i)}$ 的正态近似下，双边尾部概率为 $p_i = 2 \\left( 1 - \\Phi\\left( \\left| r_i^{(-i)} \\right|/\\sqrt{v_i^{(-i)}} \\right) \\right)$，其中 $\\Phi$ 是标准正态累积分布函数。如果 $p_i  0.05$（表示为小数 $0.05$），则将观测值 $i$ 标记为离群值。\n- Cook 距离的类似物：定义参数维度 $p = 2$，并计算 $S_{\\mathrm{N}}$ 的 $D_i = \\frac{r_i^2}{p \\,\\hat{\\sigma}^2} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^2}$。对于 $S_{\\mathrm{HT}}$，计算加权类似物，其中 $r_i^{(w)} = \\sqrt{w_i}\\,r_i$，即 $D_i^{(w)} = \\frac{(r_i^{(w)})^2}{p \\,\\hat{\\sigma}^2} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^2}$。报告每种设定的所有 $i$ 中的最大 Cook 距离。\n\n测试套件：\n为以下确定性测试用例实现上述内容，每个用例提供 $(n,\\{x_i\\},\\{y_i\\},\\alpha,\\beta)$:\n- 用例 1 (具有中度离群值的常规情况)：$n = 24$，$x_i = i/(n-1)$ 对于 $i = 0,\\dots,23$，基础信号 $y_i^{(0)} = 5 + 2 x_i + 0.1 \\sin(10 x_i)$，以及在索引 $i = 20$ 处，量级为 $+8$ 的加性离群值。因此，对于所有 $i \\ne 20$，$y_i = y_i^{(0)}$，且 $y_{20} = y_{20}^{(0)} + 8$。重尾超参数：$(\\alpha,\\beta) = (2,2)$。\n- 用例 2 (具有高杠杆强离群值的边缘情况)：相同的 $x_i$ 和基础信号，但在索引 $i = 23$ 处，量级为 $+12$ 的加性离群值。重尾超参数：$(\\alpha,\\beta) = (2,2)$。\n- 用例 3 (没有离群值的边缘情况)：相同的 $x_i$ 和基础信号，对于所有 $i$，$y_i = y_i^{(0)}$，其中 $(\\alpha,\\beta) = (2,2)$。\n\n对于每个用例，你的程序必须输出单行，其中包含跨三个用例的结果列表。对于每个用例，按顺序报告一个包含 4 个值的列表：$S_{\\mathrm{N}}$ 下标记的离群值数量（整数），$S_{\\mathrm{HT}}$ 下标记的离群值数量（整数），$S_{\\mathrm{N}}$ 下的最大 Cook 距离（四舍五入到 6 位小数的浮点数），以及 $S_{\\mathrm{HT}}$ 下的最大 Cook 距离（四舍五入到 6 位小数的浮点数）。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含一个由方括号括起来的、以逗号分隔的用例列表。例如，具有三个用例的输出应类似于 $[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],[a_3,b_3,c_3,d_3]]$，其中每个 $a_k$ 和 $b_k$ 是整数，每个 $c_k$ 和 $d_k$ 是四舍五入到 6 位小数的浮点数。", "solution": "该问题要求推导并实现用于线性回归中离群值检测的统计方法，并比较一个标准正态模型 ($S_{\\mathrm{N}}$) 和一个稳健的重尾模型 ($S_{\\mathrm{HT}}$)。这涉及到从基本原理推导参数估计方案和影响诊断。\n\n### 问题验证\n**第 1 步：提取已知条件**\n- **线性模型**：$y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$，对于 $i \\in \\{1, \\dots, n\\}$。\n- **设定 $S_{\\mathrm{N}}$**：$\\varepsilon_i \\mid \\sigma^2 \\sim \\mathcal{N}(0,\\sigma^2)$。$(\\beta_0, \\beta_1, \\sigma^2)$ 具有正常先验。通过正规方程进行估计。权重 $w_i = 1$。\n- **设定 $S_{\\mathrm{HT}}$**：$\\varepsilon_i \\mid \\lambda_i, \\sigma^2 \\sim \\mathcal{N}(0,\\lambda_i \\sigma^2)$，其中潜在尺度 $\\lambda_i \\sim \\mathrm{Inv\\text{-}Gamma}(\\alpha,\\beta)$。$(\\beta_0, \\beta_1, \\sigma^2)$ 具有正常先验。通过期望最大化 (EM) 算法进行估计。权重 $w_i$ 是残差的函数。\n- **影响诊断**：\n    - 设计矩阵 $X$ 是 $n \\times 2$ 的，列为 $1$ 和 $x_i$。\n    - 加权帽子矩阵 $H = W^{1/2} X (X^\\top W X)^{-1} X^\\top W^{1/2}$，其中 $W = \\mathrm{diag}(w_i)$。\n    - 留一法 (LOO) 残差恒等式：$r_i^{(-i)} = r_i / (1 - h_{ii})$，其中 $r_i = y_i - x_i^\\top\\hat{\\beta}$。\n    - $S_{\\mathrm{N}}$ 的近似 LOO 预测方差：$v_i^{(-i)} = \\hat{\\sigma}^2/(1 - h_{ii})$。\n    - $S_{\\mathrm{HT}}$ 的近似 LOO 预测方差：$v_i^{(-i)} = (\\hat{\\sigma}^2 / w_i)/(1 - h_{ii})$。\n- **后验预测检查 (PPC)**：如果 $p_i  0.05$，则将观测值 $i$ 标记为离群值，其中 $p_i = 2 \\left( 1 - \\Phi\\left( |r_i^{(-i)}|/\\sqrt{v_i^{(-i)}} \\right) \\right)$，$\\Phi$ 是标准正态累积分布函数。\n- **Cook 距离的类似物**：参数维度 $p=2$。\n    - 对于 $S_{\\mathrm{N}}$：$D_i = \\frac{r_i^2}{p \\hat{\\sigma}^2} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^2}$。\n    - 对于 $S_{\\mathrm{HT}}$：$D_i^{(w)} = \\frac{w_i r_i^2}{p \\hat{\\sigma}^2} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^2}$。\n- **测试用例**：提供了三个确定性用例，指定了 $n, \\{x_i\\}, \\{y_i\\}, \\alpha, \\beta$。\n- **输出**：对于每个用例，报告 `[SN 下的离群值数, SHT 下的离群值数, SN 下的最大 Cook D, SHT 下的最大 Cook D]`。\n\n**第 2 步：使用提取的已知条件进行验证**\n- **科学依据**：该问题在标准统计理论中有充分的依据。使用正态分布的尺度混合（导致 Student-t 边际误差）进行稳健回归是一种经典技术。EM 算法是潜变量模型估计的标准方法。所有诊断方法（帽子矩阵、留一法交叉验证、Cook 距离）都是回归分析的基础。\n- **良定性**：该问题是良定的。尽管它提到“最大后验 (MAP)”估计使用“未指定的正常先验”，但它立即通过指定算法（正规方程和 EM）阐明了预期的过程。这将求解者引向实际上是最大似然估计 (MLE)，这是一种常见且有效的方法，可以看作是平坦先验下的 MAP 估计。在温和的条件下，迭代方案保证收敛。\n- **客观性**：该问题以精确的数学定义和客观标准陈述。测试用例是确定性的。\n\n**第 3 步：结论与行动**\n该问题科学合理、良定且客观。判定为**有效**。我们继续进行求解。\n\n---\n\n### 估计器和诊断的推导\n\n令 $\\beta = [\\beta_0, \\beta_1]^\\top$ 为系数向量，令 $X$ 为 $n \\times 2$ 的设计矩阵，其第 $i$ 行为 $x_i^\\top = [1, x_i]$。\n\n#### 设定 $S_{\\mathrm{N}}$ (标准正态模型)\n\n**1. 参数估计**\n该问题要求进行 MAP 估计。在未指定先验的情况下，我们寻找使数据似然最大化的参数值。正态模型的对数似然函数为：\n$$ \\mathcal{L}(\\beta, \\sigma^2; y, X) = \\log p(y | X, \\beta, \\sigma^2) = \\sum_{i=1}^n \\log \\mathcal{N}(y_i; x_i^\\top\\beta, \\sigma^2) $$\n$$ \\mathcal{L} = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i - x_i^\\top\\beta)^2 $$\n为了找到 MAP/MLE 估计 $\\hat{\\beta}$，我们最小化平方误差和项 $\\sum_{i=1}^n (y_i - x_i^\\top\\beta)^2 = \\|y - X\\beta\\|^2$。这是普通最小二乘 (OLS) 准则。解由正规方程 $X^\\top X \\hat{\\beta} = X^\\top y$ 给出，得到：\n$$ \\hat{\\beta} = (X^\\top X)^{-1}X^\\top y $$\n为了找到 $\\hat{\\sigma}^2$，我们将 $\\mathcal{L}$ 对 $\\sigma^2$ 求导并令其为零，代入 $\\hat{\\beta}$：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2}\\sum_{i=1}^n (y_i - x_i^\\top\\hat{\\beta})^2 = 0 \\implies \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (y_i - x_i^\\top\\hat{\\beta})^2 = \\frac{1}{n} \\|y - X\\hat{\\beta}\\|^2 $$\n对于此模型，权重是均匀的，$w_i=1$ 对所有 $i$ 成立，因此 $W=I_n$。\n\n**2. 诊断**\n- **帽子矩阵和 LOO 残差**：帽子矩阵为 $H = X(X^\\top X)^{-1}X^\\top$。对角元素 $h_{ii}$ 是杠杆值。留一法 (LOO) 残差恒等式 $r_i^{(-i)} = r_i / (1 - h_{ii})$ 是线性投影代数中的一个标准结果。\n- **LOO 预测方差**：$y_i$ 的 LOO 预测是 $\\mu_i^{(-i)} = x_i^\\top \\hat{\\beta}^{(-i)}$。预测误差 $Y_i - \\mu_i^{(-i)}$ 的方差是观测方差和预测方差之和：$\\mathrm{Var}(Y_i - \\mu_i^{(-i)}) = \\mathrm{Var}(\\varepsilon_i) + \\mathrm{Var}(x_i^\\top \\hat{\\beta}^{(-i)})$。这计算为 $\\sigma^2 + \\sigma^2 x_i^\\top(X_{(-i)}^\\top X_{(-i)})^{-1}x_i$。使用 Sherman-Morrison-Woodbury 公式，可以证明 $x_i^\\top(X_{(-i)}^\\top X_{(-i)})^{-1}x_i = h_{ii}/(1-h_{ii})$。因此，精确的 LOO 预测方差为：\n$$ \\mathrm{Var}(Y_i - \\mu_i^{(-i)}) = \\sigma^2 \\left(1 + \\frac{h_{ii}}{1-h_{ii}}\\right) = \\frac{\\sigma^2}{1-h_{ii}} $$\n使用我们的估计 $\\hat{\\sigma}^2$，我们得到指定的近似 $v_i^{(-i)} = \\hat{\\sigma}^2 / (1-h_{ii})$。\n- **PPC p 值**：检验统计量是 $Z_i = (Y_i - \\mu_i^{(-i)}) / \\sqrt{v_i^{(-i)}}$，近似服从 $\\mathcal{N}(0,1)$ 分布。该统计量变为 $|z_i| = |r_i^{(-i)}|/\\sqrt{v_i^{(-i)}} = |r_i| / \\sqrt{\\hat{\\sigma}^2(1-h_{ii})}$。双边 p 值为 $p_i = 2(1 - \\Phi(|z_i|))$。\n- **Cook 距离**：直接使用公式 $D_i = \\frac{r_i^2}{p \\hat{\\sigma}^2} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^2}$，其中 $p=2$。\n\n#### 设定 $S_{\\mathrm{HT}}$ (重尾模型)\n\n**1. 通过 EM 算法进行参数估计**\n该模型引入了潜变量 $\\lambda_i \\sim \\mathrm{Inv\\text{-}Gamma}(\\alpha, \\beta)$，使得直接最大化似然函数变得困难。我们使用 EM 算法，该算法迭代计算完全数据对数似然的期望（E 步），然后最大化它（M 步）。\n完全数据对数似然（在 $\\beta, \\log \\sigma$ 的平坦先验下与对数后验成正比）是：\n$$ \\log p(y, \\lambda | X, \\beta, \\sigma^2) \\propto \\sum_{i=1}^n \\left( \\log p(y_i|\\lambda_i, \\beta, \\sigma^2) + \\log p(\\lambda_i|\\alpha, \\beta) \\right) $$\n$$ \\propto \\sum_{i=1}^n \\left( -\\frac{1}{2}\\log(\\lambda_i \\sigma^2) - \\frac{(y_i - x_i^\\top\\beta)^2}{2\\lambda_i\\sigma^2} - (\\alpha+1)\\log\\lambda_i - \\frac{\\beta}{\\lambda_i} \\right) $$\n\n- **E 步**：在第 $t$ 次迭代中，我们计算 $\\lambda_i$ 的充分统计量（即 $1/\\lambda_i$ 和 $\\log\\lambda_i$）在给定数据 $y$ 和当前参数 $\\theta^{(t)} = \\{\\beta^{(t)}, (\\sigma^2)^{(t)}\\}$ 条件下的期望。$\\lambda_i$ 的条件后验为：\n$$ p(\\lambda_i | y_i, \\theta^{(t)}) \\propto p(y_i|\\lambda_i, \\theta^{(t)}) p(\\lambda_i) \\propto \\lambda_i^{-1/2} e^{-\\frac{(r_i^{(t)})^2}{2\\lambda_i(\\sigma^2)^{(t)}}} \\cdot \\lambda_i^{-(\\alpha+1)} e^{-\\frac{\\beta}{\\lambda_i}} $$\n其中 $r_i^{(t)} = y_i - x_i^\\top \\beta^{(t)}$。这是一个逆伽马分布的核：\n$$ \\lambda_i | y_i, \\theta^{(t)} \\sim \\mathrm{Inv\\text{-}Gamma}\\left(\\alpha' = \\alpha + \\frac{1}{2}, \\beta' = \\beta + \\frac{(r_i^{(t)})^2}{2(\\sigma^2)^{(t)}}\\right) $$\nM 步所需的期望是 $w_i^{(t+1)} = E[1/\\lambda_i | y, \\theta^{(t)}]$。对于一个 $\\mathrm{Inv\\text{-}Gamma}(a, b)$ 分布，$E[1/X] = a/b$。因此：\n$$ w_i^{(t+1)} = \\frac{\\alpha + 1/2}{\\beta + \\frac{(r_i^{(t)})^2}{2(\\sigma^2)^{(t)}}} = \\frac{2\\alpha + 1}{2\\beta + (r_i^{(t)})^2/(\\sigma^2)^{(t)}} $$\n这些是观测特定的权重。大的残差 $|r_i^{(t)}|$ 导致小的权重 $w_i^{(t+1)}$，从而降低潜在离群值的影响。\n\n- **M 步**：我们最大化期望完全数据对数似然 $Q(\\theta|\\theta^{(t)}) = E_{\\lambda|y,\\theta^{(t)}}[\\log p(y, \\lambda|\\theta)]$:\n$$ Q(\\theta|\\theta^{(t)}) \\propto -\\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n w_i^{(t+1)} (y_i - x_i^\\top\\beta)^2 $$\n对 $\\beta$ 进行最大化是一个加权最小二乘 (WLS) 问题：\n$$ \\hat{\\beta}^{(t+1)} = (X^\\top W^{(t+1)} X)^{-1} X^\\top W^{(t+1)} y $$\n其中 $W^{(t+1)} = \\mathrm{diag}(w_i^{(t+1)})$。对 $\\sigma^2$ 进行最大化得到：\n$$ (\\hat{\\sigma}^2)^{(t+1)} = \\frac{1}{n} \\sum_{i=1}^n w_i^{(t+1)} (y_i - x_i^\\top\\hat{\\beta}^{(t+1)})^2 $$\nE 步和 M 步被迭代执行，直到参数收敛。\n\n**2. 诊断**\n- **帽子矩阵和 LOO 残差**：WLS 拟合等价于对变换后的变量 $y^* = W^{1/2}y$ 和 $X^* = W^{1/2}X$ 进行 OLS 拟合。相关的帽子矩阵是针对这个变换后问题的，$H = X^*((X^*)^\\top X^*)^{-1}(X^*)^\\top = W^{1/2}X(X^\\top WX)^{-1}X^\\top W^{1/2}$，如问题所述。其对角元素为 $h_{ii} = w_i x_i^\\top (X^\\top WX)^{-1} x_i$。对于这个 $h_{ii}$ 的定义，LOO 残差恒等式 $r_i^{(-i)} = r_i / (1 - h_{ii})$ 仍然成立。\n- **LOO 预测方差**：在 $S_{HT}$ 模型中，WLS 公式意味着观测 $i$ 的有效误差方差为 $\\sigma^2/w_i$。LOO 预测方差的推导与 OLS 情况类似，但包含了这个观测特定的方差。总方差为 $\\mathrm{Var}(Y_i) + \\mathrm{Var}(\\text{prediction}) = \\frac{\\sigma^2}{w_i} + \\mathrm{Var}(x_i^\\top \\hat{\\beta}^{(-i)})$。第二项计算为 $\\frac{\\sigma^2}{w_i} \\frac{h_{ii}}{1-h_{ii}}$。将它们相加得到：\n$$ \\mathrm{Var}(Y_i - \\mu_i^{(-i)}) = \\frac{\\sigma^2}{w_i} \\left(1 + \\frac{h_{ii}}{1-h_{ii}}\\right) = \\frac{\\sigma^2}{w_i(1-h_{ii})} $$\n使用 EM 算法的估计值，我们得到近似 $v_i^{(-i)} = (\\hat{\\sigma}^2/w_i)/(1 - h_{ii})$。\n- **PPC p 值**：标准化的 LOO 残差为 $|z_i| = |r_i^{(-i)}|/\\sqrt{v_i^{(-i)}} = |r_i|\\sqrt{w_i} / \\sqrt{\\hat{\\sigma}^2(1-h_{ii})}$。p 值为 $p_i = 2(1 - \\Phi(|z_i|))$。\n- **Cook 距离**：公式 $D_i^{(w)} = \\frac{w_i r_i^2}{p \\hat{\\sigma}^2} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^2}$ 是标准 Cook 距离在加权回归中的直接类似物。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases.\n    \"\"\"\n    \n    test_cases_params = [\n        {'n': 24, 'outlier_idx': 20, 'outlier_mag': 8.0, 'alpha': 2.0, 'beta_p': 2.0},\n        {'n': 24, 'outlier_idx': 23, 'outlier_mag': 12.0, 'alpha': 2.0, 'beta_p': 2.0},\n        {'n': 24, 'outlier_idx': None, 'outlier_mag': 0.0, 'alpha': 2.0, 'beta_p': 2.0},\n    ]\n\n    all_results = []\n    \n    for params in test_cases_params:\n        n = params['n']\n        x_vals = np.linspace(0, 1, n)\n        y_base = 5.0 + 2.0 * x_vals + 0.1 * np.sin(10.0 * x_vals)\n        y_vals = y_base.copy()\n        if params['outlier_idx'] is not None:\n            y_vals[params['outlier_idx']] += params['outlier_mag']\n        \n        alpha = params['alpha']\n        beta_p = params['beta_p']\n        \n        case_results = process_case(n, x_vals, y_vals, alpha, beta_p)\n        all_results.append(case_results)\n\n    # Format output\n    output_str = \"[\" + \",\".join([f\"[{r[0]},{r[1]},{r[2]:.6f},{r[3]:.6f}]\" for r in all_results]) + \"]\"\n    print(output_str)\n\ndef process_case(n, x_vals, y_vals, alpha, beta_p):\n    \"\"\"\n    Processes a single test case, performing analysis for S_N and S_HT models.\n    \"\"\"\n    X = np.stack([np.ones(n), x_vals], axis=1)\n    y = y_vals\n    p_dim = 2.0  # float for calculations\n\n    # ----- Specification S_N (Normal Model) -----\n    w_n = np.ones(n)\n    \n    # Parameter estimation\n    try:\n        beta_n = np.linalg.solve(X.T @ X, X.T @ y)\n    except np.linalg.LinAlgError:\n        beta_n = np.linalg.pinv(X.T @ X) @ X.T @ y\n        \n    r_n = y - X @ beta_n\n    sigma_sq_n = np.mean(r_n**2)\n    sigma_sq_n_safe = max(sigma_sq_n, 1e-12)\n\n    # Hat matrix diagonals\n    try:\n        C_n = np.linalg.inv(X.T @ X)\n        h_n = np.einsum('ij,jk,ki-i', X, C_n, X.T)\n    except np.linalg.LinAlgError:\n         h_n = np.full(n, 1.0/n) \n    \n    # PPC outlier detection\n    h_n_safe = np.minimum(h_n, 1.0 - 1e-9)\n    one_minus_h_n = 1.0 - h_n_safe\n    v_n_loo = np.divide(sigma_sq_n_safe, one_minus_h_n, out=np.full_like(one_minus_h_n, np.inf), where=one_minus_h_n!=0)\n    r_n_loo = np.divide(r_n, one_minus_h_n, out=np.full_like(one_minus_h_n, np.inf), where=one_minus_h_n!=0)\n    z_n_num = np.abs(r_n_loo)\n    z_n_den = np.sqrt(v_n_loo)\n    z_n = np.divide(z_n_num, z_n_den, out=np.zeros_like(z_n_den), where=z_n_den!=0)\n    p_vals_n = 2 * (1 - norm.cdf(z_n))\n    outliers_n = np.sum(p_vals_n  0.05)\n    \n    # Cook's distance\n    cook_d_n_term = np.divide(h_n_safe, one_minus_h_n**2, out=np.zeros_like(one_minus_h_n), where=one_minus_h_n!=0)\n    cook_d_n = (r_n**2 / (p_dim * sigma_sq_n_safe)) * cook_d_n_term\n    max_cook_d_n = np.max(cook_d_n)\n    \n    # ----- Specification S_HT (Heavy-Tailed Model) -----\n    beta_ht = beta_n.copy()\n    sigma_sq_ht = sigma_sq_n\n    \n    n_iter = 100\n    for _ in range(n_iter):\n        r_ht_iter = y - X @ beta_ht\n        sigma_sq_ht_safe_iter = max(sigma_sq_ht, 1e-12)\n        \n        w_denom = (2.0 * beta_p + r_ht_iter**2 / sigma_sq_ht_safe_iter)\n        w_ht = np.divide(2.0 * alpha + 1.0, w_denom, out=np.zeros_like(w_denom), where=w_denom!=0)\n        W_ht = np.diag(w_ht)\n        \n        X_T_W_ht = X.T @ W_ht\n        try:\n           beta_ht_new = np.linalg.solve(X_T_W_ht @ X, X_T_W_ht @ y)\n        except np.linalg.LinAlgError:\n           beta_ht_new = np.linalg.pinv(X_T_W_ht @ X) @ (X_T_W_ht @ y)\n\n        r_ht_updated = y - X @ beta_ht_new\n        sigma_sq_ht_new = np.mean(w_ht * r_ht_updated**2)\n\n        if np.allclose(beta_ht, beta_ht_new, atol=1e-6) and np.isclose(sigma_sq_ht, sigma_sq_ht_new, atol=1e-6):\n            break\n        beta_ht = beta_ht_new\n        sigma_sq_ht = sigma_sq_ht_new\n    \n    # Final values after convergence\n    r_ht = y - X @ beta_ht\n    sigma_sq_ht_safe = max(sigma_sq_ht, 1e-12)\n    w_ht_final_denom = (2.0 * beta_p + r_ht**2 / sigma_sq_ht_safe)\n    w_ht_final = np.divide(2.0 * alpha + 1.0, w_ht_final_denom, out=np.zeros_like(w_ht_final_denom), where=w_ht_final_denom!=0)\n    W_ht_final = np.diag(w_ht_final)\n    \n    # Hat matrix diagonals\n    try:\n        C_ht = np.linalg.inv(X.T @ W_ht_final @ X)\n        h_ht = w_ht_final * np.einsum('ij,jk,ki-i', X, C_ht, X.T)\n    except np.linalg.LinAlgError:\n        h_ht = np.full(n, 1.0/n)\n        \n    # PPC outlier detection\n    h_ht_safe = np.minimum(h_ht, 1.0 - 1e-9)\n    one_minus_h_ht = 1.0 - h_ht_safe\n    w_ht_final_safe = np.maximum(w_ht_final, 1e-12)\n\n    v_ht_loo = np.divide(sigma_sq_ht_safe, w_ht_final_safe * one_minus_h_ht, out=np.full_like(one_minus_h_ht, np.inf), where=(w_ht_final_safe * one_minus_h_ht)!=0)\n    r_ht_loo = np.divide(r_ht, one_minus_h_ht, out=np.full_like(one_minus_h_ht, np.inf), where=one_minus_h_ht!=0)\n\n    z_ht_num = np.abs(r_ht_loo)\n    z_ht_den = np.sqrt(v_ht_loo)\n    z_ht = np.divide(z_ht_num, z_ht_den, out=np.zeros_like(z_ht_den), where=z_ht_den!=0)\n    p_vals_ht = 2 * (1 - norm.cdf(z_ht))\n    outliers_ht = np.sum(p_vals_ht  0.05)\n    \n    # Weighted Cook's distance\n    cook_d_ht_term = np.divide(h_ht_safe, one_minus_h_ht**2, out=np.zeros_like(one_minus_h_ht), where=one_minus_h_ht!=0)\n    cook_d_ht_num = w_ht_final * r_ht**2\n    cook_d_ht = (cook_d_ht_num / (p_dim * sigma_sq_ht_safe)) * cook_d_ht_term\n    max_cook_d_ht = np.max(cook_d_ht)\n\n    return outliers_n, outliers_ht, max_cook_d_n, max_cook_d_ht\n\nif __name__ == '__main__':\n    solve()\n\n```", "id": "4959208"}]}