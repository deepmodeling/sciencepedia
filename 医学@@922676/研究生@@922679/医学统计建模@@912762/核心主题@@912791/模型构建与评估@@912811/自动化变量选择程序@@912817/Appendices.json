{"hands_on_practices": [{"introduction": "在自动化变量选择中，惩罚函数的选择至关重要，因为不同的惩罚函数在处理高度相关的预测变量时会表现出截然不同的行为。此练习 [@problem_id:4953120] 深入探讨了极小极大凹惩罚（Minimax Concave Penalty, MCP）这一非凸惩罚的性质。通过分析其行为，您将对惩罚函数的形式（由其凹度参数 $\\gamma$ 控制）如何影响模型选择（是倾向于选择单个代表性变量还是保留一组相关变量）建立更深刻的直觉，这在基因组学和生物标志物研究中是一个关键考量。", "problem": "一个临床研究团队正在开发一个惩罚线性回归模型，用以根据在 $n$ 名患者身上测量的一组 $p$ 个标准化生物标志物 $X_1,\\dots,X_p$ 来预测一个连续的风险评分 $Y$（例如，急性肾损伤的综合风险指数）。两个生物标志物 $X_1$ 和 $X_2$ 高度相关，因为它们测量的是重叠的生理过程，其经验相关性为 $\\rho \\in (0,1)$，并且标准化满足 $\\|X_j\\|_2^2 = n$（对于 $j \\in \\{1,2\\}$）。该团队通过最小化以下惩罚目标函数来拟合模型：\n$$\nQ(\\beta) \\;=\\; \\frac{1}{2n}\\,\\|Y - X\\beta\\|_2^2 \\;+\\; \\sum_{j=1}^p p_{\\lambda,\\gamma}\\!\\left(|\\beta_j|\\right),\n$$\n其中 $p_{\\lambda,\\gamma}(\\cdot)$ 是最小最大凹惩罚（Minimax Concave Penalty, MCP），对于 $t \\ge 0$ 定义为\n$$\np_{\\lambda,\\gamma}(t) \\;=\\; \\int_0^t \\left(\\lambda - \\frac{u}{\\gamma}\\right)_+\\,du,\\quad p'_{\\lambda,\\gamma}(t) \\;=\\; \\left(\\lambda - \\frac{t}{\\gamma}\\right)_+,\n$$\n调节参数为 $\\lambda>0$ 和 $\\gamma>1$，且 $(a)_+ = \\max\\{a,0\\}$。假设通常用于凸损失和可分离惩罚的 Karush–Kuhn–Tucker (KKT) 最优性条件适用于刻画坐标级的最小值。设 $(X_1,X_2)$ 的真实信号非零且量级相当，并假设 $\\rho$很大（例如，$\\rho \\approx 0.9$）。\n\n根据惩罚最小二乘法、KKT 平稳性和次梯度条件的定义，以及 MCP 导数 $p'_{\\lambda,\\gamma}(\\cdot)$ 的形状进行推理，以下哪项最能描述 MCP 何时倾向于选择一个代表性的生物标志物而不是同时选择两个相关的生物标志物，以及这种倾向如何依赖于凹度参数 $\\gamma$？\n\nA. 当 $\\rho$ 很大且两个信号量级相当时，减小 $\\gamma$（使惩罚更凹）会增加只选择 $\\{X_1, X_2\\}$ 中一个的倾向，因为对第一个被选中系数的更大、偏差更小的估计会使第二个系数的偏残差相关性降低到进入阈值以下；增大 $\\gamma$（使惩罚的凹性减弱，更接近 $\\ell_1$ 惩罚）则增加了当它们各自的偏得分都超过同一阈值时两者都进入模型的机会。\n\nB. 对于较小的 $\\gamma$（强凹性），MCP 的行为类似于二次惩罚，因此会鼓励相关的预测变量像岭回归中那样一起进入模型；因此，在高 $\\rho$ 的情况下，MCP 通常会同时保留 $X_1$ 和 $X_2$。\n\nC. 如果 $\\rho = 1$ 且真实效应满足 $\\beta_1^\\star = \\beta_2^\\star \\neq 0$，那么对于任何 $\\gamma$，一旦 $\\lambda  |\\beta_1^\\star|$，MCP 的解必然会包含 $X_1$ 和 $X_2$，因为相同的信号会压倒惩罚项。\n\nD. 增大 $\\gamma$ 会增强分组效应，迫使高度相关的生物标志物一同进入或离开模型；因此，更大的 $\\gamma$ 总是增加两者都被选中或都被排除的概率，而这与 $\\lambda$ 无关。", "solution": "用户希望在线性回归背景下，分析最小最大凹惩罚（MCP）的变量选择属性，特别是关于高度相关的预测变量。\n\n### 问题验证\n\n我将首先验证问题陈述。\n\n**步骤1：提取已知条件**\n-   **模型**：用于连续响应变量 $Y$ 的惩罚线性回归。\n-   **预测变量**：$n$ 名患者的 $p$ 个标准化生物标志物 $X_1, \\dots, X_p$。\n-   **相关性**：两个生物标志物 $X_1$ 和 $X_2$ 高度相关，经验相关性为 $\\rho \\in (0,1)$，且 $\\rho \\approx 0.9$。\n-   **标准化**：对于 $j \\in \\{1, 2\\}$，预测变量被标准化，使得其平方 $\\ell_2$-范数为 $\\|X_j\\|_2^2 = n$。这等价于说样本方差为 $1$，因为假设预测变量是中心化的，则 $\\frac{1}{n} \\|X_j\\|_2^2 = \\frac{1}{n}\\sum_{i=1}^n X_{ij}^2 = 1$。\n-   **目标函数**：模型通过最小化 $Q(\\beta) \\;=\\; \\frac{1}{2n}\\,\\|Y - X\\beta\\|_2^2 \\;+\\; \\sum_{j=1}^p p_{\\lambda,\\gamma}\\!\\left(|\\beta_j|\\right)$ 来拟合。\n-   **惩罚函数**：惩罚是最小最大凹惩罚（MCP），对于非负参数 $t \\ge 0$ 定义为 $p_{\\lambda,\\gamma}(t) \\;=\\; \\int_0^t \\left(\\lambda - \\frac{u}{\\gamma}\\right)_+\\,du$。\n-   **惩罚导数**：惩罚函数的导数为 $p'_{\\lambda,\\gamma}(t) \\;=\\; \\left(\\lambda - \\frac{t}{\\gamma}\\right)_+$。\n-   **参数**：调节参数为 $\\lambda > 0$ 和 $\\gamma > 1$。\n-   **假设**：\n    1.  Karush–Kuhn–Tucker (KKT) 最优性条件适用于刻画坐标级的最小值。\n    2.  与 $X_1$ 和 $X_2$ 对应的真实系数非零且量级相当。\n\n**步骤2：使用提取的已知条件进行验证**\n-   **科学基础**：该问题在统计学习和高维统计理论中有充分的依据。惩罚回归、MCP 惩罚和 KKT 条件都是标准概念。使用相关生物标志物进行建模是生物统计学和生物信息学中一个常见且重要的问题。所有定义和公式都是正确的。\n-   **适定性**：问题是适定的。它要求对特定条件下统计估计量的行为进行定性分析。给定的信息足以通过对目标函数及其相关最优性条件的性质进行推理来完成此分析。\n-   **客观性**：问题以精确、客观的数学和统计语言陈述。没有主观或含糊不清的术语。\n\n问题陈述内部一致、科学合理且适定。它没有违反任何无效标准。\n\n**步骤3：结论与行动**\n该问题有效。我将继续进行推导和求解。\n\n### 推导\n\n问题的核心在于理解 KKT 最优性条件，以及由 $\\gamma$ 控制的 MCP 惩罚项的形状如何影响相关预测变量的解。\n\n令 $\\hat{\\beta}$ 为最小化目标函数 $Q(\\beta)$ 的估计系数向量。每个坐标 $j \\in \\{1, \\dots, p\\}$ 的 KKT 条件由 $Q(\\beta)$ 的次梯度导出。最小二乘损失项关于 $\\beta_j$ 的梯度是 $-\\frac{1}{n}X_j^\\top(Y - X\\beta)$。$\\beta_j$ 的惩罚项的次梯度是 $\\partial p_{\\lambda,\\gamma}(|\\beta_j|)$。\n\n在最小值 $\\hat{\\beta}$ 处，坐标 $j$ 的 KKT 平稳性条件是：\n$$\n\\frac{1}{n} X_j^\\top (Y - X\\hat{\\beta}) \\in \\partial p_{\\lambda,\\gamma}(|\\hat{\\beta}_j|) \\cdot \\text{sgn}(\\hat{\\beta}_j)\n$$\n其中 $\\partial p_{\\lambda,\\gamma}(|t|)$ 是惩罚函数的次梯度。\n具体来说：\n1.  如果 $\\hat{\\beta}_j = 0$，那么 $|\\frac{1}{n} X_j^\\top (Y - X\\hat{\\beta})| \\le \\lambda$。预测变量与偏残差的偏相关性的绝对值必须低于阈值 $\\lambda$。\n2.  如果 $\\hat{\\beta}_j \\neq 0$，那么 $\\frac{1}{n} X_j^\\top (Y - X\\hat{\\beta}) = p'_{\\lambda,\\gamma}(|\\hat{\\beta}_j|) \\cdot \\text{sgn}(\\hat{\\beta}_j)$。\n\n让我们关注 $X_1$ 和 $X_2$。我们使用坐标下降法的直觉来分析其行为。假设我们处于模型中没有预测变量的阶段（$\\beta=0$）。如果预测变量 $X_j$ 的得分 $|\\frac{1}{n}X_j^\\top Y|$ 超过阈值 $\\lambda$，它就会进入模型。鉴于 $X_1$ 和 $X_2$ 具有强、相当的信号并且高度相关，很可能 $|\\frac{1}{n}X_1^\\top Y|$ 和 $|\\frac{1}{n}X_2^\\top Y|$ 都大于 $\\lambda$。\n\n不失一般性，我们假设 $X_1$ 的得分稍高并首先进入模型。我们更新其系数 $\\hat{\\beta}_1$，同时保持所有其他系数为零。$\\beta_1$ 的 KKT 条件变为：\n$$\n\\frac{1}{n} X_1^\\top (Y - X_1\\hat{\\beta}_1) = p'_{\\lambda,\\gamma}(|\\hat{\\beta}_1|) \\cdot \\text{sgn}(\\hat{\\beta}_1)\n$$\n鉴于标准化 $\\|X_1\\|_2^2=n$，我们有 $\\frac{1}{n}X_1^\\top X_1 = 1$。方程简化为：\n$$\n\\frac{1}{n} X_1^\\top Y - \\hat{\\beta}_1 = p'_{\\lambda,\\gamma}(|\\hat{\\beta}_1|) \\cdot \\text{sgn}(\\hat{\\beta}_1)\n$$\n假设得分和系数都为正，且 $|\\hat{\\beta}_1|  \\lambda\\gamma$（因此惩罚是活跃的）：\n$$\n\\frac{1}{n} X_1^\\top Y - \\hat{\\beta}_1 = \\lambda - \\frac{\\hat{\\beta}_1}{\\gamma} \\quad \\implies \\quad \\hat{\\beta}_1\\left(1 - \\frac{1}{\\gamma}\\right) = \\frac{1}{n} X_1^\\top Y - \\lambda\n$$\n解出 $\\hat{\\beta}_1$ 得到 MCP 软阈值算子：\n$$\n\\hat{\\beta}_1 = \\frac{\\frac{1}{n} X_1^\\top Y - \\lambda}{1 - 1/\\gamma} = \\frac{\\gamma}{\\gamma-1} \\left( \\frac{1}{n} X_1^\\top Y - \\lambda \\right)\n$$\n\n现在，我们必须检查 $X_2$ 是否会进入模型。$X_2$ 的新得分是其与当前残差 $r^{(1)} = Y - X_1\\hat{\\beta}_1$ 的偏相关性：\n$$\nS_2 = \\left| \\frac{1}{n} X_2^\\top r^{(1)} \\right| = \\left| \\frac{1}{n} X_2^\\top Y - \\frac{1}{n} X_2^\\top X_1 \\hat{\\beta}_1 \\right|\n$$\n鉴于标准化和相关性 $\\rho$，我们有 $\\frac{1}{n}X_2^\\top X_1 = \\rho$。\n$$\nS_2 = \\left| \\frac{1}{n} X_2^\\top Y - \\rho \\hat{\\beta}_1 \\right|\n$$\n如果 $S_2 > \\lambda$，$X_2$ 将被选中。\n\n让我们分析 $\\gamma$ 的影响：\n\n-   **情况1：减小 $\\gamma$（趋近于 $1$，凹性更强）**\n    当 $\\gamma \\to 1^+$ 时，分母 $(1 - 1/\\gamma) \\to 0^+$。乘法因子 $\\frac{\\gamma}{\\gamma-1}$ 变得非常大。这意味着得到的系数 $\\hat{\\beta}_1$ 与标准软阈值估计相比是“去偏的”或被放大的。一个小的初始超额得分 $(\\frac{1}{n}X_1^\\top Y - \\lambda)$ 会导致一个大的系数 $\\hat{\\beta}_1$。因为 $\\hat{\\beta}_1$ 很大且 $\\rho$ 很高（接近 $1$），所以得分 $S_2$ 中的项 $\\rho \\hat{\\beta}_1$ 变得很大。由于初始得分 $\\frac{1}{n}X_1^\\top Y$ 和 $\\frac{1}{n}X_2^\\top Y$ 是相当的，并且对于相关预测变量，它们的关系近似为 $\\frac{1}{n}X_2^\\top Y \\approx \\rho \\cdot (\\frac{1}{n}X_1^\\top Y)$，新得分 $S_2 \\approx |\\rho (\\frac{1}{n}X_1^\\top Y) - \\rho \\hat{\\beta}_1|$ 将被显著减小。这种减小使得 $S_2  \\lambda$ 的可能性大大增加。本质上，$X_1$ 上的大系数“解释掉”了与 $X_2$ 共享的信号，从而阻止 $X_2$ 进入模型。这种行为有利于从相关组中选择单个代表。\n\n-   **情况2：增大 $\\gamma$（趋近于 $\\infty$，凹性减弱，接近 $\\ell_1$ 惩罚）**\n    当 $\\gamma \\to \\infty$ 时，因子 $\\frac{\\gamma}{\\gamma-1} \\to 1$。MCP 惩罚的行为接近于 LASSO（$\\ell_1$）惩罚。系数估计变为 $\\hat{\\beta}_1 \\approx \\frac{1}{n} X_1^\\top Y - \\lambda$。这是标准的 LASSO 估计，已知它会向零偏置（收缩）。因为这个 $\\hat{\\beta}_1$ 比强凹情况下的要小（收缩得更多），所以得分 $S_2$ 中的项 $\\rho \\hat{\\beta}_1$ 也更小。残差得分 $S_2 = |\\frac{1}{n} X_2^\\top Y - \\rho \\hat{\\beta}_1|$ 减少的量更小，因此更有可能保持在阈值 $\\lambda$ 之上。因此，$X_1$ 和 $X_2$ 都更有可能被选中。这与 LASSO 的已知行为一致，LASSO 倾向于选择一组相关的变量（尽管它可能会在它们之间任意分配系数的权重）。\n\n### 逐项分析\n\n**A. 当 $\\rho$ 很大且两个信号量级相当时，减小 $\\gamma$（使惩罚更凹）会增加只选择 $\\{X_1, X_2\\}$ 中一个的倾向，因为对第一个被选中系数的更大、偏差更小的估计会使第二个系数的偏残差相关性降低到进入阈值以下；增大 $\\gamma$（使惩罚的凹性减弱，更接近 $\\ell_1$ 惩罚）则增加了当它们各自的偏得分都超过同一阈值时两者都进入模型的机会。**\n这个陈述与上面的推导完全一致。它正确地指出减小 $\\gamma$ 会导致选择单个变量，提供了正确的机制（一个更大、偏差更小的估计减少了另一个变量的残差相关性），并正确地描述了增大 $\\gamma$ 时行为接近 $\\ell_1$（LASSO）行为，此时两者都更可能被选中。\n**结论：正确。**\n\n**B. 对于较小的 $\\gamma$（强凹性），MCP 的行为类似于二次惩罚，因此会鼓励相关的预测变量像岭回归中那样一起进入模型；因此，在高 $\\rho$ 的情况下，MCP 通常会同时保留 $X_1$ 和 $X_2$。**\n这是不正确的。岭回归中使用的二次（$\\ell_2$）惩罚是凸的。小 $\\gamma$ 的 MCP 是强凹的。凹惩罚促进稀疏性和变量选择，而凸的 $\\ell_2$ 惩罚促进“分组效应”，使相关预测变量获得相似的系数并一起被保留。小 $\\gamma$ 的 MCP 行为更接近于最佳子集选择，这与岭回归式的分组效应正好相反。\n**结论：不正确。**\n\n**C. 如果 $\\rho = 1$ 且真实效应满足 $\\beta_1^\\star = \\beta_2^\\star \\neq 0$，那么对于任何 $\\gamma$，一旦 $\\lambda  |\\beta_1^\\star|$，MCP 的解必然会包含 $X_1$ 和 $X_2$，因为相同的信号会压倒惩罚项。**\n这是不正确的。如果 $\\rho=1$，预测变量 $X_1$ 和 $X_2$ 是完全共线的。由于标准化，$X_1 = X_2$。模型项是 $X_1\\beta_1 + X_2\\beta_2 = X_1(\\beta_1+\\beta_2)$。损失函数仅依赖于和 $\\beta_{sum} = \\beta_1+\\beta_2$。惩罚项是 $p_{\\lambda,\\gamma}(|\\beta_1|) + p_{\\lambda,\\gamma}(|\\beta_2|)$。对于固定的和 $\\beta_{sum}$，要最小化惩罚项，$p_{\\lambda,\\gamma}(\\cdot)$ 的凹性意味着解将位于边界上，即一个系数为零，另一个为 $\\beta_{sum}$。例如，$p_{\\lambda,\\gamma}(|\\beta_{sum}|)  p_{\\lambda,\\gamma}(|\\beta_{sum}/2|) + p_{\\lambda,\\gamma}(|\\beta_{sum}/2|)$。因此，解将设置 $\\hat{\\beta}_1=0$ 或 $\\hat{\\beta}_2=0$。MCP 将只选择两个预测变量中的一个，而不是两个都选。\n**结论：不正确。**\n\n**D. 增大 $\\gamma$ 会增强分组效应，迫使高度相关的生物标志物一同进入或离开模型；因此，更大的 $\\gamma$ 总是增加两者都被选中或都被排除的概率，而这与 $\\lambda$ 无关。**\n这在多个方面都是不正确的。首先，它错误地描述了 $\\gamma$ 的效应。如前所述，减小 $\\gamma$ 会增强“反分组”的选择效应。增大 $\\gamma$ 使惩罚更像 LASSO，而 LASSO 并不具有像弹性网那样强制的分组效应。其次，声称这种行为“与 $\\lambda$ 无关”是错误的。参数 $\\lambda$ 控制整体正则化的强度，并设置变量进入模型的阈值。选择过程从根本上依赖于 $\\lambda$。\n**结论：不正确。**", "answer": "$$\\boxed{A}$$", "id": "4953120"}, {"introduction": "在拟合多个惩罚模型后，一个关键步骤是选出最佳模型，这需要一种有原则的方法来平衡模型拟合度与复杂度。此练习 [@problem_id:4953129] 超越了简单的参数计数，引入了惩罚模型的“有效自由度”($df_{\\lambda}$)概念。您将为一个带惩罚的 Cox 比例风险模型推导并计算赤池信息准则（Akaike Information Criterion, AIC），这是在现代回归环境中应用信息准则的一项基本技能。", "problem": "一项基于医院的队列研究使用 Cox 比例风险模型对右删失的事件时间结果进行建模，其中，对于协变量向量为 $x \\in \\mathbb{R}^{p}$ 的患者，在时间 $t$ 的风险为 $h(t \\mid x) = h_{0}(t) \\exp(x^{\\top} \\beta)$，其中 $h_{0}(t)$ 是一个未指明的基线风险。通过在惩罚偏似然拟合中最小化不同惩罚强度下的信息准则来进行自动化变量选择。考虑一个使用二次惩罚矩阵和调整参数的惩罚拟合，该参数仅收缩系数的一个子集，得到具有以下特征的最大惩罚偏似然估计 $\\hat{\\beta}_{\\lambda}$：\n\n- 在 $\\hat{\\beta}_{\\lambda}$ 处的偏对数似然为 $\\ell_{p}(\\hat{\\beta}_{\\lambda}) = -85.3$。\n- 在 $\\hat{\\beta}_{\\lambda}$ 处偏似然的观测信息矩阵为\n$$\nJ(\\hat{\\beta}_{\\lambda}) \\;=\\;\n\\begin{pmatrix}\n50  2  1 \\\\\n2  30  0 \\\\\n1  0  20\n\\end{pmatrix}.\n$$\n- 二次惩罚项为 $\\frac{\\lambda}{2}\\,\\beta^{\\top} P \\beta$，调整参数 $\\lambda = 10$，惩罚矩阵为\n$$\nP \\;=\\;\n\\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  0\n\\end{pmatrix},\n$$\n因此第三个系数未被惩罚。\n\n从赤池信息准则 (AIC) 作为最大似然拟合的期望库尔贝克-莱布勒散度的渐近无偏估计量的信息论基础出发，并考虑到 Cox 模型的半参数结构以及惩罚引入的偏差，推导适用于通过带二次惩罚的偏似然拟合的 Cox 模型的 AIC 表达式。特别地，请论证为何用基于局部曲率的有效自由度项替换朴素的参数计数，然后使用所提供的矩阵计算给定拟合的 AIC。\n\n将最终的 AIC 数值四舍五入到四位有效数字。无需单位。", "solution": "首先验证问题，确保其科学上成立、提法恰当，并提供了所有必要信息。\n\n**步骤 1：提取已知条件**\n- 模型：Cox 比例风险模型，$h(t \\mid x) = h_{0}(t) \\exp(x^{\\top} \\beta)$。\n- 估计方法：惩罚偏似然最大化。\n- 估计量处的偏对数似然：$\\ell_{p}(\\hat{\\beta}_{\\lambda}) = -85.3$。\n- 在 $\\hat{\\beta}_{\\lambda}$ 处求值的偏似然的观测信息矩阵：\n$$\nJ(\\hat{\\beta}_{\\lambda}) =\n\\begin{pmatrix}\n50  2  1 \\\\\n2  30  0 \\\\\n1  0  20\n\\end{pmatrix}.\n$$\n- 惩罚项：$\\frac{\\lambda}{2}\\,\\beta^{\\top} P \\beta$。\n- 调整参数：$\\lambda = 10$。\n- 惩罚矩阵：\n$$\nP =\n\\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  0\n\\end{pmatrix}.\n$$\n\n**步骤 2：使用提取的已知条件进行验证**\n问题是有效的。它设置在生存分析和惩罚回归的标准、成熟的框架内。Cox 模型、偏似然、二次惩罚（岭回归）和信息准则都是现代统计学中的基本概念。所有必需的数值和矩阵都已提供，并且它们的维度是一致的。惩罚矩阵 $P$ 的结构正确地反映了第三个系数未被惩罚的陈述。该问题是客观的、自洽的且科学上合理的。\n\n**步骤 3：推导与求解**\n赤池信息准则 (AIC) 是一种广泛用于模型选择的度量标准，其基础原理是最小化拟合模型与真实潜在数据生成过程之间的期望库尔贝克-莱布勒散度。对于通过最大似然估计的模型，标准 AIC 定义为：\n$$\n\\text{AIC} = -2\\ell(\\hat{\\theta}) + 2k\n$$\n其中 $\\ell(\\hat{\\theta})$ 是最大化的对数似然， $k$ 是估计参数的数量。项 $2k$ 作为模型复杂度的惩罚项，纠正了使用相同数据进行模型拟合和评估所带来的乐观偏差。\n\n在 Cox 比例风险模型的背景下，回归系数 $\\beta$ 是通过最大化偏对数似然 $\\ell_p(\\beta)$ 而不是完整似然来估计的。对于一个未惩罚的 Cox 模型，AIC 类似地定义为：\n$$\n\\text{AIC} = -2\\ell_p(\\hat{\\beta}) + 2p\n$$\n其中 $\\hat{\\beta}$ 是最大偏似然估计， $p$ 是协变量的数量，即 $\\beta$ 的维度。\n\n问题指明系数是通过最大化一个*惩罚*偏对数似然来估计的：\n$$\n\\ell_{\\text{pen}}(\\beta; \\lambda) = \\ell_p(\\beta) - \\frac{\\lambda}{2} \\beta^{\\top} P \\beta\n$$\n得到的估计量 $\\hat{\\beta}_{\\lambda}$ 不再是最大偏似然估计。惩罚项在系数估计中引入了偏差（将惩罚分量向零收缩），但可以减少其方差，从而带来更好的预测性能。因此，朴素的参数计数 $p$ 不再是模型复杂度或“自由度”的准确度量。惩罚项限制了参数空间，因此拟合的有效参数数量小于 $p$。\n\n为了将 AIC 推广到此类惩罚模型，参数计数 $p$ 被一个“有效自由度”项取代，记为 $df_{\\lambda}$。该项量化了拟合的惩罚模型的复杂度。对于二次惩罚，统计理论中一个公认的结果给出的有效自由度为：\n$$\ndf_{\\lambda} = \\text{tr}\\left( \\left( J(\\hat{\\beta}_{\\lambda}) + \\lambda P \\right)^{-1} J(\\hat{\\beta}_{\\lambda}) \\right)\n$$\n此处，$J(\\hat{\\beta}_{\\lambda}) = -\\frac{\\partial^2 \\ell_p(\\beta)}{\\partial \\beta \\partial \\beta^{\\top}} \\big|_{\\beta=\\hat{\\beta}_{\\lambda}}$ 是在惩罚估计量 $\\hat{\\beta}_{\\lambda}$ 处求值的未惩罚偏似然的观测信息矩阵。矩阵 $\\left( J(\\hat{\\beta}_{\\lambda}) + \\lambda P \\right)$ 是惩罚偏对数似然的 Hessian 矩阵的负数，代表了目标函数在解处的总曲率。\n\n因此，惩罚 Cox 模型的广义 AIC 为：\n$$\n\\text{AIC}_{\\lambda} = -2\\ell_p(\\hat{\\beta}_{\\lambda}) + 2df_{\\lambda}\n$$\n关键要注意，此公式中的似然项是在惩罚估计量处求值的*未惩罚*偏对数似然。\n\n我们现在使用所提供的数据计算该值。\n首先，我们构造矩阵 $H = J(\\hat{\\beta}_{\\lambda}) + \\lambda P$：\n$$\n\\lambda P = 10 \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  0 \\end{pmatrix} = \\begin{pmatrix} 10  0  0 \\\\ 0  10  0 \\\\ 0  0  0 \\end{pmatrix}\n$$\n$$\nH = J(\\hat{\\beta}_{\\lambda}) + \\lambda P = \\begin{pmatrix} 50  2  1 \\\\ 2  30  0 \\\\ 1  0  20 \\end{pmatrix} + \\begin{pmatrix} 10  0  0 \\\\ 0  10  0 \\\\ 0  0  0 \\end{pmatrix} = \\begin{pmatrix} 60  2  1 \\\\ 2  40  0 \\\\ 1  0  20 \\end{pmatrix}\n$$\n接下来，我们必须求 $H$ 的逆矩阵。$H$ 的行列式为：\n$$\n\\det(H) = 60(40 \\cdot 20 - 0 \\cdot 0) - 2(2 \\cdot 20 - 0 \\cdot 1) + 1(2 \\cdot 0 - 40 \\cdot 1)\n$$\n$$\n\\det(H) = 60(800) - 2(40) - 40 = 48000 - 80 - 40 = 47880\n$$\n$H$ 的伴随矩阵，即其代数余子式矩阵的转置，为：\n$$\n\\text{adj}(H) = \\begin{pmatrix}\n+(40 \\cdot 20 - 0 \\cdot 0)  -(2 \\cdot 20 - 1 \\cdot 0)  +(2 \\cdot 0 - 40 \\cdot 1) \\\\\n-(2 \\cdot 20 - 0 \\cdot 1)  +(60 \\cdot 20 - 1 \\cdot 1)  -(60 \\cdot 0 - 2 \\cdot 1) \\\\\n+(2 \\cdot 0 - 40 \\cdot 1)  -(60 \\cdot 0 - 2 \\cdot 1)  +(60 \\cdot 40 - 2 \\cdot 2)\n\\end{pmatrix}^{\\top}\n$$\n$$\n\\text{adj}(H) = \\begin{pmatrix} 800  -40  -40 \\\\ -40  1199  2 \\\\ -40  2  2396 \\end{pmatrix}^{\\top} = \\begin{pmatrix} 800  -40  -40 \\\\ -40  1199  2 \\\\ -40  2  2396 \\end{pmatrix}\n$$\n逆矩阵为 $H^{-1} = \\frac{1}{47880} \\text{adj}(H)$。\n现在我们计算矩阵乘积 $M = H^{-1} J(\\hat{\\beta}_{\\lambda})$：\n$$\nM = \\frac{1}{47880} \\begin{pmatrix} 800  -40  -40 \\\\ -40  1199  2 \\\\ -40  2  2396 \\end{pmatrix} \\begin{pmatrix} 50  2  1 \\\\ 2  30  0 \\\\ 1  0  20 \\end{pmatrix}\n$$\n有效自由度是该矩阵的迹，$df_{\\lambda} = \\text{tr}(M) = M_{11} + M_{22} + M_{33}$。我们只需要计算对角线元素：\n$$\nM_{11} = \\frac{1}{47880} (800 \\cdot 50 - 40 \\cdot 2 - 40 \\cdot 1) = \\frac{40000 - 80 - 40}{47880} = \\frac{39880}{47880}\n$$\n$$\nM_{22} = \\frac{1}{47880} (-40 \\cdot 2 + 1199 \\cdot 30 + 2 \\cdot 0) = \\frac{-80 + 35970}{47880} = \\frac{35890}{47880}\n$$\n$$\nM_{33} = \\frac{1}{47880} (-40 \\cdot 1 + 2 \\cdot 0 + 2396 \\cdot 20) = \\frac{-40 + 47920}{47880} = \\frac{47880}{47880} = 1\n$$\n$M_{33} = 1$ 的值是符合预期的，因为第三个系数未被惩罚，因此恰好贡献一个完整的自由度。\n总有效自由度是这些对角线元素之和：\n$$\ndf_{\\lambda} = \\frac{39880}{47880} + \\frac{35890}{47880} + \\frac{47880}{47880} = \\frac{39880 + 35890 + 47880}{47880} = \\frac{123650}{47880}\n$$\n数值上，$df_{\\lambda} \\approx 2.582665$。正如预期的那样，这小于朴素的参数计数 $p=3$。\n\n最后，我们计算 AIC：\n$$\n\\text{AIC}_{\\lambda} = -2\\ell_p(\\hat{\\beta}_{\\lambda}) + 2df_{\\lambda} = -2(-85.3) + 2\\left(\\frac{123650}{47880}\\right)\n$$\n$$\n\\text{AIC}_{\\lambda} = 170.6 + 2(2.582665...) = 170.6 + 5.16533... = 175.76533...\n$$\n四舍五入到四位有效数字，我们得到 $175.8$。", "answer": "$$\\boxed{175.8}$$", "id": "4953129"}, {"introduction": "我们不必拘泥于寻找单一的“最佳”模型，通过组合一个由不同变量选择程序生成的多样化模型库，通常可以获得更优越的预测性能。此练习 [@problem_id:4953094] 从第一性原理的角度介绍了堆叠（stacking）这一强大的集成技术。您将通过解决一个约束最小二乘优化问题，来推导组合模型预测的最优权重，为这一广泛使用的方法奠定坚实的理论基础。", "problem": "在一项临床结局研究中，一家医院通过对一个包含$n$名患者的训练队列应用自动化变量选择程序，为连续终点$Y$开发了一个包含$m$个预后模型的库。具体来说，该模型库包括在多个惩罚水平下使用最小绝对收缩和选择算子（LASSO）以及在多个模型大小下使用前向逐步选择生成的模型。利用$K$折交叉验证，该医院为每个模型计算折外预测：对于每位患者$i \\in \\{1,\\dots,n\\}$和每个模型$j \\in \\{1,\\dots,m\\}$，数量$\\hat{y}_{ij}$是在不包含患者$i$的折上拟合模型$j$所产生的对$Y_i$的预测。设$Z \\in \\mathbb{R}^{n \\times m}$是元素为$Z_{ij} = \\hat{y}_{ij}$的矩阵，设$Y \\in \\mathbb{R}^{n}$是观测结局的向量。\n\n为了组合这个模型库，考虑一个堆叠预测器，它形成模型预测的线性组合，其权重为$w \\in \\mathbb{R}^{m}$，并受单纯形平均约束$\\sum_{j=1}^{m} w_j = 1$的限制。选择堆叠权重的目的是最小化交叉验证的均方误差，即求解以下优化问题：\n$$\n\\min_{w \\in \\mathbb{R}^{m}} \\; \\frac{1}{n} \\sum_{i=1}^{n} \\left(Y_i - \\sum_{j=1}^{m} w_j Z_{ij}\\right)^{2} \\quad \\text{subject to} \\quad \\sum_{j=1}^{m} w_j = 1.\n$$\n假设$Z$的列是线性无关的，因此$Z^{\\top} Z$是可逆的。\n\n从凸优化和最小二乘法的基本原理出发，在上述单一线性等式约束下，且不对权重施加任何非负性约束的情况下，推导最优权重向量关于$Z$和$Y$的闭式表达式。此外，论证该优化问题是否为凸问题，并说明其极小值点唯一的条件。请用标准的线性代数运算，将你的最终答案表示为最优权重向量关于$Z$和$Y$的单一解析表达式。不要引入任何数值近似。你的答案必须是一个无单位的单一表达式。", "solution": "该问题要求推导一个带约束的最小二乘问题中最优权重的闭式表达式，并证明问题的凸性及其解的唯一性。\n\n首先，我们使用矩阵表示法将问题形式化。设$w \\in \\mathbb{R}^{m}$为权重向量，$Y \\in \\mathbb{R}^{n}$为观测结局向量，$Z \\in \\mathbb{R}^{n \\times m}$为折外预测矩阵。优化问题如下：\n$$\n\\min_{w \\in \\mathbb{R}^{m}} \\; \\frac{1}{n} \\sum_{i=1}^{n} \\left(Y_i - \\sum_{j=1}^{m} Z_{ij} w_j\\right)^{2} \\quad \\text{subject to} \\quad \\sum_{j=1}^{m} w_j = 1.\n$$\n求和项$\\sum_{i=1}^{n} \\left(Y_i - \\sum_{j=1}^{m} Z_{ij} w_j\\right)^{2}$是向量$Y - Zw$的欧几里得范数的平方，记作$\\|Y - Zw\\|_2^2$。约束可以用向量形式写成$\\mathbf{1}^\\top w = 1$，其中$\\mathbf{1}$是一个包含$m$个1的列向量。常数因子$\\frac{1}{n}$不影响最小值点的位置，因此我们可以等价地最小化目标函数$f(w) = \\|Y - Zw\\|_2^2$。问题因此变为：\n$$ \\min_{w \\in \\mathbb{R}^{m}} f(w) = \\|Y - Zw\\|_2^2 \\quad \\text{subject to} \\quad \\mathbf{1}^\\top w = 1. $$\n\n接下来，我们讨论问题的凸性和解的唯一性。目标函数可以展开为：\n$$ f(w) = (Y - Zw)^\\top(Y - Zw) = Y^\\top Y - 2Y^\\top Z w + w^\\top Z^\\top Z w. $$\n这是关于权重向量$w$的二次函数。为确定其凸性，我们计算它关于$w$的Hessian矩阵。$f(w)$的梯度是：\n$$ \\nabla_w f(w) = -2Z^\\top Y + 2Z^\\top Z w. $$\nHessian矩阵是梯度对$w^\\top$的导数：\n$$ \\nabla_w^2 f(w) = 2Z^\\top Z. $$\n问题陈述矩阵$Z$的列是线性无关的。根据定义，这意味着对于任何非零向量$v \\in \\mathbb{R}^m$，乘积$Zv$是$\\mathbb{R}^n$中的一个非零向量。因此，二次型$v^\\top (Z^\\top Z) v$可以写成$(Zv)^\\top(Zv) = \\|Zv\\|_2^2$，对于任何$v \\neq 0$都严格为正。这证明了矩阵$Z^\\top Z$是正定的。\n由于Hessian矩阵$2Z^\\top Z$对所有$w$都是正定的，目标函数$f(w)$是严格凸的。约束集$\\{w \\in \\mathbb{R}^m \\mid \\mathbf{1}^\\top w = 1\\}$是一个仿射子空间，它是一个凸集。在一个非空凸集上最小化一个严格凸函数，会得到唯一的最小值点。因此，所述问题有唯一解。此唯一性的条件是$Z$的列线性无关，这一点题目已给出。\n\n为了找到最优权重向量$w$，我们使用拉格朗日乘子法。拉格朗日函数$\\mathcal{L}(w, \\lambda)$是通过将约束乘以一个拉格朗日乘子$\\lambda$并加到目标函数上构造的：\n$$ \\mathcal{L}(w, \\lambda) = (Y - Zw)^\\top(Y - Zw) + \\lambda(\\mathbf{1}^\\top w - 1). $$\n最优解必须满足一阶Karush-Kuhn-Tucker (KKT) 条件，这可以通过将拉格朗日函数对$w$和$\\lambda$的偏导数设为零来找到。\n\n关于向量$w$的偏导数是：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial w} = 2Z^\\top Z w - 2Z^\\top Y + \\lambda\\mathbf{1}. $$\n令其为零，得到：\n$$ 2Z^\\top Z w - 2Z^\\top Y + \\lambda\\mathbf{1} = 0 \\implies Z^\\top Z w = Z^\\top Y - \\frac{\\lambda}{2}\\mathbf{1}. $$\n问题陈述$Z^\\top Z$是可逆的。因此，我们可以用$\\lambda$来表示$w$：\n$$ w = (Z^\\top Z)^{-1} \\left(Z^\\top Y - \\frac{\\lambda}{2}\\mathbf{1}\\right) = (Z^\\top Z)^{-1}Z^\\top Y - \\frac{\\lambda}{2}(Z^\\top Z)^{-1}\\mathbf{1}. \\quad (1) $$\n\n关于标量$\\lambda$的偏导数是：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = \\mathbf{1}^\\top w - 1. $$\n令其为零，再现了原始约束：\n$$ \\mathbf{1}^\\top w = 1. \\quad (2) $$\n\n现在，我们将方程$(1)$中$w$的表达式代入约束方程$(2)$来求解$\\lambda$：\n$$ \\mathbf{1}^\\top \\left( (Z^\\top Z)^{-1}Z^\\top Y - \\frac{\\lambda}{2}(Z^\\top Z)^{-1}\\mathbf{1} \\right) = 1. $$\n利用矩阵乘法的分配律：\n$$ \\mathbf{1}^\\top (Z^\\top Z)^{-1}Z^\\top Y - \\frac{\\lambda}{2} \\mathbf{1}^\\top (Z^\\top Z)^{-1}\\mathbf{1} = 1. $$\n我们现在可以分离出含$\\lambda$的项：\n$$ \\frac{\\lambda}{2} \\mathbf{1}^\\top (Z^\\top Z)^{-1}\\mathbf{1} = \\mathbf{1}^\\top (Z^\\top Z)^{-1}Z^\\top Y - 1. $$\n解出$\\frac{\\lambda}{2}$得：\n$$ \\frac{\\lambda}{2} = \\frac{\\mathbf{1}^\\top (Z^\\top Z)^{-1}Z^\\top Y - 1}{\\mathbf{1}^\\top (Z^\\top Z)^{-1}\\mathbf{1}}. $$\n注意，分母$\\mathbf{1}^\\top (Z^\\top Z)^{-1}\\mathbf{1}$是一个非零标量，因为$(Z^\\top Z)^{-1}$是正定的。\n\n最后，我们将这个$\\frac{\\lambda}{2}$的表达式代回方程$(1)$，得到最优权重向量$w$的闭式解：\n$$ w = (Z^\\top Z)^{-1}Z^\\top Y - \\left( \\frac{\\mathbf{1}^\\top (Z^\\top Z)^{-1}Z^\\top Y - 1}{\\mathbf{1}^\\top (Z^\\top Z)^{-1}\\mathbf{1}} \\right) (Z^\\top Z)^{-1}\\mathbf{1}. $$\n该表达式完全根据给定的量$Z$和$Y$以及单位向量$\\mathbf{1}$，通过要求的标准线性代数运算，给出了最优权重向量$w$。", "answer": "$$\n\\boxed{(Z^\\top Z)^{-1}Z^\\top Y - \\frac{\\mathbf{1}^\\top(Z^\\top Z)^{-1}Z^\\top Y - 1}{\\mathbf{1}^\\top(Z^\\top Z)^{-1}\\mathbf{1}} (Z^\\top Z)^{-1}\\mathbf{1}}\n$$", "id": "4953094"}]}