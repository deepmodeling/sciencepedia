## 引言
在医学研究与公共卫生决策中，区分“关联”与“因果”是至关重要的一步。虽然观察性研究为我们提供了海量数据，但如何从中可靠地回答“如果采用不同干预，结果会怎样？”这类因果问题，始终是一个核心挑战。由于缺乏随机化，混杂偏倚普遍存在，使得直接比较往往会得出错误结论。为了应对这一挑战，统计学与流行病学领域发展出了一套严谨的思维与分析工具——反事实（或称潜在结果）框架。

本文旨在系统性地介绍这一强大的因果推断框架。我们将通过三个章节的递进式学习，引领读者从理论基础走向前沿应用。在第一章**“原理与机制”**中，我们将深入剖析反事实框架的核心思想，包括潜在结果的定义、识别因果效应所需的三大基石假设，以及标准化（G-公式）和逆概率加权（IPW）这两种关键的混杂调整方法。随后，在第二章**“应用与跨学科连接”**中，我们将展示该框架如何应用于流行病学、临床研究、[精准医疗](@entry_id:152668)乃至机器学习等多个领域，以解决时变混杂、中介分析和非依从性等复杂现实问题。最后，第三章**“动手实践”**将通过一系列精心设计的练习，帮助读者巩固理论知识，并将其应用于解决实际分析中的难题。

通过学习本章，您将构建起一个用于思考和分析因果问题的坚实基础。让我们首先从该框架的基石——其核心原理与机制——开始。

## 原理与机制

本章深入探讨因果推断的反事实框架的核心原理与机制。在导论章节的基础上，我们将不再赘述背景，而是直接进入定义、假设和方法的严谨论述。本章的目标是构建一个系统性的知识体系，使读者能够理解如何利用观测数据来回答“如果……会怎样？”这类因果问题，并掌握实现这一目标所需的关键假设和计算工具。

### 核心思想：潜在结果

因果推断的核心在于比较。当我们问“某种疗法是否有效？”时，我们真正想比较的是同一个体在接受治疗与未接受治疗两种情况下的结果。这种“如果……会怎样”的情境，在反事实（Counterfactual）或[潜在结果](@entry_id:753644)（Potential Outcomes）框架下被形式化。

对于每个个体 $i$，我们定义一组**[潜在结果](@entry_id:753644)**（potential outcomes），每个[潜在结果](@entry_id:753644)对应于一个个体可能接受的每一种处理。假设一个二元处理 $A$（例如，用药 $A=1$ vs. 安慰剂 $A=0$），那么个体 $i$ 就有两个[潜在结果](@entry_id:753644)：
- $Y_i(1)$: 如果个体 $i$ 接受处理 $A=1$ 时的结果。
- $Y_i(0)$: 如果个体 $i$ 接受处理 $A=0$ 时的结果。

这两个结果都是“潜在的”，因为在任何给定的现实中，我们最多只能观测到其中之一。如果个体 $i$ 实际接受了治疗（$A_i=1$），我们观测到的是 $Y_i = Y_i(1)$；反之，如果他接受了安慰剂（$A_i=0$），我们观测到的是 $Y_i = Y_i(0)$。我们永远无法同时观测到同一个体在同一时间点的 $Y_i(1)$ 和 $Y_i(0)$。这就是**因果推断的根本问题**（Fundamental Problem of Causal Inference）。

由于**个体因果效应**（individual causal effect），即 $Y_i(1) - Y_i(0)$，是无法直接计算的，我们将目标转向在群体水平上定义和估计**平均因果效应**（Average Causal Effect, ACE），也常被称为平均[处理效应](@entry_id:636010)（Average Treatment Effect, ATE）。例如，风险差（risk difference）尺度上的 ATE 定义为：

$$
\Delta = \mathbb{E}[Y(1) - Y(0)] = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]
$$

这里的期望 $\mathbb{E}[\cdot]$ 是对目标人群中所有个体取平均。我们的任务，便是利用可观测的数据（即每个人的实际处理 $A_i$ 和实际结果 $Y_i$）来估计不可观测的群体平均潜在结果 $\mathbb{E}[Y(1)]$ 和 $\mathbb{E}[Y(0)]$。为了实现从观测数据到因果效应的跨越，我们需要一系列严格的假设。

### 因果识别的三大基石假设

为了从观测数据中识别（identify）因果效应，即为了建立观测数据分布与[潜在结果](@entry_id:753644)分布之间的桥梁，我们需要三个核心假设。

#### 稳定单位处理价值假设 (SUTVA)

SUTVA（Stable Unit Treatment Value Assumption）是连接我们观测到的世界和我们希望了解的[潜在结果](@entry_id:753644)世界的第一个关键环节。它包含两个子假设：

**1. 无干扰 (No Interference)**

此假设规定，一个个体的[潜在结果](@entry_id:753644)仅取决于其自身所受的处理，而与其他个体所受的处理无关。在一个包含 $N$ 个体的研究中，个体 $i$ 的[潜在结果](@entry_id:753644)最完整的形式可以写为 $Y_i(\mathbf{t})$，其中 $\mathbf{t} = (t_1, t_2, \ldots, t_N)$ 是涵盖所有研究单位的处理分配向量。无干扰假设意味着这个复杂的函数可以被简化，即对于任意两个处理向量 $\mathbf{t}$ 和 $\mathbf{t}'$，只要 $t_i=t'_i$，那么就有 $Y_i(\mathbf{t}) = Y_i(\mathbf{t}')$。这通常被简记为：

$$
Y_i(\mathbf{t}) = Y_i(t_i)
$$

这个假设使得我们可以简洁地用 $Y_i(1)$ 和 $Y_i(0)$ 来表示每个个体的[潜在结果](@entry_id:753644)。然而，无干扰假设并非总是成立。在一个关于医院内[病原体传播](@entry_id:138852)的研究中，对一个病人使用预防性药物可能会降低其邻近病人的感染风险，这就构成了干扰 [@problem_id:4987070]。在这种情况下，要使无干扰假设成立，研究设计必须采取极端措施，例如将每位患者置于[负压](@entry_id:161198)隔离病房，并配备专属的医护人员和设备，以阻断所有可能的传播途径。而在许多其他情境下，如口服药物对非传染性疾病的影响，该假设通常被认为是合理的。

**2. 一致性与明确定义的干预 (Consistency and Well-Defined Interventions)**

一致性（Consistency）假设声明，如果一个个体实际接受的处理是 $A_i=a$，那么他/她观测到的结果 $Y_i$ 就等于其在该处理下的潜在结果 $Y_i(a)$。形式上：

$$
\text{若 } A_i=a, \text{ 则 } Y_i = Y_i(a)
$$

这个假设看似平凡，但它隐含了一个深刻的要求：处理 $a$ 必须被**明确定义**（well-defined），不存在可能影响结果的“隐藏版本”。如果“处理 $A=1$”的定义含糊不清，那么 $Y(1)$ 这个概念本身也是模糊的。

例如，在一项心血管药物研究中，如果我们将干预定义为“积极的降压管理，由临床医生自行决定”[@problem_id:4987060]，那么两个同样被标记为“接受治疗”的病人，可能接受了完全不同的药物、剂量和随访计划。他们的结果差异不能归因于一个统一的“积极管理”，因为该干预有无数个隐藏的版本。因此，一致性假设在这种情况下不成立。

相反，一个明确定义的干预会具体到每一个细节。例如，一个**动态治疗方案**（dynamic treatment regime）可以规定：“基线时开始服用赖诺普利20mg每日一次；若2周后血压超过140mmHg，则增至40mg；禁止添加其他降压药；通过电子监控依从性，若低于0.9则转为直接观察治疗……”[@problem_id:4987060]。对于这样精确的方案 $\bar{d}$，潜在结果 $Y^{\bar{d}}$ 的含义是清晰无[歧义](@entry_id:276744)的，一致性假设也因此变得可信。

#### 可交换性 (Exchangeability)

可交换性（Exchangeability），或称**可忽略性**（ignorability），是处理混杂偏倚（confounding bias）的核心。它要求处理组和[对照组](@entry_id:188599)在影响结果的各种基线特征上是可比的。

在**随机对照试验**（Randomized Controlled Trial, RCT）中，随机化过程保证了**无条件可交换性**（unconditional exchangeability）。这意味着，在足够大的样本中，处理组和[对照组](@entry_id:188599)在所有基线特征（无论是已测量的还是未测量的）的分布上都是相似的。因此，我们可以认为两组的平均潜在结果是相同的，即 $\mathbb{E}[Y(1) | A=1] = \mathbb{E}[Y(1) | A=0] = \mathbb{E}[Y(1)]$，同样地 $\mathbb{E}[Y(0) | A=1] = \mathbb{E}[Y(0) | A=0] = \mathbb{E}[Y(0)]$。这样，观测到的结果差异就可以直接归因于处理本身：

$$
\mathbb{E}[Y | A=1] - \mathbb{E}[Y | A=0] = \mathbb{E}[Y(1) | A=1] - \mathbb{E}[Y(0) | A=0] = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)] = \Delta
$$

在因果推断的另一种视角，即基于随机化的推断中，我们可以利用**[尖锐零假设](@entry_id:177768)**（sharp null hypothesis），即 $H_0: Y_i(1) = Y_i(0)$ 对所有个体 $i$ 成立。该假设意味着处理对任何人都无效。在此假设下，每个人的结果是固定的，与处理分配无关。我们可以通过计算所有可能的随机分配（例如，$\binom{10}{5}$ 种分配方式）下[检验统计量](@entry_id:167372)（如组间均值差）的分布，来确定观测到的差异是否属于极端事件，从而得到一个精确的 p 值 [@problem_id:4987063]。这纯粹依赖于研究设计中的随机化机制，是理解[可交换性](@entry_id:263314)力量的一个有力范例。

然而，在**观测研究**（observational studies）中，处理分配不是随机的，处理组和[对照组](@entry_id:188599)往往在很多方面存在系统性差异。例如，在评估社区获得性肺炎患者使用皮质[类固醇](@entry_id:146569)效果的研究中，病情更重（如严重缺氧）的患者可能更倾向于接受治疗。这种情况下，我们转而依赖一个更弱的假设：**条件可交换性**（conditional exchangeability），或称“给定协变量后无未测量混杂”（no unmeasured confounding given covariates）。该假设表述为：

$$
(Y(1), Y(0)) \perp A \mid L
$$

其中 $L$ 是所有共同影响处理选择和结果的**混杂因素**（confounders）集合。这个假设的含义是，在混杂因素 $L$ 的任何一个特定层（stratum）内，处理分配是“仿佛随机的”。也就是说，在具有相同混杂因[素特征](@entry_id:155979)的个体亚群中，接受治疗者的潜在结果分布与未接受治疗者是相同的。

#### [正定性](@entry_id:149643) (Positivity)

[正定性](@entry_id:149643)（Positivity），也称为**重叠**（overlap）或共同支撑（common support），是另一个至关重要的假设。它要求在混杂因素 $L$ 的每个层（即对于所有使 $\Pr(L=l) > 0$ 的 $l$），每个处理水平的分配概率都必须严格介于0和1之间。形式上，对于二元处理：

$$
0  \Pr(A=1 \mid L=l)  1
$$

这个假设确保了在混杂因素的任何一个亚组内，我们都能找到接受治疗和未接受治疗的个体进行比较。如果某个亚组（例如，某个特定年龄和病症严重程度的患者）中的所有人总是接受治疗（或总是不接受治疗），那么我们就无法在该亚组内估计治疗的效果，因为缺乏反事实的参照。

[正定性](@entry_id:149643)违规的一个明显信号是处理组和[对照组](@entry_id:188599)之间协变量分布的**缺乏重叠**。例如，如果处理组的患者年龄普遍较大，而[对照组](@entry_id:188599)患者普遍较年轻，那么在极端年龄段可能存在正定性问题。我们可以通过一些诊断指标来评估重叠程度，例如**重叠系数**（Overlap Coefficient, OVL），它量化了两个分布密度函数的重叠面积 [@problem_id:4987066]。缺乏重叠会导致因果效应估计变得不稳定和不可靠。

### 混杂调整的机制

当上述三大假设（SUTVA、条件可交换性、正定性）均成立时，因果效应就是“可识别的”。接下来，我们介绍两种主流的混杂调整方法，它们利用这些假设来估计因果效应。

#### 标准化 (Standardization) 与 G-公式

标准化的核心思想是：计算出在混杂因素分布与目标人群完全相同的标准化人群中，处理组和[对照组](@entry_id:188599)的平均结果会是多少。这一过程也称为**G-计算**（G-computation），其结果被称为**G-公式**（G-formula）。

我们从第一性原理出发，推导单时间点处理的G-公式。我们的目标是识别 $\mathbb{E}[Y(a)]$。
1.  首先，根据[全期望定律](@entry_id:265946)，我们将对协变量 $L$ 的分布进行积分（或求和）：
    $$
    \mathbb{E}[Y(a)] = \mathbb{E}_L[\mathbb{E}[Y(a) \mid L]] = \sum_l \mathbb{E}[Y(a) \mid L=l] \Pr(L=l)
    $$
2.  然后，应用**条件可交换性** $(Y(a) \perp A \mid L)$，我们可以在给定 $L=l$ 的条件下，加入对处理 $A=a$ 的条件，而[期望值](@entry_id:150961)不变：
    $$
    \mathbb{E}[Y(a) \mid L=l] = \mathbb{E}[Y(a) \mid L=l, A=a]
    $$
3.  最后，应用**一致性**假设，当 $A=a$ 时, $Y=Y(a)$。因此，我们可以用观测结果 $Y$ 替换潜在结果 $Y(a)$：
    $$
    \mathbb{E}[Y(a) \mid L=l, A=a] = \mathbb{E}[Y \mid L=l, A=a]
    $$

将这三步结合，我们得到了G-公式的识别结果 [@problem_id:4987081]：

$$
\mathbb{E}[Y(a)] = \sum_l \mathbb{E}[Y \mid A=a, L=l] \Pr(L=l)
$$

这个公式的直观解释是：我们首先在观测数据中，为每个混杂因素层 $l$ 和每个处理组 $a$ 计算一个特定的平均结果 $\mathbb{E}[Y \mid A=a, L=l]$。然后，我们用目标人群中 $L$ 的总体分布 $\Pr(L=l)$ 对这些特定的平均结果进行加权平均，从而得到一个标准化的、无混杂的平均[潜在结果](@entry_id:753644) $\mathbb{E}[Y(a)]$。

这个原理可以轻松扩展到连续协变量和更复杂的模型。例如，当结果模型包含处理与协变量的**交互项**（即效应修饰）时，我们可以先估计一个条件均值模型 $\mu_a(x,z) = \mathbb{E}[Y | A=a, X=x, Z=z]$，然后通过对目标人群的协变量联合分布进行积分来计算平均潜在结果 $\mathbb{E}[Y(a)] = \mathbb{E}_{(X,Z)}[\mu_a(X,Z)]$ [@problem_id:4987076]。

G-公式最强大的应用是在**纵向研究**中处理**时变混杂**（time-varying confounding）。当一个变量（如中间生物标志物 $L_1$）既是未来处理（$A_1$）的混杂因素，又受到过去处理（$A_0$）的影响时，传统的单时间点调整方法会失效。G-公式通过一个迭代的过程解决了这个问题：它按照时间顺序，依次设定处理，并对每个时间点的协变量分布进行积分，从而“打破”了处理与混杂之间的反馈循环 [@problem_id:4987061]。对于一个两阶段处理方案 $\bar{a}=(a_0, a_1)$，其完整的G-公式为：

$$
\mathbb{E}[Y^{\bar{a}}] = \int_{l_0} \int_{l_1} \mathbb{E}[Y \mid \bar{A}=\bar{a}, L_0=l_0, L_1=l_1] f(l_1 \mid A_0=a_0, L_0=l_0) f(l_0) dl_1 dl_0
$$

#### [逆概率](@entry_id:196307)加权 (Inverse Probability Weighting)

[逆概率](@entry_id:196307)加权（IPW）是另一种强大的混杂调整方法。其核心思想并非标准化，而是通过**加权**来创建一个“伪人群”（pseudo-population）。在这个伪人群中，协变量的分布在处理组和[对照组](@entry_id:188599)之间变得相同，从而消除了混杂。

权重是根据每个个体接受其所受处理的概率来计算的。这个概率被称为**[倾向得分](@entry_id:635864)**（propensity score），$e(X) = \Pr(A=1 \mid X)$。对于接受治疗的个体（$A_i=1$），其权重是 $1/e(X_i)$；对于未接受治疗的个体（$A_i=0$），其权重是 $1/(1-e(X_i))$。直观上，一个本不该接受治疗（[倾向得分](@entry_id:635864)很低）却接受了治疗的个体，在人群中是“罕见”的，因此他/她代表了许多类似但未被治疗的个体，应该被赋予更高的权重。

利用这些权重，我们可以通过加权平均来估计平均[潜在结果](@entry_id:753644)：

$$
\widehat{\mathbb{E}[Y(1)]}_{\text{IPW}} = \frac{1}{n} \sum_{i=1}^{n} \frac{A_i Y_i}{e(X_i)}
$$

$$
\widehat{\mathbb{E}[Y(0)]}_{\text{IPW}} = \frac{1}{n} \sum_{i=1}^{n} \frac{(1-A_i) Y_i}{1-e(X_i)}
$$

通过对一个包含糖尿病史和年龄评分的小型队列数据进行手动计算，我们可以清晰地看到IPW如何运作：首先为每位患者计算[倾向得分](@entry_id:635864)，然后为有事件发生的患者（$Y_i=1$）计算其对相应潜在结果均值的贡献，最后汇总并求得ATE [@problem_id:4987077]。

### 进阶议题：[交互作用](@entry_id:164533)与敏感性分析

#### 效应修饰与[交互作用](@entry_id:164533)

平均因果效应描述的是处理在整个群体中的平均效果，但这可能会掩盖处理效果在不同亚群中的异质性。当一个处理的因果效应随着另一个变量（如基线生物标志物或另一个处理）的水平变化而变化时，我们称之为**效应修饰**（effect modification）。当两个处理共同作用时，这种现象被称为**[交互作用](@entry_id:164533)**（interaction）。

在一个2x2[析因设计](@entry_id:166667)的随机试验中，我们可以明确地定义和估计[交互作用](@entry_id:164533)。例如，在**相加尺度**（additive scale）上，我们可以定义一个交互对比项，它等于“处理A1在A2=1人群中的效应”减去“处理A1在A2=0人群中的效应”[@problem_id:4987072]。其形式化的潜在结果表达为：

$$
\Delta_{\text{int}} = \mathbb{E}[Y(1,1)-Y(1,0)] - \mathbb{E}[Y(0,1)-Y(0,0)] = \mathbb{E}[Y(1,1)-Y(1,0)-Y(0,1)+Y(0,0)]
$$

如果这个交互项不为零，就说明两种处理之间存在相加[交互作用](@entry_id:164533)，即它们的联合效应不等于各自效应的简单加和。

#### 未测量混杂与[敏感性分析](@entry_id:147555)

观测研究的一个根本局限是，“无未测量混杂”的假设是无法被数据验证的。我们永远无法确定是否已经测量并调整了所有相关的混杂因素。因此，一个严谨的观测研究需要进行**[敏感性分析](@entry_id:147555)**（sensitivity analysis），以评估研究结论对潜在的未测量混杂的稳健性。

敏感性分析旨在回答这样一个问题：“一个未测量的混杂因素U需要多强，才能‘解释掉’我们观测到的关联？” 我们可以通过设定这个未测量混杂U与处理A的关联强度（$PR_{UA}$）以及它与结果Y的关联强度（$RR_{YU}$）的合理上限，来计算出混杂偏倚因子 $B$ 的最大可[能值](@entry_id:187992) [@problem_id:4987062]。观测到的效应（$RR_{obs}$）可以被分解为真实因果效应（$RR_{true}$）与偏倚因子（$B$）的乘积：$RR_{obs} = RR_{true} \times B$。

通过计算 $B$ 的[上界](@entry_id:274738)，我们就能得到真实因果效应 $RR_{true}$ 的下界：$RR_{true, min} = RR_{obs} / B_{max}$。如果即使在相当强的未测量混杂假设下，这个效应下界仍然在临床上有意义（例如，远大于1），那么我们就对研究结论更有信心。这个过程为我们提供了一种定量的方式来思考和讨论观测研究结果的不确定性。