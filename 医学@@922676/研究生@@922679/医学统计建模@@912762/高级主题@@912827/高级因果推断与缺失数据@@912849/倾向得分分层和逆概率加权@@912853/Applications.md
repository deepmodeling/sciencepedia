## 应用与跨学科联系

在前面的章节中，我们已经详细阐述了倾向性评分（Propensity Score, PS）分层与逆概率加权（Inverse Probability Weighting, IPW）的基本原理和核心机制。这些方法为从观测数据中估计因果效应提供了严谨的统计框架。本章的目标是从“如何做”转向“为何做”以及“在何处做”。我们将探讨这些核心原理在多样化、真实世界和跨学科背景下的实际应用，展示它们作为从复杂数据中提取因果推断的强大工具的效用。

我们的旅程将从这些方法在临床医学和药物流行病学中的核心应用开始，这是倾向性评分方法最成熟且影响最深远的领域。随后，我们将探讨如何将这些方法扩展到更复杂的[数据结构](@entry_id:262134)，例如生存数据、纵向数据，以及处理多类别或连续性处理的情况。最后，我们将讨论一些现代挑战，包括如何将倾向性评分方法与复杂调查数据相结合，以及如何应对生物信息学和医学大数据时代的高维变量选择问题。

### 核心应用：临床流行病学与药物流行病学

在医学研究中，尤其是利用电子健康记录（Electronic Health Records, EHR）或大型管理数据库进行的观测研究中，一个核心挑战是**适应症混杂（Confounding by Indication）**。这种偏倚的产生是因为在临床实践中，患者的疾病严重程度或预后风险既是决定其接受何种治疗（即“适应症”）的关键因素，也是影响其临床结局的决定性因素。

例如，在评估早期使用血管加压素对脓毒症重症监护（ICU）患者死亡率的影响时，病情更危重的患者（以未测量的疾病严重度评分 $S$ 为代表）更有可能接受早期血管加压素治疗（$A=1$），同时他们自身的死亡风险也更高（影响潜在结局 $Y(0)$ 和 $Y(1)$）。同样，在评估[他汀类药物](@entry_id:167025)对高脂血症患者心血管事件的影响时，基线心血管风险更高的患者（如有更高的[低密度脂蛋白胆固醇](@entry_id:172654)水平）更有可能被处方他汀类药物，而他们本身发生不良心血管事件的风险也更高。在比较剖宫产后阴道分娩试验（TOLAC）与择期重复剖宫产（ERCS）对子宫破裂风险的影响时，具有更有利分娩条件（如较高的Bishop评分）的孕妇更可能被建议并选择TOLAC，而这些有利条件本身也与较低的子宫破裂风险相关。

在所有这些情景中，治疗组和[对照组](@entry_id:188599)在基线时的可比性被打破，因为决定治疗选择的预后因素成为了混杂因素。这种混杂结构导致了潜在结局与治疗分配在基线时并非独立，即 $ (Y(0), Y(1)) \not\perp A $。直接比较两组的结局将系统性地高估或低估治疗的真实效果。例如，在脓毒症研究中，由于接受血管加压素的患者本身就更危重，直接比较可能会错误地得出血管加压素增加死亡率的结论。

为了解决这一问题，研究者们越来越多地采用**目标试验模拟（Target Trial Emulation）**的框架。其核心思想是，尽管我们无法进行真正的随机对照试验（RCT），但我们可以利用观测数据来模拟一个理想化的目标试验。倾向性评分方法正是实现这一模拟的关键技术。通过对所有已测量的、影响治疗决策和结局的基线混杂因素 $X$ 进行建模，我们可以估计出每个个体在给定其协变量条件下接受治疗的概率，即倾向性评分 $e(X) = \Pr(A=1 \mid X)$。随后，通过匹配、分层或加权，我们可以在一个“伪人群”（pseudo-population）中平衡这些已测量的混杂因素，使得在这个伪人群中，治疗分配近似独立于这些协变量，从而在一定程度上恢复了随机化试验的可比性。这个过程旨在实现**条件可交换性（Conditional Exchangeability）**，即 $ (Y(0), Y(1)) \perp A \mid X $，这是所有倾向性评分方法有效性的基石。[@problem_id:5221159] [@problem_id:4612511] [@problem_id:4517763]

一个严谨的、基于倾向性评分的临床研究工作流通常包含以下关键步骤 [@problem_id:4980961]：
1.  **明确定义因果问题**：精确定义目标试验的各个要素，包括合格标准、时间零点、治疗策略、结局以及因果估计量（例如，平均处理效应ATE）。
2.  **构建倾向性评分模型**：使用灵活的统计或[机器学习模型](@entry_id:262335)（如正则化逻辑回归、[梯度提升](@entry_id:636838)机）来估计倾向性评分。至关重要的是，该模型只能包含**治疗前**测量的基线协变量，绝不能包含任何治疗后或与结局相关的变量，以避免引入偏倚。
3.  **评估模型性能与诊断**：这是确保分析有效性的核心环节。它不仅仅是检查模型的预测能力，更重要的是评估其平衡混杂因素的能力。
4.  **估计治疗效应**：在通过诊断确认协变量已充分平衡后，使用加权或匹配后的样本来估计治疗效应。
5.  **进行稳健的[统计推断](@entry_id:172747)**：使用能够恰当处理[数据结构](@entry_id:262134)和估计过程不确定性的方法（如稳健三明治[方差估计](@entry_id:268607)或[自助法](@entry_id:139281)）来计算[置信区间](@entry_id:138194)和p值。

#### 诊断的关键作用：确保可比性

在应用倾向性评分方法时，最关键的一步是进行全面的诊断，以验证我们是否成功地创建了一个可比较的伪人群。

**评估协变量平衡**是诊断的核心。我们的目标是检查在加权或匹配后，治疗组和[对照组](@entry_id:188599)中所有基线协变量的分布是否相似。评估平衡的首选指标是**标准化均数差（Standardized Mean Difference, SMD）**。对于一个连续性协变量，其加权SMD的计算方式是将原始定义中的样本均值和方差替换为它们的加权对应项。具体而言，我们会分别计算治疗组和[对照组](@entry_id:188599)的加权均值和加权方差，然后用加权均值之差除以一个基于组内加权方差的[合并标准差](@entry_id:198759)。这个度量是无量纲的，并且对样本量不敏感，因此优于依赖[p值](@entry_id:136498)的假设检验。在医学研究中，普遍接受的[经验法则](@entry_id:262201)是，所有协变量的绝对SMD值都应小于 $0.1$，这表明混杂因素得到了充分的平衡。对于多分类的名义变量，平衡性检查应确保所有类别的分布都得到平衡，而不仅仅是单一的对比。除了检查均值（一阶矩），全面的诊断还应包括检查方差（二阶矩）以及分布的整体形状（例如，通过加权的[分位数-分位数图](@entry_id:174944)）。[@problem_id:4980925] [@problem_id:4980939]

**评估权重分布与重叠性**同样至关重要，尤其是在使用IPTW时。[逆概率](@entry_id:196307)权重的大小直接反映了观测数据与一个随机试验的“距离”。当存在倾向性评分接近0或1的个体时（即**重叠性（Overlap）**或**正性（Positivity）**假设受到挑战），会导致产生极端大的权重。这些极端权重会使效应估计变得非常不稳定，并显著增大其方差。

为了量化这种由于权重不均一而导致的统计精度损失，我们可以计算**[有效样本量](@entry_id:271661)（Effective Sample Size, ESS）**。其公式为 $n^{\star} = \frac{(\sum_i w_i)^2}{\sum_i w_i^2}$。这个公式揭示了一个深刻的道理：一个大小为 $N$ 的加权样本，其在估计均值时的统计精度，仅相当于一个大小为 $n^{\star}$ 的未加权简单随机样本。当所有权重相等时（即在未加权分析或完美平衡的随机试验中），$n^{\star} = N$。然而，当权重分布不均、存在极端值时，权重的变异系数增大，导致 $n^{\star}$ 远小于 $N$。例如，一个拥有12名受试者的研究，由于权重的高度不均，其有效样本量可能降至不足8，这相当于在统计精度上损失了超过三分之一的受试者。因此，检查权重分布（如均值、方差、最大/最小值）并计算ESS，是评估IPTW估计稳定性的关键步骤。在实践中，研究者常会对极端权重进行截断（truncation），以牺牲微小的偏倚为代价来换取方差的大幅降低。[@problem_id:4980912]

### 高级建模与方法扩展

倾向性评分方法不仅限于简单的二元处理比较，其理论框架可以灵活地扩展，以适应更复杂的模型和[数据结构](@entry_id:262134)。

#### 与[统计模型](@entry_id:755400)的结合：边际结构模型

倾向性评分加权最强大的应用之一是作为估计**边际结构模型（Marginal Structural Models, MSMs）**参数的工具。MSMs直接对潜在结局的边际（即人群平均）分布与治疗之间的关系进行建模，而不受混杂因素的干扰。

例如，如果我们想估计治疗对一个二元结局的边际因果比值比（odds ratio），我们可以设定一个边际[逻辑斯谛模型](@entry_id:268065)：$\operatorname{logit}\{\Pr(Y(a)=1)\} = \beta_{0} + \beta_{1} a$。这里的 $\beta_{1}$ 就是我们感兴趣的边际因果对数比值比。在存在混杂的情况下，直接用观测数据拟合 $Y$ 对 $A$ 的[逻辑斯谛回归](@entry_id:136386)会得到一个有偏的估计。然而，通过IPTW，我们创建了一个伪人群，其中混杂因素与治疗无关。在这个加权后的伪人群中，我们可以直接拟合一个简单的加权[逻辑斯谛回归模型](@entry_id:637047)，其形式为 $Y$ 对 $A$ 回归，**且模型中不再包含其他协变量**。从这个加权模型中得到的系数 $\hat{\beta}_1$ 就是 $\beta_1$ 的一个相合估计。需要注意的是，由于权重本身是估计出来的，我们需要使用稳健（“三明治”）[方差估计](@entry_id:268607)量来获得有效的[置信区间](@entry_id:138194)。这种方法巧妙地将一个复杂的多变量混杂问题，转化为一个简单的加权单变量回归问题，并且正确地估计了[边际效应](@entry_id:634982)，避免了像标准多变量[逻辑斯谛回归](@entry_id:136386)那样因比值比的不可坍缩性（non-collapsibility）而只能估计条件效应的问题。[@problem_id:4980890]

#### 处理复杂数据结构

**生存数据与信息性删失**：在许多医学研究中，结局是事件发生时间，如死亡或疾病复发。分析这类数据时，除了处理基线混杂，还必须处理**信息性删失（Informative Censoring）**。当删失（即随访中断）的发生与个体的预后风险相关时，信息性删失就会出现，即使在调整了基线混杂之后也是如此。例如，病情恶化的患者可能更倾向于退出研究，这就会导致对治疗效果的偏倚估计。

为了同时解决基线混杂和信息性删失，我们可以将IPW的思想进行扩展，构建一个包含两个部分的乘积权重。第一部分是标准的**逆治疗概率权重（IPTW）**，用于平衡基线混杂。第二部分是**逆删失概率权重（Inverse Probability of Censoring Weighting, IPCW）**，用于平衡由删失带来的选择偏倚。在每个时间点，我们估计个体在该时间点之前保持未删失的概率（条件于其基线协变量和治疗史），并用其倒数作为删失权重。最终的组合权重 $w_i(t) = w_{i}^{\text{IPTW}} \times w_{i}^{\text{IPCW}}(t)$ 是随时间变化的。然后，我们可以将这些随时间变化的组合权重应用于标准的生存分析方法，如拟合一个加权的[Cox比例风险模型](@entry_id:174252)。在这个加权模型中，我们同样只需包含治疗变量 $A$，其系数的指数化 $\exp(\hat\beta)$ 就是对边际风险比（marginal hazard ratio）的相合估计。[@problem_id:4980948] [@problem_id:4980921]

**纵向数据与时依性混杂**：倾向性评分方法最引人注目的扩展之一是其在处理**时依性混杂（Time-dependent Confounding）**时的应用。在纵向研究中，一个随时间变化的协变量（如某个实验室指标 $L_t$）可能既是未来治疗决策（$A_t$）的混杂因素，又是过去治疗（$A_{t-1}$）所影响的一个中间变量。例如，在慢性病治疗中，医生根据患者当前的肝功能指标（$L_t$）来决定是否继续用药（$A_t$），而这个肝功能指标本身可能受到了上一周期用药（$A_{t-1}$）的影响。

这种 $A_{t-1} \rightarrow L_t \rightarrow Y$ （$L_t$ 作为中介）和 $L_t \rightarrow A_t$（$L_t$ 作为混杂）并存的结构，对标准回归调整方法构成了致命挑战。如果在模型中为了控制 $A_t$ 的混杂而调整 $L_t$，我们会无意中阻断了过去治疗 $A_{t-1}$ 通过 $L_t$ 介导的因果路径，导致对整个治疗策略总效应的估计产生偏倚。

纵向IPTW（也称为边际结构模型的G-方法）通过在每个时间点上序贯地进行加权，优雅地解决了这个问题。其稳定化权重 $w_i = \prod_{t=1}^T \frac{P(A_{it} \mid \bar{A}_{i,t-1})}{P(A_{it} \mid \bar{A}_{i,t-1}, \bar{X}_{it})}$ 的分母是给定过去所有协变量和治疗史的条件下，个体在时间点 $t$ 接受其实际所受治疗的概率。通过对其取倒数，我们打破了在每个时间点上，时依性混杂因素与当前治疗之间的关联。分子的作用是稳定权重，减小方差。通过这种方式，我们创建了一个伪人群，其中在任何时间点，治疗决策都独立于时依性混杂因素的历史，从而允许我们无偏地估计动态治疗策略的边际因果效应。[@problem_id:4980960] [@problem_id:4980947]

#### 推广治疗类型

倾向性评分的框架同样适用于非二元的治疗情况。

**多分类治疗**：当研究比较两种以上治疗方案时（例如，三种不同的降压药），我们可以使用**广义倾向性评分（Generalized Propensity Score, GPS）**。对于每个治疗类别 $a$，GPS被定义为给定协变量 $X$ 的条件下，个体接受治疗 $a$ 的概率：$p_a(X) = P(A=a \mid X)$。这通常可以通过多项[逻辑斯谛回归](@entry_id:136386)来估计。IPW估计量也相应地被推广。例如，为了估计治疗 $a$ 下的平均潜在结局 $E[Y^a]$，我们可以使用Horvitz-Thompson估计量：$\hat{\mu}_a = \frac{1}{n} \sum_{i=1}^n \frac{I(A_i=a)Y_i}{\hat{p}_a(X_i)}$，其中 $I(A_i=a)$ 是一个[指示函数](@entry_id:186820)。[@problem_id:4980944]

**连续性治疗**：当治疗是一个连续变量时（如药物剂量），GPS被定义为给定协变量 $X$ 的条件下，治疗变量 $A$ 的[条件概率密度函数](@entry_id:190422)：$r(a,X) = f_{A \mid X}(a \mid X)$。此时，IPW的权重被定义为边际治疗密度与条件治疗密度的比值：$w(A,X) = \frac{f_A(A)}{f_{A \mid X}(A \mid X)}$。这个权重的作用同样是创建一个伪人群，其中治疗水平（剂量）与基线协变量相互独立，从而允许我们估计剂量-反应曲线的因果关系。[@problem_id:4980922]

### 跨学科与现代挑战

倾向性评分方法的应用远不止于临床流行病学，它们在社会科学、经济学以及其他依赖观测数据的领域中也发挥着重要作用。同时，随着数据科学的发展，这些经典方法也面临着新的挑战和机遇。

#### 与复杂调查数据的整合

在公共卫生和政策评估中，数据常常来自于具有分层、整群和不等概率抽样设计的**复杂调查**（例如，国家健康与营养调查，NHANES）。分析这类数据时，我们不仅要处理混杂问题，还必须考虑抽样设计，以确保我们的结果能够代表目标人群。

这时，我们需要将用于处理混杂的**逆治疗概率权重（$w^{IPTW}$）**与用于反映抽样设计的**调查权重（$w^{survey}$）**结合起来。一个通用的方法是构建一个复合权重，即两者的乘积：$w_i = w_i^{survey} \cdot w_i^{IPTW}$。这个复合权重在概念上执行了两个任务：$w^{IPTW}$部分首先在样本内部创建一个平衡的、如同随机化的伪人群来解决混杂问题；然后$w^{survey}$部分将这个经过调整的样本结果推广到目标有限人群。

然而，一个更为严峻的挑战在于**方差估计**。由于复合权重和复杂的样本结构（如聚类导致观测值不独立），标准的方差计算公式会严重低估真实的不确定性，导致[置信区间](@entry_id:138194)过窄和错误的统计推断。正确的[方差估计](@entry_id:268607)必须同时考虑抽样设计和倾向性评分估计这两个变异来源。主要有两种严谨的方法：
1.  **重复权重法**：如自助法（Bootstrap）、[刀切法](@entry_id:174793)（Jackknife）或平衡重复复制法（BRR）。这种方法通过从原始样本中反复抽样创建多个“重复样本”，并在每个重复样本上完整地重新计算整个分析过程（包括倾向性评分估计、复合权重构建和效应估计），最后根据重复样本效应估计值的变化来计算方差。
2.  **[泰勒级数](@entry_id:147154)线性化法**：这是一种基于[渐近理论](@entry_id:162631)的解析方法。它将效应估计量和倾向性评分模型参数的估计方程“堆叠”在一起，然后使用泰勒展开来推导整个系统的方差-协方差矩阵。这个过程必须整合调查设计的结构信息（分层和聚类）。

这两种方法都是计算密集型的，但对于从复杂调查数据中获得有效的因果推断至关重要。[@problem_id:4980883]

#### 在高维数据时代的倾向性评分

随着生物信息学和基因组学的发展，研究者们常常面临**[高维数据](@entry_id:138874)**的挑战，即协变量的数量 $p$ 远大于样本量 $n$（$p \gg n$）。在这种情况下，为倾向性评分[模型选择](@entry_id:155601)正确的变量集本身就成为一个复杂的问题。

一个核心原则是，倾向性评分模型的构建应该是一个**“无结局”（outcome-free）**的过程。也就是说，在选择变量和构建模型时，我们应该只使用治疗变量 $T$ 和基线协变量 $X$，而完全不应查看结局变量 $Y$。这可以防止有意识或无意识地“数据挖掘”，即通过调整模型来得到期望的效应估计。

在高维设置下，一些策略被提出来用于[变量选择](@entry_id:177971)：
*   **基于机器学习的无结局选择**：使用如LASSO（[最小绝对收缩和选择算子](@entry_id:751223)）等正则化回归方法，对 $T$ 关于 $X$ 的模型进行拟合。LASSO能够在一个高维空间中自动选择一个稀疏的预测变量子集，这非常适合基因组学等领域，因为我们通常假设只有少数变量是真正的预测因子。在满足某些正则化条件下，这种方法可以有效地识别出治疗的强预测因子，从而构建一个稳健的倾向性评分模型。[@problem_id:4599481]
*   **基于单变量关联的筛选**：这是一种更简单的方法，即预先筛选出与治疗（或结局）具有最强单变量关联的协变量子集，然后基于这个子集构建模型。然而，这种方法存在严重缺陷。一个真正的混杂因素可能其边际关联很弱（例如，由于多重共线性或抑制效应），但在多变量模型中却至关重要。单变量筛选很可能会漏掉这样的混杂因素，导致**残余混杂（residual confounding）**和有偏的效应估计。特别是，基于与结局关联的筛选方法，从根本上混淆了预后模型和倾向性评分模型的目的，是不可取的。[@problem_id:4599481]

此外，关于在倾向性评分模型中应包含哪些变量，也存在一些微妙的权衡。模型必须包含所有**混杂因素**（与治疗和结局均相关的变量）才能确保无偏性。包含**纯预后变量**（仅与结局相关）不会减少偏倚，但可能增加倾向性评分估计的方差。而包含**纯工具变量**（仅与治疗相关）虽然不影响偏倚，但会极大地增加IPW[估计量的方差](@entry_id:167223)（因为它们会使倾向性评分更趋近于0或1），因此通常应予以避免。[@problem_id:4599481]

### 结论

本章通过一系列应用案例，展示了倾向性评分分层与[逆概率](@entry_id:196307)加权方法在解决现实世界因果推断问题时的广度和深度。从药物流行病学中应对适应症混杂的核心应用，到处理[生存数据](@entry_id:165675)、纵向数据以及不同类型治疗的复杂扩展，再到与调查统计学和高维数据分析等领域的交叉融合，这些方法共同构成了一个强大而灵活的工具箱。

然而，我们必须时刻铭记，这些方法的有效性根植于一些强大的、通常无法被完全检验的假设之上，其中最核心的就是**无混杂假设（unconfoundedness）**。倾向性评分并非解决所有问题的万能药；它是一种使混杂问题变得透明、可量化并可在一定假设下进行调整的工具。任何基于这些方法的严谨研究，都必须伴随着细致的诊断、对方法局限性的清醒认识以及对结果的审慎解释。