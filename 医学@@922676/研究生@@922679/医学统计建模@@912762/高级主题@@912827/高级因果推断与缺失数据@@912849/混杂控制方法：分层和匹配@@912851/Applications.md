## 应用与跨学科联系

### 引言

在前面的章节中，我们已经详细阐述了分层与匹配作为控制混杂偏倚的核心原理与机制。这些方法通过在协变量的特定层面内或通过构建具有相似背景特征的组来创造“局部”可比性，从而模拟随机对照试验的关键特性。本章的目标是[超越理论](@entry_id:203777)，展示这些基本原理如何在多样化的真实世界和跨学科背景下被应用、扩展和整合。

我们将不再重复介绍核心概念，而是通过一系列以应用为导向的科学问题，探索分层与匹配在临床流行病学、基因组学、卫生服务研究等领域的实际效用。本章将揭示，这些方法不仅是数据分析的工具，更是一种贯穿研究设计、过程诊断、结果解释以及处理复杂数据结构的综合性思维框架。通过学习这些应用，读者将能更深刻地理解如何将理论转化为严谨、可信的科学证据。

### 临床流行病学的核心应用

分层与匹[配方法](@entry_id:265480)在临床流行病学研究中扮演着基石角色，用于从观察性数据中获得有效的因果推断。以下小节将探讨其在估计治疗效应、进行[假设检验](@entry_id:142556)和分析特定[数据结构](@entry_id:262134)中的核心应用。

#### 估计标准化因果效应

在[观察性研究](@entry_id:174507)中，一个常见的目标是估计某项干预（如一种新药或一项公共卫生计划）在特定目标人群中的平均治疗效应（Average Treatment Effect, ATE）。然而，研究样本的协变量分布可能与目标人群不同。分层分析为此提供了一个强大的解决方案：直接标准化。

该方法首先将研究人群根据一个或多个关键混杂因素（如年龄、疾病严重程度）划分为若干个同质的亚组或“层”。在每个层内，由于混杂因素的水平是恒定的，我们可以计算出一个相对无偏的层内治疗效应估计值，例如风险差（Risk Difference, RD）或风险比（Risk Ratio, RR）。接着，为了得到目标人群的总体ATE，我们将这些层内效应值按照目标人群中相应分层的比例进行加权平均。这个过程本质上是回答这样一个问题：“如果目标人群接受了这项干预，其平均效应会是什么？”

例如，在一项评估预防性抗凝剂对血栓栓塞事件影响的医院队列研究中，患者可能根据基线风险评分被分层。研究人员可以计算出每个风险分层中的治疗组与[对照组](@entry_id:188599)之间的风险差。如果研究者希望将结果推广到整个医院的目标人群，他们就需要知道该目标人群中不同风险分层的患者分布情况。最终的平均治疗效应（ATE）估计值，即为各分层的风险差以其在目标人群中的相应占比为权重所计算的加权平均值。这个过程在形式上可以通过[全期望定律](@entry_id:265946)（Law of Total Expectation）和因果推断的潜在结局框架严格导出，它依赖于层内条件[可交换性](@entry_id:263314)（conditional exchangeability）的关键假设，即在每个分层内，治疗分配与潜在结局是独立的。[@problem_id:4973428]

#### 分层数据的[假设检验](@entry_id:142556)

当研究数据来自多个中心或被自然地划分为多个亚组时（例如，不同的医院、不同的地理区域或不同的研究批次），研究者需要一种方法来整合来自所有分层的信息，以对处理与结局之间的总体关联性进行假设检验。Mantel-Haenszel (MH)检验就是为此目的而设计的经典统计方法，尤其适用于分析多个$2 \times 2$[列联表](@entry_id:162738)。

MH检验的基本思想是，在“处理与结局在各层内均无关联”的原假设下，比较每个分层中观测到的事件数与期望的事件数。具体来说，对于每个分层的$2 \times 2$表，在固定行和与列和（即边际总数）的条件下，原假设意味着表中的任意单元格计数（例如，暴露组中的病例数$a_k$）服从[超几何分布](@entry_id:193745)。MH[检验统计量](@entry_id:167372)计算每个分层中观测值$a_k$与原假设下其[期望值](@entry_id:150961)$E_k$的偏差$(a_k - E_k)$，然后将所有分层的偏差加总。为了构建一个标准的[检验统计量](@entry_id:167372)，这个总偏差将被其方差标准化。由于各分层被假定为独立的，总偏差的方差等于各层内方差之和。最终得到的MH统计量近似服从自由度为1的[卡方分布](@entry_id:165213)。

这种方法的优势在于，它提供了一个单一的、综合性的关联性检验，即使在各分层中效应的方向一致但大小不同时，或者在单个分层样本量过小而无法提供足够证据时，它仍然具有良好的统计功效。它通过在分层水平上进行比较，有效地控制了由分层变量所定义的混杂因素。[@problem_id:4973451]

#### 分析匹配配对数据

匹配，尤其是1:1配对，是一种强大的设计策略，旨在为每个接受治疗的个体找到一个或多个具有非常相似基线特征的未治疗个体。然而，这种设计也创造了一种特殊的数据结构：配对数据。配对中的个体不再是相互独立的，因为他们是基于共享的协变量特征被选择在一起的。因此，他们的结局也可能存在相关性。

分析[匹配数](@entry_id:274175)据时必须考虑到这种相关性。例如，在一项评估新降压疗法对连续性结局（如收缩压）影响的1:1匹配研究中，我们可以为每对$i$计算一个配对差值$D_i = Y_{Ti} - Y_{Ci}$，其中$Y_{Ti}$和$Y_{Ci}$分别是该配对中治疗者和对照者的结局。平均治疗效应$\Delta$的[无偏估计量](@entry_id:756290)就是这些差值的样本均值，即$\hat{\Delta} = \frac{1}{m} \sum_{i=1}^{m} D_i$。

关键在于计算这个估计量的[标准误](@entry_id:635378)（Standard Error）。如果我们错误地将治疗组和[对照组](@entry_id:188599)视为两个独立的样本，我们会忽略由于匹配而产生的正相关性（$\rho > 0$）。在配对分析中，差值$D_i$的方差是$\operatorname{Var}(D_i) = \operatorname{Var}(Y_{Ti}) + \operatorname{Var}(Y_{Ci}) - 2\operatorname{Cov}(Y_{Ti}, Y_{Ci})$。由于匹配使得协变量相似的个体配对，他们的结局往往会朝同一方向变化（即$\operatorname{Cov}(Y_{Ti}, Y_{Ci}) > 0$），这会使得$\operatorname{Var}(D_i)$小于[独立样本](@entry_id:177139)假设下的方差之和。因此，正确的配对分析通过利用这种相关性，通常会得到一个比不当的独立样本分析更小的标准误，从而获得更高的统计检验效力。[@problem_id:4973438]

### 混杂控制的实践：诊断与优化

应用分层与匹[配方法](@entry_id:265480)不仅仅是执行一个算法，它是一个涉及仔细规划、诊断和迭代改进的严谨过程。本节将探讨这一过程中的关键实践步骤。

#### 协[变量选择](@entry_id:177971)与平衡性评估

成功控制混杂的第一步是审慎地选择需要控制的协变量。在倾向性评分（Propensity Score, PS）匹配或分层中，协变量集合$X$的选择至关重要。基本原则是，集合$X$必须包含所有被认为是治疗分配和结局的[共同原因](@entry_id:266381)（即混杂因素）的变量。至关重要的是，这些变量必须是在治疗开始前测量的。纳入治疗开始后测量的变量可能会导致严重的偏倚，例如，通过调整因果路径上的中介变量（这会削弱真实的治疗效应）或调整对撞变量（这会引入虚假的关联）。

例如，在一项比较两种抗凝药物（如直接口服抗凝药与华法林）的电子健康记录研究中，一个合适的协变量集应包括：[人口统计学](@entry_id:143605)特征（年龄、性别）、临床合并症（如先前卒中史、高血压、肾病）、基线实验室检查值（如[肾小球滤过率](@entry_id:164274)、血红蛋白）、以及治疗前的用药史和医疗服务利用情况。这些变量都可能影响医生的处方决策以及患者的结局风险，因此都是潜在的混杂因素。[@problem_id:5221140]

在实施匹配或分层后，下一步是评估这些方法是否成功地平衡了所选协变量的分布。这项诊断任务的目标是量化治疗组和[对照组](@entry_id:188599)之间在协变量上的剩余差异的大小。标准化的均数差（Standardized Mean Difference, SMD）是完成这项任务的首选指标。对于连续变量，SMD是两组均值之差除以一个合并的标准差；对于[二元变量](@entry_id:162761)（如糖尿病史），它是两组比例之差除以一个合并的标准差。SMD是一个不受样本量影响的效应量指标，通常认为其绝对值小于0.1表示协变量已达到可接受的平衡。

与SMD相比，使用p值进行平衡性检验是一种错误的做法。p值检验的是“两组均值完全相等”这一零假设，其大小同时受到差异幅度和样本量的影响。在一个大样本研究中，即使是临床上微不足道的、极小的差异也可能产生统计上显著的p值（例如，$p  0.05$），从而错误地引导研究者认为平衡性不佳。反之，在小样本研究中，一个巨大且重要的差异可能由于检验效力不足而无法达到统计学显著性，从而给人以平衡性良好的假象。因此，SMD提供了一个关于不平衡“量级”的直接度量，这正是平衡性评估的核心问题。Love图是一种标准的可视化工具，它能清晰地展示匹配或分层前后所有协变量的SMD变化，从而直观地评估平衡改善的程度。[@problem_id:4973495]

#### 处理残余不平衡

平衡性评估之后，研究者可能会发现，尽管尽了最大努力，某些协变量在匹配或分层后仍然存在不可忽略的残余不平衡（例如，SMD  0.1）。忽略这种残余不平衡可能会导致有偏倚的治疗效应估计。因此，处理残余不平衡是混杂控制实践中的一个关键环节。

当Love图显示某些关键协变量（如基线收缩压、合并症指数）的平衡性不佳时，这表明最初的混杂控制策略（如倾向性评分模型）可能不够充分。此时，应采取迭代改进的策略。一个多管齐下的方法通常是最佳选择：
1.  **优化倾向性评分模型**：初始模型可能错误地设定了协变量与治疗分配之间的关系。可以尝试更灵活的模型，例如为连续性协变量加入高阶项（如平方项）或[样条](@entry_id:143749)项，以及纳入理论上合理的协变量交互项。
2.  **改进[匹配算法](@entry_id:269190)**：可以尝试更严格的匹配标准，例如使用更小的卡尺（caliper）值，或者改用允许一个对照被多次使用的有放回匹配。
3.  **在匹配样本中进行回归调整**：这是一个非常强大且被广泛推荐的策略。即使匹配后仍有残余不平衡，也可以在匹配后的样本中，拟合一个以结局为因变量的[回归模型](@entry_id:163386)，模型中同时包含治疗分组[指示变量](@entry_id:266428)以及那些仍不平衡的协变量。这种“双重稳健”（doubly robust）的方法结合了匹配和回归的优点，能进一步校正由残余协变量不平衡所导致的偏倚。[@problem_id:4973464]

这种将匹配与回归相结合的思想可以被形式化为“增广匹配估计量”（augmented matching estimator）。其核心思想是从一个简单的匹配估计量（例如，治疗组个体与其匹配[对照组](@entry_id:188599)个体之间的结局差异）出发，然后减去一个用于校正残余协变量差异所致偏倚的项。这个偏倚校正项是基于一个结局[回归模型](@entry_id:163386)（分别在治疗组和[对照组](@entry_id:188599)中拟合）的斜率（即协变量对结局的影响）与该个体及其匹配集之间的协变量残差$\Delta_i$的乘积来估计的。这种方法系统性地利用了结局模型的信息来“填补”匹配未能完全消除的协变量差异，从而提供了一个对[模型设定错误](@entry_id:170325)更不敏感的、更稳健的因果效应估计量。[@problem_id:4973444]

### 跨学科联系与高级情景

分层与匹配所体现的控制混杂思想具有普适性，其应用远远超出了传统的临床流行病学范畴，并能适应更复杂的[数据结构](@entry_id:262134)和科学问题。

#### 基因组学中的群体分层

在[全基因组](@entry_id:195052)关联研究（GWAS）中，一个主要的偏倚来源是“[群体分层](@entry_id:175542)”（population stratification）。当一个研究群体由多个具有不同祖源的亚群体混合而成，且这些亚群体的[等位基因频率](@entry_id:146872)和疾病基线风险都不同时，就会发生[群体分层](@entry_id:175542)。例如，某个单核苷酸多态性（SNP）的等位基因可能在某个祖源亚群中更常见，而该亚群恰好由于环境或其他遗传因素而具有更高的疾病风险。在这种情况下，即使该SNP本身与疾病没有因果关系，在对整个混合群体进行分析时，也会观察到该SNP与疾病之间的虚假关联。

这本质上是经典的混杂问题，其中“祖源”是混杂因素，它同时与“暴露”（特定等位基因）和“结局”（疾病状态）相关。为了解决这个问题，基因组学领域发展出了与流行病学中的分层和匹配思想相通的专门方法：
*   **[主成分分析](@entry_id:145395) (PCA)**：通过对大量[遗传标记](@entry_id:202466)进行PCA，可以识别出代表个体连续遗传祖源的主要变异轴（即主成分）。在关联分析的[回归模型](@entry_id:163386)中，将前几个主成分作为协变量进行调整，就相当于根据个体的遗传背景进行了“分层”或调整，从而有效地控制了由祖源差异引起的混杂。
*   **[线性混合模型](@entry_id:139702) (LMM)**：这种方法通过构建一个遗传关系矩阵（GRM）来量化任意两个个体间的[全基因组](@entry_id:195052)遗传相似度，然后将这种相似度作为随机效应纳入模型。这可以同时控制由群体分层（远亲关系）和家庭内部的亲缘关系（近亲关系）所导致的混杂。
*   **基于家庭的设计**：例如，传递不平衡检验（TDT）通过在家庭内部比较父母传递给患病子女的等位基因与未传递的等位基因，完美地实现了“匹配”。由于父母的遗传背景是固定的，这种在家庭内部的比较天然地对群体分层免疫。[@problem_id:4835243]

#### 处理聚[类数](@entry_id:156164)据与时间-事件结局

在许多医学研究中，数据具有层级或聚类结构，例如患者嵌套在不同的诊所或医院中。不同诊所的诊疗实践、设备水平或患者构成可能存在系统性差异，这些未被测量的诊所级别因素（$u_c$）可能同时影响该诊所内的治疗决策和患者结局，从而成为聚类级别的混杂因素。

处理这类问题的一种有效方法是使用**[固定效应模型](@entry_id:142997)（Fixed Effects Model）**。通过在回归模型中为每个诊所（聚类）设置一个独立的指示变量（即固定效应），模型实质上是在每个诊所内部进行比较。由于特定诊所的所有患者共享相同的诊所级别混杂因素$u_c$，这种“内部”比较自然地消除了$u_c$的影响，其逻辑与分层分析中在层内进行比较以控制分层变量是完全一致的。这种方法在存在未测量且与治疗选择相关的聚类级别混杂时尤为可取。此外，在分析从电子健康记录（EHR）中收集的跨越数年的数据时，还可以同时纳入日历时间固定效应，以控制随时间变化的外部因素（如指南更新、技术进步）所产生的“时期效应”混杂。[@problem_id:4612551]

当研究结局是“事件发生时间”（如感染时间、死亡时间）时，匹配的应用也需要与专门的生存分析方法相结合。在对一个匹配好的队列（例如，通过1:N匹配得到多个匹配集）进行分析时，标准的做法是使用**分层Cox比例风险模型（Stratified Cox Proportional Hazards Model）**。在这种模型中，每一个匹配集被视为一个独立的“层”（stratum）。模型假定各层的基线风险函数$h_{0s}(t)$可以完全不同，但[处理效应](@entry_id:636010)（即风险比（Hazard Ratio））在所有层中是恒定的。在构建部分[似然函数](@entry_id:141927)时，每个事件的风险集仅限于该事件发生时所在层内的其他在风险个体。由于基线[风险函数](@entry_id:166593)$h_{0s}(t)$在[似然比](@entry_id:170863)中被约去，这种方法能够以一种非参数的方式完美地控制所有匹配变量（无论是连续的还是分类的）所带来的混杂，而无需对这些变量如何影响基线风险做出任何函数形式假设。[@problem_id:4973456]

#### 研究设计的作用

最后，必须强调的是，混杂控制不仅是数据分析阶段的任务，它始于研究设计之初。一个精心设计的观察性研究方案能够从源头上最大程度地减少偏倚，使得后续的统计分析更加可信和稳健。例如，在一项旨在评估短期碘暴露是否会急性加重疱疹样皮炎（DH）的研究中，一个关键的混杂因素是变化的麸质摄入量。

一个优秀的设计方案，如**巢式病例对照研究（Nested Case-Control Study）**，会从一个定义明确的前瞻性DH患者队列中识别病例（经历病情加重的患者）和对照。通过在每个病例出现的时间点从当时的风险集中抽样对照（即发生密度抽样），可以确保病例和对照在时间趋势上是可比的。更重要的是，通过将研究对象限制（restriction）在一个严格遵守无麸质饮食（GFD）的队列中，并使用客观生物标志物（如粪便麸质免疫原性肽）来验证依从性，可以极大地减小麸质这一主要混杂因素的变异范围。最后，在分析阶段，仍可通过在条件logistic回归中调整残余的、时变的麸质暴露生物标志物来做进一步的校正。这种从设计（前瞻性队列、限制）到抽样（发生密度抽样）再到分析（回归调整）的多层次控制策略，体现了混杂控制的综合性思维。[@problem_id:4433691]

### 处理未测量混杂

尽管分层和匹[配方法](@entry_id:265480)能够有效处理已测量的混杂因素，但“未测量混杂”（unmeasured confounding）的威胁在所有[观察性研究](@entry_id:174507)中始终存在。这是指存在一些我们未能测量或无法测量的变量$U$，它同时影响治疗选择和结局。

#### 未测量混杂问题：适应症混杂

在医学研究中，一个典型的未测量混杂例子是“适应症混杂”（confounding by indication）。这种情况发生在决定是否给予某种治疗的临床指征（即疾病的严重程度或预后）本身就是结局的一个强预测因子时。例如，在一项使用电子健康记录（EHR）评估早期使用升压药对脓毒症患者死亡率影响的研究中，医生更可能对病情最危重（例如，具有更高的未记录的SOFA评分）的患者使用早期升压药。病情严重程度既是治疗（升压药）的原因，也是结局（死亡）的原因。如果这个关键的严重程度变量没有被完整地记录在案和纳入分析，那么即使我们对所有已测量的协变量$X$（如年龄、基础病）进行了调整，治疗组和[对照组](@entry_id:188599)之间在未测量的严重程度上仍然存在系统性差异。

在潜在结局的框架下，这意味着即使我们调整了$X$，条件可交换性假设$Y^a \perp A \mid X$仍然不成立，因为在$X$的任何一个水平上，$A$的分配仍然与潜在结局$Y^a$通过未测量的$U$相关联。因此，基于倾向性评分$e(X)$的任何调整方法（匹配、分层或加权）都无法消除由$U$引起的偏倚，从而导致对治疗效应的估计出现偏差。[@problem_id:5221159]

#### 使用阴性对照探测偏倚

由于未测量混杂的关键假设（条件[可交换性](@entry_id:263314)）在根本上是无法直接检验的，研究者们发展了一些巧妙的诊断工具来间接探测其是否存在，其中最重要的是**阴性对照（negative controls）**。其核心思想是，检验一个已知不存在因果关系的暴露-结局关联，如果该关联在调整了已测量协变量后仍然存在，则强烈暗示存在未测量的混杂因素。

阴性对照有两种主要类型：
1.  **阴性对照结局 (NCO)**：这是一个我们根据生物学或临床知识确信不受研究暴露影响的结局。例如，在研究某种药物对心脏病风险的影响时，可以选择“意外伤害”作为NCO。如果在对已测量协变量进行倾向性评分调整后，该药物的服用仍然与“意外伤害”的发生率相关，这就暗示存在某种未测量的生活方式或行为因素（如鲁莽程度）同时影响了用药决策和意外伤害风险，而这个因素也可能混杂了药物与心脏病风险的真实关系。
2.  **阴性对照暴露 (NCE)**：这是一种已知不会影响研究结局的暴露。例如，在研究某种特定[他汀类药物](@entry_id:167025)对心血管事件的影响时，可以选择另一种用于治疗完全不同疾病（如痛风）且无已知心血管效应的药物作为NCE。如果在调整了协变量后，NCE的使用仍然与心血管事件相关，这表明存在未测量的患者特征（如就医行为、健康意识）同时影响了多种药物的选择和心血管结局。

使用阴性对照的前提是，这些虚假的关联必须共享与主要研究问题相同的未测量混杂结构。一个非零的阴性对照结果并不能“校正”偏倚，但它是一个强烈的警示信号，表明主要结果可能也受到了类似偏倚的影响，需要谨慎解释。[@problem_id:5221131]

#### 使用敏感性分析量化稳健性

当有理由怀疑存在未测量混杂时，即使阴性对照检验发出了警报，我们还想知道这种混杂需要有多“强”才能改变研究的结论。**[敏感性分析](@entry_id:147555)（Sensitivity Analysis）**，特别是Rosenbaum界限法，为这个问题提供了一个定量的回答。

这种方法通常用于匹配研究，它引入一个敏感性参数$\Gamma \ge 1$。$\Gamma$量化了一个未测量混杂因素可能对治疗分配概率产生的最大影响。例如，$\Gamma = 2$意味着在一个匹配对中，一个具有某种未测量混杂因[素特征](@entry_id:155979)的个体，其接受治疗的几率（odds）可能是另一个缺乏该特征的个体的两倍。

[敏感性分析](@entry_id:147555)通过计算在不同$\Gamma$值下，“最坏情况”的p值上限来进行。所谓“最坏情况”，是指假设这个未测量的混杂因素以一种最有利于产生虚假显著结果的方式与结局相关。然后，我们可以观察随着$\Gamma$值的增加，p值会如何变化。如果在一个相对较小（即更合理）的$\Gamma$值（如1.5）下，[p值](@entry_id:136498)就变得不再显著（例如，超过0.05），那么研究结论对微小的未测量混杂就很敏感，结论是脆弱的。相反，如果只有在一个非常大且在临床上不切实际的$\Gamma$值（如4或5）下，[p值](@entry_id:136498)才会变得不显著，那么研究结论就是相对稳健的，因为它需要一个极强的未测量混杂因素才能被推翻。通过这种方式，敏感性分析为我们解释观察性研究结果提供了一个重要的“不确定性边界”。[@problem_id:4973439]

### 结论

本章通过一系列真实和跨学科的应用场景，展示了分层与匹配作为混杂控制方法的广度和深度。我们看到，这些方法不仅仅是简单的统计技术，而是一个从研究设计到数据分析再到结果解释的完整思想体系。

我们从临床流行病学的核心应用开始，学习了如何使用分层来估计标准化的治疗效应（[@problem_id:4973428]）和通过Mantel-Haenszel检验来整合多中心的数据（[@problem_id:4973451]）。我们还探讨了匹配设计如何要求特定的配对分析方法来处理其诱导的相关性（[@problem_id:4973438]）。

接着，我们深入到应用这些方法的实践层面。这包括从审慎选择用于倾向性评分模型的协变量（[@problem_id:5221140]），到使用SMD和Love图等工具诊断匹配后的平衡性，并批判性地审视了p值在这一任务中的不适用性（[@problem_id:4973495]）。我们还学习了当残余不平衡存在时，如何通过优化模型或结合回归调整等策略进行补救（[@problem_id:4973464], [@problem_id:4973444]）。

通过将视野扩展到其他学科，我们发现同样是混杂控制的逻辑，在基因组学中表现为对[群体分层](@entry_id:175542)的校正（[@problem_id:4835243]），在处理聚类数据时则与[固定效应模型](@entry_id:142997)思想相通（[@problem_id:4612551]），而在面对时间-事件数据时则需要与分层Cox模型巧妙结合（[@problem_id:4973456]）。这些例子凸显了混杂控制原理的普适性。

最后，我们直面了所有观察性研究的根本挑战——未测量混杂。我们讨论了如何识别“适应症混杂”这类常见的偏倚来源（[@problem_id:5221159]），并介绍了使用阴性对照作为诊断工具来探测残余混杂（[@problem_id:5221131]），以及利用敏感性分析来量化研究结论对未测量混杂的稳健性（[@problem_id:4973439]）。

在实践中，不同的倾向性评分方法——匹配、加权和分层——服务于不同的目标并伴随着各自的偏倚-方差权衡。例如，标准的逆概率加权（IPTW）通常旨在估计整个人群的平均治疗效应（ATE），但可能因极端权重而导致方差增大；而典型的1:1匹配则旨在估计治疗人群的平均效应（ATT），且对极端倾向性评分不那么敏感。理解这些方法的特点、它们所估计的目标（estimand）以及它们对极端倾向性评分的处理方式，对于为特定研究问题选择最合适的分析策略至关重要。[@problem_id:4511116]

总之，分层与匹配不仅仅是技术操作，它们体现了在缺乏随机化的世界里追求因果推断的严谨科学精神。掌握这些应用，将使研究者能够更深刻地设计、执行和解读观察性研究，从而为科学和临床实践提供更可信的证据。