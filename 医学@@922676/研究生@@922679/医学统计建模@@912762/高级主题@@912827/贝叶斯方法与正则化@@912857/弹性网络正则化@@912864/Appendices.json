{"hands_on_practices": [{"introduction": "在正则化模型中，我们为何要区别对待截距项？这项练习阐明了截距的角色，并从数学上证明了对其进行惩罚为何不恰当，从而确保我们的模型得到正确校准。这是构建任何正则化回归模型时基础而关键的一步。[@problem_id:4961465]", "problem": "一个临床研究团队正在一个大型的患者观察性队列中，为一个连续的生物标志物结果开发一个线性预测器。对于患者 $i \\in \\{1,\\dots,n\\}$，设 $y_{i} \\in \\mathbb{R}$ 表示测量的结果（例如，收缩压），并设 $\\boldsymbol{x}_{i} \\in \\mathbb{R}^{p}$ 表示一个包含 $p$ 个临床协变量（例如，实验室值和人口统计信息）的向量。该团队提出了带截距的弹性网络正则化线性模型，该模型通过最小化经验风险来估计参数 $(\\beta_{0},\\boldsymbol{\\beta}) \\in \\mathbb{R} \\times \\mathbb{R}^{p}$\n$$\n\\frac{1}{2n}\\sum_{i=1}^{n}\\big(y_{i}-\\beta_{0}-\\boldsymbol{x}_{i}^{\\top}\\boldsymbol{\\beta}\\big)^{2}+\\lambda\\left(\\frac{1-\\alpha}{2}\\|\\boldsymbol{\\beta}\\|_{2}^{2}+\\alpha\\|\\boldsymbol{\\beta}\\|_{1}\\right),\n$$\n其中 $\\lambda \\ge 0$ 和 $\\alpha \\in [0,1]$ 是正则化超参数。假设每个协变量已在患者间进行了中心化，因此设计矩阵的列均值为零；等价地，$\\overline{\\boldsymbol{x}}=\\frac{1}{n}\\sum_{i=1}^{n}\\boldsymbol{x}_{i}=\\boldsymbol{0}$。设 $\\overline{y}=\\frac{1}{n}\\sum_{i=1}^{n}y_{i}$ 表示结果的样本均值。\n\n从凸优化的基本原理和均方误差的基本求导法则出发，并且不借助任何预先指定的回归恒等式，执行以下操作：\n\n- 在任意固定的 $\\boldsymbol{\\beta}$ 条件下，推导出关于 $\\beta_{0}$ 的唯一最小化子，并仅使用 $\\overline{y}$、$\\overline{\\boldsymbol{x}}$ 和 $\\boldsymbol{\\beta}$ 将其表示为闭式解。\n- 在所述的协变量中心化假设下，将此条件最小化子简化为一个不依赖于 $\\boldsymbol{\\beta}$ 或正则化超参数的闭式表达式。\n- 提供一个严谨的论证，该论证基于从目标函数导出的不变性和校准考虑，解释为什么对 $\\beta_{0}$ 应用任何显式惩罚项（例如，添加 $\\lambda_{0}|\\beta_{0}|$ 或 $\\lambda_{0}\\beta_{0}^{2}$，其中 $\\lambda_{0}0$）在临床统计建模中是不合适的。\n\n作为你的最终答案，仅报告在中心化协变量下最优 $\\beta_{0}$ 的简化闭式表达式。不需要数值近似，也不适用任何物理单位。", "solution": "所述问题是有效的。它在科学上基于正则化线性模型（特别是弹性网络回归）的既定理论。该问题是适定的，提供了一个凸目标函数和一组明确的任务，这些任务导向一个唯一的、可推导的解。所有术语都已定义，并且假设（例如，中心化的协变量）也已明确说明。该问题是客观的，没有任何事实或逻辑上的不一致之处。\n\n根据问题陈述的要求，解答分为三个部分。设目标函数表示为 $L(\\beta_{0}, \\boldsymbol{\\beta})$。\n$$\nL(\\beta_{0}, \\boldsymbol{\\beta}) = \\frac{1}{2n}\\sum_{i=1}^{n}\\big(y_{i}-\\beta_{0}-\\boldsymbol{x}_{i}^{\\top}\\boldsymbol{\\beta}\\big)^{2}+\\lambda\\left(\\frac{1-\\alpha}{2}\\|\\boldsymbol{\\beta}\\|_{2}^{2}+\\alpha\\|\\boldsymbol{\\beta}\\|_{1}\\right)\n$$\n\n首先，对于任意但固定的 $\\boldsymbol{\\beta}$ 值，我们推导 $L$ 关于 $\\beta_{0}$ 的唯一最小化子。惩罚项 $\\lambda\\left(\\frac{1-\\alpha}{2}\\|\\boldsymbol{\\beta}\\|_{2}^{2}+\\alpha\\|\\boldsymbol{\\beta}\\|_{1}\\right)$ 不依赖于 $\\beta_{0}$。因此，最小化 $L(\\beta_{0}, \\boldsymbol{\\beta})$ 关于 $\\beta_{0}$ 的问题等价于最小化均方误差（MSE）项：\n$$\nf(\\beta_{0}) = \\frac{1}{2n}\\sum_{i=1}^{n}\\big(y_{i}-\\beta_{0}-\\boldsymbol{x}_{i}^{\\top}\\boldsymbol{\\beta}\\big)^{2}\n$$\n函数 $f(\\beta_{0})$ 是一个关于 $\\beta_{0}$ 的二次函数，因此是凸的。可以通过将其关于 $\\beta_{0}$ 的一阶导数设为零来找到唯一的最小值。使用链式求导法则：\n$$\n\\frac{\\partial f}{\\partial \\beta_{0}} = \\frac{1}{2n}\\sum_{i=1}^{n} 2 \\cdot \\big(y_{i}-\\beta_{0}-\\boldsymbol{x}_{i}^{\\top}\\boldsymbol{\\beta}\\big) \\cdot \\frac{\\partial}{\\partial \\beta_{0}}\\big(y_{i}-\\beta_{0}-\\boldsymbol{x}_{i}^{\\top}\\boldsymbol{\\beta}\\big)\n$$\n$$\n\\frac{\\partial f}{\\partial \\beta_{0}} = \\frac{1}{n}\\sum_{i=1}^{n} \\big(y_{i}-\\beta_{0}-\\boldsymbol{x}_{i}^{\\top}\\boldsymbol{\\beta}\\big) \\cdot (-1) = -\\frac{1}{n}\\sum_{i=1}^{n} \\big(y_{i}-\\beta_{0}-\\boldsymbol{x}_{i}^{\\top}\\boldsymbol{\\beta}\\big)\n$$\n为了找到最优值（我们记为 $\\hat{\\beta}_{0}$），我们将此导数设为零：\n$$\n-\\frac{1}{n}\\sum_{i=1}^{n} \\big(y_{i}-\\hat{\\beta}_{0}-\\boldsymbol{x}_{i}^{\\top}\\boldsymbol{\\beta}\\big) = 0\n$$\n乘以 $-n$ 得到：\n$$\n\\sum_{i=1}^{n} \\big(y_{i}-\\hat{\\beta}_{0}-\\boldsymbol{x}_{i}^{\\top}\\boldsymbol{\\beta}\\big) = 0\n$$\n我们可以分配求和符号：\n$$\n\\sum_{i=1}^{n}y_{i} - \\sum_{i=1}^{n}\\hat{\\beta}_{0} - \\sum_{i=1}^{n}\\boldsymbol{x}_{i}^{\\top}\\boldsymbol{\\beta} = 0\n$$\n由于 $\\hat{\\beta}_{0}$ 相对于求和索引 $i$ 是一个常数，所以 $\\sum_{i=1}^{n}\\hat{\\beta}_{0} = n\\hat{\\beta}_{0}$。方程变为：\n$$\n\\sum_{i=1}^{n}y_{i} - n\\hat{\\beta}_{0} - \\left(\\sum_{i=1}^{n}\\boldsymbol{x}_{i}^{\\top}\\right)\\boldsymbol{\\beta} = 0\n$$\n求解 $\\hat{\\beta}_{0}$：\n$$\nn\\hat{\\beta}_{0} = \\sum_{i=1}^{n}y_{i} - \\left(\\sum_{i=1}^{n}\\boldsymbol{x}_{i}^{\\top}\\right)\\boldsymbol{\\beta}\n$$\n两边除以 $n$ 得到 $\\hat{\\beta}_{0}$ 的表达式：\n$$\n\\hat{\\beta}_{0} = \\frac{1}{n}\\sum_{i=1}^{n}y_{i} - \\left(\\frac{1}{n}\\sum_{i=1}^{n}\\boldsymbol{x}_{i}^{\\top}\\right)\\boldsymbol{\\beta}\n$$\n使用所提供的定义 $\\overline{y}=\\frac{1}{n}\\sum_{i=1}^{n}y_{i}$ 和 $\\overline{\\boldsymbol{x}}=\\frac{1}{n}\\sum_{i=1}^{n}\\boldsymbol{x}_{i}$，表达式变为：\n$$\n\\hat{\\beta}_{0} = \\overline{y} - \\overline{\\boldsymbol{x}}^{\\top}\\boldsymbol{\\beta}\n$$\n这是条件最小化子的闭式表达式。为了确认它是一个最小值，我们检查二阶导数：$\\frac{\\partial^{2} f}{\\partial \\beta_{0}^{2}} = \\frac{\\partial}{\\partial \\beta_{0}} \\left[ -\\frac{1}{n}\\sum_{i=1}^{n} (y_{i}-\\beta_{0}-\\boldsymbol{x}_{i}^{\\top}\\boldsymbol{\\beta}) \\right] = -\\frac{1}{n} \\sum_{i=1}^{n} (-1) = \\frac{1}{n} \\sum_{i=1}^{n} 1 = \\frac{n}{n} = 1$。由于二阶导数为正 ($1 > 0$)，该函数在 $\\beta_{0}$ 上是严格凸的，因此 $\\hat{\\beta}_{0}$ 是一个唯一的最小值。\n\n其次，我们在协变量已中心化（即 $\\overline{\\boldsymbol{x}} = \\boldsymbol{0}$）的既定假设下简化此表达式。将 $\\overline{\\boldsymbol{x}} = \\boldsymbol{0}$ 代入推导出的 $\\hat{\\beta}_{0}$ 表达式中：\n$$\n\\hat{\\beta}_{0} = \\overline{y} - \\boldsymbol{0}^{\\top}\\boldsymbol{\\beta}\n$$\n零向量与任何向量 $\\boldsymbol{\\beta}$ 的乘积是标量 $0$。因此，表达式简化为：\n$$\n\\hat{\\beta}_{0} = \\overline{y}\n$$\n这个结果表明，当协变量中心化时，最优的截距项就是结果变量的样本均值 $\\overline{y}$。重要的是，这个 $\\beta_{0}$ 的最优值与协变量效应 $\\boldsymbol{\\beta}$ 以及正则化超参数 $\\lambda$ 和 $\\alpha$ 无关。在实践中，这允许进行计算简化：可以（在中心化协变量 $\\boldsymbol{x}$ 的基础上）再中心化结果变量 $y$，然后拟合一个没有截距的模型。原始、未中心化问题的截距则可恢复为 $\\overline{y}$。\n\n第三，我们提供一个严谨的论证，解释为什么在统计建模中为截距 $\\beta_0$ 添加惩罚项是不合适的。\n1.  **对位置平移的不变性**：统计模型的一个基本要求是，其核心结论（即协变量的估计效应 $\\boldsymbol{\\beta}$）应对于结果变量原点的任意选择保持不变。例如，如果结果 $y_i$ 是温度，那么温度与一组预测变量之间的估计关系不应取决于温度是用摄氏度还是开尔文度（原点的平移）来衡量。让我们考虑结果的平移 $y_i \\to y_i' = y_i + c$，其中 $c$ 为某个常数。一个直观的模型应该通过平移截距 $\\beta_0 \\to \\beta_0' = \\beta_0 + c$ 来适应这种情况，同时保持 $\\boldsymbol{\\beta}$ 不变。在没有对 $\\beta_0$ 进行惩罚的情况下，最优截距是 $\\hat{\\beta}_0 = \\overline{y} - \\overline{\\boldsymbol{x}}^\\top\\boldsymbol{\\beta}$。对于平移后的结果 $y'$，新的均值是 $\\overline{y'} = \\overline{y} + c$，因此新的截距是 $\\hat{\\beta}_0' = (\\overline{y}+c) - \\overline{\\boldsymbol{x}}^\\top\\boldsymbol{\\beta} = \\hat{\\beta}_0 + c$。这种所期望的不变性是成立的。现在，假设我们向目标函数添加一个类似 $\\lambda_0 \\beta_0^2$（其中 $\\lambda_0 > 0$）的惩罚项。关于 $\\beta_0$ 的导数变为 $-\\frac{1}{n}\\sum (y_i - \\beta_0 - \\boldsymbol{x}_i^\\top\\boldsymbol{\\beta}) + 2\\lambda_0 \\beta_0$。新的最优值 $\\hat{\\beta}_0$ 将是一个有偏估计，被拉向 $0$。模型将不再对 $y$ 的平移保持不变，因为 $\\boldsymbol{\\beta}$ 的解将与 $y$ 的原点选择纠缠在一起。这将使得协变量效应的解释依赖于一个任意的基线，这在科学上是不合理的。\n2.  **模型校准与偏差**：截距 $\\beta_0$ 确保模型是经过校准的，即残差的均值为零。如我们最初的推导所示，将 $\\frac{\\partial L}{\\partial \\beta_0} = 0$ 设为零等价于确保 $\\frac{1}{n}\\sum(y_i - \\hat{y}_i) = 0$，其中 $\\hat{y}_i = \\hat{\\beta}_0 + \\boldsymbol{x}_i^\\top\\boldsymbol{\\beta}$。这意味着模型没有系统性偏差（即，它不会持续地高估或低估）。如果我们惩罚 $\\beta_0$，导数条件会改变，我们将不再有 $\\frac{1}{n}\\sum(y_i - \\hat{y}_i) = 0$。惩罚项会在模型的预测中引入系统性偏差，这在任何应用中都是非常不希望看到的，特别是在无偏预测至关重要的临床环境中。\n3.  **正则化的概念作用**：正则化惩罚的目的是通过收缩协变量的估计效应 $\\boldsymbol{\\beta}$ 来控制模型复杂度和防止过拟合。系数 $\\beta_j$ 的大小反映了协变量 $x_j$ 与结果 $y$ 之间关联的强度。将这些系数向零收缩对应于假设某些协变量可能几乎没有或完全没有真实效应。然而，截距 $\\beta_0$ 并不衡量协变量的效应。它代表当所有协变量为零时（或者如果中心化了，则在它们的均值处）结果的基线值。正如我们已经证明的，对于中心化的协变量，$\\beta_0$ 就是结果的均值 $\\overline{y}$。惩罚 $\\beta_0$ 将意味着将数据的估计均值向零收缩。这样做没有任何科学或统计上的理由；样本均值 $\\overline{y}$ 是总体均值的最佳无偏估计。因此，惩罚截距与正则化的根本目标不符，正则化的目标是调节预测变量的效应，而不是结果的整体水平。\n\n总而言之，惩罚截距会破坏基本的不变性属性，引入系统性偏差，并误用了正则化的原理。正是由于这些原因，在正则化回归模型中，截距项通常不被惩罚。", "answer": "$$\n\\boxed{\\overline{y}}\n$$", "id": "4961465"}, {"introduction": "弹性网络的一个关键动机是它能有效处理高度相关的预测变量，这在医学数据中是常见情景。本练习提供了一个具体的例子，对比了 Lasso 如何在相关变量中任意选择其一，而弹性网络则倾向于将它们分组，从而产生更稳定且易于解释的模型。[@problem_id:4961425]", "problem": "一个临床研究团队正在使用一个线性模型对一个连续性心脏代谢结局进行建模，该模型包含两个已知高度相关的标准化预测变量：低密度脂蛋白胆固醇和载脂蛋白B。令设计矩阵的列被标准化为零均值和单位方差，并假设经验格拉姆矩阵和预测变量与结局的内积（按样本量缩放）由以下公式概括：\n$$\nG \\equiv \\frac{X^{\\top}X}{n} = \\begin{pmatrix} 1  0.9 \\\\ 0.9  1 \\end{pmatrix}, \\qquad c \\equiv \\frac{X^{\\top}y}{n} = \\begin{pmatrix} 1.0 \\\\ 0.95 \\end{pmatrix}.\n$$\n考虑最小化以下凸目标函数的惩罚最小二乘估计量：\n$$\n\\frac{1}{2}\\,\\beta^{\\top} G \\beta - c^{\\top} \\beta \\;+\\; \\lambda_{1}\\,\\|\\beta\\|_{1} \\;+\\; \\frac{\\lambda_{2}}{2}\\,\\|\\beta\\|_{2}^{2},\n$$\n其中 $\\beta \\in \\mathbb{R}^{2}$，$\\|\\cdot\\|_{1}$ 是 $\\ell_{1}$ 范数，$\\|\\cdot\\|_{2}$ 是欧几里得范数。最小绝对值收缩和选择算子 (Lasso) 对应于 $\\lambda_{2}=0$ 和 $\\lambda_{1}0$ 的情况，而弹性网络 (elastic net) 对应于 $\\lambda_{1}0$ 和 $\\lambda_{2}0$ 的情况。\n\n仅使用凸最优性的基本定义和 Karush–Kuhn–Tucker (KKT) 条件，完成以下任务：\n- 首先，对于 Lasso，取 $\\lambda_{1}=0.6$ 和 $\\lambda_{2}=0$，判断最优解是否将其中一个系数设为零。确定哪个预测变量被保留，并计算其系数。\n- 其次，对于弹性网络，取 $\\lambda_{1}=0.3$ 和 $\\lambda_{2}=0.3$。在与正相关生物标志物一致的符号模式下，求解精确的弹性网络系数，并验证两个系数都严格为正。\n\n作为最终答案，报告第二个预测变量（载脂蛋白B的代理变量）的弹性网络系数值，将其简化为单个有理数。不要包含单位，也不要四舍五入。", "solution": "我们从 $\\beta \\in \\mathbb{R}^{2}$ 的凸惩罚最小二乘目标函数开始：\n$$\nQ(\\beta) \\equiv \\frac{1}{2}\\,\\beta^{\\top} G \\beta - c^{\\top} \\beta \\;+\\; \\lambda_{1}\\,\\|\\beta\\|_{1} \\;+\\; \\frac{\\lambda_{2}}{2}\\,\\|\\beta\\|_{2}^{2},\n$$\n其中\n$$\nG=\\begin{pmatrix} 1  0.9 \\\\ 0.9  1 \\end{pmatrix}, \\qquad c=\\begin{pmatrix} 1.0 \\\\ 0.95 \\end{pmatrix}.\n$$\n因为 $G$ 是半正定的且 $\\lambda_{2}\\ge 0$，所以 $Q$ 是凸函数。Karush–Kuhn–Tucker (KKT) 条件刻画了最优性。将 $\\|\\beta\\|_{1}$ 的次梯度写为 $s \\in \\partial \\|\\beta\\|_{1}$，其中当 $\\beta_{j}\\ne 0$ 时 $s_{j}=\\operatorname{sgn}(\\beta_{j})$，当 $\\beta_{j}=0$ 时 $s_{j}\\in[-1,1]$。KKT 的平稳性条件为：\n$$\nG\\beta - c + \\lambda_{1}\\,s + \\lambda_{2}\\,\\beta = 0.\n$$\n我们将此应用于两种情况。\n\nLasso 情况 ($\\lambda_{1}=0.6$, $\\lambda_{2}=0$) 。我们检验一个仅有第一个预测变量是活跃的稀疏解是否是 KKT 可行的。假设 $\\beta_{1}0$ 且 $\\beta_{2}=0$。那么 $s_{1}=+1$, $s_{2}\\in[-1,1]$。平稳性方程按分量写出为：\n$$\n\\begin{aligned}\n\\text{for } j=1:\\quad (G\\beta)_{1} - c_{1} + \\lambda_{1} s_{1} = 0 \\;\\Rightarrow\\; 1\\cdot \\beta_{1} + 0.9\\cdot 0 - 1.0 + 0.6\\cdot 1 = 0 \\;\\Rightarrow\\; \\beta_{1} = 0.4,\\\\\n\\text{for } j=2:\\quad (G\\beta)_{2} - c_{2} + \\lambda_{1} s_{2} = 0 \\;\\Rightarrow\\; 0.9\\cdot \\beta_{1} + 1\\cdot 0 - 0.95 + 0.6\\,s_{2} = 0.\n\\end{aligned}\n$$\n为了使 $\\beta_{2}=0$ 是最优的，我们需要 $s_{2}\\in[-1,1]$ 使得第二个方程成立，这等价于次梯度不等式：\n$$\n\\left| (G\\beta)_{2} - c_{2} \\right| \\le \\lambda_{1} \\;\\;\\Longleftrightarrow\\;\\; \\left| 0.9\\cdot 0.4 - 0.95 \\right| \\le 0.6 \\;\\;\\Longleftrightarrow\\;\\; | -0.59 | \\le 0.6.\n$$\n此不等式成立，因为 $0.59 \\le 0.6$。因此，当 $\\beta_{1}=0.4$ 和 $\\beta_{2}=0$ 时，KKT 条件得到满足。所以，Lasso 只选择了第一个预测变量，并将第二个预测变量的系数设为零。\n\n弹性网络情况 ($\\lambda_{1}=0.3$, $\\lambda_{2}=0.3$) 。我们现在求解弹性网络问题，假设两个预测变量都是正相关的，即 $\\beta_{1}0, \\beta_{2}0$，因此 $s=(1,1)^{\\top}$。KKT 平稳性条件变为一个线性系统：\n$$\n\\left( G + \\lambda_{2} I \\right) \\beta - c + \\lambda_{1} \\begin{pmatrix}1\\\\ 1\\end{pmatrix} = 0\n\\;\\;\\Longleftrightarrow\\;\\;\n\\left( G + \\lambda_{2} I \\right) \\beta = c - \\lambda_{1} \\begin{pmatrix}1\\\\ 1\\end{pmatrix}.\n$$\n使用给定的值，\n$$\nA \\equiv G + \\lambda_{2} I = \\begin{pmatrix} 1+0.3  0.9 \\\\ 0.9  1+0.3 \\end{pmatrix} = \\begin{pmatrix} 1.3  0.9 \\\\ 0.9  1.3 \\end{pmatrix}, \\qquad\nb \\equiv c - \\lambda_{1} \\begin{pmatrix}1\\\\ 1\\end{pmatrix} = \\begin{pmatrix} 1.0-0.3 \\\\ 0.95-0.3 \\end{pmatrix} = \\begin{pmatrix} 0.7 \\\\ 0.65 \\end{pmatrix}.\n$$\n我们计算 $A^{-1}$。其行列式为\n$$\n\\det(A) = (1.3)^{2} - (0.9)^{2} = 1.69 - 0.81 = 0.88,\n$$\n且\n$$\nA^{-1} = \\frac{1}{\\det(A)} \\begin{pmatrix} 1.3  -0.9 \\\\ -0.9  1.3 \\end{pmatrix} = \\frac{1}{0.88} \\begin{pmatrix} 1.3  -0.9 \\\\ -0.9  1.3 \\end{pmatrix}.\n$$\n因此，\n$$\n\\beta = A^{-1} b = \\frac{1}{0.88} \\begin{pmatrix} 1.3  -0.9 \\\\ -0.9  1.3 \\end{pmatrix} \\begin{pmatrix} 0.7 \\\\ 0.65 \\end{pmatrix}.\n$$\n将其展开相乘，\n$$\n\\beta_{1} = \\frac{1}{0.88}\\left( 1.3\\cdot 0.7 - 0.9\\cdot 0.65 \\right) = \\frac{1}{0.88}\\left( 0.91 - 0.585 \\right) = \\frac{0.325}{0.88} = \\frac{65}{176},\n$$\n$$\n\\beta_{2} = \\frac{1}{0.88}\\left( -0.9\\cdot 0.7 + 1.3\\cdot 0.65 \\right) = \\frac{1}{0.88}\\left( -0.63 + 0.845 \\right) = \\frac{0.215}{0.88} = \\frac{43}{176}.\n$$\n两个系数都严格为正，这证实了我们假设的符号模式，因此解是 KKT 可行的。所以，在这种高度相关的设定下，Lasso 只选择了第一个预测变量（$\\beta_{1}=0.4$, $\\beta_{2}=0$），而弹性网络则在两个变量之间分配了权重（$\\beta_{1}=\\frac{65}{176}$ 和 $\\beta_{2}=\\frac{43}{176}$）。\n\n所要求的量是第二个预测变量的精确弹性网络系数，即 $\\beta_{2}=\\frac{43}{176}$。", "answer": "$$\\boxed{\\frac{43}{176}}$$", "id": "4961425"}, {"introduction": "从线性模型拓展到更广泛的应用，我们如何为疾病发生率这类二元结果拟合弹性网络模型？这项练习将引导你推导用于逻辑回归的坐标下降算法，揭示了广泛使用的统计软件背后的核心机制，从而解开了模型拟合过程的神秘面纱。[@problem_id:4961374]", "problem": "一项临床研究将术后并发症的二元发生建模为 $n$ 名患者的 $p$ 个术前生物标志物的函数。令 $y_i \\in \\{0,1\\}$ 表示患者 $i$ 的并发症状态，令 $\\boldsymbol{x}_i \\in \\mathbb{R}^p$ 表示其生物标志物向量。考虑一个逻辑斯谛回归模型，其线性预测器为 $\\eta_i = \\beta_0 + \\sum_{j=1}^{p} x_{ij} \\beta_j$，概率为 $p_i = \\left(1 + \\exp(-\\eta_i)\\right)^{-1}$。为处理多重共线性并鼓励稀疏性，使用弹性网络正则化：通过最小化惩罚负对数似然来估计 $(\\beta_0, \\boldsymbol{\\beta})$\n$$\n\\mathcal{L}(\\beta_0, \\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left[ \\ln\\!\\left(1 + \\exp(\\eta_i)\\right) - y_i \\eta_i \\right] + \\lambda \\left( \\alpha \\|\\boldsymbol{\\beta}\\|_1 + \\frac{1-\\alpha}{2} \\|\\boldsymbol{\\beta}\\|_2^2 \\right),\n$$\n其中调整参数为 $\\lambda  0$ 和 $\\alpha \\in [0,1]$，且截距项 $\\beta_0$ 不受惩罚。\n\n使用数据保真项在当前迭代值（迭代重加权最小二乘法，IRLS）附近的二阶泰勒展开，然后对 $\\ell_1$ 项应用坐标级近端更新，推导单个系数 $\\beta_j$（其中 $j \\in \\{1,\\dots,p\\}$，即不包括截距项）的坐标下降更新。在您的推导中，明确指出由二阶近似产生的工作权重和伪响应，将关于 $\\beta_j$ 的子问题简化为一维惩罚二次型，并求出其闭式解。\n\n作为最终输出，提供坐标级更新 $\\beta_j^{\\text{new}}$ 的闭式解析表达式，该表达式仅用当前工作权重 $w_i$、伪响应 $z_i$、预测变量 $x_{ij}$、其他系数的当前值 $\\{\\beta_k: k \\neq j\\}$ 以及调整参数 $\\lambda$ 和 $\\alpha$ 来表示。截距项 $\\beta_0$ 不应被惩罚。您的最终答案必须是单个闭式表达式。不要四舍五入。不要包含单位。", "solution": "用户提供了一个来自统计建模领域的有效且提法恰当的问题陈述。它要求推导弹性网络惩罚的逻辑斯谛回归模型的坐标下降更新法则。推导过程将首先对目标函数的似然部分应用二阶泰勒展开，这与迭代重加权最小二乘 (IRLS) 框架一致，然后求解得到的关于单个系数 $\\beta_j$ 的一维惩罚二次子问题。\n\n需要最小化的目标函数是惩罚负对数似然：\n$$\n\\mathcal{L}(\\beta_0, \\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left[ \\ln\\left(1 + \\exp(\\eta_i)\\right) - y_i \\eta_i \\right] + \\lambda \\left( \\alpha \\|\\boldsymbol{\\beta}\\|_1 + \\frac{1-\\alpha}{2} \\|\\boldsymbol{\\beta}\\|_2^2 \\right)\n$$\n其中 $\\eta_i = \\beta_0 + \\sum_{j=1}^{p} x_{ij} \\beta_j$。我们将数据保真项（负对数似然）表示为 $L(\\beta_0, \\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\ell_i(\\eta_i)$，其中 $\\ell_i(\\eta_i) = \\ln(1 + \\exp(\\eta_i)) - y_i \\eta_i$。惩罚项为 $P(\\boldsymbol{\\beta}) = \\lambda ( \\alpha \\|\\boldsymbol{\\beta}\\|_1 + \\frac{1-\\alpha}{2} \\|\\boldsymbol{\\beta}\\|_2^2 )$。\n\n迭代重加权最小二乘 (IRLS) 方法的核心是围绕参数的当前估计值，构建数据保真项 $L(\\beta_0, \\boldsymbol{\\beta})$ 的二次近似。设基于当前系数估计值 $(\\tilde{\\beta}_0, \\tilde{\\boldsymbol{\\beta}})$ 的线性预测器为 $\\tilde{\\eta}_i$。我们对 $\\ell_i(\\eta_i)$ 在 $\\tilde{\\eta}_i$ 附近进行二阶泰勒展开。这需要 $\\ell_i$ 关于 $\\eta_i$ 的一阶和二阶导数。\n令 $p_i = (1 + \\exp(-\\eta_i))^{-1}$。一阶导数为：\n$$\n\\frac{\\partial \\ell_i}{\\partial \\eta_i} = \\frac{\\exp(\\eta_i)}{1 + \\exp(\\eta_i)} - y_i = p_i - y_i\n$$\n二阶导数为：\n$$\n\\frac{\\partial^2 \\ell_i}{\\partial \\eta_i^2} = \\frac{\\partial p_i}{\\partial \\eta_i} = \\frac{\\exp(-\\eta_i)}{\\left(1 + \\exp(-\\eta_i)\\right)^2} = p_i (1-p_i)\n$$\n$L$ 的二阶泰勒展开为：\n$$\nL(\\beta_0, \\boldsymbol{\\beta}) \\approx \\sum_{i=1}^{n} \\left[ \\ell_i(\\tilde{\\eta}_i) + (\\eta_i - \\tilde{\\eta}_i)\\left.\\frac{\\partial \\ell_i}{\\partial \\eta_i}\\right|_{\\tilde{\\eta}_i} + \\frac{1}{2}(\\eta_i - \\tilde{\\eta}_i)^2 \\left.\\frac{\\partial^2 \\ell_i}{\\partial \\eta_i^2}\\right|_{\\tilde{\\eta}_i} \\right]\n$$\n代入在当前估计值（记为 $\\tilde{p}_i = p_i(\\tilde{\\eta}_i)$）处计算的导数：\n$$\nL(\\beta_0, \\boldsymbol{\\beta}) \\approx \\sum_{i=1}^{n} \\left[ \\ell_i(\\tilde{\\eta}_i) + (\\eta_i - \\tilde{\\eta}_i)(\\tilde{p}_i - y_i) + \\frac{1}{2}(\\eta_i - \\tilde{\\eta}_i)^2 \\tilde{p}_i(1-\\tilde{p}_i) \\right]\n$$\n省略相对于新参数 $(\\beta_0, \\boldsymbol{\\beta})$ 为常数的项，我们最小化一个与下式成正比的目标：\n$$\n\\sum_{i=1}^{n} \\left[ (\\eta_i - \\tilde{\\eta}_i)(\\tilde{p}_i - y_i) + \\frac{1}{2}(\\eta_i - \\tilde{\\eta}_i)^2 \\tilde{p}_i(1-\\tilde{p}_i) \\right]\n$$\n该表达式确定了**工作权重** $w_i = \\tilde{p}_i(1-\\tilde{p}_i)$。目标函数变为：\n$$\n\\frac{1}{2} \\sum_{i=1}^{n} w_i \\left( (\\eta_i - \\tilde{\\eta}_i)^2 + 2(\\eta_i - \\tilde{\\eta}_i) \\frac{\\tilde{p}_i - y_i}{w_i} \\right)\n$$\n对 $\\eta_i$ 进行配方，得到：\n$$\n\\frac{1}{2} \\sum_{i=1}^{n} w_i \\left( \\eta_i - \\left(\\tilde{\\eta}_i - \\frac{\\tilde{p}_i - y_i}{w_i}\\right) \\right)^2 - \\frac{1}{2} \\sum_{i=1}^n w_i \\left( \\frac{\\tilde{p}_i - y_i}{w_i} \\right)^2\n$$\n这确定了**伪响应** $z_i = \\tilde{\\eta}_i - \\frac{\\tilde{p}_i - y_i}{w_i} = \\tilde{\\eta}_i + \\frac{y_i - \\tilde{p}_i}{\\tilde{p}_i(1-\\tilde{p}_i)}$。因此，数据保真项 $L$ 在相差一个常数的情况下，可由加权最小二乘项 $\\frac{1}{2} \\sum_{i=1}^n w_i (z_i - \\eta_i)^2$ 近似。\n\n在此 IRLS 迭代步骤中需要最小化的完整目标函数是：\n$$\n\\mathcal{L}_{\\text{quad}}(\\beta_0, \\boldsymbol{\\beta}) = \\frac{1}{2} \\sum_{i=1}^{n} w_i \\left(z_i - \\beta_0 - \\sum_{k=1}^{p} x_{ik} \\beta_k\\right)^2 + \\lambda \\alpha \\sum_{k=1}^p |\\beta_k| + \\lambda \\frac{1-\\alpha}{2} \\sum_{k=1}^p \\beta_k^2\n$$\n我们使用坐标下降法求解此问题，即对单个系数 $\\beta_j$ 进行最小化，同时将所有其他系数（$\\beta_0$ 和 $k \\neq j$ 的 $\\beta_k$）固定在当前值。关于 $\\beta_j$ 的一维子问题是最小化：\n$$\nf(\\beta_j) = \\frac{1}{2} \\sum_{i=1}^{n} w_i \\left( \\left(z_i - \\beta_0 - \\sum_{k \\neq j} x_{ik} \\beta_k\\right) - x_{ij}\\beta_j \\right)^2 + \\lambda \\alpha |\\beta_j| + \\lambda \\frac{1-\\alpha}{2} \\beta_j^2\n$$\n令部分残差为 $r_{ij} = z_i - \\beta_0 - \\sum_{k \\neq j} x_{ik} \\beta_k$。目标函数简化为一维惩罚二次型：\n$$\nf(\\beta_j) = \\frac{1}{2} \\sum_{i=1}^{n} w_i (r_{ij} - x_{ij}\\beta_j)^2 + \\lambda \\alpha |\\beta_j| + \\lambda \\frac{1-\\alpha}{2} \\beta_j^2\n$$\n展开平方项并按 $\\beta_j$ 的幂次对各项进行分组：\n$$\nf(\\beta_j) = \\frac{1}{2} \\left( \\sum_{i=1}^n w_i x_{ij}^2 + \\lambda(1-\\alpha) \\right) \\beta_j^2 - \\left( \\sum_{i=1}^n w_i r_{ij} x_{ij} \\right) \\beta_j + \\lambda \\alpha |\\beta_j| + \\text{constant}\n$$\n为求最小值，我们取 $f(\\beta_j)$ 的次梯度并将其设为 $0$：\n$$\n\\partial f(\\beta_j) = \\left( \\sum_{i=1}^n w_i x_{ij}^2 + \\lambda(1-\\alpha) \\right) \\beta_j - \\left( \\sum_{i=1}^n w_i r_{ij} x_{ij} \\right) + \\lambda \\alpha \\cdot \\operatorname{sgn}(\\beta_j)\n$$\n其中 $\\operatorname{sgn}(0)$ 是区间 $[-1, 1]$。令次梯度包含 $0$ 可得：\n$$\n\\left( \\sum_{i=1}^n w_i r_{ij} x_{ij} \\right) - \\left( \\sum_{i=1}^n w_i x_{ij}^2 + \\lambda(1-\\alpha) \\right) \\beta_j \\in \\lambda \\alpha \\cdot \\operatorname{sgn}(\\beta_j)\n$$\n令 $\\hat{B}_j = \\sum_{i=1}^n w_i r_{ij} x_{ij}$ 和 $\\hat{A}_j = \\sum_{i=1}^n w_i x_{ij}^2 + \\lambda(1-\\alpha)$。条件变为 $\\hat{B}_j - \\hat{A}_j \\beta_j \\in \\lambda \\alpha \\cdot \\operatorname{sgn}(\\beta_j)$。这个问题可以通过软阈值算子求解，其定义为 $S(u, \\gamma) = \\operatorname{sgn}(u) \\max(0, |u|-\\gamma)$。\n$\\beta_j$ 的解由下式给出：\n$$\n\\beta_j^{\\text{new}} = \\frac{S(\\hat{B}_j, \\lambda \\alpha)}{\\hat{A}_j}\n$$\n代入 $\\hat{A}_j$ 和 $\\hat{B}_j$ 的表达式，并用问题陈述中指定的变量来表示 $\\hat{B}_j$，我们得到 $\\beta_j$ 的最终闭式更新法则：\n$$\n\\beta_j^{\\text{new}} = \\frac{S\\left( \\sum_{i=1}^n w_i x_{ij} \\left(z_i - \\beta_0 - \\sum_{k \\neq j} x_{ik} \\beta_k\\right), \\lambda \\alpha \\right)}{\\sum_{i=1}^n w_i x_{ij}^2 + \\lambda(1-\\alpha)}\n$$\n该表达式提供了 $\\beta_j$ 的更新值，以当前工作权重 $w_i$、伪响应 $z_i$、预测变量 $x_{ij}$、所有其他系数的当前值（$\\beta_0$ 和 $k \\neq j$ 的 $\\beta_k$）以及调整参数 $\\lambda$ 和 $\\alpha$ 表示。", "answer": "$$\n\\boxed{\\frac{\\operatorname{sgn}\\left(\\sum_{i=1}^n w_i x_{ij} \\left(z_i - \\beta_0 - \\sum_{k \\neq j} x_{ik} \\beta_k\\right)\\right) \\max\\left(0, \\left|\\sum_{i=1}^n w_i x_{ij} \\left(z_i - \\beta_0 - \\sum_{k \\neq j} x_{ik} \\beta_k\\right)\\right| - \\lambda \\alpha \\right)}{\\sum_{i=1}^n w_i x_{ij}^2 + \\lambda(1-\\alpha)}}\n$$", "id": "4961374"}]}