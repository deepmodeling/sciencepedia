{"hands_on_practices": [{"introduction": "本章的实践练习旨在加深您对岭回归理论和应用的理解。我们将从最基本的原理出发，逐步探索其更深层次的特性。第一个练习将带您回到岭回归的根本定义。通过对一个简单的单预测变量模型最小化带惩罚的目标函数，您将亲手从零开始推导岭回归估计量，从而为惩罚项 $\\lambda$ 如何影响解建立起坚实的直觉 [@problem_id:1951876]。", "problem": "在机器学习背景下，我们的任务是使用一组 $n$ 个数据点 $(x_i, y_i)$ 来拟合一个不含截距的简单线性模型 $y = \\beta x$。为防止在小数据集上过拟合，我们采用岭回归。系数 $\\beta$ 的岭估计值是指能使惩罚平方和误差（也称为目标函数 $L(\\beta)$）最小化的那个值：\n$$L(\\beta) = \\sum_{i=1}^{n} (y_i - \\beta x_i)^2 + \\lambda \\beta^2$$\n其中 $\\lambda > 0$ 是控制收缩量的正则化参数。\n\n你的任务有两部分。首先，通过最小化目标函数 $L(\\beta)$，推导出岭估计 $\\hat{\\beta}_{\\text{ridge}}$ 关于数据点 $(x_i, y_i)$ 和参数 $\\lambda$ 的通用闭式表达式。\n\n其次，将此推导出的表达式应用到一个由两个点组成的数据集：$(x_1, y_1) = (1, 3)$ 和 $(x_2, y_2) = (2, 5)$。当正则化参数 $\\lambda = 1$ 时，计算岭估计 $\\hat{\\beta}_{\\text{ridge}}$ 的数值。\n\n以精确分数的形式给出最终的数值。", "solution": "我们对不含截距的线性模型 $y=\\beta x$ 最小化其惩罚平方和误差，目标函数为\n$$\nL(\\beta)=\\sum_{i=1}^{n}(y_{i}-\\beta x_{i})^{2}+\\lambda \\beta^{2}, \\quad \\lambda>0.\n$$\n展开平方项，并合并关于 $\\beta$ 的同次幂项：\n$$\nL(\\beta)=\\sum_{i=1}^{n}\\left(y_{i}^{2}-2\\beta x_{i}y_{i}+\\beta^{2}x_{i}^{2}\\right)+\\lambda \\beta^{2}\n= \\sum_{i=1}^{n}y_{i}^{2}-2\\beta \\sum_{i=1}^{n}x_{i}y_{i}+\\beta^{2}\\sum_{i=1}^{n}x_{i}^{2}+\\lambda \\beta^{2}.\n$$\n对 $\\beta$ 求导，并将导数设为零（一阶最优性条件）：\n$$\n\\frac{dL}{d\\beta}=-2\\sum_{i=1}^{n}x_{i}y_{i}+2\\beta \\sum_{i=1}^{n}x_{i}^{2}+2\\lambda \\beta=0.\n$$\n解出 $\\beta$：\n$$\n2\\beta\\left(\\sum_{i=1}^{n}x_{i}^{2}+\\lambda\\right)=2\\sum_{i=1}^{n}x_{i}y_{i}\n\\quad \\Longrightarrow \\quad\n\\hat{\\beta}_{\\text{ridge}}=\\frac{\\sum_{i=1}^{n}x_{i}y_{i}}{\\sum_{i=1}^{n}x_{i}^{2}+\\lambda}.\n$$\n二阶导数为\n$$\n\\frac{d^{2}L}{d\\beta^{2}}=2\\sum_{i=1}^{n}x_{i}^{2}+2\\lambda>0,\n$$\n因此该解是唯一的极小值点。\n\n将此公式应用于 $(x_{1},y_{1})=(1,3)$，$(x_{2},y_{2})=(2,5)$ 且 $\\lambda=1$ 的情况：\n$$\n\\sum_{i=1}^{2}x_{i}y_{i}=1\\cdot 3+2\\cdot 5=13,\\qquad\n\\sum_{i=1}^{2}x_{i}^{2}=1^{2}+2^{2}=5.\n$$\n因此，\n$$\n\\hat{\\beta}_{\\text{ridge}}=\\frac{13}{5+1}=\\frac{13}{6}.\n$$", "answer": "$$\\boxed{\\frac{13}{6}}$$", "id": "1951876"}, {"introduction": "尽管矩阵形式提供了岭回归的“闭式解”，但在高维情况下，矩阵求逆的计算成本可能非常高昂。本练习将探索一种强大的替代方法：坐标下降法。您将推导单个系数的更新规则，这一规则是高效拟合高维岭回归模型算法的核心，从而让您深入了解现代统计学习背后的计算机制 [@problem_id:1951864]。", "problem": "一位数据科学家正在构建一个线性模型，以从一组 $p$ 个特征中预测目标变量 $y$。数据集包含 $n$ 个观测值。对于第 $i$ 个观测值，其特征由向量 $\\mathbf{x}_i \\in \\mathbb{R}^p$ 给出，观测结果为 $y_i$。线性模型的预测由 $f(\\mathbf{x}_i; \\beta) = \\sum_{j=1}^{p} x_{ij} \\beta_j$ 给出，其中 $\\beta = (\\beta_1, \\dots, \\beta_p)$ 是模型系数的向量，$x_{ij}$ 是第 $i$ 个观测值的第 $j$ 个特征的值。\n\n为了减轻过拟合并处理特征之间的多重共线性，该数据科学家选择使用岭回归 (ridge regression)。相关的目标函数 $L(\\beta)$ 必须被最小化，它由一个误差平方和项和一个 L2 正则化项组成：\n$$L(\\beta) = \\frac{1}{2} \\sum_{i=1}^{n} \\left(y_i - \\sum_{j=1}^{p} x_{ij} \\beta_j\\right)^2 + \\frac{\\lambda}{2} \\sum_{j=1}^{p} \\beta_j^2$$\n此处，$\\lambda > 0$ 是一个预先确定的正则化参数。\n\n该数据科学家决定实现一种一次更新一个系数的优化算法。在此迭代过程的单步中，一个系数（比如 $\\beta_j$）被更新为其最优值，而所有其他系数 $\\beta_k$（对于所有 $k \\neq j$）保持不变。\n\n你的任务是，在假设所有其他系数 $\\beta_k$（对于 $k \\neq j$）固定的情况下，推导出最小化目标函数 $L(\\beta)$ 的 $\\beta_j$ 值的闭式表达式。你的最终答案应该是一个用 $y_i$、$x_{ik}$、$\\lambda$ 以及固定的系数 $\\beta_k$（对于 $k \\neq j$）表示的 $\\beta_j$ 表达式。", "solution": "给定目标函数\n$$L(\\beta) = \\frac{1}{2} \\sum_{i=1}^{n} \\left(y_i - \\sum_{k=1}^{p} x_{ik} \\beta_k\\right)^{2} + \\frac{\\lambda}{2} \\sum_{k=1}^{p} \\beta_k^{2},$$\n其中 $\\lambda>0$。在坐标级更新中，所有 $k \\neq j$ 的 $\\beta_k$ 都被视为常数，我们对 $L$ 关于 $\\beta_j$ 进行最小化。\n\n使用链式法则对误差平方和项求导，并对二次惩罚项求导，从而得到 $L$ 关于 $\\beta_j$ 的导数：\n- 对于误差项，记 $r_i(\\beta) = y_i - \\sum_{k=1}^{p} x_{ik} \\beta_k$。那么\n$$\\frac{\\partial}{\\partial \\beta_j} \\left(\\frac{1}{2} \\sum_{i=1}^{n} r_i(\\beta)^{2}\\right) = \\sum_{i=1}^{n} r_i(\\beta) \\frac{\\partial r_i(\\beta)}{\\partial \\beta_j} = \\sum_{i=1}^{n} r_i(\\beta)\\,(-x_{ij}) = - \\sum_{i=1}^{n} x_{ij} \\left(y_i - \\sum_{k=1}^{p} x_{ik} \\beta_k\\right)。$$\n- 对于正则化项，\n$$\\frac{\\partial}{\\partial \\beta_j} \\left(\\frac{\\lambda}{2} \\sum_{k=1}^{p} \\beta_k^{2}\\right) = \\lambda \\beta_j.$$\n\n因此，偏导数为\n$$\\frac{\\partial L}{\\partial \\beta_j} = - \\sum_{i=1}^{n} x_{ij} \\left(y_i - \\sum_{k=1}^{p} x_{ik} \\beta_k\\right) + \\lambda \\beta_j.$$\n\n将此导数设为零，得到一阶最优性条件：\n$$- \\sum_{i=1}^{n} x_{ij} \\left(y_i - \\sum_{k=1}^{p} x_{ik} \\beta_k\\right) + \\lambda \\beta_j = 0.$$\n等价地，\n$$\\sum_{i=1}^{n} x_{ij} \\left(y_i - \\sum_{k=1}^{p} x_{ik} \\beta_k\\right) = \\lambda \\beta_j.$$\n\n展开关于 $k$ 的内层求和，并将 $k=j$ 的项与 $k \\neq j$ 的项分开：\n$$\\sum_{i=1}^{n} x_{ij} y_i - \\sum_{i=1}^{n} \\sum_{k \\neq j} x_{ij} x_{ik} \\beta_k - \\sum_{i=1}^{n} x_{ij}^{2} \\beta_j = \\lambda \\beta_j.$$\n\n将包含 $\\beta_j$ 的项收集到右侧：\n$$\\sum_{i=1}^{n} x_{ij} y_i - \\sum_{k \\neq j} \\beta_k \\sum_{i=1}^{n} x_{ij} x_{ik} = \\beta_j \\left(\\lambda + \\sum_{i=1}^{n} x_{ij}^{2}\\right)。$$\n\n解出 $\\beta_j$：\n$$\\beta_j = \\frac{\\sum_{i=1}^{n} x_{ij} y_i - \\sum_{k \\neq j} \\beta_k \\sum_{i=1}^{n} x_{ij} x_{ik}}{\\lambda + \\sum_{i=1}^{n} x_{ij}^{2}}.$$\n\n等价地，写成一种通过部分残差显式表达对固定系数的依赖关系的形式，\n$$\\beta_j = \\frac{\\sum_{i=1}^{n} x_{ij} \\left(y_i - \\sum_{k \\neq j} x_{ik} \\beta_k\\right)}{\\lambda + \\sum_{i=1}^{n} x_{ij}^{2}}.$$\n这是在所有其他系数保持不变的情况下，岭回归中 $\\beta_j$ 的闭式坐标级最小化解。", "answer": "$$\\boxed{\\frac{\\sum_{i=1}^{n} x_{ij} \\left(y_i - \\sum_{k \\neq j} x_{ik} \\beta_k\\right)}{\\lambda + \\sum_{i=1}^{n} x_{ij}^{2}}}$$", "id": "1951864"}, {"introduction": "岭回归通常被描述为将系数“收缩”到零。然而，这个练习揭示了一个更复杂的现实。通过分析一个精心构建的、包含相关预测变量的场景，您将证明一个系数的正则化路径并非总是单调地趋向于零。这个探索性的问题突显了岭回归在处理多重共线性时一个微妙但至关重要的特性，从而加深您对其行为的理解 [@problem_id:1951905]。", "problem": "考虑一个基于大小为 $n$ 的数据集的无噪声线性模型。该模型包含两个预测变量 $x_1$ 和 $x_2$，它们已经经过中心化和标准化处理，使得对于每个预测变量 $j \\in \\{1, 2\\}$，我们有 $\\sum_{i=1}^n x_{ij} = 0$ 且 $\\sum_{i=1}^n x_{ij}^2 = n$。两个预测变量之间的样本相关系数为 $\\rho = \\frac{4}{5}$。响应变量 $y$ 的真实关系由方程 $y = \\beta_1 x_1$ 给出，其中 $\\beta_1$ 是一个正常数。注意，第二个预测变量的真实系数 $\\beta_2$ 为零。\n\n一位分析师拟合了一个形式为 $y = \\hat{\\beta}_{1, \\lambda} x_1 + \\hat{\\beta}_{2, \\lambda} x_2$ 的岭回归模型。岭回归系数 $\\hat{\\beta}_{\\lambda} = (\\hat{\\beta}_{1, \\lambda}, \\hat{\\beta}_{2, \\lambda})^T$ 是对于给定的惩罚参数 $\\lambda > 0$ 最小化惩罚平方和的解：\n$$ L(\\beta_1, \\beta_2; \\lambda) = \\sum_{i=1}^n (y_i - \\beta_1 x_{i1} - \\beta_2 x_{i2})^2 + \\lambda (\\beta_1^2 + \\beta_2^2) $$\n由于预测变量之间存在相关性，对于 $\\lambda > 0$，估计值 $\\hat{\\beta}_{2, \\lambda}$ 将不为零。当 $\\lambda$ 从 0 开始增加时，$\\hat{\\beta}_{2, \\lambda}$ 的值会先增大，达到一个峰值，然后衰减回其真实值 0。\n\n求系数估计值 $\\hat{\\beta}_{2, \\lambda}$ 在所有可能的惩罚参数 $\\lambda > 0$ 上所能达到的最大值。请用 $\\beta_1$ 的函数表示你的答案。", "solution": "设 $X$ 是一个 $n \\times 2$ 的设计矩阵，其列为 $x_1$ 和 $x_2$。标准化意味着\n$$\nx_{1}^{T}x_{1}=n,\\quad x_{2}^{T}x_{2}=n,\\quad x_{1}^{T}x_{2}=x_{2}^{T}x_{1}=n\\rho,\n$$\n并且真实响应满足 $y=\\beta_{1}x_{1}$，因此\n$$\nX^{T}y=\\begin{pmatrix}x_{1}^{T}y\\\\ x_{2}^{T}y\\end{pmatrix}\n=\\begin{pmatrix}\\beta_{1}x_{1}^{T}x_{1}\\\\ \\beta_{1}x_{2}^{T}x_{1}\\end{pmatrix}\n=\\begin{pmatrix}n\\beta_{1}\\\\ n\\rho\\,\\beta_{1}\\end{pmatrix}.\n$$\n岭估计量求解 $(X^{T}X+\\lambda I)\\hat{\\beta}_{\\lambda}=X^{T}y$。使用\n$$\nX^{T}X=\\begin{pmatrix}n  n\\rho\\\\ n\\rho  n\\end{pmatrix},\\quad\nX^{T}X+\\lambda I=\\begin{pmatrix}n+\\lambda  n\\rho\\\\ n\\rho  n+\\lambda\\end{pmatrix},\n$$\n其逆矩阵为\n$$\n\\left(X^{T}X+\\lambda I\\right)^{-1}\n=\\frac{1}{(n+\\lambda)^{2}-n^{2}\\rho^{2}}\n\\begin{pmatrix}n+\\lambda  -n\\rho\\\\ -n\\rho  n+\\lambda\\end{pmatrix}.\n$$\n因此，\n$$\n\\hat{\\beta}_{\\lambda}\n=\\left(X^{T}X+\\lambda I\\right)^{-1}X^{T}y\n=\\frac{1}{(n+\\lambda)^{2}-n^{2}\\rho^{2}}\n\\begin{pmatrix}n+\\lambda  -n\\rho\\\\ -n\\rho  n+\\lambda\\end{pmatrix}\n\\begin{pmatrix}n\\beta_{1}\\\\ n\\rho\\,\\beta_{1}\\end{pmatrix}.\n$$\n提取第二个分量可得\n$$\n\\hat{\\beta}_{2,\\lambda}\n=\\frac{-n\\rho\\cdot n\\beta_{1}+(n+\\lambda)\\cdot n\\rho\\,\\beta_{1}}\n{(n+\\lambda)^{2}-n^{2}\\rho^{2}}\n=\\frac{n\\rho\\,\\beta_{1}\\lambda}{(n+\\lambda)^{2}-n^{2}\\rho^{2}}.\n$$\n令 $t=\\lambda/n>0$。则\n$$\n\\hat{\\beta}_{2,\\lambda}\n=\\beta_{1}\\,\\rho\\,\\frac{t}{(1+t)^{2}-\\rho^{2}}.\n$$\n定义 $h(t)=\\dfrac{t}{(1+t)^{2}-\\rho^{2}}$，其中 $t>0$。使用商法则求导。设 $d(t)=(1+t)^{2}-\\rho^{2}$ 且 $d'(t)=2(1+t)$，\n$$\nh'(t)=\\frac{d(t)-t\\,d'(t)}{d(t)^{2}}\n=\\frac{(1+t)^{2}-\\rho^{2}-t\\cdot 2(1+t)}{d(t)^{2}}\n=\\frac{1-\\rho^{2}-t^{2}}{d(t)^{2}}.\n$$\n令 $h'(t)=0$ 得到 $t^{2}=1-\\rho^{2}$，又因为 $t>0$，我们得到最大值点\n$$\nt^{*}=\\sqrt{1-\\rho^{2}}.\n$$\n计算 $h$ 在 $t^{*}$ 处的值。记 $s=\\sqrt{1-\\rho^{2}}$，所以\n$$\nh(t^{*})=\\frac{s}{(1+s)^{2}-\\rho^{2}}\n=\\frac{s}{1+2s+s^{2}-\\rho^{2}}\n=\\frac{s}{2s+2s^{2}}\n=\\frac{1}{2(1+s)}.\n$$\n因此 $\\hat{\\beta}_{2,\\lambda}$ 在 $\\lambda>0$ 上的最大值为\n$$\n\\max_{\\lambda>0}\\hat{\\beta}_{2,\\lambda}\n=\\beta_{1}\\,\\rho\\,\\frac{1}{2\\left(1+\\sqrt{1-\\rho^{2}}\\right)}.\n$$\n给定 $\\rho=\\frac{4}{5}$，我们有 $\\sqrt{1-\\rho^{2}}=\\sqrt{\\frac{9}{25}}=\\frac{3}{5}$，因此\n$$\n\\max_{\\lambda>0}\\hat{\\beta}_{2,\\lambda}\n=\\beta_{1}\\cdot\\frac{4}{5}\\cdot\\frac{1}{2\\left(1+\\frac{3}{5}\\right)}\n=\\beta_{1}\\cdot\\frac{4}{5}\\cdot\\frac{1}{2\\cdot\\frac{8}{5}}\n=\\beta_{1}\\cdot\\frac{4}{16}\n=\\frac{\\beta_{1}}{4}.\n$$", "answer": "$$\\boxed{\\frac{\\beta_{1}}{4}}$$", "id": "1951905"}]}