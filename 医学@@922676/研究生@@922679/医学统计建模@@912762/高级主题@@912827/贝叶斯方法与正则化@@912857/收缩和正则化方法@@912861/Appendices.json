{"hands_on_practices": [{"introduction": "要真正掌握一种方法，最好的方式是从头开始构建它。本练习将剥离复杂的矩阵代数，在一个简化的正交设定（即 $X^{\\top}X$ 为对角阵）下，揭示岭回归（Ridge Regression）的核心收缩机制。通过从其目标函数 $\\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{2}^{2}$ 出发推导解析解，您将清楚地看到正则化参数 $\\lambda$ 如何系统性地将普通最小二乘（OLS）估计的系数向零收缩，这是理解所有惩罚方法的基础 [@problem_id:4983813]。", "problem": "一位生物统计学家正在为连续结果 $y$（例如，经对数转换的住院时长）构建一个线性预后模型，该模型使用了从 $n$ 名患者身上收集的 $p$ 个经过中心化和标准化的生物标志物预测变量。设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，响应向量为 $y \\in \\mathbb{R}^{n}$，并且由于所有变量都已中心化，因此没有截距。经过正交化步骤后，研究者排列了预测变量，使得经验互积矩阵满足 $X^{\\top} X = \\mathrm{diag}(d_{1},\\dots,d_{p})$，其中对于所有 $j \\in \\{1,\\dots,p\\}$ 都有 $d_{j} \\in \\mathbb{R}_{>0}$。定义 $z \\equiv X^{\\top} y \\in \\mathbb{R}^{p}$，其分量为 $z_{j}$。\n\n考虑使用调整参数 $\\lambda \\in \\mathbb{R}_{>0}$ 的岭回归，其定义为惩罚最小二乘准则\n$$\nJ(\\beta) \\equiv \\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{2}^{2}, \\quad \\beta \\in \\mathbb{R}^{p}.\n$$\n仅从此定义和给定的结构 $X^{\\top} X = \\mathrm{diag}(d_{1},\\dots,d_{p})$ 出发，计算岭回归的闭式解 $\\hat{\\beta}_{\\mathrm{ridge}}$，并推导其分量表示，以揭示相对于普通最小二乘（OLS）估计量，乘性收缩因子是如何产生的。将你的最终答案表示为关于 $D \\equiv \\mathrm{diag}(d_{1},\\dots,d_{p})$、$\\lambda$ 和 $z \\equiv X^{\\top} y$ 的单一闭式解析表达式。不需要进行数值计算。", "solution": "问题陈述经评估有效。它在科学上基于统计学习理论的原理，是适定的，具有唯一且稳定的解，并以客观的数学语言表达。因此，我们可以进行推导。\n\n岭回归估计量 $\\hat{\\beta}_{\\mathrm{ridge}}$ 定义为使惩罚最小二乘目标函数 $J(\\beta)$ 最小化的向量 $\\beta \\in \\mathbb{R}^{p}$：\n$$\nJ(\\beta) = \\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{2}^{2}\n$$\n其中 $y \\in \\mathbb{R}^{n}$ 是响应向量，$X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$\\beta \\in \\mathbb{R}^{p}$ 是系数向量，$\\lambda \\in \\mathbb{R}_{>0}$ 是调整参数。\n\n为求最小值，我们首先使用矩阵代数展开目标函数。欧几里得范数的平方可以写成点积：\n$$\n\\|v\\|_{2}^{2} = v^{\\top}v\n$$\n应用此法则，我们得到：\n$$\nJ(\\beta) = (y - X \\beta)^{\\top}(y - X \\beta) + \\lambda \\beta^{\\top}\\beta\n$$\n展开第一项：\n$$\nJ(\\beta) = (y^{\\top} - \\beta^{\\top}X^{\\top})(y - X \\beta) + \\lambda \\beta^{\\top}\\beta\n$$\n$$\nJ(\\beta) = y^{\\top}y - y^{\\top}X\\beta - \\beta^{\\top}X^{\\top}y + \\beta^{\\top}X^{\\top}X\\beta + \\lambda \\beta^{\\top}\\beta\n$$\n由于 $\\beta^{\\top}X^{\\top}y$ 是一个标量（$1 \\times 1$ 矩阵），它等于其转置，即 $( \\beta^{\\top}X^{\\top}y )^{\\top} = y^{\\top}X\\beta$。因此，我们可以合并两个交叉乘积项：\n$$\nJ(\\beta) = y^{\\top}y - 2\\beta^{\\top}X^{\\top}y + \\beta^{\\top}X^{\\top}X\\beta + \\lambda \\beta^{\\top}\\beta\n$$\n函数 $J(\\beta)$ 对于 $\\beta$ 是凸函数。对于 $\\lambda > 0$，项 $\\lambda \\beta^{\\top}\\beta$ 是严格凸的，使得 $J(\\beta)$ 是严格凸的。因此，存在唯一的最小值，可以通过对 $\\beta$ 求梯度并将其设为零来找到。使用矩阵微积分的标准法则（对于对称矩阵 $A$，有 $\\nabla_{\\beta}(\\beta^{\\top}a) = a$ 和 $\\nabla_{\\beta}(\\beta^{\\top}A\\beta) = 2A\\beta$），我们计算梯度：\n$$\n\\nabla_{\\beta} J(\\beta) = \\nabla_{\\beta} (y^{\\top}y - 2\\beta^{\\top}X^{\\top}y + \\beta^{\\top}(X^{\\top}X + \\lambda I)\\beta)\n$$\n$$\n\\nabla_{\\beta} J(\\beta) = 0 - 2X^{\\top}y + 2(X^{\\top}X + \\lambda I)\\beta\n$$\n其中 $I$ 是 $p \\times p$ 的单位矩阵。将梯度设为零向量以找到最优的 $\\hat{\\beta}_{\\mathrm{ridge}}$：\n$$\n-2X^{\\top}y + 2(X^{\\top}X + \\lambda I)\\hat{\\beta}_{\\mathrm{ridge}} = 0\n$$\n$$\n(X^{\\top}X + \\lambda I)\\hat{\\beta}_{\\mathrm{ridge}} = X^{\\top}y\n$$\n这给出了岭估计量的一般闭式解：\n$$\n\\hat{\\beta}_{\\mathrm{ridge}} = (X^{\\top}X + \\lambda I)^{-1} X^{\\top}y\n$$\n当 $\\lambda > 0$ 时，矩阵 $(X^{\\top}X + \\lambda I)$ 保证是可逆的，因为 $X^{\\top}X$ 是半正定的，而 $\\lambda I$ 是正定的，使其和为正定矩阵。\n\n现在，我们引入问题陈述中提供的特定结构。给定预测变量的排列使得 $X^{\\top}X = D$，其中 $D = \\mathrm{diag}(d_{1}, \\dots, d_{p})$ 且对于所有 $j \\in \\{1,\\dots,p\\}$ 都有 $d_{j} > 0$。我们还给定了定义 $z = X^{\\top}y$。将这些代入一般解中得到：\n$$\n\\hat{\\beta}_{\\mathrm{ridge}} = (D + \\lambda I)^{-1} z\n$$\n这就是用指定量表示的闭式解。\n\n为了揭示分量收缩，我们首先分析 $(D + \\lambda I)^{-1}$ 这一项。两个对角矩阵的和是一个对角矩阵：\n$$\nD + \\lambda I = \\mathrm{diag}(d_{1}, \\dots, d_{p}) + \\mathrm{diag}(\\lambda, \\dots, \\lambda) = \\mathrm{diag}(d_{1}+\\lambda, \\dots, d_{p}+\\lambda)\n$$\n一个对角矩阵的逆是另一个对角矩阵，其对角元素是原对角元素的倒数：\n$$\n(D + \\lambda I)^{-1} = \\mathrm{diag}\\left(\\frac{1}{d_{1}+\\lambda}, \\dots, \\frac{1}{d_{p}+\\lambda}\\right)\n$$\n因此，岭回归解向量为：\n$$\n\\hat{\\beta}_{\\mathrm{ridge}} = \\mathrm{diag}\\left(\\frac{1}{d_{1}+\\lambda}, \\dots, \\frac{1}{d_{p}+\\lambda}\\right) z\n$$\n$\\hat{\\beta}_{\\mathrm{ridge}}$ 的第 $j$ 个分量，记作 $\\hat{\\beta}_{\\mathrm{ridge}, j}$，则为：\n$$\n\\hat{\\beta}_{\\mathrm{ridge}, j} = \\frac{z_{j}}{d_{j}+\\lambda}\n$$\n为了将其理解为一种收缩机制，我们将其与普通最小二乘（OLS）估计量 $\\hat{\\beta}_{\\mathrm{ols}}$ 进行比较。OLS 估计量是通过求解正规方程 $X^{\\top}X\\beta = X^{\\top}y$ 得到的。使用我们的特定结构，这变为 $D\\hat{\\beta}_{\\mathrm{ols}} = z$。解为 $\\hat{\\beta}_{\\mathrm{ols}} = D^{-1}z$，其第 $j$ 个分量是：\n$$\n\\hat{\\beta}_{\\mathrm{ols}, j} = \\frac{z_{j}}{d_{j}}\n$$\n现在，我们可以用 OLS 分量来表示岭回归的分量：\n$$\n\\hat{\\beta}_{\\mathrm{ridge}, j} = \\frac{z_{j}}{d_{j}+\\lambda} = \\left(\\frac{d_{j}}{d_{j}+\\lambda}\\right) \\frac{z_{j}}{d_{j}} = \\left(\\frac{d_{j}}{d_{j}+\\lambda}\\right)\\hat{\\beta}_{\\mathrm{ols}, j}\n$$\n项 $s_j = \\frac{d_{j}}{d_{j}+\\lambda}$ 是第 $j$ 个分量的乘性收缩因子。由于 $d_{j} > 0$ 且 $\\lambda > 0$，我们有 $d_{j}  d_{j}+\\lambda$，这意味着 $0  s_j  1$。这表明岭估计量的每个分量都是相应 OLS 估计量分量的缩小版本，岭估计值被“收缩”向零。收缩的程度取决于预测变量对应的对角元素 $d_j$ 和全局调整参数 $\\lambda$。\n\n以 $D$、$\\lambda$ 和 $z$ 表示的岭回归解向量的最终闭式表达式是在分量分析之前推导出的矩阵表达式。", "answer": "$$\\boxed{(D + \\lambda I)^{-1} z}$$", "id": "4983813"}, {"introduction": "在掌握了岭回归的“如何收缩”之后，我们来探讨“为何要收缩”这一关键问题。在医学研究中，多重共线性是一个常见问题，它会导致普通最小二乘法中 $X^{\\top}X$ 矩阵的病态（ill-conditioned），使其估计变得极不稳定。本练习将从线性代数的角度，通过分析 $X^{\\top}X + \\lambda I$ 矩阵的特征值和条件数 $\\kappa_2$，定量地展示岭回归如何通过改善问题的数值稳定性来“驯服”共线性 [@problem_id:4983782]。", "problem": "一个生物统计团队正在拟合一个多变量线性模型，使用 $p=4$ 个标准化的实验室测量指标（设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 的列）来预测一个与肾功能相关的连续结果。已知这些指标由于共享的生理途径而高度共线性。该团队考虑用岭回归替代普通最小二乘法，以稳定系数估计。假设 $X$ 的列已经中心化并缩放到单位方差，并且假设对称矩阵 $X^{\\top}X$ 的特征值为 $\\{2000, 300, 1.5, 0.02\\}$，这反映了由共线性引起的近乎简并性。令 $I_p$ 表示 $p \\times p$ 单位矩阵。\n\n从岭回归的惩罚最小二乘目标函数出发，\n$$\nL(\\beta) \\;=\\; \\|y - X\\beta\\|_2^2 \\;+\\; \\lambda \\|\\beta\\|_2^2,\n$$\n并利用关于对称矩阵的谱分解以及矩阵2-范数条件数的基本事实，完成以下任务：\n\n1. 推导与最小化 $L(\\beta)$ 相关的正规方程，并利用谱分解 $X^{\\top}X = Q \\Lambda Q^{\\top}$（其中 $Q$ 是正交矩阵，$\\Lambda$ 是对角矩阵），确定线性系统矩阵的特征值如何被岭惩罚项 $\\lambda$ 改变。\n\n2. 使用对称半正定矩阵 $M$ 的矩阵2-范数条件数的定义 $\\kappa_2(M) = \\sigma_{\\max}(M) / \\sigma_{\\min}(M)$，将岭正则化后的条件数 $\\kappa_2\\!\\left(X^{\\top}X + \\lambda I_p\\right)$ 用 $X^{\\top}X$ 的最大和最小特征值以及 $\\lambda$ 来表示。\n\n3. 计算最小的非负 $\\lambda$，使得 $X^{\\top}X + \\lambda I_p$ 的条件数至多为 $50$。\n\n将 $\\lambda$ 的最终数值答案四舍五入到四位有效数字。只报告 $\\lambda$ 的值。", "solution": "该问题是有效的，因为它在科学上基于统计学习理论，提法明确，客观，并且包含得出唯一解所需的所有信息。\n\n解答过程根据题目要求分为三个部分。\n\n### 第1部分：正规方程的推导与特征值分析\n\n岭回归的目标函数为：\n$$\nL(\\beta) \\;=\\; \\|y - X\\beta\\|_2^2 \\;+\\; \\lambda \\|\\beta\\|_2^2\n$$\n为了找到最小化该函数的系数向量 $\\beta$，我们必须求出 $L(\\beta)$ 关于 $\\beta$ 的梯度并将其设为零。首先，我们展开平方欧几里得范数：\n$$\nL(\\beta) \\;=\\; (y - X\\beta)^{\\top}(y - X\\beta) \\;+\\; \\lambda \\beta^{\\top}\\beta\n$$\n$$\nL(\\beta) \\;=\\; (y^{\\top} - \\beta^{\\top}X^{\\top})(y - X\\beta) \\;+\\; \\lambda \\beta^{\\top}\\beta\n$$\n$$\nL(\\beta) \\;=\\; y^{\\top}y - y^{\\top}X\\beta - \\beta^{\\top}X^{\\top}y + \\beta^{\\top}X^{\\top}X\\beta \\;+\\; \\lambda \\beta^{\\top}\\beta\n$$\n由于 $\\beta^{\\top}X^{\\top}y$ 是一个标量，它等于其转置 $( \\beta^{\\top}X^{\\top}y )^{\\top} = y^{\\top}X\\beta$。因此，我们可以合并中间两项：\n$$\nL(\\beta) \\;=\\; y^{\\top}y - 2\\beta^{\\top}X^{\\top}y + \\beta^{\\top}(X^{\\top}X + \\lambda I_p)\\beta\n$$\n现在，我们计算 $L(\\beta)$ 关于向量 $\\beta$ 的梯度：\n$$\n\\frac{\\partial L(\\beta)}{\\partial \\beta} \\;=\\; -2X^{\\top}y + 2(X^{\\top}X + \\lambda I_p)\\beta\n$$\n将梯度设为零以求最小值：\n$$\n-2X^{\\top}y + 2(X^{\\top}X + \\lambda I_p)\\beta \\;=\\; 0\n$$\n$$\n(X^{\\top}X + \\lambda I_p)\\beta \\;=\\; X^{\\top}y\n$$\n这些就是岭回归的正规方程。线性系统矩阵为 $M_{\\lambda} = X^{\\top}X + \\lambda I_p$。\n\n为了确定特征值如何被改变，我们使用给定的谱分解 $X^{\\top}X = Q \\Lambda Q^{\\top}$，其中 $Q$ 是正交矩阵（$Q Q^{\\top} = Q^{\\top}Q = I_p$），$\\Lambda$ 是一个对角矩阵，其对角线上的元素是 $X^{\\top}X$ 的特征值。设这些特征值为 $\\sigma_i$，$i=1, \\dots, p$。\n我们可以将系统矩阵 $M_{\\lambda}$ 重写为：\n$$\nM_{\\lambda} \\;=\\; X^{\\top}X + \\lambda I_p \\;=\\; Q \\Lambda Q^{\\top} + \\lambda Q Q^{\\top} \\;=\\; Q(\\Lambda + \\lambda I_p)Q^{\\top}\n$$\n这是 $M_{\\lambda}$ 的谱分解。$M_{\\lambda}$ 的特征值是矩阵 $\\Lambda + \\lambda I_p$ 的对角元素。如果 $X^{\\top}X$ 的特征值是 $\\{\\sigma_1, \\sigma_2, \\dots, \\sigma_p\\}$，那么 $M_{\\lambda} = X^{\\top}X + \\lambda I_p$ 的特征值就是 $\\{\\sigma_1 + \\lambda, \\sigma_2 + \\lambda, \\dots, \\sigma_p + \\lambda\\}$。因此，岭惩罚项 $\\lambda$ 对矩阵 $X^{\\top}X$ 的每个特征值都增加了一个常数位移。\n\n### 第2部分：岭正则化后的条件数\n\n题目将对称半正定矩阵 $M$ 的矩阵2-范数条件数定义为 $\\kappa_2(M) = \\sigma_{\\max}(M) / \\sigma_{\\min}(M)$，其中 $\\sigma_{\\max}(M)$ 和 $\\sigma_{\\min}(M)$ 分别是 $M$ 的最大和最小奇异值。对于对称半正定矩阵，奇异值与特征值相同。\n所讨论的矩阵是 $M_{\\lambda} = X^{\\top}X + \\lambda I_p$。根据第1部分，其特征值为 $\\sigma_i + \\lambda$，其中 $\\sigma_i$ 是 $X^{\\top}X$ 的特征值。\n令 $\\sigma_{\\max}(X^{\\top}X)$ 和 $\\sigma_{\\min}(X^{\\top}X)$ 分别为 $X^{\\top}X$ 的最大和最小特征值。由于 $\\lambda \\ge 0$，因此 $X^{\\top}X + \\lambda I_p$ 的最大和最小特征值为：\n$$\n\\sigma_{\\max}(X^{\\top}X + \\lambda I_p) \\;=\\; \\max_{i}(\\sigma_i + \\lambda) \\;=\\; \\sigma_{\\max}(X^{\\top}X) + \\lambda\n$$\n$$\n\\sigma_{\\min}(X^{\\top}X + \\lambda I_p) \\;=\\; \\min_{i}(\\sigma_i + \\lambda) \\;=\\; \\sigma_{\\min}(X^{\\top}X) + \\lambda\n$$\n因此，岭正则化后的条件数为：\n$$\n\\kappa_2(X^{\\top}X + \\lambda I_p) \\;=\\; \\frac{\\sigma_{\\max}(X^{\\top}X + \\lambda I_p)}{\\sigma_{\\min}(X^{\\top}X + \\lambda I_p)} \\;=\\; \\frac{\\sigma_{\\max}(X^{\\top}X) + \\lambda}{\\sigma_{\\min}(X^{\\top}X) + \\lambda}\n$$\n\n### 第3部分：$\\lambda$ 的计算\n\n我们已知 $p=4$ 的矩阵 $X^{\\top}X$ 的特征值为 $\\{2000, 300, 1.5, 0.02\\}$。其中最大和最小的特征值是：\n$$\n\\sigma_{\\max}(X^{\\top}X) = 2000\n$$\n$$\n\\sigma_{\\min}(X^{\\top}X) = 0.02\n$$\n题目要求找到最小的非负 $\\lambda$，使得 $X^{\\top}X + \\lambda I_p$ 的条件数至多为 $50$。使用第2部分的公式：\n$$\n\\kappa_2(X^{\\top}X + \\lambda I_p) \\;=\\; \\frac{2000 + \\lambda}{0.02 + \\lambda} \\le 50\n$$\n为了解出 $\\lambda$，我们整理该不等式。由于 $\\lambda \\ge 0$，分母 $(0.02 + \\lambda)$ 恒为正，因此我们可以在不等式两边同乘以它而不改变不等号的方向：\n$$\n2000 + \\lambda \\le 50(0.02 + \\lambda)\n$$\n$$\n2000 + \\lambda \\le 1 + 50\\lambda\n$$\n现在，我们分离出 $\\lambda$：\n$$\n2000 - 1 \\le 50\\lambda - \\lambda\n$$\n$$\n1999 \\le 49\\lambda\n$$\n$$\n\\lambda \\ge \\frac{1999}{49}\n$$\n满足此条件的最小 $\\lambda$ 值为 $\\lambda = \\frac{1999}{49}$。为了给出数值答案，我们计算这个值：\n$$\n\\lambda = \\frac{1999}{49} \\approx 40.795918367...\n$$\n题目要求将结果四舍五入到四位有效数字。前四位有效数字是 $40.79$。第五位有效数字是 $5$，需要将最后一位数字向上取整。因此，$40.79$ 四舍五入为 $40.80$。", "answer": "$$\\boxed{40.80}$$", "id": "4983782"}, {"introduction": "岭回归有效处理共线性，但它会将所有预测变量保留在模型中。LASSO 回归通过其 $\\ell_1$ 惩罚项 $\\lambda \\sum_j |\\beta_j|$ 提供了变量选择的能力，但在处理相关预测变量组时，它往往会任意选择其中一个。本练习通过一个理想化的思想实验，对比 LASSO 和弹性网络（Elastic Net）在面对完全相关预测变量（即 $x_1 = x_2$）时的不同行为，从而揭示弹性网络著名的“分组效应” [@problem_id:4983817]。", "problem": "一个用于医院再入院的临床风险模型使用了两个高通量测量值，这两个测量值由于一个预处理伪影而完全共线：两个预测变量是相同的、经过中心化和标准化的列。假设收集了 $n \\in \\mathbb{N}$ 个观测值，设计矩阵有两列 $x_{1} \\in \\mathbb{R}^{n}$ 和 $x_{2} \\in \\mathbb{R}^{n}$，满足 $x_{1} = x_{2} = x$ 且 $(1/n)\\,x^{\\top} x = 1$。响应是一个中心化的连续变量 $y \\in \\mathbb{R}^{n}$，它与重复的信号成正比，即 $y = \\theta\\,x$，其中信号水平 $\\theta \\in \\mathbb{R}$ 未知，并假设 $\\theta  0$。考虑系数向量 $\\beta = (\\beta_{1},\\beta_{2})$ 的惩罚最小二乘估计量。\n\n将最小绝对收缩和选择算子 (LASSO) 估计量定义为下式的最小化子：\n$$\n\\frac{1}{2n}\\,\\|y - X\\beta\\|^{2} + \\lambda \\left(|\\beta_{1}| + |\\beta_{2}|\\right),\n$$\n其中惩罚水平 $\\lambda  0$，$X = [x_{1}\\;x_{2}]$。\n\n将混合参数为 $\\alpha \\in (0,1)$ 的弹性网络 (Elastic Net, EN) 估计量定义为下式的最小化子：\n$$\n\\frac{1}{2n}\\,\\|y - X\\beta\\|^{2} + \\lambda \\alpha \\left(|\\beta_{1}| + |\\beta_{2}|\\right) + \\frac{\\lambda(1-\\alpha)}{2}\\left(\\beta_{1}^{2} + \\beta_{2}^{2}\\right).\n$$\n\n对于一个系数向量 $\\beta$，定义分组效应度量\n$$\nG(\\beta) \\equiv \\frac{|\\beta_{1} - \\beta_{2}|}{|\\beta_{1} + \\beta_{2}|},\n$$\n只考虑 $|\\beta_{1} + \\beta_{2}|  0$ 的情况。\n\n在条件 $\\theta  \\lambda$ 和 $\\theta  \\lambda \\alpha$ 下，完成以下任务：\n\n- 从两个惩罚估计量的定义出发，利用 $x_{1} = x_{2}$ 和 $(1/n)\\,x^{\\top}x = 1$，将每个优化问题简化为关于和 $s \\equiv \\beta_{1} + \\beta_{2}$ 的一维问题，并确定相应的最优和 $s_{\\text{LASSO}}^{\\star}$ 和 $s_{\\text{EN}}^{\\star}$。\n\n- 对于 LASSO，论证为什么其最小化子不唯一，并且存在一个系数设置为零的最小化解，例如 $(\\hat{\\beta}_{1},\\hat{\\beta}_{2}) = (s_{\\text{LASSO}}^{\\star},0)$。对于 EN，证明在固定和为 $s$ 的所有划分中，通过均分可以使惩罚项最小化，即 $(\\hat{\\beta}_{1},\\hat{\\beta}_{2}) = \\left(s_{\\text{EN}}^{\\star}/2,\\,s_{\\text{EN}}^{\\star}/2\\right)$。\n\n- 对于具有一个零系数的极端 LASSO 最小化子，计算 $G_{\\text{LASSO}} \\equiv G(\\hat{\\beta}_{\\text{LASSO}})$；对于具有相等系数的 EN 最小化子，计算 $G_{\\text{EN}} \\equiv G(\\hat{\\beta}_{\\text{EN}})$。\n\n报告单个标量差值\n$$\n\\Delta \\equiv G_{\\text{LASSO}} - G_{\\text{EN}}.\n$$\n给出最终答案的精确值，无需四舍五入。不需要单位。", "solution": "首先验证问题，发现其是良定的、有科学依据且内部一致的。我们可以继续进行推导。\n\n问题的核心在于最小化两个不同的惩罚最小二乘目标函数。两者共有的一个项是缩放的残差平方和 (RSS)，我们首先对其进行简化。\n设计矩阵为 $X = [x_{1}\\;x_{2}]$。鉴于 $x_{1} = x_{2} = x$，预测值为 $X\\beta = x_{1}\\beta_{1} + x_{2}\\beta_{2} = x\\beta_{1} + x\\beta_{2} = x(\\beta_{1} + \\beta_{2})$。\n令 $s \\equiv \\beta_{1} + \\beta_{2}$。则预测值为 $xs$。\n响应由 $y = \\theta x$ 给出。\nRSS 项为 $\\|y - X\\beta\\|^{2} = \\|\\theta x - xs\\|^{2} = \\|(\\theta-s)x\\|^{2} = (\\theta-s)^{2}\\|x\\|^{2}$。\n问题给出了归一化条件 $(1/n)\\,x^{\\top} x = 1$。由于 $x^{\\top} x = \\|x\\|^{2}$，这意味着 $\\|x\\|^{2} = n$。\n将此代入缩放的 RSS 项，得到：\n$$\n\\frac{1}{2n}\\,\\|y - X\\beta\\|^{2} = \\frac{1}{2n}(\\theta-s)^{2}n = \\frac{1}{2}(\\theta-s)^{2}\n$$\n这个简化的损失项仅通过它们的和 $s$ 来依赖于 $\\beta_{1}$ 和 $\\beta_{2}$。\n\n首先，我们分析最小绝对收缩和选择算子 (LASSO) 估计量。要最小化的目标函数是：\n$$\nL_{\\text{LASSO}}(\\beta_{1}, \\beta_{2}) = \\frac{1}{2}(\\theta-s)^{2} + \\lambda(|\\beta_{1}| + |\\beta_{2}|)\n$$\n为了最小化此函数，我们可以采用一个两阶段方法。对于一个固定的和 $s = \\beta_{1} + \\beta_{2}$，我们首先相对于 $s$ 的所有可能划分来最小化惩罚项 $\\lambda (|\\beta_{1}| + |\\beta_{2}|)$。对于固定的 $s$，RSS 项 $\\frac{1}{2}(\\theta-s)^{2}$ 是一个常数。\n在约束 $\\beta_{1} + \\beta_{2} = s$ 下，$L_{1}$ 范数 $|\\beta_{1}| + |\\beta_{2}|$ 的最小值是 $|s|$。这个最小值当且仅当 $\\beta_{1}$ 和 $\\beta_{2}$ 符号相同或其中一个为零时达到。这意味着对于一个给定的最优和 $s_{\\text{LASSO}}^{\\star}$，$(\\beta_{1}, \\beta_{2})$ 的解是不唯一的。\n因此，LASSO 的优化问题简化为关于 $s$ 的一维问题：\n$$\n\\min_{s \\in \\mathbb{R}} \\left\\{ \\frac{1}{2}(\\theta-s)^{2} + \\lambda|s| \\right\\}\n$$\n这是一个标准问题，其解由软阈值算子给出。最优和 $s_{\\text{LASSO}}^{\\star}$ 是：\n$$\ns_{\\text{LASSO}}^{\\star} = \\operatorname{sign}(\\theta)(|\\theta|-\\lambda)_{+}\n$$\n考虑到问题条件 $\\theta  0$ 和 $\\theta  \\lambda$，这可以简化为：\n$$\ns_{\\text{LASSO}}^{\\star} = (\\theta-\\lambda)_{+} = \\theta - \\lambda\n$$\n如前所述，任何满足 $\\hat{\\beta}_{1} \\ge 0$、$\\hat{\\beta}_{2} \\ge 0$ 和 $\\hat{\\beta}_{1} + \\hat{\\beta}_{2} = s_{\\text{LASSO}}^{\\star} = \\theta - \\lambda$ 的对 $(\\hat{\\beta}_{1}, \\hat{\\beta}_{2})$ 都是一个有效的 LASSO 最小化子。问题要求我们考虑一个特定的“极端”最小化子 $\\hat{\\beta}_{\\text{LASSO}}$，其中一个系数为零，比如说 $\\hat{\\beta}_{\\text{LASSO}} = (s_{\\text{LASSO}}^{\\star}, 0) = (\\theta - \\lambda, 0)$。\n我们为此解计算分组效应度量 $G_{\\text{LASSO}}$。由于 $\\theta  \\lambda$，我们有 $|\\hat{\\beta}_{1} + \\hat{\\beta}_{2}| = |\\theta-\\lambda|  0$，因此 $G$ 是良定义的。\n$$\nG_{\\text{LASSO}} = G(\\hat{\\beta}_{\\text{LASSO}}) = \\frac{|\\hat{\\beta}_{1} - \\hat{\\beta}_{2}|}{|\\hat{\\beta}_{1} + \\hat{\\beta}_{2}|} = \\frac{|(\\theta-\\lambda) - 0|}{|(\\theta-\\lambda)+0|} = \\frac{|\\theta-\\lambda|}{|\\theta-\\lambda|} = 1\n$$\n\n接下来，我们分析弹性网络 (EN) 估计量。目标函数是：\n$$\nL_{\\text{EN}}(\\beta_{1}, \\beta_{2}) = \\frac{1}{2}(\\theta-s)^{2} + \\lambda \\alpha (|\\beta_{1}| + |\\beta_{2}|) + \\frac{\\lambda(1-\\alpha)}{2}(\\beta_{1}^{2} + \\beta_{2}^{2})\n$$\n同样，对于一个固定的和 $s = \\beta_{1}+\\beta_{2}$，我们最小化惩罚部分。假设最优的 $s  0$（我们稍后会验证这一点），任何最优解都将有 $\\beta_{1}, \\beta_{2} \\ge 0$。在这种情况下，$|\\beta_{1}| + |\\beta_{2}| = \\beta_{1} + \\beta_{2} = s$。惩罚项的$L_{1}$部分 $\\lambda \\alpha s$ 对于固定的 $s$ 成为常数。任务简化为在约束 $\\beta_{1}+\\beta_{2}=s$ 和 $\\beta_{1}, \\beta_{2} \\ge 0$ 下最小化惩罚项的$L_{2}$部分 $\\frac{\\lambda(1-\\alpha)}{2}(\\beta_{1}^{2} + \\beta_{2}^{2})$。\n我们需要最小化 $\\beta_{1}^{2} + \\beta_{2}^{2} = \\beta_{1}^{2} + (s-\\beta_{1})^{2}$。令 $f(\\beta_{1}) = 2\\beta_{1}^{2} - 2s\\beta_{1} + s^{2}$。这是一个凸抛物线，其最小值在其导数为零处取得：$f'(\\beta_{1}) = 4\\beta_{1} - 2s = 0$，这得出 $\\beta_{1} = s/2$。因此，$\\beta_{2}=s/2$。平方$L_{2}$范数的严格凸性迫使系数相等，这展示了 EN 的分组效应。\n\n对于最优划分 $(\\beta_{1}, \\beta_{2}) = (s/2, s/2)$，EN 目标函数成为一个仅关于 $s$ 的函数：\n$$\nL_{\\text{EN}}(s) = \\frac{1}{2}(\\theta-s)^{2} + \\lambda \\alpha |s| + \\frac{\\lambda(1-\\alpha)}{2}\\left(\\left(\\frac{s}{2}\\right)^{2} + \\left(\\frac{s}{2}\\right)^{2}\\right) = \\frac{1}{2}(\\theta-s)^{2} + \\lambda \\alpha |s| + \\frac{\\lambda(1-\\alpha)}{4}s^{2}\n$$\n我们通过最小化该函数来找到最优和 $s_{\\text{EN}}^{\\star}$。给定 $\\theta  \\lambda\\alpha  0$，我们可以假设 $s  0$。对 $s$ 求导并令其为零：\n$$\n\\frac{d}{ds}L_{\\text{EN}}(s) = -(\\theta-s) + \\lambda\\alpha + \\frac{\\lambda(1-\\alpha)}{2}s = 0\n$$\n$$\ns\\left(1 + \\frac{\\lambda(1-\\alpha)}{2}\\right) = \\theta - \\lambda\\alpha \\implies s_{\\text{EN}}^{\\star} = \\frac{\\theta - \\lambda\\alpha}{1 + \\frac{\\lambda(1-\\alpha)}{2}}\n$$\n条件 $\\theta  \\lambda\\alpha$ 确保了 $s_{\\text{EN}}^{\\star}  0$，这与我们的假设一致。\nEN 的估计系数为 $\\hat{\\beta}_{\\text{EN}} = (\\hat{\\beta}_{1}, \\hat{\\beta}_{2}) = (s_{\\text{EN}}^{\\star}/2, s_{\\text{EN}}^{\\star}/2)$。\n我们为此解计算分组效应 $G_{\\text{EN}}$。分母 $|\\hat{\\beta}_{1} + \\hat{\\beta}_{2}| = |s_{\\text{EN}}^{\\star}|$ 非零。\n$$\nG_{\\text{EN}} = G(\\hat{\\beta}_{\\text{EN}}) = \\frac{|\\hat{\\beta}_{1} - \\hat{\\beta}_{2}|}{|\\hat{\\beta}_{1} + \\hat{\\beta}_{2}|} = \\frac{|s_{\\text{EN}}^{\\star}/2 - s_{\\text{EN}}^{\\star}/2|}{|s_{\\text{EN}}^{\\star}/2 + s_{\\text{EN}}^{\\star}/2|} = \\frac{0}{|s_{\\text{EN}}^{\\star}|} = 0\n$$\n\n最后，我们计算所求的差值 $\\Delta = G_{\\text{LASSO}} - G_{\\text{EN}}$。\n$$\n\\Delta = 1 - 0 = 1\n$$\n这个结果量化了 LASSO 和 Elastic Net 在存在完全相关预测变量时的典型差异：LASSO 选择其中一个变量而舍弃另一个 ($G=1$)，而 Elastic Net 则通过赋予它们相等的系数将它们分组 ($G=0$)。", "answer": "$$\n\\boxed{1}\n$$", "id": "4983817"}]}