## 引言
在医学和生物学研究中，变量之间的关系往往是复杂的、非线性的，传统的[线性模型](@entry_id:178302)在捕捉这些真实模式时常常显得力不从心。例如，年龄对疾病风险的影响或生物标志物浓度与临床结局的关系很少遵循一条直线。广义相加模型（Generalized Additive Models, GAMs）作为一种强大而灵活的统计工具，应运而生。它通过将[广义线性模型](@entry_id:171019)（GLM）中的[线性预测](@entry_id:180569)变量替换为一系列平滑函数的和，从而在不预先指定函数形式的情况下，以数据驱动的方式揭示复杂的非线性关系，同时保持了模型的高度[可解释性](@entry_id:637759)。

本文旨在系统性地介绍广义相加模型，解决研究人员在面对非线性数据时所遇到的建模挑战。通过学习本文，您将能够理解GAMs的内在工作原理，掌握其应用方法，并将其有效地运用于您自己的研究中。文章分为三个核心部分：

第一部分，“原理与机制”，将深入剖析构成GAMs的核心理论，从模型的基本定义、样条基函数的角色，到惩罚似然估计、平滑参数选择以及关键的[模型诊断](@entry_id:136895)工具。

第二部分，“应用与跨学科联系”，将通过来自流行病学、生物医学、基因组学乃至[可解释人工智能](@entry_id:168774)等领域的丰富实例，展示GAMs在解决真实世界问题中的强大功能，例如处理[时间序列数据](@entry_id:262935)、建模[剂量反应关系](@entry_id:190870)以及分析纵向数据。

第三部分，“动手实践”，则为读者提供了应用前述理论解决具体问题的机会，引导读者思考如何构建模型、诊断拟合以及解释结果。

让我们首先深入GAMs的核心，探索其工作的基本原理与机制。

## 原理与机制

广义相加模型（Generalized Additive Models, GAMs）是统计学中一个强大而灵活的工具，它扩展了广义线性模型（Generalized Linear Models, GLMs），能够以数据驱动的方式捕捉预测变量与响应变量之间的非线性关系。本章旨在深入阐述构成GAMs的核心原理与机制，从模型的基本定义出发，探讨其核心组件、估计方法、参数选择策略及其诊断工具。

### 广义相加模型的定义

理解广义相加模型最直接的方式是将其视为广义线性模型的自然延伸。一个GLM包含三个要素：一个来自[指数族](@entry_id:263444)分布的响应变量 $Y$，一个线性预测变量 $\eta = \mathbf{X}\beta$，以及一个连接函数 $g(\cdot)$，它将响应变量的[期望值](@entry_id:150961) $\mu = \mathbb{E}(Y|\mathbf{X})$ 与[线性预测](@entry_id:180569)变量联系起来，即 $g(\mu) = \eta$。GLM的核心假设是效应的“线性”，即预测变量的每一个单位变化都对应于[连接函数](@entry_id:636388)尺度上响应的恒定变化。

然而，在许多科学领域，尤其是医学研究中，这种线性假设往往过于严格。例如，年龄对疾病风险的影响、生物标志物浓度与临床结局的关系，其真实模式很少是纯粹线性的。广义相加模型通过用一个“相加”预测[变量替换](@entry_id:141386)[线性预测](@entry_id:180569)变量来解决这一局限性，从而极大地增强了模型的灵活性。

一个GAM的结构可以形式化地表示为：

$g(\mu_i) = g(\mathbb{E}[Y_i | \mathbf{X}_i]) = \eta_i = \beta_0 + \sum_{j=1}^p f_j(X_{ij})$

其中：
- $Y_i$ 是第 $i$ 个观测的响应变量，其条件分布属于[指数族](@entry_id:263444)（如正态分布、泊松分布、伯努利分布等）。
- $g(\cdot)$ 是一个已知的、单调可微的连接函数。
- $\beta_0$ 是模型的截距。
- $f_j(\cdot)$ 是一个未知的“平滑函数”，用于捕捉第 $j$ 个预测变量 $X_j$ 的非线性效应。这些平滑函数的总和构成了模型的“相加”部分。

在更一般的形式中，GAM可以同时包含平滑项和传统的线性（参数）项。例如，在重症监护医学的一项前瞻性队列研究中，我们可能希望模拟急性肾损伤（一个二元响应变量 $Y$）的发生概率。模型的预测变量可能包括一组连续的生物标志物 $\mathbf{X}=(X_1,\dots,X_p)$，以及一组[参数化](@entry_id:265163)的临床协变量 $\mathbf{Z}$（如治疗分配和医院地点的指示变量）。此时，一个合适的GAM可以表示为 [@problem_id:4964072]：

$g(\mathbb{E}[Y | \mathbf{X}, \mathbf{Z}]) = \eta = \beta_0 + \sum_{j=1}^p f_j(X_j) + \mathbf{Z}\gamma$

这里的关键创新在于，模型允许每个生物标志物 $X_j$ 与（转换后的）结果之间存在灵活的、非线性的关系，同时保留了对临床协变量 $\mathbf{Z}$ 的线性效应，这使得[模型解释](@entry_id:637866)性和预测能力得到了很好的平衡。

当每个平滑函数 $f_j$ 恰好是线性函数时，即 $f_j(x) = \beta_j x$，GAM就退化为了一个标准的GLM。这种退化不仅仅是理论上的，在实践中，当对函数“摆动性”的惩罚趋于无穷大时，平滑函数会被强制变为其惩罚算子的[零空间](@entry_id:171336)（null space）中的形式，对于典型的平滑器，这通常是低阶多项式（如线性函数），从而恢复了GLM [@problem_id:4964042]。

### 平滑函数的表示：样条的角色

GAM的核心是未知平滑函数 $f_j$。为了在实践中估计这些函数，我们需要一种具体的方法来表示它们。最流行和最有效的方法是使用**基函数展开**（basis function expansion）。其思想是将未知的复杂函数 $f(x)$ 表示为一组更简单的、已知的基函数 $b_k(x)$ 的[线性组合](@entry_id:155091)：

$f(x) = \sum_{k=1}^{K} \beta_k b_k(x)$

其中，$\{\beta_k\}$ 是待估计的系数，$K$ 是基函数的数量，它控制着函数的潜在灵活性。**样条**（splines）是最常用的基函数，因为它们具有优异的局部拟合特性和良好的[数值稳定性](@entry_id:146550)。在医学应用中，常见的样条基包括 [@problem_id:4964046]：

- **三次[回归样条](@entry_id:635274)（Cubic Regression Splines）**: 这是由截断幂级数基构建的分段三次多项式。它们在概念上很简单，但在节点数量较多时，基函数之间的高度共线性可能导致数值不稳定。

- **[B样条](@entry_id:172303)（B-splines）**: [B样条基函数](@entry_id:164756)具有**局部支持**（local support）的特性，即每个基函数只在定义域的一个小子集上为非零。这一特性使得模型的设计矩阵变成稀疏的[带状矩阵](@entry_id:746657)，从而在计算上非常高效，并且[数值稳定性](@entry_id:146550)远优于[回归样条](@entry_id:635274)。

- **薄板[回归样条](@entry_id:635274)（Thin Plate Regression Splines, TPRS）**: 与前两者不同，TPRS不依赖于用户手动放置“节点”。它们是通过一个与导数相关的惩罚项的[最优性准则](@entry_id:178183)来定义的，可以看作是与该惩罚项关联的[微分算子](@entry_id:140145)的[特征函数](@entry_id:186820)。这种“无节点”的特性使其在实践中非常方便，并且能够自然地推广到多个维度，同时保持旋转不变性。

从理论上讲，对于具有足够光滑度（例如，四阶导数有界）的真实函数 $f$，上述几种[样条](@entry_id:143749)基在基维度 $K$ 趋于无穷时，都能以相同的最优速率（如 $\mathcal{O}(K^{-4})$）逼近真实函数。因此，选择哪种基通常更多地基于计算效率和数值稳定性的考量，而非理论上的逼近能力 [@problem_id:4964046]。

### [模型拟合](@entry_id:265652)：惩罚似然估计

仅仅用基函数展开来表示 $f_j$ 会带来一个新问题：如果基维度 $K$ 选得很大，模型会变得异常灵活，从而轻易地拟合数据中的随机噪声，导致**过拟合**（overfitting）。为了解决这个问题，GAM的拟合采用**惩罚似然估计**（penalized likelihood estimation）的方法。

其核心思想是在最大化[对数似然](@entry_id:273783)（衡量模型与数据的拟合优度）的同时，施加一个惩罚项来约束函数的“摆动性”或“粗糙度”。对于第 $j$ 个平滑函数 $f_j$，其粗糙度通常通过其二阶导数的积分平方来度量，这代表了函数的**积分平方曲率**。因此，对于一个具有正态误差和恒等连接的相加模型，我们需要最小化的目标函数是：

$\| \mathbf{y} - \boldsymbol{\eta} \|^2 + \sum_{j=1}^p \lambda_j \int (f_j''(x))^2 dx$

其中，第一项是残差平方和（Residual Sum of Squares, RSS），第二项是惩罚项的总和。$\lambda_j \ge 0$ 是**平滑参数**（smoothing parameter），它控制着拟合优度与函数光滑度之间的权衡。

当我们将 $f_j$ 表示为基函数的[线性组合](@entry_id:155091) $f_j(x) = \sum_k \beta_{jk} b_{jk}(x)$ 时，积分惩罚项可以被精确地写成关于系数 $\boldsymbol{\beta}_j$ 的一个二次型 [@problem_id:4964092]：

$\int (f_j''(x))^2 dx = \int \left( \sum_k \beta_{jk} b_{jk}''(x) \right)^2 dx = \boldsymbol{\beta}_j^\top \mathbf{S}_j \boldsymbol{\beta}_j$

其中，$\mathbf{S}_j$ 是一个已知的、对称的、半正定的惩罚矩阵，其元素为 $[S_j]_{kl} = \int b_{jk}''(x) b_{jl}''(x) dx$。因此，对于一个具有[伯努利分布](@entry_id:266933)响应和logit连接的GAM（例如，用于预测住院死亡率），其惩罚[对数似然](@entry_id:273783)的目标函数（在偏离度尺度上）是最小化：

$-2\ell(\boldsymbol{\theta}) + \sum_{j=1}^p \lambda_j \boldsymbol{\beta}_j^\top \mathbf{S}_j \boldsymbol{\beta}_j$

这里 $\ell(\boldsymbol{\theta})$ 是模型的[对数似然](@entry_id:273783)，$\boldsymbol{\theta}$ 是所有模型系数的集合。

平滑参数 $\lambda_j$ 的作用至关重要 [@problem_id:4964042]：

- 当 $\lambda_j \to 0$ 时，惩罚消失。[模型拟合](@entry_id:265652)变为一个无惩罚的[最大似然估计](@entry_id:142509)问题。此时，函数 $f_j$ 将会尽可能地“摆动”以拟合数据，如果基函数的数量足够多，甚至可能插值通过所有数据点，导致严重的过拟合。

- 当 $\lambda_j \to \infty$ 时，为了使总目标函数保持有限，惩罚项 $\boldsymbol{\beta}_j^\top \mathbf{S}_j \boldsymbol{\beta}_j$ 必须趋近于零。这会迫使 $f_j$ 成为一个不受惩罚的函数，即位于惩罚矩阵 $\mathbf{S}_j$ 的零空间中的函数。对于二阶导数惩罚，这个零空间由所有线性函数构成。因此，平滑函数 $\hat{f}_j$ 将被强制为一条直线，GAM退化为GLM。

### 计算机制：从理论到实践

将GAM付诸实践需要解决两个关键的计算问题：模型参数的**可识别性**和非正态响应的**估计算法**。

#### 可识别性与中心化约束

在GAM的定义 $\eta_i = \beta_0 + \sum_j f_j(x_{ij})$ 中，存在一个固有的模糊性。我们可以给任意一个平滑函数 $f_j$ 加上一个常数 $c_j$，同时从截距 $\beta_0$ 中减去这个常数，而模型的线性预测变量 $\eta_i$ 保持不变。例如，令 $f_j'(x) = f_j(x) + c_j$ 和 $\beta_0' = \beta_0 - \sum_j c_j$，新的线性预测变量 $\eta'$ 将与原始的 $\eta$ 完全相同。

由于典型的样条惩罚（如二阶导数惩罚）对[常数函数](@entry_id:152060)不施加惩罚（常数的二阶导数为零），上述变换也不会改变惩罚项的值。这意味着存在无穷多组 $(\beta_0, f_1, \dots, f_p)$ 组合可以得到完全相同的惩罚似然值。这被称为**不可识别**（non-identifiability）问题。从线性代数的角度看，这意味着模型的[设计矩阵](@entry_id:165826)是**[秩亏](@entry_id:754065)**的，因为每个平滑项的基函数矩阵中都存在一个[列的线性组合](@entry_id:150240)，可以产生一个常数向量，这与截距项的列向量（全为1的向量）是[线性相关](@entry_id:185830)的 [@problem_id:4964041]。

为了解决这个问题，我们需要对每个平滑函数施加一个约束，以确保分解是唯一的。标准做法是**中心化约束**（centering constraint），即要求每个平滑函数在所有观测点上的均值为零：

$\sum_{i=1}^n f_j(x_{ij}) = 0, \quad \text{for each } j=1, \dots, p$

这个约束有效地将每个平滑函数的贡献向量 $\mathbf{f}_j = (f_j(x_{1j}), \dots, f_j(x_{nj}))^\top$ 强制到与截距向量 $\mathbf{1}$ 正交的子空间中，从而消除了与截距的混淆。在实践中，这可以通过对基函数进行简单的重[参数化](@entry_id:265163)来实现，它不会改变模型的拟合值或预测风险，仅仅是确保了参数的唯一解释 [@problem_id:4964041]。

#### 惩罚迭代重加权最小二乘（PIRLS）

对于正态分布和恒等连接的GAM，[模型拟合](@entry_id:265652)可以通过解决一个惩罚最小二乘问题来完成。然而，对于非正态响应变量（如临床研究中常见的二元或计数数据），拟合过程更为复杂。标准的算法是**惩罚迭代重加权最小二乘**（Penalized Iteratively Reweighted Least Squares, PIRLS）。

PIRLS是用于拟合GLM的[IRLS算法](@entry_id:750839)的扩展。其核心思想是将最大化非正态惩罚似然的问题，转化为迭代求解一系列的**惩罚加权最小二乘**问题。在每次迭代中，算法会构造一个“伪”数据向量和一组权重，然后用它们来更新模型系数。

在第 $t$ 次迭代中，给定当前对均值 $\boldsymbol{\mu}^{(t)}$ 和[线性预测](@entry_id:180569)变量 $\boldsymbol{\eta}^{(t)}$ 的估计，我们定义 [@problem_id:4841721]：

- **工作响应（working response）** $z_i$：
$z_i^{(t)} = \eta_i^{(t)} + (y_i - \mu_i^{(t)}) g'(\mu_i^{(t)})$
工作响应可以被看作是对响应变量在连接函数尺度上进行的一阶泰勒展开，它将原始的、非正态的响应 $y_i$ 转化为一个近似的、在 $\eta$ 尺度上的伪响应。

- **权重（weight）** $w_i$：
$w_i^{(t)} = \frac{1}{\text{Var}(Y_i|\mu_i^{(t)})} \left( \frac{d\mu_i}{d\eta_i}\Big|_{\eta_i^{(t)}} \right)^2$
权重有两个作用：(1) 它通过 $\text{Var}(Y_i)^{-1}$ 来处理响应变量的[异方差性](@entry_id:136378)（例如，在[伯努利分布](@entry_id:266933)中，方差依赖于均值）；(2) 它通过导数项 $(\frac{d\mu_i}{d\eta_i})^2$ 来考虑连接函数引起的尺度变化。

通过这些定义，PIRLS的每一步都归结为最小化一个惩罚加权最小二乘目标：

$(\mathbf{z}^{(t)} - \mathbf{X}\boldsymbol{\beta})^\top \mathbf{W}^{(t)} (\mathbf{z}^{(t)} - \mathbf{X}\boldsymbol{\beta}) + \sum_{j=1}^p \lambda_j \boldsymbol{\beta}_j^\top \mathbf{S}_j \boldsymbol{\beta}_j$

其中 $\mathbf{X}$ 是完整的模型矩阵，$\mathbf{W}^{(t)}$ 是包含权重 $w_i^{(t)}$ 的对角矩阵。这个加权最小二乘问题有解析解，可以用来更新系数 $\boldsymbol{\beta}$。这个过程不断迭代，直到系数收敛。P[IRLS算法](@entry_id:750839)在理论上等价于使用Fisher评分法来最大化惩罚似然，是拟合所有GAMs的计算核心。

### 调优与解释：自由度与平滑参数选择

[模型拟合](@entry_id:265652)后，我们需要解释其复杂性并验证平滑参数 $\lambda_j$ 的选择是否得当。

#### [有效自由度](@entry_id:161063)（Effective Degrees of Freedom, EDF）

在传统的[线性模型](@entry_id:178302)中，模型的复杂性由其参数个数（即自由度）来衡量。然而，在GAM中，由于平滑惩罚的存在，一个包含 $K$ 个基函数的平滑项所使用的“参数个数”并非整数 $K$，而是介于惩罚零空间的维度（通常为1或2）和 $K$ 之间的一个实数。这个数被称为**[有效自由度](@entry_id:161063)**（EDF）。

形式上，对于一个高斯GAM，存在一个影响矩阵（或称[帽子矩阵](@entry_id:174084)）$\mathbf{H}$，它将观测值向量 $\mathbf{y}$ 映射到拟合值向量 $\hat{\boldsymbol{\eta}} = \mathbf{H}\mathbf{y}$。模型的总EDF定义为该矩阵的迹，$\text{EDF}_{\text{total}} = \text{tr}(\mathbf{H})$。

更进一步，我们可以为模型中的每一个平滑项 $f_j$ 定义其自身的EDF。如果 $\mathbf{A}_j$ 是将 $\mathbf{y}$ 映射到第 $j$ 个平滑项的拟合贡献 $\hat{\boldsymbol{\eta}}^{(j)}$ 的矩阵（即 $\hat{\boldsymbol{\eta}}^{(j)} = \mathbf{A}_j \mathbf{y}$），那么该项的EDF就是 [@problem_id:4964110]：

$\text{edf}_j = \text{tr}(\mathbf{A}_j)$

EDF有一个非常直观的解释：它等于拟合项对其自身观测值的敏感度之和，即 $\text{edf}_j = \sum_{i=1}^n \frac{\partial \hat{\eta}_i^{(j)}}{\partial y_i}$。这衡量了该平滑项的拟合值对观测数据的微小扰动的总体响应程度。一个EDF为1的项表现得像一个线性参数，而一个EDF远大于2的项则表明其具有显著的非线性。EDF是GAM模型摘要中最重要的输出之一，它量化了每个预测变量关系的非线性程度，并且不受用于表示平滑项的特定基函数选择的影响 [@problem_id:4964110]。

#### 平滑参数的选择

选择合适的平滑参数 $\lambda_j$ 是构建一个成功的GAM模型中最关键的一步。$\lambda_j$ 过小会导致[过拟合](@entry_id:139093)，$\lambda_j$ 过大则会[过度平滑](@entry_id:634349)，可能掩盖真实的非线性关系。主流的选择方法分为两类：基于[预测误差](@entry_id:753692)的方法和基于似然的方法。

- **[广义交叉验证](@entry_id:749781)（Generalized Cross-Validation, GCV）**: GCV是一种旨在最小化模型预测误差的流行方法。对于高斯模型，它是一个对**留一[交叉验证](@entry_id:164650)**（Leave-One-Out Cross-Validation, [LOOCV](@entry_id:637718)）的计算高效的近似。[LOOCV](@entry_id:637718)的误差为 $\frac{1}{n}\sum (y_i - \hat{y}_i^{(-i)})^2$，其中 $\hat{y}_i^{(-i)}$ 是在排除第 $i$ 个数据点后拟合的模型对 $y_i$ 的预测。对于线性平滑器，这可以被精确地写为 $\frac{1}{n}\sum (\frac{y_i - \hat{y}_i}{1 - H_{ii}})^2$。GCV通过用[帽子矩阵](@entry_id:174084)对角元素的均值 $\text{tr}(\mathbf{H})/n$ 来替换每个单独的 $H_{ii}$，从而得到GCV分数 [@problem_id:4964078]：

$\text{GCV}(\boldsymbol{\lambda}) = \frac{n \| \mathbf{y} - \hat{\mathbf{y}} \|^2}{(n - \text{tr}(\mathbf{H}))^2} = \frac{n \| \mathbf{y} - \hat{\mathbf{y}} \|^2}{(n - \text{EDF}_{\text{total}})^2}$

选择 $\lambda_j$ 的目标就是最小化这个GCV分数。GCV有效地用模型的总[有效自由度](@entry_id:161063)来惩罚模型的[训练误差](@entry_id:635648)，从而在[拟合优度](@entry_id:637026)和模型复杂性之间取得平衡。

- **限制性[最大似然](@entry_id:146147)（Restricted Maximum Likelihood, REML）**: 另一种更现代且通常更稳健的方法是将GAM视为一个**线性混合效应模型**（Linear Mixed-Effects Model）。在这种视角下，惩罚项被重新解释为对样条系数的[先验分布](@entry_id:141376)。具体来说，最小化惩罚最小二乘目标等价于在一个混合模型中估计参数，其中[样条](@entry_id:143749)系数被视为随机效应 [@problem_id:4964058]。

在这个框架下，平滑参数 $\lambda_j$ 与[方差分量](@entry_id:267561)直接相关：

$\lambda_j = \frac{\sigma^2}{\sigma_{\beta_j}^2}$

其中 $\sigma^2$ 是残差方差，$\sigma_{\beta_j}^2$ 是与第 $j$ 个平滑项相关的随机效应（即[样条](@entry_id:143749)系数）的方差。因此，选择平滑参数的问题就转化为了估计[方差分量](@entry_id:267561)的问题。REML是一种专门用于估计[方差分量](@entry_id:267561)的方法，它通过最大化经过变换后的数据的似然（该似然不依赖于模型的固定效应）来工作。与GCV相比，REML通常不易出现欠平滑，并且能够提供关于平滑项不确定性的更可靠的度量。例如，在一个模拟血清[C反应蛋白](@entry_id:148359)浓度的研究中，假设残差方差 $\sigma^2=4$ 已知，[样条](@entry_id:143749)基的维度 $q=12$，并且投影响应的平方范数为 $y_u^\top y_u = 180$，我们可以通过REML推导出样条系数的[方差估计](@entry_id:268607)为 $\hat{\sigma}_\beta^2 = \frac{180}{12} - 4 = 11$，从而得到平滑参数的REML估计值为 $\hat{\lambda} = \frac{4}{11}$ [@problem_id:4964058]。

### [模型诊断](@entry_id:136895)：共曲性

在[多元线性回归](@entry_id:141458)中，**[多重共线性](@entry_id:141597)**（multicollinearity）是指预测变量之间存在高度[线性相关](@entry_id:185830)，这会使得参数估计不稳定。在GAM中，类似的问题被称为**共曲性**（concurvity）。

共曲性发生在当一个平滑项 $f_j(X_j)$ 可以被模型中其他平滑项 $f_k(X_k), k \neq j$ 的（可能是非线性的）组合很好地近似时。例如，如果两个预测变量（如年龄和病程）高度相关，它们对应的平滑函数很可能会相互混淆。高度的共曲性使得我们难以分离单个预测变量的独立效应，导致对平滑函数的估计非常不稳定，其[置信区间](@entry_id:138194)也会异常宽泛。

我们可以借鉴多重共线性的诊断思想来量化共曲性。在[线性模型](@entry_id:178302)中，我们通过将一个预测变量对其他所有预测变量进行回归，并计算 $R^2$ 来评估共线性。在GAM中，我们可以对“拟合的平滑函数向量”做同样的事情 [@problem_id:4964083]。

具体来说，对于每个平滑项 $j$，我们将其在所有数据点上的拟合值构成一个向量 $\hat{\mathbf{f}}_j = (\hat{f}_j(x_{1j}), \dots, \hat{f}_j(x_{nj}))^\top$。然后，我们将这个向量 $\hat{\mathbf{f}}_j$ 对所有其他平滑项的拟合向量 $\hat{\mathbf{f}}_k, k \neq j$ 进行线性回归。这次回归的 $R^2$ 值，即 $\hat{\mathbf{f}}_j$ 的范数平方中可以被其他平滑项的线性空间所解释的比例，就是对平滑项 $j$ 的共曲性的度量。

$c_j = \frac{\|\Pi_{S_{-j}} \hat{\mathbf{f}}_j \|^2}{\|\hat{\mathbf{f}}_j\|^2}$

其中 $S_{-j}$ 是由向量集合 $\{\hat{\mathbf{f}}_k\}_{k \neq j}$ 张成的子空间，$\Pi_{S_{-j}}$ 是到该子空间的[正交投影](@entry_id:144168)。这个度量值介于0（无共曲性）和1（完全共曲性）之间，为评估和诊断GAM中潜在的共线性问题提供了有力的工具 [@problem_id:4964083]。