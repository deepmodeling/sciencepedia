{"hands_on_practices": [{"introduction": "在网络分析中，识别关键节点是核心任务之一。最直观地定义“枢纽”（hub）和“权威”（authority）的方式是通过节点的连接数。本练习将引导你使用一个小的信号网络，通过计算节点的出度 ($k^{\\text{out}}$) 和入度 ($k^{\\text{in}}$) 来实践这一基本概念，这是理解更复杂算法（如HITS）前的重要基础 [@problem_id:4364845]。", "problem": "考虑一个由四个生物分子实体组成的有向信号网络，这些实体由节点 $v_{1}, v_{2}, v_{3}, v_{4}$ 表示。该网络由一个二元邻接矩阵 $\\mathbf{A} \\in \\{0,1\\}^{4 \\times 4}$ 编码，其中 $a_{ij} = 1$ 表示从 $v_{i}$ 到 $v_{j}$ 的有向影响，否则 $a_{ij} = 0$。假设不存在自环，因此对于所有 $i \\in \\{1,2,3,4\\}$，都有 $a_{ii} = 0$。该矩阵为\n$$\n\\mathbf{A} = \\begin{pmatrix}\n0  1  1  1 \\\\\n0  0  1  0 \\\\\n0  0  0  1 \\\\\n0  0  0  0\n\\end{pmatrix}.\n$$\n在系统生物医学中，一个具有较大出向连接性的节点通常被视为一个枢纽，而一个具有较大入向连接性的节点则被视为一个权威。节点 $v_{i}$ 的出度定义为 $k_{i}^{\\text{out}} = \\sum_{j=1}^{4} a_{ij}$，入度定义为 $k_{i}^{\\text{in}} = \\sum_{j=1}^{4} a_{ji}$。仅使用有向图的这些核心定义，首先计算每个 $i \\in \\{1,2,3,4\\}$ 的 $k_{i}^{\\text{out}}$ 和 $k_{i}^{\\text{in}}$。然后，根据以下关于出向连接性的度阈值标准，识别哪些节点符合枢纽的条件：如果一个节点的 $k_{i}^{\\text{out}} \\ge \\tau$，则该节点被标记为枢纽，其中 $\\tau$ 是多重集 $\\{k_{i}^{\\text{out}} : i = 1,2,3,4\\}$ 的经验第90百分位数，使用最近秩规则计算（将多重集按非递减顺序排序，并选择秩为 $\\lceil 0.9 n \\rceil$ 的元素，其中 $n$ 是节点数）。最后，令 $H$ 表示根据此标准被标记为枢纽的所有节点的出度之和。将 $H$ 计算为单个实数值。无需四舍五入。", "solution": "问题陈述是有效的。它在科学上基于生物网络的标准图论分析，这是系统生物医学中的一种常见做法。所有术语，如节点、邻接矩阵、入度和出度，都得到了正式且正确的定义。这是一个适定的问题，提供了所有必要的数据——邻接矩阵 $\\mathbf{A}$ 和一个用于计算百分位数阈值 $\\tau$ 的精确、无歧义的规则——从而可以得出一个唯一、可验证的解。\n\n第一步是计算每个节点 $v_{i}$ (其中 $i \\in \\{1,2,3,4\\}$) 的出度 $k_{i}^{\\text{out}}$ 和入度 $k_{i}^{\\text{in}}$。出度 $k_{i}^{\\text{out}}$ 是邻接矩阵 $\\mathbf{A}$ 第 $i$ 行元素的和，而入度 $k_{i}^{\\text{in}}$ 是第 $i$ 列元素的和。\n\n给定的邻接矩阵为：\n$$\n\\mathbf{A} = \\begin{pmatrix}\n0  1  1  1 \\\\\n0  0  1  0 \\\\\n0  0  0  1 \\\\\n0  0  0  0\n\\end{pmatrix}\n$$\n\n出度计算如下：\n$k_{1}^{\\text{out}} = \\sum_{j=1}^{4} a_{1j} = 0 + 1 + 1 + 1 = 3$\n$k_{2}^{\\text{out}} = \\sum_{j=1}^{4} a_{2j} = 0 + 0 + 1 + 0 = 1$\n$k_{3}^{\\text{out}} = \\sum_{j=1}^{4} a_{3j} = 0 + 0 + 0 + 1 = 1$\n$k_{4}^{\\text{out}} = \\sum_{j=1}^{4} a_{4j} = 0 + 0 + 0 + 0 = 0$\n\n入度计算如下：\n$k_{1}^{\\text{in}} = \\sum_{j=1}^{4} a_{j1} = 0 + 0 + 0 + 0 = 0$\n$k_{2}^{\\text{in}} = \\sum_{j=1}^{4} a_{j2} = 1 + 0 + 0 + 0 = 1$\n$k_{3}^{\\text{in}} = \\sum_{j=1}^{4} a_{j3} = 1 + 1 + 0 + 0 = 2$\n$k_{4}^{\\text{in}} = \\sum_{j=1}^{4} a_{j4} = 1 + 0 + 1 + 0 = 2$\n\n下一步是识别枢纽节点。如果一个节点 $v_i$ 的出度 $k_{i}^{\\text{out}}$ 大于或等于阈值 $\\tau$，则该节点是一个枢纽。阈值 $\\tau$ 定义为出度多重集 $\\{k_{i}^{\\text{out}} : i = 1,2,3,4\\}$ 的经验第90百分位数，使用最近秩规则计算。\n\n出度的多重集是 $\\{3, 1, 1, 0\\}$。\n节点数为 $n=4$。\n为了找到该百分位数，我们首先将这个多重集按非递减顺序排序：$(0, 1, 1, 3)$。\n\n第90百分位数的秩计算为 $R = \\lceil 0.9 \\times n \\rceil$。\n代入 $n=4$：\n$$\nR = \\lceil 0.9 \\times 4 \\rceil = \\lceil 3.6 \\rceil = 4\n$$\n阈值 $\\tau$ 是排序后多重集中第4个位置的元素。\n排序后的多重集是 $(0, 1, 1, 3)$。秩为4的元素是 $3$。\n因此，阈值为 $\\tau=3$。\n\n现在我们应用枢纽识别标准：如果一个节点 $v_i$ 的出度 $k_{i}^{\\text{out}} \\ge 3$，则它是一个枢纽。\n我们检查每个节点：\n- 对于节点 $v_1$：$k_{1}^{\\text{out}} = 3$。因为 $3 \\ge 3$，所以 $v_1$ 是一个枢纽。\n- 对于节点 $v_2$：$k_{2}^{\\text{out}} = 1$。因为 $1  3$，所以 $v_2$ 不是一个枢纽。\n- 对于节点 $v_3$：$k_{3}^{\\text{out}} = 1$。因为 $1  3$，所以 $v_3$ 不是一个枢纽。\n- 对于节点 $v_4$：$k_{4}^{\\text{out}} = 0$。因为 $0  3$，所以 $v_4$ 不是一个枢纽。\n\n唯一被识别为枢纽的节点是 $v_1$。\n\n最后，我们计算 $H$，即所有被标记为枢纽的节点的出度之和。由于 $v_1$ 是唯一的枢纽，所以 $H$ 等于 $v_1$ 的出度。\n$$\nH = k_{1}^{\\text{out}} = 3\n$$\n$H$ 的值是一个单个实数值，如题所求。", "answer": "$$\n\\boxed{3}\n$$", "id": "4364845"}, {"introduction": "简单地计算度数忽略了一个关键思想：一个好的枢纽应该指向好的权威，反之亦然。HITS算法（Hyperlink-Induced Topic Search）正是基于这种相互增强的原则。本练习将让你通过一个基因调控网络的实例，计算HITS算法迭代收敛后的枢纽和权威分数，从而深入理解其背后的线性代数原理 [@problem_id:4364810]。", "problem": "一个包含 $3$ 个基因（索引为 $1,2,3$）的有向基因调控相互作用网络由一个邻接矩阵 $A \\in \\mathbb{R}^{3 \\times 3}$ 编码，其中如果存在从基因 $i$ 到基因 $j$ 的有向相互作用，则矩阵元素 $A_{ij}=1$，否则 $A_{ij}=0$。考虑\n$$\nA \\;=\\; \\begin{pmatrix}\n0  1  1\\\\\n0  0  1\\\\\n1  0  0\n\\end{pmatrix}.\n$$\n从超链接诱导主题搜索（Hyperlink-Induced Topic Search, HITS）算法的核心定义出发，hub 分数向量 $h^{(t)} \\in \\mathbb{R}^3$ 和 authority 分数向量 $a^{(t)} \\in \\mathbb{R}^3$ 通过应用 $A$ 和 $A^\\top$ 进行交替步骤更新，每次更新时都进行欧几里得归一化以消除任意缩放。设初始向量为 $h^{(0)} = a^{(0)} = (1,1,1)^\\top$，并在每一步执行更新 $a^{(t+1)} \\leftarrow A^\\top h^{(t)}$，然后归一化为单位欧几里得范数 $\\|a^{(t+1)}\\|_{2}=1$；接着执行更新 $h^{(t+1)} \\leftarrow A a^{(t+1)}$，然后归一化为单位欧几里得范数 $\\|h^{(t+1)}\\|_{2}=1$。此处的欧几里得范数为 $\\|x\\|_{2} = \\sqrt{\\sum_{i} x_i^2}$。仅使用这些定义以及关于非负矩阵上线性动力系统的经过充分检验的事实，确定当 $t \\to \\infty$ 时，极限的单位欧几里得范数 hub 和 authority 向量 $h^\\ast$ 和 $a^\\ast$。\n\n定义标量 $S$ 为基因 $2$ 的极限 hub 分数和 authority 分数的乘积，即 $S = h^\\ast_2 \\, a^\\ast_2$。请以单一精确的封闭形式表达式给出 $S$。不要对结果进行四舍五入。", "solution": "问题要求解一个由邻接矩阵 $A$ 描述的给定基因调控网络的极限 hub 和 authority 分数向量 $h^\\ast$ 和 $a^\\ast$。极限分数将通过 HITS 算法确定。最终要计算的量是 $S = h^\\ast_2 \\, a^\\ast_2$。\n\n该问题在科学上是有效的且是适定问题。HITS 算法是网络分析中的一个标准程序，其收敛性是线性动力系统和非负矩阵理论中的一个公认结果。我们可以继续进行求解。\n\nHITS 算法涉及两个向量，authority 分数向量 $a$ 和 hub 分数向量 $h$。提供的迭代更新规则如下：\n1. $\\tilde{a}^{(t+1)} = A^\\top h^{(t)}$，然后进行归一化 $a^{(t+1)} = \\tilde{a}^{(t+1)} / \\|\\tilde{a}^{(t+1)}\\|_2$。\n2. $\\tilde{h}^{(t+1)} = A a^{(t+1)}$，然后进行归一化 $h^{(t+1)} = \\tilde{h}^{(t+1)} / \\|\\tilde{h}^{(t+1)}\\|_2$。\n\n这个迭代过程等价于用来寻找矩阵 $A^\\top A$ 和 $A A^\\top$ 的主特征向量的幂迭代法。具体来说，authority 向量序列 $\\{a^{(t)}\\}_{t \\ge 0}$ 收敛到 $A^\\top A$ 的归一化主特征向量，而 hub 向量序列 $\\{h^{(t)}\\}_{t \\ge 0}$ 收敛到 $A A^\\top$ 的归一化主特征向量。因此，极限向量 $a^\\ast$ 和 $h^\\ast$ 就是这些归一化的主特征向量。主特征向量是与模最大的特征值相关联的那个特征向量，根据 Perron-Frobenius 定理，对于这些矩阵，该特征值是实数且为正。\n\n给定的邻接矩阵为：\n$$A = \\begin{pmatrix} 0  1  1 \\\\ 0  0  1 \\\\ 1  0  0 \\end{pmatrix}$$\n其转置矩阵为：\n$$A^\\top = \\begin{pmatrix} 0  0  1 \\\\ 1  0  0 \\\\ 1  1  0 \\end{pmatrix}$$\n\n首先，我们计算 authority 分数的矩阵，$M_a = A^\\top A$：\n$$M_a = A^\\top A = \\begin{pmatrix} 0  0  1 \\\\ 1  0  0 \\\\ 1  1  0 \\end{pmatrix} \\begin{pmatrix} 0  1  1 \\\\ 0  0  1 \\\\ 1  0  0 \\end{pmatrix} = \\begin{pmatrix} 1  0  0 \\\\ 0  1  1 \\\\ 0  1  2 \\end{pmatrix}$$\n接下来，我们计算 hub 分数的矩阵，$M_h = A A^\\top$：\n$$M_h = A A^\\top = \\begin{pmatrix} 0  1  1 \\\\ 0  0  1 \\\\ 1  0  0 \\end{pmatrix} \\begin{pmatrix} 0  0  1 \\\\ 1  0  0 \\\\ 1  1  0 \\end{pmatrix} = \\begin{pmatrix} 2  1  0 \\\\ 1  1  0 \\\\ 0  0  1 \\end{pmatrix}$$\n矩阵 $M_a$ 和 $M_h$ 具有相同的非零特征值。我们通过求解特征方程来找到特征值，以 $M_a$ 为例：\n$$\\det(M_a - \\lambda I) = \\det \\begin{pmatrix} 1-\\lambda  0  0 \\\\ 0  1-\\lambda  1 \\\\ 0  1  2-\\lambda \\end{pmatrix} = 0$$\n$$(1-\\lambda) \\left[ (1-\\lambda)(2-\\lambda) - 1 \\right] = (1-\\lambda) (2 - 3\\lambda + \\lambda^2 - 1) = (1-\\lambda)(\\lambda^2 - 3\\lambda + 1) = 0$$\n根为 $\\lambda = 1$ 以及 $\\lambda^2 - 3\\lambda + 1 = 0$ 的根，由二次公式给出：\n$$\\lambda = \\frac{-(-3) \\pm \\sqrt{(-3)^2 - 4(1)(1)}}{2(1)} = \\frac{3 \\pm \\sqrt{5}}{2}$$\n三个特征值是 $\\lambda_1 = \\frac{3+\\sqrt{5}}{2}$，$\\lambda_2 = 1$ 和 $\\lambda_3 = \\frac{3-\\sqrt{5}}{2}$。最大特征值（主特征值）是 $\\lambda_{\\text{max}} = \\frac{3+\\sqrt{5}}{2}$。这个值也被称为 $\\phi^2$，其中 $\\phi = \\frac{1+\\sqrt{5}}{2}$ 是黄金比例。\n\n现在，我们求 $M_a$ 对应于 $\\lambda_{\\text{max}}$ 的主特征向量。设该特征向量为 $v_a = (x_1, x_2, x_3)^\\top$。\n$$(M_a - \\lambda_{\\text{max}}I)v_a = \\begin{pmatrix} 1 - \\frac{3+\\sqrt{5}}{2}  0  0 \\\\ 0  1 - \\frac{3+\\sqrt{5}}{2}  1 \\\\ 0  1  2 - \\frac{3+\\sqrt{5}}{2} \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$$\n$$\\begin{pmatrix} \\frac{-1-\\sqrt{5}}{2}  0  0 \\\\ 0  \\frac{-1-\\sqrt{5}}{2}  1 \\\\ 0  1  \\frac{1-\\sqrt{5}}{2} \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$$\n从第一行，我们有 $\\frac{-1-\\sqrt{5}}{2} x_1 = 0$，这意味着 $x_1 = 0$。\n从第二行，我们有 $\\frac{-1-\\sqrt{5}}{2} x_2 + x_3 = 0$，所以 $x_3 = \\frac{1+\\sqrt{5}}{2} x_2 = \\phi x_2$。\n因此，一个未归一化的特征向量是 $(0, 1, \\phi)^\\top$。\n\n接下来，我们求 $M_h$ 对应于 $\\lambda_{\\text{max}}$ 的主特征向量。设该特征向量为 $v_h = (y_1, y_2, y_3)^\\top$。\n$$(M_h - \\lambda_{\\text{max}}I)v_h = \\begin{pmatrix} 2 - \\frac{3+\\sqrt{5}}{2}  1  0 \\\\ 1  1 - \\frac{3+\\sqrt{5}}{2}  0 \\\\ 0  0  1 - \\frac{3+\\sqrt{5}}{2} \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$$\n$$\\begin{pmatrix} \\frac{1-\\sqrt{5}}{2}  1  0 \\\\ 1  \\frac{-1-\\sqrt{5}}{2}  0 \\\\ 0  0  \\frac{-1-\\sqrt{5}}{2} \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$$\n从第三行，我们有 $\\frac{-1-\\sqrt{5}}{2} y_3 = 0$，这意味着 $y_3=0$。\n从第一行，我们有 $\\frac{1-\\sqrt{5}}{2} y_1 + y_2 = 0$，所以 $y_2 = -\\frac{1-\\sqrt{5}}{2} y_1 = \\frac{\\sqrt{5}-1}{2} y_1 = \\frac{1}{\\phi} y_1$。\n选择 $y_1 = \\phi$，我们得到 $y_2 = 1$。因此，一个未归一化的特征向量是 $(\\phi, 1, 0)^\\top$。\n\n极限向量 $a^\\ast$ 和 $h^\\ast$ 是这些主特征向量的欧几里得归一化版本。\n对于 $a^\\ast$，未归一化的向量是 $(0, 1, \\phi)^\\top$。范数的平方是 $0^2 + 1^2 + \\phi^2 = 1 + (\\frac{1+\\sqrt{5}}{2})^2 = 1 + \\frac{1+2\\sqrt{5}+5}{4} = 1 + \\frac{6+2\\sqrt{5}}{4} = 1 + \\frac{3+\\sqrt{5}}{2} = \\frac{5+\\sqrt{5}}{2}$。\n范数是 $\\sqrt{\\frac{5+\\sqrt{5}}{2}}$。\n所以，$a^\\ast = \\frac{1}{\\sqrt{\\frac{5+\\sqrt{5}}{2}}} (0, 1, \\phi)^\\top$。\n\n对于 $h^\\ast$，未归一化的向量是 $(\\phi, 1, 0)^\\top$。范数的平方是 $\\phi^2 + 1^2 + 0^2 = \\frac{5+\\sqrt{5}}{2}$。\n范数是 $\\sqrt{\\frac{5+\\sqrt{5}}{2}}$。\n所以，$h^\\ast = \\frac{1}{\\sqrt{\\frac{5+\\sqrt{5}}{2}}} (\\phi, 1, 0)^\\top$。\n\n问题要求解标量 $S = h^\\ast_2 \\, a^\\ast_2$。我们提取归一化向量的第二个分量：\n$h^\\ast_2 = \\frac{1}{\\sqrt{\\frac{5+\\sqrt{5}}{2}}} \\cdot 1 = \\sqrt{\\frac{2}{5+\\sqrt{5}}}$\n$a^\\ast_2 = \\frac{1}{\\sqrt{\\frac{5+\\sqrt{5}}{2}}} \\cdot 1 = \\sqrt{\\frac{2}{5+\\sqrt{5}}}$\n乘积是：\n$$S = h^\\ast_2 \\, a^\\ast_2 = \\left(\\sqrt{\\frac{2}{5+\\sqrt{5}}}\\right) \\left(\\sqrt{\\frac{2}{5+\\sqrt{5}}}\\right) = \\frac{2}{5+\\sqrt{5}}$$\n为了得到一个简化的封闭形式表达式，我们对分母进行有理化：\n$$S = \\frac{2}{5+\\sqrt{5}} \\cdot \\frac{5-\\sqrt{5}}{5-\\sqrt{5}} = \\frac{2(5-\\sqrt{5})}{5^2 - (\\sqrt{5})^2} = \\frac{2(5-\\sqrt{5})}{25-5} = \\frac{2(5-\\sqrt{5})}{20} = \\frac{5-\\sqrt{5}}{10}$$\n这就是 $S$ 的最终精确表达式。", "answer": "$$\\boxed{\\frac{5-\\sqrt{5}}{10}}$$", "id": "4364810"}, {"introduction": "HITS算法并非衡量节点重要性的唯一方法，特征向量中心性是另一种广泛使用的方法。这两种方法在某些网络结构下会给出截然不同的结果。本练习提供了一个精心设计的网络 [@problem_id:4364781]，旨在揭示HITS权威分与特征向量中心性之间的差异，通过分析这种差异的根源，你将能更深刻地把握HITS算法的独特之处。", "problem": "考虑一个有向加权的基因调控网络，其节点标记为 $1$ 到 $7$。从节点 $i$ 到节点 $j$ 的权重为 $A_{ij}$ 的边表示 $i$ 对 $j$ 的转录激活强度。邻接矩阵 $A \\in \\mathbb{R}^{7 \\times 7}$ 为\n$$\nA \\;=\\;\n\\begin{bmatrix}\n0  1  1  1  0  0  0\\\\\n1  0  1  1  0  0  0\\\\\n1  1  0  1  0  0  0\\\\\n0  0  0  0  0  0  0\\\\\n0  0  0  0  0  0  0\\\\\n0  0  0  0  3  0  0\\\\\n0  0  0  0  3  0  0\n\\end{bmatrix}.\n$$\n你可以假设以下基本定义。\n- 对于一个有向网络，其特征向量中心性 $x \\in \\mathbb{R}^7_{\\ge 0}$ 是满足 $A^{\\top} x = \\lambda x$ 的非负主特征向量，其中 $\\lambda$ 是 $A^{\\top}$（等价于 $A$）的谱半径，这是根据 Perron–Frobenius 定理在合适的不可约块上得出的。\n- 超链接诱导主题搜索 (Hyperlink-Induced Topic Search, HITS) 方法定义了权威向量 $a \\in \\mathbb{R}^7_{\\ge 0}$ 和中心向量 $h \\in \\mathbb{R}^7_{\\ge 0}$，它们满足 $a \\propto A^{\\top} h$ 和 $h \\propto A a$，这意味着 $a$ 是 $A^{\\top} A$ 的主特征向量，$h$ 是 $A A^{\\top}$ 的主特征向量。\n\n仅使用这些定义和标准的线性代数推理（例如，块结构、强连通分量和谱半径），确定该网络中哪个节点的特征向量中心性排名高但 HITS 权威值排名低，并选择最准确解释这种差异原因的选项。\n\nA. 节点 $4$；它从一个具有高特征向量中心性的强连通入邻域（节点 $1$–$3$）接收输入，但这些输入节点在 HITS 下是弱中心，因为主导的中心-权威强化集中在子图 $6,7 \\to 5$ 中，因此 HITS 权威值流向节点 $5$ 而非节点 $4$。\n\nB. 节点 $5$；它的特征向量中心性很高，因为它由强中心节点 $6$ 和 $7$ 馈入，但 HITS 权威值很低，因为它是一个没有出边的汇点，HITS 会惩罚这种情况。\n\nC. 节点 $4$；它的特征向量中心性和 HITS 权威值排名同样高，因为在有向图上，这两种方法在归一化后是等价的。\n\nD. 节点 $1$；它因参与一个环而具有高特征向量中心性，但 HITS 权威值低，因为它指向低权威值的汇点节点 $4$，这降低了它的中心分数，从而抑制了它的权威值。", "solution": "用户希望我根据给定的网络，找出一个特征向量中心性高但 HITS 权威值低的节点，并选择能最好地解释这种差异的选项。\n\n### 步骤 1：问题验证\n\n问题提供了一个由 $7 \\times 7$ 邻接矩阵 $A$ 定义的有向加权网络。它还为特征向量中心性（基于 $A^{\\top}$ 的主特征向量）和 HITS 权威值与中心值（分别基于 $A^{\\top}A$ 和 $A A^{\\top}$ 的主特征向量）提供了精确的标准定义。问题是具体的，需要应用这些定义和线性代数原理来分析给定的网络。\n\n- **已知条件**：\n    - 包含节点 $\\{1, 2, 3, 4, 5, 6, 7\\}$ 的网络。\n    - 邻接矩阵 $A \\in \\mathbb{R}^{7 \\times 7}$：\n    $$\n    A \\;=\\;\n    \\begin{bmatrix}\n    0  1  1  1  0  0  0\\\\\n    1  0  1  1  0  0  0\\\\\n    1  1  0  1  0  0  0\\\\\n    0  0  0  0  0  0  0\\\\\n    0  0  0  0  0  0  0\\\\\n    0  0  0  0  3  0  0\\\\\n    0  0  0  0  3  0  0\n    \\end{bmatrix}.\n    $$\n    - 特征向量中心性 $x$ 满足 $A^{\\top} x = \\lambda x$，其中 $\\lambda = \\rho(A^{\\top})$。\n    - HITS 权威值 $a$ 是 $A^{\\top}A$ 的主特征向量。\n    - HITS 中心值 $h$ 是 $AA^{\\top}$ 的主特征向量。\n\n- **验证检查**：\n    - **科学依据**：该问题在网络理论和线性代数方面有充分的依据。特征向量中心性和 HITS 是标准算法。\n    - **适定性**：该问题在数学上是适定的。所涉及的矩阵是非负的，根据 Perron-Frobenius 定理保证了主特征向量的存在。问题正确地暗示了涉及可约矩阵的微妙之处。\n    - **客观性**：问题以精确的数学语言陈述，没有歧义或主观性。\n    - **完整性**：提供了所有必要的定义和数据。\n    - **可行性**：计算是可行的，网络结构足够简单，无需大量计算即可分析。\n\n- **结论**：问题有效。\n\n### 步骤 2：求解推导\n\n我们将计算每个节点的特征向量中心性和 HITS 权威值，然后进行比较。\n\n#### 特征向量中心性\n\n特征向量中心性 $x$ 是 $A^{\\top}$ 的主特征向量。\n$$\nA^{\\top} \\;=\\;\n\\begin{bmatrix}\n0  1  1  0  0  0  0\\\\\n1  0  1  0  0  0  0\\\\\n1  1  0  0  0  0  0\\\\\n1  1  1  0  0  0  0\\\\\n0  0  0  0  0  3  3\\\\\n0  0  0  0  0  0  0\\\\\n0  0  0  0  0  0  0\n\\end{bmatrix}.\n$$\n这个矩阵是可约的，并且是块下三角矩阵。其特征值是其对角块的特征值。对角块对应于由 $A^{\\top}$ 表示的图的强连通分量（SCC）（注意：终端 SCC）。\n- 节点 $\\{1, 2, 3\\}$ 的子矩阵是 $B = \\begin{bmatrix} 0  1  1 \\\\ 1  0  1 \\\\ 1  1  0 \\end{bmatrix}$。该矩阵对应一个不可约分量（一个 3-团）。其特征多项式为 $(\\lambda+1)^2(\\lambda-2)=0$。谱半径为 $\\rho(B) = 2$。\n- 节点 $4, 5, 6, 7$ 的对角线元素均为 $0$。\n$A^{\\top}$ 的谱半径是其对角块谱半径的最大值，因此 $\\lambda = \\rho(A^{\\top}) = 2$。特征向量中心性 $x$ 必须满足 $A^{\\top}x = 2x$。设 $x = (x_1, \\dots, x_7)^{\\top}$。\n$$\n\\begin{bmatrix}\n0  1  1  0  0  0  0\\\\\n1  0  1  0  0  0  0\\\\\n1  1  0  0  0  0  0\\\\\n1  1  1  0  0  0  0\\\\\n0  0  0  0  0  3  3\\\\\n0  0  0  0  0  0  0\\\\\n0  0  0  0  0  0  0\n\\end{bmatrix}\n\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6 \\\\ x_7 \\end{pmatrix}\n\\;=\\; 2\n\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6 \\\\ x_7 \\end{pmatrix}\n$$\n这得到以下方程组：\n1. $x_2 + x_3 = 2x_1$\n2. $x_1 + x_3 = 2x_2$\n3. $x_1 + x_2 = 2x_3$\n4. $x_1 + x_2 + x_3 = 2x_4$\n5. $3x_6 + 3x_7 = 2x_5$\n6. $0 = 2x_6 \\implies x_6 = 0$\n7. $0 = 2x_7 \\implies x_7 = 0$\n\n由 $(6)$ 和 $(7)$ 得，$x_6=0$ 且 $x_7=0$。代入 $(5)$，我们发现 $2x_5=0$，所以 $x_5=0$。\n前三个方程对应于块 $B$ 的特征值问题，对于 $\\lambda=2$，其特征向量为 $(c, c, c)^{\\top}$，其中 $c0$ 是任意常数。所以，$x_1=x_2=x_3=c$。\n将这些代入 $(4)$ 得到 $c+c+c = 2x_4$，这意味着 $3c = 2x_4$，或 $x_4 = \\frac{3}{2}c$。\n\n特征向量中心性向量为 $x \\propto (c, c, c, \\frac{3}{2}c, 0, 0, 0)^{\\top}$。归一化使得 $c=1$，我们有 $x \\propto (1, 1, 1, 1.5, 0, 0, 0)^{\\top}$。\n- **特征向量中心性排名**：节点 $4$ 最高，其次是节点 $1, 2, 3$。节点 $5, 6, 7$ 的中心性为零。\n\n#### HITS 权威值\n\n权威向量 $a$ 是 $M = A^{\\top}A$ 的主特征向量。$A^{\\top}A$ 的非零特征值与 $AA^{\\top}$ 的非零特征值相同。分析这两个矩阵的块结构通常更容易。\n我们来计算 $AA^{\\top}$。其 $(i,j)$ 项是 $A$ 的第 $i$ 行和第 $j$ 行的点积。\nA 的行向量为：$r_1=(0,1,1,1,0,0,0)$，$r_2=(1,0,1,1,0,0,0)$，$r_3=(1,1,0,1,0,0,0)$，$r_4=r_5=(0,0,0,0,0,0,0)$，$r_6=r_7=(0,0,0,0,3,0,0)$。\n$$\nAA^{\\top} \\;=\\;\n\\begin{bmatrix}\n3  2  2  0  0  0  0 \\\\\n2  3  2  0  0  0  0 \\\\\n2  2  3  0  0  0  0 \\\\\n0  0  0  0  0  0  0 \\\\\n0  0  0  0  0  0  0 \\\\\n0  0  0  0  0  9  9 \\\\\n0  0  0  0  0  9  9\n\\end{bmatrix}.\n$$\n这个矩阵是块对角矩阵。其特征值是各块的特征值。\n- 节点 $\\{1, 2, 3\\}$ 的块 $N_1$：$N_1 = \\begin{bmatrix} 3  2  2 \\\\ 2  3  2 \\\\ 2  2  3 \\end{bmatrix} = 2J + I$，其中 $J$ 是全一矩阵。$J$ 的特征值为 $3,0,0$。$N_1$ 的特征值为 $2(3)+1=7$ 和 $2(0)+1=1$（重根）。谱半径为 $\\rho(N_1)=7$。\n- 节点 $4, 5$ 的块是零矩阵，特征值为 $0$。\n- 节点 $\\{6, 7\\}$ 的块 $N_2$：$N_2 = \\begin{bmatrix} 9  9 \\\\ 9  9 \\end{bmatrix}$。特征方程为 $(9-\\lambda)^2 - 81 = 0$，得出 $\\lambda(\\lambda-18)=0$。特征值为 $18, 0$。谱半径为 $\\rho(N_2)=18$。\n\n$AA^{\\top}$（以及 $A^{\\top}A$）的主特征值为 $\\lambda_{HITS} = \\max(7, 0, 18) = 18$。\n中心向量 $h$ 是 $AA^{\\top}$ 对应于 $\\lambda=18$ 的特征向量。由于该特征值来自块 $N_2$，所以特征向量只在节点 $6$ 和 $7$ 上有非零分量。\n$(N_2 - 18I)v = \\begin{pmatrix} -9  9 \\\\ 9  -9 \\end{pmatrix} \\begin{pmatrix}v_6 \\\\ v_7\\end{pmatrix} = 0 \\implies v_6=v_7$。\n因此，中心向量为 $h \\propto (0, 0, 0, 0, 0, 1, 1)^{\\top}$。节点 $6$ 和 $7$ 是仅有的中心。\n\n权威向量 $a$ 可以通过关系 $a \\propto A^{\\top}h$ 找到。\n$$\na \\propto A^{\\top}h = \\begin{bmatrix}\n0  1  1  0  0  0  0\\\\\n1  0  1  0  0  0  0\\\\\n1  1  0  0  0  0  0\\\\\n1  1  1  0  0  0  0\\\\\n0  0  0  0  0  3  3\\\\\n0  0  0  0  0  0  0\\\\\n0  0  0  0  0  0  0\n\\end{bmatrix}\n\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 6 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\n因此，权威向量为 $a \\propto (0, 0, 0, 0, 1, 0, 0)^{\\top}$。\n- **HITS 权威值排名**：节点 $5$ 是唯一具有非零权威值的节点。所有其他节点的权威值为零。\n\n#### 比较\n\n- **特征向量中心性**：$x \\propto (1, 1, 1, 1.5, 0, 0, 0)^{\\top}$\n- **HITS 权威值**：$a \\propto (0, 0, 0, 0, 1, 0, 0)^{\\top}$\n\n我们在寻找一个特征向量中心性排名高但 HITS 权威值排名低的节点。\n- 节点 $4$ 具有最高的特征向量中心性（$x_4=1.5$），但 HITS 权威值为零（$a_4=0$）。节点 $1, 2, 3$ 也符合这一描述（$x_{1,2,3}=1, a_{1,2,3}=0$）。节点 $4$ 是最显著的例子。\n\n### 步骤 3：选项分析\n\n- **A. 节点 $4$；它从一个具有高特征向量中心性的强连通入邻域（节点 $1$–$3$）接收输入，但这些输入节点在 HITS 下是弱中心，因为主导的中心-权威强化集中在子图 $6,7 \\to 5$ 中，因此 HITS 权威值流向节点 $5$ 而非节点 $4$。**\n  - 该选项正确地指出了节点 $4$。\n  - 它正确地陈述了节点 $4$ 从强连通分量 $\\{1, 2, 3\\}$ 接收输入。\n  - 它正确地陈述了这个邻域本身具有很高的特征向量中心性（并传递给了节点 $4$）。\n  - 它正确地指出了节点 $1, 2, 3$ 在 HITS 下是弱中心（它们的中心分数为零）。\n  - 它正确地解释了 HITS 分数为零的*原因*：迭代的 HITS 过程由子图 $\\{6, 7\\} \\to \\{5\\}$ 主导，这对应于矩阵 $AA^{\\top}$ 和 $A^{\\top}A$ 的主特征值（$\\lambda=18$）。因此，权威值集中在节点 $5$ 上。\n  - 这个解释是详尽和准确的。\n  - **结论：正确**\n\n- **B. 节点 $5$；它的特征向量中心性很高，因为它由强中心节点 $6$ 和 $7$ 馈入，但 HITS 权威值很低，因为它是一个没有出边的汇点，HITS 会惩罚这种情况。**\n  - 该选项指出了节点 $5$，但它的特征向量中心性为零，HITS 权威值很高。这与搜索标准相反。\n  - 其推理也是有缺陷的。没有出边（作为汇点）使一个节点成为一个差的中心，而不是差的权威。\n  - **结论：错误**\n\n- **C. 节点 $4$；它的特征向量中心性和 HITS 权威值排名同样高，因为在有向图上，这两种方法在归一化后是等价的。**\n  - 该选项正确地指出了节点 $4$，但提出了两个错误的断言。\n  - 首先，节点 $4$ 的特征向量中心性排名最高，而 HITS 权威值排名最低（为零）。它们不相等。\n  - 其次，特征向量中心性（基于 $A^{\\top}$）和 HITS 权威值（基于 $A^{\\top}A$）是根本不同的度量，在一般有向图上并不等价。\n  - **结论：错误**\n\n- **D. 节点 $1$；它因参与一个环而具有高特征向量中心性，但 HITS 权威值低，因为它指向低权威值的汇点节点 $4$，这降低了它的中心分数，从而抑制了它的权威值。**\n  - 虽然节点 $1$ 确实具有高特征向量中心性和低 HITS 权威值，但节点 $4$ 是一个更好的例子（最高的 EC 与零 HITS）。\n  - 为节点 $1$ 的低 HITS 权威值提供的推理是正确的（它的中心分数为零，因为它指向权威值为零的节点），但这只解释了节点 $1$ 的情况。选项 A 提供了一个更全局的解释，说明了为什么 HITS 分数在整个网络中会这样分布，这是节点 $1$-$4$ 分数低的根本原因。\n  - **结论：错误**\n\n分析证实，选项 A 是唯一正确识别了主要目标节点，并为两种中心性度量排名之间的差异提供了完整和准确解释的选项。", "answer": "$$\\boxed{A}$$", "id": "4364781"}]}