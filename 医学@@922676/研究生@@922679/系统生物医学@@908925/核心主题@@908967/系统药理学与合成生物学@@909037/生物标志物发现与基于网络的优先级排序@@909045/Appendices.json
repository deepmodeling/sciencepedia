{"hands_on_practices": [{"introduction": "在系统生物医学中，我们常常面临高维数据（例如，基因数量 $p$ 远大于样本数量 $n$）带来的挑战。Lasso（最小绝对收缩和选择算子）回归是一种强大的统计方法，它通过在目标函数中加入 $\\ell_1$ 范数惩罚项，能够同时进行特征选择和模型拟合。[@problem_id:4320540] 这项实践将引导您从第一性原理出发，推导并实现Lasso的核心算法——坐标下降法，从而掌握从海量分子特征中筛选出潜在生物标志物的关键技术。", "problem": "考虑最小绝对收缩和选择算子 (Lasso) 回归的目标函数，该函数由以下最小化问题定义：$$\\hat{\\beta} \\;=\\; \\arg\\min_{\\beta \\in \\mathbb{R}^p}\\left\\{\\frac{1}{2n}\\left\\|y - X\\beta\\right\\|_2^2 \\;+\\; \\lambda\\left\\|\\beta\\right\\|_1\\right\\},$$ 其中 $X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$y \\in \\mathbb{R}^n$ 是响应向量，$\\beta \\in \\mathbb{R}^p$ 是系数向量，$\\lambda \\in \\mathbb{R}_{\\ge 0}$ 是正则化参数，$\\|\\cdot\\|_2$ 表示欧几里得范数，$\\|\\cdot\\|_1$ 表示绝对值之和。在系统生物医学中，稀疏线性模型被广泛用于在相关的分子特征下发现生物标志物；两个相关的特征可以代表分子网络中相互作用基因的信号。推导必须从基本的凸性原理和次梯度最优性出发，不得假设使用快捷公式。您的任务有两部分：推导估计器条件，然后为具有2个相关特征的玩具问题计算Lasso解路径。\n\n使用以下基本原理：平方损失的凸性、$\\ell_1$ 惩罚项的可分离性以及次梯度最优性（卡罗需-库恩-塔克 (KKT) 条件）。通过对 $X$ 和 $y$ 进行中心化，假设截距为零，使得 $X$ 的列和向量 $y$ 的均值为0。从第一性原理推导坐标级最小化规则，并实现一个算法，对于每个玩具数据集，在三个正则化水平上计算Lasso系数：$$\\lambda_{\\max} \\;=\\; \\max_{j \\in \\{1,\\dots,p\\}} \\left|\\frac{1}{n} x_j^\\top y\\right|,$$ $$\\lambda_{\\text{mid}} \\;=\\; \\frac{1}{2}\\lambda_{\\max},$$ $$\\lambda_{0} \\;=\\; 0,$$ 其中 $x_j$ 表示 $X$ 的第 $j$ 列。对于 $\\lambda_0$，计算普通最小二乘解。\n\n您的程序必须实现一个正确的坐标下降方法，该方法使用通过最小化Lasso目标函数（相对于单个坐标）而产生的软阈值更新，并且必须在内部对数据进行中心化。该方法必须收敛到足够小的容差，以确保数值稳定性。\n\n测试套件：\n提供三个测试用例（每个用例 $p=2$）如下。对于每个案例 $k \\in \\{1,2,3\\}$，$X^{(k)}$ 和 $y^{(k)}$ 都被明确给出。所有数字都是实数，并且数据已经过均值中心化。\n\n案例 1（$n=8$，强正相关特征）：\n$$\nX^{(1)} \\;=\\;\n\\begin{bmatrix}\n0.9  0.85 \\\\\n0.7  0.75 \\\\\n-0.4  -0.35 \\\\\n0.1  0.2 \\\\\n-0.8  -0.7 \\\\\n1.2  1.1 \\\\\n-0.3  -0.25 \\\\\n-1.4  -1.6 \\\\\n\\end{bmatrix},\\quad\ny^{(1)} \\;=\\;\n\\begin{bmatrix}\n1.025 \\\\\n0.575 \\\\\n-0.375 \\\\\n0.0 \\\\\n-0.85 \\\\\n1.35 \\\\\n-0.425 \\\\\n-1.3 \\\\\n\\end{bmatrix}.\n$$\n\n案例 2（$n=6$，中度负相关特征）：\n$$\nX^{(2)} \\;=\\;\n\\begin{bmatrix}\n1.0  -0.9 \\\\\n0.5  -0.4 \\\\\n-0.5  0.6 \\\\\n-1.0  1.1 \\\\\n0.8  -0.7 \\\\\n-0.8  0.3 \\\\\n\\end{bmatrix},\\quad\ny^{(2)} \\;=\\;\n\\begin{bmatrix}\n-1.67 \\\\\n-0.87 \\\\\n1.03 \\\\\n1.83 \\\\\n-1.36 \\\\\n1.04 \\\\\n\\end{bmatrix}.\n$$\n\n案例 3（$n=10$，中度正相关特征）：\n$$\nX^{(3)} \\;=\\;\n\\begin{bmatrix}\n0.4  0.2 \\\\\n1.0  0.8 \\\\\n-0.3  -0.1 \\\\\n-0.9  -0.7 \\\\\n0.7  0.5 \\\\\n-1.1  -0.9 \\\\\n0.6  0.4 \\\\\n-0.2  -0.1 \\\\\n0.3  0.1 \\\\\n-0.5  -0.2 \\\\\n\\end{bmatrix},\\quad\ny^{(3)} \\;=\\;\n\\begin{bmatrix}\n0.41 \\\\\n0.85 \\\\\n-0.25 \\\\\n-0.83 \\\\\n0.66 \\\\\n-1.02 \\\\\n0.54 \\\\\n-0.18 \\\\\n0.27 \\\\\n-0.45 \\\\\n\\end{bmatrix}.\n$$\n\n对于每个案例 $k$，令 $n$ 为 $X^{(k)}$ 的行数，并计算 $\\lambda_{\\max}^{(k)}=\\max_j\\left|\\frac{1}{n}(x^{(k)}_j)^\\top y^{(k)}\\right|$，然后计算 $\\lambda_{\\text{mid}}^{(k)}=\\frac{1}{2}\\lambda_{\\max}^{(k)}$ 和 $\\lambda_0^{(k)}=0$。对于每个 $\\lambda \\in \\{\\lambda_{\\max}^{(k)},\\lambda_{\\text{mid}}^{(k)},\\lambda_0^{(k)}\\}$，计算Lasso解 $\\hat{\\beta}^{(k)}(\\lambda)$，其中 $\\hat{\\beta}^{(k)}(\\lambda) \\in \\mathbb{R}^2$。您必须为每个案例 $k$ 返回按以下固定顺序展平的系数列表：\n$$\n\\left[\\hat{\\beta}^{(k)}_1(\\lambda_{\\max}^{(k)}),\\;\\hat{\\beta}^{(k)}_2(\\lambda_{\\max}^{(k)}),\\;\\hat{\\beta}^{(k)}_1\\left(\\lambda_{\\text{mid}}^{(k)}\\right),\\;\\hat{\\beta}^{(k)}_2\\left(\\lambda_{\\text{mid}}^{(k)}\\right),\\;\\hat{\\beta}^{(k)}_1(\\lambda_0^{(k)}),\\;\\hat{\\beta}^{(k)}_2(\\lambda_0^{(k)})\\right],\n$$\n表示为实数。将所有案例的结果按 $k=1$、$k=2$、$k=3$ 的顺序串联成一个列表，并将每个系数四舍五入到 $6$ 位小数。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含所有案例的串联结果，形式为用方括号括起来的逗号分隔列表（例如，$\\left[\\text{result}_1,\\text{result}_2,\\dots\\right]$）。不应打印任何其他文本。输出为四舍五入到 $6$ 位小数的实数（浮点数）。\n\n您的推导必须从凸性和次梯度最优性出发，并且除了从第一性原理明确推导出的公式外，不得提供任何快捷公式。数值计算必须科学真实且自洽。", "solution": "该问题要求从第一性原理推导Lasso回归的坐标级更新规则，并实现该规则以在三个指定正则化水平上求解三个测试数据集的系数。\n\n**1. 理论推导**\n\nLasso回归的目标函数由下式给出：\n$$\nJ(\\beta) = \\frac{1}{2n}\\|y - X\\beta\\|_2^2 + \\lambda\\|\\beta\\|_1\n$$\n其中 $J(\\beta)$ 是系数向量 $\\beta \\in \\mathbb{R}^p$ 的函数。其他量包括设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$、响应向量 $y \\in \\mathbb{R}^n$ 和非负正则化参数 $\\lambda$。\n\n**1.1. 凸性与最优性**\n目标函数 $J(\\beta)$ 是两个函数之和：残差平方和 (RSS) 项 $f(\\beta) = \\frac{1}{2n}\\|y - X\\beta\\|_2^2$ 和 $\\ell_1$-惩罚项 $g(\\beta) = \\lambda\\|\\beta\\|_1$。\n- RSS项 $f(\\beta)$ 是 $\\beta$ 的二次函数，它是可微且凸的。其Hessian矩阵 $\\frac{1}{n}X^\\top X$ 是半正定的。\n- $\\ell_1$-范数 $\\|\\beta\\|_1 = \\sum_{j=1}^p |\\beta_j|$ 是一个凸函数。由于 $\\lambda \\ge 0$，缩放后的惩罚项 $g(\\beta)$ 也是凸的。\n- 两个凸函数之和是凸函数。因此，$J(\\beta)$ 是一个凸函数。\n\n对于一个凸函数，当且仅当零向量包含在 $J$于$\\hat{\\beta}$ 处的次微分中时，点 $\\hat{\\beta}$ 是全局最小值点。次微分 $\\partial J(\\hat{\\beta})$ 是所有次梯度的集合。这个最优性条件表示为：\n$$\n0 \\in \\partial J(\\hat{\\beta})\n$$\n由于 $f(\\beta)$ 是可微的，和的次微分等于 $f(\\beta)$ 的梯度与 $g(\\beta)$ 的次微分之和：\n$$\n\\partial J(\\beta) = \\{\\nabla f(\\beta)\\} + \\partial g(\\beta)\n$$\nRSS项的梯度是：\n$$\n\\nabla f(\\beta) = \\frac{1}{n} X^\\top(X\\beta - y) = -\\frac{1}{n} X^\\top(y - X\\beta)\n$$\n$\\ell_1$-范数是可分离的，即 $\\|\\beta\\|_1 = \\sum_{j=1}^p |\\beta_j|$。它的次微分是其各分量次微分的笛卡尔积：$\\partial \\|\\beta\\|_1 = \\partial|\\beta_1| \\times \\dots \\times \\partial|\\beta_p|$。绝对值函数 $|\\beta_j|$ 的次微分是：\n$$\n\\partial |\\beta_j| = \\begin{cases} \\{1\\} & \\text{if } \\beta_j > 0 \\\\ \\{-1\\} & \\text{if } \\beta_j < 0 \\\\ [-1, 1] & \\text{if } \\beta_j = 0 \\end{cases}\n$$\n这可以紧凑地写为 $\\text{sgn}(\\beta_j)$（当 $\\beta_j \\ne 0$）和 $[-1, 1]$（当 $\\beta_j = 0$）。\n\n结合这些，最优性条件 $0 \\in \\partial J(\\hat{\\beta})$ 转化为对每个分量 $j \\in \\{1, \\dots, p\\}$ 的一组条件：\n$$\n-\\frac{1}{n} x_j^\\top(y - X\\hat{\\beta}) + \\lambda s_j = 0, \\quad \\text{其中 } s_j \\in \\partial|\\hat{\\beta}_j|\n$$\n这里，$x_j$ 是 $X$ 的第 $j$ 列。\n\n**1.2. 坐标级最小化**\n坐标下降算法每次只针对单个系数 $\\beta_j$ 最小化目标函数，同时保持所有其他系数 $\\beta_{k \\ne j}$ 固定。我们寻求找到使下式最小化的 $\\beta_j$ 的值：\n$$\nJ(\\beta_j) = \\frac{1}{2n}\\left\\|y - \\sum_{k \\ne j}x_k\\beta_k - x_j\\beta_j\\right\\|_2^2 + \\lambda\\sum_{k \\ne j}|\\beta_k| + \\lambda|\\beta_j|\n$$\n忽略不依赖于 $\\beta_j$ 的项，最小化等价于：\n$$\n\\hat{\\beta}_j = \\arg\\min_{\\beta_j \\in \\mathbb{R}} \\left\\{ \\frac{1}{2n}\\|r_{-j} - x_j\\beta_j\\|_2^2 + \\lambda|\\beta_j| \\right\\}\n$$\n其中 $r_{-j} = y - \\sum_{k \\ne j}x_k\\beta_k$ 是部分残差向量。\n\n让我们求这个一维目标函数关于 $\\beta_j$ 的次梯度。二次部分的导数是：\n$$\n\\frac{d}{d\\beta_j}\\left(\\frac{1}{2n}\\|r_{-j} - x_j\\beta_j\\|_2^2\\right) = \\frac{1}{n} (-x_j^\\top)(r_{-j} - x_j\\beta_j) = \\frac{1}{n} (x_j^\\top x_j \\beta_j - x_j^\\top r_{-j})\n$$\n这个一维问题的次梯度最优性条件是：\n$$\n0 \\in \\frac{1}{n} (x_j^\\top x_j \\beta_j - x_j^\\top r_{-j}) + \\lambda \\partial|\\beta_j|\n$$\n这可以重写为：\n$$\n\\frac{1}{n} x_j^\\top r_{-j} - \\frac{1}{n} (x_j^\\top x_j) \\beta_j \\in \\lambda \\partial|\\beta_j|\n$$\n让我们分析 $\\beta_j$ 的三种情况：\n1. 如果 $\\beta_j > 0$，则 $\\partial|\\beta_j| = \\{1\\}$。方程变为 $\\frac{1}{n} x_j^\\top r_{-j} - \\frac{1}{n} (x_j^\\top x_j) \\beta_j = \\lambda$。解出 $\\beta_j$ 得 $\\beta_j = \\frac{x_j^\\top r_{-j} - n\\lambda}{x_j^\\top x_j}$。为使此解一致（$\\beta_j > 0$），我们必须有 $x_j^\\top r_{-j} > n\\lambda$。\n2. 如果 $\\beta_j < 0$，则 $\\partial|\\beta_j| = \\{-1\\}$。方程变为 $\\frac{1}{n} x_j^\\top r_{-j} - \\frac{1}{n} (x_j^\\top x_j) \\beta_j = -\\lambda$。解出 $\\beta_j$ 得 $\\beta_j = \\frac{x_j^\\top r_{-j} + n\\lambda}{x_j^\\top x_j}$。为保持一致性（$\\beta_j < 0$），我们必须有 $x_j^\\top r_{-j} < -n\\lambda$。\n3. 如果 $\\beta_j = 0$，则 $\\partial|\\beta_j| = [-1, 1]$。条件是 $|\\frac{1}{n} x_j^\\top r_{-j}| \\le \\lambda$，可简化为 $|x_j^\\top r_{-j}| \\le n\\lambda$。\n\n结合这些情况，我们得到更新规则。令 $\\tilde{c}_j = x_j^\\top r_{-j} = x_j^\\top (y - \\sum_{k \\ne j}x_k\\beta_k)$ 且 $\\tilde{a}_j = x_j^\\top x_j$。$\\beta_j$ 的更新为：\n$$\n\\beta_j \\leftarrow \\begin{cases} (\\tilde{c}_j - n\\lambda) / \\tilde{a}_j & \\text{if } \\tilde{c}_j > n\\lambda \\\\ (\\tilde{c}_j + n\\lambda) / \\tilde{a}_j & \\text{if } \\tilde{c}_j < -n\\lambda \\\\ 0 & \\text{if } |\\tilde{c}_j| \\le n\\lambda \\end{cases}\n$$\n这被称为软阈值更新。使用软阈值算子 $S_\\alpha(z) = \\text{sgn}(z)\\max(0, |z|-\\alpha)$，更新可以写为：\n$$\n\\beta_j \\leftarrow \\frac{S_{n\\lambda}(\\tilde{c}_j)}{\\tilde{a}_j}\n$$\n坐标下降算法循环遍历 $j=1, \\dots, p$，应用此更新规则，直到系数向量 $\\beta$ 收敛。\n\n**1.3. 正则化参数选择**\n问题指定了三个正则化水平：\n- $\\lambda_0 = 0$：这对应于普通最小二乘法 (OLS) 回归。解为 $\\hat{\\beta}_{\\text{OLS}} = (X^\\top X)^{-1}X^\\top y$，假设 $X^\\top X$ 是可逆的。\n- $\\lambda_{\\max} = \\max_{j} |\\frac{1}{n} x_j^\\top y|$：这是 $\\lambda$ 的一个临界值，当 $\\lambda$ 高于此值时，解保证为 $\\hat{\\beta}=0$。从 $\\hat{\\beta}=0$ 的 KKT 条件可知，对于所有 $j$，我们有 $|\\frac{1}{n}x_j^\\top y| \\le \\lambda$。因此，对于任何 $\\lambda \\ge \\max_j |\\frac{1}{n}x_j^\\top y|$，$\\hat{\\beta}=0$ 是唯一解。在 $\\lambda=\\lambda_{\\max}$ 时，解为 $\\hat{\\beta}=0$。\n- $\\lambda_{\\text{mid}} = \\frac{1}{2}\\lambda_{\\max}$：这个中间值通常会产生一个稀疏但非全零的解。\n\n**2. 算法实现**\n对于每个测试用例，实现将遵循以下步骤：\n1. 确认输入数据 $X$ 和 $y$ 已按规定进行了均值中心化。令 $n$ 为样本数。\n2. 计算 $\\lambda_{\\max} = \\max_j |\\frac{1}{n} x_j^\\top y|$。设置 $\\lambda_{\\text{mid}} = 0.5 \\lambda_{\\max}$ 和 $\\lambda_0 = 0$。\n3. 对于每个 $\\lambda \\in \\{\\lambda_{\\max}, \\lambda_{\\text{mid}}\\}$，执行坐标下降：\n    a. 初始化 $\\beta = 0 \\in \\mathbb{R}^p$。\n    b. 为每个 $j$ 预计算 $\\tilde{a}_j = x_j^\\top x_j$。\n    c. 迭代直至收敛：\n        i. 对于 $j = 1, \\dots, p$：\n            - 计算 $\\tilde{c}_j = x_j^\\top (y - X\\beta + x_j\\beta_j)$。\n            - 应用软阈值更新：$\\beta_j \\leftarrow S_{n\\lambda}(\\tilde{c}_j) / \\tilde{a}_j$。\n        ii. 检查收敛性，例如，如果 $\\beta$ 的变化量小于一个小的容差。\n4. 对于 $\\lambda_0 = 0$，直接计算OLS解：$\\hat{\\beta} = (X^\\top X)^{-1}X^\\top y$。\n5. 按规定收集并格式化生成的系数。", "answer": "[0.000000,0.000000,0.505036,0.165430,0.472915,0.518607,-0.000000,-0.000000,-0.231998,-0.662058,-0.323529,-1.338235,0.000000,0.000000,0.413289,0.407147,0.689502,0.303046]", "id": "4320540"}, {"introduction": "识别出候选生物标志物之后，下一步是探究它们在复杂生物系统中的功能背景。加权基因共表达网络分析 (WGCNA) 是一个核心工具，它基于“关联有罪”（guilt-by-association）原则，即表达模式相似的基因可能参与共同的生物学过程。[@problem_id:4320638] 在这项实践中，您将学习构建这种网络的基础步骤：计算基因间的皮尔逊相关性，应用软阈值构建加权邻接矩阵，并计算节点的度，这些都是理解基因间相互作用和识别关键调控因子的基础。", "problem": "您的任务是构建一个加权基因共表达网络，并计算节点度，用于生物标志物发现和基于网络的优先级排序。从以下基本原则开始：两个基因在不同样本间表达谱的皮尔逊积矩相关系数（PPMCC）、由相关性大小构建的加权邻接矩阵的定义，以及加权网络中节点度的图论定义。\n\n定义和要求：\n- 设存在 $n$ 个样本和 $p$ 个基因。将表达矩阵表示为 $X \\in \\mathbb{R}^{n \\times p}$，其中第 $i$ 列是基因 $i$ 在 $n$ 个样本中的表达向量 $x_i \\in \\mathbb{R}^n$。\n- 基因 $i$ 和 $j$ 之间的皮尔逊积矩相关系数（PPMCC）为\n$$\nr_{ij} = \\frac{\\sum_{t=1}^{n} \\left(x_{it} - \\bar{x}_i\\right)\\left(x_{jt} - \\bar{x}_j\\right)}{\\sqrt{\\left(\\sum_{t=1}^{n} \\left(x_{it} - \\bar{x}_i\\right)^2\\right)\\left(\\sum_{t=1}^{n} \\left(x_{jt} - \\bar{x}_j\\right)^2\\right)}},\n$$\n其中 $\\bar{x}_i$ 是 $x_i$ 在 $n$ 个样本中的均值。如果 $\\sum_{t=1}^{n} \\left(x_{it} - \\bar{x}_i\\right)^2 = 0$ 或 $\\sum_{t=1}^{n} \\left(x_{jt} - \\bar{x}_j\\right)^2 = 0$（即某个基因在所有样本中表达量恒定），则定义 $r_{ij} = 0$。\n- 构建一个加权邻接矩阵 $W \\in \\mathbb{R}^{p \\times p}$，其元素为\n$$\nw_{ij} = \\begin{cases}\n\\left| r_{ij} \\right|^{\\beta},  i \\neq j, \\\\\n0,  i = j,\n\\end{cases}\n$$\n其中 $\\beta > 0$ 是一个给定的软阈值幂参数。\n- 节点 $i$ 的加权度为\n$$\nk_i = \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{p} w_{ij}.\n$$\n\n您的程序必须精确实现这些定义，并为指定测试套件中的每个基因计算加权度 $k_i$。不涉及任何物理单位。在最终输出中，将每个度 $k_i$ 表示为至少有六位小数精度的浮点数。\n\n测试套件：\n- 案例 1：$X$ 有 $n = 6$ 个样本和 $p = 5$ 个基因。\n  - $x_1 = [1,2,3,4,5,6]$\n  - $x_2 = [6,5,4,3,2,1]$\n  - $x_3 = [2,4,6,8,10,12]$\n  - $x_4 = [1,1,2,2,3,3]$\n  - $x_5 = [3,3,3,3,3,3]$\n  - $\\beta = 2$\n- 案例 2：$X$ 有 $n = 5$ 个样本和 $p = 4$ 个基因。\n  - $x_1 = [0,0,0,0,0]$\n  - $x_2 = [1,2,3,4,5]$\n  - $x_3 = [2,1,0,-1,-2]$\n  - $x_4 = [5,4,3,2,1]$\n  - $\\beta = 1$\n- 案例 3：$X$ 有 $n = 3$ 个样本和 $p = 2$ 个基因。\n  - $x_1 = [1,2,3]$\n  - $x_2 = [3,2,1]$\n  - $\\beta = 3$\n- 案例 4：$X$ 有 $n = 4$ 个样本和 $p = 3$ 个基因。\n  - $x_1 = [1,0,-1,0]$\n  - $x_2 = [1,1,1,1]$\n  - $x_3 = [1,-1,1,-1]$\n  - $\\beta = 10$\n- 案例 5：$X$ 有 $n = 6$ 个样本和 $p = 3$ 个基因。\n  - $x_1 = [0,1,0,1,0,1]$\n  - $x_2 = [0.1,0.9,0.2,0.8,0.05,0.95]$\n  - $x_3 = [-0.1,-0.9,-0.2,-0.8,-0.05,-0.95]$\n  - $\\beta = 4$\n\n最终输出格式：\n- 您的程序应生成一行输出，包含一个用方括号括起来的逗号分隔列表。此顶层列表的每个元素对应一个测试案例，并且本身必须是该案例的 $p$ 个加权度列表 $[k_1, k_2, \\ldots, k_p]$。例如，对于以上五个案例，输出应类似于 $[[\\dots],[\\dots],[\\dots],[\\dots],[\\dots]]$，其中浮点值至少有六位小数精度。", "solution": "我们将加权共表达网络的构建基于两个公认的原则：统计学中基于相关的相似性和图论中的节点度。在系统生物医学中，候选生物标志物的基于网络的优先级排序通常依赖于从共表达关系中导出的中心性度量，例如在加权基因共表达网络分析（WGCNA）中，邻接矩阵是基于软阈值化的相似性构建的。我们按以下步骤进行。\n\n两个基因 $i$ 和 $j$ 之间的皮尔逊积矩相关系数（PPMCC）是衡量线性关联的标准化度量。给定 $x_i \\in \\mathbb{R}^n$ 和 $x_j \\in \\mathbb{R}^n$，样本均值分别为 $\\bar{x}_i$ 和 $\\bar{x}_j$，分子\n$$\n\\sum_{t=1}^{n} \\left(x_{it} - \\bar{x}_i\\right)\\left(x_{jt} - \\bar{x}_j\\right)\n$$\n是未归一化的协方差，分母\n$$\n\\sqrt{\\left(\\sum_{t=1}^{n} \\left(x_{it} - \\bar{x}_i\\right)^2\\right)\\left(\\sum_{t=1}^{n} \\left(x_{jt} - \\bar{x}_j\\right)^2\\right)}\n$$\n是未归一化的标准差的乘积。它们的比率消除了单位和尺度，得到 $r_{ij} \\in [-1,1]$。这与经过充分检验的统计实践一致，并且与协方差中使用除数 $n$ 还是 $n-1$ 无关，因为这些因子在相关性比率中会抵消。当一个基因在所有样本中表达量恒定时，其方差为零，分母为零，$r_{ij}$ 未定义；在操作上，我们对任何涉及此类基因的对设置 $r_{ij} = 0$，以避免引入人为的边。\n\n为了构建加权邻接矩阵，我们使用相关性绝对值大小 $\\left|r_{ij}\\right|$ 来反映共表达强度，无论方向（正或负）。这在基于网络的优先级排序中是一个常见且有科学动机的选择，当目标是量化连通性时。然后我们应用一个软阈值幂 $\\beta > 0$：\n$$\nw_{ij} = \\begin{cases}\n\\left| r_{ij} \\right|^{\\beta},  i \\neq j, \\\\\n0,  i = j,\n\\end{cases}\n$$\n其中 $w_{ii} = 0$ 以移除自环。参数 $\\beta$ 控制对强相关的强调程度：较大的 $\\beta$ 会抑制较弱的边并凸显较强的边，这是用于在共表达网络中促进近似无标度拓扑的一个原则。\n\n节点 $i$ 的加权度（也称为强度）为\n$$\nk_i = \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{p} w_{ij},\n$$\n它量化了基因 $i$ 与所有其他基因的总连通性。在生物标志物发现中，较大的 $k_i$ 表明其网络中心性更高，可能具有更重要的调控作用。\n\n算法步骤：\n- 对于每个测试案例，计算每个基因 $i$ 的均值 $\\bar{x}_i$，通过减去其均值来中心化每个基因向量，并计算每个基因的未归一化平方和。对于每对 $(i,j)$，计算未归一化的协方差，即中心化向量的逐元素乘积之和。如果任一基因的平方和为零，则设置 $r_{ij} = 0$；否则计算\n$$\nr_{ij} = \\frac{\\sum_{t=1}^{n} \\left(x_{it} - \\bar{x}_i\\right)\\left(x_{jt} - \\bar{x}_j\\right)}{\\sqrt{\\left(\\sum_{t=1}^{n} \\left(x_{it} - \\bar{x}_i\\right)^2\\right)\\left(\\sum_{t=1}^{n} \\left(x_{jt} - \\bar{x}_j\\right)^2\\right)}}.\n$$\n- 对于 $i \\neq j$，通过 $w_{ij} = \\left|r_{ij}\\right|^{\\beta}$ 构建 $W$，并设置 $w_{ii} = 0$。\n- 对每个 $i$ 计算 $k_i = \\sum_{j \\neq i} w_{ij}$。\n\n测试套件涵盖的边界情况：\n- 案例 1 包括完全正相关和负相关、一个中等相关性，以及一个恒定基因 ($x_5$)。例如，$x_1$ 和 $x_3$ 是线性相关的 ($x_3 = 2 x_1$)，得到 $r_{13} = 1$，因此 $w_{13} = 1^{2} = 1$，而 $x_1$ 和 $x_2$ 是完全反相关的 ($r_{12} = -1$, $w_{12} = 1$)。$x_1$ 和 $x_4$ 之间的相关性很高但并非完全；具体来说，\n$$\nr_{14} = \\frac{8}{\\sqrt{70}} \\approx 0.956182887,\\quad w_{14} \\approx (0.956182887)^{2} \\approx 0.914297,\n$$\n而 $x_5$ 是恒定的，所以所有的 $r_{i5} = 0$ 且 $w_{i5} = 0$。加权度 $k_i$ 是这些贡献的总和。\n- 案例 2 强调了对恒定基因 ($x_1$) 和完全负相关（例如 $r_{23} = -1$, $r_{24} = -1$）的处理，其中 $\\beta = 1$，因此 $w_{ij} = \\left| r_{ij} \\right|$。\n- 案例 3 是一个最小网络 ($p = 2$)，具有完全反相关和 $\\beta = 3$，得到 $w_{12} = 1$ 且 $k_1 = k_2 = 1$。\n- 案例 4 包括一个恒定基因 ($x_2$) 和一个正交对 ($x_1$ vs $x_3$)，产生 $r_{13} = 0$；由于 $\\beta = 10$，所有的度都为 $0$。\n- 案例 5 包含强但非完全的相关和反相关，展示了 $\\beta = 4$ 在强调强边同时保留细微差异的效果。例如，对于中心化的向量 $x_1$ 和 $x_2$，可以得到\n$$\nr_{12} \\approx \\frac{1.15}{\\sqrt{1.5 \\cdot 0.915}} \\approx 0.9827,\\quad w_{12} \\approx (0.9827)^{4} \\approx 0.931,\n$$\n类似地，$r_{23} \\approx -0.989$ 得到 $w_{23} \\approx 0.957$。度 $k_i$ 是相应入射权重的总和。\n\n实现细节：\n- 使用数组表示 $X$，并根据上述公式计算均值、中心化向量、平方和以及成对相关性。\n- 构建 $W$ 时，对角线为零，对非对角线上的 $\\left|r_{ij}\\right|$ 应用幂 $\\beta$。\n- 通过对行求和（不包括对角线）来计算 $k_i$。\n\n程序返回一行，其中包含五个案例的度列表的列表，格式为 $[[k_1,\\dots,k_p],\\dots]$，浮点值具有至少六位小数的精度，以确保下游优先级比较的稳定性。", "answer": "[[2.914286, 2.914286, 2.914286, 2.742857, 0.0],[0.0, 2.0, 2.0, 2.0],[1.0, 1.0],[0.0, 0.0, 0.0],[1.862846, 1.931423, 1.931423]]", "id": "4320638"}, {"introduction": "拥有一个生物分子互作网络后，我们可以利用其拓扑结构来对候选生物标志物进行排序。网络扩散算法，如带重启的随机游走 (RWR)，通过模拟信息从一组已知的“种子”节点（例如已知的疾病基因）在网络中传播的过程，为所有节点赋予一个优先级分数。[@problem_id:4320621] 本实践将带您实现并比较两种不同的网络归一化扩散策略，揭示算法选择如何影响网络中心节点（hubs）的排序，这对于深入理解和应用网络优先化方法至关重要。", "problem": "考虑一个基因或蛋白质相互作用网络，它被表示为一个包含 $n$ 个节点的有限、简单、无向、加权图。设该网络由一个邻接矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 编码，其中 $A_{ij} \\ge 0$ 且 $A_{ij} = A_{ji}$。设节点 $i$ 的度为 $d_i = \\sum_{j=1}^{n} A_{ij}$，度矩阵为 $D = \\mathrm{diag}(d_1,\\dots,d_n)$。在基于网络的生物标志物优先化中，初始信号 $s \\in \\mathbb{R}^{n}$ 在网络上的扩散被用来获得最终的优先化得分 $x \\in \\mathbb{R}^{n}$，并根据这些得分对节点进行排名。两种常用的扩散度归一化方法是行随机归一化和对称归一化，它们可能会产生不同的排名，特别是对于高度节点（集散节点）。\n\n从马尔可夫链中的概率守恒和图信号归一化的基本原则出发，实现一个与这些原则一致的迭代扩散过程。请使用以下规范：\n\n1. 扩散状态更新必须遵循重复传播与初始信号 $s$ 再注入的思想，捕捉带重启的随机游走（RWR）的概念。在每次迭代中，新状态必须通过组合传播分量和初始信号 $s$ 的再注入分量来计算，由参数 $\\alpha$（其中 $0 < \\alpha < 1$）控制，确保收缩并收敛到一个不动点。\n\n2. 实现传播算子 $W$ 的两种变体：\n   - 行随机归一化：基于 $D$ 和 $A$ 构建一个矩阵 $W_{\\mathrm{row}}$，使得对于度 $d_i > 0$ 的节点，其行和为 $1$。度 $d_i = 0$ 的节点不得传播任何质量。\n   - 对称归一化：使用 $D$ 和 $A$ 通过对称度归一化推导出一个矩阵 $W_{\\mathrm{sym}}$，以减少集散节点的主导作用并考虑图的几何结构。度 $d_i = 0$ 的节点不得传播任何质量。\n\n3. 对于给定的种子集 $S \\subset \\{1,\\dots,n\\}$，定义一个初始信号 $s$，将相等的质量置于 $S$ 中的节点上，其他节点上为零。总质量必须为 $1$。\n\n4. 迭代扩散直至收敛，收敛条件为连续迭代结果之差的范数小于指定的容差，或达到最大迭代次数。\n\n5. 收敛后，分别计算 $W_{\\mathrm{row}}$ 和 $W_{\\mathrm{sym}}$ 的最终得分 $x_{\\mathrm{row}}$ 和 $x_{\\mathrm{sym}}$。按得分降序对节点进行排名。如果出现平分，则通过较小的节点索引来打破平局（即，索引较小的节点获得更好的排名）。将排序转换为排名位置 $r_{\\mathrm{row}}(i)$ 和 $r_{\\mathrm{sym}}(i)$，其中 $1$ 为最佳排名。\n\n6. 将高度节点集 $H$ 定义为度 $d_i$ 最高的 $k$ 个节点，平局由较小的索引打破。计算高度节点的有符号排名变化，$\\Delta r(i) = r_{\\mathrm{sym}}(i) - r_{\\mathrm{row}}(i)$，适用于所有 $i \\in H$。一个正的 $\\Delta r(i)$ 意味着该节点在对称归一化下的排名比在行随机归一化下更差（数值排名更高）；一个负值意味着其排名更好。\n\n7. 对于每个测试用例，产生三个输出：\n   - $H$ 上的平均绝对排名变化，为一个实数。\n   - $H$ 上的最大绝对排名变化，为一个整数。\n   - 有符号排名变化列表 $\\left[\\Delta r(i)\\right]_{i \\in H}$，按节点索引递增排序。\n\n您的程序应生成一行输出，其中包含一个逗号分隔的列表，列表本身用方括号括起来（例如，$[result\\_1,result\\_2,\\dots]$），其中每个 $result\\_t$ 本身就是一个列表 $[\\text{mean\\_abs\\_change},\\text{max\\_abs\\_change},[\\Delta r(i) \\text{ for } i \\in H]]$。\n\n为以下测试套件实现上述功能。每个测试用例提供 $A$、一个种子集 $S$、一个重启参数 $\\alpha$、一个高度节点截断值 $k$、最大迭代次数 $T_{\\max}$ 和一个容差 $\\varepsilon$。矩阵以显式形式给出，行和列的索引从 $0$ 到 $n-1$。\n\n测试用例 1 (正常路径，单个连通分量带一个集散节点):\n- $n = 6$\n- $$A = \\begin{bmatrix}\n0  3  2  4  1  1 \\\\\n3  0  2  0  0  0 \\\\\n2  2  0  1  0  0 \\\\\n4  0  1  0  2  0 \\\\\n1  0  0  2  0  3 \\\\\n1  0  0  0  3  0\n\\end{bmatrix}$$\n- $S = \\{4,5\\}$\n- $\\alpha = 0.85$\n- $k = 2$\n- $T_{\\max} = 10000$\n- $\\varepsilon = 10^{-12}$\n\n测试用例 2 (边缘案例，一个星形集散节点和一个局部三角形):\n- $n = 7$\n- $$A = \\begin{bmatrix}\n0  2  2  2  2  2  0 \\\\\n2  0  0  0  0  0  0 \\\\\n2  0  0  0  0  0  0 \\\\\n2  0  0  0  0  2  1 \\\\\n2  0  0  0  0  0  0 \\\\\n2  0  0  2  0  0  3 \\\\\n0  0  0  1  0  3  0\n\\end{bmatrix}$$\n- $S = \\{0\\}$\n- $\\alpha = 0.90$\n- $k = 2$\n- $T_{\\max} = 10000$\n- $\\varepsilon = 10^{-12}$\n\n测试用例 3 (边界条件，两个不连通分量，每个分量中都有集散节点):\n- $n = 8$\n- $$A = \\begin{bmatrix}\n0  4  0  0  0  0  0  0 \\\\\n4  0  3  2  0  0  0  0 \\\\\n0  3  0  1  0  0  0  0 \\\\\n0  2  1  0  0  0  0  0 \\\\\n0  0  0  0  0  1  2  0 \\\\\n0  0  0  0  1  0  2  1 \\\\\n0  0  0  0  2  2  0  2 \\\\\n0  0  0  0  0  1  2  0\n\\end{bmatrix}$$\n- $S = \\{0,6\\}$\n- $\\alpha = 0.70$\n- $k = 3$\n- $T_{\\max} = 10000$\n- $\\varepsilon = 10^{-12}$\n\n所有输出必须在没有任何外部输入的情况下产生。您的程序必须使用这些确切的测试用例，并按照上述确切格式输出单行结果。", "solution": "### 步骤 1：提取已知条件\n\n问题提供了以下数据和定义：\n- 基因/蛋白质相互作用网络是一个包含 $n$ 个节点的有限、简单、无向、加权图。\n- 邻接矩阵：$A \\in \\mathbb{R}^{n \\times n}$，其中 $A_{ij} \\ge 0$ 且 $A_{ij} = A_{ji}$。\n- 节点 $i$ 的度：$d_i = \\sum_{j=1}^{n} A_{ij}$。\n- 度矩阵：$D = \\mathrm{diag}(d_1,\\dots,d_n)$。\n- 初始信号向量：$s \\in \\mathbb{R}^{n}$。\n- 最终优先化得分向量：$x \\in \\mathbb{R}^{n}$。\n- 扩散更新规则：基于“重复传播与初始信号 $s$ 再注入”的迭代过程，由参数 $\\alpha$（其中 $0 < \\alpha < 1$）控制。\n- 传播算子 $W$：\n    - 行随机归一化：$W_{\\mathrm{row}}$，对于度 $d_i > 0$ 的节点，其行和为 $1$。度 $d_i = 0$ 的节点不传播质量。\n    - 对称归一化：$W_{\\mathrm{sym}}$，使用 $D$ 和 $A$ 通过对称度归一化推导。度 $d_i = 0$ 的节点不传播质量。\n- 初始信号定义：对于种子集 $S$，$s$ 在 $S$ 中的节点上具有相等的质量，在其他节点上为零，总质量为 $1$。\n- 收敛标准：连续迭代结果之差的L2范数小于容差 $\\varepsilon$，或达到最大迭代次数 $T_{\\max}$。\n- 排名：节点按得分降序排名。平局由较小的节点索引打破。排名 $r(i)$ 从 $1$ 开始。\n- 高度节点 $H$：度 $d_i$ 最高的 $k$ 个节点的集合，平局由较小的节点索引打破。\n- 对于 $i \\in H$ 的有符号排名变化：$\\Delta r(i) = r_{\\mathrm{sym}}(i) - r_{\\mathrm{row}}(i)$。\n- 每个测试用例的必需输出：一个包含 $H$ 上的平均绝对排名变化、$H$ 上的最大绝对排名变化以及按节点索引递增排序的有符号排名变化列表 $[\\Delta r(i)]_{i \\in H}$ 的列表。\n- 测试用例 1：$n=6$， $A=\\begin{bmatrix} 0  3  2  4  1  1 \\\\ 3  0  2  0  0  0 \\\\ 2  2  0  1  0  0 \\\\ 4  0  1  0  2  0 \\\\ 1  0  0  2  0  3 \\\\ 1  0  0  0  3  0 \\end{bmatrix}$，$S=\\{4,5\\}$，$\\alpha=0.85$，$k=2$，$T_{\\max}=10000$，$\\varepsilon=10^{-12}$。\n- 测试用例 2：$n=7$， $A=\\begin{bmatrix} 0  2  2  2  2  2  0 \\\\ 2  0  0  0  0  0  0 \\\\ 2  0  0  0  0  0  0 \\\\ 2  0  0  0  0  2  1 \\\\ 2  0  0  0  0  0  0 \\\\ 2  0  0  2  0  0  3 \\\\ 0  0  0  1  0  3  0 \\end{bmatrix}$，$S=\\{0\\}$，$\\alpha=0.90$，$k=2$，$T_{\\max}=10000$，$\\varepsilon=10^{-12}$。\n- 测试用例 3：$n=8$， $A=\\begin{bmatrix} 0  4  0  0  0  0  0  0 \\\\ 4  0  3  2  0  0  0  0 \\\\ 0  3  0  1  0  0  0  0 \\\\ 0  2  1  0  0  0  0  0 \\\\ 0  0  0  0  0  1  2  0 \\\\ 0  0  0  0  1  0  2  1 \\\\ 0  0  0  0  2  2  0  2 \\\\ 0  0  0  0  0  1  2  0 \\end{bmatrix}$，$S=\\{0,6\\}$，$\\alpha=0.70$，$k=3$，$T_{\\max}=10000$，$\\varepsilon=10^{-12}$。\n\n### 步骤 2：使用提取的已知条件进行验证\n\n根据验证标准对问题进行评估。\n\n- **科学依据**：该问题基于网络科学和图论中已成熟的原理，特别是带重启的随机游走（RWR）算法。邻接矩阵、度归一化（行随机和对称）以及迭代扩散等概念在计算系统生物学中用于基因优先化等任务是标准方法。该问题具有科学合理性。\n- **适定性**：该问题是适定的。它为每个测试用例提供了所有必要的参数（$A, S, \\alpha, k, T_{\\max}, \\varepsilon$）。将RWR算法描述为传播和再注入的组合，由 $\\alpha$ 控制，对应于迭代过程 $x_{t+1} = \\alpha W x_t + (1-\\alpha)s$。对于 $0 < \\alpha < 1$ 且 $W$ 是一个有效的传播算子（谱半径 $\\rho(W) \\le 1$），此过程是一个收缩映射，保证了唯一且稳定的不动点解的存在。排名和集散节点选择的平局打破规则确保了唯一的结果。\n- **客观性**：所有术语都通过数学和程序化方式定义。语言精确，没有主观或含糊的陈述。\n- **完整性与一致性**：该问题是自包含的。传播算子、初始信号、收敛、排名和最终指标的定义都已提供并且相互一致。对度 $d_i=0$ 的节点的处理已明确规定，防止了除以零的错误并确保算法的鲁棒性。\n- **现实性**：网络规模和参数值对于计算问题是现实的，没有可行性问题。\n\n### 步骤 3：结论与行动\n\n该问题是**有效的**。这是一个来自网络生物学领域、定义明确、有科学依据且计算上可行的问​​题。我现在将着手构建解决方案。\n\n### 基于原理的解决方案设计\n\n问题的核心是实现并比较带重启的随机游走（RWR）算法在图上的两种变体。该算法模拟一个过程，其中信号在网络中扩散，在每一步，它既可以继续传播到邻居，也可以从一组种子节点“重启”。\n\n**1. 网络和信号初始化**\n网络由其邻接矩阵 $A$ 表示。每个节点 $i$ 的度 $d_i$ 计算为其关联边权重的总和，$d_i = \\sum_{j} A_{ij}$。初始信号 $s$ 是一个向量，表示“信号质量”的初始分布。对于给定的种子节点集 $S$，信号在它们之间均匀分布，使得如果节点 $i$ 在 $S$ 中，则 $s_i = 1/|S|$，否则 $s_i = 0$。这确保了总信号质量归一化，即 $\\sum_{i} s_i = 1$。\n\n**2. 传播算子**\n扩散过程由一个传播算子 $W$ 控制，它是邻接矩阵 $A$ 的一个归一化版本。指定了两种归一化方案：\n\n- **行随机归一化 ($W_{\\mathrm{row}}$)**：这种归一化模拟了图上的经典随机游走。从节点 $i$ 到节点 $j$ 的转移概率与边权重 $A_{ij}$ 成正比。我们构建 $W_{\\mathrm{row}} = D^{-1}A$，其中 $D^{-1}$ 是度矩阵 $D$ 的逆。对于度为 $d_i > 0$ 的节点 $i$，$D^{-1}$ 的第 $(i,i)$ 个条目是 $1/d_i$。这确保了 $W_{\\mathrm{row}}$ 中对应于已连接节点的每一行之和为 $1$，从而在局部上守恒概率质量。如果 $d_i = 0$，则 $D^{-1}$ 中的相应条目为 $0$，确保孤立节点不传播任何信号，如规范所述。\n\n- **对称归一化 ($W_{\\mathrm{sym}}$)**：这种归一化通常更受青睐，因为它使算子对称，这对应于可逆的马尔可夫链，并具有良好的谱特性。它被定义为 $W_{\\mathrm{sym}} = D^{-1/2} A D^{-1/2}$，其中 $D^{-1/2}$ 是一个对角矩阵，其对角线上的条目对于度 $d_i > 0$ 的节点为 $1/\\sqrt{d_i}$，否则为 $0$。这种归一化倾向于减少高度节点（集散节点）的影响，因为与它们相连的边的权重比在行随机情况下被更重地降低了。\n\n**3. 迭代扩散（带重启的随机游走）**\n扩散过程是对得分向量 $x$ 的迭代更新，从 $x_0 = s$ 开始。更新规则结合了传播与再注入：\n$$x_{t+1} = \\alpha W x_t + (1-\\alpha)s$$\n这里，$\\alpha \\in (0, 1)$ 是继续概率。在每一步 $t$，每个节点上的一部分信号 $\\alpha$ 根据 $W$ 传播到其邻居，而剩余部分 $(1-\\alpha)$ 则从初始源分布 $s$ 中补充。由于迭代映射是收缩的，该过程保证收敛到一个唯一的稳态分布 $x_{ss}$。迭代持续进行，直到连续得分向量之间的变化可以忽略不计，即 $\\|x_{t+1} - x_t\\|_2 < \\varepsilon$，或直到达到最大迭代次数 $T_{\\max}$。\n\n**4. 排名与比较**\n收敛后，我们获得两个最终得分向量，$x_{\\mathrm{row}}$ 和 $x_{\\mathrm{sym}}$，对应于两种归一化方案。为了比较它们的效果，我们执行以下步骤：\n- **排名**：对于每个得分向量，根据得分降序对节点进行从 $1$ 到 $n$ 的排名。得分的平局通过将更好的排名分配给索引较小的节点来打破。这产生了两个排名向量，$r_{\\mathrm{row}}$ 和 $r_{\\mathrm{sym}}$。\n- **高度节点识别**：通过找到度 $d_i$ 最高的 $k$ 个节点来识别高度节点集 $H$。度的平局通过选择索引较小的节点来打破。\n- **排名变化分析**：对于高度节点集 $H$ 中的每个节点 $i$，计算有符号排名变化 $\\Delta r(i) = r_{\\mathrm{sym}}(i) - r_{\\mathrm{row}}(i)$。一个正的 $\\Delta r(i)$ 表示该节点在对称归一化下的排名不如在行随机归一化下（即，排名数字更大）。\n- **最终指标**：比较由三个指标总结：\n    1.  $H$ 中所有节点绝对排名变化的均值：$\\frac{1}{k} \\sum_{i \\in H} |\\Delta r(i)|$。\n    2.  $H$ 上绝对排名变化的最大值：$\\max_{i \\in H} |\\Delta r(i)|$。\n    3.  有符号排名变化列表 $[\\Delta r(i)]_{i \\in H}$，按节点索引 $i$ 递增排序。\n\n这种结构化的分析将系统地揭示在网络扩散背景下，归一化选择如何影响集散节点的优先化。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the execution for all test cases.\n    It defines the test cases and calls the core analysis function for each,\n    then formats and prints the final output.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"A\": np.array([\n                [0, 3, 2, 4, 1, 1],\n                [3, 0, 2, 0, 0, 0],\n                [2, 2, 0, 1, 0, 0],\n                [4, 0, 1, 0, 2, 0],\n                [1, 0, 0, 2, 0, 3],\n                [1, 0, 0, 0, 3, 0]\n            ]),\n            \"S\": {4, 5},\n            \"alpha\": 0.85,\n            \"k\": 2,\n            \"T_max\": 10000,\n            \"epsilon\": 1e-12\n        },\n        {\n            \"A\": np.array([\n                [0, 2, 2, 2, 2, 2, 0],\n                [2, 0, 0, 0, 0, 0, 0],\n                [2, 0, 0, 0, 0, 0, 0],\n                [2, 0, 0, 0, 0, 2, 1],\n                [2, 0, 0, 0, 0, 0, 0],\n                [2, 0, 0, 2, 0, 0, 3],\n                [0, 0, 0, 1, 0, 3, 0]\n            ]),\n            \"S\": {0},\n            \"alpha\": 0.90,\n            \"k\": 2,\n            \"T_max\": 10000,\n            \"epsilon\": 1e-12\n        },\n        {\n            \"A\": np.array([\n                [0, 4, 0, 0, 0, 0, 0, 0],\n                [4, 0, 3, 2, 0, 0, 0, 0],\n                [0, 3, 0, 1, 0, 0, 0, 0],\n                [0, 2, 1, 0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0, 1, 2, 0],\n                [0, 0, 0, 0, 1, 0, 2, 1],\n                [0, 0, 0, 0, 2, 2, 0, 2],\n                [0, 0, 0, 0, 0, 1, 2, 0]\n            ]),\n            \"S\": {0, 6},\n            \"alpha\": 0.70,\n            \"k\": 3,\n            \"T_max\": 10000,\n            \"epsilon\": 1e-12\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_diffusion_analysis(\n            A=case[\"A\"],\n            S=case[\"S\"],\n            alpha=case[\"alpha\"],\n            k=case[\"k\"],\n            T_max=case[\"T_max\"],\n            epsilon=case[\"epsilon\"]\n        )\n        results.append(result)\n\n    # Custom string formatting to avoid spaces and ensure required output format\n    result_strs = []\n    for res in results:\n        mean_val, max_val, delta_list = res\n        delta_str = '[' + ','.join(map(str, delta_list)) + ']'\n        result_strs.append(f'[{mean_val},{max_val},{delta_str}]')\n    \n    print(f\"[{','.join(result_strs)}]\")\n\ndef run_diffusion_analysis(A, S, alpha, k, T_max, epsilon):\n    \"\"\"\n    Performs the full diffusion analysis for a single test case.\n    \"\"\"\n    n = A.shape[0]\n\n    # Step 1: Calculate degrees and identify the high-degree node set H\n    degrees = A.sum(axis=1)\n    # Sort by degree (desc) and then index (asc)\n    node_degree_pairs = sorted(range(n), key=lambda i: (-degrees[i], i))\n    H = sorted(node_degree_pairs[:k])\n\n    # Step 2: Construct the initial signal vector s\n    s = np.zeros(n)\n    if S:\n        s[[i for i in S]] = 1.0 / len(S)\n\n    # Step 3: Define the diffusion iterator\n    def perform_rwr(W, s_vec, alpha_param, T_max_iter, eps):\n        x = np.copy(s_vec)\n        reinjection_term = (1 - alpha_param) * s_vec\n        for _ in range(T_max_iter):\n            x_new = alpha_param * (W @ x) + reinjection_term\n            if np.linalg.norm(x_new - x)  eps:\n                return x_new\n            x = x_new\n        return x\n\n    # Step 4: Construct propagation operators W_row and W_sym\n    # The problem description states W_row is a row-stochastic matrix, which means\n    # it's A normalized by rows. So W_row = D_inv @ A.\n    # The diffusion formula uses column-stochastic matrices. x_t+1 = W x_t.\n    # If x is a column vector of scores, W must be column-stochastic.\n    # The standard RWR uses a column-stochastic transition matrix, W_ij = A_ij / d_j.\n    # This corresponds to A @ D_inv.\n    # However, the problem text asks for row-stochastic normalization for W_row, and then\n    # implies using it in the formula. If x is a column vector, we should use the transpose.\n    # x_t+1 = alpha * (W_row.T @ x_t) + (1-alpha)s. Let's assume this standard.\n    with np.errstate(divide='ignore', invalid='ignore'):\n        d_inv = np.array([1.0 / d if d > 0 else 0 for d in degrees])\n        d_inv_sqrt = np.array([1.0 / np.sqrt(d) if d > 0 else 0 for d in degrees])\n    \n    D_inv = np.diag(d_inv)\n    D_inv_sqrt = np.diag(d_inv_sqrt)\n\n    # Row-stochastic W_row needs to be transposed for column-vector multiplication\n    W_row_T = (D_inv @ A).T \n    # W_sym is symmetric, so W_sym.T = W_sym\n    W_sym = D_inv_sqrt @ A @ D_inv_sqrt\n\n    # Step 5: Run diffusion for both normalizations\n    # We use W.T in the RWR function call to match the column-vector math.\n    # The Python implementation uses W @ x, so we need to pass the transposed matrix.\n    x_row = perform_rwr(W_row_T, s, alpha, T_max, epsilon)\n    x_sym = perform_rwr(W_sym, s, alpha, T_max, epsilon)\n\n    # Step 6: Define a ranking function\n    def get_ranks(scores):\n        n_nodes = len(scores)\n        # Sort by score (desc) and then index (asc)\n        sorted_nodes = sorted(range(n_nodes), key=lambda i: (-scores[i], i))\n        ranks = np.zeros(n_nodes, dtype=int)\n        for rank_pos, node_idx in enumerate(sorted_nodes):\n            ranks[node_idx] = rank_pos + 1\n        return ranks\n\n    # Step 7: Calculate ranks\n    r_row = get_ranks(x_row)\n    r_sym = get_ranks(x_sym)\n\n    # Step 8: Calculate rank changes and summary metrics for nodes in H\n    delta_r_values = [int(r_sym[i] - r_row[i]) for i in H]\n    abs_delta_r_values = np.abs(delta_r_values)\n    \n    mean_abs_change = np.mean(abs_delta_r_values) if delta_r_values else 0.0\n    max_abs_change = int(np.max(abs_delta_r_values)) if delta_r_values else 0\n    \n    return [mean_abs_change, max_abs_change, delta_r_values]\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "4320621"}]}