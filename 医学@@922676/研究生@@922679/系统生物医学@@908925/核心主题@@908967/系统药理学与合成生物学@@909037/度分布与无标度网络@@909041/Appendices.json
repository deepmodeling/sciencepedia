{"hands_on_practices": [{"introduction": "在分析生物网络（如蛋白质-蛋白质相互作用网络）时，第一步通常是可视化其度分布，以判断是否存在无标度特性。然而，由于高阶“枢纽”节点数量稀少，分布的尾部数据往往充满噪声，使得直接绘制频率图难以得出清晰结论。本练习将引导你掌握对数分箱法，这是一种强大的统计技术，可以通过平滑稀疏数据来清晰地揭示幂律尾部的潜在趋势，这是正确表征无标度网络的关键一步 ([@problem_id:4333586])。", "problem": "您正在系统生物医学中分析一个蛋白质-蛋白质相互作用（Protein–Protein Interaction (PPI)）网络的度分布 $P(k)$，该网络有 $N$ 个节点，其度为整数集 $\\{k_{\\ell}\\}_{\\ell=1}^{N}$。对于大的 $k$ 值，数据是稀疏的，并且会受到较大的采样方差影响。您决定使用对数分箱来减小方差，同时旨在保留对度等于 $k$ 的底层概率定律 $P(k)$ 的无偏估计。仅从以下核心定义出发：(i) 度分布 $P(k)$ 是整值随机变量 $K$ 的概率质量函数，即 $P(k) = \\Pr(K = k)$ 且 $\\sum_{k=0}^{\\infty} P(k) = 1$；(ii) 在大 $N$ 极限下，任何区间内观测到的经验频率近似于相应的概率。请选择最正确地描述了为离散度数据构建对数分箱的规范程序，以及与这些分箱兼容的偏差减小的分箱 $P(k)$ 估计量的选项。\n\nA. 选择一个最小度 $k_{\\min} \\geq 1$ 和一个比率 $b > 1$。定义箱边界 $k_{j} = \\left\\lfloor k_{\\min} \\, b^{j} \\right\\rfloor$（$j = 0,1,2,\\dots$ 直至最大观测度），以及分箱区间 $[k_{j}, k_{j+1})$。令 $n_{j}$ 为度在 $[k_{j}, k_{j+1})$ 内的节点数。对于低度值 $k  k_{\\mathrm{switch}}$，使用单位宽度的分箱以尊重其离散性；对于 $k \\geq k_{\\mathrm{switch}}$，估计\n$$\n\\widehat{P}\\!\\left(k_{j}^{\\star}\\right) \\;=\\; \\frac{n_{j}}{N \\,\\big(k_{j+1} - k_{j}\\big)} \\quad \\text{其中} \\quad k_{j}^{\\star} \\;=\\; \\sqrt{k_{j}\\,k_{j+1}}。\n$$\n在对数坐标轴上绘制 $\\widehat{P}(k_{j}^{\\star})$ 相对于 $k_{j}^{\\star}$ 的图像。\n\nB. 选择一个最小度 $k_{\\min} \\geq 1$ 和一个比率 $b > 1$。定义箱边界 $k_{j} = \\left\\lfloor k_{\\min} \\, b^{j} \\right\\rfloor$ 和分箱区间 $[k_{j}, k_{j+1})$。令 $n_{j}$ 为度在 $[k_{j}, k_{j+1})$ 内的节点数。估计\n$$\n\\widehat{P}\\!\\left(k_{j}^{\\star}\\right) \\;=\\; \\frac{n_{j}}{N} \\quad \\text{其中} \\quad k_{j}^{\\star} \\;=\\; \\frac{k_{j} + k_{j+1}}{2}。\n$$\n在对数坐标轴上绘制 $\\widehat{P}(k_{j}^{\\star})$ 相对于 $k_{j}^{\\star}$ 的图像。\n\nC. 选择一个最小度 $k_{\\min} \\geq 1$ 和一个比率 $b > 1$。定义箱边界 $k_{j} = \\left\\lfloor k_{\\min} \\, b^{j} \\right\\rfloor$ 和分箱区间 $[k_{j}, k_{j+1})$。令 $n_{j}$ 为度在 $[k_{j}, k_{j+1})$ 内的节点数。估计\n$$\n\\widehat{P}\\!\\left(k_{j}^{\\star}\\right) \\;=\\; \\frac{n_{j}}{N\\,\\big(\\log k_{j+1} - \\log k_{j}\\big)} \\quad \\text{其中} \\quad k_{j}^{\\star} \\;=\\; \\frac{k_{j} + k_{j+1}}{2}。\n$$\n在对数坐标轴上绘制 $\\widehat{P}(k_{j}^{\\star})$ 相对于 $k_{j}^{\\star}$ 的图像。\n\nD. 避免分箱，而是通过估计互补累积分布函数（CCDF）$\\widehat{S}(k) = \\frac{1}{N}\\sum_{\\ell=1}^{N} \\mathbf{1}\\{k_{\\ell} \\geq k\\}$ 来减小方差，然后通过有限差分\n$$\n\\widehat{P}(k) \\;=\\; \\widehat{S}(k) - \\widehat{S}(k+1)\n$$\n来近似 $P(k)$。报告每个整数 $k$ 的 $\\widehat{P}(k)$。", "solution": "该问题陈述在网络科学和统计数据分析领域提出了一个有效且定义明确的问题。它涉及如何从尾部的稀疏数据中对离散概率质量函数——度分布 $P(k)$——进行原则性估计，这是分析如蛋白质-蛋白质相互作用（PPI）网络等无标度网络时的一个常见挑战。给定的条件在科学上是合理的且内部一致的。\n\n**已知条件：**\n1.  研究对象是具有 $N$ 个节点的网络的度分布 $P(k)$。\n2.  度 $\\{k_{\\ell}\\}_{\\ell=1}^{N}$ 是整数。\n3.  对于大的 $k$，数据是稀疏的，导致高采样方差。\n4.  目标是使用对数分箱来减小方差，同时获得对概率定律 $P(k)$ 的偏差减小的估计。\n5.  定义 (i)：$P(k) = \\Pr(K = k)$ 是离散随机变量 $K$ 的概率质量函数 (PMF)，且 $\\sum_{k=0}^{\\infty} P(k) = 1$。\n6.  定义 (ii)：在大 $N$ 极限下，一个区间内的观测经验频率近似于真实的概率。\n\n**从第一性原理推导**\n\n令 $K$ 为代表随机选择节点的度的离散随机变量。度分布是其概率质量函数 (PMF)，$P(k) = \\Pr(K=k)$。对于一个有 $N$ 个节点的网络，其中度为 $k$ 的节点数为 $n_k$，$P(k)$ 的经验估计由 $\\widehat{P}(k) = n_k/N$ 给出。对于大的 $k$，$n_k$ 通常很小（常常是 $0$ 或 $1$），导致该估计的方差很高。\n\n对数分箱的策略是将度分组到宽度随 $k$ 增加的箱中。这能确保每个箱都包含合理数量的节点，从而稳定估计值。让我们定义一组按几何级数增加的箱边界 $k_0, k_1, k_2, \\dots$，例如，对于某个 $b>1$，$k_j \\approx k_{\\min} b^j$。第 $j$ 个箱是整数区间 $[k_j, k_{j+1})$。令 $n_j$ 为度落入此箱的节点数。\n\n一个节点的度落在第 $j$ 个箱中的总概率为：\n$$\n\\Pr(k_j \\le K  k_{j+1}) = \\sum_{k=k_j}^{k_{j+1}-1} P(k)\n$$\n在大 $N$ 极限下，这个概率可以通过经验频率来近似：\n$$\n\\sum_{k=k_j}^{k_{j+1}-1} P(k) \\approx \\frac{n_j}{N}\n$$\n问题在于如何从分箱后的计数 $n_j$ 中获得 $P(k)$ 本身的点估计。对于大的 $k$，我们可以用一个连续的概率密度函数 (PDF) $p(k)$ 来近似 PMF $P(k)$。然后，求和可以用积分来近似：\n$$\n\\sum_{k=k_j}^{k_{j+1}-1} P(k) \\approx \\int_{k_j}^{k_{j+1}} p(x) \\, dx\n$$\n如果箱足够窄，使得 $p(x)$ 在其中变化不大，我们可以使用积分中值定理。该积分约等于函数在区间内某个代表点 $k_j^{\\star}$ 处的值乘以区间的宽度。第 $j$ 个箱的宽度是 $w_j = k_{j+1} - k_j$。\n$$\n\\int_{k_j}^{k_{j+1}} p(x) \\, dx \\approx p(k_j^{\\star}) \\cdot (k_{j+1} - k_j)\n$$\n结合这些近似，我们得到：\n$$\n\\frac{n_j}{N} \\approx p(k_j^{\\star}) \\cdot (k_{j+1} - k_j)\n$$\n这使我们能够构建在点 $k_j^{\\star}$ 处的密度 $p(k)$ 的估计量：\n$$\n\\widehat{p}(k_j^{\\star}) = \\frac{n_j}{N (k_{j+1} - k_j)}\n$$\n这个PDF的估计量通常（尽管略有不精确）在对数坐标轴上被绘制为“度分布”。按箱宽 $k_{j+1} - k_j$ 进行归一化是至关重要的。否则，较宽的箱会纯粹因为其尺寸较大而显示出更多的计数，这会扭曲估计分布的形状。如果 $P(k) \\sim k^{-\\gamma}$，这个估计量 $\\widehat{p}(k_j^{\\star})$ 也会按 $(k_j^{\\star})^{-\\gamma}$ 的比例缩放，从而在对数-对数图上正确地揭示指数。\n\n对于代表点 $k_j^{\\star}$，当箱是对数间隔的且底层函数是幂律时，几何平均数 $k_j^{\\star} = \\sqrt{k_j k_{j+1}}$ 是比算术平均数更优越的选择。这是因为在对数坐标轴上，几何平均数恰好位于中点：$\\log(\\sqrt{k_j k_{j+1}}) = \\frac{1}{2}(\\log k_j + \\log k_{j+1})$。这导致在估计幂律指数时偏差更低。\n\n最后，对于小的 $k$，数据不稀疏，对数分箱是不必要的，并且可能会隐藏分布的特征。对于小的 $k$ 使用单位宽度的箱（即不分箱），而对于大的 $k$ 切换到对数分箱，是一种合理的做法。\n\n**评估选项**\n\n**A. 选择一个最小度 $k_{\\min} \\geq 1$ 和一个比率 $b > 1$。定义箱边界 $k_{j} = \\left\\lfloor k_{\\min} \\, b^{j} \\right\\rfloor$（$j = 0,1,2,\\dots$ 直至最大观测度），以及分箱区间 $[k_{j}, k_{j+1})$。令 $n_{j}$ 为度在 $[k_{j}, k_{j+1})$ 内的节点数。对于低度值 $k  k_{\\mathrm{switch}}$，使用单位宽度的分箱以尊重其离散性；对于 $k \\geq k_{\\mathrm{switch}}$，估计\n$$\n\\widehat{P}\\!\\left(k_{j}^{\\star}\\right) \\;=\\; \\frac{n_{j}}{N \\,\\big(k_{j+1} - k_{j}\\big)} \\quad \\text{其中} \\quad k_{j}^{\\star} \\;=\\; \\sqrt{k_{j}\\,k_{j+1}}。\n$$\n在对数坐标轴上绘制 $\\widehat{P}(k_{j}^{\\star})$ 相对于 $k_{j}^{\\star}$ 的图像。**\n\n此选项正确地指定了标准的、规范的程序。\n1.  它使用由 $k_j = \\lfloor k_{\\min} b^j \\rfloor$ 定义的对数分箱。\n2.  关键地，估计量 $\\frac{n_{j}}{N(k_{j+1} - k_{j})}$ 正确地将箱内的经验频率 $n_j/N$ 按箱宽 $k_{j+1} - k_j$ 进行归一化。这产生了一个概率密度的估计，这是用于可视化缩放行为的正确量。\n3.  它提出了几何平均数 $k_{j}^{\\star} = \\sqrt{k_{j}k_{j+1}}$ 作为箱的代表点，这是对数尺度上幂律分布最合适的选择。\n4.  它明智地采用了混合策略，对数据稠密的小 $k$ 值使用单位宽度的箱。\n这个程序正确地减小了尾部的方差，同时最小化了偏差。\n结论：**正确**。\n\n**B. 选择一个最小度 $k_{\\min} \\geq 1$ 和一个比率 $b > 1$。定义箱边界 $k_{j} = \\left\\lfloor k_{\\min} \\, b^{j} \\right\\rfloor$ 和分箱区间 $[k_{j}, k_{j+1})$。令 $n_{j}$ 为度在 $[k_{j}, k_{j+1})$ 内的节点数。估计\n$$\n\\widehat{P}\\!\\left(k_{j}^{\\star}\\right) \\;=\\; \\frac{n_{j}}{N} \\quad \\text{其中} \\quad k_{j}^{\\star} \\;=\\; \\frac{k_{j} + k_{j+1}}{2}。\n$$\n在对数坐标轴上绘制 $\\widehat{P}(k_{j}^{\\star})$ 相对于 $k_{j}^{\\star}$ 的图像。**\n\n这个选项存在根本性缺陷。估计量 $\\widehat{P}(k_j^*) = n_j/N$ 仅仅是落在第 $j$ 个箱内的总概率。由于对数分箱的宽度是递增的（$k_{j+1}-k_j$ 随 $j$ 增长），这个量 $n_j/N$ 对于较大的 $k$ 会被人为地放大。绘制这个值相对于 $k_j^*$ 的图像不会揭示真实的底层分布 $P(k)$。对于一个幂律 $P(k) \\propto k^{-\\gamma}$，这个估计量的行为将是 $\\approx \\int_{k_j}^{k_{j+1}}C k^{-\\gamma} dk \\propto k_j^{1-\\gamma}$。对数-对数图的斜率将是 $1-\\gamma$，而不是 $-\\gamma$。未按箱宽进行归一化是一个关键错误。\n结论：**不正确**。\n\n**C. 选择一个最小度 $k_{\\min} \\geq 1$ 和一个比率 $b > 1$。定义箱边界 $k_{j} = \\left\\lfloor k_{\\min} \\, b^{j} \\right\\rfloor$ 和分箱区间 $[k_{j}, k_{j+1})$。令 $n_{j}$ 为度在 $[k_{j}, k_{j+1})$ 内的节点数。估计\n$$\n\\widehat{P}\\!\\left(k_{j}^{\\star}\\right) \\;=\\; \\frac{n_{j}}{N\\,\\big(\\log k_{j+1} - \\log k_{j}\\big)} \\quad \\text{其中} \\quad k_{j}^{\\star} \\;=\\; \\frac{k_{j} + k_{j+1}}{2}。\n$$\n在对数坐标轴上绘制 $\\widehat{P}(k_{j}^{\\star})$ 相对于 $k_{j}^{\\star}$ 的图像。**\n\n这个选项按对数空间中的箱宽 $\\log k_{j+1} - \\log k_j$ 进行归一化。这对于估计 $P(k)$ 是不正确的。如思路过程所示，这个估计量近似的是 $k P(k)$ 这个量。如果 $P(k) \\propto k^{-\\gamma}$，这个估计量将按 $k \\cdot k^{-\\gamma} = k^{1-\\gamma}$ 的比例缩放。这个程序不估计 $P(k)$ 或一个类似的密度函数，而是估计一个不同的关于 $k$ 的函数。这种归一化有时被专门用来检验 $P(k) \\propto k^{-1}$ 的分布，因为在这种情况下 $kP(k)$ 将是常数，但它不是一个通用的 $P(k)$ 估计量。\n结论：**不正确**。\n\n**D. 避免分箱，而是通过估计互补累积分布函数（CCDF）$\\widehat{S}(k) = \\frac{1}{N}\\sum_{\\ell=1}^{N} \\mathbf{1}\\{k_{\\ell} \\geq k\\}$ 来减小方差，然后通过有限差分\n$$\n\\widehat{P}(k) \\;=\\; \\widehat{S}(k) - \\widehat{S}(k+1)\n$$\n来近似 $P(k)$。报告每个整数 $k$ 的 $\\widehat{P}(k)$。**\n\n虽然估计CCDF（或生存函数 $S(k)$）是可视化幂律数据的一种有效且流行的方法，因为它能减少噪声，但这个选项并没有止步于此。它继续通过有限差分 $\\widehat{S}(k) - \\widehat{S}(k+1)$ 来计算 $\\widehat{P}(k)$。这个过程恰好抵消了噪声的减少。一个简单的计算表明，这在数学上与原始的、高方差的、未分箱的估计量是相同的：\n$$\n\\widehat{P}(k) = \\widehat{S}(k) - \\widehat{S}(k+1) = \\frac{\\#\\{k_\\ell \\ge k\\}}{N} - \\frac{\\#\\{k_\\ell \\ge k+1\\}}{N} = \\frac{\\#\\{k_\\ell = k\\}}{N} = \\frac{n_k}{N}\n$$\n因此，这个选项描述了一种对朴素经验PMF的迂回计算，而问题陈述明确指出该PMF在大的k值下具有高方差。该方法未能实现为 $P(k)$ 估计减小方差的既定目标。方差的减小是通过直接分析CCDF $\\widehat{S}(k)$ 获得的，而不是通过对其求差分来恢复带噪声的PMF。\n结论：**不正确**。\n\n总之，选项A是唯一一个描述了科学和统计上都合理的对数分箱程序来估计度分布的方法，正确地处理了方差、归一化和离散性的问题。", "answer": "$$\\boxed{A}$$", "id": "4333586"}, {"introduction": "通过可视化确认了网络可能具有幂律度分布后，下一步便是对其进行精确定量。本练习将带你从第一性原理出发，推导用于估计幂律指数 $\\gamma$ 的黄金标准方法——最大似然估计 (MLE)。掌握这个核心公式的推导，不仅能让你在实践中准确地拟合数据，还能深化你对背后统计模型的理解 ([@problem_id:4333638])。", "problem": "在系统生物医学中，分子相互作用网络（如蛋白质-蛋白质相互作用网络）的度分布通常被建模为重尾分布。考虑这样一个网络的单个连通分量，其中观测到了所有满足 $k_{i} \\ge k_{\\min}$ 的节点的度 $\\{k_i\\}_{i=1}^n$。假设在下限截断值 $k_{\\min}$ 以上，度分布可以很好地用连续幂律来近似，因此对于 $k \\ge k_{\\min}$，概率密度函数 $p(k \\mid \\gamma)$ 与 $k^{-\\gamma}$ 成正比，其中指数 $\\gamma > 1$ 以确保可归一化性。\n\n仅使用基本原理——即概率密度归一化和最大似然原理的要求——从观测样本 $\\{k_i\\}_{i=1}^n$ 中为连续模型推导出指数的最大似然估计量 $\\hat{\\gamma}$。然后，认识到度本质上是整数值，陈述对该估计量的标准离散连续性校正，该校正通常用于在将连续近似应用于离散度数据时减少偏差。\n\n您的最终答案应表示为一个行矩阵中的一对符号表达式，使用 LaTeX 的 $\\mathrm{pmatrix}$ 环境，其中第一个条目是连续模型的最大似然估计量 $\\hat{\\gamma}$，第二个条目是其离散连续性校正。不要四舍五入，也不要包含单位。", "solution": "问题陈述经评估有效。它在科学上基于统计力学和网络理论，问题适定，目标明确，语言客观，并包含足够的信息以进行严谨的推导。未发现任何缺陷。\n\n按要求，解答过程分为两部分。首先，推导连续幂律分布指数的最大似然估计量 (MLE)。其次，陈述将此估计量应用于离散数据时的标准连续性校正。\n\n**第一部分：最大似然估计量 $\\hat{\\gamma}$ 的推导**\n\n推导遵循概率密度归一化和最大似然估计的原理。\n\n**1. 概率密度函数 (PDF) 的归一化**\n问题陈述对于度 $k \\ge k_{\\min}$，概率密度函数 $p(k \\mid \\gamma)$ 与 $k^{-\\gamma}$ 成正比。我们可以将其写为：\n$$p(k \\mid \\gamma) = C k^{-\\gamma}$$\n其中 $C$ 是一个归一化常数。为了使 $p(k \\mid \\gamma)$ 成为一个有效的 PDF，它在其定义域上的积分必须等于 $1$。\n$$\\int_{k_{\\min}}^{\\infty} p(k \\mid \\gamma) \\, dk = 1$$\n代入 PDF 的形式，我们有：\n$$\\int_{k_{\\min}}^{\\infty} C k^{-\\gamma} \\, dk = C \\int_{k_{\\min}}^{\\infty} k^{-\\gamma} \\, dk = 1$$\n我们利用给定的约束条件，即指数 $\\gamma > 1$，来计算这个积分：\n$$C \\left[ \\frac{k^{-\\gamma+1}}{-\\gamma+1} \\right]_{k=k_{\\min}}^{k \\to \\infty} = 1$$\n由于 $\\gamma > 1$，指数 $1-\\gamma$ 为负。因此，当 $k \\to \\infty$ 时，项 $k^{1-\\gamma} \\to 0$。积分的计算结果为：\n$$C \\left( 0 - \\frac{k_{\\min}^{1-\\gamma}}{1-\\gamma} \\right) = C \\frac{k_{\\min}^{1-\\gamma}}{\\gamma-1} = 1$$\n求解归一化常数 $C$：\n$$C = \\frac{\\gamma-1}{k_{\\min}^{1-\\gamma}} = (\\gamma-1) k_{\\min}^{\\gamma-1}$$\n因此，连续幂律分布的归一化 PDF 为：\n$$p(k \\mid \\gamma) = (\\gamma-1) k_{\\min}^{\\gamma-1} k^{-\\gamma}, \\quad \\text{for } k \\ge k_{\\min}$$\n\n**2. 对数似然函数**\n给定一个包含 $n$ 个观测度的样本 $\\{k_i\\}_{i=1}^{n}$，其中每个 $k_i \\ge k_{\\min}$ 都被假定为来自 $p(k \\mid \\gamma)$ 的独立同分布 (IID) 抽样，似然函数 $L(\\gamma \\mid \\{k_i\\})$ 是观测到这个特定样本的联合概率：\n$$L(\\gamma \\mid \\{k_i\\}) = \\prod_{i=1}^{n} p(k_i \\mid \\gamma) = \\prod_{i=1}^{n} \\left[ (\\gamma-1) k_{\\min}^{\\gamma-1} k_i^{-\\gamma} \\right]$$\n为了简化最大化过程，我们使用对数似然函数 $\\mathcal{L}(\\gamma) = \\ln L(\\gamma \\mid \\{k_i\\})$：\n$$\\mathcal{L}(\\gamma) = \\ln \\left( \\prod_{i=1}^{n} \\left[ (\\gamma-1) k_{\\min}^{\\gamma-1} k_i^{-\\gamma} \\right] \\right)$$\n利用对数的性质 ($\\ln(ab) = \\ln(a)+\\ln(b)$ 和 $\\ln(a^b) = b\\ln(a)$)：\n$$\\mathcal{L}(\\gamma) = \\sum_{i=1}^{n} \\ln \\left( (\\gamma-1) k_{\\min}^{\\gamma-1} k_i^{-\\gamma} \\right)$$\n$$\\mathcal{L}(\\gamma) = \\sum_{i=1}^{n} \\left[ \\ln(\\gamma-1) + (\\gamma-1)\\ln(k_{\\min}) - \\gamma\\ln(k_i) \\right]$$\n由于求和中的项除了 $\\ln(k_i)$ 外不依赖于索引 $i$，我们可以写成：\n$$\\mathcal{L}(\\gamma) = n \\ln(\\gamma-1) + n(\\gamma-1)\\ln(k_{\\min}) - \\gamma \\sum_{i=1}^{n} \\ln(k_i)$$\n\n**3. 对数似然的最大化**\n$\\gamma$ 的最大似然估计 (MLE)，记为 $\\hat{\\gamma}$，是使 $\\mathcal{L}(\\gamma)$ 最大化的 $\\gamma$ 值。我们通过对 $\\mathcal{L}(\\gamma)$ 关于 $\\gamma$ 求导并将其设为零来找到这个值：\n$$\\frac{d\\mathcal{L}}{d\\gamma} = \\frac{d}{d\\gamma} \\left[ n \\ln(\\gamma-1) + n\\gamma\\ln(k_{\\min}) - n\\ln(k_{\\min}) - \\gamma \\sum_{i=1}^{n} \\ln(k_i) \\right]$$\n$$\\frac{d\\mathcal{L}}{d\\gamma} = \\frac{n}{\\gamma-1} + n\\ln(k_{\\min}) - \\sum_{i=1}^{n} \\ln(k_i)$$\n将导数在 $\\gamma = \\hat{\\gamma}$ 处设为零：\n$$\\frac{n}{\\hat{\\gamma}-1} + n\\ln(k_{\\min}) - \\sum_{i=1}^{n} \\ln(k_i) = 0$$\n现在，我们求解 $\\hat{\\gamma}$：\n$$\\frac{n}{\\hat{\\gamma}-1} = \\sum_{i=1}^{n} \\ln(k_i) - n\\ln(k_{\\min})$$\n$$\\frac{n}{\\hat{\\gamma}-1} = \\sum_{i=1}^{n} (\\ln(k_i) - \\ln(k_{\\min}))$$\n$$\\frac{n}{\\hat{\\gamma}-1} = \\sum_{i=1}^{n} \\ln\\left(\\frac{k_i}{k_{\\min}}\\right)$$\n分离出 $\\hat{\\gamma}-1$：\n$$\\hat{\\gamma}-1 = \\frac{n}{\\sum_{i=1}^{n} \\ln\\left(\\frac{k_i}{k_{\\min}}\\right)}$$\n最后，连续模型的 MLE 为：\n$$\\hat{\\gamma} = 1 + \\frac{n}{\\sum_{i=1}^{n} \\ln\\left(\\frac{k_i}{k_{\\min}}\\right)}$$\n\n**第二部分：离散连续性校正**\n\n上面推导出的估计量 $\\hat{\\gamma}$ 是针对连续变量 $k$ 的。然而，度数据 $\\{k_i\\}$ 本质上是离散整数。将连续估计量直接应用于离散数据会引入已知的正偏差。为了减少这种偏差，通常会应用连续性校正。\n\n标准校正涉及调整数据点，以使离散概率质量函数与连续概率密度函数更好地对齐。这是通过将每个离散数据点 $k_i$ 平移 $-\\frac{1}{2}$ 来实现的，实际上是将整数值 $k_i$ 视为代表连续区间 $[k_i - \\frac{1}{2}, k_i + \\frac{1}{2})$。同样的平移也应用于下限截断值 $k_{\\min}$。\n\n按要求，我们直接陈述校正后的估计量而不进行推导。该校正是通过在 $\\hat{\\gamma}$ 的公式中将 $k_i$ 替换为 $k_i - \\frac{1}{2}$ 并将 $k_{\\min}$ 替换为 $k_{\\min} - \\frac{1}{2}$ 来应用的。得到的估计量是：\n$$\\hat{\\gamma}_{\\text{corr}} = 1 + \\frac{n}{\\sum_{i=1}^{n} \\ln\\left(\\frac{k_i - \\frac{1}{2}}{k_{\\min} - \\frac{1}{2}}\\right)}$$\n\n最终答案由未校正和已校正估计量的一对符号表达式组成。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 + \\frac{n}{\\sum_{i=1}^{n} \\ln\\left(\\frac{k_i}{k_{\\min}}\\right)}  1 + \\frac{n}{\\sum_{i=1}^{n} \\ln\\left(\\frac{k_i - \\frac{1}{2}}{k_{\\min} - \\frac{1}{2}}\\right)}\n\\end{pmatrix}\n}\n$$", "id": "4333638"}, {"introduction": "理论知识的价值最终体现在解决实际问题的能力上。在系统生物医学中，我们使用的数据库（如PPI网络）往往是通过整合多个来源的数据构建的，不可避免地存在数据不一致和冗余的问题。本练习将前一个练习中推导出的最大似然估计方法 ([@problem_id:4333638]) 应用于一个真实的场景，通过计算展示了数据清洗（如别名去重）这一看似常规的操作如何显著影响网络的关键拓扑参数——尾部指数 $\\gamma$ 的估计值，从而强调了数据质量在网络分析中的重要性 ([@problem_id:4333591])。", "problem": "通过整合多个来源的相互作用报告，组建了一个经过整理的蛋白质-蛋白质相互作用（PPI）数据库。由于同义词的模糊性和标识符使用的不一致，在最初的网络构建中，当不同来源引用同一蛋白质的不同别名时，同一蛋白质对之间会存在多条平行边。经过一轮去重处理后，平行边被合并为单条无向边。在第二轮整理步骤中，当有证据表明合并的节点包含了不同的蛋白质时，通过拆分错误合并的标识符，修正了一小部分别名混淆的情况。考虑以下一个简化的分析，该分析专注于度分布的高区度尾部。\n\n已知在去重前后，提取了度至少为 $k_{\\min} = 40$ 的节点的尾部度（每个蛋白质的唯一邻居数）。去重前，尾部度为 $[100, 85, 65, 55, 50, 45, 40]$。去重后，尾部度为 $[80, 70, 60, 50, 45, 42, 40]$。假设尾部可以很好地由一个连续幂律模型近似，其概率密度 $p(k)$ 在 $k \\ge k_{\\min}$ 时与 $k^{-\\gamma}$ 成正比，并且这些尾部度是从该分布中独立抽取的样本。\n\n仅使用第一性原理——即幂律尾部的核心定义、概率密度的归一化、以及独立样本似然函数的构建和最大化——来确定去重如何影响拟合的尾部指数，并指出以下哪些陈述是正确的。当需要数值估计时，将指数报告到最接近 $0.1$ 的值。\n\n哪个（些）选项是正确的？\n\n- A. 移除平行边副本会减少高度中心节点的数量，并使拟合的尾部分布更陡峭；对于给定的数据，最大似然尾部指数在去重后从去重前的大约 $3.5$ 增加到大约 $4.4$。\n- B. 去重增加了高度节点的数量，使尾部更重；对于给定的数据，最大似然尾部指数在去重后从去重前的大约 $4.4$ 减少到大约 $3.5$。\n- C. 如果去重仅移除平行边而保持 $k_{\\min} = 40$ 不变，则尾部指数的最大似然估计会产生向上偏差，必须增加 $k_{\\min}$ 以避免偏差。\n- D. 别名混淆，即将不同的蛋白质合并到一个标识符下，会人为地制造出中心节点，通过模拟更重的尾部来降低拟合指数；通过拆分标识符来纠正这些混淆，会倾向于增加拟合指数。\n- E. 幂律的最大似然尾部指数对于度的任何单调变换都是不变的，因此去重不能改变拟合的指数。", "solution": "我们从第一性原理开始。无标度尾部由一个幂律概率密度 $p(k)$ 建模，其中 $k \\ge k_{\\min}$ 且 $p(k) = C k^{-\\gamma}$，$C$ 是归一化常数，$\\gamma  1$ 是尾部指数。常数 $C$ 通过强制执行以下条件来确定：\n$$\n\\int_{k_{\\min}}^{\\infty} p(k) \\, dk \\;=\\; \\int_{k_{\\min}}^{\\infty} C k^{-\\gamma} \\, dk \\;=\\; 1.\n$$\n对于 $\\gamma  1$，\n$$\n\\int_{k_{\\min}}^{\\infty} k^{-\\gamma} \\, dk \\;=\\; \\left[ \\frac{k^{-(\\gamma-1)}}{(\\gamma-1)} \\right]_{k_{\\min}}^{\\infty} \\;=\\; \\frac{k_{\\min}^{-(\\gamma-1)}}{\\gamma - 1},\n$$\n因此 $C = (\\gamma - 1) k_{\\min}^{\\gamma - 1}$，归一化后的密度为\n$$\np(k) \\;=\\; (\\gamma - 1) k_{\\min}^{\\gamma - 1} k^{-\\gamma}, \\quad k \\ge k_{\\min}.\n$$\n\n设观察到的尾部度为 $k_1, k_2, \\dots, k_n$，其中 $k_i \\ge k_{\\min}$。在独立性假设下，似然函数为\n$$\n\\mathcal{L}(\\gamma) \\;=\\; \\prod_{i=1}^{n} p(k_i) \\;=\\; \\prod_{i=1}^{n} \\left[ (\\gamma - 1) k_{\\min}^{\\gamma - 1} k_i^{-\\gamma} \\right].\n$$\n对数似然函数为\n$$\n\\ln \\mathcal{L}(\\gamma) \\;=\\; n \\ln(\\gamma - 1) + (\\gamma - 1) n \\ln k_{\\min} - \\gamma \\sum_{i=1}^{n} \\ln k_i.\n$$\n对 $\\gamma$ 求导并令导数等于零，得到\n$$\n\\frac{d}{d\\gamma} \\ln \\mathcal{L}(\\gamma) \\;=\\; \\frac{n}{\\gamma - 1} + n \\ln k_{\\min} - \\sum_{i=1}^{n} \\ln k_i \\;=\\; 0.\n$$\n求解 $\\gamma$ 得到最大似然估计量\n$$\n\\hat{\\gamma} \\;=\\; 1 + n \\left[ \\sum_{i=1}^{n} \\ln \\left( \\frac{k_i}{k_{\\min}} \\right) \\right]^{-1}.\n$$\n当 $k$ 很大且离散归一化能被很好地近似时，这个连续尾部估计量被广泛使用。\n\n我们现在使用 $k_{\\min} = 40$ 为给定的尾部度计算 $\\hat{\\gamma}$。\n\n去重前，尾部度为 $[100, 85, 65, 55, 50, 45, 40]$，因此 $n = 7$。我们计算对数：\n- $ \\ln(100/40) = \\ln(2.5) \\approx 0.916290732 $,\n- $ \\ln(85/40) = \\ln(2.125) \\approx 0.753771802 $,\n- $ \\ln(65/40) = \\ln(1.625) \\approx 0.485507815 $,\n- $ \\ln(55/40) = \\ln(1.375) \\approx 0.318453731 $,\n- $ \\ln(50/40) = \\ln(1.25) \\approx 0.223143551 $,\n- $ \\ln(45/40) = \\ln(1.125) \\approx 0.117310926 $,\n- $ \\ln(40/40) = \\ln(1) = 0 $.\n\n求和，\n$$\nS_{\\text{pre}} \\;=\\; 0.916290732 + 0.753771802 + 0.485507815 + 0.318453731 + 0.223143551 + 0.117310926 + 0 \\;\\approx\\; 2.814478557.\n$$\n则\n$$\n\\hat{\\gamma}_{\\text{pre}} \\;=\\; 1 + \\frac{7}{2.814478557} \\;\\approx\\; 1 + 2.487 \\;\\approx\\; 3.487,\n$$\n四舍五入到最近的 $0.1$ 约为 $3.5$。\n\n去重后，尾部度为 $[80, 70, 60, 50, 45, 42, 40]$，因此 $n = 7$ 且\n- $ \\ln(80/40) = \\ln(2) \\approx 0.693147181 $,\n- $ \\ln(70/40) = \\ln(1.75) \\approx 0.559615788 $,\n- $ \\ln(60/40) = \\ln(1.5) \\approx 0.405465108 $,\n- $ \\ln(50/40) = \\ln(1.25) \\approx 0.223143551 $,\n- $ \\ln(45/40) = \\ln(1.125) \\approx 0.117310926 $,\n- $ \\ln(42/40) = \\ln(1.05) \\approx 0.048790164 $,\n- $ \\ln(40/40) = \\ln(1) = 0 $.\n\n求和，\n$$\nS_{\\text{post}} \\;=\\; 0.693147181 + 0.559615788 + 0.405465108 + 0.223143551 + 0.117310926 + 0.048790164 + 0 \\;\\approx\\; 2.047472718.\n$$\n则\n$$\n\\hat{\\gamma}_{\\text{post}} \\;=\\; 1 + \\frac{7}{2.047472718} \\;\\approx\\; 1 + 3.418 \\;\\approx\\; 4.418,\n$$\n约等于 $4.4$。\n\n解释。移除平行边副本会优先降低高端的测量度（因为中心节点会累积更多的重复报告），从而降低大 $k$ 值的相对频率。在幂律拟合中，这会使尾部更陡峭，并增加 $\\hat{\\gamma}$，正如数值结果所反映的那样。同样，纠正别名混淆（拆分一个错误地合并了不同蛋白质的标识符）会降低虚高的中心节点度，也会使尾部更陡峭。\n\n逐项分析：\n- A. 该选项指出去重会减少高度节点的数量，使尾部更陡峭，并给出了从大约 $3.5$ 增加到大约 $4.4$ 的正确数值变化。这与我们的推导和计算相符。结论：正确。\n- B. 该选项声称了相反的方向（去重会增加高度节点的数量并减小 $\\hat{\\gamma}$），并颠倒了数值。我们的计算表明，去重后尾部指数增加。结论：不正确。\n- C. 该选项断言，在去重后保持 $k_{\\min} = 40$ 会在最大似然估计量中引入向上偏差，因此需要增加 $k_{\\min}$。只要尾部模型是合适的，上面推导的最大似然估计量对于相同的 $k_{\\min}$ 仍然有效；如果下限截止值保持不变并且仍然可以通过拟合优度标准来证明其合理性，去重本身并不会引入偏差。没有普遍的必要性仅仅因为移除了副本就增加 $k_{\\min}$；是否调整 $k_{\\min}$ 是一个独立的模型选择问题，而不是去重所要求的偏差校正。结论：不正确。\n- D. 该选项强调了别名混淆（将不同蛋白质合并到一个标识符下），这会人为地夸大被混淆节点的度，通过模拟更重的尾部来降低拟合指数。通过拆分标识符来纠正这些混淆会降低虚高的中心节点度，并倾向于增加拟合指数，这与机制和我们的解释一致。结论：正确。\n- E. 该选项声称幂律的最大似然尾部指数在度的任何单调变换下都是不变的。该估计量明确地依赖于 $k_i$ 相对于 $k_{\\min}$ 的数值；单调变换（除了平凡的恒等变换）通常会改变 $\\sum \\ln(k_i/k_{\\min})$，从而改变 $\\hat{\\gamma}$。此外，去重不是一个单调变换；它通过合并副本并有时拆分混淆来改变经验样本，从而改变了值的多重集及其分布。结论：不正确。", "answer": "$$\\boxed{AD}$$", "id": "4333591"}]}