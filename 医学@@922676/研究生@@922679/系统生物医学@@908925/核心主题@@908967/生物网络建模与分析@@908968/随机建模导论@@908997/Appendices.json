{"hands_on_practices": [{"introduction": "Ornstein-Uhlenbeck 过程是随机建模中的一个基石，它描述了系统在噪声干扰下如何回归到平衡状态。这个练习将指导你分析求解一个线性随机微分方程（SDE），这对于理解如基因产物浓度在稳态调节下的波动等生物过程至关重要 [@problem_id:4357844]。通过亲手推导，你将掌握伊东积分（Itô integral）的核心技巧，并看到 SDE 的解如何与对应的 Fokker-Planck 方程的预测相吻合。", "problem": "在系统生物医学中，一个关于单一基因产物浓度的稳态调节简化模型里，与标称水平的微小偏差可以被一个由有效分子噪声驱动的线性随机微分方程（SDE）建模。考虑以下一维SDE\n$$\ndX_t \\;=\\; -\\theta\\,X_t\\,dt \\;+\\; \\sigma\\,dW_t,\\qquad X_0 \\;=\\; x_0,\n$$\n其中 $W_t$ 是一个标准维纳过程，$\\theta \\gt 0$ 和 $\\sigma \\gt 0$ 是常数，且 $x_0 \\in \\mathbb{R}$。\n\n任务：\n- 仅从伊藤积分的基本性质和线性SDE的标准技巧出发，推导唯一强解 $X_t$ 的闭式解表达式。\n- 计算均值 $\\mathbb{E}[X_t]$ 和方差 $\\mathrm{Var}(X_t)$，将其显式地表示为 $t$、$\\theta$、$\\sigma$ 和 $x_0$ 的函数。\n- 利用与漂移项 $a(x) = -\\theta x$ 和扩散系数 $b(x) = \\sigma$ 相关联的柯尔莫哥洛夫前向（福克-普朗克）方程，证明 $X_t$ 的概率密度保持为高斯分布，并且其均值和方差服从与从SDE解中得到的结果相一致的常微分方程。\n\n将你的最终答案以一个单行矩阵的形式给出，该矩阵由 $\\mathbb{E}[X_t]$ 和 $\\mathrm{Var}(X_t)$ 构成。不需要数值近似，也不需要单位。所有指数函数请使用 $\\exp(\\cdot)$ 函数表示。", "solution": "用户提供了一个有效的问题陈述。该问题要求解一个线性随机微分方程（SDE）的解、均值和方差，并使用相关的福克-普朗克方程进行一致性检验。该SDE是一个标准的奥恩斯坦-乌伦贝克过程，是随机微积分及其应用中的一个基本模型。\n\n该问题分为三个部分。我们将依次解决每个部分。\n\n给定的SDE是：\n$$dX_t = -\\theta X_t dt + \\sigma dW_t, \\quad X_0 = x_0$$\n其中 $\\theta > 0$ 且 $\\sigma > 0$ 是常数。\n\n### 第一部分：闭式解的推导\n\n我们寻求强解 $X_t$。这是一个线性SDE，可以使用积分因子法求解，类似于求解线性常微分方程（ODE）的方法。让我们为一个合适的函数 $f(t)$ 定义一个新过程 $Y_t = f(t) X_t$。我们对函数 $g(t, x) = f(t)x$ 应用伊藤引理。其偏导数为：\n$$\\frac{\\partial g}{\\partial t} = f'(t)x, \\quad \\frac{\\partial g}{\\partial x} = f(t), \\quad \\frac{\\partial^2 g}{\\partial x^2} = 0$$\n根据伊藤引理，微分 $dY_t = d(f(t)X_t)$ 由下式给出：\n$$dY_t = \\frac{\\partial g}{\\partial t} dt + \\frac{\\partial g}{\\partial x} dX_t + \\frac{1}{2} \\frac{\\partial^2 g}{\\partial x^2} (dX_t)^2$$\n由于 $dX_t$ 的二次变分是 $(dX_t)^2 = (\\sigma dW_t)^2 = \\sigma^2 dt$，且 $\\frac{\\partial^2 g}{\\partial x^2} = 0$，最后一项为零。代入偏导数和 $dX_t$ 的表达式：\n$$dY_t = f'(t)X_t dt + f(t)(-\\theta X_t dt + \\sigma dW_t)$$\n$$dY_t = (f'(t) - \\theta f(t))X_t dt + \\sigma f(t) dW_t$$\n为了简化方程，我们选择积分因子 $f(t)$，使得 $X_t dt$ 项的系数为零。\n$$f'(t) - \\theta f(t) = 0$$\n这是一个关于 $f(t)$ 的一阶线性ODE。其解为 $f(t) = C \\exp(\\theta t)$，其中 $C$ 为某个常数。我们可以设 $C=1$，因此我们的积分因子是 $f(t) = \\exp(\\theta t)$。\n\n根据这个选择，$Y_t = \\exp(\\theta t) X_t$ 的SDE简化为：\n$$d(\\exp(\\theta t) X_t) = \\sigma \\exp(\\theta t) dW_t$$\n我们对两边从 $s=0$ 到 $s=t$ 进行积分：\n$$\\int_0^t d(\\exp(\\theta s) X_s) = \\int_0^t \\sigma \\exp(\\theta s) dW_s$$\n$$\\exp(\\theta t) X_t - \\exp(\\theta \\cdot 0) X_0 = \\sigma \\int_0^t \\exp(\\theta s) dW_s$$\n代入 $X_0 = x_0$：\n$$\\exp(\\theta t) X_t - x_0 = \\sigma \\int_0^t \\exp(\\theta s) dW_s$$\n解出 $X_t$，我们得到唯一的强解：\n$$X_t = x_0 \\exp(-\\theta t) + \\sigma \\exp(-\\theta t) \\int_0^t \\exp(\\theta s) dW_s$$\n另一种写法是通过在随机积分中改变积分变量：\n$$X_t = x_0 \\exp(-\\theta t) + \\sigma \\int_0^t \\exp(-\\theta(t-s)) dW_s$$\n\n### 第二部分：均值和方差的计算\n\n有了 $X_t$ 的显式解，我们可以计算其统计矩。\n\n**均值 $\\mathbb{E}[X_t]$:**\n我们对解取期望。根据期望算子的线性性：\n$$\\mathbb{E}[X_t] = \\mathbb{E}\\left[ x_0 \\exp(-\\theta t) + \\sigma \\int_0^t \\exp(-\\theta(t-s)) dW_s \\right]$$\n$$\\mathbb{E}[X_t] = \\mathbb{E}[x_0 \\exp(-\\theta t)] + \\mathbb{E}\\left[ \\sigma \\int_0^t \\exp(-\\theta(t-s)) dW_s \\right]$$\n由于 $x_0$、$\\theta$ 和 $t$ 是确定性的，第一项是 $\\mathbb{E}[x_0 \\exp(-\\theta t)] = x_0 \\exp(-\\theta t)$。\n第二项涉及一个伊藤积分的期望。对于确定性被积函数 $g(s) = \\sigma \\exp(-\\theta(t-s))$，该伊藤积分的均值为零：\n$$\\mathbb{E}\\left[ \\int_0^t g(s) dW_s \\right] = 0$$\n因此，第二项为零。\n$$\\mathbb{E}[X_t] = x_0 \\exp(-\\theta t)$$\n\n**方差 $\\mathrm{Var}(X_t)$:**\n方差定义为 $\\mathrm{Var}(X_t) = \\mathbb{E}[(X_t - \\mathbb{E}[X_t])^2]$。\n首先，我们计算与均值的偏差：\n$$X_t - \\mathbb{E}[X_t] = \\left( x_0 \\exp(-\\theta t) + \\sigma \\int_0^t \\exp(-\\theta(t-s)) dW_s \\right) - x_0 \\exp(-\\theta t)$$\n$$X_t - \\mathbb{E}[X_t] = \\sigma \\int_0^t \\exp(-\\theta(t-s)) dW_s$$\n现在，我们计算方差：\n$$\\mathrm{Var}(X_t) = \\mathbb{E}\\left[ \\left( \\sigma \\int_0^t \\exp(-\\theta(t-s)) dW_s \\right)^2 \\right]$$\n$$\\mathrm{Var}(X_t) = \\sigma^2 \\mathbb{E}\\left[ \\left( \\int_0^t \\exp(-\\theta(t-s)) dW_s \\right)^2 \\right]$$\n我们使用伊藤等距性，该性质指出对于一个确定性的平方可积函数 $g(s)$，有 $\\mathbb{E}[(\\int_0^t g(s)dW_s)^2] = \\int_0^t \\mathbb{E}[g(s)^2]ds = \\int_0^t g(s)^2 ds$。\n这里，我们的被积函数是 $g(s) = \\exp(-\\theta(t-s))$。\n$$\\mathrm{Var}(X_t) = \\sigma^2 \\int_0^t \\left( \\exp(-\\theta(t-s)) \\right)^2 ds = \\sigma^2 \\int_0^t \\exp(-2\\theta(t-s)) ds$$\n$$\\mathrm{Var}(X_t) = \\sigma^2 \\exp(-2\\theta t) \\int_0^t \\exp(2\\theta s) ds$$\n计算该积分：\n$$\\int_0^t \\exp(2\\theta s) ds = \\left[ \\frac{1}{2\\theta} \\exp(2\\theta s) \\right]_0^t = \\frac{1}{2\\theta}(\\exp(2\\theta t) - 1)$$\n将此结果代回方差的表达式：\n$$\\mathrm{Var}(X_t) = \\sigma^2 \\exp(-2\\theta t) \\left( \\frac{\\exp(2\\theta t) - 1}{2\\theta} \\right)$$\n$$\\mathrm{Var}(X_t) = \\frac{\\sigma^2}{2\\theta} (1 - \\exp(-2\\theta t))$$\n\n### 第三部分：福克-普朗克方程分析\n\n柯尔莫哥洛夫前向方程，或称福克-普朗克方程（FPE），描述了过程 $X_t$ 的概率密度函数 $p(x,t)$ 的时间演化。对于一个一般的SDE $dX_t = a(X_t,t)dt + b(X_t,t)dW_t$，FPE为：\n$$\\frac{\\partial p}{\\partial t} = -\\frac{\\partial}{\\partial x}[a(x,t)p(x,t)] + \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2}[b(x,t)^2 p(x,t)]$$\n在我们的例子中，漂移项为 $a(x) = -\\theta x$，扩散系数为 $b(x) = \\sigma$。FPE变为：\n$$\\frac{\\partial p}{\\partial t} = -\\frac{\\partial}{\\partial x}[-\\theta x p] + \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2}[\\sigma^2 p] = \\theta \\frac{\\partial}{\\partial x}(x p) + \\frac{\\sigma^2}{2} \\frac{\\partial^2 p}{\\partial x^2}$$\n展开右侧第一项得到 $\\theta(p + x \\frac{\\partial p}{\\partial x})$。\n\n解 $X_t$ 是一个确定性项和一个高斯过程（伊藤积分）的和。因此，$X_t$ 本身是一个高斯过程。我们假设对于 $t > 0$，密度 $p(x,t)$ 是高斯分布的：\n$$p(x,t) = \\frac{1}{\\sqrt{2\\pi V(t)}} \\exp\\left( -\\frac{(x - \\mu(t))^2}{2V(t)} \\right)$$\n其中 $\\mu(t) = \\mathbb{E}[X_t]$ 且 $V(t) = \\mathrm{Var}(X_t)$。我们将此拟设代入FPE，以求得 $\\mu(t)$ 和 $V(t)$ 的常微分方程。处理 $\\ln p$ 会更方便：\n$$\\ln p = -\\frac{1}{2}\\ln(2\\pi V) - \\frac{(x-\\mu)^2}{2V}$$\n由此，我们求得偏导数：\n$$\\frac{\\partial p}{\\partial x} = p \\frac{\\partial(\\ln p)}{\\partial x} = p \\left( -\\frac{x-\\mu}{V} \\right)$$\n$$\\frac{\\partial^2 p}{\\partial x^2} = \\frac{\\partial p}{\\partial x}\\left(-\\frac{x-\\mu}{V}\\right) + p\\left(-\\frac{1}{V}\\right) = p\\left(\\frac{(x-\\mu)^2}{V^2} - \\frac{1}{V}\\right)$$\n$$\\frac{\\partial p}{\\partial t} = p \\frac{\\partial(\\ln p)}{\\partial t} = p \\left( -\\frac{\\dot{V}}{2V} + \\frac{(x-\\mu)^2 \\dot{V}}{2V^2} + \\frac{(x-\\mu)\\dot{\\mu}}{V} \\right)$$\n现在我们将这些代入FPE，即 $\\frac{\\partial p}{\\partial t} = \\theta(p + x \\frac{\\partial p}{\\partial x}) + \\frac{\\sigma^2}{2} \\frac{\\partial^2 p}{\\partial x^2}$，并除以 $p(x,t) \\neq 0$：\n$$-\\frac{\\dot{V}}{2V} + \\frac{\\dot{V}}{2V^2}(x-\\mu)^2 + \\frac{\\dot{\\mu}}{V}(x-\\mu) = \\theta + \\theta x \\left(-\\frac{x-\\mu}{V}\\right) + \\frac{\\sigma^2}{2} \\left(\\frac{(x-\\mu)^2}{V^2} - \\frac{1}{V}\\right)$$\n为了按 $(x-\\mu)$ 的幂次对各项进行分组，我们将 $x$ 重写为 $(x-\\mu)+\\mu$：\n$$\\theta x \\left(-\\frac{x-\\mu}{V}\\right) = -\\frac{\\theta}{V}((x-\\mu)+\\mu)(x-\\mu) = -\\frac{\\theta}{V}(x-\\mu)^2 - \\frac{\\theta\\mu}{V}(x-\\mu)$$\nFPE变为：\n$$-\\frac{\\dot{V}}{2V} + \\frac{\\dot{\\mu}}{V}(x-\\mu) + \\frac{\\dot{V}}{2V^2}(x-\\mu)^2 = \\left(\\theta - \\frac{\\sigma^2}{2V}\\right) - \\frac{\\theta\\mu}{V}(x-\\mu) + \\left(\\frac{\\sigma^2}{2V^2}-\\frac{\\theta}{V}\\right)(x-\\mu)^2$$\n此等式必须对所有 $x \\in \\mathbb{R}$ 成立。因此，我们可以令 $(x-\\mu)$ 的各次幂的系数相等：\n\\begin{itemize}\n    \\item $(x-\\mu)^1$ 的系数： $\\displaystyle \\frac{\\dot{\\mu}}{V} = -\\frac{\\theta\\mu}{V} \\implies \\dot{\\mu} = -\\theta\\mu$\n    \\item $(x-\\mu)^2$ 的系数： $\\displaystyle \\frac{\\dot{V}}{2V^2} = \\frac{\\sigma^2}{2V^2} - \\frac{\\theta}{V} \\implies \\dot{V} = \\sigma^2 - 2\\theta V$\n    \\item $(x-\\mu)^0$ 的系数： $\\displaystyle -\\frac{\\dot{V}}{2V} = \\theta - \\frac{\\sigma^2}{2V} \\implies \\dot{V} = -2\\theta V + \\sigma^2$\n\\end{itemize}\n从二次项和常数项得到的 $\\dot{V}$ 方程是相同的，这表明高斯拟设是一致的。我们现在求解 $\\mu(t)$ 和 $V(t)$ 的常微分方程。\n\n对于均值 $\\mu(t)$，初始条件为 $\\mu(0) = x_0$：\n$$\\frac{d\\mu}{dt} = -\\theta\\mu \\implies \\mu(t) = \\mu(0)\\exp(-\\theta t) = x_0\\exp(-\\theta t)$$\n对于方差 $V(t)$，初始条件为 $V(0) = 0$（因为 $X_0=x_0$ 是一个确定值）：\n$$\\frac{dV}{dt} + 2\\theta V = \\sigma^2$$\n这是一个一阶线性ODE。其解为：\n$$V(t) = V(0)\\exp(-2\\theta t) + \\frac{\\sigma^2}{2\\theta}(1 - \\exp(-2\\theta t))$$\n$$V(t) = 0 \\cdot \\exp(-2\\theta t) + \\frac{\\sigma^2}{2\\theta}(1 - \\exp(-2\\theta t)) = \\frac{\\sigma^2}{2\\theta}(1 - \\exp(-2\\theta t))$$\n从福克-普朗克方程推导出的 $\\mu(t)$ 和 $V(t)$ 的结果，与直接从SDE解计算出的均值 $\\mathbb{E}[X_t]$ 和方差 $\\mathrm{Var}(X_t)$ 完全相同。这证实了 $X_t$ 的概率密度确实是具有这些矩的高斯分布。\n\n最终要求的量是作为时间函数的均值和方差。\n均值： $\\mathbb{E}[X_t] = x_0 \\exp(-\\theta t)$\n方差： $\\mathrm{Var}(X_t) = \\frac{\\sigma^2}{2\\theta}(1 - \\exp(-2\\theta t))$", "answer": "$$\\boxed{\\begin{pmatrix} x_0 \\exp(-\\theta t) & \\frac{\\sigma^2}{2\\theta} (1 - \\exp(-2\\theta t)) \\end{pmatrix}}$$", "id": "4357844"}, {"introduction": "当分子数量较少时，化学反应的随机性变得不可忽视，确定性的常微分方程（ODE）模型可能不再适用。Gillespie 算法（也称随机模拟算法，SSA）是精确模拟这类离散随机过程的黄金标准 [@problem_id:4357865]。在这个实践中，你将为一个经典的 mRNA 生成与降解模型编写 SSA 模拟程序，并将其统计结果与确定性模型的预测进行比较，从而直观地理解内在噪声（intrinsic noise）如何产生方差，以及它为何是生物系统中一个至关重要的特征。", "problem": "考虑一个用于系统生物医学中基因表达的典型信使核糖核酸（mRNA）生灭模型，该模型表示为非负整数上的连续时间马尔可夫链，状态为 $X(t) \\in \\{0,1,2,\\dots\\}$。该系统有两种反应：转录（迁入）和降解（死亡）。对于任意状态 $x \\in \\mathbb{N}_0$，反应及其倾向函数定义如下：\n- 转录：$\\varnothing \\rightarrow \\text{mRNA}$，倾向为 $a_1(x) = k$，其中 $k \\ge 0$ 是转录速率。\n- 降解：$\\text{mRNA} \\rightarrow \\varnothing$，倾向为 $a_2(x) = \\gamma x$，其中 $\\gamma \\ge 0$ 是单位分子的降解速率。\n\n你需要使用精确的随机模拟算法（SSA），也称为 Gillespie 直接法，来模拟这个系统，从初始整数拷贝数 $X(0)=x_0$ 开始，直到一个固定的终止时间 $T > 0$。对于每组参数，执行 $R$ 次独立同分布的样本路径模拟，并计算在时间 $T$ 的以下统计量：\n- $R$ 条路径上 $X(T)$ 的经验样本均值 $\\widehat{m}(T)$。\n- $R$ 条路径上 $X(T)$ 的经验无偏样本方差 $\\widehat{v}(T)$，计算时分母为 $R-1$。\n\n另外，计算由一阶矩方程控制的确定性平均场常微分方程（ODE）解 $x_{\\text{det}}(t)$\n$$\n\\frac{d}{dt} x_{\\text{det}}(t) = k - \\gamma \\, x_{\\text{det}}(t), \\quad x_{\\text{det}}(0)=x_0,\n$$\n并计算 $x_{\\text{det}}(T)$。该 ODE 提供了期望拷贝数的平均场预测，但预测的方差为零。\n\n为每组参数定义以下量化指标：\n- 时间 $T$ 的平均绝对误差：$E_m = \\left| \\widehat{m}(T) - x_{\\text{det}}(T) \\right|$。\n- 时间 $T$ 的解析方差 $v_{\\text{theory}}(T)$，由迁入-死亡过程的第一性原理定义：\n  - 如果 $\\gamma > 0$，\n    $$\n    v_{\\text{theory}}(T) = x_0 \\, e^{-\\gamma T} \\bigl(1 - e^{-\\gamma T}\\bigr) + \\frac{k}{\\gamma}\\bigl(1 - e^{-\\gamma T}\\bigr).\n    $$\n  - 如果 $\\gamma = 0$，\n    $$\n    v_{\\text{theory}}(T) = k\\,T.\n    $$\n- 时间 $T$ 的方差绝对误差：$E_v = \\left| \\widehat{v}(T) - v_{\\text{theory}}(T) \\right|$。\n- 相对于平均场预测（其值为零）的方差差异，定义为 $D_v = \\widehat{v}(T)$。\n\n算法要求：\n- 实现精确的随机模拟算法（Gillespie 直接法）。在任意状态 $x$ 和时间 $t$，计算总倾向 $a_0(x) = a_1(x)+a_2(x) = k + \\gamma x$。如果 $a_0(x) = 0$，系统处于吸收态，不再发生任何反应。否则，从速率为 $a_0(x)$ 的指数分布中抽取反应间隔时间 $\\tau$，并将时间推进 $\\tau$。如果 $t+\\tau > T$，则停止并将当前状态记录为终止状态。否则，通过抽取一个均匀分布的随机变量 $u \\in (0,1)$ 来选择反应类型；如果 $u < a_1(x)/a_0(x)$，则选择转录，否则选择降解。相应地更新状态，并迭代直到 $t \\ge T$。\n- 对于确定性平均场，解析求解线性 ODE：\n  - 如果 $\\gamma > 0$，$x_{\\text{det}}(T) = x_0 e^{-\\gamma T} + \\dfrac{k}{\\gamma}\\bigl(1 - e^{-\\gamma T}\\bigr)$。\n  - 如果 $\\gamma = 0$，$x_{\\text{det}}(T) = x_0 + k\\,T$。\n- 使用固定的伪随机数生成器种子 $1729$ 以确保可复现性。\n\n测试套件：\n为以下四组参数提供结果，每组参数由 $(k,\\gamma,x_0,T,R)$ 指定：\n- 案例 $1$：$(k=\\;5.0,\\;\\gamma=\\;1.0,\\;x_0=\\;10,\\;T=\\;2.0,\\;R=\\;3000)$。\n- 案例 $2$：$(k=\\;0.0,\\;\\gamma=\\;0.7,\\;x_0=\\;25,\\;T=\\;3.5,\\;R=\\;3000)$。\n- 案例 $3$：$(k=\\;12.0,\\;\\gamma=\\;4.0,\\;x_0=\\;0,\\;T=\\;1.5,\\;R=\\;3000)$。\n- 案例 $4$：$(k=\\;5.0,\\;\\gamma=\\;0.0,\\;x_0=\\;3,\\;T=\\;1.2,\\;R=\\;3000)$。\n\n你的程序必须：\n- 精确地按规定实现随机模拟算法。\n- 对于每种情况，从 $R$ 条独立轨迹中估计 $\\widehat{m}(T)$ 和 $\\widehat{v}(T)$，计算 $x_{\\text{det}}(T)$ 和 $v_{\\text{theory}}(T)$，然后计算三元组 $(E_m, E_v, D_v)$。\n- 生成单行输出，其中包含一个逗号分隔的列表，按案例 1 到 4 的顺序连接所有结果，每个案例按 $(E_m, E_v, D_v)$ 的顺序贡献其三个数字。输出必须是以下形式的单个列表字符串\n  $$\n  [E_m^{(1)},E_v^{(1)},D_v^{(1)},E_m^{(2)},E_v^{(2)},D_v^{(2)},E_m^{(3)},E_v^{(3)},D_v^{(3)},E_m^{(4)},E_v^{(4)},D_v^{(4)}].\n  $$\n\n本问题中没有物理单位；拷贝数是无量纲的计数，速率是单位时间的速率。不涉及角度和百分比。输出值必须是实数（浮点数）。不需要用户输入；程序必须按原样运行并按规定打印最后一行。", "solution": "该问题要求使用精确的随机模拟算法（SSA），也称为 Gillespie 直接法，来模拟一个典型的 mRNA 生灭过程。随机模拟的结果将与该过程均值和方差的解析解进行比较。\n\n该系统由一个连续时间马尔可夫链描述，其状态 $X(t)$ 表示在时间 $t$ 的 mRNA 分子数量。状态空间为非负整数集 $\\mathbb{N}_0 = \\{0, 1, 2, \\dots\\}$。控制系统动态的两个反应是：\n1.  **转录（生成/迁入）：** $\\varnothing \\rightarrow \\text{mRNA}$，以恒定的倾向 $a_1(x) = k$ 发生。此反应使分子数增加一。\n2.  **降解（死亡）：** $\\text{mRNA} \\rightarrow \\varnothing$，发生的倾向与分子数成正比，即 $a_2(x) = \\gamma x$。此反应使分子数减少一。\n\n参数 $k \\ge 0$ 和 $\\gamma \\ge 0$ 分别是转录速率和单位分子的降解速率。\n\n### 随机模拟算法（Gillespie 直接法）\n\nSSA 提供了一种统计上精确的方法来模拟这种随机系统的时间演化。从初始状态 $X(0) = x_0$ 和时间 $t=0$ 开始，算法按以下步骤进行：\n\n1.  **计算倾向：** 在给定状态 $x$下，任何反应发生的总倾向是各个倾向的总和：\n    $$\n    a_0(x) = a_1(x) + a_2(x) = k + \\gamma x.\n    $$\n    如果 $a_0(x) = 0$（当且仅当 $k=0$ 且 $x=0$ 时发生），系统处于吸收态，不再有反应发生。该轨迹的模拟终止。\n\n2.  **抽样下一个反应的时间：** 到下一个反应的时间间隔 $\\tau$ 是一个随机变量，服从速率参数为 $a_0(x)$ 的指数分布：\n    $$\n    \\tau \\sim \\text{Exponential}(a_0(x)).\n    $$\n    其概率密度函数为 $p(\\tau) = a_0(x) e^{-a_0(x)\\tau}$。模拟时间向前推进该间隔，即 $t \\leftarrow t + \\tau$。如果新时间 $t+\\tau$ 超过指定的终止时间 $T$，模拟将停止，并将系统的当前状态记录为其在时间 $T$ 的值。\n\n3.  **抽样发生的反应类型：** 下一个发生的反应是根据其对总倾向的相对贡献来选择的。从 $(0, 1)$ 上的均匀分布中抽取一个随机数 $u$。\n    -   如果 $u < \\frac{a_1(x)}{a_0(x)} = \\frac{k}{k+\\gamma x}$，则发生转录，状态更新为：$x \\leftarrow x+1$。\n    -   否则，发生降解，状态更新为：$x \\leftarrow x-1$。\n\n这个计算倾向、抽样时间和抽样反应类型的过程会一直迭代，直到模拟时间 $t$ 达到或超过终止时间 $T$。\n\n为了获得稳健的统计数据，整个模拟过程会对 $R$ 条独立的轨迹重复进行。从得到的最终状态样本 $\\{X_1(T), X_2(T), \\dots, X_R(T)\\}$ 中，我们计算：\n-   经验样本均值：$\\widehat{m}(T) = \\frac{1}{R} \\sum_{i=1}^{R} X_i(T)$。\n-   经验无偏样本方差：$\\widehat{v}(T) = \\frac{1}{R-1} \\sum_{i=1}^{R} (X_i(T) - \\widehat{m}(T))^2$。\n\n### 解析解\n\n$X(t)$ 分布的矩之所以能推导出精确的解析表达式，是因为倾向函数具有线性性质。\n\n**平均场 ODE 解：**\n$X(t)$ 的期望值（记为 $\\mathbb{E}[X(t)]$）的时间演化可以用一个确定性的常微分方程（ODE）来近似，该方程通常被称为平均场或速率方程。这是通过考虑平均变化率推导出来的：\n$$\n\\frac{d}{dt} \\mathbb{E}[X(t)] \\approx k - \\gamma \\mathbb{E}[X(t)].\n$$\n在初始条件 $x_{\\text{det}}(0) = x_0$ 下，求解这个关于 $x_{\\text{det}}(t) = \\mathbb{E}[X(t)]$ 的线性 ODE，会根据 $\\gamma$ 的值得到不同的解：\n-   如果 $\\gamma > 0$：\n    $$\n    x_{\\text{det}}(T) = x_0 e^{-\\gamma T} + \\frac{k}{\\gamma}\\bigl(1 - e^{-\\gamma T}\\bigr).\n    $$\n-   如果 $\\gamma = 0$：\n    $$\n    x_{\\text{det}}(T) = x_0 + k T.\n    $$\n\n**精确解析方差：**\n该过程的精确方差 $v_{\\text{theory}}(T) = \\text{Var}(X(T))$ 也可以从化学主方程推导得出。其解为：\n-   如果 $\\gamma > 0$：\n    $$\n    v_{\\text{theory}}(T) = x_0 e^{-\\gamma T} \\bigl(1 - e^{-\\gamma T}\\bigr) + \\frac{k}{\\gamma}\\bigl(1 - e^{-\\gamma T}\\bigr).\n    $$\n    值得注意的是，对于这个特定过程，方差可以通过 $v_{\\text{theory}}(T) = x_{\\text{det}}(T) - x_0 e^{-2\\gamma T}$ 与均值相关联。\n-   如果 $\\gamma = 0$（一个简单的泊松生成过程）：\n    $$\n    v_{\\text{theory}}(T) = k T.\n    $$\n\n### 指标计算\n\n对于每组参数，计算以下指标以量化随机模拟结果与理论预测之间的关系：\n1.  **平均绝对误差：** $E_m = \\left| \\widehat{m}(T) - x_{\\text{det}}(T) \\right|$。该指标衡量经验估计均值与解析预测均值之间的偏差。\n2.  **方差绝对误差：** $E_v = \\left| \\widehat{v}(T) - v_{\\text{theory}}(T) \\right|$。该指标衡量经验估计方差与精确解析方差之间的偏差。\n3.  **方差差异：** $D_v = \\widehat{v}(T)$。这就是估计方差本身，用作衡量随机波动的指标，而这种波动在确定性平均场模型中是完全不存在的（其方差隐含为零）。\n\n具体的实现将首先定义一个运行单次 SSA 轨迹的函数，然后定义一个函数来组织给定参数集下的 $R$ 次运行，计算经验统计量，计算解析解，并最终导出所需的三个指标。对所有四个测试用例重复此过程，并使用固定的随机数生成器种子以保证可复现性。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run simulations for all test cases and print results.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (k, gamma, x0, T, R)\n        (5.0, 1.0, 10, 2.0, 3000),  # Case 1\n        (0.0, 0.7, 25, 3.5, 3000),  # Case 2\n        (12.0, 4.0, 0, 1.5, 3000),  # Case 3\n        (5.0, 0.0, 3, 1.2, 3000),   # Case 4\n    ]\n\n    # Initialize a single random number generator with the specified seed for reproducibility.\n    rng = np.random.default_rng(1729)\n    \n    all_results = []\n    for case in test_cases:\n        k, gamma, x0, T, R = case\n        \n        # --- Stochastic Simulation (SSA) ---\n        final_states = np.empty(R, dtype=int)\n        for i in range(R):\n            final_states[i] = _run_single_ssa_trajectory(k, gamma, x0, T, rng)\n        \n        # Compute empirical statistics\n        m_hat_T = np.mean(final_states)\n        v_hat_T = np.var(final_states, ddof=1) if R > 1 else 0.0\n\n        # --- Analytical Solutions ---\n        # Deterministic mean-field solution\n        if gamma > 0:\n            x_det_T = x0 * np.exp(-gamma * T) + (k / gamma) * (1.0 - np.exp(-gamma * T))\n        else: # gamma == 0\n            x_det_T = x0 + k * T\n            \n        # Analytical variance\n        if gamma > 0:\n            v_theory_T = x0 * np.exp(-gamma * T) * (1.0 - np.exp(-gamma * T)) + (k / gamma) * (1.0 - np.exp(-gamma * T))\n        else: # gamma == 0\n            v_theory_T = k * T\n            \n        # --- Compute Metrics ---\n        E_m = np.abs(m_hat_T - x_det_T)\n        E_v = np.abs(v_hat_T - v_theory_T)\n        D_v = v_hat_T\n        \n        all_results.extend([E_m, E_v, D_v])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\ndef _run_single_ssa_trajectory(k, gamma, x0, T, rng):\n    \"\"\"\n    Implements the Gillespie direct method for a single trajectory of the mRNA birth-death model.\n    \n    Args:\n        k (float): Transcription rate.\n        gamma (float): Degradation rate.\n        x0 (int): Initial number of molecules.\n        T (float): Terminal time.\n        rng (numpy.random.Generator): The random number generator instance.\n        \n    Returns:\n        int: The number of molecules at time T.\n    \"\"\"\n    t = 0.0\n    x = x0\n    \n    while t  T:\n        # Calculate total propensity a0\n        a0 = k + gamma * x\n        \n        # If a0 is zero, no more reactions can occur (absorbing state)\n        if a0 == 0:\n            break\n        \n        # Sample time to next reaction, tau\n        # tau ~ Exponential(rate=a0) which is equivalent to Exponential(scale=1/a0)\n        tau = rng.exponential(scale=1.0 / a0)\n        \n        # If the next reaction occurs after T, the simulation ends.\n        if t + tau > T:\n            break\n        \n        # Advance time\n        t += tau\n        \n        # Choose which reaction occurs\n        # Draw a uniform random number u\n        u = rng.random()\n        \n        # Probability of transcription is a1/a0 = k/a0\n        if u  k / a0:\n            # Transcription event\n            x += 1\n        else:\n            # Degradation event\n            # Ensure x does not go below 0 (though this is guaranteed by a2=gamma*x)\n            if x > 0:\n                x -= 1\n    \n    # The state at the end of the simulation is the current value of x\n    return x\n\n# Execute the main function\nsolve()\n```", "id": "4357865"}, {"introduction": "理论模型最终需要通过实验数据来检验其有效性。在单细胞生物学领域，一个核心问题是如何描述基因表达的随机性 [@problem_id:4357822]。这个练习将带你进入真实的数据分析场景，使用最大似然估计（Maximum Likelihood Estimation, MLE）以及赤池信息准则（Akaike Information Criterion, AIC）和贝叶斯信息准则（Bayesian Information Criterion, BIC）等工具，来判断泊松（Poisson）分布和负二项（Negative Binomial）分布中哪一个能更好地拟合单细胞 RNA 测序得到的基因表达计数数据，这是计算系统生物学家的一项基本技能。", "problem": "考虑在独立的单细胞检测中测量的单个基因，其产生非负整数计数 $\\{x_1, x_2, \\dots, x_n\\}$，其中每个 $x_i \\in \\{0,1,2,\\dots\\}$。您需要比较两种用于这些计数的随机模型：一个参数为 $\\lambda$ 的泊松模型，以及一个由均值 $\\mu$ 和一个正的离散（形状）参数 $r$ 参数化的负二项模型。假设这些计数是独立同分布的。您的任务是实现一个程序，对于每个提供的测试用例，该程序计算每个模型下模型参数的最大似然估计（Maximum Likelihood Estimation (MLE)），计算每个拟合模型的赤池信息准则（Akaike Information Criterion, AIC）和贝叶斯信息准则（Bayesian Information Criterion, BIC），并根据每个准则输出模型偏好。\n\n该问题的基本原理如下。从每个模型下独立观测值的似然函数定义，以及最大似然估计（MLE）原理出发，该原理选择使似然函数（等价于对数似然函数）最大化的参数。使用赤池信息准则（AIC）和贝叶斯信息准则（BIC）的公认定义，两者都通过估计的自由参数数量来对模型复杂度进行惩罚。决策应通过比较同一数据下不同模型的这些准则值来做出，偏好值较小的模型；若在小的数值容差范围内值相等，则偏好参数较少的更简单的模型。\n\n对于负二项模型，使用均值为 $\\mu$、离散度为 $r0$ 的参数化形式，其对于观测值 $x \\in \\{0,1,2,\\dots\\}$ 的概率质量函数为\n$$\n\\mathbb{P}(X=x \\mid r,\\mu) \\;=\\; \\binom{x+r-1}{x} \\left(\\frac{r}{r+\\mu}\\right)^r \\left(\\frac{\\mu}{r+\\mu}\\right)^x,\n$$\n其均值为 $\\mu$，方差为 $\\mu \\left(1 + \\frac{\\mu}{r}\\right)$。\n\n对于每个数据集，将泊松模型视为有 $k_{\\text{Pois}} = 1$ 个自由参数，负二项模型视为有 $k_{\\text{NB}} = 2$ 个自由参数。当基于AIC或BIC比较模型时，偏好准则值较小的模型；如果两个值在容差 $\\varepsilon = 10^{-8}$ 内相等，则选择泊松模型作为更简约的模型。\n\n不涉及物理单位。所有输出都应为纯整数。不需要百分比或角度单位。您的程序必须在没有MLE闭式解的情况下实现数值优化，并且必须在评估对数似然函数时确保数值稳定性。\n\n使用以下包含五个不同数据集的测试套件，每个数据集代表来自不同数量细胞的单个基因的计数：\n- 测试用例 $1$：$\\{0,1,1,2,1,0,1,2,1,1,2,1,3,0\\}$。\n- 测试用例 $2$：$\\{0,0,5,10,0,3,20,0,7,15,0,1,12\\}$。\n- 测试用例 $3$：$\\{0,0,0,0,0,0,0,0,0\\}$。\n- 测试用例 $4$：$\\{4,5,6,5,4,5,6,5,5,4,6\\}$。\n- 测试用例 $5$：$\\{0,50,0,0,100,2,3,70,90,0,150,1,0\\}$。\n\n您的程序应生成单行输出，其中包含聚合所有测试用例的结果，格式为方括号括起来的逗号分隔列表。对于每个测试用例，输出一个双元素列表 $[a,b]$，其中 $a$ 是AIC准则下的偏好模型，$b$ 是BIC准_则下的偏好模型，均编码为整数，$0$ 代表泊松模型，$1$ 代表负二项模型。因此，最终的输出行应具有 $[[a_1,b_1],[a_2,b_2],[a_3,b_3],[a_4,b_4],[a_5,b_5]]$ 的形式，其中每个 $a_i$ 和 $b_i$ 都是 $\\{0,1\\}$ 中的整数。", "solution": "问题陈述要求对两种离散概率分布——泊松分布和负二项分布——进行比较分析，将它们作为给定单细胞基因表达计数集的模型。该比较将使用赤池信息准则（AIC）和贝叶斯信息准则（BIC）进行，这两个准则旨在平衡模型的拟合优度与模型复杂度。每个准则的决策取决于选择具有较小准则值的模型。\n\n设观测数据为一组 $n$ 个独立同分布（i.i.d.）的非负整数计数 $\\mathbf{x} = \\{x_1, x_2, \\dots, x_n\\}$，其中每个 $x_i \\in \\{0, 1, 2, \\dots\\}$。任务的核心是找到每个模型参数的最大似然估计（MLEs），然后计算相应的AIC和BIC值。\n\n### 模型1：泊松分布\n泊松分布由单个率参数 $\\lambda  0$ 表征，其对于观测值 $x$ 的概率质量函数（PMF）为：\n$$ \\mathbb{P}(X=x \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^x}{x!} $$\n在独立同分布的假设下，数据集 $\\mathbf{x}$ 的似然函数是各个概率的乘积：\n$$ L(\\lambda; \\mathbf{x}) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{x_i}}{x_i!} $$\n为了分析和数值计算的方便，我们使用对数似然函数 $\\ell(\\lambda; \\mathbf{x}) = \\log L(\\lambda; \\mathbf{x})$：\n$$ \\ell(\\lambda; \\mathbf{x}) = \\sum_{i=1}^n \\left( x_i \\log \\lambda - \\lambda - \\log(x_i!) \\right) = (\\log \\lambda) \\sum_{i=1}^n x_i - n\\lambda - \\sum_{i=1}^n \\log(x_i!) $$\n为了找到 $\\lambda$ 的最大似然估计，我们将 $\\ell$ 对 $\\lambda$ 的导数设为零：\n$$ \\frac{d\\ell}{d\\lambda} = \\frac{1}{\\lambda} \\sum_{i=1}^n x_i - n = 0 $$\n这就得到了众所周知的 $\\lambda$ 的闭式MLE，即样本均值：\n$$ \\hat{\\lambda}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n x_i = \\bar{x} $$\n最大化的对数似然值 $\\ell_{\\text{Pois,max}}$ 是通过将 $\\hat{\\lambda}_{\\text{MLE}}$ 代回到对数似然函数中得到的。当所有 $x_i=0$ 时，出现一个特殊情况，此时 $\\bar{x}=0$。在这种情况下，似然函数 $L(\\lambda; \\mathbf{x}) = e^{-n\\lambda}$ 在 $\\lambda \\to 0^+$ 时最大化，因此 $\\hat{\\lambda}_{\\text{MLE}} = 0$，最大化对数似然为 $\\ell_{\\text{Pois,max}} = 0$。\n\n### 模型2：负二项分布\n负二项（NB）分布由均值 $\\mu \\ge 0$ 和一个正的离散参数 $r  0$ 指定。其PMF如下所示：\n$$ \\mathbb{P}(X=x \\mid r, \\mu) = \\binom{x+r-1}{x} \\left(\\frac{r}{r+\\mu}\\right)^r \\left(\\frac{\\mu}{r+\\mu}\\right)^x $$\n该分布的方差为 $\\mu + \\mu^2/r$。项 $\\mu^2/r$ 用于模拟过度离散，即与具有相同均值的泊松分布相比多出的方差。\n\n数据集 $\\mathbf{x}$ 的对数似然为 $\\ell(r, \\mu; \\mathbf{x}) = \\sum_{i=1}^n \\log \\mathbb{P}(X=x_i \\mid r, \\mu)$。使用恒等式 $\\binom{n}{k} = \\frac{\\Gamma(n+1)}{\\Gamma(k+1)\\Gamma(n-k+1)}$，单个观测值 $x_i$ 的对数似然可以用对数伽马函数 $\\log \\Gamma(\\cdot)$ 写成：\n$$ \\ell_i(r, \\mu) = \\log\\Gamma(x_i+r) - \\log\\Gamma(r) - \\log\\Gamma(x_i+1) + r\\log r + x_i\\log\\mu - (x_i+r)\\log(r+\\mu) $$\n将总对数似然 $\\ell = \\sum_i \\ell_i$ 对 $\\mu$ 求导并将结果设为零，可以发现均值参数的MLE也是样本均值：\n$$ \\hat{\\mu}_{\\text{MLE}} = \\bar{x} $$\n这简化了问题，因为我们只需要通过最大化剖面对数似然 $\\ell(r; \\mathbf{x}, \\hat{\\mu}_{\\text{MLE}})$ 来找到 $r$ 的MLE。$\\hat{r}_{\\text{MLE}}$ 没有闭式解，因此必须使用数值优化来求解。我们旨在最大化以下函数：\n$$ \\ell(r; \\mathbf{x}) = \\sum_{i=1}^n \\left[ \\log\\Gamma(x_i+r) - \\log\\Gamma(r) - \\log\\Gamma(x_i+1) + r\\log r + x_i\\log\\bar{x} - (x_i+r)\\log(r+\\bar{x}) \\right] $$\n当样本方差 $\\hat{\\sigma}^2 = \\frac{1}{n}\\sum(x_i-\\bar{x})^2$ 小于或等于样本均值 $\\bar{x}$ 时，会出现一个重要的特殊情况。这种情况被称为低度离散，它意味着NB模型增加的复杂度没有得到数据的支持。在这种情况下，对数似然函数是关于 $r$ 的单调递增函数，并且 $r$ 的MLE趋于无穷大（$r \\to \\infty$）。当 $r \\to \\infty$ 时，NB分布收敛于均值为 $\\mu$ 的泊松分布。因此，如果 $\\hat{\\sigma}^2 \\le \\bar{x}$，则泊松模型是天然更受偏好的。\n\n### 模型选择准则\nAIC和BIC定义如下：\n$$ \\text{AIC} = 2k - 2\\ell_{\\text{max}} $$\n$$ \\text{BIC} = k \\log(n) - 2\\ell_{\\text{max}} $$\n其中 $k$ 是估计的参数数量，$n$ 是样本大小，$\\ell_{\\text{max}}$ 是最大化的对数似然值。对于此问题，参数数量为 $k_{\\text{Pois}} = 1$（对于 $\\lambda$）和 $k_{\\text{NB}} = 2$（对于 $\\mu$ 和 $r$）。\n\n### 决策过程\n对于每个数据集，我们遵循以下步骤：\n1.  计算样本大小 $n$ 和样本均值 $\\bar{x}$。\n2.  如果 $\\bar{x}=0$，则数据完全由零组成。$\\hat{\\lambda}=0$ 的泊松模型提供了完美拟合（$\\ell_{\\text{Pois,max}}=0$）。$\\hat{\\mu}=0$ 的NB模型也得到 $\\ell_{\\text{NB,max}}=0$，但它更复杂（$k=2$ vs $k=1$）。因此，对于AIC和BIC，都偏好泊松模型。\n3.  计算样本方差 $\\hat{\\sigma}^2$。如果 $\\hat{\\sigma}^2 \\le \\bar{x}$，数据没有过度离散。NB模型的MLE收敛到泊松模型，因此由于AIC和BIC中的惩罚项，更简单的泊松模型更受偏好。\n4.  如果数据是过度离散的（$\\bar{x}  0$ 且 $\\hat{\\sigma}^2  \\bar{x}$），我们进行全面比较：\n    a.  使用 $\\hat{\\lambda} = \\bar{x}$ 计算 $\\ell_{\\text{Pois,max}}$。\n    b.  在保持 $\\mu = \\bar{x}$ 的条件下，对NB对数似然函数关于 $r0$ 进行数值优化，以找到 $\\hat{r}$ 和相应的 $\\ell_{\\text{NB,max}}$。这是通过最小化负对数似然函数来实现的。\n    c.  计算 $\\text{AIC}_{\\text{Pois}}$、$\\text{BIC}_{\\text{Pois}}$、$\\text{AIC}_{\\text{NB}}$ 和 $\\text{BIC}_{\\text{NB}}$。\n    d.  对于每个准则，选择值较小的模型。如果值在 $\\varepsilon = 10^{-8}$ 的容差范围内，则选择更简约的泊松模型。偏好被编码为：泊松模型为 $0$，负二项模型为 $1$。\n这个过程为模型选择提供了一个严谨的框架，其基础是最大似然原理和信息论。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize_scalar\nfrom scipy.special import gammaln\n\ndef _calculate_model_preference(counts: list[int]) -> list[int]:\n    \"\"\"\n    Calculates model preference for a single dataset.\n    \n    Compares Poisson and Negative Binomial models using AIC and BIC.\n\n    Args:\n        counts: A list of non-negative integer counts.\n\n    Returns:\n        A list [aic_preference, bic_preference] where 0 indicates a preference\n        for the Poisson model and 1 for the Negative Binomial model.\n    \"\"\"\n    x = np.array(counts)\n    n = len(x)\n    mu_mle = np.mean(x)\n    tol = 1e-8\n\n    # Case 1: Mean is zero. Data is all zeros.\n    # Poisson with lambda=0 is a perfect fit (LL=0).\n    # NB(mu=0, r) also has LL=0 but is more complex (k=2 vs k=1).\n    # The more parsimonious Poisson model is always preferred.\n    if mu_mle == 0:\n        return [0, 0]\n        \n    # Case 2: Data is under-dispersed or equi-dispersed.\n    # The MLE for NB dispersion 'r' tends to infinity, meaning the NB model\n    # converges to the Poisson model. The additional parameter is not justified.\n    var_mle = np.var(x, ddof=0)\n    if var_mle = mu_mle:\n        return [0, 0]\n\n    # ----- Poisson Model Calculation -----\n    # The log-likelihood is sum(x_i*log(lambda) - lambda - log(x_i!)).\n    # We use gammaln(x+1) for log(x!).\n    log_lambda = np.log(mu_mle)\n    ll_pois = np.sum(x * log_lambda - mu_mle - gammaln(x + 1))\n\n    k_pois = 1\n    aic_pois = 2 * k_pois - 2 * ll_pois\n    bic_pois = k_pois * np.log(n) - 2 * ll_pois\n\n    # ----- Negative Binomial Model Calculation -----\n    \n    def neg_log_likelihood_nb_r(r: float, data: np.ndarray, mu: float) -> float:\n        \"\"\"Negative log-likelihood of NB model, to be minimized for r.\"\"\"\n        if r = 0:\n            return np.inf\n        # The log-likelihood is sum(log(P(X=x_i | r, mu)))\n        log_likelihood = np.sum(\n            gammaln(data + r) - gammaln(r) - gammaln(data + 1) +\n            r * np.log(r) + data * np.log(mu) - (data + r) * np.log(r + mu)\n        )\n        return -log_likelihood\n\n    # Numerically optimize for r, given mu_mle.\n    # The 'bounded' method is used to ensure r > 0.\n    res = minimize_scalar(\n        neg_log_likelihood_nb_r,\n        args=(x, mu_mle),\n        bounds=(1e-8, 1e8),  # Search for r in a reasonable positive range.\n        method='bounded'\n    )\n    \n    # Maximized log-likelihood for NB is the negative of the minimized function value.\n    ll_nb = -res.fun\n    \n    k_nb = 2\n    aic_nb = 2 * k_nb - 2 * ll_nb\n    bic_nb = k_nb * np.log(n) - 2 * ll_nb\n\n    # ----- Model Comparison -----\n    # Prefer the model with the lower criterion value.\n    # If values are within tolerance, prefer the simpler Poisson model (output 0).\n    aic_pref = 1 if aic_nb  aic_pois - tol else 0\n    bic_pref = 1 if bic_nb  bic_pois - tol else 0\n    \n    return [aic_pref, bic_pref]\n\ndef solve():\n    \"\"\"\n    Main function to run the model comparison on all test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        [0, 1, 1, 2, 1, 0, 1, 2, 1, 1, 2, 1, 3, 0],\n        [0, 0, 5, 10, 0, 3, 20, 0, 7, 15, 0, 1, 12],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [4, 5, 6, 5, 4, 5, 6, 5, 5, 4, 6],\n        [0, 50, 0, 0, 100, 2, 3, 70, 90, 0, 150, 1, 0]\n    ]\n\n    results = []\n    for case_data in test_cases:\n        a, b = _calculate_model_preference(case_data)\n        results.append(f\"[{a},{b}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "4357822"}]}