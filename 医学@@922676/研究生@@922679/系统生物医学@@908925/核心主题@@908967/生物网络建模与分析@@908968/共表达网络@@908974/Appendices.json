{"hands_on_practices": [{"introduction": "构建一个有意义的共表达网络，首要任务是将基因间的相关性矩阵转化为一个能够有效反映生物学结构的邻接矩阵。这一转化的关键在于选择合适的软阈值幂（$\\beta$），它直接决定了网络的拓扑结构。本练习将指导您设计一个标准化的流程，通过权衡网络的无标度特性和平均连接度，来科学地选择最佳的$\\beta$值[@problem_id:4328704]。这是一个在真实数据分析中至关重要的实践步骤，能帮助您构建出既不过于稀疏也不过于嘈杂的稳健网络。", "problem": "您的任务是为软阈值幂参数 $ \\beta $ 设计并实现一个有原则的选择程序，用于从基因表达数据构建加权共表达网络，并与系统生物医学的目标保持一致。该网络通过对绝对皮尔逊相关系数 $ |r_{ij}| $进行 $ a_{ij} = |r_{ij}|^{\\beta} $ 变换来构建一个加权邻接矩阵，其中 $ a_{ii} = 0 $ 且 $ i \\neq j $ 索引不同的基因。目标是选择一个能够平衡两个目标的 $ \\beta $ 值：近似的无标度行为和足够的平均连接度。\n\n使用的基本定义和出发点：\n- 基因 $ i $ 和基因 $ j $ 之间的皮尔逊相关性是为表达向量 $ \\mathbf{x}_i \\in \\mathbb{R}^m $ 和 $ \\mathbf{x}_j \\in \\mathbb{R}^m $（在 $ m $ 个样本中）定义的，具体如下：\n$$\nr_{ij} = \\frac{\\sum_{s=1}^{m} (x_{i,s} - \\bar{x}_i)(x_{j,s} - \\bar{x}_j)}{\\sqrt{\\sum_{s=1}^{m} (x_{i,s} - \\bar{x}_i)^2} \\sqrt{\\sum_{s=1}^{m} (x_{j,s} - \\bar{x}_j)^2}},\n$$\n其中 $ \\bar{x}_i $ 是 $ \\mathbf{x}_i $ 的样本均值。\n- 加权邻接矩阵定义为：\n$$\na_{ij}(\\beta) = \n\\begin{cases}\n|r_{ij}|^{\\beta},  i \\neq j, \\\\\n0,  i = j.\n\\end{cases}\n$$\n- 节点 $ i $ 的加权连接度（度）为\n$$\nk_i(\\beta) = \\sum_{j \\neq i} a_{ij}(\\beta).\n$$\n- 平均连接度为\n$$\n\\bar{k}(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} k_i(\\beta),\n$$\n其中 $ n $ 是基因数量。\n- 近似无标度行为通过经验分布 $ p(k) $ 与幂律 $ p(k) \\propto k^{-\\gamma} $ 的线性拟合来评估，并通过 $ \\log p(k) $ 对 $ \\log k $ 进行线性回归的决定系数 $ R^2(\\beta) $ 来量化。为近似 $ p(k) $，使用 $ \\{k_i(\\beta)\\}_{i=1}^{n} $ 的直方图，其分箱数由 Sturges 公式 $ B = \\lceil \\log_2(n) + 1 \\rceil $ 确定，中点为 $ \\{m_b\\}_{b=1}^{B} $。为保证稳定性，排除计数为零的箱，并加上一个小的常数 $ \\delta $ 以避免对零取对数，即在回归中计算 $ y_b = \\log(p_b + \\delta) $ 和 $ x_b = \\log(m_b + \\delta) $。线性回归是普通最小二乘法，决定系数为\n$$\nR^2(\\beta) = 1 - \\frac{\\sum_{b} (y_b - \\hat{y}_b)^2}{\\sum_{b} (y_b - \\bar{y})^2},\n$$\n其中 $ \\hat{y}_b $ 是拟合值，$ \\bar{y} $ 是 $ \\{y_b\\} $ 的均值。如果非空箱少于 $ 2 $ 个或 $ \\{y_b\\} $ 的方差为零，则设 $ R^2(\\beta) = 0 $。\n\n按如下方式设计选择程序：\n1. 对于给定集合中的每个候选 $ \\beta $，计算如上定义的 $ R^2(\\beta) $ 和平均连接度 $ \\bar{k}(\\beta) $。\n2. 将平均连接度按其理论最大值 $ n - 1 $ 进行归一化，得到\n$$\nc(\\beta) = \\frac{\\bar{k}(\\beta)}{n - 1},\n$$\n其值在 $ [0, 1] $ 区间内。\n3. 定义一个标量目标，使用权重 $ \\alpha \\in [0, 1] $ 来平衡两个目标：\n$$\nO(\\beta) = \\alpha \\, R^2(\\beta) + (1 - \\alpha) \\, c(\\beta).\n$$\n4. 选择使 $ O(\\beta) $ 最大化的 $ \\beta^\\star $。如果在数值容差范围内出现平局，则优先选择最小的 $ \\beta $。\n5. 报告三元组 $ [\\beta^\\star, R^2(\\beta^\\star), \\bar{k}(\\beta^\\star)] $，其中第二和第三个条目以小数形式表示（无百分号），并四舍五入到四位小数。\n\n实现此程序并将其应用于以下测试套件。在所有情况下，取对数时设置 $ \\delta = 10^{-10} $。不涉及物理单位。不出现角度。所有结果必须是小数或整数，不带百分号。\n\n测试套件：\n- 情况 $ A $（理想情况，模块化结构）：\n    - 基因数量 $ n = 50 $，样本数量 $ m = 60 $。\n    - 构建 $ 2 $ 个潜模块因子 $ \\mathbf{L}_1, \\mathbf{L}_2 \\in \\mathbb{R}^m $，其条目独立地从标准正态分布中抽取。\n    - 将基因 $ 1 $ 到 $ 20 $ 分配给模块 $ 1 $，基因 $ 21 $ 到 $ 35 $ 分配给模块 $ 2 $，基因 $ 36 $ 到 $ 50 $ 为非模块化基因。\n    - 对于模块 $ j \\in \\{1, 2\\} $ 中的基因，其表达生成方式为 $ \\mathbf{x}_i = 0.9 \\, \\mathbf{L}_j + \\boldsymbol{\\epsilon}_i $，其中独立噪声 $ \\boldsymbol{\\epsilon}_i $ 的条目从均值为 $ 0 $、标准差为 $ 0.5 $ 的正态分布中抽取。\n    - 对于非模块化基因，使用 $ \\mathbf{x}_i = \\boldsymbol{\\epsilon}_i $，其条目从均值为 $ 0 $、标准差为 $ 1 $ 的正态分布中抽取。\n    - 计算样本间的 $ r_{ij} $，然后计算 $ a_{ij}(\\beta) $。\n    - 候选幂次：$ \\beta \\in \\{1, 2, 3, 4, 5, 6, 7, 8, 9, 10\\} $。\n    - 权重：$ \\alpha = 0.8 $。\n    - 随机种子：在此情况下使用固定的种子 $ 42 $ 以保证可复现性。\n\n- 情况 $ B $（边界情况，具有均匀相关性的近完全网络）：\n    - 基因数量 $ n = 30 $。\n    - 定义 $ r_{ij} = 0.9 $（对于 $ i \\neq j $）和 $ r_{ii} = 1 $。\n    - 候选幂次：$ \\beta \\in \\{1, 2, 4, 8\\} $。\n    - 权重：$ \\alpha = 0.5 $。\n\n- 情况 $ C $（边缘情况，弱共表达）：\n    - 基因数量 $ n = 40 $，样本数量 $ m = 50 $。\n    - 生成每个 $ \\mathbf{x}_i $，其条目独立地从标准正态分布中抽取（基因间独立）。\n    - 计算样本间的 $ r_{ij} $，然后计算 $ a_{ij}(\\beta) $。\n    - 候选幂次：$ \\beta \\in \\{1, 2, 3, 4, 5\\} $。\n    - 权重：$ \\alpha = 0.7 $。\n    - 随机种子：在此情况下使用固定的种子 $ 123 $ 以保证可复现性。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个元素对应一个测试用例，其本身为 $ [\\beta^\\star, R^2(\\beta^\\star), \\bar{k}(\\beta^\\star)] $ 形式的列表。例如，格式必须是 $ [[b_1,r_1,k_1],[b_2,r_2,k_2],[b_3,r_3,k_3]] $。所有数值条目必须是小数或整数，不带百分号，其中 $ R^2 $ 和 $ \\bar{k} $ 四舍五入到四位小数。", "solution": "任务是设计并实现一个程序，用于在构建加权基因共表达网络时选择最佳的软阈值幂 $\\beta$。该选择必须平衡两个相互竞争的目标：实现近似无标度的网络拓扑结构，并保持足够的平均连接度。这是系统生物医学中一个常见且根本性的问题，尤其是在加权基因共表达网络分析 (WGCNA) 等方法中。\n\n此程序背后的核心原则是在选择 $\\beta$ 时固有的权衡。网络的邻接矩阵通过将基因表达谱之间的绝对皮尔逊相关值 $|r_{ij}|$ 进行幂函数变换 $a_{ij} = |r_{ij}|^{\\beta}$（对于 $i \\neq j$）来定义。较高的 $\\beta$ 值会放大高相关性和低相关性之间的对比度，从而有效滤除弱连接并强调强连接。这可以将网络的度分布 $p(k)$ 推向幂律 $p(k) \\propto k^{-\\gamma}$，这是无标度拓扑的特征。无标度网络被认为是稳健的，并拥有关键的枢纽节点，这是识别重要基因的一个理想特性。然而，过高的 $\\beta$ 也可能导致网络非常稀疏，整体连接度低，可能会丢弃中等相关性中包含的有价值的生物学信息。\n\n我们的程序根据一个封装了这种权衡的形式化目标函数，系统地评估一组候选的 $\\beta$ 值。设计的算法对每个测试用例按以下步骤进行。\n\n首先，我们建立基础相关结构。对于需要生成数据的情况（情况 A 和 C），我们根据指定的统计模型和随机种子，合成一个大小为 $n \\times m$（$n$ 个基因，$m$ 个样本）的表达矩阵 $\\mathbf{X}$。对于情况 B，相关矩阵 $\\mathbf{R}$ 被直接定义。对于情况 A 和 C，皮尔逊相关矩阵 $\\mathbf{R}$ 从 $\\mathbf{X}$ 计算得出，其中每个条目 $r_{ij}$ 衡量基因 $i$ 和基因 $j$ 表达之间的线性关系。\n\n其次，对于所提供集合中的每个候选幂 $\\beta$，我们执行以下计算：\n1.  **构建邻接矩阵**：计算加权邻接矩阵 $\\mathbf{A}(\\beta)$。其元素由 $a_{ij}(\\beta) = |r_{ij}|^{\\beta}$ 给出（对于 $i \\neq j$），对角线元素 $a_{ii}(\\beta)$ 设置为 $0$ 以排除自环。\n\n2.  **计算节点连接度**：每个基因 $i$ 的连接度（或度）$k_i(\\beta)$ 通过对其所有连接的权重求和来计算：$k_i(\\beta) = \\sum_{j \\neq i} a_{ij}(\\beta)$。\n\n3.  **评估无标度拓扑**：我们量化网络连接度分布近似幂律的程度。这通过计算线性回归的决定系数 $R^2(\\beta)$ 来实现。\n    - 将连接度集合 $\\{k_i(\\beta)\\}_{i=1}^{n}$ 分箱成一个直方图。分箱数 $B$ 由 Sturges 公式确定，$B = \\lceil \\log_2(n) + 1 \\rceil$。\n    - 幂律分布 $p(k) \\propto k^{-\\gamma}$ 的核心思想是其对数与变量的对数呈线性关系：$\\log p(k) = -\\gamma \\log k + C$。我们检验这种关系。\n    - 对于直方图的每个非空箱 $b$，我们确定其中点 $m_b$ 和其频率计数 $p_b$。然后我们对变换后的坐标 $y_b = \\log(p_b + \\delta)$ 对 $x_b = \\log(m_b + \\delta)$ 进行普通最小二乘线性回归，其中 $\\delta=10^{-10}$ 是一个小的常数，以防止对零取对数时出现数值问题。\n    - 此回归的 $R^2(\\beta)$ 值衡量了对线性模型的拟合优度。接近 $1$ 的值表示与幂律拟合良好，因此拓扑近似无标度。在非空箱少于 $2$ 个或 $\\{y_b\\}$ 的方差为零的特殊情况下，$R^2(\\beta)$ 定义为 $0$。\n\n4.  **计算平均连接度**：计算网络的平均连接度 $\\bar{k}(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} k_i(\\beta)$。为了创建一个标准化指标，该值通过最大可能的平均连接度 $n-1$ 进行归一化，得到 $c(\\beta) = \\frac{\\bar{k}(\\beta)}{n-1}$。这个归一化值 $c(\\beta)$ 位于 $[0, 1]$ 区间内。\n\n第三，我们选择最优幂 $\\beta^\\star$。两个指标 $R^2(\\beta)$ 和 $c(\\beta)$ 被组合成一个单一的标量目标函数 $O(\\beta) = \\alpha \\, R^2(\\beta) + (1 - \\alpha) \\, c(\\beta)$，其中 $\\alpha \\in [0, 1]$ 是一个权重，用于调整无标度拓扑拟合与平均连接度的相对重要性。我们为所有候选幂计算 $O(\\beta)$。\n\n最优幂 $\\beta^\\star$ 被选为最大化 $O(\\beta)$ 的那个。如果在 $O(\\beta)$ 的值上出现平局（在数值精度范围内），则选择竞争者中最小的 $\\beta$。当主要目标指标饱和时，此平局打破规则偏好更简单的模型和更密集的网络。\n\n最后，对于每个测试用例，报告所选的三元组 $[\\beta^\\star, R^2(\\beta^\\star), \\bar{k}(\\beta^\\star)]$，其中 $R^2$ 和平均连接度值四舍五入到四位小数。整个过程被实现并应用于所提供的测试套件。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef calculate_r_squared(k, n, delta):\n    \"\"\"\n    Calculates the scale-free topology fit R^2 for a given connectivity vector k.\n    \n    Args:\n        k (np.ndarray): Vector of node connectivities.\n        n (int): Number of genes (nodes).\n        delta (float): Small constant to avoid log(0).\n\n    Returns:\n        float: The R^2 value.\n    \"\"\"\n    if n == 1:\n        return 0.0\n    num_bins = int(np.ceil(np.log2(n) + 1))\n    \n    # Filter out any potential non-finite values from k\n    k_finite = k[np.isfinite(k)]\n    if k_finite.size == 0:\n        return 0.0\n\n    counts, bin_edges = np.histogram(k_finite, bins=num_bins)\n    \n    non_empty_indices = np.where(counts > 0)[0]\n    \n    # If there are fewer than 2 non-empty bins, R^2 is undefined/unstable.\n    if len(non_empty_indices)  2:\n        return 0.0\n        \n    non_empty_counts = counts[non_empty_indices]\n    \n    bin_midpoints = (bin_edges[:-1] + bin_edges[1:]) / 2.0\n    non_empty_midpoints = bin_midpoints[non_empty_indices]\n    \n    # Avoid log of zero for midpoints\n    # Connectivities k_i are >= 0, so midpoints are >= 0.\n    valid_midpoints_mask = non_empty_midpoints > 0\n    if np.sum(valid_midpoints_mask)  2:\n        return 0.0\n\n    x = np.log(non_empty_midpoints[valid_midpoints_mask] + delta)\n    y = np.log(non_empty_counts[valid_midpoints_mask] + delta)\n    \n    if x.size  2:\n        return 0.0\n\n    # If variance of y is zero, R^2 is 0 or undefined.\n    if np.var(y)  1e-12:\n        return 0.0\n        \n    # If variance of x is zero (should not happen if x.size > 1 and midpoints are distinct)\n    if np.var(x)  1e-12:\n        return 0.0\n\n    lin_reg_result = stats.linregress(x, y)\n    r_squared = lin_reg_result.rvalue**2\n    \n    return r_squared\n\ndef solve_case(params):\n    \"\"\"\n    Solves a single test case for beta selection.\n    \"\"\"\n    case_name = params['name']\n    n = params['n']\n    beta_candidates = params['betas']\n    alpha = params['alpha']\n    delta = 1e-10\n    \n    R = None\n\n    if case_name == 'A':\n        m = params['m']\n        seed = params['seed']\n        rng = np.random.default_rng(seed)\n        \n        L1 = rng.normal(size=m)\n        L2 = rng.normal(size=m)\n        X = np.zeros((n, m))\n        \n        # Module 1: genes 1-20 (indices 0-19)\n        eps1 = rng.normal(loc=0.0, scale=0.5, size=(20, m))\n        X[0:20, :] = 0.9 * L1[np.newaxis, :] + eps1\n        # Module 2: genes 21-35 (indices 20-34)\n        eps2 = rng.normal(loc=0.0, scale=0.5, size=(15, m))\n        X[20:35, :] = 0.9 * L2[np.newaxis, :] + eps2\n        # Non-modular: genes 36-50 (indices 35-49)\n        eps3 = rng.normal(loc=0.0, scale=1.0, size=(15, m))\n        X[35:50, :] = eps3\n        \n        R = np.corrcoef(X)\n        \n    elif case_name == 'B':\n        R = np.full((n, n), 0.9)\n        np.fill_diagonal(R, 1.0)\n        \n    elif case_name == 'C':\n        m = params['m']\n        seed = params['seed']\n        rng = np.random.default_rng(seed)\n        X = rng.normal(loc=0.0, scale=1.0, size=(n, m))\n        R = np.corrcoef(X)\n    \n    abs_R = np.abs(R)\n    \n    results = []\n    for beta in beta_candidates:\n        A = np.power(abs_R, beta)\n        np.fill_diagonal(A, 0)\n        \n        k = A.sum(axis=1)\n        k_bar = k.mean()\n        \n        r_sq = calculate_r_squared(k, n, delta)\n        \n        c = k_bar / (n - 1) if n > 1 else 0\n        \n        O = alpha * r_sq + (1 - alpha) * c\n        \n        results.append({'beta': beta, 'O': O, 'r_sq': r_sq, 'k_bar': k_bar})\n        \n    # Sort by objective value descending, then by beta ascending to break ties\n    best_result = sorted(results, key=lambda x: (-x['O'], x['beta']))[0]\n    \n    return [best_result['beta'], best_result['r_sq'], best_result['k_bar']]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        {\n            'name': 'A', 'n': 50, 'm': 60, 'seed': 42,\n            'betas': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'alpha': 0.8\n        },\n        {\n            'name': 'B', 'n': 30, 'betas': [1, 2, 4, 8], 'alpha': 0.5\n        },\n        {\n            'name': 'C', 'n': 40, 'm': 50, 'seed': 123,\n            'betas': [1, 2, 3, 4, 5], 'alpha': 0.7\n        }\n    ]\n\n    all_results = []\n    for case_params in test_cases:\n        result = solve_case(case_params)\n        all_results.append(result)\n\n    # Format the final output string as per requirements\n    formatted_parts = []\n    for part in all_results:\n        beta_val = part[0]\n        r2_val = part[1]\n        k_val = part[2]\n        formatted_parts.append(f\"[{beta_val},{r2_val:.4f},{k_val:.4f}]\")\n    \n    print(f\"[{','.join(formatted_parts)}]\")\n\nsolve()\n\n```", "id": "4328704"}, {"introduction": "在确定了邻接矩阵后，为了进一步提炼网络结构并降低假阳性连接，加权基因共表达网络分析（WGCNA）引入了拓扑重叠矩阵（TOM）。这项练习将带您回归基础，通过一个具体的三基因系统，亲手计算从邻接矩阵到TOM值的全过程[@problem_id:2854762]。掌握这个核心计算不仅能帮助您深刻理解TOM的定义，也为后续辨析TOM相较于普通邻接矩阵的优势奠定了基础。", "problem": "在系统遗传学中，通过将基因间的成对相关性转换为加权邻接矩阵，然后通过拓扑重叠测量 (Topological Overlap Measure, TOM) 来量化节点的邻近度，从而构建基因共表达网络。考虑三个基因 $G_{1}$、$G_{2}$ 和 $G_{3}$，其成对 Pearson 相关性的对称矩阵由 $r_{12} = 0.8$、$r_{13} = 0.3$ 和 $r_{23} = -0.4$ 给出，且对于 $i \\in \\{1,2,3\\}$，$r_{ii} = 1$。使用无符号加权基因共表达网络分析 (WGCNA) 的约定，软阈值幂 $\\beta = 6$ 来构建加权邻接矩阵，然后计算 $G_{1}$ 和 $G_{2}$ 之间的加权拓扑重叠测量 (TOM)。假设标准的 WGCNA 约定，即自邻接性满足 $a_{ii} = 0$。\n\n执行以下操作：\n- 根据给定的成对相关性，计算所有非对角邻接项 $a_{ij}$。\n- 使用加权基因共表达网络分析 (WGCNA) 中使用的 TOM 标准加权定义，计算 $G_{1}$ 和 $G_{2}$ 之间的 TOM。\n\n作为最终答案，仅报告 $G_{1}$ 和 $G_{2}$ 之间的 TOM 值，四舍五入到四位有效数字。无需单位。", "solution": "问题陈述具有科学依据，定义明确，并为得出唯一解提供了所有必要信息。这是加权基因共表达网络分析 (WGCNA) 方法的一个标准应用。我们继续进行计算。\n\n第一步是根据给定的 Pearson 相关矩阵 $R = [r_{ij}]$ 构建加权邻接矩阵 $A = [a_{ij}]$。对于无符号网络，基因 $i$ 和 $j$ 之间的邻接性 $a_{ij}$ 使用软阈值幂 $\\beta$ 进行计算。公式如下：\n$$a_{ij} = |r_{ij}|^{\\beta}$$\n问题给出的软阈值幂为 $\\beta = 6$。给定的非对角线 Pearson 相关性为 $r_{12} = 0.8$，$r_{13} = 0.3$ 和 $r_{23} = -0.4$。邻接矩阵是对称的（$a_{ij} = a_{ji}$），并且问题指定对角线元素为零（$a_{ii} = 0$）。我们计算所需的非对角邻接项：\n$$a_{12} = |r_{12}|^{\\beta} = |0.8|^{6} = (0.8)^{6} = 0.262144$$\n$$a_{13} = |r_{13}|^{\\beta} = |0.3|^{6} = (0.3)^{6} = 0.000729$$\n$$a_{23} = |r_{23}|^{\\beta} = |-0.4|^{6} = (0.4)^{6} = 0.004096$$\n\n第二步是计算基因 $G_1$ 和 $G_2$ 之间的拓扑重叠测量 (TOM)，记为 $\\omega_{12}$。加权 TOM 的标准公式为：\n$$\\omega_{ij} = \\frac{l_{ij} + a_{ij}}{\\min(k_i, k_j) + 1 - a_{ij}}$$\n在这里，$k_i$ 是基因 $i$ 的总网络连通性（或节点度），$l_{ij}$ 表示基因 $i$ 和 $j$ 之间的共享连通性。这些术语定义如下：\n$$k_i = \\sum_{u \\neq i} a_{iu}$$\n$$l_{ij} = \\sum_{u \\neq i, j} a_{iu} a_{uj}$$\n我们必须为基因对 $(i, j) = (1, 2)$ 计算这些量。\n\n首先，我们计算连通性 $k_1$ 和 $k_2$：\n$$k_1 = \\sum_{u \\neq 1} a_{1u} = a_{12} + a_{13} = 0.262144 + 0.000729 = 0.262873$$\n$$k_2 = \\sum_{u \\neq 2} a_{2u} = a_{21} + a_{23} = a_{12} + a_{23} = 0.262144 + 0.004096 = 0.26624$$\n\n接下来，我们计算共享连通性项 $l_{12}$。求和是针对所有非 $1$ 或 $2$ 的节点 $u$。在这个三基因系统中，唯一这样的节点是 $u=3$。\n$$l_{12} = \\sum_{u \\neq 1, 2} a_{1u} a_{u2} = a_{13} a_{32} = a_{13} a_{23}$$\n$$l_{12} = (0.000729) \\times (0.004096) = 0.000002985984 = 2.985984 \\times 10^{-6}$$\n\n现在我们有了计算 $\\omega_{12}$ 的所有组成部分。分子是：\n$$l_{12} + a_{12} = 2.985984 \\times 10^{-6} + 0.262144 = 0.262146985984$$\n分母需要 $\\min(k_1, k_2)$：\n$$\\min(k_1, k_2) = \\min(0.262873, 0.26624) = 0.262873$$\n所以，分母是：\n$$\\min(k_1, k_2) + 1 - a_{12} = 0.262873 + 1 - 0.262144 = 1.000729$$\n\n最后，我们计算 TOM 值 $\\omega_{12}$：\n$$\\omega_{12} = \\frac{0.262146985984}{1.000729} \\approx 0.26196232$$\n问题要求答案四舍五入到四位有效数字。\n$$\\omega_{12} \\approx 0.2620$$\n该值表示基因 $G_1$ 和 $G_2$ 之间的拓扑重叠，它基于它们在网络中的直接连接以及与其他基因的共享连接来量化它们的相似性。", "answer": "$$\\boxed{0.2620}$$", "id": "2854762"}, {"introduction": "我们已经学习了如何计算拓扑重叠矩阵（TOM），但其真正的威力在于它能够区分真实的生物模块信号和随机的强相关噪声。这项练习旨在通过一系列精心设计的思想实验，从原理上阐明并量化TOM的这一优势[@problem_id:4328763]。您将看到，一个缺乏共享邻居支持的孤立强相关边，其权重在TOM计算中会受到抑制，从而验证了TOM作为一种更可靠的生物连接性度量方法的价值。", "problem": "您的任务是，从第一性原理出发，形式化阐述拓扑重叠指数（TOM）为何能减少加权共表达网络中那些因缺乏更广泛拓扑支持、仅由单一高相关性边产生的伪连接，然后，在几个科学上合理的小型测试网络上量化这种效应。请从以下在系统生物医学和基因共表达分析中标准的基础定义和假设开始。\n\n基本定义与假设：\n- 一个基因共表达网络是一个包含 $n$ 个基因的无向加权图，其中基因 $i$ 和基因 $j$ 之间的权重是它们两两之间皮尔逊相关性 $r_{ij}$ 的函数，根据定义，$-1 \\le r_{ij} \\le 1$ 且 $r_{ii} = 1$。\n- 一个加权基因共表达网络分析（WGCNA）风格的网络，其软阈值邻接矩阵构建为 $a_{ij} = |r_{ij}|^\\beta$（当 $i \\ne j$ 时），同时 $a_{ii} = 0$ 且 $\\beta  0$。这是一种经过充分检验的方法，用于在保留连续权重的同时强调强相关性。\n- 两个节点之间的拓扑重叠指数（TOM）是一个依赖于加权共享邻域重叠和它们的节点度的函数。它被广泛用于通过考虑网络拓扑而不仅仅是两两之间的相关性来优化邻接关系。\n\n您的程序必须在不依赖任何外部输入的情况下实现以下内容：\n1. 给定一个对称相关矩阵 $R \\in \\mathbb{R}^{n \\times n}$（其中 $R_{ii} = 1$ 且 $|R_{ij}| \\le 1$）和一个软阈值指数 $\\beta  0$，计算加权邻接矩阵 $A$，其定义为 $A_{ij} = |R_{ij}|^\\beta$（当 $i \\ne j$ 时）且 $A_{ii} = 0$。\n2. 基于标准的邻域重叠构建方法，从 $A$ 计算加权拓扑重叠指数（TOM）矩阵 $T$。该方法融合了共享邻居连通性和节点度。使用对称公式，该公式依赖于两两共享邻居之和，以及一个分母项，该分母项包含两个节点度中较小的一个和一个防止在没有共享邻域支持时数值膨胀的归一化项。\n3. 对于每个测试网络中一个指定的边 $(p,q)$，计算缩减因子 $r_{pq} = T_{pq} / A_{pq}$，该因子量化了TOM对该边的原始邻接值所做的修正。此因子是一个无量纲浮点数。\n\n概念目标：\n- 提供一个概念性论证，并由您实现中编码的数学原理支持，解释为什么TOM会降低那些尽管具有高两两相关性但缺乏共享邻居的边的权重，以及为什么TOM倾向于保留那些嵌入在共享邻域内的边。该论证应从共享邻居连通性、节点度和归一化三者之间的相互作用中推导出其逻辑。\n\n测试套件：\n所有测试用例均使用 $\\beta = 6$，并为每个用例中指定的 $(p,q)$ 计算 $r_{pq}$。\n\n- 测试用例1（一个包含单个离群边和不相交邻居集的玩具网络，$n=6$）：\n  $$R^{(1)} = \\begin{pmatrix}\n  1   0.1   0.1   0.1   0.6   0.1 \\\\\n  0.1   1   0.1   0.1   0.6   0.1 \\\\\n  0.1   0.1   1   0.1   0.1   0.6 \\\\\n  0.1   0.1   0.1   1   0.1   0.6 \\\\\n  0.6   0.6   0.1   0.1   1   0.9 \\\\\n  0.1   0.1   0.6   0.6   0.9   1\n  \\end{pmatrix},\\quad (p,q) = (4,5)。$$\n  该网络包含一个高相关性边 $(4,5)$，其 $R_{45} = 0.9$，而节点4和5与不相交的邻居集有中等程度的相关性，导致它们之间没有共享邻居。\n\n- 测试用例2（一个具有强共享邻居的嵌入式模块边，$n=6$）：\n  $$R^{(2)} = \\begin{pmatrix}\n  1   0.9   0.9   0.9   0.1   0.1 \\\\\n  0.9   1   0.9   0.9   0.1   0.1 \\\\\n  0.9   0.9   1   0.85   0.1   0.1 \\\\\n  0.9   0.9   0.85   1   0.1   0.1 \\\\\n  0.1   0.1   0.1   0.1   1   0.1 \\\\\n  0.1   0.1   0.1   0.1   0.1   1\n  \\end{pmatrix},\\quad (p,q) = (0,1)。$$\n  节点 $(0,1)$ 共享强邻居 $(2,3)$，形成一个类似嵌入式模块的结构。\n\n- 测试用例3（均匀网络基线，$n=5$）：\n  $$R^{(3)}_{ij} = \\begin{cases} 1  \\text{if } i=j, \\\\ 0.8  \\text{if } i \\ne j, \\end{cases} \\quad (p,q) = (0,1)。$$\n  该用例检验所有边强度相等时的行为。\n\n- 测试用例4（一个具有单条非零边的边界情况，$n=5$）：\n  $$R^{(4)}_{ij} = \\begin{cases}\n  1  \\text{if } i=j, \\\\\n  0.9  \\text{if } \\{i,j\\} = \\{3,4\\}, \\\\\n  0  \\text{otherwise},\n  \\end{cases} \\quad (p,q) = (3,4)。$$\n  该用例测试在单一边的网络环境之外没有其他网络上下文时的极限情况。\n\n最终输出规格：\n- 您的程序应生成单行输出，其中包含四个缩减因子，按测试用例1到4的顺序，以逗号分隔的列表形式并用方括号括起来，每个因子四舍五入到六位小数（例如，$[0.914999,1.000000,1.000000,1.000000]$）。不得打印任何额外文本。", "solution": "该问题是有效的，因为它在科学上植根于网络生物学原理，特别是加权基因共表达网络分析（WGCNA），其定义明确，提供了所有必要的数据，并且可以进行数学形式化。\n\n目标是形式化拓扑重叠指数（TOM）背后的数学原理，并量化其基于邻域环境对网络边进行重新加权的效果。我们将分析为什么TOM倾向于抑制缺乏共享邻域支持的边的权重，同时保留甚至放大嵌入在连接良好的模块内的边的权重。我们将使用TOM的标准公式，并将其应用于几个测试网络。\n\n假设网络是基于一组 $n$ 个基因定义的。分析主要分为三个步骤：\n\n**1. 从相关性到邻接矩阵**\n\n初始输入是一个对称的皮尔逊相关矩阵 $R \\in \\mathbb{R}^{n \\times n}$，其中 $R_{ij} = r_{ij}$ 是基因 $i$ 和基因 $j$ 表达谱之间的相关性。根据定义，$r_{ii} = 1$ 且 $-1 \\le r_{ij} \\le 1$。\n\n为了构建一个加权共表达网络，我们首先使用一个软阈值函数将相关矩阵 $R$ 转换为邻接矩阵 $A$。两个不同基因 $i$ 和 $j$ 之间的邻接值 $A_{ij} = a_{ij}$ 定义为：\n$$\na_{ij} = |r_{ij}|^\\beta \\quad \\text{for } i \\ne j\n$$\n这里，$\\beta  0$ 是软阈值幂。此操作旨在放大强相关性（其中 $|r_{ij}|$ 接近1）并抑制弱相关性（其中 $|r_{ij}|$ 接近0），从而在保持连接强度连续性的同时，强调最重要的生物学关系。按照惯例，我们将自连接设置为零，即 $a_{ii} = 0$，以避免在拓扑计算中出现自环。对于所有测试用例，我们使用指定的 $\\beta = 6$。\n\n**2. 拓扑重叠指数（TOM）**\n\n邻接矩阵 $A$ 仅捕捉了两个基因之间的直接关系。TOM的核心思想是通过整合关于这两个基因共享的网络拓扑信息来优化这一度量。基因 $i$ 和 $j$ 之间的TOM，记为 $T_{ij}$，是基于共享邻居的概念定义的。\n\n加权TOM的标准对称公式为：\n$$\nT_{ij} = \\frac{l_{ij} + a_{ij}}{\\min(k_i, k_j) + 1 - a_{ij}}\n$$\n该公式的每个组成部分都有其独特的拓扑意义：\n- $a_{ij}$：这是节点 $i$ 和 $j$ 之间的直接连接强度，如邻接矩阵中所定义。\n- $k_i = \\sum_{u} a_{iu}$：这是节点 $i$ 的总连通性，或称加权度。它表示从节点 $i$ 到网络中所有其他节点的连接强度之和。\n- $l_{ij} = \\sum_{u} a_{iu} a_{uj}$：此项量化了节点 $i$ 和 $j$ 之间共享邻域连通性的程度。它是 $i$ 和 $j$ 之间所有长度为2的加权路径之和。请注意，由于 $a_{uu}=0$，对于具有不同中间节点的路径，节点 $u$ 不能是 $i$ 或 $j$。在计算上，所有 $l_{ij}$ 值的矩阵可以通过矩阵乘积 $L = A^2 = A \\cdot A$ 得到。\n\n分子 $l_{ij} + a_{ij}$ 代表 $i$ 和 $j$ 之间的总拓扑关系，它结合了它们的直接连接（$a_{ij}$）和通过所有可能的一步共享邻居的间接连接（$l_{ij}$）。\n\n分母 $\\min(k_i, k_j) + 1 - a_{ij}$ 作为一个归一化因子。它确保 $T_{ij}$ 是有界的（通常在 $[0, 1]$ 之间）。它将共享连通性 $l_{ij}$ 与可能的最大共享连通性关联起来，后者受限于两个节点连通性中的较小者 $\\min(k_i, k_j)$。项 $1 - a_{ij}$ 是一个关键的稳定项，可防止在节点连通性较低时分母变为零。\n\n**3. 量化TOM的影响：重新加权因子**\n\n为了分析TOM如何修正初始的邻接关系，我们计算一个重新加权因子，在问题中定义为 $r_{pq} = T_{pq} / A_{pq}$，用于指定的边 $(p,q)$。该因子量化了从原始邻接值 $A_{pq}$ 到经拓扑信息修正的度量 $T_{pq}$ 的边权重变化。\n\n- 如果 $r_{pq}  1$，TOM对该边进行了降权或“惩罚”。\n- 如果 $r_{pq} = 1$，TOM保留了该边的权重。\n- 如果 $r_{pq}  1$，TOM对该边进行了增权或“放大”。\n\n该因子的行为由共享邻居和总连通性之间的相互作用决定：\n\n- **孤立的高相关性边的情况（测试用例1和4）：**\n考虑两个节点 $p$ 和 $q$，它们之间有很高的邻接值 $A_{pq}$，但很少或没有共享邻居。这意味着 $l_{pq} \\approx 0$。TOM公式变为 $T_{pq} \\approx \\frac{A_{pq}}{\\min(k_p, k_q) + 1 - A_{pq}}$。连通性 $k_p$ 和 $k_q$ 由直接边 $A_{pq}$ 加上它们与其他（大部分不相交的）邻居的连接构成。如果这些其他连接很重要，$\\min(k_p, k_q)$ 将显著大于 $A_{pq}$。这使得分母变大，导致 $T_{pq}  A_{pq}$ 和重新加权因子 $r_{pq}  1$。这就是TOM减少缺乏更广泛拓扑支持的“伪”连接影响的机制。测试用例1旨在说明这一点，其中节点4和5有很强的直接联系，但连接到网络的不同部分，导致了权重的降低。在测试用例4的极端情况下，边 $(3,4)$ 是网络的*全部*，没有任何其他拓扑环境。此时，$k_p = k_q = A_{pq}$ 且 $l_{pq}=0$，这导致 $T_{pq} = \\frac{A_{pq}}{A_{pq} + 1 - A_{pq}} = A_{pq}$。因此，$r_{pq}=1$。当没有外部拓扑可供考虑时，TOM的作用是中性的。\n\n- **嵌入模块中的边的情况（测试用例2和3）：**\n考虑两个节点 $p$ 和 $q$，它们不仅具有高邻接值 $A_{pq}$，还共享许多强连接的邻居。这是功能模块内节点的典型特征。在这种情况下，共享邻居项 $l_{pq}$ 很大。\n因子 $r_{pq} = \\frac{l_{pq} + A_{pq}}{A_{pq}(\\min(k_p, k_q) + 1 - A_{pq})} = \\frac{l_{pq}/A_{pq} + 1}{\\min(k_p, k_q) + 1 - A_{pq}}$。如果共享连接很强，$l_{pq}$ 相对于连通性可能很大，导致 $r_{pq} \\ge 1$。在测试用例2中，节点0和1是一个密集簇的一部分，导致了较大的 $l_{01}$，从而放大了边权重（$r_{01}  1$）。这表明TOM会加强那些在邻域结构中处于中心地位的连接。在测试用例3的完全均匀网络中，每条边相对于图结构都具有相同且最大的拓扑支持。公式会巧妙地简化，使得 $T_{pq} = A_{pq}$，得出 $r_{pq}=1$。\n\n总之，TOM不仅仅是一种削减机制，而是一种精密的重新加权方案。它利用邻域重叠的原理来评估一个直接连接是否在拓扑上得到证实。它惩罚拓扑上孤立的边，保留处于均匀环境中的边，并奖励位于密集局部邻域中心的边。正是这一特性使TOM成为定义和识别稳健网络模块的强大工具。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n\n    def compute_tom_reduction_factor(R: np.ndarray, p: int, q: int, beta: float) - float:\n        \"\"\"\n        Computes the adjacency matrix, TOM matrix, and the re-weighting factor for a given edge.\n\n        The function implements the following steps:\n        1.  Computes the soft-thresholded adjacency matrix A from the correlation matrix R.\n        2.  Calculates the Topological Overlap Measure (TOM) matrix T from A.\n        3.  Computes the re-weighting factor r_pq = T_pq / A_pq for the specified edge (p,q).\n\n        Args:\n            R (np.ndarray): The n x n symmetric correlation matrix.\n            p (int): The first node index of the target edge (0-based).\n            q (int): The second node index of the target edge (0-based).\n            beta (float): The soft-thresholding power.\n\n        Returns:\n            float: The re-weighting factor T_pq / A_pq.\n        \"\"\"\n        n = R.shape[0]\n\n        # Step 1: Compute the weighted adjacency matrix A\n        # A_ij = |R_ij|^beta for i != j, and A_ii = 0\n        A = np.abs(R)**beta\n        np.fill_diagonal(A, 0)\n\n        # Step 2: Compute the Weighted Topological Overlap Measure (TOM) matrix T\n        \n        # Calculate connectivity (weighted degree) for each node: k_i = sum_u a_iu\n        k = np.sum(A, axis=1)\n\n        # Calculate the shared neighbor term matrix L: L_ij = sum_u (a_iu * a_uj)\n        # This is efficiently calculated as the matrix product of A with itself.\n        L = A @ A\n\n        # Vectorized calculation of the TOM matrix T.\n        # The formula is T_ij = (L_ij + A_ij) / (min(k_i, k_j) + 1 - A_ij)\n        \n        # To vectorize min(k_i, k_j), we create an n x n matrix of these minimums.\n        k_col = k.reshape(-1, 1)\n        k_row = k.reshape(1, -1)\n        min_k_matrix = np.minimum(k_col, k_row)\n        \n        # Numerator of the TOM formula\n        numerator = L + A\n        \n        # Denominator of the TOM formula. This is guaranteed to be positive for A_ij  1.\n        denominator = min_k_matrix + 1 - A\n        \n        # Element-wise division to get the TOM matrix T\n        T = numerator / denominator\n        # By convention, the topological overlap of a node with itself is 1.\n        np.fill_diagonal(T, 1)\n\n        # Step 3: Compute the re-weighting factor r_pq = T_pq / A_pq\n        A_pq = A[p, q]\n        T_pq = T[p, q]\n\n        # The problem's test cases all have A_pq > 0.\n        if A_pq == 0:\n            if T_pq == 0:\n                return 1.0  # Convention for no change when both are zero.\n            else:\n                return np.inf # TOM found a path where no direct adjacency existed.\n        \n        factor = T_pq / A_pq\n        return factor\n\n    # Shared parameter for all test cases\n    beta = 6.0\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1: Outlier edge with disjoint neighbors\n        (\n            np.array([\n                [1.0, 0.1, 0.1, 0.1, 0.6, 0.1],\n                [0.1, 1.0, 0.1, 0.1, 0.6, 0.1],\n                [0.1, 0.1, 1.0, 0.1, 0.1, 0.6],\n                [0.1, 0.1, 0.1, 1.0, 0.1, 0.6],\n                [0.6, 0.6, 0.1, 0.1, 1.0, 0.9],\n                [0.1, 0.1, 0.6, 0.6, 0.9, 1.0]\n            ]),\n            (4, 5) # Designated edge (p,q)\n        ),\n        # Test Case 2: Embedded module edge with shared neighbors\n        (\n            np.array([\n                [1.0, 0.9, 0.9, 0.9, 0.1, 0.1],\n                [0.9, 1.0, 0.9, 0.9, 0.1, 0.1],\n                [0.9, 0.9, 1.0, 0.85, 0.1, 0.1],\n                [0.9, 0.9, 0.85, 1.0, 0.1, 0.1],\n                [0.1, 0.1, 0.1, 0.1, 1.0, 0.1],\n                [0.1, 0.1, 0.1, 0.1, 0.1, 1.0]\n            ]),\n            (0, 1)\n        ),\n        # Test Case 3: Uniform network baseline\n        (\n            np.full((5, 5), 0.8, dtype=float) + np.diag([0.2]*5),\n            (0, 1)\n        ),\n        # Test Case 4: Boundary case with a single edge\n        (\n            np.array([\n                [1.0, 0.0, 0.0, 0.0, 0.0],\n                [0.0, 1.0, 0.0, 0.0, 0.0],\n                [0.0, 0.0, 1.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0, 1.0, 0.9],\n                [0.0, 0.0, 0.0, 0.9, 1.0]\n            ]),\n            (3, 4)\n        )\n    ]\n\n    results = []\n    for R, (p, q) in test_cases:\n        # Note: In the test matrices from the problem, p and q are 1-based indices.\n        # Python uses 0-based indexing, so we adjust.\n        # Case 1: (5,6) -> (4,5)\n        # Case 2: (1,2) -> (0,1)\n        # Case 3: (1,2) -> (0,1)\n        # Case 4: (4,5) -> (3,4)\n        # The provided (p,q) tuples are already 0-based as per standard CS practice.\n        # No adjustment needed.\n        factor = compute_tom_reduction_factor(R, p, q, beta)\n        results.append(factor)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4328763"}]}