{"hands_on_practices": [{"introduction": "生物网络的构建往往依赖于多种实验数据，例如酵母双杂交、免疫共沉淀等，而这些数据源的可靠性各不相同。本练习将指导你如何应用概率论的基本原理，整合这些异构数据，为网络中的每条边赋予一个代表其“可信度”的权重[@problem_id:4330440]。通过这种方式，你将学习构建一个能够反映实验数据不确定性的加权、有向蛋白质相互作用网络，这是系统生物医学研究中的一项核心技能。", "problem": "您正在使用多种异构证据来源为四种蛋白质 $P_1$、$P_2$、$P_3$ 和 $P_4$ 构建一个概率性有向蛋白质相互作用网络。在系统生物医学中，边的置信度被解释为所报道的相互作用为真的概率，而当生物学机制意味着因果关系（例如，激酶到底物或转录因子到靶基因）时，会编码方向性。提供了以下证据代码及其基础可靠性，每一种在单独考虑时都被解释为独立的真实概率：酵母双杂交 (Y2H)：$0.35$，免疫共沉淀 (Co-IP)：$0.65$，亲和纯化质谱 (AP-MS)：$0.55$，染色质免疫沉淀测序 (ChIP-seq)：$0.75$，精选文献 (CUR)：$0.90$，磷酸化基序 (PM)：$0.30$。观察到的相互作用如下：\n- $P_1$ 和 $P_2$ 由 Y2H 和 AP-MS 报道，未声明方向性。\n- $P_1$ 在 CUR 的支持下调控 $P_3$，方向为 $P_1 \\rightarrow P_3$。\n- $P_2$ 和 $P_3$ 由 Co-IP 报道，未声明方向性。\n- $P_3$ 在 ChIP-seq 的支持下调控 $P_4$，方向为 $P_3 \\rightarrow P_4$。\n- $P_4$ 和 $P_1$ 由 AP-MS 和 Co-IP 报道，未声明方向性。\n- $P_2$ 在 PM 的支持下磷酸化 $P_4$，方向为 $P_2 \\rightarrow P_4$。\n设计一种表示方法，该方法使用基于基础概率论和图论定义的、适用于系统生物医学网络构建的有原则的方法，来编码可用的边置信度和方向性。然后，为节点顺序 $(P_1, P_2, P_3, P_4)$ 构建有向边加权鄰接矩阵 $A \\in \\mathbb{R}^{4 \\times 4}$，并清楚地说明如何表示无向边。最后，计算标量\n$$S = \\sum_{i=1}^{4}\\sum_{j=1}^{4} A_{ij}$$\n并以无单位的纯数形式提供其数值。无需四舍五入。", "solution": "该问题是有效的，因为它在科学上基于概率网络构建的原理，信息充分，具有唯一解，并且陈述客观。\n\n任务是为一个由4个蛋白质 $P_1, P_2, P_3, P_4$ 组成的网络构建一个有向边加权邻接矩阵 $A$，然后计算其所有元素的总和。构建过程需要一种有原则的方法来结合来自多个独立来源的证据，并表示有向和无向的相互作用。\n\n首先，我们定义网络构建的原则。\n$1$. **结合独立的概率性证据**：问题指出，每个证据来源都提供了一个相互作用为真的独立概率。假设一个相互作用由 $n$ 个独立的证据来源支持，其各自的概率（可靠性）为 $p_1, p_2, \\dots, p_n$。这些证据来源中至少有一个是正确的概率，等于 $1$ 减去它们全部不正确的概率。来源 $i$ 不正确的概率是 $(1-p_i)$。由于这些来源是独立的，它们全部不正确的概率是它们各自不正确概率的乘积。因此，该相互作用的组合置信度分数 $c$ 为：\n$$c = 1 - \\prod_{i=1}^{n} (1 - p_i)$$\n问题给出了每种证据代码的基础概率：$p_{Y2H} = 0.35$，$p_{CoIP} = 0.65$，$p_{APMS} = 0.55$，$p_{ChIP} = 0.75$，$p_{CUR} = 0.90$，以及 $p_{PM} = 0.30$。\n\n$2$. **在邻接矩阵中表示边**：问题要求为有序节点 $(P_1, P_2, P_3, P_4)$ 构建一个有向邻接矩阵 $A \\in \\mathbb{R}^{4 \\times 4}$。一个条目 $A_{ij}$ 将表示有向相互作用 $P_i \\rightarrow P_j$ 的置信度分数。\n- 对于一个由组合置信度为 $c$ 的证据支持的**有向相互作用** $P_i \\rightarrow P_j$，我们设置 $A_{ij} = c$。在缺乏反向相互作用 $P_j \\rightarrow P_i$ 证据的情况下，我们设置 $A_{ji} = 0$。\n- 对于一个组合置信度为 $c$ 的 $P_i$ 和 $P_j$ 之间的**无向相互作用**，证据证实了物理关联但没有证实因果关系。在有向矩阵中，一种标准且有原则的图论表示方法是将置信度分数分配给两个方向的边，表示该链接对称存在。因此，我们设置 $A_{ij} = A_{ji} = c$。\n- 如果没有 $P_i$ 和 $P_j$ 之间相互作用的证据，则 $A_{ij} = A_{ji} = 0$。\n- 未提及自环，因此我们假设对所有 $i$ 都有 $A_{ii} = 0$。\n\n我们现在为每个报道的相互作用计算置信度分数并填充矩阵 $A$。\n\n- **$P_1$ 和 $P_2$ 之间的相互作用**：由 Y2H ($p=0.35$) 和 AP-MS ($p=0.55$) 支持。未声明方向性。\n组合置信度为 $c_{12} = 1 - (1 - 0.35)(1 - 0.55) = 1 - (0.65)(0.45) = 1 - 0.2925 = 0.7075$。\n由于相互作用是无向的，我们设置 $A_{12} = 0.7075$ 和 $A_{21} = 0.7075$。\n\n- **$P_1$ 和 $P_3$ 之间的相互作用**：由 CUR ($p=0.90$) 支持，方向为 $P_1 \\rightarrow P_3$。\n置信度为 $c_{13} = 0.90$。\n由于相互作用是有向的，我们设置 $A_{13} = 0.90$。没有反向相互作用的证据，所以 $A_{31} = 0$。\n\n- **$P_2$ 和 $P_3$ 之间的相互作用**：由 Co-IP ($p=0.65$) 支持。未声明方向性。\n置信度为 $c_{23} = 0.65$。\n由于相互作用是无向的，我们设置 $A_{23} = 0.65$ 和 $A_{32} = 0.65$。\n\n- **$P_3$ 和 $P_4$ 之间的相互作用**：由 ChIP-seq ($p=0.75$) 支持，方向为 $P_3 \\rightarrow P_4$。\n置信度为 $c_{34} = 0.75$。\n由于相互作用是有向的，我们设置 $A_{34} = 0.75$。没有反向相互作用的证据，所以 $A_{43} = 0$。\n\n- **$P_4$ 和 $P_1$ 之间的相互作用**：由 AP-MS ($p=0.55$) 和 Co-IP ($p=0.65$) 支持。未声明方向性。\n组合置信度为 $c_{41} = 1 - (1 - 0.55)(1 - 0.65) = 1 - (0.45)(0.35) = 1 - 0.1575 = 0.8425$。\n由于相互作用是无向的，我们设定 $A_{41} = 0.8425$ 和 $A_{14} = 0.8425$。\n\n- **$P_2$ 和 $P_4$ 之间的相互作用**：由 PM ($p=0.30$) 支持，方向为 $P_2 \\rightarrow P_4$。\n置信度为 $c_{24} = 0.30$。\n由于相互作用是有向的，我们设置 $A_{24} = 0.30$。没有反向相互作用的证据，所以 $A_{42} = 0$。\n\n综合这些结果，有向邻接矩阵 $A$ 为：\n$$A = \\begin{pmatrix} A_{11} & A_{12} & A_{13} & A_{14} \\\\ A_{21} & A_{22} & A_{23} & A_{24} \\\\ A_{31} & A_{32} & A_{33} & A_{34} \\\\ A_{41} & A_{42} & A_{43} & A_{44} \\end{pmatrix} = \\begin{pmatrix} 0 & 0.7075 & 0.90 & 0.8425 \\\\ 0.7075 & 0 & 0.65 & 0.30 \\\\ 0 & 0.65 & 0 & 0.75 \\\\ 0.8425 & 0 & 0 & 0 \\end{pmatrix}$$\n\n最后，我们计算标量 $S$，即 $A$ 中所有元素的总和。\n$$S = \\sum_{i=1}^{4}\\sum_{j=1}^{4} A_{ij}$$\n我们将上面计算出的所有非零条目相加：\n$S = A_{12} + A_{13} + A_{14} + A_{21} + A_{23} + A_{24} + A_{32} + A_{34} + A_{41}$\n$S = 0.7075 + 0.90 + 0.8425 + 0.7075 + 0.65 + 0.30 + 0.65 + 0.75 + 0.8425$\n分组各项：\n$S = (0.7075 + 0.7075) + (0.8425 + 0.8425) + (0.65 + 0.65) + 0.90 + 0.30 + 0.75$\n$S = (2 \\times 0.7075) + (2 \\times 0.8425) + (2 \\times 0.65) + 0.90 + 0.30 + 0.75$\n$S = 1.415 + 1.685 + 1.30 + 0.90 + 0.30 + 0.75$\n$S = 3.100 + 1.30 + 0.90 + 0.30 + 0.75$\n$S = 4.40 + 0.90 + 0.30 + 0.75$\n$S = 5.30 + 0.30 + 0.75$\n$S = 5.60 + 0.75$\n$S = 6.35$\n\n标量 $S$ 的数值为 $6.35$。", "answer": "$$\\boxed{6.35}$$", "id": "4330440"}, {"introduction": "构建好的生物网络如同一张复杂的地图，充满了节点和连接。我们的下一个挑战是如何在这张地图上定位出最关键的“交通枢纽”。本练习将带你深入网络拓扑分析的核心——中心性计算[@problem_id:4330471]。你将亲手计算节点的度中心性、介数中心性和特征向量中心性，从不同的维度（如连接数、桥梁作用和网络影响力）来量化每个节点的重要性，这是识别信号通路中关键调控因子和潜在药物靶点的基础。", "problem": "你的任务是使用图论中心性度量来构建和分析有向生物信号网络。考虑一个定义在节点集 $V=\\{0,1,\\ldots,n-1\\}$ 上的有向无权图，由邻接矩阵 $A \\in \\{0,1\\}^{n \\times n}$ 表示，其中 $A_{ij}=1$ 表示一个从节点 $i$ 到节点 $j$ 的有向相互作用。在系统生物医学中，这类图通常是通过将经过整理的蛋白质-蛋白质相互作用（PPI）数据和激酶-底物关系整合到一个统一的信号网络中来构建的。对于每个网络实例，都会提供一个节点集 $H \\subseteq V$，这些节点有文献支持，被称为“已知枢纽”。你的任务是计算中心性度量，并量化中心节点（按每种度量排名）与已知枢纽的对应程度。所有计算必须按照下文定义的纯数学术语进行。\n\n基本原理、定义和要求：\n- 一个有向图 $G=(V,E)$ 由 $V=\\{0,\\ldots,n-1\\}$ 和 $E=\\{(i,j)\\mid A_{ij}=1\\}$ 定义。有向路径是遵循边方向性的节点序列。图论距离 $d(s,t)$ 是从节点 $s$ 到节点 $t$ 的最短有向路径的长度（边的数量），如果路径存在的话；否则，距离被认为是无限的。\n- 度中心性：对于每个节点 $v \\in V$，将其度定义为其入度和出度之和，即考虑方向的关联边数。你必须计算这个度值用于排名。排名不需要归一化。\n- 介数中心性：对于每个节点 $v \\in V$，其分数定义为在所有有序对 $(s,t)$（其中 $s \\neq v$，$t \\neq v$ 且 $s \\neq t$）之间的最短有向路径中，经过 $v$ 的路径所占的比例。使用无权度量下的最短有向路径。使用有向图的标准归一化方法，即除以 $(n-1)(n-2)$。\n- 特征向量中心性：定义一个非负向量 $x \\in \\mathbb{R}_{\\ge 0}^n$，其条目量化了节点的影响力，使得节点 $i$ 的中心性与指向它的节点的中心性之和成正比。具体来说，$x$ 是 $A^{\\top}$ 的主特征向量（对应于最大特征值），通过从一个严格正向量开始的幂迭代法计算，并在每次迭代中进行归一化。当连续迭代向量之差的 $\\ell_2$-范数低于一个容差或达到最大迭代次数时终止。如果在任何迭代中 $A^{\\top}x$ 成为零向量，则返回零向量作为 $x$。\n- 排名与平局处理：对于每种中心性度量 $C \\in \\{\\text{degree}, \\text{betweenness}, \\text{eigenvector}\\}$，按中心性得分降序对节点进行排名。如果两个节点的得分完全相等，则优先选择节点索引较小的来打破平局。\n- 枢纽重叠量化：给定一个包含 $|H|=k$ 个已知枢纽的集合 $H$，将 top-k 集合 $T_C$ 定义为在上述排名规则下，按度量 $C$ 计算出的中心性最高的 $k$ 个节点。使用 Jaccard 指数 $J(T_C,H) = \\frac{|T_C \\cap H|}{|T_C \\cup H|}$ 来量化对应关系。该值为 $[0,1]$ 区间内的实数，必须以浮点值的形式报告。\n- 最终输出必须是四舍五入到三位小数的浮点数。\n\n测试套件：\n为以下三个有向网络中的每一个计算三个 Jaccard 指数 $[J_{\\text{deg}}, J_{\\text{bet}}, J_{\\text{eig}}]$。对于每个网络，使用给定的邻接矩阵 $A$ 和已知枢纽集 $H$。所有显示的条目都在 $\\{0,1\\}$ 中，并且节点索引是基于 $0$ 的。\n\n- 测试用例 1：\n    - 大小 $n=7$。\n    - 邻接矩阵 $A_1$ (第 $i$ 行，第 $j$ 列给出 $A_{ij}$):\n      $\n      \\begin{bmatrix}\n      0  1  1  0  0  0  0 \\\\\n      1  0  1  1  1  0  0 \\\\\n      0  0  0  1  1  1  0 \\\\\n      0  0  0  0  1  1  0 \\\\\n      0  0  0  0  0  1  1 \\\\\n      0  0  0  0  0  0  1 \\\\\n      0  0  0  0  0  0  0\n      \\end{bmatrix}\n      $\n    - 已知枢纽 $H_1=\\{1,4\\}$。\n\n- 测试用例 2：\n    - 大小 $n=8$。\n    - 邻接矩阵 $A_2$:\n      $\n      \\begin{bmatrix}\n      0  1  1  0  0  0  0  0 \\\\\n      0  0  1  0  0  0  0  0 \\\\\n      0  1  0  1  0  0  0  0 \\\\\n      0  0  0  0  0  0  0  0 \\\\\n      0  0  0  0  0  1  0  1 \\\\\n      0  0  0  0  0  0  1  1 \\\\\n      0  0  0  0  1  0  0  0 \\\\\n      0  0  0  0  0  0  0  0\n      \\end{bmatrix}\n      $\n    - 已知枢纽 $H_2=\\{2,5\\}$。\n\n- 测试用例 3：\n    - 大小 $n=9$。\n    - 邻接矩阵 $A_3$:\n      $\n      \\begin{bmatrix}\n      0  1  1  0  0  0  0  0  0 \\\\\n      0  0  0  1  0  0  0  0  0 \\\\\n      0  0  0  1  0  0  0  0  0 \\\\\n      0  0  0  0  1  1  0  0  0 \\\\\n      0  0  0  0  0  0  1  0  0 \\\\\n      0  0  0  0  0  0  1  0  0 \\\\\n      0  0  0  0  0  0  0  1  1 \\\\\n      0  0  0  0  0  0  0  0  0 \\\\\n      0  0  0  0  0  0  0  0  0\n      \\end{bmatrix}\n      $\n    - 已知枢纽 $H_3=\\{3,6\\}$。\n\n编程要求：\n- 根据上述定义实现三种中心性度量的计算，使用有向图的标准最短路径概念，并通过对 $A^{\\top}$ 进行幂迭代来计算主特征向量。\n- 对于每个测试用例，令 $k=|H|$，并根据每种中心性度量和指定的平局处理规则计算出 top-k 节点集 $T_{\\text{deg}}$、$T_{\\text{bet}}$、$T_{\\text{eig}}$。计算 $T_C$ 和 $H$ 之间的三个 Jaccard 指数。\n- 你的程序必须生成单行输出，其中包含结果，格式为一个包含三个列表（每个测试用例一个）的逗号分隔列表。每个内部列表包含三个四舍五入到三位小数的浮点 Jaccard 指数，并用方括号括起来。例如，输出将类似于 $[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3]]$，其中每个 $a_i,b_i,c_i$ 都是四舍五入到三位小数的浮点数。", "solution": "问题陈述已经过严格审查并被确定为有效。它在科学上植根于网络理论及其在系统生物学中的应用，在数学上是适定的，具有明确定义的术语和目标，并为所需计算提供了一套完整且一致的数据。该问题构成了实现和解释标准图中心性度量的一次严谨练习。因此，我们可以着手提供完整解答。\n\n解决方案涉及为每个给定网络计算三种不同的中心性度量，然后与一组给定的已知枢纽节点进行定量比较。下面将详细介绍每个步骤，这些步骤基于图论和数值线性代数的既定原理。\n\n问题的核心是一个包含 $n$ 个节点的有向图 $G=(V,E)$，由邻接矩阵 $A$ 表示。条目 $A_{ij}=1$ 表示从节点 $i$ 到节点 $j$ 的一条有向边。对于每次分析，我们都给定一个包含 $k = |H|$ 个已知枢纽节点的集合 $H$。我们的目标是，对于每种中心性度量 $C$，计算出 top-k 节点 $T_C$，然后计算 Jaccard 指数 $J(T_C, H)$。\n\n### 1. 度中心性\n\n**原理**：度中心性是衡量节点重要性的局部度量，通过其连接数来量化。对于有向图，这是一个节点的入度和出度之和。节点 $v$ 的入度是指向它的边的数量，而出度是从它发出的边的数量。\n\n**计算**：给定邻接矩阵 $A$，节点 $j$ 的入度是第 $j$ 列的和，即 $\\sum_{i=0}^{n-1} A_{ij}$。节点 $i$ 的出度是第 $i$ 行的和，即 $\\sum_{j=0}^{n-1} A_{ij}$。因此，节点 $v$ 的总度数由下式给出：\n$$\n\\text{deg}(v) = \\sum_{i=0}^{n-1} A_{iv} + \\sum_{j=0}^{n-1} A_{vj}\n$$\n这些值需要为图中的每个节点计算。\n\n### 2. 介数中心性\n\n**原理**：介数中心性是一种全局度量，量化了一个节点位于其他节点之间最短路径上的程度。具有高介数中心性的节点在网络中充当通信或流动的关键“桥梁”或“瓶颈”。对于一个节点 $v$，其介数中心性 $C_B(v)$ 定义为，在所有其他节点对 $(s, t)$ 之间的最短路径中，经过 $v$ 的路径所占比例的总和。\n\n**计算**：有向图的公式为：\n$$\nC_B(v) = \\sum_{s,t \\in V, s \\neq v \\neq t, s \\neq t} \\frac{\\sigma_{st}(v)}{\\sigma_{st}}\n$$\n其中 $\\sigma_{st}$ 是从 $s$ 到 $t$ 的最短有向路径的总数，而 $\\sigma_{st}(v)$ 是其中将 $v$ 作为中间节点的路径数量。此计算可使用 Brandes' 算法高效执行。该算法对每个源节点 $s \\in V$ 分两个主要阶段进行：\n1.  **最短路径计数**：从 $s$ 开始的广度优先搜索 (BFS) 用于计算到所有其他节点的最短路径距离 $d(s, \\cdot)$ 和最短路径数量 $\\sigma_{s\\cdot}$。\n2.  **依赖累积**：按与 $s$ 距离递减的顺序处理节点。对于每个节点 $w$，计算一个依赖分数 $\\delta_s(w)$，它表示距离 $s$ 更远的节点对经过 $w$ 的路径的依赖总和。节点 $v$ 对其后继节点 $w$ 的依赖是从 $s$到 $w$ 的最短路径中经过 $v$ 的部分。每个节点 $v$ 的中心性得分会增加其来自源 $s$ 的总依赖。\n\n在遍历所有可能的源节点 $s$ 之后，通过除以 $(n-1)(n-2)$（即排除端点的不同节点有序对的总数）来对所得分数进行归一化。\n\n### 3. 特征向量中心性\n\n**原理**：特征向量中心性是一种递归的影响力度量。一个节点的中心性不仅取决于其连接数（如度中心性），还取决于连接到它的节点的中心性。如果一个节点被其他重要节点指向，那么它就是重要的。\n\n**计算**：这个原理在数学上被形式化为一个特征向量问题。设 $x_i$ 为节点 $i$ 的中心性。定义指出 $x_i$ 与指向它的节点 $j$ 的中心性之和成正比。这可以写成：\n$$\nx_i = \\frac{1}{\\lambda} \\sum_{j: (j,i) \\in E} x_j = \\frac{1}{\\lambda} \\sum_{j=0}^{n-1} A_{ji} x_j\n$$\n以矩阵形式表示，即 $\\lambda x = A^\\top x$。因此，中心性向量 $x$ 是邻接矩阵转置 $A^\\top$ 的一个特征向量。根据 Perron-Frobenius 定理及其对非负矩阵的扩展，存在一个最大的非负特征值 $\\lambda_{\\text{max}}$，其对应一个非负特征向量。这就是我们所寻求的主特征向量。\n\n它通过幂迭代法进行数值计算。从一个严格正向量 $x^{(0)}$（例如，全为 1 的向量）开始，我们迭代计算：\n$$\nx^{(k+1)} = \\frac{A^\\top x^{(k)}}{\\|A^\\top x^{(k)}\\|_2}\n$$\n重复此过程，直到连续向量之间的变化 $\\|x^{(k+1)} - x^{(k)}\\|_2$ 小于指定的容差，或达到最大迭代次数。得到的向量 $x$ 包含特征向量中心性得分。对于有向无环图（DAG），如测试用例所示，$A^\\top$ 的最大特征值为 $0$，幂迭代会正确地收敛到相应的特征向量。在这种情况下，中心性在网络中“流动”，并在处于下游的节点处累积。\n\n### 4. 排名与枢纽重叠量化\n\n**排名**：对于三种中心性度量中的每一种，所有 $n$ 个节点都按其中心性得分降序排列。根据问题规范，得分的任何平局都通过优先选择索引较小的节点来解决。\n\n**Jaccard 指数**：为量化排名靠前的节点集与已知枢纽集之间的重叠，使用 Jaccard 指数。给定一个包含 $k$ 个已知枢纽的集合 $H$，我们根据中心性 C 确定 top-k 节点的集合 $T_C$。Jaccard 指数则为：\n$$\nJ(T_C, H) = \\frac{|T_C \\cap H|}{|T_C \\cup H|} = \\frac{|T_C \\cap H|}{|T_C| + |H| - |T_C \\cap H|}\n$$\n该值范围从 $0$（无重叠）到 $1$（完全重叠），提供了一个标准化的对应关系度量。最终结果以四舍五入到三位小数的形式呈现。", "answer": "```python\nimport numpy as np\nfrom collections import deque\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis on all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        (\n            [\n                [0, 1, 1, 0, 0, 0, 0],\n                [1, 0, 1, 1, 1, 0, 0],\n                [0, 0, 0, 1, 1, 1, 0],\n                [0, 0, 0, 0, 1, 1, 0],\n                [0, 0, 0, 0, 0, 1, 1],\n                [0, 0, 0, 0, 0, 0, 1],\n                [0, 0, 0, 0, 0, 0, 0]\n            ],\n            {1, 4}\n        ),\n        (\n            [\n                [0, 1, 1, 0, 0, 0, 0, 0],\n                [0, 0, 1, 0, 0, 0, 0, 0],\n                [0, 1, 0, 1, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0, 1, 0, 1],\n                [0, 0, 0, 0, 0, 0, 1, 1],\n                [0, 0, 0, 0, 1, 0, 0, 0],\n                [0, 0, 0, 0, 0, 0, 0, 0]\n            ],\n            {2, 5}\n        ),\n        (\n            [\n                [0, 1, 1, 0, 0, 0, 0, 0, 0],\n                [0, 0, 0, 1, 0, 0, 0, 0, 0],\n                [0, 0, 0, 1, 0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 1, 1, 0, 0, 0],\n                [0, 0, 0, 0, 0, 0, 1, 0, 0],\n                [0, 0, 0, 0, 0, 0, 1, 0, 0],\n                [0, 0, 0, 0, 0, 0, 0, 1, 1],\n                [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0, 0, 0, 0, 0]\n            ],\n            {3, 6}\n        )\n    ]\n\n    final_results = []\n    for A_list, H_set in test_cases:\n        A = np.array(A_list, dtype=np.int32)\n        n = A.shape[0]\n        k = len(H_set)\n\n        # Calculate centralities\n        deg_cen = calculate_degree_centrality(A, n)\n        bet_cen = calculate_betweenness_centrality(A, n)\n        eig_cen = calculate_eigenvector_centrality(A, n)\n\n        # Get rankings\n        deg_rank = rank_nodes(deg_cen)\n        bet_rank = rank_nodes(bet_cen)\n        eig_rank = rank_nodes(eig_cen)\n\n        # Get top-k sets\n        T_deg = set(deg_rank[:k])\n        T_bet = set(bet_rank[:k])\n        T_eig = set(eig_rank[:k])\n\n        # Calculate Jaccard indices\n        j_deg = jaccard_index(T_deg, H_set)\n        j_bet = jaccard_index(T_bet, H_set)\n        j_eig = jaccard_index(T_eig, H_set)\n\n        final_results.append([j_deg, j_bet, j_eig])\n\n    # Format output as specified\n    outer_list_str = []\n    for res_list in final_results:\n        inner_list_str = \",\".join([f\"{x:.3f}\" for x in res_list])\n        outer_list_str.append(f\"[{inner_list_str}]\")\n    print(f\"[{','.join(outer_list_str)}]\")\n\ndef calculate_degree_centrality(A, n):\n    \"\"\"Computes degree centrality (in-degree + out-degree) for each node.\"\"\"\n    in_degree = np.sum(A, axis=0)\n    out_degree = np.sum(A, axis=1)\n    return in_degree + out_degree\n\ndef calculate_betweenness_centrality(A, n):\n    \"\"\"Computes betweenness centrality using Brandes' algorithm for directed graphs.\"\"\"\n    betweenness = np.zeros(n, dtype=np.float64)\n    nodes = list(range(n))\n    \n    for s in nodes:\n        # Single-source shortest path using BFS\n        S = []  # Stack of nodes in order of non-increasing distance from s\n        P = [[] for _ in range(n)]  # List of predecessors on shortest paths from s\n        sigma = np.zeros(n, dtype=np.float64)\n        sigma[s] = 1.0\n        d = np.full(n, -1, dtype=np.int32)\n        d[s] = 0\n        \n        Q = deque([s])\n        \n        while Q:\n            v = Q.popleft()\n            S.append(v)\n            \n            # Find neighbors of v\n            neighbors = np.where(A[v, :] == 1)[0]\n            for w in neighbors:\n                # Path discovery\n                if d[w]  0:\n                    Q.append(w)\n                    d[w] = d[v] + 1\n                \n                # Path counting\n                if d[w] == d[v] + 1:\n                    sigma[w] += sigma[v]\n                    P[w].append(v)\n                    \n        # Dependency accumulation\n        delta = np.zeros(n, dtype=np.float64)\n        while S:\n            w = S.pop()\n            for v in P[w]:\n                if sigma[w] != 0:\n                    delta[v] += (sigma[v] / sigma[w]) * (1.0 + delta[w])\n            if w != s:\n                betweenness[w] += delta[w]\n\n    # Normalization\n    if n  2:\n        norm = (n - 1) * (n - 2)\n        betweenness /= norm\n    \n    return betweenness\n\ndef calculate_eigenvector_centrality(A, n, tol=1e-9, max_iters=1000):\n    \"\"\"Computes eigenvector centrality using power iteration on A.T.\"\"\"\n    At = A.T.astype(np.float64)\n    x = np.ones(n, dtype=np.float64)\n    \n    for _ in range(max_iters):\n        x_prev = x\n        x = At @ x\n        \n        norm_x = np.linalg.norm(x)\n        if norm_x == 0:\n            return np.zeros(n, dtype=np.float64)\n            \n        x = x / norm_x\n        \n        if np.linalg.norm(x - x_prev)  tol:\n            break\n            \n    return x\n\ndef rank_nodes(centrality_scores):\n    \"\"\"Ranks nodes by centrality, breaking ties with smaller node index.\"\"\"\n    nodes = list(range(len(centrality_scores)))\n    # Pair scores with node indices\n    scored_nodes = list(zip(centrality_scores, nodes))\n    \n    # Sort by score descending (-score), then by node index ascending\n    scored_nodes.sort(key=lambda item: (-item[0], item[1]))\n    \n    # Return just the ranked node indices\n    return [node for score, node in scored_nodes]\n\ndef jaccard_index(set1, set2):\n    \"\"\"Computes the Jaccard index between two sets.\"\"\"\n    intersection_size = float(len(set1.intersection(set2)))\n    union_size = float(len(set1.union(set2)))\n    \n    if union_size == 0:\n        # This case is not expected in this problem as k  0\n        return 1.0\n    \n    return intersection_size / union_size\n\nif __name__ == '__main__':\n    solve()\n\n```", "id": "4330471"}, {"introduction": "系统生物医学的一个核心目标是理解不同生理或病理状态下（例如健康与疾病）生物网络的差异。本项高级练习将引导你进入比较网络组学的领域，学习网络比对这一强大技术[@problem_id:4330436]。你将实践一个综合节点自身属性（如活性水平）和局部拓扑结构（如连接模式）的相似性度量方法，以实现两个网络的最佳匹配。通过识别网络中“保守”和“重连”的子结构，这项练习能够帮助你洞察疾病发生或药物响应背后的分子机制，是发现生物标志物和设计治疗策略的关键一步。", "problem": "您的任务是为两种癌症类型构建的有向、带符号信号网络，形式化并实现一种基于原则的比对算法。信号网络表示为一个有向图 $G = (V,E)$，其节点集 $V$ 索引了分子实体（例如，蛋白质或基因），其边集为 $E \\subseteq V \\times V \\times \\{-1,+1\\}$，其中一条边 $(u,v,s)$ 表示从节点 $u$ 到节点 $v$ 的一个有向影响，其符号 $s \\in \\{-1,+1\\}$ 代表抑制（-1）或激活（+1）。每个节点 $i \\in V$ 都有一个相关的实值活动属性 $a_i \\in \\mathbb{R}$（例如，从转录组学或磷酸化蛋白质组学中得到的对数倍数变化）。您的目标是从第一性原理出发，设计两个此类网络之间的一种可计算的比对，该比对旨在最大化网络间在节点活动和局部连接结构上的一致性，然后在导出的节点比对下量化保守和重连的子结构。\n\n基本原理：\n- 生物信号传导可以抽象为有向、带符号的相互作用。经充分验证，带符号有向图能够捕捉激活和抑制关系，而局部单跳拓扑描述符（如带符号入度和出度）能够总结直接的调控环境。\n- 活动或表达差异等属性是实值的，可以使用径向基函数核进行比较，这是一种在统计学和机器学习中被广泛接受的、基于距离相似性的相似性度量。\n- 两个节点集之间的一一比对可以表示为一个最大化总相似性得分的二分匹配，该问题可以通过线性分配被精确求解。\n\n形式化定义：\n- 设 $G_1 = (V_1,E_1)$ 和 $G_2 = (V_2,E_2)$ 是两个带节点属性 $\\{a^{(1)}_u\\}_{u \\in V_1}$ 和 $\\{a^{(2)}_v\\}_{v \\in V_2}$ 的有向、带符号图。对于一个节点 $x$，将其带符号度分布向量 $d(x) \\in \\mathbb{N}^4$ 定义为 $d(x) = (d^{\\text{out}+}(x), d^{\\text{out}-}(x), d^{\\text{in}+}(x), d^{\\text{in}-}(x))$，其中 $d^{\\text{out}+}(x)$ 计算传出激活边的数量，$d^{\\text{out}-}(x)$ 计算传出抑制边的数量，传入边同理。\n- 使用带宽为 $\\sigma  0$ 的高斯径向基函数核定义节点 $u \\in V_1$ 和 $v \\in V_2$ 之间的属性相似性：\n$$\ns_{\\text{attr}}(u,v) = \\exp\\left(-\\frac{\\left(a^{(1)}_u - a^{(2)}_v\\right)^2}{2\\sigma^2}\\right).\n$$\n- 使用其带符号度分布上的归一化曼哈顿相似性来定义节点 $u \\in V_1$ 和 $v \\in V_2$ 之间的局部拓扑相似性：\n$$\ns_{\\text{topo}}(u,v) = 1 - \\frac{\\lVert d(u) - d(v) \\rVert_1}{\\lVert d(u) \\rVert_1 + \\lVert d(v) \\rVert_1 + \\varepsilon},\n$$\n其中使用一个很小的 $\\varepsilon = 10^{-9}$ 来避免当两个度分布都为零时出现除以零的情况。\n- 使用权衡参数 $\\alpha \\in [0,1]$ 将这些组合成一个节点到节点的相似性：\n$$\ns(u,v) = \\alpha \\, s_{\\text{attr}}(u,v) + (1-\\alpha)\\, s_{\\text{topo}}(u,v).\n$$\n- 计算一个一对一比对 $f : V_1' \\to V_2'$，其中 $V_1' \\subseteq V_1$，$V_2' \\subseteq V_2$，且 $\\lvert V_1' \\rvert = \\lvert V_2' \\rvert = \\min(\\lvert V_1 \\rvert,\\lvert V_2 \\rvert)$，该比对最大化总相似性 $\\sum_{u \\in V_1'} s(u,f(u))$。这可以作为一个线性分配问题被精确求解。\n- 在导出的映射 $f$ 下，如果 $u,u'$ 均被比对且存在 $(f(u), f(u'), s_2) \\in E_2$ 满足 $s_2 = s_1$，则定义有向边 $(u,u',s_1) \\in E_1$ 为保守的。如果 $u,u'$ 均被比对，并且要么 $(f(u), f(u')) \\notin \\{(x,y) : \\exists s \\in \\{-1,+1\\}, (x,y,s) \\in E_2\\}$（缺失），要么存在边但 $s_2 \\neq s_1$（符号翻转），则定义该边为重连的。忽略 $E_1$ 中端点未同时被比对的边。\n- 将比对分数定义为已比对节点对的平均相似度：\n$$\nS_{\\text{align}} = \\frac{1}{\\lvert V_1' \\rvert} \\sum_{u \\in V_1'} s(u,f(u)).\n$$\n\n实现一个程序，对于以下每个测试用例，计算三个输出：保守边的数量（一个整数）、重连边的数量（一个整数）以及比对分数 $S_{\\text{align}}$（一个四舍五入到6位小数的浮点数）。您的程序必须将所有用例的结果聚合到一行中，格式为一个用方括号括起来的逗号分隔列表，并按 $[\\text{cons}_1,\\text{rew}_1,S_{\\text{align},1},\\text{cons}_2,\\text{rew}_2,S_{\\text{align},2},\\text{cons}_3,\\text{rew}_3,S_{\\text{align},3}]$ 的顺序展平。\n\n测试套件：\n\n- 用例1：\n    - $G_1$：$V_1 = \\{0,1,2,3,4,5\\}$，$E_1$ 包含带符号的边\n      $(0,1,+1)$, $(1,2,+1)$, $(2,3,-1)$, $(0,4,+1)$, $(4,5,+1)$, $(1,4,-1)$, $(3,5,+1)$。\n      属性 $a^{(1)} = [1.2, 0.9, 0.5, -0.2, 0.8, 0.3]$。\n    - $G_2$：$V_2 = \\{0,1,2,3,4,5\\}$，$E_2$ 包含带符号的边\n      $(0,1,+1)$, $(1,2,+1)$, $(2,3,+1)$, $(0,4,+1)$, $(4,5,+1)$, $(3,5,+1)$, $(0,3,+1)$。\n      属性 $a^{(2)} = [1.1, 0.85, 0.6, -0.1, 0.75, 0.35]$。\n    - 参数：$\\alpha = 0.6$, $\\sigma = 0.5$。\n- 用例2：\n    - $G_1$：$V_1 = \\{0,1,2,3\\}$，$E_1$ 包含 $(0,1,+1)$, $(1,2,+1)$, $(2,3,-1)$。\n      属性 $a^{(1)} = [0.9, 0.7, 0.6, 0.4]$。\n    - $G_2$：$V_2 = \\{0,1,2,3\\}$，$E_2$ 包含 $(0,2,-1)$, $(2,1,-1)$, $(3,0,+1)$。\n      属性 $a^{(2)} = [-0.9, -0.7, -0.6, -0.4]$。\n    - 参数：$\\alpha = 0.8$, $\\sigma = 0.2$。\n- 用例3：\n    - $G_1$：$V_1 = \\{0,1,2,3,4\\}$，$E_1$ 包含 $(0,1,+1)$, $(1,2,-1)$, $(2,3,+1)$, $(3,4,-1)$, $(0,2,+1)$。\n      属性 $a^{(1)} = [0.2, 0.1, 0.05, -0.1, -0.2]$。\n    - $G_2$：$V_2 = \\{0,1,2\\}$，$E_2$ 包含 $(0,1,+1)$, $(1,2,-1)$, $(0,2,+1)$。\n      属性 $a^{(2)} = [0.18, 0.12, 0.06]$。\n    - 参数：$\\alpha = 0.5$, $\\sigma = 0.1$。\n\n角度单位不适用。物理单位不适用。百分比不适用。最终输出格式必须是一个单行字符串，表示如上所述的扁平化Python风格列表，每个用例的三个值按指定顺序排列。每个用例中的浮点数 $S_{\\text{align}}$ 必须使用实数的标准四舍五入规则精确到6位小数。", "solution": "用户提供了一个有效的问题陈述。任务是为两个有向、带符号的信号网络 $G_1 = (V_1, E_1)$ 和 $G_2 = (V_2, E_2)$ 形式化并实现一种比对算法。该比对旨在最大化节点级属性和局部网络拓扑两方面的相似性。随后，在计算出的比对下，我们必须量化保守和重连边的数量，并计算一个总体比对分数。该问题具有科学依据，定义明确，并且为求解提供了所有必要的组成部分。\n\n问题的核心是在 $V_1$ 和 $V_2$ 的节点子集之间找到一个最优的一一映射 $f$。这被构建为一个最大权二分匹配问题，是组合优化中的一个经典问题，可以使用线性分配问题（LAP）框架精确求解。匹配一个节点 $u \\in V_1$ 和一个节点 $v \\in V_2$ 的“权重”由一个复合相似性得分 $s(u,v)$ 给出。\n\n整体算法流程如下：\n$1$. 对于图 $G_1$ 和 $G_2$ 中的每个节点，计算其带符号度分布。\n$2$. 构建一个大小为 $|V_1| \\times |V_2|$ 的相似性矩阵 $S$，其中每个条目 $S_{uv}$ 是节点 $u \\in V_1$ 和节点 $v \\in V_2$ 之间的复合相似性得分 $s(u,v)$。\n$3$. 对矩阵 $S$ 求解线性分配问题，以找到最大化已比对节点对总相似性的节点到节点映射 $f$。\n$4$. 使用确定的映射 $f$，分析 $G_1$ 的边，将其分类为保守或重连。\n$5$. 计算最终比对分数 $S_{\\text{align}}$，即所有匹配对的平均相似性。\n\n我们现在从第一性原理详细说明每个步骤。\n\n**1. 节点特征提取：带符号度分布**\n为了捕捉节点 $x$ 的局部拓扑环境，我们使用其带符号度分布，这是一种简洁且信息丰富的描述符。它被定义为一个4维向量 $d(x) \\in \\mathbb{N}^4$：\n$$d(x) = (d^{\\text{out}+}(x), d^{\\text{out}-}(x), d^{\\text{in}+}(x), d^{\\text{in}-}(x))$$\n其中各分量分别计算传出激活（$s=+1$）、传出抑制（$s=-1$）、传入激活（$s=+1$）和传入抑制（$s=-1$）边的数量。通过遍历各自的边集 $E_1$ 和 $E_2$，为 $V_1$ 和 $V_2$ 中的每个节点计算这些向量。\n\n**2. 节点到节点相似性得分**\n节点 $u \\in V_1$ 和节点 $v \\in V_2$ 之间的相似性 $s(u,v)$ 是两个分量的加权和：属性相似性和拓扑相似性。\n$$s(u,v) = \\alpha \\, s_{\\text{attr}}(u,v) + (1-\\alpha)\\, s_{\\text{topo}}(u,v)$$\n参数 $\\alpha \\in [0,1]$ 平衡了这两个方面的贡献。\n\n**属性相似性, $s_{\\text{attr}}$**：节点属性 $a_u^{(1)}$ 和 $a_v^{(2)}$ 是实值活动。衡量其相似性的一个自然方法是通过一个将距离映射到相似性的核函数。问题指定使用高斯径向基函数（RBF）核，这是机器学习中的一个标准选择，因其具有理想的属性（例如，局部性、平滑性）。\n$$s_{\\text{attr}}(u,v) = \\exp\\left(-\\frac{\\left(a^{(1)}_u - a^{(2)}_v\\right)^2}{2\\sigma^2}\\right)$$\n在这里，$\\sigma  0$ 是带宽参数，控制相似性得分对属性值差异的敏感度。较小的 $\\sigma$ 会导致更严格的相似性度量。该得分范围从0（对于无限差异的属性）到1（对于相同的属性）。\n\n**拓扑相似性, $s_{\\text{topo}}$**：为了比较两个节点的局部连接结构，我们比较它们的带符号度分布 $d(u)$ 和 $d(v)$。问题定义了一个基于曼哈顿距离（$\\ell_1$ 范数）的相似性度量，并将其归一化到 $[0, 1]$ 区间。\n$$s_{\\text{topo}}(u,v) = 1 - \\frac{\\lVert d(u) - d(v) \\rVert_1}{\\lVert d(u) \\rVert_1 + \\lVert d(v) \\rVert_1 + \\varepsilon}$$\n项 $\\lVert d(u) - d(v) \\rVert_1$ 度量了带符号度计数的总差异。通过总度数之和 $\\lVert d(u) \\rVert_1 + \\lVert d(v) \\rVert_1$ 进行归一化，使得该度量具有尺度不变性。添加一个小的常数 $\\varepsilon = 10^{-9}$ 是为了数值稳定性，防止当两个节点都是孤立的（即度分布为零）时出现除以零的情况。在这种情况下，相似性正确地评估为1，因为两个孤立节点在拓扑上是相同的。\n\n**3. 通过线性分配实现最优比对**\n目标是在大小为 $k = \\min(|V_1|, |V_2|)$ 的节点子集 $V_1' \\subseteq V_1$ 和 $V_2' \\subseteq V_2$ 之间找到一个一一映射 $f: V_1' \\to V_2'$，以最大化总相似性：$\\sum_{u \\in V_1'} s(u, f(u))$。这正是最大权二分匹配问题。我们可以构建一个 $|V_1| \\times |V_2|$ 的矩阵 $S$，其中 $S_{uv} = s(u,v)$。问题就变成了从 $S$ 中选择 $k$ 个条目，其中任意两个条目都不在同一行或同一列，使得它们的和最大化。\n\n这个问题可以通过考虑成本矩阵 $C = -S$ 转化为一个最小化问题。最小化 $\\sum C_{uv}$ 的解等价于最大化 $\\sum S_{uv}$ 的解。这个最小化问题是标准的线性分配问题，可以通过匈牙利算法或拍卖算法等高效求解。我们将使用 `scipy.optimize.linear_sum_assignment` 中提供的实现，它能正确处理矩形矩阵，自然地选择出最优的 $k = \\min(|V_1|, |V_2|)$ 个匹配。\n\n**4. 边分类和分数计算**\n一旦找到最优比对 $f$，我们就分析其结构保守性。对于 $E_1$ 中的每条边 $(u, u', s_1)$，我们检查其端点 $u$ 和 $u'$ 是否都属于比对的一部分（即 $u, u' \\in V_1'$）。如果是，我们就检查它们在 $G_2$ 中的对应点，即 $f(u)$ 和 $f(u')$。\n- 如果存在一条边 $(f(u), f(u'), s_2) \\in E_2$ 且其符号相同 $s_2 = s_1$，则该边是**保守的**。\n- 如果在 $G_2$ 中 $f(u)$ 和 $f(u')$ 之间不存在边（“缺失”边），或者存在边但符号相反 $s_2 \\neq s_1$（“符号翻转”），则该边是**重连的**。\n$E_1$ 中至少有一个端点未被比对的边将被忽略。\n\n最后，比对的总体质量由比对分数 $S_{\\text{align}}$ 量化。它被定义为已比对节点对的平均相似度：\n$$S_{\\text{align}} = \\frac{1}{k} \\sum_{(u,f(u)) \\in f} s(u, f(u))$$\n其中 $k$ 是已比对的节点对数量。这个分数提供了一个归一化的度量，用于衡量比对所达到的拟合优度。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and print the final results.\n    \"\"\"\n\n    test_cases = [\n        # Case 1\n        {\n            \"V1_size\": 6,\n            \"E1\": [(0, 1, 1), (1, 2, 1), (2, 3, -1), (0, 4, 1), (4, 5, 1), (1, 4, -1), (3, 5, 1)],\n            \"a1\": np.array([1.2, 0.9, 0.5, -0.2, 0.8, 0.3]),\n            \"V2_size\": 6,\n            \"E2\": [(0, 1, 1), (1, 2, 1), (2, 3, 1), (0, 4, 1), (4, 5, 1), (3, 5, 1), (0, 3, 1)],\n            \"a2\": np.array([1.1, 0.85, 0.6, -0.1, 0.75, 0.35]),\n            \"alpha\": 0.6,\n            \"sigma\": 0.5\n        },\n        # Case 2\n        {\n            \"V1_size\": 4,\n            \"E1\": [(0, 1, 1), (1, 2, 1), (2, 3, -1)],\n            \"a1\": np.array([0.9, 0.7, 0.6, 0.4]),\n            \"V2_size\": 4,\n            \"E2\": [(0, 2, -1), (2, 1, -1), (3, 0, 1)],\n            \"a2\": np.array([-0.9, -0.7, -0.6, -0.4]),\n            \"alpha\": 0.8,\n            \"sigma\": 0.2\n        },\n        # Case 3\n        {\n            \"V1_size\": 5,\n            \"E1\": [(0, 1, 1), (1, 2, -1), (2, 3, 1), (3, 4, -1), (0, 2, 1)],\n            \"a1\": np.array([0.2, 0.1, 0.05, -0.1, -0.2]),\n            \"V2_size\": 3,\n            \"E2\": [(0, 1, 1), (1, 2, -1), (0, 2, 1)],\n            \"a2\": np.array([0.18, 0.12, 0.06]),\n            \"alpha\": 0.5,\n            \"sigma\": 0.1\n        }\n    ]\n    \n    results = []\n    for case in test_cases:\n        res_tuple = solve_case(**case)\n        results.extend(list(res_tuple))\n\n    # The final print statement must follow the specified format exactly.\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef solve_case(V1_size, E1, a1, V2_size, E2, a2, alpha, sigma):\n    \"\"\"\n    Solves a single network alignment instance.\n    \"\"\"\n    epsilon = 1e-9\n\n    # 1. Calculate Signed Degree Profiles for G1 and G2\n    d1 = np.zeros((V1_size, 4), dtype=int)\n    for u, v, s in E1:\n        if s == 1:\n            d1[u, 0] += 1  # out-plus\n            d1[v, 2] += 1  # in-plus\n        else:\n            d1[u, 1] += 1  # out-minus\n            d1[v, 3] += 1  # in-minus\n\n    d2 = np.zeros((V2_size, 4), dtype=int)\n    for u, v, s in E2:\n        if s == 1:\n            d2[u, 0] += 1\n            d2[v, 2] += 1\n        else:\n            d2[u, 1] += 1\n            d2[v, 3] += 1\n\n    # 2. Construct the Similarity Matrix\n    sim_matrix = np.zeros((V1_size, V2_size))\n    for u in range(V1_size):\n        for v in range(V2_size):\n            # Attribute similarity (s_attr)\n            a_diff_sq = (a1[u] - a2[v])**2\n            s_attr = np.exp(-a_diff_sq / (2 * sigma**2))\n\n            # Topological similarity (s_topo)\n            du_vec = d1[u]\n            dv_vec = d2[v]\n            l1_diff = np.sum(np.abs(du_vec - dv_vec))\n            l1_sum = np.sum(du_vec) + np.sum(dv_vec)\n            s_topo = 1.0 - l1_diff / (l1_sum + epsilon)\n\n            # Combined similarity\n            sim_matrix[u, v] = alpha * s_attr + (1 - alpha) * s_topo\n\n    # 3. Solve the Linear Assignment Problem\n    # To maximize similarity, we minimize the negative similarity\n    cost_matrix = -sim_matrix\n    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n\n    # 4. Calculate Alignment Score\n    num_aligned = len(row_ind)\n    total_similarity = sim_matrix[row_ind, col_ind].sum()\n    alignment_score = total_similarity / num_aligned if num_aligned  0 else 0.0\n\n    # 5. Count Conserved and Rewired Edges\n    alignment_map = {u: v for u, v in zip(row_ind, col_ind)}\n    v1_aligned_nodes = set(row_ind)\n    e2_map = {(u, v): s for u, v, s in E2}\n\n    conserved_count = 0\n    rewired_count = 0\n    for u1, v1, s1 in E1:\n        if u1 in v1_aligned_nodes and v1 in v1_aligned_nodes:\n            u2 = alignment_map[u1]\n            v2 = alignment_map[v1]\n            if (u2, v2) in e2_map:\n                s2 = e2_map[(u2, v2)]\n                if s1 == s2:\n                    conserved_count += 1\n                else:  # Sign-flip\n                    rewired_count += 1\n            else:  # Missing edge\n                rewired_count += 1\n    \n    return conserved_count, rewired_count, round(alignment_score, 6)\n\nsolve()\n```", "id": "4330436"}]}