{"hands_on_practices": [{"introduction": "皮尔逊相关性是推断基因共表达网络最直接的方法。本练习将通过一个具体案例，带你从零开始计算两个基因表达谱之间的相关系数$r$，并推导用于检验其统计显著性的$t$检验统计量。更重要的是，本练习旨在揭示该检验背后的关键假设，并探讨真实数据中常见的异方差性和异常值等问题如何影响检验的有效性，从而为理解更稳健的网络推断方法奠定基础。[@problem_id:4365175]", "problem": "您正在研究共表达关系，以从跨独立生物样本测量的稳态信使核糖核酸 (mRNA) 表达中推断基因网络中的调控相互作用。考虑两个基因，表示为 $G_{1}$ 和 $G_{2}$，其在 $n=10$ 个样本中的标准化对数转换表达值由以下给出\n$G_{1}: \\{2.0,\\ 2.5,\\ 3.0,\\ 3.5,\\ 4.0,\\ 4.5,\\ 5.0,\\ 5.5,\\ 6.0,\\ 8.0\\}$ 和 $G_{2}: \\{1.9,\\ 2.6,\\ 2.7,\\ 3.6,\\ 3.9,\\ 4.7,\\ 5.1,\\ 5.4,\\ 6.1,\\ 6.5\\}$。目标是量化 $G_{1}$ 和 $G_{2}$ 之间的线性关联，并在共表达网络推断中常用的假设下评估其统计显著性。\n\n从样本均值、样本协方差和皮尔逊积矩相关系数的定义出发，计算 $G_{1}$ 和 $G_{2}$ 之间的样本皮尔逊相关性 $r$。然后，从经典线性模型假设（即 $(G_{1},G_{2})$ 是联合高斯分布，样本是独立同分布的 (i.i.d.)）出发，检验原假设 $H_{0}:\\rho=0$。通过将基于相关的检验统计量与对 $G_{1}$ 回归 $G_{2}$ 的简单线性回归中斜率的普通最小二乘法 $t$-检验联系起来，推导出该统计量的零分布。使用此推导计算上述数据的 $t$-统计量的值。\n\n将计算出的 $t$-统计量四舍五入到四位有效数字。\n\n最后，讨论表达数据中的异方差性（方差随均值变化）和离群值（包括高杠杆点）如何影响这种基于相关的 $t$-检验的有效性以及下游网络推断；特别地，解释对假阳性边的影响，并与高斯假设下基于互信息的推断以及非参数估计量进行对比。\n\n您最终报告的值必须是等于 $t$-统计量的单个实数，四舍五入到四位有效数字，不带单位。", "solution": "用户提供了一个问题，要求计算两个变量之间关联性的统计度量，推导相关的检验统计量，并讨论其基本假设。\n\n首先，我将验证问题陈述。\n\n### 第1步：提取给定信息\n-   基因1表达数据 ($G_{1}$): $\\{2.0,\\ 2.5,\\ 3.0,\\ 3.5,\\ 4.0,\\ 4.5,\\ 5.0,\\ 5.5,\\ 6.0,\\ 8.0\\}$\n-   基因2表达数据 ($G_{2}$): $\\{1.9,\\ 2.6,\\ 2.7,\\ 3.6,\\ 3.9,\\ 4.7,\\ 5.1,\\ 5.4,\\ 6.1,\\ 6.5\\}$\n-   样本数 ($n$): $10$\n-   任务1：计算样本皮尔逊相关系数 $r$。\n-   任务2：通过将基于相关的检验统计量与简单线性回归中斜率的普通最小二乘法 (OLS) $t$-检验联系起来，推导出该统计量的零分布。\n-   任务3：计算 $t$-统计量的值。\n-   任务4：将 $t$-统计量四舍五入到四位有效数字。\n-   任务5：讨论异方差性和离群值对检验有效性的影响，并与基于互信息的推断进行比较。\n\n### 第2步：使用提取的给定信息进行验证\n-   **科学依据**：该问题是生物统计学中的一个标准练习，特别是在用于网络推断的基因表达数据分析领域。皮尔逊相关、线性回归、$t$-检验以及这些模型的假设都是基本的统计学原理。讨论点（异方差性、离群值、互信息）是基因组学中稳健统计推断的核心。该问题在科学上是合理的。\n-   **问题明确**：该问题提供了所有必要的数据，并清楚地指明了所需的计算、推导和讨论主题。计算部分存在唯一的数值答案。\n-   **客观性**：该问题使用精确、客观和标准的科学术语进行陈述。\n\n### 第3步：结论与行动\n问题有效。我将继续提供完整解答。\n\n***\n\n**第一部分：样本皮尔逊相关系数 ($r$) 的计算**\n\n设基因 $G_{1}$ 的表达值为集合 $\\{x_i\\}$，基因 $G_{2}$ 的表达值为 $\\{y_i\\}$，其中 $i=1, \\dots, n$，$n=10$。\n\n样本皮尔逊相关系数 $r$ 定义为：\n$$r = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\sum_{i=1}^{n} (y_i - \\bar{y})^2}} = \\frac{SS_{xy}}{\\sqrt{SS_{xx} SS_{yy}}}$$\n其中 $\\bar{x}$ 和 $\\bar{y}$ 是样本均值，$SS_{xx}$、$SS_{yy}$ 和 $SS_{xy}$ 是平方和与交叉乘积和。\n\n首先，我们计算样本均值：\n$$\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i = \\frac{1}{10}(2.0+2.5+3.0+3.5+4.0+4.5+5.0+5.5+6.0+8.0) = \\frac{44.0}{10} = 4.4$$\n$$\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i = \\frac{1}{10}(1.9+2.6+2.7+3.6+3.9+4.7+5.1+5.4+6.1+6.5) = \\frac{42.5}{10} = 4.25$$\n\n接下来，我们计算平方和与交叉乘积和：\n$$SS_{xx} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2 = (2.0-4.4)^2 + (2.5-4.4)^2 + \\dots + (8.0-4.4)^2$$\n$$SS_{xx} = (-2.4)^2 + (-1.9)^2 + (-1.4)^2 + (-0.9)^2 + (-0.4)^2 + (0.1)^2 + (0.6)^2 + (1.1)^2 + (1.6)^2 + (3.6)^2$$\n$$SS_{xx} = 5.76 + 3.61 + 1.96 + 0.81 + 0.16 + 0.01 + 0.36 + 1.21 + 2.56 + 12.96 = 29.4$$\n\n$$SS_{yy} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2 = (1.9-4.25)^2 + (2.6-4.25)^2 + \\dots + (6.5-4.25)^2$$\n$$SS_{yy} = (-2.35)^2 + (-1.65)^2 + (-1.55)^2 + (-0.65)^2 + (-0.35)^2 + (0.45)^2 + (0.85)^2 + (1.15)^2 + (1.85)^2 + (2.25)^2$$\n$$SS_{yy} = 5.5225 + 2.7225 + 2.4025 + 0.4225 + 0.1225 + 0.2025 + 0.7225 + 1.3225 + 3.4225 + 5.0625 = 21.925$$\n\n$$SS_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y}) = (2.0-4.4)(1.9-4.25) + \\dots + (8.0-4.4)(6.5-4.25)$$\n$$SS_{xy} = (-2.4)(-2.35) + (-1.9)(-1.65) + (-1.4)(-1.55) + (-0.9)(-0.65) + (-0.4)(-0.35) + (0.1)(0.45) + (0.6)(0.85) + (1.1)(1.15) + (1.6)(1.85) + (3.6)(2.25)$$\n$$SS_{xy} = 5.64 + 3.135 + 2.17 + 0.585 + 0.14 + 0.045 + 0.51 + 1.265 + 2.96 + 8.1 = 24.55$$\n\n最后，我们计算 $r$：\n$$r = \\frac{24.55}{\\sqrt{29.4 \\times 21.925}} = \\frac{24.55}{\\sqrt{644.595}} \\approx 0.96696025$$\n\n**第二部分：相关性检验统计量的零分布推导**\n\n我们的目标是检验原假设 $H_0: \\rho = 0$，其中 $\\rho$ 是总体相关系数。这等价于在简单线性回归模型 $Y = \\beta_0 + \\beta_1 X + \\epsilon$ 中检验原假设 $H_0: \\beta_1 = 0$，其中我们将 $G_2$ 作为 $Y$ 对 $G_1$ 作为 $X$ 进行回归。\n\nOLS 斜率估计值 $b_1$ 的检验统计量由下式给出：\n$$t = \\frac{b_1 - \\beta_1}{SE(b_1)}$$\n在 $H_0: \\beta_1 = 0$ 的假设下，这简化为 $t = \\frac{b_1}{SE(b_1)}$。如果模型假设（线性、独立性、同方差性、误差正态性）成立，已知该统计量服从自由度为 $n-2$ 的学生 $t$-分布。\n\n斜率的OLS估计值为 $b_1 = \\frac{SS_{xy}}{SS_{xx}}$。\n斜率估计值的标准误为 $SE(b_1) = \\frac{\\hat{\\sigma}}{\\sqrt{SS_{xx}}}$，其中 $\\hat{\\sigma}^2$ 是均方误差 (MSE)，是误差方差 $\\sigma^2_ \\epsilon$ 的一个无偏估计量。\nMSE计算为 $MSE = \\frac{SSE}{n-2}$，其中 $SSE$ 是误差平方和（残差平方和）。\nSSE可以用平方和表示：$SSE = SS_{yy} - \\frac{(SS_{xy})^2}{SS_{xx}}$。\n\n现在，我们将 $t$-统计量用 $r$ 来重写。首先，认识到 $r$ 和 $b_1$ 之间的关系：\n$$r = \\frac{SS_{xy}}{\\sqrt{SS_{xx} SS_{yy}}} \\implies b_1 = \\frac{SS_{xy}}{SS_{xx}} = \\frac{r \\sqrt{SS_{xx} SS_{yy}}}{SS_{xx}} = r \\sqrt{\\frac{SS_{yy}}{SS_{xx}}}$$\n\n接下来，我们用 $r$ 来表示 $SSE$：\n$$SSE = SS_{yy} - \\frac{(r \\sqrt{SS_{xx} SS_{yy}})^2}{SS_{xx}} = SS_{yy} - \\frac{r^2 SS_{xx} SS_{yy}}{SS_{xx}} = SS_{yy} (1 - r^2)$$\n\n现在将这些表达式代入 $t$-统计量的公式中：\n$$t = \\frac{b_1}{SE(b_1)} = \\frac{b_1}{\\sqrt{\\frac{SSE}{n-2}} / \\sqrt{SS_{xx}}} = \\frac{b_1 \\sqrt{SS_{xx}}}{\\sqrt{\\frac{SS_{yy}(1 - r^2)}{n-2}}}$$\n\n代入 $b_1$ 的表达式：\n$$t = \\frac{\\left(r \\sqrt{\\frac{SS_{yy}}{SS_{xx}}}\\right) \\sqrt{SS_{xx}}}{\\sqrt{\\frac{SS_{yy}(1 - r^2)}{n-2}}} = \\frac{r \\sqrt{SS_{yy}}}{\\frac{\\sqrt{SS_{yy}} \\sqrt{1 - r^2}}{\\sqrt{n-2}}}$$\n$$t = \\frac{r \\sqrt{n-2}}{\\sqrt{1 - r^2}}$$\n这就是所求的检验统计量。在原假设 $H_0: \\rho = 0$ 下，该统计量服从自由度为 $n-2$ 的学生 $t$-分布。这就是零分布。\n\n**第三部分：t-统计量的计算**\n\n使用第一部分计算的值：\n$n = 10$\n$r \\approx 0.96696025$\n\n$$t = \\frac{0.96696025 \\sqrt{10-2}}{\\sqrt{1 - (0.96696025)^2}} = \\frac{0.96696025 \\sqrt{8}}{\\sqrt{1 - 0.93501192}} = \\frac{0.96696025 \\times 2.828427}{\\sqrt{0.06498808}} \\approx \\frac{2.73500}{0.2549178} \\approx 10.7290$$\n将结果四舍五入到四位有效数字，我们得到 $10.73$。\n\n**第四部分：关于模型有效性的讨论与比较**\n\n相关性 $t$-检验的有效性，以及基于它的共表达网络推断的有效性，关键取决于基础线性模型的假设。两个主要的违规情况是异方差性和离群值的存在。\n\n**异方差性**：指残差的方差在预测变量的值域内不恒定。在基因表达数据中，平均表达水平较高的基因通常也表现出较高的方差。OLS 程序假设同方差性（方差恒定）。当这一假设被违反时，斜率的OLS估计量 ($b_1$) 仍然是无偏的，但其标准误 ($SE(b_1)$) 的公式是不正确的，并且通常是有偏的。这种偏差导致计算出的 $t$-统计量不正确，在原假设下不再服从 $t$-分布。其后果是无效的p值和通常膨胀的第一类错误率。对于网络推断而言，这意味着**假阳性边**的数量增加，即由于有缺陷的统计检验而非潜在的生物学关联而宣告存在显著相关性。\n\n**离群值和高杠杆点**：离群值是与其他观测值显著偏离的数据点。高杠杆点是具有极端预测变量值的观测值（例如，本问题中的点 $(x_{10}, y_{10}) = (8.0, 6.5)$，其中 $x_{10}=8.0$ 远离 $\\bar{x}=4.4$）。既是离群值又具有高杠杆的点是影响点。众所周知，皮尔逊相关系数对这类点高度敏感。单个影响点可以在大部分数据不存在相关性的地方制造出强烈的、统计上显著的相关性，或者它可以掩盖一个真实的潜在相关性。这导致**假阳性**（由于人为因素推断出一条边）和**假阴性**（未能推断出一条真实的边）。推断出的网络连接变得高度依赖于单个、可能错误的数据点，而不是整体的生物学趋势。\n\n**与互信息 (MI) 的对比**：\n\n1.  **高斯假设下的 MI**：如果数据 $(G_1, G_2)$ 确实来自二元高斯分布，那么互信息 $I(G_1, G_2)$ 是皮尔逊相关系数平方 $\\rho^2$ 的单调函数：$I(G_1, G_2) = -\\frac{1}{2} \\ln(1 - \\rho^2)$。在这种特定的、理想化的情况下，检验 $I=0$ 与检验 $\\rho=0$ 完全等价。使用 MI 相对于相关性没有优势，因为它们衡量的是相同的潜在线性关联。\n\n2.  **使用非参数估计量的 MI**：MI 的真正优势在于与非参数估计量（例如，k-近邻或核密度估计量）一起使用时，这些估计量不假设高斯（或任何其他）分布。\n    -   **捕捉非线性**：MI 量化任何统计依赖关系，而不仅仅是线性关系。对于开关式或S型的调控关系，皮尔逊相关可能接近于零，导致假阴性。MI 可以检测到这种非线性关联。\n    -   **稳健性**：非参数 MI 通常对某些类型的离群值比皮尔逊相关更稳健。因为它基于概率密度（通常从秩或局部密度估计），所以它较少受到极端值大小的扭曲。它可以减轻由影响点驱动的、困扰基于相关性方法的假阳性和假阴性。\n    -   **缺点**：非参数 MI 的主要挑战是从有限数据中进行估计。它容易出现系统性估计偏差，特别是在样本量较小的情况下（$n=10$ 对于此目的极小），并且其方差可能很高。准确的 MI 估计是数据密集型的，如果处理不当，其本身也可能导致错误的推断。\n\n总之，对于网络推断，一个显著的基于相关的 $t$-检验只有在严格的假设下才是线性关联的可靠指标。异方差性和离群值等在真实数据中常见的违规情况，可能导致虚假的网络边。非参数 MI 提供了一种更稳健、更通用的检测依赖关系的方法，但其实际应用需要更大的样本量和谨慎的实施以避免其自身的估计陷阱。", "answer": "$$\n\\boxed{10.73}\n$$", "id": "4365175"}, {"introduction": "构建精确的生物网络不仅需要识别相互作用，还需要区分直接作用与由中介导致的间接作用。本练习将带你从简单的成对关联分析提升到网络层面的“三元组”分析，引入条件互信息（CMI）作为甄别间接关联的利器。你将学习并实现一种强大的非参数方法——置换检验（permutation test），用以评估中介效应的统计显著性，这是从海量数据中提炼真实直接调控关系的关键一步。[@problem_id:4365154]", "problem": "给定四个合成的基因表达三联体 $(X,Y,Z)$，它们是在零均值三元高斯模型下根据指定的相关矩阵生成的。任务是，在考虑一个潜在中介变量 $Z$ 的情况下，推断基因调控网络中基因 $(X,Y)$ 之间的无向边是否应被保留。该决策依赖于使用基于置换的显著性评估方法，来检验 $X$ 和 $Y$ 之间的互信息 (MI) 在以 $Z$ 为条件后是否显著降低。\n\n从连续随机变量的微分熵和互信息 (MI) 的基本定义出发，我们假设以下建模和估计框架：\n\n- $(X,Y,Z)$ 的联合分布是协方差矩阵 $\\boldsymbol{\\Sigma}$ 未知的三元正态分布。\n- 使用基于样本的协方差最大似然估计量来计算：\n  1. 在高斯模型下，根据 $X$ 和 $Y$ 的估计皮尔逊相关性计算互信息 (MI) $I(X;Y)$。\n  2. 在高斯模型下，根据控制了 $Z$ 的 $X$ 和 $Y$ 的估计偏相关性计算条件互信息 (CMI) $I(X;Y \\mid Z)$。\n- 为中介效应实现一个非参数置换检验：通过在观测样本间随机置换 $Z$ 的样本，来构建互信息减少量 $\\Delta = I(X;Y) - I(X;Y \\mid Z)$ 的经验零分布。对每次置换，重新计算 $I(X;Y \\mid Z^{\\pi})$ 和 $\\Delta^{\\pi}$（其中 $Z^{\\pi}$ 表示置换后的中介变量），并将满足 $\\Delta^{\\pi} \\ge \\Delta$ 的置换次数所占的比例估计为经验 $p$ 值。如果 $p$ 值严格小于显著性水平 $\\alpha$，则判定该减少量是显著的，应移除边 $(X,Y)$；否则保留该边。\n\n你的程序必须实现上述逻辑，并将其应用于以下测试套件。对每个测试用例，从 $\\mathcal{N}(\\boldsymbol{0}, \\mathbf{R})$ 生成 $n$ 个独立样本，其中 $\\mathbf{R}$ 是一个对角线上为单位方差的相关矩阵。使用指定的置换次数 $B$ 和显著性水平 $\\alpha$。为了保证可复现性，请使用固定的随机种子。\n\n测试套件（每个 $\\mathbf{R}$ 均表示为相关矩阵）：\n1. 案例 A（条件化后直接关联仍然存在）：\n   $$\n   n = 500,\\quad\n   \\mathbf{R} =\n   \\begin{bmatrix}\n   1  0.8  0.2 \\\\\n   0.8  1  0.2 \\\\\n   0.2  0.2  1\n   \\end{bmatrix},\\quad\n   B = 300,\\quad\n   \\alpha = 0.05.\n   $$\n2. 案例 B（相关性完全由 $Z$ 中介）：\n   $$\n   n = 500,\\quad\n   \\mathbf{R} =\n   \\begin{bmatrix}\n   1  0.49  0.7 \\\\\n   0.49  1  0.7 \\\\\n   0.7  0.7  1\n   \\end{bmatrix},\\quad\n   B = 300,\\quad\n   \\alpha = 0.05.\n   $$\n3. 案例 C（近边界的部分中介）：\n   $$\n   n = 300,\\quad\n   \\mathbf{R} =\n   \\begin{bmatrix}\n   1  0.3  0.4 \\\\\n   0.3  1  0.4 \\\\\n   0.4  0.4  1\n   \\end{bmatrix},\\quad\n   B = 300,\\quad\n   \\alpha = 0.05.\n   $$\n4. 案例 D（小样本，弱中介）：\n   $$\n   n = 60,\\quad\n   \\mathbf{R} =\n   \\begin{bmatrix}\n   1  0.3  0.05 \\\\\n   0.3  1  0.05 \\\\\n   0.05  0.05  1\n   \\end{bmatrix},\\quad\n   B = 200,\\quad\n   \\alpha = 0.05.\n   $$\n\n算法要求：\n- 从模拟数据中为每个案例估计样本协方差 $\\boldsymbol{\\hat{\\Sigma}}$，并计算 $X$ 和 $Y$ 之间的皮尔逊相关性。\n- 通过精度矩阵 $\\mathbf{P} = \\boldsymbol{\\hat{\\Sigma}}^{-1}$ 计算在给定 $Z$ 的条件下 $X$ 和 $Y$ 之间的偏相关性。\n- 在高斯模型下，分别从相关性和偏相关性计算 $I(X;Y)$ 和 $I(X;Y \\mid Z)$。\n- 执行包含 $B$ 次置换的置换检验，计算 MI 减少量的经验 $p$ 值，并决定是否保留边：如果保留边 $(X,Y)$，返回 $\\,\\text{True}\\,$；如果移除，返回 $\\,\\text{False}\\,$。\n- 使用固定的随机种子以确保每次运行结果的可复现性。\n\n你的程序应生成单行输出，其中包含案例 A–D 的四个布尔决策，形式为方括号内以逗号分隔的列表，例如，$[\\,\\text{True},\\text{False},\\text{True},\\text{False}\\,]$。不涉及任何物理单位或角度，所有输出均为布尔值。", "solution": "该问题要求实现一个统计程序，以根据第三个基因 $Z$ 的潜在中介效应，决定在调控网络中是否保留或移除两个基因 $X$ 和 $Y$ 之间的边。该框架基于三元高斯模型下的互信息，并通过置换检验评估其显著性。\n\n### 1. 理论基础：高斯变量的互信息\n\n本分析的基础是信息论。对于一个服从多元正态分布 $\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ 的连续 $d$ 维随机变量 $\\mathbf{X}$，其微分熵 $h(\\mathbf{X})$ 由下式给出：\n$$\nh(\\mathbf{X}) = \\frac{1}{2} \\ln \\left( (2\\pi e)^d \\det(\\boldsymbol{\\Sigma}) \\right)\n$$\n互信息 (MI) 量化了两个变量之间的统计依赖性。对于变量 $X$ 和 $Y$，其定义为 $I(X;Y) = h(X) + h(Y) - h(X,Y)$。假设 $(X,Y)$ 联合服从高斯分布，其方差为 $\\sigma_X^2=1$，$\\sigma_Y^2=1$，皮尔逊相关性为 $\\rho_{XY}$，则互信息可简化为相关性的直接函数：\n$$\nI(X;Y) = -\\frac{1}{2} \\ln(1 - \\rho_{XY}^2)\n$$\n此公式表明，随着相关性大小 $|\\rho_{XY}|$ 趋近于 $1$，互信息随之增加。\n\n### 2. 条件互信息与中介效应\n\n条件互信息 (CMI)，$I(X;Y \\mid Z)$，衡量在已知 $Z$ 的条件下 $X$ 和 $Y$ 之间的依赖关系。对于高斯变量，一个重要的结论将 CMI 与偏相关 $\\rho_{XY \\mid Z}$ 联系起来，后者是在回归掉 $Z$ 的线性效应后 $X$ 和 $Y$ 之间的相关性。其公式与互信息的公式类似：\n$$\nI(X;Y \\mid Z) = -\\frac{1}{2} \\ln(1 - \\rho_{XY \\mid Z}^2)\n$$\n如果 $Z$ 完全中介了 $X$ 和 $Y$ 之间的关系，则 $X$ 和 $Y$ 在给定 $Z$ 的条件下是条件独立的，这意味着 $\\rho_{XY \\mid Z} = 0$，因此 $I(X;Y \\mid Z) = 0$。这为识别中介效应提供了一个明确的标准：在以中介变量 $Z$ 为条件后，互信息出现显著下降。\n\n### 3. 从样本数据进行估计\n\n在实际应用中，真实的协方差矩阵 $\\mathbf{R}$ 是未知的，必须从 $n$ 个 $(X,Y,Z)$ 样本中进行估计。设 $n \\times 3$ 的数据矩阵为 $\\mathbf{D}$。\n\n-   **样本协方差与相关性**：协方差矩阵的最大似然估计 (MLE) 为 $\\boldsymbol{\\hat{\\Sigma}} = \\frac{1}{n} (\\mathbf{D}-\\bar{\\mathbf{D}})^T(\\mathbf{D}-\\bar{\\mathbf{D}})$，其中 $\\bar{\\mathbf{D}}$ 包含样本均值。样本皮尔逊相关性 $\\hat{\\rho}_{XY}$ 是根据 $\\boldsymbol{\\hat{\\Sigma}}$ 的元素计算的。然后我们计算估计的互信息：$\\hat{I}(X;Y) = -\\frac{1}{2}\\ln(1-\\hat{\\rho}_{XY}^2)$。\n\n-   **样本偏相关**：偏相关 $\\hat{\\rho}_{XY \\mid Z}$ 是使用样本精度矩阵 $\\mathbf{P} = \\boldsymbol{\\hat{\\Sigma}}^{-1}$ 估计的。其公式为：\n    $$\n    \\hat{\\rho}_{XY \\mid Z} = -\\frac{p_{12}}{\\sqrt{p_{11} p_{22}}}\n    $$\n    其中 $p_{ij}$ 是 $\\mathbf{P}$ 的元素。估计的条件互信息则为 $\\hat{I}(X;Y \\mid Z) = -\\frac{1}{2}\\ln(1-\\hat{\\rho}_{XY \\mid Z}^2)$。\n\n### 4. 用于显著性检验的置换检验\n\n核心假设是检验观测到的互信息减少量 $\\hat{\\Delta} = \\hat{I}(X;Y) - \\hat{I}(X;Y \\mid Z)$ 是否具有统计显著性。我们采用置换检验来生成该减少量的经验零分布。零假设 $H_0$ 是 $Z$ 与 $(X,Y)$ 对无关，因此不是一个中介变量。\n\n该过程如下：\n1.  从 $\\mathcal{N}(\\boldsymbol{0}, \\mathbf{R})$ 生成 $n$ 个样本，并计算观测到的统计量 $\\hat{I}(X;Y)$、$\\hat{I}(X;Y \\mid Z)$ 和观测到的减少量 $\\hat{\\Delta}$。\n2.  执行 $B$ 次置换。在每次置换 $b \\in \\{1, \\dots, B\\}$ 中：\n    a. 创建一个打乱后的数据集 $(X, Y, Z^{\\pi_b})$，其中 $Z^{\\pi_b}$ 是原始 $Z$ 数据向量的一个随机置换。这个过程打破了每个 $z_i$ 与其对应的 $(x_i, y_i)$ 对之间的特定关联，从而模拟了零假设。\n    b. 计算置换后数据的条件互信息 $\\hat{I}_b(X;Y \\mid Z^{\\pi_b})$。\n    c. 计算本次置换的互信息减少量：$\\hat{\\Delta}^{\\pi_b} = \\hat{I}(X;Y) - \\hat{I}_b(X;Y \\mid Z^{\\pi_b})$。请注意，$\\hat{I}(X;Y)$ 是来自原始数据的值，在整个置换过程中保持不变。\n3.  值集合 $\\{\\hat{\\Delta}^{\\pi_b}\\}$ 构成了经验零分布。\n4.  经验 $p$ 值是置换后的减少量大于或等于观测减少量的次数所占的比例：\n    $$\n    p = \\frac{|\\{b \\mid \\hat{\\Delta}^{\\pi_b} \\ge \\hat{\\Delta}\\}|}{B}\n    $$\n5.  最后，应用决策规则：如果 $p$ 值严格小于显著性水平 $\\alpha$（即 $p  \\alpha$），我们拒绝零假设，认为存在显著的中介效应，应移除边 $(X,Y)$（返回 `False`）。否则（即 $p \\ge \\alpha$），中介效应的证据不足，应保留该边（返回 `True`）。\n\n这个完整的算法将为所提供的 4 个测试用例分别实现，并使用固定的随机种子来确保结果的可复现性。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the network inference test for all specified cases.\n    \"\"\"\n    \n    # Fixed random seed for reproducibility as required by the problem.\n    seed = 42\n    rng = np.random.default_rng(seed)\n\n    # Test suite from the problem statement.\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"n\": 500,\n            \"R\": np.array([[1.0, 0.8, 0.2], [0.8, 1.0, 0.2], [0.2, 0.2, 1.0]]),\n            \"B\": 300,\n            \"alpha\": 0.05,\n        },\n        {\n            \"name\": \"Case B\",\n            \"n\": 500,\n            \"R\": np.array([[1.0, 0.49, 0.7], [0.49, 1.0, 0.7], [0.7, 0.7, 1.0]]),\n            \"B\": 300,\n            \"alpha\": 0.05,\n        },\n        {\n            \"name\": \"Case C\",\n            \"n\": 300,\n            \"R\": np.array([[1.0, 0.3, 0.4], [0.3, 1.0, 0.4], [0.4, 0.4, 1.0]]),\n            \"B\": 300,\n            \"alpha\": 0.05,\n        },\n        {\n            \"name\": \"Case D\",\n            \"n\": 60,\n            \"R\": np.array([[1.0, 0.3, 0.05], [0.3, 1.0, 0.05], [0.05, 0.05, 1.0]]),\n            \"B\": 200,\n            \"alpha\": 0.05,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        decision = perform_mediation_test(\n            n=case[\"n\"],\n            R=case[\"R\"],\n            B=case[\"B\"],\n            alpha=case[\"alpha\"],\n            rng=rng\n        )\n        results.append(decision)\n\n    # Print the final result in the specified format.\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef compute_mi_from_corr(rho):\n    \"\"\"Computes mutual information from correlation for Gaussian variables.\"\"\"\n    # Clamp rho**2 to avoid domain errors with log due to floating point inaccuracies.\n    rho2 = np.clip(rho * rho, 0.0, 1.0 - 1e-15)\n    return -0.5 * np.log(1.0 - rho2)\n\n\ndef get_mi_cmi_stats(data_x, data_y, data_z):\n    \"\"\"\n    Computes MI(X;Y) and CMI(X;Y|Z) from sample data.\n    \"\"\"\n    data = np.column_stack((data_x, data_y, data_z))\n    \n    # Compute the sample covariance matrix (MLE, ddof=0).\n    try:\n        cov_matrix = np.cov(data, rowvar=False, ddof=0)\n    except Exception: # Fails if data is constant\n        return 0.0, 0.0\n\n    # ----- MI(X;Y) from Pearson correlation -----\n    var_x, var_y = cov_matrix[0, 0], cov_matrix[1, 1]\n    if var_x  1e-15 or var_y  1e-15:\n        mi_xy = 0.0\n    else:\n        rho_xy = cov_matrix[0, 1] / np.sqrt(var_x * var_y)\n        mi_xy = compute_mi_from_corr(rho_xy)\n\n    # ----- CMI(X;Y|Z) from partial correlation -----\n    try:\n        # Precision matrix is the inverse of the covariance matrix.\n        prec_matrix = np.linalg.inv(cov_matrix)\n    except np.linalg.LinAlgError:\n        # If matrix is singular, CMI is ill-defined. Treat as maximal dependence.\n        # This implies huge reduction, but since it's a numeric error,\n        # we return a value that will cause retention of the edge.\n        return mi_xy, np.inf\n\n    p11, p22, p12 = prec_matrix[0, 0], prec_matrix[1, 1], prec_matrix[0, 1]\n    if p11  1e-15 or p22  1e-15:\n        cmi_xy_z = 0.0\n    else:\n        rho_xy_z = -p12 / np.sqrt(p11 * p22)\n        cmi_xy_z = compute_mi_from_corr(rho_xy_z)\n\n    return mi_xy, cmi_xy_z\n\n\ndef perform_mediation_test(n, R, B, alpha, rng):\n    \"\"\"\n    Performs the full permutation test for a single case.\n    Returns True to retain the edge, False to remove.\n    \"\"\"\n    # 1. Generate data from the specified trivariate Gaussian model.\n    mean = np.zeros(3)\n    data = rng.multivariate_normal(mean, R, size=n)\n    X, Y, Z = data[:, 0], data[:, 1], data[:, 2]\n\n    # 2. Calculate the observed MI reduction.\n    mi_obs, cmi_obs = get_mi_cmi_stats(X, Y, Z)\n\n    # If CMI is infinite due to singularity, the reduction is undefined/negative.\n    # This leads to retaining the edge, which is a safe choice.\n    if np.isinf(cmi_obs):\n        return True\n    \n    delta_obs = mi_obs - cmi_obs\n\n    # 3. Perform permutation test to build the null distribution of the MI reduction.\n    permuted_deltas_ge_observed = 0\n    Z_perm = Z.copy() # Create a copy to shuffle in-place\n    for _ in range(B):\n        rng.shuffle(Z_perm)\n        # For permuted data, we only need CMI. MI(X;Y) is invariant.\n        _, cmi_perm = get_mi_cmi_stats(X, Y, Z_perm)\n\n        # Handle numerical issues in permuted data\n        if np.isinf(cmi_perm):\n            delta_perm = -np.inf # Effectively ensures delta_perm  delta_obs\n        else:\n            delta_perm = mi_obs - cmi_perm\n        \n        if delta_perm >= delta_obs:\n            permuted_deltas_ge_observed += 1\n\n    # 4. Calculate the empirical p-value.\n    p_value = permuted_deltas_ge_observed / B\n\n    # 5. Make the decision: retain if p >= alpha, remove if p  alpha.\n    return p_value >= alpha\n\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "4365154"}]}