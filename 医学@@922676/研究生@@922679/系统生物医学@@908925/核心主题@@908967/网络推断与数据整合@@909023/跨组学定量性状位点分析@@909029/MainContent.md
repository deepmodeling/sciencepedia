## 引言
遗传变异是决定个体间复杂性状差异的基础，但其影响如何通过一系列精密的分子事件最终体现为生物学表型，这一过程在很大程度上仍是一个“黑箱”。传统的数量性状位点（QTL）分析通常只关注遗传与单一分子层面（如基因表达）的关联，忽略了生物系统固有的多层次调控网络。为了填补这一认知空白，跨组学数量性状位点（Trans-omics QTL）分析应运而生，它提供了一个系统性的框架，旨在整合多维度组学数据，追踪遗传效应在表观、转录、蛋白及代谢等层面间的传递路径。

本文将带领读者全面掌握这一前沿领域。在“原理与机制”一章中，我们将深入探讨其核心统计学基础和因果推断逻辑。随后的“应用与跨学科连接”一章将通过丰富的实例，展示该方法在解析分子通路、阐明疾病机理等方面的强大应用。最后，“动手实践”部分将介绍一些关键的分析挑战，帮助读者巩固理论知识。通过这一结构，我们将从基本原理出发，逐步深入，最终揭示如何利用跨组学QTL分析将遗传密码与生命功能联系起来。

## 原理与机制

本章旨在深入探讨跨组学[数量性状](@entry_id:144946)位点（QTL）分析的核心科学原理与关键技术机制。在前一章介绍其背景与意义的基础上，本章将系统性地阐述从数据生成到因果推断的全过程中所涉及的定义、[统计模型](@entry_id:755400)、混杂因素控制方法以及机制解释框架。我们将从基本概念出发，逐步构建一个完整的跨组学QTL分析知识体系。

### 跨组学QTL分析的核心概念

在进入复杂的[统计模型](@entry_id:755400)之前，我们必须首先明确跨组学QTL分析的基本定义及其核心组成部分。

#### 定义与目标

传统的QTL分析，如表达数量性状位点（eQTL）或蛋白质[数量性状](@entry_id:144946)位点（pQTL）分析，通常将遗传变异与单一分子层面的表型联系起来。然而，生物过程是一个多层次、相互关联的复杂系统。跨组学QTL分析的核心思想，正是将这一系统性观点融入[遗传关联](@entry_id:195051)研究中。

**跨组学[数量性状](@entry_id:144946)位点（Trans-omics QTL）分析**旨在联合建模并解析遗传变异（$G$）如何通过一系列相互关联的分子层面——例如[表观基因组](@entry_id:272005)（$Y^{(\mathrm{epi})}$）、转录组（$Y^{(\mathrm{tx})}$）、[蛋白质组](@entry_id:150306)（$Y^{(\mathrm{pr})}$）和代谢组（$Y^{(\mathrm{met})}$）——最终影响生物学性状。与简单地对每个组学层面进行独立的QTL分析，然后进行事后比较（例如，通过基因组坐标重叠关联信号）不同，跨组学分析的根本目标是阐明遗传效应在[生物学层次结构](@entry_id:137757)中传播的**因果路径** [@problem_id:4395259]。

这一目标要求我们采用能够捕捉层面间依赖关系的统计结构。例如，根据分子生物学的[中心法则](@entry_id:136612)，一个遗传变异可能首先影响[基因转录](@entry_id:155521)（$G \to Y^{(\mathrm{tx})}$），进而影响蛋白质丰度（$Y^{(\mathrm{tx})} \to Y^{(\mathrm{pr})}$）。跨组学分析的精髓在于，利用因果推断方法（如遗传[工具变量法](@entry_id:204495)和中介分析），区分遗传变异对某一特定层面的直接效应（direct effect）和通过上游层面介导的间接效应（mediated effect）。因此，它回答的不仅仅是“哪些位点与哪些分子相关”，更是“遗传效应是如何通过一个分子网络逐步传递的”。其最终产出是对路径特异性效应（path-specific effects）的估计，而不仅仅是层面特异性的[边际效应](@entry_id:634982)（layer-specific marginal effects）[@problem_id:4395259]。

#### 顺式与反式QTL：基因组距离的决定性作用

在QTL分析中，根据遗传变异与其调控的分子性状在基因组上的相对位置，关联被分为两大类：**顺式（cis）QTL** 和 **反式（trans）QTL**。这个分类不仅是操作上的，更反映了背后潜在的生物学机制和统计学特征的根本差异。

一个严谨的操作性定义如下：给定一个分子性状（如基因表达量）在基因组上的锚定位置$g$和一个遗传变异的位置$s$，我们预先设定一个基因组距离阈值$d$（例如$1$兆碱基对，即$1 \text{Mb}$）。
- 如果变异$s$和性状锚$g$位于**同一条染色体**上，并且它们之间的绝对基因组距离$|s - g|$不大于$d$，则该变异-性状对被分类为**顺式（cis）**。
- 所有其他情况，包括位于不同染色体上，或位于同一染色体但距离大于$d$的配对，均被分类为**反式（trans）** [@problem_id:4395292]。

顺式效应通常被认为是通过影响局部的DNA调控元件（如启动子、增[强子](@entry_id:198809)）来直接[调控基因](@entry_id:199295)的转录或[表观遗传](@entry_id:143805)状态。相比之下，反式效应则是通过一个可扩散的分子（如转录因子、非编码RNA）作为中介，该分子由基因组上某处的遗传变异调控，并作用于基因组其他位置的目标分子。

这种分类直接影响着后续的统计分析，特别是[多重检验校正](@entry_id:167133)的策略。由于顺式QTL的搜索空间被严格限制在每个性状周围一个相对较小的窗口内，而反式QTL的搜索空间则涵盖了整个基因组，两者的[检验数](@entry_id:173345)量存在巨大差异，这一点我们将在后续章节详细讨论。

### QTL作图的统计学基础

要从海量的组学数据中准确识别QTL，必须依赖于严谨的统计学方法。这包括对原始数据的精心处理、对各种混杂因素的精确控制以及对[多重检验](@entry_id:636512)的合理校正。

#### [数据标准化](@entry_id:147200)与预处理：以RNA测序为例

QTL分析的输入数据质量直接决定了结果的可靠性。以转录组QTL（eQTL）分析中常用的[RNA测序](@entry_id:178187)（[RNA-seq](@entry_id:140811)）数据为例，原始的基因计数值（counts）并不能直接用于线性关联分析，因为它受到多种技术因素的干扰，并且其统计学特性不满足标准线性模型的假设。一个统计上合理的处理流程通常包含以下关键步骤 [@problem_id:4395220]：

1.  **文库大小校正（Library Size Correction）**：不同样本的测序深度（即总读数）不同，这是一个主要的技术变异来源。假设原始计数值$Y_{gi}$的期望$\mu_{gi}$可以分解为样本特异性的文库大小因子$s_i$和真实的潜在表达水平$q_{gi}$的乘积，即$\mu_{gi} = s_i q_{gi}$。我们的目标是消除$s_i$的影响。诸如**TMM（Trimmed Mean of M-values）**或**[中位数](@entry_id:264877)比率法（median-of-ratios）**等稳健方法被用于估计$s_i$，然后通过将原始计数值除以对应的$s_i$来得到标准化的表达量。

2.  **方差稳定化（Variance Stabilization）**：RNA-seq计数值的方差与其均值相关（即存在[异方差性](@entry_id:136378)），高表达基因的方差通常远大于低表达基因。例如，在负二项（Negative Binomial, NB）分布模型中，方差可表示为$\operatorname{Var}(Y) = \mu + \phi \mu^2$，其中$\phi$是[离散度](@entry_id:168823)参数。直接对这样的数据使用[普通最小二乘法](@entry_id:137121)会违反其同方差假设，导致[统计推断](@entry_id:172747)的错误。为了解决这个问题，可以采用两种主流策略：
    *   **方差稳定化转换（Variance-Stabilizing Transformation, VST）**：应用一个数学函数（如对数转换或更复杂的、基于NB分布模型的rlog或vst转换），使得转换后的数据[方差近似](@entry_id:268585)独立于其均值。
    *   **权重线性模型（Weighted Linear Models）**：例如`voom`方法，它首先将计数值转换为$\log_2$-CPM（每百万计数的对数），然后根据均值-方差关系估计每个观测值的精确度权重$w_{gi}$，进而在[加权最小二乘法](@entry_id:177517)（WLS）中使用这些权重，从而在[模型拟合](@entry_id:265652)层面解决异方差问题。

3.  **协变量校正（Covariate Residualization）**：许多已知的技术（如实验批次、RNA完整性分数）和生物学（如年龄、性别）协变量同样会影响基因表达。在经过方差稳定化处理后，[数据近似](@entry_id:635046)满足线性模型的假设，此时可以通过将表达量对这些协变量进行[线性回归](@entry_id:142318)，然后取其**残差（residuals）**作为最终的表型进行QTL关联分析。或者，等价地，将这些协变量直接纳入最终的QTL关联模型中进行联合调整 [@problem_id:4395220]。

#### 控制混杂因素：稳健关联分析的关键

在[遗传关联](@entry_id:195051)研究中，任何与基因型和表型都相关的系统性因素都可能导致虚假关联。对这些**混杂因素（confounders）**的有效控制是确保结果可靠性的核心。

##### [群体结构](@entry_id:148599)与主成分分析

在人[类群](@entry_id:182524)体中，由于迁移、隔离和通婚等历史原因，不同祖源的亚群之间存在系统性的[等位基因频率](@entry_id:146872)差异。同时，这些亚群也可能因为环境、生活方式等非遗传因素导致表型均值的差异。当一个研究队列包含来自不同亚群的个体时，**[群体结构](@entry_id:148599)（population structure）**就成了一个主要的混杂因素。具体来说，如果某个SNP的[等位基因频率](@entry_id:146872)在亚群A中高于亚群B，而某个表型（如某代谢物水平）的均值也因环境原因在亚群A中更高，那么即使该SNP与该表型之间没有直接的因果联系，简单的关联检验也会显示出显著的（虚假）正相关 [@problem_id:4395218]。

一个标准的解决方案是使用**主成分分析（Principal Component Analysis, PCA）**。通过对[全基因组](@entry_id:195052)范围内的SNP基因型矩阵进行PCA，我们可以得到一系列**主成分（Principal Components, PCs）**。这些PCs捕捉了样本间遗传变异的主要轴，通常与个体的祖源背景高度相关。将前几个（例如，前5到10个）PCs作为协变量加入到QTL关联的[回归模型](@entry_id:163386)中，可以有效地校正由群体分层引起的混杂。

其代数等价性可以通过[Frisch-Waugh-Lovell定理](@entry_id:145855)来理解：将PCs作为协变量加入[多元回归](@entry_id:144007)模型，其效果等同于先分别将基因型和表型对PCs进行回归，然后对两者的残差进行关联分析。如果由祖源差异引起的[表型变异](@entry_id:163153)（即混杂效应）能够被所选的PCs[线性表示](@entry_id:139970)（即位于PCs张成的[列空间](@entry_id:156444)中），那么校正后的关联分析将不再受到该混杂效应的偏倚影响 [@problem_id:4395218]。

诊断[群体结构](@entry_id:148599)是否得到充分校正的一个常用工具是**QQ图（Quantile-Quantile plot）**。在存在系统性混杂时，大量本应为零假设的[检验统计量](@entry_id:167372)会发生膨胀，导致p值系统性偏小，QQ图上的点会整体偏离对角线。这可以通过**基因组膨胀因子（genomic inflation factor, $\lambda_{\mathrm{GC}}$）**来量化，通常$\lambda_{\mathrm{GC}} > 1$表示存在混杂。经过恰当的PC校正后，QQ图上大部分点会回到对角线附近，$\lambda_{\mathrm{GC}}$也应回归到接近1的水平 [@problem_id:4395218]。

##### [线性混合模型](@entry_id:139702)：整合[群体结构](@entry_id:148599)与亲缘关系

虽然PC校正对于大的群体结构非常有效，但它可能无法完全捕捉由远近亲缘关系（cryptic relatedness）引起的更细微的[遗传相关](@entry_id:176283)性。**[线性混合模型](@entry_id:139702)（Linear Mixed Model, LMM）**提供了一个更为全面的框架来解决这个问题。

LMM将表型$y$的变异分解为多个部分：
$$ y = G\beta + X\gamma + u_{poly} + \epsilon $$
其中：
-   $G\beta$ 是待检验候选变异的**固定效应（fixed effect）**。$G$是$n \times p$的基因型矩阵（$n$个样本，$p$个待测变异），$\beta$是其效应大小。
-   $X\gamma$ 是其他已知协变量（如年龄、性别、PCs）的固定效应。
-   $u_{poly}$ 是一个**随机效应（random effect）**项，代表了除候选变异外，全基因组所有其他遗传变异累积产生的**多基因背景效应（polygenic background effect）**。根据[数量遗传学](@entry_id:154685)原理，个体间的多基因效应的协方差结构与他们的遗传相似度成正比。因此，该项被建模为服从一个均值为0，协方差矩阵为$\sigma_{g}^{2} \Phi$的[多元正态分布](@entry_id:175229)，即$u_{poly} \sim \mathcal{N}(0, \sigma_{g}^{2} \Phi)$。这里的$\Phi$是一个$n \times n$的**遗传关系矩阵（Genetic Relationship Matrix, GRM）**或**亲缘关系矩阵（kinship matrix）**，根据全基因组范围的标记计算得出，$\sigma_{g}^{2}$是多基因方差组分。
-   $\epsilon$ 是独立的残差项，代表非遗传因素和测量误差，通常假设$\epsilon \sim \mathcal{N}(0, \sigma_{e}^{2} I_{n})$。

在实际应用中，随机效应项$u_{poly}$常被构造成$Ku$的形式，其中$u \sim \mathcal{N}(0, \sigma_{g}^{2} I_{n})$是一个独立的潜在高斯向量，而矩阵$K$是亲缘关系矩阵$\Phi$的一个因子分解（例如Cholesky分解），满足$KK^{\top} = \Phi$。这样，$\operatorname{Cov}(Ku) = K(\sigma_{g}^{2}I_n)K^{\top} = \sigma_{g}^{2} \Phi$，从而在模型中正确地编码了基于亲缘关系的协方差结构 [@problem_id:4395330]。通过将个体的遗传背景作为一个随机效应来建模，LMM能够有效地区分候选变异的固定效应和由群体结构及亲缘关系引起的多基因背景效应，从而提供更准确的效应估计和更稳健的[假阳性](@entry_id:635878)控制。

##### 隐藏混杂因素的推断与校正

除了[群体结构](@entry_id:148599)，组学数据中还普遍存在其他未被测量的**隐藏混杂因素（hidden confounders）**，例如未知的实验[批次效应](@entry_id:265859)、样本处理差异、组织细胞类型比例的变化等。这些因素同样可能同时影响基因型和表型，导致虚假关联。

**代理变量分析（Surrogate Variable Analysis, SVA）**或**PEER（Probabilistic Estimation of Expression Residuals）**等方法被设计用来从高维度的组学数据（如基因表达矩阵）本身推断这些隐藏的代理变量。其核心思想是，这些混杂因素会在许多基因或分子特征上留下系统性的印记。通过对数据矩阵进行[降维](@entry_id:142982)（如[奇异值分解](@entry_id:138057)或[贝叶斯因子](@entry_id:143567)分析），可以提取出这些主要的变异模式作为代理变量。

在QTL分析中应用这些方法的关键原则是：**在估计代理变量时，必须排除待检验的主要遗传变异（$G$）的影响** [@problem_id:4395215]。如果将$G$包含在用于因子估计的模型中，那么所得到的代理变量很可能会吸收掉真实的遗传信号。随后若将这些“污染”的代理变量作为协变量加入QTL模型中，就会过度校正，削弱甚至完全消除真实的QTL信号，导致假阴性。正确的做法是，从已校正已知协变量（如年龄、性别，但不包括$G$）的表达数据残差中推断代理变量。然后，将这些推断出的代理变量$\hat{Z}$与已知的协变量$X$和主要遗传变异$G$一起放入最终的关联模型中：
$$ y^{(l)}_{j} \;=\; \beta^{(l,j)}_{g} \, G \;+\; X \, \beta^{(l,j)} \;+\; \hat{Z} \, \gamma^{(l,j)} \;+\; \varepsilon^{(l,j)} $$
通过这种方式，我们可以有效地校正未知混杂因素的影响，同时保护我们感兴趣的遗传信号不被错误地“校正掉” [@problem_id:4395215]。

#### [多重检验校正](@entry_id:167133)：在发现与错误之间取得平衡

QTL分析，特别是[全基因组](@entry_id:195052)范围的扫描，涉及进行数百万甚至数十亿次的假设检验。如果不进行校正，即使在完全没有真实信号的情况下，仅凭随机波动也会产生大量的[假阳性](@entry_id:635878)结果。

##### 顺式与反式QTL的检验负担当量

[多重检验](@entry_id:636512)的严重程度与总检验次数直接相关。在QTL分析中，顺式和反式QTL的检验次数存在巨大差异。考虑一个包含$m$个SNP和$t$个分子性状的基因组，其长度为$L$，顺式窗口总长度为$2d$。
-   **顺式检验总数（$N_{cis}$）**：对于每个性状，我们只检验其$2d$窗口内的SNP。假设SNP均匀分布，每个性状的顺式[检验数](@entry_id:173345)约为$m \cdot (2d/L)$。因此，总的顺式[检验数](@entry_id:173345)为 $N_{cis} \approx t \cdot m \cdot (2d/L)$。
-   **反式检验总数（$N_{trans}$）**：对于每个性状，我们检验窗口外的所有SNP。每个性状的反式[检验数](@entry_id:173345)约为$m \cdot ((L-2d)/L)$。因此，总的反式检验数为 $N_{trans} \approx t \cdot m \cdot ((L-2d)/L)$。

两者的比率$R$为：
$$ R = \frac{N_{trans}}{N_{cis}} = \frac{L - 2d}{2d} = \frac{L}{2d} - 1 $$
在人类基因组中，$L \approx 3 \times 10^9$ bp，一个典型的顺式窗口$2d$可能是$1 \times 10^6$ bp。代入这些值，我们得到$R \approx 3000 - 1 = 2999$ [@problem_id:4395345]。这意味着，对于每一个顺式检验，大约有3000个反式检验。这种[检验数](@entry_id:173345)量上的巨大差异（通常是几个数量级）导致反式QTL分析面临着比顺式分析严峻得多的多重检验负担，需要更严格的p值阈值来控制[假阳性](@entry_id:635878)，这也使得检测真实但效应较弱的反式信号变得更具挑战性 [@problem_id:4395292]。

##### 控制错误率：FWER与FDR

为了应对[多重检验问题](@entry_id:165508)，研究者们采用不同的统计策略来控制错误率。

-   **[Bonferroni校正](@entry_id:261239)**：这是一种旨在控制**族群错误率（Family-Wise Error Rate, FWER）**的方法。FWER被定义为在所有检验中出现至少一个[假阳性](@entry_id:635878)结果的概率，即$\mathbb{P}(V \ge 1)$，其中$V$是[假阳性](@entry_id:635878)的数量。[Bonferroni校正](@entry_id:261239)非常简单和保守：如果进行了$m$次检验，且希望将FWER控制在水平$\alpha$（如$0.05$）以下，那么单次检验的p值阈值就被设为$\alpha/m$。这种方法非常严格，虽然有力地控制了[假阳性](@entry_id:635878)，但也可能导致较高的假阴性率，错失许多真实的信号。

-   **[Benjamini-Hochberg](@entry_id:269887) (BH) 程序**：这是一种旨在控制**错误发现率（False Discovery Rate, FDR）**的方法。FDR被定义为所有被拒绝的零假设中，[假阳性](@entry_id:635878)所占的预期比例，即$FDR = \mathbb{E}[V/R]$（当总拒绝数$R=0$时，取$V/R=0$）。BH程序的步骤是：首先将所有$m$个p值从小到大排序$p_{(1)} \le p_{(2)} \le \dots \le p_{(m)}$；然后，找到最大的索引$k$，使得$p_{(k)} \le \frac{k}{m}q$，其中$q$是目标FDR水平（如$0.05$）；最后，拒绝所有[p值](@entry_id:136498)小于等于$p_{(k)}$的假设。

在假说独立或存在正相关的条件下（这在生物数据中很常见），BH程序能将FDR控制在$q$以下。与控制“不犯任何一个错误”的概率（FWER）相比，控制“错误发现所占的平均比例”（FDR）是一个更宽松的标准。因此，BH程序通常比[Bonferroni校正](@entry_id:261239)具有更高的[统计功效](@entry_id:197129)（power），能够在控制[假阳性](@entry_id:635878)在可接受范围内的同时，发现更多的真实关联，因此在探索性的高通量研究（如QTL扫描）中被广泛应用 [@problem_id:4395322]。

### 因果推断与机制解析

识别出QTL仅仅是第一步，跨组学分析的最终目标是利用这些关联来推断分子间的因果关系，并构建起从基因到性状的[调控网络](@entry_id:754215)。

#### [共定位](@entry_id:187613)分析：区分共享因果与连锁不平衡

当我们在一个基因组区域内同时观测到两个不同性状（如一个eQTL和一个mQTL）的显著关联信号时，一个关键问题是：这两个信号是由同一个潜在的因果变异驱动的（**共享因果**），还是由两个紧密连锁、处于**连锁不平衡（Linkage Disequilibrium, LD）**状态的不同因果变异分别驱动的？仅仅观察到信号峰的重叠是不足以回答这个问题的，因为LD效应会使一个因果位点周围的非因果位点的关联信号也一并升高。

**统计共定位（Statistical Colocalization）**分析提供了一个形式化的贝叶斯框架来解决这个问题。以`coloc`方法为例，它在一个给定的基因组区域内，基于“每个性状最多只有一个因果变异”的假设，比较五种互斥的假设的后验概率 [@problem_id:4395276]：
-   $H_0$：该区域内没有与任一性状相关的因果变异。
-   $H_1$：该区域内只有与性状1相关的因果变异。
-   $H_2$：该区域内只有与性状2相关的因果变异。
-   $H_3$：该区域内有两个不同的因果变异，分别与性状1和性状2相关。
-   $H_4$：该区域内存在一个**共享的**因果变异，它同时与性状1和性状2相关。

通过整合两个性状的关联分析摘要统计数据（如效应量估计及其方差）和该区域的LD信息，该方法利用贝叶斯定理计算每个假设的后验概率$P(H_k | \text{Data})$。如果$P(H_4 | \text{Data})$很高（例如$>0.8$），则我们有强有力的证据支持这两个性状在该区域共享同一个因果变异，这为后续的因果中介分析提供了坚实的基础。

#### 孟德尔随机化：利用遗传变异进行因果推断

**[孟德尔随机化](@entry_id:147183)（Mendelian Randomization, MR）**是一种利用遗传变异作为**工具变量（Instrumental Variable, IV）**来推断暴露（如某个分子的丰度）与结局（如某个疾病或下游分子）之间因果关系的方法。由于等位基因在[减数分裂](@entry_id:140281)过程中的随机分配，一个人的基因型在很大程度上独立于后天的环境和生活方式等混杂因素，这使其成为理想的“自然随机实验”中的工具。

为了使一个遗传变异$G$成为一个有效的工具变量来估计暴露$X$对结局$Y$的因果效应，必须满足三个核心假设 [@problem_id:4395217]：
1.  **相关性假设（Relevance）**：工具变量$G$必须与暴露$X$强相关（$\operatorname{Cov}(G, X) \neq 0$）。在QTL背景下，这意味着$G$必须是一个与分子暴露$X$显著关联的QTL。
2.  **独立性假设（Independence）**：工具变量$G$必须独立于所有可能混淆$X$与$Y$关系的未测量混杂因素$U$（$G \perp U$）。由于基因型的先天性，这一假设通常被认为是相对可信的。
3.  **排他性限制假设（Exclusion Restriction）**：工具变量$G$只能通过暴露$X$来影响结局$Y$，不能存在绕过$X$的其他旁路（$Y \perp G \mid X, U$）。

在跨组学研究中，对排他性限制假设的违反，即**[水平多效性](@entry_id:269508)（horizontal pleiotropy）**，是一个主要的挑战。例如，一个SNP可能同时影响一个基因的表达（暴露$X$）和另一个不相关的代谢通路（旁路$W$），而这两者都影响最终的结局$Y$。此时，SNP对$Y$的总效应就混合了通过$X$的路径和通过$W$的路径，导致因果估计的偏倚。[连锁不平衡](@entry_id:146203)是另一个潜在的威胁：如果作为工具变量的SNP与另一个真正具有多效性的SNP处于LD状态，也会导致排他性限制假设的违反 [@problem_id:4395217]。

#### 因果中介分析：解析分子调控通路

在建立了从遗传变异到不同分子层面的关联后，我们可以通过**因果中介分析（Causal Mediation Analysis）**来具体检验这些关联是否构成了一个因果链，例如$G \to M \to Y$。

##### 中介效应的基本模型与识别

在一个简单的[线性高斯模型](@entry_id:268963)框架下，我们可以通过一系列回归来分解总效应。考虑以下模型：
$$ M = \beta_{M0} + \alpha G + \epsilon_M $$
$$ Y = \beta_{Y0} + c' G + \beta M + \epsilon_Y $$
-   $\alpha$ 是$G$对中介物$M$的效应。
-   $\beta$ 是在控制了$G$之后，$M$对结局$Y$的效应。
-   $c'$ 是在控制了$M$之后，$G$对$Y$的**直接效应**。
-   **间接效应（indirect effect）**或**中介效应（mediated effect）**被定义为路径系数的乘积：$\alpha\beta$。
-   $G$对$Y$的**总效应**$c$等于直接效应与间接效应之和：$c = c' + \alpha\beta$。

当直接效应$c'$在统计上不显著（即接近于0）时，我们认为$G$对$Y$的效应被$M$**完全中介（full mediation）**。这在概率上等价于，在给定中介物$M$的条件下，结局$Y$与基因型$G$条件独立（$Y \perp G \mid M$）[@problem_id:4395331]。

##### 中介分析中的高级挑战与假设

要将上述[回归系数解释](@entry_id:635491)为因果效应，需要满足比MR更严格的一系列假设。除了对$G \to M$和$G \to Y$关系的无混杂要求外，还有两个关键假设尤为重要 [@problem_id:4395358]：

1.  **不存在未测量的中介物-结局混杂（No unmeasured mediator-outcome confounding）**：必须没有未测量的因素同时影响中介物$M$和结局$Y$。例如，一个未知的批次效应$B$如果同时影响了转录本$M$的测量和代谢物$Y$的测量（$M \leftarrow B \rightarrow Y$），它就会成为$M-Y$关系的混杂因素。即使$G$与$B$无关，这种混杂也会使对$\beta$和$c'$的估计产生偏倚。因此，准确测量并校正所有已知的$M-Y$[共同原因](@entry_id:266381)（如实验批次）是至关重要的。在存在未测量混杂$U$（$M \leftarrow U \rightarrow Y$）的情况下，对$M$进行条件化（这是回归分析的内在操作）会打开$G \to M \leftarrow U$这条通路上的[对撞偏倚](@entry_id:163186)（collider bias），从而扭曲对直接效应的估计 [@problem_id:4395358]。

2.  **不存在由暴露引起的中介物-结局混杂（No exposure-induced mediator-outcome confounding）**：不能存在一个$M-Y$的混杂因素，它本身是由暴露$G$引起的。例如，假设某个SNP $X$会影响组织中某种细胞类型的比例$C$（$X \to C$），而细胞类型比例$C$又同时影响基因表达$M$和代谢物$Y$（$C \to M$且$C \to Y$）。在这种情况下，$C$就是一个由暴露引起的混杂因素。即使我们测量了$C$，标准的回归调整方法也会失效，因为对$C$进行条件化会阻断部分中介路径并可能引入其他偏倚。处理此类问题需要更高级的因果推断方法，如G-computation或估计干预（inventional）效应。

理解并审慎评估这些假设在具体组学研究背景下的合理性，是进行可靠因果中介分析、从而真正揭示分子机制的前提。