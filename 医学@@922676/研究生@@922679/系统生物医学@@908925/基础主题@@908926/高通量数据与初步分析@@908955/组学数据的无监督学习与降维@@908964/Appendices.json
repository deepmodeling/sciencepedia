{"hands_on_practices": [{"introduction": "主成分分析 (PCA) 是探索高维组学数据的基石。通过这个实践，你将通过奇异值分解 (SVD) 亲手实现 PCA，并学习如何通过计算解释方差比来量化其有效性，从而巩固对该方法核心机制的理解。这个练习是掌握任何高维数据分析流程的第一个关键步骤 [@problem_id:4397353]。", "problem": "在系统生物医学中，高维的转录组、蛋白质组和代谢组学图谱可以被建模为一个实值数据矩阵，其中行代表生物样本，列代表分子特征。考虑一个中心化数据矩阵 $X_c \\in \\mathbb{R}^{n \\times p}$，其各列在样本间的均值为零。主成分分析（PCA，Principal Component Analysis）用于寻找使方差最大化的正交方向。使用奇异值分解（SVD，Singular Value Decomposition），实现一个程序，对每个提供的测试用例，计算前 $k$ 个主成分，并输出这 $k$ 个主成分所解释的总方差比例。该比例定义为前 $k$ 个主成分捕获的方差与所有可采纳主成分的总方差之比。可采纳的主成分数量受中心化引入的自由度和环境维度的限制。如果总方差为零，则将解释方差比例定义为 $0$。\n\n您的实现必须：\n- 接受一个中心化矩阵 $X_c$ 和一个整数 $k$。\n- 计算 SVD $X_c = U \\Sigma V^\\top$ 并用它来获取作为前 $k$ 个右奇异向量（主轴）的前 $k$ 个主成分，并如上所述计算解释方差比例。\n- 遵循惯例，即中心化将非零主成分的最大数量减少到至多 $\\min(n-1, p)$。\n- 不对特征进行标准化或重新缩放；将 $X_c$ 视为列已中心化的矩阵。\n- 处理 $k=0$ 的边界情况，返回解释方差比例为 $0$。\n- 处理总方差为零的情况（例如，一个零矩阵），返回 $0$。\n\n测试套件：\n提供一个单一的程序，为以下四个测试用例计算解释方差比例。每个 $X_c^{(i)}$ 都被明确给出，并指定了 $k^{(i)}$。所有矩阵都是列中心化的。对每个案例，将该比例计算为小数（而非百分比）。\n\n- 案例 1：\n  - 维度：$n = 6$, $p = 4$, $k^{(1)} = 2$。\n  - 矩阵：\n  $$\n  X_c^{(1)} =\n  \\begin{bmatrix}\n  -2  3  1  2 \\\\\n  -1  -1  -1  2 \\\\\n  0  0  1  -1 \\\\\n  1  -2  -1  -1 \\\\\n  2  1  0  -1 \\\\\n  0  -1  0  -1\n  \\end{bmatrix}\n  $$\n\n- 案例 2：\n  - 维度：$n = 4$, $p = 6$, $k^{(2)} = 1$。\n  - 矩阵：\n  $$\n  X_c^{(2)} =\n  \\begin{bmatrix}\n  3  1  2  1  0  1 \\\\\n  -1  1  -2  0  0  1 \\\\\n  -2  -1  0  -1  0  -1 \\\\\n  0  -1  0  0  0  -1\n  \\end{bmatrix}\n  $$\n\n- 案例 3：\n  - 维度：$n = 3$, $p = 2$, $k^{(3)} = 2$。\n  - 矩阵：\n  $$\n  X_c^{(3)} =\n  \\begin{bmatrix}\n  1  -1 \\\\\n  0  0 \\\\\n  -1  1\n  \\end{bmatrix}\n  $$\n\n- 案例 4：\n  - 维度：$n = 5$, $p = 3$, $k^{(4)} = 0$。\n  - 矩阵：\n  $$\n  X_c^{(4)} =\n  \\begin{bmatrix}\n  2  1  0 \\\\\n  -2  -1  0 \\\\\n  0  2  1 \\\\\n  1  -2  -1 \\\\\n  -1  0  0\n  \\end{bmatrix}\n  $$\n\n最终输出格式：\n- 您的程序应生成单行输出，其中按顺序包含对应于四个案例的四个解释方差比例，每个比例都四舍五入到小数点后恰好六位，并以逗号分隔列表的形式用方括号括起来。例如，输出必须看起来像 $[\\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4]$，其中每个 $\\alpha_i$ 是小数点后恰好有六位的小数。", "solution": "用户提供了一个有效且定义明确的问题陈述，其基础是应用于计算生物学的线性代数和统计学原理。任务是对于一个给定的中心化数据矩阵 $X_c$，计算前 $k$ 个主成分所解释的总方差比例。这将通过奇异值分解（SVD）来完成。\n\n### 原理与推导\n\n主成分分析（PCA）是一种降维技术，它将一个包含潜在相关变量的数据集转换为一组称为主成分的线性不相关变量。这些主成分被排序，使得前几个主成分保留了原始数据集中存在的大部分变异。\n\n设数据由一个矩阵 $X_c \\in \\mathbb{R}^{n \\times p}$ 表示，其中 $n$ 是样本数， $p$ 是特征数。问题指定该矩阵是中心化的，意味着每列（特征）的均值为零。\n\n样本协方差矩阵捕获了特征对之间的协方差，其公式为：\n$$\nC = \\frac{1}{n-1} X_c^\\top X_c\n$$\nPCA寻找该协方差矩阵 $C$ 的特征向量。这些特征向量是主轴，而对应的特征值 $\\lambda_j$ 代表数据投影到这些轴上时的方差。数据集中的总方差是所有特征值的和，也等于该协方差矩阵的迹：\n$$\n\\text{Total Variance} = \\text{tr}(C) = \\sum_{j=1}^{\\text{rank}(C)} \\lambda_j\n$$\n前 $k$ 个主成分解释的方差比例是前 $k$ 个最大特征值的和与所有特征值的和之比：\n$$\n\\text{Explained Variance Ratio} = \\frac{\\sum_{j=1}^k \\lambda_j}{\\sum_{j=1}^{\\text{rank}(C)} \\lambda_j}\n$$\n问题要求使用奇异值分解（SVD）来实现这一点。矩阵 $X_c$ 的SVD是一种形式如下的分解：\n$$\nX_c = U \\Sigma V^\\top\n$$\n其中 $U \\in \\mathbb{R}^{n \\times n}$ 和 $V \\in \\mathbb{R}^{p \\times p}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{n \\times p}$ 是一个包含按降序排列的非负奇异值 $\\sigma_j$ 的矩形对角矩阵。$V$ 的列是右奇异向量。\n\n我们可以建立SVD与协方差矩阵 $C$ 之间的直接关系：\n$$\nC = \\frac{1}{n-1} X_c^\\top X_c = \\frac{1}{n-1} (U \\Sigma V^\\top)^\\top (U \\Sigma V^\\top) = \\frac{1}{n-1} (V \\Sigma^\\top U^\\top U \\Sigma V^\\top)\n$$\n由于 $U$ 是正交矩阵，所以 $U^\\top U = I$ （单位矩阵）。这可将表达式简化为：\n$$\nC = V \\left( \\frac{\\Sigma^\\top \\Sigma}{n-1} \\right) V^\\top\n$$\n这个方程是 $C$ 的特征分解。$V$ 的列是 $C$ 的特征向量（主轴），而矩阵 $\\frac{\\Sigma^\\top \\Sigma}{n-1}$ 的对角元素是特征值 $\\lambda_j$。具体而言，$C$ 的特征值通过以下关系与 $X_c$ 的奇异值相关：\n$$\n\\lambda_j = \\frac{\\sigma_j^2}{n-1}\n$$\n将此代入解释方差比例公式，常数因子 $\\frac{1}{n-1}$ 会被消掉：\n$$\n\\text{Explained Variance Ratio} = \\frac{\\sum_{j=1}^k \\frac{\\sigma_j^2}{n-1}}{\\sum_{j=1}^{\\text{rank}(X_c)} \\frac{\\sigma_j^2}{n-1}} = \\frac{\\sum_{j=1}^k \\sigma_j^2}{\\sum_{j=1}^{\\text{rank}(X_c)} \\sigma_j^2}\n$$\n$X_c$ 的秩，记为 $\\text{rank}(X_c)$，是非零奇异值的数量。因此，分母是所有奇异值平方的和。问题指出，对于一个中心化矩阵，其秩最多为 $\\min(n-1, p)$。一个标准的SVD算法将计算 $\\min(n, p)$ 个奇异值，任何超出矩阵秩的分量所对应的奇异值都将为零，因此对求和没有贡献。所以，实现时只需将所有计算出的奇异值的平方相加即可得到分母。\n\n### 算法\n对于给定的矩阵 $X_c$ 和整数 $k$，计算解释方差比例的算法如下：\n1.  处理问题中指定的边界情况：如果 $k=0$，解释方差比例为 $0$。\n2.  计算矩阵 $X_c$ 的奇异值 $\\sigma_j$。大多数数值库都提供了执行此操作的函数（例如，`numpy.linalg.svd`）。\n3.  计算奇异值的平方 $\\sigma_j^2$。\n4.  计算总方差项，即所有奇异值平方的和：$\\sum_j \\sigma_j^2$。\n5.  处理总方差为零的边界情况（即 $X_c$ 是一个零矩阵）。在这种情况下，该比例定义为 $0$。\n6.  计算解释方差项，即前 $k$ 个奇异值平方的和：$\\sum_{j=1}^k \\sigma_j^2$。\n7.  解释方差比例是解释方差项除以总方差项。\n此过程将应用于所提供的四个测试用例中的每一个。", "answer": "```python\nimport numpy as np\n\ndef compute_explained_variance_ratio(X_c, k):\n    \"\"\"\n    Computes the fraction of total variance explained by the top k principal components.\n\n    Args:\n        X_c (np.ndarray): A centered data matrix of shape (n, p).\n        k (int): The number of top principal components to consider.\n\n    Returns:\n        float: The fraction of total variance explained.\n    \"\"\"\n    # Per the problem statement, if k=0, the ratio is 0.\n    if k == 0:\n        return 0.0\n\n    # The number of samples n is needed for the covariance definition, but the\n    # 1/(n-1) factor cancels in the ratio, so n is not explicitly used in the final formula.\n    # However, if n=1, the matrix must be a zero matrix (if centered), leading to zero variance.\n    if X_c.shape[0] == 1:\n        return 0.0\n        \n    # Compute the singular values of the centered data matrix X_c.\n    # We only need the singular values, so we set compute_uv=False for efficiency.\n    singular_values = np.linalg.svd(X_c, compute_uv=False)\n    \n    # The variance of each principal component is proportional to the square of its corresponding singular value.\n    # Eigenvalues of the covariance matrix are lambda_j = sigma_j^2 / (n-1).\n    squared_singular_values = singular_values**2\n    \n    # The total variance is proportional to the sum of all squared singular values.\n    # This corresponds to the trace of X_c.T @ X_c.\n    total_variance_term = np.sum(squared_singular_values)\n    \n    # Per the problem statement, if the total variance is zero, the ratio is 0.\n    # We use a small tolerance for floating point comparison.\n    if total_variance_term  1e-12:\n        return 0.0\n        \n    # The variance explained by the top k components is proportional to the sum\n    # of the first k squared singular values.\n    explained_variance_term = np.sum(squared_singular_values[:k])\n    \n    # The explained variance ratio is the division of the two terms.\n    # The scaling factor 1/(n-1) from the covariance definition cancels out.\n    ratio = explained_variance_term / total_variance_term\n    \n    return ratio\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite.\n    \"\"\"\n    # Case 1: n=6, p=4, k=2\n    X1 = np.array([\n        [-2., 3., 1., 2.],\n        [-1., -1., -1., 2.],\n        [0., 0., 1., -1.],\n        [1., -2., -1., -1.],\n        [2., 1., 0., -1.],\n        [0., -1., 0., -1.]\n    ])\n    k1 = 2\n\n    # Case 2: n=4, p=6, k=1\n    X2 = np.array([\n        [3., 1., 2., 1., 0., 1.],\n        [-1., 1., -2., 0., 0., 1.],\n        [-2., -1., 0., -1., 0., -1.],\n        [0., -1., 0., 0., 0., -1.]\n    ])\n    k2 = 1\n\n    # Case 3: n=3, p=2, k=2\n    X3 = np.array([\n        [1., -1.],\n        [0., 0.],\n        [-1., 1.]\n    ])\n    k3 = 2\n\n    # Case 4: n=5, p=3, k=0\n    X4 = np.array([\n        [2., 1., 0.],\n        [-2., -1., 0.],\n        [0., 2., 1.],\n        [1., -2., -1.],\n        [-1., 0., 0.]\n    ])\n    k4 = 0\n\n    test_cases = [\n        (X1, k1),\n        (X2, k2),\n        (X3, k3),\n        (X4, k4)\n    ]\n    \n    results = []\n    for X_c, k in test_cases:\n        ratio = compute_explained_variance_ratio(X_c, k)\n        # Format the result to exactly six digits after the decimal point.\n        results.append(f\"{ratio:.6f}\")\n        \n    # Print the final output in the specified format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "4397353"}, {"introduction": "在掌握了 PCA 的基础知识后，下一个关键问题是如何为该算法准备数据。这个练习将挑战你分析一个常见的预处理步骤——特征缩放——对 PCA 结果的深远影响。通过计算和推理，你将揭示为何对于稀疏的组学计数数据，单位方差缩放可能不仅无益，甚至会产生误导性的结果，这是一个在实际数据分析中至关重要的实践洞见 [@problem_id:4397370]。", "problem": "您正在分析一个包含两个基因的大量样本（细胞）的基因表达数据集，其中的特征已经进行了均值中心化。设这两个基因的经验协方差矩阵为\n$$\nS \\;=\\; \\begin{pmatrix} 100  9 \\\\ 9  1 \\end{pmatrix}.\n$$\n您将比较对未缩放特征执行的主成分分析（PCA；Principal Component Analysis）与对每个特征缩放至单位方差后执行的PCA。\n\n对于未缩放的PCA，将第一个载荷向量定义为与 $S$ 的最大特征值相关联的单位范数特征向量（即，对协方差矩阵进行PCA）。\n\n对于缩放至单位方差的PCA，将 $D$ 定义为特征方差的对角矩阵，\n$$\nD \\;=\\; \\mathrm{diag}(S_{11}, S_{22}) \\;=\\; \\mathrm{diag}(100, 1),\n$$\n并定义相关性-相似度矩阵\n$$\nC \\;=\\; D^{-1/2} \\, S \\, D^{-1/2}.\n$$\n设 $a$ 是与 $C$ 的最大特征值相关联的单位范数特征向量。为了在原始特征坐标中表示相应的第一个载荷方向，将 $a$ 映射回 $v_{\\mathrm{scaled}} \\propto D^{-1/2} a$，然后归一化为单位范数，以获得原始坐标系中经过缩放的第一个载荷向量。\n\n计算在原始特征坐标中，未缩放和缩放后的第一个载荷向量之间的角度 $\\theta$（以弧度为单位），定义为\n$$\n\\theta \\;=\\; \\arccos\\!\\big(|u^{\\top} v_{\\mathrm{scaled}}|\\big),\n$$\n其中 $u$ 是来自未缩放PCA的单位范数第一载荷向量，而 $v_{\\mathrm{scaled}}$ 是如上所述从缩放后PCA映射回来的单位范数第一载荷向量。报告 $\\theta$ 的值（以弧度为单位），四舍五入到四位有效数字。\n\n然后，从计数数据的基本均值-方差关系（例如，泊松分布方差 $\\mathrm{Var}(Y)=\\mu$ 和负二项分布方差 $\\mathrm{Var}(Y)=\\mu + \\mu^{2}/\\theta$）出发，论证对高通量转录组学中的稀疏计数数据将每个特征缩放至单位方差是否以及何时会有害，并从缩放如何改变低均值、零膨胀特征对PCA目标的贡献方面解释原因。您的解释必须基于这些基础关系进行论证，而不能依赖于预先推导出的启发式方法。\n\n您的最终数值答案应仅为 $\\theta$ 的值；最终方框答案中不要包含任何单位。解释性论证不需要包含在最终方框答案中，但必须出现在您的解题过程中。四舍五入到四位有效数字，并以弧度表示角度。", "solution": "用户提供了一个由两部分组成的问题。第一部分要求数值计算从未缩放和缩放数据中获得的第一主成分载荷向量之间的角度。第二部分要求对稀疏计数数据进行特征缩放的潜在问题进行概念性解释，该解释应基于此类数据的基本统计特性。\n\n问题陈述已经过验证，被认为是科学合理的、定义明确的、客观的和完整的。我现在将着手解决。\n\n### 第1部分：角度 $\\theta$ 的计算\n\n第一步是计算未缩放数据的第一个主成分（PC1）载荷向量。这被定义为与经验协方差矩阵 $S$ 的最大特征值相关联的单位范数特征向量。\n\n给定的协方差矩阵是：\n$$ S = \\begin{pmatrix} 100  9 \\\\ 9  1 \\end{pmatrix} $$\n对于一个形如 $\\begin{pmatrix} a  b \\\\ b  d \\end{pmatrix}$ 的 $2 \\times 2$ 对称矩阵，主特征向量与第一轴的夹角 $\\phi$ 可以通过以下关系找到：\n$$ \\tan(2\\phi) = \\frac{2b}{a-d} $$\n在我们的例子中，$a=100$，$b=9$，$d=1$。\n$$ \\tan(2 \\phi_u) = \\frac{2 \\times 9}{100 - 1} = \\frac{18}{99} = \\frac{2}{11} $$\n其中 $\\phi_u$ 是未缩放的PC1载荷向量 $u$ 与第一坐标轴的夹角。由于非对角元素为正且 $a  d$，特征向量将位于第一象限，所以我们可以写成：\n$$ \\phi_u = \\frac{1}{2} \\arctan\\left(\\frac{2}{11}\\right) $$\n载荷向量 $u$ 由 $u = \\begin{pmatrix} \\cos(\\phi_u) \\\\ \\sin(\\phi_u) \\end{pmatrix}$ 给出。\n\n第二步是计算缩放后数据的PC1载荷向量。首先，我们构建相关矩阵 $C$。方差的对角矩阵 $D$ 及其逆平方根 $D^{-1/2}$ 分别是：\n$$ D = \\mathrm{diag}(100, 1) = \\begin{pmatrix} 100  0 \\\\ 0  1 \\end{pmatrix} $$\n$$ D^{-1/2} = \\mathrm{diag}(100^{-1/2}, 1^{-1/2}) = \\begin{pmatrix} 1/10  0 \\\\ 0  1 \\end{pmatrix} $$\n那么相关矩阵 $C$ 是：\n$$ C = D^{-1/2} S D^{-1/2} = \\begin{pmatrix} 1/10  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 100  9 \\\\ 9  1 \\end{pmatrix} \\begin{pmatrix} 1/10  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 10  9/10 \\\\ 9  1 \\end{pmatrix} \\begin{pmatrix} 1/10  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1  9/10 \\\\ 9/10  1 \\end{pmatrix} $$\n接下来，我们找到缩放后数据的第一个载荷向量 $a$，它是与 $C$ 的最大特征值对应的单位范数特征向量。对于一个形如 $\\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$ 的相关矩阵，其特征向量总是与 $\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ 和 $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$ 成比例。特征向量 $\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ 对应于特征值 $1+\\rho$，而 $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$ 对应于 $1-\\rho$。这里，$\\rho = 9/10$，所以最大特征值是 $1+9/10=19/10$，其对应的未归一化特征向量是 $\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$。单位范数特征向量 $a$ 是：\n$$ a = \\frac{1}{\\sqrt{1^2+1^2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\n这个向量 $a$ 是在缩放后特征的坐标系中。为了找到在原始特征坐标系中对应的方向，我们使用 $D^{-1/2}$ 将其映射回去：\n$$ v_{\\mathrm{temp}} \\propto D^{-1/2} a = \\begin{pmatrix} 1/10  0 \\\\ 0  1 \\end{pmatrix} \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1/10 \\\\ 1 \\end{pmatrix} $$\n为了得到最终的载荷向量 $v_{\\mathrm{scaled}}$，我们对 $v_{\\mathrm{temp}}$ 进行归一化：\n$$ v_{\\mathrm{scaled}} = \\frac{\\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1/10 \\\\ 1 \\end{pmatrix}}{\\left\\| \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1/10 \\\\ 1 \\end{pmatrix} \\right\\|} = \\frac{\\begin{pmatrix} 1/10 \\\\ 1 \\end{pmatrix}}{\\sqrt{(1/10)^2 + 1^2}} = \\frac{\\begin{pmatrix} 1/10 \\\\ 1 \\end{pmatrix}}{\\sqrt{1/100 + 1}} = \\frac{\\begin{pmatrix} 1/10 \\\\ 1 \\end{pmatrix}}{\\sqrt{101/100}} = \\frac{\\sqrt{100}}{\\sqrt{101}} \\begin{pmatrix} 1/10 \\\\ 1 \\end{pmatrix} = \\frac{10}{\\sqrt{101}} \\begin{pmatrix} 1/10 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{101}} \\begin{pmatrix} 1 \\\\ 10 \\end{pmatrix} $$\n$v_{\\mathrm{scaled}}$ 与第一坐标轴的夹角 $\\phi_v$ 由以下公式给出：\n$$ \\tan(\\phi_v) = \\frac{10}{1} = 10 \\implies \\phi_v = \\arctan(10) $$\n问题要求计算角度 $\\theta = \\arccos(|u^{\\top} v_{\\mathrm{scaled}}|)$。点积 $u^{\\top} v_{\\mathrm{scaled}}$ 等于 $\\cos(\\phi_v - \\phi_u)$。因为 $\\phi_u$ 和 $\\phi_v$ 都在第一象限，且 $\\phi_v  \\phi_u$，它们的差是一个锐角。因此，其 cosines 值为正，所以 $\\arccos(\\cos(\\phi_v - \\phi_u)) = \\phi_v - \\phi_u$。\n$$ \\theta = \\phi_v - \\phi_u = \\arctan(10) - \\frac{1}{2}\\arctan\\left(\\frac{2}{11}\\right) $$\n我们现在计算其以弧度为单位的数值：\n$$ \\arctan(10) \\approx 1.47112767 \\text{ rad} $$\n$$ \\arctan\\left(\\frac{2}{11}\\right) \\approx 0.18005051 \\text{ rad} $$\n$$ \\theta \\approx 1.47112767 - \\frac{1}{2}(0.18005051) = 1.47112767 - 0.090025255 = 1.381102415 \\text{ rad} $$\n四舍五入到四位有效数字，我们得到 $\\theta \\approx 1.381$。\n\n### 第2部分：关于缩放稀疏计数数据的理由\n\n任务是基于基本的均值-方差关系，论证为什么将特征缩放至单位方差对于高通量转录组学中的稀疏计数数据可能是有害的。\n\n1.  **计数数据中的均值-方差耦合**：对于计数数据，一个特征（基因）的方差与其平均表达水平内在地联系在一起。问题给出了泊松模型，$\\mathrm{Var}(Y) = \\mu$，和负二项模型，$\\mathrm{Var}(Y) = \\mu + \\mu^2/\\phi$（使用 $\\phi$ 作为离散参数，通常表示为 $\\theta$），其中 $\\mu$ 是平均计数值。在这两个基础模型中，方差都是均值的增函数。平均表达水平较高的基因自然表现出较高的方差。\n\n2.  **转录组数据的性质**：高通量转录组数据集的特点是高度稀疏。许多基因的平均表达量非常低（$\\mu_j \\approx 0$），并且只在小部分细胞中被检测到（即计数值大于零）。对于这样的基因 $j$，其方差 $\\sigma_j^2 = \\mathrm{Var}(Y_j)$ 因此会非常低。\n\n3.  **单位方差缩放的影响**：对一个特征 $j$ 进行缩放，涉及将每个细胞 $i$ 的值 $Y_{ij}$ 转换为 $Y'_{ij} = Y_{ij} / \\sigma_j$。\n    - 对于未检测到基因 $j$ 的细胞，$Y_{ij} = 0$，因此缩放后的值仍然是 $Y'_{ij}=0$。\n    - 对于少数检测到基因 $j$ 的细胞，$Y_{ij}  0$。由于其方差 $\\sigma_j^2$（以及其标准差 $\\sigma_j$）非常小，缩放后的值 $Y'_{ij} = Y_{ij}/\\sigma_j$ 会变得极大。\n    实际上，缩放将一个大部分为零、夹杂少数小整数计数的向量，转换成一个大部分为零、夹杂少数极大数值的向量。\n\n4.  **对PCA目标的影响**：PCA旨在寻找能最大化方差的主成分（特征的线性组合）。\n    - **不进行缩放**时，总方差是单个基因方差的总和 $\\sum_j \\sigma_j^2$。具有高生物学方差的基因（例如，高表达的管家基因或驱动主要生物过程如细胞周期的基因）主导了这一总和。第一主成分将自然地与这些主要的变异轴对齐，这通常在生物学上是期望的。一个稀疏、低均值基因的贡献可以忽略不计，因为它的 $\\sigma_j^2$ 很小。\n    - **进行缩放**后，每个基因都被强制具有为1的方差。PCA目标现在给予每个基因同等的先验权重。然而，第3点中描述的转换具有有害影响。一个单一的、稀疏表达的基因，在缩放后，现在变成一个在一小部分细胞中具有极端值的特征。这样的特征可能表现出很高的“方差贡献”，不是因为它代表一个连续的生物过程，而是因为它像一个少数“离群”细胞的二元指示器一样起作用。一个主成分可以简单地通过给这个单一的、被人为夸大的基因分配一个大权重来获得高方差。\n\n5.  **有害结果**：最终得到的主成分并不代表多个基因间的协同表达模式，而这才是降维的目标。相反，它可能只是捕捉了单个、充满噪声的稀疏基因的模式，实际上是隔离了少数随机表达该基因的细胞。这掩盖了基因间共变异的更微妙但更有意义的生物学模式，而这些模式对于理解系统生物学至关重要。缩放过程不成比例地放大了低均值、低方差特征的贡献，这些特征通常最不可靠且最受采样噪声影响，使它们看起来像是数据集中异质性的主要驱动因素。这种人为的膨胀是将小的非零计数值除以一个小的标准差的直接后果，而这个小的标准差本身又是计数数据中基本均值-方差耦合的直接后果。\n\n总而言之，缩放是有害的，因为它破坏了基因平均表达量与其方差之间的自然关系，对稀疏基因中罕见的、充满噪声的表达事件给予了不当的影响，这可能会主导并破坏主成分。", "answer": "$$\n\\boxed{1.381}\n$$", "id": "4397370"}, {"introduction": "本练习将引导你超越 PCA，进入专为单细胞计数数据这类具有独特统计特性的数据量身定制的高级方法。你将实现一个基于泊松分布的非负矩阵分解 (NMF) 主题模型，用以从基因表达计数中发现潜在的“生物学程序”。更重要的是，这个练习将模型拟合与生物学意义上的评估（基因本体论富集分析）相结合，搭建起从计算方法到生物学解释的桥梁 [@problem_id:4397356]。", "problem": "给定一个单细胞基因-细胞计数矩阵和一组以预定义基因集形式表示的基因本体论术语，这些基因集覆盖了相同的基因全集。您的任务是形式化并实现一个针对该计数矩阵的无监督主题模型，并评估一个源自每个主题顶部基因的基因本体论富集的一致性度量。\n\n从以下基本基础开始：在单细胞转录组学中，信使核糖核酸（mRNA）分子计数是离散的非负变量。对此类计数进行建模的一个经典假设是它们由泊松过程生成，其中对于每个基因和每个细胞，观察到的计数是来自泊松分布的一个实现，其率参数取决于潜在的生物程序。在基因表达的主题模型中，我们将这些潜在程序表示为非负因子，从而将期望率分解为非负贡献。\n\n形式上，设 $X \\in \\mathbb{R}_{\\ge 0}^{G \\times C}$ 为计数矩阵，包含 $G$ 个基因和 $C$ 个细胞，其中 $X_{gc}$ 是基因 $g$ 在细胞 $c$ 中的观察计数。设 $K$ 为潜在主题的数量。估计非负矩阵 $W \\in \\mathbb{R}_{\\ge 0}^{G \\times K}$ 和 $H \\in \\mathbb{R}_{\\ge 0}^{K \\times C}$，使得基因 $g$ 在细胞 $c$ 中的泊松率近似为 $\\lambda_{gc} = \\sum_{k=1}^{K} W_{gk} H_{kc}$。使用以下原则：在非负性约束下最大化关于 $W$ 和 $H$ 的泊松似然，等同于最小化 $X$ 和 $W H$ 之间的广义Kullback-Leibler散度。在估计 $W$ 和 $H$ 之后，为每个主题 $k$ 按 $W_{gk}$ 的降序定义基因排名，并将排名前 $T$ 的基因作为主题 $k$ 的 $T$ 个排名最高的基因。\n\n为了评估一致性，使用基因本体论（Gene Ontology, GO）术语对顶部基因进行富集分析，具体如下。设 $\\mathcal{U}$ 为所有 $G$ 个基因的全集，设 $\\mathcal{S} = \\{S_1, \\dots, S_J\\}$ 为一组GO术语，每个术语表示为 $\\mathcal{U}$ 的一个子集 $S_j \\subseteq \\mathcal{U}$。对于给定的主题 $k$，考虑排名前 $T$ 的基因与 $S_j$ 之间的重叠 $m_{kj}$。在零假设下，即排名前 $T$ 的基因是从 $\\mathcal{U}$ 中无放回地均匀随机选择的，重叠由超几何分布建模，其中总体大小 $M = G$，总体中成功个体的数量 $K_j = |S_j|$，抽取数量 $n = T$。计算 $m_{kj}$ 或更多重叠的单侧富集 $p$ 值，该值通过超几何尾部概率计算。将每个主题的一致性得分定义为 $-\\log_{10}(\\min_j p_{kj} + \\epsilon)$，其中 $\\epsilon$ 是一个很小的数，以避免对零取对数。将一个数据集的总体一致性定义为所有主题 $k = 1, \\dots, K$ 的算术平均值。\n\n请在单个程序中实现上述内容，并在以下确定性测试套件上进行评估。所有随机数生成必须使用指定的种子。本问题不涉及物理单位。所有输出均为实值浮点数。不使用角度。不使用百分比。\n\n测试套件定义：\n\n- 测试用例1（结构化主题与对齐的GO术语）：\n  - 基因全集：$G = 60$ 个基因，标识符为 $\\{ \\text{G}0, \\text{G}1, \\dots, \\text{G}59 \\}$。\n  - GO术语：三组，\n    - $\\text{GO\\_A} = \\{ \\text{G}0, \\dots, \\text{G}14 \\}$，\n    - $\\text{GO\\_B} = \\{ \\text{G}20, \\dots, \\text{G}34 \\}$，\n    - $\\text{GO\\_C} = \\{ \\text{G}40, \\dots, \\text{G}54 \\}$。\n  - 潜在主题：$K = 3$。\n  - 每个主题的顶部基因数：$T = 10$。\n  - 构建 $W_{\\text{true}} \\in \\mathbb{R}_{\\ge 0}^{60 \\times 3}$，其列归一化使其总和为 $1$，方法如下：对于每个主题 $k \\in \\{1,2,3\\}$，用全1初始化该列，然后为对应于其指定GO术语（主题1对应$\\text{GO\\_A}$，主题2对应$\\text{GO\\_B}$，主题3对应$\\text{GO\\_C}$）的条目加上 $5$，然后将该列归一化使其总和为 $1$。\n  - 构建 $H_{\\text{true}} \\in \\mathbb{R}_{\\ge 0}^{3 \\times 80}$，包含 $C = 80$ 个细胞，分为大小分别为 $27$、$27$ 和 $26$ 的三个连续块，分别分配给主题 $1$、$2$ 和 $3$。对于分配给主题 $t$ 的细胞 $c$，设置 $H_{\\text{true}}(t, c) = 200$ 和 $H_{\\text{true}}(k \\ne t, c) = 50$。\n  - 使用固定的伪随机种子 $7$ 生成计数 $X \\sim \\text{Poisson}(W_{\\text{true}} H_{\\text{true}})$。\n  - 使用与最大化泊松似然下的非负性约束一致的非负因式分解方法拟合主题模型，模型包含 $K=3$ 个主题，使用固定的伪随机种子 $1234$ 初始化，并运行 $300$ 次迭代。\n  - 使用 $\\epsilon = 10^{-300}$ 计算所述的总体一致性。\n\n- 测试用例2（弱结构）：\n  - 基因全集：与测试用例1相同。\n  - GO术语：与测试用例1相同。\n  - 用于拟合的潜在主题：$K = 3$。\n  - 每个主题的顶部基因数：$T = 10$。\n  - 对于 $G = 60$ 和 $C = 80$，通过独立抽取 $X_{gc} \\sim \\text{Poisson}(3)$ 生成计数矩阵 $X$，使用固定的伪随机种子 $17$。\n  - 使用与测试用例1中相同的程序拟合主题模型，使用固定的伪随机种子 $2024$ 和 $300$ 次迭代。\n  - 使用 $\\epsilon = 10^{-300}$ 计算所述的总体一致性。\n\n- 测试用例3（更小的系统，部分对齐）：\n  - 基因全集：$G = 30$ 个基因，标识符为 $\\{ \\text{G}0, \\text{G}1, \\dots, \\text{G}29 \\}$。\n  - GO术语：三组，\n    - $\\text{GO\\_D} = \\{ \\text{G}0, \\dots, \\text{G}9 \\}$，\n    - $\\text{GO\\_E} = \\{ \\text{G}10, \\dots, \\text{G}19 \\}$，\n    - $\\text{GO\\_F} = \\{ \\text{G}20, \\dots, \\text{G}29 \\}$。\n  - 潜在主题：$K = 2$。\n  - 每个主题的顶部基因数：$T = 5$。\n  - 构建 $W_{\\text{true}} \\in \\mathbb{R}_{\\ge 0}^{30 \\times 2}$，其列归一化使其总和为 $1$，方法如下：对于主题1，为对应$\\text{GO\\_D}$的条目加上 $5$；对于主题2，为对应$\\text{GO\\_E}$的条目加上 $5$。每列都从全1开始，然后归一化使其总和为 $1$。\n  - 构建 $H_{\\text{true}} \\in \\mathbb{R}_{\\ge 0}^{2 \\times 20}$，包含 $C = 20$ 个细胞：前 $10$ 个细胞分配给主题1，其余 $10$ 个分配给主题2。对于分配给主题 $t$ 的细胞 $c$，设置 $H_{\\text{true}}(t, c) = 150$ 和 $H_{\\text{true}}(k \\ne t, c) = 30$。\n  - 使用固定的伪随机种子 $29$ 生成计数 $X \\sim \\text{Poisson}(W_{\\text{true}} H_{\\text{true}})$。\n  - 使用与测试用例1中相同的程序拟合主题模型，模型包含 $K=2$ 个主题，使用固定的伪随机种子 $4321$ 初始化，并运行 $300$ 次迭代。\n  - 使用 $\\epsilon = 10^{-300}$ 计算所述的总体一致性。\n\n实现要求：\n\n- 您必须仅使用非负因子和与泊松似然一致的目标函数来实现主题模型估计，并且不得使用任何外部机器学习库。\n- 对于富集分析，在超几何模型下（总体大小 $M = G$，总体中成功个体数 $K_j = |S_j|$，样本大小 $n = T$），计算观察到至少 $m$ 个重叠的单侧尾部概率。\n- 最终输出格式必须是单行文本，包含三个测试用例的总体一致性得分，按顺序排列，形式为用方括号括起来的逗号分隔列表。每个分数必须四舍五入到六位小数。例如，输出可能看起来像 $[0.123456,0.234567,0.345678]$，但数值由您的实现确定。\n\n您的程序应产生单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，\"[result1,result2,result3]\"）。", "solution": "该问题要求实现和评估一个用于单细胞基因表达计数数据的无监督主题模型。每个测试用例的解决方案包括三个主要阶段：数据生成、模型拟合和一致性评估。\n\n### 1. 理论框架\n\n核心假设是，观察到的基因 $g$ 在细胞 $c$ 中的计数 $X_{gc}$ 是来自泊松分布的样本，即 $X_{gc} \\sim \\text{Poisson}(\\lambda_{gc})$。率参数 $\\lambda_{gc}$ 被建模为 $K$ 个潜在主题的线性组合。这通过率矩阵 $\\Lambda = (\\lambda_{gc})$ 的矩阵分解来表示。我们寻求找到两个非负矩阵，$W \\in \\mathbb{R}_{\\ge 0}^{G \\times K}$ 和 $H \\in \\mathbb{R}_{\\ge 0}^{K \\times C}$，使得它们的乘积近似于率矩阵：\n$$ \\Lambda = WH $$\n这里，$G$ 是基因数量，$C$ 是细胞数量，$K$ 是主题数量。矩阵 $W$ 是“主题-基因”矩阵，其中 $W_{gk}$ 代表基因 $g$ 在主题 $k$ 中的权重。矩阵 $H$ 是“细胞-主题”矩阵，其中 $H_{kc}$ 代表主题 $k$ 对细胞 $c$ 的贡献。\n\n### 2. 模型拟合：非负矩阵分解 (NMF)\n\n参数 $W$ 和 $H$ 是通过最大化观察数据 $X$ 在泊松模型下的对数似然来估计的。对数似然函数为：\n$$ \\mathcal{L}(W,H | X) = \\sum_{g=1}^{G} \\sum_{c=1}^{C} \\left( X_{gc} \\log(\\lambda_{gc}) - \\lambda_{gc} - \\log(X_{gc}!) \\right) $$\n代入 $\\lambda_{gc} = (WH)_{gc}$，并去掉常数项 $\\log(X_{gc}!)$，最大化对数似然等价于最小化以下目标函数：\n$$ D(W,H) = \\sum_{g=1}^{G} \\sum_{c=1}^{C} \\left( (WH)_{gc} - X_{gc} \\log((WH)_{gc}) \\right) $$\n这个目标与 $X$ 和 $WH$ 之间的广义Kullback-Leibler (KL) 散度密切相关：\n$$ D_{KL}(X || WH) = \\sum_{g=1}^{G} \\sum_{c=1}^{C} \\left( X_{gc} \\log\\frac{X_{gc}}{(WH)_{gc}} - X_{gc} + (WH)_{gc} \\right) $$\n最小化 $D_{KL}(X || WH)$ 等价于最大化泊松对数似然。解决这个非负矩阵分解问题的一个标准算法是基于乘法更新，它能保证非负性并收敛到局部最小值。$W$ 和 $H$ 的更新规则源自目标函数的梯度：\n$$ H_{kc} \\leftarrow H_{kc} \\frac{\\sum_{g=1}^G W_{gk} \\frac{X_{gc}}{(WH)_{gc}}}{\\sum_{g=1}^G W_{gk}} $$\n$$ W_{gk} \\leftarrow W_{gk} \\frac{\\sum_{c=1}^C H_{kc} \\frac{X_{gc}}{(WH)_{gc}}}{\\sum_{c=1}^C H_{kc}} $$\n这些更新从随机初始化的非负矩阵 $W$ 和 $H$ 开始迭代执行。为了数值稳定性，在分母和矩阵乘积 $WH$ 中添加一个小的正常数 $\\delta$ 以防止除以零。该过程运行固定的迭代次数。\n\n### 3. 一致性评估\n\n模型拟合后，评估所学主题的生物学相关性。这是通过测量每个主题相对于一组已知基因注释（在此案例中是基因本体论（GO）术语）的“一致性”来完成的。\n\n**基因排序：** 对于每个主题 $k$，根据基因在 $W$ 矩阵相应列 $W_{:k}$ 中的权重，对基因进行降序排序。选择排名前 $T$ 的基因集合进行进一步分析。\n\n**富集分析：** 主题的一致性通过其排名前 $T$ 的基因集对于任何预定义GO术语的统计富集来量化。零假设是，排名前 $T$ 的基因是从 $G$ 个基因的全集中无放回地随机抽取的样本。在此零假设下，一个主题的排名前 $T$ 列表与一个GO术语集 $S_j$ 之间的重叠基因数量遵循超几何分布。\n\n超几何检验的参数是：\n- 总体大小，$M$：总基因数，$G$。\n- 总体中成功个体的数量，$K_j$：GO术语集的大小，$|S_j|$。\n- 样本大小，$n$：选择的顶部基因数量，$T$。\n- 观察到的成功次数，$m_{kj}$：主题 $k$ 的排名前 $T$ 基因集与GO集 $S_j$ 的交集中的基因数量。\n\n单侧p值 $p_{kj}$ 是偶然观察到 $m_{kj}$ 或更多重叠的概率：\n$$ p_{kj} = P(\\text{overlap} \\ge m_{kj}) = \\sum_{i=m_{kj}}^{\\min(T, |S_j|)} \\frac{\\binom{|S_j|}{i} \\binom{G-|S_j|}{T-i}}{\\binom{G}{T}} $$\n这是使用超几何分布的生存函数（sf）计算的。\n\n**一致性得分：** 对于每个主题 $k$，我们通过取所有GO术语 $S_j$ 的最小p值来找到最显著的富集：$p_{k, \\min} = \\min_j p_{kj}$。主题 $k$ 的一致性得分则定义为：\n$$ C_k = -\\log_{10}(p_{k, \\min} + \\epsilon) $$\n小常数 $\\epsilon$（给定为 $10^{-300}$）防止对零取对数。得分越高表明主题的一致性越好。\n\n最后，整个模型的总体一致性是各个主题一致性得分的算术平均值：\n$$ \\bar{C} = \\frac{1}{K} \\sum_{k=1}^K C_k $$\n\n### 4. 测试用例的实现\n根据上述方法处理指定的测试用例。\n1.  **数据生成**：对于每个测试用例，根据指定的参数（结构化或随机）和随机数生成器种子生成计数矩阵 $X$。\n2.  **模型拟合**：将 `nmf_kl` 函数应用于生成的矩阵 $X$，使用指定的主题数 $K$、初始化种子和迭代次数，得到估计的 $W$ 和 $H$ 矩阵。\n3.  **一致性计算**：`calculate_coherence` 函数接受估计的 $W$ 矩阵、GO术语定义以及其他参数（$T, G, \\epsilon$）来计算最终的总体一致性得分。\n对所有三个测试用例重复此过程，并按要求格式化结果。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import hypergeom\n\n# A small constant for numerical stability in NMF updates.\nNMF_EPS = 1e-9\n\ndef nmf_kl(X, n_components, seed, max_iter=300):\n    \"\"\"\n    Performs Non-negative Matrix Factorization using Kullback-Leibler\n    divergence with multiplicative updates.\n\n    Args:\n        X (np.ndarray): The input data matrix (G x C).\n        n_components (int): The number of topics (K).\n        seed (int): The seed for random initialization.\n        max_iter (int): The number of update iterations.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: The estimated W and H matrices.\n    \"\"\"\n    G, C = X.shape\n    rng = np.random.default_rng(seed)\n    \n    # Initialize W and H with random positive values scaled to prevent\n    # large initial lambda values destroying the count matrix structure.\n    W = rng.random((G, n_components), dtype=np.float64)\n    H = rng.random((n_components, C), dtype=np.float64)\n    \n    for _ in range(max_iter):\n        # Update rule for H\n        Lambda = W @ H + NMF_EPS\n        H_numerator = W.T @ (X / Lambda)\n        H_denominator = W.sum(axis=0)[:, np.newaxis]\n        H *= (H_numerator / (H_denominator + NMF_EPS))\n        \n        # Update rule for W\n        Lambda = W @ H + NMF_EPS\n        W_numerator = (X / Lambda) @ H.T\n        W_denominator = H.sum(axis=1) # Shape (K,) broadcast to (G, K)\n        W *= (W_numerator / (W_denominator + NMF_EPS))\n        \n    return W, H\n\ndef calculate_coherence(W, go_sets, T, G, epsilon):\n    \"\"\"\n    Calculates the overall coherence score based on GO term enrichment.\n\n    Args:\n        W (np.ndarray): The topic-gene matrix (G x K).\n        go_sets (list[set]): A list of GO terms, each a set of gene indices.\n        T (int): The number of top genes to consider per topic.\n        G (int): The total number of genes in the universe.\n        epsilon (float): A small constant for the log calculation.\n\n    Returns:\n        float: The mean coherence score over all topics.\n    \"\"\"\n    K = W.shape[1]\n    topic_coherences = []\n    \n    for k in range(K):\n        # Get top T genes for topic k\n        top_gene_indices = np.argsort(W[:, k])[::-1][:T]\n        \n        min_p_value = 1.0\n        \n        for go_set in go_sets:\n            K_j = len(go_set)  # Population successes\n            m_kj = len(set(top_gene_indices)  go_set)  # Observed successes\n            \n            # Hypergeometric p-value for m_kj or more overlaps\n            # P(X >= m_kj) = 1 - P(X = m_kj - 1) = sf(m_kj - 1)\n            # M = G (population size), n = K_j (successes in pop), N = T (sample size)\n            if m_kj > 0:\n                p_val = hypergeom.sf(m_kj - 1, G, K_j, T)\n            else:\n                p_val = 1.0\n\n            if p_val  min_p_value:\n                min_p_value = p_val\n                \n        topic_score = -np.log10(min_p_value + epsilon)\n        topic_coherences.append(topic_score)\n        \n    return np.mean(topic_coherences)\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Test case 1\n        {\n            \"G\": 60, \"C\": 80, \"K\": 3, \"T\": 10,\n            \"go_sets\": [set(range(15)), set(range(20, 35)), set(range(40, 55))],\n            \"data_gen\": {\n                \"type\": \"structured\", \"seed\": 7,\n                \"W_spec\": [(set(range(15)), 0), (set(range(20, 35)), 1), (set(range(40, 55)), 2)],\n                \"H_spec\": [(range(0, 27), 0, 200, 50), (range(27, 54), 1, 200, 50), (range(54, 80), 2, 200, 50)]\n            },\n            \"fit_params\": {\"seed\": 1234, \"iter\": 300}, \"epsilon\": 1e-300\n        },\n        # Test case 2\n        {\n            \"G\": 60, \"C\": 80, \"K\": 3, \"T\": 10,\n            \"go_sets\": [set(range(15)), set(range(20, 35)), set(range(40, 55))],\n            \"data_gen\": {\"type\": \"random\", \"seed\": 17, \"lambda\": 3},\n            \"fit_params\": {\"seed\": 2024, \"iter\": 300}, \"epsilon\": 1e-300\n        },\n        # Test case 3\n        {\n            \"G\": 30, \"C\": 20, \"K\": 2, \"T\": 5,\n            \"go_sets\": [set(range(10)), set(range(10, 20)), set(range(20, 30))],\n            \"data_gen\": {\n                \"type\": \"structured\", \"seed\": 29,\n                \"W_spec\": [(set(range(10)), 0), (set(range(10, 20)), 1)],\n                \"H_spec\": [(range(0, 10), 0, 150, 30), (range(10, 20), 1, 150, 30)]\n            },\n            \"fit_params\": {\"seed\": 4321, \"iter\": 300}, \"epsilon\": 1e-300\n        }\n    ]\n    \n    results = []\n    for case in test_cases:\n        G, C, K, T = case[\"G\"], case[\"C\"], case[\"K\"], case[\"T\"]\n        go_sets = case[\"go_sets\"]\n        data_gen_params = case[\"data_gen\"]\n        \n        # --- Data Generation ---\n        rng_data = np.random.default_rng(data_gen_params[\"seed\"])\n        if data_gen_params[\"type\"] == \"structured\":\n            W_true = np.ones((G, K), dtype=np.float64)\n            for gene_set, k_idx in data_gen_params[\"W_spec\"]:\n                for g_idx in gene_set:\n                    W_true[g_idx, k_idx] += 5\n            W_true /= W_true.sum(axis=0)\n            \n            H_true = np.zeros((K, C), dtype=np.float64)\n            for cell_range, k_idx, high_val, low_val in data_gen_params[\"H_spec\"]:\n                for k_iter in range(K):\n                    H_true[k_iter, cell_range] = low_val\n                H_true[k_idx, cell_range] = high_val\n            \n            Lambda = W_true @ H_true\n            X = rng_data.poisson(Lambda)\n        elif data_gen_params[\"type\"] == \"random\":\n            X = rng_data.poisson(data_gen_params[\"lambda\"], size=(G, C))\n            \n        # --- Topic Model Fitting ---\n        fit_params = case[\"fit_params\"]\n        W_fit, _ = nmf_kl(X, K, fit_params[\"seed\"], fit_params[\"iter\"])\n        \n        # --- Coherence Calculation ---\n        coherence = calculate_coherence(W_fit, go_sets, T, G, case[\"epsilon\"])\n        results.append(coherence)\n        \n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "4397356"}]}