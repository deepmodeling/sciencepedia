## 引言
在系统生物医学时代，我们正面临着由高通量测序技术带来的数据洪流。从基因组到[转录组](@entry_id:274025)，这些[高维数据](@entry_id:138874)集蕴含着揭示生命过程、疾病机理和治疗靶点的关键信息。然而，其巨大的复杂性和维度也构成了一道知识发现的鸿沟：我们如何才能从看似杂乱无章的数据中，识别出有意义的生物学模式，例如协同调控的基因模块或隐藏的疾病亚型？

本文旨在系统性地解决这一挑战，聚焦于[聚类分析](@entry_id:637205)与热图可视化这一对在模式发现中相辅相成的强大工具。我们将带领读者穿越理论的丛林，深入实践的前沿，全面掌握从原始数据中提取、验证并解释生物学模式的核心技能。

为此，文章将分为三个章节逐步展开。在**第一章“原理与机制”**中，我们将奠定坚实的基础，从定义数据点间的“相似性”出发，详细剖析k-means、[层次聚类](@entry_id:268536)等核心算法的内在逻辑，并探讨高维数据带来的独特挑战。接下来，在**第二章“应用与交叉学科联系”**中，我们将理论付诸实践，通过[单细胞分析](@entry_id:274805)、[基因共表达网络](@entry_id:267805)构建等真实案例，展示这些方法如何在一个完整的分析流程中被整合与应用。最后，**第三章“动手实践”**将提供一系列精心设计的问题，引导您亲手操作，巩固所学知识。

通过本篇文章的学习，您将不仅理解“如何”使用这些工具，更将领会“为何”如此选择，从而在未来的科研工作中更加自信地驾驭数据，发现新知。让我们从聚类的最基本原理开始探索。

## 原理与机制

在本章中，我们将深入探讨在系统生物医学中发现模式所使用的核心计算方法——[聚类分析](@entry_id:637205)的原理与机制。[聚类分析](@entry_id:637205)旨在将数据对象根据其相似性划分为不同的组或“簇”。一个成功的[聚类分析](@entry_id:637205)能够揭示高维数据中固有的结构，例如在热图（heatmap）中将具有相似表达模式的基因和样本聚合在一起，从而发现潜在的生物学子类型或共调控模块。我们将从定义数据点之间的相似性出发，逐步介绍处理高维生物数据所面临的挑战，然后详细阐述几种关键的[聚类算法](@entry_id:146720)，最后讨论如何通[过热](@entry_id:147261)图可视化和聚类稳定性评估来解释和验证所发现的模式。

### 聚类的基础：距离与相似性

所有[聚类算法](@entry_id:146720)的核心思想都是将“相似”的对象归为一类，将“不相似”的对象分到不同类。因此，如何定量地定义“相似性”或“不相似性”（即距离）是[聚类分析](@entry_id:637205)的第一步。选择合适的[距离度量](@entry_id:636073)对于最终结果的生物学意义至关重要。

#### [距离度量](@entry_id:636073)：闵可夫斯基距离族

在多维空间中，我们通常用距离函数来衡量样本或基因表达谱之间的不相似性。一个广泛使用的[距离度量](@entry_id:636073)家族是**闵可夫斯基距离**（Minkowski distance），也称为 $L_p$ 范数。对于两个分别由向量 $x, y \in \mathbb{R}^n$ 表示的样本（其中 $n$ 是基因数量），它们之间的闵可夫斯基距离定义为：

$d_p(x, y) = \left(\sum_{i=1}^n |x_i - y_i|^p\right)^{1/p}$

其中 $p \ge 1$。通过选择不同的 $p$ 值，我们可以得到具有不同性质的[距离度量](@entry_id:636073)。

*   **[曼哈顿距离](@entry_id:141126) (Manhattan Distance, $p=1$)**: 当 $p=1$ 时，距离是各坐标差值绝对值之和，$d_1(x, y) = \sum_{i=1}^n |x_i - y_i|$。它也被称为“城市街区距离”。由于它对所有坐标的差异进行线性求和，因此对于个别基因的极端差异（即坐标上的离群值）相对不敏感。当生物学信号被认为是由许多基因的微小到中等程度的变化累积而成时，[曼哈顿距离](@entry_id:141126)是一个稳健的选择。

*   **欧几里得距离 (Euclidean Distance, $p=2$)**: 当 $p=2$ 时，我们得到最直观的“直线距离”，$d_2(x, y) = \left(\sum_{i=1}^n (x_i - y_i)^2\right)^{1/2}$。这是[聚类分析](@entry_id:637205)中最常用的[距离度量](@entry_id:636073)。然而，由于它对坐标差值进行平方求和，所以一个大的差值（离群值）会对总距离产生不成比例的巨大影响。

*   **[切比雪夫距离](@entry_id:174938) (Chebyshev Distance, $p \to \infty$)**: 当 $p$ 趋于无穷大时，闵可夫斯基距离收敛于[切比雪夫距离](@entry_id:174938)，即所有坐标差值中的最大值，$d_\infty(x, y) = \max_i |x_i - y_i|$。这个度量完全由单个差异最大的坐标决定，因此对离群值最为敏感。

在实践中，对 $p$ 值的选择反映了我们对数据中“显著差异”的假设 [@problem_id:4328392]。随着 $p$ 的增加，距离计算会越来越被少数几个具有极大差异的基因（或特征）所主导。例如，使用[欧几里得距离](@entry_id:143990)（$p=2$）或更高的 $p$ 值进行聚类，往往会使结果偏向于根据少数几个“标记基因”的极端表达变化来对样本进行分组，这在热图上可能表现为围绕这些标记基因形成的清晰模块结构。相反，使用[曼哈顿距离](@entry_id:141126)（$p=1$）则更能容忍个别基因的极端噪声，从而能够更好地揭示由多个基因协同产生的整体表达谱变化。

#### 关联度量：[基于相关的距离](@entry_id:172255)

在[基因表达分析](@entry_id:138388)中，我们常常更关心基因表达谱的“形状”或“模式”是否相似，而非其绝对表达值的差异。例如，两个基因即使绝对表达水平相差很大，但如果在不同样本间的表达趋势（上调或下调）完全一致，我们通常认为它们是“共表达”的。为了捕捉这种关联性，我们使用**皮尔逊相关系数 (Pearson Correlation Coefficient, PCC)** 作为相似性的度量。对于两个基因的表达向量 $x, y \in \mathbb{R}^n$，PCC 定义为：

$\rho(x,y) = \frac{\mathrm{cov}(x,y)}{\sigma_{x}\sigma_{y}}$

其中 $\mathrm{cov}(x,y)$ 是协方差，$\sigma_x$ 和 $\sigma_y$ 是标准差。PCC 的取值范围在 $[-1, 1]$ 之间，其中 $1$ 表示完全正相关，$0$ 表示无[线性相关](@entry_id:185830)，$-1$ 表示完全负相关。

为了将其转化为一个[距离度量](@entry_id:636073)（高相关性对应小距离），一个常见的定义是**[相关距离](@entry_id:634939) (correlation distance)**：$d_{\text{corr}}(x,y) = 1 - \rho(x,y)$。这个距离的取值范围是 $[0, 2]$。

有趣的是，[相关距离](@entry_id:634939)与标准化后的欧几里得距离有直接的数学联系 [@problem_id:4328359]。如果我们首先对每个基因的表达向量进行 z-score 标准化（使其均值为0，标准差为1），得到标准化向量 $z_x$ 和 $z_y$，那么它们之间的[欧几里得距离](@entry_id:143990)的平方为：

$d_E^2(z_x, z_y) = \|z_x - z_y\|_2^2 = \|z_x\|_2^2 + \|z_y\|_2^2 - 2 (z_x \cdot z_y) = 2(n-1)(1 - \rho(x,y))$

这表明，在标准化数据上使用欧几里得距离进行聚类，其结果的排序与使用[相关距离](@entry_id:634939)是等价的。这种[距离度量](@entry_id:636073)对于[线性变换](@entry_id:143080)是不变的。例如，如果两个基因的表达满足 $y = ax + b$ 且 $a > 0$ 的关系，它们的皮尔逊相关系数为 $1$，[相关距离](@entry_id:634939)为 $0$，这意味着它们在聚类中会被视为最近邻，这完美地捕捉了共表达的概念。然而，[皮尔逊相关](@entry_id:260880)对于非线性变换或某些复杂的[批次效应](@entry_id:265859)（如样本特异性的[乘性](@entry_id:187940)效应）则不是不变的 [@problem_id:4328359]。

#### 高级[距离度量](@entry_id:636073)：马氏距离

[欧几里得距离](@entry_id:143990)和[相关距离](@entry_id:634939)都有一个共同的局限性：它们要么假设特征之间是独立的（欧几里得），要么只考虑成对关系（相关）。**马氏距离 (Mahalanobis Distance)** 提供了一种更强大的方式来度量距离，它考虑了特征（例如基因）之间的完整协方差结构。其定义为：

$d_M(x,y) = \sqrt{(x-y)^\top \Sigma^{-1}(x-y)}$

其中 $\Sigma$ 是特征的协方差矩阵。马氏距离可以被理解为在经过“[白化变换](@entry_id:637327)”（即通过乘以 $\Sigma^{-1/2}$ 来移除特征间的相关性并使方差归一化）后的空间中计算的[欧几里得距离](@entry_id:143990) [@problem_id:4328338] [@problem_id:4328329]。通过这种方式，它能够对数据进行缩放和旋转，使得沿着方差较大方向的差异被压缩，而沿着方差较小方向的差异被放大，从而给出一种不受坐标系尺度和相关性影响的[距离度量](@entry_id:636073)。

### 高维组学数据的挑战

现代系统生物医学研究，如RNA测序（[RNA-seq](@entry_id:140811)），通常会产生具有极高维度的数据。一个典型的数据集可能包含数万个基因（特征 $p$）但只有几十个或几百个样本（$n$）。这种 $p \gg n$ 的情况给[聚类分析](@entry_id:637205)带来了独特的挑战。

#### [数据预处理](@entry_id:197920)与归一化

在进行任何[聚类分析](@entry_id:637205)之前，原始组学数据必须经过仔细的预处理和归一化，以消除技术性偏差并使数据适用于距离计算。一个典型的RNA-seq[数据预处理](@entry_id:197920)流程包括 [@problem_id:4328354]：

1.  **库大小归一化**: 原始的[RNA-seq](@entry_id:140811)计数数据受[测序深度](@entry_id:178191)（库大小）的影响。必须通过除以一个样本特异性的缩放因子来调整，使得基因表达水平在不同样本之间具有可比性。
2.  **方差稳定化变换**: 计数数据的方差通常与其均值相关（即表达越高的基因，其计数的方差也越大）。这违反了许多统计方法的假设。因此，需要应用一个变换（如[对数变换](@entry_id:267035)或更复杂的`vst`变换）来使方差在整个表达范围内近似恒定。
3.  **标准化 (Z-scoring)**: 为了防止少数高表达或高方差的基因主导距离计算，通常会对每个基因在所有样本中的表达值进行z-score标准化，即减去均值并除以标准差。这使得每个基因的表达谱都具有零均值和单位方差，确保了所有基因在聚类时具有平等的贡献权重。

#### “[维度灾难](@entry_id:143920)”与距离集中现象

在高维空间中，我们关于距离的直觉往往会失效。一个被称为“**[维度灾难](@entry_id:143920) (curse of dimensionality)**”的现象是**距离集中 (distance concentration)**。随着维度 $p$ 的增加，任意两点之间的距离趋于变得彼此相等 [@problem_id:4328354]。从数学上讲，对于许多常见的数据分布，成对平方欧几里得距离的[变异系数](@entry_id:272423)（标准差与均值的比值）会以 $p^{-1/2}$ 的速率递减。这意味着，当 $p$ 变得非常大时，所有点对之间的距离分布会变得非常窄，导致“最近邻”和“最远邻”之间的距离差异变得微乎其微。这种对比度的丧失使得基于距离的[聚类算法](@entry_id:146720)（如k-means或[层次聚类](@entry_id:268536)）难以有效区分真正的簇结构。包含大量不提供信息的“噪声”基因会极大地加剧这一问题。

#### 使用[主成分分析](@entry_id:145395)（PCA）进行降维

应对维度灾难的一个主要策略是**降维 (dimensionality reduction)**，而**[主成分分析](@entry_id:145395) (Principal Component Analysis, PCA)** 是其中最常用的技术。PCA的目标是找到一组新的[正交坐标](@entry_id:166074)轴（称为主成分），使得数据在这些轴上的投影方差被最大化。

从数学上讲，PCA通过对数据的协方差矩阵 $S = \frac{1}{n-1} X^{\top} X$ 进行[特征值分解](@entry_id:272091)来找到这些方向 [@problem_id:4328369]。第一个主成分（PC1）的方向是与 $S$ 的最大特征值 $\lambda_1$ 相关联的特征向量 $v_1$。数据在该方向上的投影方差恰好等于 $\lambda_1$。第二个主成分（PC2）是在与PC1正交的所有方向中，使投影方差最大的方向，它对应于第二大特征值 $\lambda_2$ 的特征向量 $v_2$，其解释的方差为 $\lambda_2$。以此类推，第 $k$ 个主成分解释的方差等于第 $k$ 大的特征值 $\lambda_k$。

通过保留前 $k$ 个主成分，我们可以将原始的 $p$ 维数据投影到一个低维的 $k$ 维空间中，同时保留了数据中最大部分的方差。总方差是所有特征值之和，$\text{Total Variance} = \sum_{i=1}^{p} \lambda_i$。因此，前 $k$ 个主成分所解释的[方差比](@entry_id:162608)例为 $(\sum_{i=1}^{k} \lambda_i) / (\sum_{i=1}^{p} \lambda_i)$。例如，在一个包含5个模块的研究中，如果协方差矩阵的特征值为 $4.2, 3.1, 0.9, 0.6, 0.2$，那么总方差为 $9.0$，前两个主成分解释的[方差比](@entry_id:162608)例为 $(4.2 + 3.1) / 9.0 = 7.3 / 9.0 = 73/90$ [@problem_id:4328369]。

值得注意的是，一个常见的误解是PCA在 $p>n$ 时无效。实际上，PCA（通常通过[奇异值分解](@entry_id:138057)SVD实现）对于任何形状的矩阵都是明确定义的，并且正是在 $p \gg n$ 的高维场景中，它作为一种[降维](@entry_id:142982)和探索性分析工具发挥着至关重要的作用 [@problem_id:4328354]。

#### 协方差矩阵的估计问题

虽然[马氏距离](@entry_id:269828)在理论上非常吸引人，但在 $p \gg n$ 的情况下，它的应用面临一个严重障碍：协方差矩阵 $\Sigma$ 的估计和求逆。

当我们使用样本协方差矩阵 $S$ 作为 $\Sigma$ 的估计时，由于数据矩阵 $X$ 的秩最多为 $n-1$，在 $p > n-1$ 的情况下，$S$ 是一个**[奇异矩阵](@entry_id:148101)**（非满秩），因此不可逆 [@problem_id:4328354] [@problem_id:4328329]。这意味着无法直接计算 $S^{-1}$，标准的[马氏距离](@entry_id:269828)也就无法定义。

虽然可以使用**摩尔-彭若斯伪逆 (Moore-Penrose pseudoinverse)** $S^+$ 来代替 $S^{-1}$，但这种方法在统计上性质不佳。$S^+$ 是一个有偏且高度不稳定的估计量，它会过度放大由样本变异性引起的微小特征值，导致距离计算结果不可靠 [@problem_id:4328329]。

一个更稳健和现代的解决方案是**收缩正则化 (shrinkage regularization)**。其思想是构建一个更好的协方差矩阵估计量 $S_{\text{shrink}}$，它是一个经验协方差矩阵 $S$ 和一个结构简单、性质良好的“目标矩阵” $T$（如一个标量的[单位矩阵](@entry_id:156724) $\nu I$）的加权平均（[凸组合](@entry_id:635830)）：

$S_{\text{shrink}} = (1-\lambda)S + \lambda T$

其中 $\lambda \in (0, 1]$ 是收缩强度。这个[收缩估计量](@entry_id:171892)有几个优点 [@problem_id:4328329]：
*   **非奇异性**: 只要 $\lambda > 0$ 且 $T$ 是正定的，$S_{\text{shrink}}$ 就保证是正定且可逆的。
*   **稳定性**: 收缩通过将不稳定的经验特征值“拉向”一个稳定的目标，减小了估计量的方差。这在统计上对应于经典的偏倚-方差权衡：我们引入了少量偏倚，以换取方差的大幅降低，从而减小了总体的均方误差。
*   **插值作用**: 收缩强度 $\lambda$ 控制着[距离度量](@entry_id:636073)的性质。当 $\lambda \to 1$ 时，$S_{\text{shrink}} \to T = \nu I$，[马氏距离](@entry_id:269828)退化为（缩放的）[欧几里得距离](@entry_id:143990)。当 $\lambda \to 0$ 时，它接近于由经验协方差定义的（不稳定的）马氏距离。
通过选择一个最优的 $\lambda$（例如，通过最小化协方差矩阵的预期样本外误差的估计），我们可以获得一个在不同数据重抽样（如[自助法](@entry_id:139281)）中更稳定、更可复现的[距离度量](@entry_id:636073)，从而提高聚类结果的可靠性 [@problem_id:4328329]。

### 主要的[聚类算法](@entry_id:146720)及其机制

在建立了[距离度量](@entry_id:636073)和处理高维性的策略后，我们现在可以介绍几种主流的[聚类算法](@entry_id:146720)。

#### 划分式聚类：k-means

**k-means** 是一种最著名和最广泛使用的划分式[聚类算法](@entry_id:146720)。它的目标是将 $n$ 个数据点划分到预先指定的 $k$ 个互不重叠的簇中。

k-means 的核心是最小化一个目标函数，即**簇内平方和 (Within-Cluster Sum of Squares, WCSS)**，通常记为 $J$ [@problem_id:4328401]。对于一个给定的聚类结果，其中样本 $i$ 被分配到簇 $c(i)$，该簇的[质心](@entry_id:138352)为 $\mu_{c(i)}$，WCSS 定义为：

$J = \text{WCSS} = \sum_{i=1}^n \|x_i - \mu_{c(i)}\|^2$

这个目标函数衡量了每个数据点到其所属簇[质心](@entry_id:138352)的距离平方之和。一个好的聚类应该使得簇内的数据点紧密地聚集在它们的[质心](@entry_id:138352)周围，即具有较小的WCSS。

k-means 算法通过一个简单的迭代过程来优化这个目标函数：
1.  **初始化**: 随机选择 $k$ 个点作为初始[质心](@entry_id:138352)。
2.  **分配步骤**: 将每个数据点分配给离它最近的[质心](@entry_id:138352)所在的簇。
3.  **更新步骤**: 重新计算每个簇的[质心](@entry_id:138352)，即簇内所有点的算术平均值。这一步恰好是能使固定分配下的 WCSS 最小化的选择 [@problem_id:4328401]。
4.  **重复**: 重复步骤2和3，直到簇的分配不再改变或达到最大迭代次数。

根据方差分解定理（TSS = WCSS + BSS），数据集的总平方和 (TSS) 是一个常数，它等于簇内平方和 (WCSS) 与**簇间平方和 (Between-Cluster Sum of Squares, BSS)** 之和。因此，最小化WCSS等价于最大化BSS [@problem_id:4328401]。BSS衡量了各簇[质心](@entry_id:138352)相对于全局数据中心的离散程度。在[热图](@entry_id:273656)上，一个具有较小WCSS和较大BSS的聚类结果会直观地表现为：簇内样本的表达模式高度相似（形成均匀的颜色块），而不同簇之间的表达模式则有明显差异（颜色块之间有清晰的对比） [@problem_id:4328401]。

尽管k-means简单高效，但它也有局限性：(1) 结果对初始[质心](@entry_id:138352)的选择敏感；(2) 算法仅保证收敛到局部最优解，而非全局最优解（k-means的优化问题是[NP难](@entry_id:264825)的）；(3) 它倾向于发现球状且大小相似的簇；(4) 它的标准形式基于[欧几里得距离](@entry_id:143990)，如果需要使用其他[距离度量](@entry_id:636073)（如[曼哈顿距离](@entry_id:141126)），[质心](@entry_id:138352)的更新规则也需要相应改变（例如，对于[曼哈顿距离](@entry_id:141126)，[质心](@entry_id:138352)应为坐标中位数，即k-medians算法）[@problem_id:4328401]。

#### 层次化聚类

与k-means产生单一划分不同，**层次化聚类 (Hierarchical Clustering)** 构建一个嵌套的簇的层次结构，并用一个称为**[树状图](@entry_id:266792) (dendrogram)** 的树形图来表示。这种方法不需要预先指定簇的数量 $k$。

层次化聚类主要有两种策略 [@problem_id:4328381]：
*   **凝聚式 (Agglomerative)**: 一种“自下而上”的方法。开始时，每个数据点都是一个独立的簇。在每一步，算法会合并最相似（距离最近）的两个簇，直到所有点都合并成一个簇。
*   **分裂式 (Divisive)**: 一种“自上而下”的方法。开始时，所有数据点都在一个大簇中。在每一步，算法会将一个现有的簇分裂成两个最不相似的子簇，直到每个点都成为一个独立的簇。

在生物信息学中，凝聚式[层次聚类](@entry_id:268536)（HAC）更为常用。HAC的关键在于如何定义两个簇（而不仅仅是两个点）之间的距离。这个定义被称为**[连接准则](@entry_id:634279) (linkage criterion)**。以下是几种常见的[连接准则](@entry_id:634279) [@problem_id:4328381]：

*   **[单连接](@entry_id:635417) (Single Linkage)**: 簇间距离定义为两个簇中最近的两个点之间的距离。$D_{\text{single}}(A,B) = \min_{i \in A, j \in B} d(i,j)$。
*   **全连接 (Complete Linkage)**: 簇间距离定义为两个簇中最远的两个点之间的距离。$D_{\text{complete}}(A,B) = \max_{i \in A, j \in B} d(i,j)$。
*   **平均连接 (Average Linkage, [UPGMA](@entry_id:172615))**: 簇间距离定义为两个簇中所有点对之间距离的平均值。$D_{\text{average}}(A,B) = \frac{1}{|A||B|} \sum_{i \in A} \sum_{j \in B} d(i,j)$。
*   **[质心](@entry_id:138352)连接 (Centroid Linkage)**: 簇间距离定义为两个簇的[质心](@entry_id:138352)之间的距离。$D_{\text{centroid}}(A,B) = \|\bar{x}_A - \bar{x}_B\|_2$。
*   **[沃德方法](@entry_id:636890) (War[d'](@entry_id:189153)s Method)**: 合并那对使得总簇内平方和（WCSS）增加最小的簇。其合并成本 $\Delta(A,B)$ 可以表示为 $\frac{|A||B|}{|A|+|B|} \|\bar{x}_A - \bar{x}_B\|_2^2$。

不同的[连接准则](@entry_id:634279)会导致截然不同的聚类结果。一个经典的例子可以说明这一点 [@problem_id:4328408]。想象一下，我们有两个紧凑的点群，它们之间由一条稀疏的点链连接。使用**[单连接](@entry_id:635417)**时，由于它只关心最近的点，算法会沿着这条“链”一步步地将两个紧凑的[点群](@entry_id:142456)连接起来，形成一个长长的、延伸的簇。这种现象被称为“**链接效应 (chaining effect)**”。而**全连接**则要求簇中所有点都相对靠近，因此它会倾向于发现紧凑的、球状的簇，并且会把那条点链断开成几个小簇，而不会将两个远处的紧凑点群合并。在[热图](@entry_id:273656)中，这两种方法产生的行或列的排序会截然不同，[单连接](@entry_id:635417)可能产生一个长而弥散的对角线信号，而全连接则会产生几个清晰、分离的方形模块。

#### 基于密度的聚类：DBSCAN

k-means和[层次聚类](@entry_id:268536)都难以处理形状不规则的簇和识别噪声点。**DBSCAN (Density-Based Spatial Clustering of Applications with Noise)** 是一种基于密度的算法，它将簇定义为空间中被低密度区域分隔开的稠密区域。

DBSCAN的核心思想基于两个参数：邻域半径 $\epsilon$ 和密度阈值 `minPts`。根据这两个参数，数据点被分为三类 [@problem_id:4328338]：
*   **[核心点](@entry_id:636711) (Core Point)**: 在其半径为 $\epsilon$ 的邻域内，至少包含 `minPts` 个点（包括自身）的点。
*   **[边界点](@entry_id:176493) (Border Point)**: 不是[核心点](@entry_id:636711)，但落在某个[核心点](@entry_id:636711)的 $\epsilon$ 邻域内的点。
*   **噪声点 (Noise Point)**: 既不是[核心点](@entry_id:636711)也不是[边界点](@entry_id:176493)的点。

DBSCAN的算法过程如下：从任意一个未访问过的点开始，如果它是[核心点](@entry_id:636711)，就以它为种子创建一个新簇，并将其所有密度可达（即通过一连串[核心点](@entry_id:636711)连接）的点都加入该簇。如果一个点是噪声点，则暂时忽略它。这个过程重复进行，直到所有点都被访问。

DBSCAN的优势在于它能够发现任意形状的簇，并且能够明确地将噪声点识别出来，而无需将它们强行归入任何一个簇。它也不需要预先指定簇的数量。例如，在一个二维路径活性得分的数据中，我们可以使用带协方差矩阵的马氏距离作为[距离度量](@entry_id:636073)，通过计算每个样本[点的邻域](@entry_id:144055)大小来判断其类别（核心、边界或噪声），从而发现由数据密度自然定义的样本亚群 [@problem_id:4328338]。

### 从聚类到模式：[热图](@entry_id:273656)与验证

[聚类算法](@entry_id:146720)本身只是工具，其最终目的是帮助我们从数据中发现有意义的生物学模式。[热图](@entry_id:273656)可视化和聚类稳定性验证是实现这一目标的关键环节。

#### 基于热图的模式发现

**[热图](@entry_id:273656) (Heatmap)** 是生物信息学中最具标志性的可视化工具之一。它通过颜色的变化来展示数据矩阵中的数值大小。然而，一张热图的威力并不在于其颜色，而在于其**行和列的排序**。如果不对数据进行排序，热图可能看起来就像一片随机的噪声。

通过对[热图](@entry_id:273656)的行（例如，基因）和列（例如，样本）应用层次化聚类，我们可以将相似的行和列重新排列在一起。一个成功的聚类和排序会产生一张结构清晰的[热图](@entry_id:273656)，上面呈现出鲜明的矩形**颜色块 (block structures)**。每个颜色块代表一组基因在一组样本上表现出协同的表达模式（例如，在一组疾病样本中共同上调或下调）。这些模式正是系统生物学家寻求的生物学信号 [@problem_id:4328354]。

#### 评估聚类稳定性：[共识聚类](@entry_id:747702)

任何[聚类算法](@entry_id:146720)都会在数据上产生一个划分，但我们如何知道这个划分是反映了数据中真实的、稳健的结构，还是仅仅是算法或特定数据集的偶然产物？**[共识聚类](@entry_id:747702) (Consensus Clustering)** 提供了一种强大的、基于重抽样的方法来评估聚类的稳定性。

[共识聚类](@entry_id:747702)的过程如下 [@problem_id:4328403]：
1.  **重抽样**: 对原始的 $n$ 个样本进行多次（例如 $R$ 次）重抽样，通常采用[自助法](@entry_id:139281)（bootstrap），即有放回地抽取 $n$ 个样本。
2.  **重复聚类**: 对每个重抽样得到的数据子集，使用相同的[聚类算法](@entry_id:146720)（和固定的簇数 $k$）进行聚类。
3.  **构建共识矩阵**: 构建一个 $n \times n$ 的对称**共识矩阵** $C$。矩阵中的每个元素 $c_{ij}$ 表示样本 $i$ 和样本 $j$ 在所有重抽样中被分到同一个簇的频率。$c_{ij}$ 的值在 $[0,1]$ 之间。
4.  **可视化**: 对共识矩阵 $C$ 本身进行重新排序（例如，使用[层次聚类](@entry_id:268536)，距离为 $1-c_{ij}$），然后将其可视化为一张[热图](@entry_id:273656)。

共识[热图](@entry_id:273656)的解读非常直观 [@problem_id:4328403]。如果数据中存在非常稳定的簇结构，热图上会沿着对角线出现颜色极深（$c_{ij} \approx 1$）且边界清晰的方形色块。这些色块内部的样本对几乎总是在一起聚类。色块之间的区域颜色极浅（$c_{ij} \approx 0$），表示这些区域的样本对几乎从不一起聚类。如果[热图](@entry_id:273656)上的色块边界模糊，或者颜色介于0和1之间，则表明相应的聚类结构不稳定。通过这种方式，[共识聚类](@entry_id:747702)为我们提供了一种定量和可视化的方法来衡量所发现模式的[置信度](@entry_id:267904)，这对于得出可靠的生物学结论至关重要。