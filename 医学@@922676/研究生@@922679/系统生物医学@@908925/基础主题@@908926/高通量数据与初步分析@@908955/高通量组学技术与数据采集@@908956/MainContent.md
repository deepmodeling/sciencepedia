## 引言
[高通量组学](@entry_id:750323)技术已经彻底改变了生物医学研究，使我们能够以前所未有的规模和分辨率对细胞和组织的分子构成进行系统性测量。从基因组到转录组，再到蛋白质组和代谢组，这些技术为我们提供了描绘生命系统复杂性的强大工具。然而，从仪器产生的海量原始数据到可靠的生物学洞见之间存在着巨大的鸿沟。这个鸿沟充满了技术噪声、系统性偏倚和复杂的统计挑战，若不加以妥善处理，极易导致错误的结论。

本篇文章旨在为跨越这一鸿沟提供一张路线图。我们将系统性地剖析[高通量组学](@entry_id:750323)数据从产生到解读的全过程，重点关注那些决定数据质量和最终分析成败的关键概念。通过本文的学习，您将能够理解并应对组学数据中常见的陷阱，从而更有信心地设计实验、评估[数据质量](@entry_id:185007)，并从数据中提取有意义的生物学信号。

为实现这一目标，本文分为三个核心章节。第一章**“原理与机制”**将深入探讨数据生成的基础，从测量的哲学问题出发，到具体的测序工作流程、数据格式和核心统计特性。第二章**“应用与交叉学科联系”**将展示这些基础原理如何在多样的生物学问题中发挥作用，从解析[基因调控](@entry_id:143507)到探索[微生物群落](@entry_id:167568)，再到临床诊断。最后，在**“动手实践”**部分，您将有机会通过解决实际计算问题，将理论知识转化为实践技能。让我们首先从理解[高通量测量](@entry_id:200163)中最基本的问题开始：我们所测量的数据，与我们所追寻的真理之间的关系。

## 原理与机制

本章深入探讨驱动[高通量组学](@entry_id:750323)技术的核心原理和机制。我们将从测量的基本哲学问题出发，即我们如何区分生物学真理与技术伪影，然后逐步深入到数据生成的具体步骤、[数据表示](@entry_id:636977)的格式标准，以及最终解读这些数据所需的关键统计学原理。本章旨在为后续章节中更高级的应用和[算法分析](@entry_id:264228)奠定坚实的概念基础。

### [高通量组学](@entry_id:750323)中的[测量问题](@entry_id:189139)：数据与真理

系统生物学的核心挑战在于，我们测量的并非生物系统的完整状态，而是其可观测的代理指标。理解测量数据（我们所拥有的）与生物学真理（我们所寻求的）之间的区别至关重要。这一认识论上的鸿沟，是由测量误差和仪器偏倚共同造成的。[@problem_id:4350604]

#### [可观测量](@entry_id:267133)与[潜变量](@entry_id:143771)

在任何组学实验中，我们必须首先区分**可观测量 (observable)** 和**[潜变量](@entry_id:143771) (latent variable)**。[可观测量](@entry_id:267133)是仪器直接输出的数值，例如RNA测序（[RNA-seq](@entry_id:140811)）实验中，对应于基因 $i$ 和样本 $j$ 的原始读数计数 $Y_{ij}$。这些是我们数据矩阵的直接组成部分。

然而，我们真正感兴趣的通常是[潜变量](@entry_id:143771)——那些无法直接观测，但驱动着[可观测量](@entry_id:267133)的潜在生物学量。在[RNA-seq](@entry_id:140811)的例子中，这个[潜变量](@entry_id:143771)可能是细胞内真实的转录本丰度 $\Lambda_{ij}$。这个值代表了在进行任何实验操作之前，样本中存在的分子“真理”。我们的目标是通过分析可观测的 $Y_{ij}$ 来推断不可观测的 $\Lambda_{ij}$。

#### 测量误差与仪器偏倚

[可观测量](@entry_id:267133)与潜变量之间的关系受到两类主要噪声源的干扰：测量误差和仪器偏倚。

**测量误差 (measurement error)**，或称[随机误差](@entry_id:144890)，是指测量过程中的随机波动。即使对完全相同的样本进行重复测量，由于固有的随机性（如测序过程中的泊松抽样噪声），每次得到的读数也会有所不同。这种变异性由[可观测量](@entry_id:267133)在其[期望值](@entry_id:150961)周围的方差来表征，即 $\operatorname{Var}(Y_{ij} \mid \Lambda_{ij}, \dots)$。通过技术重复可以减少随机误差的影响。对 $R$ 次技术重复的测量值 $Y_{ij}^{(1)}, \dots, Y_{ij}^{(R)}$ 进行平均，其均值 $\bar{Y}_{ij}$ 的方差会以 $1/R$ 的速率减小。

**仪器偏倚 (instrument bias)**，或称系统误差，是指测量[期望值](@entry_id:150961)与潜在真值之间存在的系统性、可重复的偏差。它并非随机波动，而是由实验方案和仪器特性决定的。在RNA-seq中，偏倚来源包括样本特异性的测序深度 $s_j$ 和基因特异性的捕获效率 $b_{ij}$（例如，由[GC含量](@entry_id:275315)或转录本长度引起的偏倚）。这些因素使得测量值的期望 $\operatorname{E}(Y_{ij} \mid \Lambda_{ij}, s_j, b_{ij})$ 系统性地偏离真实的 $\Lambda_{ij}$。例如，一个常见的模型是 $\operatorname{E}(Y_{ij}) \propto s_j b_{ij} \Lambda_{ij}$。技术重复无法消除偏倚，因为均值 $\bar{Y}_{ij}$ 会收敛到被偏倚的[期望值](@entry_id:150961)，而不是真实的 $\Lambda_{ij}$。

为了弥合数据与真理之间的鸿沟，我们需要采用**校准 (calibration)** 策略。通过在样本中加入已知浓度的**参照物 (spike-in controls)**，我们可以对系统偏倚进行建模和校正。由于参照物的真实丰度已知，我们可以利用其观测读数来估计测序深度 $s_j$ 和其他偏倚来源 $b_{ij}$，从而更准确地从观测数据 $Y_{ij}$ 中推断出生物学真理 $\Lambda_{ij}$。[@problem_id:4350604]

### 从[生物分子](@entry_id:176390)到数字数据：测序工作流程

将生物样本转化为数字数据的过程涉及一系列复杂的步骤，每一步都会引入特定的技术[特征和](@entry_id:189446)潜在的伪影。

#### 文库制备与多重测序

为了提高通量和效率，高通量测序通常采用**多重测序 (multiplexing)**，即在一次测序运行中混合来自多个样本的文库。这是通过给每个样本的DNA[片段连接](@entry_id:183102)上称为**样本条形码 (sample barcodes)** 或**索引 (indexes)** 的短 DNA 序列来实现的。在数据分析的初始阶段，一个称为**解复用 (demultiplexing)** 的计算步骤会根据这些索引序列将读数分配回各自的原始样本。[@problem_id:4350645]

索引的设计和使用策略对数据质量至关重要。为确保即使在存在测序错误的情况下也能准确解复用，索引序列通常被设计为彼此之间具有至少为3的**[汉明距离](@entry_id:157657) (Hamming distance)**。[汉明距离](@entry_id:157657)是指两个等长字符串在相应位置上不同字符的数量。如果解复用时允许一个碱基的错配（即[容错](@entry_id:142190)距离 $d=1$），那么[汉明距离](@entry_id:157657)至少为3的设计可以保证一个样本的索引在发生单个错误后不会变成另一个合法样本的索引，从而避免了因测序错误直接导致的样本误判。

然而，一个更隐蔽的问题是**索引跳跃 (index hopping)**。这是一种在测序仪上（特别是在采用ExAmp成簇扩增的平台上）发生的现象，即一个DNA片段的索引被错误地替换为来自同一泳道中另一个片段的索引。在**单索引 (single-indexing)** 策略中，这种跳跃会导致读数被直接误分配到错误的样本。

**双索引 (dual-indexing)** 策略为解决这一问题提供了强大的解决方案。在这种策略中，DNA片段的两端都连接有索引（例如，i7和i5索引）。一个读数对只有在两个索引都与同一预设样本的索引对匹配时才被接受。因此，要发生样本误判，必须在i7和i5两端同时发生索引跳却，并且恰好都跳跃到属于同一个“受体”样本的索引上。这是一个极小概率的“协同”事件。例如，如果i7端的[跳跃概率](@entry_id:272660)为 $p_7 = 0.006$，i5端的[跳跃概率](@entry_id:272660)为 $p_5 = 0.004$，那么在双索引策略下，导致误判的概率大致为 $p_7 \cdot p_5 = 0.000024$，远低于单索引策略中由单次跳跃主导的误判率（约 $0.006$）。这极大地提高了数据分配的准确性。[@problem_id:4350645]

#### [分子定量](@entry_id:193528)与重复鉴定

在许多组学应用中，我们的目标是计算分子数量，而不仅仅是检测其存在。为此，**[唯一分子标识符](@entry_id:192673) (Unique Molecular Identifiers, UMIs)** 应运而生。UMI是在文库扩增（如PCR）之前，附加到每个原始DNA或cDNA分子上的短随机[核苷](@entry_id:195320)酸序列。它的核心功能是为每个起始分子打上一个独一无二的“指纹”。[@problem_id:4350577]

这使得我们能够精确地区分源于技术扩增的重复和源于生物学本身的重复。在测[序数](@entry_id:150084)据中，我们主要关注两种类型的技术重复：

1.  **PCR重复 (PCR duplicates)**：在PCR扩增过程中，同一个原始文库分子被多次复制，产生了多个具有相同序列的拷贝。这些拷贝在比对到[参考基因组](@entry_id:269221)后，会具有完全相同的起始和终止坐标。
2.  **光学重复 (optical duplicates)**：这是测序仪成像过程中的一种伪影。单个DNA分子簇在测序芯片（flow cell）上可能被错误地识别为两个或多个邻近的簇。因此，光学重复的特征是它们在物理空间上（即在同一成像瓦片 (tile) 内的X-Y坐标）彼此非常接近。[@problem_id:4350578]

在没有UMI的情况下，分析流程通常通过寻找具有相同基因组比对坐标的读数对来标记“重复”，但这种方法无法区分真正的PCR重复和两个不同的原始分子恰好被断裂成相同片段的“巧合”。去除所有这些坐标重复是一个常用的[启发式方法](@entry_id:637904)，旨在减少由早期PCR错误扩增导致的[假阳性](@entry_id:635878)变异，但它有过度校正的风险，可能会丢弃真实的生物学信息。[@problem_id:4350578]

UMIs通过提供最终的裁决依据解决了这个问题。来自同一个原始分子的所有读数，无论经过多少轮PCR扩增，都将共享相同的基因组坐标和**相同的UMI序列**。而来自不同原始分子的读数，即使它们偶然共享相同的基因组坐标，也将具有**不同的UMI序列**。因此，UMI使得我们可以精确地对原始分子进行计数，这个过程称为**去重复 (deduplication)**。[@problem_id:4350577] [@problem_id:4350578]

然而，UMI序列本身也可能在测序过程中发生错误。一个长度为 $L$、碱基错误率为 $\epsilon$ 的UMI，其序列中出现一个错误的概率约为 $L\epsilon$。如果不加以校正，这些带有错误的UMI会被误认为是新的、独立的分子，从而导致分子计数的虚高。为此，发展出了多种UMI纠错算法。常见的策略，如**[邻接法](@entry_id:163788) (adjacency-based methods)**，会检查UMI序列之间的[汉明距离](@entry_id:157657)。一个低计数的UMI如果与一个高计数的UMI仅相差一个碱基（$d_H=1$），则很可能源于后者的测序错误。在这种情况下，低计数的UMI会被合并到高计数的“父”UMI中。

例如，在一个基因座上，我们观测到UMI序列 `ACGTACGTAA` 的读数为120，而另外三个序列 `ACGTACGCAA`、`ACGTACGTAT` 和 `ACGTACGTGA` 的读数分别为3、2、4。这三个低丰度UMI与高丰度UMI的[汉明距离](@entry_id:157657)均为1。假设UMI长度为10，测序错误率为0.008，我们期望从120个读数中产生的单碱基错误读数数量约为 $120 \times 10 \times 0.008 = 9.6$。观测到的单碱基错误读数总数为 $3 + 2 + 4 = 9$，与[期望值](@entry_id:150961)高度吻合。这为我们将这三个低丰度UMI合并到 `ACGTACGTAA` 提供了强有力的统计支持。如果此时还观测到一个读数为1的UMI `TTGTACGTAA`，其与主UMI的[汉明距离](@entry_id:157657)为2，那么它将被视为一个独立的分子。因此，经过[纠错](@entry_id:273762)和去重复，该基因座的最终分子计数为2。值得注意的是，[UMI去重](@entry_id:756286)复必须在每个基因/位点的层面上独立进行，因为不同的基因会独立地被不同的UMI标记。[@problem_id:4350577]

### 测序的语言：数据格式与质量评估

原始测[序数](@entry_id:150084)据及其比对结果以标准化的文件格式存储，这些格式的设计旨在高效地存储海量数据并保留必要的元信息。

#### 原始读数与[质量分数](@entry_id:161575)

原始测序读数最常以 **[FASTQ](@entry_id:201775)** 格式存储。这是一种文本格式，每个读数由四行记录组成：[@problem_id:4350638]
1.  以 `@` 开头的序列标识符。
2.  原始的核苷酸序列（A, C, G, T, N）。
3.  一个 `+` 分隔符，有时后面会重复序列标识符。
4.  一个与核苷酸序列等长的[ASCII](@entry_id:163687)字符，代表每个碱基的**Phred[质量分数](@entry_id:161575) (Phred quality score)**。

Phred质量分数 $Q$ 是衡量碱基识别[错误概率](@entry_id:267618) $p_e$ 的一个指标，其定义为 $Q = -10 \log_{10}(p_e)$。这意味着 $Q=10$ 对应10%的错误率，$Q=20$ 对应1%，$Q=30$ 对应0.1%，以此类推。高的 $Q$ 值表示高的碱基识别[置信度](@entry_id:267904)。这些数值被编码为[ASCII](@entry_id:163687)字符以便于文本存储。

一个[FASTQ](@entry_id:201775)文件主要包含两个等长的[数据流](@entry_id:748201)：碱基序列和[质量分数](@entry_id:161575)序列。碱基序列的字母表很小（{A, C, G, T, N}），其[信息熵](@entry_id:144587)较低。而质量分数的字母表要大得多（例如，[Illumina](@entry_id:201471)平台的[Q值](@entry_id:265045)可达40多种），通常具有更高的[信息熵](@entry_id:144587)。根据香农的[信源编码定理](@entry_id:138686)，信源的熵决定了其[无损压缩](@entry_id:271202)的理论下限。因此，在使用如 `gzip` 这样的通用[无损压缩](@entry_id:271202)算法时，熵更高的质量分数流通常比碱基流更难压缩，并最终占据压缩文件大小的主要部分。一个常见的误解是，高[质量数](@entry_id:142580)据（即高Q值）会更难压缩。事实恰恰相反，当许多碱基都获得同样的高分时（例如，一长串的`I`字符，对应Q=40），会形成低熵的重复模式，从而获得很高的压缩率。[@problem_id:4350638]

#### 比对数据

将原始读数与参考基因组比对后，结果通常存储在**SAM (Sequence Alignment/Map)** 格式或其二进制版本 **BAM (Binary Alignment/Map)** 中。[BAM格式](@entry_id:169833)通过二进制编码和分块压缩（通常是`bgzip`）减小了文件大小，同时通过一个独立的索引文件（`.bai`）支持对基因组特定区域的快速随机访问。[@problem_id:4350638]

为了进一步压缩数据，**CRAM (Compressed Reference-based Alignment Map)** 格式被开发出来。C[RAM](@entry_id:173159)的核心思想是**基于参考的压缩**：它不存储与[参考基因组](@entry_id:269221)完全匹配的碱基序列，而只记录差异（错配、插入、删除）。这极大地减小了文件大小，尤其是在低变异率的情况下。从信息论角度看，对于一个错配率为 $p$ 且不考虑插入删除的简化模型，编码每个碱基所需的平均信息量接近 $h(p) + p \log_2(3)$ 比特，其中 $h(p)$ 是二元熵函数，而 $p \log_2(3)$ 是在发生错配时，指定是哪三种其他碱基之一所需的额外信息。CRAM的代价是，解码（即重构原始读数序列）必须依赖于编码时使用的同一个[参考基因组](@entry_id:269221)文件。[@problem_id:4350638]

在性能方面，C[RAM](@entry_id:173159)的极小文件体积意味着更少的磁盘I/O，但这并不总是意味着更快的分析速度。CRAM的解码过程（重构读数）比BAM的解码（解压）计算量更大。因此，在具有高速I/O（如[固态硬盘](@entry_id:755039)）但CPU成为瓶颈的系统上，对于需要频繁随机访问和解码的任务（如生成pileup文件），BAM的性能可能反而优于CRAM。[@problem_id:4350638]

除了测[序数](@entry_id:150084)据，其他组学技术也有其标准格式。例如，质谱数据通常使用**mzML**格式存储，这是一种基于XML的、不依赖于供应商的标准。它主要存储质荷比（$m/z$）和离子强度数据，不包含Phred质量分数。[@problem_id:4350638]

#### 数据量与质量的核心指标

评估测序实验成功与否，依赖于几个核心指标：
- **测序覆盖度 (Sequencing Coverage)** 或深度 (depth)，指基因组中每个碱基被独立测序读数覆盖的平均次数。计算公式为：$C = \frac{N \times L}{G}$，其中 $N$ 是读数总数， $L$ 是读数长度， $G$ 是基因组大小。例如，一个单倍体基因组大小为 $3.1 \times 10^9$ bp的物种，如果产生了 $8.0 \times 10^8$ 个读数对（即 $1.6 \times 10^9$ 个读数），每个读数长150 bp，那么其原始覆盖度约为 $\frac{(1.6 \times 10^9) \times 150}{3.1 \times 10^9} \approx 77 \times$。[@problem_id:4350578]
- **错误率 (Error Rate)**，即测序过程中发生碱基替换的概率 $\epsilon$。在一个长度为 $L$ 的读数中，期望的错误数量为 $L\epsilon$。假设错误是[独立事件](@entry_id:275822)，一个读数完全没有错误的概率为 $(1-\epsilon)^L$。当 $L\epsilon$ 较小时，这个概率可以用[泊松近似](@entry_id:265225) $e^{-L\epsilon}$ 来估计。例如，对于一个长度为150 bp、错误率为0.005的读数，其期望错误数为 $0.75$，完全正确的概率约为 $e^{-0.75} \approx 0.47$。[@problem_id:4350578]

### 组学数据的统计特性：从原始计数到有意义的信号

从仪器获得的原始数据矩阵仅仅是分析的起点。要从中提取可靠的生物学洞见，必须理解并妥善处理这些数据固有的统计特性。

#### 变异的来源与分解

组学测量中的总变异可以分解为两个主要部分：**生物学变异 (biological variance)** 和**技术变异 (technical variance)**。生物学变异是我们感兴趣的，它反映了不同生物样本（如不同个体、不同细胞类型）之间的真实差异。技术变异则是测量过程本身引入的噪声。

通过精心的实验设计，我们可以量化这两种变异。例如，在一个研究中，我们对 $N$ 个生物学供体进行测量，并对每个供体进行 $R$ 次独立的技术重复。我们可以建立一个层次化的随机效应模型来描述测量值 $y_{ij} = \mu + b_i + e_{ij}$，其中 $b_i$ 是供体 $i$ 的生物学效应（方差为 $\sigma_b^2$），$e_{ij}$ 是技术重复 $j$ 的技术误差（方差为 $\sigma_e^2$）。通过分析法（ANOVA）的原理，我们可以从数据中估计这些方差组分。具体来说，技术方差 $\sigma_e^2$ 可以通过计算每个供体内部技术重复的平均方差（$s_{\text{within}}^2$）来估计。生物学方差 $\sigma_b^2$ 则可以通过供体均值之间的方差（$s_{\text{between}}^2$）减去被技术重复平均后的技术方差贡献来估计，即 $\hat{\sigma}_b^2 = s_{\text{between}}^2 - s_{\text{within}}^2 / R$。单次测量的总方差就是这两者之和：$\sigma_{\text{total}}^2 = \sigma_b^2 + \sigma_e^2$。例如，如果有6个供体，每个3次技术重复，测得 $s_{\text{between}}^2 = 5.2$ 和 $s_{\text{within}}^2 = 1.8$，那么技术方差的估计值为 $\hat{\sigma}_e^2 = 1.8$，生物学方差的估计值为 $\hat{\sigma}_b^2 = 5.2 - 1.8/3 = 4.6$，总方差为 $\hat{\sigma}_{\text{total}}^2 = 1.8 + 4.6 = 6.4$。[@problem_id:4350632]

#### 批次效应与混杂因素

在大型研究中，样本通常分批处理（例如，不同的制备日期、不同的试剂批次、不同的测序仪）。这常常会引入**批次效应 (batch effects)**，即归因于技术处理组的系统性、非生物学来源的变异。如果批次与我们感兴趣的生物学变量（如病例vs对照）相关联，[批次效应](@entry_id:265859)就会成为一个**混杂因素 (confounder)**，它会同时影响测量结果和表型分组，从而在我们估计生物学效应时引入偏倚。[@problem_id:4350646]

**[主成分分析](@entry_id:145395) (Principal Component Analysis, PCA)** 是一种强大的无监督探索性工具。PCA通过寻找数据中方差最大的方向来对样本进行降维。如果[批次效应](@entry_id:265859)是数据中一个主要的变异来源，那么样本在主成分空间（通常是PC1和PC2）中的分布会清晰地按批次聚集，从而直观地揭示出批次效应的存在。[@problem_id:4350646]

为了校正这些未知的或已知的变异来源，**代理变量分析 (Surrogate Variable Analysis, SVA)** 等方法被开发出来。SVA可以从数据中估计出潜在的、未被记录的结构性变异来源（即代理变量），然后将这些代理变量作为协变量纳入下游的[统计模型](@entry_id:755400)中（如[差异表达分析](@entry_id:266370)）。这有助于“清理”数据，吸收不必要的噪声，从而降低[假阳性](@entry_id:635878)并提高检测真实信号的能力。然而，选择代理变量的数量存在一个权衡：包含太少的代理变量可能无法充分校正变异，而包含太多则可能过度拟合，甚至移除部分真实的生物学信号，导致[统计功效](@entry_id:197129)下降。[@problem_id:4350646]

#### 计数数据的统计分布

RNA-seq等技术产生的计数数据具有独特的统计特性。一个核心特征是**过度离散 (overdispersion)**，即数据的方差远大于其均值。一个简单的泊松分布模型假定方差等于均值，这通常无法充分描述[RNA-seq](@entry_id:140811)数据中的变异。

[过度离散](@entry_id:263748)的根源在于一个层次化的变异过程。在给定一个固定的潜在表达率 $\lambda_{gi}$ 的条件下，测序读数的产生可以被建模为一个泊松过程，其方差等于均值。然而，在不同的生物学重复样本之间，真实的表达率 $\lambda_{gi}$ 本身就存在波动，这源于转录的脉冲性、细胞状态的异质性等生物学因素。根据[全方差公式](@entry_id:177482)，总方差 $\mathrm{Var}(Y_{gi})$ 等于[条件方差](@entry_id:183803)的期望加上[条件期望](@entry_id:159140)的方差：$\mathrm{Var}(Y_{gi}) = \mathbb{E}[\mathrm{Var}(Y_{gi}|\lambda_{gi})] + \mathrm{Var}(\mathbb{E}[Y_{gi}|\lambda_{gi}]) = \mathbb{E}[s_i \lambda_{gi}] + \mathrm{Var}(s_i \lambda_{gi}) = s_i \mu_g + s_i^2 \mathrm{Var}(\lambda_{gi})$。这个总方差显然大于其均值 $s_i \mu_g$。[@problem_id:4350625]

**[负二项分布](@entry_id:262151) (Negative Binomial distribution)** 为这种过度离散的数据提供了非常合适的模型。从数学上讲，[负二项分布](@entry_id:262151)可以看作是一个[复合分布](@entry_id:150903)：当泊松分布的率参数本身服从一个伽马分布时，其[边际分布](@entry_id:264862)就是[负二项分布](@entry_id:262151)。这恰好与我们描述的生物学过程（随机的表达率）和技术过程（泊松抽样）的层次化模型相吻合。因此，负二项分布已成为[差异表达分析](@entry_id:266370)等[RNA-seq](@entry_id:140811)下游分析的标准工具。[@problem_id:4350625]

#### 单细胞数据中的零计数问题

在[单细胞RNA测序](@entry_id:142269)（scRNA-seq）数据中，一个突出的特征是数据矩阵中存在大量的零。区分这些零的来源至关重要。一个零可以是**真实的生物学零**，即该细胞确实没有表达该基因（$M_{ig}=0$）。它也可以是一个**技术性零**，通常称为**脱扣 (dropout)**，即基因有表达（$M_{ig}>0$），但由于RNA捕获效率低、逆转录失败或[测序深度](@entry_id:178191)不足，没有任何分子被成功检测到，导致观测计数为零（$C_{ig}=0$）。[@problem_id:4350592]

在早期的非UMI单细胞测序技术中，由于存在剧烈的PCR扩增偏倚，观测到的零的频率常常远超标准计数分布的预测，这种现象被称为**零膨胀 (zero-inflation)**。然而，随着基于UMI的技术的普及，越来越多的证据表明，在这些现代数据中，大量的零其实可以由标准的、非零膨胀的计数模型（如负二项分布）很好地解释。这是因为在捕获效率普遍较低的情况下（通常只有5-20%），即使对于中低表达的基因，由于[随机抽样](@entry_id:175193)，也很容易一次都捕获不到，从而产生一个技术性零。[@problem_id:4350592]

ERCC spike-ins提供了一个有力的诊断工具来检验是否存在真正的零膨胀。由于spike-in的输入浓度已知，我们可以预测在给定的细胞捕获效率下，观测到零的概率。如果实际观测到的零频率与基于泊松或负二项抽样模型预测的零频率相符，则没有证据支持零膨胀。反之，如果观测到的零远多于预测，则表明存在额外的技术伪影，可能需要使用[零膨胀模型](@entry_id:756817)。[@problem_id:4350592]

#### 组分数据的约束

最后，一个深刻而常被忽视的统计问题是，高通量测序数据本质上是**组分数据 (compositional data)**。每个样本的总读数（或UMI总数）是一个固定的总和（即文库大小），我们测量的是每个基因在这个总和中所占的**[相对丰度](@entry_id:754219)**或比例，而不是它们的绝对丰度。这意味着所有基因的丰度被约束在一个单位和的单纯形上：$\sum_{i=1}^{D} p_{i,s} = 1$。[@problem_id:4350586]

这个约束具有深远的统计学后果。它在原本独立的变量之间引入了人为的负相关。如果一个基因的[相对丰度](@entry_id:754219)增加，其他一些基因的相对丰度必然会减少，以保持总和为1。因此，直接在相对丰度（比例）数据上计算标准的相关性系数（如[皮尔逊相关系数](@entry_id:270276)）是极具误导性的。即使两个基因的绝对丰度在生物学上是完全独立的，它们的相对丰度也可能表现出显著的负相关。这种由分母共享（即总文库大小）引起的**[伪相关](@entry_id:755254) (spurious correlation)** 是组分数据分析中的一个经典陷阱。在一个对称的三组分系统中，如果三个组分的绝对丰度独立波动，它们的[相对丰度](@entry_id:754219)两两之间的协[方差近似](@entry_id:268585)为负值。[@problem_id:4350586]

处理组分数据的正确方法是使用**对数比变换 (log-ratio transformations)**，如Aitchison提出的**中心对数比变换 (centered log-ratio, CLR)**：$y_{i,s} = \ln p_{i,s} - \frac{1}{D} \sum_{k=1}^{D} \ln p_{k,s}$。这种变换将数据从受约束的单纯形空间投影到一个不受约束的欧几里得空间中。在CLR变换后的空间中，由单位和约束引起的人为依赖关系被移除，使得协方差和相关性的计算变得有意义，从而能够更准确地推断基因间的相互作用关系。[@problem_id:4350586]