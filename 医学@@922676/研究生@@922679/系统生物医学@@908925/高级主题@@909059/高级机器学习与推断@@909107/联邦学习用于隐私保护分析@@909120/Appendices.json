{"hands_on_practices": [{"introduction": "在深入探讨联邦学习复杂的隐私保障机制之前，解决一个根本性的实践挑战至关重要：通信开销。在任何分布式系统中，传输大量数据都可能缓慢且昂贵，联邦学习也不例外，因为每一轮都必须交换大型模型更新。本练习 [@problem_id:4840332] 提供了一个动手计算，用以量化这种通信成本，并探索像模型量化这样的技术如何在现实的网络条件下，使训练大规模生物医学模型变得可行。", "problem": "一个医院联盟正在使用联邦平均（Federated Averaging, FedAvg）算法训练一个共享诊断模型。FedAvg是联邦学习中的一种标准算法，通过将数据保留在本地来保护患者隐私。在每一轮通信中，每家医院都将其本地模型更新传输给协调器，然后接收聚合后的全局模型。假设模型有 $d=10^{7}$ 个可训练参数，并且一次传输更新的大小等于参数向量的大小。考虑两种参数编码方案：每参数使用 $32$ 位的单精度浮点数，以及每参数使用 $8$ 位的均匀量化。每家医院可用的网络链接速率为每秒 $100$ 兆比特（megabits），其中一兆比特等于 $10^{6}$ 比特。\n\n从数据大小和吞吐量的核心定义（数据大小以比特为单位，吞吐量以比特/秒为单位，时间等于大小除以速率）出发，推导在两种编码方案下，每家医院每轮的总通信时间（上传加下载）。假设没有压缩，没有协议开销，并且协调器发回的模型大小与医院发送的更新大小相同。\n\n判断在每轮 $2\\,\\mathrm{s}$ 的墙钟时间预算下，每种方案是否可行。可行性定义为每轮总通信时间小于或等于 $2\\,\\mathrm{s}$。\n\n将你最终的数值时间四舍五入到四位有效数字，并以秒为单位表示。以行矩阵的形式提供你的最终答案，第一个条目等于 $32$ 位时间，第二个条目等于 $8$ 位时间。", "solution": "首先根据要求标准对问题进行验证。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n-   模型参数数量：$d = 10^7$\n-   编码方案1（单精度）：$b_{32} = 32$ 比特/参数\n-   编码方案2（量化）：$b_8 = 8$ 比特/参数\n-   网络链接速率（吞吐量）：$R = 100$ 兆比特/秒，其中 $1$ 兆比特 $= 10^6$ 比特。\n-   通信轮次结构：每家医院执行一次上传和一次下载。\n-   下载模型的大小：与上传的模型更新相同。\n-   假设：无压缩，无协议开销。\n-   可行性预算：每轮总通信时间 $\\le T_{budget} = 2\\,\\mathrm{s}$。\n-   计算基础：时间 = 数据大小 / 吞吐量。\n-   取整要求：最终时间四舍五入至四位有效数字。\n\n**步骤2：使用提取的已知条件进行验证**\n该问题在科学上基于计算机网络和联邦学习的原理。该场景是联邦学习在医学信息学中的一个标准应用。所提供的参数（$10^7$ 个参数、$100\\,\\mathrm{Mbps}$ 链接、$32$ 位和 $8$ 位编码）在现代机器学习和网络基础设施中是现实的数值。该问题提法明确 (well-posed)，提供了计算唯一解所需的所有信息和清晰、客观的定义（例如，$1$ 兆比特 $= 10^6$ 比特）。它没有科学缺陷、歧义或矛盾。\n\n**步骤3：结论与行动**\n该问题被视为有效。将提供解决方案。\n\n### 求解推导\n\n分析过程首先为每种编码方案定义单次通信轮次中要传输的总数据大小。然后，利用给定的网络吞吐量，计算此次传输所需的时间。最后，将该时间与指定的可行性预算进行比较。\n\n设 $d$ 为模型中可训练参数的数量。\n设 $b$ 为用于编码每个参数的比特数。\n模型更新的大小 $S$（以比特为单位）由参数数量和每参数比特数的乘积给出：\n$$S = d \\times b$$\n\n网络链接速率 $R$ 为每秒 $100$ 兆比特。根据所提供的定义，即：\n$$R = 100 \\times 10^6 \\frac{\\mathrm{bits}}{\\mathrm{second}} = 10^8 \\frac{\\mathrm{bits}}{\\mathrm{second}}$$\n\n对于一家医院来说，单次通信轮次包括一次向协调器上传模型更新和一次从协调器下载聚合后的全局模型。问题指出，下载的模型大小与上传的更新大小相同。因此，每家医院每轮传输的总数据量 $S_{total}$ 是单次更新大小的两倍：\n$$S_{total} = S_{upload} + S_{download} = S + S = 2S$$\n\n一轮的总时间 $T_{round}$ 是总数据大小除以网络链接速率：\n$$T_{round} = \\frac{S_{total}}{R} = \\frac{2S}{R} = \\frac{2db}{R}$$\n\n我们现在为两种编码方案计算这个时间。\n\n**方案1：单精度浮点数（$32$ 位）**\n\n在这种情况下，每参数的比特数是 $b_{32} = 32$。\n参数数量为 $d = 10^7$。\n单次模型更新的大小是：\n$$S_{32} = d \\times b_{32} = 10^7 \\times 32 = 3.2 \\times 10^8 \\text{ bits}$$\n\n每轮总通信时间 $T_{round, 32}$ 是：\n$$T_{round, 32} = \\frac{2 S_{32}}{R} = \\frac{2 \\times (3.2 \\times 10^8 \\text{ bits})}{10^8 \\text{ bits/s}} = 2 \\times 3.2\\,\\mathrm{s} = 6.4\\,\\mathrm{s}$$\n\n问题要求四舍五入到四位有效数字。因此，$T_{round, 32} = 6.400\\,\\mathrm{s}$。\n\n**方案1的可行性检查：**\n可行性预算为 $T_{budget} = 2\\,\\mathrm{s}$。\n我们比较计算出的时间：$6.400\\,\\mathrm{s} > 2\\,\\mathrm{s}$。\n因此，在给定的约束条件下，$32$ 位编码方案是**不可行**的。\n\n**方案2：均匀量化（$8$ 位）**\n\n在这种情况下，每参数的比特数是 $b_8 = 8$。\n参数数量为 $d = 10^7$。\n单次模型更新的大小是：\n$$S_8 = d \\times b_8 = 10^7 \\times 8 = 8 \\times 10^7 \\text{ bits}$$\n\n每轮总通信时间 $T_{round, 8}$ 是：\n$$T_{round, 8} = \\frac{2 S_8}{R} = \\frac{2 \\times (8 \\times 10^7 \\text{ bits})}{10^8 \\text{ bits/s}} = 2 \\times 0.8\\,\\mathrm{s} = 1.6\\,\\mathrm{s}$$\n\n问题要求四舍五入到四位有效数字。因此，$T_{round, 8} = 1.600\\,\\mathrm{s}$。\n\n**方案2的可行性检查：**\n可行性预算为 $T_{budget} = 2\\,\\mathrm{s}$。\n我们比较计算出的时间：$1.600\\,\\mathrm{s} \\le 2\\,\\mathrm{s}$。\n因此，在给定的约束条件下，$8$ 位编码方案是**可行**的。\n\n最终答案要求将两个计算出的时间（以秒为单位并四舍五入到四位有效数字）以行矩阵的形式呈现。\n$32$ 位方案的时间是 $6.400\\,\\mathrm{s}$。\n$8$ 位方案的时间是 $1.600\\,\\mathrm{s}$。\n该行矩阵为 $\\begin{pmatrix} 6.400  1.600 \\end{pmatrix}$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n6.400  1.600\n\\end{pmatrix}\n}\n$$", "id": "4840332"}, {"introduction": "一个常见的误解是，联邦学习仅通过不共享原始数据就能固有地保证隐私。然而，模型更新本身也可能泄露敏感信息。这个实践问题 [@problem_id:4341228] 将引导你体验一个惊人的梯度泄露攻击示例，展示在一个简单的线性模型中，对手如何从单次梯度更新中完美地重建客户的私有输入特征。通过完成此推导，你将深刻体会到机器学习中隐私问题的微妙之处，并理解为何迫切需要加密或基于噪声的防御措施。", "problem": "一个医院联合体在联邦学习（FL）框架下合作，为系统生物医学中的一个连续生物标志物构建一个线性预测器。在每一轮中，一个客户端使用单个随机抽样的患者记录计算正则化平方损失的梯度，并将此梯度发送给协调者。一个敌对的协调者从旁路信息中得知了当前的模型参数和患者的标签。考虑一个单一样本，其特征为 $x \\in \\mathbb{R}^{d}$，非负标签为 $y \\in \\mathbb{R}_{\\ge 0}$，模型参数为 $w \\in \\mathbb{R}^{d}$，其中由于领域约束（例如，非负的表达特征和由可解释性约束施加的非负权重），$x$ 和 $w$ 的所有条目都是非负的。客户端使用正则化平方损失\n$$\n\\mathcal{L}(w;x,y) = \\frac{1}{2}\\left(w^{\\top} x - y\\right)^{2} + \\frac{\\lambda}{2}\\|w\\|^{2},\n$$\n其中正则化系数 $\\lambda \\ge 0$，并将梯度 $\\nabla_{w}\\mathcal{L}$ 传输给服务器。假设敌手观察到 $g = \\nabla_{w}\\mathcal{L}(w;x,y)$，并且也知道 $w$、$y$ 和 $\\lambda$。你可以进一步假设 $w^{\\top}\\left(g - \\lambda w\\right) > 0$，这是在上述非负性假设下保证唯一性的一个充分条件。\n\n仅使用基本定义，推导一个闭式解析表达式 $x^{\\star}(g,w,y,\\lambda)$，该表达式可以从观察到的梯度 $g$、已知的标签 $y$、参数 $w$ 和正则化 $\\lambda$ 中精确地重构输入特征 $x$。作为一种保护隐私的缓解措施，请提出一个在客户端应用的线性披露映射，以改变通信的数据量，使得从单轮通信中精确重构 $x$ 变得不再可能，同时保留其用于聚合的效用。你的推导必须从给定损失的梯度定义和标准的线性代数恒等式开始，并且不得预先假设或陈述目标重构公式。\n\n将你最终重构的 $x^{\\star}$ 表示为单个闭式解析表达式。不需要进行数值计算，也不需要四舍五入。如果你的缓解措施中包含任何角度或物理单位，请分别指明为弧度或适当的单位；否则，保持缓解措施为符号形式。", "solution": "该问题经评估是有效的。它在机器学习和隐私领域有科学依据，问题提法得当，信息充分，并有确保唯一性的特定条件，且使用标准数学符号进行了客观表述。\n\n解答过程分为两部分。首先，我们推导特征向量 $x$ 的闭式表达式。其次，我们提出一个线性披露映射来缓解这种隐私泄露。\n\n**第一部分：$x^{\\star}$ 重构公式的推导**\n\n客户端计算正则化平方损失函数的梯度：\n$$\n\\mathcal{L}(w;x,y) = \\frac{1}{2}\\left(w^{\\top} x - y\\right)^{2} + \\frac{\\lambda}{2}\\|w\\|^{2}\n$$\n其中 $\\|w\\|^2 = w^\\top w$。关于模型参数 $w$ 的梯度 $g$ 由下式给出：\n$$\ng = \\nabla_{w}\\mathcal{L} = \\frac{\\partial}{\\partial w} \\left( \\frac{1}{2}\\left(w^{\\top} x - y\\right)^{2} + \\frac{\\lambda}{2}w^{\\top}w \\right)\n$$\n对第一项使用链式法则，对第二项使用二次型的标准梯度，我们得到：\n$$\n\\nabla_{w} \\left( \\frac{1}{2}\\left(w^{\\top} x - y\\right)^{2} \\right) = \\frac{1}{2} \\cdot 2 \\left(w^{\\top} x - y\\right) \\cdot \\nabla_{w}(w^{\\top}x - y) = \\left(w^{\\top} x - y\\right)x\n$$\n$$\n\\nabla_{w} \\left( \\frac{\\lambda}{2}w^{\\top}w \\right) = \\lambda w\n$$\n结合这些项，得到观察到的梯度 $g$ 的表达式：\n$$\ng = (w^{\\top}x - y)x + \\lambda w\n$$\n敌手的目标是在已知 $g$、$w$、$y$ 和 $\\lambda$ 的情况下，解出此方程中的未知特征向量 $x$。我们可以重新整理该方程，以分离出包含 $x$ 的项：\n$$\ng - \\lambda w = (w^{\\top}x - y)x\n$$\n这个方程揭示了左侧的已知向量 $g - \\lambda w$ 是未知向量 $x$ 的一个标量倍数。这意味着 $x$ 必须与 $g - \\lambda w$ 平行。然而，由于 $x$ 也出现在标量项 $w^{\\top}x - y$ 内部，直接求解 $x$ 会很复杂。\n\n为了继续求解，我们可以首先求出标量 $S = w^{\\top}x$ 的值。我们将整理后的方程两边与 $w$ 做内积：\n$$\nw^{\\top}(g - \\lambda w) = w^{\\top}((w^{\\top}x - y)x)\n$$\n利用内积的线性性质，我们得到：\n$$\nw^{\\top}(g - \\lambda w) = (w^{\\top}x - y)(w^{\\top}x)\n$$\n我们代入 $S = w^{\\top}x$ 并定义已知常数 $C = w^{\\top}(g - \\lambda w)$。该方程变成一个关于 $S$ 的二次方程：\n$$\nC = (S - y)S\n$$\n$$\nS^2 - yS - C = 0\n$$\n使用二次公式求解 $S$：\n$$\nS = \\frac{-(-y) \\pm \\sqrt{(-y)^2 - 4(1)(-C)}}{2(1)} = \\frac{y \\pm \\sqrt{y^2 + 4C}}{2}\n$$\n将 $C = w^{\\top}(g - \\lambda w)$ 代回：\n$$\nS = w^{\\top}x = \\frac{y \\pm \\sqrt{y^2 + 4w^{\\top}(g - \\lambda w)}}{2}\n$$\n问题指明特征 $x \\in \\mathbb{R}^d$ 和权重 $w \\in \\mathbb{R}^d$ 的所有条目都是非负的。因此，它们的内积 $w^{\\top}x = \\sum_{i=1}^d w_i x_i$ 必须是非负的，即 $w^{\\top}x \\ge 0$。\n问题还提供了条件 $C = w^{\\top}(g - \\lambda w) > 0$。由于 $y \\ge 0$，我们有 $y^2 \\ge 0$。因此，平方根下的项 $y^2 + 4C$ 是严格为正的。令 $D = \\sqrt{y^2 + 4C}$。由于 $C>0$，我们有 $D > \\sqrt{y^2} = |y| = y$。\n\n$S$ 的两个可能解是 $S_1 = \\frac{y+D}{2}$ 和 $S_2 = \\frac{y-D}{2}$。\n由于 $D > y$，第二个解 $S_2$ 是负的。根据约束 $S = w^{\\top}x \\ge 0$，这个解是无效的。\n第一个解 $S_1$ 是正的，因为 $y \\ge 0$ 且 $D > 0$。因此，我们得到 $S$ 的唯一解：\n$$\nw^{\\top}x = \\frac{y + \\sqrt{y^2 + 4w^{\\top}(g - \\lambda w)}}{2}\n$$\n既然我们已经确定了 $w^{\\top}x$ 的值，我们就可以求出标量乘数 $(w^{\\top}x - y)$ 的值。\n$$\nw^{\\top}x - y = \\left( \\frac{y + \\sqrt{y^2 + 4w^{\\top}(g - \\lambda w)}}{2} \\right) - y = \\frac{-y + \\sqrt{y^2 + 4w^{\\top}(g - \\lambda w)}}{2}\n$$\n由于 $\\sqrt{y^2 + 4w^{\\top}(g - \\lambda w)} > y$，这个标量项是严格为正的。这意味着我们可以用它来做除法。回到方程 $g - \\lambda w = (w^{\\top}x - y)x$，我们现在可以解出 $x$：\n$$\nx^{\\star} = \\frac{g - \\lambda w}{w^{\\top}x - y}\n$$\n代入分母的表达式，我们得到闭式重构公式：\n$$\nx^{\\star} = \\frac{g - \\lambda w}{\\frac{-y + \\sqrt{y^2 + 4w^{\\top}(g - \\lambda w)}}{2}} = \\frac{2(g - \\lambda w)}{\\sqrt{y^2 + 4w^{\\top}(g - \\lambda w)} - y}\n$$\n这就是从敌手可用的信息中精确重构特征向量 $x$ 的方法。\n\n**第二部分：保护隐私的缓解措施**\n\n推导出的重构公式表明存在一个严重的隐私泄露。为缓解此问题，我们提出一个线性披露映射，通过改变通信的梯度 $g$ 使得精确重构不再可能，同时保留其对于联邦聚合过程的效用。\n\n提出的缓解措施是**梯度投影**。客户端不传输原始梯度 $g$，而是传输一个投影后的版本 $g'$。\n\n**机制：** 客户端像之前一样计算梯度 $g=\\nabla_w \\mathcal{L}$。然后，它计算 $g$ 在与当前权重向量 $w$ 正交的子空间上的投影。这是一个由投影矩阵 $P_w = I - \\frac{w w^{\\top}}{\\|w\\|^2}$ 定义的线性映射。客户端将改变后的量 $g'$ 发送给服务器：\n$$\ng' = P_w g = \\left(I - \\frac{w w^{\\top}}{\\|w\\|^2}\\right) g\n$$\n\n**防止重构：** 观察到 $g'$ 的敌手会尝试同样的重构攻击。他们知道 $g'$、$w$、$y$ 和 $\\lambda$。该攻击从根本上依赖于 $g - \\lambda w$ 与 $x$ 成比例这一事实。当敌手接收到 $g'$ 时，他们不能再做这个假设了。如果他们试图建立一个关于 $x$ 的方程，他们将从投影对原始梯度方程的影响开始：\n$$\ng' = P_w g = P_w \\left( (w^{\\top}x - y)x + \\lambda w \\right)\n$$\n由于 $P_w w = 0$，正则化项在投影下消失了：\n$$\ng' = (w^{\\top}x - y) P_w x\n$$\n敌手观察到 $g'$ 并希望求解 $x$。向量 $x$ 可以分解为两个正交分量：$x = x_{||w} + x_{\\perp w}$，其中 $x_{||w}$ 与 $w$ 平行，而 $x_{\\perp w}$ 与 $w$ 正交。投影 $P_w x$ 正是 $x_{\\perp w}$。方程变为：\n$$\ng' = (w^{\\top}x - y) x_{\\perp w}\n$$\n这个方程只包含关于 $x$ 与 $w$ 正交的分量的信息。$x$ 与 $w$ 平行的分量 $x_{||w} = \\frac{w^\\top x}{\\|w\\|^2}w$ 已从方程中完全消除。敌手可以确定 $x_{\\perp w}$（在类似于原始攻击的标量模糊度下），但无法从 $g'$ 中获得关于 $x_{||w}$ 的任何信息。由于对于任意标量 $\\alpha$，任何形如 $x' = x_{\\perp w} + \\alpha w$ 的向量都有相同的投影 $P_w x' = x_{\\perp w}$，重构问题变得欠定。因此，精确重构 $x$ 是不可能的。\n\n**聚合的效用：** 服务器的目标是聚合梯度，例如 $\\frac{1}{N} \\sum_i g_i$，以执行模型更新。在这种缓解措施下，服务器聚合的是投影后的梯度：\n$$\n\\frac{1}{N} \\sum_i g'_i = \\frac{1}{N} \\sum_i P_w g_i = P_w \\left( \\frac{1}{N} \\sum_i g_i \\right)\n$$\n服务器获得的是真实平均梯度的投影。形如 $w_{t+1} \\leftarrow w_t - \\eta \\left( \\sum_i g'_i \\right)$ 的模型更新现在是沿与 $w_t$ 正交方向的更新。这对应于一种投影梯度下降算法，该算法将优化轨迹约束在半径为 $\\|w_t\\|$ 的超球面上。虽然这改变了标准梯度下降的优化动态，但它是一个定义明确的优化过程，在这个新约束下仍然寻求最小化损失函数，因此保留了一种明确的效用形式。", "answer": "$$\n\\boxed{\\frac{2(g - \\lambda w)}{\\sqrt{y^2 + 4w^{\\top}(g - \\lambda w)} - y}}\n$$", "id": "4341228"}, {"introduction": "在明确了梯度泄露等隐私风险的现实性之后，我们现在转向一种强大且数学上严谨的防御方法：差分隐私 (Differential Privacy, DP)。在像联邦学习这样的迭代过程中实施 DP，需要仔细核算在多轮训练中累积的隐私损失。这个高级练习 [@problem_id:4840301] 介绍了 Rényi 差分隐私 (RDP) 框架——一种用于追踪和组合隐私成本的先进工具，用以推导出一个完整的训练过程要达到目标 $(\\epsilon, \\delta)$-DP 保证所需的确切噪声水平 $\\sigma$。这项实践填补了 DP 理论与其在安全联邦学习系统中实际部署之间的鸿沟。", "problem": "一个医院网络使用带有客户端级差分隐私 (DP) 的联邦学习 (FL) 在 $N$ 个诊所间训练一个分类器。在客户端级随机梯度下降 (SGD) 的每一轮通信中，每个被采样的诊所的模型更新被裁剪，使其 $\\ell_{2}$-范数至多为 $C$，服务器在平均之前，对裁剪后更新的总和的每个坐标独立地添加标准差为 $\\sigma$ 的零均值高斯噪声。采样通过独立的泊松子采样完成，每轮客户端采样率为 $q$，总共执行 $T$ 轮。\n\n假设隐私会计使用 Rényi 差分隐私 (RDP)。您可以从以下广为接受的基本原则出发：\n- $\\ell_{2}$ 敏感度为 $S$、高斯噪声标准差为 $\\sigma$ 的高斯机制 (GM)，在 Rényi 阶 $\\alpha>1$ 时，满足 RDP 参数 $\\epsilon_{\\alpha}=\\alpha S^{2}/(2\\sigma^{2})$。\n- 泊松子采样（以速率 $q$ 进行独立包含）能放大隐私保护；在 $q$ 足够小以至于关于 $q$ 的二阶近似足够精确的小采样范围内，子采样高斯机制的每轮 RDP 可以通过在前述表达式中用 $q^{2}S^{2}$ 替换 $S^{2}$ 来近似。\n- RDP 在 $T$ 轮中是可加性组合的。\n- 一个 $(\\alpha,\\epsilon_{\\alpha})$-RDP 机制对于任何 $\\delta\\in(0,1)$ 也是一个 $(\\epsilon,\\delta)$-DP 机制，其中 $\\epsilon=\\epsilon_{\\alpha}+\\ln(1/\\delta)/(\\alpha-1)$。\n\n在这些假设下，并且只使用这些基本原则，推导出一个闭式解析表达式，用于计算最小高斯噪声标准差 $\\sigma$（作为 $C$、$q$、$T$、$\\epsilon$ 和 $\\delta$ 的函数），以确保整个训练过程在 $T$ 轮结束时满足 $(\\epsilon,\\delta)$-DP。请将您的最终答案表示为关于 $C$、$q$、$T$、$\\epsilon$ 和 $\\delta$ 的单个简化代数表达式。不需要进行数值代入。您的答案必须是单个解析表达式；不要给出不等式。如果您选择进行任何近似，它们必须与所述的小 $q$ 范围一致。仅陈述您最终的 $\\sigma$ 表达式。", "solution": "该问题经评估有效。它在科学上基于差分隐私的原理，特别是应用于联邦学习的 Rényi 差分隐私 (RDP)。该问题是适定的、客观的，并提供了一套足够完整和一致的基本原则来推导出唯一的、有意义的解。\n\n最小高斯噪声标准差 $\\sigma$ 的推导过程遵循已建立的 RDP 分析基础。\n\n1.  **每轮更新的敏感度：**\n    在每一轮中，服务器计算来自客户端样本的模型更新的总和。每个客户端的更新是一个向量，被裁剪以使其 $\\ell_2$-范数至多为 $C$。我们感兴趣的机制是这些裁剪后更新的求和。这个求和函数相对于单个客户端数据的 $\\ell_2$-敏感度 $S$ 是，如果该客户端的更新发生改变，总和的 $\\ell_2$-范数可能发生的最大变化。这个最大变化受单个客户端更新的最大范数限制。因此，敏感度为 $S = C$。\n\n2.  **每轮的 RDP 成本：**\n    问题陈述，在每一轮中，服务器以速率 $q$ 执行泊松子采样，然后应用高斯机制。对于这一步，我们有两个基本原则：\n    - $\\ell_2$ 敏感度为 $S$、噪声标准差为 $\\sigma$ 的高斯机制 (GM) 满足 $(\\alpha, \\epsilon_{\\alpha})$-RDP，其中 $\\epsilon_{\\alpha} = \\frac{\\alpha S^2}{2\\sigma^2}$。\n    - 对于小采样率 $q$ 的泊松子采样，RDP 成本可以通过在 GM-RDP 公式中用 $(qS)^2$ 替换 $S^2$ 来近似。\n    结合这两点，并代入 $S=C$，单轮的 RDP 成本，记为 $\\epsilon_{\\alpha, \\text{round}}$，为：\n    $$ \\epsilon_{\\alpha, \\text{round}} \\approx \\frac{\\alpha (qC)^2}{2\\sigma^2} = \\frac{\\alpha q^2 C^2}{2\\sigma^2} $$\n\n3.  **$T$ 轮后的总 RDP 成本：**\n    第三个基本原则指出 RDP 是可加性组合的。由于训练过程包含 $T$ 个独立的轮次，总 RDP 成本 $\\epsilon_{\\alpha, T}$ 是每轮成本的 $T$ 倍：\n    $$ \\epsilon_{\\alpha, T} = T \\cdot \\epsilon_{\\alpha, \\text{round}} = \\frac{T \\alpha q^2 C^2}{2\\sigma^2} $$\n\n4.  **从 RDP 转换为 $(\\epsilon, \\delta)$-DP：**\n    第四个基本原则提供了从 $(\\alpha, \\epsilon_{\\alpha})$-RDP 到 $(\\epsilon, \\delta)$-DP 的转换公式，适用于任何 $\\delta \\in (0, 1)$：\n    $$ \\epsilon = \\epsilon_{\\alpha} + \\frac{\\ln(1/\\delta)}{\\alpha-1} $$\n    将总 RDP 成本 $\\epsilon_{\\alpha, T}$ 代入此公式，我们得到 $T$ 轮训练的总体隐私保证：\n    $$ \\epsilon = \\frac{T \\alpha q^2 C^2}{2\\sigma^2} + \\frac{\\ln(1/\\delta)}{\\alpha-1} $$\n\n5.  **关于 RDP 阶 $\\alpha$ 的优化：**\n    对于给定的噪声水平 $\\sigma$ 和选定的 RDP 阶 $\\alpha > 1$，上述方程给出了所实现的隐私损失 $\\epsilon$。为了找到达到目标 $(\\epsilon, \\delta)$-DP 保证所需的最小噪声 $\\sigma$，我们必须找到最紧密的界。这通过选择使 $\\epsilon$ 表达式（对于固定的 $\\sigma$ 和其他参数）最小化的 $\\alpha$ 值来实现。我们定义要最小化的函数：\n    $$ \\epsilon(\\alpha) = \\left(\\frac{T q^2 C^2}{2\\sigma^2}\\right) \\alpha + \\frac{\\ln(1/\\delta)}{\\alpha-1} $$\n    为了找到最小值，我们计算关于 $\\alpha$ 的导数并将其设为零：\n    $$ \\frac{d\\epsilon}{d\\alpha} = \\frac{T q^2 C^2}{2\\sigma^2} - \\frac{\\ln(1/\\delta)}{(\\alpha-1)^2} = 0 $$\n    求解 $(\\alpha-1)^2$：\n    $$ (\\alpha-1)^2 = \\frac{2\\sigma^2 \\ln(1/\\delta)}{T q^2 C^2} $$\n    由于 $\\alpha > 1$，我们取正平方根：\n    $$ \\alpha_{\\text{opt}} - 1 = \\sqrt{\\frac{2\\sigma^2 \\ln(1/\\delta)}{T q^2 C^2}} = \\frac{\\sigma \\sqrt{2\\ln(1/\\delta)}}{\\sqrt{T} q C} $$\n    最优的 $\\alpha$ 是：\n    $$ \\alpha_{\\text{opt}} = 1 + \\frac{\\sigma \\sqrt{2\\ln(1/\\delta)}}{\\sqrt{T} q C} $$\n    现在我们将这个最优的 $\\alpha_{\\text{opt}}$ 代回到 $\\epsilon$ 的方程中：\n    $$ \\epsilon = \\frac{T q^2 C^2}{2\\sigma^2} \\left(1 + \\frac{\\sigma \\sqrt{2\\ln(1/\\delta)}}{\\sqrt{T} q C}\\right) + \\frac{\\ln(1/\\delta)}{\\left(1 + \\frac{\\sigma \\sqrt{2\\ln(1/\\delta)}}{\\sqrt{T} q C}\\right) - 1} $$\n    $$ \\epsilon = \\frac{T q^2 C^2}{2\\sigma^2} + \\frac{T q^2 C^2}{2\\sigma^2} \\frac{\\sigma \\sqrt{2\\ln(1/\\delta)}}{\\sqrt{T} q C} + \\frac{\\ln(1/\\delta)}{\\frac{\\sigma \\sqrt{2\\ln(1/\\delta)}}{\\sqrt{T} q C}} $$\n    $$ \\epsilon = \\frac{T q^2 C^2}{2\\sigma^2} + \\frac{\\sqrt{T} q C \\sqrt{2\\ln(1/\\delta)}}{2\\sigma} + \\frac{\\sqrt{T} q C \\sqrt{\\ln(1/\\delta)}}{\\sigma\\sqrt{2}} $$\n    $$ \\epsilon = \\frac{T q^2 C^2}{2\\sigma^2} + \\frac{\\sqrt{2T} q C \\sqrt{\\ln(1/\\delta)}}{2\\sigma} + \\frac{\\sqrt{2T} q C \\sqrt{\\ln(1/\\delta)}}{2\\sigma} $$\n    $$ \\epsilon = \\frac{T q^2 C^2}{2\\sigma^2} + \\frac{\\sqrt{2T} q C \\sqrt{\\ln(1/\\delta)}}{\\sigma} $$\n\n6.  **求解最小噪声 $\\sigma$：**\n    前面的方程将目标隐私 $\\epsilon$ 与所需的最小噪声 $\\sigma$ 联系起来。为了找到 $\\sigma$，我们解这个方程。令 $x = 1/\\sigma$。该方程是关于 $x$ 的二次方程：\n    $$ \\left(\\frac{T q^2 C^2}{2}\\right) x^2 + \\left(\\sqrt{2T} q C \\sqrt{\\ln(1/\\delta)}\\right) x - \\epsilon = 0 $$\n    这是一个标准形式为 $ax^2 + bx + c = 0$ 的二次方程，其中 $ a = \\frac{T q^2 C^2}{2} $，$ b = \\sqrt{2T} q C \\sqrt{\\ln(1/\\delta)} $，以及 $ c = -\\epsilon $。由于 $\\sigma > 0$，我们需要 $x > 0$。我们使用二次公式 $x = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$ 并选择正根。\n    判别式为 $ b^2 - 4ac = \\left(2T q^2 C^2 \\ln(1/\\delta)\\right) - 4\\left(\\frac{T q^2 C^2}{2}\\right)(-\\epsilon) = 2T q^2 C^2 \\left(\\ln(1/\\delta) + \\epsilon \\right) $。\n    $$ x = \\frac{-\\sqrt{2T} q C \\sqrt{\\ln(1/\\delta)} + \\sqrt{2T q^2 C^2 \\left(\\epsilon + \\ln(1/\\delta)\\right)}}{T q^2 C^2} $$\n    $$ x = \\frac{-\\sqrt{2T} q C \\sqrt{\\ln(1/\\delta)} + \\sqrt{2T} q C \\sqrt{\\epsilon + \\ln(1/\\delta)}}{T q^2 C^2} $$\n    因式分解并简化：\n    $$ x = \\frac{\\sqrt{2T} q C \\left(\\sqrt{\\epsilon + \\ln(1/\\delta)} - \\sqrt{\\ln(1/\\delta)}\\right)}{T q^2 C^2} = \\frac{\\sqrt{2} \\left(\\sqrt{\\epsilon + \\ln(1/\\delta)} - \\sqrt{\\ln(1/\\delta)}\\right)}{\\sqrt{T} q C} $$\n    因为我们需要找到 $\\sigma = 1/x$，所以我们有：\n    $$ \\sigma = \\frac{\\sqrt{T} q C}{\\sqrt{2} \\left(\\sqrt{\\epsilon + \\ln(1/\\delta)} - \\sqrt{\\ln(1/\\delta)}\\right)} $$\n    为了获得一个代数上更稳定和简化的形式，我们通过分子和分母同乘以 $\\left(\\sqrt{\\epsilon + \\ln(1/\\delta)} + \\sqrt{\\ln(1/\\delta)}\\right)$ 来有理化分母：\n    $$ \\sigma = \\frac{\\sqrt{T} q C \\left(\\sqrt{\\epsilon + \\ln(1/\\delta)} + \\sqrt{\\ln(1/\\delta)}\\right)}{\\sqrt{2} \\left((\\epsilon + \\ln(1/\\delta)) - \\ln(1/\\delta)\\right)} $$\n    $$ \\sigma = \\frac{\\sqrt{T} q C \\left(\\sqrt{\\epsilon + \\ln(1/\\delta)} + \\sqrt{\\ln(1/\\delta)}\\right)}{\\sqrt{2}\\epsilon} $$\n    这是最小噪声标准差 $\\sigma$ 的最终闭式表达式。", "answer": "$$ \\boxed{\\frac{q C \\sqrt{T} \\left( \\sqrt{\\epsilon + \\ln(1/\\delta)} + \\sqrt{\\ln(1/\\delta)} \\right)}{\\sqrt{2}\\epsilon}} $$", "id": "4840301"}]}