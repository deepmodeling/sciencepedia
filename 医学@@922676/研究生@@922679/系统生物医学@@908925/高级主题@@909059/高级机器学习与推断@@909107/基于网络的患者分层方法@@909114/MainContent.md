## 引言
随着高通量测序技术的发展，现代生物医学研究正在进入一个以系统生物学为导向、以大数据为驱动的“后基因组时代”。面对为每位患者生成的基因组、转录组、蛋白质组等多维度、高复杂的“组学”数据，传统的“一刀切”式疾病诊断和治疗模式已显得力不从心。[个性化医疗](@entry_id:152668)的愿景要求我们能够从这些复杂数据中识别出具有不同生物学机制和临床特征的患者亚群，即实现精准的患者分层。然而，如何系统性地整合[异构数据](@entry_id:265660)、滤除噪声、并揭示隐藏在海量数据背后的有意义的生物学模式，是当前面临的核心挑战。

基于网络的方法为此提供了一个强大而灵活的分析框架。通过将患者群体抽象为一个数学网络，其中节点代表患者，边的权重代表他们生物学上的相似程度，我们可以将复杂的患者分层问题转化为一个可计算、可解释的[图分析](@entry_id:750011)任务。这种方法不仅能够直观地表示患者间的关系，还能利用图论和机器学习领域的成熟工具来发现稳健的患者亚群。

本文将系统性地引导读者深入理解基于网络的患者分层方法。在第一章“原理与机制”中，我们将奠定理论基础，详细阐述如何将患者[数据表示](@entry_id:636977)为不同类型的网络，如何选择合适的相似性度量，以及如何利用谱聚类等核心算法通过分析[网络结构](@entry_id:265673)来揭示患者亚群。随后的第二章“应用与跨学科交叉”将这些理论付诸实践，展示如何应用这些方法整合多组学数据、识别[疾病模块](@entry_id:271920)、预测临床结局，并探讨其与因果推断、算法公平性等前沿领域的交叉。最后，在第三章“动手实践”中，我们将通过具体的编程练习，让读者亲手实现从数据到分层的核心分析流程，巩固所学知识。

## 原理与机制

在患者分层中，基于网络的方法提供了一个强大的框架，用于整合[异构数据](@entry_id:265660)、发现有意义的亚群，并最终实现个体化医疗。本章深入探讨了支撑这些方法的核心原理和机制，从将患者队列形式化为数学图谱，到通过分析其结构来揭示临床相关的分层。我们将系统地阐述网络的构建、分析和解释所涉及的基本概念。

### 将患者队列表示为网络：一个形式化框架

患者分层任务的核心是将一组患者及其复杂的生物和临床数据转化为一个可分析的数学结构。网络或图提供了一种自然的语言来描述实体（患者）及其之间的关系（相似性）。然而，根据具体的研究问题和数据类型，选择正确的[网络表示](@entry_id:752440)至关重要。[@problem_id:4368717]

最直接的表示形式是一个**简单图** (simple graph)，其中每个节点代表一个患者，连接任意两个节点的**边 (edge)** 的**权重 (weight)** 表示他们之间的总体相似性。这种表示适用于将来自多个来源（例如，基因表达、[蛋白质组学](@entry_id:155660)、临床记录）的相似性[信息聚合](@entry_id:137588)成一个单一的标量值，从而构建一个统一的患者相似性矩阵。这个单一加权网络随后可以用于各种下游分析，如谱聚类。

然而，有时我们需要表示不同类型实体之间的关系。例如，为了发现由共同的基因异常驱动的患者亚群，我们可能希望对患者与基因之间的关联进行建模，而不是直接计算患者之间的相似性。在这种情况下，**二分图** (bipartite graph) 是一个更合适的模型。[二分图](@entry_id:262451)包含两组不相交的节点（例如，一组是患者，另一组是基因），边只存在于两组之间，表示一种关联（例如，某个患者的某个基因存在显著异常）。这种结构避免了引入直接的患者-患者边，同时允许通过[图算法](@entry_id:148535)（如社区检测）来识别共享基因特征的患者群体。

生物学关系通常比成对关系更复杂。例如，某些疾病表型可能是由三个或更多基因的变异共同作用引起的。将这种高阶关系分解为独立的成对关系会丢失关键的生物学信息。**[超图](@entry_id:270943)** (hypergraph) 通过引入**超边 (hyperedge)** 的概念解决了这个问题，超边是边的推广，可以连接任意数量的节点。在患者分层的背景下，节点仍然是患者，而一条超边可以代表所有共同携带一组特定遗传变异或满足某个复杂复合表型的患者。这保留了关系的“多对多”性质。

最后，随着多组学数据的普及，如何整合来自不同数据模态（如基因组学、[转录组学](@entry_id:139549)、[蛋白质组学](@entry_id:155660)）的信息成为一个关键挑战。直接将所有模态的相似性融合成一个单一网络可能会掩盖特定于模态的信号。**多重网络** (multiplex network) 或[多层网络](@entry_id:270365)提供了一个优雅的解决方案。在这种表示中，每一层都是一个单独的图，代表一个特定的数据模态（例如，一个基于基因表达的患者相似性网络，另一个基于蛋白质谱）。所有层共享相同的患者节点集。重要的是，**层间边 (inter-layer edges)** 连接了不同层中代表同一患者的节点。这种结构既保留了每个模态内的相似性结构，又允许开发能够利用跨层信息进行更稳健分层的方法。[@problem_id:4368717]

### 量化患者相似性：从原始数据到网络边

一旦确定了合适的[网络结构](@entry_id:265673)，下一步就是计算定义[网络拓扑](@entry_id:141407)的边权重。这个过程本身就是一个多步骤的工作流程，涉及从原始高维数据到有意义的相似性度量的转换。

#### 数据到网络的流程：一个实践工作流

以[转录组](@entry_id:274025)测序（RNA-seq）数据为例，构建一个高质量的患者相似性网络需要一个严谨的统计流程，以处理原始数据中固有的技术变异和噪声。[@problem_id:4368746]

1.  **质量控制 (Quality Control)**：分析始于原始计数矩阵。必须识别并移除低质量的样本（例如，由于测序深度不足导致的总计数极低的样本）和信息量不足的基因（例如，在绝大多数样本中不表达的基因）。在初步的[数据转换](@entry_id:170268)后，可以使用主成分分析（PCA）等方法来检测和处理异常样本。

2.  **归一化与方差稳定化转换 (Normalization and Variance-Stabilizing Transformation)**：由于文库大小和[测序深度](@entry_id:178191)的差异，原始计数值在样本之间不具有直接可比性。归一化旨在校正这些差异。像“[中位数](@entry_id:264877)比率法”(median-of-ratios method) 这样的稳健方法能够准确估计每个样本的**尺寸因子 (size factors)** $s_i$。此外，计数数据表现出**异方差性 (heteroscedasticity)**，即方差依赖于均值。对于许多依赖于距离或假设[数据近似](@entry_id:635046)正态分布的下游算法（如PCA、相关性分析），必须对数据进行转换以稳定方差。**方差稳定化转换 (Variance-Stabilizing Transformation, VST)** 专为此类计数数据设计，它能使数据在对数尺度上近似满足**[同方差性](@entry_id:634679) (homoscedasticity)**，即方差不再依赖于均值。

3.  **[批次效应校正](@entry_id:269846) (Batch Correction)**：在大型研究中，样本通常在不同的批次中处理，这会引入非生物学的系统性变异，即**批次效应 (batch effects)**。在对数转换后的数据上，这些效应通常表现为加性。可以使用线性模型，结合**[经验贝叶斯](@entry_id:171034) (Empirical Bayes)** 方法来稳健地估计和移除批次效应，同时保留真实的生物学变异。

4.  **[特征选择](@entry_id:177971) (Feature Selection)**：并非所有基因都对区分患者亚型有用。为了增强[信噪比](@entry_id:271196)，应选择那些在患者群体中表现出高生物学变异的基因。一个标准的无监督方法是识别**高变异基因 (Highly Variable Genes, HVGs)**。这涉及对所有基因的均值-方差关系进行建模，并选择那些方差显著高于模型预测的基因。

5.  **相似性计算 (Similarity Computation)**：在经过上述处理后，我们得到一个干净的数据矩阵。此时，可以计算患者间的相似性。例如，可以先将每个选定基因的表达值在所有患者中进行标准化（使其均值为0，方差为1），然后计算患者间的**皮尔逊相关系数 (Pearson correlation)**。得到的系数值域为 $[-1, 1]$，可以被转换为非负的权重（例如，通过 $W_{ij} = (r_{ij} + 1)/2$）用于网络构建。

#### 选择正确的相似性度量

相似性或[距离度量](@entry_id:636073)的选择深刻地影响着网络的最终结构。不同的度量捕捉了数据的不同几何特性。[@problem_id:4368777]

-   **[欧几里得距离](@entry_id:143990) (Euclidean distance)**: 定义为 $d_E(x, y) = \sqrt{\sum_{i=1}^p (x_i - y_i)^2}$，它测量[特征空间](@entry_id:638014)中两点间的直线距离。它对特征的绝对数值差异敏感。因此，当所有特征处于同一可比尺度且近似独立时，[欧几里得距离](@entry_id:143990)是合适的。如果特征协方差矩阵是单位矩阵 $I$，那么[欧几里得距离](@entry_id:143990)等同于[马氏距离](@entry_id:269828)。

-   **余弦相似度 (Cosine similarity)**: 定义为 $S_C(x, y) = \frac{\langle x, y \rangle}{\|x\| \|y\|}$，它测量两个向量之间的夹角余弦。余弦相似度对向量的长度（或模）不敏感，只关注它们的方向。这使得它在比较特征的相对模式（例如，基因表达谱的“形状”）时特别有用，尤其是在存在跨样本的全局强度差异（如[RNA-seq](@entry_id:140811)中的文库大小差异）时。

-   **[皮尔逊相关系数](@entry_id:270276) (Pearson correlation)**: [皮尔逊相关系数](@entry_id:270276)等价于对每个患者的特征向量进行中心化（减去均值）后计算余弦相似度。因此，它不仅对向量的缩放不敏感，还对平移不敏感。这使得它非常适合于比较特征谱的形状，同时消除每个谱自身的整体均值和方差（或标准差）的影响，对加性[批次效应](@entry_id:265859)具有稳健性。

-   **[马氏距离](@entry_id:269828) (Mahalanobis distance)**: 定义为 $d_M(x, y) = \sqrt{(x-y)^{\top} \Sigma^{-1} (x-y)}$，其中 $\Sigma$ 是特征的协方差矩阵。[马氏距离](@entry_id:269828)通过使用**[逆协方差矩阵](@entry_id:138450) (inverse covariance matrix)** $\Sigma^{-1}$ 对数据进行了“白化”变换，从而考虑了特征之间的相关性和异质方差。它有效地降低了高方差和高度相关方向上的权重。当特征之间存在复杂的依赖关系时，马氏距离是衡量患者间差异的最合适的度量。

#### 相似性的数学基础：核函数与度量

从更深层次的数学角度看，一个“好”的相似性函数 $s(x, y)$ 通常应具备某些理想的性质。其中两个最重要的概念是**正半定核 (Positive Semidefinite, PSD, kernel)** 和**度量 (metric)**。[@problem_id:4368738]

一个函数 $s(x,y)$ 被称为**正半定核**，如果它对所有输入 $x, y$ 都是对称的（即 $s(x,y) = s(y,x)$），并且对于任意有限的数据点集合 $\{x_1, \dots, x_n\}$ 和任意实数 $\{a_1, \dots, a_n\}$，都满足 $\sum_{i=1}^n \sum_{j=1}^n a_i a_j s(x_i, x_j) \ge 0$。这个条件等价于说，由 $K_{ij} = s(x_i, x_j)$ 构成的**[格拉姆矩阵](@entry_id:203297) (Gram matrix)** $K$ 是一个正半定矩阵。[@problem_id:4368738] 许多[机器学习算法](@entry_id:751585)，特别是[核方法](@entry_id:276706)，都要求相似性矩阵是PSD的，因为它保证了存在一个到希尔伯特特征空间的映射 $\phi$，使得 $s(x,y) = \langle \phi(x), \phi(y) \rangle_{\mathcal{H}}$。这允许我们将线性算法应用于非线性可分的数据。

一个函数 $d(x,y)$ 被称为**度量**，如果它满足四个公理：非负性 ($d(x,y) \ge 0$)，不可分者同一性 ($d(x,y) = 0 \iff x=y$)，对称性 ($d(x,y) = d(y,x)$)，以及三角不等式 ($d(x,z) \le d(x,y) + d(y,z)$)。

[核函数](@entry_id:145324)和度量之间存在深刻的联系。如果一个相似性函数 $s(x,y)$ 是一个PSD核，那么它可以通过 $d(x,y) = \sqrt{s(x,x) - 2s(x,y) + s(y,y)}$ 诱导出一个距离函数。这个距离实际上是在希尔伯特特征空间中对应点的[欧几里得距离](@entry_id:143990)，即 $\|\phi(x) - \phi(y)\|_{\mathcal{H}}$。如果特征映射 $\phi$ 是[单射](@entry_id:183792)的（即不同的输入点映射到不同的特征向量），或者等价地，在有限数据集上[格拉姆矩阵](@entry_id:203297) $K$ 是严格正定的，那么这个诱导出的距离 $d(x,y)$ 将满足所有四个[度量公理](@entry_id:152114)，成为一个真正的度量。[@problem_id:4368738] 然而，并非所有看似合理的相似性到距离的转换都能保证度量性质。例如，对于一个取值在 $[0,1]$ 的相似性函数 $s(x,y)$，转换 $d(x,y) = 1 - s(x,y)$ 通常不满足三角不等式，因此一般不是一个度量。

### 通过[网络结构分析](@entry_id:276819)进行患者分层

构建了患者相似性网络后，我们的目标是从其结构中提取亚群。这通常被形式化为一个[图分割](@entry_id:152532)问题。

#### [图分割](@entry_id:152532)与谱聚类

**谱聚类 (Spectral clustering)** 是一类强大的[图分割](@entry_id:152532)算法，它通过分析图的**[拉普拉斯矩阵](@entry_id:152110) (Graph Laplacian)** 的谱（即特征值和特征向量）来对节点进行聚类。

最基本的[拉普拉斯矩阵](@entry_id:152110)是**未归一化[拉普拉斯矩阵](@entry_id:152110) (unnormalized graph Laplacian)**，定义为 $L = D - W$，其中 $W$ 是加权邻接矩阵，$D$ 是对角度的矩阵 ($D_{ii} = \sum_j W_{ij}$)。$L$ 的一个重要性质是其二次型可以写为：
$$x^{\top} L x = \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} W_{ij} (x_i - x_j)^2$$
这里 $x \in \mathbb{R}^n$ 是一个在图节点上定义的信号（或标签向量）。这个二次型也被称为图的**[狄利克雷能量](@entry_id:276589) (Dirichlet energy)**。最小化 $x^{\top} L x$ 意味着寻找一个信号 $x$，使得在具有高权重边的节点上，信号值的差异尽可能小。这直观地对应于[图分割](@entry_id:152532)的目标：在簇内保持标签一致，而在簇间允许标签变化。[@problem_id:4368755]

然而，直接使用 $L$ 进行谱聚类（即寻找其特征向量）可能会导致问题，尤其是在患者网络中。这类网络通常具有高度异质的度分布，即存在一些与其他许多患者都有高度相似性的“**中心节点 (hubs)**”。使用未归一化拉普拉斯矩阵的[聚类算法](@entry_id:146720)（对应于最小化**比率切割 (RatioCut)** 目标）倾向于将这些中心节点和少量外围节点分割出来，形成非常不平衡的划分，这通常不具有生物学意义。[@problem_id:4368698]

#### 归一化切割与对称归一化拉普拉斯矩阵

为了克服上述问题，**归一化切割 (Normalized Cut, Ncut)** 被提出。对于一个将[图划分](@entry_id:152532)为两个子集 $A$ 和 $B$ 的方案，Ncut 目标函数定义为：
$$ \mathrm{Ncut}(A,B) = \frac{\mathrm{cut}(A,B)}{\mathrm{vol}(A)} + \frac{\mathrm{cut}(A,B)}{\mathrm{vol}(B)} $$
其中 $\mathrm{cut}(A,B) = \sum_{i \in A, j \in B} W_{ij}$ 是两个子集之间的总边权重，而 $\mathrm{vol}(S) = \sum_{i \in S} d_i$ 是子集 $S$ 中所有节点的度之和。通过用子集的**体积 (volume)** 来归一化切割值，Ncut 惩罚了那些将[图分割](@entry_id:152532)成体积悬殊的子集的方案，从而鼓励产生更平衡的划分。

这个目标函数有一个优美的概率解释。Ncut值等于在一个图上进行随机游走时，从子集 $A$ 一步跳出到 $B$ 的概率，与从 $B$ 一步跳出到 $A$ 的概率之和。最小化Ncut相当于寻找一个划分，使得随机游走被“困在”子集内部的概率最大化。[@problem_id:4368719]

最小化Ncut是一个[NP难问题](@entry_id:146946)，但可以通过谱方法进行松弛近似。这个松弛问题恰好对应于使用**对称归一化拉普拉斯矩阵 (symmetric normalized Laplacian)** $L_{\mathrm{sym}}$ 进行谱分析。$L_{\mathrm{sym}}$ 定义为：
$$ L_{\mathrm{sym}} = I - D^{-1/2} W D^{-1/2} $$
$L_{\mathrm{sym}}$ 的二次型可以表示为：
$$ y^{\top} L_{\mathrm{sym}} y = \frac{1}{2} \sum_{i,j=1}^n W_{ij} \left( \frac{y_i}{\sqrt{d_i}} - \frac{y_j}{\sqrt{d_j}} \right)^2 $$
与 $L$ 的二次型相比，这里的惩罚项是施加在经度缩放后的坐标 $z_i = y_i/\sqrt{d_i}$ 上的。对于一个高度数的中心节点 $i$，$d_i$ 很大，因此它的坐标 $y_i$ 的变化对总惩罚的贡献被有效减弱了。这种**度归一化 (degree normalization)** 机制平衡了不同度数节点的影响力，防止了中心节点主导嵌入结果，从而产生更均衡和有意义的患者分层。[@problem_id:4368698]

#### 一个计算示例：谱二分

为了具体说明这一过程，我们考虑一个包含4个患者的小型网络。[@problem_id:4368702] 假设其权重矩阵为：
$$ W =\begin{pmatrix} 0 & 1 & 0 & 0 \\ 1 & 0 & \frac{1}{5} & 0 \\ 0 & \frac{1}{5} & 0 & 1 \\ 0 & 0 & 1 & 0 \end{pmatrix} $$
这代表患者1和2强相关，患者3和4强相关，而患者2和3之间存在微弱的联系。

1.  **计算度矩阵 $D$**：
    $d_1 = 1$, $d_2 = 1 + 1/5 = 6/5$, $d_3 = 1/5 + 1 = 6/5$, $d_4 = 1$。
    $$ D = \mathrm{diag}(1, 6/5, 6/5, 1) $$

2.  **计算对称归一化拉普拉斯矩阵 $L_{\mathrm{sym}}$**：
    根据定义 $L_{\mathrm{sym}} = I - D^{-1/2} W D^{-1/2}$，我们得到：
    $$ L_{\mathrm{sym}} = \begin{pmatrix} 1 & -\sqrt{5/6} & 0 & 0 \\ -\sqrt{5/6} & 1 & -1/6 & 0 \\ 0 & -1/6 & 1 & -\sqrt{5/6} \\ 0 & 0 & -\sqrt{5/6} & 1 \end{pmatrix} $$

3.  **计算特征值和特征向量**：
    $L_{\mathrm{sym}}$ 的特征值按升序排列为 $\lambda_1 = 0, \lambda_2 = 1/6, \lambda_3 = 11/6, \lambda_4 = 2$。第二小的特征值 $\lambda_2 = 1/6$ 对应的特征向量，即**费德勒向量 (Fiedler vector)**，其分量的符号决定了图的最佳二分。该向量近似为 $f_2 \propto (2.45, 2.24, -2.24, -2.45)^T$。

4.  **根据费德勒向量进行划分**：
    费德勒向量的符号为 $(+, +, -, -)$。这表明最佳的二分方案是将节点分为两组：$A = \{1, 2\}$ 和 $B = \{3, 4\}$。

5.  **评估划分质量**：
    我们可以计算这个划分的Ncut值。$\mathrm{cut}(A,B) = W_{23} = 1/5$。子集的体积为 $\mathrm{vol}(A) = d_1 + d_2 = 11/5$ 和 $\mathrm{vol}(B) = d_3 + d_4 = 11/5$。
    因此，$\mathrm{Ncut}(A,B) = \frac{1/5}{11/5} + \frac{1/5}{11/5} = \frac{2}{11} \approx 0.1818$。这个值与第二小的特征值 $\lambda_2 = 1/6 \approx 0.1667$ 密切相关，验证了谱方法与Ncut目标之间的联系。

### 先进的基于网络的方法与解释

除了经典的谱聚类，更先进的基于网络的方法已经被开发出来，以应对多组学整合和[深度学习](@entry_id:142022)的挑战。

#### 使用相似性网络融合（SNF）整合[多组学](@entry_id:148370)数据

**相似性网络融合 (Similarity Network Fusion, SNF)** 是一种专门用于整合[多组学](@entry_id:148370)数据的强大方法。[@problem_id:4368722] 其核心思想是：尽管每个单一的数据模态都可能包含噪声，但跨模态共享的相似性模式是反映真实生物学信号的有力证据。SNF通过一个迭代过程来放大这种共享信号。

该流程如下：
1.  **构建模态特异性网络**：为每个数据模态（例如，mRNA表达、DNA甲基化、miRNA表达）分别构建一个患者相似性网络 $W^{(v)}$。这一步使用局部缩放的核函数来处理不同模态数据的异质性，并仅保留每个患者的K个最近邻以关注最可靠的局部相似性。
2.  **网络归一化**：将每个权重矩阵 $W^{(v)}$ 转换为一个包含[自环](@entry_id:274670)的[行随机矩阵](@entry_id:266181) $P^{(v)}$ 和一个纯粹的局部邻域矩阵 $S^{(v)}$。
3.  **迭代跨网络信息扩散**：这是SNF的关键步骤。对于每个网络，其状态在每次迭代中通过与所有其他网络的平均状态进行“通信”来更新。具体来说，第 $v$ 个网络在第 $t+1$ 次迭代的状态 $P^{(v)}_{t+1}$ 是通过将其他所有网络的平均状态 $\left(\frac{1}{m-1}\sum_{u \neq v} P^{(u)}_t\right)$ 与第 $v$ 个网络的局部结构 $S^{(v)}$ 相结合来计算的。
4.  **融合**：经过多次迭代，原本在不同模态中看起来不同的网络会变得越来越相似，因为它们之间共享的结构被反复加强。最终，将所有收敛后的网络平均起来，就得到了一个单一的、稳健的融合网络 $P_{\mathrm{fused}}$。这个融合网络可以用于后续的患者分层。

SNF的成功依赖于几个关键假设：患者在所有数据视图中被正确对齐；不同数据视图提供互补的信息；局部相似性比远距离相似性更可靠；以及迭代[扩散过程](@entry_id:170696)能够有效地区分信号和噪声。

#### 使用[图卷积网络](@entry_id:194500)（GCN）学习患者表示

近年来，以**[图卷积网络](@entry_id:194500) (Graph Convolutional Networks, GCNs)** 为代表的[几何深度学习](@entry_id:636472)方法为分析图结构数据提供了新的范式。[@problem_id:4368714] 在患者相似性网络上应用GCN，可以将每个患者的初始特征（例如，高维分子谱）转换为低维、信息量更丰富的嵌入表示，这些嵌入向量可以直接用于聚类。

GCN的核心是**[消息传递](@entry_id:751915) (message passing)**机制。在GCN的每一层，每个节点（患者）都会从其邻居那里收集特征信息，并结合自身的特征来更新自己的表示。一个典型的GCN层更新规则如下：
$$ H^{(l+1)} = \sigma\left(\tilde{D}^{-1/2} \tilde{W} \tilde{D}^{-1/2} H^{(l)} U^{(l)}\right) $$
这里，$H^{(l)}$ 是第 $l$ 层所有节点的特征矩阵，$\tilde{W} = W + I$ 是加入了自环的[邻接矩阵](@entry_id:151010)（以确保节点在更新时也考虑自身信息），$\tilde{D}$ 是 $\tilde{W}$ 对应的度矩阵，$U^{(l)}$ 是该层可训练的权重矩阵，$\sigma$ 是[非线性激活函数](@entry_id:635291)（如ReLU）。

这个传播规则中的 $\tilde{D}^{-1/2} \tilde{W} \tilde{D}^{-1/2}$ 项至关重要。它本质上是对称归一化的[拉普拉斯算子](@entry_id:262740)的一种变体，它对邻居的特征进行了加权平均。这种归一化不仅稳定了训练过程，防止了特征尺度因节点度的不同而爆炸，而且它的作用可以被看作是一种在图上的**平滑 (smoothing)** 或低通滤波操作。通过多层GCN的堆叠，信息可以在网络中传播得更远。对于患者分层而言，这意味着在[密集连接](@entry_id:634435)的模块（即潜在的亚群）内的患者会反复交换和融合他们的特征信息，使得他们的最终嵌入表示变得更加相似和内聚，从而更容易被[聚类算法](@entry_id:146720)区分开。

#### 超越聚类：图上的[半监督学习](@entry_id:636420)

图拉普拉斯矩阵的价值不仅限于无监督的聚类。它在**[半监督学习](@entry_id:636420) (Semi-Supervised Learning, SSL)** 中也扮演着核心角色。[@problem_id:4368755] 在许多临床场景中，我们可能只有一小部分患者有明确的标签（例如，已知的亚型或风险分层），而绝大多数患者是未标记的。

在基于图的SSL中，我们的目标是为所有患者预测标签。这通常通过最小化一个包含两部分的[损失函数](@entry_id:136784)来实现：一部分是拟合项，确保模型在已标记患者上的预测是准确的；另一部分是正则化项，利用图结构来约束未标记患者的预测。这个正则化项通常就是图的[狄利克雷能量](@entry_id:276589)，即 $\lambda x^{\top} L x$。

如前所述，最小化这一项会惩罚在强连接（高相似度）的患者之间出现大的标签差异。因此，该正则化项有效地将标签信息从已标记的节点“平滑”或“传播”到其邻居，并进一步扩散到整个网络。这背后的直觉是**平滑性假设**：相似的患者应该有相似的标签。通过调整正则化强度 $\lambda$，我们可以在拟合已知标签和保持图上[标签平滑](@entry_id:635060)性之间进行权衡。这种方法使得仅用少量标签就能对整个患者队列进行有效的分类或评分。

### 从相关到因果：一个批判性视角

尽管基于网络的相似性方法非常强大，但对其结果的解释需要保持审慎。一个关键的区分是**相关性网络 (correlational networks)** 和**因果图 (causal graphs)** 之间的区别。[@problem_id:4368706]

我们到目前为止讨论的患者相似性网络属于相关性网络：节点是数据点（患者），边表示观测数据（如症状向量）之间的[统计关联](@entry_id:172897)（如[皮尔逊相关](@entry_id:260880)）。在这类网络中发现的簇代表了“表型相似”的患者群体。

然而，在系统生物医学中，我们的最终目标是理解疾病的**因果机制**，并将患者分层到由不同因果路径驱动的亚型中。因果机制由因果图来描述，这通常是一个**[有向无环图](@entry_id:164045) (Directed Acyclic Graph, DAG)**，其节点代表变量（如基因、环境暴露、疾病状态），有向边代表直接的因果关系。

关联不等于因果。两个变量之间的相关性可能由多种[因果结构](@entry_id:159914)产生：直接因果关系 ($X \to Y$)，共同上游原因（即**混杂 (confounding)**, $X \leftarrow Z \to Y$)，或者间接路径 ($X \to Z \to Y$)。一个简单的相关性网络无法区分这些情况。因此，在患者相似性网络中发现的一个紧密的簇，可能仅仅是因为这些患者共享一个未被测量的混杂因素（例如，共同的生活方式或环境暴露），而不是因为他们共享同一个核心的疾病驱动机制。

因果推断理论通过**因果马尔可夫假设 (Causal Markov assumption)** 和**忠实性假设 (faithfulness assumption)** 在因果图结构和观测数据的[条件独立性](@entry_id:262650)之间建立了联系。因果马尔可夫假设指出，在因果图中，一个变量在给定其直接原因（父节点）的条件下，与其非后代节点条件独立。忠实性假设则反向断言，观测数据中存在的所有条件独立性都在因果图结构中有所体现。这些原则是试图从观测数据中推断因果关系的基础。

因此，虽然基于相似性的患者分层是发现潜在生物学模式的宝贵探索性工具，但我们必须认识到，这些模式是相关性的产物。将这些统计亚群提升到“因果亚型”的层面，需要额外的证据和更复杂的因果建模方法，这超出了单纯相似性网络的范畴。