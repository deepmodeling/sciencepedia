{"hands_on_practices": [{"introduction": "深入理解变分自编码器 (Variational Autoencoder, VAE) 的第一步是掌握其核心目标函数——证据下界 (Evidence Lower Bound, ELBO)。本练习将引导你从基本定义出发，为一个标准的线性高斯VAE推导出ELBO的解析表达式。通过这个推导 ([@problem_id:4397996])，你将清晰地看到ELBO如何由重构项和作为正则项的KL散度构成，从而为后续更复杂的应用打下坚实的数学基础。", "problem": "考虑单个组学样本，其表示为通过批量核糖核酸测序 (RNA-seq) 获得的对数归一化基因表达向量 $x \\in \\mathbb{R}^{d}$。指定一个变分自编码器 (VAE)，其潜变量为 $z \\in \\mathbb{R}^{k}$，先验分布为标准高斯分布 $p(z) = \\mathcal{N}(z; 0, I)$。生成模型（解码器）为 $p_{\\theta}(x \\mid z) = \\mathcal{N}(x; \\mu(z), \\sigma^{2} I)$，其中均值是 $z$ 的线性函数，即 $\\mu(z) = W z + b$，其中 $W \\in \\mathbb{R}^{d \\times k}$，$b \\in \\mathbb{R}^{d}$，标量观测方差 $\\sigma^{2} > 0$ 已知。推断模型（编码器）为 $q_{\\phi}(z \\mid x) = \\mathcal{N}(z; \\mu_{\\phi}(x), \\operatorname{diag}(\\sigma_{\\phi}^{2}(x)))$，其均值为 $\\mu_{\\phi}(x) \\in \\mathbb{R}^{k}$，逐元素方差为 $\\sigma_{\\phi}^{2}(x) \\in \\mathbb{R}_{>0}^{k}$。\n\n从证据下界 (ELBO) 的定义出发，即编码器下的对数似然期望与编码器和先验之间的 Kullback–Leibler 散度之差，为给定样本 $x$ 推导 ELBO 的闭式解析表达式。表达式需用 $x$, $W$, $b$, $\\sigma^{2}$, $\\mu_{\\phi}(x)$ 和 $\\sigma_{\\phi}^{2}(x)$ 以及 $d$ 和 $k$ 表示。您的推导必须从多元正态分布的概率密度和 Kullback–Leibler 散度的基本定义开始，不得使用任何简便公式。请将最终答案表示为单个闭式解析表达式。无需进行数值取整。", "solution": "证据下界 (ELBO) 定义为：\n$$\n\\mathcal{L}(x; \\theta, \\phi) = \\mathbb{E}_{q_{\\phi}(z \\mid x)}[\\log p_{\\theta}(x \\mid z)] - D_{KL}(q_{\\phi}(z \\mid x) \\| p(z))\n$$\n我们将分别推导右侧的两项。\n\n**第 1 部分：对数似然期望项**\n\n第一项是在近似后验 $q_{\\phi}(z \\mid x)$ 下数据的对数似然期望。生成模型（解码器）由 $p_{\\theta}(x \\mid z) = \\mathcal{N}(x; W z + b, \\sigma^{2} I)$ 给出。\n\n这个 $d$ 维多元正态分布的概率密度函数 (PDF) 是：\n$$\np_{\\theta}(x \\mid z) = \\frac{1}{(2\\pi)^{d/2} \\det(\\sigma^2 I)^{1/2}} \\exp\\left(-\\frac{1}{2}(x - (Wz+b))^T (\\sigma^2 I)^{-1} (x - (Wz+b))\\right)\n$$\n协方差矩阵的行列式为 $\\det(\\sigma^2 I) = (\\sigma^2)^d$。其逆矩阵为 $(\\sigma^2 I)^{-1} = \\frac{1}{\\sigma^2}I$。将这些代入可得：\n$$\np_{\\theta}(x \\mid z) = \\frac{1}{(2\\pi \\sigma^2)^{d/2}} \\exp\\left(-\\frac{1}{2\\sigma^2}\\|x - Wz - b\\|_2^2\\right)\n$$\n取自然对数，我们得到：\n$$\n\\log p_{\\theta}(x \\mid z) = -\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\|x - Wz - b\\|_2^2\n$$\n现在，我们必须对该量求关于编码器分布 $q_{\\phi}(z \\mid x) = \\mathcal{N}(z; \\mu_{\\phi}(x), \\operatorname{diag}(\\sigma_{\\phi}^{2}(x)))$ 的期望。为简化符号，令 $\\mu_q = \\mu_{\\phi}(x)$ 和 $\\Sigma_q = \\operatorname{diag}(\\sigma_{\\phi}^{2}(x))$。\n$$\n\\mathbb{E}_{q_{\\phi}(z \\mid x)}[\\log p_{\\theta}(x \\mid z)] = \\mathbb{E}_{q_{\\phi}}\\left[-\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\|x - Wz - b\\|_2^2\\right]\n$$\n根据期望的线性性质，这变为：\n$$\n\\mathbb{E}_{q_{\\phi}}[\\log p_{\\theta}(x \\mid z)] = -\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\mathbb{E}_{q_{\\phi}}[\\|x - Wz - b\\|_2^2]\n$$\n我们需要计算平方范数的期望。让我们展开期望内的项：\n$$\n\\|x - Wz - b\\|_2^2 = (x - b - Wz)^T(x - b - Wz) = (x-b)^T(x-b) - 2(x-b)^T W z + z^T W^T W z\n$$\n现在，我们对 $z \\sim q_{\\phi}$ 求期望：\n$$\n\\mathbb{E}_{q_{\\phi}}[\\|x - Wz - b\\|_2^2] = \\mathbb{E}_{q_{\\phi}}[(x-b)^T(x-b)] - 2(x-b)^T W \\mathbb{E}_{q_{\\phi}}[z] + \\mathbb{E}_{q_{\\phi}}[z^T W^T W z]\n$$\n我们知道 $\\mathbb{E}_{q_{\\phi}}[z] = \\mu_q$。对于二次项，我们使用恒等式 $\\mathbb{E}[y^T A y] = \\text{Tr}(A \\text{Cov}[y]) + \\mathbb{E}[y]^T A \\mathbb{E}[y]$。这里，$y=z$，$A = W^T W$，$\\mathbb{E}[z]=\\mu_q$，$\\text{Cov}[z]=\\Sigma_q$。\n$$\n\\mathbb{E}_{q_{\\phi}}[z^T W^T W z] = \\text{Tr}(W^T W \\Sigma_q) + \\mu_q^T W^T W \\mu_q\n$$\n将这些代回，期望平方范数为：\n$$\n\\mathbb{E}_{q_{\\phi}}[\\|x - Wz - b\\|_2^2] = \\|x-b\\|_2^2 - 2(x-b)^T W \\mu_q + \\mu_q^T W^T W \\mu_q + \\text{Tr}(W^T W \\Sigma_q)\n$$\n前三项可以重新组合成一个平方范数：\n$$\n\\|x-b\\|_2^2 - 2(x-b)^T W \\mu_q + \\|W\\mu_q\\|_2^2 = \\|(x-b) - W\\mu_q\\|_2^2 = \\|x - W\\mu_q - b\\|_2^2\n$$\n所以，我们有：\n$$\n\\mathbb{E}_{q_{\\phi}}[\\|x - Wz - b\\|_2^2] = \\|x - W\\mu_q - b\\|_2^2 + \\text{Tr}(W^T W \\Sigma_q)\n$$\n迹项可以明确地写出。令 $w_j$ 为 $W$ 的第 $j$ 列，$(\\sigma_q^2)_j$ 为方差向量 $\\sigma_{\\phi}^2(x)$ 的第 $j$ 个分量。\n$$\n\\text{Tr}(W^T W \\Sigma_q) = \\sum_{j=1}^k (W^T W \\Sigma_q)_{jj} = \\sum_{j=1}^k \\left(\\sum_{l=1}^k (W^T W)_{jl} (\\Sigma_q)_{lj}\\right)\n$$\n由于 $\\Sigma_q$ 是对角矩阵，$(\\Sigma_q)_{lj} = (\\sigma_q^2)_j \\delta_{lj}$。\n$$\n\\text{Tr}(W^T W \\Sigma_q) = \\sum_{j=1}^k (W^T W)_{jj} (\\sigma_q^2)_j = \\sum_{j=1}^k \\left(\\sum_{i=1}^d W_{ij}^2\\right) (\\sigma_q^2)_j = \\sum_{j=1}^k \\|w_j\\|_2^2 (\\sigma_q^2)_j\n$$\n最后，将其代回得到 ELBO 的第一项：\n$$\n\\mathbb{E}_{q_{\\phi}(z \\mid x)}[\\log p_{\\theta}(x \\mid z)] = -\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\left( \\|x - W\\mu_{\\phi}(x) - b\\|_2^2 + \\sum_{j=1}^k \\|w_j\\|_2^2 (\\sigma_{\\phi}^2(x))_j \\right)\n$$\n\n**第 2 部分：Kullback-Leibler 散度项**\n\n第二项是编码器分布 $q_{\\phi}(z \\mid x) = \\mathcal{N}(z; \\mu_q, \\Sigma_q)$ 与先验分布 $p(z) = \\mathcal{N}(z; 0, I)$ 之间的 KL 散度。\nKL 散度定义为：\n$$\nD_{KL}(q_{\\phi} \\| p) = \\int q_{\\phi}(z \\mid x) \\log \\frac{q_{\\phi}(z \\mid x)}{p(z)} dz = \\mathbb{E}_{q_{\\phi}}[\\log q_{\\phi}(z \\mid x) - \\log p(z)]\n$$\n由于 $q_{\\phi}$ 和 $p$ 都是对角协方差高斯分布，它们可以在 $z$ 的维度上分解。因此，KL 散度是每个维度的 KL 散度之和：\n$$\nD_{KL}(q_{\\phi} \\| p) = \\sum_{j=1}^k D_{KL}(\\mathcal{N}(z_j; (\\mu_q)_j, (\\sigma_q^2)_j) \\| \\mathcal{N}(z_j; 0, 1))\n$$\n对于单个维度 $j$，我们有 $q_j(z_j) = \\mathcal{N}(z_j; (\\mu_q)_j, (\\sigma_q^2)_j)$ 和 $p_j(z_j) = \\mathcal{N}(z_j; 0, 1)$。它们的对数概率密度函数为：\n$$\n\\log q_j(z_j) = -\\frac{1}{2}\\log(2\\pi(\\sigma_q^2)_j) - \\frac{(z_j - (\\mu_q)_j)^2}{2(\\sigma_q^2)_j}\n$$\n$$\n\\log p_j(z_j) = -\\frac{1}{2}\\log(2\\pi) - \\frac{z_j^2}{2}\n$$\n差值为：\n$$\n\\log q_j(z_j) - \\log p_j(z_j) = -\\frac{1}{2}\\log((\\sigma_q^2)_j) - \\frac{(z_j - (\\mu_q)_j)^2}{2(\\sigma_q^2)_j} + \\frac{z_j^2}{2}\n$$\n对 $z_j \\sim q_j(z_j)$ 求期望：\n$$\nD_{KL}(q_j \\| p_j) = \\mathbb{E}_{q_j}\\left[ -\\frac{1}{2}\\log((\\sigma_q^2)_j) - \\frac{(z_j - (\\mu_q)_j)^2}{2(\\sigma_q^2)_j} + \\frac{z_j^2}{2} \\right]\n$$\n使用期望的线性性质：\n$$\nD_{KL}(q_j \\| p_j) = -\\frac{1}{2}\\log((\\sigma_q^2)_j) - \\frac{1}{2(\\sigma_q^2)_j}\\mathbb{E}_{q_j}[(z_j - (\\mu_q)_j)^2] + \\frac{1}{2}\\mathbb{E}_{q_j}[z_j^2]\n$$\n我们有 $\\mathbb{E}_{q_j}[(z_j - (\\mu_q)_j)^2] = \\text{Var}_{q_j}(z_j) = (\\sigma_q^2)_j$，且 $\\mathbb{E}_{q_j}[z_j^2] = \\text{Var}_{q_j}(z_j) + (\\mathbb{E}_{q_j}[z_j])^2 = (\\sigma_q^2)_j + ((\\mu_q)_j)^2$。\n$$\nD_{KL}(q_j \\| p_j) = -\\frac{1}{2}\\log((\\sigma_q^2)_j) - \\frac{(\\sigma_q^2)_j}{2(\\sigma_q^2)_j} + \\frac{(\\sigma_q^2)_j + ((\\mu_q)_j)^2}{2}\n$$\n$$\nD_{KL}(q_j \\| p_j) = \\frac{1}{2} \\left[ -\\log((\\sigma_q^2)_j) - 1 + (\\sigma_q^2)_j + ((\\mu_q)_j)^2 \\right]\n$$\n对所有 $k$ 个维度求和：\n$$\nD_{KL}(q_{\\phi}(z \\mid x) \\| p(z)) = \\frac{1}{2} \\sum_{j=1}^k \\left( ((\\mu_{\\phi}(x))_j)^2 + (\\sigma_{\\phi}^2(x))_j - \\log((\\sigma_{\\phi}^2(x))_j) - 1 \\right)\n$$\n\n**ELBO 的最终表达式**\n\n结合两部分，ELBO = (第 1 部分) - (第 2 部分)：\n$$\n\\mathcal{L}(x; \\theta, \\phi) = \\left[ -\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\left( \\|x - W\\mu_{\\phi}(x) - b\\|_2^2 + \\sum_{j=1}^k \\|w_j\\|_2^2 (\\sigma_{\\phi}^2(x))_j \\right) \\right] - \\left[ \\frac{1}{2} \\sum_{j=1}^k \\left( ((\\mu_{\\phi}(x))_j)^2 + (\\sigma_{\\phi}^2(x))_j - \\log((\\sigma_{\\phi}^2(x))_j) - 1 \\right) \\right]\n$$\n重新整理各项得到最终的单一表达式：\n$$\n\\mathcal{L}(x; \\theta, \\phi) = -\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\|x - W\\mu_{\\phi}(x) - b\\|_2^2 - \\frac{1}{2\\sigma^2}\\sum_{j=1}^k \\|w_j\\|_2^2 (\\sigma_{\\phi}^2(x))_j + \\frac{1}{2} \\sum_{j=1}^k \\left(1 - ((\\mu_{\\phi}(x))_j)^2 - (\\sigma_{\\phi}^2(x))_j + \\log((\\sigma_{\\phi}^2(x))_j) \\right)\n$$\n其中 $w_j$ 是矩阵 $W$ 的第 $j$ 列。", "answer": "$$ \\boxed{-\\frac{d}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\|x - W\\mu_{\\phi}(x) - b\\|_2^2 - \\frac{1}{2\\sigma^2}\\sum_{j=1}^k \\|w_j\\|_2^2 (\\sigma_{\\phi}^2(x))_j + \\frac{1}{2} \\sum_{j=1}^k \\left(1 + \\log((\\sigma_{\\phi}^2(x))_j) - (\\mu_{\\phi}(x))_j^2 - (\\sigma_{\\phi}^2(x))_j \\right)} $$", "id": "4397996"}, {"introduction": "在推导出完整的ELBO之后，我们将聚焦于其关键的正则化部分——KL散度。本练习 ([@problem_id:4397947]) 提供了一个具体的数值案例，让你亲手计算编码器输出的近似后验与先验分布之间的KL散度值。这个实践不仅能让你对VAE训练中的数值有更直观的感受，还引入了重要的β-VAE概念，帮助你理解如何调整模型以平衡重构质量和潜空间的结构。", "problem": "考虑一个在系统生物医学流程中，使用单细胞转录组数据（单细胞RNA测序，scRNA-seq）训练的变分自编码器（Variational Autoencoder, VAE）。对于一个表达向量为 $x$ 的特定细胞，编码器输出潜变量 $z \\in \\mathbb{R}^{d}$ 的一个对角高斯近似后验，其形式为 $q_{\\phi}(z \\mid x) = \\mathcal{N}\\!\\left(\\mu_{\\phi}(x), \\mathrm{diag}\\left(\\sigma_{\\phi}(x)^{2}\\right)\\right)$，其中 $\\mu_{\\phi}(x) \\in \\mathbb{R}^{d}$ 是均值向量，$\\sigma_{\\phi}(x) \\in \\mathbb{R}^{d}$ 的元素严格为正，并被解释为标准差。先验是标准多元正态分布 $p(z) = \\mathcal{N}(0, I)$。VAE的证据下界（Evidence Lower Bound, ELBO）包含一个由 Kullback–Leibler 散度（Kullback–Leibler divergence, KL）$D_{\\mathrm{KL}}\\!\\left(q_{\\phi}(z \\mid x) \\,\\|\\, p(z)\\right)$ 给出的正则化项，而在 $\\beta$-VAE 中，这个KL项会乘以一个正标量权重 $\\beta > 0$。\n\n从 Kullback–Leibler 散度的定义\n$$\nD_{\\mathrm{KL}}\\!\\left(q \\,\\|\\, p\\right) \\equiv \\int q(z) \\,\\log\\!\\left(\\frac{q(z)}{p(z)}\\right) \\,\\mathrm{d}z = \\mathbb{E}_{q}\\!\\left[\\log q(z) - \\log p(z)\\right],\n$$\n和经过充分检验的对角协方差多元正态密度形式出发，推导 $D_{\\mathrm{KL}}\\!\\left(q_{\\phi}(z \\mid x) \\,\\|\\, p(z)\\right)$ 关于 $\\mu_{\\phi}(x)$ 和 $\\sigma_{\\phi}(x)$ 的闭式解析表达式。然后，解释在 $\\beta$ 加权下，KL 项对 $\\beta$-VAE 目标函数的贡献如何变化，将 $\\beta$ 加权的 KL 项表示为 $\\beta$ 的函数。\n\n对于一个特定细胞，假设学习到的编码器在维度 $d=4$ 的潜空间中输出以下参数：\n$$\n\\mu_{\\phi}(x) = \\begin{pmatrix} 0.8 \\\\ -0.25 \\\\ 0.0 \\\\ 1.1 \\end{pmatrix}, \\quad\n\\sigma_{\\phi}(x) = \\begin{pmatrix} 1.0 \\\\ 0.6 \\\\ 1.3 \\\\ 0.7 \\end{pmatrix}.\n$$\n使用自然对数，计算并简化该细胞的 $\\beta$ 加权的 KL 贡献，并将最终答案表示为关于 $\\beta$ 的单个闭式解析表达式。无需四舍五入，且不得引入任何百分号或单位。请以简化的精确形式表示最终答案。", "solution": "我们从近似后验 $q_{\\phi}(z \\mid x)$ 和先验 $p(z)$ 之间的 Kullback–Leibler 散度（Kullback–Leibler divergence, KL）的定义开始：\n$$\nD_{\\mathrm{KL}}\\!\\left(q_{\\phi}(z \\mid x) \\,\\|\\, p(z)\\right) = \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[\\log q_{\\phi}(z \\mid x) - \\log p(z)\\right].\n$$\n一个均值为 $\\mu$、对角协方差为 $\\mathrm{diag}\\!\\left(\\sigma^{2}\\right)$ 的 $d$ 维多元正态分布的对数密度的成熟公式是\n$$\n\\log \\mathcal{N}(z \\mid \\mu, \\mathrm{diag}(\\sigma^{2})) = -\\frac{d}{2}\\log(2\\pi) - \\frac{1}{2}\\sum_{i=1}^{d}\\log(\\sigma_{i}^{2}) - \\frac{1}{2}\\sum_{i=1}^{d}\\frac{(z_{i} - \\mu_{i})^{2}}{\\sigma_{i}^{2}}.\n$$\n对于标准正态先验 $p(z) = \\mathcal{N}(0, I)$，其对数密度为\n$$\n\\log \\mathcal{N}(z \\mid 0, I) = -\\frac{d}{2}\\log(2\\pi) - \\frac{1}{2}\\sum_{i=1}^{d} z_{i}^{2}.\n$$\n因此，\n\\begin{align*}\n\\log q_{\\phi}(z \\mid x) - \\log p(z)\n= \\left[-\\frac{d}{2}\\log(2\\pi) - \\frac{1}{2}\\sum_{i=1}^{d}\\log(\\sigma_{i}^{2}) - \\frac{1}{2}\\sum_{i=1}^{d}\\frac{(z_{i} - \\mu_{i})^{2}}{\\sigma_{i}^{2}}\\right] \\\\\n\\quad - \\left[-\\frac{d}{2}\\log(2\\pi) - \\frac{1}{2}\\sum_{i=1}^{d} z_{i}^{2}\\right] \\\\\n= -\\frac{1}{2}\\sum_{i=1}^{d}\\log(\\sigma_{i}^{2}) - \\frac{1}{2}\\sum_{i=1}^{d}\\frac{(z_{i} - \\mu_{i})^{2}}{\\sigma_{i}^{2}} + \\frac{1}{2}\\sum_{i=1}^{d} z_{i}^{2}.\n\\end{align*}\n对 $q_{\\phi}(z \\mid x)$ 取期望，并使用高斯分布 $z \\sim \\mathcal{N}(\\mu, \\mathrm{diag}(\\sigma^{2}))$ 的标准恒等式，\n$$\n\\mathbb{E}_{q}\\!\\left[(z_{i} - \\mu_{i})^{2}\\right] = \\sigma_{i}^{2}, \\quad \\mathbb{E}_{q}\\!\\left[z_{i}^{2}\\right] = \\mu_{i}^{2} + \\sigma_{i}^{2},\n$$\n我们得到\n\\begin{align*}\nD_{\\mathrm{KL}}\\!\\left(q_{\\phi}(z \\mid x) \\,\\|\\, p(z)\\right)\n= -\\frac{1}{2}\\sum_{i=1}^{d}\\log(\\sigma_{i}^{2}) - \\frac{1}{2}\\sum_{i=1}^{d}\\frac{\\mathbb{E}_{q}\\!\\left[(z_{i} - \\mu_{i})^{2}\\right]}{\\sigma_{i}^{2}} + \\frac{1}{2}\\sum_{i=1}^{d}\\mathbb{E}_{q}\\!\\left[z_{i}^{2}\\right] \\\\\n= -\\frac{1}{2}\\sum_{i=1}^{d}\\log(\\sigma_{i}^{2}) - \\frac{1}{2}\\sum_{i=1}^{d}\\frac{\\sigma_{i}^{2}}{\\sigma_{i}^{2}} + \\frac{1}{2}\\sum_{i=1}^{d}\\left(\\mu_{i}^{2} + \\sigma_{i}^{2}\\right) \\\\\n= -\\frac{1}{2}\\sum_{i=1}^{d}\\log(\\sigma_{i}^{2}) - \\frac{1}{2}\\sum_{i=1}^{d} 1 + \\frac{1}{2}\\sum_{i=1}^{d}\\mu_{i}^{2} + \\frac{1}{2}\\sum_{i=1}^{d}\\sigma_{i}^{2} \\\\\n= \\frac{1}{2}\\sum_{i=1}^{d}\\left(\\sigma_{i}^{2} + \\mu_{i}^{2} - 1 - \\log(\\sigma_{i}^{2})\\right).\n\\end{align*}\n这是从第一性原理推导出的，对角高斯近似后验与标准正态先验之间的 KL 散度的闭式表达式。\n\n在 $\\beta$-VAE 中，证据下界 (ELBO) 中的 KL 项乘以一个正标量 $\\beta > 0$。因此，$\\beta$ 加权的 KL 贡献为\n$$\n\\beta \\, D_{\\mathrm{KL}}\\!\\left(q_{\\phi}(z \\mid x) \\,\\|\\, p(z)\\right) = \\frac{\\beta}{2}\\sum_{i=1}^{d}\\left(\\sigma_{i}^{2} + \\mu_{i}^{2} - 1 - \\log(\\sigma_{i}^{2})\\right).\n$$\n这种缩放对 $\\beta$ 是线性的，因此增加 $\\beta$ 会按比例增加 KL 正则化器相对于 ELBO 中重构项的贡献，从而加强了匹配先验的压力，并以牺牲重构保真度为代价，鼓励潜变量使用中更多的解耦或稀疏性。\n\n现在我们为给定的细胞计算具体值，其中 $d=4$，$\\mu_{\\phi}(x) = (0.8, -0.25, 0.0, 1.1)^{\\top}$，且 $\\sigma_{\\phi}(x) = (1.0, 0.6, 1.3, 0.7)^{\\top}$。首先，我们列出各项的平方：\n\\begin{align*}\n\\mu_{1}^{2} = (0.8)^{2} = \\frac{16}{25} = 0.64,  \\sigma_{1}^{2} = (1.0)^{2} = 1, \\\\\n\\mu_{2}^{2} = (-0.25)^{2} = \\frac{1}{16} = 0.0625,  \\sigma_{2}^{2} = (0.6)^{2} = \\frac{9}{25} = 0.36, \\\\\n\\mu_{3}^{2} = (0.0)^{2} = 0,  \\sigma_{3}^{2} = (1.3)^{2} = \\frac{169}{100} = 1.69, \\\\\n\\mu_{4}^{2} = (1.1)^{2} = \\frac{121}{100} = 1.21,  \\sigma_{4}^{2} = (0.7)^{2} = \\frac{49}{100} = 0.49.\n\\end{align*}\n计算非对数项的和：\n\\begin{align*}\n\\sum_{i=1}^{4} \\left(\\sigma_{i}^{2} + \\mu_{i}^{2} - 1\\right)\n= \\left(1 + 0.64 - 1\\right) + \\left(0.36 + 0.0625 - 1\\right) + \\left(1.69 + 0 - 1\\right) + \\left(0.49 + 1.21 - 1\\right) \\\\\n= 0.64 + \\left(-0.5775\\right) + 0.69 + 0.70 \\\\\n= \\frac{581}{400} = 1.4525.\n\\end{align*}\n计算对数项的和：\n\\begin{align*}\n\\sum_{i=1}^{4} \\log\\!\\left(\\sigma_{i}^{2}\\right)\n= \\log(1) + \\log\\!\\left(\\frac{9}{25}\\right) + \\log\\!\\left(\\frac{169}{100}\\right) + \\log\\!\\left(\\frac{49}{100}\\right) \\\\\n= 0 + \\log\\!\\left(\\frac{9}{25}\\right) + \\log\\!\\left(\\frac{169}{100}\\right) + \\log\\!\\left(\\frac{49}{100}\\right) \\\\\n= \\log\\!\\left(\\frac{9 \\cdot 169 \\cdot 49}{25 \\cdot 100 \\cdot 100}\\right) \\\\\n= \\log\\!\\left(\\frac{74529}{250000}\\right).\n\\end{align*}\n因此，未加权的 KL 散度等于\n$$\nD_{\\mathrm{KL}}\\!\\left(q_{\\phi}(z \\mid x) \\,\\|\\, p(z)\\right) = \\frac{1}{2}\\left(\\frac{581}{400} - \\log\\!\\left(\\frac{74529}{250000}\\right)\\right).\n$$\n乘以 $\\beta$ 得到 $\\beta$ 加权的 KL 贡献\n$$\n\\beta \\, D_{\\mathrm{KL}}\\!\\left(q_{\\phi}(z \\mid x) \\,\\|\\, p(z)\\right) = \\frac{\\beta}{2}\\left(\\frac{581}{400} - \\log\\!\\left(\\frac{74529}{250000}\\right)\\right).\n$$\n这个表达式对 $\\beta$ 是线性的，因此它关于 $\\beta$ 的变化率是恒定的，等于未加权的 $D_{\\mathrm{KL}}\\!\\left(q_{\\phi}(z \\mid x) \\,\\|\\, p(z)\\right)$，这反映了随着 $\\beta$ 的增长，正则化强度成比例增加。\n\n所要求的最终答案即为上述关于 $\\beta$ 的简化闭式解析表达式。", "answer": "$$\\boxed{\\frac{\\beta}{2}\\left(\\frac{581}{400}-\\log\\left(\\frac{74529}{250000}\\right)\\right)}$$", "id": "4397947"}, {"introduction": "最后一个练习将理论付诸实践，解决系统生物医学中的一个核心问题：对实验协变量（如批次效应和处理条件）进行建模，并进行反事实预测。你将通过编程实现一个条件变分自编码器 (Conditional VAE, CVAE)，它能够学习数据中有条件的变异 ([@problem_id:4397973])。这项练习展示了VAE如何被用来回答关键的“如果…会怎样？”问题，例如预测一个细胞在接受不同处理后的基因表达谱，这是计算生物学中的一个重要应用。", "problem": "给定一个线性高斯条件变分自编码器 (CVAE)，它适用于处理如基因表达等高维生物测量数据。该 CVAE 以已知的批次标签和处理指标为条件，这些信息被编码在一个条件向量中。目标是针对几个测试用例，计算在观测条件下对应的证据下界 (ELBO)，以及在改变后的条件下对应的反事实重构均值。您的实现必须是一个完整、可运行的程序，仅使用提供的常量和测试输入，基于第一性原理执行确定性计算，并以指定格式打印所需的输出。\n\n模型定义与假设：\n- 潜变量的先验分布为标准正态分布：$p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z}\\,;\\,\\mathbf{0}, \\mathbf{I})$，其中潜变量维度 $L = 2$。\n- 似然是带有线性解码器的条件高斯分布：\n  $$p(\\mathbf{x}\\mid \\mathbf{z}, \\mathbf{c}) = \\mathcal{N}\\!\\left(\\mathbf{x}\\,;\\, \\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{c} + \\mathbf{b},\\, \\sigma_x^2 \\mathbf{I}\\right),$$\n  其中数据维度为 $D = 4$，条件维度为 $C = 3$。\n- 近似后验（编码器）是一个对角高斯分布，其参数是 $(\\mathbf{x}, \\mathbf{c})$ 的仿射函数：\n  $$q(\\mathbf{z}\\mid \\mathbf{x}, \\mathbf{c}) = \\mathcal{N}\\!\\left(\\mathbf{z}\\,;\\, \\boldsymbol{\\mu}_z(\\mathbf{x},\\mathbf{c}),\\, \\operatorname{diag}(\\boldsymbol{\\sigma}^2_z(\\mathbf{x},\\mathbf{c}))\\right),$$\n  其中\n  $$\\boldsymbol{\\mu}_z(\\mathbf{x},\\mathbf{c}) = \\mathbf{M}\\mathbf{x} + \\mathbf{V}\\mathbf{c} + \\mathbf{a}, \\quad \\log \\boldsymbol{\\sigma}^2_z(\\mathbf{x},\\mathbf{c}) = \\mathbf{S}_x \\mathbf{x} + \\mathbf{S}_c \\mathbf{c} + \\mathbf{s}_0,$$\n  其中对数是逐元素应用的。\n\n所有参数都是固定的，并在下面给出。除非另有说明，向量均为列向量。\n\n待使用的参数值：\n- 解码器参数：\n  - $$\\mathbf{W} = \\begin{bmatrix}\n  0.8  -0.3\\\\\n  0.1  0.5\\\\\n  -0.4  0.2\\\\\n  0.0  0.3\n  \\end{bmatrix}, \\quad\n  \\mathbf{U} = \\begin{bmatrix}\n  0.6  -0.2  0.3\\\\\n  -0.1  0.4  -0.5\\\\\n  0.2  0.1  -0.2\\\\\n  0.5  -0.3  0.2\n  \\end{bmatrix}, \\quad\n  \\mathbf{b} = \\begin{bmatrix} 0.05\\\\ -0.02\\\\ 0.01\\\\ 0.0 \\end{bmatrix}, \\quad \\sigma_x = 0.1.$$\n- 编码器参数：\n  - $$\\mathbf{M} = \\begin{bmatrix}\n  0.2  0.0  -0.1  0.3\\\\\n  -0.2  0.1  0.4  -0.1\n  \\end{bmatrix}, \\quad\n  \\mathbf{V} = \\begin{bmatrix}\n  0.1  -0.2  0.3\\\\\n  0.0  0.2  -0.1\n  \\end{bmatrix}, \\quad\n  \\mathbf{a} = \\begin{bmatrix} 0.05\\\\ -0.05 \\end{bmatrix},$$\n  $$\\mathbf{S}_x = \\begin{bmatrix}\n  -0.1  0.2  0.0  -0.2\\\\\n  0.1  -0.1  0.3  0.0\n  \\end{bmatrix}, \\quad\n  \\mathbf{S}_c = \\begin{bmatrix}\n  0.05  0.05  -0.1\\\\\n  -0.05  0.1  0.0\n  \\end{bmatrix}, \\quad\n  \\mathbf{s}_0 = \\begin{bmatrix} -1.0\\\\ -1.2 \\end{bmatrix}.$$\n\n条件向量的构造：\n- 条件向量 $\\mathbf{c} \\in \\mathbb{R}^3$ 按如下方式拼接批次标签和处理指标：前两个条目是两个批次的独热编码指标（A批次为 $[1,0]^\\top$，B批次为 $[0,1]^\\top$），第三个条目是处理指标，取值为 $\\{0,1\\}$。\n\n需要计算的量：\n- 对于每个具有观测对 $(\\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})$ 和改变后的反事实条件 $\\mathbf{c}_{\\mathrm{alt}}$ 的测试用例，计算：\n  1. 观测对的证据下界 (ELBO)：\n     $$\\mathcal{L}(\\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}}) = \\mathbb{E}_{q(\\mathbf{z}\\mid \\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})}\\!\\left[\\log p(\\mathbf{x}_{\\mathrm{obs}} \\mid \\mathbf{z}, \\mathbf{c}_{\\mathrm{obs}})\\right] - \\mathrm{KL}\\!\\left(q(\\mathbf{z}\\mid \\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})\\,\\|\\, p(\\mathbf{z})\\right).$$\n     您必须使用线性高斯恒等式以闭式形式计算期望，不得使用蒙特卡洛采样。\n  2. 在改变后的条件下的反事实重构均值：\n     $$\\mathbb{E}_{q(\\mathbf{z}\\mid \\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})}\\!\\left[\\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{c}_{\\mathrm{alt}} + \\mathbf{b}\\right],$$\n     同样以闭式形式计算。\n\n可作为起点的数学事实：\n- 对于高斯似然 $p(\\mathbf{x}\\mid \\mathbf{z}) = \\mathcal{N}(\\mathbf{x}; \\mathbf{A}\\mathbf{z}+\\boldsymbol{\\mu}, \\sigma^2 \\mathbf{I})$ 和对角高斯分布 $q(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z}; \\boldsymbol{\\mu}_z, \\operatorname{diag}(\\boldsymbol{\\sigma}^2_z))$，期望为\n  $$\\mathbb{E}_q\\!\\left[\\lVert \\mathbf{x} - (\\mathbf{A}\\mathbf{z}+\\boldsymbol{\\mu}) \\rVert_2^2\\right] = \\lVert \\mathbf{x} - (\\mathbf{A}\\boldsymbol{\\mu}_z+\\boldsymbol{\\mu}) \\rVert_2^2 + \\operatorname{tr}\\!\\left(\\mathbf{A}\\,\\operatorname{diag}(\\boldsymbol{\\sigma}^2_z)\\,\\mathbf{A}^\\top\\right).$$\n- 对角高斯分布 $q(\\mathbf{z}) = \\mathcal{N}(\\boldsymbol{\\mu}_z, \\operatorname{diag}(\\boldsymbol{\\sigma}^2_z))$ 与标准正态分布 $p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ 之间的Kullback–Leibler散度为\n  $$\\mathrm{KL}(q\\|p) = \\tfrac{1}{2} \\sum_{i=1}^{L} \\left( \\sigma_{z,i}^2 + \\mu_{z,i}^2 - 1 - \\log \\sigma_{z,i}^2 \\right).$$\n\n测试套件：\n- 使用以下三个测试用例。对于每个用例，都给定了 $\\mathbf{x}_{\\mathrm{obs}}$、$\\mathbf{c}_{\\mathrm{obs}}$ 和 $\\mathbf{c}_{\\mathrm{alt}}$。\n  - 用例1（正常路径）：\n    $$\\mathbf{x}_{\\mathrm{obs}}^{(1)} = \\begin{bmatrix} 1.2\\\\ 0.3\\\\ -0.5\\\\ 0.0 \\end{bmatrix}, \\quad \\mathbf{c}_{\\mathrm{obs}}^{(1)} = \\begin{bmatrix} 1\\\\ 0\\\\ 1 \\end{bmatrix}, \\quad \\mathbf{c}_{\\mathrm{alt}}^{(1)} = \\begin{bmatrix} 1\\\\ 0\\\\ 0 \\end{bmatrix}.$$\n  - 用例2（批次反事实）：\n    $$\\mathbf{x}_{\\mathrm{obs}}^{(2)} = \\begin{bmatrix} -0.2\\\\ 0.5\\\\ 1.0\\\\ -1.0 \\end{bmatrix}, \\quad \\mathbf{c}_{\\mathrm{obs}}^{(2)} = \\begin{bmatrix} 0\\\\ 1\\\\ 0 \\end{bmatrix}, \\quad \\mathbf{c}_{\\mathrm{alt}}^{(2)} = \\begin{bmatrix} 1\\\\ 0\\\\ 0 \\end{bmatrix}.$$\n  - 用例3（零输入的边界情况）：\n    $$\\mathbf{x}_{\\mathrm{obs}}^{(3)} = \\begin{bmatrix} 0.0\\\\ 0.0\\\\ 0.0\\\\ 0.0 \\end{bmatrix}, \\quad \\mathbf{c}_{\\mathrm{obs}}^{(3)} = \\begin{bmatrix} 0\\\\ 1\\\\ 1 \\end{bmatrix}, \\quad \\mathbf{c}_{\\mathrm{alt}}^{(3)} = \\begin{bmatrix} 1\\\\ 0\\\\ 1 \\end{bmatrix}.$$\n\n所需输出：\n- 对于每个用例 $k \\in \\{1,2,3\\}$，计算：\n  - 标量ELBO值 $\\mathcal{L}(\\mathbf{x}_{\\mathrm{obs}}^{(k)}, \\mathbf{c}_{\\mathrm{obs}}^{(k)})$。\n  - 在 $\\mathbf{c}_{\\mathrm{alt}}^{(k)}$ 条件下的 $D$ 维反事实重构均值向量。\n- 您的程序应生成单行输出，其中包含一个含三个元素的列表，每个元素对应一个测试用例。每个元素本身是一个双元素列表，形式为 $[\\text{ELBO}, [\\text{cf}_1,\\dots,\\text{cf}_D]]$，所有浮点数需四舍五入到恰好 $6$ 位小数。例如：\n  $$[[\\ell_1,[r_{1,1},r_{1,2},r_{1,3},r_{1,4}]],[\\ell_2,[\\dots]],[\\ell_3,[\\dots]]].$$\n\n实现约束：\n- 不允许使用随机性或采样；仅可使用基于给定线性高斯恒等式的闭式期望。\n- 代码必须完全自包含，且不得读取任何输入。\n- 仅使用Python标准库和指定的数值库。", "solution": "目标是为每个测试用例计算两个量，每个测试用例提供一个观测数据向量 $\\mathbf{x}_{\\mathrm{obs}}$、一个观测条件向量 $\\mathbf{c}_{\\mathrm{obs}}$ 和一个反事实条件向量 $\\mathbf{c}_{\\mathrm{alt}}$。这些量是：\n1. ELBO, $\\mathcal{L}(\\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})$。\n2. 反事实重构均值，$\\mathbb{E}_{q(\\mathbf{z}\\mid \\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})}\\!\\left[\\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{c}_{\\mathrm{alt}} + \\mathbf{b}\\right]$。\n\n首先，对于给定的数据对 $(\\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})$，我们必须确定近似后验分布 $q(\\mathbf{z}\\mid \\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})$ 的参数。问题将此定义为一个对角高斯分布，$q(\\mathbf{z}\\mid \\mathbf{x}, \\mathbf{c}) = \\mathcal{N}\\!\\left(\\mathbf{z}\\,;\\, \\boldsymbol{\\mu}_z(\\mathbf{x},\\mathbf{c}),\\, \\operatorname{diag}(\\boldsymbol{\\sigma}^2_z(\\mathbf{x},\\mathbf{c}))\\right)$。参数计算如下：\n\n后验均值向量 $\\boldsymbol{\\mu}_z \\in \\mathbb{R}^L$ 为：\n$$ \\boldsymbol{\\mu}_z(\\mathbf{x}_{\\mathrm{obs}},\\mathbf{c}_{\\mathrm{obs}}) = \\mathbf{M}\\mathbf{x}_{\\mathrm{obs}} + \\mathbf{V}\\mathbf{c}_{\\mathrm{obs}} + \\mathbf{a} $$\n\n后验逐元素对数方差向量 $\\log \\boldsymbol{\\sigma}^2_z \\in \\mathbb{R}^L$ 为：\n$$ \\log \\boldsymbol{\\sigma}^2_z(\\mathbf{x}_{\\mathrm{obs}},\\mathbf{c}_{\\mathrm{obs}}) = \\mathbf{S}_x \\mathbf{x}_{\\mathrm{obs}} + \\mathbf{S}_c \\mathbf{c}_{\\mathrm{obs}} + \\mathbf{s}_0 $$\n由此，通过逐元素取指数得到后验方差向量 $\\boldsymbol{\\sigma}^2_z \\in \\mathbb{R}^L$：\n$$ \\boldsymbol{\\sigma}^2_z(\\mathbf{x}_{\\mathrm{obs}},\\mathbf{c}_{\\mathrm{obs}}) = \\exp(\\log \\boldsymbol{\\sigma}^2_z(\\mathbf{x}_{\\mathrm{obs}},\\mathbf{c}_{\\mathrm{obs}})) $$\n为简洁起见，我们将这些特定的后验参数表示为 $\\boldsymbol{\\mu}_z$ 和 $\\boldsymbol{\\sigma}_z^2$，并将相应的分布表示为 $q(\\mathbf{z})$。\n\n确定了后验参数后，我们现在可以推导这两个所需量的表达式。\n\n**1. 证据下界 (ELBO)**\n\nELBO定义为：\n$$ \\mathcal{L}(\\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}}) = \\mathbb{E}_{q(\\mathbf{z})}\\!\\left[\\log p(\\mathbf{x}_{\\mathrm{obs}} \\mid \\mathbf{z}, \\mathbf{c}_{\\mathrm{obs}})\\right] - \\mathrm{KL}\\!\\left(q(\\mathbf{z})\\,\\|\\, p(\\mathbf{z})\\right) $$\n\n我们将分别计算这两个项。\n\n第二项是近似后验 $q(\\mathbf{z}) = \\mathcal{N}(\\boldsymbol{\\mu}_z, \\operatorname{diag}(\\boldsymbol{\\sigma}_z^2))$ 与标准正态先验 $p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ 之间的Kullback-Leibler (KL) 散度。提供的公式为：\n$$ \\mathrm{KL}(q\\|p) = \\frac{1}{2} \\sum_{i=1}^{L} \\left( \\sigma_{z,i}^2 + \\mu_{z,i}^2 - 1 - \\log \\sigma_{z,i}^2 \\right) $$\n其中 $\\mu_{z,i}$ 和 $\\sigma_{z,i}^2$ 是先前计算的向量 $\\boldsymbol{\\mu}_z$ 和 $\\boldsymbol{\\sigma}_z^2$ 的元素。\n\n第一项是期望重构对数似然。似然函数由 $p(\\mathbf{x}\\mid \\mathbf{z}, \\mathbf{c}) = \\mathcal{N}\\!\\left(\\mathbf{x}\\,;\\, \\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{c} + \\mathbf{b},\\, \\sigma_x^2 \\mathbf{I}\\right)$ 给出。其对数概率为：\n$$ \\log p(\\mathbf{x} \\mid \\mathbf{z}, \\mathbf{c}) = -\\frac{D}{2} \\log(2\\pi \\sigma_x^2) - \\frac{1}{2\\sigma_x^2} \\|\\mathbf{x} - (\\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{c} + \\mathbf{b})\\|^2_2 $$\n我们必须计算该量在 $q(\\mathbf{z})$ 下的期望。第一项相对于 $\\mathbf{z}$ 是一个常数。\n$$ \\mathbb{E}_{q(\\mathbf{z})}\\!\\left[\\log p(\\mathbf{x}_{\\mathrm{obs}} \\mid \\mathbf{z}, \\mathbf{c}_{\\mathrm{obs}})\\right] = -\\frac{D}{2} \\log(2\\pi \\sigma_x^2) - \\frac{1}{2\\sigma_x^2} \\mathbb{E}_{q(\\mathbf{z})}\\!\\left[ \\|\\mathbf{x}_{\\mathrm{obs}} - (\\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{c}_{\\mathrm{obs}} + \\mathbf{b})\\|^2_2 \\right] $$\n为了评估平方范数的期望，我们使用提供的恒等式：$\\mathbb{E}_q\\!\\left[\\lVert \\mathbf{x} - (\\mathbf{A}\\mathbf{z}+\\boldsymbol{\\mu}) \\rVert_2^2\\right] = \\lVert \\mathbf{x} - (\\mathbf{A}\\boldsymbol{\\mu}_z+\\boldsymbol{\\mu}) \\rVert_2^2 + \\operatorname{tr}\\!\\left(\\mathbf{A}\\,\\operatorname{diag}(\\boldsymbol{\\sigma}^2_z)\\,\\mathbf{A}^\\top\\right)$。我们进行替换：$\\mathbf{x} \\rightarrow \\mathbf{x}_{\\mathrm{obs}}$，$\\mathbf{A} \\rightarrow \\mathbf{W}$，以及 $\\boldsymbol{\\mu} \\rightarrow \\mathbf{U}\\mathbf{c}_{\\mathrm{obs}} + \\mathbf{b}$。期望变为：\n$$ \\|\\mathbf{x}_{\\mathrm{obs}} - (\\mathbf{W}\\boldsymbol{\\mu}_z + \\mathbf{U}\\mathbf{c}_{\\mathrm{obs}} + \\mathbf{b})\\|^2_2 + \\operatorname{tr}\\!\\left(\\mathbf{W}\\,\\operatorname{diag}(\\boldsymbol{\\sigma}^2_z)\\,\\mathbf{W}^\\top\\right) $$\n第一项是输入 $\\mathbf{x}_{\\mathrm{obs}}$ 与其从后验均值重构值之间的欧几里得距离的平方。迹项可以使用迹的循环性质和协方差矩阵 $\\boldsymbol{\\Sigma}_z = \\operatorname{diag}(\\boldsymbol{\\sigma}^2_z)$ 的对角性质进行简化：\n$$ \\operatorname{tr}(\\mathbf{W} \\boldsymbol{\\Sigma}_z \\mathbf{W}^\\top) = \\operatorname{tr}(\\mathbf{W}^\\top \\mathbf{W} \\boldsymbol{\\Sigma}_z) = \\sum_{i=1}^L (\\mathbf{W}^\\top \\mathbf{W})_{ii} \\sigma_{z,i}^2 = \\sum_{i=1}^L \\|\\mathbf{W}_{:,i}\\|_2^2 \\sigma_{z,i}^2 $$\n其中 $\\mathbf{W}_{:,i}$ 是 $\\mathbf{W}$ 的第 $i$ 列。\n\n将所有部分结合起来，ELBO 为：\n$$ \\mathcal{L} = \\left[ -\\frac{D}{2}\\log(2\\pi\\sigma_x^2) - \\frac{1}{2\\sigma_x^2}\\left( \\|\\mathbf{x}_{\\mathrm{obs}} - (\\mathbf{W}\\boldsymbol{\\mu}_z + \\mathbf{U}\\mathbf{c}_{\\mathrm{obs}} + \\mathbf{b})\\|^2_2 + \\sum_{i=1}^L \\|\\mathbf{W}_{:,i}\\|_2^2 \\sigma_{z,i}^2 \\right) \\right] - \\left[ \\frac{1}{2}\\sum_{i=1}^L(\\mu_{z,i}^2 + \\sigma_{z,i}^2 - 1 - \\log\\sigma_{z,i}^2) \\right] $$\n\n**2. 反事实重构均值**\n\n要计算的第二个量是重构数据分布的均值，其中潜变量 $\\mathbf{z}$ 从根据观测数据 $(\\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})$ 推断出的后验分布中抽取，但解码过程使用改变后的条件 $\\mathbf{c}_{\\mathrm{alt}}$。\n$$ \\mathbf{x}_{\\mathrm{cf}} = \\mathbb{E}_{q(\\mathbf{z}\\mid \\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})}\\!\\left[ \\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{c}_{\\mathrm{alt}} + \\mathbf{b} \\right] $$\n根据期望的线性性质，并且由于 $\\mathbf{c}_{\\mathrm{alt}}$ 和 $\\mathbf{b}$ 相对于关于 $\\mathbf{z}$ 的期望是常数：\n$$ \\mathbf{x}_{\\mathrm{cf}} = \\mathbf{W} \\mathbb{E}_{q(\\mathbf{z})}[\\mathbf{z}] + \\mathbf{U}\\mathbf{c}_{\\mathrm{alt}} + \\mathbf{b} $$\n$\\mathbf{z}$ 在 $q(\\mathbf{z})$ 分布下的期望就是其均值 $\\boldsymbol{\\mu}_z$。因此，反事实重构均值为：\n$$ \\mathbf{x}_{\\mathrm{cf}} = \\mathbf{W}\\boldsymbol{\\mu}_z + \\mathbf{U}\\mathbf{c}_{\\mathrm{alt}} + \\mathbf{b} $$\n其中 $\\boldsymbol{\\mu}_z$ 是从 $(\\mathbf{x}_{\\mathrm{obs}}, \\mathbf{c}_{\\mathrm{obs}})$ 计算出的后验均值。\n\n实现将针对每个测试用例遵循这些推导出的闭式表达式。所有向量和矩阵运算将使用 `numpy` 库来执行。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the CVAE problem by computing the ELBO and counterfactual reconstruction mean\n    for three test cases based on the provided linear-Gaussian model.\n    \"\"\"\n    \n    # Model dimensions\n    L = 2  # Latent dimension\n    D = 4  # Data dimension\n    C = 3  # Condition dimension\n    \n    # Decoder parameters\n    W = np.array([\n        [0.8, -0.3],\n        [0.1, 0.5],\n        [-0.4, 0.2],\n        [0.0, 0.3]\n    ])\n    U = np.array([\n        [0.6, -0.2, 0.3],\n        [-0.1, 0.4, -0.5],\n        [0.2, 0.1, -0.2],\n        [0.5, -0.3, 0.2]\n    ])\n    b = np.array([0.05, -0.02, 0.01, 0.0]).reshape(D, 1)\n    sigma_x = 0.1\n    \n    # Encoder parameters\n    M = np.array([\n        [0.2, 0.0, -0.1, 0.3],\n        [-0.2, 0.1, 0.4, -0.1]\n    ])\n    V = np.array([\n        [0.1, -0.2, 0.3],\n        [0.0, 0.2, -0.1]\n    ])\n    a = np.array([0.05, -0.05]).reshape(L, 1)\n    \n    S_x = np.array([\n        [-0.1, 0.2, 0.0, -0.2],\n        [0.1, -0.1, 0.3, 0.0]\n    ])\n    S_c = np.array([\n        [0.05, 0.05, -0.1],\n        [-0.05, 0.1, 0.0]\n    ])\n    s_0 = np.array([-1.0, -1.2]).reshape(L, 1)\n    \n    # Test suite\n    test_cases = [\n        {\n            \"x_obs\": np.array([1.2, 0.3, -0.5, 0.0]).reshape(D, 1),\n            \"c_obs\": np.array([1, 0, 1]).reshape(C, 1),\n            \"c_alt\": np.array([1, 0, 0]).reshape(C, 1)\n        },\n        {\n            \"x_obs\": np.array([-0.2, 0.5, 1.0, -1.0]).reshape(D, 1),\n            \"c_obs\": np.array([0, 1, 0]).reshape(C, 1),\n            \"c_alt\": np.array([1, 0, 0]).reshape(C, 1)\n        },\n        {\n            \"x_obs\": np.array([0.0, 0.0, 0.0, 0.0]).reshape(D, 1),\n            \"c_obs\": np.array([0, 1, 1]).reshape(C, 1),\n            \"c_alt\": np.array([1, 0, 1]).reshape(C, 1)\n        }\n    ]\n    \n    results = []\n    \n    for case in test_cases:\n        x_obs = case[\"x_obs\"]\n        c_obs = case[\"c_obs\"]\n        c_alt = case[\"c_alt\"]\n        \n        # 1. Compute posterior parameters (mu_z, sigma_z^2)\n        # Use observed data and conditions\n        mu_z = M @ x_obs + V @ c_obs + a\n        log_sigma_z_sq = S_x @ x_obs + S_c @ c_obs + s_0\n        sigma_z_sq = np.exp(log_sigma_z_sq)\n        \n        # 2. Compute the KL divergence term of the ELBO\n        # KL(q(z|x,c) || p(z))\n        kl_div = 0.5 * np.sum(sigma_z_sq + mu_z**2 - 1 - log_sigma_z_sq)\n        \n        # 3. Compute the expected log-likelihood term of the ELBO\n        # E_q[log p(x|z,c)]\n        \n        # Term 1: Reconstruction error from posterior mean\n        recon_mean_obs = W @ mu_z + U @ c_obs + b\n        recon_error_sq_norm = np.sum((x_obs - recon_mean_obs)**2)\n        \n        # Term 2: Trace term from variance propagation\n        w_col_sq_norms = np.sum(W**2, axis=0) # shape (L,)\n        trace_term = np.sum(w_col_sq_norms * sigma_z_sq.flatten())\n        \n        # Constant from Gaussian PDF\n        log_p_constant = -D / 2.0 * np.log(2 * np.pi * sigma_x**2)\n        \n        # Combine parts for expected log-likelihood\n        expected_log_p = log_p_constant - (1 / (2 * sigma_x**2)) * (recon_error_sq_norm + trace_term)\n        \n        # 4. Compute the ELBO\n        elbo = expected_log_p - kl_div\n        \n        # 5. Compute the counterfactual reconstruction mean\n        # E_q[decoder(z, c_alt)]\n        cf_recon_mean = W @ mu_z + U @ c_alt + b\n        \n        # Format the results as required\n        elbo_rounded = round(elbo, 6)\n        cf_recon_mean_list = [round(x, 6) for x in cf_recon_mean.flatten()]\n        \n        results.append([elbo_rounded, cf_recon_mean_list])\n        \n    # Print the final result in the exact required format\n    # Manual string construction to avoid spaces added by standard list-to-string conversion\n    result_strings = []\n    for elbo_val, cf_mean_list in results:\n        cf_mean_str = f\"[{','.join(f'{x:.6f}' for x in cf_mean_list)}]\"\n        result_strings.append(f\"[{elbo_val:.6f},{cf_mean_str}]\")\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "4397973"}]}