{"hands_on_practices": [{"introduction": "本练习将卷积层的抽象参数与基因组基序（motif）检测的实际任务联系起来。通过推导输出尺寸公式并分析该层的感受野，您将具体理解核尺寸和步幅等设计选择如何直接影响模型“看见”和处理生物序列的能力。[@problem_id:4553876]", "problem": "您正在设计一个卷积神经网络（CNN；Convolutional Neural Network）中的第一层一维卷积，用于检测沿通道进行独热编码（one-hot）的脱氧核糖核酸（DNA；Deoxyribonucleic Acid）序列中的转录因子结合基序。考虑一个长度为 $L_{\\mathrm{in}} = 1000$ 个碱基的输入序列。该卷积层使用长度为 $k = 15$ 的卷积核，步长 $s = 3$，在两端各进行 $p = 2$ 个碱基的对称零填充，膨胀率为 $d = 1$。\n\n从在填充输入上进行带步长和填充的离散一维卷积的定义出发，推导出确定输出位置数量（即有效卷积核对齐次数）的通用表达式。使用该推理计算给定参数下的输出序列长度 $L_{\\mathrm{out}}$。然后，基于感受野的概念和步长引起的采样，分析此配置如何影响检测跨度为 $M = 20$ 个碱基的基序的能力，特别是在单次滤波器应用内的完全覆盖、位置敏感性以及序列边界附近的检测方面。\n\n报告输出长度 $L_{\\mathrm{out}}$，格式为单个整数，不带单位。无需四舍五入。您的定性分析应包含在推理过程中，但最终答案必须仅为计算出的 $L_{\\mathrm{out}}$。", "solution": "用户要求推导一维卷积层的输出长度，并针对其在生物信息学中的特定应用对其性质进行定性分析。解决方案将首先建立输出维度的通用公式，然后将其应用于给定参数，最后分析该配置所带来的影响。\n\n**问题验证**\n\n在继续之前，对问题陈述进行严格验证。\n1.  **提取的已知条件**：\n    -   输入序列长度：$L_{\\mathrm{in}} = 1000$ 个碱基。\n    -   卷积核长度：$k = 15$。\n    -   步长：$s = 3$。\n    -   对称零填充：$p = 2$（在两端）。\n    -   膨胀率：$d = 1$。\n    -   用于分析的目标基序长度：$M = 20$ 个碱基。\n\n2.  **验证结论**：该问题是**有效的**。它在科学上基于深度学习（特别是卷积神经网络）的标准原理，并应用于基因组分析这一成熟的生物信息学领域。问题提法得当，提供了一套完整且一致的参数（$L_{\\mathrm{in}}, k, s, p, d$），可以据此确定输出长度（$L_{\\mathrm{out}}$）的唯一、有意义的解。语言客观，并采用了标准术语。这些参数对于所描述的任务是符合实际的。\n\n**输出长度通用表达式的推导**\n\n一维卷积层的输出位置数量，即输出序列长度 $L_{\\mathrm{out}}$，取决于卷积核沿输入序列的有效放置次数。这可以从第一性原理推导得出。\n\n1.  一个长度为 $L_{\\mathrm{in}}$ 的输入序列首先进行对称填充，即在其两端各添加 $p$ 个元素。这个新的、填充后的序列长度（表示为 $L_{\\mathrm{padded}}$）为：\n    $$L_{\\mathrm{padded}} = L_{\\mathrm{in}} + 2p$$\n\n2.  长度为 $k$ 的卷积核可能会被膨胀。膨胀因子 $d$ 在连续的卷积核元素之间引入 $d-1$ 个间隙。因此，卷积核在输入上的有效空间范围，即有效卷积核大小 $k_{\\mathrm{eff}}$ 为：\n    $$k_{\\mathrm{eff}} = k + (k-1)(d-1)$$\n\n3.  卷积核在填充后的序列上移动或“跨步”。每个位置都会计算一个输出。卷积核的第一次放置从填充序列的索引 $0$ 开始，覆盖的范围是从索引 $0$ 到 $k_{\\mathrm{eff}}-1$。最后一次可能的放置也必须完全在填充序列内。卷积核的最后一个元素必须与不大于 $L_{\\mathrm{padded}}-1$ 的索引对齐。这意味着卷积核的最后一个起始位置在索引 $L_{\\mathrm{padded}} - k_{\\mathrm{eff}}$ 处。\n\n4.  因此，可能的起始位置的总范围是从 $0$ 到 $L_{\\mathrm{padded}} - k_{\\mathrm{eff}}$，如果步长为 $1$，则包含 $L_{\\mathrm{padded}} - k_{\\mathrm{eff}} + 1$ 个可能的位置。\n\n5.  当步长为 $s$ 时，卷积核不会占据每个可能的位置。它会占据位置 $0, s, 2s, \\dots, Ns$，其中 $Ns$ 是最后一个有效的起始位置。这个最后位置必须满足 $Ns \\le L_{\\mathrm{padded}} - k_{\\mathrm{eff}}$。因此，步数 $N$ 是满足此条件的最大整数，可以使用向下取整函数找到：\n    $$N = \\left\\lfloor \\frac{L_{\\mathrm{padded}} - k_{\\mathrm{eff}}}{s} \\right\\rfloor$$\n    输出位置的总数 $L_{\\mathrm{out}}$ 是这些放置的次数，即 $N+1$（包括在位置 $0$ 的初始放置）。\n\n6.  将 $L_{\\mathrm{padded}}$ 和 $k_{\\mathrm{eff}}$ 的表达式代入，得到输出长度的通用公式：\n    $$L_{\\mathrm{out}} = \\left\\lfloor \\frac{(L_{\\mathrm{in}} + 2p) - (k + (k-1)(d-1))}{s} \\right\\rfloor + 1$$\n\n**根据给定参数计算 $L_{\\mathrm{out}}$**\n\n问题提供了以下参数：$L_{\\mathrm{in}} = 1000$，$k = 15$，$s = 3$，$p = 2$ 以及 $d = 1$。\n\n首先，我们将这些值代入推导出的通用表达式中。由于膨胀率为 $d=1$，有效卷积核长度 $k_{\\mathrm{eff}}$ 就是 $k$：\n$$k_{\\mathrm{eff}} = 15 + (15-1)(1-1) = 15 + (14)(0) = 15$$\n$L_{\\mathrm{out}}$ 的公式简化为：\n$$L_{\\mathrm{out}} = \\left\\lfloor \\frac{L_{\\mathrm{in}} + 2p - k}{s} \\right\\rfloor + 1$$\n代入数值：\n$$L_{\\mathrm{out}} = \\left\\lfloor \\frac{1000 + 2(2) - 15}{3} \\right\\rfloor + 1$$\n$$L_{\\mathrm{out}} = \\left\\lfloor \\frac{1000 + 4 - 15}{3} \\right\\rfloor + 1$$\n$$L_{\\mathrm{out}} = \\left\\lfloor \\frac{989}{3} \\right\\rfloor + 1$$\n除法得出一个非整数值：\n$$\\frac{989}{3} \\approx 329.667$$\n应用向下取整函数，得到小于或等于该值的最大整数：\n$$\\lfloor 329.667 \\rfloor = 329$$\n最后，加 $1$ 得出输出长度：\n$$L_{\\mathrm{out}} = 329 + 1 = 330$$\n\n**配置的定性分析**\n\n问题进一步要求分析此配置检测长度为 $M = 20$ 个碱基的基序的能力。\n\n1.  **完全覆盖与感受野**：该卷积层中任何单个神经元的感受野是其有效卷积核大小，即 $k_{\\mathrm{eff}} = 15$ 个碱基。感兴趣的基序长度为 $M=20$ 个碱基。由于 $M > k_{\\mathrm{eff}}$，**没有任何单次滤波器应用能够看到整个基序**。该滤波器至多只能学会识别长度不超过 $15$ 个碱基的基序片段。为了识别完整的 $20$ 碱基基序，网络必须学会在输出特征图中结合来自空间上相邻神经元的信息，这是后续层（例如，另一个卷积层或全连接层）的任务。\n\n2.  **位置敏感性与步长**：$s=3$ 的步长引入了对输入的下采样。这意味着卷积核每移动一步会“跳过”$2$ 个碱基。因此，该层对单碱基平移不具有平移等变性。一个从输入位置 $i$ 开始的基序与一个从位置 $i+1$ 或 $i+2$ 开始的基序，将被卷积核以不同的方式看待。这种对模 $s$ 的位置移位的高度敏感性可能是有害的，因为基序位置的微小移动可能会导致其与卷积核的步长错位，从而导致激活值减弱和可能的检测失败。\n\n3.  **序列边界附近的检测**：该层使用 $p=2$ 的对称填充。对于大小为 $k=15$ 的卷积核，这种填充不足以使卷积核能够中心对齐于序列最两端的基序。为了使一个大小为 $k$ 的卷积核中心对齊于第一个碱基（索引 $0$），需要 $p_{\\mathrm{center}} = \\lfloor k/2 \\rfloor = \\lfloor 15/2 \\rfloor = 7$ 的填充。而当 $p=2$ 时，第一个输出神经元的感受野中心位于原始序列位置 $7-2 = 5$ 上。这意味着序列最前面约 $5$ 个碱基和最后面约 $5$ 个碱基内的基序永远无法在卷积核的视野中居中，这会损害网络检测它们的能力。\n\n总之，对于检测一个 $20$ 碱基基序的任务来说，这个特定的层配置由于其卷积核尺寸过小而在结构上受到限制，并且其性能会因大步长和不充分的填充而进一步受损，这会影响位置敏感性和边界检测。", "answer": "$$\\boxed{330}$$", "id": "4553876"}, {"introduction": "池化层对于构建层次化特征和实现不变性至关重要，但它们天生会降低空间分辨率。本实践题要求您通过将位置信息的损失建模为一种量化误差来量化这种权衡。这种分析对于设计能够精确定位基因组特征（如转录起始位点或核酸酶切点）的网络至关重要。[@problem_id:4331413]", "problem": "在系统生物医学中，卷积神经网络（Convolutional Neural Networks (CNN)）被用于从脱氧核糖核酸（DNA）序列中预测核酸酶切位点。考虑一个一维CNN，它接收一个以每碱基对$1$个样本采样的DNA序列，应用步长为$1$的卷积层，然后应用$K$个最大池化层，其中第$i$个池化层使用$s_i$个碱基对的窗口大小和$s_i$的步长（非重叠池化）。该网络输出一个粗糙的空间图谱，从中选择一个网格位置（例如，通过对粗糙图谱进行$\\arg\\max$运算），预测的切位点位置$\\hat{x}$被取为所选粗糙单元映射回输入坐标时的中心。\n\n假设如下：\n- 真实的切位点位置$x^{\\ast}$（相对于池化所产生的粗糙网格）由于真实位置与池化网格之间的未知对齐，在任何粗糙单元内均匀分布。\n- 上游的卷积在输入分辨率上保持平移等变性，且在输出端不执行亚单元插值；预测被限制在池化所产生的粗糙网格上。\n- 你可以引用奈奎斯特-香农采样定理和最大池化的定义来形式化堆叠池化层如何改变有效采样分辨率。将这一结果建模为对输入坐标的均匀量化，其步长由$\\{s_i\\}_{i=1}^{K}$决定。\n\n从这些基础出发，推导期望平方定位误差\n$$\nE = \\mathbb{E}\\big[(\\hat{x} - x^{\\ast})^{2}\\big]\n$$\n将其表示为关于池化步长$\\{s_i\\}_{i=1}^{K}$的封闭形式解析表达式。解释为什么增加$K$或任何$s_i$会因减少位置信息而增加$E$。请以平方碱基对为单位表示你最终的$E$，并以单个封闭形式解析表达式提供答案。无需四舍五入，最终的方框答案中不要包含单位。", "solution": "该问题被认为是有效的，因为它具有科学依据、问题明确且客观。它提出了一个简化但标准的模型，用于描述卷积神经网络中下采样操作（最大池化）所产生的量化误差，这是信号处理和机器学习应用于基因组学中的一个常见主题。所有必要的参数和假设都已提供，以推导出一个唯一的解析解。\n\n问题要求计算期望平方定位误差 $E = \\mathbb{E}[(\\hat{x} - x^{\\ast})^{2}]$，其中$\\hat{x}$是预测的切位点位置，$x^{\\ast}$是真实位置。这个问题的核心在于理解最大池化层的顺序应用如何影响输入信号的空间分辨率。\n\n首先，让我们确定最终输出图谱中单个分辨率元素（一个“粗糙单元”）在映射回输入坐标系时的大小。输入的DNA序列以每碱基对$1$个样本进行采样。网络架构包括步长为$1$的卷积层（保留空间分辨率），随后是$K$个最大池化层。第$i$个池化层的窗口大小为$s_i$，步长也为$s_i$。这种非重叠池化操作将数据点的数量减少了$s_i$倍。当这$K$个层堆叠时，总的下采样因子是各个步长因子的乘积。\n\n设$S$为原始输入坐标中单个粗糙单元的大小，单位为碱基对。这个大小是所有$K$个池化操作的累积效应。\n$$\nS = \\prod_{i=1}^{K} s_i\n$$\n最终粗糙图谱中的单个网格位置对应于输入DNA中长度为$S$个碱基对的一个片段。让我们将这样一个单元表示为某个起始位置$a$的区间$[a, a+S)$。\n\n问题指出，预测的切位点位置$\\hat{x}$是所选粗糙单元的中心。因此，对于一个跨越$[a, a+S)$的单元，预测值为：\n$$\n\\hat{x} = a + \\frac{S}{2}\n$$\n假设由于未知的对齐，真实的切位点位置$x^{\\ast}$在此粗糙单元内均匀分布。这可以表示为：\n$$\nx^{\\ast} \\sim U[a, a+S)\n$$\n其中$U[c, d)$表示在区间$[c, d)$上的连续均匀分布。\n\n我们关心的是定位误差，即差值$\\hat{x} - x^{\\ast}$。我们定义一个随机变量$\\epsilon$来表示这个误差：\n$$\n\\epsilon = \\hat{x} - x^{\\ast} = \\left(a + \\frac{S}{2}\\right) - x^{\\ast}\n$$\n由于$x^{\\ast}$在$[a, a+S)$上均匀分布，误差变量$\\epsilon$在以下区间上均匀分布：\n$$\n\\left[ \\left(a + \\frac{S}{2}\\right) - (a+S), \\left(a + \\frac{S}{2}\\right) - a \\right] = \\left[ -\\frac{S}{2}, \\frac{S}{2} \\right]\n$$\n所以，$\\epsilon \\sim U[-S/2, S/2]$。$\\epsilon$的概率密度函数（PDF），记为$p(\\epsilon)$，是：\n$$\np(\\epsilon) = \\begin{cases} \\frac{1}{(S/2) - (-S/2)} = \\frac{1}{S}  \\text{当 } \\epsilon \\in [-S/2, S/2] \\\\ 0  \\text{其他情况} \\end{cases}\n$$\n需要推导的量是期望平方定位误差$E$：\n$$\nE = \\mathbb{E}[(\\hat{x} - x^{\\ast})^{2}] = \\mathbb{E}[\\epsilon^2]\n$$\n这个期望可以通过将$\\epsilon^2$对其概率密度函数在其支撑集上积分来计算：\n$$\nE = \\int_{-\\infty}^{\\infty} \\epsilon^2 p(\\epsilon) \\, d\\epsilon = \\int_{-S/2}^{S/2} \\epsilon^2 \\left(\\frac{1}{S}\\right) \\, d\\epsilon\n$$\n现在我们可以计算这个积分：\n$$\nE = \\frac{1}{S} \\left[ \\frac{\\epsilon^3}{3} \\right]_{-S/2}^{S/2} = \\frac{1}{3S} \\left[ \\left(\\frac{S}{2}\\right)^3 - \\left(-\\frac{S}{2}\\right)^3 \\right]\n$$\n$$\nE = \\frac{1}{3S} \\left[ \\frac{S^3}{8} - \\left(-\\frac{S^3}{8}\\right) \\right] = \\frac{1}{3S} \\left[ \\frac{S^3}{8} + \\frac{S^3}{8} \\right] = \\frac{1}{3S} \\left[ \\frac{2S^3}{8} \\right] = \\frac{1}{3S} \\left[ \\frac{S^3}{4} \\right]\n$$\n$$\nE = \\frac{S^2}{12}\n$$\n这个结果是中心在$0$的均匀分布的方差，因为$\\epsilon$的均值为$\\mathbb{E}[\\epsilon]=0$。\n\n最后，我们将总单元大小$S$的表达式代入，该表达式用各个池化步长$\\{s_i\\}_{i=1}^{K}$表示：\n$$\nE = \\frac{\\left( \\prod_{i=1}^{K} s_i \\right)^2}{12}\n$$\n这就是期望平方定位误差的封闭形式解析表达式，单位是平方碱基对。\n\n这个表达式说明了为什么增加池化层的数量$K$或任何给定池化层的步长$s_i$会导致期望误差$E$的增加。总下采样因子$S = \\prod s_i$代表了空间量化区间的宽度。任何$K$的增加都会引入一个新的乘法因子$s_K > 1$（假设是非平凡的池化），而增加任何$s_i$会直接增大乘积$S$。期望平方误差$E$与$S^2$成正比。更大的单元尺寸$S$意味着在该单元内真实位置$x^{\\ast}$的不确定性更大。这种位置信息的损失在数学上通过更宽区间上均匀分布的更大方差来体现，从而导致平方级增大的期望定位误差。这说明了网络设计中特征不变性/抽象与空间精度之间的一个基本权衡。", "answer": "$$\n\\boxed{\\frac{\\left( \\prod_{i=1}^{K} s_i \\right)^2}{12}}\n$$", "id": "4331413"}, {"introduction": "基因组数据集通常是高度不平衡的，这可能导致像交叉熵这样的标准训练目标失效。本练习探讨了焦点损失（focal loss），这是一种强大的解决方案，它动态地调整每个样本对损失的贡献，从而迫使模型关注于稀有和困难的样本。通过推导和比较梯度，您将揭示出焦点损失在转录因子结合位点预测等应用场景中如此有效的作用机制。[@problem_id:4331425]", "problem": "在系统生物医学中，一个卷积神经网络（CNN）被训练用于从脱氧核糖核酸（DNA）序列中预测转录因子结合位点，其中正类（结合）相对于负类（非结合）是稀有的。每个序列都经过独热编码，由卷积层处理，然后通过一个最终的 logistic (sigmoid) 输出层。设模型的 logit 为 $z \\in \\mathbb{R}$，其预测概率为 $p = \\sigma(z) = \\frac{1}{1 + \\exp(-z)}$。对于一个真实标签为 $y \\in \\{0,1\\}$ 的序列，二元交叉熵（逻辑损失）定义为 $L_{\\mathrm{log}}(p,y) = - y \\ln(p) - (1-y) \\ln(1-p)$，而焦点损失 (focal loss) 定义为 $L_{\\mathrm{foc}}(p,y) = - \\alpha \\,(1-p)^{\\gamma} y \\ln(p) - \\alpha\\, p^{\\gamma} (1-y) \\ln(1-p)$，其中 $\\alpha \\in (0,1]$ 是一个平衡因子，$\\gamma \\ge 0$ 是聚焦参数。考虑正类 $y = 1$ 并设置 $\\alpha = 1$。\n\n从上述定义和链式法则出发，推导在 $y=1$ 条件下，二元交叉熵和焦点损失关于 logit 的梯度 $\\frac{\\partial L}{\\partial z}$。然后，定义梯度幅度缩放比\n$$S(p,\\gamma) = \\frac{\\left|\\frac{\\partial L_{\\mathrm{foc}}(p,1)}{\\partial z}\\right|}{\\left|\\frac{\\partial L_{\\mathrm{log}}(p,1)}{\\partial z}\\right|}.$$\n在基因组序列分类中，针对稀有正样本的两个感兴趣的工作点，使用 $\\gamma = 2$ 来评估 $S(p,\\gamma)$：\n- 一个“简单”正样本，其 $p = 0.99$；\n- 一个“困难”正样本，其 $p = 0.01$。\n\n将您计算出的 $S(0.99,2)$ 和 $S(0.01,2)$ 的两个数值，按简单和困难情况的顺序，以行矩阵的形式报告。将每个条目四舍五入到四位有效数字。无需单位。", "solution": "用户在计算生物学的机器学习领域提供了一个定义明确的问题。我将首先根据指定的协议来验证问题陈述。\n\n### 步骤 1：提取已知条件\n- 模型 logit: $z \\in \\mathbb{R}$\n- 预测概率: $p = \\sigma(z) = \\frac{1}{1 + \\exp(-z)}$\n- 真实标签: $y \\in \\{0,1\\}$\n- 二元交叉熵（逻辑损失）: $L_{\\mathrm{log}}(p,y) = - y \\ln(p) - (1-y) \\ln(1-p)$\n- 焦点损失 (Focal loss): $L_{\\mathrm{foc}}(p,y) = - \\alpha \\,(1-p)^{\\gamma} y \\ln(p) - \\alpha\\, p^{\\gamma} (1-y) \\ln(1-p)$\n- 平衡因子: $\\alpha \\in (0,1]$\n- 聚焦参数: $\\gamma \\geq 0$\n- 分析条件: 正类, $y = 1$\n- 参数值假设: $\\alpha = 1$\n- 梯度幅度缩放比定义: $S(p,\\gamma) = \\frac{\\left|\\frac{\\partial L_{\\mathrm{foc}}(p,1)}{\\partial z}\\right|}{\\left|\\frac{\\partial L_{\\mathrm{log}}(p,1)}{\\partial z}\\right|}$\n- 用于评估的参数值: $\\gamma = 2$\n- 评估点:\n  - “简单”正样本: $p = 0.99$\n  - “困难”正样本: $p = 0.01$\n- 要求的输出: $S(0.99,2)$ 和 $S(0.01,2)$ 的数值，四舍五入到四位有效数字。\n\n### 步骤 2：使用提取的已知条件进行验证\n1.  **科学性**：该问题在机器学习理论中有充分的依据。sigmoid 函数、二元交叉熵和焦点损失的定义都是标准的。它们在基因组序列分类中的应用是系统生物医学的一个主流研究领域。\n2.  **适定性**：该问题的结构确保了其有唯一解。它提供了所有必需的函数、参数和评估点。推导梯度并计算特定比率的要求是明确的。\n3.  **客观性**：该问题使用精确的数学定义进行陈述。术语“简单”和“困难”样本是通过预测概率值（对于真阳性样本 $y=1$，$p=0.99$ 和 $p=0.01$）客观定义的，这是一个标准惯例。\n4.  **完整性与一致性**：该问题是自洽且内部一致的。所有必需的参数（$\\alpha$, $\\gamma$）和变量（$y$, $p$）都为最终计算赋了值。\n5.  **现实性**：使用此类模型识别转录因子结合位点并处理类别不平衡问题的前提，在现代生物信息学中非常现实。所用的参数值也是典型的。\n\n### 步骤 3：结论与行动\n该问题有效。我将继续进行完整的推导和求解。\n\n任务是推导二元交叉熵损失和焦点损失在正样本（$y=1$）情况下相对于 logit $z$ 的梯度，然后计算它们在特定点上梯度的幅度之比。\n\n首先，我们需要预测概率 $p$ 相对于 logit $z$ 的导数。函数 $p = \\sigma(z)$ 是 sigmoid 函数。\n$$p = \\sigma(z) = \\frac{1}{1 + \\exp(-z)} = (1 + \\exp(-z))^{-1}$$\n使用链式法则，我们求其关于 $z$ 的导数：\n$$\\frac{dp}{dz} = \\frac{d\\sigma(z)}{dz} = -1 (1 + \\exp(-z))^{-2} \\cdot (-\\exp(-z)) = \\frac{\\exp(-z)}{(1 + \\exp(-z))^2}$$\n我们可以用 $p$ 来表示这个导数：\n$$\\frac{dp}{dz} = \\frac{1}{1 + \\exp(-z)} \\cdot \\frac{\\exp(-z)}{1 + \\exp(-z)} = p \\cdot \\left(1 - \\frac{1}{1 + \\exp(-z)}\\right) = p(1-p)$$\n这是一个标准结果。现在可以使用链式法则求出损失函数关于 $z$ 的导数：$\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial p} \\frac{dp}{dz}$。\n\n**1. 二元交叉熵损失（$L_{\\mathrm{log}}$）的梯度**\n\n对于一个正样本，$y=1$。二元交叉熵损失简化为：\n$$L_{\\mathrm{log}}(p,1) = - (1) \\ln(p) - (1-1) \\ln(1-p) = -\\ln(p)$$\n其关于 $p$ 的导数是：\n$$\\frac{\\partial L_{\\mathrm{log}}(p,1)}{\\partial p} = -\\frac{1}{p}$$\n应用链式法则求得关于 $z$ 的梯度：\n$$\\frac{\\partial L_{\\mathrm{log}}(p,1)}{\\partial z} = \\frac{\\partial L_{\\mathrm{log}}(p,1)}{\\partial p} \\frac{dp}{dz} = \\left(-\\frac{1}{p}\\right) \\cdot (p(1-p)) = -(1-p) = p-1$$\n\n**2. 焦点损失（$L_{\\mathrm{foc}}$）的梯度**\n\n对于一个正样本（$y=1$）且平衡因子 $\\alpha=1$，焦点损失简化为：\n$$L_{\\mathrm{foc}}(p,1) = - (1) (1-p)^{\\gamma} (1) \\ln(p) - (1) p^{\\gamma} (1-1) \\ln(1-p) = -(1-p)^{\\gamma} \\ln(p)$$\n我们使用乘法法则对其求关于 $p$ 的导数：\n$$\\frac{\\partial L_{\\mathrm{foc}}(p,1)}{\\partial p} = \\frac{\\partial}{\\partial p} \\left[ -(1-p)^{\\gamma} \\ln(p) \\right]$$\n$$= -\\left( (\\gamma(1-p)^{\\gamma-1}(-1)) \\ln(p) + (1-p)^{\\gamma} \\frac{1}{p} \\right)$$\n$$= -\\left( -\\gamma(1-p)^{\\gamma-1} \\ln(p) + \\frac{(1-p)^{\\gamma}}{p} \\right)$$\n$$= \\gamma(1-p)^{\\gamma-1} \\ln(p) - \\frac{(1-p)^{\\gamma}}{p}$$\n现在，我们应用链式法则求得关于 $z$ 的梯度：\n$$\\frac{\\partial L_{\\mathrm{foc}}(p,1)}{\\partial z} = \\frac{\\partial L_{\\mathrm{foc}}(p,1)}{\\partial p} \\frac{dp}{dz} = \\left( \\gamma(1-p)^{\\gamma-1} \\ln(p) - \\frac{(1-p)^{\\gamma}}{p} \\right) \\cdot (p(1-p))$$\n将 $p(1-p)$ 项分配进去：\n$$\\frac{\\partial L_{\\mathrm{foc}}(p,1)}{\\partial z} = \\gamma p (1-p)^{\\gamma} \\ln(p) - (1-p)^{\\gamma+1}$$\n$$= (1-p)^{\\gamma} \\left[ \\gamma p \\ln(p) - (1-p) \\right] = (1-p)^{\\gamma} \\left[ \\gamma p \\ln(p) + p - 1 \\right]$$\n\n**3. 梯度幅度缩放比（$S(p,\\gamma)$）**\n\n该比率定义为：\n$$S(p,\\gamma) = \\frac{\\left|\\frac{\\partial L_{\\mathrm{foc}}(p,1)}{\\partial z}\\right|}{\\left|\\frac{\\partial L_{\\mathrm{log}}(p,1)}{\\partial z}\\right|}$$\n我们已求得梯度为：\n$$\\frac{\\partial L_{\\mathrm{log}}(p,1)}{\\partial z} = p-1$$\n$$\\frac{\\partial L_{\\mathrm{foc}}(p,1)}{\\partial z} = (1-p)^{\\gamma} (\\gamma p \\ln(p) + p-1)$$\n对于 $p \\in (0,1)$，我们有 $p-1  0$ 且 $\\ln(p)  0$。因为 $\\gamma \\ge 0$，所以项 $\\gamma p \\ln(p)$ 是非正的。因此，整个括号内的项 $(\\gamma p \\ln(p) + p-1)$ 是负的。项 $(1-p)^{\\gamma}$ 是正的。所以，对于 $p \\in (0,1)$，两个梯度都是负的。\n它们的绝对值是它们的相反数：\n$$ \\left|\\frac{\\partial L_{\\mathrm{log}}(p,1)}{\\partial z}\\right| = -(p-1) = 1-p $$\n$$ \\left|\\frac{\\partial L_{\\mathrm{foc}}(p,1)}{\\partial z}\\right| = -(1-p)^{\\gamma} (\\gamma p \\ln(p) + p-1) = (1-p)^{\\gamma} (1-p - \\gamma p \\ln(p)) $$\n该比率为：\n$$S(p,\\gamma) = \\frac{(1-p)^{\\gamma} (1-p - \\gamma p \\ln(p))}{1-p} = (1-p)^{\\gamma-1} (1-p - \\gamma p \\ln(p))$$\n\n**4. 数值计算**\n\n我们必须在 $p=0.99$ 和 $p=0.01$ 处，对 $\\gamma=2$ 的情况计算 $S(p,\\gamma)$。针对 $\\gamma=2$ 的具体公式是：\n$$S(p,2) = (1-p)^{2-1} (1-p - 2p \\ln(p)) = (1-p)(1-p - 2p \\ln(p))$$\n\n情况1：“简单”正样本，$p = 0.99$。\n$$S(0.99,2) = (1-0.99)(1 - 0.99 - 2(0.99)\\ln(0.99))$$\n$$S(0.99,2) = (0.01)(0.01 - 1.98 \\ln(0.99))$$\n使用 $\\ln(0.99) \\approx -0.01005033585$：\n$$S(0.99,2) \\approx (0.01)(0.01 - 1.98(-0.01005033585))$$\n$$S(0.99,2) \\approx (0.01)(0.01 + 0.019899665)$$\n$$S(0.99,2) \\approx (0.01)(0.029899665) = 0.00029899665$$\n四舍五入到四位有效数字，我们得到 $0.0002990$。\n\n情况2：“困难”正样本，$p = 0.01$。\n$$S(0.01,2) = (1-0.01)(1 - 0.01 - 2(0.01)\\ln(0.01))$$\n$$S(0.01,2) = (0.99)(0.99 - 0.02 \\ln(0.01))$$\n使用 $\\ln(0.01) = \\ln(10^{-2}) = -2\\ln(10) \\approx -4.605170186$：\n$$S(0.01,2) \\approx (0.99)(0.99 - 0.02(-4.605170186))$$\n$$S(0.01,2) \\approx (0.99)(0.99 + 0.0921034037)$$\n$$S(0.01,2) \\approx (0.99)(1.0821034037) = 1.07128237$$\n四舍五入到四位有效数字，我们得到 $1.071$。\n\n结果展示了焦点损失的“聚焦”特性。对于一个简单的正样本（$p=0.99$），梯度幅度被缩小了约 $3 \\times 10^{-4}$ 倍，从而有效减少了其对权重更新的贡献。对于一个困难的正样本（$p=0.01$），梯度幅度与标准交叉熵损失的梯度幅度相当（甚至略大），确保模型将其学习重点放在这个被错误分类的样本上。\n\n所要求的值为 $S(0.99,2) = 0.0002990$ 和 $S(0.01,2) = 1.071$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.0002990  1.071\n\\end{pmatrix}\n}\n$$", "id": "4331425"}]}