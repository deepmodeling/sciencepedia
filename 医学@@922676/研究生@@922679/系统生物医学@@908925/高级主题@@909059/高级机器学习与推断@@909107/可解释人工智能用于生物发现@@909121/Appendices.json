{"hands_on_practices": [{"introduction": "在可解释人工智能（XAI）中，生成特征归因是理解模型决策的第一步，但这引出了一个关键问题：这些解释在多大程度上“忠实”地反映了模型的内部推理过程？此练习将介绍一种名为“扰动曲线下面积”（Area Over the Perturbation Curve, AOPC）的实用评估指标。通过系统性地移除或“扰动”排名靠前的特征并观察模型输出的变化，AOPC 为我们提供了一种量化和比较不同归因方法（如梯度显著性和 SHAP）保真度的方法 [@problem_id:4340491]。", "problem": "在系统生物医学背景下，给定一个简易的基因组学分类器，它将一个基因表达水平向量映射到一个被解释为决策分数的标量分数。该分类器是线性的，定义如下：对于特征向量 $\\mathbf{x} \\in \\mathbb{R}^d$、权重向量 $\\mathbf{w} \\in \\mathbb{R}^d$、偏置 $b \\in \\mathbb{R}$ 以及代表“无表达”背景的基线向量 $\\mathbf{z} \\in \\mathbb{R}^d$，模型输出为 $f(\\mathbf{x}) = b + \\sum_{i=1}^{d} w_i x_i$。考虑两种归因方法：梯度显著性（gradient saliency）和 Shapley 加性解释（Shapley Additive exPlanations, SHAP）。目标是计算每种方法的扰动曲线下面积（Area Over the Perturbation Curve, AOPC），以比较其归因的忠实度。\n\n需要使用的基本原则和定义：\n- 根据多变量微积分，标量函数 $f(\\mathbf{x})$ 相对于 $\\mathbf{x}$ 的梯度是 $\\nabla f(\\mathbf{x})$，其第 $i$ 个分量是 $\\frac{\\partial f}{\\partial x_i}$。\n- 根据合作博弈论，特征 $i$ 的 Shapley 值是该特征在所有联盟上的平均边际贡献，通过对子集 $S \\subseteq \\{1,\\dots,d\\} \\setminus \\{i\\}$ 进行枚举或积分来计算，并在一个明确定义的缺失模型下比较特征 $i$ 存在与缺失时的 $f$ 值，该模型用基线 $\\mathbf{z}$ 替换缺失的特征。\n- 扰动曲线由输出序列 $f(\\mathbf{x}^{(k)})$ 定义，其中 $\\mathbf{x}^{(k)}$ 是通过根据所选归因排名对前 $k$ 个特征进行消融（用 $\\mathbf{z}$ 替换）而形成的。扰动曲线下面积（AOPC）是模型输出在 $k = 1,\\dots,K$ 上的平均下降量，其中 $K$ 是您选择的消融步骤数（此处设 $K=d$）。\n\n任务要求：\n- 仅使用给定的基本原则和定义，推导在指定的线性模型和基线替换下的梯度显著性和 Shapley 加性解释 (SHAP) 的归因分数，然后按如下方式实现 AOPC 计算流程：\n    1. 根据每种方法计算每个特征 $i$ 的归因分数 $a_i$。\n    2. 按归因绝对值 $\\lvert a_i \\rvert$ 的降序对特征进行排序。如果出现平局，则按特征索引 $i$ 的升序打破平局。\n    3. 对于 $k \\in \\{1,\\dots,K\\}$ 且 $K=d$，通过将排名中的前 $k$ 个特征替换为其基线值 $\\mathbf{z}$（逐特征消融至 $\\mathbf{z}$）来定义 $\\mathbf{x}^{(k)}$。令 $\\Delta_k = f(\\mathbf{x}) - f(\\mathbf{x}^{(k)})$。\n    4. 定义 $\\mathrm{AOPC} = \\frac{1}{K} \\sum_{k=1}^{K} \\Delta_k$。\n- 所有计算均使用指定的线性分数 $f(\\mathbf{x})$ 和基线 $\\mathbf{z}$。不需要概率性或随机性假设。此计算不涉及物理单位，输出结果是无单位的。\n\n测试套件和参数：\n- 所有情况下均使用 $d = 6$ 个特征。所有情况下的基线均为零向量 $\\mathbf{z} = (0,0,0,0,0,0)$。请精确实现以下四个测试用例，每个用例由 $(\\mathbf{w}, b, \\mathbf{x})$ 定义：\n    1. 用例 A（一般情况，混合符号）：$\\mathbf{w} = (1.5, -0.9, 0.4, 0.0, 1.0, -0.2)$, $b = 0.3$, $\\mathbf{x} = (2.0, 1.0, 0.0, 5.0, 0.1, 3.0)$。\n    2. 用例 B（边界情况，全零输入）：$\\mathbf{w} = (1.5, -0.9, 0.4, 0.0, 1.0, -0.2)$, $b = 0.3$, $\\mathbf{x} = (0.0, 0.0, 0.0, 0.0, 0.0, 0.0)$。\n    3. 用例 C（边缘情况，归因平局）：$\\mathbf{w} = (0.5, -0.5, 0.5, -0.5, 0.0, 0.5)$, $b = 0.0$, $\\mathbf{x} = (1.0, 2.0, 3.0, 0.0, 10.0, 0.0)$。\n    4. 用例 D（边缘情况，零值特征具有大权重）：$\\mathbf{w} = (2.0, 0.1, 0.1, 0.1, 0.1, 0.1)$, $b = -0.5$, $\\mathbf{x} = (0.0, 10.0, 10.0, 10.0, 10.0, 10.0)$。\n- 对于梯度显著性，根据第一性原理计算给定 $f(\\mathbf{x})$ 的显著性，并按这些显著性值的大小进行排序。对于 SHAP，在将缺失特征替换为 $\\mathbf{z}$ 的缺失模型下，使用针对此线性模型特化的合作博弈论定义计算 Shapley 值，然后按这些值的大小进行排序。\n- 所有情况下设 $K = d$。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。列表中的每个元素对应于四个用例之一，并且其本身是一个包含梯度显著性和 SHAP 的 AOPC 值的双元素列表（按此顺序）。例如，最终输出格式必须为 $[ [\\mathrm{AOPC}_{\\mathrm{grad},A}, \\mathrm{AOPC}_{\\mathrm{shap},A}], [\\mathrm{AOPC}_{\\mathrm{grad},B}, \\mathrm{AOPC}_{\\mathrm{shap},B}], [\\mathrm{AOPC}_{\\mathrm{grad},C}, \\mathrm{AOPC}_{\\mathrm{shap},C}], [\\mathrm{AOPC}_{\\mathrm{grad},D}, \\mathrm{AOPC}_{\\mathrm{shap},D}] ]$，并包含实际数值。", "solution": "该问题要求计算并比较两种归因方法——梯度显著性（gradient saliency）和 Shapley 加性解释（SHAP）——应用于系统生物医学背景下的线性分类器时的扰动曲线下面积（AOPC）。对问题陈述的验证证实了其具有科学依据、是良定的和客观的，并提供了所有必要的定义和数据。因此，我们可以进行基于原则的推导和求解。\n\n该分类器是一个线性模型，由函数 $f(\\mathbf{x}): \\mathbb{R}^d \\to \\mathbb{R}$ 定义，具体如下：\n$$f(\\mathbf{x}) = b + \\sum_{i=1}^{d} w_i x_i = b + \\mathbf{w}^T \\mathbf{x}$$\n其中 $\\mathbf{x}$ 是基因表达水平的特征向量，$\\mathbf{w}$ 是权重向量， $b$ 是一个偏置项。代表“无表达”状态的基线向量为 $\\mathbf{z}$。对于所有测试用例，$\\mathbf{z}$ 均为零向量，即 $\\mathbf{z} = \\mathbf{0}$。\n\nAOPC 定义为 $\\frac{1}{K} \\sum_{k=1}^{K} \\Delta_k$，其中 $K=d$ 且 $\\Delta_k = f(\\mathbf{x}) - f(\\mathbf{x}^{(k)})$。扰动向量 $\\mathbf{x}^{(k)}$ 是通过将按其归因分数绝对值排名的前 $k$ 个特征进行消融（设置为基线值 $z_i$）而构建的。\n\n让我们推导每种方法的归因分数，然后概述 AOPC 的计算流程。\n\n**1. 梯度显著性归因**\n\n特征 $i$ 的梯度显著性归因（记为 $a_i^{\\text{grad}}$）定义为模型输出相对于特征输入 $x_i$ 的偏导数：\n$$a_i^{\\text{grad}} = \\frac{\\partial f(\\mathbf{x})}{\\partial x_i}$$\n对于给定的线性模型 $f(\\mathbf{x}) = b + w_1 x_1 + w_2 x_2 + \\dots + w_d x_d$，其关于 $x_i$ 的偏导数为：\n$$\\frac{\\partial}{\\partial x_i} \\left( b + \\sum_{j=1}^{d} w_j x_j \\right) = w_i$$\n因此，特征 $i$ 的梯度归因就是其对应的权重，$a_i^{\\text{grad}} = w_i$。请注意，对于线性模型，此归因与输入向量 $\\mathbf{x}$ 无关。归因向量为 $\\mathbf{a}^{\\text{grad}} = \\mathbf{w}$。\n\n**2. SHAP (Shapley 加性解释) 归因**\n\n特征 $i$ 的 SHAP 归因 $a_i^{\\text{shap}}$ 是其在合作博弈论中的 Shapley 值。它是特征 $i$ 在其他特征所有可能联盟（子集）上的平均边际贡献。特征联盟 $S \\subseteq \\{1, \\dots, d\\}$ 的“价值”是指当仅这些特征取自 $\\mathbf{x}$ 的值，而其余特征取自基线 $\\mathbf{z}$ 的值时的模型输出。我们将此价值函数表示为 $v(S)$。\n$$v(S) = f(\\mathbf{x}_S, \\mathbf{z}_{\\bar{S}}) = b + \\sum_{j \\in S} w_j x_j + \\sum_{j \\notin S} w_j z_j$$\n其中 $\\mathbf{x}_S$ 表示对于 $S$ 中的索引，使用来自 $\\mathbf{x}$ 的值，而 $\\mathbf{z}_{\\bar{S}}$ 表示对于不在 $S$ 中的索引（补集 $\\bar{S}$），使用来自 $\\mathbf{z}$ 的值。\n\n特征 $i$ 对联盟 $S$（其中 $i \\notin S$）的边际贡献是 $v(S \\cup \\{i\\}) - v(S)$。让我们计算这个量：\n$$v(S \\cup \\{i\\}) = b + w_i x_i + \\sum_{j \\in S} w_j x_j + \\sum_{j \\notin S \\cup \\{i\\}} w_j z_j$$\n$$v(S) = b + w_i z_i + \\sum_{j \\in S} w_j x_j + \\sum_{j \\notin S \\cup \\{i\\}} w_j z_j$$\n差值为：\n$$v(S \\cup \\{i\\}) - v(S) = (w_i x_i) - (w_i z_i) = w_i(x_i - z_i)$$\n值得注意的是，对于线性模型，此边际贡献与联盟 $S$ 无关。Shapley 值是这些边际贡献在所有联盟上的加权平均值。由于贡献是恒定的，其平均值就是该恒定值本身。\n因此，特征 $i$ 的 SHAP 归因是：\n$$a_i^{\\text{shap}} = w_i(x_i - z_i)$$\n由于问题指定基线是零向量 $\\mathbf{z} = \\mathbf{0}$，因此简化为：\n$$a_i^{\\text{shap}} = w_i x_i$$\n每个特征的归因是其相对于偏置对模型输出分数的精确贡献。归因向量为 $\\mathbf{a}^{\\text{shap}} = \\mathbf{w} \\odot \\mathbf{x}$，其中 $\\odot$ 表示逐元素乘积。\n\n**3. AOPC 计算流程**\n\n两种归因方法的计算流程相同，仅在用于排名的初始分数上有所不同。\n1.  **计算归因**：对于给定的测试用例 $(\\mathbf{w}, b, \\mathbf{x})$，计算归因向量 $\\mathbf{a}$：\n    -   对于梯度显著性：$\\mathbf{a} = \\mathbf{w}$。\n    -   对于 SHAP：$\\mathbf{a} = \\mathbf{w} \\odot \\mathbf{x}$。\n2.  **特征排序**：为 $i \\in \\{0, \\dots, d-1\\}$ 创建一个元组列表 $(|a_i|, i)$。按 $|a_i|$ 的降序对此列表进行排序，通过按特征索引 $i$ 的升序来打破平局。令得到的排序后索引序列为 $I = [i_1, i_2, \\dots, i_d]$。\n3.  **计算扰动下降量 ($\\Delta_k$)**：我们需要计算 $\\Delta_k = f(\\mathbf{x}) - f(\\mathbf{x}^{(k)})$。让我们简化这个表达式。设 $I_k = \\{i_1, \\dots, i_k\\}$ 为前 $k$ 个被消融的特征索引的集合。\n    $$f(\\mathbf{x}) = b + \\sum_{j=1}^{d} w_j x_j$$\n    $$f(\\mathbf{x}^{(k)}) = b + \\sum_{j \\notin I_k} w_j x_j + \\sum_{j \\in I_k} w_j z_j$$\n    由于 $\\mathbf{z} = \\mathbf{0}$，$f(\\mathbf{x}^{(k)}) = b + \\sum_{j \\notin I_k} w_j x_j$。则下降量为：\n    $$\\Delta_k = \\left(b + \\sum_{j=1}^{d} w_j x_j\\right) - \\left(b + \\sum_{j \\notin I_k} w_j x_j\\right) = \\sum_{j \\in I_k} w_j x_j$$\n    这是一个显著的简化：$\\Delta_k$ 是已被消融特征的个体贡献 $w_j x_j$ 的总和。\n4.  **计算 AOPC**：计算下降量的平均值：\n    $$\\mathrm{AOPC} = \\frac{1}{d} \\sum_{k=1}^{d} \\Delta_k$$\n现在将实现这个完整的、基于原则的流程，并将其应用于四个指定的测试用例。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is a tuple of (w, b, x)\n    test_cases = [\n        # Case A (general, mixed signs)\n        (np.array([1.5, -0.9, 0.4, 0.0, 1.0, -0.2]), 0.3, np.array([2.0, 1.0, 0.0, 5.0, 0.1, 3.0])),\n        # Case B (boundary, all-zero input)\n        (np.array([1.5, -0.9, 0.4, 0.0, 1.0, -0.2]), 0.3, np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0])),\n        # Case C (edge, attribution ties)\n        (np.array([0.5, -0.5, 0.5, -0.5, 0.0, 0.5]), 0.0, np.array([1.0, 2.0, 3.0, 0.0, 10.0, 0.0])),\n        # Case D (edge, large weight on a zero-valued feature)\n        (np.array([2.0, 0.1, 0.1, 0.1, 0.1, 0.1]), -0.5, np.array([0.0, 10.0, 10.0, 10.0, 10.0, 10.0])),\n    ]\n\n    results = []\n    for w, b, x in test_cases:\n        d = len(w)\n        case_results = []\n        \n        # Calculate attributions for both methods\n        grad_attributions = w\n        shap_attributions = w * x\n        \n        attributions_list = [grad_attributions, shap_attributions]\n        \n        for attributions in attributions_list:\n            # Step 1: Rank features based on attributions\n            # Create a list of tuples: (absolute attribution, index)\n            # The sorting key uses -abs(attribution) for descending order of value,\n            # and index for ascending order to break ties.\n            indexed_attributions = [(abs(val), i) for i, val in enumerate(attributions)]\n            indexed_attributions.sort(key=lambda item: (-item[0], item[1]))\n            ranked_indices = [item[1] for item in indexed_attributions]\n\n            # Step 2: Calculate perturbation drops and AOPC\n            # Pre-compute the true contributions of each feature\n            contributions = w * x\n            \n            delta_k_sum = 0.0\n            current_delta_k = 0.0\n            \n            # The problem sets K=d, so we iterate d times\n            for k in range(d):\n                # The index of the feature to ablate at this step\n                idx_to_ablate = ranked_indices[k]\n                \n                # Delta_k is the sum of contributions of the top k ablated features\n                current_delta_k += contributions[idx_to_ablate]\n                \n                # Add the current Delta_k to the total sum for AOPC\n                delta_k_sum += current_delta_k\n            \n            # AOPC is the average of the Delta_k values\n            if d == 0:\n                aopc = 0.0\n            else:\n                aopc = delta_k_sum / d\n                \n            case_results.append(aopc)\n            \n        results.append(case_results)\n\n    # Final print statement in the exact required format without extra spaces\n    # [[AOPC_grad_A,AOPC_shap_A],[AOPC_grad_B,AOPC_shap_B],...]\n    formatted_results = [f\"[{res[0]},{res[1]}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4340491"}, {"introduction": "在从 XAI 模型中获得一系列特征归因分数后，我们面临着从高维数据中区分真实信号与随机噪声的挑战。本练习将引导您应用基本的统计学原理来解决这一问题。我们将使用单样本学生 t 检验（Student's $t$-test）并结合 Bonferroni 校正，以识别那些在样本群体中具有统计显著性贡献的特征，这是确保生物学发现稳健性的关键一步 [@problem_id:4340397]。", "problem": "考虑一个系统生物医学中的场景，其中可解释人工智能 (XAI) 用于从一个将多组学特征映射到表型的预测模型中，为每个样本生成特征归因。假设有一个包含 $n$ 个样本和 $d$ 个特征的归因矩阵，记为 $X \\in \\mathbb{R}^{n \\times d}$，其中 $X_{i,j}$ 是特征 $j$ 对于样本 $i$ 的归因值。我们希望确定哪些特征在所有样本中具有非零的平均归因值，同时使用 Bonferroni 程序控制族系误差率。\n\n您的任务是编写一个完整的程序，对于每个提供的测试用例，执行以下基于基本统计原理的步骤：\n- 对于每个特征 $j \\in \\{0,\\dots,d-1\\}$，检验原假设 $H_0: \\mu_j = 0$ 与双侧备择假设 $H_1: \\mu_j \\neq 0$，其中 $\\mu_j$ 是特征 $j$ 的总体平均归因值。\n- 使用经典的单样本学生 t 检验：计算样本均值 $\\hat{\\mu}_j = \\frac{1}{n}\\sum_{i=1}^{n} X_{i,j}$ 和无偏样本标准差 $s_j = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} \\left(X_{i,j}-\\hat{\\mu}_j\\right)^2}$。当 $s_j \\neq 0$ 时，t 统计量为 $t_j = \\frac{\\hat{\\mu}_j}{s_j / \\sqrt{n}}$，自由度为 $\\nu = n-1$。\n- 当 $s_j = 0$ 时，使用 t 统计量所蕴含的极限决策：如果 $\\hat{\\mu}_j = 0$，则将双侧 p 值设为 $p_j = 1$，否则设为 $p_j = 0$。\n- 否则，当 $s_j \\neq 0$ 时，计算双侧 p 值 $p_j = 2\\left(1 - F_{\\nu}\\left(|t_j|\\right)\\right)$，其中 $F_{\\nu}$ 是自由度为 $\\nu = n-1$ 的学生 t 分布的累积分布函数。\n- 对 $d$ 个假设应用 Bonferroni 校正：定义校正后的 p 值 $q_j = \\min\\{1, d \\cdot p_j\\}$。如果 $q_j  \\alpha$，则称特征 $j$ 是显著的，其中 $\\alpha$ 是目标族系误差率。\n- 返回所有显著特征的从零开始的索引，并按升序排列。\n\n使用的基本原理：\n- 当样本量 $n$ 适中且总体方差未知时，中心极限定理为使用学生 t 检验来检验均值提供了理论依据。\n- 单样本 t 检验依赖于在 $H_0$ 下，$\\frac{\\bar{X}-\\mu}{S/\\sqrt{n}}$ 的抽样分布是自由度为 $\\nu = n-1$ 的学生 t 分布，其中 $\\bar{X}$ 和 $S$ 分别是样本均值和样本标准差。\n- Bonferroni 程序通过在 $\\alpha/d$ 水平上检验每个假设，或等效地通过使用校正后的 p 值 $q_j = \\min\\{1, d \\cdot p_j\\}$ 并与 $\\alpha$ 进行比较来控制族系误差率。\n\n输入规范是隐式的：您的程序必须内部评估以下测试套件。对于每个案例，给定一个归因矩阵 $X$ 和一个显著性水平 $\\alpha$：\n\n- 测试用例 1：\n  - $X \\in \\mathbb{R}^{8 \\times 5}$，其列（特征）定义如下：\n    - 特征 0: $[\\,0.65, 0.52, 0.48, 0.51, 0.62, 0.55, 0.46, 0.57\\,]$\n    - 特征 1: $[\\,-0.12, 0.07, -0.04, 0.01, 0.02, -0.03, 0.05, 0.04\\,]$\n    - 特征 2: $[\\,-0.41, -0.38, -0.43, -0.47, -0.44, -0.39, -0.42, -0.45\\,]$\n    - 特征 3: $[\\,0.10, 0.02, -0.10, 0.05, -0.02, 0.01, -0.01, -0.03\\,]$\n    - 特征 4: $[\\,0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00\\,]$\n  - $\\alpha = 0.05$。\n- 测试用例 2：\n  - $X \\in \\mathbb{R}^{5 \\times 3}$，其列为：\n    - 特征 0: $[\\,0.10, 0.05, -0.02, 0.03, -0.04\\,]$\n    - 特征 1: $[\\,0.30, -0.30, 0.30, -0.30, 0.30\\,]$\n    - 特征 2: $[\\,0.00, 0.00, 0.00, 0.00, 0.00\\,]$\n  - $\\alpha = 0.01$。\n- 测试用例 3：\n  - $X \\in \\mathbb{R}^{6 \\times 2}$，其列为：\n    - 特征 0: $[\\,0.20, 0.20, 0.20, 0.20, 0.20, 0.20\\,]$\n    - 特征 1: $[\\,0.00, 0.00, 0.00, 0.00, 0.00, 0.00\\,]$\n  - $\\alpha = 0.05$。\n- 测试用例 4：\n  - $X \\in \\mathbb{R}^{10 \\times 4}$，其列为：\n    - 特征 0: $[\\,0.15, 0.18, 0.10, 0.12, 0.20, 0.14, 0.16, 0.13, 0.19, 0.11\\,]$\n    - 特征 1: $[\\,0.05, 0.02, 0.01, 0.00, 0.04, -0.01, 0.03, 0.00, 0.02, 0.01\\,]$\n    - 特征 2: $[\\,-0.12, -0.15, -0.11, -0.13, -0.14, -0.16, -0.10, -0.14, -0.13, -0.12\\,]$\n    - 特征 3: $[\\,-0.01, 0.00, 0.01, -0.02, 0.02, 0.00, 0.01, -0.01, 0.00, 0.00\\,]$\n  - $\\alpha = 0.05$。\n\n对于每个测试用例，你的程序必须以升序整数列表的形式输出显著特征的索引。将所有测试用例的结果汇总到一行输出中，该输出包含一个由方括号括起来的逗号分隔列表。例如，如果四个用例的结果分别是列表 $L_1, L_2, L_3, L_4$，则输出格式必须为 $[L_1,L_2,L_3,L_4]$，且在单行上。\n\n不涉及物理单位或角度单位。所有结果均为纯数值。每个测试用例的最终答案是整数列表。最终打印的行必须是单行，并与所述格式匹配。", "solution": "该问题提出了一个定义明确且具有统计学基础的任务。它是自洽、一致且科学合理的。目标是从给定的归因矩阵 $X \\in \\mathbb{R}^{n \\times d}$ 中识别出具有统计显著性非零均值归因的特征，同时控制族系误差率 (FWER)。所指定的方法，即对每个特征采用单样本学生 t 检验，然后进行 Bonferroni 校正以处理多重假设检验，是解决此问题的经典且合适的方法。因此，该问题被认为是有效的，下面提供了正式的解决方案。\n\n问题的核心在于执行 $d$ 个并行的假设检验，每个特征一个。对于每个特征 $j \\in \\{0, 1, \\dots, d-1\\}$，我们检验原假设 $H_0: \\mu_j = 0$ 与双侧备择假设 $H_1: \\mu_j \\neq 0$，其中 $\\mu_j$ 代表特征 $j$ 归因的真实但未知的总体均值。整个 $d$ 个检验族的显著性水平用 $\\alpha$ 表示。\n\n对每个特征 $j$ 的步骤如下：\n\n1.  **计算样本统计量**：给定特征 $j$ 的 $n$ 个归因值样本 $\\{X_{1,j}, X_{2,j}, \\dots, X_{n,j}\\}$，我们首先计算样本均值 $\\hat{\\mu}_j$ 和无偏样本标准差 $s_j$。\n    $$ \\hat{\\mu}_j = \\frac{1}{n} \\sum_{i=1}^{n} X_{i,j} $$\n    $$ s_j = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (X_{i,j} - \\hat{\\mu}_j)^2} $$\n    量 $s_j / \\sqrt{n}$ 是均值的标准误。\n\n2.  **计算检验统计量和 p 值**：检验方法的选择取决于样本标准差 $s_j$。\n    *   **案例 1：$s_j > 0$**。当样本中存在变异时，适合使用学生 t 检验。t 统计量衡量样本均值与假设均值 0 相差多少个标准误。其计算公式为：\n        $$ t_j = \\frac{\\hat{\\mu}_j - 0}{s_j / \\sqrt{n}} = \\frac{\\hat{\\mu}_j \\sqrt{n}}{s_j} $$\n        在原假设 $H_0$ 下，该统计量服从自由度为 $\\nu = n-1$ 的学生 t 分布。双侧 p 值 $p_j$ 是在原假设为真的情况下，观测到至少与 $|t_j|$ 一样极端的检验统计量的概率。\n        $$ p_j = 2 \\cdot P(T_{\\nu} \\ge |t_j|) = 2 \\left(1 - F_{\\nu}(|t_j|)\\right) $$\n        其中 $F_{\\nu}$ 是自由度为 $\\nu$ 的学生 t 分布的累积分布函数 (CDF)。\n\n    *   **案例 2：$s_j = 0$**。这种情况当且仅当特征 $j$ 的所有样本归因值都相同时发生，即 $X_{1,j} = X_{2,j} = \\dots = X_{n,j} = c$。在这种情况下，$\\hat{\\mu}_j = c$。由于除以零，t 统计量的标准公式未定义。我们必须使用指定的极限逻辑：\n        *   如果 $\\hat{\\mu}_j \\neq 0$，样本提供了反对原假设 $\\mu_j=0$ 的完美证据。如果 $H_0$ 为真，观测到此结果的概率为零。因此，我们设定 $p_j = 0$。\n        *   如果 $\\hat{\\mu}_j = 0$，则所有样本值都恰好为 0。数据与原假设完全一致。检验是不确定的，按照惯例，我们采取保守立场，不拒绝原假设。这通过将 p 值设为其最大可能值 $p_j = 1$ 来实现。\n\n3.  **多重比较校正**：由于我们正在执行 $d$ 个独立或相关的检验，犯至少一个 I 类错误（假阳性）的概率会增加。为了将 FWER（犯一个或多个 I 类错误的概率）控制在水平 $\\alpha$ 以下，我们应用 Bonferroni 校正。这通过调整每个单独的 p 值来实现。特征 $j$ 的校正后 p 值 $q_j$ 为：\n    $$ q_j = \\min\\{1, d \\cdot p_j\\} $$\n\n4.  **决策规则**：如果一个特征 $j$ 的校正后 p 值小于预先设定的 FWER 水平 $\\alpha$，则该特征被宣告具有统计上显著的非零均值归因。\n    $$ \\text{特征 } j \\text{ 在 } q_j  \\alpha \\text{ 时是显著的} $$\n\n对每个测试用例中的所有 $d$ 个特征执行此完整过程。每个测试用例的最终输出是与被识别为显著的特征相对应的从零开始的索引集合，按升序排序。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Performs one-sample t-tests with Bonferroni correction on multiple features\n    to identify those with a non-zero mean.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"X\": np.array([\n                [0.65, -0.12, -0.41, 0.10, 0.00],\n                [0.52, 0.07, -0.38, 0.02, 0.00],\n                [0.48, -0.04, -0.43, -0.10, 0.00],\n                [0.51, 0.01, -0.47, 0.05, 0.00],\n                [0.62, 0.02, -0.44, -0.02, 0.00],\n                [0.55, -0.03, -0.39, 0.01, 0.00],\n                [0.46, 0.05, -0.42, -0.01, 0.00],\n                [0.57, 0.04, -0.45, -0.03, 0.00]\n            ]),\n            \"alpha\": 0.05\n        },\n        {\n            \"X\": np.array([\n                [0.10, 0.30, 0.00],\n                [0.05, -0.30, 0.00],\n                [-0.02, 0.30, 0.00],\n                [0.03, -0.30, 0.00],\n                [-0.04, 0.30, 0.00]\n            ]),\n            \"alpha\": 0.01\n        },\n        {\n            \"X\": np.array([\n                [0.20, 0.00],\n                [0.20, 0.00],\n                [0.20, 0.00],\n                [0.20, 0.00],\n                [0.20, 0.00],\n                [0.20, 0.00]\n            ]),\n            \"alpha\": 0.05\n        },\n        {\n            \"X\": np.array([\n                [0.15, 0.05, -0.12, -0.01],\n                [0.18, 0.02, -0.15, 0.00],\n                [0.10, 0.01, -0.11, 0.01],\n                [0.12, 0.00, -0.13, -0.02],\n                [0.20, 0.04, -0.14, 0.02],\n                [0.14, -0.01, -0.16, 0.00],\n                [0.16, 0.03, -0.10, 0.01],\n                [0.13, 0.00, -0.14, -0.01],\n                [0.19, 0.02, -0.13, 0.00],\n                [0.11, 0.01, -0.12, 0.00]\n            ]),\n            \"alpha\": 0.05\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        X = case[\"X\"]\n        alpha = case[\"alpha\"]\n        \n        n, d = X.shape\n        significant_indices = []\n\n        for j in range(d):\n            feature_data = X[:, j]\n            \n            mu_hat = np.mean(feature_data)\n            # Use ddof=1 for unbiased sample standard deviation\n            s_j = np.std(feature_data, ddof=1)\n            \n            p_value = 0.0\n\n            if s_j == 0:\n                if mu_hat == 0:\n                    p_value = 1.0\n                else: # mu_hat != 0\n                    p_value = 0.0\n            else:\n                nu = n - 1\n                t_statistic = mu_hat / (s_j / np.sqrt(n))\n                # Two-sided p-value\n                # Using survival function (1-cdf) is more numerically stable for large t\n                p_value = 2 * t.sf(np.abs(t_statistic), df=nu)\n\n            # Bonferroni correction\n            q_value = min(1.0, d * p_value)\n\n            if q_value  alpha:\n                significant_indices.append(j)\n        \n        all_results.append(significant_indices)\n\n    # Format the output exactly as specified.\n    # The str() of a list, e.g., [0, 2], will be used directly.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "4340397"}, {"introduction": "可解释人工智能的威力不仅在于解释模型*为何*做出特定预测，更在于指导我们*如何*通过干预来改变预测结果。本练习将深入探讨反事实解释的生成，并将其构建为一个约束优化问题。您将学习如何在一个线性预测模型上，找到能够逆转表型预测的最小干预措施，同时确保该干预满足预设的生物学可行性约束，从而为设计靶向干预提供理论依据 [@problem_id:4340444]。", "problem": "一个系统生物医学实验室正在使用可解释人工智能（XAI）来设计对基因表达谱的反事实干预，以在保持生物可行性的同时翻转预测的表型。考虑一个在线性表型预测器，该预测器在经过对数转换的基因表达特征上进行训练，由权重向量 $w \\in \\mathbb{R}^{3}$ 和截距 $b \\in \\mathbb{R}$ 表示。当前样本的特征向量为 $x \\in \\mathbb{R}^{3}$。分类器的分数为 $s(x) = w^{\\top} x + b$，预测的表型标签为 $\\operatorname{sign}(s(x))$。一次反事实干预将状态修改为 $x' = x + \\delta$，其中 $\\delta \\in \\mathbb{R}^{3}$ 是扰动。该干预必须翻转表型预测，即必须满足 $w^{\\top} (x + \\delta) + b \\leq 0$，并且必须遵守编码为线性不等式的生物可行性约束。\n\n假设：\n- 当前状态为 $x = (1.2,\\, 0.3,\\, -0.1)$。\n- 预测器参数为 $w = (2,\\, -1,\\, 1)$ 和 $b = -0.5$，因此当前分数为 $s(x) = w^{\\top} x + b > 0$。\n- 可操作性仅限于两个特征：只有第二和第三个坐标可以改变。这被编码为约束 $\\delta_{1} = 0$。\n- 生物可行性由以下线性不等式编码：\n  1. $-\\ln(3) \\leq \\delta_{2} \\leq \\ln(3)$,\n  2. $-\\ln(3) \\leq \\delta_{3} \\leq \\ln(3)$,\n  3. $\\delta_{3} - 0.5\\,\\delta_{2} \\leq 0.2$,\n  4. $0.2\\,(x_{2} + \\delta_{2}) + 0.1\\,(x_{3} + \\delta_{3}) \\leq 0.5$.\n\n从第一性原理出发，推导出能够翻转表型预测并满足所有可行性约束的最小范数可操作扰动 $\\delta^{\\star}$。清晰地构建优化问题并精确求解。将最终答案表示为使用 $\\mathrm{pmatrix}$ 环境的单个行向量。无需四舍五入，不使用单位。", "solution": "该问题被评估为有效。这是一个适定的约束优化问题，其基础是机器学习模型反事实解释的形式化，这是系统生物医学中的一个合理主题。该问题是自洽的，数学上一致的，并且所有术语都得到了明确定义。\n\n目标是找到最小范数的可操作扰动 $\\delta^{\\star} \\in \\mathbb{R}^{3}$。范数假定为欧几里得范数（$L_2$-范数）。最小化范数 $\\|\\delta\\|_2$ 等价于最小化其平方 $\\|\\delta\\|_2^2 = \\delta_1^2 + \\delta_2^2 + \\delta_3^2$。这是一个二次规划问题。\n\n首先，我们通过陈述目标函数和所有约束来构建优化问题。\n\n待最小化的目标函数：\n$$ f(\\delta) = \\delta_1^2 + \\delta_2^2 + \\delta_3^2 $$\n\n约束条件如下：\n1.  可操作性约束：$\\delta_1 = 0$。\n2.  表型翻转约束：$w^{\\top}(x + \\delta) + b \\leq 0$。\n3.  生物可行性约束：\n    a. $-\\ln(3) \\leq \\delta_2 \\leq \\ln(3)$\n    b. $-\\ln(3) \\leq \\delta_3 \\leq \\ln(3)$\n    c. $\\delta_3 - 0.5\\,\\delta_2 \\leq 0.2$\n    d. $0.2\\,(x_2 + \\delta_2) + 0.1\\,(x_3 + \\delta_3) \\leq 0.5$\n\n接下来，我们将给定值代入这些表达式中。\n可操作性约束 $\\delta_1 = 0$ 将目标函数简化为最小化 $f(\\delta_2, \\delta_3) = \\delta_2^2 + \\delta_3^2$。这是 $(\\delta_2, \\delta_3)$ 平面中到原点的距离的平方。\n\n让我们简化不等式约束：\n表型翻转约束为 $w^{\\top}x + b + w^{\\top}\\delta \\leq 0$。\n初始分数为 $s(x) = w^{\\top}x+b = (2)(1.2) + (-1)(0.3) + (1)(-0.1) - 0.5 = 2.4 - 0.3 - 0.1 - 0.5 = 1.5$。\n扰动项为 $w^{\\top}\\delta = w_1\\delta_1 + w_2\\delta_2 + w_3\\delta_3 = (2)(0) + (-1)\\delta_2 + (1)\\delta_3 = \\delta_3 - \\delta_2$。\n因此，约束变为 $1.5 + \\delta_3 - \\delta_2 \\leq 0$，即：\n$$ \\delta_3 - \\delta_2 \\leq -1.5 $$\n\n现在，我们使用 $x_2 = 0.3$ 和 $x_3 = -0.1$ 简化最后一个可行性约束 (3d)：\n$0.2\\,(0.3 + \\delta_2) + 0.1\\,(-0.1 + \\delta_3) \\leq 0.5$\n$0.06 + 0.2\\delta_2 - 0.01 + 0.1\\delta_3 \\leq 0.5$\n$0.2\\delta_2 + 0.1\\delta_3 \\leq 0.5 - 0.05$\n$0.2\\delta_2 + 0.1\\delta_3 \\leq 0.45$\n为清晰起见，乘以 $10$：\n$$ 2\\delta_2 + \\delta_3 \\leq 4.5 $$\n\n完整、简化的优化问题是：\n最小化 $f(\\delta_2, \\delta_3) = \\delta_2^2 + \\delta_3^2$\n约束条件：\n1.  $\\delta_3 - \\delta_2 \\leq -1.5$\n2.  $-\\ln(3) \\leq \\delta_2 \\leq \\ln(3)$\n3.  $-\\ln(3) \\leq \\delta_3 \\leq \\ln(3)$\n4.  $\\delta_3 - 0.5\\,\\delta_2 \\leq 0.2$\n5.  $2\\delta_2 + \\delta_3 \\leq 4.5$\n\n这个问题是要在可行域（$\\delta_2\\delta_3$平面上的一个凸多边形）中找到离原点 $(0,0)$ 最近的点。无约束最小值位于 $(\\delta_2, \\delta_3)=(0,0)$。然而，该点违反了约束 (1)，因为 $0-0=0 \\not\\leq -1.5$。因此，解必须位于可行域的边界上。\n\n这个问题的解是无约束最小值（原点）在可行集上的几何投影。由于可行集是一个凸多边形且原点在其外部，解要么是多边形的一个顶点，要么是原点在其一条边上的投影。\n\n让我们求原点在由第一个约束定义的直线 $\\delta_3 - \\delta_2 = -1.5$（即 $\\delta_2 - \\delta_3 - 1.5 = 0$）上的投影。\n我们可以在约束 $\\delta_3 = \\delta_2 - 1.5$ 下最小化平方距离 $d^2 = \\delta_2^2 + \\delta_3^2$。\n$d^2 = \\delta_2^2 + (\\delta_2 - 1.5)^2$。\n为求最小值，我们将关于 $\\delta_2$ 的导数设为零：\n$\\frac{d(d^2)}{d\\delta_2} = 2\\delta_2 + 2(\\delta_2 - 1.5) = 4\\delta_2 - 3 = 0$。\n这得到 $\\delta_2 = \\frac{3}{4} = 0.75$。\n那么，$\\delta_3 = \\delta_2 - 1.5 = 0.75 - 1.5 = -0.75$。\n候选点是 $(\\delta_2, \\delta_3) = (0.75, -0.75)$。\n\n现在，我们必须验证该点是否满足所有其他约束。\n我们使用 $\\ln(3) \\approx 1.0986$。\n1.  $\\delta_3 - \\delta_2 \\leq -1.5 \\implies -0.75 - 0.75 = -1.5 \\leq -1.5$。（满足，因为它在边界上）。\n2.  $-\\ln(3) \\leq \\delta_2 \\leq \\ln(3) \\implies -1.0986 \\leq 0.75 \\leq 1.0986$。（满足）。\n3.  $-\\ln(3) \\leq \\delta_3 \\leq \\ln(3) \\implies -1.0986 \\leq -0.75 \\leq 1.0986$。（满足）。\n4.  $\\delta_3 - 0.5\\,\\delta_2 \\leq 0.2 \\implies -0.75 - 0.5(0.75) = -0.75 - 0.375 = -1.125 \\leq 0.2$。（满足）。\n5.  $2\\delta_2 + \\delta_3 \\leq 4.5 \\implies 2(0.75) + (-0.75) = 1.5 - 0.75 = 0.75 \\leq 4.5$。（满足）。\n\n由于点 $(0.75, -0.75)$ 是原点在直线 $\\delta_3 - \\delta_2 = -1.5$ 上的投影，并且该点位于所有其他约束定义的可行域内，因此它是可行域中离原点最近的唯一点。因此，它就是我们优化问题的解。\n\n最小范数扰动向量 $\\delta^{\\star}$ 的分量为：\n$\\delta_1^{\\star} = 0$\n$\\delta_2^{\\star} = 0.75$\n$\\delta_3^{\\star} = -0.75$\n\n所以，解为 $\\delta^{\\star} = (0, 0.75, -0.75)$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0  0.75  -0.75\n\\end{pmatrix}\n}\n$$", "id": "4340444"}]}