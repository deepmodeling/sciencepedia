## 引言
在当今的药物研发领域，从数以百万计的候选化合物中筛选和设计出安全有效的药物，是一个时间、成本和智力投入巨大的挑战。深度学习的崛起为这一传统过程带来了革命性的变革，它凭借从海量、高维的化学与生物数据中自动学习复杂模式的强大能力，展现出加速新药发现的巨大潜力。然而，如何将这些先进的计算工具有效地应用于独特的分子结构和复杂的生物系统，填补纯数据驱动方法与药物化学基本原理之间的知识鸿沟，是当前面临的关键问题。

本文旨在系统性地阐述深度学习在[药物发现](@entry_id:261243)和设计中的应用。我们将分三个部分展开：在**第一章：原理与机制**中，我们将深入探讨深度学习模型的基础，从如何将分子转化为机器可读的语言，到为这些数据量身定制的专用[网络架构](@entry_id:268981)，再到不同的学习范式。在**第二章：应用与交叉学科联系**中，我们将展示这些原理如何应用于实际的科学挑战，例如预测分子性质、[从头设计](@entry_id:170778)新分子，并探讨其如何与物理化学、系统生物学和[合成化学](@entry_id:189310)等领域深度融合。最后，在**第三章：动手实践**中，我们提供了一系列精心设计的编程练习，帮助读者将理论知识转化为实践技能。

通过本篇章的学习，您将建立一个从基础理论到前沿应用、从[计算建模](@entry_id:144775)到现实考量的全面知识框架，为在[药物发现](@entry_id:261243)这一交叉学科领域中有效利用[深度学习](@entry_id:142022)奠定坚实的基础。

## 原理与机制

在[药物发现](@entry_id:261243)和设计的复杂领域中，[深度学习](@entry_id:142022)已经成为一种强大的工具，能够从庞大的化学和生物数据中提取有价值的模式。本章旨在深入阐述支撑这些应用的核心原理和关键机制。我们将从最基础的问题开始：如何将分子这种物理实体转化为机器可以理解的语言。接着，我们将探讨为这些独特的[数据结构](@entry_id:262134)量身定制的专用[神经网络架构](@entry_id:637524)。然后，我们将研究不同的学习范式，这些范式使得模型能够从数据中学习，无论是用于预测特性、学习普适的化学表征，还是从头生成全新的分子。最后，我们将讨论确保模型可靠性和稳健性的关键实践，包括严格的评估策略和对预测不确定性的量化。

### [分子表征](@entry_id:752125)：将化学转化为数据

深度学习模型的性能在很大程度上取决于其输入数据的质量和形式。因此，将分子结构转化为信息丰富的数字表征是[药物发现](@entry_id:261243)流程中的第一步，也是至关重要的一步。一个理想的表征应该编码与目标属性相关的化学和物理信息，同时遵循基本的物理[不变性原理](@entry_id:199405)。

最基础的物理原理要求，分子的内在属性（如溶解度或结合能）不应依赖于我们如何任意地标记其原子（**[置换不变性](@entry_id:753356)**），也不应依赖于其在空间中的刚性平移和旋转（**[刚体运动](@entry_id:193355)不变性**）。不同的表征方法在满足这些不变性以及编码信息的完整性方面各有优劣。

**一维表征：SMILES字符串**

简化[分子线](@entry_id:198003)性输入规范（**SMILES**）是一种将分子图转化为 [ASCII](@entry_id:163687) 字符串的紧凑方法。它通过图的深度优先遍历来编码原子和键的连接性，并使用特殊字符表示支链、环和立体化学。例如，乙醇可以表示为 `CCO`。SMILES 的主要优点是其简洁性和人类可读性，使其易于存储和在文本处理模型（如[循环神经网络](@entry_id:171248)或 Transformer）中使用。然而，SMILES 存在固有的局限性。首先，对于同一个分子，可以存在多种有效的 SMILES 字符串（例如，乙醇也可以是 `OCC`），除非通过一个称为**规范化**的确定性算法来生成唯一的SMILES，否则它本身不具有[置换不变性](@entry_id:753356)。更重要的是，SMILES 主要编码了分子的拓扑结构和局部[立体化学](@entry_id:166094)，但完全丢失了[三维几何](@entry_id:176328)信息，如[键长](@entry_id:144592)、键角和二面角。这意味着依赖于分子三维形状的属性，如构象[应变能](@entry_id:162699)，无法仅从 SMILES 中确定。[@problem_id:4332963]

**二维表征：分[子图](@entry_id:273342)**

将分子自然地视为一个图是一种更强大、更灵活的表征方式。在这个**分子图**中，原子是节点（nodes），[化学键](@entry_id:145092)是边（edges）。每个节点可以关联一个特征向量，编码原子序数、[形式电荷](@entry_id:140002)、化合价、是否为芳香性等属性。同样，每条边也可以携带特征，如键的类型（单键、双键等）、是否共轭等。这种表征方式非常适合[图神经网络](@entry_id:136853)（GNNs）。通过在图的邻接结构上直接操作，GNN 能够以一种满足[置换不变性](@entry_id:753356)的方式学习。然而，标准的二维分子图本身同样不包含[三维几何](@entry_id:176328)信息。因此，它无法区分[对映异构体](@entry_id:149008)（互为镜像但不能重合的分子），也无法捕捉构象依赖的性质。为了克服这一点，可以在节点或边特征中明确加入手性标签或扭转角等信息，但这是一种补充而非内生。[@problem_id:4332963]

**三维表征：[分子构象](@entry_id:163456)**

为了最完整地描述分子，我们可以直接使用其在三维欧几里得空间中的原子坐标。一个**三维构象**为每个原子提供了 $x, y, z$ 坐标。这种表征对于需要精确几何信息的任务至关重要，例如预测与蛋白质口袋的结合姿态或计算依赖于距离的相互作用。然而，这种表征也带来了新的挑战。首先，分子的内在属性必须对整体的[旋转和平移](@entry_id:175994)保持不变（即满足 $SE(3)$ 不变性），因此需要专门设计的模型架构来保证这一性质。其次，许多宏观性质（如溶解度或结合自由能）并非由单一的静态构象决定，而是分子在溶液中所有可能构象的**玻尔兹曼系综平均**。使用单个构象（即使是能量最低的构象）作为输入，会忽略构象多样性和熵的贡献，可能导致预测出现偏差。[@problem_id:4332963] 从三维坐标导出的特征，如原子间[距离矩阵](@entry_id:165295)，本身对[旋转和平移](@entry_id:175994)是不变的，但它们无法区分手性分子，因为一个分子与其镜像分子的所有原子间距离都完全相同。

### [深度学习架构](@entry_id:634549)：为分子结构量身定制

一旦选定了[分子表征](@entry_id:752125)，下一步就是设计能够有效处理这些数据的[神经网络架构](@entry_id:637524)。由于分子的结构特性（如图结构、[三维几何](@entry_id:176328)和序列性），通用的深度学习模型往往不是最佳选择。

#### [图神经网络](@entry_id:136853)

对于以图形式表征的分子，**[图神经网络 (GNNs)](@entry_id:750014)** 已成为主导架构。其中，**[消息传递神经网络](@entry_id:751916) (MPNN)** 提供了一个通用且强大的框架。[@problem_id:4332967]

MPNN 的核心思想是，每个原子（节点）的状态通过与其邻居交换“消息”来迭代更新。一个典型的 MPNN 层包含三个步骤：
1.  **消息计算 (Message Computation)**：对于每个节点 $v$，其邻居 $u$ 会生成一条消息 $m_{uv}^{(t)}$。这条消息通常是一个可微函数 $\phi^{(t)}$ 的输出，该函数依赖于中心节点 $v$ 的当前状态 $x_v^{(t)}$、邻居节点 $u$ 的状态 $x_u^{(t)}$ 以及它们之间[化学键](@entry_id:145092)的特征 $e_{uv}$。
2.  **聚合 (Aggregation)**：节点 $v$ 将从其所有邻居收到的消息聚合成一条总消息 $m_v^{(t)}$。这一步通过一个对输入顺序不敏感的函数（如求和、均值或最大值）来完成，例如 $m_v^{(t)} = \sum_{u \in \mathcal{N}(v)} m_{uv}^{(t)}$。这个聚合操作是保证 GNN 对节点索引重排具有[置换不变性](@entry_id:753356)的关键。
3.  **更新 (Update)**：最后，节点 $v$ 的状态会根据其旧状态 $x_v^{(t)}$ 和聚合后的消息 $m_v^{(t)}$ 进行更新，生成新状态 $x_v^{(t+1)}$。这一步由另一个[可微函数](@entry_id:144590) $\psi^{(t)}$ 控制：$x_v^{(t+1)} = \psi^{(t)}(x_v^{(t)}, m_v^{(t)})$。

经过多层[消息传递](@entry_id:751915)后，每个节点的最终[状态编码](@entry_id:169998)了其局部邻域的丰富信息。为了得到整个分子的性质预测，需要一个**读出 (Readout)** 函数，它将所有节点的最终状态集合 $\{x_v^{(T)}\}_{v \in V}$ 转化为一个固定大小的图级别表示，同样需要使用置换不变的操作（如求和或均值）。

MPNN 框架的通用性使其能够涵盖许多其他 GNN 变体。例如，标准的**[图卷积网络](@entry_id:194500) (GCN)** 可以被看作是一个简化的 MPNN，其消息函数不依赖于边的特征 $e_{uv}$，并且聚合方式由标准化的[邻接矩阵](@entry_id:151010)固定。**[图注意力网络](@entry_id:634951) (GAT)** 则通过一个[注意力机制](@entry_id:636429)动态地计算邻居的权重，但标准 GAT 的注意力权重通常也仅由节[点特征](@entry_id:155984)计算得出，而没有原生机制来整合化学上至关重要的键特征。MPNN 框架通过其通用的消息函数 $\phi^{(t)}(x_v^{(t)}, x_u^{(t)}, e_{uv})$，能够自然地将[化学键](@entry_id:145092)的信息融入到学习过程中，这对于精确的[分子性质预测](@entry_id:169815)至关重要。[@problem_id:4332967]

#### [几何深度学习](@entry_id:636472)与 E(3) [等变性](@entry_id:636671)

当模型直接处理分子的三维坐标时，必须尊重物理对称性。一个孤立分子的能量不应因其在空间中的位置或朝向而改变。这种性质被称为**不变性 (invariance)**。同时，作用在每个原子上的力是一个矢量，当整个分子被旋转时，力矢量也应该随之旋转。这种性质被称为**[等变性](@entry_id:636671) (equivariance)**。

形式上，三维空间中的[刚体运动](@entry_id:193355)由欧几里得群 $E(3)$ 描述，其元素 $g = (R, t)$ 由一个旋转/反射矩阵 $R \in O(3)$ 和一个平移向量 $t \in \mathbb{R}^3$ 组成。该[群作用](@entry_id:268812)于原子坐标集合 $X = (x_1, \dots, x_N)$，将每个原子移动到 $g \cdot x_i = R x_i + t$。

一个为物理系统设计的神经网络 $f$ 必须满足以下 $E(3)$ 对称性要求：
*   对于标量输出（如能量 $E(X)$），它必须是**不变的**：$E(g \cdot X) = E(X)$。
*   对于矢量输出（如每个原子上的力 $F(X)$），它必须是**等变的**：$F(g \cdot X) = (R F_1(X), \dots, R F_N(X))$。这意味着力矢量会随着系统的旋转而旋转，但不受系统平移的影响。

这两个条件并非孤立，而是通过物理定律内在关联。力是势能对位置的负梯度，即 $F(X) = -\nabla_X E(X)$。可以证明，如果能量 $E(X)$ 满足 $E(3)$ 不变性，那么其梯度 $\nabla_X E(X)$ 自然会满足 $E(3)$ [等变性](@entry_id:636671)。专门为处理三维点云和[分子构象](@entry_id:163456)而设计的 $E(3)$ [等变神经网络](@entry_id:137437)（如 SchNet, DimeNet++ 等）通过其架构设计，天生就能满足这些对称性约束，从而能够学习到物理上有效和可泛化的分子间作用力势。[@problem_id:4332942]

#### 序列模型与生物大分子的方向性

对于蛋白质和[核酸](@entry_id:164998)等[生物大分子](@entry_id:265296)，它们通常被表示为单体（氨基酸或[核苷](@entry_id:195320)酸）的序列。一个关键的生物学事实是，这些序列具有固有的**方向性**。例如，蛋白质的合成（翻译）过程是单向的：核糖体沿着信使 RNA (mRNA) 从 $5^\prime$ 端移动到 $3^\prime$ 端，并从氨基端（N端）开始，逐个添加氨基酸，直到[羧基](@entry_id:196503)端（C端）完成。

这种单向合成过程具有深刻的建模意义。[@problem_id:4332971]
1.  **[共翻译折叠](@entry_id:266033)**：蛋白质链在完全合成之前，其 N 端部分就已经开始折叠。这意味着蛋白质的最终三维结构和功能（用 $y$ 表示）取决于其序列 $x = (x_1, \dots, x_T)$ 的N-C方向性。反转的序列 $R(x) = (x_T, \dots, x_1)$ 会经历完全不同的折叠路径，因此通常不具备与 $x$ 相同的功能。
2.  **化学和功能的非对称性**：N 端和 C 端具有不同的化学性质（如在生理 pH 下分别为 $\mathrm{-NH_3^+}$ 和 $\mathrm{-COO^-}$）和生物学功能。许多信号肽（如指导[蛋白质分泌](@entry_id:163828)的 N 端信号）和定位信号都是位置特异性的。将一个 N 端信号放到 C 端会使其失效。

因此，强制模型对序列反转具有不变性（即假设 $p(y|x) = p(y|R(x))$）是一个与生物学现实相悖的错误假设。正确的建模策略应该尊重这种固有的因果顺序。对于生成模型，应采用自回归（autoregressive）方法，如 $p(x) = \prod_{t=1}^T p(x_t | x_{1:t-1})$，这恰好模拟了蛋白质的逐个残基延伸过程。对于预测模型，虽然可以使用双向编码器（如标准 Transformer）来捕捉整个序列的上下文，但模型必须能够区分 N 端和 C 端，并且不应通过数据增强等方式强行使 $x$ 和 $R(x)$ 的预测相同。[@problem_id:4332971]

### 学习范式：从数据中提取知识

有了合适的表征和架构，接下来的问题是模型如何从数据中学习。在[药物发现](@entry_id:261243)中，主要应用三种学习范式：监督学习、[自监督学习](@entry_id:173394)和[生成建模](@entry_id:165487)。

#### 监督学习：[定量构效关系](@entry_id:175003) (QSAR)

**[定量构效关系](@entry_id:175003) (QSAR)** 是[药物化学](@entry_id:178806)中的一个经典概念，旨在建立分子结构与其生物活性或理化性质之间的数学关系。在[深度学习](@entry_id:142022)时代，QSAR 被形式化为一个监督学习问题：学习一个从[分子表征](@entry_id:752125) $x$ 到目标属性 $y$ 的映射 $f_\theta(x) = y$。训练过程通过在带标签的数据集上最小化一个[损失函数](@entry_id:136784)（即**[经验风险](@entry_id:633993)**）来进行。

在[药物发现](@entry_id:261243)中，我们通常需要同时预测多种性质，如吸收、分布、代谢、排泄和毒性（ADMET）。这引出了**单任务学习 (STL)** 和**[多任务学习](@entry_id:634517) (MTL)** 之间的选择。[@problem_id:4332972]
*   **单任务学习**：为每个性质（任务）独立训练一个模型。
*   **[多任务学习](@entry_id:634517)**：使用一个共享的编码器 $\phi_\theta$ 来为所有任务生成一个共同的[分子表征](@entry_id:752125) $h = \phi_\theta(x)$，然后再将这个表征送入各自的任务专用预测头 $g_{\psi_t}$。所有任务的损失被联合优化。

[多任务学习](@entry_id:634517)的成功与否取决于任务之间的**相关性**。如果多个 ADMET 属性依赖于共同的潜在分子决定因素（如[分子大小](@entry_id:752128)、极性、[氢键](@entry_id:136659)能力），那么[多任务学习](@entry_id:634517)可以通过汇集来自所有任务的信号来学习一个更鲁棒、更通用的表征 $\phi_\theta$。这相当于为学习共享编码器提供了更多的有效样本，从而降低了[模型参数估计](@entry_id:752080)的方差，提高了泛化能力，这种现象被称为**正向迁移 (positive transfer)**。然而，如果任务是不相关甚至是冲突的（例如，它们需要编码器关注分子完全不同的方面），强制共享表征可能会损害所有任务的性能，因为模型被迫在相互矛盾的目标之间做出妥协。这会导致[模型偏差](@entry_id:184783)的增加，即所谓的**负向迁移 (negative transfer)**。[@problem_id:4332972]

#### [自监督学习](@entry_id:173394)：从未标记数据中学习表征

监督学习需要大量的标记数据，而这在[药物发现](@entry_id:261243)中往往是昂贵和稀缺的。相比之下，已知的化学结构数量巨大。**[自监督学习](@entry_id:173394) (SSL)** 旨在利用这些海量的未标记数据来预训练一个强大的分子编码器，该编码器可以捕捉到普适的化学知识。预训练好的模型随后可以在少量标记数据上进行微调 (fine-tune)，以用于下游的 QSAR 任务。

主要有两种[自监督学习](@entry_id:173394)策略：[@problem_id:4332956]
1.  **生成式 SSL**：这类方法通过“损坏-修复”的方式学习。例如，在**掩蔽原子/键预测**中，我们随机地掩盖分子图中的一些原子或键的类型，然后训练模型根据剩余的上下文来恢复被掩盖的信息。这迫使模型学习原子和键之间的化学环境和规则。这个过程本质上是在最大化给定上下文下，被掩蔽部分的条件似然。
2.  **对比式 SSL**：这类方法的目标是学习一种表征，使得同一分子的不同“视图”在表征空间中彼此接近，而不同分子的表征则相互远离。这里的“视图”是通过对原始分[子图](@entry_id:273342)进行随机增强（如删除原子或键）得到的。这种学习方式可以被看作是在最大化不同视图表征之间的互信息，从而学习到对微小扰动不敏感的、本质的分子特征。

SSL 预训练的核心优势在于，通过在庞大的、多样化的化学空间上学习，模型可以构建一个强大的化学“先验知识”。当这个预训练模型被用于微调一个数据量小或标签有噪声的 QSAR 任务时，这个先验可以起到强大的正则化作用，防止模型对[标签噪声](@entry_id:636605)[过拟合](@entry_id:139093)，并提高其泛化到新化学结构的能力。在微调阶段，模型的目标函数完全切换到下游的监督任务，不再使用预训练时定义的[伪标签](@entry_id:635860)。[@problem_id:4332956]

#### [生成建模](@entry_id:165487)：[从头设计](@entry_id:170778)新分子

除了预测现有分子的性质，深度学习还可以用于一个更具创造性的任务：**从头 (de novo) 设计**全新的、具有理想性质的分子。这需要**生成模型**，其目标是学习训练数据的潜在分布 $p_{\text{data}}(x)$，然后从这个学到的分布中采样，生成新的分子。

目前有几类主流的[生成模型](@entry_id:177561)，它们在原理和特性上各有不同：[@problem_id:4332938]
*   **[变分自编码器 (VAE)](@entry_id:141132)**：VAE 学习一个从数据空间到低维潜在空间的编码器和一个从[潜在空间](@entry_id:171820)到数据空间的解码器。它通过最大化数据[对数似然](@entry_id:273783)的一个下界（ELBO）来进行训练。采样过程是从一个简单的[先验分布](@entry_id:141376)（如高斯分布）中抽取一个潜在向量，然后通过解码器生成一个新分子。VAE 的训练相对稳定，但有时生成的样本质量不高。
*   **[生成对抗网络 (GAN)](@entry_id:141938)**：GAN 由一个生成器和一个判别器组成，两者在博弈中共同训练。生成器试图生成以假乱真的样本，而[判别器](@entry_id:636279)则试图区分真实样本和生成样本。GAN 能够生成高质量的样本，但其训练过程通常不稳定，并且它不直接提供数据似然的评估。
*   **[归一化流](@entry_id:272573) (Normalizing Flows)**：流模型通过一系列可逆的变换，将一个简单的基础分布（如高斯分布）精确地映射到复杂的数据分布。由于其可逆性，流模型可以精确计算任何数据点的[对数似然](@entry_id:273783)，这对于模型评估非常有利。采样和似然计算都非常高效，但其架构必须是可逆的，这在一定程度上限制了其[表达能力](@entry_id:149863)。
*   **[扩散模型](@entry_id:142185) (Diffusion Models)**：[扩散模型](@entry_id:142185)是最新且非常强大的一类生成模型。它首先通过一个固定的过程（[前向过程](@entry_id:634012)）逐步向数据中添加噪声，然后训练一个神经网络来学习如何逆转这个过程（反向过程），即从纯噪声中逐步去噪以恢复出一个清晰的样本。[扩散模型](@entry_id:142185)能够生成极高质量和多样性的样本，但其缺点是采样速度慢，因为需要进行多次迭代[去噪](@entry_id:165626)步骤。

这些[生成模型](@entry_id:177561)为在广阔的化学空间中探索和发现具有新颖骨架和优良特性的候选药物提供了强大的计算引擎。[@problem_id:4332938]

### 稳健性与可靠性：评估与信任模型

在药物发现这样一个高风险领域，仅仅构建一个预测准确的模型是远远不够的。我们还必须能够严格地评估其泛化能力，理解其潜在的失效模式，并量化其预测的[置信度](@entry_id:267904)。

#### 模型评估的陷阱：数据集划分策略

评估[模型泛化](@entry_id:174365)能力的标准方法是在留出的测试集上衡量其性能。然而，如何划分数据集对评估结果有巨大影响。[@problem_id:4332940]
*   **随机划分 (Random Split)**：这是最常见的划分方式，即从整个数据集中随机抽取样本作为[测试集](@entry_id:637546)。在[药物发现](@entry_id:261243)数据集中，分子通常以“类似物系列”的形式存在，即大量分子共享相同的核心骨架（scaffold）。随机划分会将这些高度相似的分子分散到[训练集](@entry_id:636396)和[测试集](@entry_id:637546)中。结果是，模型可能只需在[训练集](@entry_id:636396)中见过的骨架上进行简单的“插值”，就能在[测试集](@entry_id:637546)上取得很高的分数。这种评估结果往往过于乐观，并不能真实反映模型在面对全新化学类型时的表现。
*   **骨架划分 (Scaffold Split)**：为了更严格地测试模型的“外推”能力，可以采用骨架划分。该方法首先识别每个分子的 Bemis-Murcko 骨架，然后确保所有具有相同骨架的分子都只出现在[训练集](@entry_id:636396)或测试集其中之一。这意味着[测试集](@entry_id:637546)中的所有分子都具有模型在训练期间从未见过的骨架。这种划分方式通常会导致性能得分显著低于随机划分，但它能更真实地衡量[模型泛化](@entry_id:174365)到新化学空间的能力。
*   **时间划分 (Temporal Split)**：在实际的药物研发项目中，数据是随时间累积的。时间划分模拟了这种前瞻性部署场景：使用较早的数据作为[训练集](@entry_id:636396)，使用较晚的数据作为测试集。由于化学家们常常会在一段时间内持续优化同一个骨架，因此时间划分的测试集中可能包含训练集中已经出现过的骨架。这使得时间划分在评估对新骨架的泛化能力方面不如骨架划分严格，但它能更真实地反映模型在实际持续部署中的性能表现。

选择正确的划分策略对于获得关于模型真实泛化能力的、无偏的评估至关重要。[@problem_id:4332940]

#### 分布外泛化：理解[域漂移](@entry_id:637840)

模型在实际部署中性能下降的一个常见原因是**[域漂移](@entry_id:637840) (domain shift)**，即测试数据的分布与训练数据的分布不同。形式上，这意味着源域（训练）的联合分布 $P_S(X, Y)$ 与目标域（测试）的联合分布 $P_T(X, Y)$ 不一致。[域漂移](@entry_id:637840)可以分为几种类型：[@problem_id:4332948]

1.  **协变量漂移 (Covariate Shift)**：这是指输入的边缘分布发生了变化（$P_S(X) \neq P_T(X)$），但输入和标签之间的条件关系保持不变（$P_S(Y|X) = P_T(Y|X)$）。在[药物发现](@entry_id:261243)中，一个典型的例子是：模型在一个以激酶为靶点的化合物库上训练，然后被用于筛选一个富含天然产物的化合物库。两个库的化学结构分布（即 $P(X)$）截然不同，但如果筛选的生物学测定方法保持不变，那么分子结构与活性之间的潜在关系（即 $P(Y|X)$）是稳定的。

2.  **标签漂移 (Label Shift)**：这是指标签的边缘分布发生了变化（$P_S(Y) \neq P_T(Y)$），但类别内部的条件分布保持不变（$P_S(X|Y) = P_T(X|Y)$）。例如，模型在一个活性物比例为 $1\%$ 的初筛数据集上训练，然后被用于一个经过富集的、活性物比例为 $10\%$ 的靶向库。尽管活性物和非活性物本身的化学特征分布没有改变，但它们在数据集中出现的频率发生了变化。

3.  **概念漂移 (Concept Shift)**：这是最严重的一种漂移，指输入和标签之间的条件关系本身发生了变化（$P_S(Y|X) \neq P_T(Y|X)$）。这意味着模型需要学习的“概念”已经改变。例如，如果在筛选过程中更换了检测技术（如从荧光偏振法换成另一种方法），并相应地调整了“阳性”的判定标准。那么，对于同一个化合物 $X$，其被标记为活性 ($Y=1$) 的概率就可能发生改变，即 $P(Y|X)$ 发生了变化。

识别出可能发生的[域漂移](@entry_id:637840)类型，对于设计稳健的模型和采取适当的适应策略至关重要。[@problem_id:4332948]

#### 量化预测不确定性

一个负责任的预测模型不仅应该给出预测值，还应该给出对该预测的[置信度](@entry_id:267904)。这种预测的**不确定性**可以分解为两种主要类型：[@problem_id:4332973]

*   **[认知不确定性](@entry_id:149866) (Epistemic Uncertainty)**：源于模型本身知识的局限性，即由于训练数据有限导致我们对模型参数的不确定。它反映了模型的“无知”。这种不确定性是**可约减的**，通过增加更多的训练数据，我们可以让模型学得更好，从而降低这种不确定性。在模型训练数据覆盖不足的化学区域，认知不确定性会很高。

*   **[偶然不确定性](@entry_id:154011) (Aleatoric Uncertainty)**：源于数据本身固有的、无法消除的随机性或噪声。例如，生物学测定中的[实验误差](@entry_id:143154)，或者分子性质本身的内在变异性。这种不确定性是**不可约减的**，即使拥有无限的训练数据，也无法消除。如果这种噪声水平是依赖于输入的（例如，某些分子的性质测量起来就比其他分子更困难），则称之为**异方差的 (heteroscedastic)**。

在实践中，有多种方法可以近似估计这两种不确定性：
*   **[深度集成](@entry_id:636362) (Deep Ensembles)**：通过训练多个结构相同但初始化不同（或训练数据不同）的模型，并观察它们预测结果的方差。这个方差主要反映了模型之间的[分歧](@entry_id:193119)，是[认知不确定性](@entry_id:149866)的一个良好估计。
*   **蒙特卡洛 Dropout (MC Dropout)**：在训练时使用 Dropout，并在预测时也保持其开启，进行多次[前向传播](@entry_id:193086)。这些预测结果的方差同样可以用来估计认知不确定性。这在理论上被解释为对模型参数进行近似的[贝叶斯推断](@entry_id:146958)。
*   **异方差回归 (Heteroscedastic Regression)**：修改神经网络，使其除了预测目标值 $\mu(x)$ 外，还预测一个与输入相关的方差 $\sigma^2(x)$。通过最小化高斯分布的[负对数似然](@entry_id:637801)[损失函数](@entry_id:136784)进行训练，模型可以直接学习到数据的[偶然不确定性](@entry_id:154011)。

在理想情况下，随着训练数据趋于无穷，认知不确定性将趋近于零，而一个设计良好的异方差回归模型所预测的[偶然不确定性](@entry_id:154011)将收敛于数据真实的[条件方差](@entry_id:183803) $\text{Var}(y|x)$。理解并量化这两种不确定性，对于在药物发现中做出明智的决策（例如，决定优先合成哪些高置信度的候选化合物）至关重要。[@problem_id:4332973]