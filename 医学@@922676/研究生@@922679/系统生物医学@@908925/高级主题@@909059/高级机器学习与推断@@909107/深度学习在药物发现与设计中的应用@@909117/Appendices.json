{"hands_on_practices": [{"introduction": "在将深度学习应用于药物发现之前，我们必须解决一个根本问题：如何将分子的结构语言转化为机器能够理解的数学语言。分子指纹是一种经典且强大的方法，它将复杂的拓扑结构编码为固定长度的向量。通过本练习，您将亲手计算一个扩展连通性指纹（ECFP），深入了解原子信息是如何通过迭代邻域信息聚合，并最终被哈希到一个比特向量中的。这个过程不仅揭示了分子表示的核心机制，也让您直面指纹技术的一个关键挑战——哈希碰撞。[@problem_id:4333002]", "problem": "一个分子被表示为一个无向标记图，其邻接矩阵为 $\\mathbf{A} \\in \\{0,1\\}^{6 \\times 6}$，原子特征矩阵为 $\\mathbf{X} \\in \\mathbb{Z}^{6 \\times 5}$。邻接矩阵编码了重原子（非氢原子）的连接性（氢原子不是显式节点），原子特征按以下顺序使用列：原子序数 $Z$、重原子度 $d$、形式电荷 $q$、隐式氢原子数 $h$ 和芳香性标志 $a \\in \\{0,1\\}$。数据如下：\n$$\n\\mathbf{A} \\;=\\;\n\\begin{pmatrix}\n0  1  0  0  0  0 \\\\\n1  0  1  0  0  0 \\\\\n0  1  0  1  1  0 \\\\\n0  0  1  0  0  0 \\\\\n0  0  1  0  0  1 \\\\\n0  0  0  0  1  0\n\\end{pmatrix},\n\\qquad\n\\mathbf{X} \\;=\\;\n\\begin{pmatrix}\n6  1  0  3  0 \\\\\n6  2  0  2  0 \\\\\n6  3  0  1  0 \\\\\n7  1  0  2  0 \\\\\n6  2  0  0  0 \\\\\n8  1  0  0  0\n\\end{pmatrix}.\n$$\n您将根据以下明确的、确定性的方案计算半径为 $2$ 的扩展连接性指纹（ECFP，也称为 Morgan 指纹），该方案与标准实践兼容，同时仍可手动计算：\n\n1. 初始原子不变量。对于每个原子 $v \\in \\{1,\\dots,6\\}$，定义初始标识符\n$$\nI^{(0)}(v) \\;=\\; Z_v \\cdot 10^{4} \\;+\\; d_v \\cdot 10^{3} \\;+\\; q_v \\cdot 10^{2} \\;+\\; h_v \\cdot 10 \\;+\\; a_v.\n$$\n\n2. 邻域扩展。对于迭代 $t \\in \\{1,2\\}$，定义\n$$\nI^{(t)}(v) \\;=\\; I^{(t-1)}(v) \\;+\\; \\sum_{u \\in \\mathcal{N}(v)} I^{(t-1)}(u),\n$$\n其中 $\\mathcal{N}(v)$ 是由 $\\mathbf{A}$ 定义的图中原子 $v$ 的邻居集合。\n\n3. 位向量哈希。设 $m = 16$ 为固定位向量长度。通过以下方式将任何标识符 $y \\in \\mathbb{Z}$ 映射到一个比特位：\n$$\nh(y) \\;=\\; \\left( 7 y \\;+\\; 13 \\right) \\bmod 16.\n$$\n半径为 $2$ 的 Morgan 指纹是比特位集合 $\\{\\, h(y) \\mid y \\in \\mathcal{S} \\,\\}$，其中\n$$\n\\mathcal{S} \\;=\\; \\big\\{ I^{(0)}(v), I^{(1)}(v), I^{(2)}(v) \\;\\big|\\; v \\in \\{1,\\dots,6\\} \\big\\}.\n$$\n\n在固定长度的指纹中，不同的标识符 $y$ 可能会哈希到相同的比特索引，从而产生冲突。将冲突计数定义为\n$$\nC \\;=\\; |\\mathcal{S}| \\;-\\; \\big|\\{\\, h(y) \\mid y \\in \\mathcal{S} \\,\\}\\big|.\n$$\n\n从提供的 $\\mathbf{A}$ 和 $\\mathbf{X}$ 出发，并仅使用上述定义，计算 $C$。将您的最终答案表示为一个没有单位的整数。不需要四舍五入。", "solution": "用户提供了一个需要计算扩展连接性指纹（ECFP）的冲突计数的问题。我已经验证了该问题，并发现它具有科学依据、问题明确、客观且自成体系。邻接矩阵 $\\mathbf{A}$ 和特征矩阵 $\\mathbf{X}$ 中提供的数据是一致的，特别是在重原子度方面。指定的算法是确定性的和完备的。因此，我将开始解答。\n\n解决方案需要遵循一个多步骤算法。\n\n**步骤 1：计算初始原子不变量, $I^{(0)}(v)$**\n\n每个原子 $v$ 的初始标识符由以下公式给出：\n$$I^{(0)}(v) = Z_v \\cdot 10^4 + d_v \\cdot 10^3 + q_v \\cdot 10^2 + h_v \\cdot 10 + a_v$$\n使用给定特征矩阵 $\\mathbf{X}$ 的行：\n- 原子 1 ($Z=6, d=1, q=0, h=3, a=0$):\n$I^{(0)}(1) = 6 \\cdot 10000 + 1 \\cdot 1000 + 0 \\cdot 100 + 3 \\cdot 10 + 0 = 61030$\n- 原子 2 ($Z=6, d=2, q=0, h=2, a=0$):\n$I^{(0)}(2) = 6 \\cdot 10000 + 2 \\cdot 1000 + 0 \\cdot 100 + 2 \\cdot 10 + 0 = 62020$\n- 原子 3 ($Z=6, d=3, q=0, h=1, a=0$):\n$I^{(0)}(3) = 6 \\cdot 10000 + 3 \\cdot 1000 + 0 \\cdot 100 + 1 \\cdot 10 + 0 = 63010$\n- 原子 4 ($Z=7, d=1, q=0, h=2, a=0$):\n$I^{(0)}(4) = 7 \\cdot 10000 + 1 \\cdot 1000 + 0 \\cdot 100 + 2 \\cdot 10 + 0 = 71020$\n- 原子 5 ($Z=6, d=2, q=0, h=0, a=0$):\n$I^{(0)}(5) = 6 \\cdot 10000 + 2 \\cdot 1000 + 0 \\cdot 100 + 0 \\cdot 10 + 0 = 62000$\n- 原子 6 ($Z=8, d=1, q=0, h=0, a=0$):\n$I^{(0)}(6) = 8 \\cdot 10000 + 1 \\cdot 1000 + 0 \\cdot 100 + 0 \\cdot 10 + 0 = 81000$\n\n初始标识符的集合是 $\\{61030, 62020, 63010, 71020, 62000, 81000\\}$。\n\n**步骤 2：计算第一次迭代的不变量, $I^{(1)}(v)$**\n\n更新规则为 $I^{(t)}(v) = I^{(t-1)}(v) + \\sum_{u \\in \\mathcal{N}(v)} I^{(t-1)}(u)$。首先，我们从邻接矩阵 $\\mathbf{A}$ 确定邻居集合 $\\mathcal{N}(v)$：\n- $\\mathcal{N}(1) = \\{2\\}$\n- $\\mathcal{N}(2) = \\{1, 3\\}$\n- $\\mathcal{N}(3) = \\{2, 4, 5\\}$\n- $\\mathcal{N}(4) = \\{3\\}$\n- $\\mathcal{N}(5) = \\{3, 6\\}$\n- $\\mathcal{N}(6) = \\{5\\}$\n\n现在我们计算 $v \\in \\{1,\\dots,6\\}$ 的 $I^{(1)}(v)$ 值：\n- $I^{(1)}(1) = I^{(0)}(1) + I^{(0)}(2) = 61030 + 62020 = 123050$\n- $I^{(1)}(2) = I^{(0)}(2) + I^{(0)}(1) + I^{(0)}(3) = 62020 + 61030 + 63010 = 186060$\n- $I^{(1)}(3) = I^{(0)}(3) + I^{(0)}(2) + I^{(0)}(4) + I^{(0)}(5) = 63010 + 62020 + 71020 + 62000 = 258050$\n- $I^{(1)}(4) = I^{(0)}(4) + I^{(0)}(3) = 71020 + 63010 = 134030$\n- $I^{(1)}(5) = I^{(0)}(5) + I^{(0)}(3) + I^{(0)}(6) = 62000 + 63010 + 81000 = 206010$\n- $I^{(1)}(6) = I^{(0)}(6) + I^{(0)}(5) = 81000 + 62000 = 143000$\n\n第一次迭代的标识符集合是 $\\{123050, 186060, 258050, 134030, 206010, 143000\\}$。\n\n**步骤 3：计算第二次迭代的不变量, $I^{(2)}(v)$**\n\n我们再次应用更新规则，使用 $I^{(1)}(v)$ 的值：\n- $I^{(2)}(1) = I^{(1)}(1) + I^{(1)}(2) = 123050 + 186060 = 309110$\n- $I^{(2)}(2) = I^{(1)}(2) + I^{(1)}(1) + I^{(1)}(3) = 186060 + 123050 + 258050 = 567160$\n- $I^{(2)}(3) = I^{(1)}(3) + I^{(1)}(2) + I^{(1)}(4) + I^{(1)}(5) = 258050 + 186060 + 134030 + 206010 = 784150$\n- $I^{(2)}(4) = I^{(1)}(4) + I^{(1)}(3) = 134030 + 258050 = 392080$\n- $I^{(2)}(5) = I^{(1)}(5) + I^{(1)}(3) + I^{(1)}(6) = 206010 + 258050 + 143000 = 607060$\n- $I^{(2)}(6) = I^{(1)}(6) + I^{(1)}(5) = 143000 + 206010 = 349010$\n\n第二次迭代的标识符集合是 $\\{309110, 567160, 784150, 392080, 607060, 349010\\}$。\n\n**步骤 4：构建所有标识符的集合, $\\mathcal{S}$**\n\n集合 $\\mathcal{S}$ 是所有生成的标识符的并集：\n$\\mathcal{S} = \\{I^{(0)}(v), I^{(1)}(v), I^{(2)}(v) \\mid v \\in \\{1,\\dots,6\\}\\}$。\n- $I^{(0)}$ 集合: $\\{61030, 62020, 63010, 71020, 62000, 81000\\}$\n- $I^{(1)}$ 集合: $\\{123050, 186060, 258050, 134030, 206010, 143000\\}$\n- $I^{(2)}$ 集合: $\\{309110, 567160, 784150, 392080, 607060, 349010\\}$\n所有 $6 \\times 3 = 18$ 个标识符都是唯一的。因此，集合 $\\mathcal{S}$ 的基数是 $|\\mathcal{S}|=18$。\n\n**步骤 5：对所有标识符进行哈希，并确定唯一哈希值的集合**\n\n哈希函数是 $h(y) = (7y + 13) \\pmod{16}$。我们将其应用于 $\\mathcal{S}$ 中的每个标识符（共18个）。为提高效率，我们使用属性 $h(y) = (7(y \\pmod{16}) + 13) \\pmod{16}$。\n\n$I^{(0)}$ 标识符的哈希值：\n- $h(61030) = (7(61030 \\bmod 16) + 13) \\bmod 16 = (7 \\cdot 6 + 13) \\bmod 16 = 55 \\bmod 16 = 7$\n- $h(62020) = (7(62020 \\bmod 16) + 13) \\bmod 16 = (7 \\cdot 4 + 13) \\bmod 16 = 41 \\bmod 16 = 9$\n- $h(63010) = (7(63010 \\bmod 16) + 13) \\bmod 16 = (7 \\cdot 2 + 13) \\bmod 16 = 27 \\bmod 16 = 11$\n- $h(71020) = (7(71020 \\bmod 16) + 13) \\bmod 16 = (7 \\cdot 12 + 13) \\bmod 16 = 97 \\bmod 16 = 1$\n- $h(62000) = (7(62000 \\bmod 16) + 13) \\bmod 16 = (7 \\cdot 0 + 13) \\bmod 16 = 13 \\bmod 16 = 13$\n- $h(81000) = (7(81000 \\bmod 16) + 13) \\bmod 16 = (7 \\cdot 8 + 13) \\bmod 16 = 69 \\bmod 16 = 5$\n\n$I^{(1)}$ 标识符的哈希值：\n- $h(123050) = (7 \\cdot 10 + 13) \\bmod 16 = 83 \\bmod 16 = 3$\n- $h(186060) = (7 \\cdot 12 + 13) \\bmod 16 = 97 \\bmod 16 = 1$\n- $h(258050) = (7 \\cdot 2 + 13) \\bmod 16 = 27 \\bmod 16 = 11$\n- $h(134030) = (7 \\cdot 14 + 13) \\bmod 16 = 111 \\bmod 16 = 15$\n- $h(206010) = (7 \\cdot 10 + 13) \\bmod 16 = 83 \\bmod 16 = 3$\n- $h(143000) = (7 \\cdot 8 + 13) \\bmod 16 = 69 \\bmod 16 = 5$\n\n$I^{(2)}$ 标识符的哈希值：\n- $h(309110) = (7 \\cdot 6 + 13) \\bmod 16 = 55 \\bmod 16 = 7$\n- $h(567160) = (7 \\cdot 8 + 13) \\bmod 16 = 69 \\bmod 16 = 5$\n- $h(784150) = (7 \\cdot 6 + 13) \\bmod 16 = 55 \\bmod 16 = 7$\n- $h(392080) = (7 \\cdot 0 + 13) \\bmod 16 = 13 \\bmod 16 = 13$\n- $h(607060) = (7 \\cdot 4 + 13) \\bmod 16 = 41 \\bmod 16 = 9$\n- $h(349010) = (7 \\cdot 2 + 13) \\bmod 16 = 27 \\bmod 16 = 11$\n\n计算出的所有18个哈希值的集合是 $\\{7, 9, 11, 1, 13, 5, 3, 1, 11, 15, 3, 5, 7, 5, 7, 13, 9, 11\\}$。\n唯一哈希值的集合是这些计算值的并集：\n$\\{\\, h(y) \\mid y \\in \\mathcal{S} \\,\\} = \\{1, 3, 5, 7, 9, 11, 13, 15\\}$。\n唯一哈希值的数量是 $|\\{\\, h(y) \\mid y \\in \\mathcal{S} \\,\\}| = 8$。\n\n**步骤 6：计算冲突计数, $C$**\n\n冲突计数定义为 $C = |\\mathcal{S}| - |\\{\\, h(y) \\mid y \\in \\mathcal{S} \\,\\}|$。代入计算出的值：\n$$C = 18 - 8 = 10$$\n冲突计数为 $10$。", "answer": "$$\\boxed{10}$$", "id": "4333002"}, {"introduction": "一旦我们将分子转化为机器可读的格式，下一步就是学习“优秀”的分子表示，即能够捕捉到与生物活性等关键性质相关的复杂化学信息的嵌入向量。自监督对比学习是实现这一目标的强大范式，它使模型能够从未标记的海量化学数据中学习。本练习将带您深入对比学习损失函数的核心，通过一个具体的计算案例，揭示模型是如何通过“拉近”相似分子（正样本）的表示并“推开”不相似分子（负样本）的表示来学习的，并探讨温度参数 $\\tau$ 在调节表示空间结构中的关键作用。[@problem_id:4332990]", "problem": "一个系统生物医学研究团队正在训练一个自监督表示学习模型，用于为下游的药物发现任务嵌入分子。每个分子被表示为一个图，模型是一个图神经网络（GNN）。在训练过程中，团队对每个分子使用两种随机增强来形成正样本对，而小批量（mini-batch）中的所有其他视图则作为隐式负样本。嵌入向量经过 $\\ell_{2}$-归一化，因此它们的点积等于余弦相似度。训练目标遵循最大似然原理：对于每个锚点视图 $i$，模型使用能量 $E_{ij} = - s_{ij}/\\tau$ 在所有其他视图 $j \\neq i$ 上定义一个吉布斯分布，其中 $s_{ij}$ 是相似度，$\\tau$ 是一个温度参数。分配给正确匹配 $j^{+}$（同一分子的另一个增强视图）的概率是通过对指数化的负能量进行归一化得到的，每个锚点的损失是 $j^{+}$ 的负对数似然。批次损失是所有视图的单个锚点损失的平均值。\n\n考虑一个包含3个不同分子的小批量，每个分子有2个增强，总共产生6个视图。它们在 $\\mathbb{R}^{2}$ 中的 $\\ell_{2}$-归一化嵌入向量如下：\n- 分子 A: $a_{1} = (1, 0)$ 和 $a_{2} = (0.8, 0.6)$。\n- 分子 B: $b_{1} = (0, 1)$ 和 $b_{2} = (-0.6, 0.8)$。\n- 分子 C: $c_{1} = (-1, 0)$ 和 $c_{2} = (-0.8, -0.6)$。\n\n温度为 $\\tau = 0.5$。对于每个锚点（6个视图中的每一个），真正的正样本是来自同一分子的配对增强视图，所有其他5个视图构成吉布斯分布归一化所使用的候选集。使用上述基本定义（能量为 $E_{ij} = -s_{ij}/\\tau$ 的吉布斯分布、每个锚点的负对数似然损失以及对所有锚点进行平均），计算此小批量的批次对比损失。将最终的批次损失表示为一个无量纲量，并将您的答案四舍五入到四位有效数字。\n\n此外，基于相同的基础设置，定性和定量地解释改变温度 $\\tau$ 如何影响学习到的表示空间中正负嵌入向量之间的分离度。您的解释必须基于第一性原理（概率归一化、交叉熵和带温度的 softmax 函数的梯度行为），不得使用未经证明的简化公式。", "solution": "用户提供了一个关于系统生物医学深度学习领域的问题，具体聚焦于分子的对比表示学习。任务有两部分：首先，根据一组给定的嵌入向量和温度参数，计算一个具体的批次损失值；其次，解释温度参数在此学习框架中的作用。\n\n### **问题验证**\n\n**步骤1：提取已知条件**\n- **模型：** 一个用于自监督表示学习的图神经网络（GNN）。\n- **数据：** 3个分子，每个分子有2个增强，在一个小批量中总共产生6个视图。\n- **嵌入向量：** 所有嵌入向量都是 $\\mathbb{R}^2$ 中经过 $\\ell_2$-归一化的向量。\n    - 分子 A: $a_{1} = (1, 0)$ 和 $a_{2} = (0.8, 0.6)$。\n    - 分子 B: $b_{1} = (0, 1)$ 和 $b_{2} = (-0.6, 0.8)$。\n    - 分子 C: $c_{1} = (-1, 0)$ 和 $c_{2} = (-0.8, -0.6)$。\n- **相似度度量：** 对于 $\\ell_2$-归一化的嵌入向量，相似度 $s_{ij}$ 是余弦相似度，等同于嵌入向量的点积。\n- **损失公式：**\n    - 训练目标基于最大似然原理。\n    - 对于一个锚点视图 $i$ 和任何其他视图 $j \\neq i$，能量为 $E_{ij} = -s_{ij}/\\tau$。\n    - 锚点 $i$ 的正样本匹配 $j^{+}$ 的概率由一个在所有其他视图 $k \\neq i$ 上归一化的吉布斯分布给出：\n      $$P(j^{+}|i) = \\frac{\\exp(-E_{ij^{+}})}{\\sum_{k \\neq i} \\exp(-E_{ik})} = \\frac{\\exp(s_{ij^{+}}/\\tau)}{\\sum_{k \\neq i} \\exp(s_{ik}/\\tau)}$$\n    - 锚点 $i$ 的损失是负对数似然：$\\mathcal{L}_i = -\\ln(P(j^{+}|i))$。\n    - 总批次损失 $\\mathcal{L}_{\\text{batch}}$ 是所有6个视图的单个锚点损失的平均值。\n- **参数：** 温度 $\\tau = 0.5$。\n- **任务1：** 计算 $\\mathcal{L}_{\\text{batch}}$ 并四舍五入到四位有效数字。\n- **任务2：** 解释 $\\tau$ 对嵌入向量分离度的定性和定量影响。\n\n**步骤2：使用提取的已知条件进行验证**\n该问题具有科学依据，描述了一个标准的对比学习框架（InfoNCE损失）。所有需要的数据（嵌入向量、温度）和定义（损失函数、相似度）都已提供。给定的嵌入向量经核实是 $\\ell_2$-归一化的：例如，对于 $a_2$，$\\|a_2\\|_2 = \\sqrt{0.8^2 + 0.6^2} = \\sqrt{0.64 + 0.36} = \\sqrt{1} = 1$。这对所有6个向量都成立，确保了问题陈述的自洽性且不矛盾。该问题是适定的、客观的且可形式化的。\n\n**步骤3：结论与行动**\n问题有效。将提供完整的解决方案。\n\n### **第一部分：批次对比损失的计算**\n\n设6个视图的集合为 $V = \\{v_1, v_2, v_3, v_4, v_5, v_6\\}$，分别对应 $\\{a_1, a_2, b_1, b_2, c_1, c_2\\}$。温度 $\\tau = 0.5$，因此缩放因子为 $1/\\tau = 2$。\n\n首先，我们计算成对相似度矩阵 $S$，其中 $S_{ij} = v_i \\cdot v_j$。\n- **正样本对相似度：**\n  $s_{a_1, a_2} = (1, 0) \\cdot (0.8, 0.6) = 0.8$\n  $s_{b_1, b_2} = (0, 1) \\cdot (-0.6, 0.8) = 0.8$\n  $s_{c_1, c_2} = (-1, 0) \\cdot (-0.8, -0.6) = 0.8$\n  每个正样本对的相似度都是 $0.8$。\n\n- **负样本对相似度：** 我们计算来自不同分子的视图之间的所有点积。\n  $s_{a_1, b_1} = (1, 0) \\cdot (0, 1) = 0$\n  $s_{a_1, b_2} = (1, 0) \\cdot (-0.6, 0.8) = -0.6$\n  $s_{a_1, c_1} = (1, 0) \\cdot (-1, 0) = -1$\n  $s_{a_1, c_2} = (1, 0) \\cdot (-0.8, -0.6) = -0.8$\n  $s_{a_2, b_1} = (0.8, 0.6) \\cdot (0, 1) = 0.6$\n  $s_{a_2, b_2} = (0.8, 0.6) \\cdot (-0.6, 0.8) = -0.48 + 0.48 = 0$\n  $s_{a_2, c_1} = (0.8, 0.6) \\cdot (-1, 0) = -0.8$\n  $s_{a_2, c_2} = (0.8, 0.6) \\cdot (-0.8, -0.6) = -0.64 - 0.36 = -1$\n  $s_{b_1, c_1} = (0, 1) \\cdot (-1, 0) = 0$\n  $s_{b_1, c_2} = (0, 1) \\cdot (-0.8, -0.6) = -0.6$\n  $s_{b_2, c_1} = (-0.6, 0.8) \\cdot (-1, 0) = 0.6$\n  $s_{b_2, c_2} = (-0.6, 0.8) \\cdot (-0.8, -0.6) = 0.48 - 0.48 = 0$\n\n对于一个锚点 $i$ 及其正样本对伙伴 $j^{+}$，其单个锚点损失为：\n$$ \\mathcal{L}_i = -\\ln \\left( \\frac{\\exp(s_{ij^{+}}/\\tau)}{\\sum_{k \\neq i} \\exp(s_{ik}/\\tau)} \\right) = -s_{ij^{+}}/\\tau + \\ln \\left( \\sum_{k \\neq i} \\exp(s_{ik}/\\tau) \\right) $$\n\n我们为6个锚点中的每一个计算此损失。设正样本相似度 $s_{\\text{pos}} = 0.8$。那么 $s_{\\text{pos}}/\\tau = 0.8 / 0.5 = 1.6$。\n\n- **锚点 $a_1$**：其他视图为 $\\{a_2, b_1, b_2, c_1, c_2\\}$。相似度为 $\\{0.8, 0, -0.6, -1, -0.8\\}$。分母的和为：\n  $D_{a_1} = \\exp(1.6) + \\exp(0/0.5) + \\exp(-0.6/0.5) + \\exp(-1/0.5) + \\exp(-0.8/0.5) = \\exp(1.6) + \\exp(0) + \\exp(-1.2) + \\exp(-2) + \\exp(-1.6)$。\n  $\\mathcal{L}_{a_1} = -\\ln(\\exp(1.6)/D_{a_1}) = \\ln(D_{a_1}) - 1.6$。\n\n- **锚点 $a_2$**：其他视图为 $\\{a_1, b_1, b_2, c_1, c_2\\}$。相似度为 $\\{0.8, 0.6, 0, -0.8, -1\\}$。分母的和为：\n  $D_{a_2} = \\exp(1.6) + \\exp(0.6/0.5) + \\exp(0/0.5) + \\exp(-0.8/0.5) + \\exp(-1/0.5) = \\exp(1.6) + \\exp(1.2) + \\exp(0) + \\exp(-1.6) + \\exp(-2)$。\n  $\\mathcal{L}_{a_2} = \\ln(D_{a_2}) - 1.6$。\n\n由于嵌入向量的对称性（$c_1 = -a_1$, $c_2 = -a_2$），我们发现锚点 $c_1$ 和 $c_2$ 的相似度集合与 $a_2$ 和 $a_1$ 的相似度集合相关。$\\mathcal{L}_{c_1} = \\mathcal{L}_{a_2}$ 且 $\\mathcal{L}_{c_2} = \\mathcal{L}_{a_1}$。\n\n- **锚点 $b_1$**：其他视图为 $\\{b_2, a_1, a_2, c_1, c_2\\}$。相似度为 $\\{0.8, 0, 0.6, 0, -0.6\\}$。分母的和为：\n  $D_{b_1} = \\exp(1.6) + \\exp(0/0.5) + \\exp(0.6/0.5) + \\exp(0/0.5) + \\exp(-0.6/0.5) = \\exp(1.6) + \\exp(1.2) + 2\\exp(0) + \\exp(-1.2)$。\n  $\\mathcal{L}_{b_1} = \\ln(D_{b_1}) - 1.6$。\n\n- **锚点 $b_2$**：其他视图为 $\\{b_1, a_1, a_2, c_1, c_2\\}$。相似度为 $\\{0.8, -0.6, 0, 0.6, 0\\}$。这与 $b_1$ 的集合相同。\n  $D_{b_2} = D_{b_1}$，因此 $\\mathcal{L}_{b_2} = \\mathcal{L}_{b_1}$。\n\n总批次损失为 $\\mathcal{L}_{\\text{batch}} = \\frac{1}{6} \\sum_i \\mathcal{L}_i = \\frac{1}{6} (2\\mathcal{L}_{a_1} + 2\\mathcal{L}_{a_2} + 2\\mathcal{L}_{b_1}) = \\frac{1}{3} (\\mathcal{L}_{a_1} + \\mathcal{L}_{a_2} + \\mathcal{L}_{b_1})$。\n\n现在，我们计算数值：\n$D_{a_1} \\approx 4.95303 + 1 + 0.30119 + 0.13534 + 0.20190 = 6.59146$\n$\\mathcal{L}_{a_1} = \\ln(6.59146) - 1.6 \\approx 1.88577 - 1.6 = 0.28577$\n\n$D_{a_2} \\approx 4.95303 + 3.32012 + 1 + 0.20190 + 0.13534 = 9.61039$\n$\\mathcal{L}_{a_2} = \\ln(9.61039) - 1.6 \\approx 2.26282 - 1.6 = 0.66282$\n\n$D_{b_1} \\approx 4.95303 + 3.32012 + 2(1) + 0.30119 = 10.57434$\n$\\mathcal{L}_{b_1} = \\ln(10.57434) - 1.6 \\approx 2.35846 - 1.6 = 0.75846$\n\n$\\mathcal{L}_{\\text{batch}} = \\frac{1}{3} (0.28577 + 0.66282 + 0.75846) = \\frac{1}{3}(1.70705) \\approx 0.569016...$\n\n四舍五入到四位有效数字，批次损失为 $0.5690$。\n\n### **第二部分：温度 $\\tau$ 的影响**\n\n温度参数 $\\tau$ 是一个关键的超参数，它调节学习到的表示空间中正负嵌入向量之间的分离度。其影响可以通过检查它对概率分布以及驱动学习的最终梯度所产生的作用来理解。\n\n对于一个锚点 $i$，识别出正确的正样本对 $j^{+}$ 的概率由应用于缩放后相似度的 softmax 函数给出：\n$$ P(j^{+}|i) = \\frac{\\exp(s_{ij^{+}}/\\tau)}{\\sum_{k \\neq i} \\exp(s_{ik}/\\tau)} $$\n损失函数 $\\mathcal{L}_i = -\\ln P(j^{+}|i)$ 是模型的预测概率分布与一个理想的 one-hot 分布之间的交叉熵，在理想分布中，正样本对的概率为1。\n\n**定性影响：**\n温度 $\\tau$ 控制模型输出分布的“尖锐度”。\n- 当 $\\tau$ **很低**（接近0）时，项 $1/\\tau$ 变得非常大。这会放大相似度分数之间的差异。softmax 函数的输出会高度集中在具有最大相似度 $s_{ik}$ 的视图 $k$ 上。低 $\\tau$ 迫使模型使正样本对的相似度 $s_{ij^{+}}$ 显著大于所有负样本对的相似度。任何与正样本相似度接近的负样本都会导致巨大的损失。这有效地实现了一种“困难负样本挖掘”，因为损失及其梯度由最具挑战性的负样本（那些与锚点最相似的样本）主导。这鼓励在嵌入空间中正负样本对之间形成一个大的间隔。\n\n- 当 $\\tau$ **很高**（接近 $\\infty$）时，对于所有的 $k$，项 $s_{ik}/\\tau$ 都趋近于0。因此，$\\exp(s_{ik}/\\tau)$ 趋近于1。概率分布在所有 $N-1$ 个候选视图上变得近似均匀，其中 $P(j^{+}|i) \\approx \\frac{1}{N-1}$。损失趋近于一个常数值 $\\ln(N-1)$。在这种情况下，损失对实际的相似度值变得不敏感，导致梯度趋近于零。模型接收到非常弱的学习信号，导致收敛性差，正负嵌入向量之间的分离度也极小。\n\n**定量影响（梯度分析）：**\n学习过程由损失 $\\mathcal{L}_i$ 对模型参数的梯度所控制，该梯度通过相似度分数进行传导。我们来分析 $\\mathcal{L}_i$ 对相似度分数 $s_{iz}$ 的梯度。根据链式法则，$\\frac{\\partial \\mathcal{L}_i}{\\partial s_{iz}} = \\frac{\\partial \\mathcal{L}_i}{\\partial u_z} \\frac{\\partial u_z}{\\partial s_{iz}}$，其中 $u_z = s_{iz}/\\tau$。\n\n损失对相似度分数的梯度为：\n1.  对于正样本对相似度 $s_{ij^{+}}$：\n    $$ \\frac{\\partial \\mathcal{L}_i}{\\partial s_{ij^{+}}} = \\frac{1}{\\tau} (P(j^{+}|i) - 1) $$\n2.  对于负样本对相似度 $s_{im}$（其中 $m$ 是一个负样本）：\n    $$ \\frac{\\partial \\mathcal{L}_i}{\\partial s_{im}} = \\frac{1}{\\tau} P(m|i) $$\n\n从这些表达式中，我们可以推断出 $\\tau$ 的定量效应：\n- 因子 $1/\\tau$ 缩放了所有梯度的幅度。较低的 $\\tau$ 会导致较大的梯度，可能加速学习（如果不引起不稳定性的话）。\n- 关键的是，$\\tau$ 也塑造了决定梯度相对强度的概率值 $P(\\cdot|i)$。\n- 对于**低 $\\tau$**，分布 $P$ 是尖锐的。如果模型表现良好，$P(j^{+}|i) \\to 1$ 并且对于所有负样本 $m$，$P(m|i) \\to 0$。正样本对的梯度 $\\frac{1}{\\tau}(P(j^{+}|i)-1)$ 趋近于0，这意味着模型停止进一步推开已经分离得很好的正样本。对于一个“简单”负样本（低 $s_{im}$），其梯度也接近于零。然而，对于一个具有不可忽略相似度的“困难”负样本，$P(m|i)$ 会很小但非零，而梯度 $\\frac{1}{\\tau}P(m|i)$ 可能相当大。因此，模型被迫集中精力将这些困难负样本推离锚点。\n- 对于**高 $\\tau$**，分布是扁平的，对于所有 $m$，$P(m|i) \\approx \\frac{1}{N-1}$。正样本对的梯度是一个小的负常数 $\\approx \\frac{1}{\\tau}(\\frac{1}{N-1}-1)$，所有负样本对的梯度是一个小的正常数 $\\approx \\frac{1}{\\tau}\\frac{1}{N-1}$。更新信号很弱，并且不区分简单和困难的负样本，这为学习一个有区分度的表示空间提供了一个无效的惩罚结构。\n\n总之，$\\tau$ 调整了对排序错误的相似度的惩罚。低温通过更严厉地惩罚困难负样本来强制实现更强的分离，而高温则削弱了学习信号，并且无法强制实现一个清晰的间隔。", "answer": "$$\\boxed{0.5690}$$", "id": "4332990"}, {"introduction": "深度学习模型在药物发现中展现出强大的预测能力，但其“黑箱”特性往往使其决策过程难以理解，这在科学驱动的药物研发中是不可接受的。因此，模型可解释性是至关重要的一环。本练习将引导您使用一种严谨的归因方法——积分梯度（Integrated Gradients），来剖析一个训练好的图神经网络（GNN）的预测逻辑。通过计算每个原子对最终预测的贡献得分，并将其与药物化学专家的结构-活性关系（SAR）知识进行验证，您将学会如何打开模型的“黑箱”，从而验证其化学直觉的合理性。[@problem_id:4333008]", "problem": "您将处理一个简化的、但科学上真实的归因任务。该任务针对一个已训练的图神经网络（GNN；Graph Neural Network），该网络用于小分子活性预测。考虑一个分子，它表示为由三个原子组成的链，连接方式为 $1 \\text{--} 2 \\text{--} 3$。学习到的第一个消息传递层是线性的，并由对称混合矩阵表示\n$$\nM \\;=\\; \\begin{pmatrix}\n1  0.5  0 \\\\\n0.5  1  0.5 \\\\\n0  0.5  1\n\\end{pmatrix}.\n$$\n每个原子有一个标量输入特征，这些特征被收集到向量 $x \\in \\mathbb{R}^{3}$ 中，其值为\n$$\nx \\;=\\; \\begin{pmatrix} 1.2 \\\\ 0.7 \\\\ 0.3 \\end{pmatrix}.\n$$\n设此层之后的已训练读出层由权重向量 $w \\in \\mathbb{R}^{3}$ 定义，\n$$\nw \\;=\\; \\begin{pmatrix} 0.8 \\\\ -0.3 \\\\ 0.5 \\end{pmatrix},\n$$\n标量预测值由以下复合函数给出\n$$\nF(x) \\;=\\; g\\!\\left( w^{\\top} (M x)\\right),\n$$\n其中 $g(z) \\;=\\; \\ln\\!\\big(1 + \\exp(z)\\big)$ 是 softplus 非线性函数。使用从零基线 $x' = 0$ 到输入 $x$ 的直线路径积分梯度（IG；Integrated Gradients）来计算原子级归因分数 $a \\in \\mathbb{R}^{3}$，其中对于每个原子 $i$，\n$$\na_{i} \\;=\\; (x_{i} - x'_{i}) \\int_{0}^{1} \\frac{\\partial F\\!\\big(x' + \\alpha (x - x')\\big)}{\\partial x_{i}} \\, d\\alpha.\n$$\n为了进行药物化学验证，一个构效关系（SAR；structure-activity relationship）专家小组提供了一个三原子参考贡献向量\n$$\nr \\;=\\; \\begin{pmatrix} 1.0 \\\\ -0.5 \\\\ 0.2 \\end{pmatrix}.\n$$\n计算 IG 归因向量 $a$ 与 SAR 向量 $r$ 之间的余弦相似度，其定义为\n$$\n\\cos\\big(a, r\\big) \\;=\\; \\frac{a^{\\top} r}{\\|a\\|_{2} \\, \\|r\\|_{2}},\n$$\n使用从零基线出发的直线路径。将最终的标量验证指标四舍五入至四位有效数字。仅提供最终的标量作为您的答案（无单位）。", "solution": "该问题要求计算积分梯度（IG）归因向量 $a$ 和参考构效关系（SAR）向量 $r$ 之间的余弦相似度。\n\n首先，我们形式化预测函数 $F(x)$ 以及 IG 计算中涉及的量。该函数由 $F(x) = g(w^{\\top}(Mx))$ 给出，其中 $g(z) = \\ln(1 + \\exp(z))$ 是 softplus 函数。每个原子 $i$ 的 IG 归因是沿着从基线 $x' = 0$ 到输入 $x$ 的直线路径定义的：\n$$\na_{i} = (x_{i} - x'_{i}) \\int_{0}^{1} \\frac{\\partial F(x' + \\alpha (x - x'))}{\\partial x_{i}} \\, d\\alpha\n$$\n当 $x' = 0$ 时，该式可简化为：\n$$\na_{i} = x_{i} \\int_{0}^{1} \\frac{\\partial F(\\alpha x)}{\\partial x_{i}} \\, d\\alpha\n$$\n为了继续计算，我们首先计算 $F(x)$ 的梯度。我们定义一个中间向量 $v = M^{\\top}w$。由于给定的混合矩阵 $M$ 是对称的，所以 $M^{\\top} = M$，因此 $v = Mw$。预测函数可以写成 $F(x) = g(v^{\\top}x)$。\n使用链式法则，$F$ 对 $x_i$ 的偏导数为：\n$$\n\\frac{\\partial F(x)}{\\partial x_i} = g'(v^{\\top}x) \\cdot \\frac{\\partial (v^{\\top}x)}{\\partial x_i}\n$$\nsoftplus 函数 $g(z)$ 的导数是 sigmoid 函数，$\\sigma(z) = \\frac{\\exp(z)}{1 + \\exp(z)}$。线性项 $v^{\\top}x = \\sum_j v_j x_j$ 对 $x_i$ 的导数是 $v_i$。因此，\n$$\n\\frac{\\partial F(x)}{\\partial x_i} = \\sigma(v^{\\top}x) v_i\n$$\n现在我们沿着积分路径 $x(\\alpha) = \\alpha x$ 计算这个导数：\n$$\n\\frac{\\partial F(\\alpha x)}{\\partial x_{i}} = \\sigma(v^{\\top}(\\alpha x)) v_i = \\sigma(\\alpha(v^{\\top}x)) v_i\n$$\n将此代入 $a_i$ 的 IG 公式中：\n$$\na_i = x_i \\int_{0}^{1} \\sigma(\\alpha(v^{\\top}x)) v_i \\, d\\alpha = (x_i v_i) \\int_{0}^{1} \\sigma(\\alpha(v^{\\top}x)) \\, d\\alpha\n$$\n积分项 $\\int_{0}^{1} \\sigma(\\alpha(v^{\\top}x)) \\, d\\alpha$ 是一个标量值，我们称之为 $k_s$，它对所有分量 $a_i$ 都是公共的。因此，归因向量 $a$ 可以写成：\n$$\na = k_s (x \\odot v)\n$$\n其中 $\\odot$ 表示逐元素（哈达玛）积。\n\n我们需要计算余弦相似度 $\\cos(a, r) = \\frac{a^{\\top} r}{\\|a\\|_{2} \\|r\\|_{2}}$。代入 $a$ 的表达式：\n$$\n\\cos(a, r) = \\frac{(k_s (x \\odot v))^{\\top} r}{\\|k_s (x \\odot v)\\|_{2} \\|r\\|_{2}} = \\frac{k_s ((x \\odot v)^{\\top} r)}{|k_s| \\|x \\odot v\\|_{2} \\|r\\|_{2}}\n$$\n由于 sigmoid 函数的值域是 $(0, 1)$，被积函数 $\\sigma(\\alpha(v^{\\top}x))$ 总是正的。因此，积分 $k_s$ 必须是正的，这意味着 $|k_s| = k_s$。标量常数 $k_s$ 可以被消去，从而显著简化表达式：\n$$\n\\cos(a, r) = \\frac{(x \\odot v)^{\\top} r}{\\|x \\odot v\\|_{2} \\|r\\|_{2}}\n$$\n这使我们无需显式计算积分即可计算余弦相似度。我们定义一个向量 $a' = x \\odot v$。问题简化为计算 $\\cos(a', r)$。\n\n首先，我们使用给定的矩阵计算向量 $v = Mw$：\n$$\nv = \\begin{pmatrix} 1  0.5  0 \\\\ 0.5  1  0.5 \\\\ 0  0.5  1 \\end{pmatrix} \\begin{pmatrix} 0.8 \\\\ -0.3 \\\\ 0.5 \\end{pmatrix} = \\begin{pmatrix} 1(0.8) + 0.5(-0.3) + 0(0.5) \\\\ 0.5(0.8) + 1(-0.3) + 0.5(0.5) \\\\ 0(0.8) + 0.5(-0.3) + 1(0.5) \\end{pmatrix} = \\begin{pmatrix} 0.8 - 0.15 \\\\ 0.4 - 0.3 + 0.25 \\\\ -0.15 + 0.5 \\end{pmatrix} = \\begin{pmatrix} 0.65 \\\\ 0.35 \\\\ 0.35 \\end{pmatrix}\n$$\n接下来，我们使用输入特征向量 $x$ 计算向量 $a' = x \\odot v$：\n$$\na' = \\begin{pmatrix} 1.2 \\\\ 0.7 \\\\ 0.3 \\end{pmatrix} \\odot \\begin{pmatrix} 0.65 \\\\ 0.35 \\\\ 0.35 \\end{pmatrix} = \\begin{pmatrix} 1.2 \\times 0.65 \\\\ 0.7 \\times 0.35 \\\\ 0.3 \\times 0.35 \\end{pmatrix} = \\begin{pmatrix} 0.78 \\\\ 0.245 \\\\ 0.105 \\end{pmatrix}\n$$\n现在我们计算与参考向量 $r = \\begin{pmatrix} 1.0 \\\\ -0.5 \\\\ 0.2 \\end{pmatrix}$ 进行余弦相似度计算所需的各项。\n点积 $a'^{\\top} r$ 是：\n$$\na'^{\\top} r = (0.78)(1.0) + (0.245)(-0.5) + (0.105)(0.2) = 0.78 - 0.1225 + 0.021 = 0.6785\n$$\n$a'$ 的欧几里得范数的平方是：\n$$\n\\|a'\\|_{2}^2 = (0.78)^2 + (0.245)^2 + (0.105)^2 = 0.6084 + 0.060025 + 0.011025 = 0.67945\n$$\n所以，$\\|a'\\|_{2} = \\sqrt{0.67945}$。\n$r$ 的欧几里得范数的平方是：\n$$\n\\|r\\|_{2}^2 = (1.0)^2 + (-0.5)^2 + (0.2)^2 = 1.0 + 0.25 + 0.04 = 1.29\n$$\n所以，$\\|r\\|_{2} = \\sqrt{1.29}$。\n\n最后，我们计算余弦相似度：\n$$\n\\cos(a, r) = \\frac{a'^{\\top} r}{\\|a'\\|_{2} \\|r\\|_{2}} = \\frac{0.6785}{\\sqrt{0.67945} \\sqrt{1.29}} = \\frac{0.6785}{\\sqrt{0.67945 \\times 1.29}} = \\frac{0.6785}{\\sqrt{0.8764905}}\n$$\n$$\n\\cos(a, r) \\approx \\frac{0.6785}{0.9362107} \\approx 0.7247334\n$$\n问题要求将最终答案四舍五入到四位有效数字。将 $0.7247334...$ 四舍五入得到 $0.7247$。", "answer": "$$\\boxed{0.7247}$$", "id": "4333008"}]}