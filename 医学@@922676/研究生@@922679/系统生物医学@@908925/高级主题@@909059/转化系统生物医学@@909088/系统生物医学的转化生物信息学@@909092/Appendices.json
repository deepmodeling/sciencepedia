{"hands_on_practices": [{"introduction": "在转化生物信息学中，一个核心挑战是整合来自不同研究的基因变异数据。本练习将指导您构建一个计算流程，用于将变异记录统一到标准的HGVS命名法下，这是确保数据可比性和可重复性的关键步骤。通过这个实践（[@problem_id:4396025]），您将掌握处理基因组坐标、转录本结构和链特异性等基本概念的实用技能，这对于任何基因组学分析都至关重要。", "problem": "你的任务是设计并实现一个计算流程，该流程通过将多个变异调用格式 (VCF) 文件中的单核苷酸变异注释转换为使用类Ensembl基因模型的人类基因组变异协会 (HGVS) 编码DNA (c.) 命名法，来统一这些注释，然后计算表现出模糊转录本映射的记录比例。该流程必须从核心生物学定义和关于基因结构与转录的普遍接受事实出发，并且必须能够完全通过代码实现。\n\n基本原理：\n- 分子生物学的中心法则指出，脱氧核糖核酸 (DNA) 转录为核糖核酸 (RNA)，并翻译成蛋白质。一个基因由外显子 (表达区域) 和内含子 (内含区域) 组成。一个转录本是由外显子按特定顺序和方向组成的剪接后信使RNA (mRNA)，其方向由链定义：正链 (“$+$”) 转录本遵循递增的基因组坐标，而负链 (“$-$”) 转录本遵循递减的基因组坐标。\n- 转录本的编码DNA序列 (CDS) 坐标是沿着剪接后的cDNA测量的，cDNA是所有外显子按转录本顺序串联而成的。\n- 双链DNA中的碱基配对使用Watson–Crick互补性：腺嘌呤–胸腺嘧啶和胞嘧啶–鸟嘌呤配对。在负链上，转录本序列对应于基因组正链的反向互补序列。因此，在表达负链转录本的HGVS c.取代时，等位基因必须进行互补。\n\n流程要求与定义：\n- 输入由多个类VCF的单核苷酸变异 (SNV) 集合组成，每个记录定义为一个元组 $(\\text{chrom}, \\text{pos}, \\text{ref}, \\text{alt})$，其中 $\\text{pos}$ 是基于 $1$ 的且包含在内，$\\text{ref}, \\text{alt} \\in \\{\\text{A}, \\text{C}, \\text{G}, \\text{T}\\}$ 且满足 $|\\text{ref}| = |\\text{alt}| = 1$。\n- 转录本由元组 $(\\text{id}, \\text{chrom}, \\text{strand}, \\text{exons})$ 定义，其中 $\\text{exons}$ 是一个闭合基因组区间 $[\\text{start}, \\text{end}]$ 的有序列表，满足 $\\text{start} \\le \\text{end}$，所有位置都是基于 $1$ 的且包含在内。假设整个外显子跨度都是编码区 (没有非翻译区)，并且在一个转录本内外显子列表不重叠。\n- 如果一个变异的基因组位置落在某个转录本的任何一个外显子内，并且在同一条染色体上，则该变异映射到该转录本。\n- 对于一个位于基因组位置 $g$ 的变异，在一个具有外显子 $E_1, E_2, \\dots, E_n$ 的转录本中，其编码DNA坐标 $c$ 定义如下：\n  - 对于正链 (“$+$”)：设 $L_k = \\sum_{i=1}^{k-1} (|E_i|)$ 是包含 $g$ 的外显子 $E_k$ 之前所有外显子的累积长度，并设 $E_k = [s_k, e_k]$。则\n    $$ c = L_k + (g - s_k + 1). $$\n  - 对于负链 (“$-$”)：按递减的基因组坐标定义转录本顺序。设 $E'_1, E'_2, \\dots, E'_n$ 是按起始位置递减（等效于按结束位置递减）排序的外显子。设 $L'_k = \\sum_{i=1}^{k-1} (|E'_i|)$ 并且设 $E'_k = [s'_k, e'_k]$ 是此顺序中包含 $g$ 的外显子。则\n    $$ c = L'_k + (e'_k - g + 1). $$\n- 取代的HGVS编码DNA命名法表示为 $\\text{id}:\\mathrm{c}.c\\ \\text{r}\\text{a}$，其中 $\\text{id}$ 是转录本标识符，$c$ 是编码DNA坐标，$\\text{r}$ 和 $\\text{a}$ 分别是参考等位基因和变异等位基因。对于负链转录本，在构成 $\\text{r}$ 和 $\\text{a}$ 时，对两个等位基因都使用核苷酸互补。\n- 模糊映射定义为单个唯一的基因组变异 $(\\text{chrom}, \\text{pos}, \\text{ref}, \\text{alt})$ 映射到多个转录本（产生多个有效的HGVS c. 字符串）。\n- 跨VCF文件的统一化必须使用键 $(\\text{chrom}, \\text{pos}, \\text{ref}, \\text{alt})$ 对跨文件的相同变异记录进行去重。\n- 所需的统计量是在映射到至少一个转录本的唯一记录集合中，模糊记录的比例。形式上，如果 $A$ 是具有至少两个转录本映射的唯一变异数量，而 $M$ 是具有至少一个转录本映射的唯一变异数量，则报告\n  $$ p = \\begin{cases}\n  \\dfrac{A}{M}  \\text{if } M  0,\\\\\n  0.0  \\text{if } M = 0.\n  \\end{cases} $$\n- 不会出现角度。所有答案必须是实值小数。报告每个比例，四舍五入到恰好 $3$ 位小数。\n\n实现约束：\n- 仅限于如上定义的单核苷酸变异。\n- 对于负链转录本，使用互补的等位基因生成HGVS c. 取代。\n- 外显子区间是包含性的，并且可能在不同转录本之间共享。\n\n测试套件：\n- 测试用例1：\n  - 转录本：\n    - ENST000001: 染色体 “chr1”，链 “$+$”，外显子 $[100,110]$ 和 $[200,210]$。\n    - ENST000002: 染色体 “chr1”，链 “$+$”，外显子 $[205,215]$。\n    - ENST000003: 染色体 “chr1”，链 “$-$”，外显子 $[500,505]$ 和 $[510,515]$。\n  - VCF A 唯一记录：\n    - $(\\text{chr1}, 205, \\text{G}, \\text{A})$。\n    - $(\\text{chr1}, 102, \\text{A}, \\text{C})$。\n    - $(\\text{chr1}, 600, \\text{T}, \\text{C})$。\n  - VCF B 唯一记录：\n    - $(\\text{chr1}, 205, \\text{G}, \\text{A})$ (VCF A 的重复项)。\n    - $(\\text{chr1}, 512, \\text{C}, \\text{T})$。\n    - $(\\text{chr1}, 106, \\text{T}, \\text{G})$。\n  - 预期行为：统一化后，唯一集合的大小为 $5$；按定义计算 $p$。\n- 测试用例2：\n  - 转录本：\n    - ENST000010: 染色体 “chr2”，链 “$+$”，外显子 $[1000,1005]$ 和 $[1010,1015]$。\n    - ENST000011: 染色体 “chr2”，链 “$+$”，外显子 $[1005,1012]$。\n    - ENST000012: 染色体 “chr2”，链 “$-$”，外显子 $[2000,2003]$ 和 $[2005,2008]$。\n  - VCF A 唯一记录：\n    - $(\\text{chr2}, 1005, \\text{A}, \\text{G})$。\n    - $(\\text{chr2}, 1012, \\text{C}, \\text{A})$。\n    - $(\\text{chr2}, 2006, \\text{G}, \\text{A})$。\n  - VCF B 唯一记录：\n    - $(\\text{chr2}, 1005, \\text{A}, \\text{G})$ (VCF A 的重复项)。\n    - $(\\text{chr2}, 3000, \\text{T}, \\text{G})$。\n    - $(\\text{chr2}, 2002, \\text{T}, \\text{C})$。\n  - 预期行为：统一化后，唯一集合的大小为 $5$；按定义计算 $p$。\n- 测试用例3：\n  - 转录本：\n    - ENST000020: 染色体 “chr3”，链 “$-$”，外显子 $[400,405]$。\n  - VCF A 唯一记录：\n    - $(\\text{chr3}, 100, \\text{A}, \\text{C})$。\n    - $(\\text{chr3}, 200, \\text{G}, \\text{T})$。\n  - VCF B 唯一记录：\n    - $(\\text{chr3}, 300, \\text{T}, \\text{A})$。\n  - 预期行为：没有变异映射到任何转录本；根据定义报告 $p = 0.0$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含测试用例1、2和3的三个比例结果，以逗号分隔，并用方括号括起来，每个比例四舍五入到恰好 $3$ 位小数。例如，输出必须类似于“$[0.123,0.456,0.789]$”，但使用此问题的正确值。", "solution": "所提供的问题在科学上和计算上都是有效的。它基于分子生物学的基本原理，包括中心法则、基因结构（外显子和内含子）、转录以及遗传变异的标准化命名法 (HGVS)。该问题定义明确，提供了一套完整且一致的定义、规则和约束，允许推导出唯一、可验证的解。算法任务是实现一个生物信息学流程，用于统一单核苷酸变异 (SNV) 数据并量化转录本映射的模糊性，这是转化基因组学中的一个相关问题。\n\n该解决方案通过一系列逻辑步骤实现指定的流程来设计。\n\n**步骤1：变异记录的统一化**\n\n初始步骤涉及将来自多个输入源（类VCF集合）的变异记录聚合到一个单一的、非冗余的集合中。一个变异由元组 $(\\text{chrom}, \\text{pos}, \\text{ref}, \\text{alt})$ 唯一标识，其中 $\\text{chrom}$ 是染色体，$\\text{pos}$ 是基于 $1$ 的基因组位置，$\\text{ref}$ 是参考等位基因，$\\text{alt}$ 是变异等位基因。通过将所有来源的所有变异元组插入到一个集合数据结构中，重复项会自动被消除。这个过程产生了待分析的唯一基因组变异集合。\n\n**步骤2：变异到转录本的映射**\n\n对于每个唯一变异，流程必须确定它影响哪些转录本。位于染色体 $\\text{chrom}_v$ 上基因组位置 $g$ 的变异被认为映射到一个转录本，如果：\n$1$. 该转录本位于同一染色体上，即 $\\text{chrom}_t = \\text{chrom}_v$。\n$2$. 变异的位置 $g$ 落在该转录本的任何一个外显子内。一个外显子是一个闭区间 $[s, e]$，因此条件是 $s \\le g \\le e$。\n\n该过程涉及遍历每个唯一变异，并为每个变异遍历所提供的整个转录本列表。对每个变异都维护一个映射计数。\n\n**步骤3：模糊性的定义与识别**\n\n当单个唯一的基因组变异映射到多个转录本时，就会发生模糊映射。由于可变剪接，这在人类基因组中是一种常见情况，即单个基因可以产生具有不同外显子组成的多种转录本异构体。每个不同的映射都会产生一个不同的 HGVS c. 注释。\n\n问题要求能够生成这些注释，这取决于转录本的链。\n- 对于一个**正链 (\"$+$\")** 转录本，其外显子 $E_1, E_2, \\dots, E_n$ 按递增的基因组坐标排序，位于外显子 $E_k = [s_k, e_k]$ 内基因组位置 $g$ 的变异的编码DNA (cDNA) 坐标 $c$ 由以下公式给出：\n$$ c = \\left( \\sum_{i=1}^{k-1} (e_i - s_i + 1) \\right) + (g - s_k + 1) $$\n术语 $\\sum_{i=1}^{k-1} (e_i - s_i + 1)$ 代表了在 $E_k$ 之前的外显子的累积长度。\n\n- 对于一个**负链 (\"$-$\")** 转录本，外显子 $E'_1, E'_2, \\dots, E'_n$ 按递减的基因组坐标顺序考虑。位于外显子 $E'_k = [s'_k, e'_k]$ 内基因组位置 $g$ 的变异的cDNA坐标 $c$ 计算如下：\n$$ c = \\left( \\sum_{i=1}^{k-1} (e'_i - s'_i + 1) \\right) + (e'_k - g + 1) $$\n此外，因为该转录本在反向互补链上编码，所以参考等位基因和变异等位基因必须为HGVS字符串进行互补（A$\\leftrightarrow$T, C$\\leftrightarrow$G）。\n\n一个变异映射到两个不同的转录本，例如ENST001和ENST002，将生成两个不同的HGVS字符串，例如 `ENST001:c.150GA` 和 `ENST002:c.95GA`。这满足了模糊性的定义。为了计算所需的统计量，只需要计算每个变异映射到的转录本数量，而不必构建完整的HGVS字符串。大于 $1$ 的计数表示存在模糊性。\n\n**步骤4：模糊性比例的计算**\n\n最终目标是计算模糊映射变异的比例 $p$。这由以下公式定义：\n$$ p = \\frac{A}{M} $$\n其中：\n- $M$ 是映射到至少一个转录本的唯一变异总数。\n- $A$ 是来自集合 $M$ 中映射到多个转录本（即至少有两个映射）的唯一变异数量。\n\n在没有变异映射到任何转录本的特定情况下，$M=0$，比例 $p$ 被定义为 $0.0$。\n\n计算 $p$ 的算法过程如下：\n$1$. 初始化两个计数器：`mapped_variants_count` ($M$) 为 $0$ 和 `ambiguous_variants_count` ($A$) 为 $0$。\n$2$. 对于统一化集合中的每个唯一变异：\n    a. 计算它映射到的转录本数量，设为 `num_mappings`。\n    b. 如果 `num_mappings` $\\ge 1$，则增加 `mapped_variants_count`。\n    c. 如果 `num_mappings` $ 1$，则增加 `ambiguous_variants_count`。\n$3$. 处理完所有唯一变异后，如果 `mapped_variants_count` $ 0$，则计算比例 $p = \\text{ambiguous\\_variants\\_count} / \\text{mapped\\_variants\\_count}$。否则，$p = 0.0$。\n$4$. 最终结果按要求四舍五入到 $3$ 位小数。此过程系统地应用于每个测试用例。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to process test cases and compute the ambiguity proportion.\n    \"\"\"\n    \n    # Test Case 1 Data\n    transcripts_1 = [\n        (\"ENST000001\", \"chr1\", \"+\", [(100, 110), (200, 210)]),\n        (\"ENST000002\", \"chr1\", \"+\", [(205, 215)]),\n        (\"ENST000003\", \"chr1\", \"-\", [(500, 505), (510, 515)]),\n    ]\n    vcf_a_1 = [\n        (\"chr1\", 205, \"G\", \"A\"),\n        (\"chr1\", 102, \"A\", \"C\"),\n        (\"chr1\", 600, \"T\", \"C\"),\n    ]\n    vcf_b_1 = [\n        (\"chr1\", 205, \"G\", \"A\"),\n        (\"chr1\", 512, \"C\", \"T\"),\n        (\"chr1\", 106, \"T\", \"G\"),\n    ]\n\n    # Test Case 2 Data\n    transcripts_2 = [\n        (\"ENST000010\", \"chr2\", \"+\", [(1000, 1005), (1010, 1015)]),\n        (\"ENST000011\", \"chr2\", \"+\", [(1005, 1012)]),\n        (\"ENST000012\", \"chr2\", \"-\", [(2000, 2003), (2005, 2008)]),\n    ]\n    vcf_a_2 = [\n        (\"chr2\", 1005, \"A\", \"G\"),\n        (\"chr2\", 1012, \"C\", \"A\"),\n        (\"chr2\", 2006, \"G\", \"A\"),\n    ]\n    vcf_b_2 = [\n        (\"chr2\", 1005, \"A\", \"G\"),\n        (\"chr2\", 3000, \"T\", \"G\"),\n        (\"chr2\", 2002, \"T\", \"C\"),\n    ]\n\n    # Test Case 3 Data\n    transcripts_3 = [\n        (\"ENST000020\", \"chr3\", \"-\", [(400, 405)]),\n    ]\n    vcf_a_3 = [\n        (\"chr3\", 100, \"A\", \"C\"),\n        (\"chr3\", 200, \"G\", \"T\"),\n    ]\n    vcf_b_3 = [\n        (\"chr3\", 300, \"T\", \"A\"),\n    ]\n\n    test_cases = [\n        (transcripts_1, vcf_a_1, vcf_b_1),\n        (transcripts_2, vcf_a_2, vcf_b_2),\n        (transcripts_3, vcf_a_3, vcf_b_3),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        transcripts, vcf_a, vcf_b = case\n        \n        # Step 1: Harmonize variant records into a unique set\n        unique_variants = set(vcf_a) | set(vcf_b)\n        \n        mapped_variants_count = 0  # Counter for M\n        ambiguous_variants_count = 0  # Counter for A\n        \n        # Step 2: Iterate through unique variants to find mappings\n        for variant in unique_variants:\n            v_chrom, v_pos, _, _ = variant\n            \n            num_mappings = 0\n            for transcript in transcripts:\n                t_id, t_chrom, _, t_exons = transcript\n                \n                # Check for chromosome match first\n                if v_chrom != t_chrom:\n                    continue\n                \n                # Check if variant position is within any exon\n                for exon_start, exon_end in t_exons:\n                    if exon_start = v_pos = exon_end:\n                        num_mappings += 1\n                        # A variant can't be in two different exons of the same transcript\n                        # as exons are non-overlapping. So we can break.\n                        break\n            \n            # Step 3: Tally counts for M and A\n            if num_mappings >= 1:\n                mapped_variants_count += 1\n            if num_mappings > 1:\n                ambiguous_variants_count += 1\n        \n        # Step 4: Calculate the proportion p\n        if mapped_variants_count > 0:\n            proportion = ambiguous_variants_count / mapped_variants_count\n        else:\n            proportion = 0.0\n            \n        results.append(proportion)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(f'{r:.3f}' for r in results)}]\")\n\nsolve()\n```", "id": "4396025"}, {"introduction": "在系统生物医学中，从观测数据中推断因果关系是一项核心任务，而孟德尔随机化（Mendelian Randomization, MR）为此提供了强有力的遗传学方法。本练习将引导您从基本原理出发，实现MR-Egger回归，这是一种先进的MR技术，能够评估和校正由基因多效性引起的潜在偏见。通过亲手构建和测试该模型（[@problem_id:4396045]），您将深刻理解其核心假设（如InSIDE假设）以及如何利用截距项来诊断定向多效性，这对于得出可靠的因果结论至关重要。", "problem": "给定一个基于工具变量的、用于系统生物医学中转化生物信息学的汇总级别孟德尔随机化问题。考虑一组由 $i \\in \\{1,\\dots,n\\}$ 索引的独立遗传变异（工具变量）。对于每个工具变量 $i$，用 $\\hat{\\beta}_{XGi}$ 表示工具变量 $i$ 与暴露的估计关联，用 $\\hat{\\beta}_{YGi}$ 表示工具变量 $i$ 与结局的估计关联。假设 $\\hat{\\beta}_{YGi}$ 的标准误 $\\sigma_{Yi}$ 已知，逆方差权重为 $w_i = 1/\\sigma_{Yi}^2$。结构性汇总级别模型为\n$$\n\\mathbb{E}[\\hat{\\beta}_{YGi} \\mid \\hat{\\beta}_{XGi}] = \\beta_0 + \\beta_{\\text{MR}}\\hat{\\beta}_{XGi},\n$$\n其中 $\\beta_{\\text{MR}}$ 是待估计的因果效应参数，$\\beta_0$ 是一个截距项，捕捉了所有工具变量的平均直接（多效性）效应。孟德尔随机化-Egger（MR-Egger）方法使用带截距的加权线性回归。在工具强度独立于直接效应（InSIDE）假设下，该截距项为定向多效性提供了一个检验。该假设指出，工具变量对暴露的强度与其对结局的直接效应是独立的。\n\n从加权最小二乘法和线性回归的定义出发，根据第一性原理推导出加权正规方程，并用它们计算 MR-Egger 估计值 $\\hat{\\beta}_0$ 和 $\\hat{\\beta}_{\\text{MR}}$ 及其估计的协方差矩阵，将权重视为 $w_i = 1/\\sigma_{Yi}^2$。使用残差加权平方和来估计残差方差，然后计算自由度为 $n - 2$ 的截距项 $\\beta_0$ 的双侧 $t$ 检验 p 值，以评估定向多效性。\n\n您必须实现一个程序，该程序：\n- 为下述每个测试用例，在真实的线性数据生成机制下模拟汇总级别数据。\n- 计算 MR-Egger 加权最小二乘估计值、其标准误，以及作为定向多效性检验的截距项的双侧 p 值。\n- 生成单行输出，其中包含所有测试用例的串联结果，形式为一个扁平列表。\n\n每个工具变量 $i$ 的数据生成机制：\n- 从 $ \\alpha_i \\sim \\mathcal{N}(0, \\sigma_\\alpha^2)$ 抽取暴露的工具强度 $ \\alpha_i $。\n- 抽取直接（多效性）效应 $ \\delta_i $，使得 $ \\delta_i $ 的均值为 $\\mu$，与 $\\alpha_i$ 的相关性为 $\\rho$，通过以下方式实现\n$$\n\\delta_i = \\mu + \\rho \\frac{\\sigma_\\delta}{\\sigma_\\alpha} \\alpha_i + \\sqrt{1 - \\rho^2}\\,\\sigma_\\delta z_i,\\quad z_i \\sim \\mathcal{N}(0,1).\n$$\n- 真实的基因-结局效应为 $ \\beta \\alpha_i + \\delta_i $，其中 $\\beta$ 是真实的因果效应。\n- 观测到的汇总统计量为\n$$\n\\hat{\\beta}_{XGi} = \\alpha_i + \\varepsilon_{Xi},\\quad \\varepsilon_{Xi} \\sim \\mathcal{N}(0, \\sigma_X^2),\n$$\n$$\n\\hat{\\beta}_{YGi} = \\beta \\alpha_i + \\delta_i + \\varepsilon_{Yi},\\quad \\varepsilon_{Yi} \\sim \\mathcal{N}(0, \\sigma_Y^2),\n$$\n其中对所有 $i$，$\\sigma_{Yi} = \\sigma_Y$。\n\n您的程序必须：\n- 每个测试用例使用固定的随机种子以确保可复现性。\n- 对于每个案例，使用权重 $w_i = 1/\\sigma_{Yi}^2$ 对 $\\hat{\\beta}_{YGi}$ 和 $\\hat{\\beta}_{XGi}$ 进行带截距的加权回归。\n- 为每个测试用例返回：MR-Egger 斜率估计值 $\\hat{\\beta}_{\\text{MR}}$、截距估计值 $\\hat{\\beta}_0$ 以及截距项的双侧 p 值。将每个值四舍五入到 $4$ 位小数。\n- 将所有结果聚合到单行中，形式为用方括号括起来的逗号分隔列表，顺序为 $[\\hat{\\beta}_{\\text{MR}}^{(1)}, \\hat{\\beta}_0^{(1)}, p_0^{(1)}, \\hat{\\beta}_{\\text{MR}}^{(2)}, \\hat{\\beta}_0^{(2)}, p_0^{(2)}, \\hat{\\beta}_{\\text{MR}}^{(3)}, \\hat{\\beta}_0^{(3)}, p_0^{(3)}]$，其中上标表示测试用例的索引。\n\n测试套件（每个案例是一个参数元组 $(\\text{seed}, n, \\beta, \\mu, \\rho, \\sigma_\\alpha, \\sigma_\\delta, \\sigma_X, \\sigma_Y)$）：\n- 案例 1（理想情况，InSIDE 假设下无定向多效性）：$(\\;12345,\\; 50,\\; 0.2,\\; 0.0,\\; 0.0,\\; 0.1,\\; 0.05,\\; 0.005,\\; 0.05\\;)$。\n- 案例 2（InSIDE 假设下的定向多效性）：$(\\;67890,\\; 50,\\; 0.2,\\; 0.1,\\; 0.0,\\; 0.1,\\; 0.05,\\; 0.005,\\; 0.05\\;)$。\n- 案例 3（InSIDE 违规，但无定向平均效应）：$(\\;54321,\\; 50,\\; 0.2,\\; 0.0,\\; 0.9,\\; 0.1,\\; 0.05,\\; 0.005,\\; 0.05\\;)$。\n\n角度单位不适用。无需物理单位。将所有输出表示为四舍五入到 $4$ 位小数的实数，不含任何附加文本。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，$[\\text{result}_1,\\text{result}_2,\\text{result}_3]$）。此问题所需的确切输出为 $[\\hat{\\beta}_{\\text{MR}}^{(1)}, \\hat{\\beta}_0^{(1)}, p_0^{(1)}, \\hat{\\beta}_{\\text{MR}}^{(2)}, \\hat{\\beta}_0^{(2)}, p_0^{(2)}, \\hat{\\beta}_{\\text{MR}}^{(3)}, \\hat{\\beta}_0^{(3)}, p_0^{(3)}]$，所有值均四舍五入至 $4$ 位小数。", "solution": "该问题要求从加权最小二乘法（WLS）的第一性原理出发，推导并实现孟德尔随机化（MR）-Egger 回归估计量，并计算截距项的 p 值以检验定向多效性。\n\n### **1. 问题表述**\n\n令 $n$ 个独立遗传变异的工具变量汇总统计量表示为 $(\\hat{\\beta}_{XGi}, \\hat{\\beta}_{YGi})$，其中 $i=1, \\dots, n$。MR-Egger 模型是基因-结局关联 $\\hat{\\beta}_{YGi}$ 对基因-暴露关联 $\\hat{\\beta}_{XGi}$ 的线性回归。为简化符号，令 $Y_i = \\hat{\\beta}_{YGi}$ 和 $X_i = \\hat{\\beta}_{XGi}$。模型为：\n$$\nY_i = \\beta_0 + \\beta_{\\text{MR}} X_i + \\epsilon_i\n$$\n其中 $\\beta_0$ 是截距，表示平均多效性效应，$\\beta_{\\text{MR}}$ 是斜率，表示因果效应估计。回归由 $w_i = 1/\\sigma_{Yi}^2$ 加权，其中 $\\sigma_{Yi}^2$ 是估计值 $\\hat{\\beta}_{YGi}$ 的方差。\n\n### **2. 加权最小二乘估计量的推导**\n\nWLS 方法通过最小化残差加权平方和（WSSR）来找到参数 $(\\beta_0, \\beta_{\\text{MR}})$：\n$$\nS(\\beta_0, \\beta_{\\text{MR}}) = \\sum_{i=1}^n w_i (Y_i - \\beta_0 - \\beta_{\\text{MR}} X_i)^2\n$$\n为找到最小值，我们计算 $S$ 对 $\\beta_0$ 和 $\\beta_{\\text{MR}}$ 的偏导数并将其设为零。这些就是加权正规方程。\n\n**对 $\\beta_0$ 的导数：**\n$$\n\\frac{\\partial S}{\\partial \\beta_0} = \\sum_{i=1}^n 2 w_i (Y_i - \\beta_0 - \\beta_{\\text{MR}} X_i)(-1) = 0\n$$\n$$\n\\implies \\sum_{i=1}^n w_i (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_{\\text{MR}} X_i) = 0\n$$\n$$\n\\implies \\hat{\\beta}_0 \\sum_{i=1}^n w_i + \\hat{\\beta}_{\\text{MR}} \\sum_{i=1}^n w_i X_i = \\sum_{i=1}^n w_i Y_i \\quad (1)\n$$\n\n**对 $\\beta_{\\text{MR}}$ 的导数：**\n$$\n\\frac{\\partial S}{\\partial \\beta_{\\text{MR}}} = \\sum_{i=1}^n 2 w_i (Y_i - \\beta_0 - \\beta_{\\text{MR}} X_i)(-X_i) = 0\n$$\n$$\n\\implies \\sum_{i=1}^n w_i X_i (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_{\\text{MR}} X_i) = 0\n$$\n$$\n\\implies \\hat{\\beta}_0 \\sum_{i=1}^n w_i X_i + \\hat{\\beta}_{\\text{MR}} \\sum_{i=1}^n w_i X_i^2 = \\sum_{i=1}^n w_i X_i Y_i \\quad (2)\n$$\n\n可以求解这两个线性方程组以获得估计量 $\\hat{\\beta}_0$ 和 $\\hat{\\beta}_{\\text{MR}}$。一个更通用且计算上更方便的方法是使用矩阵代数。\n\n### **3. 矩阵表述**\n\n令 $\\mathbf{Y}$ 为 $n \\times 1$ 的结局向量 $[Y_1, \\dots, Y_n]^T$，$\\mathbf{W}$ 为 $n \\times n$ 的对角权重矩阵，其中 $W_{ii} = w_i$，$\\mathbf{X}$ 为 $n \\times 2$ 的设计矩阵，第一列全为 1，第二列包含预测变量值 $[X_1, \\dots, X_n]^T$。\n$$\n\\mathbf{Y} = \\begin{pmatrix} Y_1 \\\\ \\vdots \\\\ Y_n \\end{pmatrix}, \\quad\n\\mathbf{X} = \\begin{pmatrix} 1  X_1 \\\\ \\vdots  \\vdots \\\\ 1  X_n \\end{pmatrix}, \\quad\n\\mathbf{W} = \\begin{pmatrix} w_1   0 \\\\  \\ddots  \\\\ 0   w_n \\end{pmatrix}\n$$\n参数向量为 $\\boldsymbol{\\beta} = [\\beta_0, \\beta_{\\text{MR}}]^T$。矩阵形式的正规方程为：\n$$\n(\\mathbf{X}^T \\mathbf{W} \\mathbf{X}) \\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T \\mathbf{W} \\mathbf{Y}\n$$\n估计参数向量 $\\hat{\\boldsymbol{\\beta}}$ 的解为：\n$$\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W} \\mathbf{Y}\n$$\n其中 $\\hat{\\boldsymbol{\\beta}} = [\\hat{\\beta}_0, \\hat{\\beta}_{\\text{MR}}]^T$。\n\n### **4. 标准误估计与假设检验**\n\n为了对截距 $\\beta_0$ 进行假设检验，我们需要其标准误。这需要模型的残差方差估计和估计量的协方差矩阵。\n\n**残差方差：**\n第 $i$ 个观测的残差为 $e_i = Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_{\\text{MR}} X_i)$。残差加权平方和（WSSR）为：\n$$\n\\text{WSSR} = \\sum_{i=1}^n w_i e_i^2\n$$\n残差方差 $\\hat{\\sigma}^2$ 的一个无偏估计量是 WSSR 除以自由度，即 $n-p$，其中 $n$ 是观测数，$p$ 是估计的参数数量。这里，$p=2$（对于 $\\beta_0$ 和 $\\beta_{\\text{MR}}$）。\n$$\n\\hat{\\sigma}^2 = \\frac{\\text{WSSR}}{n-2}\n$$\n\n**估计量的协方差矩阵：**\n参数向量 $\\hat{\\boldsymbol{\\beta}}$ 的估计协方差矩阵由下式给出：\n$$\n\\widehat{\\text{Cov}}(\\hat{\\boldsymbol{\\beta}}) = \\hat{\\sigma}^2 (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1}\n$$\n这是一个 $2 \\times 2$ 的矩阵：\n$$\n\\widehat{\\text{Cov}}(\\hat{\\boldsymbol{\\beta}}) = \\begin{pmatrix} \\widehat{\\text{Var}}(\\hat{\\beta}_0)  \\widehat{\\text{Cov}}(\\hat{\\beta}_0, \\hat{\\beta}_{\\text{MR}}) \\\\ \\widehat{\\text{Cov}}(\\hat{\\beta}_0, \\hat{\\beta}_{\\text{MR}})  \\widehat{\\text{Var}}(\\hat{\\beta}_{\\text{MR}}) \\end{pmatrix}\n$$\n截距的标准误 $SE(\\hat{\\beta}_0)$ 是其估计方差的平方根：\n$$\nSE(\\hat{\\beta}_0) = \\sqrt{\\widehat{\\text{Var}}(\\hat{\\beta}_0)} = \\sqrt{[\\widehat{\\text{Cov}}(\\hat{\\boldsymbol{\\beta}})]_{1,1}}\n$$\n\n**截距的 t 检验：**\n为检验定向多效性，我们检验零假设 $H_0: \\beta_0 = 0$。检验统计量是一个 $t$ 统计量：\n$$\nt_{\\text{stat}} = \\frac{\\hat{\\beta}_0 - 0}{SE(\\hat{\\beta}_0)}\n$$\n在零假设下，此统计量服从自由度为 $n-2$ 的 $t$ 分布。双侧 p 值计算如下：\n$$\np_0 = 2 \\cdot P(T_{n-2} \\ge |t_{\\text{stat}}|)\n$$\n其中 $T_{n-2}$ 是一个服从自由度为 $n-2$ 的 $t$ 分布的随机变量。\n\n### **5. 实现策略**\n\n程序将为每个测试用例实现以下步骤：\n1.  **数据模拟：** 使用提供的参数（$\\text{seed}, n, \\beta, \\mu, \\rho, \\dots$），根据指定的数据生成机制为 $n$ 个工具变量生成汇总统计量 $\\hat{\\beta}_{XGi}$ 和 $\\hat{\\beta}_{YGi}$。\n2.  **WLS 回归：**\n    *   从 $\\hat{\\beta}_{YGi}$ 构建响应向量 $\\mathbf{Y}$，从一列 1 和 $\\hat{\\beta}_{XGi}$ 向量构建设计矩阵 $\\mathbf{X}$。\n    *   使用 $w_i = 1/\\sigma_Y^2$ 构建对角权重矩阵 $\\mathbf{W}$。\n    *   使用矩阵公式 $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W} \\mathbf{Y}$ 计算参数估计值 $\\hat{\\boldsymbol{\\beta}} = [\\hat{\\beta}_0, \\hat{\\beta}_{\\text{MR}}]^T$。\n3.  **P 值计算：**\n    *   计算预测值 $\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}$ 和残差 $\\mathbf{e} = \\mathbf{Y} - \\hat{\\mathbf{Y}}$。\n    *   计算残差方差估计量 $\\hat{\\sigma}^2$。\n    *   计算参数协方差矩阵 $\\widehat{\\text{Cov}}(\\hat{\\boldsymbol{\\beta}})$。\n    *   提取截距的标准误 $SE(\\hat{\\beta}_0)$。\n    *   使用自由度为 $n-2$ 的 $t$ 分布计算 $t$ 统计量和相应的双侧 p 值。\n4.  **输出格式化：** 将得到的 $\\hat{\\beta}_{\\text{MR}}$、$\\hat{\\beta}_0$ 和 $p_0$ 四舍五入到 4 位小数，并按规定将它们聚合成一个逗号分隔的列表。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Main function to run the MR-Egger simulation and analysis for all test cases.\n    \"\"\"\n    # Test suite (each case is a tuple of parameters \n    # (seed, n, beta, mu, rho, sigma_alpha, sigma_delta, sigma_X, sigma_Y))\n    test_cases = [\n        # Case 1 (happy path, no directional pleiotropy under InSIDE)\n        (12345, 50, 0.2, 0.0, 0.0, 0.1, 0.05, 0.005, 0.05),\n        # Case 2 (directional pleiotropy under InSIDE)\n        (67890, 50, 0.2, 0.1, 0.0, 0.1, 0.05, 0.005, 0.05),\n        # Case 3 (InSIDE violation with no directional mean effect)\n        (54321, 50, 0.2, 0.0, 0.9, 0.1, 0.05, 0.005, 0.05),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = run_mr_egger_analysis(*case)\n        all_results.extend(result)\n    \n    # Format the final list as a string \"[val1,val2,...]\" with 4 decimal places.\n    formatted_results = [f'{val:.4f}' for val in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef run_mr_egger_analysis(seed, n, beta, mu, rho, sigma_alpha, sigma_delta, sigma_X, sigma_Y):\n    \"\"\"\n    Simulates summary-level data and computes MR-Egger estimates and statistics for one case.\n    \n    Args:\n        seed (int): Random seed for reproducibility.\n        n (int): Number of instruments.\n        beta (float): True causal effect.\n        mu (float): Mean of direct (pleiotropic) effects.\n        rho (float): Correlation between instrument strength and direct effects.\n        sigma_alpha (float): Standard deviation of instrument strength.\n        sigma_delta (float): Standard deviation of direct effects.\n        sigma_X (float): Standard deviation of measurement error for exposure associations.\n        sigma_Y (float): Standard deviation of measurement error for outcome associations.\n\n    Returns:\n        tuple[float, float, float]: A tuple containing the estimated slope (beta_MR_hat),\n                                     intercept (beta0_hat), and the p-value for the intercept.\n    \"\"\"\n    # 1. Data Simulation\n    rng = np.random.default_rng(seed)\n    \n    # Instrument strength for exposure\n    alpha = rng.normal(0, sigma_alpha, n)\n    \n    # Direct (pleiotropic) effect\n    z = rng.normal(0, 1, n)\n    delta = mu + rho * (sigma_delta / sigma_alpha) * alpha + np.sqrt(1 - rho**2) * sigma_delta * z\n    \n    # Measurement errors\n    eps_X = rng.normal(0, sigma_X, n)\n    eps_Y = rng.normal(0, sigma_Y, n)\n    \n    # Observed summary statistics\n    beta_X_hat = alpha + eps_X\n    beta_Y_hat = beta * alpha + delta + eps_Y\n\n    # 2. MR-Egger Weighted Least Squares Regression\n    # Let Y_vec = beta_Y_hat, X_vec = beta_X_hat\n    Y_vec = beta_Y_hat\n    \n    # Construct the design matrix X_mat (n x 2)\n    X_mat = np.vstack((np.ones(n), beta_X_hat)).T\n    \n    # Construct the diagonal weight matrix W (n x n)\n    # Weights are constant since sigma_Yi is constant\n    weights = np.full(n, 1.0 / (sigma_Y**2))\n    W = np.diag(weights)\n\n    # Compute (X^T * W * X)\n    XTW = X_mat.T @ W\n    XTWX = XTW @ X_mat\n    \n    # Compute inverse of (X^T * W * X)\n    try:\n        XTWX_inv = np.linalg.inv(XTWX)\n    except np.linalg.LinAlgError:\n        # Handle cases of singular matrix (collinear predictors)\n        return (np.nan, np.nan, np.nan)\n\n    # Compute (X^T * W * Y)\n    XTWY = XTW @ Y_vec\n\n    # Compute parameter estimates: beta_hat = (X^T*W*X)^-1 * (X^T*W*Y)\n    beta_hats = XTWX_inv @ XTWY\n    beta0_hat = beta_hats[0]\n    beta_MR_hat = beta_hats[1]\n    \n    # 3. Standard Error and P-value Calculation\n    # Predicted values and residuals\n    Y_hat = X_mat @ beta_hats\n    residuals = Y_vec - Y_hat\n    \n    # Residual weighted sum of squares (WSSR)\n    wssr = np.sum(weights * (residuals**2))\n    \n    # Degrees of freedom\n    df = n - 2\n    if df = 0:\n        return (beta_MR_hat, beta0_hat, np.nan)\n\n    # Residual variance estimate\n    residual_variance = wssr / df\n    \n    # Covariance matrix of parameter estimates\n    cov_beta_hats = residual_variance * XTWX_inv\n    \n    # Standard error of the intercept\n    var_beta0_hat = cov_beta_hats[0, 0]\n    se_beta0_hat = np.sqrt(var_beta0_hat)\n    \n    # t-statistic for the intercept (H0: beta0 = 0)\n    if se_beta0_hat > 0:\n        t_stat = beta0_hat / se_beta0_hat\n        # Two-sided p-value\n        p_value_intercept = 2 * t.sf(np.abs(t_stat), df=df)\n    else:\n        p_value_intercept = np.nan\n\n    return (beta_MR_hat, beta0_hat, p_value_intercept)\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "4396045"}, {"introduction": "随着复杂预测模型在生物医学中的应用日益增多，解释其决策过程对于临床转化和建立信任变得至关重要。本练习将带您深入了解SHAP（SHapley Additive exPlanations）值，这是一种基于合作博弈论的强大方法，可将模型的单次预测公平地归因于每个输入特征。通过从头开始实现SHAP值的精确计算（[@problem_id:4396100]），您将亲手验证其核心的公理属性（如可加性），并深刻体会该如何量化每个生物标志物对个体化治疗反应预测的贡献。", "problem": "给定一个在系统生物医学转化生物信息学背景下用于个体化治疗反应的预测模型。该模型接受一个患者特征向量并输出一个标量响应分数。您的任务是计算用于特征归因的 SHapley 加性解释 (SHAP) 值，并验证其公理化性质：可加性（也称为局部准确性或效率）和一致性。请以纯数学术语进行研究，并通过枚举特征联盟来实现一个精确解。\n\n定义与设置：设有 $M$ 个特征，由集合 $N = \\{1,2,\\dots,M\\}$ 索引。一个训练好的模型是一个函数 $f: \\mathbb{R}^M \\to \\mathbb{R}$，对于一个特征向量为 $x \\in \\mathbb{R}^M$ 的目标患者，其合作博弈集合函数为每个联盟 $S \\subseteq N$ 定义为\n$$\nv(S) = \\mathbb{E}_{Z}\\big[f\\big(x_S \\oplus Z_{\\overline{S}}\\big)\\big],\n$$\n其中 $\\overline{S} = N \\setminus S$，$Z$ 是代表参考总体的背景数据集，$x_S \\oplus Z_{\\overline{S}}$ 是通过将集合 $S$ 中的特征固定为 $x$ 的值，并从背景数据集中抽取的随机样本 $Z$ 中填补其余特征而形成的混合输入，期望 $\\mathbb{E}_Z[\\cdot]$ 是在给定的背景数据集上根据经验计算的。请注意，$v(\\emptyset) = \\mathbb{E}_{Z}[f(Z)]$ 且 $v(N) = f(x)$。\n\n需满足的公理：\n- 可加性（效率、局部准确性）：对于一个给定的 $x$，基线贡献 $v(\\emptyset)$ 加上各特征归因之和等于预测值 $v(N)$。\n- 一致性：如果在保持其他所有因素不变的情况下，特征 $i$ 对任何联盟 $S \\subseteq N \\setminus \\{i\\}$ 的边际贡献从一个模型到另一个模型有所增加，那么对特征 $i$ 的归因不应减少。\n\n您必须实现一个由上述合作博弈公理唯一确定的精确归因，方法是为每个特征 $i$ 枚举所有联盟 $S \\subseteq N$。您必须从第一性原理推导算法，并且不得在问题陈述中使用任何快捷公式。\n\n模型类别：为了在转化生物信息学应用中保证具体性和科学真实性，考虑线性响应模型\n$$\nf(x) = b + \\sum_{j=1}^{M} w_j x_j,\n$$\n其中 $b \\in \\mathbb{R}$ 是一个偏置项，$w_j \\in \\mathbb{R}$ 是学习到的系数。\n\n计算协议：\n1. 将所有 $S \\subseteq N$ 的 $v(S)$ 计算为背景数据集上的经验均值。\n2. 通过与上述公理一致的精确联盟枚举，计算目标 $x$ 的各特征归因。\n3. 通过检查 $v(\\emptyset) + \\sum_{i=1}^{M} \\phi_i \\approx v(N)$（数值容差为 $10^{-12}$）来验证可加性。\n4. 通过构建两个模型，其中所选特征 $i$ 的边际贡献对所有联盟都增加，来验证一致性，并证明相应的归因 $\\phi_i$ 没有减少。\n\n测试套件：为以下参数集实现计算。所有响应都是无单位的实值分数。\n\n- 测试 1（正常路径，$M=3$）：\n  - 权重 $w = [0.8, -0.5, 1.2]$，偏置 $b = 0.3$。\n  - 背景数据集 $Z$：\n    $$\n    \\begin{bmatrix}\n    0.9  0.2  0.1 \\\\\n    1.1  0.3  0.4 \\\\\n    0.8  0.5  0.6 \\\\\n    1.0  0.4  0.3 \\\\\n    0.7  0.2  0.5\n    \\end{bmatrix}\n    $$\n  - 目标 $x = [1.0, 0.4, 0.7]$。\n  - 输出：归因列表 $[\\phi_1,\\phi_2,\\phi_3]$ 和一个布尔值，表示可加性在 $10^{-12}$ 容差内成立。\n\n- 测试 2（边界情况：一个零权重特征，$M=3$）：\n  - 权重 $w = [0.9, 0.0, 0.7]$，偏置 $b = -0.1$。\n  - 背景数据集 $Z$：\n    $$\n    \\begin{bmatrix}\n    0.1  2.0  0.2 \\\\\n    0.2  2.5  0.4 \\\\\n    0.0  1.5  0.3 \\\\\n    0.3  3.0  0.1\n    \\end{bmatrix}\n    $$\n  - 目标 $x = [0.2, 5.0, 0.8]$。\n  - 输出：归因列表 $[\\phi_1,\\phi_2,\\phi_3]$，一个布尔值表示零权重特征的归因在 $10^{-12}$ 内数值上为零，以及一个布尔值表示可加性在 $10^{-12}$ 容差内成立。\n\n- 测试 3（模型间的一致性，$M=3$）：\n  - 背景数据集 $Z$：\n    $$\n    \\begin{bmatrix}\n    0.5  0.4  0.3 \\\\\n    0.6  0.3  0.2 \\\\\n    0.4  0.5  0.1\n    \\end{bmatrix}\n    $$\n  - 目标 $x = [0.9, 0.1, 0.5]$。\n  - 模型 $f$：权重 $w^{(f)} = [0.5, 0.4, 0.1]$，偏置 $b^{(f)} = 0.0$。\n  - 模型 $g$：权重 $w^{(g)} = [0.9, 0.4, 0.1]$，偏置 $b^{(g)} = 0.0$（只有第一个权重增加）。\n  - 输出：两个模型的归因列表 $[\\phi^{(f)}_1,\\phi^{(f)}_2,\\phi^{(f)}_3]$ 和 $[\\phi^{(g)}_1,\\phi^{(g)}_2,\\phi^{(g)}_3]$，一个布尔值表示特征 $1$ 的一致性成立（即 $\\phi^{(g)}_1 \\ge \\phi^{(f)}_1$），以及分别表示每个模型的可加性在 $10^{-12}$ 容差内成立的布尔值。\n\n- 测试 4（边缘情况：单一特征，$M=1$）：\n  - 权重 $w = [1.5]$，偏置 $b = 0.2$。\n  - 背景数据集 $Z$：\n    $$\n    \\begin{bmatrix}\n    1.0 \\\\\n    0.8 \\\\\n    1.2 \\\\\n    0.9\n    \\end{bmatrix}\n    $$\n  - 目标 $x = [2.0]$。\n  - 输出：归因列表 $[\\phi_1]$ 和一个布尔值，表示可加性在 $10^{-12}$ 容差内成立。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔的结果列表。各项必须严格按以下顺序出现：\n- 测试 1：归因列表，可加性布尔值。\n- 测试 2：归因列表，零权重布尔值，可加性布尔值。\n- 测试 3：模型 $f$ 的归因列表，模型 $g$ 的归因列表，特征 $1$ 的一致性布尔值，模型 $f$ 的可加性布尔值，模型 $g$ 的可加性布尔值。\n- 测试 4：归因列表，可加性布尔值。\n例如，输出应如下所示：\n$$\n[\\,[\\phi_1,\\phi_2,\\phi_3],\\,\\text{bool1},\\,[\\phi_1,\\phi_2,\\phi_3],\\,\\text{bool2},\\,\\text{bool3},\\,[\\phi^{(f)}_1,\\phi^{(f)}_2,\\phi^{(f)}_3],\\,[\\phi^{(g)}_1,\\phi^{(g)}_2,\\phi^{(g)}_3],\\,\\text{bool4},\\,\\text{bool5},\\,\\text{bool6},\\,[\\phi_1],\\,\\text{bool7}\\,].\n$$\n所有布尔值必须是 $True$ 或 $False$，所有数字必须是实值浮点数。不应打印任何无关文本。", "solution": "该问题是有效的。它在科学上基于已建立的合作博弈理论及其在机器学习可解释性（SHAP值）中的应用，问题陈述良定，提供了所有必要信息，并且表述客观。我们将着手解决。\n\n该问题要求通过枚举所有可能的特征联盟来计算线性模型的 SHapley 加性解释 (SHAP) 值。该方法提供了一种独特的特征归因，满足几个理想的公理，包括问题陈述中指定的可加性和一致性。\n\n### 1. 理论基础：Shapley 值\n\n核心任务是将模型 $f(x)$ 的预测值（相对于一个基线预测）归因于其各个特征。问题定义了一个合作博弈，其中“参与者”是特征 $i \\in N = \\{1, 2, \\dots, M\\}$。一个“联盟”（特征子集 $S \\subseteq N$）的价值由以下函数给出：\n$$\nv(S) = \\mathbb{E}_{Z}\\big[f\\big(x_S \\oplus Z_{\\overline{S}}\\big)\\big]\n$$\n这里，$x_S \\oplus Z_{\\overline{S}}$ 表示一个混合输入向量，其构造方法是：对于联盟 $S$ 中的特征，取目标实例 $x$ 的值；对于其余特征（$\\overline{S} = N \\setminus S$），从背景数据集 $Z$ 中填补其值。期望 $\\mathbb{E}_Z$ 是通过对 $Z$ 中所有样本的平均预测来经验性计算的。基线预测是 $v(\\emptyset) = \\mathbb{E}_{Z}[f(Z)]$，而针对特定实例 $x$ 的完整模型预测是 $v(N)=f(x)$。\n\n满足可加性（效率）、对称性和线性性（或一致性，它与对称性和效率一起也确保了唯一性）公理的唯一归因方案是 Shapley 值，一个源自合作博弈论的概念。特征 $i$ 的 Shapley 值 $\\phi_i$ 是其在所有可能的特征排序排列中对博弈价值的平均边际贡献。\n\n考虑 $N$ 中特征的一个排列 $\\pi$。令 $P_i(\\pi)$ 为在该排序中位于特征 $i$ 之前的特征集合。对于这个特定排序，特征 $i$ 的边际贡献是当 $i$ 加入其前置集合时价值函数的变化：\n$$\n\\Delta v(i | \\pi) = v(P_i(\\pi) \\cup \\{i\\}) - v(P_i(\\pi))\n$$\n为确保公平性，我们对所有 $M!$ 种可能的特征排列的边际贡献取平均值：\n$$\n\\phi_i = \\frac{1}{M!} \\sum_{\\pi} \\Delta v(i | \\pi)\n$$\n这个公式在计算上具有挑战性。一个更实用、等价的公式是通过根据前置集合 $S = P_i(\\pi)$ 对排列进行分组来推导。对于一个大小为 $|S|=s$ 的给定联盟 $S \\subseteq N \\setminus \\{i\\}$，有 $s!$ 种方式对 $S$ 内的特征进行排序，有 $(M-s-1)!$ 种方式对不在 $S \\cup \\{i\\}$ 内的特征进行排序。因此，共有 $s!(M-s-1)!$ 种排列，其中特征 $i$ 被添加到联盟 $S$ 中。Shapley 值于是可以表示为对所有特征 $i$ 可能加入的联盟求和：\n$$\n\\phi_i = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!(M-|S|-1)!}{M!} \\left( v(S \\cup \\{i\\}) - v(S) \\right)\n$$\n这就是我们将要实现的公式。它满足可加性公理：$v(\\emptyset) + \\sum_{i=1}^{M} \\phi_i = v(N)$。\n\n### 2. 算法实现\n\n根据推导出的公式，算法主要分为两个阶段：\n\n**阶段 1：为所有联盟计算价值函数 $v(S)$**\n我们必须首先为所有 $2^M$ 个可能的子集 $S \\subseteq N$ 计算 $v(S)$。\n1.  生成特征索引集 $\\{0, 1, \\dots, M-1\\}$ 的所有子集 $S$。\n2.  对于每个子集 $S$：\n    a. 为了计算 $v(S)$，我们遍历背景数据集 $Z$ 中的每个样本 $z^{(k)}$。\n    b. 对于每个 $z^{(k)}$，我们构造一个混合输入向量 $x'_{k, S}$。对于每个特征索引 $j \\in \\{0, \\dots, M-1\\}$，如果 $j \\in S$，则 $(x'_{k, S})_j$ 的值设为 $x_j$；如果 $j \\notin S$，则设为 $z^{(k)}_j$。\n    c. 我们计算模型对这个混合向量的预测：$f(x'_{k, S}) = b + \\sum_{j=0}^{M-1} w_j (x'_{k, S})_j$。\n    d. $v(S)$ 是这些预测值在所有 $k$ 上的平均值。\n3.  存储计算出的 $v(S)$ 值，例如在一个将联盟映射到其值的字典中。\n\n**阶段 2：计算特征归因 $\\phi_i$**\n在所有 $v(S)$ 值都已预先计算好的情况下，我们现在可以计算归因。\n1. 对于每个特征 $i \\in \\{0, \\dots, M-1\\}$：\n    a. 将其归因 $\\phi_i$ 初始化为 $0$。\n    b. 遍历 $N \\setminus \\{i\\}$ 的所有子集 $S$。\n    c. 对于每个大小为 $s=|S|$ 的子集 $S$，计算组合权重 $W = \\frac{s!(M-s-1)!}{M!}$。\n    d. 计算特征 $i$ 对联盟 $S$ 的边际贡献：$\\Delta v = v(S \\cup \\{i\\}) - v(S)$。\n    e. 将加权贡献加到总归因上：$\\phi_i \\leftarrow \\phi_i + W \\cdot \\Delta v$。\n\n### 3. 公理验证\n- **可加性：** 我们通过计算 $v(\\emptyset) + \\sum_{i=0}^{M-1} \\phi_i$ 并检查结果是否在 $10^{-12}$ 的数值容差内等于 $v(N)$ 来验证此性质。\n- **一致性：** 对于测试 3，我们有两个模型 $f$ 和 $g$，其中 $w_1^{(g)}  w_1^{(f)}$ 且其他权重相等。对于线性模型，特征 $i$ 的边际贡献是 $v(S \\cup \\{i\\}) - v(S) = w_i(x_i - \\mathbb{E}[Z_i])$，这与 $S$ 无关。由于在此测试案例中 $x_1  \\mathbb{E}[Z_1]$，对于所有联盟，特征 1 的边际贡献对于模型 $g$ 来说都严格更大。一致性公理指出 $\\phi_1$ 不应减少，即 $\\phi_1^{(g)} \\ge \\phi_1^{(f)}$。我们为两个模型计算归因并验证此不等式。\n\n这种枚举方法是精确的，并直接遵循 Shapley 值的定义，符合要求。", "answer": "```python\nimport numpy as np\nfrom itertools import combinations\nfrom scipy.special import factorial\n\ndef get_attributions_and_value_function(w, b, background_z, target_x):\n    \"\"\"\n    Computes SHAP values by exactly enumerating all feature coalitions,\n    as derived from the axiomatic definition of Shapley values.\n\n    Args:\n        w (np.ndarray): Model weights.\n        b (float): Model bias.\n        background_z (np.ndarray): Background dataset for expectation calculation.\n        target_x (np.ndarray): The specific instance for which to explain the prediction.\n\n    Returns:\n        tuple: A tuple containing:\n            - list[float]: The list of SHAP values (attributions) for each feature.\n            - dict[frozenset, float]: The computed value function v(S) for all coalitions S.\n    \"\"\"\n    num_features = len(w)\n    \n    # Use frozensets for hashable dictionary keys representing coalitions.\n    value_function = {}\n\n    # Stage 1: Compute the value function v(S) for all 2^M coalitions S.\n    all_feature_indices = set(range(num_features))\n    for s_size in range(num_features + 1):\n        for s_indices_tuple in combinations(all_feature_indices, s_size):\n            coalition_s = frozenset(s_indices_tuple)\n            \n            # Construct hybrid inputs x_S + Z_{\\bar{S}} by iterating through background samples.\n            hybrid_predictions = []\n            for z_sample in background_z:\n                hybrid_instance = np.copy(z_sample)\n                if coalition_s: # Avoid operation for empty set\n                    for feature_idx in coalition_s:\n                        hybrid_instance[feature_idx] = target_x[feature_idx]\n                \n                # Calculate model prediction for the hybrid instance.\n                prediction = b + np.dot(w, hybrid_instance)\n                hybrid_predictions.append(prediction)\n            \n            # v(S) is the empirical expectation (mean) of these predictions.\n            value_function[coalition_s] = np.mean(hybrid_predictions)\n\n    # Stage 2: Compute Shapley values phi_i using the pre-computed value function.\n    attributions = [0.0] * num_features\n    fact_M = factorial(num_features)\n\n    for i in range(num_features):\n        # Iterate over all coalitions S that do NOT contain feature i.\n        feature_set_without_i = all_feature_indices - {i}\n        for s_size in range(num_features):\n            for s_indices_tuple in combinations(feature_set_without_i, s_size):\n                coalition_s = frozenset(s_indices_tuple)\n                \n                # Shapley weight for a coalition of size |S|.\n                weight = (factorial(s_size) * factorial(num_features - s_size - 1)) / fact_M\n                \n                # Marginal contribution of feature i to coalition S.\n                coalition_s_with_i = coalition_s.union({i})\n                marginal_contribution = value_function[coalition_s_with_i] - value_function[coalition_s]\n                \n                attributions[i] += weight * marginal_contribution\n                \n    return attributions, value_function\n\ndef solve():\n    \"\"\"\n    Solves the problem by running all test cases and formatting the output.\n    \"\"\"\n    results = []\n    TOLERANCE = 1e-12\n\n    # --- Test 1 ---\n    w1 = np.array([0.8, -0.5, 1.2])\n    b1 = 0.3\n    z1 = np.array([\n        [0.9, 0.2, 0.1], [1.1, 0.3, 0.4], [0.8, 0.5, 0.6],\n        [1.0, 0.4, 0.3], [0.7, 0.2, 0.5]\n    ])\n    x1 = np.array([1.0, 0.4, 0.7])\n    \n    phi1, v1 = get_attributions_and_value_function(w1, b1, z1, x1)\n    \n    empty_set_key = frozenset()\n    full_set_key = frozenset(range(len(w1)))\n    v1_empty = v1[empty_set_key]\n    v1_full = v1[full_set_key]\n    additivity_check1 = abs((v1_empty + sum(phi1)) - v1_full)  TOLERANCE\n    results.extend([phi1, additivity_check1])\n\n    # --- Test 2 ---\n    w2 = np.array([0.9, 0.0, 0.7])\n    b2 = -0.1\n    z2 = np.array([\n        [0.1, 2.0, 0.2], [0.2, 2.5, 0.4],\n        [0.0, 1.5, 0.3], [0.3, 3.0, 0.1]\n    ])\n    x2 = np.array([0.2, 5.0, 0.8])\n    \n    phi2, v2 = get_attributions_and_value_function(w2, b2, z2, x2)\n    \n    # Feature 2 (index 1) has zero weight.\n    zeroweight_check2 = abs(phi2[1])  TOLERANCE\n    \n    empty_set_key_2 = frozenset()\n    full_set_key_2 = frozenset(range(len(w2)))\n    v2_empty = v2[empty_set_key_2]\n    v2_full = v2[full_set_key_2]\n    additivity_check2 = abs((v2_empty + sum(phi2)) - v2_full)  TOLERANCE\n    results.extend([phi2, zeroweight_check2, additivity_check2])\n\n    # --- Test 3 ---\n    z3 = np.array([\n        [0.5, 0.4, 0.3], [0.6, 0.3, 0.2], [0.4, 0.5, 0.1]\n    ])\n    x3 = np.array([0.9, 0.1, 0.5])\n    \n    # Model f\n    w3f = np.array([0.5, 0.4, 0.1])\n    b3f = 0.0\n    phi3f, v3f = get_attributions_and_value_function(w3f, b3f, z3, x3)\n    \n    # Model g\n    w3g = np.array([0.9, 0.4, 0.1])\n    b3g = 0.0\n    phi3g, v3g = get_attributions_and_value_function(w3g, b3g, z3, x3)\n    \n    # Consistency for feature 1 (index 0)\n    consistency_check3 = phi3g[0] >= phi3f[0]\n    \n    # Additivity for model f\n    v3f_empty = v3f[frozenset()]\n    v3f_full = v3f[frozenset(range(len(w3f)))]\n    additivity_check3f = abs((v3f_empty + sum(phi3f)) - v3f_full)  TOLERANCE\n    \n    # Additivity for model g\n    v3g_empty = v3g[frozenset()]\n    v3g_full = v3g[frozenset(range(len(w3g)))]\n    additivity_check3g = abs((v3g_empty + sum(phi3g)) - v3g_full)  TOLERANCE\n    \n    results.extend([phi3f, phi3g, consistency_check3, additivity_check3f, additivity_check3g])\n\n    # --- Test 4 ---\n    w4 = np.array([1.5])\n    b4 = 0.2\n    z4 = np.array([[1.0], [0.8], [1.2], [0.9]])\n    x4 = np.array([2.0])\n    \n    phi4, v4 = get_attributions_and_value_function(w4, b4, z4, x4)\n    \n    v4_empty = v4[frozenset()]\n    v4_full = v4[frozenset(range(len(w4)))]\n    additivity_check4 = abs((v4_empty + sum(phi4)) - v4_full)  TOLERANCE\n    results.extend([phi4, additivity_check4])\n    \n    # Format representations for printing\n    def format_item(item):\n        if isinstance(item, list):\n            return str(item)\n        if isinstance(item, bool):\n            return str(item)\n        return item\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4396100"}]}