## 引言
基因的差异化表达是细胞身份和功能的基石，而[表观遗传调控](@entry_id:202273)，特别是[染色质可及性](@entry_id:163510)，是控制这一过程的核心开关。单细胞[染色质可及性](@entry_id:163510)测序（[scATAC-seq](@entry_id:166214)）技术的发展，使我们能够以前所未有的分辨率，逐个细胞地绘制出这些调控开关的开放[状态图](@entry_id:176069)谱。然而，这项技术产生的原始数据具有高维度、高稀疏性的特点，对数据分析提出了巨大的挑战，原始的测序读段与深刻的生物学洞见之间存在着巨大的鸿沟。本文旨在系统性地填补这一鸿沟，为研究者提供一份从理论到应用的完整指南。

为实现这一目标，本文分为三个核心部分。在第一章“原理与机制”中，我们将深入剖析从数据生成到细胞聚类的每一步背后的科学原理和计算方法，揭示如何从稀疏的数据中提取稳健的生物学信号。接着，在第二章“应用与交叉学科联系”中，我们将展示这些分析方法在发育生物学、免疫学和精准医学等前沿领域的强大应用，并探讨如何通过[多组学整合](@entry_id:267532)构建细胞状态的全景视图。最后，“动手实践”部分将通过具体的编程练习，帮助读者将理论知识转化为实际操作技能。通过这一结构化的学习路径，读者将能够全面掌握单细胞染色质可及性分析的核心思想与实践能力，从而有效地利用这一前沿技术解答复杂的生物学问题。

## 原理与机制

本章旨在深入剖析单细胞[染色质可及性](@entry_id:163510)分析背后的核心科学原理与计算机制。我们将遵循一条从原始测序数据到高级生物学洞见的逻辑路径，系统性地阐述数据生成、预处理、质量控制、降维、聚类以及调控推断等各个关键环节的理论基础。

### 数据生成的基本原理

单细胞染色质可及性测序（[scATAC-seq](@entry_id:166214)）的分析始于对数据生成过程的深刻理解。这不仅涉及实验技术，更关乎其内在的物理化学与统计学原理。

#### [Tn5转座酶](@entry_id:171347)的[生物物理学](@entry_id:200723)机制

[scATAC-seq](@entry_id:166214)的核心是利用**[Tn5转座酶](@entry_id:171347)**探测染色质的开放状态。染色质由DNA缠绕[组蛋白](@entry_id:196283)八聚体形成的**核小体**串联而成。当DNA紧密包裹在[核小体](@entry_id:153162)上时，其对大多数酶（包括Tn5）是**不可及的**。相反，位于[核小体](@entry_id:153162)之间的**连接DNA**（linker DNA）或因调控活动导致[核小体](@entry_id:153162)被移除的区域，则暴露在外，形成**可及的**或**开放的**染色质区域 [@problem_id:4314889]。

[Tn5转座酶](@entry_id:171347)优先在这些可及的DNA位点进行切割和测序接头的插入。这种偏好性并非绝对，而是由[生物物理学](@entry_id:200723)原理决定的。我们可以通过一个简化的模型来量化这一过程 [@problem_id:4314885]。Tn5插入事件的速率主要受两个因素控制：

1.  **可及接触分数（Accessible Contact Fraction）**：在一段染色质中，只有连接DNA是主要的可接触靶点。因此，一个区域的连接DNA长度（$\ell_{L}$）相对于其重复单元总长度（$\ell_{L} + \ell_{N}$，其中$\ell_{N}$为[核小体](@entry_id:153162)核心颗粒长度，约为$147$ bp）的比例，即 $S_i = \ell_{L}^{i}/(\ell_{N} + \ell_{L}^{i})$，决定了酶与DNA的[接触概率](@entry_id:194741)。开放染色质（区域O）通常具有较长的连接DNA（例如，$\ell_{L}^{O} = 50$ bp），而致密染色质（区域C）的连接DNA则很短（例如，$\ell_{L}^{C} = 10$ bp），因此$S_O$远大于$S_C$。

2.  **[空间位阻](@entry_id:156748)与[大分子拥挤](@entry_id:170968)**：细胞核内环境高度拥挤，这种拥挤对酶的活性构成了能量屏障。我们可以用一个有效的[自由能垒](@entry_id:203446) $\Delta G$ 来描述这种[位阻效应](@entry_id:148138)。致密染色质的局部大分子体积分数（$\phi_C$）远高于开放染色质（$\phi_O$），导致其能量壁垒更高（$\Delta G_C > \Delta G_O$）。根据[玻尔兹曼原理](@entry_id:148572)，穿越能量壁垒的概率与 $\exp(-\Delta G/k_{B}T)$ 成正比。

综合这两个因素，Tn5在开放区域O相对于致密区域C的插入[速率比](@entry_id:164491)值（$r_O/r_C$）可以近似为：
$$
\frac{r_O}{r_C} = \frac{S_O}{S_C} \cdot \exp\left(\frac{\Delta G_C - \Delta G_O}{k_{B}T}\right)
$$
在一个现实的假设场景中（$\ell_{N} = 147$ bp, $\ell_{L}^{O} = 50$ bp, $\ell_{L}^{C} = 10$ bp, $\phi_O = 0.1$, $\phi_C=0.6$），该比值可高达约$30$倍 [@problem_id:4314885]。这一定量关系从根本上解释了[ATAC-seq](@entry_id:169892)能够有效富集开放染色质信号的原因。

这种插入偏好性也塑造了测序文库的**片段长度分布**。在开放染色质区域，由于插入事件频繁，两个相近的插入事件很可能发生在同一段连接DNA上，产生大量短于[核小体](@entry_id:153162)长度（例如，小于$150$ bp）的**亚[核小体](@entry_id:153162)片段**。而在致密染色质区域，插入事件稀少且主要发生在极短的连接DNA上，因此片段大多由跨越一个或多个核小体的插入事件形成，其长度呈现出以核小体重复长度（约$170-200$ bp）及其倍数为周期的特征性峰 [@problem_id:4314885]。因此，片段长度分布本身就是一种重要的数据质量控制指标。

#### 数据的稀疏性：结构性零与取样性零

[scATAC-seq](@entry_id:166214)数据的另一个核心特征是其高度的**稀疏性**——数据矩阵中绝大多数的计数值为零。理解这些零的来源至关重要。我们可以将零计数值分为两类 [@problem_id:4314898]：

1.  **结构性零（Structural Zeros）**：这些零源于生物学现实。如果在一个特定的细胞中，某个基因组区域的染色质是关闭的（即不可及的），那么[Tn5转座酶](@entry_id:171347)从一开始就无法在该区域发生插入。无论测序多深，该细胞在该区域的计数值都将是零。

2.  **取样性零（Sampling Zeros）**：这些零是技术限制的结果。即使一个区域是开放的，Tn5的插入也是一个[随机过程](@entry_id:268487)，且整个文库制备和测序过程只捕获了细胞中一小部分的DNA片段。因此，一个开放的区域可能因为偶然没有被Tn5切到，或者产生的片段在后续步骤中丢失，从而导致计数值为零。

我们可以用一个分层模型来形式化这个过程。对于每个细胞$c$和区域$r$，存在一个潜在的、不可观测的“可及状态”变量 $S_{cr} \in \{0,1\}$。该区域为可及的概率为 $\pi_r$。观测到的片段计数 $Y_{cr}$ 的生成过程如下：
-   如果 $S_{cr}=0$（不可及），则 $Y_{cr}=0$。
-   如果 $S_{cr}=1$（可及），插入事件遵循泊松过程，其速率 $\lambda_r$ 受到细胞特异性捕获效率 $\epsilon_c$ 的调节，使得观测计数 $Y_{cr} \sim \text{Poisson}(\lambda_r \epsilon_c)$。

根据这个模型，在一个细胞中观测到零计数的总概率为：
$$
\Pr(Y_{cr}=0) = \underbrace{(1-\pi_r)}_{\text{结构性零}} + \underbrace{\pi_r \exp(-\epsilon_c \lambda_r)}_{\text{取样性零}}
$$
这个公式清晰地揭示了零计数的双重来源。细胞捕获效率 $\epsilon_c$ 越低，取样性零的比例就越高。区分这两种零是后续定量分析（如差异可及性分析）的关键，因为只有结构性零的变化才直接反映了生物学状态的改变 [@problem_id:4314898]。

### 从原始读段到计数矩阵

在获得原始测序读段后，需要经过一系列计算步骤将其转化为可供分析的“区域-细胞”计数矩阵。

#### [细胞条形码](@entry_id:171163)的解析与校正

在[scATAC-seq](@entry_id:166214)实验中，来自单个细胞的DNA片段会被标记上独特的**[细胞条形码](@entry_id:171163)（cell barcode）**。这一步使得我们可以将混合测序的读段准确地追溯回其来源细胞。然而，测序过程不可避免地会引入错误，导致观测到的条形码可能与其原始序列不符。

为了解决这个问题，通常会使用一个预先设计好的**条形码白名单（whitelist）**。这个白名单中的条形码具有良好的**[纠错](@entry_id:273762)编码**属性，例如，任意两个有效条形码之间的**[汉明距离](@entry_id:157657)**（Hamming distance，即对应位置上不同字符的数量）至少为$3$ [@problem_id:4314860]。

这个属性至关重要。根据编码理论，一个最小距离为 $d_{\text{min}}$ 的码本可以明确地纠正最多 $\lfloor (d_{\text{min}} - 1) / 2 \rfloor$ 个错误。当 $d_{\text{min}} \ge 3$ 时，这保证了任何发生单个碱基替换错误的观测条形码，其[汉明距离](@entry_id:157657)为$1$的邻域内最多只包含一个白名单条形码。由于测序错误以单个碱基替换为主（其概率服从[二项分布](@entry_id:141181) $B(k; L, p)$，其中$L$为条形码长度，$p$为单碱基错误率），绝大多数错误都是单个错误。因此，一个稳健的策略是：将任何与白名单中的唯一一个条形码[汉明距离](@entry_id:157657)为$1$的观测条形码，校正为该白名单条形码。尝试校正[汉明距离](@entry_id:157657)为$2$或以上的错误则有风险，因为观测序列可能与多个白名单条形码等距，从而导致模糊性和错误分配 [@problem_id:4314860]。

#### 数据质量控制

在构建计数矩阵之前，必须评估每个细胞数据的质量，以滤除低质量的细胞。**[转录起始位点](@entry_id:263682)（TSS）富集分数**是[scATAC-seq](@entry_id:166214)中最核心的质量控制指标之一 [@problem_id:4314921]。

其基本思想是，在有活性的细胞中，基因启动子区域（TSS周围）通常是高度开放的，因此[ATAC-seq](@entry_id:169892)信号应在此处呈现显著富集。TSS富集分数的计算方法如下：
1.  对所有注释的TSS，将其为中心对齐，并计算TSS上游和下游一定窗口内（例如，$\pm 2000$ bp）所有细胞的平均Tn5插入密度。
2.  定义一个小的**中心窗口**（例如，TSS $\pm 50$ bp）和一个或多个远离中心的**侧翼背景窗口**（例如，TSS $\pm 200$ 到 $\pm 1000$ bp）。
3.  计算中心窗口内的平均每碱基插入密度（$\bar{f}_{\mathrm{center}}$）和侧翼窗口内的平均每碱基插入密度（$\bar{f}_{\mathrm{bg}}$）。
4.  **TSS富集分数**定义为二者之比：$\text{Score} = \bar{f}_{\mathrm{center}} / \bar{f}_{\mathrm{bg}}$。

在一个“信号+噪声”模型中，插入率 $r(d) = \lambda_{\mathrm{bg}} + \lambda_{\mathrm{sig}} s(d)$，其中 $\lambda_{\mathrm{bg}}$ 是背景插入率，$\lambda_{\mathrm{sig}}s(d)$ 是TSS附近的信号。侧翼窗口的平均[密度估计](@entry_id:634063)了背景 $\lambda_{\mathrm{bg}}$，而中心窗口的平均[密度估计](@entry_id:634063)了信号与背景之和 $\lambda_{\mathrm{bg}} + \lambda_{\mathrm{sig}}\bar{s}_{\mathrm{center}}$。因此，TSS富集分数近似于 $1 + (\lambda_{\mathrm{sig}}/\lambda_{\mathrm{bg}})\bar{s}_{\mathrm{center}}$，它直接量化了TSS信号相对于背景噪声的强度。例如，观测到 $\bar{f}_{\mathrm{center}} = 1.50 \times 10^{-2}$ 和 $\bar{f}_{\mathrm{bg}} = 3.00 \times 10^{-3}$，计算出的富集分数为$5.0$，这通常表示[数据质量](@entry_id:185007)良好 [@problem_id:4314921]。

#### 特征的定义：Peaks vs. Bins

计数矩阵的行代表细胞，列代表基因组特征。特征的选择是一个关键决策，主要有两种策略 [@problem_id:4314910]：

1.  **基于Peak的特征（Peak-based Features）**：此方法首先将所有细胞的数据聚合起来，形成一个**伪批量（pseudo-bulk）**的信号图谱。然后使用像**MACS2**这样的工具在这个图谱上识别信号显著富集的区域，即**peaks**。这些peaks（通常几万到几十万个）被作为特征，然后对每个单细胞分别统计落在每个peak内的片段数。这种方法的优点是**生物学可解释性强**，因为peaks通常对应着[顺式调控元件](@entry_id:275840)（如增[强子](@entry_id:198809)、启动子）。其缺点是，peak的定义依赖于所有细胞的平均信号，可能会忽略掉稀有细胞亚群特有的可及性区域。

2.  **基于窗口的特征（Bin-based Features）**：此方法将整个基因组或特定[区域划分](@entry_id:748628)为固定宽度的**窗口（bins）**，例如每$5000$ bp一个窗口。然后统计每个细胞在每个窗口中的片段数。这种方法的优点是**无偏的**，因为它不依赖于任何先验的信号富集信息，能够捕捉到任何位置的可及性变化。其缺点是**分辨率和[可解释性](@entry_id:637759)**可能较低。宽窗口会降低分辨率，而窗口的边界是任意的，通常不直接对应于功能元件。

这两种策略在数据矩阵的**稀疏性**、**分辨率**和**可解释性**之间构成了权衡。基于peak的矩阵通常特征数量较少，且由于只关注信号富集区域，非零项的比例可能相对较高（即矩阵相对不那么稀疏）。而基于基因组窗口的矩阵特征数量巨大，其中大部分窗口位于非可及区域，导致矩阵极度稀疏。然而，当窗口大小与peak宽度相同时，由于包含了大量几乎恒为零的背景窗口，基因组分箱策略反而会产生一个整体上更稀疏的矩阵 [@problem_id:4314910]。选择哪种策略取决于具体的分析目标，例如，发现新细胞类型可能受益于无偏的窗口法，而研究已知细胞类型的[调控网络](@entry_id:754215)则更适合于可解释性强的peak法。

### 在[高维数据](@entry_id:138874)中发掘结构

生成的“区域-细胞”计数矩阵维度极高（例如，$100,000$个特征 $\times$ $10,000$个细胞），且高度稀疏。直接在此空间中进行分析是困难的。因此，必须采用[降维技术](@entry_id:169164)来提取有意义的低维结构。

#### 潜在语义索引（LSI）

**潜在语义索引（Latent Semantic Indexing, LSI）** 是[scATAC-seq](@entry_id:166214)分析中一种标准且强大的[降维](@entry_id:142982)方法，它借鉴了自然语言处理中的概念 [@problem_id:4314895]。该流程包含两个主要步骤：

1.  **[TF-IDF](@entry_id:634366)加权**：首先，对原始计数矩阵进行**[词频-逆文档频率](@entry_id:634366)（Term Frequency-Inverse Document Frequency, [TF-IDF](@entry_id:634366)）**变换。
    -   **词频（TF）**：对于每个细胞，将其在各个peak上的计数值归一化（除以该细胞的总计数值）。这校正了不同细胞间**[测序深度](@entry_id:178191)**的差异。$tf_{ij} = x_{ij} / \sum_{i'} x_{i'j}$。
    -   **逆文档频率（IDF）**：计算一个权重，它与一个peak在多少细胞中被检测到（$df_i$）成反比。公式为 $idf_i = \log(N / (1 + df_i))$，其中$N$是细胞总数。这一步的目的是降低在几乎所有细胞中都开放的“管家”peaks的权重，同时提升只在少数特定细胞亚群中开放的、具有更高信息量的peaks的权重。

2.  **[奇异值分解](@entry_id:138057)（SVD）**：对得到的[TF-IDF](@entry_id:634366)矩阵 $M$ 进行**奇异值分解**：$M = U \Sigma V^{\top}$。SVD将[矩阵分解](@entry_id:139760)为三个矩阵的乘积：$U$ 的列是与特征（peaks）相关的[左奇异向量](@entry_id:751233)，$V$ 的列是与样本（细胞）相关的[右奇异向量](@entry_id:754365)，$\Sigma$ 是一个[对角矩阵](@entry_id:637782)，其对角线上的元素是**[奇异值](@entry_id:171660)** $\sigma_i$，按从大到小排列。[奇异值](@entry_id:171660)的大小反映了对应维度（成分）所解释的数据方差量。

细胞的**LSI嵌入**（低维表示）通常由前$k$个[右奇异向量](@entry_id:754365)与对应的[奇异值](@entry_id:171660)加权得到，即 $V_k \Sigma_k$ 的行向量。每个细胞被表示为一个$k$维向量。

选择合适的维度$k$至关重要。过小的$k$会丢失重要的生物学结构，过大的$k$则会引入噪声。通常结合两种策略来确定$k$ [@problem_id:4314895]：
-   **[碎石图](@entry_id:143396)（Scree Plot）**：绘制[奇异值](@entry_id:171660) $\sigma_i$ 随维度 $i$ 变化的曲线。寻找曲线斜率急剧变缓的“[拐点](@entry_id:144929)（elbow）”。拐点之前的维度被认为是捕获主要信号的维度。
-   **下游任务性能**：评估不同$k$值对下游分析（如聚类）的影响。例如，可以通过[自助法](@entry_id:139281)[重采样](@entry_id:142583)来评估聚类结果的**稳定性**，并选择使稳定性指标（如归一化互信息NMI）达到平台期的最小$k$值。

#### 批次效应的识别

[scATAC-seq](@entry_id:166214)数据常受到**[批次效应](@entry_id:265859)（batch effects）**的干扰，即由不同实验批次（如不同的测序运行、不同的样本处理日期）引入的系统性技术变异 [@problem_id:4314916]。这些效应可能掩盖真实的生物学信号，导致细胞错误地按批次而非按生物学类型聚类。

在LSI空间中，[批次效应](@entry_id:265859)有其独特的表现形式：
-   **[全局效率](@entry_id:749922)差异**：如果[批次效应](@entry_id:265859)仅仅是全局性的（例如，一个批次的Tn5酶活性或测序效率整体偏低），TF归一化在理论上可以校正这种影响。然而，较低的效率会导致该批次细胞的数据更稀疏，这种由采样差异引起的结构性变化仍可能被LSI捕获，导致细胞在LSI空间中按批次分离。
-   **特征特异性偏好**：更棘手的情况是，批次效应可能对特定的peak子集有偏好（例如，某个批次的技术对富含GC的区域有更高的捕获效率）。这种非均匀的偏好无法被TF归一化完全消除。如果这些受影响的peaks恰好是稀有的（IDF权重高），那么[TF-IDF](@entry_id:634366)变换会放大这种批次相关的信号，导致某个LSI主成分专门用来区分不同批次的细胞。

一个有效的诊断方法是检查LSI主成分是否与已知的QC指标（如TSS富集分数、FRiP分数等）高度相关。如果第一或第二个LSI成分（解释方差最大的成分）与一个技术性QC指标强相关，这强烈暗示该成分捕获的是[批次效应](@entry_id:265859)而非生物学变异 [@problem_id:4314916]。

### 识别[细胞异质性](@entry_id:262569)

拥有了经过[降维](@entry_id:142982)和批次校正的细胞低维表示后，下一步是识别其中存在的不同细胞亚群。

#### [基于图的聚类](@entry_id:174462)

目前，最流行和有效的方法是**[基于图的聚类](@entry_id:174462)** [@problem_id:4314868]。该方法包括两个步骤：

1.  **构建[k-近邻图](@entry_id:751051)（kNN Graph）**：在LSI空间中，为每个细胞找到其最近的$k$个邻居细胞。然后构建一个图，其中每个细胞是一个节点，如果两个细胞互为近邻，则在它们之间连一条边。边的权重可以设置为与它们在LSI空间中的距离成反比。这个图捕捉了细胞间的局部相似性结构。

2.  **社区检测（Community Detection）**：在构建好的[kNN图](@entry_id:751051)上，应用社区检测算法来识别节点的[密集连接](@entry_id:634435)子图，即“社区”。每个社区对应于一个细胞簇。**[Louvain算法](@entry_id:270022)**和**[Leiden算法](@entry_id:751237)**是两种广泛使用的[启发式算法](@entry_id:176797)，它们通过优化一个名为**模块度（modularity）**的指标来寻找社区。模块度$Q$衡量了一个社区内部边的密度与在[随机网络](@entry_id:263277)中期望的内部[边密度](@entry_id:271104)的差异。

[Leiden算法](@entry_id:751237)是Louvain的改进版，它保证了检测出的社区都是连通的，避免了[Louvain算法](@entry_id:270022)有时会产生内部不连通社区的问题。这些算法通常包含一个**分辨[率参数](@entry_id:265473)（resolution parameter）$\gamma$**。通过调节$\gamma$，可以控制聚类的粒度：增加$\gamma$会倾向于发现更多、更小的社区（高分辨率），而减小$\gamma$则会产生更少、更大的社区（低分辨率）。选择合适的分辨率需要结合对聚类稳定性的评估和对已知生物学标记（如特定细胞类型的marker peak）在簇间表达模式的检查 [@problem_id:4314868]。

### 推断调控动态

识别出细胞簇后，最终目标是理解是什么调控机制驱动了它们的身份和状态差异。

#### chromVAR：量化转录因子活性

**chromVAR**是一种创新的方法，旨在从[scATAC-seq](@entry_id:166214)数据中推断**转录因子（TF）**的活性变化 [@problem_id:4314944]。它不直接比较单个peak的开放性，而是考察包含特定TF结合基序（motif）的**一组peaks**的整体可及性变化。

其核心思想是，如果一个TF在某个细胞中是活跃的，那么包含该TF结合基序的peaks集合应该会表现出比预期更高的可及性。为进行准确定量，必须校正已知的技术混杂因素。chromVAR通过计算每个细胞、每个motif的**偏差Z-分数（deviation Z-score）**来实现这一点：

1.  对于每个motif $m$，首先确定基因组中所有包含该motif的peaks集合 $P_m$。
2.  对于每个细胞$c$，计算这些peaks的总标准化可及性 $S_{c,m} = \sum_{p \in P_m} \tilde{y}_{c,p}$，其中 $\tilde{y}_{c,p}$ 是经过文库大小标准化的计数值。
3.  **关键步骤**：为了获得一个无偏的[期望值](@entry_id:150961)，chromVAR为每个motif peak集合 $P_m$ 创建了多个（例如$K$个）**背景peak集合** $\{B_m^{(k)}\}$。这些背景集合中的peaks与 $P_m$ 中的peaks在**平均可及性**和**GC含量**这两个主要的技术混杂因素上进行匹配。
4.  对于每个细胞，计算其在所有背景peak集合上的总可及性 $\{S_{c,m}^{(k)}\}$。这个集合构成了在该细胞中，与motif $m$ 具有相同技术偏好的随机peak集合的“空值分布”。
5.  最后，将观测到的motif可及性 $S_{c,m}$ 与这个经验空值分布进行比较，计算一个Z-分数：
    $$
    D_{c,m} = \frac{S_{c,m} - \mathbb{E}[S_{c,m}^{(k)}]}{\text{sd}[S_{c,m}^{(k)}]} = \dfrac{S_{c,m} - \left(\dfrac{1}{K}\sum_{k=1}^{K} S_{c,m}^{(k)}\right)}{\sqrt{\dfrac{1}{K-1}\sum_{k=1}^{K}\left(S_{c,m}^{(k)} - \mathbb{E}[S_{c,m}^{(j)}]\right)^{2}}}
    $$
这个偏差分数 $D_{c,m}$ 量化了motif $m$ 在细胞$c$中的可及性超出技术背景的程度，因此可以被解释为该TF活性的一个代理指标。通过比较不同细胞簇的chromVAR偏差分数，研究人员可以识别出驱动细胞类型特异性的关键转录因子。

总之，单细胞染色质可及性分析是一个多阶段的复杂过程，它依赖于从[生物物理学](@entry_id:200723)到统计学和计算机科学的坚实原理。通过对这些原理和机制的理解，研究人员能够将稀疏、高维的测[序数](@entry_id:150084)据转化为关于细胞身份、异质性及其背后调控程序的深刻生物学洞见 [@problem_id:4314889]。