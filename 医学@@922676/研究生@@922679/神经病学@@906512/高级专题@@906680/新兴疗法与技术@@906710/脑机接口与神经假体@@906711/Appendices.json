{"hands_on_practices": [{"introduction": "在设计任何脑机接口系统时，一个首要的实际问题是处理海量神经数据流。本练习 [@problem_id:4457855] 将通过一个具体场景，帮助您掌握计算多通道记录系统原始数据速率和存储需求的基本方法。这不仅是一项基础的计算技能，更是进行系统设计、选择硬件和规划数据管理策略的关键第一步。", "problem": "在脑机接口（BCI）应用中，一个多通道细胞外记录系统使用一个具有 $96$ 个独立通道的微电极阵列（MEA）来采集神经元脉冲波形。每个通道以 $30\\,\\mathrm{kHz}$ 的恒定采样频率进行数字化，模数转换器（ADC）的分辨率为每样本 $12$ 比特。假设连续采集，所有通道同步采样，并且是原始流式传输，没有压缩、成帧、元数据或线路编码开销；假设样本按位紧密打包，因此每个样本恰好存储 $12$ 比特。\n\n从采样频率（每秒样本数）和数字系统中信息表示（每样本比特数）的基本定义出发，推导计算以下内容所需的表达式：\n1. 整个系统的原始数据速率。\n2. 录制 $1$ 小时所需的总存储空间。\n\n将原始数据速率以兆字节每秒（MB/s）表示，总存储空间以吉字节（GB）表示，使用十进制定义 $1\\,\\mathrm{MB} = 10^{6}\\,\\mathrm{bytes}$ 和 $1\\,\\mathrm{GB} = 10^{9}\\,\\mathrm{bytes}$。提供最终的数值，无需四舍五入。", "solution": "我们使用的基本定义是：\n- 采样频率 $f_{s}$ 给出每通道每秒产生的样本数。\n- 模数转换器分辨率 $n_{b}$ 给出用于表示每个样本的比特数。\n- 对于 $N_{c}$ 个独立且同步采样的通道，每秒的总样本数为 $N_{c} f_{s}$。\n- 原始比特率是每秒样本数与每样本比特数的乘积。\n\n设 $f_{s} = 30\\,\\mathrm{kHz} = 30{,}000\\,\\mathrm{s}^{-1}$，$n_{b} = 12\\,\\mathrm{bits/sample}$，以及 $N_{c} = 96$ 个通道。\n\n原始比特率 $R_{\\mathrm{bits}}$ 为\n$$\nR_{\\mathrm{bits}} = f_{s} \\times n_{b} \\times N_{c}.\n$$\n代入符号，然后代入数值：\n$$\nR_{\\mathrm{bits}} = 30{,}000 \\times 12 \\times 96 = 34{,}560{,}000 \\ \\text{bits/s}.\n$$\n\n为了转换为字节每秒，使用 $1\\,\\mathrm{byte} = 8\\,\\mathrm{bits}$：\n$$\nR_{\\mathrm{bytes}} = \\frac{R_{\\mathrm{bits}}}{8} = \\frac{34{,}560{,}000}{8} = 4{,}320{,}000 \\ \\text{bytes/s}.\n$$\n\n为了以兆字节每秒（十进制）表示原始数据速率，使用 $1\\,\\mathrm{MB} = 10^{6}\\,\\mathrm{bytes}$：\n$$\nR_{\\mathrm{MB/s}} = \\frac{R_{\\mathrm{bytes}}}{10^{6}} = \\frac{4{,}320{,}000}{1{,}000{,}000} = 4.32.\n$$\n\n接下来，计算录制时长 $T = 1\\,\\mathrm{hour} = 3600\\,\\mathrm{s}$ 的总存储量。存储的总字节数 $S_{\\mathrm{bytes}}$ 是\n$$\nS_{\\mathrm{bytes}} = R_{\\mathrm{bytes}} \\times T = 4{,}320{,}000 \\times 3{,}600 = 15{,}552{,}000{,}000 \\ \\text{bytes}.\n$$\n\n使用 $1\\,\\mathrm{GB} = 10^{9}\\,\\mathrm{bytes}$ 转换为吉字节（十进制）：\n$$\nS_{\\mathrm{GB}} = \\frac{S_{\\mathrm{bytes}}}{10^{9}} = \\frac{15{,}552{,}000{,}000}{1{,}000{,}000{,}000} = 15.552.\n$$\n\n因此，在所述假设和十进制单位定义下，原始数据速率为 $4.32\\,\\mathrm{MB/s}$，一小时所需的存储空间为 $15.552\\,\\mathrm{GB}$。", "answer": "$$\\boxed{\\begin{pmatrix}4.32  15.552\\end{pmatrix}}$$", "id": "4457855"}, {"introduction": "从信号采集转向信号输出，神经刺激是神经修复设备提供感觉反馈或治疗效果的核心。然而，向大脑施加电流必须严格遵守安全准则，以避免对娇嫩的神经组织造成伤害。本练习 [@problem_id:4457814] 将引导您计算在给定电极材料属性和脉冲参数下的最大安全刺激电流，这对于理解和设计安全有效的神经修复设备至关重要。", "problem": "一种由溅射氧化铱薄膜制成的皮层微刺激电极被用于脑机接口中，以提供体感反馈。为避免不可逆的法拉第反应和组织损伤，刺激受到材料每相可逆电荷注入极限的限制。假设使用一个矩形、恒流、阴极优先的双相脉冲，其阴极相的持续时间为 $200\\,\\mu\\text{s}$。电极的几何表面积为 $A=0.02\\,\\text{cm}^2$，制造商报告在相关波形和偏置条件下，阴极相的每相可逆电荷注入极限为 $\\sigma_{\\text{max}}=0.35\\,\\text{mC/cm}^2$。为安全起见，该设备在运行时带有一个 $10\\%$ 的安全裕度，此处定义为将操作限制在每相理论最大可逆电荷的 $90\\%$ 以内。\n\n请仅使用以下基本关系：(i) 一相中传递的电荷是电流在该相上的时间积分，以及 (ii) 每相电荷不得超过可逆面电荷极限与几何面积的乘积，计算阴极相的最大安全电流幅值。请以毫安为单位表示最终答案，并四舍五入到三位有效数字。", "solution": "目标是计算双相刺激脉冲阴极相的最大安全电流幅值 $I_{\\text{safe, max}}$。这将通过遵循电荷注入的限制条件来完成。\n\n首先，让我们用符号定义所提供的参数：\n- 阴极相持续时间：$t_c = 200\\,\\mu\\text{s}$\n- 电极几何表面积：$A = 0.02\\,\\text{cm}^2$\n- 每相可逆电荷注入极限（面电荷密度）：$\\sigma_{\\text{max}} = 0.35\\,\\text{mC/cm}^2$\n- 安全系数：$S_f = 0.90$（根据 $10\\%$ 的安全裕度，代表在最大极限的 $90\\%$ 下运行）。\n\n该问题遵循两个明确陈述的基本关系。\n\n首先，单相期间可注入的理论最大可逆电荷 $Q_{\\text{max, theoretical}}$ 是材料的单位面积电荷注入极限 $\\sigma_{\\text{max}}$ 与电极几何表面积 $A$ 的乘积。\n$$Q_{\\text{max, theoretical}} = \\sigma_{\\text{max}} \\times A$$\n\n其次，为安全起见，操作电荷不得超过此理论最大值的特定部分。因此，每相最大安全电荷 $Q_{\\text{safe}}$ 是通过应用安全系数 $S_f$ 来确定的。\n$$Q_{\\text{safe}} = S_f \\times Q_{\\text{max, theoretical}} = S_f \\times \\sigma_{\\text{max}} \\times A$$\n\n第三，在持续时间 $t$ 内由恒定电流 $I$ 传递的总电荷 $Q$ 是电流和持续时间的乘积。这是将恒定电流在时间间隔上积分的结果。对于阴极相，传递的电荷为：\n$$Q_c = I_c \\times t_c$$\n其中 $I_c$ 是阴极相期间的电流幅值。\n\n为求得最大安全电流幅值 $I_{\\text{safe, max}}$，我们将阴极相期间传递的电荷设置为等于最大安全电荷极限 $Q_{\\text{safe}}$。\n$$I_{\\text{safe, max}} \\times t_c = Q_{\\text{safe}}$$\n\n代入 $Q_{\\text{safe}}$ 的表达式可得：\n$$I_{\\text{safe, max}} \\times t_c = S_f \\times \\sigma_{\\text{max}} \\times A$$\n\n现在我们可以解出 $I_{\\text{safe, max}}$：\n$$I_{\\text{safe, max}} = \\frac{S_f \\times \\sigma_{\\text{max}} \\times A}{t_c}$$\n\n现在，我们将给定的数值代入此方程。保持单位一致性至关重要。我们将首先以毫库仑（$ \\text{mC} $）为单位计算 $Q_{\\text{safe}}$。\n$$Q_{\\text{safe}} = 0.90 \\times (0.35\\,\\frac{\\text{mC}}{\\text{cm}^2}) \\times (0.02\\,\\text{cm}^2)$$\n$$Q_{\\text{safe}} = 0.90 \\times 0.007\\,\\text{mC}$$\n$$Q_{\\text{safe}} = 0.0063\\,\\text{mC}$$\n\n接下来，我们计算 $I_{\\text{safe, max}}$。电荷单位为毫库仑（$ \\text{mC} $），持续时间单位为微秒（$ \\mu\\text{s} $）。谨慎的做法是先将这些单位转换为国际单位制基本单位（库仑和秒），以安培为单位求出电流，最后再按要求转换为毫安。\n$Q_{\\text{safe}} = 0.0063\\,\\text{mC} = 0.0063 \\times 10^{-3}\\,\\text{C} = 6.3 \\times 10^{-6}\\,\\text{C}$\n$t_c = 200\\,\\mu\\text{s} = 200 \\times 10^{-6}\\,\\text{s} = 2.0 \\times 10^{-4}\\,\\text{s}$\n\n现在，我们计算以安培（$ \\text{A} $）为单位的电流：\n$$I_{\\text{safe, max}} = \\frac{Q_{\\text{safe}}}{t_c} = \\frac{6.3 \\times 10^{-6}\\,\\text{C}}{2.0 \\times 10^{-4}\\,\\text{s}}$$\n$$I_{\\text{safe, max}} = 3.15 \\times 10^{-2}\\,\\text{A}$$\n$$I_{\\text{safe, max}} = 0.0315\\,\\text{A}$$\n\n问题要求最终答案以毫安（$ \\text{mA} $）表示，并四舍五入到三位有效数字。\n为了从安培转换为毫安，我们乘以 $1000$：\n$$I_{\\text{safe, max}} = 0.0315\\,\\text{A} \\times \\frac{1000\\,\\text{mA}}{1\\,\\text{A}} = 31.5\\,\\text{mA}$$\n\n数值 $31.5$ 恰好有三位有效数字（$3$、$1$ 和 $5$）。因此，无需进一步四舍五入。\n阴极相的最大安全电流幅值为 $31.5\\,\\text{mA}$。", "answer": "$$\n\\boxed{31.5}\n$$", "id": "4457814"}, {"introduction": "脑机接口的核心挑战之一是构建能够准确从高维神经活动中“解码”出用户意图的算法。本练习 [@problem_id:5002219] 深入探讨了岭回归（Ridge Regression），这是一种用于处理此类解码问题的强大正则化技术。通过推导其解析解并分析其样本外预测误差，您将深刻理解偏差-方差权衡（Bias-Variance Tradeoff）如何通过正则化参数 $\\lambda$ 进行调控，这是构建稳健且泛化能力强的神经解码器的理论基石。", "problem": "一个实验室正在开发一种脑机接口（BCI）解码器，该解码器在中心向外伸手任务中，将神经元放电率映射到手的标量切向速度。在每个宽度为 $\\Delta t$、由 $t \\in \\{1,\\ldots,N\\}$ 索引的时间窗内，预处理后的放电率向量为 $\\mathbf{r}_{t} \\in \\mathbb{R}^{p}$（已在神经元间进行z-score标准化和白化处理，因此样本协方差为单位矩阵），同时测得的手部速度为 $v_{t} \\in \\mathbb{R}$。假设一个线性高斯编码模型 $v_{t} = \\mathbf{r}_{t}^{\\top} \\boldsymbol{\\beta} + \\varepsilon_{t}$，其中 $\\varepsilon_{t} \\sim \\mathcal{N}(0,\\sigma^{2})$ 在时间 $t$ 上独立，并且独立于 $\\mathbf{r}_{t}$。将数据堆叠成设计矩阵 $\\mathbf{X} \\in \\mathbb{R}^{N \\times p}$（其行向量为 $\\mathbf{r}_{t}^{\\top}$）和目标向量 $\\mathbf{y} \\in \\mathbb{R}^{N}$（其元素为 $v_{t}$）。由于神经元特征是使用训练数据进行白化的，您可以假设经验二阶矩满足 $\\mathbf{X}^{\\top}\\mathbf{X} = N \\mathbf{I}_{p}$。\n\n您通过最小化惩罚最小二乘目标函数来训练一个岭回归解码器\n$$\nJ(\\mathbf{w};\\lambda) = \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^{2} + \\lambda \\|\\mathbf{w}\\|^{2},\n$$\n其中 $\\lambda \\ge 0$ 是正则化参数，$\\|\\cdot\\|$ 表示欧几里得范数。\n\n任务：\n1) 从模型 $ \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$（其中 $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2} \\mathbf{I}_{N})$）出发，推导最小化 $J(\\mathbf{w};\\lambda)$ 的闭式解 $\\widehat{\\mathbf{w}}$。\n\n2) 对于一个从相同分布中抽取的新的、独立的测试样本 $(\\mathbf{r}_{\\mathrm{new}}, v_{\\mathrm{new}})$，其中 $\\mathbf{r}_{\\mathrm{new}}$ 独立于训练数据且满足 $\\mathbb{E}[\\mathbf{r}_{\\mathrm{new}}] = \\mathbf{0}$ 和 $\\mathbb{E}[\\mathbf{r}_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top}] = \\mathbf{I}_{p}$，推导期望的样本外均方预测误差\n$$\n\\mathcal{E}(\\lambda) = \\mathbb{E}\\big[(v_{\\mathrm{new}} - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}})^{2}\\big]\n$$\n将其表示为 $\\lambda$、$N$、$p$、$\\sigma^{2}$ 和 $\\boldsymbol{\\beta}$ 的显式函数。您的推导必须从上述定义以及关于 $\\mathbf{X}$ 和 $\\mathbf{r}_{\\mathrm{new}}$ 的假设出发，并且必须揭示依赖于 $\\lambda$ 的偏差-方差分解。\n\n3) 为了使权衡关系明确且不依赖于某个特定的未知 $\\boldsymbol{\\beta}$，假设一个与神经群体编码一致的层级先验：$\\boldsymbol{\\beta} \\sim \\mathcal{N}(\\mathbf{0}, \\tau^{2}\\mathbf{I}_{p})$，其中 $\\tau^{2} > 0$。在此先验上对您得到的 $\\mathcal{E}(\\lambda)$ 表达式进行平均，并将其简化为一个关于 $\\lambda$、$N$、$p$、$\\sigma^{2}$ 和 $\\tau^{2}$ 的标量函数。\n\n4) 使用您平均后的表达式，确定使期望的样本外均方预测误差最小化的值 $\\lambda^{\\star}$。然后，对下列数值进行数值计算以求得该最优值：\n- $p = 100$，\n- $N = 10000$，\n- $\\sigma^{2} = 0.04$，\n- $\\tau^{2} = 0.01$。\n将 $\\lambda^{\\star}$ 的最终值表示为一个无单位的纯数。如果需要四舍五入，请保留四位有效数字。如果不需要，请提供精确值。", "solution": "该问题要求在脑机接口（BCI）的背景下，对岭回归解码器进行多步分析。分析内容包括推导解码器、其样本外误差，以及在特定数据模型和先验下的最优正则化参数。\n\n### 任务1：岭回归估计量 $\\widehat{\\mathbf{w}}$ 的推导\n岭回归估计量 $\\widehat{\\mathbf{w}}$ 是通过最小化目标函数得到的\n$$\nJ(\\mathbf{w};\\lambda) = \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^{2} + \\lambda \\|\\mathbf{w}\\|^{2}\n$$\n其中 $\\|\\cdot\\|$ 是欧几里得范数。我们可以用向量转置来表示平方范数：\n$$\nJ(\\mathbf{w};\\lambda) = (\\mathbf{y} - \\mathbf{X}\\mathbf{w})^{\\top}(\\mathbf{y} - \\mathbf{X}\\mathbf{w}) + \\lambda \\mathbf{w}^{\\top}\\mathbf{w}\n$$\n展开第一项得到：\n$$\nJ(\\mathbf{w};\\lambda) = \\mathbf{y}^{\\top}\\mathbf{y} - \\mathbf{y}^{\\top}\\mathbf{X}\\mathbf{w} - \\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf{y} + \\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf{X}\\mathbf{w} + \\lambda \\mathbf{w}^{\\top}\\mathbf{w}\n$$\n由于 $\\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf{y}$ 是一个标量，它等于其转置 $\\mathbf{y}^{\\top}\\mathbf{X}\\mathbf{w}$。因此，我们可以合并交叉项：\n$$\nJ(\\mathbf{w};\\lambda) = \\mathbf{y}^{\\top}\\mathbf{y} - 2\\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf{y} + \\mathbf{w}^{\\top}(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})\\mathbf{w}\n$$\n为了找到最小值，我们对 $J(\\mathbf{w};\\lambda)$ 关于 $\\mathbf{w}$ 求梯度，并令其为零。使用标准矩阵微积分结果（对于对称矩阵 $\\mathbf{M}$，有 $\\nabla_{\\mathbf{w}} \\mathbf{w}^{\\top}\\mathbf{a} = \\mathbf{a}$ 和 $\\nabla_{\\mathbf{w}} \\mathbf{w}^{\\top}\\mathbf{M}\\mathbf{w} = 2\\mathbf{M}\\mathbf{w}$）：\n$$\n\\nabla_{\\mathbf{w}} J(\\mathbf{w};\\lambda) = -2\\mathbf{X}^{\\top}\\mathbf{y} + 2(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})\\mathbf{w}\n$$\n将梯度设为零向量，得到解 $\\widehat{\\mathbf{w}}$：\n$$\n-2\\mathbf{X}^{\\top}\\mathbf{y} + 2(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})\\widehat{\\mathbf{w}} = \\mathbf{0}\n$$\n$$\n(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})\\widehat{\\mathbf{w}} = \\mathbf{X}^{\\top}\\mathbf{y}\n$$\n形式解为 $\\widehat{\\mathbf{w}} = (\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})^{-1}\\mathbf{X}^{\\top}\\mathbf{y}$。问题陈述了经验二阶矩为 $\\mathbf{X}^{\\top}\\mathbf{X} = N\\mathbf{I}_{p}$ 的假设。将此代入 $\\widehat{\\mathbf{w}}$ 的表达式中：\n$$\n\\widehat{\\mathbf{w}} = (N\\mathbf{I}_{p} + \\lambda\\mathbf{I}_{p})^{-1}\\mathbf{X}^{\\top}\\mathbf{y} = ((N+\\lambda)\\mathbf{I}_{p})^{-1}\\mathbf{X}^{\\top}\\mathbf{y}\n$$\n$$\n\\widehat{\\mathbf{w}} = \\frac{1}{N+\\lambda}\\mathbf{I}_{p}^{-1}\\mathbf{X}^{\\top}\\mathbf{y} = \\frac{1}{N+\\lambda}\\mathbf{X}^{\\top}\\mathbf{y}\n$$\n这是在给定假设下 $\\widehat{\\mathbf{w}}$ 的闭式解。\n\n### 任务2：期望的样本外均方预测误差\n我们被要求推导 $\\mathcal{E}(\\lambda) = \\mathbb{E}\\big[(v_{\\mathrm{new}} - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}})^{2}\\big]$。期望是针对训练数据中的随机性（这使得 $\\widehat{\\mathbf{w}}$ 成为随机的）和新的测试样本 $(\\mathbf{r}_{\\mathrm{new}}, v_{\\mathrm{new}})$ 计算的。\n新样本的真实模型是 $v_{\\mathrm{new}} = \\mathbf{r}_{\\mathrm{new}}^{\\top}\\boldsymbol{\\beta} + \\varepsilon_{\\mathrm{new}}$，其中 $\\varepsilon_{\\mathrm{new}} \\sim \\mathcal{N}(0,\\sigma^{2})$。将此代入误差项中：\n$$\nv_{\\mathrm{new}} - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}} = (\\mathbf{r}_{\\mathrm{new}}^{\\top}\\boldsymbol{\\beta} + \\varepsilon_{\\mathrm{new}}) - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}} = \\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}) + \\varepsilon_{\\mathrm{new}}\n$$\n对该表达式求平方：\n$$\n(v_{\\mathrm{new}} - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}})^{2} = (\\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}))^{2} + 2\\varepsilon_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}) + \\varepsilon_{\\mathrm{new}}^{2}\n$$\n现在我们取期望。新的噪声项 $\\varepsilon_{\\mathrm{new}}$ 独立于训练数据（因此也独立于 $\\widehat{\\mathbf{w}}$）和新的特征 $\\mathbf{r}_{\\mathrm{new}}$。由于 $\\mathbb{E}[\\varepsilon_{\\mathrm{new}}] = 0$，交叉项消失。我们有 $\\mathbb{E}[\\varepsilon_{\\mathrm{new}}^{2}] = \\sigma^{2}$。\n$$\n\\mathcal{E}(\\lambda) = \\mathbb{E}\\left[(\\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}))^{2}\\right] + \\sigma^{2}\n$$\n剩余的期望是关于 $\\mathbf{r}_{\\mathrm{new}}$ 和 $\\widehat{\\mathbf{w}}$ 的。我们可以使用迹技巧重写期望内的项：$(\\mathbf{a}^{\\top}\\mathbf{b})^2 = \\mathbf{b}^{\\top}\\mathbf{a}\\mathbf{a}^{\\top}\\mathbf{b} = \\mathrm{tr}(\\mathbf{b}^{\\top}\\mathbf{a}\\mathbf{a}^{\\top}\\mathbf{b}) = \\mathrm{tr}(\\mathbf{a}\\mathbf{a}^{\\top}\\mathbf{b}\\mathbf{b}^{\\top})$。使用 $\\mathbb{E}[x^2] = \\mathbb{E}[\\mathrm{tr}(x^2)]$（其中 $x$ 是标量）更为简单。\n$$\n\\mathbb{E}\\left[(\\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}))^{2}\\right] = \\mathbb{E}\\left[\\mathrm{tr}\\left((\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})^{\\top} \\mathbf{r}_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top} (\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})\\right)\\right]\n$$\n根据迹和期望的线性性质，并且由于 $\\widehat{\\mathbf{w}}$（来自训练数据）独立于 $\\mathbf{r}_{\\mathrm{new}}$：\n$$\n= \\mathrm{tr}\\left(\\mathbb{E}\\left[(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})^{\\top} \\mathbb{E}[\\mathbf{r}_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top}] (\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})\\right]\\right)\n$$\n使用假设 $\\mathbb{E}[\\mathbf{r}_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top}] = \\mathbf{I}_{p}$：\n$$\n= \\mathrm{tr}\\left(\\mathbb{E}\\left[(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})^{\\top}\\mathbf{I}_{p}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})\\right]\\right) = \\mathbb{E}\\left[\\|\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}\\|^{2}\\right]\n$$\n所以，$\\mathcal{E}(\\lambda) = \\mathbb{E}[\\|\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}\\|^{2}] + \\sigma^2$。期望 $\\mathbb{E}[\\cdot]$ 现在只针对训练数据的随机性。我们现在进行偏差-方差分解：\n$$\n\\mathbb{E}\\left[\\|\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}\\|^{2}\\right] = \\left\\|\\boldsymbol{\\beta} - \\mathbb{E}[\\widehat{\\mathbf{w}}]\\right\\|^{2} + \\mathbb{E}\\left[\\|\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}]\\|^{2}\\right] = \\text{Bias}(\\widehat{\\mathbf{w}})^{2} + \\text{Var}(\\widehat{\\mathbf{w}})\n$$\n我们推导偏差和方差项。首先，将 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$ 代入 $\\widehat{\\mathbf{w}}$ 的表达式中：\n$$\n\\widehat{\\mathbf{w}} = \\frac{1}{N+\\lambda}\\mathbf{X}^{\\top}(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}) = \\frac{1}{N+\\lambda}(\\mathbf{X}^{\\top}\\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon}) = \\frac{1}{N+\\lambda}(N\\boldsymbol{\\beta} + \\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon})\n$$\n$\\widehat{\\mathbf{w}}$ 的期望（对 $\\boldsymbol{\\varepsilon}$ 取期望）是：\n$$\n\\mathbb{E}[\\widehat{\\mathbf{w}}] = \\mathbb{E}\\left[\\frac{1}{N+\\lambda}(N\\boldsymbol{\\beta} + \\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon})\\right] = \\frac{1}{N+\\lambda}(N\\boldsymbol{\\beta} + \\mathbf{X}^{\\top}\\mathbb{E}[\\boldsymbol{\\varepsilon}]) = \\frac{N}{N+\\lambda}\\boldsymbol{\\beta}\n$$\n偏差的平方是：\n$$\n\\text{Bias}(\\widehat{\\mathbf{w}})^{2} = \\left\\|\\mathbb{E}[\\widehat{\\mathbf{w}}] - \\boldsymbol{\\beta}\\right\\|^{2} = \\left\\|\\frac{N}{N+\\lambda}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}\\right\\|^{2} = \\left\\|-\\frac{\\lambda}{N+\\lambda}\\boldsymbol{\\beta}\\right\\|^{2} = \\left(\\frac{\\lambda}{N+\\lambda}\\right)^{2}\\|\\boldsymbol{\\beta}\\|^{2}\n$$\n方差项是 $\\mathbb{E}[\\|\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}]\\|^2] = \\mathrm{tr}(\\mathrm{Cov}(\\widehat{\\mathbf{w}}))$。\n$$\n\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}] = \\frac{1}{N+\\lambda}\\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon}\n$$\n$$\n\\mathrm{Cov}(\\widehat{\\mathbf{w}}) = \\mathbb{E}\\left[(\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}])(\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}])^{\\top}\\right] = \\frac{1}{(N+\\lambda)^2}\\mathbb{E}\\left[\\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\top}\\mathbf{X}\\right] = \\frac{1}{(N+\\lambda)^2}\\mathbf{X}^{\\top}\\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\top}]\\mathbf{X}\n$$\n使用 $\\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\top}]=\\sigma^2\\mathbf{I}_N$ 和 $\\mathbf{X}^{\\top}\\mathbf{X}=N\\mathbf{I}_p$：\n$$\n\\mathrm{Cov}(\\widehat{\\mathbf{w}}) = \\frac{\\sigma^2}{(N+\\lambda)^2}\\mathbf{X}^{\\top}\\mathbf{I}_{N}\\mathbf{X} = \\frac{\\sigma^2}{(N+\\lambda)^2}\\mathbf{X}^{\\top}\\mathbf{X} = \\frac{N\\sigma^2}{(N+\\lambda)^2}\\mathbf{I}_{p}\n$$\n方差是该协方差矩阵的迹：\n$$\n\\text{Var}(\\widehat{\\mathbf{w}}) = \\mathrm{tr}(\\mathrm{Cov}(\\widehat{\\mathbf{w}})) = \\mathrm{tr}\\left(\\frac{N\\sigma^2}{(N+\\lambda)^2}\\mathbf{I}_{p}\\right) = \\frac{pN\\sigma^2}{(N+\\lambda)^2}\n$$\n合并所有项，期望的样本外误差是：\n$$\n\\mathcal{E}(\\lambda) = \\left(\\frac{\\lambda}{N+\\lambda}\\right)^{2}\\|\\boldsymbol{\\beta}\\|^{2} + \\frac{pN\\sigma^2}{(N+\\lambda)^2} + \\sigma^{2}\n$$\n\n### 任务3：对 $\\boldsymbol{\\beta}$ 的先验进行平均\n我们被给予了真实权重的一个先验分布，$\\boldsymbol{\\beta} \\sim \\mathcal{N}(\\mathbf{0}, \\tau^{2}\\mathbf{I}_{p})$。我们需要计算 $\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\mathbb{E}_{\\boldsymbol{\\beta}}[\\mathcal{E}(\\lambda)]$。$\\mathcal{E}(\\lambda)$ 中唯一依赖于 $\\boldsymbol{\\beta}$ 的项是 $\\|\\boldsymbol{\\beta}\\|^{2}$。我们计算其在先验下的期望：\n$$\n\\mathbb{E}_{\\boldsymbol{\\beta}}\\left[\\|\\boldsymbol{\\beta}\\|^{2}\\right] = \\mathbb{E}_{\\boldsymbol{\\beta}}\\left[\\sum_{i=1}^{p}\\beta_{i}^{2}\\right] = \\sum_{i=1}^{p}\\mathbb{E}_{\\boldsymbol{\\beta}}[\\beta_{i}^{2}]\n$$\n对于每个分量 $\\beta_i \\sim \\mathcal{N}(0, \\tau^{2})$，其二阶矩为 $\\mathbb{E}[\\beta_{i}^{2}] = \\mathrm{Var}(\\beta_i) + (\\mathbb{E}[\\beta_i])^{2} = \\tau^{2} + 0^{2} = \\tau^{2}$。\n因此，期望的平方范数是：\n$$\n\\mathbb{E}_{\\boldsymbol{\\beta}}\\left[\\|\\boldsymbol{\\beta}\\|^{2}\\right] = \\sum_{i=1}^{p}\\tau^{2} = p\\tau^{2}\n$$\n将此代入 $\\mathcal{E}(\\lambda)$ 的表达式中：\n$$\n\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\left(\\frac{\\lambda}{N+\\lambda}\\right)^{2}p\\tau^{2} + \\frac{pN\\sigma^2}{(N+\\lambda)^2} + \\sigma^{2}\n$$\n$$\n\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\frac{p\\lambda^{2}\\tau^{2} + pN\\sigma^2}{(N+\\lambda)^2} + \\sigma^{2}\n$$\n\n### 任务4：最优正则化参数 $\\lambda^{\\star}$\n为了找到最小化 $\\mathcal{E}_{\\mathrm{avg}}(\\lambda)$ 的 $\\lambda$，我们对其关于 $\\lambda$ 求导并将导数设为零。在最小化过程中可以忽略常数项 $\\sigma^{2}$。\n$$\n\\frac{d}{d\\lambda}\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\frac{d}{d\\lambda}\\left( \\frac{p(\\lambda^{2}\\tau^{2} + N\\sigma^2)}{(N+\\lambda)^2} \\right)\n$$\n使用商法则 $\\frac{d}{dx}(\\frac{u}{v}) = \\frac{u'v - uv'}{v^2}$：\n设 $u(\\lambda) = p(\\lambda^{2}\\tau^{2} + N\\sigma^2)$ 且 $v(\\lambda) = (N+\\lambda)^2$。\n那么 $u'(\\lambda) = 2p\\lambda\\tau^{2}$ 且 $v'(\\lambda) = 2(N+\\lambda)$。\n$$\n\\frac{d}{d\\lambda}\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\frac{(2p\\lambda\\tau^{2})(N+\\lambda)^2 - p(\\lambda^{2}\\tau^{2} + N\\sigma^2) \\cdot 2(N+\\lambda)}{(N+\\lambda)^4}\n$$\n将导数设为零，并假设 $\\lambda \\ge 0, N \\ge 1$，我们可以通过除以非零因子 $2p(N+\\lambda)$ 来简化：\n$$\n(\\lambda\\tau^{2})(N+\\lambda) - (\\lambda^{2}\\tau^{2} + N\\sigma^2) = 0\n$$\n$$\nN\\lambda\\tau^{2} + \\lambda^{2}\\tau^{2} - \\lambda^{2}\\tau^{2} - N\\sigma^2 = 0\n$$\n$$\nN\\lambda\\tau^{2} = N\\sigma^2\n$$\n由于 $N \\ge 1$ 且 $\\tau^{2} > 0$（给定），我们可以除以 $N\\tau^{2}$ 来找到最优的 $\\lambda^{\\star}$：\n$$\n\\lambda^{\\star} = \\frac{\\sigma^2}{\\tau^2}\n$$\n这个结果异常简洁，它表示测量中的噪声方差与模型参数的先验方差之比。可以检查二阶导数以确认这是一个最小值。导数的分子简化为 $2pN(\\lambda\\tau^2 - \\sigma^2)(N+\\lambda)$，当 $\\lambda  < \\sigma^2/\\tau^2$ 时为负，当 $\\lambda > \\sigma^2/\\tau^2$ 时为正，从而证实了这是一个最小值。\n\n最后，我们使用给定的值对这个表达式进行数值计算：\n- $\\sigma^{2} = 0.04$\n- $\\tau^{2} = 0.01$\n在这种理想化的设置下，找到 $\\lambda^{\\star}$ 不需要 $p$ 和 $N$ 的值。\n$$\n\\lambda^{\\star} = \\frac{0.04}{0.01} = 4\n$$\n最优值恰好是 $4$。", "answer": "$$\n\\boxed{4}\n$$", "id": "5002219"}]}