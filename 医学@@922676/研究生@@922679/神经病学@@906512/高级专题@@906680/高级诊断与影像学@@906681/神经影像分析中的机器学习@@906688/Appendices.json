{"hands_on_practices": [{"introduction": "来自不同扫描仪和研究的神经影像数据通常具有不同的空间方向。为了进行群体分析或训练机器学习模型，首先将所有图像转换到一个通用的标准坐标系中至关重要。本实践将引导您了解 NIfTI 仿射变换的基本原理，使您能够编写一个程序来自动标准化图像方向，这是任何稳健的神经影像分析流程中关键的第一步。[@problem_id:4491629]", "problem": "给定一小组旨在模拟不同轴顺序和翻转下，真实的 Neuroimaging Informatics Technology Initiative (NIfTI) 图像方向的合成神经影像头文件仿射矩阵。在 NIfTI 中，体素坐标 $\\mathbf{i} = (i_x, i_y, i_z)$ 通过一个 $4\\times 4$ 仿射矩阵 $A\\in\\mathbb{R}^{4\\times 4}$ 和齐次坐标映射到扫描仪/世界坐标 $\\mathbf{x} = (x, y, z)$，目标是遵循 Right-Anterior-Superior (RAS) 约定。左上角的 $3\\times 3$ 块 $R\\in\\mathbb{R}^{3\\times 3}$ 编码旋转、缩放和可能的剪切，最后一列编码平移。世界轴定义如下：$+x$ 朝右方增加，$+y$ 朝前方增加，$+z$ 朝上方增加。物理长度单位为毫米 (mm)。\n\n从线性映射的基本原理出发，确定一个规范方向，该方向对体素轴进行重排序和翻转，使得变换后，新的 $3\\times 3$ 块 $R_{\\mathrm{can}}$ 的列在指定的容差范围内，按顺序与世界坐标正半轴 $(+x,+y,+z)$ 对齐。这需要：\n- 检查 $R$ 的列；$R$ 的第 $j$ 列描述了体素轴 $j$ 对世界坐标的贡献。\n- 对于每个体素轴 $j\\in\\{0,1,2\\}$，识别具有最大绝对贡献的世界轴索引 $k\\in\\{0,1,2\\}$（即最大化 $\\lvert R_{k,j}\\rvert$ 的索引 $k$），以及表示方向性的符号 $\\operatorname{sign}(R_{k,j})\\in\\{-1,+1\\}$（正号表示与 $+x,+y,+z$ 对齐，负号表示与 $-x,-y,-z$ 对齐）。\n- 构建一个规范化变换 $S\\in\\mathbb{R}^{4\\times 4}$，其左上角 $3\\times 3$ 部分对体素轴进行排列和翻转，使得 $R_{\\mathrm{can}} = R S_{3\\times 3}$ 的列分别对应于 $(+x,+y,+z)$。使用 $A_{\\mathrm{can}} = A S$ 进行完整的头文件变换；由于缺少图像维度信息，平移项无需调整，并且对齐验证应仅考虑 $R_{\\mathrm{can}}$。\n\n验证规则：规范化后，对于 $R_{\\mathrm{can}}$ 的每一列 $i\\in\\{0,1,2\\}$，\n- 最大幅值分量必须在第 $i$ 行（即世界轴匹配），并且\n- 对角线元素 $R_{\\mathrm{can}}[i,i]$ 必须为严格正值，\n- 所有列的最大非对角线幅值，通过最大对角线幅值进行归一化后，不得超过一个容差 $\\tau$（设 $\\tau=0.25$）。\n\n如果从体素轴到世界轴的初始映射不是双射（例如，重复分配或零长度列），则将该情况视为无效映射，并报告验证为假，但仍通过贪婪地将每个世界轴 $i$ 分配给最大化 $\\lvert R_{i,j}\\rvert$ 的未分配体素轴 $j$，并根据 $R_{i,j}$ 的符号进行翻转，来计算一个尽力而为的规范化，以产生一个确定性输出。\n\n在 Python 中实现一个程序，为每个测试用例的仿射矩阵计算：\n- 排列列表 $\\mathrm{perm} = [p_0,p_1,p_2]$，其中 $p_i$ 是规范化后分配给世界轴 $i$ 的原始体素轴索引（即，$R_{\\mathrm{can}}$ 的第 $i$ 列源自 $R$ 的第 $p_i$ 列）。\n- 翻转列表 $\\mathrm{flips} = [f_0,f_1,f_2]$，其中 $f_i\\in\\{0,1\\}$，$f_i=1$ 表示轴 $i$ 必须反转以与世界坐标正半轴对齐。\n- 布尔值 $\\mathrm{ok}$，指示验证是否根据规则成功。\n- 浮点数 $\\mathrm{max\\_offdiag}$，等于 $R_{\\mathrm{can}}$ 中的最大绝对非对角线元素除以所有三列中的最大对角线幅值。\n\n测试套件（所有条目单位为 mm；角度由 $R$ 的线性分量隐含）：\n1. 理想情况（已是 RAS，单位体素大小）：\n   $$A_0 = \\begin{bmatrix}\n   1  0  0  0 \\\\\n   0  1  0  0 \\\\\n   0  0  1  0 \\\\\n   0  0  0  1\n   \\end{bmatrix}.$$\n2. 轴交换，带一次翻转和非均匀体素大小（包含平移，但验证仅使用 $R$）：\n   $$A_1 = \\begin{bmatrix}\n   0  1  0  10 \\\\\n   -2  0  0  -5 \\\\\n   0  0  3  2 \\\\\n   0  0  0  1\n   \\end{bmatrix}.$$\n3. 小剪切，带一个轴反转：\n   $$A_2 = \\begin{bmatrix}\n   1.0  0.0  0.01  0 \\\\\n   0.05  1.0  0.0  0 \\\\\n   0.0  0.02  -1.0  0 \\\\\n   0  0  0  1\n   \\end{bmatrix}.$$\n4. 退化情况，带重复的主导世界轴分配（无效映射）：\n   $$A_3 = \\begin{bmatrix}\n   1.0  0.8  0.0  3 \\\\\n   0.0  0.1  0.0  4 \\\\\n   0.0  0.0  1.0  5 \\\\\n   0  0  0  1\n   \\end{bmatrix}.$$\n\n算法要求：\n- 只应使用针对 $R$ 的线性代数；不要依赖外部神经影像库。\n- 使用一个小的数值阈值 $\\varepsilon = 10^{-8}$ 来检测零长度列和进行正性检查。\n- 在验证规则中使用容差 $\\tau = 0.25$。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含一个逗号分隔的列表，列表本身包含在方括号内。每个结果本身都是一个形如 $[\\mathrm{perm},\\mathrm{flips},\\mathrm{ok},\\mathrm{max\\_offdiag}]$ 的列表，其中 $\\mathrm{perm}$ 和 $\\mathrm{flips}$ 是整数列表，$\\mathrm{ok}$ 是布尔值，$\\mathrm{max\\_offdiag}$ 是浮点数。例如，输出必须看起来像：\n$$[\\,[ [p_0,p_1,p_2],[f_0,f_1,f_2],\\mathrm{ok},m_0 ],\\,\\ldots\\,]$$\n每个测试用例对应一个这样的子列表，顺序与测试套件相同。", "solution": "我们从 Neuroimaging Informatics Technology Initiative (NIfTI) 中仿射变换的定义开始。体素坐标 $\\mathbf{i} = (i_x, i_y, i_z)$ 使用齐次坐标映射到世界坐标 $\\mathbf{x} = (x, y, z)$：\n$$\n\\begin{bmatrix}\nx \\\\ y \\\\ z \\\\ 1\n\\end{bmatrix}\n=\nA\n\\begin{bmatrix}\ni_x \\\\ i_y \\\\ i_z \\\\ 1\n\\end{bmatrix},\n\\quad\nA =\n\\begin{bmatrix}\nR  \\mathbf{t} \\\\\n\\mathbf{0}^\\top  1\n\\end{bmatrix},\n$$\n其中 $R\\in\\mathbb{R}^{3\\times 3}$ 编码旋转、缩放和剪切，$\\mathbf{t}\\in\\mathbb{R}^3$ 编码平移。Right-Anterior-Superior (RAS) 约定将世界坐标正半轴定义为：$+x$ 为右，$+y$ 为前，$+z$ 为上。\n\n根据线性代数， $R$ 的第 $j$ 列给出了沿第 $j$ 个体素轴单位步长对世界坐标向量的贡献。为了确定体素轴 $j$ 的主导世界轴，我们找到使 $\\lvert R_{k,j}\\rvert$ 最大化的索引 $k$。$R_{k,j}$ 的符号指示了体素轴 $j$ 是与世界轴 $k$ 的正方向对齐（符号 $+1$）还是负方向对齐（符号 $-1$）。对每个 $j\\in\\{0,1,2\\}$ 执行此过程，可以得到一个从体素轴到世界轴的映射，以及方向性（如果为负则翻转）。\n\n一个一致的规范方向旨在重排和翻转体素轴，使得在变换后的仿射矩阵 $A_{\\mathrm{can}} = A S$ 中，其左上角 $3\\times 3$ 块 $R_{\\mathrm{can}}$ 的列分别与 $(+x,+y,+z)$ 对齐。变换矩阵 $S\\in\\mathbb{R}^{4\\times 4}$ 的形式为：\n$$\nS =\n\\begin{bmatrix}\nS_{3\\times 3}  \\mathbf{0} \\\\\n\\mathbf{0}^\\top  1\n\\end{bmatrix},\n$$\n其中 $S_{3\\times 3}$ 是一个列选择器和符号翻转矩阵。具体来说，$S_{3\\times 3}$ 的第 $i$ 列是 $\\pm \\mathbf{e}_{p_i}$，其中 $\\mathbf{e}_{p_i}$ 是第 $p_i$ 个标准基向量（选择原始体素轴 $p_i$），如果 $R_{i,p_i} > 0$ 则符号为正，否则为负。这种构造确保：\n$$\nR_{\\mathrm{can}} = R S_{3\\times 3}, \\quad \\text{and} \\quad R_{\\mathrm{can}}[:,i] = \\operatorname{sign}(R_{i,p_i})\\, R[:,p_i].\n$$\n因此，$R_{\\mathrm{can}}$ 的列的最大幅值分量位于相应的对角线元素上，并且是正值。\n\n验证条件源于几何对齐要求：\n- 对于每一列 $i$，最大幅值分量位于第 $i$ 行（即 $\\arg\\max_{k\\in\\{0,1,2\\}} \\lvert R_{\\mathrm{can}}[k,i]\\rvert = i$）。这确保了正确的轴匹配。\n- 对角线元素 $R_{\\mathrm{can}}[i,i]$ 严格为正，确保与世界坐标正半轴对齐。\n- 剪切或离轴污染必须有界；我们测量所有列中的最大非对角线幅值，并用最大对角线幅值进行归一化（一种尺度不变的度量）。如果这个归一化的最大值最多为容差 $\\tau$，则认为对齐对于规范化是可接受的。我们设置 $\\tau = 0.25$ 以允许典型的重采样或次要头文件不一致性引起的小剪切。\n\n如果从体素轴到世界轴的初始映射不是双射（例如，两个体素轴都主导同一个世界轴，或零长度列表示没有贡献），则规范化是不可靠的。我们仍然通过贪婪地为每个世界轴 $i$ 分配能最大化 $\\lvert R_{i,j}\\rvert$ 的未分配体素轴 $j$ ，并根据 $\\operatorname{sign}(R_{i,j})$ 进行翻转，来生成一个确定性的尽力而为的 $S$ 。在这种情况下，验证标志 $\\mathrm{ok}$ 被设置为 false。\n\n我们现在将此过程应用于测试套件：\n\n1. 情况 $A_0$：\n   $R = I_3$。对于 $j=0,1,2$，主导世界索引分别为 $0,1,2$，所有符号均为正。因此 $p = [0,1,2]$，flips $= [0,0,0]$，并且 $S_{3\\times 3} = I_3$。那么 $R_{\\mathrm{can}} = I_3$，非对角线元素为零，所以 $\\mathrm{max\\_offdiag} = 0$，并且 $\\mathrm{ok} = \\text{True}$。\n\n2. 情况 $A_1$：\n   $R$ 的列为 $R[:,0] = [0,-2,0]^\\top$，$R[:,1] = [1,0,0]^\\top$，$R[:,2] = [0,0,3]^\\top$。主导轴：体素 $0\\to y$ 负号，体素 $1\\to x$ 正号，体素 $2\\to z$ 正号。对齐到 $(x,y,z)$ 的排列是 $p = [1,0,2]$，flips $= [0,1,0]$。用这些列构建 $S_{3\\times 3}$ 会得到 $R_{\\mathrm{can}}$，其列为 $[1,0,0]^\\top$、$[0,2,0]^\\top$ 和 $[0,0,3]^\\top$，这满足条件；非对角线元素为零，所以 $\\mathrm{max\\_offdiag}=0$，并且 $\\mathrm{ok} = \\text{True}$。\n\n3. 情况 $A_2$：\n   $R$ 的列为 $[1.0, 0.05, 0.0]^\\top$、$[0.0, 1.0, 0.02]^\\top$ 和 $[0.01, 0.0, -1.0]^\\top$。主导轴：体素 $0\\to x$ 正号，体素 $1\\to y$ 正号，体素 $2\\to z$ 负号。因此 $p = [0,1,2]$ 且 flips $= [0,0,1]$。翻转第三列得到 $R_{\\mathrm{can}}$ 的列为 $[1.0, 0.05, 0.0]^\\top$、$[0.0, 1.0, 0.02]^\\top$ 和 $[-0.01, 0.0, 1.0]^\\top$。最大分量位于对角线上且值为正，最大非对角线幅值为 $0.05$；最大对角线幅值为 $1.0$，归一化的 $\\mathrm{max\\_offdiag}=0.05\\le \\tau$，所以 $\\mathrm{ok}=\\text{True}$。\n\n4. 情况 $A_3$：\n   $R$ 的列为 $[1.0, 0.0, 0.0]^\\top$、$[0.8, 0.1, 0.0]^\\top$ 和 $[0.0, 0.0, 1.0]^\\top$。两个体素轴主要映射到 $+x$（重复），产生一个非双射映射；这是无效的。为产生确定性输出而进行的贪婪分配给出 $p = [0,1,2]$ 和 flips $= [0,0,0]$。那么 $R_{\\mathrm{can}}$ 的列是 $[1.0, 0.0, 0.0]^\\top$、$[0.8, 0.1, 0.0]^\\top$、$[0.0, 0.0, 1.0]^\\top$。第 1 列的轴匹配失败（其最大分量在第 0 行而不是第 1 行），并且归一化的最大非对角线幅值为 $0.8/1.0 = 0.8 > \\tau$。因此 $\\mathrm{ok}=\\text{False}$。\n\n该程序实现了这些步骤，强制使用一个小的阈值 $\\varepsilon = 10^{-8}$ 来检测零长度列和正性，使用 $\\tau = 0.25$ 作为剪切容差，并为每个测试用例生成列表 $[\\mathrm{perm},\\mathrm{flips},\\mathrm{ok},\\mathrm{max\\_offdiag}]$。最终输出将这些列表按顺序聚合为单行，用逗号分隔并括在方括号中。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef infer_mapping(R, eps=1e-8):\n    \"\"\"\n    Infer mapping from voxel axes (columns) to world axes (rows) based on the\n    dominant absolute contribution in each column.\n    Returns:\n        world_idx_by_voxel: list of length 3, each entry in {0,1,2}\n        sign_by_voxel: list of length 3, each entry in {+1,-1}\n        valid: boolean, True if mapping is a bijection and columns non-degenerate\n    \"\"\"\n    world_idx_by_voxel = []\n    sign_by_voxel = []\n    valid = True\n    # Check for zero-length columns\n    for j in range(3):\n        col = R[:, j]\n        norm = np.linalg.norm(col)\n        if norm  eps:\n            valid = False\n        k = int(np.argmax(np.abs(col)))\n        s = 1 if col[k] >= 0 else -1\n        world_idx_by_voxel.append(k)\n        sign_by_voxel.append(s)\n    # Check bijection (unique assignment of world axes)\n    counts = [world_idx_by_voxel.count(i) for i in range(3)]\n    if any(c == 0 for c in counts) or any(c > 1 for c in counts):\n        valid = False\n    return world_idx_by_voxel, sign_by_voxel, valid\n\ndef greedy_assign(R, eps=1e-8):\n    \"\"\"\n    Greedily assign world axes i to voxel axes j when mapping is invalid.\n    For each world axis i=0..2, choose the unassigned voxel axis j maximizing |R[i,j]|.\n    Returns:\n        perm: list p_i = selected voxel axis index for world axis i\n        flips: list f_i = 0 if R[i,p_i] >= 0 else 1\n    \"\"\"\n    assigned = set()\n    perm = [None, None, None]\n    flips = [0, 0, 0]\n    for i in range(3):\n        best_j = None\n        best_val = -np.inf\n        for j in range(3):\n            if j in assigned:\n                continue\n            val = abs(R[i, j])\n            if val > best_val + eps:  # strict preference\n                best_val = val\n                best_j = j\n        if best_j is None:\n            # fallback: pick any unassigned\n            for j in range(3):\n                if j not in assigned:\n                    best_j = j\n                    break\n        assigned.add(best_j)\n        perm[i] = best_j\n        flips[i] = 0 if R[i, best_j] >= 0 else 1\n    return perm, flips\n\ndef build_S_from_perm_flips(perm, flips):\n    \"\"\"\n    Build the 4x4 canonicalization transform S given perm and flips.\n    The top-left 3x3 columns select original voxel axes perm[i] and apply sign (flip).\n    \"\"\"\n    S = np.zeros((4, 4), dtype=float)\n    # Construct S_{3x3} by columns\n    for i in range(3):\n        j = perm[i]\n        s = -1.0 if flips[i] == 1 else 1.0\n        S[j, i] = s  # place s at row j, column i\n    S[3, 3] = 1.0\n    return S\n\ndef canonicalize_affine(A, eps=1e-8, tau=0.25):\n    \"\"\"\n    Given affine A, compute canonicalization transform and verification.\n    Returns:\n        perm: list of original voxel axis indices used for world axes [x,y,z]\n        flips: list of 0/1 indicating axis reversal\n        ok: boolean indicating verification success\n        max_offdiag: float, normalized maximum off-diagonal magnitude\n    \"\"\"\n    R = A[:3, :3].astype(float)\n    world_idx_by_voxel, sign_by_voxel, valid_mapping = infer_mapping(R, eps=eps)\n\n    # Build perm and flips\n    if valid_mapping:\n        # Construct perm: for each world axis i, find voxel j with world_idx[j] == i\n        perm = [None, None, None]\n        flips = [0, 0, 0]\n        for i in range(3):\n            # find j where world_idx_by_voxel[j] == i\n            candidates = [j for j in range(3) if world_idx_by_voxel[j] == i]\n            if len(candidates) == 1:\n                j = candidates[0]\n            else:\n                # Should not happen when valid_mapping is True, but handle gracefully\n                j = candidates[0] if candidates else i\n            perm[i] = j\n            flips[i] = 0 if sign_by_voxel[j] == 1 else 1\n    else:\n        # Greedy assignment\n        perm, flips = greedy_assign(R, eps=eps)\n\n    # Build S and canonicalize\n    S = build_S_from_perm_flips(perm, flips)\n    A_can = A @ S\n    R_can = A_can[:3, :3]\n\n    # Verification\n    # 1) Largest magnitude component at matching row i\n    axis_match = True\n    for i in range(3):\n        col = R_can[:, i]\n        if np.linalg.norm(col)  eps:\n            axis_match = False\n            break\n        k = int(np.argmax(np.abs(col)))\n        if k != i:\n            axis_match = False\n            break\n        # 2) Positive diagonal\n        if R_can[i, i] = eps:\n            axis_match = False\n            break\n\n    # 3) Off-diagonal tolerance\n    diag_mags = []\n    offdiag_max = 0.0\n    for i in range(3):\n        col = R_can[:, i]\n        diag_mags.append(abs(col[i]))\n        off = np.max(np.abs(np.delete(col, i)))  # max off-diagonal in this column\n        if off > offdiag_max:\n            offdiag_max = off\n    max_diag_mag = max(diag_mags) if diag_mags else 0.0\n    if max_diag_mag > eps:\n        max_offdiag = offdiag_max / max_diag_mag\n    else:\n        # If all diagonal magnitudes are near zero, treat as degenerate\n        max_offdiag = float('inf')\n        axis_match = False\n\n    ok = bool(axis_match and (max_offdiag = tau) and valid_mapping)\n\n    return perm, flips, ok, float(max_offdiag)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    A0 = np.array([\n        [1, 0, 0, 0],\n        [0, 1, 0, 0],\n        [0, 0, 1, 0],\n        [0, 0, 0, 1]\n    ], dtype=float)\n\n    A1 = np.array([\n        [0, 1, 0, 10],\n        [-2, 0, 0, -5],\n        [0, 0, 3, 2],\n        [0, 0, 0, 1]\n    ], dtype=float)\n\n    A2 = np.array([\n        [1.0, 0.0, 0.01, 0.0],\n        [0.05, 1.0, 0.0, 0.0],\n        [0.0, 0.02, -1.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0]\n    ], dtype=float)\n\n    A3 = np.array([\n        [1.0, 0.8, 0.0, 3.0],\n        [0.0, 0.1, 0.0, 4.0],\n        [0.0, 0.0, 1.0, 5.0],\n        [0.0, 0.0, 0.0, 1.0]\n    ], dtype=float)\n\n    test_cases = [A0, A1, A2, A3]\n\n    results = []\n    for A in test_cases:\n        perm, flips, ok, max_offdiag = canonicalize_affine(A, eps=1e-8, tau=0.25)\n        # Assemble result as required: [perm, flips, ok, max_offdiag]\n        results.append([perm, flips, ok, max_offdiag])\n\n    # Final print statement in the exact required format.\n    # Single line containing the results as a comma-separated list enclosed in square brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4491629"}, {"introduction": "在将卷积神经网络应用于像 MRI 扫描这样的三维数据时，一个基本的设计选择是将其作为 2D 切片堆栈处理，还是作为小型 3D 区块处理。这一决策涉及到计算内存、模型可用的空间上下文以及最终预测性能之间的关键权衡。本练习提供了一个量化框架来分析这些权衡，帮助您为自己的项目做出明智的架构决策。[@problem_id:4491613]", "problem": "您的任务是设计并实现一个程序，用于比较两种用于三维神经影像的卷积神经网络（CNN）输入范式：一种是基于二维切片的模型，另一种是基于三维区块的模型。您需要对每种范式量化三个方面：每个训练批次的内存消耗、输入所捕获的物理上下文，以及在简单生成检测模型下预测的临床判别性能代理指标。程序必须为多个给定的参数集计算结果，并以单一、精确指定的格式输出。\n\n请从以下基础和核心定义开始您的推导，您必须使用这些定义来构建您的解决方案，而不能调用任何特定于问题的快捷公式：\n\n- 一个形状为 $n_1 \\times n_2 \\times \\cdots \\times n_k$、每个元素占用 $b$ 字节的标量类型的浮点张量，其内存消耗正好是 $n_1 n_2 \\cdots n_k \\cdot b$ 字节。训练内存主要由激活值主导，一个常数乘法开销因子可以近似计算梯度和优化器状态所需的额外存储。\n- 卷积神经网络（CNN）在每个池化阶段后将空间维度减小一个因子 $2$。对于一个有 $L$ 个阶段的网络，在阶段 $l \\in \\{0,1,\\dots,L-1\\}$ 的特征图空间维度约等于输入维度除以 $2^l$，并向下取整以保持整数网格索引。在编码器式的特征金字塔中，标准做法是在每个更深的阶段将通道数加倍。\n- 从一个体素间距为 $s_x, s_y, s_z$（单位：毫米）的体素网格派生出的输入张量，其物理视场等于每个维度中的体素数量与其间距的乘积，从而得出每个轴向的物理长度。三维物理视场体积是这三个物理长度的乘积。\n- 根据信号检测理论中的高斯等方差检测模型，聚合 $N_{\\text{eff}}$ 个独立观测，每个观测贡献一个均值差为 $s$、噪声标准差为 $\\sigma$ 的信号，其可判别性与有效独立观测数量的平方根成比例。体素级的空间相关性可以通过一个有效相关体积来概括。最终高斯模型的受试者工作特征曲线下面积（AUC）可以从可判别性指数中推导出来。\n\n您必须实现以下精确的建模假设：\n\n1) 架构和内存模型\n- 考虑一个仅有编码器的特征金字塔，包含 $L$ 个阶段，索引为 $l \\in \\{0,\\dots,L-1\\}$。在阶段 $l$，通道数为 $C_0 \\cdot 2^l$，其中 $C_0$ 是基础通道数。\n- 阶段 $l$ 的空间分辨率由 $\\left(\\max\\{1,\\left\\lfloor \\frac{n_x}{2^l} \\right\\rfloor\\}, \\max\\{1,\\left\\lfloor \\frac{n_y}{2^l} \\right\\rfloor\\}, \\max\\{1,\\left\\lfloor \\frac{n_z}{2^l} \\right\\rfloor\\}\\right)$ 给出，其中 $(n_x,n_y,n_z)$ 是提交给CNN的样本的输入体素数。对于二维切片模型，根据构造使用 $n_z = 1$。\n- 每个样本的总激活元素数量是各阶段 $l$ 的空间维度与通道数的乘积之和。\n- 每批次的激活内存（以字节为单位）等于 $\\gamma \\cdot B \\cdot E \\cdot b_f$，其中 $\\gamma$ 是一个常数开销乘数， $B$ 是批次大小， $E$ 是各阶段加总的每个样本的激活元素数量， $b_f$ 是每个浮点数的字节数。内存以兆字节（MB）表示，其中 $1 \\text{ MB} = 1024^2$ 字节。\n\n2) 上下文捕获\n- 设体素间距为 $(s_x,s_y,s_z)$，单位为毫米。对于一个具有 $(n_x,n_y)$ 体素的二维切片输入，其厚度被视为恰好一个切片的厚度，即物理厚度为 $s_z$。该切片输入的物理视场（FOV）体积为 $V_{\\text{2D}} = (n_x s_x) \\cdot (n_y s_y) \\cdot (1 \\cdot s_z)$ 立方毫米。\n- 对于一个具有 $(n_x,n_y,n_z)$ 体素的三维区块，其物理FOV体积为 $V_{\\text{3D}} = (n_x s_x) \\cdot (n_y s_y) \\cdot (n_z s_z)$ 立方毫米。\n- 报告无量纲的上下文比率 $R_V = \\frac{V_{\\text{3D}}}{V_{\\text{2D}}}$。\n\n3) 预测的临床性能代理指标\n- 假设一个在高斯等方差模型下的二元病变检测任务。假定每个体素在类别之间贡献一个附加的信号差异 $s$，以及标准差为 $\\sigma$ 的独立噪声。空间相关性通过一个有效相关体积（以体素为单位）$v_c = \\max\\{1, \\lambda_x \\lambda_y \\lambda_z\\}$ 来概括，其中 $(\\lambda_x,\\lambda_y,\\lambda_z)$ 是沿各轴测量的相关范围（以体素为单位）。\n- 对于一个具有 $n_x n_y n_z$ 个体素的输入，其有效独立观测数量为 $N_{\\text{eff}} = \\max\\left\\{1, \\frac{n_x n_y n_z}{v_c}\\right\\}$。\n- 使用高斯等方差模型的信号检测理论，根据 $N_{\\text{eff}}$、$s$ 和 $\\sigma$ 推导出可判别性指数，然后根据可判别性指数推导出AUC。以 $[0,1]$ 范围的小数形式报告 AUC，不带百分号。\n\n您的程序必须为每个测试案例计算：\n- 二维批次内存（MB）。\n- 三维批次内存（MB）。\n- 上下文比率 $R_V$（无量纲）。\n- 二维模型的 AUC。\n- 三维模型的 AUC。\n\n每个测试案例的架构配置和数据参数如下。所有整数和实数都是精确的，您必须严格遵守规范。\n\n所有案例的通用常量：\n- 浮点数存储大小：$b_f = 4$ 字节。\n\n测试套件：\n- 案例 A（正常路径，各向同性间距）：\n  - 二维切片输入尺寸 $(n_x,n_y) = (256, 256)$。\n  - 三维区块尺寸 $(n_x,n_y,n_z) = (64, 64, 16)$。\n  - 体素间距 $(s_x,s_y,s_z) = (1.0, 1.0, 1.0)$ 毫米。\n  - 网络阶段数 $L = 3$，基础通道数 $C_0 = 16$，开销 $\\gamma = 3.0$。\n  - 批次大小：二维 $B_{\\text{2D}} = 8$，三维 $B_{\\text{3D}} = 2$。\n  - 检测模型：每体素信号 $s = 0.5$，噪声标准差 $\\sigma = 1.0$，相关范围 $(\\lambda_x,\\lambda_y,\\lambda_z) = (3, 3, 3)$ 体素。\n\n- 案例 B（各向异性间距，浅深度区块）：\n  - 二维切片输入尺寸 $(n_x,n_y) = (512, 512)$。\n  - 三维区块尺寸 $(n_x,n_y,n_z) = (128, 128, 4)$。\n  - 体素间距 $(s_x,s_y,s_z) = (0.8, 0.8, 3.0)$ 毫米。\n  - 网络阶段数 $L = 4$，基础通道数 $C_0 = 8$，开销 $\\gamma = 2.5$。\n  - 批次大小：二维 $B_{\\text{2D}} = 4$，三维 $B_{\\text{3D}} = 1$。\n  - 检测模型：每体素信号 $s = 0.3$，噪声标准差 $\\sigma = 1.2$，相关范围 $(\\lambda_x,\\lambda_y,\\lambda_z) = (5, 5, 1)$ 体素。\n\n- 案例 C（边界情况，三维深度等于一）：\n  - 二维切片输入尺寸 $(n_x,n_y) = (64, 64)$。\n  - 三维区块尺寸 $(n_x,n_y,n_z) = (64, 64, 1)$。\n  - 体素间距 $(s_x,s_y,s_z) = (1.0, 1.0, 1.0)$ 毫米。\n  - 网络阶段数 $L = 2$，基础通道数 $C_0 = 16$，开销 $\\gamma = 3.0$。\n  - 批次大小：二维 $B_{\\text{2D}} = 8$，三维 $B_{\\text{3D}} = 8$。\n  - 检测模型：每体素信号 $s = 0.6$，噪声标准差 $\\sigma = 0.9$，相关范围 $(\\lambda_x,\\lambda_y,\\lambda_z) = (2, 2, 2)$ 体素。\n\n- 案例 D（边缘情况，高基础通道数，各向异性间距）：\n  - 二维切片输入尺寸 $(n_x,n_y) = (128, 128)$。\n  - 三维区块尺寸 $(n_x,n_y,n_z) = (96, 96, 24)$。\n  - 体素间距 $(s_x,s_y,s_z) = (0.5, 0.5, 2.0)$ 毫米。\n  - 网络阶段数 $L = 2$，基础通道数 $C_0 = 32$，开销 $\\gamma = 3.5$。\n  - 批次大小：二维 $B_{\\text{2D}} = 16$，三维 $B_{\\text{3D}} = 1$。\n  - 检测模型：每体素信号 $s = 0.8$，噪声标准差 $\\sigma = 0.8$，相关范围 $(\\lambda_x,\\lambda_y,\\lambda_z) = (2, 2, 2)$ 体素。\n\n实现细节：\n- 对于二维模型，在所有计算中将深度视为 $n_z = 1$。\n- 对于每个阶段 $l$，通过整数向下取整计算空间维度，每个维度最少 1 个体素：对于维度 $d \\in \\{x,y,z\\}$，$n_d^{(l)} = \\max\\{1,\\left\\lfloor \\frac{n_d}{2^l} \\right\\rfloor\\}$。\n- 对于每个阶段 $l$，通道数为 $C^{(l)} = C_0 \\cdot 2^l$。\n- 每个样本的激活元素数量为 $E = \\sum_{l=0}^{L-1} \\left(n_x^{(l)} n_y^{(l)} n_z^{(l)} C^{(l)}\\right)$。\n- 每批次的内存（以兆字节为单位）为 $M_{\\text{MB}} = \\frac{\\gamma \\cdot B \\cdot E \\cdot b_f}{1024^2}$，以 MB 为单位报告。\n- 物理视场体积为 $V_{\\text{2D}} = (n_x s_x)(n_y s_y)(1 \\cdot s_z)$ 和 $V_{\\text{3D}} = (n_x s_x)(n_y s_y)(n_z s_z)$ 立方毫米，上下文比率为 $R_V = \\frac{V_{\\text{3D}}}{V_{\\text{2D}}}$。\n- 有效相关体积为 $v_c = \\max\\{1, \\lambda_x \\lambda_y \\lambda_z\\}$，有效数量为 $N_{\\text{eff}} = \\max\\left\\{1, \\frac{n_x n_y n_z}{v_c}\\right\\}$，AUC 必须从高斯等方差信号检测模型推导得出。以小数形式报告 AUC。\n\n您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔的结果列表。每个测试案例的结果本身必须是一个包含五个浮点数的列表，顺序如下：二维批次内存（MB）、三维批次内存（MB）、上下文比率 $R_V$、二维模型的 AUC、三维模型的 AUC。因此，最终输出必须是一个列表的列表，例如：“[[m2d_A,m3d_A,RV_A,auc2d_A,auc3d_A],[m2d_B,m3d_B,RV_B,auc2d_B,auc3d_B],...]”，除非数值格式需要，否则不添加空格。", "solution": "该问题要求对神经影像中卷积神经网络（CNN）的两种输入范式进行定量比较：一种是二维（2D）切片方法，另一种是三维（3D）区块方法。比较基于三个指标：内存消耗、捕获的物理上下文以及临床性能的代理指标。解决方案将根据提供的基本原理和建模假设推导得出。\n\n### 1. 内存消耗模型\n\n训练神经网络所需的总内存很复杂，但通常由用于反向传播的激活值的存储主导。问题通过将每批次内存建模为与总激活元素数量成正比来简化这一点，其中包含一个恒定的开销因子 $\\gamma$。\n\n设CNN的输入为一个尺寸为 $n_x \\times n_y \\times n_z$ 的张量。对于2D切片，根据定义我们使用 $n_z=1$。该网络是一个具有 $L$ 个阶段的编码器，索引为 $l \\in \\{0, 1, \\dots, L-1\\}$。\n\n在每个阶段 $l$，空间维度减小一个因子 $2^l$。为了处理整数网格大小并防止维度消失，维度 $d \\in \\{x,y,z\\}$ 在阶段 $l$ 的大小由以下公式给出：\n$$ n_d^{(l)} = \\max\\left\\{1, \\left\\lfloor \\frac{n_d}{2^l} \\right\\rfloor\\right\\} $$\n\n通道数（特征图数量）在每个阶段加倍，从基础数量 $C_0$ 开始：\n$$ C^{(l)} = C_0 \\cdot 2^l $$\n\n在阶段 $l$ 单个样本的总激活元素数量是空间维度和通道数的乘积：$n_x^{(l)} n_y^{(l)} n_z^{(l)} C^{(l)}$。每个样本的总激活元素数量 $E$ 是所有 $L$ 个阶段的总和：\n$$ E = \\sum_{l=0}^{L-1} n_x^{(l)} n_y^{(l)} n_z^{(l)} C^{(l)} $$\n\n给定一批 $B$ 个样本，浮点数大小为 $b_f$ 字节，开销因子为 $\\gamma$，总内存消耗（以字节为单位）为 $\\gamma \\cdot B \\cdot E \\cdot b_f$。为了将其转换为兆字节（MB），我们使用转换关系 $1 \\text{ MB} = 1024^2$ 字节。\n$$ M_{\\text{MB}} = \\frac{\\gamma \\cdot B \\cdot E \\cdot b_f}{1024^2} $$\n\n此计算将分别针对2D切片配置（使用其特定的维度 $n_x, n_y$ 与 $n_z=1$，以及批次大小 $B_{\\text{2D}}$）和3D区块配置（使用其维度 $n_x, n_y, n_z$ 和批次大小 $B_{\\text{3D}}$）执行。\n\n### 2. 物理上下文捕获模型\n\n输入捕获的物理上下文由其物理视场（FOV）体积来量化。这是沿每个轴的体素数量与这些体素的物理间距的乘积。\n\n给定体素间距 $(s_x, s_y, s_z)$（单位：毫米）：\n-   对于尺寸为 $(n_x^{\\text{2D}}, n_y^{\\text{2D}})$ 的2D切片输入，其厚度被建模为对应于 $z$ 维度的一个体素。FOV体积为：\n    $$ V_{\\text{2D}} = (n_x^{\\text{2D}} s_x) \\cdot (n_y^{\\text{2D}} s_y) \\cdot (1 \\cdot s_z) $$\n-   对于尺寸为 $(n_x^{\\text{3D}}, n_y^{\\text{3D}}, n_z^{\\text{3D}})$ 的3D区块输入，FOV体积为：\n    $$ V_{\\text{3D}} = (n_x^{\\text{3D}} s_x) \\cdot (n_y^{\\text{3D}} s_y) \\cdot (n_z^{\\text{3D}} s_z) $$\n\n无量纲的上下文比率 $R_V$ 用于比较3D区块与2D切片的体积：\n$$ R_V = \\frac{V_{\\text{3D}}}{V_{\\text{2D}}} = \\frac{(n_x^{\\text{3D}} s_x) (n_y^{\\text{3D}} s_y) (n_z^{\\text{3D}} s_z)}{(n_x^{\\text{2D}} s_x) (n_y^{\\text{2D}} s_y) (s_z)} = \\frac{n_x^{\\text{3D}} n_y^{\\text{3D}} n_z^{\\text{3D}}}{n_x^{\\text{2D}} n_y^{\\text{2D}}} $$\n\n### 3. 预测性能代理指标 (AUC)\n\n性能代理指标基于高斯等方差信号检测模型。我们假设一个二元分类任务（例如，病变与非病变）。对于每个体素，两个类别被建模为从两个具有相同标准差 $\\sigma$ 但均值相差一个信号差 $s$ 的正态分布中抽样。\n\n单个体素观测的可判别性由指数 $d'_1 = s/\\sigma$ 给出。聚合跨多个体素的信息可以提高可判别性。由于相邻体素之间存在空间相关性，有效独立观测数 $N_{\\text{eff}}$ 小于总的体素数 $N_{\\text{vox}} = n_x n_y n_z$。这通过一个由相关范围 $(\\lambda_x, \\lambda_y, \\lambda_z)$ 定义的有效相关体积 $v_c$ 来建模：\n$$ v_c = \\max\\{1, \\lambda_x \\lambda_y \\lambda_z\\} $$\n有效独立观测数则为：\n$$ N_{\\text{eff}} = \\max\\left\\{1, \\frac{N_{\\text{vox}}}{v_c}\\right\\} = \\max\\left\\{1, \\frac{n_x n_y n_z}{v_c}\\right\\} $$\n对于2D切片模型，$n_z=1$。\n\n在整个输入体积上聚合信号的可判别性指数 $d'$ 与 $N_{\\text{eff}}$ 的平方根成正比：\n$$ d' = d'_1 \\sqrt{N_{\\text{eff}}} = \\frac{s}{\\sigma} \\sqrt{N_{\\text{eff}}} $$\n\n对于高斯等方差模型，受试者工作特征曲线下面积（AUC）通过标准正态分布的累积分布函数（CDF）$\\Phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{z} e^{-t^2/2} dt$ 与可判别性指数 $d'$ 相关联。公式为：\n$$ \\text{AUC} = \\Phi\\left(\\frac{d'}{\\sqrt{2}}\\right) $$\n这可以使用误差函数 $\\text{erf}(z) = \\frac{2}{\\sqrt{\\pi}} \\int_0^z e^{-t^2} dt$ 通过恒等式 $\\Phi(z) = \\frac{1}{2}\\left(1 + \\text{erf}\\left(\\frac{z}{\\sqrt{2}}\\right)\\right)$ 来计算。代入 $z=d'/\\sqrt{2}$：\n$$ \\text{AUC} = \\frac{1}{2}\\left(1 + \\text{erf}\\left(\\frac{d'/\\sqrt{2}}{\\sqrt{2}}\\right)\\right) = \\frac{1}{2}\\left(1 + \\text{erf}\\left(\\frac{d'}{2}\\right)\\right) $$\n此计算将分别使用2D和3D输入配置各自的总 voxel 数量进行。\n\n### 计算流程\n实现将包含将这些派生模型编码的函数。对于每个指定的测试案例，将使用2D切片和3D区块模型的相应参数调用这些函数，以计算所需的五个输出值：2D内存、3D内存、上下文比率、2D AUC和3D AUC。", "answer": "```python\nimport math\nfrom scipy.special import erf\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    It calculates and prints the comparative metrics for 2D vs. 3D CNN inputs.\n    \"\"\"\n\n    def calculate_memory(dims, L, C0, gamma, B, bf):\n        \"\"\"\n        Calculates the estimated memory consumption for a single training batch.\n\n        Args:\n            dims (tuple): Input dimensions (nx, ny, nz).\n            L (int): Number of network stages.\n            C0 (int): Base number of channels.\n            gamma (float): Memory overhead factor.\n            B (int): Batch size.\n            bf (int): Bytes per floating-point number.\n\n        Returns:\n            float: Memory consumption in Megabytes (MB).\n        \"\"\"\n        nx, ny, nz = dims\n        total_elements_per_sample = 0\n        for l in range(L):\n            pow_2_l = 2**l\n            \n            # Spatial dimensions at stage l\n            nx_l = max(1, math.floor(nx / pow_2_l))\n            ny_l = max(1, math.floor(ny / pow_2_l))\n            nz_l = max(1, math.floor(nz / pow_2_l))\n            \n            # Channel count at stage l\n            channels_l = C0 * pow_2_l\n            \n            elements_at_stage = nx_l * ny_l * nz_l * channels_l\n            total_elements_per_sample += elements_at_stage\n            \n        mem_bytes = gamma * B * total_elements_per_sample * bf\n        mem_mb = mem_bytes / (1024**2)\n        return mem_mb\n\n    def calculate_auc(dims, s, sigma, lambdas):\n        \"\"\"\n        Calculates the predicted AUC using a signal detection theory model.\n\n        Args:\n            dims (tuple): Input dimensions (nx, ny, nz).\n            s (float): Per-voxel signal difference.\n            sigma (float): Per-voxel noise standard deviation.\n            lambdas (tuple): Correlation extents (lx, ly, lz) in voxels.\n\n        Returns:\n            float: The Area Under the ROC Curve (AUC).\n        \"\"\"\n        nx, ny, nz = dims\n        lx, ly, lz = lambdas\n        \n        n_vox = nx * ny * nz\n        v_c = max(1, lx * ly * lz)\n        n_eff = max(1, n_vox / v_c)\n        \n        d_prime = (s / sigma) * math.sqrt(n_eff)\n        \n        auc = 0.5 * (1 + erf(d_prime / 2.0))\n        return auc\n\n    # Constants common to all cases\n    bf = 4  # bytes per float\n\n    # Test suite definition\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"n_2d\": (256, 256),\n            \"n_3d\": (64, 64, 16),\n            \"s_xyz\": (1.0, 1.0, 1.0),\n            \"L\": 3, \"C0\": 16, \"gamma\": 3.0,\n            \"B_2d\": 8, \"B_3d\": 2,\n            \"s\": 0.5, \"sigma\": 1.0, \"lambdas\": (3, 3, 3)\n        },\n        {\n            \"name\": \"Case B\",\n            \"n_2d\": (512, 512),\n            \"n_3d\": (128, 128, 4),\n            \"s_xyz\": (0.8, 0.8, 3.0),\n            \"L\": 4, \"C0\": 8, \"gamma\": 2.5,\n            \"B_2d\": 4, \"B_3d\": 1,\n            \"s\": 0.3, \"sigma\": 1.2, \"lambdas\": (5, 5, 1)\n        },\n        {\n            \"name\": \"Case C\",\n            \"n_2d\": (64, 64),\n            \"n_3d\": (64, 64, 1),\n            \"s_xyz\": (1.0, 1.0, 1.0),\n            \"L\": 2, \"C0\": 16, \"gamma\": 3.0,\n            \"B_2d\": 8, \"B_3d\": 8,\n            \"s\": 0.6, \"sigma\": 0.9, \"lambdas\": (2, 2, 2)\n        },\n        {\n            \"name\": \"Case D\",\n            \"n_2d\": (128, 128),\n            \"n_3d\": (96, 96, 24),\n            \"s_xyz\": (0.5, 0.5, 2.0),\n            \"L\": 2, \"C0\": 32, \"gamma\": 3.5,\n            \"B_2d\": 16, \"B_3d\": 1,\n            \"s\": 0.8, \"sigma\": 0.8, \"lambdas\": (2, 2, 2)\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        # Unpack case parameters\n        dims_2d = (case[\"n_2d\"][0], case[\"n_2d\"][1], 1)\n        dims_3d = case[\"n_3d\"]\n        L, C0, gamma = case[\"L\"], case[\"C0\"], case[\"gamma\"]\n        B_2d, B_3d = case[\"B_2d\"], case[\"B_3d\"]\n        s, sigma, lambdas = case[\"s\"], case[\"sigma\"], case[\"lambdas\"]\n        s_xyz = case[\"s_xyz\"]\n        \n        # 1. Memory Calculation\n        mem_2d = calculate_memory(dims_2d, L, C0, gamma, B_2d, bf)\n        mem_3d = calculate_memory(dims_3d, L, C0, gamma, B_3d, bf)\n\n        # 2. Context Ratio Calculation\n        v_2d = (dims_2d[0] * s_xyz[0]) * (dims_2d[1] * s_xyz[1]) * (1 * s_xyz[2])\n        v_3d = (dims_3d[0] * s_xyz[0]) * (dims_3d[1] * s_xyz[1]) * (dims_3d[2] * s_xyz[2])\n        v_ratio = v_3d / v_2d\n\n        # 3. AUC Calculation\n        auc_2d = calculate_auc(dims_2d, s, sigma, lambdas)\n        auc_3d = calculate_auc(dims_3d, s, sigma, lambdas)\n\n        case_results = [mem_2d, mem_3d, v_ratio, auc_2d, auc_3d]\n        all_results.append(case_results)\n\n    # Format the final output string as specified\n    result_strings = [f\"[{','.join(map(str, r))}]\" for r in all_results]\n    final_output = f\"[{','.join(result_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "4491613"}, {"introduction": "对神经系统疾病的全面理解通常需要整合来自多种成像模式的证据。本实践介绍了一种强大的概率方法，用于融合来自 sMRI、fMRI 和 DTI 等多种来源的数据，以估计一个单一的潜在变量，如疾病严重程度。您不仅将学习如何从 Bayes 的第一性原理构建此融合模型，还将学习如何使其对缺失数据具有鲁棒性，这是现实世界临床数据集中一个常见的挑战。[@problem_id:4491601]", "problem": "您的任务是设计并分析一个用于神经病学中多模态神经影像融合的概率图模型，其中结构磁共振成像 (sMRI)、功能磁共振成像 (fMRI) 和扩散张量成像 (DTI) 等模态为未观测到的连续神经系统严重性变量提供条件独立的观测值。您必须从贝叶斯定理、概率图模型中的条件独立性以及高斯先验和高斯似然的共轭性这些基本原理出发。您必须推导其推断过程并将其实现为一个程序。\n\n考虑一个潜变量 $z \\in \\mathbb{R}$，代表神经系统疾病的严重程度。$z$ 的先验是高斯分布，$z \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$。每个模态 $i$ 产生一个观测值 $x_i \\in \\mathbb{R}$，其模型为线性高斯条件分布，$x_i \\mid z \\sim \\mathcal{N}(a_i z + b_i, \\sigma_i^2)$，且各模态在给定 $z$ 的条件下是独立的。多模态融合使用这些似然与先验的乘积来形成 $z$ 的后验分布。缺失的模态必须通过边缘化来处理：在联合模型中对缺失的观测值进行积分，以获得仅给定已观测模态的后验分布。\n\n您的任务是：\n- 根据所述基本原理，推导当仅观测到模态子集 $\\mathcal{O}$ 时的后验分布 $p(z \\mid \\{x_i\\}_{i \\in \\mathcal{O}})$。用 $(\\mu_0, \\sigma_0^2)$ 以及观测到的模态参数和数据来表示后验均值和方差。\n- 解释为什么在这个线性高斯模型中，通过对每个缺失模态 $i \\in \\mathcal{M}$ 使用属性 $\\int p(x_i \\mid z) \\, dx_i = 1$，对缺失模态 $\\mathcal{M}$ 进行边缘化会消除它们的似然贡献。\n- 定义稳健性度量，用于比较缺失模态下的后验与所有模态可用时的后验：Kullback–Leibler 散度 $\\mathrm{KL}(\\mathcal{N}(\\mu_{\\text{miss}}, \\sigma^2_{\\text{miss}}) \\Vert \\mathcal{N}(\\mu_{\\text{full}}, \\sigma^2_{\\text{full}}))$、方差比 $r = \\sigma^2_{\\text{miss}} / \\sigma^2_{\\text{full}}$，以及对于给定的真实值 $z^\\star$ 的均方误差变化量 $\\Delta \\mathrm{MSE} = (\\mu_{\\text{miss}} - z^\\star)^2 - (\\mu_{\\text{full}} - z^\\star)^2$。\n\n实现一个程序，对于下述每个测试用例，计算后验均值 $\\mu_{\\text{miss}}$、后验方差 $\\sigma^2_{\\text{miss}}$、与全模态后验的 Kullback–Leibler 散度、方差比 $r$，以及相对于基线全模态后验的均方误差变化量 $\\Delta \\mathrm{MSE}$。所有值必须作为浮点数返回，并四舍五入到 $6$ 位小数。\n\n使用以下测试套件，其中所有模态分别为 sMRI ($i=1$)、fMRI ($i=2$) 和 DTI ($i=3$)。对于每个测试用例，都指定了先验参数 $(\\mu_0, \\sigma_0^2)$、模态参数 $(a_i, b_i, \\sigma_i^2)$、完整观测向量 $(x_1, x_2, x_3)$、真实值 $z^\\star$ 以及缺失模态的集合：\n\n- 测试用例 $1$（理想路径，一个中等噪声的模态缺失）：$(\\mu_0, \\sigma_0^2) = (0.0, 1.0)$；sMRI $(a_1, b_1, \\sigma_1^2) = (1.2, 0.0, 0.25)$；fMRI $(a_2, b_2, \\sigma_2^2) = (0.8, 0.0, 0.64)$；DTI $(a_3, b_3, \\sigma_3^2) = (0.5, 0.0, 0.36)$；$(x_1, x_2, x_3) = (0.65, 0.30, 0.35)$；$z^\\star = 0.5$；缺失模态 $\\{2\\}$。\n- 测试用例 $2$（无模态缺失的同一性情况）：与测试用例 $1$ 相同的参数和观测值；缺失模态 $\\varnothing$。\n- 测试用例 $3$（边界情况，所有模态均缺失）：与测试用例 $1$ 相同的参数和观测值；缺失模态 $\\{1,2,3\\}$。\n- 测试用例 $4$（边缘情况，移除无信息量的模态）：$(\\mu_0, \\sigma_0^2) = (0.0, 1.0)$；sMRI $(a_1, b_1, \\sigma_1^2) = (1.2, 0.0, 0.25)$；fMRI $(a_2, b_2, \\sigma_2^2) = (0.8, 0.0, 0.64)$；DTI $(a_3, b_3, \\sigma_3^2) = (0.0, 0.0, 0.36)$；$(x_1, x_2, x_3) = (0.65, 0.30, 0.35)$；$z^\\star = 0.5$；缺失模态 $\\{3\\}$。\n- 测试用例 $5$（稳健性压力测试，缺失信息量最大的模态）：与测试用例 $1$ 相同的参数和观测值；缺失模态 $\\{1\\}$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果。对于从 $1$ 到 $5$ 的每个测试用例，按顺序附加以下五个四舍五入的值：$[\\mu_{\\text{miss}}, \\sigma^2_{\\text{miss}}, \\mathrm{KL}, r, \\Delta \\mathrm{MSE}]$。因此，最终输出包含 $25$ 个浮点数，按测试用例的顺序排列，并四舍五入到 $6$ 位小数，例如，$[\\text{tc1\\_mu},\\text{tc1\\_var},\\text{tc1\\_kl},\\text{tc1\\_r},\\text{tc1\\_dmse},\\ldots,\\text{tc5\\_dmse}]$。", "solution": "该问题是有效的，因为它在科学上基于贝叶斯统计，问题设定良好，具有唯一可解的结构，并且其表述是客观的。我们可以着手推导解决方案。\n\n该问题要求对用于多模态神经影像数据融合的概率图模型进行分析。该模型包含一个表示疾病严重程度的连续潜变量 $z \\in \\mathbb{R}$（具有高斯先验），以及一组来自不同成像模态的观测值 $\\{x_i\\}$，每个观测值都由以 $z$ 为条件的高斯似然来建模。\n\n### 1. 后验分布的推导\n\n令 $\\mathcal{O}$ 为与观测到的模态相对应的索引集合。我们被要求推导后验分布 $p(z \\mid \\{x_i\\}_{i \\in \\mathcal{O}})$。根据贝叶斯定理以及各模态在给定 $z$ 时的条件独立性，后验分布正比于先验分布与观测到的模态的似然函数的乘积：\n$$\np(z \\mid \\{x_i\\}_{i \\in \\mathcal{O}}) \\propto p(z) \\prod_{i \\in \\mathcal{O}} p(x_i \\mid z)\n$$\n先验是高斯分布：\n$$\np(z) = \\mathcal{N}(z \\mid \\mu_0, \\sigma_0^2) \\propto \\exp\\left( -\\frac{(z - \\mu_0)^2}{2\\sigma_0^2} \\right)\n$$\n每个观测模态 $i \\in \\mathcal{O}$ 的似然也是高斯分布：\n$$\np(x_i \\mid z) = \\mathcal{N}(x_i \\mid a_i z + b_i, \\sigma_i^2) \\propto \\exp\\left( -\\frac{(x_i - (a_i z + b_i))^2}{2\\sigma_i^2} \\right)\n$$\n因此，后验分布正比于这些指数项的乘积：\n$$\np(z \\mid \\{x_i\\}_{i \\in \\mathcal{O}}) \\propto \\exp\\left( -\\frac{(z - \\mu_0)^2}{2\\sigma_0^2} - \\sum_{i \\in \\mathcal{O}} \\frac{(x_i - a_i z - b_i)^2}{2\\sigma_i^2} \\right)\n$$\n高斯分布的乘积是一个未归一化的高斯分布。为了找到后验分布的参数，我们将其表示为 $\\mathcal{N}(z \\mid \\mu_{\\mathcal{O}}, \\sigma_{\\mathcal{O}}^2)$。其概率密度函数的形式为：\n$$\np(z \\mid \\{x_i\\}_{i \\in \\mathcal{O}}) \\propto \\exp\\left( -\\frac{(z - \\mu_{\\mathcal{O}})^2}{2\\sigma_{\\mathcal{O}}^2} \\right)\n$$\n我们可以通过展开后验分布指数中的二次项，并匹配 $z^2$ 和 $z$ 的系数来找到 $\\mu_{\\mathcal{O}}$ 和 $\\sigma_{\\mathcal{O}}^2$。指数的参数为：\n$$\nL(z) = -\\frac{1}{2} \\left[ \\frac{(z - \\mu_0)^2}{\\sigma_0^2} + \\sum_{i \\in \\mathcal{O}} \\frac{(a_i z - (x_i - b_i))^2}{\\sigma_i^2} \\right] \\\\\n= -\\frac{1}{2} \\left[ \\frac{z^2 - 2z\\mu_0 + \\mu_0^2}{\\sigma_0^2} + \\sum_{i \\in \\mathcal{O}} \\frac{a_i^2 z^2 - 2a_i z (x_i - b_i) + (x_i - b_i)^2}{\\sigma_i^2} \\right]\n$$\n收集 $z^2$ 和 $z$ 的项：\n$$\nL(z) = -\\frac{1}{2} \\left[ z^2 \\left( \\frac{1}{\\sigma_0^2} + \\sum_{i \\in \\mathcal{O}} \\frac{a_i^2}{\\sigma_i^2} \\right) - 2z \\left( \\frac{\\mu_0}{\\sigma_0^2} + \\sum_{i \\in \\mathcal{O}} \\frac{a_i(x_i - b_i)}{\\sigma_i^2} \\right) + C \\right]\n$$\n其中 $C$ 包含不依赖于 $z$ 的项。\n通过将其与高斯指数的标准二次形式 $-\\frac{1}{2\\sigma_{\\mathcal{O}}^2}(z^2 - 2z\\mu_{\\mathcal{O}} + \\mu_{\\mathcal{O}}^2)$ 进行比较，我们可以确定后验的逆方差（精度）和均值。\n\n$z^2$ 的系数给出了后验精度：\n$$\n\\frac{1}{\\sigma_{\\mathcal{O}}^2} = \\frac{1}{\\sigma_0^2} + \\sum_{i \\in \\mathcal{O}} \\frac{a_i^2}{\\sigma_i^2}\n$$\n所以，后验方差是：\n$$\n\\sigma_{\\mathcal{O}}^2 = \\left( \\frac{1}{\\sigma_0^2} + \\sum_{i \\in \\mathcal{O}} \\frac{a_i^2}{\\sigma_i^2} \\right)^{-1}\n$$\n$-2z$ 的系数使我们能够找到后验均值：\n$$\n\\frac{\\mu_{\\mathcal{O}}}{\\sigma_{\\mathcal{O}}^2} = \\frac{\\mu_0}{\\sigma_0^2} + \\sum_{i \\in \\mathcal{O}} \\frac{a_i(x_i - b_i)}{\\sigma_i^2}\n$$\n因此，后验均值是：\n$$\n\\mu_{\\mathcal{O}} = \\sigma_{\\mathcal{O}}^2 \\left( \\frac{\\mu_0}{\\sigma_0^2} + \\sum_{i \\in \\mathcal{O}} \\frac{a_i(x_i - b_i)}{\\sigma_i^2} \\right)\n$$\n这些方程定义了后验分布 $p(z \\mid \\{x_i\\}_{i \\in \\mathcal{O}}) = \\mathcal{N}(z \\mid \\mu_{\\mathcal{O}}, \\sigma_{\\mathcal{O}}^2)$ 的参数。对于给定的测试用例，$\\mu_{\\text{miss}}$ 和 $\\sigma^2_{\\text{miss}}$ 使用这些公式计算，其中 $\\mathcal{O}$ 是观测到的模态集合。完整的后验参数 $\\mu_{\\text{full}}$ 和 $\\sigma^2_{\\text{full}}$ 的计算方式类似，但此时 $\\mathcal{O}$ 是所有模态的集合。\n\n### 2. 对缺失模态的边缘化\n\n令所有模态索引的完整集合为 $\\mathcal{I}$，它是观测模态 $\\mathcal{O}$ 和缺失模态 $\\mathcal{M}$ 的不相交并集，即 $\\mathcal{I} = \\mathcal{O} \\cup \\mathcal{M}$。我们寻求后验 $p(z \\mid \\{x_i\\}_{i \\in \\mathcal{O}})$。根据条件概率的定义：\n$$\np(z \\mid \\{x_i\\}_{i \\in \\mathcal{O}}) = \\frac{p(z, \\{x_i\\}_{i \\in \\mathcal{O}})}{p(\\{x_i\\}_{i \\in \\mathcal{O}})}\n$$\n$z$ 和观测数据 $\\{x_i\\}_{i \\in \\mathcal{O}}$ 的联合分布是通过从完整联合分布 $p(z, \\{x_i\\}_{i \\in \\mathcal{I}})$ 中对缺失模态的变量 $\\{x_j\\}_{j \\in \\mathcal{M}}$ 进行边缘化（积分掉）得到的：\n$$\np(z, \\{x_i\\}_{i \\in \\mathcal{O}}) = \\int p(z, \\{x_i\\}_{i \\in \\mathcal{I}}) \\prod_{j \\in \\mathcal{M}} dx_j\n$$\n使用链式法则和条件独立性，完整联合分布为 $p(z, \\{x_i\\}_{i \\in \\mathcal{I}}) = p(z) \\prod_{i \\in \\mathcal{I}} p(x_i \\mid z)$。将其代入积分中：\n$$\np(z, \\{x_i\\}_{i \\in \\mathcal{O}}) = \\int p(z) \\left(\\prod_{i \\in \\mathcal{O}} p(x_i \\mid z)\\right) \\left(\\prod_{j \\in \\mathcal{M}} p(x_j \\mid z)\\right) \\prod_{j \\in \\mathcal{M}} dx_j\n$$\n仅依赖于 $z$ 和 $\\{x_i\\}_{i \\in \\mathcal{O}}$ 的项可以移到积分之外：\n$$\np(z, \\{x_i\\}_{i \\in \\mathcal{O}}) = p(z) \\left(\\prod_{i \\in \\mathcal{O}} p(x_i \\mid z)\\right) \\left(\\prod_{j \\in \\mathcal{M}} \\int p(x_j \\mid z) dx_j \\right)\n$$\n对于任何缺失的模态 $j \\in \\mathcal{M}$，项 $p(x_j \\mid z)$ 是一个概率密度函数。根据定义，任何概率密度函数（PDF）在其整个定义域上的积分等于 $1$：\n$$\n\\int_{-\\infty}^{\\infty} p(x_j \\mid z) dx_j = 1\n$$\n无论 $z$ 的值是多少，这都成立。因此，这些积分的乘积也为 $1$。这将联合分布简化为：\n$$\np(z, \\{x_i\\}_{i \\in \\mathcal{O}}) = p(z) \\prod_{i \\in \\mathcal{O}} p(x_i \\mid z)\n$$\n因此，后验变为：\n$$\np(z \\mid \\{x_i\\}_{i \\in \\mathcal{O}}) \\propto p(z) \\prod_{i \\in \\mathcal{O}} p(x_i \\mid z)\n$$\n这表明，通过边缘化处理缺失模态在数学上等同于在贝叶斯法则的乘积中直接省略它们的似然项。来自缺失模态的信息被抵消了。\n\n### 3. 稳健性度量\n\n该问题指定了三个度量标准，用以量化缺失数据对后验分布的影响。令来自全模态集的后验为 $\\mathcal{N}_{\\text{full}} = \\mathcal{N}(\\mu_{\\text{full}}, \\sigma^2_{\\text{full}})$，来自观测模态子集的后验为 $\\mathcal{N}_{\\text{miss}} = \\mathcal{N}(\\mu_{\\text{miss}}, \\sigma^2_{\\text{miss}})$。\n\n1.  **Kullback–Leibler (KL) 散度**：从 $\\mathcal{N}_{\\text{miss}}$ 到 $\\mathcal{N}_{\\text{full}}$ 的KL散度衡量了用 $\\mathcal{N}_{\\text{full}}$ 近似 $\\mathcal{N}_{\\text{miss}}$ 时丢失的信息。对于两个一元高斯分布 $\\mathcal{N}_1 = \\mathcal{N}(\\mu_1, \\sigma_1^2)$ 和 $\\mathcal{N}_2 = \\mathcal{N}(\\mu_2, \\sigma_2^2)$ 的公式是：\n    $$\n    \\mathrm{KL}(\\mathcal{N}_1 \\Vert \\mathcal{N}_2) = \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}\n    $$\n    这里，$\\mathcal{N}_1 = \\mathcal{N}_{\\text{miss}}$ 且 $\\mathcal{N}_2 = \\mathcal{N}_{\\text{full}}$。\n\n2.  **方差比**：这是缺失数据情况下的后验方差与全数据情况下后验方差的比率。\n    $$\n    r = \\frac{\\sigma^2_{\\text{miss}}}{\\sigma^2_{\\text{full}}}\n    $$\n    由于观测更多数据只会增加后验精度（或保持不变），我们期望 $\\sigma^2_{\\text{miss}} \\geq \\sigma^2_{\\text{full}}$，因此 $r \\geq 1$。\n\n3.  **均方误差的变化量 ($\\Delta \\mathrm{MSE}$)**：该度量评估了 $z$ 的点估计（取后验均值）相对于真实值 $z^\\star$ 的退化程度。\n    $$\n    \\Delta \\mathrm{MSE} = (\\mu_{\\text{miss}} - z^\\star)^2 - (\\mu_{\\text{full}} - z^\\star)^2\n    $$\n    正值表示缺失数据增加了估计的平方误差。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the multimodal neuroimaging fusion problem for the given test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"prior\": (0.0, 1.0),\n            \"modalities\": {\n                1: {\"params\": (1.2, 0.0, 0.25), \"obs\": 0.65},  # sMRI\n                2: {\"params\": (0.8, 0.0, 0.64), \"obs\": 0.30},  # fMRI\n                3: {\"params\": (0.5, 0.0, 0.36), \"obs\": 0.35},  # DTI\n            },\n            \"z_star\": 0.5,\n            \"missing\": {2}\n        },\n        {\n            \"prior\": (0.0, 1.0),\n            \"modalities\": {\n                1: {\"params\": (1.2, 0.0, 0.25), \"obs\": 0.65},\n                2: {\"params\": (0.8, 0.0, 0.64), \"obs\": 0.30},\n                3: {\"params\": (0.5, 0.0, 0.36), \"obs\": 0.35},\n            },\n            \"z_star\": 0.5,\n            \"missing\": set()\n        },\n        {\n            \"prior\": (0.0, 1.0),\n            \"modalities\": {\n                1: {\"params\": (1.2, 0.0, 0.25), \"obs\": 0.65},\n                2: {\"params\": (0.8, 0.0, 0.64), \"obs\": 0.30},\n                3: {\"params\": (0.5, 0.0, 0.36), \"obs\": 0.35},\n            },\n            \"z_star\": 0.5,\n            \"missing\": {1, 2, 3}\n        },\n        {\n            \"prior\": (0.0, 1.0),\n            \"modalities\": {\n                1: {\"params\": (1.2, 0.0, 0.25), \"obs\": 0.65},\n                2: {\"params\": (0.8, 0.0, 0.64), \"obs\": 0.30},\n                3: {\"params\": (0.0, 0.0, 0.36), \"obs\": 0.35},  # Non-informative DTI\n            },\n            \"z_star\": 0.5,\n            \"missing\": {3}\n        },\n        {\n            \"prior\": (0.0, 1.0),\n            \"modalities\": {\n                1: {\"params\": (1.2, 0.0, 0.25), \"obs\": 0.65},\n                2: {\"params\": (0.8, 0.0, 0.64), \"obs\": 0.30},\n                3: {\"params\": (0.5, 0.0, 0.36), \"obs\": 0.35},\n            },\n            \"z_star\": 0.5,\n            \"missing\": {1}\n        },\n    ]\n\n    def calculate_posterior(prior, modalities, observations, observed_indices):\n        \"\"\"\n        Calculates the posterior mean and variance given a set of observations.\n        \n        Args:\n            prior (tuple): (mu_0, sigma_0_sq).\n            modalities (dict): Dictionary of modality parameters.\n            observations (dict): Dictionary of modality observations.\n            observed_indices (set): Set of indices for observed modalities.\n\n        Returns:\n            tuple: (posterior_mean, posterior_variance).\n        \"\"\"\n        mu_0, sigma_0_sq = prior\n        \n        # Precision = 1 / variance\n        post_precision = 1.0 / sigma_0_sq\n        mu_prec_term = mu_0 / sigma_0_sq\n        \n        for i in observed_indices:\n            a_i, b_i, sigma_i_sq = modalities[i]\n            x_i = observations[i]\n            \n            # Add likelihood precision\n            lik_precision = (a_i ** 2) / sigma_i_sq\n            post_precision += lik_precision\n            \n            # Add weighted mean term from likelihood\n            mu_prec_term += (a_i * (x_i - b_i)) / sigma_i_sq\n            \n        post_variance = 1.0 / post_precision\n        post_mean = post_variance * mu_prec_term\n        \n        return post_mean, post_variance\n\n    final_results = []\n    \n    for case in test_cases:\n        prior_params = case[\"prior\"]\n        modality_params = {k: v[\"params\"] for k, v in case[\"modalities\"].items()}\n        observations = {k: v[\"obs\"] for k, v in case[\"modalities\"].items()}\n        z_star = case[\"z_star\"]\n        missing_indices = case[\"missing\"]\n        all_indices = set(modality_params.keys())\n        observed_indices = all_indices - missing_indices\n\n        # 1. Calculate full posterior (all modalities observed)\n        mu_full, var_full = calculate_posterior(\n            prior_params, modality_params, observations, all_indices\n        )\n\n        # 2. Calculate posterior with missing data\n        mu_miss, var_miss = calculate_posterior(\n            prior_params, modality_params, observations, observed_indices\n        )\n\n        # 3. Calculate metrics\n        # KL Divergence: KL(miss || full)\n        if var_miss == 0 or var_full == 0: # Avoid log(0) or division by zero\n            kl_div = float('inf') if var_full > 0 else 0.0\n        else:\n            term1 = np.log(np.sqrt(var_full) / np.sqrt(var_miss))\n            term2 = (var_miss + (mu_miss - mu_full)**2) / (2 * var_full)\n            kl_div = term1 + term2 - 0.5\n\n        # Variance Ratio\n        var_ratio = var_miss / var_full if var_full > 0 else float('inf')\n\n        # Change in MSE\n        delta_mse = (mu_miss - z_star)**2 - (mu_full - z_star)**2\n\n        # In case of perfect match (e.g. no missing data), KL can be tiny negative due to float precision. Clamp to 0.\n        if abs(kl_div)  1e-12: kl_div = 0.0\n        if abs(var_ratio - 1.0)  1e-12: var_ratio = 1.0\n        if abs(delta_mse)  1e-12: delta_mse = 0.0\n\n        case_results = [mu_miss, var_miss, kl_div, var_ratio, delta_mse]\n        final_results.extend(case_results)\n\n    # Format output\n    formatted_results = [f\"{val:.6f}\" for val in final_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```", "id": "4491601"}]}