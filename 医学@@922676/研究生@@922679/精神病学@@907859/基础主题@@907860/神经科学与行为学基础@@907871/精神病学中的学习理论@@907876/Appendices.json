{"hands_on_practices": [{"introduction": "精神病学中的许多症状，尤其是焦虑和创伤相关障碍，都源于后天学习到的联结。Rescorla-Wagner 模型为我们提供了一个经典而强大的数学框架，用以量化联结强度如何根据经验进行更新。本练习将通过一个具体的分步计算，让你亲手实践在一个复合线索学习事件中，不同线索的联结强度是如何变化的，从而深刻理解预测误差和线索显著性在学习过程中的核心作用 [@problem_id:4721764]。", "problem": "一位治疗师在对惊恐障碍实施暴露疗法时，使用一个正式的联结学习框架来追踪患者在面对先前已获得对内感受性痛苦具有预测价值的复合线索时，其条件反应如何变化。考虑一个单一的强化复合试验，其中两个线索，$A$（一种内感受）和 $B$（一种外感受情境），被同时呈现，随后出现被视为非条件刺激的惊恐反应。试验前，线索的联结强度分别为 $V_{A}=0.4$ 和 $V_{B}=0.2$。治疗师为线索指定了特定的显著性参数 $\\alpha_{A}=0.3$ 和 $\\alpha_{B}=0.1$，并对非条件刺激使用一个共同的学习率参数 $\\beta=0.5$。该试验中非条件刺激的强化量为 $\\lambda=1$。使用预测误差原理和 Rescorla-Wagner 联结学习框架，从第一性原理推导更新规则，并计算此次复合试验的联结强度变化量 $\\Delta V_{A}$ 和 $\\Delta V_{B}$。将你的最终答案 $(\\Delta V_{A}, \\Delta V_{B})$ 表示为一个行矩阵。无需四舍五入。", "solution": "该问题陈述经评估具有科学有效性。它基于公认的 Rescorla-Wagner 联结学习模型，该模型是学习理论的基石，在临床心理学和计算精神病学中有直接应用。该问题提法恰当，提供了计算唯一解所需的所有参数（$\\alpha_A$, $\\alpha_B$, $\\beta$, $\\lambda$, $V_A$, $V_B$）。所用术语客观、正式，与该科学领域一致。因此，可以推导出解。\n\nRescorla-Wagner 模型假定，在一次学习试验中，一个刺激的联结强度变化（$\\Delta V$）取决于实际结果与预期结果之间的差异。这种差异被称为预测误差。在给定试验中出现的单个线索 $i$ 的联结强度更新规则，便是从此原理推导而来。\n\n联结强度的变化量 $\\Delta V_i$ 与线索 $i$ 的显著性（用 $\\alpha_i$ 表示）以及与非条件刺激（US）相关的学习率参数（用 $\\beta$ 表示）成正比。关键在于，这一变化是由预测误差项 $(\\lambda - V_{total})$ 驱动的，其中 $\\lambda$ 是非条件刺激所能支持的联结强度的渐近水平，而 $V_{total}$ 是该试验中出现的所有线索的联结强度之和。\n\n线索 $i$ 的 Rescorla-Wagner 更新规则的一般形式为：\n$$\n\\Delta V_i = \\alpha_i \\beta (\\lambda - V_{total})\n$$\n\n在本问题中，我们考虑的是一个复合试验，其中两个线索 $A$ 和 $B$ 同时呈现。因此，该试验的总联结强度 $V_{total}$ 是当前呈现的线索各自强度之和：\n$$\nV_{total} = V_A + V_B\n$$\n这个和代表了对非条件刺激出现的总预期。强化由 $\\lambda$ 给出，即惊恐反应的强度。\n\n将 $V_{total}$ 的表达式代入通用更新规则，我们得到线索 $A$（$\\Delta V_A$）和线索 $B$（$\\Delta V_B$）的联结强度变化的具体方程：\n$$\n\\Delta V_A = \\alpha_A \\beta (\\lambda - (V_A + V_B))\n$$\n$$\n\\Delta V_B = \\alpha_B \\beta (\\lambda - (V_A + V_B))\n$$\n请注意，预测误差项 $(\\lambda - (V_A + V_B))$ 对两个线索是相同的，因为它们是同一复合刺激事件的一部分。它们强度的不同变化源于它们不同的显著性值 $\\alpha_A$ 和 $\\alpha_B$。\n\n问题提供了以下数值：\n- 线索 $A$ 的初始联结强度：$V_A = 0.4$\n- 线索 $B$ 的初始联结强度：$V_B = 0.2$\n- 线索 $A$ 的显著性：$\\alpha_A = 0.3$\n- 线索 $B$ 的显著性：$\\alpha_B = 0.1$\n- 非条件刺激的学习率：$\\beta = 0.5$\n- 强化量：$\\lambda = 1$\n\n首先，我们计算强化试验前复合线索 $AB$ 的总联结强度：\n$$\nV_{total} = V_A + V_B = 0.4 + 0.2 = 0.6\n$$\n接下来，我们计算预测误差：\n$$\n\\text{Prediction Error} = \\lambda - V_{total} = 1 - 0.6 = 0.4\n$$\n正的预测误差表明结果比预期的要强，导致当前呈现的线索的联结强度增加（兴奋性条件反射）。\n\n现在，我们可以计算每个线索的联结强度变化。\n对于线索 $A$：\n$$\n\\Delta V_A = \\alpha_A \\beta (\\lambda - V_{total}) = (0.3)(0.5)(0.4)\n$$\n$$\n\\Delta V_A = (0.15)(0.4) = 0.06\n$$\n对于线索 $B$：\n$$\n\\Delta V_B = \\alpha_B \\beta (\\lambda - V_{total}) = (0.1)(0.5)(0.4)\n$$\n$$\n\\Delta V_B = (0.05)(0.4) = 0.02\n$$\n因此，此次复合试验的联结强度变化为 $\\Delta V_A = 0.06$ 和 $\\Delta V_B = 0.02$。结果以行矩阵 $(\\Delta V_A, \\Delta V_B)$ 的形式呈现。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.06  0.02\n\\end{pmatrix}\n}\n$$", "id": "4721764"}, {"introduction": "在掌握了 Rescorla-Wagner 模型的基本更新规则后，我们可以进一步探索其更深远、有时甚至反直觉的预测，而这些预测对临床实践具有重要指导意义。本练习聚焦于“阻断效应” (blocking) 这一经典现象，即一个已充分建立的预测性线索会“阻断”大脑对同时出现的另一个新线索的学习。理解阻断效应对设计有效的暴露疗法至关重要，因为它揭示了为何对某些创伤提醒物的暴露训练可能无法自动泛化到其他提醒物。此练习旨在挑战你批判性地思考如何构建治疗情境以克服这一学习障碍，从而最大化治疗效果 [@problem_id:4721800]。", "problem": "一名创伤后应激障碍（PTSD）患者报告称，当暴露于城市紧急事件提示物时，会感到极度痛苦。确定了三个与创伤相关的条件刺激（CS）：警报声（$A$）、柴油废气的气味（$B$）和闪烁的蓝光（$C$）。非条件刺激（US）是治疗师引导下对指标性创伤最糟糕时刻的想象重现。治疗师采用受控暴露疗程，系统地改变想象性US的强度和CS复合刺激的构成，以研究和利用联结学习机制。\n\n治疗前的历史学习已在警报声（$A$）和创伤记忆之间产生了主导性的联结，使得在一个归一化尺度上，$A$的联结强度接近渐近线，当想象性US强度为$\\lambda = 1.0$时，$V_A \\approx 1.0$。其他线索在基线时的联结强度可忽略不计，$V_B \\approx 0$且$V_C \\approx 0$。CS的凸显性参数在各疗程中保持稳定，$A$为中等凸显性（$\\alpha_A = 0.4$），$B$为较低凸显性（$\\alpha_B = 0.3$），$C$为最低凸显性（$\\alpha_C = 0.2$）。US处理参数由疗程节奏设定，并在各疗程中保持恒定（$\\beta_{\\mathrm{US}} = 0.5$）。\n\n治疗师计划在四个阶段中进行以下操作：\n\n- 阶段 $1$：进行十次复合疗程，同时呈现$A$和$B$（$AB$），并以与基线相同的强度（$\\lambda = 1.0$）唤起想象性US。\n- 阶段 $2$：进行十次复合疗程，同时呈现$A$和$C$（$AC$），并有意将想象性US的强度增加到$\\lambda' = 1.4$以引发意外（surprise）。\n- 阶段 $3$：通过重复呈现$A$而不伴随US，对$A$进行消退处理，直到$V_A$降低至约$0.6$。\n- 阶段 $4$：进行数次疗程，单独将$B$与US一同呈现（不存在$A$或$C$），强度为$\\lambda = 1.0$。\n\n运用适用于精神病学中经典条件反射的、由预测误差驱动的联结学习原理，确定在阶段 $1$中哪个线索会表现出阻断效应，并指出在所描述的阶段中，哪一个单一操作最直接地解除对$B$的学习的阻断（即，由于预测误差增加而导致$V_B$产生正向变化）。选择正确陈述阶段 $1$中阻断结果以及对$B$最有效的解除阻断设计的选项。\n\nA. 在阶段 $1$中，$B$将被$A$阻断，因为当$V_A \\approx 1.0$时，复合刺激$AB$在$\\lambda = 1.0$的情况下不会增加意外。在阶段 $2$中，$C$将获得联结强度，因为尽管$A$占主导地位，将US增加到$\\lambda' = 1.4$会产生预测误差。对$B$最直接的解除阻断是通过首先通过消退（阶段 $3$）降低$V_A$，然后单独将$B$与US呈现（阶段 $4$），这样在$B$–US试验中$\\sum V$较低，$V_B$会增加。\n\nB. 在阶段 $1$中，$B$不会被阻断，因为即使$A$占主导地位，$\\alpha_B = 0.3$也足以进行学习。在$\\lambda = 1.0$的条件下持续进行$AB$疗程将无需额外操作即可逐渐解除对$B$的阻断，并且在阶段 $2$中增加US是不必要的。\n\nC. 在阶段 $1$中，无论US强度如何变化，只要$A$存在，$B$和$C$都将被阻断。要解除对$B$的阻断，应在$ABD$复合刺激中引入一个新的线索$D$，因为向复合刺激中添加新奇性比移除$A$或改变$\\lambda$更能增加对$B$的学习。\n\nD. 在阶段 $1$中，$B$将被阻断，在阶段 $2$中，$C$也将被阻断，因为$A$的联结强度仍然很高。对$B$的最佳解除阻断方法是将想象性US的强度降低到基线以下（例如，降至$\\lambda = 0.6$），这样$B$可以在没有压倒性恐惧反应的情况下学习；消退$A$是不必要的。", "solution": "该问题遵循由Rescorla-Wagner模型形式化的、由预测误差驱动的联结学习原理。在任何给定的学习试验中，条件刺激（CS）$i$的联结强度变化量$\\Delta V_i$由以下方程确定：\n$$ \\Delta V_i = \\alpha_i \\beta (\\lambda - V_{total}) $$\n其中，$\\alpha_i$是CS $i$的凸显性，$\\beta$是与非条件刺激（US）相关的速率参数，$\\lambda$是US的大小，$V_{total}$是该试验中所有存在的CS的联结强度之和，即$\\sum V_j$。项$(\\lambda - V_{total})$代表预测误差。\n\n问题提供了以下初始条件和参数：\n- 初始联结强度：$V_A \\approx 1.0$, $V_B \\approx 0$, $V_C \\approx 0$。\n- 基线时的US大小：$\\lambda = 1.0$。\n- 凸显性参数：$\\alpha_A = 0.4$, $\\alpha_B = 0.3$, $\\alpha_C = 0.2$。\n- US处理参数：$\\beta_{\\mathrm{US}} = 0.5$。\n\n**阶段 $1$ 分析：阻断效应**\n在阶段 $1$中，复合刺激$AB$与强度为$\\lambda = 1.0$的US一同呈现。在此阶段开始时，复合刺激的总联结强度为$V_{total} = V_A + V_B \\approx 1.0 + 0 = 1.0$。\n因此，预测误差为：\n$$ \\lambda - V_{total} \\approx 1.0 - 1.0 = 0 $$\n在此阶段的第一次试验中，线索$B$的联结强度变化为：\n$$ \\Delta V_B = \\alpha_B \\beta (\\lambda - (V_A + V_B)) \\approx (0.3)(0.5)(1.0 - (1.0 + 0)) = 0 $$\n由于预测误差约等于$0$，线索$B$不会发生新的学习。线索$A$预先存在的联结强度完全预测了US，从而“阻断”了线索$B$获得联结强度。这证实了在阶段 $1$中，$B$将被$A$阻断。\n\n**解除阻断机制分析**\n解除阻断的核心原理是在本会发生阻断的情况下创建一个非零的预测误差。问题描述了两种实现这一目标的独特方法。\n\n1.  **增加US大小（阶段 $2$）：** 在阶段 $2$中，复合刺激$AC$与强度增加的US（$\\lambda' = 1.4$）配对。初始总联结强度为$V_{total} = V_A + V_C \\approx 1.0 + 0 = 1.0$。预测误差变为：\n    $$ \\lambda' - V_{total} \\approx 1.4 - 1.0 = 0.4 $$\n    这个正向预测误差（“意外”）驱动了所有当前线索的新学习。对于线索$C$，其强度变化为：\n    $$ \\Delta V_C = \\alpha_C \\beta (\\lambda' - (V_A + V_C)) \\approx (0.2)(0.5)(0.4) = 0.04 $$\n    因此，$V_C$将增加。这展示了通过增加US大小来解除阻断。\n\n2.  **降低阻断物强度（阶段 $3$ 和 $4$）：** 此策略旨在促成关于线索$B$的学习。\n    - **阶段 $3$（$A$的消退）：** 单独呈现线索$A$而不伴随US（$\\lambda = 0$）。预测误差为$\\lambda - V_A = 0 - V_A$。这个负向预测误差导致$V_A$下降。该阶段持续进行，直到$V_A$降低到约$0.6$。\n    - **阶段 $4$（$B$的学习获得）：** 然后，单独将线索$B$与原始强度的US（$\\lambda = 1.0$）一同呈现。在此阶段开始时，$V_B \\approx 0$。总联结强度就是$V_B$。预测误差为：\n      $$ \\lambda - V_B \\approx 1.0 - 0 = 1.0 $$\n      线索$B$的联结强度变化为：\n      $$ \\Delta V_B = \\alpha_B \\beta (\\lambda - V_B) \\approx (0.3)(0.5)(1.0 - 0) = 0.15 $$\n      这个大的正向变化表明，线索$B$现在将很容易获得联结强度。这个过程包括消退阻断物，然后对目标线索进行条件化试验，是克服阻断效应的有效方法，通过确保目标线索与US配对时有大的预测误差，从而使先前被阻断的线索获得强度。\n\n问题要求确定阶段 $1$中的阻断效应以及最直接地解除对$B$的学习阻断的操作。根据我们的分析，在阶段 $1$中$B$被阻断。问题中描述的解除对$B$学习阻断的操作是阶段 $3$和阶段 $4$的序列。\n\n**选项评估：**\n\n-   **A.** 该选项指出，在阶段 $1$中，$B$被$A$阻断，因为复合刺激$AB$不产生意外。这是正确的。它正确地解释了在阶段 $2$中，将$\\lambda$增加到$1.4$会解除对$C$的学习阻断。最后，它准确地描述了阶段 $3$和阶段 $4$程序的逻辑，即通过降低$V_A$然后将$B$与US配对，确保在$B$-US试验期间有高的预测误差，以此作为解除对$B$阻断的直接方法。该选项与我们的推导完全一致。**正确**。\n\n-   **B.** 该选项错误地声称，由于其凸显性$\\alpha_B = 0.3$，$B$不会被阻断。Rescorla-Wagner模型规定，学习是凸显性和预测误差的乘积。如果预测误差为零，无论凸显性如何，学习都为零。**错误**。\n\n-   **C.** 该选项犯了几个错误。它错误地指出$C$在阶段 $1$中被阻断（它并不存在）。它错误地声称US强度的变化不影响阻断。阶段 $2$就是一个直接的反例。在标准的Rescorla-Wagner模型中，增加一个新线索$D$的建议不是一个有效的解除阻断方法，因为增加一个$V_D=0$的线索并不会改变预测误差。**错误**。\n\n-   **D.** 该选项正确地指出$B$在阶段 $1$中被阻断，但错误地声称$C$在阶段 $2$中也被阻断。如分析所示，$C$在阶段 $2$中被解除了阻断。此外，它建议降低US强度（例如，降至$\\lambda=0.6$）来解除对$B$的阻断。这将产生一个负向预测误差$(\\lambda - (V_A+V_B)) \\approx 0.6 - 1.0 = -0.4$，这将导致抑制性条件反射（$V_B$的减少），而不是兴奋性条件反射。**错误**。\n\n因此，根据指定的学习模型，选项A提供了对该现象唯一准确和完整的描述。", "answer": "$$\\boxed{A}$$", "id": "4721800"}, {"introduction": "除了基于“试验”的联结模型，现代计算精神病学越来越多地采用更动态的强化学习模型来理解行为和决策。时间差分 (Temporal Difference, TD) 学习是其中的一个核心框架，它精妙地模拟了大脑如何根据即时回报和对未来价值的预期来逐刻更新其对世界状态的评估。本练习将引导你计算状态价值函数 $V(s)$ 的更新以及至关重要的“奖励预测误差” (reward prediction error, $\\delta_t$) [@problem_id:4721727]。通过这个计算，你将能够把这些抽象的计算概念与它们在神经生物学上的假说对应物（如中脑多巴胺神经元的相位性放电）以及它们在解释抑郁症等精神疾病核心症状（如快感缺乏）时的作用直接联系起来。", "problem": "一位患有重度抑郁症的患者参与了一项基于强化的决策任务，该任务被建模为马尔可夫决策过程 (MDP)。临床医生使用时间差分 (TD) 学习来量化患者在折扣累积回报和贝尔曼一致性的假设下，如何在每次试验中更新状态值。在试验 $t$ 中，患者处于状态 $s_t$，其当前价值估计为 $V(s_t) = 0.5$。在选择一个动作后，患者接收到即时结果 $r_t = 1$，并转移到状态 $s_{t+1}$，其价值估计为 $V(s_{t+1}) = 0.6$。学习率为 $\\alpha = 0.2$，折扣因子为 $\\gamma = 0.9$。使用期望折扣回报的基本定义和贝尔曼一致性原则，推导观测回报与当前估计之间的基于样本的差异（奖励预测误差），并计算对 $V(s_t)$ 的单步 TD 更新。报告奖励预测误差的数值和 $V(s_t)$ 的更新价值估计。此外，从患者学习（趋近与回避）和精神病学模型中可能的神经调质信号传导方面，解释奖励预测误差符号的临床意义。将您的最终数值答案表示为一个行矩阵，其中包含奖励预测误差，后跟更新后的价值估计。无需四舍五入。", "solution": "首先提取给定的参数和条件，然后评估其科学和逻辑完整性，从而验证该问题。\n\n### 步骤 1：提取已知条件\n- 建模框架：马尔可夫决策过程 (MDP)\n- 学习算法：时间差分 (TD) 学习\n- 试验 $t$ 时的状态：$s_t$\n- 状态 $s_t$ 的当前价值估计：$V(s_t) = 0.5$\n- 试验 $t$ 的即时结果（奖励）：$r_t = 1$\n- 后续状态：$s_{t+1}$\n- 状态 $s_{t+1}$ 的价值估计：$V(s_{t+1}) = 0.6$\n- 学习率：$\\alpha = 0.2$\n- 折扣因子：$\\gamma = 0.9$\n- 目标：推导并计算奖励预测误差，计算更新后的价值 $V(s_t)$，并解释其临床意义。\n\n### 步骤 2：使用提取的已知条件进行验证\n- **科学依据**：该问题基于成熟的强化学习框架，特别是时间差分 (TD) 学习。该模型是计算神经科学和计算精神病学研究决策与学习的基石，尤其与多巴胺功能相关。参数和更新规则是该模型的标准组成部分。该问题在科学上是合理的。\n- **适定性**：问题提供了所有必要的数值和明确定义的任务。标准的 TD 学习方程可以计算出奖励预测误差和更新后状态值的唯一解。\n- **客观性**：问题陈述客观，基于一个形式化模型定义了一项计算任务。所提供的临床背景用于解释，并不会给数学计算过程带来主观性。\n- **结论**：该问题是有效的。它是一个适定的、有科学依据的问题，可以使用强化学习的标准原理来解决。\n\n### 步骤 3：推导与求解\n\n该问题需要应用单步时间差分 (TD) 学习算法，通常称为 TD($0$)。该算法根据单次转移的经验来更新状态的价值估计 $V(s_t)$，该经验由当前状态 $s_t$、收到的奖励 $r_t$ 和下一个状态 $s_{t+1}$ 组成。\n\n其基本原理是贝尔曼一致性，即一个状态的价值应该等于期望的即时奖励加上期望的下一状态的折扣价值。TD 算法使用此期望的一个样本来更新其当前估计。基于样本的 $s_t$ 真实价值的估计被称为“TD 目标”。对于单步前瞻，TD 目标定义为：\n$$\n\\text{TD Target} = r_t + \\gamma V(s_{t+1})\n$$\n这个表达式代表了从这一步经验中观察到的总折扣回报。\n\n这个新的、基于经验的估计与旧的价值估计之间的差异就是奖励预测误差 (RPE)，用 $\\delta_t$ 表示。RPE 量化了结果的“意外”程度。其形式化定义为：\n$$\n\\delta_t = (\\text{TD Target}) - V(s_t) = [r_t + \\gamma V(s_{t+1})] - V(s_t)\n$$\n使用问题陈述中提供的值：\n- $r_t = 1$\n- $\\gamma = 0.9$\n- $V(s_{t+1}) = 0.6$\n- $V(s_t) = 0.5$\n\n我们可以计算 RPE 的数值：\n$$\n\\delta_t = [1 + (0.9)(0.6)] - 0.5\n$$\n$$\n\\delta_t = [1 + 0.54] - 0.5\n$$\n$$\n\\delta_t = 1.54 - 0.5\n$$\n$$\n\\delta_t = 1.04\n$$\n第一个要求的值，即奖励预测误差，是 $1.04$。\n\nTD 更新规则通过将原始价值估计 $V(s_t)$ 部分地移向 TD 目标来对其进行修正。这个调整的大小由学习率 $\\alpha$ 控制。更新后的价值，我们可以表示为 $V_{\\text{new}}(s_t)$，计算如下：\n$$\nV_{\\text{new}}(s_t) = V(s_t) + \\alpha \\delta_t\n$$\n代入已知值和计算出的 RPE：\n- $V(s_t) = 0.5$\n- $\\alpha = 0.2$\n- $\\delta_t = 1.04$\n\n我们计算更新后的价值估计：\n$$\nV_{\\text{new}}(s_t) = 0.5 + (0.2)(1.04)\n$$\n$$\nV_{\\text{new}}(s_t) = 0.5 + 0.208\n$$\n$$\nV_{\\text{new}}(s_t) = 0.708\n$$\n第二个要求的值，即 $V(s_t)$ 的更新价值估计，是 $0.708$。\n\n### 临床解释\n\n奖励预测误差 ($\\delta_t$) 的符号对其解释至关重要。在这里，$\\delta_t = 1.04$，是正数。\n\n1.  **在患者学习中的意义**：正的 RPE 表明结果“好于预期”。由价值函数 $V$ 代表的患者对世界的模型，低估了处于状态 $s_t$ 的价值。经历的回报 ($1.54$) 显著高于预期的回报 ($0.5$)。这个正向的意外信号用于强化导致这一有利结果的行为（在状态 $s_t$ 中选择的动作）。它促进“趋近”行为，增加了患者未来在面对状态 $s_t$ 时做出类似选择的概率。状态 $s_t$ 的价值被向上更新（从 $0.5$ 到 $0.708$），反映了这种新的学习。\n\n2.  **可能的神经调质信号传导**：在计算精神病学中，假设 RPE 是由中脑多巴胺神经元的相位性放电编码的。如此处计算出的正 RPE，对应于目标脑区（如纹状体和前额叶皮层）中多巴胺的相位性爆发式释放。这种多巴胺信号被认为是学习积极结果和激励未来目标导向行为的神经生物学基础。在重度抑郁症的背景下，一个关键的假说（快感缺乏）是该奖励系统变得迟钝。在该患者的任务表现中观察到强健的正 RPE 可能表明，奖励学习回路中至少这个组成部分是有反应的，这为评估治疗效果或疾病进展提供了一个量化目标。反之，在许多试验中，对于积极结果，RPE 持续低于预期可能成为快感缺乏的生物标志物。", "answer": "$$\n\\boxed{\\begin{pmatrix} 1.04  0.708 \\end{pmatrix}}\n$$", "id": "4721727"}]}