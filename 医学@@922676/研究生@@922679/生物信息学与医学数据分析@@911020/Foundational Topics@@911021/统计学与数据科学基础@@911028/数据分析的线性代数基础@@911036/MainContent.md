## 引言
在生物信息学和医学数据分析领域，海量高维数据的涌现既是机遇也是挑战。从基因组学、蛋白质组学到数字病理学，我们面临的数据集常常包含数以万计的特征，但样本量相对有限。如何从这些复杂的数据中提取有意义的生物学信号、构建稳定且可解释的模型，是现代生物医学研究的核心难题。线性代数，作为数学的一个基础分支，为我们提供了应对这些挑战的强大语言和工具集。它不仅是算法的计算基础，更是一种深刻的思维框架，帮助我们理解数据的内在几何结构、发现潜在模式并解决实际问题。

本文旨在系统性地梳理用于数据分析的线性代数基础，并将其与生物信息学和医学研究中的具体应用紧密结合。我们的目标是超越孤立的公式和定理，构建一个连贯的知识体系。

- 在“**原理与机制**”一章中，我们将深入探讨线性代数的核心概念，从构成数据矩阵骨架的[四个基本子空间](@entry_id:154834)，到驱动稀疏性的范数几何，再到通过矩阵分解揭示数据结构，为您打下坚实的理论基础。
- 随后的“**应用与跨学科连接**”一章将理论付诸实践，展示这些原理如何应用于解决真实世界的问题，例如利用PCA进行医学影像降维、通过正交投影校正[批次效应](@entry_id:265859)，以及如何利用正则化应对“大p，小n”的挑战。
- 最后，在“**动手实践**”部分，您将通过一系列精心设计的练习，亲手操作和巩固所学知识，将理论理解转化为实践能力。

通过本次学习，您将能够掌握运用线性代数思想来分析和解决复杂生物医学数据问题的能力。

## 原理与机制

本章将深入探讨应用于生物信息学和医学数据分析的线性代数核心原理与机制。我们将从数据矩阵的基本结构出发，逐步深入到[线性建模](@entry_id:171589)、数值稳定性、[正则化方法](@entry_id:150559)、[矩阵分解](@entry_id:139760)技术，最终延伸至非线性方法。我们的目标是建立一个坚实的理论框架，使读者能够理解和驾驭[高维数据](@entry_id:138874)分析中的关键挑战。

### 数据矩阵的剖析：[四个基本子空间](@entry_id:154834)

在数据分析中，矩阵不仅是存储数据的表格，更是一种强大的数学工具——[线性映射](@entry_id:185132)。例如，一个基因表达矩阵 $A \in \mathbb{R}^{m \times n}$ 可以被看作一个[线性变换](@entry_id:143080) $T: \mathbb{R}^n \to \mathbb{R}^m$，其定义为 $T(x) = Ax$。这里，$m$ 是基因数量，$n$ 是样本数量。向量 $x \in \mathbb{R}^n$ 代表不同样本的某种加权组合，而结果 $Ax$ 则是在基因空间 $\mathbb{R}^m$ 中预测的表达谱。理解这个映射的内在结构，对于揭示数据中的生物学意义至关重要。这个结构由[四个基本子空间](@entry_id:154834)定义。

这四个子空间分别是：

1.  **列空间 (Column Space)** $\mathcal{C}(A)$：矩阵 $A$ 的列向量的所有可能[线性组合](@entry_id:155091)构成的集合。它是目标空间 $\mathbb{R}^m$ 的一个子空间。在基因表达数据的背景下，矩阵的每一列代表一个样本中所有基因的表达水平。因此，[列空间](@entry_id:156444) $\mathcal{C}(A)$ 代表了所有可以通过对原始样本谱进行[线性组合](@entry_id:155091)而合成的“虚拟样本”的集合。任何形式为 $Ax$ 的向量都位于列空间中，因此它也被称为变换 $T$ 的**像 (image)** [@problem_id:4578481]。[列空间](@entry_id:156444)的维度，即 $\dim \mathcal{C}(A)$，被称为矩阵 $A$ 的**秩 (rank)**，它量化了数据在样本维度上的内在复杂性。

2.  **[行空间](@entry_id:148831) (Row Space)** $\mathcal{R}(A)$：矩阵 $A$ 的行向量的所有可能[线性组合](@entry_id:155091)构成的集合。它等价于其[转置](@entry_id:142115)矩阵 $A^\top$ 的[列空间](@entry_id:156444)，即 $\mathcal{C}(A^\top)$，是源空间 $\mathbb{R}^n$ 的一个子空间。在我们的例子中，每一行代表一个基因在所有样本中的表达模式。因此，[行空间](@entry_id:148831) $\mathcal{R}(A)$ 构成了所有可以通过对基因表达模式进行[线性组合](@entry_id:155091)而合成的“元基因”模式的集合 [@problem_id:4578481]。

3.  **零空间 (Null Space)** $\mathcal{N}(A)$：所有被映射到目标空间中[零向量](@entry_id:156189)的源空间向量的集合，即 $\mathcal{N}(A) = \{ x \in \mathbb{R}^n : Ax = 0 \}$。它是源空间 $\mathbb{R}^n$ 的一个子空间。在数据分析的背景下，[零空间](@entry_id:171336)中的非[零向量](@entry_id:156189)代表了样本权重的一种特定组合，这种组合在通过矩阵 $A$ 变换后，不会产生任何可观测的基因表达信号。这通常意味着样本之间存在某种线性冗余或相关性。

4.  **左零空间 (Left Null Space)** $\mathcal{N}(A^\top)$：矩阵 $A$ 的[转置](@entry_id:142115) $A^\top$ 的零空间，即 $\mathcal{N}(A^\top) = \{ y \in \mathbb{R}^m : A^\top y = 0 \}$。它是目标空间 $\mathbb{R}^m$ 的一个子空间。[左零空间](@entry_id:150506)中的向量 $y$ 与 $A$ 的所有列向量正交。在生物学背景下，这意味着一个存在于左[零空间的基](@entry_id:194338)因权重向量 $y$ 与数据集中所有样本的表达谱都正交。

这四个子空间并非孤立存在，它们之间的关系由**[线性代数基本定理](@entry_id:190797)**深刻地揭示。该定理包含两个核心部分 [@problem_id:4578501]：

-   **正交性**：源空间 $\mathbb{R}^n$ 可以被分解为行空间和[零空间](@entry_id:171336)这两个相互正交的[子空间之和](@entry_id:180324)，记为 $\mathbb{R}^n = \mathcal{C}(A^\top) \oplus \mathcal{N}(A)$。同样，目标空间 $\mathbb{R}^m$ 可以被分解为列空间和左零空间这两个相互正交的[子空间之和](@entry_id:180324)，记为 $\mathbb{R}^m = \mathcal{C}(A) \oplus \mathcal{N}(A^\top)$。这里的 $\oplus$ 表示直和，意味着空间中的任何向量都可以唯一地表示为来自两个子空间的向量之和。

-   **维度**：这四个子空间的维度由[矩阵的秩](@entry_id:155507) $r = \operatorname{rank}(A)$ 唯一确定。一个基础性的结论是，行秩等于列秩，因此 $\dim \mathcal{C}(A) = \dim \mathcal{C}(A^\top) = r$。根据**[秩-零度定理](@entry_id:154441) (Rank-Nullity Theorem)**，我们有 $\dim \mathcal{N}(A) = n - r$ 以及 $\dim \mathcal{N}(A^\top) = m - r$。

这个定理为我们提供了一幅完整的几何图景：任何数据矩阵都将输入[空间分解](@entry_id:755142)为“信号”部分（行空间）和“噪声”或“冗余”部分（零空间），并将前者一对一地映射到输出空间的“信号”部分（列空间）。

### [线性模型](@entry_id:178302)：从理论到实践

线性模型是数据分析的基石，尤其在探索基因表达与临床表型之间关系的研究中。一个典型的线性模型表示为：

$y = X\beta + \varepsilon$

其中，$y \in \mathbb{R}^{n}$ 是 $n$ 个样本的响应向量（如某项临床指标），$X \in \mathbb{R}^{n \times p}$ 是[设计矩阵](@entry_id:165826)，其列代表 $p$ 个特征（如基因表达水平、临床协变量等），$\beta \in \mathbb{R}^{p}$ 是待估计的参数向量，而 $\varepsilon \in \mathbb{R}^{n}$ 是[随机误差](@entry_id:144890)向量 [@problem_id:4578494]。

为了从数据中可靠地估计参数 $\beta$，必须满足两个基本条件：**可识别性**和**无偏性**。

**可识别性 (Identifiability)** 是指模型参数 $\beta$ 是否可以被唯一确定。从线性代数的角度看，这要求从[参数空间](@entry_id:178581) $\mathbb{R}^p$ 到列空间 $\mathcal{C}(X)$ 的映射 $\beta \mapsto X\beta$ 是[单射](@entry_id:183792)的（一对一）。这意味着，如果 $X\beta_1 = X\beta_2$，则必须有 $\beta_1 = \beta_2$。这等价于方程 $X(\beta_1 - \beta_2) = 0$ 只有唯一的零解，即 $X$ 的零空间仅包含[零向量](@entry_id:156189)。这一条件的直接后果是，$X$ 的列向量必须是线性无关的。对于一个 $n \times p$ 的矩阵，这要求[矩阵的秩](@entry_id:155507)等于其列数，即 $\operatorname{rank}(X) = p$，并且必然有 $n \ge p$。

当 $\operatorname{rank}(X) = p$ 时，我们可以通过**[普通最小二乘法](@entry_id:137121) (Ordinary Least Squares, OLS)** 找到一个唯一的估计量 $\hat{\beta}$，它使得残差的平方和 $\lVert y - X\beta \rVert_2^2$ 最小化。该解由**正规方程 (normal equations)** $X^\top X \beta = X^\top y$ 给出。由于 $X$ 列满秩，矩阵 $X^\top X$ 是一个可逆的 $p \times p$ 矩阵，从而得到唯一的 OLS 解：

$\hat{\beta}_{OLS} = (X^\top X)^{-1} X^\top y$

值得注意的是，只有当表型向量 $y$ 恰好位于 $X$ 的列空间 $\mathcal{C}(X)$ 中时，方程组 $y = X\beta$ 才有精确解。在大多数实际情况中，由于噪声 $\varepsilon$ 的存在，$y$ 并不在 $\mathcal{C}(X)$ 中。此时，OLS 估计给出的预测值 $\hat{y} = X\hat{\beta}$ 是原始向量 $y$ 在子空间 $\mathcal{C}(X)$ 上的**[正交投影](@entry_id:144168)**。这是在由特征所张成的空间中，能够找到的与观测数据 $y$ 最接近的近似 [@problem_id:4578481]。

**无偏性 (Unbiasedness)** 要求估计量在期望意义上等于真实的参数值，即 $\mathbb{E}[\hat{\beta} | X] = \beta$。要保证 OLS 估计量的无偏性，我们需要对误差项 $\varepsilon$ 做出假设。将真实模型 $y = X\beta + \varepsilon$ 代入 OLS 解的表达式中，我们得到：

$\hat{\beta} = (X^\top X)^{-1} X^\top (X\beta + \varepsilon) = \beta + (X^\top X)^{-1} X^\top \varepsilon$

取以 $X$ 为条件的期望，得到 $\mathbb{E}[\hat{\beta} | X] = \beta + (X^\top X)^{-1} X^\top \mathbb{E}[\varepsilon | X]$。为了使 $\hat{\beta}$ 无偏，我们需要的只是 $(X^\top X)^{-1} X^\top \mathbb{E}[\varepsilon | X] = 0$。保证这一点的最弱、也是最标准的假设是**零条件均值假设**：$\mathbb{E}[\varepsilon | X] = 0$。这意味着误差项的期望不依赖于任何特征的值。值得强调的是，仅为保证无偏性，我们不需要关于误差分布（如正态性）或其方差结构（如等方差性）的更强假设 [@problem_id:4578494]。

### 解的稳定性：条件数与数值健康

在高维生物数据中，如[基因共表达网络](@entry_id:267805)，特征之间常常高度相关，即存在**多重共线性 (multicollinearity)**。这导致[设计矩阵](@entry_id:165826) $X$ 的列向量近似线性相关，使得矩阵 $X^\top X$ 接近奇异（不可逆）。这不仅挑战了参数的可识别性，也严重影响了解的[数值稳定性](@entry_id:146550)。

**条件数 (Condition Number)** 是衡量这种稳定性的关键指标。对于矩阵 $X$，其[谱范数](@entry_id:143091)条件数定义为其最大[奇异值](@entry_id:171660) $\sigma_{\max}$ 与最小非零[奇异值](@entry_id:171660) $\sigma_{\min}$ 之比：

$\kappa(X) = \frac{\sigma_{\max}(X)}{\sigma_{\min}(X)} = \frac{\sigma_1}{\sigma_p}$

其中我们假设 $\operatorname{rank}(X)=p$。条件数也可以表示为 $\kappa(X) = \lVert X \rVert_2 \lVert X^+ \rVert_2$，其中 $X^+$ 是 $X$ 的 Moore-Penrose 伪逆。从几何上看，条件数量化了[线性变换](@entry_id:143080) $X$ 在不同方向上拉伸程度的差异。一个非常大的条件数（即“病态”矩阵）意味着矩阵在某些方向上几乎将向量压缩为零，而在其他方向上则显著拉伸。

条件数直接控制着[最小二乘解](@entry_id:152054)对数据扰动的敏感性。考虑对观测向量 $y$ 的微小扰动 $\delta y$，解的[相对误差](@entry_id:147538) $\frac{\lVert \delta \beta \rVert_2}{\lVert \beta^\star \rVert_2}$ 的上界与 $\kappa(X)$ 成正比。具体来说 [@problem_id:4578518]：

-   如果系统是相容的（即 $y \in \mathcal{C}(X)$），[相对误差](@entry_id:147538)的放大满足：
    $\frac{\lVert\delta \boldsymbol{\beta}\rVert_{2}}{\lVert\boldsymbol{\beta}^{\star}\rVert_{2}} \lesssim \kappa(\mathbf{X})\,\frac{\lVert\delta \mathbf{y}\rVert_{2}}{\lVert\mathbf{y}\rVert_{2}}$

-   如果系统是不相容的（即 $y \notin \mathcal{C}(X)$），其残差不为零，则敏感性会进一步增加。设 $\theta$ 是 $y$ 与其在 $\mathcal{C}(X)$ 上的投影之间的夹角，则：
    $\frac{\lVert\delta \boldsymbol{\beta}\rVert_{2}}{\lVert\boldsymbol{\beta}^{\star}\rVert_{2}} \lesssim \kappa(\mathbf{X})\,\sec(\theta)\,\frac{\lVert\delta \mathbf{y}\rVert_{2}}{\lVert\mathbf{y}\rVert_{2}}$

当残差很大时（$\theta$ 接近 $\frac{\pi}{2}$），$\sec(\theta)$ 因子会急剧增大，使得解对噪声极为敏感。因此，一个大的条件数是模型不稳定的明确信号。

在数值计算中，病态性给求解[正规方程](@entry_id:142238) $X^\top X \beta = X^\top y$ 带来了巨大挑战。一种高效的求解方法是利用**[乔列斯基分解](@entry_id:166031) (Cholesky factorization)**，它将一个[对称正定矩阵](@entry_id:136714) $S$ 分解为一个下三角矩阵 $L$ 与其转置的乘积，$S=LL^\top$。然而，当 $S = X^\top X$ 由于多重共线性而接近奇异时，它便不再是严格正定的，标准的[乔列斯基分解](@entry_id:166031)算法会因为试图对非正数开方而失败 [@problem_id:4578509]。

为了解决这个问题，有两种常用的策略：

1.  **对角线[抖动](@entry_id:262829) (Diagonal Jittering)**：将原始矩阵 $S$ 替换为 $S' = S + \alpha I$，其中 $\alpha$ 是一个小的正数。这个操作确保了 $S'$ 是严格正定的，因为它的所有特征值都比 $S$ 的特征值大了 $\alpha$。因此，它的[最小特征值](@entry_id:177333)至少为 $\alpha > 0$。这使得[乔列斯基分解](@entry_id:166031)总是可以进行。更重要的是，这个看似简单的数值技巧在统计学中有深刻的含义：它等价于**[岭回归](@entry_id:140984) (Ridge Regression)**。在[岭回归](@entry_id:140984)中，惩罚项 $\lambda \lVert\beta\rVert_2^2$ 最终导致在求解时对 $X^\top X$ 加上一个 $\lambda I$ 的项，从而稳定了解。因此，对角线[抖动](@entry_id:262829)不仅是数值上的补救，其本身就是一种正则化方法 [@problem_id:4578509]。

2.  **带主元的[乔列斯基分解](@entry_id:166031) (Pivoted Cholesky Factorization)**：这是一种更精巧的算法，它在分解的每一步都重新排序矩阵的行和列，将当前最大的对角元素作为主元。这种策略不仅增强了[数值稳定性](@entry_id:146550)，还能在矩阵是半正定（即奇异）的情况下揭示其内在的秩。对于一个秩为 $k  p$ 的矩阵 $S$，带主元的[乔列斯基分解](@entry_id:166031)会生成一个秩为 $k$ 的低秩近似 $S \approx L_k L_k^\top$，其中 $L_k$ 是一个 $p \times k$ 的矩阵。这为处理高维数据中的[秩亏](@entry_id:754065)问题提供了一个有效工具 [@problem_id:4578509]。

### 度量向量与驱动稀疏性：范数的作用

在数据分析中，我们经常需要量化向量的大小或误差的幅度。**范数 (norm)** 提供了这种度量的数学框架。一个函数 $\lVert \cdot \rVert : \mathbb{R}^p \to \mathbb{R}$ 被称为范数，如果它满足以下三个公理 [@problem_id:4578459]：
1.  **[正定性](@entry_id:149643)**：$\lVert x \rVert \ge 0$，且 $\lVert x \rVert = 0$ 当且仅当 $x = 0$。
2.  **[绝对齐次性](@entry_id:274917)**：对任意实数 $\alpha$，$\lVert \alpha x \rVert = |\alpha| \lVert x \rVert$。
3.  **三角不等式**：$\lVert x + y \rVert \le \lVert x \rVert + \lVert y \rVert$。

在统计建模中，最常用的两种范数是 $\ell_2$ 范数（[欧几里得范数](@entry_id:172687)）和 $\ell_1$ 范数：
-   **$\ell_2$ 范数**: $\lVert x \rVert_2 = \sqrt{\sum_{i=1}^p x_i^2}$
-   **$\ell_1$ 范数**: $\lVert x \rVert_1 = \sum_{i=1}^p |x_i|$

这两种范数虽然在有限维空间中是等价的（即可以通过常数相互约束），但它们之间的关系依赖于空间的维度 $p$。具体而言，$\lVert x \rVert_2 \le \lVert x \rVert_1 \le \sqrt{p} \lVert x \rVert_2$。在 $p$ 非常大的高维设置中，$\sqrt{p}$ 因子变得不可忽略，导致这两种范数诱导的几何和统计性质截然不同 [@problem_id:4578459]。

这种差异在模型对**异常值 (outliers)** 的**鲁棒性 (robustness)** 上表现得尤为明显。OLS 最小化的是残差的 $\ell_2$ 范数平方，即 $\sum r_i^2$。一个巨大的残差项 $r_k$ 会被平方，从而在总损失中占据主导地位，迫使模型不惜以牺牲其他数据点的拟合为代价来减小这个异常值的影响。因此，$\ell_2$ 损失对异常值非常敏感。相比之下，$\ell_1$ 损失 $\sum |r_i|$ 对大残差的惩罚是线性的，因此更加鲁棒。从更专业的角度看，基于 $\ell_2$ 的估计器（如均值）的**[崩溃点](@entry_id:165994) (breakdown point)** 为0，意味着单个异常值就足以将其完全破坏；而基于 $\ell_1$ 的估计器（如[中位数](@entry_id:264877)）的[崩溃点](@entry_id:165994)可以高达 $0.5$，显示出卓越的鲁棒性 [@problem_id:4578459]。

范数的选择在[正则化方法](@entry_id:150559)中扮演着核心角色，最著名的例子是[岭回归](@entry_id:140984)和 [LASSO](@entry_id:751223) (Least Absolute Shrinkage and Selection Operator)。这两种方法都可以被看作是在最小化残差平方和的同时，对参数 $\beta$ 的范数施加约束：
-   **[岭回归](@entry_id:140984)**: $\min_{\beta} \lVert y - X\beta \rVert_2^2$ subject to $\lVert \beta \rVert_2 \le t$
-   **[LASSO](@entry_id:751223)**: $\min_{\beta} \lVert y - X\beta \rVert_2^2$ subject to $\lVert \beta \rVert_1 \le t$

这两种方法的根本区别在于其约束区域的几何形状 [@problem_id:4578486]。
-   岭回归的 $\ell_2$ 约束区域 $\lVert \beta \rVert_2 \le t$ 是一个**超球面 (hypersphere)**。它是一个光滑、处处可微（除原点外）且旋转不变的[凸体](@entry_id:183909)。当代表[损失函数](@entry_id:136784)的椭球[等值面](@entry_id:196027)从 OLS 解的位置扩张并首次接触这个球面时，接触点几乎总是位于球面的光滑部分，其所有坐标都非零。
-   [LASSO](@entry_id:751223) 的 $\ell_1$ 约束区域 $\lVert \beta \rVert_1 \le t$ 是一个**[交叉多胞体](@entry_id:748072) (cross-polytope)**，在二维空间中是菱形，三维空间中是正八面体。这个形状的显著特征是它有尖锐的“角点”（顶点）和“棱”。这些角点恰好位于坐标轴上，对应于某些系数为零的[稀疏解](@entry_id:187463)。当[损失函数](@entry_id:136784)的椭球等值面扩张时，它有相当大的概率首先触碰到这些角点之一，从而产生一个部分系数恰好为零的解 [@problem_id:4578459] [@problem_id:4578486]。

正是这种几何上的差异，赋予了 LASSO **稀疏性 (sparsity)** 的特性，使其能够同时进行[参数估计](@entry_id:139349)和变量选择。当面对一组高度相关的预测变量时，这种差异表现得更加突出：[岭回归](@entry_id:140984)倾向于将系数权重均等地分配给相关变量群组；而 LASSO 则倾向于从群组中选择一个有代表性的变量，并将其余变量的系数压缩至零，从而实现更具解释性的模型 [@problem_id:4578486]。

### 数据矩阵的分解：揭示潜在结构

矩阵分解是揭示数据内在低维结构的核心技术。通过将一个复杂的数据[矩阵分解](@entry_id:139760)为几个更简单、更具解释性的矩阵的乘积，我们可以提取出数据中的主要模式。

#### 主成分分析 (Principal Component Analysis, PCA)

PCA 是一种广泛应用的无监督[降维技术](@entry_id:169164)，其目标是找到数据中方差最大的正交方向，即**主成分 (principal components)**。从线性代数的角度，PCA 可以通过两种等价的方式来定义和计算 [@problem_id:4578504]：

1.  **基于协方差矩阵的[特征分解](@entry_id:181333)**：对于一个中心化的数据矩阵 $X \in \mathbb{R}^{n \times p}$（$n$ 个样本，$p$ 个特征），首先计算样本协方差矩阵 $S = \frac{1}{n-1} X^\top X$。这是一个 $p \times p$ 的[对称半正定矩阵](@entry_id:163376)。PCA 的主方向（也称为**载荷 (loadings)**）是 $S$ 的特征向量，按其对应的特征值大小降序排列。特征值 $\lambda_i$ 本身代表了数据在对应[主方向](@entry_id:276187) $v_i$ 上的方差。将原始数据投影到这些[主方向](@entry_id:276187)上，得到的坐标被称为**得分 (scores)**，得分矩阵为 $Z = XV$。

2.  **基于数据矩阵的[奇异值分解 (SVD)](@entry_id:172448)**：对中心化的数据矩阵 $X$ 进行 SVD 分解，$X = U \Sigma V^\top$。其中 $U$ ($n \times n$) 和 $V$ ($p \times p$) 是[正交矩阵](@entry_id:169220)，分别包含[左奇异向量](@entry_id:751233)和右奇异向量；$\Sigma$ ($n \times p$) 是一个[对角矩阵](@entry_id:637782)，其对角线上的元素是[奇异值](@entry_id:171660) $\sigma_i$。

这两种方法在数学上是严格等价的。将 SVD 表达式代入协方差矩阵的定义：
$S = \frac{1}{n-1} (U \Sigma V^\top)^\top (U \Sigma V^\top) = \frac{1}{n-1} V (\Sigma^\top \Sigma) V^\top$
这个表达式正是 $S$ 的[特征分解](@entry_id:181333)形式。通过比较可以得出如下精确的对应关系：
-   **[主方向](@entry_id:276187)/载荷**：协方差矩阵的特征向量（$V$ 的列）就是数据矩阵的**[右奇异向量](@entry_id:754365)**。
-   **解释的方差**：特征值 $\lambda_i$ 与[奇异值](@entry_id:171660) $\sigma_i$ 的关系为 $\lambda_i = \frac{\sigma_i^2}{n-1}$。
-   **得分**：PCA 的得分矩阵 $Z = XV = (U\Sigma V^\top)V = U\Sigma$。

这种等价性普遍成立，与矩阵的维度或秩无关。SVD 不仅提供了一种数值上更稳定的计算 PCA 的方法，还深刻地揭示了 PCA 与[四个基本子空间](@entry_id:154834)的关系：$V$ 的前 $r$ 列构成了行空间 $\mathcal{R}(X)$ 的一组标准正交基，而 $U$ 的前 $r$ 列构成了[列空间](@entry_id:156444) $\mathcal{C}(X)$ 的一组[标准正交基](@entry_id:147779)，其中 $r$ 是[矩阵的秩](@entry_id:155507) [@problem_id:4578481]。

#### [非负矩阵分解](@entry_id:635553) (Non-negative Matrix Factorization, NMF)

在许多生物学应用中，如基因表达计数或图像像素强度，数据本身是非负的。在这种情况下，施加非负约束的 NMF 分解 $X \approx WH$（其中 $X, W, H$ 的所有元素均 $\ge 0$）往往能产生比 PCA 更具物理解释性的结果。

NMF 的核心在于其**基于部分的表示 (parts-based representation)** [@problem_id:4578499]。与允许正负系数相消的 PCA 不同，NMF 的重建过程是纯粹**加性的 (additive)**。考虑一个基因表达矩阵 $X$（$n$ 个基因 $\times$ $p$ 个样本），NMF 将其近似分解为 $W \in \mathbb{R}_+^{n \times r}$ 和 $H \in \mathbb{R}_+^{r \times p}$。矩阵 $W$ 的每一列 $w_k$ 可以被看作一个“元基因”或“基因模块”，它是一个包含 $n$ 个基因的非负表达模式。矩阵 $H$ 的每一列 $h_j$ 包含了样本 $j$ 对这 $r$ 个基因模块的“激活”或“贡献”权重。

根据[矩阵乘法](@entry_id:156035)，对样本 $j$ 的表达谱 $x_j$ 的重构为：
$x_j \approx \sum_{k=1}^r h_{kj} w_k$

由于所有的权重 $h_{kj}$ 和模块 $w_k$ 中的元素都是非负的，每个样本的表达谱被表示为基础基因模块的**加性组合**。这里不存在减法或抵消，就像用不同的积木块（部分）以不同的数量（权重）搭建一个物体。这种加性结构使得分解出的 $W$ 和 $H$ 更容易被赋予生物学意义，例如，$W$ 的列可能对应于特定的生物通路，而 $H$ 的行则揭示了这些通路在不同样本或疾病状态下的活性。

### 超越线性：[核技巧](@entry_id:144768)

线性方法虽然强大，但无法捕捉数据中复杂的非线性关系。**[核技巧](@entry_id:144768) (kernel trick)** 是一种巧妙的策略，它允许我们在一个极高维甚至无限维的[特征空间](@entry_id:638014)中应用线性算法，而无需显式地计算数据点在该空间中的坐标。

其核心思想是将算法中所有涉及数据点的计算都用**[内积](@entry_id:750660) (inner product)** 的形式表达。然后，我们引入一个**核函数 (kernel function)** $k(x_i, x_j)$，它直接计算原始空间中两个数据点 $x_i, x_j$ 经过某个[非线性映射](@entry_id:272931) $\phi$ 后的像 $\phi(x_i), \phi(x_j)$ 在高维[特征空间](@entry_id:638014)中的[内积](@entry_id:750660)：

$k(x_i, x_j) = \langle \phi(x_i), \phi(x_j) \rangle$

这样，我们就可以在特征空间中进行计算，而完全绕开了可能非常复杂甚至无法计算的映射 $\phi$。

一个函数 $k$ 能否成为一个合法的核函数，其关键在于它所生成的**核矩阵 (kernel matrix)** $K$ 的性质。对于任意一组数据点 $\{x_1, \dots, x_n\}$，其核矩阵 $K$ 的元素为 $K_{ij} = k(x_i, x_j)$。一个基本定理（**Mercer 定理**的推广，即 **Moore-Aronszajn 定理**）指出，一个[对称函数](@entry_id:177113) $k$ 是一个有效的[核函数](@entry_id:145324)，当且仅当它生成的任意核矩阵 $K$ 都是**对称半正定 (symmetric positive semidefinite, PSD)** 的 [@problem_id:4578465]。一个矩阵 $K$ 是 PSD 的，意味着对于任意非[零向量](@entry_id:156189) $z \in \mathbb{R}^n$，都有 $z^\top K z \ge 0$。

PSD 性质与[内积](@entry_id:750660)之间存在着深刻的联系：一个[实对称矩阵](@entry_id:192806)是 PSD 的，当且仅当它是一个**[格拉姆矩阵](@entry_id:203297) (Gram matrix)**，即一个由某组向量的[内积](@entry_id:750660)构成的矩阵。我们可以通过谱分解来构造性地证明这一点：任何对称 PSD 矩阵 $K$ 都可以分解为 $K = Q \Sigma Q^\top$，其中 $Q$ 是正交矩阵，$\Sigma$ 是包含非负特征值的[对角矩阵](@entry_id:637782)。我们可以定义一组向量 $\psi_i = \Sigma^{1/2} Q^\top e_i$，其中 $e_i$ 是[标准基向量](@entry_id:152417)。直接计算可得 $\langle \psi_i, \psi_j \rangle = \psi_i^\top \psi_j = e_i^\top (Q \Sigma Q^\top) e_j = K_{ij}$。这表明，任何对称 PSD 矩阵 $K$ 都可以被看作是某组向量（这里是 $\{\psi_i\}$）的[格拉姆矩阵](@entry_id:203297) [@problem_id:4578465]。

综上所述，[核技巧](@entry_id:144768)通过[核函数](@entry_id:145324)将非线性问题转化为高维[特征空间](@entry_id:638014)中的线性问题。只要我们选择的核函数能保证生成的核矩阵是 PSD 的，我们就保证了背后存在一个合法的[内积空间](@entry_id:271570)（称为**[再生核希尔伯特空间](@entry_id:633928), RKHS**），从而保证了所有基于[内积](@entry_id:750660)的线性算法（如[支持向量机](@entry_id:172128)、核 PCA）的有效性。这为处理生物医学数据中的复杂非线性模式提供了强大而严谨的数学框架。