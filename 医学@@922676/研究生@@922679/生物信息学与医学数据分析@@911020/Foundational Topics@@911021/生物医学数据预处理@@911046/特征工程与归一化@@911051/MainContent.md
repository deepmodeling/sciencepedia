## 引言
在生物信息学和医学数据分析领域，原始[高通量数据](@entry_id:275748)中蕴含着丰富的生物学信息，但同时也夹杂着由实验流程、仪器设备和批次差异等引入的技术性变异。这些非生物学因素若不加处理，会严重干扰甚至扭曲数据分析结果，导致错误的[科学推断](@entry_id:155119)。特征工程与归一化，正是应对这一挑战、从噪声中提炼真实信号的关键步骤，是连接原始数据与可靠生物学洞见的桥梁。

本文旨在系统性地阐述特征工程与归一化的核心理论与实践。我们将从根本上回答“为何需要归一化”以及“如何选择正确的方法”。读者将通过本文学习到：

*   在**第一章：原理与机制**中，我们将深入探讨测量的基本理论，剖析不同归一化策略（如[分位数归一化](@entry_id:267331)、TPM、[方差稳定变换](@entry_id:273381)）背后的统计学原理，并揭示其在处理RNA-seq、单细胞测序等数据时的适用性与假设。

*   在**第二章：应用与跨学科连接**中，我们将展示这些技术在实际分析中的关键作用，从[无监督学习](@entry_id:160566)（如PCA）中的尺度效应，到监督学习（如正则化模型）中的公平惩罚，再到跨组学领域（如基因组学、代谢组学）的特异性校正策略。

*   在**第三章：动手实践**中，您将通过具体的编程练习，亲手实现核心归一化算法，将理论知识转化为实践技能。

通过这一结构化的学习路径，本文将引导您掌握[数据预处理](@entry_id:197920)的精髓，为后续的[统计建模](@entry_id:272466)与生物学发现奠定坚实的基础。

## 原理与机制

本章旨在深入探讨生物信息学和医学数据分析中[特征工程](@entry_id:174925)的核心组成部分——归一化（Normalization）与[数据转换](@entry_id:170268)（Transformation）的基本原理和机制。我们将从测量的基本理论出发，阐述不同类型技术变异的校正策略，并重点剖析高通量测序数据和[微阵列数据分析](@entry_id:172617)中主流归一化方法的统计学基础、适用场景及其内在假设。本章的目标是不仅介绍“如何做”，更要阐明“为何如此做”，从而为后续的统计建模与推断奠定坚实的基础。

### 测量的基本原则与数据校正的分类

在进行任何定量分析之前，我们必须首先理解我们所处理的数据的性质。统计学中的[测量理论](@entry_id:153616)为我们提供了一个严谨的框架，用于根据数据所蕴含信息的层次来对其进行分类。错误地理解数据尺度（scale of measurement）可能会导致采用不恰当的变换和归一化方法，从而得出错误的科学结论。

#### 测量尺度与容许变换

根据心理物理学家 Stanley Smith Stevens 的经典分类，测量尺度可分为四种类型，每种类型都定义了何种数学运算和陈述是有意义的，并相应地规定了何种数据变换是“容许的”，即变换后不改变数据所蕴含的真实信息。

*   **名义尺度 (Nominal Scale)**：数据仅作为标签或类别使用，如样本来源地、疾病亚型。唯一有意义的陈述是“相等”或“不等”。任何一对一的重编码（relabeling）都是容许的。

*   **[序数](@entry_id:150084)尺度 (Ordinal Scale)**：数据具有顺序或等级关系，如疾病分期（I, II, III, IV）。我们可以说一个值大于或小于另一个值，但不能量化其差异的大小。任何严格单调递增的[函数变换](@entry_id:141095)都是容许的，因为它保留了顺序关系。

*   **区间尺度 (Interval Scale)**：数据不仅有顺序，而且任意两点之间的差异是可比较的，即具有恒定的度量单位。一个典型的例子是摄氏温度，我们可以说 20°C 到 30°C 的温差等于 30°C 到 40°C 的温差。然而，区间尺度没有一个绝对的、非任意的零点（0°C 并不意味着没有热能）。因此，比例陈述（如“40°C 是 20°C 的两倍热”）是无意义的。其容许的变换是正仿射变换（positive affine transformation），形式为 $x' = ax + b$，其中 $a > 0$。

*   **比率尺度 (Ratio Scale)**：这是信息最丰富的尺度，它拥有区间尺度的所有性质，并额外拥有一个绝对的、有物理意义的零点，代表着所测属性的“不存在”。例如，身高、体重、分子计数等。由于存在绝对零点，值的比例是有意义的（例如，“A 的身高是 B 的两倍”）。其容许的变换是[相似变换](@entry_id:152935)（similarity transformation），即乘以一个正常数，$x' = ax$，其中 $a > 0$。

理解这些尺度至关重要，因为它直接约束了我们能对数据进行的归一化操作。例如，一个常见的场景是处理从光电容积描记法（Photoplethysmography, PPG）信号中提取的脉搏振幅特征。假设信号处理流程确保振幅 $A_{s,t}$ 总是非负，且 $A_{s,t} = 0$ 明确表示“没有脉搏成分”。这就建立了一个绝对零点。进一步假设，由于传感器与皮肤接触等因素，每个受试者 $s$ 的测量都受到一个未知的、个体特异的正增益因子 $g_s$ 的影响，即 $A_{s,t} = g_s \theta_{s,t}$，其中 $\theta_{s,t}$ 是真实的生理振幅。在这种情况下，$A_{s,t}$ 属于 **比率尺度**，因为两个振幅的比值 $A_{s,t_1} / A_{s,t_2} = \theta_{s,t_1} / \theta_{s,t_2}$ 不受增益因子 $g_s$ 的影响，是具有生理意义的。

如果研究者错误地将其视为区间尺度数据，他可能会采用 Z-score 标准化，即 $A'_{s,t} = (A_{s,t} - \bar{A}_s) / \sigma_s$。这种包含加/减运算（中心化）的操作对于比率尺度数据是不容许的。它会破坏数据的比例结构，并可能产生无物理意义的负值。正确的归一化策略应当与比率尺度的性质相符，例如，通过[乘性](@entry_id:187940)归一化（如 $A_{s,t} / \bar{A}_s$）来消除 $g_s$ 的影响，或者通过[对数变换](@entry_id:267035)将乘性效应变为加性效应（$\ln(A_{s,t}) = \ln(g_s) + \ln(\theta_{s,t})$），然后在[对数空间](@entry_id:270258)中进行中心化处理 [@problem_id:4562763]。

#### 物理校正与统计归一化

在生物医学数据分析中，消除技术变异的步骤可以根据其动机和依据分为两大类：物理校正和统计归一化。

*   **[背景校正](@entry_id:200834) (Background Correction)**：这是一种基于检测仪器物理模型的校正。许多检测平台（如[微阵列](@entry_id:270888)、ELISA）会产生一个基线信号，即使在没有目标分析物的情况下也存在。例如，一个观测信号 $X$ 可以建模为 $X = B + S + \varepsilon$，其中 $B$ 是背景信号，$S$ 是与分析物相关的真实信号，$\varepsilon$ 是随机噪声。通过测量“空白”样本（blanks），我们可以估计背景信号的强度 $\hat{B}$，然后从所有测量值中减去它：$X^{\text{bc}} = X - \hat{B}$。这是一种基于物理原理的加性校正。

*   **校准 (Calibration)**：校准的目的是将仪器输出的任意信号[单位转换](@entry_id:136593)为有物理意义的浓度单位（如 $\text{mol/L}$）。这通过测量一系列已知浓度的标准品（standards）来建立信号与浓度之间的函数关系，即校准曲线 $X = f(C)$。然后，通过求解[反函数](@entry_id:141256) $\hat{C} = f^{-1}(X)$ 来估计未知样本的浓度。校准是一种更深层次的物理校正，旨在将数据锚定到绝对尺度上。

*   **统计归一化 (Statistical Normalization)**：与上述两种方法不同，统计归一化通常不依赖于特定的物理模型或外部[标准品](@entry_id:754189)。它的目标是调整一批样本的统计分布，使其具有可比性，其假设是观测到的分布差异主要是由技术因素而[非生物因素](@entry_id:203288)引起的。统计归一化可进一步细分为：
    *   **位置归一化 (Location Normalization)**：校正分布的中心趋势（如均值或中位数）的偏移。例如，从每个样本的数据中减去该样本的[中位数](@entry_id:264877)，使得所有样本的中位数都对齐到零。
    *   **尺度归一化 (Scale Normalization)**：校正分布的离散程度（如标准差或[四分位距](@entry_id:169909)）的差异。例如，将中心化后的数据除以其标准差（即 Z-score 变换），使得所有样本的方差都对齐到 1。
    *   **形状归一化 (Shape Normalization)**：这是更全面的归一化，旨在对齐整个分布的形状，而不仅仅是前两个矩（均值和方差）。一个典型的方法是 **[分位数归一化](@entry_id:267331) (Quantile Normalization)**，它通过一个非线性的单调变换，强制使得每个样本的[经验累积分布函数](@entry_id:167083)（ECDF）变得完全相同。

在一个跨多个实验室进行的 ELISA 研究中，区分这些概念至关重要 [@problem_id:4562807]。首先，利用空白孔读数进行 **[背景校正](@entry_id:200834)** 是去除仪器基线噪音的第一步。其次，如果使用了标准品，可以进行 **校准** 将荧[光强度](@entry_id:177094)转换为[蛋白质浓度](@entry_id:191958)。最后，即使经过校准，不同实验室的数据分布可能仍然存在系统性的偏移（位置）、伸缩（尺度）乃至[偏度](@entry_id:178163)（形状）上的差异，这些被称为“批次效应”（batch effects）。此时，就需要采用 **统计归一化** 方法（如中心化、标准化或[分位数归一化](@entry_id:267331)）来消除这些残余的技术变异，以便进行跨实验室的有效比较。

### 高通量测序数据的归一化

高通量测序（HTS），特别是 RNA 测序（RNA-seq），已经成为功能基因组学研究的基石。然而，RNA-seq 产生的原始读数（counts）数据本质上是相对的，其比较和解释受到多种技术因素的强烈影响。本节将深入探讨这些因素，并介绍解决它们的主流归一化方法。

#### 组合性问题与稳健尺度因子估计

[RNA-seq](@entry_id:140811) 的一个核心挑战是 **组合性 (Compositionality)**。测序实验通常对每个样本（文库）产生一个固定容量的总读数。因此，我们观测到的是每个基因的读数在总读数中所占的 *比例*，而非其 *绝对* 丰度。这意味着所有基因的[相对丰度](@entry_id:754219)之和必须为 1（或 100%）。这个看似无害的约束却有着深刻的后果。

设想一个场景：在两种实验条件下（A 和 B），绝大多数基因（如 4500 个）的绝对分子丰度保持不变，但一小部分基因（如 500 个免疫反应基因）在条件 B 中的绝对丰度急剧上调了 10 倍。这导致条件 B 中细胞内的总 RNA 分子数量（$\Lambda_B$）显著高于条件 A（$\Lambda_A$）。如果两个条件的测序深度（总读数 $N$）大致相同，那么在条件 B 中，那 500 个上调基因会“吃掉”更大比例的测序读数。由于总比例必须为 1，这意味着那 4500 个绝对丰度不变的基因，其 *相对比例* 必定会下降。

如果此时我们采用简单的全局缩放归一化方法，如计算每百万读数中的计数（Counts Per Million, CPM），即用每个基因的读数除以该样本的总读数，那么我们将观察到：
1.  那 4500 个绝对丰度不变的基因，由于其相对比例下降，会被错误地判断为“下调”。
2.  那 500 个真正上调 10 倍的基因，其观测到的[倍数变化](@entry_id:272598)（fold-change）将被严重压缩，因为分母（总读数）中也包含了它们自身的大幅增加。

这个例子清晰地揭示了组合性偏差：少数基因的剧烈变化会扭曲所有其他基因的相对测量值 [@problem_id:4562817]。

为了克服这一问题，现代 [RNA-seq](@entry_id:140811) 分析流程采用更稳健的方法来估计样本间的[尺度因子](@entry_id:266678)（size factors）。这些方法的核心思想是：假设大部分基因的表达水平在样本间是不变的，[尺度因子](@entry_id:266678)的估计应该基于这些“稳定”的基因，而忽略或降低那些剧烈变化的基因的影响。

*   **[中位数](@entry_id:264877)比例法 (Median-of-Ratios Method)**：这是 [DESeq2](@entry_id:167268) 包中使用的策略。它的步骤如下：首先，为每个基因 $i$ 计算其在所有样本中的[几何平均数](@entry_id:275527) $g_i = (\prod_{k=1}^m x_{ik})^{1/m}$。这个 $g_i$ 可以被看作是一个所有样本共享的“伪参考基因”的表达水平。然后，对于每个样本 $j$，计算每个基因的读数 $x_{ij}$ 与其对应伪参考 $g_i$ 的比值 $x_{ij}/g_i$。理想情况下，对于非差异表达的基因，这个比值应该约等于样本 $j$ 的真实尺度因子 $s_j$（相对于伪参考的尺度）。最后，取这些比值在样本 $j$ 内所有基因中的中位数，作为该样本的[尺度因子](@entry_id:266678)估计值 $\hat{s}_j = \text{median}_i (x_{ij}/g_i)$。由于[中位数](@entry_id:264877)对离群值（即[差异表达](@entry_id:748396)强烈的基因）具有很强的稳健性，这种方法能够有效抵抗[组合性](@entry_id:637804)偏差 [@problem_id:4562789]。该估计量具有良好的统计性质，例如，它对基因层面的任意乘性重缩放是不变的。

*   **M-值的加权截尾均值 (Trimmed Mean of M-values, TMM)**：这是 edgeR 包中采用的策略。它在一个参考样本和一个待测样本之间进行成对比较。它首先计算每个基因的 M-值（log-fold-change）和 A-值（average log-abundance）。其核心假设是，大部分基因不是差异表达的，因此它们的 M-值应该集中在某个常数附近（该常数反映了[组合性](@entry_id:637804)偏差）。TMM 方法通过截尾（trimming）来移除 M-值和 A-值极端（即差异表达最显著和表达量过高/过低的基因）的基因，然后对剩余基因的 M-值计算加权平均数，以得到对组合性偏差的[稳健估计](@entry_id:261282)。权重是基于每个 M-值估计的逆方差来计算的，给予计数高、变异小的基因更大的权重。从[稳健统计学](@entry_id:270055)的角度看，截尾操作赋予了 TMM 估计器一个非零的“[崩溃点](@entry_id:165994)”（breakdown point），意味着只要极端差异表达的基因比例低于某个阈值，估计结果就不会被它们“污染” [@problem_id:4562771]。

这些稳健的方法，通过识别并依赖于基因表达谱中的稳定部分，能够更准确地估计出反映[测序深度](@entry_id:178191)和文库组成的[尺度因子](@entry_id:266678)，从而为后续的[差异表达分析](@entry_id:266370)提供无偏的基准 [@problem_id:4562817]。

#### 转录本长度与 [TPM](@entry_id:170576)

除了文库大小（library size）和组合性，RNA-seq 数据中还存在另一个重要的偏倚来源：**转录本长度**。在标准的 RNA-seq 流程中，长的转录本比较短的转录本更容易产生更多的测序片段。因此，即使两个基因的分子丰度（molar concentration）完全相同，仅仅因为它们的转录本长度不同，它们获得的读数也会不同。

为了同时校正文库大小和转录本长度，发展出了多种归一化指标：

*   **CPM (Counts Per Million)**: $\text{CPM}_i = \frac{c_i}{N} \times 10^6$。其中 $c_i$ 是基因 $i$ 的读数，$N$ 是总读数。CPM 只校正了文库大小，没有校正转录本长度。

*   **RPKM/FPKM (Reads/Fragments Per Kilobase of transcript per Million mapped reads)**: $\text{RPKM}_i = \frac{10^9 c_i}{L_i N}$。其中 $L_i$ 是基因 $i$ 的长度（以碱基为单位）。这个指标试图同时校正文库大小 ($N$) 和转录本长度 ($L_i$) 。然而，RPKM 有一个微妙但严重的问题。它的分母 $L_i N$ 中包含了总读数 $N$，而 $N$ 本身的值又受到样本中所有基因的长度和丰度的影响。如前文所述，如果一个样本中富含长转录本，其总 RNA 质量会偏高，这会影响到 RPKM 值的分布，导致不同样本间的 RPKM 值不可直接比较。

*   **[TPM](@entry_id:170576) (Transcripts Per Million)**: [TPM](@entry_id:170576) 的计算分为两步，这使得它在概念上更清晰且性质更优。
    1.  首先，对每个基因，用其读数除以其长度，得到一个“每碱基读数率”：$r_i = c_i / L_i$。这一步校正了[长度偏倚](@entry_id:269579)。
    2.  然后，将所有基因的 $r_i$ 值加和，得到一个总率 $R = \sum_j r_j$。
    3.  最后，将每个基因的 $r_i$ 用总率 $R$ 进行缩放，并乘以一百万：$\text{TPM}_i = \frac{r_i}{R} \times 10^6 = \frac{c_i/L_i}{\sum_j (c_j/L_j)} \times 10^6$。

[TPM](@entry_id:170576) 的关键优势在于，它是一个真正的相对摩尔丰度的估计。可以证明，在理想的测序模型下，$\text{TPM}_i \propto \theta_i / \sum_j \theta_j$，其中 $\theta_i$ 是基因 $i$ 的摩尔丰度。这意味着 TPM 值直接反映了某个基因的转录本数量占总转录本数量的比例。这个比例在不同样本间是可比的，因为它不受样本间平均转录本长度或总 RNA 质量差异的影响。相比之下，RPKM 值则依赖于样本的总 RNA 质量，因此在样本成分（如平均转录本长度）不同时，RPKM 值的比较是有偏的 [@problem_id:4562800]。因此，当目标是比较不同样本间基因的相对表达丰度时，TPM 是比 RPKM/FPKM 更优越的指标。

#### 单细胞数据的稀疏性与零膨胀

单细胞 RNA 测序（scRNA-seq）将 [RNA-seq](@entry_id:140811) 的分辨率提升到了单个细胞的层面，但也带来了新的挑战，其中最突出的就是数据的 **高度稀疏性 (sparsity)**，表现为计数矩阵中存在大量的零值。理解这些零值的来源对于正确建模和归一化至关重要。

scRNA-seq 数据中的零值主要有两种来源：
1.  **抽样零 (Sampling Zeros)**：这源于单细胞中 mRNA 分子的数量本身就很少，加上逆转录和 PCR 扩增过程中的随机捕获效率有限。一个基因可能在细胞中是低表达的（例如，每个细胞只有几个拷贝），在测序过程中，这些分子很可能一个都没有被捕获和测序，从而产生一个零计数。这种零是泊松或负二项分布等计数模型在低均值下自然产生的结果。
2.  **结构零 (Structural Zeros)**：这代表了真正的生物学或技术性“关闭”状态。例如，一个基因可能在一个细胞亚群中是完全不表达的（生物学原因），或者由于技术原因（如基因在测序过程中完全脱落，即 dropout），其表达信号完全丢失。这种零超出了标准计数分布模型的预期，需要用所谓的 **零膨胀 (Zero-Inflation)** 模型来描述。[零膨胀模型](@entry_id:756817)通常是一个[混合模型](@entry_id:266571)，它假设数据来自两部分：一部分是一个总是产生零的过程（“膨胀”部分），另一部分是一个标准的计数分布（如泊松或[负二项分布](@entry_id:262151)）。

区分这两种零至关重要。对于使用[唯一分子标识符](@entry_id:192673)（UMI）的现代 droplet-based scRNA-seq 技术，越来越多的证据表明，其数据中的绝大多数零都可以被一个标准的负二项（NB）分布模型很好地解释，而不需要额外的零膨胀部分。这意味着高比例的零值主要是由低表达和有限的抽样深度导致的“稀疏性”，而非“零膨胀”。在这种情况下，错误地使用[零膨胀模型](@entry_id:756817)（如 ZINB）反而会因为模型过度[参数化](@entry_id:265163)而引入偏差，影响对均值和[离散度](@entry_id:168823)等关键参数的估计。相反，在一些较早的、非 UMI 的全长转录本测序技术中，技术性 dropout 问题可能更严重，导致真正的零膨胀现象 [@problem_id:4562776]。

因此，一个严谨的 [scRNA-seq](@entry_id:155798) 分析流程应当：
*   通过[模型比较](@entry_id:266577)检验（如 Vuong's test）来判断数据是否真的存在零膨胀，而不是先验地假设存在。
*   如果数据可以用 NB 模型充分描述（如许多 UMI 数据），则应基于 NB 模型进行后续处理，例如，在估计了细胞特异的尺度因子 $s_i$ 后，可以应用从 NB 模型推导出的[方差稳定变换](@entry_id:273381)。
*   如果数据确实显示出显著的零膨胀，那么在进行[特征选择](@entry_id:177971)或[差异表达分析](@entry_id:266370)时，考虑使用 ZINB 模型来分别对[计数过程](@entry_id:260664)和零膨胀过程进行建模是合理的 [@problem_id:4562776]。

### 方差、异方差性与数据变换

许多标准的统计方法，如[线性回归](@entry_id:142318)、[主成分分析](@entry_id:145395)（PCA），都隐含着一个假设，即数据的方差是恒定的（[同方差性](@entry_id:634679)，homoscedasticity）。然而，对于计数数据，这是一个通常不被满足的假设。

#### 异方差性与负[二项模型](@entry_id:275034)

计数数据的方差通常与其均值相关，这种现象称为 **异方差性 (heteroscedasticity)**。对于泊松分布，方差严格等于均值。在 [RNA-seq](@entry_id:140811) 数据中，由于存在额外的生物学变异（例如，不同细胞或个体间基因表达水平的内在波动），方差通常大于均值，这种现象称为 **过离散 (overdispersion)**。

一个广泛用于描述这种过离散计数数据的模型是 **负二项 (Negative Binomial, NB)** 分布。NB 分布可以看作是一个 Gamma-Poisson 混合模型：我们假设一个基因的表达率 $\Lambda$ 本身是一个服从 Gamma 分布的随机变量，而观测到的计数 $X$ 则是在给定 $\Lambda$ 的条件下服从 Poisson 分布。通过这个层级模型，可以推导出 $X$ 的[边际分布](@entry_id:264862)，即 NB 分布。其方差与均值 $\mu$ 的关系可以表示为：
$$ \operatorname{Var}(X) = \mu + \phi \mu^2 $$
其中，$\mu$ 项反映了泊松分布带来的抽样方差（技术方差），而 $\phi \mu^2$ 项则反映了 Gamma 分布带来的额外生物学方差。$\phi$ 被称为[离散度](@entry_id:168823)参数（dispersion parameter），它量化了生物学变异的程度。这个二次关系清晰地表明，表达水平越高的基因，其方差也越大，这是 [RNA-seq](@entry_id:140811) 数据异方差性的典型特征 [@problem_id:4562742]。

#### [方差稳定变换](@entry_id:273381) (Variance-Stabilizing Transformation, VST)

为了能在这些异方差数据上应用依赖于同方差假设的统计方法，我们需要对数据进行 **[方差稳定变换](@entry_id:273381)**。VST 的目标是找到一个函数 $g(\cdot)$，使得变换后的变量 $g(X)$ 的[方差近似](@entry_id:268585)为一个与均值 $\mu$ 无关的常数。

利用 **delta 方法**，我们可以推导出这样的变换。Delta 方法指出，对于一个均值为 $\mu$、方差为 $\sigma^2(\mu)$ 的随机变量 $X$，其变换后 $g(X)$ 的[方差近似](@entry_id:268585)为 $(g'(\mu))^2 \sigma^2(\mu)$。为了使这个方差为常数（如 1），我们需要 $g'(\mu) = 1/\sigma(\mu)$。对于 NB 分布，$\sigma(\mu) = \sqrt{\mu + \phi\mu^2}$，因此我们需要求解积分 $g(\mu) = \int (t + \phi t^2)^{-1/2} dt$。求解这个积分可以得到：
$$ g(\mu) = \frac{2}{\sqrt{\phi}} \operatorname{arcsinh}(\sqrt{\phi\mu}) $$
这个反双曲[正弦变换](@entry_id:754896)就是 NB 分布的一个[方差稳定变换](@entry_id:273381) [@problem_id:4562742]。在实践中，在对原始计数进行尺度因子归一化后，应用这种 VST 或类似的[对数变换](@entry_id:267035)（如 $\log_2(x+1)$），可以将[数据转换](@entry_id:170268)到一个方差更稳定、分布更对称的尺度上，从而更适合进行 PCA、聚类或[线性建模](@entry_id:171589)等下游分析。

#### 案例研究：甲基化数据的 Beta-值与 M-值

方差与均值依赖的问题不仅限于测序数据。在 [Illumina](@entry_id:201471) Infinium DNA 甲基化[微阵列](@entry_id:270888)数据中，也存在类似的选择。对于每个 CpG位点，我们测量甲基化信号 $S_m$ 和非甲基化信号 $S_u$。通常有两种方式来量化甲基化水平：

*   **Beta-值 ($\beta$)**: 定义为比例 $\beta = S_m / (S_m + S_u)$。它直观地表示甲基化位点的百分比，范围在 $[0, 1]$。然而，作为比例值，其方差天然地依赖于均值（类似于[二项分布](@entry_id:141181)，方差在均值为 0.5 时最大，在 0 或 1 时最小），且其分布在均值靠近边界时会表现出强烈的[偏态](@entry_id:178163)。这种[异方差性](@entry_id:136378)和非正态性使其不适合直接作为高斯线性模型的响应变量。

*   **M-值 (M)**: 定义为对数比值 $M = \log_2(S_m / S_u)$。在一个合理的[生成模型](@entry_id:177561)（例如，$\log S_m$ 和 $\log S_u$ 服从[方差近似](@entry_id:268585)相等的正态分布）下，M-值（即两个对数正态变量的对数比）近似服从一个方差不依赖于均值 methylation level 的正态分布。换句话说，M-值在统计学上是 **近似同方差和正态分布的**。

因此，尽管 Beta-值在生物学解释上更直观，但 M-值因其更优越的统计学性质（满足[线性模型](@entry_id:178302)的假设）而被强烈推荐用于差异甲基化分析等统计推断任务 [@problem_id:4562782]。这再次强调了在特征工程中，选择一种变换不仅仅是为了方便，更是为了使数据符合后续[统计模型](@entry_id:755400)的基本假设。