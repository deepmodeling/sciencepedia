## 引言
[下一代测序](@entry_id:141347)（NGS）技术已经彻底改变了生物医学研究，但其产出的海量原始数据中不可避免地夹杂着各种技术性误差和系统性偏倚。若不加以识别和校正，这些瑕疵将严重影响下游分析的可靠性，甚至导致错误的科学结论。因此，对NGS数据进行严格的质量控制（QC）和预处理，是确保任何基因组学、转录组学或[表观基因组学](@entry_id:175415)研究获得可信结果的基石。本文旨在系统性地解决这一关键问题，为研究人员提供一套从原始数据到“分析就绪”数据的完整知识框架。

本文将分为三个核心章节，带领读者逐步深入NGS数据质控的世界。在“原理与机制”一章中，我们将剖析NGS数据的基本构成，如[FASTQ](@entry_id:201775)文件和Phred质量分，并详细阐述测序错误、GC偏倚、PCR重复及索引跳跃等常见偏差的来源与机制，同时介绍相应的校正策略，如接头切除、碱基[质量分数](@entry_id:161575)重校准（BQSR）和独特分子标识符（UMI）的应用。随后，在“应用与跨学科连接”一章中，我们将展示这些原理在真实世界中的应用，探讨如何针对[全基因组](@entry_id:195052)、[转录组](@entry_id:274025)（RNA-Seq）、[表观基因组](@entry_id:272005)（ChIP-Seq, ATAC-Seq）以及临床[液体活检](@entry_id:267934)等不同场景，设计和解读特定的QC指标。最后，通过“动手实践”部分，读者将有机会将理论应用于实际问题，加深对核心算法的理解。通过这一结构化的学习路径，读者将能够全面掌握NGS数据质量控制与预处理的关键技能。

## 原理与机制

继上一章对[下一代测序](@entry_id:141347)（NGS）技术的基本介绍之后，本章将深入探讨其数据的基本构成、内在的质量度量、常见的技术性偏差，以及在下游分析开始前纠正这些偏差所必需的关键预处理步骤。对这些原理与机制的深刻理解，是确保任何基因组学、[转录组学](@entry_id:139549)或[表观基因组学](@entry_id:175415)研究获得准确可靠结论的基石。我们将遵循从原始数据到分析就绪（analysis-ready）数据的逻辑流程，系统性地剖析每个环节的核心概念。

### 原始测[序数](@entry_id:150084)据的剖析：[FASTQ](@entry_id:201775)文件与质量评分

NGS仪器产生的原始数据通常以一种称为 **[FASTQ](@entry_id:201775)** 的文本格式存储。这种格式是后续所有生物信息学分析的起点，其结构简洁而信息丰富。每一个测序读段（read）在[FASTQ](@entry_id:201775)文件中都由四行记录构成 [@problem_id:4590222]。

1.  **第一行**：以 `@` 字符开头，其后跟随一个唯一的读段标识符以及可选的描述信息。这些信息通常包含了测序仪、运行编号、flowcell（流通池）上的位置坐标等[元数据](@entry_id:275500)。
2.  **第二行**：包含了原始的核苷酸序列（A, C, G, T）。如果碱基无法被明确识别，则会用 `N` 表示。
3.  **第三行**：以 `+` 字符开头，其后可以重复第一行的标识符，但通常为空。
4.  **第四行**：包含了与第二行[核苷](@entry_id:195320)酸序列逐碱基对应的质量评分字符串。该字符串的长度必须与[核苷](@entry_id:195320)酸序列的长度完全相同。

第四行中的质量评分是评估测[序数](@entry_id:150084)据可靠性的核心。这个评分系统被称为 **Phred质量分 (Phred quality score)**，用 $Q$ 表示。它量化了碱基检出（base-calling）过程中发生错误的概率 $p_e$。其定义是基于对数尺度的：

$Q = -10 \log_{10}(p_e)$

这个对数关系非常直观：$Q$ 值每增加10，代表碱基的准确率提高一个数量级 [@problem_id:4590226]。例如：
*   $Q=10$ 意味着[错误概率](@entry_id:267618)为 $10^{-1}$ (或 $0.1$)，即碱基的准确率为 $90\%$。
*   $Q=20$ 意味着[错误概率](@entry_id:267618)为 $10^{-2}$ (或 $0.01$)，准确率为 $99\%$。
*   $Q=30$ 意味着[错误概率](@entry_id:267618)为 $10^{-3}$ (或 $0.001$)，准确率为 $99.9\%$。
*   $Q=40$ 意味着[错误概率](@entry_id:267618)为 $10^{-4}$ (或 $0.0001$)，准确率为 $99.99\%$。

由于[FASTQ](@entry_id:201775)是文本文件，整数形式的 $Q$ 值需要通过转换为[ASCII](@entry_id:163687)字符来存储。这是通过在一个整数偏移量（offset）的基础上进行编码实现的。然而，不同的测序技术发展阶段和平台采用了不同的偏移量，这导致了历史上一度混乱的局面。主要的两种编码标准是 **Phred+33** 和 **Phred+64** [@problem_id:4590222]。

*   **Sanger / [Illumina](@entry_id:201471) 1.8+ (Phred+33)**：这是当前最广泛使用的标准，由Sanger测序时代建立，并被[Illumina](@entry_id:201471)自1.8版本流水线后采用。它使用[ASCII](@entry_id:163687)偏移量33。例如，$Q=0$ 对应[ASCII](@entry_id:163687)码为33的字符 `!`。
*   **[Illumina](@entry_id:201471) 1.3-1.7 (Phred+64)**：较早版本的[Illumina](@entry_id:201471)流水线使用[ASCII](@entry_id:163687)偏移量64。在此标准下，$Q=0$ 对应[ASCII](@entry_id:163687)码为64的字符 `@`。

错误地解析编码标准会对质量评估产生灾难性的后果。假设一个质量字符串中包含字符 `I`（其[ASCII](@entry_id:163687)码为73）。如果数据被正确地按Phred+33解析，其对应的质量分是 $Q = 73 - 33 = 40$，这是一个极高的质量，代表万分之一的出错率。然而，如果它被错误地当作Phred+64数据解析，质量分则为 $Q = 73 - 64 = 9$，这是一个非常低的质量，出错率高达约 $12.6\%$（$10^{-0.9}$）。两者相差31个质量单位，对应的错误率估计相差超过三个数量级（$10^{3.1} \approx 1259$ 倍）。因此，在任何分析开始之前，正确识别并统一[FASTQ](@entry_id:201775)文件的质量编码标准是至关重要的第一步。

### NGS数据中的误差来源与系统性偏倚

理想情况下，测序读段应是基因组真实序列的无偏、随机样本。然而，在实际操作中，从文库构建到测序成像的每一步都可能引入各种误差和系统性偏倚。

#### 测序错误谱

不同测序平台由于其核心化学原理和信号获取方式的差异，表现出截然不同的错误特征 [@problem_id:4590226]。

*   **[Illumina](@entry_id:201471)（[边合成边测序](@entry_id:185545)，Sequencing-by-Synthesis）**: 该技术以其高准确率著称，其错误模式主要为 **替换错误 (substitution errors)**。这是因为其测序过程是离散的、逐周期进行的。在每个化学循环中，[可逆终止子](@entry_id:177254)确保只有一个荧光标记的[核苷](@entry_id:195320)酸被掺入。这种严格的“一次一碱基”机制使得[插入和删除](@entry_id:178621)（indels）变得极为罕见。主要的替换错误来源于：
    1.  **移相/脱相 (Phasing/Dephasing)**：一个簇（cluster）中的少数DNA链与主循环失去同步，导致在后续循环中产生混合信号而被误读。
    2.  **[信号串扰](@entry_id:188529) (Crosstalk)**：四种[荧光基团](@entry_id:202467)的发射光谱存在重叠，一个碱基的强信号可能被误判为另一个碱基的弱信号。

*   **[PacBio](@entry_id:264261) (SMRT) / Oxford Nanopore (ONT)**：这些长读长技术通过实时监测分子过程来生成连续的时间信号，因此它们的错误谱以 **[插入和删除](@entry_id:178621)错误 (indels)** 为主。
    1.  **[PacBio](@entry_id:264261) SMRT（[单分子实时测序](@entry_id:183138)）**：通过观测DNA聚合酶掺入荧光核苷酸时发出的光脉冲来进行测序。在均聚物（homopolymer，如 `AAAAA`）区域，聚合酶可能快速连续掺入多个碱基，产生一个单一的长脉冲。精确判断这个脉冲对应于几个碱基是困难的，误判会导致插入或删除。
    2.  **ONT（[纳米孔测序](@entry_id:136932)）**：DNA单链穿过一个纳米孔，通过监测[离子电流](@entry_id:170309)的变化来推断序列。电流水平对应于特定长度的[k-mer](@entry_id:166084)（例如5-mer）。碱基检出软件必须对这个连续信号进行事件分割和分类。分割错误（合并或分裂事件）直接导致删除或插入。同样，均聚物区域会产生一个长期稳定的电流，精确计时以推断其长度是indel错误的主要来源。

#### 文库构建产生的非理想信号

**接头污染 (Adapter Contamination)**

在短读长测序的文库制备中，DNA片段的两端会被连接上平台特异性的 **接头 (adapters)**。测序仪执行固定数量的化学循环，产生一个固定长度的读段（例如，$r=150$ bp）。如果一个文库分子的插入片段长度 $L_{ins}$ 小于读段长度 $r$，那么测序聚合酶在读完整个插入片段后不会停止，而是会继续“读穿”(read-through)到下游连接的接头序列上。这导致读段的 $3'$ 末端包含了非基因组来源的接头序列，即 **$3'$ 接头污染** [@problem_id:4590241]。在[双端测序](@entry_id:272784)中，如果 $L_{ins}  r$，那么两个读段都会发生读穿。如果 $r \le L_{ins}  2r$，两个读段会在插入片段内部重叠，但不会产生接头污染。因此，接头污染的明确条件是 $L_{ins}  r$。像FastQC这样的质量控制工具，通过检测在读段 $3'$ 端显著富集的已知接头序列来识别这一问题。

**GC偏倚 (GC Bias)**

**GC偏倚** 是指测序覆盖度（coverage）与基因组局部区域的GC含量（Guanine-Cytosine fraction）之间存在的系统性依赖关系 [@problem_id:4590245]。这种偏倚主要源于PCR扩增过程和[Illumina](@entry_id:201471)的成簇过程。GC含量过高或过低的DNA片段在PCR扩增时的效率较低，并且在flowcell上形成簇的效率也可能不同。这通常导致一个“苦脸”模式：与[GC含量](@entry_id:275315)适中（约40-50%）的区域相比，GC含量极高或极低的区域获得的读段数量显著偏少。

这种系统性的覆盖度不均一会严重影响下游分析：
*   **[变异检测](@entry_id:177461)**：在覆盖度低的区域，检测杂合变异（特别是单核苷酸变异SNV）的灵敏度会下降。例如，如果一个变异检出算法要求至少观察到3个支持变异的等位基因读段，那么在一个由于GC偏倚导致平均覆盖度从30x降至12x的区域，满足这一要求的概率将显著降低 [@problem_id:4590245]。
*   **[拷贝数变异 (CNV)](@entry_id:150333) 分析**：基于[读段深度](@entry_id:178601)的CNV检测方法假设，在排除了其他偏倚后，[读段深度](@entry_id:178601)与基因拷贝数成正比。如果不进行GC偏倚校正，算法会将由[GC含量](@entry_id:275315)驱动的系统性深度下降错误地解释为拷贝数丢失（deletion），或将深度上升解释为拷贝数增加（duplication），从而产生大量与真实生物学无关的[假阳性](@entry_id:635878)结果。

#### 多重测序与扩增产生的非理想信号

**索引跳跃 (Index Hopping)**

为了提高测序通量和降低成本，通常会将来自不同样本的文库混合（pooling）在一起进行 **多重测序 (multiplexing)**。为了在测序后区分这些样本，每个样本的文库会被标记上一个独特的短DNA序列，称为 **样本索引 (sample index)** 或条形码 (barcode)。测序后，**样本拆分 (demultiplexing)** 过程会读取每个读段的索引序列，并将其分配给其来源样本 [@problem_id:4590211]。

然而，尤其是在使用共享试剂和Exclusion Amplification (ExAmp)化学的[Illumina](@entry_id:201471)高通量平台（如NovaSeq）上，会发生一种称为 **索引跳跃** 的现象。其主要机制是，文库池中残留的、未连接到插入片段上的游离带索引的接头，在flowcell上的成簇扩增过程中，错误地[退火](@entry_id:159359)到另一个样本的DNA模板上并被延伸，从而“嫁接”了一个错误的索引。使用 **唯一双索引 (Unique Dual Indexing, UDI)** 是检测这种现象的关键策略。UDI要求文库分子的两端都有一个独特的索引。一个索引跳跃事件通常只影响一端，产生的“$i7$端索引来自样本A，$i5$端索引来自样本B”的嵌合读段，因为它不匹配任何一个预设的有效索引对，从而在拆分时被识别并丢弃。一个读段被错误地分配给另一个样本，需要两端索引同时发生跳跃并恰好组成另一个样本的有效索引对，这是一个概率极低的事件 [@problem_id:4590211]。

**PCR重复与光学重复 (PCR and Optical Duplicates)**

在文库制备过程中，PCR扩增会从一个原始DNA模板分子产生多个相同的拷贝。这些拷贝在测序后产生的读段被称为 **PCR重复**。此外，在基于成像的测序仪上，一个物理上的DNA簇可能因为过于靠近或信号过强，被成像系统错误地识别为两个或多个独立的簇，这些簇产生的读段被称为 **光学重复** [@problem_id:4590248]。

这两种重复都代表了冗余信息，它们源于同一个原始分子。如果不加以处理，它们会人为地夸大支持某个特定等位基因的证据，从而导致在变异检测中出现[假阳性](@entry_id:635878)，并扭曲[等位基因频率](@entry_id:146872)的估计。因此，标记和移除重复读段是质量控制的一个关键步骤。

### [数据预处理](@entry_id:197920)与质量校正策略

识别出上述误差和偏倚后，下一步就是通过一系列计算步骤来清理和校正数据，使其达到“分析就绪”状态。

#### 样本拆分与接头切除

对于多重测序的数据，第一步是根据索引序列进行 **样本拆分**。这个过程还包括处理索引序列本身的测序错误，通常允许与预设索引有1个碱基的错配（[Hamming距离](@entry_id:157657)≤1），前提是索引设计保证了任意两个有效索引之间的[最小Hamming距离](@entry_id:272322)至少为3，从而避免[歧义](@entry_id:276744) [@problem_id:4590211]。

接下来是 **接头切除 (Adapter Trimming)**。专门的工具会搜索读段的 $3'$ 端是否存在已知的接头序列，如果找到，则将接头部分以及之后的所有序列从读段中移除。这是解决由短插入片段引起的接头污染问题的直接方法 [@problem_id:4590241]。

#### 比对及其相关的质量度量

[数据清理](@entry_id:748218)后，读段需要被 **比对 (align)** 到[参考基因组](@entry_id:269221)上，以确定它们在基因组中的原始位置。比对的质量由一个不同于碱基质量的度量来评估，即 **[比对质量](@entry_id:170584) (Mapping Quality, MAPQ)**。

**MAPQ** 是一个Phred标度的分数，它量化了比对位置是错误的后验概率 [@problem_id:4590229]。
$MAPQ = -10 \log_{10}(P(\text{比对位置错误}))$

一个高MAPQ值（如40或更高）意味着比对位置非常可信，而一个低MAP[Q值](@entry_id:265045)（如0或3）则表示该读段可能以同等的[可能性比](@entry_id:170863)对到基因组的多个位置。这种情况在[参考基因组](@entry_id:269221)的 **重复区域** 中非常常见。一个读段即使所有碱基的质量分都非常高（即序列本身非常准确），但如果其序列在基因组中多次出现，其MAPQ值也会很低。对于[双端测序](@entry_id:272784)，比对算法会利用配对读段的预期距离和方向信息来辅助比对，这能极大地帮助解决单一读段的比对模糊性，并为整个读段对（fragment）计算出一个更可靠的MAPQ值 [@problem_id:4590229]。

#### 校正比对数据中的系统性误差

**碱基质量分数重校准 (Base Quality Score Recalibration, BQSR)**

测序仪报告的原始碱基质量分（即[FASTQ](@entry_id:201775)文件中的$Q$值）虽然有用，但常常存在系统性偏差。例如，读段末端的碱基质量通常会系统性地偏高或偏低，或者在特定的三核苷酸（tri-nucleotide）上下文中，错误率会固定地偏离预期。**BQSR** 是一个精细的校正过程，旨在使报告的质量分更接近于真实的错误率 [@problem_id:4590247]。

该过程通常分两步：
1.  **建立误差模型**：算法会扫描所有的比对数据，并根据一系列 **协变量 (covariates)** 将碱基进行分层。典型的协变量包括：报告的原始质量分、读段中的位置（测序循环数）、以及其前后的[核苷](@entry_id:195320)酸上下文（例如，dinucleotide或trinucleotide context）。在某些情况下，flowcell上的物理位置（tile）也会被用作协变量。为了避免将真实的生物学变异误判为测序错误，这个建模过程会忽略或排除那些位于已知[多态性](@entry_id:159475)位点（如dbSNP）的碱基。
2.  **应用校正**：基于上一步建立的经验误差模型，算法会为每个分层（即每一种协变量的组合）计算一个校正值。然后，它会第二次遍历所有读段，将这个校正值（在对数尺度上是加性的）应用到每个碱基的原始质量分上，生成一个经过重校准的、更准确的质量分。

经过BQSR后，更准确的碱基质量分不仅能提高下游变异检测的准确性，还能被比对器用于计算更精确的MAPQ值 [@problem_id:4590229]。

**[Indel](@entry_id:173062)区域局部重比对 (Local Realignment Around Indels)**

如前所述，[启发式](@entry_id:261307)的短读长比对器为了避免高昂的[空位罚分](@entry_id:176259)（gap penalty），可能会将真实的短indel错误地表示为一簇连续的错配（mismatches）。在比对文件中，这会呈现为许多读段在同一位置附近开始出现一连串的“假SNPs”。

**[Indel](@entry_id:173062)局部重比对** 是一种旨在纠正这类比对错误的后处理步骤 [@problem_id:4590237]。其工作原理是：
1.  识别可能含有indel的区域，通常是那些富集了错配或已有indel信号的区域。
2.  在每个目标区域内，收集所有覆盖该区域的读段。
3.  从这些读段中，动态地生成几个候选的局部单倍型（haplotypes），包括参考序列以及包含不同indel的变异序列。
4.  使用计算上更密集但更精确的比对算法（如[Smith-Waterman](@entry_id:175582)），将该区域的所有读段与每个候选单倍型进行重比对。
5.  最终，将每个读段的比对更新为其与最佳匹配单倍型的比对结果。这个过程能有效地“清理”比对，将原来分散的错配簇重新解释为一个单一、干净的indel，并由所有支持该变异的读段一致地表示。

#### 重复标记与UMI的威力

**基于坐标的重复标记**

在没有UMI的情况下，标记PCR和光学重复的标准方法是基于比对坐标 [@problem_id:4590248]。对于单端读段，具有相同起始坐标和链方向的读段被视为重复。对于[双端读段](@entry_id:176330)，具有相同的外侧起始坐标（即Read 1的起始位置和Read 2的终止位置）和链方向的读段对被视为重复。在被标记为重复的读段组中，通常只保留[比对质量](@entry_id:170584)最高的一个进行下游分析。

然而，这种纯粹基于坐标的方法存在一个根本性的 **混淆问题**。它无法区分真正的技术性重复（PCR/光学重复）和 **巧合性碰撞**——即两个或多个来自不同原始DNA分子的片段，由于随机断裂或断裂偏好性（fragmentation bias），恰好具有完全相同的起始和终止坐标。这个问题类似于经典的“[生日问题](@entry_id:268167)”：随着测序深度的增加，发生这种巧合碰撞的概率会以近似平方的速度增长 ($n^2$)，导致越来越多的真实生物学信号被错误地当作战器丢弃 [@problem_id:4590248]。

**独特分子标识符 (Unique Molecular Identifiers, UMIs)**

**UMI** 为解决上述重复标记的混淆问题提供了根本性的解决方案 [@problem_id:4590208]。UMI是一段短的（通常为6-12 bp）随机DNA序列，它在PCR扩增 **之前** 被连接到每一个原始DNA分子上。这样，源自同一个原始分子的所有PCR扩增产物都将共享完全相同的UMI序列。

利用UMI进行去重，过程就变得更加精确：
1.  首先，按比对坐标将读段分组。
2.  在每个坐标组内，再根据UMI序列进行细分。
3.  具有相同起始/终止坐标 **和** 相同UMI序列的读段，才被视为真正的技术性重复，并被合并成一个共识读段（consensus read）或只保留一个代表。

这种方法能够精确地计算出每一个原始DNA分子的数量，极大地提高了定量分析（如基因表达定量、稀有[变异检测](@entry_id:177461)）的准确性。然而，UMI自身也面临挑战：UMI序列在测序过程中可能发生错误；不同的原始分子也可能由于巧合（UMI碰撞）而被标记上相同的UMI。先进的[UMI去重](@entry_id:756286)工具通过构建UMI序列网络图来应对这些挑战：它们连接[Hamming距离](@entry_id:157657)为1的UMI，并基于每个UMI的支持读段数和碱基质量信息，使用[统计模型](@entry_id:755400)来判断一个低频UMI是源于一个高频UMI的测序错误，还是一个独立的、真实的分子，从而在纠正测序错误和避免过度合并之间取得平衡 [@problem_id:4590208]。

总之，从理解[FASTQ](@entry_id:201775)文件的基本结构到应用复杂的UMI纠错算法，NGS数据的质量控制和预处理是一个多步骤、环环相扣的过程。每一步都旨在识别和纠正特定的技术偏差，为最终从海量测序数据中提取可靠生物学见解奠定坚实的基础。