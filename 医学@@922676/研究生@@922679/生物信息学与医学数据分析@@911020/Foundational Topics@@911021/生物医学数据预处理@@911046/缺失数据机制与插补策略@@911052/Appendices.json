{"hands_on_practices": [{"introduction": "在执行多重插补后，关键步骤是如何正确地合并多个完整数据集的分析结果，以获得单一、有效的统计推断。本练习将引导你推导并应用 Rubin 合并规则，这是多重插补后进行统计推断的基石。通过这个练习，你将深入理解总不确定性是如何分解为插补内部方差（反映抽样误差）和插补之间方差（反映缺失数据带来的额外不确定性）的 [@problem_id:4584865]。", "problem": "一项临床生物标志物研究使用逻辑回归检验基因表达生物标志物与二元疾病结局之间的关联。由于生物标志物测量值存在缺失，该分析在随机缺失（MAR）假设下，使用适当插补法进行多重插补（MI）。设 $Q$ 表示一个标量回归系数（对数优势比标度）。在 MI 的贝叶斯后验预测框架下，每个补全的数据集 $j \\in \\{1,\\dots,m\\}$ 产生一个估计值 $Q^{(j)}$ 及其对应的完整数据方差估计值 $U^{(j)}$。假设 $Q$ 具有大样本近似正态性，且各次插补相互独立。\n\n任务：\n1) 从给定观测数据的 $Q$ 的全方差定律出发，推导一个 MI 点估计总方差的大样本估计量，该估计量结合了平均完整数据方差和插补间变异性，并识别出用于解释仅使用 $m$ 次插补所引入的蒙特卡洛误差的有限 $m$ 校正项。清晰解释插补内方差和插补间方差在分解中的不同作用，并分析插补次数 $m$ 如何影响蒙特卡洛误差分量。\n\n2) 在一项使用 $m=10$ 次插补的具体分析中，标量估计值和完整数据方差如下：\n- $Q^{(1)}=0.72$, $U^{(1)}=0.038$\n- $Q^{(2)}=0.80$, $U^{(2)}=0.041$\n- $Q^{(3)}=0.85$, $U^{(3)}=0.040$\n- $Q^{(4)}=0.90$, $U^{(4)}=0.039$\n- $Q^{(5)}=0.88$, $U^{(5)}=0.042$\n- $Q^{(6)}=0.79$, $U^{(6)}=0.037$\n- $Q^{(7)}=0.92$, $U^{(7)}=0.040$\n- $Q^{(8)}=0.86$, $U^{(8)}=0.039$\n- $Q^{(9)}=0.81$, $U^{(9)}=0.040$\n- $Q^{(10)}=0.87$, $U^{(10)}=0.041$\n\n计算缺失信息分数，其定义为由数据缺失引起的额外方差与第 (1) 部分推导出的总方差之比。将最终数值答案表示为小数，并四舍五入至 $4$ 位有效数字。最终答案中请勿使用百分号。", "solution": "本问题探讨了合并多重插补（MI）结果的原则，这是一种处理统计分析中缺失数据的标准技术。解答按要求分为两部分。\n\n### 第 1 部分：总方差估计量的推导与分析\n\n设 $Y$ 表示完整数据集，它可以被划分为观测数据 $Y_{obs}$ 和缺失数据 $Y_{mis}$。设 $Q$ 为我们感兴趣的标量，在本例中是逻辑回归模型的一个回归系数。MI 的目标是基于 $Y_{obs}$ 对 $Q$ 进行推断。\n\n在 MI 的贝叶斯框架中，我们感兴趣的是给定观测数据的 $Q$ 的后验分布 $p(Q|Y_{obs})$。该分布的均值是 $Q$ 的点估计，其方差 $\\text{Var}(Q|Y_{obs})$ 代表了关于 $Q$ 的总不确定性。\n\n多重插补过程包括三个步骤：\n1.  对缺失值进行 $m$ 次插补，从缺失数据的后验预测分布 $p(Y_{mis}|Y_{obs})$ 中抽样，以创建 $m$ 个补全的数据集 $\\{(Y_{obs}, Y_{mis}^{(1)}), ..., (Y_{obs}, Y_{mis}^{(m)})\\}$。\n2.  使用标准的完整数据分析方法，分析这 $m$ 个补全的数据集中的每一个。对于每个数据集 $j$，这会产生一个点估计 $Q^{(j)}$ 及其相关的方差估计 $U^{(j)}$。\n3.  将这 $m$ 个结果合并为一组估计值和置信区间。$Q$ 的 MI 点估计是个体估计值的平均值：\n    $$ \\bar{Q}_m = \\frac{1}{m} \\sum_{j=1}^{m} Q^{(j)} $$\n\n为了推导该估计量的总方差，我们从全方差定律入手，将其应用于给定 $Y_{obs}$ 的 $Q$ 的后验分布：\n$$ \\text{Var}(Q|Y_{obs}) = E[\\text{Var}(Q|Y_{obs}, Y_{mis})] + \\text{Var}(E[Q|Y_{obs}, Y_{mis}]) $$\n此表达式中的期望和方差是针对给定 $Y_{obs}$ 的 $Y_{mis}$ 的后验分布计算的。我们来分析每一项：\n\n1.  **插补内方差**：第一项 $E[\\text{Var}(Q|Y_{obs}, Y_{mis})]$ 代表平均完整数据方差。量 $\\text{Var}(Q|Y_{obs}, Y_{mis})$ 是在数据完整（即我们知道 $Y_{mis}$）的情况下 $Q$ 的方差。这是由有限抽样引起的不确定性的来源，即使在没有缺失数据的情况下也存在。在单个补全数据集 $j$ 中，此方差的估计量是 $U^{(j)}$。通过对 $m$ 次插补的这些值取平均，我们得到期望完整数据方差的估计：\n    $$ \\bar{U}_m = \\frac{1}{m} \\sum_{j=1}^{m} U^{(j)} $$\n    这被称为**插补内方差**，因为它捕捉了每个补全数据集*内部*的平均抽样方差。\n\n2.  **插补间方差**：第二项 $\\text{Var}(E[Q|Y_{obs}, Y_{mis}])$ 代表因缺失数据 $Y_{mis}$ 未知而产生的额外方差。点估计 $E[Q|Y_{obs}, Y_{mis}]$ 会根据为 $Y_{mis}$ 插补的具体值而变化。因此，这些点估计在 $Y_{mis}$ 的后验预测分布上的方差量化了由数据缺失引起的不确定性。我们通过 $m$ 个点估计 $Q^{(j)}$ 在其均值 $\\bar{Q}_m$ 周围的变异性来估计它：\n    $$ B_m = \\frac{1}{m-1} \\sum_{j=1}^{m} (Q^{(j)} - \\bar{Q}_m)^2 $$\n    这是**插补间方差**。它直接反映了因数据缺失而引入的额外不确定性。如果没有缺失数据，所有的 $Q^{(j)}$ 都将相同，且 $B_m$ 将为 $0$。\n\n综合这些分量，$Q$ 的总后验方差估计为插补内方差和插补间方差之和：$T \\approx \\bar{U}_m + B_m$。然而，我们使用的是 MI 估计量 $\\bar{Q}_m$，它本身是基于有限次数 $m$ 的插补得到的样本均值。这引入了额外的蒙特卡洛模拟误差。估计量 $\\bar{Q}_m$ 的方差不仅包括 $Q$ 的后验方差，还包括使用有限插补样本所产生的方差。\n\n用于构建置信区间和进行假设检验的 $\\bar{Q}_m$ 的方差估计量必须考虑到这一点。因此，总方差估计量，记为 $T_m$，由鲁宾法则（Rubin's rule）给出：\n$$ T_m = \\bar{U}_m + B_m + \\frac{B_m}{m} = \\bar{U}_m + \\left(1 + \\frac{1}{m}\\right)B_m $$\n在这里，项 $\\frac{B_m}{m}$ 是**有限 $m$ 校正**。它明确地解释了因使用有限次数插补而非无限次插补而引入的蒙特卡洛误差。\n\n插补次数 $m$ 直接影响这个蒙特卡洛误差分量。随着 $m$ 的增加，项 $\\frac{B_m}{m}$ 减小并趋近于 $0$。对于无限大的 $m$，总方差将是 $T_\\infty = \\bar{U}_\\infty + B_\\infty$，这代表了给定观测数据的 $Q$ 的真实后验方差。较小的 $m$ 值会导致较大的蒙特卡洛误差，从而导致总方差的估计精度较低（即方差较高）且可重复性较差。较大的 $m$ 值会减少这种模拟误差，从而产生更稳定和可靠的推断。\n\n### 第 2 部分：缺失信息分数的计算\n\n我们得到了来自 $m=10$ 次插补的数据。我们首先计算必要的组成部分：平均插补内方差（$\\bar{U}_{10}$）和插补间方差（$B_{10}$）。\n\nMI 点估计为：\n$$ \\bar{Q}_{10} = \\frac{1}{10} \\sum_{j=1}^{10} Q^{(j)} = \\frac{1}{10}(0.72 + 0.80 + 0.85 + 0.90 + 0.88 + 0.79 + 0.92 + 0.86 + 0.81 + 0.87) = \\frac{8.40}{10} = 0.84 $$\n\n平均插补内方差为：\n$$ \\bar{U}_{10} = \\frac{1}{10} \\sum_{j=1}^{10} U^{(j)} = \\frac{1}{10}(0.038 + 0.041 + 0.040 + 0.039 + 0.042 + 0.037 + 0.040 + 0.039 + 0.040 + 0.041) = \\frac{0.397}{10} = 0.0397 $$\n\n插补间方差为：\n$$ B_{10} = \\frac{1}{10-1} \\sum_{j=1}^{10} (Q^{(j)} - \\bar{Q}_{10})^2 $$\n平方偏差之和为：\n$$ \\sum (Q^{(j)} - 0.84)^2 = (-0.12)^2 + (-0.04)^2 + (0.01)^2 + (0.06)^2 + (0.04)^2 + (-0.05)^2 + (0.08)^2 + (0.02)^2 + (-0.03)^2 + (0.03)^2 $$\n$$ \\sum (Q^{(j)} - 0.84)^2 = 0.0144 + 0.0016 + 0.0001 + 0.0036 + 0.0016 + 0.0025 + 0.0064 + 0.0004 + 0.0009 + 0.0009 = 0.0324 $$\n因此，\n$$ B_{10} = \\frac{0.0324}{9} = 0.0036 $$\n\n接下来，我们使用第 1 部分中推导出的公式，并代入 $m=10$ 来计算总方差：\n$$ T_{10} = \\bar{U}_{10} + \\left(1 + \\frac{1}{10}\\right)B_{10} = 0.0397 + (1.1)(0.0036) = 0.0397 + 0.00396 = 0.04366 $$\n\n问题将缺失信息分数（FMI）定义为由数据缺失引起的额外方差与推导出的总方差之比。总方差是 $T_{10}$。“由数据缺失引起的额外方差”是总方差中如果数据完整就会消失的部分。这对应于插补间方差和蒙特卡洛校正项之和，即 $(1 + 1/m)B_m$。\n\n因此，缺失信息分数的计算如下：\n$$ \\text{FMI} = \\frac{(1 + 1/m)B_m}{T_m} = \\frac{(1 + 1/10)B_{10}}{T_{10}} $$\n$$ \\text{FMI} = \\frac{0.00396}{0.04366} \\approx 0.09070087036... $$\n\n将此结果四舍五入至 $4$ 位有效数字，我们得到 $0.09070$。", "answer": "$$\\boxed{0.09070}$$", "id": "4584865"}, {"introduction": "当面临基因组学或蛋白质组学等高维数据集时，传统的基于回归的多重插补方法可能由于变量过多而失效。本练习介绍了一种基于低秩假设的现代插补方法——矩阵补全。你将学习如何通过凸优化来填补数据矩阵中的缺失值，这是一种强大的机器学习技术，特别适用于揭示高维数据中潜在的结构性信息 [@problem_id:4584852]。", "problem": "考虑一个多组学数据矩阵 $X \\in \\mathbb{R}^{n \\times p}$，其条目表示标准化的分子测量值（例如，横跨 $n$ 个样本和 $p$ 个基因的转录本丰度）。分析师观察到一个部分观测矩阵 $Y \\in \\mathbb{R}^{n \\times p}$，其中索引集 $\\Omega^{c}$ 上的条目缺失，而已观测到的索引收集在 $\\Omega \\subset \\{1,\\dots,n\\} \\times \\{1,\\dots,p\\}$ 中。到已观测索引上的投影算子定义为：如果 $(i,j) \\in \\Omega$，则 $P_{\\Omega}(Z)_{ij} = Z_{ij}$；否则 $P_{\\Omega}(Z)_{ij} = 0$。假设缺失机制是完全随机缺失（MCAR）或随机缺失（MAR），因此似然分解证明了从已观测条目进行一致性估计是合理的。在组学中标准的潜在因子模型（由于少量生物学和技术潜在因子，$X$ 近似于低秩）以及非相干性的可识别性条件（$X$ 的奇异向量不过度与坐标轴对齐）下，从第一性原理出发，构建一个凸优化问题，该问题用凸代理替换直接的秩最小化，惩罚模型复杂度，并且仅对已观测条目进行拟合。明确说明建模假设（近似低秩和非相干性），以及它们在证明高维组学数据的松弛和可识别性合理性方面所起的作用。\n\n然后，为了通过近端方法具体化插补过程，考虑一个玩具组学矩阵 $Y \\in \\mathbb{R}^{2 \\times 2}$，其条目为 $Y_{11} = 4$，$Y_{12} = 2$，$Y_{21} = 2$，而 $Y_{22}$ 缺失，其中 $\\Omega = \\{(1,1),(1,2),(2,1)\\}$。从 $X^{(0)} = 0$ 开始，执行一次 soft-impute 过程的迭代，该过程包括形成 $W^{(1)} = P_{\\Omega}(Y)$ 并应用水平为 $\\lambda = 1$ 的奇异值阈值化来获得 $X^{(1)}$。将插补值 $X^{(1)}_{22}$ 报告为精确的代数表达式（不要四舍五入）。在你的推导中，清楚地指出你使用的任何性质（例如，对于对称矩阵，左右奇异向量与归一化特征向量一致，最多相差一个符号）。你的最终答案必须是 $X^{(1)}_{22}$ 的单一表达式。", "solution": "用户希望我解决一个关于组学数据矩阵补全的两部分问题。\n\n**第一部分：问题验证**\n\n我将首先按照要求的程序验证问题陈述。\n\n*   **步骤1：提取给定信息**\n    *   数据矩阵：$X \\in \\mathbb{R}^{n \\times p}$ (真实，未观测)，$Y \\in \\mathbb{R}^{n \\times p}$ (部分观测)。\n    *   索引集：$\\Omega$ (已观测索引)，$\\Omega^c$ (缺失索引)。\n    *   算子：$P_{\\Omega}(Z)$ 是到已观测索引上的投影，其中如果 $(i,j) \\in \\Omega$，则 $P_{\\Omega}(Z)_{ij} = Z_{ij}$；否则为 $0$。\n    *   假设：\n        1.  缺失机制是完全随机缺失（MCAR）或随机缺失（MAR）。\n        2.  真实矩阵 $X$ 是近似低秩的（潜在因子模型）。\n        3.  矩阵 $X$ 满足非相干性条件。\n    *   任务1：构建一个用于矩阵补全的凸优化问题，该问题用凸代理替换秩最小化，惩罚复杂度，并仅对已观测条目进行拟合。解释低秩和非相干性假设的作用。\n    *   任务2 (数值计算)：\n        *   矩阵：$Y \\in \\mathbb{R}^{2 \\times 2}$，其中 $Y_{11} = 4, Y_{12} = 2, Y_{21} = 2$，$Y_{22}$ 缺失。\n        *   已观测集合：$\\Omega = \\{(1,1),(1,2),(2,1)\\}$。\n        *   算法：soft-impute 的一次迭代。\n        *   初始猜测：$X^{(0)} = 0$。\n        *   迭代1的步骤：形成 $W^{(1)} = P_{\\Omega}(Y)$，然后对 $W^{(1)}$ 应用奇异值阈值化以得到 $X^{(1)}$。\n        *   正则化参数：$\\lambda = 1$。\n        *   要求输出：插补值 $X^{(1)}_{22}$ 的精确代数表达式。\n\n*   **步骤2：使用提取的给定信息进行验证**\n    *   **科学上成立：** 该问题坚实地建立在矩阵补全的既定理论之上，这是机器学习、信号处理和计算生物学的核心课题。使用核范数作为秩的凸代理是该领域的开创性成果。低秩结构和非相干性的假设是标准的，并且是理论保证所必需的。\n    *   **良定的：** 问题的两个部分都是良定的。第一部分要求从第一性原理推导一个标准公式。第二部分是一个直接的计算，提供了所有必要的数据和参数，可以得到唯一的答案。\n    *   **客观的：** 问题以精确、客观且无歧义的数学语言陈述。\n    *   **完整性：** 问题是自洽的，并提供了所有必要的信息。没有矛盾之处。\n    *   **现实性：** 所描述的情景——在低秩组学数据中插补缺失值——是生物信息学中一个常见且现实的任务。数值示例虽然是一个玩具问题，但在维度和数学上是一致的，足以说明该算法。\n\n*   **步骤3：结论与行动**\n    该问题是有效的，因为它科学上合理、良定、客观且完整。我将继续提供完整的解答。\n\n***\n\n**第一部分：凸优化问题的构建**\n\n矩阵补全的基本目标是从其部分观测的版本 $Y$ 中恢复完整的数据矩阵 $X$。核心假设是真实矩阵 $X$ 具有低秩结构。矩阵的秩是其复杂性的度量。因此，一个自然的出发点是寻找与观测数据一致的秩尽可能低的矩阵。\n\n如果假设 $Y$ 中的已观测条目是无噪声的，这可以被构建为一个约束秩最小化问题：\n$$\n\\min_{Z \\in \\mathbb{R}^{n \\times p}} \\text{rank}(Z) \\quad \\text{约束条件} \\quad P_{\\Omega}(Z) = P_{\\Omega}(Y)\n$$\n在这里，约束 $P_{\\Omega}(Z) = P_{\\Omega}(Y)$ 强制要求候选矩阵 $Z$ 与 $Y$ 的已观测条目完全匹配。\n\n然而，这个问题在计算上是难解的（NP-hard），因为秩函数是非凸和离散的。矩阵补全的突破来自于将秩函数松弛为其凸包络。在谱范数小于等于 $1$ 的矩阵集合上，秩函数最紧的凸松弛是**核范数**，记为 $\\|Z\\|_*$。核范数是矩阵 $Z$ 的奇异值之和：\n$$\n\\|Z\\|_* = \\sum_{i=1}^{\\min(n,p)} \\sigma_i(Z)\n$$\n其中 $\\sigma_i(Z)$ 是 $Z$ 的奇异值。\n\n在实践中，组学数据包含测量噪声，因此强制与已观测条目完全匹配是不可取的，因为这会导致过拟合。一个更稳健的公式是最小化数据保真项和复杂度惩罚（核范数）的组合。这导出了问题的无约束拉格朗日形式。我们寻找一个矩阵 $X$，它最小化在观测集 $\\Omega$ 上的平方误差和，加上一个与核范数成比例的正则化项：\n$$\n\\min_{X \\in \\mathbb{R}^{n \\times p}} \\frac{1}{2} \\|P_{\\Omega}(Y - X)\\|_F^2 + \\lambda \\|X\\|_*\n$$\n在这里，$\\|A\\|_F^2 = \\sum_{i,j} A_{ij}^2$ 是弗罗贝尼乌斯范数的平方，所以 $\\|P_{\\Omega}(Y - X)\\|_F^2$ 项就是 $Y$ 的已观测条目与 $X$ 相应条目之间差的平方和。参数 $\\lambda > 0$ 是一个正则化参数，它控制着拟合数据与强制低秩结构之间的权衡。较大的 $\\lambda$ 会鼓励产生更低秩的解。这是矩阵补全的标准凸公式，通常被称为 soft-impute 目标函数。\n\n**建模假设的作用：**\n\n1.  **近似低秩：** 这是证明整个方法合理性的基础假设。其前提是高维组学数据（例如，数千个基因）由少数潜在的生物过程、通路或技术因素（例如，批次效应）所控制。这些潜在因子在数据矩阵 $X$ 的行和列之间引起强烈的相关性，导致 $X$ 是（或可以被很好地近似为）一个低秩矩阵。没有这个假设，就没有可用于插补的潜在结构，填充缺失值的问题将是不适定的。核范数惩罚是用来强制实现这种假定的低秩结构的数学工具。\n\n2.  **非相干性：** 这是一个技术性但关键的条件，用以保证凸问题的解接近于真实的低秩矩阵 $X$。非相干性要求 $X$ 的奇异向量是“分散的”，而不是集中在少数几个坐标上。例如，如果一个左奇异向量 $u_k$ 等于标准基向量 $e_i$（即，在位置 $i$ 为 $1$ 其余为零的向量），那么与该潜在因子相关的所有信息都将包含在第 $i$ 行中。如果第 $i$ 行的很大一部分缺失，那么信号的该分量将不可恢复。非相干性确保了对应于每个潜在因子的信息分布在矩阵的许多条目中，因此观测条目的一个随机子集足以恢复潜在的结构。\n\n***\n\n**第二部分：玩具示例的计算**\n\n我们被要求以初始猜测 $X^{(0)} = 0$ 执行 soft-impute 算法的一次迭代。通用的更新步骤是 $X^{(k+1)} = S_{\\lambda}(P_{\\Omega}(Y) + P_{\\Omega^c}(X^{(k)}))$，其中 $S_{\\lambda}$ 是奇异值阈值化算子。\n\n对于第一次迭代（$k=0$），我们有：\n$X^{(1)} = S_{\\lambda}(P_{\\Omega}(Y) + P_{\\Omega^c}(X^{(0)})) = S_{\\lambda}(P_{\\Omega}(Y) + P_{\\Omega^c}(0)) = S_{\\lambda}(P_{\\Omega}(Y))$。\n问题陈述指导我们计算 $W^{(1)} = P_{\\Omega}(Y)$，然后以 $\\lambda = 1$ 对其应用奇异值阈值化。\n\n**步骤1：构建矩阵 $W^{(1)}$**\n给定 $Y_{11} = 4, Y_{12} = 2, Y_{21} = 2$，且 $Y_{22}$ 缺失，矩阵 $W^{(1)}$ 通过保留观测值并将缺失值设为 $0$ 来形成：\n$$\nW^{(1)} = P_{\\Omega}(Y) = \\begin{pmatrix} 4  2 \\\\ 2  0 \\end{pmatrix}\n$$\n\n**步骤2：计算 $W^{(1)}$ 的奇异值分解（SVD）**\n令 $W = W^{(1)}$。算子 $S_{\\lambda}(W)$ 需要计算 $W$ 的SVD，设为 $W = U \\Sigma V^T$，将奇异值 $\\sigma_i$ 阈值化为 $\\max(\\sigma_i - \\lambda, 0)$，然后重构矩阵。\n\n由于 $W$ 是一个对称矩阵，其SVD与其特征分解密切相关。奇异值 $\\sigma_i$ 是特征值 $\\mu_i$ 的绝对值。左、右奇异向量 ($u_i, v_i$) 是相应的特征向量，如果 $\\mu_i < 0$，则 $u_i$ 可能有符号变化。\n\n我们通过求解特征方程 $\\det(W - \\mu I) = 0$ 来找到 $W$ 的特征值：\n$$\n\\det\\begin{pmatrix} 4-\\mu  2 \\\\ 2  -\\mu \\end{pmatrix} = (4-\\mu)(-\\mu) - (2)(2) = \\mu^2 - 4\\mu - 4 = 0\n$$\n使用二次求根公式，特征值为：\n$$\n\\mu = \\frac{-(-4) \\pm \\sqrt{(-4)^2 - 4(1)(-4)}}{2(1)} = \\frac{4 \\pm \\sqrt{16 + 16}}{2} = \\frac{4 \\pm 4\\sqrt{2}}{2} = 2 \\pm 2\\sqrt{2}\n$$\n两个特征值是 $\\mu_1 = 2 + 2\\sqrt{2}$ 和 $\\mu_2 = 2 - 2\\sqrt{2}$。\n\n奇异值是特征值的绝对值：\n$\\sigma_1 = |\\mu_1| = 2 + 2\\sqrt{2}$\n$\\sigma_2 = |\\mu_2| = |2 - 2\\sqrt{2}| = 2\\sqrt{2} - 2$ (因为 $2\\sqrt{2} > 2$)。\n\n**步骤3：应用软阈值化**\n正则化参数为 $\\lambda = 1$。新的奇异值 $\\sigma'_i$ 由 $\\sigma'_i = \\max(\\sigma_i - \\lambda, 0)$ 给出：\n$$\n\\sigma'_1 = \\sigma_1 - \\lambda = (2 + 2\\sqrt{2}) - 1 = 1 + 2\\sqrt{2}\n$$\n$$\n\\sigma'_2 = \\sigma_2 - \\lambda = (2\\sqrt{2} - 2) - 1 = 2\\sqrt{2} - 3\n$$\n由于 $2\\sqrt{2} = \\sqrt{8}$ 且 $3 = \\sqrt{9}$，我们发现 $2\\sqrt{2} - 3 < 0$。因此，阈值化后的奇异值为：\n$$\n\\sigma'_2 = \\max(2\\sqrt{2} - 3, 0) = 0\n$$\n得到的矩阵 $X^{(1)}$ 将是一个秩为1的矩阵，因为只有一个奇异值非零。\n\n**步骤4：重构矩阵 $X^{(1)}$**\n重构的矩阵是 $X^{(1)} = U \\Sigma_{\\lambda} V^T = \\sigma'_1 u_1 v_1^T$。\n我们需要找到对应于 $\\sigma_1$ 的奇异向量 $u_1$ 和 $v_1$。由于 $W$ 是对称的且 $\\mu_1 = \\sigma_1 > 0$，我们可以选择 $u_1 = v_1$，其中 $v_1$ 是对应于 $\\mu_1$ 的归一化特征向量。\n\n我们通过求解 $(W - \\mu_1 I)v=0$ 来找到 $\\mu_1 = 2 + 2\\sqrt{2}$ 的特征向量：\n$$\n\\begin{pmatrix} 4 - (2+2\\sqrt{2})  2 \\\\ 2  -(2+2\\sqrt{2}) \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\n从第一行，$(2-2\\sqrt{2})x + 2y = 0$，化简得 $y = (\\sqrt{2}-1)x$。\n因此，一个未归一化的特征向量是 $\\begin{pmatrix} 1 \\\\ \\sqrt{2}-1 \\end{pmatrix}$。\n其平方范数是 $1^2 + (\\sqrt{2}-1)^2 = 1 + (2 - 2\\sqrt{2} + 1) = 4 - 2\\sqrt{2}$。\n归一化特征向量是 $v_1 = \\frac{1}{\\sqrt{4-2\\sqrt{2}}} \\begin{pmatrix} 1 \\\\ \\sqrt{2}-1 \\end{pmatrix}$。\n\n由于 $u_1=v_1$，秩为1的矩阵是 $X^{(1)} = \\sigma'_1 v_1 v_1^T$。我们关心的元素是 $X^{(1)}_{22}$：\n$$\nX^{(1)}_{22} = \\sigma'_1 \\times (v_1)_2 \\times (v_1)_2 = \\sigma'_1 \\left( \\frac{\\sqrt{2}-1}{\\sqrt{4-2\\sqrt{2}}} \\right)^2\n$$\n代入 $\\sigma'_1$ 的值并化简：\n$$\nX^{(1)}_{22} = (1 + 2\\sqrt{2}) \\frac{(\\sqrt{2}-1)^2}{4-2\\sqrt{2}} = (1 + 2\\sqrt{2}) \\frac{2 - 2\\sqrt{2} + 1}{4-2\\sqrt{2}} = (1 + 2\\sqrt{2}) \\frac{3 - 2\\sqrt{2}}{4-2\\sqrt{2}}\n$$\n我们展开分子：\n$$\n(1 + 2\\sqrt{2})(3 - 2\\sqrt{2}) = 1(3) - 1(2\\sqrt{2}) + (2\\sqrt{2})(3) - (2\\sqrt{2})(2\\sqrt{2}) = 3 - 2\\sqrt{2} + 6\\sqrt{2} - 8 = 4\\sqrt{2} - 5\n$$\n所以我们有：\n$$\nX^{(1)}_{22} = \\frac{4\\sqrt{2} - 5}{4-2\\sqrt{2}}\n$$\n为了化简，我们通过分子分母同乘以共轭项 $4+2\\sqrt{2}$ 来使分母有理化：\n$$\nX^{(1)}_{22} = \\frac{4\\sqrt{2} - 5}{4-2\\sqrt{2}} \\times \\frac{4+2\\sqrt{2}}{4+2\\sqrt{2}} = \\frac{(4\\sqrt{2})(4) + (4\\sqrt{2})(2\\sqrt{2}) - 5(4) - 5(2\\sqrt{2})}{4^2 - (2\\sqrt{2})^2}\n$$\n$$\nX^{(1)}_{22} = \\frac{16\\sqrt{2} + 16 - 20 - 10\\sqrt{2}}{16 - 8} = \\frac{6\\sqrt{2} - 4}{8}\n$$\n最后，化简该分数得到插补值的精确表达式：\n$$\nX^{(1)}_{22} = \\frac{3\\sqrt{2} - 2}{4}\n$$", "answer": "$$\\boxed{\\frac{3\\sqrt{2}-2}{4}}$$", "id": "4584852"}, {"introduction": "大多数插补方法都依赖于一个核心假设：数据是随机缺失（MAR）的，但这个假设在实际应用中往往无法检验且可能不成立。本练习是一项高级实践，将指导你从零开始实现两种经典的敏感性分析方法：模式混合模型和选择模型。通过这项练习，你将掌握评估 MAR 假设偏离对研究结论影响的关键技能，从而进行更稳健和可信的数据分析 [@problem_id:4584871]。", "problem": "您的任务是设计并实现一个完整的、可运行的程序，在连续结局的医学数据场景中执行两种非随机缺失 (MNAR) 敏感性分析：一种是模式混合 delta 调整引爆点分析，另一种是使用选择偏倚函数的选择模型敏感性分析。所有任务都必须使用根据基本统计学原理生成的合成数据来执行。程序必须生成具有指定格式和数值的单行输出。下文中的每个数学符号、变量、函数、运算符和数字都以 LaTeX 格式表示。不涉及任何物理单位。\n\n问题背景与基本原理。考虑一个在线性回归下建模的连续临床结局。设 $Y \\in \\mathbb{R}$ 表示连续终点，$T \\in \\{0,1\\}$ 表示处理指标，$Z \\in \\mathbb{R}$ 表示基线协变量，$R \\in \\{0,1\\}$ 表示响应指标，其中 $R=1$ 表示结局被观测到。假设完整数据生成遵循线性模型 $Y = \\beta_0 + \\beta_1 T + \\beta_2 Z + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$，并且 $T \\sim \\mathrm{Bernoulli}(0.5)$，$Z \\sim \\mathcal{N}(0,1)$。缺失指标 $R$ 遵循一个依赖于 $Y$ 的逻辑斯蒂选择模型：$\\operatorname{logit}\\, \\mathbb{P}(R=1 \\mid Y=y) = \\alpha + \\gamma \\, g(y)$，其中 $g(y)$ 是一个指定的选择偏倚函数，$\\gamma$ 是敏感性参数，且 $\\operatorname{logit}(p) = \\log\\{p/(1-p)\\}$。\n\n模式混合 delta 调整。在模式混合视角下，假设对于处理组 $T=1$ 中 $R=0$ 的受试者，其插补结局被一个偏移量 $\\delta$ 所移动（而对于 $T=0$ 的受试者则不移动）。随机缺失 (MAR) 下的插补使用在观测数据上拟合的线性回归来生成条件均值插补；然后，通过将 $\\delta$ 加到 $T=1$ 组中脱落者的这些插补值上来编码 MNAR 敏感性。我们将引爆点定义为在指定搜索区间内，使得拟合的处理系数 $\\hat{\\beta}_1(\\delta)$ 的符号相较于其在 $\\delta=0$ 时的值发生改变的最小 $\\delta$。我们假设 $\\beta_1$ 为负值对应于有益效应。\n\n选择模型敏感性。在选择模型 $\\operatorname{logit}\\, \\mathbb{P}(R=1 \\mid Y=y) = \\alpha + \\gamma g(y)$ 和选定的 $g(y)$ 下，假设 $X=(1,T,Z)^\\top$ 是完全观测的，且缺失仅依赖于 $Y$。迭代期望定律和逆概率加权逻辑表明，对于任何形式为 $\\mathbb{E}[X\\{Y - X^\\top \\beta\\}] = 0$ 的完整数据估计方程，可以通过使用权重 $w(y) = 1/\\mathbb{P}(R=1 \\mid Y=y)$ 在观测数据上构建一个加权估计方程来恢复完整数据矩条件。要在有限样本和固定 $\\gamma$ 下实现这一点，必须使用选择模型和观测结局来确定与观测响应率一致的 $\\alpha$，然后执行加权最小二乘法来估计感兴趣的回归系数（处理效应 $\\beta_1$）。选择偏倚函数选为 $g(y) = \\{y - \\bar{y}_{\\mathrm{obs}}\\}/s_{\\mathrm{obs}}$，其中 $\\bar{y}_{\\mathrm{obs}}$ 和 $s_{\\mathrm{obs}}$ 是观测到的 $Y$ 值的经验均值和标准差。\n\n要实现的任务。您的程序必须：\n\n1. 为每个测试用例生成数据。对于每个测试用例，按如下方式生成 $n$ 个独立观测值：抽取 $T_i \\sim \\mathrm{Bernoulli}(0.5)$，$Z_i \\sim \\mathcal{N}(0,1)$，$\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$，然后 $Y_i = \\beta_0 + \\beta_1 T_i + \\beta_2 Z_i + \\varepsilon_i$。使用所有 $n$ 个结局的经验均值 $\\bar{Y}$ 和经验标准差 $s_Y$ 来定义 $g_i = \\{Y_i - \\bar{Y}\\}/s_Y$。固定一个目标观测比例 $p_{\\mathrm{obs}} \\in (0,1)$ 和一个真实选择斜率 $\\gamma_{\\mathrm{true}}$，然后求解 $\\alpha_{\\mathrm{true}}$ 使得样本平均响应概率等于 $p_{\\mathrm{obs}}$，即 $\\frac{1}{n} \\sum_{i=1}^n \\operatorname{expit}(\\alpha_{\\mathrm{true}} + \\gamma_{\\mathrm{true}} g_i) = p_{\\mathrm{obs}}$，其中 $\\operatorname{expit}(x) = \\{1+\\exp(-x)\\}^{-1}$。然后，独立抽取 $R_i \\sim \\mathrm{Bernoulli}(\\operatorname{expit}(\\alpha_{\\mathrm{true}} + \\gamma_{\\mathrm{true}} g_i))$。对于回归和插补任务，只使用观测到的数据 $\\{(T_i,Z_i,Y_i): R_i=1\\}$。\n\n2. 模式混合 delta 调整引爆点。仅使用观测数据 $(R=1)$ 拟合一个 $Y$ 对 $(1,T,Z)$ 的线性回归，以获得一个 MAR 插补模型（条件均值模型）。对于每个缺失结局 ($R=0$)，将其插补值 $\\tilde{Y}_i(0)$ 设为使用观测数据回归得到的拟合条件均值；然后，对于那些 $T_i=1$ 且 $R_i=0$ 的个体，定义 $\\tilde{Y}_i(\\delta) = \\tilde{Y}_i(0) + \\delta$，而对于那些 $T_i=0$ 且 $R_i=0$ 的个体，保持 $\\tilde{Y}_i(\\delta) = \\tilde{Y}_i(0)$。通过取 $R_i=1$ 时的 $Y_i$ 和 $R_i=0$ 时的 $\\tilde{Y}_i(\\delta)$ 来构建一个补全数据集 $Y_i(\\delta)$。使用全部 $n$ 条记录对 $Y(\\delta)$ 关于 $(1,T,Z)$ 进行线性回归拟合，并将处理系数表示为 $\\hat{\\beta}_1(\\delta)$。将引爆点定义为在 $[\\delta_{\\min}, \\delta_{\\max}]$ 区间内，$\\hat{\\beta}_1(\\delta)$ 的符号相对于 $\\hat{\\beta}_1(0)$ 发生变化的最小 $\\delta$。如果在 $[\\delta_{\\min}, \\delta_{\\max}]$ 内没有发生符号改变，则报告 $\\mathrm{NaN}$。使用 $\\delta_{\\min}=-5$ 和 $\\delta_{\\max}=5$。\n\n3. 选择模型敏感性格栅。使用观测结局 $\\{Y_i: R_i=1\\}$，构建 $g_{\\mathrm{obs}}(y) = \\{y - \\bar{y}_{\\mathrm{obs}}\\}/s_{\\mathrm{obs}}$。对于格栅 $\\{-2,-1,0,1,2\\}$ 中的每个 $\\gamma$，通过全概率定律确定由选择模型和观测响应率所隐含的截距 $\\alpha(\\gamma)$，以使模型与有限样本响应率兼容。然后计算观测结局的选择概率 $p_i(\\gamma) = \\operatorname{expit}\\{\\alpha(\\gamma) + \\gamma g_{\\mathrm{obs}}(Y_i)\\}$ 和权重 $w_i(\\gamma) = 1/p_i(\\gamma)$。通过在观测数据集上对 $Y$ 关于 $(1,T,Z)$ 的回归求解加权正规方程，使用加权最小二乘法估计处理效应 $\\hat{\\beta}_1(\\gamma)$。通过格栅上 $\\max_{\\gamma} \\hat{\\beta}_1(\\gamma) - \\min_{\\gamma} \\hat{\\beta}_1(\\gamma)$ 的范围宽度来量化选择模型的影响。\n\n测试套件和输出规范。使用以下三个测试用例，每个用例都由一个元组 $(\\text{seed}, n, \\beta_0, \\beta_1, \\beta_2, \\sigma, \\gamma_{\\mathrm{true}}, p_{\\mathrm{obs}})$ 完全定义：\n\n- Case A: $(20251, 1000, 0.0, -0.5, 0.8, 1.0, -1.0, 0.7)$。\n- Case B: $(20252, 1000, 0.0, -1.0, 0.5, 1.2, -1.5, 0.6)$。\n- Case C: $(20253, 1000, 0.0, -0.2, 0.8, 1.0, 1.0, 0.7)$。\n\n角度单位不适用。不涉及任何物理单位。百分比必须表示为小数；例如，$0.7$ 表示 $70/100$ 的响应比例。\n\n最终输出格式。您的程序应生成一个单行输出，其中包含一个用方括号括起来的逗号分隔列表。对于每个案例，返回一个双元素列表 $[\\delta^\\star, \\Delta_{\\mathrm{range}}]$，其中 $\\delta^\\star$ 是区间 $[-5,5]$ 内的引爆点值（如果没有发生变化，则为 $\\mathrm{NaN}$），$\\Delta_{\\mathrm{range}}$ 是格栅 $\\{-2,-1,0,1,2\\}$ 上的选择模型范围宽度。所有浮点数均四舍五入到 $4$ 位小数。因此，最终输出必须是形如 $[[\\delta^\\star_A,\\Delta_A],[\\delta^\\star_B,\\Delta_B],[\\delta^\\star_C,\\Delta_C]]$ 的单行，其中包含按 A、B、C 顺序排列的三个案例的数值。", "solution": "用户提供了一个定义明确的计算统计学问题，要求实现两种不同的敏感性分析，以处理非随机缺失 (MNAR) 数据。该问题有科学依据、内部一致，并且在程序上是可行的。我将继续提供一个完整的解决方案。\n\n该解决方案涉及为每个测试用例结构化地实现三个主要组件：一个数据生成模块、一个模式混合引爆点分析模块和一个选择模型敏感性分析模块。\n\n### 原理与方法论\n\n#### 1. 数据生成\n\n该问题的基础是一项模拟研究。我们首先生成“完整”数据，然后根据指定的 MNAR 机制引入缺失。\n\n- **完整数据生成**：我们模拟 $n$ 个受试者。对于每个受试者 $i=1, \\dots, n$：\n    -   从概率为 $0.5$ 的伯努利分布中抽取一个二元处理指标 $T_i$，即 $T_i \\sim \\mathrm{Bernoulli}(0.5)$。\n    -   从标准正态分布中抽取一个连续基线协变量 $Z_i$，即 $Z_i \\sim \\mathcal{N}(0,1)$。\n    -   从均值为 $0$、方差为 $\\sigma^2$ 的正态分布中抽取一个随机误差项 $\\varepsilon_i$，即 $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$。\n    -   通过线性模型生成连续结局 $Y_i$：$Y_i = \\beta_0 + \\beta_1 T_i + \\beta_2 Z_i + \\varepsilon_i$。\n\n- **缺失数据生成 (MNAR)**：缺失机制是 MNAR，因为观测到结局 $Y_i$ 的概率取决于 $Y_i$ 本身的值。响应指标 $R_i$（其中 $R_i=1$ 表示 $Y_i$ 被观测到，$R_i=0$ 表示缺失）由一个逻辑斯蒂回归模型生成：\n    $$ \\operatorname{logit}\\, \\mathbb{P}(R_i=1 \\mid Y_i=y_i) = \\log\\left(\\frac{\\mathbb{P}(R_i=1 \\mid Y_i=y_i)}{1-\\mathbb{P}(R_i=1 \\mid Y_i=y_i)}\\right) = \\alpha_{\\mathrm{true}} + \\gamma_{\\mathrm{true}} g_i $$\n    其中 $g_i = (Y_i - \\bar{Y})/s_Y$ 是标准化结局，$\\operatorname{expit}(x) = (1+e^{-x})^{-1}$ 是逆 logit 函数。参数 $\\gamma_{\\mathrm{true}}$ 控制缺失对 Y 的依赖强度。截距 $\\alpha_{\\mathrm{true}}$ 通过数值方法确定，以确保整个完整样本中观测数据的期望比例与目标值 $p_{\\text{obs}}$ 相匹配。这是通过求解以下关于 $\\alpha_{\\mathrm{true}}$ 的方程来实现的：\n    $$ \\frac{1}{n} \\sum_{i=1}^n \\operatorname{expit}(\\alpha_{\\mathrm{true}} + \\gamma_{\\mathrm{true}} g_i) = p_{\\mathrm{obs}} $$\n    这是一个关于 $\\alpha_{\\mathrm{true}}$ 的单调函数的求根问题，可以使用诸如 Brent-Dekker 算法之类的数值方法高效求解。\n\n#### 2. 模式混合 Delta 调整引爆点分析\n\n这种敏感性分析探讨了如果我们假设处理组中缺失结局与随机缺失 (MAR) 模型预测的结果有系统性差异，那么处理效应估计会如何变化。\n\n- **MAR 插补模型**：首先，仅使用观测数据（即 $R_i=1$ 的受试者）拟合一个标准线性回归模型（$Y$ 对 $T$ 和 $Z$）。设估计系数为 $\\hat{\\beta}_{\\text{MAR}}$。\n- **插补**：对于每个有缺失结局的受试者 $i$ ($R_i=0$)，我们首先根据 MAR 模型的预测生成一个基线插补 $\\tilde{Y}_i(0)$：$\\tilde{Y}_i(0) = \\hat{\\beta}_{\\text{MAR},0} + \\hat{\\beta}_{\\text{MAR},1} T_i + \\hat{\\beta}_{\\text{MAR},2} Z_i$。\n- **Delta 调整**：我们引入一个敏感性参数 $\\delta$。对于处理组中的缺失受试者 ($R_i=0, T_i=1$)，插补值被调整为 $\\tilde{Y}_i(\\delta) = \\tilde{Y}_i(0) + \\delta$。对于对照组中的缺失受试者 ($R_i=0, T_i=0$)，插补值保持为 $\\tilde{Y}_i(\\delta) = \\tilde{Y}_i(0)$。\n- **引爆点计算**：使用 $R_i=1$ 时的观测值 $Y_i$ 和 $R_i=0$ 时的插补值 $\\tilde{Y}_i(\\delta)$ 形成一个新的、补全的数据集 $Y(\\delta)$。对所有 $n$ 个受试者拟合 $Y(\\delta)$ 对 $(1, T, Z)$ 的线性回归，得到一个处理效应估计 $\\hat{\\beta}_1(\\delta)$。\n    关键在于，$\\hat{\\beta}_1(\\delta)$ 是 $\\delta$ 的一个简单线性函数。设 $X$ 是完整设计矩阵。补全的数据向量可以写成 $Y(\\delta) = Y(0) + \\delta \\cdot d$，其中 $d_i = T_i(1-R_i)$。普通最小二乘 (OLS) 估计为 $\\hat{\\beta}(\\delta) = (X^\\top X)^{-1} X^\\top Y(\\delta) = \\hat{\\beta}(0) + \\delta (X^\\top X)^{-1} X^\\top d$。\n    因此，$\\hat{\\beta}_1(\\delta) = \\hat{\\beta}_1(0) + \\delta \\cdot B_1$，其中 $B_1$ 是向量 $(X^\\top X)^{-1} X^\\top d$ 的第二个分量。引爆点是使 $\\hat{\\beta}_1(\\delta)$ 相对于 $\\hat{\\beta}_1(0)$ 改变符号的 $\\delta$ 值。这发生在 $\\hat{\\beta}_1(\\delta)=0$ 时，可通过 $\\delta^\\star = -\\hat{\\beta}_1(0) / B_1$ 求解。如果这个 $\\delta^\\star$ 落在指定的搜索区间 $[\\delta_{\\min}, \\delta_{\\max}]$ 内，它就是引爆点；否则，区间内没有发生引爆。\n\n#### 3. 选择模型敏感性分析\n\n该分析直接对 MNAR 机制建模，并使用加权最小二乘法评估处理效应对于此机制假设的敏感性。\n\n- **模型和加权**：该方法假设缺失概率遵循选择模型 $\\operatorname{logit}\\,\\mathbb{P}(R=1|Y=y) = \\alpha(\\gamma) + \\gamma g_{\\text{obs}}(y)$，其中 $g_{\\text{obs}}(y) = (y - \\bar{y}_{\\text{obs}}) / s_{\\text{obs}}$ 是基于观测结局进行标准化的。参数 $\\gamma$ 是一个在格栅 $\\{-2, -1, 0, 1, 2\\}$ 上变化的敏感性参数，而 $\\alpha(\\gamma)$ 是一个讨厌参数。\n- **估计 $\\alpha(\\gamma)$**：对于格栅中的每个给定 $\\gamma$，$\\alpha(\\gamma)$ 由一个自洽条件确定。逆概率加权原理要求观测受试者的权重之和应重构总样本量 $n$。一个观测受试者 $i$ 的权重是 $w_i = 1/\\mathbb{P}(R_i=1|Y_i)$。因此，我们求解以下关于 $\\alpha(\\gamma)$ 的非线性方程：\n    $$ \\sum_{i: R_i=1} \\frac{1}{\\operatorname{expit}(\\alpha(\\gamma) + \\gamma g_{\\text{obs}}(Y_i))} = n $$\n- **加权最小二乘法 (WLS)**：一旦找到 $\\alpha(\\gamma)$，就为所有观测受试者计算权重 $w_i$。然后，在观测数据上使用这些权重进行 $Y$ 对 $(1, T, Z)$ 的加权最小二乘回归。WLS 问题的解给出了处理效应估计 $\\hat{\\beta}_1(\\gamma)$：\n    $$ \\hat{\\beta}(\\gamma) = (X_{\\text{obs}}^\\top W(\\gamma) X_{\\text{obs}})^{-1} X_{\\text{obs}}^\\top W(\\gamma) Y_{\\text{obs}} $$\n    其中 $W(\\gamma)$ 是权重 $w_i(\\gamma)$ 的对角矩阵。\n- **敏感性范围**：对格栅中的每个 $\\gamma$ 重复此过程。处理效应估计对 MNAR 假设的敏感性通过所得估计值的范围来衡量：$\\Delta_{\\text{range}} = \\max_{\\gamma} \\hat{\\beta}_1(\\gamma) - \\min_{\\gamma} \\hat{\\beta}_1(\\gamma)$。\n\n这种综合方法可以对潜在的 MNAR 机制可能如何影响研究结论进行稳健的评估。", "answer": "```python\nimport numpy as np\nfrom scipy.special import expit\nfrom scipy.optimize import brentq\nfrom typing import List, Tuple\n\ndef ols_fit(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n    \"\"\"Performs Ordinary Least Squares regression using a stable solver.\"\"\"\n    return np.linalg.solve(X.T @ X, X.T @ y)\n\ndef wls_fit(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> np.ndarray:\n    \"\"\"Performs Weighted Least Squares regression using a stable solver.\"\"\"\n    w_sqrt = np.sqrt(w)\n    X_w = w_sqrt[:, np.newaxis] * X\n    y_w = w_sqrt * y\n    return np.linalg.solve(X_w.T @ X_w, X_w.T @ y_w)\n\ndef generate_data(seed: int, n: int, beta0: float, beta1: float, beta2: float, \n                  sigma: float, gamma_true: float, p_obs_target: float) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Generates synthetic data according to the problem specification.\"\"\"\n    rng = np.random.default_rng(seed)\n    \n    T = rng.binomial(1, 0.5, size=n)\n    Z = rng.normal(0, 1, size=n)\n    epsilon = rng.normal(0, sigma, size=n)\n    \n    X_full = np.stack([np.ones(n), T, Z], axis=1)\n    \n    betas_true = np.array([beta0, beta1, beta2])\n    Y = X_full @ betas_true + epsilon\n    \n    y_mean = np.mean(Y)\n    y_std = np.std(Y)\n    g = (Y - y_mean) / y_std if y_std > 1e-9 else np.zeros_like(Y)\n    \n    def alpha_fn(alpha: float) -> float:\n        return np.mean(expit(alpha + gamma_true * g)) - p_obs_target\n    \n    alpha_true = brentq(alpha_fn, -40.0, 40.0)\n    \n    p_R = expit(alpha_true + gamma_true * g)\n    R = rng.binomial(1, p_R, size=n)\n    \n    return T, Z, Y, R.astype(int), X_full\n\ndef tipping_point_analysis(T: np.ndarray, Z: np.ndarray, Y: np.ndarray, R: np.ndarray, X_full: np.ndarray,\n                           delta_min: float, delta_max: float) -> float:\n    \"\"\"Performs pattern-mixture delta-adjustment tipping-point analysis.\"\"\"\n    obs_idx = (R == 1)\n    mis_idx = (R == 0)\n    \n    X_obs, Y_obs = X_full[obs_idx], Y[obs_idx]\n    \n    beta_mar = ols_fit(X_obs, Y_obs)\n    \n    Y_imputed_mar = X_full[mis_idx] @ beta_mar\n    Y_completed_0 = Y.copy()\n    Y_completed_0[mis_idx] = Y_imputed_mar\n    \n    beta_hat_0 = ols_fit(X_full, Y_completed_0)\n    beta1_at_0 = beta_hat_0[1]\n\n    d_vec = T * (1 - R)\n    # The vector B is the solution to (X.T @ X) B = X.T @ d\n    B = np.linalg.solve(X_full.T @ X_full, X_full.T @ d_vec)\n    B1 = B[1]\n\n    if abs(B1) < 1e-12:\n        return np.nan\n        \n    delta_root = -beta1_at_0 / B1\n    \n    if delta_min <= delta_root <= delta_max:\n        return delta_root\n    else:\n        return np.nan\n\ndef selection_model_analysis(T: np.ndarray, Z: np.ndarray, Y: np.ndarray, R: np.ndarray, X_full: np.ndarray) -> float:\n    \"\"\"Performs selection-model sensitivity analysis.\"\"\"\n    obs_idx = (R == 1)\n    n = len(Y)\n    \n    Y_obs, T_obs, Z_obs = Y[obs_idx], T[obs_idx], Z[obs_idx]\n    X_obs = X_full[obs_idx]\n\n    y_obs_mean = np.mean(Y_obs)\n    y_obs_std = np.std(Y_obs)\n    g_obs = (Y_obs - y_obs_mean) / y_obs_std if y_obs_std > 1e-9 else np.zeros_like(Y_obs)\n    \n    gamma_grid = [-2.0, -1.0, 0.0, 1.0, 2.0]\n    beta1_hats = []\n    \n    for gamma in gamma_grid:\n        def alpha_fn(alpha: float) -> float:\n            logits = alpha + gamma * g_obs\n            # Numerically stable version of sum(1/expit(logits)) - n\n            return np.sum(1.0 + np.exp(-logits)) - n\n            \n        try:\n            alpha_gamma = brentq(alpha_fn, a=-50.0, b=50.0)\n        except ValueError:\n            continue\n            \n        probs = expit(alpha_gamma + gamma * g_obs)\n        weights = 1.0 / probs\n        \n        beta_wls = wls_fit(X_obs, Y_obs, weights)\n        beta1_hats.append(beta_wls[1])\n        \n    if not beta1_hats:\n        return np.nan\n        \n    range_width = np.max(beta1_hats) - np.min(beta1_hats)\n    return range_width\n\ndef format_val(v: float) -> str:\n    \"\"\"Formats a float to 4 decimal places, or 'NaN' for np.nan.\"\"\"\n    if np.isnan(v):\n        return \"NaN\"\n    return f\"{v:.4f}\"\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (seed, n, beta0, beta1, beta2, sigma, gamma_true, p_obs)\n        (20251, 1000, 0.0, -0.5, 0.8, 1.0, -1.0, 0.7), # Case A\n        (20252, 1000, 0.0, -1.0, 0.5, 1.2, -1.5, 0.6), # Case B\n        (20253, 1000, 0.0, -0.2, 0.8, 1.0, 1.0, 0.7),  # Case C\n    ]\n    \n    all_results = []\n    for params in test_cases:\n        seed, n, beta0, beta1, beta2, sigma, gamma_true, p_obs = params\n        \n        T, Z, Y, R, X_full = generate_data(seed, n, beta0, beta1, beta2, sigma, gamma_true, p_obs)\n        \n        delta_star = tipping_point_analysis(T, Z, Y, R, X_full, delta_min=-5.0, delta_max=5.0)\n        \n        range_width = selection_model_analysis(T, Z, Y, R, X_full)\n        \n        all_results.append([delta_star, range_width])\n        \n    formatted_pairs = [f\"[{format_val(pair[0])},{format_val(pair[1])}]\" for pair in all_results]\n    print(f\"[[{','.join(formatted_pairs)}]]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "4584871"}]}