{"hands_on_practices": [{"introduction": "广义线性模型（GLM）的拟合过程通常基于最大似然估计（MLE）的原理。要真正理解模型是如何学习参数的，关键在于掌握其目标函数——对数似然函数——的数学构造。本练习将引导你从最基本的伯努利分布假设出发，为逻辑回归模型推导出其对数似然函数、梯度（得分向量）和Hessian矩阵，这些都是诸如牛顿-拉弗森法等优化算法的核心组成部分。[@problem_id:4578844]", "problem": "在一个针对二元疾病结局的病例对照生物标志物研究中，假设我们观测了 $n$ 个独立个体，索引为 $i = 1, \\dots, n$。对于每个个体 $i$，令 $\\mathbf{x}_{i} \\in \\mathbb{R}^{p}$ 表示一个固定的 $p$ 维生物信息学特征协变量向量（例如，$p$ 个基因的标准化表达量测量值），令 $y_{i} \\in \\{0,1\\}$ 表示其疾病状态（其中 $y_{i} = 1$ 表示病例，$y_{i} = 0$ 表示对照）。假设一个具有伯努利响应和逻辑斯谛链接的广义线性模型 (GLM)，即在给定 $\\mathbf{x}_{i}$ 和参数 $\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}$ 的条件下，$Y_{i}$ 的条件分布是均值为 $\\mu_{i} = \\Pr(Y_{i} = 1 \\mid \\mathbf{x}_{i}, \\boldsymbol{\\beta})$ 的伯努利分布，线性预测器为 $\\eta_{i} = \\mathbf{x}_{i}^{\\top}\\boldsymbol{\\beta}$，逻辑斯蒂均值函数为 $\\mu_{i} = \\frac{1}{1 + \\exp(-\\eta_{i})}$。\n\n仅从以下基本出发点开始：\n- 伯努利概率质量函数 $p(y \\mid \\mu) = \\mu^{y}(1 - \\mu)^{1 - y}$，其中 $y \\in \\{0,1\\}$。\n- 各个体之间的观测独立性。\n- 逻辑斯谛链接 $\\eta = \\ln\\left(\\frac{\\mu}{1-\\mu}\\right)$ 及其逆函数 $\\mu = \\frac{1}{1 + \\exp(-\\eta)}$。\n\n推导该逻辑斯蒂回归模型的全样本对数似然函数 $\\ell(\\boldsymbol{\\beta})$（用 $\\{\\mathbf{x}_{i}, y_{i}\\}_{i=1}^{n}$ 和 $\\boldsymbol{\\beta}$ 表示），然后明确计算：\n1. 得分向量（对数似然函数关于 $\\boldsymbol{\\beta}$ 的梯度）。\n2. 海森矩阵（对数似然函数关于 $\\boldsymbol{\\beta}$ 的二阶导数矩阵）。\n\n您的推导应从上述基本事实出发，不得引用现成的逻辑斯蒂回归公式。通过引入 $n \\times p$ 设计矩阵 $\\mathbf{X}$（其第 $i$ 行为 $\\mathbf{x}_{i}^{\\top}$）、响应向量 $\\mathbf{y} = (y_{1}, \\dots, y_{n})^{\\top}$、均值向量 $\\boldsymbol{\\mu}(\\boldsymbol{\\beta}) = (\\mu_{1}, \\dots, \\mu_{n})^{\\top}$ 和对角权重矩阵 $\\mathbf{W}(\\boldsymbol{\\beta}) = \\mathrm{diag}\\big(\\mu_{i}(1 - \\mu_{i})\\big)$，使用标准矩阵表示法来表达您的最终结果。以封闭形式提供这三个表达式。最终答案应为单个封闭形式的解析表达式。无需四舍五入。", "solution": "该问题陈述构成了广义线性模型 (GLM) 理论中的一个标准、适定的推导。它在科学上是合理的、客观的，并包含唯一解所需的所有信息。因此，我们可以进行推导。\n\n目标是为逻辑斯蒂回归模型推导对数似然函数、其梯度（得分向量）及其海森矩阵。我们从提供的基本原理开始。\n\n**1. 对数似然函数 $\\ell(\\boldsymbol{\\beta})$ 的推导**\n\n对于单个观测 $i$，响应变量 $Y_i$ 服从参数为 $\\mu_i = \\Pr(Y_i=1 \\mid \\mathbf{x}_i, \\boldsymbol{\\beta})$ 的伯努利分布。其概率质量函数 (PMF) 给出为 $p(y_i \\mid \\mu_i) = \\mu_i^{y_i} (1 - \\mu_i)^{1 - y_i}$。由于 $n$ 个观测是独立的，参数 $\\boldsymbol{\\beta}$ 的总似然函数是各个 PMF 的乘积：\n$$ L(\\boldsymbol{\\beta}) = \\prod_{i=1}^{n} p(y_i \\mid \\mu_i(\\boldsymbol{\\beta})) = \\prod_{i=1}^{n} \\mu_i^{y_i} (1 - \\mu_i)^{1-y_i} $$\n对数似然函数 $\\ell(\\boldsymbol{\\beta})$ 是似然函数的自然对数：\n$$ \\ell(\\boldsymbol{\\beta}) = \\ln(L(\\boldsymbol{\\beta})) = \\ln\\left(\\prod_{i=1}^{n} \\mu_i^{y_i} (1 - \\mu_i)^{1-y_i}\\right) = \\sum_{i=1}^{n} \\ln\\left(\\mu_i^{y_i} (1 - \\mu_i)^{1-y_i}\\right) $$\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left[ y_i \\ln(\\mu_i) + (1-y_i)\\ln(1-\\mu_i) \\right] $$\n该表达式可以重新整理为：\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left[ y_i \\ln(\\mu_i) - y_i \\ln(1-\\mu_i) + \\ln(1-\\mu_i) \\right] = \\sum_{i=1}^{n} \\left[ y_i \\ln\\left(\\frac{\\mu_i}{1-\\mu_i}\\right) + \\ln(1-\\mu_i) \\right] $$\n该模型使用逻辑斯谛链接函数 $\\eta_i = \\ln\\left(\\frac{\\mu_i}{1-\\mu_i}\\right)$，其中线性预测器为 $\\eta_i = \\mathbf{x}_i^\\top\\boldsymbol{\\beta}$。将 $\\eta_i$ 代入 $\\ell(\\boldsymbol{\\beta})$ 的表达式中得到：\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} [y_i \\eta_i + \\ln(1-\\mu_i)] $$\n为了用 $\\eta_i$ 表示 $\\ln(1-\\mu_i)$，我们使用逆链接函数 $\\mu_i = \\frac{1}{1 + \\exp(-\\eta_i)}$。\n$$ 1 - \\mu_i = 1 - \\frac{1}{1 + \\exp(-\\eta_i)} = \\frac{1 + \\exp(-\\eta_i) - 1}{1 + \\exp(-\\eta_i)} = \\frac{\\exp(-\\eta_i)}{1 + \\exp(-\\eta_i)} $$\n通过将分子和分母同乘以 $\\exp(\\eta_i)$，可以得到一个等价的表达式：\n$$ 1 - \\mu_i = \\frac{1}{ \\exp(\\eta_i) + 1 } $$\n取自然对数，我们得到：\n$$ \\ln(1-\\mu_i) = \\ln\\left(\\frac{1}{1+\\exp(\\eta_i)}\\right) = -\\ln(1+\\exp(\\eta_i)) $$\n将此代回 $\\ell(\\boldsymbol{\\beta})$ 的表达式中，得到以线性预测器 $\\eta_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$ 表示的对数似然函数的最终形式：\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left[ y_i (\\mathbf{x}_i^\\top \\boldsymbol{\\beta}) - \\ln(1 + \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta})) \\right] $$\n\n**2. 得分向量 $\\nabla_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta})$ 的推导**\n\n得分向量是对数似然函数关于参数向量 $\\boldsymbol{\\beta}$ 的梯度。它是一个 $p \\times 1$ 的向量。\n$$ \\nabla_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta}) = \\frac{\\partial \\ell(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} \\left[ y_i (\\mathbf{x}_i^\\top \\boldsymbol{\\beta}) - \\ln(1 + \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta})) \\right] $$\n我们对和中的每一项使用链式法则。令 $\\eta_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$。则 $\\frac{\\partial \\eta_i}{\\partial \\boldsymbol{\\beta}} = \\mathbf{x}_i$。\n第 $i$ 项的导数是：\n$$ \\frac{\\partial \\ell_i(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = y_i \\frac{\\partial(\\mathbf{x}_i^\\top \\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}}[\\ln(1 + \\exp(\\eta_i))] $$\n$$ \\frac{\\partial \\ell_i(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = y_i \\mathbf{x}_i - \\frac{1}{1 + \\exp(\\eta_i)} \\cdot \\exp(\\eta_i) \\cdot \\frac{\\partial \\eta_i}{\\partial \\boldsymbol{\\beta}} $$\n注意到 $\\mu_i = \\frac{\\exp(\\eta_i)}{1 + \\exp(\\eta_i)} = \\frac{1}{1 + \\exp(-\\eta_i)}$，我们有：\n$$ \\frac{\\partial \\ell_i(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = y_i \\mathbf{x}_i - \\mu_i \\mathbf{x}_i = (y_i - \\mu_i) \\mathbf{x}_i $$\n对所有观测求和，得到得分向量：\n$$ \\nabla_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} (y_i - \\mu_i(\\boldsymbol{\\beta})) \\mathbf{x}_i $$\n为了用矩阵表示法表示，令 $\\mathbf{X}$ 为行是 $\\mathbf{x}_i^\\top$ 的 $n \\times p$ 设计矩阵，$\\mathbf{y}$ 为 $n \\times 1$ 的结果向量，$\\boldsymbol{\\mu}(\\boldsymbol{\\beta})$ 为 $n \\times 1$ 的均值向量。该和等价于矩阵乘积：\n$$ \\nabla_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta}) = \\mathbf{X}^\\top (\\mathbf{y} - \\boldsymbol{\\mu}(\\boldsymbol{\\beta})) $$\n\n**3. 海森矩阵 $\\nabla_{\\boldsymbol{\\beta}}^2 \\ell(\\boldsymbol{\\beta})$ 的推导**\n\n海森矩阵是对数似然函数的 $p \\times p$ 二阶偏导数矩阵。它通过对得分向量关于 $\\boldsymbol{\\beta}^\\top$ 求导得到。\n$$ \\nabla_{\\boldsymbol{\\beta}}^2 \\ell(\\boldsymbol{\\beta}) = \\frac{\\partial}{\\partial \\boldsymbol{\\beta}^\\top} \\left( \\nabla_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta}) \\right) = \\frac{\\partial}{\\partial \\boldsymbol{\\beta}^\\top} \\left( \\sum_{i=1}^{n} (y_i - \\mu_i) \\mathbf{x}_i \\right) $$\n$$ \\nabla_{\\boldsymbol{\\beta}}^2 \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\boldsymbol{\\beta}^\\top} \\left( y_i \\mathbf{x}_i - \\mu_i \\mathbf{x}_i \\right) $$\n由于 $y_i$ 和 $\\mathbf{x}_i$ 是固定的，第一项 $y_i\\mathbf{x}_i$ 的导数为零。对于第二项，我们应用向量微积分的乘法法则，注意到 $\\mathbf{x}_i$ 不是 $\\boldsymbol{\\beta}$ 的函数：\n$$ \\frac{\\partial}{\\partial \\boldsymbol{\\beta}^\\top}(-\\mu_i \\mathbf{x}_i) = - \\mathbf{x}_i \\frac{\\partial \\mu_i}{\\partial \\boldsymbol{\\beta}^\\top} $$\n我们需要求 $\\mu_i$ 关于 $\\boldsymbol{\\beta}^\\top$ 的导数。我们再次使用链式法则：$\\frac{\\partial \\mu_i}{\\partial \\boldsymbol{\\beta}^\\top} = \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\boldsymbol{\\beta}^\\top}$。我们有 $\\frac{\\partial \\eta_i}{\\partial \\boldsymbol{\\beta}^\\top} = \\mathbf{x}_i^\\top$。均值函数 $\\mu_i$ 关于线性预测器 $\\eta_i$ 的导数是：\n$$ \\frac{\\partial \\mu_i}{\\partial \\eta_i} = \\frac{\\partial}{\\partial \\eta_i}\\left(\\frac{1}{1+\\exp(-\\eta_i)}\\right) = - (1+\\exp(-\\eta_i))^{-2} \\cdot (\\exp(-\\eta_i)) \\cdot (-1) = \\frac{\\exp(-\\eta_i)}{(1+\\exp(-\\eta_i))^2} $$\n这可以被因式分解为：\n$$ \\frac{\\partial \\mu_i}{\\partial \\eta_i} = \\left(\\frac{1}{1+\\exp(-\\eta_i)}\\right) \\left(\\frac{\\exp(-\\eta_i)}{1+\\exp(-\\eta_i)}\\right) = \\mu_i (1-\\mu_i) $$\n因此，$\\mu_i$ 关于 $\\boldsymbol{\\beta}^\\top$ 的导数是：\n$$ \\frac{\\partial \\mu_i}{\\partial \\boldsymbol{\\beta}^\\top} = \\mu_i(1-\\mu_i) \\mathbf{x}_i^\\top $$\n将此代入海森矩阵第 $i$ 项的表达式中，得到：\n$$ \\frac{\\partial}{\\partial \\boldsymbol{\\beta}^\\top}(-\\mu_i \\mathbf{x}_i) = - \\mathbf{x}_i (\\mu_i(1-\\mu_i)\\mathbf{x}_i^\\top) = - \\mu_i(1-\\mu_i) \\mathbf{x}_i \\mathbf{x}_i^\\top $$\n这是一个 $p \\times p$ 矩阵。对所有观测求和，得到海森矩阵：\n$$ \\nabla_{\\boldsymbol{\\beta}}^2 \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} - \\mu_i(1-\\mu_i) \\mathbf{x}_i \\mathbf{x}_i^\\top = - \\sum_{i=1}^{n} \\mu_i(1-\\mu_i) \\mathbf{x}_i \\mathbf{x}_i^\\top $$\n使用定义的权重矩阵 $\\mathbf{W}(\\boldsymbol{\\beta}) = \\mathrm{diag}(\\mu_i(1-\\mu_i))$，这个和就是矩阵乘积 $-\\mathbf{X}^\\top \\mathbf{W} \\mathbf{X}$ 的定义。\n$$ \\nabla_{\\boldsymbol{\\beta}}^2 \\ell(\\boldsymbol{\\beta}) = - \\mathbf{X}^\\top \\mathbf{W}(\\boldsymbol{\\beta}) \\mathbf{X} $$\n该矩阵对于拟合广义线性模型的 Newton-Raphson 算法至关重要，它与费雪信息矩阵有关。\n\n所要求的三个表达式是：\n1.  对数似然函数：$\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left( y_i \\mathbf{x}_i^\\top \\boldsymbol{\\beta} - \\ln(1 + \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta})) \\right)$\n2.  得分向量：$\\nabla_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta}) = \\mathbf{X}^\\top (\\mathbf{y} - \\boldsymbol{\\mu}(\\boldsymbol{\\beta}))$\n3.  海森矩阵：$\\nabla_{\\boldsymbol{\\beta}}^2 \\ell(\\boldsymbol{\\beta}) = - \\mathbf{X}^\\top \\mathbf{W}(\\boldsymbol{\\beta}) \\mathbf{X}$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\ell(\\boldsymbol{\\beta}) \\\\\n\\nabla_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta}) \\\\\n\\nabla_{\\boldsymbol{\\beta}}^2 \\ell(\\boldsymbol{\\beta})\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\sum_{i=1}^{n} \\left( y_i \\mathbf{x}_i^\\top \\boldsymbol{\\beta} - \\ln(1 + \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta})) \\right) \\\\\n\\mathbf{X}^\\top (\\mathbf{y} - \\boldsymbol{\\mu}(\\boldsymbol{\\beta})) \\\\\n- \\mathbf{X}^\\top \\mathbf{W}(\\boldsymbol{\\beta}) \\mathbf{X}\n\\end{pmatrix}\n}\n$$", "id": "4578844"}, {"introduction": "在获得模型参数后，下一个关键步骤是解释这些参数的实际意义。本练习将理论与实践相结合，重点关注如何将逻辑回归模型中抽象的系数（如 $\\beta_1$）转化为一个在生物信息学和流行病学中直观且有意义的统计量——比值比（Odds Ratio）。通过从模型结构中推导出比值比，你将学会如何量化预测变量对二元结局发生几率的影响，这是将模型结果有效传达给领域专家的核心技能。[@problem_id:4595172]", "problem": "一项队列研究旨在探究一个二元暴露变量 $X$（暴露组编码为 $X=1$，非暴露组编码为 $X=0$）是否与一年内二元疾病结局 $Y$ 的发生相关，同时校正一个连续协变量 $Z$（中心化年龄）。研究人员拟合了一个广义线性模型 (GLM)，其结局为二项分布，并使用标准 logit 链接函数，使得条件均值满足 $g(\\mathbb{E}[Y \\mid X,Z]) = \\log\\!\\left(\\frac{\\Pr(Y=1 \\mid X,Z)}{1-\\Pr(Y=1 \\mid X,Z)}\\right)$，且线性预测量为 $\\eta = \\beta_{0} + \\beta_{1} X + \\beta_{2} Z$。暴露变量的拟合系数为 $\\hat{\\beta}_{1} = 1.1$。\n\n仅使用逻辑斯蒂链接下“比值”（odds）和“比值比”（odds ratio, OR）的核心定义以及上述模型的结构，在固定 $Z$ 值的情况下，推导比较 $X=1$ 与 $X=0$ 的暴露比值比（即该拟合模型所隐含的比值比），然后使用 $\\hat{\\beta}_{1} = 1.1$ 计算其数值。将最终数值答案作为一个无单位的比值，并四舍五入到 $3$ 位有效数字。", "solution": "在进行解答之前，首先对问题陈述进行验证。\n\n### 第1步：提取已知信息\n-   **模型类型**：广义线性模型 (GLM)。\n-   **结局**：二元疾病结局 $Y$。$Y=1$ 表示疾病发生，$Y=0$ 表示未发生。结局服从二项分布。\n-   **暴露**：二元暴露变量 $X$，$X=1$ 表示暴露，$X=0$ 表示未暴露。\n-   **协变量**：连续协变量 $Z$（中心化年龄）。\n-   **链接函数**：标准 logit 链接函数，$g(\\mu) = \\log\\!\\left(\\frac{\\Pr(Y=1 \\mid X,Z)}{1-\\Pr(Y=1 \\mid X,Z)}\\right)$。\n-   **线性预测量**：$\\eta = \\beta_{0} + \\beta_{1} X + \\beta_{2} Z$。\n-   **拟合系数**：暴露变量的估计系数为 $\\hat{\\beta}_{1} = 1.1$。\n-   **任务**：\n    1. 在固定 $Z$ 值的情况下，推导比较 $X=1$ 与 $X=0$ 的暴露比值比 (OR)。\n    2. 使用给定的拟合系数计算该 OR 的数值。\n    3. 报告结果，并四舍五入到 $3$ 位有效数字。\n\n### 第2步：使用提取的已知信息进行验证\n-   **科学性**：该问题描述了一个逻辑斯蒂回归模型，这是流行病学和生物统计学中分析二元结局的标准和基本工具。比值、比值比、logit 链接和线性预测量等概念都是标准的、科学合理的。\n-   **良构性**：问题定义清晰。它提供了具体的模型结构和必要的系数来推导和计算比值比。问题明确，且有唯一、稳定的解。\n-   **客观性**：问题以精确、正式、无偏的数学和统计语言陈述。\n-   **完整性和一致性**：所提供的信息是充分且自洽的。为了推导所要求的比值比，模型结构已完全指定。没有遗漏任何必要信息，也没有矛盾之处。\n-   **现实性**：一个队列研究在校正年龄等混杂变量的同时，检验暴露与疾病关系的情景，是 GLM 在流行病学中一个经典且非常现实的应用。系数 $\\hat{\\beta}_{1} = 1.1$ 对于此类研究中的效应估计值来说是一个合理的量级。\n\n### 第3步：结论与行动\n该问题被认为是有效的，因为它具有科学性、良构性、客观性、完整性、一致性和现实性。我将继续提供一个完整且合理的解答。\n\n问题的核心是从给定的逻辑斯蒂回归模型中推导出暴露变量 $X$ 的比值比 (OR)。该模型通过 logit 链接函数指定了预测变量（$X$，$Z$）与结局概率（$Y=1$）之间的关系。\n\n模型方程表明，疾病的对数比值是预测变量的线性函数：\n$$\n\\log\\!\\left(\\frac{\\Pr(Y=1 \\mid X,Z)}{1-\\Pr(Y=1 \\mid X,Z)}\\right) = \\beta_{0} + \\beta_{1} X + \\beta_{2} Z\n$$\n\n事件的“比值”(odds) 定义为事件发生的概率除以事件不发生的概率。在此背景下，对于暴露状态为 $X$、年龄为 $Z$ 的个体，其患病比值为：\n$$\n\\text{Odds}(Y=1 \\mid X, Z) = \\frac{\\Pr(Y=1 \\mid X,Z)}{1-\\Pr(Y=1 \\mid X,Z)}\n$$\n\n对模型方程两边取指数，我们可以直接表示比值：\n$$\n\\text{Odds}(Y=1 \\mid X, Z) = \\exp(\\beta_{0} + \\beta_{1} X + \\beta_{2} Z)\n$$\n\n暴露的比值比 (OR) 比较的是在保持所有其他协变量不变的情况下，暴露组（$X=1$）的患病比值与非暴露组（$X=0$）的患病比值。在这里，我们将协变量 $Z$ 保持在一个固定的、任意的值。\nOR 定义为该比率：\n$$\n\\text{OR} = \\frac{\\text{Odds}(Y=1 \\mid X=1, Z)}{\\text{Odds}(Y=1 \\mid X=0, Z)}\n$$\n\n现在我们将基于模型的比值表达式代入此比率中。\n\n对于暴露组 ($X=1$)，比值为：\n$$\n\\text{Odds}(Y=1 \\mid X=1, Z) = \\exp(\\beta_{0} + \\beta_{1}(1) + \\beta_{2} Z) = \\exp(\\beta_{0} + \\beta_{1} + \\beta_{2} Z)\n$$\n\n对于非暴露组 ($X=0$)，比值为：\n$$\n\\text{Odds}(Y=1 \\mid X=0, Z) = \\exp(\\beta_{0} + \\beta_{1}(0) + \\beta_{2} Z) = \\exp(\\beta_{0} + \\beta_{2} Z)\n$$\n\n现在，我们构建这个比率来求比值比：\n$$\n\\text{OR} = \\frac{\\exp(\\beta_{0} + \\beta_{1} + \\beta_{2} Z)}{\\exp(\\beta_{0} + \\beta_{2} Z)}\n$$\n\n使用指数的性质 $\\frac{\\exp(a)}{\\exp(b)} = \\exp(a-b)$，我们简化表达式：\n$$\n\\text{OR} = \\exp\\left((\\beta_{0} + \\beta_{1} + \\beta_{2} Z) - (\\beta_{0} + \\beta_{2} Z)\\right)\n$$\n$$\n\\text{OR} = \\exp(\\beta_{0} + \\beta_{1} + \\beta_{2} Z - \\beta_{0} - \\beta_{2} Z)\n$$\n$$\n\\text{OR} = \\exp(\\beta_{1})\n$$\n\n这个推导表明，在一个没有交互项的逻辑斯蒂回归模型中，预测变量每单位变化（本例中为 $X$ 从 0 变为 1）的比值比就是该变量系数的指数，即 $\\exp(\\beta_{1})$。该 OR 在模型中其他协变量的所有水平上（本例中为 $Z$）都是恒定的。\n\n问题提供了暴露变量的拟合系数 $\\hat{\\beta}_{1} = 1.1$。因此，估计的比值比为：\n$$\n\\widehat{\\text{OR}} = \\exp(\\hat{\\beta}_{1}) = \\exp(1.1)\n$$\n\n我们被要求计算该值并四舍五入到 3 位有效数字。\n$$\n\\exp(1.1) \\approx 3.0041660239\n$$\n将此值四舍五入到 3 位有效数字得到 3.00。前三位有效数字是 $3$、$0$ 和 $0$。第四位数字是 $4$，小于 $5$，因此我们向下取整（即保持第三位有效数字不变）。\n\n因此，比较暴露个体与非暴露个体的估计比值比为 $3.00$。", "answer": "$$\n\\boxed{3.00}\n$$", "id": "4595172"}, {"introduction": "本练习将引导你完成一个完整的、现实世界中的数据科学工作流，从理论推导到编程实现。我们将处理计数数据，为此采用泊松回归模型，并引入 $L_2$ 正则化（岭回归）来防止模型过拟合，这在处理高维生物信息学数据时尤其重要。你将通过交叉验证来系统地选择最优的正则化参数，并应用“一个标准误”规则来平衡模型的预测性能与简洁性，最终构建一个稳健的预测模型。[@problem_id:5197921]", "problem": "你需要设计、推导并实现一个完整的程序，通过最小化正则化经验风险来估计一个带惩罚项的泊松广义线性模型（GLM），并使用交叉验证偏差（cross-validated deviance）来评估此程序，同时采用单标准误规则（one-standard-error rule）选择正则化参数。你的实现必须是一个单一、可运行的程序，不接受任何输入，并打印所需的输出。该场景在医疗人工智能和数据科学中很常见，其中计数结果（例如，临床事件的计数）使用泊松回归进行建模。\n\n从泊松分布和广义线性模型框架的基本原理出发：\n- 观测值 $i$ 的响应 $y_i$ 服从均值参数为 $\\mu_i$ 的泊松分布，并使用典则对数链接（canonical log link），使得 $\\log \\mu_i = \\eta_i$，其中 $\\eta_i = \\alpha + \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$ 且 $\\mathbf{x}_i \\in \\mathbb{R}^p$。\n- 目标是通过最小化一个正则化经验损失来估计截距 $\\alpha$ 和系数 $\\boldsymbol{\\beta}$，该损失函数结合了负对数似然与仅对 $\\boldsymbol{\\beta}$ 生效的 $\\ell_2$（岭）惩罚项（截距不被惩罚）。\n\n你的任务如下：\n1. 从泊松概率质量函数和 GLM 典则链接的定义出发，推导模型 $y_i \\sim \\mathrm{Poisson}(\\mu_i)$ 且 $\\log \\mu_i = \\alpha + \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$ 的负对数似然，并由此推导偏差（deviance）为饱和模型对数似然与拟合模型对数似然之差的两倍。仅在需要时使用良态极限（well-posed limits），陈述泊松响应下每个观测值的偏差贡献的闭式解。\n2. 推导不带惩罚项的泊松 GLM（使用典则链接）的迭代重加权最小二乘（IRLS）更新式，然后修改这些更新式以包含一个排除截距项的 $\\ell_2$ 惩罚项 $\\lambda \\lVert \\boldsymbol{\\beta} \\rVert_2^2/2$。对于固定的 $\\lambda$，清晰地阐明在每次 IRLS 迭代中必须求解的关于工作响应（working response）和权重的线性系统。\n3. 设计一个用于估计样本外期望偏差（expected out-of-sample deviance）的 $K$-折交叉验证估计器。对于给定的 $\\lambda$，交叉验证偏差应为各折中留出（held-out）偏差的均值，其标准误应为各折偏差的样本标准差除以 $\\sqrt{K}$。定义“最小偏差”选择 $\\lambda_{\\mathrm{min}}$ 和“单标准误”选择 $\\lambda_{\\mathrm{1SE}}$，其中 $\\lambda_{\\mathrm{1SE}}$ 是指其均值偏差与最小均值偏差之差在单标准误范围内的最大 $\\lambda$。\n4. 实现一个完整的算法，该算法：\n   - 对每一折，使用训练折的均值和标准差对预测变量进行标准化（为避免除以零，将零标准差替换为 $1$）。不要对响应进行标准化。\n   - 使用 IRLS 和路径上的热启动（warm starts），在一系列 $\\lambda$ 值上拟合带惩罚项的模型。使用不被惩罚的截距。\n   - 计算每个 $\\lambda$ 的交叉验证偏差及其标准误。\n   - 为下面定义的每个测试用例选择 $\\lambda_{\\mathrm{min}}$ 和 $\\lambda_{\\mathrm{1SE}}$ 并输出它们。\n   - 在计算偏差时使用数值稳定的方法，特别是当 $y_i = 0$ 时，并根据需要裁剪线性预测值以避免指数映射中的溢出。\n5. 用简洁而精确的术语，讨论此模型中由 $\\ell_2$ 惩罚项引起的偏差-方差权衡，并解释为什么单标准误规则是一种偏向方差的选择，通常只会带来很小的偏差增加。\n\n不涉及物理单位。在计算中不要使用百分比；如果需要比例，则必须是小数，但此处仅需要实值浮点数。\n\n测试套件：\n实现你的程序，以根据以下规范生成合成数据，然后为每个用例运行你的交叉验证和选择例程。对于每个测试用例，程序必须输出一对值 $[\\lambda_{\\mathrm{min}}, \\lambda_{\\mathrm{1SE}}]$。\n\n- 用例 A（理想路径，信号强度中等）：\n  - $n=200$, $p=5$, 种子 $42$。\n  - 真实截距 $\\alpha = 1.0$，真实系数 $\\boldsymbol{\\beta} = [0.2, -0.5, 0.0, 0.3, 0.7]$。\n  - 预测变量 $\\mathbf{x}_i \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_p)$ 独立同分布，响应 $y_i \\sim \\mathrm{Poisson}(\\exp(\\alpha + \\mathbf{x}_i^\\top \\boldsymbol{\\beta}))$ 独立同分布。\n  - 交叉验证：$K=5$ 折，使用给定种子进行随机打乱后划分为连续块来确定折分配。\n  - 正则化网格：$\\lambda \\in \\{\\lambda_j\\}_{j=1}^{20}$，在 $10^{-3}$ 到 $10^{1}$ 之间对数均匀分布。\n\n- 用例 B（边界情况，存在大量零值，均值较小，且有轻度稀疏性）：\n  - $n=120$, $p=8$, 种子 $123$。\n  - 真实截距 $\\alpha = 0.3$，真实系数 $\\boldsymbol{\\beta} = [-0.8, 0.0, 0.0, 0.5, 0.0, 0.2, 0.0, 0.0]$。\n  - 预测变量和响应的生成方式与用例 A 相同。\n  - 交叉验证：如上所述，使用 $K=6$ 折。\n  - 正则化网格：$\\lambda \\in \\{\\lambda_j\\}_{j=1}^{30}$，在 $10^{-4}$ 到 $10^{2}$ 之间对数均匀分布。\n\n- 用例 C（高维情况，$p > n$，需要强正则化）：\n  - $n=60$, $p=80$, 种子 $777$。\n  - 真实截距 $\\alpha = 1.5$，真实系数 $\\boldsymbol{\\beta}$ 均为零，除了在固定索引 $[0, 5, 10, 20, 50]$ 处的 $5$ 个非零项，其值为 $[0.7, -0.6, 0.5, 0.4, -0.3]$。\n  - 预测变量和响应的生成方式与用例 A 相同。\n  - 交叉验证：如上所述，使用 $K=5$ 折。\n  - 正则化网格：$\\lambda \\in \\{\\lambda_j\\}_{j=1}^{25}$，在 $10^{-3}$ 到 $10^{3}$ 之间对数均匀分布。\n\n最终输出格式：\n你的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表。具体来说，它必须打印一个包含三个列表的列表，每个内部列表包含两个浮点数值，保留六位小数，并按测试用例的顺序排列。所需格式为：\n[[lambda_min_A,lambda_1se_A],[lambda_min_B,lambda_1se_B],[lambda_min_C,lambda_1se_C]]", "solution": "该问题要求推导、设计和实现一个用于拟合带惩罚项的泊松广义线性模型（GLM）并通过交叉验证选择正则化参数的程序。这是统计机器学习中的一个标准且定义明确的问题。\n\n假设观测值 $i$ 的响应变量 $y_i$ 服从泊松分布，即 $y_i \\sim \\mathrm{Poisson}(\\mu_i)$。均值 $\\mu_i$ 通过典则对数链接函数与一组预测变量 $\\mathbf{x}_i \\in \\mathbb{R}^p$ 相关联，使得 $\\log(\\mu_i) = \\eta_i = \\alpha + \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$。这里，$\\alpha$ 是截距，$\\boldsymbol{\\beta}$ 是一个包含 $p$ 个系数的向量。\n\n参数 $\\alpha$ 和 $\\boldsymbol{\\beta}$ 通过最小化一个带惩罚的负对数似然函数来估计。惩罚项是施加在系数 $\\boldsymbol{\\beta}$ 上（但不包括截距 $\\alpha$）的 $\\ell_2$（岭）惩罚。需要最小化的目标函数是 $J(\\alpha, \\boldsymbol{\\beta}) = -\\mathcal{L}(\\alpha, \\boldsymbol{\\beta}) + \\frac{\\lambda}{2} \\lVert \\boldsymbol{\\beta} \\rVert_2^2$，其中 $\\mathcal{L}$ 是对数似然，$\\lambda$ 是正则化参数。\n\n**1. 负对数似然和偏差的推导**\n\n对于均值为 $\\mu_i$ 的泊松分布，单个观测值 $y_i$ 的概率质量函数（PMF）为 $P(y_i ; \\mu_i) = \\frac{e^{-\\mu_i} \\mu_i^{y_i}}{y_i!}$。该观测值的对数似然为 $\\ell_i(\\mu_i; y_i) = \\log P(y_i ; \\mu_i) = y_i \\log \\mu_i - \\mu_i - \\log(y_i!)$。使用链接函数 $\\mu_i = e^{\\eta_i} = e^{\\alpha + \\mathbf{x}_i^\\top \\boldsymbol{\\beta}}$，对数似然变为 $\\ell_i(\\alpha, \\boldsymbol{\\beta}; y_i, \\mathbf{x}_i) = y_i \\eta_i - e^{\\eta_i} - \\log(y_i!)$。\n\n对于一个包含 $n$ 个独立观测值的数据集，总对数似然是各项之和 $\\mathcal{L}(\\alpha, \\boldsymbol{\\beta}) = \\sum_{i=1}^n \\ell_i = \\sum_{i=1}^n (y_i \\eta_i - e^{\\eta_i} - \\log(y_i!))$。负对数似然（NLL）是 $-\\mathcal{L}(\\alpha, \\boldsymbol{\\beta}) = \\sum_{i=1}^n (e^{\\eta_i} - y_i \\eta_i + \\log(y_i!))$。由于 $\\log(y_i!)$ 相对于参数是常数，最小化 NLL 等价于最小化 $\\sum_{i=1}^n (e^{\\eta_i} - y_i \\eta_i)$。\n\nGLM 的偏差（deviance）定义为 $D = 2(\\mathcal{L}_{\\text{sat}} - \\mathcal{L}_{\\text{fit}})$，其中 $\\mathcal{L}_{\\text{sat}}$ 是饱和模型的对数似然，$\\mathcal{L}_{\\text{fit}}$ 是拟合模型的对数似然。在饱和模型中，每个观测值的均值参数 $\\tilde{\\mu}_i$ 被设定为与数据完美匹配，对于泊松分布，即 $\\tilde{\\mu}_i = y_i$。饱和模型的对数似然为 $\\mathcal{L}_{\\text{sat}} = \\sum_{i=1}^n (y_i \\log y_i - y_i - \\log(y_i!))$。拟合模型的均值 $\\hat{\\mu}_i$ 由 GLM 估计得出，其对数似然为 $\\mathcal{L}_{\\text{fit}} = \\sum_{i=1}^n (y_i \\log \\hat{\\mu}_i - \\hat{\\mu}_i - \\log(y_i!))$。\n\n因此，总偏差为：\n$$ D = 2 \\left[ \\sum_{i=1}^n (y_i \\log y_i - y_i) - \\sum_{i=1}^n (y_i \\log \\hat{\\mu}_i - \\hat{\\mu}_i) \\right] = 2 \\sum_{i=1}^n \\left( y_i \\log\\left(\\frac{y_i}{\\hat{\\mu}_i}\\right) - (y_i - \\hat{\\mu}_i) \\right) $$\n单个观测值的偏差贡献为 $d_i = 2 ( y_i \\log(y_i/\\hat{\\mu}_i) - (y_i - \\hat{\\mu}_i) )$。对于 $y_i=0$ 的情况，我们使用良态极限 $\\lim_{y \\to 0} y \\log y = 0$。因此，当观测值为 $y_i=0$ 时，其偏差贡献简化为 $d_i = 2\\hat{\\mu}_i$。\n\n**2. 带惩罚项的迭代重加权最小二乘（IRLS）推导**\n\nIRLS 算法是一种用于寻找 GLM 最大似然估计的方法，等价于牛顿-拉弗森法。我们将其扩展到带惩罚项的情况。令 $\\boldsymbol{\\theta} = [\\alpha, \\boldsymbol{\\beta}^\\top]^\\top$ 为 $(p+1)$ 维的参数向量，$\\mathbf{X}^*$ 为 $n \\times (p+1)$ 的设计矩阵，其第一列为全1向量，因此 $\\boldsymbol{\\eta} = \\mathbf{X}^* \\boldsymbol{\\theta}$。\n\n需要最小化的目标函数是 $J(\\boldsymbol{\\theta}) = \\sum_{i=1}^n (e^{\\eta_i} - y_i \\eta_i) + \\frac{\\lambda}{2} \\boldsymbol{\\beta}^\\top \\boldsymbol{\\beta}$。\n在第 $(t)$ 次迭代中，$\\boldsymbol{\\theta}$ 的牛顿-拉弗森更新式为 $\\boldsymbol{\\theta}^{(t+1)} = \\boldsymbol{\\theta}^{(t)} - (\\mathbf{H}_J^{(t)})^{-1} \\nabla J^{(t)}$，其中 $\\nabla J$ 是目标函数的梯度，$\\mathbf{H}_J$ 是其海森矩阵。\n\n负对数似然部分的梯度为 $\\nabla(-\\mathcal{L}) = (\\mathbf{X}^*)^\\top(\\boldsymbol{\\mu} - \\mathbf{y})$。负对数似然部分的海森矩阵为 $\\mathbf{H}_{-\\mathcal{L}} = (\\mathbf{X}^*)^\\top \\mathbf{W} \\mathbf{X}^*$，其中 $\\mathbf{W}$ 是一个对角矩阵，其对角元素为 $W_{ii} = \\mu_i$。\n令 $\\mathbf{\\Lambda}$ 为一个 $(p+1) \\times (p+1)$ 的对角矩阵，其中 $\\Lambda_{00}=0$ 且对于 $j=1, \\dots, p$ 有 $\\Lambda_{jj}=\\lambda$。惩罚项的梯度为 $\\mathbf{\\Lambda}\\boldsymbol{\\theta}$，其海森矩阵为 $\\mathbf{\\Lambda}$。\n总梯度为 $\\nabla J(\\boldsymbol{\\theta}) = (\\mathbf{X}^*)^\\top(\\boldsymbol{\\mu} - \\mathbf{y}) + \\mathbf{\\Lambda}\\boldsymbol{\\theta}$。\n总海森矩阵为 $\\mathbf{H}_J(\\boldsymbol{\\theta}) = (\\mathbf{X}^*)^\\top \\mathbf{W} \\mathbf{X}^* + \\mathbf{\\Lambda}$。\n\n将这些代入牛顿-拉弗森公式并整理，得到 IRLS 更新式。在每一步，我们求解以下线性系统来获得 $\\boldsymbol{\\theta}^{(t+1)}$：\n$$ (\\mathbf{H}_J^{(t)}) \\boldsymbol{\\theta}^{(t+1)} = (\\mathbf{H}_J^{(t)}) \\boldsymbol{\\theta}^{(t)} - \\nabla J^{(t)} $$\n$$ \\left((\\mathbf{X}^*)^{\\top} \\mathbf{W}^{(t)} \\mathbf{X}^* + \\mathbf{\\Lambda}\\right) \\boldsymbol{\\theta}^{(t+1)} = \\left((\\mathbf{X}^*)^{\\top} \\mathbf{W}^{(t)} \\mathbf{X}^* + \\mathbf{\\Lambda}\\right) \\boldsymbol{\\theta}^{(t)} - \\left((\\mathbf{X}^*)^{\\top}(\\boldsymbol{\\mu}^{(t)} - \\mathbf{y}) + \\mathbf{\\Lambda}\\boldsymbol{\\theta}^{(t)}\\right) $$\n$$ \\left((\\mathbf{X}^*)^{\\top} \\mathbf{W}^{(t)} \\mathbf{X}^* + \\mathbf{\\Lambda}\\right) \\boldsymbol{\\theta}^{(t+1)} = (\\mathbf{X}^*)^{\\top} \\mathbf{W}^{(t)} \\mathbf{X}^* \\boldsymbol{\\theta}^{(t)} + (\\mathbf{X}^*)^{\\top}(\\mathbf{y} - \\boldsymbol{\\mu}^{(t)}) $$\n通过定义一个“工作响应”向量 $\\mathbf{z}^{(t)}$，其分量为 $z_i^{(t)} = \\eta_i^{(t)} + \\frac{y_i - \\mu_i^{(t)}}{\\mu_i^{(t)}}$，上式可以表示为一个带惩罚项的加权最小二乘问题。更新方程的右侧等价于 $(\\mathbf{X}^*)^{\\top} \\mathbf{W}^{(t)} \\mathbf{z}^{(t)}$。因此，每次迭代需要求解的线性系统是：\n$$ \\left((\\mathbf{X}^*)^{\\top} \\mathbf{W}^{(t)} \\mathbf{X}^* + \\mathbf{\\Lambda}\\right) \\boldsymbol{\\theta}^{(t+1)} = (\\mathbf{X}^*)^{\\top} \\mathbf{W}^{(t)} \\mathbf{z}^{(t)} $$\n\n**3. 交叉验证设计和 Lambda 选择**\n\n采用 $K$-折交叉验证程序来估计使用不同 $\\lambda$ 值拟合的模型的样本外偏差。\n1. 将数据划分为 $K$ 个不相交的折。\n2. 对于每一折 $k=1, \\dots, K$，在其余的 $K-1$ 折上训练模型，并在第 $k$ 折上进行验证。\n3. 对于每次训练/验证划分，将训练集中的预测变量标准化为零均值和单位方差。然后使用计算出的均值和标准差来标准化验证集的预测变量。\n4. 对于预定义网格中的每个 $\\lambda$，在训练数据上拟合一个模型，并在验证数据上计算每个观测值的平均偏差。记为 $d_k(\\lambda)$。\n5. 遍历所有折之后，每个 $\\lambda$ 的交叉验证偏差是各折偏差的均值：$\\mathrm{CV}(\\lambda) = \\frac{1}{K}\\sum_{k=1}^K d_k(\\lambda)$。\n6. 同时计算标准误：$\\mathrm{SE}(\\lambda) = \\sqrt{\\frac{1}{K(K-1)} \\sum_{k=1}^K (d_k(\\lambda) - \\mathrm{CV}(\\lambda))^2}$。\n7. 根据两条规则选择最优的 $\\lambda$ 值：\n   - $\\lambda_{\\mathrm{min}}$：产生最小交叉验证偏差的 $\\lambda$ 值，$\\mathrm{CV}_{\\min} = \\mathrm{CV}(\\lambda_{\\mathrm{min}})$。\n   - $\\lambda_{\\mathrm{1SE}}$：使得交叉验证偏差在最小偏差的一个标准误范围内的最大 $\\lambda$ 值（即正则化最强的模型），满足 $\\mathrm{CV}(\\lambda) \\le \\mathrm{CV}_{\\min} + \\mathrm{SE}(\\lambda_{\\min})$。\n\n**4. 实现说明**\n\n实现过程需要仔细处理数值细节。线性预测值 $\\eta_i$ 在应用指数函数之前应进行裁剪以防止溢出。热启动（warm starts）——即使用某个 $\\lambda$ 的解作为网格中下一个 $\\lambda$ 的初始猜测——可以显著加速 IRLS 算法的收敛。\n\n**5. 偏差-方差权衡和单标准误规则**\n\n$\\ell_2$ 惩罚项引入了偏差-方差权衡。通过将系数估计值 $\\boldsymbol{\\beta}$ 向零收缩，惩罚项为模型增加了偏差，因为真实系数不太可能为零。然而，这种收缩降低了估计值的方差，使得模型对训练数据的特定样本不那么敏感。对于一个最优的 $\\lambda$，方差的减少超过了偏差的增加，从而提高了在未见数据上的预测性能。单标准误规则是一种倾向于选择更简单、正则化更强的模型的启发式方法。它选择了一个预测性能与交叉验证找到的最佳模型在统计上无法区分的最简约模型（$\\lambda_{\\mathrm{1SE}} \\ge \\lambda_{\\min}$）。这是一种偏向方差的策略，因为它接受了预测误差（偏差）上一个小的、统计上不显著的增加，以换取一个可能更稳定、泛化能力更强（方差更低）的模型。", "answer": "```python\nimport numpy as np\nfrom scipy.special import xlogy\n\ndef make_beta_c():\n    \"\"\"Creates the specific beta vector for Test Case C.\"\"\"\n    beta = np.zeros(80)\n    indices = [0, 5, 10, 20, 50]\n    values = [0.7, -0.6, 0.5, 0.4, -0.3]\n    beta[indices] = values\n    return beta\n\ndef generate_data(spec):\n    \"\"\"Generates synthetic data for a given test case specification.\"\"\"\n    rng = np.random.default_rng(spec['seed'])\n    X = rng.normal(size=(spec['n'], spec['p']))\n    eta = spec['alpha'] + X @ spec['beta']\n    mu = np.exp(np.clip(eta, -30, 30)) # Clip for numerical stability\n    y = rng.poisson(mu)\n    return X, y\n\ndef create_folds(n, K, seed):\n    \"\"\"Creates deterministic K-fold cross-validation indices.\"\"\"\n    rng = np.random.default_rng(seed)\n    indices = np.arange(n)\n    rng.shuffle(indices)\n    fold_indices = np.array_split(indices, K)\n    folds = []\n    for k in range(K):\n        val_idx = fold_indices[k]\n        train_idx = np.concatenate([fold_indices[i] for i in range(K) if i != k])\n        folds.append((train_idx, val_idx))\n    return folds\n\ndef standardize(X_train, X_val):\n    \"\"\"Standardizes training and validation sets based on training set statistics.\"\"\"\n    means = np.mean(X_train, axis=0)\n    stds = np.std(X_train, axis=0)\n    stds[stds == 0] = 1.0  # As per problem, avoid division by zero\n    X_train_std = (X_train - means) / stds\n    X_val_std = (X_val - means) / stds\n    return X_train_std, X_val_std\n\ndef poisson_deviance(y, mu):\n    \"\"\"Computes the per-observation Poisson deviance.\"\"\"\n    mu = np.maximum(mu, 1e-15)  # Avoid log(0)\n    # Deviance = 2 * (y*log(y/mu) - (y-mu))\n    term1 = xlogy(y, y) - xlogy(y, mu)\n    term2 = mu - y\n    return 2 * (term1 + term2)\n\ndef fit_penalized_poisson(X_aug, y, lam, initial_theta, max_iter=100, tol=1e-7):\n    \"\"\"\n    Fits a penalized Poisson GLM using Iteratively Reweighted Least Squares (IRLS).\n    \"\"\"\n    p_plus_1 = X_aug.shape[1]\n    theta = np.copy(initial_theta)\n    \n    # Penalty matrix (intercept is not penalized)\n    Lambda = np.diag([0.0] + [lam] * (p_plus_1 - 1))\n    \n    for _ in range(max_iter):\n        eta = X_aug @ theta\n        eta = np.clip(eta, -30, 30) # Prevent overflow/underflow\n        mu = np.exp(eta)\n        \n        # Ensure mu is positive to avoid division by zero in working response\n        mu = np.maximum(mu, 1e-8)\n        \n        W_diag = mu  # Diagonal of the weight matrix W\n        z = eta + (y - mu) / mu  # Working response\n        \n        # Efficiently compute (X_aug.T @ W @ X_aug + Lambda) and (X_aug.T @ W @ z)\n        X_t_W = X_aug.T * W_diag  # Broadcasting for X_aug.T @ W\n        \n        A = X_t_W @ X_aug + Lambda\n        b = X_t_W @ z\n        \n        try:\n            theta_new = np.linalg.solve(A, b)\n        except np.linalg.LinAlgError:\n            # Fallback for ill-conditioned matrices\n            theta_new = np.linalg.pinv(A) @ b\n            \n        # Check for convergence using relative change\n        if np.linalg.norm(theta_new - theta) / (np.linalg.norm(theta) + 1e-8)  tol:\n            theta = theta_new\n            break\n            \n        theta = theta_new\n        \n    return theta\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        {\n            'n': 200, 'p': 5, 'seed': 42, 'alpha': 1.0, \n            'beta': np.array([0.2, -0.5, 0.0, 0.3, 0.7]), \n            'K': 5, 'lambda_grid': np.logspace(-3, 1, 20)\n        },\n        {\n            'n': 120, 'p': 8, 'seed': 123, 'alpha': 0.3, \n            'beta': np.array([-0.8, 0.0, 0.0, 0.5, 0.0, 0.2, 0.0, 0.0]), \n            'K': 6, 'lambda_grid': np.logspace(-4, 2, 30)\n        },\n        {\n            'n': 60, 'p': 80, 'seed': 777, 'alpha': 1.5, \n            'beta': make_beta_c(), \n            'K': 5, 'lambda_grid': np.logspace(-3, 3, 25)\n        }\n    ]\n    \n    all_final_results = []\n\n    for case in test_cases:\n        X, y = generate_data(case)\n        folds = create_folds(case['n'], case['K'], case['seed'])\n        \n        num_lambdas = len(case['lambda_grid'])\n        fold_deviances = np.zeros((case['K'], num_lambdas))\n\n        for k in range(case['K']):\n            train_idx, val_idx = folds[k]\n            X_train, y_train = X[train_idx], y[train_idx]\n            X_val, y_val = X[val_idx], y[val_idx]\n            \n            X_train_std, X_val_std = standardize(X_train, X_val)\n            \n            X_train_aug = np.c_[np.ones(X_train_std.shape[0]), X_train_std]\n            X_val_aug = np.c_[np.ones(X_val_std.shape[0]), X_val_std]\n            \n            # Use warm starts for coefficients across the lambda path\n            theta = np.zeros(case['p'] + 1)\n            \n            for j, lam in enumerate(case['lambda_grid']):\n                theta = fit_penalized_poisson(X_train_aug, y_train, lam, initial_theta=theta)\n                \n                eta_val = X_val_aug @ theta\n                mu_val = np.exp(np.clip(eta_val, -30, 30))\n                \n                dev = poisson_deviance(y_val, mu_val)\n                fold_deviances[k, j] = np.mean(dev)\n\n        mean_cv_deviances = np.mean(fold_deviances, axis=0)\n        std_err_cv_deviances = np.std(fold_deviances, axis=0, ddof=1) / np.sqrt(case['K'])\n        \n        # Select lambda_min\n        lambda_min_idx = np.argmin(mean_cv_deviances)\n        lambda_min = case['lambda_grid'][lambda_min_idx]\n        \n        # Select lambda_1se\n        min_dev_val = mean_cv_deviances[lambda_min_idx]\n        min_dev_se = std_err_cv_deviances[lambda_min_idx]\n        threshold = min_dev_val + min_dev_se\n        \n        eligible_indices = np.where(mean_cv_deviances = threshold)[0]\n        # The 1SE rule takes the largest lambda (most regularized) from this set\n        lambda_1se_idx = np.max(eligible_indices)\n        lambda_1se = case['lambda_grid'][lambda_1se_idx]\n        \n        all_final_results.append([lambda_min, lambda_1se])\n\n    case_results_str = []\n    for res_pair in all_final_results:\n        case_results_str.append(f\"[{res_pair[0]:.6f},{res_pair[1]:.6f}]\")\n    \n    # Print in the exact required format\n    print(f\"[{','.join(case_results_str)}]\")\n\nsolve()\n```", "id": "5197921"}]}