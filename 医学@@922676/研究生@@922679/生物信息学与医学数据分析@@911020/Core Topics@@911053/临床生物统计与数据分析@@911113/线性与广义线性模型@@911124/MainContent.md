## 引言
线性和[广义线性模型](@entry_id:171019)（GLM）是现代统计学和数据科学的基石，尤其在生物信息学和医学数据分析领域，它们为从复杂数据中提取洞见提供了强大的框架。然而，经典的线性模型在处理非连续、非正态分布的响应变量（如基因是否激活的二元数据或突变计数的计数数据）时面临着根本性的局限。本文旨在系统性地填补这一认知空白，为读者构建一个从理论到实践的完整知识体系。

在接下来的内容中，我们将分三个部分展开：第一章“原理与机制”将深入剖析从[高斯-马尔可夫定理](@entry_id:138437)到GLM核心三要素的数学基础，并详解[模型拟合](@entry_id:265652)算法；第二章“应用与跨学科联系”将展示这些模型如何在生物统计学、流行病学和计算神经科学等领域解决实际问题，并探讨其与生存分析、因果推断等领域的深刻联系；最后，“动手实践”部分将通过具体的编程练习，引导读者将理论知识应用于实际的数据分析挑战中。通过本次学习，您将掌握这一强大建模工具的核心思想与应用技巧，为您的研究和分析工作打下坚实的基础。

## 原理与机制

本章深入探讨线性和广义线性模型的数学原理与核心机制。我们将从经典[线性模型](@entry_id:178302)的基石——[高斯-马尔可夫定理](@entry_id:138437)——开始，阐明其对普通最小二乘（OLS）估计量性质的保证。随后，我们将模型推广至[广义线性模型](@entry_id:171019)（GLM）的框架，该框架能够处理非正态响应变量，如生物信息学中常见的二元或计数数据。我们将详细剖析GLM的三个核心组成部分：[指数族](@entry_id:263444)分布、[线性预测](@entry_id:180569)器和连接函数。接着，我们将阐述[模型拟合](@entry_id:265652)的关键算法——[迭代重加权最小二乘法](@entry_id:175255)（IRLS），并通过具体计算案例来展示其运作方式。我们还将讨论[准似然](@entry_id:169341)方法如何处理[过度离散](@entry_id:263748)问题，并重点阐释如何在生物医学背景下准确解释模型系数。最后，本章将介绍用于[模型比较](@entry_id:266577)和选择的[信息准则](@entry_id:636495)，如AIC和BIC，为在多个候选模型中进行抉择提供理论依据。

### 经典线性模型：[高斯-马尔可夫定理](@entry_id:138437)

[线性回归](@entry_id:142318)是统计建模的基石。在许多生物医学研究中，例如一个旨在通过临床和分子协变量（如年龄、肿瘤分期、通路活性得分等）来建模连续[信使核糖核酸](@entry_id:147846)（mRNA）表达表型的项目中，线性模型是首选的分析工具 [@problem_id:4578852]。

该模型可以严谨地表述为：
$$
y = X\beta + \varepsilon
$$
其中：
- $y$ 是一个 $n \times 1$ 的响应向量，代表 $n$ 个病人的mRNA表达水平。
- $X$ 是一个 $n \times p$ 的[设计矩阵](@entry_id:165826)，包含了 $p$ 个协变量。在经典理论中，该矩阵被视为固定的（非随机的）。
- $\beta$ 是一个 $p \times 1$ 的未知系数向量，代表了各个协变量对响应的影响，是我们需要估计的目标。
- $\varepsilon$ 是一个 $n \times 1$ 的误差向量，代表了模型未能解释的随机变异。

**普通最小二乘法（OLS）** 是一种估计参数 $\beta$ 的标准方法，其目标是最小化残差平方和（RSS）：
$$
\text{RSS}(\beta) = \varepsilon^\top\varepsilon = (y - X\beta)^\top(y - X\beta)
$$
通过对 $\beta$ 求导并令其为零，我们得到[OLS估计量](@entry_id:177304)：
$$
\hat{\beta}_{\text{OLS}} = (X^\top X)^{-1} X^\top y
$$
为了使这个解唯一存在，矩阵 $X$ 必须具有**[满列秩](@entry_id:749628)**，即不存在完全[多重共线性](@entry_id:141597)。

然而，仅仅得到一个估计量是不够的，我们必须了解其统计性质。**[高斯-马尔可夫定理](@entry_id:138437)**为我们提供了保证[OLS估计量](@entry_id:177304)具有优良性质的一组核心假设。这些假设对于确保推断的科学有效性至关重要 [@problem_id:4578852]。

**[高斯-马尔可夫假设](@entry_id:165534)**如下：
1.  **线性**：模型在参数上是线性的，即 $y = X\beta + \varepsilon$。
2.  **严格[外生性](@entry_id:146270)**：给定设计矩阵 $X$，误差项的[条件期望](@entry_id:159140)为零，即 $E[\varepsilon \mid X] = 0$。这个假设意味着协变量与任何观测的误差项都不相关。这是确保估计量无偏性的关键。
3.  **满秩**：[设计矩阵](@entry_id:165826) $X$ 的列是[线性独立](@entry_id:153759)的，即 $\text{rank}(X) = p$。这确保了 $\hat{\beta}_{\text{OLS}}$ 的唯一性。
4.  **球形误差**：误差项具有**[同方差性](@entry_id:634679)**（homoscedasticity）并且**互不相关**（non-autocorrelation）。这通过误差向量的条件协方差矩阵来表示：
    $$
    \text{Var}(\varepsilon \mid X) = E[\varepsilon\varepsilon^\top \mid X] = \sigma^2 I_n
    $$
    其中 $\sigma^2$ 是一个正常数标量，代表[误差方差](@entry_id:636041)，$I_n$ 是 $n \times n$ 的[单位矩阵](@entry_id:156724)。

[高斯-马尔可夫定理](@entry_id:138437)的结论是：
- 在假设1-3成立时，[OLS估计量](@entry_id:177304)是**无偏的**（unbiased），即 $E[\hat{\beta}_{\text{OLS}} \mid X] = \beta$。
- 在假设1-4全部成立时，[OLS估计量](@entry_id:177304)是**[最佳线性无偏估计量](@entry_id:137602)**（Best Linear Unbiased Estimator, BLUE）。“最佳”意味着在所有线性和无偏的估计量中，$\hat{\beta}_{\text{OLS}}$ 具有最小的方差。

值得强调的是，[高斯-马尔可夫定理](@entry_id:138437)**不要求**误差项服从正态分布。[正态性假设](@entry_id:170614)（即 $\varepsilon \mid X \sim \mathcal{N}(0, \sigma^2 I_n)$）是进行精确小样本推断（如构建[t检验](@entry_id:272234)和F检验）所需要的，但对于BLUE性质本身并非必需 [@problem_id:4578850]。

如果球形误差假设（假设4）被违反，例如出现**异方差性**（heteroskedasticity, $\text{Var}(\varepsilon_i \mid X) = \sigma_i^2$）或**[自相关](@entry_id:138991)**（autocorrelation, $\text{Cov}(\varepsilon_i, \varepsilon_j \mid X) \neq 0$），那么[误差协方差矩阵](@entry_id:749077)将变为 $\text{Var}(\varepsilon \mid X) = \Omega \neq \sigma^2 I_n$。在这种情况下，[OLS估计量](@entry_id:177304)虽然仍然无偏，但不再是BLUE，即它不再是方差最小的线性无偏估计量。更有效的估计量是**[广义最小二乘法](@entry_id:272590)（GLS）** 估计量，它考虑了误差的协方差结构。

### OLS与最大似然估计（MLE）的关系

除了最小二乘法，**[最大似然估计](@entry_id:142509)（MLE）** 是另一种强大的参数估计框架。MLE需要为数据指定一个完整的[参数化](@entry_id:265163)概率分布。为了理解OLS与MLE的关系，我们考虑在线性模型中加入[误差的正态性](@entry_id:634130)假设 [@problem_id:4578850]。

假设误差是独立同分布（i.i.d.）的正态随机变量，即 $\varepsilon \sim \mathcal{N}(0, \sigma^2 I_n)$。这意味着响应向量 $y$ 服从[多元正态分布](@entry_id:175229) $y \sim \mathcal{N}(X\beta, \sigma^2 I_n)$。其对数似然函数为：
$$
\ell(\beta, \sigma^2 \mid y, X) = -\frac{n}{2}\ln(2\pi) - \frac{n}{2}\ln(\sigma^2) - \frac{1}{2\sigma^2} (y - X\beta)^\top(y - X\beta)
$$
为了找到 $\beta$ 的[最大似然估计](@entry_id:142509)，我们需要最大化这个表达式。观察上式，最大化 $\ell(\beta, \sigma^2)$ 等价于最小化残差平方和 $(y - X\beta)^\top(y - X\beta)$。这正是OLS的目标函数。因此，**当且仅当线性模型的误差项服从i.i.d.正态分布时，[OLS估计量](@entry_id:177304)与MLE是完全一致的**。

如果误差服从正态分布但非球形，即 $\varepsilon \sim \mathcal{N}(0, \Sigma)$，其中 $\Sigma \neq \sigma^2 I_n$。那么对数似然函数中的二次型变为 $(y - X\beta)^\top \Sigma^{-1} (y - X\beta)$。此时，$\beta$ 的MLE是通过最小化这个加权的二次型得到的，这恰恰是GLS估计量的定义。

这个联系揭示了OLS的深刻地位：它既是一个在相当宽松的假设下具有最优性的[非参数方法](@entry_id:138925)（从BLUE的角度看），又可以在更强的正态分布假设下被视为一个完全[参数化](@entry_id:265163)的[最大似然](@entry_id:146147)方法。

### [广义线性模型](@entry_id:171019)（GLM）框架

经典[线性模型](@entry_id:178302)在响应变量为连续且近似正态分布时表现优异。但在生物医学数据分析中，我们经常遇到其他类型的响应变量，例如：
- 临床试验中的不良事件是否发生（二元数据：0或1）。
- 基因组测序中每个肿瘤的体细胞突变数量（计数数据：0, 1, 2, ...）。

对于这[类数](@entry_id:156164)据，[线性模型](@entry_id:178302)的基本假设（如常数方差和正态性）不再成立。**广义线性模型（GLM）** 正是为了处理这些情况而生，它通过以下三个组成部分对经典模型进行了扩展 [@problem_id:4578835]：

1.  **随机部分（Random Component）**：假定响应变量 $Y$ 的条件分布属于**[指数族](@entry_id:263444)分布**（Exponential Family）。这是一个广泛的分布族，包括了正态分布、泊松分布、二项分布、伽马分布和[负二项分布](@entry_id:262151)等。

2.  **系统部分（Systematic Component）**：与线性模型一样，协变量通过一个**线性预测器（linear predictor）** $\eta$ 对模型产生影响：
    $$
    \eta = X\beta
    $$

3.  **连接部分（Link Component）**：通过一个可微的[单调函数](@entry_id:145115)——**[连接函数](@entry_id:636388)（link function）** $g(\cdot)$，将响应变量的[条件期望](@entry_id:159140) $\mu = E[Y \mid X]$ 与线性预测器联系起来：
    $$
    g(\mu) = \eta
    $$
    其逆函数 $g^{-1}(\cdot)$ 则将线性预测器映射回[期望值](@entry_id:150961)：$\mu = g^{-1}(\eta)$。

这个三元结构 $(Y \sim \text{ExpFam}, \eta = X\beta, g(\mu)=\eta)$ 提供了极大的灵活性，允许我们为不同类型的数据选择合适的分布和[连接函数](@entry_id:636388)，同时保留了线性预测器的简洁和[可解释性](@entry_id:637759)。

### [指数族](@entry_id:263444)分布的原理

[指数族](@entry_id:263444)是GL[M理论](@entry_id:161892)的核心。一个单参数的**正则[指数族](@entry_id:263444)（regular canonical exponential family）** 的[概率密度](@entry_id:143866)或[质量函数](@entry_id:158970)可以写成以下标准形式 [@problem_id:5197907]：
$$
f(y; \theta) = h(y)\exp\left(y\theta - b(\theta)\right)
$$
其中：
- $y$ 是观测值。
- $\theta$ 是分布的**自然参数（natural parameter）** 或**正则参数（canonical parameter）**。
- $h(y)$ 是一个非负的基函数。
- $b(\theta)$ 被称为**[对数配分函数](@entry_id:165248)（log-partition function）** 或[累积量生成函数](@entry_id:748109)，它确保了概率分布积分为1，即 $b(\theta) = \log \int h(y) \exp(y\theta) d\nu(y)$，其中 $\nu$ 是一个固定的支配测度。

一个分布属于正则[指数族](@entry_id:263444)的条件是严格的。除了能够写成上述形式，还需要满足：分布的支撑集（即 $f(y; \theta)>0$ 的 $y$ 的集合）不依赖于 $\theta$，并且自然[参数空间](@entry_id:178581) $\Theta = \{\theta \in \mathbb{R} : \int h(y) e^{y\theta} d\nu(y)  \infty\}$ 是一个开区间 [@problem_id:5197907]。

[指数族](@entry_id:263444)形式之所以强大，是因为它提供了一套通用的工具来推导[分布的矩](@entry_id:156454)。通过对对数似然函数求导，可以证明以下重要关系：
- **均值**：$E[Y] = \mu = b'(\theta)$
- **方差**：$\text{Var}(Y) = b''(\theta)$

在更一般的GLM框架中，我们引入一个**[离散度](@entry_id:168823)参数（dispersion parameter）** $\phi$，此时密度函数为 $f(y; \theta, \phi) = \exp\left(\frac{y\theta - b(\theta)}{a(\phi)} + c(y, \phi)\right)$，方差则变为 $\text{Var}(Y) = b''(\theta) a(\phi)$。$V(\mu) = b''(\theta)$ 被称为**方差函数（variance function）**，它描述了方差如何随均值变化，这是该分布的固有特征。

### 连接函数与方差函数

[连接函数](@entry_id:636388)的选择是GLM建模中的一个关键步骤。

#### 正则连接函数

对于[指数族](@entry_id:263444)中的每个分布，都存在一个特殊的连接函数，称为**正则连接函数（canonical link）**。这个[连接函数](@entry_id:636388)使得线性预测器 $\eta$ 直接等于自然参数 $\theta$，即 $g(\mu) = \theta$ [@problem_id:4578835]。这种选择在数学上最为简洁，并能简化最大似然估计的计算。

以下是几种常见分布的正则连接函数和方差函数 [@problem_id:4578840]：
- **伯努利/二项分布**：用于建模二[元数据](@entry_id:275500)（如基因激活状态）或比例数据。
    - 均值 $\mu$ 是成功的概率。
    - 正则连接是 **logit** 函数：$g(\mu) = \ln\left(\frac{\mu}{1-\mu}\right)$。
    - 方差函数是 $V(\mu) = \mu(1-\mu)$。
- **泊松分布**：用于建模计数数据（如UMI计数）。
    - 均值 $\mu$ 是事件发生的速率。
    - 正则连接是 **log** 函数：$g(\mu) = \ln(\mu)$。
    - 方差函数是 $V(\mu) = \mu$。
- **伽马分布**：用于建模[右偏](@entry_id:180351)的正连续数据（如归一化读数深度）。
    - 正则连接是 **inverse** 函数：$g(\mu) = \mu^{-1}$。
    - 方差函数是 $V(\mu) = \mu^2$。
- **高斯（正态）分布**：
    - 正则连接是 **identity** 函数：$g(\mu) = \mu$。这使得GLM退化为经典[线性模型](@entry_id:178302)。
    - 方差函数是 $V(\mu) = 1$（常数）。

#### 非正则[连接函数](@entry_id:636388)

除了正则连接，我们也可以选择其他满足条件的函数作为**非正则连接函数（non-canonical link）**。例如，对于二元数据，除了logit，**probit** 函数（$g(\mu) = \Phi^{-1}(\mu)$，其中 $\Phi$ 是标准正态CDF）也是一个常见的选择。重要的是，选择不同的连接函数会改变模型（即均值与协变量的关系），但不会改变随机部分所决定的方差函数 $V(\mu)$ [@problem_id:4578835]。

### [模型拟合](@entry_id:265652)：[迭代重加权最小二乘法](@entry_id:175255)（IRLS）

GLM的参数 $\beta$ 通常通过[最大似然估计](@entry_id:142509)得到。由于[似然方程](@entry_id:164995)通常是非线性的，需要使用[数值优化](@entry_id:138060)算法求解。**[迭代重加权最小二乘法](@entry_id:175255)（IRLS）** 是最常用的算法，它等价于Fisher评分法（Fisher Scoring）。

IRLS的核心思想是通过一系列加权[最小二乘回归](@entry_id:262382)来逼近最大似然估计。在每次迭代中，算法会更新一个“工作响应”（working response）向量 $z$ 和一个权重[对角矩阵](@entry_id:637782) $W$，然后求解加权[最小二乘问题](@entry_id:164198)来更新 $\beta$。

在第 $(t)$ 次迭代，更新 $\beta$ 的方程为：
$$
(X^\top W^{(t)} X) \beta^{(t+1)} = X^\top W^{(t)} z^{(t)}
$$
其中，工作响应 $z^{(t)}$ 和权重 $W^{(t)}$ 的定义与模型的[连接函数](@entry_id:636388)和方差函数直接相关。对于第 $i$ 个观测：
- **工作响应**：$z_i^{(t)} = \eta_i^{(t)} + (y_i - \mu_i^{(t)})g'(\mu_i^{(t)})$，其中 $\eta_i^{(t)}$ 和 $\mu_i^{(t)}$ 是基于当前估计 $\beta^{(t)}$ 计算的[线性预测](@entry_id:180569)器和均值。
- **权重**：$w_i^{(t)} = \frac{1}{a(\phi)V(\mu_i^{(t)})(g'(\mu_i^{(t)}))^2}$。

这个权重公式揭示了IRLS的精髓：在每次迭代中，对每个观测值的权重由其自身的方差决定。具体来说，工作响应的方差为 $a(\phi)V(\mu_i)(g'(\mu_i))^2$，而权重正是这个方差的倒数。因此，方差较小的观测值（即信息量更大的观测值）在拟合中被赋予更大的权重 [@problem_id:4578840]。

例如，对于使用正则log连接的泊松回归，我们有 $V(\mu)=\mu$ 和 $g'(\mu)=1/\mu$。代入权重公式，我们得到一个非常简洁的结果：$w_i = \mu_i$。这意味着在拟合过程中，[期望计数](@entry_id:162854)值越高的观测点，其权重也越大。

**IRLS计算示例**
让我们通过一个具体例子来演示IRLS的单步计算过程 [@problem_id:4578845]。假设我们正在为一个转录因子敲低实验建模mRNA片段计数，使用泊松回归和log连接。数据如下：
- 协变量（剂量）：$x = \begin{pmatrix} 0  1  2  3 \end{pmatrix}^\top$
- 观测计数：$y = \begin{pmatrix} 1  2  4  8 \end{pmatrix}^\top$
- [设计矩阵](@entry_id:165826)：$X = \begin{pmatrix} 1  0 \\ 1  1 \\ 1  2 \\ 1  3 \end{pmatrix}$
- 初始系数：$\beta^{(0)} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}$

**第1步：计算基于 $\beta^{(0)}$ 的量**
- [线性预测](@entry_id:180569)器：$\eta^{(0)} = X \beta^{(0)} = \begin{pmatrix} 0  0  0  0 \end{pmatrix}^\top$。
- 期望均值：$\mu_i^{(0)} = \exp(\eta_i^{(0)})$，所以 $\mu^{(0)} = \begin{pmatrix} 1  1  1  1 \end{pmatrix}^\top$。
- 权重矩阵：$W^{(0)}$ 是一个对角矩阵，对角[线元](@entry_id:196833)素为 $\mu_i^{(0)}$。因此，$W^{(0)} = I_4$（4x4单位矩阵）。
- 工作响应：$z_i^{(0)} = \eta_i^{(0)} + \frac{y_i - \mu_i^{(0)}}{\mu_i^{(0)}}$。
    - $z_1^{(0)} = 0 + (1-1)/1 = 0$
    - $z_2^{(0)} = 0 + (2-1)/1 = 1$
    - $z_3^{(0)} = 0 + (4-1)/1 = 3$
    - $z_4^{(0)} = 0 + (8-1)/1 = 7$
    所以 $z^{(0)} = \begin{pmatrix} 0  1  3  7 \end{pmatrix}^\top$。

**第2步：求解加权[正规方程](@entry_id:142238)**
我们需要求解 $(X^\top W^{(0)} X) \beta^{(1)} = X^\top W^{(0)} z^{(0)}$。由于 $W^{(0)} = I_4$，方程简化为 $(X^\top X) \beta^{(1)} = X^\top z^{(0)}$。
- $X^\top X = \begin{pmatrix} 4  6 \\ 6  14 \end{pmatrix}$
- $X^\top z^{(0)} = \begin{pmatrix} 11 \\ 28 \end{pmatrix}$
求解线性方程组 $\begin{pmatrix} 4  6 \\ 6  14 \end{pmatrix} \beta^{(1)} = \begin{pmatrix} 11 \\ 28 \end{pmatrix}$，我们得到：
$$
\beta^{(1)} = \begin{pmatrix} 4  6 \\ 6  14 \end{pmatrix}^{-1} \begin{pmatrix} 11 \\ 28 \end{pmatrix} = \frac{1}{20} \begin{pmatrix} 14  -6 \\ -6  4 \end{pmatrix} \begin{pmatrix} 11 \\ 28 \end{pmatrix} = \begin{pmatrix} -7/10 \\ 23/10 \end{pmatrix}
$$
这就是[IRLS算法](@entry_id:750839)一步迭代后更新的系数向量。通过重复这个过程，算法将收敛到[最大似然估计值](@entry_id:165819)。

### 扩展GLM：[准似然](@entry_id:169341)与过度离散

GLM的一个重要假设是均值和方差之间的关系由所选的分布（即方差函数 $V(\mu)$）精确定义。然而在实践中，数据的变异性常常超出模型的预期，这种现象称为**过度离散（overdispersion）**。例如，对于泊松数据，我们可能发现样本方差显著大于样本均值。

**[准似然](@entry_id:169341)（Quasi-likelihood）** 方法为处理过度离散提供了一个强大的框架 [@problem_id:4578839]。它放宽了对完整概率分布的假设，仅需指定：
1.  均值与协变量的关系（通过[连接函数](@entry_id:636388)）。
2.  方差与均值的关系，通常形式为 $\text{Var}(Y) = \phi V(\mu)$，其中 $\phi$ 是未知的[离散度](@entry_id:168823)参数。

[准似然](@entry_id:169341)的估计方程（或称准得分方程）与最大似然的得分方程形式非常相似：
$$
\sum_{i=1}^{m} \frac{\partial \mu_i}{\partial \beta_j} \frac{1}{\text{Var}(Y_i)} (y_i - \mu_i) = 0
$$
将 $\text{Var}(Y_i) = \phi V(\mu_i)$ 代入，我们发现[离散度](@entry_id:168823)参数 $\phi$ 可以从方程中消去。这意味着**对均值参数 $\beta$ 的[点估计](@entry_id:174544)不受过度离散的影响**。例如，在一个用于分析靶向[亚硫酸氢盐测序](@entry_id:274841)数据的[过度离散](@entry_id:263748)[二项模型](@entry_id:275034)中，即使真实方差是 $\phi n_i \mu_i(1-\mu_i)$，我们求出的公共甲基化概率的估计值 $\hat{\mu}$ 仍然是总甲基化读数除以总读数，$\hat{\mu} = (\sum y_i) / (\sum n_i)$，这与标准[二项模型](@entry_id:275034)的估计完全相同 [@problem_id:4578839]。[过度离散](@entry_id:263748)参数 $\phi$ 会在后续步骤中被估计，并用于调整标准误和进行假设检验，使得推断更加稳健。

### 模型系数的解释

正确解释模型系数是应用GLM的关键。由于连接函数的存在，系数的解释不再像[线性模型](@entry_id:178302)中那样直观。

#### 逻辑斯蒂回归（Logit连接）
在用于[二元结果](@entry_id:173636)（如疾病状态）的逻辑斯蒂回归中，模型为 $\ln(\frac{p}{1-p}) = X\beta$ [@problem_id:4578882]。
- **线性预测器** $\eta = X\beta$ 直接等于**对数优势（log-odds）**。
- 系数 $\beta_j$ 表示当协变量 $X_j$ 增加一个单位时，对数优势的**加性**变化量。
- 对系数进行指数化，$\exp(\beta_j)$，得到**优势比（Odds Ratio, OR）**。这意味着 $X_j$ 每增加一个单位，事件发生的优势将乘以 $\exp(\beta_j)$。例如，如果一个生物标志物的系数是 $0.7$，那么该标志物每增加一个单位，疾病的优势将变为原来的 $\exp(0.7) \approx 2.01$ 倍。
- 这种效应是在**优势尺度**上是乘性的，而不是在概率尺度上。一个单位的协变量变化对概率的绝对影响取决于基线概率。

#### 泊松回归（Log连接）
在用于计数数据（如突变计数）的泊松回归或对数线性模型中，模型为 $\ln(\mu) = X\beta$ [@problem_id:4578880]。
- **[线性预测](@entry_id:180569)器** $\eta = X\beta$ 等于**[对数期](@entry_id:165031)望均值**。
- 系数 $\beta_j$ 表示当协变量 $X_j$ 增加一个单位时，对数均值的**加性**变化量。
- 对系数进行指数化，$\exp(\beta_j)$，得到一个**率比（Rate Ratio, RR）** 或**相对风险（Relative Risk）**。这意味着 $X_j$ 每增加一个单位，期望均值（计数）将乘以 $\exp(\beta_j)$。
- **偏移项（Offset）** 的作用：在基因组学中，我们常常关心的是[突变率](@entry_id:136737)而不是原始计数。如果基因长度 $L$ 不同，我们可以将 $\ln(L)$ 作为偏移项加入模型：$\ln(\mu) = \ln(L) + X\beta$。这等价于对[突变率](@entry_id:136737) $\mu/L$ 建模：$\ln(\mu/L) = X\beta$。在这种情况下，$\exp(\beta_j)$ 解释为协变量 $X_j$ 每增加一个单位，**期望率**（而非[期望计数](@entry_id:162854)）乘以的倍数 [@problem_id:4578880]。

### [模型比较](@entry_id:266577)与选择

在实际应用中，我们常常需要比较多个候选模型。选择模型是在**[拟合优度](@entry_id:637026)（goodness-of-fit）** 和**[模型复杂度](@entry_id:145563)（model complexity）** 之间进行权衡。过于简单的模型可能无法捕捉数据中的重要信号（[欠拟合](@entry_id:634904)），而过于复杂的模型可能拟合了噪声，导致其在新数据上表现不佳（过拟合）。**[信息准则](@entry_id:636495)（Information Criteria）** 为此提供了一个量化的框架。

#### [赤池信息准则](@entry_id:139671)（AIC）

**AIC** 源于对模型预测性能的估计。其目标是选择一个在Kullback-Leibler散度意义下最接近真实数据生成过程的模型。它通过对模型在训练数据上的最大化[对数似然](@entry_id:273783)进行惩罚来修正[过拟合](@entry_id:139093)的偏误 [@problem_id:4578849]。
$$
\text{AIC} = -2\ell(\hat{\theta}) + 2k
$$
- $\ell(\hat{\theta})$ 是模型的最大化[对数似然](@entry_id:273783)值，反映了模型的[拟合优度](@entry_id:637026)。
- $k$ 是模型中自由参数的总数，包括所有回归系数以及任何被估计的[离散度](@entry_id:168823)参数（如高斯模型中的 $\sigma^2$）。
- $2k$ 是对[模型复杂度](@entry_id:145563)的惩罚项。
AIC值越小，模型越优。在实践中，AIC倾向于选择具有最佳预测性能的模型，即使真实模型不在候选集中，它也会选择“最佳近似”模型。

#### [贝叶斯信息准则](@entry_id:142416)（BIC）

**BIC**（也称Schwarz[信息准则](@entry_id:636495)，SIC）源于对模型后验概率的贝叶斯近似 [@problem_id:4578849]。
$$
\text{BIC} = -2\ell(\hat{\theta}) + k\ln(n)
$$
- $\ell(\hat{\theta})$ 和 $k$ 的含义与AIC中相同。
- $n$ 是样本量。
- 惩罚项 $k\ln(n)$ 随样本量 $n$ 的增加而增加。

#### AIC vs. BIC
- **惩罚力度**：当样本量 $n \ge 8$ 时，$\ln(n) > 2$，因此BIC对[模型复杂度](@entry_id:145563)的惩罚比AIC更重。
- **理论性质**：
    - AIC是**[渐近有效](@entry_id:167883)（asymptotically efficient）**的。在真实模型未知或非常复杂的情况下，它倾向于选择能给出最佳预测的模型。
    - BIC是**一致（consistent）**的。如果真实模型在候选模型集中，并且模型数量固定，随着样本量 $n \to \infty$，BIC选择真实模型的概率将趋近于1。
- **实践选择**：如果目标是**预测**，AIC通常是更好的选择。如果目标是**解释**或识别出“真实”的、更简洁的[生成模型](@entry_id:177561)，BIC可能更受青睐，因为它倾向于选择更简单的模型 [@problem_id:4578849]。