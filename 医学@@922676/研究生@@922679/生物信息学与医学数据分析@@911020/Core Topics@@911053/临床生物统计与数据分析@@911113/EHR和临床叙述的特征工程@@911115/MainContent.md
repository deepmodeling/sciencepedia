## 引言
随着电子健康记录（EHR）和临床叙事文本数据的爆炸式增长，我们迎来了前所未有的机遇，能够利用这些海量数据改进临床决策、预测疾病风险并推动[精准医疗](@entry_id:152668)。然而，这些原始数据本质上是异构、稀疏且充满噪声的，直接将其用于高级分析极具挑战性。如何系统性地将这些杂乱无章的原始记录转化为可供[机器学习模型](@entry_id:262335)使用的、高质量且有临床意义的特征，是释放其全部潜力的关键瓶颈，也是[特征工程](@entry_id:174925)在生物医学数据科学领域的核心任务。

本文旨在全面解答这一问题，为读者提供一个从理论到实践的完整指南。我们将深入探讨从原始EHR和临床文本中提取、构建和验证特征的全过程，确保分析的稳健性与[可重复性](@entry_id:194541)。

在接下来的章节中，你将学习到：
*   **原理与机制**：我们将首先奠定基础，介绍表示异构临床数据的统一事件模型，并分别探讨针对结构化数据（如诊断编码、实验室结果）和非结构化数据（如医生笔记）的[特征提取](@entry_id:164394)技术，包括语义标准化、文本[向量化](@entry_id:193244)以及断言状态检测。同时，我们还将讨论如何应对时序性、数据缺失等现实世界的数据质量挑战。
*   **应用与跨学科交叉**：本章将展示这些特征工程技术在真实世界中的强大应用，例如构建临床共病指数、进行计算表型分析以及融合多模态证据。我们还将探索其与因果推断、人因工程学和[网络医学](@entry_id:273823)等领域的交叉，揭示[特征工程](@entry_id:174925)在推动现代医学研究中的核心作用。
*   **动手实践**：通过一系列精心设计的编程练习，你将有机会亲手实现关键的[特征工程](@entry_id:174925)管道，从而将在前两章学到的理论知识转化为解决实际问题的能力，加深对核心概念的理解。

通过学习本文，你将掌握一套将原始临床数据转化为可操作洞见的系统方法论，为构建可靠、可解释的临床智能系统打下坚实的基础。

## 原理与机制

本章旨在阐述从原始电子健康记录 (EHR) 和临床叙事文本中构建有意义特征的核心原理与关键机制。我们将系统性地探讨[数据表示](@entry_id:636977)、针对结构化与非结构化数据的[特征提取](@entry_id:164394)技术，以及在处理现实世界临床数据时遇到的关键挑战，如时序性、数据缺失和评估稳健性。本章的目标是为读者提供一个将原始临床数据转化为可用于高级分析与[预测建模](@entry_id:166398)的、高质量特征的理论与实践框架。

### 基础[数据表示](@entry_id:636977)：事件模型

在深入[特征工程](@entry_id:174925)的具体技术之前，我们必须首先建立一个能够一致且无[歧义](@entry_id:276744)地表示异构临床数据的统一框架。EHR 系统中的数据源通常可分为两类：**结构化数据**和**非结构化数据**。结构化数据遵循预定义的模式和标准化的词汇表，例如实验室检测结果、诊断编码和药物处方。非结构化数据则是自由格式的文本，如医生病程记录、影像学报告和出院小结，它们缺乏强制的字段结构。

为了进行可重复的分析，任何特征提取流程都必须基于一个能够无损表示原始信息的事件模型。一个可重复的转换是指一个确定性函数 $f$，它将一组原始事件 $E$ 映射到一个特征向量 $F$，即 $F = f(E)$，并且该映射对于与分析无关的事件排列应保持不变。这意味着，我们需要一个足够丰富的事件模式，以明确地捕捉每个数据点的来源、语义和上下文。

一个能够保证特征工程**[可重复性](@entry_id:194541) (reproducibility)** 的最小事件模式，必须包含以下几个关键字段 [@problem_id:4563144]：

*   `patient_id`: 患者唯一标识符，用于将数据聚合到特定个体。
*   `visit_id`: 就诊或入院唯一标识符，用于区分不同的临床环境（如住院与门诊）。
*   `timestamp`: 事件时间戳，是所有[时序分析](@entry_id:178997)的基础。
*   `source_type`: 数据来源类型，指明数据是来自实验室系统、药房、还是临床笔记等，这对于正确解释数据至关重要。
*   `code`: 标准化概念编码，用于无歧义地标识测量或观察的语义内容（如一个特定的实验室检验项目）。
*   `value`: 记录的数值，即测量的量值。
*   `unit`: 测量单位，它赋予数值以物理意义（如 mg/dL）。
*   `note_text`: 笔记文本，对于非结构化数据，这是原始信息的载体。

这个八元组 $(p, v, t, s, c, x, u, n)$ 构成了一个通用的事件表示。对于结构化事件，如实验室结果，`value` ($x$) 和 `unit` ($u$) 字段会被填充，而 `note_text` ($n$) 可能为空。相反，对于非结构化事件，如临床笔记，`note_text` ($n$) 会被填充，而 `value` 和 `unit` 可能为空。这种统一的模型确保了所有原始信息的**溯源性 (provenance)** 和语义完整性，为后续所有特征的计算提供了坚实且可审计的基础。

### 结构化数据的特征工程

结构化数据，特别是那些以编码形式存在的数据，是临床[预测建模](@entry_id:166398)的基石。本节将探讨如何从这些编码数据中提取有意义的特征。

#### 语义标准化：临床术语系统

EHR 中的原始编码数据可能来自不同的系统，使用不同的本地编码。为了实现数据的**互操作性 (interoperability)** 和分析的一致性，首要步骤是将其映射到标准化的临床术语系统。不同的术语系统被设计用来覆盖特定的语义领域 [@problem_id:4563189]：

*   **国际疾病分类 (International Classification of Diseases, ICD)**: 主要用于编码诊断和死亡原因，常用于计费和流行病学研究。
*   **医疗程序与服务代码 (Current Procedural Terminology, CPT)**: 主要用于编码医生和门诊环境下的医疗服务和程序，是计費的核心。
*   **医学系统命名法—临床术语 (Systematized Nomenclature of Medicine—Clinical Terms, SNOMED CT)**: 这是一个全面的临床本体，覆盖了临床发现、疾病、程序、身体结构等多种概念，其丰富的层级关系支持高级的推理和分组。
*   **逻辑观察标识符名称和代码 (Logical Observation Identifiers Names and Codes, LOINC)**: 专注于唯一标识实验室检验、临床测量和观察项目。
*   **RxNorm**: 由美国国家医学图书馆维护，为临床药物及其成分、剂量形式提供标准化的名称和唯一标识符。

一个语义连贯的特征工程流程，会将出院诊断映射到 **ICD**，将实验室测试映射到 **LOINC**，将药物映射到 **RxNorm**，将程序映射到 **CPT**，而将从临床笔记中提取的更细粒度的临床发现映射到 **SNOMED CT**。这种标准化的映射 $g: E \to C$ 将原始事件 $E$ 转换为标准概念 $C$，是构建可比较和临床意义明确的特征的第一步。

#### 从编码中派生特征

一旦数据被标准化，我们就可以从编码的出现模式中提取多种类型的特征。三种最常见的特征原型是**发生 (occurrence)**、**频率 (frequency)** 和**负荷 (burden)** 特征。这些特征可以在不同的粒度上进行聚合，最常见的是**单次就诊 (per-visit)** 层面和**单个患者 (per-patient)** 层面 [@problem_id:4563111]。

*   **发生特征**: 这是一个二元指标，表示一个特定的编码是否在聚合单元（一次就诊或一个患者的全部记录）中至少出现一次。它捕捉了“有没有”这个信息。例如，一个患者级别的糖尿病发生特征，如果该患者在观察期内有任何一次就诊记录了糖尿病的 ICD 编码，则该特征值为 $1$，否则为 $0$。

*   **频率特征**: 这表示一个编码在聚合单元中出现的次数。它可以是编码在所有就诊中出现的总次数（患者级别），也可以是其在单次就诊中出现的次数（就诊级别）。它捕捉了“有多少”这个信息。

*   **负荷特征**: 这是一种加权频率，它将编码的出现与一个具有临床意义的权重（如疾病严重性、医疗成本或风险评分）相结合。患者级别的负荷特征通常是该编码在所有就诊中出现的总次数乘以其对应的权重。例如，给定患者的就诊记录为：
    *   $v_1$: ICD 编码 $\{c_{\mathrm{I}2}\}$
    *   $v_2$: ICD 编码 $\{c_{\mathrm{I}2}\}$
    *   $v_3$: 无 $c_{\mathrm{I}2}$ 编码
    如果编码 $c_{\mathrm{I}2}$ 的负荷权重为 $1.5$，则该患者关于 $c_{\mathrm{I}2}$ 的频率特征为 $2$ (出现两次)，而负荷特征为 $1.5 \times 2 = 3.0$。发生特征为 $1$。这些不同的[特征类](@entry_id:160596)型允许模型从不同角度利用编码信息。

#### 高级主题：通过编码分组降低维度

直接使用细粒度的 ICD 或 SNOMED CT 编码作为特征（如通过[独热编码](@entry_id:170007)）会导致一个维度极高且极其稀疏的[特征空间](@entry_id:638014)（例如，可能有数千个不同的编码）。这会给许多[机器学习模型](@entry_id:262335)带来挑战。一种有效的[降维](@entry_id:142982)策略是**基于本体的编码分组 (ontology-based code grouping)**，例如，将数千个 ICD-10 编码聚合成几百个临床分类软件 (Clinical Classifications Software, CCS) 类别。

这种分组操作，可以形式化为一个从细粒度编码 $X$ 到粗粒度分组 $Z$ 的确定性映射 $Z = \pi(X)$。从信息论的角度看，由于 $Z$ 是 $X$ 的一个函数，变量构成了[马尔可夫链](@entry_id:150828) $Y \to X \to Z$，其中 $Y$ 是我们的预测目标（如再入院）。根据**[数据处理不等式](@entry_id:142686) (Data Processing Inequality)**，我们必然有 $I(Y; Z) \le I(Y; X)$，这意味着信息量在处理过程中不会增加，通常会减少 [@problem_id:4563188]。

信息损失的大小，即 $I(Y;X) - I(Y;Z)$，可以精确地表示为 KL 散度的[期望值](@entry_id:150961)：
$$\mathbb{E}_X\left[ \mathrm{KL}(\mathbb{P}(Y \mid X) \,\|\, \mathbb{P}(Y \mid Z)) \right]$$
这个公式揭示了一个深刻的见解：当且仅当一个分组 $z$ 内的所有细粒度编码 $x$ 对于目标 $Y$ 的后验概率 $\mathbb{P}(Y \mid X=x)$ 都相同时，该分组操作才不会丢失任何关于 $Y$ 的信息。换句话说，如果一个[本体](@entry_id:264049)分组将临床上（相对于预测任务 $Y$）同质的编码聚集在一起，那么这种[降维](@entry_id:142982)就是无损的。对于分类任务，一个更宽松的条件是，只要组内所有编码的贝叶斯最优决策相同，0-1 损失就不会增加 [@problem_id:4563188]。这为我们选择和评估编码分组策略提供了理论依据。

### 临床叙事（非结构化数据）的特征工程

临床叙事文本蕴含着丰富的、结构化数据中缺失的细节信息。然而，将这些自由文本转化为机器可读的特征是一项独特的挑战。

#### 从文本到向量：基础表示方法

最基础的[文本表示](@entry_id:635254)方法是**[词袋模型](@entry_id:635726) (Bag-of-Words, BoW)**。该模型忽略单词顺序和语法，将文档表示为一个向量，其中每个维度对应词汇表中的一个词，其值可以是该词的计数或权重。

为了量化单词的重要性，**[词频-逆文档频率](@entry_id:634366) (Term Frequency-Inverse Document Frequency, [TF-IDF](@entry_id:634366))** 是一种广泛使用的加权方案 [@problem_id:4563206]。

*   **词频 (Term Frequency, TF)**: $TF(t, d)$ 是词 $t$ 在文档 $d$ 中出现的次数。它衡量了一个词在局部（单个文档中）的重要性。

*   **逆文档频率 (Inverse Document Frequency, IDF)**: $IDF(t) = \log\left(\frac{N}{DF(t)}\right)$，其中 $N$ 是语料库中的文档总数，$DF(t)$ 是包含词 $t$ 的文档数。它衡量了一个词在全局（整个语料库中）的稀有度或信息量。在语料库中普遍出现的词（如“patient”）其 IDF 值较低，而稀有但专业的词（如“sepsis”）其 IDF 值较高。

[TF-IDF](@entry_id:634366) 权重 $w(t,d) = TF(t, d) \times IDF(t)$ 因此能够突显那些在特定文档中频繁出现但在整个语料库中相对稀有的词，这些词往往最具信息量。

#### 捕获上下文和形态

BoW 模型的主要缺点是丢失了词序信息。**N-gram 模型**通过将连续的 $n$ 个词作为一个特征单元来部分弥补这一缺陷。例如，**bigrams (2-grams)** 对于理解否定语境至关重要。在句子 “no fever” 中，unigram 模型会分别看到特征 “no” 和 “fever”，而 bigram 模型会捕获到一个独立的特征 “no fever”。这使得线性模型可以为 “fever” 学习一个正权重（表示风险增加），同时为 “no fever” 学习一个负权重，从而有效区分肯定与否定的临床陈述，减少误报 [@problem_id:4563206]。

此外，标准的词级模型对形态变化很敏感。例如，未经词形还[原时](@entry_id:192124)，“diabetes” 和 “diabetic” 会被视为两个完全不同的特征。**字符 n-gram (Character n-grams)** 通过将单词分解为重叠的子字符串（如 trigrams “dia”, “iab”, “abe”）来解决这个问题。由于形态相关的词共享许多相同的字符 n-grams，它们的[向量表示](@entry_id:166424)会变得更加相似，从而提高了模型处理词形变化和拼写错误的鲁棒性，并有效缓解了[数据稀疏性](@entry_id:136465)问题 [@problem_id:4563206]。

#### 文本的语义标准化：UMLS 和概念唯一标识符

与结构化数据一样，临床文本也存在巨大的变异性。同一种临床概念可能以多种方式表达，例如，“myocardial infarction” 和 “heart attack” 指的是同一件事。这种**同义词现象 (synonymy)** 在不同医生、不同机构间普遍存在，导致基于表面词形的模型难以泛化。

**统一医学语言系统 (Unified Medical Language System, UMLS)** 提供了一个解决方案。UMLS 的 Metathesaurus 是一个庞大的知识库，它将来自不同医学词汇表（如 SNOMED CT, RxNorm）的[语义等价](@entry_id:754673)术语聚合到同一个**概念唯一标识符 (Concept Unique Identifier, CUI)**下。通过自然语言处理技术将文本中的实体链接到对应的 CUI，我们可以实现**概念标准化 (concept normalization)**。

这个过程在数学上等同于将词汇空间中的向量投影到一个语义概念空间中。考虑两个文档，$d_a$ 包含 “heart attack”，$d_b$ 包含 “myocardial infarction”。在基于词形的 BoW 空间中，它们的[向量表示](@entry_id:166424)是正交的，余弦相似度为 $0$。然而，在映射到 CUI 空间后，由于这两个术语共享同一个 CUI, 它们的[向量表示](@entry_id:166424)将变得完全相同，余弦相似度为 $1$ [@problem_id:4563183]。这种标准化消除了由同义词选择引起的无关变异，极大地增强了跨文档和跨机构的可比性，使模型能够学习到基于潜在临床概念而非表面词形的模式。

#### 理解断言状态：超越概念提取

从文本中成功提取出一个临床概念（如“肺炎”）只是第一步。我们还必须理解该概念的**断言状态 (assertion status)**，即作者是如何描述这个概念的。一个概念可能是：

*   **存在的 (present)**: 患者当前患有此病症。
*   **不存在的 (absent)**: 患者被明确指出没有此病症。
*   **可能的 (possible)**: 作者不确定此病症是否存在。
*   **过去的 (historical)**: 患者过去曾患有此病症，但当前没有。
*   **计划的 (planned)**: 计划为患者进行与此概念相关的诊断或治疗。

断言状态通常可以通过分析概念周围的**线索词 (cues)** 来确定。这些线索词可分为三类 [@problem_id:4563161]：

*   **否定线索词 (Negation cues)**: 如 "denies", "no signs of"，它们将其作用域内的概念标记为 `absent`。
*   **不确定性线索词 (Uncertainty cues)**: 如 "suggests", "possible", "rule out"，它们将其作用域内的概念标记为 `possible`。
*   **时序线索词 (Temporality cues)**: 如 "history of", "plan to"，它们将概念的时间参照点从当前（文档创建时间）转移到过去或未来，从而将其标记为 `historical` 或 `planned`。

通过构建基于规则或基于机器学习的分类器来识别这些线索词及其作用域，我们可以为每个提取的概念赋予更精确的语义标签，这对于构建高质量的临床表型和预测模型至关重要。

### 应对时序与数据质量挑战

现实世界的 EHR 数据本质上是时序性的、不完整的。成功的特征工程必须正面应对这些挑战。

#### 用于[预测建模](@entry_id:166398)的时序特征工程

在构建预测模型时（例如，预测患者出院后30天内的再入院风险），必须小心地处理时间轴，以避免**信息泄露 (information leakage)**。这需要一套严谨的窗口化定义 [@problem_id:4563180]：

*   **索引时间 (Index time, $\tau$)**: 这是我们进行预测的时间点，例如患者的出院时刻。
*   **观察窗口 (Observation window, $w_o$)**: 这是从索引时间回溯的一段时间，我们从这个窗口内的数据中提取特征。
*   **预测窗口 (Prediction window, $w_p$)**: 这是从索引时间向前看的一段时间，我们在这个窗口内观察目标事件是否发生（例如，30天）。
*   **间隔时间 (Gap time, $g$)**: 这是观察窗口结束点与索引时间之间的一个缓冲区。设置一个非零的间隔时间至关重要，因为临床数据的记录存在延迟（例如，一个在 $\tau$ 时刻前不久抽血的检验结果，可能在 $\tau$ 时刻之后才出现在 EHR 系统中）。忽略这种延迟会导致模型在训练时“看到未来”，从而产生过于乐观的性能估计。特征必须仅使用在 $\tau - g$ 时刻之前可用的数据构建。

基于这些定义，我们可以采用两种主要策略来生成训练样本：

1.  **里程碑法 (Landmarking)**: 在每个患者的轨迹中选择一些临床上有意义的、固定的时间点作为索引时间 $\tau$（如入院后的第1天、第3天、第7天）。在每个里程碑时刻，我们仅纳入当时仍“处于风险中”（即尚未发生目标事件）的患者，并为他们构建特征和标签。这在统计上对应于建模条件概率 $P(T_i \le \tau + w_p \mid T_i > \tau)$。

2.  **滚动窗口法 (Rolling window)**: 沿着每个患者的时间轴，以固定的步长（如每天）生成密集的索引时间点。对于每个有效的索引时间（即在目标事件发生之前），我们都生成一个训练样本。这种方法可以从单个患者身上产生大量样本，但必须注意样本间的相关性。

这两种方法都要求特征构建严格遵守观察窗口和间隔时间的约束，以确保模型的时序有效性 [@problem_id:4563180]。

#### 数据缺失问题

EHR 数据中的**缺失值 (missingness)** 无处不在，并且其模式本身可能就包含信息。理解缺失的机制对于正确处理至关重要。缺失机制通常分为三类 [@problem_id:4563199]：

*   **[完全随机缺失](@entry_id:170286) (Missing Completely At Random, MCAR)**: 缺失的概率与任何观测到的或未观测到的数据都无关。即 $P(R=1 \mid Y,X)=P(R=1)$，其中 $R=1$ 表示数据被观测到，$Y$ 是可能缺失的变量，$X$ 是其他观测变量。例如，随机的样本损坏。

*   **[随机缺失](@entry_id:168632) (Missing At Random, MAR)**: 缺失的概率与未观测到的值 $Y$ 本身无关，但可能与其他观测到的变量 $X$ 相关。即 $P(R=1 \mid Y,X)=P(R=1 \mid X)$。例如，医生可能更倾向于为年长的患者（年龄在 $X$ 中） ordering a certain lab test, but for any given age, the decision is not related to the patient's true (but yet unknown) lab value.

*   **[非随机缺失](@entry_id:163489) (Missing Not At Random, MNAR)**: 缺失的概率依赖于未观测到的值 $Y$ 本身，即使在控制了所有观测变量 $X$ 之后依然如此。这种情况也称为**信息性缺失 (informative missingness)**。例如，如果医生因为凭临床直觉认为患者的某项指标正常而没有开具检验单，那么该指标的缺失就与它的“正常”值有关，这就是 MNAR。

此外，必须将数据缺失与**删失 (censoring)** 区分开来。删失是指我们获得了关于一个值的部分信息，而非完全没有信息。例如，一个实验室检测的报告可能显示“低于检测下限 $L$”。这并不意味着值是缺失的；相反，它告诉我们该值落在区间 $(-\infty, L)$ 内。将删失数据错误地当作[缺失数据](@entry_id:271026)处理会丢弃宝贵的信息 [@problem_id:4563199]。

### 确保稳健性：评估与数据划分

特征工程的最终目标是提升模型的性能。然而，如果评估方法不当，我们可能会被特征的虚假表现所误导。核心风险在于**数据泄露 (data leakage)**，即训练过程无意中获取了[测试集](@entry_id:637546)的信息。

在处理纵向的 EHR 数据时，由于同一个患者在不同时间的记录（就诊）高度相关，传统的随机数据划分方法是危险的。以下是几种划分策略及其泄露风险 [@problem_id:4563205]：

*   **就诊层面划分 (Visit-level split)**: 随机地将所有就诊记录划分为训练集和[测试集](@entry_id:637546)。这种方法风险极高，因为同一个患者的就诊记录可能同时出现在训练集和[测试集](@entry_id:637546)。模型可以利用患者的固有特征（这些特征在多次就诊中是稳定的）来“记住”患者，而不是学习可泛化的模式。

*   **患者层面划分 (Patient-level split)**: 确保[训练集](@entry_id:636396)和测试集中的患者是完全不相交的。这是处理 EHR 数据时的黄金标准。它能更好地模拟模型在未来应用于全新患者时的表现。

*   **时序划分 (Temporal split)**: 选择一个时间点 $T^\star$，将该时间点之前的所有数据用作训练，之后的数据用作测试。这种方法模拟了模型的真实部署场景，但如果特征工程（如文本嵌入预训练、[数据标准化](@entry_id:147200)）是在整个数据集上完成的，信息仍然可能从未来“泄露”到过去。

为了进行稳健的模型评估和[超参数调优](@entry_id:143653)，**[嵌套交叉验证](@entry_id:176273) (nested cross-validation)** 是一个关键工具。外层循环用于评估模型的泛化性能，而内层循环用于选择最佳超参数。至关重要的是，所有依赖于数据的预处理步骤——包括特征标准化、文本嵌入训练、[特征选择](@entry_id:177971)等——都必须严格地在每个训练折叠 (fold) 内部重新计算，而不能接触到外层的测试数据。将患者层面的划分与[嵌套交叉验证](@entry_id:176273)相结合，并确保所有数据驱动的特征工程步骤都被正确地封装在验证流程之内，是构建和评估可靠临床预测模型的必要条件 [@problem_id:4563205]。