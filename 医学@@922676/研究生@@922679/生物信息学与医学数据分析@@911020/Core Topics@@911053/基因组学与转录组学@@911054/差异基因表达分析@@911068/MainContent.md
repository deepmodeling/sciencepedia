## 引言
[差异基因表达](@entry_id:140753)（Differential Gene Expression, DGE）分析是现代生物学和生物医学研究的基石。从揭示疾病的分子机制到评估新药疗效，再到破译复杂的生命过程，量化并检验基因在不同条件下表达水平的变化，是我们理解[功能基因组学](@entry_id:155630)的核心任务。然而，从原始的RNA测序数据中获得可靠且有生物学意义的结论并非易事。这其中充满了统计学上的挑战，例如样本间的技术性差异、数据固有的噪音以及同时检验成千上万个基因所带来的[假阳性](@entry_id:635878)风险。

本文旨在为您构建一个关于现代DGE分析的坚实理论框架。我们将超越表面的工具使用，深入其核心的统计思想。在第一章“原理与机制”中，我们将从因果推断的视角出发，系统性地拆解DGE分析的每一步，从实验设计、[数据标准化](@entry_id:147200)到作为分析核心的负二项[广义线性模型](@entry_id:171019)。接着，在第二章“应用与交叉学科联系”中，我们将探索DGE分析的广阔应用场景，看它如何解决从单细胞研究、[精准医疗](@entry_id:152668)到[宏基因组学](@entry_id:146980)，乃至非生物学领域的具体问题。最后，第三章“动手实践”提供了一系列精心设计的问题，旨在通过实践加深您对关键概念的理解。

通过这趟旅程，您将不仅学会如何进行DGE分析，更将理解其背后的“为什么”，从而能够更自信、更批判性地设计实验、执行分析并解读结果。让我们首先深入其统计学的根基，探究DGE分析的原理与机制。

## 原理与机制

### 将[差异表达分析](@entry_id:266370)框定为因果推断问题

在深入探讨[差异基因表达](@entry_id:140753)（Differential Gene Expression, DGE）分析的统计机制之前，我们必须首先精确定义我们试图量化的目标。一个基因在两种条件下（例如，接受靶向治疗与未接受治疗）的表达水平不同，这具体意味着什么？最严谨的回答源于因果推断的框架。

想象一个精准肿瘤学的研究，我们比较接受某种新疗法的患者与[对照组](@entry_id:188599)患者的肿瘤活检样本。对于每个样本 $i$ 和每个基因 $g$，我们测量其表达量 $y_{gi}$。我们感兴趣的问题是：该疗法是否 *导致* 了基因 $g$ 表达水平的变化？

为了形式化这个问题，我们引入 **[潜在结果](@entry_id:753644) (potential outcomes)** 的概念。对于每个样本 $i$，存在两个潜在的表达结果：$y_{gi}(1)$，即该样本如果接受治疗（$T_i=1$）将会观察到的表达量；以及 $y_{gi}(0)$，即该样本如果接受对照处理（$T_i=0$）将会观察到的表达量。在现实中，我们对每个样本只能观察到其中一个结果，即实际接受的处理所对应的结果。

在此框架下，治疗对基因 $g$ 的因果效应可以被定义为 **平均治疗效应 (Average Treatment Effect, ATE)**，记作 $\tau_g$：

$$
\tau_g = E[y_{gi}(1) - y_{gi}(0)]
$$

这个量代表在目标人群中，治疗平均能带来多大的基因表达变化。我们的核心任务就是从观测数据中估计出 $\tau_g$。

然而，尤其是在非随机对照研究中，直接比较治疗组和[对照组](@entry_id:188599)的平均表达量（即 $E[y_{gi} \mid T_i=1] - E[y_{gi} \mid T_i=0]$）通常会得到一个有偏的估计。这是因为接受治疗的患者可能在其他方面（如年龄、性别、基线肿瘤纯度、细胞类型组成等）与[对照组](@entry_id:188599)系统性地不同，而这些因素本身也可能影响基因表达。这种现象被称为 **混杂 (confounding)**。

为了从观测数据中识别出因果效应 $\tau_g$，我们必须依赖一系列关键假设 [@problem_id:4333044]：

1.  **稳定单位治疗价值假设 (SUTVA)**：一个样本的潜在结果不受其他样本治疗分配的影响，且治疗的实现方式没有隐藏的变异。

2.  **一致性 (Consistency)**：一个样本被观察到的结果与其接受的治疗水平所对应的潜在结果一致。

3.  **正性 (Positivity)**：在任何给定的协变量组合下，接受治疗和对照处理的概率都大于零。

4.  **条件[可交换性](@entry_id:263314) (Conditional Exchangeability)**：给定一组充分的预处理协变量 $\mathbf{X}_i$（包括所有已知的生物和技术混杂因素，如批次效应、RNA质量等），治疗分配 $T_i$ 与潜在结果 $\{y_{gi}(0), y_{gi}(1)\}$ 相互独立。通俗地说，这意味着在控制了这些协变量后，治疗组和[对照组](@entry_id:188599)在其他所有未测量的方面都是可比的。

在这些假设下，我们可以通过建立一个[统计模型](@entry_id:755400)来调整混杂因素，从而估计因果效应。例如，一个[广义线性模型 (GLM)](@entry_id:749787) 可以将基因表达的[期望值](@entry_id:150961)与治疗状态及协变量联系起来：

$$
E[y_{gi} \mid T_i, \mathbf{X}_i] = g^{-1}(\alpha_g + \beta_g T_i + \boldsymbol{\gamma}_g^{\top}\mathbf{X}_i)
$$

在这里，$g^{-1}$ 是一个链接函数，$\beta_g$ 是我们感兴趣的系数，它代表在调整了协变量 $\mathbf{X}_i$ 后，治疗对基因 $g$ 表达（在链接函数尺度上）的效应。对 $\beta_g$ 进行[假设检验](@entry_id:142556)，就构成了[差异表达分析](@entry_id:266370)的核心。这个因果框架为我们后续的所有建模选择提供了理论基础，确保我们的分析目标是估计一个有生物学意义的量，而不仅仅是报告一个可能被混淆的关联。

### 实验设计与数据生成的基础

在我们能够对数据进行建模之前，必须理解数据是如何产生的，以及实验设计如何影响我们推断的有效性。[RNA测序](@entry_id:178187) ([RNA-seq](@entry_id:140811)) 数据具有一些独特的统计特性，若不加以妥善处理，将导致错误的结论。

#### [生物学重复与技术重复](@entry_id:199856)

任何[差异表达分析](@entry_id:266370)的基石都是有效的重复实验。然而，并非所有重复都具有相同的价值。区分 **生物学重复 (biological replicates)** 和 **技术重复 (technical replicates)** 至关重要 [@problem_id:4556286]。

*   **生物学重复** 是指来自同一实验条件下的不同独立生物学样本（例如，不同的患者、不同的小鼠）。每个生物学样本 $i$ 都具有其独特的潜在真实[基因丰度](@entry_id:174481) $X_{ij}$（对于基因 $j$），这是从该条件下种群丰度分布中抽取的一个随机实例。

*   **技术重复** 是指对 *同一个* 生物学样本进行的重复测量（例如，对同一份RNA提取物构建多个测序文库，或对同一个文库进行多次测序）。对于固定的生物学样本 $i$，其所有技术重复 $t$ 共享相同的潜在真实丰度 $X_{ij}$，但由于测量过程中的随机性，其观测计数值 $Y_{ijt}$ 可能不同。

为什么这种区分如此关键？答案在于变异的来源。我们可以使用 **全变异定律 (Law of Total Variance)** 来分解观测计数值 $Y_{ijt}$ 的总变异：

$$
Var(Y_{ijt}) = \underbrace{Var(E[Y_{ijt} \mid X_{ij}])}_{\text{生物学变异}} + \underbrace{E[Var(Y_{ijt} \mid X_{ij}])}_{\text{技术变异}}
$$

第一项 $Var(E[Y_{ijt} \mid X_{ij}])$ 反映了不同生物样本之间真实表达丰度 $X_{ij}$ 的变异，即 **生物学变异**。这正是我们在进行群体水平推断时需要估计的变异。要估计这个量，我们必须观察到多个独立的 $X_{ij}$ 实例，这只有通过收集多个生物学重复才能实现。

第二项 $E[Var(Y_{ijt} \mid X_{ij})]$ 反映了对于一个给定的真实丰度，由测序过程（文库制备、测序读数抽样等）引入的测量误差，即 **技术变异**。技术重复只能帮助我们更精确地测量单个样本的表达量，从而估计技术变异，但它们无法提供任何关于群体中个体间变异的信息。

因此，一个根本性的原则是：**只有生物学重复才能对群体中的生物学变异进行有效估计，从而进行有效的差异表达统计推断。** 如果没有足够的生物学重复（通常每个条件至少需要3个），我们就无法可靠地区分[处理效应](@entry_id:636010)和随机的个体差异，从而导致分析的[统计功效](@entry_id:197129)低下或假阳性率失控。

#### RNA-seq 计数的固有特性

[RNA-seq](@entry_id:140811) 的原始输出是每个基因在每个样本中被测序片段覆盖的次数，即 **读数计数 (read counts)**。这些计数数据具有两个关键特性，必须在分析前得到解决。

首先是 **测序深度 (Sequencing Depth)** 或称 **文库大小 (Library Size)** 的差异 [@problem_id:4556331]。在实验中，不同样本产生的总读数几乎总是不同的。例如，假设样本 $S_1$ 和 $S_2$ 是生物学上完全相同的重复，但由于测序仪上样量的差异，$S_1$ 的总读数为1千万，而 $S_2$ 的总读数为2千万。对于一个非[差异表达](@entry_id:748396)的基因，我们期望其在 $S_2$ 中的原始计数值大约是 $S_1$ 的两倍。若直接比较原始计数值，我们将错误地认为该基因在 $S_2$ 中是上调的。这种差异纯粹是技术性的，必须被移除。

其次是数据的 **成分性 (Compositional Nature)** [@problem_id:4333081]。RNA-seq 实验本质上是在一个固定的测序“预算”（即总读数）下，对样本中所有转录本进行抽样。因此，我们观察到的计数值反映的是每个基因的 *相对* 丰度，而非绝对分子数量。这种成分约束带来一个微妙但重要的问题：如果一小部分高表达基因的丰度发生巨大变化，它们将占据更多的测序读数，从而导致所有其他基因的观测计数值系统性地下降，即使这些基因的绝对丰度并未改变。这就是 **[成分偏倚](@entry_id:174591) (compositional bias)**。

### 标准化：使计数具有可比性

为了解决测序深度和成分性带来的问题，我们需要对原始计数值进行 **标准化 (Normalization)**。标准化的目标是计算每个样本的 **校正因子 (size factors)**，用以调整原始计数，使得在调整后，样本间的计数差异能够更真实地反映生物学差异，而非技术性伪影。

一个简单的方法是使用总读数来计算校正因子（例如，转换为每百万读数中的计数，即 Counts Per Million, CPM）。然而，这种方法对[成分偏倚](@entry_id:174591)很敏感。

更先进和稳健的方法，如 **M值的修剪均值 (Trimmed Mean of M-values, TMM)**，被广泛使用 [@problem_id:4556331] [@problem_id:4333081]。TMM方法基于一个关键假设：大多数基因在不同样本间并非[差异表达](@entry_id:748396)。其流程如下：
1.  选择一个参考样本。
2.  对于每一个其他样本（目标样本），计算其与参考样本之间每个基因的 **M值** (log-fold change, [对数倍数变化](@entry_id:272578)) 和 **A值** (average log-intensity, 平均对数强度)。
    $$ M_i = \log_2\left(\frac{y_{ik}/N_k}{y_{ir}/N_r}\right) $$
    $$ A_i = \frac{1}{2}\log_2\left(\left(\frac{y_{ik}}{N_k}\right) \cdot \left(\frac{y_{ir}}{N_r}\right)\right) $$
    其中 $y_{ik}$ 和 $y_{ir}$ 分别是基因 $i$ 在目标样本 $k$ 和参考样本 $r$ 中的计数值，$N_k$ 和 $N_r$ 是相应的文库大小。
3.  为了获得稳健的估计，TMM会剔除M值和A值最极端的基因（即表达差异最大和表达量最高/最低的基因）。
4.  对剩余基因的M值进行加权平均（权重与计数的统计精度成反比）。
5.  将这个加权平均值进行指数转换，得到最终的TMM校正因子。

这个校正因子能够更准确地捕捉由[测序深度](@entry_id:178191)和[成分偏倚](@entry_id:174591)共同构成的技术性缩放效应。值得注意的是，这些校正因子并不会直接用于修改计数值，而是作为模型的一部分，以 **偏移量 (offset)** 的形式被整合进后续的[统计模型](@entry_id:755400)中 [@problem_id:4556331]。

### 基因计数的[统计建模](@entry_id:272466)

标准化解决了样本间的可比性问题后，下一步是为每个基因的计数数据选择一个合适的概率分布模型。

#### 泊松模型的不足：过度离散

计数数据的最基本模型是 **泊松分布 (Poisson distribution)**。泊松分布由一个单一参数 $\mu$（[率参数](@entry_id:265473)）决定，其概率质量函数为：
$$ p(y \mid \mu) = \frac{\exp(-\mu)\,\mu^{y}}{y!} $$
泊松分布有一个标志性特征——**等离散 (equidispersion)**，即其方差等于其均值：$\mathbb{E}[Y] = \mathrm{Var}(Y) = \mu$ [@problem_id:4556321]。

然而，RNA-seq的计数数据几乎从不满足此属性。在实践中，我们观察到的方差几乎总是显著大于均值，这种现象被称为 **[过度离散](@entry_id:263748) (overdispersion)**。例如，在一个包含4个生物学重复的试点实验中，某基因的计数值为 $\{50, 120, 80, 200\}$。其样本均值为 $112.5$，而样本方差高达 $4225$，远大于均值 [@problem_id:4556321]。

过度离散的产生根源在于生物学和技术上的异质性 [@problem_id:4333016]。即使在同一实验条件下，不同生物学重复样本的基因表达真实速率也不是一个恒定的 $\mu$，而是一个随机变量 $\Lambda$。每个样本的计数可以被看作一个两阶段过程：首先，从一个代表生物学异质性的分布中抽取一个速率 $\lambda$；然后，在给定该速率 $\lambda$ 的情况下，从 $\text{Poisson}(\lambda)$ 分布中生成一个计数值 $Y$。根据全变异定律，计数的总方差为：
$$ \mathrm{Var}(Y) = \mathbb{E}[\mathrm{Var}(Y \mid \Lambda)] + \mathrm{Var}(\mathbb{E}[Y \mid \Lambda]) = \mathbb{E}[\Lambda] + \mathrm{Var}(\Lambda) $$
由于生物学异质性意味着 $\mathrm{Var}(\Lambda) > 0$，所以总方差 $\mathrm{Var}(Y)$ 必然大于总均值 $\mathbb{E}[Y] = \mathbb{E}[\Lambda]$。这种由未观测到的异质性导致的额外变异，是过度离散的根本原因。其来源包括真实的生物学差异（如样本间细胞类型比例不同、转录的随机爆发性）和技术差异（如PCR扩增的随机性、批次效应）。

#### 负二项分布：[差异表达分析](@entry_id:266370)的主力模型

由于泊松模型的局限性，**[负二项分布](@entry_id:262151) (Negative Binomial, NB)** 已成为RNA-seq[差异表达分析](@entry_id:266370)的标准模型。NB分布可以被看作是泊松-伽马[混合分布](@entry_id:276506)，它自然地包含了[过度离散](@entry_id:263748)。NB分布有两个参数：均值 $\mu$ 和一个 **离散参数 (dispersion parameter)** $\alpha$。其关键的均值-方差关系为 [@problem_id:4333016]：
$$ \operatorname{Var}(y) = \mu + \alpha \mu^{2} $$
这个关系式直观地捕捉了[RNA-seq](@entry_id:140811)计数的变异结构。方差由两部分组成：第一部分 $\mu$ 是泊松分布所描述的、源于[随机抽样](@entry_id:175193)的[散粒噪声](@entry_id:140025) (shot noise)；第二部分 $\alpha \mu^2$ 是额外的变异，它随均值的平方增长，并由离散参数 $\alpha$ 调节。离散参数 $\alpha$ 捕获了所有未在模型中明确解释的生物学和技术异质性。对于一个给定的基因，$\alpha$ 越大，其[过度离散](@entry_id:263748)程度越高。

通过考察平方变异系数 (squared coefficient of variation, $\text{CV}^2$)，我们可以更深入地理解 $\alpha$ 的意义：
$$ \operatorname{CV}^2 = \frac{\operatorname{Var}(y)}{\mu^{2}} = \frac{1}{\mu} + \alpha $$
对于高表达的基因（$\mu$ 很大），$1/\mu$ 项趋近于零，此时 $\text{CV}^2 \approx \alpha$。这意味着离散参数 $\alpha$ 代表了该基因固有的、独立于平均表达水平的生物学变异的平方 [@problem_id:4333016]。

#### 负二项广义线性模型 (NB-GLM)

为了检验基因在不同条件下的表达差异，我们将[负二项分布](@entry_id:262151)整合到 **[广义线性模型](@entry_id:171019) (Generalized Linear Model, GLM)** 的框架中。一个完整的NB-GLM模型包含以下三个部分 [@problem_id:4333054]：

1.  **随机部分**：指定了响应变量的分布。对于基因 $g$ 和样本 $i$ 的计数值 $y_{gi}$，我们假设它服从一个均值为 $\mu_{gi}$、基因特异性离散参数为 $\alpha_g$ 的负二项分布：
    $$ y_{gi} \sim \text{NB}(\mu_{gi}, \alpha_g) $$

2.  **系统部分**：将解释变量通过一个线性预测器结合起来，形式为 $x_i^\top\beta_g$。其中 $x_i$ 是样本 $i$ 的 **设计矩阵 (design matrix)** 的一行，它编码了所有我们感兴趣的变量（例如，一个代表截距的常数项，一个指示样本属于治疗组还是[对照组](@entry_id:188599)的0/1变量，以及其他需要校正的协变量如年龄、性别等）。$\beta_g$ 是基因 $g$ 特有的系数向量，其中的元素代表了基线表达水平以及各个协变量对表达水平的效应大小（通常是[对数倍数变化](@entry_id:272578)）。

3.  **链接函数**：通过一个链接函数 $\log(\cdot)$ 将均值 $\mu_{gi}$ 与线性预测器联系起来。对数链接函数确保了模型预测的均值总是正数，这对于计数数据是必要的。

结合前面讨论的标准化，完整的NB-GLM模型可以写为：
$$ \log(\mu_{gi}) = x_i^\top\beta_g + o_i $$
其中 $o_i = \log(s_i)$ 是样本 $i$ 的标准化校正因子 $s_i$ 的对数，它作为一个已知的 **偏移量 (offset)** 被包含在模型中。通过这种方式，模型在估计系数 $\beta_g$ 时，已经考虑了样本间测序深度的差异。

#### 处理混杂因素：协变量与批次效应

在构建设计矩阵 $x_i$ 时，我们有机会将因果推断框架中讨论的混杂因素（如年龄、肿瘤纯度）作为协变量纳入模型进行调整。一个尤其重要且普遍的混杂因素是 **[批次效应](@entry_id:265859) (batch effects)** [@problem_id:4333028]。当样本在不同时间、由不同实验人员或在不同实验室进行处理时，会产生系统性的、非生物学来源的变异。这些变异会影响大量基因，如果不加以校正，可能会完全掩盖真实的生物学信号或产生大量的[假阳性](@entry_id:635878)结果。

虽然简单的批次效应可以通过在设计矩阵中为每个批次加入一个[指示变量](@entry_id:266428)来校正，但更复杂的情况需要专门的方法。例如，**ComBat算法** 是一种广泛应用的[经验贝叶斯方法](@entry_id:169803)，它能够同时校正批次引起的加性效应（均值的系统性偏移）和乘性效应（方差的系统性改变）。ComBat会为每个基因和每个批次估计这些效应参数，并通过跨基因“借用信息”来稳定这些估计，从而在保留生物学变异的同时有效地移除技术性批次差异。

### 改善推断：[收缩估计](@entry_id:636807)与[多重检验校正](@entry_id:167133)

在拟合了NB-GLM模型后，我们还需要解决两个统计挑战，以确保最终的推断是稳健和可靠的，尤其是在生物学重复样本量较少的情况下。

#### [经验贝叶斯收缩](@entry_id:748954)以稳定估计

在小样本实验中（例如，每组只有3个重复），对每个基因独立估计其离散参数 $\alpha_g$ 和[对数倍数变化](@entry_id:272578)系数 $\beta_g$ 是非常不稳定的 [@problem_id:4556290]。一个基因可能由于随机波动，其样本内方差恰好很小，导致其离散参数 $\hat{\alpha}_g$ 被严重低估。在检验时，这会导致该基因的统计显著性被夸大，从而产生[假阳性](@entry_id:635878)。

现代[差异表达分析](@entry_id:266370)工具（如[DESeq2](@entry_id:167268), edgeR, limma）普遍采用 **[经验贝叶斯](@entry_id:171034) (Empirical Bayes, EB) 收缩** 的思想来解决这个问题。EB的核心理念是“**信息共享 (borrowing strength)**”：假设所有基因的参数（如离散参数或效应大小）都来自某个共同的[先验分布](@entry_id:141376)，然后利用数据中所有基因的信息来估计这个[先验分布](@entry_id:141376)的参数。最终，每个基因的参数估计值将是其自身数据给出的“原始”估计与从所有基因中学习到的先验期望的一个加权平均。这个过程被称为 **收缩 (shrinkage)**。

主要有两种类型的收缩：

1.  **离散参数收缩**：将每个基因的原始离散估计值 $\hat{\alpha}_g$ 向一个拟合的、与基因平均表达水平相关的趋势线上收缩。这可以防止因离群的离散估计值而导致的不可靠的统计检验。

2.  **[对数倍数变化 (LFC)](@entry_id:166203) 收缩**：将效应大小的估计值 $\hat{\beta}_g$ 向零收缩，尤其对于那些低计数、信息量少的基因，其原始LFC估计可能极不稳定。这种收缩可以产生更准确的效应大小排序，并减少因低表达基因的巨大噪音而产生的[假阳性](@entry_id:635878)。从频率学角度看，这种收缩等价于在模型中加入一个$L_2$惩罚项（岭回归） [@problem_id:4556290]。

收缩是一种典型的 **偏倚-方差权衡 (bias-variance trade-off)**。通过向整体趋势收缩，我们为单个基因的估计引入了少量偏倚，但作为交换，我们极大地降低了估计的方差，从而使整体均方误差更小，推断结果更稳健。

#### 控制[多重假设检验](@entry_id:171420)中的错误

[RNA-seq](@entry_id:140811)实验通常同时检测数万个基因，这意味着我们需要同时进行数万次假设检验（即检验每个基因的 $\beta_g$ 系数是否为零）。如果我们对每次检验都使用传统的 $p$ 值阈值（如 $0.05$），那么即使所有基因都没有[差异表达](@entry_id:748396)，我们也会因为纯粹的随机性而期望看到数千个“显著”的结果。这个问题被称为 **[多重检验问题](@entry_id:165508)**。

为了应对这个问题，我们需要校正我们的显著性标准。在基因组学中，控制 **伪发现率 (False Discovery Rate, FDR)** 通常比控制更为严格的 **族系误差率 (Family-Wise Error Rate, FWER)** 更为可取。FDR被定义为在所有被宣布为“显著”的结果中，实际上是[假阳性](@entry_id:635878)的结果所占的预期比例 [@problem_id:4333073]。

控制FDR最常用的方法是 **[Benjamini-Hochberg](@entry_id:269887) (BH)** 程序。其步骤如下：
1.  对所有 $m$ 个基因的 $p$ 值进行排序，从小到大为 $p_{(1)} \leq p_{(2)} \leq \dots \leq p_{(m)}$。
2.  设定一个目标FDR水平 $q$（例如，$0.05$）。
3.  找到最大的索引 $k$，使得其对应的 $p$ 值满足：
    $$ p_{(k)} \leq \frac{k}{m}q $$
4.  如果找到了这样的 $k$，则拒绝所有排名在前 $k$ 位的零假设（即宣布基因 $1, \dots, k$ 为差异表达）。

BH程序提供了一个数据驱动的、动态的显著性阈值。与固定的[Bonferroni校正](@entry_id:261239)相比，它在保持对错误发现的严格控制的同时，具有更高的[统计功效](@entry_id:197129)，能够发现更多真实的差异表达基因。经过FDR校正后得到的“q值”或“adjusted p-value”，是评估差异表达显著性的最终标准。