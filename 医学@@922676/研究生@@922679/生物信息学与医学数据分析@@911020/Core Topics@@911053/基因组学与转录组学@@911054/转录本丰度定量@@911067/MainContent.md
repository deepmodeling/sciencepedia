## 引言
转录本丰度定量是现代生物学研究的基石，它通过[RNA测序](@entry_id:178187)（[RNA-seq](@entry_id:140811)）技术揭示了基因表达的动态图景，为理解细胞功能、发育过程和疾病机制提供了核心线索。然而，从数以亿计的短测序读段中精确推断出每个转录本的真实丰度，是一项充满挑战的计算与统计任务。研究人员必须克服由测序技术引入的系统性偏好、因可变剪接导致的[读段比对](@entry_id:265329)模糊性，以及在样本间进行可靠比较的统计学难题。若缺乏对底层原理的深刻理解，极易得出错误的生物学结论。

本文旨在为这一关键领域提供一个系统而深入的指南。在第一章“原理与机制”中，我们将从第一性原理出发，构建[RNA-seq](@entry_id:140811)数据的概率[生成模型](@entry_id:177561)，并详细阐述如何利用[期望最大化](@entry_id:273892)（EM）算法来解决丰度推断问题。接着，在第二章“应用与交叉学科联系”中，我们将展示这些核心原理如何应用于解决实际生物学问题，从提升定量准确性到在单细胞和空间水平上解构转录的复杂性。最后，通过“动手实践”部分，您将有机会亲手实现关键算法，将理论知识转化为实践技能。通过这一结构化的学习路径，本文将引导您掌握转录本丰度定量的精髓，为您的研究工作奠定坚实的理论基础。

## 原理与机制

本章旨在深入探讨转录本丰度定量的核心原理与计算机制。我们将从RNA测序（RNA-seq）数据的生成过程这一第一性原理出发，构建一个概率生成模型。随后，我们将阐述如何通过[统计推断](@entry_id:172747)，特别是[期望最大化](@entry_id:273892)（Expectation-Maximization, EM）算法，从测序读段（reads）中估计转录本的丰度。最后，我们将讨论丰度值的标准化、解释，以及用于[差异表达分析](@entry_id:266370)的[统计模型](@entry_id:755400)，从而为理解和应用这些计算方法奠定坚实的理论基础。

### RNA-Seq数据的生成过程

从根本上说，[RNA-seq](@entry_id:140811)实验是一个复杂的抽样过程。细胞中的RNA分子群体构成一个巨大的“池”，测序过程从中随机抽取片段进行测序。因此，一个转录本被测序到的频率，直观上应与其在“池”中的相对摩尔丰度及其自身长度有关。为了精确地描述这一过程，我们可以构建一个概率生成模型。

#### 概率[生成模型](@entry_id:177561)

一个测序片段（fragment）的产生可以被概念化为一个多步骤的随机事件。首先，从转录本混合物中选择一个转录本，然后从该转录本上生成一个片段。

1.  **转录本选择**：假设一个样本中存在一个转录本集合 $\mathcal{T}$。每个转录本 $t \in \mathcal{T}$ 具有一个未知的相对摩尔丰度 $\pi_t$，其中 $\pi_t \ge 0$ 且 $\sum_{t \in \mathcal{T}} \pi_t = 1$。在生成一个片段时，首先根据这些丰度，以概率 $\pi_t$ 选中转录本 $t$。

2.  **片段生成**：一旦转录本 $t$ 被选中，一个片段将从其序列上产生。这个过程受到转录本长度、片段长度分布以及各种测序偏好的影响。

一个更精细的模型需要考虑以下因素 [@problem_id:4614662]：

*   **转录本长度 ($\ell_t$)**：转录本 $t$ 的核苷酸序列长度。
*   **片段长度分布 ($f(l)$)**：文库制备过程产生的片段具有不同的长度 $l$，其分布由概率质量函数 $f(l)$ 描述，其中 $\sum_l f(l) = 1$。
*   **片段起始位点分布 ($g_t(p|l)$)**：在转录本 $t$ 上，一个长度为 $l$ 的片段的起始位点 $p$ 的选择并非完全随机，而是遵循一个位置偏好分布 $g_t(p|l)$。这个分布可能依赖于片段长度。一个长度为 $l$ 的片段，其起始位点 $p$ 必须满足 $1 \le p \le \ell_t - l + 1$，以确保整个片段都位于转录本内部。

综合这些因素，生成一个特定片段（源自转录本 $t$，长度为 $l$，起始于 $p$）的完整概率链可以被形式化。基于此模型，我们可以推导出任意一个碱基被测序片段覆盖的概率，这对于理解[测序深度](@entry_id:178191)的分布至关重要。

#### 有效转录本长度

在上述模型中，一个核心概念是并非转录本的全长都同等地贡献于片段的生成。一个长度为 $\ell_t$ 的转录本，能够产生一个长度为 $l$ 的片段的有效起始位点只有 $\ell_t - l + 1$ 个。较长的转录本或较短的片段，其潜在的起始位点就越多。因此，为了准确地将观察到的读段数与转录本丰度联系起来，我们必须对这种长度效应进行校正。

这引出了**有效转录本长度（effective transcript length）** $\ell_t^{\text{eff}}$ 的概念，它被定义为在给定的片段长度分布 $f(l)$ 下，一个转录本上所有可行片段起始位点的期望数量 [@problem_id:4614629]。其数学表达式为：

$$
\ell_t^{\text{eff}} = \sum_{l=1}^{\infty} f(l) \cdot \max(0, \ell_t - l + 1)
$$

在这个公式中，$\max(0, \ell_t - l + 1)$ 计算了对于一个特定长度 $l$ 的片段，在转录本 $t$ 上有多少个有效起始位点（如果 $l > \ell_t$，则为0）。然后，通过对所有可能的片段长度 $l$ 进行加权平均（权重为 $f(l)$），我们得到了一个综合考虑了整个文库片段长度特征的[有效长度](@entry_id:184361)。这个 $\ell_t^{\text{eff}}$ 是一个比原始长度 $\ell_t$ 更精确的参数，它真正反映了一个转录本产生可测序片段的“潜力”。在后续的丰度计算和标准化中，使用[有效长度](@entry_id:184361)是至关重要的。

#### 测序过程中的系统性偏好

理想化的生成模型假设片段起始位点是均匀分布的，但这在实际中很少成立。[RNA-seq](@entry_id:140811)数据生成过程受到多种**系统性偏好（systematic biases）** 的影响，导致某些类型的片段被过高或过低地代表。若不加以校正，这些偏好将严重扭曲丰度估计的准确性。主要偏好包括 [@problem_id:4614704]：

*   **位置偏好（Positional Bias）**：由于非随机的RNA断裂和引物起始，片段在转录本上的分布常常不均匀，例如，在转录本的两端可能出现片段堆积或缺失的现象。这种偏好通常通过对转录本的相对位置（例如，从5'端到3'端的0到1的归一化坐标）建模来捕捉，以实现跨不同长度转录本的可比性。

*   **序列偏好（Sequence-specific Bias）**：在RNA片段化（特别是使用随机六聚体引物进行逆转录时）和连接过程中，某些特定的[核苷](@entry_id:195320)酸序列（例如，片段末端的 $k$-mer 上下文）可能更容易被处理，导致具有这些序列特征的片段被优先测序。

*   **[GC含量](@entry_id:275315)偏好（GC-content Bias）**：PCR扩增步骤对模板的[GC含量](@entry_id:275315)很敏感。GC含量过高或过低的片段可能扩增效率较低，导致其在最终的测序文库中代表性不足。

为了校正这些偏好，可以在[生成模型](@entry_id:177561)中引入一个偏好函数 $b_t(p_i, s_i, g_i)$，它根据片段 $i$ 的位置 $p_i$、序列上下文 $s_i$ 和GC含量 $g_i$ 来调整其采样概率。一个严谨的偏好模型通常采用[乘性](@entry_id:187940)形式，并将基础采样概率 $\frac{1}{\ell_t^{\text{eff}}}$ 调整为：

$$
\pi_{it} \propto \frac{b_t(p_i, s_i, g_i)}{\ell_t^{\text{eff}}}
$$

其中 $\pi_{it}$ 是在给定转录本 $t$ 的情况下，观察到片段 $i$ 的[条件概率](@entry_id:151013)。至关重要的是，为了保证模型参数（特别是丰度 $\theta_t$）的可识别性，偏好函数 $b_t$ 必须被恰当地归一化，使其在所有可能的片段上的期望效应为1。这确保了偏好校正主要是重新分配了[概率密度](@entry_id:143866)，而不是系统性地改变了从某个转录本产生的总概率 [@problem_id:4614704]。

### 从读段到丰度：推断问题

有了对数据生成过程的精确模型，下一个核心任务就是“[逆向工程](@entry_id:754334)”：如何从观察到的数百万个测序读段中，推断出每个转录本的潜在丰度 $\pi_t$？

#### [读段比对](@entry_id:265329)的模糊性

这一推断过程的主要挑战之一是**[读段比对](@entry_id:265329)的模糊性（read ambiguity）**。由于[可变剪接](@entry_id:142813)产生的不同**异构体（isoforms）** 共享大量外显[子序列](@entry_id:147702)，以及基因家族中存在高度同源的**旁系同源基因（paralogs）**，一个测序读段可能能够完美地或近乎完美地比对到多个不同的转录本上。

为了处理这种模糊性，我们首先为每个读段 $r_i$ 定义一个**转录本兼容集（transcript compatibility class）**，记为 $E_i$。这个集合包含了所有与读段 $r_i$ 的序列和链方向兼容的转录本 [@problem_id:4614664]。

*   如果一个读段的兼容集 $E_i$ 只包含一个转录本（即 $|E_i|=1$），我们称之为**唯一比对读段（unique mapping read）**。
*   如果兼容集包含多个转录本（即 $|E_i|>1$），则称之为**多比对读段（multimapping read）** [@problem_id:4614632]。

简单地丢弃多比对读段会丢失大量信息，并导致与其它转录本共享序列的基因的丰度被严重低估。因此，一个稳健的定量方法必须能够概率性地解决这种模糊性。

#### 似然函数的构建

统计推断的核心是构建一个**[似然函数](@entry_id:141927)（likelihood function）**，它描述了在给定模型参数（即丰度向量 $\pi$）的情况下，观察到我们手中这组测序数据（读段集合 $\lbrace r_i \rbrace_{i=1}^N$）的概率。

由于每个读段的真实来源（即它源自哪个转录本）是未知的[隐变量](@entry_id:150146)，我们可以通过对所有可能的来源进行边缘化（marginalization）来构建[似然函数](@entry_id:141927)。根据[全概率定律](@entry_id:268479)，观察到单个读段 $r_i$ 的概率是它源自任何一个兼容转录本 $t \in E_i$ 的概率之和：

$$
P(r_i | \pi) = \sum_{t \in E_i} P(\text{源自 } t) \cdot P(r_i | \text{源自 } t) = \sum_{t \in E_i} \pi_t \cdot w_{it}
$$

这里，$\pi_t$ 是选择转录本 $t$ 的[先验概率](@entry_id:275634)（即其丰度），而 $w_{it} = P(r_i | t)$ 是在给定它源自转录本 $t$ 的条件下，生成读段 $r_i$ 的[条件概率](@entry_id:151013)。这个 $w_{it}$ 项就包含了我们之前讨论的所有因素：[有效长度](@entry_id:184361)、各种偏好等。

假设所有读段是条件独立的，整个数据集的[联合似然](@entry_id:750952)函数就是每个读段似然的乘积 [@problem_id:4614664]：

$$
\mathcal{L}(\pi) = \prod_{i=1}^{N} P(r_i | \pi) = \prod_{i=1}^{N} \left( \sum_{t \in E_i} \pi_t \cdot w_{it} \right)
$$

我们的目标就是找到一组参数 $\hat{\pi}$，使得这个似然函数 $\mathcal{L}(\pi)$ 最大化，这被称为最大似然估计（Maximum Likelihood Estimation, MLE）。

#### [期望最大化](@entry_id:273892)（EM）算法

直接最大化上述[似然函数](@entry_id:141927)是困难的，因为对数函数内部存在一个求和项。**[期望最大化](@entry_id:273892)（Expectation-Maximization, EM）算法** 是解决这类含有[隐变量](@entry_id:150146)的 MLE 问题的标准迭代方法。其核心思想是，如果我们知道了每个读段的来源，问题就会变得简单得多。[EM算法](@entry_id:274778)通过以下两个步骤的交替迭代来逼近最优解：

1.  **E-步（Expectation Step）**：在这一步，我们使用当前的丰度估计值 $\pi^{(k)}$ 来“猜测”每个读段的来源。具体来说，我们计算每个读段 $r_i$ 来自其兼容集 $E_i$ 中每一个转录本 $t$ 的**后验概率（posterior probability）**。这个概率通常被称为“责任（responsibility）”，表示转录本 $t$ 对读段 $i$ 的“贡献”程度。根据[贝叶斯定理](@entry_id:151040)，这个责任 $r_{it}$ 可以计算为 [@problem_id:4614692] [@problem_id:4614632]：

    $$
    r_{it} = P(t | r_i, \pi^{(k)}) = \frac{P(r_i | t) P(t | \pi^{(k)})}{\sum_{j \in E_i} P(r_i | j) P(j | \pi^{(k)})} = \frac{\pi_t^{(k)} w_{it}}{\sum_{j \in E_i} \pi_j^{(k)} w_{ij}}
    $$

    对于唯一比对读段，其责任完全分配给唯一兼容的转录本（$r_{it}=1$）。对于多比对读段，其责任被[按比例分配](@entry_id:634725)给所有兼容的转录本，比例由各转录本当前的“有效丰度”($\pi_j^{(k)} w_{ij}$)决定。

2.  **M-步（Maximization Step）**：在这一步，我们使用上一步计算出的责任（即读段的期望分配）来更新丰度估计值。新的丰度估计 $\pi_t^{(k+1)}$ 被计算为分配给转录本 $t$ 的总期望读段数占总读段数的比例。这可以直观地理解为对所有读段的“部分计数”进行汇总 [@problem_id:4614692]：

    $$
    \pi_t^{(k+1)} = \frac{1}{N} \sum_{i=1}^{N} r_{it}
    $$
    其中 $N$ 是总读段数。这个新估计的 $\pi^{(k+1)}$ 将在下一轮的 E-步中使用。

通过反复迭代E-步和M-步，算法会收敛到[似然函数](@entry_id:141927)的一个局部最大值，从而得到最终的丰度估计。

### 丰度估计的标准化与解释

通过[EM算法](@entry_id:274778)等方法，我们得到了每个转录本的相对摩尔丰度 $\pi_t$ 或与之成比例的原始计数估计。然而，这些原始值并不能直接用于样本间的比较，因为它们受到**转录本长度**和**测序深度（或文库大小）** 这两个主要技术因素的强烈影响。因此，必须进行标准化。

#### 每百万转录本的转录本数（[TPM](@entry_id:170576)）

**Transcripts Per Million ([TPM](@entry_id:170576))** 是一种广泛使用的标准化度量，它能同时校正转录本长度和测序深度，并提供一个直观的解释。TPM的计算分两步 [@problem_id:4614691]：

1.  **长度归一化**：首先，将每个转录本的原始读段计数 $c_t$ 除以其[有效长度](@entry_id:184361) $\ell_t^{\text{eff}}$（通常以千碱基为单位，kb）。这得到了一个“每千碱基的读段率”（reads per kilobase, RPK），这个值与转录本的摩尔丰度成正比。
    $$
    \text{Rate}_t = \frac{c_t}{\ell_t^{\text{eff, kb}}}
    $$

2.  **深度归一化**：然后，将所有转录本的RPK值相加，得到一个样本的总RPK值。用每个转录本的RPK值除以这个总和，再乘以一百万 ($10^6$)，就得到了[TPM](@entry_id:170576)。
    $$
    \text{TPM}_t = \left( \frac{\text{Rate}_t}{\sum_{s \in \mathcal{T}} \text{Rate}_s} \right) \times 10^6 = \left( \frac{c_t / \ell_t^{\text{eff}}}{\sum_{s \in \mathcal{T}} c_s / \ell_s^{\text{eff}}} \right) \times 10^6
    $$

[TPM](@entry_id:170576)的优越性在于其**组合性（compositional）** 特征：对于同一个样本，所有转录本的TPM值之和恒等于一百万。这使得TPM值可以被直观地解释为：“如果我们从该样本的[cDNA文库](@entry_id:262174)中随机抽取一百万个转录本分子，我们期望能看到多少个转录本 $t$ 的分子”。这对于评估单个样本内部各转录本的相对表达量非常有用。

#### [组合性](@entry_id:637804)数据的陷阱

尽管[TPM](@entry_id:170576)在样本内解释上很直观，但其组合性也带来了一个用于**样本间比较**的巨大陷阱。由于每个样本的[TPM](@entry_id:170576)总和被强制固定为一百万，一个或少数几个高表达转录本的丰度发生剧烈变化，会迫使所有其他转录本的TPM值发生相反方向的改变，即使它们的绝对分子数在细胞中保持不变 [@problem_id:4614707]。

例如，假设在处理组中，几个线粒体转录本的表达量急剧上升。这将导致处理组样本的[TPM](@entry_id:170576)计算公式中的分母（总Rate）显著增大。因此，对于一个绝对表达量未发生变化的“[看家基因](@entry_id:197045)”，其TPM值在处理组中会“被动地”下降，从而产生一个虚假的下调信号。

这个例子揭示了一个深刻的原理：RNA-seq数据本质上是**组合数据（compositional data）**，我们只能测量到相对比例，而非绝对数量。直接比较两个样本的[TPM](@entry_id:170576)值来进行[差异表达分析](@entry_id:266370)，在统计上是不严谨的，因为它混淆了真实的生物学变化和组合效应所产生的技术假象。

### 用于[差异表达分析](@entry_id:266370)的[统计模型](@entry_id:755400)

为了在样本间进行可靠的[差异表达分析](@entry_id:266370)，我们必须回到受组合效应影响较小的**计数数据**，并使用专门为此设计的[统计模型](@entry_id:755400)。

#### 计数数据的[统计分布](@entry_id:182030)模型

生物学重复实验中的[RNA-seq](@entry_id:140811)计数数据显示出特定的统计特性，尤其是**过离散（overdispersion）**，即计数的方差大于其均值。这主要源于样本间的生物学变异。因此，简单的泊松分布模型（其假设方差等于均值）通常不适用。

目前最主流的[RNA-seq](@entry_id:140811)[差异表达分析](@entry_id:266370)工具（如[DESeq2](@entry_id:167268), edgeR）大多基于**负二项分布（Negative Binomial, NB）模型** [@problem_id:4614679]。NB分布可以被看作是一个**伽马-泊松混合模型**：我们假设每个转录本的真实表达水平在不同生物学重复之间存在波动（遵循伽马分布），而测序过程本身则是在此基础上叠加了泊松分布的抽样噪声。这种两层模型自然地导出了NB分布，其方差与均值之间存在二次关系：

$$
\text{Var}(C) = \mu + \alpha\mu^2
$$

其中 $\mu$ 是均值计数，$\alpha$ 是离散系数，它捕捉了超出泊松噪声的额外变异。通过对每个基因的离散系数进行稳健的估计，N[B模型](@entry_id:159413)能够提供更准确的统计推断。进行[差异表达分析](@entry_id:266370)的正确流程通常是 [@problem_id:4614707]：

1.  从丰度定量软件中导入**估计的计数值**。
2.  使用稳健的方法（如[DESeq2](@entry_id:167268)中的“median-of-ratios”或edgeR中的TMM）计算样本间的**归一化因子**，以校正测序深度的差异。这些方法对少数基因的剧烈变化不敏感。
3.  在[广义线性模型](@entry_id:171019)（GLM）框架下拟合N[B模型](@entry_id:159413)，同时将**有效转录本长度**作为模型中的一个**偏移项（offset）**，以在计数模型中正确地校正长度偏好。
4.  进行假设检验，以确定不同条件下转录本丰度的显著变化。

#### 组合数据分析的高级框架

除了主流的N[B模型](@entry_id:159413)，还有更深入的理论框架来直接处理RNA-seq数据的[组合性](@entry_id:637804)。

*   **狄利克雷-[多项分布](@entry_id:189072)（Dirichlet-Multinomial, DM）模型**：此模型将一个样本中的所有转录本计数视为一个整体向量。它假设每个样本的真实转录本比例向量本身是从一个[狄利克雷分布](@entry_id:274669)中抽取的随机变量。这不仅能捕捉到过离散，还能显式地模拟出由组合约束导致的转录本计数之间的**负相关性** [@problem_id:4614679]。

*   **对数比变换（Log-ratio Transformations）**：组合数据分析的数学理论指出，对组合数据（如TPM或[相对丰度](@entry_id:754219)）直接进行标准统计分析（如相关性、PCA、[t检验](@entry_id:272234)）是错误的，因为这些分析所依赖的欧几里得几何在组合数据的样本空间（单纯形）上不成立。正确的做法是先通过**对数比变换**，如**中心对数比变换（Centered Log-Ratio, CLR）** 或 **等距对数比变换（Isometric Log-Ratio, ILR）**，将数据从单纯形空间映射到标准的欧几里得空间 [@problem_id:4614676]。在变换后的空间中，分量之间不再有总和约束，可以安全地使用各种标准统计方法。这些变换构成了对RNA-seq等组合型数据进行严谨探索性分析和建模的理论基石。