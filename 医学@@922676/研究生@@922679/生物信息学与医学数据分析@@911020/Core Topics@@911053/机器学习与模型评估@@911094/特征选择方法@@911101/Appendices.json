{"hands_on_practices": [{"introduction": "在过滤式特征选择中，第一步是量化每个特征与目标变量之间的关联强度。本练习旨在通过基本原理，将皮尔逊（Pearson）线性和斯皮尔曼（Spearman）等级相关性的通用定义，应用于生物信息学中常见的场景：一个连续特征（如基因表达水平）和一个二元结果（如疾病状态）。通过推导和计算，您将加深对点双列相关和等级双列相关背后统计原理的理解，这是评估生物标志物潜力的基础技能。[@problem_id:4563593]", "problem": "在用于疾病预测的高维基因表达研究中，单变量筛选方法通常通过特征与二元临床终点的关联性来对特征进行评分。两种广泛使用的关联性度量是 Pearson 线性相关（当结果是二元时，特化为点二列相关）和 Spearman 等级相关（在二元结果设置中，与等级二列度量相关）。从第一性原理出发，仅使用协方差、相关性和秩变换的核心定义，写下将这两种度量特化到连续特征和二元结果情况下的解析定义。然后为下面的数据集计算样本 Pearson 相关系数。\n\n给定一个连续特征 $X$（例如，某个基因的归一化表达量）和一个二元结果 $Y \\in \\{0,1\\}$，样本包含 $n$ 名患者。样本由以下配对 $(x_i,y_i)$ 组成，其中 $i=1,\\dots,n$：\n- 患者 1： $(x_1,y_1) = (8.1,1)$\n- 患者 2： $(x_2,y_2) = (6.2,0)$\n- 患者 3： $(x_3,y_3) = (7.5,1)$\n- 患者 4： $(x_4,y_4) = (5.9,0)$\n- 患者 5： $(x_5,y_5) = (9.2,1)$\n- 患者 6： $(x_6,y_6) = (6.5,0)$\n- 患者 7： $(x_7,y_7) = (8.0,1)$\n- 患者 8： $(x_8,y_8) = (5.7,0)$\n- 患者 9： $(x_9,y_9) = (7.7,1)$\n- 患者 10： $(x_{10},y_{10}) = (6.0,0)$\n\n任务：\n- 仅使用协方差和相关性的基本定义，提供解析定义来表达在此设置下连续变量 $X$ 和二元变量 $Y$ 之间的 Pearson 线性相关（即点二列解释），以及作为秩的 Pearson 相关的 Spearman 等级相关（即当 $Y$ 是二元时的等级二列解释）。\n- 使用标准样本定义，计算上述样本中 $X$ 和 $Y$ 之间的样本 Pearson 相关系数。\n\n将最终数值答案四舍五入到四位有效数字。不包含任何单位。如有角度，请以弧度表示。最终答案必须是单个实数。", "solution": "该问题需要两部分回答：首先，从基本原理出发，推导当一个变量是连续的而另一个是二元的时，Pearson 和 Spearman 相关系数的解析定义；其次，为给定数据集计算样本 Pearson 相关系数。\n\n### 第 1 部分：从第一性原理推导解析定义\n\n设 $X$ 为一个连续特征， $Y$ 为一个二元结果变量，其中 $Y \\in \\{0,1\\}$。我们有一个包含 $n$ 个配对 $(x_i, y_i)$ 的样本。令 $n_1$ 为 $y_i=1$ 的观测数量， $n_0$ 为 $y_i=0$ 的观测数量，使得 $n = n_0 + n_1$。\n\n#### Pearson 线性相关（点二列相关）\n\n样本 Pearson 相关系数 $r_{xy}$ 定义为 $X$ 和 $Y$ 的样本协方差除以它们的样本标准差的乘积：\n$$ r_{xy} = \\frac{\\text{cov}(x,y)}{s_x s_y} $$\n样本协方差定义为 $\\text{cov}(x,y) = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})$，样本标准差为 $s_x = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2}$ 和 $s_y = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (y_i - \\bar{y})^2}$。\n\n首先，我们特化二元变量 $Y$ 的标准差。 $Y$ 的样本均值为 $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i = \\frac{n_1}{n}$。\n$Y$ 的离差平方和为：\n$$ \\sum_{i=1}^n (y_i - \\bar{y})^2 = \\sum_{y_i=1} (1 - \\bar{y})^2 + \\sum_{y_i=0} (0 - \\bar{y})^2 $$\n$$ = n_1 \\left(1 - \\frac{n_1}{n}\\right)^2 + n_0 \\left(0 - \\frac{n_1}{n}\\right)^2 = n_1 \\left(\\frac{n_0}{n}\\right)^2 + n_0 \\left(-\\frac{n_1}{n}\\right)^2 $$\n$$ = \\frac{n_1 n_0^2 + n_0 n_1^2}{n^2} = \\frac{n_0 n_1 (n_0 + n_1)}{n^2} = \\frac{n_0 n_1 n}{n^2} = \\frac{n_0 n_1}{n} $$\n因此，$Y$ 的样本方差为 $s_y^2 = \\frac{1}{n-1} \\frac{n_0 n_1}{n}$，标准差为 $s_y = \\sqrt{\\frac{n_0 n_1}{n(n-1)}}$。\n\n接下来，我们特化协方差项。令 $\\bar{x}_1$ 为 $Y=1$ 组中 $X$ 的均值，$\\bar{x}_0$ 为 $Y=0$ 组中 $X$ 的均值。\n$$ (n-1)\\text{cov}(x,y) = \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) = \\sum_{y_i=1} (x_i - \\bar{x})(1 - \\bar{y}) + \\sum_{y_i=0} (x_i - \\bar{x})(0 - \\bar{y}) $$\n$$ = \\left(1 - \\frac{n_1}{n}\\right) \\sum_{y_i=1} (x_i - \\bar{x}) - \\frac{n_1}{n} \\sum_{y_i=0} (x_i - \\bar{x}) $$\n$$ = \\frac{n_0}{n} (n_1 \\bar{x}_1 - n_1 \\bar{x}) - \\frac{n_1}{n} (n_0 \\bar{x}_0 - n_0 \\bar{x}) $$\n使用 $n\\bar{x} = n_1\\bar{x}_1 + n_0\\bar{x}_0$：\n$$ n_1(\\bar{x}_1 - \\bar{x}) = n_1\\bar{x}_1 - n_1\\frac{n_1\\bar{x}_1+n_0\\bar{x}_0}{n} = \\frac{n_1(n-n_1)\\bar{x}_1 - n_1n_0\\bar{x}_0}{n} = \\frac{n_1n_0(\\bar{x}_1-\\bar{x}_0)}{n} $$\n$$ n_0(\\bar{x}_0 - \\bar{x}) = n_0\\bar{x}_0 - n_0\\frac{n_1\\bar{x}_1+n_0\\bar{x}_0}{n} = \\frac{n_0(n-n_0)\\bar{x}_0 - n_0n_1\\bar{x}_1}{n} = \\frac{n_0n_1(\\bar{x}_0-\\bar{x}_1)}{n} $$\n代回到协方差表达式中：\n$$ (n-1)\\text{cov}(x,y) = \\frac{n_0}{n} \\left(\\frac{n_1n_0(\\bar{x}_1-\\bar{x}_0)}{n}\\right) - \\frac{n_1}{n} \\left(\\frac{n_0n_1(\\bar{x}_0-\\bar{x}_1)}{n}\\right) $$\n$$ = \\frac{n_0^2 n_1}{n^2}(\\bar{x}_1-\\bar{x}_0) + \\frac{n_1^2 n_0}{n^2}(\\bar{x}_1-\\bar{x}_0) = \\frac{n_0 n_1(n_0+n_1)}{n^2}(\\bar{x}_1-\\bar{x}_0) = \\frac{n_0 n_1}{n}(\\bar{x}_1-\\bar{x}_0) $$\n所以，$\\text{cov}(x,y) = \\frac{n_0 n_1}{n(n-1)}(\\bar{x}_1 - \\bar{x}_0)$。\n\n综合这些结果，得到点二列相关系数 $r_{\\text{pb}}$：\n$$ r_{\\text{pb}} = \\frac{\\text{cov}(x,y)}{s_x s_y} = \\frac{\\frac{n_0 n_1}{n(n-1)}(\\bar{x}_1 - \\bar{x}_0)}{s_x \\sqrt{\\frac{n_0 n_1}{n(n-1)}}} = \\frac{\\bar{x}_1 - \\bar{x}_0}{s_x} \\sqrt{\\frac{n_0 n_1}{n(n-1)}} $$\n该表达式根据两组的均值和特征的总体标准差，定义了连续特征和二元结果之间的 Pearson 相关。\n\n#### Spearman 等级相关（等级二列相关）\n\nSpearman 等级相关 $r_s$ 定义为在变量的秩上计算的 Pearson 相关系数。令 $u_i = \\text{rank}(x_i)$ 和 $v_i = \\text{rank}(y_i)$。\n$$ r_s(x,y) = r(u,v) = \\frac{\\text{cov}(u,v)}{s_u s_v} $$\n对于二元变量 $Y$，$n_0$ 个值为 0 的观测将占据前 $n_0$ 个秩次，而 $n_1$ 个值为 1 的观测将占据剩余的 $n_1$ 个秩次。由于存在结 (ties)，所有 $y_i=0$ 的观测都被赋予前 $n_0$ 个秩次的平均值，即 $v_0 = \\frac{1+2+...+n_0}{n_0} = \\frac{n_0+1}{2}$。所有 $y_i=1$ 的观测都被赋予后续 $n_1$ 个秩次的平均值，即 $v_1 = \\frac{(n_0+1)+...+(n_0+n_1)}{n_1} = n_0 + \\frac{n_1+1}{2}$。\n因此，秩变量 $v$ 也是一个二元变量，取值为 $v_0$ 和 $v_1$。关键的是，$v$ 是 $y$ 的一个简单线性变换：$v_i = (v_1-v_0)y_i + v_0$。令 $a = v_1-v_0$ 和 $b=v_0$。\nPearson 相关在正线性变换下是不变的。由于 $v_1  v_0$，变换因子 $a$ 是正的。因此：\n$$ r(u,v) = r(u, ay+b) = \\frac{\\text{cov}(u, ay+b)}{s_u s_{ay+b}} = \\frac{a \\cdot \\text{cov}(u,y)}{s_u \\cdot |a| \\cdot s_y} = \\frac{a}{a} \\cdot r(u,y) = r(u,y) $$\n这表明 $X$ 和二元变量 $Y$ 之间的 Spearman 相关等价于 $X$ 的秩与原始二元变量 $Y$ 之间的 Pearson 相关。这种特定的度量被称为等级二列相关， $r_{\\text{rb}}$。\n\n我们可以使用上面推导的点二列公式，只需将连续变量 $X$ 替换为其秩变换版本 $U$：\n$$ r_{\\text{rb}} = r_s(x,y) = \\frac{\\bar{u}_1 - \\bar{u}_0}{s_u} \\sqrt{\\frac{n_0 n_1}{n(n-1)}} $$\n这里，$\\bar{u}_1$ 是 $Y=1$ 组中 $X$ 的平均秩，$\\bar{u}_0$ 是 $Y=0$ 组中 $X$ 的平均秩，而 $s_u$ 是 $X$ 所有秩的标准差。\n\n### 第 2 部分：样本 Pearson 相关系数的计算\n\n样本包含 $n=10$ 个配对：\n$x = (8.1, 6.2, 7.5, 5.9, 9.2, 6.5, 8.0, 5.7, 7.7, 6.0)$\n$y = (1, 0, 1, 0, 1, 0, 1, 0, 1, 0)$\n\n我们将使用样本 Pearson 相关系数 $r_{xy}$ 的计算公式：\n$$ r_{xy} = \\frac{n \\sum_{i=1}^n x_i y_i - (\\sum_{i=1}^n x_i)(\\sum_{i=1}^n y_i)}{\\sqrt{\\left[n \\sum_{i=1}^n x_i^2 - (\\sum_{i=1}^n x_i)^2\\right]\\left[n \\sum_{i=1}^n y_i^2 - (\\sum_{i=1}^n y_i)^2\\right]}} $$\n\n首先，我们计算所需的总和：\n1.  $\\sum_{i=1}^{10} x_i = 8.1+6.2+7.5+5.9+9.2+6.5+8.0+5.7+7.7+6.0 = 70.8$\n2.  $\\sum_{i=1}^{10} y_i = 1+0+1+0+1+0+1+0+1+0 = 5$\n3.  $\\sum_{i=1}^{10} x_i y_i = (8.1 \\times 1) + (6.2 \\times 0) + (7.5 \\times 1) + (5.9 \\times 0) + (9.2 \\times 1) + (6.5 \\times 0) + (8.0 \\times 1) + (5.7 \\times 0) + (7.7 \\times 1) + (6.0 \\times 0) = 8.1+7.5+9.2+8.0+7.7 = 40.5$\n4.  $\\sum_{i=1}^{10} x_i^2 = 8.1^2+6.2^2+7.5^2+5.9^2+9.2^2+6.5^2+8.0^2+5.7^2+7.7^2+6.0^2 = 65.61+38.44+56.25+34.81+84.64+42.25+64.00+32.49+59.29+36.00 = 513.78$\n5.  由于 $y_i \\in \\{0,1\\}$，所以 $y_i^2 = y_i$。因此，$\\sum_{i=1}^{10} y_i^2 = \\sum_{i=1}^{10} y_i = 5$。\n\n现在，我们将这些总和代入公式中。\n分子：\n$$ n \\sum x_i y_i - (\\sum x_i)(\\sum y_i) = 10 \\times 40.5 - (70.8)(5) = 405 - 354 = 51 $$\n分母，第 1 部分（对于 $X$）：\n$$ n \\sum x_i^2 - (\\sum x_i)^2 = 10 \\times 513.78 - (70.8)^2 = 5137.8 - 5012.64 = 125.16 $$\n分母，第 2 部分（对于 $Y$）：\n$$ n \\sum y_i^2 - (\\sum y_i)^2 = 10 \\times 5 - 5^2 = 50 - 25 = 25 $$\n综合这些部分进行最终计算：\n$$ r_{xy} = \\frac{51}{\\sqrt{125.16 \\times 25}} = \\frac{51}{\\sqrt{3129}} \\approx 0.9117329... $$\n\n将结果四舍五入到四位有效数字，我们得到 $0.9117$。", "answer": "$$\\boxed{0.9117}$$", "id": "4563593"}, {"introduction": "在处理基因组学等高维数据时，对成千上万个特征同时进行统计检验会极大地增加假阳性发现的风险。本练习要求您从第一性原理出发，推导两种关键的多重检验校正方法——Bonferroni校正和Benjamini-Hochberg（BH）程序——的显著性阈值。通过比较这两种方法，您将能够深入理解控制族系谬误率（FWER）和错误发现率（FDR）之间的权衡，这对于在探索性研究和预测建模中做出明智的方法选择至关重要。[@problem_id:4563535]", "problem": "一个研究团队在训练疾病分类器之前，正在一个基因组学流程中执行基于过滤器的特征选择阶段。他们针对一个二元疾病表型，对 $m = 20{,}000$ 个转录本特征进行关联性检验，每个特征获得一个独立的 $p$ 值。他们希望在水平 $\\alpha = 0.05$ 下，根据两种不同的误差控制机制设置一个固定的 $p$ 值截断值：\n\n1. 族系谬误率 (FWER)，定义为在所有 $m$ 次检验中至少犯一次假阳性错误的概率。\n\n2. 错误发现率 (FDR)，定义为在所有被拒绝的假设中，假阳性所占的期望比例。\n\n从第一性原理出发，仅使用用于控制FWER的并集界、FDR的定义以及已知为真的零假设下$p$值随机不小于$[0,1]$上均匀分布这一性质，推导出：\n\n- 将FWER控制在水平 $\\alpha$ 的Bonferroni单特征显著性阈值。\n\n- 在$p$值独立的条件下，将FDR控制在水平 $\\alpha$ 的Benjamini–Hochberg (BH) 步升临界值函数 $t(k)$，其中 $k \\in \\{1,2,\\ldots,m\\}$ 是升序排列的$p$值的秩指数。\n\n计算当 $m = 20{,}000$ 和 $\\alpha = 0.05$ 时这些量的值。将你的最终答案表示为一个单行矩阵，其第一个元素是Bonferroni单特征阈值，第二个元素是BH临界值函数 $t(k)$ 作为秩指数 $k$ 的函数的闭式表达式。无需四舍五入。\n\n最后，请用文字论证，在大规模组学特征选择的背景下，控制FWER和控制FDR之间的权衡，以及当后续目标是预测性能而非验证性推断时，从业者为何可能更偏好其中一种。你的论证必须基于上述定义和相应阈值的行为，不得引用未经证实的启发式方法。", "solution": "该问题已被验证为具有科学依据、问题明确且客观。它基于统计学和生物信息学中多重假设检验的既定原则。所有必要的定义和数据均已提供，问题没有矛盾或模糊之处。\n\n设 $m = 20{,}000$ 为特征总数，对应 $m$ 次独立的假设检验。设 $\\alpha = 0.05$ 为期望的误差控制显著性水平。对于每个假设 $H_i$，$i \\in \\{1, 2, \\ldots, m\\}$，我们有一个对应的 $p$ 值 $p_i$。如果一个假设 $H_i$ 的 $p$ 值 $p_i$ 小于或等于某个阈值，则该假设被拒绝。\n\n### 1. 族系谬误率 (FWER) 与 Bonferroni 校正\n\n族系谬误率 (FWER) 定义为在所有 $m$ 次检验中至少犯一次 I 类错误（假阳性）的概率。设 $I_0$ 是对应于真实零假设的索引集合，并设 $m_0 = |I_0|$ 为真实零假设的数量。对于假设 $H_i$，如果 $H_i$ 为真 ($i \\in I_0$) 但被拒绝，则发生 I 类错误。设该事件为 $E_i$。\n\n那么 FWER 就是 $P(\\cup_{i \\in I_0} E_i)$。题目要求使用并集界（也称为布尔不等式）来控制 FWER。并集界指出，对于任何事件集合，其并集的概率不大于它们各自概率的总和。\n$$\n\\text{FWER} = P(\\cup_{i \\in I_0} E_i) \\le \\sum_{i \\in I_0} P(E_i)\n$$\n我们采用一个固定的单一 $p$ 值截断值，称之为 $t_{bonf}$。如果 $p_i \\le t_{bonf}$，则假设 $H_i$ 被拒绝。因此，事件 $E_i$ 就是 $\\{p_i \\le t_{bonf}\\}$。在真实零假设下，$p$ 值 $p_i$ 随机不小于一个在 $[0,1]$ 上的均匀随机变量。对于标准的连续检验统计量，$p_i \\sim U(0,1)$。因此，对于任何 $i \\in I_0$，犯 I 类错误的概率为 $P(p_i \\le t_{bonf}) = t_{bonf}$。\n\n将此代入并集界不等式：\n$$\n\\text{FWER} \\le \\sum_{i \\in I_0} t_{bonf} = m_0 t_{bonf}\n$$\n由于真实零假设的数量 $m_0$ 是未知的，我们采用最保守的情况，即所有零假设都可能为真，即 $m_0 \\le m$。这给出：\n$$\n\\text{FWER} \\le m_0 t_{bonf} \\le m t_{bonf}\n$$\n为了将 FWER 控制在水平 $\\alpha$，我们强制要求这个上界不大于 $\\alpha$：\n$$\nm t_{bonf} \\le \\alpha\n$$\n解出单特征显著性阈值 $t_{bonf}$，得到 Bonferroni 校正：\n$$\nt_{bonf} = \\frac{\\alpha}{m}\n$$\n对于给定的值 $m = 20{,}000$ 和 $\\alpha = 0.05$：\n$$\nt_{bonf} = \\frac{0.05}{20{,}000} = \\frac{5 \\times 10^{-2}}{2 \\times 10^4} = 2.5 \\times 10^{-6}\n$$\n\n### 2. 错误发现率 (FDR) 与 Benjamini-Hochberg (BH) 程序\n\n错误发现率 (FDR) 定义为在所有被拒绝的假设（发现）中，假阳性所占的期望比例。设 $R$ 为被拒绝假设的总数，$V$ 为假阳性的数量（被拒绝的真实零假设）。FDR 为 $E\\left[\\frac{V}{R}\\right]$（如果 $R=0$，则该分数为 $0$）。\n\nBenjamini-Hochberg (BH) 程序提供了一种比 FWER 更不严格的控制方法，这在探索性分析中通常更具效力。我们被要求从第一性原理推导出其临界值函数 $t(k)$。\n\n其核心思想是找到一个自适应阈值。让我们寻找一个阈值 $t$，使得我们对 FDR 的估计被控制在水平 $\\alpha$。对于给定的阈值 $t$，发现的数量为 $R(t) = \\sum_{i=1}^m \\mathbb{I}(p_i \\le t)$，其中 $\\mathbb{I}(\\cdot)$ 是指示函数。假阳性的期望数量是 $E[V(t)] = E[\\sum_{i \\in I_0} \\mathbb{I}(p_i \\le t)] = \\sum_{i \\in I_0} P(p_i \\le t) = m_0 t$。\n\n对于给定的阈值 $t$，一个简单但非严格形式化的 FDR 插入式估计量是 $\\widehat{\\text{FDR}}(t) = \\frac{E[V(t)]}{R(t)} = \\frac{m_0 t}{R(t)}$。由于 $m_0$ 未知，我们可以使用保守的上界 $m_0 \\le m$，这导出条件：\n$$\n\\frac{m t}{R(t)} \\le \\alpha \\implies t \\le \\frac{R(t)}{m} \\alpha\n$$\n这个不等式必须对我们的拒绝阈值 $t$ 成立。然而，阈值 $t$ 本身决定了拒绝的数量 $R(t)$。这种循环性表明需要一个搜索过程。BH 程序提供了一种系统化的方式来执行此搜索。\n\n让我们将 $p$ 值按升序排列：$p_{(1)} \\le p_{(2)} \\le \\ldots \\le p_{(m)}$。假设我们考虑将第 $k$ 个有序 $p$ 值 $p_{(k)}$ 作为我们的潜在阈值。如果我们将阈值设为 $t = p_{(k)}$，那么我们将恰好做出 $k$ 个拒绝。所以，$R(t) = R(p_{(k)}) = k$。将 $t=p_{(k)}$ 和 $R(t)=k$ 代入上述条件得到：\n$$\np_{(k)} \\le \\frac{k}{m} \\alpha\n$$\n这为我们提供了对每个排序后的 $p$ 值的条件。BH 程序对所有 $k \\in \\{1, 2, \\ldots, m\\}$ 检查此条件。为了最大化发现的数量（即获得最高的效力），我们找到满足此条件的最大 $k$。设其为 $k_{max} = \\max\\{k : p_{(k)} \\le \\frac{k\\alpha}{m}\\}$。然后，所有 $p$ 值不大于 $p_{(k_{max})}$ 的假设都被拒绝。\n\n从这个推导中，第 $k$ 个排序的 $p$ 值 $p_{(k)}$ 所比较的临界值为：\n$$\nt(k) = \\frac{k \\alpha}{m}\n$$\n这就是 Benjamini-Hochberg 步升临界值函数。对于给定的值，它变为：\n$$\nt(k) = \\frac{k \\times 0.05}{20{,}000} = (2.5 \\times 10^{-6}) k\n$$\n\n### 关于预测性能权衡的论证\n\n在控制FWER和FDR之间的选择关键取决于科学目标。FWER适用于验证性研究，在此类研究中，单个错误声明的代价很高。FDR适用于探索性研究，例如用于预测的特征选择，其目标是为下游任务生成一组有潜力的候选特征。\n\n用于控制FWER的Bonferroni阈值是 $t_{bonf} = \\alpha/m = 2.5 \\times 10^{-6}$。这是一个单一、极其严格且不随数据自适应的阈值。在典型的大规模组学研究中，真实效应可能不大，这种高显著性门槛常常导致很少或没有特征被选中。这对应着高假阴性率（真正具有预测性的特征被错过），这对于构建一个强大的预测模型是有害的。建立在贫乏特征集上的模型很可能会欠拟合，并且预测准确性差。\n\n用于控制FDR的BH临界值是 $t(k) = k\\alpha/m$。这些值形成一个递增的斜坡：$t(1) = \\alpha/m$, $t(2) = 2\\alpha/m$，以此类推。任何在Bonferroni校正下被选中的特征（即 $p_{(1)} \\le \\alpha/m$）也同样会被BH方法选中。然而，BH允许选择更多的特征，因为对于不那么显著的 $p$ 值，其阈值变得更为宽松。例如，排名第100的特征将与阈值 $100\\alpha/m$ 进行比较，该阈值是Bonferroni阈值的100倍。\n\nFDR控制的这种更宽松的方法允许更多的真阳性被包含在所选特征集中。虽然这也增加了假阳性的数量，但FDR框架保证了在所有选定特征中，这些“噪声”特征的*期望比例*被控制在水平 $\\alpha$。对于预测任务而言，这是一个非常理想的权衡。许多现代机器学习算法（例如，L1正则化回归、随机森林）对于包含有限数量的无信息特征是稳健的；它们有内部机制可以为这些特征分配低重要性或零权重。包含少数伪特征造成的损害，通常远小于排除大量真正具有预测性特征所造成的损害。\n\n因此，当目标是预测性能而不是验证性推断时，从业者会更偏好FDR控制而非FWER控制。FDR优先考虑更高的效力（敏感性）来发现潜在的预测性特征，同时接受一个受控比例的假发现，这种权衡与构建稳健且准确的预测模型的目标非常吻合。FWER的极端保守性（优先考虑特异性）在这种情况下会适得其反。", "answer": "$$\n\\boxed{\\begin{pmatrix} 2.5 \\times 10^{-6}  2.5 \\times 10^{-6} k \\end{pmatrix}}\n$$", "id": "4563535"}, {"introduction": "单变量过滤方法虽然计算高效，但存在一个根本性的局限性：它们无法检测到多变量之间的复杂相互作用。本练习通过一个经典的“异或”（XOR）思想实验来揭示这一弱点。您需要证明，在一个特征的预测能力完全依赖于另一个特征的场景中，该特征与目标变量之间的边际关联（例如，互信息）可能为零，从而阐明为何更复杂的包裹式或嵌入式方法在某些情况下是必需的。[@problem_id:4563606]", "problem": "您正在一项全基因组关联研究中分析一个合成队列，其中两个二元单核苷酸多态性 (SNP) 特征 $X_{1}$ 和 $X_{2}$ 被编码为 $X_{1} \\in \\{0,1\\}$ 和 $X_{2} \\in \\{0,1\\}$。二元表型 $Y \\in \\{0,1\\}$ 由异或 (XOR) 规则 $Y = X_{1} \\oplus X_{2}$ 确定性地生成，其中 $\\oplus$ 表示异或 (XOR)。假设 $X_{1}$ 和 $X_{2}$ 是参数为 $1/2$ 的独立同分布的伯努利随机变量，并且该合成队列由 $N = 4n$ 个个体组成（对于某个整数 $n \\geq 1$），其中四种 $(X_{1}, X_{2})$ 组合中的每一种都恰好出现 $n$ 次。在基于过滤器的特征选择中，特征根据与目标的单变量关联（例如，通过互信息 (MI) 或相关性）进行排序，并据此进行选择。\n\n从概率论中独立性的基本定义和互信息 (MI) 的定义出发\n$$\nI(X;Y) = \\sum_{x} \\sum_{y} p(x,y) \\ln\\!\\left(\\frac{p(x,y)}{p(x)\\,p(y)}\\right),\n$$\n推导在所描述的数据生成机制下，$X_{1}$ 和 $Y$ 之间的边际关系对于特征选择是否具有信息量。然后，使用指定的合成队列，计算由队列构建所隐含的互信息 $I(X_{1}; Y)$ 的精确值。\n\n请以精确值的形式表达您的最终答案。无需四舍五入。此计算不涉及物理单位。", "solution": "用户提供了一个需要解决的问题。根据说明，第一步是验证问题陈述。\n\n### 第1步：提取已知信息\n- **特征：** 两个二元单核苷酸多态性 (SNP) 特征，编码为 $X_{1} \\in \\{0,1\\}$ 和 $X_{2} \\in \\{0,1\\}$。\n- **表型：** 一个二元表型，编码为 $Y \\in \\{0,1\\}$。\n- **数据生成规则：** $Y$ 由异或 (XOR) 规则 $Y = X_{1} \\oplus X_{2}$ 确定性地生成。\n- **特征分布（概率性）：** $X_{1}$ 和 $X_{2}$ 是参数为 $1/2$ 的独立同分布 (i.i.d.) 伯努利随机变量。\n- **队列构建（确定性）：** 合成队列由 $N = 4n$ 个个体组成（对于某个整数 $n \\geq 1$）。四种 $(X_{1}, X_{2})$ 组合——$(0,0)$、$(0,1)$、$(1,0)$、$(1,1)$——中的每一种都恰好出现 $n$ 次。\n- **方法论：** 分析是在使用单变量关联的基于过滤器的特征选择的背景下进行的。\n- **任务1：** 从独立性和互信息的基本定义出发，推导 $X_{1}$ 和 $Y$ 之间的边际关系是否具有信息量。\n- **任务2：** 计算由队列构建所隐含的互信息 $I(X_{1}; Y)$ 的精确值。\n- **互信息 (MI) 公式：** $I(X;Y) = \\sum_{x} \\sum_{y} p(x,y) \\ln\\!\\left(\\frac{p(x,y)}{p(x)\\,p(y)}\\right)$。\n\n### 第2步：使用提取的已知信息进行验证\n- **科学依据：** 该问题牢固地植根于信息论、概率论和机器学习。异或问题是用于说明线性模型和单变量特征选择方法局限性的经典示例。伯努利变量、互信息和过滤方法的概念在生物信息学和数据分析中是标准概念。问题设置在科学上是合理的。\n- **良态问题：** 所有必要的信息都已提供。特征的概率性描述和队列的确定性构建彼此一致，因为队列完美地代表了潜在的概率分布。任务定义清晰，存在唯一且有意义的解。\n- **客观性：** 问题以精确的数学语言陈述。它没有任何主观性或模糊性。\n- **缺陷清单：**\n    1.  **科学/事实不合理：** 无。前提在逻辑上一致，并基于已建立的理论。\n    2.  **不可形式化/不相关：** 无。该问题与生物信息学中的特征选择方法直接相关。\n    3.  **不完整/矛盾的设置：** 无。数据的两种描述（概率性的和基于队列的）内部一致且完整。\n    4.  **不切实际/不可行：** 无。它描述了一个合成数据集，这是理论分析中常用且有效的工具。\n    5.  **病态/结构不良：** 无。问题具体，并导向唯一答案。\n    6.  **伪深刻/琐碎：** 无。该问题探讨了一类常用算法在概念上一个重要的局限性。\n    7.  **超出科学可验证性：** 无。推导和计算在数学上是可验证的。\n\n### 第3步：结论与行动\n问题是**有效的**。现在将开始解答过程。\n\n### 解答推导\n\n该问题要求两个相关的结果：首先，判断 $X_{1}$ 和 $Y$ 之间的边际关系是否具有信息量；其次，计算互信息 $I(X_{1}; Y)$。\n\n**第1部分：边际关系的信息量**\n\n在特征选择的背景下，如果一个特征在单独考虑时能提供关于目标变量的信息，那么其边际关系就是“有信息量的”。从统计学上讲，这意味着特征和目标变量不是独立的。如果它们在统计上是独立的，那么知道特征的值并不能提供关于目标值的任何信息。我们将检验 $X_{1}$ 和 $Y$ 的独立性。\n\n两个随机变量是独立的，当且仅当它们的联合概率分布是其边际概率分布的乘积，即对于所有可能的 $x_{1}$ 和 $y$ 值，都有 $p(x_{1}, y) = p(x_{1})p(y)$。我们将从给定的数据生成过程中推导出这些分布。\n\n首先，我们确定 $X_{1}$ 和 $Y$ 的边际分布。\n问题陈述 $X_{1}$ 是一个参数为 $1/2$ 的伯努利随机变量。因此，其概率分布为：\n$$ p(X_{1}=0) = \\frac{1}{2} $$\n$$ p(X_{1}=1) = \\frac{1}{2} $$\n\n接下来，我们求 $Y = X_{1} \\oplus X_{2}$ 的分布。如果 $X_{1} = X_{2}$，变量 $Y$ 的值为 $0$；如果 $X_{1} \\neq X_{2}$，变量 $Y$ 的值为 $1$。\n$Y=0$ 的值在两个互斥事件下发生：$(X_{1}=0, X_{2}=0)$ 或 $(X_{1}=1, X_{2}=1)$。由于 $X_{1}$ 和 $X_{2}$ 是独立的，其概率为：\n$$ p(Y=0) = p(X_{1}=0, X_{2}=0) + p(X_{1}=1, X_{2}=1) $$\n$$ p(Y=0) = p(X_{1}=0)p(X_{2}=0) + p(X_{1}=1)p(X_{2}=1) = \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) + \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2} $$\n$Y=1$ 的值在两个互斥事件下发生：$(X_{1}=0, X_{2}=1)$ 或 $(X_{1}=1, X_{2}=0)$。其概率为：\n$$ p(Y=1) = p(X_{1}=0, X_{2}=1) + p(X_{1}=1, X_{2}=0) $$\n$$ p(Y=1) = p(X_{1}=0)p(X_{2}=1) + p(X_{1}=1)p(X_{2}=0) = \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) + \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2} $$\n所以，$Y$ 也是一个参数为 $1/2$ 的伯努利随机变量。\n\n现在，我们确定联合分布 $p(X_{1}, Y)$。\n-   $p(X_{1}=0, Y=0)$: 这要求 $X_{1}=0$ 且 $Y=0$。由于 $Y=X_{1} \\oplus X_{2}$，我们有 $0 = 0 \\oplus X_{2}$，这意味着 $X_{2}=0$。因此，这个事件对应于 $(X_{1}=0, X_{2}=0)$。\n    $$ p(X_{1}=0, Y=0) = p(X_{1}=0, X_{2}=0) = p(X_{1}=0)p(X_{2}=0) = \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4} $$\n-   $p(X_{1}=0, Y=1)$: 这要求 $X_{1}=0$ 且 $Y=1$。我们有 $1 = 0 \\oplus X_{2}$，这意味着 $X_{2}=1$。这个事件是 $(X_{1}=0, X_{2}=1)$。\n    $$ p(X_{1}=0, Y=1) = p(X_{1}=0, X_{2}=1) = p(X_{1}=0)p(X_{2}=1) = \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4} $$\n-   $p(X_{1}=1, Y=0)$: 这要求 $X_{1}=1$ 且 $Y=0$。我们有 $0 = 1 \\oplus X_{2}$，这意味着 $X_{2}=1$。这个事件是 $(X_{1}=1, X_{2}=1)$。\n    $$ p(X_{1}=1, Y=0) = p(X_{1}=1, X_{2}=1) = p(X_{1}=1)p(X_{2}=1) = \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4} $$\n-   $p(X_{1}=1, Y=1)$: 这要求 $X_{1}=1$ 且 $Y=1$。我们有 $1 = 1 \\oplus X_{2}$，这意味着 $X_{2}=0$。这个事件是 $(X_{1}=1, X_{2}=0)$。\n    $$ p(X_{1}=1, Y=1) = p(X_{1}=1, X_{2}=0) = p(X_{1}=1)p(X_{2}=0) = \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4} $$\n\n最后，我们检验独立性条件 $p(x_{1}, y) = p(x_{1})p(y)$：\n-   对于 $(x_{1}=0, y=0)$: $p(X_{1}=0, Y=0) = \\frac{1}{4}$。边际概率的乘积是 $p(X_{1}=0)p(Y=0) = (\\frac{1}{2})(\\frac{1}{2}) = \\frac{1}{4}$。条件成立。\n-   对于 $(x_{1}=0, y=1)$: $p(X_{1}=0, Y=1) = \\frac{1}{4}$。边际概率的乘积是 $p(X_{1}=0)p(Y=1) = (\\frac{1}{2})(\\frac{1}{2}) = \\frac{1}{4}$。条件成立。\n-   对于 $(x_{1}=1, y=0)$: $p(X_{1}=1, Y=0) = \\frac{1}{4}$。边际概率的乘积是 $p(X_{1}=1)p(Y=0) = (\\frac{1}{2})(\\frac{1}{2}) = \\frac{1}{4}$。条件成立。\n-   对于 $(x_{1}=1, y=1)$: $p(X_{1}=1, Y=1) = \\frac{1}{4}$。边际概率的乘积是 $p(X_{1}=1)p(Y=1) = (\\frac{1}{2})(\\frac{1}{2}) = \\frac{1}{4}$。条件成立。\n\n由于独立性条件对所有可能的结果都成立，所以 $X_{1}$ 和 $Y$ 是统计独立的。因此，$X_{1}$ 和 $Y$ 之间的边际关系对于特征选择是无信息量的。一个基于单变量关联的过滤方法会给 $X_1$ 分配零或接近零的重要性，从而无法选择它，尽管它在决定 $Y$ 的多变量关系中扮演着关键角色。\n\n**第2部分：互信息 $I(X_{1}; Y)$ 的计算**\n\n我们现在将使用指定的合成队列计算互信息 $I(X_{1}; Y)$ 的精确值。该队列由 $N = 4n$ 个个体组成，其中 $(X_1, X_2)$ 的四种可能组合每种都恰好出现 $n$ 次。$Y$ 的值由 $Y=X_1 \\oplus X_2$ 决定。\n\n$(X_1, X_2, Y)$ 的每个联合结果的计数如下：\n-   $(X_1=0, X_2=0) \\implies Y=0$: 计数为 $n$。\n-   $(X_1=0, X_2=1) \\implies Y=1$: 计数为 $n$。\n-   $(X_1=1, X_2=0) \\implies Y=1$: 计数为 $n$。\n-   $(X_1=1, X_2=1) \\implies Y=0$: 计数为 $n$。\n\n根据这些计数，我们可以计算 MI 公式所需的经验概率。个体总数为 $4n$。\n-   $p(X_{1}=0, Y=0)$: $(X_1=0, Y=0)$ 的计数来自 $(X_1=0, X_2=0)$，为 $n$。所以，$p(X_{1}=0, Y=0) = \\frac{n}{4n} = \\frac{1}{4}$。\n-   $p(X_{1}=0, Y=1)$: $(X_1=0, Y=1)$ 的计数来自 $(X_1=0, X_2=1)$，为 $n$。所以，$p(X_{1}=0, Y=1) = \\frac{n}{4n} = \\frac{1}{4}$。\n-   $p(X_{1}=1, Y=0)$: $(X_1=1, Y=0)$ 的计数来自 $(X_1=1, X_2=1)$，为 $n$。所以，$p(X_{1}=1, Y=0) = \\frac{n}{4n} = \\frac{1}{4}$。\n-   $p(X_{1}=1, Y=1)$: $(X_1=1, Y=1)$ 的计数来自 $(X_1=1, X_2=0)$，为 $n$。所以，$p(X_{1}=1, Y=1) = \\frac{n}{4n} = \\frac{1}{4}$。\n\n边际概率是：\n-   $p(X_{1}=0) = p(X_{1}=0, Y=0) + p(X_{1}=0, Y=1) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$。\n-   $p(X_{1}=1) = p(X_{1}=1, Y=0) + p(X_{1}=1, Y=1) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$。\n-   $p(Y=0) = p(X_{1}=0, Y=0) + p(X_{1}=1, Y=0) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$。\n-   $p(Y=1) = p(X_{1}=0, Y=1) + p(X_{1}=1, Y=1) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$。\n\n来自队列的这些经验概率与第1部分中推导的理论概率相同。\n\n现在，我们使用 MI 公式：\n$$ I(X_{1}; Y) = \\sum_{x_{1} \\in \\{0,1\\}} \\sum_{y \\in \\{0,1\\}} p(x_{1},y) \\ln\\!\\left(\\frac{p(x_{1},y)}{p(x_{1})\\,p(y)}\\right) $$\n对于求和中的每一项，对数内的比率为：\n$$ \\frac{p(x_{1},y)}{p(x_{1})\\,p(y)} = \\frac{1/4}{(1/2)(1/2)} = \\frac{1/4}{1/4} = 1 $$\n这对于 $(x_1, y)$ 的所有四种组合都成立。\n\n因此，MI 的计算变为：\n$$ I(X_1; Y) = p(0,0)\\ln(1) + p(0,1)\\ln(1) + p(1,0)\\ln(1) + p(1,1)\\ln(1) $$\n$$ I(X_1; Y) = \\frac{1}{4}\\ln(1) + \\frac{1}{4}\\ln(1) + \\frac{1}{4}\\ln(1) + \\frac{1}{4}\\ln(1) $$\n由于 $\\ln(1) = 0$，所以总和中的每一项都为零。\n$$ I(X_1; Y) = \\frac{1}{4}(0) + \\frac{1}{4}(0) + \\frac{1}{4}(0) + \\frac{1}{4}(0) = 0 $$\n$X_{1}$ 和 $Y$ 之间的互信息恰好为 $0$。这证实了第1部分的结论，即 $X_1$ 和 $Y$ 是统计独立的，因此边际关系是无信息量的。", "answer": "$$\n\\boxed{0}\n$$", "id": "4563606"}]}