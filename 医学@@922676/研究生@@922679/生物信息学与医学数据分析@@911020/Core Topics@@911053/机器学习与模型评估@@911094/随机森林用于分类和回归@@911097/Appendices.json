{"hands_on_practices": [{"introduction": "随机森林的基石是决策树。为了真正理解“森林”，我们必须首先理解单棵“树”是如何学习和生长的。本练习将引导您通过一个具体的、分步的计算，亲手构建一个分类树的最初两个分裂节点。通过计算基尼不纯度 (Gini impurity) 及相应的不纯度降低量，您将对驱动决策树生长的贪心、数据驱动的逻辑建立起直观的认识 [@problem_id:4603324]。", "problem": "考虑一个生物信息学和医疗数据分析中的二元疾病状态预测任务，其中一个随机森林基学习器使用分类与回归树 (CART)。给定一个包含 $n = 12$ 名患者的玩具数据集，每名患者有两个连续的生物标志物和一个二元结果。这些生物标志物是标准化的且无量纲。对于患者 $i$，数据集表示为 $(x^{(i)}_1, x^{(i)}_2, y^{(i)})$，其中 $y^{(i)} \\in \\{0, 1\\}$。数据如下：\n$(2.0, 8.0, 0)$, $(2.5, 7.5, 0)$, $(3.0, 7.0, 0)$, $(3.5, 6.8, 1)$, $(4.0, 6.5, 0)$, $(4.5, 6.2, 1)$, $(5.0, 5.8, 1)$, $(5.5, 5.3, 1)$, $(6.0, 4.9, 1)$, $(6.5, 4.6, 1)$, $(7.0, 4.0, 0)$, $(7.5, 3.6, 0)$。\n\n从适用于 CART 的第一性原理出发，以杂质减少量作为选择标准，手动构建该树的前两个二元分裂。在根节点，选择在两个生物标志物的所有候选阈值上最大化杂质减少量的分裂。然后，在下一步，选择杂质不为零的单个子节点，并再次通过在该节点上两个生物标志物的所有候选阈值中最大化杂质减少量来选择其最佳分裂。通过明确的计算和有原则的推理来证明您的选择。\n\n最后，计算这两个分裂实现的总杂质减少量，该减少量定义为根节点的杂质与第二次分裂后树的最终加权杂质之间的差值。将最终的杂质减少量表示为小数，并四舍五入到四位有效数字。", "solution": "该问题要求为一个给定的二元分类数据集手动构建一个分类与回归树（CART）的前两层。分裂标准是最大化杂质减少量。我们将遵循用于分类的标准 CART 算法，该算法利用基尼杂质度（Gini impurity）作为节点杂质的度量。\n\n首先，我们来定义一组数据点 $S$ 的基尼杂质度。如果有 $K$ 个类别，$p_k$ 是 $S$ 中属于类别 $k$ 的项目所占的比例，则基尼杂质度由下式给出：\n$$I_G(S) = 1 - \\sum_{k=1}^{K} p_k^2$$\n对于我们的二元分类问题，类别为 $y=0$ 和 $y=1$，设 $p_0$ 是类别 $0$ 的比例，$p_1$ 是类别 $1$ 的比例。公式简化为：\n$$I_G(S) = 1 - (p_0^2 + p_1^2)$$\n对于一个节点 $P$ 分裂成两个子节点（左节点 $L$ 和右节点 $R$），其大小分别为 $N_L$ 和 $N_R$（$N_P = N_L + N_R$），分裂的杂质是子节点杂质的加权平均值：\n$$I_G(L, R) = \\frac{N_L}{N_P} I_G(L) + \\frac{N_R}{N_P} I_G(R)$$\n杂质减少量，或称基尼增益，是父节点的杂质与子节点的加权杂质之差：\n$$\\Delta I_G = I_G(P) - I_G(L, R)$$\n最优分裂是使该增益最大化的分裂。对于连续特征，候选分裂阈值通常选择为特征连续唯一排序值之间的中点。一个优化方法是只考虑不同类别数据点之间的中点。\n\n给定的数据集有 $n=12$ 名患者：\n$(2.0, 8.0, 0)$, $(2.5, 7.5, 0)$, $(3.0, 7.0, 0)$, $(3.5, 6.8, 1)$, $(4.0, 6.5, 0)$, $(4.5, 6.2, 1)$, $(5.0, 5.8, 1)$, $(5.5, 5.3, 1)$, $(6.0, 4.9, 1)$, $(6.5, 4.6, 1)$, $(7.0, 4.0, 0)$, $(7.5, 3.6, 0)$。\n\n**步骤1：根节点分析与第一次分裂**\n\n根节点包含所有 $12$ 个数据点。我们计算每个类别中的患者数量：$N_0 = 6$（类别 $0$）和 $N_1 = 6$（类别 $1$）。\n比例为 $p_0 = \\frac{6}{12} = 0.5$ 和 $p_1 = \\frac{6}{12} = 0.5$。\n根节点的基尼杂质度是：\n$$I_G(\\text{root}) = 1 - (0.5^2 + 0.5^2) = 1 - (0.25 + 0.25) = 0.5$$\n\n现在，我们评估两个生物标志物 $x_1$ 和 $x_2$ 的候选分裂。\n\n**对生物标志物 $x_1$ 的分析：**\n$x_1$ 的唯一排序值为 $2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0, 7.5$。对应的类别标签是 $0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0$。\n我们在类别标签发生变化的地方测试阈值。\n1.  **阈值 $t_1 = \\frac{3.0+3.5}{2} = 3.25$：**\n    - 左侧 ($x_1 \\le 3.25$)：$3$ 个点，全为类别 $0$。（$N_L=3, N_{L,0}=3, N_{L,1}=0$）。$I_G(L) = 1 - (1^2 + 0^2) = 0$。\n    - 右侧 ($x_1 > 3.25$)：$9$ 个点，$3$ 个为类别 $0$，$6$ 个为类别 $1$。（$N_R=9, N_{R,0}=3, N_{R,1}=6$）。$p_{R,0}=\\frac{3}{9}, p_{R,1}=\\frac{6}{9}$。$I_G(R) = 1 - ((\\frac{1}{3})^2 + (\\frac{2}{3})^2) = 1 - \\frac{5}{9} = \\frac{4}{9}$。\n    - 杂质减少量 $\\Delta I_G = 0.5 - (\\frac{3}{12}(0) + \\frac{9}{12}(\\frac{4}{9})) = 0.5 - \\frac{1}{3} = \\frac{1}{6} \\approx 0.1667$。\n\n2.  **阈值 $t_2 = \\frac{4.0+4.5}{2} = 4.25$：**\n    - 左侧 ($x_1 \\le 4.25$)：$5$ 个点，$4$ 个为类别 $0$，$1$ 个为类别 $1$。（$N_L=5, N_{L,0}=4, N_{L,1}=1$）。$I_G(L) = 1 - ((\\frac{4}{5})^2 + (\\frac{1}{5})^2) = 1 - \\frac{17}{25} = \\frac{8}{25}$。\n    - 右侧 ($x_1 > 4.25$)：$7$ 个点，$2$ 个为类别 $0$，$5$ 个为类别 $1$。（$N_R=7, N_{R,0}=2, N_{R,1}=5$）。$I_G(R) = 1 - ((\\frac{2}{7})^2 + (\\frac{5}{7})^2) = 1 - \\frac{29}{49} = \\frac{20}{49}$。\n    - $\\Delta I_G = 0.5 - (\\frac{5}{12}(\\frac{8}{25}) + \\frac{7}{12}(\\frac{20}{49})) = 0.5 - (\\frac{2}{15} + \\frac{5}{21}) = 0.5 - \\frac{14+25}{105} = 0.5 - \\frac{39}{105} = \\frac{1}{2} - \\frac{13}{35} = \\frac{9}{70} \\approx 0.1286$。\n\n3.  **阈值 $t_3 = \\frac{6.5+7.0}{2} = 6.75$：**\n    - 左侧 ($x_1 \\le 6.75$)：$10$ 个点，$4$ 个为类别 $0$，$6$ 个为类别 $1$。（$N_L=10, N_{L,0}=4, N_{L,1}=6$）。$I_G(L) = 1 - ((\\frac{4}{10})^2 + (\\frac{6}{10})^2) = 1 - \\frac{52}{100} = \\frac{12}{25}$。\n    - 右侧 ($x_1 > 6.75$)：$2$ 个点，均为类别 $0$。（$N_R=2, N_{R,0}=2, N_{R,1}=0$）。$I_G(R) = 0$。\n    - $\\Delta I_G = 0.5 - (\\frac{10}{12}(\\frac{12}{25}) + \\frac{2}{12}(0)) = 0.5 - \\frac{2}{5} = 0.1$。\n\n$x_1$ 的最佳分裂增益为 $\\frac{1}{6}$。\n\n**对生物标志物 $x_2$ 的分析：**\n$x_2$ 的唯一排序值对应的类别标签是 $0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0$。\n我们在类别标签发生变化的地方测试阈值。\n1.  **阈值 $t_1 = \\frac{4.0+4.6}{2} = 4.3$：**\n    - 左侧 ($x_2 \\le 4.3$)：$2$ 个点，均为类别 $0$。$I_G(L) = 0$。\n    - 右侧 ($x_2 > 4.3$)：$10$ 个点，$4$ 个为类别 $0$，$6$ 个为类别 $1$。$I_G(R) = \\frac{12}{25}$。\n    - $\\Delta I_G = 0.5 - (\\frac{2}{12}(0) + \\frac{10}{12}(\\frac{12}{25})) = 0.5 - \\frac{2}{5} = 0.1$。\n\n2.  **阈值 $t_2 = \\frac{6.2+6.5}{2} = 6.35$：**\n    - 左侧 ($x_2 \\le 6.35$)：$7$ 个点，$2$ 个为类别 $0$，$5$ 个为类别 $1$。$I_G(L) = \\frac{20}{49}$。\n    - 右侧 ($x_2 > 6.35$)：$5$ 个点，$4$ 个为类别 $0$，$1$ 个为类别 $1$。$I_G(R) = \\frac{8}{25}$。\n    - $\\Delta I_G = 0.5 - (\\frac{7}{12}(\\frac{20}{49}) + \\frac{5}{12}(\\frac{8}{25})) = 0.5 - (\\frac{5}{21} + \\frac{2}{15}) = \\frac{9}{70} \\approx 0.1286$。\n\n$x_2$ 的最佳分裂增益为 $\\frac{9}{70}$。\n\n**第一次分裂的结论：**\n比较每个特征的最佳增益：$\\Delta I_G(x_1, t=3.25) = \\frac{1}{6}$ 和 $\\Delta I_G(x_2, t=6.35) = \\frac{9}{70}$。\n由于 $\\frac{1}{6} \\approx 0.1667$ 和 $\\frac{9}{70} \\approx 0.1286$，我们有 $\\frac{1}{6} > \\frac{9}{70}$。\n第一次分裂在生物标志物 $x_1$ 上，阈值为 $t=3.25$。\n- 节点 L1（左子节点，$x_1 \\le 3.25$）：$3$ 个点，全为类别 $0$。这是一个纯节点，其 $I_G(L1)=0$。它成为一个叶节点。\n- 节点 R1（右子节点，$x_1 > 3.25$）：$9$ 个点，$3$ 个为类别 $0$，$6$ 个为类别 $1$。这个节点是不纯的，其 $I_G(R1)=\\frac{4}{9}$。\n\n**步骤2：第二次分裂**\n\n我们现在分裂不纯节点 R1。需要减少的杂质是 $I_G(R1) = \\frac{4}{9} \\approx 0.4444$。节点 R1 中的数据如下：\n$(3.5, 6.8, 1)$, $(4.0, 6.5, 0)$, $(4.5, 6.2, 1)$, $(5.0, 5.8, 1)$, $(5.5, 5.3, 1)$, $(6.0, 4.9, 1)$, $(6.5, 4.6, 1)$, $(7.0, 4.0, 0)$, $(7.5, 3.6, 0)$。\n\n**在节点 R1 中对生物标志物 $x_1$ 的分析：**\n按 $x_1$ 排序后的标签：$1, 0, 1, 1, 1, 1, 1, 0, 0$。\n1.  **阈值 $t_1 = \\frac{4.0+4.5}{2} = 4.25$：**\n    - 左侧：$2$ 个点 $(3.5, 1), (4.0, 0)$。$N_L=2, N_{L,0}=1, N_{L,1}=1$。$I_G(L) = 1 - (0.5^2+0.5^2) = 0.5$。\n    - 右侧：$7$ 个点，$2$ 个为类别 $0$，$5$ 个为类别 $1$。$I_G(R) = \\frac{20}{49}$。\n    - $\\Delta I_G(\\text{R1}) = \\frac{4}{9} - (\\frac{2}{9}(0.5) + \\frac{7}{9}(\\frac{20}{49})) = \\frac{4}{9} - (\\frac{1}{9} + \\frac{20}{63}) = \\frac{4}{9} - \\frac{27}{63} = \\frac{4}{9} - \\frac{3}{7} = \\frac{1}{63}$。\n\n2.  **阈值 $t_2 = \\frac{6.5+7.0}{2} = 6.75$：**\n    - 左侧：$7$ 个点，$1$ 个为类别 $0$，$6$ 个为类别 $1$。$I_G(L) = 1 - ((\\frac{1}{7})^2 + (\\frac{6}{7})^2) = \\frac{12}{49}$。\n    - 右侧：$2$ 个点，均为类别 $0$。$I_G(R) = 0$。\n    - $\\Delta I_G(\\text{R1}) = \\frac{4}{9} - (\\frac{7}{9}(\\frac{12}{49}) + \\frac{2}{9}(0)) = \\frac{4}{9} - \\frac{12}{63} = \\frac{4}{9} - \\frac{4}{21} = \\frac{28-12}{63} = \\frac{16}{63}$。\n\n在节点 R1 上对 $x_1$ 的最佳分裂增益为 $\\frac{16}{63}$。\n\n**在节点 R1 中对生物标志物 $x_2$ 的分析：**\n按 $x_2$ 排序后的标签：$0, 0, 1, 1, 1, 1, 1, 0, 1$。\n1.  **阈值 $t_1 = \\frac{4.0+4.6}{2} = 4.3$：**\n    - 左侧 ($x_2 \\le 4.3$)：$2$ 个点，均为类别 $0$。$I_G(L)=0$。\n    - 右侧 ($x_2 > 4.3$)：$7$ 个点，$1$ 个为类别 $0$，$6$ 个为类别 $1$。$I_G(R) = \\frac{12}{49}$。\n    - $\\Delta I_G(\\text{R1}) = \\frac{4}{9} - (\\frac{2}{9}(0) + \\frac{7}{9}(\\frac{12}{49})) = \\frac{4}{9} - \\frac{12}{63} = \\frac{16}{63}$。\n\n2. **阈值 $t_2 = \\frac{6.5+6.8}{2} = 6.65$：**\n    - 左侧 ($x_2 \\le 6.65$)：$8$ 个点，$3$ 个类别 $0$，$5$ 个类别 $1$。$I_G(L) = 1-((\\frac{3}{8})^2+(\\frac{5}{8})^2) = \\frac{30}{64}=\\frac{15}{32}$。\n    - 右侧 ($x_2 > 6.65$)：$1$ 个点，类别 $1$。$I_G(R)=0$。\n    - $\\Delta I_G(\\text{R1}) = \\frac{4}{9} - (\\frac{8}{9}(\\frac{15}{32}) + \\frac{1}{9}(0)) = \\frac{4}{9} - \\frac{15}{36} = \\frac{4}{9} - \\frac{5}{12} = \\frac{16-15}{36} = \\frac{1}{36}$。\n\n**第二次分裂的结论：**\n节点 R1 的最大杂质减少量是 $\\frac{16}{63} \\approx 0.2540$。这个增益可以通过两个不同的分裂实现：($x_1 \\le 6.75$) 和 ($x_2 \\le 4.3$)。在出现平局的情况下，标准实现可能会根据预定义的顺序（例如，特征索引）进行选择。我们将任意选择对 $x_2$ 的分裂，因为它创建了一个纯节点。第二次分裂在节点 R1 上，使用规则 $x_2 \\le 4.3$。\n\n**步骤3：总杂质减少量**\n\n总杂质减少量是根节点的初始杂质与两次分裂后树的叶节点的最终加权杂质之差。\n三个叶节点是：\n1.  来自第一次分裂的节点 L1 ($x_1 \\le 3.25$)：$N_{L1}=3$, $I_G(L1)=0$。\n2.  来自第二次分裂的节点 L2 ($x_1 > 3.25$ 且 $x_2 \\le 4.3$)：$N_{L2}=2$, $I_G(L2)=0$。\n3.  来自第二次分裂的节点 R2 ($x_1 > 3.25$ 且 $x_2 > 4.3$)：$N_{R2}=7$, $I_G(R2)=\\frac{12}{49}$。\n\n树的最终加权杂质是：\n$$I_{final} = \\frac{N_{L1}}{N} I_G(L1) + \\frac{N_{L2}}{N} I_G(L2) + \\frac{N_{R2}}{N} I_G(R2)$$\n$$I_{final} = \\frac{3}{12}(0) + \\frac{2}{12}(0) + \\frac{7}{12}\\left(\\frac{12}{49}\\right) = \\frac{7 \\times 12}{12 \\times 49} = \\frac{7}{49} = \\frac{1}{7}$$\n总杂质减少量是：\n$$\\Delta I_{G, total} = I_G(\\text{root}) - I_{final} = 0.5 - \\frac{1}{7} = \\frac{1}{2} - \\frac{1}{7} = \\frac{7-2}{14} = \\frac{5}{14}$$\n将其转换为小数并四舍五入到四位有效数字：\n$$\\frac{5}{14} \\approx 0.3571428... \\approx 0.3571$$", "answer": "$$\\boxed{0.3571}$$", "id": "4603324"}, {"introduction": "随机森林虽然功能强大，但并非万能。高级建模的一个关键方面是理解模型的内在偏见和局限性。本练习探讨了随机森林面临的一个典型挑战：对强交互效应的建模。通过理论推导，您将量化当一个像随机森林这样的轴对齐模型试图逼近一个乘法关系时所产生的“不可约偏见” (irreducible bias)，这将为您提供关于模型表达能力局限性的重要一课 [@problem_id:4603289]。", "problem": "在医学数据分析情境下，假设一位研究人员试图通过两个（$2$）标准化的分子生物标志物 $X = (X_1, X_2)$ 来预测心血管并发症的连续风险评分 $Y$，其中每个生物标志物都被标准化到区间 $[0,1]$ 内。数据生成过程使得条件均值函数由 $\\mathbb{E}[Y \\mid X] = X_1 X_2$ 给出，这反映了生物标志物之间的乘性上位效应。假设协变量分布满足 $X_1 \\sim \\mathrm{Uniform}(0,1)$ 和 $X_2 \\sim \\mathrm{Uniform}(0,1)$，且它们相互独立。\n\n考虑在以下结构性约束下，训练一个用于回归的随机森林（Random Forest, RF）：森林中的每棵树都是深度为 $1$ 的决策树桩，也就是说，在 $X_1$ 或 $X_2$ 上进行单次轴对齐的划分，划分的每一侧都有一个恒定的预测值。假设森林的构建方式是最小化每棵树的经验均方误差，并通过对所有树的结果取平均来进行聚合。在拥有无限多训练样本和无限多棵树的渐近情况下，期望的随机森林预测器属于由决策树桩的凸组合生成的函数类的闭包（在均方意义下），因此可以简化为以下形式的加性轴对齐模型：\n$$\ns(X_1,X_2) = a(X_1) + b(X_2) + c,\n$$\n其中 $a$ 和 $b$ 是具有有限二阶矩的可测函数，且 $c \\in \\mathbb{R}$。\n\n从平方损失下的风险最小化定义以及条件期望的塔性质出发，推导能够最小化\n$$\n\\mathcal{R}(s) = \\mathbb{E}\\!\\left[\\left(\\mathbb{E}[Y \\mid X] - s(X_1,X_2)\\right)^2\\right]\n$$\n的渐近随机森林预测器 $s^{\\star}(X_1,X_2)$，其中该最小化过程遍历所有上述形式的加性函数 $s$。然后，通过计算积分平方偏差\n$$\nB = \\mathbb{E}\\!\\left[\\left(\\mathbb{E}[Y \\mid X] - s^{\\star}(X_1,X_2)\\right)^2\\right]\n$$\n来量化不可约的轴对齐偏差，其中期望是针对 $(X_1,X_2)$ 的联合分布计算的。请给出 $B$ 的精确值。最终答案必须是一个实数。不要四舍五入；请给出精确值。", "solution": "该问题要求我们找到函数 $f(X_1, X_2) = X_1 X_2$ 在均方误差意义下的最佳加性近似 $s^{\\star}(X_1,X_2)$。这等价于找到 $f$ 在加性函数空间上的 $L_2$ 投影。我们希望找到 $s^{\\star}(X_1,X_2) = a^{\\star}(X_1) + b^{\\star}(X_2) + c^{\\star}$ 来最小化风险泛函：\n$$\n\\mathcal{R}(a, b, c) = \\mathbb{E}\\left[ \\left(X_1 X_2 - (a(X_1) + b(X_2) + c)\\right)^2 \\right]\n$$\n期望是针对 $(X_1, X_2)$ 的联合分布计算的，该分布是单位正方形 $[0,1] \\times [0,1]$ 上的均匀分布。对于 $X_1, X_2 \\sim \\mathrm{Uniform}(0,1)$，我们有以下矩：\n$\\mathbb{E}[X_i] = \\int_0^1 x \\,dx = \\frac{1}{2}$ 对于 $i \\in \\{1,2\\}$。\n$\\mathbb{E}[X_i^2] = \\int_0^1 x^2 \\,dx = \\frac{1}{3}$ 对于 $i \\in \\{1,2\\}$。\n\n加性模型 $s(X_1,X_2) = a(X_1) + b(X_2) + c$ 存在可识别性问题，因为我们可以给 $a(X_1)$ 加上一个常数，同时从 $b(X_2)$ 中减去这个常数，而它们的和不变。为确保解的唯一性，我们施加约束，要求函数 $a$ 和 $b$ 是中心化的，即 $\\mathbb{E}[a(X_1)] = 0$ 和 $\\mathbb{E}[b(X_2)] = 0$。\n\n我们通过计算 $\\mathcal{R}$ 关于 $c$ 的偏导数并令其为零来找到最优常数 $c^{\\star}$：\n$$\n\\frac{\\partial \\mathcal{R}}{\\partial c} = \\mathbb{E}\\left[ -2(X_1 X_2 - a(X_1) - b(X_2) - c) \\right] = 0\n$$\n$$\nc = \\mathbb{E}[X_1 X_2] - \\mathbb{E}[a(X_1)] - \\mathbb{E}[b(X_2)]\n$$\n在我们的约束条件 $\\mathbb{E}[a(X_1)] = 0$ 和 $\\mathbb{E}[b(X_2)] = 0$ 下，最优常数为：\n$$\nc^{\\star} = \\mathbb{E}[X_1 X_2]\n$$\n由于 $X_1$ 和 $X_2$ 的独立性，我们有：\n$$\nc^{\\star} = \\mathbb{E}[X_1] \\mathbb{E}[X_2] = \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4}\n$$\n现在我们来求最优函数 $a^{\\star}(X_1)$ 和 $b^{\\star}(X_2)$。我们最小化 $\\mathbb{E}\\left[ \\left(X_1 X_2 - c^{\\star} - a(X_1) - b(X_2)\\right)^2 \\right]$。这个投影问题的解可以通过取条件期望得到。最优中心化函数 $a^{\\star}(X_1)$ 是通过将中心化目标函数 $X_1 X_2 - c^{\\star}$ 投影到关于 $X_1$ 的函数空间上得到的。\n$$\na^{\\star}(X_1) = \\mathbb{E}[X_1 X_2 - c^{\\star} \\mid X_1]\n$$\n利用期望的塔性质和线性性质，以及在条件中 $X_1$ 是给定的这一事实：\n$$\na^{\\star}(X_1) = X_1 \\mathbb{E}[X_2 \\mid X_1] - \\mathbb{E}[c^{\\star} \\mid X_1] = X_1 \\mathbb{E}[X_2] - c^{\\star}\n$$\n$X_1$ 和 $X_2$ 的独立性意味着 $\\mathbb{E}[X_2 \\mid X_1] = \\mathbb{E}[X_2] = \\frac{1}{2}$。\n$$\na^{\\star}(X_1) = X_1 \\left(\\frac{1}{2}\\right) - \\frac{1}{4} = \\frac{1}{2}X_1 - \\frac{1}{4}\n$$\n我们检查该函数是否满足我们的中心化约束：$\\mathbb{E}[a^{\\star}(X_1)] = \\mathbb{E}\\left[\\frac{1}{2}X_1 - \\frac{1}{4}\\right] = \\frac{1}{2}\\mathbb{E}[X_1] - \\frac{1}{4} = \\frac{1}{2}\\left(\\frac{1}{2}\\right) - \\frac{1}{4} = 0$。约束得到满足。\n\n根据对称性，最优函数 $b^{\\star}(X_2)$ 为：\n$$\nb^{\\star}(X_2) = \\mathbb{E}[X_1 X_2 - c^{\\star} \\mid X_2] = \\mathbb{E}[X_1]\\,X_2 - c^{\\star} = \\frac{1}{2}X_2 - \\frac{1}{4}\n$$\n这也满足 $\\mathbb{E}[b^{\\star}(X_2)]=0$。\n\n将这些部分组合起来，最优的渐近随机森林预测器是：\n$$\ns^{\\star}(X_1, X_2) = a^{\\star}(X_1) + b^{\\star}(X_2) + c^{\\star} = \\left(\\frac{1}{2}X_1 - \\frac{1}{4}\\right) + \\left(\\frac{1}{2}X_2 - \\frac{1}{4}\\right) + \\frac{1}{4}\n$$\n$$\ns^{\\star}(X_1, X_2) = \\frac{1}{2}X_1 + \\frac{1}{2}X_2 - \\frac{1}{4}\n$$\n推导出最优预测器 $s^{\\star}$ 后，我们现在计算不可约的轴对齐偏差 $B$。这是可实现的最小风险，即我们最佳近似的均方误差：\n$$\nB = \\mathbb{E}\\left[\\left(X_1 X_2 - s^{\\star}(X_1, X_2)\\right)^2\\right]\n$$\n代入 $s^{\\star}(X_1, X_2)$ 的表达式：\n$$\nB = \\mathbb{E}\\left[\\left(X_1 X_2 - \\left(\\frac{1}{2}X_1 + \\frac{1}{2}X_2 - \\frac{1}{4}\\right)\\right)^2\\right]\n$$\n期望内的表达式可以进行因式分解：\n$$\nX_1 X_2 - \\frac{1}{2}X_1 - \\frac{1}{2}X_2 + \\frac{1}{4} = X_1\\left(X_2 - \\frac{1}{2}\\right) - \\frac{1}{2}\\left(X_2 - \\frac{1}{2}\\right) = \\left(X_1 - \\frac{1}{2}\\right)\\left(X_2 - \\frac{1}{2}\\right)\n$$\n这表明加性近似的残差恰好是中心化变量的乘积。偏差 $B$ 是期望的残差平方：\n$$\nB = \\mathbb{E}\\left[\\left( \\left(X_1 - \\frac{1}{2}\\right)\\left(X_2 - \\frac{1}{2}\\right) \\right)^2\\right] = \\mathbb{E}\\left[ \\left(X_1 - \\frac{1}{2}\\right)^2 \\left(X_2 - \\frac{1}{2}\\right)^2 \\right]\n$$\n由于 $X_1$ 和 $X_2$ 是独立的，乘积的期望等于期望的乘积：\n$$\nB = \\mathbb{E}\\left[ \\left(X_1 - \\frac{1}{2}\\right)^2 \\right] \\mathbb{E}\\left[ \\left(X_2 - \\frac{1}{2}\\right)^2 \\right]\n$$\n项 $\\mathbb{E}\\left[ \\left(X_i - \\frac{1}{2}\\right)^2 \\right]$ 是随机变量 $X_i$ 方差的定义，因为 $\\frac{1}{2} = \\mathbb{E}[X_i]$。所以，$B = \\mathrm{Var}(X_1) \\mathrm{Var}(X_2)$。\n\n对于一个随机变量 $X \\sim \\mathrm{Uniform}(a,b)$，其方差为 $\\frac{(b-a)^2}{12}$。对于 $X_i \\sim \\mathrm{Uniform}(0,1)$，方差为：\n$$\n\\mathrm{Var}(X_i) = \\frac{(1-0)^2}{12} = \\frac{1}{12}\n$$\n或者，我们可以直接从矩来计算它：\n$$\n\\mathrm{Var}(X_i) = \\mathbb{E}[X_i^2] - (\\mathbb{E}[X_i])^2 = \\frac{1}{3} - \\left(\\frac{1}{2}\\right)^2 = \\frac{1}{3} - \\frac{1}{4} = \\frac{4-3}{12} = \\frac{1}{12}\n$$\n最后，我们计算 $B$：\n$$\nB = \\mathrm{Var}(X_1) \\mathrm{Var}(X_2) = \\left(\\frac{1}{12}\\right) \\left(\\frac{1}{12}\\right) = \\frac{1}{144}\n$$\n这个值代表了真实信号 $X_1 X_2$ 的方差中，由交互项引起且无法被任何形式为 $a(X_1) + b(X_2) + c$ 的加性模型所捕获的部分。当真实关系是乘性关系时，这是加性模型类的根本局限性或“偏差”。", "answer": "$$\\boxed{\\frac{1}{144}}$$", "id": "4603289"}, {"introduction": "随机森林的价值远不止于其最终的预测结果。树的集成在数据点之间隐式地定义了一种相似性度量，即“邻近度” (proximity)。这最后一个练习将挑战您利用这种内部结构来完成一项无监督任务：异常点检测。通过实现一个基于邻近度的密度度量，您将学会如何识别那些偏离了训练数据模式的“异常”患者画像，从而展示该模型的一种创造性且强大的应用 [@problem_id:4603276]。", "problem": "您将负责形式化并实现一种离群点检测方法，该方法基于随机森林（RF）分类器所引出的邻近结构，用于检测临床化学数据中的异常患者画像。您必须仅从基本定义出发，推导出一个基于邻近度的离群点度量，证明其可作为训练集上的密度代理，然后设计一种评估方案，根据此度量将测试患者标记为异常。\n\n推导基础和定义：\n- 随机森林（RF）是在训练数据的自助法（bootstrap）复制上训练的决策树集合。每棵树将特征空间划分为不相交的区域（叶节点）。如果在多棵树中，两个样本频繁落入同一个叶节点，则认为它们在 RF 模型下是相似的。\n- 定义任意两个样本之间的 RF 邻近度为它们落入同一终端叶节点的树所占的比例。\n- 某一点的邻近密度定义为该点到所有训练样本的邻近度的平方和，这反映了训练数据在该点周围的集中程度。\n- 离群分数定义为此邻近密度的倒数，因此稀疏区域对应于较大的离群分数。\n\n数学公式：\n- 令 $T$ 为树的数量，令 $L_t(\\mathbf{x})$ 表示树 $t \\in \\{1,\\dots,T\\}$ 为特征向量为 $\\mathbf{x} \\in \\mathbb{R}^p$ 的样本分配的叶节点索引。\n- 对于训练样本 $i$ 和 $j$，定义邻近度\n$$\nP_{ij} \\;=\\; \\frac{1}{T}\\sum_{t=1}^T \\mathbf{1}\\!\\left\\{ L_t(\\mathbf{x}_i) \\,=\\, L_t(\\mathbf{x}_j)\\right\\},\n$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。\n- 对于一个样本 $\\mathbf{x}$（训练或测试），其相对于训练集 $\\{\\mathbf{x}_j\\}_{j=1}^n$ 的邻近密度定义为\n$$\nD(\\mathbf{x}) \\;=\\; \\sum_{j=1}^n \\left( \\frac{1}{T}\\sum_{t=1}^T \\mathbf{1}\\!\\left\\{ L_t(\\mathbf{x}) \\,=\\, L_t(\\mathbf{x}_j)\\right\\} \\right)^2.\n$$\n- 定义离群分数\n$$\nO(\\mathbf{x}) \\;=\\; \\frac{1}{\\varepsilon + D(\\mathbf{x})},\n$$\n其中 $\\varepsilon>0$ 是一个用于保证数值稳定性的很小的值。$O(\\mathbf{x})$ 的值越大，表示相对于训练分布而言，该画像越异常。\n\n用于评估的训练数据生成：\n- 考虑 $p = 8$ 种临床化学分析物，顺序如下：钠 $\\mathrm{[mmol/L]}$、钾 $\\mathrm{[mmol/L]}$、氯 $\\mathrm{[mmol/L]}$、碳酸氢盐 $\\mathrm{[mmol/L]}$、葡萄糖 $\\mathrm{[mg/dL]}$、肌酐 $\\mathrm{[mg/dL]}$、天冬氨酸转氨酶（AST）$\\mathrm{[U/L]}$、丙氨酸转氨酶（ALT）$\\mathrm{[U/L]}$。\n- 使用以下参数，从具有对角协方差的多元正态分布中独立生成 $n_\\text{healthy} = 300$ 个健康画像和 $n_\\text{disease} = 200$ 个疾病画像。所有随机抽样必须使用固定的种子 $s = 12345$ 以保证可复现性。\n- 健康组均值向量 $\\boldsymbol{\\mu}_H$ 和标准差 $\\boldsymbol{\\sigma}_H$：\n  - $\\boldsymbol{\\mu}_H = [140,\\, 4.2,\\, 103,\\, 24,\\, 90,\\, 0.9,\\, 22,\\, 21]$\n  - $\\boldsymbol{\\sigma}_H = [2,\\, 0.3,\\, 2,\\, 2,\\, 10,\\, 0.2,\\, 5,\\, 5]$\n- 疾病组均值向量 $\\boldsymbol{\\mu}_D$ 和标准差 $\\boldsymbol{\\sigma}_D$：\n  - $\\boldsymbol{\\mu}_D = [134,\\, 4.8,\\, 100,\\, 20,\\, 140,\\, 1.8,\\, 80,\\, 90]$\n  - $\\boldsymbol{\\sigma}_D = [4,\\, 0.5,\\, 3,\\, 3,\\, 25,\\, 0.5,\\, 30,\\, 35]$\n\n随机森林和邻近度规格：\n- 训练一个二元 RF 分类器以区分健康（$0$）与疾病（$1$）画像，使用以下固定超参数：\n  - 树的数量 $T = 64$，\n  - 最大深度 $d_{\\max} = 8$，\n  - 每叶最小样本数 $m_{\\min} = 5$，\n  - 每次分裂考虑的特征数 $m_{\\text{try}} = 3$，\n  - 在树构建阶段的节点处进行有放回的自助法采样，使用相同的固定种子 $s = 12345$ 为整个训练流程提供种子。\n- 训练后，使用所有树和所有训练样本（不限于袋外样本），计算 $n \\times n$ 的训练邻近度矩阵 $P$，其元素 $P_{ij}$ 如上定义。\n\n离群点阈值：\n- 计算所有训练样本的离群分数 $O(\\mathbf{x}_i)$，在密度计算中排除自邻近度（即，在求和前将邻近度矩阵的对角线置零以省略 $D(\\mathbf{x}_i)$ 中的 $j=i$ 项）。\n- 令 $\\theta$ 为训练离群分数在水平 $q = 0.95$ 处的经验分位数。当且仅当 $O(\\mathbf{x}) > \\theta$ 时，一个画像 $\\mathbf{x}$ 被标记为异常。\n\n测试套件：\n您必须评估以下4个明确的测试患者（以与上述特征顺序相同的向量形式提供）。这些是确定性输入，而非采样所得：\n\n- 测试1（典型的类健康）：\n  - $\\mathbf{x}^{(1)} = [140,\\, 4.2,\\, 103,\\, 24,\\, 90,\\, 0.9,\\, 20,\\, 22]$。\n- 测试2（临界代谢紊乱）：\n  - $\\mathbf{x}^{(2)} = [133,\\, 4.5,\\, 101,\\, 22,\\, 120,\\, 1.3,\\, 30,\\, 35]$。\n- 测试3（显著的电解质失衡和类酸中毒）：\n  - $\\mathbf{x}^{(3)} = [120,\\, 2.5,\\, 85,\\, 12,\\, 180,\\, 1.0,\\, 25,\\, 25]$。\n- 测试4（严重的类肝细胞损伤）：\n  - $\\mathbf{x}^{(4)} = [138,\\, 4.0,\\, 102,\\, 24,\\, 100,\\, 0.8,\\, 600,\\, 800]$。\n\n所需输出：\n- 对于每个测试患者 $\\mathbf{x}^{(k)}$（$k \\in \\{1,2,3,4\\}$），计算其相对于训练集的离群分数 $O(\\mathbf{x}^{(k)})$ 并与 $\\theta$ 进行比较。对每个患者，输出一个布尔值以指示其是否异常，即如果 $O(\\mathbf{x}^{(k)}) > \\theta$ 则输出 $\\text{True}$，否则输出 $\\text{False}$。\n- 您的程序应生成单行输出，其中包含一个方括号括起来的、以逗号分隔的结果列表，顺序为 $k=1,2,3,4$，例如：$[\\text{True},\\text{False},\\text{True},\\text{True}]$。\n\n实现约束：\n- 您必须仅使用上述定义，从基本原理开始实现 RF 和邻近度计算。不允许使用任何外部机器学习库。\n- 您的代码必须是一个完整、可运行的程序，能够确定性地执行以下步骤：数据生成、RF 训练、邻近度计算、阈值计算、测试患者评分，并以指定格式输出最终的布尔值。", "solution": "该问题要求使用随机森林（RF）邻近度，为临床数据制定并实现一种离群点检测算法。从数据生成到最终评估的整个流程，都必须按照问题陈述中的定义，从基本原理开始构建。\n\n### 问题陈述的验证\n问题陈述已经过严格验证，并被认为是有效的。它在科学上基于已建立的机器学习概念（基于随机森林的非参数密度估计），具有精确的数学定义，问题适定，并且其规范足够详细，可以导出一个唯一、确定性的解。所有必需的参数、数据生成过程和评估指标均已提供。单一的随机种子（$s = 12345$）确保了可复现性。唯一一个微小的不明确之处是决策树分裂标准的选择，在此合理地假设为分类任务中标准的基尼不纯度。数值稳定性常数 $\\varepsilon$ 的值也未指定；将使用一个标准的小值，如 $10^{-8}$。\n\n### 方法论框架\n\n核心原理是使用训练好的随机森林，不是为了分类，而是为了定义数据点之间的相似性或“邻近度”度量。在 RF 中，如果两个数据点在多棵树中始终落入同一个终端叶节点，则认为它们在模型划分的特征空间中非常接近。这种邻近度度量可用于构建训练数据概率密度的一个代理。\n\n位于训练数据分布密集区域的点将与许多其他训练点具有高邻近度。相反，位于稀疏区域的离群点将与大多数训练点具有低邻近度。该算法将此直觉形式化如下：\n\n1.  **邻近密度：** 对于任何点 $\\mathbf{x}$（无论是来自训练集还是新的测试点），我们定义一个“邻近密度”$D(\\mathbf{x})$。该值通过将 $\\mathbf{x}$ 与训练集中所有点的邻近度的平方求和来计算。平方操作赋予了非常近（高邻近度）的点更大的权重。\n2.  **离群分数：** 离群分数 $O(\\mathbf{x})$ 定义为邻近密度的倒数，即 $O(\\mathbf{x}) = 1 / (\\varepsilon + D(\\mathbf{x}))$。这种公式确保了稀疏区域中的点（低密度 $D(\\mathbf{x})$）会获得较高的离群分数。\n3.  **阈值化：** 通过检查训练数据本身的离群分数分布来建立一个离群阈值 $\\theta$。选择这些分数的一个高分位数（具体为第 $95$ 百分位数）作为临界值。任何分数超过此阈值的新点都将被标记为“异常”画像。\n\n### 算法实现步骤\n\n该解决方案通过遵循以下步骤确定性地实现：\n\n1.  **数据生成：**\n    *   使用指定的种子 $s=12345$ 初始化一个随机数生成器。\n    *   从多元正态分布 $\\mathcal{N}(\\boldsymbol{\\mu}_H, \\text{diag}(\\boldsymbol{\\sigma}_H^2))$ 中抽取 $n_\\text{healthy} = 300$ 个“健康”画像，并标记为类别 $0$。\n    *   从多元正态分布 $\\mathcal{N}(\\boldsymbol{\\mu}_D, \\text{diag}(\\boldsymbol{\\sigma}_D^2))$ 中抽取 $n_\\text{disease} = 200$ 个“疾病”画像，并标记为类别 $1$。\n    *   这 $n = 500$ 个样本构成了训练数据集 $(\\mathbf{X}_{\\text{train}}, \\mathbf{y}_{\\text{train}})$。\n\n2.  **随机森林训练（从头开始）：**\n    *   构建一个包含 $T=64$ 棵决策树的随机森林。训练过程由单一的、带有种子的随机数生成器控制。\n    *   对于 $T$ 棵树中的每一棵：\n        *   从训练集中有放回地抽取一个自助法样本。\n        *   在此自助法样本上生长一棵决策树，并遵循以下超参数：最大深度 $d_{\\max}=8$，每叶最小样本数 $m_{\\min}=5$，以及每次分裂时考虑的特征数 $m_{\\text{try}}=3$。\n        *   每个节点使用的分裂标准是基尼不纯度的减少量，这是分类树的标准选择。通过评估 $m_{\\text{try}}$ 个随机选择的特征并为每个特征搜索最优阈值来找到最佳分裂点。\n    *   每棵树中的每个唯一叶节点都被分配一个唯一的标识符。\n\n3.  **邻近度矩阵计算：**\n    *   训练后，将所有 $n=500$ 个训练样本传递通过所有 $T=64$ 棵树，以确定它们落入的叶节点。这将生成一个 `[500 x 64]` 的叶节点标识符矩阵。\n    *   计算 $n \\times n$ 的邻近度矩阵 $\\mathbf{P}$。元素 $P_{ij}$ 是样本 $\\mathbf{x}_i$ 和 $\\mathbf{x}_j$ 落入同一叶节点的树所占的比例：\n    $$ P_{ij} = \\frac{1}{T}\\sum_{t=1}^T \\mathbf{1}\\{ L_t(\\mathbf{x}_i) = L_t(\\mathbf{x}_j)\\} $$\n\n4.  **离群阈值确定：**\n    *   计算每个训练样本 $\\mathbf{x}_i$ 的离群分数。为防止样本被认为与自身具有平凡的近邻关系，自邻近项被排除。这通过将密度计算为 $D(\\mathbf{x}_i) = \\sum_{j \\neq i} P_{ij}^2$ 来实现。\n    *   训练离群分数计算为 $O(\\mathbf{x}_i) = 1/(\\varepsilon + D(\\mathbf{x}_i))$。\n    *   阈值 $\\theta$ 被设置为这 $n$ 个训练离群分数在 $q=0.95$ 处的经验分位数。\n\n5.  **测试患者评估：**\n    *   对于4个指定的测试患者中的每一个 $\\mathbf{x}^{(k)}$：\n        *   将患者的特征向量传递通过 $T$ 棵树，以获取其叶节点分配。\n        *   计算其与每个训练样本 $\\mathbf{x}_j$ 的邻近度，形成一个邻近度向量 $P(\\mathbf{x}^{(k)}, \\mathbf{x}_j)$。\n        *   通过对这些邻近度的平方求和来计算邻近密度：\n          $$ D(\\mathbf{x}^{(k)}) = \\sum_{j=1}^n \\left( P(\\mathbf{x}^{(k)}, \\mathbf{x}_j) \\right)^2 $$\n        *   计算离群分数 $O(\\mathbf{x}^{(k)})$。\n        *   将该分数与阈值 $\\theta$ 进行比较。如果 $O(\\mathbf{x}^{(k)}) > \\theta$ 则结果为 `True`，否则为 `False`。\n\n这一系列操作产生一个确定性的布尔标志列表，指示哪些测试画像相对于所学习到的训练数据分布被认为是异常的。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates a Random Forest proximity-based outlier detection method.\n    \"\"\"\n    # -------------------\n    # Configuration\n    # -------------------\n    # Data generation parameters\n    P_FEATURES = 8\n    N_HEALTHY = 300\n    N_DISEASE = 200\n    MU_H = np.array([140., 4.2, 103., 24., 90., 0.9, 22., 21.])\n    SIGMA_H = np.array([2., 0.3, 2., 2., 10., 0.2, 5., 5.])\n    MU_D = np.array([134., 4.8, 100., 20., 140., 1.8, 80., 90.])\n    SIGMA_D = np.array([4., 0.5, 3., 3., 25., 0.5, 30., 35.])\n    \n    # RF hyperparameters\n    T_TREES = 64\n    D_MAX = 8\n    M_MIN = 5\n    M_TRY = 3\n    \n    # Outlier detection parameters\n    Q_THRESHOLD = 0.95\n    EPSILON = 1e-8\n    \n    # Random seed\n    SEED = 12345\n    \n    # Test cases\n    test_cases = [\n        np.array([140., 4.2, 103., 24., 90., 0.9, 20., 22.]), # Test 1\n        np.array([133., 4.5, 101., 22., 120., 1.3, 30., 35.]), # Test 2\n        np.array([120., 2.5, 85., 12., 180., 1.0, 25., 25.]), # Test 3\n        np.array([138., 4.0, 102., 24., 100., 0.8, 600., 800.]),# Test 4\n    ]\n    \n    # Initialize random number generator for reproducibility\n    rng = np.random.default_rng(SEED)\n\n    # -------------------\n    # Helper Classes for RF\n    # -------------------\n    class Node:\n        def __init__(self, feature_index=None, threshold=None, left=None, right=None, *, value=None, leaf_id=None):\n            self.feature_index = feature_index\n            self.threshold = threshold\n            self.left = left\n            self.right = right\n            self.value = value  # Majority class if leaf node\n            self.leaf_id = leaf_id # Unique ID if leaf node\n\n    class DecisionTree:\n        def __init__(self, rng_instance, max_depth, min_samples_leaf, m_try):\n            self.rng = rng_instance\n            self.max_depth = max_depth\n            self.min_samples_leaf = min_samples_leaf\n            self.m_try = m_try\n            self.root = None\n            self.leaf_counter = 0\n\n        def _gini(self, y):\n            if y.size == 0:\n                return 0\n            _, counts = np.unique(y, return_counts=True)\n            probas = counts / y.size\n            return 1 - np.sum(probas**2)\n\n        def _find_best_split(self, X, y):\n            n_samples, n_features = X.shape\n            current_gini = self._gini(y)\n            best_gain = -1.0\n            best_feat, best_thresh = None, None\n            \n            feat_idxs = self.rng.choice(n_features, self.m_try, replace=False)\n\n            for feat_idx in feat_idxs:\n                thresholds = np.unique(X[:, feat_idx])\n                if thresholds.size > 1:\n                    thresholds = (thresholds[:-1] + thresholds[1:]) / 2.0\n                else: \n                    continue # Cannot split on a single value\n                \n                for thresh in thresholds:\n                    left_idxs = np.where(X[:, feat_idx] = thresh)[0]\n                    right_idxs = np.where(X[:, feat_idx] > thresh)[0]\n                    \n                    if left_idxs.size == 0 or right_idxs.size == 0:\n                        continue\n                    \n                    y_left, y_right = y[left_idxs], y[right_idxs]\n                    \n                    p_left = left_idxs.size / n_samples\n                    p_right = right_idxs.size / n_samples\n                    \n                    weighted_gini = p_left * self._gini(y_left) + p_right * self._gini(y_right)\n                    gain = current_gini - weighted_gini\n                    \n                    if gain > best_gain:\n                        best_gain = gain\n                        best_feat = feat_idx\n                        best_thresh = thresh\n            \n            return best_feat, best_thresh\n\n        def _build_tree(self, X, y, depth):\n            n_samples = y.size\n            \n            # Stopping criteria\n            if (depth >= self.max_depth or\n                n_samples  self.min_samples_leaf or\n                np.unique(y).size == 1):\n                leaf_value = np.bincount(y).argmax()\n                self.leaf_counter += 1\n                return Node(value=leaf_value, leaf_id=self.leaf_counter)\n\n            feat_idx, threshold = self._find_best_split(X, y)\n            \n            if feat_idx is None: # No beneficial split found\n                leaf_value = np.bincount(y).argmax()\n                self.leaf_counter += 1\n                return Node(value=leaf_value, leaf_id=self.leaf_counter)\n\n            left_idxs = np.where(X[:, feat_idx] = threshold)[0]\n            right_idxs = np.where(X[:, feat_idx] > threshold)[0]\n            \n            left_child = self._build_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n            right_child = self._build_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n            \n            return Node(feature_index=feat_idx, threshold=threshold, left=left_child, right=right_child)\n\n        def fit(self, X, y):\n            self.root = self._build_tree(X, y, depth=0)\n        \n        def _get_leaf_id(self, x, node):\n            if node.leaf_id is not None:\n                return node.leaf_id\n            if x[node.feature_index] = node.threshold:\n                return self._get_leaf_id(x, node.left)\n            return self._get_leaf_id(x, node.right)\n            \n        def get_leaf_for_sample(self, x):\n            return self._get_leaf_id(x, self.root)\n\n    class RandomForest:\n        def __init__(self, rng_instance, n_trees, max_depth, min_samples_leaf, m_try):\n            self.rng = rng_instance\n            self.n_trees = n_trees\n            self.max_depth = max_depth\n            self.min_samples_leaf = min_samples_leaf\n            self.m_try = m_try\n            self.trees = []\n\n        def fit(self, X, y):\n            n_samples = X.shape[0]\n            total_leaf_offset = 0\n            for _ in range(self.n_trees):\n                idxs = self.rng.choice(n_samples, n_samples, replace=True)\n                X_boot, y_boot = X[idxs], y[idxs]\n                \n                tree = DecisionTree(self.rng, self.max_depth, self.min_samples_leaf, self.m_try)\n                tree.fit(X_boot, y_boot)\n                \n                # Make leaf IDs unique across all trees in the forest\n                tree.leaf_counter += total_leaf_offset\n                \n                # Python passes objects by reference, so we need to traverse and update\n                nodes_to_visit = [tree.root]\n                while nodes_to_visit:\n                    node = nodes_to_visit.pop(0)\n                    if node:\n                        if node.leaf_id is not None:\n                            node.leaf_id += total_leaf_offset\n                        nodes_to_visit.append(node.left)\n                        nodes_to_visit.append(node.right)\n\n                self.trees.append(tree)\n                total_leaf_offset = tree.leaf_counter\n\n        def get_leaf_indices(self, X_eval):\n            n_eval = X_eval.shape[0]\n            leaf_indices = np.zeros((n_eval, self.n_trees), dtype=int)\n            for i, x in enumerate(X_eval):\n                for t_idx, tree in enumerate(self.trees):\n                    leaf_indices[i, t_idx] = tree.get_leaf_for_sample(x)\n            return leaf_indices\n\n    # -------------------\n    # Main Logic\n    # -------------------\n    # 1. Generate Data\n    X_healthy = rng.normal(loc=MU_H, scale=SIGMA_H, size=(N_HEALTHY, P_FEATURES))\n    X_disease = rng.normal(loc=MU_D, scale=SIGMA_D, size=(N_DISEASE, P_FEATURES))\n    X_train = np.vstack((X_healthy, X_disease))\n    y_train = np.hstack((np.zeros(N_HEALTHY, dtype=int), np.ones(N_DISEASE, dtype=int)))\n    n_train = X_train.shape[0]\n\n    # 2. Train Random Forest\n    rf = RandomForest(rng, T_TREES, D_MAX, M_MIN, M_TRY)\n    rf.fit(X_train, y_train)\n\n    # 3. Compute training proximity matrix\n    train_leaf_indices = rf.get_leaf_indices(X_train)\n    prox_matrix = np.zeros((n_train, n_train))\n    for i in range(n_train):\n        for j in range(i, n_train):\n            proximity = np.sum(train_leaf_indices[i, :] == train_leaf_indices[j, :]) / T_TREES\n            prox_matrix[i, j] = proximity\n            prox_matrix[j, i] = proximity\n            \n    # 4. Calculate outlier threshold\n    prox_matrix_no_diag = prox_matrix.copy()\n    np.fill_diagonal(prox_matrix_no_diag, 0)\n    \n    D_train = np.sum(np.square(prox_matrix_no_diag), axis=1)\n    O_train = 1 / (EPSILON + D_train)\n    theta = np.quantile(O_train, Q_THRESHOLD)\n\n    # 5. Evaluate test patients\n    results = []\n    X_test = np.array(test_cases)\n    test_leaf_indices = rf.get_leaf_indices(X_test)\n    \n    for k in range(len(test_cases)):\n        proximities_to_train = np.sum(test_leaf_indices[k, :] == train_leaf_indices, axis=1) / T_TREES\n        D_test = np.sum(np.square(proximities_to_train))\n        O_test = 1 / (EPSILON + D_test)\n        results.append(O_test > theta)\n\n    # Final print statement\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4603276"}]}