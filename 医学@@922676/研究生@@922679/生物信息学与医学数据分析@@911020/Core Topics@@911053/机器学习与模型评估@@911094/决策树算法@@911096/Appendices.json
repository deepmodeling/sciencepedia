{"hands_on_practices": [{"introduction": "在处理生物信息学中的大规模数据集（如电子健康记录 EHR）时，算法的计算效率至关重要。在投入实际开发之前，理解模型的计算和内存需求是项目规划的关键一步。本练习将引导您从第一性原理出发，分析一个标准决策树训练算法的时间和空间复杂度，这对于在资源有限的环境下设计和部署可扩展的机器学习解决方案至关重要。[@problem_id:4553439]", "problem": "考虑一个为电子健康记录 (EHR) 数据集训练的用于分类的二叉决策树，其中 EHR 代表电子健康记录。数据集中有 $n$ 个患者（样本），每个患者有 $p$ 个连续特征。在每个内部节点，训练算法选择单个特征和一个实值阈值来分裂该节点的样本子集，以最小化一个可加的不纯度准则，例如基尼不纯度。假设使用以下算法设计，并从分裂评估的第一性原理出发，论证您分析的每一步：\n\n- 在训练开始时，对于每个特征，算法会构建一个包含所有 $n$ 个样本的排序索引数组，该数组根据该特征的值升序排列。\n- 在一个节点上，对于每个特征，算法会评估在到达该节点的样本中观察到的、位于连续不同特征值之间的所有有效阈值。为了评估特定阈值，算法利用节点内样本在该特征排序下的单次累积类别计数来计算该阈值产生的左右子节点的不纯度。\n- 决策树贪婪地生长，直到每个叶节点最多包含 $m$ 个样本，其中 $m \\in \\mathbb{N}$ 是用户指定的最小叶节点大小。假设分裂是“平衡的”，即在忽略取整效应的情况下，每次分裂都将大约一半的节点样本分配给每个子节点。\n\n仅使用这些假设和评估分裂所涉及操作的定义，推导：\n\n1) 训练整个决策树所需的总基本操作数的一个渐近紧界表达式 $T(n,p,m)$，该表达式是 $n$、$p$ 和 $m$ 的函数，结果应表示为一个乘法常数内的差异。您必须从第一性原理出发，论证所有节点上每个特征、每个阈值的扫描总和可以简化为 $n$、$p$ 和树深度的函数，并且最终的界限必须使用明确指定底数的标准对数来表示。\n\n2) 如果实现方式如下，训练期间主要内存占用 $M(n,p)$ 的一个渐近紧界表达式：\n- 将特征矩阵存储为 $8$ 字节的浮点值。\n- 对于每个特征，存储一个长度为 $n$ 的数组，其中包含表示预排序样本索引的 $4$ 字节整数。\n忽略诸如标签和树节点元数据等低阶内存项，并以字节为单位表示 $M(n,p)$。\n\n最后，对于一个 EHR 规模的数据集，其中 $n=1.5 \\times 10^{6}$，$p=500$，$m=200$，使用您推导的表达式 $M(n,p)$ 计算以 GB 为单位的主要内存占用，取 $1\\,\\mathrm{GB} = 10^{9}$ 字节。将您的答案四舍五入到三位有效数字，并以千兆字节（GB）表示最终内存。", "solution": "用户提供了一个描述特定决策树训练算法计算复杂度的题目。\n\n### 问题验证\n\n**第一步：提取已知条件**\n-   样本数（患者数）：$n$。\n-   特征数：$p$（连续）。\n-   算法：用于分类的二叉决策树。\n-   分裂准则：最小化一个可加的不纯度准则。\n-   预处理：对于 $p$ 个特征中的每一个，构建一个包含所有 $n$ 个样本的排序索引数组。\n-   在节点处寻找分裂：对于每个特征，评估所有位于连续不同特征值之间的阈值。对于给定特征的所有阈值，其不纯度计算通过使用累积类别计数的单次扫描完成。\n-   停止准则：叶节点最多有 $m$ 个样本，其中 $m \\in \\mathbb{N}$。\n-   分裂平衡假设：分裂大致是平衡的，每个子节点接收父节点大约一半的样本。\n-   任务1：推导总基本操作数的渐近紧界表达式 $T(n,p,m)$。\n-   任务2：推导主要内存占用的渐近紧界表达式 $M(n,p)$。\n-   内存存储细节：特征矩阵使用 $8$ 字节浮点数；预排序索引数组使用 $p$ 个长度为 $n$ 的 $4$ 字节整数数组。\n-   任务3：对于 $n=1.5 \\times 10^{6}$，$p=500$，$m=200$，计算 $M(n,p)$ 的值（以千兆字节为单位），其中 $1\\,\\mathrm{GB} = 10^{9}$ 字节，并四舍五入到三位有效数字。\n\n**第二步：使用提取的已知条件进行验证**\n该问题具有科学依据，提问明确且客观。它描述了针对连续特征的 CART（分类与回归树）算法的一种标准、高效的实现。诸如平衡分裂和使用累积计数计算不纯度等假设，在此类算法的分析中很常见。问题是自洽的，并提供了推导所需表达式和进行计算的所有必要信息。没有违反任何科学原理，也不存在歧义或矛盾。\n\n**第三步：结论与行动**\n该问题被判定为**有效**。现在开始解答过程。\n\n### 第一部分：时间复杂度 $T(n,p,m)$ 的推导\n\n总时间复杂度是两个不同阶段成本的总和：（1）初始的一次性预处理步骤，即对数据进行排序；（2）构建树的迭代过程。\n\n**阶段 A：预处理成本**\n算法首先为 $p$ 个特征中的每一个创建一个排序索引数组。这涉及对 $n$ 个样本进行排序。一个标准的、基于比较的排序算法的时间复杂度为 $\\Theta(n \\log n)$。由于需要对 $p$ 个特征中的每一个都执行此操作，因此该预处理阶段的总成本，记为 $T_{\\text{sort}}$，为：\n$$T_{\\text{sort}} \\propto p \\cdot n \\log_2 n$$\n我们选择以 $2$ 为底的对数，这是分析二元分裂过程的惯例，也符合题目中要求明确底数的要求。\n\n**阶段 B：树构建成本**\n树是通过递归地分裂节点来构建的。我们通过首先确定分裂单个节点的成本，然后将此成本累加到树中的所有节点来分析此阶段的成本。\n\n1.  **分裂单个节点的成本**：考虑一个包含 $n_k$ 个样本的节点。为了找到最优分裂，算法会遍历所有 $p$ 个特征。对于单个特征，它必须评估所有潜在的分裂点。潜在的分裂点位于该特征在 $n_k$ 个样本中连续的唯一值之间。题目指出，算法“利用节点内样本在该特征排序下的单次累积类别计数”来计算所产生子节点的不纯度。\n    这是一个关键的优化。这意味着对于给定的特征，我们可以遍历 $n_k$ 个样本（这些样本已在预处理步骤中排序），并在每一步中以常数时间 $O(1)$ 更新潜在左右子节点的类别计数（假设类别数量固定）。从这些计数中计算不纯度也是一个 $O(1)$ 的操作。因此，评估一个特征的所有 $n_k-1$ 个潜在分裂点所需的时间与 $n_k$ 成正比。\n    由于必须对所有 $p$ 个特征都这样做，找到一个包含 $n_k$ 个样本的节点的最佳分裂的总成本为：\n    $$T_{\\text{node}}(n_k) \\propto p \\cdot n_k$$\n\n2.  **所有节点的总成本**：为了找到总构建成本 $T_{\\text{build}}$，我们将 $T_{\\text{node}}(n_k)$ 在树的所有节点上求和。一个更有效的方法是逐层累加成本。在树的任意深度 $d$，该层的所有节点构成了原始 $n$ 个样本的一个划分。因此，在第 $d$ 层所有节点上的样本数 $n_k$ 之和为 $n$。\n    $$\\sum_{k \\in \\text{第 } d \\text{ 层节点}} n_k = n$$\n    树的整个一个层的总计算成本为：\n    $$T_{\\text{level}} = \\sum_{k \\in \\text{第 } d \\text{ 层节点}} T_{\\text{node}}(n_k) \\propto \\sum_{k \\in \\text{第 } d \\text{ 层节点}} p \\cdot n_k = p \\sum_{k \\in \\text{第 } d \\text{ 层节点}} n_k = p \\cdot n$$\n    这表明处理树的每一整层的成本是相同的，与 $p \\cdot n$ 成正比。\n\n3.  **树的深度**：最后一步是确定树的深度 $D$。当叶节点包含 $m$ 个或更少的样本时，树停止生长。根据“平衡分裂”的假设，深度为 $d$ 的节点大约包含 $n/2^d$ 个样本。当这个数量约等于 $m$ 时，过程终止。\n    $$\\frac{n}{2^D} \\approx m \\implies 2^D \\approx \\frac{n}{m} \\implies D \\approx \\log_2\\left(\\frac{n}{m}\\right)$$\n    因此，需要构建的总层数与 $\\log_2(n/m)$ 成正比。\n\n4.  **总构建成本**：构建树的总成本是每层的成本乘以层数：\n    $$T_{\\text{build}} \\propto T_{\\text{level}} \\times D \\propto (p \\cdot n) \\cdot \\log_2\\left(\\frac{n}{m}\\right)$$\n\n**总时间复杂度 $T(n,p,m)$**\n总时间是预处理成本和构建成本之和，在一个乘法常数范围内。\n$$T(n,p,m) \\propto T_{\\text{sort}} + T_{\\text{build}} \\propto p \\cdot n \\log_2 n + p \\cdot n \\log_2\\left(\\frac{n}{m}\\right)$$\n我们可以合并这些项：\n$$T(n,p,m) \\propto p \\cdot n \\left( \\log_2 n + \\log_2\\left(\\frac{n}{m}\\right) \\right) = p \\cdot n \\left( \\log_2 n + \\log_2 n - \\log_2 m \\right)$$\n$$T(n,p,m) \\propto p \\cdot n (2\\log_2 n - \\log_2 m)$$\n由于 $m \\ge 1$，$\\log_2(n/m) \\le \\log_2 n$。$p \\cdot n \\log_2 n$ 和 $p \\cdot n \\log_2(n/m)$ 这两项具有相同的渐近阶 $\\Theta(p \\cdot n \\log n)$，除非 $m$ 非常接近 $n$。一个能够捕捉两个主要阶段贡献的渐近紧界表达式是 $\\Theta(p \\cdot n \\log_2 n)$。然而，题目要求的是一个表达式，两个推导成本之和 $p \\cdot n \\log_2 n + p \\cdot n \\log_2(n/m)$ 是最直接且符合原理的推导结果。\n\n### 第二部分：内存占用 $M(n,p)$ 的推导\n\n题目要求的是主要内存占用，忽略低阶项。我们对所述的两个主要数据结构所需的内存求和。\n\n1.  **特征矩阵存储**：数据集有 $n$ 个样本和 $p$ 个特征。每个特征值存储为一个 $8$ 字节的浮点数。\n    $$M_{\\text{features}} = n \\times p \\times 8 \\text{ 字节}$$\n\n2.  **排序索引存储**：对于 $p$ 个特征中的每一个，都存储一个长度为 $n$ 的预排序索引数组。每个索引是一个 $4$ 字节的整数。\n    $$M_{\\text{indices}} = p \\times n \\times 4 \\text{ 字节}$$\n\n**总主要内存 $M(n,p)$**\n总主要内存占用是这两个部分的总和。\n$$M(n,p) = M_{\\text{features}} + M_{\\text{indices}} = 8np + 4np = 12np \\text{ 字节}$$\n\n### 第三部分：特定数据集的数值计算\n\n给定：\n-   $n = 1.5 \\times 10^{6}$\n-   $p = 500$\n-   $1\\,\\mathrm{GB} = 10^{9}$ 字节\n\n我们使用推导出的 $M(n,p)$ 表达式：\n$$M = 12 \\times (1.5 \\times 10^{6}) \\times 500$$\n$$M = (12 \\times 1.5) \\times 500 \\times 10^{6}$$\n$$M = 18 \\times 500 \\times 10^{6}$$\n$$M = 9000 \\times 10^{6} = 9 \\times 10^{3} \\times 10^{6} = 9 \\times 10^{9} \\text{ 字节}$$\n要将其转换为千兆字节 (GB)，我们除以 $10^9$：\n$$M_{\\text{GB}} = \\frac{9 \\times 10^{9} \\text{ 字节}}{10^{9} \\text{ 字节/GB}} = 9 \\text{ GB}$$\n四舍五入到三位有效数字，结果为 $9.00$ GB。", "answer": "$$\\boxed{p \\cdot n \\left(\\log_2 n + \\log_2\\left(\\frac{n}{m}\\right)\\right), \\quad 12np, \\quad 9.00}$$", "id": "4553439"}, {"introduction": "标准的决策树算法对所有类型的分类错误一视同仁，但在医疗诊断中，不同错误的代价（例如，将患病者诊断为健康者）往往截然不同。为了构建临床上真正有用的模型，我们必须将这些非对称成本纳入考量。本练习将深入探讨如何通过修改基尼不纯度和熵等核心分裂准则来实现成本敏感学习，并要求您从头推导加权公式并完成具体计算，从而掌握构建更符合现实世界需求的模型的方法。[@problem_id:4553423]", "problem": "在生物信息学和医疗数据分析的二元医学诊断任务中，每位患者被标记为 $y \\in \\{0,1\\}$，其中 $y=1$ 表示患有某种疾病，$y=0$ 表示未患病。将患病患者错误分类为健康会产生假阴性成本 $c_{\\mathrm{FN}}$，而将健康患者错误分类为患病会产生假阳性成本 $c_{\\mathrm{FP}}$。为了在决策树算法中考虑这些与类别相关的成本，每个训练实例都被赋予一个与类别相关的权重 $w_i = c(y_i)$，其中 $c(1) = c_{\\mathrm{FN}}$，$c(0) = c_{\\mathrm{FP}}$。考虑一个决策树，它在连续的生物标志物 $x$ 上搜索阈值，以分裂一个包含一组患者 $N$ 的节点。\n\n从节点不纯度度量是节点内经验类别分布的函数，并且经验概率通过归一化总和来估计这一基本定义出发，完成以下任务：\n\n1.  在节点 $N$ 中，使用与类别相关的权重 $\\{w_i\\}_{i \\in N}$ 推导类别 $k \\in \\{0,1\\}$ 的加权经验类别概率 $\\hat{p}_k$，并解释为什么在对排序后的 $x$ 进行阈值搜索时，所使用的累积和必须累加特定于类别的权重，而不是原始计数。\n2.  从第一性原理出发，为成本加权标签推导调整后的不纯度公式：\n    a) 节点 $N$ 的加权基尼不纯度 $G(N)$，用加权经验类别概率表示。\n    b) 节点 $N$ 的加权香农熵 $H(N)$，用加权经验类别概率表示。\n3.  对于以下数据集，当 $c_{\\mathrm{FN}} = 10$ 且 $c_{\\mathrm{FP}} = 2$ 时，计算通过在生物标志物 $x$ 上设置阈值 $t = 1.5$ 所实现的精确加权基尼不纯度下降值。该数据集包含6名患者：\n    -   患者 1：$(x_1, y_1) = (0.5, 1)$\n    -   患者 2：$(x_2, y_2) = (1.2, 0)$\n    -   患者 3：$(x_3, y_3) = (1.4, 1)$\n    -   患者 4：$(x_4, y_4) = (2.0, 0)$\n    -   患者 5：$(x_5, y_5) = (2.3, 1)$\n    -   患者 6：$(x_6, y_6) = (3.1, 0)$\n\n假设分裂前的节点包含所有6名患者，在阈值 $t$ 处的分割将所有 $x \\leq t$ 的患者分到左子节点，其余的到右子节点。对父节点和子节点使用加权基尼不纯度，并将不纯度下降定义为父节点不纯度减去子节点不纯度的加权平均值，其中权重是每个子节点中与类别相关的总权重。将最终答案表示为一个无四舍五入的精确分数。", "solution": "该问题是有效的，因为它在科学上基于成本敏感机器学习的原理，问题表述清晰且提供了所有必要信息，并且陈述客观。我们将按要求分三部分进行解答。\n\n### 第1部分：加权经验概率与累积和的作用\n\n在标准的分类设置中，节点 $N$ 中类别 $k$ 的经验概率是 $N$ 中属于类别 $k$ 的实例所占的比例。设 $|N_k|$ 是节点中类别 $k$ 的实例数，$|N|$ 是总实例数。标准经验概率 $p_k$ 由 $p_k = \\frac{|N_k|}{|N|}$ 给出。\n\n在所述的成本敏感设置中，每个实例 $i$ 根据其类别 $y_i$ 被赋予一个权重 $w_i$。具体来说，$w_i = c(y_i)$，其中 $c(1) = c_{\\mathrm{FN}}$，$c(0) = c_{\\mathrm{FP}}$。“比例”的概念必须进行调整以考虑这些权重。节点 $N$ 中实例的总“质量”不再是计数 $|N|$，而是其权重的总和，$W_N = \\sum_{i \\in N} w_i$。类似地，对应于类别 $k$ 的总质量是属于该类别的实例的权重总和，$W_{N,k} = \\sum_{i \\in N, y_i=k} w_i$。\n\n加权经验类别概率 $\\hat{p}_k$ 是标准概率的推广，定义为类别 $k$ 的总权重与节点中所有实例的总权重之比。\n$$\n\\hat{p}_k = \\frac{\\sum_{i \\in N, y_i=k} w_i}{\\sum_{j \\in N} w_j} = \\frac{W_{N,k}}{W_N}\n$$\n这种表述确保了具有较高错分成本（因此权重较高）的类别对经验概率分布的贡献更大，从而影响不纯度度量，使其优先考虑对这些类别的正确分类。\n\n在搜索连续生物标志物 $x$ 的最优分裂阈值时，决策树算法通常根据实例的 $x$ 值对其进行排序。潜在的分裂点在相邻的 $x$ 值之间进行评估。一种朴素的方法是为每个潜在的分裂重新计算左右子节点的类别分布（以及不纯度），这在计算上是昂贵的。一种更有效的方法是使用累积和。当我们遍历排序后的实例时，可以维护类别分布的动态累计。在非加权情况下，这涉及到为每个类别增加计数。在成本加权的情况下，这推广为累加特定于类别的权重 $w_i$。通过维护每个类别权重的累积和（例如，$W_0(x_j) = \\sum_{i \\mid x_i \\leq x_j, y_i=0} w_i$ 和 $W_1(x_j) = \\sum_{i \\mid x_i \\leq x_j, y_i=1} w_i$），对于任何分裂阈值 $t$，左子节点（$x \\leq t$）和右子节点（$x  t$）的总权重可以在常数时间内计算出来。因此，为了有效地发现最优的成本敏感分裂，累积和必须累加特定于类别的权重，而不仅仅是原始计数。\n\n### 第2部分：调整后的不纯度公式\n\n调整后的不纯度公式是通过在标准不纯度定义中用加权经验概率 $\\hat{p}_k$ 替换标准经验概率 $p_k$ 得出的。\n\n**a) 加权基尼不纯度 $G(N)$**\n\n一组类别的标准基尼不纯度是 $G_{\\text{std}} = 1 - \\sum_k p_k^2$。对于二元情况（$k \\in \\{0, 1\\}$），这是 $G_{\\text{std}} = 1 - (p_0^2 + p_1^2)$。\n通过用 $\\hat{p}_k$ 替换 $p_k$，我们得到节点 $N$ 的加权基尼不纯度：\n$$\nG(N) = 1 - \\sum_{k \\in \\{0,1\\}} \\hat{p}_k^2 = 1 - \\left(\\hat{p}_0^2 + \\hat{p}_1^2\\right)\n$$\n代入 $\\hat{p}_k$ 的定义：\n$$\nG(N) = 1 - \\left[ \\left(\\frac{W_{N,0}}{W_N}\\right)^2 + \\left(\\frac{W_{N,1}}{W_N}\\right)^2 \\right]\n$$\n\n**b) 加权香农熵 $H(N)$**\n\n标准香农熵是 $H_{\\text{std}} = - \\sum_k p_k \\log(p_k)$，其中对数的底决定了单位（例如，以2为底单位是比特）。使用在理论推导中常见的自然对数，熵为 $H_{\\text{std}} = - \\sum_k p_k \\ln(p_k)$。\n用 $\\hat{p}_k$ 替换 $p_k$，加权香农熵为：\n$$\nH(N) = - \\sum_{k \\in \\{0,1\\}} \\hat{p}_k \\ln(\\hat{p}_k) = - \\left( \\hat{p}_0 \\ln(\\hat{p}_0) + \\hat{p}_1 \\ln(\\hat{p}_1) \\right)\n$$\n代入 $\\hat{p}_k$ 的定义：\n$$\nH(N) = - \\left[ \\frac{W_{N,0}}{W_N} \\ln\\left(\\frac{W_{N,0}}{W_N}\\right) + \\frac{W_{N,1}}{W_N} \\ln\\left(\\frac{W_{N,1}}{W_N}\\right) \\right]\n$$\n\n### 第3部分：加权基尼不纯度下降的计算\n\n给定的成本是 $c_{\\mathrm{FN}} = 10$ 和 $c_{\\mathrm{FP}} = 2$。因此，任何标签为 $y=1$ 的实例的权重是 $w_1 = 10$，标签为 $y=0$ 的实例的权重是 $w_0 = 2$。\n\n带权重的数据集如下：\n- 患者 1：$(0.5, 1)$，权重 $w_1 = 10$\n- 患者 2：$(1.2, 0)$，权重 $w_2 = 2$\n- 患者 3：$(1.4, 1)$，权重 $w_3 = 10$\n- 患者 4：$(2.0, 0)$，权重 $w_4 = 2$\n- 患者 5：$(2.3, 1)$，权重 $w_5 = 10$\n- 患者 6：$(3.1, 0)$，权重 $w_6 = 2$\n\n**父节点不纯度 ($N_p$)**\n父节点包含所有6名患者。\n类别0 ($y=0$) 的总权重：$W_{N_p,0} = w_2 + w_4 + w_6 = 2+2+2 = 6$。\n类别1 ($y=1$) 的总权重：$W_{N_p,1} = w_1 + w_3 + w_5 = 10+10+10 = 30$。\n父节点中的总权重：$W_{N_p} = W_{N_p,0} + W_{N_p,1} = 6 + 30 = 36$。\n加权经验概率：\n$\\hat{p}_{p,0} = \\frac{W_{N_p,0}}{W_{N_p}} = \\frac{6}{36} = \\frac{1}{6}$\n$\\hat{p}_{p,1} = \\frac{W_{N_p,1}}{W_{N_p}} = \\frac{30}{36} = \\frac{5}{6}$\n父节点的加权基尼不纯度：\n$G(N_p) = 1 - (\\hat{p}_{p,0}^2 + \\hat{p}_{p,1}^2) = 1 - \\left[ \\left(\\frac{1}{6}\\right)^2 + \\left(\\frac{5}{6}\\right)^2 \\right] = 1 - \\left(\\frac{1}{36} + \\frac{25}{36}\\right) = 1 - \\frac{26}{36} = \\frac{10}{36} = \\frac{5}{18}$。\n\n**在 $t=1.5$ 处分裂后的子节点**\n分裂规则是 $x \\leq t$ 的到左子节点 ($N_L$)，$x  t$ 的到右子节点 ($N_R$)。\n$N_L = \\{(0.5, 1), (1.2, 0), (1.4, 1)\\}$\n$N_R = \\{(2.0, 0), (2.3, 1), (3.1, 0)\\}$\n\n**左子节点不纯度 ($N_L$)**\n类别0的总权重：$W_{N_L,0} = w_2 = 2$。\n类别1的总权重：$W_{N_L,1} = w_1 + w_3 = 10 + 10 = 20$。\n左子节点中的总权重：$W_{N_L} = W_{N_L,0} + W_{N_L,1} = 2 + 20 = 22$。\n加权经验概率：\n$\\hat{p}_{L,0} = \\frac{2}{22} = \\frac{1}{11}$\n$\\hat{p}_{L,1} = \\frac{20}{22} = \\frac{10}{11}$\n左子节点的加权基尼不纯度：\n$G(N_L) = 1 - \\left[ \\left(\\frac{1}{11}\\right)^2 + \\left(\\frac{10}{11}\\right)^2 \\right] = 1 - \\left(\\frac{1}{121} + \\frac{100}{121}\\right) = 1 - \\frac{101}{121} = \\frac{20}{121}$。\n\n**右子节点不纯度 ($N_R$)**\n类别0的总权重：$W_{N_R,0} = w_4 + w_6 = 2 + 2 = 4$。\n类别1的总权重：$W_{N_R,1} = w_5 = 10$。\n右子节点中的总权重：$W_{N_R} = W_{N_R,0} + W_{N_R,1} = 4 + 10 = 14$。\n加权经验概率：\n$\\hat{p}_{R,0} = \\frac{4}{14} = \\frac{2}{7}$\n$\\hat{p}_{R,1} = \\frac{10}{14} = \\frac{5}{7}$\n右子节点的加权基尼不纯度：\n$G(N_R) = 1 - \\left[ \\left(\\frac{2}{7}\\right)^2 + \\left(\\frac{5}{7}\\right)^2 \\right] = 1 - \\left(\\frac{4}{49} + \\frac{25}{49}\\right) = 1 - \\frac{29}{49} = \\frac{20}{49}$。\n\n**不纯度下降**\n不纯度下降 $\\Delta G$ 是父节点的不纯度减去子节点不纯度的加权平均值。用于求平均的权重是每个子节点中总权重的比例。\n$$\n\\Delta G = G(N_p) - \\left( \\frac{W_{N_L}}{W_{N_p}} G(N_L) + \\frac{W_{N_R}}{W_{N_p}} G(N_R) \\right)\n$$\n代入计算出的值：\n$$\n\\Delta G = \\frac{5}{18} - \\left( \\frac{22}{36} \\cdot \\frac{20}{121} + \\frac{14}{36} \\cdot \\frac{20}{49} \\right)\n$$\n$$\n\\Delta G = \\frac{5}{18} - \\left( \\frac{11}{18} \\cdot \\frac{20}{121} + \\frac{7}{18} \\cdot \\frac{20}{49} \\right)\n$$\n提取公因式 $\\frac{20}{18}$：\n$$\n\\Delta G = \\frac{5}{18} - \\frac{20}{18} \\left( \\frac{11}{121} + \\frac{7}{49} \\right) = \\frac{5}{18} - \\frac{10}{9} \\left( \\frac{1}{11} + \\frac{1}{7} \\right)\n$$\n计算括号中的和：\n$$\n\\frac{1}{11} + \\frac{1}{7} = \\frac{7}{77} + \\frac{11}{77} = \\frac{18}{77}\n$$\n代回：\n$$\n\\Delta G = \\frac{5}{18} - \\frac{10}{9} \\cdot \\frac{18}{77} = \\frac{5}{18} - \\frac{10 \\cdot 2}{77} = \\frac{5}{18} - \\frac{20}{77}\n$$\n找到公分母，即 $18 \\times 77 = 1386$：\n$$\n\\Delta G = \\frac{5 \\cdot 77}{18 \\cdot 77} - \\frac{20 \\cdot 18}{77 \\cdot 18} = \\frac{385}{1386} - \\frac{360}{1386} = \\frac{25}{1386}\n$$\n分数 $\\frac{25}{1386}$ 是最简形式，因为分子的质因数分解是 $5^2$，分母的质因数分解是 $1386 = 2 \\cdot 3^2 \\cdot 7 \\cdot 11$。没有公因子。", "answer": "$$\\boxed{\\frac{25}{1386}}$$", "id": "4553423"}, {"introduction": "一个在特定人群上训练的机器学习模型，当部署到不同的人群或环境中时，其性能可能会发生变化，这一现象被称为“分布偏移”。评估模型在面对这种变化时的稳健性，是确保其在现实世界中安全有效的关键。本练习提供了一个动手实践的编码任务，您将通过模拟特征分布的变化来执行敏感性分析，量化模型预测风险的变化，这是模型验证和部署后监控的核心技能。[@problem_id:4553448]", "problem": "给定一个二元决策树模型，该模型利用三个特征来评估临床人群的疾病风险，其数学定义如下。设特征向量为 $x = (x_1, x_2, x_3)$，其中 $x_1$ 和 $x_2$ 是实值临床生物标志物，$x_3$ 是一个二元指示变量。该决策树产生一个具有四个叶节点的分段常数预测 $f(x)$。决策路径及其相关的叶节点风险值如下：\n\n- 叶节点 $L_1$：条件为 $(x_1 \\le \\tau_1)$ 和 $(x_2  \\tau_2)$，风险值为 $r_1$。\n- 叶节点 $L_2$：条件为 $(x_1 \\le \\tau_1)$ 和 $(x_2 \\le \\tau_2)$，风险值为 $r_2$。\n- 叶节点 $L_3$：条件为 $(x_1  \\tau_1)$ 和 $(x_3 = 1)$，风险值为 $r_3$。\n- 叶节点 $L_4$：条件为 $(x_1  \\tau_1)$ 和 $(x_3 = 0)$，风险值为 $r_4$。\n\n假设在训练环境和部署环境下，这些特征都是统计独立的。在所有测试用例中，决策树的阈值和叶节点风险是固定的，分别为 $\\tau_1 = 50$、$\\tau_2 = 2.5$、$r_1 = 0.7$、$r_2 = 0.2$、$r_3 = 0.9$ 和 $r_4 = 0.1$（均为无量纲值）。\n\n在任何特征分布下，一个群体的预期预测风险是决策树预测的数学期望，即 $E[f(X)]$，其中 $X$ 是一个根据特征分布进行分布的随机向量。在部署时发生分布偏移的情况下，由于遍历不同决策路径的概率发生变化，预期预测风险也会改变。\n\n您的任务是构建一个灵敏度分析，以量化由特征分布扰动引起的决策路径上预测风险的变化。仅使用以下基本定义和假设：\n\n- 在分布 $\\mathcal{D}$ 下，函数 $g(X)$ 的期望值为 $E_{\\mathcal{D}}[g(X)] = \\int g(x) \\, d\\mathcal{D}(x)$。\n- 对于独立特征，事件合取的概率等于各个事件概率的乘积。\n- 对于成功概率为 $p$ 的二元变量 $x_3 \\in \\{0,1\\}$，$P(x_3 = 1) = p$ 且 $P(x_3 = 0) = 1 - p$。\n- 对于一个正态（高斯）随机变量 $Z \\sim \\mathcal{N}(\\mu, \\sigma^2)$，在阈值 $t$ 处的累积分布函数是 $\\Phi\\left(\\frac{t - \\mu}{\\sigma}\\right)$，其中 $\\Phi$ 是标准正态累积分布函数。\n- 对于正态分布的有限混合 $Z \\sim \\sum_{k=1}^K w_k \\, \\mathcal{N}(\\mu_k, \\sigma_k^2)$，其中非负权重满足 $\\sum_{k=1}^K w_k = 1$，在阈值 $t$ 处的累积概率是 $\\sum_{k=1}^K w_k \\, \\Phi\\left(\\frac{t - \\mu_k}{\\sigma_k}\\right)$。\n\n对于每个叶节点 $L_i$，将其在某个分布下的路径概率定义为随机抽取的样本 $X$ 满足该叶节点不等式和等式合取条件的概率。对预期预测风险的路径贡献是叶节点风险与路径概率的乘积。由于部署偏移导致的某条路径上预测风险的变化，是偏移后和基准路径贡献之间的差值。\n\n您必须为下面的每个测试用例计算以下两个量：\n- 预期预测风险的总变化，$\\Delta_{\\mathrm{total}}$，定义为差值 $E_{\\mathrm{shift}}[f(X)] - E_{\\mathrm{base}}[f(X)]$，以小数形式表示（而非百分比）。\n- 四个叶节点中最大的绝对路径贡献变化，$\\Delta_{\\max}$，以小数形式表示（而非百分比）。\n\n该测试套件包含三个科学上合理的部署偏移场景，每个场景都指定了基准分布和扰动，且所有特征均独立：\n\n测试用例 1（连续特征的均值偏移和二元特征的概率偏移）：\n- 基准分布：$x_1 \\sim \\mathcal{N}(55, 10^2)$，$x_2 \\sim \\mathcal{N}(2.0, 0.5^2)$，$x_3 \\sim \\text{Bernoulli}(0.3)$。\n- 偏移后分布：$x_1 \\sim \\mathcal{N}(50, 10^2)$，$x_2 \\sim \\mathcal{N}(2.5, 0.5^2)$，$x_3 \\sim \\text{Bernoulli}(0.4)$。\n\n测试用例 2（阈值处近确定性的连续特征和高概率的二元特征）：\n- 基准分布：$x_1 \\sim \\mathcal{N}(50, (10^{-4})^2)$，$x_2 \\sim \\mathcal{N}(1.5, 0.01^2)$，$x_3 \\sim \\text{Bernoulli}(0.99)$。\n- 偏移后分布：$x_1 \\sim \\mathcal{N}(50.1, (10^{-4})^2)$，$x_2 \\sim \\mathcal{N}(1.4, 0.01^2)$，$x_3 \\sim \\text{Bernoulli}(0.95)$。\n\n测试用例 3（一个生物标志物的混合分布偏移和另一个特征的均值偏移）：\n- 基准分布：$x_1 \\sim \\mathcal{N}(40, 15^2)$，$x_2 \\sim 0.6 \\cdot \\mathcal{N}(1.8, 0.4^2) + 0.4 \\cdot \\mathcal{N}(3.0, 0.3^2)$，$x_3 \\sim \\text{Bernoulli}(0.5)$。\n- 偏移后分布：$x_1 \\sim \\mathcal{N}(45, 15^2)$，$x_2 \\sim 0.5 \\cdot \\mathcal{N}(1.9, 0.4^2) + 0.5 \\cdot \\mathcal{N}(2.9, 0.3^2)$，$x_3 \\sim \\text{Bernoulli}(0.4)$。\n\n您的程序应在特征独立的假设下，为这三个用例分别计算 $\\Delta_{\\mathrm{total}}$ 和 $\\Delta_{\\max}$，并生成一行输出。该输出包含一个用方括号括起来的逗号分隔列表，其中每个元素对应一个测试用例，并且本身是一个包含两个元素的列表 $[\\Delta_{\\mathrm{total}}, \\Delta_{\\max}]$。例如，最终输出格式必须为 $[[a_1,b_1],[a_2,b_2],[a_3,b_3]]$，其中包含小数。不应打印任何其他文本。", "solution": "该问题是有效的，因为它在科学上基于概率论和统计学，定义明确，提供了所有必要的参数和定义，并且陈述客观。它代表了对决策树模型在分布偏移下的一个形式化和可量化的灵敏度分析，这是机器学习鲁棒性中的一个标准问题。\n\n解决方案首先建立计算预期预测风险的数学框架，然后将此框架应用于每个测试用例以计算所需的指标。\n\n**1. 理论框架**\n\n设特征向量为 $X = (X_1, X_2, X_3)$，其中 $X_1, X_2, X_3$ 是代表临床特征的独立随机变量。决策树模型是一个函数 $f(X)$，它将特征向量映射到一个风险值。树的叶节点，表示为 $L_1, L_2, L_3, L_4$，构成了特征空间的一个划分。预测函数是分段常数的，对于任何落入叶节点 $L_i$ 的输入，其取值为 $r_i$。\n\n预期预测风险 $E[f(X)]$ 是该函数在特征的联合概率分布上的期望。根据全期望定律（或离散值函数的期望定义），这可以表示为叶节点风险的加权和，其中权重是特征向量落入每个叶节点的概率：\n$$E[f(X)] = \\sum_{i=1}^{4} r_i P(X \\in L_i)$$\n$C_i = r_i P(X \\in L_i)$ 项被定义为叶节点 $L_i$ 对预期风险的路径贡献。\n\n落入一个叶节点的概率 $P(X \\in L_i)$ 由从根到该叶节点的路径上的条件合取决定。由于题目中声明了特征独立的假设，事件合取的概率是它们各自概率的乘积。\n\n让我们根据决策阈值 $\\tau_1=50$ 和 $\\tau_2=2.5$ 定义以下基本概率：\n- $p_1 = P(X_1 \\le \\tau_1)$\n- $p_2 = P(X_2 \\le \\tau_2)$\n- $p_3 = P(X_3 = 1)$\n\n四个叶节点的路径概率则为：\n- $P(L_1) = P(X_1 \\le \\tau_1 \\text{ and } X_2  \\tau_2) = P(X_1 \\le \\tau_1) P(X_2  \\tau_2) = p_1 (1 - p_2)$\n- $P(L_2) = P(X_1 \\le \\tau_1 \\text{ and } X_2 \\le \\tau_2) = P(X_1 \\le \\tau_1) P(X_2 \\le \\tau_2) = p_1 p_2$\n- $P(L_3) = P(X_1  \\tau_1 \\text{ and } X_3 = 1) = P(X_1  \\tau_1) P(X_3 = 1) = (1 - p_1) p_3$\n- $P(L_4) = P(X_1  \\tau_1 \\text{ and } X_3 = 0) = P(X_1  \\tau_1) P(X_3 = 0) = (1 - p_1) (1 - p_3)$\n\n固定的风险值为 $r_1 = 0.7$、$r_2 = 0.2$、$r_3 = 0.9$ 和 $r_4 = 0.1$。\n\n**2. 基本概率的计算**\n\n问题提供了基于 $X_1$ 和 $X_2$ 的分布来计算概率 $p_1$ 和 $p_2$ 的公式。\n- 如果一个特征 $Z$ 服从正态分布 $Z \\sim \\mathcal{N}(\\mu, \\sigma^2)$，其累积概率为 $P(Z \\le t) = \\Phi\\left(\\frac{t - \\mu}{\\sigma}\\right)$，其中 $\\Phi$ 是标准正态累积分布函数（CDF）。\n- 如果一个特征 $Z$ 服从正态分布的有限混合 $Z \\sim \\sum_{k=1}^K w_k \\, \\mathcal{N}(\\mu_k, \\sigma_k^2)$，其累积概率为 $P(Z \\le t) = \\sum_{k=1}^K w_k \\, \\Phi\\left(\\frac{t - \\mu_k}{\\sigma_k}\\right)$。\n- 对于二元特征 $X_3 \\sim \\text{Bernoulli}(p)$，概率 $p_3 = P(X_3 = 1) = p$ 是直接给出的。\n\n**3. 量化分布偏移的影响**\n\n该分析需要比较一个“基准”分布 $\\mathcal{D}_{\\text{base}}$ 和一个“偏移后”的分布 $\\mathcal{D}_{\\text{shift}}$。我们将用上标 'b' 表示在基准分布下计算的量（例如，$E_{\\text{base}}[f(X)]$，$P(L_i)^b$），用上标 's' 表示在偏移后分布下计算的量。\n\n需要计算的两个指标是：\n- 预期预测风险的总变化：\n  $$\\Delta_{\\mathrm{total}} = E_{\\text{shift}}[f(X)] - E_{\\text{base}}[f(X)]$$\n- 四个叶节点中最大的绝对路径贡献变化：\n  $$\\Delta_{\\max} = \\max_{i \\in \\{1,2,3,4\\}} |C_i^s - C_i^b| = \\max_{i \\in \\{1,2,3,4\\}} |r_i P(L_i)^s - r_i P(L_i)^b|$$\n\n**4. 算法流程**\n\n对于每个测试用例，执行以下流程：\n1.  为 $X_1, X_2, X_3$ 定义基准和偏移后分布的参数。\n2.  **基准计算**：\n    a. 使用基准分布参数和适当的 CDF 公式计算基本概率 $p_1^b$, $p_2^b$ 和 $p_3^b$。\n    b. 计算基准路径概率 $P(L_1)^b, P(L_2)^b, P(L_3)^b, P(L_4)^b$。\n    c. 计算基准路径贡献 $C_i^b = r_i P(L_i)^b$（$i=1..4$）。\n    d. 计算总的基准预期风险 $E_{\\text{base}}[f(X)] = \\sum_{i=1}^4 C_i^b$。\n3.  **偏移后计算**：\n    a. 使用偏移后分布参数计算基本概率 $p_1^s$, $p_2^s$ 和 $p_3^s$。\n    b. 计算偏移后路径概率 $P(L_1)^s, P(L_2)^s, P(L_3)^s, P(L_4)^s$。\n    c. 计算偏移后路径贡献 $C_i^s = r_i P(L_i)^s$（$i=1..4$）。\n    d. 计算总的偏移后预期风险 $E_{\\text{shift}}[f(X)] = \\sum_{i=1}^4 C_i^s$。\n4.  **最终指标**：\n    a. 计算 $\\Delta_{\\mathrm{total}} = E_{\\text{shift}}[f(X)] - E_{\\text{base}}[f(X)]$。\n    b. 计算路径贡献的绝对变化 $|\\Delta C_i| = |C_i^s - C_i^b|$（$i=1..4$）。\n    c. 找出 $\\Delta_{\\max} = \\max_{i} |\\Delta C_i|$。\n\n该流程将为提供的三个测试用例中的每一个实施。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the change in decision tree risk prediction under distributional shift.\n    \"\"\"\n    \n    # Fixed parameters from the problem statement\n    tau1 = 50.0\n    tau2 = 2.5\n    risks = {\n        'r1': 0.7, \n        'r2': 0.2, \n        'r3': 0.9, \n        'r4': 0.1\n    }\n\n    # Test cases: Each case is a tuple of (base_params, shifted_params).\n    # Distribution parameters are structured as follows:\n    # - Normal: ('norm', mu, sigma)\n    # - Mixture of Normals: ('mixture', [(w1, mu1, sigma1), ...])\n    # - Bernoulli: ('bern', p)\n    test_cases = [\n        # Test Case 1\n        (\n            ( ('norm', 55.0, 10.0), ('norm', 2.0, 0.5), ('bern', 0.3) ),\n            ( ('norm', 50.0, 10.0), ('norm', 2.5, 0.5), ('bern', 0.4) )\n        ),\n        # Test Case 2\n        (\n            ( ('norm', 50.0, 1e-4), ('norm', 1.5, 0.01), ('bern', 0.99) ),\n            ( ('norm', 50.1, 1e-4), ('norm', 1.4, 0.01), ('bern', 0.95) )\n        ),\n        # Test Case 3\n        (\n            ( ('norm', 40.0, 15.0), \n              ('mixture', [(0.6, 1.8, 0.4), (0.4, 3.0, 0.3)]), \n              ('bern', 0.5) ),\n            ( ('norm', 45.0, 15.0), \n              ('mixture', [(0.5, 1.9, 0.4), (0.5, 2.9, 0.3)]), \n              ('bern', 0.4) )\n        ),\n    ]\n\n    def get_prob_le_threshold(dist_params, threshold):\n        \"\"\"\n        Calculates P(X = threshold) for Normal or mixture-of-Normals distributions.\n        \"\"\"\n        dist_type = dist_params[0]\n        if dist_type == 'norm':\n            _, mu, sigma = dist_params\n            return norm.cdf(threshold, loc=mu, scale=sigma)\n        elif dist_type == 'mixture':\n            _, components = dist_params\n            total_prob = 0.0\n            for w, mu, sigma in components:\n                total_prob += w * norm.cdf(threshold, loc=mu, scale=sigma)\n            return total_prob\n        else:\n            raise ValueError(\"Unsupported distribution type\")\n\n    def calculate_metrics(params, thresholds, risk_values):\n        \"\"\"\n        Calculates path probabilities and total expected risk for a given set of distribution parameters.\n        \"\"\"\n        x1_params, x2_params, x3_params = params\n        \n        # Calculate elemental probabilities\n        p1 = get_prob_le_threshold(x1_params, thresholds['tau1'])\n        p2 = get_prob_le_threshold(x2_params, thresholds['tau2'])\n        p3 = x3_params[1] # For Bernoulli ('bern', p)\n\n        # Calculate path probabilities\n        path_probs = {\n            'L1': p1 * (1 - p2),\n            'L2': p1 * p2,\n            'L3': (1 - p1) * p3,\n            'L4': (1 - p1) * (1 - p3)\n        }\n        \n        # Calculate path contributions\n        path_contributions = {\n            'L1': risk_values['r1'] * path_probs['L1'],\n            'L2': risk_values['r2'] * path_probs['L2'],\n            'L3': risk_values['r3'] * path_probs['L3'],\n            'L4': risk_values['r4'] * path_probs['L4']\n        }\n        \n        # Calculate total expected risk\n        total_risk = sum(path_contributions.values())\n        \n        return total_risk, path_contributions\n\n    results = []\n    for base_params, shift_params in test_cases:\n        \n        thresholds = {'tau1': tau1, 'tau2': tau2}\n        \n        # Calculate for base distribution\n        base_risk, base_contributions = calculate_metrics(base_params, thresholds, risks)\n        \n        # Calculate for shifted distribution\n        shift_risk, shift_contributions = calculate_metrics(shift_params, thresholds, risks)\n        \n        # Compute delta_total\n        delta_total = shift_risk - base_risk\n        \n        # Compute delta_max\n        path_changes = []\n        for leaf in ['L1', 'L2', 'L3', 'L4']:\n            change = shift_contributions[leaf] - base_contributions[leaf]\n            path_changes.append(abs(change))\n        \n        delta_max = max(path_changes)\n        \n        results.append([delta_total, delta_max])\n\n    # Final print statement in the exact required format.\n    # The format is a list of lists, so we map each inner list to its string representation.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4553448"}]}