{"hands_on_practices": [{"introduction": "K-均值聚类依赖于欧几里得距离，因此对特征的尺度非常敏感。这个练习将通过一个思想实验，量化地展示压缩一个高方差特征将如何从根本上改变聚类结果，从而清晰地揭示了特征标准化在患者分层中的重要性。通过推导一个临界压缩因子，您将深入理解特征权重是如何影响最终的聚类分配的 [@problem_id:4576104]。", "problem": "一个由 $N$ 名患者组成的生物医学队列由每位患者的两个特征表示：一个高方差的实验室测量值 $L$ 和一个标准化的风险评分 $R$。使用 $K=2$ 进行聚类，其K-均值目标是最小化簇内欧几里得距离的平方和。从K-均值目标的定义出发，$$J=\\sum_{k=1}^{K}\\sum_{i \\in C_k}\\|x_i-\\mu_k\\|_2^2,$$ 其中 $x_i \\in \\mathbb{R}^2$ 是患者特征向量，$\\mu_k$ 是簇均值，仅考虑对特征 $L$ 进行以下仿射最小-最大压缩：\n- 设队列中 $L$ 的经验最小值为 $L_{\\min}$，最大值为 $L_{\\max}$，其范围为 $r=L_{\\max}-L_{\\min}$。\n- 对 $L$ 定义一个压缩坐标 $z=\\frac{c}{r}(L-L_{\\min})$，其中 $c \\in (0,r]$ 是选定的压缩因子。\n- 保持 $R$ 不变。\n\n任务 A. 仅使用 $J$ 的定义以及均值和方差的基本线性性质，推导在对 $L$ 应用上述压缩（$R$ 保持不变）并在压缩空间中重新计算簇均值之后，变换后的目标函数 $J_c$。将 $J_c$ 用原始的各特征簇内平方和表示。\n\n任务 B. 现在，量化当 $L$ 的范围被压缩时，特定患者的簇分配变化。假设在原始未压缩的尺度上，整个队列的 $L_{\\min}=0$ 且 $L_{\\max}=100$（因此 $r=100$）。考虑在原始尺度上估计的两个簇中心，$\\mu_1=(L_1,R_1)=(35.5,0)$ 和 $\\mu_2=(L_2,R_2)=(39.5,1)$。考虑一个患者 $x=(L_x,R_x)=(50,0)$。假设当压缩因子为 $c$ 的压缩应用于 $L$（而 $R$ 保持不变）时，压缩空间中的簇均值是 $\\mu_1$ 和 $\\mu_2$ 在对 $L$ 进行相同仿射变换（对 $R$ 进行恒等变换）下的像。确定精确的临界值 $c^\\star \\in (0,100]$，使得当 $c=c^\\star$ 时，在压缩空间中点 $x$ 到两个簇中心的平方距离相等，并且当 $c>c^\\star$ 时，$x$ 的分配与原始尺度上的分配相匹配，而当 $c  c^\\star$ 时，该点的分配会发生翻转。", "solution": "用户提供了一个由两部分组成的问题，关于特征缩放对K-均值聚类目标函数和簇分配的影响。该问题提法明确，具有科学依据，并包含了完整解答所需的所有信息。\n\n**A 部分：变换后目标函数 $J_c$ 的推导**\n\n原始的K-均值目标函数由下式给出\n$$J=\\sum_{k=1}^{K}\\sum_{i \\in C_k}\\|x_i-\\mu_k\\|_2^2$$\n其中 $x_i = (L_i, R_i)$ 是患者 $i$ 的特征向量，$\\mu_k = (\\mu_{L,k}, \\mu_{R,k})$ 是簇 $C_k$ 的均值向量。欧几里得范数的平方可以按分量展开：\n$$J = \\sum_{k=1}^{K}\\sum_{i \\in C_k} \\left[ (L_i - \\mu_{L,k})^2 + (R_i - \\mu_{R,k})^2 \\right]$$\n这可以分解为每个特征的簇内平方和：\n$$J = WSS_L + WSS_R$$\n其中 $WSS_L = \\sum_{k=1}^{K}\\sum_{i \\in C_k} (L_i - \\mu_{L,k})^2$ 且 $WSS_R = \\sum_{k=1}^{K}\\sum_{i \\in C_k} (R_i - \\mu_{R,k})^2$。\n\n问题仅对特征 $L$ 引入了仿射压缩。患者 $i$ 的新坐标是 $x_{i,c} = (z_i, R_i)$，其中 $z_i = \\frac{c}{r}(L_i - L_{\\min})$。特征 $R_i$ 保持不变。特征 $L$ 的范围是 $r = L_{\\max} - L_{\\min}$。\n\n新的簇均值 $\\mu_{k,c}$ 在变换后的空间中计算。特征 $R$ 的均值 $\\mu_{R,k}$ 不受影响。新特征 $z$ 的均值 $\\mu_{z,k}$ 是簇 $C_k$ 中所有点的 $z_i$ 值的平均值：\n$$\\mu_{z,k} = \\frac{1}{|C_k|} \\sum_{i \\in C_k} z_i = \\frac{1}{|C_k|} \\sum_{i \\in C_k} \\frac{c}{r}(L_i - L_{\\min})$$\n根据求和与期望的线性性质，我们可以提出常数：\n$$\\mu_{z,k} = \\frac{c}{r} \\left( \\left(\\frac{1}{|C_k|} \\sum_{i \\in C_k} L_i\\right) - L_{\\min} \\right) = \\frac{c}{r}(\\mu_{L,k} - L_{\\min})$$\n因此，新的簇均值向量为 $\\mu_{k,c} = \\left(\\frac{c}{r}(\\mu_{L,k} - L_{\\min}), \\mu_{R,k}\\right)$。\n\n新的目标函数 $J_c$ 是压缩空间中欧几里得距离的平方和：\n$$J_c = \\sum_{k=1}^{K}\\sum_{i \\in C_k} \\|x_{i,c} - \\mu_{k,c}\\|_2^2 = \\sum_{k=1}^{K}\\sum_{i \\in C_k} \\left[ (z_i - \\mu_{z,k})^2 + (R_i - \\mu_{R,k})^2 \\right]$$\n让我们分析变换后特征的项：\n$$z_i - \\mu_{z,k} = \\frac{c}{r}(L_i - L_{\\min}) - \\frac{c}{r}(\\mu_{L,k} - L_{\\min}) = \\frac{c}{r}(L_i - L_{\\min} - \\mu_{L,k} + L_{\\min}) = \\frac{c}{r}(L_i - \\mu_{L,k})$$\n将此代入 $J_c$ 的表达式中：\n$$J_c = \\sum_{k=1}^{K}\\sum_{i \\in C_k} \\left[ \\left(\\frac{c}{r}(L_i - \\mu_{L,k})\\right)^2 + (R_i - \\mu_{R,k})^2 \\right]$$\n$$J_c = \\sum_{k=1}^{K}\\sum_{i \\in C_k} \\left[ \\left(\\frac{c}{r}\\right)^2 (L_i - \\mu_{L,k})^2 + (R_i - \\mu_{R,k})^2 \\right]$$\n我们现在可以分离各项的和：\n$$J_c = \\left(\\frac{c}{r}\\right)^2 \\sum_{k=1}^{K}\\sum_{i \\in C_k} (L_i - \\mu_{L,k})^2 + \\sum_{k=1}^{K}\\sum_{i \\in C_k} (R_i - \\mu_{R,k})^2$$\n识别出 $WSS_L$ 和 $WSS_R$ 的定义，我们得到变换后目标函数的最终表达式：\n$$J_c = \\left(\\frac{c}{r}\\right)^2 WSS_L + WSS_R$$\n\n**B 部分：临界压缩因子 $c^\\star$ 的计算**\n\n在这部分中，我们被给予了具体数值，并被要求找出使得患者簇分配变得模糊的临界压缩因子 $c^\\star$。\n给定的数据如下：\n-   $L_{\\min} = 0$， $L_{\\max} = 100$，所以 $r = 100$。\n-   原始簇均值：$\\mu_1 = (L_1, R_1) = (35.5, 0)$ 和 $\\mu_2 = (L_2, R_2) = (39.5, 1)$。\n-   患者数据点：$x = (L_x, R_x) = (50, 0)$。\n\n对 $L$ 的压缩变换简化为 $z = \\frac{c}{100}(L - 0) = \\frac{c}{100}L$。\n患者数据点在压缩空间中为 $x_c = (z_x, R_x) = \\left(\\frac{c}{100} \\cdot 50, 0\\right)$。\n假设压缩空间中的簇均值是原始均值在相同变换下的像：\n-   $\\mu_{1,c} = \\left(\\frac{c}{100} \\cdot 35.5, 0\\right)$\n-   $\\mu_{2,c} = \\left(\\frac{c}{100} \\cdot 39.5, 1\\right)$\n\n临界值 $c^\\star$ 定义为使得患者点 $x_c$ 到两个簇均值 $\\mu_{1,c}$ 和 $\\mu_{2,c}$ 的欧几里得距离平方相等的 $c$ 值。\n让我们计算这两个平方距离，$d_1^2$ 和 $d_2^2$。\n\n$d_1^2 = \\|x_c - \\mu_{1,c}\\|_2^2 = \\left(\\frac{c}{100} \\cdot 50 - \\frac{c}{100} \\cdot 35.5\\right)^2 + (0 - 0)^2$\n$d_1^2 = \\left(\\frac{c}{100}(50 - 35.5)\\right)^2 = \\left(\\frac{c}{100} \\cdot 14.5\\right)^2 = \\left(\\frac{c}{100}\\right)^2 (14.5)^2$\n\n$d_2^2 = \\|x_c - \\mu_{2,c}\\|_2^2 = \\left(\\frac{c}{100} \\cdot 50 - \\frac{c}{100} \\cdot 39.5\\right)^2 + (0 - 1)^2$\n$d_2^2 = \\left(\\frac{c}{100}(50 - 39.5)\\right)^2 + (-1)^2 = \\left(\\frac{c}{100} \\cdot 10.5\\right)^2 + 1 = \\left(\\frac{c}{100}\\right)^2 (10.5)^2 + 1$\n\n在临界值 $c = c^\\star$ 处，我们令 $d_1^2 = d_2^2$：\n$$\\left(\\frac{c^\\star}{100}\\right)^2 (14.5)^2 = \\left(\\frac{c^\\star}{100}\\right)^2 (10.5)^2 + 1$$\n重新整理各项以求解 $c^\\star$：\n$$\\left(\\frac{c^\\star}{100}\\right)^2 \\left( (14.5)^2 - (10.5)^2 \\right) = 1$$\n我们使用平方差公式 $a^2 - b^2 = (a-b)(a+b)$：\n$$(14.5)^2 - (10.5)^2 = (14.5 - 10.5)(14.5 + 10.5) = (4)(25) = 100$$\n将此结果代回方程中：\n$$\\left(\\frac{c^\\star}{100}\\right)^2 (100) = 1$$\n$$\\frac{(c^\\star)^2}{10000} \\cdot 100 = 1$$\n$$\\frac{(c^\\star)^2}{100} = 1$$\n$$(c^\\star)^2 = 100$$\n由于问题陈述 $c \\in (0, r]$，即 $c \\in (0, 100]$，我们取正根：\n$$c^\\star = 10$$\n\n为了验证此行为，我们首先检查原始尺度上的分配。\n在原始尺度上 ($c=r=100$)：\n$d(x, \\mu_1)^2 = (50 - 35.5)^2 + (0 - 0)^2 = (14.5)^2 = 210.25$\n$d(x, \\mu_2)^2 = (50 - 39.5)^2 + (0 - 1)^2 = (10.5)^2 + 1 = 110.25 + 1 = 111.25$\n由于 $111.25  210.25$，在原始尺度上，患者 $x$ 被分配到簇2。\n\n现在回到压缩空间：我们发现 $d_1^2 - d_2^2 = \\frac{c^2}{100} - 1$。\n-   当 $c > 10$ 时，$c^2 > 100$，所以 $\\frac{c^2}{100} - 1 > 0$，即 $d_1^2 > d_2^2$。患者被分配到簇2。这与原始分配相符。\n-   当 $c  10$ 时，$c^2  100$，所以 $\\frac{c^2}{100} - 1  0$，即 $d_1^2  d_2^2$。患者被分配到簇1。分配发生翻转。\n因此，临界值 $c^\\star=10$ 是正确的。", "answer": "$$\\boxed{10}$$", "id": "4576104"}, {"introduction": "在处理基因组学或临床面板等高维数据时，主成分分析（PCA）是进行患者分层前常用的降维方法。然而，选择保留多少主成分是一个关键的权衡，因为它需要在保留足够信息和获得清晰聚类结构之间取得平衡。本练习 [@problem_id:4576042] 引导您通过构建一个结合了轮廓系数（衡量聚类质量）和方差保留率的优化目标函数 $J(m)$，来系统地选择最佳的主成分数量，从而将模型选择过程从直觉判断转变为严谨的算法决策。", "problem": "给定一个通用的无监督患者分层流程，该流程首先应用主成分分析（PCA）来降低标准化临床特征向量的维度，然后在降维后的空间中应用K-均值聚类。核心设计问题是选择主成分的数量，以便在尽可能多地保留数据方差的同时，产生分离良好的聚类。\n\n从第一性原理出发，使用以下基础理论：\n- 主成分分析（PCA）被定义为一种正交线性变换，它对零均值数据集的经验协方差矩阵进行对角化，从而产生主方向（特征向量）以及相关的、量化了每个方向上可解释方差的非负特征值。\n- K-均值聚类的目标是最小化每个点与其所属聚类质心之间的平方欧几里得距离之和。\n- 单个样本的轮廓系数定义为其平均簇间距离与平均簇内距离之差，再用两者中的较大值进行归一化；整个数据集的轮廓系数是所有样本轮廓系数的平均值，其中单个样本组成的聚类（孤立点簇）的轮廓系数被指定为$0$。\n\n将数据集定义为一个矩阵 $X \\in \\mathbb{R}^{n \\times d}$，包含 $n$ 个患者和 $d$ 个标准化特征，其中每个特征都中心化至均值为 $0$ 并缩放至单位方差。设对 $X$ 进行PCA后得到有序的可解释方差比率 $\\{\\rho_i\\}_{i=1}^d$，满足 $\\rho_1 \\ge \\rho_2 \\ge \\cdots \\ge \\rho_d \\ge 0$ 且 $\\sum_{i=1}^d \\rho_i = 1$。对于任何满足 $1 \\le m \\le d$ 的整数 $m$，定义：\n- 累积方差保留率为 $R(m) = \\sum_{i=1}^m \\rho_i$。\n- 在 $m$ 维PCA降维空间中的轮廓系数为 $S(m)$，它是通过计算投影到前 $m$ 个主成分上的点之间的欧几里得距离，并结合在该降维空间中通过固定 $k$ 值的K-均值聚类找到的簇分配来计算的。\n\n我们寻求一种算法来选择 $m$，通过最大化一个组合目标函数\n$$\nJ(m) = S(m) - \\alpha \\,\\big(1 - R(m)\\big),\n$$\n其中 $\\alpha \\ge 0$ 是一个权衡参数，用于控制对低方差保留率的惩罚程度；当出现平局时，选择能够达到 $J(m)$ 最大值的最小的 $m$。PCA必须在标准化数据上执行，聚类分配必须通过使用欧几里得距离的K-均值算法计算，并且轮廓系数 $S(m)$ 必须使用基于欧几里得距离的标准轮廓系数定义进行计算。使用k-means++初始化以减少空聚类的概率，运行多次随机初始化，并采用确定性的随机数生成器种子以保证可复现性。如果在K-均值聚类过程中出现空聚类，则通过采样一个随机数据点来重新初始化其质心。\n\n构建并解决以下由合成但科学上合理的患者队列组成的测试套件。每个队列都是通过从具有指定均值和协方差的多元正态分布中采样特征向量，然后对每个特征进行标准化以达到零均值和单位方差而生成的。所有量，包括大小、均值和协方差，都以精确数值形式表示。所有距离计算均使用欧几里得距离。没有物理单位。不涉及角度。\n\n对于每个测试用例 $t \\in \\{1,2,3,4\\}$，生成所选的主成分数量 $m^\\star_t$（整数）。您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如，“[1,3,2,4]”）。\n\n通用生成组件：\n- 设 $w \\in \\mathbb{R}^d$ 是所有元素都等于$1$的向量，并定义一个秩为一的相关矩阵 $C = \\beta \\,\\frac{w w^\\top}{\\|w\\|_2^2}$，其中 $\\beta$ 是一个小的非负标量。\n- 设 $D = \\mathrm{diag}(\\sigma_1^2,\\ldots,\\sigma_d^2)$ 是特征方差的对角矩阵。\n- 用于采样的协方差矩阵为 $\\Sigma = D + C$，对于给定的参数，该矩阵是半正定的。\n\n采样后，连接所有聚类样本以形成 $X$，然后将 $X$ 的每个特征标准化为零均值和单位方差。接着在PCA降维空间中执行PCA和K-均值聚类，以计算 $S(m)$、$R(m)$ 和 $J(m)$，并根据所述的平局处理规则选择使 $J(m)$ 最大化的 $m^\\star$。\n\n使用以下测试套件；为每个用例提供每个聚类的均值、每个特征的方差、相关强度 $\\beta$、聚类大小、$k$、$\\alpha$、最大主成分数 $m_{\\max}$ 和用于可复现性的随机种子：\n\n- 测试用例1（$d=6$维的平衡三聚类队列）：\n    - 维度：$d = 6$，聚类数量：$k = 3$，权衡权重：$\\alpha = 0.2$，最大主成分数：$m_{\\max} = 6$，随机种子：$42$。\n    - 聚类大小：$(n_1,n_2,n_3) = (40,35,30)$，总样本数 $n = 105$。\n    - 均值：\n        - $\\mu_1 = [0,0,0,0,0,0]$,\n        - $\\mu_2 = [2.5,-2.0,0.5,1.0,-1.5,2.0]$,\n        - $\\mu_3 = [-2.0,2.5,-0.3,-1.2,1.3,-2.2]$。\n    - 各特征方差：$(\\sigma_1^2,\\sigma_2^2,\\sigma_3^2,\\sigma_4^2,\\sigma_5^2,\\sigma_6^2) = (1.0, 1.2, 0.8, 0.9, 1.1, 0.7)$。\n    - 相关强度：$\\beta = 0.1$。\n\n- 测试用例2（$d=5$维中方差由一个非信息性特征主导）：\n    - 维度：$d = 5$，聚类数量：$k = 3$，权衡权重：$\\alpha = 0.8$，最大主成分数：$m_{\\max} = 5$，随机种子：$123$。\n    - 聚类大小：$(n_1,n_2,n_3) = (50,50,50)$，总样本数 $n = 150$。\n    - 均值：\n        - $\\mu_1 = [0,0,0,0,0]$,\n        - $\\mu_2 = [0,0.5,-0.5,0.4,-0.3]$,\n        - $\\mu_3 = [0,-0.5,0.5,-0.4,0.3]$。\n    - 各特征方差：$(\\sigma_1^2,\\sigma_2^2,\\sigma_3^2,\\sigma_4^2,\\sigma_5^2) = (10.0, 0.3, 0.3, 0.3, 0.3)$。\n    - 相关强度：$\\beta = 0.05$。\n\n- 测试用例3（$d=4$维中沿一个轴清晰分离）：\n    - 维度：$d = 4$，聚类数量：$k = 3$，权衡权重：$\\alpha = 0.0$，最大主成分数：$m_{\\max} = 4$，随机种子：$7$。\n    - 聚类大小：$(n_1,n_2,n_3) = (60,60,60)$，总样本数 $n = 180$。\n    - 均值：\n        - $\\mu_1 = [-3,0,0,0]$,\n        - $\\mu_2 = [0,0,0,0]$,\n        - $\\mu_3 = [3,0,0,0]$。\n    - 各特征方差：$(\\sigma_1^2,\\sigma_2^2,\\sigma_3^2,\\sigma_4^2) = (0.5, 0.2, 0.2, 0.2)$。\n    - 相关强度：$\\beta = 0.01$。\n\n- 测试用例4（$d=6$维的四聚类队列）：\n    - 维度：$d = 6$，聚类数量：$k = 4$，权衡权重：$\\alpha = 0.4$，最大主成分数：$m_{\\max} = 6$，随机种子：$99$。\n    - 聚类大小：$(n_1,n_2,n_3,n_4) = (30,30,30,30)$，总样本数 $n = 120$。\n    - 均值：\n        - $\\mu_1 = [2,2,0,0,0,0]$,\n        - $\\mu_2 = [-2,2,0,0,0,0]$,\n        - $\\mu_3 = [-2,-2,0,0,0,0]$,\n        - $\\mu_4 = [2,-2,0,0,0,0]$。\n    - 各特征方差：$(\\sigma_1^2,\\sigma_2^2,\\sigma_3^2,\\sigma_4^2,\\sigma_5^2,\\sigma_6^2) = (1.5, 1.0, 1.0, 0.8, 0.6, 1.2)$。\n    - 相关强度：$\\beta = 0.05$。\n\n算法要求：\n- 在进行PCA之前，将$X$的每个特征标准化为零均值和单位方差。\n- 通过正交分解计算PCA，得到主成分和可解释方差比率 $\\rho_i$。\n- 对于每个候选 $m \\in \\{1,\\ldots,m_{\\max}\\}$，将 $X$ 投影到前 $m$ 个主成分上，使用欧几里得距离和k-means++ 初始化并进行多次随机重启来运行K-均值聚类，计算该降维空间中的轮廓系数 $S(m)$，从PCA谱中计算 $R(m)$，并评估 $J(m) = S(m) - \\alpha(1 - R(m))$。\n- 选择使 $J(m)$ 最大化的 $m^\\star$；如果在数值容差 $\\varepsilon = 10^{-9}$ 内有多个 $m$ 得到相同的 $J(m)$，则选择达到该最大值的最小的 $m$。\n\n您的程序应生成单行输出，其中包含四个测试用例所选的主成分数量，以逗号分隔列表的形式并用方括号括起来（例如，“[2,3,1,3]”）。不应打印任何其他文本。所有计算都必须在给定种子和参数下是确定性的。四个测试用例的最终输出必须均为整数。", "solution": "我们推导出一个可实现的方法，用于选择主成分的数量。该方法通过平衡聚类可分性（由轮廓系数度量）和方差保留率（由累积可解释方差度量）来实现。\n\n起点与定义。考虑一个标准化的数据集矩阵 $X \\in \\mathbb{R}^{n \\times d}$，意味着每列的均值为 $0$、单位方差为 $1$。对于零均值的 $X$，主成分分析（PCA）由经验协方差矩阵 $\\Sigma_X = \\frac{1}{n-1} X^\\top X$ 的特征分解定义。设 $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_d \\ge 0$ 为有序的特征值，$v_1,\\ldots,v_d$ 为对应的标准正交特征向量。主成分是 $X$ 在方向 $v_i$ 上的投影。第 $i$ 个成分的可解释方差比率为 $\\rho_i = \\frac{\\lambda_i}{\\sum_{j=1}^d \\lambda_j}$，因此，前 $m$ 个成分的累积方差保留率为 $R(m) = \\sum_{i=1}^m \\rho_i$。\n\n聚类目标。K-均值方法旨在通过最小化簇内平方欧几里得距离之和，将数据点划分为 $k$ 个聚类。对于一组质心 $\\{c_j\\}_{j=1}^k$，目标函数为\n$$\n\\Phi = \\sum_{i=1}^n \\left\\| x_i - c_{\\ell(i)} \\right\\|_2^2,\n$$\n其中 $\\ell(i)$ 将点 $x_i$ 映射到其所属的聚类。该算法在分配步骤和质心更新步骤之间交替进行。一种被称为k-means++的、有良好理论基础的初始化方法通过与距离成比例的采样方案来初始化质心，以改善收敛性并避免空聚类。如果出现空聚类，一个实际的解决方法是将其质心重新初始化为一个随机选择的数据点。\n\n通过轮廓系数衡量聚类可分性。对于在某个度量空间中的给定聚类，点 $i$ 的轮廓系数为 $s_i = \\frac{b_i - a_i}{\\max(a_i, b_i)}$，其中 $a_i$ 是点 $i$ 到其自身聚类中所有其他点的平均距离（不包括其自身），而 $b_i$ 是点 $i$ 到任何其他聚类中点的最小平均距离。对于只包含单个样本的聚类，我们设定 $s_i = 0$。整个数据集的轮廓系数是 $S = \\frac{1}{n}\\sum_{i=1}^n s_i$。轮廓系数值在 $[-1,1]$ 之间，值越大表示分离效果越好。\n\n组合选择准则。为了在PCA降维空间中平衡方差保留与聚类可分性，为每个候选的主成分数量 $m$ 定义\n$$\nJ(m) = S(m) - \\alpha \\left(1 - R(m)\\right),\n$$\n其中 $S(m)$ 是在 $m$ 维PCA空间中应用K-均值聚类后的轮廓系数，$R(m)$ 是前 $m$ 个主成分的累积可解释方差。参数 $\\alpha \\ge 0$ 控制对损失方差的惩罚权重。这种线性标量化是组合多个目标的标准方法，并产生一个单一的标量 $J(m)$，以便在 $m \\in \\{1,\\ldots,m_{\\max}\\}$ 上进行优化。\n\n算法步骤。完整的流程如下：\n1. 将 $X$ 的每个特征标准化为零均值和单位方差，以确保PCA不会因特征尺度不同而给予不成比例的权重。\n2. 通过正交分解（例如，使用奇异值分解(SVD)）计算PCA。对于零均值标准化的 $X$，SVD分解 $X = U \\Sigma V^\\top$ 产生的右奇异向量 $V$ 即为主轴，奇异值的平方与协方差矩阵的特征值成正比。可解释方差比率为 $\\rho_i = \\frac{\\sigma_i^2}{\\sum_{j=1}^d \\sigma_j^2}$，且 $R(m) = \\sum_{i=1}^m \\rho_i$。\n3. 对于从 $1$ 到 $m_{\\max}$ 的每个候选 $m$：\n   - 将 $X$ 投影到前 $m$ 个主轴上，得到 $X^{(m)} = X V_m$，其中 $V_m$ 由 $V^\\top$ 的前 $m$ 行组成（等价于 $V$ 的前 $m$ 列）。\n   - 使用欧几里得距离、k-means++ 初始化、多次随机重启和收敛性检查，对 $X^{(m)}$ 进行K-均值聚类。通过将空聚类的质心重新分配给随机点来处理任何空聚类。\n   - 在 $X^{(m)}$ 和找到的聚类上计算轮廓系数 $S(m)$。\n   - 从PCA谱计算 $R(m)$。\n   - 计算 $J(m) = S(m) - \\alpha(1 - R(m))$。\n4. 选择 $m^\\star = \\arg\\max_{m \\in \\{1,\\ldots,m_{\\max}\\}} J(m)$。在数值容差 $\\varepsilon = 10^{-9}$ 内，通过选择达到 $J(m)$ 最大值的最小 $m$ 来打破平局。\n\n科学真实性与测试套件设计。合成的队列是从具有指定均值和协方差的多元正态分布中采样的，然后进行标准化，以模拟生物信息学和医学数据分析中常见的特征（例如，生物标志物面板测量值）。每个测试用例中的协方差构造为 $\\Sigma = D + C$，其中 $D$ 是对角矩阵，包含各特征的方差，$C$ 是一个小的秩一相关项 $C = \\beta \\,\\frac{w w^\\top}{\\|w\\|_2^2}$，以引入轻微的相关性，确保半正定性。这种构造捕捉了现实中的特征交互，而不会出现病态行为。四个测试用例涵盖了：\n- 一个在 $d=6$ 维中具有中等特征间相关性的典型三聚类场景，测试通用的选择行为。\n- 一个单一特征主导方差但对聚类无信息的案例，测试权衡参数 $\\alpha$ 以及该方法是否能避免仅对方差的过分强调。\n- 一个在 $\\alpha = 0$ 时沿单轴清晰分离的案例，测试该方法是否会简化为最大化轮廓系数，并在平局时倾向于选择足够小的 $m$。\n- 一个在 $d=6$ 维中具有中等噪声的四聚类案例，测试当聚类数量增加且PCA谱可能需要更多成分时的行为。\n\n可复现性的实现细节。为每个测试用例的数据生成和K-均值重启使用确定性的随机种子。按如下方式实现k-means++ 种子选择：从数据点中均匀随机选择第一个质心；对于后续每个质心，根据其与已选质心最近距离的平方成比例的概率来采样一个点。为实现收敛，迭代分配和质心更新步骤，直到达到最大迭代次数或质心位移低于容差为止。使用通过内积和范数计算的成对距离矩阵来计算基于欧几里得距离的轮廓系数，以保持数值稳定性。为单个样本组成的聚类分配轮廓系数 $0$。在处理 $J(m)$ 的平局时使用 $\\varepsilon = 10^{-9}$。\n\n输出。对于四个指定的测试用例中的每一个，输出所选的主成分数量 $m^\\star$（整数）。将这四个整数聚合成一个列表，并以“[m1,m2,m3,m4]”的格式单行打印。不应产生其他输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef standardize(X: np.ndarray) - np.ndarray:\n    \"\"\"Standardize columns of X to zero mean and unit variance.\"\"\"\n    X = X.astype(float)\n    mean = X.mean(axis=0)\n    X_centered = X - mean\n    std = X_centered.std(axis=0, ddof=1)\n    std[std == 0] = 1.0  # Avoid division by zero for constant features\n    return X_centered / std\n\ndef pca_svd(X: np.ndarray):\n    \"\"\"Compute PCA via SVD, returning components (V), singular values, and explained variance ratios.\"\"\"\n    # X is assumed standardized and zero-mean per feature\n    U, s, Vt = np.linalg.svd(X, full_matrices=False)\n    # Explained variance ratios from singular values s: s^2 proportional to eigenvalues of covariance\n    s2 = s**2\n    total = s2.sum()\n    if total == 0:\n        # Degenerate case; return zeros\n        evr = np.zeros(Vt.shape[0])\n    else:\n        evr = s2 / total\n    # Components as rows of Vt (each row is a principal axis)\n    return Vt, s, evr\n\ndef kmeans_pp_init(X: np.ndarray, k: int, rng: np.random.Generator) - np.ndarray:\n    \"\"\"Initialize k-means centroids using k-means++.\"\"\"\n    n_samples = X.shape[0]\n    centroids = np.empty((k, X.shape[1]), dtype=float)\n    # Choose first centroid uniformly\n    first_idx = rng.integers(0, n_samples)\n    centroids[0] = X[first_idx]\n    # Choose remaining centroids\n    # Precompute squared distances to current centroids\n    for i in range(1, k):\n        # Compute squared distances to nearest centroid\n        d2 = np.min(squared_distances_to_centroids(X, centroids[:i]), axis=1)\n        # Probability proportional to d2\n        total = d2.sum()\n        if total == 0:\n            # All points identical to centroids; pick random\n            next_idx = rng.integers(0, n_samples)\n        else:\n            probs = d2 / total\n            next_idx = rng.choice(n_samples, p=probs)\n        centroids[i] = X[next_idx]\n    return centroids\n\ndef squared_distances_to_centroids(X: np.ndarray, centroids: np.ndarray) - np.ndarray:\n    \"\"\"Compute squared distances from each point in X to each centroid.\"\"\"\n    # Use broadcasting: ||x - c||^2 = ||x||^2 + ||c||^2 - 2 x.c\n    x_norm2 = np.sum(X**2, axis=1)[:, None]  # (n,1)\n    c_norm2 = np.sum(centroids**2, axis=1)[None, :]  # (1,k)\n    cross = X @ centroids.T  # (n,k)\n    d2 = x_norm2 + c_norm2 - 2.0 * cross\n    # Numerical guard\n    d2 = np.maximum(d2, 0.0)\n    return d2\n\ndef run_kmeans(X: np.ndarray, k: int, rng: np.random.Generator, n_init: int = 10, max_iter: int = 100, tol: float = 1e-6):\n    \"\"\"Run k-means with k-means++ initialization and return best labels (lowest inertia).\"\"\"\n    best_inertia = np.inf\n    best_labels = None\n    n_samples, dim = X.shape\n    for _ in range(n_init):\n        centroids = kmeans_pp_init(X, k, rng)\n        labels = np.zeros(n_samples, dtype=int)\n        for _it in range(max_iter):\n            d2 = squared_distances_to_centroids(X, centroids)  # (n,k)\n            labels = np.argmin(d2, axis=1)\n            # Handle empty clusters: reinitialize empty centroid to random data point\n            for c in range(k):\n                if np.sum(labels == c) == 0:\n                    # reassign centroid\n                    centroids[c] = X[rng.integers(0, n_samples)]\n                    # recompute distances and labels after fix\n                    d2 = squared_distances_to_centroids(X, centroids)\n                    labels = np.argmin(d2, axis=1)\n            new_centroids = np.zeros_like(centroids)\n            for c in range(k):\n                cluster_points = X[labels == c]\n                if cluster_points.shape[0]  0:\n                    new_centroids[c] = cluster_points.mean(axis=0)\n                else:\n                    # Shouldn't happen due to fix; safeguard\n                    new_centroids[c] = X[rng.integers(0, n_samples)]\n            shift = np.linalg.norm(new_centroids - centroids)\n            centroids = new_centroids\n            if shift  tol:\n                break\n        # Compute inertia\n        d2_final = squared_distances_to_centroids(X, centroids)\n        inertia = np.sum(d2_final[np.arange(n_samples), labels])\n        if inertia  best_inertia:\n            best_inertia = inertia\n            best_labels = labels.copy()\n    return best_labels\n\ndef pairwise_distances(X: np.ndarray) - np.ndarray:\n    \"\"\"Compute full pairwise Euclidean distance matrix.\"\"\"\n    norms = np.sum(X**2, axis=1)\n    d2 = norms[:, None] + norms[None, :] - 2.0 * (X @ X.T)\n    np.maximum(d2, 0.0, out=d2)\n    D = np.sqrt(d2, dtype=float)\n    return D\n\ndef silhouette_score(X: np.ndarray, labels: np.ndarray) - float:\n    \"\"\"Compute mean silhouette score for clustering labels on data X.\"\"\"\n    n = X.shape[0]\n    D = pairwise_distances(X)\n    unique_labels = np.unique(labels)\n    # Precompute indices per cluster\n    cluster_indices = {c: np.where(labels == c)[0] for c in unique_labels}\n    s_vals = np.zeros(n, dtype=float)\n    for i in range(n):\n        c_i = labels[i]\n        idx_same = cluster_indices[c_i]\n        # Intra-cluster distance a_i\n        if idx_same.size = 1:\n            a_i = 0.0\n            s_vals[i] = 0.0\n            continue\n        # Exclude self\n        mask = idx_same[idx_same != i]\n        if mask.size == 0:\n            a_i = 0.0\n        else:\n            a_i = D[i, mask].mean()\n        # Inter-cluster distance b_i: minimum average distance to points in other clusters\n        b_i = np.inf\n        for c in unique_labels:\n            if c == c_i:\n                continue\n            idx_other = cluster_indices[c]\n            if idx_other.size == 0:\n                continue\n            b_i = min(b_i, D[i, idx_other].mean())\n        if b_i == np.inf:\n            # Degenerate: all points in one cluster - silhouette 0\n            s_vals[i] = 0.0\n        else:\n            denom = max(a_i, b_i)\n            if denom == 0.0:\n                s_vals[i] = 0.0\n            else:\n                s_vals[i] = (b_i - a_i) / denom\n    return float(np.mean(s_vals))\n\ndef select_num_components(X: np.ndarray, k: int, alpha: float, m_max: int, seed: int) - int:\n    \"\"\"Select number of components by maximizing J(m) = S(m) - alpha*(1 - R(m)).\"\"\"\n    rng = np.random.default_rng(seed)\n    X_std = standardize(X)\n    Vt, s, evr = pca_svd(X_std)\n    # Project and evaluate for m = 1..m_max\n    m_max = min(m_max, X_std.shape[1])\n    best_m = 1\n    best_J = -np.inf\n    eps = 1e-9\n    for m in range(1, m_max + 1):\n        components = Vt[:m, :]  # (m, d)\n        X_proj = X_std @ components.T  # (n, m)\n        labels = run_kmeans(X_proj, k, rng, n_init=10, max_iter=100, tol=1e-6)\n        S_m = silhouette_score(X_proj, labels)\n        R_m = float(np.sum(evr[:m]))\n        J_m = S_m - alpha * (1.0 - R_m)\n        if (J_m  best_J + eps) or (abs(J_m - best_J) = eps and m  best_m):\n            best_J = J_m\n            best_m = m\n    return int(best_m)\n\ndef generate_covariance(cov_diag: np.ndarray, corr_strength: float) - np.ndarray:\n    \"\"\"Construct covariance as diag(cov_diag) + corr_strength * (w w^T / ||w||^2).\"\"\"\n    d = cov_diag.shape[0]\n    D = np.diag(cov_diag)\n    w = np.ones(d, dtype=float)\n    outer = np.outer(w, w) / float(d)  # ||w||^2 = d\n    C = corr_strength * outer\n    Sigma = D + C\n    return Sigma\n\ndef sample_clusters(means: list, cov_diag: np.ndarray, corr_strength: float, sizes: list, seed: int) - np.ndarray:\n    \"\"\"Sample data from specified Gaussian clusters and concatenate.\"\"\"\n    Sigma = generate_covariance(cov_diag, corr_strength)\n    rng = np.random.default_rng(seed)\n    X_list = []\n    for mu, n in zip(means, sizes):\n        X_list.append(rng.multivariate_normal(mean=np.array(mu, dtype=float), cov=Sigma, size=int(n)))\n    X = np.vstack(X_list)\n    return X\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1\n        {\n            \"means\": [\n                [0, 0, 0, 0, 0, 0],\n                [2.5, -2.0, 0.5, 1.0, -1.5, 2.0],\n                [-2.0, 2.5, -0.3, -1.2, 1.3, -2.2],\n            ],\n            \"cov_diag\": np.array([1.0, 1.2, 0.8, 0.9, 1.1, 0.7], dtype=float),\n            \"corr_strength\": 0.1,\n            \"sizes\": [40, 35, 30],\n            \"k\": 3,\n            \"alpha\": 0.2,\n            \"m_max\": 6,\n            \"seed\": 42,\n        },\n        # Test Case 2\n        {\n            \"means\": [\n                [0, 0, 0, 0, 0],\n                [0, 0.5, -0.5, 0.4, -0.3],\n                [0, -0.5, 0.5, -0.4, 0.3],\n            ],\n            \"cov_diag\": np.array([10.0, 0.3, 0.3, 0.3, 0.3], dtype=float),\n            \"corr_strength\": 0.05,\n            \"sizes\": [50, 50, 50],\n            \"k\": 3,\n            \"alpha\": 0.8,\n            \"m_max\": 5,\n            \"seed\": 123,\n        },\n        # Test Case 3\n        {\n            \"means\": [\n                [-3, 0, 0, 0],\n                [0, 0, 0, 0],\n                [3, 0, 0, 0],\n            ],\n            \"cov_diag\": np.array([0.5, 0.2, 0.2, 0.2], dtype=float),\n            \"corr_strength\": 0.01,\n            \"sizes\": [60, 60, 60],\n            \"k\": 3,\n            \"alpha\": 0.0,\n            \"m_max\": 4,\n            \"seed\": 7,\n        },\n        # Test Case 4\n        {\n            \"means\": [\n                [2, 2, 0, 0, 0, 0],\n                [-2, 2, 0, 0, 0, 0],\n                [-2, -2, 0, 0, 0, 0],\n                [2, -2, 0, 0, 0, 0],\n            ],\n            \"cov_diag\": np.array([1.5, 1.0, 1.0, 0.8, 0.6, 1.2], dtype=float),\n            \"corr_strength\": 0.05,\n            \"sizes\": [30, 30, 30, 30],\n            \"k\": 4,\n            \"alpha\": 0.4,\n            \"m_max\": 6,\n            \"seed\": 99,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        X = sample_clusters(\n            means=case[\"means\"],\n            cov_diag=case[\"cov_diag\"],\n            corr_strength=case[\"corr_strength\"],\n            sizes=case[\"sizes\"],\n            seed=case[\"seed\"],\n        )\n        m_star = select_num_components(\n            X=X,\n            k=case[\"k\"],\n            alpha=case[\"alpha\"],\n            m_max=case[\"m_max\"],\n            seed=case[\"seed\"],\n        )\n        results.append(m_star)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4576042"}, {"introduction": "真实世界的临床数据常常存在缺失值，而处理不当可能导致严重的分析偏差。本练习 [@problem_id:4576069] 模拟了一个高级且现实的挑战：非随机缺失（MNAR）的数据。您将通过从头开始构建一个模拟研究，来揭示看似无害的均值插补方法如何在特定条件下产生完全虚假的患者亚群，并学会如何设计敏感性分析来检测这类由数据预处理引入的人为结果。", "problem": "您被要求形式化并实现一项基于模拟的研究，探讨当应用K-均值聚类进行患者分层时，一个关键临床实验室变量中的非随机缺失（MNAR）如何在朴素插补下导致伪聚类。您的程序必须是完全确定性的，并且必须仅使用基本数值运算从第一性原理实现所有算法。\n\n从以下基础和定义开始：\n\n1. 聚类目标。对于数据矩阵 $X \\in \\mathbb{R}^{n \\times d}$ 和簇中心 $C = \\{c_{1},\\dots,c_{k}\\}$，以及标签 $\\ell_{i} \\in \\{1,\\dots,k\\}$，K-均值聚类的目标是最小化簇内平方和\n$$\n\\sum_{i=1}^{n} \\left\\| x_{i} - c_{\\ell_{i}} \\right\\|_{2}^{2}.\n$$\n一个标准算法会交替地对标签和簇中心进行最小化。\n\n2. 内部验证。对于一个标签为 $\\ell_{i}$ 的点 $i$，其轮廓系数为\n$$\ns_{i} = \\frac{b_{i} - a_{i}}{\\max\\{a_{i}, b_{i}\\}},\n$$\n其中 $a_{i}$ 是点 $i$ 到其所在簇中所有其他点的平均距离，而 $b_{i}$ 是点 $i$ 到任何其他簇中点的最小平均距离。整体轮廓系数是所有 $s_{i}$（其中 $i \\in \\{1,\\dots,n\\}$）的算术平均值。使用欧几里得距离。\n\n3. 缺失机制。令 $X = (X_{1}, X_{2}, X_{3})$ 表示三个临床测量值，其中 $X_{1}$ 是一个关键实验室变量。令 $R$ 表示 $X_{1}$ 的缺失指示符，如果观测到则 $R=1$，如果缺失则 $R=0$。完全随机缺失（MCAR）意味着 $\\mathbb{P}(R=1 \\mid X)$ 是一个常数；非随机缺失（MNAR）意味着 $\\mathbb{P}(R=1 \\mid X)$ 依赖于未观测到的值，例如，直接依赖于 $X_{1}$。\n\n4. 朴素插补。当 $X_{1}$ 缺失时（$R=0$），朴素均值插补会用观测到的 $X_{1}$ 值的经验均值来替换它。\n\n5. 对关键实验室变量依赖性的敏感性。一个简单的敏感性分析是在排除关键实验室特征 $X_{1}$ 后重新计算聚类有效性，从而检验表观的聚类结构是否是由 $X_{1}$ 中插补产生的伪影所驱动。\n\n仅根据这些基本定义需要实现的任务：\n\nA. 合成患者生成器。对于下面的每个测试用例，生成 $n$ 个患者，其真实潜在分层为 $Y \\in \\{0,1\\}$（等概率），并具有坐标独立的条件高斯特征 $X \\in \\mathbb{R}^{3}$。使用两个分层，其均值主要在关键实验室变量 $X_{1}$ 上有所不同，从而使疾病信号集中在 $X_{1}$ 中。给定 $Y=y$ 时 $X$ 的条件分布为多元正态分布，其均值向量为 $\\mu^{(y)}$，协方差为对角阵，标准差如下文指定。\n\nB. 缺失。对于关键实验室变量 $X_{1}$，根据每个测试用例中指定的机制引入缺失：\n- 对于参数为 $q \\in [0,1]$ 的 MCAR，设置 $\\mathbb{P}(R=1 \\mid X) = 1 - q$，且独立于 $X$。\n- 对于参数为 $\\alpha \\in \\mathbb{R}$ 和 $\\tau \\in \\mathbb{R}$ 的 MNAR，设置\n$$\n\\mathbb{P}(R=1 \\mid X_{1}=x) = 1 - \\sigma\\left(\\alpha (x - \\tau)\\right), \\quad \\sigma(z) = \\frac{1}{1 + e^{-z}}.\n$$\n当 $\\alpha  0$ 且 $\\tau$ 是一个阈值时，这一选择使得较高的 $x$ 值更不容易被观测到。\n\nC. 插补。计算所有 $R=1$ 的患者中观测到的 $X_{1}$ 的经验均值 $\\bar{x}_{1,\\text{obs}}$，并为所有 $R=0$ 的患者设置 $X_{1} \\leftarrow \\bar{x}_{1,\\text{obs}}$（保持 $X_{2}$ 和 $X_{3}$ 不变，因为它们没有缺失值）。\n\nD. 聚类和内部验证。对于每个数据集，使用K-均值聚类（具有固定次数的随机初始化和迭代）计算 $k \\in \\{2,3\\}$ 时的轮廓系数。使用欧几里得距离，并通过最小化簇内平方和来选择最佳的运行结果。为以下情况计算轮廓系数：\n- 插补后的完整特征集 $(X_{1}, X_{2}, X_{3})$。\n- 完全排除 $X_{1}$ 后得到的简化特征集 $(X_{2}, X_{3})$。\n\nE. 伪影检测规则。当且仅当以下两个条件同时成立时，声明检测到一个“由朴素插补引起的伪聚类”：\n- 使用完整特征时，$k=3$ 的轮廓系数超过 $k=2$ 的轮廓系数，即 $S_{\\text{full}}(3)  S_{\\text{full}}(2)$，表明倾向于三个簇。\n- 排除 $X_{1}$ 后，轮廓系数不再倾向于三个簇，即 $S_{\\setminus X_{1}}(3) \\le S_{\\setminus X_{1}}(2)$。\n每个测试用例返回一个布尔值，指示是否检测到此伪影。\n\n必须遵循的实现和数值细节：\n- 对所有随机性使用固定的随机种子 $42$。\n- 使用K-均值聚类，其中 $k \\in \\{2,3\\}$，$n_{\\text{init}} = 20$ 次随机初始化，每次初始化最多进行 $100$ 次 Lloyd 迭代。\n- 对于轮廓系数计算，如果一个点是其簇的唯一成员，则将其轮廓系数贡献定义为 $0$。\n\n测试套件。您的程序必须按顺序准确运行以下三个测试用例，并为每个用例输出一个布尔结果：\n\n- 测试用例 $1$（MNAR，预期有强伪影）：$n=600$ 名患者；维度 $d=3$ 个特征；各分层均值\n$$\n\\mu^{(0)} = (0, 0, 0), \\quad \\mu^{(1)} = (5, 0, 0),\n$$\n两个分层的标准差均为 $(0.7, 0.6, 0.6)$；在 $X_{1}$ 上存在 MNAR 缺失，参数为 $\\alpha = 1.2$，$\\tau = 4.0$。\n\n- 测试用例 $2$（MCAR，预期无伪影）：与测试用例 $1$ 具有相同的数据生成参数，但在 $X_{1}$ 上为 MCAR 缺失，参数 $q = 0.05$。\n\n- 测试用例 $3$（无缺失，基线）：与测试用例 $1$ 具有相同的数据生成参数，但在 $X_{1}$ 上无缺失，即 $q = 0$。\n\n要求的最终输出格式。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔的结果列表（例如，\"[true_case1,true_case2,true_case3]\"），使用首字母大写的 Python 布尔字面量，按测试用例的顺序排列，且不含多余空格。具体来说，打印如下形式的单行：\n$$\n[\\text{result}_{1},\\text{result}_{2},\\text{result}_{3}],\n$$\n其中每个 $\\text{result}_{j}$ 为 True 或 False。", "solution": "该问题要求实现一项模拟研究，以证明非随机缺失（MNAR）数据机制与朴素均值插补相结合，在使用K-均值聚类进行分析时如何引入伪聚类。该问题陈述的有效性得到了确认，因为它在科学上基于已建立的统计学原理，问题定义清晰，指令具体、确定，并且表述客观。所有算法和指标都是数据分析和生物信息学领域的标准方法。\n\n需要证明的核心原理是，特定的缺失模式与简单的插补方法之间的相互作用，会在数据中产生人为的结构，而聚类算法可能会错误地将其识别为有意义的生物亚群。本模拟旨在受控条件下检验这一假设。\n\n解决方案将通过按规定实现一系列组件来构建，从数据生成开始，到最终的伪影检测。所有数学实体都按要求使用 LaTeX 渲染。\n\n**A. 合成患者数据生成**\n\n首先，我们生成一个包含 $n$ 名患者的合成数据集。每位患者以等概率属于两个真实潜在分层 $Y \\in \\{0, 1\\}$ 中的一个，即 $\\mathbb{P}(Y=0) = \\mathbb{P}(Y=1) = 0.5$。每位患者的数据包含 $d=3$ 个特征，$X = (X_1, X_2, X_3)$，在给定分层 $Y$ 的条件下，这些特征是条件独立的。对于分层为 $y$ 的患者，$X$ 的条件分布是一个多元正态分布，$X | (Y=y) \\sim \\mathcal{N}(\\mu^{(y)}, \\Sigma)$，其中协方差矩阵 $\\Sigma$ 是对角阵。均值向量给定为 $\\mu^{(0)} = (0, 0, 0)$ 和 $\\mu^{(1)} = (5, 0, 0)$。特征的标准差在两个分层中均为 $(0.7, 0.6, 0.6)$。这种设置确保了区分两个分层的真实潜在生物信号完全集中在关键实验室特征 $X_1$ 中。对于所有模拟，使用固定的随机种子 42 以确保可复现性。\n\n**B. 缺失引入**\n\n仅在关键特征 $X_1$ 中引入缺失。我们对两种不同的机制进行建模：\n\n1.  **完全随机缺失（MCAR）**：$X_1$ 的值被观测到的概率是恒定的，并且独立于任何数据值。令 $R_i=1$ 表示第 $i$ 位患者的 $X_1$ 被观测到，$R_i=0$ 表示其缺失。对于参数为 $q$ 的 MCAR，我们为所有 $i$ 设置 $\\mathbb{P}(R_i=1) = 1-q$。\n\n2.  **非随机缺失（MNAR）**：缺失的概率取决于 $X_1$ 本身的值。指定的模型是：\n    $$\n    \\mathbb{P}(R_i=1 \\mid X_{1,i}=x) = 1 - \\sigma\\left(\\alpha (x - \\tau)\\right)\n    $$\n    其中 $\\sigma(z) = (1 + e^{-z})^{-1}$ 是标准逻辑（sigmoid）函数。在此模型中，如果参数 $\\alpha  0$，显著大于阈值 $\\tau$ 的 $x$ 值将有很高的概率缺失，因为 $\\sigma(\\cdot)$ 将趋近于 $1$。这模拟了一种可能的临床情景，例如，极高的实验室值可能需要重新检测或因仪器故障，导致它们在初始数据提取中缺失。\n\n**C. 朴素均值插补**\n\n$X_1$ 中的缺失值使用朴素均值插补进行处理。首先，计算所有*已观测*到的 $X_1$ 值的经验均值：\n$$\n\\bar{x}_{1,\\text{obs}} = \\frac{\\sum_{i=1}^{n} X_{1,i} \\cdot \\mathbb{I}(R_i=1)}{\\sum_{i=1}^{n} \\mathbb{I}(R_i=1)}\n$$\n其中 $\\mathbb{I}(\\cdot)$ 是指示函数。然后，$X_1$ 中的每个缺失条目（其中 $R_i=0$）都被替换为这个单一值 $\\bar{x}_{1,\\text{obs}}$。当缺失模式为 MNAR 且高值优先缺失时，与缺失值所在分层的真实均值相比，$\\bar{x}_{1,\\text{obs}}$ 将会有系统性的向下偏差。这个过程在 $X_1$ 特征上，于 $\\bar{x}_{1,\\text{obs}}$ 这一数值点人为地造成了数据点的集中。\n\n**D. 聚类与内部验证**\n\n分析的核心是应用K-均值聚类算法并使用轮廓系数评估结果。\n\n**K-均值算法**（Lloyd 算法）是一个将 $n$ 个数据点划分到 $k$ 个簇中的迭代过程。其目标是最小化簇内平方和（WCSS）：\n$$\n\\text{WCSS} = \\sum_{j=1}^{k} \\sum_{x_i \\in S_j} \\| x_i - c_j \\|_2^2\n$$\n其中 $S_j$ 是簇 $j$ 中的点集，$c_j$ 是 $S_j$ 中点的质心（均值）。该算法在将每个点分配给其最近的质心和重新计算每个质心作为其分配点的均值之间交替进行。为减轻对初始条件的敏感性，该过程使用不同的随机初始质心重复 $n_{\\text{init}} = 20$ 次，并选择产生最小 WCSS 的划分。每次初始化最多执行 100 次迭代。\n\n**轮廓系数**提供了一种衡量所生成簇的分离程度的指标。对于每个点 $i$，轮廓值 $s_i$ 定义为：\n$$\ns_i = \\frac{b_i - a_i}{\\max\\{a_i, b_i\\}}\n$$\n这里，$a_i$ 是点 $i$ 到同一簇内所有其他点的平均欧几里得距离。$b_i$ 是点 $i$ 到任何*其他*簇中所有点的最小平均欧几里得距离。给定聚类的总体轮廓分数是所有 $s_i$ 的平均值。接近 1 的分数表示簇分离良好，而接近 0 的分数则表示簇之间存在重叠。如果一个簇只包含一个点，其轮廓系数值定义为 0。\n\n该分析在两个数据集上针对 $k=2$ 和 $k=3$ 进行：\n1.  完整的、经过插补的、包含特征 $(X_1, X_2, X_3)$ 的数据集。\n2.  简化的、完全排除了关键实验室特征 $X_1$、包含特征 $(X_2, X_3)$ 的数据集。\n\n**E. 伪聚类伪影检测**\n\n核心假设是 MNAR 机制与均值插补相结合，会产生一个伪第三簇，该簇由其高 $X_1$ 值被插补均值所替代的患者组成。这导致轮廓系数错误地倾向于 $k=3$ 的解，而不是真实的 $k=2$ 解。如果同时满足两个条件，则正式检测到伪影：\n\n1.  对于完整的、经过插补的数据集，其 $k=3$ 的轮廓系数高于 $k=2$ 的轮廓系数：$S_{\\text{full}}(3)  S_{\\text{full}}(2)$。\n2.  移除驱动伪影的特征 $X_1$ 后，对 $k=3$ 的偏好消失。剩余的数据 $(X_2, X_3)$ 没有真实结构（因为两个真实分层在这些特征上的均值都为 0），因此不应偏好 $k=3$ 的解：$S_{\\setminus X_{1}}(3) \\le S_{\\setminus X_{1}}(2)$。\n\n这条由两部分组成的规则构成了一项敏感性分析。该规则的阳性结果表明聚类结构不稳定，并且很可能是数据处理步骤产生的伪影。该程序将应用于指定的三个测试用例中的每一个。", "answer": "```python\nimport numpy as np\nfrom scipy.special import expit as sigmoid\n\n# Use a fixed random number generator for reproducibility as per problem specification.\nRNG = np.random.default_rng(42)\n\ndef generate_data(n, mu0, mu1, stds):\n    \"\"\"\n    Generates synthetic patient data with two latent strata.\n    \"\"\"\n    true_labels = RNG.integers(0, 2, size=n)\n    X = np.zeros((n, 3))\n    \n    mask0 = (true_labels == 0)\n    n0 = np.sum(mask0)\n    X[mask0, :] = RNG.normal(loc=mu0, scale=stds, size=(n0, 3))\n    \n    mask1 = (true_labels == 1)\n    n1 = n - n0\n    X[mask1, :] = RNG.normal(loc=mu1, scale=stds, size=(n1, 3))\n    \n    return X, true_labels\n\ndef induce_missingness(X1, mechanism, params):\n    \"\"\"\n    Induces missingness in a single feature vector X1.\n    \"\"\"\n    n = len(X1)\n    X1_missing = X1.copy()\n    \n    if mechanism == 'mcar':\n        q = params['q']\n        if q > 0:\n            is_missing = RNG.uniform(0, 1, size=n)  q\n            X1_missing[is_missing] = np.nan\n    elif mechanism == 'mnar':\n        alpha, tau = params['alpha'], params['tau']\n        # Probability of being MISSING is sigma(alpha * (x - tau))\n        # P(R=1) = 1 - sigma(...) = P(R=0) = sigma(...)\n        prob_missing = sigmoid(alpha * (X1 - tau))\n        is_missing = RNG.uniform(0, 1, size=n)  prob_missing\n        X1_missing[is_missing] = np.nan\n    \n    return X1_missing\n\ndef impute_mean(X_with_nan):\n    \"\"\"\n    Performs mean imputation on a feature vector with np.nan for missing values.\n    \"\"\"\n    X_imputed = X_with_nan.copy()\n    if np.any(np.isnan(X_imputed)):\n        mean_obs = np.nanmean(X_imputed)\n        X_imputed[np.isnan(X_imputed)] = mean_obs\n    return X_imputed\n\ndef kmeans(X, k, n_init, max_iter):\n    \"\"\"\n    Implements k-means clustering from first principles.\n    \"\"\"\n    best_wcss = np.inf\n    best_labels = None\n    best_centroids = None\n\n    for _ in range(n_init):\n        # Randomly initialize centroids by picking k unique points from data\n        initial_indices = RNG.choice(X.shape[0], size=k, replace=False)\n        centroids = X[initial_indices]\n        \n        for i in range(max_iter):\n            # Assignment step: compute distances and assign labels\n            dist_sq = np.sum((X[:, np.newaxis, :] - centroids[np.newaxis, :, :])**2, axis=2)\n            labels = np.argmin(dist_sq, axis=1)\n            \n            # Update step: compute new centroids\n            new_centroids = np.zeros_like(centroids)\n            for j in range(k):\n                cluster_points = X[labels == j]\n                if len(cluster_points) > 0:\n                    new_centroids[j] = cluster_points.mean(axis=0)\n                else: # Handle empty cluster\n                    new_centroids[j] = X[RNG.choice(X.shape[0])] # Re-initialize\n            \n            # Check for convergence\n            if np.allclose(new_centroids, centroids):\n                break\n            centroids = new_centroids\n\n        # Calculate WCSS for this run\n        wcss = 0\n        for j in range(k):\n            cluster_points = X[labels == j]\n            if len(cluster_points)  0:\n                wcss += np.sum((cluster_points - centroids[j])**2)\n        \n        if wcss  best_wcss:\n            best_wcss = wcss\n            best_labels = labels\n            best_centroids = centroids\n            \n    return best_labels, best_centroids\n\ndef silhouette_score(X, labels):\n    \"\"\"\n    Implements silhouette index calculation from first principles.\n    \"\"\"\n    n_samples = X.shape[0]\n    unique_labels = np.unique(labels)\n    n_clusters = len(unique_labels)\n\n    if n_clusters  2:\n        return 0.0\n\n    # Pre-compute pairwise distance matrix\n    dist_matrix = np.sqrt(np.sum((X[:, np.newaxis, :] - X[np.newaxis, :, :])**2, axis=2))\n    \n    silhouette_vals = np.zeros(n_samples)\n    \n    for i in range(n_samples):\n        label_i = labels[i]\n        \n        # a_i: mean intra-cluster distance\n        mask_i = (labels == label_i)\n        cluster_size_i = np.sum(mask_i)\n        if cluster_size_i == 1:\n            silhouette_vals[i] = 0  # As per problem definition\n            continue\n        a_i = np.sum(dist_matrix[i, mask_i]) / (cluster_size_i - 1)\n        \n        # b_i: mean nearest-cluster distance\n        min_mean_dist = np.inf\n        for j in unique_labels:\n            if j == label_i:\n                continue\n            mask_j = (labels == j)\n            mean_dist_j = np.mean(dist_matrix[i, mask_j])\n            min_mean_dist = min(min_mean_dist, mean_dist_j)\n        b_i = min_mean_dist\n        \n        # Silhouette for point i\n        if max(a_i, b_i) == 0:\n            silhouette_vals[i] = 0\n        else:\n            silhouette_vals[i] = (b_i - a_i) / max(a_i, b_i)\n            \n    return np.mean(silhouette_vals)\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases.\n    \"\"\"\n    # K-means parameters\n    N_INIT = 20\n    MAX_ITER = 100\n\n    test_cases = [\n        # Case 1: MNAR, strong artifact expected\n        {'n': 600, 'mu0': [0, 0, 0], 'mu1': [5, 0, 0], 'stds': [0.7, 0.6, 0.6], \n         'missingness': {'mechanism': 'mnar', 'params': {'alpha': 1.2, 'tau': 4.0}}},\n        # Case 2: MCAR, no artifact expected\n        {'n': 600, 'mu0': [0, 0, 0], 'mu1': [5, 0, 0], 'stds': [0.7, 0.6, 0.6], \n         'missingness': {'mechanism': 'mcar', 'params': {'q': 0.05}}},\n        # Case 3: No missingness, baseline\n        {'n': 600, 'mu0': [0, 0, 0], 'mu1': [5, 0, 0], 'stds': [0.7, 0.6, 0.6], \n         'missingness': {'mechanism': 'mcar', 'params': {'q': 0.0}}},\n    ]\n    \n    results = []\n\n    for case in test_cases:\n        # A. Generate data\n        X, _ = generate_data(\n            n=case['n'], \n            mu0=np.array(case['mu0']), \n            mu1=np.array(case['mu1']), \n            stds=np.array(case['stds'])\n        )\n\n        # B. Induce missingness\n        X1_with_nan = induce_missingness(\n            X[:, 0], \n            case['missingness']['mechanism'], \n            case['missingness']['params']\n        )\n        \n        # C. Impute\n        X1_imputed = impute_mean(X1_with_nan)\n        X_full = X.copy()\n        X_full[:, 0] = X1_imputed\n        \n        # Define reduced feature set\n        X_reduced = X[:, 1:]\n\n        # D. Clustering and validation for full features\n        labels_full_k2, _ = kmeans(X_full, k=2, n_init=N_INIT, max_iter=MAX_ITER)\n        s_full_k2 = silhouette_score(X_full, labels_full_k2)\n        \n        labels_full_k3, _ = kmeans(X_full, k=3, n_init=N_INIT, max_iter=MAX_ITER)\n        s_full_k3 = silhouette_score(X_full, labels_full_k3)\n\n        # D. Clustering and validation for reduced features\n        labels_reduced_k2, _ = kmeans(X_reduced, k=2, n_init=N_INIT, max_iter=MAX_ITER)\n        s_reduced_k2 = silhouette_score(X_reduced, labels_reduced_k2)\n\n        labels_reduced_k3, _ = kmeans(X_reduced, k=3, n_init=N_INIT, max_iter=MAX_ITER)\n        s_reduced_k3 = silhouette_score(X_reduced, labels_reduced_k3)\n        \n        # E. Artifact detection rule\n        is_artifact_detected = (s_full_k3 > s_full_k2) and (s_reduced_k3 = s_reduced_k2)\n        results.append(is_artifact_detected)\n\n    # Final output formatting\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the simulation\nsolve()\n```", "id": "4576069"}]}