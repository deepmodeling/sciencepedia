{"hands_on_practices": [{"introduction": "在我们开始编程之前，必须先理解其背后的机理。本练习为求解正则化回归问题最高效的算法之一——坐标下降法——奠定了数学基础。你将从目标函数出发，推导岭回归（$\\ell_2$）和Lasso回归（$\\ell_1$）的坐标更新法则，这一过程将加深你对正则化惩罚项如何转化为具体算法步骤的理解，并引入通过Lipschitz连续性来保证收敛性的概念 ([@problem_id:4605272])。", "problem": "在一个来自一组患者队列的基因表达预测任务中，令响应向量为 $y \\in \\mathbb{R}^{n}$（例如，一个连续的临床表型），设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，其中第 $j$ 列 $x_{j} \\in \\mathbb{R}^{n}$ 对应于候选生物标志物在 $n$ 个患者中的测量丰度。考虑正则化线性回归目标函数，该函数将一个平滑的平方误差数据拟合项与一个$\\ell_{2}$范数惩罚项（岭回归）或一个$\\ell_{1}$范数惩罚项（LASSO）相结合。定义平滑部分 $g(\\beta)$ 和非平滑部分 $h(\\beta)$ 如下\n$$\ng(\\beta) \\equiv \\frac{1}{2n}\\|y - X\\beta\\|_{2}^{2} + \\frac{\\alpha}{2}\\|\\beta\\|_{2}^{2}, \\quad h(\\beta) \\equiv \\lambda \\|\\beta\\|_{1},\n$$\n其中 $\\beta \\in \\mathbb{R}^{p}$，$\\alpha \\ge 0$ 且 $\\lambda \\ge 0$。对于纯岭回归问题，设 $\\lambda = 0$ 且 $\\alpha  0$。对于纯LASSO问题，设 $\\alpha = 0$ 且 $\\lambda  0$。令坐标下降算法在保持其他系数固定的同时，一次更新一个系数。对于坐标 $j \\in \\{1,\\dots,p\\}$，定义偏残差\n$$\nr^{(j)} \\equiv y - X\\beta + x_{j}\\beta_{j},\n$$\n从而有 $y - X\\beta = r^{(j)} - x_{j}\\beta_{j}$。从凸函数的坐标级最小化、$\\ell_{1}$正则化的次梯度最优性条件以及平滑函数梯度的坐标级利普希茨连续性的定义出发，执行以下任务：\n(i) 在岭回归情况下，推导出 $\\beta_{j}$ 的精确闭式坐标下降更新式，该表达式仅用 $n$、$\\alpha$、$x_{j}$ 和 $r^{(j)}$ 表示。\n(ii) 在LASSO情况下，推导出 $\\beta_{j}$ 的精确闭式坐标下降更新式，该表达式仅用 $n$、$\\lambda$、$x_{j}$ 和 $r^{(j)}$ 表示，并使用标准标量函数（如绝对值、最大值和符号函数）。\n(iii) 确定平滑部分 $g(\\beta)$ 关于 $\\beta_{j}$ 的偏导数的一个有效的坐标级利普希茨常数 $L_{j}$，用 $n$、$\\alpha$ 和 $x_{j}$ 表示，并根据 $g(\\beta)$ 的凸性、其梯度的坐标级利普希茨连续性以及 $h(\\beta)$ 的可分离性，陈述循环坐标下降收敛的充分条件。以一个行矩阵的形式提供您的最终答案，该矩阵按顺序包含 $\\beta_{j}$ 的岭回归更新式、$\\beta_{j}$ 的LASSO更新式以及坐标级利普希茨常数 $L_{j}$。不需要进行数值近似或四舍五入，也无需单位。", "solution": "问题陈述已经过验证，被认为是合理的。它在科学上基于标准的优化和统计学习理论，是适定的，提供了所有必要信息，并且表述客观。这是指定领域中的一个形式化且相关的问题。我现在开始进行解答。\n\n要最小化的目标函数是 $F(\\beta) = g(\\beta) + h(\\beta)$，其中\n$$\ng(\\beta) = \\frac{1}{2n}\\|y - X\\beta\\|_{2}^{2} + \\frac{\\alpha}{2}\\|\\beta\\|_{2}^{2}\n$$\n和\n$$\nh(\\beta) = \\lambda \\|\\beta\\|_{1}\n$$\n坐标下降一次更新单个系数 $\\beta_j$，同时保持所有其他系数 $\\beta_{k \\neq j}$ 固定。通过将 $\\beta$ 的所有其他分量视为常数，目标函数可以写成仅关于 $\\beta_j$ 的函数。\n\n我们使用给定的偏残差定义，$r^{(j)} \\equiv y - X\\beta + x_{j}\\beta_{j} = y - \\sum_{k \\neq j} x_k \\beta_k$。这使我们能够将完整残差表示为 $y - X\\beta = r^{(j)} - x_j \\beta_j$。\n\n数据拟合项变为：\n$$\n\\|y - X\\beta\\|_{2}^{2} = \\|r^{(j)} - x_j \\beta_j\\|_{2}^{2} = (r^{(j)} - x_j \\beta_j)^T(r^{(j)} - x_j \\beta_j) = (r^{(j)})^T r^{(j)} - 2\\beta_j x_j^T r^{(j)} + \\beta_j^2 x_j^T x_j\n$$\n注意 $x_j^T x_j = \\|x_j\\|_2^2$。项 $(r^{(j)})^T r^{(j)}$ 在关于 $\\beta_j$ 的优化中是常数。\n\n$\\ell_2$惩罚项变为：\n$$\n\\|\\beta\\|_{2}^{2} = \\sum_{k=1}^{p} \\beta_k^2 = \\beta_j^2 + \\sum_{k \\neq j} \\beta_k^2\n$$\n项 $\\sum_{k \\neq j} \\beta_k^2$ 在关于 $\\beta_j$ 的优化中是常数。\n\n$\\ell_1$惩罚项变为：\n$$\n\\|\\beta\\|_{1} = \\sum_{k=1}^{p} |\\beta_k| = |\\beta_j| + \\sum_{k \\neq j} |\\beta_k|\n$$\n项 $\\sum_{k \\neq j} |\\beta_k|$ 在关于 $\\beta_j$ 的优化中是常数。\n\n### (i) 岭回归坐标更新 ($\\lambda = 0, \\alpha  0$)\n\n对于纯岭回归，目标函数是 $F(\\beta) = g(\\beta)$ 且 $\\lambda=0$。我们想找到在保持 $\\beta_{k \\neq j}$ 固定的情况下使 $F(\\beta)$ 最小化的 $\\beta_j$ 值。$F(\\beta)$ 中依赖于 $\\beta_j$ 的项是：\n$$\nF(\\beta_j) = \\frac{1}{2n} (-2\\beta_j x_j^T r^{(j)} + \\beta_j^2 \\|x_j\\|_2^2) + \\frac{\\alpha}{2} \\beta_j^2 + \\text{常数}\n$$\n这是一个关于 $\\beta_j$ 的平滑、凸（二次）函数。我们通过对 $\\beta_j$ 求导并令其等于零来找到最小值。\n$$\n\\frac{dF}{d\\beta_j} = \\frac{1}{2n} (-2 x_j^T r^{(j)} + 2\\beta_j \\|x_j\\|_2^2) + \\alpha \\beta_j = 0\n$$\n$$\n-\\frac{1}{n} x_j^T r^{(j)} + \\frac{1}{n} \\beta_j \\|x_j\\|_2^2 + \\alpha \\beta_j = 0\n$$\n现在，我们求解 $\\beta_j$：\n$$\n\\beta_j \\left(\\frac{1}{n} \\|x_j\\|_2^2 + \\alpha\\right) = \\frac{1}{n} x_j^T r^{(j)}\n$$\n$$\n\\beta_j \\left(\\|x_j\\|_2^2 + n\\alpha\\right) = x_j^T r^{(j)}\n$$\n在岭回归情况下，$\\beta_j$ 的坐标更新式为：\n$$\n\\beta_j = \\frac{x_j^T r^{(j)}}{\\|x_j\\|_2^2 + n\\alpha}\n$$\n\n### (ii) LASSO回归坐标更新 ($\\alpha = 0, \\lambda  0$)\n\n对于纯LASSO回归，目标函数是 $F(\\beta) = g(\\beta) + h(\\beta)$ 且 $\\alpha=0$。任务是最小化以下关于 $\\beta_j$ 的函数：\n$$\nF(\\beta_j) = \\frac{1}{2n} \\|r^{(j)} - x_j \\beta_j\\|_{2}^{2} + \\lambda |\\beta_j| + \\text{常数}\n$$\n$$\nF(\\beta_j) = \\frac{1}{2n} (\\beta_j^2 \\|x_j\\|_2^2 - 2\\beta_j x_j^T r^{(j)}) + \\lambda |\\beta_j| + \\text{常数}\n$$\n由于绝对值项的存在，该函数是凸的但非平滑。我们使用次梯度最优性条件，该条件表明在最小值处，$0$ 必须在 $F(\\beta_j)$ 的次微分中。我们记 $c_j = x_j^T r^{(j)}$ 和 $a_j = \\|x_j\\|_2^2$。目标简化为最小化 $f(\\beta_j) = \\frac{1}{2n}(a_j \\beta_j^2 - 2c_j \\beta_j) + \\lambda|\\beta_j|$。\n次微分是 $\\partial F(\\beta_j) = \\{ \\frac{1}{n}(a_j\\beta_j - c_j) + \\lambda \\cdot s \\}$，其中如果 $\\beta_j \\neq 0$，则 $s = \\text{sign}(\\beta_j)$；如果 $\\beta_j = 0$，则 $s \\in [-1, 1]$。\n设 $0 \\in \\partial F(\\beta_j)$:\n$$\n0 = \\frac{1}{n}(a_j\\beta_j - c_j) + \\lambda s \\implies c_j - a_j\\beta_j = n\\lambda s\n$$\n情况1：$\\beta_j  0$。那么 $s=1$。\n$c_j - a_j\\beta_j = n\\lambda \\implies \\beta_j = \\frac{c_j - n\\lambda}{a_j}$。这仅在 $\\beta_j  0$ 时有效，这要求 $c_j  n\\lambda$。\n\n情况2：$\\beta_j  0$。那么 $s=-1$。\n$c_j - a_j\\beta_j = -n\\lambda \\implies \\beta_j = \\frac{c_j + n\\lambda}{a_j}$。这仅在 $\\beta_j  0$ 时有效，这要求 $c_j  -n\\lambda$。\n\n情况3：$\\beta_j = 0$。那么 $s \\in [-1, 1]$。\n$c_j - a_j(0) = n\\lambda s \\implies c_j = n\\lambda s$。因为 $s \\in [-1, 1]$，这意味着 $|c_j| \\le n\\lambda$。\n\n结合这三种情况并代回 $c_j = x_j^T r^{(j)}$ 和 $a_j = \\|x_j\\|_2^2$（假设 $\\|x_j\\|_2^2  0$）：\n- 如果 $x_j^T r^{(j)}  n\\lambda$，那么 $\\beta_j = \\frac{x_j^T r^{(j)} - n\\lambda}{\\|x_j\\|_2^2}$。\n- 如果 $x_j^T r^{(j)}  -n\\lambda$，那么 $\\beta_j = \\frac{x_j^T r^{(j)} + n\\lambda}{\\|x_j\\|_2^2}$。\n- 如果 $|x_j^T r^{(j)}| \\le n\\lambda$，那么 $\\beta_j = 0$。\n\n这是软阈值算子。该更新可以使用符号函数和最大值函数紧凑地写为：\n$$\n\\beta_j = \\frac{1}{\\|x_j\\|_2^2} \\text{sign}(x_j^T r^{(j)}) \\max(|x_j^T r^{(j)}| - n\\lambda, 0)\n$$\n\n### (iii) 坐标级利普希茨常数与收敛条件\n\n对于平滑部分梯度的坐标级利普希茨常数 $L_j$（即 $\\nabla_j g(\\beta)$），它是在我们只改变 $\\beta_j$ 时该偏导数变化量的上界。形式上，对于任何标量 $h$，有 $|\\nabla_j g(\\beta + h e_j) - \\nabla_j g(\\beta)| \\le L_j |h|$。对于二阶可导函数，$L_j$ 可以取为关于 $\\beta_j$ 的二阶偏导数的最大绝对值。\n\n平滑部分是 $g(\\beta) = \\frac{1}{2n}\\|y - X\\beta\\|_{2}^{2} + \\frac{\\alpha}{2}\\|\\beta\\|_{2}^{2}$。\n首先，我们求偏导数 $\\nabla_j g(\\beta) = \\frac{\\partial g}{\\partial \\beta_j}$：\n$$\n\\frac{\\partial g}{\\partial \\beta_j} = \\frac{1}{2n} \\frac{\\partial}{\\partial \\beta_j} (y - X\\beta)^T(y - X\\beta) + \\frac{\\alpha}{2} \\frac{\\partial}{\\partial \\beta_j} \\sum_k \\beta_k^2\n$$\n$$\n\\frac{\\partial g}{\\partial \\beta_j} = \\frac{1}{n} (X\\beta - y)^T (-x_j) + \\alpha \\beta_j = -\\frac{1}{n} (y - X\\beta)^T x_j + \\alpha \\beta_j = \\frac{1}{n} x_j^T(X\\beta - y) + \\alpha \\beta_j\n$$\n现在，我们求二阶偏导数 $\\frac{\\partial^2 g}{\\partial \\beta_j^2}$：\n$$\n\\frac{\\partial^2 g}{\\partial \\beta_j^2} = \\frac{\\partial}{\\partial \\beta_j} \\left( \\frac{1}{n} x_j^T(X\\beta - y) + \\alpha\\beta_j \\right) = \\frac{1}{n} x_j^T x_j + \\alpha = \\frac{1}{n}\\|x_j\\|_2^2 + \\alpha\n$$\n这个二阶导数关于 $\\beta$ 是一个常数。因此，$\\nabla_j g$ 的一个有效的坐标级利普希茨常数正是这个值：\n$$\nL_j = \\frac{1}{n}\\|x_j\\|_2^2 + \\alpha\n$$\n对于形式为 $F(\\beta) = g(\\beta) + h(\\beta)$ 的目标函数，循环坐标下降的收敛充分条件是：\n1.  $g(\\beta)$ 是凸函数，并且其梯度 $\\nabla g(\\beta)$ 是坐标级利普希茨连续的。函数 $g(\\beta)$ 是凸的，因为它的海森矩阵 $\\nabla^2 g(\\beta) = \\frac{1}{n}X^T X + \\alpha I$ 是半正定的（因为 $X^T X$ 是半正定的且 $\\alpha \\ge 0$）。我们刚刚证明了它的梯度是坐标级利普希茨连续的，常数为 $L_j = \\frac{1}{n}\\|x_j\\|_2^2 + \\alpha$。\n2.  $h(\\beta)$ 是凸的且可分离的。函数 $h(\\beta) = \\lambda \\|\\beta\\|_1 = \\lambda \\sum_j |\\beta_j|$ 是凸函数的和，因此是凸的。它是可分离的，因为它是多个函数的和，其中每个函数只依赖于一个分量 $\\beta_j$。\n\n这些条件确保了由循环坐标下降生成的迭代序列收敛到目标函数 $F(\\beta)$ 的一个全局最小值点。", "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{x_j^T r^{(j)}}{\\|x_j\\|_2^2 + n\\alpha}  \\frac{\\text{sign}(x_j^T r^{(j)}) \\max(|x_j^T r^{(j)}| - n\\lambda, 0)}{\\|x_j\\|_2^2}  \\frac{1}{n}\\|x_j\\|_2^2 + \\alpha \\end{pmatrix} } $$", "id": "4605272"}, {"introduction": "现在，让我们将理论付诸实践，探索Lasso回归的标志性特征：其自动进行特征选择的能力。本练习模拟了一个常见的生物信息学场景，即从大量遗传标记中筛选出相关的标记。你将通过实现坐标下降算法来计算Lasso的“正则化路径” ([@problem_id:4605278])，并观察随着惩罚参数 $\\lambda$ 的减小，不同特征的系数是如何依次变为非零值的。这项编程任务让你直观地体验稀疏性，并帮助你建立关于如何通过调整正则化参数来控制模型复杂度的直觉。", "problem": "给定一个合成的单核苷酸多态性（SNP）基因型数据集和一个由稀疏线性模型构建的连续表型。特征被编码为次要等位基因计数 $0$、$1$ 或 $2$，然后被标准化以具有零均值和单位方差，表型也被中心化至零均值。考虑使用坐标下降法求解线性回归的最小绝对收缩和选择算子（LASSO）问题。您的任务是实现一个完整的、可运行的程序，该程序使用坐标下降法，沿着一个递减的正则化强度 $\\lambda$ 网格计算 LASSO 解，并报告系数的进入路径：即随着 $\\lambda$ 的减小，标准化特征系数变为非零的顺序。您必须严格遵守下面所述的最终输出格式要求。\n\n使用的基本原理：\n- 对于一个中心化的表型 $y \\in \\mathbb{R}^n$ 和标准化的设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 的线性模型，其中矩阵的列经过中心化和缩放，以使得对每个特征索引 $j$ 都有 $\\frac{1}{n}\\sum_{i=1}^n x_{ij}^2 = 1$。\n- LASSO 目标函数，它惩罚系数的 $\\ell_1$ 范数：最小化\n$$\n\\frac{1}{2n}\\lVert y - X\\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1,\n$$\n其中 $\\beta \\in \\mathbb{R}^p$ 是系数，$\\lambda \\ge 0$ 是正则化参数。\n- 针对标准化特征，从次梯度/Karush–Kuhn–Tucker (KKT) 平稳性条件推导出的逐坐标软阈值更新法则为：\n$$\n\\beta_j \\leftarrow \\frac{1}{\\frac{1}{n}\\lVert X_{\\cdot j} \\rVert_2^2} \\cdot S\\!\\left(\\frac{1}{n}X_{\\cdot j}^\\top \\left(y - \\sum_{k \\ne j} X_{\\cdot k}\\beta_k\\right), \\lambda \\right),\n$$\n其中软阈值算子定义为 $S(a,t) = \\operatorname{sign}(a)\\max(|a| - t, 0)$。在给定的标准化条件下，$\\frac{1}{n}\\lVert X_{\\cdot j} \\rVert_2^2 = 1$，因此分母简化为 $1$。\n\n数据构建：\n- 设有个体数 $n = 60$ 和 SNP 数 $p = 8$。为保证可复现性，使用一个固定的随机种子，将每个 SNP 列 $X_{\\cdot j}$ 独立地从 $\\operatorname{Binomial}(2, q_j)$ 分布中抽取，其中次要等位基因频率 (MAF) 固定为 $(q_1,\\dots,q_8) = (0.05, 0.10, 0.20, 0.15, 0.30, 0.25, 0.40, 0.35)$。生成后，如上所述，将每个特征标准化为零均值和单位方差。\n- 使用固定的真实系数 $\\beta^\\star = (0.0, 1.2, 0.0, -0.8, 0.0, 0.0, 0.5, 0.0)$ 和一个确定性扰动 $\\epsilon_i = 0.1\\sin(i)$（对于索引 $i = 0, 1, \\dots, n-1$）从稀疏线性模型构建中心化的表型 $y$，然后将 $y$ 中心化：\n$$\ny \\leftarrow X\\beta^\\star + \\epsilon, \\quad \\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i, \\quad y \\leftarrow y - \\bar{y}.\n$$\n\n算法要求：\n- 使用坐标下降法，对 $j = 1, \\dots, p$ 进行循环更新，以在每个 $\\lambda$ 值上求解 LASSO 目标函数，直至收敛。在递减的 $\\lambda$ 路径上使用热启动策略。\n- 使用停止准则 $\\max_j |\\Delta\\beta_j|  10^{-6}$ 或最多进行 $1000$ 次完整的坐标轮换，以先达到的条件为准。\n- 令 $\\lambda_{\\max} = \\max_j \\left|\\frac{1}{n} X_{\\cdot j}^\\top y\\right|$，在此 $\\lambda$ 值下，解为 $\\beta = 0$。根据下文测试套件中的规定，从 $\\lambda_{\\max}$ 开始构建递减至更小值的 $\\lambda$ 网格。\n\n系数进入路径：\n- 对于每个特征索引 $j \\in \\{0,1,\\dots,p-1\\}$，将其“进入 $\\lambda$”定义为（在递减网格中）使得收敛后的 $\\beta_j$ 首次变为非零的 $\\lambda$ 值（在数值上，将 $|\\beta_j| > 10^{-8}$ 视为非零）。如果一个系数在整个网格上从未变为非零，则它没有进入点，不应出现在顺序列表中。如果出现并列情况（多个特征在同一个网格点 $\\lambda$ 进入），则按索引升序列出特征。系数进入路径是随着 $\\lambda$ 的减小，根据特征首次进入的顺序排列的特征索引列表。\n\n测试套件：\n使用相同的基础数据集和明确的 $\\lambda$ 网格，实现以下四个测试用例。此设计确保了对理想路径、边界情况、极端情况和共线性边缘情况的覆盖。\n\n- 测试用例 1（理想路径）：使用一个包含 50 个值的等比网格，从 $\\lambda_{\\max}$ 递减到 $0.01\\lambda_{\\max}$，即 $\\lambda_k = \\lambda_{\\max} \\cdot r^{k-1}$，其中公比 $r$ 的选择使得 $\\lambda_{50} = 0.01\\lambda_{\\max}$。报告在此路径上首次变为非零的特征索引的有序列表。\n- 测试用例 2（边界情况）：仅使用单点网格 $\\{\\lambda_{\\max}\\}$。报告特征索引的有序列表（预期为空）。\n- 测试用例 3（极端小 $\\lambda$ 情况）：使用一个包含 120 个值的等比网格，从 $\\lambda_{\\max}$ 递减到 $10^{-6}\\lambda_{\\max}$。报告在此路径上首次变为非零的特征索引的有序列表。\n- 测试用例 4（共线性边缘情况）：通过复制 SNP 特征索引 3 来创建一个增广数据集，形成第九个特征（使得 $p=9$）。使用相同的协议按列标准化该增广矩阵，为增广数据集重新计算 $\\lambda_{\\max}$，并使用一个包含 50 个值的等比网格，从这个增广的 $\\lambda_{\\max}$ 递减到其 $0.01$ 倍。报告在此路径上首次变为非零的特征索引的有序列表。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表形式的结果。每个元素本身都应是一个由方括号括起来的、逗号分隔的整数列表，代表对应测试用例中特征索引（从零开始）按其首次进入顺序的排列。例如，一个包含四个测试用例的输出可能看起来像 “[[1,3,2],[],[1,7,5,3,6],[1,3,8]]”。不应打印任何其他文本。\n\n不涉及物理单位或角度单位。所有数值答案均为纯数字。每个测试用例的结果必须是按规定格式的整数列表。", "solution": "用户提供了一个统计机器学习领域的、定义明确的计算问题，该问题特别关注 LASSO 回归模型的实现。任务是使用坐标下降算法，为一个合成的单核苷酸多态性（SNP）数据集计算系数进入路径。问题陈述经校验在科学上是合理的、良定的和完整的。\n\n### 第一步：问题校验\n\n**1.1. 提取已知条件：**\n- **模型：** 带 LASSO 惩罚的线性回归：$\\min_{\\beta} \\frac{1}{2n}\\lVert y - X\\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1$。\n- **数据属性：** $y$ 是 $\\mathbb{R}^n$ 中的一个中心化表型向量。$X$ 是 $\\mathbb{R}^{n \\times p}$ 中的一个标准化设计矩阵，其列被中心化至零均值并被缩放，以使 $\\frac{1}{n}\\sum_{i=1}^n x_{ij}^2 = 1$。\n- **算法：** 带循环更新的坐标下降法。对一个标准化特征 $j$ 的更新规则是 $\\beta_j \\leftarrow S(\\rho_j, \\lambda)$，其中 $\\rho_j = \\frac{1}{n}X_{\\cdot j}^\\top (y - \\sum_{k \\ne j} X_{\\cdot k}\\beta_k)$，$S(a,t) = \\operatorname{sign}(a)\\max(|a| - t, 0)$ 是软阈值算子。\n- **收敛准则：** 系数最大变化量 $\\max_j |\\Delta\\beta_j|  10^{-6}$ 或最多进行 $1000$ 次轮换。\n- **正则化路径：** 路径从 $\\lambda_{\\max} = \\max_j |\\frac{1}{n} X_{\\cdot j}^\\top y|$ 开始，并沿着一个指定的网格递减。使用热启动，即一个 $\\lambda$ 的解用来初始化下一个更小的 $\\lambda$ 的求解器。\n- **数据集生成：** $n=60$，$p=8$。使用固定的随机种子，从 $\\operatorname{Binomial}(2, q_j)$ 分布中抽取 SNP 特征 $X_{\\cdot j}$，其中指定的次要等位基因频率 (MAF) 为 $(q_j) = (0.05, 0.10, \\dots, 0.35)$。表型为 $y = X\\beta^\\star + \\epsilon - \\operatorname{mean}(X\\beta^\\star + \\epsilon)$，其中 $\\beta^\\star$ 是一个固定的真实系数向量，$\\epsilon_i = 0.1\\sin(i)$ 是一个确定性扰动。\n- **输出定义：** “系数进入路径”是指随着 $\\lambda$ 沿着其网格递减，首次变为非零（定义为 $|\\beta_j| > 10^{-8}$）的特征索引的有序列表。在给定的 $\\lambda$ 处的并列情况通过特征索引升序来解决。\n- **测试用例：** 定义了四个具体的测试用例，它们改变了 $\\lambda$ 网格，并包含一个引入了共线性的情况。\n\n**1.2. 校验结论：**\n该问题是 **有效的**。这是一个清晰、自包含且有科学依据的任务。它指定了一个标准算法（用于 LASSO 的坐标下降法）应用于一个合成生成的数据集，该数据集模拟了生物信息学中的真实世界应用。所有必需的参数、常数和程序都已定义，使得问题可复现和可验证。“一个固定的随机种子”这一轻微的模糊之处通过选择一个标准的常规值来解决，这符合可复现性的意图。\n\n### 第二步：解决方案实现\n\n解决方案首先实现必要的组件：数据生成、坐标下降算法以及追踪系数路径的逻辑。然后协调这些组件来执行四个指定的测试用例。\n\n**2.1. 数据生成：**\n定义一个函数，根据问题的规格生成合成数据集。它使用固定的随机种子以保证可复现性。该函数首先从二项分布中生成原始 SNP 数据（等位基因计数 $0, 1, 2$）。然后，通过将每列中心化至均值为 $0$ 并将其缩放至单位经验方差（即 $\\frac{1}{n}\\sum_i (x_{ij} - \\bar{x}_j)^2 = 1$），来标准化数据矩阵 $X$。表型 $y$ 是使用标准化的 $X$、真实系数向量 $\\beta^\\star$ 和指定的确定性扰动 $\\epsilon$ 构建的，之后将其中心化至零均值。\n\n**2.2. 用于 LASSO 的坐标下降法：**\n解决方案的核心是坐标下降求解器。对于给定的正则化参数 $\\lambda$，该算法迭代更新每个系数 $\\beta_j$ 直至收敛。实现采用了一种高效的 Gauss-Seidel 风格更新，其中使用了其他系数的最新值。为了高效地实现这一点，我们迭代地维护和更新模型的残差向量 $r = y - X\\beta$。\n\n单个系数 $\\beta_j$ 的更新过程如下：\n1.  高效地计算软阈值函数的参数 $\\rho_j = \\frac{1}{n}X_{\\cdot j}^\\top (y - \\sum_{k \\ne j} X_{\\cdot k}\\beta_k)$。认识到 $y - \\sum_{k \\ne j} X_{\\cdot k}\\beta_k = r + X_{\\cdot j}\\beta_j^{\\text{old}}$，且对于标准化特征 $\\frac{1}{n}X_{\\cdot j}^\\top X_{\\cdot j} = 1$，参数变为 $\\rho_j = \\frac{1}{n}X_{\\cdot j}^\\top r + \\beta_j^{\\text{old}}$。在这里，$r$ 是使用最新系数向量计算的残差。\n2.  系数被更新：$\\beta_j^{\\text{new}} \\leftarrow S(\\rho_j, \\lambda)$。\n3.  系数的变化量 $\\Delta\\beta_j = \\beta_j^{\\text{new}} - \\beta_j^{\\text{old}}$ 被用来高效地更新全局残差向量：$r \\leftarrow r - X_{\\cdot j}\\Delta\\beta_j$。这避免了重复的、代价高昂的矩阵-向量乘法。\n\n对所有特征的这个循环过程构成一次轮换。当一次完整轮换中任何系数的最大变化量低于容差 $10^{-6}$ 或完成 $1000$ 次轮换时，算法终止。\n\n**2.3. 系数路径计算：**\n为了确定进入路径，我们在一个递减的网格上为每个 $\\lambda$ 求解 LASSO 问题。较大 $\\lambda$ 的解作为下一个较小 $\\lambda$ 的“热启动”，这会加速收敛。\n\n在每个 $\\lambda$ 处收敛后，我们识别出具有非零系数（其中 $|\\beta_j| > 10^{-8}$）的特征集合。通过将此集合与前一个（更大的）$\\lambda$ 的非零特征集合进行比较，我们可以识别出任何新进入的特征。这些新特征按其索引排序以处理并列情况，然后附加到总的进入路径列表中。\n\n**2.4. 测试用例执行：**\n主程序执行四个指定的测试用例：\n1.  **理想路径：** 为基础数据集（$p=8$）创建一个包含 50 个 $\\lambda$ 值的标准等比网格。\n2.  **边界情况：** 网格仅包含单个值 $\\lambda_{\\max}$。正如理论预测，没有任何系数会变为非零。\n3.  **极端小 $\\lambda$ 情况：** 使用一个包含 120 个值的更长网格，达到一个更小的最小 $\\lambda$ 值，从而允许更多系数进入模型。\n4.  **共线性情况：** 通过复制一个特征列来增广设计矩阵，从而产生完全共线性。增广矩阵被重新标准化，计算一个新的 $\\lambda_{\\max}$，并在一个新的网格上追踪路径。这在一个已知的挑战性条件下测试算法的行为和并列处理规则。\n\n这四个用例的结果被收集起来，并格式化为问题陈述所要求的精确字符串表示。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the LASSO coefficient entry path for four test cases\n    using coordinate descent on a synthetic SNP dataset.\n    \"\"\"\n\n    # --- Problem Constants and Algorithm Parameters ---\n    N_SAMPLES = 60\n    N_FEATURES_BASE = 8\n    MAF_RATES = np.array([0.05, 0.10, 0.20, 0.15, 0.30, 0.25, 0.40, 0.35])\n    BETA_STAR = np.array([0.0, 1.2, 0.0, -0.8, 0.0, 0.0, 0.5, 0.0])\n    EPSILON = 0.1 * np.sin(np.arange(N_SAMPLES))\n    RANDOM_SEED = 0\n\n    CONV_TOL = 1e-6\n    NON_ZERO_TOL = 1e-8\n    MAX_SWEEPS = 1000\n\n    # --- Helper Functions ---\n\n    def soft_threshold(a, t):\n        \"\"\"Soft-thresholding operator S(a, t).\"\"\"\n        return np.sign(a) * np.maximum(np.abs(a) - t, 0)\n\n    def generate_base_data():\n        \"\"\"\n        Generates and standardizes the base dataset (X, y) for p=8\n        and also returns the raw, unstandardized X matrix.\n        \"\"\"\n        rng = np.random.default_rng(RANDOM_SEED)\n        X_raw = np.zeros((N_SAMPLES, N_FEATURES_BASE))\n        for j in range(N_FEATURES_BASE):\n            X_raw[:, j] = rng.binomial(2, MAF_RATES[j], size=N_SAMPLES)\n\n        # Standardize X: zero mean, unit variance.\n        # This implies (1/n) * sum(X_scaled_j^2) = 1 for each column j.\n        mean_X = np.mean(X_raw, axis=0)\n        std_X = np.std(X_raw, axis=0)\n        std_X[std_X == 0] = 1.0  # Avoid division by zero\n        X = (X_raw - mean_X) / std_X\n\n        # Generate y and center it\n        y_raw = X @ BETA_STAR + EPSILON\n        y = y_raw - np.mean(y_raw)\n\n        return X, y, X_raw\n\n    def coordinate_descent_lasso(X, y, lambda_val, beta_init):\n        \"\"\"\n        Solves the LASSO objective for a single lambda value.\n        Uses an efficient coordinate descent with Gauss-Seidel updates.\n        \"\"\"\n        n, p = X.shape\n        beta = np.copy(beta_init)\n        \n        for _ in range(MAX_SWEEPS):\n            beta_old_sweep = np.copy(beta)\n            for j in range(p):\n                # Calculate rho_j = (1/n) * X_j^T * (y - sum_{k!=j} X_k * beta_k)\n                # This uses the most recent beta values (Gauss-Seidel style).\n                r_partial = y - (X @ beta - X[:, j] * beta[j])\n                rho_j = (X[:, j] @ r_partial) / n\n                beta[j] = soft_threshold(rho_j, lambda_val)\n            \n            if np.max(np.abs(beta - beta_old_sweep))  CONV_TOL:\n                break\n        return beta\n\n    def solve_lasso_path(X, y, lambda_grid):\n        \"\"\"\n        Computes the coefficient entry path along a decreasing lambda grid.\n        \"\"\"\n        p = X.shape[1]\n        beta = np.zeros(p)\n        entry_path = []\n        entered_indices = set()\n\n        for lambda_val in lambda_grid:\n            # Use warm starts: previous solution initializes the next run\n            beta = coordinate_descent_lasso(X, y, lambda_val, beta_init=beta)\n\n            # Check for newly entered features\n            current_nonzero_indices = set(np.where(np.abs(beta) > NON_ZERO_TOL)[0])\n            newly_entered = sorted(list(current_nonzero_indices - entered_indices))\n\n            if newly_entered:\n                entry_path.extend(newly_entered)\n                entered_indices.update(newly_entered)\n        \n        return entry_path\n\n    # --- Main Execution Logic for All Test Cases ---\n    \n    results = []\n    \n    # Generate the base dataset once\n    X_base, y, X_raw_base = generate_base_data()\n    n_base, _ = X_base.shape\n\n    # -- Test Case 1: Happy Path --\n    lambda_max_1 = np.max(np.abs(X_base.T @ y / n_base))\n    lambda_grid_1 = np.geomspace(lambda_max_1, 0.01 * lambda_max_1, 50)\n    path_1 = solve_lasso_path(X_base, y, lambda_grid_1)\n    results.append(path_1)\n\n    # -- Test Case 2: Boundary Case --\n    lambda_grid_2 = np.array([lambda_max_1])\n    path_2 = solve_lasso_path(X_base, y, lambda_grid_2)\n    results.append(path_2)\n\n    # -- Test Case 3: Extreme Small Lambda --\n    lambda_grid_3 = np.geomspace(lambda_max_1, 1e-6 * lambda_max_1, 120)\n    path_3 = solve_lasso_path(X_base, y, lambda_grid_3)\n    results.append(path_3)\n    \n    # -- Test Case 4: Collinearity Edge Case --\n    X_aug_raw = np.hstack((X_raw_base, X_raw_base[:, 3:4]))\n    \n    # Re-standardize the augmented matrix\n    mean_aug = np.mean(X_aug_raw, axis=0)\n    std_aug = np.std(X_aug_raw, axis=0)\n    std_aug[std_aug == 0] = 1.0\n    X_aug = (X_aug_raw - mean_aug) / std_aug\n    n_aug, _ = X_aug.shape\n    \n    # Recompute lambda_max and grid for the augmented dataset\n    lambda_max_4 = np.max(np.abs(X_aug.T @ y / n_aug))\n    lambda_grid_4 = np.geomspace(lambda_max_4, 0.01 * lambda_max_4, 50)\n    \n    path_4 = solve_lasso_path(X_aug, y, lambda_grid_4)\n    results.append(path_4)\n\n    # --- Format and Print Final Output ---\n    output_str = \"[\" + \",\".join(f\"[{','.join(map(str, r))}]\" for r in results) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "4605278"}, {"introduction": "与Lasso的特征选择能力形成对比，岭回归以其在处理多重共线性问题上的有效性而著称——这在生物数据中很常见，因为基因往往以相关联的方式协同作用。本练习将探讨岭回归的这一特性，并引入一个衡量模型复杂度的关键指标。你将使用基于奇异值分解（SVD）的数值稳定方法来实现岭回归 ([@problem_id:4605283])，观察它如何在高度相关的特征之间分配系数，并计算“有效自由度”这一模型复杂度的连续度量。这项练习阐明了 $\\ell_2$ 正则化的独特优势，尤其是在处理具有相关预测变量的预测任务时，并加深了你对模型复杂度的理解——它不再仅仅是参数数量的简单计数。", "problem": "您将处理一个线性建模任务，该任务的动机源于基因表达分析，但被纯粹地构建为一个数值线性代数问题。考虑一个线性模型，其设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，响应向量为 $y \\in \\mathbb{R}^{n}$。岭估计量被定义为对系数施加 $\\ell_2$ 惩罚的罚最小二乘目标函数的最小化器。您必须分析岭估计量如何在共线预测变量之间重新分配系数，并通过其线性平滑算子的迹来量化其有效自由度。\n\n您的程序必须为每个测试用例实现以下步骤：\n\n- 预处理：\n  - 标准化 $X$ 的每一列，使其均值为零，方差为一。\n  - 中心化 $y$，使其均值为零。\n  - 不要添加截距项，并且除了中心化之外，不要对 $y$ 进行标准化。\n\n- 岭估计量：\n  - 对于指定的惩罚参数 $\\lambda \\in \\mathbb{R}_{\\ge 0}$，使用一种不显式求逆矩阵的数值稳定方法，计算岭系数向量 $\\hat{\\beta}_{\\lambda} \\in \\mathbb{R}^{p}$。您必须使用一种基于奇异值分解的计算方法，该方法在 $X$ 秩亏时仍然有效。\n\n- 重新分配度量：\n  - 对于前两个预测变量（预处理后 $X$ 的前两列），计算比率 $r = \\hat{\\beta}_{\\lambda,1} / \\hat{\\beta}_{\\lambda,2}$，以量化系数在共线预测变量间的重新分配情况。\n\n- 有效自由度：\n  - 计算有效自由度，其值为将 $y$ 映射到由岭估计量产生的拟合值的线性算子的迹。该值必须利用奇异值分解以数值稳定的方式计算，而无需显式地构造或求逆病态矩阵。\n\n- 四舍五入与输出：\n  - 将 $r$ 和有效自由度都四舍五入到6位小数以便报告。\n\n基本依据与约束：\n- 您必须从岭估计量的定义（即罚最小二乘目标函数的最小化器）以及有效自由度的定义（即将 $y$ 映射到其在岭回归下的拟合值的线性算子的迹）出发。您使用的任何计算公式都必须从这些基础推导而来。在您的实现中，不得假设可以使用未从这些基础推导出的快捷公式。\n- 您的算法必须对包括精确共线性在内的多重共線性具有鲁棒性。\n\n测试套件：\n为以下三个测试用例实现上述要求。为清楚起見，向量均为列向量。\n\n- 案例 A（高相关性，中等正则化）：\n  - 令 $x_1 = [-3,-2,-1,0,1,2,3,4,5,6]^{\\top}$。\n  - 令 $\\delta = [0.1, -0.2, 0.05, -0.05, 0.02, -0.01, 0.08, -0.03, 0.04, -0.02]^{\\top}$。\n  - 令 $x_2 = 0.99\\,x_1 + \\delta$。\n  - 令 $x_3 = [2,-1,3,-2,0,1,-3,2,-2,4]^{\\top}$。\n  - 构造 $X = [x_1, x_2, x_3]$ 和 $y = 1.5\\,x_1 + 1.5\\,x_2 + 0.2\\,x_3$。\n  - 使用 $\\lambda = 1.0$。\n\n- 案例 B（精确共线性，接近无惩罚极限）：\n  - 令 $x_1' = [-4,-3,-2,-1,0,1,2,3]^{\\top}$。\n  - 令 $x_2' = x_1'$。\n  - 令 $x_3' = [1,-2,3,-4,4,-3,2,-1]^{\\top}$。\n  - 构造 $X' = [x_1', x_2', x_3']$ 和 $y' = 3\\,x_1' + 0.5\\,x_3'$。\n  - 使用 $\\lambda = 1\\times 10^{-8}$。\n\n- 案例 C（高相关性，强正则化）：\n  - 使用与案例 A 相同的 $X$ 和 $y$。\n  - 使用 $\\lambda = 1\\times 10^{6}$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含按 A、B、C 顺序排列的三个案例的结果列表。\n- 每个案例的结果是一个列表 $[r, \\mathrm{df}]$，其中 $r$ 是比率，$\\mathrm{df}$ 是有效自由度，两者都四舍五入到6位小数。\n- 最终输出必须是严格符合以下格式的单行内容：\n  - $[[r_A,\\mathrm{df}_A],[r_B,\\mathrm{df}_B],[r_C,\\mathrm{df}_C]]$\n- 不应打印任何额外文本。", "solution": "所述问题是一个良构的数值线性代数练习，其基础是正则化线性模型的原理。所有数据、参数和程序都得到了明确定义，并且该任务在科学上和数学上都是合理的。其中不存在不一致、歧義或事實錯誤。因此，该问题被认定为有效。\n\n问题的核心是使用一种基于奇异值分解（SVD）的数值稳定方法，来计算岭回归系数向量 $\\hat{\\beta}_{\\lambda}$ 和有效自由度 $\\mathrm{df}(\\lambda)$。\n\n### 预处理\n\n令设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，响应向量为 $y \\in \\mathbb{R}^{n}$。\n首先，$X$ 矩阵的每一列 $x_j$ 都被标准化，使其均值为0，标准差为1。令 $\\mu_j = \\frac{1}{n} \\sum_{i=1}^{n} X_{ij}$ 和 $\\sigma_j = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (X_{ij} - \\mu_j)^2}$ 分别为第 $j$ 列的均值和总体标准差。标准化后的矩阵（在接下来的推导中，为简单起见，我们仍将其表示为 $X$）的列为 $x'_j = (x_j - \\mu_j) / \\sigma_j$。如果 $\\sigma_j=0$，则标准化后的列是零向量。\n响应向量 $y$ 被中心化，使其均值为0。令 $\\mu_y = \\frac{1}{n} \\sum_{i=1}^{n} y_i$。中心化后的向量（同样表示为 $y$）是 $y' = y - \\mu_y$。\n\n### 岭回归估计量\n\n岭回归估计量 $\\hat{\\beta}_{\\lambda}$ 是使罚最小二乘目标函数最小化的向量 $\\beta \\in \\mathbb{R}^{p}$：\n$$\nL(\\beta) = \\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_2^2\n$$\n其中 $\\lambda \\ge 0$ 是正则化参数。为了找到最小化器，我们计算 $L(\\beta)$ 关于 $\\beta$ 的梯度，并将其设为零：\n$$\n\\nabla_{\\beta} L(\\beta) = \\nabla_{\\beta} \\left( (y - X\\beta)^T(y - X\\beta) + \\lambda\\beta^T\\beta \\right)\n$$\n$$\n\\nabla_{\\beta} L(\\beta) = \\nabla_{\\beta} \\left( y^Ty - 2y^TX\\beta + \\beta^TX^TX\\beta + \\lambda\\beta^T\\beta \\right)\n$$\n$$\n\\nabla_{\\beta} L(\\beta) = -2X^Ty + 2X^TX\\beta + 2\\lambda\\beta\n$$\n将梯度设为零，得到岭回归的正规方程：\n$$\n(X^TX + \\lambda I)\\hat{\\beta}_{\\lambda} = X^Ty\n$$\n这导出了形式解 $\\hat{\\beta}_{\\lambda} = (X^TX + \\lambda I)^{-1}X^Ty$。然而，直接对矩阵 $(X^TX + \\lambda I)$ 求逆可能在数值上不稳定，特别是当 $X$ 是病態的（即其列高度相关）时。\n\n一种更稳定的方法是利用 $X$ 的奇异值分解（SVD）。令 $n \\times p$ 矩阵 $X$ 的 SVD 为：\n$$\nX = U D V^T\n$$\n其中 $U$ 是一个具有标准正交列的 $n \\times p$ 矩阵（$U^TU = I_p$），$D$ 是一个 $p \\times p$ 的对角矩阵，其对角线上的元素是奇异值 $d_1, d_2, \\ldots, d_p$，$V$ 是一个 $p \\times p$ 的正交矩阵（$V^TV = VV^T = I_p$）。请注意，由于在测试用例中 $n \\ge p$，我们使用的是经济型 SVD。\n\n将 SVD 代入正规方程：\n$$\n\\hat{\\beta}_{\\lambda} = ( (VD^TU^T)(UDV^T) + \\lambda I )^{-1} (VD^TU^T)y\n$$\n$$\n\\hat{\\beta}_{\\lambda} = ( V D^T D V^T + \\lambda V I V^T )^{-1} V D^T U^T y\n$$\n$$\n\\hat{\\beta}_{\\lambda} = ( V(D^2 + \\lambda I)V^T )^{-1} V D^T U^T y\n$$\n使用属性 $(ABC)^{-1} = C^{-1}B^{-1}A^{-1}$：\n$$\n\\hat{\\beta}_{\\lambda} = (V^T)^{-1} (D^2 + \\lambda I)^{-1} V^{-1} V D^T U^T y\n$$\n因为 $V$ 是正交的，所以 $(V^T)^{-1} = V$ 且 $V^{-1}=V^T$。\n$$\n\\hat{\\beta}_{\\lambda} = V (D^2 + \\lambda I)^{-1} D^T U^T y\n$$\n因为 $D$（以及 $D^T$）是对角矩阵，所以矩阵 $(D^2 + \\lambda I)^{-1}D^T$ 也是对角矩阵。其第 $j$ 个对角元素是 $d_j / (d_j^2 + \\lambda)$。这个公式是数值稳定的，因为它避免了矩阵求逆，并且当 $\\lambda>0$ 时，项 $d_j^2+\\lambda$ 严格为正。该计算涉及稳定的运算：SVD、矩阵-向量乘积和逐元素缩放。\n\n然后计算系数比率 $r = \\hat{\\beta}_{\\lambda,1} / \\hat{\\beta}_{\\lambda,2}$。\n\n### 有效自由度\n\n岭回归的拟合值由 $\\hat{y} = X\\hat{\\beta}_{\\lambda}$ 给出。我们可以将 $\\hat{y}$ 表示为 $y$ 的线性变换：\n$$\n\\hat{y} = X \\left( (X^TX + \\lambda I)^{-1}X^T y \\right) = S_{\\lambda} y\n$$\n矩阵 $S_{\\lambda} = X(X^TX + \\lambda I)^{-1}X^T$ 被称为岭回归的平滑矩阵或帽子矩阵。有效自由度 $\\mathrm{df}(\\lambda)$ 定义为该矩阵的迹：\n$$\n\\mathrm{df}(\\lambda) = \\mathrm{tr}(S_{\\lambda}) = \\mathrm{tr}(X(X^TX + \\lambda I)^{-1}X^T)\n$$\n利用迹的循环性质 $\\mathrm{tr}(ABC) = \\mathrm{tr}(CAB)$，我们可以写出：\n$$\n\\mathrm{df}(\\lambda) = \\mathrm{tr}((X^TX + \\lambda I)^{-1}X^TX)\n$$\n现在，将 $X$ 的 SVD 代入此表达式：\n$$\nX^TX = (UDV^T)^T(UDV^T) = VD^TU^TUDV^T = VD^2V^T\n$$\n$$\n(X^TX + \\lambda I)^{-1} = (VD^2V^T + \\lambda VIV^T)^{-1} = (V(D^2 + \\lambda I)V^T)^{-1} = V(D^2 + \\lambda I)^{-1}V^T\n$$\n因此，\n$$\n\\mathrm{df}(\\lambda) = \\mathrm{tr} \\left( V(D^2 + \\lambda I)^{-1}V^T \\cdot VD^2V^T \\right)\n$$\n$$\n\\mathrm{df}(\\lambda) = \\mathrm{tr} \\left( V(D^2 + \\lambda I)^{-1}D^2V^T \\right)\n$$\n再次利用迹的循环性质，$\\mathrm{tr}(V A V^T) = \\mathrm{tr}(A V^T V) = \\mathrm{tr}(A)$：\n$$\n\\mathrm{df}(\\lambda) = \\mathrm{tr} \\left( (D^2 + \\lambda I)^{-1}D^2 \\right)\n$$\n由于 $(D^2 + \\lambda I)^{-1}D^2$ 是一个对角矩阵，其对角元素为 $d_j^2 / (d_j^2 + \\lambda)$，所以它的迹是这些元素的总和：\n$$\n\\mathrm{df}(\\lambda) = \\sum_{j=1}^{p} \\frac{d_j^2}{d_j^2 + \\lambda}\n$$\n该公式提供了一种直接、高效且数值稳定的方法，用于根据 $X$ 的奇异值计算有效自由度。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the specified ridge regression analysis for three test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    x1_a = np.array([-3, -2, -1, 0, 1, 2, 3, 4, 5, 6])\n    delta_a = np.array([0.1, -0.2, 0.05, -0.05, 0.02, -0.01, 0.08, -0.03, 0.04, -0.02])\n    x2_a = 0.99 * x1_a + delta_a\n    x3_a = np.array([2, -1, 3, -2, 0, 1, -3, 2, -2, 4])\n    X_a = np.vstack([x1_a, x2_a, x3_a]).T\n    y_a = 1.5 * x1_a + 1.5 * x2_a + 0.2 * x3_a\n    \n    x1_b = np.array([-4, -3, -2, -1, 0, 1, 2, 3])\n    x2_b = x1_b.copy()\n    x3_b = np.array([1, -2, 3, -4, 4, -3, 2, -1])\n    X_b = np.vstack([x1_b, x2_b, x3_b]).T\n    y_b = 3 * x1_b + 0.5 * x3_b\n\n    test_cases = [\n        {\"X\": X_a, \"y\": y_a, \"lambda\": 1.0},\n        {\"X\": X_b, \"y\": y_b, \"lambda\": 1e-8},\n        {\"X\": X_a, \"y\": y_a, \"lambda\": 1e6},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        X, y, lambda_val = case[\"X\"], case[\"y\"], case[\"lambda\"]\n        n, p = X.shape\n\n        # Preprocessing:\n        # 1. Center y\n        y_centered = y - np.mean(y)\n        \n        # 2. Standardize X\n        X_mean = np.mean(X, axis=0)\n        # Use ddof=0 for population standard deviation to match problem's context\n        X_std = np.std(X, axis=0)\n        \n        # Standardize X, handling columns with zero standard deviation\n        X_scaled = np.zeros_like(X, dtype=float)\n        non_zero_std = X_std > 1e-12 # A small tolerance for floating point\n        X_scaled[:, non_zero_std] = (X[:, non_zero_std] - X_mean[non_zero_std]) / X_std[non_zero_std]\n\n        # Singular Value Decomposition of the standardized matrix\n        # Use economy SVD since n >= p\n        U, s, Vt = np.linalg.svd(X_scaled, full_matrices=False)\n        V = Vt.T\n\n        # Compute the ridge coefficient vector beta_hat\n        # beta_hat = V @ diag(s / (s^2 + lambda)) @ U.T @ y_centered\n        # This is implemented efficiently without forming diagonal matrices\n        tmp = U.T @ y_centered\n        d_term = s / (s**2 + lambda_val)\n        beta_hat = V @ (d_term * tmp)\n\n        # Compute redistribution metric r\n        # The problem statement implies this won't be a division by zero for the cases.\n        if abs(beta_hat[1])  1e-12:\n            r = np.inf if beta_hat[0] > 0 else -np.inf if beta_hat[0]  0 else np.nan\n        else:\n            r = beta_hat[0] / beta_hat[1]\n\n        # Compute effective degrees of freedom df\n        # df = sum(s_j^2 / (s_j^2 + lambda))\n        s_squared = s**2\n        df = np.sum(s_squared / (s_squared + lambda_val))\n\n        # Rounding and appending results\n        r_rounded = round(r, 6)\n        df_rounded = round(df, 6)\n        results.append([r_rounded, df_rounded])\n\n    # Format the final output string exactly as required\n    formatted_results = \",\".join([f\"[{res[0]},{res[1]}]\" for res in results])\n    print(f\"[{formatted_results}]\")\n\nsolve()\n```", "id": "4605283"}]}