{"hands_on_practices": [{"introduction": "本节练习旨在从第一性原理出发，构建轮廓系数（Silhouette Coefficient）这一核心的聚类评估指标。通过亲手实现该算法，您将深入理解其如何通过比较簇内凝聚度与簇间分离度来量化聚类质量。此外，本练习将引导您在一系列假设的生物信息学场景中，探索Z-score标准化、最小-最大缩放等不同数据预处理方法对最终评估结果的决定性影响，从而揭示特征缩放在基于距离的分析中的关键作用。[@problem_id:4561589]", "problem": "您会获得多个小型特征矩阵，代表生物信息学和医疗数据分析场景，其中已执行聚类并指定了簇标签。您必须通过计算每个样本基于簇内凝聚度和最近簇间分离度的标准化指数来评估聚类结果，然后生成所有样本的总体平均指数。评估必须在不同的预处理流程下完成，包括特征缩放、归一化和方差稳定化，并使用欧几里得度量。\n\n使用的基本定义：\n- 设数据集为 $\\mathbb{R}^d$中的一组点 $\\{x_i\\}_{i=1}^n$。\n- 设相异性由 $\\mathbb{R}^d$ 上的度量 $d(\\cdot,\\cdot)$ 定义；具体地，使用欧几里得度量 $d(x,y)=\\sqrt{\\sum_{j=1}^d (x_j - y_j)^2}$。\n- 设聚类是将 $\\{1,\\dots,n\\}$ 划分为 $K$ 个不相交的非空索引集 $\\{C_k\\}_{k=1}^K$，其中每个 $C_k$ 索引属于簇 $k$ 的样本。样本 $i$ 的簇标签是一个整数 $L_i \\in \\{0,1,\\dots,K-1\\}$，它将样本 $i$ 映射到其所在的簇。\n- 对于每个样本 $i$，将平均簇内相异性 $a(i)$ 定义为 $x_i$ 与其所在簇中所有其他样本 $j$（$j \\neq i$）的 $d(x_i, x_j)$ 的平均值。如果包含 $i$ 的簇大小为 $1$，则按惯例直接将该样本的标准化指数设置为 $0$。\n- 对于每个样本 $i$，定义最近的其他簇：对每个簇 $k \\neq L_i$，计算从 $i$ 到 $C_k$ 中所有样本的平均相异性，并令 $b(i)$ 为这些平均相异性的最小值。\n- 从 $a(i)$ 和 $b(i)$ 推导出每个样本的标准化指数，该指数有界于 $\\left[-1, 1\\right]$，当 $x_i$ 与其自身簇成员的平均距离小于其与最近其他簇成员的平均距离时，该指数增加；如果在您推导的表达式中归一化分母为 $0$，则为保持数值稳定性，将该指数设置为 $0$。总体评估指标，即轮廓系数 (SC)，定义为所有样本的该标准化指数的平均值。\n\n计算度量前应用的预处理流程：\n- Z-score标准化：对于每个特征 $j \\in \\{1,\\dots,d\\}$，计算所有样本上该特征的均值 $\\mu_j$ 和标准差 (SD) $\\sigma_j$，并将每个 $x_{ij}$ 转换为 $(x_{ij} - \\mu_j)/\\sigma_j$。如果 $\\sigma_j = 0$，则为避免除以零，将所有样本的该转换后特征值设置为 $0$。\n- 最小-最大缩放：对于每个特征 $j$，计算所有样本上该特征的最小值 $m_j$ 和最大值 $M_j$，并将每个 $x_{ij}$ 转换为 $(x_{ij} - m_j)/(M_j - m_j)$。如果 $M_j = m_j$，则为避免除以零，将所有样本的该转换后特征值设置为 $0$。\n- 计数数据的方差稳定化：对于计数型特征，对数据矩阵逐元素应用自然对数变换 $\\log(1+x)$ (log-one-plus)，然后如上所述应用Z-score标准化。\n\n距离计算必须在应用指定的预处理流程之后执行。使用欧几里得距离，不加任何额外权重。\n\n约定和处理方式：\n- 如果只有一个簇 ($K=1$)，则定义总体SC为 $0$。\n- 如果一个样本属于单例簇（簇大小为 $1$），则将其样本标准化指数设置为 $0$。\n- 如果标准化指数中的归一化分母为 $0$，则将该样本的标准化指数设置为 $0$。\n\n测试套件规范：\n为以下每个测试用例计算总体SC。每个用例都指定了一个数据矩阵、一个标签向量和一个预处理流程。\n\n- 测试用例1（理想情况，混合尺度数据使用Z-score标准化）：\n  数据矩阵 $X_{\\text{mixed}} \\in \\mathbb{R}^{6 \\times 3}$，行向量为：\n  $\\left[100.0, 1.2, 5000.0\\right]$,\n  $\\left[110.0, 0.8, 5100.0\\right]$,\n  $\\left[95.0, 1.0, 4900.0\\right]$,\n  $\\left[5.0, 50.0, 180.0\\right]$,\n  $\\left[7.0, 49.5, 200.0\\right]$,\n  $\\left[6.0, 48.0, 220.0\\right]$.\n  标签 $L_{\\text{mixed}}$：\n  $\\left[0, 0, 0, 1, 1, 1\\right]$.\n  预处理流程：Z-score标准化。\n\n- 测试用例2（混合尺度数据不进行缩放的效果）：\n  使用与测试用例1相同的 $X_{\\text{mixed}}$ 和 $L_{\\text{mixed}}$。\n  预处理流程：无。\n\n- 测试用例3（边界条件，包含单例簇并使用最小-最大缩放）：\n  数据矩阵 $X_{\\text{singleton}} \\in \\mathbb{R}^{5 \\times 2}$，行向量为：\n  $\\left[1.0, 100.0\\right]$,\n  $\\left[1.1, 98.0\\right]$,\n  $\\left[10.0, 5.0\\right]$,\n  $\\left[9.5, 6.0\\right]$,\n  $\\left[100.0, 1000.0\\right]$.\n  标签 $L_{\\text{singleton}}$：\n  $\\left[0, 0, 1, 1, 2\\right]$.\n  预处理流程：最小-最大缩放。\n\n- 测试用例4（计数数据不进行方差稳定化）：\n  数据矩阵 $X_{\\text{counts}} \\in \\mathbb{R}^{9 \\times 4}$，行向量为：\n  $\\left[0, 10, 500, 2000\\right]$,\n  $\\left[1, 12, 520, 2100\\right]$,\n  $\\left[0, 9, 480, 1950\\right]$,\n  $\\left[50, 0, 30, 0\\right]$,\n  $\\left[45, 1, 28, 2\\right]$,\n  $\\left[55, 2, 35, 1\\right]$,\n  $\\left[0, 300, 0, 100\\right]$,\n  $\\left[1, 290, 0, 90\\right]$,\n  $\\left[2, 310, 1, 110\\right]$.\n  标签 $L_{\\text{counts}}$：\n  $\\left[0, 0, 0, 1, 1, 1, 2, 2, 2\\right]$.\n  预处理流程：无。\n\n- 测试用例5（计数数据使用 $\\log(1+x)$ 进行方差稳定化，然后进行Z-score标准化）：\n  使用与测试用例4相同的 $X_{\\text{counts}}$ 和 $L_{\\text{counts}}$。\n  预处理流程：log-one-plus，然后进行Z-score标准化。\n\n- 测试用例6（边缘情况，在Z-score标准化下存在零方差特征）：\n  数据矩阵 $X_{\\text{zerovar}} \\in \\mathbb{R}^{6 \\times 3}$，行向量为：\n  $\\left[1.0, 2.0, 10.0\\right]$,\n  $\\left[1.2, 1.8, 10.0\\right]$,\n  $\\left[0.8, 2.1, 10.0\\right]$,\n  $\\left[5.0, 6.0, 10.0\\right]$,\n  $\\left[5.2, 6.1, 10.0\\right]$,\n  $\\left[4.8, 5.9, 10.0\\right]$.\n  标签 $L_{\\text{zerovar}}$：\n  $\\left[0, 0, 0, 1, 1, 1\\right]$.\n  预处理流程：Z-score标准化。\n\n程序要求：\n- 严格按照规定实现预处理流程。\n- 预处理后使用欧几里得度量作为相异性。\n- 根据上述 $a(i)$ 和 $b(i)$ 的定义，推导并实现每个样本的标准化指数，并计算总体SC作为所有样本的平均值。\n\n输出格式：\n- 您的程序应生成单行输出，其中包含 $6$ 个测试用例的总体SC，四舍五入到 $6$ 位小数，并聚合为逗号分隔的列表，用方括号括起来，例如，$\\left[\\text{sc}_1,\\text{sc}_2,\\dots,\\text{sc}_6\\right]$，以Python风格的列表语法打印。\n\n此问题不涉及物理单位和角度。所有输出均为实值浮点数。程序必须是自包含的，无需输入，并且可以按原样运行。确保计算和约定遵循上述规范。", "solution": "对用户提供的问题陈述进行严格验证。\n\n### 步骤1：提取给定信息\n- **数据集**：$\\mathbb{R}^d$中的一组点 $\\{x_i\\}_{i=1}^n$。\n- **相异性度量**：欧几里得距离，$d(x,y)=\\sqrt{\\sum_{j=1}^d (x_j - y_j)^2}$。\n- **聚类**：将样本索引划分为 $K$ 个不相交的非空集合 $\\{C_k\\}_{k=1}^K$。样本 $i$ 的标签为 $L_i \\in \\{0, 1, \\dots, K-1\\}$。\n- **簇内相异性 $a(i)$**：对于样本 $i$，$a(i)$ 是其到同一簇中所有其他样本 $j$ 的平均相异性：$a(i) = \\frac{1}{|C_{L_i}| - 1} \\sum_{j \\in C_{L_i}, j \\neq i} d(x_i, x_j)$。\n- **簇间相异性 $b(i)$**：对于样本 $i$，$b(i)$ 是其到任何其他簇 $C_k$（$k \\neq L_i$）中所有样本的平均相异性的最小值：$b(i) = \\min_{k \\neq L_i} \\left\\{ \\frac{1}{|C_k|} \\sum_{j \\in C_k} d(x_i, x_j) \\right\\}$。\n- **每个样本的标准化指数 $s(i)$**：从 $a(i)$ 和 $b(i)$ 导出，有界于 $[-1, 1]$。\n- **总体评估指标（轮廓系数，SC）**：所有样本的标准化指数的平均值：$SC = \\frac{1}{n} \\sum_{i=1}^n s(i)$。\n- **预处理流程**：\n  1. **Z-score标准化**：将每个特征值 $x_{ij}$ 转换为 $(x_{ij} - \\mu_j)/\\sigma_j$，其中 $\\mu_j$ 是特征均值，$\\sigma_j$ 是特征标准差。如果 $\\sigma_j=0$，转换后的特征为 $0$。\n  2. **最小-最大缩放**：将每个特征值 $x_{ij}$ 转换为 $(x_{ij} - m_j)/(M_j - m_j)$，其中 $m_j$ 和 $M_j$ 分别是特征的最小值和最大值。如果 $M_j = m_j$，转换后的特征为 $0$。\n  3. **计数数据的方差稳定化**：逐元素应用自然对数变换 $\\log(1+x)$，然后应用Z-score标准化。\n- **约定**：\n  1. 如果 $K=1$，总体SC为 $0$。\n  2. 如果样本属于单例簇，其样本指数为 $0$。\n  3. 如果标准化指数中的归一化分母为 $0$，则该指数设为 $0$。\n- **测试用例**：提供了六个具体的测试用例，每个都包含数据矩阵、标签向量和指定的预处理流程。\n\n### 步骤2：使用提取的给定信息进行验证\n根据验证标准对问题进行分析：\n- **科学依据**：该问题要求计算轮廓系数，这是机器学习和数据分析中评估聚类质量的标准且广泛使用的指标。簇内凝聚度 ($a(i)$) 和簇间分离度 ($b(i)$) 的定义、每个样本分数的公式以及预处理方法（Z-score、最小-最大、对数变换）都是来自统计学和数据科学的、数学上合理的标准概念。该问题牢固地植根于已建立的原则。\n- **定义明确**：该问题是适定的。对于每个测试用例，输入（数据、标签、预处理方法）都已明确定义。所有相关边缘情况（单例簇、总体只有一个簇、除以零）的计算步骤和处理方式都已明确规定。这种确定性确保了每个测试用例都存在唯一且有意义的数值解。\n- **客观性**：问题以精确、客观的数学语言陈述。没有主观或模糊的术语。\n- **完整性和一致性**：问题是自包含的。它提供了求解所需的所有必要数据和定义。处理边缘情况的约定是明确的，并且与标准实践一致，避免了歧义。\n- **现实性**：测试用例虽然是合成的，但旨在模拟生物信息学中遇到的现实场景，例如具有巨大尺度差异的特征和基于计数的数据，这证明了使用不同预处理策略的合理性。任务本身——在各种数据转换下评估聚类——是实际数据分析中的核心活动。\n\n### 步骤3：结论与行动\n该问题是**有效的**。它是一个定义明确、科学上合理的计算任务，基于已建立的数据分析原则。我将继续进行求解。\n\n###\n该问题要求计算多个聚类结果的轮廓系数 (SC)，每个结果都针对不同的数据集，并经过特定的预处理流程。解决方案涉及系统地应用数据转换，然后根据其形式化定义计算SC。\n\n问题的核心是轮廓系数，一个量化聚类定义优良程度的指标。每个样本的轮廓分数 $s(i)$ 定义为：\n$$\ns(i) = \\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}}\n$$\n其中：\n- $a(i)$ 是样本 $i$ 到同一簇中所有其他样本的平均距离。这衡量了样本与其自身簇的凝聚度。值越小越好。\n- $b(i)$ 是样本 $i$ 到最近的相邻簇中所有样本的平均距离。这衡量了样本与其他簇的分离度。值越大越好。\n\n$s(i)$ 的值范围从 $-1$ 到 $1$。接近 $1$ 的值表示样本聚类良好，因为其簇内距离远小于其最近的簇间距离。接近 $0$ 的值表示样本位于或非常接近两个簇之间的决策边界。接近 $-1$ 的值表示样本可能被错误分类。总体SC是所有样本的 $s(i)$ 的平均值，提供了对聚类质量的全局度量。\n\n问题指定了在距离计算之前要应用于数据的三种预处理流程：\n1. **Z-score标准化（Standardization）**：这将每个特征转换为均值为 $0$、标准差为 $1$。对于一个特征向量 $X_j = [x_{1j}, \\dots, x_{nj}]^T$，转换公式为：\n    $$ x'_{ij} = \\frac{x_{ij} - \\mu_j}{\\sigma_j} $$\n    其中 $\\mu_j = \\frac{1}{n} \\sum_{i=1}^n x_{ij}$ 且 $\\sigma_j = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_{ij} - \\mu_j)^2}$。如果 $\\sigma_j = 0$，转换后的特征 $X'_j$ 变为零向量。当特征具有不同尺度且数据近似正态分布时，此方法很有效。\n\n2. **最小-最大缩放（Normalization）**：这将每个特征缩放到一个固定范围，通常是 $[0, 1]$。对于一个特征向量 $X_j$，转换公式为：\n    $$ x'_{ij} = \\frac{x_{ij} - \\min(X_j)}{\\max(X_j) - \\min(X_j)} $$\n    如果范围 $\\max(X_j) - \\min(X_j) = 0$，转换后的特征 $X'_j$ 变为零向量。此方法对异常值敏感，但保留了原始分布的形状。\n\n3. **方差稳定化**：此流程专为计数数据设计，这类数据通常表现出均值-方差关系（例如，在泊松分布数据中，方差等于均值）。变换 $x \\to \\log(1+x)$ 有助于稳定方差，使数据更适用于像Z-score标准化这样假定方差齐性的方法。之后如上所述进行Z-score标准化。\n\n距离度量固定为欧几里得距离，在预处理后的数据 $X'$ 上计算：\n$$ d(x'_i, x'_j) = \\sqrt{\\sum_{k=1}^d (x'_{ik} - x'_{jk})^2} $$\n\n对于每个测试用例，总体算法流程如下：\n1. 将指定的预处理流程应用于输入数据矩阵 $X$，得到转换后的矩阵 $X'$。\n2. 确定唯一簇的数量 $K$。如果 $K \\le 1$，根据定义SC为 $0$。\n3. 计算 $X'$ 中所有样本的成对欧几里得距离矩阵。\n4. 对于每个样本 $i$：\n    a. 如果样本 $i$ 位于单例簇中，其分数 $s(i)$ 为 $0$。\n    b. 否则，计算 $a(i)$，即到其簇中其他点的平均距离。\n    c. 计算 $b(i)$，即到每个其他簇中点的平均距离的最小值。\n    d. 计算 $s(i) = (b(i) - a(i))/\\max\\{a(i), b(i)\\}$。如果分母为 $0$，$s(i)$ 为 $0$。\n5. 最终的SC是所有样本分数 $s(i)$ 的算术平均值。\n\n这种结构化方法确保了每个测试用例都根据指定的规则、预处理和数学定义得到正确评估。", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing the Silhouette Coefficient (SC)\n    for six test cases, each with a specified preprocessing pipeline.\n    \"\"\"\n\n    def preprocess_data(X, method):\n        \"\"\"\n        Applies a specified preprocessing pipeline to the data matrix X.\n\n        Args:\n            X (np.ndarray): The input data matrix (n_samples, n_features).\n            method (str): The preprocessing method ('zscore', 'minmax', 'log_zscore', 'none').\n\n        Returns:\n            np.ndarray: The preprocessed data matrix.\n        \"\"\"\n        if method == 'none':\n            return X.copy()\n        \n        X_proc = X.copy().astype(float)\n        \n        if method == 'log_zscore':\n            X_proc = np.log1p(X_proc)\n            # Fall through to 'zscore'\n            method = 'zscore'\n\n        if method == 'zscore':\n            mean = np.mean(X_proc, axis=0)\n            std = np.std(X_proc, axis=0)\n            # Avoid division by zero for zero-variance features\n            # np.divide handles this with a 'where' argument\n            return np.divide(X_proc - mean, std, out=np.zeros_like(X_proc), where=std!=0)\n        \n        if method == 'minmax':\n            min_val = np.min(X_proc, axis=0)\n            max_val = np.max(X_proc, axis=0)\n            data_range = max_val - min_val\n            # Avoid division by zero for zero-range features\n            return np.divide(X_proc - min_val, data_range, out=np.zeros_like(X_proc), where=data_range!=0)\n            \n        return X_proc\n\n    def calculate_silhouette_score(X, labels):\n        \"\"\"\n        Computes the overall Silhouette Coefficient for a given dataset and labels.\n\n        Args:\n            X (np.ndarray): The data matrix (n_samples, n_features), potentially preprocessed.\n            labels (np.ndarray): The cluster labels for each sample.\n\n        Returns:\n            float: The mean Silhouette Coefficient for all samples.\n        \"\"\"\n        unique_labels = np.unique(labels)\n        n_clusters = len(unique_labels)\n        n_samples = X.shape[0]\n\n        if n_clusters = 1 or n_samples  2:\n            return 0.0\n\n        # Pre-compute the pairwise Euclidean distance matrix\n        dist_matrix = squareform(pdist(X, metric='euclidean'))\n\n        sample_scores = np.zeros(n_samples)\n\n        for i in range(n_samples):\n            current_label = labels[i]\n            \n            # Mask for samples in the same cluster as sample i (excluding i)\n            in_cluster_mask = (labels == current_label)\n            in_cluster_mask[i] = False\n            \n            n_in_cluster = np.sum(in_cluster_mask)\n            \n            # If singleton cluster, silhouette score is 0 by convention\n            if n_in_cluster == 0:\n                sample_scores[i] = 0.0\n                continue\n\n            # Calculate a(i): average intra-cluster distance\n            a_i = np.mean(dist_matrix[i, in_cluster_mask])\n            \n            # Calculate b(i): minimum average inter-cluster distance\n            b_i = np.inf\n            for label in unique_labels:\n                if label == current_label:\n                    continue\n                \n                # Mask for samples in the other cluster\n                out_cluster_mask = (labels == label)\n                mean_dist_to_other_cluster = np.mean(dist_matrix[i, out_cluster_mask])\n                b_i = min(b_i, mean_dist_to_other_cluster)\n            \n            # Calculate the silhouette score for sample i\n            denominator = max(a_i, b_i)\n            if denominator == 0:\n                sample_scores[i] = 0.0 # Handle case for numerical stability\n            else:\n                sample_scores[i] = (b_i - a_i) / denominator\n        \n        return np.mean(sample_scores)\n\n    # Test case definitions\n    test_cases = [\n        {\n            \"data\": np.array([\n                [100.0, 1.2, 5000.0], [110.0, 0.8, 5100.0], [95.0, 1.0, 4900.0],\n                [5.0, 50.0, 180.0], [7.0, 49.5, 200.0], [6.0, 48.0, 220.0]\n            ]),\n            \"labels\": np.array([0, 0, 0, 1, 1, 1]),\n            \"preprocess\": \"zscore\"\n        },\n        {\n            \"data\": np.array([\n                [100.0, 1.2, 5000.0], [110.0, 0.8, 5100.0], [95.0, 1.0, 4900.0],\n                [5.0, 50.0, 180.0], [7.0, 49.5, 200.0], [6.0, 48.0, 220.0]\n            ]),\n            \"labels\": np.array([0, 0, 0, 1, 1, 1]),\n            \"preprocess\": \"none\"\n        },\n        {\n            \"data\": np.array([\n                [1.0, 100.0], [1.1, 98.0], [10.0, 5.0], [9.5, 6.0], [100.0, 1000.0]\n            ]),\n            \"labels\": np.array([0, 0, 1, 1, 2]),\n            \"preprocess\": \"minmax\"\n        },\n        {\n            \"data\": np.array([\n                [0, 10, 500, 2000], [1, 12, 520, 2100], [0, 9, 480, 1950],\n                [50, 0, 30, 0], [45, 1, 28, 2], [55, 2, 35, 1],\n                [0, 300, 0, 100], [1, 290, 0, 90], [2, 310, 1, 110]\n            ]),\n            \"labels\": np.array([0, 0, 0, 1, 1, 1, 2, 2, 2]),\n            \"preprocess\": \"none\"\n        },\n        {\n            \"data\": np.array([\n                [0, 10, 500, 2000], [1, 12, 520, 2100], [0, 9, 480, 1950],\n                [50, 0, 30, 0], [45, 1, 28, 2], [55, 2, 35, 1],\n                [0, 300, 0, 100], [1, 290, 0, 90], [2, 310, 1, 110]\n            ]),\n            \"labels\": np.array([0, 0, 0, 1, 1, 1, 2, 2, 2]),\n            \"preprocess\": \"log_zscore\"\n        },\n        {\n            \"data\": np.array([\n                [1.0, 2.0, 10.0], [1.2, 1.8, 10.0], [0.8, 2.1, 10.0],\n                [5.0, 6.0, 10.0], [5.2, 6.1, 10.0], [4.8, 5.9, 10.0]\n            ]),\n            \"labels\": np.array([0, 0, 0, 1, 1, 1]),\n            \"preprocess\": \"zscore\"\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        X = case[\"data\"]\n        labels = case[\"labels\"]\n        preprocess_method = case[\"preprocess\"]\n\n        X_processed = preprocess_data(X, preprocess_method)\n        sc = calculate_silhouette_score(X_processed, labels)\n        results.append(sc)\n\n    # Format the output as specified\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4561589"}, {"introduction": "精确的轮廓系数计算虽然是评估聚类质量的金标准，但其$O(n^2)$的时间复杂度在处理大规模基因组或临床数据集时往往变得不切实际。本练习将直面这一挑战，要求您同时实现精确算法与基于确定性采样的近似算法。通过比较二者的评估分数和计算开销，您将获得关于设计可扩展分析流程的实践经验，并深刻理解在大数据分析中固有的准确性与效率之间的权衡。[@problem_id:4561574]", "problem": "一个临床生物信息学团队通过比较簇内和簇间相异性，来评估一小组基因表达谱上的无监督聚类分配品质。考虑一个嵌入在维度为 $d$ 的实向量空间中的有限数据点集，其指定的度量是在 $\\mathbb{R}^d$ 上的标准欧几里得距离。该数据集被划分为带有标签的簇。一个点的评估指标应基于其与自身簇中点的平均相异性，以及与任何其他簇的最小平均相异性，再由这两个平均值中的较大者进行归一化，最后对所有点取平均以得出单一的数据集分数。如果一个点是其簇的唯一成员，则该点的分数定义为 $0$。如果某个点的平均簇内相异性和最小平均簇间相异性均为 $0$，则其点级别分数定义为 $0$。\n\n您必须实现两种算法，根据此定义计算数据集级别分数：\n\n- 一种精确算法，它一次性预先计算所有成对距离，然后计算数据集级别分数。对于精确算法，将距离评估计数定义为被评估距离的唯一无序点对的数量。如果有 $n$ 个点，此计数必须为 $n(n-1)/2$。\n\n- 一种带有采样参数 $m \\in \\mathbb{N}$ 的确定性近似算法。对于每个点 $i$，使用其自身簇中最多 $m$ 个其他点（如果可用点数少于 $m$，则使用所有可用点）来近似计算平均簇内相异性。用于近似的所选点必须是索引最小的簇成员，在适用情况下不包括 $i$ 本身。为了近似最小平均簇间相异性，对于每个其他簇，计算与该簇中最多 $m$ 个点（选择为该簇中索引最小的成员）的平均相异性，然后取这些平均值在所有簇上的最小值。该近似必须是确定性的，不含随机性。对于此近似算法，将距离评估计数定义为在所有点上执行的定向点对点距离计算总数，等于所有点的选定簇内伙伴数量之和，加上对所有其他簇的选定簇间伙伴数量之和。\n\n您的程序不得接受任何输入，而是必须计算以下测试套件的结果。每个测试案例指定一个数据矩阵 $X \\in \\mathbb{R}^{n \\times d}$、一个指示簇成员关系的整数标签向量 $y \\in \\mathbb{Z}^n$ 以及采样参数 $m$：\n\n测试案例 1（分离良好且平衡的簇，$m=2$）：\n$$\nX = \\begin{bmatrix}\n0  0\\\\\n0  1\\\\\n1  0\\\\\n1  1\\\\\n5  5\\\\\n5  6\\\\\n6  5\\\\\n6  6\n\\end{bmatrix},\\quad\ny = [0,0,0,0,1,1,1,1],\\quad\nm = 2.\n$$\n\n测试案例 2（单例簇，$m=2$）：\n$$\nX = \\begin{bmatrix}\n0  0\\\\\n0  0.1\\\\\n10  10\n\\end{bmatrix},\\quad\ny = [0,0,1],\\quad\nm = 2.\n$$\n\n测试案例 3（不平衡的簇，$m=1$）：\n$$\nX = \\begin{bmatrix}\n0  0\\\\\n0  1\\\\\n1  0\\\\\n3  0\\\\\n3  1\\\\\n4  0\\\\\n8  0\\\\\n8  1\n\\end{bmatrix},\\quad\ny = [0,0,0,1,1,1,2,2],\\quad\nm = 1.\n$$\n\n测试案例 4（簇内相异性为零的点对，$m=3$）：\n$$\nX = \\begin{bmatrix}\n0  0\\\\\n0  0\\\\\n10  0\\\\\n10  0\n\\end{bmatrix},\\quad\ny = [0,0,1,1],\\quad\nm = 3.\n$$\n\n对于每个测试案例，计算：\n\n- 使用所述精确算法计算的精确数据集级别分数，\n- 使用给定 $m$ 的确定性采样算法计算的近似数据集级别分数，\n- 精确算法的唯一无序对距离评估计数 $n(n-1)/2$，\n- 如上定义的近似算法的定向距离评估计数。\n\n数据集级别分数的结果需四舍五入至 $6$ 位小数。所有计数必须是整数。此问题中没有物理单位。\n\n您的程序应产生单行输出，其中包含一个结果列表，每个测试案例一个结果，每个结果的形式为 $[\\text{exact\\_score}, \\text{approx\\_score}, \\text{exact\\_count}, \\text{approx\\_count}]$ 的列表。该行必须包含一个由方括号括起来的逗号分隔列表，其中不含空格，例如：\n$[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],[a_3,b_3,c_3,d_3],[a_4,b_4,c_4,d_4]]$。\n\n约束与假设：\n\n- 所有距离均为 $\\mathbb{R}^d$ 上的欧几里得距离。\n- 如果一个点在其自身簇中没有其他点，根据定义，其点级别分数为 $0$，并且在近似算法中不贡献任何簇内距离计算。\n- 如果用于某点归一化的两个平均值中的最大值等于 $0$，则将该点的分数定义为 $0$ 以避免除以零。", "solution": "此问题要求实现两种算法，以评估给定数据聚类的品质。评估基于一种度量标准，该标准针对每个数据点，将其与同一簇中其他点的平均距离与其与其它簇中点的平均距离进行比较。这是无监督学习中一种常用方法，用于评估簇的内聚性（簇内点的紧密程度）和分离度（不同簇之间的分离程度）。指定的点级别分数是著名的轮廓系数的一种变体。\n\n设数据集为 $d$ 维欧几里得空间 $\\mathbb{R}^d$ 中的 $n$ 个点的集合 $\\{p_0, p_1, \\dots, p_{n-1}\\}$。这些点被划分为一组簇 $C = \\{C_1, C_2, \\dots, C_K\\}$。两点 $p_i$ 和 $p_j$ 之间的距离是欧几里得距离，记为 $d(p_i, p_j) = \\|p_i - p_j\\|_2$。\n\n对于每个点 $p_i$，问题定义了两个关键量：\n1.  平均簇内相异性 $a(i)$，即从 $p_i$ 到同一簇内所有其他点的平均距离。\n2.  最小平均簇间相异性 $b(i)$，即从 $p_i$ 到任何单一其他簇中所有点的平均距离的最小值。\n\n点级别分数 $s(i)$ 则定义为：\n$$s(i) = \\begin{cases}\n    \\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}}  \\text{if } \\max\\{a(i), b(i)\\}  0 \\\\\n    0  \\text{if } \\max\\{a(i), b(i)\\} = 0\n\\end{cases}$$\n对于一个点 $p_i$ 是其簇的唯一成员（单例簇）的情况，给出了一个特殊条件：其分数 $s(i)$ 定义为 $0$。\n\n整个数据集的分数是数据集中所有点的点级别分数的算术平均值：\n$$\\text{Dataset Score} = \\frac{1}{n} \\sum_{i=0}^{n-1} s(i)$$\n\n我们现在将详细说明计算此分数所需的两种算法。\n\n### 精确算法\n\n精确算法完全按照定义计算分数。它遵循一个两阶段过程：预先计算距离，然后计算分数。\n\n1.  **距离预计算**：$n$ 个点之间的所有唯一成对距离只计算一次。对于 $n$ 个点，存在 $\\binom{n}{2} = \\frac{n(n-1)}{2}$ 个这样的唯一无序对。这些距离储存在一个对称的 $n \\times n$ 距离矩阵 $D$ 中，其中 $D_{ij} = d(p_i, p_j)$。此算法的距离评估计数固定为此值：$\\text{exact\\_count} = \\frac{n(n-1)}{2}$。\n\n2.  **分数计算**：对于属于簇 $C_k$ 的每个点 $p_i$：\n    *   如果 $C_k$ 是一个单例簇（即 $|C_k| = 1$），则 $s(i) = 0$。\n    *   否则，簇内相异性 $a(i)$ 计算为到 $C_k$ 中所有其他点的距离的平均值：\n        $$a(i) = \\frac{1}{|C_k|-1} \\sum_{p_j \\in C_k, j \\neq i} D_{ij}$$\n    *   计算到每个其他簇 $C_l$（其中 $l \\neq k$）的平均相异性：\n        $$d(p_i, C_l) = \\frac{1}{|C_l|} \\sum_{p_j \\in C_l} D_{ij}$$\n    *   最小平均簇间相异性 $b(i)$ 是这些值的最小值：\n        $$b(i) = \\min_{l \\neq k} \\{d(p_i, C_l)\\}$$\n    *   然后使用提供的公式计算点级别分数 $s(i)$，最终的数据集分数是所有 $s(i)$ 的平均值。\n\n### 确定性近似算法\n\n此算法通过使用固定大小的点样本进行距离计算来近似分数，该过程由参数 $m \\in \\mathbb{N}$ 控制。采样是确定性的，基于点的索引。\n\n对于每个带有标签 $y_i$ 和索引 $i$ 的点 $p_i$：\n\n1.  **近似簇内相异性 $\\tilde{a}(i)$**：\n    *   如果 $p_i$ 在一个单例簇中，其分数 $\\tilde{s}(i)$ 为 $0$，并且不会为其执行任何簇内距离计算。\n    *   否则，识别其簇中其他点的索引集 $I = \\{j \\mid y_j = y_i, j \\neq i\\}$。从此集合中，选取最多 $m$ 个具有最小值的索引。设此采样索引集为 $I' \\subseteq I$，其中 $|I'| = \\min(m, |I|)$。\n    *   近似簇内相异性是到这些采样点的平均距离：\n        $$\\tilde{a}(i) = \\frac{1}{|I'|} \\sum_{j \\in I'} d(p_i, p_j)$$\n    *   此步骤的距离计算次数为 $|I'|$。\n\n2.  **近似簇间相异性 $\\tilde{b}(i)$**：\n    *   对于每个其他簇 $C_l$（其标签不为 $y_i$），识别其成员的索引集 $J_l = \\{j \\mid y_j = l\\}$。从此集合中，选取最多 $m$ 个具有最小值的索引。设此采样索引集为 $J'_l \\subseteq J_l$，其中 $|J'_l| = \\min(m, |J_l|)$。\n    *   到簇 $C_l$ 的近似平均距离为：\n        $$\\tilde{d}(p_i, C_l) = \\frac{1}{|J'_l|} \\sum_{j \\in J'_l} d(p_i, p_j)$$\n    *   此簇的距离计算次数为 $|J'_l|$。\n    *   近似最小平均簇间相异性 $\\tilde{b}(i)$ 是这些平均距离在所有其他簇 $l$ 上的最小值：\n        $$\\tilde{b}(i) = \\min_{l \\neq y_i} \\{\\tilde{d}(p_i, C_l)\\}$$\n\n3.  **分数与计数计算**：\n    *   近似点级别分数 $\\tilde{s}(i)$ 使用与精确方法相同的公式，通过 $\\tilde{a}(i)$ 和 $\\tilde{b}(i)$ 计算得出。\n    *   总距离评估计数 `approx_count` 是在所有点上执行的所有距离计算的总和。对于单个点 $p_i$，计数增加的量等于其簇内样本数加上来自所有其他簇的簇间样本数之和。\n\n最终的近似数据集分数是所有 $\\tilde{s}(i)$ 的平均值。此方法通过限制距离计算的次数（尤其对于大簇）来降低计算成本。", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\ndef compute_exact(X, y):\n    \"\"\"\n    Computes the exact dataset-level score and distance evaluation count.\n    \"\"\"\n    n = X.shape[0]\n    if n = 1:\n        return 0.0, 0\n    \n    # Precompute all pairwise distances. pdist computes n(n-1)/2 distances.\n    # This corresponds exactly to the definition of exact_count.\n    exact_count = n * (n - 1) // 2\n    dist_matrix = squareform(pdist(X, 'euclidean'))\n    \n    unique_labels = np.unique(y)\n    cluster_indices = {label: np.where(y == label)[0] for label in unique_labels}\n    \n    point_scores = np.zeros(n)\n    \n    for i in range(n):\n        label_i = y[i]\n        indices_in_cluster = cluster_indices[label_i]\n        \n        # Handle singleton cluster\n        if len(indices_in_cluster) == 1:\n            point_scores[i] = 0.0\n            continue\n            \n        # Calculate a(i): mean intra-cluster distance\n        mask_a = np.ones_like(indices_in_cluster, dtype=bool)\n        mask_a[indices_in_cluster == i] = False\n        other_indices_in_cluster = indices_in_cluster[mask_a]\n        a_i = np.mean(dist_matrix[i, other_indices_in_cluster])\n        \n        # Calculate b(i): smallest mean inter-cluster distance\n        mean_inter_dists = []\n        for other_label in unique_labels:\n            if other_label == label_i:\n                continue\n            indices_other_cluster = cluster_indices[other_label]\n            mean_dist = np.mean(dist_matrix[i, indices_other_cluster])\n            mean_inter_dists.append(mean_dist)\n            \n        b_i = np.min(mean_inter_dists) if mean_inter_dists else 0\n        \n        # Calculate point score s(i)\n        denominator = max(a_i, b_i)\n        if denominator == 0:\n            point_scores[i] = 0.0\n        else:\n            point_scores[i] = (b_i - a_i) / denominator\n            \n    exact_score = np.mean(point_scores)\n    return exact_score, exact_count\n\ndef compute_approx(X, y, m):\n    \"\"\"\n    Computes the approximate dataset-level score and distance evaluation count.\n    \"\"\"\n    n = X.shape[0]\n    if n = 1:\n        return 0.0, 0\n\n    approx_count = 0\n    point_scores = np.zeros(n)\n    \n    # Pre-sort indices for each cluster label for deterministic sampling\n    unique_labels = np.unique(y)\n    cluster_indices = {label: np.sort(np.where(y == label)[0]) for label in unique_labels}\n    \n    for i in range(n):\n        label_i = y[i]\n        indices_in_cluster = cluster_indices[label_i]\n        p_i = X[i]\n\n        # Handle singleton cluster\n        if len(indices_in_cluster) == 1:\n            point_scores[i] = 0.0\n            # Still need to calculate inter-cluster distances for the count\n            for other_label in unique_labels:\n                if other_label == label_i:\n                    continue\n                indices_other_cluster = cluster_indices[other_label]\n                num_samples = min(m, len(indices_other_cluster))\n                approx_count += num_samples\n            continue\n\n        # Calculate a(i): approximate mean intra-cluster distance\n        other_indices_in_cluster = indices_in_cluster[indices_in_cluster != i]\n        num_samples_a = min(m, len(other_indices_in_cluster))\n        sample_indices_a = other_indices_in_cluster[:num_samples_a]\n        \n        dists_a = np.linalg.norm(X[sample_indices_a] - p_i, axis=1)\n        a_i_tilde = np.mean(dists_a)\n        approx_count += num_samples_a\n\n        # Calculate b(i): approximate smallest mean inter-cluster distance\n        mean_inter_dists_tilde = []\n        for other_label in unique_labels:\n            if other_label == label_i:\n                continue\n            indices_other_cluster = cluster_indices[other_label]\n            num_samples_b = min(m, len(indices_other_cluster))\n            sample_indices_b = indices_other_cluster[:num_samples_b]\n            \n            dists_b = np.linalg.norm(X[sample_indices_b] - p_i, axis=1)\n            mean_dist_tilde = np.mean(dists_b)\n            mean_inter_dists_tilde.append(mean_dist_tilde)\n            approx_count += num_samples_b\n        \n        b_i_tilde = np.min(mean_inter_dists_tilde) if mean_inter_dists_tilde else 0\n        \n        # Calculate point score s(i)\n        denominator = max(a_i_tilde, b_i_tilde)\n        if denominator == 0:\n            point_scores[i] = 0.0\n        else:\n            point_scores[i] = (b_i_tilde - a_i_tilde) / denominator\n            \n    approx_score = np.mean(point_scores)\n    return approx_score, approx_count\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        (\n            np.array([[0,0], [0,1], [1,0], [1,1], [5,5], [5,6], [6,5], [6,6]], dtype=np.float64),\n            np.array([0,0,0,0,1,1,1,1]),\n            2\n        ),\n        (\n            np.array([[0,0], [0,0.1], [10,10]], dtype=np.float64),\n            np.array([0,0,1]),\n            2\n        ),\n        (\n            np.array([[0,0], [0,1], [1,0], [3,0], [3,1], [4,0], [8,0], [8,1]], dtype=np.float64),\n            np.array([0,0,0,1,1,1,2,2]),\n            1\n        ),\n        (\n            np.array([[0,0], [0,0], [10,0], [10,0]], dtype=np.float64),\n            np.array([0,0,1,1]),\n            3\n        )\n    ]\n\n    all_results = []\n    for X, y, m in test_cases:\n        exact_score, exact_count = compute_exact(X, y)\n        approx_score, approx_count = compute_approx(X, y, m)\n        \n        # Format results as specified\n        result_str = (\n            f\"[{exact_score:.6f},\"\n            f\"{approx_score:.6f},\"\n            f\"{exact_count},\"\n            f\"{approx_count}]\"\n        )\n        all_results.append(result_str)\n\n    # Print the final output in the required single-line format\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "4561574"}, {"introduction": "许多现代生物信息学数据（如单细胞测序数据）在高维空间中呈现出复杂的流形结构，传统的欧几里得距离可能无法捕捉样本间的真实关系。本练习将引导您超越标准的距离假设，将轮廓系数的评估框架推广到基于图的测地距离（geodesic distance）。通过构建一个$k$-近邻图并在此之上计算轮廓分数，您将学会如何使评估指标适应复杂数据的内在几何特性，并探索关键超参数（如$k$值）对分析稳定性的影响。[@problem_id:4561549]", "problem": "一个生物信息学团队正在评估源自基因表达数据的低维嵌入中，细胞群体的聚类分配的稳定性。他们使用基于图的测地距离，其中图是$k$-近邻对称图，其边权重等于嵌入中样本之间的欧几里得距离。他们试图量化平均轮廓系数对于计算图距离时所选择的 $k$-近邻参数的敏感程度。\n\n给定一个包含 $n$ 个二维点 $x_i \\in \\mathbb{R}^2$ 的固定数据集（代表基因表达谱的低维嵌入）、一个聚类分配向量以及一个 $k$ 值，按如下方式构建对称 $k$-近邻图 $G_k$：\n- 对于每个节点 $i$，计算其到所有其他节点 $j \\neq i$ 的欧几里得距离。\n- 将 $i$ 与其 $k$ 个最近的邻居连接；通过取这些有向边的无向并集来进行对称化。\n- 边权重为欧几里得距离。设 $d_{ij}$ 表示 $i$ 和 $j$ 之间边的权重（如果存在），否则为 $+\\infty$。\n- 定义节点 $i$ 和 $j$ 之间的图测地距离为在 $G_k$ 中使用边权重 $d_{uv}$ 的最短路径长度，其中对所有 $i$ 都有 $d_{ii} = 0$。\n\n使用聚类评估文献中的规范定义来定义每个样本的轮廓值，该定义为：对于每个样本 $i$，比较其到自身簇的平均图距离与到其他簇的平均图距离的最小值，然后用这两个平均值中较大的一个进行归一化。平均轮廓值是所有样本轮廓值的算术平均值。\n\n为确保在可能不连通的图上行为有明确定义，遵循以下约定：\n- 如果样本 $i$ 是其所在簇的唯一成员（孤立点），则其轮廓值定义为 $0$。\n- 在计算簇内或簇间的平均值时，仅使用有限的图距离（即可通过某条路径到达的节点）。如果 $i$ 在其自身簇中没有有限距离的邻居，或者在任何其他簇中都没有有限距离的邻居，则其轮廓值定义为 $0$。\n\n您必须实现整个计算过程，并将其应用于下方的测试套件。所有量纲均为无量纲。您的程序必须生成单行输出，其中包含一个逗号分隔的列表，列表内容为每个测试用例的平均轮廓值，四舍五入到恰好六位小数，并用方括号括起来。\n\n数据集：\n- 点 $x_i = (x_i^{(1)}, x_i^{(2)})$，其中 $i \\in \\{0,1,\\dots,7\\}$：\n  - $x_0 = (0, 0)$\n  - $x_1 = (1, 0)$\n  - $x_2 = (2, 0)$\n  - $x_3 = (10, 0)$\n  - $x_4 = (11, 0)$\n  - $x_5 = (12, 0)$\n  - $x_6 = (25, 0)$\n  - $x_7 = (26, 0)$\n\n包含五个用例的测试套件。在每个用例中，$k$ 是 $k$-近邻参数，`labels` 是聚类分配向量 $c \\in \\mathbb{Z}^n$，其中 $c_i$ 是点 $i$ 的簇标签：\n- 用例 1：$k = 3$，labels $= [0,0,0,1,1,1,2,2]$。\n- 用例 2：$k = 1$，labels $= [0,0,0,1,1,1,2,2]$。\n- 用例 3：$k = 7$，labels $= [0,0,0,1,1,1,2,2]$。\n- 用例 4：$k = 3$，labels $= [0,0,0,1,1,1,1,1]$。\n- 用例 5：$k = 3$，labels $= [0,0,0,1,1,1,1,2]$。\n\n要求：\n- 按照规定计算对称 $k$-近邻图 $G_k$，使用欧几里得边权重。\n- 通过 $G_k$ 上的最短路径计算所有点对之间的图测地距离。\n- 根据上述约定计算每个测试用例的平均轮廓值。\n- 最终输出格式：单行输出，包含一个由五个浮点数组成的列表，按用例1到5的顺序排列，每个浮点数四舍五入到恰好六位小数，以逗号分隔并用方括号括起来（例如，$[a,b,c,d,e]$，其中 $a,b,c,d,e$ 每个数小数点后都有六位数字）。", "solution": "该问题要求针对给定数据集上的几种聚类场景计算平均轮廓系数。问题的核心在于使用一种非标准的距离度量：源自对称 $k$-近邻（$k$-NN）图的图测地距离。解决方案涉及一个基于原则的多步骤过程，该过程结合了图论、矩阵计算和聚类评估中的概念。\n\n整体算法方法如下：\n1.  对于每个由参数 $k$ 和聚类分配向量定义的测试用例，构建相应的对称 $k$-NN 图。\n2.  在此图上计算所有点对之间的最短路径，以确定测地距离矩阵。\n3.  使用此距离矩阵，根据规范定义为每个数据点计算其样本轮廓值，并严格遵守处理不连通分量和孤立点簇的指定约定。\n4.  最后，计算所有样本轮廓值的算术平均值，以获得该测试用例的平均轮廓分数。\n\n该方法的详细分步实现基于以下原则：\n\n首先，我们构建加权无向图 $G_k = (V, E_k)$，其中顶点集 $V$ 对应于 $n$ 个数据样本。边集 $E_k$ 及相关权重分三个阶段确定。\na. 我们首先计算所有点对的欧几里得距离矩阵，称之为 $D_{Euc}$，其中 $D_{Euc}[i, j]$ 是 $\\mathbb{R}^2$ 中点 $x_i$ 和 $x_j$ 之间的欧几里得距离。该矩阵构成了所有后续基于距离的操作的基础。对于任意两点 $x_i = (x_i^{(1)}, x_i^{(2)})$ 和 $x_j = (x_j^{(1)}, x_j^{(2)})$，距离为 $D_{Euc}[i,j] = \\sqrt{(x_i^{(1)} - x_j^{(1)})^2 + (x_i^{(2)} - x_j^{(2)})^2}$。\nb. 接下来，对于每个点 $i$，我们通过在 $D_{Euc}$ 的第 $i$ 行中找到 $k$ 个值最小的点 $j \\ne i$ 来确定其 $k$ 个最近邻居的集合 $N_k(i)$。这定义了一个有向 $k$-NN 图。\nc. 然后通过取边的“无向并集”来使图对称化。当且仅当 $j \\in N_k(i)$ 或 $i \\in N_k(j)$ 时， $G_k$ 中存在无向边 $(i, j)$。该边的权重是其欧几里得距离 $D_{Euc}[i, j]$。此过程生成一个加权邻接矩阵 $W$，其中如果边 $(i, j)$ 存在，则 $W_{ij} = D_{Euc}[i, j]$，否则 $W_{ij} = +\\infty$。根据定义，$W_{ii} = 0$。\n\n第二，我们计算图测地距离矩阵 $D_{geo}$。值 $D_{geo}[i, j]$ 是图 $G_k$ 中节点 $i$ 和 $j$ 之间最短路径的长度。对于稠密图表示（邻接矩阵 $W$）和少量节点（$n=8$），Floyd-Warshall 算法是解决此所有点对最短路径问题的最合适方法。该算法迭代地将每个节点视为所有其他节点对之间路径上的潜在中间点，系统地松弛路径长度，直到找到最短路径。输出是一个矩阵 $D_{geo}$，如果相应节点之间不存在路径，则条目为 $+\\infty$。\n\n第三，我们计算每个样本 $i$ 的轮廓分数。这需要使用 $D_{geo}$ 中的测地距离计算两个量，$a(i)$ 和 $b(i)$。设 $C(i)$ 表示样本 $i$ 被分配到的簇。\n-   $a(i)$：平均簇内距离。它是样本 $i$ 与同一簇中所有其他样本 $j$（$j \\in C(i), j \\ne i$）的 $D_{geo}[i,j]$ 的平均值。平均值中仅包含有限距离。\n-   $b(i)$：最小平均簇间距离。对于每个其他簇 $C_m$（其中 $m \\ne C(i)$），我们计算从 $i$ 到 $C_m$ 中所有样本 $j$ 的平均距离，同样仅使用有限距离。$b(i)$ 是在所有其他簇 $C_m$ 上这些平均值的最小值。\n\n问题为定义不明确的情况指定了关键约定：\n-   如果样本 $i$ 是一个孤立点（其所在簇的唯一成员），其轮廓值 $s(i)$ 为 $0$。这是一个基本情况。\n-   如果样本 $i$ 到其自身簇的任何其他成员都没有有限距离的路径，则 $a(i)$ 未定义。此时轮廓值 $s(i)$ 被设置为 $0$。\n-   如果样本 $i$ 到任何其他簇中的任何样本都没有有限距离的路径，则 $b(i)$ 未定义。此时轮廓值 $s(i)$ 被设置为 $0$。\n\n一旦获得 $a(i)$ 和 $b(i)$ 的明确定义值，样本 $i$ 的轮廓值就使用规范公式计算：\n$$ s(i) = \\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}} $$\n如果 $a(i)=b(i)=0$，这将导致除以零；然而，鉴于问题的数据和结构，对于非零轮廓值，这种情况不会出现。值为零的约定处理了所有退化场景。\n\n最后，整个数据集的平均轮廓分数就是所有 $n$ 个样本的单个 $s(i)$ 值的算术平均值。对所提供的五个测试用例中的每一个都重复此过程。", "answer": "```python\nimport numpy as np\nfrom scipy.sparse.csgraph import floyd_warshall\nfrom scipy.spatial.distance import cdist\n\ndef calculate_mean_silhouette(points, labels, k):\n    \"\"\"\n    Computes the mean silhouette score based on graph geodesic distances.\n\n    Args:\n        points (np.ndarray): An (n, d) array of n points in d dimensions.\n        labels (np.ndarray): An (n,) array of cluster labels.\n        k (int): The number of nearest neighbors for graph construction.\n\n    Returns:\n        float: The mean silhouette score.\n    \"\"\"\n    n = points.shape[0]\n    \n    # Step A: Construct the symmetric k-NN graph\n    # 1. Compute Euclidean distance matrix\n    euc_dist = cdist(points, points, 'euclidean')\n    \n    # 2. Build weighted adjacency matrix W for the symmetric k-NN graph\n    adj_matrix = np.full((n, n), np.inf)\n    np.fill_diagonal(adj_matrix, 0)\n    \n    # Find k-nearest neighbors for each point\n    # argsort returns indices, [:, 1:k+1] skips self (dist=0) and takes k neighbors\n    # A stable sort tie-breaking rule (e.g., lower index first) is implicitly\n    # handled by how np.argsort is implemented, which is sufficient here.\n    neighbor_indices = np.argsort(euc_dist, axis=1)[:, 1:k+1]\n    \n    for i in range(n):\n        for j_idx in neighbor_indices[i]:\n            # Symmetrize by taking the union of directed edges\n            if adj_matrix[i, j_idx] == np.inf:\n                adj_matrix[i, j_idx] = euc_dist[i, j_idx]\n            if adj_matrix[j_idx, i] == np.inf:\n                adj_matrix[j_idx, i] = euc_dist[j_idx, i]\n\n    # Step B: Compute all-pairs geodesic distances using Floyd-Warshall\n    geo_dist = floyd_warshall(csgraph=adj_matrix, directed=False)\n    \n    # Step C: Compute per-sample silhouette values\n    silhouette_values = np.zeros(n)\n    unique_labels = np.unique(labels)\n    \n    for i in range(n):\n        label_i = labels[i]\n        \n        # Identify points in the same cluster and other clusters\n        in_cluster_mask = (labels == label_i)\n        in_cluster_mask[i] = False  # Exclude the point itself\n        \n        # Handle singleton clusters\n        if not np.any(in_cluster_mask):\n            silhouette_values[i] = 0.0\n            continue\n            \n        # Calculate a(i): average intra-cluster distance\n        a_dists = geo_dist[i, in_cluster_mask]\n        a_dists_finite = a_dists[np.isfinite(a_dists)]\n        \n        # Handle disconnection within the cluster\n        if len(a_dists_finite) == 0:\n            silhouette_values[i] = 0.0\n            continue\n        \n        a_i = np.mean(a_dists_finite)\n        \n        # Calculate b(i): minimum average inter-cluster distance\n        other_cluster_means = []\n        other_labels = [ul for ul in unique_labels if ul != label_i]\n        \n        for other_label in other_labels:\n            other_cluster_mask = (labels == other_label)\n            b_dists = geo_dist[i, other_cluster_mask]\n            b_dists_finite = b_dists[np.isfinite(b_dists)]\n            \n            if len(b_dists_finite) > 0:\n                other_cluster_means.append(np.mean(b_dists_finite))\n        \n        # Handle disconnection from all other clusters\n        if len(other_cluster_means) == 0:\n            silhouette_values[i] = 0.0\n            continue\n            \n        b_i = np.min(other_cluster_means)\n        \n        # Calculate silhouette value s(i)\n        numerator = b_i - a_i\n        denominator = max(a_i, b_i)\n        \n        silhouette_values[i] = numerator / denominator if denominator > 0 else 0.0\n        \n    # Step D: Compute mean silhouette score\n    return np.mean(silhouette_values)\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Dataset definition\n    points = np.array([\n        [0, 0], [1, 0], [2, 0],\n        [10, 0], [11, 0], [12, 0],\n        [25, 0], [26, 0]\n    ], dtype=float)\n\n    # Test suite definition\n    test_cases = [\n        {'k': 3, 'labels': np.array([0, 0, 0, 1, 1, 1, 2, 2])},\n        {'k': 1, 'labels': np.array([0, 0, 0, 1, 1, 1, 2, 2])},\n        {'k': 7, 'labels': np.array([0, 0, 0, 1, 1, 1, 2, 2])},\n        {'k': 3, 'labels': np.array([0, 0, 0, 1, 1, 1, 1, 1])},\n        {'k': 3, 'labels': np.array([0, 0, 0, 1, 1, 1, 1, 2])}\n    ]\n\n    results = []\n    for case in test_cases:\n        mean_silhouette = calculate_mean_silhouette(points, case['labels'], case['k'])\n        results.append(mean_silhouette)\n    \n    # Format and print the final output\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4561549"}]}