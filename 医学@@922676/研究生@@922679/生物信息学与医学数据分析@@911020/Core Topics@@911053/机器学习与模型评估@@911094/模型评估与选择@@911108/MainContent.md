## 引言
在生物信息学和医学数据分析中，构建能够准确预测未来未知数据的模型是推动精准医疗和科学发现的核心。一个仅在训练数据上表现优异的模型，若无法泛化至新的患者群体，其临床应用价值便微乎其微。因此，模型评估与选择成为连接数据与决策的关键桥梁，其根本任务是解决一个核心难题：如何在我们拥有的有限数据之上，诚实且可靠地估计模型在真实世界中的表现，即其泛化能力。

本文旨在为研究生及科研人员提供一个关于模型评估与选择的系统性框架，以应对这一挑战。我们将深入探讨从理论基础到高级应用的完整知识体系：
在第一部分“原理与机制”中，我们将奠定理论基石，解释为何[训练误差](@entry_id:635648)是泛化风险的有偏估计，并详细介绍K-折交叉验证等[重采样方法](@entry_id:144346)如何提供更可靠的性能度量。同时，我们会剖析数据泄露的风险，并介绍一系列用于分类和生存分析的关键性能指标。

接下来，在“应用与跨学科连接”部分，我们将理论付诸实践。通过具体案例，您将学习如何比较不同模型的性能，如何通过决策曲线分析等方法评估模型的临床效用，以及如何为具有复杂依赖结构（如时间序列或聚类数据）的数据设计严谨的验证方案。我们还将探讨模型可靠性的高级主题，并最终辨析预测与因果推断这两个重要但常被混淆的分析范式。

最后，“动手实践”部分将通过编码练习，巩固您对核心概念的理解，确保您能将所学知识应用于实际研究中。通过本文的学习，您将不仅掌握评估模型的技术，更能建立起一套贯穿研究设计、模型构建到结果解读的严谨[科学思维](@entry_id:268060)。

## 原理与机制

在生物信息学和医学数据分析中，我们的核心目标是构建不仅能解释现有数据，而且能对未来未见数据做出准确预测的模型。一个在训练数据上表现完美的模型，如果不能推广到新的数据上，那么它在临床实践中几乎没有价值。因此，模型评估和选择的中心任务是：可靠地估计一个模型在未来数据上的表现，即其**泛化风险 (generalization risk)**，并利用这个估计来选择最佳模型。本章将深入探讨支撑这一过程的基本原理和核心机制。

### 核心问题：估计泛化风险

在[统计学习理论](@entry_id:274291)的框架下，我们假设存在一个固定但未知的联合概率分布 $P_{X,Y}$，它生成了我们观察到的数据，其中 $X$ 是特征向量（例如，基因表达谱），$Y$ 是结果（例如，疾病状态）。一个预测模型，或称预测器，是一个函数 $\hat{f}$，它将特征 $X$ 映射到一个预测值 $\hat{f}(X)$。我们通过一个**[损失函数](@entry_id:136784) (loss function)** $\ell(Y, \hat{f}(X))$ 来量化单次预测的误差。

模型的最终性能由其**[期望风险](@entry_id:634700) (expected risk)** 或**泛化风险** $R(\hat{f})$ 来衡量，它是[损失函数](@entry_id:136784)在整个数据生成分布 $P_{X,Y}$ 上的[期望值](@entry_id:150961)：
$$ R(\hat{f}) = \mathbb{E}_{(X,Y)\sim P_{X,Y}}\big[\ell\big(Y,\hat{f}(X)\big)\big] $$
这个量代表了模型在遇到来自目标人群的新数据时，平均会犯多大的错误。它是我们真正关心的黄金标准，但由于我们不知道 $P_{X,Y}$，所以无法直接计算它。[@problem_id:4585245]

我们能够计算的是基于有限、可用的数据集 $\mathcal{D}_n = \{(x_i, y_i)\}_{i=1}^n$ 的**[经验风险](@entry_id:633993) (empirical risk)** $\hat{R}_n(\hat{f})$，即在样本上的平均损失：
$$ \hat{R}_n(\hat{f})=\frac{1}{n}\sum_{i=1}^n \ell\big(y_i,\hat{f}(x_i)\big) $$
根据大数定律，对于一个*固定*的、独立于数据集 $\mathcal{D}_n$ 的预测器 $\hat{f}$，当样本量 $n$ 趋于无穷时，其[经验风险](@entry_id:633993)会收敛于其[期望风险](@entry_id:634700)。[@problem_id:4585245] 然而，在实践中，我们的模型 $\hat{f}$ 本身就是从数据集 $\mathcal{D}_n$ 中学习得到的。特别是，许多学习算法遵循**[经验风险最小化](@entry_id:633880) (Empirical Risk Minimization, ERM)** 原则，即在某个假设类别中寻找能最小化 $\hat{R}_n(f)$ 的函数。当模型是在同一份数据上训练和评估时，其[经验风险](@entry_id:633993)（即**[训练误差](@entry_id:635648)**）通常会系统性地低估其真实的泛化风险。这是因为模型不仅学习了数据中普适的规律，也适应了样本特有的噪声和偶然性，这种现象称为**[过拟合](@entry_id:139093) (overfitting)**。因此，[训练误差](@entry_id:635648)是一个具有**乐观偏误 (optimistic bias)** 的估计量，不能用于诚实地评估模型的泛化能力。[@problem_id:4585245]

### 诚实[风险估计](@entry_id:754371)的策略

为了获得对泛化风险的更可靠的估计，我们必须在与训练数据相互独立的数据上评估模型。最简单的方法是将数据分为训练集和测试集（即留出验证），但这会减少用于训练的数据量。更稳健和数据高效的策略是**[重采样方法](@entry_id:144346) (resampling methods)**。

#### K-折[交叉验证](@entry_id:164650)

**K-折交叉验证 (K-fold Cross-Validation, CV)** 是一种广泛使用的[重采样](@entry_id:142583)技术。其过程如下：
1.  将原始数据集的索引 $\\{1, \dots, n\\}$ 随机地划分为 $K$ 个互不相交的子集（或称“折”），即 $\\{V_1, V_2, \dots, V_K\\}$。
2.  对于每一折 $j=1, \dots, K$：
    a. 将该折 $V_j$ 作为**验证集 (validation set)**。
    b. 将所有其他折的数据（即 $\mathcal{D}_n \setminus V_j$）作为**[训练集](@entry_id:636396) (training set)**。
    c. 在该[训练集](@entry_id:636396)上训练模型，得到一个预测器 $\hat{f}^{(-V_j)}$。
    d. 使用 $\hat{f}^{(-V_j)}$ 对[验证集](@entry_id:636445) $V_j$ 中的样本进行预测，并计算这些样本的损失。
3.  最终的交叉验证[风险估计](@entry_id:754371) $\hat{R}_{\mathrm{CV}}$ 是所有 $n$ 个样本的损失的平均值，其中每个样本的损失都是在其作为[验证集](@entry_id:636445)样本时计算的。

如果所有折的大小相等（$|V_j| = n/K$），则 CV 估计量可以写作：
$$ \hat{R}_{\mathrm{CV}}=\frac{1}{n}\sum_{j=1}^K\sum_{i\in V_j}\ell\left(y_i,\hat{f}^{(-V_j)}(x_i)\right) = \frac{1}{n}\sum_{i=1}^n \ell\left(y_i,\hat{f}^{(-V(i))}(x_i)\right) $$
其中 $V(i)$ 表示包含样本 $i$ 的那一折。[@problem_id:4585299] 在 $K$-折 CV 中，每个样本被精确地用作一次验证。训练过程是在大小为 $n(1-1/K)$ 的数据集上进行的。由于训练集比完整的 $n$ 个样本要小，如果学习算法的性能随数据量的增加而提升，那么 CV 估计量通常是对在完整数据集上训练出的最终模型的风险的一个**悲观偏误 (pessimistic bias)** 估计。例如，5-折 CV 估计的是在 $0.8n$ 个样本上训练的模型的平均风险，而不是在 $n$ 个样本上训练的模型的风险。这种偏误会随着 $K$ 的增加而减小。[@problem_id:4585245]

#### 自助法 OOB 估计

**[自助法](@entry_id:139281)袋外 (Bootstrap Out-of-Bag, OOB)** 估计是另一种[重采样方法](@entry_id:144346)，尤其在[集成方法](@entry_id:635588)（如随机森林）中很常用。其过程与 CV 有显著不同：
1.  进行 $B$ 次（例如，$B=500$）自助抽样。在每一次抽样 $b=1, \dots, B$ 中，从原始的 $n$ 个样本中进行**有放回地 (with replacement)** 抽取 $n$ 个样本，形成一个自助样本集 $S_b$。
2.  由于是[有放回抽样](@entry_id:274194)，每个自助样本集 $S_b$ 中很可能包含重复的样本。原始数据集中大约有 $1 - (1 - 1/n)^n \approx 1 - e^{-1} \approx 63.2\%$ 的[独立样本](@entry_id:177139)会出现在给定的自助样本集中。
3.  对于每一次抽样 $b$，那些**未被**抽入 $S_b$ 的样本构成了**袋外 (Out-of-Bag, OOB)** 样本集 $O_b$。任何一个特定样本成为 OOB 样本的概率约为 $e^{-1} \approx 36.8\%$。
4.  在每个自助样本集 $S_b$ 上训练一个模型 $\hat{f}^{(b)}$。
5.  OOB [风险估计](@entry_id:754371)是通过对每个样本 $i$ 计算其在所有视其为 OOB 样本的模型上的平均损失，然后对所有 $n$ 个样本求平均得到的。

与 CV 的关键区别在于：CV 通过**无放回 (without replacement)** 的分割来构建[训练集](@entry_id:636396)，而 Bootstrap 通过**有放回**的抽样；在 CV 中，每个样本被用作验证一次，而在 Bootstrap 中，一个样本可能在多次抽样中都是 OOB 样本；CV 的训练集大小是固定的 $n(1-1/K)$，而 Bootstrap 的有效训练集大小是一个期望约为 $0.632n$ 的随机变量。由于 OOB 估计所基于的模型的训练数据量通常比标准 CV（如 $K=10$）更小，其悲观偏误可能更严重。[@problem_id:4585299]

### 验证流程中的数据泄露风险

[交叉验证](@entry_id:164650)的有效性根本上依赖于一个核心假设：用于训练模型的数据与用于评估模型的数据是相互独立的。当[验证集](@entry_id:636445)的信息以任何形式“泄露”到模型的训练过程中时，这种独立性就被破坏了，我们称之为**数据泄露 (data leakage)**。数据泄露会导致对模型性能的严重乐观偏误估计，使我们误以为[模型泛化](@entry_id:174365)能力很强，而实际上它只是在“作弊”。[@problem_id:4958059]

在包含多个步骤的建模流程（pipeline）中，数据泄露是一个常见的陷阱。一个典型的流程可能包括[数据预处理](@entry_id:197920)（如缺失值[插补](@entry_id:270805)、[特征缩放](@entry_id:271716)）和[模型拟合](@entry_id:265652)。黄金法则是：**整个建模流程的每一步都必须在[交叉验证](@entry_id:164650)的每个训练折叠内部独立进行。**

具体来说，对于每一折 $k$：
1.  所有预处理步骤的参数（例如，用于缩放的均值和标准差，用于[插补](@entry_id:270805)的中位数，或通过统计检验选择的特征子集）必须**仅**从[训练集](@entry_id:636396) $D_{\mathrm{train}}^{(k)}$ 中学习。
2.  然后，使用这些学到的参数来转换训练集 $D_{\mathrm{train}}^{(k)}$ 和验证集 $D_{\mathrm{val}}^{(k)}$。
3.  最后，在转换后的[训练集](@entry_id:636396)上拟合模型，并在转换后的验证集上进行评估。

一个常见的错误是在交叉验证**之前**，对整个数据集进行预处理。例如：
*   **监督式特征选择**：在整个数据集上计算特征与结果变量（如使用 t-检验）的关联，并选出“最佳”特征。这会挑选出那些偶然在验证集上也表现良好的特征，导致性能估计严重虚高。
*   **无监督预处理**：即使是像[特征缩放](@entry_id:271716)或均值[插补](@entry_id:270805)这样的无监督方法，如果在整个数据集上进行，也构成数据泄露。因为用于计算缩放参数（均值/标准差）或插补值的数据包含了来自验证集的信息，从而在模型构建和评估之间引入了依赖性。[@problem_id:4958059]

严格遵守将整个流程嵌入交叉验证循环的原则，是获得可信的模型性能估计的唯一途径。

### 分类模型的性能度量

在了解了如何可靠地估计性能后，我们需要选择合适的度量标准。对于[分类问题](@entry_id:637153)，度量的选择取决于具体的临床目标。

#### 基于阈值的二[分类度量](@entry_id:637806)

当模型输出一个连续分数，并通过一个固定的决策阈值将其转换为二元预测（例如，阳性/阴性）时，其性能可以通过一个**[混淆矩阵](@entry_id:635058) (confusion matrix)** 来总结。[混淆矩阵](@entry_id:635058)包含四个基本计数：
*   **[真阳性](@entry_id:637126) (True Positives, $\mathrm{TP}$)**：实际为阳性，且被正确预测为阳性的样本数。
*   **[假阳性](@entry_id:635878) (False Positives, $\mathrm{FP}$)**：实际为阴性，但被错误预测为阳性的样本数。
*   **真阴性 (True Negatives, $\mathrm{TN}$)**：实际为阴性，且被正确预测为阴性的样本数。
*   **假阴性 (False Negatives, $\mathrm{FN}$)**：实际为阳性，但被错误预测为阴性的样本数。

基于这些计数，我们可以定义几个关键的性能度量 [@problem_id:4585265]：

*   **灵敏度 (Sensitivity)**，也称为**召回率 (Recall)** 或**真阳性率 (True Positive Rate, TPR)**：在所有真实阳性样本中，被正确识别为阳性的比例。它衡量了模型“发现”阳性病例的能力。
    $$ \text{灵敏度} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FN}} $$
*   **特异度 (Specificity)**，也称为**真阴性率 (True Negative Rate, TNR)**：在所有真实阴性样本中，被正确识别为阴性的比例。它衡量了模型正确排除阴性病例的能力。
    $$ \text{特异度} = \frac{\mathrm{TN}}{\mathrm{TN} + \mathrm{FP}} $$
*   **阳性预测值 (Positive Predictive Value, PPV)**，也称为**精确率 (Precision)**：在所有被预测为阳性的样本中，真实为阳性的比例。它回答了这样一个问题：“如果模型报警，这个警报有多大概率是正确的？”
    $$ \text{PPV} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FP}} $$
*   **阴性预测值 (Negative Predictive Value, NPV)**：在所有被预测为阴性的样本中，真实为阴性的比例。它回答了：“如果模型说没事，那么有多大概率是真的没事？”
    $$ \text{NPV} = \frac{\mathrm{TN}}{\mathrm{TN} + \mathrm{FN}} $$

值得注意的是，灵敏度和特异度是基于真实类别进行条件化的，因此它们是模型内在属性的度量，不受疾病**患病率 (prevalence)** $\pi = P(Y=1)$ 的影响。相反，PPV 和 NPV 是基于预测类别进行条件化的，它们严重依赖于患病率。

*   **准确率 (Accuracy)**：所有样本中被正确分类的比例。准确率的计算公式为 $(\mathrm{TP}+\mathrm{TN}) / (\mathrm{TP}+\mathrm{FP}+\mathrm{TN}+\mathrm{FN})$。通过[全概率公式](@entry_id:194231)，我们可以将其表示为灵敏度、特异度和患病率的函数：
    $$ \text{准确率} = \text{灵敏度} \cdot \pi + \text{特异度} \cdot (1-\pi) $$
    这个表达式揭示了为什么准确率在处理类别不平衡问题（如罕见病筛查）时具有误导性。当患病率 $\pi$ 非常低时（例如，$\pi=0.01$），$(1-\pi)$ 就非常接近 1。此时，准确率主要由特异度决定，而灵敏度的贡献几乎可以忽略不计。一个将所有人都预测为阴性的“平凡”分类器，其灵敏度为 0，特异度为 1，但其准确率将是 $1-\pi$，一个非常高的数值。因此，一个在识别阳性病例上毫无用处的模型，却可能获得极高的准确率。[@problem_id:4585295]

#### 独立于阈值的度量

评估一个产生连续风险评分的模型时，通常最好在选择特定决策阈值之前评估其整体的**区分能力 (discrimination)**。这可以通过评估所有可能阈值下的性能来实现。

*   **[受试者工作特征曲线](@entry_id:754147) (ROC) 和曲线下面积 (AUROC)**：**ROC 曲线**绘制了所有阈值下的真阳性率（TPR，即灵敏度）相对于[假阳性率](@entry_id:636147)（FPR，其中 FPR = 1 - 特异度）的变化。**[AUROC](@entry_id:636693)** 是该曲线下的面积。由于 TPR 和 FPR 都是以真实类别为条件的，它们不依赖于患病率。因此，**ROC 曲线和 [AUROC](@entry_id:636693) 对于[类别不平衡](@entry_id:636658)是稳健的**。AUROC 为 1 表示完美分类器，为 0.5 表示随机猜测。[@problem_id:4585294]

*   **[精确率-召回率曲线](@entry_id:637864) (PR) 和曲线下面积 (AUPRC)**：**PR 曲线**绘制了精确率（PPV）相对于召回率（TPR）的变化。与 ROC 曲线不同，PR 曲线对患病率非常敏感。我们可以通过[贝叶斯定理](@entry_id:151040)将 PR 曲线的坐标与 ROC 曲线的坐标联系起来。对于给定的召回率（即 TPR）$r$ 和对应的 FPR $f(r)$，精确率可以表示为：
    $$ \text{精确率}(r) = \frac{\pi \cdot r}{\pi \cdot r + (1-\pi) \cdot f(r)} $$
    这个公式清楚地表明，即使 ROC 曲线（由函数 $f(r)$ 定义）是固定的，PR 曲线的形状和位置也会随着患病率 $\pi$ 的变化而改变。当阳性类别非常罕见时（$\pi$ 很小），PR 曲线对模型在低召回率区域的性能改进尤其敏感，而 ROC 曲线可能变化不大。对于一个随机分类器，其 [AUROC](@entry_id:636693) 恒为 0.5，而其 **AUPRC 近似等于患病率 $\pi$**。因此，在罕见病检测等场景中，AUPRC 通常被认为是比 AUROC 更具信息量的度量，因为它能更好地反映出在低患病率下提升精确率的挑战。[@problem_id:4585294]

### 模型构建与正则化

模型评估不仅是为了选择模型，也是为了指导模型的构建。为了[防止过拟合](@entry_id:635166)，我们经常在模型训练过程中引入**正则化 (regularization)**，这是一种通过对模型复杂性施加惩罚来提升泛化能力的技术。

在逻辑回归中，我们最大化[对数似然函数](@entry_id:168593) $l(\beta)$。正则化通过在目标函数中加入一个关于系数向量 $\beta$ 的惩罚项来实现。常用的两种[正则化方法](@entry_id:150559)是：

*   **$\ell_2$ 正则化 (Ridge 回归)**：惩罚项是系数向量的 $\ell_2$ 范数的平方。最大化的目标函数为：
    $$ l(\beta) - \frac{\lambda}{2} \sum_{j=1}^p \beta_j^2 = l(\beta) - \frac{\lambda}{2} \lVert \beta_{-0} \rVert_2^2 $$
    其中 $\lambda$ 是正则化强度参数，$\beta_{-0}$ 表示不包括截距项的系数。$\ell_2$ 惩罚会**增加估计量的偏误**，但**减小其方差**。它倾向于将所有系数向零收缩，但通常不会将任何系数精确地设置为零。这在处理高度相关的预测变量（[多重共线性](@entry_id:141597)）时特别有效，因为它会稳定估计过程。[@problem_id:4585280]

*   **$\ell_1$ 正则化 (Lasso 回归)**：惩罚项是系数的 $\ell_1$ 范数。最大化的目标函数为：
    $$ l(\beta) - \lambda \sum_{j=1}^p |\beta_j| = l(\beta) - \lambda \lVert \beta_{-0} \rVert_1 $$
    与 $\ell_2$ 类似，$\ell_1$ 正则化也通过引入偏误来降低方差。但其关键特性是能够产生**[稀疏解](@entry_id:187463)**，即将许多不重要的特征的系数精确地压缩为零。这使得 $\ell_1$ 正则化不仅是一种[正则化技术](@entry_id:261393)，也是一种嵌入式的**[特征选择](@entry_id:177971)**方法。在 $p \gg n$ 的高维场景（如基因组学）中，当我们假设只有少数特征是真正相关的时，这一特性尤其有价值。[@problem_id:4585280]

通常，截距项 $\beta_0$ 不被惩罚，因为它代表了所有预测变量为零时的基线[对数几率](@entry_id:141427)，惩罚它没有实际意义。正则化强度的选择通常通过交叉验证来完成。

### 超越区分能力：评估[模型校准](@entry_id:146456)

一个好的预测模型不仅要有高的区分能力（如高 AUROC），还应该具有良好的**校准性 (calibration)**。一个模型如果经过良好校准，意味着其预测的概率可以被直接解释为真实的事件发生率。例如，如果模型对一组患者预测的患病风险为 $0.2$，那么在这组患者中，应该有大约 $20\%$ 的人最终会患病，即 $\Pr(Y=1 \mid \hat{p}=p) = p$。

在临床决策中，校准性至关重要，因为它影响风险沟通和决策制定的可靠性。我们可以通过一个逻辑重[校准模型](@entry_id:180554)来评估和修正校准性：
$$ \operatorname{logit}(\Pr(Y_i=1 \mid \hat{p}_i))=\alpha+\beta\,\operatorname{logit}(\hat{p}_i) $$
其中 $\operatorname{logit}(p) = \log(p/(1-p))$ 是[对数几率](@entry_id:141427)变换。一个完美校准的模型应该有 $\alpha=0$ 和 $\beta=1$。

*   **宏观校准 (Calibration-in-the-large)**：由参数 $\alpha$ (当 $\beta$ 固定为 1 时) 衡量。一个非零的 $\alpha$ 表示模型系统性地高估或低估了所有风险。例如，如果 $\alpha > 0$，模型预测的整体风险水平低于观察到的平均风险。通过最大似然估计 $\alpha$ 可以调整模型的平均预测概率，使其与样本的整体事件发生率相匹配。[@problem_id:4585284] [@problem_id:4585284]

*   **校准斜率 (Calibration slope)**：由参数 $\beta$ 衡量。它反映了预测的极端程度。
    *   $\beta  1$ 表示模型过于自信或**过拟合**：高风险预测过高，低风险预测过低。
    *   $\beta > 1$ 表示模型过于保守或**欠拟合**：所有预测都过于趋向于平均风险。
    通过在验证数据上拟合这个包含 $\alpha$ 和 $\beta$ 的逻辑[回归模型](@entry_id:163386)，我们可以定量地评估模型的校准性能。[@problem_id:4585284]

### 用于模型选择的[信息准则](@entry_id:636495)

除了交叉验证，**[信息准则](@entry_id:636495) (information criteria)** 也为在嵌套或非嵌套的[参数化](@entry_id:265163)模型之间进行选择提供了另一种途径。最常见的两个是 AIC 和 BIC。

*   **[赤池信息准则](@entry_id:139671) (Akaike Information Criterion, AIC)**：
    $$ \mathrm{AIC} = 2k - 2\log(\hat{L}) $$
    其中 $k$ 是模型中的参数数量，$\hat{L}$ 是[最大似然](@entry_id:146147)值。AIC 的目标是选择能最好地预测新数据的模型。它通过最小化模型与真实数据生成分布之间的**Kullback-Leibler (KL) 散度**来衡量信息损失。因此，AIC 旨在实现**[渐近效率](@entry_id:168529)**（最优预测性能）。然而，AIC 不是**一致**的，意味着即使真实模型在候选集中，随着样本量的增加，AIC 仍有一定概率选择一个更复杂的模型。[@problem_id:4585274]

*   **[贝叶斯信息准则](@entry_id:142416) (Bayesian Information Criterion, BIC)**：
    $$ \mathrm{BIC} = k\log(n) - 2\log(\hat{L}) $$
    其中 $n$ 是样本量。BIC 源于对模型后验概率的近似。它的目标是选择具有最高后验概率的模型。由于其惩罚项随样本量 $n$ 增长，BIC 对模型复杂度的惩罚比 AIC 严厉得多。这使得 BIC 是一个**一致**的准则：如果真实模型在候选集中，随着 $n \to \infty$，BIC 选择真实模型的概率会趋近于 1。[@problem_id:4585274]

在实践中，特别是在所有模型都可能是对现实的错误设定（misspecified）的简化时，AIC 和 BIC 的行为差异变得很关键。AIC 倾向于选择一个更复杂的模型，以更好地逼近复杂的现实。而 BIC 的强惩罚可能导致它选择一个更简单的、可能[欠拟合](@entry_id:634904)的模型。因此，如果目标是**预测**，AIC 通常是更好的选择；如果目标是**解释**或识别“真实”模型结构，BIC 可能更受青睐。[@problem_id:4585274]