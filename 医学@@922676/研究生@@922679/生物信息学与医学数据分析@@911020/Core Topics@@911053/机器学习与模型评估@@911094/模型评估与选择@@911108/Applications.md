## 应用与跨学科连接

### 引言

前面的章节已经系统地阐述了模型评估与选择的核心原理和机制。然而，这些理论的真正价值在于其解决实际科学问题的能力。本章旨在搭建理论与实践之间的桥梁，展示这些核心原理如何在生物信息学和医学数据分析的复杂、多样的真实世界情境中被应用、扩展和整合。

我们将不再重复介绍核心概念，而是通过一系列应用驱动的案例，探索模型评估如何从简单的性能比较扩展到确保模型在临床环境中安全、有效和公平部署的全过程。我们将讨论在处理具有内在相关性（如来自同一患者的多个样本）或时序依赖性（如电子健康记录）的复杂[数据结构](@entry_id:262134)时，如何设计严谨的验证方案。我们还将深入探讨一些高级主题，例如如何量化和区分[模型不确定性](@entry_id:265539)的不同来源，以及如何构建能够识别自身局限性的可靠模型。

最终，本章将以一个根本性的概念辨析作结：预测与因果推断。理解这两种分析范式在目标、假设和验证策略上的本质区别，对于任何有志于利用数据驱动临床决策的科研人员来说都至关重要。通过本章的学习，读者将能够更深刻地理解模型评估不仅仅是一个技术步骤，更是一种贯穿于从研究设计到临床转化的整个[科学思维](@entry_id:268060)方式。

### 核心比较与诊断应用

在开发预测模型的过程中，一个基本任务是量化和比较不同模型的性能，或者评估新增一个生物标志物能否带来实质性的改进。这需要超越简单的准确率指标，采用更具信息量的统计工具。

#### 在配对数据上比较分类器性能

当两个或多个模型在同一个数据集上进行评估时，它们的预测误差通常是相关的，因为它们面对的是相同的样本。忽略这种相关性会得出不准确的结论。因此，统计学提供了专门用于配对比较的方法。

对于两个输出二元决策（例如“阳性”或“阴性”）的分类器，**[McNemar检验](@entry_id:166950)**是一种经典的[非参数检验](@entry_id:176711)方法。该检验巧妙地将焦点放在两个分类器预测结果不一致的样本上。假设在一个包含$N$个样本的测试集上，我们记录了两个分类器（$\mathcal{A}$和$\mathcal{B}$）预测正确与否的四种组合频数：两者皆对 ($n_{11}$)，两者皆错 ($n_{00}$)，$\mathcal{A}$对$\mathcal{B}$错 ($n_{10}$)，以及$\mathcal{A}$错$\mathcal{B}$对 ($n_{01}$)。两个分类器表现一致的样本（$n_{11}$和$n_{00}$）对于判断谁更优没有提供信息。真正的差异体现在它们不一致的“争议”样本上。[McNemar检验](@entry_id:166950)的原假设是，两种分类器犯错的概率相等，这等价于$p_{10} = p_{01}$。在原假设下，总共$n_{10} + n_{01}$个不一致的样本中，每种不一致情况的发生可以被看作是抛硬币。因此，我们可以使用精确的二项检验来计算观察到$n_{10}$和$n_{01}$这样或更极端分布的概率，从而判断两个分类器的错误率是否存在统计学上的显著差异。[@problem_id:4585230]

对于输出连续风险评分的模型，受试者工作特征曲线下面积（[AUROC](@entry_id:636693)）是衡量其区分能力的金标准。当比较在同一组患者上评估的两个模型的AUROC时，我们必须考虑其估计值的相关性。**DeLong检验**为此提供了一个严谨的[非参数方法](@entry_id:138925)。该方法基于[AUROC](@entry_id:636693)可以表示为一个两样本[U-统计量](@entry_id:171057)（U-statistic）的深刻见解。具体来说，[AUROC](@entry_id:636693)等价于从正负样本中各随机抽取一个个体，正样本得分高于负样本得分的概率。DeLong检验利用[U-统计量](@entry_id:171057)的[渐近正态性](@entry_id:168464)，通过计算每个受试者对[AUROC](@entry_id:636693)的贡献（即该受试者与所有相反类别受试者得分比较的结果），来估计每个模型AUROC的方差以及两个模型AUROC估计值之间的协方差。这种基于影响函数的分解方法，精确地捕捉了由于在相同受试者上进行评估而产生的相关性，从而构建出统计效力更强的检验统计量$Z$，用于判断两个模型的AUROC是否存在显著差异。[@problem_id:4585229]

#### 量化新预测因子的增量价值

在临床和生物信息学研究中，一个常见的问题是评估在一个现有风险模型中加入一个新的生物标志物（biomarker）或特征集是否值得。即使新模型的AUROC有统计上的显著提升，这种提升在临床实践中是否具有意义仍需量化。**净重分类改善（NRI）**和**综合判别改善（IDI）**是两种被广泛用于回答此问题的指标。

NRI关注的是，与旧模型相比，新模型能在多大程度上将患者“正确地”移动到更高或更低的风险类别中。它分别计算在最终发生事件的患者中，向上重分类的比例减去向下重分类的比例，以及在未发生事件的患者中，向下重分类的比例减去向上重分类的比例，然后将两者相加。一个正的NRI值表明新模型在整体上改善了风险分层。

IDI则从另一个角度评估改进，它衡量的是模型区分能力的提升。一个模型的“区分斜率”可以被定义为事件组的平均预测风险与非事件组的平均预测风险之差。IDI即为新模型的区分斜率与旧模型的区分斜率之差。一个正的IDI值意味着新模型在拉开事件组与非事件组之间平均预测概率的差距方面做得更好。虽然NRI和IDI因其统计特性受到一些批评，但它们提供了一种直观的方式来量化模型改进的幅度，超越了单一AUROC比较的范畴。[@problem_id:4585227]

### 从统计性能到临床效用

一个预测模型即使在统计指标上表现优异，也未必能在临床实践中创造价值。它的部署可能会导致过度治疗或治疗不足，带来不必要的成本和风险。因此，评估模型的临床效用至关重要。

#### 决策曲线分析

**决策曲线分析（Decision Curve Analysis, DCA）**是一个强大的框架，用于评估预测模型在临床决策中的净获益（Net Benefit）。与AUROC等纯粹衡量区分度的指标不同，DCA将模型的预测性能与患者或决策者的偏好直接联系起来。

DCA的核心思想是，使用一个模型来决定是否对患者进行干预（如推荐一项预防性治疗），其价值取决于它能在多大程度上增加真阳性（正确治疗了需要治疗的患者）同时避免[假阳性](@entry_id:635878)（避免了不必要的治疗及其相关的危害和成本）。这种权衡可以通过“风险阈值”（threshold probability, $t$）来量化。风险阈值$t$代表决策者愿意接受的、为了换取一个[真阳性](@entry_id:637126)而容忍的[假阳性](@entry_id:635878)治疗数量与不治疗一个真阳性所带来的损失之间的平衡点，其数学关系可以表示为危害-获益比 $\frac{H}{B} = \frac{t}{1-t}$。

在给定的风险阈值$t$下，一个模型的净获益被定义为：
$$ \mathrm{NB}(t) = \frac{\mathrm{TP}}{n} - \frac{\mathrm{FP}}{n} \cdot \frac{t}{1-t} $$
其中，$n$是总样本量，$\mathrm{TP}$和$\mathrm{FP}$分别是使用该模型在阈值$t$下决策时的[真阳性](@entry_id:637126)和[假阳性](@entry_id:635878)数量。这个公式直观地表示了模型带来的平均每个人的获益（以[真阳性](@entry_id:637126)为单位），减去了因[假阳性](@entry_id:635878)决策而产生的加权“成本”。

DCA通过绘制净获益$\mathrm{NB}(t)$随风险阈值$t$变化的曲线，来展示模型在不同临床偏好下的表现。这条曲线通常会与两条基线策略进行比较：“治疗全部”策略（treat-all）和“不治疗”策略（treat-none）。“不治疗”策略的净获益恒为零。“治疗全部”策略的净获益为 $\pi - (1-\pi)\frac{t}{1-t}$，其中$\pi$是疾病患病率。如果一个模型的决策曲线在某个阈值范围内高于这两条基线，就意味着在该临床偏好下，使用该模型指导决策比“一刀切”的策略更有价值。DCA因此为模型选择提供了一个超越统计指标、直接与临床后果挂钩的实用工具。[@problem_id:4585278]

#### 生存分析中的模型评估

在许多医学应用中，我们关心的结局是事件发生的时间，例如患者的生存时间或疾病复发时间。这类数据通常存在“[右删失](@entry_id:164686)”（right-censoring），即在研究结束时，我们只知道某些患者的生存时间大于某个观察时长，但并不知道确切的事件发生时间。这为模型评估带来了独特的挑战。

**Harrell's C-index**（一致性指数）是评估生存风险模型时最常用的区分能力指标。它推广了[AUROC](@entry_id:636693)的概念，以适应[删失数据](@entry_id:173222)的存在。C-index估计的是一个概率：从所有患者中随机抽取两个可比较的患者，风险评分较高的患者其真实事件发生时间也更早。

这里的关键在于“可比较”的定义。一对患者$(i, j)$只有在我们能够明确判断其真实事件时间$T_i$和$T_j$的先后顺序时，才被认为是可比较的。例如，如果患者$i$在时间$Y_i$发生了事件（$\Delta_i=1$），而患者$j$的观察时间$Y_j$晚于$Y_i$（无论患者$j$是否删失），我们就能确定$T_i  T_j$，因此这对患者是可比较的。反之，如果事件发生时间较早的患者被删失了，我们就无法确定他们之间真实事件时间的顺序，这对患者就不是可比较的。在计算C-index时，所有不可比较的配对都会被忽略。

对于可比较的配对，如果风险评分的排序与真实事件时间的排序一致（即事件发生更早的患者有更高的风险评分），则该配对是“一致的”；如果相反，则是“不一致的”。如果风险评分相同（出现平局），则算作0.5个一致配对。C-index最终的计算结果就是一致配对的比例。通过这种方式，C-index有效地利用了删失数据中包含的排序信息，为生存模型提供了一个稳健的性能度量。[@problem_id:4585242]

### 复杂数据与部署场景中的验证

教科书中的模型评估通常假设数据是独立同分布（i.i.d.）的，但这在现实世界的医学数据中很少成立。来自同一患者的多个影像切片、随时间演变的电子健康记录（EHR）、以及不同医院之间的异质性，都对模型的验证提出了严峻的挑战。

#### 应对[数据依赖](@entry_id:748197)性：聚[类数](@entry_id:156164)据的交叉验证

在医学影像分析或基因组学研究中，我们常常会遇到层次化或聚类的数据结构，例如一个患者可能提供多张影像切片、多个组织样本或在不同时间点进行多次测量。来自同一患者的样本共享着共同的生物学特性，因此它们之间不是相互独立的。如果忽略这种内在相关性，采用标准的$K$-折交叉验证（例如，将所有影像切片随机打乱后分组），就会导致严重的数据泄露。

具体来说，当同一个患者的某些切片被分到训练集，而另一些被分到测试集时，模型在训练时实际上已经“看到”了[测试集](@entry_id:637546)中患者的部分信息。这使得模型能够学习到该患者特有的、而非普适的模式，从而在测试集上表现出虚高的性能。这种评估结果无法代表模型在面对一个全新患者时的真实泛化能力，造成了乐观偏误。

为了获得无偏的性能估计，[交叉验证](@entry_id:164650)的划分必须在患者（或聚类单元）层面进行。例如，在放射学研究中，应采用**“留一患者交叉验证”（Leave-One-Patient-Out CV）**或其变体，即确保来自同一患者的所有数据要么全部在[训练集](@entry_id:636396)，要么全部在[测试集](@entry_id:637546)。这种以分组为单位的划分策略（Group-wise CV）尊重了数据的内在结构，确保了[训练集](@entry_id:636396)和[测试集](@entry_id:637546)在患者层面的独立性，从而能够对模型处理新患者的能力给出一个更真实、更可靠的评估。[@problem_id:4585261]

#### 应对时间漂移：前瞻性部署模型的验证

电子健康记录（EHR）数据本质上是动态的。随着时间的推移，临床实践、诊断标准、编码规则、乃至患者群体的构成都可能发生变化，这种现象被称为“数据漂移”（data drift）。一个在2015年的数据上训练得很好的模型，到2020年部署时可能性能已经严重下降。因此，为EHR数据开发的模型，其验证方案必须模拟其未来的前瞻性应用场景。

在这种情况下，标准的随机$K$-折交叉验证是完全错误的。因为它将不同时间点的数据混合在一起，导致模型可以在“未来”的数据上进行训练，然后在“过去”的数据上进行测试，这与实际部署的单向时间流完全相悖。其评估结果是对历史平均性能的估计，而非对未来性能的可靠预测。

正确的做法是采用**时序[交叉验证](@entry_id:164650)（Temporal Cross-Validation）**。一种常见的策略是**“滚动原点”或“扩展窗口”验证**。具体做法是，设定一系列时间切分点，对于每个切分点，使用该点之前的所有数据作为[训练集](@entry_id:636396)，之后的一小段时间窗口内的数据作为验证集。这严格模拟了“用过去预测未来”的真实场景。此外，还需要特别注意[信息泄露](@entry_id:155485)。如果模型的标签需要一个未来的观察窗口来确定（例如，预测出院后30天内的再入院），那么训练集和验证集之间必须设置一个足够长的“间隔期”（gap），以确保训练标签的确定时间不会晚于验证样本的出现时间。通过多次滚动评估并汇总结果，我们可以得到模型在面对时间流逝和分布变化时的性能鲁棒性的更准确估计。同时，我们还可以使用如**[最大均值差异](@entry_id:636886)（MMD）**等统计工具来量化[训练集](@entry_id:636396)和[验证集](@entry_id:636445)之间的协变量分布差异，从而主动监测和理解数据漂移的程度。[@problem_id:4585283]

#### 模型的可移植性：从内部验证到外部验证

开发一个临床预测模型，最终目标是希望它能在更广泛的人群和医疗环境中应用。模型的“可移植性”（transportability）或泛化能力，需要通过一个层次化的验证框架来逐步建立信心。

- **内部验证（Internal Validation）**：这是验证的第一步，指的是在模型开发的原始数据集上，通过[重采样](@entry_id:142583)技术（如交叉验证或[自助法](@entry_id:139281)Bootstrap）来评估模型的性能并修正[过拟合](@entry_id:139093)带来的乐观偏误。它主要回答的问题是：这个模型在我所研究的这个特定人群中表现如何？

- **外部验证（External Validation）**：这是检验[模型泛化](@entry_id:174365)能力的关键一步，指的是在一个或多个完全独立于开发数据集的新数据集上评估模型性能。外部验证可以根据数据来源的差异进一步细分：
    - **时序验证（Temporal Validation）**：在同一家医疗机构，但在未来的时间段收集的数据上进行验证。这检验了模型对抗时间漂移的稳定性。[@problem_id:4585258]
    - **地理验证（Geographic Validation）**：在不同地理位置的另一家医疗机构或人群中进行验证。这检验了模型对于不同患者构成、地方性临床实践和数据采集系统差异的鲁棒性。[@problem_id:4585258]

当模型在一个新的外部数据集中表现不佳时，一个常见的问题是“校准失灵”（miscalibration），即模型的预测概率与新人群中的实际事件发生率系统性地不符。例如，由于新医院的患者平均风险更高，原模型可能会系统性地低估风险。在这种情况下，一个重要的实践步骤是**模型再校准（recalibration）**。最简单的再校准方法是**截距调整（intercept adjustment）**，即在保持模型原有预测结构（所有$\beta$系数不变）的基础上，为[线性预测](@entry_id:180569)值（logit）加上一个新的截距项$\alpha$。这个$\alpha$可以通过在外部验证数据上拟合一个以原模型[线性预测](@entry_id:180569)值为偏移量（offset）的逻辑回归模型来估计。这个简单的步骤通常能显著改善模型在新环境下的校准度，使其预测值与当地的基线风险保持一致，是提升模型可移植性的一个实用技巧。[@problem_id:4585232]

一个模型可以被认为具有良好的可移植性，必须满足一系列严苛的标准：(1) **区分度**（如[AUROC](@entry_id:636693)）在多个外部验证中保持稳定且具有临床意义；(2) **校准度**良好，或可以通过简单的再校准进行修正；(3) **临床效用**（如通过DCA评估）在目标人群中得到证实；(4) 数据生成过程具有可比性，没有因测量或编码方式的根本性变化导致的概念漂移。只有通过这样全面而严格的多维度、多中心外部验证，我们才能有信心将一个模型推广到临床实践中。[@problem_id:4585258]

### 模型构建与评估中的高级方法论

现代机器学习模型的开发是一个包含[数据预处理](@entry_id:197920)、特征选择、[超参数调优](@entry_id:143653)和模型训练的复杂流程。对整个流程进行严谨的评估，是确保最终模型可靠性的关键。

#### 严谨的[超参数调优](@entry_id:143653)与流程验证

机器学习模型的性能高度依赖于超参数的选择，例如正则化强度$\lambda$或[网络结构](@entry_id:265673)参数。评估和选择超参数的常用方法是[交叉验证](@entry_id:164650)。然而，一个常见的、但极其严重的错误是，将整个数据集用于模型选择和性能评估。

这种做法会导致“[选择偏误](@entry_id:172119)”（selection bias），也被称为“赢家诅咒”（winner's curse）。假设我们测试了大量超参数组合，每个组合的真实性能（$R_m$）因有限样本的[交叉验证](@entry_id:164650)而产生一个带噪音的估计值（$\hat{R}_m = R_m + \epsilon_m$）。当我们选择那个具有最低CV误差$\hat{R}_{\hat{m}} = \min_m \hat{R}_m$的组合时，我们很可能无意中挑选了一个其随机噪音$\epsilon_m$恰好为负向最大的组合。因此，报告这个最小值$\hat{R}_{\hat{m}}$作为最终模型的性能，几乎总是对真实泛化性能的一个过分乐观的估计。即$\mathbb{E}[\hat{R}_{\hat{m}}]  \mathbb{E}[R_{\hat{m}}]$。[@problem_id:4320596]

同样的问题也发生在[数据预处理](@entry_id:197920)步骤中。例如，在处理缺失值时，如果在交叉验证之前就对整个数据集进行**插补（imputation）**，那么在任何一折（fold）的验证中，用于训练[插补模型](@entry_id:169403)（如计算均值）的数据已经包含了该验证集的信息。这是一种微妙但严重的信息泄露，它使得[验证集](@entry_id:636445)不再是“未见过”的数据，从而导致性能评估的乐观偏误。这个问题与插补过程是否使用标签无关，因为特征空间本身的信息已经被泄露。[@problem_id:4585237]

解决这些问题的唯一严谨方法是**[嵌套交叉验证](@entry_id:176273)（Nested Cross-Validation）**。嵌套CV包含两层循环：
- **外层循环**：将数据划分为$K_o$折，其唯一目的是进行最终的性能评估。每次循环中，一折作为最终的[测试集](@entry_id:637546)，其余作为训练集。
- **内层循环**：在每次外层循环的训练集上，再进行一次独立的$K_i$-折交叉验证。内层循环的唯一目的是为当前外层循环选择最佳的超参数组合，以及确定最佳的预处理流程。

在每个外层循环中，超参数选择和模型训练的整个过程（包括所有预处理步骤，如插补）都只使用当前的训练数据完成，而完全不接触外层测试集。然后，用选出的最佳模型对外层[测试集](@entry_id:637546)进行一次评估。最后，将所有外层循环得到的性能评估结果进行平均，这个平均值才是对整个“调优加训练”流程真实泛化能力的一个无偏估计。[@problem_id:4320596] [@problem_id:4585237]

#### 构建更强大的模型：[集成学习](@entry_id:637726)

[集成学习](@entry_id:637726)（Ensemble Learning）是一种通过组合多个模型的预测来获得更好性能的强大技术。**堆叠（Stacking）**或称[堆叠泛化](@entry_id:636548)（stacked generalization）是其中一种高级形式。

Stacking的基本思想是训练一个“[元学习器](@entry_id:637377)”（meta-learner），这个[元学习器](@entry_id:637377)的输入不是原始特征，而是多个“基学习器”（base learners）的预测结果。这里的关键挑战在于如何为[元学习器](@entry_id:637377)生成训练数据而又不引入数据泄露。如果直接用基学习器在整个[训练集](@entry_id:636396)上训练，再用其对同一训练集的预测来训练[元学习器](@entry_id:637377)，那么基学习器的任何过拟合都会被[元学习器](@entry_id:637377)学到，导致最终模型的泛化能力很差。

正确的做法是利用交叉验证来生成**“折外”（Out-of-Fold, OOF）**预测。具体流程如下：首先，将训练数据划分为$K$折。然后，在第$j$折循环中，使用除第$j$折之外的所有数据来训练所有的基学习器。训练好后，让这些基学习器对第$j$折数据进行预测。这个预测是“干净”的，因为生成它的模型没有见过第$j$折的任何数据。将这个过程对所有$K$折重复一遍，我们就能为[训练集](@entry_id:636396)中的每一个样本都生成一个OOF预测向量（由所有基学习器的OOF预测组成）。最后，使用这些OOF预测作为特征，原始标签作为目标，来训练[元学习器](@entry_id:637377)。这个过程确保了[元学习器](@entry_id:637377)是在学习如何修正和组合基学习器在未知数据上的泛化行为，而非它们的[过拟合](@entry_id:139093)行为。[@problem_id:4585256]

#### 超越点预测：不确定性与可靠性

在医疗等高风险领域，一个模型仅仅给出预测结果是不够的，它还必须能够量化其预测的**不确定性（uncertainty）**。这对于建立模型的可靠性、决定何时可以信任模型、何时需要人类专家介入至关重要。预测不确定性可以分解为两种主要类型：

- **[认知不确定性](@entry_id:149866)（Epistemic Uncertainty）**：这是模型自身的不确定性，源于训练数据的有限性。如果给予模型更多的数据，这种不确定性可以被降低。它反映了模型在其不熟悉的输入区域“不知道自己不知道”的程度。
- **[偶然不确定性](@entry_id:154011)（Aleatoric Uncertainty）**：这是数据本身固有的、不可约减的随机性或噪声。即使拥有无限的数据，这种不确定性依然存在。例如，在[医学影像](@entry_id:269649)中，由于成像过程中的随机噪声或疾病本身的内在异质性，即使是完美的模型也无法做出100%确定的预测。

通过结合**模型集成（ensembles）**和**测试时数据增强（test-time augmentation, TTA）**等技术，我们可以对这两种不确定性进行分解和估计。例如，通过训练多个具有不同随机初始化的模型，这些模型预测结果之间的一致性（或方差）可以用来估计[认知不确定性](@entry_id:149866)。模型预测越不一致，认知不确定性越高。而在单个模型上，通过对输入图像进行多次随机增强（如旋转、缩放）并观察预测结果的变异，可以估计由数据噪声引起的[偶然不确定性](@entry_id:154011)。通过总方差公式，可以将总的预测方差严谨地分解为这两部分。评估这些[不确定性估计](@entry_id:191096)的质量同样重要，例如，可以使用风险-覆盖率曲线（Risk-Coverage Curve）来评估认知不确定性是否有助于识别出模型可能犯错的样本并拒绝预测。[@problem_id:4585257]

与[量化不确定性](@entry_id:272064)相关的一个重要任务是**分布外（Out-of-Distribution, OOD）检测**。一个安全的医疗AI系统应该能够识别出那些与它训练时所见数据分布显著不同的输入样本（例如，来自不同成像设备或罕见病症的图像），并拒绝给出预测。OOD检测器通常会为每个输入计算一个“异常分数”。评估这类检测器的性能，通常是将OOD检测问题视为一个二分类问题（将OOD样本视为“正类”），然后计算其AUROC，它衡量了检测器在所有可能阈值下区分分布内和分布外样本的能力。[@problem_id:4585296]

### 更广阔的视角：预测与因果推断

在医学数据分析领域，有两个核心但经常被混淆的分析目标：**风险预测（prediction）**和**因果效应估计（causal effect estimation）**。虽然两者都使用相似的[统计模型](@entry_id:755400)，但它们的目标、假设、模型选择和验证策略存在根本性的区别。

- **风险预测**的目标是为新个体准确地估计其未来发生某个结果的概率。
    - **目标估计量（Estimand）**：条件期望函数 $E[Y|X]$。我们关心的是$X$和$Y$之间的**关联**。
    - **核心假设**：训练数据和部署数据来自相同或足够相似的分布（即可移植性）。
    - **[模型选择](@entry_id:155601)与验证**：目标是最小化预测误差。我们通过[交叉验证](@entry_id:164650)来选择能最大化区分度（AUROC）和校准度的模型。验证的重点是模型在未见数据上的预测准确性。

- **因果推断**的目标是估计一个干预（treatment）对结局的影响，即回答“如果我采取行动A而不是行动B，结局会发生什么改变？”这样的反事实问题。
    - **目标估计量**：反事实的量，例如平均[处理效应](@entry_id:636010)（ATE） $E[Y^1 - Y^0]$，表示如果整个人群都接受治疗与都不接受治疗相比，结局的平均差异。我们关心的是干预与结局之间的**因果**关系。
    - **核心假设**：由于我们无法在同一个人身上同时观察到接受和不接受干预的两种结果，我们需要强有力的、通常无法检验的假设来从观测数据中识别因果效应。这些假设包括：**条件可交换性**（无未测量混杂因素）、**正性**（所有类型的患者都有可能接受或不接受干预）和**一致性**等。
    - **[模型选择](@entry_id:155601)与验证**：目标是获得因果效应参数的[无偏估计](@entry_id:756289)。这通常涉及对“滋扰函数”（nuisance functions）如倾向性得分模型$P(A|X)$和结局[回归模型](@entry_id:163386)$E(Y|A,X)$进行建模。模型的选择标准不是最大化对结局$Y$的预测准确性，而是最小化最终因果估计量的偏误。验证的重点是检查假设的合理性，例如，评估干预组和[对照组](@entry_id:188599)在调整混杂因素后的**协变量平衡**情况，以及进行**[敏感性分析](@entry_id:147555)**来评估未测量混杂可能带来的影响。

总之，一个优秀的预测模型可能完全无法给出正确的因果结论，反之亦然。例如，一个包含疾病后果（如某个生物标志物）的预测模型可能预测能力很强，但该标志物本身并非病因。混淆这两个任务，例如用预测模型的[AUROC](@entry_id:636693)来评估一个因果效应模型，或用因果推断的框架来构建一个纯粹的风险预测器，都是严重的方法论错误。为特定的临床问题选择正确的分析框架和相应的评估策略，是数据科学在医学中成功应用的前提。[@problem_id:4985134]