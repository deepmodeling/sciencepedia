## 引言
你是否曾想过，在嘈杂的鸡尾酒会中，我们的大脑如何能奇迹般地锁定一个人的声音，而忽略周围的喧嚣？这个被称为“鸡尾酒会问题”的现象，正是独立成分分析（Independent Component Analysis, ICA）试图在数学上解决的核心挑战。作为一种强大的[盲源分离](@entry_id:196724)技术，ICA的目标是从观测到的混合信号中恢复出原始的、相互独立的未知信源。与主成分分析（PCA）等仅依赖于二阶统计量（如相关性）的方法不同，ICA深入挖掘数据的高阶统计特性，从而解决了传统方法无法应对的难题。

本文将系统性地引导您深入理解ICA的世界。在第一章“原理与机制”中，我们将揭示ICA的数学模型、非高斯性核心原理以及FastICA等经典算法的运作方式。接着，在第二章“应用与跨学科联系”中，我们将探索ICA如何在脑电图（EEG）伪影去除、功能磁共振成像（fMRI）脑[网络分析](@entry_id:139553)以及基因组学数据解卷积等前沿领域中发挥关键作用。最后，在第三章“动手实践”中，您将通过具体的编程练习，将理论知识转化为解决实际问题的能力。通过这一系列的学习，您将不仅掌握ICA的技术细节，更能培养出在复杂数据中发现潜在结构的洞察力。

## 原理与机制

独立成分分析（Independent Component Analysis, ICA）旨在解决一个经典而富有挑战性的问题：如何从一组混合信号中分离出原始的、独立的源信号。这个问题常被称为“鸡尾酒会问题”—— 在嘈杂的派对上，我们的耳朵（和大脑）能奇迹般地专注于一个人的声音，而忽略其他人的谈话和背景音乐。ICA正是这一过程的数学化身。本章将深入探讨支撑ICA的基本原理和实用机制，阐述其如何实现[盲源分离](@entry_id:196724)（Blind Source Separation）。

### 基本模型及其固有模糊性

ICA的核心是一个线性瞬时混合模型。假设我们有 $n$ 个未知的、相互统计独立的源信号 $s_1(t), s_2(t), \dots, s_n(t)$，它们可以被组织成一个源向量 $\mathbf{s}(t) = [s_1(t), \dots, s_n(t)]^\top$。在任意时刻 $t$，我们通过 $m$ 个传感器观测到一组混合信号 $x_1(t), x_2(t), \dots, x_m(t)$，构成观测向量 $\mathbf{x}(t) = [x_1(t), \dots, x_m(t)]^\top$。IC[A模型](@entry_id:158323)假设观测信号是源信号的[线性组合](@entry_id:155091) [@problem_id:4572752]：

$$
\mathbf{x}(t) = A \mathbf{s}(t)
$$

其中，$A$ 是一个未知的 $m \times n$ **混合矩阵**（mixing matrix）。“[盲源分离](@entry_id:196724)”的“盲”字正体现在我们既不知道混合矩阵 $A$，也不知道源信号 $\mathbf{s}(t)$。我们的目标是仅通过观测数据 $\mathbf{x}(t)$ 来估计源信号 $\mathbf{s}(t)$。这通常通过寻找一个 $n \times m$ 的**解混矩阵**（unmixing matrix）$W$ 来实现，使得向量 $\mathbf{y}(t) = W \mathbf{x}(t)$ 的分量尽可能地相互独立，从而成为对原始源信号 $\mathbf{s}(t)$ 的良好估计。

然而，在探索如何找到 $W$ 之前，我们必须认识到ICA问题存在两个固有的、无法避免的模糊性（ambiguities）：**尺度模糊性**（scaling ambiguity）和**置换模糊性**（permutation ambiguity）。

我们可以通过一个简单的代数变换来理解这一点 [@problem_id:4572808]。考虑任意一个可逆的对角矩阵 $D$ 和任意一个[置换矩阵](@entry_id:136841) $P$。[置换矩阵](@entry_id:136841) $P$ 的作用是重新排列向量中元素的位置，而[对角矩阵](@entry_id:637782) $D$ 的作用是缩放每个元素。现在，我们定义一组新的源信号 $\mathbf{s}' = D P \mathbf{s}$ 和一个新的混合矩阵 $A' = A P^{-1} D^{-1}$。让我们看看这对新的组合会产生什么观测信号：

$$
A' \mathbf{s}' = (A P^{-1} D^{-1}) (D P \mathbf{s}) = A (P^{-1} P) (D^{-1} D) \mathbf{s} = A I I \mathbf{s} = A \mathbf{s} = \mathbf{x}
$$

这个结果表明，$(\mathbf{s}', A')$ 和 $(\mathbf{s}, A)$ 这两组不同的源信号和混合矩阵，产生了完全相同的观测数据 $\mathbf{x}$。由于ICA算法只能接触到 $\mathbf{x}$，它没有任何依据来区分原始的解 $(\mathbf{s}, A)$ 和变换后的解 $(\mathbf{s}', A')$。这意味着：

1.  **尺度模糊性**：我们无法确定源信号的真实幅度。如果一个源信号 $s_i$ 被乘以一个因子 $\alpha$，而混合矩阵的对应列被除以 $\alpha$，那么观测信号将保持不变。因此，ICA恢复的独立成分的尺度是任意的。
2.  **置换模糊性**：我们无法确定源信号的真实顺序。ICA算法可以成功地分离出所有源信号，但无法告知我们哪个恢复的信号对应于哪个原始信号。

这两个模糊性是IC[A模型](@entry_id:158323)的内禀属性。因此，ICA的成功标准并非完美恢复 $\mathbf{s}$，而是在这些模糊性（即任意的缩放和排序）的前提下恢复源信号。

### [统计独立性](@entry_id:150300)的作用

IC[A模型](@entry_id:158323)的核心假设是源信号 $\mathbf{s}(t)$ 的各个分量是**统计独立的**。这是一个比“不相关”更强的条件，理解它们的区别是掌握ICA的关键。

对于两个随机变量 $X$ 和 $Y$，**二阶不相关**（second-order uncorrelatedness）意味着它们的协方差为零：

$$
\operatorname{Cov}(X, Y) = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])] = 0
$$

如果 $X$ 和 $Y$ 是零均值的，这个条件简化为 $\mathbb{E}[XY] = 0$。不相关性仅描述了变量之间不存在**线性**关系。

相比之下，**[统计独立性](@entry_id:150300)**（statistical independence）是一个更深刻的概念。如果两个随机变量 $X$ 和 $Y$ 是独立的，那么关于一个变量取值的信息不会提供任何关于另一个变量取值的信息。在数学上，这意味着它们的[联合概率密度函数](@entry_id:267139)（joint PDF）可以分解为各自边缘概率密度函数（marginal PDF）的乘积 [@problem_id:4572756]：

$$
p_{X,Y}(x,y) = p_X(x) p_Y(y)
$$

[统计独立性](@entry_id:150300)意味着对于任意函数 $g$ 和 $h$，都有 $\mathbb{E}[g(X)h(Y)] = \mathbb{E}[g(X)]\mathbb{E}[h(Y)]$。不相关性仅仅是这个性质在 $g(x)=x$ 和 $h(y)=y$ 时的特例。因此，独立性必然导致不相关性，但反之不然。

为了具体说明这一点，我们可以构造一个经典的反例 [@problem_id:4572756]。假设 $X$ 是一个标准正态分布的随机变量，即 $X \sim \mathcal{N}(0,1)$，并定义另一个随机变量 $Y = X^2 - 1$。显然，$Y$ 的值完全由 $X$ 决定，因此它们是高度依赖的。例如，如果我们知道 $X=2$，我们就能确定 $Y=3$。然而，让我们计算它们的协方差：

首先，$\mathbb{E}[X] = 0$。其次，$\mathbb{E}[Y] = \mathbb{E}[X^2 - 1] = \mathbb{E}[X^2] - 1$。由于 $X$ 的方差为 $1$，$\operatorname{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2 = 1$，我们得到 $\mathbb{E}[X^2] = 1$。因此，$\mathbb{E}[Y] = 1 - 1 = 0$。

现在计算协方差：
$$
\operatorname{Cov}(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y] = \mathbb{E}[X(X^2 - 1)] - (0)(0) = \mathbb{E}[X^3] - \mathbb{E}[X]
$$
对于对称于零的标准正态分布，所有奇数阶矩（odd-order moments）都为零，所以 $\mathbb{E}[X^3]=0$ 且 $\mathbb{E}[X]=0$。因此，$\operatorname{Cov}(X, Y) = 0 - 0 = 0$。

这个例子清晰地表明，$X$ 和 $Y$ 是不相关的，但它们远非独立。这种区别正是ICA与许多其他方法的分野所在。

### 为何[主成分分析](@entry_id:145395)（PCA）还不够

[主成分分析](@entry_id:145395)（Principal Component Analysis, PCA）是一种广泛使用的数据[降维](@entry_id:142982)和特征提取技术。其目标是找到一组正交的基向量（主成分），使得数据在这些方向上的投影方差最大化。经过PCA变换后的数据分量是**不相关**的。鉴于我们刚刚讨论了[不相关与独立](@entry_id:264327)的区别，我们不难预见PCA在[盲源分离](@entry_id:196724)任务上的局限性 [@problem_id:4572825]。

1.  **PCA的目标是方差最大化，而非独立性。** PCA通过对数据的协方差矩阵进行[特征分解](@entry_id:181333)来找到主成分方向。这是一种只依赖于二阶统计量（方差和协方差）的方法。正如我们所见，仅仅使分量不相关并不足以保证它们的独立性。

2.  **PCA对旋转不敏感。** Whitening（白化）是PCA的一个常见应用，它将数据变换为具有单位协方差矩阵的形式。白化后的数据分量是不相关的，且方差均为1。然而，对白化数据进行任意的旋转（[正交变换](@entry_id:155650)），得到的新数据仍然是白化的（不相关且单位方差）。PCA无法在这些旋转中作出选择，因为它无法分辨哪一个旋转对应于真正的独立源信号。

让我们通过两个场景来具体说明PCA的失败 [@problem_id:4572825]：

*   **场景1：正交混合与等方差源。** 假设源信号 $s_1$ 和 $s_2$ 独立且方差相等（$\operatorname{Var}(s_1) = \operatorname{Var}(s_2) = \sigma^2$），混合矩阵 $A$ 是一个[正交矩阵](@entry_id:169220)（即旋转）。观测数据 $x$ 的协方差矩阵为 $\Sigma_x = A \Sigma_s A^\top = A (\sigma^2 I) A^\top = \sigma^2 A A^\top = \sigma^2 I$。协方差矩阵是一个单位矩阵的倍数，这意味着数据在所有方向上的方差都相等。此时数据分布是“球形”的，PCA的特征值是简并的，它无法找到任何“主”方向。PCA完全失效。

*   **场景2：非正交混合。** 假设混合矩阵 $A$ 的列向量（即混合方向）不是相互正交的。PCA找到的主成分方向总是正交的，因为它们是协方差矩阵的特征向量。因此，PCA找到的方向不可能与非正交的混合方向重合。PCA会找到数据云的[主轴](@entry_id:172691)，但这与分离源信号所需的解混方向是不同的。

综上所述，PCA是一种强大的工具，用于 decorrelation 和降维，但它的目标和能力与ICA的[盲源分离](@entry_id:196724)目标有着本质区别。要实现分离，我们需要比二阶统计量更强大的工具。

### 非高斯性原理

ICA能够超越PCA的限制，其成功的秘诀在于一个深刻的统计学原理：**非高斯性**（non-Gaussianity）。

这个原理的直观理解来自于**[中心极限定理](@entry_id:143108)**（Central Limit Theorem, CLT）。CLT告诉我们，大量[独立随机变量](@entry_id:273896)的和（经过适当标准化后）的分布趋向于高斯分布（正态分布），无论[原始变量](@entry_id:753733)的分布是什么。我们可以将IC[A模型](@entry_id:158323)中的混合信号 $x_j = \sum_{i=1}^n a_{ji} s_i$ 视为源信号 $s_i$ 的加权和。根据CLT的精神，这个和（混合信号）的分布通常会比任何单个源信号的分布“更接近”高斯分布 [@problem_id:4572793, @problem_id:4572777]。

反过来思考，这意味着如果我们有一个混合信号，并试图找到一个[线性变换](@entry_id:143080)来最大化其输出的**非高斯性**，我们就有可能逆转混合过程，从而恢复出原始的独立成分。换句话说，ICA算法的策略就是寻找一个解混矩阵 $W$，使得输出 $y = Wx$ 的每个分量 $y_i$ 的非高斯性达到最大。当 $y_i$ 的非高斯性最大化时，它通常就对应于一个原始的源信号 $s_j$。

这一思想引出了ICA最根本的**[可辨识性](@entry_id:194150)定理**（identifiability theorem）[@problem_id:4572719, @problem_id:4572752]：

> 在噪声无关的ICA模型 $\mathbf{x} = A \mathbf{s}$ 中，如果源信号 $\mathbf{s}$ 的各分量是统计独立的，那么只要**最多只有一个源信号是高斯分布**，混合矩阵 $A$ 和源信号 $\mathbf{s}$ 就是可辨识的（在尺度和置换模糊性下）。

为什么高斯分布如此特殊？原因在于高斯分布的对称性和稳定性：

*   **多维高斯分布的旋转不变性**：如果两个或更多的源信号 $s_i$ 是高斯分布，那么问题就变得无法解决。假设 $s_1$ 和 $s_2$ 是独立的标准[高斯变量](@entry_id:276673)。它们的联合分布是二维[标准正态分布](@entry_id:184509)，在几何上是旋转对称的。这意味着，如果我们对 $(s_1, s_2)$ 进行任意的旋转（[正交变换](@entry_id:155650)），得到的新坐标 $(s'_1, s'_2)$ 仍然是两个独立的标准[高斯变量](@entry_id:276673)。因此，算法无法区分 $(s_1, s_2)$ 和 $(s'_1, s'_2)$，解混矩阵存在无限多个，导致可辨识性失败 [@problem_id:4572719]。
*   **非高斯分布的混合特性**：相反，如果源信号是非高斯分布的，它们的线性混合通常会改变分布的形状（使其“更”高斯化）。根据Darmois-Skitovich等定理，只有当源信号是高斯分布时，它们的[线性组合](@entry_id:155091)才能保持独立性。对于非[高斯源](@entry_id:271482)，要求输出独立会强烈约束解混矩阵，最终使其（在模糊性下）唯一。

因此，非高斯性是ICA能够工作的根本前提。它提供了超越二阶统计量（协方差）的信息，使我们能够在PCA失效的地方取得成功。

### 度量非高斯性：分离的关键

为了将非高斯性原理付诸实践，我们需要一种数学方法来量化一个分布距离高斯分布有多远。主要有两种方法：[高阶统计量](@entry_id:193349)和信息论。

#### [高阶统计量](@entry_id:193349)：[峰度](@entry_id:269963)

最简单的高阶[累积量](@entry_id:152982)（cumulant）之一是四阶[累积量](@entry_id:152982)，通常用**峰度**（kurtosis）来衡量。对于一个经过标准化（零均值，单位方差）的随机变量 $y$，其（超额）峰度定义为：

$$
\operatorname{kurt}(y) = \mathbb{E}[y^4] - 3
$$

对于高斯分布，$\mathbb{E}[y^4] = 3$，所以其峰度为0。
*   **超高斯分布**（Super-Gaussian）：[峰度](@entry_id:269963)大于0，分布比高斯分布更“尖峰”，尾部更“重”，如[拉普拉斯分布](@entry_id:266437)。
*   **亚高斯分布**（Sub-Gaussian）：峰度小于0，分布比高斯分布更“平坦”，尾部更“轻”，如均匀分布。

[峰度](@entry_id:269963)的一个关键性质是其可加性 [@problem_id:4572793]。对于两个独立的零均值随机变量 $s_1$ 和 $s_2$，它们的和 $y=c_1 s_1 + c_2 s_2$ (假设 $y$ 被标准化为单位方差) 的[峰度](@entry_id:269963) $|\operatorname{kurt}(y)|$ 通常小于源信号的[峰度](@entry_id:269963)。具体来说，当 $y = \sum c_i s_i$ 且 $\sum c_i^2 = 1$ 时，我们有 $\operatorname{kurt}(y) = \sum c_i^4 \operatorname{kurt}(s_i)$。由于 $\sum c_i^4 \le (\sum c_i^2)^2 = 1$，混合信号的[峰度](@entry_id:269963)绝对值不会超过源信号中的最大[峰度](@entry_id:269963)绝对值。当且仅当只有一个 $c_i$ 非零时（即 $y$ 就是某个源信号），等号成立。

因此，最大化输出信号的[峰度](@entry_id:269963)绝对值，是找到独立成分的一个有效策略。

#### 信息论：[负熵](@entry_id:194102)

虽然峰度简单，但它对异常值很敏感，且在某些情况下可能产生误导。一个更稳健、更具理论基础的非高斯性度量来自于信息论，即**[负熵](@entry_id:194102)**（negentropy）。

首先，一个[连续随机变量](@entry_id:166541) $y$ 的**[微分熵](@entry_id:264893)**（differential entropy）定义为 [@problem_id:4572777]：

$$
H(y) = - \int p(y) \ln p(y) dy
$$

熵度量了随机变量的不确定性。信息论中一个著名的结论是：**在所有具有相同方差的分布中，高斯分布的熵最大。**

利用这一性质，我们可以定义[负熵](@entry_id:194102) $J(y)$ 为：

$$
J(y) = H(y_{\text{gauss}}) - H(y)
$$

其中 $y_{\text{gauss}}$ 是一个与 $y$ 具有相同均值和方差的高斯随机变量。根据熵的最大化性质，$J(y) \ge 0$，并且当且仅当 $y$ 是高斯分布时 $J(y) = 0$。因此，[负熵](@entry_id:194102)是一个理想的非高斯性度量：它总是非负的，且当变量越偏离高斯分布时值越大。ICA的目标可以重新表述为寻找一个投影方向，使得输出的[负熵](@entry_id:194102)最大化。

### 实用机制与算法

理解了ICA的基本原理后，我们来看看一个典型的ICA算法是如何工作的。大多数ICA算法包含两个主要步骤：预处理和优化。

#### 步骤一：预处理 — 中心化与白化

在应用核心ICA算法之前，对数据进行预处理是至关重要的一步。

1.  **中心化（Centering）**：将数据减去其均值，使其均值为零。即 $x \leftarrow x - \mathbb{E}[x]$。这是大多数ICA模型的技术要求。

2.  **白化（Whitening）**：白化是一种[线性变换](@entry_id:143080)，旨在使数据的协方差矩阵变为[单位矩阵](@entry_id:156724) $I$。这意味着白化后的数据各分量是不相关的，并且方差为1。白化可以通过对数据的协方差矩阵 $\Sigma_x = \mathbb{E}[xx^\top]$ 进行[特征值分解](@entry_id:272091)来实现 [@problem_id:3990037]。

    设 $\Sigma_x = E D E^\top$，其中 $E$ 是特征向量组成的[正交矩阵](@entry_id:169220)，$D$ 是特征值组成的[对角矩阵](@entry_id:637782)。白化矩阵 $V$ 可以构造为：

    $$
    V = D^{-1/2} E^\top
    $$

    其中 $D^{-1/2}$ 是将 $D$ 的对角元素取其平方根的倒数后得到的对角矩阵。变换后的数据 $z = Vx$ 满足：
    $$
    \mathbb{E}[zz^\top] = \mathbb{E}[(Vx)(Vx)^\top] = V \mathbb{E}[xx^\top] V^\top = V \Sigma_x V^\top = (D^{-1/2} E^\top) (E D E^\top) (E D^{-1/2}) = I
    $$
    
    例如，对于协方差矩阵 $\Sigma_x = \begin{pmatrix} 5  4 \\ 4  5 \end{pmatrix}$ [@problem_id:3990037]，其特征值为 $\lambda_1=9, \lambda_2=1$，对应的归一化特征向量为 $e_1 = \frac{1}{\sqrt{2}}\begin{pmatrix} 1 \\ 1 \end{pmatrix}, e_2 = \frac{1}{\sqrt{2}}\begin{pmatrix} 1 \\ -1 \end{pmatrix}$。因此，$E = \begin{pmatrix} \frac{1}{\sqrt{2}}  \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}  -\frac{1}{\sqrt{2}} \end{pmatrix}$，$D^{-1/2} = \begin{pmatrix} 1/3  0 \\ 0  1 \end{pmatrix}$。白化矩阵为 $V = D^{-1/2} E^\top = \begin{pmatrix}\frac{1}{3\sqrt{2}}  \frac{1}{3\sqrt{2}} \\ \frac{1}{\sqrt{2}}  -\frac{1}{\sqrt{2}}\end{pmatrix}$。

    白化的关键作用是**简化问题**。原始模型为 $x=As$。白化后 $z = Vx = VAs$。令 $Q=VA$，则 $z=Qs$。由于 $z$ 的协方差矩阵为 $I$，我们有 $I = \mathbb{E}[zz^\top] = \mathbb{E}[(Qs)(Qs)^\top] = Q \mathbb{E}[ss^\top] Q^\top$。如果源信号也被标准化为单位方差（即 $\mathbb{E}[ss^\top]=I$），那么 $I = QQ^\top$。这意味着新的混合矩阵 $Q$ 是一个**[正交矩阵](@entry_id:169220)**（即旋转或反射）。因此，白化将寻找任意可逆矩阵 $A$ 的复杂问题，简化为寻找一个正交矩阵 $Q$ 的问题 [@problem_id:4572805, @problem_id:4572752]。

#### 步骤二：优化 — 寻找旋转

白化之后，我们的任务是在这个被“漂白”的空间中找到正确的“旋转”角度，以对齐到各个独立成分的轴。这通过优化非高斯性度量来完成。**FastICA** 是实现这一目标的最流行和最高效的算法之一。

FastICA通过一种巧妙的方式来逼近[负熵](@entry_id:194102)，从而避免了直接计算[概率密度](@entry_id:143866)和积分的困难 [@problem_id:4572723]。它使用以下近似关系：

$$
J(y) \approx [\mathbb{E}\{G(y)\} - \mathbb{E}\{G(v)\}]^2
$$

其中，$y$ 是单位方差的投影输出，$v$ 是标准正态变量，$G$ 是一个精心选择的**非二次函数**。如果 $G$ 是二次函数（如 $G(u)=u^2$），则 $\mathbb{E}\{G(y)\} - \mathbb{E}\{G(v)\} = \mathbb{E}\{y^2\} - \mathbb{E}\{v^2\} = 1-1=0$，这个近似就失去了意义。因此，必须选择非二次函数 $G$ 来捕获高阶统计信息。

FastICA算法中常用的 $G$ 函数包括 [@problem_id:4572723]：

1.  $G_1(u) = \frac{1}{a} \log \cosh(au)$：这是一个通用且稳健的选择，尤其适用于分离超[高斯源](@entry_id:271482)（如生物医学信号中的稀疏脉冲）。
2.  $G_2(u) = -\exp(-u^2/2)$：与 $G_1$ 类似，这也是一个对异常值稳健的选择。
3.  $G_3(u) = \frac{u^4}{4}$：这个选择实质上是优化峰度，对亚[高斯源](@entry_id:271482)信号更敏感。

FastICA采用一种高效的[定点迭代](@entry_id:137769)算法，通过使用 $G$ 的导数 $g=G'$ 来快速收敛到最大化非高斯性的投影方向。

#### 替代机制：利用时间结构

除了基于[高阶统计量](@entry_id:193349)的方法外，如果源信号具有时间结构（即时间上不是[白噪声](@entry_id:145248)），我们还可以利用二阶统计量来分离信号。这类方法的代表是 **SOBI (Second Order Blind Identification)** [@problem_id:4572805]。

其核心思想如下：在白化空间中，我们有 $z(t) = Qs(t)$。让我们考察具有时间延迟 $\tau$ 的协方差矩阵 $R_z(\tau) = \mathbb{E}[z(t) z(t-\tau)^\top]$。
$$
R_z(\tau) = \mathbb{E}[(Qs(t))(Qs(t-\tau))^\top] = Q \, \mathbb{E}[s(t)s(t-\tau)^\top] \, Q^\top
$$
由于源信号 $s_i$ 是相互独立的，$\mathbb{E}[s(t)s(t-\tau)^\top] = R_s(\tau)$ 是一个[对角矩阵](@entry_id:637782)。因此，我们得到 $R_z(\tau) = Q D_\tau Q^\top$，其中 $D_\tau$ 是一个[对角矩阵](@entry_id:637782)。

这意味着，对于**任意**的时间延迟 $\tau$，白化后信号的延迟协方差矩阵 $R_z(\tau)$ 都被同一个[正交矩阵](@entry_id:169220) $Q$ 对角化。因此，我们可以通过**联合对角化**（joint diagonalization）一组不同延迟的协方差矩阵 $\{R_z(\tau_1), R_z(\tau_2), \dots \}$ 来稳健地估计出正交矩阵 $Q$。这种方法避免了对[高阶统计量](@entry_id:193349)的计算，在处理具有丰富时间结构的信号（如脑电图EEG或功能磁共振成像fMRI信号）时非常有效。