{"hands_on_practices": [{"introduction": "理论是抽象的，而实践则赋予其生命。本节的第一个练习将引导您手动完成梯度提升机（GBM）的两次迭代过程，以此来揭开其神秘面纱。通过在一个简单的玩具数据集上使用绝对值损失函数，您将亲身体验泛函梯度下降的核心思想，理解模型如何通过顺序拟合残差的符号来逐步逼近目标函数 [@problem_id:4544498]。", "problem": "一个研究团队正在建立一个预测模型，用于根据从组学图谱中提取的两个协变量来估计患者的对数转换后的肿瘤增殖指数：单个通路聚合基因评分的归一化对数表达值 $x_{1}$ 和一个β值甲基化分数 $x_{2}$。他们决定在经验风险最小化（ERM）框架下，使用梯度提升机（GBM）和绝对损失 $\\ell(y,f)=|y-f|$。GBM的应用设置如下：基学习器族由深度为1的回归树（决策树桩）组成，采用轴对齐分裂，学习率 $\\nu=\\frac{1}{2}$，以及两次迭代。这个问题的基本原理是带有绝对损失的ERM和函数空间中的最速下降法，其中训练点上的负函数梯度等于带符号的误差。\n\n给定一个包含 $n=5$ 名患者的玩具训练集：\n- 患者 1：$(x_{1},x_{2},y)=(0.8,0.2,1.0)$。\n- 患者 2：$(x_{1},x_{2},y)=(0.9,0.5,1.5)$。\n- 患者 3：$(x_{1},x_{2},y)=(1.5,0.4,2.0)$。\n- 患者 4：$(x_{1},x_{2},y)=(1.6,0.7,3.0)$。\n- 患者 5：$(x_{1},x_{2},y)=(1.7,0.9,4.0)$。\n\n请遵循以下源自ERM原则和绝对损失函数梯度下降法的科学标准步骤，不要使用任何未经推导的快捷公式：\n1. 将模型 $f_{0}(x)$ 初始化为一个常数，该常数最小化经验绝对偏差 $\\sum_{i=1}^{n}|y_{i}-c|$（其中 $c\\in\\mathbb{R}$）。利用最小化器是 $\\{y_{i}\\}_{i=1}^{n}$ 的中位数这一性质。\n2. 在每次迭代 $m\\in\\{1,2\\}$ 中，计算训练点上的伪残差 $r_{i}^{(m)}=\\operatorname{sign}(y_{i}-f_{m-1}(x_{i}))$，其中如果 $z < 0$，则 $\\operatorname{sign}(z)=-1$；如果 $z=0$，则 $\\operatorname{sign}(z)=0$；如果 $z>0$，则 $\\operatorname{sign}(z)=+1$。\n3. 对 $\\{(x_{i},r_{i}^{(m)})\\}_{i=1}^{n}$ 拟合一个决策树桩，通过选择一个能够一致地分离带符号伪残差的轴对齐分裂。对于迭代 $m=1$，使用在阈值 $x_{1}\\geq 1.4$ 处对 $x_{1}$ 的分裂，产生两个叶节点区域。对于迭代 $m=2$，使用在阈值 $x_{2}\\geq 0.6$ 处对 $x_{2}$ 的分裂，产生两个叶节点区域。\n4. 在每个叶节点区域中执行精确线搜索，以找到最小化受限绝对损失的区域特定常数增量。具体来说，如果在迭代 $m$ 时区域 $R$ 包含索引 $\\mathcal{I}_{R}$，则选择该区域的更新值为当前残差 $\\{y_{i}-f_{m-1}(x_{i})\\}_{i\\in\\mathcal{I}_{R}}$ 的中位数。对于元素数量为偶数的集合，将中位数取为中间两个次序统计量的平均值。\n5. 通过将迭代 $m$ 中拟合的树桩得到的区域特定常数乘以 $\\nu$ 并加到 $f_{m-1}(x)$ 上来更新模型，从而得到 $f_{m}(x)$。\n\n完成两次提升迭代后，对一个特征为 $(x_{1}^{\\ast},x_{2}^{\\ast})=(1.55,0.80)$ 的新患者，评估这个两阶段提升模型 $f_{2}(x)$。将 $f_{2}(x_{1}^{\\ast},x_{2}^{\\ast})$ 的最终值以一个实数的形式给出。不需要四舍五入，最终答案中不报告单位。", "solution": "该问题陈述被认为是有效的，因为它具有科学依据、适定且客观。它描述了带有绝对偏差损失的梯度提升机（GBM）算法的标准实现，并为获得唯一解提供了所有必要的数据和参数。所概述的步骤与函数梯度下降的原理一致。\n\n任务是为一个新数据点 $x^* = (x_{1}^{*}, x_{2}^{*}) = (1.55, 0.80)$ 计算一个两阶段GBM模型 $f_{2}(x)$ 的预测值。该模型在一个包含 $n=5$ 名患者的数据集上进行训练：\n- 患者 1：$(x_{1,1}, x_{2,1}, y_1) = (0.8, 0.2, 1.0)$\n- 患者 2：$(x_{1,2}, x_{2,2}, y_2) = (0.9, 0.5, 1.5)$\n- 患者 3：$(x_{1,3}, x_{2,3}, y_3) = (1.5, 0.4, 2.0)$\n- 患者 4：$(x_{1,4}, x_{2,4}, y_4) = (1.6, 0.7, 3.0)$\n- 患者 5：$(x_{1,5}, x_{2,5}, y_5) = (1.7, 0.9, 4.0)$\n\n算法使用学习率 $\\nu = \\frac{1}{2}$。我们遵循指定的步骤。\n\n**第1步：初始化**\n初始模型 $f_{0}(x)$ 是一个常数 $c$，它最小化绝对损失的经验风险 $\\sum_{i=1}^{n}|y_{i}-c|$。最小化器 $c$ 是目标值 $\\{y_{i}\\}_{i=1}^{n}$ 的中位数。\n目标值集合为 $\\{1.0, 1.5, 2.0, 3.0, 4.0\\}$。\n观测数量为 $n=5$，是奇数。中位数是排序后集合的中间值。\n$$f_{0}(x) = \\operatorname{median}(\\{1.0, 1.5, 2.0, 3.0, 4.0\\}) = 2.0$$\n因此，初始模型是一个常数函数，$f_{0}(x) = 2.0$。\n\n**迭代 $m=1$**\n首先，我们计算伪残差，对于绝对损失而言，伪残差是当前误差的符号：$r_{i}^{(1)} = \\operatorname{sign}(y_{i} - f_{0}(x_{i})) = \\operatorname{sign}(y_{i} - 2.0)$。\n- $r_{1}^{(1)} = \\operatorname{sign}(1.0 - 2.0) = \\operatorname{sign}(-1.0) = -1$\n- $r_{2}^{(1)} = \\operatorname{sign}(1.5 - 2.0) = \\operatorname{sign}(-0.5) = -1$\n- $r_{3}^{(1)} = \\operatorname{sign}(2.0 - 2.0) = \\operatorname{sign}(0) = 0$\n- $r_{4}^{(1)} = \\operatorname{sign}(3.0 - 2.0) = \\operatorname{sign}(1.0) = +1$\n- $r_{5}^{(1)} = \\operatorname{sign}(4.0 - 2.0) = \\operatorname{sign}(2.0) = +1$\n\n接下来，使用在阈值 $1.4$ 处对 $x_{1}$ 的指定分裂，对这些伪残差拟合一个决策树桩。这定义了两个叶节点区域：\n- 区域 $R_{1,1} = \\{x | x_{1}  1.4\\}$：该区域包括患者1和2，所以索引集为 $\\mathcal{I}_{R_{1,1}} = \\{1, 2\\}$。\n- 区域 $R_{1,2} = \\{x | x_{1} \\ge 1.4\\}$：该区域包括患者3、4和5，所以索引集为 $\\mathcal{I}_{R_{1,2}} = \\{3, 4, 5\\}$。\n\n对每个区域执行精确线搜索，以找到最优更新常数 $\\gamma_R^{(1)}$，即该区域中索引 $i$ 对应的残差 $\\{y_i - f_{0}(x_i)\\}$ 的中位数。\n- 对于区域 $R_{1,1}$：残差为 $\\{y_{1} - 2.0, y_{2} - 2.0\\} = \\{-1.0, -0.5\\}$。由于元素数量为偶数，中位数是它们的平均值。\n$$ \\gamma_{R_{1,1}}^{(1)} = \\frac{-1.0 + (-0.5)}{2} = -0.75 $$\n- 对于区域 $R_{1,2}$：残差为 $\\{y_{3} - 2.0, y_{4} - 2.0, y_{5} - 2.0\\} = \\{0.0, 1.0, 2.0\\}$。由于元素数量为奇数，中位数是中间值。\n$$ \\gamma_{R_{1,2}}^{(1)} = 1.0 $$\n\n第一个基学习器模型 $h_1(x)$ 在其各自的区域中产生这些常数值。然后模型被更新：$f_{1}(x) = f_{0}(x) + \\nu h_{1}(x)$。\n$$ f_{1}(x) = 2.0 + \\frac{1}{2} h_{1}(x) = \\begin{cases} 2.0 + \\frac{1}{2}(-0.75) = 1.625  \\text{如果 } x_1  1.4 \\\\ 2.0 + \\frac{1}{2}(1.0) = 2.5  \\text{如果 } x_1 \\ge 1.4 \\end{cases} $$\n\n**迭代 $m=2$**\n我们计算新的伪残差 $r_{i}^{(2)} = \\operatorname{sign}(y_{i} - f_{1}(x_{i}))$。\n- 对于 $i=1$：$x_{1,1}  1.4 \\implies f_1(x_1) = 1.625$。残差是 $1.0 - 1.625 = -0.625$。$r_{1}^{(2)} = -1$。\n- 对于 $i=2$：$x_{1,2}  1.4 \\implies f_1(x_2) = 1.625$。残差是 $1.5 - 1.625 = -0.125$。$r_{2}^{(2)} = -1$。\n- 对于 $i=3$：$x_{1,3} \\ge 1.4 \\implies f_1(x_3) = 2.5$。残差是 $2.0 - 2.5 = -0.5$。$r_{3}^{(2)} = -1$。\n- 对于 $i=4$：$x_{1,4} \\ge 1.4 \\implies f_1(x_4) = 2.5$。残差是 $3.0 - 2.5 = 0.5$。$r_{4}^{(2)} = +1$。\n- 对于 $i=5$：$x_{1,5} \\ge 1.4 \\implies f_1(x_5) = 2.5$。残差是 $4.0 - 2.5 = 1.5$。$r_{5}^{(2)} = +1$。\n\n接下来，使用在阈值 $0.6$ 处对 $x_{2}$ 的指定分裂，拟合一个决策树桩。这定义了两个新的叶节点区域：\n- 区域 $R_{2,1} = \\{x | x_{2}  0.6\\}$：这包括患者1、2和3（其 $x_{2}$ 值分别为 $0.2$, $0.5$, $0.4$）。$\\mathcal{I}_{R_{2,1}} = \\{1, 2, 3\\}$。\n- 区域 $R_{2,2} = \\{x | x_{2} \\ge 0.6\\}$：这包括患者4和5（其 $x_{2}$ 值分别为 $0.7$, $0.9$）。$\\mathcal{I}_{R_{2,2}} = \\{4, 5\\}$。\n\n基于当前的残差 $\\{y_i - f_{1}(x_i)\\}$ 对每个新区域执行精确线搜索。\n- 对于区域 $R_{2,1}$：残差为 $\\{-0.625, -0.125, -0.5\\}$。排序后的集合是 $\\{-0.625, -0.5, -0.125\\}$。中位数是中间值。\n$$ \\gamma_{R_{2,1}}^{(2)} = -0.5 $$\n- 对于区域 $R_{2,2}$：残差为 $\\{0.5, 1.5\\}$。中位数是这两个值的平均值。\n$$ \\gamma_{R_{2,2}}^{(2)} = \\frac{0.5 + 1.5}{2} = 1.0 $$\n\n第二个基学习器 $h_2(x)$ 产生这些值。最终模型是 $f_{2}(x) = f_{1}(x) + \\nu h_{2}(x)$。\n\n**最终预测**\n我们必须对特征为 $(x_{1}^{*}, x_{2}^{*}) = (1.55, 0.80)$ 的新患者评估 $f_{2}(x^{*})$。完整的模型是一个加法展开式：\n$$ f_{2}(x) = f_{0}(x) + \\nu h_{1}(x) + \\nu h_{2}(x) $$\n- 初始预测值为 $f_{0}(x^{*}) = 2.0$。\n- 对于第一个树桩 $h_{1}(x)$，分裂是基于 $x_{1}$。由于 $x_{1}^{*} = 1.55 \\ge 1.4$，该点落入区域 $R_{1,2}$。相应的更新值为 $\\gamma_{R_{1,2}}^{(1)} = 1.0$。所以，$h_{1}(x^{*}) = 1.0$。\n- 对于第二个树桩 $h_{2}(x)$，分裂是基于 $x_{2}$。由于 $x_{2}^{*} = 0.80 \\ge 0.6$，该点落入区域 $R_{2,2}$。相应的更新值为 $\\gamma_{R_{2,2}}^{(2)} = 1.0$。所以，$h_{2}(x^{*}) = 1.0$。\n\n将这些分量代入最终模型方程：\n$$ f_{2}(1.55, 0.80) = f_{0}(x^{*}) + \\nu h_{1}(x^{*}) + \\nu h_{2}(x^{*}) $$\n$$ f_{2}(1.55, 0.80) = 2.0 + \\left(\\frac{1}{2}\\right)(1.0) + \\left(\\frac{1}{2}\\right)(1.0) $$\n$$ f_{2}(1.55, 0.80) = 2.0 + 0.5 + 0.5 = 3.0 $$", "answer": "$$\\boxed{3.0}$$", "id": "4544498"}, {"introduction": "梯度提升框架的强大之处在于其普适性，它能够适配任何可微的损失函数，从而解决各种类型的预测问题。这个练习将展示如何将梯度提升应用于对数线性泊松模型，这在对临床事件计数等数据进行建模时非常有用。您将从第一性原理出发，推导泊松损失下的伪残差，并计算特定终端节点的最佳更新值，从而加深对该框架灵活性的理解 [@problem_id:4544511]。", "problem": "一个医院信息学团队正在构建一个梯度提升机（GBM），用于从高维电子健康记录中预测临床事件（三十天急性加重）的计数。假设该计数的数据生成模型服从泊松分布，其中条件均值通过具有标准对数连接的广义线性模型（GLM）与特征 $\\mathbf{x}$ 相关联，因此模型响应为 $f(\\mathbf{x})=\\log \\lambda(\\mathbf{x})$，逐点损失为负对数似然（在相差一个加性常数的情况下），由 $\\ell(y,f)=\\exp(f)-y f$ 给出。GBM通过将回归树拟合到从逐点损失计算出的伪残差来更新当前模型 $f$，然后对终端节点值执行线搜索。\n\n从伪残差的定义（即经验风险相对于在每个训练点上评估的当前模型响应的负梯度）出发，推导出泊松损失下伪残差 $r_i$ 关于 $y_i$ 和 $f(\\mathbf{x}_i)$ 的显式解析形式。然后，解释应如何将回归树拟合到这些 $r_i$ 上，以改进临床事件计数的模型。请从最小化经验风险这一基本目标出发，并且不假设任何预先指定的更新公式。\n\n最后，考虑在某次GBM迭代中，基于伪残差构建的浅层回归树的单个终端区域 $\\mathcal{R}$。设该区域包含四名患者，其观测计数和当前对数均值预测如下：\n- 患者 1：$y_1=4$, $f(\\mathbf{x}_1)=1.2$\n- 患者 2：$y_2=0$, $f(\\mathbf{x}_2)=-0.5$\n- 患者 3：$y_3=1$, $f(\\mathbf{x}_3)=0.0$\n- 患者 4：$y_4=3$, $f(\\mathbf{x}_4)=0.7$\n\n将该终端节点的输出视为一个常数偏移量 $\\gamma$，对于所有 $\\mathbf{x}\\in\\mathcal{R}$，该偏移量被加到 $f(\\mathbf{x})$ 上。确定使限制在 $\\mathcal{R}$ 内的经验泊松损失最小化的精确值 $\\gamma_{\\mathcal{R}}^{\\star}$。给出 $\\gamma_{\\mathcal{R}}^{\\star}$ 的最终数值，四舍五入到四位有效数字。最终答案无需单位。", "solution": "该问题要求对用于泊松分布计数数据的梯度提升机（GBM）进行三部分分析。首先，我们必须推导伪残差的解析形式。其次，我们必须解释将回归树拟合到这些残差上的过程。第三，我们必须计算树的特定终端节点的最优更新值。\n\n**第1部分：伪残差的推导**\n\n第 $i$ 个训练观测的伪残差（记为 $r_i$）被定义为逐点损失函数 $\\ell(y_i, f)$ 相对于模型当前预测 $f(\\mathbf{x}_i)$ 的负梯度。总经验风险是所有 $N$ 个训练点上逐点损失的总和，$R_{\\text{emp}} = \\sum_{i=1}^{N} \\ell(y_i, f(\\mathbf{x}_i))$。对于第 $i$ 个观测，函数空间中的梯度下降步长与该点处损失的负偏导数方向一致。\n\n对于观测 $(\\mathbf{x}_i, y_i)$ 和当前模型预测 $f_i = f(\\mathbf{x}_i)$，伪残差 $r_i$ 由下式给出：\n$$\nr_i = - \\left[ \\frac{\\partial \\ell(y, f)}{\\partial f} \\right]_{y=y_i, f=f_i}\n$$\n问题指定了带对数连接的泊松模型的逐点损失函数（在相差一个加性常数的情况下）为：\n$$\n\\ell(y, f) = \\exp(f) - y f\n$$\n这里，$f = f(\\mathbf{x}) = \\log \\lambda(\\mathbf{x})$，其中 $\\lambda(\\mathbf{x})$ 是泊松分布的条件均值。因此，$\\lambda(\\mathbf{x}) = \\exp(f(\\mathbf{x}))$。\n\n为求伪残差，我们首先计算 $\\ell(y, f)$ 关于 $f$ 的偏导数：\n$$\n\\frac{\\partial \\ell}{\\partial f} = \\frac{\\partial}{\\partial f} \\left( \\exp(f) - y f \\right) = \\exp(f) - y\n$$\n将此代入伪残差的定义，我们得到其显式解析形式：\n$$\nr_i = - (\\exp(f_i) - y_i) = y_i - \\exp(f_i)\n$$\n由于 $\\exp(f_i)$ 是模型对观测 $i$ 的当前预测平均计数 $\\hat{\\lambda}_i$，因此带对数连接的泊松损失的伪残差就是观测计数减去预测的平均计数：$r_i = y_i - \\hat{\\lambda}_i$。这就是标准的原始残差。\n\n**第2部分：拟合回归树**\n\n在GBM算法的每次迭代中，目标是通过向当前模型（我们称之为 $f_m(\\mathbf{x})$）添加一个新函数（通常是回归树 $h(\\mathbf{x})$）来改进它。新模型是 $f_{m+1}(\\mathbf{x}) = f_m(\\mathbf{x}) + h(\\mathbf{x})$。选择函数 $h(\\mathbf{x})$ 是为了最小化新模型的经验风险：\n$$\n\\min_{h} \\sum_{i=1}^{N} \\ell(y_i, f_m(\\mathbf{x}_i) + h(\\mathbf{x}_i))\n$$\n这是一个在高维函数空间中难以解决的优化问题。梯度提升通过沿着损失函数的负梯度方向迈出一步来简化这个问题。在当前预测 $\\{f_m(\\mathbf{x}_i)\\}_{i=1}^N$ 处评估的负梯度向量，恰好就是伪残差向量 $\\{r_i\\}_{i=1}^N = \\{y_i - \\exp(f_m(\\mathbf{x}_i))\\}_{i=1}^N$。\n\n核心思想是找到一个能最好地逼近这些伪残差的回归树 $h(\\mathbf{x})$。这通过将该树拟合到训练数据 $\\{(\\mathbf{x}_i, r_i)\\}_{i=1}^N$ 来实现。树的构建是通过递归地将特征空间 $\\mathbf{x}$ 划分成一组不相交的终端区域（叶节点）$\\{\\mathcal{R}_j\\}_{j=1}^{J}$，以最小化不纯度度量。对于回归问题，这通常是伪残差 $r_i$ 与分配给包含 $\\mathbf{x}_i$ 的区域的值之间的平方误差和。得到的树 $h(\\mathbf{x})$ 提供了对伪残差的分段常数逼近。我们不是直接加上树的预测值，而是执行单独的线搜索，为每个终端区域 $\\mathcal{R}_j$ 找到最优的常数更新值 $\\gamma_j$，如下一部分所示。\n\n**第3部分：最优终端节点值的计算**\n\n我们给定一个回归树的单个终端区域 $\\mathcal{R}$。该区域包含四名患者，其观测计数 $y_i$ 和当前对数均值预测 $f(\\mathbf{x}_i)$ 如下：\n- 患者 1：$y_1=4$, $f(\\mathbf{x}_1)=1.2$\n- 患者 2：$y_2=0$, $f(\\mathbf{x}_2)=-0.5$\n- 患者 3：$y_3=1$, $f(\\mathbf{x}_3)=0.0$\n- 患者 4：$y_4=3$, $f(\\mathbf{x}_4)=0.7$\n\n对于所有特征向量 $\\mathbf{x}_i$ 落入该区域 $\\mathcal{R}$ 的患者 $i$，我们通过加上一个常数偏移量 $\\gamma$ 来更新模型。新的预测值为 $f(\\mathbf{x}_i) + \\gamma$。最优值 $\\gamma_{\\mathcal{R}}^{\\star}$ 是使该区域内的总泊松损失最小化的值：\n$$\nL_{\\mathcal{R}}(\\gamma) = \\sum_{i \\in \\mathcal{R}} \\ell(y_i, f(\\mathbf{x}_i) + \\gamma) = \\sum_{i \\in \\mathcal{R}} \\left[ \\exp(f(\\mathbf{x}_i) + \\gamma) - y_i (f(\\mathbf{x}_i) + \\gamma) \\right]\n$$\n为了找到最小值，我们求 $L_{\\mathcal{R}}(\\gamma)$ 关于 $\\gamma$ 的一阶导数，并令其为零。\n$$\n\\frac{dL_{\\mathcal{R}}}{d\\gamma} = \\frac{d}{d\\gamma} \\sum_{i \\in \\mathcal{R}} \\left[ \\exp(f(\\mathbf{x}_i))\\exp(\\gamma) - y_i f(\\mathbf{x}_i) - y_i \\gamma \\right]\n$$\n$$\n\\frac{dL_{\\mathcal{R}}}{d\\gamma} = \\sum_{i \\in \\mathcal{R}} \\left[ \\exp(f(\\mathbf{x}_i))\\exp(\\gamma) - y_i \\right]\n$$\n令导数为零：\n$$\n\\sum_{i \\in \\mathcal{R}} \\left[ \\exp(f(\\mathbf{x}_i))\\exp(\\gamma) - y_i \\right] = 0\n$$\n$$\n\\exp(\\gamma) \\sum_{i \\in \\mathcal{R}} \\exp(f(\\mathbf{x}_i)) - \\sum_{i \\in \\mathcal{R}} y_i = 0\n$$\n解出 $\\exp(\\gamma)$：\n$$\n\\exp(\\gamma) = \\frac{\\sum_{i \\in \\mathcal{R}} y_i}{\\sum_{i \\in \\mathcal{R}} \\exp(f(\\mathbf{x}_i))}\n$$\n因此，最优更新值 $\\gamma_{\\mathcal{R}}^{\\star}$ 为：\n$$\n\\gamma_{\\mathcal{R}}^{\\star} = \\ln \\left( \\frac{\\sum_{i \\in \\mathcal{R}} y_i}{\\sum_{i \\in \\mathcal{R}} \\exp(f(\\mathbf{x}_i))} \\right)\n$$\n现在，我们代入给定的数值。\n分子中观测计数的总和是：\n$$\n\\sum_{i \\in \\mathcal{R}} y_i = 4 + 0 + 1 + 3 = 8\n$$\n分母中预测均值的总和是：\n$$\n\\sum_{i \\in \\mathcal{R}} \\exp(f(\\mathbf{x}_i)) = \\exp(1.2) + \\exp(-0.5) + \\exp(0.0) + \\exp(0.7)\n$$\n使用计算器计算指数值：\n$$\n\\exp(1.2) \\approx 3.320117\n$$\n$$\n\\exp(-0.5) \\approx 0.606531\n$$\n$$\n\\exp(0.0) = 1\n$$\n$$\n\\exp(0.7) \\approx 2.013753\n$$\n总和为：\n$$\n\\sum_{i \\in \\mathcal{R}} \\exp(f(\\mathbf{x}_i)) \\approx 3.320117 + 0.606531 + 1 + 2.013753 = 6.940401\n$$\n最后，我们计算 $\\gamma_{\\mathcal{R}}^{\\star}$：\n$$\n\\gamma_{\\mathcal{R}}^{\\star} = \\ln \\left( \\frac{8}{6.940401} \\right) \\approx \\ln(1.152671) \\approx 0.142103\n$$\n将结果四舍五入到四位有效数字，得到 $0.1421$。", "answer": "$$\\boxed{0.1421}$$", "id": "4544511"}, {"introduction": "为了从“梯度提升”的概念飞跃到像XGBoost这样高效的现代算法，理解其背后的优化原理至关重要。本练习将带您深入XGBoost的核心，通过二阶泰勒展开来推导其正则化目标函数和最优叶节点权重的解析解。这项实践揭示了利用损失函数的一阶和二阶导数（即梯度和曲率）如何实现更精确、更稳健的模型更新，这是XGBoost卓越性能的关键所在 [@problem_id:4544555]。", "problem": "一个转化生物信息学研究团队正在构建一个集成模型，以利用异构临床和多组学特征来预测住院败血症的死亡率。假设有 $N$ 个患者，其特征为 $x_i \\in \\mathbb{R}^d$，标签为 $y_i \\in \\{0,1\\}$。考虑分阶段加性建模，其中在第 $t$ 轮提升中的预测为 $ \\hat{y}_i^{(t)} = \\sum_{k=1}^{t} f_k(x_i)$，其中 $f_k$ 是一个回归树。假设存在一个二阶可微的凸样本损失函数 $\\ell(y_i, \\hat{y}_i)$，以及对每棵树的复杂度惩罚，该惩罚随叶子节点数量和叶子权重平方大小的增加而增加。$t$ 轮的正则化经验风险是经验风险与 $t$ 棵树的累积复杂度之和。\n\n对于第 $t$ 轮固定的树结构，其拥有 $T_t$ 个叶子节点和对应的叶子权重 $w_{tj}$（$j \\in \\{1,\\dots, T_t\\}$），令 $q_t(x)$ 将输入 $x$ 映射到其叶子节点索引，并为叶子节点 $j$ 定义索引集 $I_j = \\{ i \\in \\{1,\\dots,N\\} : q_t(x_i) = j \\}$。将损失函数关于当前预测的一阶和二阶导数定义为 $g_i = \\left.\\frac{\\partial \\ell(y_i, \\hat{y}_i)}{\\partial \\hat{y}_i}\\right|_{\\hat{y}_i=\\hat{y}_i^{(t-1)}}$ 和 $h_i = \\left.\\frac{\\partial^2 \\ell(y_i, \\hat{y}_i)}{\\partial \\hat{y}_i^2}\\right|_{\\hat{y}_i=\\hat{y}_i^{(t-1)}}$，其中 $t-1$ 表示添加第 $t$ 棵树之前的状态。设树的复杂度惩罚由两个非负超参数 $\\gamma$ 和 $\\lambda$ 给出，它们分别惩罚叶子节点的数量和叶子权重的平方和。\n\n从分阶段加性模型、经验风险最小化和二阶可微凸损失展开的基本定义出发，执行以下操作：\n\n- 使用与叶子节点数量和叶子权重平方和成比例的、按树相加的复杂度惩罚，来形式化 $t$ 棵树的正则化经验风险。\n- 推导第 $t$ 轮目标函数的近似值，该近似值仅依赖于 $\\{w_{tj}\\}_{j=1}^{T_t}$、固定分区 $\\{I_j\\}_{j=1}^{T_t}$ 以及在 $\\hat{y}_i^{(t-1)}$ 处求值的导数 $\\{g_i, h_i\\}_{i=1}^{N}$。\n- 证明对于固定的树结构，近似目标函数可以分解为关于叶子节点的总和，并识别出每个叶子节点的项。\n- 针对单个叶子权重 $w_{tj}$ 最小化分解后的目标函数，并根据累积梯度 $G_j = \\sum_{i \\in I_j} g_i$、累积曲率 $H_j = \\sum_{i \\in I_j} h_i$ 和超参数 $\\lambda$ 提供最优权重的闭式表达式。\n\n将最终答案表示为给定叶子节点 $j$ 的最优叶子权重的单个闭式表达式。无需进行数值计算或四舍五入。", "solution": "问题陈述被认为是有效的，因为它科学地基于机器学习原理，特别是梯度提升模型。该问题定义明确、客观，并包含推导所要求表达式的所有必要信息。推导过程如下：首先形式化目标函数，然后使用泰勒级数展开对其进行近似，最后针对模型参数最小化所得的表达式。\n\n设第 $t$ 轮提升中第 $i$ 个样本的预测为 $\\hat{y}_i^{(t)}$。该模型是分阶段加性的，因此预测的构建方式如下：\n$$ \\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + f_t(x_i) $$\n其中 $\\hat{y}_i^{(t-1)} = \\sum_{k=1}^{t-1} f_k(x_i)$ 是前 $t-1$ 轮的预测，而 $f_t$ 是在第 $t$ 轮添加的回归树。\n\n问题要求形式化 $t$ 棵树的正则化经验风险。这是经验损失和正则化项的总和。正则化项是集成模型中所有树的累积复杂度。对于具有 $T_k$ 个叶子节点和叶子权重 $\\{w_{kj}\\}_{j=1}^{T_k}$ 的单棵树 $f_k$，其复杂度惩罚与叶子节点数量以及叶子权重平方和成正比。我们可以将其形式化为 $\\Omega(f_k) = \\gamma T_k + \\frac{1}{2}\\lambda\\sum_{j=1}^{T_k} w_{kj}^2$，其中 $\\gamma$ 和 $\\lambda$ 是非负超参数。因子 $\\frac{1}{2}$ 是一个惯例，可以简化求导。\n\n在第 $t$ 轮，我们表示为 $\\mathcal{L}^{(t)}$ 的总体正则化目标函数是：\n$$ \\mathcal{L}^{(t)} = \\sum_{i=1}^{N} \\ell(y_i, \\hat{y}_i^{(t)}) + \\sum_{k=1}^{t} \\Omega(f_k) $$\n代入模型的加性性质 $\\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + f_t(x_i)$，我们得到：\n$$ \\mathcal{L}^{(t)} = \\sum_{i=1}^{N} \\ell(y_i, \\hat{y}_i^{(t-1)} + f_t(x_i)) + \\sum_{k=1}^{t} \\Omega(f_k) $$\n在第 $t$ 轮，我们的目标是找到使该目标函数最小化的树 $f_t$。来自前几轮的预测 $\\hat{y}_i^{(t-1)}$ 和先前树的复杂度 $\\sum_{k=1}^{t-1} \\Omega(f_k)$ 都是固定的。因此，对于 $f_t$ 来说，它们是常数。我们可以通过去掉这些常数项来定义在第 $t$ 轮要最小化的目标，记为 $\\text{Obj}^{(t)}$：\n$$ \\text{Obj}^{(t)} = \\sum_{i=1}^{N} \\ell(y_i, \\hat{y}_i^{(t-1)} + f_t(x_i)) + \\Omega(f_t) $$\n\n下一步是近似这个目标函数。由于损失函数 $\\ell$ 是二阶可微的，我们可以使用 $\\ell(y_i, \\hat{y}_i^{(t-1)} + f_t(x_i))$ 在点 $\\hat{y}_i^{(t-1)}$ 附近的二阶泰勒展开：\n$$ \\ell(y_i, \\hat{y}_i^{(t-1)} + f_t(x_i)) \\approx \\ell(y_i, \\hat{y}_i^{(t-1)}) + \\left[\\frac{\\partial \\ell(y_i, \\hat{y})}{\\partial \\hat{y}}\\right]_{\\hat{y}=\\hat{y}_i^{(t-1)}} f_t(x_i) + \\frac{1}{2} \\left[\\frac{\\partial^2 \\ell(y_i, \\hat{y})}{\\partial \\hat{y}^2}\\right]_{\\hat{y}=\\hat{y}_i^{(t-1)}} f_t(x_i)^2 $$\n使用提供的一阶和二阶导数定义 $g_i = \\left.\\frac{\\partial \\ell(y_i, \\hat{y}_i)}{\\partial \\hat{y}_i}\\right|_{\\hat{y}_i=\\hat{y}_i^{(t-1)}}$ 和 $h_i = \\left.\\frac{\\partial^2 \\ell(y_i, \\hat{y}_i)}{\\partial \\hat{y}_i^2}\\right|_{\\hat{y}_i=\\hat{y}_i^{(t-1)}}$，展开式可简化为：\n$$ \\ell(y_i, \\hat{y}_i^{(t-1)} + f_t(x_i)) \\approx \\ell(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \\frac{1}{2} h_i f_t(x_i)^2 $$\n将此近似代入 $\\text{Obj}^{(t)}$：\n$$ \\text{Obj}^{(t)} \\approx \\sum_{i=1}^{N} \\left[ \\ell(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \\frac{1}{2} h_i f_t(x_i)^2 \\right] + \\Omega(f_t) $$\n项 $\\sum_{i=1}^{N} \\ell(y_i, \\hat{y}_i^{(t-1)})$ 对于 $f_t$ 是一个常数，可以从最小化目标中移除。令近似目标为 $\\tilde{\\text{Obj}}^{(t)}$：\n$$ \\tilde{\\text{Obj}}^{(t)} = \\sum_{i=1}^{N} \\left[ g_i f_t(x_i) + \\frac{1}{2} h_i f_t(x_i)^2 \\right] + \\Omega(f_t) $$\n现在，我们引入回归树 $f_t$ 的结构。该树将输入空间划分为 $T_t$ 个不相交的区域（叶子节点），并为每个叶子节点 $j \\in \\{1, \\dots, T_t\\}$ 分配一个恒定的权重 $w_{tj}$。因此，函数 $f_t(x)$ 由权重和从输入 $x$ 到叶子索引的映射 $q_t(x)$ 定义：$f_t(x) = w_{t,q_t(x)}$。复杂度惩罚为 $\\Omega(f_t) = \\gamma T_t + \\frac{1}{2}\\lambda\\sum_{j=1}^{T_t} w_{tj}^2$。\n将这些代入 $\\tilde{\\text{Obj}}^{(t)}$：\n$$ \\tilde{\\text{Obj}}^{(t)} = \\sum_{i=1}^{N} \\left[ g_i w_{t,q_t(x_i)} + \\frac{1}{2} h_i w_{t,q_t(x_i)}^2 \\right] + \\gamma T_t + \\frac{1}{2}\\lambda\\sum_{j=1}^{T_t} w_{tj}^2 $$\n为了证明这个目标函数可以按叶子节点分解，我们根据样本 $i \\in \\{1, \\dots, N\\}$ 所属的叶子节点来重新组合求和。令 $I_j = \\{ i \\in \\{1,\\dots,N\\} : q_t(x_i) = j \\}$ 为叶子节点 $j$ 中样本的索引集。求和可以重写为：\n$$ \\tilde{\\text{Obj}}^{(t)} = \\sum_{j=1}^{T_t} \\sum_{i \\in I_j} \\left[ g_i w_{tj} + \\frac{1}{2} h_i w_{tj}^2 \\right] + \\gamma T_t + \\frac{1}{2}\\lambda\\sum_{j=1}^{T_t} w_{tj}^2 $$\n由于对于所有样本 $i \\in I_j$，$w_{tj}$ 是常数，我们可以将其从内层求和中提出：\n$$ \\tilde{\\text{Obj}}^{(t)} = \\sum_{j=1}^{T_t} \\left[ \\left(\\sum_{i \\in I_j} g_i\\right) w_{tj} + \\frac{1}{2} \\left(\\sum_{i \\in I_j} h_i\\right) w_{tj}^2 \\right] + \\gamma T_t + \\frac{1}{2}\\lambda\\sum_{j=1}^{T_t} w_{tj}^2 $$\n现在我们使用提供的累积梯度 $G_j = \\sum_{i \\in I_j} g_i$ 和累积曲率 $H_j = \\sum_{i \\in I_j} h_i$ 的定义：\n$$ \\tilde{\\text{Obj}}^{(t)} = \\sum_{j=1}^{T_t} \\left[ G_j w_{tj} + \\frac{1}{2} H_j w_{tj}^2 \\right] + \\gamma T_t + \\frac{1}{2}\\lambda\\sum_{j=1}^{T_t} w_{tj}^2 $$\n合并叶子节点 $j$ 的求和项下的各项：\n$$ \\tilde{\\text{Obj}}^{(t)} = \\sum_{j=1}^{T_t} \\left[ G_j w_{tj} + \\frac{1}{2} (H_j + \\lambda) w_{tj}^2 \\right] + \\gamma T_t $$\n此表达式展示了目标函数的分解。对于固定的树结构（即固定的分区 $\\{I_j\\}$ 和固定的叶子数 $T_t$），项 $\\gamma T_t$ 是一个常数。叶子权重的优化问题分解为 $T_t$ 个独立的最小化问题，每个叶子节点一个。叶子节点 $j$ 的单叶目标为：\n$$ \\tilde{\\text{Obj}}_j(w_{tj}) = G_j w_{tj} + \\frac{1}{2} (H_j + \\lambda) w_{tj}^2 $$\n为了找到最小化这个二次函数的最优权重 $w_{tj}^*$，我们对 $w_{tj}$ 求导并令其为零。\n$$ \\frac{\\partial \\tilde{\\text{Obj}}_j(w_{tj})}{\\partial w_{tj}} = G_j + (H_j + \\lambda) w_{tj} $$\n将导数设为 $0$：\n$$ G_j + (H_j + \\lambda) w_{tj} = 0 $$\n求解 $w_{tj}$ 得到叶子节点 $j$ 的最优权重：\n$$ w_{tj}^* = - \\frac{G_j}{H_j + \\lambda} $$\n损失函数 $\\ell$ 的凸性确保了 $h_i \\ge 0$，这意味着 $H_j = \\sum_{i \\in I_j} h_i \\ge 0$。超参数 $\\lambda$ 是非负的。通常使用 $\\lambda  0$，这保证了 $H_j + \\lambda  0$，从而确保目标函数关于 $w_{tj}$ 是严格凸的，并且存在唯一的最小值。", "answer": "$$\n\\boxed{-\\frac{G_j}{H_j + \\lambda}}\n$$", "id": "4544555"}]}