{"hands_on_practices": [{"introduction": "要构建高效的图神经网络，我们必须首先理解其基础的运作机制。本练习将深入图卷积网络 (GCN) 的核心，探究归一化图拉普拉斯算子 $L$ 的作用。通过为一个简单的分子图计算 $L$ 矩阵，您将具体地理解 GCN 如何利用此算子来处理图拓扑结构并保持数值稳定性，这对于分析异构的生物网络至关重要。[@problem_id:4570165]", "problem": "考虑一种分子图表示法，其中原子是顶点，共价键是无向边。在谱图理论中，定义归一化拉普拉斯矩阵为 $L = I - D^{-1/2} A D^{-1/2}$，其中 $A$ 是邻接矩阵，$D$ 是度矩阵，$I$ 是单位矩阵。在图神经网络（GNN）中，特别是在图卷积网络（GCN）中，图滤波使用由 $L$ 构建的算子作用于图信号，以实现尊重相互作用网络拓扑的局部平滑。从谱域中的图卷积对应于在 $L$ 的特征值上乘以一个谱滤波器，以及 $L$ 的多项式实现局部空间滤波器这一基本出发点，解释为什么在生物分子相互作用网络中，对称归一化 $L = I - D^{-1/2} A D^{-1/2}$ 被用来稳定不同度的节点间的滤波。\n\n现在考虑水分子的无权无向图，其中顶点 $2$ 是氧原子，顶点 $1$ 和 $3$ 是氢原子，化学键为 $(1,2)$ 和 $(2,3)$。构建邻接矩阵 $A$、度矩阵 $D$，并计算归一化拉普拉斯矩阵 $L$。最后，计算该图的 $L$ 的第二小特征值（代数连通度）$\\lambda_{2}$。将 $\\lambda_{2}$ 作为你的最终答案报告。无需四舍五入，该量是无量纲的。", "solution": "该问题经评估有效。它在科学上基于谱图理论和图神经网络（GNN）的原理，问题设定良好，具有唯一定义的计算任务，且语言客观。问题为解释和计算部分提供了所有必要信息。\n\n问题包括两部分。第一部分要求解释在 GNN 中使用对称归一化拉普拉斯矩阵的原因。第二部分要求计算水分子图的归一化拉普拉斯矩阵及其第二小特征值。\n\n\\subsection*{第一部分：对称归一化的解释}\n\n在图信号处理和GNN中，图卷积操作可以被解释为一个滤波器，它平滑定义在图顶点上的信号。图信号是一个向量 $x \\in \\mathbb{R}^N$，其中 $x_i$ 是顶点 $i$ 上的信号值（例如，一个特征向量）。最简单的滤波操作涉及聚合来自邻居节点的信息。这可以表示为信号 $x$ 与图的邻接矩阵 $A$ 的乘法。新的信号 $y$ 由 $y = Ax$ 给出，其中 $y_i = \\sum_{j \\in \\mathcal{N}(i)} x_j$。\n\n在具有异构度分布的网络（如生物分子相互作用网络）中使用原始邻接矩阵 $A$ 会出现一个重要问题。度数高的节点（中心节点）的 $y_i$ 值会比度数低的节点大得多，这仅仅是因为它们对更多的邻居进行了求和。这可能导致特征向量的尺度高度依赖于节点度，从而在深度神经网络的训练过程中引起数值不稳定和梯度爆炸。\n\n为了解决这个问题，引入了归一化。一个常见的初始方法是行归一化，使用算子 $D^{-1}A$，其中 $D$ 是度矩阵。滤波操作变为 $y = D^{-1}Ax$，计算出的条目为 $y_i = \\frac{1}{\\deg(i)} \\sum_{j \\in \\mathcal{N}(i)} x_j$。此操作将求和替换为邻居节点特征的平均值。虽然这解决了接收节点 $i$ 的特征尺度爆炸问题，但它引入了另一种偏差：它没有考虑发送节点 $j$ 的度。来自中心节点的消息与来自度为 $1$ 的节点的消息被同等对待。\n\n对称归一化，即归一化拉普拉斯矩阵 $L = I - D^{-1/2} A D^{-1/2}$ 的基础，解决了这个局限性。在图卷积网络（GCN）传播规则中使用的算子实际上是 $\\tilde{A} = D^{-1/2} A D^{-1/2}$（通常会添加自环，但我们在这里考虑其基本形式）。滤波操作 $y = \\tilde{A}x$ 的元素形式如下：\n$$y_i = \\sum_{j=1}^N (\\tilde{A})_{ij} x_j = \\sum_{j=1}^N \\frac{A_{ij}}{\\sqrt{\\deg(i)\\deg(j)}} x_j = \\frac{1}{\\sqrt{\\deg(i)}} \\sum_{j \\in \\mathcal{N}(i)} \\frac{x_j}{\\sqrt{\\deg(j)}}$$\n这种形式通过源节点和目标节点的度来归一化信息流。它防止高度数节点在聚合过程中占主导地位，从而产生更稳定和平衡的更新规则。\n\n从谱域的角度来看，使用归一化拉普拉斯矩阵 $L$至关重要。$L$ 的特征值保证在 $[0, 2]$ 范围内。这个特性对于深度 GNN 的稳定性至关重要。一个 GCN 层对图信号应用一个滤波器。堆叠多个层对应于应用图滤波算子的多项式。如果算子的特征值大小超过 $1$，重复应用可能导致信号（和梯度）爆炸或消失。通过确保 $L$ 的特征值被限制在一个小的、稳定的范围内，对称归一化允许构建行为良好的多项式滤波器，从而能够创建深度且有效的 GNN 模型。总而言之，使用对称归一化 $D^{-1/2} A D^{-1/2}$ 是因为它 (1) 在空间上平衡了不同度数节点之间的影响，以及 (2) 在谱域上确保了深度网络中图滤波操作的稳定性。\n\n\\subsection*{第二部分：水分子的计算}\n\n水分子 H-O-H 表示为一个无权无向图，其顶点为 $V = \\{1, 2, 3\\}$，其中顶点 $2$ 是氧 (O)，顶点 $1$ 和 $3$ 是氢 (H)。化学键作为边给出 $E = \\{(1, 2), (2, 3)\\}$。\n\n首先，我们构建邻接矩阵 $A$。这是一个 $3 \\times 3$ 的对称矩阵，如果顶点 $i$ 和 $j$ 之间存在边，则 $A_{ij} = 1$，否则 $A_{ij} = 0$。\n$$A = \\begin{pmatrix} 0  1  0 \\\\ 1  0  1 \\\\ 0  1  0 \\end{pmatrix}$$\n\n接下来，我们构建度矩阵 $D$。一个顶点的度是连接到它的边的数量。\n$\\deg(1) = 1$\n$\\deg(2) = 2$\n$\\deg(3) = 1$\n度矩阵 $D$ 的对角线上是这些度，其他地方为零。\n$$D = \\begin{pmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  1 \\end{pmatrix}$$\n\n然后我们计算矩阵 $D^{-1/2}$，它是通过对 $D$ 的每个对角元素取逆平方根得到的。\n$$D^{-1/2} = \\begin{pmatrix} 1^{-1/2}  0  0 \\\\ 0  2^{-1/2}  0 \\\\ 0  0  1^{-1/2} \\end{pmatrix} = \\begin{pmatrix} 1  0  0 \\\\ 0  \\frac{1}{\\sqrt{2}}  0 \\\\ 0  0  1 \\end{pmatrix}$$\n\n现在，我们计算对称归一化的邻接矩阵 $\\tilde{A} = D^{-1/2} A D^{-1/2}$。\n$$\\tilde{A} = \\begin{pmatrix} 1  0  0 \\\\ 0  \\frac{1}{\\sqrt{2}}  0 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} 0  1  0 \\\\ 1  0  1 \\\\ 0  1  0 \\end{pmatrix} \\begin{pmatrix} 1  0  0 \\\\ 0  \\frac{1}{\\sqrt{2}}  0 \\\\ 0  0  1 \\end{pmatrix}$$\n$$\\tilde{A} = \\begin{pmatrix} 0  1  0 \\\\ \\frac{1}{\\sqrt{2}}  0  \\frac{1}{\\sqrt{2}} \\\\ 0  1  0 \\end{pmatrix} \\begin{pmatrix} 1  0  0 \\\\ 0  \\frac{1}{\\sqrt{2}}  0 \\\\ 0  0  1 \\end{pmatrix} = \\begin{pmatrix} 0  \\frac{1}{\\sqrt{2}}  0 \\\\ \\frac{1}{\\sqrt{2}}  0  \\frac{1}{\\sqrt{2}} \\\\ 0  \\frac{1}{\\sqrt{2}}  0 \\end{pmatrix}$$\n\n归一化拉普拉斯矩阵 $L$ 定义为 $L = I - \\tilde{A}$。\n$$L = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix} - \\begin{pmatrix} 0  \\frac{1}{\\sqrt{2}}  0 \\\\ \\frac{1}{\\sqrt{2}}  0  \\frac{1}{\\sqrt{2}} \\\\ 0  \\frac{1}{\\sqrt{2}}  0 \\end{pmatrix} = \\begin{pmatrix} 1  -\\frac{1}{\\sqrt{2}}  0 \\\\ -\\frac{1}{\\sqrt{2}}  1  -\\frac{1}{\\sqrt{2}} \\\\ 0  -\\frac{1}{\\sqrt{2}}  1 \\end{pmatrix}$$\n\n最后，我们通过求解特征方程 $\\det(L - \\lambda I) = 0$ 来计算 $L$ 的特征值。\n$$\\det \\begin{pmatrix} 1-\\lambda  -\\frac{1}{\\sqrt{2}}  0 \\\\ -\\frac{1}{\\sqrt{2}}  1-\\lambda  -\\frac{1}{\\sqrt{2}} \\\\ 0  -\\frac{1}{\\sqrt{2}}  1-\\lambda \\end{pmatrix} = 0$$\n沿第一行展开行列式：\n$$(1-\\lambda) \\left| \\begin{matrix} 1-\\lambda  -\\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}}  1-\\lambda \\end{matrix} \\right| - \\left(-\\frac{1}{\\sqrt{2}}\\right) \\left| \\begin{matrix} -\\frac{1}{\\sqrt{2}}  -\\frac{1}{\\sqrt{2}} \\\\ 0  1-\\lambda \\end{matrix} \\right| = 0$$\n$$(1-\\lambda) \\left[ (1-\\lambda)^2 - \\left(-\\frac{1}{\\sqrt{2}}\\right)^2 \\right] + \\frac{1}{\\sqrt{2}} \\left[ \\left(-\\frac{1}{\\sqrt{2}}\\right)(1-\\lambda) - 0 \\right] = 0$$\n$$(1-\\lambda) \\left[ (1-\\lambda)^2 - \\frac{1}{2} \\right] - \\frac{1}{2}(1-\\lambda) = 0$$\n我们可以提出公因式 $(1-\\lambda)$：\n$$(1-\\lambda) \\left[ (1-\\lambda)^2 - \\frac{1}{2} - \\frac{1}{2} \\right] = 0$$\n$$(1-\\lambda) \\left[ (1-\\lambda)^2 - 1 \\right] = 0$$\n该方程产生 $\\lambda$ 的三个解：\n1. $1-\\lambda = 0 \\implies \\lambda = 1$\n2. $(1-\\lambda)^2 - 1 = 0 \\implies (1-\\lambda)^2 = 1 \\implies 1-\\lambda = \\pm 1$\n   - $1-\\lambda = 1 \\implies \\lambda = 0$\n   - $1-\\lambda = -1 \\implies \\lambda = 2$\n\n$L$ 的特征值按非递减顺序排序为 $\\lambda_1 = 0$, $\\lambda_2 = 1$, 和 $\\lambda_3 = 2$。最小的特征值是 $\\lambda_1 = 0$，这对于连通图是预期的。第二小的特征值（代数连通度）是 $\\lambda_2$。\n\n因此，$\\lambda_2 = 1$。", "answer": "$$\\boxed{1}$$", "id": "4570165"}, {"introduction": "强大的模型架构只是成功的一半；有效的训练，尤其是在处理真实世界数据时，同样至关重要。药物-靶点相互作用数据集以其严重的数据不平衡性而著称，本练习将通过探索高级损失函数来直面这一挑战。您将推导并比较标准的二元交叉熵损失与类别加权和焦点损失 (focal loss) 等变体，从而学会如何设计一个能够迫使模型从稀有但至关重要的正例中学习的训练目标。[@problem_id:4570205]", "problem": "考虑一个药物-靶点相互作用（DTI）的二分网络，其中图神经网络（GNN）为每个候选的药物-靶点对（边）生成一个实值 logit $z \\in \\mathbb{R}$，该 logit 通过 logistic sigmoid 函数 $p = \\sigma(z) = \\frac{1}{1 + \\exp(-z)}$ 映射到一个伯努利参数 $p$。假设对于一小批（mini-batch）边，在给定 $p$ 的条件下，观测到的二元边标签是条件独立的。该问题的基础是二元结果的伯努利似然、最大似然估计（MLE）原理以及初等微积分中的链式法则。\n\n从伯努利似然 $p(y \\mid p) = p^{y} (1-p)^{1-y}$ 出发，并应用最大似然估计原理和负对数似然构造，完成以下任务：\n\n1. 推导适用于 DTI 链接预测的逐边二元交叉熵（BCE）损失 $L_{\\mathrm{BCE}}(y, p)$，并求出其关于 logit $z$ 的梯度 $\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z}$。\n\n2. 推导一个类别加权的 BCE 损失 $L_{\\mathrm{wBCE}}(y, p)$，该损失使用不同的正类权重 $w_{1} > 0$ 和负类权重 $w_{0} > 0$ 来处理类别不平衡问题。求出其梯度 $\\frac{\\partial L_{\\mathrm{wBCE}}}{\\partial z}$。\n\n3. 推导一个二元 focal loss（焦点损失） $L_{\\mathrm{focal}}(y, p)$，其中包含聚焦参数 $\\gamma \\ge 0$ 和平衡参数 $\\alpha \\in (0,1)$，适用于 DTI 的类别不平衡问题。从伯努利似然出发，并解释调制因子在衰减简单样本中的作用。求出其梯度 $\\frac{\\partial L_{\\mathrm{focal}}}{\\partial z}$。\n\n4. 对于一个包含 2 条边的小批量，其标签和 logit 分别为 $(y_{p}, z_{p}) = (1, 1.2)$ 和 $(y_{n}, z_{n}) = (0, -0.7)$，使用 focal loss，其中参数 $\\gamma = 2$ 和 $\\alpha = 0.25$。计算比率\n$$\nR \\;=\\; \\frac{\\left| \\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{p}} \\right|}{\\left| \\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{n}} \\right|}.\n$$\n将 R 的最终数值四舍五入至五位有效数字。将最终答案表示为一个无单位的标量。", "solution": "问题陈述已经过验证，被认为是合理、适定且在统计机器学习原理方面具有科学依据的。我们可以进行推导和计算。\n\n连接 logit $z$ 和伯努利概率 $p$ 的核心关系是 logistic sigmoid 函数， $p = \\sigma(z) = \\frac{1}{1 + \\exp(-z)}$。这个函数的一个关键性质是它关于 $z$ 的导数，这个性质将被反复使用：\n$$\n\\frac{dp}{dz} = \\frac{d}{dz} (1 + \\exp(-z))^{-1} = -1 \\cdot (1 + \\exp(-z))^{-2} \\cdot (-\\exp(-z)) = \\frac{\\exp(-z)}{(1 + \\exp(-z))^{2}}\n$$\n这可以很简洁地用 $p$ 本身来表示：\n$$\n\\frac{dp}{dz} = \\frac{1}{1 + \\exp(-z)} \\cdot \\frac{\\exp(-z)}{1 + \\exp(-z)} = p \\cdot (1-p)\n$$\n所有后续的梯度计算都将依赖于链式法则，$\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial p} \\frac{dp}{dz}$。\n\n**1. 二元交叉熵（BCE）损失**\n\n最大似然估计（MLE）原理要求我们最大化观测数据的似然。对于单个二元观测值 $y \\in \\{0, 1\\}$，伯努利似然为 $P(y \\mid p) = p^y(1-p)^{1-y}$。在计算上，最大化对数似然更为方便：\n$$\n\\ell(p \\mid y) = \\ln(P(y \\mid p)) = y \\ln(p) + (1-y)\\ln(1-p)\n$$\n在机器学习中，优化通常被构建为最小化一个损失函数。标准的选择是负对数似然（NLL）。因此，逐边的二元交叉熵损失被定义为伯努利分布的负对数似然。\n$$\nL_{\\mathrm{BCE}}(y, p) = -\\ell(p \\mid y) = -[y \\ln(p) + (1-y)\\ln(1-p)]\n$$\n为了求出关于 logit $z$ 的梯度，我们应用链式法则：\n$$\n\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z} = \\frac{\\partial L_{\\mathrm{BCE}}}{\\partial p} \\frac{dp}{dz}\n$$\n首先，我们计算关于 $p$ 的偏导数：\n$$\n\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial p} = - \\left[ y \\frac{1}{p} + (1-y) \\frac{-1}{1-p} \\right] = - \\left[ \\frac{y}{p} - \\frac{1-y}{1-p} \\right] = \\frac{1-y}{1-p} - \\frac{y}{p} = \\frac{p(1-y) - y(1-p)}{p(1-p)} = \\frac{p-y}{p(1-p)}\n$$\n现在，乘以 sigmoid 函数的导数：\n$$\n\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z} = \\left( \\frac{p-y}{p(1-p)} \\right) \\cdot (p(1-p)) = p - y\n$$\n代入 $p=\\sigma(z)$，梯度为 $\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z} = \\sigma(z) - y$。\n\n**2. 类别加权的二元交叉熵（wBCE）损失**\n\n为了解决类别不平衡问题，可以通过为正类（$y=1$）和负类（$y=0$）引入权重来修改标准的 BCE 损失，权重分别记为 $w_1 > 0$ 和 $w_0 > 0$。损失函数变为：\n$$\nL_{\\mathrm{wBCE}}(y, p) = -[w_1 y \\ln(p) + w_0 (1-y)\\ln(1-p)]\n$$\n我们再次使用链式法则来求关于 $z$ 的梯度。关于 $p$ 的偏导数为：\n$$\n\\frac{\\partial L_{\\mathrm{wBCE}}}{\\partial p} = - \\left[ w_1 y \\frac{1}{p} + w_0 (1-y) \\frac{-1}{1-p} \\right] = \\frac{w_0(1-y)}{1-p} - \\frac{w_1 y}{p}\n$$\n乘以 $\\frac{dp}{dz} = p(1-p)$：\n$$\n\\frac{\\partial L_{\\mathrm{wBCE}}}{\\partial z} = \\left( \\frac{w_0(1-y)}{1-p} - \\frac{w_1 y}{p} \\right) \\cdot p(1-p) = w_0(1-y)p - w_1 y(1-p)\n$$\n该梯度可以用 $z$ 表示为 $\\frac{\\partial L_{\\mathrm{wBCE}}}{\\partial z} = w_0(1-y)\\sigma(z) - w_1 y(1-\\sigma(z))$。\n\n**3. 二元 Focal Loss**\n\n二元 focal loss 是对加权 BCE 的一种改进，它通过根据模型的置信度动态缩放交叉熵损失来解决类别不平衡问题。它引入了一个调制因子，该因子降低了来自“简单”样本（即高置信度分类的样本）的损失贡献，从而将训练重点放在“困难”的错分样本上。\n\n令 $p_t$ 为模型对真实类别估计的概率的简写：\n$$\np_t = \\begin{cases} p  \\text{if } y=1 \\\\ 1-p  \\text{if } y=0 \\end{cases}\n$$\n标准的交叉熵损失是 $-\\ln(p_t)$。focal loss 引入了两个组成部分：一个静态的平衡参数 $\\alpha \\in (0,1)$（类似于 $w_0, w_1$）和一个动态的调制因子 $(1-p_t)^{\\gamma}$（其中聚焦参数 $\\gamma \\ge 0$）。该损失定义为：\n$$\nL_{\\mathrm{focal}}(y, p) = -y \\alpha (1-p)^\\gamma \\ln(p) - (1-y)(1-\\alpha) p^\\gamma \\ln(1-p)\n$$\n对于正样本（$y=1$），当 $p \\to 1$（一个简单的正样本）时，项 $(1-p)^\\gamma$ 趋近于 0，从而减小了损失。类似地，对于负样本（$y=0$），当 $p \\to 0$（一个简单的负样本）时，项 $p^\\gamma$ 趋近于 0，同样减小了损失。当 $\\gamma=0$ 时，focal loss 退化为 $\\alpha$ 平衡的交叉熵损失。\n\n为了求梯度 $\\frac{\\partial L_{\\mathrm{focal}}}{\\partial z}$，我们再次使用链式法则，通过对 $p$ 求导然后乘以 $p(1-p)$。我们分别推导 $y=1$ 和 $y=0$ 情况下的梯度。\n\n对于 $y=1$：$L(p) = -\\alpha(1-p)^\\gamma \\ln(p)$。\n$$\n\\frac{\\partial L}{\\partial p} = -\\alpha \\left[ \\frac{d}{dp}((1-p)^\\gamma) \\ln(p) + (1-p)^\\gamma \\frac{d}{dp}(\\ln p) \\right] = -\\alpha \\left[ -\\gamma(1-p)^{\\gamma-1} \\ln(p) + \\frac{(1-p)^\\gamma}{p} \\right]\n$$\n乘以 $\\frac{dp}{dz} = p(1-p)$：\n$$\n\\frac{\\partial L}{\\partial z} = -\\alpha \\left[ -\\gamma(1-p)^{\\gamma-1} \\ln(p) + \\frac{(1-p)^\\gamma}{p} \\right] p(1-p) = -\\alpha \\left[ -\\gamma p(1-p)^\\gamma \\ln(p) + (1-p)^{\\gamma+1} \\right]\n$$\n$$\n\\implies \\frac{\\partial L}{\\partial z} = \\alpha(1-p)^\\gamma (\\gamma p \\ln(p) - (1-p)) = \\alpha(1-p)^\\gamma (\\gamma p \\ln(p) + p-1)\n$$\n对于 $y=0$：$L(p) = -(1-\\alpha)p^\\gamma \\ln(1-p)$。\n$$\n\\frac{\\partial L}{\\partial p} = -(1-\\alpha) \\left[ \\gamma p^{\\gamma-1} \\ln(1-p) + p^\\gamma \\frac{-1}{1-p} \\right]\n$$\n乘以 $\\frac{dp}{dz} = p(1-p)$：\n$$\n\\frac{\\partial L}{\\partial z} = -(1-\\alpha) \\left[ \\gamma p^{\\gamma-1} \\ln(1-p) - \\frac{p^\\gamma}{1-p} \\right] p(1-p) = -(1-\\alpha) \\left[ \\gamma p^\\gamma(1-p) \\ln(1-p) - p^{\\gamma+1} \\right]\n$$\n$$\n\\implies \\frac{\\partial L}{\\partial z} = (1-\\alpha)p^\\gamma (p - \\gamma(1-p)\\ln(1-p))\n$$\n将这些结果合并，得到完整的梯度表达式：\n$$\n\\frac{\\partial L_{\\mathrm{focal}}}{\\partial z} = y\\alpha(1-p)^\\gamma(p-1+\\gamma p\\ln p) + (1-y)(1-\\alpha)p^\\gamma(p-\\gamma(1-p)\\ln(1-p))\n$$\n\n**4. 数值计算**\n\n我们给定一个正样本 $(y_p, z_p) = (1, 1.2)$ 和一个负样本 $(y_n, z_n) = (0, -0.7)$，参数为 $\\gamma=2$ 和 $\\alpha=0.25$。我们需要计算 $R = \\frac{\\left| \\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{p}} \\right|}{\\left| \\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{n}} \\right|}$。\n\n首先，我们计算概率：\n$p_p = \\sigma(z_p) = \\sigma(1.2) = \\frac{1}{1 + \\exp(-1.2)} \\approx 0.76852479$\n$p_n = \\sigma(z_n) = \\sigma(-0.7) = \\frac{1}{1 + \\exp(0.7)} \\approx 0.33181222$\n\n接下来，我们计算正样本（$y_p=1$）的梯度：\n$$\n\\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{p}} = \\alpha(1-p_p)^\\gamma(\\gamma p_p \\ln p_p + p_p - 1)\n$$\n代入数值：\n$1-p_p \\approx 1 - 0.76852479 = 0.23147521$\n$\\ln(p_p) \\approx \\ln(0.76852479) = -0.26328242$\n$$\n\\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{p}} \\approx 0.25 \\cdot (0.23147521)^2 \\cdot (2 \\cdot 0.76852479 \\cdot (-0.26328242) + 0.76852479 - 1)\n$$\n$$\n\\approx 0.25 \\cdot (0.05358087) \\cdot (-0.40467566 - 0.23147521) = 0.01339522 \\cdot (-0.63615087) \\approx -0.00852033\n$$\n所以，$\\left| \\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{p}} \\right| \\approx 0.00852033$。\n\n接下来，我们计算负样本（$y_n=0$）的梯度：\n$$\n\\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{n}} = (1-\\alpha)p_n^\\gamma(p_n - \\gamma(1-p_n)\\ln(1-p_n))\n$$\n代入数值：\n$1-\\alpha = 0.75$\n$1-p_n \\approx 1 - 0.33181222 = 0.66818778$\n$\\ln(1-p_n) \\approx \\ln(0.66818778) = -0.40318043$\n$$\n\\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{n}} \\approx 0.75 \\cdot (0.33181222)^2 \\cdot (0.33181222 - 2 \\cdot (0.66818778) \\cdot (-0.40318043))\n$$\n$$\n\\approx 0.75 \\cdot (0.11009972) \\cdot (0.33181222 + 0.53880447) = 0.08257479 \\cdot (0.87061669) \\approx 0.0718797\n$$\n所以，$\\left| \\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{n}} \\right| \\approx 0.0718797$。\n\n最后，我们计算比率 $R$：\n$$\nR = \\frac{\\left| \\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{p}} \\right|}{\\left| \\frac{\\partial L_{\\mathrm{focal}}}{\\partial z_{n}} \\right|} \\approx \\frac{0.00852033}{0.0718797} \\approx 0.1185361\n$$\n四舍五入到五位有效数字，我们得到 $R \\approx 0.11854$。R 的小值证实了分类良好的正样本（$p_p \\approx 0.77$）的梯度幅值显著小于更模糊的负样本（$p_n \\approx 0.33$）的梯度幅值，这展示了 focal loss 的效果。", "answer": "$$\n\\boxed{0.11854}\n$$", "id": "4570205"}, {"introduction": "任何预测模型的最终考验都是其在未见数据上的表现，但我们如何定义“未见”对于有意义的评估至关重要。本练习旨在探讨不同的数据集划分策略，以评估模型在药物重定位这一特定任务上的真实泛化能力。通过分析基于时间、化学支架等不同的划分方法，您将学会设计能够真实模拟药物发现的前瞻性、并避免陷入性能被过度高估这一常见陷阱的验证方案。[@problem_id:4570155]", "problem": "一个生物信息学团队正在训练一个图神经网络（GNN），用于在药物-蛋白质相互作用网络上进行链接预测，以支持药物重定位。该相互作用网络被建模为一个图 $G=(V,E)$，其中顶点集 $V$ 包含药物分子和蛋白质，边集 $E$ 包含已知的相互作用。每条边 $e\\in E$都有一个相关的发现时间戳 $t(e)$，反映了该相互作用被发现的时间。每个分子表示为一个带标签的分子图 $g\\in\\mathcal{M}$，并通过一个标准的Bemis–Murcko骨架函数 $S:\\mathcal{M}\\rightarrow\\mathcal{S}$ 映射到一个骨架标识符。每个蛋白质 $p\\in\\mathcal{P}$ 由其氨基酸序列表示，蛋白质之间的相关性通过序列一致性函数 $I:\\mathcal{P}\\times\\mathcal{P}\\rightarrow[0,1]$ 进行量化，该函数测量对齐位置中相同部分的比例。为了评估，该团队考虑了三类数据集划分方法：\n\n- 骨架划分（scaffold split）根据骨架成员关系对药物进行划分，使得训练集和测试集具有不相交的骨架集；形式上，存在一个子集 $\\mathcal{S}_{\\mathrm{train}}\\subset\\mathcal{S}$，使得所有训练分子 $g$ 满足 $S(g)\\in\\mathcal{S}_{\\mathrm{train}}$，而所有测试分子 $g'$ 满足 $S(g')\\in\\mathcal{S}\\setminus\\mathcal{S}_{\\mathrm{train}}$。\n- 时间划分（time split）使用一个截止时间 $t_0$ 来强制执行时序性；形式上，$E_{\\mathrm{train}}=\\{e\\in E:\\ t(e)\\le t_0\\}$ 和 $E_{\\mathrm{test}}=\\{e\\in E:\\ t(e)>t_0\\}$，并且没有从未来到训练集的信息泄露。\n- 蛋白质同源性划分（protein homology split）划分蛋白质以控制相对于序列一致性的新颖性；形式上，给定一个阈值 $\\theta\\in(0,1)$，该划分确保对于任何训练蛋白质 $p$ 和任何测试蛋白质 $q$，都有 $I(p,q) < \\theta$。\n\n根据监督学习中泛化能力的第一性原理，如上定义每种划分，并选择最能正确论证哪种划分最适合衡量药物重定位任务中分布外泛化能力的选项。使用以下基本事实：监督学习旨在最小化在数据生成分布 $P_t$ 下的期望损失 $R_t(f)=\\mathbb{E}_{(x,y)\\sim P_t}[\\ell(f(x),y)]$，该分布会随日历时间 $t$ 变化；独立同分布（i.i.d.）的随机划分在固定时间点上近似于 $P_t$，但未考虑未来的漂移；药物重定位评估的是，一个在过去药物知识上训练的模型是否能前瞻性地发现新的适应症或靶点，通常使用的药物是已知的而非化学上新颖的。\n\n哪个选项最能抓住正确的定义以及衡量重定位任务中泛化能力的最合适划分方法？\n\nA. 骨架划分强制执行 $S(g)$ 的不相交性，是衡量药物重定位的最佳方法，因为重定位主要需要外推到新颖的化学骨架；时间和蛋白质同源性划分是次要的，因为它们不限制化学型的新颖性。\n\nB. 时间划分对训练集强制执行 $t(e)\\le t_0$，对测试集强制执行 $t(e)>t_0$，是衡量药物重定位的最佳方法，因为重定位本质上是前瞻性的；它在未来分布 $P_{t>t_0}$ 下评估 $R_{t>t_0}(f)$，避免了信息泄露，并反映了大多数重定位利用的是现有药物而非未见过的骨架，而骨架和蛋白质同源性划分则强调了不同的、与任务不匹配的新颖性形式。\n\nC. 蛋白质同源性划分在训练集和测试集之间强制执行 $I(p,q) < \\theta$，是衡量药物重定位的最佳方法，因为重定位专注于为与先前靶点在进化上不相关的蛋白质发现药物；时间和骨架划分不能保证靶点的新颖性。\n\nD. 对 $E$ 进行随机i.i.d.划分是衡量药物重定位的最佳方法，因为它在 $P_{t_0}$ 分布下以低方差估计 $R_{t_0}(f)$；骨架、时间和蛋白质同源性划分引入了人为的分布漂移，因此会低估真实的泛化能力。", "solution": "该问题要求对一个为药物重定位设计的图神经网络（GNN）模型的不同数据集划分策略进行评估。任务的核心是确定哪种划分最能衡量分布外（OOD）泛化能力，这对于评估模型的真实世界效用至关重要。我将首先建立与此背景相关的泛化原则，然后分析每种提议的划分方法。\n\n首先，让我们将学习问题形式化。监督学习的目标是学习一个函数 $f$，以最小化期望损失（或风险）$R(f) = \\mathbb{E}_{(x,y)\\sim P}[\\ell(f(x),y)]$，其中 $P$ 是真实的数据生成分布。问题明确指出，这个分布 $P_t$ 会随日历时间 $t$ 变化。模型在截至时间 $t_0$ 收集的数据上进行训练，这些数据来自 $P_{t \\le t_0}$。最终目标，特别是在像药物重定位这样的前瞻性应用中，是在未来的、未见过的数据上表现良好，这些数据将来自未来的分布 $P_{t > t_0}$。因此，我们关注的量是未来风险 $R_{t>t_0}(f) = \\mathbb{E}_{(x,y)\\sim P_{t>t_0}}[\\ell(f(x),y)]$。如果一种评估方法的测试集能很好地估计这个未来风险，那么该方法就被认为是适当的。\n\n该任务被指定为药物重定位，问题将其定义为“前瞻性地发现新的适应症或靶点，通常使用的药物是已知的而非化学上新颖的”。这个定义有两个关键组成部分：\n1.  **前瞻性**：该任务本质上是关于对未来的预测。我们利用过去的知识来预测未来会发现什么。\n2.  **现有药物**：重点是为现有的化学实体寻找新用途，而不必发明全新的分子结构。\n\n基于这些原则，让我们分析提到的四种划分类型：三个已定义的类别（骨架、时间、蛋白质同源性）和一种标准的随机划分。\n\n**1. 时间划分：**\n- **定义**：训练数据包含截至截止时间 $t_0$ 发现的所有已知相互作用，即 $E_{\\mathrm{train}}=\\{e\\in E:\\ t(e)\\le t_0\\}$。测试数据包含在该时间之后发现的所有相互作用，即 $E_{\\mathrm{test}}=\\{e\\in E:\\ t(e)>t_0\\}$。\n- **分析**：这种划分直接模拟了真实世界的部署场景和任务的前瞻性。模型在“过去”（$t \\le t_0$）的数据上训练，并在“未来”（$t > t_0$）的数据上评估。这种设置明确评估了模型在科学发现中固有的时间分布漂移下的泛化能力。它正确地测量了对 $P_{t>t_0}$ 的泛化，即目标量 $R_{t>t_0}(f)$。此外，它与重定位的一个关键方面相符：存在于训练集中的药物（在 $t_0$ 之前发现）可以在测试集中形成新的相互作用（在 $t_0$ 之后发现），这代表了一个药物重定位的用例。这是对药物重定位挑战最直接和最现实的模拟。\n\n**2. 骨架划分：**\n- **定义**：根据函数 $S$ 确定的化学骨架对药物集进行划分。训练集和测试集包含具有不相交骨架集的药物。形式上，对于任何训练分子 $g$ 和测试分子 $g'$，$S(g) \\neq S(g')$。\n- **分析**：这种划分评估模型泛化到新颖化学物质（化学型）的能力。它测试了模型在化学结构空间方面的OOD泛化能力。虽然这是一个非常重要且具有挑战性的任务，但它更符合*从头*药物设计的目标，即发明新药。正如问题所述，药物重定位通常涉及“已知的而非化学上新颖的药物”。因此，骨架划分强加了一种泛化挑战，但它并非药物重定位中的主要挑战，这使其成为针对此特定任务的不匹配的评估指标。\n\n**3. 蛋白质同源性划分：**\n- **定义**：对蛋白质集进行划分，使得测试集中的任何蛋白质与训练集中的所有蛋白质都不同，具体通过序列一致性低于某个阈值来量化：对于任何训练蛋白质 $p$ 和测试蛋白质 $q$，$I(p,q) < \\theta$。\n- **分析**：这种划分评估模型泛化到新颖生物靶点的能力，特别是那些与模型训练过的任何靶点在进化上都相距甚远的靶点。这是对泛化到新生物学知识的有力测试。虽然为新颖靶点寻找药物是药物发现的一部分，但重定位并不仅限于这种情况。一个已知的药物可能被重定位用于与训练集中某个蛋白质属于同一家族的蛋白质。更根本的是，这种划分与骨架划分一样，专注于一个特定的泛化轴（生物空间），但忽略了问题中最重要的时间性、前瞻性成分。与已知蛋白质类别的新相互作用可能是未来的发现，而一个旧的相互作用可能涉及一个现已过时的靶点类别。时间划分正确地捕捉了这种时间动态。\n\n**4. 随机i.i.d.划分：**\n- **定义**：将已知相互作用集 $E$ 随机划分为训练集和测试集，不考虑时间、化学结构或蛋白质相似性。\n- **分析**：这种划分基于数据点（相互作用）是独立同分布（i.i.d.）的假设。这种方法衡量的是模型在现有知识图谱内进行插值或“填空”的能力。它完全忽略了数据生成过程 $P_t$ 的非平稳性。问题正确地指出，这种方法“未考虑未来的漂移”。因此，随机划分为模型在真实前瞻性任务上的性能提供了一个不佳的、通常过于乐观的估计。它没有测量在时间漂移下的OOD泛化能力，因此不适合评估旨在进行未来预测的模型。\n\n**结论：**\n基于第一性原理，时间划分是药物重定位最合适的评估策略。它直接模拟了任务的前瞻性，并评估了模型在定义真实世界部署的时间分布漂移下的性能。\n\n现在，我将评估所提供的选项。\n\n**选项A：**“骨架划分...是衡量药物重定位的最佳方法，因为重定位主要需要外推到新颖的化学骨架...”\n这是**不正确的**。药物重定位“主要需要外推到新颖的化学骨架”这一前提与重定位的通常定义不符，后者侧重于为*现有*药物寻找新用途。这种划分更适合评估用于*从头*设计的模型。\n\n**选项B：**“时间划分...是衡量药物重定位的最佳方法，因为重定位本质上是前瞻性的；它在未来分布 $P_{t>t_0}$ 下评估 $R_{t>t_0}(f)$，避免了信息泄露，并反映了大多数重定位利用的是现有药物而非未见过的骨架...”\n这是**正确的**。该选项准确地指出重定位是一项前瞻性任务。它正确地将时间划分映射到在未来分布 $P_{t>t_0}$ 上评估性能的形式化目标，这对应于最小化未来风险 $R_{t>t_0}(f)$。它正确地指出这种设置避免了从未来到训练过程的信息泄露。最后，它正确地将其与骨架新颖性的目标进行对比，指出重定位通常涉及现有药物，这与问题定义和我们的分析完全一致。\n\n**选项C：**“蛋白质同源性划分...是衡量药物重定位的最佳方法，因为重定位专注于为与先前靶点在进化上不相关的蛋白质发现药物...”\n这是**不正确的**。这种说法过分简化并曲解了药物重定位的范围。虽然为新颖靶点家族寻找药物是一个有价值的目标，但这并非重定位的唯一甚至主要定义。这种划分忽略了问题的根本性时间维度。\n\n**选项D：**“对 $E$ 进行随机i.i.d.划分是衡量药物重定位的最佳方法，因为它在 $P_{t_0}$ 分布下以低方差估计 $R_{t_0}(f)$；骨架、时间和蛋白质同源性划分引入了人为的分布漂移...”\n这是**不正确的**。该选项显示了对OOD泛化的根本性误解。由时间、骨架和同源性划分建模的分布漂移并非“人为的”；它们反映了i.i.d.假设错误地忽略了的真实世界结构和动态。稳健评估的目标正是衡量在这些漂移下的性能。i.i.d.划分为前瞻性任务的性能提供了一个误导性的估计。它衡量的是插值能力，而不是所需的外推能力。", "answer": "$$\\boxed{B}$$", "id": "4570155"}]}