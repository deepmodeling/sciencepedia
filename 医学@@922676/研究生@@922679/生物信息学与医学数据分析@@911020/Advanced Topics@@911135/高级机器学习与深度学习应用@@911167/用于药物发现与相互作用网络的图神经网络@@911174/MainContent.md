## 引言
药物的研发是一个周期漫长、成本高昂且充满不确定性的过程。近年来，人工智能，特别是[图神经网络](@entry_id:136853)（GNN），为加速这一进程提供了革命性的新范式。与传统机器学习方法不同，GNN天生能够处理和理解分子结构与[生物相互作用](@entry_id:196274)网络这种复杂的图数据，从而解决了长期以来困扰[计算化学](@entry_id:143039)与生物信息学的一大难题。本文旨在系统性地介绍GNN在药物发现领域的原理、应用与实践。我们将深入探讨GNN如何从根本上改变我们分析和设计药物分子的方式，揭示其背后的核心机制，并展示其在前沿研究中的巨大潜力。

为帮助读者构建一个全面的知识体系，本文将分为三个核心章节。在“原理与机制”一章中，我们将首先奠定理论基础，详细解析如何将分子和相互作用网络转化为GNN能够处理的数学图，并深入剖析[消息传递](@entry_id:751915)、[注意力机制](@entry_id:636429)等关键算法。接下来，在“应用与跨学科连接”一章中，我们会将理论付诸实践，展示GNN在预测药物-靶点相互作用、进行[药物重定位](@entry_id:748682)、乃至[从头设计](@entry_id:170778)全新分子等真实世界任务中的强大能力。最后，“动手实践”部分将提供一系列精心设计的问题，旨在巩固您对核心概念的理解并培养解决实际问题的能力。现在，让我们首先深入GNN的核心，探索其强大的工作原理与机制。

## 原理与机制

本章在前一章介绍的背景基础上，深入探讨了[图神经网络](@entry_id:136853)（GNN）在[药物发现](@entry_id:261243)和相互作用网络领域的核心工作原理与关键机制。我们将从如何将分子和生物网络这些复杂的实体转化为GNN能够处理的数学对象开始，逐步深入到GNN的核心引擎——[消息传递范式](@entry_id:635682)，并最终探讨其在具体生物信息学任务中的应用、局限性以及前沿进展。

### 将分子与相互作用表示为图

[图神经网络](@entry_id:136853)的第一步，也是最关键的一步，是建立一个能准确捕捉研究对象本质特征的[图表示](@entry_id:273102)。无论是单个小分子药物，还是复杂的药物-靶点相互作用网络，都需要被形式化为由节点和边构成的图结构。

#### 分子图：从化学到图论

在计算化学和化学信息学中，将分[子表示](@entry_id:141094)为一个图是一种自然且强大的抽象。在这个表示中，每个原子对应图中的一个**节点（node）** $v$，而连接原子对的[化学键](@entry_id:145092)则对应图中的一条**边（edge）** $e$。因此，一个分子可以被严谨地定义为一个图 $G=(V, E)$，其中 $V$ 是原子集合，E 是[化学键](@entry_id:145092)集合。

由于[化学键](@entry_id:145092)是两个原子之间的相互作用，不具有内在方向性，因此分[子图](@entry_id:273342)通常被建模为**无向图（undirected graph）**。此外，两个原子之间通常只考虑一种最主要的键合关系（例如，即使存在共振，也抽象为一种特定的键类型），且原子不会与自身形成[化学键](@entry_id:145092)。这使得**简单图（simple graph）**，即不含[自环](@entry_id:274670)或[多重边](@entry_id:273920)的图，成为描述分子拓扑结构的标准选择。

然而，一个仅仅描述了连接性的图远不足以进行有意义的化学分析。为了确保[图表示](@entry_id:273102)的化学有效性，必须引入化学规则作为约束。其中最核心的约束是**化合价（valence）**。一个原子的化合价，即它在形成[共价键](@entry_id:146178)时贡献的电子数，必须符合其元素类型和电荷状态所允许的化学规则（如[八隅体规则](@entry_id:141395)）。这可以通过一个数学约束来强制执行：对于图中的任意一个原子节点 $v$，其所有键的键级（bond order）之和必须等于该原子一个被允许的化合价。形式上，这个约束可以写作：

$$
\sum_{u \in V} A_{uv} \cdot o(\{u,v\}) \in \mathcal{V}\big(Z(v), q(v)\big)
$$

这里，$A$ 是图的**[邻接矩阵](@entry_id:151010)（adjacency matrix）**，如果原子 $u$ 和 $v$ 之间有键，则 $A_{uv}=1$，否则为 $0$。$o(\{u,v\})$ 是连接原子 $u$ 和 $v$ 的[化学键](@entry_id:145092)的**键级**（例如，单键为1，双键为2，[三键](@entry_id:202498)为3）。$\mathcal{V}(Z(v), q(v))$ 代表[原子序数](@entry_id:139400)为 $Z(v)$、形式电荷为 $q(v)$ 的原子所允许的化合价整数集合。例如，一个中性的碳原子（$Z=6, q=0$），其化合价通常为4。这个约束确保了我们构建的分[子图](@entry_id:273342)不会出现化学上不可能的结构 [@problem_id:4570146]。

#### 属性图：编码化学信息

为了让GNN能够学习，纯粹的拓扑结构信息是不够的。我们需要为图的节点和边赋予丰富的特征，这便构成了**属性图（attributed graph）**。它与**标记图（labeled graph）**（其中节点和边仅被赋予离散的、有限的符号标签）相比，通常使用实值向量作为特征，这为神经网络提供了更灵活和连续的输入空间 [@problem_id:4570207]。

**节点（原子）属性**:
对于分[子图](@entry_id:273342)中的每个原子节点 $v$，其特征向量 $x_v$ 需要编码所有对化学性质至关重要的信息。一个典型的、最小化的原子特征集包括：

*   **元素类型**：如C, N, O, S等。这通常通过对一个预定义的元素词汇表进行**[独热编码](@entry_id:170007)（one-hot encoding）**来实现。例如，如果词汇表包含9种常见元素外加一个“其他”类别，那么元[素特征](@entry_id:155979)就是一个10维的向量。
*   **度（Degree）**：与该原子连接的[化学键](@entry_id:145092)数量。这可以反映原子的拥挤程度，通常也被[分箱](@entry_id:264748)和[独热编码](@entry_id:170007)。
*   **形式电荷（Formal Charge）**：原子的电荷状态，如-1, 0, +1等，对反应性至关重要。
*   **[芳香性](@entry_id:144501)（Aromaticity）**：一个布尔标志，指示该原子是否属于一个芳香环系统。
*   **手性（Chirality）**：描述[立体化学](@entry_id:166094)的属性，如R/S构型，对于药物与生物大分子的特异性识别至关重要。

将这些[独热编码](@entry_id:170007)后的向量拼接起来，就形成了一个高维的原子特征向量 $x_v \in \mathbb{R}^d$。例如，一个包含10个元素类别、6个度类别、6个电荷类别、1个[芳香性](@entry_id:144501)标志位和4个手性类别的原子，其特征向量维度将是 $d = 10 + 6 + 6 + 1 + 4 = 27$ [@problem_id:4570207]。这种基于原子内在属性的特征构建方式，保证了节[点特征](@entry_id:155984)的**[置换不变性](@entry_id:753356)（permutation invariance）**，即特征的计算与原子在分子中的任意编号顺序无关，这是GNN设计的一个基本要求。

**边（[化学键](@entry_id:145092)）属性**:
类似地，每条边 $e$ 也需要一个特征向量 $x_e$ 来[描述化学](@entry_id:148710)键的性质。关键的边特征包括：

*   **键类型（Bond Type）**：通常用[独热编码](@entry_id:170007)表示单键、双键、三键或芳香键。
*   **共轭性（Conjugation）**：一个布尔标志，指示该键是否是[共轭体系](@entry_id:195248)（如交替的单双键）的一部分。
*   **立体化学（Stereochemistry）**：如顺式/反式（Z/E）异构等。

在GNN的计算图中，虽然分子的[化学键](@entry_id:145092)是无向的，但为了实现更灵活的信息流动，我们常常将每条无向边 $(u,v)$ 视为两条有向的**消息通道（message channels）**：$u \to v$ 和 $v \to u$。这种处理方式允许模型学习非对称的相互作用，例如在预测化学反应中的电子流方向，或是在[蛋白质-配体对接](@entry_id:174031)中模拟氢键供体到受体的方向性时，这种设计尤为重要 [@problem_id:4570193]。

#### 超越单分子：相互作用网络

GNN的应用远不止于单个分子。在[药物发现](@entry_id:261243)中，我们更关心分子间的相互作用。

一个重要的例子是**药物-靶点相互作用（Drug-Target Interaction, DTI）网络**。这类网络可以用一个**二部图（bipartite graph）** $G = (D \cup T, E)$ 来表示。其中，一个节点集 $D$ 代表所有药物分子，另一个不相交的节点集 $T$ 代表所有蛋白质靶点。图中的每条边 $e \in E$ 都连接一个药物节点 $d \in D$ 和一个靶点节点 $t \in T$，表示该药物与该靶点之间存在已知的相互作用 [@problem_id:4570200]。

在更复杂的生物网络中，相互作用的类型多种多样。例如，一个蛋白质可能“抑制”另一个蛋白质的活性，而两条药物分子可能因为相似的化学结构而“相互作用”。这种包含多种关系类型的图被称为**异构图（heterogeneous graph）**或**关系图（relational graph）**。在这种图中，每条边不仅连接两个节点，还带有一个关系类型 $r$ 的标签，如 $(u \xrightarrow{r} v)$ [@problem_id:4570117]。处理这类复杂的相互作用网络是GNN在现代生物信息学中的一个核心优势。

### [消息传递范式](@entry_id:635682)

一旦我们将化学或生物实体表示为属性图，接下来的问题就是GNN如何从这些图中学习。几乎所有现代GNN的核心机制都可以归结为一个统一的框架：**[消息传递](@entry_id:751915)（Message Passing）**。

#### 核心原则：[置换不变性](@entry_id:753356)与局部聚合

GNN模型必须满足一个基本性质：其输出对于节点的任意重新排序都应保持不变（对于图级别的预测）或相应地改变（对于节点级别的预测）。这一性质称为**[置换不变性](@entry_id:753356)/等变性（permutation invariance/equivariance）**。为了满足这一要求，GNN不能像处理序列或图像的神经网络那样，依赖于输入的固定顺序。

[消息传递](@entry_id:751915)框架通过在每个节点上执行局部、对称的聚合操作来自然地实现这一性质。其核心思想源于一个深刻的数学结论：任何作用于一个集合（这里是节点的邻居集合）且对集合中元素顺序不敏感的连续函数，都可以被分解为三个部分：首先对集合中的每个元素独立地应用一个变换函数 $\psi$，然后用一个对顺序不敏感的**[聚合算子](@entry_id:746335)** $\square$（如求和、求平均或取最大值）将变换后的结果汇总，最后再用一个[更新函数](@entry_id:275392) $\phi$ 将汇总信息与节点自身的状态相结合 [@problem_id:4570168]。

这引出了GNN的通用逐层更新公式。在第 $(t+1)$ 层，节点 $v$ 的隐藏状态 $h_v^{(t+1)}$ 是根据其在第 $t$ 层的状态 $h_v^{(t)}$ 以及其所有邻居 $\mathcal{N}(v)$ 在第 $t$ 层的状态 $\{h_u^{(t)} \mid u \in \mathcal{N}(v)\}$ 来计算的：

$$
h_v^{(t+1)} = \phi \left( h_v^{(t)}, \underset{u \in \mathcal{N}(v)}{\square} \psi \left( h_v^{(t)}, h_u^{(t)}, e_{uv} \right) \right)
$$

这里的 $\psi$ 通常被称为**消息函数（message function）**，它根据源节点 $u$、目标节点 $v$ 的状态以及它们之间的边特征 $e_{uv}$ 来生成一条“消息”。[聚合算子](@entry_id:746335) $\square$ 必须是**可交换的（commutative）**，如 $\sum$（求和）、$\mathrm{mean}$（平均）或 $\max$（按元素取最大值），以保证对邻居顺序的不变性。最后，**[更新函数](@entry_id:275392)（update function）** $\phi$（通常是另一个小型神经网络）将聚合后的邻居消息与节点 $v$ 之前的状态结合，生成新的状态。

为了具体理解这一过程，我们可以看一个实例。假设我们使用求和作为聚合器，并用带有[ReLU激活函数](@entry_id:138370)的多层感知机（MLP）作为 $\psi$ 和 $\phi$。对于一个给定的节点 $v$ 和它的邻居，我们首先为每个邻居 $u$ 计算一条消息，这条消息是节点 $v$ 的状态、邻居 $u$ 的状态以及边 $e_{vu}$ 特征的函数。然后，我们将所有从邻居发来的消息向量逐元素相加，得到一个聚合后的消息向量 $m_v^{(t)}$。最后，我们将节点 $v$ 当前的状态 $h_v^{(t)}$ 与这个聚合消息 $m_v^{(t)}$ 拼接起来，输入到更新网络 $\phi$ 中，得到节点 $v$ 在下一层的新状态 $h_v^{(t+1)}$ [@problem_id:4570168]。这个过程在图中的所有节点上并行执行，逐层推进，使得信息能够在整个图中传播。

#### 处理关系数据：关系[图卷积网络](@entry_id:194500) (R-GCN)

对于前面提到的关系图，其中边被赋予了不同的类型（如“结合”、“抑制”），标准的[消息传递](@entry_id:751915)模型需要扩展。**关系[图卷积网络](@entry_id:194500)（Relational Graph Convolutional Network, R-GCN）**正是为此设计的。其核心思想是为每一种关系类型 $r \in \mathcal{R}$ 学习一个专属的[变换矩阵](@entry_id:151616) $W_r$。这样，从邻居 $j$ 发往节点 $i$ 的消息就不仅取决于 $j$ 的特征，还取决于连接它们的关系类型。

R-GCN的更新公式是对通用[消息传递](@entry_id:751915)框架的直接特化：

$$
h_i^{(l+1)} = \sigma \left( \sum_{r \in \mathcal{R}} \sum_{j \in \mathcal{N}_i^r} \frac{1}{c_{i,r}} W_r h_j^{(l)} + W_0 h_i^{(l)} \right)
$$

在这个公式中，更新节点 $i$ 的状态时，模型会遍历所有可能的关系类型 $r$。对于每一种关系，它会聚合所有通过类型为 $r$ 的边连接到 $i$ 的邻居（$\mathcal{N}_i^r$）的消息。每条消息都是邻居 $j$ 的特征 $h_j^{(l)}$ 经过关系 $r$ 的专属权重矩阵 $W_r$ [线性变换](@entry_id:143080)后得到的。$c_{i,r}$ 是一个[归一化常数](@entry_id:752675)（例如，邻居数量 $|\mathcal{N}_i^r|$），用于稳定学习过程。最后，模型还将节点 $i$ 自身的状态 $h_i^{(l)}$ 通过一个自循环权重矩阵 $W_0$ 进行变换并加入总和，这使得节点可以保留部分自身信息。$\sigma$ 是一个[非线性激活函数](@entry_id:635291) [@problem_id:4570117]。

#### 更具表达力的机制：[图注意力网络](@entry_id:634951) (GAT)

R-GCN为不同类型的邻居分配了不同的权重矩阵，但对于同一类型的所有邻居，它们的贡献在聚合前是同等重要的（仅通过归一化常数进行缩放）。然而，在很多场景下，即使是同一类型的邻居，其重要性也可能天差地别。例如，在预测一个分子的某个化学性质时，其官能团中的某个原子可能比脂肪链上的其他原子提供更关键的信息。

**[图注意力网络](@entry_id:634951)（Graph Attention Network, GAT）**通过引入**[注意力机制](@entry_id:636429)（attention mechanism）**来解决这个问题。GAT的核心思想是让模型在聚合邻居信息时，为每个邻居动态地计算一个**注意力系数（attention coefficient）** $\alpha_{uv}$，这个系数代表了在更新节点 $v$ 时，邻居 $u$ 的信息应该占多大的权重。

这些注意力系数不是固定的，而是根据源节点 $u$ 和目标节点 $v$ 的当前状态动态计算的。在包含边特征的GAT变体中，注意力分数 $s_{uv}$（在归一化之前）可以是一个同时依赖于 $h_u$、$h_v$ 和边特征 $e_{uv}$ 的函数。一个常见的形式是：

$$
s_{uv} = a^\top \left[ W h_v \, \Vert \, W h_u \, \Vert \, e_{uv} \right]
$$

这里，$W$ 是一个共享的[线性变换](@entry_id:143080)，用于提升节[点特征](@entry_id:155984)的[表达能力](@entry_id:149863)。$\Vert$ 表示向量拼接，$a$ 是一个可学习的权重向量。这个公式优雅地将目标节点、源节点和它们之间边的信息融合在一起，共同决定了注意力的大小。特别是边特征 $e_{uv}$ 的引入，它相当于为注意力分数增加了一个与[化学键](@entry_id:145092)类型相关的偏置项，从而使得模型能够学会根据键的性质（如双键 vs. [单键](@entry_id:188561)）来调节邻居的重要性 [@problem_id:4570176]。

计算出所有邻居的注意力分数后，使用**[Softmax](@entry_id:636766)**函数进行归一化，得到最终的注意力系数 $\alpha_{uv}$：

$$
\alpha_{uv} = \mathrm{softmax}_{u \in \mathcal{N}(v)}(s_{uv}) = \frac{\exp(s_{uv})}{\sum_{w \in \mathcal{N}(v)} \exp(s_{wv})}
$$

[Softmax](@entry_id:636766)的特性确保了所有邻居的注意力系数之和为1，形成一个概率分布。这也引入了一种“竞争”机制：如果某个邻居的注意力分数显著增加，其他邻居的注意力系数必然会相应下降 [@problem_id:4570176]。最终，节点 $v$ 的更新不再是简单的求和或平均，而是其邻居特征的加权和，权重就是这些动态计算出的注意力系数。值得注意的是，注意力系数通常是非对称的，即 $\alpha_{uv}$ 一般不等于 $\alpha_{vu}$，因为归一化的分母分别是基于 $v$ 的邻居集和 $u$ 的邻居集。

### 从图结构到预测

经过 $L$ 轮[消息传递](@entry_id:751915)后，每个节点 $v$ 的最终隐藏状态 $h_v^{(L)}$ 就成了一个高度浓缩的、包含了其 $L$-跳邻域内丰富结构信息的特征表示，即**节点嵌入（node embedding）**。这些高质量的嵌入向量是所有下游任务的起点。

对于**图级别（graph-level）**的任务，例如预测整个分子的性质（如溶解度、毒性或量子化学能量），我们需要将图中所有节点的嵌入向量聚合成一个单一的[图表示](@entry_id:273102)。这个过程称为**读出（readout）**或**池化（pooling）**。最简单的方法是对所有节点的嵌入向量进行求和、求平均或取最大值。得到的图级别嵌入向量随后被送入一个标准的[前馈神经网络](@entry_id:635871)（如MLP）以进行最终的预测。

对于**[链接预测](@entry_id:262538)（link prediction）**任务，目标是预测图中两个节点之间是否存在一条边。这在DTI网络中尤其重要，因为我们的目标是发现新的药物-靶点相互作用。在这种情况下，我们利用GNN为药物节点 $d$ 和靶点节点 $t$ 生成的嵌入向量 $\mathbf{z}_d$ 和 $\mathbf{z}_t$。然后，一个**解码器（decoder）**模型会接收这两个嵌入向量，并输出它们之间存在链接的概率。一个强大且灵活的解码器模型可以被形式化为：

$$
p\big((d,t) \in E \,\big|\, \mathbf{z}_d, \mathbf{z}_t\big) = \sigma\left(\mathbf{z}_d^\top W \mathbf{z}_t + b_d + c_t + \log\frac{\pi}{1-\pi}\right)
$$

其中，$\sigma$ 是 logistic sigmoid 函数，将得分映射到 $(0,1)$ 区间的概率。该模型非常精巧：
*   $\mathbf{z}_d^\top W \mathbf{z}_t$ 是一项**[双线性](@entry_id:146819)（bilinear）**交互项，通过一个可学习的矩阵 $W$ 来捕捉药物和靶点嵌入之间复杂的、非对称的匹配模式。
*   $b_d$ 和 $c_t$ 是为每个药物和靶点学习的**节点偏置（node biases）**。它们可以捕捉到节点的固有“交互倾向性”或“受欢迎程度”。例如，某些药物可能具有“滥竽充数”的特性，与许多靶点都有相互作用；同样，某些靶点也可能被许多药物靶向。这些偏置项可以校正这种由节点度（degree）引起的偏差。
*   $\log\frac{\pi}{1-\pi}$ 是一项基于先验知识的全局偏置。如果已知在所有可能的药物-靶点对中，真实相互作用的比例（先验概率）为 $\pi$，那么将其对数几率（log-odds）作为模型的基准，可以使学习更加稳定和符合[贝叶斯解释](@entry_id:265644) [@problem_id:4570200]。

### 前沿课题与局限性

尽管GNN在处理图结构数据方面取得了巨大成功，但理解其局限性并探索克服这些局限性的前沿方法，对于进行高水平的研究至关重要。

#### 二维图的局限：几何学的必要性

标准GNN操作的是分子的二维拓扑图，即原子如何通过[化学键](@entry_id:145092)连接。然而，分子是三维实体，其生物活性和物理化学性质在很大程度上取决于其在三维空间中的精确**构象（conformation）**。

药物与靶点的结合亲和力，本质上是由分子间的相互作用能决定的，而这些能量（如范德华力、[静电力](@entry_id:203379)、[氢键](@entry_id:136659)）对原子间的**距离**和**角度**极其敏感。例如，经典的Lennard-Jones势能函数 $V_{\mathrm{LJ}}(r)$ 就明确地依赖于两个非键合原子间的欧几里得距离 $r$。当两个原子在空间中靠得太近时（小于它们[范德华半径](@entry_id:142957)之和），$V_{\mathrm{LJ}}(r)$ 会因为 $r^{-12}$ 这一项而急剧增大，产生强烈的排斥力，这被称为**空间位阻（steric clash）**。

一个仅基于二维图的GNN无法获取这些三维坐标信息。它无法计算原子间的距离和角度。因此，它无法区分同一分子的不同三维构象或在蛋白口袋中的不同对接姿态（pose）。设想一个配体分子有两种不同的对接姿态A和B。在姿态A中，配体的一个原子与蛋白质口袋的一个原子距离过近，产生了巨大的排斥能，这是一个非常不利的结合模式。而在姿态B中，两者距离适中，可能形成一个微弱的吸[引力](@entry_id:189550)。对于一个二维GNN来说，由于两种姿态下的[分子连接性](@entry_id:182740)完全相同，它看到的输入图是完全一样的，因此会给出完全相同的预测结果，从而完全错失了两者在结合能上的巨大差异。这个思想实验清晰地揭示了，为了准确预测与结构相关的性质（如[结合亲和力](@entry_id:261722)），我们必须将[三维几何](@entry_id:176328)信息纳入模型中 [@problem_id:4570190]。

#### 融合[三维几何](@entry_id:176328)：SE(3)-等变GNN

如何将三维坐标信息融入GNN，同时又不破坏其对[旋转和平移](@entry_id:175994)的物理不变性？答案在于构建**SE(3)-等变（SE(3)-equivariant）**的神经网络。SE(3)是三维空间中刚体运动（[旋转和平移](@entry_id:175994)）的[特殊欧几里得群](@entry_id:139383)。一个函数 $f$ 被称为SE(3)-等变的，如果对输入进行[刚体变换](@entry_id:150396)后，其输出也相应地进行同样的变换（对于向量等几何对象）或保持不变（对于标量）。

构建SE(3)-等变GNN的关键在于只使用几何上“合法”的操作。这意味着我们只能从基本的等变和不变构建块出发。

*   **不变（Invariant）构建块**：两个点的欧几里得距离 $\|x_u - x_v\|$ 或其平方 $\|x_u - x_v\|^2$ 在[旋转和平移](@entry_id:175994)下保持不变。任意两个向量的点积 $v_a \cdot v_b$ 也具有旋转不变性。
*   **等变（Equivariant）构建块**：两个点之间的相对位置向量 $r_{uv} = x_u - x_v$ 是等变的。当整个系统被旋转时，这个相对位置向量也随之旋转。

一个SE(3)-等变的[消息传递](@entry_id:751915)层可以这样构建：首先，消息的**系数** $a_{uv}$ 必须是一个**标量**，且只能由不变的量计算得出。例如，它可以是原子标量特征 $s_u, s_v$、边标量特征 $e_{uv}$ 和原子间距离平方 $\|r_{uv}\|^2$ 的函数。然后，将这个不变的标量系数 $a_{uv}$ 乘以一个等变的向量，例如相对位置向量 $r_{uv}$，得到的消息向量 $m_{u \leftarrow v} = a_{uv} r_{uv}$ 就是等变的。最后，将这些等变的消息向量聚合起来（例如求和），去更新另一个等变向量（如节点上的矢量特征 $v_u$），最终的更新结果仍然保持[等变性](@entry_id:636671) [@problem_id:4570122]。这类模型，通常被称为**[E(3)等变网络](@entry_id:188955)（E(3) Equivariant Networks, E3NN）**，代表了GNN在分子科学应用中的一个重要前沿。

#### 关键限制：过压缩问题

除了几何信息的缺失，经典的[消息传递范式](@entry_id:635682)还面临另一个固有的结构性限制，称为**过压缩（oversquashing）**。这个问题与另一个常被提及的“过平滑（oversmoothing）”（即随着网络层数加深，所有节点嵌入趋于一致）不同。过压缩是一个关于[信息瓶颈](@entry_id:263638)的问题，它源于图的拓扑结构本身。

当大量信息需要从图的一个广大区域传递到另一个区域，而这两个区域之间仅通过少数几条边（一个狭窄的“图割”）连接时，就会发生过压缩。在多层GNN中，一个节点 $v$ 的最终表示取决于其感受野（receptive field）内的所有节点。如果感受野随着层数加深而指数级增长，但所有这些信息都必须被压缩并通过一个容量有限的瓶颈，那么信息损失将不可避免。

一个典型的例子是两个树状（或树枝状）分子片段通过一个[单键](@entry_id:188561)连接。为了让目标节点（例如，其中一棵树的根节点）的表示能够响应另一棵树上所有叶子节[点特征](@entry_id:155984)的变化，来[自指](@entry_id:153268)数级数量的叶子节点的信息必须在每一层都被编码、压缩，并通过那个唯一的[单键](@entry_id:188561)传递。在[消息传递](@entry_id:751915)模型中，跨越这个单键的信息量上限是由消息向量的维度 $d$ 决定的。无论 $d$ 有多大，它都是一个固定的常数。然而，需要传递的信息源数量却可以随着树的深度指数增长。

从数学上讲，这可以被理解为目标节点最终嵌入相对于遥远源节点输入的**[雅可比矩阵](@entry_id:178326)（Jacobian matrix）**的秩受到了瓶颈大小的限制。由于所有计算路径都必须通过那个低维的消息向量，[雅可比矩阵](@entry_id:178326)的秩最多只能等于瓶颈的维度。因此，模型无法学会需要区分大量独立输入模式的复杂函数，因为信息在通过狭窄的图结构时被“压扁”了。这揭示了标准MPNN在处理具有瓶颈拓扑的[长程依赖](@entry_id:181727)问题时的根本局限性 [@problem_id:4570172]。