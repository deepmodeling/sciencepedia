{"hands_on_practices": [{"introduction": "卷积神经网络（CNN）是识别生物序列（如DNA）中功能模式（如转录因子结合位点）的强大工具。卷积层的核心参数，包括卷积核大小、步长和填充，直接决定了模型的感受野及其处理序列的方式。这项练习将帮助你把抽象的超参数与具体的生物学意义联系起来，这是将深度学习应用于基因组学的一项关键技能。[@problem_id:4553876]", "problem": "您正在设计一个卷积神经网络（CNN）中的第一层一维卷积，用于检测以独热编码（one-hot）形式沿通道编码的脱氧核糖核酸（DNA）序列中的转录因子结合基序。考虑一个长度为 $L_{\\mathrm{in}} = 1{,}000$ 个碱基的输入序列。该卷积层使用长度为 $k = 15$ 的卷积核、步幅 $s = 3$、两端对称的零填充 $p = 2$ 个碱基，以及扩张率 $d = 1$。\n\n从带步幅和填充的离散一维卷积在填充后输入上的定义出发，推导确定输出位置数量（即有效卷积核对齐次数）的通用表达式。利用此推理过程，为给定参数计算输出序列的长度 $L_{\\mathrm{out}}$。然后，基于感受野的概念和步幅引起的采样，分析此配置如何影响检测跨越 $M = 20$ 个碱基的基序的能力，特别是关于单次滤波器应用内的完全覆盖、位置敏感性以及序列边界附近的检测。\n\n报告输出长度 $L_{\\mathrm{out}}$，为一个不带单位的整数。无需四舍五入。您的定性分析应包含在推理过程中，但最终答案必须仅为计算出的 $L_{\\mathrm{out}}$。", "solution": "用户要求推导一维卷积层的输出长度，并对其在生物信息学特定应用中的属性进行定性分析。本解答将首先建立输出维度的一般公式，然后将其应用于给定参数，最后分析该配置的意义。\n\n**问题验证**\n\n在继续之前，对问题陈述进行严格验证。\n1.  **提取的已知条件**：\n    -   输入序列长度: $L_{\\mathrm{in}} = 1{,}000$ 个碱基。\n    -   卷积核长度: $k = 15$。\n    -   步幅: $s = 3$。\n    -   对称零填充: $p = 2$（每端）。\n    -   扩张率: $d = 1$。\n    -   用于分析的目标基序长度: $M = 20$ 个碱基。\n\n2.  **验证结论**：问题是**有效的**。它在科学上基于深度学习（特别是卷积神经网络）的标准原理，并应用于基因组分析这一成熟的生物信息学领域。问题提法清晰，提供了一套完整且一致的参数（$L_{\\mathrm{in}}, k, s, p, d$），可以从中确定一个唯一且有意义的输出长度（$L_{\\mathrm{out}}$）解。语言客观，并采用了标准术语。参数对于所描述的任务是符合实际的。\n\n**输出长度通用表达式的推导**\n\n一维卷积层的输出位置数，即输出序列长度 $L_{\\mathrm{out}}$，取决于卷积核沿输入序列的有效放置次数。这可以从第一性原理推导出来。\n\n1.  一个长度为 $L_{\\mathrm{in}}$ 的输入序列首先进行对称填充，即在其两端各添加 $p$ 个元素。这个新的、经过填充的序列长度，记为 $L_{\\mathrm{padded}}$，是：\n    $$L_{\\mathrm{padded}} = L_{\\mathrm{in}} + 2p$$\n\n2.  长度为 $k$ 的卷积核可以被扩张。扩张因子 $d$ 在卷积核连续元素之间引入 $d-1$ 个间隙。因此，卷积核在输入上的有效空间范围，称为有效卷积核大小 $k_{\\mathrm{eff}}$，为：\n    $$k_{\\mathrm{eff}} = k + (k-1)(d-1)$$\n\n3.  卷积核以一定的步幅（'strided'）在填充后的序列上移动。每个位置都会计算一个输出。卷积核的第一次放置从填充后序列的索引 $0$ 开始。覆盖的范围是从索引 $0$ 到 $k_{\\mathrm{eff}}-1$。最后一次可能的放置也必须完全在填充后的序列内部。卷积核的最后一个元素必须对齐到不大于 $L_{\\mathrm{padded}}-1$ 的索引上。这意味着卷积核的最后一个起始位置在索引 $L_{\\mathrm{padded}} - k_{\\mathrm{eff}}$ 处。\n\n4.  因此，可能的起始位置总范围是从 $0$ 到 $L_{\\mathrm{padded}} - k_{\\mathrm{eff}}$，如果步幅为 $1$，则包含 $L_{\\mathrm{padded}} - k_{\\mathrm{eff}} + 1$ 个可能的位置。\n\n5.  当步幅为 $s$ 时，卷积核不会占据每个可能的位置。相反，它占据位置 $0, s, 2s, \\dots, Ns$，其中 $Ns$ 是最后一个有效的起始位置。这个最后的位置必须满足 $Ns \\le L_{\\mathrm{padded}} - k_{\\mathrm{eff}}$。因此，步数 $N$ 是满足此条件的最大整数，可以使用向下取整函数找到：\n    $$N = \\left\\lfloor \\frac{L_{\\mathrm{padded}} - k_{\\mathrm{eff}}}{s} \\right\\rfloor$$\n    输出位置的总数 $L_{\\mathrm{out}}$ 是这些放置的次数，即 $N+1$（包括在位置 $0$ 的初始放置）。\n\n6.  代入 $L_{\\mathrm{padded}}$ 和 $k_{\\mathrm{eff}}$ 的表达式，得到输出长度的通用公式：\n    $$L_{\\mathrm{out}} = \\left\\lfloor \\frac{(L_{\\mathrm{in}} + 2p) - (k + (k-1)(d-1))}{s} \\right\\rfloor + 1$$\n\n**根据给定参数计算 $L_{\\mathrm{out}}$**\n\n问题提供了以下参数：$L_{\\mathrm{in}} = 1{,}000$，$k = 15$，$s = 3$，$p = 2$ 和 $d = 1$。\n\n首先，我们将这些值代入推导出的通用表达式中。由于扩张率 $d=1$，有效卷积核长度 $k_{\\mathrm{eff}}$ 就等于 $k$：\n$$k_{\\mathrm{eff}} = 15 + (15-1)(1-1) = 15 + (14)(0) = 15$$\n$L_{\\mathrm{out}}$ 的公式简化为：\n$$L_{\\mathrm{out}} = \\left\\lfloor \\frac{L_{\\mathrm{in}} + 2p - k}{s} \\right\\rfloor + 1$$\n代入数值：\n$$L_{\\mathrm{out}} = \\left\\lfloor \\frac{1{,}000 + 2(2) - 15}{3} \\right\\rfloor + 1$$\n$$L_{\\mathrm{out}} = \\left\\lfloor \\frac{1{,}000 + 4 - 15}{3} \\right\\rfloor + 1$$\n$$L_{\\mathrm{out}} = \\left\\lfloor \\frac{989}{3} \\right\\rfloor + 1$$\n除法产生一个非整数值：\n$$\\frac{989}{3} \\approx 329.667$$\n应用向下取整函数，得到小于或等于此值的最大整数：\n$$\\lfloor 329.667 \\rfloor = 329$$\n最后，加 $1$ 得到输出长度：\n$$L_{\\mathrm{out}} = 329 + 1 = 330$$\n\n**配置的定性分析**\n\n问题还要求分析此配置检测长度为 $M = 20$ 个碱基的基序的能力。\n\n1.  **完全覆盖与感受野**：此卷积层中任何单个神经元的感受野是其有效卷积核大小，即 $k_{\\mathrm{eff}} = 15$ 个碱基。感兴趣的基序长度为 $M=20$ 个碱基。由于 $M > k_{\\mathrm{eff}}$，**没有任何单次滤波器应用能够看到整个基序**。该滤波器最多只能学会识别长度不超过 15 个碱基的基序片段。要识别完整的 20 碱基基序，网络必须学会组合来自输出特征图中空间上相邻神经元的信息，这是后续层（例如，另一个卷积层或全连接层）的任务。\n\n2.  **位置敏感性与步幅**：步幅 $s=3$ 引入了对输入的下采样。这意味着卷积核每移动一步会“跳过”2 个碱基。因此，该层对于单碱基平移不具有平移等变性。一个从输入位置 $i$ 开始的基序与一个从位置 $i+1$ 或 $i+2$ 开始的基序，将被卷积核以不同的方式看待。这种对模 $s$ 的位置移位的高度敏感性可能是有害的，因为基序位置的微小变化可能导致其与卷积核的步幅错位，从而导致激活减弱，并可能导致检测失败。\n\n3.  **序列边界附近的检测**：该层使用 $p=2$ 的对称填充。对于大小为 $k=15$ 的卷积核，此填充不足以使卷积核在序列的极端末端对基序进行中心对齐。要将大小为 $k$ 的卷积核在第一个碱基（索引 0）上居中，需要 $p_{\\mathrm{center}} = \\lfloor k/2 \\rfloor = \\lfloor 15/2 \\rfloor = 7$ 的填充。而当 $p=2$ 时，第一个输出神经元的感受野中心位于原始序列位置 $7-2 = 5$ 处。这意味着序列最前面约 5 个和最后面约 5 个碱基内的基序永远无法在卷积核的视野中居中，这会削弱网络检测它们的能力。\n\n总而言之，由于其卷积核尺寸过小，这个特定的层配置在结构上不适合检测 20 碱基的基序。其性能因大步幅和不充分的填充而进一步受损，这影响了位置敏感性和边界检测。", "answer": "$$\\boxed{330}$$", "id": "4553876"}, {"introduction": "许多生物学问题，例如从海量基因数据中识别致病性变异，都存在严重的类别不平衡问题，即阳性样本非常罕见。在这种情况下，诸如受试者工作特征曲线下面积（Area Under the Receiver Operating Characteristic curve, AUROC）等标准评估指标可能会产生误导。精确率-召回率曲线下面积（Area Under the Precision-Recall Curve, AUPRC）通过重点关注稀有的阳性类别，提供了更具信息量的性能度量。本练习旨在巩固你对为何AUPRC是评估不平衡数据集上模型性能的关键指标的理解，并让你掌握其计算方法。[@problem_id:4553811]", "problem": "一个研究团队训练了一个深度神经网络，用于在临床流程中对来自全基因组测序的候选致病性变异进行优先级排序。该模型为每个变异生成校准后的分数 $s \\in [0,1]$。评估在不同的阈值 $\\tau$ 下进行，预测结果根据分数 $s \\geq \\tau$ 时为 $\\hat{y}=1$，否则为 $\\hat{y}=0$ 来形成。设在阈值 $\\tau$ 下的混淆矩阵计数为真阳性 $TP(\\tau)$、假阳性 $FP(\\tau)$、真阴性 $TN(\\tau)$ 和假阴性 $FN(\\tau)$，并将阈值 $\\tau$ 下的标准率定义为召回率 $R(\\tau) = \\frac{TP(\\tau)}{TP(\\tau)+FN(\\tau)}$、精确率 $P(\\tau) = \\frac{TP(\\tau)}{TP(\\tau)+FP(\\tau)}$ 和假阳性率 $FPR(\\tau) = \\frac{FP(\\tau)}{FP(\\tau)+TN(\\tau)}$。该研究队列存在严重的类别不平衡问题，阳性流行率 $\\rho = \\frac{\\text{真阳性样本数}}{\\text{总样本数}}$ 为 $\\rho = 0.02$。\n\n任务：\n\n1. 使用上述依赖于阈值的率为基础，提供受试者工作特征曲线下面积 (AUROC) 和精确率-召回率曲线下面积 (AUPRC) 作为由分数诱导的排序的泛函的严格定义，并论证为何在以小的 $\\rho$ 为特征的严重类别不平衡情况下，AUPRC 比 AUROC 更具信息量。\n\n2. 考虑一个由在单调递减阈值下获得的三个点概括的精确率-召回率曲线：根据标准约定，当没有预测出阳性样本时，$(R,P)$ 等于 $(0,1)$；一个内部工作点 $(0.35,0.75)$；以及最低阈值下的终点 $(1,\\rho)$，这反映了将每个实例都预测为阳性时，精确率等于流行率。假设精确率-召回率曲线在这些点之间的召回率上是分段线性的。计算在召回率 $R \\in [0,1]$ 范围内此分段线性曲线下的精确面积，并报告精确率-召回率曲线下面积 (AUPRC)。将您的数值答案四舍五入到四位有效数字，并以小数形式表示。", "solution": "该问题被评估为有效。它具有科学依据，问题提出得当且客观。所提供的定义和数据是一致的，足以得出一个完整的解。\n\n该问题分为两个任务。第一个任务是对评估指标的概念性解释，第二个任务是基于所提供数据的数值计算。\n\n**第1部分：AUROC和AUPRC的定义及AUPRC的合理性论证**\n\n设 $P_{\\text{total}}$ 为数据集中阳性实例的总数， $N_{\\text{total}}$ 为阴性实例的总数。这些量相对于分类阈值 $\\tau$ 是不变的。问题中给出的率可以表示为：\n- 召回率：$R(\\tau) = \\frac{TP(\\tau)}{P_{\\text{total}}}$\n- 假阳性率：$FPR(\\tau) = \\frac{FP(\\tau)}{N_{\\text{total}}}$\n- 精确率：$P(\\tau) = \\frac{TP(\\tau)}{TP(\\tau) + FP(\\tau)}$\n\n流行率 $\\rho$ 是整个数据集中阳性实例的比例，由 $\\rho = \\frac{P_{\\text{total}}}{P_{\\text{total}} + N_{\\text{total}}}$ 给出。\n\n**受试者工作特征曲线下面积 (AUROC)**\n受试者工作特征 (ROC) 曲线是一个图形图，它展示了二元分类器系统在其判别阈值变化时的诊断能力。它是通过在不同阈值设置下，绘制真阳性率（召回率, $R(\\tau)$）与假阳性率 ($FPR(\\tau)$) 的关系图来创建的。\nAUROC是该曲线下的面积。它由以下积分正式定义：\n$$\nAUROC = \\int_0^1 R(FPR) \\, d(FPR)\n$$\n其中积分是在假阳性率所有可能的值（从 $0$ 到 $1$）上进行的。AUROC有一个关键的统计解释：它等于分类器将一个随机选择的阳性实例的排名高于一个随机选择的阴性实例的排名的概率。设 $s^+$ 是一个随机阳性实例的分数，$s^-$ 是一个随机阴性实例的分数。那么，$AUROC = P(s^+  s^-)$。一个随机分类器的AUROC为 $0.5$，而一个完美分类器的AUROC为 $1$。\n\n**精确率-召回率曲线下面积 (AUPRC)**\n精确率-召回率 (PR) 曲线是针对不同阈值 $\\tau$，绘制精确率 ($P(\\tau)$) 与召回率 ($R(\\tau)$) 的关系图。\nAUPRC是该曲线下的面积。它由精确率作为召回率函数的积分正式定义：\n$$\nAUPRC = \\int_0^1 P(R) \\, dR\n$$\n在实践中，由于实际的PR曲线是一组离散点，这个积分通常使用梯形法则进行近似，正如本问题第二部分所做的那样。一个随机分类器的AUPRC等于流行率 $\\rho$，而一个完美分类器的AUPRC为 $1$。\n\n**在不平衡数据集中使用AUPRC的理由**\n在严重类别不平衡（小的 $\\rho$）情况下，AUPRC比AUROC更具信息量，原因如下：\n1.  **对假阳性的敏感性**：问题的核心在于 $FPR$ 和精确率指标的分母。$FPR$ 由阴性样本总数 $N_{\\text{total}}$ 进行归一化，而精确率的分母是 $TP(\\tau) + FP(\\tau)$，即阳性预测的总数。\n    在一个阳性样本稀少的高度不平衡数据集中，$N_{\\text{total}} \\gg P_{\\text{total}}$。因此，假阳性 ($FP$) 绝对数量的大幅增加可能只会导致 $FPR$ 的微小增加，因为这个增加量被一个非常大的 $N_{\\text{total}}$ 所除。ROC曲线，以及因此的AUROC，将对这种变化不敏感，可能会给出一个具有误导性的乐观性能视图。例如，一个具有低 $FPR$ 的模型可能仍然会产生数千个假阳性，这在后续检测成本高昂或具有侵入性的临床环境中是一个致命的失败。\n2.  **关注阳性预测**：精确率 $P(\\tau) = \\frac{TP(\\tau)}{TP(\\tau) + FP(\\tau)}$，直接评估了阳性预测中正确的比例。它不受真阴性 ($TN$) 数量的影响，而在不平衡的环境中，真阴性构成了数据的绝大多数。因此，基于精确率的AUPRC直接衡量了模型在阳性类别上的性能以及它在该类别上所犯的错误（假阳性）。这通常是更相关的业务或临床问题：“在我的模型标记为致病性的变异中，真正是致病性的比例是多少？”\n3.  **信息丰富的基线**：AUROC的基线性能（随机分类器）始终是 $0.5$，无论类别是否不平衡。AUPRC的基线是类别流行率 $\\rho$。对于给定的问题，$\\rho=0.02$。一个AUPRC分数，比如说 $0.6$，是随机性能的 $30$ 倍改进，而一个看起来不错的 $0.8$ 的AUROC可能对应着很差的精确率。AUPRC基线的小数值恰当地反映了任务的难度，使得超越基线的改进更有意义。\n\n总之，对于不平衡数据，AUROC可能会给出误导性的高值，而AUPRC则提供了一个更现实和临床相关的评估，衡量分类器在不被假阳性淹没的情况下识别稀有阳性实例的能力。\n\n**第2部分：AUPRC的计算**\n\n问题指定了一个由（召回率，精确率）平面中的三个点定义的分段线性精确率-召回率曲线。该曲线下的面积可以通过对线段形成的梯形面积求和来计算。\n\n给定的点是：\n- $P_1 = (R_1, P_1) = (0, 1)$\n- $P_2 = (R_2, P_2) = (0.35, 0.75)$\n- $P_3 = (R_3, P_3) = (1, \\rho) = (1, 0.02)$\n\n总面积 (AUPRC) 是两个梯形面积之和。\n\n**梯形1**：由连接 $P_1$ 和 $P_2$ 的线段形成。梯形的“底”沿着召回率轴，从 $R_1=0$ 到 $R_2=0.35$。 “高”是对应的精确率值 $P_1=1$ 和 $P_2=0.75$。\n面积 $A_1$ 由梯形法则给出：\n$$\nA_1 = \\frac{1}{2} (P_1 + P_2) (R_2 - R_1) = \\frac{1}{2} (1 + 0.75) (0.35 - 0)\n$$\n$$\nA_1 = \\frac{1}{2} (1.75) (0.35) = 0.875 \\times 0.35 = 0.30625\n$$\n\n**梯形2**：由连接 $P_2$ 和 $P_3$ 的线段形成。底边沿着召回率轴从 $R_2=0.35$ 到 $R_3=1$。高是 $P_2=0.75$ 和 $P_3=0.02$。\n面积 $A_2$ 是：\n$$\nA_2 = \\frac{1}{2} (P_2 + P_3) (R_3 - R_2) = \\frac{1}{2} (0.75 + 0.02) (1 - 0.35)\n$$\n$$\nA_2 = \\frac{1}{2} (0.77) (0.65) = 0.385 \\times 0.65 = 0.25025\n$$\n\n总AUPRC是这两个面积之和：\n$$\nAUPRC = A_1 + A_2 = 0.30625 + 0.25025 = 0.5565\n$$\n\n问题要求将答案四舍五入到四位有效数字。计算出的值 $0.5565$ 已经有四位有效数字。", "answer": "$$\n\\boxed{0.5565}\n$$", "id": "4553811"}, {"introduction": "在临床诊断等高风险应用中，了解模型对其预测的置信度与预测本身同等重要。蒙特卡洛 Dropout（Monte Carlo dropout）提供了一种实用的方法来近似贝叶斯推断，使我们能够估算模型的不确定性。这种不确定性可以分解为认知不确定性（模型不确定性）和偶然不确定性（数据不确定性）。这项练习将向你展示如何推导和计算预测不确定性，将一个标准网络转变为一个能够表达“我不知道”的工具。[@problem_id:4553839]", "problem": "您正在使用一个在训练和推理过程中都带有 dropout 的深度神经网络，对单细胞类型的对数转换基因表达进行建模。假设在推理时保持 dropout 可以实现对贝叶斯推理的变分近似，其中网络权重的近似后验是由随机伯努利掩码引起的。设新输入 $x$ 的预测后验由权重 $W$ 上的积分定义为 $p(y \\mid x, \\mathcal{D}) = \\int p(y \\mid x, W) \\, p(W \\mid \\mathcal{D}) \\, dW$，其中 $\\mathcal{D}$ 表示训练数据。\n\n在推理时，您通过执行 $T$ 次带有独立 dropout 掩码的随机前向传播来实现蒙特卡洛 (MC) dropout，对于每次传播 $t \\in \\{1,\\dots,T\\}$，获得一对 $(\\mu_t, \\sigma_t^{2})$，代表高斯似然 $y \\mid x, W^{(t)} \\sim \\mathcal{N}(\\mu_t, \\sigma_t^{2})$ 的条件均值和任意方差。您可以假设前向传播是从权重的变分后验中进行的独立同分布抽样。任务是从预测后验的定义以及全期望和全方差的标准定律出发，推导出该设置所隐含的预测均值和预测方差的蒙特卡洛估计量。然后，使用以下 $T$ 次实现来计算预测均值和预测方差：\n- $T = 5$,\n- $(\\mu_1, \\sigma_1^{2}) = (1.10, 0.09)$,\n- $(\\mu_2, \\sigma_2^{2}) = (0.95, 0.16)$,\n- $(\\mu_3, \\sigma_3^{2}) = (1.20, 0.04)$,\n- $(\\mu_4, \\sigma_4^{2}) = (1.05, 0.09)$,\n- $(\\mu_5, \\sigma_5^{2}) = (1.15, 0.04)$.\n\n按照预测均值、预测方差的顺序，将您的最终数值结果表示为两个量。将您的数值答案四舍五入到四位有效数字。将最终答案表示为一个行矩阵。不需要单位。", "solution": "该问题要求在一个使用 MC dropout 的贝叶斯深度学习框架中，推导预测均值和预测方差的蒙特卡洛估计量，然后使用所提供的数据进行数值计算。推导过程依赖于全期望定律和全方差定律。\n\n**1. 估计量的推导**\n\n设 $y$ 是模型对新输入 $x$ 的预测的随机变量，设 $W$ 是模型权重的随机变量，从近似后验分布 $p(W \\mid \\mathcal{D})$ 中抽取。问题将给定权重集 $W$ 的似然定义为高斯分布：\n$$y \\mid x, W \\sim \\mathcal{N}(\\mu(W), \\sigma^2(W))$$\n在这里，$\\mu(W)$ 是条件均值，$\\sigma^2(W)$ 是条件方差，也称为任意不确定性。蒙特卡洛 (MC) dropout 过程从近似后验中生成 $T$ 个样本 $\\{W^{(t)}\\}_{t=1}^T$，产生一组独立同分布的配对 $\\{(\\mu_t, \\sigma_t^2)\\}_{t=1}^T$，其中 $\\mu_t = \\mu(W^{(t)})$ 且 $\\sigma_t^2 = \\sigma^2(W^{(t)})$。\n\n**1.1. 预测均值**\n\n预测均值 $\\mu_{pred}$ 是 $y$ 在预测后验分布 $p(y \\mid x, \\mathcal{D})$ 上的期望。使用全期望定律（或迭代期望定律），我们可以将其写为关于权重 $W$ 分布的期望：\n$$\\mu_{pred} = E[y \\mid x, \\mathcal{D}] = E_{W \\sim p(W \\mid \\mathcal{D})} \\left[ E[y \\mid x, W] \\right]$$\n根据我们高斯似然的定义，内部期望是高斯分布的均值，$E[y \\mid x, W] = \\mu(W)$。因此，预测均值是模型均值输出在权重分布上的期望：\n$$\\mu_{pred} = E_{W \\sim p(W \\mid \\mathcal{D})} [\\mu(W)]$$\n这个期望的蒙特卡洛估计量，记为 $\\hat{\\mu}_{pred}$，是从 $T$ 次随机前向传播中获得的均值 $\\mu_t$ 的样本平均值：\n$$\\hat{\\mu}_{pred} = \\frac{1}{T} \\sum_{t=1}^{T} \\mu_t$$\n\n**1.2. 预测方差**\n\n预测方差 $\\sigma_{pred}^2$ 是 $y$ 在预测后验上的方差。我们使用全方差定律：\n$$Var(y \\mid x, \\mathcal{D}) = E_{W \\sim p(W \\mid \\mathcal{D})} [Var(y \\mid x, W)] + Var_{W \\sim p(W \\mid \\mathcal{D})} [E[y \\mid x, W]]$$\n从我们的高斯似然中，我们确定以下各项：\n- 内部方差是任意方差：$Var(y \\mid x, W) = \\sigma^2(W)$。\n- 内部期望是条件均值：$E[y \\mid x, W] = \\mu(W)$。\n\n将这些代入全方差定律，得到预测方差的分解：\n$$\\sigma_{pred}^2 = \\underbrace{E_{W \\sim p(W \\mid \\mathcal{D})} [\\sigma^2(W)]}_{\\text{期望的任意不确定性}} + \\underbrace{Var_{W \\sim p(W \\mid \\mathcal{D})} [\\mu(W)]}_{\\text{认知不确定性}}$$\n预测方差是两个分量的和：\n1.  **任意不确定性**：模型在不同权重配置下预测的方差的平均值。这代表了数据中模型无法减少的固有噪声。\n2.  **认知不确定性**：模型均值预测的方差。这代表了模型自身对其参数 $W$ 的不确定性，可以通过更多数据来减少。\n\n$\\sigma_{pred}^2$ 的蒙特卡洛估计量，记为 $\\hat{\\sigma}_{pred}^2$，是通过用各自的样本估计来近似每一项得到的：\n- 期望的任意不确定性的估计量是方差 $\\sigma_t^2$ 的样本均值：\n$$ \\hat{E}[\\sigma^2(W)] = \\frac{1}{T} \\sum_{t=1}^{T} \\sigma_t^2 $$\n- 认知不确定性的估计量是均值 $\\mu_t$ 的样本方差：\n$$ \\widehat{Var}[\\mu(W)] = \\frac{1}{T} \\sum_{t=1}^{T} (\\mu_t - \\hat{\\mu}_{pred})^2 $$\n结合这些，预测方差的蒙特卡洛估计量是：\n$$ \\hat{\\sigma}_{pred}^2 = \\left( \\frac{1}{T} \\sum_{t=1}^{T} \\sigma_t^2 \\right) + \\left( \\frac{1}{T} \\sum_{t=1}^{T} \\mu_t^2 - \\hat{\\mu}_{pred}^2 \\right) $$\n第二项是样本方差的代数等价形式。表示总方差估计量的另一种方式是：\n$$ \\hat{\\sigma}_{pred}^2 = \\frac{1}{T} \\sum_{t=1}^{T} (\\mu_t^2 + \\sigma_t^2) - \\hat{\\mu}_{pred}^2 $$\n\n**2. 数值计算**\n\n所提供的数据是：\n- $T = 5$\n- $(\\mu_t, \\sigma_t^2)$ 的实现值：\n  - $t=1$: $(\\mu_1, \\sigma_1^2) = (1.10, 0.09)$\n  - $t=2$: $(\\mu_2, \\sigma_2^2) = (0.95, 0.16)$\n  - $t=3$: $(\\mu_3, \\sigma_3^2) = (1.20, 0.04)$\n  - $t=4$: $(\\mu_4, \\sigma_4^2) = (1.05, 0.09)$\n  - $t=5$: $(\\mu_5, \\sigma_5^2) = (1.15, 0.04)$\n\n**2.1. 计算预测均值**\n使用推导出的估计量：\n$$ \\hat{\\mu}_{pred} = \\frac{1}{5} \\sum_{t=1}^{5} \\mu_t = \\frac{1}{5} (1.10 + 0.95 + 1.20 + 1.05 + 1.15) $$\n$$ \\hat{\\mu}_{pred} = \\frac{1}{5} (5.45) = 1.09 $$\n\n**2.2. 计算预测方差**\n我们将分别计算方差的两个分量。\n\n首先，平均任意不确定性：\n$$ \\hat{E}[\\sigma^2(W)] = \\frac{1}{5} \\sum_{t=1}^{5} \\sigma_t^2 = \\frac{1}{5} (0.09 + 0.16 + 0.04 + 0.09 + 0.04) $$\n$$ \\hat{E}[\\sigma^2(W)] = \\frac{1}{5} (0.42) = 0.084 $$\n\n其次，认知不确定性（均值的样本方差）：\n$$ \\widehat{Var}[\\mu(W)] = \\frac{1}{5} \\sum_{t=1}^{5} (\\mu_t - \\hat{\\mu}_{pred})^2 $$\n$$ \\widehat{Var}[\\mu(W)] = \\frac{1}{5} \\left( (1.10 - 1.09)^2 + (0.95 - 1.09)^2 + (1.20 - 1.09)^2 + (1.05 - 1.09)^2 + (1.15 - 1.09)^2 \\right) $$\n$$ \\widehat{Var}[\\mu(W)] = \\frac{1}{5} \\left( (0.01)^2 + (-0.14)^2 + (0.11)^2 + (-0.04)^2 + (0.06)^2 \\right) $$\n$$ \\widehat{Var}[\\mu(W)] = \\frac{1}{5} (0.0001 + 0.0196 + 0.0121 + 0.0016 + 0.0036) $$\n$$ \\widehat{Var}[\\mu(W)] = \\frac{1}{5} (0.037) = 0.0074 $$\n\n总预测方差是这两个分量之和：\n$$ \\hat{\\sigma}_{pred}^2 = \\hat{E}[\\sigma^2(W)] + \\widehat{Var}[\\mu(W)] = 0.084 + 0.0074 = 0.0914 $$\n\n**3. 最终结果**\n\n计算出的预测均值为 $1.09$，预测方差为 $0.0914$。按要求四舍五入到四位有效数字：\n- 预测均值: $1.090$\n- 预测方差: $0.09140$", "answer": "$$\\boxed{\\begin{pmatrix} 1.090  0.09140 \\end{pmatrix}}$$", "id": "4553839"}]}