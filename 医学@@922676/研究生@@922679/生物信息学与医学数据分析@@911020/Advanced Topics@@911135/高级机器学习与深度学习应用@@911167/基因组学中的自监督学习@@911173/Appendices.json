{"hands_on_practices": [{"introduction": "在基因组学中，一个关键的挑战是确保计算模型能够遵循基本的生物学对称性。由于双链DNA在正向和反向互补链上编码相同的信息，一个稳健的机器学习表示应该对反向互补变换保持不变。这项实践练习 [@problem_id:4606981] 将引导您将这一原则形式化为一个可量化的度量标准，这是验证基因组模型有效性的关键技能。", "problem": "您正在自监督学习 (SSL) 的设定下，分析脱氧核糖核酸 (DNA) 序列的学习表示。在双链DNA中，碱基配对是明确定义的：Watson–Crick 互补映射为 $A \\leftrightarrow T$ 和 $C \\leftrightarrow G$。给定一个有限字母表 $\\Sigma = \\{A,C,G,T\\}$，定义互补算子 $c:\\Sigma \\to \\Sigma$ 为 $c(A)=T$、$c(T)=A$、$c(C)=G$ 和 $c(G)=C$。对于一个序列 $s = (s_1,s_2,\\dots,s_L) \\in \\Sigma^L$，定义反向互补算子 $R$ 为 $R(s) = (c(s_L), c(s_{L-1}), \\dots, c(s_1))$。在许多基因组学任务中，一个学习到的嵌入函数 $f:\\Sigma^* \\to \\mathbb{R}^d$ 应当具有反向互补不变性，即对于内容等价的序列，无论其方向如何，都应有 $f(s) = f(R(s))$。您必须提出一种能够定量验证反向互补不变性的测试，并设计一种度量标准来惩罚序列翻转下的不对称性。\n\n从双链DNA通过碱基互补性在两个方向上编码相同生物信息（互补碱基对为 $A \\leftrightarrow T$ 和 $C \\leftrightarrow G$）这一基本点出发，推导出一个有原则的算法测试和一个适用于通过自监督目标学习到的 $\\mathbb{R}^d$ 嵌入的定量度量。您的度量必须对任何有限集 $S \\subset \\Sigma^*$ 和任何嵌入函数 $f$ 满足以下性质：\n- 非负性：$M(f,S) \\ge 0$。\n- 关于反向互补不变性的不可辨识者同一性：$M(f,S) = 0$ 当且仅当对于所有 $s \\in S$ 都有 $f(s) = f(R(s))$。\n- 尺度不变性：对于任何标量 $\\alpha > 0$，用 $\\alpha f$ 替换 $f$ 不会改变度量值。\n- 能够稳健地聚合不同长度的序列，除了度量设计本身所隐含的缩放外，无需对每个输入序列进行显式重新缩放。\n\n您还必须定义一个决策规则，在给定一个非负阈值 $\\tau$ 的情况下，判定嵌入函数 $f$ 在集合 $S$ 上是否具有反向互补不变性。\n\n为以下测试套件实现您的度量和测试。每个测试用例由一个特定的嵌入函数、一个序列集和一个阈值组成。嵌入函数在数学上定义如下；令 $L$ 表示序列长度，位置索引为 $p=1,\\dots,L$：\n1. 嵌入函数 $f_{\\mathrm{inv}}:\\Sigma^* \\to \\mathbb{R}^2$（设计为具有反向互补不变性）：\n   - 定义 $\\phi_{\\mathrm{inv}}:\\Sigma \\to \\mathbb{R}^2$ 为 $\\phi_{\\mathrm{inv}}(A)=(1,0)$、$\\phi_{\\mathrm{inv}}(T)=(1,0)$、$\\phi_{\\mathrm{inv}}(C)=(0,1)$ 和 $\\phi_{\\mathrm{inv}}(G)=(0,1)$。\n   - 对于 $s=(s_1,\\dots,s_L)$，定义 $f_{\\mathrm{inv}}(s) = \\frac{1}{L}\\sum_{p=1}^{L} \\phi_{\\mathrm{inv}}(s_p)$。\n2. 嵌入函数 $f_{\\mathrm{sens}}:\\Sigma^* \\to \\mathbb{R}^3$（设计为对方向敏感）：\n   - 定义 $\\phi_{\\mathrm{sens}}:\\Sigma \\to \\mathbb{R}^3$ 为 $\\phi_{\\mathrm{sens}}(A)=(1.0,0.5,0.2)$、$\\phi_{\\mathrm{sens}}(C)=(0.1,1.0,0.3)$、$\\phi_{\\mathrm{sens}}(G)=(0.3,0.2,1.0)$ 和 $\\phi_{\\mathrm{sens}}(T)=(1.2,0.8,0.4)$。\n   - 定义位置依赖权重 $w(p)=1+0.4(p-1)$ 和位置编码向量 $\\psi(p,L)=(0.05p,\\,0.02(L+1-p),\\,0.03)$。\n   - 对于 $s=(s_1,\\dots,s_L)$，定义 $f_{\\mathrm{sens}}(s) = \\frac{1}{L}\\sum_{p=1}^{L} \\big(w(p)\\,\\phi_{\\mathrm{sens}}(s_p) + \\psi(p,L)\\big)$。\n3. 嵌入函数 $f_{\\mathrm{part}}:\\Sigma^* \\to \\mathbb{R}^2$（设计为部分不变，但有轻微的方向敏感性）：\n   - 定义 $\\phi_{\\mathrm{part}}:\\Sigma \\to \\mathbb{R}^2$ 为 $\\phi_{\\mathrm{part}}(A)=(1,0)$、$\\phi_{\\mathrm{part}}(T)=(1,0)$、$\\phi_{\\mathrm{part}}(C)=(0,1)$ 和 $\\phi_{\\mathrm{part}}(G)=(0,1)$。\n   - 定义标量系数 $c:\\Sigma \\to \\mathbb{R}$ 为 $c(A)=1.0$、$c(T)=-1.0$、$c(C)=0.5$ 和 $c(G)=-0.5$，并令 $\\varepsilon=0.02$。\n   - 对于 $s=(s_1,\\dots,s_L)$，定义 $g(s)=\\sum_{p=1}^{L} c(s_p)\\,\\frac{p}{L}$ 和 $v_{\\mathrm{noise}}(s)=\\varepsilon\\,\\frac{g(s)}{L}\\,(1,-1)$，然后定义 $f_{\\mathrm{part}}(s)=\\frac{1}{L}\\sum_{p=1}^{L}\\phi_{\\mathrm{part}}(s_p) + v_{\\mathrm{noise}}(s)$。\n\n使用以下序列集：\n- $S_{\\mathrm{gen}} = \\{\\text{\"ACGTAC\"}, \\text{\"TTGCA\"}, \\text{\"CGAT\"}, \\text{\"GATTACA\"}, \\text{\"CCGGTTAA\"}\\}$。\n- $S_{\\mathrm{pal}} = \\{\\text{\"AT\"}, \\text{\"GC\"}, \\text{\"AGCT\"}, \\text{\"AATT\"}, \\text{\"CGCG\"}\\}$。\n- $S_{\\mathrm{sing}} = \\{\\text{\"A\"}, \\text{\"C\"}, \\text{\"G\"}, \\text{\"T\"}\\}$。\n- $S_{\\mathrm{long}} = \\{\\text{\"ACGTACGTACGTACGTACGT\"}\\}$。\n\n按以下顺序将测试用例定义为有序三元组 $(f,S,\\tau)$：\n- 用例 1: $(f_{\\mathrm{inv}}, S_{\\mathrm{gen}}, 10^{-12})$。\n- 用例 2: $(f_{\\mathrm{sens}}, S_{\\mathrm{gen}}, 10^{-1})$。\n- 用例 3: $(f_{\\mathrm{part}}, S_{\\mathrm{long}}, 5\\times 10^{-2})$。\n- 用例 4: $(f_{\\mathrm{inv}}, S_{\\mathrm{pal}}, 10^{-12})$。\n- 用例 5: $(f_{\\mathrm{sens}}, S_{\\mathrm{sing}}, 2\\times 10^{-1})$。\n\n您的程序必须：\n- 实现一个与上述定义一致的反向互补算子 $R$。\n- 精确实现指定的三种嵌入函数。\n- 设计并计算一个满足上述四个性质的定量不对称性度量 $M(f,S)$。\n- 实现一个决策规则，该规则返回一个布尔值，指示是否 $M(f,S) \\le \\tau$。\n- 按指定顺序生成五个用例的结果。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。每个结果必须是一个包含两个项目的列表，其中包含度量值（四舍五入到六位小数）和布尔决策，例如 $[m,\\mathrm{True}]$ 或 $[m,\\mathrm{False}]$。因此，最终输出行必须具有形式 $[[m_1,d_1],[m_2,d_2],[m_3,d_3],[m_4,d_4],[m_5,d_5]]$，其中每个 $m_i$ 是一个四舍五入到六位小数的浮点数，每个 $d_i$ 是一个布尔值。", "solution": "该问题要求设计一种定量测试，以验证为脱氧核糖核酸 (DNA) 序列学习到的嵌入函数是否表现出反向互补不变性。这是基因组学中的一个关键属性，因为双链DNA分子在其正向链和反向互补链中携带相同的生物信息。我们必须为一个嵌入函数 $f$ 在序列集合 $S$ 上推导出一个有原则的度量 $M(f,S)$，以及一个相关的决策规则。\n\n对于嵌入函数 $f:\\Sigma^* \\to \\mathbb{R}^d$，反向互补不变性的基本原则是，对于任何序列 $s \\in \\Sigma^*$，其嵌入应与其反向互补序列（表示为 $R(s)$）的嵌入相同。在数学上，这表示为等式 $f(s) = f(R(s))$。任务是量化与此等式的偏差。\n\n我们的第一步是为单个序列 $s$ 定义一个不对称性的度量。一个自然的起点是向量差 $v_{\\mathrm{diff}}(s) = f(s) - f(R(s))$。在不变的嵌入中，$v_{\\mathrm{diff}}(s) = \\vec{0}$。这个差的大小可以通过向量范数来衡量，例如欧几里得范数 $\\|v_{\\mathrm{diff}}(s)\\|_2 = \\|f(s) - f(R(s))\\|_2$。这个量是非负的，并且当且仅当 $f(s) = f(R(s))$ 时为零，满足了对单个序列的两个所需属性。\n\n然而，问题要求度量是尺度不变的。如果我们将嵌入函数按因子 $\\alpha > 0$ 缩放，得到新函数 $f' = \\alpha f$，则差的范数变为 $\\|\\alpha f(s) - \\alpha f(R(s))\\|_2 = \\alpha \\|f(s) - f(R(s))\\|_2$。这不是尺度不变的。为了解决这个问题，我们必须对差异进行归一化。创建两个向量 $u$ 和 $v$ 之间尺度不变的相对差异度量的一种标准方法，是用它们的范数之和来归一化它们差的范数。这引出了我们对单个序列不对称性度量 $m(s, f)$ 的定义：\n$$\nm(s, f) = \\frac{\\|f(s) - f(R(s))\\|_2}{\\|f(s)\\|_2 + \\|f(R(s))\\|_2}\n$$\n在 $f(s)$ 和 $f(R(s))$ 都是零向量的特殊情况下，它们是相等的，表达式变为 $\\frac{0}{0}$。在这种情况下，我们定义 $m(s, f) = 0$，这与完全不变性的条件一致。\n\n这个单序列度量 $m(s, f)$ 满足对单个序列所要求的属性：\n1.  **非负性**：由于范数是非负的，因此 $m(s, f) \\ge 0$。\n2.  **不可辨识者同一性**：$m(s,f) = 0$ 当且仅当 $\\|f(s) - f(R(s))\\|_2 = 0$，这当且仅当 $f(s) = f(R(s))$ 时成立（假设分母不为零，或按定义处理零向量情况）。\n3.  **尺度不变性**：对于 $\\alpha > 0$ 和 $f' = \\alpha f$，\n    $$\n    m(s, f') = \\frac{\\|\\alpha f(s) - \\alpha f(R(s))\\|_2}{\\|\\alpha f(s)\\|_2 + \\|\\alpha f(R(s))\\|_2} = \\frac{\\alpha \\|f(s) - f(R(s))\\|_2}{\\alpha \\left( \\|f(s)\\|_2 + \\|f(R(s))\\|_2 \\right)} = m(s, f)\n    $$\n    该属性成立。\n\n接下来，我们必须将这些单序列分数在一个有限序列集 $S \\subset \\Sigma^*$ 上进行聚合，以形成最终的度量 $M(f,S)$。问题要求在不同长度的序列间进行稳健的聚合。对归一化后的单序列分数取平均是一种合适的方法，因为每个分数 $m(s,f)$ 都已经是介于0和1之间的无量纲量（与余弦距离相关）。这可以防止产生大范数嵌入的序列对聚合分数产生不成比例的影响。因此，我们将聚合度量定义为单序列分数的算术平均值：\n$$\nM(f,S) = \\frac{1}{|S|} \\sum_{s \\in S} m(s, f) = \\frac{1}{|S|} \\sum_{s \\in S} \\frac{\\|f(s) - f(R(s))\\|_2}{\\|f(s)\\|_2 + \\|f(R(s))\\|_2}\n$$\n这个聚合度量 $M(f,S)$ 继承了所需的属性：它非负、尺度不变，并且当且仅当对所有 $s \\in S$ 都有 $f(s) = f(R(s))$ 时等于零。通过平均进行聚合提供了所要求的稳健性。\n\n最后，我们定义算法测试和决策规则。该测试包括为给定的函数 $f$ 和序列集 $S$ 计算 $M(f,S)$ 的值。给定一个非负阈值 $\\tau$，决策规则是：\n- 如果 $M(f,S) \\le \\tau$，则声明嵌入函数 $f$ 在集合 $S$ 上是**反向互补不变的**。\n- 否则，声明它在 $S$ 上**不是反向互补不变的**。\n\n这个框架为评估DNA序列嵌入的反向互补不变性提供了一个完整的、有原则的、定量的方法，正如问题所规定的那样。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the analysis, and print results.\n    \"\"\"\n    \n    # ------------------ Operators and Helper Maps ------------------\n\n    def reverse_complement(s: str) -> str:\n        \"\"\"Computes the reverse complement of a DNA sequence.\"\"\"\n        complement_map = str.maketrans('ACGT', 'TGCA')\n        return s.translate(complement_map)[::-1]\n\n    # Maps for embedding function definitions\n    PHI_INV_MAP = {\n        'A': np.array([1.0, 0.0]), 'T': np.array([1.0, 0.0]),\n        'C': np.array([0.0, 1.0]), 'G': np.array([0.0, 1.0]),\n    }\n    PHI_SENS_MAP = {\n        'A': np.array([1.0, 0.5, 0.2]), 'C': np.array([0.1, 1.0, 0.3]),\n        'G': np.array([0.3, 0.2, 1.0]), 'T': np.array([1.2, 0.8, 0.4]),\n    }\n    PHI_PART_MAP = PHI_INV_MAP # Base is the same as f_inv\n    COEFF_PART_MAP = {'A': 1.0, 'T': -1.0, 'C': 0.5, 'G': -0.5}\n\n    # ------------------ Embedding Functions ------------------\n\n    def f_inv(s: str) -> np.ndarray:\n        \"\"\"Invariant embedding function.\"\"\"\n        L = len(s)\n        if L == 0:\n            return np.zeros(2)\n        \n        total = np.zeros(2)\n        for base in s:\n            total += PHI_INV_MAP[base]\n        \n        return total / L\n\n    def f_sens(s: str) -> np.ndarray:\n        \"\"\"Orientation-sensitive embedding function.\"\"\"\n        L = len(s)\n        if L == 0:\n            return np.zeros(3)\n\n        total = np.zeros(3)\n        for p_one_based in range(1, L + 1):\n            idx = p_one_based - 1\n            base = s[idx]\n            \n            w_p = 1.0 + 0.4 * (p_one_based - 1)\n            psi_p = np.array([0.05 * p_one_based, 0.02 * (L + 1 - p_one_based), 0.03])\n            \n            term = w_p * PHI_SENS_MAP[base] + psi_p\n            total += term\n\n        return total / L\n\n    def f_part(s: str) -> np.ndarray:\n        \"\"\"Partially invariant embedding function.\"\"\"\n        L = len(s)\n        if L == 0:\n            return np.zeros(2)\n\n        # Invariant part\n        f_base_total = np.zeros(2)\n        for base in s:\n            f_base_total += PHI_PART_MAP[base]\n        f_base = f_base_total / L\n\n        # Noise part\n        epsilon = 0.02\n        g_s = 0.0\n        for p_one_based in range(1, L + 1):\n            idx = p_one_based - 1\n            base = s[idx]\n            g_s += COEFF_PART_MAP[base] * (p_one_based / L)\n        \n        v_noise_vec = np.array([1.0, -1.0])\n        v_noise = epsilon * (g_s / L) * v_noise_vec\n        \n        return f_base + v_noise\n\n    # ------------------ Metric and Test Implementation ------------------\n\n    def calculate_metric(f, S: list[str]) -> float:\n        \"\"\"Calculates the asymmetry metric M(f, S).\"\"\"\n        if not S:\n            return 0.0\n        \n        total_metric = 0.0\n        for s in S:\n            rs = reverse_complement(s)\n            \n            # For palindromic sequences, s == rs, metric contribution is 0\n            if s == rs:\n                continue\n\n            v_s = f(s)\n            v_rs = f(rs)\n            \n            norm_s = np.linalg.norm(v_s)\n            norm_rs = np.linalg.norm(v_rs)\n            \n            denominator = norm_s + norm_rs\n            \n            if denominator == 0.0:\n                # If both norms are 0, vectors are equal (both zero), so diff is 0\n                seq_metric = 0.0\n            else:\n                diff_norm = np.linalg.norm(v_s - v_rs)\n                seq_metric = diff_norm / denominator\n            \n            total_metric += seq_metric\n            \n        return total_metric / len(S)\n\n    # ------------------ Test Case Setup and Execution ------------------\n\n    # Sequence Sets\n    S_gen = [\"ACGTAC\", \"TTGCA\", \"CGAT\", \"GATTACA\", \"CCGGTTAA\"]\n    S_pal = [\"AT\", \"GC\", \"AGCT\", \"AATT\", \"CGCG\"]\n    S_sing = [\"A\", \"C\", \"G\", \"T\"]\n    S_long = [\"ACGTACGTACGTACGTACGT\"]\n\n    # Embedding function map\n    func_map = {\n        'f_inv': f_inv,\n        'f_sens': f_sens,\n        'f_part': f_part,\n    }\n    \n    # Sequence set map\n    set_map = {\n        'S_gen': S_gen,\n        'S_pal': S_pal,\n        'S_sing': S_sing,\n        'S_long': S_long,\n    }\n\n    # Test Cases: (function_name, set_name, threshold)\n    test_cases = [\n        ('f_inv', 'S_gen', 1e-12),\n        ('f_sens', 'S_gen', 1e-1),\n        ('f_part', 'S_long', 5e-2),\n        ('f_inv', 'S_pal', 1e-12),\n        ('f_sens', 'S_sing', 2e-1),\n    ]\n\n    results = []\n    for f_name, s_name, tau in test_cases:\n        f = func_map[f_name]\n        S = set_map[s_name]\n        \n        metric_value = calculate_metric(f, S)\n        decision = metric_value = tau\n        \n        rounded_metric = round(metric_value, 6)\n        results.append(f\"[{rounded_metric},{decision}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "4606981"}, {"introduction": "将语言模型应用于基因组学时，分词（Tokenization）是一个至关重要的预处理步骤，它将原始的DNA字符序列转换为模型可以处理的单元。选择正确的分词策略会显著影响模型的性能及其捕捉有意义生物单元（如基序）的能力。这项练习 [@problem_id:4606969] 提供了实现和比较两种流行的子词（subword）分词算法——字节对编码（BPE）和一元语言模型（ULM）的实践经验，以理解它们在压缩效率和与基因组重复元件对齐方面的表现。", "problem": "给定一组脱氧核糖核酸 (DNA) 序列，要求您评估字节对编码 (BPE) 和 Unigram语言模型 (ULM) 作为基于分词的压缩器在基因组学自监督学习环境中的适用性。利用基本编码理论，将压缩定义为由概率分词器产生的期望编码长度，并量化重复区域附近的分词错误。您需要从头开始实现这两种方法，并在一套测试用例上计算比较指标。\n\n从以下基本原理开始：\n- 香农信源编码原理指出，对于一个在标记 (token) $w$ 上的离散概率质量函数 $p(w)$，一个标记 $w$ 的最优期望编码长度（以比特为单位）由 $-\\log_2 p(w)$ 给出，而一个被分割成标记序列 $\\{w_1, w_2, \\dots, w_T\\}$ 的序列的总编码长度为 $$L = \\sum_{t=1}^{T} -\\log_2 p(w_t).$$\n- 对于长度为 $N$ 的DNA序列，每个核苷酸的平均编码长度为 $$\\bar{\\ell} = \\frac{L}{N}.$$\n- 重复区域由一个连续重复的固定长度的基序 (motif) 定义。设 $s$ 为该区域在序列中的起始索引（从零开始），$m$ 为基序长度，$r$ 为基序重复次数，因此该区域跨越的索引范围为 $[s, s + r \\cdot m)$。\n\n您必须实现以下方法：\n1. 字节对编码 (BPE)，其定义为从字符级词汇表 $\\{A, C, G, T\\}$ 开始，迭代地合并最频繁的相邻标记对以形成新的标记，直到达到指定的词汇表大小 $V_{\\mathrm{BPE}}$。在每个合并步骤中，通过选择表示为连接 $(\\text{left} + \\text{right})$ 时字典序最小的对来解决平局问题。通过对训练序列重复应用学习到的合并规则来获得训练分词结果。使用加性平滑从训练分词结果中的标记频率估计概率 $$p_{\\mathrm{BPE}}(w) = \\frac{c(w) + \\lambda}{\\sum_{u} c(u) + \\lambda \\cdot |V|},$$ 其中 $c(w)$ 是标记 $w$ 的计数，$|V|$ 是最终词汇表的大小，$\\lambda$ 是平滑参数。通过按顺序应用相同的合并操作来对测试序列进行分词。\n\n2. Unigram语言模型 (ULM) 分词：构建一个候选词汇表 $V_{\\mathrm{ULM}}$，其中包含训练序列中所有长度从 $1$ 到 $L_{\\max}$ 的子串，并计入重叠部分。使用加性平滑估计概率 $$p_{\\mathrm{ULM}}(w) = \\frac{c(w) + \\alpha}{\\sum_{u \\in V_{\\mathrm{ULM}}} c(u) + \\alpha \\cdot |V_{\\mathrm{ULM}}|},$$ 其中 $\\alpha$ 是平滑参数。通过动态规划对测试序列进行分词，以找到使总负对数概率最小化的分词方式 $$\\min_{\\text{segmentations}} \\sum_{t} -\\log_2 p_{\\mathrm{ULM}}(w_t),$$ 约束条件是每个标记 $w_t \\in V_{\\mathrm{ULM}}$ 且标记长度最多为 $L_{\\max}$。\n\n定义并计算重复区域附近的以下误差指标：\n- 设内部基序边界的集合为 $$B = \\{s + m, s + 2m, \\dots, s + (r - 1)m\\},$$ 对于 $r \\geq 2$，其基数为 $|B| = r - 1$。设 $T$ 为该区域内的标记边界位置集合，其中每个边界是该区域内第一个标记之后的任何标记的起始索引。将边界错位误差定义为 $$e = \\begin{cases}1 - \\frac{|B \\cap T|}{|B|},  \\text{if } |B|  0, \\\\ 0,  \\text{if } |B| = 0. \\end{cases}$$ 该指标衡量未被标记边界捕获的基序边界的比例。\n\n对于每个测试用例，计算：\n- BPE的每个核苷酸平均比特数 $\\bar{\\ell}_{\\mathrm{BPE}}$。\n- ULM的每个核苷酸平均比特数 $\\bar{\\ell}_{\\mathrm{ULM}}$。\n- 边界错位误差 $e_{\\mathrm{BPE}}$。\n- 边界错位误差 $e_{\\mathrm{ULM}}$。\n\n对于两种方法，均使用给定的完整序列作为训练序列和测试序列。所有对数必须以 $2$ 为底。对于概率估计，使用指定的 $\\lambda$ 和 $\\alpha$ 值进行加性平滑。\n\n实现程序以解决以下测试用例集，其中索引从零开始，序列已明确提供：\n\n- 测试用例 1 (正常路径): 序列 $S_1 =$ \"ACGTAC\" + \"ATG\" 重复 $10$ 次 + \"GGAAC\"。参数: $V_{\\mathrm{BPE}} = 12$, $L_{\\max} = 5$, $\\lambda = 1$, $\\alpha = 1$; 重复区域起始位置 $s = 6$，基序长度 $m = 3$，重复次数 $r = 10$。\n\n- 测试用例 2 (边界，均聚物): 序列 $S_2 =$ \"CG\" + \"A\" 重复 $20$ 次 + \"TGC\"。参数: $V_{\\mathrm{BPE}} = 8$, $L_{\\max} = 10$, $\\lambda = 1$, $\\alpha = 0.5$; 重复区域起始位置 $s = 2$，基序长度 $m = 1$，重复次数 $r = 20$。\n\n- 测试用例 3 (边缘，稀疏重复): 序列 $S_3 =$ \"GCTATCGAGT\" + \"CGT\" 重复 $3$ 次 + \"ACGTA\"。参数: $V_{\\mathrm{BPE}} = 6$, $L_{\\max} = 4$, $\\lambda = 1$, $\\alpha = 1$; 重复区域起始位置 $s = 10$，基序长度 $m = 3$，重复次数 $r = 3$。\n\n- 测试用例 4 (混合富含GC的重复): 序列 $S_4 =$ \"AT\" + \"GC\" 重复 $12$ 次 + \"TAAC\"。参数: $V_{\\mathrm{BPE}} = 10$, $L_{\\max} = 4$, $\\lambda = 1$, $\\alpha = 0.1$; 重复区域起始位置 $s = 2$，基序长度 $m = 2$，重复次数 $r = 12$。\n\n您的程序必须：\n- 完全按照描述实现两种分词器。\n- 为每个测试用例计算四个量 $(\\bar{\\ell}_{\\mathrm{BPE}}, \\bar{\\ell}_{\\mathrm{ULM}}, e_{\\mathrm{BPE}}, e_{\\mathrm{ULM}})$。\n- 生成单行输出，其中包含用方括号括起来的逗号分隔列表的结果，每个测试用例表示为一个四元组列表，顺序为 $[\\bar{\\ell}_{\\mathrm{BPE}}, \\bar{\\ell}_{\\mathrm{ULM}}, e_{\\mathrm{BPE}}, e_{\\mathrm{ULM}}]$。\n- 将每个测试用例的所有四个值表示为保留 $6$ 位小数的十进制数。例如，最终输出必须如下所示：$$[[x_1,y_1,z_1,w_1],[x_2,y_2,z_2,w_2],[x_3,y_3,z_3,w_3],[x_4,y_4,z_4,w_4]].$$", "solution": "该问题要求对字节对编码 (BPE) 和 Unigram语言模型 (ULM) 这两种分词算法进行比较分析，以评估它们在压缩和分割基因组序列，特别是重复区域周围序列时的效能。此分析将通过从头实现这两种算法，并使用源于信息论和序列分析的指标，在一组给定的DNA序列上对它们进行评估来完成。\n\n### 基本原理\n\n压缩指标的核心是香农信源编码定理。对于一组具有概率分布 $p(w)$ 的标记 $W$，一个标记 $w$ 的理论最优编码长度由其自信息 $I(w) = -\\log_2 p(w)$ 给出。对于一个标记序列 $w_1, w_2, \\dots, w_T$，总编码长度是各个编码长度之和：\n$$L = \\sum_{t=1}^{T} -\\log_2 p(w_t)$$\n将此值除以原始DNA序列中的核苷酸总数 $N$，得到每个核苷酸的平均比特数 $\\bar{\\ell} = L/N$，这是衡量压缩效率的指标。值越低表示压缩效果越好。\n\n第二个指标，边界错位误差 $e$，量化了标记边界与重复基序的自然边界的对齐程度。对于一个从索引 $s$ 开始、长度为 $m$ 的基序重复 $r$ 次的区域，其内部基序边界位于位置 $B = \\{s+m, s+2m, \\dots, s+(r-1)m\\}$。给定一个在该区域内产生标记边界 $T$ 的分词结果，误差是分词器错过的基序边界的比例：\n$$e = 1 - \\frac{|B \\cap T|}{|B|}$$\n（对于 $|B|  0$）。误差 $e=0$ 表示完美对齐。\n\n### 方法1：字节对编码 (BPE)\n\nBPE 是一种贪婪数据压缩算法，它迭代地将最频繁的相邻符号（标记）对替换为一个新符号。\n\n**1. 训练与分词：**\n该过程始于将DNA序列表示为其构成核苷酸（字符）的序列，这些核苷酸构成了初始词汇表 $V = \\{'A', 'C', 'G', 'T'\\}$。然后，算法执行指定数量的合并操作 ($V_{\\mathrm{BPE}} - |V_{\\text{initial}}|$)。在每个步骤中：\na. 统计当前序列表示中所有相邻的标记对。\nb. 选择频率最高的对进行合并。频率相同时，通过选择连接字符串 $t_1+t_2$ 字典序最小的对 $(t_1, t_2)$ 来打破平局。\nc. 创建一个代表该对的新标记，并将其添加到词汇表中。\nd. 序列中所有出现的该选定对都被新标记替换。这种贪婪替换从左到右执行。\n\n由于问题规定训练序列和测试序列相同，训练过程产生的最终标记序列将同时用于概率估计和指标计算的分词结果。\n\n**2. 概率估计与编码长度：**\n在获得最终分词结果 $\\{w_1, w_2, \\dots, w_T\\}$ 后，确定词汇表中每个唯一标记 $w$ 的计数 $c(w)$。使用带有参数 $\\lambda$ 的加性（拉普拉斯）平滑来估计概率 $p_{\\mathrm{BPE}}(w)$：\n$$p_{\\mathrm{BPE}}(w) = \\frac{c(w) + \\lambda}{\\sum_{u \\in V} c(u) + \\lambda \\cdot |V|}$$\n其中 $|V| = V_{\\mathrm{BPE}}$ 是最终词汇表的大小，而总和 $\\sum_{u \\in V} c(u)$ 是最终分词结果中的标记总数。然后总编码长度 $L_{\\mathrm{BPE}}$ 计算为 $\\sum_{t=1}^{T} -\\log_2 p_{\\mathrm{BPE}}(w_t)$。每个核苷酸的平均比特数为 $\\bar{\\ell}_{\\mathrm{BPE}} = L_{\\mathrm{BPE}} / N$。\n\n### 方法2：Unigram语言模型 (ULM)\n\nULM分词将问题框定为寻找序列最可能的分词方式，其中可能性基于预先计算的标记概率模型。\n\n**1. 词汇表与概率估计：**\n首先，构建一个候选词汇表 $V_{\\mathrm{ULM}}$。它包含训练序列中所有长度从 $1$ 到最大值 $L_{\\max}$ 的子串。计算每个子串 $w \\in V_{\\mathrm{ULM}}$ 的频率 $c(w)$（计入重叠部分）。然后使用带有参数 $\\alpha$ 的加性平滑来估计概率 $p_{\\mathrm{ULM}}(w)$：\n$$p_{\\mathrm{ULM}}(w) = \\frac{c(w) + \\alpha}{\\sum_{u \\in V_{\\mathrm{ULM}}} c(u) + \\alpha \\cdot |V_{\\mathrm{ULM}}|}$$\n为计算方便，我们使用负对数概率，它代表每个标记的“成本”：$\\text{cost}(w) = -\\log_2 p_{\\mathrm{ULM}}(w)$。\n\n**2. 通过动态规划进行最优分词：**\n测试序列（与训练序列相同）的最优分词是使总成本最小化的那一种。这是一个有向无环图 (DAG) 上的经典最短路径问题，可以使用动态规划（维特比算法）高效解决。\n设 $dp[i]$ 为分割长度为 $i$ 的序列前缀 $S[0 \\dots i-1]$ 的最小成本。递推关系为：\n$$dp[i] = \\min_{1 \\le j \\le \\min(i, L_{\\max})} \\left( dp[i-j] + \\text{cost}(S[i-j:i]) \\right)$$\n基准情况为 $dp[0] = 0$。整个长度为 $N$ 的序列的最小成本是 $dp[N]$，这正是总编码长度 $L_{\\mathrm{ULM}}$。每个核苷酸的平均比特数为 $\\bar{\\ell}_{\\mathrm{ULM}} = L_{\\mathrm{ULM}} / N$。\n通过在动态规划过程中存储回溯指针（即在每一步 $i$ 中是哪个 $j$ 产生了最小值），可以重构出最优的标记序列。\n\n### 指标计算\n\n对于BPE和ULM，一旦确定了最终的分词结果和相应的起始索引，就计算边界错位误差 $e$。\n\n1.  **确定真实边界 ($B$)**：对于由 $(s, m, r)$ 定义的重复区域，内部基序边界的集合是 $B = \\{s + k \\cdot m \\mid k = 1, 2, \\dots, r-1\\}$。如果 $r  2$，$B$ 为空。\n2.  **确定标记边界 ($T$)**：\n    a. 设完整序列分词结果中标记的起始索引为 $p_1, p_2, \\dots, p_K$。\n    b. 找到与指定重复区域 $[s, s + r \\cdot m)$ 重叠的第一个标记。设该标记在分词结果中的索引为 $j$。\n    c. 后续标记起始索引的集合是 $\\{p_{j+1}, p_{j+2}, \\dots, p_K\\}$。\n    d. 区域内的标记边界集合 $T$ 是由落在该区域范围内的后续起始索引构成的：$T = \\{ p_k \\mid k  j \\text{ and } s \\le p_k  s+r \\cdot m \\}$。\n3.  **计算误差 ($e$)**：误差计算公式为 $e = 1 - \\frac{|B \\cap T|}{|B|}$，如果 $|B|=0$ 则为 $0$。\n\n这种结构化的方法允许对所提供的测试用例上的两种分词策略进行直接和定量的比较。", "answer": "```python\nimport numpy as np\nimport collections\nfrom math import log2\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases and produce the final output.\n    \"\"\"\n\n    def get_bpe_pairs(tokens):\n        \"\"\"Counts adjacent token pairs.\"\"\"\n        return collections.Counter(zip(tokens, tokens[1:]))\n\n    def merge_bpe_tokens(tokens, pair, new_token):\n        \"\"\"Merges a specific pair in a token list.\"\"\"\n        new_tokens = []\n        i = 0\n        while i  len(tokens):\n            if i  len(tokens) - 1 and (tokens[i], tokens[i+1]) == pair:\n                new_tokens.append(new_token)\n                i += 2\n            else:\n                new_tokens.append(tokens[i])\n                i += 1\n        return new_tokens\n\n    def bpe_trainer(sequence, vocab_size):\n        \"\"\"Trains BPE and returns the final tokenization and merges.\"\"\"\n        if not sequence:\n            return [], []\n        \n        tokens = list(sequence)\n        initial_vocab_size = len(set(tokens))\n        num_merges = vocab_size - initial_vocab_size\n        \n        merges = []\n        if num_merges = 0:\n            return tokens, merges\n\n        for _ in range(num_merges):\n            pairs = get_bpe_pairs(tokens)\n            if not pairs:\n                break\n            \n            max_freq = max(pairs.values())\n            # Tie-breaking with lexicographical order\n            best_pairs_candidates = [p for p, freq in pairs.items() if freq == max_freq]\n            best_pairs_candidates.sort(key=lambda p: p[0] + p[1])\n            best_pair = best_pairs_candidates[0]\n            \n            new_token = best_pair[0] + best_pair[1]\n            tokens = merge_bpe_tokens(tokens, best_pair, new_token)\n            merges.append((best_pair, new_token))\n        \n        return tokens, merges\n\n    def ulm_trainer(sequence, l_max):\n        \"\"\"Builds ULM vocabulary and probabilities from substrings.\"\"\"\n        if not sequence:\n            return {}, {}\n            \n        vocab_counts = collections.Counter()\n        total_substrings = 0\n        for length in range(1, l_max + 1):\n            for i in range(len(sequence) - length + 1):\n                substring = sequence[i:i+length]\n                vocab_counts[substring] += 1\n                total_substrings += 1\n        return vocab_counts, total_substrings\n\n    def ulm_tokenizer(sequence, probs, vocab, l_max):\n        \"\"\"Segments a sequence using ULM via dynamic programming.\"\"\"\n        n = len(sequence)\n        costs = {token: -log2(p) for token, p in probs.items()}\n        \n        dp = [np.inf] * (n + 1)\n        backpointers = [-1] * (n + 1)\n        dp[0] = 0\n        \n        for i in range(1, n + 1):\n            for j in range(1, min(i, l_max) + 1):\n                sub = sequence[i-j:i]\n                if sub in costs:\n                    cost = dp[i-j] + costs[sub]\n                    if cost  dp[i]:\n                        dp[i] = cost\n                        backpointers[i] = i-j\n\n        if np.isinf(dp[n]):\n             return [], [], np.inf\n        \n        tokens = []\n        indices = []\n        end = n\n        while end > 0:\n            start = backpointers[end]\n            tokens.append(sequence[start:end])\n            indices.append(start)\n            end = start\n        \n        tokens.reverse()\n        indices.reverse()\n        return tokens, indices, dp[n]\n\n    def calculate_error(tokenization, start_indices, s, m, r, seq_len):\n        \"\"\"Calculates the boundary misalignment error.\"\"\"\n        if r  2:\n            return 0.0\n        \n        motif_boundaries = {s + k * m for k in range(1, r)}\n        \n        first_token_idx_in_seg = -1\n        region_start, region_end = s, s + r * m\n        \n        for i, token_start in enumerate(start_indices):\n            token_len = len(tokenization[i])\n            if token_start  region_end and token_start + token_len > region_start:\n                first_token_idx_in_seg = i\n                break\n        \n        token_boundaries = set()\n        if first_token_idx_in_seg != -1:\n            for i in range(first_token_idx_in_seg + 1, len(start_indices)):\n                token_start = start_indices[i]\n                if region_start = token_start  region_end:\n                    token_boundaries.add(token_start)\n                    \n        intersection_size = len(motif_boundaries.intersection(token_boundaries))\n        \n        return 1.0 - (intersection_size / len(motif_boundaries))\n\n\n    test_cases = [\n        {\"seq\": \"ACGTAC\" + \"ATG\" * 10 + \"GGAAC\", \"V_BPE\": 12, \"L_max\": 5, \"lambda\": 1, \"alpha\": 1, \"s\": 6, \"m\": 3, \"r\": 10},\n        {\"seq\": \"CG\" + \"A\" * 20 + \"TGC\", \"V_BPE\": 8, \"L_max\": 10, \"lambda\": 1, \"alpha\": 0.5, \"s\": 2, \"m\": 1, \"r\": 20},\n        {\"seq\": \"GCTATCGAGT\" + \"CGT\" * 3 + \"ACGTA\", \"V_BPE\": 6, \"L_max\": 4, \"lambda\": 1, \"alpha\": 1, \"s\": 10, \"m\": 3, \"r\": 3},\n        {\"seq\": \"AT\" + \"GC\" * 12 + \"TAAC\", \"V_BPE\": 10, \"L_max\": 4, \"lambda\": 1, \"alpha\": 0.1, \"s\": 2, \"m\": 2, \"r\": 12},\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        seq = case[\"seq\"]\n        n = len(seq)\n        v_bpe, l_max, lam, alpha = case[\"V_BPE\"], case[\"L_max\"], case[\"lambda\"], case[\"alpha\"]\n        s, m, r = case[\"s\"], case[\"m\"], case[\"r\"]\n\n        # --- BPE ---\n        bpe_tokens, _ = bpe_trainer(seq, v_bpe)\n        bpe_vocab = set(bpe_tokens) | set(list('ACGT')) # Ensure base vocab is included\n        \n        # In case merging reduces vocab size below V_BPE, use actual final size.\n        final_bpe_vocab_size = len(set(bpe_trainer(seq, v_bpe)[0]))\n        \n        bpe_counts = collections.Counter(bpe_tokens)\n        total_bpe_tokens = len(bpe_tokens)\n        \n        bpe_probs = {\n            token: (bpe_counts.get(token, 0) + lam) / (total_bpe_tokens + lam * final_bpe_vocab_size)\n            for token in set(bpe_tokens)\n        }\n        \n        l_bpe = sum(-log2(bpe_probs[token]) for token in bpe_tokens)\n        l_bar_bpe = l_bpe / n if n > 0 else 0\n        \n        bpe_start_indices = [0]\n        pos = 0\n        for token in bpe_tokens[:-1]:\n            pos += len(token)\n            bpe_start_indices.append(pos)\n        \n        e_bpe = calculate_error(bpe_tokens, bpe_start_indices, s, m, r, n)\n\n        # --- ULM ---\n        ulm_vocab_counts, total_substrings = ulm_trainer(seq, l_max)\n        ulm_vocab_size = len(ulm_vocab_counts)\n        denominator = total_substrings + alpha * ulm_vocab_size\n        ulm_probs = {\n            token: (count + alpha) / denominator\n            for token, count in ulm_vocab_counts.items()\n        }\n        \n        ulm_tokens, ulm_indices, l_ulm = ulm_tokenizer(seq, ulm_probs, ulm_vocab_counts, l_max)\n        l_bar_ulm = l_ulm / n if n > 0 else 0\n        \n        e_ulm = calculate_error(ulm_tokens, ulm_indices, s, m, r, n)\n        \n        # Round and append results\n        all_results.append([\n            round(l_bar_bpe, 6),\n            round(l_bar_ulm, 6),\n            round(e_bpe, 6),\n            round(e_ulm, 6)\n        ])\n\n    # Format final output string\n    result_str = \",\".join([str(res) for res in all_results])\n    print(f\"[{result_str.replace(' ', '')}]\")\n\nsolve()\n\n```", "id": "4606969"}, {"introduction": "虽然大型自监督模型能达到很高的预测性能，但它们通常被视为“黑箱”。为了在科学研究中信任这些模型的预测，我们需要有效的方法来解释它们的决策过程。这项实践练习 [@problem_id:4606960] 介绍了积分梯度（Integrated Gradients）这一强大的归因技术，并要求您从零开始实现它，以确定模型在进行预测时依赖于哪些输入碱基，从而验证模型是否学到了具有生物学意义的特征，例如特定的基序。", "problem": "给定一个简化的脱氧核糖核酸 (DNA) 序列的掩码词元预测设置。在此设置中，一个长度为 $L$、基于字母表 $\\{A,C,G,T\\}$ 的序列在索引 $m$（从零开始）处有一个掩码位置。考虑一个线性-softmax模型，该模型根据剩余的上下文来预测被掩码的词元（即索引 $m$ 处的碱基）。该模型对输入序列的独热编码进行操作，其中掩码位置被设置成零向量。设输入表示为一个向量 $x \\in \\mathbb{R}^{4L}$，该向量是通过将每个位置上按固定碱基顺序 $\\{A,C,G,T\\}$ 排列的独热向量拼接而成的。模型为对应于类别 $\\{A,C,G,T\\}$ 的 $k \\in \\{0,1,2,3\\}$ 定义了类别logit $z_k(x) = w_k^\\top x$，以及预测概率 $p_k(x) = \\exp(z_k(x)) / \\sum_{j=0}^{3} \\exp(z_j(x))$。权重的构造方式使得对应于碱基 A 的真实类别 $k=0$ 对掩码上游的一个已知生物基序有强烈的响应，而在其他地方响应较弱。\n\n您必须计算模型预测的掩码词元类别对输入碱基的积分梯度 (IG) 归因，并评估这些归因是否与已知的基序对齐。请使用以下定义和约束。\n\n1. 独热编码和基线：\n   - 对于每个位置 $j \\in \\{0,\\dots,L-1\\}$ 和碱基 $b \\in \\{A,C,G,T\\}$，令 $x_{j,b} \\in \\{0,1\\}$ 表示相应的独热分量，但在掩码索引 $j=m$ 处，对所有 $b$ 设置 $x_{m,b} = 0$（掩码）。\n   - 基线为 $x' = \\mathbf{0} \\in \\mathbb{R}^{4L}$，直线路径为 $\\gamma(\\alpha) = x' + \\alpha (x-x') = \\alpha x$，其中 $\\alpha \\in [0,1]$。\n\n2. 模型构造：\n   - 在掩码的上游，相对偏移量为 $r \\in \\{-4,-3,-2,-1\\}$ 处，固定一个长度为 $4$ 的基序，其碱基分别为 $\\{T, A, T, A\\}$。对于任何在边界 $0 \\le j  L$ 内的位置 $j = m + r$，且碱基 $b$ 等于该偏移处的基序碱基，将一个大权重 $w$ 分配给类别 A（类别索引 0）的权重向量中的相应特征。对于所有类别的所有其他特征，分配一个小的正背景权重 $\\epsilon$。形式上，令 $M \\in \\mathbb{R}^{4 \\times 4L}$ 为权重矩阵，其行 $M_k^\\top = w_k^\\top$：\n     - 对于类别 $k=0$（碱基 A），如果 $j=m+r$ 有效且 $b$ 等于在偏移 $r$ 处指定的基序碱基，则设置 $M_{0, (4j + \\text{idx}(b))} = w$；否则设置 $M_{0, (4j + \\text{idx}(b))} = \\epsilon$。\n     - 对于类别 $k \\in \\{1,2,3\\}$（碱基 C,G,T），对所有特征 $i$ 设置 $M_{k, i} = \\epsilon$。\n   - 使用碱基索引映射 $\\text{idx}(A)=0$, $\\text{idx}(C)=1$, $\\text{idx}(G)=2$, $\\text{idx}(T)=3$。\n\n3. 归因的预测目标：\n   - 令 $p(x) = \\text{softmax}(M x)$。确定预测类别 $c^\\star = \\arg\\max_{k \\in \\{0,1,2,3\\}} p_k(x)$。计算关于标量函数 $F(x) = p_{c^\\star}(x)$ 的 IG。\n\n4. 积分梯度的定义：\n   - 对于每个特征 $i \\in \\{1,\\dots,4L\\}$，积分梯度定义为\n     $$\\mathrm{IG}_i(x) = (x_i - x_i') \\int_{0}^{1} \\frac{\\partial F(\\gamma(\\alpha))}{\\partial x_i} \\, d\\alpha,$$\n     您必须使用包含 $S$ 个步骤的黎曼和近似：\n     $$\\widehat{\\mathrm{IG}}_i(x) = (x_i - x_i') \\cdot \\frac{1}{S} \\sum_{s=1}^{S} \\left. \\frac{\\partial F(\\gamma(\\alpha))}{\\partial x_i} \\right|_{\\alpha = s/S}.$$\n\n5. 按位置的归因和基序对齐分数：\n   - 通过对每个位置的四个通道的绝对值求和，将特征级归因聚合到位置级，即对于位置 $j$，\n     $$A_j = \\sum_{b \\in \\{A,C,G,T\\}} \\left| \\widehat{\\mathrm{IG}}_{4j + \\text{idx}(b)}(x) \\right|.$$\n   - 从总数中排除掩码位置，因为其独热编码为零，并计算总归因\n     $$T = \\sum_{\\substack{j=0 \\\\ j \\ne m}}^{L-1} A_j.$$\n   - 给定一个注释的基序区间列表 $[s, e)$（起始位置包含，结束位置不包含）的绝对位置，将对齐分数定义为这些区间并集内的总归因比例：\n     $$\\text{score} = \\frac{\\sum_{[s,e)} \\sum_{j=s}^{e-1} A_j}{T}.$$\n\n6. 使用的数学和算法基础：\n   - 您必须从核心定义开始：独热编码、线性映射、softmax函数以及上述积分梯度的定义。不要使用任何外部机器学习包或自动微分；从这些定义解析地计算梯度。\n\n7. 所需的数值：\n   - 使用 $w = 3.0$ 和 $\\epsilon = 0.05$。\n   - 使用指定的碱基顺序 $\\{A,C,G,T\\}$ 和索引映射。\n   - 对于每个案例，使用提供的 $S$ 作为黎曼和的步数。\n\n8. 测试套件：\n   实现程序以处理以下案例，每个案例由元组 $(\\text{sequence}, m, \\text{intervals}, S)$ 定义，其中序列长度 $L=20$：\n   - 案例 1（上游有清晰的基序）：\n     - 序列：\"GCCGACTATANGTCCAAGTT\"，其中索引 $m$ 处的字符 'N' 表示掩码输入。\n     - 掩码索引 $m = 10$。\n     - 注释区间：$[(6,10)]$。\n     - 步数 $S = 50$。\n   - 案例 2（两个基序出现，只注释了近端的一个；离散化中的边界情况，使用最少步数）：\n     - 序列：\"TATAGCTATANGCGTCAAGT\"。\n     - 掩码索引 $m = 10$。\n     - 注释区间：$[(6,10)]$。\n     - 步数 $S = 1$。\n   - 案例 3（无近端基序）：\n     - 序列：\"GACGTCGCGCNATGCATAGC\"。\n     - 掩码索引 $m = 10$。\n     - 注释区间：$[(6,10)]$。\n     - 步数 $S = 50$。\n\n9. 程序输出：\n   - 您的程序应生成单行输出，其中包含三个案例的对齐分数，形式为用方括号括起来的逗号分隔列表（例如，\"[r1,r2,r3]\"）。每个 $r_i$ 必须是浮点数。\n\n本问题不要求任何物理单位、角度或百分比。所有数学计算必须符合上述定义，所有输出必须是遵循指定格式的数值浮点数。", "solution": "该问题要求计算一个应用于DNA序列的线性-softmax模型的积分梯度 (IG)，并根据得到的归因评估一个对齐分数。我将首先验证问题陈述，然后提供一个基于基本原理的分步解决方案。\n\n### 问题验证\n\n**步骤1：提取给定条件**\n- **字母表**：$\\{A,C,G,T\\}$\n- **序列长度**：$L=20$\n- **输入**：独热编码向量 $x \\in \\mathbb{R}^{4L}$，其中掩码位置 $m$ 由零向量表示。碱基顺序 $\\{A,C,G,T\\}$ 给出索引映射 $\\text{idx}(A)=0, \\text{idx}(C)=1, \\text{idx}(G)=2, \\text{idx}(T)=3$。\n- **模型**：线性-softmax模型，其logit为 $z_k(x) = w_k^\\top x$，概率为 $p_k(x) = \\text{softmax}(z)_k$。\n- **权重矩阵 ($M$)**：$M_{k,i}$ 是类别 $k$ 和特征 $i$ 的权重。\n  - 基序：$\\{T,A,T,A\\}$，位于距离掩码的相对偏移 $\\{-4,-3,-2,-1\\}$ 处。\n  - 类别 $k=0$ ('A')：如果 $j=m+r$ 对应一个有效的基序偏移 $r$ 且 $b$ 是该偏移处的基序碱基，则 $M_{0, (4j + \\text{idx}(b))} = w$。否则，$M_{0,i} = \\epsilon$。\n  - 类别 $k \\in \\{1,2,3\\}$ ('C','G','T')：对所有特征 $i$，有 $M_{k,i} = \\epsilon$。\n- **数值常量**：$w=3.0$, $\\epsilon=0.05$。\n- **积分梯度 (IG)**：\n  - 基线：$x' = \\mathbf{0}$。\n  - 路径：$\\gamma(\\alpha) = \\alpha x$，其中 $\\alpha \\in [0,1]$。\n  - 归因目标：$F(x) = p_{c^\\star}(x)$，其中 $c^\\star = \\arg\\max_k p_k(x)$。\n  - 黎曼和近似：$\\widehat{\\mathrm{IG}}_i(x) = x_i \\cdot \\frac{1}{S} \\sum_{s=1}^{S} \\left. \\frac{\\partial F(\\gamma(\\alpha))}{\\partial y_i} \\right|_{y = \\gamma(\\alpha_s)}$，其中 $\\alpha_s = s/S$。\n- **归因聚合和评分**：\n  - 按位置归因：$A_j = \\sum_{b \\in \\{A,C,G,T\\}} \\left| \\widehat{\\mathrm{IG}}_{4j + \\text{idx}(b)}(x) \\right|$。\n  - 总归因（不包括掩码）：$T = \\sum_{j=0, j \\ne m}^{L-1} A_j$。\n  - 对齐分数：$\\text{score} = (\\sum_{[s,e)} \\sum_{j=s}^{e-1} A_j) / T$。\n- **测试用例**：\n  - 案例 1：$(\\text{sequence} = \\text{\"GCCGACTATANGTCCAAGTT\"}, m=10, \\text{intervals}=[(6,10)], S=50)$\n  - 案例 2：$(\\text{sequence} = \\text{\"TATAGCTATANGCGTCAAGT\"}, m=10, \\text{intervals}=[(6,10)], S=1)$\n  - 案例 3：$(\\text{sequence} = \\text{\"GACGTCGCGCNATGCATAGC\"}, m=10, \\text{intervals}=[(6,10)], S=50)$\n\n**步骤2：使用提取的给定条件进行验证**\n该问题有科学依据，使用了机器学习中已建立的概念（线性模型、softmax、积分梯度），并将其应用于一个标准的生物信息学任务（基序分析）。问题是良定的，所有变量、函数和常量都已明确定义，从而为每个案例导出一个唯一的、可计算的解。语言客观而精确。没有发现与科学合理性、可形式化性、完整性、可行性或结构相关的缺陷。该问题是一个非平凡的计算任务，需要仔细实现解析梯度和数值积分。\n\n**步骤3：结论和行动**\n问题有效。我将继续提供完整解决方案。\n\n### 解决方案\n\n解决方案分五个阶段进行：形式化模型和输入，推导IG所需的解析梯度，详述IG的计算，定义归因聚合，以及分析每个测试用例。\n\n**1. 模型和输入的形式化**\n输入是一个长度为 $L=20$ 的DNA序列，被转换为一个独热向量 $x \\in \\mathbb{R}^{80}$。对于一个非掩码位置 $j$ 处的碱基 $b$，分量 $x_{4j + \\text{idx}(b)} = 1$，该位置的所有其他分量为 $0$。在掩码位置 $m$ 处，所有四个分量都为 $0$。\n\n模型由权重矩阵 $M \\in \\mathbb{R}^{4 \\times 80}$ 定义。问题指定了其构造方式：\n- 对于行 $k \\in \\{1,2,3\\}$，对应于类别 'C', 'G', 'T'，所有权重都是 $\\epsilon = 0.05$。因此，对于 $k0$，有 $M_{k,i} = \\epsilon$。\n- 对于行 $k=0$，对应于类别 'A'，对于匹配基序 $\\{T,A,T,A\\}$ 在位置 $\\{m-4, m-3, m-2, m-1\\}$ 上的特征，权重为 $w=3.0$。对于该行中的所有其他特征，权重为 $\\epsilon$。令 $I_{motif}$ 为对应于此基序的特征索引集合。那么，如果 $i \\in I_{motif}$，则 $M_{0,i} = w$；如果 $i \\notin I_{motif}$，则 $M_{0,i} = \\epsilon$。\n\nlogit为 $z = Mx$，概率为 $p = \\text{softmax}(z)$。\n\n**2. 解析梯度推导**\nIG计算的核心是目标函数 $F(y) = p_{c^\\star}(y)$ 关于其输入向量 $y$ 的梯度。这里 $y$ 将是插值输入 $\\gamma(\\alpha)$。预测类别 $c^\\star = \\arg\\max_k p_k(x)$ 是根据原始输入 $x$ 确定的，并在整个IG计算过程中保持不变。\n\n使用链式法则，概率 $p_k$ 关于输入特征 $y_i$ 的导数为：\n$$ \\frac{\\partial p_k(y)}{\\partial y_i} = \\sum_{l=0}^{3} \\frac{\\partial p_k(y)}{\\partial z_l(y)} \\frac{\\partial z_l(y)}{\\partial y_i} $$\nsoftmax函数的雅可比矩阵是 $\\frac{\\partial p_k}{\\partial z_l} = p_k (\\delta_{kl} - p_l)$，其中 $\\delta_{kl}$ 是克罗内克δ函数。线性层的导数是 $\\frac{\\partial z_l}{\\partial y_i} = M_{li}$。\n代入可得：\n$$ \\frac{\\partial p_k(y)}{\\partial y_i} = \\sum_{l=0}^{3} p_k(y)(\\delta_{kl} - p_l(y)) M_{li} = p_k(y) \\left( M_{ki} - \\sum_{l=0}^{3} p_l(y) M_{li} \\right) $$\n我们来分析 $E_i(y) = \\sum_{l=0}^{3} p_l(y) M_{li}$ 这一项。这是向量 $p(y)^\\top M$ 的第 $i$ 个分量。考虑到我们特定的权重矩阵 $M$，其中对于 $l0$ 有 $M_{li} = \\epsilon$，我们可以简化 $E_i(y)$：\n$$ E_i(y) = p_0(y)M_{0i} + p_1(y)M_{1i} + p_2(y)M_{2i} + p_3(y)M_{3i} = p_0(y)M_{0i} + (p_1(y)+p_2(y)+p_3(y))\\epsilon $$\n由于 $\\sum_l p_l(y) = 1$，我们有 $p_1+p_2+p_3 = 1-p_0$。\n$$ E_i(y) = p_0(y)M_{0i} + (1-p_0(y))\\epsilon $$\n我们感兴趣的是类别 $c^\\star$ 的梯度。如果 $c^\\star=0$，梯度为：\n$$ \\frac{\\partial p_0(y)}{\\partial y_i} = p_0(y) (M_{0i} - E_i(y)) = p_0(y)(M_{0i} - [p_0(y)M_{0i} + (1-p_0(y))\\epsilon]) $$\n$$ \\frac{\\partial p_0(y)}{\\partial y_i} = p_0(y)(M_{0i}(1-p_0(y)) - \\epsilon(1-p_0(y))) = p_0(y)(1-p_0(y))(M_{0i} - \\epsilon) $$\n这是一个关键的简化。类别 'A' 关于特征 $i$ 的梯度与 $(M_{0i} - \\epsilon)$ 成正比。\n如果特征 $i$ 不是 'A' 类基序的一部分，则 $M_{0i}=\\epsilon$，梯度为零。\n如果特征 $i$ 是基序的一部分，则 $M_{0i}=w$，梯度为 $p_0(y)(1-p_0(y))(w - \\epsilon)$。\n\n**3. 积分梯度计算**\nIG的黎曼和近似为：\n$$ \\widehat{\\mathrm{IG}}_i(x) = x_i \\cdot \\frac{1}{S} \\sum_{s=1}^{S} \\left. \\frac{\\partial p_{c^\\star}(y)}{\\partial y_i} \\right|_{y = \\gamma(\\alpha_s)} \\quad \\text{其中 } \\alpha_s = s/S $$\n梯度项在插值输入 $y = \\gamma(\\alpha_s) = \\alpha_s x$ 处进行评估。此时的logit是 $z(\\alpha_s) = M(\\alpha_s x) = \\alpha_s z$，其中 $z=Mx$ 是原始输入的logit。概率为 $p(\\alpha_s) = \\text{softmax}(\\alpha_s z)$。\n\n**4. 归因聚合和评分**\n特征级归因 $\\widehat{\\mathrm{IG}}_i$ 通过对每个位置 $j$ 处四个特征归因的绝对值求和来聚合成位置级归因 $A_j$：\n$$ A_j = \\sum_{b=0}^{3} |\\widehat{\\mathrm{IG}}_{4j+b}| $$\n总归因 $T$ 是所有非掩码位置上 $A_j$ 的总和。最终的对齐分数是在注释的基序区间内的总归因比例。\n\n**5. 测试用例分析**\n\n- **案例 1: `seq=\"GCCGACTATANGTCCAAGTT\"`, `m=10`, `intervals=[(6,10)]`, `S=50`**\n  相对于掩码 $m=10$，基序位置为 $\\{6,7,8,9\\}$。序列在这些位置上为 `TATA`，与模型 'A' 类权重敏感的基序 $\\{T,A,T,A\\}$ 完美匹配。\n  输入向量 $x$ 将对于序列中的碱基有 $x_{4j+\\text{idx}(b)}=1$。具体来说，对于基序位置，我们有 $x_{27}=1$ (`T` at 6), $x_{28}=1$ (`A` at 7), $x_{35}=1$ (`T` at 8), $x_{36}=1$ (`A` at 9)。\n  logit为 $z=Mx$。对于 $k=0$，$z_0 = 4w + (19-4)\\epsilon = 4(3.0) + 15(0.05) = 12.75$。对于 $k0$，$z_k=19\\epsilon=0.95$。\n  显然，$z_0$ 是最大的，所以预测类别是 $c^\\star=0$。\n  我们使用 $p_0$ 的梯度公式。梯度 $\\frac{\\partial p_0}{\\partial y_i}$ 仅在 $M_{0i} \\ne \\epsilon$ 时非零，这只发生在四个基序特征 $i \\in I_{motif}$ 上。\n  IG归因 $\\widehat{\\mathrm{IG}}_i$ 仅在 $x_i=1$ 且梯度非零时才非零。这两个条件对于四个基序特征都满足，因为序列与基序匹配。对于所有其他特征，要么梯度为零，要么 $x_i$ 为零。\n  因此，归因仅对位置 $\\{6,7,8,9\\}$ 对应的特征非零。按位置归因 $A_j$ 仅对 $j \\in \\{6,7,8,9\\}$ 非零。\n  总归因是 $T = A_6+A_7+A_8+A_9$。\n  注释区间是 $[6,10)$，覆盖了位置 $\\{6,7,8,9\\}$。此区间内的归因也是 $A_6+A_7+A_8+A_9$。\n  分数为 $(A_6+A_7+A_8+A_9) / (A_6+A_7+A_8+A_9) = 1.0$。\n\n- **案例 2: `seq=\"TATAGCTATANGCGTCAAGT\"`, `m=10`, `intervals=[(6,10)]`, `S=1`**\n  序列在位置 $\\{6,7,8,9\\}$ 处再次有一个完美的 `TATA` 基序。分析与案例1相同。预测类别是 $c^\\star=0$。所有归因都局限于位置 $\\{6,7,8,9\\}$。注释区间是 $[6,10)$。因此分数为 $1.0$。在这个高度结构化的问题中，步数 $S$ 会影响归因值的大小，但不会影响它们的比例。\n\n- **案例 3: `seq=\"GACGTCGCGCNATGCATAGC\"`, `m=10`, `intervals=[(6,10)]`, `S=50`**\n  在基序敏感位置 $\\{6,7,8,9\\}$ 的序列是 `GCGC`。这与 `TATA` 基序不匹配。\n  当计算logit $z=Mx$ 时，没有输入特征 $x_i=1$ 与特殊权重 $M_{0,i}=w$ 对齐。\n  因此，对于所有 $k \\in \\{0,1,2,3\\}$，logit是 $19$ 个 $\\epsilon \\cdot 1$ 项的和，所以 $z_k=19\\epsilon=0.95$。\n  所有logit都相等。概率对所有 $k$ 均为 $p_k=0.25$。按照惯例（例如 `numpy.argmax`），预测类别是第一个，即 $c^\\star=0$。\n  我们再次计算 $p_0$ 的IG。梯度公式 $\\frac{\\partial p_0}{\\partial y_i} = p_0(y)(1-p_0(y))(M_{0i} - \\epsilon)$ 仍然有效。\n  归因 $\\widehat{\\mathrm{IG}}_i$ 要求 $x_i=1$ 和 $M_{0i} \\ne \\epsilon$ 同时成立。$M_{0i}=w$ 的特征集合与 $x_i=1$ 的特征集合是不相交的。对于任何在输入中被激活的特征 $i$ ($x_i=1$)，相应的权重是 $M_{0i}=\\epsilon$，这使得梯度项 $(M_{0i}-\\epsilon)$ 为零。\n  因此，对所有 $i$，$\\widehat{\\mathrm{IG}}_i(x) = 0$。所有按位置的归因 $A_j$ 都为零。总归因 $T=0$。\n  分数为 $\\frac{0}{0}$，逻辑上解释为 $0.0$，因为没有归因落在基序区域内。\n\n最终预测分数：$[1.0, 1.0, 0.0]$。", "answer": "```python\nimport numpy as np\nfrom scipy.special import softmax\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    \n    # Test cases as defined in the problem statement\n    test_cases = [\n        # (sequence, mask_index, annotated_intervals, num_steps)\n        (\"GCCGACTATANGTCCAAGTT\", 10, [(6, 10)], 50),\n        (\"TATAGCTATANGCGTCAAGT\", 10, [(6, 10)], 1),\n        (\"GACGTCGCGCNATGCATAGC\", 10, [(6, 10)], 50),\n    ]\n\n    results = []\n    for case in test_cases:\n        score = calculate_score(*case)\n        results.append(score)\n\n    # Format the output as a comma-separated list in brackets\n    formatted_results = f\"[{','.join(f'{r:.7f}' for r in results)}]\"\n    print(formatted_results)\n\ndef calculate_score(sequence, m, intervals, S):\n    \"\"\"\n    Calculates the motif alignment score for a single test case.\n    \"\"\"\n    # 1. Define constants and mappings\n    L = 20\n    w = 3.0\n    epsilon = 0.05\n    char_to_idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n    num_features = 4 * L\n    \n    # 2. Build one-hot encoded input vector x\n    x = np.zeros(num_features)\n    for j, char in enumerate(sequence):\n        if j != m and char in char_to_idx:\n            base_idx = char_to_idx[char]\n            x[4 * j + base_idx] = 1\n            \n    # 3. Build weight matrix M\n    M = np.full((4, num_features), epsilon)\n    motif_bases = {'T': -4, 'A': -3, 'T': -2, 'A': -1}\n    for base_char, offset in motif_bases.items():\n        pos = m + offset\n        if 0 = pos  L:\n            base_idx = char_to_idx[base_char]\n            feature_idx = 4 * pos + base_idx\n            M[0, feature_idx] = w  # Class 0 is for 'A'\n            \n    # 4. Determine the predicted class c_star for the input x\n    z = M @ x\n    p = softmax(z)\n    c_star = np.argmax(p)\n    \n    # 5. Compute Integrated Gradients using Riemann sum\n    sum_of_grads = np.zeros(num_features)\n    for s in range(1, S + 1):\n        alpha = s / S\n        \n        # Calculate logits and probabilities for the interpolated input\n        z_alpha = alpha * z\n        p_alpha = softmax(z_alpha)\n        \n        # Calculate the gradient of the predicted class probability w.r.t. the interpolated input\n        # This is the vector form of the analytical gradient:\n        # grad_i = p_alpha[c_star] * (M[c_star, i] - sum_l(p_alpha[l] * M[l, i]))\n        expected_weights = p_alpha @ M\n        grad_at_alpha = p_alpha[c_star] * (M[c_star, :] - expected_weights)\n        \n        sum_of_grads += grad_at_alpha\n\n    # Final IG is the average gradient multiplied by the input feature value\n    ig = x * (sum_of_grads / S)\n    \n    # 6. Aggregate feature attributions to position-wise attributions A_j\n    # Reshape to (L, 4) and sum absolute values over the base channels\n    A = np.sum(np.abs(ig.reshape((L, 4))), axis=1)\n\n    # 7. Compute the final alignment score\n    total_attribution = 0\n    motif_attribution = 0\n    \n    # Create a set of indices for annotated regions for efficient lookup\n    annotated_indices = set()\n    for start, end in intervals:\n        annotated_indices.update(range(start, end))\n\n    for j in range(L):\n        if j != m: # Exclude masked position from total attribution\n            total_attribution += A[j]\n            if j in annotated_indices:\n                motif_attribution += A[j]\n                \n    if total_attribution == 0:\n        score = 0.0\n    else:\n        score = motif_attribution / total_attribution\n        \n    return score\n\nif __name__ == '__main__':\n    solve()\n```", "id": "4606960"}]}