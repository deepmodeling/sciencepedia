## 引言
随着人工智能技术在医疗健康领域的广泛应用，从疾病风险预测到治疗方案推荐，临床模型正深刻地改变着现代医学的面貌。然而，这些强大的工具也带来了一个严峻的挑战：它们可能在不经意间学习、复制甚至放大训练数据中潜藏的社会和历史偏见，从而导致不同患者群体间的健康服务差距扩大。因此，系统性地理解、量化并缓解临床模型中的偏见，已成为构建负责任、可信赖医疗AI的当务之急。

本文旨在为解决这一核心问题提供一个全面的框架。我们将深入探讨偏见产生的机制，并系统介绍贯穿模型开发全生命周期的缓解策略。通过阅读本文，您将能够：

在第一章“原理与机制”中，我们将建立理解偏见的理论基础。您将学习如何用精确的数学语言定义和衡量不同的公平性，理解它们之间不可避免的冲突与权衡，并追溯偏见在数据层面的根本来源，例如测量误差、数据缺失和[标签噪声](@entry_id:636605)。

随后，在第二章“应用与跨学科联系”中，我们将理论与实践相结合。本章将展示偏见缓解技术如何在复杂的真实世界临床场景中应用，例如在处理多中心研究数据、生存分析和代理标签等问题时。您将看到，有效的偏见缓解方案常常需要跨越统计学、临床医学和伦理学的界限。

最后，在第三章“动手实践”中，您将有机会通过一系列具体的编程练习，将所学知识付诸实践。这些练习将引导您亲手实现关键的偏见量化和缓解算法，从而巩固您对核心概念的理解，并为您在自己的研究或工作中应用这些技术打下坚实的基础。

## 原理与机制

本章旨在深入探讨在临床模型中减轻偏见的核心原理和机制。我们将从精确定义和衡量偏见的多种统计学方法入手，揭示不同公平性目标之间固有的紧张关系与权衡。随后，我们将追溯偏见的根源，不仅探究算法本身，更深入分析数据生成过程中可能引入的系统性偏差，例如测量误差、数据缺失和[标签噪声](@entry_id:636605)。最后，本章将视野拓宽至更高级的公平性范式，包括交叉性公平、因果公平以及在时间到事件模型等复杂场景下的应用，为构建更负责任、更公平的临床决策支持系统奠定坚实的理论基础。

### 临床模型中的算法公平性分类

为了系统地分析和缓解偏见，我们必须首先建立一套精确的语言来描述它。在临床环境中，一个模型可能基于患者特征 $X$ 对某个真实标签 $Y \in \{0, 1\}$（例如，不良事件是否发生）做出预测。该模型通常会输出一个连续的风险评分 $R \in [0, 1]$，并通过设定阈值将其转换为一个二元决策 $\hat{Y} \in \{0, 1\}$。偏见的评估则围绕着一个或多个敏感属性 $A$（例如，性别、种族）展开，考察模型的表现在不同群体间是否存在差异。以下是衡量公平性的几类核心准则 [@problem_id:4542386]。

#### 基于平等的准则

这类准则关注模型输出或错误率在不同群体间的均等性。

**人口统计均等 (Demographic Parity)**：也称为统计均等，要求模型的预测决策 $\hat{Y}$ 在统计上独立于敏感属性 $A$。其形式化表达为 $\hat{Y} \perp \!\!\! \perp A$。这意味着模型在各个群体中做出阳性预测的比例是相同的，即 $P(\hat{Y}=1 \mid A=a)$ 对于所有群体 $a$ 都是一个常数。这一准则的直观吸[引力](@entry_id:189550)在于它确保了模型决策的“影响力”在各群体间均等分配。然而，其主要缺陷在于完全忽略了不同群体间真实基础发病率（即 $P(Y=1 \mid A=a)$）可能存在的差异。如果一个群体本身就有更高的患病风险，强行拉平预测率可能导致对高风险群体的诊断不足或对低风险群体的过度诊断。

**[均等化赔率](@entry_id:637744) (Equalized Odds)**：此准则要求在给定真实标签 $Y$ 的条件下，模型的预测 $\hat{Y}$ 独立于敏感属性 $A$。其形式化表达为 $\hat{Y} \perp \!\!\! \perp A \mid Y$。这等价于要求模型在所有群体中具有相同的**[真阳性率](@entry_id:637442) (True Positive Rate, TPR)** 和**假阳性率 (False Positive Rate, FPR)**。
$P(\hat{Y}=1 \mid Y=1, A=a_1) = P(\hat{Y}=1 \mid Y=1, A=a_2)$
$P(\hat{Y}=1 \mid Y=0, A=a_1) = P(\hat{Y}=1 \mid Y=0, A=a_2)$
[均等化赔率](@entry_id:637744)的理念是，无论患者属于哪个群体，只要他们的真实临床状况相同，就应该有同等的机会被模型正确（或错误）地识别。

**均等机会 (Equal Opportunity)**：这是[均等化赔率](@entry_id:637744)的一个宽松版本，它只要求各群体间的[真阳性率](@entry_id:637442)相等 [@problem_id:4542391]。其主要关注点是确保那些真正需要被识别出的患者（$Y=1$）不会因为其群体身份而处于不利地位。由于[真阳性率](@entry_id:637442)和假阴性率 (False Negative Rate, FNR) 之间存在直接关系（$FNR = 1 - TPR$），均等机会也等同于要求所有群体具有相等的假阴性率 [@problem_id:4542391]。

#### 基于校准的准则

这类准则关注风险评分 $R$ 的解释性，即分数是否准确地反映了真实的事件概率。

**群体内校准 (Groupwise Calibration)**：这是校准概念在公平性语境下的关键应用。它要求风险评分 $R$ 在每个由敏感属性 $A$ 定义的子群体内都是一个准确的概率估计。形式上，这意味着给定风险评分 $R$ 和群体属性 $A$，真实的阳性结果 $Y$ 的[期望值](@entry_id:150961)等于风险评分本身，即 $E[Y \mid R, A] = R$。对于[二元结果](@entry_id:173636) $Y$，这等价于 $P(Y=1 \mid R=r, A=a) = r$ [@problem_id:4542386]。这个性质至关重要，因为它确保了医生在使用模型评分时，对于来自不同群体的两位具有相同风险评分的患者，可以相信他们具有相同的真实事件概率。

值得注意的是，群体内校准是一个比**整体校准 (Overall Calibration)** 更强的条件。整体校准只要求 $E[Y \mid R] = R$，不考虑群体属性 $A$。一个在整体上校准良好的模型，在分解到不同子群体时，其校准性可能存在严重偏差 [@problem_id:4542362]。例如，模型可能系统性地高估一个群体的风险，同时低估另一个群体的风险，而这两种偏差在混合计算整体校准度时恰好相互抵消，从而掩盖了群体间的不公平。从数学上讲，群体内校准必然蕴含整体校准，但反之不成立，除非满足一个额外的、通常不成立的条件，即在给定风险评分 $R$ 的情况下，真实结果 $Y$ 与敏感属性 $A$ 相互独立 ($Y \perp \!\!\! \perp A \mid R$) [@problem_id:4542362]。

#### 基于预测值的准则

**预测值均等 (Predictive Parity)**：此准则要求模型的**阳性预测值 (Positive Predictive Value, PPV)** 在不同群体间相等。PPV 定义为在模型预测为阳性的条件下，真实结果也为阳性的概率，即 $P(Y=1 \mid \hat{Y}=1)$。预测值均等的形式化表达为 $Y \perp \!\!\! \perp A \mid \hat{Y}=1$，或等价地，$P(Y=1 \mid \hat{Y}=1, A=a_1) = P(Y=1 \mid \hat{Y}=1, A=a_2)$。该准则的直观意义是，一个阳性的预测结果，无论对于哪个群体的患者，都应该具有相同的“可信度”或临床意义。

### 固有权衡：公平性中的不可能定理

在定义了多个看似都合理的公平性准则后，一个核心问题随之而来：我们能否同时满足所有这些准则？答案是否定的。在现实世界的临床场景中，不同公平性目标之间存在着深刻的、数学上不可避免的冲突。

最著名的例子是 Kleinberg 等人提出的不可能定理，它揭示了在特定条件下，三个关键属性无法共存：(1) 群体内校准，(2) [均等化赔率](@entry_id:637744)，以及 (3) 预测值均等 [@problem_id:4542388]。

**定理**：对于一个非退化（即非完美或非随机猜测）的二元分类器，如果不同群体之间的基础发病率不同（即 $\pi_a := P(Y=1 \mid A=a)$ 不全相等），那么该分类器不可能同时满足[均等化赔率](@entry_id:637744)和预测值均等。

**证明思路**：我们可以通过贝叶斯定理将这几个准则联系起来。一个群体的阳性预测值 $PPV_a$ 可以表示为：
$PPV_a = P(Y=1 \mid \hat{Y}=1, A=a) = \frac{P(\hat{Y}=1 \mid Y=1, A=a) P(Y=1 \mid A=a)}{P(\hat{Y}=1 \mid A=a)}$
利用[全概率公式](@entry_id:194231)展开分母，并代入 TPR, FPR 和基础发病率 $\pi_a$ 的定义，我们得到：
$PPV_a = \frac{TPR_a \cdot \pi_a}{TPR_a \cdot \pi_a + FPR_a \cdot (1-\pi_a)}$

现在，假设一个分类器同时满足[均等化赔率](@entry_id:637744)和预测值均等。
*   [均等化赔率](@entry_id:637744)意味着 $TPR_a = TPR$ 且 $FPR_a = FPR$ 对所有群体 $a$ 成立。
*   预测值均等意味着 $PPV_a = PPV$ 对所有群体 $a$ 成立。

将这些等式代入上述公式，我们得到：
$\frac{TPR \cdot \pi_a}{TPR \cdot \pi_a + FPR \cdot (1-\pi_a)} = \text{常数}$
对于一个非退化的分类器（$TPR > FPR$），上述表达式是关于 $\pi_a$ 的严格单调递增函数。因此，要使该表达式为常数，唯一的可能是所有的 $\pi_a$ 都相等。这与我们“不同群体基础发病率不同”的核心假设相矛盾。因此，除非分类器是退化的（例如，TPR=FPR，即随机猜测；或 FPR=0 且 TPR=1，即完美分类），否则这三个属性无法共存。

这一深刻的结果意味着，在设计和部署临床模型时，我们必须做出选择。我们无法拥有一个既能保证风险评分在各群体内都准确（校准），又能保证错误率在各群体间均等（[均等化赔率](@entry_id:637744)），同时还能保证阳性预测的意义在各群体间一致（预测值均等）的模型。选择哪一个或哪几个公平性准则优先，是一个依赖于具体临床情境、社会价值观和伦理考量的决策过程，而非纯粹的技术问题。类似地，其他公平性组合也存在冲突，例如，对于一个经过群体内校准的非退化评分模型，在基础发病率不同的情况下，同时实现**正类平衡 (balance for the positive class)** 和**负类平衡 (balance for the negative class)** 也是不可能的 [@problem_id:4542391]。

### 偏见的来源：数据本身的问题

[算法偏见](@entry_id:637996)不仅源于模型设计或优化目标，更深层次的原因往往植根于训练数据本身。一个完美的算法如果“喂食”的是有偏的数据，其输出结果也必然会带有偏见。以下是临床数据中几种常见的偏见来源及其机制。

#### 协变量的测量误差

临床实践中，许多关键的生物标志物或生理指标并非被完美测量，而是伴随着测量误差。当这种测量误差的大小或方向在不同群体间存在系统性差异时，就会引入偏见。

设想一个药理基因组学风险模型，其真实模型为 $Y = \beta_0 + \beta_j X_j + \varepsilon$，其中 $X_j$ 是一个关键的生物标志物。然而，我们实际观测到的值是 $W_j = X_j + U_A$，其中测量误差 $U_A$ 的分布（例如均值 $m_A$ 和方差 $s_A^2$）依赖于群体 $A$ [@problem_id:4542390]。如果我们天真地使用观测值 $W_j$ 来训练或预测，即 $\hat{Y}_{\text{naive}} = \beta_0 + \beta_j W_j$，那么模型的平均预测在群体 $a$ 中将是 $\mathbb{E}[\hat{Y}_{\text{naive}} \mid A=a] = \beta_0 + \beta_j (\mu_{X,a} + m_a)$。即使真实的生物标志物均值 $\mu_{X,a}$ 在各群体间相同，只要误差均值 $m_a$ 不同，就会导致预测结果的系统性差异。

一种有效的预处理缓解策略是**回归校准 (Regression Calibration)**。其核心思想是，用我们基于带噪观测值 $W_j$ 对真实值 $X_j$ 的最佳估计 $\mathbb{E}[X_j \mid W_j, A=a]$ 来替代 $W_j$。在变量服从高斯分布的假设下，这个[条件期望](@entry_id:159140)可以被推导为一个关于 $W_j$ 的线性函数：
$\mathbb{E}[X_j \mid W_j, A=a] = \mu_{X,a} + \frac{\sigma_X^2}{\sigma_X^2 + s_a^2} (W_j - (\mu_{X,a} + m_a))$
其中 $\sigma_X^2$ 是 $X_j$ 的方差。使用这个校正后的预测器 $\hat{Y}_{\text{corr}} = \beta_0 + \beta_j \mathbb{E}[X_j \mid W_j, A=a]$，其在群体 $a$ 内的平均预测值将变为 $\mathbb{E}[\hat{Y}_{\text{corr}} \mid A=a] = \beta_0 + \beta_j \mu_{X,a}$。这样，由测量误差引入的系统性偏差就被消除了，使得群体间平均预测值的差异仅反映真实生物标志物水平的差异 [@problem_id:4542390]。

#### 数据缺失机制

在电子健康记录（EHR）数据中，数据缺失是常态而非例外。当数据缺失的模式与敏感属性相关时，它就成为偏见的一个重要来源。根据 Rubin 的框架，数据缺失机制可分为三类 [@problem_id:4542417]：

1.  **[完全随机缺失](@entry_id:170286) (Missing Completely At Random, MCAR)**：缺失概率与任何变量（无论观测与否）都无关。如果结果 $Y$ 的缺失[指示变量](@entry_id:266428) $R_Y$ 在每个群体内是 MCAR 的，那么基于完整数据（$R_Y=1$）天真计算的[公平性指标](@entry_id:634499)（如 TPR 和 PPV）是无偏的 [@problem_id:4542417]。

2.  **[随机缺失](@entry_id:168632) (Missing At Random, MAR)**：缺失概率仅依赖于已观测的变量，而在给定这些观测变量后，与未观测的变量本身无关。例如，结果 $Y$ 的缺失可能与某些协变量 $X$ 相关，但与 $Y$ 本身无关，即 $R_Y \perp Y \mid X, G$。在这种情况下，仅使用完整数据的朴素估计通常是有偏的。例如，如果医生更倾向于为病情更严重的患者（由 $X$ 体现，进而影响模型预测 $\hat{Y}$）确认最终诊断结果，那么这种选择性验证就会导致评估[公平性指标](@entry_id:634499)时产生偏见。要获得[无偏估计](@entry_id:756289)，需要使用诸如**[逆概率](@entry_id:196307)加权 (Inverse Probability Weighting, IPW)** 等统计方法进行校正 [@problem_id:4542417]。

3.  **[非随机缺失](@entry_id:163489) (Missing Not At Random, MNAR)**：缺失概率依赖于未观测的变量本身。例如，如果病情特别轻微或特别严重的患者的诊断结果更容易缺失，即使在控制了所有其他观测变量后依然如此。MNAR 是最难处理的情况，它会导致几乎所有朴素的分析和标准的校正方法都产生偏见。例如，如果一个关键协变量是 MNAR 的，使用简单的均值插补会系统性地扭曲该变量的分布，从而污染模型的预测 $\hat{Y}$，进而使所有基于 $\hat{Y}$ 的[公平性指标](@entry_id:634499)（无论是 TPR 还是 PPV）的评估都产生偏误 [@problem_id:4542417]。

#### [标签噪声](@entry_id:636605)

即使数据是完整的，其“真实”标签 $Y$ 本身也可能不准确，而是被一个有噪声的标签 $\tilde{Y}$ 所替代。当产生噪声的过程（例如，诊断错误、编码错误）在不同群体间存在差异时，就会严重扭曲公平性的评估。

考虑一个由群体特异的噪声矩阵 $T^{(a)}$ 描述的**类别条件[标签噪声](@entry_id:636605) (class-conditional label noise)** 模型 [@problem_id:4542421]。该模型定义了在真实标签为 $y$ 且属于群体 $a$ 的情况下，观测到噪声标签为 $\tilde{y}$ 的概率。例如，$\eta_0^{(a)} = P(\tilde{Y}=1 \mid Y=0, A=a)$ 是群体 $a$ 的[假阳性](@entry_id:635878)噪声率。

在这种情况下，我们基于噪声标签 $\tilde{Y}$ 计算出的“观测”[公平性指标](@entry_id:634499)（如 $\tilde{FPR}^{(a)} = P(\hat{Y}=1 \mid \tilde{Y}=0, A=a)$）与真实的[公平性指标](@entry_id:634499)（$FPR^{(a)}$）之间存在复杂的关系。可以证明，观测到的错误率实际上是真实错误率和模型其他性能指标的混合体。例如，观测到的[假阳性率](@entry_id:636147)可以表示为：
$\tilde{FPR}^{(a)} = w_0^{(a)} FPR^{(a)} + w_1^{(a)} TPR^{(a)}$
其中权重 $w_0^{(a)}$ 和 $w_1^{(a)}$ 是依赖于噪声率（$\eta_0^{(a)}, \eta_1^{(a)}$）和群体基础发病率（$\pi^{(a)}$）的复杂函数 [@problem_id:4542421]。

这意味着，群体间不同的[标签噪声](@entry_id:636605)不仅会改变公平性差异的**大小**，甚至可能改变其**方向**。一个模型在真实标签下可能对群体 A 更公平，但在噪声标签下观测到的结果可能恰恰相反。这突出表明，在进行公平性审计和缓解时，评估和处理标签质量的差异至关重要。

### 公平性的高级与扩展视角

随着领域的发展，对公平性的理解也在不断深化，超越了传统的统计均等性，并扩展到更复杂的模型和场景中。

#### 交叉性公平

现实世界中的个体往往具有多重身份，隶属于多个敏感属性的交叉组合（例如，特定种族和性别的老年女性）。仅仅在单个属性（如种族或性别）上实现公平，可能会掩盖在这些交叉子群体中存在的严重不公。

**交叉性公平 (Intersectional Fairness)** 的理念要求我们关注由多个敏感属性 $A=(A_1, \dots, A_k)$ 的笛卡尔积 $\mathcal{G} = \mathcal{A}_1 \times \cdots \times \mathcal{A}_k$ 所定义的所有细分群体 [@problem_id:4542381]。一个有力的公平性框架是**最差子群体风险 (worst-case subgroup risk)**，也称为**群体[分布鲁棒优化](@entry_id:636272) (Group DRO)**。其目标是最小化所有交叉子群体中的最大损失：
$R_{\text{fair}}(f_{\theta}) = \max_{a \in \mathcal{G}} \mathbb{E}[\ell(f_{\theta}(X), Y) \mid A=a]$
其中 $\ell$ 是[损失函数](@entry_id:136784)。这种方法的实证估计量相应地是所有子群体[经验风险](@entry_id:633993)的最大值：
$\widehat{R}_{\text{fair}}(f_{\theta}) = \max_{a \in \mathcal{G}} \frac{1}{n_a} \sum_{i: A_i=a} \ell(f_{\theta}(X_i), Y_i)$
其中 $n_a$ 是子群体 $a$ 的样本量 [@problem_id:4542381]。关注最差子群体的表现，可以防止模型通过牺牲小型、边缘化群体的利益来换取在大型群体或整体上的良好平均表现。

#### 因果公平性

统计公平性准则本质上是观察性的，它们描述了数据中的相关性，但未能触及偏见产生的因果机制。**因果公平性 (Causal Fairness)** 试图从“为什么”的层面来理解和定义公平。

其中一个核心概念是**反事实公平 (Counterfactual Fairness)** [@problem_id:4542359]。其核心思想是：对于同一个个体，如果其敏感属性在反事实的世界里发生了改变，而其他所有背景因素都保持不变，那么模型的预测结果不应该改变。在一个结构因果模型 (Structural Causal Model, SCM) 中，这可以形式化地表示为：
$\hat{Y}_{A \leftarrow a}(U) = \hat{Y}_{A \leftarrow a'}(U)$
对于所有的群体 $a, a'$ 和几乎所有的个体背景因素 $U$。

如何实现反事实公平？一个充分条件是，构建一个只依赖于那些在因果图中**不是**敏感属性 $A$ 的后代（descendants）的变量的预测器 [@problem_id:4542359]。换句话说，模型不应该使用任何被 $A$ 所影响的中间变量（mediators）。例如，如果种族 $A$ 影响了就诊频率 $X_1$，而就诊频率又影响了疾病诊断 $Y$，那么一个使用 $X_1$ 的模型就可能违反反事实公平，因为它将 $A$ 的部分影响（通过 $X_1$ 传递）纳入了预测。

需要强调的是，反事实公平（一个因果概念）与人口统计均等（一个统计概念）并不等价。一个反事实公平的模型，其预测在不同群体间的分布仍然可能不同。这通常发生在敏感属性 $A$ 和某个预测变量 $Z$ 共享一个共同的未观测祖先节点时（即存在混淆），即使 $Z$ 本身不是 $A$ 的后代 [@problem_id:4542359]。

#### 在时间到事件模型中的公平性

公平性的核心原则也可以被推广到更复杂的模型类型，例如用于预测生存时间的**时间到事件模型 (time-to-event models)**。在这类模型中，我们不仅关心事件是否发生，还关心事件发生的时间 $T$。模型的输出通常是一个预测的生存函数 $\hat{S}(t \mid X)$。

在这种背景下，公平性准则也需要相应地调整以适应时变性和数据删失（censoring）的特点 [@problem_id:4542368]：

*   **时变群体内校准**：对于任意一个时间点 $t$，模型预测的生存概率 $\hat{S}(t \mid X)$ 应该在每个群体内都是校准的。即：
    $P(T > t \mid \hat{S}(t \mid X) = s, A = a) = s$
    这确保了在任何时间点，对于任何群体的患者，预测的生存概率都有着准确的含义。

*   **时变判别能力的均等**：模型的判别能力——即区分高风险和低风险患者的能力——也应该在不同群体间均等。这通常通过**时变一致性指数 (time-dependent concordance index, C-index)** 来衡量。该指标评估的是对于一对可比较的患者（一个在时间 $t$ 前发生事件，另一个在 $t$ 后），模型能否正确地为前者赋予更高的风险评分。公平性要求这个在各个群体内部计算出的一致性指数是相等的，即 $C_{\text{td}}(t; a) = C_{\text{td}}(t; a')$。在存在右[删失数据](@entry_id:173222)的情况下，必须使用如 IPCW 等方法来获得对该指标的[无偏估计](@entry_id:756289)。

通过将核心的校准和均等原则应用于生存分析的特定框架，我们能够系统地评估和缓解复杂临床模型中的偏见。