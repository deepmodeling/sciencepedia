## 应用与跨学科连接

### 引言

在前面的内容中，我们已经详细阐述了倾向性评分（Propensity Score, PS）方法的基础理论和核心机制，包括[潜在结果框架](@entry_id:636884)、可交换性假设，以及匹配、分层和加权等基本技术。本章的目标是将这些理论知识付诸实践，探讨倾向性评分方法在多样化、复杂且充满挑战的真实世界研究场景中的应用。我们将不再重复介绍核心概念，而是聚焦于展示这些工具如何在不同的研究领域中被灵活运用、扩展和整合。

通过一系列跨学科的应用案例，我们将看到倾向性评分方法如何帮助研究人员在药物流行病学、药物基因组学、医疗人工智能（AI）、微生物组学和卫生经济学等领域中，从观察性数据中提取可靠的因果效应估计。本章旨在带领读者从理论原则走向实际应用，理解倾向性评分方法作为现代数据科学中一个强大且不可或缺的因果推断框架的真正价值。

### 临床与健康研究中的核心应用

倾向性评分方法在医学和公共卫生研究中得到了最广泛的应用，尤其是在评估治疗、干预措施或暴露因素的有效性和安全性方面。

#### 药物流行病学中的混杂控制

在药物流行病学中，一个核心挑战是“适应证混杂”（confounding by indication），即患者的疾病严重程度或其他预后因素既影响了医生选择何种治疗方案，也影响了最终的健康结局。这使得在接受不同治疗的患者组之间进行直接比较会产生偏倚。

例如，在一项使用常规实践数据评估噻嗪类利尿剂对原发性高血压患者平均动脉压（Mean Arterial Pressure, MAP）影响的研究中，医生可能会倾向于给病情更复杂的患者开具非利尿剂类的降压药。倾向性评分分析为此类问题提供了一个标准的解决方案。一个严谨的分析流程包括以下关键步骤：
1.  **协变量选择**：仅选择在治疗开始前测量的、可能同时影响治疗决策和结局（MAP）的基线协变量，如年龄、性别、基线MAP、体重指数（BMI）等，用于构建倾向性评分模型。一个常见的严重错误是包含了治疗后测量的变量，这会引入偏倚。
2.  **倾向性评分估计**：通常使用逻辑[回归模型](@entry_id:163386)估计每个患者接受噻嗪类[利尿剂](@entry_id:155404)治疗（$T=1$）的概率 $e(X)$。
3.  **诊断与评估**：在进行效应估计之前，必须进行两项关键的诊断检查。首先，通过绘制倾向性评分在不同治疗组的分布图来评估“重叠性”（overlap），确保对于所有协变量组合，患者都有可能接受任一治疗。其次，评估协变量的“平衡性”（balance），即在使用倾向性评分进行调整后（例如，通过加权或匹配），各治疗组的基线协变量分布是否相似。平衡性通常通过计算“标准化均数差”（Standardized Mean Difference, SMD）来衡量，一般认为SMD小于 $0.1$ 表示平衡性良好。依赖于样本量的[p值](@entry_id:136498)不被推荐用于平衡性检验。
4.  **效应估计**：在确认平衡性得到改善后，使用某种基于倾向性评分的方法来估计平均治疗效应（Average Treatment Effect, ATE）。

“逆概率治疗加权”（Inverse Probability of Treatment Weighting, IPTW）是其中最核心的技术之一。其基本思想是为每个研究对象赋予一个权重，这个权重等于他们所接受治疗的概率的倒数。具体而言，接受治疗（$T=1$）的个体权重为 $\frac{1}{\hat{e}(X_i)}$，未接受治疗（$T=0$）的个体权重为 $\frac{1}{1-\hat{e}(X_i)}$。通过这个加权过程，我们创建了一个“伪人群”（pseudo-population），在这个伪人群中，治疗分配与测量的基线协变量之间不再存在关联，从而消除了这些协变量带来的混杂。此时，ATE可以通过计算加权后的两组样本结果的均值差异来估计，其估计量为：
$$
\widehat{ATE} = \frac{1}{n} \sum_{i=1}^n \left( \frac{T_i Y_i}{\hat{e}(X_i)} - \frac{(1-T_i)Y_i}{1-\hat{e}(X_i)} \right)
$$
这个估计量在可交换性、正性假设成立以及倾向性评分模型设定正确的前提下，是ATE的一致估计。[@problem_id:4828155] [@problem_id:4599477]

#### 药物基因组学与个体化医疗

随着个体化医疗的发展，基因信息越来越多地被用于指导临床决策，这也为因果推断带来了新的挑战。在药物基因组学研究中，患者的特定基因型本身就可能成为一个强大的混杂因素。

考虑一项比较氯吡格雷与替格瑞洛在急性冠脉综合征患者中疗效的研究。氯吡格雷是一种[前体药物](@entry_id:263412)，需要经由肝脏中的CYP2C19酶代谢才能活化。对于携带CYP2C19功能缺失等位基因的患者，氯吡格雷效果可能较差。如果临床医生了解这一信息，他们可能更倾向于为这些患者选择不受该基因影响的替格瑞洛。在这种情况下，CYP2C19基因型就成为了一个典型的混杂因素，因为它既影响治疗选择，也与（在使用氯吡格雷时的）临床结局相关。为了得到无偏的治疗效果估计，必须将基因型以及其他可能相关的协变量（如由基因型反映的种族背景主成分）一同纳入倾向性评分模型中进行调整。

值得强调的是，**混杂**与**效应修饰**（effect modification）是两个不同的概念。CYP2C19基因型在此例中既是混杂因素，也是效应修饰因素（因为氯吡格雷的疗效在不同基因型亚组中不同）。倾向性评分方法旨在解决混杂问题，其估计的是人群的*平均*治疗效应，即使存在效应修饰，该平均效应依然是一个有意义的因果参数。事实上，若想在观察性研究中进一步探究效应修饰（即比较不同基因型亚组的治疗效应），研究者仍然需要在每个基因型亚组内部使用倾向性评分等方法来控制其他协变量的混杂。[@problem_id:4814015]

#### 评估临床决策支持与医疗AI

倾向性评分方法同样适用于评估新兴的医疗人工智能（AI）或临床决策支持（Clinical Decision Support, CDS）系统的效果。例如，我们可以研究一个AI警报系统是否能有效降低术后并发症的风险。在这种场景下，研究者可能更关心该系统对于那些实际被系统干预的患者群体（即“处理组”）的平均效应（Average Treatment effect on the Treated, ATT）。

倾向性评分匹配（Propensity Score Matching）是估计ATT的常用方法。其核心思想是为每个接受了干预的个体（$T=1$），从没有接受干预的个体（$T=0$）中寻找一个或多个倾向性评分值最接近的“反事实副本”。最简单的1:1最近邻匹配的ATT估计量可以表示为：
$$
\hat{\tau}_{\text{ATT}} = \frac{1}{N_1} \sum_{i \in \{T=1\}} (Y_i - Y_{j(i)})
$$
其中 $N_1$ 是处理组的样本量，$Y_i$ 是处理组个体的观察结果，$Y_{j(i)}$ 是与个体 $i$ 匹配上的[对照组](@entry_id:188599)个体的观察结果。这个过程为每个接受干预的个体构建了一个在基线特征上可比的对照，从而估计出干预对他们的特定影响。[@problem_id:5221156]

在实施匹配时，一些技术细节对保证结果的稳健性至关重要。
*   **在Logit尺度上匹配**：研究者通常选择在倾向性评分的Logit转换值（$\ln(\frac{e(X)}{1-e(X)})$）上而非原始概率值上进行匹配。这有两个主要原因：其一，如果倾向性评分由逻辑[回归模型](@entry_id:163386)估计，Logit值是协变量的[线性组合](@entry_id:155091)，在该尺度上匹配相当于直接控制了导致治疗分配差异的协变量线性成分；其二，Logit变换能够“拉伸”概率尺度两端（接近0或1的区域），使得在这些区域内，一个微小的概率差异会对应一个巨大的Logit尺度差异，从而有效避免将概率值看似接近但实际协变量特征差异巨大的个体匹配在一起。
*   **使用卡尺（Caliper）**：为了防止“劣质匹配”（即匹配对象的倾向性评分差异过大），研究者常会设定一个“卡尺”，即一个预设的最大允许匹配距离。任何超出这个距离的潜在匹配对象都会被排除。一个广泛采用的[经验法则](@entry_id:262201)是将卡尺宽度设为倾向性评分Logit尺度标准差的0.2倍。这个数值并非来自某个精确的数学定理，而是大量模拟研究中发现的、能够在“降低偏倚”（通过排除劣质匹配）和“增加方差”（由于样本量减少）之间取得良好平衡的实用选择。[@problem_id:5221122]

#### 新兴研究领域：微生物组学

倾向性评分方法的应用范围正不断扩展到新的生物医学领域，如微生物组学。例如，在研究新生儿早期抗生素暴露对其[肠道菌群](@entry_id:142053)多样性（如香农指数）的长期影响时，由于抗生素的使用决策受到多种围产期因素（如胎龄、分娩方式、母亲是否使用抗生素等）的影响，因此存在显著的混杂。

这类研究再次凸显了进行严格诊断评估的重要性。如前所述，**标准化均数差（SMD）**是评估协变量平衡性的金标准，因为它不受样本量影响。此外，研究者也在探索不同的倾向性评分应用策略以提高估计的稳健性。

除了传统的IPTW和匹配，**重叠权重（Overlap Weighting）**是另一种值得关注的加权方法。与旨在使加权后样本代表全人群（ATE）或处理组人群（ATT）的IPTW不同，重叠权重法通过给处理组个体赋予 $1-\hat{e}(X_i)$ 的权重，给[对照组](@entry_id:188599)个体赋予 $\hat{e}(X_i)$ 的权重，来重点关注那些处理组和[对照组](@entry_id:188599)特征高度重叠的“中间人群”。这种方法估计的因果参数被称为“重叠人群的平均治疗效应”（Average Treatment effect on the Overlap population, ATO），其正式定义为：
$$
\text{ATO} = \frac{\mathbb{E}\left[\tau(X)\, e(X)\left(1 - e(X)\right)\right]}{\mathbb{E}\left[e(X)\left(1 - e(X)\right)\right]}
$$
其中 $\tau(X)$ 是条件平均治疗效应。重叠权重法的优势在于其估计非常稳健，因为它天然地降低了倾向性评分接近0或1的个体的权重，避免了IPTW中常见的极端权重问题，从而使得估计量的方差更小。ATO所关注的人群——即那些治疗分配倾向最不明确的个体——通常也是临床决策中最具不确定性的群体，因此ATO本身就是一个具有重要临床和政策意义的因果参数。[@problem_id:5211115] [@problem_id:4599514]

最后，作为倾向性评分三大基本应用之一的**分层法（Stratification）**，其思想是根据倾向性评分值将样本分为若干个（如5个或10个）子层。在每个子层内部，个体的倾向性评分相似，因此协变量分布也近似平衡，可以直接比较处理组和[对照组](@entry_id:188599)的结局均值。最终的ATE估计值是所有子层内效应估计值的加权平均，权重为各子层在总样本中的占比。其估计量为：
$$
\widehat{ATE} = \sum_{k=1}^{K} \frac{N_k}{N} (\bar{Y}_{1k} - \bar{Y}_{0k})
$$
其中 $K$ 是子层数，$N_k$ 是第 $k$ 层的样本量，$N$ 是总样本量，$\bar{Y}_{1k}$ 和 $\bar{Y}_{0k}$ 分别是第 $k$ 层内处理组和[对照组](@entry_id:188599)的结局均值。[@problem_id:4599519]

### 高级方法学扩展

倾向性评分框架的强大之处在于其[可扩展性](@entry_id:636611)，能够应对更复杂的数据结构和研究问题。

#### 处理多分类治疗

当治疗方案不止两种时（例如，比较三种不同的肿瘤化疗方案），二元的倾向性评分定义需要被推广。对于一个有 $K$ 个[互斥](@entry_id:752349)治疗组的名义治疗变量 $T \in \{1, \dots, K\}$，其**广义倾向性评分（Generalized Propensity Score, GPS）**不再是一个标量，而是一个 $K$ 维的[概率向量](@entry_id:200434) $e(X) = (e_1(X), \dots, e_K(X))$，其中每个分量 $e_k(X) = P(T=k|X)$。由于各分量之和为1，这个向量实际定义在 $K-1$ 维的单纯形空间中。

为了在 $K$ 个治疗组之间同时平衡协变量，必须基于整个 $K-1$ 维的GPS向量（或其充分统计量，如[多项逻辑回归](@entry_id:275878)模型中的 $K-1$ 个[线性预测](@entry_id:180569)值）进行调整。仅仅根据其中一个分量（例如，$e_1(X)$）进行分层或匹配是**不充分**的，因为它只能保证治疗组1与其他所有组合并的组之间达到平衡，但无法保证治疗组2和治疗组3之间也[达到平衡](@entry_id:170346)。因此，对于多分类治疗，必须采用多维度的调整策略，例如在GPS[向量空间](@entry_id:177989)中进行[聚类分析](@entry_id:637205)或多维[网格划分](@entry_id:269463)，以确保在每个子类中的个体都具有相似的GPS向量，从而在所有 $K$ 个治疗组之间实现协变量平衡。[@problem_id:4599467]

#### 处理时变混杂：边际结构模型

在纵向研究中，当协变量和治疗都随时间变化时，会出现一种更复杂的混杂——**时变混杂（time-varying confounding）**。一个时变协变量（如某项实验室检查结果）可能既是过去治疗的结局，又是未来治疗决策的预测因子，同时也影响最终的临床结局。在这种情况下，传统的基于基线协变量的倾向性评分方法或标准[回归模型](@entry_id:163386)都会失效。

**边际结构模型（Marginal Structural Models, MSMs）**是为解决这一难题而设计的强大工具。MSM通过使用随时间变化的[逆概率](@entry_id:196307)权重来估计动态治疗方案（dynamic treatment regimes）的效果。具体来说，每个研究对象的权重是一个时间序列上各个时间点权重的连乘积。在每个时间点 $t$，权重是该个体在该时间点接受其实际所受治疗的概率的倒数，这个概率是基于其截至 $t$ 点的所有历史信息（包括过去的协变量和治疗）来计算的。

这个加权过程创建了一个伪人群，在这个伪人群中，时变混杂因素与后续治疗之间的关联在每个时间点都被打破，仿佛治疗是每个时间点在历史信息条件下被随机分配的。这一方法依赖于一个更强的假设——**序贯[可交换性](@entry_id:263314)（sequential ignorability）**，即在任何时间点，在给定已观察到的过去历史信息的条件下，当前的治疗分配与所有潜在的未来结局是独立的。当这个假设与一致性和正性假设都成立时（并且适当地处理了信息性审查），MSM就能够识别和估计动态治疗方案的因果效应。[@problem_id:4599479]

#### 处理协变量缺失数据

在处理真实世界数据（如电子健康记录，EHR）时，协变量数据缺失是一个普遍存在的问题。在进行倾向性评分分析之前，必须妥善处理这些缺失值。**[多重插补](@entry_id:177416)（Multiple Imputation, MI）**是处理该问题的标准方法，但其与倾向性评分方法的结合需要遵循严格的原则。

首先，我们需要理解[缺失数据](@entry_id:271026)的机制：
*   **[完全随机缺失](@entry_id:170286)（MCAR）**：缺失与任何观测或未观测的数据都无关。
*   **[随机缺失](@entry_id:168632)（MAR）**：缺失仅与观测到的数据有关。
*   **[非随机缺失](@entry_id:163489)（MNAR）**：缺失与未观测到的值本身有关。

在大多数EHR研究中，MCAR假设过于理想，而MAR是通常情况下我们所能依赖的最佳假设。MAR假设意味着，一个值的缺失概率可以依赖于其他观测到的变量，包括**治疗变量 $T$ 和结局变量 $Y$**。

这一事实引出了在MI和PS结合应用时一个至关重要的原则：**用于[插补](@entry_id:270805)缺失协变量的模型必须包含治疗变量 $T$ 和结局变量 $Y$**。如果从[插补模型](@entry_id:169403)中遗漏了结局变量 $Y$，就等于隐性地做出了一个更强的、通常不成立的MAR假设，这会导致[插补](@entry_id:270805)出的协变量分布不正确，进而使得后续的倾向性评分估计和因果效应估计产生偏倚。

此外，**共融性（congeniality）**原则要求[插补模型](@entry_id:169403)和分析模型（在这里是倾向性评分模型和最终的结局模型）之间要协调一致，即[插补模型](@entry_id:169403)应该至少和分析模型一样复杂，包含所有分析模型中涉及的变量和关系。一个标准的、正确的流程是：
1.  使用包含 $T$ 和 $Y$ 的模型创建 $M$ 个完整的数据集。
2.  在**每一个**[插补](@entry_id:270805)数据集上独立地进行完整的倾向性评分分析（包括PS估计、平衡性诊断和效应估计）。
3.  使用**鲁宾法则（Rubin's Rules）**合并这 $M$ 个效应估计值及其方差，得到最终的点估计和[置信区间](@entry_id:138194)。这个过程能够正确地将由于数据缺失所带来的[不确定性传播](@entry_id:146574)到最终的推断中。

如果数据是MNAR，那么标准的[多重插补](@entry_id:177416)和倾向性评分方法通常无法识别因果效应，需要依赖更强的、无法被数据验证的假设。[@problem_id:5221133] [@problem_id:4612514]

### 与现代[统计机器学习](@entry_id:636663)的联系

倾向性评分方法正与现代[统计机器学习](@entry_id:636663)技术深度融合，以应对高维数据、模型设定不确定性等前沿挑战。

#### 高维倾向性评分

在基因组学或生物信息学等领域，协变量的数量 $p$ 常常远大于样本量 $n$（即“高维”）。在这种情况下，标准的逻辑回归无法用于估计倾向性评分。研究者必须转向带有正则化功能的机器学习方法，例如**LASSO（Least Absolute Shrinkage and Selection Operator）回归**。

然而，使用这[类数](@entry_id:156164)据驱动的模型选择方法会带来“选择后推断”（post-selection inference）问题：如果我们用同一份数据来选择模型（例如，通过交叉验证选择[LASSO](@entry_id:751223)的惩罚参数）并进行最终的[统计推断](@entry_id:172747)（计算p值和[置信区间](@entry_id:138194)），那么得到的推断结果是无效的，因为它没有考虑到[模型选择](@entry_id:155601)过程本身的不确定性。

**双重/去偏机器学习（Double/Debiased Machine Learning, DML）**框架为解决这一问题提供了严谨的方案。它整合了三个关键思想：
1.  **估计“滋扰函数”**：使用灵活的机器学习方法（如LASSO）来估计倾向性评分 $e(X)$ 和条件结局模型 $m_t(X) = E[Y|T=t, X]$。这两个模型被认为是“滋扰函数”，因为我们对它们本身不感兴趣，但需要它们来估计我们关心的因果参数。
2.  **使用正交/双重稳健的估计量**：采用一种特殊的估计方程，该方程对滋扰函数的微小估计误差不敏感（即“奈曼正交性”）。**增广逆概率加权（Augmented IPTW, AIPW）**估计量是这类估计量的杰出代表。
3.  **交叉拟合（Cross-fitting）**：为了避免选择后推断偏倚，将样本随机分成 $K$ 折。对于每一折，滋扰函数都在另外的 $K-1$ 折数据上进行训练，然后用于对当前这一折进行预测。这确保了用于任何一个个体计算的滋扰函数值都来自于一个没有见过该个体数据的模型，从而打破了导致偏倚的统计依赖。

这一综合流程产生的ATE估计量是渐近正态的，并且可以构建有效的[置信区间](@entry_id:138194)，即使滋扰函数是用复杂的黑箱[机器学习模型](@entry_id:262335)估计的。[@problem_id:4599493]

#### 双重稳健与混合估计量

**双重稳健性（Double Robustness）**是现代因果推断中的一个核心概念。一个双重稳健的估计量拥有“两次机会做对”的优良特性：只要两个模型（倾向性评分模型和结局模型）中**至少有一个**设定正确，它就能得到一致的因果效应估计。

一个直观的例子是结合了分层法和回归调整的混合估计量。该方法首先根据倾向性评分将样本分层，然后在每个子层内部，拟合一个包含治疗变量和协变量的结局回归模型。最终的ATE估计值是各子层内经回归调整后的治疗效应的加权平均。这个估计量是双重稳健的：
*   如果倾向性评分模型正确，那么分层已经成功平衡了协变量，即使结局回归模型中的协变量部分设定错误，治疗变量的系数也能一致地估计出子层内的效应。
*   如果结局[回归模型](@entry_id:163386)正确，那么它能在每个子层内有效地控制残余的混杂，即使倾向性评分模型不正确（导致分层不完美），治疗变量的系数也能一致地估计出子层内的效应。

这种双重保护使得估计结果对模型设定的错误不那么敏感，从而更加可靠。[@problem_id:4599504]

#### 目标[最大似然估计](@entry_id:142509)（TMLE）

**目标最大似然估计（Targeted Maximum Likelihood Estimation, TMLE）**是另一种先进的、双重稳健的半[参数估计](@entry_id:139349)方法。它提供了一个优雅的框架来结合机器学习的灵活性和因果推断的严谨性。

TMLE的哲学是“靶向学习”。它分两步进行：
1.  **初始估计**：使用任意的机器学习方法（如Super Learner）得到对倾向性评分 $g(W)$ 和结局回归 $Q(A,W)$ 的初始估计 $g_0(W)$ 和 $Q_0(A,W)$。
2.  **靶向更新**：不直接使用初始的结局模型估计，而是对其进行一个巧妙的、有针对性的“波动”（fluctuation）更新。这一步会拟合一个小的参数子模型，该模型使用一个由倾向性评分构成的“巧妙协变量”（clever covariate），其目标是精确地求解“高效影响函数”方程。通过这个更新步骤，得到的最终结局模型 $Q^*(A,W)$ 被“靶向”优化，使得基于它计算出的ATE估计量具有最小的渐近偏倚和方差。

TMLE同样是双重稳健的，并且具有优良的统计特性，代表了因果推断领域的前沿方向。[@problem_id:5221171]

### 倾向性评分在因果推断工具箱中的定位

最后，将倾向性评分方法与其他因果推断工具进行比较，有助于我们理解其独特的优势和局限性。

#### 倾向性评分 vs. [工具变量](@entry_id:142324)

**[工具变量](@entry_id:142324)（Instrumental Variables, IV）**方法是处理**未测量混杂**的经典工具。当存在一个我们无法测量到的混杂因素 $U$ 时，倾向性评分方法（依赖于“所有混杂因素都已测量”的[可交换性](@entry_id:263314)假设）会失效。此时，如果能找到一个有效的[工具变量](@entry_id:142324) $Z$，IV方法就能派上用场。

一个有效的[工具变量](@entry_id:142324) $Z$ 必须满足三个核心假设：
1.  **相关性（Relevance）**：$Z$ 必须与治疗变量 $T$ 相关。
2.  **独立性（Independence / Exogeneity）**：$Z$ 必须与任何影响结局 $Y$ 的未测量混杂因素 $U$ 无关。
3.  **排他性限制（Exclusion Restriction）**：$Z$ 影响结局 $Y$ 的唯一路径是通过治疗变量 $T$。

例如，在卫生经济学研究中，某项医保报销政策的变化（如降低某种新药的共付费用）可以作为一个[工具变量](@entry_id:142324)。这项政策变化会影响患者选择该新药的概率（满足相关性），但政策本身（通常）与患者的内在健康状况（未测量的混杂因素）无关（满足独立性），并且除了通过影响药物选择外，不会直接影响患者的生理结局（满足排他性限制）。

当面对未测量混杂时，IV方法显然优于PS方法。然而，两者之间存在一个关键区别：
*   **估计对象不同**：PS方法通常估计的是全人群（ATE）或处理组人群（ATT）的平均效应。而IV方法在标准设定下估计的是**局部平均治疗效应（Local Average Treatment Effect, LATE）**，即仅对那些因为工具变量的改变而改变了自己治疗选择的“依从者”亚群的平均治疗效应。LATE所对应的亚群虽然可能不是全体人群，但通常具有明确的政策或临床意义。
*   **假设不同**：PS方法依赖于“可交换性”假设，而IV方法依赖于上述三个核心IV假设。这两个假设集都是不可被数据完全检验的。

因此，在PS和IV之间做选择，不仅取决于研究者对哪一套假设更有信心，也取决于研究者最关心的因果问题和目标人群。两者并非互相排斥，而是因果推断工具箱中应对不同研究挑战的互补工具。[@problem_id:5051592]

### 结论

本章通过一系列真实世界的研究案例，系统地展示了倾向性评分方法的广泛应用与深刻内涵。我们看到，它不仅仅是一种控制基线混杂的静态技术，更是一个能够被扩展和改造的动态框架。从经典的药物流行病学应用，到处理多分类治疗、时变混杂和缺失数据等复杂情况，再到与高维机器学习和双重[稳健估计](@entry_id:261282)等前沿理论的融合，倾向性评分方法始终处于应用统计学和数据科学发展的核心。通过理解其在不同场景下的应用、假设和局限性，并将其与工具变量等其他因果推断方法进行比较，研究者可以更有效地利用这一强大工具，从纷繁复杂的观察性数据中获得更可靠、更有价值的因果洞见。