## 引言
随着智能手机和可穿戴设备的普及，我们获得了一个前所未有的机会，能够以高分辨率、客观、连续的方式捕捉人类在自然环境中的行为与生理状态。这种利用个人数字设备被动收集的数据来量化个体表型的方法，被称为**数字表型分析 (digital phenotyping)**。它正逐渐成为理解人类健康和疾病的革命性工具。

传统的临床评估和心理学研究方法，如回顾性问卷和实验室观察，往往存在[采样频率](@entry_id:264884)低、依赖主观回忆、无法反映真实生活动态等局限性，从而在捕捉疾病的细微波动和行为的复杂模式方面留下了巨大的知识鸿沟。数字表型分析旨在通过被动、纵向的[数据采集](@entry_id:273490)，填补这一鸿沟，为[精准医疗](@entry_id:152668)和行为科学提供更精细、更客观的数据基础。

本文将系统性地引导您进入这一前沿领域。我们首先将在**“原理与机制”**一章中，深入剖析从原始传感器信号到可解释行为特征的完整技术流程，为您构建坚实的理论基础。接着，在**“应用与跨学科连接”**一章中，我们将展示数字表型分析如何在临床医学、心理健康和[计算社会科学](@entry_id:269777)等领域催生创新应用。最后，通过**“动手实践”**环节，您将有机会亲手处理真实世界的传感数据，巩固所学知识。通过这三个章节的学习，您将全面掌握数字表型分析的核心概念、关键技术与实际应用。

## 原理与机制

本章深入探讨了驱动数字表型分析的核心原理和技术机制。我们将剖析从原始传感器数据到可解释行为特征的完[整流](@entry_id:197363)程，涵盖[数据采集](@entry_id:273490)、信号处理、[特征工程](@entry_id:174925)、统计建模以及评估中的关键方法论挑战。本章旨在为读者构建一个坚实的理论基础，以便能够严谨地设计、实施和解读数字表型研究。

### 从数字表型分析到数字生物标志物

在深入技术细节之前，我们必须首先精确区分两个核心概念：**数字表型分析（digital phenotyping）**与**数字生物标志物（digital biomarker）**。这两个术语虽然相关，但描述的是科学探究中不同层面和严谨程度的活动。

**数字表型分析**是一个广义的、探索性的过程，其目标是利用移动和可穿戴设备被动、连续、纵向地收集的数据，构建个体在自然环境中的高维行为与生理[特征图](@entry_id:637719)谱。这个过程始于采集一系列原始传感器数据流，这些[数据流](@entry_id:748201)可以被建模为一组[随机过程](@entry_id:268487) $\\{s_i(t)\\}$，其中 $i$ 表示传感器类型（如加速度计、GPS），$t$ 表示连续时间。通过以[采样频率](@entry_id:264884) $f_{s,i}$ 进行采样，我们得到离散时间序列 $\\{x_i[n]\}$。接着，通过一个特征提取映射 $\phi$，这些原始数据被转换为一个多变量的特征轨迹 $\mathbf{y}(t) = (y_1(t), \dots, y_p(t))$。这个轨迹 $\mathbf{y}(t)$ 就是个体的**数字表型**——一个描述其在一段时间内（例如 $t \in [0,T]$）可观察行为和生理模式的丰富数据集。数字表型分析的本质是描述和量化，旨在全面地刻画个体。

与此相对，**数字生物标志物**则是一个具体的、经过严格验证的指标。它通常是数字表型轨迹 $\mathbf{y}(t)$ 中的某个特定特征 $y_j(t)$ 或其派生出的统计量。要从一个描述性特征晋升为生物标志物，它必须满足经典[测量理论](@entry_id:153616)中的严格标准 [@problem_id:4557362]。这些标准主要包括：

1.  **构念效度（Construct Validity）**：该特征必须被证实能够准确测量其意图测量的潜在构念 $Z(t)$（例如，抑郁严重程度、睡眠质量）。构念效度的验证涉及：
    *   **聚合效度（Convergent Validity）**：特征 $y_j(t)$ 必须与测量相同或相似构念的“金标准”$G(t)$（如临床量表、实验室检查）表现出强相关性，即相关系数 $|r(y_j(t), G(t))|$ 较高。
    *   **区分效度（Discriminant Validity）**：该特征不应与理论上不相关的构念的测量指标有强相关性。

2.  **信度（Reliability）**：测量结果必须是稳定和可复现的。信度的评估包括：
    *   **重测信度（Test-Retest Reliability）**：在相似条件下，不同时间点的测量结果应具有一致性。
    *   **组内[相关系数](@entry_id:147037)（Intraclass Correlation Coefficient, ICC）**：ICC 量化了测量变异的来源，其定义为 $ICC = \frac{\sigma^2_{\text{between}}}{\sigma^2_{\text{between}} + \sigma^2_{\text{within}}}$。一个高 ICC 值（接近 1）表明测量值的变异主要来源于被试*之间*的真实差异（$\sigma^2_{\text{between}}$），而非单个被试重复测量*内部*的[随机误差](@entry_id:144890)或波动（$\sigma^2_{\text{within}}$）。因此，一个可靠的生物标志物必须满足 $\sigma^2_{\text{between}} \gg \sigma^2_{\text{within}}$。

综上所述，数字表型分析是生成丰富行为数据集的“上游”过程，而数字生物标志物则是从该数据集中筛选并经过严格验证、用于特定目的（如诊断、预后或治疗[反应监测](@entry_id:201786)）的“下游”产物。

### 传感器的工作原理与信号特性

数字表型的基石是传感器。对传感器物理原理的深刻理解对于数据解读、噪声识别和伪影（artifact）处理至关重要。考虑一个典型的数字表型研究，参与者佩戴腕带并携带智能手机，在日常生活中（如安静地坐着、在楼宇间行走）被动地收集数据 [@problem_id:4557367]。以下是几种核心传感器的原理及其在真实世界中的信号特征。

**加速度计（Accelerometer）**：现代智能手机和可穿戴设备中的微机电系统（MEMS）加速度计并非直接测量加速度，而是测量**比力（specific force）** $f$。比力是作用在传感器内部一个微小“检验质量”（proof mass）上的非[引力](@entry_id:189550)总和，等于传感器的自身加速度 $a$ 与局部[引力](@entry_id:189550)加速度 $g$ 的矢量差，即 $f = a - g$。因此，当设备静止时（$a=0$），它会测量到一个指向地心[引力](@entry_id:189550)相反方向、大小约为 $9.8 \, \mathrm{m/s^2}$ 的恒定向量。在行走等周期性活动中，加速度计信号会表现出与步频（通常为 $1-3 \, \mathrm{Hz}$）及其[谐波](@entry_id:170943)相关的显著周期性成分。其主要噪声源包括热噪声和电子[白噪声](@entry_id:145248)，以及导致积分后产生速度和位置漂移的低频偏置不稳定性。

**陀螺仪（Gyroscope）**：MEMS 陀螺仪用于测量**角速度** $\omega$。其工作原理通常基于**[科里奥利效应](@entry_id:168866)**：在一个振动的检验质量上，外部旋转会产生一个垂直于振动和旋转轴的科里奥利力，通过测量这个力可以推算出角速度。在日常活动中，如行走时的手臂摆动或身体转向，陀螺仪信号的能量主要集中在 $5 \, \mathrm{Hz}$ 以下。其噪声主要包括导致积分后产生显著方向漂移的偏置不稳定性（低频）和角度随机游走（高频）。

**全球定位系统（GPS）**：GPS 接收器通过测量其到多颗卫星信号的**飞行时间（time-of-flight）**来计算自身位置。这些时间测量值（伪距）构成一个方程组，通过求解该方程组可得到接收器的三维坐标及其内部时钟偏差。在“城市峡谷”（高楼林立的街道）中，主要的误差来源是**多路径效应**，即信号经建筑物反射后到达接收器，导致测量路径变长，产生大的、时间相关的定位误差。而在室内，微弱的卫星信号通常会被严重衰减，导致接收器频繁“失锁”，输出跳跃性或缺失的定位点。GPS 的采样率通常较低（约 $1 \, \mathrm{Hz}$）。

**光电容积描记（Photoplethysmography, PPG）**：腕带式心率监测器大多采用 PPG 技术。它向皮肤组织发射光线（通常是绿光或红外光），然后测量反射或透射的[光强度](@entry_id:177094)。其原理基于**比尔-朗伯定律（Beer-Lambert law）**。心脏搏动导致动脉血管中的血容量发生周期性变化，从而调制了光的吸收量。这会在一个较大的直流（DC）基线上产生一个微小的交流（AC）信号，即光电容积图。这个 AC 信号的基频就是心率（例如，$60 \, \mathrm{bpm}$ 的心率对应约 $1 \, \mathrm{Hz}$ 的信号）。PPG 的主要噪声来源是**运动伪影**，由传感器与皮肤之间的相对位移引起，其幅度可能比生理信号大几个数量级。此外，若遮光不佳，[日光灯](@entry_id:189788)以两倍于电网频率（如 $100$ 或 $120 \, \mathrm{Hz}$）的频率闪烁，也会引入环境光噪声。

**皮电活动（Electrodermal Activity, [EDA](@entry_id:172341)）**：也称为皮肤电导（skin conductance）。[EDA](@entry_id:172341) 传感器通过在两个电极间施加一个微弱、无感的电压或电流，并根据欧姆定律测量相应的电流或电压来计算皮肤电导。电导的变化主要由受交感神经系统控制的汗腺导管中充满离子的汗液所引起。[EDA](@entry_id:172341) 是一个非常低频的信号，主要包含两种成分：在数秒到数分钟尺度上缓慢变化的**紧张性皮电水平（Tonic Skin Conductance Level, SCL）**，以及响应内外刺激、具有秒级[上升时间](@entry_id:263755)和较长恢复时间的瞬时**时相性皮电反应（Phasic Skin Conductance Responses, SCRs）**。其噪声源包括影响电极接触的运动伪影和温度引起的漂移。[EDA](@entry_id:172341) 信号在心率频率范围内几乎没有能量。

### 数据采集与处理的基本参数

将连续的物理世界转化为离散的[数字信号](@entry_id:188520)，再从中提取有意义的特征，这一过程受到一系列关键参数的制约。这些参数的选择直接影响了我们能从数据中恢复的行为和生理现象的[时间分辨率](@entry_id:194281) [@problem_id:4557338]。我们以一个利用加速度计监测步态的研究为例，探讨几个核心参数。

**采样频率（Sampling Frequency, $f_s$）**：这是传感器在工作状态下每秒采集的样本数。根据**[奈奎斯特-香农采样定理](@entry_id:262499)（Nyquist-Shannon sampling theorem）**，为了无失真地恢复一个信号，[采样频率](@entry_id:264884)必须严格大于该信号最高频率分量的两倍。这个临界频率被称为奈奎斯特率。反之，对于给定的[采样频率](@entry_id:264884) $f_s$，能够被唯一表示的最高频率是**[奈奎斯特频率](@entry_id:276417)**，定义为 $f_N = f_s / 2$。例如，如果加速度计以 $f_s = 40 \, \mathrm{Hz}$ 采样，那么它能捕捉的最高频率为 $20 \, \mathrm{Hz}$。这对于分析频率范围在 $0.5$ 到 $3 \, \mathrm{Hz}$ 的步态 cadence 来说是绰绰有余的，可以有效避免**混叠（aliasing）**现象。

**窗口长度（Window Length, $w$）与重叠（Overlap, $o$）**：由于行为和生理状态是动态变化的，我们通常将时间序列分割成短的、固定长度的窗口进行分析。
*   **窗口长度 $w$** 决定了**频[谱分辨率](@entry_id:263022)** $\Delta f$，即区分两个相近频率的能力。根据傅里叶变换的[不确定性原理](@entry_id:141278)，频[谱分辨率](@entry_id:263022)与窗口长度成反比，近似为 $\Delta f \approx 1/w$。例如，一个 $5$ 秒的窗口能提供约 $0.2 \, \mathrm{Hz}$ 的[频率分辨率](@entry_id:143240)，这决定了我们估计步频的精度。
*   **重叠 $o$** 指的是相邻窗口之间数据复用的比例。它决定了特征流的**更新速率**。两个连续窗口起始点之间的时间间隔被称为**跳跃尺寸（hop size）** $h$，计算公式为 $h = w \times (1 - o)$。例如，当窗口长度 $w=5$ 秒，重叠率 $o=0.8$ 时，跳跃尺寸为 $h = 5 \times (1-0.8) = 1$ 秒。这意味着我们每秒钟都能计算一组新的特征，从而以 $1$ 秒的粒度追踪行为的变化。

**任务周期（Duty Cycling, $T_{\text{on}}, T_{\text{off}}$）**：为了节省电池电量，许多移动传感应用采用任务周期策略，即传感器在开启（$T_{\text{on}}$）和关闭（$T_{\text{off}}$）之间循环。占空比 $D = T_{\text{on}}/(T_{\text{on}} + T_{\text{off}})$。这种策略对可恢复的[时间分辨率](@entry_id:194281)有巨大影响。在不进行复杂插值的情况下，任何完全发生在 $T_{\text{off}}$ 时间段内的短暂事件或状态转换都将被完全错过。因此，即使在传感器开启期间我们能以秒级精度更新特征，但对于保证捕捉到所有状态转换而言，其时间分辨率被粗化到了 $T_{\text{off}}$ 的尺度。例如，如果 $T_{\text{off}} = 45$ 秒，那么我们无法保证检测到任何持续时间小于 $45$ 秒并完全落入关闭窗口的事件 [@problem_id:4557338]。

这三个层面的时间分辨率——微观的采样率、中观的特征更新率、宏观的监测连续性——共同定义了数字表型分析系统的时间解析能力。

### 从原始信号到可解释特征

原始传感器数据本身往往充满噪声且难以直接解释。[特征工程](@entry_id:174925)（feature engineering）是将其转化为有意义、可解释的量化指标的关键步骤，它通常包括信号预处理和特征提取两个阶段。

#### 信号预处理与滤波

预处理的首要任务是滤除噪声和伪影，分离出我们感兴趣的生理信号。**[数字滤波器](@entry_id:181052)（digital filters）**是实现这一目标的标准工具 [@problem_id:4557403]。[线性时不变](@entry_id:276287)（LTI）滤波器通过其[频率响应](@entry_id:183149) $H(e^{j\omega})$ 来表征，它决定了信号中每个频率分量的幅度和相位将如何被改变。

*   **低通滤波器（Low-pass filter）**：保留低于某个[截止频率](@entry_id:276383) $\omega_c$ 的分量，衰减高于 $\omega_c$ 的分量。例如，要从 PPG 信号中提取呼吸信号（通常在 $0.1-0.5 \, \mathrm{Hz}$），我们可以使用一个截止频率约为 $0.5 \, \mathrm{Hz}$ 的低通滤波器，以抑制更高频率的心脏搏动成分。

*   **[高通滤波器](@entry_id:274953)（High-pass filter）**：保留高于 $\omega_c$ 的分量，衰减低于 $\omega_c$ 的分量。例如，在处理 PPG 信号以估计心率时，可以使用一个[截止频率](@entry_id:276383)约为 $0.3-0.5 \, \mathrm{Hz}$ 的[高通滤波器](@entry_id:274953)来移除基线漂移和低频运动伪影。

*   **带通滤波器（Band-pass filter）**：仅保留某个频率带 $[\omega_1, \omega_2]$ 内的分量。这是分离特定[生理节律](@entry_id:150420)最常用的方法。例如，要从 PPG 信号中分离出心率（通常在 $0.67-3.0 \, \mathrm{Hz}$），可以设计一个[通带](@entry_id:276907)大约在 $0.7-5.0 \, \mathrm{Hz}$ 的带通滤波器。这个[通带](@entry_id:276907)既能覆盖正常的心率范围，又能包含其[谐波](@entry_id:170943)成分（这对于峰值检测很重要），同时滤除低频噪声和高频噪声。

在选择滤波器时，除了[幅度响应](@entry_id:271115)，**相位响应**也至关重要。非线性的相位响应会使信号的不同频率成分产生不同的时间延迟，从而扭[曲波](@entry_id:748118)形。这对于需要精确计时（如计算心跳间期 IBI）的应用是致命的。

*   **[线性相位](@entry_id:274637) FIR 滤波器**：具有对称系数的[有限脉冲响应](@entry_id:192542)（FIR）滤波器可以实现精确的[线性相位](@entry_id:274637)，这意味着所有频率成分被延迟相同的时间，从而保持了波形形状。因此，在需要保留波形形态的应用中，它们是理想选择。

*   **[零相位滤波](@entry_id:262381)**：无限脉冲响应（IIR）滤波器（如[巴特沃斯滤波器](@entry_id:276314)）虽然可以实现非常平坦的[通带](@entry_id:276907)和陡峭的过渡带，但其相位响应是天然非线性的。为了克服这一点，可以采用**前向-后向滤波（forward-backward filtering）**。该方法先对信号进行一次滤波，然后将信号反转，再用同一个滤波器进行一次滤波。两次滤波的相位延迟相互抵消，最终实现零[相位失真](@entry_id:184482)。这样做的代价是滤波器的[幅度响应](@entry_id:271115)被平方（$|H(e^{j\omega})|^2$），这会使过渡带更陡峭，但也会增加信号两端的瞬态效应 [@problem_id:4557403]。

#### 特征的提取与分类

经过预处理后，我们从干净的信号窗口中提取各种特征，以量化行为和生理的不同方面 [@problem_id:4557334]。这些特征大致可分为三类：

1.  **时域特征（Time-Domain Features）**：直接在信号的时间波形上计算，描述信号的总体统计特性。
    *   **均值（Mean）** $\mu_w = \frac{1}{L}\sum_{t=1}^{L} x_t$：表示信号在一个窗口内的平均水平，如平均活动强度。
    *   **方差（Variance）** $\sigma_w^2 = \frac{1}{L}\sum_{t=1}^{L} (x_t - \mu_w)^2$：衡量信号的波动性或离散程度。更大的方差意味着更大的行为变异性。
    *   **百分位数（Percentiles）** $p_\alpha$：描述信号的分布形态。例如，第95百分位数 $p_{0.95}$ 的值较高，可能表示存在短暂但高强度的活动。

2.  **频域特征（Frequency-Domain Features）**：通过傅里叶变换等方法将信号转换到频域，分析其振荡和周期性内容。
    *   **[功率谱密度](@entry_id:141002)（Power Spectral Density, PSD）** $S_x(f)$：描述了[信号功率](@entry_id:273924)在不同频率上的分布。根据**维纳-辛钦定理（Wiener-Khinchin theorem）**，它是信号[自相关函数](@entry_id:138327)的傅里叶变换。PSD 中的峰值表示信号中存在主导的周期性成分，例如，在步态对应的频率（约 $1-3 \, \mathrm{Hz}$）或昼夜节律对应的频率（约 $1/24 \, \mathrm{h}^{-1}$）上的峰值，分别表示规律的步行或稳定的日常作息。
    *   **谱熵（Spectral Entropy）** $H = -\sum_{f} p(f)\,\log p(f)$：这是对归一化[功率谱](@entry_id:159996) $p(f)$ 计算的香农熵。它衡量了[功率谱](@entry_id:159996)的平坦程度。低谱熵意味着功率集中在少数几个频率上，代表信号规则、单一（如稳定的行走或睡眠）；高谱熵意味着功率均匀分布在多个频率上，代表信号复杂、不规则（如多样的日常活动或混乱的行为模式）。

3.  **非线性特征（Nonlinear Features）**：超越了线性和[频域分析](@entry_id:265642)，用于捕捉信号的复杂性、规律性和可预测性。
    *   **样本熵（Sample Entropy, SampEn）**：量化时间序列的规律性或可预测性。其定义为在序列中长度为 $m$ 的模板能够匹配（在一定容差 $r$ 内）的条件下，这些模板在长度增加到 $m+1$ 时仍然能够匹配的负对数条件概率，并且在计数中排除了自匹配。较低的样本熵表示信号高度可预测和规律（如重复性任务或稳定的睡眠-觉醒周期），而较高的样本熵则表示信号不规则、复杂或难以预测（如混乱的日常安排或受到干扰的[生理节律](@entry_id:150420)）。

### 表型的验证与建模

从传感器数据中提取出特征后，接下来的核心任务是验证这些特征的意义，并将它们与临床相关的结果联系起来。

#### 效度、信度与地面[真值](@entry_id:636547)

一个数字表型或生物标志物的价值取决于其测量的有效性和可靠性。我们在本章开头已经讨论了这些概念的定义，现在我们将其置于一个具体的研究设计中来理解它们的实际应用 [@problem_id:4557336]。

假设我们正在进行一项研究，旨在利用手机和可穿戴设备监测抑郁症的严重程度。要评估我们从GPS、加速度计等传感器提取的特征（如移动半径、活动规律性）是否有效，我们需要一个可靠的参考标准，即**地面[真值](@entry_id:636547)（ground truth）**。在临床研究中，地面真值通常来自于金标准临床评估（如医生填写的汉密尔顿抑郁量表）或经过验证的自评问卷（如PHQ-9）。然而，这些评估通常是低频的（如每周或每两周一次），且依赖于回顾性记忆。

为了获得更高时间分辨率、更贴近即时体验的地面[真值](@entry_id:636547)，研究人员常采用**生态瞬时评估（Ecological Momentary Assessment, EMA）**。EMA是一种在参与者的自然环境中，通过智能手机等设备，每天多次实时或近实时地收集其主观体验（如情绪、疲劳感）、行为和情境信息的方法。其“瞬时”特性旨在最大限度地减少回忆偏见，捕捉状态的动态波动。

有了地面真值后，我们就可以系统地评估我们数字表型的**效度（validity）**：
*   **内容效度（Content Validity）**：评估我们的传感器特征是否全面地覆盖了抑郁症的各个理论方面。这需要领域专家进行判断，例如，他们会评估我们从加速度计提取的“日间活动规律性”特征是否能合理地代表抑郁症的“精神运动改变”症状，从手机通话和短信[元数据](@entry_id:275500)提取的社交特征是否能代表“社交退缩”。
*   **构念效度（Construct Validity）**：检验我们的特征是否符合关于抑郁症的理论假设。例如，根据抑郁理论，社交退缩是核心症状之一，那么我们应该会发现，GPS移动半径的减小和通话对象多样性的降低（聚合效度）与临床医生在同一次访视中评定的负性症状得分呈正相关。同时，这些特征不应与理论上无关的构念（如躁狂发作）相关（区分效度）。
*   **效标效度（Criterion Validity）**：评估我们的测量在多大程度上与一个外部“金标准”效标相关。
    *   **同期效度（Concurrent Validity）**：当我们的测量和效标在同一时间点进行时。例如，验证腕带PPG测量的心率是否与同时记录的心电图（ECG）心率一致。
    *   **预测效度（Predictive Validity）**：当我们的测量被用来预测未来的效标结果时。例如，检验本周通过传感器数据计算的睡眠不规律性复合特征，是否能预测下一周的PHQ-9抑郁问卷得分。

#### 纵向[数据建模](@entry_id:141456)

数字表型数据本质上是**纵向数据（longitudinal data）**，即对同一批个体进行重复测量。这种数据结构的一个关键特征是，来自同一个体的观测值之间通常是相关的，而不是独立的。在建模时忽略这种内部相关性会导致错误的推断。

**广义线性混合效应模型（Generalized Linear Mixed-Effects Models, GLMMs）**是分析此类数据的强大框架 [@problem_id:4557342]。它们是[广义线性模型](@entry_id:171019)（GLM）的扩展，通过引入**随机效应（random effects）**来处理组内相关性。

在一个利用传感器特征预测每日情绪症状（[二元结果](@entry_id:173636) $y_{it} \in \{0,1\}$，其中 $i$ 是个体，$t$ 是天）的场景中，一个典型的GLMM可以这样构建：
首先，我们假设 $y_{it}$ 服从**[伯努利分布](@entry_id:266933)**，并通过 **logit 链接函数**将其概率 $p_{it}$ 与[线性预测](@entry_id:180569)器 $\eta_{it}$ 联系起来：$\mathrm{logit}(p_{it}) = \log(p_{it}/(1-p_{it})) = \eta_{it}$。

GLMM 的核心在于[线性预测](@entry_id:180569)器 $\eta_{it}$ 的结构，它同时包含**固定效应（fixed effects）**和**随机效应（random effects）**。固定效应代表了在整个群体中普适的平均效应，而随机效应则捕捉了个体间的异质性。一个包含随机截距和随机斜率的模型形式如下：
$$ \mathrm{logit}(p_{it}) = (\beta_0 + b_{0i}) + (\beta_1 + b_{1i}) x_{it} + \beta_2 z_{it} $$
其中：
*   $\beta_0, \beta_1, \beta_2$ 是固定效应，分别代表群体的平均基线[对数几率](@entry_id:141427)、活动强度 $x_{it}$ 的平均效应和睡眠时长 $z_{it}$ 的平均效应。
*   $b_{0i}$ 是个体 $i$ 的**随机截距（random intercept）**。它表示个体 $i$ 的基线对数几率相对于群体平均水平的偏离。$b_{0i}$ 对每个个体是一个常数（不随时间 $t$ 变化），捕捉了某些人天生就比其他人更容易出现症状的持续性差异。
*   $b_{1i}$ 是个体 $i$ 关于预测变量 $x_{it}$ 的**随机斜率（random slope）**。它表示个体 $i$ 对活动强度的反应（即活动对症状几率的影响）相对于群体平均效应的偏离。这允许模型捕捉到“活动对某些人更有效”的个体化效应。

随机效应向量 $\mathbf{b}_i = (b_{0i}, b_{1i})^\top$ 通常被假设服从一个均值为零的[多元正态分布](@entry_id:175229)，$\mathbf{b}_i \sim \mathcal{N}(\mathbf{0}, \mathbf{D})$。协方差矩阵 $\mathbf{D}$ 描述了随机效应的大小和它们之间的相关性。例如，如果 $\mathrm{Cov}(b_{0i}, b_{1i}) > 0$，则意味着那些基线风险较高的个体（$b_{0i}>0$），其症状对活动的反应也往往更积极（$b_{1i}>0$）。通过这种方式，GLMMs不仅能估计群体平均趋势，还能精细地刻画个体间的异质性。

### 方法论挑战与最佳实践

在数字表型研究的实践中，研究者会面临一系列严峻的方法论挑战。妥善处理这些挑战对于保证研究结果的有效性和可信度至关重要。

#### 缺失数据的处理

在真实的移动传感环境中，数据缺失是常态而非例外。手机关机、电池耗尽、软件崩溃或用户主动关闭应用等都可能导致数据丢失。如何处理这些[缺失数据](@entry_id:271026)，取决于其背后的机制 [@problem_id:4557356]。[缺失数据机制](@entry_id:173251)通常分为三类：

1.  **[完全随机缺失](@entry_id:170286)（Missing Completely At Random, MCAR）**：缺失的发生与任何已观测或未观测的数据都无关。例如，传感器因一个完全随机的硬件故障而停止工作。此时，缺失概率 $p(R_t=1 \mid Y_t, D, X_t)$ 是一个常数。在这种理想情况下，仅对完整数据进行分析（complete-case analysis）虽然会损失统计功效，但不会引入系统性偏差。

2.  **[随机缺失](@entry_id:168632)（Missing At Random, MAR）**：缺失的发生与未观测的数据本身无关，但可能与其他已观测的数据相关。例如，当手机电池电量（已观测变量 $X_t$）过低时，系统会自动关闭数据采集应用，导致活动数据（$Y_t$）缺失。此时，缺失概率仅依赖于已观测变量，即 $p(R_t=1 \mid Y_t, D, X_t) = g(D, X_t)$。在MAR假设下，缺失机制对于基于似然的推断是“可忽略的”（ignorable），前提是[参数空间](@entry_id:178581)可分。这意味着，只要我们的[统计模型](@entry_id:755400)能够正确地利用所有已[观测信息](@entry_id:165764)（例如，通过[多重插补](@entry_id:177416)或逆概率加权），我们仍然可以获得对模型参数的无偏估计。

3.  **[非随机缺失](@entry_id:163489)（Missing Not At Random, MNAR）**：缺失的发生与未观测的数据值本身直接相关。例如，抑郁症患者在情绪极度低落（对应低的活动水平 $Y_t$）时，可能倾向于将手机关机，导致活动数据缺失。此时，缺失概率依赖于 $Y_t$ 本身，$p(R_t=1 \mid Y_t, D, X_t) = h(Y_t, D, X_t)$。MNAR是最棘手的情况，因为缺失机制是“不可忽略的”。简单地忽略缺失或使用为MAR设计的方法通常会导致有偏的估计。处理MNAR需要对缺失机制本身进行建模，这往往需要强有力的、无法从数据本身验证的假设，或者需要借助外部信息进行敏感性分析。

#### 模型的稳健评估

由于数字表型数据具有复杂的依赖结构（如时间[自相关](@entry_id:138991)、个体内重复测量），评估模型的泛化性能时极易因**信息泄露（information leakage）**而产生过于乐观的偏差。因此，必须采用能够尊重[数据结构](@entry_id:262134)的评估协议 [@problem_id:4557345]。

标准的随机K折交叉验证在这里是完全不适用的，因为它会打乱数据的时间和个体结构，导致来自同一个体的、时间上邻近的数据点被分到[训练集](@entry_id:636396)和[测试集](@entry_id:637546)中，从而使模型看似表现优异。正确的评估策略取决于模型的预期应用场景：

*   **对新个体的泛化（Unseen-subject generalization）**：如果模型的目标是部署到从未在训练中出现过的新用户身上，评估协议必须模拟这种情景。**留一被试[交叉验证](@entry_id:164650)（Leave-One-Subject-Out, LOSO-CV）**是标准做法。在每一折中，将一个被试的所有数据作为[测试集](@entry_id:637546)，用其余所有被试的数据进行训练。至关重要的是，所有预处理步骤（如特征标准化）的参数都必须仅从[训练集](@entry_id:636396)中学习，然后应用到[测试集](@entry_id:637546)。[超参数调优](@entry_id:143653)则应通过在训练集内部进行嵌套的LOSO-CV来完成。

*   **对同一个体的未来预测（Within-subject forecasting）**：如果模型的目标是为已知用户预测其未来的状态，评估协议必须尊重时间的单向性。**滚动原点评估（rolling-origin evaluation）**或称前向链式评估是标准方法。该方法将数据按时间排序，使用过去的数据（例如，第 $1$ 天到第 $\tau-1$ 天）训练模型，然后在未来的数据（例如，第 $\tau$ 天）上进行测试。这个过程可以滚动进行，不断将新的数据点纳入[训练集](@entry_id:636396)，以模拟真实世界的预测流程。同样，所有预处理和[超参数调优](@entry_id:143653)都必须严格遵守因果关系，仅使用训练时间段内的数据。

#### 隐私风险与保障

数字表型数据，尤其是GPS等位置数据，具有极高的敏感性，带来了严峻的隐私挑战。简单地移除姓名、电话号码等直接身份标识符是远远不够的 [@problem_id:4557375]。

**再识别风险（Re-identification Risk）**：攻击者可以通过所谓的**准标识符（quasi-identifiers）**将匿名数据与外部信息关联起来，从而重新识别个体。在移动传感数据中，个体的时空轨迹是极其强大的准标识符。例如，一个人的家庭和工作地点组合可能在很大的人群中都是独一无二的。如果一个研究发布的数据集中包含了每个（匿名）参与者的家庭和工作地点，攻击者只需知道目标的这两个地点，就可以在数据集中进行匹配。如果匹配结果是唯一的，那么该参与者的身份就被暴露了。如果在一个 $N$ 人的数据集中，有 $U$ 个人的家庭-工作地点对是独一无二的，那么对于这 $U$ 个人，仅基于这一信息的再识别风险就是 $100\%$，整个群体的平均再识别风险至少为 $U/N$。

为了应对这种风险，研究者探索了不同的隐私保护技术，但它们的保障强度存在本质差异：
*   **隐私保护[启发式方法](@entry_id:637904)（Privacy Heuristics）**：例如**k-匿名（k-anonymity）**，其要求数据集中的每条记录都无法与另外至少 $k-1$ 条记录在准标识符上区分开来。这可以将再识别的概率降低到最多 $1/k$。然而，k-匿名的保障是脆弱的，它无法抵御掌握了额外背景知识的攻击者。例如，即使一个目标处于一个大小为 $k=10$ 的匿名集中，但如果攻击者还知道目标的年龄，而这个匿名集中只有一个人的年龄与目标匹配，再识别依然会发生。同样，对用户ID进行哈希处理也只是一种伪匿名化，无法阻止基于准标识符的链接攻击。

*   **形式化隐私保障（Formal Privacy Guarantees）**：**[差分隐私](@entry_id:261539)（Differential Privacy, DP）**是当前隐私保护领域的黄金标准。它提供了一个数学上可证明的、强大的隐私保障。一个满足 $\varepsilon$-[差分隐私](@entry_id:261539)的算法保证，对于任何两个仅相差一个个体数据的相邻数据集，其输出任何结果的概率变化不会超过一个小的乘法因子 $e^\varepsilon$。这个保障的深刻之处在于，它对于攻击者所拥有的任何背景知识都是稳健的。无论攻击者知道什么，他们都无法从算法的输出中确定性地判断某个特定个体的数据是否包含在原始数据集中。这为个体提供了强有力的“合理否认性”（plausible deniability），是数字表型数据发布和共享时应追求的理想目标。