{"hands_on_practices": [{"introduction": "原始的全球定位系统（GPS）坐标本身并不是一种表型；我们需要从中提取能够捕捉行为模式的概括性统计量。本练习将引导你计算“回转半径”，这是一个衡量个体活动范围的经典指标。通过这个过程，你将实践如何处理地理坐标数据，并应用统计学原理来量化空间行为模式 [@problem_id:4557417]。", "problem": "一部进行数字表型分析的智能手机在一周内每天被动收集一次全球定位系统（GPS）坐标（纬度和经度对），从而产生七个位置点。在移动性分析中，需要一个标量摘要来捕捉这些位置相对于其中心趋势的空间离散度。请从二维欧几里得平面中有限点集的质心定义以及离散分布的二阶中心矩出发，推导一个用于衡量点集与其质心之间代表性距离的标量表达式。然后，实现一个计算过程，该过程使用小区域切平面近似法，将给定周的七个地理坐标从以度为单位的经纬度映射到以米为单位的局部平面坐标系，并为每个提供的测试用例评估其标量离散度。\n\n请使用以下经过充分测试的小区域大地测量近似法。设地球半径为 $R = 6371000$ 米。对于一组 $N$ 个以度为单位的经纬度位置 $\\{(\\phi_i^{\\text{deg}}, \\lambda_i^{\\text{deg}})\\}_{i=1}^{N}$，首先将角度转换为弧度 $\\phi_i = \\phi_i^{\\text{deg}} \\cdot \\pi/180$ 和 $\\lambda_i = \\lambda_i^{\\text{deg}} \\cdot \\pi/180$。设 $(\\phi_0, \\lambda_0)$ 为该点集的平均纬度和经度（以弧度为单位）。定义以米为单位的局部东北坐标如下：\n$$\nx_i = R \\cos(\\phi_0) \\, (\\lambda_i - \\lambda_0), \\quad y_i = R \\, (\\phi_i - \\phi_0).\n$$\n将 $\\mathbf{x}_i = (x_i, y_i)$ 视为平面位置，并继续计算 $\\{\\mathbf{x}_i\\}_{i=1}^{N}$ 所需的标量离散度。\n\n您的程序必须应用上述步骤，并为以下测试套件生成结果。给定的角度以度为单位。所有输出必须以米为单位表示，并四舍五入到三位小数。最终输出必须是单行，包含一个由方括号括起来的逗号分隔列表。\n\n测试套件（每行为一组周数据）：\n- 情况 A（城市常规路线，波士顿地区，七个每日位置）：\n$(\\,$$42.3601$$,\\,$$-71.0589$$\\,)$, $(\\,$$42.3736$$,\\,$$-71.1097$$\\,)$, $(\\,$$42.3467$$,\\,$$-71.0972$$\\,)$, $(\\,$$42.3954$$,\\,$$-71.1056$$\\,)$, $(\\,$$42.3320$$,\\,$$-71.1167$$\\,)$, $(\\,$$42.3521$$,\\,$$-71.0645$$\\,)$, $(\\,$$42.4075$$,\\,$$-71.1190$$\\,)$.\n- 情况 B（退化的相同位置边界情况）：\n$(\\,$$42.3601$$,\\,$$-71.0589$$\\,)$ 重复 $7$ 次。\n- 情况 C（近赤道城市常规路线，新加坡地区，七个每日位置）：\n$(\\,$$1.3521$$,\\,$$103.8198$$\\,)$, $(\\,$$1.3000$$,\\,$$103.8000$$\\,)$, $(\\,$$1.3700$$,\\,$$103.9500$$\\,)$, $(\\,$$1.2800$$,\\,$$103.8400$$\\,)$, $(\\,$$1.3300$$,\\,$$103.7000$$\\,)$, $(\\,$$1.3600$$,\\,$$103.8200$$\\,)$, $(\\,$$1.3100$$,\\,$$103.7700$$\\,)$.\n- 情况 D（高纬度常规路线，赫尔辛基地区，七个每日位置）：\n$(\\,$$60.1699$$,\\,$$24.9384$$\\,)$, $(\\,$$60.1730$$,\\,$$24.8440$$\\,)$, $(\\,$$60.1840$$,\\,$$24.9500$$\\,)$, $(\\,$$60.2200$$,\\,$$25.0800$$\\,)$, $(\\,$$60.3000$$,\\,$$25.0400$$\\,)$, $(\\,$$60.1500$$,\\,$$25.0100$$\\,)$, $(\\,$$60.1200$$,\\,$$24.7000$$\\,)$.\n- 情况 E（单位置边界情况）：\n$(\\,$$34.0522$$,\\,$$-118.2437$$\\,)$.\n\n您的程序应生成一行输出，其中包含一个由方括号括起来的逗号分隔列表形式的结果，例如 $$[r_A, r_B, r_C, r_D, r_E]$$，其中每个条目是对应情况下以米为单位的标量离散度，四舍五入到三位小数。不应打印任何额外文本。", "solution": "经评估，用户提供的问题是有效的。该问题在科学上基于统计学和大地测量学的标准原理，问题陈述清晰，目标明确，数据充分，并以客观、正式的语言表述。该问题与数字表型分析领域相关，并具有唯一、可验证的解。\n\n该问题要求推导并实现一个用于衡量一组全球定位系统（GPS）坐标空间离散度的标量度量。该推导从基本统计概念出发，并应用于通过大地测量近似获得的平面坐标。\n\n**1. 标量离散度度量的推导**\n\n目标是定义一个标量，用于表示一组点与其中心趋势的代表性距离。该量是基于质心和二阶中心矩的概念推导出来的。\n\n设在二维欧几里得平面中有一组 $N$ 个点，记为 $\\{\\mathbf{x}_i = (x_i, y_i)\\}_{i=1}^{N}$。\n\n首先，这些点的中心趋势由其质心 $\\bar{\\mathbf{x}}$ 给出，质心是位置向量的算术平均值：\n$$\n\\bar{\\mathbf{x}} = (\\bar{x}, \\bar{y}) = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{x}_i = \\left( \\frac{1}{N} \\sum_{i=1}^{N} x_i, \\frac{1}{N} \\sum_{i=1}^{N} y_i \\right)\n$$\n\n接下来，我们寻求一个围绕该质心的离散度度量。对于一维数值分布 $\\{z_i\\}_{i=1}^{N}$，其二阶中心矩是方差 $\\sigma_z^2$，定义为与均值 $\\bar{z}$ 的离差平方的均值：\n$$\n\\sigma_z^2 = \\frac{1}{N} \\sum_{i=1}^{N} (z_i - \\bar{z})^2\n$$\n方差的平方根，即标准差 $\\sigma_z$，提供了一个与原始数据单位相同的离散度度量。\n\n为了将此推广到二维点分布，我们考虑每个点 $\\mathbf{x}_i$ 到质心 $\\bar{\\mathbf{x}}$ 的欧几里得距离的平方，即 $d_i^2 = ||\\mathbf{x}_i - \\bar{\\mathbf{x}}||^2$。这些平方距离的均值是一个标量离散度度量，类似于方差：\n$$\n\\text{Mean Squared Distance} = \\frac{1}{N} \\sum_{i=1}^{N} ||\\mathbf{x}_i - \\bar{\\mathbf{x}}||^2 = \\frac{1}{N} \\sum_{i=1}^{N} \\left( (x_i - \\bar{x})^2 + (y_i - \\bar{y})^2 \\right)\n$$\n通过分配求和，此表达式等价于各个坐标方差之和：\n$$\n= \\left( \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\bar{x})^2 \\right) + \\left( \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\bar{y})^2 \\right) = \\sigma_x^2 + \\sigma_y^2\n$$\n这个量 $\\sigma_x^2 + \\sigma_y^2$ 是点分布协方差矩阵的迹，并作为二阶中心矩的标量推广。\n\n为了获得一个以原始长度单位（米）表示的“代表性距离”度量，我们取平均平方距离的平方根。我们记为 $S_d$ 的这个度量，在地理学中通常称为标准距离，在物理学中称为回转半径。\n$$\nS_d = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} ||\\mathbf{x}_i - \\bar{\\mathbf{x}}||^2} = \\sqrt{\\sigma_x^2 + \\sigma_y^2}\n$$\n这是点集与其质心之间距离的均方根，也是需要计算的标量离散度度量。\n\n**2. 计算流程**\n\n实现过程遵循两步：坐标投影，然后评估离散度度量 $S_d$。\n\n**步骤 2a：大地坐标到平面坐标的转换**\n给定一组 $N$ 个以度为单位的地理坐标 $\\{(\\phi_i^{\\text{deg}}, \\lambda_i^{\\text{deg}})\\}_{i=1}^{N}$，应用以下转换。地球半径取为 $R = 6371000$ 米。\n\n1.  将所有纬度和经度值从度转换为弧度：\n    $$ \\phi_i = \\phi_i^{\\text{deg}} \\cdot \\frac{\\pi}{180}, \\quad \\lambda_i = \\lambda_i^{\\text{deg}} \\cdot \\frac{\\pi}{180} $$\n2.  计算参考点 $(\\phi_0, \\lambda_0)$，即弧度坐标的平均值：\n    $$ \\phi_0 = \\frac{1}{N} \\sum_{i=1}^{N} \\phi_i, \\quad \\lambda_0 = \\frac{1}{N} \\sum_{i=1}^{N} \\lambda_i $$\n3.  将每个点 $(\\phi_i, \\lambda_i)$ 投影到以参考点为中心的局部切平面，以获得以米为单位的东北坐标 $(x_i, y_i)$：\n    $$\n    x_i = R \\cos(\\phi_0) \\, (\\lambda_i - \\lambda_0)\n    $$\n    $$\n    y_i = R \\, (\\phi_i - \\phi_0)\n    $$\n\n**步骤 2b：离散度度量的评估**\n利用这组平面坐标 $\\{(x_i, y_i)\\}_{i=1}^{N}$，我们现在计算 $S_d$。投影的选择带来了一个关键的简化。投影坐标的质心 $(\\bar{x}, \\bar{y})$ 位于原点 $(0, 0)$。这是因为：\n$$\n\\bar{x} = \\frac{1}{N} \\sum_{i=1}^{N} x_i = \\frac{1}{N} \\sum_{i=1}^{N} R \\cos(\\phi_0) (\\lambda_i - \\lambda_0) = R \\cos(\\phi_0) \\left( \\left(\\frac{1}{N} \\sum_{i=1}^{N} \\lambda_i\\right) - \\lambda_0 \\right) = R \\cos(\\phi_0) (\\lambda_0 - \\lambda_0) = 0\n$$\n同样地，$\\bar{y} = 0$。\n\n将 $\\bar{x}=0$ 和 $\\bar{y}=0$ 代入 $S_d$ 的表达式中：\n$$\nS_d = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^{N} \\left( (x_i - 0)^2 + (y_i - 0)^2 \\right) } = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^{N} (x_i^2 + y_i^2) }\n$$\n因此，标量离散度就是投影平面中位置向量大小的均方根。\n\n特殊情况：对于 $N=1$ 个点（情况 E）或 $N$ 个相同的点（情况 B），每个点都与质心相同。偏差 $(x_i - \\bar{x})$ 和 $(y_i - \\bar{y})$ 均为零，导致离散度 $S_d = 0$。该实现能够正确处理这些情况。\n\n以下程序为提供的测试套件实现了这一逻辑。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the scalar dispersion for sets of GPS coordinates.\n    The scalar dispersion is defined as the standard distance (or radius of gyration),\n    which is the root-mean-square distance of points from their centroid.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A (urban routine, Boston-area, seven daily positions):\n        [\n            (42.3601, -71.0589), (42.3736, -71.1097), (42.3467, -71.0972),\n            (42.3954, -71.1056), (42.3320, -71.1167), (42.3521, -71.0645),\n            (42.4075, -71.1190)\n        ],\n        # Case B (degenerate identical-positions boundary case):\n        [\n            (42.3601, -71.0589), (42.3601, -71.0589), (42.3601, -71.0589),\n            (42.3601, -71.0589), (42.3601, -71.0589), (42.3601, -71.0589),\n            (42.3601, -71.0589)\n        ],\n        # Case C (near-equator urban routine, Singapore-area, seven daily positions):\n        [\n            (1.3521, 103.8198), (1.3000, 103.8000), (1.3700, 103.9500),\n            (1.2800, 103.8400), (1.3300, 103.7000), (1.3600, 103.8200),\n            (1.3100, 103.7700)\n        ],\n        # Case D (high-latitude routine, Helsinki-area, seven daily positions):\n        [\n            (60.1699, 24.9384), (60.1730, 24.8440), (60.1840, 24.9500),\n            (60.2200, 25.0800), (60.3000, 25.0400), (60.1500, 25.0100),\n            (60.1200, 24.7000)\n        ],\n        # Case E (single-position boundary case):\n        [\n            (34.0522, -118.2437)\n        ]\n    ]\n\n    R = 6371000.0  # Earth's mean radius in meters\n    \n    results = []\n    for case_coords in test_cases:\n        coords_deg = np.array(case_coords, dtype=np.float64)\n        N = coords_deg.shape[0]\n\n        # For a single point or identical points, the dispersion is 0.\n        # The logic below handles the identical points case naturally.\n        # The single point case is a boundary condition worth checking explicitly.\n        if N == 1:\n            result_str = \"0.000\"\n        else:\n            # Step 1: Convert degrees to radians\n            coords_rad = np.deg2rad(coords_deg)\n            \n            # Step 2: Calculate the mean latitude and longitude for the tangent plane center\n            phi0, lambda0 = np.mean(coords_rad, axis=0)\n\n            # Separate latitude (phi) and longitude (lambda) columns for clarity\n            phi_i = coords_rad[:, 0]\n            lambda_i = coords_rad[:, 1]\n            \n            # Step 3: Project to local East-North coordinates (x, y)\n            x_i = R * np.cos(phi0) * (lambda_i - lambda0)\n            y_i = R * (phi_i - phi0)\n            \n            # Step 4: Calculate the scalar dispersion (standard distance)\n            # This is the root-mean-square distance from the centroid.\n            # As derived, the centroid of the (x_i, y_i) coordinates is (0, 0).\n            # So, the formula simplifies to sqrt(mean(x_i^2 + y_i^2)).\n            scalar_dispersion = np.sqrt(np.mean(x_i**2 + y_i**2))\n            \n            # Format the result to three decimal places\n            result_str = f\"{scalar_dispersion:.3f}\"\n        \n        results.append(result_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "4557417"}, {"introduction": "数字表型分析旨在量化复杂的行为，例如社交互动。此实践展示了从原始蓝牙接近感知日志到标准化社交活动分数的完整流程。你将通过构建一个接触网络并计算关键的图论指标，来学习如何进行有原则的数据筛选、图构建以及最终的表型标准化 [@problem_id:4557348]。", "problem": "您将获得由智能手机上的低功耗蓝牙（BLE）扫描器被动收集的、为期一周的接近事件，这些事件以一组无向二元接触日志的形式表示。每个事件是一个形式为 $(t_s,t_e,u,v,r)$ 的元组，其中 $t_s$ 是接触开始时间（秒），$t_e$ 是接触结束时间（秒），$u$ 和 $v$ 是用户标识符（整数），$r$ 是接收信号强度指示（RSSI），单位为分贝毫瓦（dBm）。您必须根据这些日志推断出一个每周的无向、无权、简单接触网络 $G=(V,E)$，并计算个体层面的图指标，从而为每个用户推导出一个经过原则性归一化的社交活动表型。\n\n从图论的核心定义和接近感知的公认建模实践出发，请使用以下基础依据：\n\n- 令 $V$ 为日志中出现的所有唯一用户标识符的集合。\n- 对于一个固定的最小接近强度阈值 $r_{\\min}$ 和一个最小累积接触时长阈值 $\\tau$（两者均在下文指定），当且仅当对于无序对 $\\{u,v\\}$ 所有满足 $r \\ge r_{\\min}$ 的事件，其时长总和 $\\sum_{(t_s,t_e,u,v,r)\\ \\text{or}\\ (t_s,t_e,v,u,r)} \\max(0, t_e - t_s)$ 至少为 $\\tau$ 秒时，才在 $E$ 中包含一条无向边 $\\{u,v\\}$。所有时间必须以秒为单位进行解释和计算，RSSI 阈值的单位是 dBm。\n- 对每个节点 $i \\in V$，令其（未归一化的）度为 $k_i = \\deg(i)$。归一化度中心性定义为 $\\hat d_i = \\frac{k_i}{|V|-1}$，其值在 $[0,1]$ 区间内。\n- 对每个满足 $k_i \\ge 2$ 的节点 $i \\in V$，令 $e_i$ 表示节点 $i$ 的邻居之间存在的边数。局部聚类系数为 $C_i = \\frac{2 e_i}{k_i(k_i-1)} \\in [0,1]$。对于 $k_i \\in \\{0,1\\}$ 的情况，设 $C_i = 0$。\n\n每个个体的社交活动表型必须通过由有界比例的分布性考量驱动的归一化方法来构建：\n\n- 对每个比例 $p \\in [0,1]$ 应用反正弦平方根方差稳定变换：$z(p) = \\arcsin(\\sqrt{p})$，其中角度以弧度为单位。\n- 为每个 $i \\in V$ 定义复合得分 $S_i = \\frac{1}{2} z(\\hat d_i) + \\frac{1}{2} z(C_i)$。\n- 通过对所有 $i \\in V$ 计算总体均值和总体标准差，对群体内的 $S_i$ 进行标准化：令 $\\mu_S = \\frac{1}{|V|}\\sum_{i \\in V} S_i$ 和 $\\sigma_S = \\sqrt{\\frac{1}{|V|}\\sum_{i \\in V} (S_i - \\mu_S)^2}$。将标准化表型定义为 $\\tilde S_i = \\frac{S_i - \\mu_S}{\\sigma_S}$（如果 $\\sigma_S > 0$），如果 $\\sigma_S = 0$，则对所有 $i$ 定义 $\\tilde S_i = 0$。\n\n您的程序必须对下述每个测试用例构建 $G$，计算 $\\hat d_i$、$C_i$、$S_i$ 和 $\\tilde S_i$，然后返回由以下两项组成的对：\n- 度最大的用户 $i^*$ 的标准化表型 $\\tilde S_{i^*}$（通过选择最小的用户标识符来打破平局），\n- 度最小的用户 $j^*$ 的标准化表型 $\\tilde S_{j^*}$（通过选择最小的用户标识符来打破平局）。\n\n所有角度均以弧度为单位。所有时间差必须以秒为单位计算。最终输出必须是实数，并四舍五入到6位小数。\n\n对所有测试用例使用以下固定阈值：$r_{\\min} = -75$ dBm, $\\tau = 120$ 秒。\n\n测试套件：\n\n- 测试用例1（正常路径，混合稀疏结构）：\n  - 用户：$\\{0,1,2,3,4\\}$。\n  - 事件 $(t_s,t_e,u,v,r)$，其中 $t_s, t_e$ 单位为秒，$r$ 单位为 dBm：\n    - $(100,250,0,1,-65)$\n    - $(500,620,1,0,-70)$\n    - $(200,280,1,2,-80)$\n    - $(300,500,1,2,-72)$\n    - $(600,690,2,3,-68)$\n    - $(100,220,3,4,-60)$\n    - $(221,260,3,4,-60)$\n    - $(700,760,0,4,-74)$\n    - $(1000,1100,0,2,-73)$\n    - $(400,430,1,3,-77)$\n\n- 测试用例2（边界条件：完全图）：\n  - 用户：$\\{0,1,2,3\\}$。\n  - 事件：\n    - $(0,200,0,1,-60)$\n    - $(0,180,0,2,-70)$\n    - $(0,180,0,3,-70)$\n    - $(50,200,1,2,-70)$\n    - $(60,200,1,3,-71)$\n    - $(70,200,2,3,-73)$\n\n- 测试用例3（邻居间的非平凡聚类）：\n  - 用户：$\\{0,1,2,3\\}$。\n  - 事件：\n    - $(0,200,0,1,-60)$\n    - $(10,150,0,2,-70)$\n    - $(300,450,0,3,-74)$\n    - $(80,210,1,2,-73)$\n    - $(100,150,1,3,-80)$\n    - $(200,210,1,3,-70)$\n    - $(500,560,2,3,-70)$\n\n最终输出格式：\n\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序为 $[\\text{用例 }1\\ i^*, \\text{用例 }1\\ j^*, \\text{用例 }2\\ i^*, \\text{用例 }2\\ j^*, \\text{用例 }3\\ i^*, \\text{用例 }3\\ j^*]$，每个条目是对应的标准化表型值，四舍五入到6位小数，例如 $[0.123456,-0.654321,0.000000,0.000000,0.281900,-1.706900]$。", "solution": "用户提供的问题陈述已经过严格验证，被认为是具有科学依据、提法明确且客观的。它基于网络科学和统计学的既定原则，提供了一个清晰的多步骤流程，用于从传感器数据构建社交网络并推导社交表型。所有定义、常数和约束都已提供且内部一致。该问题要求对一组指定的测试用例给出计算解决方案。\n\n解决过程包括几个连续的阶段：图推断、指标计算、统计变换和标准化。每个阶段都由下文详述的基本原则指导。\n\n### 1. 从接近数据推断图\n基础步骤是从二元接近事件的时间序列中推导出一个静态、简单、无权和无向的图 $G=(V, E)$。一个事件被记录为 $(t_s, t_e, u, v, r)$，表示用户 $u$ 和 $v$ 在时间 $t_s$ 到 $t_e$ 之间发生了一次接触，信号强度为 $r$。\n\n- **基于信号的接近筛选原则**：并非所有检测到的信号都意味着有意义的社交接触。接收信号强度指示（RSSI）是距离的一个代理指标，尽管它带有噪声。通过施加一个最小 RSSI 阈值 $r \\ge r_{\\min}$，我们过滤掉那些可能对应于远距离接触的事件，因为这些事件对社交互动的指示性较弱。对于本问题，$r_{\\min} = -75$ dBm。\n\n- **时长显著性原则**：短暂或转瞬即逝的接触通常比持续的接触在社交上意义更小。我们为每对用户 $\\{u,v\\}$ 聚合所有有效接近事件（即满足 $r \\ge r_{\\min}$ 的事件）的时长。当且仅当这个累积时长达到或超过一个最小阈值 $\\tau$ 时，一条边 $\\{u,v\\}$ 才被包含在边集 $E$ 中。这将时间数据转换为一个代表稳定社交联系的静态网络结构。对于本问题，$\\tau = 120$ 秒。\n一对用户 $\\{u, v\\}$ 的累积时长由下式给出：\n$$ T_{uv} = \\sum_{(t_s,t_e,u,v,r)\\ \\text{or}\\ (t_s,t_e,v,u,r), r \\ge r_{\\min}} \\max(0, t_e - t_s) $$\n如果 $T_{uv} \\ge \\tau$，则边 $\\{u,v\\} \\in E$。顶点集 $V$ 由所有唯一用户组成。\n\n### 2. 个体层面的网络指标\n一旦图 $G$ 构建完成，我们使用标准的图论指标来刻画每个用户在社交网络中的位置。\n\n- **度中心性**：一个节点 $i$ 的度 $k_i = \\deg(i)$ 是与其相连的边数。它是一个节点社交活动性或连接性的最直接度量。为了使其在不同规模的网络中具有可比性，需要对其进行归一化。归一化度中心性 $\\hat d_i$ 定义为用户 $i$ 所连接的其他节点在网络中所占的比例：\n$$ \\hat d_i = \\frac{k_i}{|V|-1} $$\n该值位于 $[0, 1]$ 区间内。\n\n- **局部聚类系数**：该指标 $C_i$ 量化了用户直接邻域内的社交凝聚力。它衡量节点 $i$ 的邻居之间接近于形成一个团（完全连接的子图）的程度。它是节点 $i$ 的邻居之间存在的边数（记为 $e_i$）与这类边的最大可能数量之比。\n$$ C_i = \\begin{cases} \\frac{2 e_i}{k_i(k_i-1)} & \\text{if } k_i \\ge 2 \\\\ 0 & \\text{if } k_i \\in \\{0, 1\\} \\end{cases} $$\n针对 $k_i  2$ 的条件处理了用户邻居太少而无法形成三角形的情况，此时分母会为零或无定义。$C_i$ 的值也位于 $[0, 1]$ 区间内。\n\n### 3. 表型构建与标准化\n原始指标 $\\hat d_i$ 和 $C_i$ 都是比例值。为了构建一个稳健的复合得分，我们首先应用一个统计变换，然后将这些指标结合起来。\n\n- **方差稳定变换**：比例值通常表现出非恒定方差（即方差依赖于均值），这在统计建模中可能是有问题的。反正弦平方根变换 $z(p) = \\arcsin(\\sqrt{p})$ 是一种用于稳定比例数据方差的标准技术。角度以弧度计算。\n\n- **复合得分**：社交活动表型 $S_i$ 定义为变换后的度指标和聚类指标的线性组合。在这里，我们使用简单平均，给予个体的直接连接性及其局部网络的凝聚力同等权重：\n$$ S_i = \\frac{1}{2} z(\\hat d_i) + \\frac{1}{2} z(C_i) $$\n\n- **标准化（Z-score 化）**：为了解释一个用户的得分相对于其所在群体的意义，我们对 $S_i$ 值进行标准化。这是通过计算所有用户 $V$ 中 $S_i$ 得分的总体均值 $\\mu_S$ 和总体标准差 $\\sigma_S$ 来实现的。最终的标准化表型 $\\tilde S_i$ 是 $S_i$ 的 Z-分数。\n$$ \\mu_S = \\frac{1}{|V|}\\sum_{i \\in V} S_i \\quad \\text{and} \\quad \\sigma_S = \\sqrt{\\frac{1}{|V|}\\sum_{i \\in V} (S_i - \\mu_S)^2} $$\n$$ \\tilde S_i = \\begin{cases} \\frac{S_i - \\mu_S}{\\sigma_S}  \\text{if } \\sigma_S > 0 \\\\ 0  \\text{if } \\sigma_S = 0 \\end{cases} $$\n得到的 $\\tilde S_i$ 值表示个体的社交活动得分与群体平均值相差多少个标准差。$\\tilde S_i = 0$ 表示个体完全处于平均水平，而正值和负值分别表示高于平均和低于平均的社交活动水平。\n\n### 4. 应用于测试用例 1\n让我们为第一个测试用例演示该过程。\n- **用户**：$V = \\{0, 1, 2, 3, 4\\}$，所以 $|V|=5$。\n- **图推断**：在筛选掉 $r  -75$ dBm 的事件并加总时长后，我们发现以下累积接触超过 $\\tau = 120$ 秒：$\\{0, 1\\}$（$270$秒）、$\\{1, 2\\}$（$200$秒）和 $\\{3, 4\\}$（$159$秒）。得到的边集为 $E = \\{\\{0, 1\\}, \\{1, 2\\}, \\{3, 4\\}\\}$。\n- **指标计算**：\n  - 度：$k_0=1, k_1=2, k_2=1, k_3=1, k_4=1$。\n  - 归一化度（$\\hat d_i = k_i/4$）：$\\hat d_0=0.25, \\hat d_1=0.5, \\hat d_2=0.25, \\hat d_3=0.25, \\hat d_4=0.25$。\n  - 聚类系数：对于用户 $1$，$k_1=2$，其邻居是 $\\{0, 2\\}$。$0$ 和 $2$ 之间没有边，所以 $e_1=0$ 且 $C_1=0$。所有其他用户的 $k_i  2$，所以他们的 $C_i$ 也为 $0$。因此，对所有 $i \\in V$，$C_i = 0$。\n- **表型构建**：\n  - $z(\\hat d_0) = \\arcsin(\\sqrt{0.25}) = \\pi/6$。$S_0 = 0.5(\\pi/6) = \\pi/12$。用户 $2, 3, 4$ 的情况类似。\n  - $z(\\hat d_1) = \\arcsin(\\sqrt{0.5}) = \\pi/4$。$S_1 = 0.5(\\pi/4) = \\pi/8$。\n- **标准化**：\n  - 得分为 $S = \\{\\pi/12, \\pi/8, \\pi/12, \\pi/12, \\pi/12\\}$。\n  - $\\mu_S = (4 \\cdot \\pi/12 + \\pi/8)/5 = 11\\pi/120$。\n  - $\\sigma_S = \\sqrt{(4(\\pi/12 - 11\\pi/120)^2 + (\\pi/8 - 11\\pi/120)^2)/5} = \\pi/60$。\n  - $\\tilde S_0 = (\\pi/12 - 11\\pi/120) / (\\pi/60) = -0.5$。\n  - $\\tilde S_1 = (\\pi/8 - 11\\pi/120) / (\\pi/60) = 2.0$。\n- **最终选择**：\n  - 最大度为 $k_1=2$，所以 $i^*=1$。其表型为 $\\tilde S_1=2.0$。\n  - 最小度为 $k=1$，对应用户 $\\{0, 2, 3, 4\\}$。通过最小 ID 打破平局得到 $j^*=0$。其表型为 $\\tilde S_0=-0.5$。\n\n此完整、有原则的流程被实现以解决所有测试用例。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom collections import defaultdict\n\ndef solve():\n    \"\"\"\n    Solves the digital phenotyping problem for all test cases.\n    The overall process for each case is:\n    1. Filter BLE events by RSSI and aggregate cumulative contact durations.\n    2. Construct an undirected, simple graph based on a duration threshold.\n    3. For each user, calculate normalized degree and local clustering coefficient.\n    4. Apply arcsine square-root transform to these metrics.\n    5. Compute a composite score for each user.\n    6. Standardize the composite scores across the cohort.\n    7. Identify users with max/min degree and return their standardized scores.\n    \"\"\"\n    # Fixed thresholds as per the problem statement.\n    r_min = -75.0\n    tau = 120.0\n\n    # Test suite data.\n    test_cases = [\n        {\n            \"users\": {0, 1, 2, 3, 4},\n            \"events\": [\n                (100, 250, 0, 1, -65), (500, 620, 1, 0, -70),\n                (200, 280, 1, 2, -80), (300, 500, 1, 2, -72),\n                (600, 690, 2, 3, -68), (100, 220, 3, 4, -60),\n                (221, 260, 3, 4, -60), (700, 760, 0, 4, -74),\n                (1000, 1100, 0, 2, -73), (400, 430, 1, 3, -77),\n            ],\n        },\n        {\n            \"users\": {0, 1, 2, 3},\n            \"events\": [\n                (0, 200, 0, 1, -60), (0, 180, 0, 2, -70),\n                (0, 180, 0, 3, -70), (50, 200, 1, 2, -70),\n                (60, 200, 1, 3, -71), (70, 200, 2, 3, -73),\n            ],\n        },\n        {\n            \"users\": {0, 1, 2, 3},\n            \"events\": [\n                (0, 200, 0, 1, -60), (10, 150, 0, 2, -70),\n                (300, 450, 0, 3, -74), (80, 210, 1, 2, -73),\n                (100, 150, 1, 3, -80), (200, 210, 1, 3, -70),\n                (500, 560, 2, 3, -70),\n            ],\n        },\n    ]\n\n    final_results = []\n\n    for case in test_cases:\n        users = sorted(list(case[\"users\"]))\n        events = case[\"events\"]\n        V_size = len(users)\n\n        # Step 1: Aggregate contact durations for pairs of users.\n        durations = defaultdict(float)\n        for ts, te, u, v, r in events:\n            if r >= r_min:\n                duration = max(0.0, float(te - ts))\n                # Normalize pair order for undirected graph\n                key = tuple(sorted((u, v)))\n                durations[key] += duration\n\n        # Step 2: Construct the graph (adjacency list and edge set).\n        adj = defaultdict(set)\n        edges = set()\n        for (u, v), total_duration in durations.items():\n            if total_duration >= tau:\n                adj[u].add(v)\n                adj[v].add(u)\n                edges.add(tuple(sorted((u, v))))\n\n        # Step 3: Compute graph metrics for each user.\n        degrees = {}\n        norm_degrees = {}\n        clustering_coeffs = {}\n\n        for user in users:\n            # Degree k_i\n            k_i = len(adj[user])\n            degrees[user] = k_i\n            \n            # Normalized Degree d_i\n            if V_size > 1:\n                norm_degrees[user] = k_i / (V_size - 1)\n            else:\n                norm_degrees[user] = 0.0\n\n            # Local Clustering Coefficient C_i\n            if k_i  2:\n                clustering_coeffs[user] = 0.0\n            else:\n                neighbors = list(adj[user])\n                num_neighbor_edges = 0\n                # Count edges between pairs of neighbors.\n                for i in range(len(neighbors)):\n                    for j in range(i + 1, len(neighbors)):\n                        if tuple(sorted((neighbors[i], neighbors[j]))) in edges:\n                            num_neighbor_edges += 1\n                clustering_coeffs[user] = (2.0 * num_neighbor_edges) / (k_i * (k_i - 1))\n        \n        # Step 4: Apply variance-stabilizing transform.\n        def z(p):\n            # Arcsine square-root transform\n            return np.arcsin(np.sqrt(p))\n            \n        # Step 5: Compute composite scores.\n        composite_scores = {}\n        for user in users:\n            score_d = z(norm_degrees[user])\n            score_C = z(clustering_coeffs[user])\n            composite_scores[user] = 0.5 * score_d + 0.5 * score_C\n\n        # Step 6: Standardize scores.\n        score_values = np.array([composite_scores[user] for user in users])\n        mean_S = np.mean(score_values)\n        std_S = np.std(score_values)  # Uses N in denominator (population std dev)\n\n        standardized_scores = {}\n        if std_S > 1e-9: # Use a small threshold for floating point comparison\n            for user in users:\n                standardized_scores[user] = (composite_scores[user] - mean_S) / std_S\n        else:\n            for user in users:\n                standardized_scores[user] = 0.0\n\n        # Step 7: Identify i* (max degree) and j* (min degree).\n        max_degree = -1\n        i_star = -1\n        # Iterating through sorted users ensures smallest ID is chosen on tie.\n        for user in users:\n            if degrees[user] > max_degree:\n                max_degree = degrees[user]\n                i_star = user\n        \n        min_degree = float('inf')\n        j_star = -1\n        for user in users:\n            if degrees[user]  min_degree:\n                min_degree = degrees[user]\n                j_star = user\n        \n        final_results.append(standardized_scores[i_star])\n        final_results.append(standardized_scores[j_star])\n        \n    print(f\"[{','.join([f'{x:.6f}' for x in final_results])}]\")\n\nsolve()\n```", "id": "4557348"}, {"introduction": "真实世界中的传感器数据很少是完整的，设备可能会耗尽电量、被遗忘或遭遇传输失败。本练习直面这一关键挑战，介绍如何利用低秩矩阵补全技术来估算缺失的数据。该方法通过发掘多模态行为数据中的潜在结构，甚至能够重建整个缺失的传感器数据流 [@problem_id:4557361]。", "problem": "考虑一个多模态数字表型研究，其中来自移动和可穿戴传感器的参与者级别特征被汇总成一个矩形数据矩阵。令 $X^\\star \\in \\mathbb{R}^{n \\times p}$ 表示潜在的、无噪声的特征矩阵，其中每一列对应于一个特定模态块（例如，加速度测量、心率、睡眠）的特征。在实际部署中，由于设备不可用，整个模态可能会在某些时段内缺失，并且由于异步采样和传输失败，可能会有额外的随机条目缺失。假设存在由少量行为因素生成的低秩潜在结构，因此 $X^\\star$ 可以很好地被两个低维矩阵的乘积所近似。\n\n给定一个含噪测量矩阵 $M \\in \\mathbb{R}^{n \\times p}$ 和一个二元掩码算子 $P_\\Omega:\\mathbb{R}^{n \\times p}\\rightarrow\\mathbb{R}^{n \\times p}$，该算子保留观测到的条目，并将缺失条目置零。观测到的测量值满足\n$$\nM \\;=\\; X^\\star \\;+\\; E,\n$$\n其中 $E$ 是加性噪声，其条目是独立同分布的。观测到的索引集合由 $\\Omega \\subseteq \\{1,\\dots,n\\}\\times\\{1,\\dots,p\\}$ 表示，并且 $P_\\Omega$ 定义如下\n$$\n\\left[P_\\Omega(X)\\right]_{ij} \\;=\\;\n\\begin{cases}\nX_{ij},  (i,j)\\in\\Omega,\\\\\n0,  (i,j)\\notin\\Omega.\n\\end{cases}\n$$\n\n假设存在模态块缺失：对于预留的模态块，其对应列中的所有条目都从 $\\Omega$ 中移除（因此在重构期间是未观测的），并且仅用于评估。重构问题被表述为一个使用核范数惩罚的凸优化问题（核范数是奇异值的总和）：\n$$\n\\min_{X\\in\\mathbb{R}^{n\\times p}} \\;\\; \\frac{1}{2}\\,\\big\\|P_\\Omega(X - M)\\big\\|_F^2 \\;+\\; \\lambda\\,\\|X\\|_*,\n$$\n其中 $\\|\\cdot\\|_F$ 表示 Frobenius 范数，$\\|\\cdot\\|_*$ 表示核范数，$\\lambda > 0$ 是一个正则化参数。\n\n您的任务是实现一个基于近端梯度迭代和奇异值阈值的求解器，以近似最小化上述目标函数，然后评估在预留传感器（即从 $\\Omega$ 中移除的整个列块）上的重构质量。使用仅限于预留列的归一化均方根误差 (NRMSE)：\n$$\n\\mathrm{NRMSE} \\;=\\; \\frac{\\sqrt{\\sum_{(i,j)\\in\\Theta} \\left(X_{ij}-X^\\star_{ij}\\right)^2}}{\\sqrt{\\sum_{(i,j)\\in\\Theta} \\left(X^\\star_{ij}\\right)^2}},\n$$\n其中 $\\Theta$ 索引了预留列的所有条目。这个量是无单位的，必须以小数形式报告。\n\n在您的推导和实现中需要假设的基本原理：\n- 低秩潜在因子模型 $X^\\star \\approx U V^\\top$，其中 $U\\in\\mathbb{R}^{n\\times r}$ 且 $V\\in\\mathbb{R}^{p\\times r}$，对于一个小的 $r$。\n- $P_\\Omega$ 的正交投影性质，即 $P_\\Omega^2 = P_\\Omega$ 且 $\\|P_\\Omega\\|_2 = 1$。\n- 核范数的近端算子是奇异值软阈值。\n\n实现一个完整的程序，该程序：\n- 通过从标准正态分布中采样 $U$ 和 $V$ 的条目来合成 $X^\\star$，形成 $X^\\star = U V^\\top$，并通过特定于模态的正标量来缩放模态块内的列，以反映异构的传感器振幅。\n- 添加标准差为 $\\sigma$ 的独立高斯噪声以形成 $M$。\n- 通过从指定的预留模态块中移除所有条目，并从剩余列中进一步移除随机比例的条目，来构建观测掩码 $P_\\Omega$。\n- 使用步长为 $\\mu$ 的近端梯度迭代和阈值为 $\\mu\\lambda$ 的奇异值软阈值来求解核范数正则化目标函数，如果需要可以使用加速。使用零矩阵或观测数据进行初始化，并迭代直到达到最大迭代次数或关于相对 Frobenius 范数变化的容差。\n- 仅在预留的模态列上计算 NRMSE。\n\n测试套件。您的程序必须运行以下三个测试用例，每个用例由一个参数元组 $(n,p,r,\\sigma,\\lambda,\\mu,\\mathrm{max\\_iters},\\mathrm{tol},\\mathrm{seed},\\mathrm{block\\_sizes},\\mathrm{held\\_out\\_blocks},\\mathrm{missing\\_frac})$ 定义，其中模态块由 $\\mathrm{block\\_sizes}$ 中的连续列大小定义，$\\mathrm{held\\_out\\_blocks}$ 中的索引指的是从 $0$ 开始的块位置。\n\n- 情况 $1$（一般情况，块缺失并伴有中度随机缺失）：\n  - $(n,p,r) = (\\,48,\\,30,\\,3\\,)$，\n  - $\\sigma = 0.05$，\n  - $\\lambda = 0.8$，\n  - $\\mu = 1.0$，\n  - $\\mathrm{max\\_iters} = 500$，\n  - $\\mathrm{tol} = 10^{-6}$，\n  - $\\mathrm{seed} = 42$，\n  - $\\mathrm{block\\_sizes} = [\\,10,\\,10,\\,10\\,]$，\n  - $\\mathrm{held\\_out\\_blocks} = \\{\\,1\\,\\}$，\n  - $\\mathrm{missing\\_frac} = 0.25$（对非预留列）。\n\n- 情况 $2$（边界情况，无噪声且训练模态完全观测）：\n  - $(n,p,r) = (\\,48,\\,30,\\,2\\,)$，\n  - $\\sigma = 0.0$，\n  - $\\lambda = 0.1$，\n  - $\\mu = 1.0$，\n  - $\\mathrm{max\\_iters} = 500$，\n  - $\\mathrm{tol} = 10^{-7}$，\n  - $\\mathrm{seed} = 7$，\n  - $\\mathrm{block\\_sizes} = [\\,10,\\,10,\\,10\\,]$，\n  - $\\mathrm{held\\_out\\_blocks} = \\{\\,2\\,\\}$，\n  - $\\mathrm{missing\\_frac} = 0.0$（对非预留列）。\n\n- 情况 $3$（边缘情况，严重缺失且有多个预留块）：\n  - $(n,p,r) = (\\,60,\\,36,\\,2\\,)$，\n  - $\\sigma = 0.1$，\n  - $\\lambda = 1.0$，\n  - $\\mu = 1.0$，\n  - $\\mathrm{max\\_iters} = 600$，\n  - $\\mathrm{tol} = 10^{-6}$，\n  - $\\mathrm{seed} = 123$，\n  - $\\mathrm{block\\_sizes} = [\\,12,\\,12,\\,12\\,]$，\n  - $\\mathrm{held\\_out\\_blocks} = \\{\\,0,\\,2\\,\\}$，\n  - $\\mathrm{missing\\_frac} = 0.6$（对非预留列）。\n\n最终输出格式。您的程序应生成单行输出，其中包含三个用例的 NRMSE 结果，按顺序排列，形式为方括号内以逗号分隔的列表。每个值必须四舍五入到 $4$ 位小数，例如 $\\left[\\,0.1234,0.0000,0.5678\\,\\right]$。不应打印任何额外文本。NRMSE 是无单位的，必须以上述格式的小数表示。", "solution": "用户提供的问题经严格验证过程评估为**有效**。所有提供的数据和条件在科学上是合理的、一致的且适定的。该问题是机器学习和信号处理领域中一个标准的、尽管复杂的实现任务，特别是低秩矩阵补全，并直接且现实地应用于数字表型数据。\n\n本文提供了一个完整的解决方案，详细说明了其理论基础、算法设计和实现策略。\n\n**1. 问题表述与目标函数**\n\n问题的核心是从一个含噪且不完整的观测矩阵 $M \\in \\mathbb{R}^{n \\times p}$ 中重构一个低秩矩阵 $X^\\star \\in \\mathbb{R}^{n \\times p}$。潜在真实矩阵与观测值之间的关系由 $M = X^\\star + E$ 给出，其中 $E$ 是一个独立同分布（i.i.d.）的噪声矩阵。观测仅对集合 $\\Omega$ 中的索引 $(i,j)$ 可用。这被表述为以下凸优化问题：\n$$\n\\min_{X\\in\\mathbb{R}^{n\\times p}} \\;\\; \\frac{1}{2}\\,\\big\\|P_\\Omega(X - M)\\big\\|_F^2 \\;+\\; \\lambda\\,\\|X\\|_*\n$$\n这个目标函数是复合的，由两项组成。第一项，$f(X) = \\frac{1}{2}\\,\\big\\|P_\\Omega(X - M)\\big\\|_F^2$，是一个数据保真项，它惩罚估计值 $X$ 和测量值 $M$ 在观测条目上的差的 Frobenius 范数的平方。这个函数是凸的且可微的。第二项，$g(X) = \\lambda\\,\\|X\\|_*$，是一个正则化项。这里，$\\|X\\|_* = \\sum_i \\sigma_i(X)$ 是核范数（矩阵奇异值的和），它是矩阵秩的一个凸代理。这一项鼓励解 $X$ 是低秩的。参数 $\\lambda > 0$ 控制数据保真度和低秩结构之间的权衡。\n\n**2. 算法：近端梯度法**\n\n目标函数具有 $\\min_X f(X) + g(X)$ 的结构，其中 $f$ 是光滑的，而 $g$ 是凸的但不可微（由于奇异值中隐含的绝对值函数）。这种结构非常适合使用近端梯度算法。该方法的迭代更新规则是：\n$$\nX_{k+1} = \\mathrm{prox}_{\\mu g}\\left(X_k - \\mu \\nabla f(X_k)\\right)\n$$\n其中 $k$ 是迭代索引，$\\mu > 0$ 是步长，$\\mathrm{prox}_{\\mu g}$ 是函数 $\\mu g$ 的近端算子。\n\n**梯度步**：需要光滑项 $f(X)$ 的梯度。利用矩阵微积分和投影算子 $P_\\Omega$ 是自伴和幂等的性质（$P_\\Omega = P_\\Omega^\\top$, $P_\\Omega^2=P_\\Omega$），梯度为：\n$$\n\\nabla f(X) = P_\\Omega\\left(P_\\Omega(X) - P_\\Omega(M)\\right) = P_\\Omega(X - M)\n$$\n梯度下降步骤通过沿负梯度方向移动来更新当前估计 $X_k$：\n$$\nY_k = X_k - \\mu \\nabla f(X_k) = X_k - \\mu P_\\Omega(X_k - M)\n$$\n为保证收敛，步长 $\\mu$ 必须小于或等于 $1/L$，其中 $L$ 是 $\\nabla f$ 的利普希茨常数。对于 $\\nabla f(X) = P_\\Omega(X-M)$，利普希茨常数为 $\\|P_\\Omega\\|_2^2=1$。问题中指定的 $\\mu=1$ 满足此条件。\n\n**近端步**：核范数的近端算子 $\\mathrm{prox}_{\\tau\\|\\cdot\\|_*}(Y)$ 是奇异值阈值 (SVT) 算子，记为 $D_\\tau(Y)$。如果 $Y$ 的奇异值分解 (SVD) 是 $Y = U \\Sigma V^\\top$，其中 $\\Sigma = \\mathrm{diag}(\\sigma_1, \\sigma_2, \\dots)$，则 SVT 算子定义为：\n$$\nD_\\tau(Y) = U \\Sigma_\\tau V^\\top \\quad \\text{其中} \\quad [\\Sigma_\\tau]_{ii} = \\max(\\sigma_i - \\tau, 0)\n$$\n在我们的情况下，函数是 $g(X) = \\lambda \\|X\\|_*$，因此近端算子应用的阈值为 $\\tau = \\mu\\lambda$。在第 $k+1$ 次迭代中，对估计值 $X$ 的完整更新变为：\n$$\nX_{k+1} = D_{\\mu\\lambda}(Y_k) = D_{\\mu\\lambda}\\left(X_k - \\mu P_\\Omega(X_k - M)\\right)\n$$\n算法以 $X_0 = 0$ 初始化，并进行迭代，直到达到最大迭代次数，或者 $X$ 的 Frobenius 范数相对变化量 $\\|X_{k+1}-X_k\\|_F / \\|X_k\\|_F$ 低于指定的容差 $\\mathrm{tol}$。\n\n**3. 数据模拟流程**\n\n为了测试算法，按照规定实现了一个数据生成流程：\n- **潜在矩阵 ($X^\\star$)**：从标准正态分布中采样两个矩阵 $U \\in \\mathbb{R}^{n \\times r}$ 和 $V \\in \\mathbb{R}^{p \\times r}$。潜在低秩矩阵形成为 $X^\\star = UV^\\top$。为模拟不同的传感器尺度，将 $X^\\star$ 中对应于不同模态块的列乘以从均匀分布 $\\mathcal{U}(0.5, 1.5)$ 中采样的不同正标量。\n- **测量矩阵 ($M$)**：生成一个均值为零、标准差为 $\\sigma$ 的独立同分布高斯噪声矩阵 $E \\in \\mathbb{R}^{n \\times p}$，并将其加到潜在矩阵上，以产生含噪测量矩阵 $M = X^\\star + E$。\n- **观测和评估掩码**：生成两个布尔掩码。评估掩码 $\\Theta$ 标识了指定为 `held_out_blocks` 的列。观测掩码 $\\Omega$ 是其补集，然后通过随机将其中 `True` 条目的一部分（比例为 `missing_frac`）设置为 `False` 来进一步下采样。投影算子 $P_\\Omega(A)$ 通过将矩阵 $A$ 与对应于 $\\Omega$ 的二元掩码进行逐元素相乘来实现。\n\n每个测试用例使用单一的、有种子的随机数生成器，确保整个过程是确定性和可复现的。\n\n**4. 性能评估**\n\n重构矩阵 $\\hat{X}$ 的质量使用在预留列（由集合 $\\Theta$ 索引）上的归一化均方根误差 (NRMSE) 进行评估。NRMSE 定义为：\n$$\n\\mathrm{NRMSE} = \\frac{\\|P_\\Theta(\\hat{X} - X^\\star)\\|_F}{\\|P_\\Theta(X^\\star)\\|_F} = \\frac{\\sqrt{\\sum_{(i,j)\\in\\Theta} \\left(\\hat{X}_{ij}-X^\\star_{ij}\\right)^2}}{\\sqrt{\\sum_{(i,j)\\in\\Theta} \\left(X^\\star_{ij}\\right)^2}}\n$$\n该指标量化了相对于未观测模态中真实信号幅度的重构误差，提供了一个尺度无关的性能度量。\n\n**5. 实现总结**\n\n解决方案被封装在一个单一的 Python 程序中。一个主函数遍历所有指定的测试用例。对于每个用例，它初始化一个带种子的随机数生成器，并调用辅助函数来生成数据和掩码。然后，它运行近端梯度求解器以获得重构矩阵。最后，它计算预留列上的 NRMSE 并存储结果。处理完所有用例后，程序以逗号分隔列表的形式打印 NRMSE 值，并格式化为四位小数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the matrix completion problem for all test cases.\n    \"\"\"\n\n    def generate_data(n, p, r, sigma, block_sizes, rng):\n        \"\"\"\n        Generates the latent low-rank matrix X_star and its noisy observation M.\n        \"\"\"\n        U = rng.standard_normal(size=(n, r))\n        V = rng.standard_normal(size=(p, r))\n        X_star_unscaled = U @ V.T\n        X_star = np.copy(X_star_unscaled)\n\n        # Scale columns by modality blocks to simulate heterogeneous sensor amplitudes\n        num_blocks = len(block_sizes)\n        scales = rng.uniform(0.5, 1.5, size=num_blocks)\n        \n        col_idx = 0\n        for i in range(num_blocks):\n            block_size = block_sizes[i]\n            X_star[:, col_idx : col_idx + block_size] *= scales[i]\n            col_idx += block_size\n            \n        # Add Gaussian noise\n        noise = rng.normal(0, sigma, size=(n, p))\n        M = X_star + noise\n        \n        return X_star, M\n\n    def create_masks(n, p, block_sizes, held_out_blocks, missing_frac, rng):\n        \"\"\"\n        Creates the observation mask (omega_mask) and evaluation mask (theta_mask).\n        \"\"\"\n        # theta_mask identifies held-out columns for evaluation\n        theta_mask = np.zeros((n, p), dtype=bool)\n        \n        block_col_indices = []\n        col_idx = 0\n        for size in block_sizes:\n            block_col_indices.append(list(range(col_idx, col_idx + size)))\n            col_idx += size\n            \n        held_out_cols = []\n        for block_idx in held_out_blocks:\n            held_out_cols.extend(block_col_indices[block_idx])\n            \n        if held_out_cols:\n            theta_mask[:, held_out_cols] = True\n        \n        # omega_mask identifies observed entries for training\n        omega_mask = np.ones((n, p), dtype=bool)\n        if held_out_cols:\n            omega_mask[:, held_out_cols] = False\n        \n        # Introduce additional random missingness in non-held-out columns\n        if missing_frac > 0.0:\n            training_cols_mask = ~theta_mask[0, :]\n            training_cols_indices = np.where(training_cols_mask)[0]\n            \n            if len(training_cols_indices) > 0:\n                n_training_entries = n * len(training_cols_indices)\n                n_to_remove = int(round(n_training_entries * missing_frac))\n                \n                # Get all (row, col) pairs in training columns\n                row_indices_grid, col_indices_grid = np.meshgrid(\n                    np.arange(n), training_cols_indices, indexing='ij'\n                )\n                flat_row_indices = row_indices_grid.flatten()\n                flat_col_indices = col_indices_grid.flatten()\n                \n                # Randomly choose indices to set to False\n                indices_to_remove = rng.choice(\n                    n_training_entries, size=n_to_remove, replace=False\n                )\n                \n                rows_to_remove = flat_row_indices[indices_to_remove]\n                cols_to_remove = flat_col_indices[indices_to_remove]\n                \n                omega_mask[rows_to_remove, cols_to_remove] = False\n                \n        return omega_mask, theta_mask\n\n    def singular_value_thresholding(Y, threshold):\n        \"\"\"\n        Performs the singular value thresholding operation.\n        \"\"\"\n        U, s, Vt = np.linalg.svd(Y, full_matrices=False)\n        s_thresh = np.maximum(s - threshold, 0)\n        return (U * s_thresh) @ Vt\n\n    def matrix_completion_solver(M, omega_mask, lambda_val, mu, max_iters, tol):\n        \"\"\"\n        Solves the nuclear norm regularized matrix completion problem using\n        proximal gradient descent.\n        \"\"\"\n        n, p = M.shape\n        X = np.zeros((n, p))\n        \n        for _ in range(max_iters):\n            X_prev = np.copy(X)\n            \n            # Gradient descent step\n            grad = (X - M) * omega_mask\n            Y = X - mu * grad\n            \n            # Proximal mapping step (SVT)\n            threshold = mu * lambda_val\n            X = singular_value_thresholding(Y, threshold)\n            \n            # Check for convergence\n            change = np.linalg.norm(X - X_prev, 'fro')\n            norm_prev = np.linalg.norm(X_prev, 'fro')\n            if norm_prev > 0:\n                rel_change = change / norm_prev\n            else: # Handle case X_prev is the zero matrix\n                rel_change = change / (1e-9)\n\n            if rel_change  tol:\n                break\n                \n        return X\n\n    def calculate_nrmse(X_recon, X_star, theta_mask):\n        \"\"\"\n        Calculates the Normalized Root Mean Squared Error on held-out columns.\n        \"\"\"\n        # Use boolean indexing to select elements from held-out columns\n        diff_held_out = (X_recon - X_star)[theta_mask]\n        true_held_out = X_star[theta_mask]\n        \n        numerator = np.linalg.norm(diff_held_out)\n        denominator = np.linalg.norm(true_held_out)\n        \n        if denominator == 0:\n            return 0.0 if numerator == 0.0 else np.inf\n            \n        return numerator / denominator\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        (48, 30, 3, 0.05, 0.8, 1.0, 500, 1e-6, 42, [10, 10, 10], {1}, 0.25),\n        # Case 2\n        (48, 30, 2, 0.0, 0.1, 1.0, 500, 1e-7, 7, [10, 10, 10], {2}, 0.0),\n        # Case 3\n        (60, 36, 2, 0.1, 1.0, 1.0, 600, 1e-6, 123, [12, 12, 12], {0, 2}, 0.6)\n    ]\n\n    results = []\n    for params in test_cases:\n        (n, p, r, sigma, lambda_val, mu, max_iters, tol, seed, \n         block_sizes, held_out_blocks, missing_frac) = params\n\n        # Seed a random number generator for reproducibility of the entire case\n        rng = np.random.default_rng(seed)\n\n        # Generate data and masks\n        X_star, M = generate_data(n, p, r, sigma, block_sizes, rng)\n        omega_mask, theta_mask = create_masks(n, p, block_sizes, held_out_blocks, missing_frac, rng)\n        \n        # Solve for the reconstructed matrix\n        X_recon = matrix_completion_solver(M, omega_mask, lambda_val, mu, max_iters, tol)\n        \n        # Calculate NRMSE on the held-out data\n        nrmse = calculate_nrmse(X_recon, X_star, theta_mask)\n        results.append(nrmse)\n\n    # Format and print the final output as specified.\n    formatted_results = [f\"{res:.4f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4557361"}]}