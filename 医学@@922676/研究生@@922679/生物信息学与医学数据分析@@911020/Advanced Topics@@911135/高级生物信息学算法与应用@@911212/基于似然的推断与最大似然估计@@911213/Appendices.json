{"hands_on_practices": [{"introduction": "要真正掌握基于似然的推断，第一步是牢固理解其在基础模型中的应用。本练习将从最根本的二元结果模型——伯努利分布——入手，这在生物信息学中常用于模拟基因变异存在与否等情况。通过从头推导伯努利参数的最大似然估计 (MLE) 及其使用费雪信息计算的渐近方差，你将能够巩固对 MLE 核心推导机制的理解。[@problem_id:4578114]", "problem": "一项使用下一代测序（NGS）的肿瘤测序研究，旨在估计某个特定基因中具有临床可操作性变异的队列水平患病率。经过严格的质量控制过滤以去除低置信度的读数后，每个肿瘤的变异检测结果被建模为一个独立的伯努利随机变量，其共同参数为 $p \\in (0,1)$，其中 $p$ 代表从研究群体中随机抽样的肿瘤中该变异真实存在的概率。设 $X_{1}, X_{2}, \\dots, X_{n}$ 为 $n$ 个独立的检测指标，其中 $X_{i} \\in \\{0,1\\}$，并定义充分统计量 $S = \\sum_{i=1}^{n} X_{i}$。假设 $0  S  n$ 以避免边界解。\n\n使用独立同分布伯努利观测的似然函数和标量参数的费雪信息的基本定义，推导 $p$ 的最大似然估计量，以及基于费雪信息的该估计量的渐近方差。将您的最终答案表示为最大似然估计量的关于 $S$ 和 $n$ 的封闭形式解析表达式，以及其渐近方差的关于 $p$ 和 $n$ 的表达式。无需数值四舍五入，也无物理单位。将这两个量作为一个最终答案一起提供。", "solution": "在尝试任何解答之前，首先对问题陈述进行验证。\n\n### 第1步：提取已知条件\n-   每个肿瘤的检测结果 $X_i$ 是一个独立的伯努利随机变量。\n-   伯努利分布的共同参数是 $p \\in (0,1)$。\n-   观测值为 $X_{1}, X_{2}, \\dots, X_{n}$，它们是 $n$ 个独立同分布（i.i.d.）的指标，其中 $X_{i} \\in \\{0,1\\}$。\n-   充分统计量是 $S = \\sum_{i=1}^{n} X_{i}$。\n-   一个明确的假设是：$0  S  n$。\n-   任务是推导 $p$ 的最大似然估计量（MLE），并用 $S$ 和 $n$ 表示。\n-   任务还包括推导基于费雪信息的该估计量的渐近方差，并用 $p$ 和 $n$ 表示。\n\n### 第2步：使用提取的已知条件进行验证\n这个问题提法恰当且有科学依据。\n-   **科学依据：** 使用伯努利分布对二元结果（变异存在/不存在）进行建模是统计学和生物信息学中基本且标准的做法。最大似然估计和费雪信息的使用是统计推断的核心原则。\n-   **提法恰当：** 该问题提供了一个清晰的统计模型（独立同分布的伯努利试验）和一个明确定义的目标（推导最大似然估计量及其渐近方差）。假设 $0  S  n$ 是一个简化条件，以确保估计量不会落在参数空间的边界上，这是许多理论处理中的标准程序。预期会有一个唯一且有意义的解。\n-   **客观性：** 该问题以精确、客观的数学语言陈述，没有歧义或主观论断。\n\n### 第3步：结论与行动\n问题有效。将提供一个完整的、有理有据的解答。\n\n### 解答推导\n问题要求推导伯努利分布参数 $p$ 的最大似然估计量（MLE）以及该估计量的渐近方差。\n\n#### 第1部分：$p$ 的最大似然估计量\n\n设 $X_1, X_2, \\dots, X_n$ 是来自参数为 $p$ 的伯努利分布的一组 $n$ 个独立同分布的随机变量。对于单次观测 $X_i=x_i$，其中 $x_i \\in \\{0,1\\}$，其概率质量函数（PMF）为：\n$$\nP(X_i=x_i | p) = p^{x_i}(1-p)^{1-x_i}\n$$\n由于观测的独立性，观测到整个数据集 $x = (x_1, x_2, \\dots, x_n)$ 的联合概率是个体概率的乘积。这个联合概率，在给定数据集下视为参数 $p$ 的函数，就是似然函数 $L(p|\\boldsymbol{x})$。\n$$\nL(p | x_1, \\dots, x_n) = \\prod_{i=1}^{n} P(X_i=x_i | p) = \\prod_{i=1}^{n} p^{x_i}(1-p)^{1-x_i}\n$$\n该表达式可以使用充分统计量 $S = \\sum_{i=1}^{n} x_i$ 来简化，它代表了成功的总次数（检测到的变异数量）。\n$$\nL(p) = p^S (1-p)^{n-S}\n$$\n为了找到使 $L(p)$ 最大化的 $p$ 值，从分析角度看，最大化似然函数的自然对数，即对数似然函数 $\\ell(p)$，更为方便，因为对数函数是单调递增函数。\n$$\n\\ell(p) = \\ln(L(p)) = \\ln(p^S (1-p)^{n-S}) = S \\ln(p) + (n-S) \\ln(1-p)\n$$\n为求最大值，我们计算 $\\ell(p)$ 对 $p$ 的一阶导数并令其为零。这个导数被称为得分函数。\n$$\n\\frac{d\\ell}{dp} = \\frac{d}{dp} \\left( S \\ln(p) + (n-S) \\ln(1-p) \\right) = \\frac{S}{p} - \\frac{n-S}{1-p}\n$$\n将得分设为零以找到临界点，我们求解 $p$。令 $\\hat{p}$ 表示该估计量。\n$$\n\\frac{S}{\\hat{p}} - \\frac{n-S}{1-\\hat{p}} = 0 \\implies \\frac{S}{\\hat{p}} = \\frac{n-S}{1-\\hat{p}}\n$$\n$$\nS(1-\\hat{p}) = (n-S)\\hat{p}\n$$\n$$\nS - S\\hat{p} = n\\hat{p} - S\\hat{p}\n$$\n$$\nS = n\\hat{p}\n$$\n求解 $\\hat{p}$ 得到最大似然估计量：\n$$\n\\hat{p} = \\frac{S}{n}\n$$\n条件 $0  S  n$ 确保了 $\\hat{p}$ 在开区间 $(0,1)$ 内，与参数空间 $p \\in (0,1)$ 一致，并避免了得分函数中出现除以零的情况。为确认这是一个最大值，我们检查对数似然的二阶导数：\n$$\n\\frac{d^2\\ell}{dp^2} = \\frac{d}{dp} \\left( \\frac{S}{p} - \\frac{n-S}{1-p} \\right) = -\\frac{S}{p^2} - \\frac{n-S}{(1-p)^2}\n$$\n由于 $S>0$，$n-S>0$，以及 $p^2 > 0$，$(1-p)^2 > 0$，二阶导数 $\\frac{d^2\\ell}{dp^2}$ 对所有 $p \\in (0,1)$ 都是严格为负的。这证实了对数似然函数是凹函数，并且临界点 $\\hat{p} = S/n$ 确实是一个唯一的最大值。\n\n#### 第2部分：最大似然估计量的渐近方差\n\n最大似然估计量（MLE）的渐近方差由费雪信息 $I(p)$ 的逆给出。对于单个标量参数，费雪信息的定义为：\n$$\nI(p) = -E\\left[ \\frac{d^2\\ell}{dp^2} \\right]\n$$\n其中期望是关于数据分布计算的。我们有二阶导数：\n$$\n\\frac{d^2\\ell}{dp^2} = -\\frac{S}{p^2} - \\frac{n-S}{(1-p)^2}\n$$\n现在，我们计算期望。该表达式中的随机变量是 $S = \\sum_{i=1}^n X_i$。和的期望是期望的和。对于单次伯努利试验 $X_i$，$E[X_i] = 1 \\cdot p + 0 \\cdot (1-p) = p$。因此，$S$ 的期望是：\n$$\nE[S] = E\\left[\\sum_{i=1}^n X_i\\right] = \\sum_{i=1}^n E[X_i] = \\sum_{i=1}^n p = np\n$$\n将 $E[S]$ 代入期望二阶导数的表达式中：\n$$\nE\\left[ \\frac{d^2\\ell}{dp^2} \\right] = E\\left[ -\\frac{S}{p^2} - \\frac{n-S}{(1-p)^2} \\right] = -\\frac{E[S]}{p^2} - \\frac{n-E[S]}{(1-p)^2}\n$$\n$$\nE\\left[ \\frac{d^2\\ell}{dp^2} \\right] = -\\frac{np}{p^2} - \\frac{n-np}{(1-p)^2} = -\\frac{n}{p} - \\frac{n(1-p)}{(1-p)^2}\n$$\n$$\nE\\left[ \\frac{d^2\\ell}{dp^2} \\right] = -\\frac{n}{p} - \\frac{n}{1-p} = -n \\left( \\frac{1}{p} + \\frac{1}{1-p} \\right) = -n \\left( \\frac{1-p+p}{p(1-p)} \\right) = -\\frac{n}{p(1-p)}\n$$\n费雪信息是这个量的负值：\n$$\nI(p) = - \\left( -\\frac{n}{p(1-p)} \\right) = \\frac{n}{p(1-p)}\n$$\n根据克拉默-拉奥下界（Cramer-Rao Lower Bound）和最大似然估计量的大样本理论，$\\hat{p}$ 的渐近方差是费雪信息的逆：\n$$\n\\text{Asymptotic Variance}(\\hat{p}) = [I(p)]^{-1} = \\left( \\frac{n}{p(1-p)} \\right)^{-1} = \\frac{p(1-p)}{n}\n$$\n这个方差是独立同分布伯努利随机变量样本均值的方差，这是一个众所周知的结果，并且最大似然估计量的渐近方差收敛于此值。\n\n最终答案需要最大似然估计量 $\\hat{p}$ 及其渐近方差的表达式。\n-   最大似然估计量：$\\hat{p} = \\frac{S}{n}$\n-   渐近方差：$\\frac{p(1-p)}{n}$\n\n这两个量将以单行矩阵的形式呈现。", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{S}{n}  \\frac{p(1-p)}{n} \\end{pmatrix}}\n$$", "id": "4578114"}, {"introduction": "在理论上推导出最大似然估计后，下一步是面对在实践中计算它的数值挑战。尽管 MLE 的不变性保证了在不同参数化下理论估计值的一致性，但优化算法的数值稳定性和效率却可能天差地别。本练习将探讨一个常见问题：估计一个必须为正的方差分量 $\\sigma^2$，这给无约束优化带来了边界问题。你将通过分析不同参数化方案（例如，使用标准差 $\\sigma$ 与其对数 $\\eta = \\log \\sigma$），深入理解如何通过重参数化来提高数值稳定性并解决实际优化中的难题。[@problem_id:4578105]", "problem": "一项纵向蛋白质组学研究使用一种经过校准实验验证的、测量误差方差为 $ \\tau^{2} $ 的分析方法，从 $n$ 名患者中收集血浆生物标志物的重复测量值。经过标准的预处理，移除了固定效应均值（例如，批次内去均值）后，中心化强度 $ y_{1},\\dots,y_{n} $ 的工作模型是\n$$\ny_{i} \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}\\!\\left(0,\\; \\sigma^{2} + \\tau^{2}\\right),\n$$\n其中 $ \\sigma^{2}  0 $ 代表受试者间的生物学变异性。您希望通过最大似然估计 (MLE) 对 $ \\sigma^{2} $ 进行基于似然的推断。考虑针对不同的一维参数化来优化似然函数：标准差 $ \\sigma $、方差 $ \\sigma^{2} $、对数标准差 $ \\eta = \\log \\sigma $，以及对数方差 $ \\zeta = \\log \\sigma^{2} $。\n\n仅使用似然函数的基本定义、最大似然估计 (MLE) 的定义、MLE 在平滑一对一重参数化下的不变性，以及费雪信息和链式法则，选择最能在此生物信息学背景下，以正则性和数值稳定性为由证明首选参数化的选项。\n\nA. 直接在 $ \\sigma $ 中优化。因为约束 $ \\sigma  0 $ 很简单，无约束方法保持稳定，不会出现边界问题；此外，关于 $ \\sigma $ 的负对数似然函数是全局凸的，并且在各种尺度下都是良态的，所以牛顿类型的步骤不需要任何特殊处理。\n\nB. 在 $ \\eta = \\log \\sigma $ 中优化。根据 MLE 的不变性， $ \\eta $ 中的最大化子通过 $ \\widehat{\\sigma} = \\exp(\\widehat{\\eta}) $ 映射回 $ \\sigma $ 中的同一最大化子。参数空间变为所有实数，这避免了边界违规，并且通常能在 $ \\sigma $ 的多个数量级上产生更好的缩放梯度和海森矩阵。$ \\sigma $ 的标准误应通过对 $ \\eta $ 的标准误进行 delta 方法变换得到。\n\nC. 在 $ \\nu = \\sigma^{2} $ 中优化。对于均值已知的高斯数据，关于 $ \\nu $ 的负对数似然函数是全局凸的，与数据无关，这保证了唯一的全局最优解，并消除了在 $ \\sigma \\approx 0 $ 附近的病态条件。\n\nD. 在 $ \\zeta = \\log \\sigma^{2} $ 中优化。这在常数重缩放的意义上等价于 $ \\eta = \\log \\sigma $，因此它继承了相同的数值优势和无约束域。费雪信息根据链式法则进行变换，得到 $ I_{\\zeta}(\\zeta) = I_{\\sigma}(\\sigma)\\left(\\frac{\\partial \\sigma}{\\partial \\zeta}\\right)^{2} = I_{\\sigma}(\\sigma)\\,\\frac{\\sigma^{2}}{4} $，因此，在反向变换后，对 $ \\sigma $ 的渐近推断与直接在 $ \\sigma $ 中工作得到的推断一致。", "solution": "问题要求在一个简单的随机效应模型中，对生物学方差分量 $\\sigma^2$ 进行最大似然估计 (MLE) 时，评估不同的参数化方法。中心化生物标志物强度的模型由下式给出\n$$\ny_{i} \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}\\!\\left(0,\\; \\sigma^{2} + \\tau^{2}\\right), \\quad i=1, \\dots, n,\n$$\n其中 $\\tau^2$ 是已知的测量误差方差，$\\sigma^2  0$ 是我们感兴趣的参数。\n\n首先，我们建立对数似然函数，这是分析的基础。总方差为 $V = \\sigma^2 + \\tau^2$。似然函数为：\n$$\nL(\\sigma^2 | \\mathbf{y}) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi (\\sigma^2 + \\tau^2)}} \\exp\\left(-\\frac{y_i^2}{2(\\sigma^2 + \\tau^2)}\\right) = (2\\pi (\\sigma^2 + \\tau^2))^{-n/2} \\exp\\left(-\\frac{\\sum_{i=1}^n y_i^2}{2(\\sigma^2 + \\tau^2)}\\right)\n$$\n对数似然函数，忽略不依赖于 $\\sigma^2$ 的常数，为：\n$$\n\\ell(\\sigma^2) = -\\frac{n}{2} \\log(\\sigma^2 + \\tau^2) - \\frac{S_y}{2(\\sigma^2 + \\tau^2)}\n$$\n其中 $S_y = \\sum_{i=1}^n y_i^2$。令 $s^2 = S_y/n = \\frac{1}{n}\\sum_{i=1}^n y_i^2$ 为经验二阶矩（由于均值为 $0$，这也是样本方差）。我们旨在最小化的负对数似然函数是：\n$$\n-\\ell = \\frac{n}{2} \\left[ \\log(\\sigma^2 + \\tau^2) + \\frac{s^2}{\\sigma^2 + \\tau^2} \\right]\n$$\n总方差 $V = \\sigma^2 + \\tau^2$ 的 MLE 是 $\\widehat{V} = s^2$。根据 MLE 的不变性，$\\sigma^2$ 的 MLE 是 $\\widehat{\\sigma^2} = s^2 - \\tau^2$。然而，我们有约束条件 $\\sigma^2  0$。因此，$\\sigma^2$ 的约束 MLE 是 $\\widehat{\\sigma^2}_{\\text{MLE}} = \\max(0, s^2 - \\tau^2)$。$\\sigma$ 的 MLE 是 $\\widehat{\\sigma}_{\\text{MLE}} = \\sqrt{\\max(0, s^2 - \\tau^2)}$。\n\n现在我们基于此设置评估每个选项。\n\n### 选项 A：在 $\\sigma$ 中优化\n\n此选项声称在 $\\sigma$ 中优化是稳定的，没有边界问题，并且负对数似然函数是全局凸的。我们来检验这些说法。\n\n1.  **边界问题**：$\\sigma$ 的参数空间是 $\\sigma  0$。如果数据满足 $s^2 \\le \\tau^2$，则 MLE 是 $\\widehat{\\sigma} = 0$。这位于可行域的边界上。像牛顿法这样的标准无约束优化算法本身不遵守这类边界，可能会因为试图步进到 $\\sigma$ 的负值而失败。因此，边界问题可能并且确实会发生，需要特殊处理（例如，约束优化方法或重参数化）。声称“不会出现边界问题”是错误的。\n\n2.  **全局凸性**：我们来分析负对数似然函数 $f(\\sigma) = -\\ell(\\sigma) = \\frac{n}{2} \\left[ \\log(\\sigma^2 + \\tau^2) + \\frac{s^2}{\\sigma^2 + \\tau^2} \\right]$ 的凸性。关于 $\\sigma$ 的一阶导数是：\n    $$\n    f'(\\sigma) = \\frac{n}{2} \\left[ \\frac{2\\sigma}{\\sigma^2 + \\tau^2} - \\frac{s^2 \\cdot 2\\sigma}{(\\sigma^2 + \\tau^2)^2} \\right] = n\\sigma \\frac{(\\sigma^2 + \\tau^2) - s^2}{(\\sigma^2 + \\tau^2)^2}\n    $$\n    二阶导数是：\n    $$\n    f''(\\sigma) = \\frac{d}{d\\sigma} f'(\\sigma) = \\frac{n}{(\\sigma^2+\\tau^2)^3} \\left[ (\\sigma^2 + \\tau^2)(\\tau^2 - \\sigma^2 - s^2) + 4s^2\\sigma^2 \\right]\n    $$\n    为了检查全局凸性，我们需要对所有 $\\sigma  0$ 都有 $f''(\\sigma)  0$。考虑 $\\sigma$ 很大时的行为。当 $\\sigma \\to \\infty$ 时，$\\sigma^2 + \\tau^2 \\approx \\sigma^2$。\n    $$\n    f''(\\sigma) \\approx \\frac{n}{(\\sigma^2)^3} \\left[ \\sigma^2(-\\sigma^2 - s^2) + 4s^2\\sigma^2 \\right] = \\frac{n}{\\sigma^6} [-\\sigma^4 + 3s^2\\sigma^2] = \\frac{n}{\\sigma^2}[-1 + 3s^2/\\sigma^2]\n    $$\n    对于足够大的 $\\sigma$，括号中的项会变为负数。因此，该函数在 $\\sigma$ 很大时不是凸的。全局凸性的说法是错误的。\n\n由于关于边界问题和全局凸性的说法都是错误的，选项 A 是不正确的。\n\n### 选项 C：在 $\\nu = \\sigma^2$ 中优化\n\n为了将其与另一个非对数参数化分组，我们不按顺序评估此陈述。它声称在 $\\nu = \\sigma^2$ 中优化会得到一个全局凸的负对数似然函数。我们来检验一下。以 $\\nu$ 表示的负对数似然函数是：\n$$\nh(\\nu) = -\\ell(\\nu) = \\frac{n}{2} \\left[ \\log(\\nu + \\tau^2) + \\frac{s^2}{\\nu + \\tau^2} \\right]\n$$\n关于 $\\nu$ 的一阶导数是：\n$$\nh'(\\nu) = \\frac{n}{2} \\left[ \\frac{1}{\\nu + \\tau^2} - \\frac{s^2}{(\\nu + \\tau^2)^2} \\right]\n$$\n二阶导数是：\n$$\nh''(\\nu) = \\frac{n}{2} \\left[ -\\frac{1}{(\\nu + \\tau^2)^2} + \\frac{2s^2}{(\\nu + \\tau^2)^3} \\right] = \\frac{n}{2(\\nu + \\tau^2)^3} \\left[ -(\\nu + \\tau^2) + 2s^2 \\right] = \\frac{n}{2(\\nu + \\tau^2)^3} [2s^2 - \\tau^2 - \\nu]\n$$\n对于全局凸性，我们需要对所有 $\\nu  0$ 都有 $h''(\\nu)  0$。$h''(\\nu)$ 的符号由项 $2s^2 - \\tau^2 - \\nu$ 决定。随着 $\\nu$ 的增加，该项最终将变为负数。因此，负对数似然函数在 $\\nu = \\sigma^2$ 中不是全局凸的。该选项的核心主张是错误的。优化仍然受限于 $\\nu  0$。\n\n由于全局凸性的说法是错误的，选项 C 是不正确的。\n\n### 选项 B：在 $\\eta = \\log \\sigma$ 中优化\n\n此选项建议使用参数化 $\\eta = \\log \\sigma$，这意味着 $\\sigma = e^\\eta$。\n\n1.  **定义域**：约束 $\\sigma  0$ 被转换为 $-\\infty  \\eta  \\infty$。优化问题在实线 $\\mathbb{R}$ 上变为无约束问题。这对于标准的数值优化器（如 Newton-Raphson 或 BFGS）来说是一个主要优点，因为它完全“避免了边界违规”。这个说法是正确的。\n2.  **MLE 的不变性**：MLE 在重参数化下是不变的。如果 $\\widehat{\\eta}$ 是 $\\eta$ 的 MLE，那么 $\\sigma$ 的 MLE 就是 $\\widehat{\\sigma} = g(\\widehat{\\eta}) = e^{\\widehat{\\eta}}$。这个陈述是正确的。\n3.  **数值稳定性**：对于像 $\\sigma$ 这样可以跨越多个数量级的尺度参数，对数似然曲面在对数参数化 $\\eta$ 下通常表现得更好（即更接近二次型），并且梯度也被更好地缩放。$\\eta$ 中的固定步长对应于 $\\sigma$ 中的相对（乘法）变化，这对于尺度参数来说比固定的绝对变化更自然。声称这种参数化“通常能产生更好的缩放梯度和海森矩阵”是统计计算中一条标准且有效的经验法则。\n4.  **推断**：从 $\\widehat{\\eta}$ 的标准误获得 $\\widehat{\\sigma}$ 的标准误是通过 delta 方法完成的。给定 $Var(\\widehat{\\eta}) \\approx [I(\\widehat{\\eta})]^{-1}$，$\\widehat{\\sigma} = e^{\\widehat{\\eta}}$ 的方差近似为 $Var(\\widehat{\\sigma}) \\approx [\\frac{d\\sigma}{d\\eta}|_{\\widehat{\\eta}}]^2 Var(\\widehat{\\eta}) = (e^{\\widehat{\\eta}})^2 Var(\\widehat{\\eta}) = \\widehat{\\sigma}^2 Var(\\widehat{\\eta})$。这是一个标准的、正确的程序。它还有一个好处，即在 $\\eta$ 尺度上构建然后反向变换的置信区间，例如 $\\exp(\\widehat{\\eta} \\pm z_{\\alpha/2} \\text{SE}(\\widehat{\\eta}))$，对于 $\\sigma$ 将始终为正。\n\n此选项中的所有主张都是准确的，并代表了解决此类问题的最佳实践。它为偏好使用 $\\eta = \\log \\sigma$ 参数化提供了强有力的理由。\n\n因此，选项 B 是正确的。\n\n### 选项 D：在 $\\zeta = \\log \\sigma^2$ 中优化\n\n此选项建议使用参数化 $\\zeta = \\log \\sigma^2$。\n\n1.  **与 $\\eta$ 的等价性**：我们有 $\\zeta = \\log(\\sigma^2) = 2 \\log \\sigma = 2\\eta$。这意味着 $\\zeta$ 是 $\\eta$ 的一个简单线性重缩放。因此，它也将约束空间 $\\sigma^2  0$ 映射到无约束的实线 $\\mathbb{R}$，并继承了选项 B 中提到的对数尺度参数化的数值稳定性优势。这部分陈述是正确的。\n2.  **费雪信息**：该选项说明了费雪信息如何变换。对于重参数化 $\\theta_2 = g(\\theta_1)$，一般规则是 $I(\\theta_2) = I(\\theta_1) |\\frac{d\\theta_1}{d\\theta_2}|^2$。这里，$\\theta_1 = \\sigma$ 且 $\\theta_2 = \\zeta = 2 \\log \\sigma$。逆变换是 $\\sigma = \\exp(\\zeta/2)$。所需的导数是 $\\frac{\\partial\\sigma}{\\partial\\zeta} = \\frac{1}{2}\\exp(\\zeta/2) = \\frac{\\sigma}{2}$。因此，变换规则是 $I_\\zeta(\\zeta) = I_\\sigma(\\sigma) \\left(\\frac{\\partial\\sigma}{\\partial\\zeta}\\right)^2 = I_\\sigma(\\sigma) \\left(\\frac{\\sigma}{2}\\right)^2 = I_\\sigma(\\sigma)\\frac{\\sigma^2}{4}$。选项中提供的公式是正确的。\n3.  **渐近推断**：这种变换使我们能够关联估计量的渐近方差。直接导出的 $\\widehat{\\sigma}$ 的渐近方差是 $Var(\\widehat{\\sigma}) \\approx [I_\\sigma(\\sigma)]^{-1}$。如果我们使用 $\\zeta$，其 MLE 的方差是 $Var(\\widehat{\\zeta}) \\approx [I_\\zeta(\\zeta)]^{-1} = [I_\\sigma(\\sigma)\\frac{\\sigma^2}{4}]^{-1} = \\frac{4}{\\sigma^2}[I_\\sigma(\\sigma)]^{-1}$。使用 delta 方法得到 $\\widehat{\\sigma} = \\exp(\\widehat{\\zeta}/2)$ 的方差：\n    $$\n    Var(\\widehat{\\sigma}) \\approx \\left(\\frac{\\partial\\sigma}{\\partial\\zeta}\\right)^2 Var(\\widehat{\\zeta}) = \\left(\\frac{\\sigma}{2}\\right)^2 \\left(\\frac{4}{\\sigma^2}[I_\\sigma(\\sigma)]^{-1}\\right) = \\frac{\\sigma^2}{4} \\frac{4}{\\sigma^2} [I_\\sigma(\\sigma)]^{-1} = [I_\\sigma(\\sigma)]^{-1}\n    $$\n    $\\widehat{\\sigma}$ 的最终渐近方差是相同的。这表明在这些参数化下推断是一致的。因此，“对 $\\sigma$ 的渐近推断一致”的说法是正确的。这证明了该参数化的理论正则性。\n\n该选项正确地指出，由于与 $\\eta = \\log\\sigma$ 相同的原因，$\\zeta = \\log\\sigma^2$ 是一个极好的选择，并且它使用费雪信息正确地验证了在此重参数化下渐近推断的理论一致性。这为基于数值稳定性（继承自 $\\eta$）和理论正则性的选择提供了坚实的理由。\n\n因此，选项 D 是正确的。\n\n选项 B 和 D 都为基于数值稳定性和/或正则性的理由，使用对数尺度参数化（$\\log \\sigma$ 或 $\\log \\sigma^2$）提供了准确且有说服力的论证。\n\n最终评估：\n- A：不正确。\n- B：正确。\n- C：不正确。\n- D：正确。", "answer": "$$\\boxed{BD}$$", "id": "4578105"}, {"introduction": "标准的 MLE 方法在某些情况下可能会完全失效，理解并解决这些问题是高级数据分析的关键。本练习将引导你处理逻辑回归中一个著名的问题——“完全分离”，此时似然函数会变得无界，导致 MLE 无法收敛到有限值。这个动手编程实践不仅会让你亲眼见证标准 MLE 的失败，还将指导你实现一种强大的解决方案——基于杰弗里斯先验的惩罚似然估计（即 Firth 回归），它能保证在任何情况下都得到有限且偏差更小的估计。[@problem_id:4578052]", "problem": "考虑一个临床试验中的二元结果，其中每个观测值被建模为一个伯努利随机变量，其成功概率通过带有逻辑斯蒂连接函数的广义线性模型 (GLM) 与协变量相关联。基本假设是：(i) 给定参数时观测值条件独立；(ii) 每个结果服从伯努利似然；(iii) 逻辑斯蒂连接函数将线性预测器与成功概率关联起来。逻辑斯蒂回归的标准最大似然估计 (MLE) 是通过最大化对数似然函数得到的。然而，在小样本情况下，可能会出现两个问题：(a) 完全分离，此时似然函数在某些方向上无界，导致 MLE 不存在；(b) 小样本偏差，即有限样本估计存在系统性偏差。一种基于 Jeffreys 先验的惩罚似然方法（通常称为 Firth 偏差缩减逻辑斯蒂回归）修改了目标函数，以确保估计的存在性并减少偏差。\n\n您的任务是从第一性原理出发实现以下内容：\n- 通过牛顿-拉夫逊 (Newton–Raphson) 方法或迭代重加权最小二乘法 (IRLS) 实现的无惩罚逻辑斯蒂回归 MLE，从伯努利似然定义和逻辑斯蒂连接函数出发，并能检测在存在分离情况下的不存在性或收敛失败。\n- 一个使用 Jeffreys 先验的惩罚似然估计器，其构建方法是在伯努利对数似然上加上观测费雪信息行列式对数的一半作为惩罚项，并通过从相应的惩罚得分方程推导出的迭代方案求解。该估计器在完全分离情况下必须保持有限，并且相对于标准 MLE 必须能减少小样本偏差。\n\n使用以下定义作为唯一的出发点：\n- 对于观测 $i \\in \\{1,\\dots,n\\}$，其协变量行向量为 $\\mathbf{x}_i \\in \\mathbb{R}^{p}$，系数向量为 $\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}$，伯努利似然为 $L_i(\\boldsymbol{\\beta}) = \\pi_i^{y_i} (1 - \\pi_i)^{1 - y_i}$，其中 $\\pi_i = \\Pr(Y_i = 1 \\mid \\mathbf{x}_i)$。逻辑斯蒂连接函数要求 $\\operatorname{logit}(\\pi_i) = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$，因此 $\\pi_i = \\frac{1}{1 + e^{-\\mathbf{x}_i^\\top \\boldsymbol{\\beta}}}$。\n- 完整的（无惩罚）对数似然为 $\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left[ y_i \\log \\pi_i + (1 - y_i) \\log(1 - \\pi_i) \\right]$。\n- 逻辑斯蒂模型的观测费雪信息矩阵为 $\\mathbf{I}(\\boldsymbol{\\beta}) = \\mathbf{X}^\\top \\mathbf{W}(\\boldsymbol{\\beta}) \\mathbf{X}$，其中 $\\mathbf{X}$ 是 $n \\times p$ 的设计矩阵，其行为 $\\mathbf{x}_i^\\top$，$\\mathbf{W}(\\boldsymbol{\\beta})$ 是对角矩阵，其元素为 $w_i(\\boldsymbol{\\beta}) = \\pi_i (1 - \\pi_i)$。\n\n基于这些基本定义，推导您将为无惩罚和惩罚问题实现的迭代更新。不要使用任何用于逻辑斯蒂回归的预打包优化程序；相反，应明确构建牛顿或 IRLS 更新，并为惩罚方法构建惩罚得分和解决它的实用算法。\n\n科学真实性约束：\n- 所有模拟数据必须是小样本，并且对于二元临床试验是合理的。\n- 任何关于 MLE 不存在的声明都必须通过分离情况下似然函数的行为来证明，该行为由您的算法通过费雪信息和得分产生的奇异性或发散指标来识别。\n\n实现一个单一程序，该程序：\n- 构建三个测试用例（每个都包含一个截距项）：\n    1. 完全分离（一个预测变量）：$n = 8$, $\\mathbf{x} = [-2.0, -1.0, -0.5, -0.1, 0.1, 0.5, 1.0, 2.0]$, $\\mathbf{y} = [0, 0, 0, 0, 1, 1, 1, 1]$。\n    2. 非分离小样本（一个预测变量）：$n = 8$, $\\mathbf{x} = [-1.5, -0.9, -0.5, 0.0, 0.2, 0.4, 1.0, 1.3]$, $\\mathbf{y} = [0, 0, 1, 0, 0, 1, 1, 1]$。\n    3. 边界情况，仅有截距（无斜率）：$n = 6$, $\\mathbf{y} = [0, 0, 0, 0, 0, 0]$。\n- 对于每个用例：\n    - 通过牛顿/IRLS 方法拟合无惩罚逻辑斯蒂回归。\n    - 声明一个 MLE 成功的布尔指标，其定义为：在固定的迭代预算内收敛到一个有限的参数向量，其中线性系统是良态的，并且没有系数的绝对值超过一个大的阈值。如果未发生这种情况（例如，由于发散、病态或未能满足容差），则声明失败。\n    - 使用 Jeffreys 先验拟合惩罚似然估计器，并返回估计的系数。\n\n您必须实现的算法要求：\n- 从伯努利对数似然和逻辑斯蒂连接函数推导出的无惩罚 MLE 牛顿/IRLS 更新。\n- 从惩罚对数似然 $\\ell_{\\text{pen}}(\\boldsymbol{\\beta}) = \\ell(\\boldsymbol{\\beta}) + \\frac{1}{2} \\log \\left|\\mathbf{I}(\\boldsymbol{\\beta})\\right|$ 推导出的惩罚估计器，其相应的迭代更新仅依赖于 $\\mathbf{X}$、$\\mathbf{y}$ 和上面定义的量。您的推导必须从所提供的定义开始，并产生一个在所有三种情况下都能收敛的稳定算法。\n\n数值和输出要求：\n- 完全按照上述规定使用确定性数据；不要使用任何外部随机性。\n- 对于每个测试用例，返回一个列表，其第一个元素是一个布尔值，指示无惩罚 MLE 是否收敛；第二个元素是一个包含惩罚（Jeffreys 先验）系数估计值的列表，顺序为：首先是截距，然后是斜率。将所有报告的系数估计值四舍五入到六位小数。\n- 您的程序应生成单行输出，其中包含三个测试用例的结果，格式为一个由方括号括起来的逗号分隔列表，其中每个元素是上面指定的每个测试用例的列表。例如，整体结构必须类似于 $[ [\\cdot, [\\cdot,\\dots]], [\\cdot, [\\cdot,\\dots]], [\\cdot, [\\cdot,\\dots]] ]$。\n\n测试套件摘要：\n- 用例 1（完全分离）：预期无惩罚 MLE 会失败；惩罚估计器将返回有限的系数。\n- 用例 2（非分离）：预期无惩罚 MLE 会成功；惩罚估计器将返回与无惩罚估计值相近（但不完全相同）的有限系数。\n- 用例 3（全为零，仅截距）：预期无惩罚 MLE 会失败；惩罚估计器将返回一个有限的截距。\n\n最终打印的输出必须是包含指定聚合列表的单行。不涉及单位或角度；所有数值答案都是无单位的实数。", "solution": "用户要求从第一性原理出发实现两种逻辑斯蒂回归估计器：标准的无惩罚最大似然估计器 (MLE) 和使用 Jeffreys 先验的惩罚估计器（也称为 Firth 逻辑斯蒂回归）。实现必须能处理诸如完全分离之类的小样本问题。\n\n### 问题验证\n该问题定义明确且科学上合理，基于广义线性模型 (GLM) 的似然推断的既定原则。所有数学定义、数据和约束都已提供且内部一致。这是一个计算统计学中标准但高级的练习。该问题是有效的。\n\n### 算法推导\n\n#### 1. 无惩罚最大似然估计器 (MLE)\n\n目标是找到系数向量 $\\boldsymbol{\\beta}$，以最大化一系列独立伯努利试验的对数似然函数：\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left[ y_i \\log \\pi_i + (1 - y_i) \\log(1 - \\pi_i) \\right] $$\n其中 $\\pi_i = (1 + e^{-\\mathbf{x}_i^\\top \\boldsymbol{\\beta}})^{-1}$ 是观测 $i$ 在给定其协变量向量 $\\mathbf{x}_i$ 时的成功概率。\n\n我们使用牛顿-拉夫逊 (Newton-Raphson) 方法，该方法通过更新参数估计来迭代地寻找最大值：\n$$ \\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - [\\mathbf{H}(\\boldsymbol{\\beta}^{(t)})]^{-1} \\mathbf{g}(\\boldsymbol{\\beta}^{(t)}) $$\n其中 $\\mathbf{g}(\\boldsymbol{\\beta})$ 是梯度（得分向量），$\\mathbf{H}(\\boldsymbol{\\beta})$ 是对数似然的海森矩阵。\n\n**梯度（得分向量）：**$\\ell(\\boldsymbol{\\beta})$ 关于 $\\beta_j$ 的梯度是：\n$$ \\frac{\\partial \\ell}{\\partial \\beta_j} = \\sum_{i=1}^n (y_i - \\pi_i) x_{ij} $$\n用向量形式表示，即为 $\\mathbf{g}(\\boldsymbol{\\beta}) = \\mathbf{X}^\\top (\\mathbf{y} - \\boldsymbol{\\pi})$，其中 $\\mathbf{X}$ 是 $n \\times p$ 的设计矩阵，$\\mathbf{y}$ 是结果向量，$\\boldsymbol{\\pi}$ 是概率向量。\n\n**海森矩阵：**二阶导数为：\n$$ \\frac{\\partial^2 \\ell}{\\partial \\beta_k \\partial \\beta_j} = -\\sum_{i=1}^n \\pi_i(1-\\pi_i) x_{ij} x_{ik} $$\n用矩阵形式表示，$\\mathbf{H}(\\boldsymbol{\\beta}) = -\\mathbf{X}^\\top \\mathbf{W}(\\boldsymbol{\\beta}) \\mathbf{X}$，其中 $\\mathbf{W}(\\boldsymbol{\\beta})$ 是一个 $n \\times n$ 的对角矩阵，其元素为 $w_i = \\pi_i(1-\\pi_i)$。海森矩阵的负数是观测费雪信息矩阵，$\\mathbf{I}(\\boldsymbol{\\beta}) = -\\mathbf{H}(\\boldsymbol{\\beta}) = \\mathbf{X}^\\top \\mathbf{W}(\\boldsymbol{\\beta}) \\mathbf{X}$。\n\n**牛顿-拉夫逊更新：**将梯度和海森矩阵代入更新规则，得到：\n$$ \\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} + [\\mathbf{I}(\\boldsymbol{\\beta}^{(t)})]^{-1} \\mathbf{g}(\\boldsymbol{\\beta}^{(t)}) $$\n$$ \\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} + (\\mathbf{X}^\\top \\mathbf{W}^{(t)} \\mathbf{X})^{-1} \\mathbf{X}^\\top (\\mathbf{y} - \\boldsymbol{\\pi}^{(t)}) $$\n这等价于迭代重加权最小二乘法 (IRLS) 算法。在存在完全或准完全分离的情况下，某些 $\\pi_i$ 将趋近于 $0$ 或 $1$，导致相应的权重 $w_i$ 趋近于 $0$。这会导致费雪信息矩阵 $\\mathbf{I}$ 的奇异性，并且系数会发散。算法失败，我们通过监控 $\\mathbf{I}$ 的条件数和系数 $\\boldsymbol{\\beta}$ 的大小来检测这一点。\n\n#### 2. 惩罚估计器（Firth 逻辑斯蒂回归）\n\n该方法最大化一个包含了 Jeffreys 先验的惩罚对数似然：\n$$ \\ell_{\\text{pen}}(\\boldsymbol{\\beta}) = \\ell(\\boldsymbol{\\beta}) + \\frac{1}{2} \\log |\\mathbf{I}(\\boldsymbol{\\beta})| $$\n惩罚项 $\\frac{1}{2} \\log |\\mathbf{I}(\\boldsymbol{\\beta})|$ 防止了在分离情况下似然函数无界，从而确保 $\\boldsymbol{\\beta}$ 的有限估计始终存在。\n\n我们旨在求解惩罚得分方程 $\\mathbf{g}_{\\text{pen}}(\\boldsymbol{\\beta}) = \\nabla \\ell_{\\text{pen}}(\\boldsymbol{\\beta}) = \\mathbf{0}$。惩罚得分是标准得分与惩罚项梯度的和。\n$$ \\mathbf{g}_{\\text{pen}}(\\boldsymbol{\\beta}) = \\mathbf{g}(\\boldsymbol{\\beta}) + \\nabla \\left( \\frac{1}{2} \\log |\\mathbf{I}(\\boldsymbol{\\beta})| \\right) $$\n\n**惩罚项的梯度：**使用对数行列式导数的公式，惩罚项梯度的第 $j$ 个分量是：\n$$ \\frac{\\partial}{\\partial \\beta_j} \\left(\\frac{1}{2} \\log |\\mathbf{I}(\\boldsymbol{\\beta})|\\right) = \\frac{1}{2} \\operatorname{tr}\\left( \\mathbf{I}(\\boldsymbol{\\beta})^{-1} \\frac{\\partial \\mathbf{I}(\\boldsymbol{\\beta})}{\\partial \\beta_j} \\right) $$\n费雪信息矩阵的导数是 $\\frac{\\partial \\mathbf{I}}{\\partial \\beta_j} = \\mathbf{X}^\\top \\frac{\\partial \\mathbf{W}}{\\partial \\beta_j} \\mathbf{X}$。权重 $w_i = \\pi_i(1-\\pi_i)$ 的导数是 $\\frac{\\partial w_i}{\\partial \\beta_j} = (1-2\\pi_i)\\pi_i(1-\\pi_i)x_{ij} = (1-2\\pi_i)w_i x_{ij}$。\n利用迹的循环性质进行代入和简化，我们发现：\n$$ \\frac{\\partial}{\\partial \\beta_j} \\left(\\frac{1}{2} \\log |\\mathbf{I}(\\boldsymbol{\\beta})|\\right) = \\frac{1}{2} \\sum_{i=1}^n h_{ii} (1-2\\pi_i) w_i x_{ij} $$\n其中 $h_{ii}$ 是“帽子”矩阵 $\\mathbf{A} = \\mathbf{X} \\mathbf{I}(\\boldsymbol{\\beta})^{-1} \\mathbf{X}^\\top$ 的对角元素。这些也被称为杠杆值。\n\n**惩罚得分向量：**完整的惩罚得分向量是：\n$$ \\mathbf{g}_{\\text{pen}}(\\boldsymbol{\\beta}) = \\mathbf{X}^\\top (\\mathbf{y} - \\boldsymbol{\\pi}) + \\frac{1}{2} \\mathbf{X}^\\top \\mathbf{v}(\\boldsymbol{\\beta}) $$\n其中 $\\mathbf{v}(\\boldsymbol{\\beta})$ 是一个向量，其元素为 $v_i = h_{ii} (1 - 2\\pi_i) w_i$。\n\n**迭代解法：**我们使用类牛顿法求解 $\\mathbf{g}_{\\text{pen}}(\\boldsymbol{\\beta}) = \\mathbf{0}$，用观测费雪信息矩阵 $-\\mathbf{I}(\\boldsymbol{\\beta})$ 来近似惩罚对数似然的海森矩阵。这给出了更新步骤：\n$$ \\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} + [\\mathbf{I}(\\boldsymbol{\\beta}^{(t)})]^{-1} \\mathbf{g}_{\\text{pen}}(\\boldsymbol{\\beta}^{(t)}) $$\n该算法保证即使在涉及分离的测试用例中也能收敛到 $\\boldsymbol{\\beta}$ 的一个有限估计。对于所有结果均为零（$y=0$）的仅截距模型，截距 $\\beta_0$ 的标准 MLE 为 $-\\infty$。惩罚估计器产生一个有限值，对应于有效概率 $\\pi = 1/(2n+2)$。\n\n### 实现策略\n\n该解决方案使用 Python 的 `numpy` 库实现。\n- 函数 `fit_mle` 实现了无惩罚的牛顿-拉夫逊算法。它包括检查收敛性、系数发散（绝对值超过阈值）和费雪矩阵的病态性，以确定成功或失败。\n- 函数 `fit_penalized` 实现了惩罚估计器的迭代方案。在每一步中，它计算标准得分和惩罚梯度以形成惩罚得分，然后应用牛顿更新。\n- 一个主程序按规定设置三个测试用例，为每个用例调用两个拟合函数，并将结果格式化为所需的单行字符串输出。通过将概率值裁剪以远离 $0$ 和 $1$ 来处理数值稳定性问题。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_case(X, y):\n    \"\"\"\n    Fits both unpenalized and penalized logistic regression models.\n    \n    Args:\n        X (np.ndarray): Design matrix (n_samples, n_features).\n        y (np.ndarray): Response vector (n_samples,).\n        \n    Returns:\n        list: A list containing [mle_success (bool), penalized_coeffs (list)].\n    \"\"\"\n    # Part 1: Unpenalized Maximum Likelihood Estimation (MLE)\n    beta_mle = np.zeros(X.shape[1])\n    mle_success = False\n    max_iter = 100\n    tol = 1e-8\n    beta_threshold = 50.0  # Threshold to detect diverging coefficients\n    cond_threshold = 1e12 # Threshold for matrix condition number\n\n    mle_converged_in_time = False\n    try:\n        for i in range(max_iter):\n            eta = X @ beta_mle\n            pi = 1 / (1 + np.exp(-eta))\n            \n            # Clip for numerical stability\n            pi = np.clip(pi, 1e-10, 1 - 1e-10)\n            \n            w = pi * (1 - pi)\n            \n            # Gradient (score vector)\n            gradient = X.T @ (y - pi)\n            \n            # Fisher Information Matrix (using efficient multiplication)\n            I = X.T * w @ X\n            \n            # Check for singularity/ill-conditioning, a sign of separation\n            if np.linalg.cond(I) > cond_threshold:\n                mle_success = False\n                break\n            \n            # Solve for the update step using a stable solver\n            delta = np.linalg.solve(I, gradient)\n            beta_mle += delta\n            \n            # Check for coefficient divergence, another sign of separation\n            if np.any(np.abs(beta_mle) > beta_threshold):\n                mle_success = False\n                break\n            \n            # Check for convergence\n            if np.linalg.norm(delta)  tol:\n                mle_success = True\n                mle_converged_in_time = True\n                break\n    except np.linalg.LinAlgError:\n        # Failure due to singular matrix\n        mle_success = False\n        \n    if not mle_converged_in_time:\n        mle_success = False\n\n    # Part 2: Penalized Likelihood Estimator (Firth's method)\n    beta_pen = np.zeros(X.shape[1])\n    max_iter_pen = 100\n    tol_pen = 1e-8\n    \n    for _ in range(max_iter_pen):\n        beta_old = beta_pen.copy()\n        \n        eta = X @ beta_pen\n        pi = 1 / (1 + np.exp(-eta))\n        \n        # Clip for numerical stability\n        pi = np.clip(pi, 1e-10, 1 - 1e-10)\n        \n        w = pi * (1 - pi)\n        \n        # Fisher Information Matrix\n        I = X.T * w @ X\n        \n        try:\n            I_inv = np.linalg.inv(I)\n        except np.linalg.LinAlgError:\n            # Add a small ridge for stability if computationally singular\n            I_inv = np.linalg.inv(I + np.eye(I.shape[0]) * 1e-9)\n\n        # Leverages: diagonal of X @ I_inv @ X.T\n        h = np.sum((X @ I_inv) * X, axis=1)\n        \n        # Standard score vector\n        gradient = X.T @ (y - pi)\n        \n        # Gradient of the penalty term\n        penalty_gradient = 0.5 * X.T @ (h * (1 - 2 * pi) * w)\n        \n        # Penalized score vector\n        penalized_gradient = gradient + penalty_gradient\n        \n        # Newton-Raphson update step\n        delta = I_inv @ penalized_gradient\n        beta_pen += delta\n        \n        if np.linalg.norm(beta_pen - beta_old)  tol_pen:\n            break\n            \n    # Round coefficients to six decimal places for output\n    pen_coeffs_rounded = [round(b, 6) for b in beta_pen]\n    \n    return [mle_success, pen_coeffs_rounded]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Case 1: Complete separation\n    x1 = np.array([-2.0, -1.0, -0.5, -0.1, 0.1, 0.5, 1.0, 2.0])\n    y1 = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n    X1 = np.c_[np.ones(x1.shape[0]), x1]\n\n    # Case 2: Non-separated small sample\n    x2 = np.array([-1.5, -0.9, -0.5, 0.0, 0.2, 0.4, 1.0, 1.3])\n    y2 = np.array([0, 0, 1, 0, 0, 1, 1, 1])\n    X2 = np.c_[np.ones(x2.shape[0]), x2]\n\n    # Case 3: Boundary case (all zeros, intercept-only)\n    y3 = np.array([0, 0, 0, 0, 0, 0])\n    X3 = np.ones((y3.shape[0], 1))\n    \n    test_cases = [\n        (X1, y1),\n        (X2, y2),\n        (X3, y3),\n    ]\n\n    results = []\n    for X, y in test_cases:\n        case_result = solve_case(X, y)\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The str() representation of a list of lists is '[[], [], []]'\n    # We remove spaces to match the implicit tight format requested.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n\n```", "id": "4578052"}]}