{"hands_on_practices": [{"introduction": "组织样本的批量基因表达数据（bulk RNA-seq）实际上是其包含的多种细胞类型信号的混合体。在许多疾病研究中，理解细胞组成的改变是揭示疾病机制的关键。本实践练习 [@problem_id:4574619] 将带你运用一种关键的生物信息学技术——细胞类型反卷积（cell-type deconvolution），利用单细胞参考数据来估算批量样本中的细胞组分比例。通过这个练习，你将亲手生成具有明确生物学意义的特征，这些特征可以直接用于与临床表型进行整合分析。", "problem": "您将获得一个框架，通过使用线性混合模型和非负最小二乘法，从单细胞参考中估计细胞类型比例，从而将核糖核酸测序 (RNA-seq) 的批量基因表达数据与临床表型相结合。其原理是，批量表达是细胞类型特异性表达谱的混合物，这与分子生物学中心法则以及组织样本内各组成细胞类型的转录本计数的加和性相一致。由此可得一个线性模型：对于 $G$ 个基因和 $C$ 种细胞类型，批量表达向量 $\\mathbf{y} \\in \\mathbb{R}^{G}$ 可建模为\n$$\n\\mathbf{y} = X \\mathbf{p} + \\boldsymbol{\\varepsilon},\n$$\n其中 $X \\in \\mathbb{R}^{G \\times C}$ 是参考矩阵，其元素为源自单细胞测量的每个基因在每种细胞类型中的平均表达；$\\mathbf{p} \\in \\mathbb{R}^{C}$ 是未知的细胞类型比例非负向量；$\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^{G}$ 是测量噪声。生物学约束意味着对所有 $c$ 都有 $p_c \\geq 0$，并且在将 $\\mathbf{p}$ 解释为分数时，有 $\\sum_{c=1}^{C} p_c = 1$。在实践中，可以通过求解以下非负最小二乘问题来估计 $\\mathbf{p}$：\n$$\n\\min_{\\mathbf{p} \\in \\mathbb{R}^{C}} \\|\\;X \\mathbf{p} - \\mathbf{y}\\;\\|_2^2 \\quad \\text{subject to} \\quad p_c \\geq 0 \\text{ for all } c,\n$$\n然后将 $\\mathbf{p}$ 重新归一化使其和为1，因为最小二乘法不强制施加和为1的约束。其准确性可以通过与已知比例的模拟混合物进行比较来评估。\n\n请实现一个程序，为提供的测试套件完成以下任务：\n- 对于每个测试用例，根据案例描述构建指定的单细胞参考矩阵 $X$、真实比例 $\\mathbf{p}^{\\star}$（或其变体）以及批量向量 $\\mathbf{y}$。\n- 通过求解非负最小二乘问题来估计 $\\widehat{\\mathbf{p}}$，并在 $\\sum_{c=1}^{C} \\widehat{p}_c > 0$ 时将其重新归一化以满足 $\\sum_{c=1}^{C} \\widehat{p}_c = 1$，否则对所有 $c$ 设置 $\\widehat{p}_c = \\frac{1}{C}$。\n- 使用以下公式计算每个案例中 $\\widehat{\\mathbf{p}}$ 与指定的真实比例之间的均方根误差 (RMSE)：\n$$\n\\mathrm{RMSE}(\\widehat{\\mathbf{p}}, \\mathbf{p}^{\\dagger}) = \\sqrt{\\frac{1}{C} \\sum_{c=1}^{C} \\left(\\widehat{p}_c - p^{\\dagger}_c\\right)^2}.\n$$\n- 对于临床表型案例，模拟一小批具有已知比例的批量样本，并计算指定细胞类型的估计分数与临床严重程度评分向量 $\\mathbf{s}$ 之间的皮尔逊相关系数。使用以下公式：\n$$\nr = \\frac{\\sum_{t=1}^{T} \\left(\\widehat{p}_{j}^{(t)} - \\overline{\\widehat{p}_{j}}\\right)\\left(s^{(t)} - \\overline{s}\\right)}{\\sqrt{\\sum_{t=1}^{T}\\left(\\widehat{p}_{j}^{(t)} - \\overline{\\widehat{p}_{j}}\\right)^2}\\sqrt{\\sum_{t=1}^{T}\\left(s^{(t)} - \\overline{s}\\right)^2}},\n$$\n其中 $\\widehat{p}_{j}^{(t)}$ 是样本 $t$ 中细胞类型 $j$ 的估计分数，$\\overline{\\widehat{p}_{j}}$ 是其跨样本的均值，$s^{(t)}$ 是样本 $t$ 的临床严重程度，$\\overline{s}$ 是其均值。比例应以小数形式表示，而非百分比。\n\n测试套件规范：\n- 所有矩阵和向量都已明确给出。请完全按照规定使用它们，不要进行任何超出上述描述的额外归一化或变换。所有数值均为小数，任何派生值也应视为小数。\n\n案例1（良态参考，无噪声混合物）：\n- 基因数 $G = 5$，细胞类型数 $C = 3$。\n- 参考矩阵\n$$\nX_1 = \\begin{bmatrix}\n8  & 2  & 1 \\\\\n4  & 1.5  & 2 \\\\\n3  & 3  & 6 \\\\\n5  & 1  & 0.5 \\\\\n7  & 2.5  & 1\n\\end{bmatrix}.\n$$\n- 真实比例\n$$\n\\mathbf{p}^{\\star}_1 = \\begin{bmatrix} 0.5 \\\\ 0.3 \\\\ 0.2 \\end{bmatrix}.\n$$\n- 批量向量\n$$\n\\mathbf{y}_1 = X_1 \\mathbf{p}^{\\star}_1 = \\begin{bmatrix} 4.8 \\\\ 2.85 \\\\ 3.6 \\\\ 2.9 \\\\ 4.45 \\end{bmatrix}.\n$$\n- 用于计算RMSE的真实值：$\\mathbf{p}^{\\dagger} = \\mathbf{p}^{\\star}_1$。\n\n案例2（细胞类型表达谱之间存在近共线性）：\n- 参考矩阵\n$$\nX_2 = \\begin{bmatrix}\n6  & 6.2  & 1 \\\\\n3  & 3.1  & 2 \\\\\n5  & 5.1  & 6 \\\\\n4  & 4.05  & 0.5 \\\\\n2  & 2.02  & 1\n\\end{bmatrix}.\n$$\n- 真实比例\n$$\n\\mathbf{p}^{\\star}_2 = \\begin{bmatrix} 0.4 \\\\ 0.4 \\\\ 0.2 \\end{bmatrix}.\n$$\n- 批量向量\n$$\n\\mathbf{y}_2 = X_2 \\mathbf{p}^{\\star}_2 = \\begin{bmatrix} 5.08 \\\\ 2.84 \\\\ 5.24 \\\\ 3.32 \\\\ 1.808 \\end{bmatrix}.\n$$\n- 用于计算RMSE的真实值：$\\mathbf{p}^{\\dagger} = \\mathbf{p}^{\\star}_2$。\n\n案例3（批量样本中存在未建模的细胞类型）：\n- 使用 $X_3 = X_1$。\n- 已知细胞类型比例（和不为1）\n$$\n\\mathbf{p}^{\\text{known}}_3 = \\begin{bmatrix} 0.5 \\\\ 0.25 \\\\ 0.15 \\end{bmatrix}, \\quad \\text{所以} \\quad \\sum_{c=1}^{3} p^{\\text{known}}_{3,c} = 0.9.\n$$\n- 未知细胞类型分数为 $0.1$，其表达向量为\n$$\n\\mathbf{u} = \\begin{bmatrix} 4 \\\\ 1 \\\\ 3 \\\\ 2 \\\\ 5 \\end{bmatrix}.\n$$\n- 批量向量\n$$\n\\mathbf{y}_3 = X_3 \\mathbf{p}^{\\text{known}}_3 + 0.1 \\cdot \\mathbf{u} = \\begin{bmatrix} 5.05 \\\\ 2.775 \\\\ 3.45 \\\\ 3.025 \\\\ 4.775 \\end{bmatrix}.\n$$\n- 用于计算RMSE的真实值：将已知比例归一化，使其和为1，\n$$\n\\mathbf{p}^{\\dagger} = \\frac{\\mathbf{p}^{\\text{known}}_3}{0.9} = \\begin{bmatrix} 0.5555555556 \\\\ 0.2777777778 \\\\ 0.1666666667 \\end{bmatrix}.\n$$\n\n案例4（测量噪声）：\n- 使用 $X_4 = X_1$。\n- 真实比例\n$$\n\\mathbf{p}^{\\star}_4 = \\begin{bmatrix} 0.3 \\\\ 0.5 \\\\ 0.2 \\end{bmatrix}.\n$$\n- 无噪声批量向量\n$$\nX_4 \\mathbf{p}^{\\star}_4 = \\begin{bmatrix} 3.6 \\\\ 2.35 \\\\ 3.6 \\\\ 2.1 \\\\ 3.55 \\end{bmatrix}.\n$$\n- 加性噪声向量\n$$\n\\boldsymbol{\\eta} = \\begin{bmatrix} 0.05 \\\\ -0.02 \\\\ 0.03 \\\\ -0.04 \\\\ 0.01 \\end{bmatrix}.\n$$\n- 批量向量\n$$\n\\mathbf{y}_4 = X_4 \\mathbf{p}^{\\star}_4 + \\boldsymbol{\\eta} = \\begin{bmatrix} 3.65 \\\\ 2.33 \\\\ 3.63 \\\\ 2.06 \\\\ 3.56 \\end{bmatrix}.\n$$\n- 用于计算RMSE的真实值：$\\mathbf{p}^{\\dagger} = \\mathbf{p}^{\\star}_4$。\n\n案例5（文库大小缩放不匹配）：\n- 使用 $X_5 = X_1$。\n- 真实比例\n$$\n\\mathbf{p}^{\\star}_5 = \\mathbf{p}^{\\star}_1 = \\begin{bmatrix} 0.5 \\\\ 0.3 \\\\ 0.2 \\end{bmatrix}.\n$$\n- 批量向量是案例1的缩放版本，\n$$\n\\mathbf{y}_5 = 2 \\cdot \\mathbf{y}_1 = \\begin{bmatrix} 9.6 \\\\ 5.7 \\\\ 7.2 \\\\ 5.8 \\\\ 8.9 \\end{bmatrix}.\n$$\n- 用于计算RMSE的真实值：$\\mathbf{p}^{\\dagger} = \\mathbf{p}^{\\star}_5$。\n\n案例6（与临床严重程度表型整合）：\n- 使用 $X_6 = X_1$。\n- 样本数 $T = 6$。\n- 每个样本的真实比例（列表示细胞类型1,2,3）：\n$$\n\\mathbf{p}^{\\star (1)} = \\begin{bmatrix} 0.2 \\\\ 0.5 \\\\ 0.3 \\end{bmatrix},\\quad\n\\mathbf{p}^{\\star (2)} = \\begin{bmatrix} 0.1 \\\\ 0.5 \\\\ 0.4 \\end{bmatrix},\\quad\n\\mathbf{p}^{\\star (3)} = \\begin{bmatrix} 0.05 \\\\ 0.45 \\\\ 0.5 \\end{bmatrix},\\quad\n\\mathbf{p}^{\\star (4)} = \\begin{bmatrix} 0.3 \\\\ 0.6 \\\\ 0.1 \\end{bmatrix},\\quad\n\\mathbf{p}^{\\star (5)} = \\begin{bmatrix} 0.05 \\\\ 0.35 \\\\ 0.6 \\end{bmatrix},\\quad\n\\mathbf{p}^{\\star (6)} = \\begin{bmatrix} 0.15 \\\\ 0.55 \\\\ 0.3 \\end{bmatrix}.\n$$\n- 临床严重程度评分向量\n$$\n\\mathbf{s} = \\begin{bmatrix} 0.3 \\\\ 0.5 \\\\ 0.7 \\\\ 0.2 \\\\ 0.9 \\\\ 0.4 \\end{bmatrix}.\n$$\n- 对于每个样本 $t \\in \\{1,\\dots,6\\}$，批量向量是\n$$\n\\mathbf{y}_6^{(t)} = X_6 \\mathbf{p}^{\\star (t)} + \\boldsymbol{\\delta},\n$$\n带有小的固定加性噪声\n$$\n\\boldsymbol{\\delta} = \\begin{bmatrix} 0.01 \\\\ -0.01 \\\\ 0.0 \\\\ 0.02 \\\\ -0.02 \\end{bmatrix}.\n$$\n- 为每个样本估计 $\\widehat{\\mathbf{p}}^{(t)}$，并计算细胞类型3的估计分数（$\\widehat{\\mathbf{p}}^{(t)}$ 的第三个分量）与向量 $\\mathbf{s}$ 之间的皮尔逊相关系数 $r$。以小数形式报告 $r$。\n\n输出规范：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按顺序为：案例1的RMSE、案例2的RMSE、案例3的RMSE、案例4的RMSE、案例5的RMSE以及案例6的皮尔逊相关系数r。每个值必须四舍五入到6位小数。例如，输出应如下所示：\n$$\n[\\text{value}_1,\\text{value}_2,\\text{value}_3,\\text{value}_4,\\text{value}_5,\\text{value}_6].\n$$\n所有比例都应解释为小数（0到1之间的分数），而不是百分比。", "solution": "出发点是用于批量基因表达的线性混合模型。由于来自不同细胞类型的转录本计数是可加的，因此基因 $g$ 的一个批量测量值可以表示为\n$$\ny_g = \\sum_{c=1}^{C} X_{gc} p_c + \\varepsilon_g,\n$$\n其中 $X_{gc}$ 是基因 $g$ 在细胞类型 $c$ 中的平均表达，$p_c$ 是样本中细胞类型 $c$ 的比例，而 $\\varepsilon_g$ 则捕捉了测量噪声和模型不匹配。由于这些量是计数或分数，我们有 $X_{gc} \\geq 0$，$p_c \\geq 0$，并且在分数解释下 $\\sum_{c=1}^{C} p_c = 1$。跨所有基因的模型可以简洁地写成\n$$\n\\mathbf{y} = X \\mathbf{p} + \\boldsymbol{\\varepsilon}.\n$$\n\n为了估计 $\\mathbf{p}$，我们将其转换为一个凸优化问题。在没有噪声的情况下，最小二乘估计量求解\n$$\n\\min_{\\mathbf{p}} \\left\\| X \\mathbf{p} - \\mathbf{y} \\right\\|_2^2.\n$$\n生物学约束意味着 $\\mathbf{p}$ 的非负性，从而得到非负最小二乘 (NNLS) 问题：\n$$\n\\min_{\\mathbf{p} \\in \\mathbb{R}^{C}} \\left\\| X \\mathbf{p} - \\mathbf{y} \\right\\|_2 \\quad \\text{subject to} \\quad p_c \\geq 0 \\text{ for all } c.\n$$\n这是一个凸问题，当 $X$ 具有满列秩时，它有唯一的极小值点，并且由于非负性约束和二次目标函数，在近共线性情况下表现稳定。Karush–Kuhn–Tucker (KKT) 条件确保在最优点，要么 $\\widehat{p}_c = 0$，要么对于活跃变量满足相应的梯度条件。在计算上，我们通过激活集方法（已在标准库中实现）求解NNLS，该方法迭代地识别被约束在零的变量子集，并对其余变量求解无约束最小二乘问题，直到达到最优性。\n\nNNLS解 $\\widehat{\\mathbf{p}}$ 并不强制和为1的约束。然而，当将 $\\widehat{\\mathbf{p}}$ 解释为分数时，重新归一化是直接的：\n$$\n\\widehat{\\mathbf{p}} \\leftarrow \\frac{\\widehat{\\mathbf{p}}}{\\sum_{c=1}^{C} \\widehat{p}_c} \\quad \\text{if} \\quad \\sum_{c=1}^{C} \\widehat{p}_c > 0,\n$$\n以确保 $\\sum_{c=1}^{C} \\widehat{p}_c = 1$。如果由于退化（全零估计）导致 $\\sum_{c=1}^{C} \\widehat{p}_c = 0$，一个合理的备用方案是均匀分布\n$$\n\\widehat{p}_c = \\frac{1}{C} \\quad \\text{for all } c.\n$$\n\n准确性通过 $\\widehat{\\mathbf{p}}$ 和真实值 $\\mathbf{p}^{\\dagger}$ 之间的均方根误差 (RMSE) 来量化：\n$$\n\\mathrm{RMSE}(\\widehat{\\mathbf{p}}, \\mathbf{p}^{\\dagger}) = \\sqrt{\\frac{1}{C} \\sum_{c=1}^{C} \\left(\\widehat{p}_c - p^{\\dagger}_c\\right)^2}.\n$$\n在案例1中，参考是良态的，混合物是无噪声的，因此经过重新归一化后，NNLS应该能以可忽略的误差恢复 $\\mathbf{p}^{\\star}_1$。在案例2中，$X_2$ 的列近似共线；NNLS仍然适用，但由于病态条件，解可能不那么精确，不过重新归一化仍将保证其可解释性。在案例3中，批量样本包含一个未建模的细胞类型。正确的评估应将 $\\widehat{\\mathbf{p}}$ 与归一化的已知比例 $\\mathbf{p}^{\\dagger} = \\mathbf{p}^{\\text{known}}_3 / 0.9$ 进行比较，因为未知组分并未在 $X$ 中表示。在案例4中，加性噪声扰动了 $\\mathbf{y}$，NNLS会权衡残差误差以拟合带噪声的观测值，导致一个虽小但非零的RMSE。在案例5中，$\\mathbf{y}$ 的全局缩放对应于文库大小的差异；NNLS后跟重新归一化的过程对此类缩放是不变的，因此RMSE应接近于零。\n\n对于与临床表型的整合（案例6），我们为每个样本 $t$ 估计 $\\widehat{\\mathbf{p}}^{(t)}$，并关注特定细胞类型（此处为细胞类型3）的比例。估计的比例 $\\{\\widehat{p}_3^{(t)}\\}_{t=1}^{T}$ 与严重程度评分 $\\{s^{(t)}\\}_{t=1}^{T}$ 之间的皮尔逊相关系数为\n$$\nr = \\frac{\\sum_{t=1}^{T} \\left(\\widehat{p}_{3}^{(t)} - \\overline{\\widehat{p}_{3}}\\right)\\left(s^{(t)} - \\overline{s}\\right)}{\\sqrt{\\sum_{t=1}^{T}\\left(\\widehat{p}_{3}^{(t)} - \\overline{\\widehat{p}_{3}}\\right)^2}\\sqrt{\\sum_{t=1}^{T}\\left(s^{(t)} - \\overline{s}\\right)^2}},\n$$\n该系数衡量了估计的细胞类型3丰度与临床严重程度之间的线性关联，这是将组学衍生的特征与临床表型整合的一种标准方法。\n\n每个测试用例的算法步骤：\n- 完全按照规定构建 $X$ 和 $\\mathbf{y}$。\n- 求解NNLS以获得 $\\widehat{\\mathbf{p}}$。\n- 如果可能，将 $\\widehat{\\mathbf{p}}$ 重新归一化使其和为1，否则将其设置为均匀分数。\n- 对于案例1-5，根据相应的 $\\mathbf{p}^{\\dagger}$ 计算 $\\mathrm{RMSE}$；对于案例6，计算皮尔逊相关系数 $r$。\n\n最后，将六个结果聚合到一行中，格式为 $[\\text{value}_1,\\dots,\\text{value}_6]$，每个值四舍五入到6位小数并以小数形式表示（无百分号）。这为不同条件下的反卷积准确性及其与临床表型整合的相关性提供了一个简洁的总结。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import nnls\n\ndef nnls_deconvolution(X, y):\n    \"\"\"\n    Solve nonnegative least squares for p in y ≈ X p.\n    Returns p_hat renormalized to sum to 1 (if possible).\n    \"\"\"\n    p_hat, _ = nnls(X, y)\n    s = p_hat.sum()\n    if s > 0:\n        p_hat = p_hat / s\n    else:\n        # Fallback to uniform distribution if degenerate (sum == 0)\n        C = X.shape[1]\n        p_hat = np.full(C, 1.0 / C)\n    return p_hat\n\ndef rmse(p_hat, p_true):\n    \"\"\"\n    Root Mean Squared Error between estimated and true proportions.\n    \"\"\"\n    diff = p_hat - p_true\n    return np.sqrt(np.mean(diff * diff))\n\ndef pearson_correlation(x, y):\n    \"\"\"\n    Pearson correlation coefficient between two 1D arrays.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    x_mean = np.mean(x)\n    y_mean = np.mean(y)\n    x_centered = x - x_mean\n    y_centered = y - y_mean\n    denom = np.sqrt(np.sum(x_centered**2) * np.sum(y_centered**2))\n    if denom == 0:\n        return 0.0\n    return float(np.sum(x_centered * y_centered) / denom)\n\ndef solve():\n    results = []\n\n    # Case 1\n    X1 = np.array([\n        [8.0, 2.0, 1.0],\n        [4.0, 1.5, 2.0],\n        [3.0, 3.0, 6.0],\n        [5.0, 1.0, 0.5],\n        [7.0, 2.5, 1.0]\n    ], dtype=float)\n    p1_true = np.array([0.5, 0.3, 0.2], dtype=float)\n    y1 = np.array([4.8, 2.85, 3.6, 2.9, 4.45], dtype=float)\n    p1_hat = nnls_deconvolution(X1, y1)\n    results.append(rmse(p1_hat, p1_true))\n\n    # Case 2\n    X2 = np.array([\n        [6.0, 6.2, 1.0],\n        [3.0, 3.1, 2.0],\n        [5.0, 5.1, 6.0],\n        [4.0, 4.05, 0.5],\n        [2.0, 2.02, 1.0]\n    ], dtype=float)\n    p2_true = np.array([0.4, 0.4, 0.2], dtype=float)\n    y2 = np.array([5.08, 2.84, 5.24, 3.32, 1.808], dtype=float)\n    p2_hat = nnls_deconvolution(X2, y2)\n    results.append(rmse(p2_hat, p2_true))\n\n    # Case 3 (unmodeled cell type)\n    X3 = X1.copy()\n    p3_known = np.array([0.5, 0.25, 0.15], dtype=float)  # sums to 0.9\n    u = np.array([4.0, 1.0, 3.0, 2.0, 5.0], dtype=float)\n    y3 = X3 @ p3_known + 0.1 * u\n    p3_true_norm = p3_known / 0.9\n    p3_hat = nnls_deconvolution(X3, y3)\n    results.append(rmse(p3_hat, p3_true_norm))\n\n    # Case 4 (measurement noise)\n    X4 = X1.copy()\n    p4_true = np.array([0.3, 0.5, 0.2], dtype=float)\n    y4_noiseless = X4 @ p4_true\n    eta = np.array([0.05, -0.02, 0.03, -0.04, 0.01], dtype=float)\n    y4 = y4_noiseless + eta\n    p4_hat = nnls_deconvolution(X4, y4)\n    results.append(rmse(p4_hat, p4_true))\n\n    # Case 5 (scaling mismatch)\n    X5 = X1.copy()\n    p5_true = p1_true.copy()\n    y5 = 2.0 * y1\n    p5_hat = nnls_deconvolution(X5, y5)\n    results.append(rmse(p5_hat, p5_true))\n\n    # Case 6 (clinical phenotype correlation)\n    X6 = X1.copy()\n    p6_list = [\n        np.array([0.2, 0.5, 0.3], dtype=float),\n        np.array([0.1, 0.5, 0.4], dtype=float),\n        np.array([0.05, 0.45, 0.5], dtype=float),\n        np.array([0.3, 0.6, 0.1], dtype=float),\n        np.array([0.05, 0.35, 0.6], dtype=float),\n        np.array([0.15, 0.55, 0.3], dtype=float),\n    ]\n    delta = np.array([0.01, -0.01, 0.0, 0.02, -0.02], dtype=float)\n    s = np.array([0.3, 0.5, 0.7, 0.2, 0.9, 0.4], dtype=float)\n    est_cell3 = []\n    for p_true in p6_list:\n        y6 = X6 @ p_true + delta\n        p_hat6 = nnls_deconvolution(X6, y6)\n        est_cell3.append(p_hat6[2])\n    r = pearson_correlation(np.array(est_cell3, dtype=float), s)\n    results.append(r)\n\n    # Round each result to 6 decimal places and print in specified format\n    rounded = [f\"{x:.6f}\" for x in results]\n    print(f\"[{','.join(rounded)}]\")\n\nsolve()\n```", "id": "4574619"}, {"introduction": "将组学数据与临床表型整合时，一个核心挑战是在特征数量远大于样本数量（$p \\gg n$）的高维场景下构建稳健的预测模型。本实践练习 [@problem_id:4574633] 旨在深入探讨这一问题，要求你通过编程模拟比较两种经典的多变量分析方法：偏最小二乘法（PLS）和典范相关分析（CCA）。通过从第一性原理出发进行实现，你将揭示这两种方法不同的优化目标，并理解在典型的充满噪声的高维临床组学研究中，为何一种方法可能优于另一种。", "problem": "您需要编写一个完整、可运行的程序，模拟一项组学到表型的整合任务，并比较偏最小二乘法（PLS）和典型相关分析（CCA）在弱互相关和高噪声条件下，利用高维组学测量数据预测临床表型的性能。该程序必须使用线性代数和经过充分检验的统计学定义，从第一性原理出发实现这两种方法。该问题围绕一个线性高斯生成模型构建，该模型被广泛用于模拟组学特征与临床结局之间的关系。目标是展示并量化哪种方法能产生更好的表型预测，并从基本原理上解释其原因。\n\n基本原理。使用以下基本定义和事实：\n- 分子生物学的中心法则指出，信息从脱氧核糖核酸（DNA）流向核糖核酸（RNA），再到蛋白质；在实践中，高维组学测量被视为与可能与临床表型相关的潜在生物过程有关的定量特征。在本问题中，您将使用一个线性高斯潜因子模型来捕捉组学特征和表型之间的共享变异性。\n- 两个中心化随机向量的样本协方差定义为 $$\\mathrm{Cov}(U,V)=\\frac{1}{n}\\sum_{i=1}^{n} U_i V_i,$$ 样本相关性是协方差通过标准差乘积进行归一化。从特征对标量响应进行线性预测时，使用一个系数向量通过线性组合来形成预测。\n- 偏最小二乘法（PLS）寻求特征的线性组合，以最大化与响应的样本协方差；而典型相关分析（CCA）则寻求在归一化约束下最大化样本相关性的线性组合。这两种方法都可以通过线性代数优化原理实现，而无需依赖特定领域的捷径。\n\n数据生成模型。对于每个测试用例，根据一个共享的潜因子模型模拟训练和测试数据：\n- 设 $p$ 为组学特征的数量。构建一个载荷向量 $a \\in \\mathbb{R}^{p}$，其中有且仅有 $k_{\\mathrm{signal}}$ 个非零项，这些项的位置是均匀随机选择的，然后归一化为单位欧几里得范数。这代表一个潜在的组学模块。\n- 对于每个样本 $i$，抽取一个潜标量 $g_i \\sim \\mathcal{N}(0,1)$，抽取特征噪声 $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma_x^2 I_p)$，并构建组学特征为 $$X_i = g_i a + \\epsilon_i.$$\n- 抽取表型噪声 $\\eta_i \\sim \\mathcal{N}(0,\\sigma_y^2)$，并构建临床表型为 $$y_i = c \\cdot g_i + \\eta_i,$$ 其中 $c$ 控制组学和表型之间真实共享信号的强度。所有抽取在样本和变量之间都是独立的。\n- 在每个测试用例中，对训练集和测试集使用相同的 $a$，以表示一致的底层生物学机制。\n\n预处理。对于每个测试用例，将训练集中的每个特征进行中心化和缩放，使其均值为零，方差为单位一；同时将训练表型进行中心化和缩放，使其均值为零，方差为单位一。仅使用训练集的统计数据，对测试集应用相同的中心化和缩放处理。\n\n需实现的方法。\n- 针对单一响应的单组分PLS：通过最大化由特征形成的潜得分 $t$ 与中心化表型之间的样本协方差，推导出第一个预测方向和相应的回归系数。使用单个潜组分形成一个系数向量 $b_{\\mathrm{pls}} \\in \\mathbb{R}^{p}$，并生成测试预测 $\\hat{y} = X_{\\mathrm{test}} b_{\\mathrm{pls}}$，然后将其映射回原始（未标准化）的表型尺度。\n- 基于单组分正则化CCA的预测器：通过在特征侧方差的归一化约束下，最大化 $u^\\top X$ 与中心化表型之间的样本相关性，为特征推导出典则方向 $u \\in \\mathbb{R}^{p}$，并通过在特征协方差矩阵上添加岭参数 $\\lambda>0$ 来稳定解。然后，对中心化表型在典则得分 $z = X u$ 上进行最小二乘回归，以获得标量系数 $\\alpha$，从而得出测试预测 $\\hat{y} = \\alpha (X_{\\mathrm{test}} u)$，并将其映射回原始表型尺度。\n\n评估。对于每个测试用例，计算测试集上的均方根误差（RMSE），\n$$\\mathrm{RMSE}=\\sqrt{\\frac{1}{n_{\\mathrm{test}}} \\sum_{i=1}^{n_{\\mathrm{test}}} \\left(y_i - \\hat{y}_i\\right)^2},$$\n并对PLS和CCA两种方法都进行计算。定义一个指示符来说明哪种方法表现更好：如果PLS的RMSE严格小于CCA的RMSE且差值超过一个很小的容差，则输出 $1$；如果CCA的RMSE严格小于PLS的RMSE且差值超过一个很小的容差，则输出 $-1$；否则输出 $0$。使用 $10^{-6}$ 的容差。\n\n确定性随机性。使用一个固定的基础随机种子 $s_0=12345$，并且对于第 $j$ 个测试用例（从 $j=0$ 开始），使用种子 $s_j = s_0 + j$ 来确保可复现性和不同用例间的变化。\n\n测试套件。您的程序必须运行以下四个测试用例，它们共同探讨了一个理想路径、一个极高噪声的边界条件、一个 $p \\gg n$ 的高维边缘情况，以及一个中等强度的共享信号：\n- 用例 A（理想路径，弱相关，高噪声）：$n_{\\mathrm{train}}=120$, $n_{\\mathrm{test}}=280$, $p=200$, $k_{\\mathrm{signal}}=15$, $c=0.15$, $\\sigma_x=1.5$, $\\sigma_y=1.5$, $\\lambda=0.5$。\n- 用例 B（边界情况，极高噪声）：$n_{\\mathrm{train}}=120$, $n_{\\mathrm{test}}=280$, $p=200$, $k_{\\mathrm{signal}}=15$, $c=0.10$, $\\sigma_x=3.0$, $\\sigma_y=3.0$, $\\lambda=1.0$。\n- 用例 C（边缘情况，$p \\gg n$）：$n_{\\mathrm{train}}=80$, $n_{\\mathrm{test}}=320$, $p=500$, $k_{\\mathrm{signal}}=25$, $c=0.12$, $\\sigma_x=2.0$, $\\sigma_y=2.0$, $\\lambda=2.0$。\n- 用例 D（中等强度的共享信号，但仍有噪声）：$n_{\\mathrm{train}}=150$, $n_{\\mathrm{test}}=350$, $p=200$, $k_{\\mathrm{signal}}=20$, $c=0.30$, $\\sigma_x=1.8$, $\\sigma_y=1.8$, $\\lambda=0.8$。\n\n最终输出格式。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。按A、B、C、D的顺序，将每个测试用例的三元组 $[\\mathrm{RMSE}_{\\mathrm{PLS}}, \\mathrm{RMSE}_{\\mathrm{CCA}}, \\mathrm{indicator}]$ 连接成一个扁平化的列表。例如，输出必须采用以下格式：\n$$[\\mathrm{RMSE}_A^{\\mathrm{PLS}},\\mathrm{RMSE}_A^{\\mathrm{CCA}},I_A,\\mathrm{RMSE}_B^{\\mathrm{PLS}},\\mathrm{RMSE}_B^{\\mathrm{CCA}},I_B,\\mathrm{RMSE}_C^{\\mathrm{PLS}},\\mathrm{RMSE}_C^{\\mathrm{CCA}},I_C,\\mathrm{RMSE}_D^{\\mathrm{PLS}},\\mathrm{RMSE}_D^{\\mathrm{CCA}},I_D].$$\n所有值都必须是数值。不应打印任何其他文本。", "solution": "该问题要求对偏最小二乘法（PLS）和正则化典型相关分析（CCA）在高维组学数据预测临床表型方面的性能进行比较分析。该分析将在一个由线性高斯潜因子模型定义的模拟环境中进行，这是生物信息学中对此类关系进行建模的标准方法。解决方案涉及从第一性原理推导和实现这两种方法，并在弱信号和高噪声条件下评估它们的预测性能。\n\n首先，我们建立数学框架。设 $X_{\\text{train}} \\in \\mathbb{R}^{n_{\\text{train}} \\times p}$ 和 $y_{\\text{train}} \\in \\mathbb{R}^{n_{\\text{train}}}$ 分别表示包含 $n_{\\text{train}}$ 个样本的训练集，其中有 $p$ 个组学特征和一个表型。类似地，设 $X_{\\text{test}} \\in \\mathbb{R}^{n_{\\text{test}} \\times p}$ 和 $y_{\\text{test}} \\in \\mathbb{R}^{n_{\\text{test}}}$ 为相应的测试数据。\n\n### 数据预处理\n在应用这些方法之前，必须对数据进行预处理。我们从训练数据 $X_{\\text{train}}$ 中为每个特征 $j=1, \\dots, p$ 计算均值 $\\mu_{X_j}$ 和标准差 $\\sigma_{X_j}$。标准差计算时使用分母 $n_{\\text{train}}$，与问题中样本协方差的定义保持一致。类似地，我们为训练表型 $y_{\\text{train}}$ 计算均值 $\\mu_y$ 和标准差 $\\sigma_y$。\n然后对训练数据进行标准化：\n$$ X_{s, \\text{train}} = (X_{\\text{train}} - \\mu_X) / \\sigma_X $$\n$$ y_{s, \\text{train}} = (y_{\\text{train}} - \\mu_y) / \\sigma_y $$\n其中减法和除法运算是逐元素执行的。测试数据使用训练数据的统计量进行标准化：\n$$ X_{s, \\text{test}} = (X_{\\text{test}} - \\mu_X) / \\sigma_X $$\n这确保了测试集的信息不会泄漏到训练过程中。预测将在标准化尺度上进行，然后转换回原始表型尺度进行评估。\n\n### 方法1：偏最小二乘（PLS）回归\n单组分PLS的目标是找到特征的一个线性组合，称为潜得分 $t = X_{s, \\text{train}} w$，该组合与表型 $y_{s, \\text{train}}$ 的协方差最大。权重向量 $w \\in \\mathbb{R}^p$ 定义了投影方向。优化问题是：\n$$ \\max_{w} \\mathrm{Cov}(X_{s, \\text{train}} w, y_{s, \\text{train}}) \\quad \\text{满足} \\quad \\|w\\|_2 = 1 $$\n对于长度为 $n_{\\text{train}}$ 的中心化数据，样本协方差由下式给出：\n$$ \\mathrm{Cov}(t, y_{s, \\text{train}}) = \\frac{1}{n_{\\text{train}}} t^\\top y_{s, \\text{train}} = \\frac{1}{n_{\\text{train}}} (X_{s, \\text{train}} w)^\\top y_{s, \\text{train}} = \\frac{1}{n_{\\text{train}}} w^\\top X_{s, \\text{train}}^\\top y_{s, \\text{train}} $$\n在单位范数约束下最大化此式是一个经典的优化问题，其解是 $w$ 必须与梯度对齐，即 $w \\propto X_{s, \\text{train}}^\\top y_{s, \\text{train}}$。因此，我们可以将PLS权重向量（未归一化）定义为：\n$$ w_{\\text{pls}} = X_{s, \\text{train}}^\\top y_{s, \\text{train}} $$\n向量 $X_{s, \\text{train}}^\\top y_{s, \\text{train}}$ 包含了每个特征与响应之间的协方差。因此，PLS会提高那些与表型有强边际协方差的特征的权重。\n\n训练数据的潜得分计算为 $t_{\\text{pls}} = X_{s, \\text{train}} w_{\\text{pls}}$。为了构建预测模型，我们对标准化表型 $y_{s, \\text{train}}$ 在这些得分上进行简单线性回归：\n$$ y_{s, \\text{train}} = \\beta t_{\\text{pls}} + \\text{误差} $$\n标量系数 $\\beta$ 的普通最小二乘估计为：\n$$ \\hat{\\beta} = (t_{\\text{pls}}^\\top t_{\\text{pls}})^{-1} t_{\\text{pls}}^\\top y_{s, \\text{train}} $$\n在标准化特征空间中的最终回归系数向量是 $b_{s, \\text{pls}} = w_{\\text{pls}} \\hat{\\beta}$。该向量结合了协方差最大化方向和回归缩放因子。\n\n### 方法2：基于正则化典型相关分析（CCA）的回归\nCCA寻求一个特征投影 $z = X_{s, \\text{train}} u$，该投影与表型 $y_{s, \\text{train}}$ 的*相关性*最大化。优化问题是：\n$$ \\max_{u} \\mathrm{Corr}(X_{s, \\text{train}} u, y_{s, \\text{train}}) = \\max_{u} \\frac{\\mathrm{Cov}(X_{s, \\text{train}} u, y_{s, \\text{train}})}{\\sqrt{\\mathrm{Var}(X_{s, \\text{train}} u) \\mathrm{Var}(y_{s, \\text{train}})}} $$\n由于 $y_{s, \\text{train}}$ 是标准化的，其方差为 $1$。表达式简化为：\n$$ \\max_{u} \\frac{u^\\top S_{xy}}{\\sqrt{u^\\top S_{xx} u}} $$\n其中 $S_{xy} = \\frac{1}{n_{\\text{train}}} X_{s, \\text{train}}^\\top y_{s, \\text{train}}$ 是特征-表型协方差向量，$S_{xx} = \\frac{1}{n_{\\text{train}}} X_{s, \\text{train}}^\\top X_{s, \\text{train}}$ 是特征协方差矩阵。在高维设置（$p > n_{\\text{train}}$）中，$S_{xx}$ 是奇异的。为了稳定问题，按规定向特征协方差矩阵添加一个岭正则化项 $\\lambda > 0$。问题变为最大化：\n$$ \\frac{u^\\top S_{xy}}{\\sqrt{u^\\top (S_{xx} + \\lambda I) u}} $$\n该最大化问题的解由 $u \\propto (S_{xx} + \\lambda I)^{-1} S_{xy}$ 给出。代入数据矩阵，我们得到：\n$$ u \\propto \\left(\\frac{1}{n_{\\text{train}}} X_{s, \\text{train}}^\\top X_{s, \\text{train}} + \\lambda I\\right)^{-1} \\left(\\frac{1}{n_{\\text{train}}} X_{s, \\text{train}}^\\top y_{s, \\text{train}}\\right) $$\n因子 $1/n_{\\text{train}}$ 可以消去，从而得到典则投影的权重向量：\n$$ u_{\\text{cca}} \\propto (X_{s, \\text{train}}^\\top X_{s, \\text{train}} + n_{\\text{train}}\\lambda I)^{-1} X_{s, \\text{train}}^\\top y_{s, \\text{train}} $$\n这正是岭回归的解。问题指定了一个两步法：首先找到这个方向 $u_{\\text{cca}}$，然后基于得到的得分建立一个回归模型。我们计算典则得分 $z_{\\text{cca}} = X_{s, \\text{train}} u_{\\text{cca}}$。接下来，我们对 $y_{s, \\text{train}}$ 在 $z_{\\text{cca}}$ 上进行简单线性回归，以找到一个标量系数 $\\alpha$：\n$$ \\hat{\\alpha} = (z_{\\text{cca}}^\\top z_{\\text{cca}})^{-1} z_{\\text{cca}}^\\top y_{s, \\text{train}} $$\n用于预测的最终系数向量为 $b_{s, \\text{cca}} = u_{\\text{cca}} \\hat{\\alpha}$。\n\n### 概念比较和预测\n根本区别在于如何处理特征相关性。PLS 中 $w_{\\text{pls}} \\propto X_{s, \\text{train}}^\\top y_{s, \\text{train}}$，忽略了特征间的相关性，并基于直接的特征-表型协方差贪婪地选择一个方向。CCA 中 $u_{\\text{cca}} \\propto (X_{s, \\text{train}}^\\top X_{s, \\text{train}} + \\dots)^{-1} X_{s, \\text{train}}^\\top y_{s, \\text{train}}$，明确地包含了特征协方差结构 $X_{s, \\text{train}}^\\top X_{s, \\text{train}}$。矩阵求逆有效地“白化”了特征空间，降低了冗余信息的权重。\n然而，在高噪声设置中，样本协方差矩阵 $X_{s, \\text{train}}^\\top X_{s, \\text{train}}$ 是真实协方差的一个非常嘈杂的估计。其求逆过程可能会放大噪声，可能导致泛化能力差。PLS通过避免这种求逆，可能更具鲁棒性，即使它在理论上是一个不那么复杂的模型。这是高维统计学中常见的权衡。\n\n### 预测和评估\n对于每种方法 $m \\in \\{\\text{pls}, \\text{cca}\\}$，我们有一个系数向量 $b_{s,m}$。测试集的预测通过以下步骤生成：\n$1$。标准化测试特征：$X_{s, \\text{test}} = (X_{\\text{test}} - \\mu_X) / \\sigma_X$。\n$2$。在标准化尺度上进行预测：$\\hat{y}_{s, \\text{test}} = X_{s, \\text{test}} b_{s,m}$。\n$3$。对预测进行去标准化：$\\hat{y}_{\\text{test}} = \\hat{y}_{s, \\text{test}} \\sigma_y + \\mu_y$。\n\n性能使用测试集上的均方根误差（RMSE）进行评估：\n$$ \\mathrm{RMSE} = \\sqrt{\\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} (y_{i, \\text{test}} - \\hat{y}_{i, \\text{test}})^2} $$\n计算一个指示变量，以确定哪种方法在 $10^{-6}$ 的容差基础上表现更好。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates omics-to-phenotype integration, comparing PLS and regularized CCA.\n    \"\"\"\n\n    def _solve_single_case(params):\n        \"\"\"\n        Executes a single test case for data generation, analysis, and evaluation.\n        \"\"\"\n        n_train, n_test, p, k_signal, c, sigma_x, sigma_y, lambda_val, seed = params\n        \n        rng = np.random.default_rng(seed)\n        \n        # Data Generation\n        # 1. Construct loading vector a\n        a = np.zeros(p)\n        nonzero_indices = rng.choice(p, k_signal, replace=False)\n        a[nonzero_indices] = rng.standard_normal(k_signal)\n        a /= np.linalg.norm(a)\n        \n        # 2. Generate training data\n        g_train = rng.standard_normal((n_train, 1))\n        epsilon_train = rng.standard_normal((n_train, p)) * sigma_x\n        X_train = g_train @ a.reshape(1, p) + epsilon_train\n        eta_train = rng.standard_normal(n_train) * sigma_y\n        y_train = c * g_train.flatten() + eta_train\n        \n        # 3. Generate testing data\n        g_test = rng.standard_normal((n_test, 1))\n        epsilon_test = rng.standard_normal((n_test, p)) * sigma_x\n        X_test = g_test @ a.reshape(1, p) + epsilon_test\n        eta_test = rng.standard_normal(n_test) * sigma_y\n        y_test = c * g_test.flatten() + eta_test\n\n        # Preprocessing using training set statistics\n        # Use ddof=0 for variance/std to align with the 1/n definition in problem statement.\n        mu_X = np.mean(X_train, axis=0)\n        std_X = np.std(X_train, axis=0, ddof=0)\n        std_X[std_X  1e-8] = 1.0  # Prevent division by zero for constant features\n\n        mu_y = np.mean(y_train)\n        std_y = np.std(y_train, ddof=0)\n        if std_y  1e-8:\n            std_y = 1.0\n            \n        X_train_s = (X_train - mu_X) / std_X\n        y_train_s = (y_train - mu_y) / std_y\n        X_test_s = (X_test - mu_X) / std_X\n        \n        # --- PLS Implementation ---\n        # Weight vector is proportional to covariance between features and response\n        w_pls = X_train_s.T @ y_train_s\n        \n        # PLS scores (projection of data onto the weight vector)\n        t_pls = X_train_s @ w_pls\n        \n        # Regress response onto scores to find the scalar coefficient beta\n        t_dot_t = t_pls.T @ t_pls\n        beta = (t_pls.T @ y_train_s) / t_dot_t if t_dot_t > 1e-12 else 0.0\n            \n        # Full PLS coefficient vector for standardized data\n        b_pls_s = w_pls * beta\n        \n        # Predict on test set and un-standardize\n        y_hat_pls_s = X_test_s @ b_pls_s\n        y_hat_pls = y_hat_pls_s * std_y + mu_y\n        \n        rmse_pls = np.sqrt(np.mean((y_test - y_hat_pls)**2))\n        \n        # --- Regularized CCA-based Regression Implementation ---\n        # Covariance matrices from standardized data\n        cov_xx = X_train_s.T @ X_train_s\n        cov_xy = X_train_s.T @ y_train_s\n        \n        # Regularization term is n_train * lambda * I\n        reg_term = n_train * lambda_val * np.eye(p)\n        \n        # Solve (cov_xx + reg_term) u = cov_xy for the canonical direction u\n        u_cca_raw = np.linalg.solve(cov_xx + reg_term, cov_xy)\n        \n        # Canonical scores\n        z_cca = X_train_s @ u_cca_raw\n        \n        # Regress response onto scores to get the scalar coefficient alpha\n        z_dot_z = z_cca.T @ z_cca\n        alpha = (z_cca.T @ y_train_s) / z_dot_z if z_dot_z > 1e-12 else 0.0\n            \n        # Full CCA coefficient vector for standardized data\n        b_cca_s = u_cca_raw * alpha\n        \n        # Predict on test set and un-standardize\n        y_hat_cca_s = X_test_s @ b_cca_s\n        y_hat_cca = y_hat_cca_s * std_y + mu_y\n        \n        rmse_cca = np.sqrt(np.mean((y_test - y_hat_cca)**2))\n        \n        # Evaluation\n        tol = 1e-6\n        if rmse_pls  rmse_cca - tol:\n            indicator = 1\n        elif rmse_cca  rmse_pls - tol:\n            indicator = -1\n        else:\n            indicator = 0\n            \n        return rmse_pls, rmse_cca, indicator\n\n    # --- Test Suite ---\n    s0 = 12345\n    test_cases = [\n        # Case A: Happy path\n        (120, 280, 200, 15, 0.15, 1.5, 1.5, 0.5, s0 + 0),\n        # Case B: Extremely high noise\n        (120, 280, 200, 15, 0.10, 3.0, 3.0, 1.0, s0 + 1),\n        # Case C: Edge case, p >> n\n        (80, 320, 500, 25, 0.12, 2.0, 2.0, 2.0, s0 + 2),\n        # Case D: Moderately stronger signal\n        (150, 350, 200, 20, 0.30, 1.8, 1.8, 0.8, s0 + 3),\n    ]\n\n    all_results = []\n    for params in test_cases:\n        rmse_pls, rmse_cca, indicator = _solve_single_case(params)\n        all_results.extend([rmse_pls, rmse_cca, indicator])\n\n    # Final output formatting\n    # Example format: [RMSE_A_PLS,RMSE_A_CCA,I_A,RMSE_B_PLS,RMSE_B_CCA,I_B,...]\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "4574633"}, {"introduction": "有效的数据整合必须尊重不同组学数据类型独特的统计属性。例如，微生物组数据是典型的成分数据（compositional data），其特征值为相对丰度，总和为一，被约束在单纯形（simplex）空间中。本实践练习 [@problem_id:4574651] 旨在探索对数比变换（log-ratio transformation）的理论基础与实际应用，该方法是将成分数据正确映射到标准欧几里得空间以进行有效统计分析的关键。掌握这一技术对于正确建模微生物组与临床结局之间的关系至关重要。", "problem": "一項臨床基因組學研究旨在將腸道微生物組圖譜與一種二元疾病表型相結合。微生物組特徵以相對豐度向量 $x \\in \\Delta^{p-1}$ 的形式被觀察，其中 $\\Delta^{p-1} = \\{x \\in \\mathbb{R}^{p}: x_{i} \\ge 0, \\sum_{i=1}^{p} x_{i} = 1\\}$，並被用作疾病風險廣義線性模型中的協變量。由於 $x$ 是組成數據，建模必須遵循只有各組分之間的比例才有意義的原則。從這一原則以及組成數據在縮放與閉合下的不變性出發，推導一種對數比轉換的形式，該轉換 (i) 將 $\\Delta^{p-1}$ 映射到 $\\mathbb{R}^{p}$ 的一個具有零和約束的子空間，並且 (ii) 產生一個適用於歐幾里德方法的正交標準坐標表示。在您的推導中，清晰解釋中心對數比轉換 (CLR) 和等距對數比轉換 (ILR) 的概念作用和定義，不要使用未經縮放的原始比例，並論證為什麼基於比例的對數轉換在臨床建模中科學地適用於組成微生物組數據。\n\n然後，對以下具有一個零值的具體微生物組組成，使用加性偽計數替換後再重新歸一化的方法，計算其中心對數比轉換。设 $p=5$ 且\n$$\nx = \\begin{pmatrix}\n0.40 \\\\\n0.30 \\\\\n0.20 \\\\\n0.10 \\\\\n0.00\n\\end{pmatrix}.\n$$\n通過向每個組分添加相同的偽計數 $\\epsilon = 10^{-4}$ 來替換零值，然後將該向量重新歸一化至單純形 $\\Delta^{4}$。使用得到的組成來計算中心對數比轉換。將最終轉換後向量的每個組分四捨五入至四位有效數字。答案無需物理單位。", "solution": "該問題提出了一個在組成數據（如微生物組相對豐度）分析中至關重要且定義明確的問題。它要求對對數比轉換進行理論推導和具體數值計算。\n\n### 第一部分：理論推導與論證\n\n建模組成數據（如微生物組相對豐度向量 $x \\in \\Delta^{p-1}$）的根本挑戰在於，它們被约束在一個單純形上，而不是一個標準的歐幾里德空間 $\\mathbb{R}^p$。樣本空間定義為 $\\Delta^{p-1} = \\{x \\in \\mathbb{R}^{p}: x_{i}  0, \\sum_{i=1}^{p} x_{i} = 1\\}$。請注意，為了使對數比方法有定義，這裡有嚴格的不等式 $x_i  0$；零值的情況需要特殊處理，正如問題的數值部分所解決的那樣。標準的統計方法（例如，線性回歸、主成分分析）依賴於向量空間的代數結構，該結構假設向量加法和純量乘法是定義明確且結果仍在同一空間內的運算。然而，對於一個組成 $x$，無論是 $x+y$（其中 $y$ 是另一個組成）還是 $c \\cdot x$（對於純量 $c \\ne 1$），通常都不會落在單純形上，因為其組分之和將不再是 $1$。\n\n分析此類數據的科學原則是，信息不包含在組分的絕對值中，而是包含在它們的比例中。來自高通量測序的微生物組數據本質上是相對的；讀取的總數是測序深度的產物，並不反映絕對的微生物負荷。因此，一個原始計數向量 $(c_1, \\dots, c_p)$ 在信息上被認為等同於 $(k \\cdot c_1, \\dots, k \\cdot c_p)$，其中 $k  0$ 是任意正縮放因子。將這些計數標準化為相對豐度 $x_i = c_i / \\sum c_j$ 的過程稱為**閉合 (closure)**。任何有效的統計分析都必須對原始縮放不變；這就是**尺度不變性原理 (principle of scale invariance)**。\n\n對數比轉換在科學上是合適的，因為它們將比例的乘法關係轉換為加性的線性關係。比例的對數 $\\ln(x_i/x_j) = \\ln(x_i) - \\ln(x_j)$ 天然地適合這種結構，並且對縮放不變：$\\ln((k x_i)/(k x_j)) = \\ln(x_i/x_j)$。這些轉換將具有特定 Aitchison 幾何的單純形映射到一個實向量空間，在該空間中，標準的歐幾里德幾何和多變量方法是適用的。\n\n**i) 中心對數比 (CLR) 轉換**\n\n中心對數比 (CLR) 轉換將一個組成 $x$ 從單純形 $\\Delta^{p-1}$ 映射到一個 $p$ 維實向量。它通過取每個組分的對數，然後減去對數組分的均值來中心化所得向量。該均值等同於原始組成的幾何平均值的對數。\n\n其定義為：\n$$\n\\text{clr}(x) = \\begin{pmatrix} \\ln\\left(\\frac{x_1}{g(x)}\\right) \\\\ \\ln\\left(\\frac{x_2}{g(x)}\\right) \\\\ \\vdots \\\\ \\ln\\left(\\frac{x_p}{g(x)}\\right) \\end{pmatrix}\n= \\begin{pmatrix} \\ln(x_1) - \\ln(g(x)) \\\\ \\ln(x_2) - \\ln(g(x)) \\\\ \\vdots \\\\ \\ln(x_p) - \\ln(g(x)) \\end{pmatrix}\n$$\n其中 $g(x) = \\left(\\prod_{i=1}^{p} x_i\\right)^{1/p}$ 是 $x$ 各組分的幾何平均值。\n\nCLR 轉換的關鍵特性是所得向量的組分之和為零。這表明該轉換將 $\\Delta^{p-1}$ 映射到 $\\mathbb{R}^p$ 的一個特定子空間。\n為證明此零和約束：\n$$\n\\sum_{i=1}^{p} \\text{clr}(x)_i = \\sum_{i=1}^{p} (\\ln(x_i) - \\ln(g(x))) = \\left(\\sum_{i=1}^{p} \\ln(x_i)\\right) - p \\cdot \\ln(g(x))\n$$\n根據幾何平均值的定義，$\\ln(g(x)) = \\frac{1}{p} \\sum_{i=1}^{p} \\ln(x_i)$。將此代入方程：\n$$\n\\sum_{i=1}^{p} \\text{clr}(x)_i = \\left(\\sum_{i=1}^{p} \\ln(x_i)\\right) - p \\cdot \\left(\\frac{1}{p} \\sum_{i=1}^{p} \\ln(x_i)\\right) = \\left(\\sum_{i=1}^{p} \\ln(x_i)\\right) - \\left(\\sum_{i=1}^{p} \\ln(x_i)\\right) = 0\n$$\n因此，CLR 轉換將 $(p-1)$ 維的單純形映射到 $\\mathbb{R}^p$ 中由約束 $\\sum z_i = 0$ 定義的 $(p-1)$ 維線性子空間。這滿足了要求 (i)。然而，CLR 向量的組分是完全共線的，這意味著 CLR 轉換後數據的協方差矩陣是奇異的。對於需要矩陣求逆的統計方法來說，這可能是一個問題。\n\n**ii) 等距對數比 (ILR) 轉換**\n\n等距對數比 (ILR) 轉換通過創建一組 $p-1$ 個正交標準坐標來解決 CLR 的奇異性問題。它將單純形 $\\Delta^{p-1}$ 映射到歐幾里德空間 $\\mathbb{R}^{p-1}$。「等距」(isometric) 一詞表示單純形上兩個組成之間的 Aitchison 距離被保留為它們對應的 ILR 轉換後向量之間的標準歐幾里德距離。\n\nILR 轉換是通過將 CLR 轉換後的向量投影到零和子空間的一個正交標準基上來定義的。設 $\\Psi$ 為一個 $p \\times (p-1)$ 矩陣，其列 $\\psi_1, \\dots, \\psi_{p-1}$ 構成 $\\mathbb{R}^p$ 中和為零的向量空間的一個正交標準基。ILR 坐標由下式給出：\n$$\n\\text{ilr}(x) = \\text{clr}(x)^T \\Psi\n$$\n結果是一個包含 $p-1$ 個實值坐標的向量。一種構建基矩陣 $\\Psi$ 的常用方法是通過序列二元劃分 (SBP) 系統，這使得所得坐標具有可解釋性，即為原始組分組之間的對數比。$\\Psi$ 元素的一個通用構造是：\n$$\n\\psi_{ij} = \\begin{cases} \\sqrt{\\frac{j}{j+1}} \\frac{1}{j}  \\text{if } i \\le j \\\\ -\\sqrt{\\frac{j}{j+1}}  \\text{if } i = j+1 \\\\ 0  \\text{if } i  j+1 \\end{cases}\n$$\n這需要對原始組分進行重新排序。其關鍵特徵是，對於任何這樣有效的基 $\\Psi$，其列是相互正交的（對於 $k \\ne l$，$\\psi_k^T \\psi_l = 0$）且具有單位長度（$\\psi_k^T \\psi_k = 1$）。這確保了所得的 ILR 坐標是正交標準的，並且位於 $\\mathbb{R}^{p-1}$ 中，這是一個適用於任何傳統多變量分析的標準歐幾里德空間。這滿足了要求 (ii)。\n\n總而言之，CLR 轉換是一個概念上重要的中間步驟，它揭示了相對於組成中心的對數比結構。ILR 轉換是最後一步，它產生了一個數學上合理的坐標系，以便應用來自歐幾里德幾何的方法。\n\n### 第二部分：數值計算\n\n給定 $p=5$ 的組成向量 $x \\in \\mathbb{R}^5$：\n$$\nx = \\begin{pmatrix} 0.40 \\\\ 0.30 \\\\ 0.20 \\\\ 0.10 \\\\ 0.00 \\end{pmatrix}^T\n$$\n該向量包含一個零，因此對數無定義。我們必須首先應用指定的零值替換程序。\n\n**步驟 1：加性偽計數與重新歸一化**\n\n我們將偽計數 $\\epsilon = 10^{-4} = 0.0001$ 加到 $x$ 的每個組分上：\n$$\nx_{\\text{pseudo}} = x + \\begin{pmatrix} \\epsilon \\\\ \\epsilon \\\\ \\epsilon \\\\ \\epsilon \\\\ \\epsilon \\end{pmatrix} = \\begin{pmatrix} 0.40 \\\\ 0.30 \\\\ 0.20 \\\\ 0.10 \\\\ 0.00 \\end{pmatrix} + \\begin{pmatrix} 0.0001 \\\\ 0.0001 \\\\ 0.0001 \\\\ 0.0001 \\\\ 0.0001 \\end{pmatrix} = \\begin{pmatrix} 0.4001 \\\\ 0.3001 \\\\ 0.2001 \\\\ 0.1001 \\\\ 0.0001 \\end{pmatrix}\n$$\n接下來，我們重新歸一化這個向量，使其組分之和為 $1$。其和為：\n$$\nS = \\sum_{i=1}^5 (x_{\\text{pseudo}})_i = 0.4001 + 0.3001 + 0.2001 + 0.1001 + 0.0001 = 1.0005\n$$\n新的組成 $x'$ 是通過將每個組分除以 $S$ 得到的：\n$$\nx' = \\frac{1}{S} x_{\\text{pseudo}} = \\frac{1}{1.0005} \\begin{pmatrix} 0.4001 \\\\ 0.3001 \\\\ 0.2001 \\\\ 0.1001 \\\\ 0.0001 \\end{pmatrix} = \\begin{pmatrix} 0.3999000... \\\\ 0.2999500... \\\\ 0.2000000... \\\\ 0.1000500... \\\\ 0.0000999... \\end{pmatrix}\n$$\n\n**步驟 2：計算中心對數比 (CLR) 轉換**\n\nCLR 轉換需要計算 $\\ln(g(x'))$，即 $x'$ 的幾何平均值的對數。\n$$\n\\ln(g(x')) = \\frac{1}{p} \\sum_{i=1}^p \\ln(x'_i) = \\frac{1}{5} \\sum_{i=1}^5 \\ln(x'_i)\n$$\n我們首先計算 $x'$ 每個組分的自然對數：\n$$\n\\ln(x'_1) = \\ln\\left(\\frac{0.4001}{1.0005}\\right) \\approx -0.91653303 \\\\\n\\ln(x'_2) = \\ln\\left(\\frac{0.3001}{1.0005}\\right) \\approx -1.20414230 \\\\\n\\ln(x'_3) = \\ln\\left(\\frac{0.2001}{1.0005}\\right) \\approx -1.60943791 \\\\\n\\ln(x'_4) = \\ln\\left(\\frac{0.1001}{1.0005}\\right) \\approx -2.30208852 \\\\\n\\ln(x'_5) = \\ln\\left(\\frac{0.0001}{1.0005}\\right) \\approx -9.21084050\n$$\n這些對數的和是：\n$$\n\\sum_{i=1}^5 \\ln(x'_i) \\approx -15.24304226\n$$\n現在，我們求幾何平均值的對數：\n$$\n\\ln(g(x')) = \\frac{1}{5}(-15.24304226) \\approx -3.04860845\n$$\n最後，我們計算 CLR 向量的每个組分，$z_i = \\text{clr}(x')_i = \\ln(x'_i) - \\ln(g(x'))$：\n$$\nz_1 \\approx -0.91653303 - (-3.04860845) \\approx 2.13207542 \\\\\nz_2 \\approx -1.20414230 - (-3.04860845) \\approx 1.84446615 \\\\\nz_3 \\approx -1.60943791 - (-3.04860845) \\approx 1.43917054 \\\\\nz_4 \\approx -2.30208852 - (-3.04860845) \\approx 0.74651993 \\\\\nz_5 \\approx -9.21084050 - (-3.04860845) \\approx -6.16223205\n$$\n將每個組分四捨五入至四位有效數字：\n$z_1 \\approx 2.132$\n$z_2 \\approx 1.844$\n$z_3 \\approx 1.439$\n$z_4 \\approx 0.7465$\n$z_5 \\approx -6.162$\n\n最終的 CLR 轉換向量是：\n$$\n\\text{clr}(x') \\approx \\begin{pmatrix} 2.132 \\\\ 1.844 \\\\ 1.439 \\\\ 0.7465 \\\\ -6.162 \\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} 2.132 \\\\ 1.844 \\\\ 1.439 \\\\ 0.7465 \\\\ -6.162 \\end{pmatrix}}\n$$", "id": "4574651"}]}