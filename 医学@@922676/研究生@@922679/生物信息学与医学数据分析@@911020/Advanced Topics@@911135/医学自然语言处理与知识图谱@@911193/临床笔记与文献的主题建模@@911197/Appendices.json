{"hands_on_practices": [{"introduction": "在将主题模型应用于临床文本之前，一个关键的初始步骤是选择合适的数据表示方法。尽管词频-逆文档频率（$TF-IDF$）在信息检索领域中是一种常见的特征加权方法，但像潜在狄利克雷分配（$LDA$）这样的概率生成模型对其输入有着特定的假设。本练习 [@problem_id:4613952] 将引导您计算一个临床笔记的 $TF-IDF$ 向量，并从根本上探讨为何不应将这些加权值直接用于 $LDA$，从而加深您对数据预处理与模型假设保持一致的重要性的理解。", "problem": "一个医院研究团队整合了临床笔记和同行评审文献摘要的语料库，以支持无监督地发现症状与疾病之间的关联。考虑一个固定的词汇表，包含五个医学上显著的术语，顺序为 $t_1=\\text{dyspnea}$、$t_2=\\text{tachycardia}$、$t_3=\\text{influenza}$、$t_4=\\text{fever}$ 和 $t_5=\\text{chest\\_pain}$。合并后的语料库包含 $N=50000$ 篇文档。各术语的文档频率分别为 $\\text{df}(t_1)=4200$、$\\text{df}(t_2)=3000$、$\\text{df}(t_3)=800$、$\\text{df}(t_4)=12000$ 和 $\\text{df}(t_5)=2500$。一篇临床笔记的词袋模型词频向量为 $\\big(\\text{tf}(t_1),\\text{tf}(t_2),\\text{tf}(t_3),\\text{tf}(t_4),\\text{tf}(t_5)\\big)=(3,1,2,4,1)$。\n\n假设术语频率-逆文档频率 (TF-IDF) 使用自然对数定义，采用次线性词频和未平滑的逆文档频率，并且 $\\text{tf}=0$ 的术语对 TF-IDF 向量的贡献为 $0$。计算该笔记的 TF-IDF 向量。然后，计算此 TF-IDF 向量的欧几里得范数的平方。将最终数值答案四舍五入到四位有效数字。\n\n最后，基于诸如潜在狄利克雷分布 (Latent Dirichlet Allocation, LDA) 等概率主题模型的生成假设，讨论对于此任务是否应将 TF-IDF 直接用作此类模型的输入。", "solution": "此推导的基础包括词袋表示法、采用次线性词频和未平滑逆文档频率的术语频率-逆文档频率 (TF-IDF) 的标准定义，以及潜在狄利克雷分布 (LDA) 的生成结构。\n\n在词袋模型下，一篇文档由词汇表中每个术语 $t_i$ 的词频计数 $\\text{tf}(t_i)$ 表示。次线性词频将每个原始计数转换为 $1+\\ln\\big(\\text{tf}(t_i)\\big)$（当 $\\text{tf}(t_i)\\geq 1$ 时），对于 $\\text{tf}(t_i)=0$ 则权重为零。逆文档频率由 $\\ln\\!\\big(N/\\text{df}(t_i)\\big)$ 给出。因此，对于每个术语 $t_i$，其 TF-IDF 权重为\n$$\nw_i \\;=\\; \\big(1+\\ln(\\text{tf}(t_i))\\big)\\,\\ln\\!\\Big(\\frac{N}{\\text{df}(t_i)}\\Big),\n$$\n按照惯例，文档中未出现的术语贡献值为 $0$。\n\n由于题目要求提供最终数值答案，我们先符号化地计算每个因子，然后再进行数值计算。\n\n给定 $N=50000$ 和文档频率：\n- 对于 $t_1=\\text{dyspnea}$：$\\text{df}(t_1)=4200$，$\\text{tf}(t_1)=3$。\n  - 词频因子：$1+\\ln(3)=1+1.098612289=2.098612289$。\n  - 逆文档频率：$\\ln\\!\\big(50000/4200\\big)=\\ln(11.9047619)=2.476938479$。\n  - TF-IDF：$w_1=2.098612289\\times 2.476938479=5.198129906$。\n- 对于 $t_2=\\text{tachycardia}$：$\\text{df}(t_2)=3000$，$\\text{tf}(t_2)=1$。\n  - 词频因子：$1+\\ln(1)=1+0=1$。\n  - 逆文档频率：$\\ln\\!\\big(50000/3000\\big)=\\ln(16.6666667)=2.813410716$。\n  - TF-IDF：$w_2=1\\times 2.813410716=2.813410716$。\n- 对于 $t_3=\\text{influenza}$：$\\text{df}(t_3)=800$，$\\text{tf}(t_3)=2$。\n  - 词频因子：$1+\\ln(2)=1+0.693147181=1.693147181$。\n  - 逆文档频率：$\\ln\\!\\big(50000/800\\big)=\\ln(62.5)=4.135166555$。\n  - TF-IDF：$w_3=1.693147181\\times 4.135166555=7.001444144$。\n- 对于 $t_4=\\text{fever}$：$\\text{df}(t_4)=12000$，$\\text{tf}(t_4)=4$。\n  - 词频因子：$1+\\ln(4)=1+1.386294361=2.386294361$。\n  - 逆文档频率：$\\ln\\!\\big(50000/12000\\big)=\\ln(4.166666667)=1.427116356$。\n  - TF-IDF：$w_4=2.386294361\\times 1.427116356=3.405519713$。\n- 对于 $t_5=\\text{chest\\_pain}$：$\\text{df}(t_5)=2500$，$\\text{tf}(t_5)=1$。\n  - 词频因子：$1+\\ln(1)=1$。\n  - 逆文档频率：$\\ln\\!\\big(50000/2500\\big)=\\ln(20)=2.995732274$。\n  - TF-IDF：$w_5=1\\times 2.995732274=2.995732274$。\n\n因此，该笔记的 TF-IDF 向量为\n$$\n\\mathbf{w}=\\big(w_1,w_2,w_3,w_4,w_5\\big)=\\big(5.198129906,\\;2.813410716,\\;7.001444144,\\;3.405519713,\\;2.995732274\\big).\n$$\n\n$\\mathbf{w}$ 的欧几里得范数的平方为\n$$\n\\|\\mathbf{w}\\|_2^2 \\;=\\; \\sum_{i=1}^{5} w_i^2\n\\;=\\; 5.198129906^2 + 2.813410716^2 + 7.001444144^2 + 3.405519713^2 + 2.995732274^2.\n$$\n计算每一项的平方：\n- $5.198129906^2=27.020554519$，\n- $2.813410716^2=7.915279857$，\n- $7.001444144^2=49.020220102$，\n- $3.405519713^2=11.597564516$，\n- $2.995732274^2=8.974411857$。\n求和，\n$$\n\\|\\mathbf{w}\\|_2^2 \\;=\\; 27.020554519 + 7.915279857 + 49.020220102 + 11.597564516 + 8.974411857 \\;=\\; 104.528030851.\n$$\n四舍五入到四位有效数字，结果为 $104.5$。\n\n关于 TF-IDF 与概率主题模型的讨论：潜在狄利克雷分布 (Latent Dirichlet Allocation, LDA) 是一个生成式概率模型，它假设每篇文档的生成过程如下：首先从一个狄利克雷先验分布中抽取文档特有的主题混合比例，然后为每个词元 (token) 从该混合比例中抽取一个主题，最后从该主题特有的词汇表多项分布中抽取一个词。该模型下的似然函数取决于整数词元计数，而后验推断的充分统计量基于离散的词语出现次数。TF-IDF 对计数应用了一种确定性的重加权方法，这会引入非整数的实值权重，并以一种 LDA 生成过程未考虑到的方式改变了词元的相对贡献。将 TF-IDF 直接作为 LDA 的输入违反了模型的假设（多项式词元生成和狄利克雷-多项式共轭性），并破坏了计数的概率解释。从第一性原理出发，LDA 的合适输入是原始计数（可能经过了停用词移除、词形还原和词汇表裁剪等标准预处理）。TF-IDF 可能适用于相似性搜索、文档检索，或作为非概率矩阵分解方法（如非负矩阵分解 Non-negative Matrix Factorization, NMF）的输入，但不适合直接替代 LDA 或相关概率主題模型中的计数。", "answer": "$$\\boxed{104.5}$$", "id": "4613952"}, {"introduction": "选择了合适的输入后，理解模型如何从数据中“学习”主题至关重要。潜在狄利克雷分配（$LDA$）的核心在于其推理算法，其中折叠吉布斯采样是一种常见方法。本练习 [@problem_id:4613930] 通过让您手动执行采样过程中的一个更新步骤，揭示了 $LDA$ 引擎的内部工作原理。通过这个具体的计算，您将亲身体验模型是如何结合文档级信息和主题级信息来推断单个词语的主题归属的。", "problem": "一家医院正在构建一个概率主题模型，以挖掘临床笔记和文献摘要中的主题。考虑潜在狄利克雷分配（Latent Dirichlet Allocation, LDA）的生成过程，其中对于每个文档 $d$，文档-主题比例向量 $\\boldsymbol{\\theta}_{d}$ 从狄利克雷分布 $\\mathrm{Dir}(\\boldsymbol{\\alpha})$ 中抽取；对于每个主题 $k$，主题-词分布 $\\boldsymbol{\\phi}_{k}$ 从狄利克雷分布 $\\mathrm{Dir}(\\beta \\mathbf{1}_{V})$ 中抽取，其中 $\\mathbf{1}_{V}$ 是 $V$ 维的全1向量。词的生成过程是：首先为词元 $i$ 从多项式分布 $\\mathrm{Multinomial}(\\boldsymbol{\\theta}_{d})$ 中抽取一个主题 $z_{i}$，然后从多项式分布 $\\mathrm{Multinomial}(\\boldsymbol{\\phi}_{z_{i}})$ 中抽取一个词 $w_{i}$。假设词汇表大小为 $V$，词先验为对称的 $\\beta$，但文档-主题先验 $\\boldsymbol{\\alpha}$ 可能是不对称的。\n\n对于一篇特定的临床笔记 $d$，考虑在折叠吉布斯采样（collapsed Gibbs sampling）下对词元“fever”的单次出现进行主题分配的重采样，其中 $\\boldsymbol{\\theta}_{d}$ 和所有的 $\\boldsymbol{\\phi}_{k}$ 都被积分掉。给定以下不包括当前词元（用上标 $^{-i}$ 表示）的量：\n\n- 主题数量为 $K=3$。\n- 词汇表大小为 $V=15000$。\n- 对称词先验为 $\\beta=0.002$。\n- 文档-主题狄利克雷先验为 $\\boldsymbol{\\alpha}=(\\alpha_{1},\\alpha_{2},\\alpha_{3})=(0.4,0.3,0.2)$。\n- 文档 $d$ 的文档-主题计数为：$(n_{d,1}^{-i},n_{d,2}^{-i},n_{d,3}^{-i})=(15,3,6)$。\n- 词“fever”的主题-词计数为：$(n_{1,\\mathrm{fever}}^{-i},n_{2,\\mathrm{fever}}^{-i},n_{3,\\mathrm{fever}}^{-i})=(120,5,10)$。\n- 主题词元总数为：$(n_{1}^{-i},n_{2}^{-i},n_{3}^{-i})=(8000,6000,7000)$。\n\n从标准的LDA生成假设和狄利克雷-多项式共轭性出发，推导将该词元分配给主题 $k$ 的折叠吉布斯条件概率，并用它计算 $k=1,2,3$ 的未归一化权重。将这些权重归一化，以获得该词元在主题上的一个合规分布。将该词元分配给主题 $k=2$ 的归一化概率以小数形式报告，并将最终答案四舍五入到五位有效数字。无需单位。", "solution": "任务是在给定所有其他主题分配的情况下，计算单个词元的主题分配的条件概率。在LDA的折叠吉布斯采样中，连续参数 $\\boldsymbol{\\theta}$（文档-主题分布）和 $\\boldsymbol{\\phi}$（主题-词分布）被积分掉。由于狄利克雷分布和多项式分布的共轭性，这成为可能。\n\n令 $z_i$ 为语料库中第 $i$ 个词元的主题分配。我们感兴趣的是对一个特定词元进行主题重采样，该词元是文档 $d$ 中单词“fever”的一个实例。假设这个词元位于位置 $i$，其词类型为 $w_i=v$（其中 $v$ 对应于“fever”）。在给定所有其他分配 $z_{\\neg i}$ 和所有词 $W$ 的情况下，将此词元分配给主题 $k$ 的全条件概率由下式给出：\n$$ P(z_i = k | z_{\\neg i}, W, \\boldsymbol{\\alpha}, \\beta) \\propto P(\\text{词 } v \\text{ 来自主题 } k) \\times P(\\text{主题 } k \\text{ 在文档 } d \\text{ 中}) $$\n右侧的两项对应于主题和文档的折叠狄利克雷-多项式模型的预测分布。\n\n最终的公式是：\n$$ P(z_i = k | z_{\\neg i}, W, \\boldsymbol{\\alpha}, \\beta) \\propto \\frac{n_{k,v}^{-i} + \\beta}{n_k^{-i} + V\\beta} \\times (n_{d,k}^{-i} + \\alpha_k) $$\n这里，符号表示如下：\n- $n_{k,v}^{-i}$ 是词类型 $v$ 被分配给主题 $k$ 的次数，不包括当前词元 $i$。\n- $n_k^{-i} = \\sum_{v'} n_{k,v'}^{-i}$ 是分配给主题 $k$ 的词元总数，不包括当前词元 $i$。\n- $n_{d,k}^{-i}$ 是文档 $d$ 中分配给主题 $k$ 的词元数量，不包括当前词元 $i$。\n- $\\beta$ 是主题-词狄利克雷先验的对称超参数。\n- $V$ 是词汇表大小。\n- $\\alpha_k$ 是文档-主题狄利克雷先验中主题 $k$ 的超参数。\n\n我们被给予了以下值：\n- 主题数量 $K=3$。\n- 词汇表大小 $V=15000$。\n- 对称词先验 $\\beta=0.002$。\n- 文档-主题先验 $\\boldsymbol{\\alpha}=(\\alpha_{1}, \\alpha_{2}, \\alpha_{3})=(0.4, 0.3, 0.2)$。\n- 待重采样的词元对应于词 $v=\\text{\"fever\"}$。\n- 文档-主题计数：$(n_{d,1}^{-i}, n_{d,2}^{-i}, n_{d,3}^{-i})=(15, 3, 6)$。\n- 词“fever”的主题-词计数：$(n_{1,v}^{-i}, n_{2,v}^{-i}, n_{3,v}^{-i})=(120, 5, 10)$。\n- 主题总计数：$(n_{1}^{-i}, n_{2}^{-i}, n_{3}^{-i})=(8000, 6000, 7000)$。\n\n首先，我们计算项 $V\\beta$，它在所有主题中都是常数：\n$$ V\\beta = 15000 \\times 0.002 = 30 $$\n\n现在，我们计算将该词元分配给每个主题 $k \\in \\{1, 2, 3\\}$ 的未归一化权重 $w_k$。\n$$ w_k = (n_{d,k}^{-i} + \\alpha_k) \\times \\frac{n_{k,v}^{-i} + \\beta}{n_k^{-i} + V\\beta} $$\n\n对于主题 $k=1$：\n$$ w_1 = (n_{d,1}^{-i} + \\alpha_1) \\times \\frac{n_{1,v}^{-i} + \\beta}{n_1^{-i} + V\\beta} = (15 + 0.4) \\times \\frac{120 + 0.002}{8000 + 30} $$\n$$ w_1 = 15.4 \\times \\frac{120.002}{8030} $$\n$$ w_1 \\approx 0.23014082 $$\n\n对于主题 $k=2$：\n$$ w_2 = (n_{d,2}^{-i} + \\alpha_2) \\times \\frac{n_{2,v}^{-i} + \\beta}{n_2^{-i} + V\\beta} = (3 + 0.3) \\times \\frac{5 + 0.002}{6000 + 30} $$\n$$ w_2 = 3.3 \\times \\frac{5.002}{6030} $$\n$$ w_2 \\approx 0.00273741 $$\n\n对于主题 $k=3$：\n$$ w_3 = (n_{d,3}^{-i} + \\alpha_3) \\times \\frac{n_{3,v}^{-i} + \\beta}{n_3^{-i} + V\\beta} = (6 + 0.2) \\times \\frac{10 + 0.002}{7000 + 30} $$\n$$ w_3 = 6.2 \\times \\frac{10.002}{7030} $$\n$$ w_3 \\approx 0.00882111 $$\n\n为了获得归一化概率，我们将未归一化的权重相加：\n$$ S = w_1 + w_2 + w_3 \\approx 0.23014082 + 0.00273741 + 0.00882111 \\approx 0.24169934 $$\n\n将该词元分配给主题 $k=2$ 的归一化概率是：\n$$ P(z_i = 2 | \\dots) = \\frac{w_2}{S} = \\frac{w_2}{w_1 + w_2 + w_3} $$\n$$ P(z_i = 2 | \\dots) \\approx \\frac{0.00273741}{0.24169934} \\approx 0.0113256958 $$\n\n问题要求将答案四舍五入到五位有效数字。\n$$ 0.0113256958 \\approx 0.011326 $$\n第五位有效数字是 $5$，其后的数字是 $6 \\ge 5$，因此我们向上取整。", "answer": "$$ \\boxed{0.011326} $$", "id": "4613930"}, {"introduction": "在现实世界的生物医学应用中，临床笔记和文献摘要的语料库规模可能非常庞大，这使得计算效率成为一个决定性因素。最后一个练习 [@problem_id:4613954] 将我们的关注点从模型的基本机制转向实际的性能工程。您将分析标准吉布斯采样器的计算复杂度，并探索如何利用数据中固有的稀疏性来设计优化算法，从而实现显著的性能提升，这是将主题模型应用于大规模数据集的关键技能。", "problem": "您正在分析潜在狄利克雷分配 (Latent Dirichlet Allocation, LDA) 及其折叠吉布斯采样 (collapsed Gibbs sampling, GS) 算法，该算法用于对由临床笔记和生物医学文献摘要组成的语料库进行主题建模。假设整个语料库共有 $N$ 个词元，固定数量为 $K$ 个主题，词汇表大小为 $V$。在用于LDA的折叠吉布斯采样中，每个词元的主题分配的条件分布是通过文档-主题计数和主题-词项计数来计算的，然后对新主题进行采样。朴素实现方法在每次迭代中为语料库中的每个词元评估所有 $K$ 个主题的未归一化概率，这导致每次迭代的计算成本与 $K$ 成正比。\n\n基本上，以下经过充分检验的事实适用：\n- 在用于LDA的折叠吉布斯采样中，更新每个词元的分配需要评估一个未归一化的概率，该概率依赖于文档-主题计数和主题-词项计数，然后从得到的关于 $K$ 个主题的分类分布中抽取一个样本。\n- 朴素实现方法为每个词元评估所有 $K$ 个主题，这意味着每个词元的成本与 $K$ 呈线性关系。\n- 在典型的语料库（如临床笔记和生物医学摘要）中，会出现稀疏性，因为每个文档只使用一小部分主题，每个词也只出现在一小部分主题中。定义 $s_d$ 为每个词元所在文档中遇到的非零文档-主题计数的平均数量，定义 $s_w$ 为每个词元的词项遇到的非零主题-词项计数的平均数量。根据经验，$s_d$ 和 $s_w$ 通常远小于 $K$。\n\n您的任务是：\n1. 从上述基本事实出发，基于第一性原理推导朴素折叠吉布斯采样器每次迭代操作计数的符号表达式，用 $N$、$K$ 和每个候选评估常数 $c_{\\text{cand}}$ 表示。您的推导应仔细证明其对 $K$ 的线性依赖性，并解释 $c_{\\text{cand}}$ 的作用。\n2. 提出一种利用稀疏性的优化方法，并通过分解为稀疏部分和密集背景部分来从理论上证明其合理性。假设密集部分可以使用 Walker 别名方法（也称为别名表）在期望常数时间内进行采样，该方法必须在每次迭代时构建一次，成本与 $K$ 成线性关系。设 $c_{\\text{bucket}}$ 为每个词元组合稀疏部分的恒定操作数，$c_{\\text{alias}}$ 为每个词元从别名表采样的期望恒定操作数，$c_{\\text{build}}$ 为每次迭代中为每个主题构建别名表的恒定操作数。推导这种稀疏-加-别名优化方法每次迭代操作计数的符号表达式，用 $N$、$K$、$s_d$、$s_w$、$c_{\\text{cand}}$、$c_{\\text{bucket}}$、$c_{\\text{alias}}$ 和 $c_{\\text{build}}$ 表示。\n3. 实现一个完整、可运行的程序，为下面的测试套件中的每个测试用例计算以下三个量：\n   - 朴素的每次迭代操作计数 $C_{\\text{naive}}$。\n   - 使用稀疏-加-别名方法时的优化后每次迭代操作计数 $C_{\\text{opt}}$。\n   - 加速比 $R = \\frac{C_{\\text{naive}}}{C_{\\text{opt}}}$，以小数形式表示（非百分比）。\n4. 在您的计算中使用以下固定常量值：$c_{\\text{cand}} = 1$，$c_{\\text{bucket}} = 3$，$c_{\\text{alias}} = 1$，以及 $c_{\\text{build}} = 2$。所有计数都是标量操作计数，因此是无量纲的。\n5. 测试套件包含以下参数集 $(N, K, s_d, s_w)$，旨在覆盖不同的情况：\n   - 用例1（典型临床笔记规模）：$(10^6, 100, 5, 10)$。\n   - 用例2（大规模主题库，稀疏使用）：$(10^5, 10^3, 3, 3)$。\n   - 用例3（单主题边缘情况）：$(10^4, 1, 1, 1)$。\n   - 用例4（无稀疏性边界条件）：$(5 \\cdot 10^5, 200, 200, 200)$。\n6. 您的程序必须为每个测试用例计算第3项中的三个量，并生成单行输出，将所有用例的结果聚合为一个逗号分隔的列表，并用方括号括起来。每个用例的结果本身必须是一个包含三个小数 $[C_{\\text{naive}}, C_{\\text{opt}}, R]$ 的列表，每个小数四舍五入到六位小数。最终输出格式应为：\n   - 示例：$[[x_1,y_1,z_1],[x_2,y_2,z_2],[x_3,y_3,z_3],[x_4,y_4,z_4]]$。\n\n通过证明分解的合理性以及使用别名方法作为在支付每次迭代的一次性构建成本后即可实现常数时间采样的采样器，来确保科学真实性。该问题必须在数学上进行框架设计，并且无需外部数据即可解决，其难度应符合生物信息学和医学数据分析领域中专注于临床笔记和文献主题建模的高级研究生水平。", "solution": "### 第1部分：朴素折叠吉布斯采样器操作计数的推导\n\nLDA的朴素折叠吉布斯采样器实现在每次迭代中对语料库进行一次完整遍历。单次迭代包括更新语料库中每个词元的主题分配。\n\n设 $N$ 为语料库中的词元总数，$K$ 为主题数。\n对于 $N$ 个词元中的每一个，采样器必须计算该词元属于 $K$ 个可能主题中每一个的条件概率。这涉及为 $K$ 个候选主题中的每一个计算一个未归一化的概率得分。\n问题将 $c_{\\text{cand}}$ 定义为一个常数，表示评估一个此类候选概率所需的初等操作数。这个常数概括了诸如从内存中获取计数、执行乘法和除法等操作，这些操作由LDA的条件概率公式指定：\n$$P(z_i=k | \\mathbf{z}_{\\neg i}, \\mathbf{w}) \\propto (n_{d,k}^{\\neg i} + \\alpha) \\frac{n_{k,w}^{\\neg i} + \\beta}{n_k^{\\neg i} + V\\beta}$$\n其中 $z_i$ 是第 $i$ 个词元的主题分配，$d$ 是其所在文档，$w$ 是其词项类型，$n$ 是各种计数，$\\alpha, \\beta$ 是超参数。计算单个主题 $k$ 的此表达式的成本对应于 $c_{\\text{cand}}$。\n\n单个词元的总操作成本 $C_{\\text{token}}$ 是评估所有 $K$ 个主题的成本：\n$$C_{\\text{token}} = K \\cdot c_{\\text{cand}}$$\n\n采样器一次完整迭代的总操作成本 $C_{\\text{naive}}$ 是每个词元的成本乘以词元总数 $N$：\n$$C_{\\text{naive}} = N \\cdot C_{\\text{token}}$$\n代入 $C_{\\text{token}}$ 的表达式，我们得到朴素采样器每次迭代操作计数的符号表达式：\n$$C_{\\text{naive}} = N \\cdot K \\cdot c_{\\text{cand}}$$\n此推导清晰地显示了成本对 $N$ 和 $K$ 的线性依赖关系。\n\n### 第2部分：稀疏-加-别名优化采样器操作计数的推导\n\n该优化利用了文本语料库主题模型中固有的稀疏性：任何给定的文档通常只涉及少数几个主题，任何给定的词也只与少数几个主题强相关。这意味着对于一个词元 $i$（文档 $d$ 中的词项类型 $w$），计数 $n_{d,k}$ 和 $n_{k,w}$ 仅在 $K$ 个主题的一小部分子集中为非零。完整的条件概率分布可以分解为稀疏部分（这些计数非零的地方）和密集背景部分（由平滑超参数 $\\alpha$ 和 $\\beta$ 驱动）。\n\n优化采样器每次迭代的总成本 $C_{\\text{opt}}$ 是一次性设置成本与所有 $N$ 个词元采样累积成本之和。\n\n**每次迭代的设置成本：**\n概率分布的密集背景部分与一个仅依赖于主题级别计数的项（例如 $(\\sum_v n_{k,v} + V\\beta)^{-1}$）成正比。如果准备了合适的数据结构，这个关于所有 $K$ 个主题的分布可以被高效地采样。问题指定使用 Walker 别名方法。为具有 $K$ 个结果的分类分布构建别名表的计算成本与 $K$ 成线性关系。设 $c_{\\text{build}}$ 为构建该表时每个主题所需的恒定操作数。每次迭代的总一次性设置成本为：\n$$C_{\\text{build\\_iter}} = K \\cdot c_{\\text{build}}$$\n\n**每个词元的采样成本：**\n对于 $N$ 个词元中的每一个，采样器执行一系列操作。\n1.  **稀疏部分评估：** 我们不再评估所有 $K$ 个主题，而是只显式评估那些文档-主题计数 ($n_{d,k}$) 或主题-词项计数 ($n_{k,w}$) 非零的主题。问题提供 $s_d$ 作为遇到的非零文档-主题计数的平均数量，$s_w$ 作为遇到的非零主题-词项计数的平均数量。因此，平均需要评估的稀疏候选总数为 $s_d + s_w$。（为简单起见，我们假设可加性，这在此类复杂度分析中很常见，忽略了两组主题之间可能的重叠）。每个候选的成本为 $c_{\\text{cand}}$，因此此步骤的成本为 $(s_d + s_w) \\cdot c_{\\text{cand}}$。\n\n2.  **部分组合：** 评估稀疏部分后，必须计算它们对总概率质量的贡献，并与密集部分的质量相结合。问题将此逻辑的成本（例如，管理稀疏概率的数据结构、对它们求和以及为最终采样决策做准备）抽象为一个单一的恒定成本 $c_{\\text{bucket}}$。\n\n3.  **采样步骤：** 最后一步是从组合（混合）分布中抽取一个新主题。这涉及决定是从稀疏集还是密集集中采样，然后执行采样。问题通过说明通过别名表从密集部分采样的期望恒定成本为 $c_{\\text{alias}}$ 来简化这一复杂步骤。此成本被理解为代表此优化方案中每个词元的最终有效采样操作。\n\n将这些成本相加，得到每个词元的平均总操作成本 $C_{\\text{token\\_opt}}$：\n$$C_{\\text{token\\_opt}} = (s_d + s_w) \\cdot c_{\\text{cand}} + c_{\\text{bucket}} + c_{\\text{alias}}$$\n\n**总优化成本：**\n优化采样器每次迭代的总操作计数 $C_{\\text{opt}}$ 是初始构建成本与所有 $N$ 个词元采样总成本之和：\n$$C_{\\text{opt}} = C_{\\text{build\\_iter}} + N \\cdot C_{\\text{token\\_opt}}$$\n代入推导出的表达式，我们得到最终的符号公式：\n$$C_{\\text{opt}} = K \\cdot c_{\\text{build}} + N \\cdot ((s_d + s_w) \\cdot c_{\\text{cand}} + c_{\\text{bucket}} + c_{\\text{alias}})$$\n当 $s_d, s_w \\ll K$ 时，此表达式有效，此时每个词元的成本近似为常数或增长远慢于 $K$，从而导致相对于朴素方法显著的加速。如果不存在稀疏性（即 $s_d, s_w \\approx K$），此方法的开销使其效率低于朴素方法。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and prints the operational costs and speedup for naive and \n    optimized LDA Gibbs samplers based on derived complexity formulas.\n    \"\"\"\n\n    # Define the fixed constant values from the problem statement.\n    # c_cand: per-candidate evaluation constant\n    # c_bucket: constant operations to combine sparse components per token\n    # c_alias: expected constant operations to sample from alias table per token\n    # c_build: constant operations to build alias table per topic per iteration\n    c_cand = 1.0\n    c_bucket = 3.0\n    c_alias = 1.0\n    c_build = 2.0\n\n    # Define the test suite of parameter sets (N, K, s_d, s_w).\n    # N: total number of tokens\n    # K: number of topics\n    # s_d: average number of nonzero document-topic counts\n    # s_w: average number of nonzero topic-word counts\n    test_cases = [\n        (10**6, 100, 5, 10),      # Case 1: typical clinical notes scale\n        (10**5, 10**3, 3, 3),      # Case 2: large topic inventory, sparse usage\n        (10**4, 1, 1, 1),          # Case 3: single topic edge case\n        (5 * 10**5, 200, 200, 200), # Case 4: no sparsity boundary condition\n    ]\n\n    # A list to store the results for each test case.\n    results = []\n\n    for case in test_cases:\n        N, K, s_d, s_w = case\n\n        # Convert integers to floats for calculations to ensure float division.\n        N, K, s_d, s_w = float(N), float(K), float(s_d), float(s_w)\n\n        # 1. Calculate the naive per-iteration operation count (C_naive).\n        # Formula: C_naive = N * K * c_cand\n        c_naive = N * K * c_cand\n\n        # 2. Calculate the optimized per-iteration operation count (C_opt).\n        # Formula: C_opt = K * c_build + N * ((s_d + s_w) * c_cand + c_bucket + c_alias)\n        per_token_cost_opt = (s_d + s_w) * c_cand + c_bucket + c_alias\n        c_opt = K * c_build + N * per_token_cost_opt\n        \n        # 3. Calculate the speedup ratio R.\n        # Formula: R = C_naive / C_opt\n        # Handle division by zero, although not expected with the given formulas/inputs.\n        if c_opt == 0:\n            speedup_ratio = float('inf')\n        else:\n            speedup_ratio = c_naive / c_opt\n\n        # Round the results to six decimal places as required.\n        c_naive_rounded = round(c_naive, 6)\n        c_opt_rounded = round(c_opt, 6)\n        speedup_ratio_rounded = round(speedup_ratio, 6)\n\n        # Append the formatted result for the current case.\n        # Required format for each case is a list of three decimal numbers.\n        results.append(\n            [c_naive_rounded, c_opt_rounded, speedup_ratio_rounded]\n        )\n    \n    # Final print statement in the exact required format.\n    output_parts = []\n    for r in results:\n        # Format each number to 6 decimal places and join into a string like `[num1,num2,num3]`\n        output_parts.append(f\"[{r[0]:.6f},{r[1]:.6f},{r[2]:.6f}]\")\n    \n    print(f\"[{','.join(output_parts)}]\")\n\n\nsolve()\n```", "id": "4613954"}]}