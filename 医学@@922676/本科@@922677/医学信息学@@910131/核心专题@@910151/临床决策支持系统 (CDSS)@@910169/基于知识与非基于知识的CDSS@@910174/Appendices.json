{"hands_on_practices": [{"introduction": "要真正理解基于知识的临床决策支持系统（CDSS），我们必须深入其“引擎盖下”，审视其规则引擎。本练习模拟了一个简单的前向链接系统，以探讨一个至关重要的现实世界特性：计算性能。通过推导时间复杂度，您将具体理解规则数量、条件数量和患者事实如何影响系统的可扩展性。[@problem_id:4846725]", "problem": "一家医院正在部署一个基于知识的临床决策支持系统（CDSS），该系统使用一个正向链接的产生式规则引擎。在该引擎中，每条规则恰好有 $m$ 个前件条件，总共有 $n$ 条规则。对于某次患者就诊，工作内存包含 $k$ 个活动事实，每个事实是一个可以满足条件的编码谓词。假设单次评估过程的操作模型如下：\n1. 引擎执行朴素的、无索引的模式匹配：为了检查单个条件是否满足，引擎线性扫描当前工作内存，并将该条件与每个活动事实进行比较，直到找到匹配项或检查完所有事实。每次比较的成本为常数时间。\n2. 在此过程中，规则只能生成警报对象，这些对象不会成为新的工作内存事实，因此活动事实的数量保持为 $k$ 不变；此过程中不会断言新的事实。\n3. 只有当一条规则的所有 $m$ 个条件都匹配后，才会考虑执行该规则；无论其他规则是否已触发，都会尝试对所有 $n$ 条规则进行条件匹配。\n\n使用时间复杂度的标准定义，即作为输入大小函数的原始比较的渐近计数，从第一性原理推导此单次正向链接评估过程对患者记录的最坏情况时间复杂度，并以包含 $n$、$m$ 和 $k$ 的单个大O表达式表示。此外，简要讨论基于数据结构选择和模式匹配算法的原则性优化策略，这些策略可以在其他假设相同的情况下降低每次评估的渐近复杂度，但不要改变您报告的最终数值表达式。\n\n以包含 $n$、$m$ 和 $k$ 的单个大O表达式形式提供您的最终答案。不需要单位。不要简化为不等式或方程式。最终答案必须是单个解析表达式。", "solution": "在继续之前，对问题陈述进行验证。\n\n步骤1：提取给定条件\n- 该系统是一个基于知识的临床决策支持系统（CDSS），带有一个正向链接的产生式规则引擎。\n- 总共有 $n$ 条规则。\n- 每条规则恰好有 $m$ 个前件条件。\n- 工作内存包含某次患者就诊的 $k$ 个活动事实。\n- 模式匹配模型是朴素且无索引的。\n- 为检查一个条件，引擎对工作内存中的 $k$ 个事实进行线性扫描。\n- 条件与事实之间的每次比较成本为常数时间。\n- 在单次评估过程中，事实的数量 $k$ 保持不变。\n- 只有当一条规则的所有 $m$ 个条件都匹配后，才会考虑执行该规则。\n- 每次评估都会尝试对所有 $n$ 条规则进行条件匹配。\n\n步骤2：使用提取的给定条件进行验证\n该问题具有科学依据，因为它描述了一个简化但标准的、教科书式的产生式规则系统模型，这是人工智能和专家系统中的一个基本概念。术语和过程（正向链接、工作内存、模式匹配）在计算机科学中都有明确的定义。该问题是适定的，提供了所有必要的变量（$n$、$m$、$k$）和待分析算法的完整描述，确保时间复杂度存在唯一解。语言客观而精确。问题内部一致，没有矛盾，并且可以直接形式化为一个算法分析任务。除了为便于分析而进行的简化外，它没有违反任何科学原理，也不包含不切实际的假设。\n\n步骤3：结论与行动\n问题有效。将制定一个合理的解决方案。\n\n任务是推导所述正向链接引擎单次评估过程的最坏情况时间复杂度。时间复杂度定义为原始比较的渐近计数。这里的原始操作是规则条件与工作内存中事实的单次比较，给定其耗时为常数时间。我们可以将此常数时间表示为 $c$。\n\n让我们从最细粒度的操作开始，逐步分析计算成本。\n\n1.  匹配单个条件的成本：\n    问题陈述中提到，为检查单个条件，引擎“线性扫描当前工作内存，并将该条件与每个活动事实进行比较”。工作内存包含 $k$ 个事实。在最坏情况下，找不到匹配项，或者匹配的事实是最后一个被检查的。这需要引擎执行 $k$ 次比较。因此，匹配一个条件的最坏情况时间成本与 $k$ 成正比。用渐近表示法，即为 $O(k)$。\n\n2.  评估单条规则的成本：\n    每条规则恰好有 $m$ 个前件条件。为了考虑执行某条规则，其所有 $m$ 个条件都必须满足。根据所描述的模型，引擎会独立地尝试匹配这 $m$ 个条件中的每一个。在最坏情况下，匹配一个条件的成本是 $O(k)$。由于单条规则有 $m$ 个这样的条件，评估一条规则的总最坏情况成本是匹配其每个条件成本的总和。即：\n    $$ \\text{Cost}_{\\text{rule}} = \\sum_{i=1}^{m} O(k) = m \\times O(k) = O(m \\cdot k) $$\n    这假设每个条件的验证都需要重新扫描工作内存，这与“朴素的、无索引的模式匹配”模型一致。\n\n3.  单次评估过程的成本：\n    单次评估过程涉及尝试匹配规则库中的所有 $n$ 条规则。问题明确指出，“无论其他规则是否已触发，都会尝试对所有 $n$ 条规则进行条件匹配”。因此，该过程的总成本是评估 $n$ 条规则中每一条规则的成本总和。\n    $$ \\text{Cost}_{\\text{pass}} = \\sum_{j=1}^{n} \\text{Cost}_{\\text{rule}} = \\sum_{j=1}^{n} O(m \\cdot k) = n \\times O(m \\cdot k) $$\n    因此，单次评估过程的最终最坏情况时间复杂度为 $O(n \\cdot m \\cdot k)$。\n\n优化策略讨论：\n推导出的复杂度 $O(n \\cdot m \\cdot k)$ 凸显了朴素匹配策略的计算负担。关键瓶颈在于对每个条件的线性扫描（$O(k)$）以及在每次评估中重新评估所有规则。原则性的优化旨在解决这些低效问题。\n\n1.  工作内存的数据结构选择：对 $k$ 个事实的线性扫描可以被优化。可以将事实存储在哈希表（或哈希映射）中，而不是简单的列表或数组中。事实可以按其谓词类型或其他关键属性进行索引。例如，对于像“Patient.Age > $65$”这样的条件，可以对“Patient.Age”进行哈希。这将使得检查相关事实的存在性的期望时间从 $O(k)$ 降至 $O(1)$。这种优化会降低匹配单个条件的复杂度。在平均情况下，整个评估过程的复杂度将改善为 $O(n \\cdot m)$。\n\n2.  模式匹配算法：所描述的模型在每次评估中都重新评估所有 $n \\times m$ 个条件。这是高度冗余的，因为工作内存在两次评估之间可能只有微小的变化。更高级的算法，如 Rete 算法，通过将规则编译成数据流网络来解决这个问题。事实被断言到这个网络中，部分匹配的状态在网络节点内得以维持。这个特性，被称为时间冗余性，意味着计算量与工作内存的*变化*数量成正比，而不是其总大小。算法只计算增量，而不是重新匹配所有内容。虽然 Rete 的最坏情况复杂度仍然可能很高，但其在典型问题上的平均性能远优于朴素的 $O(n \\cdot m \\cdot k)$ 方法，因为依赖于 $k$ 的项被有效地摊销了。其他算法如 TREAT 和 LEAPS 在此状态保存原则上提供了变体。", "answer": "$$\\boxed{O(n \\cdot m \\cdot k)}$$", "id": "4846725"}, {"introduction": "非基于知识的系统从数据中学习，但这种能力也带来了“过拟合”的风险——即学习到数据中的噪声而非真实模式。本练习深入探讨正则化，这是一种确保这些模型稳定和泛化的关键技术。通过分步计算，您将亲眼见证 $L_2$ 惩罚项如何“收缩”模型参数，从而构建一个更稳健的预测器。[@problem_id:4846692]", "problem": "一家医院正在评估两种实施临床决策支持系统 (CDSS) 的方法：一种是使用明确的、由专家编写的规则的基于知识的CDSS，另一种是从数据中学习模式的非基于知识的CDSS。为了量化非基于知识的模型中的正则化如何控制过拟合，我们考虑一个逻辑回归模型，该模型用于根据从实验室数据中得出的单个标准化预测变量来预测二元结果（例如，是否需要干预）的概率。\n\n设模型为 $p(y=1 \\mid x) = \\sigma(b + w x)$，其中 $\\sigma(z) = \\frac{1}{1+\\exp(-z)}$，$b$ 是截距，$w$ 是单个特征 $x$ 的斜率。给定 $n=4$ 个数据点，其特征值为 $x_{1}=-1$，$x_{2}=0$，$x_{3}=1$，$x_{4}=2$，标签为 $y_{1}=0$，$y_{2}=0$，$y_{3}=1$，$y_{4}=1$。设设计矩阵为 $X \\in \\mathbb{R}^{n \\times 2}$，其行向量为 $[1, x_{i}]$，参数向量为 $\\theta = \\begin{pmatrix} b \\\\ w \\end{pmatrix}$。\n\n从基本定义开始：\n- 逻辑回归的负对数似然为 $\\ell(\\theta) = -\\sum_{i=1}^{n} \\left[ y_{i} \\ln p_{i} + (1-y_{i}) \\ln (1-p_{i}) \\right]$，其中 $p_{i} = \\sigma(b + w x_{i})$。\n- 用于估计的 $L_{2}$ 正则化目标函数为 $J(\\theta) = \\ell(\\theta) + \\frac{\\lambda}{2} \\|w\\|_{2}^{2}$，其中截距 $b$ 不受惩罚。\n\n使用二阶牛顿法最小化 $J(\\theta)$：\n1. 从第一性原理推导牛顿步，用在当前迭代值 $\\theta^{(t)}$ 处计算的 $J(\\theta)$ 的梯度和海森矩阵来表示。\n2. 在 $\\theta^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 处初始化，并计算非正则化情况（$\\lambda = 0$）下的第一次牛顿迭代值 $\\theta^{(1)}$。\n3. 在相同的 $\\theta^{(0)}$ 处初始化，并计算正则化情况（$\\lambda = 1$，仅惩罚 $w$）下的第一次牛顿迭代值 $\\theta^{(1)}_{\\lambda}$。\n4. 将收缩因子 $s$ 定义为第一次迭代的参数向量的欧几里得范数之比，$s = \\dfrac{\\|\\theta^{(1)}_{\\lambda=1}\\|_{2}}{\\|\\theta^{(1)}_{\\lambda=0}\\|_{2}}$。\n\n基于以上定义，定性解释与基于知识的CDSS相比，$L_{2}$ 正则化如何在非基于知识的CDSS中控制过拟合。然后根据给定的数据和初始点精确计算 $s$。将 $s$ 的最终答案表示为一个无单位的最简分数。不要四舍五入。", "solution": "该问题被评估为有效，因为它科学地基于正则化逻辑回归的原理，问题陈述清晰且提供了唯一解所需的所有信息，并且其表述是客观的。\n\n首先，我们解决问题的定性部分。一个非基于知识的临床决策支持系统 (CDSS) 直接从数据中学习关系。当使用像逻辑回归这样的模型时，它容易出现过拟合，这意味着模型学习到的是训练数据集特有的细微差别和噪声，而不是真实的潜在模式。这导致模型在新的、未见过的数据上表现不佳。在线性和逻辑回归模型中，过拟合通常对应于具有较大数值的参数向量。例如，一个大的权重 $w$ 会使预测对输入特征 $x$ 的微小变化高度敏感，这是一个复杂的、不可泛化模型的特征。$L_2$ 正则化通过在目标函数中添加一个惩罚项 $\\frac{\\lambda}{2}w^2$ 来解决这个问题。这个惩罚项不鼓励产生大的权重。优化过程必须在拟合数据（最小化负对数似然 $\\ell(\\theta)$）和保持模型参数较小（最小化惩罚项）之间找到平衡。超参数 $\\lambda$ 控制这种权衡的强度。通过将权重向零收缩，正则化促进了更简单的模型，这些模型不太可能过拟合，因此能更好地泛化到新的患者数据上。相比之下，一个基于知识的CDSS是根据人类专家定义的一套明确规则来运行的（例如，“IF lab_value > threshold THEN recommend_action”）。这类系统不以相同的方式从数据中学习参数，因此过拟合的概念以及作为缓解策略的正则化在这里不直接适用。它们的有效性是通过专家共识和临床试验来检验的，而不是通过惩罚学习到的模型权重。\n\n现在，我们进行定量分析。逻辑回归模型为 $p_i = p(y=1|x_i) = \\sigma(b + w x_i)$，其中 $\\sigma(z) = (1+\\exp(-z))^{-1}$。参数向量为 $\\theta = \\begin{pmatrix} b \\\\ w \\end{pmatrix}$。正则化目标函数为 $J(\\theta) = \\ell(\\theta) + \\frac{\\lambda}{2}w^2$，其中 $\\ell(\\theta)$ 是负对数似然：\n$$\n\\ell(\\theta) = -\\sum_{i=1}^{n} \\left[ y_i \\ln(p_i) + (1-y_i) \\ln(1-p_i) \\right]\n$$\n牛顿法用二阶泰勒展开在当前迭代值 $\\theta^{(t)}$ 附近近似目标函数 $J(\\theta)$：\n$$\nJ(\\theta) \\approx J(\\theta^{(t)}) + (\\nabla J(\\theta^{(t)}))^T (\\theta - \\theta^{(t)}) + \\frac{1}{2} (\\theta - \\theta^{(t)})^T (\\nabla^2 J(\\theta^{(t)})) (\\theta - \\theta^{(t)})\n$$\n为了找到这个二次近似的最小值，我们将其关于 $\\theta$ 的梯度设为零。设 $g(\\theta^{(t)}) = \\nabla J(\\theta^{(t)})$ 为梯度，$H(\\theta^{(t)}) = \\nabla^2 J(\\theta^{(t)})$ 为海森矩阵。\n$$\ng(\\theta^{(t)}) + H(\\theta^{(t)}) (\\theta - \\theta^{(t)}) = 0\n$$\n求解下一个迭代值 $\\theta^{(t+1)} = \\theta$，得到更新规则：\n$$\n\\theta^{(t+1)} = \\theta^{(t)} - [H(\\theta^{(t)})]^{-1} g(\\theta^{(t)})\n$$\n牛顿步为 $\\Delta\\theta^{(t)} = -[H(\\theta^{(t)})]^{-1} g(\\theta^{(t)})$。\n\n我们需要 $J(\\theta)$ 的梯度和海森矩阵。梯度为 $g(\\theta) = \\nabla J(\\theta) = \\begin{pmatrix} \\partial J/\\partial b \\\\ \\partial J/\\partial w \\end{pmatrix}$：\n$$\ng(\\theta) = X^T(p-y) + \\begin{pmatrix} 0 \\\\ \\lambda w \\end{pmatrix} = \\begin{pmatrix} \\sum_{i=1}^n (p_i - y_i) \\\\ \\left(\\sum_{i=1}^n x_i(p_i - y_i)\\right) + \\lambda w \\end{pmatrix}\n$$\n海森矩阵为 $H(\\theta) = \\nabla^2 J(\\theta)$：\n$$\nH(\\theta) = X^T W X + \\begin{pmatrix} 0 & 0 \\\\ 0 & \\lambda \\end{pmatrix}\n$$\n其中 $W$ 是一个对角矩阵，其对角元素为 $W_{ii} = p_i(1-p_i)$。\n\n给定 $n=4$ 个数据点：$(x_1, y_1) = (-1, 0)$，$(x_2, y_2) = (0, 0)$，$(x_3, y_3) = (1, 1)$，$(x_4, y_4) = (2, 1)$。设计矩阵为 $X = \\begin{pmatrix} 1 & -1 \\\\ 1 & 0 \\\\ 1 & 1 \\\\ 1 & 2 \\end{pmatrix}$。\n初始迭代值为 $\\theta^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。此时，$b=0$ 且 $w=0$，所以对所有 $i$ 都有 $p_i = \\sigma(0) = \\frac{1}{2}$。\n概率向量为 $p^{(0)} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 1/2 \\\\ 1/2 \\end{pmatrix}$，标签向量为 $y = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix}$。\n因此，$p^{(0)} - y = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ -1/2 \\\\ -1/2 \\end{pmatrix}$。\n\n首先，我们计算非正则化情况（$\\lambda=0$）下的第一次迭代值。\n在 $\\theta^{(0)}$ 处的梯度为：\n$$\ng^{(0)} = X^T(p^{(0)}-y) = \\begin{pmatrix} 1 & 1 & 1 & 1 \\\\ -1 & 0 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ -1/2 \\\\ -1/2 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2}+\\frac{1}{2}-\\frac{1}{2}-\\frac{1}{2} \\\\ -\\frac{1}{2}+0-\\frac{1}{2}-1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -2 \\end{pmatrix}\n$$\n对于海森矩阵，我们首先计算权重矩阵 $W$。在 $\\theta^{(0)}$ 处，对所有 $i$ 都有 $W_{ii} = p_i(1-p_i) = \\frac{1}{2}(1-\\frac{1}{2}) = \\frac{1}{4}$。所以 $W = \\frac{1}{4}I$。\n在 $\\theta^{(0)}$ 处，当 $\\lambda=0$ 时的海森矩阵为：\n$$\nH^{(0)} = X^T W X = \\frac{1}{4} X^T X = \\frac{1}{4} \\begin{pmatrix} 1 & 1 & 1 & 1 \\\\ -1 & 0 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 & -1 \\\\ 1 & 0 \\\\ 1 & 1 \\\\ 1 & 2 \\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix} 4 & 2 \\\\ 2 & 6 \\end{pmatrix} = \\begin{pmatrix} 1 & 1/2 \\\\ 1/2 & 3/2 \\end{pmatrix}\n$$\n海森矩阵的逆矩阵为：\n$$\n[H^{(0)}]^{-1} = \\frac{1}{1(\\frac{3}{2}) - (\\frac{1}{2})(\\frac{1}{2})} \\begin{pmatrix} 3/2 & -1/2 \\\\ -1/2 & 1 \\end{pmatrix} = \\frac{1}{5/4} \\begin{pmatrix} 3/2 & -1/2 \\\\ -1/2 & 1 \\end{pmatrix} = \\frac{4}{5} \\begin{pmatrix} 3/2 & -1/2 \\\\ -1/2 & 1 \\end{pmatrix} = \\begin{pmatrix} 6/5 & -2/5 \\\\ -2/5 & 4/5 \\end{pmatrix}\n$$\n当 $\\lambda=0$ 时的第一次迭代值为：\n$$\n\\theta^{(1)} = \\theta^{(0)} - [H^{(0)}]^{-1} g^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 6/5 & -2/5 \\\\ -2/5 & 4/5 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -2 \\end{pmatrix} = - \\begin{pmatrix} 4/5 \\\\ -8/5 \\end{pmatrix} = \\begin{pmatrix} -4/5 \\\\ 8/5 \\end{pmatrix}\n$$\n\n其次，我们计算正则化情况（$\\lambda=1$）下的第一次迭代值。\n在 $\\theta^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 处，梯度 $g_\\lambda^{(0)}$ 与非正则化情况下的梯度相同，因为正则化项是 $\\lambda w$ 而 $w=0$。\n$$\ng_\\lambda^{(0)} = \\begin{pmatrix} 0 \\\\ -2 \\end{pmatrix}\n$$\n在 $\\theta^{(0)}$ 处的正则化海森矩阵为：\n$$\nH_\\lambda^{(0)} = H^{(0)} + \\begin{pmatrix} 0 & 0 \\\\ 0 & \\lambda \\end{pmatrix} = \\begin{pmatrix} 1 & 1/2 \\\\ 1/2 & 3/2 \\end{pmatrix} + \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 1/2 \\\\ 1/2 & 5/2 \\end{pmatrix}\n$$\n正则化海森矩阵的逆矩阵为：\n$$\n[H_\\lambda^{(0)}]^{-1} = \\frac{1}{1(\\frac{5}{2}) - (\\frac{1}{2})(\\frac{1}{2})} \\begin{pmatrix} 5/2 & -1/2 \\\\ -1/2 & 1 \\end{pmatrix} = \\frac{1}{9/4} \\begin{pmatrix} 5/2 & -1/2 \\\\ -1/2 & 1 \\end{pmatrix} = \\frac{4}{9} \\begin{pmatrix} 5/2 & -1/2 \\\\ -1/2 & 1 \\end{pmatrix} = \\begin{pmatrix} 10/9 & -2/9 \\\\ -2/9 & 4/9 \\end{pmatrix}\n$$\n当 $\\lambda=1$ 时的第一次迭代值为：\n$$\n\\theta^{(1)}_{\\lambda=1} = \\theta^{(0)} - [H_\\lambda^{(0)}]^{-1} g_\\lambda^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 10/9 & -2/9 \\\\ -2/9 & 4/9 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -2 \\end{pmatrix} = - \\begin{pmatrix} 4/9 \\\\ -8/9 \\end{pmatrix} = \\begin{pmatrix} -4/9 \\\\ 8/9 \\end{pmatrix}\n$$\n\n最后，我们计算收缩因子 $s = \\frac{\\|\\theta^{(1)}_{\\lambda=1}\\|_{2}}{\\|\\theta^{(1)}_{\\lambda=0}\\|_{2}}$。\n首先，我们求参数向量的欧几里得范数。\n$$\n\\|\\theta^{(1)}_{\\lambda=0}\\|_{2} = \\left\\| \\begin{pmatrix} -4/5 \\\\ 8/5 \\end{pmatrix} \\right\\|_{2} = \\sqrt{\\left(-\\frac{4}{5}\\right)^2 + \\left(\\frac{8}{5}\\right)^2} = \\sqrt{\\frac{16}{25} + \\frac{64}{25}} = \\sqrt{\\frac{80}{25}} = \\sqrt{\\frac{16 \\cdot 5}{25}} = \\frac{4\\sqrt{5}}{5}\n$$\n$$\n\\|\\theta^{(1)}_{\\lambda=1}\\|_{2} = \\left\\| \\begin{pmatrix} -4/9 \\\\ 8/9 \\end{pmatrix} \\right\\|_{2} = \\sqrt{\\left(-\\frac{4}{9}\\right)^2 + \\left(\\frac{8}{9}\\right)^2} = \\sqrt{\\frac{16}{81} + \\frac{64}{81}} = \\sqrt{\\frac{80}{81}} = \\sqrt{\\frac{16 \\cdot 5}{81}} = \\frac{4\\sqrt{5}}{9}\n$$\n收缩因子 $s$ 是这些范数的比值：\n$$\ns = \\frac{\\frac{4\\sqrt{5}}{9}}{\\frac{4\\sqrt{5}}{5}} = \\frac{4\\sqrt{5}}{9} \\cdot \\frac{5}{4\\sqrt{5}} = \\frac{5}{9}\n$$", "answer": "$$\n\\boxed{\\frac{5}{9}}\n$$", "id": "4846692"}, {"introduction": "在医疗保健这种高风险环境中，CDSS 必须能抵御不完美的数据，无论是来自测量误差还是恶意输入。这最后一个练习将基于知识和非基于知识的模型都置于压力之下，以评估它们的稳健性。通过计算改变临床决策所需输入的精确扰动量，您将建立一个量化框架，用于比较这些不同系统的弹性。[@problem_id:4846799]", "problem": "您需要形式化并评估一个基于知识的临床决策支持系统（CDSS）和一个非基于知识的CDSS对对抗性输入和错误输入的弹性。基于知识的CDSS被建模为一个基于显式规则的线性评分系统，而非基于知识的CDSS被建模为一个学习到的线性分类器（逻辑斯谛模型的线性对数几率）。您的任务是从关于范数和线性决策函数的核心定义和经过充分检验的事实出发，推导并实现输入扰动和误差下的弹性计算。\n\n基本基础和定义：\n- 临床决策支持系统（CDSS）根据输入特征产生二元决策。基于知识的CDSS使用编码为权重的显式规则，而非基于知识的CDSS使用学习到的函数。\n- 设输入为一个特征向量 $\\mathbf{x} \\in \\mathbb{R}^n$。\n- 将基于知识的决策分数定义为 $s_R(\\mathbf{x}) = \\langle \\mathbf{w}_R, \\mathbf{x} \\rangle - T_R$，其决策为 $\\mathrm{sign}(s_R(\\mathbf{x})) \\in \\{ -1, +1 \\}$，其中若 $u > 0$ 则 $\\mathrm{sign}(u) = +1$，否则为 $-1$。\n- 将机器学习决策分数（线性对数几率）定义为 $g_M(\\mathbf{x}) = b_M + \\langle \\mathbf{w}_M, \\mathbf{x} \\rangle$，其决策为 $\\mathrm{sign}(g_M(\\mathbf{x})) \\in \\{ -1, +1 \\}$。\n- 考虑满足 $L_\\infty$ 约束 $\\|\\boldsymbol{\\delta}\\|_\\infty \\le \\epsilon$ 的扰动 $\\boldsymbol{\\delta}$，其中 $\\epsilon \\ge 0$ 是一个以小数（非百分比）表示的幅度预算。您必须从第一性原理出发进行推理，计算在给定 $\\mathbf{x}$ 的情况下，能为每个模型引发决策翻转（决策分数的符号变化）的最小 $\\epsilon$。\n- 错误输入包括缺失特征值和超范围值。缺失值将由给定的基线向量 $\\boldsymbol{\\mu}$ 替换以形成 $\\mathbf{x}^{\\mathrm{miss}}$，超范围输入将被钳制在给定的逐特征区间 $[c_i^{\\min}, c_i^{\\max}]$ 内以形成 $\\mathbf{x}^{\\mathrm{clamp}}$，其中 $\\tilde{x}_i = \\min(\\max(x_i, c_i^{\\min}), c_i^{\\max})$。\n\n任务：\n1. 仅使用上述定义和基本事实，推导并实现一个计算，以求得在给定 $\\mathbf{x}$ 的情况下，翻转基于知识的CDSS决策所需的最小 $L_\\infty$ 扰动幅度 $\\epsilon^\\star$，以及针对非基于知识的CDSS的类似 $\\epsilon^\\star$。然后，对于给定的预算 $\\epsilon$，判断最坏情况下的对抗性扰动是否能翻转每个模型中的决策。\n2. 在以下错误输入情况下，实现决策变化的检测：\n   - 缺失特征：通过用 $\\boldsymbol{\\mu}$ 替换指定索引来构建 $\\mathbf{x}^{\\mathrm{miss}}$，然后计算误差后的分数 $s_R(\\mathbf{x}^{\\mathrm{miss}})$ 和 $g_M(\\mathbf{x}^{\\mathrm{miss}})$，并判断每个模型的决策是否相对于原始 $\\mathbf{x}$ 发生翻转。\n   - 超范围值：通过将每个分量钳制到 $[c_i^{\\min}, c_i^{\\max}]$ 来构建 $\\mathbf{x}^{\\mathrm{clamp}}$，然后计算 $s_R(\\mathbf{x}^{\\mathrm{clamp}})$ 和 $g_M(\\mathbf{x}^{\\mathrm{clamp}})$，并判断每个模型的决策是否相对于原始的 raw $\\mathbf{x}$ 发生翻转。\n3. 基于您所使用的数学原理，提出有原则的缓解策略，例如边界保障、范数感知正则化和输入验证。这些策略应在您的解决方案中进行概念性解释；程序计算这些策略将监控的指标和翻转指示符。\n\n参数化：\n- 特征维度 $n = 5$。\n- 基于知识的CDSS参数：$\\mathbf{w}_R = [0.8,\\,0.4,\\,-0.6,\\,0.3,\\,0.2]$，$T_R = 0.5$。\n- 非基于知识的CDSS参数：$\\mathbf{w}_M = [1.1,\\,-0.7,\\,0.5,\\,0.2,\\,0.0]$，$b_M = -0.5$。\n- 基线插补向量：$\\boldsymbol{\\mu} = [0.5,\\,0.5,\\,0.5,\\,0.5,\\,0.5]$。\n- 逐特征钳制边界：$\\mathbf{c}^{\\min} = [0.0,\\,0.0,\\,0.0,\\,0.0,\\,0.0]$，$\\mathbf{c}^{\\max} = [1.0,\\,1.0,\\,1.0,\\,1.0,\\,1.0]$。\n\n测试套件：\n为以下五个案例计算指定的输出：\n- 案例 1（正常路径噪声）：$\\mathbf{x} = [0.9,\\,0.1,\\,0.1,\\,0.6,\\,0.8]$，扰动预算 $\\epsilon = 0.05$（小数）。输出格式为 $[m_R,\\,m_M,\\,\\mathrm{flip}_R,\\,\\mathrm{flip}_M]$，其中 $m_R$ 和 $m_M$ 是在给定 $\\mathbf{x}$ 的情况下翻转相应模型决策所需的最小预算 $\\epsilon^\\star$，而 $\\mathrm{flip}_R$、$\\mathrm{flip}_M$ 是布尔值，指示在给定 $\\epsilon$ 下是否可能发生翻转。\n- 案例 2（边界噪声）：$\\mathbf{x} = [0.5,\\,0.5,\\,0.5,\\,0.5,\\,0.5]$，扰动预算 $\\epsilon = 0.05$（小数）。输出格式同案例 1。\n- 案例 3（对抗性目标噪声）：$\\mathbf{x} = [0.3,\\,0.9,\\,0.1,\\,0.2,\\,0.4]$，扰动预算 $\\epsilon = 0.2$（小数）。输出格式同案例 1。\n- 案例 4（缺失值错误）：$\\mathbf{x} = [0.7,\\,0.9,\\,0.2,\\,0.8,\\,0.1]$，缺失索引 $\\{1,\\,3\\}$ 用 $\\boldsymbol{\\mu}$ 替换。输出格式为 $[s_R(\\mathbf{x}^{\\mathrm{miss}}),\\,g_M(\\mathbf{x}^{\\mathrm{miss}}),\\,\\mathrm{flip}_R^{\\mathrm{miss}},\\,\\mathrm{flip}_M^{\\mathrm{miss}}]$，其中 $\\mathrm{flip}^{\\mathrm{miss}}$ 指示每个模型的决策是否相对于原始 $\\mathbf{x}$ 发生翻转。\n- 案例 5（超范围钳制错误）：原始 $\\mathbf{x} = [1.5,\\,-0.2,\\,0.8,\\,1.3,\\,0.4]$，被钳制到 $[c_i^{\\min}, c_i^{\\max}]$。输出格式为 $[s_R(\\mathbf{x}^{\\mathrm{clamp}}),\\,g_M(\\mathbf{x}^{\\mathrm{clamp}}),\\,\\mathrm{flip}_R^{\\mathrm{clamp}},\\,\\mathrm{flip}_M^{\\mathrm{clamp}}]$，其中 $\\mathrm{flip}^{\\mathrm{clamp}}$ 指示每个模型的决策是否相对于原始的 raw $\\mathbf{x}$ 发生翻转。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含五个案例的聚合结果，形式为一个逗号分隔的列表，并用方括号括起来，无空格，每个案例的结果本身也是一个列表，格式如上文各案例所述。例如，输出应类似于 $[\\,[\\cdots],[\\cdots],[\\cdots],[\\cdots],[\\cdots]\\,]$，但用数字和布尔值代替省略号。不涉及角度；没有物理单位；所有预算都是小数。输出必须是布尔或浮点类型，严格按照每个案例的规定。", "solution": "该问题要求对两种类型的临床决策支持系统（CDSS）对抗对抗性输入和错误输入的弹性进行形式化和评估。分析将基于线性代数和范数理论的第一性原理。\n\n这两个系统分别是：一个基于知识的CDSS，通过基于线性规则的分数进行建模；以及一个非基于知识的CDSS，通过学习到的线性分类器进行建模。\n\n基于知识的决策分数由以下公式给出：\n$$s_R(\\mathbf{x}) = \\langle \\mathbf{w}_R, \\mathbf{x} \\rangle - T_R$$\n非基于知识的机器学习决策分数（对数几率）为：\n$$g_M(\\mathbf{x}) = b_M + \\langle \\mathbf{w}_M, \\mathbf{x} \\rangle$$\n在这两种情况下，输入都是一个特征向量 $\\mathbf{x} \\in \\mathbb{R}^n$，二元决策由 $\\mathrm{sign}(\\text{score})$ 确定，问题定义 $\\mathrm{sign}(u) = +1$ 如果 $u > 0$，以及 $\\mathrm{sign}(u) = -1$ 如果 $u \\le 0$。如果输入被修改后分数的符号发生变化，则发生决策翻转。\n\n参数指定如下：\n- 特征维度：$n = 5$。\n- 基于知识的模型：$\\mathbf{w}_R = [0.8,\\,0.4,\\,-0.6,\\,0.3,\\,0.2]$ 和 $T_R = 0.5$。\n- 非基于知识的模型：$\\mathbf{w}_M = [1.1,\\,-0.7,\\,0.5,\\,0.2,\\,0.0]$ 和 $b_M = -0.5$。\n\n让我们整合分数函数。我们可以将两者都写成一般形式 $f(\\mathbf{x}) = \\langle \\mathbf{w}, \\mathbf{x} \\rangle + b$。\n对于基于规则的模型，$f(\\mathbf{x}) \\equiv s_R(\\mathbf{x})$，$\\mathbf{w} = \\mathbf{w}_R$，且 $b = -T_R = -0.5$。\n对于机器学习模型，$f(\\mathbf{x}) \\equiv g_M(\\mathbf{x})$，$\\mathbf{w} = \\mathbf{w}_M$，且 $b = b_M = -0.5$。\n\n### 任务 1：对$L_\\infty$对抗性扰动的弹性\n\n该任务要求推导能引发决策翻转的最小$L_\\infty$扰动幅度，记为 $\\epsilon^\\star$。对抗性输入的形式为 $\\mathbf{x}' = \\mathbf{x} + \\boldsymbol{\\delta}$，其中扰动 $\\boldsymbol{\\delta}$ 受 $\\|\\boldsymbol{\\delta}\\|_\\infty \\le \\epsilon$ 约束。$L_\\infty$范数定义为 $\\|\\boldsymbol{\\delta}\\|_\\infty = \\max_i |\\delta_i|$。\n\n如果 $\\mathrm{sign}(f(\\mathbf{x})) \\neq \\mathrm{sign}(f(\\mathbf{x} + \\boldsymbol{\\delta}))$，则发生决策翻转。假设 $f(\\mathbf{x}) \\neq 0$，能导致翻转的最小扰动是能将分数推至决策边界的扰动，即 $f(\\mathbf{x} + \\boldsymbol{\\delta}) = 0$。\n\n让我们分析分数函数的变化：\n$$f(\\mathbf{x} + \\boldsymbol{\\delta}) = \\langle \\mathbf{w}, \\mathbf{x} + \\boldsymbol{\\delta} \\rangle + b = (\\langle \\mathbf{w}, \\mathbf{x} \\rangle + b) + \\langle \\mathbf{w}, \\boldsymbol{\\delta} \\rangle = f(\\mathbf{x}) + \\langle \\mathbf{w}, \\boldsymbol{\\delta} \\rangle$$\n为了使分数变为零，由扰动引起的变化必须是 $\\langle \\mathbf{w}, \\boldsymbol{\\delta} \\rangle = -f(\\mathbf{x})$。\n\n我们需要找到能满足此等式且最小的 $\\epsilon = \\|\\boldsymbol{\\delta}\\|_\\infty$。我们必须确定在给定扰动范数 $\\|\\boldsymbol{\\delta}\\|_\\infty \\le \\epsilon$ 的情况下，变化 $|\\langle \\mathbf{w}, \\boldsymbol{\\delta} \\rangle|$ 的最大可能幅度。\n\n这是Hölder不等式的一个经典应用，该不等式指出，对于对偶范数（其中 $1/p+1/q=1$），$|\\langle \\mathbf{u}, \\mathbf{v} \\rangle| \\le \\|\\mathbf{u}\\|_p \\|\\mathbf{v}\\|_q$。$L_\\infty$范数的对偶范数是$L_1$范数。因此，当 $p=\\infty$ 和 $q=1$ 时：\n$$|\\langle \\mathbf{w}, \\boldsymbol{\\delta} \\rangle| \\le \\|\\mathbf{w}\\|_1 \\|\\boldsymbol{\\delta}\\|_\\infty$$\n权重向量的$L_1$范数是 $\\|\\mathbf{w}\\|_1 = \\sum_i |w_i|$。\n因此，对于给定的预算 $\\epsilon$，分数的最大可能变化是 $\\epsilon \\|\\mathbf{w}\\|_1$。这个最大值是通过一个与权重“对齐”的“最坏情况”扰动来实现的：一个扰动 $\\boldsymbol{\\delta}$，其每个分量 $\\delta_i$ 都是 $\\pm\\epsilon$，符号的选择旨在最大化内积的幅度。具体来说，为了使 $\\langle \\mathbf{w}, \\boldsymbol{\\delta} \\rangle$ 尽可能为负，可以选择 $\\delta_i = -\\epsilon \\cdot \\mathrm{sign}(w_i)$。\n\n要翻转决策，可达到的变化必须至少与当前分数的绝对值 $|f(\\mathbf{x})|$ 一样大。\n$$\\epsilon \\|\\mathbf{w}\\|_1 \\ge |f(\\mathbf{x})|$$\n因此，能够实现翻转的最小扰动幅度 $\\epsilon^\\star$ 是满足此等式关系的值：\n$$\\epsilon^\\star = \\frac{|f(\\mathbf{x})|}{\\|\\mathbf{w}\\|_1}$$\n这提供了一个直接的鲁棒性度量：$\\epsilon^\\star$ 越大，意味着系统对$L_\\infty$攻击的弹性越强。\n\n对于这两个模型，我们有：\n基于知识的模型的最小扰动：\n$$\\epsilon^\\star_R = \\frac{|s_R(\\mathbf{x})|}{\\|\\mathbf{w}_R\\|_1} = \\frac{|\\langle \\mathbf{w}_R, \\mathbf{x} \\rangle - T_R|}{\\sum_i |w_{R,i}|}$$\n非基于知识的模型的最小扰动：\n$$\\epsilon^\\star_M = \\frac{|g_M(\\mathbf{x})|}{\\|\\mathbf{w}_M\\|_1} = \\frac{|b_M + \\langle \\mathbf{w}_M, \\mathbf{x} \\rangle|}{\\sum_i |w_{M,i}|}$$\n对于给定的扰动预算 $\\epsilon$，当且仅当 $\\epsilon \\ge \\epsilon^\\star$ 时，才可能发生翻转。\n\n权重向量的$L_1$范数是：\n$$\\|\\mathbf{w}_R\\|_1 = |0.8| + |0.4| + |-0.6| + |0.3| + |0.2| = 2.3$$\n$$\\|\\mathbf{w}_M\\|_1 = |1.1| + |-0.7| + |0.5| + |0.2| + |0.0| = 2.5$$\n\n### 任务 2：对错误输入的弹性\n\n该任务涉及评估两种常见数据质量问题的影响：缺失值和超范围值。\n\n**缺失特征：**\n当特征缺失时，它们将被基线向量 $\\boldsymbol{\\mu}$ 中的值替换。给定原始输入 $\\mathbf{x}$ 和一组缺失特征的索引 $I_{\\mathrm{miss}}$，构建一个新向量 $\\mathbf{x}^{\\mathrm{miss}}$，使得：\n$$x^{\\mathrm{miss}}_i =\n\\begin{cases}\n\\mu_i  \\text{if } i \\in I_{\\mathrm{miss}} \\\\\nx_i  \\text{if } i \\notin I_{\\mathrm{miss}}\n\\end{cases}$$\n然后我们计算原始分数 $s_R(\\mathbf{x})$ 和 $g_M(\\mathbf{x})$，以及插补后分数 $s_R(\\mathbf{x}^{\\mathrm{miss}})$ 和 $g_M(\\mathbf{x}^{\\mathrm{miss}})$。如果 $\\mathrm{sign}(\\text{score}(\\mathbf{x})) \\neq \\mathrm{sign}(\\text{score}(\\mathbf{x}^{\\mathrm{miss}}))$，则检测到每个模型的决策翻转。\n\n**超范围值：**\n特征值可能被记录在其预期或有效范围之外。指定的缓解措施是为每个特征 $i$ 将它们钳制在预定义的区间 $[c_i^{\\min}, c_i^{\\max}]$ 内。给定一个原始输入向量 $\\mathbf{x}_{\\mathrm{raw}}$，钳制后的向量 $\\mathbf{x}^{\\mathrm{clamp}}$ 构建如下：\n$$x^{\\mathrm{clamp}}_i = \\min(\\max(x_{\\mathrm{raw},i}, c_i^{\\min}), c_i^{\\max})$$\n与缺失值情况类似，我们计算原始向量 $\\text{score}(\\mathbf{x}_{\\mathrm{raw}})$ 和钳制后向量 $\\text{score}(\\mathbf{x}^{\\mathrm{clamp}})$ 的分数。如果它们的符号不同，则记录为决策翻转。\n\n### 任务 3：有原则的缓解策略\n\n上述数学推导为提高模型弹性提出了具体的策略。\n\n1.  **边界保障：** 鲁棒性度量 $\\epsilon^\\star = |f(\\mathbf{x})| / \\|\\mathbf{w}\\|_1$ 与分数的绝对值 $|f(\\mathbf{x})|$ 成正比。该值代表了点 $\\mathbf{x}$ 到决策超平面的几何距离。靠近边界的点（$|f(\\mathbf{x})|$ 较小）天生鲁棒性较差。一个实用的缓解措施是建立一个“置信边界”。如果 $|f(\\mathbf{x})|$ 低于某个安全阈值，系统可以标记该决策为低置信度，并建议人工审查。这是机器学习中的一个核心概念，特别是在支持向量机（SVM）中，其设计目的就是最大化这个边界。\n\n2.  **范数感知正则化：** 鲁棒性 $\\epsilon^\\star$ 与 $\\|\\mathbf{w}\\|_1$ 成反比。在训练机器学习模型（非基于知识的CDSS）期间，可以在损失函数中加入一个惩罚项，以抑制大的权重。具体来说，增加一个与 $\\|\\mathbf{w}\\|_1$ 成正比的项（称为 $L_1$ 或 Lasso 正则化）将引导学习算法找到具有较小 $L_1$ 范数的权重向量。这不仅提高了对 $L_\\infty$ 扰动的鲁棒性，而且通常还能促进稀疏性（许多权重变为零），从而产生更具可解释性的模型。\n\n3.  **输入验证和清理：** 对错误输入的分析表明了鲁棒数据预处理的重要性。\n    -   **钳制：** 钳制机制本身就是一种缓解策略，可以防止极端异常值对线性分数产生不成比例的影响。\n    -   **插补策略：** 插补向量 $\\boldsymbol{\\mu}$ 的选择至关重要。使用简单的基线或均值可能不是最优的。更复杂的插补技术，如k-近邻（k-NN）插补或基于模型的插补，可能会为缺失值提供更可靠的估计。无论使用何种方法，一个关键的缓解策略是跟踪哪些输入已被插补，并为基于这些输入的决策赋予更高程度的不确定性。\n\n这些策略展示了一种构建更具弹性的CDSS的有原则的方法，其中弹性不是事后添加的，而是被整合到模型的设计和操作协议中，并以对其脆弱性的严格数学理解为基础。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the CDSS resilience problem by calculating metrics for five test cases.\n    \"\"\"\n\n    # --- Problem Parameters ---\n    w_R = np.array([0.8, 0.4, -0.6, 0.3, 0.2])\n    T_R = 0.5\n    w_M = np.array([1.1, -0.7, 0.5, 0.2, 0.0])\n    b_M = -0.5\n    mu = np.array([0.5, 0.5, 0.5, 0.5, 0.5])\n    c_min = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n    c_max = np.array([1.0, 1.0, 1.0, 1.0, 1.0])\n\n    # --- Helper Functions ---\n    def s_R(x):\n        return np.dot(w_R, x) - T_R\n\n    def g_M(x):\n        return np.dot(w_M, x) + b_M\n\n    # Sign function as defined in the problem: sign(u) = +1 if u > 0, -1 otherwise.\n    def problem_sign(u):\n        return 1 if u > 0 else -1\n\n    # --- Case-by-Case Computation ---\n    results = []\n\n    # Cases 1, 2, 3: Adversarial Perturbation Resilience\n    def compute_adversarial_resilience(x_val, epsilon):\n        # Calculate L1 norms of weight vectors\n        norm_w_R_l1 = np.linalg.norm(w_R, ord=1)\n        norm_w_M_l1 = np.linalg.norm(w_M, ord=1)\n\n        # Calculate initial scores\n        score_R = s_R(x_val)\n        score_M = g_M(x_val)\n\n        # Calculate minimal epsilon to flip\n        m_R = np.abs(score_R) / norm_w_R_l1\n        m_M = np.abs(score_M) / norm_w_M_l1\n\n        # Check if given epsilon is sufficient to flip\n        flip_R = epsilon >= m_R\n        flip_M = epsilon >= m_M\n\n        return [m_R, m_M, flip_R, flip_M]\n\n    # Case 1\n    x1 = np.array([0.9, 0.1, 0.1, 0.6, 0.8])\n    eps1 = 0.05\n    results.append(compute_adversarial_resilience(x1, eps1))\n\n    # Case 2\n    x2 = np.array([0.5, 0.5, 0.5, 0.5, 0.5])\n    eps2 = 0.05\n    results.append(compute_adversarial_resilience(x2, eps2))\n\n    # Case 3\n    x3 = np.array([0.3, 0.9, 0.1, 0.2, 0.4])\n    eps3 = 0.2\n    results.append(compute_adversarial_resilience(x3, eps3))\n\n    # Case 4: Missing Values Error\n    def compute_missing_value_impact(x_orig, missing_indices):\n        # Calculate original scores and signs\n        score_R_orig = s_R(x_orig)\n        score_M_orig = g_M(x_orig)\n        sign_R_orig = problem_sign(score_R_orig)\n        sign_M_orig = problem_sign(score_M_orig)\n\n        # Construct vector with missing values imputed\n        x_miss = x_orig.copy()\n        # In Python, set indices {1, 3} are 0-based\n        for i in missing_indices:\n            x_miss[i] = mu[i]\n        \n        # Calculate new scores and signs\n        score_R_miss = s_R(x_miss)\n        score_M_miss = g_M(x_miss)\n        sign_R_miss = problem_sign(score_R_miss)\n        sign_M_miss = problem_sign(score_M_miss)\n\n        # Check for flips\n        flip_R_miss = sign_R_orig != sign_R_miss\n        flip_M_miss = sign_M_orig != sign_M_miss\n\n        return [score_R_miss, score_M_miss, flip_R_miss, flip_M_miss]\n\n    x4 = np.array([0.7, 0.9, 0.2, 0.8, 0.1])\n    missing_idx4 = {1, 3} \n    results.append(compute_missing_value_impact(x4, missing_idx4))\n\n    # Case 5: Out-of-Range Clamping Error\n    def compute_clamping_impact(x_raw):\n        # Calculate scores and signs on raw data\n        score_R_raw = s_R(x_raw)\n        score_M_raw = g_M(x_raw)\n        sign_R_raw = problem_sign(score_R_raw)\n        sign_M_raw = problem_sign(score_M_raw)\n\n        # Construct clamped vector\n        x_clamp = np.clip(x_raw, c_min, c_max)\n\n        # Calculate new scores and signs\n        score_R_clamp = s_R(x_clamp)\n        score_M_clamp = g_M(x_clamp)\n        sign_R_clamp = problem_sign(score_R_clamp)\n        sign_M_clamp = problem_sign(score_M_clamp)\n\n        # Check for flips\n        flip_R_clamp = sign_R_raw != sign_R_clamp\n        flip_M_clamp = sign_M_raw != sign_M_clamp\n\n        return [score_R_clamp, score_M_clamp, flip_R_clamp, flip_M_clamp]\n\n    x5_raw = np.array([1.5, -0.2, 0.8, 1.3, 0.4])\n    results.append(compute_clamping_impact(x5_raw))\n\n    # Format the final output string as specified\n    output_str = str(results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```", "id": "4846799"}]}