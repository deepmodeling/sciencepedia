{"hands_on_practices": [{"introduction": "理论知识是基础，但真正的理解来自于实践。在本节中，我们将通过一系列动手实践来巩固和深化我们对临床结果预测建模的理解。这些练习旨在模拟您在实际工作中可能遇到的挑战，从严谨的研究设计到复杂的模型构建和决策优化。\n\n我们的第一个实践聚焦于预测建模的基石：正确的研究设计。在观察性研究中，一个被称为“不朽时间偏见”（immortal time bias）的微妙陷阱常常会扭曲风险评估的结果。这个练习将引导您识别并纠正这种偏见，通过重新设计一个基于实验室检测的队列研究，您将亲身体会到选择正确的时间起点（time-zero）对于获得无偏估计是多么重要 [@problem_id:4853264]。", "problem": "给定一个围绕某项实验室检验组建的队列，该队列最初以检验结果时间为索引时间，这无意中引入了不朽时间偏倚。请重新设计该队列，以检验开单时间为索引时间，并重新计算结局风险窗口，然后比较朴素设计（以结果时间为索引）与校正设计（以开单时间为索引）。目标是计算一个固定长度结局窗口的风险，并量化由不朽时间所导致的差异。\n\n基于事件时间分析的基本定义：\n1. 对于由 $i \\in \\{1,\\dots,n\\}$ 索引的每个个体，您可以观察到检验开单时间 $t_i^{\\mathrm{ord}} \\in \\mathbb{R}_{\\ge 0}$、处理延迟 $d_i \\in \\mathbb{R}_{\\ge 0}$、检验结果时间 $t_i^{\\mathrm{res}} = t_i^{\\mathrm{ord}} + d_i$、结局时间 $t_i^{\\mathrm{out}} \\in \\mathbb{R}_{\\ge 0} \\cup \\{+\\infty\\}$ 以及删失时间 $t_i^{\\mathrm{cens}} \\in \\mathbb{R}_{\\ge 0} \\cup \\{+\\infty\\}$。所有时间均在以天为单位的同一时间轴上测量，其中 $+\\infty$ 表示在观察期内未发生相应事件。\n2. 定义一个索引时间函数 $t_i^{\\mathrm{idx},s}$，该函数依赖于方案 $s \\in \\{\\mathrm{naive}, \\mathrm{corr}\\}$，具体为 $t_i^{\\mathrm{idx},\\mathrm{naive}} := t_i^{\\mathrm{res}}$ 和 $t_i^{\\mathrm{idx},\\mathrm{corr}} := t_i^{\\mathrm{ord}}$。\n3. 对于一个固定的窗口长度 $W \\in \\mathbb{R}_{> 0}$（以天为单位），为方案 $s$ 定义风险集纳入规则如下：个体 $i$ 被纳入当且仅当 $t_i^{\\mathrm{cens}} \\ge t_i^{\\mathrm{idx},s}$ 且 $t_i^{\\mathrm{out}} \\ge t_i^{\\mathrm{idx},s}$。此规则编码了在索引时间点无事件发生且未被删失的状态。\n4. 对于被纳入的个体，定义结局指标 $y_i^{(s)}$ 为：若 $t_i^{\\mathrm{out}} \\in [\\,t_i^{\\mathrm{idx},s}, \\min\\{t_i^{\\mathrm{idx},s} + W, t_i^{\\mathrm{cens}}\\}\\,]$，则 $y_i^{(s)} = 1$，否则 $y_i^{(s)} = 0$。区间端点为闭合区间。\n5. 将方案 $s$ 的风险定义为 $R^{(s)} = \\left(\\sum_{i \\in \\mathcal{I}_s} y_i^{(s)}\\right) / \\left|\\mathcal{I}_s\\right|$，其中 $\\mathcal{I}_s$ 是在方案 $s$ 下被纳入的个体集合。对于每个数据集，关注的量为 $(R^{(\\mathrm{naive})}, R^{(\\mathrm{corr})}, R^{(\\mathrm{corr})} - R^{(\\mathrm{naive})})$。\n\n所有时间值均以天为单位；输出的风险值必须以小数形式报告（而非百分比），并四舍五入到小数点后六位。\n\n请实现一个程序，为下方的每个测试用例，使用上述定义计算三元组 $(R^{(\\mathrm{naive})}, R^{(\\mathrm{corr})}, R^{(\\mathrm{corr})} - R^{(\\mathrm{naive})})$，并将所有结果以逗号分隔的列表形式输出到一行，并用方括号括起来。\n\n测试套件（所有时间单位为天；在说明处使用 $+\\infty$）：\n- 测试用例 1，窗口 $W = 7$，个体数据如下：\n  - $i_1$：$t^{\\mathrm{ord}} = 0$，$d = 2$ 因此 $t^{\\mathrm{res}} = 2$，$t^{\\mathrm{out}} = 5$，$t^{\\mathrm{cens}} = +\\infty$。\n  - $i_2$：$t^{\\mathrm{ord}} = 1$，$d = 3$ 因此 $t^{\\mathrm{res}} = 4$，$t^{\\mathrm{out}} = 3$，$t^{\\mathrm{cens}} = +\\infty$。\n  - $i_3$：$t^{\\mathrm{ord}} = 4$，$d = 1$ 因此 $t^{\\mathrm{res}} = 5$，$t^{\\mathrm{out}} = +\\infty$，$t^{\\mathrm{cens}} = 10$。\n  - $i_4$：$t^{\\mathrm{ord}} = 2$，$d = 2$ 因此 $t^{\\mathrm{res}} = 4$，$t^{\\mathrm{out}} = 12$，$t^{\\mathrm{cens}} = +\\infty$。\n  - $i_5$：$t^{\\mathrm{ord}} = 3$，$d = 7$ 因此 $t^{\\mathrm{res}} = 10$，$t^{\\mathrm{out}} = 8$，$t^{\\mathrm{cens}} = +\\infty$。\n  - $i_6$：$t^{\\mathrm{ord}} = 6$，$d = 2$ 因此 $t^{\\mathrm{res}} = 8$，$t^{\\mathrm{out}} = +\\infty$，$t^{\\mathrm{cens}} = 7$。\n- 测试用例 2，窗口 $W = 3$，个体数据如下：\n  - $i_1$：$t^{\\mathrm{ord}} = 0$，$d = 0$ 因此 $t^{\\mathrm{res}} = 0$，$t^{\\mathrm{out}} = 0$，$t^{\\mathrm{cens}} = +\\infty$。\n  - $i_2$：$t^{\\mathrm{ord}} = 5$，$d = 2$ 因此 $t^{\\mathrm{res}} = 7$，$t^{\\mathrm{out}} = 8$，$t^{\\mathrm{cens}} = +\\infty$。\n  - $i_3$：$t^{\\mathrm{ord}} = 1$，$d = 3$ 因此 $t^{\\mathrm{res}} = 4$，$t^{\\mathrm{out}} = 2$，$t^{\\mathrm{cens}} = +\\infty$。\n  - $i_4$：$t^{\\mathrm{ord}} = 3$，$d = 1$ 因此 $t^{\\mathrm{res}} = 4$，$t^{\\mathrm{out}} = +\\infty$，$t^{\\mathrm{cens}} = 4$。\n  - $i_5$：$t^{\\mathrm{ord}} = 2$，$d = 5$ 因此 $t^{\\mathrm{res}} = 7$，$t^{\\mathrm{out}} = 1$，$t^{\\mathrm{cens}} = +\\infty$。\n  - $i_6$：$t^{\\mathrm{ord}} = 6$，$d = 0$ 因此 $t^{\\mathrm{res}} = 6$，$t^{\\mathrm{out}} = +\\infty$，$t^{\\mathrm{cens}} = 6$。\n- 测试用例 3，窗口 $W = 5$，个体数据如下：\n  - $i_1$：$t^{\\mathrm{ord}} = 0$，$d = 4$ 因此 $t^{\\mathrm{res}} = 4$，$t^{\\mathrm{out}} = 3$，$t^{\\mathrm{cens}} = +\\infty$。\n  - $i_2$：$t^{\\mathrm{ord}} = 2$，$d = 1$ 因此 $t^{\\mathrm{res}} = 3$，$t^{\\mathrm{out}} = +\\infty$，$t^{\\mathrm{cens}} = 2$。\n  - $i_3$：$t^{\\mathrm{ord}} = 5$，$d = 10$ 因此 $t^{\\mathrm{res}} = 15$，$t^{\\mathrm{out}} = +\\infty$，$t^{\\mathrm{cens}} = 7$。\n  - $i_4$：$t^{\\mathrm{ord}} = 8$，$d = 0$ 因此 $t^{\\mathrm{res}} = 8$，$t^{\\mathrm{out}} = 12$，$t^{\\mathrm{cens}} = +\\infty$。\n  - $i_5$：$t^{\\mathrm{ord}} = 1$，$d = 2$ 因此 $t^{\\mathrm{res}} = 3$，$t^{\\mathrm{out}} = 20$，$t^{\\mathrm{cens}} = +\\infty$。\n  - $i_6$：$t^{\\mathrm{ord}} = 9$，$d = 1$ 因此 $t^{\\mathrm{res}} = 10$，$t^{\\mathrm{out}} = +\\infty$，$t^{\\mathrm{cens}} = +\\infty$。\n\n您的程序必须：\n- 精确实现上述定义。\n- 对每个测试用例，计算 $(R^{(\\mathrm{naive})}, R^{(\\mathrm{corr})}, R^{(\\mathrm{corr})} - R^{(\\mathrm{naive})})$。\n- 生成一行输出，包含三个测试用例的全部九个数字，按顺序排列，四舍五入至小数点后六位，格式为逗号分隔的列表并用方括号括起。格式如下：\n  $[r_{1,\\mathrm{naive}}, r_{1,\\mathrm{corr}}, r_{1,\\mathrm{diff}}, r_{2,\\mathrm{naive}}, r_{2,\\mathrm{corr}}, r_{2,\\mathrm{diff}}, r_{3,\\mathrm{naive}}, r_{3,\\mathrm{corr}}, r_{3,\\mathrm{diff}}]$ 其中 $r_{k,\\mathrm{diff}} = r_{k,\\mathrm{corr}} - r_{k,\\mathrm{naive}}$ 对于测试用例 $k \\in \\{1,2,3\\}$。", "solution": "该问题陈述在形式上是明确的，科学上基于事件时间分析的原则，并且计算上是可行的。它提供了一套清晰、自洽的定义和数据，从而能够得到唯一且可验证的解。该问题探讨了不朽时间偏倚的概念，这是观察性研究和临床预测模型中的一个关键问题，其中不恰当的索引时间（零点时间）选择可能导致对风险的估计出现偏倚。该问题是有效的。\n\n核心任务是比较两种队列构建方案，用以估计在一次实验室检验后固定时间窗口 $W$ 内发生某一结局的风险。关键区别在于索引时间 $t^{\\mathrm{idx}}$ 的定义，所有随访均从该时间点开始测量。\n\n1.  **朴素方案 ($s = \\mathrm{naive}$)**：索引时间为检验结果时间，$t_i^{\\mathrm{idx},\\mathrm{naive}} = t_i^{\\mathrm{res}}$。这种设计很常见但存在缺陷。对于要被纳入队列的个体 $i$，他们必须在 $t_i^{\\mathrm{res}}$ 时刻仍然存活且未被删失。从开具检验 ($t_i^{\\mathrm{ord}}$) 到收到结果 ($t_i^{\\mathrm{res}}$) 之间的时间间隔，即处理延迟 $d_i$，成为了“不朽时间”。根据定义，对于被纳入队列的个体，在此期间不可能观察到不良结局，因为任何在此期间经历结局或被删失的人都已被排除在分析之外。这系统性地移除了早期事件，从而导致对真实风险的低估。\n\n2.  **校正方案 ($s = \\mathrm{corr}$)**：索引时间为检验开单时间，$t_i^{\\mathrm{idx},\\mathrm{corr}} = t_i^{\\mathrm{ord}}$。这在概念上是正确的方法。检验被开具的时刻，即是临床决策过程开始的时刻，患者也从那一刻起正式就该检验将提供的信息而言“处于风险中”。这种设计正确地允许事件和删失在开单时间后的任何时刻发生，包括在处理延迟期间，从而提供了对风险的无偏估计。\n\n每种方案 $s$ 的风险，记作 $R^{(s)}$，计算方法为在指定风险窗口内经历结局的个体数量与该方案下风险集中所包含的总个体数量之比。计算过程通过应用所提供的形式化定义来进行。我们用 $\\infty$ 表示 $+\\infty$。\n\n### 测试用例 1：$W=7$\n\n- 个体数据：\n  - $i_1$：$t^{\\mathrm{ord}} = 0$，$d = 2$，$t^{\\mathrm{res}} = 2$，$t^{\\mathrm{out}} = 5$，$t^{\\mathrm{cens}} = \\infty$。\n  - $i_2$：$t^{\\mathrm{ord}} = 1$，$d = 3$，$t^{\\mathrm{res}} = 4$，$t^{\\mathrm{out}} = 3$，$t^{\\mathrm{cens}} = \\infty$。\n  - $i_3$：$t^{\\mathrm{ord}} = 4$，$d = 1$，$t^{\\mathrm{res}} = 5$，$t^{\\mathrm{out}} = \\infty$，$t^{\\mathrm{cens}} = 10$。\n  - $i_4$：$t^{\\mathrm{ord}} = 2$，$d = 2$，$t^{\\mathrm{res}} = 4$，$t^{\\mathrm{out}} = 12$，$t^{\\mathrm{cens}} = \\infty$。\n  - $i_5$：$t^{\\mathrm{ord}} = 3$，$d = 7$，$t^{\\mathrm{res}} = 10$，$t^{\\mathrm{out}} = 8$，$t^{\\mathrm{cens}} = \\infty$。\n  - $i_6$：$t^{\\mathrm{ord}} = 6$，$d = 2$，$t^{\\mathrm{res}} = 8$，$t^{\\mathrm{out}} = \\infty$，$t^{\\mathrm{cens}} = 7$。\n\n#### 朴素方案 ($t^{\\mathrm{idx}} = t^{\\mathrm{res}}$)\n个体 $i$ 被纳入，如果 $t_i^{\\mathrm{cens}} \\ge t_i^{\\mathrm{res}}$ 且 $t_i^{\\mathrm{out}} \\ge t_i^{\\mathrm{res}}$。\n- $i_1$：$t^{\\mathrm{res}}=2$。纳入（$\\infty \\ge 2$，$5 \\ge 2$）。\n- $i_2$：$t^{\\mathrm{res}}=4$。排除（$t^{\\mathrm{out}}=3  4$）。\n- $i_3$：$t^{\\mathrm{res}}=5$。纳入（$10 \\ge 5$，$\\infty \\ge 5$）。\n- $i_4$：$t^{\\mathrm{res}}=4$。纳入（$\\infty \\ge 4$，$12 \\ge 4$）。\n- $i_5$：$t^{\\mathrm{res}}=10$。排除（$t^{\\mathrm{out}}=8  10$）。\n- $i_6$：$t^{\\mathrm{res}}=8$。排除（$t^{\\mathrm{cens}}=7  8$）。\n\n被纳入的个体集合为 $\\mathcal{I}_{\\mathrm{naive}} = \\{i_1, i_3, i_4\\}$，所以 $|\\mathcal{I}_{\\mathrm{naive}}| = 3$。\n结局指标 $y_i^{(s)}=1$ 如果 $t_i^{\\mathrm{out}} \\in [t_i^{\\mathrm{idx},s}, \\min\\{t_i^{\\mathrm{idx},s} + W, t_i^{\\mathrm{cens}}\\}]$。\n- $i_1$：$t^{\\mathrm{idx}}=2$。窗口为 $[2, \\min\\{2+7, \\infty\\}]=[2, 9]$。$t^{\\mathrm{out}}=5 \\in [2, 9]$，因此 $y_1=1$。\n- $i_3$：$t^{\\mathrm{idx}}=5$。窗口为 $[5, \\min\\{5+7, 10\\}]=[5, 10]$。$t^{\\mathrm{out}}=\\infty \\notin [5, 10]$，因此 $y_3=0$。\n- $i_4$：$t^{\\mathrm{idx}}=4$。窗口为 $[4, \\min\\{4+7, \\infty\\}]=[4, 11]$。$t^{\\mathrm{out}}=12 \\notin [4, 11]$，因此 $y_4=0$。\n\n结局总数为 1。风险为 $R^{(\\mathrm{naive})} = 1/3 \\approx 0.333333$。\n\n#### 校正方案 ($t^{\\mathrm{idx}} = t^{\\mathrm{ord}}$)\n个体 $i$ 被纳入，如果 $t_i^{\\mathrm{cens}} \\ge t_i^{\\mathrm{ord}}$ 且 $t_i^{\\mathrm{out}} \\ge t_i^{\\mathrm{ord}}$。\n- $i_1$：$t^{\\mathrm{ord}}=0$。纳入（$\\infty \\ge 0$，$5 \\ge 0$）。\n- $i_2$：$t^{\\mathrm{ord}}=1$。纳入（$\\infty \\ge 1$，$3 \\ge 1$）。\n- $i_3$：$t^{\\mathrm{ord}}=4$。纳入（$10 \\ge 4$，$\\infty \\ge 4$）。\n- $i_4$：$t^{\\mathrm{ord}}=2$。纳入（$\\infty \\ge 2$，$12 \\ge 2$）。\n- $i_5$：$t^{\\mathrm{ord}}=3$。纳入（$\\infty \\ge 3$，$8 \\ge 3$）。\n- $i_6$：$t^{\\mathrm{ord}}=6$。纳入（$7 \\ge 6$，$\\infty \\ge 6$）。\n\n所有 6 个个体均被纳入，$\\mathcal{I}_{\\mathrm{corr}} = \\{i_1, i_2, i_3, i_4, i_5, i_6\\}$，所以 $|\\mathcal{I}_{\\mathrm{corr}}| = 6$。\n- $i_1$：$t^{\\mathrm{idx}}=0$。窗口为 $[0, 7]$。$t^{\\mathrm{out}}=5 \\in [0, 7]$，$y_1=1$。\n- $i_2$：$t^{\\mathrm{idx}}=1$。窗口为 $[1, 8]$。$t^{\\mathrm{out}}=3 \\in [1, 8]$，$y_2=1$。\n- $i_3$：$t^{\\mathrm{idx}}=4$。窗口为 $[4, 10]$。$t^{\\mathrm{out}}=\\infty \\notin [4, 10]$，$y_3=0$。\n- $i_4$：$t^{\\mathrm{idx}}=2$。窗口为 $[2, 9]$。$t^{\\mathrm{out}}=12 \\notin [2, 9]$，$y_4=0$。\n- $i_5$：$t^{\\mathrm{idx}}=3$。窗口为 $[3, 10]$。$t^{\\mathrm{out}}=8 \\in [3, 10]$，$y_5=1$。\n- $i_6$：$t^{\\mathrm{idx}}=6$。窗口为 $[6, \\min\\{6+7, 7\\}]=[6, 7]$。$t^{\\mathrm{out}}=\\infty \\notin [6, 7]$，$y_6=0$。\n\n结局总数为 $1+1+0+0+1+0 = 3$。风险为 $R^{(\\mathrm{corr})} = 3/6 = 0.5$。\n结果三元组为 $(0.333333, 0.500000, 0.166667)$。\n\n### 测试用例 2：$W=3$\n遵循相同的步骤：\n- **朴素方案**：纳入集合 $\\mathcal{I}_{\\mathrm{naive}} = \\{i_1, i_2, i_4, i_6\\}$， $|\\mathcal{I}_{\\mathrm{naive}}| = 4$。结局：$y_1=1, y_2=1, y_4=0, y_6=0$。风险 $R^{(\\mathrm{naive})} = 2/4 = 0.5$。\n- **校正方案**：纳入集合 $\\mathcal{I}_{\\mathrm{corr}} = \\{i_1, i_2, i_3, i_4, i_6\\}$， $|\\mathcal{I}_{\\mathrm{corr}}| = 5$。结局：$y_1=1, y_2=1, y_3=1, y_4=0, y_6=0$。风险 $R^{(\\mathrm{corr})} = 3/5 = 0.6$。\n- 结果三元组为 $(0.500000, 0.600000, 0.100000)$。\n\n### 测试用例 3：$W=5$\n遵循相同的步骤：\n- **朴素方案**：纳入集合 $\\mathcal{I}_{\\mathrm{naive}} = \\{i_4, i_5, i_6\\}$， $|\\mathcal{I}_{\\mathrm{naive}}| = 3$。结局：$y_4=1, y_5=0, y_6=0$。风险 $R^{(\\mathrm{naive})} = 1/3 \\approx 0.333333$。\n- **校正方案**：纳入集合 $\\mathcal{I}_{\\mathrm{corr}} = \\{i_1, i_2, i_3, i_4, i_5, i_6\\}$， $|\\mathcal{I}_{\\mathrm{corr}}| = 6$。结局：$y_1=1, y_2=0, y_3=0, y_4=1, y_5=0, y_6=0$。风险 $R^{(\\mathrm{corr})} = 2/6 = 1/3 \\approx 0.333333$。\n- 结果三元组为 $(0.333333, 0.333333, 0.000000)$。在这个例子中，队列构建中的偏倚恰好被分母和分子的变化所抵消，偶然导致风险估计值没有净变化。\n\n全部计算过程验证了将在程序中实现的逻辑和结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the immortal time bias problem for the given test cases.\n    \"\"\"\n    \n    # Use np.inf to represent infinity for time values.\n    inf = np.inf\n\n    test_cases = [\n        (7, [\n            (0, 2, 5, inf),   # i_1\n            (1, 3, 3, inf),   # i_2\n            (4, 1, inf, 10),  # i_3\n            (2, 2, 12, inf),  # i_4\n            (3, 7, 8, inf),   # i_5\n            (6, 2, inf, 7)    # i_6\n        ]),\n        (3, [\n            (0, 0, 0, inf),   # i_1\n            (5, 2, 8, inf),   # i_2\n            (1, 3, 2, inf),   # i_3\n            (3, 1, inf, 4),   # i_4\n            (2, 5, 1, inf),   # i_5\n            (6, 0, inf, 6)    # i_6\n        ]),\n        (5, [\n            (0, 4, 3, inf),   # i_1\n            (2, 1, inf, 2),   # i_2\n            (5, 10, inf, 7),  # i_3\n            (8, 0, 12, inf),  # i_4\n            (1, 2, 20, inf),  # i_5\n            (9, 1, inf, inf)  # i_6\n        ])\n    ]\n\n    all_results = []\n\n    for W, individuals in test_cases:\n        # --- Naive Schema Calculation ---\n        included_naive_count = 0\n        outcome_naive_count = 0\n        for ind in individuals:\n            t_ord, d, t_out, t_cens = ind\n            t_res = t_ord + d\n            \n            # Inclusion rule: individual is event-free and uncensored at index time\n            if t_cens >= t_res and t_out >= t_res:\n                included_naive_count += 1\n                \n                # Outcome rule: outcome occurs within the risk window\n                t_idx_naive = t_res\n                risk_window_end = min(t_idx_naive + W, t_cens)\n                if t_idx_naive = t_out = risk_window_end:\n                    outcome_naive_count += 1\n        \n        r_naive = outcome_naive_count / included_naive_count if included_naive_count > 0 else 0.0\n\n        # --- Corrected Schema Calculation ---\n        included_corr_count = 0\n        outcome_corr_count = 0\n        for ind in individuals:\n            t_ord, d, t_out, t_cens = ind\n            \n            # Inclusion rule\n            if t_cens >= t_ord and t_out >= t_ord:\n                included_corr_count += 1\n                \n                # Outcome rule\n                t_idx_corr = t_ord\n                risk_window_end = min(t_idx_corr + W, t_cens)\n                if t_idx_corr = t_out = risk_window_end:\n                    outcome_corr_count += 1\n                    \n        r_corr = outcome_corr_count / included_corr_count if included_corr_count > 0 else 0.0\n        \n        r_diff = r_corr - r_naive\n        \n        all_results.extend([r_naive, r_corr, r_diff])\n    \n    # Format results to six decimal places for the final output string\n    formatted_results = [f\"{round(r, 6):.6f}\" for r in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4853264"}, {"introduction": "在确保了研究设计的严谨性之后，我们进入了预测建模的核心环节：模型构建。虽然现成的软件包可以让我们轻松地训练模型，但真正掌握其工作原理需要我们深入其内部机制。这个练习将指导您从零开始，基于基本统计原理，实现一个带有 $L_2$ 正则化的逻辑回归分类器，用于预测30天死亡率 [@problem_id:4853338]。通过亲手推导和实现，您将深刻理解正则化是如何帮助模型防止过拟合，从而提高其在未见数据上的泛化能力。", "problem": "您将为一个医学信息学中的临床预测任务实现一个带惩罚的二元分类器，该模型将患者特征映射到$30$天死亡率的概率。目标变量是$30$天死亡率，编码为$0$（存活）或$1$（死亡）。特征集包括年龄、生命体征和合并症。您必须推导、实现并应用带有$L_2$（欧几里得）惩罚项的正则化逻辑回归，然后解释年龄每增加一个单位时几率的变化。您的实现必须从基本原理出发，即将二元结果建模为独立的伯努利随机变量，其概率由线性预测器的可微连接函数确定，并结合抑制参数值过大的惩罚项。不要假设或使用任何现成的简便公式；从基本定义中推导出您所需要的一切。\n\n临床变量及单位：\n- 年龄 $a$，单位为年。\n- 收縮压 $s$，单位为毫米汞柱（mmHg）。\n- 心率 $h$，单位为次/分钟（bpm）。\n- 糖尿病指标 $d \\in \\{0,1\\}$。\n- 慢性肾病指标 $c \\in \\{0,1\\}$。\n- 死亡结局 $y \\in \\{0,1\\}$。\n\n数据集（每行为一位患者，数据为 $(a, s, h, d, c, y)$）：\n- 患者 $1$：$a=72$ 岁，$s=110$ mmHg，$h=92$ bpm，$d=1$，$c=1$，$y=1$。\n- 患者 $2$：$a=55$ 岁，$s=130$ mmHg，$h=78$ bpm，$d=0$，$c=0$，$y=0$。\n- 患者 $3$：$a=80$ 岁，$s=100$ mmHg，$h=88$ bpm，$d=1$，$c=1$，$y=1$。\n- 患者 $4$：$a=45$ 岁，$s=120$ mmHg，$h=70$ bpm，$d=0$，$c=0$，$y=0$。\n- 患者 $5$：$a=67$ 岁，$s=115$ mmHg，$h=85$ bpm，$d=0$，$c=1$，$y=1$。\n- 患者 $6$：$a=60$ 岁，$s=140$ mmHg，$h=75$ bpm，$d=1$，$c=0$，$y=0$。\n- 患者 $7$：$a=50$ 岁，$s=125$ mmHg，$h=68$ bpm，$d=0$，$c=0$，$y=0$。\n- 患者 $8$：$a=76$ 岁，$s=105$ mmHg，$h=95$ bpm，$d=1$，$c=1$，$y=1$。\n- 患者 $9$：$a=65$ 岁，$s=110$ mmHg，$h=82$ bpm，$d=1$，$c=0$，$y=0$。\n- 患者 $10$：$a=58$ 岁，$s=135$ mmHg，$h=80$ bpm，$d=0$，$c=0$，$y=0$。\n- 患者 $11$：$a=85$ 岁，$s=98$ mmHg，$h=100$ bpm，$d=1$，$c=1$，$y=1$。\n- 患者 $12$：$a=70$ 岁，$s=108$ mmHg，$h=90$ bpm，$d=0$，$c=1$，$y=1$。\n\n建模与任务：\n- 使用带有显式截距（常数项）的线性预测器。不要对截距施加任何惩罚；将$L_2$惩罚应用于所有其他系数。\n- 从伯努利似然和线性预测器与事件概率之间的可微连接函数的定义出发，推导出目标函数，该函数是负对数似然与非截距系数的$L_2$惩罚项之和。\n- 推导目标函数相对于系数向量的梯度和Hessian矩阵，并实现一个带有回溯线搜索的牛顿类方法来找到系数估计值。\n- 为指定的测试套件计算系数估计值。\n- 将年龄每增加一个单位（年）的几率变化解释为年龄系数的指数。为每个测试案例报告此解释。\n\n测试套件：\n- 案例 $1$：惩罚参数 $\\lambda = 0$，对所有 $12$ 名患者使用基础特征 $(a, s, h, d, c)$，无共线性。\n- 案例 $2$：惩罚参数 $\\lambda = 0.1$，特征和患者与案例 $1$ 相同。\n- 案例 $3$：惩罚参数 $\\lambda = 10$，特征和患者与案例 $1$ 相同。\n- 案例 $4$：惩罚参数 $\\lambda = 1$，对所有 $12$ 名患者增加一个与年龄完全共线的特征（即，同时包括 $a$ 和一个精确副本 $a'$）。\n- 案例 $5$：惩罚参数 $\\lambda = 1$，使用基础特征 $(a, s, h, d, c)$，但仅使用前 $6$ 名患者。\n\n答案规格与单位：\n- 年龄系数必须按增加 $1$ 年进行解释，因此几率乘数为 $\\exp(\\beta_{\\text{age}})$，这是一个无量纲因子。请将几率乘数表示为小数。\n- 您的程序应为每个测试案例计算年龄系数 $\\beta_{\\text{age}}$ 和几率乘数 $\\exp(\\beta_{\\text{age}})$。\n- 输出格式：单行输出，包含一个用方括号括起来的逗号分隔列表，序列为以下浮点数，四舍五入至六位小数：$[\\beta_{\\text{age}}^{(1)}, \\exp(\\beta_{\\text{age}}^{(1)}), \\beta_{\\text{age}}^{(2)}, \\exp(\\beta_{\\text{age}}^{(2)}), \\beta_{\\text{age}}^{(3)}, \\exp(\\beta_{\\text{age}}^{(3)}), \\beta_{\\text{age}}^{(4)}, \\exp(\\beta_{\\text{age}}^{(4)}), \\beta_{\\text{age}}^{(5)}, \\exp(\\beta_{\\text{age}}^{(5)})]$，其中上标表示从 $1$ 到 $5$ 的案例编号。\n\n您的程序必须是自包含的，不接受任何输入，并严格按照上述规定产生单行输出。", "solution": "该问题要求从基本原理出发，推导并实现一个带有$L_2$正则化（也称为岭回归）的逻辑回归模型，用于根据一组临床特征预测30天死亡率。实现必须从模型的统计基础开始，最终形成一个带有回溯线搜索的牛顿-拉弗森优化算法。\n\n### I. 模型构建\n\n我们将每个患者 $i$ 的二元结果 $y_i \\in \\{0, 1\\}$ 建模为一个独立的伯努利随机变量。正向结果（$y_i=1$，死亡）的概率记为 $p_i$，它是患者特征向量 $\\mathbf{x}_i$ 的函数。患者 $i$ 的特征向量为 $\\mathbf{x}_i = [1, a_i, s_i, h_i, d_i, c_i]$，其中初始的 $1$ 对应一个截距项。完整的系数向量是 $\\boldsymbol{\\beta} = [\\beta_0, \\beta_a, \\beta_s, \\beta_h, \\beta_d, \\beta_c]^T$，其中 $\\beta_0$ 是截距。\n\n线性预测器 $\\eta_i$ 定义为特征向量与系数向量的内积：\n$$\n\\eta_i = \\mathbf{x}_i \\boldsymbol{\\beta} = \\beta_0 + \\beta_a a_i + \\beta_s s_i + \\beta_h h_i + \\beta_d d_i + \\beta_c c_i\n$$\n\n线性预测器 $\\eta_i$ 与概率 $p_i$ 之间的关系由逻辑斯蒂连接函数（logit函数的逆函数）建立：\n$$\np_i = P(y_i=1 | \\mathbf{x}_i; \\boldsymbol{\\beta}) = \\sigma(\\eta_i) = \\frac{1}{1 + e^{-\\eta_i}}\n$$\n其中 $\\sigma(\\cdot)$ 是 sigmoid 函数。相应地，存活的概率是 $P(y_i=0) = 1 - p_i = \\frac{e^{-\\eta_i}}{1 + e^{-\\eta_i}} = \\frac{1}{1 + e^{\\eta_i}}$。\n\n### II. 带惩罚项目标函数的推导\n\n目标是找到系数向量 $\\boldsymbol{\\beta}$，使其在最大化观测数据似然的同时，惩罚较大的系数值以防止过拟合。这等同于最小化带惩罚的负对数似然。\n\n**1. 似然函数：**\n鉴于 $N$ 个观测是独立的，数据 $(\\mathbf{y}, X)$ 的总似然是各个伯努利概率的乘积：\n$$\nL(\\boldsymbol{\\beta}) = \\prod_{i=1}^{N} p_i^{y_i} (1 - p_i)^{1-y_i}\n$$\n\n**2. 对数似然函数：**\n处理似然函数的对数更为方便：\n$$\n\\ell(\\boldsymbol{\\beta}) = \\log L(\\boldsymbol{\\beta}) = \\sum_{i=1}^{N} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right]\n$$\n我们可以用线性预测器 $\\eta_i$ 来表达它。对数几率（logit）是 $\\log\\left(\\frac{p_i}{1-p_i}\\right) = \\eta_i$。此外，$\\log(1-p_i) = -\\log(1+e^{\\eta_i})$。将这些代入对数似然表达式得到：\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{N} \\left[ y_i \\eta_i - \\log(1+e^{\\eta_i}) \\right] = \\sum_{i=1}^{N} \\left[ y_i (\\mathbf{x}_i \\boldsymbol{\\beta}) - \\log(1+e^{\\mathbf{x}_i \\boldsymbol{\\beta}}) \\right]\n$$\n\n**3. 带惩罚项目标函数：**\n我们的目标是最小化负对数似然，并加上一个 $L_2$ 惩罚项。该惩罚应用于除截距 $\\beta_0$ 外的所有系数。设 $p$ 为特征数量（不包括截距）。惩罚项为 $\\frac{\\lambda}{2} \\sum_{j=1}^{p} \\beta_j^2$，其中 $\\lambda \\ge 0$ 是正则化参数。\n最终要最小化的目标函数是 $J(\\boldsymbol{\\beta})$：\n$$\nJ(\\boldsymbol{\\beta}) = -\\ell(\\boldsymbol{\\beta}) + \\frac{\\lambda}{2} \\sum_{j=1}^{p} \\beta_j^2 = \\sum_{i=1}^{N} \\left[ \\log(1+e^{\\mathbf{x}_i \\boldsymbol{\\beta}}) - y_i (\\mathbf{x}_i \\boldsymbol{\\beta}) \\right] + \\frac{\\lambda}{2} \\sum_{j=1}^{p} \\beta_j^2\n$$\n\n### III. 使用牛顿法进行优化\n\n为了找到最小化 $J(\\boldsymbol{\\beta})$ 的最优 $\\boldsymbol{\\beta}$，我们使用牛顿-拉弗森法，这是一种迭代的二阶优化算法。更新规则是：\n$$\n\\boldsymbol{\\beta}^{(k+1)} = \\boldsymbol{\\beta}^{(k)} - \\alpha [H(\\boldsymbol{\\beta}^{(k)})]^{-1} \\mathbf{g}(\\boldsymbol{\\beta}^{(k)})\n$$\n其中 $\\mathbf{g}(\\boldsymbol{\\beta})$ 是 $J(\\boldsymbol{\\beta})$ 的梯度，$H(\\boldsymbol{\\beta})$ 是其Hessian矩阵，$\\alpha$ 是由线搜索过程确定的步长。\n\n**1. 目标函数的梯度：**\n梯度 $\\mathbf{g}(\\boldsymbol{\\beta}) = \\nabla J(\\boldsymbol{\\beta})$ 是一个由偏导数 $\\frac{\\partial J}{\\partial \\beta_j}$ 组成的向量。\n$$\n\\frac{\\partial J}{\\partial \\beta_j} = \\sum_{i=1}^{N} \\left[ \\frac{\\partial}{\\partial \\beta_j} \\log(1+e^{\\mathbf{x}_i \\boldsymbol{\\beta}}) - y_i \\frac{\\partial}{\\partial \\beta_j} (\\mathbf{x}_i \\boldsymbol{\\beta}) \\right] + \\frac{\\partial}{\\partial \\beta_j} \\left( \\frac{\\lambda}{2} \\sum_{k=1}^{p} \\beta_k^2 \\right)\n$$\n使用链式法则，$\\frac{\\partial}{\\partial \\beta_j} \\log(1+e^{\\eta_i}) = \\frac{e^{\\eta_i}}{1+e^{\\eta_i}} \\frac{\\partial \\eta_i}{\\partial \\beta_j} = p_i x_{ij}$。同样，$\\frac{\\partial}{\\partial \\beta_j} (\\mathbf{x}_i \\boldsymbol{\\beta}) = x_{ij}$。惩罚项的导数对于 $j \\ge 1$ 是 $\\lambda \\beta_j$，对于 $j=0$ 是 $0$。\n$$\n\\frac{\\partial J}{\\partial \\beta_j} = \\sum_{i=1}^{N} [p_i x_{ij} - y_i x_{ij}] + \\lambda \\beta_j \\cdot \\mathbb{I}(j \\ge 1) = \\sum_{i=1}^{N} (p_i - y_i) x_{ij} + \\lambda \\beta_j \\cdot \\mathbb{I}(j \\ge 1)\n$$\n在矩阵表示法中，设 $X$ 为 $N \\times (p+1)$ 的设计矩阵，$\\mathbf{y}$ 为 $N \\times 1$ 的结果向量，$\\mathbf{p}$ 为 $N \\times 1$ 的概率向量。设 $\\boldsymbol{\\beta}^*$ 为截距项为零的系数向量（$\\beta_0^*=0, \\beta_j^*=\\beta_j$ 对 $j \\ge 1$ 成立）。梯度为：\n$$\n\\mathbf{g}(\\boldsymbol{\\beta}) = \\nabla J(\\boldsymbol{\\beta}) = X^T(\\mathbf{p} - \\mathbf{y}) + \\lambda \\boldsymbol{\\beta}^*\n$$\n\n**2. 目标函数的Hessian矩阵：**\nHessian矩阵 $H(\\boldsymbol{\\beta}) = \\nabla^2 J(\\boldsymbol{\\beta})$ 是一个由二阶偏导数 $H_{jk} = \\frac{\\partial^2 J}{\\partial \\beta_k \\partial \\beta_j}$ 组成的矩阵。\n$$\nH_{jk} = \\frac{\\partial}{\\partial \\beta_k} \\left[ \\sum_{i=1}^{N} (p_i - y_i) x_{ij} \\right] + \\frac{\\partial}{\\partial \\beta_k} (\\lambda \\beta_j \\cdot \\mathbb{I}(j \\ge 1))\n$$\n$p_i$ 相对于 $\\eta_i$ 的导数是 $\\frac{dp_i}{d\\eta_i} = p_i(1-p_i)$。使用链式法则，$\\frac{\\partial p_i}{\\partial \\beta_k} = \\frac{dp_i}{d\\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_k} = p_i(1-p_i)x_{ik}$。\n$$\nH_{jk} = \\sum_{i=1}^{N} x_{ij} \\frac{\\partial p_i}{\\partial \\beta_k} + \\lambda \\delta_{jk} \\cdot \\mathbb{I}(j \\ge 1) = \\sum_{i=1}^{N} x_{ij} p_i(1-p_i) x_{ik} + \\lambda \\delta_{jk} \\cdot \\mathbb{I}(j \\ge 1)\n$$\n其中 $\\delta_{jk}$ 是克罗内克$\\delta$。在矩阵形式中，设 $W$ 是一个 $N \\times N$ 的对角矩阵，其对角线元素为 $W_{ii} = p_i(1-p_i)$。设 $\\boldsymbol{\\Lambda}$ 是一个 $(p+1) \\times (p+1)$ 的对角矩阵，其中 $\\Lambda_{00}=0$ 且对于 $j \\ge 1$ 有 $\\Lambda_{jj}=\\lambda$。Hessian矩阵为：\n$$\nH(\\boldsymbol{\\beta}) = X^T W X + \\boldsymbol{\\Lambda}\n$$\n该Hessian矩阵保证是半正定的。对于 $\\lambda  0$ 的情况，该矩阵是正定的，即使存在共线性（如案例4）或数据分离（如案例5）的情况，也能确保解的唯一性。\n\n**3. 回溯线搜索：**\n为确保收敛，完整的牛顿步长可能过大。我们引入一个步长 $\\alpha \\in (0, 1]$，选择它来满足Armijo-Goldstein充分下降条件。给定搜索方向 $\\mathbf{d} = -H^{-1}\\mathbf{g}$，我们从 $\\alpha=1$ 开始，并以一个因子 $\\tau$（例如 $0.5$）减小它，直到 $J(\\boldsymbol{\\beta} + \\alpha \\mathbf{d}) \\le J(\\boldsymbol{\\beta}) + c_1 \\alpha \\mathbf{g}^T \\mathbf{d}$ 对一个小常数 $c_1$（例如 $10^{-4}$）成立。\n\n### IV. 系数的解释\n在逻辑回归中，系数表示预测变量每改变一个单位，结果的对数几率的变化。死亡几率为 $\\frac{p_i}{1-p_i} = e^{\\eta_i}$。对于特征 $x_j$ 每增加一个单位，新的对数几率为 $\\eta_i + \\beta_j$。新的几率为 $e^{\\eta_i + \\beta_j} = e^{\\eta_i}e^{\\beta_j}$。因此，几率乘以一个因子 $e^{\\beta_j}$，这就是几率比。对于年龄系数 $\\beta_a$，年龄每增加一年的几率比是 $\\exp(\\beta_a)$。\n\n### V. 在测试案例中的应用\n将推导出的算法应用于五个指定的测试案例。对于每个案例，构建相应的设计矩阵 $X$、结果向量 $\\mathbf{y}$ 和惩罚参数 $\\lambda$。然后使用牛顿-拉弗森优化器找到系数向量 $\\boldsymbol{\\beta}$。最后，计算并报告年龄系数 $\\beta_a$（在我们的零索引向量中是 $\\beta_1$）和相应的几率比 $\\exp(\\beta_a)$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives, implements, and applies a regularized logistic regression model\n    from first principles to a clinical prediction task.\n    \"\"\"\n\n    # Dataset: (age, sbp, hr, diabetes, ckd, mortality)\n    data = np.array([\n        [72, 110, 92, 1, 1, 1],\n        [55, 130, 78, 0, 0, 0],\n        [80, 100, 88, 1, 1, 1],\n        [45, 120, 70, 0, 0, 0],\n        [67, 115, 85, 0, 1, 1],\n        [60, 140, 75, 1, 0, 0],\n        [50, 125, 68, 0, 0, 0],\n        [76, 105, 95, 1, 1, 1],\n        [65, 110, 82, 1, 0, 0],\n        [58, 135, 80, 0, 0, 0],\n        [85, 98, 100, 1, 1, 1],\n        [70, 108, 90, 0, 1, 1]\n    ], dtype=float)\n\n    X_raw = data[:, :-1]\n    y_raw = data[:, -1]\n\n    def _logistic_func(eta):\n        \"\"\"Numerically stable logistic sigmoid function.\"\"\"\n        # p = 1 / (1 + exp(-eta))\n        p = np.empty_like(eta)\n        pos_mask = eta >= 0\n        neg_mask = ~pos_mask\n        p[pos_mask] = 1. / (1. + np.exp(-eta[pos_mask]))\n        exp_eta_neg = np.exp(eta[neg_mask])\n        p[neg_mask] = exp_eta_neg / (1. + exp_eta_neg)\n        return p\n\n    def _objective_function(X, y, beta, lambda_val):\n        \"\"\"Calculates the penalized negative log-likelihood.\"\"\"\n        eta = X @ beta\n        beta_penalized = beta.copy()\n        beta_penalized[0] = 0.0\n        \n        # Numerically stable calculation of log(1 + exp(eta))\n        log_likelihood_term = np.empty_like(eta)\n        pos_mask = eta >= 0\n        neg_mask = ~pos_mask\n        log_likelihood_term[pos_mask] = eta[pos_mask] + np.log(1. + np.exp(-eta[pos_mask]))\n        log_likelihood_term[neg_mask] = np.log(1. + np.exp(eta[neg_mask]))\n        \n        neg_log_likelihood = np.sum(log_likelihood_term - y * eta)\n        penalty = (lambda_val / 2.0) * np.sum(beta_penalized**2)\n        return neg_log_likelihood + penalty\n\n    def solve_logistic_newton(X, y, lambda_val, max_iter=100, tol=1e-8):\n        \"\"\"\n        Solves regularized logistic regression using Newton-Raphson with backtracking.\n        \"\"\"\n        n_samples, n_features = X.shape\n        beta = np.zeros(n_features)\n        \n        # Backtracking line search parameters\n        alpha_init = 1.0\n        c1 = 1e-4\n        tau = 0.5\n\n        for i in range(max_iter):\n            beta_old = beta.copy()\n\n            # Calculate probabilities, gradient, and Hessian\n            eta = X @ beta\n            p = _logistic_func(eta)\n            \n            # Gradient\n            beta_penalized = beta.copy()\n            beta_penalized[0] = 0.0\n            grad = X.T @ (p - y) + lambda_val * beta_penalized\n\n            # Hessian\n            W = np.diag(p * (1.0 - p))\n            hessian = X.T @ W @ X\n            hessian_penalty = np.diag(np.full(n_features, lambda_val))\n            hessian_penalty[0, 0] = 0.0\n            hessian += hessian_penalty\n\n            # Newton step direction\n            try:\n                step = np.linalg.solve(hessian, -grad)\n            except np.linalg.LinAlgError:\n                # Fallback to gradient descent if Hessian is singular\n                step = -grad\n\n            # Backtracking line search\n            alpha = alpha_init\n            obj_val = _objective_function(X, y, beta, lambda_val)\n            grad_dot_step = grad.T @ step\n            \n            while True:\n                beta_new = beta + alpha * step\n                obj_val_new = _objective_function(X, y, beta_new, lambda_val)\n                \n                # Check Armijo-Goldstein condition\n                if obj_val_new = obj_val + c1 * alpha * grad_dot_step or alpha  1e-9:\n                    break\n                alpha *= tau\n            \n            beta = beta_new\n\n            # Check for convergence\n            if np.linalg.norm(beta - beta_old)  tol:\n                break\n        \n        return beta\n\n    test_cases = [\n        {'lambda': 0.0, 'data': (X_raw, y_raw), 'collinear': False},\n        {'lambda': 0.1, 'data': (X_raw, y_raw), 'collinear': False},\n        {'lambda': 10.0, 'data': (X_raw, y_raw), 'collinear': False},\n        {'lambda': 1.0, 'data': (X_raw, y_raw), 'collinear': True},\n        {'lambda': 1.0, 'data': (X_raw[:6], y_raw[:6]), 'collinear': False},\n    ]\n\n    results = []\n    for case in test_cases:\n        lambda_val = case['lambda']\n        X_case_raw, y_case = case['data']\n        n_samples = X_case_raw.shape[0]\n\n        if case['collinear']:\n            age_col = X_case_raw[:, 0].reshape(-1, 1)\n            X_case_features = np.hstack([X_case_raw, age_col])\n        else:\n            X_case_features = X_case_raw\n\n        X = np.hstack([np.ones((n_samples, 1)), X_case_features])\n        \n        beta_solution = solve_logistic_newton(X, y_case, lambda_val)\n        \n        beta_age = beta_solution[1]  # Age is the first feature after intercept\n        odds_multiplier = np.exp(beta_age)\n        \n        results.append(round(beta_age, 6))\n        results.append(round(odds_multiplier, 6))\n\n    print(f\"[{','.join(f'{x:.6f}' for x in results)}]\")\n\nsolve()\n```", "id": "4853338"}, {"introduction": "拥有一个能够输出准确风险概率的模型固然重要，但这只是故事的一半。在临床实践中，我们最终需要根据这些概率做出具体的决策——例如，是否对患者进行干预。这个最终的实践将引导您运用贝叶斯决策理论，推导出在给定不同错分成本（即假阴性 $C_{FN}$ 和假阳性 $C_{FP}$ 的代价）下的最优分类阈值 [@problem_id:4853255]。这个练习揭示了一个核心原则：最优决策不仅取决于模型的预测，还取决于决策错误的后果，这对于在资源有限和风险并存的医疗环境中制定明智的策略至关重要。", "problem": "一家医院部署了一个二元决策支持系统，用于标记需要针对某种临床结局进行早期干预的患者。该结局被编码为一个随机变量 $Y \\in \\{0,1\\}$，其中 $Y=1$ 表示该结局在30天内发生，$Y=0$ 表示未发生。在目标人群中，已知患病率（基准率）为 $0.01$。对于每个特征为 $X=x$ 的患者，模型会输出一个校准风险 $p(x)$，在 $0.01$ 的基准率下，该风险被解释为后验概率 $p(x)=\\mathbb{P}(Y=1\\mid X=x)$。一个带有参数 $t \\in [0,1]$ 的阈值规则在 $p(x)\\geq t$ 时给出阳性预测，在 $p(x)  t$ 时给出阴性预测。该系统旨在最小化期望成本，该成本是根据假阴性（$C_{FN}  0$）和假阳性（$C_{FP}  0$）的误分类成本来计算的。从基本原理出发，推导出最小化期望成本的贝叶斯最优决策阈值 $t^\\ast$，并将其表示为 $C_{FN}$ 和 $C_{FP}$ 的函数。", "solution": "用户希望找到一个二元分类问题的贝叶斯最优决策阈值。\n\n### 步骤 1：提取已知条件\n-   临床结局是一个二元随机变量 $Y \\in \\{0, 1\\}$，其中 $Y=1$ 代表结局发生。\n-   基准率（患病率）为 $\\mathbb{P}(Y=1) = 0.01$。\n-   对于特征为 $X=x$ 的患者，模型提供一个校准的后验概率 $p(x) = \\mathbb{P}(Y=1|X=x)$。\n-   决策规则是：如果 $p(x) \\geq t$，则预测为阳性（$\\hat{Y}=1$）；如果 $p(x)  t$，则预测为阴性（$\\hat{Y}=0$）。\n-   假阴性的成本为 $C_{FN} > 0$。这是当真实状态为 $Y=1$ 时，预测为 $\\hat{Y}=0$ 的成本。\n-   假阳性的成本为 $C_{FP} > 0$。这是当真实状态为 $Y=0$ 时，预测为 $\\hat{Y}=1$ 的成本。\n-   真阳性（当 $Y=1$ 时预测为 $\\hat{Y}=1$）和真阴性（当 $Y=0$ 时预测为 $\\hat{Y}=0$）的成本被隐式地设为零，因为它们代表的是相对于正确分类的误分类成本。\n\n### 步骤 2：使用提取的已知条件进行验证\n对问题进行验证。\n\n-   **科学依据**：该问题是贝叶斯决策理论在分类任务中的一个标准且基本的应用。后验概率、成本敏感分类和期望损失等概念是统计机器学习和医学信息学的核心。该问题的设置在科学上和数学上都是合理的。\n-   **适定性**：该问题是适定的。它要求找到一个能最小化明确定义的成本函数（条件期望损失）的最优阈值。所有必要的组成部分（后验概率、成本、决策规则结构）都已提供，可以推导出一个唯一的解。\n-   **客观性**：该问题以精确、定量和客观的语言陈述。\n-   **完整性与一致性**：该问题是自洽的。虽然提供了患病率 $\\mathbb{P}(Y=1) = 0.01$，但对于基于条件风险推导最优阈值而言，这个信息是外在的，因为模型的输出 $p(x)$ 已经是后验概率 $\\mathbb{P}(Y=1|X=x)$。这个信息的存在不会造成矛盾；它只是在当前推导中不是必需的。该问题内部是一致的。\n-   **未发现其他缺陷。** 该问题是现实的、可形式化的且非平凡的。\n\n### 步骤 3：结论与行动\n该问题是**有效**的。将推导解答。\n\n贝叶斯决策理论的核心原理是，对于给定的观测值 $X=x$，选择能最小化条件期望损失（或风险）的行动。我们定义两种可能的行动：\n1.  行动 $\\alpha_1$：预测结局将会发生（$\\hat{Y}=1$）。\n2.  行动 $\\alpha_0$：预测结局不会发生（$\\hat{Y}=0$）。\n\n自然状态是 $Y$ 的真实值。对于一个特征为 $X=x$ 的患者，真实状态为 $Y=1$ 的概率是后验概率 $p(x) = \\mathbb{P}(Y=1|X=x)$。因此，真实状态为 $Y=0$ 的概率是 $\\mathbb{P}(Y=0|X=x) = 1 - p(x)$。\n\n成本定义如下：\n-   假阴性成本，$L(\\hat{Y}=0, Y=1) = C_{FN}$。\n-   假阳性成本，$L(\\hat{Y}=1, Y=0) = C_{FP}$。\n-   真阴性成本，$L(\\hat{Y}=0, Y=0) = 0$。\n-   真阳性成本，$L(\\hat{Y}=1, Y=1) = 0$。\n\n现在，我们计算在患者特征 $X=x$ 的条件下，每个行动的条件期望损失。\n\n采取行动 $\\alpha_1$（预测为阳性，$\\hat{Y}=1$）的条件期望损失是：\n$$ R(\\alpha_1 | x) = L(\\hat{Y}=1, Y=1) \\mathbb{P}(Y=1|X=x) + L(\\hat{Y}=1, Y=0) \\mathbb{P}(Y=0|X=x) $$\n代入概率和成本：\n$$ R(\\alpha_1 | x) = (0) \\cdot p(x) + C_{FP} \\cdot (1 - p(x)) = C_{FP}(1 - p(x)) $$\n\n采取行动 $\\alpha_0$（预测为阴性，$\\hat{Y}=0$）的条件期望损失是：\n$$ R(\\alpha_0 | x) = L(\\hat{Y}=0, Y=1) \\mathbb{P}(Y=1|X=x) + L(\\hat{Y}=0, Y=0) \\mathbb{P}(Y=0|X=x) $$\n代入概率和成本：\n$$ R(\\alpha_0 | x) = C_{FN} \\cdot p(x) + (0) \\cdot (1 - p(x)) = C_{FN} p(x) $$\n\n贝叶斯最优决策规则是选择具有较低条件期望损失的行动。我们应该当且仅当预测为阳性（$\\hat{Y}=1$）的损失小于或等于预测为阴性的损失时，才做出阳性预测。我们使用“小于或等于”是为了与问题中的阈值规则 $p(x) \\geq t$ 保持一致。\n$$ R(\\alpha_1 | x) \\leq R(\\alpha_0 | x) $$\n$$ C_{FP}(1 - p(x)) \\leq C_{FN} p(x) $$\n\n我们现在解这个关于 $p(x)$ 的不等式，以找到我们应该预测为阳性的条件：\n$$ C_{FP} - C_{FP} p(x) \\leq C_{FN} p(x) $$\n$$ C_{FP} \\leq C_{FN} p(x) + C_{FP} p(x) $$\n$$ C_{FP} \\leq (C_{FN} + C_{FP}) p(x) $$\n因为 $C_{FN}0$ 且 $C_{FP}0$，所以 $(C_{FN} + C_{FP})$ 项是严格为正的，因此我们可以用它来除不等式两边，而不用改变不等号的方向：\n$$ p(x) \\geq \\frac{C_{FP}}{C_{FN} + C_{FP}} $$\n\n问题陈述系统在 $p(x) \\geq t$ 时预测为阳性。我们推导出的贝叶斯最优决策规则指示，当 $p(x) \\geq \\frac{C_{FP}}{C_{FN} + C_{FP}}$ 时应预测为阳性。通过比较这两个规则，我们可以确定贝叶斯最优阈值 $t^{\\ast}$。\n$$ t^{\\ast} = \\frac{C_{FP}}{C_{FN} + C_{FP}} $$\n该表达式给出了仅用误分类成本 $C_{FN}$ 和 $C_{FP}$ 表示的最优阈值，符合题目要求。这个结果也被称为风险均衡阈值或成本比率阈值。", "answer": "$$ \\boxed{\\frac{C_{FP}}{C_{FN} + C_{FP}}} $$", "id": "4853255"}]}