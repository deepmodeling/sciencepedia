## 引言
在现代医学中，尤其是在慢性病管理和危重症监护领域，治疗决策并非一劳永逸，而是一个动态的、序贯的过程。临床医生需要根据患者不断变化的生理状态，持续调整治疗方案，以期达到最佳的长期健康结果。然而，传统的统计方法在处理这种时变混杂因素（即治疗影响未来状态，未来状态又影响后续治疗）的复杂反馈循环时常常力不从心。[强化学习](@entry_id:141144)（Reinforcement Learning, RL）作为一个强大的机器学习分支，为解决这类动态决策优化问题提供了全新的、系统性的框架。

本文旨在全面介绍[强化学习](@entry_id:141144)在治疗优化领域的核心思想与应用路径。我们将带领读者从基础理论出发，逐步深入到应用实践中的关键挑战与高级解决方案。通过学习本文，你将理解[强化学习](@entry_id:141144)如何将复杂的临床问题转化为可计算的模型，并学会在这个模型中融入安全性、公平性和伦理考量。

*   在**“原理与机制”**一章中，我们将奠定理论基石，详细讲解如何将治疗过程形式化为[马尔可夫决策过程](@entry_id:140981)（MDP），并介绍[价值函数](@entry_id:144750)、策略以及时序差分（TD）学习等核心概念。同时，我们将深入探讨在医疗领域最常见的场景——从历史数据中进行离线学习（Offline RL）所面临的根本性挑战，如[混淆偏倚](@entry_id:635723)、数据覆盖不足和[模型不稳定性](@entry_id:141491)。
*   在**“应用与跨学科连接”**一章中，我们将理论与实践相结合，展示如何将脓毒症管理等具体临床问题构建为MDP或[POMDP](@entry_id:637181)模型。此外，本章将介绍用于提升模型临床真实感的高级建模技术，并重点探讨如何利用约束性[马尔可夫决策过程](@entry_id:140981)（CMDP）框架，将“不伤害”、“公平正义”等核心生物伦理原则编码为算法必须遵守的硬性约束。
*   最后，在**“动手实践”**部分，我们提供了一系列精心设计的练习，帮助你亲手实现最优策略的计算、[离策略评估](@entry_id:181976)以及带约束的优化，从而将抽象的理论知识转化为具体的可操作技能。

## 原理与机制

### 将治疗优化问题形式化为[马尔可夫决策过程](@entry_id:140981)

为了运用强化学习（RL）来优化治疗策略，我们首先需要一个能够精确描述该问题的数学框架。[马尔可夫决策过程](@entry_id:140981)（Markov Decision Process, MDP）为此提供了理想的语言。一个MDP由五个核心要素构成，即一个元组 $(S, A, P, R, \gamma)$，它们共同定义了决策制定环境的动力学和目标。在临床治疗的背景下，这些要素具有明确且直观的医学含义。

**状态（State, $S$）**：状态 $s \in S$ 是对决策时刻所需全部相关信息的总结。在临床上，状态可以被看作是患者在某次就诊时的“临床快照”。为了满足**马尔可夫假设**——即未来只依赖于当前状态和行动，而与过去无关——[状态表示](@entry_id:141201)必须足够丰富。例如，在管理II型糖尿病患者的慢性高血糖症时，一个有效的状态 $s$ 应该包含多维度信息，如当前的糖化血红蛋白（[HbA1c](@entry_id:150571)）水平 $h$、当前的药物治疗方案 $m$、肾功能分期 $r$、身体[质量指数](@entry_id:190779)（BMI）类别 $b$、近期是否发生过低血糖事件 $y$ 以及其他相关的合并症和风险协变量 $c$。将这些信息组合成一个向量 $s=(h,m,r,b,y,c)$，就构成了一个全面的[状态表示](@entry_id:141201)，它总结了患者的过去，为未来的决策提供了充分依据 [@problem_id:4855022]。

**行动（Action, $A$）**：行动 $a \in A$ 是决策者（例如，临床医生或决策支持系统）在给定状态下可以采取的干预措施。在治疗优化中，行动通常对应于对治疗方案的调整。一个实用且清晰的行动空间应该是离散且有临床意义的。例如，针对糖尿病管理，行动可以包括：维持当前方案、增加剂量、减少剂量、添加新药、更换药物类别或减少药物种类。这些具体的临床操作构成了行动集合 $A$ [@problem_id:4855022]。

**转移概率（Transition Probability, $P$）**：转移概率函数 $P(s'|s,a)$ 描述了系统的动态性。它给出了在当前状态 $s$ 下采取行动 $a$ 后，系统将转移到下一个状态 $s'$ 的概率分布。在临床背景下，这代表了患者对治疗的生理反应、疾病的自然进展以及其他随机因素的综合影响。例如，在给予特定剂量的降糖药物后，患者在下次就诊时的[HbA1c](@entry_id:150571)水平、体重和肾功能等指标的变化，都由这个[概率模型](@entry_id:265150)所刻画。马尔可夫假设要求这个转移概率仅依赖于 $(s, a)$，而不依赖于更早的历史。

**[奖励函数](@entry_id:138436)（Reward Function, $R$）**：[奖励函数](@entry_id:138436) $R(s, a, s')$ 量化了从状态 $s$ 采取行动 $a$ 转移到状态 $s'$ 后获得的即时效用或成本。[奖励函数](@entry_id:138436)的设计是RL应用中最关键也最具挑战性的一环，因为它直接定义了优化的目标。一个精心设计的[奖励函数](@entry_id:138436)必须平衡多个、有时甚至是相互冲突的临床目标。继续以糖尿病管理为例，长期目标是维持血糖稳定，同时避免副作用和过度用药。这可以转化为一个多目标的[奖励函数](@entry_id:138436)，例如：$R(s,a,s')=-\alpha|h'-\tau|-\beta\mathbf{1}\{y'=1\}-\lambda\text{burden}(m',a)$。这里，第一项惩罚血糖值 $h'$ 偏离个体化目标 $\tau$；第二项对发生低血糖事件 $(y'=1)$ 施加重罚；第三项则惩罚药物负担（如成本、副作用风险）的增加。通过调整权重 $\alpha, \beta, \lambda$，我们可以精确地表达临床的偏好与权衡 [@problem_id:4855022]。

**折扣因子（Discount Factor, $\gamma$）**：折扣因子 $\gamma \in (0, 1]$ 决定了未来奖励在当前决策中的相对权重。对于慢性病管理这类持续性的、长周期的任务，我们不仅关心下一次就诊的结果，更关心患者长期的健康状况。$\gamma$ 的值接近于1（例如0.99）意味着长期回报非常重要，而一个较小的 $\gamma$ 则表示更注重短期效果。折扣因子的引入确保了在无限周期的任务中，累积奖励的总和是一个有限值，使得优化目标在数学上是良定的。

### 学习的目标：价值函数与策略

在MDP框架下，强化学习的目标是找到一个最优的**策略（policy）** $\pi$。一个策略 $\pi(a|s)$ 是一个从状态到行动的映射，它规定了在每个可能的状态 $s$ 下选择行动 $a$ 的概率。在临床上，一个策略就代表了一套完整的、动态的治疗指南或决策规则。

为了评估一个策略的好坏，我们引入了**价值函数（value function）** 的概念。[价值函数](@entry_id:144750)衡量了从某个状态出发，遵循特定策略所能获得的长期累积奖励的[期望值](@entry_id:150961)。主要有两种[价值函数](@entry_id:144750)：

1.  **状态价值函数（State-Value Function, $V^{\pi}(s)$）**：它定义为从状态 $s$ 开始，并此后一直遵循策略 $\pi$ 所能获得的期望回报（discounted return）。回报 $G_t$ 是从时间 $t$ 开始的所有未来奖励的折扣总和，即 $G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$。因此，状态价值函数的正式定义是：
    $$V^{\pi}(s) = \mathbb{E}_{\pi}\left[ \sum_{k=0}^{\infty} \gamma^k R_{k+1} \,\middle|\, S_0 = s \right]$$
    其中，期望 $\mathbb{E}_{\pi}$ 是针对由策略 $\pi$ 和环境动态性共同产生的所有可能轨迹（状态-行动序列）计算的。在临床上，$V^{\pi}(s)$ 代表了处于临床状态 $s$ 的患者，在遵循治疗策略 $\pi$ 的情况下，其预期的长期累积健康效用 [@problem_id:4855006]。

2.  **行动价值函数（Action-Value Function, $Q^{\pi}(s,a)$）**：它定义为在状态 $s$ 下，首先采取一个特定的行动 $a$，然后继续遵循策略 $\pi$ 所能获得的期望回报。其正式定义为：
    $$Q^{\pi}(s,a) = \mathbb{E}_{\pi}\left[ \sum_{k=0}^{\infty} \gamma^k R_{k+1} \,\middle|\, S_0 = s, A_0 = a \right]$$
    $Q^{\pi}(s,a)$ 对于决策至关重要，因为它量化了在特定状态下采取某个具体行动的长期价值。通过比较一个状态下所有可选行动的 $Q$ 值，我们就能知道哪个行动是更好的选择。临床上，$Q^{\pi}(s,a)$ 回答了这样一个问题：“对于当前处于状态 $s$ 的患者，如果我这次采取治疗行动 $a$，并在此后遵循策略 $\pi$ 进行管理，那么预期的长期累-积健康效用是多少？” [@problem_id:4855006]。

[价值函数](@entry_id:144750)之间存在重要的递归关系，即**贝尔曼期望方程（Bellman expectation equations）**。这个方程将一个状态的价值分解为即时奖励和其后继状态的折扣价值之和：
$$V^{\pi}(s) = \mathbb{E}_{\pi}\left[ R_1 + \gamma V^{\pi}(S_1) \,\middle|\, S_0 = s \right]$$
$$Q^{\pi}(s,a) = \mathbb{E}_{\pi}\left[ R_1 + \gamma V^{\pi}(S_1) \,\middle|\, S_0 = s, A_0 = a \right]$$
这个递归结构是强化学习算法的核心，它表明我们可以通过“自举”（bootstrapping）的方式，用后续状态的价值估计来更新当前状态的价值估计。

### 从经验中学习：核心机制

强化学习算法的核心任务就是从与环境的交互中估计价值函数，并最终改进策略。**[时序差分学习](@entry_id:177975)（Temporal Difference, TD learning）** 是其中最重要和最广泛使用的一类方法。与蒙特卡洛方法（需要等待一个完整序列结束后才进行更新）不同，TD学习在每一步之后都进行学习。

以[策略评估](@entry_id:136637)（即估计一个给定策略 $\pi$ 的[价值函数](@entry_id:144750) $V^{\pi}$）为例，最基础的TD算法是 **TD(0)**。假设我们有一个当前对价值函数的估计 $\hat{V}$。在一次就诊（一个时间步）后，我们观察到了一个转移 $(s, a, r, s')$，其中 $s$ 是就诊前状态， $a$ 是采取的治疗行动， $r$ 是获得的即时奖励， $s'$ 是下次就诊时的状态。

TD(0)算法使用这个转移来更新状态 $s$ 的价值估计 $\hat{V}(s)$。更新的目标是所谓的“TD目标”，即 $r + \gamma \hat{V}(s')$。这个目标由两部分组成：实际观察到的即时奖励 $r$，以及对未来所有奖励的估计，即下一状态的折扣价值估计 $\gamma \hat{V}(s')$。这种用一个估计值（$\hat{V}(s')$）来更新另一个估计值（$\hat{V}(s)$）的方法，被称为**自举（bootstrapping）**。

TD(0)的更新规则如下：
$$\hat{V}(s) \leftarrow \hat{V}(s) + \alpha \left[ r + \gamma \hat{V}(s') - \hat{V}(s) \right]$$
其中，$\alpha$ 是[学习率](@entry_id:140210)，它控制了更新的步长。括号中的项 $r + \gamma \hat{V}(s') - \hat{V}(s)$ 被称为**[TD误差](@entry_id:634080)**，它代表了当前估计 $\hat{V}(s)$ 与更“准确”的目标之间的差距。

这个机制的精妙之处在于它如何解决**信用分配（credit assignment）** 问题，尤其是在结果延迟出现的医疗场景中。例如，某次成功的剂量调整可能不会立即改善患者的化验指标，其真正的好处（如降低长期并发症风险）可能在数月甚至数年后才显现。TD学习通过自举机制巧妙地处理了这一点。更新 $\hat{V}(s)$ 时所用的 $\hat{V}(s')$ 本身就是对从 $s'$ 开始的未来所有奖励的期望。通过不断迭代，一个远期奖励的价值会像涟漪一样，从其发生的未来状态一步步向前传播，逐次更新它之前所有状态的价值。最终，一个早期决策的价值估计将能够反映其对遥远未来产生的影响 [@problem_id:4855027]。

### 从观测数据中学习的挑战

在理想情况下，RL智能体通过与环境的在线交互（试错）来学习。但在医疗领域，尤其是在治疗优化中，让一个不成熟的算法在真实患者身上进行自由探索是不可接受的。因此，我们通常依赖于**离线[强化学习](@entry_id:141144)（Offline RL）**，即从预先收集好的历史数据（如电子健康记录，EHR）中学习。这些数据由过去的临床实践（即**行为策略 $\mu$**）产生，我们无法进行新的交互。

在这种离线设置下，我们面临两种主要的学习范式：

1.  **基于模型的[强化学习](@entry_id:141144)（Model-Based RL）**：这种方法首先尝试从数据中显式地学习环境的模型，即转移概率 $\hat{P}(s'|s,a)$ 和[奖励函数](@entry_id:138436) $\hat{R}(s,a,s')$。一旦模型建立，我们就可以在模拟环境中进行规划（如通过动态规划），以找到[最优策略](@entry_id:138495)。这种方法的优点在于**样本效率**高，因为一个学成的模型可以产生无限的模拟数据。然而，它的主要缺点是**[模型偏差](@entry_id:184783)**：如果学习到的模型 $\hat{P}$ 或 $\hat{R}$ 不准确，那么基于这个错误模型规划出的策略可能会系统性地犯错，导致灾难性后果 [@problem_id:4855013]。

2.  **无模型的[强化学习](@entry_id:141144)（Model-Free RL）**：这种方法不学习显式的环境模型，而是直接从数据中估计[价值函数](@entry_id:144750)（如 $Q^{\pi}$ 或 $Q^*$）。例如，[Q学习](@entry_id:144980)算法就是一种典型的无模型方法。这种方法的优点是避免了[模型偏差](@entry_id:184783)的风险。但它在离线设置中面临的主要挑战是**外推误差（extrapolation error）**。由于历史数据集的覆盖范围有限，行为策略可能从未或很少在某些状态下尝试某些行动。当我们要评估一个新策略在这些数据稀疏区域的行动时，[价值函数](@entry_id:144750)的估计就变成了基于[函数逼近](@entry_id:141329)器的不可靠外推，其结果可能与真实情况相去甚远 [@problem_id:4855013]。

此外，从观测性的EHR数据中学习面临一个更根本的挑战：**混淆（confounding）**。在临床实践中，医生的决策（行动）往往受到患者某些未被充分记录的状况（[混淆变量](@entry_id:199777)）的影响，而这些状况本身也会影响患者的预后（下一状态）。例如，医生可能会因为察觉到患者潜在的虚弱迹象（未记录在EHR的结构化字段中）而选择一个更保守的治疗方案。如果我们忽略这些[混淆变量](@entry_id:199777)，直接从数据中学习 $(s,a)$ 与 $s'$ 之间的关系，那么学到的将是混杂了因果效应和[虚假相关](@entry_id:755254)的混合体，而不是行动的纯粹因果效应。这种混淆偏差会同时污染基于模型的方法（学到错误的 $\hat{P}$）和无模型的方法（学到错误的 $\hat{Q}$），除非我们能通过因果推断的技术进行适当的调整 [@problem_id:4855013]。

### [离策略评估](@entry_id:181976)的基本假设

在离线环境中，一个核心任务是**[离策略评估](@entry_id:181976)（Off-Policy Evaluation, OPE）**：即在不实际部署新策略的情况下，仅使用行为策略 $\mu$ 产生的历史数据来评估一个新提出的**目标策略 $\pi$** 的价值。这是决定一个新策略是否值得在临床试验中进一步验证的关键步骤。

为了使这种评估在因果意义上有效，即为了让我们相信评估结果能代表目标策略 $\pi$ *如果被部署* 时会产生的真实效果，必须满足三个来自因果推断领域的关键假设。这些假设构成了从观测数据进行有效推断的基石 [@problem_id:4855003]。

1.  **一致性（Consistency）**：这个假设将[潜在结果框架](@entry_id:636884)与观测数据联系起来。它指出，一个患者实际观测到的结局，就是对应于其所接受的实际治疗序列的潜在结局。换句话说，如果历史记录中的一个治疗序列恰好与目标策略 $\pi$ 所建议的一致，那么该患者的记录就真实反映了遵循策略 $\pi$ 会发生的情况。

2.  **序贯无混淆性（Sequential Unconfoundedness）**：也称为条件可交换性（conditional exchangeability），这是最核心也是最难满足的假设。它要求在每个决策时刻 $t$，给定过去所有可观测的历史信息 $H_t$（包括过去的状态、行动等），临床医生当时所做的治疗决策 $A_t$ 与患者未来的所有潜在结局是独立的。通俗地说，这意味着所有同时影响治疗决策和患者预后的因素（即所有[混淆变量](@entry_id:199777)）都已经被完整地记录在了历史 $H_t$ 中。在EHR数据中，这要求[状态表示](@entry_id:141201) $S_t$ 必须极为详尽，以捕获医生的全部决策依据。

3.  **正性（Positivity）** 或重叠性（Overlap）：这个假设要求，对于目标策略 $\pi$ 可能推荐的任何行动，在相似的患者历史中，行为策略 $\mu$ 也必须有大于零的概率去尝试它。即，如果对于某个历史 $h$，$\pi(a|h) > 0$，那么必须有 $\mu(a|h) > 0$。这个假设保证了我们总能从历史数据中找到一些“参照组”来评估新策略的行动。如果新策略建议了一个在历史上从未被尝试过的激进疗法，我们就没有任何数据来评估它的效果，此时正性假设被违反。

在这些假设下，我们可以使用**[重要性采样](@entry_id:145704)（importance sampling）** 等技术来进行OPE。其基本思想是通过给每条历史轨迹赋予一个权重来校正行为策略和目标策略之间的差异。这个权重是两条策略下该轨迹发生概率的比值：$w(\tau) = \frac{P_{\pi}(\tau)}{P_{\mu}(\tau)} = \prod_{t=0}^{T-1} \frac{\pi(A_t | H_t)}{\mu(A_t | H_t)}$。通过对加权后的回报求平均，我们就可以得到目标策略价值的一个无偏估计。

### 关键失效模式与实践挑战

尽管[强化学习](@entry_id:141144)为治疗优化提供了强大的理论框架，但在将其应用于真实的、充满噪声和不确定性的临床数据时，会遇到一系列严峻的挑战和潜在的失效模式。

#### 马尔可夫假设的再审视：[状态表示](@entry_id:141201)的局限性

整个MDP框架建立在马尔可夫假设之上，即当前状态 $S_t$ 包含了预测未来所需的所有历史信息。然而，在临床实践中，这个假设极易被违反，这通常源于[状态表示](@entry_id:141201)的不足 [@problem_id:4855054]。

*   **累积效应**：某些治疗的影响是累积的。例如，在肿瘤化疗中，心脏毒性风险取决于蒽环类药物历史累积总剂量，而不仅仅是最近一次的剂量。如果状态只包含当前生物标志物和最近一次剂量，就无法捕捉这种累积效应，从而违反马尔可夫假设。
*   **[延迟效应](@entry_id:199612)**：药物的药效学可能有延迟。例如，胰岛素的降糖效果可能依赖于数小时前的注射剂量。如果状态中不包含“距上次给药时间”或“体内残留药物活性”等信息，那么仅凭当前血糖值就无法准确预测下一时刻的变化。
*   **未观测变量**：患者可能存在未被观察到或未被记录的潜在疾病状态，这些状态会影响治疗反应。例如，一个未被诊断的轻度肾功能损害会影响[药物清除率](@entry_id:151181)。由于这个变量（混淆因子）没有被包含在状态中，系统就变成了部分可观测的（Partially Observable MDP, [POMDP](@entry_id:637181)），仅从观测状态出发，历史信息对于推断这个隐藏变量变得至关重要。
*   **观测噪声**：临床测量（如化验值）本身就带有噪声。如果我们将这些带有噪声的观测值直接作为状态，那么历史观测序列可以帮助我们通过滤波（如卡尔曼滤波）来更好地估计真实的潜在生理状态。由于历史信息有助于改善对当前真实状态的认知，因此基于观测值的过程不再是马尔可夫的。

#### 实践中的“正性”假设违背

理论上的正性假设要求行为策略对目标策略的行动有非零的支持。但在实践中，我们面临的是“近似违背”的问题：行为策略 $\mu(a|s)$ 可能虽然大于零，但极其微小。例如，对于某个高风险状态，新策略 $\pi_e$ 建议采取一个概率为0.5的积极干预，但在历史数据中，医生们出于保守，采取该干预的概率 $\pi_b$ 仅为 $10^{-3}$。

这种情况会导致[重要性采样](@entry_id:145704)权重中的单步比率 $\frac{\pi_e(a|s)}{\pi_b(a|s)}$ 变得极大（例如，$\frac{0.5}{10^{-3}} = 500$）。由于整个轨迹的权重是这些比率的连乘积，最终的权重可能会变得极其巨大，导致整个OPE估计的**方差爆炸**。估计值可能完全由少数几个具有巨大权重的样本所主导，变得极不稳定和不可信 [@problem_id:4855038]。

一个重要的诊断工具是**有效样本量（Effective Sample Size, ESS）**，其计算公式为 $\text{ESS} = \frac{(\sum W_i)^2}{\sum W_i^2}$。ESS衡量了加权后样本的“等效”数量。如果权重分布极不均匀（少数权重极大），ESS会远小于实际样本量 $n$。一个相对于 $n$ 非常小的ESS值，是策略支持度严重不匹配的明确警告信号。

#### [分布偏移](@entry_id:638064)：从历史数据到临床部署

另一个严峻的挑战是**[分布偏移](@entry_id:638064)（Distributional Shift）**。我们用来学习策略的EHR数据来自一个特定的历史患者群体（源域），其协变量分布为 $p_0(x)$。然而，策略未来要部署的患者群体（目标域）可能在年龄、疾病严重程度、合并症等方面有所不同，其协变量分布为 $p_1(x)$。

如果 $p_0(x) \neq p_1(x)$，那么即使我们完美地解决了[离策略学习](@entry_id:634676)问题，学到的策略在源域上的表现也可能无法代表其在目标域上的真实表现。为了准确评估策略在目标人群中的价值 $V_1(\pi)$，我们需要同时校正策略差异和人群差异。此时，重要性权重需要被增广，包含一个校正[协变量偏移](@entry_id:636196)的项 [@problem_id:4855016]：
$$w(x,a) = \frac{p_1(x)}{p_0(x)} \cdot \frac{\pi(a|x)}{\mu(a|x)}$$
忽略第一项 $\frac{p_1(x)}{p_0(x)}$ 会导致我们估计的是策略在历史人群中的价值 $V_0(\pi)$，这对于预测未来部署效果是有偏的。

#### “死亡三角”：不稳定性之源

当**[函数逼近](@entry_id:141329)（function approximation）**（如使用线性模型或神经网络来表示价值函数）、**自举（bootstrapping）**（如TD学习）和**[离策略学习](@entry_id:634676)**这三个要素同时出现时，可能会导致学习过程的灾难性不稳定，甚至发散。这个组合被称为**“死亡三角”（the deadly triad）** [@problem_id:4855012]。

其根本的数学原因在于，离策略TD学习的更新算子 $\Pi_{d_b} T_{\pi}$ 不再保证是收缩的。其中，$T_{\pi}$ 是基于目标策略 $\pi$ 的贝尔曼算子，它会朝着 $\pi$ 认为重要的方向“扩展”价值；而 $\Pi_{d_b}$ 是一个投影算子，它将更新后的价值投影回[函数逼近](@entry_id:141329)器可以表示的空间，这个投影是根据行为策略的状态分布 $d_b$ 进行加权的。当目标策略 $\pi$ 和行为策略 $b$ 差异巨大时，$T_{\pi}$ 关注的状态区域可能在 $d_b$ 下非常罕见，导致投影操作严重扭曲了贝尔曼更新的方向。这种不匹配可能导致更新[算子的谱半径](@entry_id:261858)大于1，从而使得价值函数的参数在迭代中发散至无穷。

一个典型的临床场景是：在ICU脓毒症治疗中，历史数据来自一个保守的行为策略 $b$（倾向于低剂量补液），其状态分布 $d_b$ 很少覆盖高乳酸等极端危重状态。而一个新的目标策略 $\pi$ 在这些状态下主张采取更积极的干预。在这种情况下，使用离策略TD算法进行学习，就可能触发“死亡三角”导致发散。

#### [奖励函数](@entry_id:138436)误设与“奖励操纵”

最后，也许是最关键的安全性挑战，在于**[奖励函数](@entry_id:138436)误设（reward misspecification）**。我们设计的[奖励函数](@entry_id:138436) $r$ 通常是基于易于测量的短期代理指标（如SOFA评分的降低、实验室检查值的改善）和成本考量。然而，真正的临床目标 $u$ 是更长期的、以患者为中心的结果（如90天生存率、无严重不良事件）。

当代理奖励 $r$ 与真实效用 $u$ 不完全一致时，RL智能体可能会发现“捷径”来**操纵奖励（reward hacking）**。它可能会学到一个策略，通过利用代理指标的某些漏洞或伪影来获得高分，而实际上对患者的真实长期健康是有害的 [@problem_id:4855039]。例如，一个策略可能学会使用某种药物快速降低某个炎性指标（获得高奖励 $r$），但这种药物可能导致严重的长期肾损伤（产生巨大的负效用 $u$）。

要检测这种危险的行为，我们不能只看代理奖励上的表现。一个原则性的方法是，利用OPE技术，在历史数据上同时评估候选策略 $\pi_r$ 在代理目标 $J_r$ 和真实目标 $J_u$ 上的表现，并与基线策略（如行为策略 $\pi_b$）进行比较。更进一步，为了保证安全，我们应该关注真实效用 $J_u$ 的**高[置信度](@entry_id:267904)下界（Lower Confidence Bound, LCB）**。如果一个新策略在代理奖励上看起来有显著提升（$\hat{J}_r(\pi_r) \gg \hat{J}_r(\pi_b)$），但其真实效用的置信下界反而低于基线策略（$L_u(\pi_r)  L_u(\pi_b)$），这便是一个强烈的“奖励操纵”和不安全信号，应立即警惕并阻止该策略的部署 [@problem_id:4855039]。