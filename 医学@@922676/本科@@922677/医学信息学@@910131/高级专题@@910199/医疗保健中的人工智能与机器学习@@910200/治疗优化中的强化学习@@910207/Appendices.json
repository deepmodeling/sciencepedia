{"hands_on_practices": [{"introduction": "在临床实践中，我们通常无法获知一个完美的模型，而是需要从已有的电子健康记录（EHR）中学习。因此，在部署新治疗策略前安全地评估其效果（即离策略评估，OPE）至关重要。本练习将向你展示如何使用强大的双重稳健（Doubly Robust）估计方法，利用历史数据来评估一个新策略的价值，这是在医疗领域应用强化学习的一项关键技能。[@problem_id:4855025]", "problem": "考虑一个强化学习（RL）在医学信息学中用于治疗优化的单步治疗决策问题。其临床背景是根据一个二元风险组指标 $x \\in \\{0,1\\}$ 来选择治疗脓毒症的抗生素，其中 $x=1$ 表示高风险，$x=0$ 表示低风险。一项离策略评估（OPE）任务是，使用由行为策略 $b$ 生成的回顾性电子健康记录（EHR）数据，来估计一个确定性目标策略 $\\pi$ 下的预期归一化临床改善。\n\n给定一个包含 $n=5$ 个独立患者事件的小数据集 $\\{(x_i,a_i,y_i)\\}_{i=1}^{5}$，其中 $x_i$ 是风险组，$a_i \\in \\{\\mathrm{T0},\\mathrm{T1}\\}$ 是所选的抗生素，$y_i \\in [0,1]$ 是观测到的归一化改善。确定性目标策略规定，对于高风险患者，$\\pi(\\mathrm{T1}\\mid x=1)=1$；对于低风险患者，$\\pi(\\mathrm{T0}\\mid x=0)=1$；并且 $\\pi(\\mathrm{T0}\\mid x=1)=\\pi(\\mathrm{T1}\\mid x=0)=0$。\n\n你还有一个根据历史数据训练的结果回归模型 $m(x,a)$，它提供以下动作价值预测：\n- 对于高风险 $x=1$：$m(1,\\mathrm{T1})=0.75$，$m(1,\\mathrm{T0})=0.55$。\n- 对于低风险 $x=0$：$m(0,\\mathrm{T0})=0.68$，$m(0,\\mathrm{T1})=0.52$。\n\n提供了以下事件和预先计算的重要性权重 $w_i=\\pi(a_i\\mid x_i)/b(a_i\\mid x_i)$：\n- 事件 $i=1$：$(x_1=1,\\ a_1=\\mathrm{T1},\\ y_1=0.80)$，其权重 $w_1=\\frac{10}{7}$。\n- 事件 $i=2$：$(x_2=1,\\ a_2=\\mathrm{T0},\\ y_2=0.50)$，其权重 $w_2=0$。\n- 事件 $i=3$：$(x_3=0,\\ a_3=\\mathrm{T0},\\ y_3=0.70)$，其权重 $w_3=\\frac{5}{3}$。\n- 事件 $i=4$：$(x_4=0,\\ a_4=\\mathrm{T1},\\ y_4=0.40)$，其权重 $w_4=0$。\n- 事件 $i=5$：$(x_5=1,\\ a_5=\\mathrm{T1},\\ y_5=0.60)$，其权重 $w_5=\\frac{10}{7}$。\n\n仅使用策略价值、重要性采样和全期望定律的基本定义，推导目标策略价值 $V(\\pi)$ 的单步双重稳健（DR）估计量的表达式，然后根据给定的数据集计算其经验估计值。清晰地陈述你计算出的值。将最终数值答案四舍五入到四位有效数字。将你的答案表示为一个无量纲实数。", "solution": "用户要求在单步离策略评估设置中，推导并计算目标策略价值 $V(\\pi)$ 的双重稳健（DR）估计量。\n\n首先，我们推导双重稳健估计量的一般形式。确定性目标策略 $\\pi$ 的价值是当根据该策略选择动作时所获得的预期奖励。它可以根据状态 $x$ 的分布和动作价值函数 $Q(x,a) = \\mathbb{E}[R \\mid X=x, A=a]$，使用全期望定律来定义，其中 $R$ 是奖励：\n$$V(\\pi) = \\mathbb{E}_{x \\sim p(x)}[Q(x, \\pi(x))]$$\n这里，$p(x)$ 是状态的边缘概率分布。我们给定一个结果回归模型 $m(x,a)$，它是真实动作价值函数 $Q(x,a)$ 的一个近似。\n\nDR 估计量结合了直接法（使用结果模型）和重要性采样（使用行为策略）。我们从一个关键的恒等式开始。考虑以下期望，该期望是针对数据生成（行为）分布计算的，其中状态 $x$ 从 $p(x)$ 中采样，动作 $a$ 从行为策略 $b(a|x)$ 中采样：\n$$\\mathbb{E}_{x, a \\sim b} \\left[ \\frac{\\pi(a|x)}{b(a|x)} (R - m(x,a)) \\right] = \\mathbb{E}_{x \\sim p(x)} \\left[ \\sum_{a} b(a|x) \\frac{\\pi(a|x)}{b(a|x)} (\\mathbb{E}[R|x,a] - m(x,a)) \\right]$$\n这里，$R$ 是奖励随机变量，其条件期望为 $Q(x,a)$。简化求和项：\n$$= \\mathbb{E}_{x \\sim p(x)} \\left[ \\sum_{a} \\pi(a|x) (Q(x,a) - m(x,a)) \\right]$$\n对于问题中给定的确定性策略 $\\pi$，当 $a=\\pi(x)$ 时 $\\pi(a|x)$ 为 $1$，否则为 $0$。因此，求和可以简化为单个项：\n$$= \\mathbb{E}_{x \\sim p(x)} [Q(x, \\pi(x)) - m(x, \\pi(x))]$$\n认识到 $\\mathbb{E}_{x}[Q(x, \\pi(x))] = V(\\pi)$，我们有：\n$$\\mathbb{E}_{x, a \\sim b} \\left[ \\frac{\\pi(a|x)}{b(a|x)} (R - m(x,a)) \\right] = V(\\pi) - \\mathbb{E}_{x \\sim p(x)}[m(x, \\pi(x))]$$\n重新整理此方程以求解 $V(\\pi)$，得到一个表达式，其经验平均值将构成我们的估计量：\n$$V(\\pi) = \\mathbb{E}_{x, a \\sim b} \\left[ \\frac{\\pi(a|x)}{b(a|x)} (R - m(x,a)) \\right] + \\mathbb{E}_{x \\sim p(x)}[m(x, \\pi(x))]$$\n经验双重稳健估计量 $\\hat{V}_{DR}(\\pi)$ 是此表达式的样本均值近似，基于在行为策略 $b$ 下收集的 $n$ 个样本 $\\{(x_i, a_i, y_i)\\}_{i=1}^n$ 的数据集计算得出。令 $w_i = \\frac{\\pi(a_i|x_i)}{b(a_i|x_i)}$ 为第 $i$ 个样本的重要性权重。该估计量为：\n$$\\hat{V}_{DR}(\\pi) = \\frac{1}{n} \\sum_{i=1}^{n} \\left[ w_i (y_i - m(x_i, a_i)) + m(x_i, \\pi(x_i)) \\right]$$\n该估计量是“双重稳健”的，因为如果结果模型是正确的（即 $m(x,a) = Q(x,a)$），或者用于计算权重 $w_i$ 的倾向性模型是正确的，它都能提供 $V(\\pi)$ 的无偏估计。\n\n现在，我们将针对给定的包含 $n=5$ 个患者事件的数据集计算该估计量的值。\n目标策略 $\\pi$ 是：$\\pi(x=1) = \\mathrm{T1}$ 和 $\\pi(x=0) = \\mathrm{T0}$。\n结果模型 $m(x, a)$ 的预测值为：\n$m(1, \\mathrm{T1}) = 0.75$，$m(1, \\mathrm{T0}) = 0.55$。\n$m(0, \\mathrm{T0}) = 0.68$，$m(0, \\mathrm{T1}) = 0.52$。\n\n我们为每个事件 $i \\in \\{1, 2, 3, 4, 5\\}$ 计算求和内的项 $T_i = w_i (y_i - m(x_i, a_i)) + m(x_i, \\pi(x_i))$。\n\n-   **事件 $i=1$**：$(x_1=1, a_1=\\mathrm{T1}, y_1=0.80), w_1=\\frac{10}{7}$。\n    对于 $x_1=1$，策略 $\\pi$ 指定的动作为 $\\pi(1)=\\mathrm{T1}$。\n    $T_1 = w_1 (y_1 - m(x_1, a_1)) + m(x_1, \\pi(x_1)) = \\frac{10}{7} (0.80 - m(1, \\mathrm{T1})) + m(1, \\mathrm{T1})$\n    $T_1 = \\frac{10}{7} (0.80 - 0.75) + 0.75 = \\frac{10}{7} (0.05) + 0.75 = \\frac{0.5}{7} + 0.75 = \\frac{1}{14} + \\frac{3}{4} = \\frac{2+21}{28} = \\frac{23}{28}$。\n\n-   **事件 $i=2$**：$(x_2=1, a_2=\\mathrm{T0}, y_2=0.50), w_2=0$。\n    对于 $x_2=1$，策略 $\\pi$ 指定的动作为 $\\pi(1)=\\mathrm{T1}$。\n    $T_2 = w_2 (y_2 - m(x_2, a_2)) + m(x_2, \\pi(x_2)) = 0 \\cdot (0.50 - m(1, \\mathrm{T0})) + m(1, \\mathrm{T1})$\n    $T_2 = 0 + 0.75 = 0.75 = \\frac{3}{4}$。\n\n-   **事件 $i=3$**：$(x_3=0, a_3=\\mathrm{T0}, y_3=0.70), w_3=\\frac{5}{3}$。\n    对于 $x_3=0$，策略 $\\pi$ 指定的动作为 $\\pi(0)=\\mathrm{T0}$。\n    $T_3 = w_3 (y_3 - m(x_3, a_3)) + m(x_3, \\pi(x_3)) = \\frac{5}{3} (0.70 - m(0, \\mathrm{T0})) + m(0, \\mathrm{T0})$\n    $T_3 = \\frac{5}{3} (0.70 - 0.68) + 0.68 = \\frac{5}{3} (0.02) + 0.68 = \\frac{0.1}{3} + 0.68 = \\frac{1}{30} + \\frac{68}{100} = \\frac{1}{30} + \\frac{17}{25} = \\frac{5+102}{150} = \\frac{107}{150}$。\n\n-   **事件 $i=4$**：$(x_4=0, a_4=\\mathrm{T1}, y_4=0.40), w_4=0$。\n    对于 $x_4=0$，策略 $\\pi$ 指定的动作为 $\\pi(0)=\\mathrm{T0}$。\n    $T_4 = w_4 (y_4 - m(x_4, a_4)) + m(x_4, \\pi(x_4)) = 0 \\cdot (0.40 - m(0, \\mathrm{T1})) + m(0, \\mathrm{T0})$\n    $T_4 = 0 + 0.68 = \\frac{17}{25}$。\n\n-   **事件 $i=5$**：$(x_5=1, a_5=\\mathrm{T1}, y_5=0.60), w_5=\\frac{10}{7}$。\n    对于 $x_5=1$，策略 $\\pi$ 指定的动作为 $\\pi(1)=\\mathrm{T1}$。\n    $T_5 = w_5 (y_5 - m(x_5, a_5)) + m(x_5, \\pi(x_5)) = \\frac{10}{7} (0.60 - m(1, \\mathrm{T1})) + m(1, \\mathrm{T1})$\n    $T_5 = \\frac{10}{7} (0.60 - 0.75) + 0.75 = \\frac{10}{7} (-0.15) + 0.75 = \\frac{-1.5}{7} + 0.75 = -\\frac{3}{14} + \\frac{3}{4} = \\frac{-6+21}{28} = \\frac{15}{28}$。\n\n现在，我们将这些项相加：\n$\\sum_{i=1}^{5} T_i = \\frac{23}{28} + \\frac{3}{4} + \\frac{107}{150} + \\frac{17}{25} + \\frac{15}{28}$\n$\\sum_{i=1}^{5} T_i = \\left(\\frac{23}{28} + \\frac{15}{28}\\right) + \\frac{3}{4} + \\left(\\frac{107}{150} + \\frac{17 \\cdot 6}{25 \\cdot 6}\\right)$\n$\\sum_{i=1}^{5} T_i = \\frac{38}{28} + \\frac{3}{4} + \\left(\\frac{107}{150} + \\frac{102}{150}\\right)$\n$\\sum_{i=1}^{5} T_i = \\frac{19}{14} + \\frac{3}{4} + \\frac{209}{150}$\n为了对这些分数求和，我们找到一个公分母。$14=2 \\cdot 7$、$4=2^2$ 和 $150=2 \\cdot 3 \\cdot 5^2$ 的最小公倍数是 $2^2 \\cdot 3 \\cdot 5^2 \\cdot 7 = 4 \\cdot 3 \\cdot 25 \\cdot 7 = 2100$。\n$\\sum_{i=1}^{5} T_i = \\frac{19 \\cdot 150}{2100} + \\frac{3 \\cdot 525}{2100} + \\frac{209 \\cdot 14}{2100}$\n$\\sum_{i=1}^{5} T_i = \\frac{2850}{2100} + \\frac{1575}{2100} + \\frac{2926}{2100} = \\frac{2850 + 1575 + 2926}{2100} = \\frac{7351}{2100}$\n\n最后，我们将总和除以 $n=5$ 来计算估计量 $\\hat{V}_{DR}(\\pi)$：\n$\\hat{V}_{DR}(\\pi) = \\frac{1}{5} \\left( \\frac{7351}{2100} \\right) = \\frac{7351}{10500}$\n换算成小数，结果是 $0.700095238...$。\n四舍五入到四位有效数字，我们看第五位有效数字。由于 $9 \\ge 5$，我们将第四位数字向上取整。\n$\\hat{V}_{DR}(\\pi) \\approx 0.7001$。", "answer": "$$\\boxed{0.7001}$$", "id": "4855025"}, {"introduction": "真实的医疗决策需要在最大化疗效的同时，平衡成本、副作用和风险等多种因素。约束马尔可夫决策过程（CMDP）为这种多目标优化问题提供了形式化框架。通过本练习，你将学习如何在一个带成本约束的场景中，找到既能最大化治疗回报，又不超过预设成本限制的最优随机策略。[@problem_id:4855060]", "problem": "一家医院正在使用强化学习 (RL)为一名慢性病患者设计治疗策略，该患者的健康状况在几天内保持稳定。将此问题建模为一个约束马尔可夫决策过程 (CMDP)，其定义如下：CMDP 是一个马尔可夫决策过程 (MDP)，带有一个额外的成本函数和对预期折扣累积成本的约束。一个 MDP 由一个状态集 $S$、一个动作集 $A$、转移概率 $P$、一个奖励函数 $r(s,a)$ 和一个折扣因子 $\\gamma$ 组成；在 CMDP 中，还有一个成本函数 $d(s,a)$ 和一个对预期折扣成本的允许界限。\n\n假设单个状态 $s \\in S$ 代表稳定的健康状况，两个动作 $A = \\{a_{H}, a_{L}\\}$ 分别代表高强度和低强度治疗。每个决策的奖励和成本定义为 $r(s,a_{H}) = 5$，$r(s,a_{L}) = 2$，$d(s,a_{H}) = 3$ 和 $d(s,a_{L}) = 1$。转移是确定性的，在任何动作之后状态都保持为 $s$，因此对于所有 $t \\in \\mathbb{N}$，时间 $t$ 的状态以概率 $1$ 为 $s$。折扣因子为 $\\gamma = 0.9$。CMDP 约束要求在无限时间范围内的预期折扣累积成本最多为 $D_{\\max} = 12$。\n\n一个平稳随机策略 $\\pi$ 以概率 $\\pi(a_{H} \\mid s) = x$ 选择 $a_{H}$，以概率 $\\pi(a_{L} \\mid s) = 1 - x$ 选择 $a_{L}$，其中 $x \\in [0,1]$ 待确定。目标是选择 $\\pi$ 以在满足 CMDP 成本约束的条件下最大化预期折扣累积奖励。\n\n从 MDP 中预期折扣回报的定义和有界奖励与成本的几何级数出发，推导 $x$ 的可行集，并计算在满足成本约束的同时最大化预期折扣累积奖励的约束最优策略 $\\pi^{*}$。将您的最终策略表示为行矩阵，顺序为 $(\\pi^{*}(a_{H} \\mid s), \\pi^{*}(a_{L} \\mid s))$，使用精确值。不需要四舍五入，概率也不需要单位。", "solution": "问题是为一个具有单个状态的约束马尔可夫决策过程 (CMDP) 找到一个最优的平稳随机策略 $\\pi^*$。该策略由选择高强度动作 $a_H$ 的概率 $x \\in [0,1]$ 定义。目标是在满足预期折扣累积成本约束的条件下，最大化预期折扣累积奖励。\n\n给定的参数是：\n- 状态空间：$S = \\{s\\}$\n- 动作空间：$A = \\{a_{H}, a_{L}\\}$\n- 奖励：$r(s,a_{H}) = 5$, $r(s,a_{L}) = 2$\n- 成本：$d(s,a_{H}) = 3$, $d(s,a_{L}) = 1$\n- 折扣因子：$\\gamma = 0.9$\n- 成本约束界限：$D_{\\max} = 12$\n- 策略：$\\pi(a_{H} \\mid s) = x$, $\\pi(a_{L} \\mid s) = 1 - x$\n\n由于状态始终为 $s$，策略 $\\pi$ 下的预期即时奖励在每个时间步都是恒定的。这个预期奖励，记为 $E_{\\pi}[r]$，计算如下：\n$$\nE_{\\pi}[r] = \\pi(a_{H} \\mid s) \\cdot r(s,a_{H}) + \\pi(a_{L} \\mid s) \\cdot r(s,a_{L})\n$$\n代入给定值：\n$$\nE_{\\pi}[r] = x \\cdot 5 + (1-x) \\cdot 2 = 5x + 2 - 2x = 3x + 2\n$$\n总预期折扣累积奖励，或价值函数 $V^{\\pi}(s)$，是在无限时间范围内的折扣预期奖励之和：\n$$\nV^{\\pi}(s) = \\sum_{t=0}^{\\infty} \\gamma^t E_{\\pi}[r] = (3x + 2) \\sum_{t=0}^{\\infty} \\gamma^t\n$$\n该和是一个几何级数 $\\sum_{t=0}^{\\infty} \\gamma^t = \\frac{1}{1-\\gamma}$ (对于 $|\\gamma|  1$)。给定 $\\gamma = 0.9$，该和为 $\\frac{1}{1-0.9} = \\frac{1}{0.1} = 10$。\n因此，价值函数为：\n$$\nV^{\\pi}(s) = (3x + 2) \\cdot 10 = 30x + 20\n$$\n类似地，我们计算策略 $\\pi$ 下的预期即时成本，记为 $E_{\\pi}[d]$：\n$$\nE_{\\pi}[d] = \\pi(a_{H} \\mid s) \\cdot d(s,a_{H}) + \\pi(a_{L} \\mid s) \\cdot d(s,a_{L})\n$$\n代入给定值：\n$$\nE_{\\pi}[d] = x \\cdot 3 + (1-x) \\cdot 1 = 3x + 1 - x = 2x + 1\n$$\n总预期折扣累积成本 $D^{\\pi}(s)$ 是在无限时间范围内的折扣预期成本之和：\n$$\nD^{\\pi}(s) = \\sum_{t=0}^{\\infty} \\gamma^t E_{\\pi}[d] = (2x + 1) \\sum_{t=0}^{\\infty} \\gamma^t = (2x + 1) \\frac{1}{1-\\gamma}\n$$\n使用 $\\frac{1}{1-\\gamma} = 10$，成本函数为：\n$$\nD^{\\pi}(s) = (2x + 1) \\cdot 10 = 20x + 10\n$$\n问题包含一个对总预期折扣成本的约束：$D^{\\pi}(s) \\le D_{\\max}$。\n代入 $D^{\\pi}(s)$ 的表达式和 $D_{\\max}$ 的值：\n$$\n20x + 10 \\le 12\n$$\n我们求解这个关于 $x$ 的不等式：\n$$\n20x \\le 12 - 10\n$$\n$$\n20x \\le 2\n$$\n$$\nx \\le \\frac{2}{20} = \\frac{1}{10}\n$$\n变量 $x$ 代表一个概率，因此它也必须满足 $0 \\le x \\le 1$。将此与成本函数的约束相结合，$x$ 的可行集为：\n$$\nx \\in [0, 1] \\cap (-\\infty, \\frac{1}{10}] = [0, \\frac{1}{10}]\n$$\n目标是在 $x$ 属于可行集 $[0, \\frac{1}{10}]$ 的约束下，最大化价值函数 $V^{\\pi}(s) = 30x + 20$。\n目标函数 $V^{\\pi}(s)$ 是一个关于 $x$ 的线性函数，其斜率为正数 $30$。一个正斜率的线性函数是单调递增的。因此，它在闭区间上的最大值出现在区间的右端点。\n在可行集 $[0, \\frac{1}{10}]$ 中，$x$ 的最大值为 $x = \\frac{1}{10}$。\n$x$ 的这个最优值记为 $x^*$。\n$$\nx^* = \\frac{1}{10}\n$$\n最优策略 $\\pi^*$ 由这个 $x^*$ 的值确定：\n$$\n\\pi^*(a_{H} \\mid s) = x^* = \\frac{1}{10}\n$$\n$$\n\\pi^*(a_{L} \\mid s) = 1 - x^* = 1 - \\frac{1}{10} = \\frac{9}{10}\n$$\n问题要求将最终策略表示为行矩阵，顺序为 $(\\pi^*(a_{H} \\mid s), \\pi^*(a_{L} \\mid s))$。\n因此，最优策略是 $\\left( \\frac{1}{10}, \\frac{9}{10} \\right)$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{10}  \\frac{9}{10}\n\\end{pmatrix}\n}\n$$", "id": "4855060"}]}