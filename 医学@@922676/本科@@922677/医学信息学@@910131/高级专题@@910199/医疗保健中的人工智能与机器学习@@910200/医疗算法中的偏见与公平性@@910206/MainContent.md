## 引言
算法在医疗保健领域的应用日益广泛，有望提高效率和决策准确性，但同时也带来了严峻挑战：它们可能无意中延续甚至放大社会偏见，威胁到患者福祉和健康公平。尽管AI的潜力广受关注，但对于偏见如何渗入系统、如何定义和度量“公平性”及其内在的复杂权衡，许多人仍缺乏系统的理解。本文旨在填补这一鸿沟，为读者提供一个全面的分析框架。

我们将在“原理与机制”一章中，剖析偏见的来源并建立起度量公平性的数学基础。随后，在“应用与跨学科连接”一章中，我们将探讨这些概念在临床决策、[资源分配](@entry_id:136615)等真实场景中的影响，并将其与伦理及法律原则联系起来。最后，通过“动手实践”中的具体练习，读者将有机会亲手应用所学知识。

这段旅程将使您具备批判性评估和应对医疗[算法公平性](@entry_id:143652)挑战的能力，确保技术进步能够服务于所有人的健康。

## 原理与机制

本章旨在深入探讨医疗保健领域中[算法偏见](@entry_id:637996)的根本原理与作用机制。在前一章介绍背景之后，我们将系统性地剖析偏见如何从数据生成过程的源头渗入，如何被算法模型自身所引入或放大，以及在临床部署的复杂动态中所产生的后果。此外，我们将建立一个严格的数学框架来定义和度量公平性，并探索超越传统统计公平性概念的个体化与因果公平性思想。本章的目标是为读者提供一套精确的语言和分析工具，用以批判性地评估和理解医疗算法中的公平性挑战。

### 偏见的分类：算法不公平性的来源

[算法偏见](@entry_id:637996)并非凭空产生，其根源深植于数据本身以及我们与数据互动的方式。为了系统地理解这些来源，我们可以将偏见追溯到数据生命周期的不同阶段。一个有效的分类法区分了数据生成过程中的偏差和模型构建与部署过程中的偏差。

#### 选择偏见

**选择偏见 (Selection Bias)** 源于用于训练算法的数据样本未能代表其最终将要应用的整体目标人群。当数据被纳入[训练集](@entry_id:636396)的概率与某些个体特征、临床状况或最终结果相关时，就会出现这种偏差。其结果是，模型从一个“扭曲”的世界观中学习，并可能在应用于更广泛的现实世界时表现出系统性的错误。在因果图模型中，这通常表现为存在从受保护属性 $A$、真实临床状态 $X^*$ 或真实结果 $Y^*$ 指向选择指示变量 $S$（其中 $S=1$ 表示个体被纳入样本）的箭头。例如，如果只有表现出明显症状的患者（$Y^* \rightarrow S$）或能够更好地利用医疗资源的人群（$A \rightarrow S$）才会被记录在案，那么训练数据就会产生偏差[@problem_id:4824163]。

在处理电子健康记录（EHR）时，一个普遍存在的选择偏见来源是**缺失数据 (missing data)**。当分析仅限于所有变量都完整的记录（即**完整案例分析 (complete-case analysis)**）时，我们实际上是在一个可能非随机的子集上进行分析。这种选择的非随机性会如何影响公平性评估，取决于数据缺失的机制[@problem_id:4824188]：
- **[完全随机缺失](@entry_id:170286) (Missing Completely At Random, MCAR)**：如果数据的缺失概率与任何变量（无论观察到与否）都无关，那么完整案例分析样本虽然变小了，但仍然是总体的无偏代表。在这种理想情况下，对[公平性指标](@entry_id:634499)（如[均等化赔率](@entry_id:637744)）的估计是无偏的。
- **[随机缺失](@entry_id:168632) (Missing At Random, MAR)**：如果数据的缺失概率仅依赖于已观测到的其他变量，而非缺失值本身，情况就变得复杂。例如，在一个评估分类器公平性的场景中，我们需要保证在给定真实结果 $Y$ 和受保护属性 $A$ 的条件下，特征 $X$ 的分布在完整案例中与在整个群体中是相同的。这要求成为完整案例的概率不依赖于 $X$ 本身。如果特征 $X$ 的缺失仅取决于结果 $Y$（例如，病情更重的患者更有可能完成所有检查），那么在评估以 $Y$ 为条件的[公平性指标](@entry_id:634499)（如[均等化赔率](@entry_id:637744)）时，完整案例分析仍然是无偏的。然而，如果缺失概率取决于 $X$ 本身（例如，某些实验室检查结果在极端值时更易缺失），则会引入偏见。
- **[非随机缺失](@entry_id:163489) (Missing Not At Random, MNAR)**：如果数据的缺失概率依赖于缺失值本身（例如，血压极高时，由于设备故障或记录员回避，该值更可能缺失），那么完整案例分析几乎总会产生有偏估计。这将导致对模型在不同群体间的真实表现产生误判。

#### 测量偏见

**测量偏见 (Measurement Bias)** 发生在当我们用来量化某一概念的指标（即观测变量）并不能同等地代表所有群体的潜在真实状态时。这也被称为**差异性测量误差 (differential measurement error)**。形式上，如果我们将观测变量 $X$ 视为真实临床状态 $X^*$ 的函数，即 $X = g(X^*, \epsilon)$，当这个函数 $g$ 或误差项 $\epsilon$ 的分布因受保护属性 $A$ 的不同而系统性地变化时，即 $X = g(X^*, A, \epsilon)$，测量偏见就产生了[@problem_id:4824163]。

一个在医疗保健领域极具说明性的例子是使用**医疗成本作为健康需求的代理变量**。许多风险分层系统被训练来预测未来的医疗支出，其隐含假设是高成本等同于高疾病负担。然而，这一假设是有缺陷的，因为它忽略了医疗服务利用中存在的结构性不平等。

我们可以通过一个简化的结构模型来精确阐述这个问题[@problem_id:4824156]。假设一个患者的真实疾病负担（morbidity）为 $Y$，而已实现的医疗成本为 $\tilde{Y}$。成本的产生可以模型化为 $\tilde{Y} = c H Y + \varepsilon$，其中 $c$ 是反映服务单价的常数，$H \in [0,1]$ 代表**医疗服务可及性**（即临床上需要的服务实际被提供的比例），而 $\varepsilon$ 是均值为零的随机噪声。

现在，我们考虑一个情景：一个历史上服务不足的群体 ($A=1$) 和一个服务充分的群体 ($A=0$)，他们的**真实平均疾病负担是相同的**，即 $Y \perp A$，因此 $\mathbb{E}[Y | A=a] = \mathbb{E}[Y]$。然而，由于结构性障碍，服务不足群体的医疗可及性较低，即 $\mathbb{E}[H | A=1]  \mathbb{E}[H | A=0]$。

在这种情况下，让我们计算不同群体中观测到的平均成本。利用[期望的线性](@entry_id:273513)和独立性假设，我们可以推导出：
$$
\mathbb{E}[\tilde{Y} | A=a] = \mathbb{E}[c H Y + \varepsilon | A=a] = c \mathbb{E}[H Y | A=a] + \mathbb{E}[\varepsilon | A=a]
$$
由于 $Y$ 与 $H$ 在给定 $A$ 时是独立的，并且 $\varepsilon$ 与所有变量都独立，上式简化为：
$$
\mathbb{E}[\tilde{Y} | A=a] = c \mathbb{E}[H | A=a] \mathbb{E}[Y | A=a] = c \mathbb{E}[H | A=a] \mathbb{E}[Y]
$$
这个结果清晰地表明，即使两个群体的真实平均健康状况完全相同，由于医疗可及性的差异，他们的平均医疗成本也会不同。服务不足的群体将表现出更低的平均成本，算法会错误地将他们识别为“更健康”或“低风险”的群体，从而在资源分配中处于不利地位。代理变量 $\tilde{Y}$ 与真实目标 $Y$ 之间的期望差距为：
$$
\mathbb{E}[\tilde{Y} | A=a] - \mathbb{E}[Y | A=a] = \mathbb{E}[Y] (c \mathbb{E}[H | A=a] - 1)
$$
对于服务不足的群体，如果 $c \mathbb{E}[H | A=1]  1$，这个差距将是负数，意味着其健康需求被系统性地低估了。

#### 算法与部署中的偏见

除了源于数据的偏见，算法本身以及其在真实世界中的部署方式也能引入或放大不公平性。

**[算法偏见](@entry_id:637996) (Algorithmic Bias)** 指的是由建模过程本身引入的偏见。即使我们拥有完美无偏的数据，在选择模型架构（例如，[线性模型](@entry_id:178302) vs. 深度神经网络）、定义目标函数（例如，最小化[对数损失](@entry_id:637769)）或使用特定正则化方法时，也可能产生在不同群体间表现不均的预测器 $\hat{R}(X)$。例如，一个旨在最小化总体平均误差的算法，可能会通过在多数群体上达到极高精度而在少数群体上表现很差来实现其目标。这种群体间条件期望损失 $\mathbb{E}[\ell(Y, \hat{R}(X)) | A=a]$ 的差异，就是[算法偏见](@entry_id:637996)的直接体现[@problem_id:4824163]。

**自动化偏见 (Automation Bias)** 是一种在模型部署后，发生于人机交互环节的认知偏见。它描述了人类决策者（如临床医生）过度依赖自动化系统输出的倾向。医生的决策 $D$ 是模型建议 $\hat{R}(X)$、其他临床信息以及医生自身认知与专业判断 $H$ 的函数。当医生在决策中给予 $\hat{R}(X)$ 过高权重，甚至忽略了与之矛盾的其他证据或自己的直觉时，自动化偏见就产生了。这是一种人机协作系统的失败，可能导致系统性的决策失误[@problem_id:4824163]。

更进一步，算法的部署并非一次性事件，而是一个持续的动态过程。已部署的模型会影响临床决策，而这些决策反过来又会改变患者的健康轨迹和未来的数据记录，从而形成一个**反馈循环 (feedback loop)**。这种循环可能导致偏见的**放大或固化**。

我们可以通过一个动态系统模型来理解这一机制[@problem_id:4824154]。设 $X_t$ 为 $t$ 时刻的疾病负担，$H_t$ 为分配的医疗资源强度，$\hat{Y}_t$ 为[模型风险](@entry_id:136904)评分。一个简化的[线性模型](@entry_id:178302)可以描述它们之间的关系：
1.  模型评分：$\mathbb{E}[\hat{Y}_t | A=a] = \theta \mathbb{E}[X_t | A=a] + \phi a$ (模型可能本身就存在对群体 $a$ 的直接偏见 $\phi$)
2.  [资源分配](@entry_id:136615)：$\mathbb{E}[H_t | A=a] = \alpha \mathbb{E}[\hat{Y}_t | A=a] + \beta a$ ([资源分配](@entry_id:136615)也可能存在直接偏见 $\beta$)
3.  疾病演化：$\mathbb{E}[X_{t+1} | A=a] = \rho \mathbb{E}[X_t | A=a] - \kappa \mathbb{E}[H_t | A=a] + \xi a$ (资源 $H_t$ 可以改善下一时刻的健康状况 $X_{t+1}$)

定义群体间在 $t$ 时刻的疾病负担差异为 $m_t \equiv \mathbb{E}[X_t | A=1] - \mathbb{E}[X_t | A=0]$。通过代数替换，我们可以推导出该差异随时间演化的[差分方程](@entry_id:262177)：
$$
m_{t+1} = (\rho - \kappa\alpha\theta) m_t + (\xi - \kappa\alpha\phi - \kappa\beta)
$$
这个方程揭示了差异的动态演化。系数 $(\rho - \kappa\alpha\theta)$ 决定了系统的稳定性。如果其绝对值大于1，初始的微小差异 $m_0$ 将被指数级放大，导致群体间的健康差距随着时间的推移而急剧扩大。即使模型本身是“公平的”（例如 $\phi=0$），[资源分配](@entry_id:136615)中的偏见（$\beta \neq 0$）或疾病自然演化中的差异（$\xi \neq 0$）也可能通过这个[反馈系统](@entry_id:268816)被放大。这说明，评估一个算法的公平性不能孤立地看待其本身，而必须将其置于其所嵌入的社会技术系统中进行考察。

### 公平性的形式化与度量：群体[公平性指标](@entry_id:634499)

为了从“偏见”这一模糊概念转向可操作的评估，我们需要精确的数学定义来量化公平性。这些定义通常被称为**群体公平性 (group fairness)** 标准，因为它们关注的是模型在不同受保护群体（由属性 $A$ 定义）之间的统计表现。设 $Y$ 为真实结果，$\hat{Y}$ 为模型的二元预测，$\hat{R}$ 为连续风险评分。

以下是三种主要的群体公平性范式，它们可以用条件独立性来简洁地表述[@problem_id:4824145]：

1.  **独立性 (Independence)**：这类标准要求模型的预测独立于受保护属性 $A$。
    - **人口统计学平等 (Demographic Parity)**：要求不同群体获得阳性预测的比例相同。即 $\hat{Y} \perp A$，或等价地，$P(\hat{Y}=1 | A=a) = P(\hat{Y}=1 | A=b)$ 对所有群体 $a, b$ 成立。这个标准简单直观，但它忽略了不同群体间真实基础发病率可能存在的差异。

2.  **分离 (Separation)**：这类标准要求在给定真实结果 $Y$ 的条件下，模型的预测独立于受保护属性 $A$。
    - **[均等化赔率](@entry_id:637744) (Equalized Odds)**：要求在所有真实结果为阳性（$Y=1$）的患者中，以及在所有真实结果为阴性（$Y=0$）的患者中，不同群体获得阳性预测的比例分别相同。即 $\hat{Y} \perp A \mid Y$。这等价于要求所有群体的**[真阳性率](@entry_id:637442) (True Positive Rate, TPR)** 和**假阳性率 (False Positive Rate, FPR)** 都相等：
      $P(\hat{Y}=1 | Y=1, A=a) = P(\hat{Y}=1 | Y=1, A=b)$
      $P(\hat{Y}=1 | Y=0, A=a) = P(\hat{Y}=1 | Y=0, A=b)$
    - **[机会均等](@entry_id:637428) (Equality of Opportunity)**：这是[均等化赔率](@entry_id:637744)的一个宽松版本，仅要求真阳性率在不同群体间相等，即 $\hat{Y} \perp A \mid Y=1$。它关注的是模型是否平等地识别出了所有需要被识别出的病例。

3.  **充分性 (Sufficiency)**：这类标准要求在给定模型预测 $\hat{Y}$ 或评分 $\hat{R}$ 的条件下，真实结果 $Y$ 独立于受保护属性 $A$。
    - **预测性平等 (Predictive Parity)**：要求在所有被模型预测为阳性（$\hat{Y}=1$）的患者中，不同群体真实为阳性的比例相同。即 $Y \perp A \mid \hat{Y}=1$。这等价于要求所有群体的**阳性预测值 (Positive Predictive Value, PPV)** 相等：$P(Y=1 | \hat{Y}=1, A=a) = P(Y=1 | \hat{Y}=1, A=b)$。
    - **校准 (Calibration)**：对于输出连续风险评分 $\hat{R}$ 的模型，群体校准要求对于任意给定的风险评分 $r$，不同群体的患者具有该风险的真实概率都等于 $r$。即 $P(Y=1 | \hat{R}=r, A=a) = r$ 对所有群体 $a$ 成立。一个经过良好校准的模型，其预测值可以被直接解释为概率。群体校准是充分性标准的一种体现。

#### 不可兼得性定理

这些公平性定义看似都很有道理，但一个深刻的理论结果是，它们之间存在根本性的冲突。由 Kleinberg、Mullainathan 和 Raghavan 提出的**不可兼得性定理 (impossibility theorem)** 指出，对于一个不完美的分类器，当不同群体的基础发病率不同时，满足分离标准（如[均等化赔率](@entry_id:637744)）和满足充分性标准（如预测性平等）是**不可能同时实现**的[@problem_id:4824145]。

我们可以通过贝叶斯定理直观地理解这个冲突[@problem_id:4824178]。一个群体的阳性预测值 (PPV) 可以表示为：
$$
\text{PPV}_a = P(Y=1 | \hat{Y}=1, A=a) = \frac{P(\hat{Y}=1 | Y=1, A=a) P(Y=1 | A=a)}{P(\hat{Y}=1 | A=a)}
$$
将分子中的项替换为 TPR 和群体发病率 $\pi_a$，并将分母用[全概率公式](@entry_id:194231)展开，我们得到：
$$
\text{PPV}_a = \frac{\text{TPR}_a \cdot \pi_a}{\text{TPR}_a \cdot \pi_a + \text{FPR}_a \cdot (1 - \pi_a)}
$$
现在假设一个分类器满足[均等化赔率](@entry_id:637744)，即所有群体的 TPR 和 FPR 都相等（$\text{TPR}_a = \text{TPR}$, $\text{FPR}_a = \text{FPR}$）。然而，不同群体的疾病发病率不同（$\pi_a \neq \pi_b$）。从上式可以清晰地看到，$\text{PPV}_a$ 是发病率 $\pi_a$ 的函数。只要 $\pi_a$ 不同，即使 TPR 和 FPR 完全相等，不同群体的 PPV 也必然会不同。

例如，考虑一个分类器，其 $\text{TPR} = 0.88$ 和 $\text{FPR} = 0.12$。对于发病率 $\pi_0 = 0.04$ 的群体 A，其 $\text{PPV}_0 \approx 0.23$。而对于发病率 $\pi_1 = 0.16$ 的群体 B，其 $\text{PPV}_1 \approx 0.58$。这意味着，对于群体 A 的患者，一个阳性警报只有 $23\%$ 的概率是准确的；而对于群体 B，这个概率上升到了 $58\%$。尽[管模型](@entry_id:140303)在技术上满足了[均等化赔率](@entry_id:637744)，但其预测结果的临床意义和可靠性在不同群体间却存在巨大差异。这迫使我们必须在不同的公平性目标之间做出权衡和选择。

#### 交叉性公平

在评估群体公平时，仅仅考察单一受保护属性（如只看种族或只看性别）是远远不够的。这样做可能会掩盖在多个属性[交叉形成](@entry_id:195958)的特定子群体中所存在的严重不公。**交叉性公平 (Intersectional Fairness)** 的理念要求我们必须评估模型在这些交叉子群体中的表现。

例如，一个模型在男性与女性之间，以及在黑人与白人之间可能都表现出相似的性能。然而，它可能对黑人女性的表现远差于其他任何群体。为了揭示这一点，我们需要定义交叉子群体 $G$，例如，由（性别，种族）对构成的集合：$G = \{(\text{女, 黑}), (\text{女, 白}), (\text{男, 黑}), (\text{男, 白})\}$。然后，我们对每一个子群体 $g \in G$ 分别计算其性能指标。

例如，要评估交叉性下的[均等化赔率](@entry_id:637744)，我们需要计算每个子群体的 TPR 和 FPR，然后找出所有子群体对之间的最大差异[@problem_id:4824143]。假设我们计算出四个子群体的 TPR 分别为 $\{0.70, 0.75, 0.75, 0.70\}$，FPR 分别为 $\{0.20, 0.10, 0.20, 0.10\}$。
- TPR 的最大差异为 $\max(\{0.70, 0.75\}) - \min(\{0.70, 0.75\}) = 0.05$。
- FPR 的最大差异为 $\max(\{0.10, 0.20\}) - \min(\{0.10, 0.20\}) = 0.10$。
这个分析表明，在[假阳性](@entry_id:635878)方面存在着更大的群体间不平等。交叉性分析为我们提供了一个更精细的视角来发现和定位[算法偏见](@entry_id:637996)。

### 超越群体统计：个体与因果公平性

群体[公平性指标](@entry_id:634499)虽然有用，但它们本质上是统计性的，关注的是群体的平均表现。这可能无法满足伦理上的一个核心直觉：相似的个体应该被相似地对待。此外，[统计相关性](@entry_id:267552)本身无法告诉我们一个模型为何会产生差异化的预测。为此，我们需要更精密的个体公平性和因果公平性概念。

#### 个体公平性

**个体公平性 (Individual Fairness)** 的核心思想是，如果两个人在所有与决策相关的“临床意义上”的方面都相似，那么模型对他们的预测也应该是相似的。这个原则可以通过数学上的**[利普希茨连续性](@entry_id:142246) (Lipschitz continuity)** 来形式化[@problem_id:4824181]。具体来说，我们要求对于任意两个患者的特征向量 $x$ 和 $x'$，模型的风险评分 $\hat{R}$ 必须满足：
$$
|\hat{R}(x) - \hat{R}(x')| \leq L \cdot d(x, x')
$$
其中 $L$ 是一个常数，而 $d(x, x')$ 是一个度量两个患者“相似性”的距离函数。

这个定义的美妙之处在于它将伦理考量转移到了如何定义距离函数 $d$ 上。一个有意义的 $d$ 必须满足两个条件：(1) 它必须在临床上是合理的；(2) 它必须排除那些我们认为在伦理上不应影响决策的因素。

例如，一个朴素的欧几里得距离 $\|x - x'\|_2$ 是不合适的，因为它会错误地将社会身份（如种族）或与其高度相关的代理变量（如邮政编码）也计入“相似性”的考量，并且它无法处理不同临床特征（如血压和肌酐）具有不同单位和临床重要性的问题。

一个更严谨的方法是构建一个只包含临床相关特征的加权距离。我们可以利用**最小临床重要差异 (Minimal Clinically Important Difference, MCID)** 的概念，即临床医生和患者认为一个变量的“有意义的”最小变化量。通过用 MCID 对每个临床特征进行归一化，我们可以构建一个在临床意义上标准化的[距离度量](@entry_id:636073)。例如，一个加权的[欧几里得距离](@entry_id:143990)可以定义为：
$$
d(x,x') = \sqrt{\sum_{i \in \mathcal{F}_{\text{clinical}}} \left( \frac{x_i - x'_i}{\text{MCID}_i} \right)^2}
$$
其中 $\mathcal{F}_{\text{clinical}}$ 是临床相关特征的集合。这个度量明确地将受保护属性排除在外，并确保距离的变化与临床重要性成正比。通过强制模型对这个度量保持利普希茨连续，我们就确保了临床上相似的个体能得到相似的风险评分，从而在个体层面实现了公平。

#### 因果公平性

因果公平性理论试图解决一个更深层次的问题：一个预测与受保护属性之间的关联，哪些是“正当的”，哪些是“不正当的”？例如，如果某个受保护属性与某种疾病的遗传易感性相关，那么模型利用这种关联可能在临床上是合理的。但如果这种关联是通过结构性歧视导致医疗资源不平等而产生的，那么模型利用这种关联就是不公平的。

**路径特异性公平 (Path-Specific Fairness)** 提供了一个框架来区分这些不同的因果路径[@problem_id:4824165]。在一个结构因果模型（Structural Causal Model, SCM）中，我们可以描绘出从受保护属性 $A$ 到预测 $\hat{Y}$ 的所有因果链。例如：
- 一条“正当”的生物学路径可能是：$A \rightarrow \text{生物学中介} (B) \rightarrow \text{临床风险} (R) \rightarrow \text{特征} (X) \rightarrow \hat{Y}$。
- 一条“不正当”的社会学路径可能是：$A \rightarrow \text{社会中介} (S) \rightarrow \text{特征} (X) \rightarrow \hat{Y}$，其中 $S$ 可能代表医疗可及性或保险状况。

路径特异性公平的目标是构建一个只对正当路径敏感，而对不正当路径不敏感的预测器。这需要复杂的因果推断技术来分解和阻断特定的因果效应。

一个更严格的因果公平性概念是**反事实公平 (Counterfactual Fairness)**。其核心思想是[@problem_id:4824140]：对于任何一个个体，如果我们能在一个“反事实世界”里改变其受保护属性 $A$，而保持其所有其他背景特征 $U$（即那些不受 $A$ 影响的内在特质）不变，那么模型对他的预测应该保持不变。形式上，这要求：
$$
\hat{Y}_{A \leftarrow a}(U) = \hat{Y}_{A \leftarrow a'}(U)
$$
其中 $\hat{Y}_{A \leftarrow a}(U)$ 表示在将 $A$ 强制干预为值 $a$ 的反事实世界里，对背景为 $U$ 的个体的预测。

这个定义非常强大，因为它要求模型完全忽略由 $A$ 引起的所有直接和间接效应。实现反事实公平的一种方法是，构建一个只依赖于那些在因果图中不属于 $A$ 的“后代”的变量的预测器。例如，在一个 SCM 中，我们将真实结果 $Y$ 分解为依赖于 $A$ 的[部分和](@entry_id:162077)不依赖于 $A$ 的部分。假设我们有如下[结构方程](@entry_id:274644)：
$$
X = 0.8 Z + 1.2 A + U_X
$$
$$
Y = 2.0 X + 0.1 Z + U_Y
$$
其中 $Z$ 是一个不受 $A$ 影响的变量（例如，年龄）。我们可以将 $Y$ 的完整表达式写出：
$$
Y = 2.0 (0.8 Z + 1.2 A + U_X) + 0.1 Z + U_Y = 1.7 Z + 2.4 A + 2.0 U_X + U_Y
$$
在这个表达式中，项 $2.4 A$ 是由 $A$ 产生的总因果效应。一个反事实公平的预测器 $\hat{Y}$ 就应该只包含那些不依赖于 $A$ 的部分：
$$
\hat{Y} = 1.7 Z + 2.0 U_X + U_Y
$$
通过这种方式，我们构建了一个从其定义上就消除了受保护属性 $A$ 的任何因果影响的预测器。尽管在实践中识别和分离这些因果路径极具挑战，但因果公平性为我们思考和构建真正“无偏见”的算法提供了一个强有力的理论指南。