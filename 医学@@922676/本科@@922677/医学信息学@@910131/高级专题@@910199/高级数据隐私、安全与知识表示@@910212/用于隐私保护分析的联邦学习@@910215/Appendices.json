{"hands_on_practices": [{"introduction": "要在实践中应用联邦学习，首先必须对其核心算法——联邦平均（FedAvg）——的运作机制有扎实的理解。这个练习引导你剖析FedAvg的训练流程，通过推导在所有参与方之间执行的总计算步骤和通信轮次，将抽象的算法概念转化为具体的、可量化的指标。掌握这些基本量的计算是评估和设计任何联邦学习系统的第一步 [@problem_id:4840326]。", "problem": "一个由 $K$ 家医院组成的医学信息学联盟旨在使用联邦平均（Federated Averaging, FedAvg）进行隐私保护分析，在不集中患者信息的情况下，基于电子健康记录（Electronic Health Record, EHR）数据训练一个预测模型。医院 $i$ 本地存储了 $N_{i}$ 条患者记录。在 FedAvg 中，一个中央服务器协调 $R$ 轮全局回合。在每个全局回合中，服务器将当前模型广播给所有 $K$ 家医院，每家医院使用大小为 $B$ 的小批量随机梯度下降在本地训练 $E$ 个周期（epoch），然后每家医院将其更新后的模型返回给服务器，服务器将它们聚合起来。\n\n从以下基本定义出发：客户端 $i$ 的一个周期（epoch）是完整遍历其 $N_{i}$ 条记录一次，并且对于从本地数据集中形成的每个小批量都进行一次小批量梯度步（允许最后一个小批量小于 $B$ 但仍计为单一步），推导以下表达式的封闭形式：\n- 在所有 $R$ 个全局回合中，所有医院执行的本地梯度步总数，以及\n- 在整个训练过程中，服务器与医院之间发生的通信回合总数。\n\n用符号 $K$、$E$、$B$、$R$ 和 $\\{N_{i}\\}_{i=1}^{K}$ 表示您的最终答案。无需进行舍入。使用 LaTeX 的 $\\texttt{pmatrix}$ 环境将两个量表示为单行矩阵。", "solution": "在尝试求解之前，应首先对问题陈述进行严格的验证过程。\n\n### 步骤 1：提取已知条件\n问题陈述中逐字提供了以下数据和定义：\n-   医院数量：$K$\n-   医院 $i$ 的患者记录数：$N_{i}$，其中 $i$ 是从 $1$ 到 $K$ 的索引\n-   全局回合总数：$R$\n-   每个全局回合的本地周期（epoch）数：$E$\n-   随机梯度下降（SGD）的小批量大小：$B$\n-   周期的定义：“完整遍历其 $N_{i}$ 条记录一次”\n-   梯度步的条件：“对于从本地数据集中形成的每个小批量都进行一次小批量梯度步”\n-   最后一个小批量的条件：“最后一个小批量允许小于 $B$ 并且仍然计为单一步”\n-   要求的输出：\n    1.  本地梯度步总数的封闭形式表达式。\n    2.  通信回合总数的封闭形式表达式。\n\n### 步骤 2：使用提取的已知条件进行验证\n根据所需标准对问题进行评估：\n-   **科学依据：** 该问题描述了联邦平均（FedAvg）算法，这是联邦学习领域中一种成熟且经典的方法。其在医学信息学中用于隐私保护分析是一个主要且现实的用例。该设置在科学上是合理的。\n-   **适定性：** 该问题是适定的。所有变量（$K$、$R$、$E$、$B$、$\\{N_{i}\\}_{i=1}^{K}$）均已定义。计算梯度步的过程有明确的详细说明，包括如何处理最后一个可能更小的小批量。这一细节明确地暗示了需要使用向上取整函数。在联邦学习的背景下，“通信回合”一词被理解为与“全局回合”或“聚合周期”同义。该问题是自包含的，并提供了足够的信息来推导出所求量的唯一且有意义的表达式。\n-   **客观性：** 该问题使用清晰、精确、无歧义的技术语言陈述。它不含任何主观或基于意见的内容。\n\n该问题没有表现出任何列举的缺陷（例如，科学上不合理、不完整、矛盾、含糊不清）。这是一个根植于成熟的计算机科学和机器学习原则的可形式化、可解决的问题。\n\n### 步骤 3：结论与行动\n该问题有效。现在将提供一个完整且论证充分的解决方案。\n\n### 解题推导\n\n该问题要求推导两个量：本地梯度步的总数和通信回合的总数。每个量都将根据所提供的定义进行推导。\n\n**1. 本地梯度步总数**\n\n我们首先确定单个医院（指定为医院 $i$）在单个本地训练周期内执行的梯度步数。\n-   医院 $i$ 拥有大小为 $N_i$ 的本地数据集。\n-   本地训练使用批量大小为 $B$ 的小批量 SGD 进行。\n-   每个小批量都会执行一次梯度下降步。\n-   问题明确指出最后一个小批量可能小于 $B$。这意味着记录总数 $N_i$ 被划分为尽可能多的、大小为 $B$ 的完整批次，剩余的记录则形成最后一个批次。\n-   因此，处理 $N_i$ 条记录所需的批次数是记录总数除以批量大小，并向上取整。此操作由向上取整函数表示。\n-   医院 $i$ 每个周期的梯度步数：$\\lceil \\frac{N_i}{B} \\rceil$。\n\n接下来，我们计算医院 $i$ 在一个全局回合中的总梯度步数。\n-   在每个全局回合中，每家医院都会进行 $E$ 个本地周期的训练。\n-   由于对于给定的医院，每个周期的步数是恒定的，因此医院 $i$ 在一个全局回合中的总步数是 $E$ 乘以每个周期的步数。\n-   医院 $i$ 每个全局回合的梯度步数：$E \\times \\lceil \\frac{N_i}{B} \\rceil$。\n\n现在，我们可以计算在一个全局回合中所有医院的总梯度步数。\n-   联盟中有 $K$ 家医院。\n-   一个全局回合中的总步数是每家医院所采取步数的总和。\n-   每个全局回合的总步数：$\\sum_{i=1}^{K} \\left( E \\times \\lceil \\frac{N_i}{B} \\rceil \\right) = E \\sum_{i=1}^{K} \\lceil \\frac{N_i}{B} \\rceil$。\n\n最后，我们确定整个训练过程中的本地梯度步总数。\n-   整个过程包含 $R$ 个全局回合。\n-   梯度步总数是每个全局回合的步数乘以全局回合的总数。\n-   设 $S_{total}$ 为本地梯度步的总数。\n$$S_{total} = R \\times \\left( E \\sum_{i=1}^{K} \\lceil \\frac{N_i}{B} \\rceil \\right) = R E \\sum_{i=1}^{K} \\lceil \\frac{N_i}{B} \\rceil$$\n这是第一个所求的表达式。\n\n**2. 通信回合总数**\n\n第二个任务是确定通信回合的总数。\n-   问题描述了单个“全局回合”内的一系列事件：\n    1.  中央服务器将当前全局模型广播给所有 $K$ 家医院。\n    2.  医院进行本地训练。\n    3.  每家医院将其更新后的模型返回给服务器进行聚合。\n-   这个广播、本地计算和聚合的完整周期构成了 FedAvg 协议中服务器和客户端之间交互的基本单元。在联邦学习的标准术语中，这个周期被定义为一个“通信回合”。\n-   问题陈述了训练过程总共进行 $R$ 个全局回合。\n-   因此，根据定义，通信回合的总数等于全局回合的总数。\n-   设 $C_{total}$ 为通信回合的总数。\n$$C_{total} = R$$\n这是第二个所求的表达式。\n\n最终答案按要求将这两个结果合并为一个单行矩阵。", "answer": "$$ \\boxed{ \\begin{pmatrix} R E \\sum_{i=1}^{K} \\lceil \\frac{N_i}{B} \\rceil  R \\end{pmatrix} } $$", "id": "4840326"}, {"introduction": "联邦学习的一个核心挑战是通信开销，尤其是在处理具有数百万参数的大型模型时。本练习将理论与实践相结合，要求你计算在不同参数编码方案下传输模型更新所需的具体时间，从而让你亲身体会到通信瓶颈的严重性。通过对比分析，你将理解模型量化等技术在提高联邦学习系统可行性方面扮演的关键角色 [@problem_id:4840332]。", "problem": "一个医院联盟正在使用联邦平均（Federated Averaging, FedAvg）训练一个共享的诊断模型。联邦平均是联邦学习中的一种标准算法，通过将数据保留在本地来保护患者隐私。在每一轮通信中，每家医院都将其本地模型更新发送给协调器，然后接收聚合后的全局模型。假设模型有 $d=10^{7}$ 个可训练参数，并且一次传输的更新大小等于参数向量的大小。考虑两种参数编码方案：每参数使用 $32$ 位的单精度浮点数，以及每参数使用 $8$ 位的均匀量化。每家医院可用的网络链接速度为每秒 $100$ 兆比特（megabits），其中一兆比特等于 $10^{6}$ 比特。\n\n从数据大小和吞吐量的核心定义（数据大小以比特为单位，吞吐量以比特/秒为单位，时间等于大小除以速率）出发，推导在两种编码方案下，每家医院每轮的总通信时间（上传加下载）。假设没有压缩、没有协议开销，并且协调器返回的模型大小与医院发送的更新大小相同。\n\n判断在每轮 $2\\,\\mathrm{s}$ 的墙上时钟预算下，每种方案是否可行。可行性定义为每轮总通信时间小于或等于 $2\\,\\mathrm{s}$。\n\n将你最终的数值时间四舍五入到四位有效数字，并以秒为单位表示。以行矩阵的形式提供你的最终答案，第一个条目等于 $32$ 位方案的时间，第二个条目等于 $8$ 位方案的时间。", "solution": "首先根据所需标准对问题进行验证。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n-   模型参数数量：$d = 10^7$\n-   编码方案1（单精度）：每参数 $b_{32} = 32$ 比特\n-   编码方案2（量化）：每参数 $b_8 = 8$ 比特\n-   网络链接速度（吞吐量）：$R = 100$ 兆比特/秒，其中 $1$ 兆比特 $= 10^6$ 比特。\n-   通信轮结构：每家医院执行一次上传和一次下载。\n-   下载模型的大小：与上传的模型更新相同。\n-   假设：无压缩、无协议开销。\n-   可行性预算：每轮总通信时间 $\\le T_{budget} = 2\\,\\mathrm{s}$。\n-   计算基础：时间 = 数据大小 / 吞吐量。\n-   取整要求：最终时间需四舍五入至四位有效数字。\n\n**步骤2：使用提取的已知条件进行验证**\n该问题在科学上基于计算机网络和联邦学习的原理。该场景是联邦学习在医学信息学中的一个标准应用。所提供的参数（$10^7$ 个参数、$100\\,\\mathrm{Mbps}$ 的链接速度、$32$ 位和 $8$ 位编码）在现代机器学习和网络基础设施中是现实的数值。问题陈述清晰，提供了所有必要的信息和明确、客观的定义（例如，$1$ 兆比特 $= 10^6$ 比特），足以计算出唯一解。它没有科学缺陷、歧义或矛盾。\n\n**步骤3：结论与行动**\n该问题被认为是有效的。将提供解答。\n\n### 解题推导\n\n分析过程首先为每种编码方案定义单次通信轮中要传输的总数据大小。然后，利用给定的网络吞吐量，计算此传输所需的时间。最后，将此时间与指定的可行性预算进行比较。\n\n设 $d$ 为模型中可训练参数的数量。\n设 $b$ 为用于编码每个参数的比特数。\n模型更新的大小 $S$（以比特为单位）由参数数量与每参数比特数的乘积给出：\n$$S = d \\times b$$\n\n网络链接速度 $R$ 为每秒 $100$ 兆比特。根据所提供的定义，即：\n$$R = 100 \\times 10^6 \\frac{\\mathrm{bits}}{\\mathrm{second}} = 10^8 \\frac{\\mathrm{bits}}{\\mathrm{second}}$$\n\n对于一家医院，单次通信轮包括一次向协调器上传模型更新和一次从协调器下载聚合后的全局模型。问题说明下载的模型大小与上传的更新大小相同。因此，每家医院每轮传输的总数据量 $S_{total}$ 是单次更新大小的两倍：\n$$S_{total} = S_{upload} + S_{download} = S + S = 2S$$\n\n一轮的总时间 $T_{round}$ 是总数据大小除以网络链接速度：\n$$T_{round} = \\frac{S_{total}}{R} = \\frac{2S}{R} = \\frac{2db}{R}$$\n\n我们现在为两种编码方案计算这个时间。\n\n**方案1：单精度浮点数（$32$ 位）**\n\n在这种情况下，每参数的比特数是 $b_{32} = 32$。\n参数数量为 $d = 10^7$。\n单次模型更新的大小为：\n$$S_{32} = d \\times b_{32} = 10^7 \\times 32 = 3.2 \\times 10^8 \\text{ bits}$$\n\n每轮的总通信时间 $T_{round, 32}$ 是：\n$$T_{round, 32} = \\frac{2 S_{32}}{R} = \\frac{2 \\times (3.2 \\times 10^8 \\text{ bits})}{10^8 \\text{ bits/s}} = 2 \\times 3.2\\,\\mathrm{s} = 6.4\\,\\mathrm{s}$$\n\n问题要求四舍五入到四位有效数字。因此，$T_{round, 32} = 6.400\\,\\mathrm{s}$。\n\n**方案1的可行性检查：**\n可行性预算为 $T_{budget} = 2\\,\\mathrm{s}$。\n我们比较计算出的时间：$6.400\\,\\mathrm{s}  2\\,\\mathrm{s}$。\n因此，在给定的约束条件下，$32$ 位编码方案是**不可行**的。\n\n**方案2：均匀量化（$8$ 位）**\n\n在这种情况下，每参数的比特数是 $b_8 = 8$。\n参数数量为 $d = 10^7$。\n单次模型更新的大小为：\n$$S_8 = d \\times b_8 = 10^7 \\times 8 = 8 \\times 10^7 \\text{ bits}$$\n\n每轮的总通信时间 $T_{round, 8}$ 是：\n$$T_{round, 8} = \\frac{2 S_8}{R} = \\frac{2 \\times (8 \\times 10^7 \\text{ bits})}{10^8 \\text{ bits/s}} = 2 \\times 0.8\\,\\mathrm{s} = 1.6\\,\\mathrm{s}$$\n\n问题要求四舍五入到四位有效数字。因此，$T_{round, 8} = 1.600\\,\\mathrm{s}$。\n\n**方案2的可行性检查：**\n可行性预算为 $T_{budget} = 2\\,\\mathrm{s}$。\n我们比较计算出的时间：$1.600\\,\\mathrm{s} \\le 2\\,\\mathrm{s}$。\n因此，在给定的约束条件下，$8$ 位编码方案是**可行**的。\n\n最终答案要求将两个计算出的时间（以秒为单位，并四舍五入到四位有效数字）以行矩阵的形式呈现。\n$32$ 位方案的时间是 $6.400\\,\\mathrm{s}$。\n$8$ 位方案的时间是 $1.600\\,\\mathrm{s}$。\n行矩阵为 $\\begin{pmatrix} 6.400  1.600 \\end{pmatrix}$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n6.400  1.600\n\\end{pmatrix}\n}\n$$", "id": "4840332"}, {"introduction": "在联邦学习中实现隐私保护并非只有一种方法，不同的隐私模型在安全性、信任假设和模型效用之间存在着复杂的权衡。这个练习旨在深入探讨本地差分隐私（Local DP）与中心化差分隐私（Central DP）这两种主流设计之间的根本区别。通过分析它们在不同信任场景（例如，当安全聚合协议可信或失效时）下的表现，你将学会如何评估联邦学习系统中的隐私-效用权衡，这是设计负责任的隐私保护AI系统的核心技能 [@problem_id:4840335]。", "problem": "在一个应用联邦学习（FL）的医院联盟中， $n$ 个站点中的每一个站点都在其自己的电子健康记录队列上计算一个裁剪后的局部梯度 $g_i \\in \\mathbb{R}^d$ ，并参与安全聚合（SA）。SA 是一种加密协议，它只向服务器揭示精确的向量和 $G = \\sum_{i=1}^{n} g_i$ ，并隐藏所有单个的 $g_i$ 。为了保护患者级别的信息，正在考虑两种隐私设计：一种是本地差分隐私（LDP）设计，另一种是中心化差分隐私设计。该联盟必须根据对 SA 的不同信任假设来决定哪种设计是合适的，以及应如何进行随机化。\n\n您可以假设以下经过充分检验的基本事实和定义：\n\n- 对于数据集上的随机化机制 $M$ ，差分隐私（DP）定义如下：对于任何相差一个个体记录的相邻数据集 $D$ 和 $D'$ ，以及任何可测集 $S$ ，如果满足以下条件，则该机制满足 $(\\epsilon,\\delta)$-DP：\n$$\n\\Pr[M(D) \\in S] \\le e^{\\epsilon} \\Pr[M(D') \\in S] + \\delta\n$$\n.\n- 本地差分隐私（LDP）是一种特殊情况，其中每个客户端在任何管理者看到其输入 $x_i$ 之前，都对其应用一个随机化机制 $M_i$ ；相邻关系定义在同一客户端的两个可能输入 $x_i$ 和 $x_i'$ 上。\n- 中心化差分隐私（central DP）是指管理者持有聚合数据集（或统计量），并集中应用单个随机化机制 $M$ 来保护全局数据集中任何单个个体的贡献；相邻关系定义在相差 1 条记录的数据集 $D$ 和 $D'$ 上。\n- 安全聚合（SA）是一种加密协议，确保只向服务器揭示 $G$ ，而不会揭示任何单个的 $g_i$ ；SA 本身不是一种 DP 机制，但它可以作为一个可信组件，限制对手可以观察到的内容。\n- 后处理不变性：如果一个机制 $M$ 满足 $(\\epsilon,\\delta)$-DP，那么对于任何与数据无关的函数 $f$ ，其复合输出 $f(M(\\cdot))$ 也满足 $(\\epsilon,\\delta)$-DP。\n- 对于独立的加性噪声向量，方差在求和下是可加的：如果 $\\tilde{g}_i = g_i + Z_i$ ，其中 $Z_i$ 是均值为零、协方差为 $\\Sigma_i$ 的独立噪声，那么总噪声 $Z = \\sum_{i=1}^{n} Z_i$ 的协方差为 $\\sum_{i=1}^{n} \\Sigma_i$ 。\n- 裁剪后总和的敏感度：如果每个 $g_i$ 都被裁剪以使 $\\|g_i\\|_2 \\le C$ ，那么用 $g_j'$ 替换任何单个 $g_j$ 会使 $G$ 的变化最多为 $\\|g_j - g_j'\\|_2 \\le 2C$ ，因此 $G$ 的 $\\ell_2$-敏感度是有界的。\n\n提出了两个方案：\n\n- 方案 L（本地 DP）：每个客户端对其梯度应用一个 $(\\epsilon_L,\\delta_L)$-LDP 机制，在本地生成 $\\tilde{g}_i = M_i(g_i)$ ，然后使用 SA 对 $\\tilde{G} = \\sum_{i=1}^{n} \\tilde{g}_i$ 进行求和。\n- 方案 C（中心化 DP）：每个客户端将其真实的 $g_i$ 输入到 SA 中，SA 只向管理者揭示 $G$ ；管理者对 $G$ 应用一个 $(\\epsilon_C,\\delta_C)$-DP 机制 $M$ ，以生成发布的聚合结果 $\\hat{G} = M(G)$ 。\n\n假设存在一个诚实但好奇的对手，他可以观察到服务器看到的所有内容，并考虑两种情况：SA 是可信的（只揭示 $G$ ，从不揭示单个 $g_i$ ）和 SA 灾难性地失败（单个 $g_i$ 被揭示给服务器）。\n\n在这种联邦学习设置中，以下哪些陈述正确地对比了本地 DP 和中心化 DP？\n\nA. 如果 SA 灾难性地失败，导致服务器在方案 L 中看到每个 $\\tilde{g}_i$ ，在方案 C 中看到每个原始的 $g_i$ ，那么方案 L 仍然为每个客户端的数据提供了针对服务器的 $(\\epsilon_L,\\delta_L)$ 保证，而方案 C 则不提供任何 DP 保证，因为在聚合之前没有进行任何随机化。\n\nB. 在可信的 SA 下，方案 C 可以在聚合 $G$ 的层面上实现目标 $(\\epsilon_C,\\delta_C)$ ，同时只需在中心添加一次随机噪声，这通常比方案 L 在相同隐私目标下产生更高的效用，因为方案 L 中独立的客户端噪声会在 $n$ 个参与者的聚合中累积。\n\nC. 在方案 L 中，用于 $(\\epsilon_L,\\delta_L)$ 保证的相邻关系是定义在所有客户端中相差 1 个个体的全局数据集 $D$ 和 $D'$ 上，而不是在单个客户端的两个可能输入上。\n\nD. 在方案 C 中，当 SA 可信时，SA 内部使用的加密和掩码随机性就是使发布满足 $(\\epsilon_C,\\delta_C)$-DP 的随机性；管理者不需要对 $G$ 添加任何进一步的随机化。\n\nE. 在方案 L 中，即使每个客户端的机制单独满足 $(\\epsilon_L,\\delta_L)$-LDP，单轮发布 $\\tilde{g}_1, \\ldots, \\tilde{g}_n$ 也会因组合效应导致每个客户端的隐私参数随 $n$ 线性下降。\n\n选择所有适用的选项。", "solution": "必须首先验证问题陈述的科学正确性、逻辑一致性和客观性。\n\n### 步骤 1：提取已知信息\n问题提供了以下信息：\n- 一个由 $n$ 个医院站点组成的联邦学习联盟。\n- 每个站点 $i$ 计算一个裁剪后的局部梯度 $g_i \\in \\mathbb{R}^d$，满足 $\\|g_i\\|_2 \\le C$。\n- 安全聚合 (SA) 是一种加密协议，只向服务器揭示总和 $G = \\sum_{i=1}^{n} g_i$，隐藏单个的 $g_i$。\n- 差分隐私 (DP) 定义：对于一个机制 $M$、相邻数据集 $D, D'$ 以及任何可测集 $S$， $\\Pr[M(D) \\in S] \\le e^{\\epsilon} \\Pr[M(D') \\in S] + \\delta$。\n- 本地差分隐私 (LDP) 是一种特殊情况，客户端使用机制 $M_i$ 对其自己的输入 $x_i$ 进行随机化。相邻关系定义在同一客户端的两个可能输入 $x_i, x_i'$ 之间。\n- 中心化差分隐私 (central DP) 是一种情况，管理者对聚合结果应用机制 $M$。相邻关系定义在相差一条记录的全局数据集 $D, D'$ 上。\n- SA 不是 DP 机制，但可以是一个可信组件。\n- 后处理不变性：将与数据无关的函数 $f$ 应用于受 DP 保护的输出不会降低隐私性。\n- 对于添加到 $g_i$ 的均值为零、协方差为 $\\Sigma_i$ 的独立加性噪声向量 $Z_i$，总噪声 $Z = \\sum_{i=1}^{n} Z_i$ 的协方差为 $\\sum_{i=1}^{n} \\Sigma_i$。\n- 总和 $G$ 相对于更改一个客户端贡献的 $\\ell_2$-敏感度上限为 $2C$。\n- 方案 L (本地 DP)：每个客户端通过一个 $(\\epsilon_L, \\delta_L)$-LDP 机制计算 $\\tilde{g}_i = M_i(g_i)$。然后使用 SA 计算 $\\tilde{G} = \\sum_{i=1}^{n} \\tilde{g}_i$。\n- 方案 C (中心化 DP)：客户端将其真实的 $g_i$ 提供给 SA。SA 将 $G = \\sum_{i=1}^{n} g_i$ 提供给管理者。管理者使用一个 $(\\epsilon_C, \\delta_C)$-DP 机制计算发布的聚合结果 $\\hat{G} = M(G)$。\n- 对手是诚实但好奇的，并观察服务器所看到的一切。\n- 考虑两种威胁模型：(1) SA 是可信的并按规定工作。(2) SA 灾难性失败，向服务器揭示其所有输入。\n\n### 步骤 2：使用提取的已知信息进行验证\n问题在科学上和逻辑上都是合理的。\n- **科学依据充分**：问题基于隐私保护机器学习中的基础和标准概念，即联邦学习、差分隐私（本地和中心化模型）以及安全聚合。所提供的定义和属性（DP、LDP、SA、后处理、噪声求和、敏感度）都是正确且在该领域内公认的。\n- **定义明确**：问题定义明确。它提出了两个不同且清晰定义的方案（方案 L 和方案 C），并要求在不同的信任假设下对它们的属性进行比较分析。问题的结构旨在测试对这些概念的理解，并且可以根据提供的定义对给出的陈述进行明确的评估。\n- **客观性**：语言精确、正式，不含主观性。所有关键术语都有明确的定义。\n\n问题没有表现出任何缺陷，如科学不健全、信息缺失、矛盾、不可行性或歧义。该设置是讨论 FL 中隐私问题的典型场景。\n\n### 步骤 3：结论和行动\n问题陈述是有效的。我将继续分析每个选项。\n\n### 解决方案及逐项分析\n\n**A. 如果 SA 灾难性地失败，导致服务器在方案 L 中看到每个 $\\tilde{g}_i$ ，在方案 C 中看到每个原始的 $g_i$ ，那么方案 L 仍然为每个客户端的数据提供了针对服务器的 $(\\epsilon_L,\\delta_L)$ 保证，而方案 C 则不提供任何 DP 保证，因为在聚合之前没有进行任何随机化。**\n\n- **方案 L 在 SA 失败下的分析**：在方案 L 中，每个客户端 $i$ 对其梯度 $g_i$ 应用一个本地随机化机制 $M_i$ 以生成 $\\tilde{g}_i$。此随机化是在客户端设备上本地执行的，在任何数据发送之前。根据定义，机制 $M_i$ 满足 $(\\epsilon_L, \\delta_L)$-LDP。此保证是针对客户端自身数据而言的。如果 SA 失败，服务器观察到 $\\tilde{g}_i$，那么服务器看到的是一个已经随机化的值。LDP 保证意味着观察 $\\tilde{g}_i$ 只能提供关于原始 $g_i$ 的有限信息。因此，每个客户端的隐私都受到针对服务器的 $(\\epsilon_L,\\delta_L)$-DP 级别的保护。\n\n- **方案 C 在 SA 失败下的分析**：在方案 C 中，客户端将其精确的、未经随机化的梯度 $g_i$ 发送到 SA 协议。整个隐私保证依赖于两个组成部分：(1) SA 协议成功隐藏单个 $g_i$ 并只揭示总和 $G$，以及 (2) 管理者随后向 $G$ 添加噪声。如果 SA 灾难性失败，服务器将看到原始梯度 $\\{g_1, g_2, \\ldots, g_n\\}$。由于此时尚未发生任何随机化，服务器可以访问来自每个客户端的精确、未受保护的贡献。这构成了完全的隐私泄露；没有任何 DP 保证。\n\n- **结论**：该陈述是对本地 DP 与中心化 DP 中信任模型的正确且根本的比较。LDP 对中心服务器或聚合器的故障具有鲁棒性，而中心化 DP 则严重依赖于它们。**正确**。\n\n**B. 在可信的 SA 下，方案 C 可以在聚合 $G$ 的层面上实现目标 $(\\epsilon_C,\\delta_C)$ ，同时只需在中心添加一次随机噪声，这通常比方案 L 在相同隐私目标下产生更高的效用，因为方案 L 中独立的客户端噪声会在 $n$ 个参与者的聚合中累积。**\n\n- **方案 C 的噪声分析**：在方案 C 中，可信的管理者接收到精确的总和 $G = \\sum_{i=1}^{n} g_i$。为了使此发布满足 $(\\epsilon_C, \\delta_C)$-DP，管理者会添加噪声。例如，使用高斯机制，噪声标准差与 $G$ 的 $\\ell_2$-敏感度成正比。问题指出此敏感度的上限为 $2C$。因此，添加的噪声方差与 $(2C)^2 = 4C^2$ 成正比。噪声只添加一次。\n\n- **方案 L 的噪声分析**：在方案 L 中， $n$ 个客户端中的每一个都向其本地梯度 $g_i$ 添加噪声以实现 $(\\epsilon_L, \\delta_L)$-LDP。每个客户端 $g_i$ 的敏感度为 $C$（因为 $\\|g_i\\|_2 \\le C$）。每个客户端添加的噪声（例如 $Z_i$）必须足够大，以掩盖这种幅度的变化。假设每个 $Z_i$ 的方差为 $\\sigma_L^2$，与 $C^2$ 成正比。服务器接收到的总和为 $\\tilde{G} = \\sum_{i=1}^{n} \\tilde{g}_i = G + \\sum_{i=1}^{n} Z_i$。由于客户端的噪声 $Z_i$ 是独立的，它们的方差会相加。最终聚合中的总噪声方差为 $\\sum_{i=1}^{n} \\text{Var}(Z_i) = n \\sigma_L^2$。此方差与 $nC^2$ 成正比。\n\n- **比较**：对于可比较的隐私水平，方案 L 中的噪声方差随 $n$（参与者数量）的增加而增加，而在方案 C 中它相对于 $n$ 是恒定的。假设隐私参数和机制相似，对于任何 $n  4$，方案 L 中的总噪声显著大于方案 C。聚合梯度 $\\tilde{G}$ 中如此大的噪声会降低其质量，导致模型精度降低和/或收敛速度减慢。因此，方案 C 通常比方案 L 产生更高的效用（准确性）。\n\n- **结论**：该陈述正确地指出了本地和中心化 DP 模型之间核心的效用-隐私权衡。噪声的累积是本地模型的主要缺点。**正确**。\n\n**C. 在方案 L 中，用于 $(\\epsilon_L,\\delta_L)$ 保证的相邻关系是定义在所有客户端中相差 1 个个体的全局数据集 $D$ 和 $D'$ 上，而不是在单个客户端的两个可能输入上。**\n\n- **分析**：该陈述描述的是中心化 DP 的相邻关系，而不是本地 DP。问题正确地定义了 LDP：“本地差分隐私（LDP）是一种特殊情况，其中每个客户端在任何管理者看到其输入 $x_i$ 之前，都对其应用一个随机化机制 $M_i$ … 相邻关系定义在同一客户端的两个可能输入 $x_i$ 和 $x_i'$ 上。”方案 L 明确是一个 LDP 设计。对客户端 $i$ 的 $(\\epsilon_L, \\delta_L)$ 保证确保了看到 $\\tilde{g}_i$ 的对手无法可靠地区分客户端的本地数据是 $D_i$ 还是相邻的 $D'_i$（相差一条患者记录）。隐私保证是针对客户端本地的。全局数据集的概念与单个客户端的 LDP 保证无关。\n\n- **结论**：该陈述从根本上误解并错误地描述了本地差分隐私。**不正确**。\n\n**D. 在方案 C 中，当 SA 可信时，SA 内部使用的加密和掩码随机性就是使发布满足 $(\\epsilon_C,\\delta_C)$-DP 的随机性；管理者不需要对 $G$ 添加任何进一步的随机化。**\n\n- **分析**：该陈述混淆了密码学安全与 DP 的统计隐私。安全聚合使用密码学技术（如秘密共享或同态加密）来计算总和 $G$ ，同时对服务器保密加数 $g_i$ 。SA 中的随机性（例如，加密密钥、求和后会抵消的随机掩码）旨在提供这种密码学安全性。然而，一个完美的 SA 协议的输出是*精确的*总和 $G = \\sum g_i$ 。发布这个精确的总和 $G$ 并不满足 DP。一个拥有背景知识（例如，知道所有其他梯度）的对手可以完美地推断出特定客户端的贡献。DP 要求向输出中注入经过校准的*统计噪声*，这是一个独立于 SA 的过程。在方案 C 中，管理者从 SA 接收到 $G$ ，*然后*必须应用一个 DP 机制 $M$ （例如，添加噪声）来产生公开的、保护隐私的输出 $\\hat{G}$ 。\n\n- **结论**：该陈述不正确。来自 SA 的随机性是为了安全，而不是为了 DP 风格的隐私。需要额外的、经过校准的噪声。**不正确**。\n\n**E. 在方案 L 中，即使每个客户端的机制单独满足 $(\\epsilon_L,\\delta_L)$-LDP，单轮发布 $\\tilde{g}_1, \\ldots, \\tilde{g}_n$ 也会因组合效应导致每个客户端的隐私参数随 $n$ 线性下降。**\n\n- **分析**：该陈述错误地应用了隐私组合的概念。DP 中的组合定理描述的是当*同一个数据集*被多次查询时隐私如何退化。在所述场景中，我们考虑的是单个客户端（比如客户端 $j$）的隐私。对手看到输出集合 $\\{\\tilde{g}_1, \\ldots, \\tilde{g}_n\\}$。客户端 $j$ 的数据隐私由机制 $M_j$ 保护，该机制将 $g_j$ 随机化以产生 $\\tilde{g}_j$。其他输出 $\\tilde{g}_i$（其中 $i \\neq j$）是由*其他客户端*的数据生成的，这些数据与客户端 $j$ 的数据是独立的。因为 $\\tilde{g}_i$（对于 $i \\neq j$）在统计上独立于 $g_j$，所以观察 $\\tilde{g}_i$ 并不能提供关于 $g_j$ 的任何信息。因此，客户端 $j$ 的隐私保证仅由其自身的 LDP 机制 $M_j$ 决定，并且不受其他客户端独立私有化报告发布的影响。每个客户端的隐私保证仍然是 $(\\epsilon_L, \\delta_L)$。\n\n- **结论**：该陈述错误地应用了组合原则。一个客户端数据的隐私不会因为发布来自其他客户端的独立随机化数据而退化。**不正确**。", "answer": "$$\\boxed{AB}$$", "id": "4840335"}]}