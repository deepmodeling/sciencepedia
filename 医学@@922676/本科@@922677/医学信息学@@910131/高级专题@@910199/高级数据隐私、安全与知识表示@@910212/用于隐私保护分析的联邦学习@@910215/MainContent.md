## 引言
在精准医疗和大数据时代，海量健康数据的分析为疾病诊断、治疗和预防带来了前所未有的机遇。然而，这些数据——例如电子病历和基因组信息——具有高度的敏感性，受到严格的法律法规和伦理准则的保护。传统的机器学习方法依赖于将数据汇集到中央服务器进行训练，这种中心化范式在医疗领域面临着巨大的隐私和安全壁垒，形成了难以逾越的“数据孤岛”。如何在不暴露敏感信息的前提下，安全、合规地利用这些分散的数据进行协作分析，是当前医学信息学面临的核心挑战。

[联邦学习](@entry_id:637118) (Federated Learning) 正是为应对这一挑战而生的一种革命性技术范式。其核心理念是“数据不动，模型动”：原始数据永远保留在本地机构（如医院）的服务器内，而模型则在各参与方之间安全地移动和聚合，从而在不直接访问任何私有数据的情况下，协同训练出一个性能强大的全局模型。这种去中心化的方法为保护数据隐私、打破数据孤岛、促进大规模协作研究开辟了新的道路。

为了帮助您全面掌握这一前沿技术，本文将通过三个循序渐进的章节，系统地构建您对联邦学习的认知。首先，在“原理与机制”一章中，我们将深入[联邦学习](@entry_id:637118)的技术内核，从其基本定义、核心算法（如 [FedAvg](@entry_id:634153)）到数据异构性带来的统计学挑战，并详细剖析其固有的隐私风险及相应的缓解策略，如差分隐私和[安全聚合](@entry_id:754615)。接着，在“应用与跨学科连接”一章中，我们将视野扩展到真实世界，展示联邦学习如何在生物医学研究、基因组分析、公共卫生监测等领域发挥关键作用，并探讨其与法律、伦理和[全球治理](@entry_id:202679)的深刻交集。最后，通过“动手实践”部分，您将有机会将理论知识应用于具体问题，加深对核心概念的理解。

## 原理与机制

本章在前一章介绍性概述的基础上，深入探讨[联邦学习](@entry_id:637118) (Federated Learning, FL) 的核心原理、关键机制、内在挑战及其应对策略。我们将从[联邦学习](@entry_id:637118)的基本定义和数学目标出发，逐步解析其核心算法、系统架构和数据划分模式。随后，我们将重点分析[联邦学习](@entry_id:637118)环境中的两大核心议题：由数据异构性带来的统计学挑战，以及保护[数据隐私](@entry_id:263533)所面临的攻击风险与防御机制。通过本章的学习，读者将能够建立一个关于联邦学习如何运作、为何需要隐私保护以及如何实现稳健与安全协作的系统性认知框架。

### 联邦学习的基础概念

#### 联邦学习范式：去中心化训练

传统的机器学习，特别是[深度学习](@entry_id:142022)，通常依赖于一种**中心化训练 (centralized training)** 范式。在这种模式下，来自不同来源（例如，多家医院）的原始数据被汇集到一个中央服务器或数据中心。模型训练完全在汇集后的完整数据集上进行。这种方法的优势在于能够利用全局数据分布，但其前提是数据能够并且被允许离开其原始所在地，这在处理如电子病历 (Electronic Health Records, EHR) 等高度敏感数据时，往往会面临严格的法律、伦理和隐私壁垒。

**联邦学习 (Federated Learning)** 提供了一种截然不同的去中心化训练范式，其核心思想是“数据不动，模型动”。在[联邦学习](@entry_id:637118)框架中，原始数据始终保留在数据所有者（如医院或个人设备）的本地服务器上，绝不离开。取而代之的是，一个**协调服务器 (coordinating server)** 负责分发全局模型，并收集由各个参与方（称为**客户端 (clients)**）在本地数据上计算出的**模型更新 (model updates)**。这些更新通常是模型参数的梯度 (gradients) 或参数变化量 (parameter deltas)。服务器聚合这些更新以优化全局模型，然后将优化后的模型再次分发给客户端，如此迭代，直至[模型收敛](@entry_id:634433)。

通过这种方式，[联邦学习](@entry_id:637118)实现了在不直接访问或汇集任何原始敏感数据的前提下，协同训练一个全局共享模型。这种“计算换通信”的策略，在保护[数据隐私](@entry_id:263533)方面迈出了关键一步 [@problem_id:4840279]。

#### 数学目标：联邦环境下的[经验风险最小化](@entry_id:633880)

从数学角度看，[联邦学习](@entry_id:637118)的目标与中心化学习一致，即求解一个**[经验风险最小化](@entry_id:633880) (Empirical Risk Minimization, ERM)** 问题。假设我们有一个由参数 $w$ 定义的模型，以及一个[损失函数](@entry_id:136784) $\ell(w; x, y)$，它衡量模型在单个样本 $(x, y)$ 上的[预测误差](@entry_id:753692)。如果所有数据都集中在一起，我们的目标是最小化在所有 $N$ 个样本上的平均损失，即全局[经验风险](@entry_id:633993)：
$$ F(w) = \frac{1}{N} \sum_{i=1}^{N} \ell(w; x_i, y_i) $$
在联邦学习的设定中，数据分布在 $K$ 个客户端上，每个客户端 $k$ 持有 $n_k$ 个本地样本，总样本量为 $N = \sum_{k=1}^{K} n_k$。因此，全局[经验风险](@entry_id:633993)可以重写为各个客户端本地[经验风险](@entry_id:633993) $F_k(w)$ 的加权平均值：
$$ F(w) = \sum_{k=1}^{K} \frac{n_k}{N} F_k(w), \quad \text{其中} \quad F_k(w) = \frac{1}{n_k} \sum_{j=1}^{n_k} \ell(w; x_{k,j}, y_{k,j}) $$
这个公式至关重要，它揭示了[联邦学习](@entry_id:637118)聚合过程的核心逻辑。权重 $\frac{n_k}{N}$ 确保了来自不同客户端的每个数据样本对全局目标的贡献是均等的，无论该样本属于一个拥有大量数据的大型医院，还是一个数据量较小的诊所。这种**按样本量加权**的方式，精确地复现了在中心化数据池上进行训练时的目标函数。若采用其他权重，例如对所有客户端一视同仁的等权重 $\frac{1}{K}$，则会不成比例地放大来自小型数据集客户端的影响，导致训练目标偏离真实的全局[经验风险](@entry_id:633993) [@problem_id:4840284]。

当[损失函数](@entry_id:136784)可微时，全局梯度的计算也遵循同样的加权规则：
$$ \nabla F(w) = \sum_{k=1}^{K} \frac{n_k}{N} \nabla F_k(w) $$
这意味着，通过在服务器端对本地计算的梯度进行加权平均，[联邦学习](@entry_id:637118)可以在不访问原始数据的情况下，计算出与中心化训练完全相同的全局梯度，从而保证了其在理想条件下能够达到与中心化学习相同的优化效果 [@problem_id:4840284]。

#### 核心算法：FedSGD 与 [FedAvg](@entry_id:634153)

联邦学习的训练过程通过一系列通信轮次 (communication rounds) 展开，其核心算法主要有两种形式：联邦[随机梯度下降](@entry_id:139134) (FedSGD) 和[联邦平均](@entry_id:634153) ([FedAvg](@entry_id:634153))。

- **联邦[随机梯度下降](@entry_id:139134) (Federated Stochastic Gradient Descent, FedSGD)** 是最基础的[联邦学习](@entry_id:637118)算法。在每一轮通信中，协调服务器将当前的全局模型 $w_t$ 广播给所有（或部分）参与的客户端。每个客户端 $k$ 在其本地数据的一个小批量 (mini-batch) 上计算梯度 $\nabla F_k(w_t)$，然后将这个梯度发送回服务器。服务器收集到所有参与客户端的梯度后，按照其数据量进行加权平均，并用这个聚合后的全局梯度来更新模型：
  $$ w_{t+1} = w_t - \eta \sum_{k \in \mathcal{S}_t} \frac{n_k}{n_{\mathcal{S}_t}} \nabla F_k(w_t) $$
  其中 $\mathcal{S}_t$ 是第 $t$ 轮参与的客户端集合，$n_{\mathcal{S}_t} = \sum_{k \in \mathcal{S}_t} n_k$，$\eta$ 是学习率。FedSGD 本质上是分布式计算的大批量[随机梯度下降](@entry_id:139134)，其每轮通信只对应一[次梯度](@entry_id:142710)更新，通信成本较高。

- **[联邦平均](@entry_id:634153) (Federated Averaging, [FedAvg](@entry_id:634153))** 是对 FedSGD 的一个关键改进，也是目前应用最广泛的联邦学习算法。[FedAvg](@entry_id:634153) 的核心思想是通过增加本地计算来减少通信频率。在 [FedAvg](@entry_id:634153) 中，每个被选中的客户端在收到全局模型 $w_t$ 后，不是只计算一[次梯度](@entry_id:142710)，而是在其本地数据上执行多步（例如，$E$ 步）[随机梯度下降](@entry_id:139134)，以得到一个更新后的本地模型 $w_k^{(E)}$。只有这个经过多次本地优化的模型（或其变化量）才被发送回服务器。服务器随后对收集到的本地模型进行加权平均，以产生新的全局模型 $w_{t+1}$。

我们可以将客户端的本地[更新过程](@entry_id:273573)展开。从 $w_k^{(0)} = w_t$ 开始，经过 $E$ 步本地更新，每一步都使用一个本地随机梯度 $g_{k,i}$：
$$ w_k^{(E)} = w_k^{(0)} - \eta \sum_{i=1}^{E} g_{k,i}(w_k^{(i-1)}) = w_t - \eta \sum_{i=1}^{E} g_{k,i}(w_k^{(i-1)}) $$
服务器聚合后的全局模型更新可以表示为 [@problem_id:4840343]：
$$ w_{t+1} = \sum_{k \in \mathcal{S}_t} \frac{n_k}{n_{\mathcal{S}_t}} w_k^{(E)} = w_t - \eta \sum_{k \in \mathcal{S}_t} \frac{n_k}{n_{\mathcal{S}_t}} \left( \sum_{i=1}^{E} g_{k,i}(w_k^{(i-1)}) \right) $$
通过允许 $E > 1$ 的本地更新，[FedAvg](@entry_id:634153) 显著降低了达到同样模型精度所需的通信轮次，使其在实际应用中，尤其是在网络带宽有限或延迟较高的环境中，更具可行性。

### 联邦系统的分类与挑战

#### 架构设定：跨孤岛与跨设备

根据参与客户端的性质，联邦学习系统通常被分为两种主要架构设定 [@problem_id:4840279]：

- **跨孤岛[联邦学习](@entry_id:637118) (Cross-silo FL)**：此设定通常涉及数量较少（例如，几十到上百个）但规模较大的组织机构，如医院、银行或研究中心。这些客户端被称为“数据孤岛 (silos)”。它们的特点是：
    - **高可靠性**：客户端通常是专业的服务器，拥有强大的计算能力、存储空间和稳定高速的网络连接，几乎可以持续参与训练。
    - **高可用性**：大部分或全部客户端可以参与每一轮训练，使得同步聚合策略成为可能。
    - **大数据量**：每个客户端自身就拥有大量的训练数据。
    医疗机构间的合作，如多家医院联合训练疾病预测模型，是跨孤岛[联邦学习](@entry_id:637118)的典型应用场景。

- **跨设备联邦学习 (Cross-device FL)**：此设定则涉及海量（通常是百万级甚至十亿级）的个人终端设备，如智能手机、可穿戴设备或物联网 (IoT) 设备。这些客户端的特点是：
    - **资源受限**：设备的计算能力、内存和电池续航都非常有限。
    - **连接不稳定**：网络连接可能时断时续，且通常只在连接到免费 Wi-Fi 并充电时才适合参与训练。
    - **海量且短暂**：客户端总体数量巨大，但在任何一轮训练中，只有一小部分随机子集可用。
    - **高隐私需求**：由于更新直接来自个人用户设备，隐私保护的需求极为迫切，通常需要结合更强的隐私增强技术，如**[安全聚合](@entry_id:754615) (Secure Aggregation)** 和**差分隐私 (Differential Privacy)**。
    智能手机输入法预测模型的训练就是一个经典的跨设备联邦学习应用。

#### 数据[划分方案](@entry_id:635750)：水平、垂直与[迁移学习](@entry_id:178540)

根据数据在客户端之间的划分方式，联邦学习可以进一步分为三种主要类型 [@problem_id:4840339]：

- **水平[联邦学习](@entry_id:637118) (Horizontal Federated Learning, HFL)**：当所有客户端共享相同的**[特征空间](@entry_id:638014) (feature space)**，但拥有不同的**样本空间 (sample space)** 时，我们称之为水平[联邦学习](@entry_id:637118)。这就像将一个大的数据表水平切分，每个客户端持有一部分行。例如，两个地区的医院使用完全相同的电子病历系统（相同的字段和数据模式），但记录的是各自地区不同的患者群体。在这种模式下，由于模型结构相同，可以直接对各地训练的模型参数或梯度进行聚合。因此，**不需要**对不同医院的患者进行身份对齐 (identity alignment)。

- **垂直[联邦学习](@entry_id:637118) (Vertical Federated Learning, VFL)**：当客户端共享相同的[样本空间](@entry_id:275301)，但拥有不同的[特征空间](@entry_id:638014)时，我们称之为垂直[联邦学习](@entry_id:637118)。这好比将一个数据表垂直切分，每个客户端持有一部分列。例如，一家市级化验网络拥有患者的实验室检验结果，而一家独立的影像中心拥有同一批患者的影像学衍生特征。为了训练一个同时利用这两类特征的模型，**必须**对共享的患者身份进行对齐，以确保来自不同数据源的特征能够正确地组合到同一个样本上。这个对齐过程本身也需要采用隐私保护技术，如**隐私保护记录链接 (Privacy-Preserving Record Linkage, PPRL)**。

- **联邦[迁移学习](@entry_id:178540) (Federated Transfer Learning, FTL)**：当客户端的样本空间和特征空间都只有很少或没有重叠时，就需要用到联邦[迁移学习](@entry_id:178540)。例如，一个罕见病研究中心拥有少量但高度专业化的数据（独特的表型和影像模态），而一个大型综合医院拥有海量的常规病例数据。由于患者群体和数据特征都不同，直接进行水平或垂直联邦学习是不可行的。FTL 的目标是将从综合医院的大数据模型中学到的通用知识（如图像底层特征表示）“迁移”到罕见病模型上，以提升其性能。在这种情况下，**不需要**进行大规模的身份对齐，因为其核心思想是知识的迁移，而非样本级别的特征融合。

#### 统计异构性的挑战

[联邦学习](@entry_id:637118)的一个核心假设——客户端数据是独立同分布 (independent and identically distributed, IID) 的——在现实世界中几乎不成立。不同医院的患者人群、诊疗习惯、设备型号各不相同，导致其本地数据分布 $P_k$ 存在显著差异。这种现象被称为**统计异构性 (statistical heterogeneity)** 或 **non-IID** 数据问题，它是联邦学习面临的主要挑战之一。

统计异构性可以直接通过模型最优解的差异来形式化。假设我们用一个常数 $f$ 来预测某个临床指标 $Y$（例如，评分0、1、2），并使用平方损失 $\ell(f, Y) = (f - Y)^2$。在这种情况下，使本地[期望风险](@entry_id:634700) $\mathcal{R}_k(f) = \mathbb{E}_{Y \sim P_k}[(f - Y)^2]$ 最小化的最优模型 $f_k^\star$ 恰好是该指标在本地数据分布下的[期望值](@entry_id:150961)，即 $f_k^\star = \mathbb{E}_{P_k}[Y]$。如果两家医院的数据分布 $P_1$ 和 $P_2$ 不同，它们的本地最优模型 $f_1^\star$ 和 $f_2^\star$ 也会不同。这个差异 $H = |f_1^\star - f_2^\star|$ 直接量化了由于数据分布差异导致的目标[模型差异](@entry_id:198101) [@problem_id:4840268]。

为了更精细地刻画数据分布之间的差异，我们可以使用一些标准的概率度量。例如，对于[离散分布](@entry_id:193344) $p_1$ 和 $p_2$：
- **[全变差距离](@entry_id:143997) (Total Variation, TV, distance)** 衡量两个概率分布在所有可能事件上的最大差异，其计算公式为 $d_{\mathrm{TV}}(P_1, P_2) = \frac{1}{2} \sum_{y} |p_1(y) - p_2(y)|$。
- **[推土机距离](@entry_id:147338) (Earth Mover’s Distance, EMD)**，也称为**[瓦瑟斯坦距离](@entry_id:147338) (Wasserstein-1 distance)**，衡量将一个概率分布“变换”为另一个所需的“最小代价”。在一维情况下，它等于两个[累积分布函数 (CDF)](@entry_id:264700) $F_1(y)$ 和 $F_2(y)$ 之间差值的绝对值所围成的面积，即 $W_1(P_1, P_2) = \int_{-\infty}^{\infty} |F_1(y) - F_2(y)| dy$。

例如，假设医院1的结局分布为 $p_1=\{0.2, 0.5, 0.3\}$，医院2为 $p_2=\{0.5, 0.3, 0.2\}$。我们可以计算出 $f_1^\star = 1.1$, $f_2^\star = 0.7$，因此异构性 $H = 0.4$。同时，TV距离为 $0.3$，EMD为 $0.4$ [@problem_id:4840268]。这些指标都证实了数据分布的差异。

在 [FedAvg](@entry_id:634153) 算法中，统计异构性会导致一个严重的问题，称为**[客户端漂移](@entry_id:634167) (client drift)**。每个客户端在本地训练时，其模型会朝着本地数据的最优解移动，这可能与全局最优解的方向大相径庭。当服务器聚合这些“漂移”的本地模型时，可能会导致全局[模型收敛](@entry_id:634433)缓慢，甚至在不同方向的更新之间振荡，无法收敛。

### 隐私风险与缓解策略

#### 联邦学习默认是私密的吗？内在风险

一个常见的误解是，联邦学习因为不共享原始数据，所以是“天然私密”的。然而，事实并非如此。尽管联邦学习提供了一层基础的隐私保护，但它本身并不能抵御所有类型的隐私攻击。在标准的联邦学习设置中，一个**诚实但好奇 (honest-but-curious)** 的服务器，虽然会忠实地执行协议，但它会试图从其观察到的信息（即客户端上传的模型更新）中推断出关于客户端私有数据的敏感信息。研究表明，模型梯度本身就可能泄露大量关于训练数据的信息。

#### 攻击向量：从梯度中重建数据

最令人警惕的隐私风险之一是**[模型反演](@entry_id:634463) (Model Inversion)** 或**梯度泄露 (Gradient Leakage)** 攻击。在特定条件下，攻击者可以从客户端上传的梯度中精确或近似地重建出其原始训练数据。

这种风险在处理小批量数据时尤为突出。考虑一个极端情况：客户端使用大小为1的小批量（即单个数据样本 $x$）来计算梯度。对于一个神经网络的第一层，通常是一个[仿射变换](@entry_id:144885) $z = Wx + b$，其中 $W$ 是权重矩阵，$b$ 是偏置向量。通过反向传播的链式法则，可以推导出该层参数的梯度与输入 $x$ 和来自[上层](@entry_id:198114)的误差信号 $\delta$ 之间的关系。具体来说，权重梯度 $G_W$ 和偏置梯度 $g_b$ 分别为：
$$ G_W = \delta x^T $$
$$ g_b = \delta $$
服务器能够观察到 $G_W$ 和 $g_b$。如果 $g_b$ 非零，攻击者便可以直接用一个简单的代数运算恢复出原始输入 $x$：
$$ x^T = \frac{1}{(g_b)_j} (G_W)_{j,:} $$
其中 $(g_b)_j$ 是 $g_b$ 的任意非零分量，$(G_W)_{j,:}$ 是 $G_W$ 对应的行。这意味着，对于单样本更新，原始数据可以被完全泄露，无论网络后续层有多么复杂 [@problem_id:4840281]。一个有效的诊断方法是检查服务器收到的权重梯度矩阵 $G_W$ 的秩。如果其秩为1，就强烈暗示这是一个单样本更新，存在极高的泄露风险。

更普遍地，[模型反演](@entry_id:634463)攻击被形式化为一个优化问题：寻找一个“伪造”的输入数据 $(\hat{x}, \hat{y})$，使其产生的梯度与服务器观察到的真实梯度 $\Delta \theta_i^t$ 尽可能匹配。这相当于求解 [@problem_id:4341109]：
$$ \min_{\hat{x}, \hat{y}} \left\| \Delta \theta_i^t + \eta \nabla_{\theta}\ell(\theta^t; \hat{x}, \hat{y}) \right\|^2 + R(\hat{x}) $$
其中 $R(\hat{x})$ 是一个正则化项，用于确保重建的数据看起来“真实”（例如，是一张自然的图像）。这类攻击利用了梯度向量中包含的丰富高维结构信息来进行数据重建。

#### 攻击向量：推断成员身份

另一类主要的隐私攻击是**[成员推断](@entry_id:636505)攻击 (Membership Inference Attacks, MIA)**。其目标并非重建数据，而是判断某个特定的数据记录 $z=(x,y)$ 是否被用于训练。这类攻击利用了[机器学习模型](@entry_id:262335)普遍存在的过拟合现象：模型对其见过的训练数据（成员）和未见过的数据（非成员）的反应通常是不同的。

在联邦学习中，这种差异可以通过客户端上传的更新来观察。如果记录 $z$ 是客户端 $i$ [训练集](@entry_id:636396)中的一员，那么它的梯度 $\nabla_{\theta}\ell(\theta^t; z)$ 就是构成该客户端总更新 $\Delta \theta_i^t$ 的一部分。因此，成员记录的梯度方向应与总更新的方向具有相关性。攻击者可以计算候选记录 $z$ 的梯度，并测量它与观测到的客户端更新 $\Delta \theta_i^t$ 之间的对齐程度，例如通过计算[内积](@entry_id:750660) $\langle \Delta \theta_i^t, \nabla_{\theta}\ell(\theta^t; z) \rangle$。如果这个[内积](@entry_id:750660)表现出与非成员显著不同的统计特性，攻击者就可以据此推断成员身份。与[模型反演](@entry_id:634463)利用高维梯度结构不同，MIA 通常依赖于这种低维的统计信号（如损失值或[梯度对齐](@entry_id:172328)度）[@problem_id:4341109]。

#### 缓解策略 1：算法层面的解决方案 (FedProx)

为了应对统计异构性带来的[客户端漂移](@entry_id:634167)问题，研究者提出了一系列算法层面的改进，其中**FedProx** 是一个代表性的方法。FedProx 通过在本地优化目标中增加一个**近端项 (proximal term)** 来限制本地更新的幅度。其本地目标函数变为 [@problem_id:4341219]：
$$ \min_{w} F_k(w) + \frac{\mu}{2} \| w - w_t \|^2 $$
其中 $w_t$ 是当前轮次的全局模型参数，$w$ 是客户端试[图优化](@entry_id:261938)的本地模型参数，$\mu > 0$ 是一个超参数。这个新增的二次惩罚项 $\frac{\mu}{2} \| w - w_t \|^2$ 直观上像一根“缰绳”，它要求本地模型 $w$ 在最小化本地损失 $F_k(w)$ 的同时，不能离初始的全局模型 $w_t$ “太远”。

当 $\mu > 0$ 时，这个修改使得本地问题变得强凸，从而更好地约束了本地解的范围，减轻了[客户端漂移](@entry_id:634167)。我们可以通过求解其[一阶最优性条件](@entry_id:634945)来观察其效果。对于一个二次代理目标 $f_k(w) = \frac{1}{2} w^\top H_k w - g_k^\top w$，加入近端项后的最优解 $w_k^\star$ 满足：
$$ \nabla \left( f_k(w) + \frac{\mu}{2} \| w - w_t \|^2 \right) \Big|_{w=w_k^\star} = H_k w_k^\star - g_k + \mu(w_k^\star - w_t) = 0 $$
求解可得[闭式](@entry_id:271343)解 [@problem_id:4341219]：
$$ w_k^\star = (H_k + \mu I)^{-1} (g_k + \mu w_t) $$
这个解是本地最优解（由 $H_k, g_k$ 决定）和全局模型 $w_t$ 之间的一个折衷，其平衡由 $\mu$ 控制。通过这种方式，FedProx 提升了[联邦学习](@entry_id:637118)在 non-IID 数据环境下的收敛性和稳定性。

#### 缓解策略 2：通过差分隐私实现形式化隐私

为了从根本上应对隐私泄露的风险，我们可以引入**差分隐私 (Differential Privacy, DP)**，它为数据隐私提供了一个严格的、可量化的数学定义。一个随机化机制 $\mathcal{M}$ 如果满足 **$(\epsilon, \delta)$-[差分隐私](@entry_id:261539)**，那么对于任何两个仅相差一个个体记录的**相邻数据集 (neighboring datasets)** $D$ 和 $D'$，以及任何可能的输出集合 $S$，以下不等式恒成立 [@problem_id:4840309]：
$$ \Pr[\mathcal{M}(D) \in S] \le e^\epsilon \Pr[\mathcal{M}(D') \in S] + \delta $$
其中 $\epsilon$ 是[隐私预算](@entry_id:276909)，控制着隐私保护的强度（$\epsilon$ 越小，保护越强），$\delta$ 则是一个小的松弛项。这个定义保证了攻击者无法从算法的输出中，以高置信度判断任何一个个体是否参与了计算。

在联邦学习中，DP 的应用有两个主要层面 [@problem_id:4840309]：
- **记录级 DP (Record-level DP)**：保护的对象是客户端本地数据集中的**单个记录**（例如，一个病人的数据）。此时，“相邻数据集”指的是在某个客户端的本地数据集中增加或删除一条记录。要实现记录级 DP，客户端需要在计算更新时，对每个记录产生的梯度进行范数裁剪 (clipping)，然后向聚合后的梯度中添加经过精确校准的噪声。

- **客户端级 DP (Client-level DP)**：保护的对象是**整个客户端**（例如，一家医院）。此时，“相邻数据集”指的是在联邦学习的参与方中增加或删除一个客户端。为实现客户端级 DP，需要对每个客户端上传的**整个模型更新**进行范数裁剪，然后服务器在聚合前加入噪声。

这两种级别的 DP 提供了不同粒度的保护，其选择取决于具体的应用场景和隐私目标。

#### 缓解策略 3：通过[安全聚合](@entry_id:754615)实现密码学安全

除了统计性的隐私保护方法 DP，还可以采用基于[密码学](@entry_id:139166)的**[安全聚合](@entry_id:754615) (Secure Aggregation)** 协议。[安全聚合](@entry_id:754615)的目标是让服务器能够计算出所有客户端更新的总和 $\sum_{k \in U} g_k$（其中 $U$ 是[本轮](@entry_id:169326)成功完成的客户端集合），但无法看到任何单个客户端的更新 $g_k$。

一个健壮的[安全聚合](@entry_id:754615)协议必须满足三个核心安全目标 [@problem_id:4840285]：
1.  **正确性 (Correctness)**：在足够多的客户端（例如，多于一个阈值 $q$）成功完成协议后，服务器得到的输出必须精确地等于它们更新的总和（允许有可忽略的[密码学](@entry_id:139166)失败概率）。
2.  **隐私性 (Privacy)**：服务器（即使与部分客户端合谋）从协议执行过程中获得的所有信息，除了最终的总和之外，无法泄露关于单个客户端输入的任何额外信息。这通常通过“[计算不可区分性](@entry_id:275861)”来形式化定义：对于任意两组产生相同总和的不同输入，服务器的“视图”在计算上是无法区分的。
3.  **掉线容忍性 (Dropout Resilience)**：协议必须能够处理客户端在执行过程中意外掉线的情况。如果最终完成的客户端数量少于阈值 $q$，协议必须安全地中止，并且除了“参与者不足”这一事实外，不泄露任何信息。

[安全聚合](@entry_id:754615)与差分隐私提供了两种互补的隐私保护路径：前者通过[密码学](@entry_id:139166)技术隐藏个体贡献，保证聚合结果的精确性；后者通过注入噪声来模糊个体贡献，提供统计上的隐私保证。在实践中，两者常常被结合使用，以构建一个既能抵御推断攻击又能提供形式化隐私保证的强大联邦学习系统。