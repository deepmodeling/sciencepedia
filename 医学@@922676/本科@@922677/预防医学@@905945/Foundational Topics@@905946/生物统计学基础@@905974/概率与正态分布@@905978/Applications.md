## 应用与跨学科联系

### 引言

在前面的章节中，我们深入探讨了正态分布的数学原理和核心机制。现在，我们将视角从理论转向实践，探索正态分布在预防医学及相关交叉学科中的广泛应用。本章的目的不是重复介绍基本概念，而是展示这些概念如何被用于解决现实世界中的复杂问题，从而彰显其在科学研究和临床实践中的强大效用。您将看到，正态分布不仅是一个优美的数学构造，更是一个不可或缺的工具，它帮助我们为生物过程建模、评估干预措施、综合科学证据，并最终做出更明智的公共卫生决策。

### 建模与描述健康相关现象

正态分布最直接的应用之一是作为描述连续性生物学变量的经验模型。尽管很少有生物学过程严格遵循正态分布，但在许多情况下，它提供了一个足够精确且数学上易于处理的近似，为我们理解数据变异性提供了坚实的基础。

#### 建立和解释参考区间

在临床检验和预防医学中，一个核心任务是区分“正常”与“异常”。参考区间（reference interval）便是为此目的而设定的一个数值范围，它通常包含了健康人群中某个生物标志物（biomarker）中心 $95\%$ 的数值。当有理由假设该标志物在健康人群中的分布近似正态时，我们可以利用[正态分布的性质](@entry_id:273225)来构建这一区间。具体而言，一个中心 $95\%$ 的参考区间可以通过样本均值 $\bar{x}$ 和样本标准差 $s$ 来估计，其范围为 $\bar{x} \pm 1.96s$。这里的 $1.96$ 是[标准正态分布](@entry_id:184509)的双侧 $97.5\%$ [分位数](@entry_id:178417)，代表了均值两侧各占据 $2.5\%$ 尾部概率的[临界点](@entry_id:142397)。

然而，这种基于正态理论的方法高度依赖于其分布假设的有效性。在样本量较小或分布形态未知（例如，可能存在[偏态](@entry_id:178163)或[重尾](@entry_id:274276)）的情况下，正态假设可能不成立，从而导致参考区间的估计产生偏倚。作为替代，[非参数方法](@entry_id:138925)直接从数据的[经验分布](@entry_id:274074)中估计分位数，不依赖于特定的分布形式，因此具有更好的稳健性。但其代价是，在小样本中，非参数估计的区间端点完全由少数几个极端观测值决定，因而其估计精度（或称效率）较低，在不同样本间的变异性更大。因此，在构建参考区间时，研究者必须在依赖正态假设所带来的高效率和[非参数方法](@entry_id:138925)所提供的稳健性之间做出权衡。[@problem_id:4563621]

#### 标准化实现可比性：Z-分数

在大型多中心筛查项目或流行病学研究中，不同研究中心或实验室可能使用不同的检测方法和单位来测量同一个生物标志物，这给数据汇总和标准制定带来了巨大挑战。例如，一项跨地区的心血管疾病筛查项目，各站点测量的[低密度脂蛋白胆固醇](@entry_id:172654)（LDL-C）可能因检测分析方法的不同而具有不同的均值和变异性。

为了解决这一问题，我们可以利用正态分布的原理对原始测量值进行标准化。通过将一个原始测量值 $x$ 减去其所在群体的均值 $\mu$，再除以该群体的标准差 $\sigma$，我们得到了一个标准化的分数，即 Z-分数（Z-score）：
$$ Z = \frac{x - \mu}{\sigma} $$
这个转换过程具有两个关键的数学性质。首先，Z-分数是一个无量纲的纯数，因为它是由两个具有相同物理单位的量（分子 $x-\mu$ 和分母 $\sigma$）相除得到的。这消除了不同测量单位（如 mg/dL vs. mmol/L）带来的影响。其次，如果[原始变量](@entry_id:753733) $X$ 服从正态分布 $N(\mu, \sigma^2)$，那么经过[线性变换](@entry_id:143080)得到的 $Z$ 将服从普适的标准正态分布 $N(0, 1^2)$，其均值为 $0$，标准差为 $1$。

通过这一转换，所有来自不同中心的测量值都被映射到了同一个标准正态分布上。一个来自A中心的Z-分数为 $1.5$ 的个体和一个来自B中心的Z-分数为 $1.5$ 的个体，无论他们原始的测量值和单位是什么，都具有相同的相对位置：他们的测量值都精确地高于各自中心所在人群均值的 $1.5$ 个标准差。这为跨研究中心设定统一的干预或转诊阈值提供了公平且科学的依据。[@problem_id:4563668]

#### 测量误差建模及其后果

在预防医学实践中，我们测量的许多指标都不可避免地存在误差。正态分布为我们理解和量化这些误差及其对结论的影响提供了强大的模型框架。

##### 回归到均值

“回归到均值”（regression to the mean）是筛查和临床观察中一个普遍存在但又常常被误解的现象。假设我们对一个人的某个生理指标（如血压）进行重复测量。其单次观测值 $X_t$ 可以被建模为真实稳定水平 $S$ 与一次随机测量误差 $e_t$ 的和，即 $X_t = S + e_t$。如果真实水平 $S$ 在人群中服从正态分布 $N(\mu, \sigma_S^2)$，而测量误差 $e_t$ 服从独立的均值为零的正态分布 $N(0, \sigma_e^2)$，那么单次观测值 $X_t$ 的边缘分布将是 $N(\mu, \sigma_S^2 + \sigma_e^2)$。

在一个高血压筛查项目中，如果仅根据第一次测量值 $X_1$ 偏高（例如，高于某一阈值 $T$）来筛选个体进行复测，那么这些被选中个体的第二次测量值 $X_2$ 的平均水平，将有向群体均值 $\mu$ 回落的趋势。其原因在于，第一次测量值偏高，不仅可能是因为个体的真实血压 $S$ 高，还可能叠加了一个较大的正向[随机误差](@entry_id:144890) $e_1$。而在第二次测量时，[随机误差](@entry_id:144890) $e_2$ 是重新抽样的，其期望为零。因此，第二次测量值不再受到第一次选择性偏倚的影响，其期望会更接近于该组个体的真实平均血压，而后者本身就比极端观测值更靠近总体均值。这种因选择极端样本而导致的后续测量值向均值“回归”的现象，其大小可以通过条件期望 $\mathbb{E}[X_2 - X_1 | X_1 \ge T]$ 来精确量化。这个[期望值](@entry_id:150961)可以被证明与测量的可靠性（reliability）$\rho = \frac{\sigma_S^2}{\sigma_S^2 + \sigma_e^2}$ 负相关，可靠性越低（即测量误差越大），回归到均值的效应就越显著。[@problem_id:4563676]

##### 回归衰减

测量误差的影响不仅体现在重复测量的变化中，更深刻地影响着我们对变量间关系的估计。在流行病学研究中，我们常常希望探究某个暴露因素 $X$（如每日钠摄入量）与某个健康结局 $Y$（如心血管疾病风险）之间的关系，其真实关系可表示为线性模型 $Y = \alpha + \beta X + \varepsilon$。然而，暴露因素通常是通过有误差的测量工具（如膳食问卷）获得的，我们得到的是测量值 $W$ 而非真实值 $X$。在经典[测量误差模型](@entry_id:751821)中，我们假设 $W = X + U$，其中测量误差 $U$ 独立于真实值 $X$ 且均值为零。

如果研究者在不知情的情况下，使用带有误差的测量值 $W$ 去拟合 $Y$ 的[回归模型](@entry_id:163386)，得到的斜率估计值 $\beta_W$ 将会系统性地偏离真实的斜率 $\beta$。可以从协方差和方差的基本性质推导出，在样本量足够大时，$\beta_W$ 的[期望值](@entry_id:150961)与真实值 $\beta$ 的关系为：
$$ \mathbb{E}[\hat{\beta}_W] \approx \beta \left( \frac{\sigma_X^2}{\sigma_X^2 + \sigma_U^2} \right) $$
其中 $\sigma_X^2$ 是真实暴露值的方差，$\sigma_U^2$ 是测量误差的方差。括号内的因子被称为“衰减因子”（attenuation factor），其值永远小于1。这意味着，由测量误差导致的回归斜率总是被“衰减”或“拉向”零。这种现象被称为“回归衰减”（regression attenuation）。这一结论对于解释为何在[观察性研究](@entry_id:174507)中发现的关联强度常常弱于预期至关重要，并催生了许多用于校正测量误差的复杂统计方法。[@problem_id:4563642]

### 临床实践中的决策与风险评估

正态分布模型不仅用于描述数据，更在临床决策的多个层面扮演着关键角色，从诊断测试的评估到临床操作风险的量化。

#### 评估诊断与筛查测试：ROC曲线

[受试者工作特征](@entry_id:634523)（Receiver Operating Characteristic, ROC）曲线是评估连续性诊断标志物性能的标准工具。ROC曲线描绘了当诊断阈值变化时，[真阳性率](@entry_id:637442)（灵敏度）与[假阳性率](@entry_id:636147)（1-特异度）之间的权衡关系。

在“双正态模型”（binormal model）的假设下，我们可以推导出[ROC曲线](@entry_id:182055)的精确数学形式。该模型假设标志物 $X$ 在患病群体 ($D$) 和非患病群体 ($\bar{D}$) 中分别服从不同的正态分布：$X|D \sim N(\mu_1, \sigma_1^2)$ 和 $X|\bar{D} \sim N(\mu_0, \sigma_0^2)$。对于任意一个诊断阈值 $c$，假阳性率 $u = \mathbb{P}(X > c | \bar{D})$ 和真阳性率 $v = \mathbb{P}(X > c | D)$ 都可以用标准正态[累积分布函数](@entry_id:143135) $\Phi$ 表示。通过消除参数 $c$，我们可以得到真阳性率 $v$ 作为假阳性率 $u$ 的函数，即[ROC曲线](@entry_id:182055)方程：
$$ v(u) = \Phi\left(\frac{\mu_1 - \mu_0}{\sigma_1} + \frac{\sigma_0}{\sigma_1}\Phi^{-1}(u)\right) $$
此外，[ROC曲线](@entry_id:182055)下的面积（Area Under the Curve, AUC）是一个衡量测试总体辨别能力的综合指标，其数值等于从患病群体中随机抽取一个个体，其标志物值大于从非患病群体中随机抽取一个个体标志物值的概率，即 $\mathbb{P}(X_1 > X_0)$。在双正态模型下，这个概率可以被精确计算出来，因为 $X_1 - X_0$ 的差值也服从一个正态分布。最终，AUC的闭合解形式为：
$$ \text{AUC} = \Phi\left(\frac{\mu_1 - \mu_0}{\sqrt{\sigma_1^2 + \sigma_0^2}}\right) $$
这些公式不仅为评估和比较不同的诊断测试提供了理论框架，也构成了许多现代诊断医学统计软件的算法基础。[@problem_id:4563657]

#### 量化操作风险与指导临床方案

正态分布模型同样可以用来评估医疗操作的风险，并为临床方案和设备选择提供循证依据。例如，在创伤急救中，对张力性气胸患者进行紧急针刺减压是一项拯救生命的操作。操作的成功与否取决于所用导管的长度是否足以穿透患者的胸壁到达胸膜腔。

如果我们可以通过先前的研究，将特定人群在标准穿刺点的胸壁厚度 $T$ 建模为一个正态分布，比如均值为 $\mu = 4.0 \text{ cm}$，标准差为 $\sigma = 1.0 \text{ cm}$。那么，对于一根长度为 $L=5.0 \text{ cm}$ 的导管，其穿刺失败（即 $T > L$）的概率就可以通过计算正态分布的尾部概率来得到：$\mathbb{P}(T > 5.0)$。这个概率值，例如计算得出约为 $15.87\%$，具有极其重要的临床意义。它意味着使用该长度的导管，在目标人群中平均每六次操作中就有一次会因为导管太短而失败。对于张力性气胸这样的致命急症，如此高的失败率是不可接受的。这个量化结果为临床指南推荐使用更长的导管（如 $8 \text{ cm}$）或考虑替代操作（如手指开胸）提供了强有力的统计学证据，凸显了基于[概率模型](@entry_id:265150)进行风险评估在提高患者安全方面的重要作用。[@problem_id:4644114]

#### 从心理测量分数到临床诊断

在[医学遗传学](@entry_id:262833)和精神病学领域，许多诊断涉及到对认知或行为功能的评估。智商（IQ）是一个典型的例子，其在普通人群中的分布通常被建模为均值为100，标准差为15的正态分布。根据这个模型，我们可以计算出IQ分数低于某个阈值（如70）的人群比例，这个阈值通常是智力障碍（Intellectual Disability, ID）诊断的必要条件之一。例如，IQ低于70的比例可以通过计算标准正态分布在-2以下的尾部概率得到，约为 $2.28\%$。

然而，临床上的智力障碍诊断是一个多维度的概念，它不仅要求IQ分数显著低于平均水平，还要求个体在适应性功能（如沟通、自理、社交技能）方面存在显著缺陷。因此，满足低IQ标准的人群比例，只是满足了诊断标准的一个子集。根据概率论的基本公理，两个事件（低IQ和适应性功能缺陷）同时发生的概率，必然小于或等于其中任何一个事件单独发生的概率。因此，仅由IQ分数计算出的 $2.28\%$ 构成了一个智力障碍真实患病率的理论上限。这个例子清晰地展示了如何应用正态分布来量化一个连续性状的分布，并结合基本的概率原理，来严谨地区分一个基于分数的统计学分类和一个需要综合判断的临床诊断。[@problem_id:5039738]

#### 解释不同测量环境下的差异

临床实践中，同一生理指标在不同情境下的测量值可能存在系统性差异。一个典型的例子是“白大褂效应”（white coat effect），即患者在诊室测量的血压（office blood pressure）常常会系统性地高于其在日常生活中的清醒动态血压（ambulatory blood pressure）。

我们可以将这种差异建模。假设诊室血压与动态血压的差值 $D = O - A$ 服从一个正态分布，其均值 $\mu_D > 0$ 代表了白大褂效应的平均大小，而其标准差 $\sigma_D$ 则代表了这种效应的个体间变异性。例如，假设收缩压的差值 $D_S \sim N(10, 8^2)$，舒张压的差值 $D_D \sim N(5, 6^2)$，且二者相互独立。对于一个诊室血压为 $140/90 \text{ mmHg}$ 的患者，我们可以利用这个模型来推断其真实动态血压低于某个临床目标（如 $130/80 \text{ mmHg}$）的概率。这个问题可以转化为计算 $\mathbb{P}(A_S  130 \text{ and } A_D  80)$，通过代数变换，这等价于计算 $\mathbb{P}(D_S > 10 \text{ and } D_D > 10)$。利用[正态分布的性质](@entry_id:273225)和独立性假设，我们可以精确计算出这个联合概率。这种方法使得临床医生能够超越单一的、可能具有误导性的诊室读数，对患者的真实心血管风险做出更为精确的概率性评估。[@problem_id:4507143]

### [统计推断](@entry_id:172747)与证据综合

正态分布是频率学派[统计推断](@entry_id:172747)的理论基石。许多核心统计方法，如[假设检验](@entry_id:142556)和[置信区间](@entry_id:138194)的构建，都直接或间接地依赖于正态分布或其衍生分布（如[t分布](@entry_id:267063)、卡方分布和[F分布](@entry_id:261265)）。

#### 比较干预效果的[假设检验与置信区间](@entry_id:176458)

在预防医学研究中，一个核心任务是评估一项新干预措施（如生活方式指导）是否比标准疗法更有效。这通常通过随机对照试验（RCT）来实现。

##### 推断基础：双样本比较

假设一项试验比较了干预组（T）和[对照组](@entry_id:188599)（C）在降低收缩压方面的效果。我们关心的参数是两组真实平均降压差值 $\Delta = \mu_T - \mu_C$。如果我们假设每组内的降压值服从正态分布，那么两组样本均值的差值 $\hat{\Delta} = \bar{X}_T - \bar{X}_C$ 的抽样分布也将是正态的。其均值为 $\Delta$，方差为两组样本均值方差之和，即 $\frac{\sigma_T^2}{n_T} + \frac{\sigma_C^2}{n_C}$。基于这个[抽样分布](@entry_id:269683)，我们可以构建 $\Delta$ 的 $95\%$ [置信区间](@entry_id:138194)，其形式为 $\hat{\Delta} \pm 1.96 \times \text{SE}(\hat{\Delta})$，其中 $\text{SE}(\hat{\Delta})$ 是差值的[标准误](@entry_id:635378)。这个区间为我们提供了一个关于真实效果大小的、具有指定[置信水平](@entry_id:182309)的估计范围，是报告试验结果的标准方式。构建这个区间依赖于几个关键假设：结局的正态性、组间独立性以及组内观测的独立性。[@problem_id:4563697]

##### 检验与区间的对偶性

[假设检验与置信区间](@entry_id:176458)是同一枚硬币的两面，它们之间存在着一种被称为“对偶性”（duality）的深刻联系。对于一个双侧[假设检验](@entry_id:142556)，其原假设为 $H_0: \Delta = \Delta_0$（例如，$\Delta_0=0$ 表示无效果差异），在显著性水平 $\alpha$ 下不拒绝原假设，当且仅当 $\Delta_0$ 这个值落在了为 $\Delta$ 构建的 $100(1-\alpha)\%$ [置信区间](@entry_id:138194)之内。反之，如果在显著性水平 $\alpha$ 下拒绝原假设，则等价于 $\Delta_0$ 值落在了[置信区间](@entry_id:138194)之外。例如，在一项血压研究中，如果计算出的 $\Delta$ 的 $95\%$ [置信区间](@entry_id:138194)是 $[-6.3, 0.7]$，因为它包含了0，我们就可以立即推断出，在 $\alpha=0.05$ 的水平上，我们无法拒绝“干预措施无效果”的原假设。理解这种对偶性对于正确解读统计结果至关重要。[@problem_id:4563626]

##### 处理复杂情况：[Behrens-Fisher问题](@entry_id:169861)

在更现实的情况下，我们通常不知道总体的方差，并且也没有理由假设两组的方差相等。在比较两个正态分布总体的均值时，如果方差未知且不等，就产生了所谓的“[Behrens-Fisher问题](@entry_id:169861)”。在这种情况下，用于检验的标准化统计量 $\frac{(\bar{X}_1 - \bar{X}_2) - (\mu_1 - \mu_2)}{\sqrt{s_1^2/n_1 + s_2^2/n_2}}$ 并不严格服从任何一个标准的t分布。

Welch-Satterthwaite近似方法为这个问题提供了一个优雅的解决方案。它通过[矩匹配](@entry_id:144382)的方法，推导出一个“[有效自由度](@entry_id:161063)” $\nu$，使得上述统计量的分布可以用一个自由度为 $\nu$ 的t分布来近似。这个[有效自由度](@entry_id:161063)的计算公式为：
$$ \nu \approx \frac{\left( \frac{s_{1}^{2}}{n_{1}} + \frac{s_{2}^{2}}{n_{2}} \right)^{2}}{\frac{\left(\frac{s_{1}^{2}}{n_{1}}\right)^{2}}{n_{1}-1} + \frac{\left(\frac{s_{2}^{2}}{n_{2}}\right)^{2}}{n_{2}-1}} $$
这个自由度的值通常不是整数，它依赖于两组的样本量和样本方差。这种方法，通常被称为Welch's t-test，因其在方差不等时仍能很好地控制[第一类错误](@entry_id:163360)率，已成为比较两组均值的默认和推荐方法。[@problem_id:4563684]

#### 综合证据：Meta分析的角色

在循证医学中，单一研究的结果往往不足以形成定论。Meta分析是一种统计方法，它将多个针对同一问题的独立研究结果进行定量合并，从而得到一个更精确、更可靠的总体效应估计。正态分布在Meta分析的理论框架中扮演着核心角色。

##### [固定效应模型](@entry_id:142997)

最简单的Meta分析模型是[固定效应模型](@entry_id:142997)（fixed-effects model）。它假设所有研究都在估计同一个真实的、共同的效应量 $\theta$（例如，对数风险比）。每个研究 $i$ 提供的效应估计值 $Y_i$ 被建模为从一个以 $\theta$ 为中心的正态分布中的一次抽样，即 $Y_i \sim N(\theta, \sigma_i^2)$，其中 $\sigma_i^2$ 是第 $i$ 个研究的内部方差，通常被视为已知。

在这一模型下，通过最大似然估计法可以推导出 $\theta$ 的最佳估计量 $\hat{\theta}$。这个估计量是各个研究效应值的加权平均，权重恰好是每个研究方差的倒数，即 $w_i = 1/\sigma_i^2$。这种“逆方差加权”（inverse-variance weighting）的策略非常直观：它给予了更精确（即方差更小）的研究更大的权重。合并后的效应估计量为：
$$ \hat{\theta} = \frac{\sum_{i=1}^k w_i Y_i}{\sum_{i=1}^k w_i} $$
其方差为 $\text{Var}(\hat{\theta}) = 1 / \sum_{i=1}^k w_i$，比任何单个研究的方差都要小，体现了证据合并的力量。[@problem_id:4563670]

##### 随机效应模型

[固定效应模型](@entry_id:142997)的假设过于严格，现实中不同研究的真实效应很可能存在差异，这种差异被称为“异质性”（heterogeneity）。[随机效应模型](@entry_id:143279)（random-effects model）通过引入一个额外的[方差分量](@entry_id:267561) $\tau^2$ 来对此建模。该模型是一个正态-正态层级模型：每个研究的真实效应 $\theta_i$ 本身就是从一个以总体平均效应 $\mu$ 为中心、方差为 $\tau^2$ 的超分布（hyperdistribution）中抽取的，即 $\theta_i \sim N(\mu, \tau^2)$。而观测值 $y_i$ 依然是以 $\theta_i$ 为中心的抽样，即 $y_i|\theta_i \sim N(\theta_i, s_i^2)$。

在这种模型下，合并效应量的估计不仅需要考虑研究内部的方差 $s_i^2$，还需要估计研究间的方差 $\tau^2$。经典的DerSimonian-Laird方法提供了一种基于矩量法的 $\tau^2$ 估计量。该方法的核心是计算Cochran's Q统计量，它衡量了观测到的研究间变异是否超出了纯粹由[抽样误差](@entry_id:182646)所能解释的范围。通过将观测到的Q值与其在模型下的[期望值](@entry_id:150961)相等，就可以解出 $\tau^2$ 的估计值。一旦获得了 $\hat{\tau}^2$，就可以计算新的权重 $w_i^* = 1/(s_i^2 + \hat{\tau}^2)$，并得到随机效应模型下的合并效应量及其方差。[随机效应模型](@entry_id:143279)因其对异质性的合理解释，在现代Meta分析中得到了更广泛的应用。[@problem_id:4563682]

#### 作为近似工具的正态分布

现实世界中的数据并非总是源于一个正态分布，尤其是像感染人数这样的计数数据。然而，[中心极限定理](@entry_id:143108)（Central Limit Theorem, CLT）为正态分布的广泛应用提供了强大的理论支持，使其能够作为许多其他分布在大样本下的近似。

##### 离散数据的近似：[二项分布](@entry_id:141181)与正态分布的联系

[二项分布](@entry_id:141181) $B(n,p)$ 描述了在 $n$ 次独立的[伯努利试验](@entry_id:268355)中，“成功”（如检测阳性）事件发生次数的概率分布。当试验次数 $n$ 很大，且成功概率 $p$ 不接近0或1时，[二项分布](@entry_id:141181)的形状会变得非常接近一个正态分布。具体来说，一个二项随机变量 $X$ 可以被一个均值为 $np$，方差为 $np(1-p)$ 的正态分布很好地近似。

这个近似的质量关键取决于方差 $np(1-p)$ 的大小。一个常用的[经验法则](@entry_id:262201)是，当 $np(1-p) \ge 10$ 时，近似效果通常可以接受。其背后的数学原理是，二项分布的标准化[偏度](@entry_id:178163)（skewness）为 $\frac{1-2p}{\sqrt{np(1-p)}}$。当方差 $np(1-p)$ 增大时，[偏度](@entry_id:178163)趋近于零，分布变得更加对称，从而与对称的正态分布更加吻合。这种近似使得我们可以用[正态分布的性质](@entry_id:273225)来方便地计算大规模筛查或流行病学调查中阳性案例数或患病率的概率区间和[置信区间](@entry_id:138194)，极大地简化了计算。[@problem_id:4563689]

### 跨学科联系：公共卫生监测

正态分布的应用远远超出了传统的生物统计学范畴，并渗透到许多交叉学科领域。一个突出的例子是它在[公共卫生监测](@entry_id:170581)中的应用，特别是借鉴了工业工程领域[统计过程控制](@entry_id:186744)（Statistical Process Control, SPC）的思想。

#### 用于疫情暴发探测的[统计过程控制](@entry_id:186744)图

公共卫生机构需要实时监测疾病报告数据，以便及时发现疫情暴发。这项任务可以看作是探测一个[数据流](@entry_id:748201)（如每日呼吸道疾病报告数）是否偏离了其在“受控”状态（即非暴发期）下的[正常模式](@entry_id:139640)。假设在非暴发期，经过标准化处理后的每日监测统计量 $Z_t$ 服从独立的标准正态分布 $N(0,1)$。

我们可以构建多种[控制图](@entry_id:184113)来监测这个数据流。每种[控制图](@entry_id:184113)都定义了一个报警统计量和一个控制上限，一旦统计量超过上限，系统就会发出警报。
- **休哈特（Shewhart）[控制图](@entry_id:184113)**：最简单的方法，它只关心当前的数据点。其报警规则是 $Z_t > K_S$。这种方法对突然出现的大幅信号（如单日病例数激增）反应最快。
- **累积和（CUSUM）[控制图](@entry_id:184113)**：该方法累积历史数据中的微小偏差。例如，一个移动求和规则的报警统计量是最近 $w$ 天的统计量之和 $S_t(w) = \sum_{j=0}^{w-1} Z_{t-j}$。这个和服从 $N(0,w)$ 分布。CUSUM对持久但微小的信号（如疫情的缓慢爬坡）更为敏感。
- **指数加权[移动平均](@entry_id:203766)（EWMA）[控制图](@entry_id:184113)**：它通过[递归公式](@entry_id:160630) $E_t = \lambda Z_t + (1-\lambda)E_{t-1}$ 来平滑数据，给予近期数据更高的权重。在[稳态](@entry_id:139253)下，$E_t$ 服从一个均值为0，方差为 $\frac{\lambda}{2-\lambda}$ 的正态分布。EWMA在灵敏度和响应速度之间提供了一种平衡。

对于所有这些方法，控制上限 $K$ 的设定是一个关键的权衡：设得太高会降低探测灵敏度（漏报），设得太低则会增加误报率。通过利用[正态分布的性质](@entry_id:273225)，我们可以精确地计算出在给定的控制上限下，每日的误报概率 $q$。进一步，通过概率论的[补集](@entry_id:161099)法则，我们可以将每日误报率 $q$ 与在某个监测周期（如 $n$ 天）内至少发生一次误报的总概率 $\alpha_{\text{overall}}$ 联系起来，即 $\alpha_{\text{overall}} = 1 - (1-q)^n$。这使得公共卫生官员能够根据可接受的总误报水平，科学地设定各种[控制图](@entry_id:184113)的报警阈值，从而建立起一套既灵敏又可靠的自动化监测系统。[@problem_id:4563662]