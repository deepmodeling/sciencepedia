{"hands_on_practices": [{"introduction": "在进行任何稳健性分析之前，确保数据的高质量是首要步骤。测量过程中的伪影或误差可能产生离群点，从而扭曲分析结果。本练习将引导你使用中位数绝对偏差（Median Absolute Deviation, MAD）等稳健统计学方法，这些方法能有效识别离群点而不受其极端值的影响 [@problem_id:4538489]。通过这项实践，你将掌握构建一个实用的质量控制流程的技能，这是任何可靠的放射组学工作流中不可或缺的一环。", "problem": "要求您实现一个稳健的异常值检测和质量控制程序，用于评估放射组学特征的稳健性。其应用场景是：在采集或分割过程存在扰动的情况下，对单个放射组学特征进行重复测量，通过移除异常值并检验剩余测量值的变异性来评估其稳健性。您的程序必须是一个完整的、可运行的程序，能够对预定义的测试套件执行分析，并以指定格式输出结果。\n\n使用的基本原理：\n- 一组实数 $x_1,\\dots,x_n$ 的中位数 (median) 是指一个值 $m$，使得至少一半的数据不大于 $m$，且至少一半的数据不小于 $m$。\n- 中位数绝对偏差 (Median Absolute Deviation, MAD) 定义为 $\\operatorname{MAD} = \\operatorname{median}(|x_i - m|)$，其中 $m$ 是样本的中位数。\n- 对于一个标准差为 $\\sigma$ 的正态分布变量，中位数绝对偏差的期望值与 $\\sigma$ 成正比。比例常数是标准正態随机变量绝对值的中位数，该值等于一个正常数，通常近似为 $c \\approx 0.67448975$。这就得出了标准正态一致性关系。\n- 样本均值为 $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i$，无偏样本标准差为 $s = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} (x_i - \\bar{x})^2}$（对于 $n \\ge 2$）。\n- 变异系数 (coefficient of variation, CV) 定义为 $\\operatorname{CV} = \\frac{s}{|\\bar{x}|}$，它是一个无量纲的量，用于量化相对变异性。\n\n需要实现的算法要求：\n1. 对于每个特征测量向量 $x = [x_1,\\dots,x_n]$，使用稳健 $z$ 分数检测异常值。该分数通过将偏离中位数的值用中位数绝对偏差 (MAD) 和标准正态一致性常数进行缩放来定义。如果 $\\operatorname{MAD} > 0$，计算稳健 $z$ 分数，并将绝对稳健 $z$ 分数超过给定阈值 $\\tau_z$ 的测量值 $x_i$ 标记为异常值。\n2. 如果 $\\operatorname{MAD} = 0$，则回退到基于四分位距 (IQR) 的规则：计算第一和第三四分位数 $Q_1$ 和 $Q_3$，以及四分位距 $\\operatorname{IQR} = Q_3 - Q_1$，如果观测值 $x_i$ 满足 $x_i < Q_1 - k \\cdot \\operatorname{IQR}$ 或 $x_i > Q_3 + k \\cdot \\operatorname{IQR}$（其中 $k = 1.5$），则将其标记为异常值。如果 $\\operatorname{IQR} = 0$，此规则简化为标记任何在区间 $[Q_1, Q_3]$ 之外的 $x_i$。\n3. 移除异常值，并计算剩余观测值的比例 $f = \\frac{n_{\\text{remain}}}{n}$。同时，使用无偏样本标准差 $s$ 和样本均值的绝对值 $|\\bar{x}|$，计算剩余观测值的样本变异系数 $\\operatorname{CV}$。如果 $n_{\\text{remain}} < 2$ 或 $|\\bar{x}| = 0$，则在决策时将 $\\operatorname{CV}$ 视为 $+\\infty$。\n4. 一个特征当且仅当同时满足以下两个条件时，才通过质量控制：$f \\ge p_{\\min}$ 和 $\\operatorname{CV} \\le \\tau_{\\text{cv}}$。\n\n您的程序必须精确实现上述逻辑，并评估以下测试套件。每个测试案例是一个元组，包含特征向量 $x$、稳健 $z$ 分数阈值 $\\tau_z$、变异系数阈值 $\\tau_{\\text{cv}}$，以及移除异常值后要求保留的最小比例 $p_{\\min}$。所有数值均为实数，任何小数阈值都应解释为纯小数，而非百分比。\n\n测试套件：\n- 案例 A（典型情况，变异性小，无异常值）：$x = [0.99, 1.02, 1.01, 0.98, 1.00, 1.03, 0.97]$, $\\tau_z = 3.5$, $\\tau_{\\text{cv}} = 0.025$, $p_{\\min} = 0.8$。\n- 案例 B（两个严重异常值，剩余比例足够）：$x = [1.00, 0.99, 1.01, 1.02, 0.98, 3.00, -1.50]$, $\\tau_z = 3.5$, $\\tau_{\\text{cv}} = 0.03$, $p_{\\min} = 0.7$。\n- 案例 C（若干中度偏差值，严格的异常值阈值导致剩余比例低）：$x = [1.00, 1.00, 1.00, 2.00, -3.00, 2.20, -2.50]$, $\\tau_z = 1.0$, $\\tau_{\\text{cv}} = 0.10$, $p_{\\min} = 0.8$。\n- 案例 D（中位数绝对偏差为零，但有极端值；回退到四分位距规则）：$x = [5.00, 5.00, 5.00, 5.00, 5.00, 10.00, -10.00]$, $\\tau_z = 3.5$, $\\tau_{\\text{cv}} = 0.001$, $p_{\\min} = 0.7$。\n- 案例 E（均值接近零，导致变异系数无界）：$x = [0.01, -0.01, 0.02, -0.02, 0.00, 0.005, -0.005]$, $\\tau_z = 3.5$, $\\tau_{\\text{cv}} = 1.0$, $p_{\\min} = 1.0$。\n- 案例 F（所有值相同）：$x = [2.50, 2.50, 2.50, 2.50]$, $\\tau_z = 3.5$, $\\tau_{\\text{cv}} = 0.001$, $p_{\\min} = 1.0$。\n\n输出规范：\n- 对每个案例，输出一个布尔值，指示该特征是否通过上述规则的质量控制。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的结果列表，例如 $[{\\tt True},{\\tt False},\\dots]$，每个测试案例对应一个条目，按所给顺序排列。\n- 此问题不涉及物理单位。", "solution": "该问题要求实现一个统计程序，用于评估放射组学特征的稳健性。这是通过量化从一组重复测量中移除异常值后特征的稳定性来实现的。该过程包括两个主要阶段：稳健的异常值检测和剩余数据点的稳定性分析。一个特征如果同时满足剩余数据比例和移除异常值后变异系数的标准，则被认为是稳健的。\n\n对于给定的特征测量向量 $x = [x_1, x_2, \\dots, x_n]$，分析过程如下：\n\n**1. 异常值检测**\n\n异常值检测采用两级方法，优先使用一种能够抵抗极端值存在的稳健统计方法。\n\n**1.1. 中位数绝对偏差 (MAD) 方法**\n\n识别异常值的主要方法基于中位数绝对偏差 (Median Absolute Deviation, MAD)，这是一种稳健的统计离散度度量。首先，计算样本中位数 $m$。\n$$\nm = \\operatorname{median}(x)\n$$\n然后，MAD 被计算为样本中位数绝对偏差的中位数：\n$$\n\\operatorname{MAD} = \\operatorname{median}(|x_i - m|)\n$$\n如果 $\\operatorname{MAD} > 0$，我们可以定义一个稳健 $z$ 分数，也称为修正 $z$ 分数。该分数使用从 MAD 推导出的标准差的稳健估计值，对偏离中位数的数据进行标准化。对于正态分布，标准差 $\\sigma$ 可以通过 $\\hat{\\sigma} = \\operatorname{MAD}/c$ 来估计，其中 $c \\approx 0.67448975$ 是一个一致性常数，等于标准正态变量绝对值的中位数。因此，每个测量值 $x_i$ 的稳健 $z$ 分数为：\n$$\nz_{\\text{robust}, i} = \\frac{x_i - m}{\\hat{\\sigma}} = \\frac{c \\cdot (x_i - m)}{\\operatorname{MAD}}\n$$\n如果一个测量值 $x_i$ 的绝对稳健 $z$ 分数超过预定义的阈值 $\\tau_z$，则将其标记为异常值：\n$$\n|z_{\\text{robust}, i}| > \\tau_z\n$$\n\n**1.2. 四分位距 (IQR) 回退方法**\n\n在 $\\operatorname{MAD} = 0$ 的情况下，稳健 $z$ 分数是未定义的，这种情况通常在超过一半的数据点相同时发生。对于这些情况，算法将回退到由 John Tukey 提出的广泛使用的 IQR 方法。首先，计算数据的第一四分位数（$Q_1$，第 $25$ 百分位数）和第三四分位数（$Q_3$，第 $75$ 百分位数）。四分位距是它们之间的差值：\n$$\n\\operatorname{IQR} = Q_3 - Q_1\n$$\n如果一个测量值 $x_i$ 位于以下定义的范围之外，则被识别为异常值：\n$$\nx_i < Q_1 - k \\cdot \\operatorname{IQR} \\quad \\text{或} \\quad x_i > Q_3 + k \\cdot \\operatorname{IQR}\n$$\n其中问题指定常数 $k=1.5$。如果 $\\operatorname{IQR}=0$，此规则正确地简化为标记任何不等于 $Q_1=Q_3$ 的点。\n\n**2. 移除异常值后的稳定性分析**\n\n在识别并移除所有异常值后，使用剩余的 $n_{\\text{remain}}$ 个数据点评估特征的稳定性。\n\n**2.1. 剩余观测值比例 ($f$)**\n\n非异常值的数据点比例计算如下：\n$$\nf = \\frac{n_{\\text{remain}}}{n}\n$$\n这个量衡量了测量值的一致性。低比例表示存在大量异常值，因此再现性差。\n\n**2.2. 变异系数 (CV)**\n\n剩余数据的相对变异性通过样本变异系数 ($\\operatorname{CV}$) 进行量化。首先，计算筛选后数据的样本均值 $\\bar{x}$ 和无偏样本标准差 $s$：\n$$\n\\bar{x} = \\frac{1}{n_{\\text{remain}}} \\sum_{i=1}^{n_{\\text{remain}}} x_{i, \\text{filtered}}\n$$\n$$\ns = \\sqrt{\\frac{1}{n_{\\text{remain}}-1}\\sum_{i=1}^{n_{\\text{remain}}} (x_{i, \\text{filtered}} - \\bar{x})^2}\n$$\n然后，CV 是标准差与均值绝对值的比率：\n$$\n\\operatorname{CV} = \\frac{s}{|\\bar{x}|}\n$$\nCV 的计算定义了特殊条件。如果剩余数据点的数量 $n_{\\text{remain}} < 2$，则样本标准差未定义。如果样本均值 $\\bar{x} = 0$，CV 在数学上是未定义的。在这两种情况下，为了决策目的，CV 被视为功能上的无穷大（$+\\infty$），代表最大的变异性。\n\n**3. 质量控制决策**\n\n一个特征被认为是稳健的并通过质量控制检查，当且仅当同时满足以下两个条件：\n1.  剩余观测值的比例不小于最小阈值 $p_{\\min}$：$f \\ge p_{\\min}$。\n2.  剩余观测值的变异系数不超过最大阈值 $\\tau_{\\text{cv}}$：$\\operatorname{CV} \\le \\tau_{\\text{cv}}$。\n\n该算法框架提供了一个严谨、自动化的程序，用于基于已建立的统计原理评估特征的稳健性。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for radiomic feature robustness analysis.\n    \"\"\"\n    \n    def evaluate_feature_robustness(x, tau_z, tau_cv, p_min):\n        \"\"\"\n        Performs outlier detection and quality control for a single feature vector.\n\n        Args:\n            x (list): A list of feature measurements.\n            tau_z (float): The threshold for the robust z-score.\n            tau_cv (float): The threshold for the coefficient of variation.\n            p_min (float): The minimum required fraction of remaining observations.\n\n        Returns:\n            bool: True if the feature passes quality control, False otherwise.\n        \"\"\"\n        x_arr = np.array(x, dtype=np.float64)\n        n = len(x_arr)\n\n        if n == 0:\n            # An empty feature vector has no data remaining.\n            # It fails unless p_min is 0, but CV is still infinite.\n            # Assuming it fails.\n            return p_min == 0\n\n        # Constants for outlier detection\n        c = 0.67448975  # Normal distribution consistency constant for MAD\n        k = 1.5         # Multiplier for IQR rule\n\n        # 1. Compute median and Median Absolute Deviation (MAD)\n        m = np.median(x_arr)\n        # Calculate absolute deviations from the median\n        abs_dev = np.abs(x_arr - m)\n        mad = np.median(abs_dev)\n\n        # 2. Detect outliers based on MAD or IQR\n        if mad > 0:\n            # Primary method: Robust z-score using MAD\n            # The robust z-score is c * (x_i - m) / MAD\n            robust_z_scores = c * abs_dev / mad\n            is_outlier = robust_z_scores > tau_z\n        else:\n            # Fallback method: Interquartile Range (IQR) rule\n            q1 = np.percentile(x_arr, 25)\n            q3 = np.percentile(x_arr, 75)\n            iqr = q3 - q1\n            lower_bound = q1 - k * iqr\n            upper_bound = q3 + k * iqr\n            is_outlier = (x_arr < lower_bound) | (x_arr > upper_bound)\n        \n        # 3. Filter data and compute stability metrics\n        x_filtered = x_arr[~is_outlier]\n        n_remain = len(x_filtered)\n\n        # Compute fraction of remaining observations\n        f = n_remain / n if n > 0 else 0\n\n        # Compute Coefficient of Variation (CV)\n        cv = np.inf  # Default to infinity as per problem spec\n        if n_remain >= 2:\n            mean_filtered = np.mean(x_filtered)\n            # Check for mean being close to zero to avoid division by zero\n            if not np.isclose(mean_filtered, 0):\n                std_filtered = np.std(x_filtered, ddof=1) # Unbiased sample std dev\n                cv = std_filtered / np.abs(mean_filtered)\n        \n        # 4. Apply quality control criteria\n        passes_f_check = f >= p_min\n        passes_cv_check = cv <= tau_cv\n        \n        return passes_f_check and passes_cv_check\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A (typical, small variability, no outliers)\n        ([0.99, 1.02, 1.01, 0.98, 1.00, 1.03, 0.97], 3.5, 0.025, 0.8),\n        # Case B (two gross outliers, sufficient remaining fraction)\n        ([1.00, 0.99, 1.01, 1.02, 0.98, 3.00, -1.50], 3.5, 0.03, 0.7),\n        # Case C (several moderate deviations, strict outlier threshold)\n        ([1.00, 1.00, 1.00, 2.00, -3.00, 2.20, -2.50], 1.0, 0.10, 0.8),\n        # Case D (zero MAD; fallback to IQR rule)\n        ([5.00, 5.00, 5.00, 5.00, 5.00, 10.00, -10.00], 3.5, 0.001, 0.7),\n        # Case E (mean near zero causing unbounded CV)\n        ([0.01, -0.01, 0.02, -0.02, 0.00, 0.005, -0.005], 3.5, 1.0, 1.0),\n        # Case F (all values identical)\n        ([2.50, 2.50, 2.50, 2.50], 3.5, 0.001, 1.0),\n    ]\n\n    results = []\n    for params in test_cases:\n        x_data, z_thresh, cv_thresh, p_min_thresh = params\n        result = evaluate_feature_robustness(x_data, z_thresh, cv_thresh, p_min_thresh)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4538489"}, {"introduction": "保证了数据质量后，下一步是量化特征在技术可变性（例如，扫描仪设置随时间的变化）下的稳定性。体模研究（Phantom studies）为此提供了一个受控的环境。此练习将演示如何计算关键的稳健性指标，如可重复性（以变异系数 $w\\mathrm{CV}$ 衡量）和系统性偏倚（bias），并介绍如何通过校准来修正不同扫描会话间的测量漂移 [@problem_id:4538492]。完成此练习后，你将学会一种在医学影像中广泛应用的标准方法，用于在临床研究中使用定量特征之前验证其一致性。", "problem": "您将获得一项简化的、自洽的体模研究，用于评估在校准漂移情况下放射组学特征的稳健性。一个具有已知参考插件值的体模在两个会话中进行扫描，表示为 $s \\in \\{1,2\\}$，每个会话对每个对象进行 $R$ 次重复采集。对于每个特征，提供了两个带有已知参考（基准）值的校准插件。您的任务是仅使用校准插件将每个会话校准到参考尺度，将校准应用于所有重复测量，然后根据精确定义的第一性原理度量标准来量化稳健性。\n\n推导的基本依据：\n- 样本均值：对于任何有限集 $\\{y_j\\}_{j=1}^{n}$，样本均值为 $\\bar{y} = \\frac{1}{n}\\sum_{j=1}^{n} y_j$。\n- 通过残差平方和计算样本方差：对于残差 $e_j = y_j - \\bar{y}$，残差平方和为 $\\mathrm{SSR} = \\sum_{j=1}^{n} e_j^2$，当均值从相同的 $n$ 个点估计时，无偏方差估计为 $\\hat{\\sigma}^2 = \\mathrm{SSR}/(n-1)$。\n- 线性最小二乘仿射拟合：给定数据对 $\\{(x_k, t_k)\\}_{k=1}^{K}$，最小化 $\\sum_{k=1}^{K} (a x_k + b - t_k)^2$ 的仿射映射 $y = a x + b$ 可由线性最小二乘法的正规方程导出。\n\n基于这些基础需要实现的定义：\n1) 按会话进行的线性最小二乘校准。对于每个会话 $s$，确定实数标量 $(a_s, b_s)$ 以最小化 $\\sum_{k=1}^{K} (a_s x_{s,k} + b_s - t_k)^2$，其中 $x_{s,k}$ 是会话 $s$ 中测量的校准值，$t_k$ 是对应的参考值。那么，会话 $s$ 中任何测量值 $x$ 的校准值为 $y = a_s x + b_s$。\n2) 会话内合并重复性标准差。对于每个会话 $s$ 和对象 $i$，设 $R$ 次重复测量为 $\\{x_{s,i,r}\\}_{r=1}^{R}$，其均值为 $\\bar{x}_{s,i} = \\frac{1}{R}\\sum_{r=1}^{R} x_{s,i,r}$。定义残差 $e_{s,i,r} = x_{s,i,r} - \\bar{x}_{s,i}$。跨两个会话的会话内合并标准差 $s_r$ 是以下公式的平方根：\n$$\n\\hat{\\sigma}_r^2 = \\frac{\\sum_{s=1}^{2}\\sum_{i=1}^{N}\\sum_{r=1}^{R} e_{s,i,r}^2}{\\sum_{s=1}^{2}\\sum_{i=1}^{N}(R-1)}.\n$$\n3) 跨两个会话和所有重复测量的总均值：\n$$\n\\mu = \\frac{1}{2 N R} \\sum_{s=1}^{2}\\sum_{i=1}^{N}\\sum_{r=1}^{R} x_{s,i,r}.\n$$\n4) 对象内变异系数（无量纲），定义为 $w\\mathrm{CV} = s_r / \\mu$。\n5) 使用每个对象的会话均值计算会话间偏倚。对于每个对象 $i$，计算 $\\bar{x}_{1,i}$ 和 $\\bar{x}_{2,i}$，然后定义 $d_i = \\bar{x}_{2,i} - \\bar{x}_{1,i}$，偏倚为 $b = \\frac{1}{N}\\sum_{i=1}^{N} d_i$。相对偏倚为 $|b|/\\mu$。\n6) 校准后绝对偏倚的相对减少量。如果校准前相对偏倚为 $\\beta_{\\mathrm{pre}}$，校准后相对偏倚为 $\\beta_{\\mathrm{post}}$，则定义减少量为：\n$$\n\\rho_{\\mathrm{red}} =\n\\begin{cases}\n1,  \\text{if } \\beta_{\\mathrm{pre}} = 0 \\text{ and } \\beta_{\\mathrm{post}} = 0,\\\\\n0,  \\text{if } \\beta_{\\mathrm{pre}} = 0 \\text{ and } \\beta_{\\mathrm{post}} > 0,\\\\\n\\max\\left(0, \\min\\left(1, \\frac{\\beta_{\\mathrm{pre}} - \\beta_{\\mathrm{post}}}{\\beta_{\\mathrm{pre}}}\\right)\\right),  \\text{if } \\beta_{\\mathrm{pre}} > 0.\n\\end{cases}\n$$\n\n稳健性判定规则（在校准后应用）：一个特征被判定为稳健，当且仅当以下所有条件同时成立：\n- $w\\mathrm{CV}_{\\mathrm{post}} \\le \\tau$，\n- $\\beta_{\\mathrm{post}} \\le \\delta$，\n- $\\rho_{\\mathrm{red}} \\ge \\eta$，\n其中阈值固定为 $\\tau = 0.02$, $\\delta = 0.01$, $\\eta = 0.5$。\n\n本问题中的所有值都是无单位的。没有角度量。\n\n使用的数据。共有 $F = 3$ 个独立特征。对于每个特征 $f \\in \\{1,2,3\\}$，您将获得：\n- 两个带参考值的校准插件 $\\{t_k\\}_{k=1}^{2}$。\n- 会话1中测量的校准值：$\\{x_{1,k}\\}_{k=1}^{2}$ 和会话2中测量的校准值：$\\{x_{2,k}\\}_{k=1}^{2}$。\n- 非校准对象：$N = 4$ 个对象，每个会话中进行 $R = 3$ 次重复测量。对于会话1，您有一个大小为 $4 \\times 3$ 的矩阵 $\\{x_{1,i,r}\\}$；对于会话2，您有一个大小为 $4 \\times 3$ 的矩阵 $\\{x_{2,i,r}\\}$。校准插件不得用于重复性或偏倚计算；这些度量标准仅使用 $N=4$ 个非校准对象。\n\n特征 1：\n- 校准参考值：$[\\, $50$, $150$ \\,]$.\n- 会话1校准测量值：$[\\, $50$, $140$ \\,]$.\n- 会话2校准测量值：$[\\, $47$, $157$ \\,]$.\n- 会话1非校准重复测量值（行代表对象 $i=1,\\dots,4$，列代表重复次数 $r=1,2,3$）：\n  [\n    [ $67.5$, $68.0$, $68.5$ ],\n    [ $94.7$, $95.1$, $95.3$ ],\n    [ $121.6$, $122.0$, $122.4$ ],\n    [ $148.3$, $149.0$, $149.7$ ]\n  ]\n- 会话2非校准重复测量值：\n  [\n    [ $68.4$, $69.0$, $69.6$ ],\n    [ $101.6$, $102.1$, $102.5$ ],\n    [ $134.5$, $135.1$, $135.6$ ],\n    [ $167.4$, $168.0$, $168.6$ ]\n  ]\n\n特征 2：\n- 校准参考值：$[\\, $40$, $80$ \\,]$.\n- 会话1校准测量值：$[\\, $40$, $80$ \\,]$.\n- 会话2校准测量值：$[\\, $44$, $88$ \\,]$.\n- 会话1非校准重复测量值：\n  [\n    [ $45$, $50$, $55$ ],\n    [ $57$, $60$, $63$ ],\n    [ $84$, $90$, $96$ ],\n    [ $112$, $120$, $128$ ]\n  ]\n- 会话2非校准重复测量值：\n  [\n    [ $60$, $55$, $50$ ],\n    [ $72$, $66$, $60$ ],\n    [ $105$, $99$, $93$ ],\n    [ $140$, $132$, $124$ ]\n  ]\n\n特征 3：\n- 校准参考值：$[\\, $30$, $60$ \\,]$.\n- 会话1校准测量值：$[\\, $27$, $54$ \\,]$.\n- 会话2校准测量值：$[\\, $33$, $66$ \\,]$.\n- 会话1非校准重复测量值：\n  [\n    [ $36$, $36$, $36$ ],\n    [ $45$, $45$, $45$ ],\n    [ $63$, $63$, $63$ ],\n    [ $81$, $81$, $81$ ]\n  ]\n- 会话2非校准重复测量值：\n  [\n    [ $44$, $44$, $44$ ],\n    [ $55$, $55$, $55$ ],\n    [ $77$, $77$, $77$ ],\n    [ $99$, $99$, $99$ ]\n  ]\n\n编程任务：\n- 对每个特征独立地，仅使用提供的基本依据和定义，计算校准前相对偏倚 $\\beta_{\\mathrm{pre}}$ 以及校准后的 $w\\mathrm{CV}_{\\mathrm{post}}$、$\\beta_{\\mathrm{post}}$ 和减少量 $\\rho_{\\mathrm{red}}$。\n- 根据规则（$\\tau = 0.02$, $\\delta = 0.01$, $\\eta = 0.5$）判定稳健性。\n\n测试套件：\n- 这三个特征共同构成了三个测试用例，分别涵盖了一个典型案例、一个高方差案例以及一个会话内方差为零的边界案例。\n\n最终输出格式：\n- 您的程序应产生单行输出，其中包含对特征1、2和3的三个稳健性判定结果，格式为方括号内以逗号分隔的布尔值列表（例如，$[\\,\\mathrm{True},\\mathrm{False},\\mathrm{True}\\,]$）。输出必须只有一行，不含任何额外文本。", "solution": "该问题要求基于一项简化的体模研究，对放射组学特征的稳健性进行定量评估。分析涉及三个独立的特征，每个特征都根据一组精确定义的度量标准和阈值进行评估。每个特征的处理过程包括三个主要阶段：计算校准前偏倚，确定并应用特定于会话的线性校准，以及计算校准后稳健性度量。如果一个特征同时满足与其校准后重复性、偏倚以及通过校准实现的偏倚减少量相关的三个标准，则该特征被视为稳健。\n\n该方法严格遵守所提供的定义。设 $x_{s,i,r}$ 表示会话 $s \\in \\{1,2\\}$、非校准对象 $i \\in \\{1,\\dots,N\\}$ 和重复次数 $r \\in \\{1,\\dots,R\\}$ 的测量值，其中 $N=4$ 且 $R=3$。\n\n分析过程如下：\n\n首先，我们计算校准前的相对偏倚 $\\beta_{\\mathrm{pre}}$。这作为评估校准有效性的基线。对于每个对象 $i$，计算每个会话的平均测量值：\n$$ \\bar{x}_{s,i} = \\frac{1}{R}\\sum_{r=1}^{R} x_{s,i,r} $$\n对象 $i$ 的会话间差异为 $d_i = \\bar{x}_{2,i} - \\bar{x}_{1,i}$。校准前偏倚 $b_{\\mathrm{pre}}$ 是所有 $N$ 个对象这些差异的平均值：\n$$ b_{\\mathrm{pre}} = \\frac{1}{N}\\sum_{i=1}^{N} d_i $$\n此偏倚通过校准前总均值 $\\mu_{\\mathrm{pre}}$ 进行归一化，该均值是所有 $2NR$ 次测量的平均值：\n$$ \\mu_{\\mathrm{pre}} = \\frac{1}{2NR} \\sum_{s=1}^{2}\\sum_{i=1}^{N}\\sum_{r=1}^{R} x_{s,i,r} $$\n于是，校准前相对偏倚为 $\\beta_{\\mathrm{pre}} = |b_{\\mathrm{pre}}| / \\mu_{\\mathrm{pre}}$。\n\n第二，我们进行逐会话校准。对于每个会话 $s$，给定 $K=2$ 对测量的校准值 $\\{x_{s,k}\\}_{k=1}^{2}$ 及其对应的已知参考值 $\\{t_k\\}_{k=1}^{2}$。我们寻求一个仿射变换 $y = a_s x + b_s$，它将测量尺度映射到参考尺度。参数 $(a_s, b_s)$ 通过最小化平方差之和 $\\sum_{k=1}^{K} (a_s x_{s,k} + b_s - t_k)^2$ 来找到。对于 $K=2$ 的情况，这对应于找到穿过两个点 $(x_{s,1}, t_1)$ 和 $(x_{s,2}, t_2)$ 的唯一一条直线。线性方程组\n\\begin{align*} a_s x_{s,1} + b_s = t_1 \\\\ a_s x_{s,2} + b_s = t_2 \\end{align*}\n的解由以下公式给出：\n$$ a_s = \\frac{t_1 - t_2}{x_{s,1} - x_{s,2}} \\quad \\text{和} \\quad b_s = t_1 - a_s x_{s,1} $$\n条件是 $x_{s,1} \\neq x_{s,2}$，这对于本问题中的所有特征都成立。因此，为每个特征确定了两个不同的校准函数：用于会话1的 $(a_1, b_1)$ 和用于会话2的 $(a_2, b_2)$。\n\n第三，我们将这些变换应用于非校准测量值，以获得校准值 $y_{s,i,r}$：\n$$ y_{s,i,r} = a_s x_{s,i,r} + b_s $$\n所有后续的稳健性度量均使用此校准后的数据集 $\\{y_{s,i,r}\\}$ 进行计算。\n\n我们计算稳健性决策所需的三个校准后度量：\n\n1.  对象内变异系数，$w\\mathrm{CV}_{\\mathrm{post}}$。该度量量化了特征的重复性。我们首先计算会话内合并标准差 $s_{r, \\mathrm{post}}$。对于每个会话 $s$ 中的每个对象 $i$，我们计算其校准后重复测量的均值 $\\bar{y}_{s,i} = \\frac{1}{R}\\sum_{r=1}^{R} y_{s,i,r}$。残差平方和（SSR）在所有测量上进行累加：\n    $$ \\mathrm{SSR}_{\\mathrm{post}} = \\sum_{s=1}^{2}\\sum_{i=1}^{N}\\sum_{r=1}^{R} (y_{s,i,r} - \\bar{y}_{s,i})^2 $$\n    合并方差是此SSR除以总自由度 $DF = \\sum_{s=1}^{2}\\sum_{i=1}^{N}(R-1) = 2N(R-1)$：\n    $$ \\hat{\\sigma}_{r, \\mathrm{post}}^2 = \\frac{\\mathrm{SSR}_{\\mathrm{post}}}{2N(R-1)} $$\n    标准差为 $s_{r, \\mathrm{post}} = \\sqrt{\\hat{\\sigma}_{r, \\mathrm{post}}^2}$。此值通过校准后总均值 $\\mu_{\\mathrm{post}} = \\frac{1}{2NR} \\sum_{s,i,r} y_{s,i,r}$ 进行归一化，以得出无量纲系数：\n    $$ w\\mathrm{CV}_{\\mathrm{post}} = \\frac{s_{r, \\mathrm{post}}}{\\mu_{\\mathrm{post}}} $$\n\n2.  校准后相对偏倚，$\\beta_{\\mathrm{post}}$。该度量量化了校准后会话之间剩余的系统性差异。其计算过程与校准前偏倚的计算过程类似，但使用校准后的均值 $\\bar{y}_{s,i}$。校准后偏倚为 $b_{\\mathrm{post}} = \\frac{1}{N}\\sum_{i=1}^{N} (\\bar{y}_{2,i} - \\bar{y}_{1,i})$。相对偏倚为：\n    $$ \\beta_{\\mathrm{post}} = \\frac{|b_{\\mathrm{post}}|}{\\mu_{\\mathrm{post}}} $$\n\n3.  绝对偏倚的相对减少量，$\\rho_{\\mathrm{red}}$。该度量评估了校准在减少系统误差方面的有效性。其定义如下：\n    $$ \\rho_{\\mathrm{red}} =\n    \\begin{cases}\n    1,  \\text{if } \\beta_{\\mathrm{pre}} = 0 \\text{ and } \\beta_{\\mathrm{post}} = 0,\\\\\n    0,  \\text{if } \\beta_{\\mathrm{pre}} = 0 \\text{ and } \\beta_{\\mathrm{post}} > 0,\\\\\n    \\max\\left(0, \\min\\left(1, \\frac{\\beta_{\\mathrm{pre}} - \\beta_{\\mathrm{post}}}{\\beta_{\\mathrm{pre}}}\\right)\\right),  \\text{if } \\beta_{\\mathrm{pre}} > 0.\n    \\end{cases}\n    $$\n\n最后，一个特征被判定为稳健，当且仅当使用给定的阈值 $\\tau = 0.02$, $\\delta = 0.01$, and $\\eta = 0.5$，同时满足以下所有三个条件：\n- $w\\mathrm{CV}_{\\mathrm{post}} \\le \\tau$\n- $\\beta_{\\mathrm{post}} \\le \\delta$\n- $\\rho_{\\mathrm{red}} \\ge \\eta$\n\n这一完整流程将独立地应用于所提供的三个特征中的每一个。最终输出是一个布尔值列表，表明每个特征的稳健性状态。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the radiomics feature robustness problem for three given features.\n    \"\"\"\n    # Thresholds\n    TAU = 0.02\n    DELTA = 0.01\n    ETA = 0.5\n\n    # Number of objects and repeats\n    N = 4\n    R = 3\n    \n    # Structure to hold data for the three features\n    features_data = [\n        {\n            \"cal_ref\": np.array([50.0, 150.0]),\n            \"cal_s1\": np.array([50.0, 140.0]),\n            \"cal_s2\": np.array([47.0, 157.0]),\n            \"meas_s1\": np.array([\n                [67.5, 68.0, 68.5],\n                [94.7, 95.1, 95.3],\n                [121.6, 122.0, 122.4],\n                [148.3, 149.0, 149.7]\n            ]),\n            \"meas_s2\": np.array([\n                [68.4, 69.0, 69.6],\n                [101.6, 102.1, 102.5],\n                [134.5, 135.1, 135.6],\n                [167.4, 168.0, 168.6]\n            ])\n        },\n        {\n            \"cal_ref\": np.array([40.0, 80.0]),\n            \"cal_s1\": np.array([40.0, 80.0]),\n            \"cal_s2\": np.array([44.0, 88.0]),\n            \"meas_s1\": np.array([\n                [45.0, 50.0, 55.0],\n                [57.0, 60.0, 63.0],\n                [84.0, 90.0, 96.0],\n                [112.0, 120.0, 128.0]\n            ]),\n            \"meas_s2\": np.array([\n                [60.0, 55.0, 50.0],\n                [72.0, 66.0, 60.0],\n                [105.0, 99.0, 93.0],\n                [140.0, 132.0, 124.0]\n            ])\n        },\n        {\n            \"cal_ref\": np.array([30.0, 60.0]),\n            \"cal_s1\": np.array([27.0, 54.0]),\n            \"cal_s2\": np.array([33.0, 66.0]),\n            \"meas_s1\": np.array([\n                [36.0, 36.0, 36.0],\n                [45.0, 45.0, 45.0],\n                [63.0, 63.0, 63.0],\n                [81.0, 81.0, 81.0]\n            ]),\n            \"meas_s2\": np.array([\n                [44.0, 44.0, 44.0],\n                [55.0, 55.0, 55.0],\n                [77.0, 77.0, 77.0],\n                [99.0, 99.0, 99.0]\n            ])\n        }\n    ]\n\n    robustness_decisions = []\n\n    for feature in features_data:\n        x1 = feature[\"meas_s1\"]\n        x2 = feature[\"meas_s2\"]\n\n        # 1. Pre-calibration relative bias (beta_pre)\n        x1_means = np.mean(x1, axis=1)\n        x2_means = np.mean(x2, axis=1)\n        \n        b_pre = np.mean(x2_means - x1_means)\n        mu_pre = np.mean(np.concatenate((x1, x2)))\n        \n        # Handle case where mu_pre is zero, although not expected with this data\n        beta_pre = np.abs(b_pre) / mu_pre if mu_pre != 0 else 0\n\n        # 2. Session-wise calibration\n        t_k = feature[\"cal_ref\"]\n        \n        # Session 1 calibration\n        x1_k = feature[\"cal_s1\"]\n        a1 = (t_k[0] - t_k[1]) / (x1_k[0] - x1_k[1])\n        b1 = t_k[0] - a1 * x1_k[0]\n\n        # Session 2 calibration\n        x2_k = feature[\"cal_s2\"]\n        a2 = (t_k[0] - t_k[1]) / (x2_k[0] - x2_k[1])\n        b2 = t_k[0] - a2 * x2_k[0]\n\n        # Apply calibration\n        y1 = a1 * x1 + b1\n        y2 = a2 * x2 + b2\n\n        # 3. Post-calibration metrics\n        \n        # wCV_post\n        ssr_post = 0.0\n        # SSR for session 1\n        for i in range(N):\n            ssr_post += np.sum((y1[i, :] - np.mean(y1[i, :]))**2)\n        # SSR for session 2\n        for i in range(N):\n            ssr_post += np.sum((y2[i, :] - np.mean(y2[i, :]))**2)\n        \n        df = 2 * N * (R - 1)\n        pooled_var_post = ssr_post / df if df > 0 else 0\n        s_r_post = np.sqrt(pooled_var_post)\n        \n        mu_post = np.mean(np.concatenate((y1, y2)))\n        wCV_post = s_r_post / mu_post if mu_post != 0 else 0\n\n        # beta_post\n        y1_means = np.mean(y1, axis=1)\n        y2_means = np.mean(y2, axis=1)\n        \n        b_post = np.mean(y2_means - y1_means)\n        beta_post = np.abs(b_post) / mu_post if mu_post != 0 else 0\n\n        # rho_red\n        if beta_pre == 0:\n            rho_red = 1.0 if beta_post == 0 else 0.0\n        else:\n            reduction = (beta_pre - beta_post) / beta_pre\n            rho_red = max(0.0, min(1.0, reduction))\n\n        # 4. Robustness decision\n        is_robust = (wCV_post <= TAU) and (beta_post <= DELTA) and (rho_red >= ETA)\n        robustness_decisions.append(is_robust)\n        \n    print(f\"[{','.join(map(str, robustness_decisions))}]\")\n\nsolve()\n```", "id": "4538492"}, {"introduction": "为何特征的稳健性如此重要？因为不稳定的特征会使临床预测模型的输出变得不可靠。本练习将特征测量误差与其对模型性能的最终影响直接联系起来。我们将运用一阶泰勒展开这一强大的近似工具，来估算输入特征的微小误差是如何通过逻辑回归模型传播，并最终影响预测概率的 [@problem_id:4538483]。这项练习将让你对特征不稳定性的下游后果有一个具体而定量的理解，从而强调了先前练习中稳健性分析的关键价值。", "problem": "一个影像组学实验室使用从感兴趣区域（Region of Interest, ROI）中提取的三个标准化影像特征，构建了一个二元恶性肿瘤分类器。该分类器是一个逻辑斯蒂预测模型，其线性预测器 $\\eta$ 和预测概率 $p$ 定义为 $\\eta = \\beta_{0} + \\beta_{1} x_{1} + \\beta_{2} x_{2} + \\beta_{3} x_{3}$ 和 $p = \\frac{1}{1 + \\exp(-\\eta)}$。对于某位特定患者，真实的标准化特征值为 $x_{1} = 0.5$，$x_{2} = -1.0$ 和 $x_{3} = 0.8$。学习到的系数为 $\\beta_{0} = -1.2$，$\\beta_{1} = 0.8$，$\\beta_{2} = -0.5$ 和 $\\beta_{3} = 0.3$。\n\n由于不同评估者之间的分割变异性，每个测量特征 $x_{i}^{\\text{meas}}$ 与其真实值 $x_{i}$ 之间存在一个加性测量误差 $e_{i}$，因此 $x_{i}^{\\text{meas}} = x_{i} + e_{i}$。误差向量 $\\mathbf{e} = (e_{1}, e_{2}, e_{3})^{\\top}$ 被建模为一个零均值随机向量，其协方差矩阵为\n$$\n\\Sigma_{e} = \\begin{pmatrix}\n0.04  0.012  -0.002 \\\\\n0.012  0.09  0.006 \\\\\n-0.002  0.006  0.01\n\\end{pmatrix}.\n$$\n\n从逻辑斯蒂模型的定义出发，并使用关于真实特征向量的一阶泰勒展开，结合随机向量的期望、方差和协方差的性质，推导对于给定患者的预测概率平方变化的期望值的主阶近似：\n$$\n\\mathbb{E}\\!\\left[\\left(p\\!\\left(\\mathbf{x} + \\mathbf{e}\\right) - p\\!\\left(\\mathbf{x}\\right)\\right)^{2}\\right],\n$$\n计算所提供参数下的数值。将最终答案表示为小数值，并四舍五入到四位有效数字。不需要单位。", "solution": "问题要求计算由于特征 $\\mathbf{x}$ 中的测量误差 $\\mathbf{e}$ 导致的预测概率平方变化的期望值 $\\mathbb{E}\\!\\left[\\left(p(\\mathbf{x} + \\mathbf{e}) - p(\\mathbf{x})\\right)^{2}\\right]$ 的主阶近似。分析过程通过对函数 $p(\\mathbf{y})$ 在真实特征向量 $\\mathbf{x}$ 周围应用一阶泰勒展开来进行。\n\n设 $p(\\mathbf{y})$ 为输入特征向量 $\\mathbf{y} = (y_1, y_2, y_3)^\\top$ 的预测概率。测量得到的特征向量为 $\\mathbf{x}^{\\text{meas}} = \\mathbf{x} + \\mathbf{e}$，其中 $\\mathbf{x}$ 是真实特征向量，$\\mathbf{e}$ 是随机误差向量。\n\n$p(\\mathbf{x} + \\mathbf{e})$ 关于 $\\mathbf{x}$ 的一阶泰勒展开由下式给出：\n$$\np(\\mathbf{x} + \\mathbf{e}) \\approx p(\\mathbf{x}) + \\nabla p(\\mathbf{x})^\\top \\mathbf{e}\n$$\n其中 $\\nabla p(\\mathbf{x})$ 是函数 $p$ 在点 $\\mathbf{x}$ 处计算的梯度。\n\n预测概率的变化，我们记为 $\\Delta p$，可以近似为：\n$$\n\\Delta p = p(\\mathbf{x} + \\mathbf{e}) - p(\\mathbf{x}) \\approx \\nabla p(\\mathbf{x})^\\top \\mathbf{e}\n$$\n需要计算的量是这个变化的平方的期望值：\n$$\n\\mathbb{E}[(\\Delta p)^2] \\approx \\mathbb{E}\\left[\\left(\\nabla p(\\mathbf{x})^\\top \\mathbf{e}\\right)^2\\right]\n$$\n真实特征向量 $\\mathbf{x}$ 是一个固定的、非随机的量。因此，梯度 $\\nabla p(\\mathbf{x})$ 是一个常数向量。我们将这个常数向量记为 $\\mathbf{g} = \\nabla p(\\mathbf{x})$。表达式变为：\n$$\n\\mathbb{E}[(\\Delta p)^2] \\approx \\mathbb{E}\\left[(\\mathbf{g}^\\top \\mathbf{e})^2\\right]\n$$\n项 $\\mathbf{g}^\\top \\mathbf{e}$ 是一个标量随机变量。设 $S = \\mathbf{g}^\\top \\mathbf{e}$。我们感兴趣的是 $\\mathbb{E}[S^2]$。我们知道，对于任何随机变量 $S$，其方差由 $\\text{Var}(S) = \\mathbb{E}[S^2] - (\\mathbb{E}[S])^2$ 给出。\n\n让我们计算 $S$ 的期望值：\n$$\n\\mathbb{E}[S] = \\mathbb{E}[\\mathbf{g}^\\top \\mathbf{e}] = \\mathbf{g}^\\top \\mathbb{E}[\\mathbf{e}]\n$$\n问题陈述误差向量 $\\mathbf{e}$ 是一个零均值随机向量，所以 $\\mathbb{E}[\\mathbf{e}] = \\mathbf{0}$。因此，\n$$\n\\mathbb{E}[S] = \\mathbf{g}^\\top \\mathbf{0} = 0\n$$\n由此可知 $\\mathbb{E}[S^2] = \\text{Var}(S)$。线性组合 $S = \\mathbf{g}^\\top \\mathbf{e}$ 的方差由以下二次型给出：\n$$\n\\text{Var}(S) = \\mathbf{g}^\\top \\text{Cov}(\\mathbf{e}) \\mathbf{g} = \\mathbf{g}^\\top \\Sigma_e \\mathbf{g}\n$$\n其中 $\\Sigma_e$ 是误差向量 $\\mathbf{e}$ 的协方差矩阵。\n所以，主阶近似为：\n$$\n\\mathbb{E}[(\\Delta p)^2] \\approx (\\nabla p(\\mathbf{x}))^\\top \\Sigma_e (\\nabla p(\\mathbf{x}))\n$$\n接下来，我们必须找到梯度向量 $\\mathbf{g} = \\nabla p(\\mathbf{x})$。预测概率 $p$ 是通过逻辑斯蒂（sigmoid）函数 $p = (1 + \\exp(-\\eta))^{-1}$ 与线性预测器 $\\eta$ 相关的函数。线性预测器是 $\\eta = \\beta_0 + \\sum_{i=1}^3 \\beta_i x_i$。\n\n使用链式法则，$p$ 关于特征 $x_i$ 的偏导数是：\n$$\n\\frac{\\partial p}{\\partial x_i} = \\frac{d p}{d \\eta} \\frac{\\partial \\eta}{\\partial x_i}\n$$\n导数是：\n$$\n\\frac{\\partial \\eta}{\\partial x_i} = \\beta_i\n$$\n$$\n\\frac{d p}{d \\eta} = \\frac{d}{d\\eta} (1 + \\exp(-\\eta))^{-1} = -1(1 + \\exp(-\\eta))^{-2}(-\\exp(-\\eta)) = \\frac{\\exp(-\\eta)}{(1 + \\exp(-\\eta))^2}\n$$\n逻辑斯蒂函数的一个众所周知的性质是其导数可以表示为 $p(1-p)$。要证明这一点：\n$$\n\\frac{d p}{d \\eta} = \\left(\\frac{1}{1 + \\exp(-\\eta)}\\right) \\left(\\frac{\\exp(-\\eta)}{1 + \\exp(-\\eta)}\\right) = p \\left(\\frac{1 + \\exp(-\\eta) - 1}{1 + \\exp(-\\eta)}\\right) = p \\left(1 - \\frac{1}{1 + \\exp(-\\eta)}\\right) = p(1-p)\n$$\n因此，偏导数是：\n$$\n\\frac{\\partial p}{\\partial x_i} = p(\\mathbf{x})(1-p(\\mathbf{x}))\\beta_i\n$$\n因此，梯度向量 $\\nabla p(\\mathbf{x})$ 是：\n$$\n\\nabla p(\\mathbf{x}) = p(\\mathbf{x})(1-p(\\mathbf{x})) \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\end{pmatrix} = p(\\mathbf{x})(1-p(\\mathbf{x})) \\boldsymbol{\\beta}\n$$\n其中 $\\boldsymbol{\\beta} = (\\beta_1, \\beta_2, \\beta_3)^\\top$。\n\n将此梯度代回到我们的近似公式中：\n\\begin{align*}\n\\mathbb{E}[(\\Delta p)^2] \\approx \\left( p(\\mathbf{x})(1-p(\\mathbf{x})) \\boldsymbol{\\beta} \\right)^\\top \\Sigma_e \\left( p(\\mathbf{x})(1-p(\\mathbf{x})) \\boldsymbol{\\beta} \\right) \\\\\n= \\left( p(\\mathbf{x})(1-p(\\mathbf{x})) \\right)^2 \\boldsymbol{\\beta}^\\top \\Sigma_e \\boldsymbol{\\beta}\n\\end{align*}\n现在，我们使用所提供的数据计算数值。\n给定的参数是：\n$\\mathbf{x} = (0.5, -1.0, 0.8)^\\top$\n$\\beta_0 = -1.2$，$\\beta_1 = 0.8$，$\\beta_2 = -0.5$，$\\beta_3 = 0.3$，所以 $\\boldsymbol{\\beta} = (0.8, -0.5, 0.3)^\\top$。\n$$\n\\Sigma_{e} = \\begin{pmatrix}\n0.04  0.012  -0.002 \\\\\n0.012  0.09  0.006 \\\\\n-0.002  0.006  0.01\n\\end{pmatrix}\n$$\n首先，在真实特征向量 $\\mathbf{x}$ 处计算 $\\eta$ 和 $p$：\n$$\n\\eta(\\mathbf{x}) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 = -1.2 + (0.8)(0.5) + (-0.5)(-1.0) + (0.3)(0.8)\n$$\n$$\n\\eta(\\mathbf{x}) = -1.2 + 0.4 + 0.5 + 0.24 = -1.2 + 1.14 = -0.06\n$$\n$$\np(\\mathbf{x}) = \\frac{1}{1 + \\exp(-\\eta)} = \\frac{1}{1 + \\exp(-(-0.06))} = \\frac{1}{1 + \\exp(0.06)}\n$$\n使用计算器，$\\exp(0.06) \\approx 1.0618365$。\n$$\np(\\mathbf{x}) \\approx \\frac{1}{1 + 1.0618365} = \\frac{1}{2.0618365} \\approx 0.48500287\n$$\n接下来，我们计算标量因子 $(p(1-p))^2$：\n$$\np(\\mathbf{x})(1-p(\\mathbf{x})) \\approx 0.48500287 \\times (1 - 0.48500287) = 0.48500287 \\times 0.51499713 \\approx 0.2497753\n$$\n$$\n\\left( p(\\mathbf{x})(1-p(\\mathbf{x})) \\right)^2 \\approx (0.2497753)^2 \\approx 0.0623877\n$$\n接下来，我们计算二次型 $\\boldsymbol{\\beta}^\\top \\Sigma_e \\boldsymbol{\\beta}$：\n首先，$\\Sigma_e \\boldsymbol{\\beta}$：\n$$\n\\begin{pmatrix}\n0.04  0.012  -0.002 \\\\\n0.012  0.09  0.006 \\\\\n-0.002  0.006  0.01\n\\end{pmatrix}\n\\begin{pmatrix}\n0.8 \\\\\n-0.5 \\\\\n0.3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n(0.04)(0.8) + (0.012)(-0.5) + (-0.002)(0.3) \\\\\n(0.012)(0.8) + (0.09)(-0.5) + (0.006)(0.3) \\\\\n(-0.002)(0.8) + (0.006)(-0.5) + (0.01)(0.3)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0.032 - 0.006 - 0.0006 \\\\\n0.0096 - 0.045 + 0.0018 \\\\\n-0.0016 - 0.003 + 0.003\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0.0254 \\\\\n-0.0336 \\\\\n-0.0016\n\\end{pmatrix}\n$$\n然后，$\\boldsymbol{\\beta}^\\top (\\Sigma_e \\boldsymbol{\\beta})$：\n$$\n\\begin{pmatrix} 0.8  -0.5  0.3 \\end{pmatrix}\n\\begin{pmatrix}\n0.0254 \\\\\n-0.0336 \\\\\n-0.0016\n\\end{pmatrix}\n= (0.8)(0.0254) + (-0.5)(-0.0336) + (0.3)(-0.0016)\n$$\n$$\n= 0.02032 + 0.0168 - 0.00048 = 0.03712 - 0.00048 = 0.03664\n$$\n最后，我们将两部分相乘得到结果：\n$$\n\\mathbb{E}[(\\Delta p)^2] \\approx (0.0623877) \\times (0.03664) \\approx 0.00228585\n$$\n将结果四舍五入到四位有效数字得到 $0.002286$。", "answer": "$$\\boxed{0.002286}$$", "id": "4538483"}]}