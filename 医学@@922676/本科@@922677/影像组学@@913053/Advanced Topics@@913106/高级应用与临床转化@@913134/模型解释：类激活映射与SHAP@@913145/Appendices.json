{"hands_on_practices": [{"introduction": "类激活图 (Class Activation Mapping, CAM) 是一种强大的技术，可以让我们直观地看到卷积神经网络 (CNN) 在做决策时“关注”了图像的哪些区域。这个练习将通过让你从原始的特征图和权重开始，一步步手动构建一个CAM，从而揭开其神秘面纱。通过这个实践，你将对这一基础可视化方法的工作原理有一个具体而深刻的理解。[@problem_id:4551422]", "problem": "在一个简化的放射组学环境中，考虑一个用于对医学图像进行分类的卷积神经网络，其最后一个卷积层生成两个尺寸为 $2\\times 2$ 的空间特征图 $f_1$ 和 $f_2$。类别 $c$ 的分类器在全局平均特征上是线性的，其学习到的权重为 $w_1^c$ 和 $w_2^c$。假设，与类激活图（Class Activation Mapping, CAM）中的标准用法一致，某一空间位置上的正类别证据与使用相同类别权重的该位置特征图的修正线性组合成正比，并且负面证据通过修正被抑制，以强调对类别 $c$ 有正向贡献的区域。\n\n给定\n$f_1=\\begin{bmatrix}1  2\\\\0  1\\end{bmatrix}$，$f_2=\\begin{bmatrix}0  1\\\\1  0\\end{bmatrix}$，$w_1^c=2$ 和 $w_2^c=-1$。\n按以下步骤进行：\n\n1. 根据卷积特征的线性性和空间等变性的基本原理，如上所述构建类别 $c$ 的 $2\\times 2$ 类激活图 $M_c$，包括对负值的修正。\n\n2. 使用角点对齐（$2\\times 2$ 的角点映射到 $4\\times 4$ 的角点）的双线性插值将 $M_c$ 上采样到一个 $4\\times 4$ 的网格。假设间距均匀，通过先沿一个轴应用一维线性插值，再沿另一个轴应用，来定义在整数网格坐标 $(i,j)$ （其中 $i,j\\in\\{0,1,2,3\\}$）处的插值。\n\n3. 在 $4\\times 4$ 的上采样图中找出具有最大插值的三个网格坐标。如果在截断处出现值相同的情况，则按 $(i,j)$ 的字典序打破平局，即优先选择较小的 $i$，然后是较小的 $j$。\n\n4. 将输入分辨率下 SHapley Additive exPlanations (SHAP) 的逐像素归因解释为与上采样的 CAM 成正比，并在整个 $4\\times 4$ 网格上归一化，使其总和为 1。也就是说，如果上采样的值为 $\\{v_{ij}\\}$，则定义 $s_{ij}=v_{ij}/\\sum_{p,q} v_{pq}$。\n\n计算在步骤3中确定的三个坐标上，类似 SHAP 的归因的总和的精确值。将最终答案表示为最简分数。不包含任何单位。", "solution": "该问题要求我们分步计算类激活图（CAM），对其进行上采样，并计算特定坐标上类似 SHAP 的归因值之和。\n\n**第一步：构建类激活图（CAM），$M_c$**\n\n根据定义，类激活图 $M_c$ 是在每个空间位置上对特征图 $f_k$ 进行加权求和，然后应用修正线性单元（ReLU）以保留正贡献。\n线性组合为 $L = w_1^c f_1 + w_2^c f_2$。\n代入给定值：\n$f_1 = \\begin{bmatrix} 1  2 \\\\ 0  1 \\end{bmatrix}, \\quad f_2 = \\begin{bmatrix} 0  1 \\\\ 1  0 \\end{bmatrix}, \\quad w_1^c = 2, \\quad w_2^c = -1$\n$L = 2 \\begin{bmatrix} 1  2 \\\\ 0  1 \\end{bmatrix} - 1 \\begin{bmatrix} 0  1 \\\\ 1  0 \\end{bmatrix} = \\begin{bmatrix} 2  4 \\\\ 0  2 \\end{bmatrix} - \\begin{bmatrix} 0  1 \\\\ 1  0 \\end{bmatrix} = \\begin{bmatrix} 2  3 \\\\ -1  2 \\end{bmatrix}$\n应用 ReLU 函数：\n$M_c = \\text{ReLU}(L) = \\begin{bmatrix} \\max(0, 2)  \\max(0, 3) \\\\ \\max(0, -1)  \\max(0, 2) \\end{bmatrix} = \\begin{bmatrix} 2  3 \\\\ 0  2 \\end{bmatrix}$\n\n**第二步：将 $M_c$ 上采样到 $4\\times 4$ 网格**\n\n我们使用双线性插值将 $2\\times 2$ 的 CAM $M_c$ 上采样到一个 $4\\times 4$ 的网格 $V$。插值结果如下（以分数表示，分母为 9）：\n$$ V = \\frac{1}{9} \\begin{pmatrix}\n18  21  24  27 \\\\\n12  16  20  24 \\\\\n6  11  16  21 \\\\\n0  6  12  18\n\\end{pmatrix} $$\n\n**第三步：找出具有最大插值的三个网格坐标**\n\n我们检查上采样矩阵 $V$ 中的值，并按降序排序。对于相同的值，我们按坐标的字典序 $(i, j)$（先比较行索引 $i$，再比较列索引 $j$）来打破平局。\n1.  最大值是 $27/9 = 3$，位于坐标 $(0, 3)$。\n2.  第二大的值是 $24/9 \\approx 2.67$，出现在两个位置：$(0, 2)$ 和 $(1, 3)$。根据字典序，我们选择 $(0, 2)$ 作为第二个坐标。\n3.  第三个坐标是 $(1, 3)$，其值也为 $24/9$。\n因此，具有最大插值的三个坐标是 $(0, 3)$、$(0, 2)$ 和 $(1, 3)$。\n\n**第四步：计算类似 SHAP 的归因的总和**\n\n首先，计算上采样网格中所有值的总和 $\\sum v_{ij}$。\n$$ \\sum v_{ij} = \\frac{1}{9} ( (18+21+24+27) + (12+16+20+24) + (6+11+16+21) + (0+6+12+18) ) $$\n$$ \\sum v_{ij} = \\frac{1}{9} (90 + 72 + 54 + 36) = \\frac{252}{9} = 28 $$\n然后，计算在步骤3中确定的三个坐标处的归因值 $s_{ij} = v_{ij} / \\sum v$：\n-   $s_{03} = \\frac{v_{03}}{28} = \\frac{3}{28}$\n-   $s_{02} = \\frac{v_{02}}{28} = \\frac{24/9}{28} = \\frac{24}{9 \\times 28} = \\frac{8}{3 \\times 28} = \\frac{2}{21}$\n-   $s_{13} = \\frac{v_{13}}{28} = \\frac{24/9}{28} = \\frac{2}{21}$\n最后，将这三个归因值相加：\n$$ S = s_{03} + s_{02} + s_{13} = \\frac{3}{28} + \\frac{2}{21} + \\frac{2}{21} = \\frac{3}{28} + \\frac{4}{21} $$\n找到公分母 $84$：\n$$ S = \\frac{3 \\times 3}{84} + \\frac{4 \\times 4}{84} = \\frac{9 + 16}{84} = \\frac{25}{84} $$\n该分数已为最简形式。", "answer": "$$\n\\boxed{\\frac{25}{84}}\n$$", "id": "4551422"}, {"introduction": "在 CAM 的基础上，梯度加权类激活图 (Grad-CAM) 使用梯度信息，使其应用范围更广，无需对模型结构进行特殊修改。然而，简单的 Grad-CAM 存在一个潜在的陷阱：由于其最后的 ReLU 激活函数，所有负向贡献（即抑制某个类别的区域）的信息都可能被丢弃。本练习将展示这一现象，并演示如何通过像 SmoothGrad 这样的简单改进技术来获得更稳定和可靠的热图，从而更好地理解模型的决策依据。[@problem_id:4551430]", "problem": "一个卷积神经网络在计算机断层扫描放射组学影像块上进行训练，以预测与肿瘤异质性相关的二元终点。为了解释该模型，考虑最后一个卷积层，它产生两个特征图，每个特征图的空间尺寸为 $2\\times 2$，记为 $A^{(1)}$ 和 $A^{(2)}$。对于一个固定的输入，类别对数几率 $f$ 通过后续层依赖于这些激活值。在基于梯度的归因中，$f$ 在激活值周围的一阶泰勒展开给出了一个局部线性化，其中每个激活值的贡献与其偏导数成正比。在此近似下，梯度加权类激活映射（Grad-CAM）的通道级重要性权重是通过对特征图上的空间梯度进行平均得到的：对于通道 $k$，\n$$\\alpha_{k}=\\frac{1}{Z}\\sum_{i,j}\\frac{\\partial f}{\\partial A^{(k)}_{ij}},$$\n其中 $Z$ 是空间位置的数量。然后，Grad-CAM 热图由应用于特征图的通道加权和的修正线性单元（ReLU）给出：\n$$L_{ij}=\\max\\left(0,\\sum_{k}\\alpha_{k}A^{(k)}_{ij}\\right).$$\n为了探究梯度平滑（SmoothGrad）的效果，我们将标准差为 $\\sigma=0.1$ 的独立零均值高斯噪声注入输入中，为 $N=2$ 个含噪样本重新计算梯度，并平均所得的通道权重：\n$$\\alpha_{k}^{\\mathrm{smooth}}=\\frac{1}{N}\\sum_{s=1}^{N}\\left(\\frac{1}{Z}\\sum_{i,j}\\frac{\\partial f^{(s)}}{\\partial A^{(k)}_{ij}}\\right),$$\n平滑热图也类似地定义为 \n$$L^{\\mathrm{smooth}}_{ij}=\\max\\left(0,\\sum_{k}\\alpha_{k}^{\\mathrm{smooth}}A^{(k)}_{ij}\\right).$$\n假设基线输入的特征图和梯度在数值上指定如下：\n$$A^{(1)}=\\begin{pmatrix}1.2   0.0\\\\ 0.8   0.6\\end{pmatrix},\\quad A^{(2)}=\\begin{pmatrix}0.5   1.1\\\\ 0.0   0.9\\end{pmatrix},$$\n$$G^{(1)}=\\left[\\frac{\\partial f}{\\partial A^{(1)}_{ij}}\\right]=\\begin{pmatrix}-0.4   -0.2\\\\ -0.1   -0.3\\end{pmatrix},\\quad G^{(2)}=\\left[\\frac{\\partial f}{\\partial A^{(2)}_{ij}}\\right]=\\begin{pmatrix}0.3   -0.2\\\\ -0.5   0.1\\end{pmatrix}.$$\n对于在 SmoothGrad 中使用的两个含噪样本（$N=2$），假设梯度为\n$$G^{(1)}_{(s=1)}=\\begin{pmatrix}0.1   -0.05\\\\ 0.0   0.05\\end{pmatrix},\\quad G^{(1)}_{(s=2)}=\\begin{pmatrix}0.2   0.1\\\\ -0.05   0.15\\end{pmatrix},$$\n$$G^{(2)}_{(s=1)}=\\begin{pmatrix}0.15   0.05\\\\ 0.0   0.1\\end{pmatrix},\\quad G^{(2)}_{(s=2)}=\\begin{pmatrix}0.05   0.2\\\\ 0.1   0.15\\end{pmatrix}.$$\n您可以假设，在这两个含噪样本上，微小的输入扰动不会显著改变 $A^{(1)}$ 和 $A^{(2)}$。计算基线梯度的 Grad-CAM 热图 $L_{ij}$ 和使用平滑权重的平滑热图 $L^{\\mathrm{smooth}}_{ij}$。然后，报告平滑热图值的均值，\n$$\\overline{L^{\\mathrm{smooth}}}=\\frac{1}{Z}\\sum_{i,j}L^{\\mathrm{smooth}}_{ij},$$\n作为最终答案。将您的答案四舍五入到四位有效数字。为了上下文的完整性，请注意 SHapley 加性解释（SHAP）是一种互补的特征归因方法，在此数值计算中并未直接使用。", "solution": "该问题要求计算基线 Grad-CAM 热图和使用 SmoothGrad 方法平滑后的热图，并报告平滑热图的均值。特征图的空间位置数量为 $Z=4$。\n\n**第一步：计算基线 Grad-CAM 热图**\n\n首先，使用给定的基线梯度 $G^{(1)}$ 和 $G^{(2)}$ 计算通道权重 $\\alpha_k$：\n$\\alpha_1 = \\frac{1}{Z}\\sum_{i,j}G^{(1)}_{ij} = \\frac{1}{4}(-0.4 - 0.2 - 0.1 - 0.3) = \\frac{-1.0}{4} = -0.25$\n$\\alpha_2 = \\frac{1}{Z}\\sum_{i,j}G^{(2)}_{ij} = \\frac{1}{4}(0.3 - 0.2 - 0.5 + 0.1) = \\frac{-0.3}{4} = -0.075$\n\n然后，计算加权激活图并应用 ReLU：\n$L = \\text{ReLU}(\\alpha_1 A^{(1)} + \\alpha_2 A^{(2)})$\n$L = \\text{ReLU}\\left(-0.25\\begin{pmatrix}1.2  0.0\\\\ 0.8  0.6\\end{pmatrix} - 0.075\\begin{pmatrix}0.5  1.1\\\\ 0.0  0.9\\end{pmatrix}\\right)$\n$L = \\text{ReLU}\\left(\\begin{pmatrix}-0.3  0.0\\\\ -0.2  -0.15\\end{pmatrix} + \\begin{pmatrix}-0.0375  -0.0825\\\\ 0.0  -0.0675\\end{pmatrix}\\right)$\n$L = \\text{ReLU}\\left(\\begin{pmatrix}-0.3375  -0.0825\\\\ -0.2  -0.2175\\end{pmatrix}\\right) = \\begin{pmatrix}0  0\\\\ 0  0\\end{pmatrix}$\n基线 Grad-CAM 热图完全为零，说明负向贡献被 ReLU 抑制了。\n\n**第二步：计算平滑 Grad-CAM 热图**\n\n我们首先计算平滑权重 $\\alpha_{k}^{\\mathrm{smooth}}$，这是通过对两个含噪样本的梯度计算出的权重的平均值。\n对于通道1：\n$\\sum G^{(1)}_{(s=1)} = 0.1 - 0.05 + 0.0 + 0.05 = 0.1$\n$\\sum G^{(1)}_{(s=2)} = 0.2 + 0.1 - 0.05 + 0.15 = 0.4$\n$\\alpha_{1}^{\\mathrm{smooth}} = \\frac{1}{N}\\left(\\frac{\\sum G^{(1)}_{(s=1)}}{Z} + \\frac{\\sum G^{(1)}_{(s=2)}}{Z}\\right) = \\frac{1}{2}\\left(\\frac{0.1}{4} + \\frac{0.4}{4}\\right) = \\frac{1}{2}\\left(\\frac{0.5}{4}\\right) = 0.0625$\n\n对于通道2：\n$\\sum G^{(2)}_{(s=1)} = 0.15 + 0.05 + 0.0 + 0.1 = 0.3$\n$\\sum G^{(2)}_{(s=2)} = 0.05 + 0.2 + 0.1 + 0.15 = 0.5$\n$\\alpha_{2}^{\\mathrm{smooth}} = \\frac{1}{2}\\left(\\frac{0.3}{4} + \\frac{0.5}{4}\\right) = \\frac{1}{2}\\left(\\frac{0.8}{4}\\right) = 0.1$\n\n现在，使用平滑权重计算平滑热图 $L^{\\mathrm{smooth}}$：\n$L^{\\mathrm{smooth}} = \\text{ReLU}(\\alpha_{1}^{\\mathrm{smooth}} A^{(1)} + \\alpha_{2}^{\\mathrm{smooth}} A^{(2)})$\n$L^{\\mathrm{smooth}} = \\text{ReLU}\\left(0.0625\\begin{pmatrix}1.2  0.0\\\\ 0.8  0.6\\end{pmatrix} + 0.1\\begin{pmatrix}0.5  1.1\\\\ 0.0  0.9\\end{pmatrix}\\right)$\n$L^{\\mathrm{smooth}} = \\text{ReLU}\\left(\\begin{pmatrix}0.075  0.0\\\\ 0.05  0.0375\\end{pmatrix} + \\begin{pmatrix}0.05  0.11\\\\ 0.0  0.09\\end{pmatrix}\\right)$\n$L^{\\mathrm{smooth}} = \\text{ReLU}\\left(\\begin{pmatrix}0.125  0.11\\\\ 0.05  0.1275\\end{pmatrix}\\right) = \\begin{pmatrix}0.125  0.11\\\\ 0.05  0.1275\\end{pmatrix}$\n\n**第三步：计算平滑热图的均值**\n\n最后，计算 $\\overline{L^{\\mathrm{smooth}}}$：\n$\\sum_{i,j}L^{\\mathrm{smooth}}_{ij} = 0.125 + 0.11 + 0.05 + 0.1275 = 0.4125$\n$\\overline{L^{\\mathrm{smooth}}} = \\frac{1}{Z}\\sum_{i,j}L^{\\mathrm{smooth}}_{ij} = \\frac{0.4125}{4} = 0.103125$\n四舍五入到四位有效数字，结果为 $0.1031$。", "answer": "$$\\boxed{0.1031}$$", "id": "4551430"}, {"introduction": "从可视化转向定量归因，SHAP (SHapley Additive exPlanations) 提供了一个基于博弈论的坚实框架来为每个特征分配其对预测的贡献值。这个练习旨在解决一个关于线性模型特征重要性的常见误解，即认为特征的重要性仅由其系数决定。你将通过计算发现，SHAP 值通过将特征值置于其背景分布中进行考量，从而提供了更全面的归因，并揭示了为什么特征的影响力不仅仅是其模型系数那么简单。[@problem_id:4551432]", "problem": "在一项影像组学研究中，一个线性风险评分是根据从计算机断层扫描中提取的两个定量特征构建的。将这些特征记为 $X_1$ 和 $X_2$。训练队列具有以下性质：\n- 这些特征在统计上是独立的。\n- 均值为 $\\mu_1 = 0$ 和 $\\mu_2 = 0$。\n- 标准差为 $\\sigma_1 = 3$ 和 $\\sigma_2 = 1$。\n\n该模型是线性函数 $f(\\mathbf{x}) = \\beta \\left(x_1 + x_2\\right)$，其中 $\\beta = 2$ 且截距为零。为了解释模型，我们考虑 Shapley 加性解释 (SHAP)，它通过合作博弈论的 Shapley 值定义，其背景选择由上述经验训练分布给出。也就是说，对于特征的任何子集 $S$，联盟值为 $v(S) = \\mathbb{E}\\!\\left[f(X_1, X_2)\\,\\middle|\\, X_S = x_S\\right]$，其中期望是基于训练分布计算的。\n\n考虑一个特定患者，其每个测得的特征值都比队列均值高出一个标准差，即 $x_1 = \\mu_1 + \\sigma_1$ 和 $x_2 = \\mu_2 + \\sigma_2$。\n\n仅使用 Shapley 值作为所有联盟上边际贡献的平均值的定义，以及给定的背景统计性质，推导出该患者的 SHAP 归因值 $\\phi_1$ 和 $\\phi_2$。然后计算差值 $\\Delta = \\phi_1 - \\phi_2$。将最终答案表示为一个精确数（无需四舍五入）。答案无单位。", "solution": "该问题要求我们为线性模型中的两个特征计算 SHAP 值，并求出它们的差值。我们将使用 SHAP 值的基本定义，即在所有特征联盟上的平均边际贡献。\n\n**第一步：确定问题参数**\n- 模型函数：$f(x_1, x_2) = 2(x_1 + x_2)$。\n- 特征背景分布：$X_1$ 和 $X_2$ 相互独立，均值为 $\\mathbb{E}[X_1] = 0$ 和 $\\mathbb{E}[X_2] = 0$。\n- 特定实例（患者）的特征值：$x_1 = \\mu_1 + \\sigma_1 = 0 + 3 = 3$，$x_2 = \\mu_2 + \\sigma_2 = 0 + 1 = 1$。\n\n**第二步：计算所有联盟的值函数 $v(S)$**\n值函数 $v(S)$ 是在给定联盟 $S$ 中特征值的条件下，模型输出的期望值。\n1.  空联盟 $S = \\emptyset$（基线值）：\n    $v(\\emptyset) = \\mathbb{E}[f(X_1, X_2)] = \\mathbb{E}[2(X_1 + X_2)] = 2(\\mathbb{E}[X_1] + \\mathbb{E}[X_2]) = 2(0+0) = 0$。\n\n2.  单特征联盟 $S = \\{X_1\\}$：\n    由于特征独立，$\\mathbb{E}[X_2|X_1=x_1] = \\mathbb{E}[X_2]=0$。\n    $v(\\{X_1\\}) = \\mathbb{E}[f(X_1, X_2) | X_1=x_1] = \\mathbb{E}[2(x_1 + X_2)] = 2(3 + \\mathbb{E}[X_2]) = 2(3+0) = 6$。\n\n3.  单特征联盟 $S = \\{X_2\\}$：\n    $v(\\{X_2\\}) = \\mathbb{E}[f(X_1, X_2) | X_2=x_2] = \\mathbb{E}[2(X_1 + x_2)] = 2(\\mathbb{E}[X_1] + 1) = 2(0+1) = 2$。\n\n4.  全联盟 $S = \\{X_1, X_2\\}$（预测值）：\n    $v(\\{X_1, X_2\\}) = f(x_1, x_2) = 2(3 + 1) = 8$。\n\n**第三步：计算 SHAP 值 $\\phi_1$ 和 $\\phi_2$**\n对于两个特征的情况，SHAP 值的计算公式简化为对两种联盟顺序的贡献的平均：\n$\\phi_1 = \\frac{1}{2} [v(\\{X_1\\}) - v(\\emptyset)] + \\frac{1}{2} [v(\\{X_1, X_2\\}) - v(\\{X_2\\})]$\n$\\phi_1 = \\frac{1}{2} [6 - 0] + \\frac{1}{2} [8 - 2] = \\frac{6}{2} + \\frac{6}{2} = 3 + 3 = 6$\n\n$\\phi_2 = \\frac{1}{2} [v(\\{X_2\\}) - v(\\emptyset)] + \\frac{1}{2} [v(\\{X_1, X_2\\}) - v(\\{X_1\\})]$\n$\\phi_2 = \\frac{1}{2} [2 - 0] + \\frac{1}{2} [8 - 6] = \\frac{2}{2} + \\frac{2}{2} = 1 + 1 = 2$\n\n作为验证，对于具有独立特征的线性模型 $f(\\mathbf{x}) = \\sum w_j x_j + b$，SHAP 值为 $\\phi_j = w_j(x_j - \\mathbb{E}[X_j])$。\n在我们的模型 $f(x_1, x_2) = 2x_1 + 2x_2$ 中，$w_1=2, w_2=2$。\n$\\phi_1 = 2(x_1 - \\mathbb{E}[X_1]) = 2(3 - 0) = 6$。\n$\\phi_2 = 2(x_2 - \\mathbb{E}[X_2]) = 2(1 - 0) = 2$。\n结果一致。\n\n**第四步：计算差值 $\\Delta$**\n$\\Delta = \\phi_1 - \\phi_2 = 6 - 2 = 4$。", "answer": "$$\\boxed{4}$$", "id": "4551432"}]}