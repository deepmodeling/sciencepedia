{"hands_on_practices": [{"introduction": "可解释性人工智能的一个核心途径是构建本质上更简单的模型。本练习探讨了如何利用 $\\ell_1$ 正则化（LASSO）构建稀疏模型，通过将不重要特征的系数压缩至零来实现自动特征选择。通过推导其优化目标并计算特征选择的临界点，您将深入理解模型构建与可解释性之间的直接联系。[@problem_id:4538105]", "problem": "在一项放射组学研究中，你构建了一个二元分类器，用于从两个手工制作的影像学特征——灰度共生矩阵（GLCM）对比度和球形度——来预测恶性肿瘤（$y_i \\in \\{0,1\\}$）。为了在可解释人工智能（XAI）中提高可解释性，你决定使用带有 $\\ell_{1}$ 惩罚（最小绝对收缩与选择算子（LASSO））的逻辑回归，该惩罚作用于特征系数，而截距项不受惩罚。\n\n从适用于此背景的第一性原理出发，使用以下基础：\n- 标签的条件分布是伯努利分布，其参数为 $\\Pr(y_i=1 \\mid \\boldsymbol{x}_i) = \\sigma(z_i)$，其中 $\\sigma(z) = \\frac{1}{1+\\exp(-z)}$ 是逻辑函数，且 $z_i = \\beta_0 + \\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta}$。\n- 负对数似然是所有样本的伯努利概率质量函数的负对数之和。\n\n任务：\n1) 推导 $\\ell_{1}$ 正则化逻辑回归的凸优化目标函数，该函数最小化平均负对数似然加上一个强度为 $\\lambda$ 且仅作用于 $\\boldsymbol{\\beta}$（截距项 $\\beta_0$ 不受惩罚）的 $\\ell_{1}$ 惩罚项。将目标函数显式地写成 $\\beta_0$ 和 $\\boldsymbol{\\beta}$ 的函数。\n2) 使用带次梯度的一阶最优性（Karush–Kuhn–Tucker 条件），将这些条件应用于 $\\boldsymbol{\\beta}=\\boldsymbol{0}$ 的情况，其中 $\\beta_0$ 设置为 $\\boldsymbol{\\beta}=\\boldsymbol{0}$ 时的最大似然截距项。推导出一个使得 $\\boldsymbol{\\beta}=\\boldsymbol{0}$ 满足最优性条件的最小 $\\lambda$ 值（记为 $\\lambda_{\\max}$）的显式公式。你的表达式必须用 $\\{y_i\\}_{i=1}^{n}$、$\\{\\boldsymbol{x}_i\\}_{i=1}^{n}$ 和 $n$ 来表示。\n3) 为下面 $n=8$ 个患者的小型队列计算 $\\lambda_{\\max}$。这些特征已在整个队列中进行了中心化处理，使其均值为零。将 $\\lambda_{\\max}$ 报告为一个无量纲值。将你的答案四舍五入到四位有效数字。\n\n- 标签 $y$：$[\\,1,\\ 0,\\ 1,\\ 0,\\ 0,\\ 1,\\ 0,\\ 1\\,]$。\n- GLCM 对比度的特征向量 ($x_{i1}$): $[\\,1.2,\\ -0.8,\\ 0.5,\\ -1.0,\\ -0.4,\\ 0.9,\\ -0.6,\\ 0.2\\,]$。\n- 球形度的特征向量 ($x_{i2}$): $[\\,-0.5,\\ 1.1,\\ -1.3,\\ 0.7,\\ 0.4,\\ -0.9,\\ 1.5,\\ -1.0\\,]$。\n\n最后，用一到两句话解释为什么稀疏性可以带来放射组学中更简单的解释，以及为什么特征之间的高度共线性会使这些解释不稳定。\n\n按照上述说明，表达 $\\lambda_{\\max}$ 的最终数值答案。", "solution": "## 问题验证\n\n### 步骤 1：提取已知条件\n- **模型：** 用于预测恶性肿瘤的二元分类器 $y_i \\in \\{0, 1\\}$。\n- **特征：** 两个手工制作的影像学特征：GLCM 对比度 ($x_{i1}$) 和球形度 ($x_{i2}$)。第 $i$ 个患者的特征向量是 $\\boldsymbol{x}_i = [x_{i1}, x_{i2}]^{\\top}$。\n- **分类器形式：** 逻辑回归。阳性标签的条件概率是 $\\Pr(y_i=1 \\mid \\boldsymbol{x}_i) = \\sigma(z_i)$，其中 $\\sigma(z) = \\frac{1}{1+\\exp(-z)}$ 是逻辑 sigmoid 函数。\n- **线性模型：** $z_i = \\beta_0 + \\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta}$，其中 $\\beta_0$ 是截距项，$\\boldsymbol{\\beta} = [\\beta_1, \\beta_2]^{\\top}$ 是特征系数的向量。\n- **目标函数：** 最小化平均负对数似然加上一个 $\\ell_{1}$ 惩罚项。\n- **正则化：** $\\ell_{1}$ 惩罚仅应用于特征系数 $\\boldsymbol{\\beta}$，而不应用于截距项 $\\beta_0$。惩罚强度为 $\\lambda$。\n- **数据：** 一个包含 $n=8$ 个患者的队列。\n- **标签：** $\\boldsymbol{y} = [1, 0, 1, 0, 0, 1, 0, 1]$。\n- **特征 1 (GLCM 对比度)：** $\\boldsymbol{x_1} = [1.2, -0.8, 0.5, -1.0, -0.4, 0.9, -0.6, 0.2]$。\n- **特征 2 (球形度)：** $\\boldsymbol{x_2} = [-0.5, 1.1, -1.3, 0.7, 0.4, -0.9, 1.5, -1.0]$。\n- **数据预处理：** 特征已经过中心化处理，均值为零。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题根据既定标准进行评估：\n- **科学依据：** 该问题牢固地植根于统计机器学习。它涉及逻辑回归、负对数似然和 $\\ell_{1}$ (LASSO) 正则化，这些都是标准和基本的概念。放射组学和可解释人工智能 (XAI) 的背景是恰当且现实的。\n- **问题适定：** 任务是明确定义的数学推导和计算。推导目标函数，使用 KKT 条件找到 $\\lambda_{\\max}$ 的解析形式，并为给定数据集计算其值，这些都是能导向唯一且有意义解的过程。\n- **客观性：** 问题以精确、正式的语言陈述，没有模糊或主观的论断。\n- **完整性与一致性：** 提供了所有必要的信息，包括模型结构、惩罚形式、基础函数和一个完整的数据集。明确说明了特征是中心化的条件，并可以用数据验证，确保了一致性。\n- **可行性：** 数值对于中心化的特征是合理的。任务在计算上是可行的。\n\n### 步骤 3：结论与行动\n该问题具有科学合理性、问题适定且内部一致。判定为**有效**。现在开始解题过程。\n\n---\n\n## 解题过程\n\n### 1) 优化目标的推导\n\n问题要求推导 $\\ell_1$ 正则化逻辑回归的目标函数。我们从数据的似然函数开始。对于单个观测 $(y_i, \\boldsymbol{x}_i)$，标签 $y_i$ 的条件概率由伯努利概率质量函数给出：\n$$\n\\Pr(y_i \\mid \\boldsymbol{x}_i, \\beta_0, \\boldsymbol{\\beta}) = p_i^{y_i} (1 - p_i)^{1-y_i}\n$$\n其中 $p_i = \\Pr(y_i=1 \\mid \\boldsymbol{x}_i) = \\sigma(\\beta_0 + \\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta})$。\n\n单个观测的对数似然是：\n$$\n\\ell_i(\\beta_0, \\boldsymbol{\\beta}) = \\ln \\left( p_i^{y_i} (1 - p_i)^{1-y_i} \\right) = y_i \\ln(p_i) + (1-y_i) \\ln(1-p_i)\n$$\n我们使用逻辑函数 $\\sigma(z) = \\frac{1}{1+\\exp(-z)}$ 的性质：\n$$\n\\ln(p_i) = \\ln(\\sigma(z_i)) = \\ln\\left(\\frac{1}{1+\\exp(-z_i)}\\right) = -\\ln(1+\\exp(-z_i))\n$$\n$$\n\\ln(1-p_i) = \\ln(1-\\sigma(z_i)) = \\ln\\left(\\frac{\\exp(-z_i)}{1+\\exp(-z_i)}\\right) = -z_i - \\ln(1+\\exp(-z_i))\n$$\n将这些代入对数似然表达式：\n$$\n\\ell_i(\\beta_0, \\boldsymbol{\\beta}) = y_i(-\\ln(1+\\exp(-z_i))) + (1-y_i)(-z_i - \\ln(1+\\exp(-z_i)))\n$$\n$$\n\\ell_i(\\beta_0, \\boldsymbol{\\beta}) = -y_i\\ln(1+\\exp(-z_i)) - z_i + y_iz_i - \\ln(1+\\exp(-z_i)) + y_i\\ln(1+\\exp(-z_i))\n$$\n$$\n\\ell_i(\\beta_0, \\boldsymbol{\\beta}) = y_iz_i - z_i - \\ln(1+\\exp(-z_i))\n$$\n使用恒等式 $\\ln(1+\\exp(z_i)) = z_i + \\ln(1+\\exp(-z_i))$，我们可以更紧凑地写成：\n$$\n\\ell_i(\\beta_0, \\boldsymbol{\\beta}) = y_iz_i - \\ln(1+\\exp(z_i))\n$$\n$n$ 个样本的总对数似然是和 $L(\\beta_0, \\boldsymbol{\\beta}) = \\sum_{i=1}^n \\ell_i$。负对数似然 (NLL) 是：\n$$\n\\text{NLL}(\\beta_0, \\boldsymbol{\\beta}) = -\\sum_{i=1}^n \\left( y_iz_i - \\ln(1+\\exp(z_i)) \\right) = \\sum_{i=1}^n \\left( \\ln(1+\\exp(z_i)) - y_iz_i \\right)\n$$\n优化目标 $J(\\beta_0, \\boldsymbol{\\beta})$ 是平均 NLL 加上对 $\\boldsymbol{\\beta}$ 的 $\\ell_1$ 惩罚项。\n$$\nJ(\\beta_0, \\boldsymbol{\\beta}) = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\ln(1+\\exp(\\beta_0 + \\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta})) - y_i (\\beta_0 + \\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta}) \\right] + \\lambda \\|\\boldsymbol{\\beta}\\|_1\n$$\n其中 $\\|\\boldsymbol{\\beta}\\|_1 = \\sum_{j=1}^p |\\beta_j|$，在这个问题中，特征数量为 $p=2$。\n\n### 2) $\\lambda_{\\max}$ 的推导\n\n$\\lambda_{\\max}$ 是使得最优解为 $\\boldsymbol{\\beta} = \\boldsymbol{0}$ 的最小 $\\lambda \\ge 0$。我们使用 Karush–Kuhn–Tucker (KKT) 最优性条件。由于 $\\ell_1$ 范数，目标函数在 $\\beta_j=0$ 处是凸但不可微的。我们使用次梯度。\n目标函数是 $J(\\beta_0, \\boldsymbol{\\beta}) = f(\\beta_0, \\boldsymbol{\\beta}) + \\lambda g(\\boldsymbol{\\beta})$，其中 $f$ 是平均 NLL，$g(\\boldsymbol{\\beta}) = \\|\\boldsymbol{\\beta}\\|_1$。\n\n在解 $(\\hat{\\beta}_0, \\hat{\\boldsymbol{\\beta}})$ 处的优化条件是：\n1. 对于不受惩罚的截距项 $\\beta_0$：$\\frac{\\partial f}{\\partial \\beta_0}\\Big|_{(\\hat{\\beta}_0, \\hat{\\boldsymbol{\\beta}})} = 0$。\n2. 对于受惩罚的系数 $\\boldsymbol{\\beta}$：$\\boldsymbol{0} \\in \\nabla_{\\boldsymbol{\\beta}} f(\\hat{\\beta}_0, \\hat{\\boldsymbol{\\beta}}) + \\lambda \\partial g(\\hat{\\boldsymbol{\\beta}})$，其中 $\\partial g$ 是 $\\ell_1$ 范数的次梯度。\n\n让我们为解 $\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{0}$ 评估这些条件。\n首先，我们求 $f$ 的梯度：\n$$\n\\frac{\\partial f}{\\partial \\beta_0} = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\frac{\\exp(z_i)}{1+\\exp(z_i)} - y_i \\right] = \\frac{1}{n} \\sum_{i=1}^n (\\sigma(z_i) - y_i)\n$$\n$$\n\\frac{\\partial f}{\\partial \\beta_j} = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\frac{\\exp(z_i)x_{ij}}{1+\\exp(z_i)} - y_i x_{ij} \\right] = \\frac{1}{n} \\sum_{i=1}^n (\\sigma(z_i) - y_i)x_{ij}\n$$\n当 $\\boldsymbol{\\beta} = \\boldsymbol{0}$ 时，我们有 $z_i = \\beta_0$。第一个最优性条件决定了截距项 $\\hat{\\beta}_0$ 的值：\n$$\n\\frac{\\partial f}{\\partial \\beta_0}\\Big|_{(\\hat{\\beta}_0, \\boldsymbol{0})} = \\frac{1}{n} \\sum_{i=1}^n (\\sigma(\\hat{\\beta}_0) - y_i) = 0 \\implies n\\sigma(\\hat{\\beta}_0) = \\sum_{i=1}^n y_i\n$$\n这得到 $\\sigma(\\hat{\\beta}_0) = \\frac{1}{n}\\sum_{i=1}^n y_i = \\bar{y}$。这是当所有特征系数为零时，截距项的最大似然估计。\n\n现在，我们将这个结果用于第二个最优性条件。在 $(\\hat{\\beta}_0, \\boldsymbol{0})$ 处评估的关于 $\\boldsymbol{\\beta}$ 的梯度分量为：\n$$\n\\frac{\\partial f}{\\partial \\beta_j}\\Big|_{(\\hat{\\beta}_0, \\boldsymbol{0})} = \\frac{1}{n} \\sum_{i=1}^n (\\sigma(\\hat{\\beta}_0) - y_i)x_{ij} = \\frac{1}{n} \\sum_{i=1}^n (\\bar{y} - y_i)x_{ij}\n$$\n$\\ell_1$ 范数在 $\\boldsymbol{\\beta} = \\boldsymbol{0}$ 处的次梯度是集合 $[-1, 1]^p$。对于第 $j$ 个分量的第二个条件是：\n$$\n0 \\in \\frac{\\partial f}{\\partial \\beta_j}\\Big|_{(\\hat{\\beta}_0, \\boldsymbol{0})} + \\lambda [-1, 1] \\implies -\\frac{\\partial f}{\\partial \\beta_j}\\Big|_{(\\hat{\\beta}_0, \\boldsymbol{0})} \\in \\lambda [-1, 1]\n$$\n这等价于不等式：\n$$\n\\left| \\frac{\\partial f}{\\partial \\beta_j}\\Big|_{(\\hat{\\beta}_0, \\boldsymbol{0})} \\right| \\leq \\lambda \\implies \\left| \\frac{1}{n} \\sum_{i=1}^n (\\bar{y} - y_i)x_{ij} \\right| \\leq \\lambda\n$$\n为了使 $\\boldsymbol{\\beta} = \\boldsymbol{0}$ 是一个最优解，这个不等式必须对所有 $j=1, \\dots, p$ 成立。因此，$\\lambda$ 必须大于或等于这些绝对值的最大值。确保这一点的最小 $\\lambda$ 值是 $\\lambda_{\\max}$，此时不等式对至少一个 $j$ 取等：\n$$\n\\lambda_{\\max} = \\max_{j \\in \\{1,\\dots,p\\}} \\left| \\frac{1}{n} \\sum_{i=1}^n (y_i - \\bar{y})x_{ij} \\right|\n$$\n问题陈述中提到特征是中心化的，即对每个特征 $j$，$\\frac{1}{n}\\sum_{i=1}^n x_{ij} = 0$。这简化了表达式：\n$$\n\\frac{1}{n} \\sum_{i=1}^n (y_i - \\bar{y})x_{ij} = \\frac{1}{n} \\left( \\sum_{i=1}^n y_i x_{ij} - \\bar{y} \\sum_{i=1}^n x_{ij} \\right) = \\frac{1}{n} \\left( \\sum_{i=1}^n y_i x_{ij} - \\bar{y} \\cdot 0 \\right) = \\frac{1}{n} \\sum_{i=1}^n y_i x_{ij}\n$$\n因此，$\\lambda_{\\max}$ 的最终公式是：\n$$\n\\lambda_{\\max} = \\max_{j \\in \\{1,\\dots,p\\}} \\left| \\frac{1}{n} \\sum_{i=1}^n y_i x_{ij} \\right|\n$$\n\n### 3) $\\lambda_{\\max}$ 的计算\n\n我们将推导出的公式应用于给定的数据。\n- 样本量：$n=8$。\n- 标签：$y \\in [1, 0, 1, 0, 0, 1, 0, 1]$。\n- 特征 1 (GLCM)：$x_1 \\in [1.2, -0.8, 0.5, -1.0, -0.4, 0.9, -0.6, 0.2]$。\n- 特征 2 (球形度)：$x_2 \\in [-0.5, 1.1, -1.3, 0.7, 0.4, -0.9, 1.5, -1.0]$。\n\n$y_i=1$ 的索引 $i$ 为 $\\{1, 3, 6, 8\\}$。我们只需对这些索引处的特征值求和。\n\n对于特征 $j=1$ (GLCM)：\n$$\nC_1 = \\frac{1}{n} \\sum_{i=1}^n y_i x_{i1} = \\frac{1}{8} (x_{1,1} + x_{3,1} + x_{6,1} + x_{8,1})\n$$\n$$\nC_1 = \\frac{1}{8} (1.2 + 0.5 + 0.9 + 0.2) = \\frac{1}{8} (2.8) = 0.35\n$$\n\n对于特征 $j=2$ (球形度)：\n$$\nC_2 = \\frac{1}{n} \\sum_{i=1}^n y_i x_{i2} = \\frac{1}{8} (x_{1,2} + x_{3,2} + x_{6,2} + x_{8,2})\n$$\n$$\nC_2 = \\frac{1}{8} (-0.5 + (-1.3) + (-0.9) + (-1.0)) = \\frac{1}{8} (-3.7) = -0.4625\n$$\n\n最后，$\\lambda_{\\max}$ 是这些相关性绝对值的最大值：\n$$\n\\lambda_{\\max} = \\max(|C_1|, |C_2|) = \\max(|0.35|, |-0.4625|) = \\max(0.35, 0.4625)\n$$\n$$\n\\lambda_{\\max} = 0.4625\n$$\n该值按要求有四位有效数字。\n\n### 4) 概念解释\n\n$\\ell_1$ 正则化产生的稀疏性通过执行自动特征选择，将较不相关特征的系数强制变为零，从而在放射组学中产生更简单的解释。一个基于少数特征子集的解释比一个涉及数十个变量的复杂模型更具可解释性和临床可操作性。然而，如果特征高度共线性（例如，不同的纹理特征捕捉相似的信息），LASSO 可能会任意选择其中一个特征而忽略其他特征，使得选择不稳定；数据的微小扰动可能导致选择不同的特征，从而损害了解释的可靠性。", "answer": "$$\\boxed{0.4625}$$", "id": "4538105"}, {"introduction": "在模型做出预测后，我们如何以一种直观的方式解释其决策依据？本练习引入了“反事实解释”这一强大的概念，它通过回答“如果......会怎样？”的问题来提供解释。您将通过一个具体的优化问题，计算出为改变模型预测结果所需的最小且符合临床实际的特征调整，从而生成一个可操作的、清晰的解释。[@problem_id:4538076]", "problem": "一个用于肺结节的影像组学分类器使用一个带有符号输出的线性决策函数，将一个特征向量 $x \\in \\mathbb{R}^{4}$（Hounsfield单位的平均强度、灰度共生矩阵对比度、球形度和体素能量）映射到一个得分 $f(x) = w^{\\top}x + b$。负分表示预测为良性，正分表示预测为恶性。根据可解释人工智能（XAI）的原则，一个可操作的反事实解释旨在寻找对当前病例 $x$ 的最小改变 $\\delta$，该改变能够翻转 $f(x)$ 的符号，同时遵守特征变化的临床合理界限。假设以下是有效的基础事实：线性决策规则由一个分离超平面确定，$L_{2}$-距离到超平面的定义是明确的，并且带有线性约束的凸二次规划在可行时具有唯一的全局最小值。\n\n给定 $w = (0.8,\\,-0.5,\\,0.2,\\,0.1)^{\\top}$，$b = -50$，以及当前患者的特征 $x = (60,\\,1.5,\\,0.6,\\,2.0)^{\\top}$。对于一个可操作的反事实，只允许对特征进行有界调整以反映临床上合理的后处理：平均强度最多可调整 $\\pm 5$ Hounsfield单位，灰度共生矩阵对比度最多可调整 $\\pm 0.5$，球形度最多可调整 $\\pm 0.05$，而小波能量是不可变的。形式上，扰动 $\\delta \\in \\mathbb{R}^{4}$ 必须满足边界约束 $\\delta_{1} \\in [-5,\\,5]$，$\\delta_{2} \\in [-0.5,\\,0.5]$，$\\delta_{3} \\in [-0.05,\\,0.05]$，以及 $\\delta_{4} = 0$。\n\n从这些原则和定义出发，从第一性原理推导并解决优化问题，该问题旨在寻找最小的 $L_{2}$-范数扰动 $\\delta$，以翻转决策函数的符号，即在满足 $w^{\\top}(x+\\delta) + b \\ge 0$ 和给定边界的条件下，最小化 $\\|\\delta\\|_{2}$。因为当可行时，最小 $L_{2}$-范数的反事实出现在决策边界上，你可以在你的推导中设定 $w^{\\top}(x+\\delta) + b = 0$。\n\n最小 $L_{2}$-范数 $\\|\\delta\\|_{2}$ 的值是多少？将你的答案四舍五入到四位有效数字。", "solution": "该问题要求在 $\\delta$ 各分量满足特定约束的条件下，求出改变线性分类器预测结果的扰动向量 $\\delta$ 的最小 $L_2$-范数。这是一个寻找可操作反事实解释的经典问题，可以被表述为一个约束优化问题。\n\n首先，我们根据所提供的信息建立该问题的数学公式。\n决策函数由 $f(x) = w^{\\top}x + b$ 给出，其中 $x \\in \\mathbb{R}^{4}$ 是特征向量，$w \\in \\mathbb{R}^{4}$ 是权重向量，$b \\in \\mathbb{R}$ 是偏置。\n具体值为：\n$w = (0.8,\\, -0.5,\\, 0.2,\\, 0.1)^{\\top}$\n$b = -50$\n$x = (60,\\, 1.5,\\, 0.6,\\, 2.0)^{\\top}$\n\n患者的初始得分为：\n$$f(x) = w^{\\top}x + b = (0.8)(60) + (-0.5)(1.5) + (0.2)(0.6) + (0.1)(2.0) - 50$$\n$$f(x) = 48 - 0.75 + 0.12 + 0.2 - 50 = 47.57 - 50 = -2.43$$\n由于 $f(x)  0$，初始预测为“良性”。\n\n我们寻求一个能够翻转预测的扰动 $\\delta = (\\delta_1, \\delta_2, \\delta_3, \\delta_4)^{\\top}$。问题指出，最小 $L_2$-范数的反事实将位于决策边界上，因此目标条件是 $f(x+\\delta) = 0$。\n$$w^{\\top}(x+\\delta) + b = 0$$\n$$w^{\\top}x + b + w^{\\top}\\delta = 0$$\n代入 $f(x) = w^{\\top}x + b = -2.43$ 的值，我们得到：\n$$-2.43 + w^{\\top}\\delta = 0 \\implies w^{\\top}\\delta = 2.43$$\n扰动向量 $\\delta$ 受到边界约束：\n$\\delta_1 \\in [-5, 5]$\n$\\delta_2 \\in [-0.5, 0.5]$\n$\\delta_3 \\in [-0.05, 0.05]$\n$\\delta_4 = 0$\n\n将 $w$ 的分量和约束 $\\delta_4=0$ 代入等式约束中：\n$$0.8\\delta_1 - 0.5\\delta_2 + 0.2\\delta_3 + (0.1)(0) = 2.43$$\n$$0.8\\delta_1 - 0.5\\delta_2 + 0.2\\delta_3 = 2.43$$\n\n目标是最小化 $\\delta$ 的 $L_2$-范数，即 $\\|\\delta\\|_2 = \\sqrt{\\delta_1^2 + \\delta_2^2 + \\delta_3^2 + \\delta_4^2}$。最小化 $\\|\\delta\\|_2$ 等价于最小化其平方 $\\|\\delta\\|_2^2$。鉴于 $\\delta_4=0$，目标函数是最小化 $\\delta_1^2 + \\delta_2^2 + \\delta_3^2$。\n\n完整的优化问题是：\n$$ \\text{minimize} \\quad F(\\delta) = \\delta_1^2 + \\delta_2^2 + \\delta_3^2 $$\n$$ \\text{subject to:} $$\n$$ 0.8\\delta_1 - 0.5\\delta_2 + 0.2\\delta_3 = 2.43 $$\n$$ -5 \\le \\delta_1 \\le 5 $$\n$$ -0.5 \\le \\delta_2 \\le 0.5 $$\n$$ -0.05 \\le \\delta_3 \\le 0.05 $$\n这是一个凸二次规划（QP）问题，它有一个唯一的全局最小值。我们可以使用 Karush-Kuhn-Tucker（KKT）条件来解决它。\n\n首先，我们使用拉格朗日乘子法在不考虑箱式约束的情况下解决该问题，以查看无约束解是否可行。\n拉格朗日函数为 $\\mathcal{L}(\\delta_1, \\delta_2, \\delta_3, \\lambda) = \\delta_1^2 + \\delta_2^2 + \\delta_3^2 - \\lambda(0.8\\delta_1 - 0.5\\delta_2 + 0.2\\delta_3 - 2.43)$。\n将关于 $\\delta_i$ 的梯度设为零：\n$$ \\frac{\\partial\\mathcal{L}}{\\partial\\delta_1} = 2\\delta_1 - 0.8\\lambda = 0 \\implies \\delta_1 = 0.4\\lambda $$\n$$ \\frac{\\partial\\mathcal{L}}{\\partial\\delta_2} = 2\\delta_2 + 0.5\\lambda = 0 \\implies \\delta_2 = -0.25\\lambda $$\n$$ \\frac{\\partial\\mathcal{L}}{\\partial\\delta_3} = 2\\delta_3 - 0.2\\lambda = 0 \\implies \\delta_3 = 0.1\\lambda $$\n将这些代入等式约束中：\n$$ 0.8(0.4\\lambda) - 0.5(-0.25\\lambda) + 0.2(0.1\\lambda) = 2.43 $$\n$$ 0.32\\lambda + 0.125\\lambda + 0.02\\lambda = 2.43 $$\n$$ 0.465\\lambda = 2.43 \\implies \\lambda = \\frac{2.43}{0.465} \\approx 5.2258 $$\n这给出了无约束解：\n$$ \\delta_1 = 0.4 \\times 5.2258... \\approx 2.0903 $$\n$$ \\delta_2 = -0.25 \\times 5.2258... \\approx -1.3065 $$\n$$ \\delta_3 = 0.1 \\times 5.2258... \\approx 0.5226 $$\n检查箱式约束：\n- $\\delta_1 \\approx 2.0903 \\in [-5, 5]$ (满足)\n- $\\delta_2 \\approx -1.3065 \\notin [-0.5, 0.5]$ (违反)\n- $\\delta_3 \\approx 0.5226 \\notin [-0.05, 0.05]$ (违反)\n\n无约束解是不可行的。这表明最优解必须位于由箱式约束定义的边界上。违反情况表明 $\\delta_2$ 应处于其下界，$\\delta_3$ 应处于其上界。让我们假设最优解具有 $\\delta_2 = -0.5$ 和 $\\delta_3 = 0.05$。\n\n我们现在使用这些固定值通过等式约束求解 $\\delta_1$：\n$$ 0.8\\delta_1 - 0.5(-0.5) + 0.2(0.05) = 2.43 $$\n$$ 0.8\\delta_1 + 0.25 + 0.01 = 2.43 $$\n$$ 0.8\\delta_1 = 2.43 - 0.26 = 2.17 $$\n$$ \\delta_1 = \\frac{2.17}{0.8} = 2.7125 $$\n这个 $\\delta_1$ 的值在其界限 $[-5, 5]$ 内。\n因此，我们有一个候选最优解 $\\delta^* = (2.7125, -0.5, 0.05, 0)^{\\top}$。\n\n为了确认这是最优解，我们验证完整的KKT条件。此QP问题的平稳性条件是 $\\delta = \\lambda w - \\mu + \\nu$，其中 $\\mu$ 和 $\\nu$ 分别是上下界约束的拉格朗日乘子向量。对于 $\\delta_1 = 2.7125$，它不在边界上，所以它的乘子为零。因此，$\\delta_1 = \\lambda w_1$。$2.7125 = \\lambda(0.8) \\implies \\lambda = \\frac{2.7125}{0.8} = 3.390625$。等等... 这不是正确的验证方法。拉格朗日函数的不同形式可能会导致 $\\lambda$ 的符号不同。下面是严谨的方法。\n\n带有活动约束乘子的拉格朗日函数是：\n$\\mathcal{L} = (\\delta_1^2 + \\delta_2^2 + \\delta_3^2) + \\lambda_0(0.8\\delta_1 - 0.5\\delta_2 + 0.2\\delta_3 - 2.43) + \\lambda_1(-\\delta_2 - 0.5) + \\lambda_2(\\delta_3 - 0.05)$\n其中 $\\lambda_1, \\lambda_2 \\ge 0$。\n平稳性条件是 $\\nabla L = 0$：\n$$ \\frac{\\partial\\mathcal{L}}{\\partial\\delta_1}: 2\\delta_1 + 0.8\\lambda_0 = 0 $$\n$$ \\frac{\\partial\\mathcal{L}}{\\partial\\delta_2}: 2\\delta_2 - 0.5\\lambda_0 - \\lambda_1 = 0 $$\n$$ \\frac{\\partial\\mathcal{L}}{\\partial\\delta_3}: 2\\delta_3 + 0.2\\lambda_0 + \\lambda_2 = 0 $$\n将 $\\delta_1 = 2.7125$ 代入第一个方程：\n$$ 2(2.7125) + 0.8\\lambda_0 = 0 \\implies 5.425 = -0.8\\lambda_0 \\implies \\lambda_0 = -\\frac{5.425}{0.8} = -6.78125 $$\n将 $\\delta_2 = -0.5$ 和 $\\lambda_0 = -6.78125$ 代入第二个方程以求得 $\\lambda_1$：\n$$ 2(-0.5) - 0.5(-6.78125) - \\lambda_1 = 0 $$\n$$ -1 + 3.390625 - \\lambda_1 = 0 \\implies \\lambda_1 = 2.390625 $$\n由于 $\\lambda_1 \\ge 0$，此条件满足。\n将 $\\delta_3 = 0.05$ 和 $\\lambda_0 = -6.78125$ 代入第三个方程以求得 $\\lambda_2$：\n$$ 2(0.05) + 0.2(-6.78125) + \\lambda_2 = 0 $$\n$$ 0.1 - 1.35625 + \\lambda_2 = 0 \\implies \\lambda_2 = 1.25625 $$\n由于 $\\lambda_2 \\ge 0$，此条件也满足。\n\n所有 KKT 条件（原始可行性、对偶可行性、互补松弛性和平稳性）均已满足。因此，解 $\\delta^* = (2.7125, -0.5, 0.05, 0)^{\\top}$ 确实是唯一的全局最小值。\n\n最后，我们计算最小 $L_2$-范数 $\\|\\delta\\|_{2}$：\n$$ \\|\\delta\\|_{2} = \\sqrt{\\delta_1^2 + \\delta_2^2 + \\delta_3^2 + \\delta_4^2} $$\n$$ \\|\\delta\\|_{2} = \\sqrt{(2.7125)^2 + (-0.5)^2 + (0.05)^2 + 0^2} $$\n$$ \\|\\delta\\|_{2} = \\sqrt{7.35765625 + 0.25 + 0.0025} $$\n$$ \\|\\delta\\|_{2} = \\sqrt{7.61015625} $$\n$$ \\|\\delta\\|_{2} \\approx 2.758651138 $$\n四舍五入到四位有效数字，我们得到：\n$$ \\|\\delta\\|_{2} \\approx 2.759 $$", "answer": "$$ \\boxed{2.759} $$", "id": "4538076"}, {"introduction": "对于复杂的深度学习模型，显著性图（Saliency Maps）是实现可视化解释的常用工具。本练习将深入剖析一种广泛应用的显著性图方法——Grad-CAM，并揭示其一个关键的内在限制：由于使用了 ReLU 激活函数，它可能隐藏反对模型预测的“保护性”证据。这项实践旨在培养对可解释性工具的批判性思维，强调了理解其内部机制的重要性，而不仅仅是盲目应用。[@problem_id:4538134]", "problem": "一个深度放射组学分类器被训练用于通过卷积神经网络从计算机断层扫描 (CT) 图像块中预测恶性肿瘤，该网络的最后一个卷积层在包含 $P$ 个位置的空间网格上输出特征图 $\\{A_k\\}_{k=1}^{K}$。下游分类器在全局平均池化后的特征上是线性的：对于类别 $c$（其 logit 为 $y^c$），模型为 $y^c = \\sum_{k=1}^{K} w_k \\, \\overline{A_k} + b$，其中 $\\overline{A_k} = \\frac{1}{P}\\sum_{i=1}^{P} A_k(i)$ 是 $A_k$ 的空间平均值。根据定义，梯度 $\\frac{\\partial y^c}{\\partial A_k(i)}$ 表示类别分数对 $A_k(i)$ 的微小变化的局部敏感性；负梯度意味着增加 $A_k(i)$ 会导致 $y^c$ 减少。梯度加权类激活图 (Grad-CAM) 计算通道权重 $\\alpha_k^c = \\frac{1}{P} \\sum_{i=1}^{P} \\frac{\\partial y^c}{\\partial A_k(i)}$ 和一个类激活图 $L^c = \\mathrm{ReLU}\\!\\left(\\sum_{k=1}^{K} \\alpha_k^c A_k \\right)$，其中 $\\mathrm{ReLU}(x) = \\max(x,0)$。\n\n考虑一个单一的测试 CT 图像块，其中两个临床可解释的通道在最后一个卷积层中占主导地位：$A_1$ 突出显示一个毛刺状病灶核心（通常与恶性相关），而 $A_2$ 突出显示一个病灶周围脂肪环（通常与良性相关，即具有抗恶性保护作用）。假设 $K = 2$，$P = 10$，$w_1 = +2$，$w_2 = -1$，$b$ 是任意的。假设 $A_1(i) = 1$ 在对应于病灶核心的恰好 $5$ 个位置上，而在其他地方 $A_1(i) = 0$；$A_2(i) = 1$ 在对应于脂肪环的互补 $5$ 个位置上，而在其他地方 $A_2(i) = 0$。所有其他通道均不存在。仅使用上述定义以及梯度和整流线性单元 (ReLU) 的标准属性，推断 Grad-CAM 在此设置中如何处理正面证据与负面证据。\n\n下列哪个陈述是正确的？\n\nA. 在此设置中，$\\frac{\\partial y^c}{\\partial A_2(i)}$ 是负的，并在脂肪环上为 $\\sum_{k} \\alpha_k^c A_k$ 贡献一个负项；应用 $\\mathrm{ReLU}$ 会将这些负值设为 $0$，从而抑制了一个临床保护性区域。一个不使用 $\\mathrm{ReLU}$ 的带符号可视化（例如，使用发散色图）或分别显示正面和负面贡献的图，将能同时揭示风险和保护作用。\n\nB. 去除 $\\mathrm{ReLU}$ 只能改变热图的整体尺度，但不能改变哪些像素被高亮显示；因此，在此示例中，$\\mathrm{ReLU}$ 对可解释性的影响可以忽略不计。\n\nC. $\\frac{\\partial y^c}{\\partial A_k(i)}$ 中的负梯度应被视为虚假的，因为经过上游激活后 $A_k(i) \\ge 0$；因此，用 $\\mathrm{ReLU}$ 对它们进行裁剪对于避免误导性图是必要的。\n\nD. 一个对称的 $\\mathrm{ReLU}$ 替代方案是将图分解为 $L^c_{+} = \\sum_{k} \\max(\\alpha_k^c, 0) \\, A_k$ 和 $L^c_{-} = \\sum_{k} \\max(-\\alpha_k^c, 0) \\, A_k$，使临床医生能够分别查看支持 ($L^c_{+}$) 和反对 ($L^c_{-}$) 该类别的证据。\n\nE. 用绝对值图 $| \\sum_{k} \\alpha_k^c A_k |$ 替换 $\\mathrm{ReLU}$，可以在保留临床保护性信息的同时，仍然生成一个非负热图，使其在可解释性上等同于带符号的可视化。", "solution": "### 问题验证\n\n**第 1 步：提取已知条件**\n- 一个使用卷积神经网络 (CNN) 的深度放射组学分类器。\n- 最后一个卷积层在包含 $P$ 个位置的空间网格上生成特征图 $\\{A_k\\}_{k=1}^{K}$。\n- 类别 $c$ 的下游分类器 logit 由 $y^c = \\sum_{k=1}^{K} w_k \\, \\overline{A_k} + b$ 给出。\n- 全局平均池化 (GAP) 定义为 $\\overline{A_k} = \\frac{1}{P}\\sum_{i=1}^{P} A_k(i)$。\n- 梯度加权类激活图 (Grad-CAM) 定义：\n    - 通道权重：$\\alpha_k^c = \\frac{1}{P} \\sum_{i=1}^{P} \\frac{\\partial y^c}{\\partial A_k(i)}$。\n    - 类激活图：$L^c = \\mathrm{ReLU}\\!\\left(\\sum_{k=1}^{K} \\alpha_k^c A_k \\right)$。\n    - 整流线性单元 (ReLU)：$\\mathrm{ReLU}(x) = \\max(x,0)$。\n- 问题实例的具体参数：\n    - 通道数，$K = 2$。\n    - 空间位置数，$P = 10$。\n    - 分类器权重，$w_1 = +2$ 和 $w_2 = -1$。\n    - 偏置项 $b$ 是任意的。\n- 特征图规格：\n    - $A_1$（病灶核心，与恶性相关）：$A_1(i) = 1$ 在恰好 $5$ 个位置上，$A_1(i) = 0$ 在另外 $5$ 个位置上。\n    - $A_2$（病灶周围脂肪环，与良性相关）：$A_2(i) = 1$ 在 $A_1(i)=0$ 的 $5$ 个位置上，$A_2(i) = 0$ 在 $A_1(i)=1$ 的位置上。\n    - 所有其他 $k  2$ 的通道 $A_k$ 均不存在（即，$A_k = 0$）。\n\n**第 2 步：使用提取的已知条件进行验证**\n问题陈述具有科学依据、提法明确且客观。它描述了一个标准的、尽管简化的架构（带有 GAP 层的 CNN）和一种经典的可解释人工智能方法 (Grad-CAM)。所有变量和函数都已明确定义，提供的数值和条件是自洽的，足以推导出唯一解。临床场景是一个现实的简化，用于探究 XAI 方法的概念性行为。该问题没有违反任何基本原则，不含糊，并提出了一个不平凡的概念性挑战。\n\n**第 3 步：结论与行动**\n问题陈述**有效**。我们可以继续进行解答。\n\n###\n### Grad-CAM 图的推导\n\n首先，我们必须根据提供的定义计算 Grad-CAM 图 $L^c$ 的必要组成部分。感兴趣的类别 $c$ 对应于恶性。\n\n**1. 计算梯度**\n类别 $c$ 的 logit 由 $y^c = \\sum_{k=1}^{K} w_k \\, \\overline{A_k} + b$ 给出。当 $K=2$ 时，为：\n$$ y^c = w_1 \\overline{A_1} + w_2 \\overline{A_2} + b $$\n代入 GAP 的定义 $\\overline{A_k} = \\frac{1}{P}\\sum_{j=1}^{P} A_k(j)$：\n$$ y^c = w_1 \\left(\\frac{1}{P}\\sum_{j=1}^{P} A_1(j)\\right) + w_2 \\left(\\frac{1}{P}\\sum_{j=1}^{P} A_2(j)\\right) + b $$\n我们对 $y^c$ 关于每个通道 $k$ 在单个空间位置 $i$ 的激活值 $A_k(i)$ 求偏导数：\n对于通道 $k=1$：\n$$ \\frac{\\partial y^c}{\\partial A_1(i)} = \\frac{\\partial}{\\partial A_1(i)} \\left( \\frac{w_1}{P}\\sum_{j=1}^{P} A_1(j) \\right) = \\frac{w_1}{P} $$\n对于通道 $k=2$：\n$$ \\frac{\\partial y^c}{\\partial A_2(i)} = \\frac{\\partial}{\\partial A_2(i)} \\left( \\frac{w_2}{P}\\sum_{j=1}^{P} A_2(j) \\right) = \\frac{w_2}{P} $$\n注意，对于这个特定的架构（在 GAP 之上的线性分类器），梯度 $\\frac{\\partial y^c}{\\partial A_k(i)}$ 在所有空间位置 $i$ 上都是恒定的。使用给定的值 $w_1 = 2$，$w_2 = -1$ 和 $P = 10$：\n$$ \\frac{\\partial y^c}{\\partial A_1(i)} = \\frac{2}{10} = 0.2 $$\n$$ \\frac{\\partial y^c}{\\partial A_2(i)} = \\frac{-1}{10} = -0.1 $$\n通道 1 的梯度为正，表明其对恶性分数的贡献，而通道 2 的梯度为负，表明其是反对恶性的证据。\n\n**2. 计算 Grad-CAM 通道权重**\n通道权重 $\\alpha_k^c$ 定义为梯度在空间位置上的平均值：\n$$ \\alpha_k^c = \\frac{1}{P} \\sum_{i=1}^{P} \\frac{\\partial y^c}{\\partial A_k(i)} $$\n由于每个通道的梯度是恒定的：\n$$ \\alpha_1^c = \\frac{1}{10} \\sum_{i=1}^{10} (0.2) = \\frac{1}{10} (10 \\times 0.2) = 0.2 $$\n$$ \\alpha_2^c = \\frac{1}{10} \\sum_{i=1}^{10} (-0.1) = \\frac{1}{10} (10 \\times -0.1) = -0.1 $$\n\n**3. 计算特征图的线性组合**\nReLU 前的激活图是加权和 $\\sum_{k=1}^{K} \\alpha_k^c A_k$。当 $K=2$ 时：\n$$ S^c = \\alpha_1^c A_1 + \\alpha_2^c A_2 = (0.2)A_1 + (-0.1)A_2 $$\n让我们在不同的空间位置评估这个图 $S^c$。设病灶核心的 $5$ 个位置集合为 $R_{core}$，脂肪环的另外 $5$ 个位置集合为 $R_{rim}$。\n- 对于任何位置 $i \\in R_{core}$：$A_1(i)=1$ 且 $A_2(i)=0$。\n  $$ S^c(i) = (0.2)(1) + (-0.1)(0) = 0.2 $$\n- 对于任何位置 $i \\in R_{rim}$：$A_1(i)=0$ 且 $A_2(i)=1$。\n  $$ S^c(i) = (0.2)(0) + (-0.1)(1) = -0.1 $$\n\n**4. 应用 ReLU 获得最终的 Grad-CAM 图**\n最终的 Grad-CAM 图是 $L^c = \\mathrm{ReLU}(S^c)$。\n- 对于任何位置 $i \\in R_{core}$：\n  $$ L^c(i) = \\mathrm{ReLU}(0.2) = \\max(0.2, 0) = 0.2 $$\n- 对于任何位置 $i \\in R_{rim}$：\n  $$ L^c(i) = \\mathrm{ReLU}(-0.1) = \\max(-0.1, 0) = 0 $$\n最终的 Grad-CAM 图突出了病灶核心（恶性的正面证据），但为病灶周围的脂肪环分配了零值，完全抑制了这个提供负面证据（即保护性）区域的可视化。\n\n### 逐项分析\n\n**A. 在此设置中，$\\frac{\\partial y^c}{\\partial A_2(i)}$ 是负的，并在脂肪环上为 $\\sum_{k} \\alpha_k^c A_k$ 贡献一个负项；应用 $\\mathrm{ReLU}$ 会将这些负值设为 $0$，从而抑制了一个临床保护性区域。一个不使用 $\\mathrm{ReLU}$ 的带符号可视化（例如，使用发散色图）或分别显示正面和负面贡献的图，将能同时揭示风险和保护作用。**\n这个陈述是我们推导的精确总结。通道 2 的梯度是 $\\frac{\\partial y^c}{\\partial A_2(i)} = -0.1$，是负的。在脂肪环上，这导致 ReLU 前的图值为 $-0.1$。$\\mathrm{ReLU}$ 函数将其映射为 $0$，从而抑制了该区域在最终热图中的显示。脂肪环被描述为具有临床保护性，因此抑制它会丢失重要的解释性信息。一个带符号的图（在核心上值为 $0.2$，在环上值为 $-0.1$）确实能够将贡献风险和起保护作用的区域都可视化出来。\n**结论：** 正确。\n\n**B. 去除 $\\mathrm{ReLU}$ 只能改变热图的整体尺度，但不能改变哪些像素被高亮显示；因此，在此示例中，$\\mathrm{ReLU}$ 对可解释性的影响可以忽略不计。**\n这是不正确的。没有 $\\mathrm{ReLU}$，该图在所有 $10$ 个位置上都有非零值（核心上为 $0.2$，环上为 $-0.1$）。有 $\\mathrm{ReLU}$，该图仅在核心的 $5$ 个位置上有非零值。被高亮显示的像素集合发生了根本性改变。对可解释性的影响并非可以忽略不计；它是至关重要的，因为它决定了反对恶性的证据是否被可视化。\n**结论：** 不正确。\n\n**C. $\\frac{\\partial y^c}{\\partial A_k(i)}$ 中的负梯度应被视为虚假的，因为经过上游激活后 $A_k(i) \\ge 0$；因此，用 $\\mathrm{ReLU}$ 对它们进行裁剪对于避免误导性图是必要的。**\n这个陈述提出了一个有缺陷的论点。负梯度 $\\frac{\\partial y^c}{\\partial A_k(i)}$ 意味着特征 $A_k(i)$ 的增加会导致类别分数 $y^c$ 的减少。这就是反面证据的定义。在我们的例子中，特征 $A_2$（脂肪环）具有负权重 $w_2=-1$，代表其保护性质。因此，产生的负梯度并非虚假，而是模型逻辑的核心部分。用 $\\mathrm{ReLU}$ 裁剪此信息并不能避免误导性的图；它通过呈现模型推理的不完整画面来创造了一个误导性的图。\n**结论：** 不正确。\n\n**D. 一个对称的 $\\mathrm{ReLU}$ 替代方案是将图分解为 $L^c_{+} = \\sum_{k} \\max(\\alpha_k^c, 0) \\, A_k$ 和 $L^c_{-} = \\sum_{k} \\max(-\\alpha_k^c, 0) \\, A_k$，使临床医生能够分别查看支持 ($L^c_{+}$) 和反对 ($L^c_{-}$) 该类别的证据。**\n让我们分析这个提议的分解。\n我们有 $\\alpha_1^c = 0.2$ 和 $\\alpha_2^c = -0.1$。\n对于正面证据图 $L^c_{+}$：\n$$ L^c_{+} = \\max(0.2, 0)A_1 + \\max(-0.1, 0)A_2 = 0.2 A_1 + 0 \\cdot A_2 = 0.2 A_1 $$\n该图突出了病灶核心，这是支持恶性的证据。\n对于负面证据图 $L^c_{-}$：\n$$ L^c_{-} = \\max(-0.2, 0)A_1 + \\max(-(-0.1), 0)A_2 = 0 \\cdot A_1 + \\max(0.1, 0)A_2 = 0.1 A_2 $$\n该图突出了脂肪环，这是反对恶性的证据。这种分解方法成功地将两种类型的证据分离到不同的、可解释的图中。\n**结论：** 正确。\n\n**E. 用绝对值图 $| \\sum_{k} \\alpha_k^c A_k |$ 替换 $\\mathrm{ReLU}$，可以在保留临床保护性信息的同时，仍然生成一个非负热图，使其在可解释性上等同于带符号的可视化。**\n提议的图是 $|S^c| = |(0.2)A_1 - (0.1)A_2|$。\n- 在核心 ($R_{core}$) 上，值为 $|0.2| = 0.2$。\n- 在环 ($R_{rim}$) 上，值为 $|-0.1| = 0.1$。\n这张图确实突出了两个区域。然而，它在“可解释性上不等同于带符号的可视化”。带符号的可视化会使用不同的颜色或标记来区分正面贡献和负面贡献。绝对值图用正值突出显示了两个区域，将支持恶性的证据与反对恶性的证据混淆在一起。用户仅从图中无法区分高亮区域的含义。\n**结论：** 不正确。", "answer": "$$\\boxed{AD}$$", "id": "4538134"}]}