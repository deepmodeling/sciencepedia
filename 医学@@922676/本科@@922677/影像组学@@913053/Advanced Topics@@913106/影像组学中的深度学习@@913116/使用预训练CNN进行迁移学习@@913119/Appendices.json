{"hands_on_practices": [{"introduction": "理论知识需要通过实践来巩固。本节的第一个练习将带你深入了解迁移学习的数学核心。在微调预训练模型时，我们不仅仅是在新数据上最小化误差；我们还需要防止模型“忘记”从原始任务中学到的宝贵知识。这个练习要求你构建一个结合了经验风险最小化和正则化惩罚的目标函数 [@problem_id:4568475]，让你亲手感受在“适应新任务”和“保留旧知识”之间取得平衡的优化过程。", "problem": "在一个用于对计算机断层扫描中提取的病灶斑块进行分类的放射组学工作流中，您的目标是将一个预训练的卷积神经网络（CNN）适应于一个二元目标任务，其中每个样本的标签为 $y_i \\in \\{0,1\\}$。设参数为 $\\theta$ 的CNN输出为 $f_{\\theta}(x_i) \\in (0,1)$，这被解释为输入 $x_i$ 属于正类的概率。您拥有从未知数据分布中抽取的 $N$ 个独立同分布的样本 $\\{(x_i, y_i)\\}_{i=1}^{N}$。该适应过程应遵循经验风险最小化（ERM）原则，并包含一个惩罚项，该惩罚项编码了一种先验偏好，即调整后的参数 $\\theta$ 在欧几里得意义上应与预训练参数 $\\theta_0$ 保持接近。使用二元交叉熵损失 $\\ell(f_{\\theta}(x_i), y_i) = - y_i \\ln(f_{\\theta}(x_i)) - (1 - y_i) \\ln(1 - f_{\\theta}(x_i))$ 和一个形式为 $\\lambda \\|\\theta - \\theta_0\\|_2^2$ 的 $L_2$ 惩罚项，其中正则化强度 $\\lambda  0$。\n\n从ERM（即最小化每个样本损失的经验平均值）的定义出发，并加入一个Tikhonov类型的正则化项，推导出为将预训练的CNN适应于此二元目标任务而需要最小化的目标函数的解析表达式。您的最终答案必须是关于 $N$、$\\lambda$、$\\theta$、$\\theta_0$、$f_{\\theta}(x_i)$ 和 $y_i$ 的单个闭式表达式。最终答案中不得包含等号或单词“min”。", "solution": "该问题要求使用正则化经验风险最小化（ERM）原则，推导用于调整一个预训练卷积神经网络（CNN）的目标函数。我们首先根据所提供的信息，正式定义该目标函数的组成部分。\n\n一个正则化目标函数的一般形式，对于一个参数为 $\\theta$ 的模型我们记为 $J(\\theta)$，是两项之和：经验风险 $R_{\\text{emp}}(\\theta)$ 和一个正则化惩罚项 $\\Omega(\\theta)$。\n$$\nJ(\\theta) = R_{\\text{emp}}(\\theta) + \\Omega(\\theta)\n$$\n\n首先，我们定义经验风险 $R_{\\text{emp}}(\\theta)$。ERM原则指出，风险是通过训练数据集上每个样本损失的平均值来估计的。问题提供了 $N$ 个独立同分布的样本 $\\{(x_i, y_i)\\}_{i=1}^{N}$。每个样本的损失函数 $\\ell$ 被指定为单个样本 $(x_i, y_i)$ 的二元交叉熵损失：\n$$\n\\ell(f_{\\theta}(x_i), y_i) = - y_i \\ln(f_{\\theta}(x_i)) - (1 - y_i) \\ln(1 - f_{\\theta}(x_i))\n$$\n这里，$f_{\\theta}(x_i)$ 是模型的输出，代表样本 $x_i$ 属于正类的预测概率（即 $P(y_i=1|x_i)$），而 $y_i \\in \\{0, 1\\}$ 是真实标签。\n\n经验风险是这个损失函数在所有 $N$ 个样本上的平均值：\n$$\nR_{\\text{emp}}(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} \\ell(f_{\\theta}(x_i), y_i)\n$$\n代入二元交叉熵损失的表达式，我们得到：\n$$\nR_{\\text{emp}}(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} \\left[ - y_i \\ln(f_{\\theta}(x_i)) - (1 - y_i) \\ln(1 - f_{\\theta}(x_i)) \\right]\n$$\n这可以通提出负号来重写：\n$$\nR_{\\text{emp}}(\\theta) = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\ln(f_{\\theta}(x_i)) + (1 - y_i) \\ln(1 - f_{\\theta}(x_i)) \\right]\n$$\n这一项代表了在标签由模型输出参数化的伯努利分布中抽取的假设下，数据的负平均对数似然。最小化这一项等价于最大化平均对数似然。\n\n其次，我们定义正则化项 $\\Omega(\\theta)$。问题指定了一个Tikhonov类型的 $L_2$ 惩罚项，该惩罚项鼓励调整后的参数 $\\theta$ 与预训练参数 $\\theta_0$ 保持接近。该惩罚项由下式给出：\n$$\n\\Omega(\\theta) = \\lambda \\|\\theta - \\theta_0\\|_2^2\n$$\n这里，$\\|\\theta - \\theta_0\\|_2^2$ 是参数向量 $\\theta$ 和 $\\theta_0$ 之间差值的欧几里得范数平方（或 $L_2$ 范数平方）。标量 $\\lambda  0$ 是正则化强度，它控制着最小化经验风险和与初始参数 $\\theta_0$ 保持接近之间的权衡。更大的 $\\lambda$ 会对偏离 $\\theta_0$ 施加更强的惩罚。\n\n最后，我们将经验风险和正则化项结合起来，构建要最小化的总目标函数。这个函数是一个基于随机梯度下降的优化器将寻求相对于参数 $\\theta$ 最小化的量：\n$$\nJ(\\theta) = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\ln(f_{\\theta}(x_i)) + (1 - y_i) \\ln(1 - f_{\\theta}(x_i)) \\right] + \\lambda \\|\\theta - \\theta_0\\|_2^2\n$$\n这是按照问题陈述所要求的完整且最终的目标函数解析表达式。", "answer": "$$\n\\boxed{-\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\ln\\left(f_{\\theta}(x_i)\\right) + (1 - y_i) \\ln\\left(1 - f_{\\theta}(x_i)\\right) \\right] + \\lambda \\|\\theta - \\theta_0\\|_2^2}\n$$", "id": "4568475"}, {"introduction": "定义了优化目标后，我们需要准备高质量的训练数据，而数据增强是其中的关键一步。然而，将自然图像领域的增强技术直接应用于医学影像等专业领域可能会适得其反。本练习 [@problem_id:4568487] 挑战你对特定领域知识的理解，要求你辨别在放射组学任务中哪些数据增强方法能够保持临床标签的不变性，而哪些会破坏图像的物理意义或关键形态特征。", "problem": "一个放射组学流程采用迁移学习方法，使用一个在自然图像上预训练的残差网络 (Residual Network, ResNet) 卷积神经网络 (Convolutional Neural Network, CNN)，用于对计算机断层扫描 (Computed Tomography, CT) 图像中的肺结节进行恶性程度分类。每个输入都是一个以单个结节为中心的裁剪后感兴趣区域 (region of interest, ROI)，经过重采样以获得各向同性的体素间距，并使用固定的肺窗进行窗位窗宽调整。临床标签指示结节是恶性还是良性，它由内在的组织放射密度模式和形态学决定，而非由整体方向决定。目标是选择能够保持临床标签不变性的数据增强方法族，同时提供有助于微调预训练 CNN 的变异性。\n\n使用以下基本事实和定义：\n- 临床标签是关于成像病灶潜在物理特性的函数 $y(I)$。如果对于所考虑的病灶的所有图像 $I$，都有 $y(T(I))=y(I)$，那么变换 $T$ 就是标签保持的。\n- Hounsfield 单位 (HU) 定义为 $$\\mathrm{HU} = 1000 \\frac{\\mu - \\mu_{\\mathrm{water}}}{\\mu_{\\mathrm{water}}},$$ 其中 $\\mu$ 是线性衰减系数，$\\mu_{\\mathrm{water}}$ 是水的线性衰减系数。破坏此校准的图像强度单调或非线性重映射会改变 HU 值的物理意义。\n- 刚性运动（平移、旋转、反射）保持病灶的欧几里得几何形状，而非刚性形变可以改变具有临床预测性的形状描述符（例如，毛刺征、边缘锐度）。\n- 采集噪声可以建模为微小的加性扰动，这些扰动不会改变某一组织类别的 HU 期望值。\n\n下面哪些增强选项是标签保持的，并且适用于这个基于CT的放射组学任务中的迁移学习？哪些选项因为改变了HU语义或病灶几何形状而应被排除？选择所有适用选项。\n\nA. 应用平面内的小范围刚性運動：在重采样至各向同性体素间距后，进行最大 $\\pm 5^\\circ$ 的旋转和最大 $\\pm 3$ 体素的平移，并约束变换后的 ROI 仍完全包含病灶。\n\nB. 应用全局线性强度重映射以模拟对比度变化：在窗位窗宽调整之前，将所有体素强度乘以因子 $\\alpha = 1.2$ 并加上一个偏移量 $\\beta = 100$ (单位为 HU)。\n\nC. 向体素强度添加零均值高斯噪声 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$，其中 $\\sigma$ 的选择满足 $\\sigma \\ll W$，$W$ 是用于肺窗调整的窗宽。\n\nD. 应用由位移场参数化的随机弹性形变，最大振幅为 $a = 5$ 体素，以增加病灶的形状变异性。\n\nE. 在窗位窗宽调整之前，应用全局直方图均衡化或 $\\gamma = 0.6$ 的伽马校正，以改善局部对比度。\n\nF. 应用左右镜像（沿矢状面的反射），同时保留体素间距元数据并确保 ROI 仍以病灶为中心。\n\n选择所有正确选项。", "solution": "该问题要求评估几种数据增强技术，这些技术用于一个使用预训练卷积神经网络 (CNN) 对计算机断层扫描 (CT) 图像中的肺结节恶性程度进行分类的放射组学流程。核心要求是任何增强方法都必须是**标签保持**的，这意味着它不会改变结节赖以确定临床标签（恶性或良性）的基本特征。问题明确指出，标签是由“内在的组织放射密度模式和形态学”决定的，并且对“整体方向”是不变的。\n\n我们将根据这些标准评估每种提出的增强技术。\n\n**A. 应用平面内的小范围刚性运动：在重采样至各向同性体素间距后，进行最大 $\\pm 5^\\circ$ 的旋转和最大 $\\pm 3$ 体素的平移，并约束变换后的 ROI 仍完全包含病灶。**\n\n- **分析**：旋转和平移是刚性运动。根据定义，刚性运动保持物体的欧几里得几何形状。这意味着结节的所有形态学特征，如其形状、大小、边缘锐度和毛刺征，都被完美地保留下来。此外，刚性运动只改变体素的空间位置，不改变其强度值。因此，“内在的组织放射密度模式”也被保留下来。因此，该变换是标签保持的，即 $y(T(I)) = y(I)$。引入方向和位置上的微小变化有助于 CNN 对结节姿态和分割中心的微小变化变得更加鲁棒，这是一个理想的属性。\n- **结论**：**正确**。这是一种标准、合适且标签保持的增强方法。\n\n**B. 应用全局线性强度重映射以模拟对比度变化：在窗位窗宽调整之前，将所有体素强度乘以因子 $\\alpha = 1.2$ 并加上一个偏移量 $\\beta = 100$ (单位为 HU)。**\n\n- **分析**：这对体素强度（单位为 Hounsfield Units (HU)）应用了线性变换 $I' = \\alpha I + \\beta$。HU 标度是一个经过校准的物理标度，定义为 $\\mathrm{HU} = 1000 \\frac{\\mu - \\mu_{\\mathrm{water}}}{\\mu_{\\mathrm{water}}}$，其中 $\\mu$ 是材料的线性衰减系数。这种变换破坏了物理校准。例如，水（0 HU）将被映射到 $1.2 \\times 0 + 100 = 100$ HU，而空氣（约 -1000 HU）将被映射到 $1.2 \\times (-1000) + 100 = -1100$ HU。问题指出，标签是基于“放射密度模式”的，并明确警告说，重映射强度会“改变 HU 值的物理意义”。由于特定组织类型（例如，钙化、软组织）是由其特征 HU 范围定义的，这种变换以一种非物理的方式改变了用于分类的数据本身。因此，它不是标签保持的。\n- **结论**：**错误**。这种增强方法违反了 CT 数据的物理语义。\n\n**C. 向体素强度添加零均值高斯噪声 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$，其中 $\\sigma$ 的选择满足 $\\sigma \\ll W$，$W$ 是用于肺窗调整的窗宽。**\n\n- **分析**：该技术模拟了 CT 采集过程中固有的随机电子噪声。问题陈述指出这是一个有效的物理模型：“采集噪声可以建模为微小的加性扰动，这些扰动不会改变某一组织类别的 HU 期望值。” 添加零均值高斯噪声并不会系统性地改变组织的放射密度，而是引入了真实的变异性。任何组织的体素强度的期望值保持不变。这使得模型对真实世界扫描中不同级别的噪声更具鲁棒性。标准差 $\\sigma$ 远小于窗宽 $W$ 的条件确保了扰动是真实的，并且不会淹没信号。这是一种标签保持的增强方法，可以提高模型的泛化能力。\n- **结论**：**正确**。这是一种有物理动机且有益的增强方法。\n\n**D. 应用由位移场参数化的随机弹性形变，最大振幅为 $a = 5$ 体素，以增加病灶的形状变异性。**\n\n- **分析**：弹性形变是非刚性变换。问题明确警告说，“非刚性形变可以改变具有临床预测性的形状描述符（例如，毛刺征、边缘锐度）”。肺结节的恶性程度通常与形态学特征相关，如不规则或毛刺状边缘。应用随机弹性形变会扭曲结节的形状，可能使一个光滑、看起来良性的结节呈现出毛刺状，反之亦然。这直接改变了一个关键的生物标志物，因此不是一个标签保持的变换，因为 $y(T(I))$ 可能不等于 $y(I)$。\n- **结论**：**错误**。这种增强方法改变了病灶的预测性形态学特征。\n\n**E. 在窗位窗宽调整之前，应用全局直方图均衡化或 $\\gamma = 0.6$ 的伽马校正，以改善局部对比度。**\n\n- **分析**：直方图均衡化和伽马校正都是非线性强度重映射技术。直方图均衡化迫使强度分布变得均匀，从而完全破坏了 HU 标度中的定量信息。伽马校正应用了一个幂律函数（$I' \\propto I^\\gamma$），这也是非线性的。这两种方法都破坏了体素強度与物理组织密度之间的线性关系。正如问题所述，这种“破坏此校准的非线性图像强度重映射”是有问题的，因为它改变了“放射密度模式”所依赖的数据的物理意义。这些技术不适用于定量医学成像任务。\n- **结论**：**错误**。这些增强方法从根本上改变了 HU 值的物理意义。\n\n**F. 应用左右镜像（沿矢状面的反射），同时保留体素间距元数据并确保 ROI 仍以病灶为中心。**\n\n- **分析**：反射（镜像）是一种刚性运动（一种非正常旋转）。与其他刚性运动一样，它保留了结节的所有几何属性，包括其形状、大小和边缘形态。它还保留了所有的体素强度，只是重新排列了它们的空间位置。问题指出，临床标签对“整体方向”是不变的，而左右反射是一种方向上的改变。因此，这种变换保证是标签保持的。这是一种常见且高效的增强技术，它利用了问题特有的对称性来增加训练数据集的有效大小。\n- **结论**：**正确**。这是一种标签保持的刚性运动，它利用了问题的一个已知不变性。", "answer": "$$\\boxed{ACF}$$", "id": "4568487"}, {"introduction": "有了明确的目标函数和精心准备的数据，最后一步是设计一个高效且稳健的训练流程。直接对整个网络进行端到端训练，尤其是在数据量有限的情况下，可能会导致“灾难性遗忘”——即预训练模型中的宝贵特征被不成熟的梯度更新所破坏。这个练习 [@problem_id:4568525] 让你扮演一名算法工程师，设计一个分阶段的训练策略，通过冻结层和设置不同的学习率来巧妙地引导模型的学习过程，从而最大化迁移学习的效果。", "problem": "您正在构建一个放射组学分类器，用于从计算机断层扫描（CT）图像块中预测病灶的恶性程度。您使用一个在自然图像上预训练的卷积神经网络（CNN）骨干网络，表示为特征提取器映射 $f_{b}(\\cdot;\\theta_{b})$，其后连接一个随机初始化的全连接分类器头部 $f_{h}(\\cdot;\\theta_{h})$。设整体模型为 $f(x;\\theta)=f_{h}(f_{b}(x;\\theta_{b});\\theta_{h})$。您在一个包含 $n \\approx 400$ 名患者的数据集上，使用交叉熵损失 $\\mathcal{L}(\\theta)$ 进行训练，小批量大小为 $b \\approx 8$。假设您将使用形式为 $\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta}\\mathcal{L}$ 的随机梯度下降类更新方法，并且梯度是通过模型组合的反向传播获得的，因此相对于骨干网络参数的梯度通过链式法则穿过头部进行传播。假设骨干网络中存在批量归一化（BN）层。\n\n从第一性原理出发，论证训练早期和晚期梯度流的大小和方差，并选择一个两阶段的训练方案，该方案首先以较高的学习率训练分类器头部，然后以较低的学习率微调骨干网络。该方案的选择应基于梯度大小及其方差如何影响预期的参数变化 $\\|\\Delta \\theta\\|$ 和 $\\theta_{b}$ 中灾难性遗忘的风险，尤其是在标记数据有限和小批量大小的条件下。\n\n哪个选项最符合这些约束？\n\n- A. 阶段 $1$：冻结 $\\theta_{b}$，为 $\\theta_{h}$ 设置较高的学习率 $\\eta_{h}^{(1)} \\approx 10^{-3}$ 和权重衰减 $\\approx 10^{-4}$，保持骨干网络中的 BN 层处于评估模式（冻结运行统计量和仿射参数），并训练直到验证损失达到平稳。阶段 $2$：解冻最后 $2$ 个骨干网络阶段，并以较低的学习率 $\\eta_{b}^{(2)} \\approx 10^{-5}$ 对其进行微调，同时为头部保持一个中等的学习率 $\\eta_{h}^{(2)} \\approx 5\\times 10^{-4}$；由于 $b$ 很小，保持 BN 运行统计量冻结，并使用早停法来限制 $\\theta_{b}$ 的漂移。\n\n- B. 阶段 $1$：从一开始就以单一的高学习率 $\\eta^{(1)} \\approx 10^{-3}$ 端到端地训练所有层，以快速逃离不良的局部最小值；阶段 $2$：一旦损失下降，将学习率增加到 $\\eta^{(2)} \\approx 10^{-2}$ 以加速收敛，并全程启用 BN 更新，因为它们在 $b$ 很小的情况下能起到正则化作用。\n\n- C. 阶段 $1$：冻结 $\\theta_{h}$ 并以低学习率 $\\eta_{b}^{(1)} \\approx 10^{-4}$ 微调整个骨干网络，以便在学习分类器之前使特征适应放射组学领域；阶段 $2$：解冻 $\\theta_{h}$ 并仅以高学习率 $\\eta_{h}^{(2)} \\approx 10^{-3}$ 训练头部，允许 BN 层在两个阶段都更新运行统计量，以更好地匹配新领域。\n\n- D. 阶段 $1$：冻结 $\\theta_{b}$ 并以极低的学习率 $\\eta_{h}^{(1)} \\approx 10^{-5}$ 训练头部多个周期以避免过拟合；阶段 $2$：解冻所有层，并为了稳定性继续使用相同的极低学习率 $\\eta^{(2)} \\approx 10^{-5}$，保持 BN 层完全可训练以补偿小学习率。\n\n选择唯一的最佳选项，并准备好使用梯度下降更新的大小、反向传播的链式法则以及在小数据放射组学条件下预训练骨干网络中灾难性遗忘的风险来证明其合理性。", "solution": "### 从第一性原理推导\n\n核心挑战在于，如何在不破坏预训练权重 $\\theta_b$ 中编码的宝贵信息的情况下，将一个预训练模型适配到一个具有不同数据分布（自然图像 vs. CT 扫描）的新的小数据集上。这需要仔细管理参数更新 $\\Delta \\theta = -\\eta \\nabla_{\\theta}\\mathcal{L}$。\n\n**初始状态分析：**\n1.  **骨干网络参数 $\\theta_b$**：这些参数被初始化为参数空间中一个对于通用特征提取非常有效的位置，这是从一个大型数据集上学习到的。\n2.  **头部参数 $\\theta_h$**：这些参数是随机初始化的。因此，模型 $f(x;\\theta)$ 的初始输出将是随机的，导致初始损失 $\\mathcal{L}(\\theta)$ 很大。\n3.  **梯度大小**：\n    - 相对于头部的梯度 $\\nabla_{\\theta_h}\\mathcal{L}$ 的大小会很大，因为头部的输出与目标标签相差甚远。\n    - 骨干网络的梯度通过链式法则计算：$\\nabla_{\\theta_b}\\mathcal{L} = \\frac{\\partial f_b}{\\partial \\theta_b} \\frac{\\partial f_h}{\\partial f_b} \\frac{\\partial \\mathcal{L}}{\\partial f_h}$。由于最后一项（代表来自头部的误差信号）很大并且基于一个随机映射，因此得到的梯度 $\\nabla_{\\theta_b}\\mathcal{L}$ 也会很大、充满噪声，并且不一定指向能为新任务改善特征表示的方向。\n4.  **梯度方差**：对于一个非常小的批量大小 $b \\approx 8$，一个小批量梯度是对整个数据集（$n \\approx 400$）上真实梯度的高方差估计。随机性很高。\n\n**阶段 1：初始训练**\n\n阶段 $1$ 的主要目标是训练随机初始化的头部 $f_h$，使其能够基于冻结的骨干网络 $f_b$ 提供的特征进行合理的分类。\n\n-   **灾难性遗忘的风险**：如果我们解冻 $\\theta_b$ 并从一开始就以一个较大的学习率进行端到端训练，巨大且充满噪声的梯度将传播到骨干网络中。这将导致大的参数更新 $\\|\\Delta \\theta_b\\| = \\eta \\|\\nabla_{\\theta_b}\\mathcal{L}\\|$，从而急剧改变预训练的权重并摧毁已学习到的层次化特征。这就是灾难性遗忘的定义。\n-   **最优的阶段 1 策略**：为了减轻这种风险，必须冻结骨干网络参数 $\\theta_b$。这使得 $\\nabla_{\\theta_b}\\mathcal{L} = 0$，从而完全保护了预训练的权重。训练应仅专注于 $\\theta_h$。由于 $\\nabla_{\\theta_h}\\mathcal{L}$ 很大，一个中等偏高的学习率（例如 $\\eta_h^{(1)} \\approx 10^{-3}$）是合适的，可以使头部高效地学习新的映射。一个非常低的学习率会不必要地缓慢。\n-   **处理批量归一化（BN）**：BN 层根据预训练期间计算的运行统计量（均值和方差）来归一化激活值。CT 图像的统计特性与自然图像非常不同。然而，用一个小的批量大小 $b \\approx 8$ 来更新这些运行统计量会产生高度不稳定和充满噪声的估计，从而降低性能。因此，标准做法是保持 BN 层处于评估模式，即冻结其运行统计量并使用预训练时的值。BN 层的仿射参数（$\\gamma$, $\\beta$）是 $\\theta_b$ 的一部分，因此在此阶段也被冻结。\n-   **正则化**：考虑到数据集很小（$n \\approx 400$），过拟合是一个主要问题。对可训练参数（$\\theta_h$）应用权重衰减是一种标准且有效的正则化技术。\n\n**阶段 2：微调**\n\n一旦头部 $f_h$ 被训练到合理的程度（例如，验证损失已达到平稳），误差信号 $\\frac{\\partial \\mathcal{L}}{\\partial f_h}$ 会变得更小且更有意义。现在更新骨干网络参数 $\\theta_b$ 以使其适应 CT 数据的特性就更安全了。\n\n-   **最优的阶段 2 策略**：这个“微调”步骤必须谨慎进行。\n    -   **差分学习率**：骨干网络的学习率 $\\eta_b^{(2)}$ 必须非常小（例如，$\\eta_b^{(2)} \\approx 10^{-5}$），通常比头部的学习率小一到两个数量级。这确保了参数更新 $\\|\\Delta \\theta_b\\|$ 很小，只会在预训练初始化的基础上产生温和的漂移，而不是颠覆性的跳跃。\n    -   **分层解冻**：通常只解冻骨干网络的后几个阶段是有益的。早期层学习通用特征（如边缘、纹理），这些特征具有很高的可迁移性，而后期层学习更具任务特异性的特征。仅微调更专业化的后期层是一种平衡适应性和保留通用特征的稳健策略。\n    -   **头部的学习率**：当头部从骨干网络接收到的特征在变化时，它必须继续适应。因此，$\\theta_h$ 仍应被训练，通常使用一个学习率 $\\eta_h^{(2)}$，该学习率小于 $\\eta_h^{(1)}$ 但大于 $\\eta_b^{(2)}$。\n    -   **BN 和早停法**：小批量大小的问题依然存在，所以 BN 运行统计量应保持冻结。由于数据集很小，持续监控验证性能并使用早停法对于防止过拟合和限制 $\\theta_b$ 的过度漂移至关重要。\n\n### 逐个选项分析\n\n-   **A. 阶段 $1$：冻结 $\\theta_{b}$，为 $\\theta_{h}$ 设置较高的学习率 $\\eta_{h}^{(1)} \\approx 10^{-3}$ 和权重衰减 $\\approx 10^{-4}$，保持骨干网络中的 BN 层处于评估模式...，并训练直到验证损失达到平稳。阶段 $2$：解冻最后 $2$ 个骨干网络阶段，并以较低的学习率 $\\eta_{b}^{(2)} \\approx 10^{-5}$ 对其进行微调，同时为头部保持一个中等的学习率 $\\eta_{h}^{(2)} \\approx 5\\times 10^{-4}$；由于 $b$ 很小，保持 BN 运行统计量冻结，并使用早停法来限制 $\\theta_{b}$ 的漂移。**\n    -   此选项与推导出的原则完全一致。阶段 1 正确地隔离了头部，用适当的学习率进行训练，并正确处理了 BN 层以防止灾难性遗忘。阶段 2 实施了谨慎的微调，使用了非常低的差分学习率，正确地因批量大小 $b$ 很小而冻结 BN 统计量，并结合了部分解冻和早停等最佳实践。\n    -   **结论：正确。**\n\n-   **B. 阶段 $1$：从一开始就以单一的高学习率 $\\eta^{(1)} \\approx 10^{-3}$ 端到端地训练所有层... 阶段 $2$：将学习率增加到 $\\eta^{(2)} \\approx 10^{-2}$...**\n    -   这个策略是根本性错误的。从一开始就以高学习率进行端到端训练，会因为来自随机头部 $\\theta_h$ 的巨大且充满噪声的梯度而导致预训练权重 $\\theta_b$ 发生灾难性遗忘。在阶段 2 增加学习率与所有标准实践背道而驰，很可能导致训练不稳定和发散。在 $b \\approx 8$ 的情况下启用 BN 更新也是不明智的。\n    -   **结论：错误。**\n\n-   **C. 阶段 $1$：冻结 $\\theta_{h}$ 并以低学习率 $\\eta_{b}^{(1)} \\approx 10^{-4}$ 微调整个骨干网络... 阶段 $2$：解冻 $\\theta_{h}$ 并仅训练头部...**\n    -   这个顺序在逻辑上是不连贯的。在阶段 1，冻结随机初始化的头部 $\\theta_h$ 意味着它无法学习。没有一个学习中的头部，就没有有意义的误差信号来指导骨干网络的适应。流向 $\\theta_b$ 的梯度将基于一个固定的随机投影，对于任务特定的适应是无用的。\n    -   **结论：错误。**\n\n-   **D. 阶段 $1$：冻结 $\\theta_{b}$ 并以极低的学习率 $\\eta_{h}^{(1)} \\approx 10^{-5}$ 训练头部... 阶段 $2$：解冻所有层，并继续使用相同的极低学习率 $\\eta^{(2)} \\approx 10^{-5}$... 保持 BN 层完全可训练...**\n    -   这个策略是次优的且包含错误。使用 $\\eta_h^{(1)} \\approx 10^{-5}$ 这样极低的学习率从头开始训练随机头部会极其缓慢和低效。虽然冻结 $\\theta_b$ 是正确的，但 $\\theta_h$ 的学习率应该更高。此外，对于 $b \\approx 8$ 的小批量大小，保持 BN 层“完全可训练”（即更新运行统计量）是不正确的，其理由（“以补偿小学习率”）是毫无意义的。\n    -   **结论：错误。**\n\n基于从第一性原理的分析，选项 A 是唯一一个描述了在指定约束条件下，符合方法论、稳健且代表了当前先进水平的迁移学习训练策略的选项。", "answer": "$$\\boxed{A}$$", "id": "4568525"}]}