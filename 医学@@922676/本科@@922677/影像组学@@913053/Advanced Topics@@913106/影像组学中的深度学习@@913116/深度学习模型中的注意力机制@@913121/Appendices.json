{"hands_on_practices": [{"introduction": "深入理解注意力机制的核心在于掌握其基础计算过程——缩放点积注意力。这个练习将带你亲手计算一个经典的查询-键-值（$Q-K-V$）注意力操作，模拟一个多模态影像组学场景[@problem_id:4529597]。通过这个练习，你将明白模型如何通过计算查询（Query）与键（Key）之间的相似度来生成注意力权重，并最终用这些权重来加权聚合值（Value）信息。", "problem": "在一个用于病灶表征的多模态影像组学流程中，使用单个计算机断层扫描（CT）影像组学特征来查询来自两种辅助模态的互补信息：磁共振成像（MRI）和正电子发射断层扫描（PET）。查询（query）、键（key）和值（value）被嵌入到一个维度为 $d=2$ 的共同潜在空间中。查询向量为 $Q=\\begin{bmatrix}1  1\\end{bmatrix}$，键矩阵为 $K=\\begin{bmatrix}1  0 \\\\ 0  1\\end{bmatrix}$，值向量为 $V=\\begin{bmatrix}2 \\\\ 5\\end{bmatrix}$，其中第一个分量对应MRI，第二个分量对应PET。根据注意力机制中使用的标准缩放相似度加权方法，交叉注意力将 $Q$ 与 $K$ 各行之间的点积相似度转换为和为1的非负权重，然后将 $V$ 聚合为加权平均值。使用此机制，并设嵌入维度 $d=2$，计算CT查询的交叉注意力输出。此外，根据计算出的权重和值贡献，确定哪种模态（MRI或PET）对最终聚合得分的贡献更大。请将最终聚合得分提供为一个精确的实数值，不带单位。", "solution": "该问题要求计算交叉注意力输出，并分析不同模态的贡献。其底层机制是缩放点积注意力，这是基于Transformer的深度学习模型中的一个标准组件。注意力输出的公式如下：\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V $$\n其中，$Q$ 是查询矩阵，$K$ 是键矩阵，$V$ 是值矩阵，$d_k$ 是键向量的维度。\n\n首先，我们必须验证问题陈述的有效性。\n步骤1：提取已知条件。\n- 代表CT特征的查询向量为 $Q = \\begin{bmatrix}1  1\\end{bmatrix}$。\n- 键矩阵为 $K = \\begin{bmatrix}1  0 \\\\ 0  1\\end{bmatrix}$，其行分别对应MRI和PET模态。\n- 值向量为 $V = \\begin{bmatrix}2 \\\\ 5\\end{bmatrix}$，其分量分别对应MRI和PET。\n- 键和查询的潜在空间维度为 $d = 2$。\n- 所用机制为缩放相似度加权（缩放点积注意力）。输出是 $V$ 各分量的加权平均值。\n\n步骤2：使用提取的已知条件进行验证。\n该问题具有科学依据，因为它在一个合理的场景（多模态影像组学）中使用了一种标准算法（注意力机制）。这是一个适定的问题，所有必要的数据（$Q, K, V, d$）都已提供，且目标明确。向量和矩阵的维度对于所需操作是一致的（$Q$ 是 $1 \\times 2$，$K$ 是 $2 \\times 2$，$V$ 是 $2 \\times 1$）。该问题客观、完整，且不包含矛盾或歧义。这是一个有效的问题。\n\n步骤3：结论与行动。\n问题有效。我们将继续进行求解。\n\n计算过程分为以下几个步骤：\n\n1.  **计算点积相似度得分。**\n    得分是查询向量 $Q$ 与每个键向量（即键矩阵 $K$ 的行）的点积。此操作等效于查询矩阵 $Q$ 与键矩阵的转置 $K^T$ 的矩阵乘法。\n    键向量分别为对应MRI的 $K_1 = \\begin{bmatrix}1  0\\end{bmatrix}$ 和对应PET的 $K_2 = \\begin{bmatrix}0  1\\end{bmatrix}$。\n    键矩阵的转置为：\n    $$ K^T = \\begin{bmatrix}1  0 \\\\ 0  1\\end{bmatrix}^T = \\begin{bmatrix}1  0 \\\\ 0  1\\end{bmatrix} $$\n    然后计算得分向量：\n    $$ \\text{scores} = Q K^T = \\begin{bmatrix}1  1\\end{bmatrix} \\begin{bmatrix}1  0 \\\\ 0  1\\end{bmatrix} = \\begin{bmatrix}(1)(1) + (1)(0)  (1)(0) + (1)(1)\\end{bmatrix} = \\begin{bmatrix}1  1\\end{bmatrix} $$\n    MRI的原始相似度得分为 $s_1 = 1$，PET的原始相似度得分为 $s_2 = 1$。\n\n2.  **缩放得分。**\n    通过除以键向量维度 $d_k = d = 2$ 的平方根来对得分进行缩放。缩放因子为 $\\frac{1}{\\sqrt{2}}$。\n    缩放后的得分向量 $z$ 为：\n    $$ z = \\begin{bmatrix}z_1  z_2\\end{bmatrix} = \\begin{bmatrix}1 \\cdot \\frac{1}{\\sqrt{2}}  1 \\cdot \\frac{1}{\\sqrt{2}}\\end{bmatrix} = \\begin{bmatrix}\\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}}\\end{bmatrix} $$\n\n3.  **使用softmax函数计算注意力权重。**\n    注意力权重是通过对缩放后的得分应用softmax函数来计算的。第 $i$ 种模态的权重 $w_i$ 由 $w_i = \\frac{\\exp(z_i)}{\\sum_{j} \\exp(z_j)}$ 给出。\n    对于MRI模态（第一个分量）：\n    $$ w_{MRI} = w_1 = \\frac{\\exp(z_1)}{\\exp(z_1) + \\exp(z_2)} = \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{\\exp(\\frac{1}{\\sqrt{2}}) + \\exp(\\frac{1}{\\sqrt{2}})} = \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{2 \\exp(\\frac{1}{\\sqrt{2}})} = \\frac{1}{2} $$\n    对于PET模态（第二个分量）：\n    $$ w_{PET} = w_2 = \\frac{\\exp(z_2)}{\\exp(z_1) + \\exp(z_2)} = \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{\\exp(\\frac{1}{\\sqrt{2}}) + \\exp(\\frac{1}{\\sqrt{2}})} = \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{2 \\exp(\\frac{1}{\\sqrt{2}})} = \\frac{1}{2} $$\n    注意力权重相等，$w_{MRI} = 0.5$，$w_{PET} = 0.5$。这表明在该潜在空间表示中，CT查询向量 $Q$ 与MRI和PET的键向量具有同等的相似度。\n\n4.  **计算最终的聚合得分（注意力输出）。**\n    输出是值向量 $V$ 的加权平均，其中 $V$ 的分量为 $v_{MRI} = 2$ 和 $v_{PET} = 5$。\n    $$ \\text{Output} = \\sum_{i=1}^2 w_i v_i = w_{MRI} v_{MRI} + w_{PET} v_{PET} = \\left(\\frac{1}{2}\\right)(2) + \\left(\\frac{1}{2}\\right)(5) = 1 + \\frac{5}{2} = 1 + 2.5 = 3.5 $$\n    最终的聚合得分为 $3.5$。\n\n5.  **确定贡献更大的模态。**\n    每种模态对最终得分的贡献是其值乘以其注意力权重，即 $w_i v_i$。\n    - 来自MRI的贡献：$w_{MRI} v_{MRI} = \\left(\\frac{1}{2}\\right)(2) = 1$。\n    - 来自PET的贡献：$w_{PET} v_{PET} = \\left(\\frac{1}{2}\\right)(5) = 2.5$。\n    由于 $2.5 > 1$，PET模态对最终聚合得分的贡献更大。这是因为，尽管注意力机制发现两种模态同等相关（权重相等），但PET的值（$v_{PET}=5$）所代表的信息内容或信号远大于MRI的值（$v_{MRI}=2$）。", "answer": "$$\\boxed{3.5}$$", "id": "4529597"}, {"introduction": "注意力机制不仅是一个理论概念，它在现代深度网络架构中有着广泛应用，例如“挤压-激励”（Squeeze-and-Excitation, SE）模块中的通道注意力。这个练习将引导你分析SE模块中的参数数量，并探讨一个关键的设计选择——压缩率 $r$ ——如何影响模型的效率和在小型影像组学数据集上的过拟合风险[@problem_id:4529549]。这有助于你将抽象的理论与构建高效、稳健模型的实际工程问题联系起来。", "problem": "在用于放射组学的卷积神经网络中，一个基于压缩与激励（Squeeze-and-Excitation, SE）的通道注意力模块被插入到一个具有 $C$ 个输出通道的卷积块之后，以自适应地重新加权逐通道的特征响应。SE模块首先在空间维度上执行全局平均池化（此过程不引入可训练参数），然后是一个带有压缩比 $r$ 的两层多层感知机（MLP），实现映射 $\\mathbb{R}^{C} \\rightarrow \\mathbb{R}^{C/r} \\rightarrow \\mathbb{R}^{C}$。每个全连接层都包含权重和偏置。激励过程使用sigmoid非线性函数，该函数不引入可训练参数。\n\n使用核心定义，即一个将 $\\mathbb{R}^{n_{\\mathrm{in}}}$ 映射到 $\\mathbb{R}^{n_{\\mathrm{out}}}$ 的全连接层拥有 $n_{\\mathrm{in}} n_{\\mathrm{out}}$ 个权重和 $n_{\\mathrm{out}}$ 个偏置，请用 $C$ 和 $r$ 推导出SE MLP中可训练参数的总数。然后，在 $C = 64$ 和 $r = 16$ 的情况下，计算该参数数量的数值。最后，基于您推导的表达式，从第一性原理出发，论证在 $C$ 固定的假设下，增加 $r$ 如何影响样本量有限的小型放射组学队列中的参数效率和过拟合风险。\n\n请将参数数量的数值以精确整数形式报告。最终数值答案无需四舍五入，也无需单位。", "solution": "该问题要求一个包含三部分的答案：首先，推导压缩与激励（SE）模块的多层感知机（MLP）中可训练参数的总数；其次，针对 $C$ 和 $r$ 的具体值进行数值计算；第三，对压缩比 $r$ 对模型特性的影响进行合理论证。该问题是良定的且科学上合理，可以直接求解。\n\nSE模块的MLP由两个全连接（FC）层组成。可训练参数的总数（记为 $P_{\\text{total}}$）是这两层中每一层参数数量的总和。我们使用给定的定义：一个从输入维度 $n_{\\mathrm{in}}$ 映射到输出维度 $n_{\\mathrm{out}}$ 的全连接层，拥有 $n_{\\mathrm{in}} n_{\\mathrm{out}}$ 个权重和 $n_{\\mathrm{out}}$ 个偏置，总共有 $n_{\\mathrm{in}} n_{\\mathrm{out}} + n_{\\mathrm{out}}$ 个参数。\n\n让我们分析MLP的每一层，它执行映射 $\\mathbb{R}^{C} \\rightarrow \\mathbb{R}^{C/r} \\rightarrow \\mathbb{R}^{C}$。\n\n第一个全连接层（降维阶段）：\n该层将输入特征向量从维度 $C$ 映射到缩减后的维度 $C/r$。\n- 输入维度：$n_{\\mathrm{in}} = C$。\n- 输出维度：$n_{\\mathrm{out}} = \\frac{C}{r}$。\n该层中可训练权重的数量是输入和输出维度的乘积：\n$$\nW_1 = n_{\\mathrm{in}} \\times n_{\\mathrm{out}} = C \\times \\frac{C}{r} = \\frac{C^2}{r}\n$$\n该层中可训练偏置的数量等于输出维度：\n$$\nB_1 = n_{\\mathrm{out}} = \\frac{C}{r}\n$$\n因此，第一个全连接层中的总参数数量 $P_1$ 为：\n$$\nP_1 = W_1 + B_1 = \\frac{C^2}{r} + \\frac{C}{r}\n$$\n\n第二个全连接层（升维阶段）：\n该层将特征向量从缩减后的维度 $C/r$ 映射回原始的通道维度 $C$。\n- 输入维度：$n_{\\mathrm{in}} = \\frac{C}{r}$。\n- 输出维度：$n_{\\mathrm{out}} = C$。\n该层中可训练权重的数量为：\n$$\nW_2 = n_{\\mathrm{in}} \\times n_{\\mathrm{out}} = \\frac{C}{r} \\times C = \\frac{C^2}{r}\n$$\n该层中可训练偏置的数量为：\n$$\nB_2 = n_{\\mathrm{out}} = C\n$$\n因此，第二个全连接层中的总参数数量 $P_2$ 为：\n$$\nP_2 = W_2 + B_2 = \\frac{C^2}{r} + C\n$$\n\n整个SE MLP中可训练参数的总数是两层参数之和：\n$$\nP_{\\text{total}} = P_1 + P_2 = \\left(\\frac{C^2}{r} + \\frac{C}{r}\\right) + \\left(\\frac{C^2}{r} + C\\right)\n$$\n合并各项，我们得到参数总数的最终表达式：\n$$\nP_{\\text{total}} = \\frac{2C^2}{r} + \\frac{C}{r} + C\n$$\n该表达式表示了SE模块MLP中可训练参数的总数，以通道数 $C$ 和压缩比 $r$ 表示。\n\n接下来，我们计算 $C = 64$ 和 $r = 16$ 时的数值。\n将这些值代入推导出的公式中：\n$$\nP_{\\text{total}} = \\frac{2(64)^2}{16} + \\frac{64}{16} + 64\n$$\n首先，我们计算各项：\n$(64)^2 = 4096$。\n$$\n\\frac{2(4096)}{16} = \\frac{8192}{16} = 512\n$$\n$$\n\\frac{64}{16} = 4\n$$\n将各部分相加：\n$$\nP_{\\text{total}} = 512 + 4 + 64 = 580\n$$\n因此，当 $C=64$ 和 $r=16$ 时，SE MLP包含 $580$ 个可训练参数。\n\n最后，我们从第一性原理出发，论证在 $C$ 固定的假设下，增加压缩比 $r$ 如何影响参数效率和过拟合风险。我们推导出的总参数数量表达式为 $P_{\\text{total}}(r) = \\frac{2C^2+C}{r} + C$。\n\n1.  对参数效率的影响：\n    参数效率指的是模型以最少的参数数量实现高性能的能力。在表达式 $P_{\\text{total}}(r)$ 中，项 $\\frac{2C^2+C}{r}$ 与 $r$ 成反比，而 $C$ 是一个常数。随着 $r$ 的增加，这个分数项的值会减小，从而减少总参数数量 $P_{\\text{total}}$。参数较少的模型被认为参数效率更高，因为它需要更少的内存和计算开销。因此，增加 $r$ 可以提高SE模块的参数效率。\n\n2.  对过拟合风险的影响：\n    过拟合是一种现象，指高容量（通常与大量参数相关）的模型学习到训练数据的具体细节和噪声，导致其对新的、未见过的数据泛化能力差。在放射组学等通常需要处理小规模患者队列和有限样本量的学科中，过拟合是一个重要的问题。通过增加 $r$，我们减少了SE模块中可训练参数的总数 $P_{\\text{total}}$。参数的减少降低了模型的整体容量。容量较低的模型更难记忆训练数据，从而被迫学习更鲁棒、更具泛化性的模式。这起到了一种隐式正则化的作用。因此，增加 $r$ 有助于减轻过拟合的风险，这对于从小数据集构建鲁棒模型是非常理想的。", "answer": "$$\n\\boxed{580}\n$$", "id": "4529549"}, {"introduction": "标准的注意力机制通常使用 softmax 函数来归一化分数，但这并非唯一选择。不同的归一化方法会产生具有不同特性的注意力分布，例如稀疏性。这个练习将对比 softmax、sparsemax 和 entmax 这三种不同的映射函数，让你探索它们在处理相同的注意力分数时如何产生从“发散”到“聚焦”的各种效果[@problem_id:4529607]。理解这些差异对于设计能够精确锁定关键病变区域的影像组学模型至关重要。", "problem": "一个分析计算机断层扫描（CT）影像的影像组学模型，对来自含肿瘤切片的三个候选斑块使用了一个注意力模块。该模块输出对应于这三个斑块的未归一化注意力分数 $s = [2, 1, 0]$，其中 $s_{i}$ 衡量了斑块 $i$ 与恶性肿瘤的相关性。为比较在注意力分布中产生不同稀疏程度的注意力机制，请按照以下步骤进行，使用概率单纯形投影和熵正则化映射的基本定义。\n\n1. 使用注意力机制中经典的规范最大熵映射，softmax 分布定义为 $p_{i} = \\exp(s_{i}) \\big/ \\sum_{j=1}^{3} \\exp(s_{j})$。计算 $s$ 的 softmax 分布。\n\n2. sparsemax 分布被定义为 $s$ 在概率单纯形 $\\Delta^{2} = \\{p \\in \\mathbb{R}^{3} : \\sum_{i=1}^{3} p_{i} = 1, p_{i} \\ge 0\\}$ 上的欧几里得投影，即 $\\min_{p \\in \\Delta^{2}} \\|p - s\\|_{2}^{2}$ 的解。从此定义出发，推导出阈值形式 $p_{i} = \\max\\{s_{i} - \\tau, 0\\}$，其中 $\\tau$ 的选择需满足 $\\sum_{i=1}^{3} p_{i} = 1$，确定正确的激活集，并计算 $s$ 的 sparsemax 分布。\n\n3. $\\alpha$-entmax 分布源于在与分数保持线性一致性的约束下最大化 Tsallis $\\alpha$-熵，其闭式解为 $p_{i} = \\big[(\\alpha - 1)(s_{i} - \\tau)\\big]_{+}^{\\frac{1}{\\alpha - 1}}$，其中 $[x]_{+} = \\max\\{x, 0\\}$ 且 $\\tau$ 的选择需满足 $\\sum_{i=1}^{3} p_{i} = 1$。将 $\\alpha$ 特化为 $1.5$，通过确定正确的支撑集并求解阈值 $\\tau$，以精确形式计算 $s$ 的 entmax-$1.5$ 分布。\n\n简要解释，在 CT 影像组学斑块注意力机制的背景下，这三种映射中哪一种能更好地隔离单个高度可疑的斑块，而不是将注意力分散到多个斑块上，并说明原因。\n\n最后，以精确的闭式形式报告 entmax-$1.5$ 分布分配给得分最高斑块（$s_{1} = 2$）的注意力权重。不要进行数值近似；请用简化的解析表达式表示你的答案。", "solution": "问题 ROL_39 要求针对给定的未归一化分数向量，计算三种不同的注意力分布（softmax、sparsemax 和 entmax-$1.5$），然后对结果进行解释，并给出一个最终的具体值。\n\n### 步骤 1：提取已知条件\n- 未归一化分数：对于三个斑块，$s = [2, 1, 0]$，因此 $s_1 = 2$，$s_2 = 1$，$s_3 = 0$。\n- Softmax 定义：$p_{i} = \\exp(s_{i}) \\big/ \\sum_{j=1}^{3} \\exp(s_{j})$。\n- Sparsemax 定义：$p = \\arg\\min_{p \\in \\Delta^{2}} \\|p - s\\|_{2}^{2}$，其中 $\\Delta^{2} = \\{p \\in \\mathbb{R}^{3} : \\sum_{i=1}^{3} p_{i} = 1, p_{i} \\ge 0\\}$。解的形式为 $p_{i} = \\max\\{s_{i} - \\tau, 0\\}$，且满足 $\\sum p_i = 1$。\n- $\\alpha$-entmax 定义：$p_{i} = \\big[(\\alpha - 1)(s_{i} - \\tau)\\big]_{+}^{\\frac{1}{\\alpha - 1}}$，其中 $[x]_{+} = \\max\\{x, 0\\}$，$\\sum p_i=1$，且 $\\alpha=1.5$。\n- 最终答案要求：在 entmax-$1.5$ 分布下，得分最高斑块的注意力权重。\n\n### 步骤 2：使用提取的已知条件进行验证\n- **科学依据**：该问题在深度学习及其在医学影像（影像组学）中的应用领域有充分的科学依据。Softmax、sparsemax 和 $\\alpha$-entmax 都是已确立的、用作注意力机制的数学定义函数。\n- **适定性**：该问题是适定的。输入被清晰定义，数学运算是标准的，并且问题的每个部分都存在唯一解。\n- **客观性**：该问题以客观的方式陈述，使用了精确的数学定义，没有主观性语言。\n\n### 步骤 3：结论与行动\n该问题是有效的。将提供完整解答。\n\n### 解答\n\n以下各节将分别处理问题的每个部分。\n\n**1. Softmax 分布**\n\nsoftmax 函数定义为 $p_{i} = \\frac{\\exp(s_{i})}{\\sum_{j=1}^{3} \\exp(s_{j})}$。\n给定分数 $s_1 = 2$、$s_2 = 1$ 和 $s_3 = 0$，我们首先计算归一化项（分母）：\n$$\n\\sum_{j=1}^{3} \\exp(s_{j}) = \\exp(2) + \\exp(1) + \\exp(0) = e^2 + e^1 + 1\n$$\n现在，我们计算每个斑块的概率：\n$$\np_1 = \\frac{\\exp(s_1)}{e^2 + e + 1} = \\frac{e^2}{e^2 + e + 1}\n$$\n$$\np_2 = \\frac{\\exp(s_2)}{e^2 + e + 1} = \\frac{e}{e^2 + e + 1}\n$$\n$$\np_3 = \\frac{\\exp(s_3)}{e^2 + e + 1} = \\frac{1}{e^2 + e + 1}\n$$\nsoftmax 分布是向量 $p_{\\text{softmax}} = \\left[ \\frac{e^2}{e^2 + e + 1}, \\frac{e}{e^2 + e + 1}, \\frac{1}{e^2 + e + 1} \\right]$。所有概率均为正，这表明了该映射的“软”特性。\n\n**2. Sparsemax 分布**\n\nsparsemax 概率由 $p_i = \\max\\{s_i - \\tau, 0\\}$ 给出，其中选择阈值 $\\tau$ 以满足 $\\sum_{i=1}^3 p_i = 1$。令 $S = \\{i | s_i > \\tau\\}$ 为支撑集（非零概率的索引集合）。条件变为 $\\sum_{i \\in S} (s_i - \\tau) = 1$。\n\n我们通过测试可能的支撑集来找到正确的支撑集，通常按分数的降序进行。分数为 $s_1=2, s_2=1, s_3=0$。\n\n- 假设支撑集为 $S = \\{1, 2, 3\\}$。这将要求 $s_3 = 0 > \\tau$。\n$\\tau$ 的方程为 $(s_1 - \\tau) + (s_2 - \\tau) + (s_3 - \\tau) = 1$。\n$(2 - \\tau) + (1 - \\tau) + (0 - \\tau) = 1 \\implies 3 - 3\\tau = 1 \\implies 3\\tau = 2 \\implies \\tau = \\frac{2}{3}$。\n这与要求 $0 > \\tau$ 相矛盾，因此支撑集的大小不是 $3$。\n\n- 假设支撑集为 $S = \\{1, 2\\}$。这要求 $s_2 = 1 > \\tau$ 且 $s_3 = 0 \\le \\tau$。\n$\\tau$ 的方程为 $(s_1 - \\tau) + (s_2 - \\tau) = 1$。\n$(2 - \\tau) + (1 - \\tau) = 1 \\implies 3 - 2\\tau = 1 \\implies 2\\tau = 2 \\implies \\tau = 1$。\n这与要求 $1 > \\tau$ 相矛盾，因此支撑集的大小不是 $2$。\n\n- 假设支撑集为 $S = \\{1\\}$。这要求 $s_1 = 2 > \\tau$ 且 $s_2 = 1 \\le \\tau$。\n$\\tau$ 的方程为 $(s_1 - \\tau) = 1$。\n$2 - \\tau = 1 \\implies \\tau = 1$。\n条件 $2 > 1$（真）和 $1 \\le 1$（真）都满足。所以，这是正确的支撑集和阈值。\n\n当 $\\tau=1$ 时，我们计算 sparsemax 概率：\n$$\np_1 = \\max\\{s_1 - \\tau, 0\\} = \\max\\{2 - 1, 0\\} = 1\n$$\n$$\np_2 = \\max\\{s_2 - \\tau, 0\\} = \\max\\{1 - 1, 0\\} = 0\n$$\n$$\np_3 = \\max\\{s_3 - \\tau, 0\\} = \\max\\{0 - 1, 0\\} = 0\n$$\nsparsemax 分布为 $p_{\\text{sparsemax}} = [1, 0, 0]$。这是一个稀疏的、“硬”分布。\n\n**3. $\\alpha=1.5$ 时的 $\\alpha$-entmax 分布**\n\n对于 $\\alpha=1.5$，$\\alpha$-entmax 概率为 $p_{i} = \\big[(\\alpha - 1)(s_{i} - \\tau)\\big]_{+}^{\\frac{1}{\\alpha - 1}}$。\n代入 $\\alpha=1.5$：\n$$\np_i = \\big[(1.5 - 1)(s_i - \\tau)\\big]_{+}^{\\frac{1}{1.5-1}} = \\big[0.5(s_i - \\tau)\\big]_{+}^2\n$$\n支撑集为 $S = \\{i | s_i > \\tau\\}$，并且我们必须满足 $\\sum_{i \\in S} \\big[0.5(s_i - \\tau)\\big]^2 = 1$。\n这可以简化为 $\\sum_{i \\in S} (s_i - \\tau)^2 = 4$。\n\n我们再次测试可能的支撑集。\n- 假设 $S=\\{1, 2, 3\\}$。这要求 $s_3=0 > \\tau$。\n$(2-\\tau)^2 + (1-\\tau)^2 + (0-\\tau)^2 = 4$。\n$(4-4\\tau+\\tau^2) + (1-2\\tau+\\tau^2) + \\tau^2 = 4$。\n$3\\tau^2 - 6\\tau + 5 = 4 \\implies 3\\tau^2 - 6\\tau + 1 = 0$。\n方程的根为 $\\tau = \\frac{6 \\pm \\sqrt{36-12}}{6} = 1 \\pm \\frac{\\sqrt{6}}{3}$。两个根都为正，与 $0 > \\tau$ 相矛盾。\n\n- 假设 $S=\\{1, 2\\}$。这要求 $s_2=1 > \\tau$ 且 $s_3=0 \\le \\tau$。\n$(2-\\tau)^2 + (1-\\tau)^2 = 4$。\n$(4-4\\tau+\\tau^2) + (1-2\\tau+\\tau^2) = 4$。\n$2\\tau^2 - 6\\tau + 5 = 4 \\implies 2\\tau^2 - 6\\tau + 1 = 0$。\n方程的根为 $\\tau = \\frac{6 \\pm \\sqrt{36-8}}{4} = \\frac{6 \\pm \\sqrt{28}}{4} = \\frac{3 \\pm \\sqrt{7}}{2}$。\n根 $\\tau_1 = \\frac{3 + \\sqrt{7}}{2} > \\frac{3+2}{2} = 2.5$，这违反了 $1 > \\tau$。\n根 $\\tau_2 = \\frac{3 - \\sqrt{7}}{2}$。由于 $2  \\sqrt{7}  3$，我们有 $0  3-\\sqrt{7}  1$，所以 $0  \\tau_2  1/2$。这满足条件 $1 > \\tau$ 和 $0 \\le \\tau$。\n因此，正确的阈值是 $\\tau = \\frac{3 - \\sqrt{7}}{2}$，支撑集是 $S=\\{1, 2\\}$。\n\n现在我们计算 entmax-$1.5$ 概率：\n由于 $s_3=0  \\tau$，因此 $p_3=0$。\n对于 $ i \\in \\{1, 2\\}$，$p_i = [0.5(s_i - \\tau)]^2 = 0.25(s_i - \\tau)^2$。\n$$\np_1 = 0.25\\left(2 - \\frac{3 - \\sqrt{7}}{2}\\right)^2 = 0.25\\left(\\frac{4 - 3 + \\sqrt{7}}{2}\\right)^2 = 0.25\\frac{(1 + \\sqrt{7})^2}{4} = \\frac{1 + 2\\sqrt{7} + 7}{16} = \\frac{8 + 2\\sqrt{7}}{16} = \\frac{4 + \\sqrt{7}}{8}\n$$\n$$\np_2 = 0.25\\left(1 - \\frac{3 - \\sqrt{7}}{2}\\right)^2 = 0.25\\left(\\frac{2 - 3 + \\sqrt{7}}{2}\\right)^2 = 0.25\\frac{(-1 + \\sqrt{7})^2}{4} = \\frac{1 - 2\\sqrt{7} + 7}{16} = \\frac{8 - 2\\sqrt{7}}{16} = \\frac{4 - \\sqrt{7}}{8}\n$$\nentmax-$1.5$ 分布为 $p_{\\text{entmax}} = \\left[\\frac{4 + \\sqrt{7}}{8}, \\frac{4 - \\sqrt{7}}{8}, 0\\right]$。\n\n**4. 解释**\n\n在 CT 影像组学斑块注意力机制的背景下，这三种映射为关注可疑区域提供了不同的策略：\n- **Softmax**（约 $[0.665, 0.245, 0.090]$）是一种非稀疏映射，它会*分散*注意力。它为所有斑块都分配了一定的相关性，即使是分数为零的斑块。如果恶性肿瘤是由多个斑块的特征组合所指示，那么这种方法会很有用。\n- **Sparsemax** ($[1, 0, 0]$) 是一种稀疏的、“硬”映射，它会*隔离*单个斑块。它将所有注意力集中在最可疑的斑块上，并剪除所有其他斑块。这是一种激进的“赢家通吃”策略，非常适合识别单个、明确的病变。\n- **Entmax-$1.5$**（约 $[0.831, 0.169, 0]$）是一种稀疏映射，提供了一种折衷方案。它剪除了最不相关的斑块，但仍在排名前两位的斑块之间分配注意力。在集中注意力方面，它比 softmax 更好，但不如 sparsemax 极端，允许模型考虑一个主要和一个次要可疑区域。\n\n因此，sparsemax 是最能隔离单个可疑斑块的机制，而 softmax 是最能分散注意力的机制。\n\n**5. 最终答案计算**\n\n问题要求 entmax-$1.5$ 分布分配给得分最高斑块（$s_1=2$）的注意力权重。这就是我们为 entmax-$1.5$ 分布计算出的 $p_1$ 的值。\n$$\np_1 = \\frac{4 + \\sqrt{7}}{8}\n$$\n这是精确、简化的解析表达式。", "answer": "$$\\boxed{\\frac{4 + \\sqrt{7}}{8}}$$", "id": "4529607"}]}