{"hands_on_practices": [{"introduction": "要掌握自动分割，首先必须理解其核心工具——U-Net 网络的结构。本练习将带你深入 U-Net 的内部，通过手动计算特征图在网络各层之间传递时的尺寸变化，来建立对卷积、池化和上采样等基本操作的直观理解 [@problem_id:4535986]。这是设计、调试和优化分割模型的基础技能。", "problem": "一个放射组学流程使用 U 形卷积神经网络 (U-Net) 对二维医学图像进行自动病灶分割。考虑一个网络，其配置如下，输入空间尺寸为 $256 \\times 256$（高度和宽度）：\n\n- 收缩路径（编码器）：在两个层级中的每一级，应用两次核大小为 $3 \\times 3$、步长为 $1$、零填充为 $0$（有效卷积）的二维卷积，然后进行核大小为 $2 \\times 2$、步长为 $2$ 的最大池化。\n- 瓶颈层：应用两次核大小为 $3 \\times 3$、步长为 $1$、零填充为 $0$ 的二维卷积。\n- 扩展路径（解码器）：在两个层级中的每一级，应用一次核大小为 $2 \\times 2$、步长为 $2$、零填充为 $0$、输出填充为 $0$ 的转置卷积，然后将编码器的对应池化前特征图根据需要进行中心裁剪以匹配空间维度后，（沿通道维度）进行拼接。每次拼接后，应用两次核大小为 $3 \\times 3$、步长为 $1$、零填充为 $0$ 的二维卷积。\n- 输出头：应用一次核大小为 $1 \\times 1$、步长为 $1$ 的二维卷积，以生成分割的 logits。\n\n从离散卷积、离散最大池化和离散转置卷积输出的空间尺寸的标准定义出发，确定编码器、瓶颈层和解码器路径上每次操作后特征图的空间尺寸，并通过计算每个空间维度上必要的中心裁剪尺寸来验证两次跳跃连接处的维度一致性。最后，计算两次跳跃连接中总共裁剪的像素数（即，由于中心裁剪而从两个编码器特征图中移除的所有像素之和），并将该总数报告为一个无单位的整数。", "solution": "该问题是有效的。这是一个关于 U-Net 架构的、提法恰当且具有科学依据的问题，U-Net 是深度学习中用于图像分割的标准模型。所有参数都已明确定义，任务是基于卷积神经网络操作的既定公式进行一系列计算。\n\n解决方案需要追踪网络中特征图的空间维度。我们首先定义所使用的每种类型层的输出尺寸公式。设输入特征图的空间尺寸为 $H_{\\text{in}} \\times W_{\\text{in}}$，输出为 $H_{\\text{out}} \\times W_{\\text{out}}$。为简单起见，由于操作是对称的，我们只展示一个维度 $H$ 的计算。\n\n1.  **二维卷积**：对于大小为 $K \\times K$ 的核，步长为 $S$，填充为 $P$，输出维度为 $H_{\\text{out}} = \\lfloor \\frac{H_{\\text{in}} + 2P - K}{S} \\rfloor + 1$。在本例中，卷积被指定为“有效”，意味着零填充 ($P=0$)，核大小为 $K=3$，步长为 $S=1$。公式简化为 $H_{\\text{out}} = H_{\\text{in}} - 3 + 1 = H_{\\text{in}} - 2$。\n\n2.  **最大池化**：对于大小为 $K \\times K$ 的核和步长 $S$，输出维度为 $H_{\\text{out}} = \\lfloor \\frac{H_{\\text{in}} - K}{S} \\rfloor + 1$。在本例中，核大小为 $K=2$，步长为 $S=2$。假设输入维度是偶数，则公式简化为 $H_{\\text{out}} = \\frac{H_{\\text{in}}}{2}$。\n\n3.  **转置卷积**：对于大小为 $K \\times K$ 的核，步长为 $S$，填充为 $P$，输出填充为 $O_p$，输出维度为 $H_{\\text{out}} = (H_{\\text{in}} - 1)S - 2P + K + O_p$。在本例中，$K=2$，$S=2$，$P=0$，$O_p=0$。公式简化为 $H_{\\text{out}} = (H_{\\text{in}} - 1) \\times 2 + 2 = 2H_{\\text{in}}$。\n\n我们现在从 $256 \\times 256$ 的输入开始，追踪网络中的空间维度。\n\n**收缩路径（编码器）**\n\n*   **编码器层级 1：**\n    *   输入：$256 \\times 256$\n    *   第一次 $3 \\times 3$ 卷积：$256 - 2 = 254$。尺寸为 $254 \\times 254$。\n    *   第二次 $3 \\times 3$ 卷积：$254 - 2 = 252$。尺寸为 $252 \\times 252$。这是第一个跳跃连接的特征图，我们称其尺寸为 $S_1 = 252 \\times 252$。\n    *   $2 \\times 2$ 最大池化：$252 / 2 = 126$。尺寸为 $126 \\times 126$。\n\n*   **编码器层级 2：**\n    *   输入：$126 \\times 126$\n    *   第一次 $3 \\times 3$ 卷积：$126 - 2 = 124$。尺寸为 $124 \\times 124$。\n    *   第二次 $3 \\times 3$ 卷积：$124 - 2 = 122$。尺寸为 $122 \\times 122$。这是第二个跳跃连接的特征图，我们称其尺寸为 $S_2 = 122 \\times 122$。\n    *   $2 \\times 2$ 最大池化：$122 / 2 = 61$。尺寸为 $61 \\times 61$。\n\n**瓶颈层**\n\n*   输入：$61 \\times 61$\n*   第一次 $3 \\times 3$ 卷积：$61 - 2 = 59$。尺寸为 $59 \\times 59$。\n*   第二次 $3 \\times 3$ 卷积：$59 - 2 = 57$。尺寸为 $57 \\times 57$。\n\n**扩展路径（解码器）**\n\n*   **解码器层级 1：**\n    *   来自瓶颈层的输入：$57 \\times 57$\n    *   $2 \\times 2$ 转置卷积：$57 \\times 2 = 114$。尺寸为 $114 \\times 114$。这是上采样的特征图。\n    *   **跳跃连接 1 (来自编码器层级 2):** 为了拼接，必须将编码器的特征图 ($S_2 = 122 \\times 122$) 裁剪以匹配解码器的上采样图 ($114 \\times 114$)。\n        *   每个维度的裁剪尺寸：$122 - 114 = 8$ 像素。\n        *   中心裁剪从四个边中的每一边移除 $8/2 = 4$ 个像素。\n        *   从此特征图中移除的像素数：$(122 \\times 122) - (114 \\times 114) = 14884 - 12996 = 1888$。\n    *   拼接后，尺寸为 $114 \\times 114$。\n    *   第一次 $3 \\times 3$ 卷积：$114 - 2 = 112$。尺寸为 $112 \\times 112$。\n    *   第二次 $3 \\times 3$ 卷积：$112 - 2 = 110$。尺寸为 $110 \\times 110$。\n\n*   **解码器层级 2：**\n    *   来自解码器层级 1 的输入：$110 \\times 110$\n    *   $2 \\times 2$ 转置卷积：$110 \\times 2 = 220$。尺寸为 $220 \\times 220$。这是上采样的特征图。\n    *   **跳跃连接 2 (来自编码器层级 1):** 为了拼接，必须将编码器的特征图 ($S_1 = 252 \\times 252$) 裁剪以匹配解码器的上采样图 ($220 \\times 220$)。\n        *   每个维度的裁剪尺寸：$252 - 220 = 32$ 像素。\n        *   中心裁剪从四个边中的每一边移除 $32/2 = 16$ 个像素。\n        *   从此特征图中移除的像素数：$(252 \\times 252) - (220 \\times 220) = 63504 - 48400 = 15104$。\n    *   拼接后，尺寸为 $220 \\times 220$。\n    *   第一次 $3 \\times 3$ 卷积：$220 - 2 = 218$。尺寸为 $218 \\times 218$。\n    *   第二次 $3 \\times 3$ 卷积：$218 - 2 = 216$。尺寸为 $216 \\times 216$。\n\n**输出头**\n\n*   输入：$216 \\times 216$\n*   $1 \\times 1$ 卷积 ($K=1, S=1, P=0$): $H_{\\text{out}} = (216 + 2 \\times 0 - 1) / 1 + 1 = 216$。尺寸保持为 $216 \\times 216$。\n\n**最终计算**\n\n问题要求计算两次跳跃连接中总共裁剪的像素数。我们将在每个拼接步骤中移除的像素数相加。\n\n*   在第一个跳跃连接（解码器层级 1）中裁剪的像素数：$1888$。\n*   在第二个跳跃连接（解码器层级 2）中裁剪的像素数：$15104$。\n\n总裁剪像素数 = $1888 + 15104 = 16992$。", "answer": "$$\n\\boxed{16992}\n$$", "id": "4535986"}, {"introduction": "仅仅搭建网络结构是不够的，我们还需要一种机制来“训练”它，使其能够从数据中学习。这个训练过程的核心是损失函数（loss function），它量化了模型预测与真实标签之间的差距。本练习通过推导常用的 Soft Dice 损失的梯度，揭示了模型在训练过程中如何根据误差调整自身参数的数学原理，并探讨了保证训练稳定性的重要技巧 [@problem_id:4535984]。", "problem": "在使用卷积神经网络（CNN）U-Net架构从计算机断层扫描（CT）图像中进行放射组学的自动化病灶分割时，一种常见的基于重叠的损失函数是软 Dice 损失。假设有 $n$ 个体素，由 $i \\in \\{1,\\dots,n\\}$ 索引。对于每个体素，将模型预测的前景概率表示为 $p_i \\in [0,1]$，将真实标签表示为 $g_i \\in \\{0,1\\}$。软 Dice 系数 $D(\\mathbf{p},\\mathbf{g})$ 定义为\n$$\nD(\\mathbf{p},\\mathbf{g}) \\equiv \\frac{2 \\sum_{i=1}^{n} p_i g_i}{\\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i},\n$$\n而软 Dice 损失为 $L(\\mathbf{p},\\mathbf{g}) \\equiv 1 - D(\\mathbf{p},\\mathbf{g})$。\n\n仅从这些定义和微分学的标准法则出发，执行以下操作：\n\n- 对于任意固定的索引 $k \\in \\{1,\\dots,n\\}$，推导偏导数 $\\frac{\\partial L}{\\partial p_k}$ 的解析表达式。\n- 使用您推导出的表达式，论证当分母 $\\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i$ 很小时的数值稳定性，包括 $\\sum_{i=1}^{n} g_i = 0$ 的边缘情况。基于此分析，提出一个 $\\epsilon$-正则化的软 Dice 损失\n$$\nL_{\\epsilon}(\\mathbf{p},\\mathbf{g}) \\equiv 1 - \\frac{2 \\sum_{i=1}^{n} p_i g_i + \\epsilon}{\\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i + \\epsilon},\n$$\n其中 $\\epsilon > 0$，并计算 $\\frac{\\partial L_{\\epsilon}}{\\partial p_k}$ 的闭式表达式。\n\n您的最终答案必须是仅使用 $n$、$\\{p_i\\}_{i=1}^{n}$、$\\{g_i\\}_{i=1}^{n}$ 和 $\\epsilon$ 表示的 $\\frac{\\partial L_{\\epsilon}}{\\partial p_k}$ 的单个闭式解析表达式。不得包含单位。不得提供任何数值近似。", "solution": "所述问题是有效的。它在用于医学图像分析的深度学习领域有科学依据，数学上是适定的，并且是对形式化推导的客观要求。所有组成部分——软 Dice 损失、其 $\\epsilon$-正则化变体以及对偏导数的要求——都是标准且定义明确的概念。该问题是自洽的，没有矛盾或歧义。\n\n我们将按照问题陈述的要求，分三部分进行推导。\n\n首先，我们推导软 Dice 损失 $L(\\mathbf{p},\\mathbf{g})$ 的偏导数 $\\frac{\\partial L}{\\partial p_k}$ 的解析表达式。损失定义为 $L(\\mathbf{p},\\mathbf{g}) = 1 - D(\\mathbf{p},\\mathbf{g})$，其中软 Dice 系数 $D(\\mathbf{p},\\mathbf{g})$ 为：\n$$\nD(\\mathbf{p},\\mathbf{g}) = \\frac{2 \\sum_{i=1}^{n} p_i g_i}{\\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i}\n$$\n损失 $L$ 关于特定预测 $p_k$ 的偏导数为：\n$$\n\\frac{\\partial L}{\\partial p_k} = \\frac{\\partial}{\\partial p_k} \\left( 1 - D(\\mathbf{p},\\mathbf{g}) \\right) = - \\frac{\\partial D}{\\partial p_k}\n$$\n为了计算 $\\frac{\\partial D}{\\partial p_k}$，我们应用导数的商法则。我们将分子定义为 $U = 2 \\sum_{i=1}^{n} p_i g_i$，分母定义为 $V = \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i$。商法则表明 $\\frac{\\partial}{\\partial p_k} \\left(\\frac{U}{V}\\right) = \\frac{\\frac{\\partial U}{\\partial p_k} V - U \\frac{\\partial V}{\\partial p_k}}{V^2}$。\n\n我们首先求 $U$ 和 $V$ 关于 $p_k$ 的偏导数：\n$$\n\\frac{\\partial U}{\\partial p_k} = \\frac{\\partial}{\\partial p_k} \\left( 2 \\sum_{i=1}^{n} p_i g_i \\right) = 2 \\sum_{i=1}^{n} \\frac{\\partial p_i}{\\partial p_k} g_i = 2 g_k\n$$\n这是因为 $\\frac{\\partial p_i}{\\partial p_k} = \\delta_{ik}$，其中 $\\delta_{ik}$ 是克罗内克 δ 符号，当 $i=k$ 时为 $1$，否则为 $0$。\n$$\n\\frac{\\partial V}{\\partial p_k} = \\frac{\\partial}{\\partial p_k} \\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i \\right) = \\sum_{i=1}^{n} \\frac{\\partial p_i}{\\partial p_k} + 0 = 1\n$$\n项 $\\sum_{i=1}^{n} g_i$ 相对于任何 $p_k$ 都是常数。\n\n将这些代入商法则：\n$$\n\\frac{\\partial D}{\\partial p_k} = \\frac{(2 g_k) \\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i \\right) - \\left( 2 \\sum_{i=1}^{n} p_i g_i \\right) (1)}{\\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i \\right)^2}\n$$\n因此，损失函数 $L$ 的偏导数为：\n$$\n\\frac{\\partial L}{\\partial p_k} = - \\frac{2 g_k \\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i \\right) - 2 \\sum_{i=1}^{n} p_i g_i}{\\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i \\right)^2} = \\frac{2 \\sum_{i=1}^{n} p_i g_i - 2 g_k \\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i \\right)}{\\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i \\right)^2}\n$$\n\n其次，我们分析这个表达式的数值稳定性。导数的分母是 $\\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i \\right)^2$。数值不稳定性，特别是除以零或一个非常小的数导致梯度爆炸，发生在该分母趋近于零时。如果 $\\sum_{i=1}^{n} p_i \\to 0$ 和 $\\sum_{i=1}^{n} g_i \\to 0$ 都成立，就会发生这种情况。$\\sum_{i=1}^{n} g_i = 0$ 的情况尤其重要；它对应于不含任何前景像素的图像块（例如，仅有健康组织）。在这种情况下，所有的 $g_i=0$，这意味着对于任何 $k$，$g_k=0$。\n导数的分子变为 $2 \\sum_{i=1}^{n} p_i (0) - 2 (0) \\left( \\sum_{i=1}^{n} p_i + 0 \\right) = 0$。分母变为 $\\left(\\sum_{i=1}^{n} p_i\\right)^2$。如果模型正确预测没有前景，那么 $\\sum_{i=1}^{n} p_i \\to 0$，梯度变成不定式 $\\frac{0}{0}$。这是数值不稳定的，并且会干扰神经网络的训练。\n\n提出的解决方案，即添加一个小的正常数 $\\epsilon$，是一种称为平滑的标准技术。$\\epsilon$-正则化的软 Dice 损失给出如下：\n$$\nL_{\\epsilon}(\\mathbf{p},\\mathbf{g}) = 1 - \\frac{2 \\sum_{i=1}^{n} p_i g_i + \\epsilon}{\\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i + \\epsilon}\n$$\n这种修改确保分母永远不会为零（或小于 $\\epsilon$），从而防止除以零并稳定梯度计算。\n\n第三，我们计算 $\\frac{\\partial L_{\\epsilon}}{\\partial p_k}$ 的闭式表达式。计算的结构与第一部分相同，但使用了修改后的分子和分母。\n$$\n\\frac{\\partial L_{\\epsilon}}{\\partial p_k} = \\frac{\\partial}{\\partial p_k} \\left( 1 - \\frac{2 \\sum_{i=1}^{n} p_i g_i + \\epsilon}{\\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i + \\epsilon} \\right) = - \\frac{\\partial}{\\partial p_k} \\left( \\frac{2 \\sum_{i=1}^{n} p_i g_i + \\epsilon}{\\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i + \\epsilon} \\right)\n$$\n设正则化后的分子为 $U_{\\epsilon} = 2 \\sum_{i=1}^{n} p_i g_i + \\epsilon$，正则化后的分母为 $V_{\\epsilon} = \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i + \\epsilon$。\n\n我们计算它们关于 $p_k$ 的偏导数：\n$$\n\\frac{\\partial U_{\\epsilon}}{\\partial p_k} = \\frac{\\partial}{\\partial p_k} \\left( 2 \\sum_{i=1}^{n} p_i g_i + \\epsilon \\right) = 2 g_k\n$$\n$$\n\\frac{\\partial V_{\\epsilon}}{\\partial p_k} = \\frac{\\partial}{\\partial p_k} \\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i + \\epsilon \\right) = 1\n$$\n对 $\\frac{U_{\\epsilon}}{V_{\\epsilon}}$ 应用商法则：\n$$\n\\frac{\\partial}{\\partial p_k} \\left( \\frac{U_{\\epsilon}}{V_{\\epsilon}} \\right) = \\frac{\\frac{\\partial U_{\\epsilon}}{\\partial p_k} V_{\\epsilon} - U_{\\epsilon} \\frac{\\partial V_{\\epsilon}}{\\partial p_k}}{V_{\\epsilon}^2} = \\frac{(2 g_k) \\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i + \\epsilon \\right) - \\left( 2 \\sum_{i=1}^{n} p_i g_i + \\epsilon \\right) (1)}{\\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i + \\epsilon \\right)^2}\n$$\n最后，我们通过对该表达式取反，得到正则化损失函数 $L_{\\epsilon}$ 的偏导数：\n$$\n\\frac{\\partial L_{\\epsilon}}{\\partial p_k} = - \\frac{2 g_k \\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i + \\epsilon \\right) - \\left( 2 \\sum_{i=1}^{n} p_i g_i + \\epsilon \\right)}{\\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i + \\epsilon \\right)^2}\n$$\n这个表达式可以通过将负号吸收到分子中来重写：\n$$\n\\frac{\\partial L_{\\epsilon}}{\\partial p_k} = \\frac{\\left( 2 \\sum_{i=1}^{n} p_i g_i + \\epsilon \\right) - 2 g_k \\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i + \\epsilon \\right)}{\\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i + \\epsilon \\right)^2}\n$$\n这就是所要求的对于 $\\epsilon$-正则化的软 Dice 损失的梯度的闭式解析表达式。", "answer": "$$ \\boxed{ \\frac{\\left( 2 \\sum_{i=1}^{n} p_i g_i + \\epsilon \\right) - 2 g_k \\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i + \\epsilon \\right)}{\\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i + \\epsilon \\right)^{2}} } $$", "id": "4535984"}, {"introduction": "在设计并训练了我们的分割模型之后，最后一步是客观地评估其性能。本练习将教你如何解读模型的输出，并使用混淆矩阵计算如灵敏度、特异性和精确度等关键性能指标 [@problem_id:4535965]。理解这些指标对于评估模型在真实医疗场景中的有效性至关重要，尤其是在处理类别不平衡的数据时。", "problem": "一个用于影像组学（Radiomics）的生物医学图像分割流程使用一个U形卷积神经网络（U-Net），它是一种卷积神经网络（CNN），来为一项二元任务（病灶与背景）生成一个逐体素的概率图。在对网络输出应用一个固定的概率阈值 $t$ 后，会得到一个预测的二值掩模，并将其与手动标注的真实掩模进行比较，从而得出一个混淆矩阵，其中包含真阳性（$TP$）、假阳性（$FP$）、真阴性（$TN$）和假阴性（$FN$）体素的数量。假设一个轴向切片包含 $N = 100{,}000$ 个体素，真实病灶占比（prevalence）为 $p = 0.02$，因此有 $pN = 2{,}000$ 个病灶体素和 $(1 - p)N = 98{,}000$ 个背景体素。对于阈值 $t = 0.6$，假设得到的混淆矩阵为 $TP = 1{,}620$，$FP = 1{,}800$，$TN = 96{,}200$，$FN = 380$。\n\n使用从混淆矩阵推导出的分割任务二元分类率的标准、基本原理定义，计算阈值 $t = 0.6$ 时的灵敏度、特异度和精确率。然后，从这些基本定义出发，并且不使用任何提供给你的快捷公式，解释改变阈值 $t$ 和改变占比 $p$ 如何影响这三个指标，强调它们定义中分母的作用，并在适当情况下以真实类别为条件进行说明。最后，将灵敏度和精确率的调和平均数作为你唯一的数值结果报告，并四舍五入到 $4$ 位有效数字。将结果表示为一个无量纲的十进制数。", "solution": "U形卷积神经网络（U-Net）为每个体素生成其属于病灶类别的概率。在 $t = 0.6$ 处进行阈值处理，会得到一个预测的二值掩模。混淆矩阵中的 $TP$、$FP$、$TN$ 和 $FN$ 计数总结了预测与真实值之间的联合结果。\n\n根据分割中二元分类的基本原理，灵敏度（也称为真阳性率或召回率）衡量在体素确实是病灶的情况下，预测其为病灶的条件概率。特异度（真阴性率）衡量在体素确实是背景的情况下，预测其为背景的条件概率。精确率（阳性预测值）衡量在体素被预测为病灶的情况下，其确实是病灶的条件概率。这些指标使用混淆矩阵定义如下：\n- 灵敏度是条件概率 $\\Pr(\\hat{Y} = 1 \\mid Y = 1)$，在经验上计算为被正确预测的真实病灶体素所占的比例：\n$$\n\\text{sensitivity} = \\frac{TP}{TP + FN}.\n$$\n- 特异度是条件概率 $\\Pr(\\hat{Y} = 0 \\mid Y = 0)$，计算为被正确预测的真实背景体素所占的比例：\n$$\n\\text{specificity} = \\frac{TN}{TN + FP}.\n$$\n- 精确率是条件概率 $\\Pr(Y = 1 \\mid \\hat{Y} = 1)$，计算为被预测为病灶的体素中，真实为病灶的体素所占的比例：\n$$\n\\text{precision} = \\frac{TP}{TP + FP}.\n$$\n\n使用给定的计数 $TP = 1{,}620$，$FP = 1{,}800$，$TN = 96{,}200$ 和 $FN = 380$：\n1. 计算灵敏度：\n$$\n\\text{sensitivity} = \\frac{TP}{TP + FN} = \\frac{1{,}620}{1{,}620 + 380} = \\frac{1{,}620}{2{,}000} = \\frac{81}{100} = 0.81.\n$$\n2. 计算特异度：\n$$\n\\text{specificity} = \\frac{TN}{TN + FP} = \\frac{96{,}200}{96{,}200 + 1{,}800} = \\frac{96{,}200}{98{,}000} = \\frac{481}{490} \\approx 0.981632653\\ldots\n$$\n3. 计算精确率：\n$$\n\\text{precision} = \\frac{TP}{TP + FP} = \\frac{1{,}620}{1{,}620 + 1{,}800} = \\frac{1{,}620}{3{,}420} = \\frac{9}{19} \\approx 0.473684210\\ldots\n$$\n\n对阈值 $t$ 的依赖性：\n- 阈值处理通过将预测概率超过 $t$ 的体素声明为病灶，从而将概率映射到二元预测。随着 $t$ 的减小，更多的体素被标记为病灶。根据上述定义：\n  - $TP$ 通常会增加（更多的真实病灶体素超过了更低的阈值），而 $FN$ 通常会减少，所以分母 $TP + FN$ 由真实情况（ground truth）固定；因此，随着 $t$ 的减小，灵敏度 $\\frac{TP}{TP + FN}$ 倾向于增加。\n  - $FP$ 通常会增加（更多的背景体素错误地越过了更低的阈值），而 $TN$ 通常会减少，所以随着 $t$ 的减小，特异度 $\\frac{TN}{TN + FP}$ 倾向于减少。\n  - 精确率 $\\frac{TP}{TP + FP}$ 随着 $t$ 的变化可能增加也可能减少，这取决于 $TP$ 的增加与 $FP$ 的增加之间的平衡。如果 $FP$ 的增长速度快于 $TP$，精确率会下降；如果 $TP$ 的增长速度快于 $FP$，精确率会增加。\n这些单调性之所以出现，是因为分母 $TP + FN$ 和 $TN + FP$ 由真实类别的数量固定，而分子则随着阈值引起的决策变化而变化。\n\n对占比 $p$ 的依赖性：\n- 设 $p = \\Pr(Y = 1)$ 表示病灶占比，$1 - p = \\Pr(Y = 0)$ 表示背景占比。对于一个在固定阈值 $t$ 下运行的给定分类器，定义真阳性率 $\\text{TPR} = \\Pr(\\hat{Y} = 1 \\mid Y = 1)$ 和假阳性率 $\\text{FPR} = \\Pr(\\hat{Y} = 1 \\mid Y = 0)$。那么，在总体期望上，\n$$\nTP \\approx pN \\cdot \\text{TPR}, \\quad FN \\approx pN \\cdot (1 - \\text{TPR}), \\quad FP \\approx (1 - p)N \\cdot \\text{FPR}, \\quad TN \\approx (1 - p)N \\cdot (1 - \\text{FPR}).\n$$\n因此，\n$$\n\\text{sensitivity} = \\frac{TP}{TP + FN} = \\frac{pN \\cdot \\text{TPR}}{pN \\cdot \\text{TPR} + pN \\cdot (1 - \\text{TPR})} = \\text{TPR},\n$$\n这个值与 $p$ 无关，因为分母 $TP + FN$ 是以 $Y = 1$（真实病灶类别）为条件的。同样地，\n$$\n\\text{specificity} = \\frac{TN}{TN + FP} = \\frac{(1 - p)N \\cdot (1 - \\text{FPR})}{(1 - p)N \\cdot (1 - \\text{FPR}) + (1 - p)N \\cdot \\text{FPR}} = 1 - \\text{FPR},\n$$\n这个值也与 $p$ 无关，因为分母 $TN + FP$ 是以 $Y = 0$（真实背景类别）为条件的。相比之下，精确率依赖于占比：\n$$\n\\text{precision} = \\frac{TP}{TP + FP} = \\frac{pN \\cdot \\text{TPR}}{pN \\cdot \\text{TPR} + (1 - p)N \\cdot \\text{FPR}} = \\frac{p \\cdot \\text{TPR}}{p \\cdot \\text{TPR} + (1 - p) \\cdot \\text{FPR}},\n$$\n在保持 $\\text{TPR}$ 和 $\\text{FPR}$ 固定的情况下，该值随 $p$ 的增加而增加。因此，精确率对类别占比敏感，而灵敏度和特异度则不敏感，这是因为它们的分母是以真实类别为条件的。\n\n最后，所要求的单一数值结果是灵敏度和精确率的调和平均数。两个正数 $a$ 和 $b$ 的调和平均数 $H$ 定义为\n$$\nH = \\frac{2}{\\frac{1}{a} + \\frac{1}{b}} = \\frac{2ab}{a + b}.\n$$\n将 $a$ 设为灵敏度，$b$ 设为精确率：\n$$\na = \\frac{81}{100}, \\quad b = \\frac{9}{19}.\n$$\n计算\n$$\nH = \\frac{2ab}{a + b} = \\frac{2 \\cdot \\frac{81}{100} \\cdot \\frac{9}{19}}{\\frac{81}{100} + \\frac{9}{19}} = \\frac{\\frac{1458}{1900}}{\\frac{1539 + 900}{1900}} = \\frac{\\frac{1458}{1900}}{\\frac{2439}{1900}} = \\frac{1458}{2439} = \\frac{162}{271} \\approx 0.597786\\ldots\n$$\n四舍五入到 $4$ 位有效数字，调和平均数为 $0.5978$。", "answer": "$$\\boxed{0.5978}$$", "id": "4535965"}]}