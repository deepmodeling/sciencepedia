{"hands_on_practices": [{"introduction": "决策树的核心在于其递归地将数据划分为更纯净的子集。本练习将引导你亲手实践这一核心机制，通过一个简化的影像组学场景，你将计算信息增益来确定一个特征的最佳分裂点。这个计算过程将揭示决策树是如何从数据中学习并做出决策的，为你理解更复杂的模型奠定坚实的基础 ([@problem_id:4535354])。", "problem": "一个用于将肺结节分类为良性或恶性的放射组学流程，使用基于灰度共生矩阵 (GLCM) 导出的纹理特征 $x_{j}$ 的单个单变量决策树分裂。给定已排序的特征值 $x_{j} \\in \\{0.12, 0.14, 0.18, 0.30, 0.31\\}$ 及其对应的二进制标签 $y \\in \\{0,0,1,1,1\\}$，其中 $0$ 表示良性，$1$ 表示恶性。考虑一个形式为 $x_{j} \\le \\tau$ 对 $x_{j}  \\tau$ 的决策树分裂，其中阈值 $\\tau$ 从连续的不同特征值之间的中点选取。\n\n从以下定义出发，确定使信息增益最大化的阈值 $\\tau$，并通过评估所有允许的中点的信息增益来证明您的选择。\n\n使用以下基本公式：\n- 对于一个类别概率为 $\\{p_{c}\\}$ 的集合 $S$，其熵定义为 $H(S) = -\\sum_{c} p_{c} \\ln p_{c}$，使用自然对数。\n- 对于将集合 $S$ 分裂为左子集 $S_{L}$ 和右子集 $S_{R}$ 的情况，加权分裂后熵为 $\\frac{|S_{L}|}{|S|} H(S_{L}) + \\frac{|S_{R}|}{|S|} H(S_{R})$。\n- 信息增益为 $IG = H(S) - \\left(\\frac{|S_{L}|}{|S|} H(S_{L}) + \\frac{|S_{R}|}{|S|} H(S_{R})\\right)$。\n\n以单个实数的形式提供最终阈值 $\\tau$。无需进行四舍五入。", "solution": "该问题是有效的，因为它在科学上基于信息论和机器学习的原理，提法明确且包含了所有必要信息，并且陈述客观。我们接下来将寻找能够最大化信息增益的最佳决策树分裂阈值 $\\tau$。\n\n该数据集是一组放射组学特征值及其对应的类别标签，用 $S$ 表示。特征值 $x_j$ 已排序，数据对 $(x_j, y)$ 如下：\n$S = \\{(0.12, 0), (0.14, 0), (0.18, 1), (0.30, 1), (0.31, 1)\\}$。\n样本总数为 $|S| = 5$。类别分为良性（标签 $0$）和恶性（标签 $1$）。良性样本的数量为 $n_0 = 2$，恶性样本的数量为 $n_1 = 3$。\n\n第一步是计算整个数据集 $S$ 的熵，记为 $H(S)$。各个类别的概率为 $p_0 = \\frac{n_0}{|S|} = \\frac{2}{5}$ 和 $p_1 = \\frac{n_1}{|S|} = \\frac{3}{5}$。\n使用给定的熵公式 $H(S) = -\\sum_{c} p_{c} \\ln p_{c}$：\n$$H(S) = -\\left( \\frac{2}{5} \\ln\\left(\\frac{2}{5}\\right) + \\frac{3}{5} \\ln\\left(\\frac{3}{5}\\right) \\right)$$\n这个值表示在进行任何分裂之前数据集的总不确定性。\n\n接下来，我们确定候选阈值 $\\tau$。这些阈值定义为连续的不同特征值之间的中点。已排序的不同特征值为 $0.12$, $0.14$, $0.18$, $0.30$ 和 $0.31$。\n候选阈值是：\n1. $\\tau_1 = \\frac{0.12 + 0.14}{2} = 0.13$\n2. $\\tau_2 = \\frac{0.14 + 0.18}{2} = 0.16$\n3. $\\tau_3 = \\frac{0.18 + 0.30}{2} = 0.24$\n4. $\\tau_4 = \\frac{0.30 + 0.31}{2} = 0.305$\n\n我们现在为每个候选阈值评估信息增益。在阈值 $\\tau$ 处分裂的信息增益 $IG(\\tau)$ 由以下公式给出：\n$$IG(\\tau) = H(S) - H(S|\\tau)$$\n其中 $H(S|\\tau)$ 是加权分裂后熵：\n$$H(S|\\tau) = \\frac{|S_L|}{|S|} H(S_L) + \\frac{|S_R|}{|S|} H(S_R)$$\n这里，$S_L = \\{(x_j, y) \\in S | x_j \\le \\tau \\}$ 且 $S_R = \\{(x_j, y) \\in S | x_j  \\tau \\}$。最大化 $IG(\\tau)$ 等价于最小化加权分裂后熵 $H(S|\\tau)$。\n\n我们将为四个候选阈值中的每一个计算 $H(S|\\tau)$。\n\n情况 1：$\\tau = \\tau_1 = 0.13$\n分裂条件为 $x_j \\le 0.13$ 对 $x_j  0.13$。\n$S_L = \\{(0.12, 0)\\}$。因此， $|S_L| = 1$。类别分布为 $\\{n_0=1, n_1=0\\}$。这是一个纯子集。\n熵为 $H(S_L) = -(1 \\ln(1) + 0) = 0$。注意我们使用约定 $0 \\ln(0)=0$。\n$S_R = \\{(0.14, 0), (0.18, 1), (0.30, 1), (0.31, 1)\\}$。因此， $|S_R| = 4$。类别分布为 $\\{n_0=1, n_1=3\\}$。\n熵为 $H(S_R) = -\\left(\\frac{1}{4} \\ln\\left(\\frac{1}{4}\\right) + \\frac{3}{4} \\ln\\left(\\frac{3}{4}\\right)\\right)$。\n加权分裂后熵为：\n$$H(S|\\tau_1) = \\frac{1}{5} H(S_L) + \\frac{4}{5} H(S_R) = \\frac{1}{5}(0) + \\frac{4}{5}\\left(-\\frac{1}{4} \\ln\\left(\\frac{1}{4}\\right) - \\frac{3}{4} \\ln\\left(\\frac{3}{4}\\right)\\right) = -\\frac{1}{5} \\ln\\left(\\frac{1}{4}\\right) - \\frac{3}{5} \\ln\\left(\\frac{3}{4}\\right)$$\n$$H(S|\\tau_1) = \\frac{1}{5} \\ln(4) - \\frac{3}{5}(\\ln(3) - \\ln(4)) = \\frac{4}{5} \\ln(4) - \\frac{3}{5} \\ln(3) = \\frac{8}{5} \\ln(2) - \\frac{3}{5} \\ln(3)$$\n\n情况 2：$\\tau = \\tau_2 = 0.16$\n分裂条件为 $x_j \\le 0.16$ 对 $x_j  0.16$。\n$S_L = \\{(0.12, 0), (0.14, 0)\\}$。因此， $|S_L| = 2$。类别分布为 $\\{n_0=2, n_1=0\\}$。这是一个纯子集。\n熵为 $H(S_L) = -(1 \\ln(1) + 0) = 0$。\n$S_R = \\{(0.18, 1), (0.30, 1), (0.31, 1)\\}$。因此， $|S_R| = 3$。类别分布为 $\\{n_0=0, n_1=3\\}$。这也是一个纯子集。\n熵为 $H(S_R) = -(0 + 1 \\ln(1)) = 0$。\n加权分裂后熵为：\n$$H(S|\\tau_2) = \\frac{2}{5} H(S_L) + \\frac{3}{5} H(S_R) = \\frac{2}{5}(0) + \\frac{3}{5}(0) = 0$$\n\n情况 3：$\\tau = \\tau_3 = 0.24$\n分裂条件为 $x_j \\le 0.24$ 对 $x_j  0.24$。\n$S_L = \\{(0.12, 0), (0.14, 0), (0.18, 1)\\}$。因此， $|S_L| = 3$。类别分布为 $\\{n_0=2, n_1=1\\}$。\n熵为 $H(S_L) = -\\left(\\frac{2}{3} \\ln\\left(\\frac{2}{3}\\right) + \\frac{1}{3} \\ln\\left(\\frac{1}{3}\\right)\\right)$。\n$S_R = \\{(0.30, 1), (0.31, 1)\\}$。因此， $|S_R| = 2$。类别分布为 $\\{n_0=0, n_1=2\\}$。这是一个纯子集。\n熵为 $H(S_R) = -(0 + 1 \\ln(1)) = 0$。\n加权分裂后熵为：\n$$H(S|\\tau_3) = \\frac{3}{5} H(S_L) + \\frac{2}{5} H(S_R) = \\frac{3}{5}\\left(-\\frac{2}{3} \\ln\\left(\\frac{2}{3}\\right) - \\frac{1}{3} \\ln\\left(\\frac{1}{3}\\right)\\right) + \\frac{2}{5}(0) = -\\frac{2}{5} \\ln\\left(\\frac{2}{3}\\right) - \\frac{1}{5} \\ln\\left(\\frac{1}{3}\\right)$$\n$$H(S|\\tau_3) = -\\frac{2}{5}(\\ln(2) - \\ln(3)) - \\frac{1}{5}(-\\ln(3)) = \\frac{3}{5} \\ln(3) - \\frac{2}{5} \\ln(2)$$\n\n情况 4：$\\tau = \\tau_4 = 0.305$\n分裂条件为 $x_j \\le 0.305$ 对 $x_j  0.305$。\n$S_L = \\{(0.12, 0), (0.14, 0), (0.18, 1), (0.30, 1)\\}$。因此， $|S_L| = 4$。类别分布为 $\\{n_0=2, n_1=2\\}$。\n熵为 $H(S_L) = -\\left(\\frac{2}{4} \\ln\\left(\\frac{2}{4}\\right) + \\frac{2}{4} \\ln\\left(\\frac{2}{4}\\right)\\right) = -\\ln\\left(\\frac{1}{2}\\right) = \\ln(2)$。\n$S_R = \\{(0.31, 1)\\}$。因此， $|S_R| = 1$。类别分布为 $\\{n_0=0, n_1=1\\}$。这是一个纯子集。\n熵为 $H(S_R) = -(0 + 1 \\ln(1)) = 0$。\n加权分裂后熵为：\n$$H(S|\\tau_4) = \\frac{4}{5} H(S_L) + \\frac{1}{5} H(S_R) = \\frac{4}{5} \\ln(2) + \\frac{1}{5}(0) = \\frac{4}{5} \\ln(2)$$\n\n为了找到最佳阈值，我们比较加权分裂后熵。最低的熵对应着最高的信息增益。\n- $H(S|\\tau_1) = \\frac{8}{5} \\ln(2) - \\frac{3}{5} \\ln(3) \\approx \\frac{8}{5}(0.693) - \\frac{3}{5}(1.098) = 1.109 - 0.659 = 0.450$\n- $H(S|\\tau_2) = 0$\n- $H(S|\\tau_3) = \\frac{3}{5} \\ln(3) - \\frac{2}{5} \\ln(2) \\approx \\frac{3}{5}(1.098) - \\frac{2}{5}(0.693) = 0.659 - 0.277 = 0.382$\n- $H(S|\\tau_4) = \\frac{4}{5} \\ln(2) \\approx \\frac{4}{5}(0.693) = 0.554$\n\n比较这些值： $0  0.382  0.450  0.554$。\n最小的加权分裂后熵为 $0$，它在 $\\tau_2 = 0.16$ 时出现。这次分裂产生了两个纯子集，完美地分离了各个类别。因此，信息增益在此阈值下达到最大化。最大信息增益为 $IG(\\tau_2) = H(S) - 0 = H(S)$。\n\n使信息增益最大化的阈值是 $\\tau_2$。", "answer": "$$\n\\boxed{0.16}\n$$", "id": "4535354"}, {"introduction": "虽然单个决策树直观易懂，但它们可能对训练数据的微小变化非常敏感，从而导致模型不稳定。随机森林通过集成许多决策树来克服这一缺点，显著提升了模型的稳健性和预测性能。本练习将通过经典的偏倚-方差分解理论，让你量化地理解从单个决策树转向随机森林时，模型方差的减少程度，从而深入体会集成学习的力量 ([@problem_id:4535439])。", "problem": "在一项使用计算机断层扫描（CT）图像的放射组学回归研究中，一个团队使用平方误差损失，从一个高维放射组学特征集来预测一个连续的、源自影像的风险评分。对于在这些放射组学特征上训练的单个决策树，观测到的训练均方误差（MSE）为 $0.10$，观测到的测试 MSE 为 $0.18$。对于一个通过对基于相同特征构建的决策树进行 bootstrap 聚合（bagging）而获得的 Random Forest (RF) 模型，观测到的测试 MSE 为 $0.12$。在平方误差预测风险的偏差-方差-噪声分解下，并假设 bagging 改变了方差分量，而偏差和不可约噪声在这两个模型之间的变化可忽略不计，请估计由 bagging 带来的泛化误差中方差分量的绝对减少量。请以单个十进制数的形式提供您的答案，并将结果保留三位有效数字。", "solution": "该问题要求计算当从单个决策树模型转为 Random Forest 模型时，泛化误差中方差分量的绝对减少量。该解法基于平方误差损失的偏差-方差-噪声分解。\n\n期望泛化误差，可通过测试均方误差（$MSE_{test}$）来估计，可以分解为三个分量：平方偏差、方差和不可约噪声。对于一个给定的预测模型，该关系表示为：\n$$\nMSE_{test} = (\\text{Bias})^2 + \\text{Variance} + \\text{Noise}\n$$\n其中：\n- $(\\text{Bias})^2$ 是平方偏差，表示学习算法中错误假设所导致的误差。\n- $\\text{Variance}$ 是模型预测的方差，表示模型对训练集中微小波动的敏感度。\n- $\\text{Noise}$ 是不可约误差，它是数据生成过程的一个属性，代表了任何模型期望误差的下界。\n\n我们用相应的下标来表示单个决策树（DT）和 Random Forest（RF）模型的分量。\n\n对于单个决策树模型，给定的测试 MSE 为 $MSE_{test, DT} = 0.18$。其分解为：\n$$\nMSE_{test, DT} = (\\text{Bias}_{DT})^2 + \\text{Variance}_{DT} + \\text{Noise}\n$$\n代入给定值，我们得到：\n$$\n0.18 = (\\text{Bias}_{DT})^2 + \\text{Variance}_{DT} + \\text{Noise} \\quad (1)\n$$\n决策树的训练 MSE 为 $0.10$ 已被注意到，但在问题的假设下，计算并不直接需要它。训练 MSE ($0.10$) 与测试 MSE ($0.18$) 之间的差异表明存在过拟合，这是一种高方差的情况，而 bagging 正是为了缓解这种情况而设计的。\n\n对于使用 bootstrap 聚合（bagging）的 Random Forest 模型，给定的测试 MSE 为 $MSE_{test, RF} = 0.12$。其分解为：\n$$\nMSE_{test, RF} = (\\text{Bias}_{RF})^2 + \\text{Variance}_{RF} + \\text{Noise}\n$$\n代入给定值，我们得到：\n$$\n0.12 = (\\text{Bias}_{RF})^2 + \\text{Variance}_{RF} + \\text{Noise} \\quad (2)\n$$\n问题陈述了一个关键假设：“bagging 改变方差分量，而偏差和不可约噪声的变化可忽略不计”。这意味着两个条件：\n1. 不可约噪声项 $\\text{Noise}$ 是数据固有的，因此对两个模型是相同的。\n2. Random Forest 模型的平方偏差约等于单个决策树模型的平方偏差。即 $(\\text{Bias}_{RF})^2 \\approx (\\text{Bias}_{DT})^2$。我们可以将这个共同的平方偏差项表示为 $(\\text{Bias})^2$。\n\n应用此假设，方程 $(1)$ 和 $(2)$ 可以重写为：\n$$\n0.18 = (\\text{Bias})^2 + \\text{Variance}_{DT} + \\text{Noise} \\quad (1')\n$$\n$$\n0.12 = (\\text{Bias})^2 + \\text{Variance}_{RF} + \\text{Noise} \\quad (2')\n$$\n题目要求我们求出方差分量的绝对减少量，即差值 $\\text{Variance}_{DT} - \\text{Variance}_{RF}$。为了求得这个量，我们可以用方程 $(1')$ 减去方程 $(2')$：\n$$\n0.18 - 0.12 = ((\\text{Bias})^2 + \\text{Variance}_{DT} + \\text{Noise}) - ((\\text{Bias})^2 + \\text{Variance}_{RF} + \\text{Noise})\n$$\n共同的偏差项和噪声项相互抵消：\n$$\n0.06 = (\\text{Bias})^2 - (\\text{Bias})^2 + \\text{Variance}_{DT} - \\text{Variance}_{RF} + \\text{Noise} - \\text{Noise}\n$$\n$$\n0.06 = \\text{Variance}_{DT} - \\text{Variance}_{RF}\n$$\n因此，由 bagging 带来的方差分量的绝对减少量为 $0.06$。\n\n问题要求答案保留三位有效数字。计算出的值恰好是 $0.06$。为了用三位有效数字表示这个小数，我们需要在后面补零。第一个有效数字是 $6$。为了显示三位有效数字，我们将该数写为 $0.0600$。", "answer": "$$\n\\boxed{0.0600}\n$$", "id": "4535439"}, {"introduction": "在真实的医学研究中，我们经常会遇到来自同一患者的多个观测数据（例如，同一患者身上的多个病灶）。这种数据的“聚类”特性违背了许多标准机器学习模型关于样本独立性的基本假设，如果处理不当会导致数据泄漏和过于乐观的模型评估。本练习将带你直面这一影像组学研究中的关键挑战，不仅要求你设计一个严谨的、能避免信息泄漏的分析流程，还要推导出“有效样本量”的概念，帮助你理解在设计研究时如何科学地规划所需的患者数量 ([@problem_id:4535444])。", "problem": "考虑一个放射组学分类任务，其二元结果为 $Y \\in \\{0,1\\}$，放射组学特征矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，其中 $n$ 个观测值来自 $N$ 名患者。每名患者恰好提供 $m$ 个病灶，因此 $n = N m$。在此背景下，当每名患者存在多个病灶时，请以一种有原则的方式训练决策树和随机森林分类器，以避免数据泄露。您的流程应从关于独立性、簇（患者）内相关性以及患者层面划分必要性的核心定义开始，并应阐明如何在不因跨患者信息流污染评估的情况下，执行预处理、模型选择、验证和测试。然后，在一个科学上现实的假设下——即同一患者体内的病灶是可交换的，具有共同的患者内相关系数 $\\rho \\in [0,1)$，而不同患者的病灶不相关——请从第一性原理推导出一个解析表达式，用于计算达到或超过有效样本量 $n_{\\text{eff}}$ 所需的最小独立患者整数数量 $N_{\\min}$。您的最终答案应仅用 $n_{\\text{eff}}$、$m$ 和 $\\rho$ 来表示。不需要进行数值代入，并且除了最小整数的要求外，不需要其他取整。仅提供 $N_{\\min}$ 的最终表达式。", "solution": "所提出的问题在医学图像分析领域，特别是在放射组学中，是一个有效且高度相关的问题。在放射组学中，机器学习模型是利用具有分层或聚类结构的数据开发的。其核心挑战源于这样一个事实：多个观测值（病灶）来自同一分析单位（患者），这违反了许多标准统计和机器学习程序所依赖的独立性假设。\n\n首先，我们将阐述一个在训练决策树和随机森林分类器时严格避免数据泄露的有原则的流程。其基本原则是，观测值并非独立同分布（IID）。虽然可以假定来自不同患者的病灶是独立的，但来自同一患者的多个病灶通常是相关的。这种相关性源于共同的遗传、生理、环境暴露以及其他患者特异性因素。患者 $i$ 的观测值集合构成一个簇。忽略这种结构而在观测（病灶）层面进行数据划分会导致数据泄露。例如，如果来自患者 A 的一个病灶在训练集中，而来自同一患者 A 的另一个病灶在测试集中，模型可能会学习到患者特有的特性，而不是结果 $Y$ 的可泛化生物标志物。这会污染评估，导致对模型性能的估计过于乐观，而这种性能无法泛化到新的、未见过的患者身上。\n\n为防止此类信息污染，所有数据划分都必须在患者层面进行，因为患者是独立性的单位。正确的流程如下：\n1.  **数据划分**：将整个包含 $N$ 名患者的队列划分为训练集、验证集和测试集。例如，可以分配一定百分比的患者，比如 $70\\%$ 用于训练，$15\\%$ 用于验证，$15\\%$ 用于测试。任何给定患者的所有 $m$ 个病灶都必须完全位于这三个集合中的某一个之内。这确保了用于训练、超参数调整和最终评估的数据之间的严格独立性。\n2.  **预处理**：任何数据预处理步骤，例如特征标准化（缩放至零均值和单位方差），都必须仅从训练集中学习。变换的参数（例如，均值和标准差）仅使用来自训练患者的数据计算。然后，将这些相同的固定参数应用于转换验证集和测试集。这可以防止来自验证集或测试集的信息“泄露”到训练过程中。\n3.  **模型选择与超参数调整**：此阶段使用训练集和验证集来为分类器找到最佳超参数（例如，决策树的最大深度，或随机森林中树的数量）。标准方法是 $k$ 折交叉验证，该方法也必须针对聚类数据进行调整。将训练患者集划分为 $k$ 个折。在每次迭代中，模型在 $k-1$ 个患者折上进行训练，并在剩余的一个患者折上进行验证。此过程重复 $k$ 次，并对性能进行平均以选择最佳超参数集。这种患者层面的分折确保了每次迭代中的验证折始终与该次迭代的训练折相互独立。\n4.  **最终训练与测试**：在确定最佳超参数后，最终模型将在整个训练集（训练分区中所有患者的所有病灶）上进行训练。然后，其泛化性能在留出的测试集上进行一次性评估。在这个先前未曾接触过的患者集上的性能，为模型在临床环境中对新患者的表现提供了一个无偏估计。\n\n接下来，我们推导最小患者数 $N_{\\min}$ 的表达式。有效样本量 $n_{\\text{eff}}$ 的概念在这里是核心。它是一个假设的独立同分布（IID）样本的大小，该样本将产生与我们大小为 $n=Nm$ 的实际相关样本相同的统计精度（即估计量的方差相同）。\n\n设 $Y_{ij}$ 为第 $i$ 位患者的第 $j$ 个病灶的结果，其中 $i \\in \\{1, \\dots, N\\}$ 且 $j \\in \\{1, \\dots, m\\}$。不失一般性，我们假设每个 $Y_{ij}$ 具有共同方差 $\\text{Var}(Y_{ij}) = \\sigma^2$。问题陈述，来自不同患者的病灶是不相关的，而来自同一患者的病灶是可交换的，具有一个共同的患者内相关系数 $\\rho$。\n$$ \\text{Corr}(Y_{ij}, Y_{ik}) = \\rho \\quad \\text{for } j \\neq k $$\n$$ \\text{Corr}(Y_{ij}, Y_{i'k'}) = 0 \\quad \\text{for } i \\neq i' $$\n这意味着协方差结构为：\n$$ \\text{Cov}(Y_{ij}, Y_{ik}) = \\rho \\sigma^2 \\quad \\text{for } j \\neq k $$\n对于我们的聚类样本，样本均值 $\\bar{Y} = \\frac{1}{n} \\sum_{i=1}^N \\sum_{j=1}^m Y_{ij}$ 的方差是：\n$$ \\text{Var}(\\bar{Y}) = \\text{Var}\\left(\\frac{1}{Nm} \\sum_{i=1}^N \\sum_{j=1}^m Y_{ij}\\right) = \\frac{1}{(Nm)^2} \\text{Var}\\left(\\sum_{i=1}^N \\sum_{j=1}^m Y_{ij}\\right) $$\n由于来自不同患者的数据是独立的，总和的方差是患者层面总和的方差之和。设 $S_i = \\sum_{j=1}^m Y_{ij}$ 为患者 $i$ 的结果之和。\n$$ \\text{Var}\\left(\\sum_{i=1}^N S_i\\right) = \\sum_{i=1}^N \\text{Var}(S_i) $$\n假设患者在统计上是同质的，$\\text{Var}(S_i)$ 在所有患者中是恒定的，因此 $\\sum_{i=1}^N \\text{Var}(S_i) = N \\cdot \\text{Var}(S_1)$。我们来计算 $\\text{Var}(S_1)$：\n$$ \\text{Var}(S_1) = \\text{Var}\\left(\\sum_{j=1}^m Y_{1j}\\right) = \\sum_{j=1}^m \\text{Var}(Y_{1j}) + \\sum_{j \\neq k} \\text{Cov}(Y_{1j}, Y_{1k}) $$\n这里有 $m$ 个方差项和 $m(m-1)$ 个协方差项。\n$$ \\text{Var}(S_1) = m \\sigma^2 + m(m-1) \\rho \\sigma^2 = m \\sigma^2 [1 + (m-1)\\rho] $$\n将此代入 $\\text{Var}(\\bar{Y})$ 的表达式中：\n$$ \\text{Var}(\\bar{Y}) = \\frac{1}{(Nm)^2} \\cdot N \\cdot \\left( m \\sigma^2 [1 + (m-1)\\rho] \\right) = \\frac{Nm\\sigma^2 [1+(m-1)\\rho]}{N^2 m^2} $$\n$$ \\text{Var}(\\bar{Y}) = \\frac{\\sigma^2}{Nm} [1 + (m-1)\\rho] = \\frac{\\sigma^2}{n} [1 + (m-1)\\rho] $$\n项 $[1 + (m-1)\\rho]$ 是由聚类引起的方差膨胀因子。对于一个大小为 $n_{\\text{eff}}$ 的独立同分布样本，其均值的方差为 $\\frac{\\sigma^2}{n_{\\text{eff}}}$。根据有效样本量的定义，我们令这两个方差相等：\n$$ \\frac{\\sigma^2}{n_{\\text{eff}}} = \\frac{\\sigma^2}{n} [1 + (m-1)\\rho] $$\n这就给出了总观测数 $n$ 与有效样本量 $n_{\\text{eff}}$ 之间的关系：\n$$ n_{\\text{eff}} = \\frac{n}{1 + (m-1)\\rho} $$\n代入 $n = Nm$：\n$$ n_{\\text{eff}} = \\frac{Nm}{1 + (m-1)\\rho} $$\n我们需要求解的是，为使有效样本量至少达到目标值 $n_{\\text{eff}}$ 所需的最小整数患者数 $N_{\\min}$。这由不等式表示：\n$$ \\frac{N m}{1 + (m-1)\\rho} \\ge n_{\\text{eff}} $$\n我们现在求解 $N$。由于 $m \\ge 1$ 且 $\\rho \\in [0,1)$，分母 $[1+(m-1)\\rho]$ 是正的。\n$$ Nm \\ge n_{\\text{eff}} [1 + (m-1)\\rho] $$\n$$ N \\ge \\frac{n_{\\text{eff}} [1 + (m-1)\\rho]}{m} $$\n由于 $N$ 必须是整数，最小患者数 $N_{\\min}$ 是满足此不等式的最小整数，可通过对右侧表达式取上取整（ceiling function）得到。\n$$ N_{\\min} = \\left\\lceil \\frac{n_{\\text{eff}} (1 + (m-1)\\rho)}{m} \\right\\rceil $$\n该表达式提供了所需的最小独立患者数，它是期望有效样本量 $n_{\\text{eff}}$、每位患者的病灶数 $m$ 以及患者内相关系数 $\\rho$ 的函数。", "answer": "$$\\boxed{\\left\\lceil \\frac{n_{\\text{eff}} (1 + (m-1)\\rho)}{m} \\right\\rceil}$$", "id": "4535444"}]}