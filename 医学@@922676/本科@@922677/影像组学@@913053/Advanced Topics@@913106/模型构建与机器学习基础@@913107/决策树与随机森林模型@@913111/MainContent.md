## 引言
决策树与随机森林是现代机器学习工具箱中功能最强大、应用最广泛的模型之一。尤其在放射组学、生物信息学等数据密集型领域，它们凭借处理高维复杂数据的能力，正日益成为推动科学发现的关键力量。然而，这些模型强大的预测性能背后，是其精巧的统计学原理和独特的[算法设计](@entry_id:634229)。许多应用者可能仅将其作为“黑箱”使用，缺乏对其内在机制、适用边界及潜在陷阱的深入理解，这在需要高可靠性和可解释性的医学研究中构成了巨大风险。

本文旨在系统性地填补这一知识鸿沟，为读者构建一个从理论到实践的完整认知框架。我们将通过三个章节的递进式学习，带领你全面掌握[决策树](@entry_id:265930)与随机森林模型。

在“**原理与机制**”一章中，我们将从第一性原理出发，剖析构成单棵[决策树](@entry_id:265930)的分裂准则，理解其低偏差、高方差的本质，并揭示随机森林如何通过[集成学习](@entry_id:637726)的智慧，巧妙地克服单棵树的局限，成为稳定而强大的预测工具。

接着，在“**应用与跨学科联系**”一章中，我们将视野转向真实世界，探索这些模型如何在临床预测、生存分析、无监督亚型发现等任务中发挥作用，并讨论如何运用严谨的方法论处理层级数据和批次效应，以及如何解释这些复杂的“黑箱”模型。

最后，通过“**动手实践**”部分，你将有机会亲手解决基于真实研究场景设计的挑战性问题，将理论知识转化为解决实际问题的能力。

现在，让我们从最基本的构建块开始，深入探索这些模型的原理与机制。

## 原理与机制

本章深入探讨[决策树](@entry_id:265930)与随机森林模型的核心工作原理与内在机制。我们将从构成单棵决策树的基本元素——分裂准则——出发，剖析其数学基础和统计学意义。随后，我们将阐明单棵[决策树](@entry_id:265930)的固有优势与局限性，特别是它在处理高维数据时的不稳定性，从而引出[集成学习](@entry_id:637726)的必要性。最后，我们将系统地介绍[随机森林](@entry_id:146665)如何通过聚合大量[决策树](@entry_id:265930)来克服这些局限，并详细讨论其[超参数调优](@entry_id:143653)、性能评估和变量重要性解释的关键技术。

### [决策树](@entry_id:265930)的基本构建块：分裂与纯度

决策树是一种非参数监督学习方法，它通过一系列基于特征的决策规则来递归地分割数据空间，从而构建一个类似流程图的模型。每个内部节点代表一个对特征的测试，每个分支代表测试的结果，而每个[叶节点](@entry_id:266134)则代表一个类别标签（[分类树](@entry_id:635612)）或一个连续值（[回归树](@entry_id:636157)）。模型构建的核心在于如何选择最佳的分裂，这取决于我们如何量化一个数据子集的“纯度”或“[同质性](@entry_id:636502)”。

#### [分类树](@entry_id:635612)的不纯度度量

对于分类任务，目标是创建尽可能“纯”的[叶节点](@entry_id:266134)，即[叶节点](@entry_id:266134)中的样本尽可能属于同一类别。为了实现这一点，我们需要一个度量来量化节点中类别分布的“不纯度”（Impurity）。两种最常用的度量是[基尼不纯度](@entry_id:147776)（Gini Impurity）和信息熵（Information Entropy）。

从第一性原理出发，我们可以将这两个度量分别理解为回答两个不同的问题 [@problem_id:4535381]。假设一个节点中包含 $K$ 个类别，其样本比例分布为[概率质量函数](@entry_id:265484) $p = (p_1, p_2, \dots, p_K)$，其中 $\sum_{k=1}^{K} p_k = 1$。

**[基尼不纯度](@entry_id:147776)** 源于对“成对标签分歧”的考量。它回答了这样一个问题：“从该节点中随机独立地抽取两个样本，它们属于不同类别的概率是多少？” 两个样本都属于类别 $k$ 的概率是 $p_k^2$。因此，它们属于相同类别的总概率是 $\sum_{k=1}^{K} p_k^2$。它们属于不同类别的概率，即[基尼不纯度](@entry_id:147776) $G(p)$，则是：
$$
G(p) = 1 - \sum_{k=1}^{K} p_k^2
$$
当所有样本属于同一类别时（例如 $p_1=1, p_k=0 \text{ for } k>1$），$G(p) = 1 - 1^2 = 0$，表示完全纯净。当样本均匀分布在所有 $K$ 个类别时（$p_k = 1/K$），不纯度达到最大值 $1 - 1/K$。

**信息熵** 源于信息论，它量化了从该节点中随机抽取一个样本时，其类别标签所包含的“信息量”或“不确定性”的[期望值](@entry_id:150961)。一个概率为 $p_k$ 的事件所包含的信息量（或惊奇程度）定义为 $-\log(p_k)$。因此，节点的[信息熵](@entry_id:144587) $H(p)$ 是这些信息量的期望：
$$
H(p) = -\sum_{k=1}^{K} p_k \log(p_k)
$$
按照惯例，$0 \log 0$ 定义为 $0$。与[基尼不纯度](@entry_id:147776)类似，当节点完全纯净时，$H(p)=0$；当类别分布均匀时，熵达到最大值 $\log(K)$。

在决策树的每个节点，算法会遍历所有可用特征和所有可能的分裂阈值，计算每个潜在分裂所能带来的**不纯度减少量**（Impurity Reduction），并选择使该减少量最大化的分裂。

虽然这两种度量在多数情况下表现相似，但它们在处理极端情况时存在微妙差异。特别是在放射组学等可能涉及稀有亚型的医学应用中，它们的敏感度不同。假设一个几乎纯净的节点中，一个稀有类别以极小的概率 $\varepsilon$（$0  \varepsilon \ll 1$）出现。可以证明，在这种情况下，[基尼不纯度](@entry_id:147776)的增量 $\Delta G$ 近似与 $2\varepsilon$ 成正比，而[信息熵](@entry_id:144587)的增量 $\Delta H$ 近似与 $\varepsilon \log(1/\varepsilon)$ 成正比。由于当 $\varepsilon \to 0$ 时，$\log(1/\varepsilon)$ 趋向于无穷大，$\Delta H$ 相对于 $\Delta G$ 会变得大得多。这意味着**信息熵对稀有类别的出现比[基尼不纯度](@entry_id:147776)更为敏感** [@problem_id:4535381]。

#### [回归树](@entry_id:636157)的方差缩减

对于回归任务，预测目标是一个连续值。此时，不纯度的概念被**节点内的方差**所取代。一个好的分裂应该使得分裂后的子节点内，样本的响应变量 $Y$ 的变异性尽可能小。

在[回归树](@entry_id:636157)中，一个节点 $t$ 的不纯度通常用节点内样本的平方误差和（Sum of Squared Errors）来度量，记为 $R_t$：
$$
R_t = \sum_{i \in t} (y_i - \bar{y}_t)^2
$$
其中 $y_i$ 是节点 $t$ 中第 $i$ 个样本的真实值，$\bar{y}_t$ 是该节点中所有样本真实值的平均值。[决策树](@entry_id:265930)选择分裂的准则就是最大化分裂后子节点不纯度的总减少量，这等同于最小化子节点平方误差之和。

这个看似简单的准则背后有深刻的统计学意义 [@problem_id:4535352]。假设数据生成过程为 $Y = f(X) + \varepsilon$，其中 $\mathbb{E}[\varepsilon | X] = 0$，且噪声项 $\varepsilon_i$ 在节点内是[独立同分布](@entry_id:169067)的，方差为 $\operatorname{Var}(\varepsilon | X \in t) = \sigma_t^2$。[决策树](@entry_id:265930)模型的基本假设是，在每个[叶节点](@entry_id:266134) $t$ 内，真实的函数 $f(X)$ 可以被一个常数 $\mu_t$ 很好地近似。

在这些假设下，节点 $t$ 内的响应 $Y_i$ 可以看作是从一个均值为 $\mu_t$、方差为 $\sigma_t^2$ 的分布中抽取的独立同分布样本。统计学的一个基本结论是，对于这样的样本，**无偏样本方差** $s_t^2$ 是对真实方差 $\sigma_t^2$ 的一个[无偏估计](@entry_id:756289)，即 $\mathbb{E}[s_t^2] = \sigma_t^2$。这个无偏样本方差的定义是：
$$
s_t^2 = \frac{1}{n_t - 1} \sum_{i \in t} (y_i - \bar{y}_t)^2 = \frac{R_t}{n_t - 1}
$$
其中 $n_t$ 是节点 $t$ 中的样本数。因此，最小化节点不纯度 $R_t$（对于固定的 $n_t$）等价于[最小化条件](@entry_id:203120)方差 $\operatorname{Var}(Y | X \in t)$ 的一个[无偏估计](@entry_id:756289)。这表明，[回归树](@entry_id:636157)的分裂准则本质上是在试图找到能最有效降低数据局部方差的[特征和](@entry_id:189446)阈值。值得注意的是，样本方差的无偏性并不要求噪声服从高斯分布，只需要其二阶矩有限即可。

### 单棵[决策树](@entry_id:265930)的固有属性与局限

[决策树](@entry_id:265930)因其直观性和易于解释而备受青睐，但它们也具有一些独特的理论属性和实践中的局限性。

#### 对单调特征变换的不变性

一个非常重要且实用的特性是，[决策树](@entry_id:265930)（以及由其构成的随机森林）对单个特征的**严格单调变换**（strictly monotonic transformation）具有不变性 [@problem_id:45410]。严格单调变换是指任何保持或逆转数据点顺序的函数，例如[线性变换](@entry_id:143080)（如 Z-score 标准化 $z = (x-\mu)/\sigma$）、对数变换（如 $\log(x)$）或指数变换。

这种不变性的根本原因在于[决策树](@entry_id:265930)的分裂机制。如前所述，树的构建只依赖于特征值的**顺序**，因为它通过在排序后的特征值之间设置阈值来评估所有可能的数据分区。一个严格单调递增的变换会完全保留特征值的顺序，因此它不会改变任何可能的分区。一个严格单调递减的变换会逆转顺序，但这只会导致左右子节点的交换，而分裂带来的不纯度减少量是对称的，因此保持不变。

因此，对特征进行 Z-score 标准化、对数变换或任何其他严格单调的预处理，都不会改变最终生成的决策树的结构。改变的仅仅是存储在节点中的分裂阈值，它们会从原始尺度变换到新的尺度。

这对放射组学等领域具有重要意义。虽然特征的标准化（例如，跨队列的强度和谐化）对于确保模型的**[可复现性](@entry_id:151299)和可解释性**至关重要，但对于树模型算法本身而言，这类预处理并非必需品。然而，需要强调的是，这种不变性仅限于**严格单调**的变换。非单调的预处理，例如对特征值进行削波（clipping）或离散化为少数几个区间（binning），会合并原本不同的特征值，从而改变可能的分区集合，进而可能导致生成完全不同的树结构 [@problem_id:45410]。

#### 低偏差、高方差的本质

单棵[决策树](@entry_id:265930)，特别是那些生长得非常深、未经过剪枝的树，是典型的**低偏差、高方差**模型 [@problem_id:4535417]。

-   **低偏差（Low Bias）**：由于决策树可以不断分裂，直到[叶节点](@entry_id:266134)完全纯净或只包含极少数样本，它具有极强的灵活性，能够拟合非常复杂的函数关系，甚至可以完美地记住整个[训练集](@entry_id:636396)。这种灵活性意味着模型的偏差很小，即模型在“平均”意义上能够逼近真实的数据生成函数 $f(X)$。

-   **高方差（High Variance）**：正是这种灵活性导致了高方差。树的结构对训练数据的微小变化非常敏感。如果从数据中移除或添加几个样本，可能会导致顶层[分裂选择](@entry_id:139946)一个完全不同的特征，从而产生一棵结构迥异的树。这种不稳定性意味着模型的方差很大：在不同的训练集上训练出的模型，其预测结果会有很大差异。高方差模型容易过拟合，对训练数据表现优异，但在未见过的新数据上泛化能力很差。

决策树的这种高方差特性，尤其是在样本量相对于特征数较少的情况下，是其作为独立模型使用的主要障碍，并直接催生了以随机森林为代表的[集成方法](@entry_id:635588)的出现。

### 从决策树到[随机森林](@entry_id:146665)：[集成学习](@entry_id:637726)的力量

[随机森林](@entry_id:146665)通过组合多棵[决策树](@entry_id:265930)的智慧，有效克服了单棵[树高](@entry_id:264337)方差的缺陷，成为现代机器学习中最强大和最广泛使用的算法之一。

#### 高维数据带来的挑战

在放射组学等领域，我们经常面临“$p \gg n$”的困境，即特征的数量 $p$ 远大于样本量 $n$。这种情况对不同类型的模型构成了不同的挑战 [@problem_id:4535385]。

对于传统的**[线性模型](@entry_id:178302)**（如[线性回归](@entry_id:142318)），当 $p  n$ 时，[设计矩阵](@entry_id:165826) $X$ 的秩至多为 $n$。这导致 $X^\top X$ 矩阵是奇异的（singular），不可逆。因此，普通最小二乘法无法找到唯一的解；存在无限多组系数 $\beta$ 都能完美拟合训练数据。这是一种根本性的**数学不可辨识性**（non-identifiability），除非引入正则化（如 Ridge 或 [LASSO](@entry_id:751223)）来施加额外约束。

相比之下，单棵**决策树**在 $p \gg n$ 的情况下仍然是**算法上可辨识的**。给定一个确定的分裂规则（包括平局打破规则），算法总能生成一棵唯一的树。然而，它面临着严重的**[统计不稳定性](@entry_id:755393)**。当特征数量巨大时，算法有极大的可能性找到一些仅仅因为数据中的随机噪声而看起来具有很强预测能力的“伪特征”或“伪分裂”。这使得模型极易过拟合，其结构对训练样本的微小变动极其敏感，表现出极高的方差，泛化性能堪忧。

#### 通过集成降低方差：[偏差-方差分解](@entry_id:163867)的视角

随机森林的核心思想是通过**集成（ensembling）**来降低方差。对于一个预测任务，其期望[预测误差](@entry_id:753692)可以分解为三部分：
$$
\text{Error} = (\text{Bias})^2 + \text{Variance} + \text{Irreducible Error}
$$
[随机森林](@entry_id:146665)通过**自助法聚合（Bootstrap Aggregating，简称 [Bagging](@entry_id:145854)）**来构建一个由 $B$ 棵决策树组成的森林。每棵树都在一个从原始[训练集](@entry_id:636396)中[有放回抽样](@entry_id:274194)得到的自助样本（bootstrap sample）上训练。对于一个新的数据点，随机森林的预测结果是所有 $B$ 棵树预测结果的平均值（回归）或多数投票（分类）。

从[偏差-方差分解](@entry_id:163867)的角度看，这个聚合过程主要影响方差 [@problem_id:4535417]。假设我们有 $B$ 个预测器（即决策树），每个预测器的偏差均为 $b$，方差均为 $s^2$。

-   **偏差**：对 $B$ 个预测器的预测值取平均，其[期望值](@entry_id:150961)仍然与单个预测器的[期望值](@entry_id:150961)相同。因此，[随机森林](@entry_id:146665)的偏差近似等于构成它的单棵树的平均偏差。由于我们通常使用深层、未剪枝的树，这个偏差本身就很低。
-   **方差**：方差的降低是关键。如果这 $B$ 棵树是完全独立的，那么它们平均值的方差将是 $s^2/B$，会随着树的数量增加而急剧下降。然而，由于这些树都是在来自同一原始数据集的自助样本上训练的，它们之间是**相关的**。假设任意两棵树的预测结果之间的相关系数为 $\rho$。那么，森林整体预测的方差为：
    $$
    \text{Var}_{\text{RF}} = s^2 \left( \rho + \frac{1-\rho}{B} \right)
    $$
    当 $B \to \infty$ 时，第二项趋近于零，但方差的下限是 $\rho s^2$。这表明，**[集成学习](@entry_id:637726)的[方差缩减](@entry_id:145496)效果受限于基学习器之间的相关性**。如果树之间高度相关（$\rho$ 接近 1），聚合带来的好处就很小。

#### 多样性：集成成功的关键

上述方差公式揭示了[集成学习](@entry_id:637726)成功的秘诀：基学习器（决策树）不仅要表现良好（低偏差），还必须具有**多样性**（低相关性 $\rho$）。随机森林通过两种随机化机制来主动增加树之间的多样性：

1.  **[自助法](@entry_id:139281)抽样（[Bagging](@entry_id:145854)）**：每棵树使用不同的训练子集，这本身就引入了多样性。
2.  **随机特征子空间（Random Feature Subspacing）**：这是[随机森林](@entry_id:146665)相较于普通 [Bagging](@entry_id:145854) 的关键创新。在每个节点进行分裂时，算法不再考虑所有 $p$ 个特征，而是随机选择一个大小为 $m$（$m  p$）的特征子集，并只在这个子集中寻找最佳分裂。

这种随机特征选择极大地降低了树之间的相关性 $\rho$ [@problem_id:4535462]。在放射组学这类存在高度相关特征的领域尤其有效。如果没有特征子空间法，每棵树在顶层分裂时都很可能会选择同一组强相关的预测特征，导致森林中的树结构非常相似。而随机[特征选择](@entry_id:177971)迫使一些树在没有最强特征的情况下，去发掘其他次优但仍具预测价值的特征。这使得森林能够探索更广阔的[特征空间](@entry_id:638014)，生成的树更加多样化，从而显著降低整体方差，提升泛化能力 [@problem_id:4535384]。

我们可以用**[分歧](@entry_id:193119)（disagreement）**和**双重错误（double-fault）**等指标来量化集成中分类器的多样性。[分歧](@entry_id:193119)衡量两个分类器给出不同预测的比例，而双重错误衡量它们同时出错的比例。一个理想的集成应该由准确且彼此[分歧](@entry_id:193119)大的分类器组成，即它们的错误发生在不同的样本上。

### 随机森林的应用与解读

理解了[随机森林](@entry_id:146665)的内在机制后，我们还需要掌握如何有效地使用和解释它。

#### 通过超参数控制[偏差-方差权衡](@entry_id:138822)

[随机森林](@entry_id:146665)的性能受到几个关键超参数的控制，它们共同调节着模型的[偏差-方差权衡](@entry_id:138822) [@problem_id:4535423]：

-   $B$（森林中的树木数量）：这是最简单的参数。理论上，$B$ 越大越好，因为它能更充分地降低方差。在实践中，当 $B$ 达到一定数量后，模型的性能会趋于稳定。增加 $B$ **主要降低方差**，对偏差几乎没有影响。它的主要成本是计算时间。

-   $m$（每次分裂时考虑的特征数）：这是控制树之间相关性的关键参数。
    -   减小 $m$：会增加树的随机性，降低树之间的相关性，从而**降低森林的方差**。但这也限制了单棵树找到最佳分裂的能力，可能会略微**增加单棵树的偏差**。
    -   增大 $m$：使得树之间更加相似，增加了相关性，从而**增加森林的方差**。当 $m=p$ 时，[随机森林](@entry_id:146665)就退化为标准的 [Bagging](@entry_id:145854)。

-   `max_depth`（树的最大深度）：控制单棵树的复杂度。
    -   增加 `max_depth`：允许树生长得更深，更能拟合数据，从而**降低偏差**，但单棵树的**方差会增加**。
    -   减小 `max_depth`：限制树的复杂度，起到正则化的作用，会**增加偏差**但**降低方差**。

-   `min_samples_leaf`（[叶节点](@entry_id:266134)的最小样本数）：同样是控制树复杂度的参数。
    -   增加 `min_samples_leaf`：阻止树对训练集中的小群体进行过度拟合，使模型更平滑，从而**增加偏差**并**降低方差**。
    -   减小 `min_samples_leaf`：允许树创建更“纯”的[叶节点](@entry_id:266134)，**降低偏差**，但代价是**增加方差**。

#### 一种内置的性能评估：袋外误差

随机森林提供了一种优雅且计算高效的内置[交叉验证方法](@entry_id:634398)，称为**袋外（Out-of-Bag, OOB）误差**估计 [@problem_id:4535360]。

其原理在于，由于每棵树都是在自助样本上训练的，对于[训练集](@entry_id:636396)中的任何一个样本 $(x_i, y_i)$，总有一些树没有在它们的训练过程中见过这个样本。这些树被称为样本 $i$ 的“袋外”树。我们可以利用这些袋外树来对样本 $i$ 进行预测，得到一个“袋外预测值” $\hat{f}_{\text{OOB}}(x_i)$。这个预测值是通过聚合所有对 $i$ 而言是袋外的树的预测得到的。

由于 $\hat{f}_{\text{OOB}}(x_i)$ 是由从未见过 $(x_i, y_i)$ 的模型生成的，因此 $(x_i, y_i)$ 可以被看作是这些模型的一个真实的测试样本。通过计算所有训练样本的袋外预测误差（例如，[均方误差](@entry_id:175403)或[分类错误率](@entry_id:635045)），我们就可以得到 OOB 误差。
$$
\widehat{R}_{\text{OOB}} = \frac{1}{n}\sum_{i=1}^n \ell(y_i, \hat{f}_{\text{OOB}}(x_i))
$$
从统计学上讲，如果原始数据是独立同分布的，OOB 误差是[模型泛化](@entry_id:174365)误差的一个**无偏估计**。这为我们提供了一种在不牺牲训练数据（即无需划分出独立的验证集）的情况下，可靠地评估模型性能和进行[超参数调优](@entry_id:143653)的方法。

#### 变量重要性及其在相关特征下的挑战

[随机森林](@entry_id:146665)的一个吸引人之处在于它能提供对[特征重要性](@entry_id:171930)的度量。然而，当特征高度相关时，标准的重要性度量可能会产生误导 [@problem_id:4535384]。

标准的重要性度量有两种：
1.  **平均不纯度减少（Mean Decrease in Impurity, MDI）**：计算每个特征在森林中所有树上带来的[平均不纯度减少量](@entry_id:633916)。
2.  **[排列重要性](@entry_id:634821)（Permutation Feature Importance, PFI）**：对于一个已训练好的森林，通过随机打乱（排列）单个特征的数值，并观察模型性能（如 OOB 误差）下降的程度来衡量其重要性。

在存在高度相关特征（例如 $X_1$ 和 $X_2$）的情况下，这两种度量都会出现**重要性稀释**的问题。
-   对于 MDI，由于 $X_1$ 和 $X_2$ 都能提供相似的预测信息，一些树会选择 $X_1$ 进行分裂，另一些则会选择 $X_2$。结果是，本应归属于这个预测信号的总重要性被分散到了两个特征上，导致它们各自的 MDI分数都被低估。
-   对于 PFI，当我们排列 $X_1$ 的值时，模型仍然可以从保持不变的 $X_2$ 中获取几乎相同的信息。因此，模型的性能下降很小，导致 $X_1$ 的 PFI 分数被严重低估。

为了解决这个问题，可以使用**分组[排列重要性](@entry_id:634821)（Grouped Permutation Importance）**。其思想是，首先识别出相关的特征组，然后在计算重要性时，同时排列整个组内所有特征的数值。通过这种方式，我们衡量的是整个特征组的集体预测能力，从而得到一个对它们共同重要性的更准确的估计。

### [模型选择](@entry_id:155601)的考量：为何在放射组学中偏好树模型？

最后，让我们总结一下，为什么像[决策树](@entry_id:265930)和随机森林这样的[非参数模型](@entry_id:201779)在放射组学等生物医学领域中特别受青睐 [@problem_id:4535463]。

1.  **无需预设函数形式**：生物过程通常极其复杂，特征（如放射组学特征）与临床结果（如肿瘤侵袭性）之间的真实关系 $f(X)$ 往往是未知的、高度非线性的，并且充满了[交互作用](@entry_id:164533)。与需要预先指定模型形式（如线性）的参数模型不同，树模型可以自适应地从数据中学习这些复杂关系。

2.  **对异方差的稳健性**：在放射组学中，[特征提取](@entry_id:164394)的噪声水平可能本身就依赖于特征值。例如，在纹理更复杂的肿瘤中，分割的不确定性可能更大，导致提取的特征方差也更大。这种情况称为异方差（Heteroscedasticity）。树模型在估计条件均值 $f(x)$ 时，其分裂准则（无论是平方误差还是[基尼不纯度](@entry_id:147776)）并不依赖于噪声方差恒定的假设，因此对异方差具有天然的稳健性。

3.  **对特征尺度的不变性**：如前所述，树模型对特征的单调变换不敏感，这意味着我们无需担心特征尺度的差异会不成比例地影响模型，简化了预处理流程。

4.  **内在的[特征选择](@entry_id:177971)与解释性**：树模型通过分裂过程隐式地进行了特征选择。随机森林提供的变量重要性度量，尽管存在挑战，但仍然是探索高维数据和生成生物学假设的宝贵工具。

综上所述，决策树和随机森林凭借其强大的非参数建模能力、对复杂数据特性的稳健性以及内置的解释性工具，为探索和建模复杂的放射组学数据提供了一套强大而灵活的原理和机制。