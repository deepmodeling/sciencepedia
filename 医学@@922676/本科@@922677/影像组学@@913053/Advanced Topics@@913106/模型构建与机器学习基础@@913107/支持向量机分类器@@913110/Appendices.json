{"hands_on_practices": [{"introduction": "理论是指导，实践是检验。本练习将引导你从第一性原理出发，亲手构建一个线性支持向量机（SVM）分类器。通过为一项预测学生贷款违约的经济金融任务实现一个基于次梯度下降的完整训练和预测程序，你将深刻理解经验风险最小化、铰链损失（hinge loss）和正则化等核心概念如何协同工作。这项实践练习 [@problem_id:2435452] 不仅能巩固你的理论知识，还能将抽象的数学公式转化为具体、可执行的代码，从而牢固掌握SVM分类器的内在机制。", "problem": "要求您从经验风险最小化的首要原则出发，构建一个线性二元分类器，使用支持向量机（SVM）来预测一个家庭是否会拖欠其学生贷款。所期望的分类器必须是一个带截距的线性软间隔支持向量机（SVM），通过在带有 Tikhonov 正则化的凸间隔损失上进行次梯度下降来训练，并且必须在标准化特征上操作。目标是将这个经济-金融分类任务转化为一个适定的优化问题，然后再转化为一个可以作为可运行程序实现的算法。\n\n从以下基本基础开始：\n- 在一个带截距的线性决策函数上进行经验风险最小化，该函数将特征向量 $x \\in \\mathbb{R}^d$ 映射到一个带符号的分数。\n- 一个凸的基于间隔的损失函数，用于惩罚对期望间隔的违反，并结合对权重向量的 Tikhonov（岭）正则化，以控制复杂度和提高泛化能力。\n- 次梯度下降，作为一种用于最小化凸的、可能不可微的目标函数的原则性算法。\n\n实现一个程序，该程序：\n- 在每个提供的训练集上训练一个线性软间隔支持向量机（SVM）分类器。\n- 使用每特征的 $z$-score 对特征进行标准化，且仅使用训练数据：对于每个特征，减去训练均值并除以训练标准差（对于任何为零的标准差，使用 $1$ 以避免除以零）。\n- 使用次梯度下降来最小化在铰链式（hinge-type）基于间隔的损失下的正则化经验风险，并采用用户指定的学习率调度和迭代次数。\n- 在每种情况下训练后，通过学习到的线性分数的符号来预测指定测试家庭的类别标签。使用 $\\{-1,+1\\}$ 中的标签，其中 $+1$ 表示违约，$-1$ 表示不违约。\n\n测试套件。共有三种情况。在每种情况下，输入包括一个训练矩阵 $X^{(k)}$（行代表家庭，列代表特征），一个标签向量 $y^{(k)} \\in \\{-1,+1\\}^{n_k}$，一个测试向量 $x_\\star^{(k)}$，以及超参数：正则化参数 $C$、迭代次数 $T$、基础步长 $\\eta_0$ 和用于学习率调度 $\\eta_t = \\eta_0/(1+\\lambda t)$ 的步长衰减 $\\lambda$。\n\n情况 $1$（可分，混合教育/金融/人口统计学特征）：\n- 特征（列）是：受教育年限、家庭收入（千美元）、学生贷款余额（千美元）、信用利用率（无单位分数）、年龄（岁）。\n- 训练矩阵 $X^{(1)}$（形状 $6 \\times 5$）：\n  [\n  [$16$, $120$, $10$, $0.1$, $30$],\n  [$16$, $40$, $50$, $0.8$, $24$],\n  [$14$, $60$, $20$, $0.3$, $28$],\n  [$12$, $35$, $45$, $0.9$, $22$],\n  [$18$, $150$, $20$, $0.1$, $33$],\n  [$12$, $25$, $60$, $0.95$, $21$]\n  ]\n- 标签 $y^{(1)}$：[$-1$, $+1$, $-1$, $+1$, $-1$, $+1$]\n- 测试向量 $x_\\star^{(1)}$：[$15$, $55$, $35$, $0.6$, $25$]\n- 超参数：$C = 10.0$, $T = 2000$, $\\eta_0 = 0.5$, $\\lambda = 10^{-3}$。\n\n情况 $2$（不可分，重叠的金融/教育指标）：\n- 特征（列）是：家庭收入（千美元）、学生贷款余额（千美元）、平均绩点（无单位）、研究生学位指示符（二进制 $0$ 或 $1$）。\n- 训练矩阵 $X^{(2)}$（形状 $8 \\times 4$）：\n  [\n  [$70$, $30$, $3.0$, $0$],\n  [$50$, $45$, $2.6$, $0$],\n  [$65$, $40$, $3.2$, $1$],\n  [$55$, $50$, $3.0$, $0$],\n  [$80$, $20$, $3.8$, $1$],\n  [$60$, $35$, $2.8$, $0$],\n  [$75$, $25$, $3.1$, $1$],\n  [$58$, $42$, $3.4$, $0$]\n  ]\n- 标签 $y^{(2)}$：[$-1$, $+1$, $-1$, $+1$, $-1$, $+1$, $-1$, $+1$]\n- 测试向量 $x_\\star^{(2)}$：[$62$, $38$, $3.0$, $0$]\n- 超参数：$C = 5.0$, $T = 2500$, $\\eta_0 = 0.4$, $\\lambda = 5 \\times 10^{-4}$。\n\n情况 $3$（特征缩放边缘案例，异构的金融/人口统计学尺度）：\n- 特征（列）是：家庭收入（美元）、年龄（岁）、受抚养人数、FICO信用评分、毕业后月数。\n- 训练矩阵 $X^{(3)}$（形状 $6 \\times 5$）：\n  [\n  [$120000$, $34$, $0$, $780$, $12$],\n  [$45000$, $23$, $1$, $620$, $6$],\n  [$80000$, $29$, $2$, $700$, $24$],\n  [$35000$, $25$, $0$, $600$, $3$],\n  [$150000$, $40$, $3$, $800$, $60$],\n  [$40000$, $27$, $2$, $580$, $8$]\n  ]\n- 标签 $y^{(3)}$：[$-1$, $+1$, $-1$, $+1$, $-1$, $+1$]\n- 测试向量 $x_\\star^{(3)}$：[$65000$, $26$, $1$, $680$, $10$]\n- 超参数：$C = 8.0$, $T = 3000$, $\\eta_0 = 0.3$, $\\lambda = 10^{-4}$。\n\n算法要求：\n- 在铰链式（hinge-type）基于间隔的损失下，对正则化经验风险实施次梯度下降，更新权重向量和截距。\n- 对于迭代 $t \\in \\{0,1,\\dots,T-1\\}$，使用指定的学习率调度 $\\eta_t = \\eta_0/(1+\\lambda t)$。\n- 将权重向量和截距初始化为零。\n- 仅使用训练数据进行特征标准化，并对测试向量应用相同的参数。\n\n最终输出格式：\n- 对于每种情况 $k \\in \\{1,2,3\\}$，输出对应测试家庭的预测标签 $\\hat{y}_\\star^{(k)} \\in \\{-1,+1\\}$。\n- 您的程序应生成单行输出，其中包含这三个预测值，形式为逗号分隔的列表，并用方括号括起来，例如：“[$\\hat{y}_\\star^{(1)}$,$\\hat{y}_\\star^{(2)}$,$\\hat{y}_\\star^{(3)}$]”。打印的输出必须只有一行，没有附加文本。", "solution": "用户要求从首要原则出发设计并实现一个线性软间隔支持向量机（SVM）分类器。该问题是在计算经济学和金融学的背景下提出的，具体是预测学生贷款违约。我的分析分两个阶段进行：首先，对问题陈述进行严格验证；其次，基于已建立的科学原则开发解决方案。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n\n- **模型类型**：带截距的线性软间隔支持向量机。\n- **任务**：家庭学生贷款违约的二元分类（$y \\in \\{-1, +1\\}$）。\n- **理论基础**：基于线性决策函数的经验风险最小化（ERM）。\n- **决策函数**：$f(x) = w^\\top x + b$，将特征 $x \\in \\mathbb{R}^d$ 映射到一个分数。\n- **目标函数组成部分**：对权重向量 $w$ 的 Tikhonov（岭）正则化，以及一个凸的基于间隔的损失函数（铰链损失）。\n- **优化算法**：次梯度下降。\n- **预处理**：基于训练数据，对每个特征进行 $z$-score 标准化。标准差为零的情况通过除以 $1$ 来处理。\n- **初始化**：权重向量 $w$ 和截距 $b$ 初始化为零。\n- **学习率调度**：对于迭代 $t \\in \\{0, 1, \\dots, T-1\\}$，$\\eta_t = \\eta_0 / (1 + \\lambda t)$。\n- **预测规则**：$\\hat{y}_\\star = \\text{sign}(w^\\top x_\\star + b)$。标签 $+1$ 表示违约，$-1$ 表示不违约。\n- **测试案例**：\n    - **案例 1**：\n        - $X^{(1)}$（训练数据，$6 \\times 5$ 矩阵），$y^{(1)}$（训练标签，长度 $6$），$x_\\star^{(1)}$（测试向量，长度 $5$）。\n        - 超参数：$C = 10.0$, $T = 2000$, $\\eta_0 = 0.5$, $\\lambda = 10^{-3}$。\n    - **案例 2**：\n        - $X^{(2)}$（训练数据，$8 \\times 4$ 矩阵），$y^{(2)}$（训练标签，长度 $8$），$x_\\star^{(2)}$（测试向量，长度 $4$）。\n        - 超参数：$C = 5.0$, $T = 2500$, $\\eta_0 = 0.4$, $\\lambda = 5 \\times 10^{-4}$。\n    - **案例 3**：\n        - $X^{(3)}$（训练数据，$6 \\times 5$ 矩阵），$y^{(3)}$（训练标签，长度 $6$），$x_\\star^{(3)}$（测试向量，长度 $5$）。\n        - 超参数：$C = 8.0$, $T = 3000$, $\\eta_0 = 0.3$, $\\lambda = 10^{-4}$。\n- **输出格式**：单行打印一个由方括号括起来的、逗号分隔的三个预测标签列表。\n\n**步骤2：使用提取的已知条件进行验证**\n\n对该问题进行了严格的科学和结构验证。\n\n- **科学基础**：该问题牢固地植根于统计学习理论和凸优化的既定原则。经验风险最小化、Tikhonov 正则化、铰链损失和次梯度下降是现代机器学习的经典组成部分。它们在信用风险建模中的应用是计算金融学中一个标准且易于理解的任务。该问题遵循基本的数学和统计逻辑。\n- **适定性**：该问题是适定的。目标函数是一个严格凸函数（$\\frac{1}{2}\\|w\\|^2$）和一个凸函数（铰链损失之和）的和，因此是严格凸的。这保证了存在唯一的最小化子 $(w^*, b^*)$。次梯度下降算法，当学习率调度满足条件 $\\sum_{t=0}^{\\infty} \\eta_t = \\infty$ 和 $\\sum_{t=0}^{\\infty} \\eta_t^2  \\infty$ 时（指定的调度 $\\eta_t \\propto 1/t$ 满足这些条件），保证会收敛到这个唯一的最小值。\n- **客观性**：问题以精确、无偏见和形式化的数学语言陈述，没有任何主观或推测性内容。\n- **完整性与一致性**：所有必要的组成部分都已提供：数据集、标签、超参数、算法规范（初始化、学习率）和预处理步骤。内部没有矛盾。超参数 $C$ 的使用与软间隔 SVM 的标准原始形式一致。\n- **现实性**：所提供的数据集虽然规模较小，但代表了家庭合理的经济和人口特征。尺度的异质性（例如，案例3：以美元计的收入与受抚养人数）正确地说明了特征标准化的必要性。\n\n**步骤3：结论与行动**\n\n问题陈述是有效的。它在科学上是合理的、适定的、客观的、完整的且内部一致的。它代表了机器学习中一个标准的、非平凡的问题。我现在将着手提供一个有原则的解决方案。\n\n### 解决方案推导\n\n目标是从首要原则出发，推导并实现一个线性软间隔 SVM 分类器。\n\n**1. 模型与目标函数**\n\n我们从正则化经验风险最小化的原则开始。分类器是输入特征 $x \\in \\mathbb{R}^d$ 的一个线性函数：\n$$\nf(x; w, b) = w^\\top x + b\n$$\n其中 $w \\in \\mathbb{R}^d$ 是权重向量，$b \\in \\mathbb{R}$ 是截距。预测类别由 $\\hat{y} = \\text{sign}(f(x; w, b))$ 给出。我们采用惯例，即当 $z \\ge 0$ 时 $\\text{sign}(z) = +1$，当 $z  0$ 时为 $-1$。\n\n目标是找到参数 $(w, b)$，以最小化训练数据 $\\{(x_i, y_i)\\}_{i=1}^n$ 上的经验损失和一个控制模型复杂度的正则化项的组合。对于 SVM，标准选择是铰链损失，它惩罚未能达到至少为 $1$ 的间隔的预测：\n$$\nL_{\\text{hinge}}(y_i, f(x_i)) = \\max(0, 1 - y_i f(x_i)) = \\max(0, 1 - y_i(w^\\top x_i + b))\n$$\n该损失是非凸的 $0$-$1$ 分类误差的一个凸上界。\n\n为防止过拟合和提高泛化能力，我们对权重向量添加一个 Tikhonov（$\\ell_2$）正则化惩罚项，$\\frac{1}{2}\\|w\\|^2 = \\frac{1}{2} w^\\top w$。截距 $b$ 通常不被正则化。\n\n结合这些部分，我们得到软间隔 SVM 的原始目标函数：\n$$\nJ(w, b) = \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n \\max(0, 1 - y_i(w^\\top x_i + b))\n$$\n超参数 $C  0$ 控制了权衡：小的 $C$ 倾向于更大的间隔，但会容忍一些间隔违例；大的 $C$ 则更严厉地惩罚违例，导致间隔变小。最小化这个目标函数是一个凸优化问题。\n\n**2. 通过次梯度下降进行优化**\n\n铰链损失项使得目标函数 $J(w, b)$ 在 $y_i(w^\\top x_i + b) = 1$ 的地方不可微。因此，我们使用次梯度下降。次梯度是梯度对于不可微凸函数的推广。\n\n$J(w, b)$ 关于 $w$ 和 $b$ 的次梯度是可微正则化项的梯度和不可微铰链损失项的次梯度之和。对于单个数据点 $(x_i, y_i)$，铰链损失的次梯度是：\n$$\n\\partial \\max(0, 1 - y_i(w^\\top x_i + b)) =\n\\begin{cases}\n    \\{ \\mathbf{0} \\}  \\text{if } y_i(w^\\top x_i + b) > 1 \\\\\n    \\{ -y_i \\begin{pmatrix} x_i \\\\ 1 \\end{pmatrix} \\}  \\text{if } y_i(w^\\top x_i + b)  1 \\\\\n    \\{ -\\alpha y_i \\begin{pmatrix} x_i \\\\ 1 \\end{pmatrix} \\mid \\alpha \\in [0, 1] \\} \\text{if } y_i(w^\\top x_i + b) = 1\n\\end{cases}\n$$\n在算法实现中，我们可以从次梯度集合中选择任何一个元素。一个常规的选择是，如果 $y_i(w^\\top x_i + b) \\le 1$，则将次梯度取为 $-y_i \\begin{pmatrix} x_i \\\\ 1 \\end{pmatrix}$，否则取为 $\\mathbf{0}$。令 $\\mathcal{S}_t = \\{i \\mid y_i(w^{(t)\\top} x_i + b^{(t)}) \\le 1\\}$ 为在第 $t$ 次迭代时违反或位于间隔边界上的训练样本索引集合。\n\n完整目标函数 $J(w, b)$ 的次梯度是：\n$$\ng_w = \\frac{\\partial J}{\\partial w} = w - C \\sum_{i \\in \\mathcal{S}_t} y_i x_i\n$$\n$$\ng_b = \\frac{\\partial J}{\\partial b} = -C \\sum_{i \\in \\mathcal{S}_t} y_i\n$$\n第 $t$ 次迭代的次梯度下降更新规则是：\n$$\nw^{(t+1)} \\leftarrow w^{(t)} - \\eta_t g_w\n$$\n$$\nb^{(t+1)} \\leftarrow b^{(t)} - \\eta_t g_b\n$$\n学习率 $\\eta_t$ 由调度 $\\eta_t = \\eta_0 / (1 + \\lambda t)$ 给出。\n\n**3. 特征标准化**\n\n所提供的特征具有迥然不同的尺度（例如，以千美元计的收入与无单位的比率）。SVM 对特征尺度很敏感，因为目标函数涉及点积和范数。为确保所有特征都能适当地贡献，我们使用 $z$-score 变换对它们进行标准化。对于每个特征 $j \\in \\{1, \\dots, d\\}$，我们从训练数据中计算其均值 $\\mu_j$ 和标准差 $\\sigma_j$。\n$$\n\\mu_j = \\frac{1}{n} \\sum_{i=1}^n x_{ij} \\quad , \\quad \\sigma_j^2 = \\frac{1}{n} \\sum_{i=1}^n (x_{ij} - \\mu_j)^2\n$$\n标准化的特征 $x'_{ij}$ 于是为：\n$$\nx'_{ij} = \\frac{x_{ij} - \\mu_j}{\\sigma_j'}\n$$\n其中，如果 $\\sigma_j  0$，则 $\\sigma_j' = \\sigma_j$；如果 $\\sigma_j = 0$，则 $\\sigma_j' = 1$。在预测之前，必须使用从训练集导出的参数 $(\\mu_j, \\sigma_j')$ 对测试向量 $x_\\star$ 应用相同的变换。\n\n这个过程产生了一个用于训练 SVM 分类器和进行预测的有原则且完整的算法，我们现在来实现它。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the SVM classification problem for all test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"X_train\": np.array([\n                [16.0, 120.0, 10.0, 0.1, 30.0],\n                [16.0, 40.0, 50.0, 0.8, 24.0],\n                [14.0, 60.0, 20.0, 0.3, 28.0],\n                [12.0, 35.0, 45.0, 0.9, 22.0],\n                [18.0, 150.0, 20.0, 0.1, 33.0],\n                [12.0, 25.0, 60.0, 0.95, 21.0]\n            ]),\n            \"y_train\": np.array([-1, 1, -1, 1, -1, 1]),\n            \"x_test\": np.array([15.0, 55.0, 35.0, 0.6, 25.0]),\n            \"C\": 10.0,\n            \"T\": 2000,\n            \"eta0\": 0.5,\n            \"lmbda\": 1e-3\n        },\n        {\n            \"X_train\": np.array([\n                [70.0, 30.0, 3.0, 0.0],\n                [50.0, 45.0, 2.6, 0.0],\n                [65.0, 40.0, 3.2, 1.0],\n                [55.0, 50.0, 3.0, 0.0],\n                [80.0, 20.0, 3.8, 1.0],\n                [60.0, 35.0, 2.8, 0.0],\n                [75.0, 25.0, 3.1, 1.0],\n                [58.0, 42.0, 3.4, 0.0]\n            ]),\n            \"y_train\": np.array([-1, 1, -1, 1, -1, 1, -1, 1]),\n            \"x_test\": np.array([62.0, 38.0, 3.0, 0.0]),\n            \"C\": 5.0,\n            \"T\": 2500,\n            \"eta0\": 0.4,\n            \"lmbda\": 5e-4\n        },\n        {\n            \"X_train\": np.array([\n                [120000.0, 34.0, 0.0, 780.0, 12.0],\n                [45000.0, 23.0, 1.0, 620.0, 6.0],\n                [80000.0, 29.0, 2.0, 700.0, 24.0],\n                [35000.0, 25.0, 0.0, 600.0, 3.0],\n                [150000.0, 40.0, 3.0, 800.0, 60.0],\n                [40000.0, 27.0, 2.0, 580.0, 8.0]\n            ]),\n            \"y_train\": np.array([-1, 1, -1, 1, -1, 1]),\n            \"x_test\": np.array([65000.0, 26.0, 1.0, 680.0, 10.0]),\n            \"C\": 8.0,\n            \"T\": 3000,\n            \"eta0\": 0.3,\n            \"lmbda\": 1e-4\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        X_train, y_train, x_test = case[\"X_train\"], case[\"y_train\"], case[\"x_test\"]\n        C, T, eta0, lmbda = case[\"C\"], case[\"T\"], case[\"eta0\"], case[\"lmbda\"]\n\n        # 1. Feature Standardization\n        # Using ddof=0 for population standard deviation, consistent with the formula.\n        train_mean = np.mean(X_train, axis=0)\n        train_std = np.std(X_train, axis=0, ddof=0)\n        train_std[train_std == 0] = 1.0  # Avoid division by zero\n\n        X_train_std = (X_train - train_mean) / train_std\n        x_test_std = (x_test - train_mean) / train_std\n\n        n_samples, n_features = X_train.shape\n\n        # 2. Subgradient Descent for SVM\n        # Initialize parameters\n        w = np.zeros(n_features)\n        b = 0.0\n\n        for t in range(T):\n            # Calculate learning rate\n            eta_t = eta0 / (1.0 + lmbda * t)\n\n            # Calculate scores and identify margin violators\n            scores = y_train * (X_train_std @ w + b)\n            violators_mask = scores = 1.0\n            \n            # Compute subgradients\n            # The sum is over samples that violate the margin condition (or are on the boundary)\n            if np.any(violators_mask):\n                dw = w - C * np.sum(y_train[violators_mask, np.newaxis] * X_train_std[violators_mask], axis=0)\n                db = -C * np.sum(y_train[violators_mask])\n            else:\n                dw = w\n                db = 0.0\n\n            # Update parameters\n            w -= eta_t * dw\n            b -= eta_t * db\n\n        # 3. Prediction\n        test_score = x_test_std @ w + b\n        prediction = 1 if test_score >= 0 else -1\n        results.append(prediction)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2435452"}, {"introduction": "在掌握了基于次梯度下降的SVM原始问题求解之后，让我们探索一种更强大、更高效的优化技术：内点法（Interior Point Method）。此练习 [@problem_id:3242644] 要求你为一个二次规划（QP）问题开发一个简单的内点法求解器，并将其应用于求解SVM的对偶问题。通过实现这个更高级的算法，你将深入了解SVM训练背后的约束优化数学原理，并理解为何对偶形式对于引入核技巧至关重要，这为你处理非线性可分数据奠定了坚实的基础。", "problem": "您需要实现一个完整的、可运行的程序，该程序使用内点法 (Interior Point Method, IPM) 构建并求解一系列二次规划 (Quadratic Programming, QP) 问题，并将其应用于经典支持向量机 (Support Vector Machine, SVM) 二分类问题的对偶形式（使用线性核）。您的实现必须是自包含的，并按如下规定产生单行输出。\n\n核心的优化问题是求解软间隔线性核 SVM 的对偶问题。给定训练数据矩阵 $X \\in \\mathbb{R}^{n \\times d}$ 和标签 $y \\in \\{-1,+1\\}^n$，其对偶问题是找到一个 $ \\alpha \\in \\mathbb{R}^n $，在满足一个等式约束和箱式约束的条件下，最小化一个严格凸的二次目标函数。定义 Gram 矩阵 $K \\in \\mathbb{R}^{n \\times n}$ 为 $K_{ij} = x_i^\\top x_j$，标签矩阵 $Y = \\operatorname{diag}(y)$，以及二次矩阵 $Q = Y K Y$。该对偶 QP 问题为：\n$$\n\\begin{aligned}\n\\min_{\\alpha \\in \\mathbb{R}^n}\\quad  \\tfrac{1}{2}\\,\\alpha^\\top Q\\,\\alpha - \\mathbf{1}^\\top \\alpha \\\\\n\\text{subject to}\\quad  y^\\top \\alpha = 0, \\\\\n 0 \\le \\alpha \\le C,\n\\end{aligned}\n$$\n其中 $C \\in \\mathbb{R}$ 是一个给定的惩罚参数，满足 $C  0$，而 $\\mathbf{1} \\in \\mathbb{R}^n$ 是全为 1 的向量。您的求解器必须为形如 $l \\le x \\le u$ 的简单边界约束的等式约束 QP 实现一个基于障碍的内点法。\n\n从凸优化、拉格朗日最优性条件和针对不等式约束的对数障碍法的基本定义出发，实现一个可行起始点的障碍法。对于一个障碍参数 $\\mu  0$，该方法最小化障碍增广目标函数：\n$$\n\\phi_\\mu(\\alpha) = \\tfrac{1}{2}\\,\\alpha^\\top Q\\,\\alpha - \\mathbf{1}^\\top \\alpha - \\mu \\sum_{i=1}^n \\log(\\alpha_i - 0) - \\mu \\sum_{i=1}^n \\log(C - \\alpha_i)\n$$\n同时满足等式约束 $y^\\top \\alpha = 0$。在每个障碍参数值下，通过求解 Karush–Kuhn–Tucker (KKT) 系统来计算牛顿步，使用带等式约束的牛顿法来计算搜索方向。同时使用回溯线搜索，在确保障碍目标函数充分下降的同时，保持严格可行性 $0  \\alpha_i  C$ 对所有 $i \\in \\{1,\\dots,n\\}$ 成立。通过几何方式减小 $\\mu$ 直至收敛，并根据需要进行正则化以保持数值稳定性。您的算法必须在您的解决方案中从第一性原理出发进行论证。\n\n在求解出 $\\alpha$ 之后，按如下方式恢复线性核 SVM 的原始分隔超平面参数。计算\n$$\nw = \\sum_{i=1}^n \\alpha_i y_i x_i \\in \\mathbb{R}^d,\n$$\n并使用标准的互补松弛关系来估计偏置 $b \\in \\mathbb{R}$。如果存在索引 $i$ 使得 $0  \\alpha_i  C$（在一个小的数值容差范围内严格位于界内），则计算\n$$\nb = \\operatorname{mean}_{i \\,:\\, 0  \\alpha_i  C} \\left( y_i - w^\\top x_i \\right).\n$$\n如果不存在这样的 $i$，则使用基于类别边界的稳健后备方法：\n$$\nb = -\\tfrac{1}{2}\\left( \\max_{i \\,:\\, y_i=-1} w^\\top x_i + \\min_{i \\,:\\, y_i=+1} w^\\top x_i \\right).\n$$\n\n对于下面的每个测试用例，使用决策规则 $\\hat{y} = \\operatorname{sign}(w^\\top x + b)$（标签在 $\\{-1,+1\\}$ 中）计算样本内分类准确率，定义为被正确分类的训练样本的比例。将每个准确率表示为 $[0,1]$ 区间内四舍五入到六位小数的实数。\n\n测试套件。请严格使用以下三个测试用例：\n\n- 案例 A（线性可分，平衡）：\n  - $X = \\begin{bmatrix}\n  2  2 \\\\\n  2  3 \\\\\n  3  2 \\\\\n  -2  -2 \\\\\n  -2  -3 \\\\\n  -3  -2\n  \\end{bmatrix}$,\n  $y = \\begin{bmatrix} +1 \\\\ +1 \\\\ +1 \\\\ -1 \\\\ -1 \\\\ -1 \\end{bmatrix}$,\n  $C = 100.0$。\n- 案例 B（部分重叠，平衡）：\n  - $X = \\begin{bmatrix}\n  2  2 \\\\\n  2  0 \\\\\n  3  1 \\\\\n  3  2 \\\\\n  0  0 \\\\\n  1  0 \\\\\n  0  1 \\\\\n  1  1.5\n  \\end{bmatrix}$,\n  $y = \\begin{bmatrix} +1 \\\\ +1 \\\\ +1 \\\\ +1 \\\\ -1 \\\\ -1 \\\\ -1 \\\\ -1 \\end{bmatrix}$,\n  $C = 1.0$。\n- 案例 C（线性可分，不平衡）：\n  - $X = \\begin{bmatrix}\n  2  2 \\\\\n  2  3 \\\\\n  3  2 \\\\\n  -2  -2\n  \\end{bmatrix}$,\n  $y = \\begin{bmatrix} +1 \\\\ +1 \\\\ +1 \\\\ -1 \\end{bmatrix}$,\n  $C = 10.0$。\n\n最终输出格式。您的程序应产生单行输出，其中包含三个案例的结果，以逗号分隔的实数列表形式包含在一对方括号内，顺序为 A, B, C。每个数字必须四舍五入到六位小数。例如，输出必须类似于\n$$\n[\\text{acc}_A,\\text{acc}_B,\\text{acc}_C],\n$$\n其中 $\\text{acc}_A$、$\\text{acc}_B$ 和 $\\text{acc}_C$ 均为格式化为六位小数的实数。不应打印任何额外的文本或行。\n\n单位和角度单位不适用于此问题。\n\n您的实现必须是完整的，并且可以直接运行，无需任何用户输入、外部文件或网络访问。唯一允许使用的外部库是 Numerical Python (NumPy)。", "solution": "我们从凸优化的基本定义和将支持向量机 (SVM) 分类器构建为凸规划的解开始。对于使用线性核的二元分类，原始软间隔 SVM 旨在寻找一个由 $w \\in \\mathbb{R}^d$ 和 $b \\in \\mathbb{R}$ 定义的分隔超平面，该超平面在最大化间隔和 Hinge 损失之间取得平衡。相应的对偶问题是一个具有一个线性等式约束和简单边界约束的严格凸二次规划 (QP)。这种对偶形式对于核方法尤其方便，但这里我们专注于线性核。\n\n给定 $X \\in \\mathbb{R}^{n \\times d}$ 和标签 $y \\in \\{-1,+1\\}^n$，定义 Gram 矩阵 $K \\in \\mathbb{R}^{n \\times n}$ 为 $K_{ij} = x_i^\\top x_j$，以及 $Q = Y K Y$，其中 $Y = \\operatorname{diag}(y)$。带有惩罚参数 $C \\in \\mathbb{R}, C  0$ 的软间隔对偶问题是\n$$\n\\begin{aligned}\n\\min_{\\alpha \\in \\mathbb{R}^n} \\quad  f(\\alpha) = \\tfrac{1}{2} \\alpha^\\top Q \\alpha - \\mathbf{1}^\\top \\alpha \\\\\n\\text{subject to} \\quad  y^\\top \\alpha = 0, \\\\\n 0 \\le \\alpha \\le C,\n\\end{aligned}\n$$\n其中 $\\mathbf{1} \\in \\mathbb{R}^n$ 是全一向量。原始变量中的决策函数可恢复为 $w = \\sum_{i=1}^n \\alpha_i y_i x_i$ 以及一个与 Karush–Kuhn–Tucker (KKT) 条件一致的偏置 $b$。\n\n为了求解该 QP，我们使用基于对数障碍的内点法 (IPM)。不等式约束 $0 \\le \\alpha_i \\le C$ 通过向目标函数增加一个由 $\\mu  0$ 参数化的障碍项来强制执行：\n$$\n\\phi_\\mu(\\alpha) = f(\\alpha) - \\mu \\sum_{i=1}^n \\log(\\alpha_i) - \\mu \\sum_{i=1}^n \\log(C - \\alpha_i),\n$$\n同时满足等式约束 $y^\\top \\alpha = 0$。对于每个固定的 $\\mu$，我们使用牛顿法在流形 $y^\\top \\alpha = 0$ 上求解 $\\phi_\\mu(\\alpha)$ 的等式约束最小化问题。当 $\\mu \\to 0^+$ 时，解会追踪中心路径并收敛到原始 QP 的约束最小化器。\n\n我们从第一性原理推导牛顿步。障碍增广目标的梯度是\n$$\n\\nabla \\phi_\\mu(\\alpha) = Q \\alpha - \\mathbf{1} - \\mu \\left[ \\frac{1}{\\alpha_1}, \\dots, \\frac{1}{\\alpha_n} \\right]^\\top + \\mu \\left[ \\frac{1}{C - \\alpha_1}, \\dots, \\frac{1}{C - \\alpha_n} \\right]^\\top.\n$$\n其 Hessian 矩阵是\n$$\n\\nabla^2 \\phi_\\mu(\\alpha) = Q + \\operatorname{diag}\\left( \\frac{\\mu}{\\alpha_1^2} + \\frac{\\mu}{(C - \\alpha_1)^2}, \\dots, \\frac{\\mu}{\\alpha_n^2} + \\frac{\\mu}{(C - \\alpha_n)^2} \\right).\n$$\n我们通过拉格朗日乘子来强制执行等式约束。对于当前迭代的 $\\alpha$ 和拉格朗日乘子 $\\lambda \\in \\mathbb{R}$，牛顿步 $(\\Delta \\alpha, \\Delta \\lambda)$ 求解线性化的 KKT 系统\n$$\n\\begin{bmatrix}\nW  y \\\\\ny^\\top  0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\Delta \\alpha \\\\\n\\Delta \\lambda\n\\end{bmatrix}\n=\n-\n\\begin{bmatrix}\ng \\\\\nr_p\n\\end{bmatrix},\n$$\n其中 $g = \\nabla \\phi_\\mu(\\alpha)$，$W = \\nabla^2 \\phi_\\mu(\\alpha)$，而 $r_p = y^\\top \\alpha - 0$ 是原始等式残差。该系统源于 $\\phi_\\mu(\\alpha)$ 的二阶泰勒展开和等式约束的线性化。它可以通过舒尔补高效求解。求解 $W \\Delta \\alpha + y \\Delta \\lambda = -g$ 和 $y^\\top \\Delta \\alpha = -r_p$，我们得到舒尔补方程\n$$\n(y^\\top W^{-1} y)\\, \\Delta \\lambda = -\\left( r_p + y^\\top W^{-1} g \\right),\n$$\n然后\n$$\n\\Delta \\alpha = - W^{-1} (g + y \\Delta \\lambda).\n$$\n因为对于 $\\mu  0$，由于障碍项的曲率，$W$ 是严格正定的（即使 $Q$ 只是半正定的），所以线性求解存在并且在温和的正则化下是数值稳定的。\n\n步长的选择是为了保持不等式约束的严格可行性，并确保障碍目标函数的充分下降。给定一个候选方向 $\\Delta \\alpha$，保持严格内部可行性的最大步长按分量计算为\n$$\n\\alpha_{\\max} = \\min \\left\\{ 1,\\, 0.99 \\cdot \\min_{i:\\,\\Delta \\alpha_i  0} \\frac{\\alpha_i}{-\\Delta \\alpha_i},\\, 0.99 \\cdot \\min_{i:\\,\\Delta \\alpha_i  0} \\frac{C - \\alpha_i}{\\Delta \\alpha_i} \\right\\}.\n$$\n然后我们应用带有因子 $0  \\beta  1$ 的回溯法和 Armijo 条件，使用方向导数 $g^\\top \\Delta \\alpha$ 来保证 $\\phi_\\mu(\\alpha)$ 的下降，同时保持可行性。\n\n初始化必须满足严格可行性和等式约束。设 $P = \\{ i \\,:\\, y_i = +1 \\}$ 和 $N = \\{ i \\,:\\, y_i = -1 \\}$。选择标量 $s, t \\in (0,1)$ 使得 $s\\,|P| = t\\,|N|$。一个具体的选择，无论类别是否不平衡，都能保证严格的内部性，是\n$$\ns = \\min\\!\\left(0.5,\\, 0.5 \\cdot \\frac{|N|}{|P|}\\right), \\quad t = s \\cdot \\frac{|P|}{|N|}.\n$$\n然后对 $i \\in P$ 设 $\\alpha_i = C s$，对 $j \\in N$ 设 $\\alpha_j = C t$。这个选择对所有 $i$ 满足 $0  \\alpha_i  C$ 并且 $y^\\top \\alpha = 0$。\n\n外部障碍循环以几何方式减小 $\\mu$，例如 $\\mu \\leftarrow \\sigma \\mu$（其中 $\\sigma \\in (0,1)$），直到 $\\mu$ 足够小；对于每个 $\\mu$，内部等式约束牛顿法会迭代，直到 KKT 残差 $(g, r_p)$ 的范数低于一个容差。可以对 $Q$ 使用温和的对角正则化来改善小问题的数值稳定性。\n\n收敛后，恢复原始参数 $w = \\sum_{i=1}^n \\alpha_i y_i x_i$。对于 $b$，使用 KKT 互补松弛条件。如果存在索引 $i$ 严格在界限 $\\varepsilon  \\alpha_i  C - \\varepsilon$ 内（在一个小的容差范围内），那么对于这样的 $i$ 我们有 $y_i (w^\\top x_i + b) \\approx 1$，这意味着 $b \\approx y_i - w^\\top x_i$。对所有可用的此类索引取平均值，可以得到一个稳健的估计：\n$$\nb = \\operatorname{mean}_{i \\,:\\, \\varepsilon  \\alpha_i  C - \\varepsilon} \\left( y_i - w^\\top x_i \\right).\n$$\n如果不存在这样的索引，一个稳健的后备方法是选择 $b$ 位于两类最近的投影边界的中点：\n$$\nb = -\\tfrac{1}{2}\\left( \\max_{i \\,:\\, y_i=-1} w^\\top x_i + \\min_{i \\,:\\, y_i=+1} w^\\top x_i \\right).\n$$\n\n最后，对每个测试用例计算样本内准确率 $\\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\{ \\operatorname{sign}(w^\\top x_i + b) = y_i \\}$。程序必须输出单行：一个包含三个在 $[0,1]$ 区间内的实数的列表，每个数都四舍五入到六位小数，分别对应于指定的三个测试用例 A、B、C。\n\n该方法将凸性、KKT 条件和对数障碍原理的核心定义整合到一个可实现的算法中：带障碍增广 Hessian 矩阵的等式约束牛顿步确保在可行域内部下降，而舒尔补约减则利用了少量等式约束的结构。所选的初始化从第一性原理保证了严格可行性和等式满足，而 $(w,b)$ 的恢复则使用了 SVM 公式中固有的互补松弛关系。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef interior_point_qp(H, c, A, b, l, u, *,\n                      mu_start=1.0, mu_tol=1e-8, mu_decay=0.2,\n                      newton_tol=1e-8, max_newton_iter=60,\n                      backtrack_beta=0.5, armijo_c1=1e-4, ridge=1e-10):\n    \"\"\"\n    Solve: minimize 0.5 x^T H x + c^T x\n           subject to A x = b, l = x = u\n    using a log-barrier interior point method with equality-constrained Newton steps.\n\n    Parameters:\n        H: (n,n) symmetric positive semidefinite\n        c: (n,)\n        A: (m,n) equality constraints\n        b: (m,)\n        l, u: (n,) lower and upper bounds with l = u elementwise\n    Returns:\n        x: (n,) approximate solution\n    \"\"\"\n    n = H.shape[0]\n    # Symmetrize H and add small ridge for numerical stability\n    H = 0.5 * (H + H.T) + ridge * np.eye(n)\n\n    # Build strictly feasible starting point satisfying A x = b and l  x  u\n    # Here A has shape (m, n). We'll construct x0 using a problem-specific strategy when m=1,\n    # otherwise project a central point to the equality manifold with line search for feasibility.\n    m = A.shape[0]\n    # Start from midpoint\n    x = 0.5 * (l + u)\n\n    # Try to adjust to equality feasibility while staying interior\n    # Solve min ||x - x_mid|| s.t. A x = b => x = x_mid + A^T (A A^T)^{-1} (b - A x_mid)\n    # Then shrink towards midpoint if needed to maintain strict interior.\n    try:\n        At = A\n        M = At @ At.T\n        # Regularize M if near singular\n        if M.ndim == 0:\n            M = np.array([[M + 1e-12]])\n        else:\n            M = M + 1e-12 * np.eye(M.shape[0])\n        adjustment = At.T @ np.linalg.solve(M, (b - A @ x))\n        x_candidate = x + adjustment\n        # Ensure strict interior via convex combination with midpoint\n        t = 0.0\n        # If x_candidate violates bounds, pull it towards midpoint\n        while True:\n            xc = (1 - t) * x_candidate + t * x  # t increases towards midpoint\n            if np.all(xc > l) and np.all(xc  u) and np.allclose(A @ xc, b, atol=1e-10):\n                x = xc\n                break\n            t = min(1.0, t + 0.05)\n            if t >= 1.0:\n                # Fallback: stay at midpoint and enforce equality numerically later\n                x = (x + 0.0)\n                break\n    except np.linalg.LinAlgError:\n        pass\n\n    # Ensure strict interiority\n    eps = 1e-6\n    x = np.minimum(np.maximum(x, l + eps * (u - l)), u - eps * (u - l))\n\n    mu = mu_start\n\n    def barrier_grad_hess(x, mu):\n        # gradient g = H x + c - mu/(x-l) + mu/(u-x)\n        inv_xl = 1.0 / (x - l)\n        inv_ux = 1.0 / (u - x)\n        g = H @ x + c - mu * inv_xl + mu * inv_ux\n        # Hessian W = H + diag(mu/(x-l)^2 + mu/(u-x)^2)\n        diag_terms = mu * (inv_xl**2 + inv_ux**2)\n        W = H + np.diag(diag_terms)\n        return g, W\n\n    def phi(x, mu):\n        # barrier objective\n        if np.any(x = l) or np.any(x >= u):\n            return np.inf\n        return 0.5 * float(x.T @ H @ x) + float(c.T @ x) - mu * np.sum(np.log(x - l)) - mu * np.sum(np.log(u - x))\n\n    # Newton solve for each mu\n    while mu > mu_tol:\n        # Newton iterations\n        for _ in range(max_newton_iter):\n            g, W = barrier_grad_hess(x, mu)\n            r_p = A @ x - b  # primal residual\n            # Solve KKT system via Schur complement:\n            # W dx + A^T lam = -g\n            # A dx = -r_p\n            # Schur: (A W^{-1} A^T) lam = -(r_p + A W^{-1} g)\n            try:\n                Winv_A_T = np.linalg.solve(W, A.T)\n                S = A @ Winv_A_T  # (m,m)\n                rhs = -(r_p + A @ np.linalg.solve(W, g))\n                # Solve for lambda\n                lam = np.linalg.solve(S, rhs)\n                # Then dx\n                dx = -np.linalg.solve(W, g + A.T @ lam)\n            except np.linalg.LinAlgError:\n                # Add additional ridge and retry\n                W = W + 1e-8 * np.eye(n)\n                Winv_A_T = np.linalg.solve(W, A.T)\n                S = A @ Winv_A_T\n                rhs = -(r_p + A @ np.linalg.solve(W, g))\n                lam = np.linalg.solve(S, rhs)\n                dx = -np.linalg.solve(W, g + A.T @ lam)\n\n            # Check convergence of KKT residual\n            res_norm = np.sqrt(np.linalg.norm(g, 2)**2 + np.linalg.norm(r_p, 2)**2)\n            if res_norm  newton_tol:\n                break\n\n            # Compute maximum feasible step to remain strictly inside bounds\n            alpha = 1.0\n            with np.errstate(divide='ignore', invalid='ignore'):\n                idx_pos = dx > 0\n                if np.any(idx_pos):\n                    alpha = min(alpha, 0.99 * np.min((u[idx_pos] - x[idx_pos]) / dx[idx_pos]))\n                idx_neg = dx  0\n                if np.any(idx_neg):\n                    alpha = min(alpha, 0.99 * np.min((x[idx_neg] - l[idx_neg]) / (-dx[idx_neg])))\n            if not np.isfinite(alpha) or alpha = 0:\n                alpha = 0.5\n\n            # Backtracking line search for sufficient decrease\n            phi_x = phi(x, mu)\n            slope = g @ dx\n            t = alpha\n            while True:\n                x_new = x + t * dx\n                if np.any(x_new = l) or np.any(x_new >= u):\n                    t *= backtrack_beta\n                else:\n                    phi_new = phi(x_new, mu)\n                    if phi_new = phi_x + armijo_c1 * t * slope:\n                        break\n                    t *= backtrack_beta\n                if t  1e-16:\n                    break\n            x = x + t * dx\n\n        # Reduce barrier parameter\n        mu *= mu_decay\n\n    return x\n\ndef svm_linear_dual_ipm(X, y, C):\n    \"\"\"\n    Train a soft-margin linear SVM via its dual QP using an interior point method.\n    Returns: (w, b, alpha)\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float)\n    n, d = X.shape\n\n    # Build Q = Y K Y with K = X X^T\n    K = X @ X.T\n    Y = np.diag(y)\n    Q = Y @ K @ Y\n\n    # c = -1, A = y^T, b = 0, l = 0, u = C\n    c = -np.ones(n)\n    A = y.reshape(1, -1)\n    b = np.array([0.0])\n    l = np.zeros(n)\n    u = C * np.ones(n)\n\n    # Strictly feasible initializer satisfying A x = b and l  x  u\n    # Construct x0 with class-balanced interior alphas\n    pos_idx = np.where(y > 0)[0]\n    neg_idx = np.where(y  0)[0]\n    if len(pos_idx) == 0 or len(neg_idx) == 0:\n        raise ValueError(\"Both classes must be present.\")\n    s = min(0.5, 0.5 * len(neg_idx) / max(1, len(pos_idx)))\n    t = s * len(pos_idx) / len(neg_idx)\n    alpha0 = np.zeros(n)\n    alpha0[pos_idx] = C * s\n    alpha0[neg_idx] = C * t\n\n    # Solve QP via interior point\n    alpha = interior_point_qp(Q, c, A, b, l, u,\n                              mu_start=1.0, mu_tol=1e-9, mu_decay=0.2,\n                              newton_tol=1e-9, max_newton_iter=80,\n                              backtrack_beta=0.5, armijo_c1=1e-4, ridge=1e-10)\n\n    # Recover w\n    w = (alpha * y) @ X\n\n    # Recover b\n    tol = 1e-6\n    support_mask = (alpha > tol)  (alpha  C - tol)\n    scores = X @ w\n    if np.any(support_mask):\n        b_vals = y[support_mask] - scores[support_mask]\n        b = float(np.mean(b_vals))\n    else:\n        # Fallback: midpoint between class projections\n        scores_pos = scores[y > 0]\n        scores_neg = scores[y  0]\n        if len(scores_pos) == 0 or len(scores_neg) == 0:\n            b = 0.0\n        else:\n            b = -0.5 * (np.max(scores_neg) + np.min(scores_pos))\n\n    return w, b, alpha\n\ndef accuracy_score(y_true, y_pred):\n    y_true = np.asarray(y_true, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n    return float(np.mean(y_true == y_pred))\n\ndef run_case(X, y, C):\n    w, b, alpha = svm_linear_dual_ipm(X, y, C)\n    y_pred = np.sign(X @ w + b)\n    # Map zeros to +1 to avoid ambiguity\n    y_pred[y_pred == 0] = 1.0\n    return accuracy_score(y, y_pred)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    case_A_X = np.array([\n        [2.0, 2.0],\n        [2.0, 3.0],\n        [3.0, 2.0],\n        [-2.0, -2.0],\n        [-2.0, -3.0],\n        [-3.0, -2.0],\n    ], dtype=float)\n    case_A_y = np.array([+1.0, +1.0, +1.0, -1.0, -1.0, -1.0], dtype=float)\n    case_A_C = 100.0\n\n    case_B_X = np.array([\n        [2.0, 2.0],\n        [2.0, 0.0],\n        [3.0, 1.0],\n        [3.0, 2.0],\n        [0.0, 0.0],\n        [1.0, 0.0],\n        [0.0, 1.0],\n        [1.0, 1.5],\n    ], dtype=float)\n    case_B_y = np.array([+1.0, +1.0, +1.0, +1.0, -1.0, -1.0, -1.0, -1.0], dtype=float)\n    case_B_C = 1.0\n\n    case_C_X = np.array([\n        [2.0, 2.0],\n        [2.0, 3.0],\n        [3.0, 2.0],\n        [-2.0, -2.0],\n    ], dtype=float)\n    case_C_y = np.array([+1.0, +1.0, +1.0, -1.0], dtype=float)\n    case_C_C = 10.0\n\n    test_cases = [\n        (case_A_X, case_A_y, case_A_C),\n        (case_B_X, case_B_y, case_B_C),\n        (case_C_X, case_C_y, case_C_C),\n    ]\n\n    results = []\n    for X, y, C in test_cases:\n        acc = run_case(X, y, C)\n        results.append(f\"{acc:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3242644"}, {"introduction": "构建一个分类器只是任务的一半，同样重要的是如何严谨地评估其性能。在放射组学等高风险领域，简单地计算准确率或AUC值是不够的，我们还必须量化这些评估指标的不确定性。本练习 [@problem_id:4562067] 将指导你实现DeLong方法，这是一种无需参数假设的稳健统计技术，用于计算AUC的置信区间。通过这项实践，你将学会如何超越单一的性能数值，对分类器的判别能力及其可靠性进行严格的统计评估，这是解释模型并判断其临床意义的关键一步。", "problem": "给定从支持向量机（SVM）分类器（该分类器在影像组学特征上训练得到）预测的决策函数得分，以及真实的二元结果标签。您的任务是计算受试者工作特征曲线下面积（AUC）和使用 DeLong 方法计算的双侧置信区间。计算应从 AUC 的概率解释和成对比较的无偏估计量理论出发。然后，您必须根据指定的判别阈值，解释分类器的性能是否具有临床显著性。仅使用 AUC 作为概率的基本定义、无偏样本方差和正态分位数进行计算。除这些基础概念外，不要使用任何快捷方式或预打包的例程。\n\n定义和要求：\n\n- 令正类得分集合为 $\\mathbf{s}^+ = \\{s^+_1, s^+_2, \\dots, s^+_m\\}$，负类得分集合为 $\\mathbf{s}^- = \\{s^-_1, s^-_2, \\dots, s^-_n\\}$。AUC 定义为一个随机选择的正类得分超过一个随机选择的负类得分的概率。具体来说，令\n$$\n\\psi(a,b) = \n\\begin{cases}\n1,  \\text{if } a  b,\\\\\n0,  \\text{if } a  b,\\\\\n\\frac{1}{2},  \\text{if } a = b,\n\\end{cases}\n$$\n则经验 AUC 为\n$$\n\\widehat{\\theta} = \\frac{1}{mn} \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\psi\\!\\left(s^+_i, s^-_j\\right).\n$$\n- 使用基于 $U$-统计量理论的 DeLong 方法，在没有参数假设的情况下估计 $\\widehat{\\theta}$ 的方差。由此，通过正态近似构建一个双侧 $(1-\\alpha)$ 置信区间。具体来说，使用标准正态分布的临界值 $z_{1-\\alpha/2}$，并返回裁剪至区间 $[0,1]$ 内的边界。\n- 为了在影像组学中具有临床可解释性，定义一个判别阈值 $\\tau$（一个在 $[0,1]$ 内的浮点数）。如果置信下限大于或等于 $\\tau$，则宣布该分类器“具有临床显著性”。将此决策表示为一个布尔值。\n\n实现约束：\n\n- 直接根据上述定义实现所有计算，包括成对比较函数 $\\psi$、无偏样本方差和标准正态分位数。\n- 将所有报告的浮点结果（AUC 和置信界限）四舍五入到六位小数。\n- 不涉及物理单位。以小数形式报告任何分数数量。\n\n测试套件：\n\n计算并报告以下四个测试用例中每一个的结果。在每个用例中，都给定了 $\\mathbf{s}^+$、$\\mathbf{s}^-$、置信水平 $\\gamma$（因此 $\\alpha = 1-\\gamma$）和临床阈值 $\\tau$。\n\n- 案例 A（具有中等判别能力的典型影像组学 SVM）：\n  - $\\mathbf{s}^+ = [1.8, 1.2, 0.9, 1.5, 1.1, 1.7, 0.8, 1.3, 0.95, 1.6]$\n  - $\\mathbf{s}^- = [-0.4, -0.1, 0.2, -0.6, 0.0, -0.2, 0.1, -0.3, 0.05, -0.5]$\n  - $\\gamma = 0.95$\n  - $\\tau = 0.75$\n- 案例 B（近乎完美的分离）：\n  - $\\mathbf{s}^+ = [2.5, 2.2, 1.9, 2.1, 2.3]$\n  - $\\mathbf{s}^- = [-1.0, -0.8, -0.9, -1.2, -0.7]$\n  - $\\gamma = 0.95$\n  - $\\tau = 0.75$\n- 案例 C（大量平局，分布相同）：\n  - $\\mathbf{s}^+ = [0.5, 0.5, 0.5, 0.6, 0.4]$\n  - $\\mathbf{s}^- = [0.5, 0.5, 0.5, 0.6, 0.4]$\n  - $\\gamma = 0.95$\n  - $\\tau = 0.75$\n- 案例 D（小样本量，临界判别能力）：\n  - $\\mathbf{s}^+ = [0.7, 0.2, -0.1]$\n  - $\\mathbf{s}^- = [0.6, 0.3, 0.0, -0.2]$\n  - $\\gamma = 0.90$\n  - $\\tau = 0.70$\n\n最终输出格式：\n\n您的程序应生成单行输出，其中包含所有测试用例的结果，格式为逗号分隔的列表的列表，无空格，并用方括号括起来。对于每个测试用例，输出列表 $[\\widehat{\\theta}, \\text{lower}, \\text{upper}, \\text{clinically\\_significant}]$，其中前三项是四舍五入到六位小数的浮点数，最后一项是布尔值。例如，最终输出应类似于\n$[[\\theta_A,\\ell_A,u_A,B_A],[\\theta_B,\\ell_B,u_B,B_B],[\\theta_C,\\ell_C,u_C,B_C],[\\theta_D,\\ell_D,u_D,B_D]]$\n其中每个符号都被计算出的值替换。", "solution": "该问题要求计算受试者工作特征曲线下面积（AUC）、使用DeLong非参数方法计算其置信区间，并对几组支持向量机（SVM）分类器得分数据集的临床显著性进行评估。\n\n**问题验证**\n\n首先，我将根据指定标准验证问题陈述。\n\n**第 1 步：提取的已知条件**\n-   正类得分：$\\mathbf{s}^+ = \\{s^+_1, s^+_2, \\dots, s^+_m\\}$，含 $m$ 个样本。\n-   负类得分：$\\mathbf{s}^- = \\{s^-_1, s^-_2, \\dots, s^-_n\\}$，含 $n$ 个样本。\n-   成对比较函数：$\\psi(a,b) = 1$ 若 $a  b$， $0$ 若 $a  b$，$\\frac{1}{2}$ 若 $a = b$。\n-   经验 AUC 估计量：$\\widehat{\\theta} = \\frac{1}{mn} \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\psi(s^+_i, s^-_j)$。\n-   置信区间方法：基于 $U$-统计量的 DeLong 方法，使用临界值 $z_{1-\\alpha/2}$ 进行正态近似，置信水平为 $\\gamma = 1-\\alpha$。边界必须裁剪至 $[0,1]$。\n-   临床显著性阈值：$\\tau$。如果分类器的置信下限 $\\ge \\tau$，则其“具有临床显著性”。\n-   四舍五入：所有浮点输出（AUC、边界）保留六位小数。\n-   测试用例：提供了四个特定案例（A、B、C、D），包括得分向量 $\\mathbf{s}^+$、$\\mathbf{s}^-$、置信水平 $\\gamma$ 和阈值 $\\tau$。\n\n**第 2 步：使用提取的已知条件进行验证**\n-   **科学依据充分**：该问题在根本上是合理的。AUC 定义为概率 $P(S^+  S^-)$ 是标准做法。经验估计量 $\\widehat{\\theta}$ 是相应的 Wilcoxon-Mann-Whitney 统计量。DeLong 方法是一种成熟的、用于估计该统计量方差的非参数技术，其理论基础是广义 $U$-统计量理论。这是生物统计学和机器学习评估中常用且严谨的方法。\n-   **良构性**：该问题是良构的。输入定义清晰，数学公式明确，期望的输出格式没有歧义。对于每个测试用例，遵循定义的程序可以计算出唯一、稳定且有意义的解。测试用例中的样本量（$m \\ge 3, n \\ge 4$）足以计算无偏样本方差，这要求每组至少有两个样本。\n-   **客观性**：该问题以精确、客观的数学语言陈述，没有歧义或主观论断。虽然案例 A 被描述为具有“中等判别能力”，但所提供的数据实际上表现出完美的分离。这是描述性文本中的一个微小不一致，但它不会使问题本身失效。计算必须基于所提供的明确无误的数值数据进行。\n\n**第 3 步：结论与行动**\n该问题被认为是有效的，因为它科学依据充分、良构、客观，并包含进行严谨求解所需的所有必要信息。我将继续进行详细的求解。\n\n**方法论与求解**\n\n求解过程分为四个阶段：(1) 计算经验 AUC，(2) 使用 DeLong 方法估计 AUC 估计量的方差，(3) 构建置信区间，以及 (4) 评估临床显著性。\n\n**1. 经验 AUC 估计**\nAUC, $\\theta$, 表示从正类中随机选择的个体的得分高于从负类中随机选择的个体的得分的概率，即 $\\theta = P(S^+  S^-)$。数量 $\\widehat{\\theta}$ 是 $\\theta$ 的一个无偏估计量。它通过对正类和负类得分之间的所有 $m \\times n$ 次成对比较的结果取平均值来计算。\n比较由函数 $\\psi(a,b)$ 执行：\n$$\n\\psi(a,b) = \n\\begin{cases}\n1,  \\text{if } a  b,\\\\\n0,  \\text{if } a  b,\\\\\n\\frac{1}{2},  \\text{if } a = b.\n\\end{cases}\n$$\n经验 AUC 则为：\n$$\n\\widehat{\\theta} = \\frac{1}{mn} \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\psi(s^+_i, s^-_j)\n$$\n\n**2. 通过 DeLong 方法进行方差估计**\nDeLong 方法提供了 $\\widehat{\\theta}$ 方差的非参数估计。它基于 $U$-统计量理论。第一步是计算估计量的结构分量。\n我们定义两组分量。对于每个正类观测值 $s^+_i$，我们计算它与所有负类观测值的平均比较得分：\n$$\nV_{10}(s^+_i) = \\frac{1}{n} \\sum_{j=1}^{n} \\psi(s^+_i, s^-_j)\n$$\n每个 $s^+_i$ 对应一个这样的分量，共 $m$ 个。直观上，$V_{10}(s^+_i)$ 是仅使用第 $i$ 个正样本和所有负样本计算出的经验 AUC。\n\n类似地，对于每个负类观测值 $s^-_j$，我们计算它与所有正类观测值的平均比较得分：\n$$\nV_{01}(s^-_j) = \\frac{1}{m} \\sum_{i=1}^{m} \\psi(s^+_i, s^-_j)\n$$\n共有 $n$ 个这样的分量。请注意，所有 $V_{10}$ 分量的平均值和所有 $V_{01}$ 分量的平均值都等于 $\\widehat{\\theta}$。\n\n$\\widehat{\\theta}$ 的方差随后通过组合这些结构分量的样本方差来估计。$V_{10}$ 分量的无偏样本方差为：\n$$\nS_{10} = \\frac{1}{m-1} \\sum_{i=1}^{m} \\left( V_{10}(s^+_i) - \\widehat{\\theta} \\right)^2\n$$\n而 $V_{01}$ 分量的无偏样本方差为：\n$$\nS_{01} = \\frac{1}{n-1} \\sum_{j=1}^{n} \\left( V_{01}(s^-_j) - \\widehat{\\theta} \\right)^2\n$$\nAUC 估计量的估计方差 $\\widehat{\\mathrm{Var}}(\\widehat{\\theta})$ 由下式给出：\n$$\n\\widehat{\\mathrm{Var}}(\\widehat{\\theta}) = \\frac{S_{10}}{m} + \\frac{S_{01}}{n}\n$$\n$\\widehat{\\theta}$ 的标准误（SE）是此方差的平方根：\n$$\n\\mathrm{SE}(\\widehat{\\theta}) = \\sqrt{\\widehat{\\mathrm{Var}}(\\widehat{\\theta})}\n$$\n\n**3. 置信区间构建**\n$\\widehat{\\theta}$ 的分布可以由正态分布 $\\mathcal{N}(\\theta, \\mathrm{SE}(\\widehat{\\theta})^2)$ 来近似，特别是当 $m$ 和 $n$ 足够大时。$\\theta$ 的一个双侧 $(1-\\alpha)$ 置信区间构建如下：\n$$\n\\left[ \\widehat{\\theta} - z_{1-\\alpha/2} \\cdot \\mathrm{SE}(\\widehat{\\theta}), \\quad \\widehat{\\theta} + z_{1-\\alpha/2} \\cdot \\mathrm{SE}(\\widehat{\\theta}) \\right]\n$$\n其中 $z_{1-\\alpha/2}$ 是标准正态分布的 $(1-\\alpha/2)$-分位数。对于 $\\gamma=0.95$ 的置信水平，$\\alpha=0.05$，且 $z_{0.975} \\approx 1.96$。对于 $\\gamma=0.90$，$\\alpha=0.10$，且 $z_{0.95} \\approx 1.645$。由于 AUC 是一个概率，计算出的界限必须裁剪到有效范围 $[0, 1]$ 内。\n\n**4. 临床显著性**\n分类器的临床效用通常通过其在一定置信水平下区分不同类别的能力来判断。该问题定义一个分类器为“具有临床显著性”，如果其 AUC 置信区间的下限大于或等于预先指定的判别阈值 $\\tau$。这提供了一个保守的性能估计，确保以 $(1-α/2)$ 的置信度，真实的 AUC 至少为 $\\tau$。该决策是一个基于以下条件的布尔值：\n$\\text{clinically\\_significant} = (\\text{lower\\_bound} \\ge \\tau)$\n\n所有计算将对每个测试用例执行，最终结果（AUC、置信下限、置信上限、临床显著性）将按规定进行四舍五入和格式化。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves the problem of calculating AUC, its CI via DeLong's method,\n    and clinical significance for given test cases.\n    \"\"\"\n    test_cases = [\n        # Case A\n        {\n            \"s_plus\": [1.8, 1.2, 0.9, 1.5, 1.1, 1.7, 0.8, 1.3, 0.95, 1.6],\n            \"s_minus\": [-0.4, -0.1, 0.2, -0.6, 0.0, -0.2, 0.1, -0.3, 0.05, -0.5],\n            \"gamma\": 0.95,\n            \"tau\": 0.75\n        },\n        # Case B\n        {\n            \"s_plus\": [2.5, 2.2, 1.9, 2.1, 2.3],\n            \"s_minus\": [-1.0, -0.8, -0.9, -1.2, -0.7],\n            \"gamma\": 0.95,\n            \"tau\": 0.75\n        },\n        # Case C\n        {\n            \"s_plus\": [0.5, 0.5, 0.5, 0.6, 0.4],\n            \"s_minus\": [0.5, 0.5, 0.5, 0.6, 0.4],\n            \"gamma\": 0.95,\n            \"tau\": 0.75\n        },\n        # Case D\n        {\n            \"s_plus\": [0.7, 0.2, -0.1],\n            \"s_minus\": [0.6, 0.3, 0.0, -0.2],\n            \"gamma\": 0.90,\n            \"tau\": 0.70\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        s_plus = np.array(case[\"s_plus\"])\n        s_minus = np.array(case[\"s_minus\"])\n        gamma = case[\"gamma\"]\n        tau = case[\"tau\"]\n\n        m = len(s_plus)\n        n = len(s_minus)\n\n        # 1. Calculate matrix of psi(s_plus_i, s_minus_j) values\n        # This uses broadcasting to efficiently compute pairwise differences.\n        # The formula (sign(diff) + 1) / 2 is equivalent to the psi function.\n        diff_matrix = s_plus[:, np.newaxis] - s_minus\n        psi_matrix = (np.sign(diff_matrix) + 1) / 2\n\n        # 2. Calculate empirical AUC (theta_hat)\n        theta_hat = np.mean(psi_matrix)\n\n        # 3. DeLong's method for variance\n        v_10 = np.mean(psi_matrix, axis=1) # Structural components for positive class\n        v_01 = np.mean(psi_matrix, axis=0) # Structural components for negative class\n\n        # Unbiased sample variances of the components\n        s_10 = np.var(v_10, ddof=1)\n        s_01 = np.var(v_01, ddof=1)\n        \n        # Handle cases with perfect separation where variance is 0\n        if np.isnan(s_10): s_10 = 0\n        if np.isnan(s_01): s_01 = 0\n        \n        # Total variance of theta_hat\n        var_theta = (s_10 / m) + (s_01 / n)\n        se_theta = np.sqrt(var_theta)\n\n        # 4. Construct Confidence Interval\n        alpha = 1 - gamma\n        z_quantile = norm.ppf(1 - alpha / 2)\n        margin_of_error = z_quantile * se_theta\n        \n        lower_bound = theta_hat - margin_of_error\n        upper_bound = theta_hat + margin_of_error\n\n        # Clip bounds to [0, 1]\n        lower_bound = np.clip(lower_bound, 0, 1)\n        upper_bound = np.clip(upper_bound, 0, 1)\n\n        # 5. Determine Clinical Significance\n        is_significant = lower_bound >= tau\n\n        # 6. Format results\n        result = [\n            round(theta_hat, 6),\n            round(lower_bound, 6),\n            round(upper_bound, 6),\n            is_significant\n        ]\n        results.append(str(result).replace(\" \", \"\"))\n\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "4562067"}]}