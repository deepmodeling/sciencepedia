{"hands_on_practices": [{"introduction": "梯度提升模型通过序列化学习来逐步修正先前模型的误差。为了真正掌握这一过程，我们必须理解这些“误差”是如何被量化的。本练习将引导你完成梯度提升分类任务中的基础微积分计算，要求你推导逻辑损失函数的一阶梯度和二阶梯度（Hessian矩阵），并观察它们如何被用来确定模型的初始更新步骤。[@problem_id:4542167]", "problem": "一个放射组学团队正在训练一个梯度提升决策树（GBDT）分类器，以根据计算机断层扫描（CT）放射组学特征来预测肺结节的恶性程度。二元标签为 $y_i \\in \\{0,1\\}$，其中 $y_i = 1$ 表示恶性，$y_i = 0$ 表示良性。该模型使用带有逻辑斯蒂链接的伯努利负对数似然。对于每个样本 $i$，模型维持一个得分 $f_i$ 和一个概率 $p_i = \\frac{1}{1 + \\exp(-f_i)}$。每个样本的损失为 $\\ell_i(f_i) = - y_i \\ln p_i - (1 - y_i) \\ln(1 - p_i)$。在迭代 $t = 0$ 时，模型将所有 $i$ 的 $f_i^{(0)}$ 初始化为 $0$。\n\n任务：\n- 仅从上述定义和基础微积分出发，推导一阶导数 $g_i = \\frac{\\partial \\ell_i}{\\partial f_i}$ 和二阶导数 $h_i = \\frac{\\partial^2 \\ell_i}{\\partial f_i^2}$ 作为 $p_i$ 和 $y_i$ 的函数，然后在 $f_i^{(0)} = 0$ 处对 $y_i \\in \\{0,1\\}$ 求出 $g_i$ 和 $h_i$ 的值。\n- 在第一次提升迭代中，假设树有 $1$ 个叶节点（无分裂），学习率为 $1$，且没有正则化。对于一阶（梯度）提升，选择恒定叶节点值 $w_{\\mathrm{FO}}$ 以最小化 $\\sum_{i=1}^{n} (-g_i - w)^2$。对于二阶（牛顿）提升，选择恒定叶节点值 $w_{\\mathrm{SO}}$ 以最小化二阶泰勒近似 $\\sum_{i=1}^{n} \\left(g_i w + \\frac{1}{2} h_i w^2\\right)$。在初始化 $f_i^{(0)} = 0$ 的条件下，推导出 $w_{\\mathrm{FO}}$ 和 $w_{\\mathrm{SO}}$ 关于 $n$ 和 $k = \\sum_{i=1}^{n} y_i$ 的闭式表达式。\n\n在一个特定的CT放射组学队列中，有 $n = 200$ 名患者和 $k = 60$ 个恶性结节，计算在第一次迭代中的比率 $R = \\frac{w_{\\mathrm{SO}}}{w_{\\mathrm{FO}}}$。将最终答案报告为一个无单位的精确实数。", "solution": "首先验证问题的正确性和可解性。\n\n### 步骤1：提取已知条件\n-   二元标签：$y_i \\in \\{0,1\\}$，其中 $y_i = 1$ 是恶性。\n-   模型得分：$f_i$。\n-   预测概率：$p_i = \\frac{1}{1 + \\exp(-f_i)}$。\n-   单样本损失函数（伯努利负对数似然）：$\\ell_i(f_i) = - y_i \\ln p_i - (1 - y_i) \\ln(1 - p_i)$。\n-   迭代 $t=0$ 时的初始模型得分：对所有样本 $i$，$f_i^{(0)} = 0$。\n-   一阶导数（梯度）：$g_i = \\frac{\\partial \\ell_i}{\\partial f_i}$。\n-   二阶导数（海森矩阵）：$h_i = \\frac{\\partial^2 \\ell_i}{\\partial f_i^2}$。\n-   一阶提升的叶节点值 $w_{\\mathrm{FO}}$ 最小化目标函数 $\\sum_{i=1}^{n} (-g_i - w)^2$。\n-   二阶提升的叶节点值 $w_{\\mathrm{SO}}$ 最小化目标函数 $\\sum_{i=1}^{n} \\left(g_i w + \\frac{1}{2} h_i w^2\\right)$。\n-   第一次迭代中的树有 $1$ 个叶节点。\n-   学习率为 $1$。没有正则化。\n-   患者总数：$n = 200$。\n-   恶性结节数量（正标签）：$k = \\sum_{i=1}^{n} y_i = 60$。\n-   目标是计算比率 $R = \\frac{w_{\\mathrm{SO}}}{w_{\\mathrm{FO}}}$。\n\n### 步骤2：使用提取的已知条件进行验证\n该问题具有科学依据。所描述的组件是梯度提升模型理论中的标准部分。损失函数是二元交叉熵，链接函数是逻辑斯蒂（sigmoid）函数，梯度和海森矩阵的定义是正确的。用于寻找叶节点权重的目标函数对应于GBDT中一阶（拟合残差）和二阶（牛顿法）更新的标准优化问题。该问题是适定的，提供了推导所需量和计算最终比率的所有必要信息。语言客观且数学上精确。没有矛盾、歧义或不切实际的假设。\n\n### 步骤3：结论与行动\n问题有效。将提供完整解答。\n\n### 梯度和海森矩阵的推导\n单样本损失由 $\\ell_i(f_i) = - y_i \\ln p_i - (1 - y_i) \\ln(1 - p_i)$ 给出，其中 $p_i = \\frac{1}{1 + \\exp(-f_i)}$。\n为了求出 $\\ell_i$ 对 $f_i$ 的导数，我们首先求出 $p_i$ 对 $f_i$ 的导数：\n$$ \\frac{\\partial p_i}{\\partial f_i} = \\frac{\\partial}{\\partial f_i} (1 + \\exp(-f_i))^{-1} = -1 (1 + \\exp(-f_i))^{-2} \\cdot (\\exp(-f_i) \\cdot -1) = \\frac{\\exp(-f_i)}{(1 + \\exp(-f_i))^2} $$\n这可以表示为 $p_i$ 的形式：\n$$ \\frac{\\partial p_i}{\\partial f_i} = \\frac{1}{1 + \\exp(-f_i)} \\cdot \\frac{\\exp(-f_i)}{1 + \\exp(-f_i)} = p_i (1 - p_i) $$\n这是逻辑斯蒂函数的一个众所周知的性质。\n\n现在，我们使用链式法则求一阶导数 $g_i$：\n$$ g_i = \\frac{\\partial \\ell_i}{\\partial f_i} = \\frac{\\partial \\ell_i}{\\partial p_i} \\frac{\\partial p_i}{\\partial f_i} $$\n损失对 $p_i$ 的导数是：\n$$ \\frac{\\partial \\ell_i}{\\partial p_i} = -\\frac{y_i}{p_i} - (1 - y_i) \\frac{1}{1 - p_i} (-1) = -\\frac{y_i}{p_i} + \\frac{1 - y_i}{1 - p_i} = \\frac{-y_i(1 - p_i) + p_i(1 - y_i)}{p_i(1 - p_i)} = \\frac{-y_i + y_ip_i + p_i - y_ip_i}{p_i(1 - p_i)} = \\frac{p_i - y_i}{p_i(1 - p_i)} $$\n结合这些结果：\n$$ g_i = \\left( \\frac{p_i - y_i}{p_i(1 - p_i)} \\right) \\cdot (p_i(1 - p_i)) = p_i - y_i $$\n为了求二阶导数 $h_i$，我们将 $g_i$ 对 $f_i$ 求导：\n$$ h_i = \\frac{\\partial^2 \\ell_i}{\\partial f_i^2} = \\frac{\\partial g_i}{\\partial f_i} = \\frac{\\partial}{\\partial f_i} (p_i - y_i) = \\frac{\\partial p_i}{\\partial f_i} $$\n使用我们之前得到的 $\\frac{\\partial p_i}{\\partial f_i}$ 的结果：\n$$ h_i = p_i(1 - p_i) $$\n\n### 在初始化时的评估\n在初始迭代 $t=0$ 时，模型得分被初始化为 $f_i^{(0)} = 0$（对所有 $i$）。\n每个样本的初始概率是：\n$$ p_i^{(0)} = \\frac{1}{1 + \\exp(-0)} = \\frac{1}{1 + 1} = \\frac{1}{2} $$\n我们在此初始点评估梯度 $g_i$ 和海森矩阵 $h_i$：\n$$ g_i^{(0)} = p_i^{(0)} - y_i = \\frac{1}{2} - y_i $$\n$$ h_i^{(0)} = p_i^{(0)}(1 - p_i^{(0)}) = \\frac{1}{2} \\left(1 - \\frac{1}{2}\\right) = \\frac{1}{4} $$\n这些初始值将用于寻找第一棵树的叶节点权重。\n\n### 叶节点权重的推导\n对于第一次提升迭代，我们为整个数据集计算一个单一的叶节点权重 $w$。\n\n**一阶叶节点权重 ($w_{\\mathrm{FO}}$)**\n目标是找到使 $L_{\\mathrm{FO}}(w) = \\sum_{i=1}^{n} (-g_i - w)^2$ 最小化的 $w_{\\mathrm{FO}}$。为了找到最小值，我们将关于 $w$ 的导数设为零：\n$$ \\frac{d L_{\\mathrm{FO}}}{d w} = \\sum_{i=1}^{n} 2(-g_i - w)(-1) = 2 \\sum_{i=1}^{n} (g_i + w) = 0 $$\n$$ \\sum_{i=1}^{n} g_i + \\sum_{i=1}^{n} w = 0 \\implies \\left(\\sum_{i=1}^{n} g_i\\right) + n w = 0 $$\n$$ w_{\\mathrm{FO}} = -\\frac{\\sum_{i=1}^{n} g_i}{n} $$\n使用初始梯度 $g_i^{(0)} = \\frac{1}{2} - y_i$ 和定义 $k = \\sum_{i=1}^{n} y_i$：\n$$ \\sum_{i=1}^{n} g_i^{(0)} = \\sum_{i=1}^{n} \\left(\\frac{1}{2} - y_i\\right) = \\sum_{i=1}^{n} \\frac{1}{2} - \\sum_{i=1}^{n} y_i = \\frac{n}{2} - k $$\n将此代入 $w_{\\mathrm{FO}}$ 的表达式中：\n$$ w_{\\mathrm{FO}} = -\\frac{\\frac{n}{2} - k}{n} = -\\left(\\frac{1}{2} - \\frac{k}{n}\\right) = \\frac{k}{n} - \\frac{1}{2} $$\n\n**二阶叶节点权重 ($w_{\\mathrm{SO}}$)**\n目标是找到使损失的二阶泰勒近似 $L_{\\mathrm{SO}}(w) = \\sum_{i=1}^{n} \\left(g_i w + \\frac{1}{2} h_i w^2\\right)$ 最小化的 $w_{\\mathrm{SO}}$。将关于 $w$ 的导数设为零：\n$$ \\frac{d L_{\\mathrm{SO}}}{d w} = \\sum_{i=1}^{n} (g_i + h_i w) = 0 $$\n$$ \\sum_{i=1}^{n} g_i + w \\sum_{i=1}^{n} h_i = 0 $$\n$$ w_{\\mathrm{SO}} = -\\frac{\\sum_{i=1}^{n} g_i}{\\sum_{i=1}^{n} h_i} $$\n使用初始梯度和海森值，$g_i^{(0)} = \\frac{1}{2} - y_i$ 和 $h_i^{(0)} = \\frac{1}{4}$：\n分子和之前一样：$\\sum_{i=1}^{n} g_i^{(0)} = \\frac{n}{2} - k$。\n分母是：\n$$ \\sum_{i=1}^{n} h_i^{(0)} = \\sum_{i=1}^{n} \\frac{1}{4} = \\frac{n}{4} $$\n将这些总和代入 $w_{\\mathrm{SO}}$ 的表达式中：\n$$ w_{\\mathrm{SO}} = -\\frac{\\frac{n}{2} - k}{\\frac{n}{4}} = -4 \\frac{\\frac{n}{2} - k}{n} = -4\\left(\\frac{1}{2} - \\frac{k}{n}\\right) = 4\\left(\\frac{k}{n} - \\frac{1}{2}\\right) $$\n\n### 比率的计算\n问题要求计算比率 $R = \\frac{w_{\\mathrm{SO}}}{w_{\\mathrm{FO}}}$。使用推导出的表达式：\n$$ R = \\frac{w_{\\mathrm{SO}}}{w_{\\mathrm{FO}}} = \\frac{4\\left(\\frac{k}{n} - \\frac{1}{2}\\right)}{\\frac{k}{n} - \\frac{1}{2}} $$\n只要分母不为零，该比率就有效。如果 $\\frac{k}{n} = \\frac{1}{2}$，分母为零。\n对于给定的队列，$n=200$ 且 $k=60$。\n$$ \\frac{k}{n} = \\frac{60}{200} = \\frac{3}{10} = 0.3 $$\n由于 $0.3 \\neq 0.5$，分母不为零，我们可以消去项 $\\left(\\frac{k}{n} - \\frac{1}{2}\\right)$：\n$$ R = 4 $$\n比率是一个常数值 $4$，与 $n$ 和 $k$ 的具体值无关（只要 $2k \\neq n$）。\n最终答案是一个精确的实数。", "answer": "$$\n\\boxed{4}\n$$", "id": "4542167"}, {"introduction": "真实世界的医学数据集（如放射组学数据）往往是不完整的。现代梯度提升模型实现的一个关键优势，是其在构建决策树时能够内部处理缺失值。本实践问题将揭开这一过程的神秘面纱，挑战你通过直接运用模型的优化目标，来计算决策树分裂节点上缺失数据的最佳“默认方向”。[@problem_id:4542131]", "problem": "一家医院构建了一个梯度提升机（GBM）模型，用于根据一个计算机断层扫描（CT）放射组学特征 $x$ 来预测一个二分类的放射组学终点。由于感兴趣区域（region-of-interest）的可变性，该特征 $x$ 常常存在缺失值。训练过程是通过将加法树拟合到损失函数的负梯度来进行的。在某个特定的内部节点上，模型评估一个在阈值 $\\tau$ 处对特征 $x$ 进行分裂的候选方案。设该节点上每个样本的损失函数相对于当前模型输出的一阶和二阶导数分别为 $g_i$ 和 $h_i$。对于任何一组样本，定义 $G=\\sum_i g_i$ 和 $H=\\sum_i h_i$。正则化项包括一个系数为 $\\lambda$ 的 $\\ell_2$ 叶子节点惩罚项和一个每次分裂的复杂度惩罚项 $\\gamma$。该 GBM 使用标准的损失函数二阶泰勒展开来确定最佳叶子节点值和分裂增益。\n\n对于候选阈值 $\\tau$，非缺失样本（$x$ 可观测）被划分为一个“已知左”组和一个“已知右”组，其聚合导数分别为 $G_{\\mathrm{KL}}=-12, H_{\\mathrm{KL}}=30$ 和 $G_{\\mathrm{KR}}=5, H_{\\mathrm{KR}}=20$。$x$ 值缺失的样本的聚合导数为 $G_{\\mathrm{M}}=-3, H_{\\mathrm{M}}=6$。超参数为 $\\lambda=1$ 和 $\\gamma=0.1$。GBM 处理缺失值的方法是：在每次分裂时学习一个默认方向，所有缺失值在训练和推理时都将遵循该方向。\n\n哪个选项正确地描述了基于树的 GBM 在这次分裂中如何处理缺失值，并正确地计算了在两种可能的默认方向下的分裂增益，从而确定了学习到的默认方向？\n\nA. GBM 尝试两种默认方向，即将所有缺失样本分配到左子节点或右子节点，在每种情况下使用带正则化的二阶近似计算分裂增益，并选择增益较大的方向。当缺失值被发送到左侧时，聚合导数为 $G_{\\mathrm{L}}=G_{\\mathrm{KL}}+G_{\\mathrm{M}}=-15$, $H_{\\mathrm{L}}=H_{\\mathrm{KL}}+H_{\\mathrm{M}}=36$ 和 $G_{\\mathrm{R}}=G_{\\mathrm{KR}}=5$, $H_{\\mathrm{R}}=H_{\\mathrm{KR}}=20$，产生的增益约为 $2.66$。当缺失值被发送到右侧时，$G_{\\mathrm{L}}=-12$, $H_{\\mathrm{L}}=30$ 和 $G_{\\mathrm{R}}=2$, $H_{\\mathrm{R}}=26$，产生的增益约为 $1.42$。学习到的默认方向是向左。\n\nB. GBM 总是将缺失样本发送到具有更多非缺失样本的子节点。分裂增益的计算完全忽略了缺失样本。此处的增益约为 $1.00$，且默认方向是向右，因为 $H_{\\mathrm{KR}}=20$ 大于 $H_{\\mathrm{KL}}=30$。\n\nC. GBM 为该分裂创建一个第三个“缺失”子节点，并将增益计算为三个子节点得分之和减去父节点得分。将 $G_{\\mathrm{M}}=-3$, $H_{\\mathrm{M}}=6$ 作为一个单独的子节点，其增益会超过任何一种双向分配，因此学习到的默认设置是“缺失分支”。\n\nD. GBM首先将缺失的 $x$ 插补为该节点上已观测到的 $x$ 的均值，然后如同没有缺失值一样继续进行；增益是在插补后的完整数据上计算的，不尝试其他默认方向。计算出的增益约为 $2.00$，并且在推理时没有学习到的默认方向。", "solution": "用户希望我解决一个关于梯度提升机（GBM）的问题，特别是它们在构建决策树过程中如何处理缺失值。\n\n### 第一步：提取已知信息\n- **模型**：梯度提升机 (GBM)\n- **任务**：二分类放射组学终点。\n- **特征**：一个带有缺失值的放射组学特征 $x$。\n- **算法**：通过拟合损失函数负梯度的加法树进行训练。解答使用损失函数的二阶泰勒展开。\n- **导数符号**：对于当前节点上的样本 $i$，$g_i$ 是损失函数相对于模型输出的一阶导数，$h_i$ 是二阶导数。$G = \\sum g_i$ 和 $H = \\sum h_i$ 是一组样本的聚合导数。\n- **正则化**：一个系数为 $\\lambda = 1$ 的 $\\ell_2$ 叶子节点惩罚项和一个每次分裂的复杂度惩罚项 $\\gamma = 0.1$。\n- **在分裂阈值 $\\tau$ 处非缺失样本的数据**：\n    - 已知左组（样本 $x \\le \\tau$）：$G_{\\mathrm{KL}} = -12$, $H_{\\mathrm{KL}} = 30$。\n    - 已知右组（样本 $x > \\tau$）：$G_{\\mathrm{KR}} = 5$, $H_{\\mathrm{KR}} = 20$。\n- **缺失样本的数据**：\n    - 缺失组：$G_{\\mathrm{M}} = -3$, $H_{\\mathrm{M}} = 6$。\n- **缺失值处理方法**：“GBM 处理缺失值的方法是：在每次分裂时学习一个默认方向，所有缺失值在训练和推理时都将遵循该方向。”\n\n### 第二步：使用提取的已知信息进行验证\n问题陈述描述了稀疏感知分裂查找算法 (sparsity-aware split-finding algorithm)，这是像 XGBoost 这类现代 GBM 实现的一个关键组成部分。在此背景下，使用一阶和二阶导数 ($g_i, h_i$)、$\\ell_2$ 正则化 ($\\lambda$) 和分裂复杂度惩罚 ($\\gamma$) 是标准做法。所提供的值在数值上是一致的；例如，二阶导数之和 ($H$) 都是正数，对于常见的损失函数（如对数损失、平方误差损失）而言，这必须成立。关于学习“默认方向”的描述是对该算法的精确技术性描述。该问题具有科学依据、提法明确且客观。它包含了执行所需计算的所有必要信息，并且不存在任何无效性缺陷。\n\n### 第三步：结论与行动\n问题有效。我将开始推导解答。\n\n### 推导\n这个问题的核心在于计算 GBM 树中一次分裂的增益。增益源于目标函数，该函数使用二阶泰勒展开进行近似。一个叶子节点的最佳权重 $w^*$ 及其对应的目标函数值（分数）由以下公式给出：\n$$ w^* = -\\frac{G}{H + \\lambda} $$\n$$ \\text{score} = -\\frac{1}{2} \\frac{G^2}{H + \\lambda} $$\n其中 $G$ 和 $H$ 分别是该叶子节点中样本的一阶和二阶导数之和，$\\lambda$ 是 $\\ell_2$ 正则化参数。\n\n一次分裂的增益是目标函数的减少量，即子节点分数之和减去父节点分数，再减去引入该分裂的复杂度惩罚 $\\gamma$。\n$$ \\text{Gain} = \\text{score}_{\\text{Left}} + \\text{score}_{\\text{Right}} - \\text{score}_{\\text{Parent}} - \\gamma $$\n代入分数公式，我们得到：\n$$ \\text{Gain} = \\frac{1}{2} \\left[ \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{G_{\\text{Parent}}^2}{H_{\\text{Parent}} + \\lambda} \\right] - \\gamma $$\n其中下标 $L$、$R$ 和 `Parent` 分别指代左子节点、右子节点和父节点。\n\n问题陈述 GBM 会学习一个默认方向。这是通过评估两种情况来完成的：\n1.  所有缺失值样本都被分配到左子节点。\n2.  所有缺失值样本都被分配到右子节点。\n\n算法会计算这两种情况下的分裂增益，并选择产生更高增益的方向。\n\n首先，我们计算父节点的聚合导数，该节点包含所有样本（已知左、已知右和缺失）：\n$G_{\\text{Parent}} = G_{\\mathrm{KL}} + G_{\\mathrm{KR}} + G_{\\mathrm{M}} = -12 + 5 + (-3) = -10$\n$H_{\\text{Parent}} = H_{\\mathrm{KL}} + H_{\\mathrm{KR}} + H_{\\mathrm{M}} = 30 + 20 + 6 = 56$\n\n增益公式中的父节点分数项为：\n$$ \\frac{G_{\\text{Parent}}^2}{H_{\\text{Parent}} + \\lambda} = \\frac{(-10)^2}{56 + 1} = \\frac{100}{57} $$\n\n**情况1：缺失值发送到左子节点**\n\n左子节点将包含已知左样本和缺失值样本。右子节点将只包含已知右样本。\n- **左子节点：**\n  $G_L = G_{\\mathrm{KL}} + G_{\\mathrm{M}} = -12 + (-3) = -15$\n  $H_L = H_{\\mathrm{KL}} + H_{\\mathrm{M}} = 30 + 6 = 36$\n- **右子节点：**\n  $G_R = G_{\\mathrm{KR}} = 5$\n  $H_R = H_{\\mathrm{KR}} = 20$\n\n此情况下的增益 ($\\text{Gain}_{\\text{left}})$ 是：\n$$ \\text{Gain}_{\\text{left}} = \\frac{1}{2} \\left[ \\frac{(-15)^2}{36 + 1} + \\frac{5^2}{20 + 1} - \\frac{100}{57} \\right] - 0.1 $$\n$$ \\text{Gain}_{\\text{left}} = \\frac{1}{2} \\left[ \\frac{225}{37} + \\frac{25}{21} - \\frac{100}{57} \\right] - 0.1 $$\n数值计算如下：\n$$ \\frac{225}{37} \\approx 6.08108 $$\n$$ \\frac{25}{21} \\approx 1.19048 $$\n$$ \\frac{100}{57} \\approx 1.75439 $$\n$$ \\text{Gain}_{\\text{left}} \\approx \\frac{1}{2} [6.08108 + 1.19048 - 1.75439] - 0.1 = \\frac{1}{2} [5.51717] - 0.1 \\approx 2.7586 - 0.1 = 2.6586 $$\n所以，增益约为 $2.66$。\n\n**情况2：缺失值发送到右子节点**\n\n左子节点将只包含已知左样本。右子节点将包含已知右样本和缺失值样本。\n- **左子节点：**\n  $G_L = G_{\\mathrm{KL}} = -12$\n  $H_L = H_{\\mathrm{KL}} = 30$\n- **右子节点：**\n  $G_R = G_{\\mathrm{KR}} + G_{\\mathrm{M}} = 5 + (-3) = 2$\n  $H_R = H_{\\mathrm{KR}} + H_{\\mathrm{M}} = 20 + 6 = 26$\n\n此情况下的增益 ($\\text{Gain}_{\\text{right}})$ 是：\n$$ \\text{Gain}_{\\text{right}} = \\frac{1}{2} \\left[ \\frac{(-12)^2}{30 + 1} + \\frac{2^2}{26 + 1} - \\frac{100}{57} \\right] - 0.1 $$\n$$ \\text{Gain}_{\\text{right}} = \\frac{1}{2} \\left[ \\frac{144}{31} + \\frac{4}{27} - \\frac{100}{57} \\right] - 0.1 $$\n数值计算如下：\n$$ \\frac{144}{31} \\approx 4.64516 $$\n$$ \\frac{4}{27} \\approx 0.14815 $$\n$$ \\text{Gain}_{\\text{right}} \\approx \\frac{1}{2} [4.64516 + 0.14815 - 1.75439] - 0.1 = \\frac{1}{2} [3.03892] - 0.1 \\approx 1.5195 - 0.1 = 1.4195 $$\n所以，增益约为 $1.42$。\n\n**结论**\n\n比较两种增益：\n$\\text{Gain}_{\\text{left}} \\approx 2.66$\n$\\text{Gain}_{\\textright} \\approx 1.42$\n\n由于 $\\text{Gain}_{\\text{left}} > \\text{Gain}_{\\text{right}}$，GBM 将选择把缺失值发送到左子节点。学习到的默认方向是‘左’，此次分裂的增益约为 $2.66$。\n\n### 逐项分析\n\n**A. GBM 尝试两种默认方向，即将所有缺失样本分配到左子节点或右子节点，在每种情况下使用带正则化的二阶近似计算分裂增益，并选择增益较大的方向。当缺失值被发送到左侧时，聚合导数为 $G_{\\mathrm{L}}=G_{\\mathrm{KL}}+G_{\\mathrm{M}}=-15$, $H_{\\mathrm{L}}=H_{\\mathrm{KL}}+H_{\\mathrm{M}}=36$ 和 $G_{\\mathrm{R}}=G_{\\mathrm{KR}}=5$, $H_{\\mathrm{R}}=H_{\\mathrm{KR}}=20$，产生的增益约为 $2.66$。当缺失值被发送到右侧时，$G_{\\mathrm{L}}=-12$, $H_{\\mathrm{L}}=30$ 和 $G_{\\mathrm{R}}=2$, $H_{\\mathrm{R}}=26$，产生的增益约为 $1.42$。学习到的默认方向是向左。**\n- 这个选项完美地描述了稀疏感知分裂查找算法。\n- “缺失值向左”情况下的聚合导数 ($G_L=-15, H_L=36, G_R=5, H_R=20$) 是正确的。计算出的增益约 $2.66$ 与我们的推导相符。\n- “缺失值向右”情况下的聚合导数 ($G_L=-12, H_L=30, G_R=2, H_R=26$) 是正确的。计算出的增益约 $1.42$ 与我们的推导相符。\n- 基于更高增益得出学习到的默认方向是向左的结论也是正确的。\n- 结论：**正确**。\n\n**B. GBM 总是将缺失样本发送到具有更多非缺失样本的子节点。分裂增益的计算完全忽略了缺失样本。此处的增益约为 $1.00$，且默认方向是向右，因为 $H_{\\mathrm{KR}}=20$ 大于 $H_{\\mathrm{KL}}=30$。**\n- 该算法的描述不正确。决策基于增益最大化，而不是样本数量。问题没有给出关于样本数量的信息，只给出了导数的和。\n- 关于分裂增益的计算忽略缺失样本的说法是不正确的。如推导所示，它们是计算中不可或缺的一部分。\n- 增益约为 $1.00$ 的说法不正确。忽略缺失样本的计算将得到约 $2.34$ 的增益。\n- “默认方向是向右，因为 $H_{\\mathrm{KR}}=20$ 大于 $H_{\\mathrm{KL}}=30$” 这个陈述在数学上是错误的（$20  30$），在算法上也是不正确的。\n- 结论：**不正确**。\n\n**C. GBM 为该分裂创建一个第三个“缺失”子节点，并将增益计算为三个子节点得分之和减去父节点得分。将 $G_{\\mathrm{M}}=-3$, $H_{\\mathrm{M}}=6$ 作为一个单独的子节点，其增益会超过任何一种双向分配，因此学习到的默认设置是“缺失分支”。**\n- 这描述了一种不同的缺失值处理方法（例如，在 CART 或 C4.5 中使用的方法），而不是流行的 GBM 中的标准方法，也不是短语“学习……一个默认方向”所暗示的方法。\n- 这将创建一个三路分裂，而不是带有默认路径的二元分裂。问题的框架和增益公式都是针对二元分裂的。\n- 结论：**不正确**。\n\n**D. GBM首先将缺失的 $x$ 插补为该节点上已观测到的 $x$ 的均值，然后如同没有缺失值一样继续进行；增益是在插补后的完整数据上计算的，不尝试其他默认方向。计算出的增益约为 $2.00$，并且在推理时没有学习到的默认方向。**\n- 这描述的是均值插补，是另一种可能的策略，但它不是问题陈述中描述的算法（“学习……一个默认方向”）。所描述的算法是分裂查找的一个集成部分，而不是一个预处理步骤。\n- 在不知道 $x$ 的均值和分裂阈值 $\\tau$ 的情况下，不可能知道插补后的样本会去向何处，因此无法计算增益来验证约 $2.00$ 的说法。\n- 结论：**不正确**。", "answer": "$$\\boxed{A}$$", "id": "4542131"}, {"introduction": "一个预测模型的价值，取决于我们理解和信任其决策的能力，这在医疗等高风险领域尤为重要。在模型构建完成后，我们需要解释它的预测结果。本练习将通过实践让你初步了解SHAP（Shapley Additive exPlanations）方法，引导你为一个具体预测计算各特征的贡献度，并验证这一强大解释技术的核心理论属性。[@problem_id:4542187]", "problem": "一个基于梯度提升 (GB) 的放射组学回归模型被训练，用于从计算机断层扫描 (CT) 影像中提取的两个标准化放射组学特征来预测一个无量纲的恶性风险评分：灰度共生矩阵对比度 $x_{1}$ 和小波高频能量 $x_{2}$。考虑该集成中的一个决策树（学习率 $= 1$ 且基础得分 $= 0$），其结构如下。根节点在 $x_{1}$ 的阈值 $1.5$ 处分裂：如果 $x_{1} \\leq 1.5$，左子节点在 $x_{2}$ 的阈值 $0.5$ 处进一步分裂，当 $x_{2} \\leq 0.5$ 时产生叶节点值 $0.8$，当 $x_{2}  0.5$ 时产生叶节点值 $1.6$；如果 $x_{1}  1.5$，右子节点是一个值为 $2.0$ 的叶节点。假设在 TreeSHAP 中用于边缘化的训练分布将 $x_{1}$ 和 $x_{2}$ 视为独立的，每个特征低于其各自阈值的概率为 $0.5$，高于其阈值的概率也为 $0.5$。对于特定实例 $x = (x_{1}, x_{2}) = (1.8, 0.4)$，该树的模型预测值为 $f(x)$。\n\n使用上述独立性假设下两个特征的 Shapley 可加性解释 (SHAP) 值 $\\phi_{1}$ 和 $\\phi_{2}$ 的定义，精确计算 $\\phi_{1}$ 和 $\\phi_{2}$，并为此实例验证局部准确性属性 $\\sum_{j=1}^{2} \\phi_{j} = f(x) - \\mathbb{E}[f(X)]$。提供精确值。将这对值 $(\\phi_{1}, \\phi_{2})$ 以行矩阵的形式报告。", "solution": "### 第一步：问题验证\n\n**1. 提取已知条件：**\n- **模型：** 来自梯度提升 (GB) 集成的单个决策树。\n- **特征：** $x_{1}$ (灰度共生矩阵对比度)，$x_{2}$ (小波高频能量)。\n- **模型参数：** 学习率 $\\eta = 1$，基础得分 $F_{0} = 0$。模型预测值 $f(x)$ 是这个单棵树的输出。\n- **树结构：**\n    - 根节点：在 $x_{1}$ 的阈值 $1.5$ 处分裂。\n    - 左子节点 ($x_{1} \\leq 1.5$)：在 $x_{2}$ 的阈值 $0.5$ 处进一步分裂。\n        - 叶节点1 ($x_{1} \\leq 1.5, x_{2} \\leq 0.5$)：值 $0.8$。\n        - 叶节点2 ($x_{1} \\leq 1.5, x_{2}  0.5$)：值 $1.6$。\n    - 右子节点 ($x_{1}  1.5$)：是一个叶节点 (叶节点3)，值为 $2.0$。\n- **TreeSHAP 的背景分布：**\n    - 特征 $x_{1}$ 和 $x_{2}$ 被视为独立的。\n    - $P(x_{1} \\leq 1.5) = 0.5$, $P(x_{1}  1.5) = 0.5$。\n    - $P(x_{2} \\leq 0.5) = 0.5$, $P(x_{2}  0.5) = 0.5$。\n- **待解释的实例：** $x = (x_{1}, x_{2}) = (1.8, 0.4)$。\n- **任务：** 计算特征的 SHAP 值 $\\phi_{1}$ 和 $\\phi_{2}$，并验证局部准确性属性：$\\sum_{j=1}^{2} \\phi_{j} = f(x) - \\mathbb{E}[f(X)]$。\n\n**2. 使用提取的已知条件进行验证：**\n- **科学依据：** 该问题使用了已有的概念：梯度提升、决策树和 SHAP (Shapley 可加性解释) 值，特别是具有特征独立性假设的 TreeSHAP 变体。放射组学的背景是恰当的。所有概念在机器学习和数据科学中都是标准的。\n- **定义明确：** 问题陈述提供了一个完全指定的函数（决策树）、一个特定的输入点和一个明确定义的背景分布。这足以唯一地确定 SHAP 值和期望的模型输出。\n- **客观性：** 问题以精确的数学语言陈述，没有主观性。\n\n**3. 结论与行动：**\n该问题是有效的。这是一个在机器学习可解释性领域定义明确的计算问题。我将继续进行解答。\n\n### 第二步：解答\n\n由决策树定义的预测函数 $f(x_{1}, x_{2})$ 如下：\n$$\nf(x_{1}, x_{2}) = \\begin{cases}\n0.8  \\text{if } x_{1} \\leq 1.5 \\text{ and } x_{2} \\leq 0.5 \\\\\n1.6  \\text{if } x_{1} \\leq 1.5 \\text{ and } x_{2}  0.5 \\\\\n2.0  \\text{if } x_{1}  1.5\n\\end{cases}\n$$\n\n首先，我们计算特定实例 $x = (x_{1}, x_{2}) = (1.8, 0.4)$ 的模型预测值 $f(x)$。\n由于 $x_{1} = 1.8  1.5$，该实例落入根节点的右子节点，它是一个叶节点。\n$$\nf(1.8, 0.4) = 2.0\n$$\n\n接下来，我们计算在背景分布上的期望预测值 $\\mathbb{E}[f(X)]$。特征 $X_{1}$ 和 $X_{2}$ 是独立的，且每个阈值被跨过的概率为 $0.5$。落入由阈值定义的任何区域的联合概率是 $0.5 \\times 0.5 = 0.25$。\n这些区域及其相关输出如下：\n1. $X_{1} \\leq 1.5, X_{2} \\leq 0.5$：输出为 $0.8$。概率为 $0.25$。\n2. $X_{1} \\leq 1.5, X_{2}  0.5$：输出为 $1.6$。概率为 $0.25$。\n3. $X_{1}  1.5$（覆盖 $X_{2} \\leq 0.5$ 和 $X_{2}  0.5$ 两种情况）：输出为 $2.0$。概率为 $P(X_1  1.5) = 0.5$。\n\n因此，期望值为：\n$$\n\\mathbb{E}[f(X)] = P(X_{1} \\leq 1.5, X_{2} \\leq 0.5) \\cdot 0.8 + P(X_{1} \\leq 1.5, X_{2}  0.5) \\cdot 1.6 + P(X_{1}  1.5) \\cdot 2.0\n$$\n$$\n\\mathbb{E}[f(X)] = (0.5 \\cdot 0.5) \\cdot 0.8 + (0.5 \\cdot 0.5) \\cdot 1.6 + (0.5) \\cdot 2.0\n$$\n$$\n\\mathbb{E}[f(X)] = 0.25 \\cdot 0.8 + 0.25 \\cdot 1.6 + 0.5 \\cdot 2.0 = 0.2 + 0.4 + 1.0 = 1.6\n$$\n期望预测值（对应于 SHAP 中的基值 $\\phi_0$）是 $\\mathbb{E}[f(X)]=1.6$。\n\n现在，我们为实例 $x=(1.8, 0.4)$ 计算 SHAP 值 $\\phi_1$ 和 $\\phi_2$。对于特征集合 $F$ 中的一个特征 $j$，其 Shapley 值的定义为：\n$$\n\\phi_{j}(f, x) = \\sum_{S \\subseteq F\\setminus\\{j\\}} \\frac{|S|! (|F| - |S| - 1)!}{|F|!} [ \\mathbb{E}[f(X) | X_S=x_S, X_j=x_j] - \\mathbb{E}[f(X) | X_S=x_S] ]\n$$\n此处， $|F|=2$，因此公式得以简化。对于一个特征 $j$，我们考虑特征集 $S=\\emptyset$ 和 $S=F\\setminus\\{j\\}$。两种情况下的权重均为 $\\frac{0!(2-1)!}{2!} = \\frac{1}{2}$。\n\n**$\\phi_1$（对于特征 $x_1$）的计算：**\n两种排序是 $(\\{x_1\\}, \\{x_2\\})$ 和 $(\\{x_2\\}, \\{x_1\\})$。\n1.  首先加入 $x_1$ 时的边际贡献（联盟 $S=\\emptyset$）：\n    贡献为 $\\mathbb{E}[f(X)|X_1=1.8] - \\mathbb{E}[f(X)]$。\n    $\\mathbb{E}[f(X)|X_1=1.8]$ 是在 $x_1$ 固定为 $1.8$ 且 $x_2$ 被边缘化时的期望输出。由于 $x_1=1.8  1.5$，无论 $x_2$ 的值是多少，树总是输出 $2.0$。\n    $$\n    \\mathbb{E}[f(X)|X_1=1.8] = 2.0\n    $$\n    贡献为 $2.0 - \\mathbb{E}[f(X)] = 2.0 - 1.6 = 0.4$。\n\n2.  第二个加入 $x_1$ 时的边际贡献（联盟 $S=\\{x_2\\}$）：\n    贡献为 $\\mathbb{E}[f(X)|X_1=1.8, X_2=0.4] - \\mathbb{E}[f(X)|X_2=0.4]$。\n    $\\mathbb{E}[f(X)|X_1=1.8, X_2=0.4] = f(1.8, 0.4) = 2.0$。\n    $\\mathbb{E}[f(X)|X_2=0.4]$ 是在 $x_2$ 固定为 $0.4$ 且 $x_1$ 被边缘化时的期望输出。由于 $x_2=0.4 \\leq 0.5$：\n    - 如果 $X_1 \\leq 1.5$ (概率为 $0.5$)，输出为 $0.8$。\n    - 如果 $X_1  1.5$ (概率为 $0.5$)，输出为 $2.0$。\n    $$\n    \\mathbb{E}[f(X)|X_2=0.4] = 0.5 \\cdot 0.8 + 0.5 \\cdot 2.0 = 0.4 + 1.0 = 1.4\n    $$\n    贡献为 $2.0 - 1.4 = 0.6$。\n\n对两个贡献进行平均：\n$$\n\\phi_{1} = \\frac{1}{2} (0.4) + \\frac{1}{2} (0.6) = 0.2 + 0.3 = 0.5\n$$\n\n**$\\phi_2$（对于特征 $x_2$）的计算：**\n1.  首先加入 $x_2$ 时的边际贡献（联盟 $S=\\emptyset$）：\n    贡献为 $\\mathbb{E}[f(X)|X_2=0.4] - \\mathbb{E}[f(X)]$。\n    我们已经计算出 $\\mathbb{E}[f(X)|X_2=0.4] = 1.4$。\n    贡献为 $1.4 - 1.6 = -0.2$。\n\n2.  第二个加入 $x_2$ 时的边际贡献（联盟 $S=\\{x_1\\}$）：\n    贡献为 $\\mathbb{E}[f(X)|X_1=1.8, X_2=0.4] - \\mathbb{E}[f(X)|X_1=1.8]$。\n    我们已知 $f(1.8, 0.4) = 2.0$ 且 $\\mathbb{E}[f(X)|X_1=1.8] = 2.0$。\n    贡献为 $2.0 - 2.0 = 0$。\n\n对两个贡献进行平均：\n$$\n\\phi_{2} = \\frac{1}{2} (-0.2) + \\frac{1}{2} (0) = -0.1 + 0 = -0.1\n$$\n\n所以，SHAP 值为 $\\phi_1 = 0.5$ 和 $\\phi_2 = -0.1$。\n\n**局部准确性属性的验证：**\n该属性表明 $\\phi_1 + \\phi_2 = f(x) - \\mathbb{E}[f(X)]$。\n- 等式左边：$\\phi_1 + \\phi_2 = 0.5 + (-0.1) = 0.4$。\n- 等式右边：$f(x) - \\mathbb{E}[f(X)] = 2.0 - 1.6 = 0.4$。\n等式 $0.4 = 0.4$ 成立，验证了局部准确性属性。\n\n最终答案是这对值 $(\\phi_1, \\phi_2)$。", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.5  -0.1 \\end{pmatrix}}\n$$", "id": "4542187"}]}