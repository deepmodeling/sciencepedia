## 引言
在放射组学等医学领域，构建精准的预测模型是核心目标，但“类别不平衡”——即良性与恶性样本数量悬殊——是普遍存在的严峻挑战。传统的[机器学习算法](@entry_id:751585)在处理此类数据时，往往会偏向于样本量大的多数类，导致对少数关键类别（如恶性肿瘤）的识别能力低下。同时，常规的评估指标（如准确率）也变得极具误导性，掩盖了模型的真实缺陷。这不仅妨碍了科学发现，更可能对临床决策产生负面影响。

为系统性地应对这一挑战，本文将分三部分展开。我们首先在“原理与机制”章节中，深入剖析[类别不平衡](@entry_id:636658)的数学根源及其对模型训练和评估过程的影响机制。接着，在“应用与跨学科连接”章节中，我们将通过放射组学、基因组学等领域的真实案例，探讨如何在复杂的实际场景中灵活运用代价敏感学习、高级[损失函数](@entry_id:136784)和决策曲线分析等高级策略。最后，“动手实践”部分将提供具体的编程练习，引导读者将理论知识转化为解决实际问题的能力。通过本章学习，您将能够全面掌握处理[不平衡数据集](@entry_id:637844)的理论框架与实用技能，从而构建出更稳健、更具临床价值的预测模型。

## 原理与机制

在放射组学中，构建能够从[医学影像](@entry_id:269649)中准确预测临床结果（如病灶的良恶性）的分类模型是一项核心任务。然而，临床数据集中普遍存在一个严峻的挑战，即**[类别不平衡](@entry_id:636658)（class imbalance）**。这种情况指的是不同类别的样本数量差异悬殊，例如，恶性病灶（少数类）的数量远少于良性病灶（多数类）。如果不加处理，[类别不平衡](@entry_id:636658)会严重误导模型的训练过程，并导致对模型性能的评估产生偏差。本章将深入探讨[类别不平衡](@entry_id:636658)的根本原理、其对放射组学模型的影响机制，以及应对这一挑战的系统性方法。

### 定义与量化类别不平衡

从根本上说，类别不平衡是关于数据生成分布的一个特征。在一个具有 $K$ 个类别的分类问题中，我们用 $Y \in \{1, \dots, K\}$ 表示类别标签。每个类别的**先验概率（class prior）**被定义为 $\pi_k = P(Y=k)$。这些先验概率满足[概率公理](@entry_id:262004)，即 $\sum_{k=1}^K \pi_k = 1$ 且对所有 $k$ 都有 $0 \le \pi_k \le 1$。

一个数据集如果**完全平衡**，则其所有类别的先验概率均相等，即 $\pi_k = 1/K$。当[先验概率](@entry_id:275634)分布不均匀时，就出现了类别不平衡。在实践中，我们通常关心的是那些不平衡程度足以对标准学习算法产生负面影响的情况。

为了量化不平衡的程度，一个简单而有效的度量是**不平衡比率（Imbalance Ratio, IR）**。它被定义为最大先验概率（多数类）与最小先验概率（少数类）之比：
$$
IR = \frac{\max_{k} \pi_k}{\min_{k} \pi_k}
$$
尽管没有一个通用的阈值，但通常认为当 $IR \ge 2$ 或 $IR \ge 3$ 时，不平衡问题就值得关注；当 $IR \ge 10$ 时，则认为存在严重的不平衡。

在放射组学的临床研究中，数据集的类别分布受到其采集背景的深刻影响。例如，在一个大规模的肺癌**筛查队列（screening cohort）**中，恶性结节的真实患病率可能非常低，例如在 $0.01$ 到 $0.05$ 之间。然而，放射组学研究通常使用经过临床工作流程筛选后的**诊断队列（diagnostic cohort）**，这些队列通常包含那些放射科医生认为特征不明确、难以定性的“待定”病灶。在这种情况下，尽管良性病灶仍然可能占多数，但恶性病灶的比例会显著高于筛查人群。一个典型放射组学CT研究队列中，恶性病灶的先验概率 $\pi_{\text{mal}}$ 通常在 $0.1$ 到 $0.4$ 的范围内，相应的良性病灶先验概率 $\pi_{\text{ben}}$ 则在 $0.6$ 到 $0.9$ 之间，导致的不平衡比率 $IR$ 可能在 $1.5$ 到 $9$ 之间，这属于中度到重度的不平衡范畴 [@problem_id:4543136]。

### 类别不平衡对模型训练与评估的影响

类别不平衡之所以成为一个问题，是因为它系统性地破坏了标准机器学习流程中的两个核心环节：模型训练和性能评估。

#### 对[经验风险最小化](@entry_id:633880)（ERM）训练过程的影响

大多数分类算法通过最小化在[训练集](@entry_id:636396)上定义的**[经验风险](@entry_id:633993)（Empirical Risk）** $\hat{R}_n(f)$ 来学习模型参数。对于一个给定的模型 $f$ 和[损失函数](@entry_id:136784) $\ell$，[经验风险](@entry_id:633993)是所有样本损失的平均值：
$$
\hat{R}_n(f) = \frac{1}{n}\sum_{i=1}^{n} \ell(f(X_i),Y_i)
$$
在类别不平衡的情况下（例如，$\pi_1 \ll \pi_0$），训练集中的绝大多数样本属于多数类。因此，上述求和项被多数类样本的损失所主导。为了最小化总体[经验风险](@entry_id:633993)，算法会倾向于学习一个能很好地分类多数类样本的模型，即使这意味着牺牲少数类样本的性能。

从更根本的层面来看，这种标准的[经验风险最小化](@entry_id:633880)过程甚至无法准确地反映少数类的真实风险。假设我们想估计少数类（$Y=1$）的**条件风险** $R_1(f) = \mathbb{E}[\ell(f(X),1) \mid Y=1]$。如果我们错误地使用一个在整个数据集上计算的、仅包含少数类样本损失的量 $\tilde{R}_{1,n}(f) = \frac{1}{n}\sum_{i=1}^{n} \ell(f(X_i),Y_i)\,\mathbf{1}\{Y_i=1\}$ 作为估计，那么这个估计是有偏的。可以证明，其[期望值](@entry_id:150961)为 $\mathbb{E}[\tilde{R}_{1,n}(f)] = \pi_1 R_1(f)$。因此，其**偏差（Bias）**为：
$$
B = \mathbb{E}[\tilde{R}_{1,n}(f)] - R_1(f) = (\pi_1 - 1)R_1(f)
$$
由于 $\pi_1 \ll 1$，这个偏差是负的，意味着这种朴素的估计方法会系统性地低估少数类的真实风险 [@problem_id:4543133]。这从数学上揭示了为什么由多数类主导的[损失函数](@entry_id:136784)无法为优化少数类性能提供足够的“信号”。

#### 对性能评估指标的影响

在不平衡的数据集上，标准的性能评估指标可能会产生严重的误导，掩盖模型在关键的少数类上的真实表现。

**准确率的陷阱**

**准确率（Accuracy）**，即被正确分类的样本占总样本的比例，是最直观但也最具有欺骗性的指标。在一个恶性肿瘤患病率为 $0.02$ 的数据集中，一个将所有样本都预测为“良性”的平凡分类器，其准确率可以高达 $0.98$，但它对于识别任何恶性肿瘤都毫无价值。如在一个[测试集](@entry_id:637546)中，有 $900$ 个阴性样本和 $100$ 个阳性样本，一个模型取得了 $90.5\%$ 的高准确率，但其[混淆矩阵](@entry_id:635058)可能显示它只正确识别了 $50$ 个阳性样本（$TP=50, FN=50, TN=855, FP=45$），即对少数类的召回率仅为 $0.5$ [@problem_id:4543157]。这说明准确率被多数类（阴性）的高性能所支配。

**不平衡稳健的评估指标**

为了克服准确率的缺陷，我们需要使用那些能够平等对待每个类别或对[类别不平衡](@entry_id:636658)不敏感的指标。

- **[平衡准确率](@entry_id:634900)（Balanced Accuracy, BA）**：定义为所有类别召回率（Recall，即真正率）的算术平均值。对于二[分类问题](@entry_id:637153)，它是敏感性（Sensitivity, or True Positive Rate, $TPR$）和特异性（Specificity, or True Negative Rate, $TNR$）的平均值。
  $$
  BA = \frac{1}{2} \left( \frac{TP}{TP+FN} + \frac{TN}{TN+FP} \right)
  $$
  BA通过给予每个类别相同的权重来避免多数类的影响。

- **宏平均[F1分数](@entry_id:196735)（Macro-averaged F1-score）**：首先为每个类别独立计算其[F1分数](@entry_id:196735)（精确率Precision和召回率Recall的[调和平均](@entry_id:750175)数），然后对所有类别的[F1分数](@entry_id:196735)取算术平均值。这也保证了每个类别对最终得分的贡献是均等的。

- **[马修斯相关系数](@entry_id:176799)（Matthews Correlation Coefficient, MCC）**：一个综合考虑了[混淆矩阵](@entry_id:635058)所有四个值（$TP, TN, FP, FN$）的指标，其值域为 $[-1, 1]$，其中 $+1$ 代表完美预测，$0$ 代表随机预测，$-1$ 代表完全不一致。它的计算公式为：
  $$
  MCC = \frac{TP \cdot TN - FP \cdot FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}
  $$
  MCC被认为是用于不平衡学习的最均衡的单值指标之一，只有当分类器在正负类别上都表现良好时，才能获得高分。

- **科恩Kappa系数（Cohen's Kappa, $\kappa$）**：该指标衡量分类器的预测与真实标签的一致性，同时校正了随机一致的可能性。其定义为 $\kappa = \frac{p_o - p_e}{1 - p_e}$，其中 $p_o$ 是观测到的一致性（即准确率），$p_e$ 是基于类别[边际分布](@entry_id:264862)的期望随机一致性。通过减去随机表现的部分，$\kappa$ 提供了比准确率更稳健的评估 [@problem_id:4543157]。

**阈值无关评估：ROC曲线 vs. P[R曲线](@entry_id:183670)**

对于输出概率的分类器，我们通常通过改变决策阈值来评估其整体性能，这引出了两种重要的评估曲线。

- **[受试者工作特征曲线](@entry_id:754147)（Receiver Operating Characteristic, ROC）**：绘制了真阳性率（$TPR$）与[假阳性率](@entry_id:636147)（$FPR = FP/(FP+TN)$）在不同阈值下的关系。[曲线下面积](@entry_id:169174)（**ROC AUC**）衡量了模型将随机选择的正样本排在随机选择的负样本前面的概率。ROC AUC的一个关键特性是它对[类别不平衡](@entry_id:636658)不敏感，因为$TPR$和$FPR$都是在各自类别的内部进行归一化的。然而，这在严重不平衡的情况下也可能成为一个缺点。在一个患病率极低的数据集（例如，阳性样本$n_+=40$，阴性样本$n_-=1960$）中，一个很小的$FPR$（例如$0.05$）可能对应着大量的[假阳性](@entry_id:635878)病例（$1960 \times 0.05 = 98$例），这在临床上是巨大的负担。[ROC曲线](@entry_id:182055)无法直观地展示这种代价 [@problem_id:4543106]。

- **[精确率-召回率曲线](@entry_id:637864)（Precision-Recall, PR）**：绘制了精确率（$P = TP/(TP+FP)$）与召回率（$R = TP/(TP+FN)$）在不同阈值下的关系。由于精确率的分母中包含$FP$，它直接受到[假阳性](@entry_id:635878)数量的影响。因此，PR曲线对于评估在[不平衡数据](@entry_id:177545)上识别少数类的性能更为敏感和信息丰富。在上述例子中，如果两个模型ROC AUC相同，但模型A的**PR AUC**显著高于模型B，这通常意味着模型A在达到相同召出水平（例如找到同样多的癌症患者）时，能产生更少的[假阳性](@entry_id:635878)，因此是更优越的模型 [@problem_id:4543106]。对于一个随机分类器，其PR AUC的基线值等于少数类的先验概率 $\pi_1$，这也为解读PR AUC提供了有用的参考。

**预测价值在低患病率下的崩溃**

[类别不平衡](@entry_id:636658)对评估的另一个深刻影响体现在**阳性预测价值（Positive Predictive Value, PPV）**上，即一个样本被预测为阳性时，它确实是阳性的概率。PPV是临床医生和患者极为关心的指标。通过Bayes定理，我们可以推导出PPV与模型的敏感性（$s$）、特异性（$c$）以及人群患病率（$\pi$）之间的关系：
$$
\mathrm{PPV} = \mathbb{P}(Y=1 \mid \hat{Y}=1) = \frac{s \pi}{s \pi + (1-c)(1-\pi)}
$$
从这个公式可以看出，PPV不仅依赖于分类器的性能（$s$ 和 $c$），还强烈依赖于患病率 $\pi$。在低患病率的筛查场景中，即 $\pi \to 0$ 时，即使分类器具有很高的敏感性和特异性，PPV也会趋近于零。这是因为分母中的第二项 $(1-c)(1-\pi)$，即[假阳性](@entry_id:635878)，会主导整个表达式。绝大多数的阳性预测结果实际上是[假阳性](@entry_id:635878)。这揭示了一个残酷的现实：在罕见病筛查中，一个“阳性”的测试结果本身可能并不意味着患病的可能性很高 [@problem_id:4543188]。

### 放射组学中加剧不平衡影响的机制

在放射组学领域，类别不平衡的影响往往被其固有的数据特性进一步放大，特别是**高维低样本量（High-Dimensional Low-Sample-Size, HDLSS）**的特点，即特征维度 $p$ 远大于样本量 $n$ ($p \gg n$)。

1.  **[参数估计](@entry_id:139349)的高方差**：一个[线性分类器](@entry_id:637554)有 $p$ 个权重参数，而一个二次判别分析则可能涉及多达 $p(p+1)/2$ 个协方差参数。在HDLSS设定下，这些参数的数量远超总样本数，更不用说远超本已稀少的少数类样本数 $n_1$。对少数类均值 $\hat{\boldsymbol{\mu}}_1$ 的估计方差与 $1/n_1$ 成正比。当 $n_1$ 很小而 $p$ 很大时，[参数估计](@entry_id:139349)的方差会急剧增加。这导致学习到的决策边界极不稳定，微小的训练数据扰动都可能引起边界的大幅摆动，这种不确定性对样本稀疏的少数类区域的伤害尤为严重 [@problem_id:4543181]。

2.  **高维空间的几何效应（维度灾难）**：在高维空间中，我们关于距离和邻域的直觉会失效。对于依赖距离或局部[密度估计](@entry_id:634063)的方法（如K近邻、[支持向量机](@entry_id:172128)），为了在一个查询点周围找到哪怕是少数几个少数类邻居，搜索半径必须急剧扩大。在一个 $p$ 维空间中，半径为 $r$ 的球体积与 $r^p$ 成正比。这意味着，为了包含固定数量的少数类样本，所需的邻域体积会随着维度 $p$ 的增加而爆炸式增长。这个巨大的邻域将不可避免地包含大量的多数类样本，从而“淹没”了少数类的信息，使得局部判别变得极其困难 [@problem_id:4543181]。

3.  **[模型选择](@entry_id:155601)中的正则化偏差**：在HDLSS场景中，为了[防止过拟合](@entry_id:635166)并控制高方差，**正则化（regularization）**是必不可少的。正则化的强度通常通过交叉验证来选择，其目标是最小化验证集上的总误差。如前所述，这个总误差由多数类主导。当模型面临在“稍微降低多数类性能以显著提升少数类性能”和“保持或提升多数类性能而牺牲少数类”之间选择时，[交叉验证](@entry_id:164650)的准则会倾向于后者。因此，在HDLSS中为控制方差而选择的强正则化，可能会进一步加剧对少数类的忽视 [@problem_id:4543181]。

### 应对类别不平衡的原理性方法

处理[类别不平衡](@entry_id:636658)问题需要从数据、算法和评估等多个层面采取有针对性的策略。

#### 数据层面方法：重采样

[重采样](@entry_id:142583)技术通过直接修改[训练集](@entry_id:636396)中的类别分布来缓解不平衡。

- **随机[欠采样](@entry_id:272871)（Random Under-sampling, RUS）**：随机地从多数类中丢弃一部分样本，直到类别分布变得更加平衡。这种方法的主要优点是能显著减少训练数据量，加快模型训练速度。然而，其致命缺点是可能丢弃包含重要信息的多数类样本，导致对多数类特征分布的估计变差，从而增加分类器[参数估计](@entry_id:139349)的**方差**，并可能扭曲最优[决策边界](@entry_id:146073)的方向 [@problem_id:4543184]。

- **随机过采样（Random Over-sampling, ROS）**：通过随机复制少数类的样本来增加其数量，直至类别分布平衡。ROS的优点是它保留了所有的原始数据信息。与RUS相比，它通常能得到方差更低、决策边界方向更稳定的估计，因为它利用了所有多数类样本来估计其分布。然而，简单的复制可能会导致模型对少数类的特定样本过拟合 [@problem_id:4543184]。

总的来说，这两种方法都旨在通过创建一个平衡的训练集来减少优化过程中对多数类的偏好，从而**减少决策阈值的偏差**。在两者之间，ROS通常因其保留了完整信息而在估计[决策边界](@entry_id:146073)方向上更具优势。

#### 算法层面方法：代价敏感学习

代价敏感学习（Cost-Sensitive Learning）是一种更为根本的解决方法，它将不同类型错误所带来的非对称代价直接整合到学习算法中。在临床诊断中，漏诊一个恶性肿瘤（假阴性，False Negative）的代价通常远高于将一个良性病灶误判为恶性（[假阳性](@entry_id:635878)，False Positive）。

我们可以定义 $c_{01}$ 为假阴性的代价，$c_{10}$ 为[假阳性](@entry_id:635878)的代价，并假设 $c_{01} > c_{10}$。根据[Bayes风险](@entry_id:178425)[最小化原理](@entry_id:169952)，最优的决策规则是选择能最小化给定样本 $x$ 的**条件风险**的类别。预测为类别1的条件风险是 $R(\alpha(x)=1 \mid x) = c_{10} P(0 \mid x)$，而预测为类别0的风险是 $R(\alpha(x)=0 \mid x) = c_{01} P(1 \mid x)$。

因此，我们应该在 $R(\alpha(x)=1 \mid x) \le R(\alpha(x)=0 \mid x)$ 时预测为类别1。通过代数推导，这等价于：
$$
P(1 \mid x) \ge \frac{c_{10}}{c_{10} + c_{01}}
$$
这意味着，最优决策不再是与 $0.5$ 比较，而是与一个新的阈值 $\tau = \frac{c_{10}}{c_{10} + c_{01}}$ 比较 [@problem_id:4543108]。如果假阴性的代价 $c_{01}$ 远大于[假阳性](@entry_id:635878)的代价 $c_{10}$，那么这个阈值 $\tau$ 将会很低，使得分类器更容易将样本预测为阳性。这种方法提供了一个根据临床需求调整模型行为的原则性框架。

#### 适应变化的[先验概率](@entry_id:275634)：数据集偏移

在实际应用中，一个在训练数据（具有[先验概率](@entry_id:275634) $p_{\text{tr}}$）上训练好的模型，可能会被部署到一个人群分布不同（具有先验概率 $p_{\text{ext}}$）的新环境中。这种分布变化被称为**数据集偏移（Dataset Shift）**。

- **先验偏移（Prior Shift）**：仅类别[先验概率](@entry_id:275634) $\mathbb{P}(Y)$ 发生变化，而类[条件概率](@entry_id:151013) $\mathbb{P}(X|Y)$ 保持不变。这是最简单也最常见的一种偏移。
- **[协变量偏移](@entry_id:636196)（Covariate Shift）**：特征分布 $\mathbb{P}(X)$ 发生变化，但后验概率 $\mathbb{P}(Y|X)$ 保持不变。
- **概念偏移（Concept Shift）**：特征与标签之间的关系 $\mathbb{P}(Y|X)$ 发生根本性改变。

对于逻辑回归这类输出[对数几率](@entry_id:141427)（log-odds）的模型，如果发生的仅仅是先验偏移，我们可以对模型进行精确的校准。一个在[训练集](@entry_id:636396)上学习到的[对数几率](@entry_id:141427) $s(X)$ 可以被修正，以适应新的外部先验概率 $p_{\text{ext}}$。修正后的对数几率 $s'(X)$ 仅需增加一个**截距校正项** $\Delta$：
$$
s'(X) = s(X) + \Delta
$$
其中，
$$
\Delta = \ln\left(\frac{p_{\text{ext}}}{1-p_{\text{ext}}}\right) - \ln\left(\frac{p_{\text{tr}}}{1-p_{\text{tr}}}\right) = \operatorname{logit}(p_{\text{ext}}) - \operatorname{logit}(p_{\text{tr}})
$$
例如，如果一个模型在 $p_{\text{tr}}=0.30$ 的数据上训练，现在要用于 $p_{\text{ext}}=0.10$ 的人群，那么截距校正项为 $\Delta \approx -1.350$ [@problem_id:4543107]。这个简单的调整能够确保模型的概率预测在新的人群中保持校准。

### [不平衡数据](@entry_id:177545)下的[稳健模型验证](@entry_id:754390)

最后，模型的验证过程也必须适应[类别不平衡](@entry_id:636658)的挑战。标准的 $k$-折[交叉验证](@entry_id:164650)（CV）将数据随机分成 $k$ 份，轮流作为[测试集](@entry_id:637546)。在[不平衡数据](@entry_id:177545)上，这种完全随机的划分可能导致某些折中的少数类样本数量极少甚至为零，使得该折的性能估计（如TPR）方差极大或无法计算。

**分层 $k$-折交叉验证（Stratified $k$-fold CV）**通过在划分数据时保持每个折中各类别样本的比例与原始数据集一致，从而解决了这个问题。例如，如果整个数据集有 $10\%$ 的阳性样本，分层CV将确保每个折中也约有 $10\%$ 的阳性样本。

理论分析表明，相比于非分层CV，分层CV能够显著降低性能估计量的**方差**。对于TPR的估计，其方差的降低比例可以表示为 [@problem_id:4543142]：
$$
R = \frac{\operatorname{Var}_{\text{unstrat}}(\bar{r})}{\operatorname{Var}_{\text{strat}}(\bar{r})} = 1 + \frac{k - 1}{N - 1} \cdot \frac{N - m}{m}
$$
其中 $N$ 是总样本数，$m$ 是少数类样本数，$k$ 是折数。这个公式清楚地表明，当少数类越稀有（$m$ 越小）、折数 $k$ 越多时，分层带来的方差缩减效应越显著。因此，在处理[类别不平衡](@entry_id:636658)的放射组学数据集时，采用[分层交叉验证](@entry_id:635874)是进行稳健模型评估和选择的必要步骤。