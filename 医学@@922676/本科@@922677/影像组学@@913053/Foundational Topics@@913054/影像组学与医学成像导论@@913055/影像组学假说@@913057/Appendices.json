{"hands_on_practices": [{"introduction": "放射组学研究通常会从医学影像中提取成百上千个特征，这带来了所谓的“高维问题”。这项实践旨在引导你运用必要的统计学工具——多重检验校正和降维技术——来解决这一挑战。通过完成这项练习[@problem_id:4567514]，你将亲手实践如何从海量特征中可靠地筛选出与临床终点相关的候选特征，同时有效控制伪阳性发现的风险。", "problem": "给定一个放射组学假说的形式化表述，您需要构建一个程序，从统计推断的第一性原理出发，实现多重检验下的特征选择和使用主成分分析进行降维的操作。放射组学假说断言，从图像中提取的量化特征编码了潜在的生物学特性；因此，如果一个临床终点与潜在的组织状态有因果关系，那么相应的图像特征应该与该终点表现出统计上可检测的关联。在高维设置中，特征数量相对于样本数量较大，这要求对假阳性进行原则性控制，并进行仔细的降维。\n\n使用的基本原理：\n- 线性关联的统计假设检验：对于每个特征 $j$ 的样本 $\\{x_{ij}\\}_{i=1}^n$ 和一个临床终点 $\\{y_i\\}_{i=1}^n$，考虑原假设 $H_{0j}: \\rho_j = 0$，其中 $\\rho_j$ 是特征 $j$ 与终点之间的总体皮尔逊相关系数。样本皮尔逊相关系数 $r_j$ 是从中心化和缩放后的变量中导出的，并且在原假设下，对于具有有限二阶矩的独立同分布样本，它会产生一个服从自由度为 $n-2$ 的学生t分布的 $t$ 统计量。\n- 通过 Bonferroni 方法控制族内错误率 (FWER)：给定一个显著性水平 $\\alpha$ 和 $m$ 个同时进行的检验，如果双边 $p$ 值 $p_j \\le \\alpha/m$，则拒绝 $H_{0j}$。\n- 通过 Benjamini–Hochberg (BH) 程序控制错误发现率 (FDR)：给定一个目标水平 $q$ 和 $m$ 个 $p$ 值，将它们升序排列为 $p_{(1)} \\le \\cdots \\le p_{(m)}$，并找到满足 $p_{(k)} \\le (k/m)q$ 的最大索引 $k$。拒绝 $p$ 值最小的 $k$ 个假设。\n- 主成分分析 (PCA)：对于一个均值中心化的数据矩阵 $Z \\in \\mathbb{R}^{n \\times p}$，通过奇异值分解得到的奇异值可以通过 $Z = U S V^\\top$ 来量化样本协方差矩阵的特征值，其中特征值为 $S^2/(n-1)$，第 $i$ 个主成分的解释方差比等于其特征值除以所有特征值之和。解释至少目标比例 $\\tau$ 的总方差所需的最小主成分数 $K$ 满足 $\\sum_{i=1}^K \\lambda_i / \\sum_{i=1}^{r} \\lambda_i \\ge \\tau$，其中 $\\lambda_i$ 是排序后的特征值，$r$ 是秩。\n\n您的程序必须对每个测试用例严格遵循以下步骤执行：\n1. 通过减去样本均值并除以自由度为 $n-1$ 的样本标准差，对每个特征列进行标准化。将得到的标准化特征矩阵表示为 $Z$。任何样本标准差为零的特征必须在所有后续计算中被忽略。同样地标准化终点向量 $y$；您可以假设在所有测试用例中终点的样本方差非零。\n2. 计算每个标准化特征列 $Z_{\\cdot j}$ 与标准化终点 $z_y$ 之间的样本皮尔逊相关系数 $r_j$。使用恒等式 $r_j = \\frac{1}{n-1}\\sum_{i=1}^n Z_{ij} z_{y,i}$，该恒等式源于标准化后将相关性定义为协方差除以标准差的乘积。\n3. 对于每个 $r_j$，使用学生t分布计算原假设下的双边 $p$ 值：对于 $|r_j|  1$，构建统计量 $t_j = r_j \\sqrt{\\frac{n-2}{1-r_j^2}}$，并设置相应的双边 $p$ 值为 $p_j = 2\\left(1 - F_{t_{n-2}}(|t_j|)\\right)$，其中 $F_{t_{n-2}}$ 是自由度为 $n-2$ 的学生t分布的累积分布函数。如果 $|r_j| = 1$，则根据连续性设置 $p_j = 0$。\n4. 在显著性水平 $\\alpha$ 下应用 Bonferroni 校正，以获得被拒绝的原假设数量 $B$，其中当 $p_j \\le \\alpha/m$ 时拒绝原假设，$m$ 是未被忽略的特征计数。\n5. 在目标水平 $q$ 下应用 Benjamini–Hochberg 程序，以获得被拒绝的原假设数量 $H$，即满足 $p_{(k)} \\le (k/m)q$ 的最大 $k$ 值。\n6. 通过奇异值分解对 $Z$ 执行主成分分析，计算特征值 $\\lambda_i = S_i^2/(n-1)$，将其降序排列，并确定最小数量 $K$，使得累积解释方差比例达到或超过目标 $\\tau$，即满足 $\\sum_{i=1}^K \\lambda_i / \\sum_{i=1}^{r} \\lambda_i \\ge \\tau$ 的最小 $K$。如果没有未被忽略的特征，则定义 $K = 0$。\n\n您的程序必须实现上述功能，并为每个测试用例生成一个三整数结果 $[B, H, K]$。\n\n测试套件：\n使用以下四个确定性测试用例。每个用例提供一个特征矩阵 $X$、一个终点向量 $y$ 和水平 $(\\alpha, q, \\tau)$。所有条目均为整数，且大小较小以便于手动验证。\n\n- 情况1（理想情况，正交特征与完全相关的终点）：\n    - $n = 8$， $p = 4$。\n    - $X$ 的列为\n      $v_1 = [1, 1, 1, 1, -1, -1, -1, -1]$，\n      $v_2 = [1, 1, -1, -1, 1, 1, -1, -1]$，\n      $v_3 = [1, -1, 1, -1, 1, -1, 1, -1]$，\n      $v_4 = [1, -1, -1, 1, -1, 1, 1, -1]$。\n    - 终点 $y = v_3$。\n    - 水平：$\\alpha = 0.05$， $q = 0.10$， $\\tau = 0.95$。\n\n- 情况2（边界情况，无信号；终点与所有特征正交）：\n    - $n = 8$， $p = 4$， $X$ 与情况1相同。\n    - 终点 $y = [1, -1, 1, -1, -1, 1, -1, 1]$。\n    - 水平：$\\alpha = 0.05$， $q = 0.10$， $\\tau = 0.50$。\n\n- 情况3（高度共线的特征，$p  n$，多个完全相关变量和低有效秩）：\n    - $n = 6$， $p = 8$。\n    - 定义基础序列：\n      $u_1 = [1, 2, 3, -3, -2, -1]$，\n      $u_2 = [2, 0, -2, 2, 0, -2]$，\n      $u_3 = [1, -1, 1, -1, 1, -1]$。\n    - $X$ 的列为\n      $f_1 = u_1$，\n      $f_2 = u_2$，\n      $f_3 = u_3$，\n      $f_4 = f_1 + f_2$，\n      $f_5 = f_1 - f_2$，\n      $f_6 = 2 f_1$，\n      $f_7 = f_2 - f_3$，\n      $f_8 = f_3$。\n    - 终点 $y = f_6$。\n    - 水平：$\\alpha = 0.01$， $q = 0.05$， $\\tau = 0.90$。\n\n- 情况4（待忽略的常数特征；不同尺度下的重复信号）：\n    - $n = 6$， $p = 5$。\n    - $X$ 的列为\n      $g_1 = [0, 0, 0, 0, 0, 0]$，\n      $g_2 = [1, 0, -1, 1, 0, -1]$，\n      $g_3 = [-2, -1, 0, 0, 1, 2]$，\n      $g_4 = [1, 2, 1, -1, -2, -1]$，\n      $g_5 = [3, 0, -3, 3, 0, -3]$。\n    - 终点 $y = g_5$。\n    - 水平：$\\alpha = 0.05$， $q = 0.10$， $\\tau = 0.80$。\n\n输出规范：\n- 对于每个用例，计算如上定义的元组 $[B, H, K]$。\n- 您的程序应生成单行输出，其中包含四个测试用例的结果，格式为逗号分隔的列表，并用方括号括起来，每个用例的结果本身也是一个列表。例如，一个带有通用占位符的输出示例如下 $[[B_1, H_1, K_1],[B_2, H_2, K_2],[B_3, H_3, K_3],[B_4, H_4, K_4]]$。", "solution": "该问题要求实现一个统计流程来检验放射组学假说。这涉及到通过带有多种比较校正的假设检验进行特征选择，然后使用主成分分析（PCA）进行降维。该过程对多个测试用例执行，每个测试用例由一个特征矩阵 $X$、一个终点向量 $y$ 和一组参数 $(\\alpha, q, \\tau)$ 定义。每个用例所需的输出是一个三元组 $[B, H, K]$，分别代表通过 Bonferroni 和 Benjamini-Hochberg 方法发现的显著特征数量，以及解释目标方差所需的主成分数量。\n\n该编程解决方案被构建为一系列明确定义的步骤，严格遵守所提供的统计和数学形式化表述。\n\n**第1步：数据标准化**\n对于每个测试用例，我们给定一个特征矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 和一个终点向量 $y \\in \\mathbb{R}^{n}$，其中 $n$ 是样本数，$p$ 是特征数。第一步是标准化数据。\n\n对于每个特征列 $X_{\\cdot j}$（$j=1, \\dots, p$），我们计算其样本均值 $\\bar{x}_j$ 和样本标准差 $s_j$。样本标准差使用 $n-1$ 的自由度（贝塞尔校正 Bessel's correction）计算：\n$$\ns_j = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (x_{ij} - \\bar{x}_j)^2}\n$$\n识别出 $s_j = 0$ 的特征，并将其从所有后续分析中排除。设 $m$ 是 $s_j  0$ 的特征数量。我们构建一个新矩阵 $Z \\in \\mathbb{R}^{n \\times m}$，其中只包含有效特征的标准化列。每个列 $Z_{\\cdot j}$ 计算如下：\n$$\nZ_{ij} = \\frac{x_{ij} - \\bar{x}_j}{s_j}\n$$\n同样地，终点向量 $y$ 被标准化为 $z_y$，其样本均值为0，样本标准差为1。问题陈述保证 $y$ 的样本方差非零。\n\n如果没有特征具有非零标准差（$m=0$），则该用例的分析终止，结果为 $[0, 0, 0]$。\n\n**第2步：皮尔逊相关性**\n对于标准化的特征 $Z$ 和标准化的终点 $z_y$，每个有效特征 $j$ 的样本皮尔逊相关系数 $r_j$ 的计算得以简化。由于任何标准化变量的样本均值为0，样本标准差为1，因此相关系数等价于样本协方差：\n$$\nr_j = \\frac{\\text{cov}(Z_{\\cdot j}, z_y)}{s_{Z_{\\cdot j}} s_{z_y}} = \\frac{\\frac{1}{n-1}\\sum_{i=1}^n (Z_{ij} - \\bar{Z}_j)(z_{y,i} - \\bar{z}_y)}{1 \\cdot 1} = \\frac{1}{n-1}\\sum_{i=1}^n Z_{ij} z_{y,i}\n$$\n对所有 $m$ 个有效特征执行此计算，得到一个相关系数向量 $\\{r_j\\}_{j=1}^m$。\n\n**第3步：P值计算**\n对于每个相关系数 $r_j$，我们检验原假设 $H_{0j}: \\rho_j = 0$。在原假设下，统计量\n$$\nt_j = r_j \\sqrt{\\frac{n-2}{1-r_j^2}}\n$$\n服从自由度为 $\\nu = n-2$ 的学生t分布。这在 $|r_j|  1$ 时成立。如果 $|r_j|=1$，表示完全相关，分母变为零。在这种情况下，根据连续性，我们将p值设为0。对于 $|r_j|1$，双边p值 $p_j$ 计算为观察到至少与 $|t_j|$ 一样极端的检验统计量的概率：\n$$\np_j = 2 \\cdot P(T_\\nu \\ge |t_j|) = 2 \\left(1 - F_{t_{n-2}}(|t_j|)\\right)\n$$\n其中 $F_{t_{n-2}}$ 是自由度为 $n-2$ 的学生t分布的累积分布函数（CDF）。\n\n**第4步：Bonferroni校正（FWER控制）**\n为了控制 $m$ 个同时检验的族内错误率（FWER），应用了 Bonferroni 校正。给定一个显著性水平 $\\alpha$，每个检验的校正后显著性阈值为 $\\alpha_{bonf} = \\alpha/m$。如果一个原假设 $H_{0j}$ 对应的p值 $p_j \\le \\alpha_{bonf}$，则拒绝该假设。计算此类拒绝的总数以确定 $B$ 的值。\n\n**第5步：Benjamini-Hochberg程序（FDR控制）**\n为了控制错误发现率（FDR），在目标水平 $q$ 下应用了 Benjamini-Hochberg (BH) 程序。将 $m$ 个p值按升序排序：$p_{(1)} \\le p_{(2)} \\le \\cdots \\le p_{(m)}$。找到满足以下条件的最大索引 $k$：\n$$\np_{(k)} \\le \\frac{k}{m}q\n$$\nBH程序规定拒绝所有与p值 $p_{(1)}, \\dots, p_{(k)}$ 对应的原假设。拒绝的总数为 $H=k$。如果不存在这样的 $k$，则 $H=0$。\n\n**第6步：主成分分析（PCA）**\n在标准化的有效特征矩阵 $Z \\in \\mathbb{R}^{n \\times m}$ 上执行PCA。主成分源自样本协方差矩阵 $\\text{Cov}(Z) = \\frac{1}{n-1} Z^\\top Z$ 的特征分解。计算该矩阵特征值的一种高效方法是通过对 $Z$ 进行奇异值分解（SVD）。\n设 $Z = U S V^\\top$ 是 $Z$ 的SVD，其中 $S$ 是奇异值 $S_i$ 的对角矩阵。协方差矩阵的特征值 $\\lambda_i$ 与奇异值通过以下关系相关联：\n$$\n\\lambda_i = \\frac{S_i^2}{n-1}\n$$\n这些特征值按降序排序。第 $i$ 个主成分解释的总方差比例为 $\\lambda_i / \\sum_{j=1}^r \\lambda_j$，其中 $r$ 是 $Z$ 的秩（非零特征值的数量）。目标是找到最小的主成分数量 $K$，使得累积解释方差比例达到或超过目标阈值 $\\tau$：\n$$\n\\text{找到最小的 } K \\text{ 使得 } \\frac{\\sum_{i=1}^K \\lambda_i}{\\sum_{j=1}^r \\lambda_j} \\ge \\tau\n$$\n这个 $K$ 值代表了为捕获至少比例为 $\\tau$ 的总方差所需的特征集的内在维度。\n\n通过对每个测试用例执行这六个步骤，我们得出所需的整数三元组 $[B, H, K]$。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and print results in the specified format.\n    \"\"\"\n    # Case 1: Happy path, orthogonal features with a perfectly associated endpoint\n    v1 = np.array([1, 1, 1, 1, -1, -1, -1, -1], dtype=float)\n    v2 = np.array([1, 1, -1, -1, 1, 1, -1, -1], dtype=float)\n    v3 = np.array([1, -1, 1, -1, 1, -1, 1, -1], dtype=float)\n    v4 = np.array([1, -1, -1, 1, -1, 1, 1, -1], dtype=float)\n    case1_X = np.array([v1, v2, v3, v4]).T\n    case1_y = v3\n    case1_params = (0.05, 0.10, 0.95)\n\n    # Case 2: Boundary with no signal; endpoint orthogonal to all features\n    case2_X = np.array([v1, v2, v3, v4]).T\n    case2_y = np.array([1, -1, 1, -1, -1, 1, -1, 1], dtype=float)\n    case2_params = (0.05, 0.10, 0.50)\n\n    # Case 3: Highly collinear features, p > n\n    u1 = np.array([1, 2, 3, -3, -2, -1], dtype=float)\n    u2 = np.array([2, 0, -2, 2, 0, -2], dtype=float)\n    u3 = np.array([1, -1, 1, -1, 1, -1], dtype=float)\n    f1, f2, f3 = u1, u2, u3\n    f4, f5, f6 = f1 + f2, f1 - f2, 2 * f1\n    f7, f8 = f2 - f3, f3\n    case3_X = np.array([f1, f2, f3, f4, f5, f6, f7, f8]).T\n    case3_y = f6\n    case3_params = (0.01, 0.05, 0.90)\n\n    # Case 4: Constant feature to be ignored; duplicated signal\n    g1 = np.array([0, 0, 0, 0, 0, 0], dtype=float)\n    g2 = np.array([1, 0, -1, 1, 0, -1], dtype=float)\n    g3 = np.array([-2, -1, 0, 0, 1, 2], dtype=float)\n    g4 = np.array([1, 2, 1, -1, -2, -1], dtype=float)\n    g5 = np.array([3, 0, -3, 3, 0, -3], dtype=float)\n    case4_X = np.array([g1, g2, g3, g4, g5]).T\n    case4_y = g5\n    case4_params = (0.05, 0.10, 0.80)\n\n    test_cases = [\n        (case1_X, case1_y, *case1_params),\n        (case2_X, case2_y, *case2_params),\n        (case3_X, case3_y, *case3_params),\n        (case4_X, case4_y, *case4_params),\n    ]\n\n    results = []\n    for X, y, alpha, q, tau in test_cases:\n        result = process_case(X, y, alpha, q, tau)\n        results.append(result)\n\n    # Format output string to be exactly [[B1,H1,K1],[B2,H2,K2],...]\n    results_str = [f\"[{b},{h},{k}]\" for b, h, k in results]\n    print(f\"[{','.join(results_str)}]\")\n\ndef process_case(X, y, alpha, q, tau):\n    \"\"\"\n    Executes the full statistical pipeline for a single test case.\n    \"\"\"\n    n, p = X.shape\n    \n    # --- Step 1: Standardization ---\n    std_devs = np.std(X, axis=0, ddof=1)\n    valid_feature_indices = np.where(std_devs > 1e-9)[0] # Use tolerance for float\n    \n    m = len(valid_feature_indices)\n    if m == 0:\n        return [0, 0, 0]\n\n    X_valid = X[:, valid_feature_indices]\n    \n    mean_X = np.mean(X_valid, axis=0)\n    std_X = np.std(X_valid, axis=0, ddof=1)\n    Z = (X_valid - mean_X) / std_X\n    \n    mean_y = np.mean(y)\n    std_y = np.std(y, ddof=1)\n    z_y = (y - mean_y) / std_y\n    \n    # --- Step 2: Pearson Correlation ---\n    r_values = (1 / (n - 1)) * (z_y @ Z)\n    \n    # --- Step 3: P-Value Calculation ---\n    p_values = np.zeros(m)\n    df = n - 2\n    if df = 0: # Cannot form t-statistic\n        return [0, 0, 0] # Or handle as error\n        \n    for j in range(m):\n        r = r_values[j]\n        if abs(r) >= 1.0:\n            p_values[j] = 0.0\n        else:\n            t_stat = r * np.sqrt(df / (1 - r**2))\n            p_values[j] = 2 * (1 - t.cdf(np.abs(t_stat), df))\n\n    # --- Step 4: Bonferroni Correction ---\n    alpha_bonf = alpha / m\n    B = np.sum(p_values = alpha_bonf)\n\n    # --- Step 5: Benjamini-Hochberg Procedure ---\n    sorted_indices = np.argsort(p_values)\n    sorted_p_values = p_values[sorted_indices]\n    \n    k = np.arange(1, m + 1)\n    bh_thresholds = (k / m) * q\n    \n    rejections = sorted_p_values = bh_thresholds\n    \n    H = 0\n    if np.any(rejections):\n        H = np.max(np.where(rejections)[0]) + 1\n        \n    # --- Step 6: Principal Component Analysis (PCA) ---\n    if m == 0:\n        K = 0\n    else:\n        # SVD on the standardized matrix Z\n        # full_matrices=False for thin SVD, efficient for n  m\n        _, s, _ = np.linalg.svd(Z, full_matrices=False)\n        \n        # Calculate eigenvalues of covariance matrix from singular values\n        # Add small epsilon to prevent division by zero if all variances are zero\n        eigenvalues = (s**2) / (n - 1)\n        total_variance = np.sum(eigenvalues)\n        \n        if total_variance  1e-9:\n             K = 0\n        else:\n            explained_variance_ratio = eigenvalues / total_variance\n            cumulative_variance = np.cumsum(explained_variance_ratio)\n            # Find the first index where cumulative variance exceeds tau\n            K_candidates = np.where(cumulative_variance >= tau)[0]\n            if len(K_candidates) == 0:\n                K = m # All components are needed if tau is not reached (e.g. tau > 1)\n            else:\n                K = K_candidates[0] + 1\n                \n    return [B, H, int(K)]\n\nif __name__ == '__main__':\n    solve()\n```", "id": "4567514"}, {"introduction": "纵向放射组学通过随时间追踪特征变化，为监测治疗反应或疾病进展提供了有力工具。这项实践要求你将“放射组学假说”应用于时间序列数据，开发一个算法来量化评估特征的时间动态是否符合预期。通过这项练习[@problem_id:4567515]，你将学会如何区分一个平滑、连贯的生物信号与测量噪声，这是验证放射组学特征动态有效性的核心技能。", "problem": "请思考影像组学假说，该假说认为，定量的影像衍生特征是可测量的函数，其编码了潜在的生物学状态，并且在受控干预下，其时间演化是连贯、平滑的，且其变化可以被检测到，并能与测量噪声区分开来。在数学上，我们将一个影像组学特征视为一个实值时间序列 $x(t)$，在离散时间点 $t_0, t_1, \\dots, t_{N-1}$（以天为单位）进行观测。测得的特征值建模为 $x_i = s_i + n_i$，其中 $i \\in \\{0,1,\\dots,N-1\\}$，$s_i$ 表示潜在的生物学信号，$n_i$ 表示测量噪声。假设 $n_i$ 是从均值为零、方差为 $\\sigma_n^2$ 的已知正态分布中抽取的独立同分布随机变量。\n\n当以下三个标准同时成立时，时间影像组学检验应接受一个序列，认为其与影像组学假说一致：\n\n1. 特征值与时间之间存在强单调关联，其方向符合预期方向 $d \\in \\{-1, +1\\}$，其中 $d = +1$ 表示预期增加，$d = -1$ 表示预期减少。\n2. 相对于其总体变异性，该序列具有较低的离散曲率，即在时间上是平滑的。\n3. 信噪比足够高，表明观测到的变异性主要由潜在的生物学信号主导，而非测量噪声。\n\n基于以下基本原则：\n- 基于 $(t_i, x_i)$ 之间成对顺序关系的单调关联定义。\n- 使用基于索引的局部差异，将在均匀或非均匀采样序列上定义的离散二阶差分作为曲率的有限差分近似。\n- 对于加性噪声模型 $x = s + n$（其中 $n$ 独立于 $s$），进行方差分解，意味着 $\\operatorname{Var}(x) = \\operatorname{Var}(s) + \\sigma_n^2$。\n\n设计一个算法，该算法根据一个将上述三个标准操作化的决策规则，为每个测试用例决定接受或拒绝。对所有用例使用以下固定阈值：最小单调关联阈值 $\\tau_{\\min} = 0.8$，最大归一化曲率能量 $E_{\\max} = 0.25$，以及最小信噪比（SNR）阈值 $\\mathrm{SNR}_{\\min} = 1.0$。\n\n给定五个测试用例。对于每个用例，输入为时间点 $t$、特征值 $x$、预期方向 $d$ 和噪声标准差 $\\sigma_n$。所有时间值均以天为单位；不涉及角度。请实现该决策规则，并为每个用例生成一个布尔值：如果序列被接受，则为 $\\text{True}$，否则为 $\\text{False}$。您的程序应生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的结果列表（例如，$[\\text{True},\\text{False},\\dots]$）。\n\n测试套件：\n- 用例 1（每周采样的平滑递减序列）：\n  - $t = [0, 7, 14, 21, 28, 35]$\n  - $x = [1.00, 0.86, 0.73, 0.62, 0.52, 0.45]$\n  - $d = -1$\n  - $\\sigma_n = 0.02$\n- 用例 2（接近阈值的非单调性）：\n  - $t = [0, 7, 14, 21, 28, 35]$\n  - $x = [1.00, 0.90, 0.88, 0.87, 0.89, 0.88]$\n  - $d = -1$\n  - $\\sigma_n = 0.02$\n- 用例 3（恒定序列）：\n  - $t = [0, 5, 10, 15, 20]$\n  - $x = [0.70, 0.70, 0.70, 0.70, 0.70]$\n  - $d = -1$\n  - $\\sigma_n = 0.02$\n- 用例 4（大致每三周采样的振荡序列）：\n  - $t = [0, 4, 8, 12, 16, 20, 24]$\n  - $x = [0.90, 1.00, 0.85, 1.05, 0.80, 1.10, 0.75]$\n  - $d = -1$\n  - $\\sigma_n = 0.03$\n- 用例 5（大致每两周采样的平滑递增序列）：\n  - $t = [0, 6, 12, 18, 24, 30]$\n  - $x = [0.50, 0.58, 0.66, 0.75, 0.83, 0.90]$\n  - $d = +1$\n  - $\\sigma_n = 0.02$\n\n您的程序必须根据上述基本原则实现决策规则，并计算五个用例的布尔结果。最终输出必须是单行文本，包含格式完全符合 $[\\text{result1},\\text{result2},\\text{result3},\\text{result4},\\text{result5}]$ 的布尔值列表。", "solution": "问题陈述已经过分析，并被认为是有效的。它在科学上基于影像组学和信号处理的原理，是一个具有明确输入和目标的适定问题，并以客观、可形式化的语言表述。任务是将用于评估时间影像组学序列的三个标准操作化，并将其应用于一组测试用例。\n\n该算法将根据单调关联、时间平滑度和信噪比这三个标准，评估在时间点 $t_0, t_1, \\dots, t_{N-1}$ 观测到的每个时间序列 $x(t)$。一个序列当且仅当使用所提供的阈值同时满足所有三个标准时，才被接受为与影像组学假说一致。\n\n设一个序列的给定数据为时间点 $t$、特征值 $x$、预期的变化方向 $d$ 和噪声标准差 $\\sigma_n$。观测点数量为 $N$。阈值为 $\\tau_{\\min} = 0.8$，$E_{\\max} = 0.25$ 和 $\\mathrm{SNR}_{\\min} = 1.0$。\n\n### 标准 1：单调关联\n\n此标准评估序列是否在预期方向 $d \\in \\{-1, +1\\}$上表现出强烈的单调趋势。它通过计算基于成对顺序关系的一致性分数 $M$ 来实现操作化。对于一对观测值 $(x_i, x_j)$，其中 $i  j$（因此 $t_i  t_j$），如果 $x$ 变化的符号与预期方向 $d$ 匹配，则认为该对是一致的。在数学上，如果 $(x_j - x_i)d  0$，则该对是一致的。\n\n指标 $M$ 定义为一致对在所有可能唯一时间点对中所占的比例。\n设 $N$ 为观测点数量。$i  j$ 的点对总数为 $N_p = \\frac{N(N-1)}{2}$。\n设 $N_c$ 为一致对的数量，由下式给出：\n$$\nN_c = \\sum_{i=0}^{N-2} \\sum_{j=i+1}^{N-1} \\mathbf{1}[(x_j - x_i)d  0]\n$$\n其中 $\\mathbf{1}[\\cdot]$ 是指示函数。单调关联指标为：\n$$\nM = \\frac{N_c}{N_p}\n$$\n如果 $M \\ge \\tau_{\\min}$，则序列通过此标准。\n\n### 标准 2：时间平滑度\n\n此标准评估序列是否平滑，即其曲率是否较低。曲率通过离散二阶差分来近似，该差分按规定使用基于索引的局部差异计算。指标“归一化曲率能量” $E$ 将曲率的能量与序列的总体变异性联系起来。\n\n索引 $i$ 处的离散二阶差分为 $\\Delta^2 x_i = x_{i+2} - 2x_{i+1} + x_i$。计算范围为 $i \\in \\{0, 1, \\dots, N-3\\}$。\n曲率能量 $N_E$ 是这些二阶差分的平方和：\n$$\nN_E = \\sum_{i=0}^{N-3} (\\Delta^2 x_i)^2 = \\sum_{i=0}^{N-3} (x_{i+2} - 2x_{i+1} + x_i)^2\n$$\n该能量通过序列变异性的总能量 $D_E$ 进行归一化，后者是与均值 $\\bar{x}$ 的平方偏差之和：\n$$\nD_E = \\sum_{i=0}^{N-1} (x_i - \\bar{x})^2, \\quad \\text{其中 } \\bar{x} = \\frac{1}{N}\\sum_{k=0}^{N-1} x_k\n$$\n归一化曲率能量是它们的比率：\n$$\nE = \\frac{N_E}{D_E}\n$$\n在序列为常数的特殊情况下，$D_E=0$。此时，分子 $N_E$ 也为 $0$，因为常数序列的曲率为零。我们定义常数序列的 $E=0$，以反映其完美的平滑度。\n如果 $E \\le E_{\\max}$，则序列通过此标准。\n\n### 标准 3：信噪比（SNR）\n\n此标准确保潜在生物学信号 $s$ 的变异性远大于测量噪声 $n$ 的变异性。模型为 $x_i = s_i + n_i$。鉴于信号和噪声的独立性，它们的方差相加：$\\operatorname{Var}(x) = \\operatorname{Var}(s) + \\operatorname{Var}(n)$。噪声方差 $\\sigma_n^2$ 是已知的。\n\n我们首先使用无偏样本方差来估计观测序列 $x$ 的方差：\n$$\n\\hat{\\sigma}_x^2 = \\frac{1}{N-1} \\sum_{i=0}^{N-1} (x_i - \\bar{x})^2\n$$\n据此，我们估计潜在信号 $s$ 的方差：\n$$\n\\hat{\\sigma}_s^2 = \\hat{\\sigma}_x^2 - \\sigma_n^2\n$$\n由于方差不能为负，我们通过取 $\\hat{\\sigma}_s^2 = \\max(0, \\hat{\\sigma}_x^2 - \\sigma_n^2)$ 来强制执行此条件。\n信噪比（SNR）是估计的信号方差与已知的噪声方差之比：\n$$\n\\mathrm{SNR} = \\frac{\\hat{\\sigma}_s^2}{\\sigma_n^2} = \\frac{\\max(0, \\hat{\\sigma}_x^2 - \\sigma_n^2)}{\\sigma_n^2}\n$$\n这假设 $\\sigma_n  0$，所有测试用例均满足此条件。\n如果 $\\mathrm{SNR} \\ge \\mathrm{SNR}_{\\min}$，则序列通过此标准。\n\n### 最终决策规则\n\n一个测试用例被接受，其结果为布尔值 `True`，当且仅当所有三个条件都满足时：\n$$\n(M \\ge \\tau_{\\min}) \\land (E \\le E_{\\max}) \\land (\\mathrm{SNR} \\ge \\mathrm{SNR}_{\\min})\n$$\n否则，结果为 `False`。该算法将此逻辑应用于每个给定的测试用例。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the radiomics hypothesis test on the given suite of cases.\n    \"\"\"\n    \n    # Define the fixed thresholds from the problem statement.\n    TAU_MIN = 0.8\n    E_MAX = 0.25\n    SNR_MIN = 1.0\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: smooth decreasing series with weekly sampling\n        {\n            \"t\": [0, 7, 14, 21, 28, 35],\n            \"x\": [1.00, 0.86, 0.73, 0.62, 0.52, 0.45],\n            \"d\": -1,\n            \"sigma_n\": 0.02\n        },\n        # Case 2: near-threshold non-monotonicity\n        {\n            \"t\": [0, 7, 14, 21, 28, 35],\n            \"x\": [1.00, 0.90, 0.88, 0.87, 0.89, 0.88],\n            \"d\": -1,\n            \"sigma_n\": 0.02\n        },\n        # Case 3: constant series\n        {\n            \"t\": [0, 5, 10, 15, 20],\n            \"x\": [0.70, 0.70, 0.70, 0.70, 0.70],\n            \"d\": -1,\n            \"sigma_n\": 0.02\n        },\n        # Case 4: oscillatory series with roughly triweekly sampling\n        {\n            \"t\": [0, 4, 8, 12, 16, 20, 24],\n            \"x\": [0.90, 1.00, 0.85, 1.05, 0.80, 1.10, 0.75],\n            \"d\": -1,\n            \"sigma_n\": 0.03\n        },\n        # Case 5: smooth increasing series with approximately biweekly sampling\n        {\n            \"t\": [0, 6, 12, 18, 24, 30],\n            \"x\": [0.50, 0.58, 0.66, 0.75, 0.83, 0.90],\n            \"d\": +1,\n            \"sigma_n\": 0.02\n        }\n    ]\n\n    def evaluate_series(x_vals, d, sigma_n):\n        \"\"\"\n        Evaluates a single time series against the three criteria.\n        \n        Args:\n            x_vals (list): The feature values.\n            d (int): The expected direction of change (-1 or +1).\n            sigma_n (float): The noise standard deviation.\n\n        Returns:\n            bool: True if the series is accepted, False otherwise.\n        \"\"\"\n        x = np.array(x_vals, dtype=float)\n        n = len(x)\n\n        if n  2:\n            return False\n\n        # Criterion 1: Monotonic Association\n        num_pairs = n * (n - 1) / 2\n        if num_pairs == 0:\n            monotonicity_score = 0.0\n        else:\n            concordant_pairs = 0\n            for i in range(n):\n                for j in range(i + 1, n):\n                    if (x[j] - x[i]) * d > 0:\n                        concordant_pairs += 1\n            monotonicity_score = concordant_pairs / num_pairs\n\n        c1_pass = (monotonicity_score >= TAU_MIN)\n\n        # Criterion 2: Temporal Smoothness (Normalized Curvature Energy)\n        if n  3:\n            # Curvature is not defined for less than 3 points\n            # A series of 2 points is perfectly smooth (a line).\n            norm_curvature_energy = 0.0\n        else:\n            x_mean = np.mean(x)\n            denominator_energy = np.sum((x - x_mean)**2)\n            \n            if denominator_energy == 0:\n                # Perfectly constant series has zero curvature.\n                norm_curvature_energy = 0.0\n            else:\n                second_diffs = x[2:] - 2 * x[1:-1] + x[:-2]\n                numerator_energy = np.sum(second_diffs**2)\n                norm_curvature_energy = numerator_energy / denominator_energy\n\n        c2_pass = (norm_curvature_energy = E_MAX)\n\n        # Criterion 3: Signal-to-Noise Ratio (SNR)\n        noise_var = sigma_n**2\n        if noise_var == 0:\n             # If no noise is expected, any variance is signal\n             # This case is not in the test suite but is handled for robustness.\n             snr = float('inf')\n        else:\n            # Use ddof=1 for unbiased sample variance\n            obs_var = np.var(x, ddof=1) if n > 1 else 0.0\n            signal_var = max(0, obs_var - noise_var)\n            snr = signal_var / noise_var\n            \n        c3_pass = (snr >= SNR_MIN)\n        \n        return c1_pass and c2_pass and c3_pass\n\n    results = []\n    for case in test_cases:\n        # Time 't' is not explicitly used in the metric calculations as per\n        # problem interpretation (e.g., \"index-local differences\").\n        result = evaluate_series(case[\"x\"], case[\"d\"], case[\"sigma_n\"])\n        results.append(result)\n    \n    # Format the final output as a string list of booleans.\n    # JSON-style boolean format in Python is \"True\" / \"False\".\n    # The problem asks for this format, e.g. `[True,False,...]`.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "4567515"}, {"introduction": "在观察性研究中，混杂偏倚是一个核心挑战，它可能导致对变量间关系的错误解读。这项实践利用结构因果模型，探讨了一个在放射组学中至关重要且微妙的问题：当我们使用一个放射组学特征作为不可观测生物状态的“代理变量”来校正混杂时，是否总能改进我们对治疗效果的估计？通过这个练习[@problem_id:4567522]，你将从数学上揭示使用代理变量进行调整的潜在风险，例如在某些条件下反而会放大偏倚。", "problem": "考虑放射组学假说，该假说认为，从图像中定量提取的特征能够捕捉到与疾病过程相关的潜在生物学状态的信息。使用线性高斯结构因果模型 (SCM) 将此思想形式化，并分析使用带噪声的放射组学代理变量时的混淆和调整。设潜在生物学状态为标量随机变量 $B$，处理为标量随机变量 $T$，放射组学特征为标量随机变量 $R$，结果为标量随机变量 $Y$。假设变量均值为零，且噪声项相互独立。SCM 模型如下：\n$$\nB \\sim \\mathcal{N}(0,\\sigma_B^2), \\quad U_T \\sim \\mathcal{N}(0,\\sigma_T^2), \\quad U_Y \\sim \\mathcal{N}(0,\\sigma_Y^2), \\quad \\varepsilon_R \\sim \\mathcal{N}(0,\\sigma_R^2),\n$$\n其结构方程为\n$$\nT = \\alpha B + U_T, \\quad R = B + \\varepsilon_R, \\quad Y = \\tau T + \\gamma B + U_Y.\n$$\n在此模型中，$R$ 是潜在生物学状态 $B$ 的一个带噪声的代理变量，这与放射组学假说（即放射组学特征编码了潜在的生物学信息）相符。参数 $\\tau$ 是平均处理效应 (ATE)，我们的目标是从观测数据中估计 $\\tau$。你将推导两种估计量的偏差：\n1. 仅将 $Y$ 对 $T$ 进行回归的朴素普通最小二乘法 (OLS) 估计量。\n2. 将 $Y$ 对 $T$ 和 $R$ 同时进行回归的调整后 OLS 估计量。\n\n从以下基本原理出发：\n- 期望、方差和协方差的定义，以及期望的线性性质。\n- 对于任意零均值随机变量 $X$ 和 $Z$，$\\operatorname{Var}(X) = \\mathbb{E}[X^2]$ 且 $\\operatorname{Cov}(X,Z) = \\mathbb{E}[XZ]$。\n- 对于具有独立噪声的线性高斯模型，协方差可通过线性传播和独立性假设获得。\n- 经过充分验证的 OLS 恒等式：对于 $Y$ 对 $T$ 的简单回归，其系数为 $\\beta_T = \\operatorname{Cov}(Y,T)/\\operatorname{Var}(T)$。对于 $Y$ 对 $(T,R)$ 的多元回归，$T$ 的系数为\n$$\n\\beta_T^{\\text{adj}} = \\frac{\\operatorname{Cov}(Y,T)\\operatorname{Var}(R) - \\operatorname{Cov}(Y,R)\\operatorname{Cov}(T,R)}{\\operatorname{Var}(T)\\operatorname{Var}(R) - \\operatorname{Cov}(T,R)^2}.\n$$\n\n你的任务是：\n- 从上述基本原理出发，推导出 $\\operatorname{Var}(T)$、$\\operatorname{Var}(R)$、$\\operatorname{Cov}(B,T)$、$\\operatorname{Cov}(T,R)$、$\\operatorname{Cov}(Y,T)$ 和 $\\operatorname{Cov}(Y,R)$ 的闭式表达式，用 $(\\alpha, \\tau, \\gamma, \\sigma_B^2, \\sigma_T^2, \\sigma_R^2)$ 表示。\n- 使用这些表达式，推导出朴素估计量 $\\beta_T$ 并计算其偏差 $b_{\\text{naive}} = \\beta_T - \\tau$。\n- 使用上述多元回归恒等式，推导出调整后估计量 $\\beta_T^{\\text{adj}}$ 并计算其偏差 $b_{\\text{adj}} = \\beta_T^{\\text{adj}} - \\tau$。\n- 实现一个程序，该程序为每个给定的测试用例参数集计算偏差对 $(b_{\\text{naive}}, b_{\\text{adj}})$ 和一个布尔值，该布尔值指示使用放射组学代理变量进行调整是否减小了绝对偏差，即 $|b_{\\text{adj}}|  |b_{\\text{naive}}|$ 是否成立。\n\n数值测试套件：\n使用以下参数集，每个参数集指定为 $(\\alpha, \\tau, \\gamma, \\sigma_B^2, \\sigma_T^2, \\sigma_R^2)$：\n- 案例 1 (无混淆)：$(0.0, 1.2, 0.7, 1.0, 1.0, 0.5)$。\n- 案例 2 (存在混淆，强代理变量)：$(1.0, 1.0, 1.0, 1.0, 0.1, 0.01)$。\n- 案例 3 (存在混淆，弱代理变量和测量误差放大)：$(1.0, 1.0, 1.0, 1.0, 0.1, 10.0)$。\n- 案例 4 (存在混淆，完美代理变量)：$(1.0, 0.5, 2.0, 2.0, 0.2, 0.0)$。\n- 案例 5 (随机化处理)：$(0.0, 0.8, 1.5, 5.0, 2.0, 1.0)$。\n- 案例 6 (无生物学变异)：$(2.0, 1.0, 3.0, 0.0, 1.0, 0.5)$。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。对于每个案例，按顺序附加三个值：$b_{\\text{naive}}$、$b_{\\text{adj}}$ 和布尔值 $|b_{\\text{adj}}|  |b_{\\text{naive}}|$。因此，最终输出包含对应于 6 个案例的 18 个条目。", "solution": "我们使用线性高斯结构因果模型 (SCM) 对放射组学假说下的混淆和调整进行建模。假设潜在生物学状态 $B$ 对处理 $T$ 和结果 $Y$ 都有因果影响，而放射组学特征 $R$ 是 $B$ 的一个带噪声的代理变量。目标是估计平均处理效应 (ATE) $\\tau$，并量化在使用代理变量 $R$ 进行朴素估计和调整后的混淆偏差。\n\n我们从基本定义开始。所有变量的均值为零，因此对于任意随机变量 $X$ 和 $Z$：\n$$\n\\operatorname{Var}(X) = \\mathbb{E}[X^2], \\quad \\operatorname{Cov}(X,Z) = \\mathbb{E}[XZ].\n$$\n噪声项的独立性意味着独立分量之间的交叉协方差为零。结构方程为：\n$$\nT = \\alpha B + U_T, \\quad R = B + \\varepsilon_R, \\quad Y = \\tau T + \\gamma B + U_Y.\n$$\n利用线性和独立性，我们推导出普通最小二乘法 (OLS) 所需的二阶矩。\n\n首先，计算 $\\operatorname{Var}(T)$ 和 $\\operatorname{Cov}(B,T)$：\n$$\n\\operatorname{Var}(T) = \\operatorname{Var}(\\alpha B + U_T) = \\alpha^2 \\operatorname{Var}(B) + \\operatorname{Var}(U_T) = \\alpha^2 \\sigma_B^2 + \\sigma_T^2,\n$$\n$$\n\\operatorname{Cov}(B,T) = \\operatorname{Cov}(B, \\alpha B + U_T) = \\alpha \\operatorname{Var}(B) + 0 = \\alpha \\sigma_B^2.\n$$\n接下来，计算 $\\operatorname{Var}(R)$ 和 $\\operatorname{Cov}(T,R)$：\n$$\n\\operatorname{Var}(R) = \\operatorname{Var}(B + \\varepsilon_R) = \\operatorname{Var}(B) + \\operatorname{Var}(\\varepsilon_R) = \\sigma_B^2 + \\sigma_R^2,\n$$\n$$\n\\operatorname{Cov}(T,R) = \\operatorname{Cov}(\\alpha B + U_T, B + \\varepsilon_R) = \\alpha \\operatorname{Var}(B) + 0 + 0 + 0 = \\alpha \\sigma_B^2.\n$$\n现在计算 $Y$ 与 $T$ 和 $R$ 的协方差。使用 $Y = \\tau T + \\gamma B + U_Y$：\n$$\n\\operatorname{Cov}(Y,T) = \\operatorname{Cov}(\\tau T + \\gamma B + U_Y, T) = \\tau \\operatorname{Var}(T) + \\gamma \\operatorname{Cov}(B,T) + 0 = \\tau(\\alpha^2 \\sigma_B^2 + \\sigma_T^2) + \\gamma \\alpha \\sigma_B^2,\n$$\n$$\n\\operatorname{Cov(Y,R)} = \\operatorname{Cov}(\\tau T + \\gamma B + U_Y, R) = \\tau \\operatorname{Cov}(T,R) + \\gamma \\operatorname{Cov}(B,R) + 0 = \\tau \\alpha \\sigma_B^2 + \\gamma \\sigma_B^2.\n$$\n\n我们分析两种用于估计 $T$ 对 $Y$ 影响的估计量。\n\n朴素 OLS 估计量（仅将 $Y$ 对 $T$ 回归）。$T$ 的系数是\n$$\n\\beta_T = \\frac{\\operatorname{Cov}(Y,T)}{\\operatorname{Var}(T)} = \\frac{\\tau(\\alpha^2 \\sigma_B^2 + \\sigma_T^2) + \\gamma \\alpha \\sigma_B^2}{\\alpha^2 \\sigma_B^2 + \\sigma_T^2} = \\tau + \\frac{\\gamma \\alpha \\sigma_B^2}{\\alpha^2 \\sigma_B^2 + \\sigma_T^2}.\n$$\n因此，朴素偏差为\n$$\nb_{\\text{naive}} = \\beta_T - \\tau = \\frac{\\gamma \\alpha \\sigma_B^2}{\\alpha^2 \\sigma_B^2 + \\sigma_T^2}.\n$$\n当通过 $B$ 存在混淆时（$\\alpha \\neq 0$ 且 $\\gamma \\neq 0$），该项非零；如果处理是随机化的（$\\alpha = 0$）或生物学没有变异（$\\sigma_B^2 = 0$），则该项为零。\n\n调整后 OLS 估计量（将 $Y$ 对 $T$ 和 $R$ 同时回归）。在 $Y$ 对 $(T,R)$ 的多元回归中，$T$ 的系数是经过充分验证的恒等式\n$$\n\\beta_T^{\\text{adj}} = \\frac{\\operatorname{Cov}(Y,T)\\operatorname{Var}(R) - \\operatorname{Cov}(Y,R)\\operatorname{Cov}(T,R)}{\\operatorname{Var}(T)\\operatorname{Var}(R) - \\operatorname{Cov}(T,R)^2}.\n$$\n定义分母\n$$\nD = \\operatorname{Var}(T)\\operatorname{Var}(R) - \\operatorname{Cov}(T,R)^2 = (\\alpha^2 \\sigma_B^2 + \\sigma_T^2)(\\sigma_B^2 + \\sigma_R^2) - (\\alpha \\sigma_B^2)^2.\n$$\n展开并化简，\n$$\nD = \\alpha^2 \\sigma_B^2 \\sigma_R^2 + \\sigma_T^2(\\sigma_B^2 + \\sigma_R^2).\n$$\n分子是\n$$\nN_T = \\operatorname{Cov}(Y,T)\\operatorname{Var}(R) - \\operatorname{Cov}(Y,R)\\operatorname{Cov}(T,R).\n$$\n代入先前推导的协方差和方差：\n$$\nN_T = \\big[\\tau(\\alpha^2 \\sigma_B^2 + \\sigma_T^2) + \\gamma \\alpha \\sigma_B^2\\big](\\sigma_B^2 + \\sigma_R^2) - \\big[\\tau \\alpha \\sigma_B^2 + \\gamma \\sigma_B^2\\big]\\big[\\alpha \\sigma_B^2\\big].\n$$\n合并同类项，\n$$\nN_T = \\tau\\big[\\alpha^2 \\sigma_B^2 \\sigma_R^2 + \\sigma_T^2(\\sigma_B^2 + \\sigma_R^2)\\big] + \\gamma \\alpha \\sigma_B^2 \\sigma_R^2 = \\tau D + \\gamma \\alpha \\sigma_B^2 \\sigma_R^2.\n$$\n因此，\n$$\n\\beta_T^{\\text{adj}} = \\frac{\\tau D + \\gamma \\alpha \\sigma_B^2 \\sigma_R^2}{D} = \\tau + \\frac{\\gamma \\alpha \\sigma_B^2 \\sigma_R^2}{D}.\n$$\n调整后的偏差是\n$$\nb_{\\text{adj}} = \\beta_T^{\\text{adj}} - \\tau = \\frac{\\gamma \\alpha \\sigma_B^2 \\sigma_R^2}{\\alpha^2 \\sigma_B^2 \\sigma_R^2 + \\sigma_T^2(\\sigma_B^2 + \\sigma_R^2)}.\n$$\n关键结论：\n- 如果放射组学代理变量是完美的（$\\sigma_R^2 = 0$），那么 $b_{\\text{adj}} = 0$；因为 $R = B$ 阻断了路径 $T \\leftarrow B \\rightarrow Y$，所以调整通过后门准则消除了混淆。\n- 如果处理是随机化的（$\\alpha = 0$），那么 $b_{\\text{naive}} = 0$ 和 $b_{\\text{adj}} = 0$。\n- 如果代理变量噪声很大（$\\sigma_R^2$ 很大），那么 $D \\approx \\sigma_T^2 \\sigma_R^2$ 且 $b_{\\text{adj}} \\approx \\gamma \\alpha \\sigma_B^2 / \\sigma_T^2$，这可能超过朴素偏差 $b_{\\text{naive}} = \\gamma \\alpha \\sigma_B^2 / (\\alpha^2 \\sigma_B^2 + \\sigma_T^2)$，这展示了由于代理变量中的测量误差导致的偏差放大。\n\n算法设计：\n- 对于每个测试用例 $(\\alpha, \\tau, \\gamma, \\sigma_B^2, \\sigma_T^2, \\sigma_R^2)$，使用 $b_{\\text{naive}} = \\dfrac{\\gamma \\alpha \\sigma_B^2}{\\alpha^2 \\sigma_B^2 + \\sigma_T^2}$ 计算 $b_{\\text{naive}}$。\n- 计算 $D = \\alpha^2 \\sigma_B^2 \\sigma_R^2 + \\sigma_T^2(\\sigma_B^2 + \\sigma_R^2)$，然后计算 $b_{\\text{adj}} = \\dfrac{\\gamma \\alpha \\sigma_B^2 \\sigma_R^2}{D}$。\n- 对于每个案例，报告元组 $(b_{\\text{naive}}, b_{\\text{adj}}, |b_{\\text{adj}}|  |b_{\\text{naive}}|)$。\n- 按要求的输出顺序将所有案例结果汇总到一个扁平列表中。\n\n该程序仅使用确定性的闭式计算，不需要模拟。不涉及物理单位或角度单位；输出是指定格式的实数和布尔值。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_biases(alpha, tau, gamma, sigma_B2, sigma_T2, sigma_R2):\n    \"\"\"\n    Compute naive and adjusted biases for the linear Gaussian SCM:\n      T = alpha * B + U_T\n      R = B + eps_R\n      Y = tau * T + gamma * B + U_Y\n    All variables zero-mean; B, U_T, U_Y, eps_R mutually independent.\n    Parameters represent variances and coefficients.\n    Returns:\n      b_naive, b_adj\n    \"\"\"\n    # Naive bias: b_naive = (gamma * alpha * sigma_B^2) / (alpha^2 * sigma_B^2 + sigma_T^2)\n    denom_naive = alpha**2 * sigma_B2 + sigma_T2\n    # To be robust in degenerate cases, handle denominator zero (should not occur with nonnegative variances unless all zero).\n    if denom_naive == 0.0:\n        # If alpha, sigma_B2, sigma_T2 are all zero, T is identically zero.\n        # This implies gamma*alpha*sigma_B2 is also zero. We define the bias as 0.\n        b_naive = 0.0\n    else:\n        b_naive = (gamma * alpha * sigma_B2) / denom_naive\n\n    # Adjusted bias: b_adj = (gamma * alpha * sigma_B^2 * sigma_R^2) / D\n    D = (alpha**2 * sigma_B2 * sigma_R2) + (sigma_T2 * (sigma_B2 + sigma_R2))\n    if D == 0.0:\n        # Denominator is zero only in degenerate cases (e.g. sigma_T2=0 and alpha=0 or sigma_R2=0).\n        # Numerator is also zero in these cases, so bias is 0.\n        b_adj = 0.0\n    else:\n        b_adj = (gamma * alpha * sigma_B2 * sigma_R2) / D\n\n    return b_naive, b_adj\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (alpha, tau, gamma, sigma_B^2, sigma_T^2, sigma_R^2)\n    test_cases = [\n        (0.0, 1.2, 0.7, 1.0, 1.0, 0.5),   # Case 1: no confounding\n        (1.0, 1.0, 1.0, 1.0, 0.1, 0.01),  # Case 2: confounding with strong proxy\n        (1.0, 1.0, 1.0, 1.0, 0.1, 10.0),  # Case 3: confounding with weak proxy (bias amplification)\n        (1.0, 0.5, 2.0, 2.0, 0.2, 0.0),   # Case 4: confounding with perfect proxy\n        (0.0, 0.8, 1.5, 5.0, 2.0, 1.0),   # Case 5: randomized treatment\n        (2.0, 1.0, 3.0, 0.0, 1.0, 0.5),   # Case 6: no biological variability\n    ]\n\n    results = []\n    for case in test_cases:\n        alpha, tau, gamma, sigma_B2, sigma_T2, sigma_R2 = case\n        b_naive, b_adj = compute_biases(alpha, tau, gamma, sigma_B2, sigma_T2, sigma_R2)\n        # Determine whether adjustment reduces absolute bias\n        reduces_bias = (abs(b_adj)  abs(b_naive)) if (np.isfinite(b_naive) and np.isfinite(b_adj)) else False\n        # Append outputs in required order: b_naive, b_adj, boolean\n        results.append(b_naive)\n        results.append(b_adj)\n        results.append(reduces_bias)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "4567522"}]}