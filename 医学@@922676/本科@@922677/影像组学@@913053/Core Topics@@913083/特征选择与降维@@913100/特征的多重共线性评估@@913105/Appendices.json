{"hands_on_practices": [{"introduction": "多重共线性从根本上说是指特征向量在它们所张成的向量空间中不是完全独立的。奇异值分解（Singular Value Decomposition, SVD）是一种强大的线性代数工具，它通过识别数据中的“退化”维度来揭示这些线性依赖关系，这些维度表现为接近于零的奇异值。这个练习 [@problem_id:4553150] 提供了一个通过基本计算来诊断完美共线性的机会，帮助您从数学核心层面理解冗余是如何产生的。", "problem": "一项影像组学研究分析了 $4$ 个病灶，每个病灶由 $3$ 个定量特征进行概括：一个归一化的一阶强度描述符、一个标准化的纹理对比度度量和一个标准化的形状紧凑性度量。特征矩阵由下式给出\n$$\n\\mathbf{X} \\in \\mathbb{R}^{4 \\times 3}, \\quad \n\\mathbf{X} = \n\\begin{pmatrix}\n1  1  2 \\\\\n1  -1  0 \\\\\n1  1  2 \\\\\n1  -1  0\n\\end{pmatrix}.\n$$\n在多重共线性评估中，$\\mathbf{X}$ 的接近于零的奇异值表示存在冗余的特征组合。使用适用于影像组学中数值特征矩阵的线性代数基本定义，计算 $\\mathbf{X}$ 的奇异值，并识别任何揭示特征间冗余性的近零分量。最后，报告 $\\mathbf{X}$ 的最小奇异值 $s_{\\min}$ 作为您的答案。提供精确数值，无需四舍五入。", "solution": "问题陈述被评估为有效。它在影像组学和线性代数领域有科学依据，提法得当，提供了所有必要信息，并且表述客观。任务是计算给定矩阵的奇异值，以评估其代表影像组学特征的列向量之间的多重共线性。\n\n实矩阵 $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$ 的奇异值定义为对称矩阵 $\\mathbf{X}^\\top\\mathbf{X}$ 的特征值的平方根。这些特征值保证为非负。\n\n给定的特征矩阵为：\n$$\n\\mathbf{X} = \n\\begin{pmatrix}\n1  1  2 \\\\\n1  -1  0 \\\\\n1  1  2 \\\\\n1  -1  0\n\\end{pmatrix}\n$$\n这里，样本数（病灶数）为 $m=4$，特征数为 $n=3$。我们将计算 $3 \\times 3$ 矩阵 $\\mathbf{A} = \\mathbf{X}^\\top\\mathbf{X}$。$\\mathbf{X}$ 的转置为：\n$$\n\\mathbf{X}^\\top = \n\\begin{pmatrix}\n1  1  1  1 \\\\\n1  -1  1  -1 \\\\\n2  0  2  0\n\\end{pmatrix}\n$$\n乘积 $\\mathbf{A} = \\mathbf{X}^\\top\\mathbf{X}$ 为：\n$$\n\\mathbf{A} = \\begin{pmatrix}\n1  1  1  1 \\\\\n1  -1  1  -1 \\\\\n2  0  2  0\n\\end{pmatrix}\n\\begin{pmatrix}\n1  1  2 \\\\\n1  -1  0 \\\\\n1  1  2 \\\\\n1  -1  0\n\\end{pmatrix}\n$$\n执行矩阵乘法：\n$\\mathbf{A}$ 的元素 $a_{ij}$ 是 $\\mathbf{X}^\\top$ 的第 $i$ 行与 $\\mathbf{X}$ 的第 $j$ 列的点积。\n$a_{11} = (1)(1) + (1)(1) + (1)(1) + (1)(1) = 4$\n$a_{12} = (1)(1) + (1)(-1) + (1)(1) + (1)(-1) = 0$\n$a_{13} = (1)(2) + (1)(0) + (1)(2) + (1)(0) = 4$\n$a_{21} = a_{12} = 0$\n$a_{22} = (1)(1) + (-1)(-1) + (1)(1) + (-1)(-1) = 1+1+1+1 = 4$\n$a_{23} = (1)(2) + (-1)(0) + (1)(2) + (-1)(0) = 2+0+2+0 = 4$\n$a_{31} = a_{13} = 4$\n$a_{32} = a_{23} = 4$\n$a_{33} = (2)(2) + (0)(0) + (2)(2) + (0)(0) = 4+0+4+0 = 8$\n\n所以，矩阵 $\\mathbf{A}$ 为：\n$$\n\\mathbf{A} = \\mathbf{X}^\\top\\mathbf{X} = \n\\begin{pmatrix}\n4  0  4 \\\\\n0  4  4 \\\\\n4  4  8\n\\end{pmatrix}\n$$\n接下来，我们通过求解特征方程 $\\det(\\mathbf{A} - \\lambda\\mathbf{I}) = 0$ 来找到 $\\mathbf{A}$ 的特征值 $\\lambda$，其中 $\\mathbf{I}$ 是 $3 \\times 3$ 的单位矩阵。\n$$\n\\det\\left( \\begin{pmatrix} 4  0  4 \\\\ 0  4  4 \\\\ 4  4  8 \\end{pmatrix} - \\lambda \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix} \\right) = 0\n$$\n$$\n\\det \\begin{pmatrix} 4-\\lambda  0  4 \\\\ 0  4-\\lambda  4 \\\\ 4  4  8-\\lambda \\end{pmatrix} = 0\n$$\n我们沿第一行展开行列式：\n$$\n(4-\\lambda) \\begin{vmatrix} 4-\\lambda  4 \\\\ 4  8-\\lambda \\end{vmatrix} - 0 \\cdot \\begin{vmatrix} 0  4 \\\\ 4  8-\\lambda \\end{vmatrix} + 4 \\cdot \\begin{vmatrix} 0  4-\\lambda \\\\ 4  4 \\end{vmatrix} = 0\n$$\n$$\n(4-\\lambda) [ (4-\\lambda)(8-\\lambda) - (4)(4) ] + 4 [ (0)(4) - (4-\\lambda)(4) ] = 0\n$$\n$$\n(4-\\lambda) [ 32 - 12\\lambda + \\lambda^2 - 16 ] - 16(4-\\lambda) = 0\n$$\n$$\n(4-\\lambda) [ \\lambda^2 - 12\\lambda + 16 ] - 16(4-\\lambda) = 0\n$$\n我们可以提取出公共项 $(4-\\lambda)$：\n$$\n(4-\\lambda) [ (\\lambda^2 - 12\\lambda + 16) - 16 ] = 0\n$$\n$$\n(4-\\lambda) [ \\lambda^2 - 12\\lambda ] = 0\n$$\n$$\n\\lambda (4-\\lambda) (\\lambda - 12) = 0\n$$\n$\\mathbf{X}^\\top\\mathbfX$ 的特征值为 $\\lambda_1 = 12$, $\\lambda_2 = 4$ 和 $\\lambda_3 = 0$。\n\n$\\mathbf{X}$ 的奇异值，用 $s_i$ 表示，是这些特征值的平方根。按照惯例，它们按非递增顺序列出。\n$$\ns_1 = \\sqrt{\\lambda_1} = \\sqrt{12} = \\sqrt{4 \\cdot 3} = 2\\sqrt{3}\n$$\n$$\ns_2 = \\sqrt{\\lambda_2} = \\sqrt{4} = 2\n$$\n$$\ns_3 = \\sqrt{\\lambda_3} = \\sqrt{0} = 0\n$$\n奇异值的集合是 $\\{2\\sqrt{3}, 2, 0\\}$。问题陈述指出，接近于零的奇异值表示存在冗余的特征组合。在这种情况下，我们有一个奇异值恰好为零。这表示完全多重共线性，意味着 $\\mathbf{X}$ 的某一列是其他列的精确线性组合。设 $\\mathbf{X}$ 的列为 $\\mathbf{c}_1$, $\\mathbf{c}_2$ 和 $\\mathbf{c}_3$。\n$$\n\\mathbf{c}_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad \n\\mathbf{c}_2 = \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\\\ -1 \\end{pmatrix}, \\quad \n\\mathbf{c}_3 = \\begin{pmatrix} 2 \\\\ 0 \\\\ 2 \\\\ 0 \\end{pmatrix}\n$$\n很明显 $\\mathbf{c}_3 = \\mathbf{c}_1 + \\mathbf{c}_2$。这种线性相关性是奇异值为零的原因，并证实了特征之间的冗余性。\n\n问题要求的是最小奇异值 $s_{\\min}$。\n$$\ns_{\\min} = s_3 = 0\n$$", "answer": "$$\n\\boxed{0}\n$$", "id": "4553150"}, {"introduction": "虽然方差膨胀因子（Variance Inflation Factor, $\\text{VIF}$）等方法常用于检测多重共线性，但它们主要对线性关系敏感，而放射组学特征间的真实关系可能更为复杂。这个编码练习 [@problem_id:4553130] 旨在通过比较 $\\text{VIF}$ 和能够捕捉非线性依赖的互信息（Mutual Information, MI），来揭示线性方法的局限性。通过这个实践，您将学会如何评估和选择更通用的特征冗余度量方法，以应对更广泛的数据场景。", "problem": "您的任务是设计并实现一个程序，用于在特征之间存在二次依赖关系时，比较使用互信息 (MI) 和方差膨胀因子 (VIF) 对模拟影像组学特征进行冗余检测的效果。该问题的科学基础是：互信息 (MI) 定义为联合分布与边缘分布乘积之间的Kullback-Leibler散度；方差膨胀因子 (VIF) 的定义源自线性回归和决定系数。\n\n考虑代表影像组学特征的随机变量。设 $x_1$、$x_2$ 和 $x_3$ 是从下述分布中抽样的实值特征。两个连续随机变量 $X$ 和 $Y$ 之间的互信息 (MI) 定义为\n$$\nI(X;Y) = \\int \\int p_{X,Y}(x,y) \\log \\left( \\frac{p_{X,Y}(x,y)}{p_X(x)p_Y(y)} \\right) \\, dx \\, dy,\n$$\n它衡量了由于已知 $Y$ 而导致 $X$ 不确定性的减少量（反之亦然）。对于基于 MI 的冗余检测，如果在给定显著性水平 $\\alpha$ 下，通过置换检验估计的 MI 在统计上是显著的（其中零假设是独立性），我们将判定两个特征 $X$ 和 $Y$ 是冗余的。\n\n特征 $x_j$ 相对于其他一组特征的方差膨胀因子 (VIF) 定义为\n$$\n\\text{VIF}_j = \\frac{1}{1 - R_j^2},\n$$\n其中 $R_j^2$ 是将 $x_j$ 对其他特征进行线性回归时得到的决定系数。较大的 $\\text{VIF}_j$ 值表明 $x_j$ 可以很好地被其他特征的线性组合所近似，这意味着存在多重共线性。对于通过 VIF 进行的冗余检测，如果 $\\text{VIF}_j$ 超过指定的阈值，我们将判定特征 $x_j$ 是冗余的。\n\n您的程序必须：\n- 在指定关系下模拟包含三个特征 $(x_1, x_2, x_3)$ 的数据集。基础变量抽取如下：$x_1 \\sim \\mathcal{N}(0,1)$ 和 $x_3 \\sim \\mathcal{N}(0,1)$ 相互独立。特征 $x_2$ 在四种不同机制下生成：\n    1. 二次强依赖：$x_2 = x_1^2 + \\epsilon$。\n    2. 二次弱依赖：$x_2 = x_1^2 + \\epsilon$。\n    3. 线性依赖：$x_2 = 2 x_1 + \\epsilon$。\n    4. 独立：$x_2 \\sim \\mathcal{N}(0,1)$，且与 $x_1$ 和 $x_3$ 独立。\n  在所有情况下，$\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ 是独立的加性噪声。\n- 使用适用于连续变量的非参数方法估计 $x_1$ 和 $x_2$ 之间的 MI，从微分熵的定义和 Kozachenko–Leonenko 最近邻估计量出发，并应用 Kraskov–Stögbauer–Grassberger 方法。使用带有 $m$ 次随机置换的置换检验，在 $x_1$ 和 $x_2$ 独立的零假设下获得一个 $p$ 值。必须通过将 $p$ 值与显著性水平 $\\alpha$ 进行比较来做出基于 MI 的冗余决策。\n- 通过对预测变量集 $\\{x_1, x_3\\}$ 上的 $x_2$ 执行普通最小二乘线性回归来计算 $x_2$ 的 $\\text{VIF}$，从平方和与 $R^2$ 的定义开始，然后通过将 $\\text{VIF}$ 与阈值 $\\tau$ 进行比较来判定冗余。\n\n使用以下参数值测试套件：\n- 情况1（理想路径，强二次依赖）：$n = 1000$，$\\sigma = 0.05$，关系类型 $\\text{quadratic\\_strong}$。\n- 情况2（边缘情况，弱二次依赖）：$n = 1000$，$\\sigma = 3.0$，关系类型 $\\text{quadratic\\_weak}$。\n- 情况3（对比情况，线性依赖）：$n = 1000$，$\\sigma = 0.05$，关系类型 $\\text{linear}$。\n- 情况4（零假设情况，独立）：$n = 1000$，$\\sigma = 0.05$，关系类型 $\\text{independent}$。\n\n将 MI 的置换次数设置为 $m = 100$，显著性水平设置为 $\\alpha = 0.01$。将 VIF 阈值设置为 $\\tau = 5$。对于每种情况，您的程序必须按顺序输出一对布尔值 $[\\text{MI\\_redundant}, \\text{VIF\\_redundant}]$，其中如果基于 MI 的置换检验在水平 $\\alpha$ 下拒绝独立性，则 $\\text{MI\\_redundant}$ 为真；如果 $\\text{VIF} > \\tau$，则 $\\text{VIF\\_redundant}$ 为真。\n\n最终输出格式规范：\n- 您的程序应生成单行输出，包含一个由四个项目组成的逗号分隔列表，每个项目本身是对应情况的一个含两个布尔元素的列表，并用方括号括起来。例如，输出必须如下所示：\n$$\n[\\,[b_{11},b_{12}],[b_{21},b_{22}],[b_{31},b_{32}],[b_{41},b_{42}]\\,],\n$$\n其中 $b_{ij}$ 是情况 $i$ 和方法 $j$ 的布尔值。除了这个确切结构外，不允许有任何额外的文本、空格或字符。", "solution": "用户提供的问题陈述经分析验证，是科学合理、定义明确且客观的。任务是实现并比较两种评估特征冗余的方法——互信息 (MI) 和方差膨胀因子 (VIF)——在一个旨在凸显它们对线性与非线性依赖关系不同敏感性的模拟环境中。该问题是有效的，并且可以按规定解决。\n\n解决方案将以一个遵循指定环境约束的 Python 程序的形式构建。它将对四个测试用例中的每一个执行以下步骤：\n1.  **数据生成**：将创建一个包含 $n=1000$ 个样本和三个特征 $(x_1, x_2, x_3)$ 的数据集。$x_1$ 和 $x_3$ 从独立的标准正态分布中抽取，$x_1, x_3 \\sim \\mathcal{N}(0,1)$。特征 $x_2$ 根据与 $x_1$ 的四种指定关系之一生成，其中涉及加性高斯噪声 $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$。\n2.  **通过互信息 (MI) 进行冗余评估**：\n    -   将使用 Kraskov–Stögbauer–Grassberger (KSG) 方法估计特征 $x_1$ 和 $x_2$ 之间的互信息，表示为 $I(x_1; x_2)$。这种非参数方法特别适用于连续变量，并避免了分箱问题。使用的具体估计量是第一个KSG估计量，由下式给出：\n      $$\n      \\hat{I}^{(1)}(X,Y) = \\psi(k) - \\langle \\psi(n_x(i)) + \\psi(n_y(i)) \\rangle + \\psi(N)\n      $$\n      这里，$N$ 是样本数，$k$ 是最近邻的数量（我们将使用 $k=3$，这是一个稳健性的常用选择），$\\psi$ 是 digamma 函数，$\\langle \\cdot \\rangle$ 表示对所有样本 $i$ 的平均值。对于每个样本点 $z_i=(x_i, y_i)$，我们使用切比雪夫（最大）范数找到其在联合空间中到第 $k$ 个最近邻的距离 $\\epsilon_i/2$。然后，$n_x(i)$ 和 $n_y(i)$ 是在各自边缘空间内此距离内的点的数量，即 $n_x(i) = |\\{j: |x_i-x_j| \\le \\epsilon_i/2\\}|$。邻居搜索和范围计数将使用 `scipy.spatial.KDTree` 高效实现。\n    -   为了评估统计显著性，将执行置换检验。为原始对 $(x_1, x_2)$ 计算观测到的MI，$\\hat{I}_{obs}$。然后，通过重复（$m=100$ 次）打乱 $x_2$ 向量、计算MI并存储它，来生成MI值的零分布。$p$ 值是大于或等于 $\\hat{I}_{obs}$ 的置换MI值的比例。\n    -   如果此 $p$ 值小于显著性水平 $\\alpha=0.01$，则认为一对特征是冗余的。\n\n3.  **通过方差膨胀因子 (VIF) 进行冗余评估**：\n    -   将计算特征 $x_2$ 相对于特征 $x_1$ 和 $x_3$ 的 VIF。这涉及对 $x_2$ 关于 $x_1$ 和 $x_3$（包括一个截距项）执行普通最小二乘法 (OLS) 线性回归。\n    -   决定系数 $R^2$ 由总平方和 (SST) 和残差平方和 (SSR) 计算得出：\n      $$\n      R^2 = 1 - \\frac{\\text{SSR}}{\\text{SST}} = 1 - \\frac{\\sum_i (y_i - \\hat{y}_i)^2}{\\sum_i (y_i - \\bar{y})^2}\n      $$\n    -   然后 VIF 计算如下：\n      $$\n      \\text{VIF} = \\frac{1}{1 - R^2}\n      $$\n    -   如果一个特征的 VIF 超过阈值 $\\tau=5$，则将其分类为冗余。\n\n4.  **输出生成**：对于每个测试用例，程序将确定两个布尔值：`MI_redundant` 和 `VIF_redundant`。最终输出将是一个单行字符串，表示这些布尔值对的列表，并精确按照要求格式化。\n\n该设计直接解决了问题的核心科学问题：VIF 基于线性模型，预计能检测到线性多重共线性（情况3），但对于纯粹的非线性依赖关系（情况1和2）则会失效，即使依赖性很强。而MI作为一种衡量统计依赖性的通用指标，预计能同时检测到线性和非线性关系。情况4（独立）作为两种方法的阴性对照。", "answer": "```python\nimport numpy as np\nfrom scipy.special import digamma\nfrom scipy.spatial import KDTree\n\ndef calculate_vif(X, target_idx, predictor_indices):\n    \"\"\"Calculates the VIF for a target feature.\"\"\"\n    y_target = X[:, target_idx]\n    \n    # If the target feature is constant, its variance is zero. VIF is not well-defined,\n    # but there's no variance to inflate, so we can return 1.\n    if np.std(y_target) < 1e-9:\n        return 1.0\n\n    X_predictors = X[:, predictor_indices]\n    \n    # Add an intercept term to the predictors for the OLS regression\n    X_predictors_int = np.c_[np.ones(X_predictors.shape[0]), X_predictors]\n    \n    # Solve for regression coefficients using the normal equation: (X'X)w = X'y\n    try:\n        coeffs = np.linalg.solve(X_predictors_int.T @ X_predictors_int, X_predictors_int.T @ y_target)\n        y_pred = X_predictors_int @ coeffs\n    except np.linalg.LinAlgError:\n        # If the predictor matrix is singular, it means perfect collinearity among predictors.\n        # In this specific problem setup (x1, x3 are independent), this won't happen.\n        # However, if it did, the R^2 would be 1, and VIF would be infinite.\n        return np.inf\n\n    # Calculate R-squared\n    ss_total = np.sum((y_target - np.mean(y_target))**2)\n    ss_residual = np.sum((y_target - y_pred)**2)\n    \n    # If total sum of squares is zero, R^2 is undefined. Return 1 as a sensible default.\n    if ss_total == 0:\n        return 1.0\n\n    r_squared = 1 - ss_residual / ss_total\n    \n    # Calculate VIF. Avoid division by zero or negative results from floating point errors.\n    if r_squared >= 1.0 - 1e-9:\n        return np.inf\n    else:\n        return 1 / (1 - r_squared)\n\ndef calculate_ksg_mi(x, y, k):\n    \"\"\"Estimates mutual information using the KSG method (estimator 1).\"\"\"\n    n = len(x)\n    xy = np.c_[x, y]\n    tree_xy = KDTree(xy)\n    \n    # Find distance to k-th nearest neighbor for each point (using Chebyshev norm)\n    # query with k finds k neighbors, the k-th is the last one.\n    # We use k, not k+1, because we need the distance to the k-th neighbor\n    dists_k, _ = tree_xy.query(xy, k=[k], p=np.inf)\n    dists_k = dists_k.ravel()\n\n    # Build KDTrees for marginal spaces for efficient range counting\n    tree_x = KDTree(x[:, np.newaxis])\n    tree_y = KDTree(y[:, np.newaxis])\n\n    # Count number of points within the k-th neighbor distance in each marginal space\n    # The counts include the point itself.\n    nx = tree_x.query_ball_point(x[:, np.newaxis], dists_k, p=np.inf, return_length=True)\n    ny = tree_y.query_ball_point(y[:, np.newaxis], dists_k, p=np.inf, return_length=True)\n    \n    # KSG formula: I(X,Y) ~= psi(k) - E[psi(nx) + psi(ny)] + psi(N)\n    # The digamma function psi is applied to the counts.\n    # np.mean acts as the expectation operator E[...].\n    return digamma(k) - (np.mean(digamma(nx)) + np.mean(digamma(ny))) + digamma(n)\n\ndef run_mi_permutation_test(x, y, k, m, alpha, rng):\n    \"\"\"Performs a permutation test for MI significance.\"\"\"\n    observed_mi = calculate_ksg_mi(x, y, k)\n    \n    permuted_mi_dist = np.zeros(m)\n    y_shuffled = np.copy(y)\n    for i in range(m):\n        rng.shuffle(y_shuffled)\n        permuted_mi_dist[i] = calculate_ksg_mi(x, y_shuffled, k)\n    \n    # Calculate p-value: proportion of permuted MIs >= observed MI\n    p_value = (np.sum(permuted_mi_dist >= observed_mi) + 1) / (m + 1)\n    return p_value < alpha\n\ndef solve():\n    \"\"\"Main function to run the simulation for all test cases.\"\"\"\n    SEED = 42\n    rng = np.random.default_rng(SEED)\n\n    test_cases = [\n        (1000, 0.05, 'quadratic_strong'),\n        (1000, 3.0, 'quadratic_weak'),\n        (1000, 0.05, 'linear'),\n        (1000, 0.05, 'independent'),\n    ]\n\n    MI_PERMUTATIONS = 100\n    MI_ALPHA = 0.01\n    MI_K_NEIGHBORS = 3\n    VIF_THRESHOLD = 5.0\n\n    all_results = []\n\n    for n, sigma, rel_type in test_cases:\n        x1 = rng.standard_normal(n)\n        x3 = rng.standard_normal(n)\n        noise = rng.normal(0, sigma, n)\n\n        if rel_type.startswith('quadratic'):\n            x2 = np.square(x1) + noise\n        elif rel_type == 'linear':\n            x2 = 2 * x1 + noise\n        else: # 'independent'\n            x2 = rng.standard_normal(n)\n\n        mi_redundant = run_mi_permutation_test(x1, x2, k=MI_K_NEIGHBORS, m=MI_PERMUTATIONS, alpha=MI_ALPHA, rng=rng)\n\n        X = np.c_[x1, x2, x3]\n        vif = calculate_vif(X, target_idx=1, predictor_indices=[0, 2])\n        vif_redundant = vif > VIF_THRESHOLD\n        \n        all_results.append([mi_redundant, vif_redundant])\n\n    # Format the output string exactly as required\n    formatted_pairs = [f\"[{str(p[0])},{str(p[1])}]\" for p in all_results]\n    print(f\"[{','.join(formatted_pairs)}]\")\n\nsolve()\n```", "id": "4553130"}, {"introduction": "在识别出多重共线性之后，下一步通常是移除冗余特征，但执行此操作的方式和时机对构建可靠的预测模型至关重要。此练习 [@problem_id:4553147] 模拟了一个真实的机器学习工作流程，以突显在交叉验证中不当进行特征筛选时可能发生的“数据泄漏”陷阱。通过对比两种不同的筛选策略，您将亲身体会到为何必须在交叉验证的每个折叠内独立进行特征选择，以获得对模型泛化能力的无偏估计。", "problem": "您的任务是，在存在多重共线性的情况下，为类影像组学数据设计和评估两种特征剪枝方案。在影像组学中，多重共线性的一个常见来源是源于对同一底层成像信号进行相似变换所衍生出的特征组，这可以被理想化为高度相关的特征块。目标是评估一种无泄漏剪枝方案与一种易泄漏剪枝方案对预测模型估计泛化误差的影响。\n\n从以下基础概念开始：\n- 对于具有有限方差的两个随机变量 $X$ 和 $Y$，其 Pearson 相关系数定义为 $\\rho_{X,Y} = \\dfrac{\\operatorname{Cov}(X,Y)}{\\sigma_X \\sigma_Y}$，其中 $\\operatorname{Cov}$ 是协方差，$\\sigma_X$ 和 $\\sigma_Y$ 是标准差。\n- 在 $K$ 折交叉验证（CV）中，包含 $n$ 个样本的数据集 $D$ 被划分为 $K$ 个大小近似相等的不相交的折。对于每个折 $k \\in \\{1,\\dots,K\\}$，模型在其他 $K-1$ 个折的并集（训练折集）上进行训练，并在留出的折（验证折）上进行评估。交叉验证对泛化误差的估计是所有折上验证误差的平均值。\n- 对于一个固定的设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 和响应向量 $y \\in \\mathbb{R}^n$，岭回归通过最小化 $L(w) = \\|y - Xw\\|_2^2 + \\lambda \\|w\\|_2^2$ 来拟合一个线性模型参数向量 $w \\in \\mathbb{R}^p$，其中正则化参数 $\\lambda > 0$。唯一最小化解由 $w^\\star = (X^\\top X + \\lambda I_p)^{-1} X^\\top y$ 给出，其中 $I_p$ 是 $p \\times p$ 的单位矩阵。\n- 在大小为 $m$ 的验证集上的均方误差（MSE）为 $ \\text{MSE} = \\dfrac{1}{m} \\sum_{i=1}^m (y_i - \\hat{y}_i)^2 $，其中 $\\hat{y}_i$ 是模型预测值。\n\n您的程序必须纯粹以数学和算法术语执行以下任务，无需涉及任何特定的成像设备：\n1. 合成数据生成：\n   - 构建一个具有块结构相关性的特征矩阵 $X \\in \\mathbb{R}^{n \\times p}$。对于每个大小为 $s_b$、目标块内相关性为 $r_b \\in (0,1)$ 的块 $b$，生成一个潜向量 $z_b \\in \\mathbb{R}^n$，其元素是独立同分布的标准正态分布 $z_b \\sim \\mathcal{N}(0,1)$。对于块中的每个特征，设置 $X_{\\cdot j} = z_b + \\epsilon_j$，其中 $\\epsilon_j \\sim \\mathcal{N}(0, \\sigma_b^2)$ 独立同分布，$\\sigma_b^2$ 的选择应使得块内期望的两两相关性满足 $\\rho \\approx \\dfrac{1}{1 + \\sigma_b^2}$。使用 $\\sigma_b^2 = \\dfrac{1 - r_b}{r_b}$ 来近似达到目标 $r_b$。对于非块（独立）特征，设置 $X_{\\cdot j} \\sim \\mathcal{N}(0,1)$ 独立同分布。\n   - 构建响应向量 $y \\in \\mathbb{R}^n$，作为三个选定特征的线性组合加上独立噪声：$y = \\beta_1 X_{\\cdot q_1} + \\beta_2 X_{\\cdot q_2} + \\beta_3 X_{\\cdot q_3} + \\eta$，其中 $(\\beta_1, \\beta_2, \\beta_3) = (1.0, 0.8, -0.6)$ 且 $\\eta \\sim \\mathcal{N}(0, \\sigma_y^2)$ 独立同分布。索引 $(q_1, q_2, q_3)$ 的选择如下：如果至少有三个块，则 $q_1$ 是块1的第一个特征，$q_2$ 是块2的第一个特征，$q_3$ 是块3的第一个特征。如果块数少于三个，则选择 $q_1, q_2, q_3$ 为 $X$ 的前三个特征（按升序排列）。所有随机抽样必须通过测试套件中提供的固定种子来实现确定性。\n2. 特征剪枝方案：\n   - 易泄漏方案（全局剪枝）：使用所有 $n$ 个样本计算特征间相关性矩阵 $\\hat{R} \\in \\mathbb{R}^{p \\times p}$。从特征索引 $j = 0$ 开始，依次到 $p-1$，通过保留与任何先前保留的特征没有高度相关（绝对值超过阈值）的特征来进行贪婪选择。具体来说，对于相关性阈值 $\\tau \\in (0,1)$，如果对于所有先前保留的索引 $k$ 都有 $|\\hat{R}_{jk}| \\le \\tau$，则保留特征 $j$。这将为所有折生成一个单一的全局保留特征集 $S_{\\text{global}} \\subset \\{0,\\dots,p-1\\}$。\n   - 无泄漏方案（按折剪枝）：对于交叉验证中的每个训练折，仅使用该折 $k$ 的训练数据计算相关性矩阵 $\\hat{R}^{(k)}$，并使用阈值 $\\tau$ 执行相同的贪婪选择，以获得该折的保留集 $S^{(k)} \\subset \\{0,\\dots,p-1\\}$。在该交叉验证迭代中，使用 $S^{(k)}$ 来选择训练折和相应验证折中的 $X$ 列。\n3. 模型拟合与评估：\n   - 对于每个交叉验证折 $k \\in \\{1,\\dots,K\\}$ 和每种方案，使用该方案剪枝后特征集所限制的训练数据，拟合一个正则化参数为 $\\lambda$ 的岭回归模型，然后在相同的剪枝列上评估验证集均方误差。对所有折的均方误差进行平均，以获得 $\\overline{\\text{MSE}}_{\\text{leak}}$ 和 $\\overline{\\text{MSE}}_{\\text{safe}}$。\n4. 报告指标：\n   - 对于每个测试用例，计算浮点数量 $\\Delta = \\overline{\\text{MSE}}_{\\text{safe}} - \\overline{\\text{MSE}}_{\\text{leak}}$。一个正的 $\\Delta$ 值表明易泄漏方案表面上看起来比无泄漏方案更优，这与泄漏偏差相符。\n\n角度单位不适用。不涉及物理单位。所有最终数值输出均以十进制浮点数表示。\n\n测试套件：\n您的程序必须运行以下测试用例，每个用例由 $(n, p, K, \\tau, \\lambda, \\text{blocks}, \\sigma_y, \\text{seed})$ 指定：\n- 用例 1 (理想情况): $(n,p,K,\\tau,\\lambda) = (200,60,5,0.85,10^{-3})$, blocks $=[(10,0.95),(10,0.95),(10,0.95)]$, 响应噪声 $\\sigma_y = 0.5$, 种子 $= 123$。\n- 用例 2 (强共线性, 较小的n): $(n,p,K,\\tau,\\lambda) = (100,80,5,0.80,10^{-2})$, blocks $=[(15,0.98),(15,0.98),(15,0.98),(15,0.98)]$, $\\sigma_y = 0.7$, 种子 $= 456$。\n- 用例 3 (无共线性边缘情况): $(n,p,K,\\tau,\\lambda) = (150,50,5,0.80,10^{-3})$, blocks $=[]$ (无块, 所有特征独立), $\\sigma_y = 0.5$, 种子 $= 789$。\n- 用例 4 (高维, 极端共线性): $(n,p,K,\\tau,\\lambda) = (80,200,4,0.90,10^{-1})$, blocks $=[(30,0.99),(30,0.99),(30,0.99),(30,0.99),(30,0.99)]$, $\\sigma_y = 1.0$, 种子 $= 101112$。\n- 用例 5 (中等阈值变化): $(n,p,K,\\tau,\\lambda) = (120,40,6,0.50,10^{-3})$, blocks $=[(10,0.80),(10,0.80)]$, $\\sigma_y = 0.5$, 种子 $= 131415$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，$[result1,result2,result3]$）。对于上述五个用例，按顺序输出 $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4,\\Delta_5]$，其中 $\\Delta_i$ 是用例 $i$ 的计算值，以十进制浮点数表示。", "solution": "目标是设计并执行一个数值模拟，以量化特征选择过程中因数据泄漏而导致的泛化误差估计中的乐观偏差。具体来说，我们在 $K$ 折交叉验证的背景下，比较一个“易泄漏”（全局）剪枝方案和一个“无泄漏”（按折）剪枝方案。最终指标 $\\Delta = \\overline{\\text{MSE}}_{\\text{safe}} - \\overline{\\text{MSE}}_{\\text{leak}}$ 捕捉了这种偏差。一个正的 $\\Delta$ 值表明易泄漏方法得出了一个过于乐观（即更低）的均方误差估计。该模拟通过几个明确定义的阶段进行。\n\n首先，我们处理合成数据的生成。对于每个测试用例，我们给定参数 $(n, p, K, \\tau, \\lambda)$、一个块结构、一个响应噪声水平 $\\sigma_y$ 和一个随机种子。我们构建一个特征矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 和一个响应向量 $y \\in \\mathbb{R}^n$。矩阵 $X$ 按列填充。对于属于指定的大小为 $s_b$、目标相关性为 $r_b$ 的块 $b$ 的特征，我们首先计算加性噪声项所需的方差，$\\sigma_b^2 = \\frac{1 - r_b}{r_b}$。这是从 Pearson 相关性的定义推导出来的，$\\rho = \\frac{\\operatorname{Cov}(z_b + \\epsilon_i, z_b + \\epsilon_j)}{\\sqrt{\\operatorname{Var}(z_b + \\epsilon_i)\\operatorname{Var}(z_b + \\epsilon_j)}} = \\frac{\\operatorname{Var}(z_b)}{ \\operatorname{Var}(z_b) + \\operatorname{Var}(\\epsilon) } = \\frac{1}{1 + \\sigma_b^2}$，其中 $z_b \\in \\mathbb{R}^n$ 是一个潜向量，其元素从标准正态分布 $\\mathcal{N}(0,1)$ 中独立同分布地抽取，而 $\\epsilon_j \\in \\mathbb{R}^n$ 是一个独立的噪声向量，其元素来自 $\\mathcal{N}(0, \\sigma_b^2)$。对于块中的 $s_b$ 个特征中的每一个，我们生成一个独立的噪声向量 $\\epsilon_j$ 并设置特征列 $X_{\\cdot j} = z_b + \\epsilon_j$。任何未分配给块的特征都生成为独立的标准正态向量，$X_{\\cdot j} \\sim \\mathcal{N}(0,1)$。一旦 $X$ 构建完成，响应向量 $y$ 就通过三个特定特征的线性组合加上高斯噪声生成：$y = \\beta_1 X_{\\cdot q_1} + \\beta_2 X_{\\cdot q_2} + \\beta_3 X_{\\cdot q_3} + \\eta$，其中 $(\\beta_1, \\beta_2, \\beta_3) = (1.0, 0.8, -0.6)$ 且 $\\eta \\sim \\mathcal{N}(0, \\sigma_y^2)$。索引 $(q_1, q_2, q_3)$ 由特征块的数量决定：如果有三个或更多块，它们是前三个块的第一个索引；否则，它们是前三个列索引 $(0, 1, 2)$。为每个测试用例设置种子的确定性随机数生成器确保了可复现性。\n\n其次，我们定义两种特征剪枝算法。两者都采用基于 Pearson 相关性阈值 $\\tau$ 的贪婪选择策略。该过程遍历特征 $j=0, \\dots, p-1$，维护一个保留特征索引的集合。如果特征 $j$ 与所有先前保留的特征 $k$ 的绝对相关性低于或等于阈值，即 $|\\hat{R}_{jk}| \\le \\tau$，则将其添加到集合中。\n**易泄漏方案** 只执行一次此剪枝。它使用整个数据集 $X$ 计算相关性矩阵 $\\hat{R} \\in \\mathbb{R}^{p \\times p}$，并得出一个单一的、全局的剪枝后特征集 $S_{\\text{global}}$。这构成了数据泄漏，因为来自整个数据集的信息（包括稍后将用于验证的样本）影响了特征选择步骤。\n**无泄漏方案** 将剪枝适当地整合到交叉验证循环中。对于 $K$ 个折中的每一个，相关性矩阵 $\\hat{R}^{(k)}$ 仅使用该折的训练数据计算。这会产生一个特定于该折的剪枝后特征集 $S^{(k)}$。这种方法可以防止来自折 $k$ 验证集的信息影响为该折训练的模型的特征选择。\n\n第三，我们使用 $K$ 折交叉验证执行模型拟合和评估。样本索引 $\\{0, \\dots, n-1\\}$ 被划分为 $K$ 个不相交的验证折。对于每个折 $k=1, \\dots, K$，我们定义相应的训练和验证数据集。\n对于易泄漏方案，训练和验证特征矩阵 $X_{\\text{train}}$ 和 $X_{\\text{val}}$ 使用预先计算的 $S_{\\text{global}}$ 指定的列进行子集化。然后训练一个岭回归模型以找到最小化 $\\|y_{\\text{train}} - X_{\\text{train,pruned}}w\\|_2^2 + \\lambda \\|w\\|_2^2$ 的权重 $w^\\star_{\\text{leak}}$。这通过正规方程求解，$w^\\star_{\\text{leak}} = (X_{\\text{train,pruned}}^\\top X_{\\text{train,pruned}} + \\lambda I)^{-1} X_{\\text{train,pruned}}^\\top y_{\\text{train}}$。在验证集上计算均方误差 $\\text{MSE}_{\\text{leak}}^{(k)}$。\n对于无泄漏方案，重复此过程，但在循环内部执行特征选择。从 $X_{\\text{train}}$ 确定特定于折的特征集 $S^{(k)}$，然后根据 $S^{(k)}$ 对 $X_{\\text{train}}$ 和 $X_{\\text{val}}$ 进行剪枝。训练岭回归模型并计算相应的误差 $\\text{MSE}_{\\text{safe}}^{(k)}$。在没有选择任何特征的边缘情况下，预测值是训练响应的均值。然而，贪婪算法总是选择第一个特征，所以这种情况不会发生。\n\n最后，在遍历所有 $K$ 个折之后，对每种方案的均方误差进行平均，以获得交叉验证的误差估计：$\\overline{\\text{MSE}}_{\\text{leak}} = \\frac{1}{K}\\sum_{k=1}^K \\text{MSE}_{\\text{leak}}^{(k)}$ 和 $\\overline{\\text{MSE}}_{\\text{safe}} = \\frac{1}{K}\\sum_{k=1}^K \\text{MSE}_{\\text{safe}}^{(k)}$。我们关注的指标是差异 $\\Delta = \\overline{\\text{MSE}}_{\\text{safe}} - \\overline{\\text{MSE}}_{\\text{leak}}$。对问题套件中指定的每个测试用例重复这整个过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef greedy_pruning(X, tau):\n    \"\"\"\n    Performs greedy feature pruning based on a correlation threshold.\n\n    Args:\n        X (np.ndarray): The feature matrix (n_samples, n_features).\n        tau (float): The correlation threshold.\n\n    Returns:\n        list: A list of indices of the features to keep.\n    \"\"\"\n    p = X.shape[1]\n    if p == 0:\n        return []\n    \n    # Handle columns with zero variance, as corrcoef would produce NaNs or errors.\n    std_devs = np.std(X, axis=0)\n    valid_cols_mask = std_devs > 1e-9 # A small epsilon to avoid division by zero\n    \n    # If no feature has variance, keep the first one by convention.\n    if not np.any(valid_cols_mask):\n        return [0] \n\n    valid_col_indices = np.where(valid_cols_mask)[0]\n    X_valid = X[:, valid_cols_mask]\n    \n    corr_matrix = np.corrcoef(X_valid, rowvar=False)\n    # If only one valid column, corrcoef returns a scalar 1.0.\n    if corr_matrix.ndim  2:\n        return [valid_col_indices[0]]\n\n    kept_indices_in_valid_map = []\n    \n    for i in range(corr_matrix.shape[0]):\n        is_correlated = False\n        for j in kept_indices_in_valid_map:\n            correlation = corr_matrix[i, j]\n            if np.abs(correlation) > tau:\n                is_correlated = True\n                break\n        if not is_correlated:\n            kept_indices_in_valid_map.append(i)\n    \n    return [valid_col_indices[i] for i in kept_indices_in_valid_map]\n\ndef solve_ridge(X, y, lambda_reg):\n    \"\"\"\n    Solves Ridge Regression using the normal equation.\n    w = (X^T X + lambda I)^-1 X^T y\n    \"\"\"\n    p = X.shape[1]\n    I = np.identity(p)\n    A = X.T @ X + lambda_reg * I\n    b = X.T @ y\n    try:\n        weights = np.linalg.solve(A, b)\n    except np.linalg.LinAlgError:\n        weights = np.linalg.pinv(A) @ b\n    return weights\n\ndef run_simulation(n, p, K, tau, lambda_reg, blocks, sigma_y, seed):\n    \"\"\"\n    Runs a single simulation for one test case.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # 1. Synthetic Data Generation\n    X = np.empty((n, p))\n    current_col = 0\n    \n    for s_b, r_b in blocks:\n        if current_col + s_b > p: break\n        if r_b  1.0 and r_b > 0:\n            sigma_b_sq = (1 - r_b) / r_b\n            sigma_b = np.sqrt(sigma_b_sq)\n        else:\n             sigma_b = 0\n\n        z_b = rng.normal(loc=0, scale=1, size=n)\n        for _ in range(s_b):\n            epsilon_j = rng.normal(loc=0, scale=sigma_b, size=n)\n            X[:, current_col] = z_b + epsilon_j\n            current_col += 1\n\n    if current_col  p:\n        X[:, current_col:] = rng.normal(loc=0, scale=1, size=(n, p - current_col))\n\n    num_blocks = len(blocks)\n    if num_blocks >= 3:\n        q_indices = [sum(b[0] for b in blocks[:i]) for i in range(3)]\n    else:\n        q_indices = [0, 1, 2]\n\n    betas = [1.0, 0.8, -0.6]\n    y_signal = (betas[0] * X[:, q_indices[0]] +\n                betas[1] * X[:, q_indices[1]] +\n                betas[2] * X[:, q_indices[2]])\n    eta = rng.normal(loc=0, scale=sigma_y, size=n)\n    y = y_signal + eta\n\n    # CV setup\n    indices = np.arange(n)\n    fold_indices = np.array_split(indices, K)\n    \n    # Leaky Scheme: Prune once on all data\n    S_global = greedy_pruning(X, tau)\n    \n    mse_leak_list = []\n    mse_safe_list = []\n\n    for k in range(K):\n        val_idx = fold_indices[k]\n        train_idx = np.concatenate([fold_indices[i] for i in range(K) if i != k])\n        \n        X_train, y_train = X[train_idx], y[train_idx]\n        X_val, y_val = X[val_idx], y[val_idx]\n        \n        # --- Leakage-prone evaluation ---\n        if len(S_global) > 0:\n            X_train_leak = X_train[:, S_global]\n            X_val_leak = X_val[:, S_global]\n            w_leak = solve_ridge(X_train_leak, y_train, lambda_reg)\n            y_pred_leak = X_val_leak @ w_leak\n        else: # No features selected\n            y_pred_leak = np.mean(y_train) * np.ones(len(y_val))\n        mse_leak_list.append(np.mean((y_val - y_pred_leak)**2))\n        \n        # --- Leakage-free evaluation ---\n        S_k = greedy_pruning(X_train, tau)\n        if len(S_k) > 0:\n            X_train_safe = X_train[:, S_k]\n            X_val_safe = X_val[:, S_k]\n            w_safe = solve_ridge(X_train_safe, y_train, lambda_reg)\n            y_pred_safe = X_val_safe @ w_safe\n        else: # No features selected\n            y_pred_safe = np.mean(y_train) * np.ones(len(y_val))\n        mse_safe_list.append(np.mean((y_val - y_pred_safe)**2))\n\n    avg_mse_leak = np.mean(mse_leak_list)\n    avg_mse_safe = np.mean(mse_safe_list)\n    \n    delta = avg_mse_safe - avg_mse_leak\n    return delta\n\ndef solve():\n    test_cases = [\n        (200, 60, 5, 0.85, 1e-3, [(10, 0.95), (10, 0.95), (10, 0.95)], 0.5, 123),\n        (100, 80, 5, 0.80, 1e-2, [(15, 0.98), (15, 0.98), (15, 0.98), (15, 0.98)], 0.7, 456),\n        (150, 50, 5, 0.80, 1e-3, [], 0.5, 789),\n        (80, 200, 4, 0.90, 1e-1, [(30, 0.99), (30, 0.99), (30, 0.99), (30, 0.99), (30, 0.99)], 1.0, 101112),\n        (120, 40, 6, 0.50, 1e-3, [(10, 0.80), (10, 0.80)], 0.5, 131415),\n    ]\n\n    results = []\n    for params in test_cases:\n        n, p, K, tau, lambda_reg, blocks, sigma_y, seed = params\n        delta = run_simulation(n, p, K, tau, lambda_reg, blocks, sigma_y, seed)\n        results.append(delta)\n\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```", "id": "4553147"}]}