{"hands_on_practices": [{"introduction": "LASSO回归通过对系数的大小施加惩罚来进行特征选择，但这带来了一个关键问题：如果特征本身的尺度（或单位）不同，惩罚是否仍然公平？本练习旨在通过一个具体的计算案例，让您亲手验证特征标准化为何是应用LASSO前必不可少的一步[@problem_id:4538694]。您将看到，简单的尺度变换会如何戏剧性地改变LASSO对特征重要性的判断，从而理解标准化在确保模型公正性中的核心作用。", "problem": "一位放射组学分析师正在为一个定量影像生物标志物拟合一个线性模型，使用了从计算机断层扫描中测得的两个特征：$x_{1}$ 是以亨斯菲尔德单位表示的平均病灶强度，$x_{2}$ 是一个无量纲的灰度共生矩阵能量特征。响应 $y$ 和每个特征列都进行了中心化，因此 $y$ 和 $x_{j}$ 的均值为零。分析师使用了最小绝对收缩和选择算子 (LASSO) 惩罚项，其关于参数向量 $\\beta$ 的目标函数为\n$$\n\\frac{1}{2n}\\|y - X\\beta\\|^{2} + \\lambda \\sum_{j=1}^{p} |\\beta_{j}|,\n$$\n其中 $n$ 是病例数，$X$ 是以 $x_{1}$ 和 $x_{2}$ 为列的设计矩阵，$\\lambda \\ge 0$ 是正则化参数。假设这两个特征列是正交的，即 $\\frac{1}{n} x_{1}^{\\top}x_{2} = 0$。\n\n假设在 $n$ 个病灶上测得以下经验量：\n$$\n\\frac{1}{n} x_{1}^{\\top} x_{1} = 10{,}000, \\quad \\frac{1}{n} x_{2}^{\\top} x_{2} = 1, \\quad \\frac{1}{n} x_{1}^{\\top} y = 500, \\quad \\frac{1}{n} x_{2}^{\\top} y = 4.\n$$\n这些值反映出，$x_{1}$ 带有亨斯菲尔德单位，并且其方差远大于无量纲的 $x_{2}$。\n\n从 LASSO 目标函数的定义和给定的正交性出发，分析将每个特征缩放到单位方差如何改变 LASSO 系数的大小以及与 $\\lambda$ 相关的每个特征的选择阈值。令 $r_{\\text{unstd}}$ 表示在使用原始（未标准化）特征时，将 $x_{1}$ 的系数设为零所需的最小惩罚参数与将 $x_{2}$ 的系数设为零所需的最小惩罚参数之比；令 $r_{\\text{std}}$ 表示将每个特征列标准化为单位方差后的相同比率。计算单个数值 $r_{\\text{unstd}} / r_{\\text{std}}$。无需四舍五入；提供没有单位的精确值。", "solution": "该问题要求分析特征标准化对 LASSO 回归惩罚项的影响。我们需要求两个量 $r_{\\text{unstd}}$ 和 $r_{\\text{std}}$ 的比值，而这两个量本身就是正则化参数阈值的比值。\n\n需要最小化的 LASSO 目标函数由下式给出：\n$$L(\\beta) = \\frac{1}{2n}\\|y - X\\beta\\|^{2} + \\lambda \\sum_{j=1}^{p} |\\beta_{j}|$$\n其中 $\\beta$ 是系数向量，$X$ 是列代表特征的设计矩阵，$y$ 是响应向量，$n$ 是样本数量，$p$ 是特征数量（这里 $p=2$），$\\lambda \\ge 0$ 是正则化参数。\n\nLASSO 问题的解 $\\hat{\\beta}$ 必须满足 Karush-Kuhn-Tucker (KKT) 条件。在解处，目标函数关于每个系数 $\\beta_j$ 的次梯度必须包含零。$L(\\beta)$ 关于 $\\beta_j$ 的次梯度是：\n$$\\frac{\\partial L}{\\partial \\beta_j} = \\frac{1}{n} (-x_j^\\top(y-X\\beta)) + \\lambda \\cdot s_j$$\n其中 $s_j$ 是绝对值函数次微分的一个元素，即，如果 $\\beta_j \\neq 0$，则 $s_j = \\text{sgn}(\\beta_j)$；如果 $\\beta_j = 0$，则 $s_j \\in [-1, 1]$。\n将次梯度设为零，得到 $\\beta_j$ 的 KKT 条件：\n$$\\frac{1}{n} x_j^\\top(y-X\\hat{\\beta}) = \\lambda s_j$$\n展开项 $X\\hat{\\beta} = \\sum_{k=1}^p x_k \\hat{\\beta}_k$：\n$$\\frac{1}{n} x_j^\\top y - \\sum_{k=1}^p \\hat{\\beta}_k \\left(\\frac{1}{n} x_j^\\top x_k\\right) = \\lambda s_j$$\n问题陈述特征是正交的，即对于 $j \\neq k$，有 $\\frac{1}{n} x_j^\\top x_k = 0$。这极大地简化了条件，因为求和项收缩为 $k=j$ 的单个项：\n$$\\frac{1}{n} x_j^\\top y - \\hat{\\beta}_j \\left(\\frac{1}{n} x_j^\\top x_j\\right) = \\lambda s_j$$\n让我们将特征 $j$ 与响应 $y$ 的经验协方差定义为 $c_j = \\frac{1}{n} x_j^\\top y$，将特征 $j$ 的经验方差定义为 $v_j = \\frac{1}{n} x_j^\\top x_j$。该条件变为：\n$$c_j - v_j \\hat{\\beta}_j = \\lambda s_j$$\n问题要求将一个系数设为零所需的最小惩罚参数 $\\lambda$。系数 $\\hat{\\beta}_j$ 为零当且仅当 $s_j \\in [-1, 1]$。在这种情况下，条件变为 $c_j = \\lambda s_j$，这等价于 $|c_j| \\le \\lambda$。\n因此，确保 $\\hat{\\beta}_j = 0$ 的最小非负 $\\lambda$ 值为 $\\lambda_j = |c_j|$。该值表示系数 $\\beta_j$ 被收缩至恰好为零的阈值。\n\n基于这个一般性结果，我们现在可以分析两种情况。\n\n**情况1：未标准化的特征**\n我们使用给定的特征 $x_1$ 和 $x_2$。相关的经验量为：\n$$c_1 = \\frac{1}{n} x_1^\\top y = 500$$\n$$c_2 = \\frac{1}{n} x_2^\\top y = 4$$\n将 $\\beta_1$ 设为零所需的最小惩罚参数为：\n$$\\lambda_{1, \\text{unstd}} = |c_1| = |500| = 500$$\n将 $\\beta_2$ 设为零所需的最小惩罚参数为：\n$$\\lambda_{2, \\text{unstd}} = |c_2| = |4| = 4$$\n比率 $r_{\\text{unstd}}$ 定义为这些阈值的比：\n$$r_{\\text{unstd}} = \\frac{\\lambda_{1, \\text{unstd}}}{\\lambda_{2, \\text{unstd}}} = \\frac{500}{4} = 125$$\n\n**情况2：标准化的特征**\n标准化一个特征涉及将其缩放以使其具有单位方差。问题指出特征已经中心化（均值为零）。特征 $x_j$ 的样本方差为 $v_j = \\frac{1}{n} x_j^\\top x_j$。每个特征的缩放因子是其样本标准差 $s_j = \\sqrt{v_j}$。\n根据给定条件：\n$$v_1 = \\frac{1}{n} x_1^\\top x_1 = 10{,}000 \\implies s_1 = \\sqrt{10{,}000} = 100$$\n$$v_2 = \\frac{1}{n} x_2^\\top x_2 = 1 \\implies s_2 = \\sqrt{1} = 1$$\n标准化后的特征，我们记为 $x'_1$ 和 $x'_2$，是：\n$$x'_1 = \\frac{x_1}{s_1} = \\frac{x_1}{100}$$\n$$x'_2 = \\frac{x_2}{s_2} = \\frac{x_2}{1} = x_2$$\n我们现在考虑一个使用这些标准化特征的新 LASSO 问题。正交性条件得以保留：\n$$\\frac{1}{n} (x'_1)^\\top x'_2 = \\frac{1}{n} \\left(\\frac{x_1}{100}\\right)^\\top x_2 = \\frac{1}{100} \\left(\\frac{1}{n} x_1^\\top x_2\\right) = \\frac{1}{100} \\cdot 0 = 0$$\n新系数 $\\beta'_1$ 和 $\\beta'_2$ 的最小 $\\lambda$ 阈值由与响应的新经验协方差 $c'_1$ 和 $c'_2$ 决定：\n$$c'_1 = \\frac{1}{n} (x'_1)^\\top y = \\frac{1}{n} \\left(\\frac{x_1}{100}\\right)^\\top y = \\frac{1}{100} \\left(\\frac{1}{n} x_1^\\top y\\right) = \\frac{500}{100} = 5$$\n$$c'_2 = \\frac{1}{n} (x'_2)^\\top y = \\frac{1}{n} x_2^\\top y = 4$$\n将 $\\beta'_1$ 设为零所需的最小惩罚参数为：\n$$\\lambda_{1, \\text{std}} = |c'_1| = |5| = 5$$\n将 $\\beta'_2$ 设为零所需的最小惩罚参数为：\n$$\\lambda_{2, \\text{std}} = |c'_2| = |4| = 4$$\n比率 $r_{\\text{std}}$ 为：\n$$r_{\\text{std}} = \\frac{\\lambda_{1, \\text{std}}}{\\lambda_{2, \\text{std}}} = \\frac{5}{4}$$\n\n**最终计算**\n问题要求计算代表比率 $r_{\\text{unstd}} / r_{\\text{std}}$ 的单个数值。\n$$\\frac{r_{\\text{unstd}}}{r_{\\text{std}}} = \\frac{125}{\\frac{5}{4}} = 125 \\times \\frac{4}{5} = \\frac{500}{5} = 100$$\n这个结果突显了特征缩放如何显著影响 LASSO 的行为，因为 LASSO 对预测变量的尺度很敏感。未标准化的特征表明 $x_1$ 的重要性远超 $x_2$（相差125倍），而标准化后则显示它们的重要性更为可比（相差1.25倍）。这两个比率的比值为100。", "answer": "$$\\boxed{100}$$", "id": "4538694"}, {"introduction": "理解了数据准备的重要性后，我们来揭秘LASSO模型“幕后”的训练过程。LASSO模型是如何通过算法找到最优系数的呢？本练习将带您深入了解坐标下降法——一种高效拟合LASSO模型的关键算法[@problem_id:4538688]。您将亲手推导并执行单步的系数更新，即“软阈值”操作，从而直观感受模型迭代求解的机制及其在处理高维数据时的计算优势。", "problem": "一个放射组学流程被用于从计算机断层扫描（CT）纹理特征中预测肿瘤侵袭性的一个连续标志物。为执行嵌入式特征选择，该模型采用最小绝对收缩和选择算子（LASSO）回归，该方法最小化惩罚经验风险：$$\\frac{1}{2n}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$$ 其中 $y \\in \\mathbb{R}^{n}$ 是响应变量，$X \\in \\mathbb{R}^{n \\times p}$ 是特征矩阵，$\\beta \\in \\mathbb{R}^{p}$ 是系数，$n$ 是样本量，$\\lambda > 0$ 是正则化参数。特征已被中心化，因此截距项未被惩罚，并且在本次更新中可以忽略。\n\n您正在当前迭代点上实现坐标下降法，保持除 $\\beta_{j}$ 之外的所有系数固定。定义关于特征 $j$ 的偏残差为 $$r^{(j)} = y - \\sum_{k \\neq j} x_{k}\\beta_{k},$$ 其中 $x_{k} \\in \\mathbb{R}^{n}$ 表示 $X$ 的第 $k$ 列。关于 $\\beta_{j}$ 的一维子问题是 $$\\min_{\\beta_{j} \\in \\mathbb{R}} \\;\\; \\frac{1}{2n}\\sum_{i=1}^{n}\\left(r^{(j)}_{i} - x_{ij}\\beta_{j}\\right)^{2} + \\lambda |\\beta_{j}|.$$\n\n从给定的目标和定义出发，通过求解此一维凸问题来推导 $\\beta_{j}$ 的坐标下降更新规则，并根据偏残差的软阈值来解释该解。然后，使用以下针对 $n=8$ 名患者队列中单个特征的数据计算数值更新：\n- 特征列 $x_{j} = [\\,0.5,\\,-1.1,\\,0.7,\\,0.0,\\,1.4,\\,-0.6,\\,0.8,\\,-1.7\\,]$,\n- 偏残差 $r^{(j)} = [\\,2.3,\\,-0.9,\\,1.2,\\,0.0,\\,3.1,\\,-1.5,\\,1.0,\\,-2.8\\,]$,\n- 正则化参数 $\\lambda = 0.3$.\n\n提供此坐标下降步骤中 $\\beta_{j}$ 的更新值。将您的答案四舍五入到四位有效数字。最后，解释为什么在 $p \\gg n$ 的高维放射组学设置中，这种更新机制在计算上是高效的。", "solution": "该问题要求三件事：首先，推导 LASSO 回归系数 $\\beta_j$ 的坐标下降更新规则；其次，对给定特征进行此更新的数值计算；第三，解释该方法在高维设置中的计算效率。\n\n该问题被验证为有科学依据、适定且客观。所有定义和前提在统计学习理论和 LASSO 回归的背景下都是标准的。所提供的数据是自洽且一致的。因此，可以推导出解决方案。\n\n**第1部分：坐标下降更新的推导**\n\n目标是求解系数 $\\beta_j$ 的一维子问题，同时保持所有其他系数 $\\beta_k$（$k \\neq j$）固定。关于 $\\beta_j$ 要最小化的目标函数由下式给出：\n$$L(\\beta_j) = \\frac{1}{2n}\\sum_{i=1}^{n}\\left(r^{(j)}_{i} - x_{ij}\\beta_{j}\\right)^{2} + \\lambda |\\beta_{j}|$$\n其中偏残差为 $r^{(j)} = y - \\sum_{k \\neq j} x_{k}\\beta_{k}$。\n这个目标函数是凸函数，因为它是凸二次函数和凸绝对值函数之和。然而，绝对值项 $|\\beta_j|$ 使得该函数在 $\\beta_j = 0$ 处不可微。我们可以使用次梯度微积分来找到最小值。$\\beta_j$ 成为最小值点的条件是 $0$ 必须包含在 $L(\\beta_j)$ 的次微分中，记为 $\\partial L(\\beta_j)$。\n\n次微分计算如下：\n$$ \\partial L(\\beta_j) = \\frac{d}{d\\beta_j} \\left( \\frac{1}{2n}\\sum_{i=1}^{n}\\left(r^{(j)}_{i} - x_{ij}\\beta_{j}\\right)^{2} \\right) + \\lambda \\cdot \\partial(|\\beta_j|) $$\n二次部分的导数是：\n$$ \\frac{1}{2n}\\sum_{i=1}^{n} 2(r^{(j)}_{i} - x_{ij}\\beta_{j})(-x_{ij}) = -\\frac{1}{n}\\sum_{i=1}^{n} (x_{ij}r^{(j)}_{i} - x_{ij}^2\\beta_j) = \\frac{1}{n} \\left( \\beta_j \\sum_{i=1}^{n}x_{ij}^2 - \\sum_{i=1}^{n}x_{ij}r^{(j)}_{i} \\right) $$\n我们定义 $c_j = \\sum_{i=1}^{n}x_{ij}r^{(j)}_{i} = x_j^T r^{(j)}$ 和 $S_j = \\sum_{i=1}^{n}x_{ij}^2 = \\|x_j\\|_2^2$。导数变为 $\\frac{1}{n} (S_j \\beta_j - c_j)$。\n\n绝对值函数的次微分是：\n$$ \\partial(|\\beta_j|) = \\begin{cases} \\{1\\}  \\text{if } \\beta_j > 0 \\\\ \\{-1\\}  \\text{if } \\beta_j  0 \\\\ [-1, 1]  \\text{if } \\beta_j = 0 \\end{cases} $$\n\n现在，我们设置 $0 \\in \\partial L(\\beta_j)$：\n$$ 0 \\in \\frac{1}{n} (S_j \\beta_j - c_j) + \\lambda \\cdot \\partial(|\\beta_j|) $$\n\n我们分析 $\\beta_j$ 的三种情况：\n\n**情况1：$\\beta_j  0$**\n次梯度是一个单值，$\\partial(|\\beta_j|) = \\{1\\}$。条件变成一个方程：\n$$ 0 = \\frac{1}{n} (S_j \\beta_j - c_j) + \\lambda $$\n$$ S_j \\beta_j - c_j = -n\\lambda $$\n$$ \\beta_j = \\frac{c_j - n\\lambda}{S_j} $$\n为使此解与假设 $\\beta_j  0$ 一致，我们必须有 $c_j - n\\lambda  0$，这意味着 $c_j  n\\lambda$。\n\n**情况2：$\\beta_j  0$**\n次梯度是 $\\partial(|\\beta_j|) = \\{-1\\}$。条件变为：\n$$ 0 = \\frac{1}{n} (S_j \\beta_j - c_j) - \\lambda $$\n$$ S_j \\beta_j - c_j = n\\lambda $$\n$$ \\beta_j = \\frac{c_j + n\\lambda}{S_j} $$\n为使此解与假设 $\\beta_j  0$ 一致，我们必须有 $c_j + n\\lambda  0$，这意味着 $c_j  -n\\lambda$。\n\n**情况3：$\\beta_j = 0$**\n次梯度是一个区间，$\\partial(|\\beta_j|) = [-1, 1]$。条件变为一个包含关系：\n$$ 0 \\in \\frac{1}{n} (S_j \\cdot 0 - c_j) + \\lambda [-1, 1] $$\n$$ 0 \\in -\\frac{c_j}{n} + [-\\lambda, \\lambda] $$\n这意味着存在某个值 $g \\in [-\\lambda, \\lambda]$ 使得 $0 = -c_j/n + g$，或者 $g = c_j/n$。此情况成立的条件是 $g$ 的值必须在其允许的范围内：\n$$ -\\lambda \\le \\frac{c_j}{n} \\le \\lambda \\iff |c_j| \\le n\\lambda $$\n\n结合这三种情况，我们得到 $\\beta_j$ 的完整解：\n$$ \\beta_j = \\begin{cases} \\frac{c_j - n\\lambda}{S_j}  \\text{if } c_j  n\\lambda \\\\ \\frac{c_j + n\\lambda}{S_j}  \\text{if } c_j  -n\\lambda \\\\ 0  \\text{if } |c_j| \\le n\\lambda \\end{cases} $$\n这可以用软阈值算子 $S_{\\alpha}(z) = \\text{sign}(z) \\max(0, |z|-\\alpha)$ 紧凑地写出。解是：\n$$ \\beta_j = \\frac{S_{n\\lambda}(c_j)}{S_j} = \\frac{S_{n\\lambda}(x_j^T r^{(j)})}{\\|x_j\\|_2^2} $$\n该表达式表示逐坐标更新规则。它表明，更新是通过计算特征向量 $x_j$ 与其偏残差 $r^{(j)}$ 的点积，对该值进行软阈值处理，然后用特征向量的欧几里得范数的平方进行归一化来找到的。\n\n**第2部分：数值计算**\n\n给定数据：\n- 样本量 $n = 8$\n- 特征向量 $x_{j} = [\\,0.5,\\,-1.1,\\,0.7,\\,0.0,\\,1.4,\\,-0.6,\\,0.8,\\,-1.7\\,]$\n- 偏残差 $r^{(j)} = [\\,2.3,\\,-0.9,\\,1.2,\\,0.0,\\,3.1,\\,-1.5,\\,1.0,\\,-2.8\\,]$\n- 正则化参数 $\\lambda = 0.3$\n\n首先，我们计算 $c_j = x_j^T r^{(j)}$：\n$$ c_j = (0.5)(2.3) + (-1.1)(-0.9) + (0.7)(1.2) + (0.0)(0.0) + (1.4)(3.1) + (-0.6)(-1.5) + (0.8)(1.0) + (-1.7)(-2.8) $$\n$$ c_j = 1.15 + 0.99 + 0.84 + 0.0 + 4.34 + 0.90 + 0.80 + 4.76 = 13.78 $$\n\n接下来，我们计算 $S_j = \\|x_j\\|_2^2$：\n$$ S_j = (0.5)^2 + (-1.1)^2 + (0.7)^2 + (0.0)^2 + (1.4)^2 + (-0.6)^2 + (0.8)^2 + (-1.7)^2 $$\n$$ S_j = 0.25 + 1.21 + 0.49 + 0.0 + 1.96 + 0.36 + 0.64 + 2.89 = 7.80 $$\n\n然后，我们计算软阈值算子的阈值：\n$$ n\\lambda = 8 \\times 0.3 = 2.4 $$\n\n现在我们应用更新规则。我们比较 $|c_j|$ 和 $n\\lambda$：\n$$ c_j = 13.78  n\\lambda = 2.4 $$\n这对应于我们推导出的解的第一种情况。\n$$ \\beta_j = \\frac{c_j - n\\lambda}{S_j} = \\frac{13.78 - 2.4}{7.80} = \\frac{11.38}{7.80} \\approx 1.458974... $$\n四舍五入到四位有效数字，$\\beta_j$ 的更新值为 $1.459$。\n\n**第3部分：在高维设置（$p \\gg n$）中的计算效率**\n\n坐标下降算法对于高维问题非常高效，特别是当特征数量 $p$ 远大于样本数量 $n$ 时，这有几个关键原因：\n\n1.  **避免大规模矩阵运算**：最显著的优点是它避免了计算、存储或求逆 $p \\times p$ 的 Gram 矩阵 $X^T X$ 的需要。在 $p \\gg n$ 的情况下，这个矩阵将是巨大的（$p \\times p$），像求逆（成本为 $O(p^3)$）甚至其构建（成本为 $O(p^2 n)$）这样的操作在计算上是不可行的。坐标下降通过将问题分解为 $p$ 个一维优化问题来规避这一点。\n\n2.  **每次更新成本低**：每次坐标更新只涉及长度为 $n$ 的向量操作。更新 $\\beta_j$ 的主要计算是点积 $x_j^T r$ 和残差的向量更新，每个操作的成本为 $O(n)$。因此，遍历所有 $p$ 个特征的一个完整周期的总复杂度为 $O(np)$。这种与 $p$ 的线性扩展比多项式扩展（$O(p^2)$ 或 $O(p^3)$）要容易处理得多。单步近端梯度下降的成本也是 $O(np)$，但坐标下降通常在更少的有效遍数内收敛，特别是与活性集策略结合使用时。\n\n3.  **高效的残差更新**：偏残差 $r^{(j)}$ 不需要为每个 $j$ 从头重新计算。相反，可以维护一个全局残差 $r = y - X\\beta$。更新所需的项 $x_j^T r^{(j)}$ 可以高效地计算为 $x_j^T r^{(j)} = x_j^T(r + x_j \\beta_j)$，其中 $r$ 和 $\\beta_j$ 是更新前的值。将 $\\beta_j$ 更新为 $\\beta_j^{\\text{new}}$ 后，通过一个简单快速的秩-1 更新来更新全局残差：$r^{\\text{new}} = r - x_j(\\beta_j^{\\text{new}} - \\beta_j)$。这项工作的成本仅为 $O(n)$。\n\n4.  **利用稀疏性**：LASSO 旨在产生稀疏解（许多 $\\beta_j = 0$）。坐标下降法特别适合这种情况。随着算法的进行，许多系数被设置为零，并可能在后续迭代中保持为零（如果 $|x_j^T r| \\le n\\lambda$）。这允许使用“活性集”方法，即算法仅在具有非零系数的特征子集上循环，从而大大减小了 $p$ 的有效值并加速了收敛。\n\n在放射组学中，通常会从几百名患者（$n \\sim 100-500$）的队列中提取数千个特征（$p \\gg 1000$），这些计算优势使得坐标下降法成为拟合 LASSO 模型的主力算法。", "answer": "$$\\boxed{1.459}$$", "id": "4538688"}, {"introduction": "一个LASSO模型的性能和稀疏性由正则化参数 $\\lambda$ 控制，但如何选择最佳的 $\\lambda$ 值呢？本练习模拟了一个在放射组学研究中常见的场景，您需要利用交叉验证的结果来做出决策[@problem_id:4538731]。您将学习并应用“单标准误规则”，这是一种在追求最佳预测性能和模型简洁性之间取得平衡的实用策略，对于构建稳定且可复现的科学模型至关重要。", "problem": "一个放射组学团队正在构建一个二元分类器，以根据高维图像衍生特征来预测治疗反应。他们拟合了一个LASSO（Least Absolute Shrinkage and Selection Operator）逻辑回归模型，该模型通过一个由正则化参数$\\lambda$控制的$\\ell_1$惩罚项将某些系数收缩至恰好为零，从而内嵌了特征选择功能。为了估计泛化性能，他们执行$K$折交叉验证（CV），其中$K$表示数据的分区数量；在每一折中，模型在$K-1$个部分上进行训练，并在留出的部分上进行评估，然后将各折的损失进行平均，以近似预期的样本外风险。该团队考虑了在$\\lambda$的候选值下，交叉验证的平均负对数似然（交叉熵），以及相应的跨折样本标准差。结果（在$K=5$折上的平均值，以及跨折样本标准差）和拟合的LASSO模型中的非零系数数量如下：\n\n- $\\lambda = 0.001$：平均损失 $0.512$，标准差 $0.020$，非零系数 $90$。\n- $\\lambda = 0.01$：平均损失 $0.505$，标准差 $0.024$，非零系数 $45$。\n- $\\lambda = 0.1$：平均损失 $0.508$，标准差 $0.022$，非零系数 $20$。\n- $\\lambda = 0.2$：平均损失 $0.511$，标准差 $0.021$，非零系数 $12$。\n- $\\lambda = 0.5$：平均损失 $0.519$，标准差 $0.018$，非零系数 $6$。\n\n高年级本科生被要求使用以下基础知识从第一性原理出发进行推理：$K$折交叉验证的经验风险近似于预期的泛化风险；对各折结果取平均得到的均值，其不确定性可以通过均值标准误来量化；并且在$p \\gg n$且特征高度相关的情况下，选择更简单的模型（非零系数更少）通常会降低方差并提高稳定性。\n\n哪个选项最好地描述并应用了交叉验证中的“单标准误规则”，并证明了在放射组学中应用此规则以偏好更简单、更稳定的模型（而非交叉验证误差最小的模型）的合理性？\n\nA. 选择$\\lambda = 0.01$，因为它具有最小的平均交叉验证损失$0.505$；在放射组学中，最小化平均交叉验证损失总是最优的，并且在使用交叉验证时，模型复杂度是无关紧要的。\n\nB. 选择$\\lambda = 0.2$，因为其平均交叉验证损失$0.511$处于最小交叉验证损失模型的一个标准误范围内，并且它产生更少的非零系数（$12$个），这在高维放射组学中（小样本和相关特征会放大方差）促进了稳定性和可复现性。\n\nC. 选择$\\lambda = 0.5$，因为选择最大的$\\lambda$总是能因强正则化而保证最佳的泛化性能；交叉验证应该只用于检查训练损失，而不是指导模型选择。\n\nD. 选择$\\lambda = 0.1$，因为其平均交叉验证损失处于最小值的一个标准差范围内；在放射组学中，平衡偏差和方差需要使用“单标准差规则”而不是“单标准误规则”。", "solution": "用户提供了一个关于在放射组学应用的LASSO逻辑回归背景下进行模型选择的问题。任务是应用“单标准误规则”使用交叉验证数据进行超参数调整。\n\n### 问题验证\n\n首先，有必要对问题陈述进行严格的验证。\n\n**步骤1：提取已知条件**\n- **模型**：LASSO（Least Absolute Shrinkage and Selection Operator）逻辑回归。\n- **惩罚项**：由正则化参数$\\lambda$控制的$\\ell_1$惩罚项。\n- **性能估计**：$K$折交叉验证（CV），其中$K=5$。\n- **性能度量**：交叉验证的平均负对数似然（交叉熵损失），表示为“平均损失”。\n- **不确定性度量**：损失的跨折样本标准差。\n- **数据**：一组候选$\\lambda$值及其对应的性能度量：\n    - 对于 $\\lambda = 0.001$：平均损失 = $0.512$，标准差 = $0.020$，非零系数 = $90$。\n    - 对于 $\\lambda = 0.01$：平均损失 = $0.505$，标准差 = $0.024$，非零系数 = $45$。\n    - 对于 $\\lambda = 0.1$：平均损失 = $0.508$，标准差 = $0.022$，非零系数 = $20$。\n    - 对于 $\\lambda = 0.2$：平均损失 = $0.511$，标准差 = $0.021$，非零系数 = $12$。\n    - 对于 $\\lambda = 0.5$：平均损失 = $0.519$，标准差 = $0.018$，非零系数 = $6$。\n- **指导原则**：\n    - $K$折交叉验证近似于预期的泛化风险。\n    - 平均交叉验证损失的不确定性通过均值标准误来量化。\n    - 更简单的模型（非零系数更少）因其能减少方差并提高稳定性而更受青睐，尤其是在放射组学中常见的高维（$p \\gg n$）且特征相关的设置中。\n\n**步骤2：使用提取的已知条件进行验证**\n- **科学基础**：该问题在科学上是合理的。它描述了统计学习和机器学习中一种标准且被广泛接受的方法论，即使用LASSO进行特征选择和正则化，使用交叉验证进行超参数调整，以及使用“单标准误规则”进行模型选择。这些技术在放射组学等以高维特征空间为特点的领域中尤其相关且常用。\n- **适定性**：该问题是适定的。它提供了所有必要的数值数据（不同$\\lambda$下的平均损失、标准差、折数、系数数量）和一个明确的目标：将“单标准误规则”应用于这些数据以选择最优模型。从给定信息可以推导出一个唯一且稳定的解。\n- **客观性**：该问题以客观、精确和标准的技术语言陈述。它没有歧义、主观性或个人观点。\n\n**步骤3：结论与行动**\n问题陈述是有效的。我将继续推导解决方案。\n\n### 推导与解决方案\n\n目标是使用“单标准误规则”选择最优的正则化参数$\\lambda$。该规则是一种启发式方法，旨在选择一个更简约的模型，其泛化性能与具有最小交叉验证误差的模型在统计上无法区分。\n\n应用该规则的步骤如下：\n1.  确定具有最小平均交叉验证损失$L_{min}$的模型。设其对应的正则化参数为$\\lambda_{min}$。\n2.  计算该模型平均损失的标准误。标准误（$SE$）是各折损失的标准差$S$除以折数$K$的平方根。因此，$SE_{min} = S(\\lambda_{min}) / \\sqrt{K}$。\n3.  建立一个误差阈值：$L_{threshold} = L_{min} + SE_{min}$。\n4.  在所有平均损失$L(\\lambda)$小于或等于$L_{threshold}$的模型集合中，选择最简约的一个。在LASSO的背景下，最简约的模型是$\\lambda$值最大的模型，因为这对应于更强的正则化，从而非零系数最少。\n\n让我们将这些步骤应用于所提供的数据。\n\n1.  **找到最小平均损失**：通过检查数据，最小平均损失为$L_{min} = 0.505$，出现在$\\lambda_{min} = 0.01$处。\n\n2.  **计算标准误**：在$\\lambda_{min} = 0.01$处模型的标准差为$S(\\lambda_{min}) = 0.024$。折数为$K=5$。标准误为：\n    $$SE_{min} = \\frac{S(\\lambda_{min})}{\\sqrt{K}} = \\frac{0.024}{\\sqrt{5}}$$\n    数值上，$\\sqrt{5} \\approx 2.236$。\n    $$SE_{min} \\approx \\frac{0.024}{2.236} \\approx 0.01073$$\n\n3.  **建立阈值**：阈值是最小损失加上一个标准误：\n    $$L_{threshold} = L_{min} + SE_{min} \\approx 0.505 + 0.01073 = 0.51573$$\n\n4.  **在阈值内选择最简约的模型**：我们现在找出所有平均损失小于或等于$L_{threshold} \\approx 0.51573$的模型，然后选择其中$\\lambda$最大（因此非零系数最少）的模型。\n\n    - $\\lambda = 0.001$：平均损失 $0.512 \\le 0.51573$。系数：$90$。\n    - $\\lambda = 0.01$：平均损失 $0.505 \\le 0.51573$。系数：$45$。\n    - $\\lambda = 0.1$：平均损失 $0.508 \\le 0.51573$。系数：$20$。\n    - $\\lambda = 0.2$：平均损失 $0.511 \\le 0.51573$。系数：$12$。\n    - $\\lambda = 0.5$：平均损失 $0.519  0.51573$。该模型不是候选模型。\n\n    候选模型对应于$\\lambda \\in \\{0.001, 0.01, 0.1, 0.2\\}$。我们现在必须从这个集合中选择最简约的模型。简约性对应于简单性，在LASSO中意味着更少的非零系数，这通过最大的$\\lambda$实现。这个集合中最大的$\\lambda$是$\\lambda = 0.2$。该模型只有$12$个非零系数，使其比最小损失模型（有$45$个系数）简单得多。\n\n做出此选择的理由，尤其是在放射组学中，是高维数据容易出现过拟合和特征选择不稳定的问题。“单标准误规则”提供了一个数据驱动的理由，以牺牲统计上不显著的预测精度为代价，来换取一个更简单、更稳定，并且可能更具泛化性和可复现性的模型。\n\n### 逐项分析选项\n\n**A. 选择$\\lambda = 0.01$，因为它具有最小的平均交叉验证损失$0.505$；在放射组学中，最小化平均交叉验证损失总是最优的，并且在使用交叉验证时，模型复杂度是无关紧要的。**\n该选项仅选择了交叉验证误差最小的模型，忽略了“单标准误规则”以及问题对模型简单性的强调。声称模型复杂度无关紧要的说法是根本错误的，尤其是在放射组学典型的$p \\gg n$场景中。这种方法有选择一个过拟合且不稳定模型的风险。\n**结论：错误。**\n\n**B. 选择$\\lambda = 0.2$，因为其平均交叉验证损失$0.511$处于最小交叉验证损失模型的一个标准误范围内，并且它产生更少的非零系数（$12$个），这在高维放射组学中（小样本和相关特征会放大方差）促进了稳定性和可复现性。**\n该选项正确地实施了“单标准误规则”。如上所述，$\\lambda = 0.2$的模型的平均损失为$0.511$，在$0.51573$的阈值之内。它是满足此标准的最简约的模型（最大的$\\lambda$，最少的非零系数）。所提供的理由正确地阐述了这种方法的好处——通过减少方差来促进稳定性和可复现性，这在放射组学等高维领域是一个关键问题。\n**结论：正确。**\n\n**C. 选择$\\lambda = 0.5$，因为选择最大的$\\lambda$总是能因强正则化而保证最佳的泛化性能；交叉验证应该只用于检查训练损失，而不是指导模型选择。**\n该选项基于多个错误的前提。首先，选择最大的$\\lambda$并不能保证最佳的泛化性能；过度正则化会导致欠拟合（高偏差）和较差的性能，该模型较高的损失（$0.519$）就证明了这一点。其次，断言交叉验证仅用于检查训练损失是对其目的的严重误解；交叉验证是估计样本外误差和指导模型选择（超参数调整）的基石技术。第三，$\\lambda = 0.5$的模型不满足“单标准误规则”的标准。\n**结论：错误。**\n\n**D. 选择$\\lambda = 0.1$，因为其平均交叉验证损失处于最小值的一个标准差范围内；在放射组学中，平衡偏差和方差需要使用“单标准差规则”而不是“单标准误规则”。**\n该选项将“单标准误规则”与一个非标准的“单标准差规则”混淆了。正确的启发式规则使用均值标准误（$SE = S/\\sqrt{K}$），而不是各折损失的标准差（$S$）。使用标准差（在此例中为$0.024$）会产生一个过于宽松的阈值，并且不是常规方法。此外，即使在正确的“单标准误规则”下，$\\lambda=0.1$也不是最终选择，因为$\\lambda=0.2$也满足标准并且更简约。\n**结论：错误。**", "answer": "$$\\boxed{B}$$", "id": "4538731"}]}