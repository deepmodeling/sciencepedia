{"hands_on_practices": [{"introduction": "本练习为衡量神经系统中的信息传输奠定了理论基础。通过推导一个简单线性高斯信道的互信息，您将掌握信号强度、噪声与可被可靠传递的信息量之间的根本关系。这个结果是信息论的基石，并为理解神经编码的极限提供了一个强有力的类比。[@problem_id:5037464]", "problem": "在一个简化的单一感觉神经元的神经编码模型中，一个标量刺激 $S$ 代表了围绕基线的偏差，并被建模为一个均值为零、方差为 $\\sigma_{S}^{2}$ 的高斯随机变量，即 $S \\sim \\mathcal{N}(0,\\sigma_{S}^{2})$。神经元的响应 $R$ 被假定为线性的，并且被独立的加性噪声 $N \\sim \\mathcal{N}(0,\\sigma_{N}^{2})$ 所干扰，其中 $S$ 和 $N$ 相互独立，因此 $R = S + N$。以互信息关于微分熵的定义以及高斯随机变量的成熟性质作为基本出发点，推导在该模型中刺激与响应之间的互信息 $I(S;R)$ 的一个闭式表达式。使用自然对数，以便信息以奈特（nats）为单位进行度量。将您的最终答案用 $\\sigma_{S}^{2}$ 和 $\\sigma_{N}^{2}$ 进行符号化表示，并且不要近似。最终答案必须是一个单一的闭式解析表达式。", "solution": "这个问题是有效的，因为它在信息论和统计学中有科学依据，问题提法得当，有唯一且有意义的解，并且使用客观、正式的语言陈述。所有必要的信息都已提供，没有矛盾或歧义。\n\n目标是推导刺激 $S$ 和神经响应 $R$ 之间的互信息 $I(S;R)$ 的闭式表达式。该模型由以下给定条件定义：\n1.  刺激 $S$ 是一个均值为零、方差为 $\\sigma_{S}^{2}$ 的高斯随机变量，记为 $S \\sim \\mathcal{N}(0, \\sigma_{S}^{2})$。\n2.  响应 $R$ 由线性关系 $R = S + N$ 给出。\n3.  噪声 $N$ 是一个独立的均值为零、方差为 $\\sigma_{N}^{2}$ 的高斯随机变量，记为 $N \\sim \\mathcal{N}(0, \\sigma_{N}^{2})$。\n4.  刺激 $S$ 和噪声 $N$ 在统计上是独立的。\n\n我们从互信息关于微分熵的定义开始。对于连续随机变量 $S$ 和 $R$，互信息由下式给出：\n$$\nI(S;R) = H(R) - H(R|S)\n$$\n其中 $H(R)$ 是响应 $R$ 的微分熵，$H(R|S)$ 是在给定 $S$ 的情况下 $R$ 的条件微分熵。我们将分别计算这两项。\n\n首先，我们确定响应 $R$ 的分布。由于 $R$ 是两个独立的高斯随机变量 $S$ 和 $N$ 的和，因此 $R$ 本身也是一个高斯随机变量。\n$R$ 的均值是 $S$ 和 $N$ 的均值之和：\n$$\nE[R] = E[S+N] = E[S] + E[N] = 0 + 0 = 0\n$$\n由于它们的独立性，$R$ 的方差是 $S$ 和 $N$ 的方差之和：\n$$\n\\text{Var}(R) = \\text{Var}(S+N) = \\text{Var}(S) + \\text{Var}(N) = \\sigma_{S}^{2} + \\sigma_{N}^{2}\n$$\n因此，响应的分布为 $R \\sim \\mathcal{N}(0, \\sigma_{S}^{2} + \\sigma_{N}^{2})$。\n\n对于一个高斯随机变量 $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$，当使用自然对数（以奈特为单位度量）时，其微分熵 $H(X)$ 由以下公式给出：\n$$\nH(X) = \\frac{1}{2} \\ln(2\\pi e \\sigma^2)\n$$\n将此公式应用于响应变量 $R$，我们得到其熵：\n$$\nH(R) = \\frac{1}{2} \\ln\\left(2\\pi e (\\sigma_{S}^{2} + \\sigma_{N}^{2})\\right)\n$$\n\n接下来，我们计算条件熵 $H(R|S)$。这一项表示在已知 $S$ 的值时，$R$ 中剩余的不确定性。如果我们以一个特定的刺激值 $S=s$为条件，响应方程变为 $R = s + N$。由于在这种情况下 $s$ 是一个固定值，所以 $R$ 中的随机性完全来自噪声 $N$。在给定 $S=s$ 的条件下，$R$ 的分布是一个高斯分布，其均值为 $E[R|S=s] = E[s+N] = s + E[N] = s$，方差为 $\\text{Var}(R|S=s) = \\text{Var}(s+N) = \\text{Var}(N) = \\sigma_{N}^{2}$。所以，$R|S=s \\sim \\mathcal{N}(s, \\sigma_{N}^{2})$。\n\n这个条件分布的熵是：\n$$\nH(R|S=s) = \\frac{1}{2} \\ln(2\\pi e \\sigma_{N}^{2})\n$$\n注意，这个表达式是一个常数，不依赖于 $s$ 的具体值。条件熵 $H(R|S)$ 是 $H(R|S=s)$ 在 $S$ 所有可能值上的期望：\n$$\nH(R|S) = E_S[H(R|S=s)] = E_S\\left[\\frac{1}{2} \\ln(2\\pi e \\sigma_{N}^{2})\\right]\n$$\n由于期望内的表达式相对于 $S$ 是一个常数，所以期望就是该常数本身：\n$$\nH(R|S) = \\frac{1}{2} \\ln(2\\pi e \\sigma_{N}^{2})\n$$\n\n最后，我们将 $H(R)$ 和 $H(R|S)$ 的表达式代入互信息的定义中：\n$$\nI(S;R) = H(R) - H(R|S) = \\frac{1}{2} \\ln\\left(2\\pi e (\\sigma_{S}^{2} + \\sigma_{N}^{2})\\right) - \\frac{1}{2} \\ln(2\\pi e \\sigma_{N}^{2})\n$$\n使用对数性质 $\\ln(a) - \\ln(b) = \\ln(a/b)$，我们可以简化表达式：\n$$\nI(S;R) = \\frac{1}{2} \\left[ \\ln\\left(2\\pi e (\\sigma_{S}^{2} + \\sigma_{N}^{2})\\right) - \\ln(2\\pi e \\sigma_{N}^{2}) \\right] = \\frac{1}{2} \\ln\\left(\\frac{2\\pi e (\\sigma_{S}^{2} + \\sigma_{N}^{2})}{2\\pi e \\sigma_{N}^{2}}\\right)\n$$\n分子和分母中的项 $2\\pi e$ 被消去：\n$$\nI(S;R) = \\frac{1}{2} \\ln\\left(\\frac{\\sigma_{S}^{2} + \\sigma_{N}^{2}}{\\sigma_{N}^{2}}\\right)\n$$\n这可以通过分离分数写成更标准的形式：\n$$\nI(S;R) = \\frac{1}{2} \\ln\\left(1 + \\frac{\\sigma_{S}^{2}}{\\sigma_{N}^{2}}\\right)\n$$\n项 $\\frac{\\sigma_{S}^{2}}{\\sigma_{N}^{2}}$ 是系统的信噪比（SNR）。这个结果是信息论中关于高斯信道的一个基本公式。", "answer": "$$\n\\boxed{\\frac{1}{2} \\ln\\left(1 + \\frac{\\sigma_{S}^{2}}{\\sigma_{N}^{2}}\\right)}\n$$", "id": "5037464"}, {"introduction": "从纯理论转向数据分析，本实践将展示如何将模型拟合到观测到的脉冲序列中。您将为一个非齐次泊松过程（一个用于模拟时变神经放电的主力模型）实现最大似然估计（MLE）。此练习将让您亲身体验表征神经元活动如何被动态因素驱动的最基本技术之一。[@problem_id:5037459]", "problem": "考虑在区间 $[0,1]$ 秒上的一个非齐次泊松尖峰序列模型，其条件强度函数指定为 $\\lambda(t) = \\lambda_0 \\left( 1 + \\sin(2\\pi t) \\right)$，其中 $t \\in [0,1]$。假设在时间 $\\{t_i\\}_{i=1}^N$（单位为秒）观测到尖峰。使用非齐次泊松过程的以下基本事实：(i) 该过程具有独立增量，(ii) 在一个微小区间 $[t,t+\\Delta t]$ 内发生一个事件的概率约为 $\\lambda(t)\\Delta t$，以及 (iii) 在强度 $\\lambda(t)$ 下观测到尖峰时间 $\\{t_i\\}$ 的确切似然与 $\\prod_{i=1}^N \\lambda(t_i)$ 乘以强度在观测窗口上的负积分的指数成正比。您的任务是：\n\n1. 从上述基本事实出发，推导在给定强度下观测到的尖峰的对数似然 $\\log L(\\lambda_0 \\mid \\{t_i\\})$。然后，在约束 $\\lambda_0 \\ge 0$ 下，确定使该对数似然最大化的 $\\lambda_0$ 的值（最大似然估计(MLE)）。报告拟合的 $\\lambda_0$，单位为尖峰/秒。\n\n2. 计算在拟合的 $\\lambda_0$ 处的最大对数似然值。\n\n角度规定：所有三角函数均使用弧度作为角度单位。单位：$\\lambda_0$ 以尖峰/秒报告，对数似然为无量纲量。\n\n您的程序必须实现此逻辑，并将其应用于以下尖峰序列测试套件（时间在 $[0,1]$ 内，单位为秒）：\n\n- 测试用例A（一般情况）：$[0.1, 0.25, 0.5, 0.9]$。\n- 测试用例B（无尖峰）：$[]$。\n- 测试用例C（边界时间）：$[0.0, 0.5, 1.0]$。\n- 测试用例D（接近强度消失的最小值）：$[0.749999, 0.750001, 0.3]$。\n\n对于每个测试用例，计算两个输出：\n- 拟合的 $\\lambda_0$（单位：尖峰/秒）。\n- 相应的最大对数似然值（无量纲）。\n\n最终输出格式：您的程序应生成单行输出，包含一个逗号分隔的列表，用方括号括起，顺序为 $[\\hat{\\lambda}_0^{(A)}, \\log L_{\\max}^{(A)}, \\hat{\\lambda}_0^{(B)}, \\log L_{\\max}^{(B)}, \\hat{\\lambda}_0^{(C)}, \\log L_{\\max}^{(C)}, \\hat{\\lambda}_0^{(D)}, \\log L_{\\max}^{(D)}]$，其中每个数字四舍五入到六位小数。\n\n答案类型必须是实数。确保所有计算在科学上是合理的且数值稳定。您的推导和实现不得依赖于所述基本事实之外的未说明的捷径，并且必须遵守指定的单位和角度约定。", "solution": "我们从 $[0,1]$ 秒上的非齐次泊松过程的特征开始，其条件强度为 $\\lambda(t) = \\lambda_0 g(t)$，其中 $g(t) = 1 + \\sin(2\\pi t)$。该过程的基本定义指出：考虑将 $[0,1]$ 划分为 $M$ 个宽度为 $\\Delta t$ 的区间，在以时间 $t_k$ 为中心的第 $k$ 个区间中，出现一个尖峰的概率约为 $\\lambda(t_k)\\Delta t$，没有尖峰的概率约为 $1 - \\lambda(t_k)\\Delta t$，而出现两个或更多尖峰的概率在 $\\Delta t \\to 0$ 时可以忽略不计。利用独立增量，通过对这些类伯努利概率在所有区间上的乘积取极限，可以得到在确切时间 $\\{t_i\\}_{i=1}^N$ 观测到尖峰的似然。\n\n在连续时间极限下，对于在 $[0,1]$ 上观测到尖峰 $\\{t_i\\}_{i=1}^N$ 且强度为 $\\lambda(t)$ 的非齐次泊松过程，其似然的标准、经过充分检验的形式是\n$$\nL(\\lambda_0 \\mid \\{t_i\\}) = \\exp\\left(-\\int_0^1 \\lambda(t)\\,dt\\right) \\prod_{i=1}^N \\lambda(t_i).\n$$\n此表达式源于以下事实：总似然是在尖峰时刻出现尖峰的概率（与各 $\\lambda(t_i)$ 成正比）与在所有其他时间点不出现尖峰的概率的乘积。\n\n代入 $\\lambda(t)=\\lambda_0 g(t)$ 可得\n$$\nL(\\lambda_0 \\mid \\{t_i\\}) = \\exp\\left(-\\lambda_0 \\int_0^1 g(t)\\,dt\\right) \\prod_{i=1}^N \\lambda_0 g(t_i) = \\exp\\left(-\\lambda_0 \\int_0^1 g(t)\\,dt\\right) \\lambda_0^N \\prod_{i=1}^N g(t_i).\n$$\n取自然对数，得到对数似然\n$$\n\\log L(\\lambda_0 \\mid \\{t_i\\}) = -\\lambda_0 \\int_0^1 g(t)\\,dt + N \\log \\lambda_0 + \\sum_{i=1}^N \\log g(t_i),\n$$\n其中 $N$ 是观测到的尖峰数量。\n\n我们计算 $g(t)$ 在 $[0,1]$ 上的积分：\n$$\n\\int_0^1 g(t)\\,dt = \\int_0^1 \\left(1 + \\sin(2\\pi t)\\right)\\,dt = \\int_0^1 1\\,dt + \\int_0^1 \\sin(2\\pi t)\\,dt.\n$$\n第一项是 $1$。对于第二项，使用反导数：\n$$\n\\int \\sin(2\\pi t)\\,dt = -\\frac{\\cos(2\\pi t)}{2\\pi}.\n$$\n因此，\n$$\n\\int_0^1 \\sin(2\\pi t)\\,dt = -\\frac{\\cos(2\\pi)}{2\\pi} + \\frac{\\cos(0)}{2\\pi} = -\\frac{1}{2\\pi} + \\frac{1}{2\\pi} = 0.\n$$\n于是，\n$$\n\\int_0^1 g(t)\\,dt = 1.\n$$\n\n现在我们对 $\\lambda_0 \\ge 0$ 最大化对数似然。对于 $N \\ge 1$，对 $\\lambda_0$ 求导：\n$$\n\\frac{d}{d\\lambda_0} \\log L(\\lambda_0 \\mid \\{t_i\\}) = -\\int_0^1 g(t)\\,dt + \\frac{N}{\\lambda_0} = -1 + \\frac{N}{\\lambda_0}.\n$$\n令导数为零可得\n$$\n-1 + \\frac{N}{\\lambda_0} = 0 \\quad \\Rightarrow \\quad \\hat{\\lambda}_0 = N.\n$$\n由于当 $N>0$ 时，二阶导数 $-\\frac{N}{\\lambda_0^2} < 0$，因此这是一个最大值。对于边界情况 $N=0$，对数似然简化为\n$$\n\\log L(\\lambda_0 \\mid \\emptyset) = -\\lambda_0 \\int_0^1 g(t)\\,dt = -\\lambda_0,\n$$\n它在边界 $\\hat{\\lambda}_0 = 0$ 处取得最大值 $0$。\n\n将 $\\hat{\\lambda}_0$ 代回对数似然，得到最大对数似然值：\n- 当 $N \\ge 1$ 时：\n$$\n\\log L_{\\max} = \\log L(\\hat{\\lambda}_0 \\mid \\{t_i\\}) = -\\hat{\\lambda}_0 \\int_0^1 g(t)\\,dt + N \\log \\hat{\\lambda}_0 + \\sum_{i=1}^N \\log g(t_i) = -N + N \\log N + \\sum_{i=1}^N \\log g(t_i).\n$$\n- 当 $N = 0$ 时：\n$$\n\\log L_{\\max} = 0.\n$$\n\n算法实现步骤：\n1. 对于每个测试用例，设置 $g(t) = 1 + \\sin(2\\pi t)$ 并计算尖峰次数 $N$。\n2. 计算 $\\hat{\\lambda}_0 = N$（单位为尖峰/秒），因为 $\\int_0^1 g(t)\\,dt = 1$。\n3. 使用弧度计算 $\\sum_{i=1}^N \\log g(t_i)$。\n4. 使用上述公式计算最大对数似然。对于 $N=0$，返回 $\\hat{\\lambda}_0=0$ 和 $\\log L_{\\max}=0$。\n5. 将所有输出四舍五入到六位小数，并以规定的单行格式打印：$[\\hat{\\lambda}_0^{(A)}, \\log L_{\\max}^{(A)}, \\hat{\\lambda}_0^{(B)}, \\log L_{\\max}^{(B)}, \\hat{\\lambda}_0^{(C)}, \\log L_{\\max}^{(C)}, \\hat{\\lambda}_0^{(D)}, \\log L_{\\max}^{(D)}]$。\n\n此过程遵循了非齐次泊松过程的基本性质，并遵守了单位和角度的规定。数值稳定性的考虑包括通过避免未定义的项 $N \\log \\hat{\\lambda}_0$ 来处理 $N=0$ 的情况，并确保 $g(t_i) > 0$；在提供的测试套件中，接近最小值的情况仍严格为正，但可能产生非常大的负对数值，这是有效且符合预期的。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef g(t):\n    \"\"\"Shape function g(t) = 1 + sin(2*pi*t), with t in seconds and angles in radians.\"\"\"\n    return 1.0 + np.sin(2.0 * np.pi * t)\n\ndef integral_g_0_1():\n    \"\"\"Exact integral of g(t) over [0,1]: ∫_0^1 (1 + sin(2πt)) dt = 1.\"\"\"\n    # Compute analytically using antiderivative for clarity.\n    # Antiderivative of 1 is t; of sin(2πt) is -cos(2πt)/(2π).\n    a, b = 0.0, 1.0\n    antideriv = lambda x: x - np.cos(2.0 * np.pi * x) / (2.0 * np.pi)\n    return antideriv(b) - antideriv(a)\n\ndef mle_lambda0_and_loglike(spike_times):\n    \"\"\"\n    Compute MLE for lambda0 and maximum log-likelihood for the model:\n    lambda(t) = lambda0 * (1 + sin(2*pi*t)), on [0,1] seconds.\n    Returns (lambda0_hat, loglike_max).\n    \"\"\"\n    spike_times = np.array(spike_times, dtype=float)\n    N = spike_times.size\n    # Integral of g(t) over [0,1].\n    int_g = integral_g_0_1()  # should be exactly 1.0\n    # MLE for lambda0 under constraint lambda0 >= 0.\n    if N == 0:\n        lambda_hat = 0.0\n        loglike_max = 0.0\n    else:\n        lambda_hat = N / int_g  # here int_g = 1, so lambda_hat = N\n        # Sum of log g(t_i); ensure positivity (g >= 0 on [0,1], with zeros only at t=0.75).\n        gi = g(spike_times)\n        # Guard against any nonpositive value (should not occur in test suite except near-zero positive).\n        if np.any(gi <= 0):\n            # If any g(t_i) = 0, likelihood is zero -> log-likelihood is -inf.\n            # We represent this with a large negative value numerically.\n            return lambda_hat, float(\"-inf\")\n        sum_log_g = np.sum(np.log(gi))\n        loglike_max = -lambda_hat * int_g + N * np.log(lambda_hat) + sum_log_g\n    return lambda_hat, loglike_max\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case A (general case)\n        [0.1, 0.25, 0.5, 0.9],\n        # Test case B (no spikes)\n        [],\n        # Test case C (boundary times)\n        [0.0, 0.5, 1.0],\n        # Test case D (near a vanishing-intensity minimum)\n        [0.749999, 0.750001, 0.3],\n    ]\n\n    results = []\n    for spikes in test_cases:\n        lambda_hat, loglike_max = mle_lambda0_and_loglike(spikes)\n        # Round to six decimal places as specified.\n        results.append(f\"{lambda_hat:.6f}\")\n        # Handle -inf explicitly to maintain formatting.\n        if np.isneginf(loglike_max):\n            results.append(\"-inf\")\n        else:\n            results.append(f\"{loglike_max:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "5037459"}, {"introduction": "研究神经编码的最终目的通常是解码它们——即读出大脑正在表征什么。这最后一个练习将指导您构建一个贝叶斯群体解码器，它整合来自多个神经元的信息来估计一个刺激。通过将贝叶斯定理应用于调谐曲线模型和泊松脉冲统计，您将看到概率推断如何被用来从神经活动中提取有意义的信息。[@problem_id:5037476]", "problem": "你的任务是使用离散化网格，为一个一维刺激变量实现一个基于原则的贝叶斯群体解码器。目标是根据已知调谐曲线的神经元群体的观测脉冲计数来计算刺激的后验分布，然后报告每个测试用例的后验均值估计和不确定性。\n\n基本原理：\n- 假设时间窗口的持续时间为 $T$ 秒。\n- 对于给定的刺激 $s$，每个神经元 $i$ 产生的脉冲计数 $k_i$ 在神经元之间是独立的，并且每个计数被建模为一个泊松随机变量，其期望计数为 $\\lambda_i(s) = T \\, r_i(s)$，其中 $r_i(s)$ 是放电率（单位：脉冲/秒）。\n- 神经元 $i$ 的调谐曲线是高斯函数：$r_i(s) = r_{0,i} + r_{\\max,i} \\exp\\!\\left(-\\frac{1}{2}\\left(\\frac{s - \\mu_i}{\\sigma_i}\\right)^{2}\\right)$，其中 $r_{0,i} \\ge 0$ 是基线放电率，$r_{\\max,i} \\ge 0$ 是峰值振幅，$\\mu_i$ 是偏好刺激（单位：度），$\\sigma_i > 0$ 是调谐宽度（单位：度）。\n- 使用 Bayes 定理，先验可以是刺激网格上的均匀先验，也可以是高斯先验 $p(s) \\propto \\exp\\!\\left(-\\frac{1}{2}\\left(\\frac{s-\\mu_0}{\\tau}\\right)^2\\right)$，其先验均值为 $\\mu_0$，先验标准差为 $\\tau$。\n\n你的实现必须：\n1. 将刺激域离散化为一个网格 $s \\in \\{-90, -89, \\dots, 89, 90\\}$（单位：度）。\n2. 对每个测试用例，计算网格上的未归一化对数后验，\n   $$\\log p(s \\mid \\mathbf{k}) = \\log p(s) + \\sum_{i} \\left[k_i \\log \\lambda_i(s) - \\lambda_i(s)\\right],$$\n   其中 $\\lambda_i(s) = T \\, r_i(s)$ 且 $\\mathbf{k} = (k_1, \\dots, k_N)$。因子 $\\log(k_i!)$ 可以省略，因为它不依赖于 $s$。\n3. 对网格上的后验进行数值归一化，以获得一个有效的概率质量函数，\n   $$p(s \\mid \\mathbf{k}) = \\frac{\\exp(\\log p(s \\mid \\mathbf{k}) - c)}{\\sum_{s'} \\exp(\\log p(s' \\mid \\mathbf{k}) - c)},$$\n   其中 $c = \\max_s \\log p(s \\mid \\mathbf{k})$ 是一个稳定化常数。\n4. 计算后验均值，\n   $$\\mathbb{E}[s \\mid \\mathbf{k}] = \\sum_s s \\, p(s \\mid \\mathbf{k}),$$\n   和后验标准差，\n   $$\\sqrt{\\mathbb{V}[s \\mid \\mathbf{k}]} = \\sqrt{\\sum_s (s - \\mathbb{E}[s \\mid \\mathbf{k}])^2 \\, p(s \\mid \\mathbf{k})}.$$\n5. 以后验均值和后验标准差都以度为单位表示。本问题中所有角度的单位都是度。\n6. 将后验均值和后验标准差都四舍五入到 $4$ 位小数。\n\n测试套件：\n对于下面的每个测试用例，刺激网格为 $s \\in \\{-90,-89,\\dots,90\\}$（单位：度）。所需参数为时间窗口 $T$（单位：秒）、先验、观测到的脉冲计数 $\\mathbf{k}$，以及每个神经元 $i$ 的一组神经元参数 $(\\mu_i, \\sigma_i, r_{0,i}, r_{\\max,i})$。\n\n- 测试用例 1 (一般情况):\n  - $T = 0.2$。\n  - 先验：高斯分布，$\\mu_0 = 10$ 且 $\\tau = 30$。\n  - 观测计数 $\\mathbf{k} = [1, 2, 1, 3, 0]$。\n  - 神经元：\n    - $i=1$: $(\\mu_1, \\sigma_1, r_{0,1}, r_{\\max,1}) = (-40, 20, 2, 18)$。\n    - $i=2$: $(\\mu_2, \\sigma_2, r_{0,2}, r_{\\max,2}) = (-10, 20, 2, 18)$。\n    - $i=3$: $(\\mu_3, \\sigma_3, r_{0,3}, r_{\\max,3}) = (0, 20, 2, 18)$。\n    - $i=4$: $(\\mu_4, \\sigma_4, r_{0,4}, r_{\\max,4}) = (15, 20, 2, 18)$。\n    - $i=5$: $(\\mu_5, \\sigma_5, r_{0,5}, r_{\\max,5}) = (40, 20, 2, 18)$。\n\n- 测试用例 2 (零脉冲和均匀先验的边界情况):\n  - $T = 0.1$。\n  - 先验：网格上的均匀分布。\n  - 观测计数 $\\mathbf{k} = [0, 0, 0, 0]$。\n  - 神经元：\n    - $i=1$: $(\\mu_1, \\sigma_1, r_{0,1}, r_{\\max,1}) = (-30, 15, 1, 9)$。\n    - $i=2$: $(\\mu_2, \\sigma_2, r_{0,2}, r_{\\max,2}) = (0, 15, 1, 9)$。\n    - $i=3$: $(\\mu_3, \\sigma_3, r_{0,3}, r_{\\max,3}) = (30, 15, 1, 9)$。\n    - $i=4$: $(\\mu_4, \\sigma_4, r_{0,4}, r_{\\max,4}) = (60, 15, 1, 9)$。\n\n- 测试用例 3 (围绕偏好方向的高度信息性脉冲):\n  - $T = 0.2$。\n  - 先验：高斯分布，$\\mu_0 = 25$ 且 $\\tau = 20$。\n  - 观测计数 $\\mathbf{k} = [0, 1, 8, 12, 2, 1]$。\n  - 神经元：\n    - $i=1$: $(\\mu_1, \\sigma_1, r_{0,1}, r_{\\max,1}) = (-60, 12, 3, 27)$。\n    - $i=2$: $(\\mu_2, \\sigma_2, r_{0,2}, r_{\\max,2}) = (-30, 12, 3, 27)$。\n    - $i=3$: $(\\mu_3, \\sigma_3, r_{0,3}, r_{\\max,3}) = (0, 12, 3, 27)$。\n    - $i=4$: $(\\mu_4, \\sigma_4, r_{0,4}, r_{\\max,4}) = (30, 12, 3, 27)$。\n    - $i=5$: $(\\mu_5, \\sigma_5, r_{0,5}, r_{\\max,5}) = (60, 12, 3, 27)$。\n    - $i=6$: $(\\mu_6, \\sigma_6, r_{0,6}, r_{\\max,6}) = (75, 12, 3, 27)$。\n\n- 测试用例 4 (后验分布集中在网格边界附近):\n  - $T = 0.15$。\n  - 先验：高斯分布，$\\mu_0 = 85$ 且 $\\tau = 5$。\n  - 观测计数 $\\mathbf{k} = [0, 0, 5]$。\n  - 神经元：\n    - $i=1$: $(\\mu_1, \\sigma_1, r_{0,1}, r_{\\max,1}) = (80, 10, 2, 20)$。\n    - $i=2$: $(\\mu_2, \\sigma_2, r_{0,2}, r_{\\max,2}) = (85, 10, 2, 20)$。\n    - $i=3$: $(\\mu_3, \\sigma_3, r_{0,3}, r_{\\max,3}) = (90, 10, 2, 20)$。\n\n你的程序必须为每个测试用例计算后验均值 $\\mathbb{E}[s \\mid \\mathbf{k}]$（单位：度）和后验标准差 $\\sqrt{\\mathbb{V}[s \\mid \\mathbf{k}]}$（单位：度），两者均四舍五入到 $4$ 位小数。你的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表的结果，其中每个元素都是 $[\\text{mean}, \\text{std}]$ 形式的双元素列表。例如：$[[m_1, s_1],[m_2, s_2],[m_3, s_3],[m_4, s_4]]$，其中每个 $m_j$ 和 $s_j$ 都是四舍五入到 $4$ 位小数的浮点数。", "solution": "该问题要求实现一个贝叶斯群体解码器，用以根据 $N$ 个神经元群体的脉冲计数 $\\mathbf{k} = (k_1, \\dots, k_N)$ 来估计一维刺激 $s$。该解决方案在一个概率框架内制定，利用 Bayes 定理来计算给定观测到的神经活动下刺激的后验分布。\n\n贝叶斯方法的核心是后验概率分布，由 Bayes 定理给出：\n$$\np(s \\mid \\mathbf{k}) \\propto p(\\mathbf{k} \\mid s) \\, p(s)\n$$\n其中 $p(\\mathbf{k} \\mid s)$ 是在给定刺激 $s$ 的情况下观测到脉冲计数 $\\mathbf{k}$ 的似然，而 $p(s)$ 是刺激的先验分布。通过将刺激空间离散化并在网格上计算这个后验分布来解决该问题。\n\n问题陈述，刺激变量 $s$ 在从 $-90$ 到 $90$ 度的整数值网格上进行离散化。所有计算都在此网格上执行。\n\n**1. 似然模型：$p(\\mathbf{k} \\mid s)$**\n\n每个神经元的响应被建模为一个独立的泊松过程。在持续时间为 $T$ 的时间窗口内，神经元 $i$ 的脉冲计数 $k_i$ 是一个从泊松分布中抽取的随机变量，其均值 $\\lambda_i(s)$ 依赖于刺激 $s$：\n$$\nk_i \\sim \\text{Poisson}(\\lambda_i(s))\n$$\n因此，观测到特定计数 $k_i$ 的概率是：\n$$\np(k_i \\mid s) = \\frac{\\lambda_i(s)^{k_i} e^{-\\lambda_i(s)}}{k_i!}\n$$\n平均脉冲计数 $\\lambda_i(s)$ 是时间持续 $T$ 与神经元放电率 $r_i(s)$ 的乘积：\n$$\n\\lambda_i(s) = T \\cdot r_i(s)\n$$\n放电率 $r_i(s)$ 由高斯调谐曲线描述，该曲线对神经元对刺激的响应特异性进行建模：\n$$\nr_i(s) = r_{0,i} + r_{\\max,i} \\exp\\!\\left(-\\frac{1}{2}\\left(\\frac{s - \\mu_i}{\\sigma_i}\\right)^{2}\\right)\n$$\n在此，$r_{0,i}$ 是基线放电率，$r_{\\max,i}$ 是高于基线的最大放电率，$\\mu_i$ 是神经元的偏好刺激，$\\sigma_i$ 是调谐宽度。\n\n由于假设给定刺激后神经元是独立的，因此群体的总似然是各个似然的乘积：\n$$\np(\\mathbf{k} \\mid s) = \\prod_{i=1}^{N} p(k_i \\mid s) = \\prod_{i=1}^{N} \\frac{\\lambda_i(s)^{k_i} e^{-\\lambda_i(s)}}{k_i!}\n$$\n为了计算稳定性和方便性，我们使用对数似然：\n$$\n\\log p(\\mathbf{k} \\mid s) = \\sum_{i=1}^{N} \\left[ k_i \\log \\lambda_i(s) - \\lambda_i(s) - \\log(k_i!) \\right]\n$$\n项 $\\log(k_i!)$ 相对于刺激 $s$ 是一个常数，在计算后验时可以丢弃，因为它将被吸收到归一化常数中。\n\n**2. 先验模型：$p(s)$**\n\n先验分布 $p(s)$ 代表我们在观测任何数据之前对刺激值的初始信念。考虑了两种类型的先验：\n- **均匀先验**：此先验假设网格上所有刺激值都是等可能的。对数先验 $\\log p(s)$ 是一个常数，为简单起见可以设为 $0$，因为任何加性常数都将由最终的归一化处理。\n- **高斯先验**：此先验假设刺激可能接近某个值 $\\mu_0$，标准差为 $\\tau$。未归一化的先验为 $p(s) \\propto \\exp\\left(-\\frac{1}{2}\\left(\\frac{s-\\mu_0}{\\tau}\\right)^2\\right)$。因此，对数先验为：\n$$\n\\log p(s) = -\\frac{1}{2}\\left(\\frac{s-\\mu_0}{\\tau}\\right)^2 + C\n$$\n其中常数 $C$ 可以忽略。\n\n**3. 后验分布：$p(s \\mid \\mathbf{k})$**\n\n对数后验是对数似然和对数先验之和，相差一个归一化常数：\n$$\n\\log p(s \\mid \\mathbf{k}) \\approx \\log p(s) + \\log p(\\mathbf{k} \\mid s) = \\log p(s) + \\sum_{i=1}^{N} \\left[ k_i \\log \\lambda_i(s) - \\lambda_i(s) \\right]\n$$\n这个未归一化的对数后验是为离散网格上的每个刺激值 $s_j$ 计算的。为了获得一个有效的概率质量函数，我们必须对其进行归一化。一种数值上稳定的方法是使用 log-sum-exp 技巧。首先，我们找到对数后验的最大值 $c = \\max_{s_j} \\log p(s_j \\mid \\mathbf{k})$。然后，每个网格点 $s_j$ 的后验概率为：\n$$\np(s_j \\mid \\mathbf{k}) = \\frac{\\exp(\\log p(s_j \\mid \\mathbf{k}) - c)}{\\sum_{s'_l} \\exp(\\log p(s'_l \\mid \\mathbf{k}) - c)}\n$$\n\n**4. 估计和不确定性**\n\n从归一化的后验分布中，我们可以计算出刺激的估计值及其相关的不确定性。\n- **后验均值**：在平方误差损失函数下，刺激的最优估计是后验均值。它被计算为 $s$ 在后验分布下的期望值：\n$$\n\\hat{s} = \\mathbb{E}[s \\mid \\mathbf{k}] = \\sum_{j} s_j \\, p(s_j \\mid \\mathbf{k})\n$$\n- **后验标准差**：估计的不确定性由后验标准差量化，它是后验方差的平方根：\n$$\n\\sqrt{\\mathbb{V}[s \\mid \\mathbf{k}]} = \\sqrt{\\sum_{j} (s_j - \\hat{s})^2 \\, p(s_j \\mid \\mathbf{k})}\n$$\n\n实现将对每个测试用例遵循这些步骤，计算后验均值和标准差，并将它们四舍五入到指定的精度。", "answer": "```python\nimport numpy as np\n\ndef run_bayesian_decoder(T, prior_params, k, neurons):\n    \"\"\"\n    Computes the posterior mean and standard deviation for a stimulus s.\n\n    Args:\n        T (float): Time window duration in seconds.\n        prior_params (dict): Dictionary specifying the prior ('uniform' or 'gaussian' with mu0, tau).\n        k (np.ndarray): Array of observed spike counts for each neuron.\n        neurons (np.ndarray): Array of neuron parameters (mu, sigma, r0, r_max).\n\n    Returns:\n        tuple[float, float]: The posterior mean and a posteriori standard deviation.\n    \"\"\"\n    # 1. Discretize the stimulus domain\n    s_grid = np.arange(-90, 91, 1).astype(float)\n\n    # 2. Compute the log-prior probability log p(s)\n    if prior_params[\"type\"] == \"uniform\":\n        log_prior = np.zeros_like(s_grid)\n    elif prior_params[\"type\"] == \"gaussian\":\n        mu0 = prior_params[\"mu0\"]\n        tau = prior_params[\"tau\"]\n        log_prior = -0.5 * ((s_grid - mu0) / tau)**2\n    else:\n        raise ValueError(\"Invalid prior type specified\")\n\n    # 3. Compute the log-likelihood log p(k|s)\n    # Unpack neuron parameters\n    mu = neurons[:, 0]\n    sigma = neurons[:, 1]\n    r0 = neurons[:, 2]\n    r_max = neurons[:, 3]\n\n    # Use broadcasting for efficient computation across the stimulus grid\n    # s_grid shape: (181,) -> s_grid_col shape: (181, 1)\n    # neuron params mu, sigma, etc. shape: (N,) -> (1, N)\n    s_grid_col = s_grid[:, np.newaxis]\n    \n    # Firing rates r_i(s) for all neurons i and stimuli s\n    # rates shape: (181, N)\n    exponent = -0.5 * ((s_grid_col - mu) / sigma)**2\n    rates = r0 + r_max * np.exp(exponent)\n\n    # Expected spike counts lambda_i(s)\n    # lambdas shape: (181, N)\n    lambdas = T * rates\n    \n    # Total log-likelihood for each stimulus s, summed over neurons\n    # The term k_i * log(lambda_i(s)) is calculated safely.\n    # If lambda_i(s) is close to 0, log(lambda) is a large negative number.\n    # The problem parameters ensure r_i(s) > 0, so lambda_i(s) > 0, avoiding log(0).\n    log_likelihood_matrix = k * np.log(lambdas) - lambdas\n    total_log_likelihood = np.sum(log_likelihood_matrix, axis=1)\n\n    # 4. Compute the unnormalized log-posterior\n    log_posterior = log_prior + total_log_likelihood\n\n    # 5. Numerically stabilize and normalize the posterior\n    # Use the log-sum-exp trick for numerical stability\n    c = np.max(log_posterior)\n    log_posterior_stable = log_posterior - c\n    unnormalized_posterior = np.exp(log_posterior_stable)\n    normalization_constant = np.sum(unnormalized_posterior)\n    posterior = unnormalized_posterior / normalization_constant\n\n    # 6. Compute posterior mean and standard deviation\n    posterior_mean = np.sum(s_grid * posterior)\n    posterior_variance = np.sum(((s_grid - posterior_mean)**2) * posterior)\n    posterior_std_dev = np.sqrt(posterior_variance)\n\n    return posterior_mean, posterior_std_dev\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the Bayesian population decoder.\n    \"\"\"\n    test_cases = [\n        {\n            \"T\": 0.2,\n            \"prior\": {\"type\": \"gaussian\", \"mu0\": 10, \"tau\": 30},\n            \"k\": np.array([1, 2, 1, 3, 0]),\n            \"neurons\": np.array([\n                [-40, 20, 2, 18], [-10, 20, 2, 18], [0, 20, 2, 18],\n                [15, 20, 2, 18], [40, 20, 2, 18]\n            ])\n        },\n        {\n            \"T\": 0.1,\n            \"prior\": {\"type\": \"uniform\"},\n            \"k\": np.array([0, 0, 0, 0]),\n            \"neurons\": np.array([\n                [-30, 15, 1, 9], [0, 15, 1, 9], [30, 15, 1, 9], [60, 15, 1, 9]\n            ])\n        },\n        {\n            \"T\": 0.2,\n            \"prior\": {\"type\": \"gaussian\", \"mu0\": 25, \"tau\": 20},\n            \"k\": np.array([0, 1, 8, 12, 2, 1]),\n            \"neurons\": np.array([\n                [-60, 12, 3, 27], [-30, 12, 3, 27], [0, 12, 3, 27],\n                [30, 12, 3, 27], [60, 12, 3, 27], [75, 12, 3, 27]\n            ])\n        },\n        {\n            \"T\": 0.15,\n            \"prior\": {\"type\": \"gaussian\", \"mu0\": 85, \"tau\": 5},\n            \"k\": np.array([0, 0, 5]),\n            \"neurons\": np.array([\n                [80, 10, 2, 20], [85, 10, 2, 20], [90, 10, 2, 20]\n            ])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        mean, std = run_bayesian_decoder(case[\"T\"], case[\"prior\"], case[\"k\"], case[\"neurons\"])\n        # Round to 4 decimal places as required\n        rounded_mean = round(mean, 4)\n        rounded_std = round(std, 4)\n        results.append((rounded_mean, rounded_std))\n\n    # Format the output string exactly as specified: [[m1,s1],[m2,s2],...]\n    formatted_results = [f\"[{m},{s}]\" for m, s in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "5037476"}]}