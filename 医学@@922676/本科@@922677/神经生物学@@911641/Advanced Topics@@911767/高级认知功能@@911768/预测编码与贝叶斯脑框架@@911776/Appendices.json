{"hands_on_practices": [{"introduction": "贝叶斯大脑的一个核心思想是，我们的感知是对先验知识和感觉证据的结合。但是，大脑如何权衡这两者呢？这个练习将通过一个简化的单神经元模型来探讨这个问题。你将推导出在一个线性高斯模型中，后验信念是如何由先验和似然的精度（即确定性）加权的平均值决定的，并进一步量化编码效率和对噪声的鲁棒性之间的基本权衡 [@problem_id:5052097]。", "problem": "考虑一个在贝叶斯大脑（BB）框架下的单神经元预测编码（PC）模型。潜藏的感官原因 $x$ 由高斯先验 $x \\sim \\mathcal{N}(\\mu_0, \\sigma_p^2)$ 建模，测量值 $y$ 由线性高斯似然 $y = x + \\epsilon$（其中 $\\epsilon \\sim \\mathcal{N}(0, \\sigma_l^2)$）生成。定义先验精度为 $\\Pi_p = 1/\\sigma_p^2$，似然精度为 $\\Pi_l = 1/\\sigma_l^2$，两者均严格为正。在预测编码中，神经元通过最小化精度加权的预测误差来估计 $x$。\n\n您的任务：\n\n1. 从贝叶斯定理和高斯恒等式出发，为指定的线性高斯模型推导后验分布 $p(x \\mid y)$。然后，从预测编码（PC）最小化精度加权预测误差的原理出发，推导在稳态下的不动点估计 $\\hat{x}$，并用 $\\Pi_p$、$\\Pi_l$、$\\mu_0$ 和 $y$ 表示。\n\n2. 将编码效率定义为 $x$ 和 $y$ 之间的互信息（MI），单位为奈特（nats）。推导 $\\mathrm{MI}(x; y)$ 的表达式，该表达式仅由 $\\Pi_p$ 和 $\\Pi_l$ 表示，不使用任何无关常数，且不假设 $\\mu_0$ 有任何特殊值。\n\n3. 将对感官噪声的鲁棒性定义为估计量 $\\hat{x}$ 对加性测量噪声的敏感度，该敏感度量化为将 $y = x + \\epsilon$ 代入 $\\hat{x}$ 后 $\\epsilon$ 的系数。推导此敏感度 $S$，仅用 $\\Pi_p$ 和 $\\Pi_l$ 表示。\n\n4. 实现一个完整的、可运行的程序，该程序为下面列出的每个测试用例计算两个量：以奈特为单位的编码效率和鲁棒性 $S$。每个计算量必须四舍五入到 $6$ 位小数。最终输出必须是单行，包含一个逗号分隔的列表的列表，其中每个内部列表的形式为 $[\\mathrm{MI}, S]$，并严格按照指定的顺序排列。\n\n使用以下的精度对 $(\\Pi_p, \\Pi_l)$ 测试套件，该套件的选择旨在探测平衡情况、强先验、强似然以及在保持严格正性的同时接近退化的边缘情况：\n\n- 情况 A（平衡）：$(\\Pi_p, \\Pi_l) = (1, 1)$。\n- 情况 B（强先验）：$(\\Pi_p, \\Pi_l) = (100, 1)$。\n- 情况 C（强似然）：$(\\Pi_p, \\Pi_l) = (1, 100)$。\n- 情况 D（弱先验）：$(\\Pi_p, \\Pi_l) = (10^{-6}, 1)$。\n- 情况 E（弱似然）：$(\\Pi_p, \\Pi_l) = (1, 10^{-6})$。\n\n所有量均为无量纲。您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表，内部列表的顺序与上述测试套件的顺序一致（例如，$[[\\mathrm{MI}_A,S_A],[\\mathrm{MI}_B,S_B],\\dots]$）。", "solution": "该问题具有科学依据，提法明确且客观。它展示了计算神经科学中的一个典型问题，提供了推导唯一且有意义的解决方案所需的所有必要定义和约束。这些任务要求应用贝叶斯统计、信息论和优化中的基本原理，这些原理是研究预测编码和贝叶斯大脑的核心。\n\n**1. 后验分布与预测编码不动点估计的推导**\n\n首先，我们使用贝叶斯定理推导后验分布 $p(x \\mid y)$。该定理指出，后验概率正比于似然与先验的乘积：$p(x \\mid y) \\propto p(y \\mid x)p(x)$。\n\n潜藏原因 $x$ 的先验分布为高斯分布：\n$$p(x) = \\mathcal{N}(x; \\mu_0, \\sigma_p^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_p^2}} \\exp\\left(-\\frac{(x - \\mu_0)^2}{2\\sigma_p^2}\\right)$$\n给定 $x$ 时测量值 $y$ 的似然也是高斯分布，由模型 $y = x + \\epsilon$（其中 $\\epsilon \\sim \\mathcal{N}(0, \\sigma_l^2)$）导出：\n$$p(y \\mid x) = \\mathcal{N}(y; x, \\sigma_l^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_l^2}} \\exp\\left(-\\frac{(y - x)^2}{2\\sigma_l^2}\\right)$$\n因此，后验概率正比于这两个高斯函数的乘积。由于两个高斯函数的乘积是另一个高斯函数，我们可以通过关注结果分布的指数项来找到其参数，忽略常数归一化因子：\n$$\\log p(x \\mid y) \\propto -\\frac{(x - \\mu_0)^2}{2\\sigma_p^2} - \\frac{(y - x)^2}{2\\sigma_l^2}$$\n使用精度定义 $\\Pi_p = 1/\\sigma_p^2$ 和 $\\Pi_l = 1/\\sigma_l^2$，表达式变为：\n$$\\log p(x \\mid y) \\propto -\\frac{1}{2}\\left(\\Pi_p(x - \\mu_0)^2 + \\Pi_l(y - x)^2\\right)$$\n为了找到后验高斯分布的均值和方差，我们展开二次项并对 $x$ 进行配方：\n$$-\\frac{1}{2}\\left(\\Pi_p(x^2 - 2x\\mu_0 + \\mu_0^2) + \\Pi_l(y^2 - 2yx + x^2)\\right)$$\n$$-\\frac{1}{2}\\left( (\\Pi_p + \\Pi_l)x^2 - 2(\\Pi_p\\mu_0 + \\Pi_ly)x + \\text{常数项} \\right)$$\n这个关于 $x$ 的二次型对应于一个高斯分布 $\\mathcal{N}(x; \\mu_{post}, \\sigma_{post}^2)$，其对数概率形式为 $-\\frac{(x-\\mu_{post})^2}{2\\sigma_{post}^2} = -\\frac{1}{2\\sigma_{post}^2}(x^2 - 2x\\mu_{post} + \\mu_{post}^2)$。\n通过比较 $x^2$ 项的系数，我们确定后验精度 $\\Pi_{post} = 1/\\sigma_{post}^2$：\n$$\\Pi_{post} = \\Pi_p + \\Pi_l$$\n通过比较 $x$ 项的系数，我们找到后验均值 $\\mu_{post}$：\n$$\\Pi_{post}\\mu_{post} = \\Pi_p\\mu_0 + \\Pi_ly \\implies \\mu_{post} = \\frac{\\Pi_p\\mu_0 + \\Pi_ly}{\\Pi_p + \\Pi_l}$$\n因此，后验分布为：\n$$p(x \\mid y) = \\mathcal{N}\\left(x; \\frac{\\Pi_p\\mu_0 + \\Pi_ly}{\\Pi_p + \\Pi_l}, \\frac{1}{\\Pi_p + \\Pi_l}\\right)$$\n\n接下来，我们在预测编码框架下推导不动点估计 $\\hat{x}$。预测编码（PC）假设大脑最小化一个代价函数，该函数对应于精度加权的预测误差平方和。对于本模型，误差是估计值 $\\hat{x}$ 与先验均值 $\\mu_0$ 的偏差，以及感官预测 $\\hat{x}$ 与实际感官输入 $y$ 的偏差。代价函数 $L(\\hat{x})$ 为：\n$$L(\\hat{x}) = \\frac{1}{2}\\Pi_p(\\hat{x} - \\mu_0)^2 + \\frac{1}{2}\\Pi_l(y - \\hat{x})^2$$\n为了找到最小化此代价的最优估计 $\\hat{x}$，我们将 $L(\\hat{x})$ 对 $\\hat{x}$ 求导并令其为零，这定义了系统动力学的不动点：\n$$\\frac{dL}{d\\hat{x}} = \\Pi_p(\\hat{x} - \\mu_0) - \\Pi_l(y - \\hat{x}) = 0$$\n$$\\Pi_p\\hat{x} - \\Pi_p\\mu_0 - \\Pi_ly + \\Pi_l\\hat{x} = 0$$\n$$\\hat{x}(\\Pi_p + \\Pi_l) = \\Pi_p\\mu_0 + \\Pi_ly$$\n$$\\hat{x} = \\frac{\\Pi_p\\mu_0 + \\Pi_ly}{\\Pi_p + \\Pi_l}$$\n这个不动点估计 $\\hat{x}$ 与后验均值 $\\mu_{post}$ 完全相同。这证明了在预测编码中最小化精度加权预测误差与对该线性高斯模型执行最优贝叶斯推断是等价的。估计值 $\\hat{x}$ 是先验均值和感官证据的精度加权平均。\n\n**2. 编码效率（互信息）的推导**\n\n编码效率定义为互信息 $\\mathrm{MI}(x; y)$。我们使用关系式 $\\mathrm{MI}(x; y) = H(x) - H(x \\mid y)$，其中 $H$ 表示微分熵。\n一维高斯变量 $z \\sim \\mathcal{N}(\\mu, \\sigma^2)$ 的熵为 $H(z) = \\frac{1}{2}\\log(2\\pi e \\sigma^2)$。\n先验分布 $p(x) \\sim \\mathcal{N}(\\mu_0, \\sigma_p^2)$ 的熵为：\n$$H(x) = \\frac{1}{2}\\log(2\\pi e \\sigma_p^2) = \\frac{1}{2}\\log\\left(\\frac{2\\pi e}{\\Pi_p}\\right)$$\n条件熵 $H(x \\mid y)$ 是后验分布 $p(x \\mid y)$ 的熵。根据我们之前的推导，后验方差为 $\\sigma_{post}^2 = 1/(\\Pi_p + \\Pi_l)$。因此，后验分布的熵为：\n$$H(x \\mid y) = \\frac{1}{2}\\log(2\\pi e \\sigma_{post}^2) = \\frac{1}{2}\\log\\left(\\frac{2\\pi e}{\\Pi_p + \\Pi_l}\\right)$$\n互信息是这两个熵的差值：\n$$\\mathrm{MI}(x; y) = H(x) - H(x \\mid y) = \\frac{1}{2}\\log\\left(\\frac{2\\pi e}{\\Pi_p}\\right) - \\frac{1}{2}\\log\\left(\\frac{2\\pi e}{\\Pi_p + \\Pi_l}\\right)$$\n使用对数性质 $\\log(a) - \\log(b) = \\log(a/b)$：\n$$\\mathrm{MI}(x; y) = \\frac{1}{2}\\log\\left(\\frac{2\\pi e / \\Pi_p}{2\\pi e / (\\Pi_p + \\Pi_l)}\\right) = \\frac{1}{2}\\log\\left(\\frac{\\Pi_p + \\Pi_l}{\\Pi_p}\\right)$$\n编码效率的最终表达式（单位为奈特），仅取决于精度，为：\n$$\\mathrm{MI}(x; y) = \\frac{1}{2}\\log\\left(1 + \\frac{\\Pi_l}{\\Pi_p}\\right)$$\n\n**3. 对感官噪声鲁棒性的推导**\n\n鲁棒性定义为估计量 $\\hat{x}$ 对加性测量噪声 $\\epsilon$ 的敏感度 $S$，其中 $y = x + \\epsilon$。我们通过将 $y$ 的表达式代入 $\\hat{x}$ 的方程来找到它：\n$$\\hat{x} = \\frac{\\Pi_p\\mu_0 + \\Pi_l y}{\\Pi_p + \\Pi_l} = \\frac{\\Pi_p\\mu_0 + \\Pi_l (x + \\epsilon)}{\\Pi_p + \\Pi_l}$$\n我们可以分离各项来隔离噪声项 $\\epsilon$ 的贡献：\n$$\\hat{x} = \\frac{\\Pi_p\\mu_0 + \\Pi_l x}{\\Pi_p + \\Pi_l} + \\left(\\frac{\\Pi_l}{\\Pi_p + \\Pi_l}\\right)\\epsilon$$\n敏感度 $S$ 是噪声项 $\\epsilon$ 的系数。因此：\n$$S = \\frac{\\Pi_l}{\\Pi_p + \\Pi_l}$$\n该值表示应用于新感官信息的增益。相对于先验精度 $\\Pi_p$，较高的似然精度 $\\Pi_l$ 会导致较高的敏感度 $S$，这意味着估计值受（可能含噪声的）测量影响很大。相反，一个强的先验（高 $\\Pi_p$）会降低 $S$，通过更多地依赖先验信念使估计对感官噪声更具鲁棒性。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes coding efficiency (MI) and robustness (S) for a single-neuron\n    Predictive Coding model based on a set of precision pairs.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each case is a tuple (Pi_p, Pi_l).\n    test_cases = [\n        (1.0, 1.0),            # Case A (balanced)\n        (100.0, 1.0),          # Case B (strong prior)\n        (1.0, 100.0),          # Case C (strong likelihood)\n        (1e-6, 1.0),           # Case D (weak prior)\n        (1.0, 1e-6),           # Case E (weak likelihood)\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        pi_p, pi_l = case\n        \n        # 1. Calculate Coding Efficiency (Mutual Information)\n        # MI = 0.5 * log(1 + Pi_l / Pi_p)\n        # Using np.log for the natural logarithm (for units of nats).\n        # This formula is numerically stable even for very small pi_p.\n        mutual_information = 0.5 * np.log(1 + pi_l / pi_p)\n        \n        # 2. Calculate Robustness (Sensitivity to noise)\n        # S = Pi_l / (Pi_p + Pi_l)\n        sensitivity = pi_l / (pi_p + pi_l)\n        \n        # 3. Format the results for the current case.\n        # The problem requires rounding to 6 decimal places and a specific\n        # string format for each inner list without extra spaces.\n        # e.g., \"[MI,S]\"\n        # Using f-string formatting '{:.6f}' ensures 6 decimal places.\n        formatted_result = f\"[{mutual_information:.6f},{sensitivity:.6f}]\"\n        results.append(formatted_result)\n\n    # Final print statement must produce a single line in the exact required format.\n    # The format is a comma-separated list of the formatted inner lists,\n    # all enclosed in a single pair of square brackets.\n    # e.g., [[MI_A,S_A],[MI_B,S_B],...]\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "5052097"}, {"introduction": "在理解了静态世界中信念是如何形成之后，我们如何将这一原理扩展到动态变化的环境中呢？这个练习将深入探讨预测编码的算法核心：卡尔曼滤波器。通过从第一性原理推导卡尔曼滤波器的预测和更新方程，你将亲眼见证“精度加权预测误差”这一关键计算基序是如何自然产生的，它被认为是皮层回路实现动态贝叶斯推断的基础 [@problem_id:5052165]。", "problem": "在预测编码和贝叶斯大脑框架中，一个核心假设是皮层回路通过在生成模型中执行近似贝叶斯推断，来编码关于感官输入的潜在原因的信念。考虑线性高斯生成模型\n$$x_t = A x_{t-1} + \\omega_t,\\quad \\omega_t \\sim \\mathcal{N}(0,Q),$$\n$$y_t = C x_t + \\eta_t,\\quad \\eta_t \\sim \\mathcal{N}(0,R),$$\n其初始先验为 $p(x_0)=\\mathcal{N}(\\mu_0,P_0)$。从贝叶斯法则和多元正态分布的性质出发，且不假设任何已有的滤波公式，请从第一性原理推导单步预测分布 $p(x_t\\mid y_{1:t-1})$ 和后验分布 $p(x_t\\mid y_{1:t})$ 的闭式解。您的推导应按以下步骤进行：(i) 通过对 $x_{t-1}$ 进行边缘化来形成预测先验；(ii) 通过将预测先验与似然相乘，并在指数项中进行配方，以提取后验均值和协方差，从而执行观测更新。根据预测编码中使用的“由观测精度对预测误差进行加权”的思想，解释观测更新。\n\n然后，特化到所有变量均为实数值的标量情况，并根据给定的参数计算在时间 $t=1$ 时的后验均值\n$$A=\\frac{4}{5},\\quad C=\\frac{6}{5},\\quad Q=\\frac{1}{2},\\quad R=\\frac{3}{10},\\quad \\mu_0=1,\\quad P_0=\\frac{3}{2},\\quad y_1=2.$$\n将您计算出的时间 $t=1$ 时的后验均值的最终数值结果表示为一个精确分数。不要包含任何单位。", "solution": "问题陈述是有效的。它提出了在状态空间模型和贝叶斯推断背景下的一个标准但重要的推导，这是诸如预测编码等现代计算神经科学理论的基石。该问题是自洽的，有科学依据且是良定的。我们开始求解。\n\n该问题要求推导线性高斯状态空间模型的递归贝叶斯更新，这是卡尔曼滤波器的数学基础。我们将按照要求，从第一性原理分两步进行推导：预测步骤和更新步骤。\n\n设时间 $t-1$ 的后验分布为 $p(x_{t-1}\\mid y_{1:t-1})=\\mathcal{N}(x_{t-1};\\mu_{t-1|t-1}, P_{t-1|t-1})$。对于 $t=1$ 的初始步骤，这对应于给定的先验 $p(x_0) = \\mathcal{N}(x_0;\\mu_0, P_0)$。\n\n**第1部分：预测步骤（推导预测先验）**\n\n第一步是计算单步预测分布 $p(x_t\\mid y_{1:t-1})$，也称为时间 $t$ 的预测先验。该分布通过对联合分布 $p(x_t, x_{t-1}\\mid y_{1:t-1})$ 关于前一状态 $x_{t-1}$ 进行边缘化得到：\n$$p(x_t\\mid y_{1:t-1}) = \\int p(x_t, x_{t-1}\\mid y_{1:t-1}) \\, dx_{t-1}$$\n使用概率的乘法法则，我们可以将联合分布写为：\n$$p(x_t, x_{t-1}\\mid y_{1:t-1}) = p(x_t\\mid x_{t-1}, y_{1:t-1})p(x_{t-1}\\mid y_{1:t-1})$$\n生成模型的状态方程 $x_t = A x_{t-1} + \\omega_t$ 意味着，在给定紧邻的前一个状态 $x_{t-1}$ 的条件下，$x_t$ 与过去的观测 $y_{1:t-1}$ 条件独立。这就是马尔可夫性质。因此，$p(x_t\\mid x_{t-1}, y_{1:t-1}) = p(x_t\\mid x_{t-1})$。从状态方程可知，这个分布是高斯的：\n$$p(x_t\\mid x_{t-1}) = \\mathcal{N}(x_t; A x_{t-1}, Q)$$\n我们正在对两个高斯分布进行卷积：$p(x_t\\mid x_{t-1})$ 和 $p(x_{t-1}\\mid y_{1:t-1})$。结果是另一个高斯分布。随机变量 $x_t$ 是高斯变量 $x_{t-1}$ 的线性变换加上一个独立的高斯噪声项 $\\omega_t$。$x_t$ 的均值和协方差（以 $y_{1:t-1}$ 为条件）可以使用全期望定律和全方差定律找到。\n\n预测分布的均值，我们记为 $\\mu_{t|t-1}$，是：\n$$\\mu_{t|t-1} = E[x_t\\mid y_{1:t-1}] = E[A x_{t-1} + \\omega_t \\mid y_{1:t-1}] = A E[x_{t-1}\\mid y_{1:t-1}] + E[\\omega_t] = A \\mu_{t-1|t-1} + 0$$\n所以，$\\mu_{t|t-1} = A \\mu_{t-1|t-1}$。\n\n预测分布的协方差，记为 $P_{t|t-1}$，是：\n$$P_{t|t-1} = \\text{Cov}(x_t\\mid y_{1:t-1}) = \\text{Cov}(A x_{t-1} + \\omega_t \\mid y_{1:t-1})$$\n由于 $x_{t-1}$（给定 $y_{1:t-1}$）和 $\\omega_t$ 是独立的，它们和的协方差是它们协方差的和：\n$$P_{t|t-1} = \\text{Cov}(A x_{t-1}\\mid y_{1:t-1}) + \\text{Cov}(\\omega_t) = A \\, \\text{Cov}(x_{t-1}\\mid y_{1:t-1}) \\, A^T + Q$$\n所以，$P_{t|t-1} = A P_{t-1|t-1} A^T + Q$。\n\n因此，单步预测分布是：\n$$p(x_t\\mid y_{1:t-1})_ = \\mathcal{N}(x_t; \\mu_{t|t-1}, P_{t|t-1})$$\n其均值为 $\\mu_{t|t-1} = A\\mu_{t-1|t-1}$，协方差为 $P_{t|t-1} = AP_{t-1|t-1}A^T+Q$。\n\n**第2部分：更新步骤（推导后验）**\n\n第二步是在观测到 $y_t$ 后更新我们关于 $x_t$ 的信念。我们使用贝叶斯法则来找到后验分布 $p(x_t\\mid y_{1:t}) = p(x_t\\mid y_t, y_{1:t-1})$：\n$$p(x_t\\mid y_{1:t}) \\propto p(y_t\\mid x_t, y_{1:t-1}) p(x_t\\mid y_{1:t-1})$$\n观测 $y_t$ 仅依赖于当前状态 $x_t$，所以 $p(y_t\\mid x_t, y_{1:t-1}) = p(y_t\\mid x_t)$。这是似然，由观测方程给出，为 $\\mathcal{N}(y_t; C x_t, R)$。先验是第1部分推导出的预测分布 $\\mathcal{N}(x_t; \\mu_{t|t-1}, P_{t|t-1})$。\n\n后验是两个高斯分布的乘积，它是一个未归一化的高斯分布。我们通过检查其对数来分析其形式：\n$$\\ln p(x_t\\mid y_{1:t}) = \\ln p(y_t\\mid x_t) + \\ln p(x_t\\mid y_{1:t-1}) + \\text{const.}$$\n$$= -\\frac{1}{2}(y_t - C x_t)^T R^{-1} (y_t - C x_t) - \\frac{1}{2}(x_t - \\mu_{t|t-1})^T P_{t|t-1}^{-1} (x_t - \\mu_{t|t-1}) + \\text{const.}$$\n我们展开二次型并合并包含 $x_t$ 的项，以确定后验的均值和协方差。一个一般的多元正态分布 $\\mathcal{N}(x; \\mu, P)$ 的指数项形式为 $-\\frac{1}{2}x^T P^{-1}x + x^T P^{-1}\\mu - \\frac{1}{2}\\mu^T P^{-1}\\mu$。\n\n展开指数项得到：\n$$\\text{exp-term} = -\\frac{1}{2}(x_t^T C^T R^{-1} C x_t - 2x_t^T C^T R^{-1} y_t) - \\frac{1}{2}(x_t^T P_{t|t-1}^{-1} x_t - 2x_t^T P_{t|t-1}^{-1} \\mu_{t|t-1}) + \\dots$$\n其中 `...` 包含不依赖于 $x_t$ 的项。\n\n$x_t$ 的二次项是 $-\\frac{1}{2}x_t^T(C^T R^{-1} C + P_{t|t-1}^{-1})x_t$。这告诉我们后验分布 $p(x_t \\mid y_{1:t})=\\mathcal{N}(x_t; \\mu_{t|t}, P_{t|t})$ 的逆协方差（精度）是：\n$$P_{t|t}^{-1} = P_{t|t-1}^{-1} + C^T R^{-1} C$$\n\n$x_t$ 的线性项是 $x_t^T(C^T R^{-1} y_t + P_{t|t-1}^{-1} \\mu_{t|t-1})$。将其与一般形式 $x_t^T P_{t|t}^{-1} \\mu_{t|t}$ 比较，我们发现：\n$$P_{t|t}^{-1} \\mu_{t|t} = P_{t|t-1}^{-1} \\mu_{t|t-1} + C^T R^{-1} y_t$$\n乘以 $P_{t|t}$ 得到后验均值：\n$$\\mu_{t|t} = P_{t|t} (P_{t|t-1}^{-1} \\mu_{t|t-1} + C^T R^{-1} y_t)$$\n为了得到预测编码中使用的形式，我们对这个表达式进行操作。从后验精度方程，我们有 $P_{t|t-1}^{-1} = P_{t|t}^{-1} - C^T R^{-1} C$。将其代入 $\\mu_{t|t}$ 的方程中：\n$$P_{t|t}^{-1} \\mu_{t|t} = (P_{t|t}^{-1} - C^T R^{-1} C) \\mu_{t|t-1} + C^T R^{-1} y_t$$\n$$P_{t|t}^{-1} \\mu_{t|t} = P_{t|t}^{-1} \\mu_{t|t-1} - C^T R^{-1} C \\mu_{t|t-1} + C^T R^{-1} y_t$$\n$$P_{t|t}^{-1} \\mu_{t|t} = P_{t|t}^{-1} \\mu_{t|t-1} + C^T R^{-1} (y_t - C \\mu_{t|t-1})$$\n从左侧乘以 $P_{t|t}$ 得到：\n$$\\mu_{t|t} = \\mu_{t|t-1} + P_{t|t} C^T R^{-1} (y_t - C \\mu_{t|t-1})$$\n将卡尔曼增益定义为 $K_t = P_{t|t} C^T R^{-1}$，我们得到最终的更新方程：\n$$p(x_t\\mid y_{1:t}) = \\mathcal{N}(x_t; \\mu_{t|t}, P_{t|t})$$\n$$\\mu_{t|t} = \\mu_{t|t-1} + K_t(y_t - C \\mu_{t|t-1})$$\n$$P_{t|t} = (P_{t|t-1}^{-1} + C^T R^{-1} C)^{-1}$$\n这就完成了从第一性原理的推导。\n\n**在预测编码中的解释**\n\n后验均值更新方程 $\\mu_{t|t} = \\mu_{t|t-1} + K_t(y_t - C \\mu_{t|t-1})$ 在预测编码的背景下有清晰的解释。\n1.  **预测：** $\\mu_{t|t-1}$ 是在观测到 $y_t$ 之前关于状态 $x_t$ 的先验信念或预测。项 $C\\mu_{t|t-1}$ 是基于此信念的预测感官输入。\n2.  **预测误差：** 项 $\\delta_t = y_t - C \\mu_{t|t-1}$ 是预测误差，即实际感官观测 $y_t$ 与预测观测 $C \\mu_{t|t-1}$ 之间的差异。\n3.  **精度加权：** 卡尔曼增益 $K_t$ 对这个预测误差进行加权。可以证明增益等价于 $K_t = P_{t|t-1} C^T (C P_{t|t-1} C^T + R)^{-1}$。它优化地平衡了先验信念的不确定性（编码在 $P_{t|t-1}$ 中）和感官数据的不确定性（编码在 $R$ 中）。高的观测噪声（大的 $R$）导致小的增益，意味着意外的观测被降低权重。高的先验不确定性（大的 $P_{t|t-1}$）导致大的增益，意味着新数据更被信任。\n因此，更新机制是根据预测误差来修正先验信念，其比例常数（增益）由先验和似然的相对精度（逆方差）决定。这种通过精度加权的预测误差进行信念更新的过程是预测编码框架的核心计算原理。\n\n**数值计算**\n\n我们被要求计算标量后验均值 $\\mu_{1|1}$，参数如下：\n$A=\\frac{4}{5}$, $C=\\frac{6}{5}$, $Q=\\frac{1}{2}$, $R=\\frac{3}{10}$, $\\mu_0=1$, $P_0=\\frac{3}{2}$, $y_1=2$.\n\n首先，我们从 $t=0$ 时的先验开始，为 $t=1$ 执行预测步骤：$\\mu_{0|0} = \\mu_0 = 1$ 且 $P_{0|0} = P_0 = \\frac{3}{2}$。\n\n预测均值 $\\mu_{1|0}$：\n$$\\mu_{1|0} = A \\mu_{0|0} = \\frac{4}{5} \\times 1 = \\frac{4}{5}$$\n\n预测方差 $P_{1|0}$：\n$$P_{1|0} = A^2 P_0 + Q = \\left(\\frac{4}{5}\\right)^2 \\left(\\frac{3}{2}\\right) + \\frac{1}{2} = \\frac{16}{25} \\times \\frac{3}{2} + \\frac{1}{2} = \\frac{24}{25} + \\frac{1}{2} = \\frac{48}{50} + \\frac{25}{50} = \\frac{73}{50}$$\n\n接下来，我们为 $t=1$ 执行更新步骤。我们需要计算卡尔曼增益 $K_1$ 和预测误差。\n\n标量情况下的卡尔曼增益 $K_1$ 是：\n$$K_1 = \\frac{P_{1|0} C}{C^2 P_{1|0} + R}$$\n分子：\n$$P_{1|0} C = \\frac{73}{50} \\times \\frac{6}{5} = \\frac{438}{250} = \\frac{219}{125}$$\n分母：\n$$C^2 P_{1|0} + R = \\left(\\frac{6}{5}\\right)^2 \\left(\\frac{73}{50}\\right) + \\frac{3}{10} = \\frac{36}{25} \\times \\frac{73}{50} + \\frac{3}{10} = \\frac{2628}{1250} + \\frac{3}{10} = \\frac{1314}{625} + \\frac{3}{10}$$\n$$= \\frac{2628}{1250} + \\frac{375}{1250} = \\frac{3003}{1250}$$\n所以，增益是：\n$$K_1 = \\frac{\\frac{219}{125}}{\\frac{3003}{1250}} = \\frac{219}{125} \\times \\frac{1250}{3003} = 219 \\times \\frac{10}{3003} = \\frac{2190}{3003}$$\n我们化简这个分数。$219 = 3 \\times 73$ 且 $3003 = 3 \\times 1001 = 3 \\times 7 \\times 11 \\times 13$。\n$$K_1 = \\frac{3 \\times 73 \\times 10}{3 \\times 1001} = \\frac{730}{1001}$$\n\n预测误差是：\n$$y_1 - C \\mu_{1|0} = 2 - \\left(\\frac{6}{5}\\right)\\left(\\frac{4}{5}\\right) = 2 - \\frac{24}{25} = \\frac{50}{25} - \\frac{24}{25} = \\frac{26}{25}$$\n\n最后，我们计算后验均值 $\\mu_{1|1}$：\n$$\\mu_{1|1} = \\mu_{1|0} + K_1 (y_1 - C \\mu_{1|0}) = \\frac{4}{5} + \\left(\\frac{730}{1001}\\right) \\left(\\frac{26}{25}\\right)$$\n我们化简乘积项：\n$$K_1 (y_1 - C \\mu_{1|0}) = \\frac{730}{1001} \\times \\frac{26}{25} = \\frac{73 \\times 10}{7 \\times 11 \\times 13} \\times \\frac{2 \\times 13}{25} = \\frac{73 \\times 2 \\times 5}{7 \\times 11 \\times 13} \\times \\frac{2 \\times 13}{5 \\times 5}$$\n$$= \\frac{73 \\times 2 \\times 2}{7 \\times 11 \\times 5} = \\frac{292}{385}$$\n现在我们将这个修正项加到先验均值上：\n$$\\mu_{1|1} = \\frac{4}{5} + \\frac{292}{385}$$\n公分母是 $385$。$385 = 5 \\times 77$。\n$$\\mu_{1|1} = \\frac{4 \\times 77}{5 \\times 77} + \\frac{292}{385} = \\frac{308}{385} + \\frac{292}{385} = \\frac{600}{385}$$\n通过将分子和分母除以它们的最大公约数 $5$ 来化简最终分数：\n$$\\mu_{1|1} = \\frac{600 \\div 5}{385 \\div 5} = \\frac{120}{77}$$\n数字 $120$ 和 $77$ 互质（$120=2^3 \\cdot 3 \\cdot 5$, $77=7 \\cdot 11$）。", "answer": "$$\\boxed{\\frac{120}{77}}$$", "id": "5052165"}, {"introduction": "理论模型非常强大，但它们如何与真实的实验数据联系起来？这个练习将引导你面对计算神经科学中的一个关键挑战：模型参数的可辨识性问题。你将通过模拟发现，仅靠行为数据可能无法唯一确定模型中先验和感觉精度的真实值，这揭示了模型拟合的模糊性。通过引入模拟的神经信号作为额外约束，此练习展示了结合行为和神经数据对于验证和精确化我们的大脑计算理论是何等重要 [@problem_id:5052166]。", "problem": "您将实现并分析一个在贝叶斯大脑框架下的简化预测编码模型，以评估参数的可辨识性。核心模型的假设和定义如下。感觉观察值是在加性高斯噪声下从一个潜在刺激生成的。精度被定义为方差的倒数。预测通过结合先验信念和似然进行更新，两者都是高斯分布且与潜在变量呈线性关系。在线性高斯假设下，后验均值可以表示为通过一个加权预测误差对先验均值的更新，而权重取决于精度。\n\n您的程序必须使用以下组件。\n\n- 数据生成。对于每个试验索引 $t \\in \\{1,\\dots,T\\}$，刺激是恒定的并设为 $0$，观察值为 $y_t = \\epsilon_t$，其中 $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma_s^2)$，$\\sigma_s^2 = 1/\\lambda_s^{\\mathrm{true}}$，而 $\\lambda_s^{\\mathrm{true}}$ 是真实的感觉精度。在时间 $t$，智能体的先验均值等于前一个后验均值 $ \\mu_{t-1} $，并具有一个固定的先验精度 $\\lambda_p^{\\mathrm{true}}$。行为估计是在标准线性高斯假设下，通过结合高斯先验和高斯似然得到的后验均值 $ \\mu_t $。初始化 $ \\mu_0 = 0 $。神经信号被建模为一个精度加权的预测误差：$ a_t^{\\mathrm{true}} = g \\, \\lambda_s^{\\mathrm{true}} \\, (y_t - \\mu_{t-1}) + \\eta_t $，其中 $ \\eta_t \\sim \\mathcal{N}(0, \\sigma_n^2) $。标量 $g$ 是一个已知的增益。\n\n- 建模与可辨识性。将参数对 $ (\\lambda_p, \\lambda_s) $ 视为自由模型参数。在仅行为拟合的情况下，许多不同的参数对 $ (\\lambda_p, \\lambda_s) $ 可以产生无法区分的行为，因为更新取决于先验精度和感觉精度的权衡。这可能导致不可辨识性。通过引入与感觉精度成比例的神经信号来增加一个约束，并重新评估可辨识性。\n\n- 从精度为 $ \\lambda_p $ 的先验和精度为 $ \\lambda_s $ 的似然的线性高斯组合中推导行为更新规则，将后验均值 $ \\mu_t $ 表示为前一个均值加上一个权重乘以预测误差 $ (y_t - \\mu_{t-1}) $。除了已声明的高斯定义外，不要假设任何特殊情况。\n\n- 网格评估。对于每个测试用例，评估 $ \\lambda_p $ 和 $ \\lambda_s $ 的参数网格 $ \\mathcal{L} = \\{ 0.5, 1.0, 2.0, 4.0, 8.0, 16.0 \\} $，从而产生 $36$ 个候选参数对 $ (\\lambda_p, \\lambda_s) $。对于每一对参数，使用推导出的更新规则和观察到的 $ \\{ y_t \\} $ 来模拟行为轨迹 $ \\{ \\mu_t \\}_{t=1}^T $，并计算模拟的 $ \\{ \\mu_t \\} $ 与基准行为轨迹之间的行为均方误差 $ \\mathrm{MSE}_{\\mathrm{bhv}} $。同时，使用模拟的 $ \\{ \\mu_t \\} $ 计算预测的神经信号 $ a_t = g \\, \\lambda_s \\, (y_t - \\mu_{t-1}) $，并计算其相对于 $ \\{ a_t^{\\mathrm{true}} \\} $ 的神经均方误差 $ \\mathrm{MSE}_{\\mathrm{neural}} $。将组合损失定义为 $ \\mathrm{MSE}_{\\mathrm{comb}} = \\mathrm{MSE}_{\\mathrm{bhv}} + \\mathrm{MSE}_{\\mathrm{neural}} $。\n\n- 可辨识性计数。对于仅行为可辨识性，计算行为均方误差在绝对容差 $ \\varepsilon_{\\mathrm{bhv}} = 10^{-12} $ 内等于最小行为均方误差的参数对数量。对于行为加神经的可辨识性，计算组合均方误差在最小组合均方误差的相对容差 $ \\theta = 0.05 $ 内的参数对数量，即其 $ \\mathrm{MSE}_{\\mathrm{comb}} \\le (1+\\theta)\\,\\mathrm{MSE}_{\\min,\\mathrm{comb}} $。报告计数和分数缩减 $ r = 1 - \\frac{\\text{count}_{\\mathrm{comb}}}{\\text{count}_{\\mathrm{bhv}}} $。\n\n- 随机性。为每个测试用例使用下述指定的独立随机种子，并对所有高斯噪声使用标准正态分布采样。对于每个测试用例，在生成数据前使用提供的种子重置随机数生成器。\n\n测试套件。使用以下测试用例，每个用例以元组 $ (T, \\lambda_p^{\\mathrm{true}}, \\lambda_s^{\\mathrm{true}}, g, \\sigma_n, \\text{seed}) $ 的形式给出：\n- 用例 1: $ (200, 1.0, 4.0, 1.5, 0.3, 0) $\n- 用例 2: $ (20, 1.0, 4.0, 1.5, 1.0, 1) $\n- 用例 3: $ (200, 2.0, 8.0, 1.5, 0.05, 2) $\n\n要求的输出格式。您的程序应生成单行输出，其中包含一个含三个项目的列表，每个测试用例一个项目，其中每个项目本身是一个列表 $[\\text{count}_{\\mathrm{bhv}}, \\text{count}_{\\mathrm{comb}}, r]$。该行必须在打印时不含空格，分数缩减 $r$ 四舍五入到三位小数。例如，一个有效的输出行看起来像 \"[[1,1,0.000],[4,1,0.750],[4,1,0.750]]\"。", "solution": "该问题要求实现并分析一个简化的预测编码模型，以评估参数的可辨识性。这涉及推导行为更新规则、生成基准数据、使用行为和神经误差指标评估模型参数网格与此数据的匹配程度，以及量化可辨识性的改善。\n\n首先，我们推导后验均值 $\\mu_t$ 的行为更新规则。贝叶斯大脑假说认为，大脑根据感觉证据（似然）更新其信念（先验）。在这个线性高斯模型中，在试验 $t$ 时关于潜在刺激 $\\mu$ 的先验信念是一个高斯分布，其均值等于前一个后验均值 $\\mu_{t-1}$，并具有固定的先验精度 $\\lambda_p$。其概率密度函数（PDF）为：\n$$ p(\\mu) = \\mathcal{N}(\\mu | \\mu_{t-1}, 1/\\lambda_p) $$\n感觉观察值 $y_t$ 产生了关于刺激 $\\mu$ 的似然函数。在加性高斯噪声方差为 $\\sigma_s^2 = 1/\\lambda_s$ 的假设下，似然函数为：\n$$ p(y_t | \\mu) = \\mathcal{N}(y_t | \\mu, 1/\\lambda_s) $$\n根据贝叶斯法则，后验分布与似然和先验的乘积成正比：\n$$ p(\\mu | y_t) \\propto p(y_t | \\mu) p(\\mu) $$\n对于两个高斯分布的乘积，得到的后验分布也是一个高斯分布，$p(\\mu | y_t) = \\mathcal{N}(\\mu | \\mu_t, 1/\\lambda_{\\text{post}})$。其精度 $\\lambda_{\\text{post}}$ 是先验精度和似然精度的和：\n$$ \\lambda_{\\text{post}} = \\lambda_p + \\lambda_s $$\n后验均值 $\\mu_t$ 作为行为估计，是先验均值 $\\mu_{t-1}$ 和数据 $y_t$ 的精度加权平均值：\n$$ \\mu_t = \\frac{\\lambda_p \\mu_{t-1} + \\lambda_s y_t}{\\lambda_p + \\lambda_s} $$\n为了将其表示为预测误差更新的形式，我们进行代数变换：\n$$ \\mu_t = \\frac{\\lambda_p \\mu_{t-1} + \\lambda_s \\mu_{t-1} - \\lambda_s \\mu_{t-1} + \\lambda_s y_t}{\\lambda_p + \\lambda_s} $$\n$$ \\mu_t = \\frac{(\\lambda_p + \\lambda_s)\\mu_{t-1} + \\lambda_s(y_t - \\mu_{t-1})}{\\lambda_p + \\lambda_s} $$\n$$ \\mu_t = \\mu_{t-1} + \\frac{\\lambda_s}{\\lambda_p + \\lambda_s} (y_t - \\mu_{t-1}) $$\n这就是所要求的行为更新规则。项 $w = \\frac{\\lambda_s}{\\lambda_p + \\lambda_s}$ 充当学习率，对预测误差 $(y_t - \\mu_{t-1})$ 进行加权。\n\n每个测试用例的总体流程如下：\n1.  **数据生成**：使用真实参数 $(\\lambda_p^{\\mathrm{true}}, \\lambda_s^{\\mathrm{true}})$，我们首先生成感觉观察值的时间序列 $\\{y_t\\}_{t=1}^T$。由于真实刺激为 $0$，每个观察值都从一个高斯分布中抽取：$y_t \\sim \\mathcal{N}(0, 1/\\lambda_s^{\\mathrm{true}})$。随后，通过从 $\\mu_0=0$ 开始迭代应用推导出的更新规则，生成基准行为轨迹 $\\{\\mu_t^{\\mathrm{true}}\\}_{t=1}^T$。最后，使用其定义生成基准神经信号 $\\{a_t^{\\mathrm{true}}\\}_{t=1}^T$：$a_t^{\\mathrm{true}} = g \\, \\lambda_s^{\\mathrm{true}} \\, (y_t - \\mu_{t-1}^{\\mathrm{true}}) + \\eta_t$，其中 $\\eta_t \\sim \\mathcal{N}(0, \\sigma_n^2)$。\n\n2.  **网格评估**：我们遍历指定网格 $\\mathcal{L} \\times \\mathcal{L}$ 中的 $36$ 个候选参数对 $(\\lambda_p, \\lambda_s)$ 中的每一个。对于每一对参数：\n    *   使用相同的观察值 $\\{y_t\\}$ 但采用候选学习率 $w_{\\mathrm{sim}} = \\lambda_s / (\\lambda_p + \\lambda_s)$，生成一个模拟的行为轨迹 $\\{\\mu_t^{\\mathrm{sim}}\\}_{t=1}^T$。\n    *   计算行为均方误差 $\\mathrm{MSE}_{\\mathrm{bhv}} = \\frac{1}{T}\\sum_{t=1}^T (\\mu_t^{\\mathrm{sim}} - \\mu_t^{\\mathrm{true}})^2$。\n    *   计算预测的神经信号 $\\{a_t^{\\mathrm{sim}}\\}_{t=1}^T$ 为 $a_t^{\\mathrm{sim}} = g \\, \\lambda_s \\, (y_t - \\mu_{t-1}^{\\mathrm{sim}})$。\n    *   计算神经均方误差 $\\mathrm{MSE}_{\\mathrm{neural}} = \\frac{1}{T}\\sum_{t=1}^T (a_t^{\\mathrm{sim}} - a_t^{\\mathrm{true}})^2$。\n    *   组合损失计算为 $\\mathrm{MSE}_{\\mathrm{comb}} = \\mathrm{MSE}_{\\mathrm{bhv}} + \\mathrm{MSE}_{\\mathrm{neural}}$。\n\n3.  **可辨识性分析**：\n    *   **仅行为**：学习率 $w$ 仅取决于比率 $\\lambda_s/\\lambda_p$。多个参数对 $(\\lambda_p, \\lambda_s)$ 可以产生相同的比率，从而得到相同的学习率，导致完全相同的行为轨迹。如果一个候选参数对的比率与真实参数的比率相同，其 $\\mathrm{MSE}_{\\mathrm{bhv}}$ 将为零（在机器精度范围内）。我们计算在整个网格中，其 $\\mathrm{MSE}_{\\mathrm{bhv}}$ 在最小观测 $\\mathrm{MSE}_{\\mathrm{bhv}}$ 的容差 $\\varepsilon_{\\mathrm{bhv}}=10^{-12}$ 范围内的参数对数量。这就得到了 $\\text{count}_{\\mathrm{bhv}}$。\n    *   **行为加神经**：预测的神经信号 $a_t^{\\mathrm{sim}}$ 与候选的 $\\lambda_s$ 直接成比例。这打破了简并性。即使在具有相同学习率（因此 $\\mathrm{MSE}_{\\mathrm{bhv}}$ 最小）的参数对中，其 $\\lambda_s$ 最接近 $\\lambda_s^{\\mathrm{true}}$ 的那一对将具有最小的 $\\mathrm{MSE}_{\\mathrm{neural}}$，并因此具有最小的 $\\mathrm{MSE}_{\\mathrm{comb}}$。我们计算在网格上找到的其 $\\mathrm{MSE}_{\\mathrm{comb}}$ 在最小组合误差的相对容差 $\\theta=0.05$ 内的参数对数量，即 $\\mathrm{MSE}_{\\mathrm{comb}} \\le (1+\\theta)\\mathrm{MSE}_{\\min,\\mathrm{comb}}$。这就得到了 $\\text{count}_{\\mathrm{comb}}$。\n    *   最后，模糊性的分数缩减计算为 $r = 1 - \\frac{\\text{count}_{\\mathrm{comb}}}{\\text{count}_{\\mathrm{bhv}}}$。\n\n这个完整的过程被封装在提供的 Python 程序中，该程序为每个指定的测试用例执行模拟，并按要求格式化结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_case(T, lambda_p_true, lambda_s_true, g, sigma_n, seed):\n    \"\"\"\n    Runs a single test case for the predictive coding model identifiability analysis.\n    \"\"\"\n    # Use a dedicated random number generator for this case to ensure independence\n    rng = np.random.default_rng(seed)\n\n    # 1. Generate ground-truth data\n    # Generate sensory observations y_t = epsilon_t, where epsilon_t ~ N(0, 1/lambda_s_true)\n    sigma_s_true = np.sqrt(1.0 / lambda_s_true)\n    y_obs = rng.normal(loc=0.0, scale=sigma_s_true, size=T)\n\n    # Generate ground-truth behavioral trajectory {mu_t^true}\n    mu_true = np.zeros(T + 1)  # Includes mu_0\n    mu_true[0] = 0.0\n    alpha_true = lambda_s_true / (lambda_p_true + lambda_s_true)\n    for t in range(T):\n        # Update rule: mu_{t} = mu_{t-1} + alpha * (y_t - mu_{t-1})\n        # Array indices: mu_true[t+1] is mu_{t}, mu_true[t] is mu_{t-1}, y_obs[t] is y_t\n        pred_error = y_obs[t] - mu_true[t]\n        mu_true[t+1] = mu_true[t] + alpha_true * pred_error\n\n    # Generate ground-truth neural signal {a_t^true}\n    neural_noise = rng.normal(loc=0.0, scale=sigma_n, size=T)\n    a_true = np.zeros(T)\n    for t in range(T):\n        # a_t^true = g * lambda_s^true * (y_t - mu_{t-1}^true) + eta_t\n        pred_error = y_obs[t] - mu_true[t]\n        a_true[t] = g * lambda_s_true * pred_error + neural_noise[t]\n\n    # Trajectories from t=1..T correspond to array slices [1:] for mu and [:] for others\n    mu_true_traj = mu_true[1:]\n\n    # 2. Grid evaluation\n    param_grid = [0.5, 1.0, 2.0, 4.0, 8.0, 16.0]\n    eval_results = []\n\n    for lambda_p in param_grid:\n        for lambda_s in param_grid:\n            # Simulate behavioral trajectory {mu_t^sim} for the candidate pair\n            mu_sim = np.zeros(T + 1)\n            mu_sim[0] = 0.0\n            alpha_sim = lambda_s / (lambda_p + lambda_s)\n            for t in range(T):\n                pred_error = y_obs[t] - mu_sim[t]\n                mu_sim[t+1] = mu_sim[t] + alpha_sim * pred_error\n            mu_sim_traj = mu_sim[1:]\n\n            # Compute behavioral MSE\n            mse_bhv = np.mean((mu_sim_traj - mu_true_traj)**2)\n\n            # Predict neural signal {a_t^sim}\n            a_sim = np.zeros(T)\n            for t in range(T):\n                pred_error = y_obs[t] - mu_sim[t]\n                a_sim[t] = g * lambda_s * pred_error\n\n            # Compute neural MSE and combined loss\n            mse_neural = np.mean((a_sim - a_true)**2)\n            mse_comb = mse_bhv + mse_neural\n\n            eval_results.append({'mse_bhv': mse_bhv, 'mse_comb': mse_comb})\n\n    # 3. Calculate identifiability counts\n    # Behavior-only count\n    all_mse_bhv = [res['mse_bhv'] for res in eval_results]\n    min_mse_bhv = min(all_mse_bhv)\n    epsilon_bhv = 1e-12\n    count_bhv = sum(1 for mse in all_mse_bhv if mse <= min_mse_bhv + epsilon_bhv)\n\n    # Combined count\n    all_mse_comb = [res['mse_comb'] for res in eval_results]\n    min_mse_comb = min(all_mse_comb)\n    theta = 0.05\n    count_comb = sum(1 for mse in all_mse_comb if mse <= (1.0 + theta) * min_mse_comb)\n\n    # 4. Calculate fractional reduction\n    if count_bhv > 0:\n        reduction = 1.0 - (count_comb / count_bhv)\n    else:\n        reduction = 0.0\n\n    return [count_bhv, count_comb, reduction]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        (200, 1.0, 4.0, 1.5, 0.3, 0),\n        (20, 1.0, 4.0, 1.5, 1.0, 1),\n        (200, 2.0, 8.0, 1.5, 0.05, 2),\n    ]\n\n    final_results = []\n    for case in test_cases:\n        params = case\n        count_bhv, count_comb, reduction = run_case(*params)\n        \n        # Format the reduction to three decimal places\n        formatted_reduction = f\"{reduction:.3f}\"\n        \n        final_results.append(f\"[{count_bhv},{count_comb},{formatted_reduction}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(final_results)}]\")\n\nsolve()\n```", "id": "5052166"}]}