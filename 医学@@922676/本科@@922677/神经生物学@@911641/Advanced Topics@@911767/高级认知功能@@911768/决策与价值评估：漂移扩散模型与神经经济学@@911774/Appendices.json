{"hands_on_practices": [{"introduction": "这项练习深入探讨了决策制定中最经典的现象之一：速度-准确率权衡。通过推导模型参数与选择概率之间的关系，您将定量地理解决策者如何通过调整决策阈值（$A$）来策略性地牺牲准确率以换取速度，反之亦然。这项练习对于掌握漂移扩散模型（DDM）如何捕捉证据积累的核心动态至关重要 [@problem_id:5011105]。", "problem": "双择一知觉决策中的一个经典经验现象是速度-准确率权衡：被试可以通过降低决策标准来提高决策速度，但会以牺牲准确率为代价。考虑一个神经上合理的证据累积过程，该过程被建模为漂移扩散模型（DDM）。在此模型中，一个标量决策变量 $x(t)$ 根据随机微分方程 $dx(t) = \\mu \\, dt + \\sigma \\, dW_t$ 演化，其中 $W_t$ 是一个标准维纳过程，$\\mu$ 是一个恒定的漂移率，反映了支持正确选项的平均瞬时证据，而 $\\sigma$ 是扩散振幅，反映了神经变异性。该过程从 $x(0) = 0$ 开始，当它到达上吸收边界 $x = +A$（在 $\\mu > 0$ 时做出正确选择）或下吸收边界 $x = -A$（做出错误选择）时终止。假设边界对称且起点无偏。\n\n从此漂移扩散过程的 Kolmogorov 反向算子和在 $\\pm A$ 处吸收的适当边界条件出发，推导从 $x(0) = 0$ 开始，击中上边界的概率表达式，该表达式应以 $\\mu$、$\\sigma$ 和 $A$ 表示。然后，施加一个固定准确率策略，目标准确率为 $p^{\\star} = 0.82$，并计算在 $\\mu = 0.75$（证据/秒）和 $\\sigma = 1.10$（证据/$\\text{秒}^{1/2}$）的条件下，实现这一准确率所需的边界大小 $A$。最终的 $A$ 以证据单位表示。将您的答案四舍五入到四位有效数字。", "solution": "问题要求做两件事：首先，推导在具有对称边界的漂移扩散模型（DDM）中做出正确决策的概率表达式；其次，对于给定的模型参数，计算达到特定准确率 $p^{\\star}$ 所需的边界高度 $A$。\n\n### 步骤 1：问题陈述的验证\n\n我们首先验证问题陈述。\n\n**1. 提取给定信息：**\n-   **模型：** 针对标量决策变量 $x(t)$ 的漂移扩散模型（DDM）。\n-   **随机微分方程：** $dx(t) = \\mu \\, dt + \\sigma \\, dW_t$。\n-   **参数：**\n    -   $\\mu$：恒定漂移率。\n    -   $\\sigma$：扩散振幅（维纳过程增量的标准差）。\n-   **过程：** $W_t$ 是一个标准维纳过程。\n-   **初始条件：** 过程从 $x(0) = 0$ 开始。\n-   **边界条件：** 两个吸收边界分别位于 $x = +A$ 和 $x = -A$。\n-   **决策映射：** 击中 $x = +A$ 对应于正确选择（对于 $\\mu > 0$），击中 $x = -A$ 对应于错误选择。\n-   **任务 1（推导）：** 从 $x(0) = 0$ 开始，推导击中上边界 $+A$ 的概率，用 $\\mu$、$\\sigma$ 和 $A$ 表示。推导必须从 Kolmogorov 反向算子开始。\n-   **任务 2（计算）：**\n    -   目标准确率：$p^{\\star} = 0.82$。\n    -   漂移率：$\\mu = 0.75$ 证据/秒。\n    -   扩散振幅：$\\sigma = 1.10$ 证据/$\\text{秒}^{1/2}$。\n    -   计算所需的边界大小 $A$。\n    -   $A$ 的最终答案应四舍五入到四位有效数字。\n\n**2. 验证：**\n-   **科学依据：** 该问题牢固地植根于计算神经科学和数学心理学。DDM 是决策制定的经典模型，通过 Kolmogorov 反向方程（Feynman-Kac 公式的一种特定形式）对其进行分析，是研究随机过程中首达时间问题的标准而严谨的数学技术。为 $\\mu$ 和 $\\sigma$ 提供的单位在量纲上是一致的。\n-   **适定性：** 该问题是适定的。它要求推导一个标准结果，然后进行数值计算。所提供的信息充分且一致，能够导出一个唯一解。\n-   **客观性：** 问题以精确、客观和定量的术语陈述。\n\n**3. 结论：**\n问题陈述在科学上是合理的，内部是一致的，并且是适定的。因此，它被认为是 **有效的**。\n\n### 步骤 2：求解\n\n#### 第 1 部分：选择概率的推导\n\n设 $P(x)$ 为决策变量 $x(t)$ 从 $x(0) = x$ 开始，在击中下边界 $-A$ 之前击中上边界 $+A$ 的概率。我们感兴趣的是求 $P(0)$。这个概率也称为准确率或正确概率，$P_c$。\n\n对于形式为 $dx_t = a(x_t, t)dt + b(x_t, t)dW_t$ 的伊藤过程，首达事件的概率（如 $P(x)$）满足 Kolmogorov 反向方程。对于我们的时间齐次过程 $dx(t) = \\mu \\, dt + \\sigma \\, dW_t$，漂移项是 $a(x) = \\mu$，扩散项是 $b(x) = \\sigma$。Kolmogorov 反向算子 $\\mathcal{L}$ 由下式给出：\n$$\n\\mathcal{L} = a(x)\\frac{d}{dx} + \\frac{1}{2}b(x)^2\\frac{d^2}{dx^2}\n$$\n对于我们特定的 DDM，这变为：\n$$\n\\mathcal{L} = \\mu\\frac{d}{dx} + \\frac{1}{2}\\sigma^2\\frac{d^2}{dx^2}\n$$\n对于边界内的所有起始点 $x$（即 $x \\in (-A, A)$），概率 $P(x)$ 必须满足微分方程 $\\mathcal{L}P(x) = 0$。这导致了以下二阶常微分方程（ODE）：\n$$\n\\frac{1}{2}\\sigma^2 \\frac{d^2P}{dx^2} + \\mu \\frac{dP}{dx} = 0\n$$\n边界条件由吸收的性质决定。如果过程从上边界开始，它会立即被吸收，因此击中上边界的概率是 $1$。如果它从下边界开始，它也会被吸收，此时击中上边界的概率是 $0$。\n$$\nP(A) = 1\n$$\n$$\nP(-A) = 0\n$$\n为了求解该 ODE，我们可以首先令 $Q(x) = \\frac{dP}{dx}$。方程变为关于 $Q(x)$ 的一阶线性 ODE：\n$$\n\\frac{1}{2}\\sigma^2 \\frac{dQ}{dx} + \\mu Q = 0 \\implies \\frac{dQ}{Q} = -\\frac{2\\mu}{\\sigma^2} dx\n$$\n对两边积分得到：\n$$\n\\ln|Q| = -\\frac{2\\mu}{\\sigma^2}x + C_1' \\implies Q(x) = C_1 \\exp\\left(-\\frac{2\\mu}{\\sigma^2}x\\right)\n$$\n现在我们对 $Q(x)$ 积分以求得 $P(x)$：\n$$\nP(x) = \\int Q(x) dx = \\int C_1 \\exp\\left(-\\frac{2\\mu}{\\sigma^2}x\\right) dx\n$$\n如果 $\\mu \\neq 0$，积分为：\n$$\nP(x) = C_1 \\left(-\\frac{\\sigma^2}{2\\mu}\\right) \\exp\\left(-\\frac{2\\mu}{\\sigma^2}x\\right) + C_2\n$$\n为方便起见，我们重新定义常数。通解的形式为 $P(x) = K_1 + K_2 \\exp\\left(-\\frac{2\\mu}{\\sigma^2}x\\right)$。\n我们现在应用边界条件来确定常数 $K_1$ 和 $K_2$：\n1. $P(A) = K_1 + K_2 \\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right) = 1$\n2. $P(-A) = K_1 + K_2 \\exp\\left(\\frac{2\\mu A}{\\sigma^2}\\right) = 0$\n\n从方程（2），我们得到 $K_1 = -K_2 \\exp\\left(\\frac{2\\mu A}{\\sigma^2}\\right)$。将其代入方程（1）：\n$$\n-K_2 \\exp\\left(\\frac{2\\mu A}{\\sigma^2}\\right) + K_2 \\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right) = 1\n$$\n$$\nK_2 \\left( \\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right) - \\exp\\left(\\frac{2\\mu A}{\\sigma^2}\\right) \\right) = 1 \\implies K_2 = \\frac{1}{\\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right) - \\exp\\left(\\frac{2\\mu A}{\\sigma^2}\\right)}\n$$\n那么，$K_1$ 是：\n$$\nK_1 = - \\frac{\\exp\\left(\\frac{2\\mu A}{\\sigma^2}\\right)}{\\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right) - \\exp\\left(\\frac{2\\mu A}{\\sigma^2}\\right)} = \\frac{\\exp\\left(\\frac{2\\mu A}{\\sigma^2}\\right)}{\\exp\\left(\\frac{2\\mu A}{\\sigma^2}\\right) - \\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right)}\n$$\n将 $K_1$ 和 $K_2$ 代回 $P(x)$ 的通解中：\n$$\nP(x) = \\frac{\\exp\\left(\\frac{2\\mu A}{\\sigma^2}\\right) - \\exp\\left(-\\frac{2\\mu x}{\\sigma^2}\\right)}{\\exp\\left(\\frac{2\\mu A}{\\sigma^2}\\right) - \\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right)}\n$$\n问题要求的是从 $x(0)=0$ 开始时做出正确选择的概率。我们在 $x=0$ 处计算 $P(x)$：\n$$\nP(0) = \\frac{\\exp\\left(\\frac{2\\mu A}{\\sigma^2}\\right) - \\exp(0)}{\\exp\\left(\\frac{2\\mu A}{\\sigma^2}\\right) - \\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right)} = \\frac{\\exp\\left(\\frac{2\\mu A}{\\sigma^2}\\right) - 1}{\\exp\\left(\\frac{2\\mu A}{\\sigma^2}\\right) - \\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right)}\n$$\n为了简化这个表达式，我们可以将分子和分母同乘以 $\\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right)$：\n$$\nP(0) = \\frac{\\left(\\exp\\left(\\frac{2\\mu A}{\\sigma^2}\\right) - 1\\right)\\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right)}{\\left(\\exp\\left(\\frac{2\\mu A}{\\sigma^2}\\right) - \\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right)\\right)\\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right)} = \\frac{1 - \\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right)}{1 - \\exp\\left(-\\frac{4\\mu A}{\\sigma^2}\\right)}\n$$\n分母可以因式分解为平方差，$1 - y^2 = (1-y)(1+y)$，其中 $y=\\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right)$：\n$$\nP(0) = \\frac{1 - \\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right)}{\\left(1 - \\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right)\\right)\\left(1 + \\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right)\\right)}\n$$\n假设 $\\mu A \\neq 0$，我们可以消去公因子，得到做出正确选择概率的最终简化表达式：\n$$\nP(0) = \\frac{1}{1 + \\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right)}\n$$\n这是一个关于自变量 $\\frac{2\\mu A}{\\sigma^2}$ 的逻辑（sigmoid）函数。\n\n#### 第 2 部分：边界高度 A 的计算\n\n给定目标准确率为 $p^{\\star} = 0.82$。这对应于 $P(0)$。我们将我们推导的表达式设为等于 $p^{\\star}$，然后求解 $A$：\n$$\np^{\\star} = \\frac{1}{1 + \\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right)}\n$$\n重新整理方程以求解 $A$：\n$$\n1 + \\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right) = \\frac{1}{p^{\\star}}\n$$\n$$\n\\exp\\left(-\\frac{2\\mu A}{\\sigma^2}\\right) = \\frac{1}{p^{\\star}} - 1 = \\frac{1 - p^{\\star}}{p^{\\star}}\n$$\n对两边取自然对数：\n$$\n-\\frac{2\\mu A}{\\sigma^2} = \\ln\\left(\\frac{1 - p^{\\star}}{p^{\\star}}\\right)\n$$\n最后，分离出 $A$：\n$$\nA = -\\frac{\\sigma^2}{2\\mu} \\ln\\left(\\frac{1 - p^{\\star}}{p^{\\star}}\\right) = \\frac{\\sigma^2}{2\\mu} \\ln\\left(\\left(\\frac{1 - p^{\\star}}{p^{\\star}}\\right)^{-1}\\right) = \\frac{\\sigma^2}{2\\mu} \\ln\\left(\\frac{p^{\\star}}{1 - p^{\\star}}\\right)\n$$\n现在，我们代入给定的数值：$\\mu = 0.75$，$\\sigma = 1.10$，以及 $p^{\\star} = 0.82$。\n$$\nA = \\frac{(1.10)^2}{2(0.75)} \\ln\\left(\\frac{0.82}{1 - 0.82}\\right)\n$$\n$$\nA = \\frac{1.21}{1.5} \\ln\\left(\\frac{0.82}{0.18}\\right)\n$$\n$$\nA = \\frac{1.21}{1.5} \\ln\\left(\\frac{41}{9}\\right)\n$$\n计算数值：\n$$\nA \\approx (0.80666...) \\times \\ln(4.5555...)\n$$\n$$\nA \\approx (0.80666...) \\times (1.5163475...)\n$$\n$$\nA \\approx 1.2229561\n$$\n问题要求答案四舍五入到四位有效数字。\n$$\nA \\approx 1.223\n$$\n$A$ 的单位是证据单位，与决策变量 $x(t)$ 的定义一致。", "answer": "$$\\boxed{1.223}$$", "id": "5011105"}, {"introduction": "现实世界中的决策很少是在真空中做出的；它们常常受到我们先验信念和预期的影响。这项练习通过在决策过程的起点（$Y_0$）引入偏差，扩展了基本的漂移扩散模型（DDM）框架以整合这些认知因素。解决这个问题将展示该模型如何变得更具心理学现实性，以及预先存在的偏见如何系统地影响我们的选择 [@problem_id:5011103]。", "problem": "神经经济学任务中的一个双择一强迫选择决策被建模为漂移扩散模型 (DDM)。内部决策变量是对数似然比 (LLR)，表示为 $Y_t$，它作为一个连续时间随机过程演化，其动态由随机微分方程 $dY_t = v\\,dt + \\sigma\\,dW_t$ 给出，其中 $v$ 是恒定漂移率，$\\sigma$ 是扩散系数，而 $W_t$ 是标准维纳过程。当 $Y_t$ 达到上吸收边界 $+B$ (选择选项 $\\mathcal{A}$) 或下吸收边界 $-B$ (选择选项 $\\mathcal{B}$) 时，决策终止。关于选项的先验信念将初始条件设置为先验对数几率 $Y_0 = \\ln\\!\\big(\\pi_{\\mathcal{A}}/\\pi_{\\mathcal{B}}\\big)$，提供了一个起始点偏差。\n\n从DDM的定义和上述边界条件出发，推导选择选项 $\\mathcal{A}$ 的概率作为参数 $v$、$\\sigma$、$B$ 和初始条件 $Y_0$ 的函数的闭式表达式。然后，使用 $v = 0.4$ (单位：对数几率单位/秒)、$\\sigma = 1.2$ (单位：对数几率单位/秒的平方根)、$B = 1.1$ (单位：对数几率单位) 以及先验概率 $\\pi_{\\mathcal{A}} = 0.65$ 和 $\\pi_{\\mathcal{B}} = 0.35$ 来评估此概率。将最终概率表示为小数，并四舍五入到四位有效数字。", "solution": "用户希望我解决一个与神经经济学中的漂移扩散模型 (DDM) 相关的问题。\n\n这是我的计划：\n1.  **验证问题：**\n    *   **步骤1：提取给定条件：** 我将列出所有提供的方程、变量和常数。\n        *   模型：漂移扩散模型 (DDM)。\n        *   决策变量：对数似然比 (LLR), $Y_t$。\n        *   SDE: $dY_t = v\\,dt + \\sigma\\,dW_t$。\n        *   $v$: 恒定漂移率。\n        *   $\\sigma$: 扩散系数。\n        *   $W_t$: 标准维纳过程。\n        *   上边界：$+B$ (选择选项 $\\mathcal{A}$)。\n        *   下边界：$-B$ (选择选项 $\\mathcal{B}$)。\n        *   初始条件：$Y_0 = \\ln(\\pi_{\\mathcal{A}}/\\pi_{\\mathcal{B}})$。\n        *   任务：推导选择选项 $\\mathcal{A}$ 的概率, $P(\\text{选择 } \\mathcal{A})$。\n        *   任务：为特定值评估此概率：\n            *   $v = 0.4$。\n            *   $\\sigma = 1.2$。\n            *   $B = 1.1$。\n            *   $\\pi_{\\mathcal{A}} = 0.65$。\n            *   $\\pi_{\\mathcal{B}} = 0.35$。\n        *   最终答案格式：小数，四舍五入到四位有效数字。\n\n    *   **步骤2：使用提取的给定条件进行验证：**\n        *   **科学依据：** DDM是认知神经科学和神经经济学中用于双择一强迫选择任务的标准且成熟的模型。SDE $dY_t = v\\,dt + \\sigma\\,dW_t$ 是带漂移的维纳过程的典型形式。基于先验对数几率的吸收边界和初始条件的概念也是该领域的标准实践。该问题牢固地建立在已有的科学和数学原理之上。\n        *   **良构性：** 该问题要求计算一个一维扩散过程从两边界之间的一点出发，触及其中一个吸收边界的概率。这是一个经典的首次穿越时间问题，特别是在连续时间、连续空间背景下的“赌徒破产”问题。在随机过程文献中，存在一个唯一、稳定且有意义的解，并且广为人知。\n        *   **客观性：** 语言精确、定量，且没有偏见。\n        *   **科学或事实上的不健全性：** 无。该模型是决策理论的基石。\n        *   **不可形式化或不相关：** 无。该问题是一个直接与所述主题相关的形式化数学练习。\n        *   **不完整或矛盾的设置：** 所有必要的参数 ($v, \\sigma, B, \\pi_{\\mathcal{A}}, \\pi_{\\mathcal{B}})$ 都已提供。初始条件定义明确。边界是对称的，这是一种常见的设置。问题是自洽的。提供的单位（对数几率单位/秒等）与DDM框架一致。\n        *   **不现实或不可行：** 参数值对于典型的知觉决策任务来说是合理的。\n        *   **病态或结构不良：** 否。该问题是一个标准的推导后跟计算。\n        *   **伪深刻、琐碎或同义反复：** 否。推导需要随机微积分或相关技术的知识，而随后的计算是所推导公式的直接应用。这是一个标准的教科书级别的问题。\n        *   **无法通过科学方法验证：** 解可以通过数学方法进行验证。\n\n    *   **步骤3：结论和行动：** 问题是**有效的**。我将着手解决它。\n\n2.  **解决问题：**\n    *   **第一部分：概率的推导。**\n        *   问题是找到从 $Y_0$ 开始的过程 $Y_t$ 在到达 $-B$ 之前先到达边界 $+B$ 的概率。我们将这个概率称为 $P(Y_0)$。\n        *   这是随机过程中的一个经典问题。从 $Y_0 = y$ 开始，在到达 $-B$ 之前先到达边界 $+B$ 的概率 $P(y)$ 满足称为柯尔莫哥洛夫后向方程的常微分方程 (ODE)。对于过程 $dY_t = \\mu(Y_t) dt + \\sigma(Y_t) dW_t$，概率 $P(y)$ 满足：\n            $$ \\mu(y) \\frac{dP}{dy} + \\frac{1}{2}\\sigma^2(y) \\frac{d^2P}{dy^2} = 0 $$\n        *   在我们的特定情况下，漂移 $\\mu(y) = v$ 和扩散系数 $\\sigma(y) = \\sigma$ 是常数。所以ODE简化为：\n            $$ v \\frac{dP}{dy} + \\frac{1}{2}\\sigma^2 \\frac{d^2P}{dy^2} = 0 $$\n        *   边界条件由吸收边界确定。如果过程从 $+B$ 开始，它已经“选择”了 $\\mathcal{A}$，因此选择 $\\mathcal{A}$ 的概率是 $1$。如果它从 $-B$ 开始，它已经选择了 $\\mathcal{B}$，因此选择 $\\mathcal{A}$ 的概率是 $0$。\n            *   $P(+B) = 1$\n            *   $P(-B) = 0$\n        *   我们来解这个ODE。设 $P'(y) = \\frac{dP}{dy}$ 和 $P''(y) = \\frac{d^2P}{dy^2}$。方程是：\n            $$ \\frac{1}{2}\\sigma^2 P'' + v P' = 0 $$\n            $$ P'' = -\\frac{2v}{\\sigma^2} P' $$\n        *   这是关于 $P'$ 的一阶线性ODE。设 $Q(y) = P'(y)$。\n            $$ Q' = -\\frac{2v}{\\sigma^2} Q $$\n        *   解是 $Q(y) = C_1 \\exp\\left(-\\frac{2v}{\\sigma^2} y\\right)$。\n        *   现在，我们对 $Q(y)$ 积分来求 $P(y)$：\n            $$ P(y) = \\int Q(y) dy = \\int C_1 \\exp\\left(-\\frac{2v}{\\sigma^2} y\\right) dy $$\n            $$ P(y) = C_1 \\left(-\\frac{\\sigma^2}{2v}\\right) \\exp\\left(-\\frac{2v}{\\sigma^2} y\\right) + C_2 $$\n        *   这可以写成具有常系数的二阶线性齐次ODE的标准形式。特征方程是 $\\frac{1}{2}\\sigma^2 r^2 + vr = 0$，得到 $r(\\frac{1}{2}\\sigma^2 r + v) = 0$。根是 $r_1=0$ 和 $r_2 = -2v/\\sigma^2$。\n        *   因此，$P(y)$ 的通解是：\n            $$ P(y) = C_1 \\exp(r_1 y) + C_2 \\exp(r_2 y) = C_1 + C_2 \\exp\\left(-\\frac{2v}{\\sigma^2}y\\right) $$\n        *   我们应用边界条件来求解常数 $C_1$ 和 $C_2$。\n            对于下边界 $y=-B$, $P(-B) = 0$:\n            $$ C_1 + C_2 \\exp\\left(-\\frac{2v(-B)}{\\sigma^2}\\right) = C_1 + C_2 \\exp\\left(\\frac{2vB}{\\sigma^2}\\right) = 0 \\quad (1) $$\n            对于上边界 $y=B$, $P(B) = 1$:\n            $$ C_1 + C_2 \\exp\\left(-\\frac{2vB}{\\sigma^2}\\right) = 1 \\quad (2) $$\n        *   从方程(2)中减去方程(1)得到：\n            $$ C_2 \\left[\\exp\\left(-\\frac{2vB}{\\sigma^2}\\right) - \\exp\\left(\\frac{2vB}{\\sigma^2}\\right)\\right] = 1 $$\n            $$ C_2 = \\frac{1}{\\exp\\left(-\\frac{2vB}{\\sigma^2}\\right) - \\exp\\left(\\frac{2vB}{\\sigma^2}\\right)} $$\n        *   从方程(1)中，我们求得 $C_1$:\n            $$ C_1 = -C_2 \\exp\\left(\\frac{2vB}{\\sigma^2}\\right) = -\\frac{\\exp\\left(\\frac{2vB}{\\sigma^2}\\right)}{\\exp\\left(-\\frac{2vB}{\\sigma^2}\\right) - \\exp\\left(\\frac{2vB}{\\sigma^2}\\right)} $$\n        *   将 $C_1$ 和 $C_2$ 代入 $P(y)$ 的通解中：\n            $$ P(y) = \\frac{-\\exp\\left(\\frac{2vB}{\\sigma^2}\\right) + \\exp\\left(-\\frac{2vy}{\\sigma^2}\\right)}{\\exp\\left(-\\frac{2vB}{\\sigma^2}\\right) - \\exp\\left(\\frac{2vB}{\\sigma^2}\\right)} $$\n        *   为了简化这个表达式，我们可以将分子和分母同乘以 $-1$：\n            $$ P(y) = \\frac{\\exp\\left(\\frac{2vB}{\\sigma^2}\\right) - \\exp\\left(-\\frac{2vy}{\\sigma^2}\\right)}{\\exp\\left(\\frac{2vB}{\\sigma^2}\\right) - \\exp\\left(-\\frac{2vB}{\\sigma^2}\\right)} $$\n        *   一个更方便的形式是通过考虑一个从 $z_0 = y+B$ 开始，边界在 $0$ 和 $2B$ 的过程。对于边界在 $a$ 和 $b$ ($a  b$) 的通解是 $P(y;a,b) = \\frac{\\exp(-\\frac{2vy}{\\sigma^2}) - \\exp(-\\frac{2va}{\\sigma^2})}{\\exp(-\\frac{2vb}{\\sigma^2}) - \\exp(-\\frac{2va}{\\sigma^2})}$。\n        *   对于我们的边界 $a=-B$ 和 $b=B$，这给出：\n            $$ P(y) = \\frac{\\exp\\left(-\\frac{2vy}{\\sigma^2}\\right) - \\exp\\left(\\frac{2vB}{\\sigma^2}\\right)}{\\exp\\left(-\\frac{2vB}{\\sigma^2}\\right) - \\exp\\left(\\frac{2vB}{\\sigma^2}\\right)} $$\n        *   为了得到一个更简洁的形式，我们将分子和分母同乘以 $\\exp\\left(-\\frac{2vB}{\\sigma^2}\\right)$:\n            分子: $(\\exp(-\\frac{2vy}{\\sigma^2}) - \\exp(\\frac{2vB}{\\sigma^2})) \\times \\exp(-\\frac{2vB}{\\sigma^2}) = \\exp(-\\frac{2v(y+B)}{\\sigma^2}) - 1$。\n            分母: $(\\exp(-\\frac{2vB}{\\sigma^2}) - \\exp(\\frac{2vB}{\\sigma^2})) \\times \\exp(-\\frac{2vB}{\\sigma^2}) = \\exp(-\\frac{4vB}{\\sigma^2}) - 1$。\n        *   该比率变为：\n            $$ P(y) = \\frac{\\exp(-\\frac{2v(y+B)}{\\sigma^2}) - 1}{\\exp(-\\frac{4vB}{\\sigma^2}) - 1} = \\frac{1 - \\exp\\left(-\\frac{2v(y+B)}{\\sigma^2}\\right)}{1 - \\exp\\left(-\\frac{4vB}{\\sigma^2}\\right)} $$\n        *   这是在给定起始点 $y$ 的情况下选择选项 $\\mathcal{A}$ 的概率的闭式表达式。对于本问题，我们使用初始条件 $y=Y_0$。\n            $$ P(\\text{选择 } \\mathcal{A}) = \\frac{1 - \\exp\\left(-\\frac{2v(Y_0+B)}{\\sigma^2}\\right)}{1 - \\exp\\left(-\\frac{4vB}{\\sigma^2}\\right)} $$\n        *   现在，我们使用给定的参数值来评估此表达式。\n            参数为：\n            $v = 0.4$\n            $\\sigma = 1.2$\n            $B = 1.1$\n            $\\pi_{\\mathcal{A}} = 0.65$\n            $\\pi_{\\mathcal{B}} = 0.35$\n        *   首先，计算初始条件 $Y_0$：\n            $$ Y_0 = \\ln\\left(\\frac{\\pi_{\\mathcal{A}}}{\\pi_{\\mathcal{B}}}\\right) = \\ln\\left(\\frac{0.65}{0.35}\\right) = \\ln\\left(\\frac{13}{7}\\right) $$\n        *   接下来，计算指数中的项 $\\frac{2v}{\\sigma^2}$：\n            $$ \\frac{2v}{\\sigma^2} = \\frac{2 \\times 0.4}{(1.2)^2} = \\frac{0.8}{1.44} = \\frac{80}{144} = \\frac{5 \\times 16}{9 \\times 16} = \\frac{5}{9} $$\n        *   现在，我们计算指数函数的参数。\n            分子指数的参数：\n            $$ -\\frac{2v(Y_0+B)}{\\sigma^2} = -\\frac{5}{9}\\left(\\ln\\left(\\frac{13}{7}\\right) + 1.1\\right) $$\n            分母指数的参数：\n            $$ -\\frac{4vB}{\\sigma^2} = -\\frac{2v(2B)}{\\sigma^2} = -\\frac{5}{9}(2 \\times 1.1) = -\\frac{5}{9}(2.2) = -\\frac{11}{9} $$\n        *   将这些代入概率公式：\n            $$ P(\\text{选择 } \\mathcal{A}) = \\frac{1 - \\exp\\left(-\\frac{5}{9}\\left(\\ln\\left(\\frac{13}{7}\\right) + 1.1\\right)\\right)}{1 - \\exp\\left(-\\frac{11}{9}\\right)} $$\n        *   现在我们计算数值：\n            $Y_0 = \\ln(13/7) \\approx 0.6190392$\n            $Y_0 + B \\approx 0.6190392 + 1.1 = 1.7190392$\n            分子指数：$-\\frac{5}{9} \\times 1.7190392 \\approx -0.9550218$\n            $\\exp(-0.9550218) \\approx 0.384813$\n            分子 = $1 - 0.384813 = 0.615187$\n            分母指数：$-\\frac{11}{9} \\approx -1.222222$\n            $\\exp(-1.222222) \\approx 0.294570$\n            分母 = $1 - 0.294570 = 0.705430$\n            $$ P(\\text{选择 } \\mathcal{A}) \\approx \\frac{0.615187}{0.705430} \\approx 0.872080 $$\n        *   将结果四舍五入到四位有效数字，得到 $0.8721$。", "answer": "$$\n\\boxed{0.8721}\n$$", "id": "5011103"}, {"introduction": "这项练习将理论付诸实践，模拟了计算神经科学家检验关于决策的竞争性假设时的工作流程。您将从一个已知模型生成合成数据，然后使用统计技术来拟合和比较两种不同的模型——一种基于漂移率，另一种基于起始点偏差——以确定哪种模型能更好地解释数据。这种动手操作的模型恢复和比较练习，对于理解神经经济学理论如何根据经验数据进行严格测试和验证至关重要 [@problem_id:5011113]。", "problem": "你必须编写一个完整、可运行的程序，在神经经济学背景下，使用漂移扩散模型 (DDM) 和偏置起始布朗模型执行模型比较和恢复。该程序将根据指定的参数集模拟选择和反应时间数据，通过最大似然法使用源自首次穿越过程的成熟性质的摘要统计量来拟合两个候选模型，并使用贝叶斯信息准则 (BIC) 选择更优的模型。最终输出必须是单行，包含一个布尔值列表，指示每个测试用例的生成模型是否被正确恢复。\n\n从以下基本基础开始：\n\n- 在漂移扩散模型 (DDM) 下，决策变量 $x_t$ 根据一个随机微分方程演化：$$dx_t = v\\,dt + \\sigma\\,dW_t,$$ 其中 $v$ 是漂移率，$\\sigma$ 是噪声振幅，$W_t$ 是标准维纳过程。当 $x_t$ 到达两个吸收边界 $\\pm a$ 中的一个时，决策发生。非决策时间 $T_{\\mathrm{er}}$ 解释了感觉和运动延迟，并加到决策时间上，以秒为单位产生观察到的反应时间。\n- 对于从 $x_0 = 0$ 开始、在 $\\pm a$ 处有对称吸收边界的 DDM，到达上边界的概率由经过充分检验的公式给出：$$p_{\\mathrm{upper}} = \\frac{1}{1 + \\exp\\!\\left(-\\frac{2 v a}{\\sigma^2}\\right)}.$$ 到达任一边界的平均首次穿越时间为：$$\\mathbb{E}[T_{\\mathrm{dec}}] = \\begin{cases}\\frac{a}{v}\\,\\tanh\\!\\left(\\frac{a v}{\\sigma^2}\\right),  v \\neq 0, \\\\[6pt] \\frac{a^2}{\\sigma^2},  v = 0.\\end{cases}$$ 预期的观察反应时间为 $\\mathbb{E}[\\mathrm{RT}] = \\mathbb{E}[T_{\\mathrm{dec}}] + T_{\\mathrm{er}}$ 秒。\n- 对于一个无漂移布朗运动模型 ($v = 0$)，其起始偏置为 $z$，边界在 $\\pm a$，则到达上边界的概率为：$$p_{\\mathrm{upper}} = \\frac{z + a}{2a},$$ 对 $-a  z  a$ 有效，平均首次穿越时间为：$$\\mathbb{E}[T_{\\mathrm{dec}}] = \\frac{a^2 - z^2}{\\sigma^2},$$ 预期的观察反应时间为 $\\mathbb{E}[\\mathrm{RT}] = \\mathbb{E}[T_{\\mathrm{dec}}] + T_{\\mathrm{er}}$ 秒。\n\n假设在拟合时使用以下经过充分检验的统计近似：\n\n- 在 $N$ 次试验中，到达上边界的次数 $k$ 由二项分布建模，其参数 $p$ 由模型的 $p_{\\mathrm{upper}}$ 预测给出。在 $N$ 次试验中 $k$ 次成功的二项对数似然为：$$\\log L_{\\mathrm{bin}} = \\log\\binom{N}{k} + k \\log p + (N-k)\\log(1-p)。$$\n- 根据中心极限定理 (CLT)，样本平均反应时间 $\\bar{t}$ 近似服从正态分布，其均值为 $\\mathbb{E}[\\mathrm{RT}]$，方差为 $\\operatorname{Var}(\\bar{t}) \\approx s^2/N$，其中 $s^2$ 是观察到的反应时间的样本方差。$\\bar{t}$ 的正态对数似然为：$$\\log L_{\\mathrm{norm}} = -\\tfrac{1}{2}\\left(\\log(2\\pi\\,\\operatorname{Var}(\\bar{t})) + \\frac{(\\bar{t} - \\mathbb{E}[\\mathrm{RT}])^2}{\\operatorname{Var}(\\bar{t})}\\right)。$$\n- 用于拟合的总对数似然为 $\\log L = \\log L_{\\mathrm{bin}} + \\log L_{\\mathrm{norm}}$。\n\n模型比较将使用贝叶斯信息准则 (BIC) 进行，其定义为：$$\\mathrm{BIC} = -2 \\log L_{\\max} + k \\log N_{\\mathrm{eff}},$$ 其中 $\\log L_{\\max}$ 是模型的最大化对数似然， $k$ 是自由参数的数量，$N_{\\mathrm{eff}}$ 是复合似然中使用的有效样本量。为保证模型间的一致性，取 $N_{\\mathrm{eff}} = N + 1$，以考虑 $N$ 个二项观测值和一个平均反应时间观测值。\n\n程序要求：\n\n- 实现两个候选模型：\n\n  1. 漂移扩散模型 (DDM)：参数为 $(v, a, T_{\\mathrm{er}})$，固定 $\\sigma = 1$。\n  2. 偏置起始布朗模型：参数为 $(s, a, T_{\\mathrm{er}})$，固定 $\\sigma = 1$，其中起始偏置参数化为 $z = sa$ 且 $-1  s  1$，以强制 $-a  z  a$。\n\n- 通过离散时间 Euler–Maruyama 积分（时间步长 $dt$ 以秒为单位）为每个测试用例模拟数据，直到穿越边界，然后加上 $T_{\\mathrm{er}}$ 以秒为单位生成反应时间。计算到达上边界的次数以及反应时间的样本均值和方差。\n\n- 通过最大化 $\\log L$ 来拟合每个模型，参数需在以下范围内：\n  - 对于 DDM：$v \\in [-1, 1]$（单位为秒的倒数），$a \\in [0.2, 2]$（无单位决策变量），$T_{\\mathrm{er}} \\in [0.1, 0.6]$（秒）。\n  - 对于偏置起始布朗模型：$s \\in [-0.99, 0.99]$（无单位），$a \\in [0.2, 2]$，$T_{\\mathrm{er}} \\in [0.1, 0.6]$。\n  对平均反应时间部分使用正态近似方差 $\\operatorname{Var}(\\bar{t}) = s^2/N$。\n\n- 为每个数据集计算两个模型的 $\\mathrm{BIC}$，选择 $\\mathrm{BIC}$ 较低的模型，并报告它是否与生成模型匹配。\n\n物理和数值单位：\n\n- 所有反应时间必须以秒为单位处理。Euler–Maruyama 时间步长 $dt$ 以秒为单位。漂移率 $v$ 的单位是秒的倒数，$\\sigma$ 的单位是 $\\text{seconds}^{-1/2}$，这样 $\\sigma \\sqrt{dt}$ 的单位与决策变量的增量相匹配。\n\n测试套件：\n\n- 使用以下三个测试用例，以确保覆盖典型情况、偏置起始情况和模糊边界情况。对于所有情况，设置 $\\sigma = 1$，$dt = 0.001$ 秒，以及 $N = 800$ 次试验。\n  1. 生成模型索引 $0$ (DDM)，参数：$v = 0.4$, $a = 1.0$, $T_{\\mathrm{er}} = 0.3$。\n  2. 生成模型索引 $1$ (偏置起始布朗)，参数：$s = 0.3$ (因此 $z = sa$), $a = 1.0$, $T_{\\mathrm{er}} = 0.3$。\n  3. 生成模型索引 $0$ (DDM)，参数：$v = 0.0$, $a = 1.0$, $T_{\\mathrm{er}} = 0.3$。\n\n最终输出格式：\n\n- 你的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表（例如，$[result1,result2,result3]$）。每个结果必须是一个布尔值，指示相应测试用例的生成模型是否被正确恢复。", "solution": "该问题要求对两种竞争的决策模型进行模型恢复分析：漂移扩散模型 (DDM) 和偏置起始布朗模型。该分析涉及从一个已知的生成模型中模拟数据，将两个候选模型拟合到这些数据上，并使用贝叶斯信息准则 (BIC) 来确定哪个模型提供了更好的解释。通过检查 BIC 选择的模型是否与真实的数据生成模型匹配来评估恢复的成功与否。\n\n每个测试用例的总体流程如下：\n1.  **数据模拟**：为 $N=800$ 次试验生成选择和反应时间 (RT) 数据。模拟遵循指定的生成模型（DDM 或偏置起始模型）及其参数。\n2.  **摘要统计量**：将模拟数据浓缩为三个摘要统计量：选择上边界的次数 ($k$)、RT 的样本均值 ($\\bar{t}$) 和 RT 的样本方差 ($s^2_{\\mathrm{RT}}$)。\n3.  **模型拟合**：通过最大化复合对数似然函数，将两个候选模型中的每一个拟合到摘要统计量上。\n4.  **模型比较**：为每个拟合的模型计算 BIC。BIC 较低的模型被选为获胜者。\n5.  **恢复评估**：将获胜模型与真实的生成模型进行比较，并记录结果（一个布尔值）。\n\n**1. 数据模拟**\n\n决策过程被建模为决策变量 $x_t$ 的演化。使用相应随机微分方程的 Euler-Maruyama 模拟生成数据，时间步长为 $dt=0.001$ 秒。对于 $N=800$ 次试验中的每一次，过程从一个初始位置开始（DDM 为 $x_0=0$，偏置起始模型为 $x_0=z=sa$），并演化直到它穿过位于 $\\pm a$ 的两个吸收边界中的一个。达到边界所花费的时间是决策时间，向其添加一个非决策时间 $T_{\\mathrm{er}}$ 以产生观察到的 RT。模拟产生 $N$ 对选择和 RT，从中计算出摘要统计量。在所有模拟和拟合中，噪声振幅固定为 $\\sigma=1$。\n\n**2. 模型定义和分析预测**\n\n考虑两个模型：\n\n-   **漂移扩散模型 (DDM)**：决策变量根据 $dx_t = v\\,dt + \\sigma\\,dW_t$ 演化。该模型有三个自由参数：漂移率 $v$、边界分离 $a$ 和非决策时间 $T_{\\mathrm{er}}$。对于起始点 $x_0=0$，到达上边界 ($+a$) 的概率和平均决策时间由既定公式给出：\n    $$p_{\\mathrm{upper}} = \\frac{1}{1 + \\exp\\!\\left(-\\frac{2 v a}{\\sigma^2}\\right)}$$\n    $$\\mathbb{E}[T_{\\mathrm{dec}}] = \\begin{cases}\\frac{a}{v}\\,\\tanh\\!\\left(\\frac{a v}{\\sigma^2}\\right),  v \\neq 0 \\\\ \\frac{a^2}{\\sigma^2},  v = 0\\end{cases}$$\n\n-   **偏置起始布朗模型**：该模型是 DDM 的一个特例，漂移为零 ($v=0$) 但起始点非零 $x_0 = z$。其随机微分方程为 $dx_t = \\sigma\\,dW_t$。偏置被参数化为 $z=sa$，其中 $s \\in (-1, 1)$ 是一个缩放因子。该模型也有三个自由参数：偏置参数 $s$、边界分离 $a$ 和非决策时间 $T_{\\mathrm{er}}$。其分析预测为：\n    $$p_{\\mathrm{upper}} = \\frac{z + a}{2a} = \\frac{s a + a}{2a} = \\frac{s+1}{2}$$\n    $$\\mathbb{E}[T_{\\mathrm{dec}}] = \\frac{a^2 - z^2}{\\sigma^2} = \\frac{a^2 - (sa)^2}{\\sigma^2} = \\frac{a^2(1-s^2)}{\\sigma^2}$$\n\n对于两个模型，预测的平均反应时间均为 $\\mathbb{E}[\\mathrm{RT}] = \\mathbb{E}[T_{\\mathrm{dec}}] + T_{\\mathrm{er}}$。\n\n**3. 通过最大似然进行参数估计**\n\n模型参数通过最大化一个复合对数似然函数 $\\log L$ 来估计，该函数结合了来自选择和反应时间的信息。\n$$\\log L = \\log L_{\\mathrm{bin}} + \\log L_{\\mathrm{norm}}$$\n-   选择数据（$N$ 次试验中 $k$ 次到达上边界）用二项分布建模。对数似然为：\n    $$\\log L_{\\mathrm{bin}} = \\log\\binom{N}{k} + k \\log p_{\\mathrm{upper}} + (N-k)\\log(1-p_{\\mathrm{upper}})$$\n    其中 $p_{\\mathrm{upper}}$ 是模型对上边界选择概率的预测。\n-   基于中心极限定理，样本平均 RT $\\bar{t}$ 用正态分布建模。对数似然为：\n    $$\\log L_{\\mathrm{norm}} = -\\tfrac{1}{2}\\left(\\log(2\\pi\\,\\operatorname{Var}(\\bar{t})) + \\frac{(\\bar{t} - \\mathbb{E}[\\mathrm{RT}])^2}{\\operatorname{Var}(\\bar{t})}\\right)$$\n    其中 $\\mathbb{E}[\\mathrm{RT}]$ 是模型预测的平均 RT，$\\operatorname{Var}(\\bar{t})$ 是样本均值的方差，估计为 $s^2_{\\mathrm{RT}}/N$。\n\n使用 L-BFGS-B 优化算法，对总对数似然 $\\log L$ 关于模型参数（DDM 的 $(v, a, T_{\\mathrm{er}})$；偏置起始模型的 $(s, a, T_{\\mathrm{er}})$）进行最大化，该算法可处理规定的参数边界：$v \\in [-1, 1]$、$s \\in [-0.99, 0.99]$、$a \\in [0.2, 2]$ 和 $T_{\\mathrm{er}} \\in [0.1, 0.6]$。\n\n**4. 使用 BIC 进行模型比较**\n\n贝叶斯信息准则 (BIC) 用于模型选择。BIC 平衡了模型拟合度（似然）与模型复杂度（参数数量），对参数更多的模型进行惩罚以避免过拟合。公式为：\n$$\\mathrm{BIC} = -2 \\log L_{\\max} + k_{\\mathrm{params}} \\log N_{\\mathrm{eff}}$$\n这里，$\\log L_{\\max}$ 是从拟合过程中获得的最大化对数似然值。两个模型都有 $k_{\\mathrm{params}}=3$ 个自由参数。有效样本量 $N_{\\mathrm{eff}}$ 取为 $N+1 = 801$，以计入 $N$ 个选择观测值和一个平均 RT 观测值。产生较低 BIC 值的模型被认为是给定数据的更优模型。此过程系统地应用于为每个测试用例生成的数据，以确定生成模型是否可以被成功恢复。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\n\ndef solve():\n    \"\"\"\n    Main function to run the model recovery simulation for all test cases.\n    \"\"\"\n    # Set a random seed for reproducibility of the simulation.\n    np.random.seed(42)\n\n    # Define the constants used throughout the simulation and fitting.\n    SIGMA = 1.0\n    DT = 0.001\n    N_TRIALS = 800\n    N_PARAMS = 3\n    N_EFF = N_TRIALS + 1\n    \n    # Define the test cases as (generating_model_index, parameters).\n    # Model 0: DDM (v, a, Ter)\n    # Model 1: Biased-start Brownian (s, a, Ter)\n    test_cases = [\n        (0, (0.4, 1.0, 0.3)),   \n        (1, (0.3, 1.0, 0.3)),   \n        (0, (0.0, 1.0, 0.3)),   \n    ]\n\n    results = []\n    for gen_model_idx, gen_params in test_cases:\n        # 1. Simulate data from the generating model.\n        summary_stats = _simulate_data(gen_model_idx, gen_params, SIGMA, DT, N_TRIALS)\n        k_upper, mean_rt, var_rt = summary_stats\n\n        # 2. Fit DDM (model 0) and calculate its BIC.\n        max_log_L_ddm = _fit_model(0, N_TRIALS, k_upper, mean_rt, var_rt)\n        bic_ddm = _calculate_bic(max_log_L_ddm, N_PARAMS, N_EFF)\n\n        # 3. Fit Biased-start model (model 1) and calculate its BIC.\n        max_log_L_biased = _fit_model(1, N_TRIALS, k_upper, mean_rt, var_rt)\n        bic_biased = _calculate_bic(max_log_L_biased, N_PARAMS, N_EFF)\n\n        # 4. Compare BICs and determine if the generating model was recovered.\n        recovered_idx = 0 if bic_ddm  bic_biased else 1\n        is_recovered = (recovered_idx == gen_model_idx)\n        results.append(is_recovered)\n\n    # Print the final results in the specified format.\n    print(f\"[{','.join(map(lambda x: str(x).lower(), results))}]\")\n\ndef _simulate_one_trial(v, a, start_pos, sigma, dt):\n    \"\"\"\n    Simulates a single trial of a drift-diffusion process using Euler-Maruyama.\n    \"\"\"\n    x = start_pos\n    # Set a generous timeout to prevent infinite loops (e.g., 5 seconds).\n    max_time_steps = int(5.0 / dt)\n    \n    # Pre-generate noise for efficiency.\n    noise_increments = np.random.randn(max_time_steps) * sigma * np.sqrt(dt)\n    drift_increment = v * dt\n    \n    for i in range(max_time_steps):\n        x += drift_increment + noise_increments[i]\n        if np.abs(x) >= a:\n            decision_time = (i + 1) * dt\n            choice = 1 if x > 0 else 0  # 1 for upper, 0 for lower\n            return decision_time, choice\n            \n    # Fallback for timeout (should not be reached with valid parameters).\n    return max_time_steps * dt, np.random.randint(0, 2)\n\ndef _simulate_data(model_type, params, sigma, dt, n_trials):\n    \"\"\"\n    Generates choice and RT data for a given model and parameters.\n    Returns summary statistics: (k_upper, mean_rt, var_rt).\n    \"\"\"\n    if model_type == 0:  # DDM\n        v, a, Ter = params\n        start_pos = 0.0\n    else:  # Biased-start\n        s, a, Ter = params\n        v = 0.0\n        start_pos = s * a\n\n    decision_times = np.zeros(n_trials)\n    choices = np.zeros(n_trials)\n\n    for i in range(n_trials):\n        dt_i, choice_i = _simulate_one_trial(v, a, start_pos, sigma, dt)\n        decision_times[i] = dt_i\n        choices[i] = choice_i\n        \n    response_times = decision_times + Ter\n    \n    k_upper = np.sum(choices)\n    mean_rt = np.mean(response_times)\n    var_rt = np.var(response_times, ddof=1) # Sample variance\n\n    return k_upper, mean_rt, var_rt\n\ndef _predict_ddm(params, sigma):\n    \"\"\"Calculates p_upper and E[RT] for the DDM.\"\"\"\n    v, a, Ter = params\n    \n    if np.abs(v)  1e-9:\n        p_upper = 0.5\n        mean_decision_time = a**2 / sigma**2\n    else:\n        v_a_term = v * a / (sigma**2)\n        p_upper = 1.0 / (1.0 + np.exp(-2 * v_a_term))\n        mean_decision_time = (a / v) * np.tanh(v_a_term)\n        \n    mean_rt = mean_decision_time + Ter\n    return p_upper, mean_rt\n\ndef _predict_biased_start(params, sigma):\n    \"\"\"Calculates p_upper and E[RT] for the biased-start model.\"\"\"\n    s, a, Ter = params\n    z = s * a\n    \n    p_upper = (z + a) / (2 * a)\n    mean_decision_time = (a**2 - z**2) / sigma**2\n    mean_rt = mean_decision_time + Ter\n    return p_upper, mean_rt\n\ndef _composite_neg_log_likelihood(params, model_type, n_trials, k_upper, mean_rt, var_rt, sigma):\n    \"\"\"\n    Calculates the negative of the composite log-likelihood for a given model.\n    \"\"\"\n    if model_type == 0:\n        p_pred, mean_rt_pred = _predict_ddm(params, sigma)\n    else:\n        p_pred, mean_rt_pred = _predict_biased_start(params, sigma)\n\n    # Clip p_pred to avoid numerical instability with log(0).\n    epsilon = 1e-9\n    p_pred = np.clip(p_pred, epsilon, 1 - epsilon)\n\n    # Binomial log-likelihood for choices.\n    log_L_bin = (gammaln(n_trials + 1) - gammaln(k_upper + 1) - gammaln(n_trials - k_upper + 1) +\n                 k_upper * np.log(p_pred) + (n_trials - k_upper) * np.log(1 - p_pred))\n\n    # Normal log-likelihood for the mean response time.\n    var_mean_rt = var_rt / n_trials\n    if var_mean_rt = 0: return np.inf # Avoid math domain error.\n    \n    log_L_norm = -0.5 * (np.log(2 * np.pi * var_mean_rt) + \n                         (mean_rt - mean_rt_pred)**2 / var_mean_rt)\n\n    return -(log_L_bin + log_L_norm)\n\ndef _fit_model(model_type, n_trials, k_upper, mean_rt, var_rt):\n    \"\"\"\n    Fits a model by minimizing the negative log-likelihood.\n    Returns the maximized log-likelihood.\n    \"\"\"\n    if model_type == 0: # DDM\n        bounds = ((-1.0, 1.0), (0.2, 2.0), (0.1, 0.6))\n        x0 = (0.0, 1.1, 0.35)\n    else: # Biased-start\n        bounds = ((-0.99, 0.99), (0.2, 2.0), (0.1, 0.6))\n        x0 = (0.0, 1.1, 0.35)\n        \n    args = (model_type, n_trials, k_upper, mean_rt, var_rt, 1.0)\n\n    result = minimize(\n        fun=_composite_neg_log_likelihood,\n        x0=x0,\n        args=args,\n        method='L-BFGS-B',\n        bounds=bounds\n    )\n    \n    max_log_L = -result.fun\n    return max_log_L\n\ndef _calculate_bic(max_log_L, k_params, n_eff):\n    \"\"\"Calculates the Bayesian Information Criterion.\"\"\"\n    return -2 * max_log_L + k_params * np.log(n_eff)\n\nif __name__ == '__main__':\n    solve()\n```", "id": "5011113"}]}