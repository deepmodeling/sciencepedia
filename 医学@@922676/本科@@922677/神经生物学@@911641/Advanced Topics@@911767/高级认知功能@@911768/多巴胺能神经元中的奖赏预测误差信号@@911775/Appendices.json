{"hands_on_practices": [{"introduction": "我们将从掌握奖励预测误差（Reward Prediction Error, RPE）的基本计算开始。这个练习将帮助你理解即时奖励和下一状态的价值如何结合，从而产生一个相对于当前状态价值的“意外”信号。这是理解多巴胺神经元反应的基础模块，它利用了时间差分（Temporal-Difference, TD）学习的核心公式。[@problem_id:5058200]", "problem": "在一个由时间差分（TD）学习建模的巴甫洛夫条件反射任务中，假设腹侧被盖区的多巴胺能神经元编码一个奖励预测误差（RPE），该误差通过比较实际结果与基于状态价值的预期未来回报。一个动物从状态 $s_t$ 转移到 $s_{t+1}$，接收到即时奖励 $r_t$，并且拥有状态价值估计 $V(s_t)$ 和 $V(s_{t+1})$。折扣因子为 $\\gamma$。\n\n给定 $V(s_t)=0.5$，$V(s_{t+1})=0.6$，$r_t=1$ 以及 $\\gamma=0.9$，计算 RPE $\\delta_t$。然后，根据 $\\delta_t$ 的符号，说明多巴胺能神经元的放电会短暂地高于基线水平还是低于基线水平，并从业已之外的奖励的角度解释原因。\n\n将 $\\delta_t$ 表示为一个没有单位的纯数。无需四舍五入。", "solution": "该问题要求在时间差分（TD）学习框架中计算奖励预测误差（RPE），记为 $\\delta_t$，并结合多巴胺能神经元活动的背景来解释这个信号。\n\n首先，我们必须定义 RPE。在 TD 学习中，时间 $t$ 的 RPE $\\delta_t$ 量化了观测到的奖励加上后续状态的折扣估计价值与当前状态的估计价值之间的差异。这就是“TD 误差”信号。其公式为：\n$$\n\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)\n$$\n其中：\n- $r_t$ 是从状态 $s_t$ 转移时收到的即时奖励。\n- $\\gamma$ 是折扣因子，决定了未来奖励的现值。\n- $V(s_{t+1})$ 是后续状态 $s_{t+1}$ 的估计价值。\n- $V(s_t)$ 是当前状态 $s_t$ 的估计价值。\n\n项 $r_t + \\gamma V(s_{t+1})$ 被称为 TD 目标。它代表了基于该转移的结果，对状态 $s_t$ 价值的一个更新后的、更准确的估计。因此，RPE $\\delta_t$ 是这个新估计（目标）与旧估计 $V(s_t)$ 之间的误差。\n\n问题提供了以下数值：\n- $V(s_t) = 0.5$\n- $V(s_{t+1}) = 0.6$\n- $r_t = 1$\n- $\\gamma = 0.9$\n\n将这些值代入 RPE 公式：\n$$\n\\delta_t = 1 + (0.9)(0.6) - 0.5\n$$\n我们首先计算折扣因子与下一状态价值的乘积：\n$$\n(0.9)(0.6) = 0.54\n$$\n现在，我们将此结果代回主方程：\n$$\n\\delta_t = 1 + 0.54 - 0.5\n$$\n$$\n\\delta_t = 1.54 - 0.5\n$$\n$$\n\\delta_t = 1.04\n$$\n计算出的奖励预测误差为 $\\delta_t = 1.04$。\n\n接下来，我们必须结合多巴胺能神经元放电的背景来解释 $\\delta_t$ 的符号。多巴胺奖励预测误差假说提出，中脑多巴胺能神经元的相位性活动信号传递了 RPE。\n- 正的 RPE（$\\delta_t  0$）表示结果好于预期。实际奖励（$r_t$）与下一状态的折扣价值（$\\gamma V(s_{t+1})$）之和大于在状态 $s_t$ 时预测的价值。这种“正向惊喜”导致多巴胺能神经元的放电频率在其基线紧张性活动之上出现短暂的、相位性的*增加*。\n- 负的 RPE（$\\delta_t  0$）表示结果差于预期，导致放电频率短暂*减少*（暂停或下降）到基线以下。\n- 零 RPE（$\\delta_t = 0$）表示结果完全符合预期，导致基线放电频率没有变化。\n\n在本例中，$\\delta_t = 1.04$，是一个正值。因此，结果显著好于预测。智能体从状态 $s_t$ 预期的总未来回报为 $V(s_t) = 0.5$，但它收到了一个即时奖励 $r_t = 1$，并转移到了一个折扣价值为 $\\gamma V(s_{t+1}) = 0.54$ 的状态，总“实际”回报为 $1.54$。这个 $1.04$ 的正向差异构成了一个正向惊喜。根据该假说，这将导致腹侧被盖区的多巴胺能神经元放电短暂地增加到其基线水平之上，表明一个出乎意料的好结果。", "answer": "$$\n\\boxed{1.04}\n$$", "id": "5058200"}, {"introduction": "现在我们已经能够计算 RPE，让我们将其应用于一个著名的实验现象：“阻断效应”（blocking effect）。这个练习展示了 RPE 假说的强大预测能力，解释了为什么当一个新线索没有提供关于即将到来奖励的额外信息时，它就无法获得价值。理解这一点是掌握 RPE 如何驱动新学习的关键。[@problem_id:5058233]", "problem": "一项经典的巴甫洛夫条件反射实验分两个阶段进行，实验使用了在多巴胺能神经元信号传导研究中标准的线索和奖励。在阶段 $1$ 中，单个线索 $A$ 与大小为 $R_1$（无量纲归一化单位）的奖励重复配对，直到训练达到渐近状态。根据经验和理论，在渐近状态下，为线索 $A$ 编码的期望值等于所给予的奖励，即 $V(A)=R_1$。在阶段 $2$ 中，复合线索 $AB$ 在两种不同的条件下呈现。在条件 $B_{\\text{lock}}$（阻断）下，当呈现 $AB$ 时，给予的奖励仍为 $R_1$。在条件 $U_{\\text{nblock}}$（非阻断）下，当呈现 $AB$ 时，给予的奖励增加到 $R_2R_1$。假设复合线索的期望值是其关联强度的加和，因此 $V(AB)=V(A)+V(B)$，并且由于新线索 $B$ 从未与奖励配对过，因此在阶段 $2$ 开始时其初始值为 $V(B)=0$。设给予的奖励为 $R_1=1.0$ 和 $R_2=1.4$（无量纲归一化单位）。\n\n多巴胺能神经元编码奖励预测误差，该误差定义为在给予奖励时，获得的奖励与期望的奖励之间的差值。以此为基础，通过第一性原理计算奖励预测误差，从而推导出在条件 $B_{\\text{lock}}$ 和条件 $U_{\\text{nblock}}$ 下，给予奖励时刻的预测多巴胺信号。将你的最终答案以行矩阵的形式报告，该矩阵包含分别对应于条件 $B_{\\text{lock}}$ 和 $U_{\\text{nblock}}$ 的奖励预测误差的两个实数（无量纲归一化单位）。无需四舍五入。", "solution": "该问题要求计算在“阻断”（blocking）和“非阻断”（unblocking）两种条件下，奖励送达时刻的奖励预测误差（RPE）。RPE被定义为实际获得的奖励与预期奖励之间的差值。\n\n### 步骤 1：确定阶段 2 开始时的预期价值\n\n在阶段 1 结束时，动物已经完全学会了线索 $A$ 预测奖励 $R_1$。因此，线索 $A$ 的价值（期望值）为：\n$V(A) = R_1 = 1.0$\n\n线索 $B$ 是一个新线索，之前没有与任何奖励配对过，所以它的初始价值为：\n$V(B) = 0$\n\n问题假设复合线索的价值是其各组成部分价值的加和。因此，在阶段 2 首次呈现复合线索 $AB$ 时，其总预期价值为：\n$V(AB) = V(A) + V(B) = 1.0 + 0 = 1.0$\n\n### 步骤 2：计算条件 $B_{\\text{lock}}$（阻断）下的 RPE\n\n在阻断条件下，复合线索 $AB$ 之后给予的奖励是 $R_1 = 1.0$。\n奖励预测误差 $\\delta$ 是实际奖励减去预期奖励：\n$$\n\\delta_{B_{\\text{lock}}} = R_1 - V(AB) = 1.0 - 1.0 = 0\n$$\n由于 RPE 为零，没有“意外”发生。因此，没有教学信号来更新线索 $B$ 的价值。线索 $A$ 的预测能力“阻断”了线索 $B$ 的学习。\n\n### 步骤 3：计算条件 $U_{\\text{nblock}}$（非阻断）下的 RPE\n\n在非阻断条件下，复合线索 $AB$ 之后给予的奖励是 $R_2 = 1.4$。\n奖励预测误差 $\\delta$ 是实际奖励减去预期奖励：\n$$\n\\delta_{U_{\\text{nblock}}} = R_2 - V(AB) = 1.4 - 1.0 = 0.4\n$$\n这里产生了一个正的 RPE（$\\delta = 0.4$），因为实际奖励比预期的要好。这个正的教学信号将驱动学习，增加与复合线索 $AB$ 相关联的突触权重，从而导致新线索 $B$ 获得价值。\n\n### 步骤 4：整合结果\n\n将计算出的两个 RPE 值按要求格式化为行矩阵：\n- 条件 $B_{\\text{lock}}$ 的 RPE: $0$\n- 条件 $U_{\\text{nblock}}$ 的 RPE: $0.4$\n\n因此，最终答案是 $\\begin{pmatrix} 0  0.4 \\end{pmatrix}$。", "answer": "$$\n\\boxed{\\begin{pmatrix} 0  0.4 \\end{pmatrix}}\n$$", "id": "5058233"}, {"introduction": "最后，我们将把抽象的 RPE 信号与其生物学后果联系起来：改变突触的强度。这个练习将探讨“三因子”学习法则，其中 RPE 作为一个全局性的教学信号，来修改近期活跃的神经连接。这项练习弥合了计算信号与大脑中记忆和学习的物理基础之间的鸿沟。[@problem_id:5058243]", "problem": "在一个经典条件反射范式中，中脑的多巴胺能神经元广播一个奖励预测误差（RPE）信号，记为 $\\delta_t$，该信号报告了接收到的奖励与预期奖励之间的瞬时差异。一个投射到中型多棘神经元上的皮质纹状体突触维持着一个资格迹，记为 $e_t$，它反映了近期的突触前和突触后活动，并随时间衰减。经验和理论研究支持一种三因子学习法则：在时间 $t$ 的突触权重变化，记为 $\\Delta w$，取决于一个局部资格因子、一个全局调制性教学信号以及一个学习率。假设以下基本原则：\n- 如果资格迹或教学信号任一为零，则该时刻不存在可塑性。\n- 在工作点附近，突触更新可以由资格迹和教学信号之间的最低阶（线性）相互作用来建模。\n- 一个正的学习率 $\\alpha$ 决定了变化的总体幅度。\n\n给定在特定时间 $t$，资格迹为 $e_t = 0.4$，奖励预测误差为 $\\delta_t = 0.2$，学习率为 $\\alpha = 0.05$（所有量均为无单位量），请完成以下任务：\n1. 根据上述原则，写出与三因子法则一致的 $\\Delta w$ 的最简线性表达式。\n2. 使用所提供的值，计算 $\\Delta w$ 的数值。\n3. 根据 $\\Delta w$ 的符号，说明此时突触应发生增强还是抑制。\n\n将 $\\Delta w$ 的数值作为你的最终答案。最终答案以精确小数形式表示（无需四舍五入），并注意突触权重是无单位的。请将你的推理和解释与最终数值答案分开提供。", "solution": "问题陈述已经过验证，被认为是合理的。它在科学上基于计算神经科学的原理，特别是应用于突触可塑性的强化学习理论。问题提法清晰、客观，并包含了推导出唯一无矛盾解所需的所有信息。\n\n该问题要求三个部分：学习法则的推导、突触权重变化 $\\Delta w$ 的计算，以及对这一变化的解释。\n\n1.  **突触更新法则的构建**\n\n任务是找出与所描述的三因子学习法则一致的、关于突触权重变化 $\\Delta w$ 的最简数学表达式。这三个因子是：\n-   学习率, $\\alpha$。\n-   资格迹, $e_t$。\n-   教学信号，即奖励预测误差（RPE）, $\\delta_t$。\n\n问题提供了三个基本原则来指导构建过程：\n-   原则1：如果资格迹（$e_t$）或教学信号（$\\delta_t$）任一为零，则不存在可塑性。用数学语言表述，即如果 $e_t = 0$ 或 $\\delta_t = 0$，则 $\\Delta w = 0$。\n-   原则2：更新法则由资格迹 $e_t$ 和教学信号 $\\delta_t$ 之间的“最低阶（线性）相互作用”来建模。在一个关于两个变量 $f(x, y)$ 在 $(0,0)$ 点的泰勒展开中，最低阶相互作用项的形式为 $c \\cdot x \\cdot y$。更高阶的相互作用将是诸如 $x^2 y$、$x y^2$ 之类的项。因此，$e_t$ 和 $\\delta_t$ 之间的相互作用意味着一个与它们的乘积 $e_t \\delta_t$ 成比例的项。这种乘积形式也满足原则1，因为如果 $e_t$ 或 $\\delta_t$ 中任一为零，该表达式就为零。而加法形式，如 $A e_t + B \\delta_t$，则无法满足原则1（除非系数 $A$ 和 $B$ 为零，而另一个变量为任意非零值）。\n-   原则3：一个正的学习率 $\\alpha$ 决定了变化的总体幅度。这意味着 $\\alpha$ 作为一个比例常数起作用。\n\n结合这些原则，突触权重变化 $\\Delta w$ 与学习率、资格迹和奖励预测误差的乘积成正比。满足所有三个条件的最简表达式是：\n$$\n\\Delta w = \\alpha \\cdot e_t \\cdot \\delta_t\n$$\n\n这个方程是基底神经节强化学习模型的基石。\n\n2.  **$\\Delta w$ 数值的计算**\n\n问题提供了在特定时间 $t$ 的以下数值：\n-   学习率: $\\alpha = 0.05$\n-   资格迹: $e_t = 0.4$\n-   奖励预测误差: $\\delta_t = 0.2$\n\n将这些值代入推导出的 $\\Delta w$ 方程中：\n$$\n\\Delta w = (0.05) \\cdot (0.4) \\cdot (0.2)\n$$\n\n计算过程如下：\n$$\n\\Delta w = (0.020) \\cdot (0.2)\n$$\n$$\n\\Delta w = 0.004\n$$\n所有量都给定为无单位的，因此得到的突触权重变化也是无单位的。\n\n3.  **$\\Delta w$ 符号的解释**\n\n$\\Delta w$ 项代表突触强度（或权重）的变化。$\\Delta w$ 的符号决定了可塑性的方向。\n-   如果 $\\Delta w  0$，突触权重增加。这种突触的强化被称为**长时程增强（LTP）**。\n-   如果 $\\Delta w  0$，突触权重减少。这种突触的弱化被称为**长时程抑制（LTD）**。\n-   如果 $\\Delta w = 0$，突触权重没有变化。\n\n在本例中，计算出的值为 $\\Delta w = 0.004$，是一个正数。因此，该突触应该发生增强。\n\n这一结果与强化学习理论一致。正的奖励预测误差（$\\delta_t = 0.2  0$）表明结果好于预期。学习法则的作用是加强最近活跃的突触（由非零的资格迹 $e_t=0.4$ 表示），以使得未来更有可能出现导致这一积极结果的行为。", "answer": "$$\n\\boxed{0.004}\n$$", "id": "5058243"}]}