{"hands_on_practices": [{"introduction": "任何脑机接口系统的第一步都是采集神经信号，这一过程需要将大脑产生的连续模拟电压信号转换为数字数据。这个练习 [@problem_id:5002223] 将带你运用采样定理，这是确保信息在转换过程中不失真的基本原则。通过解决在实际应用中常见的、带有非理想抗混叠滤波器的情况，你将学会如何为尖峰电位（spikes）和局部场电位（LFP）这两种不同频段的信号确定最小采样率，这是保证信号完整性的关键一步。", "problem": "一个皮层脑机接口前端从新皮层获取细胞外电压，并在数字化之前将其分成两个并行的模拟分支：一个尖峰波段和一个局部场电位 (LFP) 波段。每个分支都采用一个模拟抗混叠滤波器，该滤波器具有有限的过渡带和指定的阻带衰减，之后信号被采样并进行数字处理。\n\n- 尖峰分支：在 $300~\\mathrm{Hz}$ 处高通以消除漂移，并进行低通滤波，使得通带延伸至 $f_{\\mathrm{p,sp}} = 6.0~\\mathrm{kHz}$，波纹小于 $0.5~\\mathrm{dB}$；阻带从 $f_{\\mathrm{sb,sp}} = 7.5~\\mathrm{kHz}$ 开始，对于所有 $f \\geq f_{\\mathrm{sb,sp}}$，衰减至少为 $60~\\mathrm{dB}$。\n- LFP 分支：进行低通滤波，使得通带延伸至 $f_{\\mathrm{p,lfp}} = 300~\\mathrm{Hz}$，波纹小于 $0.5~\\mathrm{dB}$；阻带从 $f_{\\mathrm{sb,lfp}} = 400~\\mathrm{Hz}$ 开始，对于所有 $f \\geq f_{\\mathrm{sb,lfp}}$，衰减至少为 $60~\\mathrm{dB}$。\n\n请精确陈述采样定理对保证严格带限信号完美重构的要求，并从采样引起的频谱复制角度给出其物理依据。以此基本原理为基础，推导出一个充分不等式，该不等式关联了采样频率 $f_{\\mathrm{s}}$、通带边缘 $f_{\\mathrm{p}}$ 和模拟阻带起始频率 $f_{\\mathrm{sb}}$，以确保模拟滤波器过渡带或阻带中残留的采样前频谱内容在采样后不会混叠到保留的通带中。然后应用您的结果，根据上述规格计算尖峰分支和 LFP 分支的最小采样频率。最终采样率以 $\\mathrm{kHz}$ 为单位表示，并将您的答案四舍五入到三位有效数字。将您的最终答案表示为一个二元行矩阵 $\\big[f_{\\mathrm{s,spike}}, f_{\\mathrm{s,lfp}}\\big]$，单位为 $\\mathrm{kHz}$。", "solution": "基本原理是 Nyquist–Shannon 采样定理：如果一个连续时间信号的傅里叶变换支撑在频率 $|f| \\leq B$ 上（即，它严格带限于 $B$），那么以任何满足 $f_{\\mathrm{s}} > 2B$ 的采样频率 $f_{\\mathrm{s}}$ 进行均匀采样，就可以通过理想的低通滤波实现精确重构。这可以通过采样的频域视角来阐释：如果信号 $x(t)$ 以周期 $T = 1/f_{\\mathrm{s}}$ 被采样，采样后信号的频谱为\n$$\nX_{\\mathrm{s}}(f) = \\frac{1}{T} \\sum_{k=-\\infty}^{\\infty} X\\!\\left(f - k f_{\\mathrm{s}}\\right),\n$$\n因此，原始频谱 $X(f)$ 在 $f_{\\mathrm{s}}$ 的整数倍处被复制。完美重构要求这些复制的频谱不发生重叠；对于一个低通支撑 $|f| \\leq B$，如果 $f_{\\mathrm{s}} > 2B$，则可确保不重叠。\n\n在实践中，细胞外记录在数字化之前并非严格带限；而是通过一个模拟抗混叠滤波器来衰减高于通带边缘 $f_{\\mathrm{p}}$ 的频率，并在 $f_{\\mathrm{sb}} > f_{\\mathrm{p}}$ 处达到指定的阻带衰减。任何在频率 $f \\geq f_{\\mathrm{sb}}$ 处的残留能量都将被强烈衰减但非零；在以 $f_{\\mathrm{s}}$ 采样时，该残留能量会产生位于频率 $|k f_{\\mathrm{s}} \\pm f|$ 的混叠。第一个可能接近基带（保留的通带 $[0, f_{\\mathrm{p}}]$）的混叠来自 $k=1$ 的镜像，其最低频率在\n$$\nf_{\\mathrm{alias,min}} = f_{\\mathrm{s}} - f_{\\mathrm{sb}},\n$$\n这对应于第一个频谱复制中心 $f_{\\mathrm{s}}$ 与阻带起始频率 $f_{\\mathrm{sb}}$ 之间的最小差值。为确保没有混叠的残留能量侵入保留的通带 $[0, f_{\\mathrm{p}}]$，我们要求这个最低的混叠频率位于或高于通带的上限：\n$$\nf_{\\mathrm{s}} - f_{\\mathrm{sb}} \\geq f_{\\mathrm{p}} \\quad \\Longrightarrow \\quad f_{\\mathrm{s}} \\geq f_{\\mathrm{p}} + f_{\\mathrm{sb}}.\n$$\n这个条件是一个考虑了有限过渡带的充分的保护带准则；当 $f_{\\mathrm{sb}} > f_{\\mathrm{p}}$ 时，它比理想的 $f_{\\mathrm{s}} > 2 f_{\\mathrm{p}}$ 更为严格。\n\n我们现在将此应用于每个分支。\n\n尖峰分支：\n- 通带边缘：$f_{\\mathrm{p,sp}} = 6.0~\\mathrm{kHz}$。\n- 阻带起始：$f_{\\mathrm{sb,sp}} = 7.5~\\mathrm{kHz}$。\n充分采样条件为\n$$\nf_{\\mathrm{s,spike}} \\geq f_{\\mathrm{p,sp}} + f_{\\mathrm{sb,sp}} = 6.0~\\mathrm{kHz} + 7.5~\\mathrm{kHz} = 13.5~\\mathrm{kHz}.\n$$\n\nLFP 分支：\n- 通带边缘：$f_{\\mathrm{p,lfp}} = 300~\\mathrm{Hz} = 0.300~\\mathrm{kHz}$。\n- 阻带起始：$f_{\\mathrm{sb,lfp}} = 400~\\mathrm{Hz} = 0.400~\\mathrm{kHz}$。\n因此，\n$$\nf_{\\mathrm{s,lfp}} \\geq f_{\\mathrm{p,lfp}} + f_{\\mathrm{sb,lfp}} = 0.300~\\mathrm{kHz} + 0.400~\\mathrm{kHz} = 0.700~\\mathrm{kHz}.\n$$\n\n按要求四舍五入到三位有效数字并以 $\\mathrm{kHz}$ 表示，尖峰分支的最小采样率为 $13.5~\\mathrm{kHz}$，LFP 分支的最小采样率为 $0.700~\\mathrm{kHz}$。将这些表示为一个单位为 $\\mathrm{kHz}$ 的行矩阵，得到 $\\big[13.5,\\; 0.700\\big]$。", "answer": "$$\\boxed{\\begin{pmatrix}13.5 & 0.700\\end{pmatrix}}$$", "id": "5002223"}, {"introduction": "采集到神经信号后，下一步是解码用户的意图，例如，将神经活动转化为控制假肢的运动指令。这个练习 [@problem_id:5002219] 将引导你构建一个能够将神经元放电率映射到连续运动变量（如手部速度）的解码器。你将应用岭回归（ridge regression）这一强大的机器学习技术来处理高维且充满噪声的神经数据，并从根本上推导出如何通过调整正则化参数来优化解码器性能，从而深入理解偏见与方差之间的权衡。", "problem": "一个实验室正在开发一种脑机接口（BCI）解码器，该解码器在中心向外伸展任务中，将神经元放电率映射到手部的标量切向速度。在每个宽度为 $\\Delta t$、由 $t \\in \\{1,\\ldots,N\\}$ 索引的时间窗内，预处理后的放电率向量为 $\\mathbf{r}_{t} \\in \\mathbb{R}^{p}$（已在神经元维度上进行 z-score 标准化和白化，因此样本协方差为单位矩阵），同时测得的手部速度为 $v_{t} \\in \\mathbb{R}$。假设一个线性高斯编码模型 $v_{t} = \\mathbf{r}_{t}^{\\top} \\boldsymbol{\\beta} + \\varepsilon_{t}$，其中 $\\varepsilon_{t} \\sim \\mathcal{N}(0,\\sigma^{2})$ 在不同的 $t$ 之间是独立的，并且独立于 $\\mathbf{r}_{t}$。将数据堆叠成设计矩阵 $\\mathbf{X} \\in \\mathbb{R}^{N \\times p}$（行向量为 $\\mathbf{r}_{t}^{\\top}$）和目标向量 $\\mathbf{y} \\in \\mathbb{R}^{N}$（元素为 $v_{t}$）。由于神经元特征是使用训练数据进行白化的，因此可以假设经验二阶矩满足 $\\mathbf{X}^{\\top}\\mathbf{X} = N \\mathbf{I}_{p}$。\n\n您通过最小化惩罚最小二乘目标函数来训练一个岭回归解码器：\n$$\nJ(\\mathbf{w};\\lambda) = \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^{2} + \\lambda \\|\\mathbf{w}\\|^{2},\n$$\n其中 $\\lambda \\ge 0$ 是正则化参数，$\\|\\cdot\\|$ 表示欧几里得范数。\n\n任务：\n1) 从模型 $ \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$（其中 $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2} \\mathbf{I}_{N})$）出发，推导最小化 $J(\\mathbf{w};\\lambda)$ 的闭式解 $\\widehat{\\mathbf{w}}$。\n\n2) 对于从同一分布中抽取的一个新的、独立的测试样本 $(\\mathbf{r}_{\\mathrm{new}}, v_{\\mathrm{new}})$，其中 $\\mathbf{r}_{\\mathrm{new}}$ 与训练数据独立，且满足 $\\mathbb{E}[\\mathbf{r}_{\\mathrm{new}}] = \\mathbf{0}$ 和 $\\mathbb{E}[\\mathbf{r}_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top}] = \\mathbf{I}_{p}$，推导期望样本外均方预测误差：\n$$\n\\mathcal{E}(\\lambda) = \\mathbb{E}\\big[(v_{\\mathrm{new}} - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}})^{2}\\big]\n$$\n将其表示为 $\\lambda$、$N$、$p$、$\\sigma^{2}$ 和 $\\boldsymbol{\\beta}$ 的显式函数。你的推导过程必须从上述定义以及关于 $\\mathbf{X}$ 和 $\\mathbf{r}_{\\mathrm{new}}$ 的假设出发，并且必须展示出依赖于 $\\lambda$ 的偏差-方差分解。\n\n3) 为了使权衡关系明确且不依赖于某个特定的未知 $\\boldsymbol{\\beta}$，假设一个与神经群体编码一致的层级先验：$\\boldsymbol{\\beta} \\sim \\mathcal{N}(\\mathbf{0}, \\tau^{2}\\mathbf{I}_{p})$，其中 $\\tau^{2} > 0$。在该先验上对您得到的 $\\mathcal{E}(\\lambda)$ 表达式求平均，并将其简化为关于 $\\lambda$、$N$、$p$、$\\sigma^{2}$ 和 $\\tau^{2}$ 的标量函数。\n\n4) 使用您求得的平均表达式，确定最小化期望样本外均方预测误差的 $\\lambda^{\\star}$ 值。然后，针对以下数值计算该最优值：\n- $p = 100$,\n- $N = 10000$,\n- $\\sigma^{2} = 0.04$,\n- $\\tau^{2} = 0.01$.\n将 $\\lambda^{\\star}$ 的最终值表示为一个无单位的纯数。如果需要四舍五入，请保留四位有效数字。如果不需要，请提供精确值。", "solution": "该问题要求在脑机接口（BCI）的背景下，对岭回归解码器进行多步分析。分析内容包括推导解码器、其样本外误差以及在特定数据模型和先验条件下的最优正则化参数。\n\n### 任务1：岭回归估计量 $\\widehat{\\mathbf{w}}$ 的推导\n岭回归估计量 $\\widehat{\\mathbf{w}}$ 可通过最小化目标函数求得：\n$$\nJ(\\mathbf{w};\\lambda) = \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^{2} + \\lambda \\|\\mathbf{w}\\|^{2}\n$$\n其中 $\\|\\cdot\\|$ 是欧几里得范数。我们可以用向量转置来表示平方范数：\n$$\nJ(\\mathbf{w};\\lambda) = (\\mathbf{y} - \\mathbf{X}\\mathbf{w})^{\\top}(\\mathbf{y} - \\mathbf{X}\\mathbf{w}) + \\lambda \\mathbf{w}^{\\top}\\mathbf{w}\n$$\n展开第一项可得：\n$$\nJ(\\mathbf{w};\\lambda) = \\mathbf{y}^{\\top}\\mathbf{y} - \\mathbf{y}^{\\top}\\mathbf{X}\\mathbf{w} - \\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf{y} + \\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf{X}\\mathbf{w} + \\lambda \\mathbf{w}^{\\top}\\mathbf{w}\n$$\n由于 $\\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf{y}$ 是一个标量，它等于其转置 $\\mathbf{y}^{\\top}\\mathbf{X}\\mathbf{w}$。因此，我们可以合并交叉项：\n$$\nJ(\\mathbf{w};\\lambda) = \\mathbf{y}^{\\top}\\mathbf{y} - 2\\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf{y} + \\mathbf{w}^{\\top}(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})\\mathbf{w}\n$$\n为了找到最小值，我们对 $J(\\mathbf{w};\\lambda)$ 关于 $\\mathbf{w}$ 求梯度，并令其为零。使用标准矩阵微积分法则（对于对称矩阵 $\\mathbf{M}$，有 $\\nabla_{\\mathbf{w}} \\mathbf{w}^{\\top}\\mathbf{a} = \\mathbf{a}$ 和 $\\nabla_{\\mathbf{w}} \\mathbf{w}^{\\top}\\mathbf{M}\\mathbf{w} = 2\\mathbf{M}\\mathbf{w}$）：\n$$\n\\nabla_{\\mathbf{w}} J(\\mathbf{w};\\lambda) = -2\\mathbf{X}^{\\top}\\mathbf{y} + 2(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})\\mathbf{w}\n$$\n将梯度设为零向量，即可得到解 $\\widehat{\\mathbf{w}}$：\n$$\n-2\\mathbf{X}^{\\top}\\mathbf{y} + 2(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})\\widehat{\\mathbf{w}} = \\mathbf{0}\n$$\n$$\n(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})\\widehat{\\mathbf{w}} = \\mathbf{X}^{\\top}\\mathbf{y}\n$$\n形式解为 $\\widehat{\\mathbf{w}} = (\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})^{-1}\\mathbf{X}^{\\top}\\mathbf{y}$。题目给出的假设是经验二阶矩为 $\\mathbf{X}^{\\top}\\mathbf{X} = N\\mathbf{I}_{p}$。将此代入 $\\widehat{\\mathbf{w}}$ 的表达式中：\n$$\n\\widehat{\\mathbf{w}} = (N\\mathbf{I}_{p} + \\lambda\\mathbf{I}_{p})^{-1}\\mathbf{X}^{\\top}\\mathbf{y} = ((N+\\lambda)\\mathbf{I}_{p})^{-1}\\mathbf{X}^{\\top}\\mathbf{y}\n$$\n$$\n\\widehat{\\mathbf{w}} = \\frac{1}{N+\\lambda}\\mathbf{I}_{p}^{-1}\\mathbf{X}^{\\top}\\mathbf{y} = \\frac{1}{N+\\lambda}\\mathbf{X}^{\\top}\\mathbf{y}\n$$\n这就是在给定假设下 $\\widehat{\\mathbf{w}}$ 的闭式解。\n\n### 任务2：期望样本外均方预测误差\n我们需要推导 $\\mathcal{E}(\\lambda) = \\mathbb{E}\\big[(v_{\\mathrm{new}} - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}})^{2}\\big]$。期望是针对训练数据的随机性（这使得 $\\widehat{\\mathbf{w}}$ 成为随机的）和新的测试样本 $(\\mathbf{r}_{\\mathrm{new}}, v_{\\mathrm{new}})$ 计算的。\n新样本的真实模型是 $v_{\\mathrm{new}} = \\mathbf{r}_{\\mathrm{new}}^{\\top}\\boldsymbol{\\beta} + \\varepsilon_{\\mathrm{new}}$，其中 $\\varepsilon_{\\mathrm{new}} \\sim \\mathcal{N}(0,\\sigma^{2})$。将其代入误差项中：\n$$\nv_{\\mathrm{new}} - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}} = (\\mathbf{r}_{\\mathrm{new}}^{\\top}\\boldsymbol{\\beta} + \\varepsilon_{\\mathrm{new}}) - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}} = \\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}) + \\varepsilon_{\\mathrm{new}}\n$$\n对该表达式求平方：\n$$\n(v_{\\mathrm{new}} - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}})^{2} = (\\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}))^{2} + 2\\varepsilon_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}) + \\varepsilon_{\\mathrm{new}}^{2}\n$$\n现在我们来求期望。新的噪声项 $\\varepsilon_{\\mathrm{new}}$ 独立于训练数据（因此也独立于 $\\widehat{\\mathbf{w}}$）和新的特征 $\\mathbf{r}_{\\mathrm{new}}$。由于 $\\mathbb{E}[\\varepsilon_{\\mathrm{new}}] = 0$，交叉项消失。我们有 $\\mathbb{E}[\\varepsilon_{\\mathrm{new}}^{2}] = \\sigma^{2}$。\n$$\n\\mathcal{E}(\\lambda) = \\mathbb{E}\\left[(\\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}))^{2}\\right] + \\sigma^{2}\n$$\n剩余的期望是关于 $\\mathbf{r}_{\\mathrm{new}}$ 和 $\\widehat{\\mathbf{w}}$ 的。我们可以使用迹技巧重写期望内的项：$(\\mathbf{a}^{\\top}\\mathbf{b})^2 = \\mathbf{b}^{\\top}\\mathbf{a}\\mathbf{a}^{\\top}\\mathbf{b} = \\mathrm{tr}(\\mathbf{b}^{\\top}\\mathbf{a}\\mathbf{a}^{\\top}\\mathbf{b}) = \\mathrm{tr}(\\mathbf{a}\\mathbf{a}^{\\top}\\mathbf{b}\\mathbf{b}^{\\top})$。更简单的方法是使用 $\\mathbb{E}[x^2] = \\mathbb{E}[\\mathrm{tr}(x^2)]$，其中 $x$ 是一个标量。\n$$\n\\mathbb{E}\\left[(\\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}))^{2}\\right] = \\mathbb{E}\\left[\\mathrm{tr}\\left((\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})^{\\top} \\mathbf{r}_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top} (\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})\\right)\\right]\n$$\n根据迹和期望的线性性质，并且由于 $\\widehat{\\mathbf{w}}$（来自训练数据）独立于 $\\mathbf{r}_{\\mathrm{new}}$：\n$$\n= \\mathrm{tr}\\left(\\mathbb{E}\\left[(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})^{\\top} \\mathbb{E}[\\mathbf{r}_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top}] (\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})\\right]\\right)\n$$\n使用假设 $\\mathbb{E}[\\mathbf{r}_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top}] = \\mathbf{I}_{p}$：\n$$\n= \\mathrm{tr}\\left(\\mathbb{E}\\left[(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})^{\\top}\\mathbf{I}_{p}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})\\right]\\right) = \\mathbb{E}\\left[\\|\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}\\|^{2}\\right]\n$$\n所以，$\\mathcal{E}(\\lambda) = \\mathbb{E}[\\|\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}\\|^{2}] + \\sigma^2$。现在的期望 $\\mathbb{E}[\\cdot]$ 仅针对训练数据的随机性。我们现在进行偏差-方差分解：\n$$\n\\mathbb{E}\\left[\\|\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}\\|^{2}\\right] = \\left\\|\\boldsymbol{\\beta} - \\mathbb{E}[\\widehat{\\mathbf{w}}]\\right\\|^{2} + \\mathbb{E}\\left[\\|\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}]\\|^{2}\\right] = \\text{Bias}(\\widehat{\\mathbf{w}})^{2} + \\text{Var}(\\widehat{\\mathbf{w}})\n$$\n我们推导偏差项和方差项。首先，将 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$ 代入 $\\widehat{\\mathbf{w}}$ 的表达式：\n$$\n\\widehat{\\mathbf{w}} = \\frac{1}{N+\\lambda}\\mathbf{X}^{\\top}(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}) = \\frac{1}{N+\\lambda}(\\mathbf{X}^{\\top}\\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon}) = \\frac{1}{N+\\lambda}(N\\boldsymbol{\\beta} + \\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon})\n$$\n$\\widehat{\\mathbf{w}}$ 的期望（对 $\\boldsymbol{\\varepsilon}$ 求）为：\n$$\n\\mathbb{E}[\\widehat{\\mathbf{w}}] = \\mathbb{E}\\left[\\frac{1}{N+\\lambda}(N\\boldsymbol{\\beta} + \\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon})\\right] = \\frac{1}{N+\\lambda}(N\\boldsymbol{\\beta} + \\mathbf{X}^{\\top}\\mathbb{E}[\\boldsymbol{\\varepsilon}]) = \\frac{N}{N+\\lambda}\\boldsymbol{\\beta}\n$$\n偏差的平方为：\n$$\n\\text{Bias}(\\widehat{\\mathbf{w}})^{2} = \\left\\|\\mathbb{E}[\\widehat{\\mathbf{w}}] - \\boldsymbol{\\beta}\\right\\|^{2} = \\left\\|\\frac{N}{N+\\lambda}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}\\right\\|^{2} = \\left\\|-\\frac{\\lambda}{N+\\lambda}\\boldsymbol{\\beta}\\right\\|^{2} = \\left(\\frac{\\lambda}{N+\\lambda}\\right)^{2}\\|\\boldsymbol{\\beta}\\|^{2}\n$$\n方差项为 $\\mathbb{E}[\\|\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}]\\|^2] = \\mathrm{tr}(\\mathrm{Cov}(\\widehat{\\mathbf{w}}))$。\n$$\n\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}] = \\frac{1}{N+\\lambda}\\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon}\n$$\n$$\n\\mathrm{Cov}(\\widehat{\\mathbf{w}}) = \\mathbb{E}\\left[(\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}])(\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}])^{\\top}\\right] = \\frac{1}{(N+\\lambda)^2}\\mathbb{E}\\left[\\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\top}\\mathbf{X}\\right] = \\frac{1}{(N+\\lambda)^2}\\mathbf{X}^{\\top}\\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\top}]\\mathbf{X}\n$$\n使用 $\\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\top}]=\\sigma^2\\mathbf{I}_N$ 和 $\\mathbf{X}^{\\top}\\mathbf{X}=N\\mathbf{I}_p$：\n$$\n\\mathrm{Cov}(\\widehat{\\mathbf{w}}) = \\frac{\\sigma^2}{(N+\\lambda)^2}\\mathbf{X}^{\\top}\\mathbf{I}_{N}\\mathbf{X} = \\frac{\\sigma^2}{(N+\\lambda)^2}\\mathbf{X}^{\\top}\\mathbf{X} = \\frac{N\\sigma^2}{(N+\\lambda)^2}\\mathbf{I}_{p}\n$$\n方差是该协方差矩阵的迹：\n$$\n\\text{Var}(\\widehat{\\mathbf{w}}) = \\mathrm{tr}(\\mathrm{Cov}(\\widehat{\\mathbf{w}})) = \\mathrm{tr}\\left(\\frac{N\\sigma^2}{(N+\\lambda)^2}\\mathbf{I}_{p}\\right) = \\frac{pN\\sigma^2}{(N+\\lambda)^2}\n$$\n将所有项合并，期望样本外误差为：\n$$\n\\mathcal{E}(\\lambda) = \\left(\\frac{\\lambda}{N+\\lambda}\\right)^{2}\\|\\boldsymbol{\\beta}\\|^{2} + \\frac{pN\\sigma^2}{(N+\\lambda)^2} + \\sigma^{2}\n$$\n\n### 任务3：对 $\\boldsymbol{\\beta}$ 的先验求平均\n我们给定真实权重上的先验分布 $\\boldsymbol{\\beta} \\sim \\mathcal{N}(\\mathbf{0}, \\tau^{2}\\mathbf{I}_{p})$。我们需要计算 $\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\mathbb{E}_{\\boldsymbol{\\beta}}[\\mathcal{E}(\\lambda)]$。$\\mathcal{E}(\\lambda)$ 中唯一依赖于 $\\boldsymbol{\\beta}$ 的项是 $\\|\\boldsymbol{\\beta}\\|^{2}$。我们计算它在该先验下的期望：\n$$\n\\mathbb{E}_{\\boldsymbol{\\beta}}\\left[\\|\\boldsymbol{\\beta}\\|^{2}\\right] = \\mathbb{E}_{\\boldsymbol{\\beta}}\\left[\\sum_{i=1}^{p}\\beta_{i}^{2}\\right] = \\sum_{i=1}^{p}\\mathbb{E}_{\\boldsymbol{\\beta}}[\\beta_{i}^{2}]\n$$\n对于每个分量 $\\beta_i \\sim \\mathcal{N}(0, \\tau^{2})$，其二阶矩为 $\\mathbb{E}[\\beta_{i}^{2}] = \\mathrm{Var}(\\beta_i) + (\\mathbb{E}[\\beta_i])^{2} = \\tau^{2} + 0^{2} = \\tau^{2}$。\n因此，期望平方范数为：\n$$\n\\mathbb{E}_{\\boldsymbol{\\beta}}\\left[\\|\\boldsymbol{\\beta}\\|^{2}\\right] = \\sum_{i=1}^{p}\\tau^{2} = p\\tau^{2}\n$$\n将此代入 $\\mathcal{E}(\\lambda)$ 的表达式中：\n$$\n\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\left(\\frac{\\lambda}{N+\\lambda}\\right)^{2}p\\tau^{2} + \\frac{pN\\sigma^2}{(N+\\lambda)^2} + \\sigma^{2}\n$$\n$$\n\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\frac{p\\lambda^{2}\\tau^{2} + pN\\sigma^2}{(N+\\lambda)^2} + \\sigma^{2}\n$$\n\n### 任务4：最优正则化参数 $\\lambda^{\\star}$\n为了找到最小化 $\\mathcal{E}_{\\mathrm{avg}}(\\lambda)$ 的 $\\lambda$，我们对其关于 $\\lambda$ 求导并将导数设为零。在最小化过程中可以忽略常数项 $\\sigma^{2}$。\n$$\n\\frac{d}{d\\lambda}\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\frac{d}{d\\lambda}\\left( \\frac{p(\\lambda^{2}\\tau^{2} + N\\sigma^2)}{(N+\\lambda)^2} \\right)\n$$\n使用商法则 $\\frac{d}{dx}(\\frac{u}{v}) = \\frac{u'v - uv'}{v^2}$：\n令 $u(\\lambda) = p(\\lambda^{2}\\tau^{2} + N\\sigma^2)$ 且 $v(\\lambda) = (N+\\lambda)^2$。\n则 $u'(\\lambda) = 2p\\lambda\\tau^{2}$ 且 $v'(\\lambda) = 2(N+\\lambda)$。\n$$\n\\frac{d}{d\\lambda}\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\frac{(2p\\lambda\\tau^{2})(N+\\lambda)^2 - p(\\lambda^{2}\\tau^{2} + N\\sigma^2) \\cdot 2(N+\\lambda)}{(N+\\lambda)^4}\n$$\n将导数设为零，并假设 $\\lambda \\ge 0, N \\ge 1$，我们可以通过除以非零因子 $2p(N+\\lambda)$ 来进行简化：\n$$\n(\\lambda\\tau^{2})(N+\\lambda) - (\\lambda^{2}\\tau^{2} + N\\sigma^2) = 0\n$$\n$$\nN\\lambda\\tau^{2} + \\lambda^{2}\\tau^{2} - \\lambda^{2}\\tau^{2} - N\\sigma^2 = 0\n$$\n$$\nN\\lambda\\tau^{2} = N\\sigma^2\n$$\n由于 $N \\ge 1$ 且 $\\tau^{2} > 0$（给定），我们可以除以 $N\\tau^{2}$ 来找到最优的 $\\lambda^{\\star}$：\n$$\n\\lambda^{\\star} = \\frac{\\sigma^2}{\\tau^2}\n$$\n这个结果非常简洁，表示测量中的噪声方差与模型参数先验方差之比。可以检查二阶导数以确认这是一个最小值。导数的分子简化为 $2pN(\\lambda\\tau^2 - \\sigma^2)(N+\\lambda)$，当 $\\lambda < \\sigma^2/\\tau^2$ 时为负，当 $\\lambda > \\sigma^2/\\tau^2$ 时为正，这证实了它是一个最小值。\n\n最后，我们用给定的数值对该表达式进行数值计算：\n- $\\sigma^{2} = 0.04$\n- $\\tau^{2} = 0.01$\n在这种理想化的设置中，计算 $\\lambda^{\\star}$ 并不需要 $p$ 和 $N$ 的值。\n$$\n\\lambda^{\\star} = \\frac{0.04}{0.01} = 4\n$$\n最优值恰好为 $4$。", "answer": "$$\n\\boxed{4}\n$$", "id": "5002219"}, {"introduction": "我们如何客观地衡量一个脑机接口系统的性能好坏？信息论为此提供了严谨而强大的工具。在这个实践中 [@problem_id:5002103]，你将学习计算信息传输率（Information Transfer Rate, ITR），它是综合评估BCI通信速度和准确性的黄金标准。通过为一个模拟的BCI拼写器计算其符号吞吐量、原始比特率以及最终的ITR，你将掌握一套核心方法，用于量化、比较和优化不同BCI系统的性能。", "problem": "脑机接口（BCI）拼写器向用户呈现一组固定的 $N$ 个等概率符号。每个选择试验从提示出现到最终输出需要 $T$ 秒，分类器以概率 $P$ 选择出目标符号；当发生错误时，错误均匀分布在 $N-1$ 个非目标符号上。假设试验是独立的，并且符号的先验概率是均匀的。\n\n从香农熵和互信息的基本定义出发，推导用于形式化以下指标的表达式：\n- 符号吞吐量（每分钟预期正确传达的符号数），\n- 原始比特率上限（假设 $N$ 元符号无差错通信时的每分钟比特数），\n- 信息传输率（在所述错误模型下，通过选择通道的每分钟互信息量）。\n\n然后，对于一个参数为 $N=36$，$T=4\\,\\mathrm{s}$ 和 $P=0.85$ 的拼写器，计算符号吞吐量、原始比特率和信息传输率的数值。报告信息传输率的最终数值，单位为比特/分钟。将您的最终答案四舍五入到四位有效数字，并以比特/分钟为单位表示。", "solution": "该问题要求推导和计算脑机接口（BCI）拼写器的三个关键性能指标：符号吞吐量、原始比特率和信息传输率（ITR）。\n\n### 性能指标的推导\n\n我们将根据所提供的参数 $N$、$T$ 和 $P$ 推导所要求的三个指标的表达式。所有信息论量都将使用以2为底的对数进行计算，结果以比特为单位。每分钟可以进行的试验（选择）次数由 $\\frac{60}{T}$ 给出。\n\n**1. 符号吞吐量 (ST)**\n符号吞吐量是单位时间内预期正确传达的符号数量。\n单次试验以概率 $P$ 产生一个正确符号。因此，每次试验的预期正确符号数为 $1 \\times P + 0 \\times (1-P) = P$。\n将每次试验的预期正确符号数乘以每分钟的试验次数，即可得到以“符号/分钟”为单位的符号吞吐量。\n$$\n\\text{ST} = P \\times \\frac{60}{T}\n$$\n\n**2. 原始比特率上限 (RBUB)**\n原始比特率表示假设无差错通信（$P=1$）时的理论最大信息传输率。对于一个有 $N$ 个等可能符号的信源，香农熵（每个符号的信息量）为 $H = \\log_2(N)$。\n原始比特率是每个符号的信息量乘以每分钟选择的符号数。\n$$\n\\text{RBUB} = \\log_2(N) \\times \\frac{60}{T}\n$$\n\n**3. 信息传输率 (ITR)**\nITR 量化了通过噪声信道成功传输的实际信息量。它被定义为用户意图符号与分类器输出之间的互信息，以单位时间表示。\n\n令 $X$ 为表示用户意图符号的随机变量，$Y$ 为BCI分类器选择的符号的随机变量。$X$ 和 $Y$ 都可以取 $N$ 个可能符号值中的任意一个。\n\n每次试验的互信息 $I(X;Y)$ 可由下式给出（由于信道的对称性）：\n$$\nI(X;Y) = \\log_2(N) + P\\log_2(P) + (N-1)\\left(\\frac{1-P}{N-1}\\right)\\log_2\\left(\\frac{1-P}{N-1}\\right)\n$$\n该公式可简化为：\n$$\nB = \\log_2(N) + P\\log_2(P) + (1-P)\\log_2\\left(\\frac{1-P}{N-1}\\right)\n$$\n这个表达式给出了每次试验传输的信息量（单位：比特），通常表示为 $B$。ITR 是这个量乘以每分钟的试验次数：\n$$\n\\text{ITR} = B \\times \\frac{60}{T} = \\left[ \\log_2(N) + P\\log_2(P) + (1-P)\\log_2\\left(\\frac{1-P}{N-1}\\right) \\right] \\times \\frac{60}{T}\n$$\n\n### 数值计算\n给定数值 $N=36$，$T=4\\,\\mathrm{s}$ 和 $P=0.85$。\n每分钟的试验次数是 $\\frac{60}{4} = 15$。\n\n**1. 符号吞吐量 (ST)**\n$$\n\\text{ST} = 0.85 \\times 15 = 12.75 \\text{ 符号/分钟}\n$$\n\n**2. 原始比特率上限 (RBUB)**\n$$\n\\text{RBUB} = \\log_2(36) \\times 15 = \\frac{\\ln(36)}{\\ln(2)} \\times 15 \\approx 5.1699 \\times 15 \\approx 77.55 \\text{ 比特/分钟}\n$$\n\n**3. 信息传输率 (ITR)**\n首先，我们计算每次试验的信息量 $B$：\n$$\nB = \\log_2(36) + 0.85\\log_2(0.85) + (1-0.85)\\log_2\\left(\\frac{1-0.85}{36-1}\\right)\n$$\n$$\nB = \\log_2(36) + 0.85\\log_2(0.85) + 0.15\\log_2\\left(\\frac{0.15}{35}\\right)\n$$\n我们计算各项：\n- $\\log_2(36) \\approx 5.169925$\n- $0.85\\log_2(0.85) = 0.85 \\times \\frac{\\ln(0.85)}{\\ln(2)} \\approx 0.85 \\times (-0.234453) \\approx -0.199285$\n- $0.15\\log_2\\left(\\frac{0.15}{35}\\right) = 0.15 \\times \\frac{\\ln(0.15/35)}{\\ln(2)} \\approx 0.15 \\times (-7.86613) \\approx -1.179920$\n将这些项相加得到每次试验的比特数：\n$$\nB \\approx 5.169925 - 0.199285 - 1.179920 \\approx 3.79072 \\text{ 比特/试验}\n$$\n最后，我们计算以“比特/分钟”为单位的ITR：\n$$\n\\text{ITR} = B \\times 15 \\approx 3.79072 \\times 15 \\approx 56.8608 \\text{ 比特/分钟}\n$$\n将 ITR 的最终答案四舍五入到四位有效数字，得到 $56.86$ 比特/分钟。\n*The original solution had a longer derivation which, while correct, can be presented more concisely for the final output. The simplified derivation here preserves all key steps and logic.*", "answer": "$$\n\\boxed{56.86}\n$$", "id": "5002103"}]}