{"hands_on_practices": [{"introduction": "动作选择不是一个静态过程；它通过经验不断塑造。这个学习过程的核心是奖励预测误差，一个在神经科学中与多巴胺神经元活动紧密相连的概念。本练习将探讨时间差分（TD）误差，这是强化学习中的一个关键概念，它量化了预期结果与实际结果之间的差异。通过在一个假设场景中计算TD误差 [@problem_id:5001000]，您将具体理解大脑如何生成指导未来选择所必需的“教学信号”。", "problem": "在基底核中，黑质致密部（SNc）的多巴胺能神经元的时相性活动被广泛建模为报告一种奖励预测误差，该误差用于训练纹状体回路以进行行为选择。考虑一个行动者-评论家（actor-critic）框架，其中评论家维持一个状态值函数 $V(s)$，并且学习由时间差分误差 $\\delta_t$ 驱动。从状态值函数的定义（即期望折扣回报）$V(s_t) = \\mathbb{E}\\left[\\sum_{k=0}^{\\infty} \\gamma^{k} r_{t+k} \\mid s_t\\right]$ 和贝尔曼一致性条件 $V(s_t) = \\mathbb{E}\\left[r_t + \\gamma V(s_{t+1}) \\mid s_t\\right]$ 出发，推导出样本时间差分误差 $\\delta_t$ 关于 $r_t$、$\\gamma$、$V(s_t)$ 和 $V(s_{t+1})$ 的表达式。然后，对于一次试验，其中 $r_t = 0$，$\\gamma = 0.9$，$V(s_t) = 0.5$ 且 $V(s_{t+1}) = 0.6$，计算 $\\delta_t$ 的数值。简要解释，根据将 $\\delta_t$ 与纹状体中的时相性多巴胺信号联系起来的标准解释，对于这次状态转移，多巴胺水平应相对于基线增加还是减少。\n\n仅报告 $\\delta_t$ 的数值，以纯数字形式（任意单位）。不包括单位，也不要四舍五入。", "solution": "首先对问题进行严格的验证过程。\n\n### 第1步：提取已知条件\n- 状态值函数的定义：$V(s_t) = \\mathbb{E}\\left[\\sum_{k=0}^{\\infty} \\gamma^{k} r_{t+k} \\mid s_t\\right]$\n- 贝尔曼一致性条件：$V(s_t) = \\mathbb{E}\\left[r_t + \\gamma V(s_{t+1}) \\mid s_t\\right]$\n- 时间差分误差用 $\\delta_t$ 表示。\n- 任务1：推导出样本时间差分误差 $\\delta_t$ 关于 $r_t$、$\\gamma$、$V(s_t)$ 和 $V(s_{t+1})$ 的表达式。\n- 任务2：对于单次试验，根据给定值计算 $\\delta_t$ 的数值：\n  - $r_t = 0$\n  - $\\gamma = 0.9$\n  - $V(s_t) = 0.5$\n  - $V(s_{t+1}) = 0.6$\n- 任务3：简要解释对时相性多巴胺信号的意义。\n\n### 第2步：使用提取的已知条件进行验证\n对问题进行有效性评估。\n- **科学依据**：该问题基于标准的时间差分（TD）学习框架，这是现代强化学习和计算神经科学的基石。状态值函数和贝尔曼方程的定义是正确的。时相性多巴胺活动报告奖励预测误差（$\\delta_t$）这一前提是神经生物学中一个核心、公认且得到经验支持的假说。该问题在科学上是合理的。\n- **提法明确**：问题陈述清晰。它要求进行一次标准推导，然后使用提供的完整数据进行数值计算。保证存在唯一、稳定且有意义的解。\n- **客观性**：该问题使用了领域内精确、标准的术语，没有主观或模糊的语言。\n\n### 第3步：结论与行动\n问题陈述是有效的。它具有科学依据，提法明确且客观。没有矛盾、信息缺失或违反基本原则的情况。将提供完整的解答。\n\n### 推导与计算\n推导时间差分（TD）误差的出发点是状态值函数 $V(s_t)$ 的贝尔曼一致性条件：\n$$V(s_t) = \\mathbb{E}\\left[r_t + \\gamma V(s_{t+1}) \\mid s_t\\right]$$\n该方程表明，当前状态 $s_t$ 的价值，是即时奖励 $r_t$ 与后续状态 $s_{t+1}$ 的折扣价值之和的期望。期望 $\\mathbb{E}[\\cdot]$ 是在给定当前状态 $s_t$ 的条件下，对所有可能的奖励和下一个状态计算的。\n\n在真实世界场景或模拟中，智能体经历的是单一轨迹，而不是对所有可能轨迹的期望。对于时间 $t$ 的一次特定转移，智能体从状态 $s_t$ 移动到 $s_{t+1}$ 并收到奖励 $r_t$。根据这个单一的样本，我们可以构建一个对 $V(s_t)$ 的改进或更新的估计。这个估计就是“TD目标”，它是通过取贝尔曼方程中期望内部的量，并应用于这个特定实例而形成的：\n$$ \\text{TD 目标} = r_t + \\gamma V(s_{t+1}) $$\n项 $V(s_{t+1})$ 是对下一状态价值的当前估计。TD目标将实际收到的奖励 $r_t$ 与这个现有估计相结合，从而提供了一个比旧值 $V(s_t)$ 更具信息量的对 $s_t$ 的估值。\n\n时间差分误差 $\\delta_t$ 被定义为这个新的、基于样本的估计（即TD目标）与旧的估计 $V(s_t)$ 之间的差值。\n$$ \\delta_t = (\\text{TD 目标}) - V(s_t) $$\n将TD目标的表达式代入，我们得到样本时间差分误差的标准公式：\n$$ \\delta_t = \\left( r_t + \\gamma V(s_{t+1}) \\right) - V(s_t) $$\n这个表达式表示了预测 $V(s_t)$ 与一个时间步之后实现的结果相比所产生的误差。\n\n题目为单次试验提供了以下值：\n- 奖励: $r_t = 0$\n- 折扣因子: $\\gamma = 0.9$\n- 当前状态的价值: $V(s_t) = 0.5$\n- 下一状态的价值: $V(s_{t+1}) = 0.6$\n\n我们将这些数值代入推导出的 $\\delta_t$ 表达式中：\n$$ \\delta_t = (0 + 0.9 \\times 0.6) - 0.5 $$\n首先，我们计算括号内的乘积：\n$$ 0.9 \\times 0.6 = 0.54 $$\n现在，我们将此结果代回 $\\delta_t$ 的方程：\n$$ \\delta_t = 0.54 - 0.5 $$\n$$ \\delta_t = 0.04 $$\n这次状态转移的时间差分误差的数值为 $0.04$。\n\n最后，我们探讨其与多巴胺信号的联系。奖励预测误差假说认为，多巴胺神经元放电的时相性变化编码了 $\\delta_t$。\n- 正向预测误差（$\\delta_t > 0$）意味着结果优于预期。这与多巴胺神经元放电的时相性增加（一阵爆发）相关联。\n- 负向预测误差（$\\delta_t  0$）意味着结果差于预期。这与多巴胺神经元放电在基线紧张性放电率之下的时相性减少（一次暂停）相关联。\n- 零预测误差（$\\delta_t = 0$）意味着结果完全符合预期，导致紧张性放电率没有变化。\n\n在本例中，$\\delta_t = 0.04$，是一个正值。这表明一个正向的奖励预测误差。尽管即时奖励为零（$r_t=0$），智能体转移到了一个价值更高（$V(s_{t+1}) = 0.6$）的状态，这超出了其初始状态价值（$V(s_t) = 0.5$）的完全预期。下一状态的折扣价值 $0.9 \\times 0.6 = 0.54$，大于初始价值 $0.5$。结果是“优于预期”。因此，这一事件应触发纹状体中多巴胺释放的时相性增加。", "answer": "$$\n\\boxed{0.04}\n$$", "id": "5001000"}, {"introduction": "基底节选择动作的能力依赖于一个包含多个具有相反效应通路的复杂回路。一个简化的基于发放率的模型可以帮助我们理解这个回路的核心逻辑。本练习聚焦于直接（“Go”）、间接（“NoGo”）和超直接（“Stop”）通路如何汇聚到苍白球内侧部（GPi）——基底节的主要输出核。通过在一个线性模型中操纵这些通路的活动 [@problem_id:5001161]，您将亲眼看到它们的平衡如何促进或抑制一个动作，为经典的基底节功能“推拉”模型提供一个定量的基础。", "problem": "基底核中的动作选择取决于汇聚到苍白球内侧部 (GPi) 的各通路活动的平衡。在一个标准的基于速率的模型中，GPi 的输出被建模为各通路活动的加权和，其中直接通路 ($D$) 减少 GPi 输出，而间接通路 ($I$) 和超直接通路 ($H$) 增加 GPi 输出。GPi 的输出由以下线性形式给出：$$g = g_{0} - w_{d} D + w_{i} I + w_{h} H$$, 其中 $g_{0}$ 是一个基线偏移量，$w_{d}$、$w_{i}$ 和 $w_{h}$ 是反映通路效能的正权重。超直接通路是皮质-丘脑底核(STN)-GPi通路，间接通路是纹状体-苍白球外侧部(GPe)-STN-GPi通路。直接通路是纹状体-GPi通路。假设一个短暂的神经调质事件导致通路活动按比例增加：直接通路增加 $20\\%$，超直接通路增加 $10\\%$，而间接通路保持不变。使用上述模型和数值 $$w_{d} = 2,\\quad w_{i} = 1.5,\\quad w_{h} = 1$$, 计算由这些变化引起的 GPi 输出的变化量 $$\\Delta g = g' - g$$，其中 $g'$ 是变化后的 GPi 输出，$g$ 是变化前的基线 GPi 输出。将您的最终答案表示为关于基线活动 $D$ 和 $H$ 的单一简化解析表达式。最终表达式中不需要进行数值舍入，也不需要单位。", "solution": "该问题是有效的。这是一个计算神经科学领域的适定问题，基于一个标准的基底核功能简化模型。它内容自洽、客观且科学合理。\n\n苍白球内侧部 (GPi) 的基线输出，记为 $g$，由以下线性模型给出：\n$$g = g_{0} - w_{d} D + w_{i} I + w_{h} H$$\n其中 $g_{0}$ 是一个基线偏移量，$D$、$I$ 和 $H$ 分别代表直接、间接和超直接通路的活动，$w_{d}$、$w_{i}$ 和 $w_{h}$ 是与这些通路相关的正权重。\n\n问题指定了以下权重值：\n$$w_{d} = 2$$\n$$w_{i} = 1.5$$\n$$w_{h} = 1$$\n\n一个神经调质事件引起了通路活动的短暂变化。设新的活动为 $D'$、$I'$ 和 $H'$。\n直接通路活动 $D$ 增加了 $20\\%$。因此，新的活动 $D'$ 为：\n$$D' = D + 0.20 D = 1.2 D$$\n超直接通路活动 $H$ 增加了 $10\\%$。新的活动 $H'$ 为：\n$$H' = H + 0.10 H = 1.1 H$$\n间接通路活动 $I$ 保持不变，所以：\n$$I' = I$$\n\n这些变化后的 GPi 输出，记为 $g'$，使用相同的模型但采用新的活动水平进行计算：\n$$g' = g_{0} - w_{d} D' + w_{i} I' + w_{h} H'$$\n将 $D'$、$I'$ 和 $H'$ 的表达式代入此方程，我们得到：\n$$g' = g_{0} - w_{d}(1.2 D) + w_{i}(I) + w_{h}(1.1 H)$$\n\n目标是计算 GPi 输出的变化量 $\\Delta g$，它被定义为新输出与基线输出之间的差值：\n$$\\Delta g = g' - g$$\n我们代入 $g'$ 和 $g$ 的完整表达式：\n$$\\Delta g = (g_{0} - 1.2 w_{d} D + w_{i} I + 1.1 w_{h} H) - (g_{0} - w_{d} D + w_{i} I + w_{h} H)$$\n\n为了简化，我们展开括号并合并同类项：\n$$\\Delta g = g_{0} - 1.2 w_{d} D + w_{i} I + 1.1 w_{h} H - g_{0} + w_{d} D - w_{i} I - w_{h} H$$\n基线偏移项 $g_{0}$ 被消掉了。间接通路的项 $w_{i} I$ 也被消掉了，因为其活动没有改变。\n$$\\Delta g = (-1.2 w_{d} D + w_{d} D) + (1.1 w_{h} H - w_{h} H)$$\n从各自的项中提取公因式 $w_{d} D$ 和 $w_{h} H$：\n$$\\Delta g = (1 - 1.2) w_{d} D + (1.1 - 1) w_{h} H$$\n$$\\Delta g = -0.2 w_{d} D + 0.1 w_{h} H$$\n\n这个关于 $\\Delta g$ 的一般表达式取决于基线活动 $D$ 和 $H$ 以及它们的权重。现在，我们代入给定的权重数值 $w_{d} = 2$ 和 $w_{h} = 1$：\n$$\\Delta g = -0.2(2)D + 0.1(1)H$$\n进行标量乘法运算，得到最终的简化表达式：\n$$\\Delta g = -0.4 D + 0.1 H$$\n这就是所要求的、用基线活动 $D$ 和 $H$ 表示的 GPi 输出变化量的解析表达式。", "answer": "$$\\boxed{-0.4 D + 0.1 H}$$", "id": "5001161"}, {"introduction": "一旦大脑学习了不同动作的价值，它如何从中选择一个呢？选择过程通常不是确定性的，而是概率性的，这允许在选择已知最佳选项（利用）和尝试新选项（探索）之间取得灵活的平衡。本练习引入了一个规范的优化原则，其目标是在最大化奖励的同时，也保持行为的多样性（用熵来量化）。推导出最终的softmax选择规则 [@problem_id:5001106]，将阐明大脑如何实现一个随机性的动作选择策略，以及一个可映射到神经调质状态的参数如何控制利用与探索之间的权衡。", "problem": "在脊椎动物的基底核中，期望动作价值的皮质-纹状体表征可以建模为主观动作价值 $Q_{i}$，而随机动作选择则源于受多巴胺调控的直接和间接通路之间的竞争。神经生物学和强化学习中一个广泛使用的规范性假设是，在决策时刻，系统会选择一个动作的概率分布 $p_{i}$，该分布在最大化期望主观价值 $\\sum_{i} p_{i} Q_{i}$ 与维持由香农熵 $H(p) = -\\sum_{i} p_{i} \\ln p_{i}$ 量化的行为变异性之间进行权衡。设价值最大化相对于熵的强度由逆温度参数 $\\beta$ 控制，因此较高的 $\\beta$ 使选择偏向于价值更高的动作，而较低的 $\\beta$ 则增加探索。\n\n从这一权衡原则出发，推导出一个选择规则，该规则给出了具有价值 $Q_{i}$ 和逆温度 $\\beta$ 的一组动作的 $p_{i}$。然后，将其应用于一个包含 $2$ 个动作、Q值为 $[1.0, 0.9]$ 且 $\\beta = 3$ 的案例。计算由此产生的动作概率，并将您的数值结果四舍五入到 $4$ 位有效数字。无需物理单位。最后，以基于此优化原则的神经生物学术语，解释降低 $\\beta$ 如何改变基底核驱动的动作选择中的探索行为。", "solution": "该问题要求基于最大化期望价值和最大化熵之间的权衡，推导出动作选择的概率选择规则，将此规则应用于一个特定案例，并解释逆温度参数 $\\beta$ 的神经生物学解释。\n\n首先，我们验证问题陈述的有效性。\n1.  **提取的已知条件**：\n    -   动作选择是基底核中直接和间接通路之间的竞争。\n    -   一组动作 $i$ 的动作价值用 $Q_i$ 表示。\n    -   选择是动作上的一个概率分布 $p_i$。\n    -   目标是选择 $p_i$，以在最大化期望价值 $\\sum_{i} p_{i} Q_{i}$ 和维持由香农熵 $H(p) = -\\sum_{i} p_{i} \\ln p_{i}$ 量化的行为变异性之间进行权衡。\n    -   逆温度参数 $\\beta$ 控制价值最大化相对于熵的强度。\n    -   任务1：推导 $p_i(Q_i, \\beta)$ 的选择规则。\n    -   任务2：将该规则应用于一个包含2个动作、Q值为 $[1.0, 0.9]$ 且 $\\beta=3$ 的案例。\n    -   任务3：计算概率，并四舍五入到4位有效数字。\n    -   任务4：从神经生物学角度解释降低 $\\beta$ 如何改变探索行为。\n\n2.  **验证**：\n    -   **科学依据**：该问题基于计算神经科学和强化学习的原理，这些原理通常用于建模决策和基底核功能。价值最大化（利用）和熵最大化（探索）之间的权衡是一个基本概念，通过softmax函数对其进行形式化是一个标准模型。\n    -   **适定性**：这个问题是适定的。它描述了一个存在唯一解的约束优化问题。所提供的数据足以进行推导和计算。\n    -   **客观性**：问题陈述使用了相关科学领域的精确、客观和标准的术语。\n\n3.  **结论**：问题有效。\n\n我们现在开始解答。\n\n**第1部分：选择规则的推导**\n\n其原理是找到一个概率分布 $\\{p_i\\}$，该分布能最大化一个代表期望价值和熵之间权衡的函数。问题指出，$\\beta$ 控制着价值最大化相对于熵的强度。这种权衡可以通过最大化以下泛函来形式化，在此背景下通常称为“自由能”：\n$F(p) = \\beta \\sum_{i} p_i Q_i + H(p) = \\beta \\sum_{i} p_i Q_i - \\sum_{i} p_i \\ln p_i$\n这个最大化过程必须在 $\\{p_i\\}$ 是一个概率分布的约束下进行，即 $\\sum_i p_i = 1$ 且对所有 $i$ 都有 $p_i \\ge 0$。非负性通常由解的形式来满足。我们使用拉格朗日乘数法来施加归一化约束。\n\n拉格朗日函数 $\\mathcal{L}$ 构建如下：\n$\\mathcal{L}(\\{p_i\\}, \\lambda) = \\left( \\beta \\sum_{i} p_i Q_i - \\sum_{i} p_i \\ln p_i \\right) - \\lambda \\left( \\sum_{i} p_i - 1 \\right)$\n其中 $\\lambda$ 是拉格朗日乘数。为了找到最优分布，我们对 $\\mathcal{L}$ 关于任意分量 $p_k$ 求偏导数，并令其为零。\n$$\\frac{\\partial \\mathcal{L}}{\\partial p_k} = \\frac{\\partial}{\\partial p_k} \\left( \\beta p_k Q_k - p_k \\ln p_k - \\lambda p_k \\right) = 0$$\n使用 $p_k \\ln p_k$ 的导数乘法法则，即 $\\frac{d}{dx}(x \\ln x) = \\ln x + 1$，我们得到：\n$$\\frac{\\partial \\mathcal{L}}{\\partial p_k} = \\beta Q_k - (\\ln p_k + 1) - \\lambda = 0$$\n现在，我们求解 $p_k$：\n$$\\ln p_k = \\beta Q_k - 1 - \\lambda$$\n$$p_k = \\exp(\\beta Q_k - 1 - \\lambda)$$\n我们可以分离指数项：\n$$p_k = \\exp(\\beta Q_k) \\exp(-1 - \\lambda)$$\n项 $\\exp(-1 - \\lambda)$ 是一个不依赖于动作索引 $k$ 的常数。我们将其表示为 $1/Z$。\n$$p_k = \\frac{\\exp(\\beta Q_k)}{Z}$$\n为了确定常数 $Z$，我们应用归一化约束 $\\sum_k p_k = 1$：\n$$\\sum_k p_k = \\sum_k \\frac{\\exp(\\beta Q_k)}{Z} = \\frac{1}{Z} \\sum_k \\exp(\\beta Q_k) = 1$$\n求解 $Z$，我们发现它是对所有动作的求和，通常称为配分函数：\n$$Z = \\sum_i \\exp(\\beta Q_i)$$\n将此代回 $p_k$ 的表达式，我们得到选择规则：\n$$p_k = \\frac{\\exp(\\beta Q_k)}{\\sum_i \\exp(\\beta Q_i)}$$\n这就是玻尔兹曼分布，在机器学习和计算神经科学的背景下通常被称为 softmax 函数。\n\n**第2部分：具体案例的应用**\n\n给定一个包含 $2$ 个动作的场景，我们称之为动作 $1$ 和动作 $2$。\nQ值为 $Q_1 = 1.0$ 和 $Q_2 = 0.9$。\n逆温度为 $\\beta = 3$。\n\n首先，我们计算归一化因子 $Z$：\n$$Z = \\sum_{i=1}^{2} \\exp(\\beta Q_i) = \\exp(\\beta Q_1) + \\exp(\\beta Q_2)$$\n$$Z = \\exp(3 \\times 1.0) + \\exp(3 \\times 0.9) = \\exp(3) + \\exp(2.7)$$\n\n现在我们计算每个动作的概率。\n对于动作 $1$：\n$$p_1 = \\frac{\\exp(\\beta Q_1)}{Z} = \\frac{\\exp(3)}{\\exp(3) + \\exp(2.7)}$$\n对于动作 $2$：\n$$p_2 = \\frac{\\exp(\\beta Q_2)}{Z} = \\frac{\\exp(2.7)}{\\exp(3) + \\exp(2.7)}$$\n为了计算数值：\n$$p_1 = \\frac{1}{1 + \\exp(2.7 - 3)} = \\frac{1}{1 + \\exp(-0.3)}$$\n$$p_2 = \\frac{1}{\\exp(3 - 2.7) + 1} = \\frac{1}{\\exp(0.3) + 1}$$\n使用数值 $\\exp(-0.3) \\approx 0.740818$ 和 $\\exp(0.3) \\approx 1.349859$：\n$$p_1 \\approx \\frac{1}{1 + 0.740818} = \\frac{1}{1.740818} \\approx 0.574442$$\n$$p_2 \\approx \\frac{1}{1.349859 + 1} = \\frac{1}{2.349859} \\approx 0.425557$$\n将这些结果四舍五入到 $4$ 位有效数字，我们得到：\n$p_1 \\approx 0.5744$\n$p_2 \\approx 0.4256$\n作为检验，它们的和是 $0.5744 + 0.4256 = 1.0000$。\n\n**第3部分：神经生物学解释**\n\n逆温度参数 $\\beta$ 控制着动作选择的随机性。降低 $\\beta$ 会增加基底核驱动的动作选择中的探索行为。这可以通过考察推导出的选择规则 $p_i \\propto \\exp(\\beta Q_i)$ 的行为来理解。\n\n从神经生物学角度来看，动作价值 $Q_i$ 由皮质-纹状体突触的强度编码。动作选择过程涉及基底核的“Go”（直接）通路和“NoGo”（间接）通路之间的竞争，这一过程受到多巴胺的强烈调控。\n\n-   **高 $\\beta$** 值对应于一个低温、确定性的系统。在这种状态下，即使是Q值中的微小差异也会被指数函数放大。具有最高Q值的动作被选择的概率将不成比例地高。这对应于一种**利用**状态，此时系统会可靠地选择它已学会的最有回报的动作。这被认为与高水平的紧张性多巴胺有关，后者会增强最有价值动作的信号，并有效地使其在竞争中更有利。\n\n-   **低 $\\beta$** 值（趋近于0）对应于一个高温、高度随机的系统。当 $\\beta \\rightarrow 0$ 时，对所有 $i$ 而言，项 $\\beta Q_i \\rightarrow 0$。因此，对所有动作而言，$\\exp(\\beta Q_i) \\rightarrow 1$。概率 $p_i$ 趋近于一个均匀分布，即 $p_i \\approx 1/N$，其中 $N$ 是可用动作的数量。这意味着动作选择变得几乎是随机的，并且对学习到的价值 $Q_i$ 不敏感。这对应于一种**探索**状态，此时系统会尝试不同的动作，包括那些目前被认为是次优的动作。这对于发现新的奖励来源或适应变化的环境至关重要。这种状态可能与较低的紧张性多巴胺水平或其他神经调节剂（如去甲肾上腺素或乙酰胆碱）的影响有关，这些调节剂会增加行为变异性，并减少系统对预先存在的价值估计的依赖。\n\n因此，降低 $\\beta$ 会使动作的概率分布变得扁平化，使得选择更少地依赖于学习到的价值，并增加了选择非最高期望价值动作的倾向。这种机制使得生物体能够在利用已知资源和探索新资源之间灵活切换。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.5744  0.4256\n\\end{pmatrix}\n}\n$$", "id": "5001106"}]}