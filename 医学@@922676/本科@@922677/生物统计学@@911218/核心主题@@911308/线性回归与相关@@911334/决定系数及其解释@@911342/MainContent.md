## 引言
[决定系数](@entry_id:142674)，即 $R^2$，是统计学中最著名且应用最广泛的指标之一，用于评估[回归模型](@entry_id:163386)对数据拟合的优良程度。它提供了一个直观的度量，量化了[自变量](@entry_id:267118)能在多大程度上解释因变量的变异。然而，尽管其概念看似简单，但在实际应用中，$R^2$ 却常常被误解、误用，导致研究结论出现偏差。许多研究者满足于报告一个“高”的 $R^2$ 值，却忽略了其背后的假设、局限性以及在不同模型框架下的正确解读方式，这构成了理论与实践之间的一道鸿沟。

本文旨在系统性地填补这一鸿沟，为读者提供一份关于[决定系数](@entry_id:142674)的全面指南。我们将超越教科书式的定义，从第一性原理出发，层层递进地揭示 $R^2$ 的深刻内涵。

- 在**第一章“原理与机制”**中，我们将深入探讨 $R^2$ 的数学基础，从[线性模型](@entry_id:178302)中的[方差分解](@entry_id:272134)到其优美的几何解释。本章还将阐述其在[多元回归](@entry_id:144007)中的演变（如调整$R^2$），并将其概念扩展到广义线性模型、生存分析和混合效应模型等更高级的统计框架中。

- 随后的**第二章“[决定系数](@entry_id:142674)：应用与跨学科联系”**将理论与实践相结合，通过来自生物统计学、遗传学、临床试验等领域的丰富案例，展示$R^2$在真实世界研究中的应用。您将学会如何根据具体情境批判性地解读$R^2$值，并识别其在面对测量误差、[高维数据](@entry_id:138874)等复杂情况时的挑战。

- 最后，**第三章“动手实践”**提供了一系列精心设计的问题，旨在巩固您对核心概念的理解，培养您在数据分析中审慎使用$R^2$的技能。

通过这一结构化的学习路径，本文将帮助您建立对[决定系数](@entry_id:142674)的深刻理解，使您能够自信而准确地在自己的研究中应用和报告这一关键的统计工具。

## 原理与机制

在统计建模中，一个核心任务是量化[模型解释](@entry_id:637866)响应变量变异性的能力。[决定系数](@entry_id:142674)，通常记为 $R^2$，是评估[模型拟合](@entry_id:265652)优度的最广泛使用的指标之一。本章将从第一性原理出发，深入探讨 $R^2$ 的基本原理、其在不同模型框架下的正确解释、局限性以及其在[广义线性模型](@entry_id:171019)、生存分析和混合效应模型中的重要扩展。

### [线性模型](@entry_id:178302)中的变异分解

理解 $R^2$ 的基石在于对响应变量总变异的分解。对于一个包含 $n$ 个观测值的[线性回归](@entry_id:142318)模型，我们有观测值 $y_i$、模型预测的拟合值 $\hat{y}_i$ 和样本均值 $\bar{y}$。

**总平方和 (Total Sum of Squares, TSS)** 度量了响应变量 $Y$ 的总变异性。它被定义为每个观测值 $y_i$ 与其样本均值 $\bar{y}$ 之间离差的平方和：
$$ \mathrm{TSS} = \sum_{i=1}^{n} (y_i - \bar{y})^2 $$
TSS 代表了模型需要解释的“总变异”。从概念上讲，一个不包含任何预测变量、仅使用样本均值 $\bar{y}$ 进行预测的“[零模型](@entry_id:181842)”或“仅截距模型”，其[预测误差](@entry_id:753692)的平方和恰好是 TSS。因此，任何有意义的回归模型都应该比这个基准模型做得更好。[@problem_id:4900986]

**[残差平方和](@entry_id:174395) (Residual Sum of Squares, RSS)**，也称为[误差平方和](@entry_id:149299) (Sum of Squared Errors, SSE)，度量了模型*未*能解释的变异性。它是每个观测值 $y_i$ 与其对应的[模型拟合](@entry_id:265652)值 $\hat{y}_i$ 之间差值（即残差）的平方和：
$$ \mathrm{RSS} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 $$
[普通最小二乘法](@entry_id:137121) (Ordinary Least Squares, OLS) 的目标正是通过选择模型参数来最小化 RSS。

**解释平方和 (Explained Sum of Squares, ESS)**，也称为回归平方和 (Regression Sum of Squares, SSR)，度量了模型所*能*解释的变异性。它是每个拟合值 $\hat{y}_i$ 与样本均值 $\bar{y}$ 之间离差的平方和：
$$ \mathrm{ESS} = \sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2 $$

这些变异的组成部分之间存在一个非常优美的关系。通过简单的代数展开，我们可以证明一个基本的[方差分解](@entry_id:272134)恒等式。考虑离差 $y_i - \bar{y}$，我们可以将其分解为：
$$ y_i - \bar{y} = (y_i - \hat{y}_i) + (\hat{y}_i - \bar{y}) $$
将上式两边平方并对所有观测值求和，我们得到：
$$ \sum_{i=1}^{n} (y_i - \bar{y})^2 = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2 + 2\sum_{i=1}^{n} (y_i - \hat{y}_i)(\hat{y}_i - \bar{y}) $$
$$ \mathrm{TSS} = \mathrm{RSS} + \mathrm{ESS} + 2\sum_{i=1}^{n} e_i(\hat{y}_i - \bar{y}) $$
其中 $e_i = y_i - \hat{y}_i$ 是残差。

这里的关键在于，对于一个包含**截距项**的 OLS 模型，交叉乘积项 $2\sum e_i(\hat{y}_i - \bar{y})$ 恰好为零。这是因为 OLS 的[一阶条件](@entry_id:140702)保证了[残差向量](@entry_id:165091)与设计矩阵的每一列都正交。由于拟合值向量 $\hat{y}$ 是[设计矩阵](@entry_id:165826)[列的线性组合](@entry_id:150240)，[残差向量](@entry_id:165091)也必然与拟合值向量正交，即 $\sum e_i \hat{y}_i = 0$。此外，包含截距项的模型还保证了残差之和为零，$\sum e_i = 0$，这意味着观测值的均值等于拟合值的均值，$\bar{y} = \bar{\hat{y}}$。因此，交叉乘积项可以写为：
$$ \sum_{i=1}^{n} e_i(\hat{y}_i - \bar{y}) = \sum_{i=1}^{n} e_i \hat{y}_i - \bar{y} \sum_{i=1}^{n} e_i = 0 - \bar{y}(0) = 0 $$
这样，我们就得到了[线性回归](@entry_id:142318)中至关重要的[方差分解](@entry_id:272134)恒等式：
$$ \mathrm{TSS} = \mathrm{ESS} + \mathrm{RSS} $$
这个恒等式表明，响应变量的总变异可以被完美地划分为[模型解释](@entry_id:637866)的部分和模型未解释的部分。截距项的存在是确保这种清晰分解的数学前提。[@problem_id:4900986]

### [决定系数](@entry_id:142674) $R^2$ 的定义与几何解释

有了方差分解的基础，**[决定系数](@entry_id:142674) ($R^2$)** 的定义就水到渠成了。它被定义为总变异中被[模型解释](@entry_id:637866)的比例：
$$ R^2 = \frac{\mathrm{ESS}}{\mathrm{TSS}} $$
利用方差分解恒等式，它也可以更方便地表示为：
$$ R^2 = \frac{\mathrm{TSS} - \mathrm{RSS}}{\mathrm{TSS}} = 1 - \frac{\mathrm{RSS}}{\mathrm{TSS}} = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2} $$
$R^2$ 的值域为 $[0, 1]$（对于包含截距的 OLS 模型），$R^2=1$ 意味着模型完美解释了数据的变异性，而 $R^2=0$ 则表示模型的解释能力不比简单地使用均值进行预测更好。

例如，假设一项关于收缩压的生物统计学研究收集了 6 名个体的数据，观测值 $Y$ 为 $\{118, 130, 142, 150, 136, 125\}$，OLS 模型给出的拟合值 $\hat{Y}$ 为 $\{120, 129, 139, 149, 136, 128\}$。我们可以计算：
1.  样本均值 $\bar{y} = (118 + 130 + 142 + 150 + 136 + 125) / 6 = 133.5$。
2.  总平方和 $\mathrm{TSS} = (118 - 133.5)^2 + \dots + (125 - 133.5)^2 = 675.5$。
3.  [残差平方和](@entry_id:174395) $\mathrm{RSS} = (118 - 120)^2 + \dots + (125 - 128)^2 = 24$。
4.  [决定系数](@entry_id:142674) $R^2 = 1 - \frac{24}{675.5} \approx 0.9645$。
这意味着该[模型解释](@entry_id:637866)了收缩压样本变异的约 $96.5\%$。[@problem_id:4900986]

$R^2$ 还有一个深刻的几何解释。我们可以将观测向量 $y = (y_1, \dots, y_n)$、拟合向量 $\hat{y} = (\hat{y}_1, \dots, \hat{y}_n)$ 和[残差向量](@entry_id:165091) $e = (e_1, \dots, e_n)$ 视为 $n$ 维欧几里得空间中的向量。OLS 拟合过程在几何上等价于将观测向量 $y$ **[正交投影](@entry_id:144168)**到由[设计矩阵](@entry_id:165826) $X$ 的列向量所张成的子空间（列空间 $\mathcal{C}(X)$）上，得到的投影向量就是 $\hat{y}$。[残差向量](@entry_id:165091) $e = y - \hat{y}$ 则是 $y$ 在与 $\mathcal{C}(X)$ 正交的子空间上的分量。

在这种几何视角下，如果我们将所有向量进行中心化（即减去各自的均值），那么向量的平方范数（$\|v\|^2 = \sum v_i^2$）就对应于统计学上的平方和。中心化后的 $y$ 向量的平方范数是 TSS，中心化后的 $\hat{y}$ 向量的平方范数是 ESS。$y = \hat{y} + e$ 的[正交分解](@entry_id:148020)意味着毕达哥拉斯定理成立：$\|y\|^2 = \|\hat{y}\|^2 + \|e\|^2$，这正是 $\mathrm{TSS} = \mathrm{ESS} + \mathrm{RSS}$ 的几何表述。

$R^2$ 的定义 $\frac{\mathrm{ESS}}{\mathrm{TSS}}$ 在几何上就是 $\frac{\|\hat{y}\|^2}{\|y\|^2}$。如果我们考虑中心化后的 $y$ 和 $\hat{y}$ 之间的夹角 $\theta$，根据夹角余弦的定义 $\cos(\theta) = \frac{\langle y, \hat{y} \rangle}{\|y\| \|\hat{y}\|}$，并利用正交性 $\langle \hat{y}, e \rangle = 0$ 得到 $\langle y, \hat{y} \rangle = \langle \hat{y}+e, \hat{y} \rangle = \|\hat{y}\|^2$，我们可以推导出：
$$ \cos(\theta) = \frac{\|\hat{y}\|^2}{\|y\| \|\hat{y}\|} = \frac{\|\hat{y}\|}{\|y\|} $$
因此，
$$ R^2 = \frac{\|\hat{y}\|^2}{\|y\|^2} = \cos^2(\theta) $$
这揭示了一个优美的结论：在线性回归中，$R^2$ 等于中心化响应向量与其在模型预测子空间上投影之间夹角的余弦平方。$R^2$ 越接近 1，意味着 $y$ 和 $\hat{y}$ 之间的夹角越小，两者在方向上越接近。[@problem_id:4900972]

在仅有一个预测变量的**简单[线性回归](@entry_id:142318)**中，这个几何关系引出了另一个著名等价性：$R^2$ 等于响应变量 $Y$ 和预测变量 $X$ 之间**[皮尔逊相关系数](@entry_id:270276) ($r$)** 的平方。这是因为在这种情况下，预测子空间是由单个中心化预测向量 $x$ 张成的一条直线，$\hat{y}$ 正是 $y$ 在 $x$ 上的投影。此时，中心化 $y$ 和 $\hat{y}$ 之间的夹角 $\theta$ 正是中心化 $y$ 和 $x$ 之间的夹角，而 $r$ 本身就是这个夹角的余弦值。因此，$R^2 = r^2$。[@problem_id:4900972] [@problem_id:4900991]

### [多元回归](@entry_id:144007)中 $R^2$ 的解释与局限性

当模型包含多个预测变量时，$R^2$ 的一些性质和解释需要特别注意。

#### $R^2$ 的非递减性与调整 $R^2$

一个关键特性是，在向模型中添加新的预测变量时，$R^2$ **永远不会减小**。这是因为 OLS 总是在扩展后的模型空间中寻找最优解，其最小化的 RSS 必然小于或等于在子空间（原始模型）中找到的 RSS。即使新加入的变量与响应变量完全无关，模型也可以通过将其系数设置为零来“忽略”它，从而至少保持原有的 $R^2$ 不变。[@problem_id:4915369]

这种非递减的性质使得 $R^2$ 成为一个有问题的[模型比较](@entry_id:266577)工具，因为它会系统性地偏爱更复杂的模型，从而导致**过拟合 (overfitting)**。一个过拟合的模型在样本数据上表现优异（$R^2$ 很高），但在预测新数据时却表现很差。

为了解决这个问题，统计学家提出了**调整 $R^2$ (Adjusted $R^2$)**。它通过对模型中的预测变量数量（$p$）进行惩罚来修正 $R^2$：
$$ R^2_{\text{adj}} = 1 - \frac{\mathrm{RSS} / (n - p - 1)}{\mathrm{TSS} / (n - 1)} = 1 - (1 - R^2) \frac{n - 1}{n - p - 1} $$
分母和分子中的项分别是**均方残差 (Mean Squared Error, MSE)** 和**均方总和 (Mean Squared Total, MST)**，它们都考虑了各自的自由度。当向模型中添加一个信息量不足的预测变量时，RSS 的微小下降不足以抵消自由度 $(n-p-1)$ 的减少，这会导致 MSE 增大，从而使 $R^2_{\text{adj}}$ 下降。因此，$R^2_{\text{adj}}$ 是一个更诚实的[模型拟合](@entry_id:265652)度量，它在[模型解释](@entry_id:637866)力与[简约性](@entry_id:141352)之间取得了平衡。[@problem_id:4915369]

值得注意的是，$R^2_{\text{adj}}$ 甚至可能为负数。当 $R^2_{\text{adj}}  0$ 时，意味着模型的 MSE 大于 MST。这表明，在考虑了自由度惩罚后，你所构建的包含 $p$ 个预测变量的模型，其预测表现甚至不如只使用样本均值进行预测的[零模型](@entry_id:181842)。例如，在一个 $n=12$ 的小样本研究中，使用 $p=5$ 个预测变量，如果 TSS=1200，RSS=1120，那么 $R^2 \approx 0.067$。然而，$R^2_{\text{adj}} = 1 - \frac{1120/6}{1200/11} \approx -0.711$。这个负值清晰地警示我们，模型中的预测变量几乎没有提供任何有价值的信息，拟合结果非常差。[@problem_id:4900973]

#### 对 $R^2$ 的常见误解

尽管 $R^2$ 非常有用，但它也常常被误解和滥用。
1.  **高 $R^2$ 不等于因果关系**：$R^2$ 是一个关联性度量，它量化了变量之间的线性关系强度，但关联不等于因果。一个很高的 $R^2$ 可能完全由一个**[混杂变量](@entry_id:199777) (confounder)** 引起。例如，假设在一项研究中，我们发现某种血液生物标志物 $B$ 与疾病严重程度 $Y$ 之间存在很强的线性关系，模型 $R^2$ 高达 0.85。然而，可能存在一个共同的原因——年龄 $A$——它既影响 $B$ 的水平，也影响 $Y$。真实的因果路径可能是 $A \to B$ 和 $A \to Y$，而 $B$ 对 $Y$ 没有直接的因果效应。在这种情况下，$B$ 与 $Y$ 之间的强关联是“[伪相关](@entry_id:755254)”。如果在[多元回归](@entry_id:144007)模型中同时包含 $A$ 和 $B$ 来预测 $Y$，我们会发现 $B$ 的偏[回归系数](@entry_id:634860)接近于零，而模型的总 $R^2$ 可能会更高（例如达到 0.99），这表明几乎所有的变异都是由 $A$ 解释的。这个例子有力地说明，高 $R^2$ 本身绝不能作为因果推断的证据。[@problem_id:4900982]

2.  **高 $R^2$ 不意味着所有预测变量都显著**：在一个[多元回归](@entry_id:144007)模型中，即使 $R^2$ 很高，也可能没有一个单独的预测变量的 $p$ 值是显著的。这种情况通常在预测变量之间存在高度相关性，即**[多重共线性](@entry_id:141597) (multicollinearity)** 时发生。预测变量作为一个整体，能够很好地解释响应变量的变异，但由于它们的信息重叠，模型很难精确地估计每个变量的独立贡献，导致[系数估计](@entry_id:175952)的标准误膨胀，从而使得单个系数的显著性检验不显著。[@problem_id:4915369]

3.  **低 $R^2$ 不意味着没有关系**：$R^2$ 衡量的是**线性**拟合的优度。如果变量之间的真实关系是强烈的非线性关系（例如 U 型曲线），那么线性模型可能会得到一个非常低的 $R^2$，但这并不意味着变量之间没有关系。它仅仅说明线性模型不是描述这种关系的合适工具。因此，在解释 $R^2$ 之前，通过[残差图](@entry_id:169585)等诊断工具检查模型的线性假设至关重要。[@problem_id:4915369]

4.  **[多元回归](@entry_id:144007)中的 $R^2$ 与[相关系数](@entry_id:147037)**：在[多元回归](@entry_id:144007)中，$R^2$ 不再等于任何单个预测变量与响应变量之间[相关系数](@entry_id:147037)的平方 ($R^2 \ne r_{YX_k}^2$)。$R^2$ 反映的是所有预测变量**共同**解释的变异比例。$R^2$ 通常会大于任何一个 $r_{YX_k}^2$，因为其他变量会提供额外（或独特）的解释信息。只有在一个非常特殊的情况下，$R^2$ 才会等于 $r_{YX_1}^2$，那就是当所有其他预测变量 $X_2, \dots, X_p$ 在 $X_1$ 存在的情况下，没有提供任何额外的解释能力。这在几何上意味着，响应向量 $y$ 在整个预测变量空间上的投影，与它在仅由 $X_1$ 张成的子空间上的投影完全重合。[@problem_id:4900991]

### $R^2$ 与数据变换

$R^2$ 的值对响应变量 $Y$ 的变换很敏感。
- 对于**[线性变换](@entry_id:143080)**（$Y^* = a + bY, b \ne 0$），$R^2$ 是**不变的**。这是因为[线性变换](@entry_id:143080)会以相同的比例缩放 TSS 和 RSS，因此它们的比率保持不变。这使得 $R^2$ 在单位变换（如从英寸到厘米）下是稳健的。[@problem_id:4900990]

- 对于**非线性变换**，$R^2$ **不是不变的**。一个经过精心选择的[非线性变换](@entry_id:636115)可以使变量之间的关系变得更线性，从而显著提高[线性模型](@entry_id:178302)的 $R^2$。例如，如果真实关系是指数增长 $y=2^x$，那么对数据 $\{ (1,2), (2,4), (3,8), (4,16) \}$ 进行[线性回归](@entry_id:142318)，得到的 $R^2$ 约为 0.92。然而，如果我们对响应变量进行[对数变换](@entry_id:267035) $z = \log_2(y)$，新的数据变为 $\{ (1,1), (2,2), (3,3), (4,4) \}$，关系变成了完美的线性关系 $z=x$。此时，对 $z$ 关于 $x$ 的[线性回归](@entry_id:142318)将得到 $R^2 = 1$。这个例子说明，变换可以改变 $R^2$，并且选择合适的变换是改善[模型拟合](@entry_id:265652)的重要策略。[@problem_id:4900990]

### 特殊情况：无截距模型

当拟合一个强制通过原点的**无截距模型**时，必须对 $R^2$ 的解释格外小心。在这种情况下，之前提到的基本方差分解 $\mathrm{TSS} = \mathrm{ESS} + \mathrm{RSS}$ 通常不再成立。这是因为模型不再保证残差之和为零，从而导致正交性被破坏。

如果继续使用标准公式 $R^2 = 1 - \mathrm{RSS}/\mathrm{TSS}$（其中 TSS 仍然是围绕均值 $\bar{y}$ 计算的），$R^2$ 可能会出现负值，因为它不再是“[已解释方差](@entry_id:172726)的比例”。

更具欺骗性的是，一些统计软件在处理无截距模型时，会采用不同的 $R^2$ 定义，通常是使用一个未经中心化的总平方和，即 $\mathrm{TSS}_0 = \sum y_i^2$。在这种情况下，报告的 $R^2$ 值为 $1 - \mathrm{RSS}/\mathrm{TSS}_0$。当响应变量的均值远离零时，$\mathrm{TSS}_0$ 会比 $\mathrm{TSS}$ 大得多。这会导致一个即使拟合很差（RSS 很大）的模型，也可能得到一个虚高且具有误导性的 $R^2$ 值。例如，对于一组数据，包含截距的模型 M1 可能得到 $R^2=0.935$，而一个拟合更差（RSS 更大）的无截距模型 M2，如果使用基于原点的 $\mathrm{TSS}_0$ 计算，可能会报告一个 $R^2 \approx 0.998$。这种虚高的值仅仅是因为分母被人为地放大了，将数据的平均水平也算作了“待解释的变异”。因此，对于无截距模型，标准 $R^2$ 的解释是不可靠的，比较有无截距模型的 $R^2$ 值通常没有意义。[@problem_id:4900976]

### 扩展到广义模型：伪 $R^2$

$R^2$ 的概念是基于[方差分解](@entry_id:272134)，这在 OLS 线性回归中是自然而然的。但对于使用**[最大似然](@entry_id:146147)法**估计的**广义线性模型 (Generalized Linear Models, GLM)**，如 Logistic 回归或泊松回归，以及生存模型，方差的概念不再那么直接。为了在这些模型中拥有一个类似的拟合优度指标，研究者开发了多种**伪 $R^2$ (pseudo-$R^2$)**。

这些伪 $R^2$ 指标旨在模仿 OLS $R^2$ 的某些特性（例如，值在 0 到 1 之间，值越高表示拟合越好），但它们**不具有**“[已解释方差](@entry_id:172726)比例”的直接解释。它们通常是基于模型[对数似然](@entry_id:273783) $(\ell)$ 的改进来构建的。

#### Logistic 回归中的伪 $R^2$

对于预测[二元结果](@entry_id:173636)（例如，患病/不患病）的 Logistic 回归，常见的伪 $R^2$ 包括 McFadden 的 $R^2$ 和 Cox  Snell 的 $R^2$。它们通常比较完整模型（带预测变量）的[对数似然](@entry_id:273783) $\ell_{\text{full}}$ 与[零模型](@entry_id:181842)（仅截距）的[对数似然](@entry_id:273783) $\ell_{\text{null}}$。

一个极其重要的特性是，伪 $R^2$ 的值**依赖于事件的患病率 (prevalence)** 或基线率。[零模型](@entry_id:181842)的[对数似然](@entry_id:273783) $\ell_{\text{null}}$ 是由样本的整体事件比例决定的。当事件非常罕见（如患病率 $p=0.05$）或非常普遍时，[零模型](@entry_id:181842)本身就已经有很高的似然值（接近其理论最大值 0）。相比之下，当事件患病率为 $p=0.50$ 时，结果的不确定性最大，[零模型](@entry_id:181842)的似然值最低。

这意味着，对于一个罕见事件，模型相对于[零模型](@entry_id:181842)能够实现的“改进空间”本身就非常有限。因此，即使一组预测变量具有很强的预测能力，在低患病率人群中计算出的伪 $R^2$ 也可能远低于在患病率为 50% 的人群中计算出的伪 $R^2$。这使得伪 $R^2$ 值**不能在具有不同基线率的群体之间直接进行比较**。它是一个特定于样本的拟合度量，而不是一个普适的效应大小度量。[@problem_id:4900994]

#### 生存模型中的伪 $R^2$

在处理[右删失](@entry_id:164686)[生存数据](@entry_id:165675)的 **Cox [比例风险模型](@entry_id:171806)**中，也可以定义基于偏[对数似然](@entry_id:273783) $(\ell)$ 的伪 $R^2$。一个常见的定义是 Cox-Snell 的 $R^2$：
$$ R^2_{\text{CS}} = 1 - \exp\left\{ - \frac{2}{n} (\ell_{\text{full}} - \ell_{\text{null}}) \right\} $$
这个指标有几个关键特性：
1.  它与用于比较[嵌套模型](@entry_id:635829)的**[似然比检验统计量](@entry_id:169778)** $\Lambda = 2(\ell_{\text{full}} - \ell_{\text{null}})$ 是单调递增关系。这意味着，反对[零模型](@entry_id:181842)的统计证据越强， $R^2_{\text{CS}}$ 的值就越大。[@problem_id:4900963]
2.  与 OLS 的 $R^2$ 不同，它不表示“生存时间方差的解释比例”。它只是一个标准化的[模型拟合](@entry_id:265652)改善度量。
3.  一个众所周知的局限是，$R^2_{\text{CS}}$ 的最大值通常**无法达到 1**，即使模型具有完美的预测能力。其理论上限依赖于[零模型](@entry_id:181842)的似然值，通常小于 1。为了解决这个问题，Nagelkerke 提出了一个重缩放版本，将 $R^2_{\text{CS}}$ 除以其理论最大值，使得新指标的范围是 $[0, 1]$。[@problem_id:4900963]

#### 线性混合效应模型中的 $R^2$

在处理分层或聚类数据时，**线性混合效应模型 (Linear Mixed-Effects Models, LMMs)** 被广泛使用，因为它能同时对固定效应和随机效应进行建模。对于 LMMs，我们可以将 $R^2$ 的概念扩展为两个有用的部分：

在一个典型的随机截距模型中，$y_{ij} = \mathbf{x}_{ij}^\top \boldsymbol{\beta} + b_{0j} + \varepsilon_{ij}$，总方差可以分解为三个部分：
- $\sigma^2_{\text{fixed}}$：由固定效应（即可测量的预测变量）解释的方差。
- $\sigma^2_{\text{random}}$：由随机效应（如聚类/组间的变异）解释的方差。
- $\sigma^2_{\varepsilon}$：残差方差，即未被[模型解释](@entry_id:637866)的个体层面的变异。

总方差为 $\mathrm{Var}_{\text{total}} = \sigma^2_{\text{fixed}} + \sigma^2_{\text{random}} + \sigma^2_{\varepsilon}$。基于此，我们可以定义：

1.  **边际 $R^2$ (Marginal $R^2$)**:
    $$ R^2_m = \frac{\sigma^2_{\text{fixed}}}{\sigma^2_{\text{fixed}} + \sigma^2_{\text{random}} + \sigma^2_{\varepsilon}} $$
    $R^2_m$ 度量了仅由**固定效应**解释的[方差比](@entry_id:162608)例。它反映了模型在**群体平均**层面上的解释能力，忽略了聚类结构。[@problem_id:4900974]

2.  **条件 $R^2$ (Conditional $R^2$)**:
    $$ R^2_c = \frac{\sigma^2_{\text{fixed}} + \sigma^2_{\text{random}}}{\sigma^2_{\text{fixed}} + \sigma^2_{\text{random}} + \sigma^2_{\varepsilon}} $$
    $R^2_c$ 度量了由**固定效应和随机效应共同**解释的[方差比](@entry_id:162608)例。它反映了模型在**特定聚类或个体层面**上的解释能力。$R^2_c$ 总是大于或等于 $R^2_m$。[@problem_id:4900974]

这对 $R^2_m$ 和 $R^2_c$ 提供了一个强大的框架，用以评估混合效应模型的不同层面的表现，这对于理解和报告 LMMs 的结果至关重要。