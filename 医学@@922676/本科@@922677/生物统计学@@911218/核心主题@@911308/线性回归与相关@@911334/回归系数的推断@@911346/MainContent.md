## 引言
对[回归系数](@entry_id:634860)的推断是[统计建模](@entry_id:272466)的核心，它使我们能够超越简单的[点估计](@entry_id:174544)，量化与模型参数相关的不确定性，从而得出有意义的科学结论。然而，从理论到实践的道路充满挑战。教科书中的经典模型依赖于一系列理想化假设，但在生物统计学、流行病学等领域的真实数据分析中，这些假设往往难以完全满足，从而对推断的有效性构成威胁。本文旨在弥合这一差距，系统性地阐述如何对[回归系数](@entry_id:634860)进行严谨且稳健的统计推断。

本文分为三个主要部分。在**“原理与机制”**一章中，我们将奠定理论基础，深入探讨经典[线性模型](@entry_id:178302)下的推断框架、[假设检验与置信区间](@entry_id:176458)的构建，并剖析多重共线性、异方差性等常见挑战及其应对策略。接着，在**“应用与跨学科联系”**一章中，我们将视野扩展到更复杂的现实世界问题，展示这些原理如何应用于[交互作用](@entry_id:164533)分析、聚类数据、生存分析乃至[高维数据](@entry_id:138874)场景，揭示统计推断在不同科学领域的强大生命力。最后，在**“动手实践”**部分，您将有机会通过解决具体问题来巩固所学知识，将理论真正转化为实践技能。通过这一结构化的学习路径，读者将建立起一个从基本原理到高级应用的完整知识体系。

## 原理与机制

在对[回归系数](@entry_id:634860)进行推断时，我们旨在超越点估计，量化与我们估计相关的[统计不确定性](@entry_id:267672)。本章深入探讨了支持这些推断程序的原理，从经典的正态线性模型框架开始，然后扩展到处理现实世界数据中经常出现的复杂情况。我们将研究[假设检验](@entry_id:142556)和[置信区间](@entry_id:138194)的构建机制，探讨多重共线性的挑战，并阐述在[模型误差](@entry_id:175815)不符合理想假设时的解决方案。

### 推断的经典框架

统计推断的基石是一个明确定义的[概率模型](@entry_id:265150)。在[线性回归](@entry_id:142318)的背景下，**经典正态[线性模型](@entry_id:178302)**是标准推断程序的出发点。其假设如下：

1.  **线性关系**：因变量 $\mathbf{Y}$ 的条件期望是预测变量 $\mathbf{X}$ 的线性函数，即 $\mathbb{E}(\mathbf{Y} | \mathbf{X}) = \mathbf{X}\boldsymbol{\beta}$。
2.  **满秩**：[设计矩阵](@entry_id:165826) $\mathbf{X}$（一个 $n \times p$ 矩阵，其中 $n$ 是观测数，$p$ 是系数个数）具有[满列秩](@entry_id:749628)，即 $\operatorname{rank}(\mathbf{X}) = p$。这意味着预测变量之间不存在完美的线性关系。
3.  **严格[外生性](@entry_id:146270)**：误差项 $\boldsymbol{\varepsilon}$ 的条件期望为零，即 $\mathbb{E}(\boldsymbol{\varepsilon} | \mathbf{X}) = \mathbf{0}$。这意味着误差与预测变量无关。
4.  **球形正态误差**：误差项服从一个均值为$\mathbf{0}$、协方差矩阵为 $\sigma^2 \mathbf{I}_n$ 的[多元正态分布](@entry_id:175229)，记为 $\boldsymbol{\varepsilon} \sim \mathcal{N}(\mathbf{0}, \sigma^2 \mathbf{I}_n)$。这个假设包含了三个部分：
    *   **正态性**：误差是正态分布的。
    *   **[同方差性](@entry_id:634679)**：所有误差项具有相同的方差 $\sigma^2$。
    *   **无自相关**：不同观测的误差项是不相关的，$\operatorname{Cov}(\varepsilon_i, \varepsilon_j | \mathbf{X}) = 0$ 对所有 $i \neq j$ 成立。

在这些假设下，普通最小二乘法 (OLS) 估计量 $\hat{\boldsymbol{\beta}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y}$ 具有几个理想的性质。首先，它不仅是**无偏的**（即 $\mathbb{E}(\hat{\boldsymbol{\beta}}) = \boldsymbol{\beta}$），而且在所有线性无偏估计量中具有最小的方差。这个结论被称为**[高斯-马尔可夫定理](@entry_id:138437)**，它确立了 OLS 为**[最佳线性无偏估计量](@entry_id:137602) (BLUE)**。值得注意的是，要证明 OLS 的无偏性和其为 BLUE，我们并不需要[误差的正态性](@entry_id:634130)假设。无偏性仅依赖于严格[外生性](@entry_id:146270)假设 $\mathbb{E}(\boldsymbol{\varepsilon} | \mathbf{X}) = \mathbf{0}$。[高斯-马尔可夫定理](@entry_id:138437)的结论则依赖于前三个假设加上[同方差性](@entry_id:634679)和无[自相关](@entry_id:138991)性，但同样不依赖于正态性 [@problem_id:4916061]。

[正态性假设](@entry_id:170614)的关键作用在于它允许我们在有限样本中精确地描述 $\hat{\boldsymbol{\beta}}$ 的抽样分布。由于 $\hat{\boldsymbol{\beta}}$ 是正态分布的 $\mathbf{Y}$（或等价地，$\boldsymbol{\varepsilon}$）的[线性变换](@entry_id:143080)，因此 $\hat{\boldsymbol{\beta}}$ 本身也服从一个[多元正态分布](@entry_id:175229)：
$$
\hat{\boldsymbol{\beta}} \sim \mathcal{N}(\boldsymbol{\beta}, \sigma^2(\mathbf{X}^T \mathbf{X})^{-1})
$$
这个精确的分布构成了所有经典 $t$ 检验和 $F$ 检验的基础 [@problem_id:4916044]。

### 单个系数的推断

在生物统计学实践中，最常见的任务是评估单个预测变量与结局之间的关联。这可以通过对相应的[回归系数](@entry_id:634860) $\beta_j$ 进行假设检验和构建[置信区间](@entry_id:138194)来完成。

#### 假设检验

检验一个预测变量是否有显著的线性关联，通常意味着检验零假设 $H_0: \beta_j = 0$ 对抗[备择假设](@entry_id:167270) $H_a: \beta_j \neq 0$。在经典正态线性模型假设下，我们可以构建一个枢轴量，其分布在零假设下是已知的。这个枢轴量就是 **t-统计量**：

$$
t = \frac{\hat{\beta}_j - \beta_j}{\widehat{\operatorname{SE}}(\hat{\beta}_j)}
$$

其中 $\hat{\beta}_j$ 是 OLS 估计值，$\beta_j$ 是其真实值，$\widehat{\operatorname{SE}}(\hat{\beta}_j)$ 是 $\hat{\beta}_j$ 的估计标准误。这个[标准误](@entry_id:635378)由 $\hat{\boldsymbol{\beta}}$ 的方差-协方差矩阵的对角元素之平方根得到，其中未知的[误差方差](@entry_id:636041) $\sigma^2$ 被其[无偏估计量](@entry_id:756290) $s^2 = \frac{\text{RSS}}{n-p}$（[残差平方和](@entry_id:174395)除以残差自由度）所替代。

这个统计量的精妙之处在于它的分布。分子 $\hat{\beta}_j - \beta_j$ 是正态分布的。分母中的 $s^2$ 可以证明，当按 $\sigma^2$ 缩放后，它服从一个自由度为 $n-p$ 的卡方 ($\chi^2$) 分布。更重要的是，由于[正态性假设](@entry_id:170614)，可以证明 $\hat{\beta}_j$ 和 $s^2$ 是相互独立的。因此，t-统计量是一个标准正态随机变量与一个独立的、除以其自由度的卡方随机变量之平方根的比值。根据定义，这个比值精确地服从一个自由度为 $n-p$ 的**学生 t-分布** [@problem_id:4916044]。

要进行检验，我们计算在零假设（$\beta_j = 0$）下的 t-统计量观测值：

$$
t_{obs} = \frac{\hat{\beta}_j - 0}{\widehat{\operatorname{SE}}(\hat{\beta}_j)}
$$

例如，假设一项研究样本量为 $n=100$，模型包含 $p=6$ 个系数（包括截距）。对于第三个协变量，我们得到估计值 $\hat{\beta}_3 = 0.8$，其估计方差为 $\widehat{\operatorname{Var}}(\hat{\beta}_3) = 0.04$。首先，我们计算[标准误](@entry_id:635378) $\widehat{\operatorname{SE}}(\hat{\beta}_3) = \sqrt{0.04} = 0.2$。然后，t-统计量的值为 $t_{obs} = \frac{0.8}{0.2} = 4$。参考的 t-分布自由度为 $df = n-p = 100-6=94$。

接下来，我们计算 **p-值**，它是在零假设为真的情况下，观测到至少与我们计算的统计量一样极端的[检验统计量](@entry_id:167372)的概率。对于双边检验，p-值是 $P(|T| \ge |t_{obs}|)$，其中 $T \sim t_{94}$。由于 t-分布的对称性，这等于 $2 \times P(T \ge 4)$ [@problem_id:4916060]。如果这个 p-值小于预先设定的显著性水平 $\alpha$（例如 0.05），我们便拒绝零假设，断定该系数与零有显著差异。

#### [置信区间](@entry_id:138194)

与假设检验相辅相成的是**[置信区间](@entry_id:138194)**的构建。一个 $(1-\alpha) \times 100\%$ 的[置信区间](@entry_id:138194)提供了一个关于真实参数 $\beta_j$ 的 plausible values 范围。其通用形式为：

$$
\text{点估计} \pm (\text{临界值}) \times (\text{标准误})
$$

具体来说，$\beta_j$ 的[置信区间](@entry_id:138194)是：

$$
\hat{\beta}_j \pm t_{1-\alpha/2, n-p} \times \widehat{\operatorname{SE}}(\hat{\beta}_j)
$$

其中 $t_{1-\alpha/2, n-p}$ 是 t-分布的临界值，它在其上尾部留下了 $\alpha/2$ 的概率面积。例如，考虑一个模型，其残差自由度为 $n-p=20$。对于系数 $\beta_4$，我们得到估计值 $\hat{\beta}_4 = 1.1$ 和[标准误](@entry_id:635378) $\widehat{\operatorname{SE}}(\hat{\beta}_4) = 0.3$。要构建一个 95% 的[置信区间](@entry_id:138194)，我们有 $\alpha=0.05$，需要找到 $t_{0.975, 20}$。这个临界值约为 2.086。因此，区间的界限为 $1.1 \pm 2.086 \times 0.3$，计算得出区间为 $[0.4742, 1.7258]$ [@problem_id:4916026]。

对此区间的正确**频率派解释**至关重要：如果我们从同一个数据生成过程中反复抽取大小为 $n$ 的样本，并为每个样本构建一个 95% [置信区间](@entry_id:138194)，那么大约 95% 的这些区间将包含真实的、未知的参数值 $\beta_4$。我们不能说真实的 $\beta_4$ 有 95% 的概率落在这个特定的 $[0.4742, 1.7258]$ 区间内；因为真实参数是固定的，而随机的是我们构建的区间。

### 实践挑战 I：[多重共线性](@entry_id:141597)的问题

经典[线性模型](@entry_id:178302)的一个核心假设是[设计矩阵](@entry_id:165826) $\mathbf{X}$ 具有[满列秩](@entry_id:749628)。当这个假设被违反或接近被违反时，就会出现**[多重共线性](@entry_id:141597)**问题。

#### 完美[多重共线性](@entry_id:141597)与可识别性

**完美多重共线性**发生在当一个预测变量可以被其他预测变量完美地[线性预测](@entry_id:180569)时。在这种情况下，[设计矩阵](@entry_id:165826) $\mathbf{X}$ 的列是线性相关的，导致 $\operatorname{rank}(\mathbf{X})  p$。其直接后果是矩阵 $\mathbf{X}^T \mathbf{X}$ 是奇异的（不可逆），OLS 估计量 $\hat{\boldsymbol{\beta}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y}$ 无法被唯一确定。我们说参数 $\boldsymbol{\beta}$ 是**不可识别的**。

例如，考虑一个模型，其[设计矩阵](@entry_id:165826)包含一个截距列（全为1的向量 $\mathbf{1}$）、一个协变量 $x$ 和另一个协变量 $z$，其中 $z$ 被构造成 $z_i = 2x_i + 3$。那么，列向量 $z$ 是 $\mathbf{1}$ 和 $x$ 的[线性组合](@entry_id:155091) ($z = 3\mathbf{1} + 2x$)。这三列是线性相关的，秩小于3，因此系数向量 $\boldsymbol{\beta}$ 是不可识别的 [@problem_id:4916022]。在实践中，这种情况通常源于[模型设定错误](@entry_id:170325)，例如同时包含年龄和出生年份以及研究开始年份。

#### 近似[多重共线性](@entry_id:141597)与[方差膨胀](@entry_id:756433)

在实践中更常见的是**近似多重共线性**，即预测变量之间存在高度相关但非完美相关。虽然 OLS 估计量在理论上仍然可以计算，但推断的稳定性会受到严重影响。系数[估计量的方差](@entry_id:167223)会变得非常大，导致：
1.  估计的[标准误](@entry_id:635378)很大，使得系数的[置信区间](@entry_id:138194)非常宽。
2.  检验统计量的绝对值变小，降低了拒绝零假设的统计功效。
3.  [系数估计](@entry_id:175952)值对数据的微小变动非常敏感。

诊断多重共线性的一个关键工具是**[方差膨胀因子 (VIF)](@entry_id:633931)**。对于第 $j$ 个预测变量，$VIF_j$ 的定义如下：
$$
\operatorname{VIF}_j = \frac{1}{1 - R_j^2}
$$
其中 $R_j^2$ 是将 $X_j$ 对模型中所有其他预测变量进行辅助回归时得到的[决定系数](@entry_id:142674)。$R_j^2$ 衡量了 $X_j$ 的方差中可以被其他预测变量解释的比例。

$VIF_j$ 的值直接量化了多重共线性对 $\hat{\beta}_j$ 方差的“膨胀”程度。具体来说，$\hat{\beta}_j$ 的方差可以表示为：
$$
\operatorname{Var}(\hat{\beta}_j) = \frac{\sigma^2}{\sum (X_{ij} - \bar{X}_j)^2} \cdot \operatorname{VIF}_j
$$
当 $X_j$ 与其他预测变量正交时，$R_j^2=0$ 且 $VIF_j=1$，此时方差达到最小值。如果一个辅助回归得到 $R_2^2 = 0.84$，那么 $\operatorname{VIF}_2 = \frac{1}{1-0.84} = \frac{1}{0.16} = 6.25$。这表明，由于 $X_2$ 与其他预测变量的[共线性](@entry_id:270224)，$\hat{\beta}_2$ 的[方差比](@entry_id:162608)其在正交情况下的方差大 6.25 倍。这显著降低了我们估计 $\beta_2$ 的精度 [@problem_id:4916069]。一般而言，VIF 值大于 5 或 10 被认为是存在问题的[多重共线性](@entry_id:141597)的标志。

### 实践挑战 II：当误差假设不成立时

经典推断的有效性严重依赖于误差项的假设。当这些假设——特别是正态性和[同方差性](@entry_id:634679)——被违反时，标准的 t-检验和 F-检验可能会产生误导性结果。

#### 非正态误差

如果误差项不是正态分布的（例如，它们是偏斜的或[重尾](@entry_id:274276)的），但满足[高斯-马尔可夫假设](@entry_id:165534)（均值为零，同方差，不相关），那么 OLS 估计量 $\hat{\boldsymbol{\beta}}$ 仍然是无偏的且是 BLUE。然而，依赖于[正态性假设](@entry_id:170614)的推断程序不再精确。t-统计量不再精确地服从 t-分布，F-统计量也不再精确地服从 F-分布。

幸运的是，对于大样本，**[中心极限定理](@entry_id:143108) (CLT)** 发挥了作用。即使单个误差项非正态，$\hat{\boldsymbol{\beta}}$（作为这些误差的[线性组合](@entry_id:155091)）的[抽样分布](@entry_id:269683)也会随着样本量 $n$ 的增大而趋于正态分布。因此，我们可以依赖**渐近推断**。对于足够大的 $n$：
-   t-统计量近似服从标准正态分布 $\mathcal{N}(0, 1)$。
-   用于检验 $q$ 个[线性约束](@entry_id:636966)的 Wald 统计量近似服从自由度为 $q$ 的[卡方分布](@entry_id:165213) $\chi^2_q$。一个等价的结果是，常用的 F-统计量乘以 $q$ ($qF$) 也近似服从 $\chi^2_q$ 分布 [@problem_id:4916044]。

因此，当样本量较大时，即使误差非正态，我们仍然可以进行近似有效的推断。然而，如果误差分布的方差是无限的（例如，非常重尾的分布），[中心极限定理](@entry_id:143108)的[标准形式](@entry_id:153058)不适用，OLS 推断将失效 [@problem_id:4916044]。

#### 异方差性

**异方差性**是指误差的方差不是一个常数，而是随着预测变量的变化而变化，即 $\operatorname{Var}(\varepsilon_i | \mathbf{X}) = \sigma_i^2$。在生物医学数据中，这种情况很常见，例如，在较高水平的测量值上，测量误差可能更大。

当存在异方差性时：
1.  OLS 估计量 $\hat{\boldsymbol{\beta}}$ 仍然是无偏和一致的（在模型均值被正确设定的情况下）。
2.  然而，$\hat{\boldsymbol{\beta}}$ 不再是 BLUE（[加权最小二乘法](@entry_id:177517)可能是更有效的）。
3.  最严重的是，**标准的 OLS 方差公式 $\sigma^2(\mathbf{X}^T \mathbf{X})^{-1}$ 是错误的**。使用这个公式计算的[标准误](@entry_id:635378)是**有偏的**，从而导致假设检验和[置信区间](@entry_id:138194)完全无效，即使在大样本中也是如此 [@problem_id:4916044]。

**诊断[异方差性](@entry_id:136378)**：有多种检验可以用来检测异方差性。一个通用且流行的方法是**怀特检验 (White's test)**。其思想是，如果误差是同方差的，那么它们的平方 $\varepsilon_i^2$ 应该与预测变量无关。由于我们无法观测到 $\varepsilon_i^2$，我们使用其代理——OLS 残差的平方 $\hat{\varepsilon}_i^2$。怀特检验的步骤如下：
1.  首先运行原始的 OLS 回归并获得残差 $\hat{\varepsilon}_i$。
2.  然后，运行一个**辅助回归**，将残差的平方 $\hat{\varepsilon}_i^2$ 对原始预测变量、它们的平方项以及它们的交叉乘积项进行回归。例如，对于一个有两个预测变量 $X_1$ 和 $X_2$ 的模型，辅助回归模型是：
    $$
    \hat{\varepsilon}_i^2 = \delta_0 + \delta_1 X_{1i} + \delta_2 X_{2i} + \delta_3 X_{1i}^2 + \delta_4 X_{2i}^2 + \delta_5 X_{1i}X_{2i} + \nu_i
    $$
3.  从这个辅助回归中计算[决定系数](@entry_id:142674) $R^2$。
4.  检验统计量是 $nR^2$。在[同方差性](@entry_id:634679)的零假设下，这个统计量在大样本中近似服从一个卡方分布，其自由度等于辅助回归中除截距外的预测变量个数（本例中为 5）。一个显著的[卡方检验](@entry_id:174175)结果表明存在异方差性 [@problem_id:4916036]。

**处理异方差性：[稳健标准误](@entry_id:146925)**：当检测到异方差性（或为了防范其可能性）时，最常见的做法不是改变估计量 $\hat{\boldsymbol{\beta}}$，而是修正其标准误的计算方法。这就是**[异方差性](@entry_id:136378)一致性 (HC) 标准误**，通常称为**[稳健标准误](@entry_id:146925)**或“三明治”估计量。

这个“三明治”估计量的名称来源于其渐近协方差矩阵的一般形式 $V = A^{-1} B A^{-1}$，在 OLS 的情况下，它变为：
$$
\operatorname{Var}(\hat{\boldsymbol{\beta}}) = (\mathbf{X}^T \mathbf{X})^{-1} (\mathbf{X}^T \boldsymbol{\Omega} \mathbf{X}) (\mathbf{X}^T \mathbf{X})^{-1}
$$
其中 $\boldsymbol{\Omega} = \operatorname{diag}(\sigma_1^2, \dots, \sigma_n^2)$。[稳健标准误](@entry_id:146925)通过用基于残差的估计值 $\hat{\boldsymbol{\Omega}}$ 替换未知的 $\boldsymbol{\Omega}$ 来工作。即使在线性模型被错误设定（即 $\mathbb{E}(\mathbf{Y}|\mathbf{X})$ 不是线性的）或存在异方差性的情况下，只要观测是独立的，这种方法仍能为 $\hat{\boldsymbol{\beta}}$（现在被解释为最佳[线性预测](@entry_id:180569)系数的估计）提供一致的方差估计和有效的推断 [@problem_id:4916024]。

在实践中，有几种不同版本的 HC [标准误](@entry_id:635378)（HC0, HC1, HC2, HC3），它们在如何使用残差 $\hat{\varepsilon}_i$ 来估计 $\sigma_i^2$ 上有所不同。
*   **HC0** 是最基础的版本，直接使用 $\hat{\varepsilon}_i^2$。
*   **HC1** 进行了一个简单的自由度校正，将所有项乘以 $\frac{n}{n-p}$。
*   **HC2** 和 **HC3** 进行了更复杂的、与[杠杆值](@entry_id:172567) $h_{ii}$（[帽子矩阵](@entry_id:174084)的对角元素）相关的校正，$\hat{\omega}_i = \frac{\hat{\varepsilon}_i^2}{1-h_{ii}}$ (HC2) 和 $\hat{\omega}_i = \frac{\hat{\varepsilon}_i^2}{(1-h_{ii})^2}$ (HC3)。这些校正旨在弥补高[杠杆值](@entry_id:172567)点上残差被过度压缩的倾向。

尽管这些校正在小样本中的表现不同（HC3 通常被认为更保守，因此在小样本中更受推荐），但它们在标准[正则性条件](@entry_id:166962)下都是**[渐近等价](@entry_id:273818)的**。这意味着随着样本量 $n \to \infty$，它们都收敛到相同的真实协方差矩阵 [@problem_id:4916058] [@problem_id:4916024]。

### 三种通用的渐近假设检验原理

当我们超越经典[线性模型](@entry_id:178302)（例如，进入[广义线性模型](@entry_id:171019)如逻辑回归）或当[正态性假设](@entry_id:170614)不成立时，我们通常依赖于三种通用的、[渐近等价](@entry_id:273818)的检验原理：**Wald 检验**、**[似然比检验](@entry_id:268070) (LRT)** 和**分数检验 (Score test)**。

假设我们正在检验关于单个参数的假设 $H_0: \beta_j=0$。这三种检验从不同的角度评估这个假设与数据的拟合程度：

1.  **Wald 检验**：它问：“在无约束模型中估计的 $\hat{\beta}_j$ 与其零假设下的值 0 相比，距离有多远（以其标准误为单位）？” Wald [检验统计量](@entry_id:167372)通常是 $(\hat{\beta}_j / \widehat{\operatorname{SE}}(\hat{\beta}_j))^2$。它仅依赖于在[备择假设](@entry_id:167270)下拟合的“完整模型”。我们之前讨论的 t-检验的平方就是一种 Wald 检验。

2.  **[似然比检验](@entry_id:268070) (LRT)**：它问：“拟合包含 $\beta_j$ 的完整模型与不包含 $\beta_j$ 的约束（或零）模型相比，[对数似然](@entry_id:273783)的改善程度是否显著？” [检验统计量](@entry_id:167372)是 $LR = 2(\ell(\hat{\boldsymbol{\beta}}_{\text{full}}) - \ell(\hat{\boldsymbol{\beta}}_{\text{null}}))$，其中 $\ell$ 是[对数似然函数](@entry_id:168593)。它需要拟合两个模型。

3.  **分数检验（或[拉格朗日乘子检验](@entry_id:176149)）**：它问：“如果在零假设为真的模型（即 $\beta_j=0$）的点上评估完整模型的[对数似然函数](@entry_id:168593)的梯度（或分数），这个梯度是否显著不为零？” 它仅需要拟合[零模型](@entry_id:181842)。

在标准[正则性条件](@entry_id:166962)下，当样本量 $n \to \infty$ 时，这三种检验统计量在零假设下都收敛于同一个卡方分布（本例中为 $\chi^2_1$），并且它们具有等价的局部功效。因此，它们是**[渐近等价](@entry_id:273818)的** [@problem_id:4916067]。

然而，在有限样本中，尤其是在具有挑战性的情况下，它们的表现可能会有显著差异。例如，在数据稀疏（事件数量少）的逻辑回归中：
*   **Wald 检验**可能表现不佳。其对[参数化](@entry_id:265163)的依赖性以及对 MLE $\hat{\beta}_j$ 及其[方差估计](@entry_id:268607)的稳定性问题，可能导致其真实的第一类错误率与名义水平有很大偏差。
*   **[似然比检验](@entry_id:268070)**和**分数检验**通常具有更好的小样本性质，因为它们对似然函数的几何形状更为敏感。
*   **分数检验**具有一个独特的实践优势：它不需要拟合完整模型。当完整模型的[最大似然估计](@entry_id:142509)由于（准）完全分离等问题而不存在时，只要[零模型](@entry_id:181842)是可估计的，分数检验仍然可以计算和执行 [@problem_id:4916067]。

理解这些原理及其差异，为生物统计学家在各种建模情景中选择最合适的推断工具提供了坚实的基础。