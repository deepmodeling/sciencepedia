{"hands_on_practices": [{"introduction": "回归系数通常是通过最小化一个损失函数来找到的。本练习将带您回到加权最小二乘法（WLS）的核心思想，这是一种处理观测值精度不等的异方差数据的重要方法。通过对一个小数据集应用微积分来最小化加权残差平方和 [@problem_id:4916039]，您将建立一个关于WLS估计量来源的坚实理解，而不仅仅是简单地套用公式。", "problem": "一位生物统计学家正在为两名独立患者建立生物标志物浓度 $x$ 与连续临床结局 $y$ 之间的关系模型，其中由于测定精度不同，测量误差是异方差的。在带截距的简单线性回归模型下，结局被建模为 $y_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\varepsilon_{i}$（$i=1,2$），其中独立的误差项 $\\varepsilon_{i}$ 满足 $\\mathbb{E}[\\varepsilon_{i} \\mid x_{i}] = 0$ 和 $\\operatorname{Var}(\\varepsilon_{i} \\mid x_{i}) \\propto 1/w_{i}$，其中 $w_{i}  0$ 是一个已知权重，与 $\\varepsilon_{i}$ 方差的倒数成正比。加权最小二乘（WLS）估计量被定义为使加权残差平方和 $\\sum_{i=1}^{2} w_{i} \\left(y_{i} - \\beta_{0} - \\beta_{1} x_{i}\\right)^{2}$ 最小化的值。\n\n给定数据向量 $w=(1,4)$，$x=(0,1)$ 和 $y=(1,3)$。仅从上述WLS定义和最小二乘法最小化的一般性质出发，计算该模型截距 $\\beta_{0}$ 和斜率 $\\beta_{1}$ 的WLS估计值。以有序对 $(\\beta_{0}, \\beta_{1})$ 的形式报告你的最终答案。无需四舍五入，最终答案中不应包含单位。", "solution": "问题陈述已经过验证，被认为是科学上可靠、提法恰当且客观的。它为一项标准的生物统计学计算提供了一个完整且一致的设置。\n\n目标是找到简单线性回归模型 $y_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\varepsilon_{i}$ 的截距 $\\beta_{0}$ 和斜率 $\\beta_{1}$ 的加权最小二乘（WLS）估计值。WLS估计值是使加权残差平方和函数 $S(\\beta_{0}, \\beta_{1})$ 最小化的 $\\beta_{0}$ 和 $\\beta_{1}$ 的值。\n\n加权残差平方和定义为：\n$$S(\\beta_{0}, \\beta_{1}) = \\sum_{i=1}^{2} w_{i} (y_{i} - \\beta_{0} - \\beta_{1} x_{i})^{2}$$\n\n问题提供了以下两名患者（$i=1, 2$）的数据：\n- 权重：$w = (w_{1}, w_{2}) = (1, 4)$\n- 生物标志物浓度：$x = (x_{1}, x_{2}) = (0, 1)$\n- 临床结局：$y = (y_{1}, y_{2}) = (1, 3)$\n\n将这些值代入 $S(\\beta_{0}, \\beta_{1})$ 的表达式中：\n$$S(\\beta_{0}, \\beta_{1}) = w_{1}(y_{1} - \\beta_{0} - \\beta_{1} x_{1})^{2} + w_{2}(y_{2} - \\beta_{0} - \\beta_{1} x_{2})^{2}$$\n$$S(\\beta_{0}, \\beta_{1}) = 1 \\cdot (1 - \\beta_{0} - \\beta_{1} \\cdot 0)^{2} + 4 \\cdot (3 - \\beta_{0} - \\beta_{1} \\cdot 1)^{2}$$\n$$S(\\beta_{0}, \\beta_{1}) = (1 - \\beta_{0})^{2} + 4(3 - \\beta_{0} - \\beta_{1})^{2}$$\n\n为了找到使该函数最小化的 $\\beta_{0}$ 和 $\\beta_{1}$ 的值，我们必须通过对 $\\beta_{0}$ 和 $\\beta_{1}$ 求偏导数并将其设为零来找到临界点。这将得到一个由两个线性方程组成的方程组，称为正规方程组。\n\n首先，我们计算关于 $\\beta_{0}$ 的偏导数：\n$$\\frac{\\partial S}{\\partial \\beta_{0}} = \\frac{\\partial}{\\partial \\beta_{0}} \\left[ (1 - \\beta_{0})^{2} + 4(3 - \\beta_{0} - \\beta_{1})^{2} \\right]$$\n$$\\frac{\\partial S}{\\partial \\beta_{0}} = 2(1 - \\beta_{0}) \\cdot (-1) + 4 \\cdot 2(3 - \\beta_{0} - \\beta_{1}) \\cdot (-1)$$\n$$\\frac{\\partial S}{\\partial \\beta_{0}} = -2(1 - \\beta_{0}) - 8(3 - \\beta_{0} - \\beta_{1})$$\n将此导数设为零以找到最小值：\n$$-2(1 - \\beta_{0}) - 8(3 - \\beta_{0} - \\beta_{1}) = 0$$\n两边除以 $-2$：\n$$(1 - \\beta_{0}) + 4(3 - \\beta_{0} - \\beta_{1}) = 0$$\n$$1 - \\beta_{0} + 12 - 4\\beta_{0} - 4\\beta_{1} = 0$$\n$$13 - 5\\beta_{0} - 4\\beta_{1} = 0$$\n这就得到了我们的第一个正规方程：\n$$5\\beta_{0} + 4\\beta_{1} = 13 \\quad (1)$$\n\n接下来，我们计算关于 $\\beta_{1}$ 的偏导数：\n$$\\frac{\\partial S}{\\partial \\beta_{1}} = \\frac{\\partial}{\\partial \\beta_{1}} \\left[ (1 - \\beta_{0})^{2} + 4(3 - \\beta_{0} - \\beta_{1})^{2} \\right]$$\n第一项 $(1 - \\beta_{0})^{2}$ 不依赖于 $\\beta_{1}$，因此其导数为零。\n$$\\frac{\\partial S}{\\partial \\beta_{1}} = 0 + 4 \\cdot 2(3 - \\beta_{0} - \\beta_{1}) \\cdot (-1)$$\n$$\\frac{\\partial S}{\\partial \\beta_{1}} = -8(3 - \\beta_{0} - \\beta_{1})$$\n将此导数设为零：\n$$-8(3 - \\beta_{0} - \\beta_{1}) = 0$$\n$$3 - \\beta_{0} - \\beta_{1} = 0$$\n这就得到了我们的第二个正规方程：\n$$\\beta_{0} + \\beta_{1} = 3 \\quad (2)$$\n\n现在我们必须解这个由两个线性方程组成的方程组：\n$$1) \\quad 5\\beta_{0} + 4\\beta_{1} = 13$$\n$$2) \\quad \\beta_{0} + \\beta_{1} = 3$$\n\n从方程（2）中，我们可以用 $\\beta_{1}$ 来表示 $\\beta_{0}$：\n$$\\beta_{0} = 3 - \\beta_{1}$$\n将这个 $\\beta_{0}$ 的表达式代入方程（1）：\n$$5(3 - \\beta_{1}) + 4\\beta_{1} = 13$$\n$$15 - 5\\beta_{1} + 4\\beta_{1} = 13$$\n$$15 - \\beta_{1} = 13$$\n$$\\beta_{1} = 15 - 13$$\n$$\\beta_{1} = 2$$\n\n最后，将 $\\beta_{1}$ 的值代回到 $\\beta_{0}$ 的表达式中：\n$$\\beta_{0} = 3 - \\beta_{1} = 3 - 2$$\n$$\\beta_{0} = 1$$\n\nWLS估计值为 $\\hat{\\beta}_{0} = 1$ 和 $\\hat{\\beta}_{1} = 2$。问题要求以有序对 $(\\beta_{0}, \\beta_{1})$ 的形式作答。", "answer": "$$\\boxed{\\begin{pmatrix} 1  2 \\end{pmatrix}}$$", "id": "4916039"}, {"introduction": "在对回归系数进行统计推断之前，我们必须确保其是唯一可被确定的，即“可识别的”。本练习将让您直面一个常见问题：模型中的预测变量完全相关，导致设计矩阵秩亏，使得单个系数无法被识别 [@problem_id:4916052]。解决这个问题对于学习如何诊断共线性，并理解哪些参数组合仍然具有明确且可估计的意义至关重要。", "problem": "一位生物统计学家将一个连续的生物标志物响应建模为两个连续暴露的线性函数，并带有一个加性复合项。对于受试者索引 $i \\in \\{1,\\dots,n\\}$，线性模型为\n$$\nY_{i} \\;=\\; \\beta_{0} + \\beta_{A}\\,A_{i} + \\beta_{B}\\,B_{i} + \\beta_{S}\\,(A_{i}+B_{i}) + \\varepsilon_{i},\n$$\n其误差项满足线性模型的标准高斯-马尔可夫条件：$\\mathbb{E}(\\varepsilon_{i}\\mid A_{i},B_{i})=0$, $\\operatorname{Var}(\\varepsilon_{i}\\mid A_{i},B_{i})=\\sigma^{2}$，以及当 $i\\ne j$ 时 $\\operatorname{Cov}(\\varepsilon_{i},\\varepsilon_{j}\\mid A_{i},B_{i},A_{j},B_{j})=0$。设设计矩阵为 $\\mathbf{X}=[\\mathbf{1},\\mathbf{A},\\mathbf{B},\\mathbf{A}+\\mathbf{B}]$，系数向量为 $\\boldsymbol{\\beta}=(\\beta_{0},\\beta_{A},\\beta_{B},\\beta_{S})^{\\top}$，其中 $\\mathbf{1}$ 是全为1的 $n$ 维向量，$\\mathbf{A},\\mathbf{B}$ 是观测到的暴露列向量。假设 $n\\geq 3$，且三个列向量 $\\mathbf{1}$、$\\mathbf{A}$ 和 $\\mathbf{B}$ 在 $\\mathbb{R}^{n}$ 中是线性无关的。\n\n从线性模型、普通最小二乘法（OLS; Ordinary Least Squares）的定义以及系数的线性泛函的可估性概念出发，确定 $\\mathbf{X}$ 的列秩，刻画可估线性组合 $c^{\\top}\\boldsymbol{\\beta}$ 的集合，并提出一个最小维度的重参数化 $\\boldsymbol{\\gamma}$，该重参数化表示为 $(\\beta_{0},\\beta_{A},\\beta_{B},\\beta_{S})$ 的线性函数，使得均值函数保持不变，且 $\\boldsymbol{\\gamma}$ 的所有分量都是可识别的。\n\n将您的最终答案以一个包含三个条目的单行矩阵形式报告，顺序如下：\n- $\\mathbf{X}$ 的秩，表示为单个整数，\n- 一个 $3\\times 4$ 矩阵，其行构成了可估系数泛函空间的一组基，\n- 一个 $3\\times 1$ 向量，给出您提出的重参数化 $\\boldsymbol{\\gamma}$，表示为 $(\\beta_{0},\\beta_{A},\\beta_{B},\\beta_{S})$ 的线性组合。\n\n无需进行数值四舍五入。所有量都以符号形式表示。", "solution": "该问题要求分析一个给定的线性回归模型，其特点是设计矩阵秩亏。分析内容包括确定设计矩阵的秩，刻画模型系数的可估线性函数集合，并提出一个最小维度的可识别重参数化。\n\n模型由下式给出\n$$\nY_{i} \\;=\\; \\beta_{0} + \\beta_{A}\\,A_{i} + \\beta_{B}\\,B_{i} + \\beta_{S}\\,(A_{i}+B_{i}) + \\varepsilon_{i}\n$$\n对于受试者 $i \\in \\{1, \\dots, n\\}$。用矩阵表示法，即为 $\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$，其中 $\\mathbf{Y}$ 是 $n \\times 1$ 的响应向量，$\\boldsymbol{\\beta} = (\\beta_{0}, \\beta_{A}, \\beta_{B}, \\beta_{S})^{\\top}$ 是 $4 \\times 1$ 的系数向量，$\\boldsymbol{\\varepsilon}$ 是满足高斯-马尔可夫条件的误差向量。$n \\times 4$ 的设计矩阵 $\\mathbf{X}$ 由其列向量指定：\n$$\n\\mathbf{X} = [\\mathbf{X}_1, \\mathbf{X}_2, \\mathbf{X}_3, \\mathbf{X}_4] = [\\mathbf{1}, \\mathbf{A}, \\mathbf{B}, \\mathbf{A}+\\mathbf{B}]\n$$\n其中 $\\mathbf{1}$、$\\mathbf{A}$ 和 $\\mathbf{B}$ 是 $n \\times 1$ 的列向量。\n\n首先，我们确定设计矩阵 $\\mathbf{X}$ 的列秩。矩阵的秩是其列空间的维度，等价于线性无关列的最大数量。通过观察 $\\mathbf{X}$ 的列，我们发现一个线性相关性：第四列是第二列和第三列的和。\n$$\n\\mathbf{X}_4 = \\mathbf{A} + \\mathbf{B} = 1 \\cdot \\mathbf{X}_2 + 1 \\cdot \\mathbf{X}_3\n$$\n这可以写成 $0 \\cdot \\mathbf{X}_1 + 1 \\cdot \\mathbf{X}_2 + 1 \\cdot \\mathbf{X}_3 - 1 \\cdot \\mathbf{X}_4 = \\mathbf{0}$，这明确表明 $\\mathbf{X}$ 的四个列是线性相关的。因此，$\\mathbf{X}$ 的秩必须小于 $4$。问题陈述假设三个列向量 $\\mathbf{1}$、$\\mathbf{A}$ 和 $\\mathbf{B}$ 是线性无关的。它们是 $\\mathbf{X}$ 的前三列。由于第四列 $\\mathbf{A}+\\mathbf{B}$ 位于前三列（特别是第2列和第3列）的生成空间中，它对增加列空间的维度没有贡献。$\\mathbf{X}$ 的列空间由 $\\{\\mathbf{1}, \\mathbf{A}, \\mathbf{B}\\}$ 生成。由于假设这个集合是线性无关的且大小为3，所以列空间的维度是3。因此，$\\mathbf{X}$ 的秩是3。\n\n其次，我们刻画可估线性组合 $c^{\\top}\\boldsymbol{\\beta}$ 的集合，其中 $c=(c_0, c_A, c_B, c_S)^{\\top}$。参数的线性组合 $c^{\\top}\\boldsymbol{\\beta}$ 被称为可估的，如果存在一个观测值的线性组合 $a^{\\top}\\mathbf{Y}$，是它的一个无偏估计量，即 $\\mathbb{E}[a^{\\top}\\mathbf{Y}] = c^{\\top}\\boldsymbol{\\beta}$。线性模型理论中的一个基本定理指出，$c^{\\top}\\boldsymbol{\\beta}$ 是可估的当且仅当向量 $c$ 位于设计矩阵 $\\mathbf{X}$ 的行空间中。$\\mathbf{X}$ 的行空间是其零空间 $N(\\mathbf{X})$ 的正交补。我们通过寻找 $N(\\mathbf{X})$ 的一组基来继续。一个向量 $v=(v_0, v_A, v_B, v_S)^{\\top}$ 属于 $N(\\mathbf{X})$ 如果 $\\mathbf{X}v = \\mathbf{0}$。\n$$\nv_0 \\mathbf{1} + v_A \\mathbf{A} + v_B \\mathbf{B} + v_S (\\mathbf{A}+\\mathbf{B}) = \\mathbf{0}\n$$\n重新整理这些项，我们得到：\n$$\nv_0 \\mathbf{1} + (v_A + v_S) \\mathbf{A} + (v_B + v_S) \\mathbf{B} = \\mathbf{0}\n$$\n因为向量 $\\mathbf{1}$、$\\mathbf{A}$ 和 $\\mathbf{B}$ 是线性无关的，所以这个线性组合中的系数必须全部为零。这得到方程组：\n$v_0 = 0$\n$v_A + v_S = 0 \\implies v_A = -v_S$\n$v_B + v_S = 0 \\implies v_B = -v_S$\n任何向量 $v \\in N(\\mathbf{X})$ 必须具有形式 $(0, -v_S, -v_S, v_S)^{\\top} = v_S(0, -1, -1, 1)^{\\top}$，其中 $v_S$ 是某个标量。因此，零空间 $N(\\mathbf{X})$ 是由向量 $k = (0, -1, -1, 1)^{\\top}$ 生成的一维空间。\n要使 $c$ 位于 $\\mathbf{X}$ 的行空间中，它必须与 $N(\\mathbf{X})$ 中的每个向量正交，这意味着 $c^{\\top}k = 0$。\n$$\nc^{\\top}k = (c_0, c_A, c_B, c_S) \\begin{pmatrix} 0 \\\\ -1 \\\\ -1 \\\\ 1 \\end{pmatrix} = -c_A - c_B + c_S = 0\n$$\n$c^{\\top}\\boldsymbol{\\beta}$ 可估的条件是 $c_S = c_A + c_B$。所有这些向量 $c$ 的集合构成了 $\\mathbb{R}^4$ 的一个 $3$ 维子空间。我们可以为这个子空间构建一组基。一个简单的基的选择是：\n$c^{(1)} = (1, 0, 0, 0)^{\\top}$，满足 $0 = 0+0$。\n$c^{(2)} = (0, 1, 0, 1)^{\\top}$，满足 $1 = 1+0$。\n$c^{(3)} = (0, 0, 1, 1)^{\\top}$，满足 $1 = 0+1$。\n这三个向量是线性无关的，并且生成了可估系数向量的空间。一个以这些向量为行的矩阵是：\n$$\n\\begin{pmatrix}\n1  0  0  0 \\\\\n0  1  0  1 \\\\\n0  0  1  1\n\\end{pmatrix}\n$$\n\n第三，我们提出了一个最小维度的重参数化。原始模型是不可识别的，因为设计矩阵 $\\mathbf{X}$ 不是满列秩的。我们寻求一个新的参数向量 $\\boldsymbol{\\gamma}$（维度为 $p=\\text{rank}(\\mathbf{X})=3$）和一个新的设计矩阵 $\\mathbf{Z}$，使得 $\\mathbb{E}[\\mathbf{Y}] = \\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{Z}\\boldsymbol{\\gamma}$ 并且 $\\mathbf{Z}$ 是满列秩的。这确保了 $\\boldsymbol{\\gamma}$ 的所有分量都是可识别的（因此也是可估的）。\n我们考察观测值 $i$ 的均值函数：\n$$\n\\mathbb{E}[Y_i] = \\beta_0 + \\beta_A A_i + \\beta_B B_i + \\beta_S(A_i+B_i)\n$$\n通过按协变量 $1$、$A_i$ 和 $B_i$ 重新组合各项，我们得到了一个提示重参数化方法的结构：\n$$\n\\mathbb{E}[Y_i] = \\beta_0 + (\\beta_A + \\beta_S) A_i + (\\beta_B + \\beta_S) B_i\n$$\n这导致了对新参数的以下选择：\n$\\gamma_0 = \\beta_0$\n$\\gamma_1 = \\beta_A + \\beta_S$\n$\\gamma_2 = \\beta_B + \\beta_S$\n新的参数向量是 $\\boldsymbol{\\gamma} = (\\gamma_0, \\gamma_1, \\gamma_2)^{\\top}$。均值模型变为 $\\mathbb{E}[Y_i] = \\gamma_0 + \\gamma_1 A_i + \\gamma_2 B_i$。相应的设计矩阵是 $\\mathbf{Z} = [\\mathbf{1}, \\mathbf{A}, \\mathbf{B}]$。根据假设，$\\mathbf{Z}$ 的列是线性无关的，所以 $\\mathbf{Z}$ 的秩为 $3$。由于 $\\mathbf{Z}$ 是一个秩为 $3$ 的 $n \\times 3$ 矩阵，它是满列秩的。$\\boldsymbol{\\gamma}$ 的维度是 $3$，这是一个可识别重参数化可能的最小维度。$\\boldsymbol{\\gamma}$ 的分量是构成一个自然基的具体的可估函数。新参数的向量表示为原始参数 $\\boldsymbol{\\beta}$ 的线性变换，如下所示：\n$$\n\\boldsymbol{\\gamma} = \\begin{pmatrix} \\beta_{0} \\\\ \\beta_{A} + \\beta_{S} \\\\ \\beta_{B} + \\beta_{S} \\end{pmatrix}\n$$\n这构成了一个满足所有要求的有效重参数化。", "answer": "$$\n\\boxed{\\begin{pmatrix} 3  \\begin{pmatrix} 1  0  0  0 \\\\ 0  1  0  1 \\\\ 0  0  1  1 \\end{pmatrix}  \\begin{pmatrix} \\beta_{0} \\\\ \\beta_{A} + \\beta_{S} \\\\ \\beta_{B} + \\beta_{S} \\end{pmatrix} \\end{pmatrix}}\n$$", "id": "4916052"}, {"introduction": "在生物统计学中，许多关键结果是二元的，例如患病与否。本练习将我们的重点从线性模型转移到逻辑回归，这是分析此类数据的核心工具 [@problem_id:4916063]。您将直接从二项分布的似然函数出发，推导对数优势比（log-odds ratio）的最大似然估计量（MLE）及其标准误，从而巩固您对广义线性模型中估计与推断背后理论基础的理解。", "problem": "一个生物统计学团队正在研究两个暴露组的二元结果 $Y \\in \\{0,1\\}$，这两个组由一个二元协变量 $x \\in \\{0,1\\}$ 索引。在 $x=0$ 组中，有 $n_0=50$ 个个体，观测到 $y_0=10$ 次成功，因此经验成功概率为 $\\hat{p}_0 = 10/50$。在 $x=1$ 组中，有 $n_1=50$ 个个体，观测到 $y_1=25$ 次成功，因此经验成功概率为 $\\hat{p}_1 = 25/50$。假设逻辑斯谛回归模型为 $\\log\\left(\\frac{p_x}{1-p_x}\\right) = \\alpha + \\beta x$，其中 $p_x = \\Pr(Y=1 \\mid x)$，并且假设两个组是独立的，且 $Y \\mid x \\sim \\text{Binomial}(n_x,p_x)$。\n\n从两个组的二项似然和 logit 连接函数的定义出发，推导对数优势比的最大似然估计量 $\\hat{\\beta}$ 以及基于观测 Fisher 信息的相应 Wald 标准误。然后使用上述数据计算其数值。将最终数值答案四舍五入到四位有效数字。最终答案必须是一个包含 $\\hat{\\beta}$ 及其 Wald 标准误的单一对象，如指定格式，不带单位。", "solution": "本题要求在一个针对两个独立组的逻辑斯谛回归模型中，推导对数优势比 $\\hat{\\beta}$ 的最大似然估计量 (MLE) 及其相应的 Wald 标准误，然后计算它们的数值。\n\n模型由 $\\log\\left(\\frac{p_x}{1-p_x}\\right) = \\alpha + \\beta x$ 给出，适用于二元协变量 $x \\in \\{0,1\\}$。这里，$p_x = \\Pr(Y=1 \\mid x)$。假设在大小为 $n_x$ 的组中，成功次数 $y_x$ 服从二项分布，$Y \\mid x \\sim \\text{Binomial}(n_x, p_x)$。\n\n对于 $x=0$，对数优势为 $\\log\\left(\\frac{p_0}{1-p_0}\\right) = \\alpha$。\n对于 $x=1$，对数优势为 $\\log\\left(\\frac{p_1}{1-p_1}\\right) = \\alpha + \\beta$。\n\n从这两个方程中，参数 $\\beta$ 可以表示为对数优势的差，即对数优势比：\n$$ \\beta = \\log\\left(\\frac{p_1}{1-p_1}\\right) - \\log\\left(\\frac{p_0}{1-p_0}\\right) = \\log\\left(\\frac{p_1/(1-p_1)}{p_0/(1-p_0)}\\right) $$\n\n在 $n_x$ 次试验中观测到 $y_x$ 次成功的似然由二项概率质量函数给出：\n$$ L(p_x; y_x) = \\binom{n_x}{y_x} p_x^{y_x} (1-p_x)^{n_x-y_x} $$\n由于两个组（$x=0$ 和 $x=1$）是独立的，总似然函数是各自似然的乘积：\n$$ L(\\alpha, \\beta; y_0, y_1) = L(p_0; y_0) L(p_1; y_1) $$\n忽略像 $\\binom{n_x}{y_x}$ 这样的常数项，对数似然函数为：\n$$ \\ell(\\alpha, \\beta) = y_0 \\log(p_0) + (n_0-y_0)\\log(1-p_0) + y_1 \\log(p_1) + (n_1-y_1)\\log(1-p_1) $$\n我们必须用 $\\alpha$ 和 $\\beta$ 来表示 $p_x$。根据模型定义，我们有 $p_x = \\frac{\\exp(\\alpha + \\beta x)}{1 + \\exp(\\alpha + \\beta x)}$。\n将此代入对数似然函数可得：\n$$ \\ell(\\alpha, \\beta) = y_0\\log\\left(\\frac{\\exp(\\alpha)}{1+\\exp(\\alpha)}\\right) + (n_0-y_0)\\log\\left(\\frac{1}{1+\\exp(\\alpha)}\\right) + y_1\\log\\left(\\frac{\\exp(\\alpha+\\beta)}{1+\\exp(\\alpha+\\beta)}\\right) + (n_1-y_1)\\log\\left(\\frac{1}{1+\\exp(\\alpha+\\beta)}\\right) $$\n化简后可得：\n$$ \\ell(\\alpha, \\beta) = y_0\\alpha - n_0\\log(1+\\exp(\\alpha)) + y_1(\\alpha+\\beta) - n_1\\log(1+\\exp(\\alpha+\\beta)) $$\n\n为了找到 MLE $\\hat{\\alpha}$ 和 $\\hat{\\beta}$，我们求 $\\ell$ 对 $\\alpha$ 和 $\\beta$ 的偏导数，并令它们为零。这些就是得分方程。\n$$ \\frac{\\partial \\ell}{\\partial \\alpha} = y_0 - n_0 \\frac{\\exp(\\alpha)}{1+\\exp(\\alpha)} + y_1 - n_1 \\frac{\\exp(\\alpha+\\beta)}{1+\\exp(\\alpha+\\beta)} = (y_0 - n_0 p_0) + (y_1 - n_1 p_1) = 0 $$\n$$ \\frac{\\partial \\ell}{\\partial \\beta} = y_1 - n_1 \\frac{\\exp(\\alpha+\\beta)}{1+\\exp(\\alpha+\\beta)} = y_1 - n_1 p_1 = 0 $$\n从第二个方程，我们立即发现在最大值处，$y_1 - n_1 \\hat{p}_1 = 0$，这得出了 $p_1$ 的 MLE：\n$$ \\hat{p}_1 = \\frac{y_1}{n_1} $$\n将此结果代入第一个方程，我们得到 $(y_0 - n_0 \\hat{p}_0) + (y_1 - n_1(y_1/n_1)) = 0$，化简为 $y_0 - n_0 \\hat{p}_0 = 0$。这得出了 $p_0$ 的 MLE：\n$$ \\hat{p}_0 = \\frac{y_0}{n_0} $$\n根据最大似然估计量的不变性，将 $p_0$ 和 $p_1$ 的 MLE 代入 $\\beta$ 的表达式中，即可得到 $\\beta$ 的 MLE：\n$$ \\hat{\\beta} = \\log\\left(\\frac{\\hat{p}_1/(1-\\hat{p}_1)}{\\hat{p}_0/(1-\\hat{p}_0)}\\right) = \\log\\left(\\frac{ (y_1/n_1)/(1-y_1/n_1) }{ (y_0/n_0)/(1-y_0/n_0) }\\right) $$\n\n接下来，我们推导 Wald 标准误。它是 $\\hat{\\beta}$ 的估计方差的平方根，该方差由观测 Fisher 信息矩阵的逆矩阵求得。观测 Fisher 信息矩阵 $I(\\hat{\\alpha}, \\hat{\\beta})$ 是对数似然函数的海森矩阵（在 MLE 处取值）的负数。\n海森矩阵 $H$ 包含 $\\ell(\\alpha, \\beta)$ 的二阶偏导数：\n$$ H = \\begin{pmatrix} \\frac{\\partial^2 \\ell}{\\partial \\alpha^2}  \\frac{\\partial^2 \\ell}{\\partial \\alpha \\partial \\beta} \\\\ \\frac{\\partial^2 \\ell}{\\partial \\beta \\partial \\alpha}  \\frac{\\partial^2 \\ell}{\\partial \\beta^2} \\end{pmatrix} $$\n这些导数是：\n$$ \\frac{\\partial^2 \\ell}{\\partial \\alpha^2} = \\frac{\\partial}{\\partial \\alpha}(y_0-n_0p_0+y_1-n_1p_1) = -n_0 \\frac{\\partial p_0}{\\partial \\alpha} - n_1 \\frac{\\partial p_1}{\\partial \\alpha} $$\n利用 $\\frac{\\partial p_x}{\\partial (\\alpha+\\beta x)} = p_x(1-p_x)$ 这一事实，我们有 $\\frac{\\partial p_0}{\\partial \\alpha} = p_0(1-p_0)$ 和 $\\frac{\\partial p_1}{\\partial \\alpha} = p_1(1-p_1)$。\n$$ \\frac{\\partial^2 \\ell}{\\partial \\alpha^2} = -n_0 p_0(1-p_0) - n_1 p_1(1-p_1) $$\n$$ \\frac{\\partial^2 \\ell}{\\partial \\beta^2} = \\frac{\\partial}{\\partial \\beta}(y_1-n_1p_1) = -n_1 \\frac{\\partial p_1}{\\partial \\beta} = -n_1 p_1(1-p_1) $$\n$$ \\frac{\\partial^2 \\ell}{\\partial \\alpha \\partial \\beta} = \\frac{\\partial}{\\partial \\alpha}(y_1-n_1p_1) = -n_1 \\frac{\\partial p_1}{\\partial \\alpha} = -n_1 p_1(1-p_1) $$\n观测 Fisher 信息矩阵是 $I(\\hat{\\alpha}, \\hat{\\beta}) = -H(\\hat{\\alpha}, \\hat{\\beta})$：\n$$ I(\\hat{\\alpha}, \\hat{\\beta}) = \\begin{pmatrix} n_0 \\hat{p}_0(1-\\hat{p}_0) + n_1 \\hat{p}_1(1-\\hat{p}_1)  n_1 \\hat{p}_1(1-\\hat{p}_1) \\\\ n_1 \\hat{p}_1(1-\\hat{p}_1)  n_1 \\hat{p}_1(1-\\hat{p}_1) \\end{pmatrix} $$\n$(\\hat{\\alpha}, \\hat{\\beta})$ 的渐近方差-协方差矩阵是该矩阵的逆矩阵，$V = I^{-1}$。对于一个 $2 \\times 2$ 矩阵 $\\begin{pmatrix} a  b \\\\ b  c \\end{pmatrix}$，其逆矩阵为 $\\frac{1}{ac-b^2}\\begin{pmatrix} c  -b \\\\ -b  a \\end{pmatrix}$。\n令 $A = n_0 \\hat{p}_0(1-\\hat{p}_0)$ 且 $B = n_1 \\hat{p}_1(1-\\hat{p}_1)$。则 $I = \\begin{pmatrix} A+B  B \\\\ B  B \\end{pmatrix}$。\n行列式为 $\\det(I) = (A+B)B - B^2 = AB$。\n逆矩阵为：\n$$ V = I^{-1} = \\frac{1}{AB} \\begin{pmatrix} B  -B \\\\ -B  A+B \\end{pmatrix} = \\begin{pmatrix} 1/A  -1/A \\\\ -1/A  1/A + 1/B \\end{pmatrix} $$\n$\\hat{\\beta}$ 的方差是该矩阵的第 $(2,2)$ 个元素：\n$$ \\text{Var}(\\hat{\\beta}) = V_{22} = \\frac{1}{A} + \\frac{1}{B} = \\frac{1}{n_0 \\hat{p}_0(1-\\hat{p}_0)} + \\frac{1}{n_1 \\hat{p}_1(1-\\hat{p}_1)} $$\nWald 标准误 (SE) 是该方差的平方根：\n$$ \\text{SE}(\\hat{\\beta}) = \\sqrt{\\frac{1}{n_0 \\hat{p}_0(1-\\hat{p}_0)} + \\frac{1}{n_1 \\hat{p}_1(1-\\hat{p}_1)}} $$\n\n现在，我们使用所提供的数据计算数值：\n$n_0=50$, $y_0=10$。\n$n_1=50$, $y_1=25$。\n\n首先，计算概率的 MLE：\n$$ \\hat{p}_0 = \\frac{10}{50} = 0.2 $$\n$$ \\hat{p}_1 = \\frac{25}{50} = 0.5 $$\n\n接下来，计算 $\\hat{\\beta}$：\n$$ \\hat{\\beta} = \\log\\left(\\frac{0.5/(1-0.5)}{0.2/(1-0.2)}\\right) = \\log\\left(\\frac{0.5/0.5}{0.2/0.8}\\right) = \\log\\left(\\frac{1}{0.25}\\right) = \\log(4) $$\n$$ \\hat{\\beta} \\approx 1.386294... $$\n四舍五入到四位有效数字，$\\hat{\\beta} \\approx 1.386$。\n\n最后，计算 $\\hat{\\beta}$ 的标准误：\n$$ \\text{Var}(\\hat{\\beta}) = \\frac{1}{50 \\times 0.2 \\times (1-0.2)} + \\frac{1}{50 \\times 0.5 \\times (1-0.5)} $$\n$$ \\text{Var}(\\hat{\\beta}) = \\frac{1}{50 \\times 0.16} + \\frac{1}{50 \\times 0.25} = \\frac{1}{8} + \\frac{1}{12.5} $$\n$$ \\text{Var}(\\hat{\\beta}) = 0.125 + 0.08 = 0.205 $$\n$$ \\text{SE}(\\hat{\\beta}) = \\sqrt{0.205} \\approx 0.452769... $$\n四舍五入到四位有效数字，$\\text{SE}(\\hat{\\beta}) \\approx 0.4528$。\n\n推导和计算出的值为 $\\hat{\\beta} \\approx 1.386$ 和 $\\text{SE}(\\hat{\\beta}) \\approx 0.4528$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1.386  0.4528\n\\end{pmatrix}\n}\n$$", "id": "4916063"}]}