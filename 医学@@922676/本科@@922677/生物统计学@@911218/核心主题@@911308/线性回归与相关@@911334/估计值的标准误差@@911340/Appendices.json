{"hands_on_practices": [{"introduction": "估计标准误 $\\hat{\\sigma}$ 不仅是衡量模型拟合优度的指标，更是检验模型基本假设的重要工具。本练习将指导您利用正态分布的理论性质，来评估模型残差的正态性假设是否成立 [@problem_id:4953182]。通过计算并比较“大”残差的观测数量与期望数量，您将学会如何将理论概率知识应用于实际的模型诊断，从而加深对模型假设检验的理解。", "problem": "一个生物统计学团队拟合了一个普通最小二乘（OLS）多元线性回归模型，以年龄、身体质量指数和性别为预测变量，来模拟一个包含 $n = 200$ 名成年人的队列中经对数转换后的C反应蛋白。根据拟合模型的残差 $e_{i}$ 计算出的估计标准误，记为 $\\hat{\\sigma}$，在对数尺度上等于 $\\hat{\\sigma} = 0.62$。在标准线性模型假设下，残差是独立同分布的，均值为 $0$，方差为 $\\sigma^{2}$，且 $\\hat{\\sigma}$ 是 $\\sigma$ 的一致估计量。在观测数据中，有 $O = 15$ 个残差满足 $|e_{i}| > 2\\hat{\\sigma}$。\n\n从残差的定义和标准正态分布的累积分布函数（CDF）出发，并且不假设任何特定的数值尾部概率，按以下步骤进行：\n\n1) 仅使用残差的正态性和独立性假设，以及 $\\hat{\\sigma}$ 估计 $\\sigma$ 这一事实，推导绝对值大于 $2\\hat{\\sigma}$ 的残差数量的期望值 $E$ 的精确表达式，用标准正态CDF表示。\n\n2) 将事件 $\\{|e_{i}| > 2\\hat{\\sigma}\\}$ 的指示变量建模为伯努利随机变量，并推导超出次数的抽样方差。由此，构建标准化偏差\n$$\nZ \\;=\\; \\frac{O - E}{\\sqrt{\\operatorname{Var}(X)}},\n$$\n其中 $X$ 是满足 $|e_{i}| > 2\\hat{\\sigma}$ 的残差计数。\n\n3) 在正态性假设下，对 $Z$ 进行数值计算。将您的答案四舍五入到四位有效数字。将最终答案表示为无单位的纯数。", "solution": "该问题被评估为有效。它在科学上基于标准统计理论，内部一致，并且在明确说明的假设下是适定的。问题提供了进行唯一解答所需的所有必要数据。我们现在将按照要求分三部分推导解答。\n\n问题的核心是评估观测到的大残差数量 $O=15$ 是否与残差服从正态分布的假设相一致。我们给定样本大小 $n=200$，估计标准误为 $\\hat{\\sigma} = 0.62$。残差 $e_i$ 被假设为独立同分布（i.i.d.）于 $N(0, \\sigma^2)$，其中 $\\hat{\\sigma}$ 是 $\\sigma$ 的一致估计量。\n\n首先，我们推导绝对值超过估计标准误两倍的残差的期望数量 $E$ 的精确表达式。令 $I_i$ 为事件 $|e_i| > 2\\hat{\\sigma}$ 的指示随机变量，其中 $i=1, 2, \\dots, n$。即，\n$$ I_i = \\begin{cases} 1  \\text{if } |e_i| > 2\\hat{\\sigma} \\\\ 0  \\text{if } |e_i| \\le 2\\hat{\\sigma} \\end{cases} $$\n这类残差的总数是一个随机变量 $X = \\sum_{i=1}^{n} I_i$。超出次数的期望数量 $E$ 是 $X$ 的期望。根据期望的线性性质，\n$$ E = E[X] = E\\left[\\sum_{i=1}^{n} I_i\\right] = \\sum_{i=1}^{n} E[I_i] $$\n指示变量的期望是它所指示事件的概率。令这个概率为 $p$。\n$$ E[I_i] = P(|e_i| > 2\\hat{\\sigma}) = p $$\n由于残差被假设为同分布，这个概率 $p$ 对所有 $i$ 都是相同的。因此，期望数量为 $E = np$。\n\n为了计算 $p$，我们使用正态性假设，$e_i \\sim N(0, \\sigma^2)$。除以 $\\sigma$，我们得到 $e_i/\\sigma$ 服从标准正态分布 $N(0, 1)$。问题指出 $\\hat{\\sigma}$ 是 $\\sigma$ 的一致估计量。对于大样本量（$n=200$），我们可以用其估计值 $\\hat{\\sigma}$ 来近似 $\\sigma$ 以计算概率。\n$$ p = P(|e_i| > 2\\hat{\\sigma}) \\approx P\\left(\\left|\\frac{e_i}{\\sigma}\\right| > 2\\right) $$\n令 $Z_{std}$ 为一个服从标准正态分布的随机变量，$Z_{std} \\sim N(0, 1)$。令 $\\Phi(z)$ 为其累积分布函数（CDF），即 $\\Phi(z) = P(Z_{std} \\le z)$。概率 $p$ 可以表示为：\n$$ p = P(|Z_{std}| > 2) = P(Z_{std} > 2) + P(Z_{std}  -2) $$\n利用CDF的性质和正态分布关于 $0$ 的对称性，我们有：\n$$ P(Z_{std} > 2) = 1 - P(Z_{std} \\le 2) = 1 - \\Phi(2) $$\n$$ P(Z_{std}  -2) = \\Phi(-2) = 1 - \\Phi(2) $$\n因此，概率 $p$ 为：\n$$ p = [1 - \\Phi(2)] + [1 - \\Phi(2)] = 2(1 - \\Phi(2)) $$\n将 $p$ 的这个表达式代入 $E$ 的公式，我们得到用 $n$ 和标准正态CDF表示的超出次数的期望数量的精确表达式：\n$$ E = np = 2n(1 - \\Phi(2)) $$\n\n其次，我们对超出次数进行建模，并推导标准化偏差 $Z$。指示变量 $I_i$ 是成功概率为 $p = 2(1 - \\Phi(2))$ 的伯努利随机变量。由于残差 $e_i$ 被假设为独立的，因此指示变量 $I_i$ 也是独立的。超出次数 $X = \\sum_{i=1}^{n} I_i$ 是 $n$ 次独立同分布的伯努利试验的总和。因此，$X$ 服从二项分布，$X \\sim \\text{Bin}(n, p)$。\n二项随机变量的方差由 $\\operatorname{Var}(X) = np(1-p)$ 给出。代入 $p$ 的表达式：\n$$ \\operatorname{Var}(X) = n [2(1 - \\Phi(2))] [1 - 2(1 - \\Phi(2))] = 2n(1 - \\Phi(2))(2\\Phi(2) - 1) $$\n标准化偏差 $Z$ 定义为观测计数 $O$ 与期望计数 $E$ 之差，再除以计数的标准差。\n$$ Z = \\frac{O - E}{\\sqrt{\\operatorname{Var}(X)}} $$\n代入推导出的 $E$ 和 $\\operatorname{Var}(X)$ 的表达式，我们得到：\n$$ Z = \\frac{O - 2n(1 - \\Phi(2))}{\\sqrt{2n(1 - \\Phi(2))(2\\Phi(2) - 1)}} $$\n\n第三，我们对 $Z$ 进行数值计算。给定的值为样本量 $n=200$ 和观测到的超出次数 $O=15$。我们使用正态CDF在 $z=2$ 处的标准值，即 $\\Phi(2) \\approx 0.97724987$。\n首先，我们计算概率 $p$：\n$$ p = 2(1 - \\Phi(2)) \\approx 2(1 - 0.97724987) = 2(0.02275013) = 0.04550026 $$\n接下来，我们计算超出次数的期望数量 $E$：\n$$ E = np \\approx 200 \\times 0.04550026 = 9.100052 $$\n然后，我们计算计数的方差 $\\operatorname{Var}(X)$：\n$$ \\operatorname{Var}(X) = np(1-p) \\approx 9.100052 \\times (1 - 0.04550026) = 9.100052 \\times 0.95449974 \\approx 8.68594 $$\n标准差是方差的平方根：\n$$ \\sqrt{\\operatorname{Var}(X)} \\approx \\sqrt{8.68594} \\approx 2.947192 $$\n最后，我们计算 $Z$ 统计量的值：\n$$ Z = \\frac{O - E}{\\sqrt{\\operatorname{Var}(X)}} \\approx \\frac{15 - 9.100052}{2.947192} = \\frac{5.899948}{2.947192} \\approx 2.00190 $$\n根据要求将结果四舍五入到四位有效数字，得到 $Z = 2.002$。该值表明，绝对值大于 $2\\hat{\\sigma}$ 的观测残差数量比在正态性假设下预期的数量高出约 $2$ 个标准差。", "answer": "$$\\boxed{2.002}$$", "id": "4953182"}, {"introduction": "在真实的生物统计学研究中，数据缺失是一个普遍存在的问题，它使得包括估计标准误在内的模型参数估计变得复杂。本练习将引导您推导并实现期望最大化（EM）算法，这是一种在数据不完整时进行最大似然估计的强大迭代方法 [@problem_id:4953183]。通过这个实践，您不仅能掌握处理缺失数据的核心技术，还能深刻理解估计标准误 $\\hat{\\sigma}$ 在复杂情景下的理论基础和计算过程。", "problem": "考虑具有独立误差的正态线性模型。令 $y \\in \\mathbb{R}^{n}$ 表示响应向量，$X \\in \\mathbb{R}^{n \\times p}$ 为满列秩的设计矩阵，$\\beta \\in \\mathbb{R}^{p}$ 为回归系数，$\\sigma^{2} \\in \\mathbb{R}_{+}$ 为误差方差。假设抽样模型为 $y \\mid X, \\beta, \\sigma^{2} \\sim \\mathcal{N}(X \\beta, \\sigma^{2} I_{n})$，其中 $I_{n}$ 是 $n \\times n$ 的单位矩阵。响应的一个子集是完全随机缺失的，并用非数字值 $\\mathrm{NaN}$ 表示。任务是推导并实现一个期望最大化（EM, Expectation-Maximization）过程，以在存在缺失响应的情况下，计算残差标准差（通常称为估计的标准误差）的最大似然估计 $\\hat{\\sigma} = \\sqrt{\\hat{\\sigma}^{2}}$。\n\n您的推导必须从以下基础开始：\n- 多元正态模型的完整数据对数似然，以及EM算法的定义（在给定观测数据和当前参数下，对缺失数据的条件分布，最大化完整数据对数似然的期望）。\n- 最小二乘法的线性代数恒等式，以及对于 $z \\sim \\mathcal{N}(\\mu, \\tau^{2})$，对任意实数 $a$ 有 $\\mathbb{E}\\left[(z - a)^{2}\\right] = \\tau^{2} + (\\mu - a)^{2}$ 这一事实。\n\n您必须：\n- 在此缺失响应的设置中，在EM框架内推导 $\\beta$ 和 $\\sigma^{2}$ 的更新方程。您的推导应明确说明对缺失响应的期望是如何进入每次迭代中最大化的目标函数，并最终给出用 $X$、 $y$ 的观测分量、当前迭代值 $(\\beta^{(t)}, \\sigma^{2\\,(t)})$ 以及缺失响应数量等计数表示的 $\\beta$ 和 $\\sigma^{2}$ 的显式更新规则。\n- 解释在收敛时如何从 $\\hat{\\sigma}^{2}$ 获得估计的标准误差 $\\hat{\\sigma}$。\n- 解释在没有缺失响应时，这些更新如何简化为通常的完整数据最大似然估计量。\n\n然后实现一个程序，执行EM迭代直至收敛，从 $\\beta^{(0)} = \\mathbf{0}_{p}$ 和 $\\sigma^{2\\,(0)} = 1$ 开始，使用以下收敛准则：当 $\\sigma^{2}$ 的相对变化和 $\\beta$ 的相对变化（欧几里得范数）的最大值小于 $10^{-10}$ 时，或当执行了 $10{,}000$ 次迭代时停止，以先发生者为准。如果问题从观测数据中无法识别（例如，零个观测响应或导致观测数据正规方程奇异的设计），您的实现应为该测试用例的 $\\hat{\\sigma}$ 返回非数字值 $\\mathrm{NaN}$。\n\n测试套件：\n- 案例1（一条直线，一个缺失响应）：\n  - $X^{(1)} = \\begin{bmatrix} 1  0 \\\\ 1  1 \\\\ 1  2 \\\\ 1  3 \\\\ 1  4 \\end{bmatrix}$，\n  - $y^{(1)} = \\begin{bmatrix} 2.1 \\\\ 2.4 \\\\ 3.2 \\\\ \\mathrm{NaN} \\\\ 3.9 \\end{bmatrix}$。\n- 案例2（无缺失响应）：\n  - $X^{(2)} = \\begin{bmatrix} 1  -2 \\\\ 1  -1 \\\\ 1  0 \\\\ 1  1 \\\\ 1  2 \\end{bmatrix}$，\n  - $y^{(2)} = \\begin{bmatrix} -1.5 \\\\ -0.4 \\\\ 0.3 \\\\ 1.6 \\\\ 2.5 \\end{bmatrix}$。\n- 案例3（两个预测变量，一个缺失响应）：\n  - $X^{(3)} = \\begin{bmatrix} 1  0  1 \\\\ 1  1  2 \\\\ 1  2  0 \\\\ 1  3  1 \\\\ 1  4  3 \\\\ 1  5  5 \\end{bmatrix}$，\n  - $y^{(3)} = \\begin{bmatrix} 0.8 \\\\ 1.2 \\\\ 1.95 \\\\ \\mathrm{NaN} \\\\ 2.3 \\\\ 2.55 \\end{bmatrix}$。\n- 案例4（边界情况，观测响应上的残差为零，导致 $\\hat{\\sigma} = 0$）：\n  - $X^{(4)} = \\begin{bmatrix} 1  0 \\\\ 1  1 \\\\ 1  2 \\end{bmatrix}$，\n  - $y^{(4)} = \\begin{bmatrix} 1.0 \\\\ 3.0 \\\\ \\mathrm{NaN} \\end{bmatrix}$。\n\n您的程序必须：\n- 实现您推导的EM更新，以估计每个案例的 $\\hat{\\sigma}$。\n- 使用上述确切的测试用例，直接嵌入程序中（无输入）。\n- 生成单行输出，其中包含一个用方括号括起来的、由逗号分隔的四个 $\\hat{\\sigma}$ 值的列表，每个值四舍五入到六位小数，例如 $\\left[0.123456,1.234568,0.000000,2.000000\\right]$。\n\n每个测试用例的最终输出必须是一个浮点数。不涉及物理单位或角度单位。", "solution": "用户提供的问题经评估是有效的。这是一个计算统计学中适定的问题，基于最大似然估计和期望最大化（EM）算法的既定原则。所有术语都经过了正式定义，设置是自洽和一致的，测试用例结构良好。\n\n### EM 算法的推导\n\n目标是为正态线性模型 $y \\sim \\mathcal{N}(X\\beta, \\sigma^2 I_n)$ 找到参数 $\\theta = (\\beta, \\sigma^2)$ 的最大似然估计（MLE），其中响应向量 $y$ 的某些分量是缺失的。我们将观测到的分量表示为 $y_{obs}$，缺失的分量表示为 $y_{mis}$。EM算法提供了一个迭代过程，通过重复最大化完整数据对数似然的期望，来最大化观测数据的对数似然 $\\ell(\\theta; y_{obs}) = \\log p(y_{obs}|\\theta)$。\n\n完整数据向量是 $y \\in \\mathbb{R}^n$，划分为 $y_{obs}$ 和 $y_{mis}$。令 $O$ 和 $M$ 分别为观测数据和缺失数据的索引集。观测总数为 $n = |O| + |M| = n_{obs} + n_{mis}$。\n\n完整数据的对数似然，忽略与 $\\theta$ 无关的常数，为：\n$$ \\ell(\\theta; y) = -\\frac{n}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} (y - X\\beta)^T(y - X\\beta) $$\n$$ \\ell(\\theta; y) = -\\frac{n}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - x_i^T\\beta)^2 $$\n其中 $x_i^T$ 是设计矩阵 $X$ 的第 $i$ 行。\n\nEM算法包括两个步骤：期望（E-step）和最大化（M-step）。\n\n#### E-步骤\n在 E-步骤中，于第 $t$ 次迭代，我们计算在给定观测数据 $y_{obs}$ 和当前参数估计值 $\\theta^{(t)} = (\\beta^{(t)}, \\sigma^{2,(t)})$ 的条件下，完整数据对数似然的期望。此函数表示为 $Q(\\theta | \\theta^{(t)})$。\n$$ Q(\\theta | \\theta^{(t)}) = \\mathbb{E}_{y_{mis} | y_{obs}, \\theta^{(t)}}[\\ell(\\theta; y)] $$\n代入对数似然表达式：\n$$ Q(\\theta | \\theta^{(t)}) = \\mathbb{E}_{y_{mis} | y_{obs}, \\theta^{(t)}} \\left[ -\\frac{n}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - x_i^T\\beta)^2 \\right] $$\n项 $-\\frac{n}{2} \\log(\\sigma^2)$ 相对于期望是常数。我们只需要计算平方和项的期望。\n$$ Q(\\theta | \\theta^{(t)}) = -\\frac{n}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\mathbb{E}_{y_{mis} | y_{obs}, \\theta^{(t)}} \\left[ \\sum_{i=1}^{n} (y_i - x_i^T\\beta)^2 \\right] $$\n求和可以分解为对观测索引和缺失索引的求和：\n$$ \\sum_{i=1}^{n} (y_i - x_i^T\\beta)^2 = \\sum_{i \\in O} (y_i - x_i^T\\beta)^2 + \\sum_{i \\in M} (y_i - x_i^T\\beta)^2 $$\n第一项仅涉及观测数据，在期望中被视为常数。第二项的期望是针对 $y_{mis}$ 的分布计算的。由于模型假设误差独立，在给定 $\\beta$ 和 $\\sigma^2$ 的情况下，$y_i$ 是独立的。因此，$y_{mis}$ 的分布独立于 $y_{obs}$。\n$$ y_i | \\theta^{(t)} \\sim \\mathcal{N}(x_i^T\\beta^{(t)}, \\sigma^{2,(t)}) \\quad \\text{对于 } i \\in M $$\n我们需要为 $i \\in M$ 计算 $\\mathbb{E}_{y_i|\\theta^{(t)}}[(y_i - x_i^T\\beta)^2]$。使用给定的恒等式 $\\mathbb{E}[(z - a)^2] = \\text{Var}(z) + (\\mathbb{E}[z] - a)^2$，其中 $z=y_i$，$\\mathbb{E}[z] = x_i^T\\beta^{(t)}$，$\\text{Var}(z) = \\sigma^{2,(t)}$，且 $a=x_i^T\\beta$，我们得到：\n$$ \\mathbb{E}_{y_i|\\theta^{(t)}}[(y_i - x_i^T\\beta)^2] = \\sigma^{2,(t)} + (x_i^T\\beta^{(t)} - x_i^T\\beta)^2 $$\n将此代回 $Q(\\theta|\\theta^{(t)})$ 的表达式中：\n$$ Q(\\theta | \\theta^{(t)}) = -\\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\left[ \\sum_{i \\in O} (y_i - x_i^T\\beta)^2 + \\sum_{i \\in M} \\left(\\sigma^{2,(t)} + (x_i^T\\beta^{(t)} - x_i^T\\beta)^2\\right) \\right] $$\n\n#### M-步骤\n在 M-步骤中，我们寻找使 $Q(\\theta | \\theta^{(t)})$ 最大化的参数 $\\theta^{(t+1)} = (\\beta^{(t+1)}, \\sigma^{2,(t+1)})$。这通过对 $\\beta$ 和 $\\sigma^2$ 求导并令其为零来实现。\n\n**$\\beta$ 的更新**：\n为了找到 $\\beta^{(t+1)}$，我们最小化 $Q$ 中依赖于 $\\beta$ 的项：\n$$ \\arg\\min_{\\beta} \\left[ \\sum_{i \\in O} (y_i - x_i^T\\beta)^2 + \\sum_{i \\in M} (x_i^T\\beta^{(t)} - x_i^T\\beta)^2 \\right] $$\n我们定义一个“补全的”响应向量 $y^{(t)}$，其中缺失值用它们当前的条件期望填充：\n$$ y_i^{(t)} = \\begin{cases} y_i  \\text{if } i \\in O \\\\ \\mathbb{E}[y_i | y_{obs}, \\theta^{(t)}] = x_i^T \\beta^{(t)}  \\text{if } i \\in M \\end{cases} $$\n为 $\\beta$ 最小化的表达式变为：\n$$ \\sum_{i \\in O} (y_i^{(t)} - x_i^T\\beta)^2 + \\sum_{i \\in M} (y_i^{(t)} - x_i^T\\beta)^2 = \\sum_{i=1}^n (y_i^{(t)} - x_i^T\\beta)^2 = (y^{(t)} - X\\beta)^T(y^{(t)} - X\\beta) $$\n这是一个标准的普通最小二乘（OLS）问题。解为：\n$$ \\beta^{(t+1)} = (X^T X)^{-1} X^T y^{(t)} $$\n这一步保证是适定的，因为问题陈述中 $X$ 具有满列秩。\n\n**$\\sigma^2$ 的更新**：\n为了找到 $\\sigma^{2,(t+1)}$，我们固定 $\\beta = \\beta^{(t+1)}$ 并对 $\\sigma^2$ 最大化 $Q$。令 $S(\\beta) = \\sum_{i \\in O} (y_i - x_i^T\\beta)^2 + \\sum_{i \\in M} (x_i^T\\beta^{(t)} - x_i^T\\beta)^2$。\n我们最大化：\n$$ -\\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\left[ S(\\beta^{(t+1)}) + n_{mis}\\sigma^{2,(t)} \\right] $$\n对 $\\sigma^2$ 求导并令其为零，得到：\n$$ -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\left[ S(\\beta^{(t+1)}) + n_{mis}\\sigma^{2,(t)} \\right] = 0 $$\n$$ \\sigma^{2,(t+1)} = \\frac{1}{n} \\left[ S(\\beta^{(t+1)}) + n_{mis}\\sigma^{2,(t)} \\right] $$\n我们使用 $y^{(t)}$ 的定义来展开 $S(\\beta^{(t+1)})$：\n$S(\\beta^{(t+1)}) = (y^{(t)} - X\\beta^{(t+1)})^T(y^{(t)} - X\\beta^{(t+1)})$，即 $\\sum_{i \\in O} (y_i - x_i^T\\beta^{(t+1)})^2 + \\sum_{i \\in M} (x_i^T\\beta^{(t)} - x_i^T\\beta^{(t+1)})^2$。\n因此，$\\sigma^2$ 的更新规则是：\n$$ \\sigma^{2,(t+1)} = \\frac{1}{n} \\left[ \\sum_{i \\in O} (y_i - x_i^T\\beta^{(t+1)})^2 + \\sum_{i \\in M} (x_i^T\\beta^{(t)} - x_i^T\\beta^{(t+1)})^2 + n_{mis}\\sigma^{2,(t)} \\right] $$\n\n### EM 算法总结\n1.  **初始化**：设置初始值 $\\beta^{(0)} = \\mathbf{0}_p$ 和 $\\sigma^{2,(0)} = 1$。\n2.  **迭代**：对于 $t = 0, 1, 2, \\dots$ 直到收敛：\n    a.  **E-步骤**：通过填补缺失值来构建补全的响应向量 $y^{(t)}$：\n        $y_i^{(t)} = y_i$ 对于 $i \\in O$，且 $y_i^{(t)} = x_i^T\\beta^{(t)}$ 对于 $i \\in M$。\n    b.  **M-步骤**：更新参数估计：\n        -   $\\beta^{(t+1)} = (X^T X)^{-1} X^T y^{(t)}$\n        -   $\\sigma^{2,(t+1)} = \\frac{1}{n} \\left( \\sum_{i \\in O} (y_i - x_i^T\\beta^{(t+1)})^2 + \\sum_{i \\in M} (x_i^T\\beta^{(t)} - x_i^T\\beta^{(t+1)})^2 + n_{mis}\\sigma^{2,(t)} \\right)$\n3.  **收敛**：当参数 $(\\beta, \\sigma^2)$ 稳定时，算法收敛。令 $(\\hat{\\beta}, \\hat{\\sigma}^2)$ 为收敛时的估计值。残差标准差的最终估计值为 $\\hat{\\sigma} = \\sqrt{\\hat{\\sigma}^2}$。\n\n### 简化为完整数据情况\n如果没有缺失的响应（$n_{mis} = 0$），那么对所有 $t$ 都有 $y^{(t)} = y$。E-步骤是平凡的。\nM-步骤的更新变为：\n-   $\\beta^{(1)} = (X^T X)^{-1} X^T y = \\hat{\\beta}_{MLE}$。此值在所有后续迭代中保持不变。\n-   $\\sigma^{2,(1)} = \\frac{1}{n} \\left( \\sum_{i \\in O} (y_i - x_i^T\\hat{\\beta}_{MLE})^2 + 0 + 0 \\right) = \\frac{1}{n} (y - X\\hat{\\beta}_{MLE})^T(y - X\\hat{\\beta}_{MLE}) = \\hat{\\sigma}^2_{MLE}$。\n算法在一次迭代中收敛到具有完整数据的正态线性模型的标准最大似然估计。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n\n    def run_em_for_sigma(X, y, tol=1e-10, max_iter=10000):\n        \"\"\"\n        Implements the EM algorithm to estimate the residual standard deviation.\n\n        Args:\n            X (np.ndarray): The design matrix of shape (n, p).\n            y (np.ndarray): The response vector of shape (n,), possibly containing NaNs.\n            tol (float): The convergence tolerance.\n            max_iter (int): The maximum number of iterations.\n\n        Returns:\n            float: The estimated residual standard deviation (sigma).\n        \"\"\"\n        n, p = X.shape\n\n        # Identify observed and missing data\n        obs_mask = ~np.isnan(y)\n        mis_mask = np.isnan(y)\n        obs_idx = np.where(obs_mask)[0]\n        mis_idx = np.where(mis_mask)[0]\n        n_obs = len(obs_idx)\n        n_mis = len(mis_idx)\n\n        # Handle non-identifiable cases\n        if n_obs == 0:\n            return np.nan\n        \n        X_obs = X[obs_idx, :]\n        # Check if the observed-data normal equations are singular\n        if np.linalg.matrix_rank(X_obs)  p:\n            return np.nan\n\n        # Pre-compute parts that are constant through iterations\n        XTX = X.T @ X\n        XTX_inv = np.linalg.inv(XTX)\n        XT_obs = X[obs_idx, :].T\n        y_obs = y[obs_idx]\n\n        # Initialization\n        beta = np.zeros(p)\n        sigma2 = 1.0\n\n        for _ in range(max_iter):\n            beta_old = beta\n            sigma2_old = sigma2\n\n            # E-step: Construct the completed response vector y_imputed\n            # by filling missing values with their conditional expectation\n            y_imputed = np.copy(y)\n            y_imputed[mis_idx] = X[mis_idx, :] @ beta_old\n\n            # M-step: Update parameters\n            # Update beta\n            beta = XTX_inv @ X.T @ y_imputed\n\n            # Update sigma^2\n            # Term 1: Sum of squared residuals for observed data\n            res_obs = y_obs - X_obs @ beta\n            term1 = np.sum(res_obs**2)\n\n            # Term 2: Contribution from the uncertainty in imputed values\n            # This is sum_{i in M} (E[y_i | old] - E[y_i | new])^2,\n            # where E[y_i] = x_i^T beta\n            imputed_change = X[mis_idx, :] @ beta_old - X[mis_idx, :] @ beta\n            term2 = np.sum(imputed_change**2)\n            \n            # Term 3: Contribution from the variance of imputed values\n            term3 = n_mis * sigma2_old\n            \n            sigma2 = (term1 + term2 + term3) / n\n            \n            # Convergence check\n            # Use small epsilon to avoid division by zero\n            eps = 1e-12\n            rel_change_beta = np.linalg.norm(beta - beta_old) / (np.linalg.norm(beta_old) + eps)\n            rel_change_sigma2 = np.abs(sigma2 - sigma2_old) / (sigma2_old + eps)\n            \n            if max(rel_change_beta, rel_change_sigma2)  tol:\n                break\n        \n        # At convergence, the final estimate is sqrt(sigma^2)\n        return np.sqrt(sigma2)\n\n    # Define the test cases from the problem statement\n    test_cases = [\n        {\n            \"X\": np.array([[1, 0], [1, 1], [1, 2], [1, 3], [1, 4]]),\n            \"y\": np.array([2.1, 2.4, 3.2, np.nan, 3.9]),\n        },\n        {\n            \"X\": np.array([[1, -2], [1, -1], [1, 0], [1, 1], [1, 2]]),\n            \"y\": np.array([-1.5, -0.4, 0.3, 1.6, 2.5]),\n        },\n        {\n            \"X\": np.array([[1, 0, 1], [1, 1, 2], [1, 2, 0], [1, 3, 1], [1, 4, 3], [1, 5, 5]]),\n            \"y\": np.array([0.8, 1.2, 1.95, np.nan, 2.3, 2.55]),\n        },\n        {\n            \"X\": np.array([[1, 0], [1, 1], [1, 2]]),\n            \"y\": np.array([1.0, 3.0, np.nan]),\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        X = case[\"X\"]\n        y = case[\"y\"]\n        sigma_hat = run_em_for_sigma(X, y)\n        results.append(sigma_hat)\n\n    # Format the final output string\n    # Round to six decimal places\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4953183"}, {"introduction": "一个模型在训练数据上的表现（样本内误差）往往会比其在新数据上的表现（样本外误差）更为乐观，这种现象被称为“过拟合”。本练习旨在通过非参数自助法（bootstrap）和袋外（out-of-bag）评估，让您亲手计算并对比样本内估计标准误与更真实的样本外预测误差 [@problem_id:4953185]。这项实践将帮助您掌握评估模型泛化能力的关键计算技能，并理解为何样本内拟合优度（如传统的 $\\hat{\\sigma}$）可能具有误导性。", "problem": "您将实现一个完整的、可运行的程序，该程序在一个生物统计预测背景下，量化并对比线性回归的两种变异性概念：样本内残差标准误和通过非参数自助法估计的样本外预测残差标准误。请从基本线性模型和抽样定义出发，避免使用任何针对目标数量的预先给定的简化公式。您的实现必须是自包含且可复现的。\n\n考虑一个带截距的普通最小二乘线性模型，其中响应向量 $y \\in \\mathbb{R}^n$ 与一个包含截距列的设计矩阵 $X_{\\text{tilde}} \\in \\mathbb{R}^{n \\times q}$ 相关，其中 $q = p + 1$ 包括 $p$ 个预测变量和截距。该模型假设 $Y = X_{\\text{tilde}} \\beta + \\varepsilon$，误差项独立，均值为 $0$，方差为恒定的 $\\sigma^2$。令 $\\hat{\\beta}$ 为普通最小二乘估计量，通过最小化残差平方和进行拟合，令拟合值为 $\\hat{y} = X_{\\text{tilde}} \\hat{\\beta}$，残差为 $e = y - \\hat{y}$。将样本内残差标准误定义为基于残差自由度的误差方差无偏估计量的平方根。\n\n将样本外预测残差标准误定义为来自相同数据生成机制的新观测值的期望均方预测误差的平方根。由于此预测损失的有限样本分布取决于未知的总体和估计的变异性，您必须根据以下过程，使用带袋外评估的非参数对偶自助法来估计它。\n\n您必须从基本原理出发，实现以下算法步骤。\n\n- 样本内残差标准误。\n  1. 对全样本拟合带截距的普通最小二乘模型。\n  2. 计算残差向量和残差自由度 $n - q$，其中 $q$ 是包括截距在内的拟合系数的数量。\n  3. 根据残差构建误差方差的无偏估计量，并取其平方根以获得样本内残差标准误。\n\n- 样本外自助法预测残差标准误（带袋外评估的对偶自助法）。\n  1. 对于指定的自助法重复次数 $B$，重复以下步骤：\n     a. 通过从 $\\{1,\\dots,n\\}$ 中有放回地抽取 $n$ 个索引来生成一个自助样本。\n     b. 对自助样本拟合带截距的普通最小二乘模型。\n     c. 将此次重复的袋外集确定为在 $\\{1,\\dots,n\\}$ 中未被自助样本选中的那些索引。\n     d. 对于每个袋外索引 $i$，计算袋外预测误差 $y_i - \\hat{y}_i^{(b)}$，其中 $\\hat{y}_i^{(b)}$ 是在自助样本上拟合的模型所得的预测值。\n  2. 汇总所有重复中的所有袋外平方预测误差，然后除以袋外预测的总数，以获得袋外均方误差。\n  3. 取其平方根以获得自助法预测残差标准误。\n\n您的程序必须为每个测试用例计算以下三个量：\n- 样本内残差标准误，\n- 自助法预测残差标准误，\n- 这两个值之间的差值（自助法预测值减去样本内值）。\n\n所有量必须是浮点数。\n\n测试套件的数据生成必须遵循此处完整指定的、可复现的设计。对于每个测试用例，您必须：\n- 使用现代伪随机数生成器将随机种子设置为给定值。\n- 生成 $X \\in \\mathbb{R}^{n \\times p}$，其条目为独立的标准正态分布。\n- 构建 $X_{\\text{tilde}} = [\\mathbf{1}, X]$，其中 $\\mathbf{1}$ 是一个长度为 $n$ 的全1列。\n- 根据 $Y = \\beta_0 \\mathbf{1} + X \\beta_{\\text{slopes}} + \\varepsilon$ 生成 $Y$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ 在观测值之间独立，所有参数如下所述。\n\n需要精确实现的测试套件：\n- 案例 1：seed $= 1729$， $n = 80$， $p = 3$， $\\beta_0 = 0.7$，斜率向量 $\\beta_{\\text{slopes}} = [1.5, -2.0, 0.5]$， $\\sigma = 1.2$， $B = 500$。\n- 案例 2：seed $= 2021$， $n = 25$， $p = 5$， $\\beta_0 = -1.0$，斜率向量 $\\beta_{\\text{slopes}} = [0.6, -0.4, 1.2, 0.0, 0.3]$， $\\sigma = 0.8$， $B = 600$。\n- 案例 3：seed $= 777$， $n = 15$， $p = 8$， $\\beta_0 = 1.0$，斜率向量 $\\beta_{\\text{slopes}} = [0.5, -0.5, 0.8, 0.0, 0.3, -0.2, 0.4, 0.1]$， $\\sigma = 1.5$， $B = 800$。\n\n实现要求：\n- 所有拟合都使用带截距的普通最小二乘法。如果自助样本是秩亏的，请使用能产生最小化残差平方和的拟合值的最小二乘解（例如，通过 Moore–Penrose 伪逆方法）。\n- 自助法袋外汇总必须同等对待每个袋外预测。如果一次重复没有袋外观测值，则它对总和没有任何贡献。\n- 必须通过使用指定的种子来确保数值稳定性和可复现性。\n\n最终输出格式：\n- 对于每个测试用例，按上述顺序输出一个包含三个浮点数的列表。\n- 在最终输出中，将每个浮点数四舍五入到恰好 $6$ 位小数。\n- 您的程序应生成单行输出，其中包含所有测试用例的结果，格式为一个由三元素列表组成的逗号分隔列表，并用方括号括起来。例如：$[[a\\_1,b\\_1,c\\_1],[a\\_2,b\\_2,c\\_2],[a\\_3,b\\_3,c\\_3]]$，其中每个 $a\\_j$、$b\\_j$、$c\\_j$ 是一个四舍五入到 $6$ 位小数的浮点数。", "solution": "本解决方案提出了一个计算框架，用于量化和比较回归模型变异性的两种不同度量：样本内残差标准误和通过非参数自助法估计的样本外预测残差标准误。该分析基于普通最小二乘（OLS）线性回归和基于重抽样的模型验证的原理。\n\n### 线性模型与普通最小二乘估计\n\n我们考虑标准线性回归模型，该模型假设响应向量 $y \\in \\mathbb{R}^n$ 与一组 $p$ 个预测变量之间存在线性关系。模型表示为：\n$$\nY = X_{\\text{tilde}} \\beta + \\varepsilon\n$$\n其中：\n- $Y$ 是响应的 $n \\times 1$ 随机向量。\n- $X_{\\text{tilde}}$ 是 $n \\times q$ 的设计矩阵，包括一个用于截距的前导全1列和 $p$ 个用于预测变量的列。因此，$q = p + 1$。\n- $\\beta$ 是未知的真实系数的 $q \\times 1$ 向量，其中 $\\beta = [\\beta_0, \\beta_1, \\dots, \\beta_p]^T$。\n- $\\varepsilon$ 是未观测到的随机误差的 $n \\times 1$ 向量，假设 $\\varepsilon_i$ 是独立同分布的，均值 $E[\\varepsilon_i] = 0$，方差恒为 $Var(\\varepsilon_i) = \\sigma^2$。\n\n普通最小二乘（OLS）估计量，记为 $\\hat{\\beta}$，通过最小化残差平方和（RSS）得到。$\\hat{\\beta}$ 的 OLS 解由正规方程给出：\n$$\n\\hat{\\beta} = (X_{\\text{tilde}}^T X_{\\text{tilde}})^{-1} X_{\\text{tilde}}^T y\n$$\n在 $X_{\\text{tilde}}$ 不满列秩（即其列线性相关）的情况下，矩阵 $X_{\\text{tilde}}^T X_{\\text{tilde}}$ 是奇异的，无法求逆。这种情况可能在自助样本中发生。在此类场景下，使用 Moore-Penrose 伪逆 $(X_{\\text{tilde}}^T X_{\\text{tilde}})^{\\dagger}$ 获得最小二乘解，该解能最小化残差的欧几里得范数 $\\|y - X_{\\text{tilde}}\\beta\\|_2$。\n\n一旦计算出 $\\hat{\\beta}$，拟合值为 $\\hat{y} = X_{\\text{tilde}} \\hat{\\beta}$，残差为 $e = y - \\hat{y}$。\n\n### 样本内残差标准误（RSE）\n\n样本内 RSE 是对误差项 $\\varepsilon$ 的标准差 $\\sigma$ 的估计。它量化了基于用于拟合模型相同的数据，观测数据点与拟合回归线的典型偏离程度。\n\n1.  **残差平方和（RSS）**：这是观测值与拟合值之间差的平方和：\n    $$\n    RSS = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = e^T e\n    $$\n\n2.  **无偏方差估计量**：为获得误差方差 $\\sigma^2$ 的无偏估计量，将 RSS 除以残差自由度 $df = n - q$。自由度减少 $q$ 是为了说明从数据中估计了 $q$ 个系数。\n    $$\n    \\hat{\\sigma}^2 = \\frac{RSS}{n - q}\n    $$\n\n3.  **残差标准误**：RSE 是此无偏方差估计量的平方根：\n    $$\n    RSE_{\\text{in-sample}} = \\sqrt{\\hat{\\sigma}^2} = \\sqrt{\\frac{e^T e}{n - q}}\n    $$\n样本内 RSE 通常是对模型在新的、未见过数据上的预测误差的一个乐观（即向下偏倚）的估计，因为模型已经过优化以最小化训练样本上的误差。\n\n### 自助法预测残差标准误\n\n为了获得对模型预测性能更现实的估计，我们使用一种重抽样方法来模拟将模型拟合到新数据的过程。带袋外（OOB）评估的非参数对偶自助法是一种计算密集但功能强大的技术。它估计模型在未用于训练的数据点上的期望预测误差。\n\n对于 $B$ 次自助法重复，算法如下进行：\n\n1.  **自助抽样**：对于每次重复 $b \\in \\{1, \\dots, B\\}$，通过从原始索引集 $\\{1, \\dots, n\\}$ 中有放回地抽取索引来创建一个大小为 $n$ 的自助样本。令所选索引集为 $I^{(b)}$。自助数据为 $(X_{\\text{tilde}}^{(b)}, y^{(b)})$，对应于这些索引。\n\n2.  **模型拟合**：将一个 OLS 模型拟合到自助样本 $(X_{\\text{tilde}}^{(b)}, y^{(b)})$ 上，得到系数向量 $\\hat{\\beta}^{(b)}$。\n\n3.  **袋外（OOB）评估**：重复 $b$ 的 OOB 样本由其索引未在 $I^{(b)}$ 中被选中的原始数据点组成。令 OOB 索引集为 $OOB^{(b)} = \\{1, \\dots, n\\} \\setminus I^{(b)}$。\n    对于每个索引 $i \\in OOB^{(b)}$，使用在自助样本上拟合的模型来预测响应：\n    $$\n    \\hat{y}_i^{(b)} = \\tilde{x}_i^T \\hat{\\beta}^{(b)}\n    $$\n    其中 $\\tilde{x}_i^T$ 是原始设计矩阵 $X_{\\text{tilde}}$ 的第 $i$ 行。此 OOB 点的平方预测误差为 $(y_i - \\hat{y}_i^{(b)})^2$。\n\n4.  **汇总**：通过对所有 $B$ 次重复中所有 OOB 集计算出的所有平方预测误差求平均，来计算袋外均方误差（MSE$_{\\text{OOB}}$）。\n    $$\n    \\text{MSE}_{\\text{OOB}} = \\frac{\\sum_{b=1}^{B} \\sum_{i \\in OOB^{(b)}} (y_i - \\hat{y}_i^{(b)})^2}{\\sum_{b=1}^{B} |OOB^{(b)}|}\n    $$\n    其中 $|OOB^{(b)}|$ 是重复 $b$ 的袋外观测数。\n\n5.  **自助法预测 RSE**：MSE$_{\\text{OOB}}$ 的平方根给出了通过自助法估计的预测残差标准误。\n    $$\n    RSE_{\\text{pred}} = \\sqrt{\\text{MSE}_{\\text{OOB}}}\n    $$\n这个值提供了对模型在新数据上性能的一个偏差较小的估计。\n\n### 比较\n差值 $\\Delta = RSE_{\\text{pred}} - RSE_{\\text{in-sample}}$ 可作为“乐观偏差”的度量。一个较大的正值表示过拟合程度更高，即模型在其训练数据上的表现显著优于其在新数据上的预期表现。当预测变量数量 $p$ 相对于样本大小 $n$ 较大时，这种差异在高维设置中尤为明显。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and contrasts in-sample and bootstrap out-of-bag predictive \n    residual standard errors for linear regression models.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"seed\": 1729, \"n\": 80, \"p\": 3, \"beta_0\": 0.7, \n            \"beta_slopes\": np.array([1.5, -2.0, 0.5]), \"sigma\": 1.2, \"B\": 500\n        },\n        {\n            \"seed\": 2021, \"n\": 25, \"p\": 5, \"beta_0\": -1.0, \n            \"beta_slopes\": np.array([0.6, -0.4, 1.2, 0.0, 0.3]), \"sigma\": 0.8, \"B\": 600\n        },\n        {\n            \"seed\": 777, \"n\": 15, \"p\": 8, \"beta_0\": 1.0, \n            \"beta_slopes\": np.array([0.5, -0.5, 0.8, 0.0, 0.3, -0.2, 0.4, 0.1]), \"sigma\": 1.5, \"B\": 800\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        # Unpack parameters\n        seed = case[\"seed\"]\n        n = case[\"n\"]\n        p = case[\"p\"]\n        beta_0 = case[\"beta_0\"]\n        beta_slopes = case[\"beta_slopes\"]\n        sigma = case[\"sigma\"]\n        B = case[\"B\"]\n        \n        # Initialize a modern pseudorandom number generator for reproducibility\n        rng = np.random.default_rng(seed)\n\n        # Generate data\n        X = rng.normal(size=(n, p))\n        X_tilde = np.hstack((np.ones((n, 1)), X))\n        beta_full = np.concatenate(([beta_0], beta_slopes))\n        epsilon = rng.normal(loc=0.0, scale=sigma, size=n)\n        y = X_tilde @ beta_full + epsilon\n\n        # --- In-sample residual standard error ---\n        q = p + 1\n        # np.linalg.lstsq handles potential rank-deficiency and returns RSS\n        _, rss_array, _, _ = np.linalg.lstsq(X_tilde, y, rcond=None)\n        rss = rss_array[0]\n        df = n - q\n        \n        # Handle case where df = 0 (more predictors than samples)\n        # In this situation, the in-sample error is 0, and RSE is undefined or 0.\n        if df  0:\n            mse_in_sample = rss / df\n            rse_in_sample = np.sqrt(mse_in_sample)\n        else:\n            rse_in_sample = 0.0\n\n\n        # --- Out-of-sample bootstrap predictive residual standard error ---\n        total_oob_sq_err = 0.0\n        total_oob_count = 0\n        all_indices = np.arange(n)\n\n        for _ in range(B):\n            # Draw a bootstrap sample of indices\n            boot_indices = rng.choice(all_indices, size=n, replace=True)\n            \n            # Identify out-of-bag (OOB) indices\n            # unique() is important as bootstrap samples may have duplicates\n            oob_indices = np.setdiff1d(all_indices, np.unique(boot_indices), assume_unique=True)\n\n            if len(oob_indices) == 0:\n                continue\n            \n            # Create bootstrap and OOB data sets\n            X_boot, y_boot = X_tilde[boot_indices], y[boot_indices]\n            X_oob, y_oob = X_tilde[oob_indices], y[oob_indices]\n\n            # Fit OLS model on bootstrap sample\n            # This automatically uses a pseudoinverse for rank-deficient matrices\n            beta_boot, _, _, _ = np.linalg.lstsq(X_boot, y_boot, rcond=None)\n\n            # Predict on OOB data\n            y_pred_oob = X_oob @ beta_boot\n\n            # Accumulate squared errors and counts\n            total_oob_sq_err += np.sum((y_oob - y_pred_oob)**2)\n            total_oob_count += len(oob_indices)\n        \n        # Calculate OOB MSE and the predictive RSE\n        if total_oob_count  0:\n            mse_oob = total_oob_sq_err / total_oob_count\n            rse_pred = np.sqrt(mse_oob)\n        else:\n            # Fallback if no OOB samples were ever generated (highly unlikely)\n            rse_pred = np.nan\n\n        # Calculate the difference\n        difference = rse_pred - rse_in_sample\n\n        results.append([rse_in_sample, rse_pred, difference])\n\n    # Format the final output string as per requirements\n    output_parts = []\n    for res in results:\n        # rounding to exactly 6 decimal places\n        part = f\"[{res[0]:.6f},{res[1]:.6f},{res[2]:.6f}]\"\n        output_parts.append(part)\n    \n    final_output_str = f\"[{','.join(output_parts)}]\"\n    print(final_output_str)\n\nsolve()\n```", "id": "4953185"}]}