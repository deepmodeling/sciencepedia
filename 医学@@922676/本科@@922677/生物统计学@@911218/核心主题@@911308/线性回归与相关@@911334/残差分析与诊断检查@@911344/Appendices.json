{"hands_on_practices": [{"introduction": "杠杆值是识别影响力数据点的关键概念，它量化了单个观测值在预测变量空间中的“极端”程度。这项练习将通过逐步推导来揭示简单线性回归中杠杆值的公式，从而帮助你理解其内在含义。完成此推导将使你深刻理解一个数据点的位置如何决定其对回归模型的潜在影响。[@problem_id:4949183]", "problem": "一项生物统计学研究将一个连续的生物标志物响应建模为一个连续剂量的线性函数，并带有一个截距。具体来说，对于由 $i \\in \\{1,\\dots,n\\}$ 索引的观测值，模型为 $y_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\varepsilon_{i}$，其中误差 $\\varepsilon_{i}$ 满足用于证明拟合值投影性质的常规普通最小二乘法 (OLS) 假设。设 $n \\times 2$ 的设计矩阵为 $X = \\begin{pmatrix} 1  x_{1} \\\\ \\vdots  \\vdots \\\\ 1  x_{n} \\end{pmatrix}$，并设投影（帽子）矩阵是将在 OLS 下观测到的响应映射到其拟合值的唯一线性算子。该投影矩阵的对角元素是杠杆值，记作 $h_{ii}$，它量化了观测值 $i$ 影响其自身拟合值的潜力。\n\n请仅从标准线性模型定义和 $2 \\times 2$ 矩阵的线性代数事实出发，推导杠杆值 $h_{ii}$ 的闭式表达式，用 $n$、$x_{i}$、$\\bar{x}$ 和 $\\sum_{j=1}^{n} (x_{j} - \\bar{x})^{2}$ 表示，其中 $\\bar{x} = \\frac{1}{n} \\sum_{j=1}^{n} x_{j}$。您的推导过程应明确说明为用 $\\bar{x}$ 和 $\\sum_{j=1}^{n} (x_{j} - \\bar{x})^{2}$ 替换无关的总和而需要的任何中间代数步骤。\n\n此外，请根据您的推导，定性地解释设计点 $\\{x_{j}\\}_{j=1}^{n}$ 的间距如何决定一个观测值的潜在影响，以及为什么相对于 $\\bar{x}$ 更极端的 $x_{i}$ 值具有更高的杠杆值。\n\n请提供简化的 $h_{ii}$ 解析表达式作为您的最终答案。无需进行数值舍入，也不涉及单位。", "solution": "本题要求推导简单线性回归模型的杠杆值 $h_{ii}$，并对其性质进行定性解释。推导必须从线性模型理论的基本定义开始。\n\n简单线性回归模型由 $y_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\varepsilon_{i}$ 给出。其矩阵形式为 $\\mathbf{y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$，其中 $\\mathbf{y}$ 是响应的 $n \\times 1$ 向量，$X$ 是 $n \\times 2$ 的设计矩阵，$\\boldsymbol{\\beta} = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix}$ 是系数向量，$\\boldsymbol{\\varepsilon}$ 是误差向量。\n\n设计矩阵 $X$ 如下所示：\n$$\nX = \\begin{pmatrix} 1  x_{1} \\\\ 1  x_{2} \\\\ \\vdots  \\vdots \\\\ 1  x_{n} \\end{pmatrix}\n$$\n普通最小二乘法 (OLS) 的拟合值向量 $\\hat{\\mathbf{y}}$ 是通过将观测响应向量 $\\mathbf{y}$ 投影到 $X$ 的列空间上得到的。投影矩阵，或称帽子矩阵，用 $H$ 表示，定义为：\n$$\nH = X(X^T X)^{-1} X^T\n$$\n第 $i$ 个观测值的杠杆值 $h_{ii}$ 是 $H$ 的第 $i$ 个对角元素。它可以计算为 $h_{ii} = \\mathbf{x}_i^T (X^T X)^{-1} \\mathbf{x}_i$，其中 $\\mathbf{x}_i^T = \\begin{pmatrix} 1  x_i \\end{pmatrix}$ 是设计矩阵 $X$ 的第 $i$ 行。\n\n我们的推导分几个步骤进行。\n\n**步骤 1：计算矩阵 $X^T X$。**\n矩阵 $X^T X$ 是一个 $2 \\times 2$ 的对称矩阵。\n$$\nX^T X = \\begin{pmatrix} 1  1  \\dots  1 \\\\ x_1  x_2  \\dots  x_n \\end{pmatrix} \\begin{pmatrix} 1  x_{1} \\\\ 1  x_{2} \\\\ \\vdots  \\vdots \\\\ 1  x_{n} \\end{pmatrix} = \\begin{pmatrix} \\sum_{j=1}^{n} 1  \\sum_{j=1}^{n} x_j \\\\ \\sum_{j=1}^{n} x_j  \\sum_{j=1}^{n} x_j^2 \\end{pmatrix}\n$$\n使用样本均值的定义 $\\bar{x} = \\frac{1}{n} \\sum_{j=1}^{n} x_j$，我们可以写出 $\\sum_{j=1}^{n} x_j = n\\bar{x}$。\n因此，\n$$\nX^T X = \\begin{pmatrix} n  n\\bar{x} \\\\ n\\bar{x}  \\sum_{j=1}^{n} x_j^2 \\end{pmatrix}\n$$\n\n**步骤 2：计算 $X^T X$ 的行列式及其逆矩阵。**\n$X^T X$ 的行列式为：\n$$\n\\det(X^T X) = n \\left(\\sum_{j=1}^{n} x_j^2\\right) - (n\\bar{x})^2 = n \\sum_{j=1}^{n} x_j^2 - n^2\\bar{x}^2\n$$\n为了用要求的项来表示，我们使用离差平方和的定义，记作 $S_{xx}$：\n$$\nS_{xx} = \\sum_{j=1}^{n} (x_j - \\bar{x})^2 = \\sum_{j=1}^{n} (x_j^2 - 2x_j\\bar{x} + \\bar{x}^2) = \\left(\\sum_{j=1}^{n} x_j^2\\right) - 2\\bar{x}\\left(\\sum_{j=1}^{n} x_j\\right) + \\sum_{j=1}^{n} \\bar{x}^2\n$$\n代入 $\\sum_{j=1}^{n} x_j = n\\bar{x}$，我们得到：\n$$\nS_{xx} = \\left(\\sum_{j=1}^{n} x_j^2\\right) - 2\\bar{x}(n\\bar{x}) + n\\bar{x}^2 = \\left(\\sum_{j=1}^{n} x_j^2\\right) - n\\bar{x}^2\n$$\n因此，行列式可以简化为：\n$$\n\\det(X^T X) = n \\left( \\left(\\sum_{j=1}^{n} x_j^2\\right) - n\\bar{x}^2 \\right) = n S_{xx} = n \\sum_{j=1}^{n} (x_j - \\bar{x})^2\n$$\n现在，我们使用通用 $2 \\times 2$ 矩阵的逆矩阵公式 $\\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}^{-1} = \\frac{1}{ad-bc}\\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}$ 来求 $X^T X$ 的逆矩阵：\n$$\n(X^T X)^{-1} = \\frac{1}{n S_{xx}} \\begin{pmatrix} \\sum_{j=1}^{n} x_j^2  -n\\bar{x} \\\\ -n\\bar{x}  n \\end{pmatrix}\n$$\n为简化此表达式，我们代入 $\\sum_{j=1}^{n} x_j^2 = S_{xx} + n\\bar{x}^2$：\n$$\n(X^T X)^{-1} = \\frac{1}{n S_{xx}} \\begin{pmatrix} S_{xx} + n\\bar{x}^2  -n\\bar{x} \\\\ -n\\bar{x}  n \\end{pmatrix} = \\begin{pmatrix} \\frac{S_{xx} + n\\bar{x}^2}{n S_{xx}}  -\\frac{n\\bar{x}}{n S_{xx}} \\\\ -\\frac{n\\bar{x}}{n S_{xx}}  \\frac{n}{n S_{xx}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}  -\\frac{\\bar{x}}{S_{xx}} \\\\ -\\frac{\\bar{x}}{S_{xx}}  \\frac{1}{S_{xx}} \\end{pmatrix}\n$$\n\n**步骤 3：计算杠杆值 $h_{ii}$。**\n使用公式 $h_{ii} = \\mathbf{x}_i^T (X^T X)^{-1} \\mathbf{x}_i$ 和 $\\mathbf{x}_i^T = \\begin{pmatrix} 1  x_i \\end{pmatrix}$：\n$$\nh_{ii} = \\begin{pmatrix} 1  x_i \\end{pmatrix} \\begin{pmatrix} \\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}  -\\frac{\\bar{x}}{S_{xx}} \\\\ -\\frac{\\bar{x}}{S_{xx}}  \\frac{1}{S_{xx}} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ x_i \\end{pmatrix}\n$$\n首先，将行向量与矩阵相乘：\n$$\n\\begin{pmatrix} 1  x_i \\end{pmatrix} \\begin{pmatrix} \\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}  -\\frac{\\bar{x}}{S_{xx}} \\\\ -\\frac{\\bar{x}}{S_{xx}}  \\frac{1}{S_{xx}} \\end{pmatrix} = \\begin{pmatrix} \\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}} - \\frac{x_i \\bar{x}}{S_{xx}}\\right)  \\left(-\\frac{\\bar{x}}{S_{xx}} + \\frac{x_i}{S_{xx}}\\right) \\end{pmatrix}\n$$\n接下来，将得到的 $1 \\times 2$ 向量与列向量 $\\begin{pmatrix} 1 \\\\ x_i \\end{pmatrix}$ 相乘：\n$$\nh_{ii} = \\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}} - \\frac{x_i \\bar{x}}{S_{xx}}\\right) \\cdot 1 + \\left(\\frac{x_i - \\bar{x}}{S_{xx}}\\right) \\cdot x_i\n$$\n$$\nh_{ii} = \\frac{1}{n} + \\frac{\\bar{x}^2 - x_i \\bar{x} + x_i^2 - \\bar{x} x_i}{S_{xx}} = \\frac{1}{n} + \\frac{x_i^2 - 2x_i\\bar{x} + \\bar{x}^2}{S_{xx}}\n$$\n注意到第二项的分子是 $(x_i - \\bar{x})^2$，我们得到最终表达式：\n$$\nh_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{S_{xx}}\n$$\n代回 $S_{xx}$ 的定义：\n$$\nh_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n} (x_j - \\bar{x})^2}\n$$\n这就是所要求的观测值 $i$ 的杠杆值的闭式表达式。\n\n**杠杆值的定性解释**\n\n推导出的杠杆值表达式 $h_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n} (x_j - \\bar{x})^2}$ 为决定一个观测值对其自身拟合值的潜在影响的因素提供了直接的洞见。杠杆值由两个非负项组成：\n\n1. 项 $\\frac{1}{n}$ 代表一个对所有观测值都统一的杠杆值基线部分。它只取决于样本量 $n$。随着观测数量的增加，这个基线影响会减小，反映了在更大的数据集中任何单个数据点的重要性降低。\n\n2. 第二项 $\\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n} (x_j - \\bar{x})^2}$ 代表由于预测变量值 $x_i$ 相对于其他预测变量值的位置而产生的杠杆值部分。\n   - 分子 $(x_i - \\bar{x})^2$ 是 $x_i$ 到预测变量数据中心 $\\bar{x}$ 的距离的平方。这一项表明，当一个观测值的预测变量值变得更极端（即离均值 $\\bar{x}$ 更远）时，其杠杆值会呈二次方增长。具有较大 $|x_i - \\bar{x}|$ 值的观测值被称为高杠杆点。这样的点就像长长的杠杆臂，使它们有更大的潜力来转动拟合的回归线。\n   - 分母 $\\sum_{j=1}^{n} (x_j - \\bar{x})^2$ 是所有预测变量值的离差平方和。它量化了设计点 $\\{x_j\\}$ 的总离散度或变异性。这一项设定了判断一个给定点 $x_i$ “极端性”的尺度。\n     - 如果设计点分布广泛，分母会很大。这会降低所有点的位置杠杆分量的大小，意味着回归线被分散的数据更稳固地“锚定”，任何单个点的影响都会减小。\n     - 相反，如果设计点紧密聚集，分母会很小。这会放大了位置杠杆分量。在这种情况下，一个离 $\\bar{x}$ 仅有中等距离的观测值也可能具有非常高的杠杆值，因为它相对于其余数据的狭窄分布是一个异常值。\n\n总之，如果一个观测值的预测变量值远离所有预测变量值的均值，那么它的杠杆值就很高，特别是当大多数其他预测变量值分布不广时。这为预测变量空间中的远端数据点在回归分析中最具影响力的直观想法提供了精确的、定量的基础。", "answer": "$$\n\\boxed{\\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n} (x_j - \\bar{x})^2}}\n$$", "id": "4949183"}, {"introduction": "在理解了杠杆值之后，我们可以探索一个更全面的影响力度量——库克距离，它同时考虑了杠杆值和残差的大小。这项练习要求你推导库克距离的一个常用表达式，从而揭示这两个组成部分如何相互作用，共同量化单个观测值对整个模型拟合结果的影响。这项练习直接建立在前一个关于杠杆值的实践之上。[@problem_id:4949167]", "problem": "一位生物统计学家拟合了一个普通最小二乘（OLS）线性回归模型，其响应向量为 $y \\in \\mathbb{R}^{n}$，设计矩阵为满秩矩阵 $X \\in \\mathbb{R}^{n \\times p}$，其中 $n > p \\ge 2$。假设经典线性模型为 $y = X\\beta + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$。令 $\\hat{\\beta} = (X^{\\top} X)^{-1} X^{\\top} y$ 为 OLS 估计量，$\\hat{y} = X \\hat{\\beta}$ 为拟合值，$e = y - \\hat{y}$ 为残差向量。定义帽子矩阵 $H = X (X^{\\top} X)^{-1} X^{\\top}$，并令 $h_{ii}$ 表示其第 $i$ 个对角元素（即观测值 $i$ 的杠杆值）。令 $s^{2} = \\frac{1}{n-p} \\|y - X \\hat{\\beta}\\|_{2}^{2}$ 为均方误差。观测值 $i$ 的内学生化残差定义为 $t_{i} = \\frac{e_{i}}{s \\sqrt{1 - h_{ii}}}$。对于省略观测值 $i$ 的留一法拟合，将其系数估计量记为 $\\hat{\\beta}_{(i)}$。观测值 $i$ 的 Cook 距离定义为\n$$\nD_{i} \\;=\\; \\frac{\\left(\\hat{\\beta} - \\hat{\\beta}_{(i)}\\right)^{\\top} X^{\\top} X \\left(\\hat{\\beta} - \\hat{\\beta}_{(i)}\\right)}{p\\, s^{2}}.\n$$\n请仅从这些定义和用于秩一更新的标准线性代数恒等式出发，推导出 $D_{i}$ 的一个显式表达式，该表达式仅用 $p$、$t_{i}$ 和 $h_{ii}$ 表示。然后，根据你的推导，简要解释杠杆值 $h_{ii}$ 和残差的大小（由 $t_{i}$ 体现）分别如何影响 $D_{i}$。\n\n请提供 $D_{i}$ 关于 $p$、$t_{i}$ 和 $h_{ii}$ 的闭式解析表达式作为最终答案。无需进行数值计算。", "solution": "该问题提法明确，在线性模型理论中有坚实的科学基础，并包含获得唯一解析解所需的所有信息。其定义和前提均为标准且内部一致。\n\n目标是推导 Cook 距離 $D_{i}$ 的表达式，用预测变量数量 $p$、内学生化残差 $t_{i}$ 和杠杆值 $h_{ii}$ 来表示。所提供的观测值 $i$ 的 Cook 距离定义为：\n$$ D_{i} \\;=\\; \\frac{\\left(\\hat{\\beta} - \\hat{\\beta}_{(i)}\\right)^{\\top} X^{\\top} X \\left(\\hat{\\beta} - \\hat{\\beta}_{(i)}\\right)}{p\\, s^{2}} $$\n推导的核心是为省略第 $i$ 个观测值所导致的系数向量变化量 $\\hat{\\beta} - \\hat{\\beta}_{(i)}$ 找出一个简化表达式。留一法估计量 $\\hat{\\beta}_{(i)}$ 是使用数据矩阵 $X_{(i)}$ 和 $y_{(i)}$ 计算的，它们分别是将 $X$ 和 $y$ 的第 $i$ 行移除后形成的。设 $x_i^{\\top}$ 为 $X$ 的第 $i$ 行。留一法估计量所需的矩阵乘积是对完整数据乘积的秩-1修正：\n$$ X_{(i)}^{\\top} X_{(i)} = X^{\\top}X - x_i x_i^{\\top} $$\n$$ X_{(i)}^{\\top} y_{(i)} = X^{\\top}y - x_i y_i $$\n为了计算 $\\hat{\\beta}_{(i)} = (X_{(i)}^{\\top} X_{(i)})^{-1} X_{(i)}^{\\top} y_{(i)}$，我们首先需要求 $X_{(i)}^{\\top} X_{(i)}$ 的逆。我们使用用于秩-1更新的 Sherman-Morrison-Woodbury 公式，该公式为 $(A - uv^{\\top})^{-1} = A^{-1} + \\frac{A^{-1}uv^{\\top}A^{-1}}{1 - v^{\\top}A^{-1}u}$。令 $A = X^{\\top}X$ 和 $u = v = x_i$，我们得到：\n$$ (X^{\\top}X - x_i x_i^{\\top})^{-1} = (X^{\\top}X)^{-1} + \\frac{(X^{\\top}X)^{-1}x_i x_i^{\\top}(X^{\\top}X)^{-1}}{1 - x_i^{\\top}(X^{\\top}X)^{-1}x_i} $$\n分母中的项 $x_i^{\\top}(X^{\\top}X)^{-1}x_i$ 对应于帽子矩阵 $H = X(X^{\\top}X)^{-1}X^{\\top}$ 的第 $i$ 个对角元素，即杠杆值 $h_{ii}$。因此，逆矩阵为：\n$$ (X_{(i)}^{\\top} X_{(i)})^{-1} = (X^{\\top}X)^{-1} + \\frac{(X^{\\top}X)^{-1}x_i x_i^{\\top}(X^{\\top}X)^{-1}}{1 - h_{ii}} $$\n现在，我们可以表示 $\\hat{\\beta}_{(i)}$：\n$$ \\hat{\\beta}_{(i)} = \\left( (X^{\\top}X)^{-1} + \\frac{(X^{\\top}X)^{-1}x_i x_i^{\\top}(X^{\\top}X)^{-1}}{1 - h_{ii}} \\right) (X^{\\top}y - x_i y_i) $$\n展开此表达式可得：\n$$ \\hat{\\beta}_{(i)} = (X^{\\top}X)^{-1}(X^{\\top}y) - (X^{\\top}X)^{-1}x_i y_i + \\frac{(X^{\\top}X)^{-1}x_i}{1-h_{ii}} \\left( x_i^{\\top}(X^{\\top}X)^{-1}X^{\\top}y - x_i^{\\top}(X^{\\top}X)^{-1}x_i y_i \\right) $$\n我们识别出几个项：$\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y$、第 $i$ 个拟合值 $\\hat{y}_i = x_i^{\\top}\\hat{\\beta} = x_i^{\\top}(X^{\\top}X)^{-1}X^{\\top}y$ 以及 $h_{ii} = x_i^{\\top}(X^{\\top}X)^{-1}x_i$。代入这些项可得：\n$$ \\hat{\\beta}_{(i)} = \\hat{\\beta} - (X^{\\top}X)^{-1}x_i y_i + \\frac{(X^{\\top}X)^{-1}x_i}{1-h_{ii}} (\\hat{y}_i - h_{ii} y_i) $$\n重新整理以求出差向量 $\\hat{\\beta} - \\hat{\\beta}_{(i)}$：\n$$ \\hat{\\beta} - \\hat{\\beta}_{(i)} = (X^{\\top}X)^{-1}x_i y_i - \\frac{(X^{\\top}X)^{-1}x_i}{1-h_{ii}} (\\hat{y}_i - h_{ii} y_i) = (X^{\\top}X)^{-1}x_i \\left( y_i - \\frac{\\hat{y}_i - h_{ii} y_i}{1 - h_{ii}} \\right) $$\n括号中的项简化为：\n$$ \\frac{y_i(1 - h_{ii}) - (\\hat{y}_i - h_{ii} y_i)}{1 - h_{ii}} = \\frac{y_i - y_i h_{ii} - \\hat{y}_i + y_i h_{ii}}{1 - h_{ii}} = \\frac{y_i - \\hat{y}_i}{1 - h_{ii}} = \\frac{e_i}{1-h_{ii}} $$\n其中 $e_i = y_i - \\hat{y}_i$ 是第 $i$ 个残差。这导出了关于系数变化的关键结果：\n$$ \\hat{\\beta} - \\hat{\\beta}_{(i)} = \\frac{(X^{\\top}X)^{-1}x_i e_i}{1 - h_{ii}} $$\n接下来，我们将此式代入 Cook 距离的分子中：\n$$ (\\hat{\\beta} - \\hat{\\beta}_{(i)})^{\\top} X^{\\top} X (\\hat{\\beta} - \\hat{\\beta}_{(i)}) = \\left( \\frac{e_i}{1-h_{ii}} x_i^{\\top}(X^{\\top}X)^{-1} \\right) (X^{\\top}X) \\left( \\frac{(X^{\\top}X)^{-1}x_i e_i}{1-h_{ii}} \\right) $$\n$$ = \\left( \\frac{e_i}{1-h_{ii}} \\right)^2 x_i^{\\top} \\left( (X^{\\top}X)^{-1} (X^{\\top}X) (X^{\\top}X)^{-1} \\right) x_i = \\frac{e_i^2}{(1-h_{ii})^2} x_i^{\\top} (X^{\\top}X)^{-1} x_i $$\n使用 $h_{ii} = x_i^{\\top}(X^{\\top}X)^{-1}x_i$，分子变为 $\\frac{e_i^2 h_{ii}}{(1-h_{ii})^2}$。将此代入 $D_i$ 的定义中：\n$$ D_i = \\frac{1}{p s^2} \\left[ \\frac{e_i^2 h_{ii}}{(1-h_{ii})^2} \\right] $$\n最后一步是引入内学生化残差 $t_{i} = \\frac{e_i}{s \\sqrt{1 - h_{ii}}}$。对其平方可得 $t_i^2 = \\frac{e_i^2}{s^2(1-h_{ii})}$，由此我们得到 $e_i^2 = t_i^2 s^2 (1 - h_{ii})$。将 $e_i^2$ 的这个表达式代入：\n$$ D_i = \\frac{1}{p s^2} \\left[ \\frac{t_i^2 s^2 (1 - h_{ii}) h_{ii}}{(1-h_{ii})^2} \\right] $$\n消去 $s^2$ 和一个因子 $(1-h_{ii})$，便得到 Cook 距离的最终表达式：\n$$ D_i = \\frac{t_i^2}{p} \\frac{h_{ii}}{1-h_{ii}} $$\n这个推导出的表达式阐明了一个观测值影响力的本质。由 $D_i$ 度量的影响力是两个因素的乘积：一个与残差大小（$t_i^2$）有关，另一个与观测值的杠杆值（$h_{ii}$）有关。\n$t_i^2$ 项表明 $D_i$ 与学生化残差的平方成正比。远离回归超平面的点（响应空间中的离群值）将有较大的 $|t_i|$ 值，从而导致更大的 $D_i$。\n$\\frac{h_{ii}}{1-h_{ii}}$ 项体现了杠杆值的影响。杠杆值 $h_{ii}$ 量化了一个观测值的预测变量值 $x_i$ 距离预测变量空间中心的远近程度。当杠杆值 $h_{ii}$ 趋近于 1 时，函数 $\\frac{h_{ii}}{1-h_{ii}}$ 会非线性地增加。这意味着高杠杆点对残差项起到了很强的乘数效应。\n总而言之，如果一个观测值具有大残差、高杠杆值，或两者兼有，那么它就具有影响力。一个高杠杆点具有产生高影响力的潜力；如果该点同时也有一个不可忽略的残差，这种潜力就会实现。反之，一个低杠杆值的点必须有一个非常大的残差才会有影响力。", "answer": "$$\\boxed{\\frac{t_{i}^{2}}{p} \\frac{h_{ii}}{1 - h_{ii}}}$$", "id": "4949167"}, {"introduction": "现代统计实践高度依赖于计算。最后的这项练习将理论付诸实践，要求你实现一个残差自助法程序——一种用于估计不确定性的强大技术。更重要的是，你还将运行诊断检验来验证自助法所依赖的假设，亲眼观察异方差性等违规情况会如何影响你的结果。[@problem_id:4949168]", "problem": "给定一个固定设计线性回归设定，其中响应向量 $y \\in \\mathbb{R}^n$ 被建模为 $y = X \\beta + \\varepsilon$，带有一个固定的设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$，该矩阵包含一个截距列。普通最小二乘估计量 $\\hat{\\beta}$ 定义为在所有 $b \\in \\mathbb{R}^p$ 中使残差平方和 $\\sum_{i=1}^n (y_i - x_i^\\top b)^2$ 最小化的向量，其中 $x_i^\\top$ 表示 $X$ 的第 $i$ 行。残差分析和诊断检验用于评估残差自助法程序在何种条件下能够一致地近似 $\\hat{\\beta}$ 的抽样分布。\n\n构建一个用于线性回归的残差自助法，该方法使用标准化和中心化的残差来模拟误差分布。您的程序必须：\n\n1. 使用最小二乘法原理计算 $\\hat{\\beta}$、残差 $e = y - X \\hat{\\beta}$、帽子矩阵的对角元素 $h_{ii}$ 以及拟合值 $\\hat{y} = X \\hat{\\beta}$。\n\n2. 构建标准化中心化残差 $u_i = \\frac{e_i - \\bar{e}}{\\sqrt{1 - h_{ii}}}$，其中 $\\bar{e}$ 是 $e$ 的经验均值。在计算分母时，通过将 $\\sqrt{1 - h_{ii}}$ 替换为 $\\sqrt{\\max(1 - h_{ii}, 10^{-8})}$ 来确保数值稳定性。\n\n3. 实现一个包含 $B$ 次重抽样的残差自助法，在每次重抽样中：\n   - 从 $\\{u_i\\}_{i=1}^n$ 中有放回地抽取 $n$ 个残差，得到 $u^\\ast$。\n   - 构建 $y^\\ast = X \\hat{\\beta} + u^\\ast$ 并通过最小二乘法重新计算 $\\hat{\\beta}^\\ast$。\n   - 收集自助法重复样本 $\\{\\hat{\\beta}^\\ast\\}$，并计算这 $B$ 次重复的自助法标准差向量 $s^\\ast \\in \\mathbb{R}^p$。\n\n4. 对每个测试用例，使用从该用例指定的真实误差机制生成的 $M$ 个独立数据集，在相同的固定 $X$ 和真实 $\\beta$ 的条件下，构建 $\\hat{\\beta}$ 的蒙特卡洛参考分布。计算这 $M$ 次重复的蒙特卡洛标准差向量 $s^{\\mathrm{MC}} \\in \\mathbb{R}^p$。\n\n5. 在用例指定的误差协方差 $\\Sigma_\\varepsilon$ 下，使用恒等式 $\\mathrm{Var}(\\hat{\\beta}) = (X^\\top X)^{-1} X^\\top \\Sigma_\\varepsilon X (X^\\top X)^{-1}$ 计算 $\\hat{\\beta}$ 的基于模型的“真实”协方差。由此得到 $s^{\\mathrm{true}} = \\sqrt{\\mathrm{diag}((X^\\top X)^{-1} X^\\top \\Sigma_\\varepsilon X (X^\\top X)^{-1})}$。\n\n6. 当且仅当以下两个标准都满足时，声明 $\\hat{\\beta}$ 分布的自助法一致性：\n   - 标准差的最大相对偏差低于阈值 $\\tau_{\\mathrm{sd}}$，即 $\\max_{j=1,\\dots,p} \\left| s^\\ast_j - s^{\\mathrm{true}}_j \\right| / s^{\\mathrm{true}}_j  \\tau_{\\mathrm{sd}}$。\n   - 每个系数的边际自助法分布与其蒙特卡洛参考分布之间的最大柯尔莫哥洛夫-斯米尔诺夫距离低于阈值 $\\tau_{\\mathrm{KS}}$。\n\n7. 从残差 $e$ 计算两个诊断检验：\n   - 异方差性诊断：使用 Breusch–Pagan 辅助回归，将 $e_i^2$ 对 $X$ 的非截距列进行回归，辅助回归中包含一个截距。令 $R^2$ 为此辅助回归的决定系数。定义 $T_{\\mathrm{BP}} = n R^2$ 和 $p$-值 $p_{\\mathrm{BP}} = 1 - F_{\\chi^2}(T_{\\mathrm{BP}}; p-1)$，其中 $F_{\\chi^2}$ 是自由度为 $p-1$ 的卡方分布的累积分布函数。如果 $p_{\\mathrm{BP}}  0.05$，则声明检测到异方差性。\n   - 自相关诊断：计算残差的滞后1阶样本自相关 $r_1$（使用 $(e_1,\\dots,e_{n-1})$ 和 $(e_2,\\dots,e_n)$ 之间的常规样本相关性）。如果 $|r_1| \\ge \\rho_{\\mathrm{thr}}$，则声明检测到自相关。\n\n您的程序必须使用以下固定的阈值和重抽样大小来实现上述内容：\n- 将自助法重抽样次数设置为 $B = 800$。\n- 将蒙特卡洛重复次数设置为 $M = 1500$。\n- 将标准差相对误差阈值设置为 $\\tau_{\\mathrm{sd}} = 0.25$。\n- 将柯尔莫哥洛夫-斯米尔诺夫距离阈值设置为 $\\tau_{\\mathrm{KS}} = 0.15$。\n- 将自相关检测阈值设置为 $\\rho_{\\mathrm{thr}} = 0.25$。\n\n测试套件：\n为以下四个科学上合理的测试用例实现该程序。所有用例共享相同的真实系数向量 $\\beta = [0.7, -1.2, 0.5]^\\top$。在每个用例中，设计矩阵 $X$ 有 $p = 3$ 列：一个全为1的截距列和两个预测变量列。除非另有说明，预测变量被生成为独立的标准正态随机变量。使用提供的随机种子以保证可复现性。\n\n- 用例 A (同方差独立误差，“理想路径”)：$n = 200$，$\\sigma^2 = 1.0$，误差 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$，种子 $123$。\n- 用例 B (异方差独立误差)：$n = 200$，$\\sigma^2 = 1.0$，误差 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_i^2)$ 独立分布，其中 $\\sigma_i^2 = \\sigma^2 (1 + |x_{i1}|)$，$x_{i1}$ 是观测值 $i$ 的第一个非截距预测变量，种子 $456$。\n- 用例 C (正自相关误差)：$n = 200$，$\\sigma^2 = 1.0$，误差服从参数为 $\\rho = 0.6$ 的1阶平稳自回归模型，即 $\\varepsilon_t = \\rho \\varepsilon_{t-1} + \\eta_t$，其中 $\\eta_t \\sim \\mathcal{N}(0, \\sigma_\\eta^2)$ 且 $\\sigma_\\eta^2 = \\sigma^2 (1 - \\rho^2)$ 以确保 $\\mathrm{Var}(\\varepsilon_t) = \\sigma^2$，种子 $789$。\n- 用例 D (极端杠杆值，小样本)：$n = 40$，$\\sigma^2 = 1.0$，误差 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$，通过将第一行的预测变量设置为 $x_{11} = 20$ 和 $x_{12} = 20$ 来使设计具有极端杠杆值，其余行生成为独立的标准正态预测变量，种子 $321$。\n\n对于每个用例，构建 $X$，根据给定的 $\\beta$ 和 $\\sigma^2$ 按用例机制生成 $y$，拟合模型，运行残差自助法，并按规定计算诊断和一致性决策。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含四个测试用例的结果，形式为一个用方括号括起来的逗号分隔列表。每个测试用例的结果必须是按 $[$一致性, 检测到异方差性, 检测到自相关$]$ 顺序排列的三个布尔值的列表。例如，一个输出可能看起来像 $[[\\mathrm{True},\\mathrm{False},\\mathrm{False}],[\\mathrm{False},\\mathrm{True},\\mathrm{False}],[\\mathrm{False},\\mathrm{False},\\mathrm{True}],[\\mathrm{False},\\mathrm{False},\\mathrm{False}]]$。", "solution": "该问题陈述在科学上是合理的、良定的，并为生物统计学中的一个计算实验提供了完整和形式化的规范。它要求在各种数据生成假设下，实现和评估用于线性回归的残差自助法程序。该问题是有效的，并且可以构建一个严谨的解决方案。\n\n所考虑的基本模型是线性回归模型 $y = X \\beta + \\varepsilon$，其中 $y \\in \\mathbb{R}^n$ 是响应向量，$X \\in \\mathbb{R}^{n \\times p}$ 是一个已知的、包含截距的满列秩固定设计矩阵，$\\beta \\in \\mathbb{R}^p$ 是未知系数向量，$\\varepsilon \\in \\mathbb{R}^n$ 是未观测到的随机误差向量。\n\n第一步是获得普通最小二乘（OLS）估计量 $\\hat{\\beta}$，它使残差平方和最小化。解由正规方程组给出：\n$$\n\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y\n$$\n由此，我们推导出几个关键量：拟合值向量 $\\hat{y} = X \\hat{\\beta}$，残差向量 $e = y - \\hat{y}$，以及帽子矩阵 $H = X(X^\\top X)^{-1}X^\\top$。帽子矩阵的对角元素 $h_{ii} = x_i^\\top(X^\\top X)^{-1}x_i$ 度量了每个观测值 $i$ 的杠杆值。\n\n问题的核心是构建和评估一个特定的残差自助法程序。该程序旨在通过模拟新数据集来近似 $\\hat{\\beta}$ 的抽样分布。标准的残差自助法假设误差 $\\varepsilon_i$ 是独立同分布的 (i.i.d.)。该程序试图通过从一组修正后的残差中重抽样来模拟未知的误差分布。指定的修正涉及中心化和标准化：\n$$\nu_i = \\frac{e_i - \\bar{e}}{\\sqrt{1 - h_{ii}}}\n$$\n其中 $\\bar{e}$ 是残差的样本均值。对于包含截距的模型，$\\bar{e}$ 理论上为零，但减去它可以提供数值稳健性。除以 $\\sqrt{1 - h_{ii}}$ 是因为即使真实误差 $\\varepsilon_i$ 的方差恒定，原始残差 $e_i$ 的方差也不是恒定的；具体来说，在经典假设下 $\\mathrm{Var}(e_i) = \\sigma^2 (1 - h_{ii})$。因此，集合 $\\{u_i\\}_{i=1}^n$ 形成了一个在理想条件下能更好地近似真实误差分布的样本。通过将 $\\sqrt{1 - h_{ii}}$ 替换为 $\\sqrt{\\max(1 - h_{ii}, 10^{-8})}$，分母在数值上变得稳定。\n\n自助法模拟进行 $B$ 次迭代。在每次迭代 $b=1, \\dots, B$ 中：\n1. 从集合 $\\{u_1, \\dots, u_n\\}$ 中有放回地抽取一个残差的自助样本 $u^\\ast = \\{u_1^\\ast, \\dots, u_n^\\ast\\}^\\top$。\n2. 生成一个自助响应向量 $y^\\ast = X\\hat{\\beta} + u^\\ast$。这种构建方法假设估计的模型 $y \\approx X\\hat{\\beta}$ 是一个良好的基线，并通过添加重抽样的误差来合成新数据。\n3. 通过将 OLS 应用于自助数据来计算自助系数估计 $\\hat{\\beta}^\\ast$：$\\hat{\\beta}^\\ast = (X^\\top X)^{-1}X^\\top y^\\ast$。\n\n这 $B$ 个重复样本的集合 $\\{\\hat{\\beta}^\\ast\\}$ 构成了对 $\\hat{\\beta}$ 抽样分布的一个经验近似。自助法标准差向量 $s^\\ast$ 是通过计算 $B \\times p$ 维自助法估计矩阵的列式样本标准差得到的。\n\n为了评估该自助法近似的“一致性”或质量，我们需要一个用于比较的黄金标准。该问题指定了两个这样的标准。首先，OLS 估计量的解析或“真实”方差-协方差矩阵由三明治公式给出：\n$$\n\\mathrm{Var}(\\hat{\\beta}) = (X^\\top X)^{-1} X^\\top \\mathrm{Var}(\\varepsilon) X (X^\\top X)^{-1} = (X^\\top X)^{-1} X^\\top \\Sigma_\\varepsilon X (X^\\top X)^{-1}\n$$\n其中 $\\Sigma_\\varepsilon$ 是误差的真实协方差矩阵。真实标准差向量 $s^{\\mathrm{true}}$ 是该矩阵对角元素的平方根。其次，执行一次高保真度的蒙特卡洛模拟。在 $M$ 次迭代中，从真实模型 $y_{\\mathrm{MC}} = X\\beta + \\varepsilon_{\\mathrm{MC}}$ 生成新的响应向量，其中 $\\varepsilon_{\\mathrm{MC}}$ 是从指定的真实误差分布中新抽取的一个样本。由此产生的估计量集合 $\\{\\hat{\\beta}_{\\mathrm{MC}}\\}$ 为 $\\hat{\\beta}$ 的真实抽样分布提供了一个高度准确的经验参考。\n\n如果满足两个标准，则声明为一致：\n1. 自助法标准差必须接近真实值：$\\max_{j} |s^\\ast_j - s^{\\mathrm{true}}_j| / s^{\\mathrm{true}}_j  \\tau_{\\mathrm{sd}}$。\n2. 边际自助法分布的形状必须与边际蒙特卡洛分布的形状相匹配，这通过确保最大柯尔莫哥洛夫-斯米尔诺夫距离低于阈值 $\\tau_{\\mathrm{KS}}$ 来验证。\n\n最后，对原始残差 $e$ 进行两个诊断检验，以检测对简单残差自助法所依据的假设的潜在违反。\n1. **异方差性**：采用 Breusch-Pagan 检验。它基于将残差平方 $e_i^2$ 对 $X$ 中的预测变量进行辅助回归。检验统计量 $T_{\\mathrm{BP}} = nR^2$（其中 $R^2$ 是该辅助回归的决定系数）与 $\\chi^2_{p-1}$ 分布进行比较。一个小的 $p$-值 ($p_{\\mathrm{BP}}  0.05$) 表明存在异方差性的证据。\n2. **自相关**：计算残差的滞后1阶样本自相关 $r_1$。一个值 $|r_1| \\ge \\rho_{\\mathrm{thr}}$ 暗示误差中存在序列相关。\n\n此框架被应用于四个测试用例，旨在探究自助法在理想条件（用例A）、异方差性（用例B）、自相关（用例C）以及存在高杠杆值数据点（用例D）的情况下的性能。这些测试的结果为我们提供了关于何时指定的残差自助法是可靠的工具，以及其失效模式如何与标准诊断检验的信号相关联的见解。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2, ks_2samp\nfrom scipy.linalg import toeplitz\n\ndef solve():\n    \"\"\"\n    Main solver function that orchestrates the analysis for all test cases.\n    \"\"\"\n    B = 800\n    M = 1500\n    TAU_SD = 0.25\n    TAU_KS = 0.15\n    RHO_THR = 0.25\n    P_VAL_THR = 0.05\n    BETA_TRUE = np.array([0.7, -1.2, 0.5])\n    P = len(BETA_TRUE)\n\n    test_cases = [\n        {\"name\": \"A\", \"n\": 200, \"sigma2\": 1.0, \"seed\": 123},\n        {\"name\": \"B\", \"n\": 200, \"sigma2\": 1.0, \"seed\": 456},\n        {\"name\": \"C\", \"n\": 200, \"sigma2\": 1.0, \"seed\": 789},\n        {\"name\": \"D\", \"n\": 40, \"sigma2\": 1.0, \"seed\": 321},\n    ]\n\n    final_results = []\n    for case in test_cases:\n        result = run_case(case, P, BETA_TRUE, B, M, TAU_SD, TAU_KS, RHO_THR, P_VAL_THR)\n        final_results.append(result)\n        \n    print(f\"[{','.join(map(str, final_results))}]\")\n\ndef run_case(case_params, p, beta_true, B, M, tau_sd, tau_ks, rho_thr, p_val_thr):\n    \"\"\"\n    Executes the full simulation and analysis for a single test case.\n    \"\"\"\n    n = case_params[\"n\"]\n    sigma2 = case_params[\"sigma2\"]\n    rng = np.random.default_rng(case_params[\"seed\"])\n\n    # 1. Generate design matrix X\n    if case_params[\"name\"] == \"D\":\n        X_pred = rng.standard_normal(size=(n - 1, p - 1))\n        X_pred = np.vstack([np.array([20.0, 20.0]), X_pred])\n    else:\n        X_pred = rng.standard_normal(size=(n, p - 1))\n    X = np.hstack([np.ones((n, 1)), X_pred])\n\n    # 2. Generate true error structure and one realization of y\n    eps_single, sigma_eps_mat, error_generator = get_error_structure(\n        case_params[\"name\"], X, n, sigma2, rng\n    )\n    y_single = X @ beta_true + eps_single\n    \n    # 3. Fit OLS model and get residuals\n    beta_hat, e, h_ii, y_hat = least_squares_and_residuals(X, y_single)\n    \n    # 4. Perform diagnostics\n    hetero_detected, ac_detected = run_diagnostics(e, X, n, p, rho_thr, p_val_thr)\n    \n    # 5. Perform residual bootstrap\n    s_star, bootstrap_betas = run_bootstrap(X, beta_hat, e, h_ii, n, p, B, rng)\n    \n    # 6. Perform Monte Carlo simulation\n    s_mc, mc_betas = run_monte_carlo(X, beta_true, error_generator, n, p, M)\n    \n    # 7. Compute true standard deviation\n    s_true = compute_true_sd(X, sigma_eps_mat)\n\n    # 8. Check for bootstrap consistency\n    rel_dev_check = np.max(np.abs(s_star - s_true) / s_true)  tau_sd\n    \n    ks_stats = [ks_2samp(bootstrap_betas[:, j], mc_betas[:, j]).statistic for j in range(p)]\n    ks_check = np.max(ks_stats)  tau_ks\n    \n    consistent = rel_dev_check and ks_check\n\n    return [consistent, hetero_detected, ac_detected]\n\ndef get_error_structure(case_name, X, n, sigma2, rng):\n    \"\"\"\n    Generates the error structure based on the case name.\n    \"\"\"\n    def generate_ar1_errors(rho, n_samples):\n        sigma_eta_sq = sigma2 * (1 - rho**2)\n        errors = np.zeros(n_samples)\n        errors[0] = rng.normal(0, np.sqrt(sigma2))\n        for t in range(1, n_samples):\n            errors[t] = rho * errors[t-1] + rng.normal(0, np.sqrt(sigma_eta_sq))\n        return errors\n\n    if case_name == \"A\" or case_name == \"D\":\n        eps_single = rng.normal(0, np.sqrt(sigma2), size=n)\n        sigma_eps_mat = sigma2 * np.identity(n)\n        error_generator = lambda: rng.normal(0, np.sqrt(sigma2), size=n)\n    elif case_name == \"B\":\n        x1 = X[:, 1]\n        sigma_i_sq = sigma2 * (1 + np.abs(x1))\n        eps_single = rng.normal(0, np.sqrt(sigma_i_sq))\n        sigma_eps_mat = np.diag(sigma_i_sq)\n        error_generator = lambda: rng.normal(0, np.sqrt(sigma_i_sq))\n    elif case_name == \"C\":\n        rho = 0.6\n        eps_single = generate_ar1_errors(rho, n)\n        r = rho ** np.arange(n)\n        sigma_eps_mat = sigma2 * toeplitz(r)\n        error_generator = lambda: generate_ar1_errors(rho, n)\n\n    return eps_single, sigma_eps_mat, error_generator\n\ndef least_squares_and_residuals(X, y):\n    \"\"\"\n    Computes OLS estimates, residuals, and hat matrix diagonals.\n    \"\"\"\n    XTX = X.T @ X\n    XTY = X.T @ y\n    beta_hat = np.linalg.solve(XTX, XTY)\n    y_hat = X @ beta_hat\n    e = y - y_hat\n    h_ii = np.sum(X * np.linalg.solve(XTX, X.T).T, axis=1)\n    return beta_hat, e, h_ii, y_hat\n\ndef run_diagnostics(e, X, n, p, rho_thr, p_val_thr):\n    \"\"\"\n    Computes Breusch-Pagan and autocorrelation diagnostics.\n    \"\"\"\n    # Breusch-Pagan Test\n    e_sq = e**2\n    Z = X # Auxiliary regression uses same design matrix with intercept.\n    gamma_hat = np.linalg.solve(Z.T @ Z, Z.T @ e_sq)\n    e_sq_hat = Z @ gamma_hat\n    sst = np.sum((e_sq - np.mean(e_sq))**2)\n    sse = np.sum((e_sq - e_sq_hat)**2)\n    r_squared = 1 - sse / sst if sst > 0 else 0\n    t_bp = n * r_squared\n    p_bp = 1 - chi2.cdf(t_bp, df=p - 1)\n    hetero_detected = p_bp  p_val_thr\n\n    # Autocorrelation Test\n    r1 = np.corrcoef(e[:-1], e[1:])[0, 1]\n    ac_detected = np.abs(r1) >= rho_thr\n    \n    return hetero_detected, ac_detected\n\ndef run_bootstrap(X, beta_hat, e, h_ii, n, p, B, rng):\n    \"\"\"\n    Performs the residual bootstrap procedure.\n    \"\"\"\n    e_bar = np.mean(e)\n    h_ii_stable = np.maximum(1 - h_ii, 1e-8)\n    u = (e - e_bar) / np.sqrt(h_ii_stable)\n    \n    bootstrap_betas = np.zeros((B, p))\n    XTX_inv = np.linalg.inv(X.T @ X)\n    XT = X.T\n    y_hat = X @ beta_hat\n    \n    for i in range(B):\n        u_star = rng.choice(u, size=n, replace=True)\n        y_star = y_hat + u_star\n        bootstrap_betas[i, :] = XTX_inv @ XT @ y_star\n\n    s_star = np.std(bootstrap_betas, axis=0, ddof=1)\n    return s_star, bootstrap_betas\n\ndef run_monte_carlo(X, beta_true, error_generator, n, p, M):\n    \"\"\"\n    Performs the Monte Carlo simulation for the reference distribution.\n    \"\"\"\n    mc_betas = np.zeros((M, p))\n    XTX_inv = np.linalg.inv(X.T @ X)\n    XT = X.T\n\n    for i in range(M):\n        eps_mc = error_generator()\n        y_mc = X @ beta_true + eps_mc\n        mc_betas[i, :] = XTX_inv @ XT @ y_mc\n        \n    s_mc = np.std(mc_betas, axis=0, ddof=1)\n    return s_mc, mc_betas\n    \ndef compute_true_sd(X, sigma_eps_mat):\n    \"\"\"\n    Computes the true model-based standard deviation of beta_hat.\n    \"\"\"\n    XTX_inv = np.linalg.inv(X.T @ X)\n    var_beta = XTX_inv @ X.T @ sigma_eps_mat @ X @ XTX_inv\n    s_true = np.sqrt(np.diag(var_beta))\n    return s_true\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "4949168"}]}