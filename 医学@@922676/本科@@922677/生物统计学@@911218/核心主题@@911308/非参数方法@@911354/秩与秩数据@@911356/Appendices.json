{"hands_on_practices": [{"introduction": "本实践将引导你探索非参数统计的基石——Wilcoxon秩和检验。你将通过递归方法，从第一性原理出发实现一个精确检验，从而构建零假设下的精确分布，这会让你对基于置换的推断有更深刻的理解。通过将精确的 $p$ 值与广泛使用的正态近似进行比较，你将体会到统计检验中计算强度与准确性之间的权衡关系 [@problem_id:4946603]。", "problem": "考虑两组独立的连续测量数据，没有重复值。Wilcoxon秩和统计量对于组$A$的定义是：将两组的所有观测值合并，并按升序排列（最小的观测值秩次为$1$）后，组$A$中所有观测值的秩次之和。在组标签可交换的原假设下，秩和统计量的精确抽样分布来自于将$n_1$个标签$A$分配给$N=n_1+n_2$个合并观测值的所有可能方式，每种分配方式都是等可能的。\n\n您的任务是，从第一性原理出发，实现一个递归来计算小样本量下Wilcoxon秩和统计量的精确双侧$p$值，并将其与带连续性校正的正态近似进行比较。推导应从秩次数据和基础概率的核心定义开始，并且不得依赖于已有的完整分布的封闭形式快捷公式。您可以使用经过充分检验的事实，例如中心极限定理下独立同分布秩次之和的行为。\n\n定义和要求：\n- 设$A=\\{a_1,\\dots,a_{n_1}\\}$和$B=\\{b_1,\\dots,b_{n_2}\\}$，$n_1\\geq 1$，$n_2\\geq 1$，且所有值均不相同。设$N=n_1+n_2$。\n- 设$W$表示观测到的组$A$的Wilcoxon秩和。\n- 在原假设下，$W$的精确分布是通过从$\\{1,2,\\dots,N\\}$中均匀选取$n_1$个秩次并求和得到的。基于此组合定义，使用递归来计算对于每个整数$s$，有多少种$n_1$个秩次的分配方式可以产生等于$s$的和。根据这些计数，计算精确的左尾概率$P(W\\leq w_{\\text{obs}})$和右尾概率$P(W\\geq w_{\\text{obs}})$，然后将精确双侧$p$值计算为$2\\cdot \\min\\{P(W\\leq w_{\\text{obs}}), P(W\\geq w_{\\text{obs}})\\}$，并截断于$1$。\n- 对于带连续性校正的正态近似，请从秩次的基本性质推导原假设下$W$的均值和方差，引用中心极限定理以正态分布近似$W$，并通过在标准化之前从绝对偏差$|w_{\\text{obs}}-\\mu|$中减去$0.5$来应用连续性校正。使用此方法计算近似的双侧$p$值。\n\n测试套件：\n实现您的程序，为以下五个测试用例计算精确的双侧$p$值和经连续性校正的正态近似值。在每个用例中，数组包含不同的实数：\n- 用例1：$A=[5.1,4.7,2.3]$, $B=[1.0,3.5,6.0,7.2]$。\n- 用例2：$A=[0.5]$, $B=[1.2,2.0,3.1,4.4,5.9]$。\n- 用例3：$A=[9.0,8.0]$, $B=[1.0,2.0,3.0]$。\n- 用例4：$A=[2.0,4.0,5.0,7.0]$, $B=[1.0,3.0,6.0,8.0]$。\n- 用例5：$A=[10.0,1.5,3.3]$, $B=[0.7,2.2,4.8,5.0,7.1]$。\n\n输出规格：\n- 对于每个用例，输出一个包含三个十进制数的列表：精确的双侧$p$值、经连续性校正的正态近似双侧$p$值，以及两者之间的绝对差。每个十进制数四舍五入到$6$位小数。\n- 将五个用例的结果汇总到一行，形式为一个逗号分隔的列表，其中包含这些三元列表，用方括号括起来，不含空格。\n- 您的程序必须是自包含的，不接受任何输入，并且仅使用指定的运行时环境。\n- 不涉及物理单位或角度，也不应打印百分比；要求使用十进制数。", "solution": "用户希望我解决一个关于Wilcoxon秩和检验的问题，这是生物统计学中的一个常见任务。该问题是有效的、有科学依据的、且定义明确。所有定义和条件都是标准的和自洽的。任务是为给定的数据集计算精确和近似的p值。我现在开始解答。\n\n### 第1部分：Wilcoxon秩和检验的形式化\n\n设有两组独立样本，组$A$有$n_1$个观测值，组$B$有$n_2$个观测值。我们假设数据是连续的，因此没有重复值。总观测数为$N = n_1 + n_2$。\n\nWilcoxon秩和检验是一种非参数检验，其原假设（$H_0$）是两个样本来自同一分布。在$H_0$下，标签'A'和'B'是可交换的。为执行此检验，我们首先合并所有$N$个观测值，并将它们从$1$（最小）到$N$（最大）进行排序。\n\n检验统计量$W$是分配给其中一组观测值的秩次之和。问题指定$W$为组$A$的秩和。\n\n在$H_0$下，组$A$的任何一组$n_1$个秩次都是等可能的。从集合$\\{1, 2, \\dots, N\\}$中选择$n_1$个秩次的总方式数由二项式系数$\\binom{N}{n_1}$给出。$W$的分布是从$\\{1, 2, \\dots, N\\}$中无放回地均匀随机选择$n_1$个整数之和的分布。\n\n### 第2部分：通过递归计算精确分布\n\n为了找到$W$的精确抽样分布，我们需要计算有多少个不同的$n_1$个秩次的集合其和为特定值$s$。这是一个经典的组合问题，可以使用递归关系解决，而递归关系适合采用动态规划实现。\n\n设$C(k, i, s)$为从集合$\\{1, 2, \\dots, k\\}$中选择$i$个不同整数，使其和恰好为$s$的方法数。我们想要找出对于所有可能的和值$s$，$C(N, n_1, s)$的分布。\n\n递归关系通过考虑最大元素$k$得出：\n1.  **情况1：不包含整数$k$。**我们必须从$\\{1, 2, \\dots, k-1\\}$中选择$i$个整数，使它们的和为$s$。这样做的方法数是$C(k-1, i, s)$。\n2.  **情况2：包含整数$k$。**我们必须从$\\{1, 2, \\dots, k-1\\}$中选择$i-1$个整数，使它们的和为$s-k$。这样做的方法数是$C(k-1, i-1, s-k)$。\n\n结合这两种情况，递归关系为：\n$$C(k, i, s) = C(k-1, i, s) + C(k-1, i-1, s-k)$$\n基本情况是：\n- 对于任何$k \\ge 0$，$C(k, 0, 0) = 1$（选择0个项目的方法只有一种，和为0）。\n- 如果$i>k$，$s<0$，或者$i=0$且$s \\neq 0$，则$C(k, i, s) = 0$。此外，如果$s$小于可能的最小和（$\\sum_{j=1}^i j$）或大于可能的最大和（$\\sum_{j=k-i+1}^k j$），则$C(k,i,s)=0$。\n\n一旦计算出所有相关$s$的计数$C(N, n_1, s)$，观测到特定秩和$s$的概率是：\n$$P(W=s) = \\frac{C(N, n_1, s)}{\\binom{N}{n_1}}$$\n对于观测到的秩和$w_{\\text{obs}}$，左尾和右尾概率为：\n$$P(W \\le w_{\\text{obs}}) = \\sum_{s \\le w_{\\text{obs}}} P(W=s)$$\n$$P(W \\ge w_{\\text{obs}}) = \\sum_{s \\ge w_{\\text{obs}}} P(W=s)$$\n然后，精确双侧p值定义为$2 \\cdot \\min\\{P(W \\le w_{\\text{obs}}), P(W \\ge w_{\\text{obs}})\\}$，并截断于$1$。\n\n### 第3部分：带连续性校正的正态近似\n\n对于较大的样本量，中心极限定理允许我们用正态分布来近似$W$的分布。为此，我们首先需要原假设下$W$的均值（$\\mu_W$）和方差（$\\sigma_W^2$）。\n\n统计量$W$是从秩次总体$\\{1, 2, \\dots, N\\}$中无放回地随机抽取的$n_1$个秩次之和。\n该秩次总体的均值为：\n$$ E[\\text{rank}] = \\frac{1}{N} \\sum_{j=1}^{N} j = \\frac{N(N+1)}{2N} = \\frac{N+1}{2} $$\n该秩次总体的方差为：\n$$ \\sigma^2_{\\text{rank}} = E[\\text{rank}^2] - (E[\\text{rank}])^2 = \\left(\\frac{1}{N} \\sum_{j=1}^{N} j^2\\right) - \\left(\\frac{N+1}{2}\\right)^2 $$\n使用平方和公式 $\\sum_{j=1}^N j^2 = \\frac{N(N+1)(2N+1)}{6}$，我们得到：\n$$ \\sigma^2_{\\text{rank}} = \\frac{(N+1)(2N+1)}{6} - \\frac{(N+1)^2}{4} = \\frac{N+1}{12} [2(2N+1) - 3(N+1)] = \\frac{N^2-1}{12} $$\n$W$的期望是抽取的$n_1$个秩次的期望之和：\n$$ \\mu_W = E[W] = E\\left[\\sum_{i=1}^{n_1} R_i\\right] = \\sum_{i=1}^{n_1} E[R_i] = n_1 E[\\text{rank}] = \\frac{n_1(N+1)}{2} $$\n从大小为$N$的总体中无放回抽样$n_1$个项目的和的方差由$Var(\\sum R_i) = n_1 \\sigma^2_{\\text{rank}} \\frac{N-n_1}{N-1}$给出。代入总体方差：\n$$ \\sigma_W^2 = Var(W) = n_1 \\left(\\frac{N^2-1}{12}\\right) \\frac{N-n_1}{N-1} = n_1 \\frac{(N-1)(N+1)}{12} \\frac{n_2}{N-1} = \\frac{n_1 n_2 (N+1)}{12} $$\n$W$在原假设下的分布可以用$\\mathcal{N}(\\mu_W, \\sigma_W^2)$来近似。由于$W$是一个离散变量，因此应用连续性校正来改善近似效果。校正涉及将观测值向均值方向调整$0.5$。标准化检验统计量$Z_c$是：\n$$ Z_c = \\frac{|w_{\\text{obs}} - \\mu_W| - 0.5}{\\sigma_W} $$\n如果$|w_{\\text{obs}} - \\mu_W| \\le 0.5$，分子变为非正数，导致$Z_c \\le 0$。对于标准正态变量$Z \\sim \\mathcal{N}(0,1)$，双侧p值计算为$2 \\cdot P(Z > Z_c)$。如果$Z_c \\le 0$，这个p值将$\\ge 1$，因此取为$1$。在其他情况下，它是根据标准正态分布的生存函数计算的。\n\n### 第4部分：测试用例的实现\n\n解决方案用Python实现。一个辅助函数为每个测试用例计算观测到的秩和$w_{\\text{obs}}$和样本大小。使用动态规划实现的递归公式找到$W$的精确分布，以避免冗余计算。根据该分布，计算出精确的p值。另一个独立的函数计算$W$的均值和方差，并使用带连续性校正的正态近似来找到近似的p值。然后计算绝对差。所有测试用例的结果都按指定格式进行格式化。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.special import comb\n\ndef get_wilcoxon_rank_sum(A, B):\n    \"\"\"\n    Calculates the Wilcoxon rank-sum statistic W for group A.\n    \n    Args:\n        A (list): Observations for group A.\n        B (list): Observations for group B.\n    \n    Returns:\n        tuple: (w_obs, n1, n2) where w_obs is the rank-sum for group A,\n               n1 is the size of A, and n2 is the size of B.\n    \"\"\"\n    n1 = len(A)\n    n2 = len(B)\n    \n    # Pool data and assign group labels\n    pooled_data = []\n    for x in A:\n        pooled_data.append((x, 'A'))\n    for x in B:\n        pooled_data.append((x, 'B'))\n    \n    # Sort pooled data by value to determine ranks\n    pooled_data.sort(key=lambda item: item[0])\n    \n    # Calculate rank sum for group A\n    w_obs = 0\n    for i, item in enumerate(pooled_data):\n        rank = i + 1  # Ranks are 1-based\n        if item[1] == 'A':\n            w_obs += rank\n            \n    return w_obs, n1, n2\n\nmemo_dist = {}\ndef compute_rank_sum_dist(n, k):\n    \"\"\"\n    Computes the exact distribution of the sum of n ranks chosen from {1,...,k}.\n    This is based on the recurrence C(k,i,s) = C(k-1,i,s) + C(k-1,i-1,s-k), where C\n    is the number of ways to choose i items from {1..k} to sum to s. This function\n    uses dynamic programming to find the counts for all possible sums.\n\n    Args:\n        n (int): The number of ranks to choose (n1).\n        k (int): The total number of ranks to choose from (N).\n    \n    Returns:\n        np.ndarray: An array where the index represents the sum and the value\n                    is the count of combinations yielding that sum.\n    \"\"\"\n    if (n, k) in memo_dist:\n        return memo_dist[(n, k)]\n\n    # The max possible sum for n items chosen from {1..k}\n    max_s = int(n * k - n * (n - 1) / 2)\n    \n    # DP table: dp[i][s] = # ways to choose i items with sum s from ranks {1..current_k}\n    # We use a space-optimized DP with a single 2D array.\n    dp = np.zeros((n + 1, max_s + 1), dtype=np.uint64)\n    dp[0, 0] = 1\n\n    for current_rank in range(1, k + 1):\n        # Iterate i backwards to use results from the previous 'current_rank' step\n        for i in range(min(current_rank, n), 0, -1):\n            for s in range(1, max_s + 1):\n                if s >= current_rank:\n                    dp[i, s] += dp[i - 1, s - current_rank]\n\n    dist = dp[n, :]\n    memo_dist[(n, k)] = dist\n    return dist\n\ndef get_exact_p_value(w_obs, n1, n2):\n    \"\"\"Calculates the exact two-sided p-value.\"\"\"\n    N = n1 + n2\n    \n    dist_counts = compute_rank_sum_dist(n1, N)\n    total_combinations = comb(N, n1, exact=True)\n    if total_combinations == 0: return 1.0\n    \n    # Calculate lower-tail and upper-tail probabilities\n    p_le = np.sum(dist_counts[:w_obs + 1]) / total_combinations\n    p_ge = np.sum(dist_counts[w_obs:]) / total_combinations\n    \n    p_val = 2 * min(p_le, p_ge)\n    return min(p_val, 1.0)\n\ndef get_normal_approx_p_value(w_obs, n1, n2):\n    \"\"\"Calculates the two-sided p-value using normal approximation with continuity correction.\"\"\"\n    N = n1 + n2\n    mu_W = n1 * (N + 1) / 2.0\n    var_W = n1 * n2 * (N + 1) / 12.0\n    \n    if var_W == 0:\n        return 1.0 if w_obs == mu_W else 0.0\n\n    sigma_W = np.sqrt(var_W)\n    deviation = abs(w_obs - mu_W)\n    \n    # Standardized score with continuity correction\n    z_c = (deviation - 0.5) / sigma_W\n    \n    if z_c <= 0:\n        p_val_approx = 1.0\n    else:\n        # Two-sided p-value is 2 * P(Z > z_c)\n        p_val_approx = 2 * norm.sf(z_c)\n        \n    return min(p_val_approx, 1.0)\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        ([5.1, 4.7, 2.3], [1.0, 3.5, 6.0, 7.2]),\n        ([0.5], [1.2, 2.0, 3.1, 4.4, 5.9]),\n        ([9.0, 8.0], [1.0, 2.0, 3.0]),\n        ([2.0, 4.0, 5.0, 7.0], [1.0, 3.0, 6.0, 8.0]),\n        ([10.0, 1.5, 3.3], [0.7, 2.2, 4.8, 5.0, 7.1]),\n    ]\n    \n    all_results = []\n\n    for A, B in test_cases:\n        w_obs, n1, n2 = get_wilcoxon_rank_sum(A, B)\n        \n        p_exact = get_exact_p_value(w_obs, n1, n2)\n        p_approx = get_normal_approx_p_value(w_obs, n1, n2)\n        abs_diff = abs(p_exact - p_approx)\n        \n        all_results.append(f\"[{p_exact:.6f},{p_approx:.6f},{abs_diff:.6f}]\")\n\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "4946603"}, {"introduction": "在精确检验原理的基础上，本练习将解决一个在真实世界数据中常见的挑战：数据结点（ties）。你将学习如何通过使用中位数秩（midranks）来处理结点观测值（例如来自李克特量表的数据），并调整枚举策略以构建Wilcoxon秩和统计量的精确零分布。这项实践对于将基于秩的检验正确应用于临床研究中常见的离散或有序数据至关重要 [@problem_id:4946639]。", "problem": "要求您实现一种针对秩统计量零分布的精确置换调整方法，该方法适用于结果为离散值（如李克特量表）的情况。在可交换性的零假设下，组成员身份的标签在合并后的响应多重集中是均匀随机分布的。对于离散结果，会出现秩次相同（结）的情况，此时必须使用中值秩。您的任务是推导、实现并评估一种精确枚举策略，该策略应遵循观测到的李克特变量的合并后类别计数。\n\n基本原理：\n- 在无组间效应的零假设下，组标签在所有合并观测值之间是可交换的。\n- 秩次相同（结）通过分配中值秩来处理：同一类别中的所有观测值获得它们在任意打破结的情况下所占秩次的平均值。\n- 对于一个大小为 $n_1$ 的组，Wilcoxon秩和统计量 $W$ 是该组中观测值的中值秩之和。\n- 合并后的观测总数为 $N = n_1 + n_2$，零分布下 $W$ 的均值为 $\\mu = n_1 (N + 1)/2$。\n\n您的程序必须：\n1. 通过对唯一的李克特类别进行升序排序，从合并数据中计算中值秩，并为每个合并后大小为 $c_k$ 的结块分配中值秩 $m_k$，其值等于该块秩次的平均值。\n2. 将Wilcoxon秩和统计量表示为 $W = \\sum_{k} x_k \\, m_k$，其中 $x_k$ 是A组在类别 $k$ 中的观测数量。向量 $(x_k)$ 的取值范围是所有满足 $0 \\le x_k \\le c_k$ 和 $\\sum_k x_k = n_1$ 的整数分配。\n3. 使用按组合计数加权的精确枚举来构建 $W$ 的精确零分布。每种分配 $(x_k)$ 对应 $\\prod_k \\binom{c_k}{x_k}$ 种不同的标签分配，在零假设下每种分配都是等可能的。标签分配的总数为 $\\binom{N}{n_1}$。因此，一种分配 $(x_k)$ 的概率为 $\\left(\\prod_k \\binom{c_k}{x_k}\\right) \\big/ \\binom{N}{n_1}$。\n4. 聚合具有相同 $W$ 值的分配，以获得 $W$ 的精确概率质量函数，并计算：\n   - 根据实际分组计算出的观测统计量 $w_{\\text{obs}}$。\n   - 左侧单边p值 $p_{\\text{left}} = \\mathbb{P}(W \\le w_{\\text{obs}})$。\n   - 右侧单边p值 $p_{\\text{right}} = \\mathbb{P}(W \\ge w_{\\text{obs}})$。\n   - 由与均值的距离定义的双侧p值：$p_{\\text{two}} = \\mathbb{P}(|W - \\mu| \\ge |w_{\\text{obs}} - \\mu|)$。\n5. 为避免计算中出现小数中值秩，将所有中值秩乘以因子2，使 $W$ 成为一个整数值统计量 $W^{\\ast} = 2 W$，同样地 $\\mu^{\\ast} = 2 \\mu = n_1 (N + 1)$。使用此缩放形式进行精确枚举和概率计算。\n\n测试套件：\n实现您的程序以处理以下四个测试用例。在每个用例中，A组和B组以李克特响应列表的形式给出。所有李克特值均为整数，并且必须从合并数据中计算中值秩。\n\n- 用例1（含多个结的常规路径）：\n  - A组：$[1,2,4,4,5]$\n  - B组：$[1,1,3,4,5,5,5]$\n\n- 用例2（退化的全等类别）：\n  - A组：$[3,3]$\n  - B组：$[3,3,3]$\n\n- 用例3（合并数据中无结）：\n  - A组：$[1,2]$\n  - B组：$[3,4,5]$\n\n- 用例4（样本量平衡且中心有结）：\n  - A组：$[2,2,3,4]$\n  - B组：$[1,2,3,3]$\n\n要求输出：\n- 对每个用例，输出一个列表 $[p_{\\text{two}}, p_{\\text{left}}, p_{\\text{right}}]$，其中每个条目都是一个精确到6位小数的浮点数。\n- 您的程序应生成单行输出，其中包含所有用例的结果，形式为一个由各用例列表组成的逗号分隔列表，并用方括号括起来。例如：$[[p_{1,\\text{two}},p_{1,\\text{left}},p_{1,\\text{right}}],[p_{2,\\text{two}},p_{2,\\text{left}},p_{2,\\text{right}}],\\dots]$。", "solution": "用户提供的问题是生物统计学领域一个有效且定义明确的问题，具体涉及使用离散数据进行非参数假设检验。所有提供的信息在科学上都是合理的、自洽的，并且足以推导出唯一的解决方案。问题的核心是为Wilcoxon秩和统计量实现一个精确置换检验，同时考虑到秩次相同（结）的情况，这是统计分析中的一个标准程序。\n\n方法论概述如下：\n\n1.  **数据合并与秩计算**：对于每个测试用例，将A组（大小为 $n_1$）和B组（大小为 $n_2$）的数据合并成一个大小为 $N = n_1 + n_2$ 的单一数据集。识别并排序其中的唯一值（李克特类别）。假设有 $K$ 个唯一类别。对于每个类别 $k \\in \\{1, \\dots, K\\}$，我们计算它在合并数据中的总频率 $c_k$。\n    类别 $k$ 中的 $c_k$ 个观测值所占的秩次位置从 $\\left(1 + \\sum_{j=1}^{k-1} c_j\\right)$ 到 $\\left(\\sum_{j=1}^{k} c_j\\right)$。类别 $k$ 的中值秩 $m_k$ 是这些秩次的平均值：\n    $$m_k = \\frac{\\left(1 + \\sum_{j=1}^{k-1} c_j\\right) + \\left(\\sum_{j=1}^{k} c_j\\right)}{2} = \\left(\\sum_{j=1}^{k-1} c_j\\right) + \\frac{c_k + 1}{2}$$\n    为了在枚举过程中避免浮点数运算，我们使用一个缩放后的中值秩 $m_k^* = 2m_k$，它保证是一个整数：\n    $$m_k^* = 2\\left(\\sum_{j=1}^{k-1} c_j\\right) + c_k + 1$$\n\n2.  **观测统计量与零假设参数**：A组的Wilcoxon秩和统计量 $W$ 是其观测值的秩之和。这可以用类别计数来表示。设 $x_k$ 是A组中落入类别 $k$ 的观测数量。那么缩放后的统计量 $W^*$ 为：\n    $$W^* = \\sum_{k=1}^{K} x_k m_k^*$$\n    我们使用提供的数据中A组的实际计数 $x_{k, \\text{obs}}$ 来计算该统计量的观测值 $w_{\\text{obs}}^*$。\n    在可交换性的零假设下，$W$ 分布的均值为 $\\mu = \\frac{n_1(N+1)}{2}$。缩放后的均值为 $\\mu^* = 2\\mu = n_1(N+1)$，它是一个整数。\n\n3.  **零分布的精确枚举**：关键步骤是在零假设下构建 $W^*$ 的精确概率质量函数 (PMF)。零假设意味着从 $N$ 个观测值的合并集中任选 $n_1$ 个观测值组成A组的概率是均等的。选择A组的总方式数为 $\\binom{N}{n_1}$。\n    一个等价且计算效率更高的角度是考虑将A组的 $n_1$ 个观测值分配到 $K$ 个类别中的所有可能分配方式。一个分配是一个计数向量 $(x_1, x_2, \\dots, x_K)$，满足以下约束条件：\n    $$0 \\le x_k \\le c_k \\quad \\text{for all } k=1, \\dots, K$$\n    $$\\sum_{k=1}^{K} x_k = n_1$$\n    对于每个这样的有效分配，形成A组的方式数为二项式系数的乘积 $\\prod_{k=1}^{K} \\binom{c_k}{x_k}$。这种分配的概率为：\n    $$\\mathbb{P}(x_1, \\dots, x_K) = \\frac{\\prod_{k=1}^{K} \\binom{c_k}{x_k}}{\\binom{N}{n_1}}$$\n    我们使用递归回溯算法生成所有有效的分配向量 $(x_k)$。对于每种分配，我们计算相应的统计量 $W^* = \\sum_k x_k m_k^*$及其概率。通过对所有产生相同 $W^*$ 值的分配的概率求和，可以构建出 $W^*$ 的完整PMF。\n\n4.  **P值计算**：利用 $W^*$ 的完整PMF，我们根据观测统计量 $w_{\\text{obs}}^*$ 和缩放后的均值 $\\mu^*$ 计算所需的p值：\n    -   **左侧p值**：$p_{\\text{left}} = \\mathbb{P}(W^* \\le w_{\\text{obs}}^*) = \\sum_{w^* \\le w_{\\text{obs}}^*} \\mathbb{P}(W^*=w^*)$\n    -   **右侧p值**：$p_{\\text{right}} = \\mathbb{P}(W^* \\ge w_{\\text{obs}}^*) = \\sum_{w^* \\ge w_{\\text{obs}}^*} \\mathbb{P}(W^*=w^*)$\n    -   **双侧p值**：这基于与均值的距离。设 $d_{\\text{obs}} = |w_{\\text{obs}}^* - \\mu^*|$。p值是观测到至少同样极端结果的概率：\n        $$p_{\\text{two}} = \\mathbb{P}(|W^* - \\mu^*| \\ge d_{\\text{obs}}) = \\sum_{w^* \\text{ s.t. } |w^*-\\mu^*| \\ge d_{\\text{obs}}} \\mathbb{P}(W^*=w^*)$$\n    由于 $W^*$、$\\mu^*$ 和 $w_{\\text{obs}}^*$ 都是整数，这些计算是精确的，不涉及浮点数比较问题。\n\n实现将遵循这一逻辑。一个递归函数将处理分配的枚举。将使用 `scipy.special.comb` 函数来计算二项式系数，以确保准确性并处理可能的大数，尽管测试用例足够小，这不成主要问题。最后，计算出的p值按要求四舍五入到6位小数。", "answer": "```python\nimport numpy as np\nfrom scipy.special import comb\nfrom collections import defaultdict\n\ndef calculate_p_values(group_a, group_b):\n    \"\"\"\n    Computes exact permutation p-values for the Wilcoxon rank-sum test with ties.\n    \n    Args:\n        group_a (list): List of observations for group A.\n        group_b (list): List of observations for group B.\n        \n    Returns:\n        list: A list containing [p_two, p_left, p_right] rounded to 6 decimal places.\n    \"\"\"\n    \n    # Step 1: Initialize data and basic parameters\n    n1 = len(group_a)\n    n2 = len(group_b)\n    N = n1 + n2\n    \n    # Handle trivial case where a group is empty\n    if n1 == 0 or n2 == 0:\n        return [1.0, 1.0, 1.0]\n\n    pooled_data = np.concatenate((group_a, group_b))\n    \n    # Step 2: Compute category counts and scaled midranks\n    unique_cats, cat_indices = np.unique(pooled_data, return_inverse=True)\n    num_cats = len(unique_cats)\n    \n    # c_k: counts of each unique category in the pooled data\n    c_k = np.bincount(cat_indices, minlength=num_cats)\n    \n    # x_k_obs: observed counts of each unique category in group A\n    group_a_cats = cat_indices[:n1]\n    x_k_obs = np.bincount(group_a_cats, minlength=num_cats)\n    \n    # m_star_k: scaled midranks for each category\n    cum_c = np.concatenate(([0], np.cumsum(c_k)))\n    m_star_k = 2 * cum_c[:-1] + c_k + 1\n\n    # Step 3: Compute observed statistic and mean\n    w_star_obs = np.sum(x_k_obs * m_star_k)\n    mu_star = n1 * (N + 1)\n\n    # Step 4: Enumerate all possible allocations and build the null distribution\n    distribution = defaultdict(float)\n    total_perms = comb(N, n1, exact=True)\n\n    allocations = []\n    def find_allocations(k_idx, n1_rem, current_alloc):\n        \"\"\"\n        Recursively find all valid allocations (x_k vectors).\n        \"\"\"\n        if k_idx == num_cats:\n            if n1_rem == 0:\n                allocations.append(current_alloc)\n            return\n\n        # Iterate through possible counts x_k for the current category\n        max_count = min(c_k[k_idx], n1_rem)\n        for x_k in range(max_count + 1):\n            find_allocations(k_idx + 1, n1_rem - x_k, current_alloc + [x_k])\n\n    find_allocations(0, n1, [])\n\n    for alloc_vec in allocations:\n        # Calculate the statistic W* for this allocation\n        w_star = np.sum(np.array(alloc_vec) * m_star_k)\n        \n        # Calculate the number of permutations for this allocation\n        prob_numerator = 1\n        for i in range(num_cats):\n            prob_numerator *= comb(c_k[i], alloc_vec[i], exact=True)\n            \n        # Add probability to the distribution map\n        probability = prob_numerator / total_perms\n        distribution[w_star] += probability\n\n    # Step 5: Calculate p-values from the null distribution\n    p_left = 0.0\n    p_right = 0.0\n    p_two = 0.0\n    \n    # d_obs is integer, as are all w_star and mu_star\n    d_obs = abs(w_star_obs - mu_star)\n\n    for w_star, prob in distribution.items():\n        if w_star <= w_star_obs:\n            p_left += prob\n        if w_star >= w_star_obs:\n            p_right += prob\n        if abs(w_star - mu_star) >= d_obs:\n            p_two += prob\n            \n    # Round results to 6 decimal places as required\n    return [round(p_two, 6), round(p_left, 6), round(p_right, 6)]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path with multiple ties)\n        {'group_a': [1, 2, 4, 4, 5], 'group_b': [1, 1, 3, 4, 5, 5, 5]},\n        \n        # Case 2 (degenerate all-equal category)\n        {'group_a': [3, 3], 'group_b': [3, 3, 3]},\n        \n        # Case 3 (no ties in the pooled data)\n        {'group_a': [1, 2], 'group_b': [3, 4, 5]},\n        \n        # Case 4 (balanced sizes with central ties)\n        {'group_a': [2, 2, 3, 4], 'group_b': [1, 2, 3, 3]}\n    ]\n    \n    results = []\n    for case in test_cases:\n        p_values = calculate_p_values(case['group_a'], case['group_b'])\n        results.append(p_values)\n        \n    # Format the final output string\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4946639"}, {"introduction": "最后的这项实践将我们的重点从假设检验转向相关性分析，展示了秩方法的多功能性。通过一个模拟研究，你将直接比较Spearman秩相关和Pearson相关在存在非线性关系和异常值时的表现。这个练习将让你亲身体验统计稳健性这一核心概念，这也是非参数方法在生物统计学中被广泛应用的一个关键原因 [@problem_id:4946608]。", "problem": "您将实现一项模拟研究，以比较 Spearman 秩相关系数（Spearman's $\\rho_s$）和 Pearson 积矩相关系数（Pearson's $\\rho$）在存在重尾噪声和点质量污染的单调非线性关系下的经验稳健性。该研究必须基于第一性原理：样本相关性的定义、秩变换，以及单调递增函数和重尾分布的基本性质。\n\n请使用以下适用于生物统计学中秩数据的基础依据：\n- 两个长度同为 $n$ 的实值向量 $x$ 和 $y$ 之间的样本 Pearson 相关性被定义为 $x$ 和 $y$ 的标准化样本协方差。\n- 两个实值向量 $x$ 和 $y$ 之间的样本 Spearman 相关性是在秩变换后的数据上计算的样本 Pearson 相关性，其中每个观测值被其在所有观测值中的秩所取代（对并列值使用平均秩）。\n- 如果对于所有 $x_1,x_2\\in\\mathbb{R}$，当 $x_1\\le x_2$ 时，总有 $g(x_1)\\le g(x_2)$ 成立，则函数 $g:\\mathbb{R}\\to\\mathbb{R}$ 是单调递增的。\n- 重尾噪声由低自由度的 Student's $t$ 分布表示。\n\n您的程序必须精确执行以下步骤：\n1. 对于每个测试用例，按如下方式模拟一个数据集 $\\{(X_i,Y_i)\\}_{i=1}^n$ 的 $R$ 次独立重复：\n   - 对于 $i=1,\\dots,n$，独立地生成 $X_i \\sim \\mathcal{N}(0,1)$。\n   - 生成独立噪声 $E_i \\sim t_{\\nu}$（自由度为 $\\nu$ 的 Student's $t$ 分布）。将噪声乘以一个固定尺度 $s$ 以控制其量级，得到 $s\\,E_i$。\n   - 选择一个单调递增函数 $g$ 并设置干净响应为 $Y_i^{(\\mathrm{clean})}=g(X_i)+s\\,E_i$。\n   - 通过无放回地均匀选择 $m=\\lfloor p\\,n\\rfloor$ 个索引，并通过 $Y_i^{(\\mathrm{cont})}=Y_i^{(\\mathrm{clean})}+M\\cdot S_i$ 替换这 $m$ 个值，来创建污染响应 $Y_i^{(\\mathrm{cont})}$。其中 $S_i$ 是独立的 Rademacher 符号，以等概率取值于 $\\{-1,+1\\}$。\n2. 对于每次重复，计算：\n   - 干净数据上的样本 Pearson 相关性 $\\widehat{\\rho}_{P}^{(\\mathrm{clean})}$ 和污染数据上的样本 Pearson 相关性 $\\widehat{\\rho}_{P}^{(\\mathrm{cont})}$。\n   - 干净数据上的样本 Spearman 相关性 $\\widehat{\\rho}_{S}^{(\\mathrm{clean})}$ 和污染数据上的样本 Spearman 相关性 $\\widehat{\\rho}_{S}^{(\\mathrm{cont})}$。\n3. 对于每次重复，计算绝对变化\n   $$\\Delta_P=\\left|\\widehat{\\rho}_{P}^{(\\mathrm{cont})}-\\widehat{\\rho}_{P}^{(\\mathrm{clean})}\\right|,\\qquad \\Delta_S=\\left|\\widehat{\\rho}_{S}^{(\\mathrm{cont})}-\\widehat{\\rho}_{S}^{(\\mathrm{clean})}\\right|.$$\n4. 将这些绝对变化在 $R$ 次重复中取平均，以获得该测试用例的 $\\overline{\\Delta}_P$ 和 $\\overline{\\Delta}_S$。\n5. 对于每个测试用例，返回一个布尔值，指示是否 $\\overline{\\Delta}_S \\le \\overline{\\Delta}_P$。\n\n您的实现必须遵循的科学原理：\n- 单调递增变换保持秩不变。因此，在没有污染的情况下，Spearman 相关性在边缘分布的严格单调递增变换下是不变的，而 Pearson 相关性则不然。\n- 重尾噪声增加了极端值的频率和量级，从而增加了依赖于有限二阶矩的估计量的敏感度。与基于秩的度量相比，已知 Pearson 相关性对这类极端值更为敏感。\n- 点质量污染引入了极端离群值，主要对稳健性构成压力。\n\n在所有测试用例中使用以下固定常数：\n- 重复次数 $R = 400$。\n- 噪声尺度 $s = 0.5$。\n- 污染量级 $M = 25$。\n- 随机数生成器种子固定为您选择的某个整数，以确保可复现性。\n\n使用以下单调递增函数 $g$：\n- $g_{\\mathrm{lin}}(x)=x$。\n- $g_{\\exp}(x)=\\exp(x/2)$。\n- $g_{\\mathrm{logit}}(x)=\\frac{1}{1+\\exp(-x)}$。\n- $g_{\\mathrm{cub}}(x)=x^3$。\n\n测试套件（请按此确切顺序提供结果）：\n- 用例 1：$g=g_{\\mathrm{lin}}$, $n=200$, $\\nu=3$, $p=0.1$。\n- 用例 2：$g=g_{\\exp}$, $n=200$, $\\nu=3$, $p=0.1$。\n- 用例 3：$g=g_{\\mathrm{logit}}$, $n=200$, $\\nu=1$, $p=0.1$。\n- 用例 4：$g=g_{\\mathrm{cub}}$, $n=80$, $\\nu=3$, $p=0.3$。\n- 用例 5：$g=g_{\\mathrm{lin}}$, $n=200$, $\\nu=3$, $p=0.0$。\n\n最终输出规范：\n- 您的程序应生成单行输出，其中包含一个含 5 个布尔值的列表，按顺序对应上述 5 个用例。当且仅当用例 $k$ 的 $\\overline{\\Delta}_S \\le \\overline{\\Delta}_P$ 时，第 $k$ 个布尔值必须为 $\\mathrm{True}$，否则为 $\\mathrm{False}$。\n- 该行必须格式化为 Python 风格的列表，使用逗号分隔且无空格，例如：$[\\mathrm{True},\\mathrm{False},\\mathrm{True},\\mathrm{True},\\mathrm{False}]$。\n- 不应打印任何额外文本。\n\n不涉及物理单位。不涉及角度。任何比例都必须以小数形式返回，但此处的最终输出仅包含布尔值。程序必须完全自包含，不需要用户输入、外部文件或网络访问。", "solution": "用户已请求实现一项模拟研究，以评估 Pearson 和 Spearman 相关系数的稳健性。该问题在科学上是合理的、良定的，并且所有参数和过程都已明确定义。因此，提供解决方案是合理的。\n\n### 基于原理的设计\n\n这个问题的核心在于比较两种常见的统计关联度量——Pearson 积矩相关系数 $\\widehat{\\rho}_P$ 和 Spearman 秩相关系数 $\\widehat{\\rho}_S$——的稳健性。在此背景下，稳健性指的是估计量在存在对理想假设的偏离（例如离群值或非正态误差分布）时的稳定性。\n\n**1. 理论基础**\n\n**Pearson 相关性 ($\\widehat{\\rho}_P$)**：样本 Pearson 相关系数度量两个连续变量 $x$ 和 $y$ 之间的线性关联。对于一个大小为 $n$ 的样本 $\\{(x_i, y_i)\\}_{i=1}^n$，它被定义为样本协方差除以样本标准差的乘积：\n$$ \\widehat{\\rho}_P(x,y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2}} $$\n其中 $\\bar{x}$ 和 $\\bar{y}$ 是样本均值。这个公式表明 $\\widehat{\\rho}_P$ 是直接根据原始数据值计算的。因此，它对离群值高度敏感。单个极端数据点可以任意地放大或缩小平方和与交叉乘积，不成比例地影响均值、方差和协方差，从而扭曲相关性估计。\n\n**Spearman 相关性 ($\\widehat{\\rho}_S$)**：样本 Spearman 相关系数评估两个变量之间单调关系的强度。它是一种非参数度量。其关键特征在于，它不是对原始数据进行操作，而是对数据的秩进行操作。该过程包括：\n1. 将向量 $x$ 转换为其秩的向量 $R_x$。\n2. 将向量 $y$ 转换为其秩的向量 $R_y$。如果出现并列值，则分配平均秩。\n3. 计算这些秩变换后向量的 Pearson 相关系数：\n$$ \\widehat{\\rho}_S(x,y) = \\widehat{\\rho}_P(R_x, R_y) $$\n通过将数据转换为秩，实际量级的影响被消除。一个极端离群值仅仅被赋予最高或最低的秩（例如，$1$ 或 $n$），这个值与所有其他秩位于相同的整数尺度上。这种变换限制了任何单个观测值的影响，使得 $\\widehat{\\rho}_S$ 在本质上比 $\\widehat{\\rho}_P$ 对离群值更稳健。\n\n**2. 模拟方法**\n\n该模拟旨在创建能够经验性地量化这些稳健性理论差异的数据集。\n\n**数据生成**：\n- 预测变量 $X_i$ 从标准正态分布 $X_i \\sim \\mathcal{N}(0,1)$ 中抽取，这是一个通用的基线。\n- 应用一个单调递增函数 $g$，即 $g(X_i)$，以创建一个非线性（或对于 $g(x)=x$ 是线性）关系。由于 $\\widehat{\\rho}_S$ 度量的是单调关联，即使关系不是线性的，它也预期表现良好，这与 $\\widehat{\\rho}_P$ 不同。\n- 添加重尾噪声 $s \\cdot E_i$，其中 $E_i \\sim t_{\\nu}$。具有低自由度 $\\nu$（例如，$\\nu=1$ 对应柯西分布，$\\nu=3$）的 Student's t 分布比正态分布具有更重的尾部，这意味着它会更频繁地产生极端值。这测试了估计量对非高斯噪声的敏感度。\n- 得到的“干净”数据是 $Y_i^{(\\mathrm{clean})} = g(X_i) + s \\cdot E_i$。\n- 对一部分比例为 $p$ 的数据引入点质量污染。一个大量级的冲击 $M \\cdot S_i$ 被加到 $m = \\lfloor p \\cdot n\\rfloor$ 个随机选择的观测值上，以创建 $Y_i^{(\\mathrm{cont})}$。这模拟了严重的测量误差或数据损坏，产生了旨在对估计量进行压力测试的极端离群值。\n\n**量化稳健性**：\n该模拟通过量化每个相关性估计在数据被污染时变化的程度来度量稳健性。对于 $R=400$ 次重复中的每一次，我们计算：\n- Pearson 相关性的变化：$\\Delta_P = |\\widehat{\\rho}_{P}^{(\\mathrm{cont})} - \\widehat{\\rho}_{P}^{(\\mathrm{clean})}|$。\n- Spearman 相关性的变化：$\\Delta_S = |\\widehat{\\rho}_{S}^{(\\mathrm{cont})} - \\widehat{\\rho}_{S}^{(\\mathrm{clean})}|$。\n\n较小的 $\\Delta$ 表示更强的稳健性。通过对多次重复的这些变化取平均，我们获得污染对每个系数的预期影响的稳定估计 $\\overline{\\Delta}_P$ 和 $\\overline{\\Delta}_S$。最终的比较 $\\overline{\\Delta}_S \\le \\overline{\\Delta}_P$ 对哪个估计量在指定条件下更稳健提供了一个直接的判断。\n\n**3. 实现计划**\n\n该程序将使用 `numpy` 和 `scipy` 库在 Python 中实现。\n- 随机数生成器（`numpy.random.default_rng`）的固定整数种子确保了可复现性。\n- 数据生成过程精确遵循指定步骤。将使用 `numpy.random.Generator.normal` 和 `numpy.random.Generator.standard_t` 来生成随机变量。\n- 通过首先创建干净响应向量的副本，然后使用 `numpy.random.Generator.choice` 无放回地选择索引，最后添加缩放后的 Rademacher 噪声来应用污染。\n- 为了计算 Pearson 相关性，使用 `numpy.corrcoef`。\n- 为了根据其定义计算 Spearman 相关性，首先使用 `scipy.stats.rankdata`（对并列值使用 `method='average'`）获取 $X$ 和 $Y$ 的秩，然后对这些秩向量应用 `numpy.corrcoef`。\n- 主要逻辑遍历五个测试用例，为每个用例执行 $R$ 次重复。在每个用例中，每次重复都会计算并累加绝对变化 $\\Delta_P$ 和 $\\Delta_S$。\n- 在一个用例的所有重复完成后，计算平均值 $\\overline{\\Delta}_P$ 和 $\\overline{\\Delta}_S$，并存储比较 $\\overline{\\Delta}_S \\le \\overline{\\Delta}_P$ 的布尔结果。\n- 最后，以指定格式打印所有测试用例的布尔值列表。$p=0.0$ 的特殊情况作为对照组；在没有污染的情况下，$\\Delta_P$ 和 $\\Delta_S$ 都应为零，使得不等式 $\\overline{\\Delta}_S \\le \\overline{\\Delta}_P$ 成立。对于所有其他情况，由于基于秩的 Spearman 相关性具有更优的稳健性，理论预测将得到相似的结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import rankdata\n\ndef solve():\n    \"\"\"\n    Implements a simulation study to compare the robustness of Pearson's and \n    Spearman's correlation coefficients under various conditions.\n    \"\"\"\n    # Define fixed constants for the simulation\n    R = 400  # Number of replicates\n    S_SCALE = 0.5  # Noise scale s\n    M_CONTAMINATION = 25.0  # Contamination magnitude M\n    SEED = 42  # Fixed seed for reproducibility\n\n    # Initialize the random number generator\n    rng = np.random.default_rng(SEED)\n\n    # Define the monotone increasing functions\n    def g_lin(x):\n        return x\n\n    def g_exp(x):\n        return np.exp(x / 2.0)\n\n    def g_logit(x):\n        return 1.0 / (1.0 + np.exp(-x))\n\n    def g_cub(x):\n        return x**3\n\n    # Define the test suite\n    test_cases = [\n        {'g': g_lin, 'n': 200, 'nu': 3, 'p': 0.1, 'name': 'g_lin'},\n        {'g': g_exp, 'n': 200, 'nu': 3, 'p': 0.1, 'name': 'g_exp'},\n        {'g': g_logit, 'n': 200, 'nu': 1, 'p': 0.1, 'name': 'g_logit'},\n        {'g': g_cub, 'n': 80, 'nu': 3, 'p': 0.3, 'name': 'g_cub'},\n        {'g': g_lin, 'n': 200, 'nu': 3, 'p': 0.0, 'name': 'g_lin_no_cont'},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        g, n, nu, p = case['g'], case['n'], case['nu'], case['p']\n        \n        total_delta_p = 0.0\n        total_delta_s = 0.0\n\n        for _ in range(R):\n            # Step 1: Generate data\n            # Generate X_i ~ N(0, 1)\n            x_data = rng.normal(loc=0.0, scale=1.0, size=n)\n            \n            # Generate noise E_i ~ t_nu\n            noise = rng.standard_t(df=nu, size=n)\n            \n            # Generate clean response Y_i^(clean)\n            y_clean = g(x_data) + S_SCALE * noise\n            \n            # Create contaminated response Y_i^(cont)\n            y_cont = np.copy(y_clean)\n            m = int(np.floor(p * n))\n            \n            if m > 0:\n                indices_to_contaminate = rng.choice(n, size=m, replace=False)\n                signs = rng.choice([-1, 1], size=m)\n                y_cont[indices_to_contaminate] += M_CONTAMINATION * signs\n            \n            # Step 2: Compute correlations\n            # Pearson correlations\n            rho_p_clean = np.corrcoef(x_data, y_clean)[0, 1]\n            rho_p_cont = np.corrcoef(x_data, y_cont)[0, 1]\n\n            # Spearman correlations (as Pearson on ranks)\n            rank_x = rankdata(x_data, method='average')\n            rank_y_clean = rankdata(y_clean, method='average')\n            rank_y_cont = rankdata(y_cont, method='average')\n            \n            rho_s_clean = np.corrcoef(rank_x, rank_y_clean)[0, 1]\n            rho_s_cont = np.corrcoef(rank_x, rank_y_cont)[0, 1]\n\n            # Step 3: Compute absolute changes\n            delta_p = np.abs(rho_p_cont - rho_p_clean)\n            delta_s = np.abs(rho_s_cont - rho_s_clean)\n            \n            total_delta_p += delta_p\n            total_delta_s += delta_s\n\n        # Step 4: Average absolute changes\n        avg_delta_p = total_delta_p / R\n        avg_delta_s = total_delta_s / R\n\n        # Step 5: Compare and store boolean result\n        results.append(avg_delta_s <= avg_delta_p)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4946608"}]}