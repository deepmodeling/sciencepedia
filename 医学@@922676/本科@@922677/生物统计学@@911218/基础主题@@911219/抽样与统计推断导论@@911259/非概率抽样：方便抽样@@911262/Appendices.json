{"hands_on_practices": [{"introduction": "在尝试对非概率样本进行任何校正之前，首要步骤是诊断和量化其与目标总体的差异。本练习将引导你使用标准化差异（standardized difference）这一核心工具来评估样本的代表性。通过计算并比较不同协变量的标准化差异，你将学会如何具体地衡量便利样本在关键特征上偏离总体的程度，这是理解和应对选择偏倚的基础 [@problem_id:4932704]。", "problem": "一个生物统计学团队正在评估一个通过智能手机应用程序招募的成年志愿者非概率便利样本，相对于同一地区已知目标总体的代表性。评估了四个协变量：年龄、身体质量指数、性别（女性）和当前吸烟状况。对于 $n_{s} = 600$ 名注册者的便利样本，观察到的摘要数据如下：平均年龄 $\\bar{x}_{\\text{age}, s} = 35$ 岁，样本标准差 $s_{\\text{age}, s} = 12$ 岁；平均身体质量指数 $\\bar{x}_{\\text{BMI}, s} = 26.8$ $\\mathrm{kg}/\\mathrm{m}^{2}$，样本标准差 $s_{\\text{BMI}, s} = 5.0$ $\\mathrm{kg}/\\mathrm{m}^{2}$；女性人数 $= 390$；当前吸烟者人数 $= 60$。从一个经过验证的人口登记处（视为参考总体），其总体大小为 $N_{p} = 50{,}000$，参考摘要数据如下：平均年龄 $\\mu_{\\text{age}, p} = 42$ 岁，总体标准差 $\\sigma_{\\text{age}, p} = 14$ 岁；平均身体质量指数 $\\mu_{\\text{BMI}, p} = 27.5$ $\\mathrm{kg}/\\mathrm{m}^{2}$，总体标准差 $\\sigma_{\\text{BMI}, p} = 5.2$ $\\mathrm{kg}/\\mathrm{m}^{2}$；女性人数 $= 26{,}000$；当前吸烟者人数 $= 9{,}000$。\n\n从均值、方差和比例的核心定义出发，计算每个协变量的绝对标准化差异，将便利样本与总体参考进行比较。将连续协变量（年龄、身体质量指数）视为在数值尺度上测量的，将二元协变量（女性、当前吸烟者）视为比例。采用以下阈值来标记代表性不佳：绝对标准化差异超过 $0.2$ 表示代表性不佳；值小于或等于 $0.1$ 表示代表性可接受。报告在这些阈值下被标记为代表性不佳的协变量数量的单个整数。无需四舍五入，因为最终答案是一个没有单位的计数。", "solution": "评估问题陈述的有效性。\n\n### 步骤1：提取已知信息\n提供了以下数据和条件：\n\n**便利样本 ($s$) 数据：**\n- 样本量：$n_{s} = 600$\n- 平均年龄：$\\bar{x}_{\\text{age}, s} = 35$ 岁\n- 年龄的样本标准差：$s_{\\text{age}, s} = 12$ 岁\n- 平均身体质量指数 (BMI)：$\\bar{x}_{\\text{BMI}, s} = 26.8$ $\\mathrm{kg}/\\mathrm{m}^{2}$\n- BMI 的样本标准差：$s_{\\text{BMI}, s} = 5.0$ $\\mathrm{kg}/\\mathrm{m}^{2}$\n- 女性人数：$390$\n- 当前吸烟者人数：$60$\n\n**参考总体 ($p$) 数据：**\n- 总体大小：$N_{p} = 50{,}000$\n- 总体平均年龄：$\\mu_{\\text{age}, p} = 42$ 岁\n- 总体年龄标准差：$\\sigma_{\\text{age}, p} = 14$ 岁\n- 总体平均 BMI：$\\mu_{\\text{BMI}, p} = 27.5$ $\\mathrm{kg}/\\mathrm{m}^{2}$\n- 总体 BMI 标准差：$\\sigma_{\\text{BMI}, p} = 5.2$ $\\mathrm{kg}/\\mathrm{m}^{2}$\n- 总体女性人数：$26{,}000$\n- 总体当前吸烟者人数：$9{,}000$\n\n**阈值和任务：**\n- 代表性不佳的阈值：绝对标准化差异 $> 0.2$\n- 代表性可接受的阈值：绝对标准化差异 $\\le 0.1$\n- 任务：计算被标记为代表性不佳的协变量数量。\n\n### 步骤2：使用提取的已知信息进行验证\n该问题具有科学依据，采用了公认的生物统计学概念“标准化差异”来评估样本代表性，这是评估选择偏倚的常见任务。所有提供的人口统计和健康指标数据（年龄、BMI、性别、吸烟状况）都是现实的。该问题提法明确，提供了所有必要信息和清晰、客观的标准以得出唯一结论。它没有歧义、矛盾或主观断言。该问题不违反任何指定的无效标准。\n\n### 步骤3：结论和行动\n问题有效。将提供完整的解决方案。\n\n主要任务是计算四个协变量中每一个的绝对标准化差异 (ASD)。ASD 的定义取决于协变量是连续的还是二元的。\n\n对于连续协变量，ASD 定义为样本均值 ($\\bar{x}_s$) 与总体均值 ($\\mu_p$) 之间的绝对差，并用总体标准差 ($\\sigma_p$) 进行标准化。\n$$\n\\text{ASD}_{\\text{continuous}} = \\frac{|\\bar{x}_s - \\mu_p|}{\\sigma_p}\n$$\n当将样本与已知真实参数 $\\sigma_p$ 可用的参考总体进行比较时，这是合适的公式。\n\n对于以比例为特征的二元协变量，ASD 定义为样本比例 ($p_s$) 与总体比例 ($P_p$) 之间的绝对差，并用总体中伯努利分布的标准差进行标准化，即 $\\sqrt{P_p(1-P_p)}$。\n$$\n\\text{ASD}_{\\text{binary}} = \\frac{|p_s - P_p|}{\\sqrt{P_p(1 - P_p)}}\n$$\n\n我们现在将计算四个协变量中每一个的 ASD。\n\n**1. 协变量：年龄（连续）**\n已知条件为 $\\bar{x}_{\\text{age}, s} = 35$，$\\mu_{\\text{age}, p} = 42$ 和 $\\sigma_{\\text{age}, p} = 14$。\n$$\n\\text{ASD}_{\\text{age}} = \\frac{|\\bar{x}_{\\text{age}, s} - \\mu_{\\text{age}, p}|}{\\sigma_{\\text{age}, p}} = \\frac{|35 - 42|}{14} = \\frac{|-7|}{14} = \\frac{7}{14} = 0.5\n$$\n\n**2. 协变量：身体质量指数 (BMI)（连续）**\n已知条件为 $\\bar{x}_{\\text{BMI}, s} = 26.8$，$\\mu_{\\text{BMI}, p} = 27.5$ 和 $\\sigma_{\\text{BMI}, p} = 5.2$。\n$$\n\\text{ASD}_{\\text{BMI}} = \\frac{|\\bar{x}_{\\text{BMI}, s} - \\mu_{\\text{BMI}, p}|}{\\sigma_{\\text{BMI}, p}} = \\frac{|26.8 - 27.5|}{5.2} = \\frac{|-0.7|}{5.2} = \\frac{0.7}{5.2} \\approx 0.1346\n$$\n\n**3. 协变量：性别（女性）（二元）**\n首先，我们计算样本比例和总体比例。\n女性的样本比例 $p_{\\text{female}, s}$ 是：\n$$\np_{\\text{female}, s} = \\frac{\\text{样本中女性人数}}{\\text{样本量}} = \\frac{390}{600} = \\frac{39}{60} = \\frac{13}{20} = 0.65\n$$\n女性的总体比例 $P_{\\text{female}, p}$ 是：\n$$\nP_{\\text{female}, p} = \\frac{\\text{总体中女性人数}}{\\text{总体大小}} = \\frac{26{,}000}{50{,}000} = \\frac{26}{50} = \\frac{13}{25} = 0.52\n$$\n现在，我们计算 ASD：\n$$\n\\text{ASD}_{\\text{female}} = \\frac{|p_{\\text{female}, s} - P_{\\text{female}, p}|}{\\sqrt{P_{\\text{female}, p}(1 - P_{\\text{female}, p})}} = \\frac{|0.65 - 0.52|}{\\sqrt{0.52(1 - 0.52)}} = \\frac{0.13}{\\sqrt{0.52 \\times 0.48}} = \\frac{0.13}{\\sqrt{0.2496}} \\approx 0.2602\n$$\n\n**4. 协变量：当前吸烟状况（二元）**\n首先，我们计算样本比例和总体比例。\n吸烟者的样本比例 $p_{\\text{smoker}, s}$ 是：\n$$\np_{\\text{smoker}, s} = \\frac{\\text{样本中吸烟者人数}}{\\text{样本量}} = \\frac{60}{600} = \\frac{1}{10} = 0.1\n$$\n吸烟者的总体比例 $P_{\\text{smoker}, p}$ 是：\n$$\nP_{\\text{smoker}, p} = \\frac{\\text{总体中吸烟者人数}}{\\text{总体大小}} = \\frac{9{,}000}{50{,}000} = \\frac{9}{50} = 0.18\n$$\n现在，我们计算 ASD：\n$$\n\\text{ASD}_{\\text{smoker}} = \\frac{|p_{\\text{smoker}, s} - P_{\\text{smoker}, p}|}{\\sqrt{P_{\\text{smoker}, p}(1 - P_{\\text{smoker}, p})}} = \\frac{|0.1 - 0.18|}{\\sqrt{0.18(1 - 0.18)}} = \\frac{|-0.08|}{\\sqrt{0.18 \\times 0.82}} = \\frac{0.08}{\\sqrt{0.1476}} \\approx 0.2082\n$$\n\n最后，我们将每个 ASD 与代表性不佳的阈值（$> 0.2$）进行比较。\n\n- **年龄**：$\\text{ASD}_{\\text{age}} = 0.5$。由于 $0.5 > 0.2$，该协变量被标记。\n- **BMI**：$\\text{ASD}_{\\text{BMI}} \\approx 0.1346$。由于 $0.1346 \\le 0.2$，该协变量未被标记。\n- **性别（女性）**：$\\text{ASD}_{\\text{female}} \\approx 0.2602$。由于 $0.2602 > 0.2$，该协变量被标记。\n- **吸烟**：$\\text{ASD}_{\\text{smoker}} \\approx 0.2082$。由于 $0.2082 > 0.2$，该协变量被标记。\n\n被标记为代表性不佳的协变量是年龄、性别（女性）和当前吸烟状况。被标记的协变量总数为 $3$。", "answer": "$$\\boxed{3}$$", "id": "4932704"}, {"introduction": "在诊断出样本不具代表性后，下一步便是采取方法进行校正。本练习将介绍校准加权（calibration weighting），这是一种强大的技术，它通过构建一个倾向性模型来生成权重，使得加权后的样本特征（如均值和二阶矩）与已知的总体基准相匹配。你将通过一个优化过程来解决这个问题，该过程不仅要实现矩匹配，还要通过惩罚项来确保权重的稳定性，这体现了在处理抽样数据时对偏倚和方差的权衡 [@problem_id:4932764]。", "problem": "在生物统计学中，考虑一个方便样本，其中观察了 $n$ 个个体的标量协变量 $X \\in \\mathbb{R}$，但其入选概率未知。设 $S \\in \\{0,1\\}$ 为选择指示符，当个体被纳入方便样本时为 $S=1$，否则为 $S=0$。在非概率抽样中，选择概率（也称为倾向性）是未知的，并且可能依赖于 $X$。假设目标（外部）总体为 $X$ 的前两个原始矩提供了可靠的基准，即 $m_1 = \\mathbb{E}[X]$ 和 $m_2 = \\mathbb{E}[X^2]$。目标是校准一个参数化的倾向性模型，使得方便样本的逆概率加权矩与这些总体矩对齐。\n\n采用逻辑斯谛倾向性模型 $p(x;\\boldsymbol{\\theta}) = \\operatorname{logit}^{-1}(\\theta_0 + \\theta_1 x)$，其参数向量为 $\\boldsymbol{\\theta} = (\\theta_0,\\theta_1)^\\top$，其中 $\\operatorname{logit}^{-1}(z) = \\frac{1}{1 + e^{-z}}$。为每个样本单元 $i=1,\\dots,n$ 定义逆倾向性权重 $w_i(\\boldsymbol{\\theta}) = \\frac{1}{p(x_i;\\boldsymbol{\\theta})}$。给定权重 $w_i(\\boldsymbol{\\theta})$，定义 $k \\in \\{1,2\\}$ 阶的加权原始矩为\n$$\n\\widehat{M}_k(\\boldsymbol{\\theta}) = \\frac{\\sum_{i=1}^n w_i(\\boldsymbol{\\theta}) \\, x_i^k}{\\sum_{i=1}^n w_i(\\boldsymbol{\\theta})}.\n$$\n为抑制不稳定的权重和极端的倾向性，引入一个基于权重平方变异系数的惩罚项，\n$$\n\\operatorname{CV}^2\\big(w(\\boldsymbol{\\theta})\\big) = \\frac{\\operatorname{Var}\\big(w_1(\\boldsymbol{\\theta}),\\dots,w_n(\\boldsymbol{\\theta})\\big)}{\\left(\\frac{1}{n}\\sum_{i=1}^n w_i(\\boldsymbol{\\theta})\\right)^2},\n$$\n以及一个针对倾向性的障碍项，\n$$\nB(\\boldsymbol{\\theta}) = \\sum_{i=1}^n \\left[-\\log\\big(p(x_i;\\boldsymbol{\\theta})\\big) - \\log\\big(1 - p(x_i;\\boldsymbol{\\theta})\\big)\\right].\n$$\n通过最小化以下损失来校准 $\\boldsymbol{\\theta}$，\n$$\nL(\\boldsymbol{\\theta}) = \\sum_{k=1}^2 \\left(\\widehat{M}_k(\\boldsymbol{\\theta}) - m_k\\right)^2 + \\alpha \\,\\operatorname{CV}^2\\big(w(\\boldsymbol{\\theta})\\big) + \\beta \\, B(\\boldsymbol{\\theta}),\n$$\n其中 $\\alpha \\ge 0$ 和 $\\beta \\ge 0$ 是用户选择的惩罚权重。\n\n从逆概率加权（Inverse Probability Weighting, IPW）的定义和逻辑斯谛倾向性模型出发，推导出一个算法，对于给定的方便样本 $x_1,\\dots,x_n$、总体基准 $m_1,m_2$ 和惩罚权重 $\\alpha,\\beta$，计算 $\\widehat{\\boldsymbol{\\theta}} = \\arg\\min_{\\boldsymbol{\\theta}} L(\\boldsymbol{\\theta})$。该算法必须为每个测试用例生成校准后的参数和诊断值。\n\n实现一个程序，对以下每个测试用例执行校准，并返回校准后的参数和诊断信息：\n- 情况 A（一般情况）：$X = [ -0.21, 0.05, 0.37, -0.62, 0.12, -0.33, -0.08, 0.41, 0.79, -0.45, 1.12, 0.58, -0.14, 0.27, -0.71, 0.93, 0.36, -0.50, 0.10, 0.02 ]$，$m_1 = 0.50$，$m_2 = 1.25$，$\\alpha = 0.10$，$\\beta = 0.01$。\n- 情况 B（接近对齐边界）：$X = [ -0.20, 0.30, 0.90, 0.60, 1.40, -0.10, 0.70, 0.20, 0.50, -0.30, 1.10, 0.40, 0.00, 1.60, -0.50, 0.80, -0.10, 0.30, 0.90, 0.40 ]$，$m_1 = 0.50$，$m_2 = 1.25$，$\\alpha = 0.01$，$\\beta = 0.01$。\n- 情况 C（极端协变量边缘）：$X = [ 5.00, -4.00, 3.50, -3.20, 2.80, -2.50, 2.30, -2.10, 1.90, -1.70, 1.50, -1.30, 1.10, -0.90, 0.70, -0.50, 0.30, -0.20, 0.10, -0.10 ]$，$m_1 = 0.00$，$m_2 = 2.00$，$\\alpha = 1.00$，$\\beta = 0.05$。\n\n在所有情况下，使用相同的逻辑斯谛链接函数，从 $\\boldsymbol{\\theta}^{(0)} = (0.00, 0.00)^\\top$ 开始初始化，并将参数约束在界限 $\\theta_0 \\in [-10.00, 10.00]$，$\\theta_1 \\in [-10.00, 10.00]$ 内。如果在计算 $B(\\boldsymbol{\\theta})$ 和权重 $w_i(\\boldsymbol{\\theta})$ 时为了数值稳定性需要，使用 $\\epsilon = 10^{-6}$ 将倾向性裁剪到开区间 $(\\epsilon, 1 - \\epsilon)$ 内。\n\n你的程序应为每种情况计算校准后的参数向量 $\\widehat{\\boldsymbol{\\theta}} = (\\widehat{\\theta}_0,\\widehat{\\theta}_1)$、优化后的加权原始矩 $\\widehat{M}_1(\\widehat{\\boldsymbol{\\theta}})$ 和 $\\widehat{M}_2(\\widehat{\\boldsymbol{\\theta}})$，以及最小化的损失 $L(\\widehat{\\boldsymbol{\\theta}})$。所有三个测试用例的最终输出必须是单行，包含一个由方括号括起来的逗号分隔列表，该列表由按顺序连接每个案例的五个浮点数构成：\n$$\n[ \\widehat{\\theta}_{0,A}, \\widehat{\\theta}_{1,A}, \\widehat{M}_{1,A}, \\widehat{M}_{2,A}, L_A, \\widehat{\\theta}_{0,B}, \\widehat{\\theta}_{1,B}, \\widehat{M}_{1,B}, \\widehat{M}_{2,B}, L_B, \\widehat{\\theta}_{0,C}, \\widehat{\\theta}_{1,C}, \\widehat{M}_{1,C}, \\widehat{M}_{2,C}, L_C ]\n$$", "solution": "### 问题验证\n\n#### 第1步：提取已知条件\n\n问题提供了以下数据、定义和约束：\n\n*   **模型**：逻辑斯谛倾向性模型 $p(x;\\boldsymbol{\\theta}) = \\operatorname{logit}^{-1}(\\theta_0 + \\theta_1 x) = \\frac{1}{1 + e^{-(\\theta_0 + \\theta_1 x)}}$，参数向量为 $\\boldsymbol{\\theta} = (\\theta_0,\\theta_1)^\\top$。\n*   **逆倾向性权重**：对于方便样本 $x_1, \\dots, x_n$，有 $w_i(\\boldsymbol{\\theta}) = \\frac{1}{p(x_i;\\boldsymbol{\\theta})}$。\n*   **加权原始矩**：对于 $k \\in \\{1,2\\}$，有 $\\widehat{M}_k(\\boldsymbol{\\theta}) = \\frac{\\sum_{i=1}^n w_i(\\boldsymbol{\\theta}) \\, x_i^k}{\\sum_{i=1}^n w_i(\\boldsymbol{\\theta})}$。\n*   **总体基准**：目标矩 $m_1 = \\mathbb{E}[X]$ 和 $m_2 = \\mathbb{E}[X^2]$。\n*   **惩罚项**：\n    *   权重的平方变异系数：$\\operatorname{CV}^2\\big(w(\\boldsymbol{\\theta})\\big) = \\frac{\\operatorname{Var}\\big(w_1(\\boldsymbol{\\theta}),\\dots,w_n(\\boldsymbol{\\theta})\\big)}{\\left(\\frac{1}{n}\\sum_{i=1}^n w_i(\\boldsymbol{\\theta})\\right)^2}$。\n    *   倾向性障碍项：$B(\\boldsymbol{\\theta}) = \\sum_{i=1}^n \\left[-\\log\\big(p(x_i;\\boldsymbol{\\theta})\\big) - \\log\\big(1 - p(x_i;\\boldsymbol{\\theta})\\big)\\right]$。\n*   **损失函数**：$L(\\boldsymbol{\\theta}) = \\sum_{k=1}^2 \\left(\\widehat{M}_k(\\boldsymbol{\\theta}) - m_k\\right)^2 + \\alpha \\,\\operatorname{CV}^2\\big(w(\\boldsymbol{\\theta})\\big) + \\beta \\, B(\\boldsymbol{\\theta})$，其中 $\\alpha \\ge 0$ 和 $\\beta \\ge 0$ 是惩罚权重。\n*   **任务**：找到 $\\widehat{\\boldsymbol{\\theta}} = \\arg\\min_{\\boldsymbol{\\theta}} L(\\boldsymbol{\\theta})$。\n*   **数值参数**：\n    *   初始猜测值：$\\boldsymbol{\\theta}^{(0)} = (0.00, 0.00)^\\top$。\n    *   参数界限：$\\theta_0 \\in [-10.00, 10.00]$，$\\theta_1 \\in [-10.00, 10.00]$。\n    *   倾向性裁剪阈值：$\\epsilon = 10^{-6}$。\n*   **测试用例**：\n    *   情况 A：$X = [ -0.21, 0.05, 0.37, -0.62, 0.12, -0.33, -0.08, 0.41, 0.79, -0.45, 1.12, 0.58, -0.14, 0.27, -0.71, 0.93, 0.36, -0.50, 0.10, 0.02 ]$，$m_1 = 0.50$，$m_2 = 1.25$，$\\alpha = 0.10$，$\\beta = 0.01$。\n    *   情况 B：$X = [ -0.20, 0.30, 0.90, 0.60, 1.40, -0.10, 0.70, 0.20, 0.50, -0.30, 1.10, 0.40, 0.00, 1.60, -0.50, 0.80, -0.10, 0.30, 0.90, 0.40 ]$，$m_1 = 0.50$，$m_2 = 1.25$，$\\alpha = 0.01$，$\\beta = 0.01$。\n    *   情况 C：$X = [ 5.00, -4.00, 3.50, -3.20, 2.80, -2.50, 2.30, -2.10, 1.90, -1.70, 1.50, -1.30, 1.10, -0.90, 0.70, -0.50, 0.30, -0.20, 0.10, -0.10 ]$，$m_1 = 0.00$，$m_2 = 2.00$，$\\alpha = 1.00$，$\\beta = 0.05$。\n\n#### 第2步：使用提取的已知条件进行验证\n\n根据验证标准对问题进行评估：\n*   **科学依据**：该问题在非概率抽样和调查数据校准的统计理论中有充分的依据。逆概率加权、使用逻辑斯谛回归进行倾向性建模以及通过惩罚函数进行正则化，都是生物统计学及相关领域的标准且成熟的技术。\n*   **适定性**：该问题是适定的。它指定了一个明确的目标函数 $L(\\boldsymbol{\\theta})$，需要相对于参数向量 $\\boldsymbol{\\theta}$ 进行最小化。所有必要的数据、模型、初始条件和约束（参数界限）都已提供，以定义一个可解的数值优化问题。损失函数是连续的，其组成部分旨在确保一个性状良好的优化曲面，这表明存在稳定且有意义的解。\n*   **客观性**：问题是使用精确、客观的数学语言陈述的。所有术语都有正式定义。测试用例由明确的数值数据组成。\n\n问题陈述没有科学上不健全、不完整、矛盾或模糊等缺陷。\n\n#### 第3步：结论与行动\n\n问题是**有效的**。将提供解决方案。\n\n### 解法推导\n\n问题的核心是找到参数向量 $\\widehat{\\boldsymbol{\\theta}} = (\\widehat{\\theta}_0, \\widehat{\\theta}_1)^\\top$ 来最小化指定的损失函数 $L(\\boldsymbol{\\theta})$。这是一个数值优化问题。该算法包括定义损失函数并使用数值求解器找到最小值。\n\n**1. 目标函数构建**\n\n需要最小化的目标函数是：\n$$\nL(\\boldsymbol{\\theta}) = \\sum_{k=1}^2 \\left(\\widehat{M}_k(\\boldsymbol{\\theta}) - m_k\\right)^2 + \\alpha \\,\\operatorname{CV}^2\\big(w(\\boldsymbol{\\theta})\\big) + \\beta \\, B(\\boldsymbol{\\theta})\n$$\n该函数包含三个关键组成部分：\n*   **矩匹配项**：$\\sum_{k=1}^2 \\left(\\widehat{M}_k(\\boldsymbol{\\theta}) - m_k\\right)^2$。该项惩罚逆倾向性加权样本矩 $\\widehat{M}_1(\\boldsymbol{\\theta})$ 和 $\\widehat{M}_2(\\boldsymbol{\\theta})$ 与已知总体矩 $m_1$ 和 $m_2$ 之间的偏差。最小化此项驱动校准过程。\n*   **权重变化惩罚项**：$\\alpha \\,\\operatorname{CV}^2\\big(w(\\boldsymbol{\\theta})\\big)$。这是一个由超参数 $\\alpha$ 控制的正则化项。它通过权重的平方变异系数来惩罚权重 $w_i(\\boldsymbol{\\theta})$ 的高变异性。这可以抑制少数个体具有极大权重从而导致估计不稳定和高方差的解。\n*   **倾向性障碍惩罚项**：$\\beta \\, B(\\boldsymbol{\\theta})$。该项由 $\\beta$ 控制，惩罚过于接近 $0$ 或 $1$ 的倾向性 $p(x_i; \\boldsymbol{\\theta})$。当 $p \\to 0$ 或 $p \\to 1$ 时，函数 $-\\log(p) - \\log(1-p)$ 趋于无穷大，起到障碍的作用。这增强了数值稳定性，并使模型保持在合理范围内。\n\n**2. 各组成部分的定义**\n\n$L(\\boldsymbol{\\theta})$ 的各组成部分建立在逻辑斯谛倾向性模型之上。对于每个观测值 $x_i$ 和参数向量 $\\boldsymbol{\\theta} = (\\theta_0, \\theta_1)^\\top$：\n*   线性预测器为 $\\eta_i = \\theta_0 + \\theta_1 x_i$。\n*   倾向性为 $p_i(\\boldsymbol{\\theta}) = \\frac{1}{1 + e^{-\\eta_i}}$。为保证数值稳定性，$p_i$ 被裁剪到区间 $[\\epsilon, 1-\\epsilon]$ 内，其中 $\\epsilon=10^{-6}$。\n*   权重为 $w_i(\\boldsymbol{\\theta}) = 1 / p_i(\\boldsymbol{\\theta})$。\n*   加权矩为 $\\widehat{M}_1(\\boldsymbol{\\theta}) = \\frac{\\sum_i w_i x_i}{\\sum_i w_i}$ 和 $\\widehat{M}_2(\\boldsymbol{\\theta}) = \\frac{\\sum_i w_i x_i^2}{\\sum_i w_i}$。\n*   平方CV是在权重集合 $\\{w_i\\}_{i=1}^n$ 上计算的 $\\frac{\\text{np.var}(w)}{\\text{np.mean}(w)^2}$。\n*   障碍项为 $B(\\boldsymbol{\\theta}) = \\sum_i [-\\log(p_i) - \\log(1-p_i)]$。\n\n**3. 优化算法**\n\n损失函数 $L(\\boldsymbol{\\theta})$ 是 $\\boldsymbol{\\theta}$ 的一个非线性函数。求 $\\arg\\min_{\\boldsymbol{\\theta}} L(\\boldsymbol{\\theta})$ 的解析解是不可行的。因此，需要使用数值优化算法。\n\n这是一个约束优化问题，因为参数有界：$\\theta_0, \\theta_1 \\in [-10.00, 10.00]$。一个合适的算法是能够处理箱式约束的拟牛顿法，例如 **L-BFGS-B（带界限的有限内存Broyden–Fletcher–Goldfarb–Shanno）** 算法。该算法使用梯度和Hessian矩阵的近似值来迭代地逼近函数的最小值。\n\n计算步骤如下：\n1.  实现一个函数，对于给定的向量 $\\boldsymbol{\\theta}$ 和输入数据（$X$, $m_1$, $m_2$, $\\alpha$, $\\beta$），计算损失 $L(\\boldsymbol{\\theta})$。该函数将包括计算倾向性、权重和损失所有组成部分的逻辑。\n2.  使用一个数值优化例程，例如 SciPy 库中的 `scipy.optimize.minimize`，并配置其使用 `L-BFGS-B` 方法。\n3.  向优化器提供损失函数、初始参数猜测值 $\\boldsymbol{\\theta}^{(0)} = (0, 0)^\\top$、参数界限 $[(-10, 10), (-10, 10)]$ 以及其他所需数据作为参数。\n4.  优化器将返回估计的最优参数向量 $\\widehat{\\boldsymbol{\\theta}}$。\n5.  使用 $\\widehat{\\boldsymbol{\\theta}}$，计算最终的诊断值：优化后的加权矩 $\\widehat{M}_1(\\widehat{\\boldsymbol{\\theta}})$、$\\widehat{M}_2(\\widehat{\\boldsymbol{\\theta}})$ 和最小化的损失 $L(\\widehat{\\boldsymbol{\\theta}})$。\n\n此过程将应用于所提供的三个测试用例中的每一个。以下程序实现了这个推导出的算法。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves the propensity score calibration problem for three test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"X\": np.array([ -0.21, 0.05, 0.37, -0.62, 0.12, -0.33, -0.08, 0.41, 0.79, -0.45, 1.12, 0.58, -0.14, 0.27, -0.71, 0.93, 0.36, -0.50, 0.10, 0.02 ]),\n            \"m1\": 0.50, \"m2\": 1.25, \"alpha\": 0.10, \"beta\": 0.01\n        },\n        {\n            \"name\": \"Case B\",\n            \"X\": np.array([ -0.20, 0.30, 0.90, 0.60, 1.40, -0.10, 0.70, 0.20, 0.50, -0.30, 1.10, 0.40, 0.00, 1.60, -0.50, 0.80, -0.10, 0.30, 0.90, 0.40 ]),\n            \"m1\": 0.50, \"m2\": 1.25, \"alpha\": 0.01, \"beta\": 0.01\n        },\n        {\n            \"name\": \"Case C\",\n            \"X\": np.array([ 5.00, -4.00, 3.50, -3.20, 2.80, -2.50, 2.30, -2.10, 1.90, -1.70, 1.50, -1.30, 1.10, -0.90, 0.70, -0.50, 0.30, -0.20, 0.10, -0.10 ]),\n            \"m1\": 0.00, \"m2\": 2.00, \"alpha\": 1.00, \"beta\": 0.05\n        }\n    ]\n\n    # Universal parameters\n    epsilon = 1e-6\n    theta_initial = np.array([0.0, 0.0])\n    bounds = ((-10.0, 10.0), (-10.0, 10.0))\n\n    def calculate_diagnostics(theta, X):\n        \"\"\"Calculates propensities, weights, and weighted moments.\"\"\"\n        theta0, theta1 = theta\n        linear_term = theta0 + theta1 * X\n        \n        # Sigmoid function for propensities\n        propensities = 1 / (1 + np.exp(-linear_term))\n        \n        # Clip for numerical stability\n        propensities = np.clip(propensities, epsilon, 1 - epsilon)\n        \n        weights = 1 / propensities\n        \n        sum_w = np.sum(weights)\n        \n        # Handle potential division by zero, although unlikely\n        if sum_w == 0:\n            M1_hat = np.nan\n            M2_hat = np.nan\n        else:\n            M1_hat = np.sum(weights * X) / sum_w\n            M2_hat = np.sum(weights * X**2) / sum_w\n            \n        return propensities, weights, M1_hat, M2_hat\n\n    def loss_function(theta, X, m1, m2, alpha, beta):\n        \"\"\"Computes the total loss L(theta).\"\"\"\n        propensities, weights, M1_hat, M2_hat = calculate_diagnostics(theta, X)\n\n        # 1. Moment matching loss\n        moment_loss = (M1_hat - m1)**2 + (M2_hat - m2)**2\n\n        # 2. Squared CV penalty\n        mean_w = np.mean(weights)\n        var_w = np.var(weights)  # ddof=0 by default\n        cv_sq_penalty = alpha * var_w / mean_w**2 if mean_w > 1e-9 else 0.0\n\n        # 3. Barrier penalty\n        barrier_penalty = beta * np.sum(-np.log(propensities) - np.log(1 - propensities))\n\n        total_loss = moment_loss + cv_sq_penalty + barrier_penalty\n        return total_loss\n\n    all_results = []\n    for case in test_cases:\n        X, m1, m2, alpha, beta = case['X'], case['m1'], case['m2'], case['alpha'], case['beta']\n        \n        # Perform optimization\n        opt_result = minimize(\n            fun=loss_function,\n            x0=theta_initial,\n            args=(X, m1, m2, alpha, beta),\n            method='L-BFGS-B',\n            bounds=bounds\n        )\n        \n        theta_hat = opt_result.x\n        loss_hat = opt_result.fun\n        \n        # Recalculate final moments with optimal theta\n        _, _, M1_hat, M2_hat = calculate_diagnostics(theta_hat, X)\n        \n        # Append results in the specified order\n        all_results.extend([theta_hat[0], theta_hat[1], M1_hat, M2_hat, loss_hat])\n\n    # Format the final output string\n    # Using a format specifier for consistent output across floating point representations\n    formatted_results = [f\"{val:.8f}\" for val in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4932764"}, {"introduction": "除了直接对样本进行加权，另一种先进的策略是“建模和事后分层”。本练习将介绍多水平回归与事后分层（Multilevel Regression and Post-stratification, MRP），这是一种强大的模型为本的方法。你将学习如何构建一个分层模型来估计不同子群体（分层）中的结果概率，然后利用已知的总体分层大小来重新组合这些估计，从而得出一个稳健的、去偏倚的总体平均值 $\\hat{\\mu}_{\\text{MRP}}$ [@problem_id:4932631]。", "problem": "给定一个从有限总体中抽取的便利样本，该总体按分类协变量（统称为 $X$）进行了分层。结果变量 $Y$ 是二元的。由于该样本是便利样本，$X$ 的样本分布可能不等于其总体分布，这会导致对 $Y$ 的总体均值的朴素估计存在选择偏差。你的任务是构建一个多水平回归与后分层（MRP）估计量，该估计量将分层结果模型与已知的 $X$ 的外部总体边缘分布相结合。\n\n基本原理和建模假设：\n- 根据全期望定律，$Y$ 的总体均值满足 $\\mathbb{E}[Y] = \\sum_{s=1}^{S} \\mathbb{E}[Y \\mid X=s] \\Pr(X=s)$，其中 $s$ 是由 $X$ 定义的层的索引，$S$ 是总层数。\n- 在存在非概率（便利）抽样的情况下，$X$ 的样本分布不一定等于其总体分布。然而，如果我们能够对 $\\mathbb{E}[Y \\mid X=s]$ 进行建模，并且拥有关于 $\\Pr_{\\text{pop}}(X=s)$ 的外部信息，我们就可以通过将 $\\mathbb{E}[Y \\mid X=s]$ 的模型估计与总体边缘分布 $\\Pr_{\\text{pop}}(X=s)$ 相结合来估计 $\\mathbb{E}[Y]$。\n- 使用一个跨层部分池化的分层结果模型。具体来说，通过一个 logit 链接来参数化层级均值：对于层 $s$，令 $p_s = \\Pr(Y=1 \\mid X=s)$，并建立模型 $\\text{logit}(p_s) = \\alpha + b_s$，其中 $\\alpha$ 是全局截距，$b_s$ 是层特定随机效应。通过最大化一个惩罚对数似然来施加高斯收缩，这对应于 $b_s \\sim \\mathcal{N}(0, \\sigma^2)$，等价于一个调整参数为 $\\lambda = 1/\\sigma^2$ 的岭惩罚。\n\n使用聚合数据的惩罚似然：\n- 对于每个层 $s$，你观测到一个便利样本量 $n_s$ 和观测到的成功次数 $y_s$。令 $\\sigma(z) = 1/(1 + e^{-z})$ 表示 logistic 函数。参数 $(\\alpha, b_1, \\dots, b_S)$ 的惩罚对数似然为\n$$\n\\ell(\\alpha, b) = \\sum_{s=1}^{S} \\left[ y_s \\log\\left(\\sigma(\\alpha + b_s)\\right) + (n_s - y_s) \\log\\left(1 - \\sigma(\\alpha + b_s)\\right) \\right] - \\frac{\\lambda}{2} \\sum_{s=1}^{S} b_s^2.\n$$\n你必须对整个参数向量使用牛顿-拉夫逊方法来找到最大化器 $(\\hat{\\alpha}, \\hat{b})$，然后计算 $\\hat{p}_s = \\sigma(\\hat{\\alpha} + \\hat{b}_s)$。\n\n后分层：\n- 给定外部总体边缘分布 $w_s = \\Pr_{\\text{pop}}(X=s)$ 且满足 $\\sum_{s=1}^{S} w_s = 1$，将 $Y$ 的总体均值的 MRP 估计量计算为\n$$\n\\hat{\\mu}_{\\text{MRP}} = \\sum_{s=1}^{S} w_s \\hat{p}_s.\n$$\n\n牛顿-拉夫逊算法规范：\n- 定义参数向量 $\\theta = (\\alpha, b_1, \\dots, b_S)^\\top$。\n- 在当前迭代步，为所有 $s$ 计算 $p_s = \\sigma(\\alpha + b_s)$。\n- 梯度分量为\n$$\n\\frac{\\partial \\ell}{\\partial \\alpha} = \\sum_{s=1}^{S} (y_s - n_s p_s), \\quad\n\\frac{\\partial \\ell}{\\partial b_s} = (y_s - n_s p_s) - \\lambda b_s.\n$$\n- Hessian 矩阵的元素为\n$$\n\\frac{\\partial^2 \\ell}{\\partial \\alpha^2} = -\\sum_{s=1}^{S} n_s p_s (1 - p_s), \\quad\n\\frac{\\partial^2 \\ell}{\\partial \\alpha \\, \\partial b_s} = - n_s p_s (1 - p_s), \\quad\n\\frac{\\partial^2 \\ell}{\\partial b_s^2} = - n_s p_s (1 - p_s) - \\lambda,\n$$\n其中当 $s \\neq s'$ 时，$\\frac{\\partial^2 \\ell}{\\partial b_s \\, \\partial b_{s'}} = 0$。\n- 求解线性系统 $H(\\theta) \\, \\Delta = - \\nabla \\ell(\\theta)$ 并更新 $\\theta \\leftarrow \\theta + \\Delta$ 直至收敛。\n\n测试套件和要求输出：\n- 每种情况下都有 $S = 6$ 个层。对于每个测试用例，你将获得 $(n_s)$、$(y_s)$、$(w_s)$、一个仅用于评估的真实层概率向量 $(p^{\\star}_s)$，以及一个惩罚参数 $\\lambda$。使用提供的 $(n_s)$ 和 $(y_s)$ 来拟合分层模型，使用 $(w_s)$ 进行后分层，并计算 $\\hat{\\mu}_{\\text{MRP}}$。\n\n- 测试用例 1（正常路径，中度偏差）：\n    - $n = [20, 15, 15, 25, 15, 60]$\n    - $y = [2, 2, 2, 8, 5, 24]$\n    - $w = [0.2, 0.15, 0.25, 0.15, 0.1, 0.15]$\n    - $p^{\\star} = [0.1, 0.12, 0.15, 0.3, 0.35, 0.4]$\n    - $\\lambda = 1.0$\n- 测试用例 2（边界情况，样本中存在缺失层）：\n    - $n = [10, 5, 40, 0, 10, 5]$\n    - $y = [2, 1, 20, 0, 1, 1]$\n    - $w = [0.1, 0.25, 0.1, 0.2, 0.2, 0.15]$\n    - $p^{\\star} = [0.18, 0.22, 0.5, 0.05, 0.1, 0.12]$\n    - $\\lambda = 2.5$\n- 测试用例 3（严重不平衡，对低风险层过采样）：\n    - $n = [200, 150, 10, 5, 2, 1]$\n    - $y = [10, 12, 1, 1, 1, 1]$\n    - $w = [0.15, 0.15, 0.2, 0.2, 0.15, 0.15]$\n    - $p^{\\star} = [0.05, 0.08, 0.12, 0.25, 0.5, 0.55]$\n    - $\\lambda = 0.5$\n\n为每个测试用例计算 MRP 估计量 $\\hat{\\mu}_{\\text{MRP}}$，并将它们作为最终程序输出。所有输出都是无单位的浮点数。你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[r_1, r_2, r_3]$），其中 $r_i$ 是按给定顺序排列的测试用例 $i$ 的 $\\hat{\\mu}_{\\text{MRP}}$。不应打印任何其他文本。", "solution": "该问题要求构建并应用多水平回归与后分层（MRP）估计量，以校正便利样本中的选择偏差。解决方案涉及使用牛顿-拉夫逊方法最大化惩罚对数似然，然后对基于模型的预测进行重新加权，以匹配已知的人口统计数据。\n\n首先，我们将问题形式化。我们拥有一个便利样本的数据，该样本被分入 $S$ 个不同的层。对于每个层 $s \\in \\{1, \\dots, S\\}$，我们有样本量 $n_s$ 和二元结果 $Y$ 的“成功”次数 $y_s$。我们还获得了每个层在总体水平上的比例 $w_s$。目标是估计总体均值 $\\mu = \\mathbb{E}[Y]$。\n\nMRP 方法的核心在于全期望定律：\n$$\n\\mu = \\mathbb{E}[Y] = \\sum_{s=1}^{S} \\mathbb{E}[Y \\mid X=s] \\Pr_{\\text{pop}}(X=s) = \\sum_{s=1}^{S} p_s w_s\n$$\n其中 $p_s = \\Pr(Y=1 \\mid X=s)$ 是层 $s$ 中的真实成功概率，$w_s$ 是该层的真实总体比例。虽然样本分布可能存在偏差，但如果我们能获得 $p_s$ 的可靠估计，并知道真实的 $w_s$，我们就可以估计 $\\mu$。\n\n我们使用分层 logistic 回归模型对层特定概率 $p_s$ 进行建模。这允许跨层进行信息的部分池化，对于样本量小的层尤其有用。模型规定如下：\n$$\n\\text{logit}(p_s) = \\log\\left(\\frac{p_s}{1-p_s}\\right) = \\alpha + b_s\n$$\n此处，$\\alpha$ 是一个全局截距，表示所有层的平均成功对数几率；$b_s$ 是一个层特定随机效应，捕捉层 $s$ 与该平均值的偏差。模型假设这些随机效应来自一个共同分布，$b_s \\sim \\mathcal{N}(0, \\sigma^2)$。这一假设引入了收缩效应，将 $b_s$ 的估计值拉向 $0$，这有助于正则化模型并防止过拟合，尤其是在数据稀疏的层中。\n\n参数 $(\\alpha, b_1, \\dots, b_S)$ 的估计是通过最大化惩罚对数似然来进行的。所有层的观测数据 $(y_s, n_s)$ 的似然由二项式似然的乘积给出。惩罚项源于随机效应的先验分布 $b_s \\sim \\mathcal{N}(0, \\sigma^2)$，这等价于对系数 $b_s$ 施加 $L_2$（岭）惩罚。令 $\\lambda = 1/\\sigma^2$，需要最大化的惩罚对数似然函数为：\n$$\n\\ell(\\alpha, b) = \\sum_{s=1}^{S} \\left[ y_s \\log\\left(\\sigma(\\alpha + b_s)\\right) + (n_s - y_s) \\log\\left(1 - \\sigma(\\alpha + b_s)\\right) \\right] - \\frac{\\lambda}{2} \\sum_{s=1}^{S} b_s^2\n$$\n其中 $\\sigma(z) = 1/(1 + e^{-z})$ 是 logistic 函数。$n_s=0$ 的层对数据似然的总和没有贡献，但其对应的 $b_s$ 仍受惩罚项影响，这会有效地将 $\\hat{b}_s$ 收缩至 $0$。\n\n为了找到最大化该函数的参数 $(\\hat{\\alpha}, \\hat{b})$，我们采用牛顿-拉夫逊算法，这是一种迭代的二阶优化方法。设完整参数向量为 $\\theta = (\\alpha, b_1, \\dots, b_S)^\\top$。迭代更新规则为：\n$$\n\\theta_{k+1} = \\theta_k - [H(\\theta_k)]^{-1} \\nabla \\ell(\\theta_k)\n$$\n其中 $\\nabla \\ell(\\theta_k)$ 是梯度向量，$H(\\theta_k)$ 是惩罚对数似然的 Hessian 矩阵，两者都在当前参数估计 $\\theta_k$ 处求值。\n\n梯度向量 $\\nabla \\ell(\\theta)$ 的分量为：\n$$\n\\frac{\\partial \\ell}{\\partial \\alpha} = \\sum_{s=1}^{S} (y_s - n_s p_s)\n$$\n$$\n\\frac{\\partial \\ell}{\\partial b_s} = (y_s - n_s p_s) - \\lambda b_s \\quad \\text{对于 } s = 1, \\dots, S\n$$\n其中 $p_s = \\sigma(\\alpha + b_s)$。\n\nHessian 矩阵 $H(\\theta)$ 是一个对称的 $(S+1) \\times (S+1)$ 矩阵。其元素由 $\\ell$ 的二阶偏导数给出。令 $W_s = n_s p_s(1-p_s)$。其元素为：\n$$\n\\frac{\\partial^2 \\ell}{\\partial \\alpha^2} = -\\sum_{s=1}^{S} W_s\n$$\n$$\n\\frac{\\partial^2 \\ell}{\\partial \\alpha \\, \\partial b_s} = -W_s\n$$\n$$\n\\frac{\\partial^2 \\ell}{\\partial b_s^2} = -W_s - \\lambda\n$$\n$$\n\\frac{\\partial^2 \\ell}{\\partial b_s \\, \\partial b_{s'}} = 0 \\quad \\text{对于 } s \\neq s'\n$$\nHessian 矩阵具有带边对角结构。牛顿-拉夫逊更新步长 $\\Delta_k = -[H(\\theta_k)]^{-1} \\nabla \\ell(\\theta_k)$ 是通过求解线性系统 $H(\\theta_k) \\Delta_k = -\\nabla \\ell(\\theta_k)$ 来计算的。\n\n该算法按以下步骤进行：\n1. 初始化参数，例如 $\\theta_0 = \\mathbf{0}$。\n2. 对 $k=0, 1, 2, \\dots$ 进行迭代，直至收敛：\n   a. 对于当前 $\\theta_k = (\\alpha_k, b_{1,k}, \\dots, b_{S,k})^\\top$，为所有层计算 $p_s = \\sigma(\\alpha_k + b_{s,k})$。\n   b. 计算梯度向量 $\\nabla \\ell(\\theta_k)$ 和 Hessian 矩阵 $H(\\theta_k)$。\n   c. 求解线性系统 $H(\\theta_k) \\Delta_k = -\\nabla \\ell(\\theta_k)$ 以获得更新步长 $\\Delta_k$。\n   d. 更新参数：$\\theta_{k+1} = \\theta_k + \\Delta_k$。\n3. 当更新向量的范数 $\\|\\Delta_k\\|$ 小于一个很小的容差时，即达到收敛。\n\n一旦算法收敛到最优参数 $(\\hat{\\alpha}, \\hat{b})$，我们计算估计的层概率：\n$$\n\\hat{p}_s = \\sigma(\\hat{\\alpha} + \\hat{b}_s) \\quad \\text{对于 } s = 1, \\dots, S\n$$\n对于 $n_s=0$ 的层 $s$，其估计值 $\\hat{b}_s$ 会被惩罚项驱动至 $0$，因此其预测概率将为 $\\hat{p}_s \\approx \\sigma(\\hat{\\alpha})$，这是从所有其他层中借鉴了信息。\n\n最后，后分层步骤将这些基于模型的概率估计与已知的总体权重 $w_s$ 相结合，以计算总体均值的 MRP 估计量：\n$$\n\\hat{\\mu}_{\\text{MRP}} = \\sum_{s=1}^{S} w_s \\hat{p}_s\n$$\n这个最终值就是对 $Y$ 总体均值的去偏估计。", "answer": "```python\nimport numpy as np\n\ndef _compute_mrp_estimate(n, y, w, lambda_val, tol=1e-8, max_iter=25):\n    \"\"\"\n    Computes the MRP estimate by maximizing a penalized logistic regression model.\n\n    Args:\n        n (list): List of sample sizes per stratum.\n        y (list): List of success counts per stratum.\n        w (list): List of population proportions per stratum.\n        lambda_val (float): The ridge penalty parameter.\n        tol (float): Convergence tolerance for the Newton-Raphson algorithm.\n        max_iter (int): Maximum number of iterations for Newton-Raphson.\n\n    Returns:\n        float: The Multilevel Regression and Post-stratification (MRP) estimate.\n    \"\"\"\n    n_arr = np.array(n, dtype=float)\n    y_arr = np.array(y, dtype=float)\n    w_arr = np.array(w, dtype=float)\n    S = len(n_arr)\n\n    # Initialize parameters theta = (alpha, b_1, ..., b_S)\n    theta = np.zeros(S + 1)\n\n    for i in range(max_iter):\n        alpha = theta[0]\n        b = theta[1:]\n\n        # Linear predictor eta = alpha + b_s for each stratum s\n        eta = alpha + b\n        \n        # Probabilities p_s = sigma(eta_s)\n        # Clip eta to prevent overflow in exp\n        eta = np.clip(eta, -500, 500)\n        p = 1.0 / (1.0 + np.exp(-eta))\n\n        # Gradient of the penalized log-likelihood\n        grad = np.zeros(S + 1)\n        # Residuals for the likelihood part\n        residual = y_arr - n_arr * p\n        # d(ell)/d(alpha)\n        grad[0] = np.sum(residual)\n        # d(ell)/d(b_s)\n        grad[1:] = residual - lambda_val * b\n\n        # Hessian matrix of the penalized log-likelihood\n        H = np.zeros((S + 1, S + 1))\n        # Weights for the Hessian W_s = n_s * p_s * (1 - p_s)\n        W = n_arr * p * (1.0 - p)\n        \n        # Fill the Hessian matrix based on its structure\n        # H_00 = d^2(ell)/d(alpha^2)\n        H[0, 0] = -np.sum(W)\n        # H_0s = H_s0 = d^2(ell)/d(alpha)d(b_s)\n        H[0, 1:] = -W\n        H[1:, 0] = -W\n        \n        # Diagonal elements for the b_s block: H_ss = d^2(ell)/d(b_s^2)\n        # This uses array broadcasting in a slightly non-obvious way. A direct\n        # assignment to the diagonal of a submatrix is safer.\n        H_sub = H[1:, 1:]\n        np.fill_diagonal(H_sub, -W - lambda_val)\n\n        # Newton-Raphson step: solve H * delta = -grad\n        try:\n            delta = np.linalg.solve(H, -grad)\n        except np.linalg.LinAlgError:\n            # Fallback to pseudo-inverse if Hessian is singular, though unlikely with lambda > 0\n            # For this problem, we assume H is always invertible.\n            # Returning NaN on failure.\n            return np.nan\n        \n        # Update parameters\n        theta += delta\n\n        # Check for convergence\n        if np.linalg.norm(delta)  tol:\n            break\n\n    # After convergence, compute final estimates\n    final_alpha = theta[0]\n    final_b = theta[1:]\n\n    # Final stratum probabilities\n    final_eta = final_alpha + final_b\n    final_eta = np.clip(final_eta, -500, 500)\n    final_p = 1.0 / (1.0 + np.exp(-final_eta))\n\n    # Post-stratification to get the MRP estimate of the population mean\n    mu_mrp = np.sum(w_arr * final_p)\n    return mu_mrp\n\ndef solve():\n    \"\"\"\n    Main function to run the MRP estimation for the given test cases.\n    \"\"\"\n    # Test cases from the problem statement\n    test_cases = [\n        {\n            \"n\": [20, 15, 15, 25, 15, 60],\n            \"y\": [2, 2, 2, 8, 5, 24],\n            \"w\": [0.2, 0.15, 0.25, 0.15, 0.1, 0.15],\n            \"lambda_val\": 1.0\n        },\n        {\n            \"n\": [10, 5, 40, 0, 10, 5],\n            \"y\": [2, 1, 20, 0, 1, 1],\n            \"w\": [0.1, 0.25, 0.1, 0.2, 0.2, 0.15],\n            \"lambda_val\": 2.5\n        },\n        {\n            \"n\": [200, 150, 10, 5, 2, 1],\n            \"y\": [10, 12, 1, 1, 1, 1],\n            \"w\": [0.15, 0.15, 0.2, 0.2, 0.15, 0.15],\n            \"lambda_val\": 0.5\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = _compute_mrp_estimate(\n            n=case[\"n\"],\n            y=case[\"y\"],\n            w=case[\"w\"],\n            lambda_val=case[\"lambda_val\"]\n        )\n        results.append(result)\n\n    # Format the output as a comma-separated list in brackets.\n    # We use a reasonable precision for floating point results.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```", "id": "4932631"}]}