{"hands_on_practices": [{"introduction": "第一个练习将带领我们回归基础，从伯努利分布开始，它是建模成功/失败等二元结果的基石。你将从零开始，为概率参数 $p$ 推导最大似然估计（MLE）。这项练习不仅对于掌握推导过程至关重要，更有助于理解一些理论上的细微之处，例如最大似然估计的存在性和唯一性如何依赖于观测数据和所选的参数空间 [@problem_id:4969170]。", "problem": "在一项关于二元临床结果的多中心观察性研究中，每位患者在固定的随访期内要么经历特定的不良事件，要么不经历。设 $X_{1},\\dots,X_{n}$ 表示独立同分布（i.i.d.）的伯努利随机变量，其未知事件概率为 $p$，其中 $X_{i}=1$ 表示患者 $i$ 发生了不良事件，$X_{i}=0$ 表示未发生。$p$ 的参数空间是闭区间 $[0,1]$，这是概率的自然空间，但您也应该分析当参数空间取为开区间 $(0,1)$ 时会发生什么。\n\n仅使用伯努利概率质量函数的定义和独立同分布数据的似然函数定义，从第一性原理推导出 $p$ 的最大似然估计量（MLE）$\\hat{p}$。然后，在参数空间为 $(0,1)$ 和 $[0,1]$ 这两种选择下，仔细分析在所有观测值为 $0$（即对所有 $i$，$X_{i}=0$）或所有观测值为 $1$（即对所有 $i$，$X_{i}=1$）的非正则边缘情况下，最大似然估计量的存在性和唯一性。您的分析应通过引用似然函数或对数似然函数的适当性质，来论证在指定的参数空间中是否存在最大化点，以及在每种边缘情况下它是否唯一。\n\n以单个闭式表达式的形式提供最大似然估计量 $\\hat{p}$ 的最终答案。不需要进行数值舍入。", "solution": "该问题要求推导伯努利分布参数 $p$ 的最大似然估计量（MLE），并仔细分析在两种不同的参数空间 $[0, 1]$ 和 $(0, 1)$ 下，该估计量在边缘情况下的存在性和唯一性。\n\n设 $X_{1}, \\dots, X_{n}$ 为来自参数为 $p$ 的伯努利分布的独立同分布（i.i.d.）随机变量，其中 $p$ 是事件发生的概率（记为 $X_i=1$）。单个观测值 $X_i$ 的概率质量函数（PMF）由下式给出：\n$$ P(X_i = x_i; p) = p^{x_i} (1-p)^{1-x_i} \\quad \\text{for } x_i \\in \\{0, 1\\} $$\n参数 $p$ 代表一个概率，因此其自然参数空间是闭区间 $[0, 1]$。\n\n首先，我们构造似然函数 $L(p)$。由于观测值是独立同分布的，似然函数是各个概率质量函数的乘积：\n$$ L(p; x_1, \\dots, x_n) = \\prod_{i=1}^{n} P(X_i = x_i; p) = \\prod_{i=1}^{n} p^{x_i} (1-p)^{1-x_i} $$\n通过合并指数可以将其简化：\n$$ L(p) = p^{\\sum_{i=1}^{n} x_i} (1-p)^{\\sum_{i=1}^{n} (1-x_i)} = p^{\\sum x_i} (1-p)^{n - \\sum x_i} $$\n设 $S = \\sum_{i=1}^{n} X_i$ 为观测到的事件总数。$S$ 是 $p$ 的一个充分统计量。似然函数可以写成：\n$$ L(p) = p^{S} (1-p)^{n-S} $$\n为了找到使 $L(p)$ 最大化的 $p$ 值，计算上更方便的是最大化似然函数的自然对数，即对数似然函数，记为 $\\ell(p)$。由于自然对数是严格递增函数，使 $\\ell(p)$ 最大化的 $p$ 值也使 $L(p)$ 最大化。\n$$ \\ell(p) = \\ln(L(p)) = \\ln(p^{S} (1-p)^{n-S}) = S \\ln(p) + (n-S) \\ln(1-p) $$\n对数似然函数在 $p \\in (0, 1)$ 上有定义。我们将首先找到解存在于此开区间内的情况下的 MLE，然后分析边界点 $p=0$ 和 $p=1$。\n\n我们通过对 $\\ell(p)$ 关于 $p$ 求导并将导数设为零来找到最大值。\n$$ \\frac{d\\ell}{dp} = \\frac{S}{p} - \\frac{n-S}{1-p} $$\n将导数设为 $0$：\n$$ \\frac{S}{p} = \\frac{n-S}{1-p} $$\n$$ S(1-p) = p(n-S) $$\n$$ S - Sp = np - Sp $$\n$$ S = np $$\n$$ p = \\frac{S}{n} $$\n这个临界点 $\\hat{p} = \\frac{S}{n}$ 就是样本均值 $\\bar{X}$。为验证这是一个最大值，我们检查二阶导数：\n$$ \\frac{d^2\\ell}{dp^2} = -\\frac{S}{p^2} - \\frac{n-S}{(1-p)^2} $$\n对于非边缘情况，我们有 $0  S  n$。这意味着 $S  0$ 且 $n-S  0$。由于 $p^2  0$ 和 $(1-p)^2  0$，二阶导数中的两项都是负的。因此，对于所有 $p \\in (0, 1)$，都有 $\\frac{d^2\\ell}{dp^2}  0$，这证实了对数似然函数是严格凹的。因此，临界点 $\\hat{p} = \\frac{S}{n}$ 是唯一的最大值点。在这种情况下，由于 $0  S  n$，我们有 $0  \\hat{p}  1$，所以 MLE 在参数空间 $(0, 1)$ 和 $[0, 1]$ 中都存在且唯一。\n\n现在，我们必须按要求分析边缘情况。\n情况 1：所有观测值均为 $0$。\n这对应于 $S = \\sum_{i=1}^{n} X_i = 0$。似然函数变为：\n$$ L(p) = p^0 (1-p)^{n-0} = (1-p)^n $$\n对于 $p \\in (0, 1)$，对数似然函数为 $\\ell(p) = n \\ln(1-p)$。\n在参数空间 $[0, 1]$ 上的分析：\n函数 $L(p) = (1-p)^n$ 在区间 $[0, 1]$ 上是关于 $p$ 的严格递减函数。在闭区间上的严格递减函数在其区间的左端点达到其唯一的最大值。因此，似然函数在 $p=0$ 时最大化。MLE 是 $\\hat{p} = 0$。这个值在参数空间 $[0, 1]$ 内。似然函数的最大值为 $L(0) = (1-0)^n = 1$。所以，对于空间 $[0, 1]$，存在唯一的 MLE，即 $\\hat{p}=0$。这与通用公式 $\\hat{p} = S/n = 0/n = 0$ 一致。\n\n在参数空间 $(0, 1)$ 上的分析：\n在开区间 $(0, 1)$ 上，函数 $L(p) = (1-p)^n$ 仍然是严格递减的。函数的上确界是 $\\sup_{p \\in (0, 1)} (1-p)^n = 1$，当 $p \\to 0^+$ 时趋近于此值。然而，对于开区间 $(0, 1)$ 内的任何 $p$，这个上确界都永远无法达到。对于任何一个提议的最大化点 $p^* \\in (0, 1)$，我们总能找到一个值 $p^{**} = p^*/2$，使得 $0  p^{**}  p^*$ 且 $L(p^{**}) = (1-p^*/2)^n  (1-p^*)^n = L(p^*)$。因此，在参数空间 $(0, 1)$ 内不存在最大值。MLE 不存在。\n\n情况 2：所有观测值均为 $1$。\n这对应于 $S = \\sum_{i=1}^{n} X_i = n$。似然函数变为：\n$$ L(p) = p^n (1-p)^{n-n} = p^n $$\n对于 $p \\in (0, 1)$，对数似然函数为 $\\ell(p) = n \\ln(p)$。\n在参数空间 $[0, 1]$ 上的分析：\n函数 $L(p) = p^n$ 在区间 $[0, 1]$ 上是关于 $p$ 的严格递增函数。在闭区间上的严格递增函数在其区间的右端点达到其唯一的最大值。因此，似然函数在 $p=1$ 时最大化。MLE 是 $\\hat{p} = 1$。这个值在参数空间 $[0, 1]$ 内。似然函数的最大值为 $L(1) = 1^n = 1$。所以，对于空间 $[0, 1]$，存在唯一的 MLE，即 $\\hat{p}=1$。这与通用公式 $\\hat{p} = S/n = n/n = 1$ 一致。\n\n在参数空间 $(0, 1)$ 上的分析：\n在开区间 $(0, 1)$ 上，函数 $L(p) = p^n$ 仍然是严格递增的。函数的上确界是 $\\sup_{p \\in (0, 1)} p^n = 1$，当 $p \\to 1^-$ 时趋近于此值。然而，对于开区间 $(0, 1)$ 内的任何 $p$，这个上确界都永远无法达到。对于任何一个提议的最大化点 $p^* \\in (0, 1)$，我们总能找到一个值 $p^{**} = (p^*+1)/2$，使得 $p^*  p^{**}  1$ 且 $L(p^{**}) = ((p^*+1)/2)^n  (p^*)^n = L(p^*)$。因此，在参数空间 $(0, 1)$ 内不存在最大值。MLE 不存在。\n\n总之，当且仅当参数空间是闭区间 $[0, 1]$ 时，$p$ 的 MLE 对于所有可能的观测集 $\\{X_1, \\dots, X_n\\}$ 都是良定义的、存在的且唯一的。这是伯努利参数的自然且标准的选择。通用表达式 $\\hat{p} = \\frac{\\sum_{i=1}^{n} X_i}{n}$ 在此参数空间选择下涵盖了所有三种情况（内部、全为 $0$、全为 $1$）。", "answer": "$$\n\\boxed{\\frac{\\sum_{i=1}^{n} X_{i}}{n}}\n$$", "id": "4969170"}, {"introduction": "从离散数据转向连续数据，这个练习将处理作为统计建模基石的正态分布。在这里，你将为均值 $\\mu$ 和方差 $\\sigma^2$ 推导联合最大似然估计。更重要的是，本练习将引导你计算方差的最大似然估计量的偏差，从而引入一个关键概念：最大似然估计量虽然具有理想的渐近性质，但在有限样本中并不保证是无偏的 [@problem_id:4969243]。", "problem": "一个生物医学实验室校准一种新的测定法，该测定法报告每个患者样本的连续生物标志物值。假设患者间的测量误差被建模为来自未知均值偏差 $\\mu$ 和未知方差 $\\sigma^{2}$ 的正态分布 $N(\\mu,\\sigma^{2})$ 的独立同分布 (i.i.d.) 抽样。您观察到一次校准运行，其值为 $x_{1},\\dots,x_{n}$，其中 $n \\geq 2$。从 i.i.d. 正态观测值的似然定义以及期望和方差的基本性质出发，推导出最大似然估计量 (Maximum Likelihood Estimators (MLE)) $\\hat{\\mu}$ 和 $\\hat{\\sigma}^{2}$，并将每个估计量用 $x_{1},\\dots,x_{n}$ 表示。然后，相对于通常的无偏方差估计量 $s^{2}_{\\text{unb}}=\\frac{1}{n-1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}$，确定 MLE $\\hat{\\sigma}^{2}$ 的确切偏差，定义为 $\\operatorname{Bias}(\\hat{\\sigma}^{2})=\\mathbb{E}[\\hat{\\sigma}^{2}]-\\sigma^{2}$。请以 $n$ 和 $\\sigma^{2}$ 的单个封闭形式表达式提供偏差的最终答案。无需数值四舍五入。这里 $\\bar{x}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}$ 表示样本均值。", "solution": "求解过程分为两个主要部分：首先，推导正态分布均值 $\\mu$ 和方差 $\\sigma^2$ 的最大似然估计量 (MLE)；其次，计算方差的 MLE $\\hat{\\sigma}^2$ 的偏差。\n\n设 $x_1, \\dots, x_n$ 是来自正态分布 $N(\\mu, \\sigma^2)$ 的一个独立同分布 (i.i.d.) 样本。单个观测值 $x_i$ 的概率密度函数 (PDF) 由下式给出：\n$$f(x_i | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x_i - \\mu)^2}{2\\sigma^2} \\right)$$\n\n似然函数 $L(\\mu, \\sigma^2 | \\mathbf{x})$ 是 $n$ 个观测值各自 PDF 的乘积，这是因为它们的独立性：\n$$L(\\mu, \\sigma^2 | \\mathbf{x}) = \\prod_{i=1}^{n} f(x_i | \\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x_i - \\mu)^2}{2\\sigma^2} \\right)$$\n$$L(\\mu, \\sigma^2 | \\mathbf{x}) = \\left(2\\pi\\sigma^2\\right)^{-n/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2 \\right)$$\n\n为了简化最大化过程，我们处理似然函数的自然对数，即对数似然函数 $\\mathcal{L}(\\mu, \\sigma^2 | \\mathbf{x})$：\n$$\\mathcal{L} = \\ln[L(\\mu, \\sigma^2 | \\mathbf{x})] = \\ln\\left[ \\left(2\\pi\\sigma^2\\right)^{-n/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2 \\right) \\right]$$\n$$\\mathcal{L} = -\\frac{n}{2} \\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2$$\n$$\\mathcal{L} = -\\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2$$\n\n为了求得 MLE，我们将 $\\mathcal{L}$ 分别对 $\\mu$ 和 $\\sigma^2$ 求偏导数，并令它们等于零。\n\n首先，对于均值 $\\mu$：\n$$\\frac{\\partial \\mathcal{L}}{\\partial \\mu} = \\frac{\\partial}{\\partial \\mu} \\left[ -\\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2 \\right]$$\n$$\\frac{\\partial \\mathcal{L}}{\\partial \\mu} = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} 2(x_i - \\mu)(-1) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)$$\n将导数设为零并求解 $\\hat{\\mu}$：\n$$\\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (x_i - \\hat{\\mu}) = 0 \\implies \\sum_{i=1}^{n} x_i - \\sum_{i=1}^{n} \\hat{\\mu} = 0$$\n$$\\sum_{i=1}^{n} x_i - n\\hat{\\mu} = 0 \\implies \\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} x_i = \\bar{x}$$\n$\\mu$ 的 MLE 是样本均值，$\\hat{\\mu} = \\bar{x}$。\n\n接下来，对于方差 $\\sigma^2$。直接对 $\\sigma^2$ 求导会更方便。\n$$\\frac{\\partial \\mathcal{L}}{\\partial (\\sigma^2)} = \\frac{\\partial}{\\partial (\\sigma^2)} \\left[ -\\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2 \\right]$$\n$$\\frac{\\partial \\mathcal{L}}{\\partial (\\sigma^2)} = -\\frac{n}{2\\sigma^2} - \\left(-\\frac{1}{2(\\sigma^2)^2}\\right) \\sum_{i=1}^{n} (x_i - \\mu)^2 = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\sum_{i=1}^{n} (x_i - \\mu)^2$$\n将导数设为零，并代入估计量 $\\hat{\\mu}$ 和 $\\hat{\\sigma}^2$：\n$$-\\frac{n}{2\\hat{\\sigma}^2} + \\frac{1}{2(\\hat{\\sigma}^2)^2} \\sum_{i=1}^{n} (x_i - \\hat{\\mu})^2 = 0$$\n两边乘以 $2(\\hat{\\sigma}^2)^2$ 得：\n$$-n\\hat{\\sigma}^2 + \\sum_{i=1}^{n} (x_i - \\hat{\\mu})^2 = 0$$\n代入 $\\hat{\\mu} = \\bar{x}$，我们解出 $\\hat{\\sigma}^2$：\n$$\\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2$$\n\n现在，我们确定该估计量的偏差。偏差定义为 $\\operatorname{Bias}(\\hat{\\sigma}^2) = \\mathbb{E}[\\hat{\\sigma}^2] - \\sigma^2$。我们必须计算 $\\hat{\\sigma}^2$ 的期望。\n$$\\mathbb{E}[\\hat{\\sigma}^2] = \\mathbb{E}\\left[\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\right] = \\frac{1}{n} \\mathbb{E}\\left[\\sum_{i=1}^{n} (x_i - \\bar{x})^2\\right]$$\n我们展开平方和项：\n$$\\sum_{i=1}^{n} (x_i - \\bar{x})^2 = \\sum_{i=1}^{n} (x_i^2 - 2x_i\\bar{x} + \\bar{x}^2) = \\sum_{i=1}^{n} x_i^2 - 2\\bar{x}\\sum_{i=1}^{n} x_i + n\\bar{x}^2$$\n因为 $\\sum_{i=1}^{n} x_i = n\\bar{x}$，上式简化为：\n$$\\sum_{i=1}^{n} x_i^2 - 2\\bar{x}(n\\bar{x}) + n\\bar{x}^2 = \\sum_{i=1}^{n} x_i^2 - n\\bar{x}^2$$\n现在我们对这个表达式求期望：\n$$\\mathbb{E}\\left[\\sum_{i=1}^{n} (x_i - \\bar{x})^2\\right] = \\mathbb{E}\\left[\\sum_{i=1}^{n} x_i^2 - n\\bar{x}^2\\right] = \\sum_{i=1}^{n} \\mathbb{E}[x_i^2] - n\\mathbb{E}[\\bar{x}^2]$$\n我们使用通用性质 $\\mathbb{E}[Y^2] = \\operatorname{Var}(Y) + (\\mathbb{E}[Y])^2$。\n对于每个 $x_i$，有 $\\mathbb{E}[x_i] = \\mu$ 和 $\\operatorname{Var}(x_i) = \\sigma^2$，所以 $\\mathbb{E}[x_i^2] = \\sigma^2 + \\mu^2$。\n对于样本均值 $\\bar{x}$，有 $\\mathbb{E}[\\bar{x}] = \\mathbb{E}[\\frac{1}{n}\\sum x_i] = \\mu$ 和 $\\operatorname{Var}(\\bar{x}) = \\operatorname{Var}(\\frac{1}{n}\\sum x_i) = \\frac{1}{n^2}\\sum\\operatorname{Var}(x_i) = \\frac{n\\sigma^2}{n^2} = \\frac{\\sigma^2}{n}$。\n因此，$\\mathbb{E}[\\bar{x}^2] = \\operatorname{Var}(\\bar{x}) + (\\mathbb{E}[\\bar{x}])^2 = \\frac{\\sigma^2}{n} + \\mu^2$。\n将这些期望代回：\n$$\\mathbb{E}\\left[\\sum_{i=1}^{n} (x_i - \\bar{x})^2\\right] = \\sum_{i=1}^{n} (\\sigma^2 + \\mu^2) - n\\left(\\frac{\\sigma^2}{n} + \\mu^2\\right)$$\n$$= n(\\sigma^2 + \\mu^2) - (\\sigma^2 + n\\mu^2) = n\\sigma^2 + n\\mu^2 - \\sigma^2 - n\\mu^2 = (n-1)\\sigma^2$$\n现在我们求 $\\hat{\\sigma}^2$ 的期望：\n$$\\mathbb{E}[\\hat{\\sigma}^2] = \\frac{1}{n} \\mathbb{E}\\left[\\sum_{i=1}^{n} (x_i - \\bar{x})^2\\right] = \\frac{1}{n}(n-1)\\sigma^2$$\n最后，我们计算偏差：\n$$\\operatorname{Bias}(\\hat{\\sigma}^2) = \\mathbb{E}[\\hat{\\sigma}^2] - \\sigma^2 = \\frac{n-1}{n}\\sigma^2 - \\sigma^2 = \\left(\\frac{n-1}{n} - 1\\right)\\sigma^2 = \\left(\\frac{n-1-n}{n}\\right)\\sigma^2 = -\\frac{1}{n}\\sigma^2$$\n方差的 MLE 的偏差是 $-\\frac{1}{n}\\sigma^2$。这表明 MLE 是一个有偏估计量，它倾向于低估真实方差。随着样本量 $n$ 的增加，偏差会减小。", "answer": "$$ \\boxed{-\\frac{1}{n}\\sigma^2} $$", "id": "4969243"}, {"introduction": "尽管前面的例子得出了简单的闭式估计量，但大多数现实世界中的模型（如逻辑斯蒂回归）需要数值方法来求解最大似然估计。本练习通过引入牛顿-拉弗森（Newton-Raphson）算法，连接了理论与实践，该算法是许多统计软件包背后的核心引擎。你将首先推导其通用的更新规则，然后在一个具体的数据集上应用单步迭代，从而揭开复杂模型实际拟合过程的神秘面纱 [@problem_id:4969348]。", "problem": "一项临床研究调查术后并发症的概率与标准化生物标志物之间的函数关系。设 $y_i \\in \\{0,1\\}$ 表示患者 $i$ 是否出现并发症，设 $x_i \\in \\mathbb{R}$ 表示生物标志物的值。假设一个参数为向量 $\\theta \\in \\mathbb{R}^{p}$ 的参数模型通过最大似然估计（MLE）原理进行拟合，该原理选择 $\\theta$ 以最大化观测数据的对数似然 $l(\\theta)$。从得分函数 $U(\\theta)$（定义为 $l(\\theta)$ 的梯度）和 $U(\\theta)$ 的雅可比矩阵 $J(\\theta)$ 的定义出发，推导用于迭代求解得分方程 $U(\\theta)=0$ 以获得MLE的牛顿-拉夫逊更新法则。\n\n然后，将其特化到具有单个预测变量的二元逻辑回归，其中对于每个患者\n$$\np_i(\\beta_0,\\beta_1) \\equiv \\Pr(y_i=1 \\mid x_i) = \\frac{1}{1+\\exp\\!\\big(-\\eta_i\\big)}, \\quad \\text{with } \\eta_i = \\beta_0 + \\beta_1 x_i,\n$$\n且独立观测值的对数似然为\n$$\nl(\\beta_0,\\beta_1) = \\sum_{i=1}^{n} \\left[ y_i \\ln p_i + (1-y_i)\\ln(1-p_i) \\right].\n$$\n使用第一性原理，不引入任何快捷公式，推导该模型的得分向量 $U(\\beta_0,\\beta_1)$ 和雅可比矩阵 $J(\\beta_0,\\beta_1)$。\n\n最后，从初始猜测 $\\beta^{(0)} = (\\beta_0^{(0)}, \\beta_1^{(0)})^{\\top} = (0,0)^{\\top}$ 开始，对以下包含 $n=6$ 名患者的数据集应用一次牛顿-拉夫逊迭代：\n- 生物标志物值 $x = (-2,\\, -1,\\, 0,\\, 1,\\, 2,\\, 0)$，\n- 结局 $y = (0,\\, 0,\\, 0,\\, 1,\\, 1,\\, 0)$。\n\n计算一次牛顿-拉夫逊步骤后更新的参数向量 $\\beta^{(1)}$。将你的最终答案表示为一个 $1 \\times 2$ 的行矩阵，并将每个元素四舍五入到四位有效数字。", "solution": "该问题分为三个部分：首先，对最大似然估计（MLE）的牛顿-拉夫逊更新法则进行一般性推导；其次，对二元逻辑回归模型的得分向量和雅可比矩阵进行具体推导；第三，将一次牛顿-拉夫逊步骤应用于给定数据集进行数值计算。\n\n### 第1部分：牛顿-拉夫逊更新法则的推导\n\n最大似然估计（MLE）的目标是找到使对数似然函数 $l(\\theta)$ 最大化的参数向量 $\\theta$。这个最大值在对数似然函数的梯度为零的临界点找到。得分函数 $U(\\theta)$ 定义为该梯度：\n$$\nU(\\theta) = \\nabla l(\\theta)\n$$\n因此，寻找 MLE $\\hat\\theta$ 等价于求解得分方程 $U(\\hat\\theta) = 0$。\n\n牛顿-拉夫逊方法是一种迭代求根算法。为了求解 $U(\\theta) = 0$，我们从一个初始猜测 $\\theta^{(k)}$ 开始，旨在找到一个更好的近似 $\\theta^{(k+1)}$。我们通过在 $\\theta^{(k)}$ 周围的一阶泰勒级数展开来近似 $U(\\theta)$：\n$$\nU(\\theta) \\approx U(\\theta^{(k)}) + J(\\theta^{(k)}) (\\theta - \\theta^{(k)})\n$$\n其中 $J(\\theta^{(k)})$ 是得分函数 $U(\\theta)$ 在 $\\theta^{(k)}$ 处求值的雅可比矩阵。雅可比矩阵的元素由 $J_{ij} = \\frac{\\partial U_i}{\\partial \\theta_j} = \\frac{\\partial^2 l}{\\partial \\theta_j \\partial \\theta_i}$ 给出。注意，这是对数似然函数 $l(\\theta)$ 的黑塞矩阵。\n\n为了找到下一个迭代值 $\\theta^{(k+1)}$，我们在近似式中设置 $U(\\theta^{(k+1)}) = 0$：\n$$\n0 \\approx U(\\theta^{(k)}) + J(\\theta^{(k)}) (\\theta^{(k+1)} - \\theta^{(k)})\n$$\n重新整理这个方程以求解更新步长 $(\\theta^{(k+1)} - \\theta^{(k)})$，我们得到：\n$$\nJ(\\theta^{(k)}) (\\theta^{(k+1)} - \\theta^{(k)}) = -U(\\theta^{(k)})\n$$\n假设雅可比矩阵 $J(\\theta^{(k)})$ 是可逆的，我们可以乘以它的逆矩阵 $[J(\\theta^{(k)})]^{-1}$：\n$$\n\\theta^{(k+1)} - \\theta^{(k)} = -[J(\\theta^{(k)})]^{-1} U(\\theta^{(k)})\n$$\n这就得到了迭代寻找 MLE 的牛顿-拉夫逊更新法则：\n$$\n\\theta^{(k+1)} = \\theta^{(k)} - [J(\\theta^{(k)})]^{-1} U(\\theta^{(k)})\n$$\n\n### 第2部分：逻辑回归的得分和雅可比矩阵\n\n我们现在特化到参数为 $\\beta = (\\beta_0, \\beta_1)^{\\top}$ 的二元逻辑回归模型。对于 $n$ 个独立观测值，对数似然函数由下式给出：\n$$\nl(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} \\left[ y_i \\ln p_i + (1-y_i)\\ln(1-p_i) \\right]\n$$\n其中 $p_i = \\Pr(y_i=1 \\mid x_i) = \\frac{1}{1+\\exp(-\\eta_i)}$ 且 $\\eta_i = \\beta_0 + \\beta_1 x_i$。\n\n首先，我们求 $p_i$ 关于 $\\eta_i$ 的一个关键导数：\n$$\n\\frac{\\partial p_i}{\\partial \\eta_i} = \\frac{\\partial}{\\partial \\eta_i} \\left( (1+\\exp(-\\eta_i))^{-1} \\right) = -1 \\cdot (1+\\exp(-\\eta_i))^{-2} \\cdot (-\\exp(-\\eta_i)) = \\frac{\\exp(-\\eta_i)}{(1+\\exp(-\\eta_i))^2}\n$$\n这可以改写为：\n$$\n\\frac{\\partial p_i}{\\partial \\eta_i} = \\frac{1}{1+\\exp(-\\eta_i)} \\cdot \\frac{\\exp(-\\eta_i)}{1+\\exp(-\\eta_i)} = p_i \\cdot \\left( \\frac{1+\\exp(-\\eta_i)-1}{1+\\exp(-\\eta_i)} \\right) = p_i \\cdot \\left( 1 - \\frac{1}{1+\\exp(-\\eta_i)} \\right) = p_i(1-p_i)\n$$\n\n得分向量 $U(\\beta_0, \\beta_1)$ 的分量是 $\\frac{\\partial l}{\\partial \\beta_0}$ 和 $\\frac{\\partial l}{\\partial \\beta_1}$。我们使用链式法则：\n$$\n\\frac{\\partial l}{\\partial \\beta_j} = \\sum_{i=1}^{n} \\frac{\\partial l_i}{\\partial \\beta_j} = \\sum_{i=1}^{n} \\frac{\\partial l_i}{\\partial p_i} \\frac{\\partial p_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_j}\n$$\n第 $i$ 个对数似然项关于 $p_i$ 的导数是：\n$$\n\\frac{\\partial l_i}{\\partial p_i} = \\frac{y_i}{p_i} - \\frac{1-y_i}{1-p_i} = \\frac{y_i(1-p_i) - (1-y_i)p_i}{p_i(1-p_i)} = \\frac{y_i - p_i}{p_i(1-p_i)}\n$$\n结合这些结果：\n$$\n\\frac{\\partial l_i}{\\partial \\beta_j} = \\left( \\frac{y_i - p_i}{p_i(1-p_i)} \\right) \\cdot (p_i(1-p_i)) \\cdot \\frac{\\partial \\eta_i}{\\partial \\beta_j} = (y_i - p_i) \\frac{\\partial \\eta_i}{\\partial \\beta_j}\n$$\n$\\eta_i$ 的导数是 $\\frac{\\partial \\eta_i}{\\partial \\beta_0} = 1$ 和 $\\frac{\\partial \\eta_i}{\\partial \\beta_1} = x_i$。\n得分向量的分量是：\n$$\nU_0 = \\frac{\\partial l}{\\partial \\beta_0} = \\sum_{i=1}^{n} (y_i - p_i) \\cdot 1 = \\sum_{i=1}^{n} (y_i - p_i)\n$$\n$$\nU_1 = \\frac{\\partial l}{\\partial \\beta_1} = \\sum_{i=1}^{n} (y_i - p_i) \\cdot x_i = \\sum_{i=1}^{n} x_i(y_i - p_i)\n$$\n所以得分向量是 $U(\\beta_0, \\beta_1) = \\begin{pmatrix} \\sum_{i=1}^{n} (y_i - p_i) \\\\ \\sum_{i=1}^{n} x_i(y_i - p_i) \\end{pmatrix}$。\n\n接下来，我们推导雅可比矩阵 $J(\\beta_0, \\beta_1)$，其元素为 $J_{jk} = \\frac{\\partial U_j}{\\partial \\beta_k} = \\frac{\\partial^2 l}{\\partial \\beta_k \\partial \\beta_j}$。\n$$\n\\frac{\\partial^2 l}{\\partial \\beta_k \\partial \\beta_j} = \\frac{\\partial}{\\partial \\beta_k} \\left( \\sum_{i=1}^{n} (y_i - p_i) \\frac{\\partial \\eta_i}{\\partial \\beta_j} \\right) = \\sum_{i=1}^{n} \\left( -\\frac{\\partial p_i}{\\partial \\beta_k} \\right) \\frac{\\partial \\eta_i}{\\partial \\beta_j}\n$$\n再次使用链式法则，$\\frac{\\partial p_i}{\\partial \\beta_k} = \\frac{\\partial p_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_k} = p_i(1-p_i) \\frac{\\partial \\eta_i}{\\partial \\beta_k}$。\n代入这个结果，得到雅可比矩阵元素的一般形式：\n$$\nJ_{jk} = -\\sum_{i=1}^{n} p_i(1-p_i) \\frac{\\partial \\eta_i}{\\partial \\beta_k} \\frac{\\partial \\eta_i}{\\partial \\beta_j}\n$$\n现在我们计算具体的元素：\n$$\nJ_{00} = \\frac{\\partial^2 l}{\\partial \\beta_0^2} = -\\sum_{i=1}^{n} p_i(1-p_i) (1)(1) = -\\sum_{i=1}^{n} p_i(1-p_i)\n$$\n$$\nJ_{01} = \\frac{\\partial^2 l}{\\partial \\beta_1 \\partial \\beta_0} = -\\sum_{i=1}^{n} p_i(1-p_i) (x_i)(1) = -\\sum_{i=1}^{n} x_i p_i(1-p_i)\n$$\n根据对称性（克莱罗定理），$J_{10} = J_{01}$。\n$$\nJ_{11} = \\frac{\\partial^2 l}{\\partial \\beta_1^2} = -\\sum_{i=1}^{n} p_i(1-p_i) (x_i)(x_i) = -\\sum_{i=1}^{n} x_i^2 p_i(1-p_i)\n$$\n因此，雅可比矩阵是 $J(\\beta_0, \\beta_1) = \\begin{pmatrix} -\\sum p_i(1-p_i)  -\\sum x_i p_i(1-p_i) \\\\ -\\sum x_i p_i(1-p_i)  -\\sum x_i^2 p_i(1-p_i) \\end{pmatrix}$。\n\n### 第3部分：数值应用\n\n我们从 $\\beta^{(0)} = (\\beta_0^{(0)}, \\beta_1^{(0)})^{\\top} = (0,0)^{\\top}$ 开始，应用一次牛顿-拉夫逊步骤。\n数据集为 $n=6$ 时的 $x = (-2, -1, 0, 1, 2, 0)$ 和 $y = (0, 0, 0, 1, 1, 0)$。\n\n首先，在 $\\beta^{(0)}$ 处计算概率。对于任何 $x_i$，$\\eta_i^{(0)} = \\beta_0^{(0)} + \\beta_1^{(0)} x_i = 0 + 0 \\cdot x_i = 0$。\n每个患者的概率是 $p_i^{(0)} = \\frac{1}{1+\\exp(-0)} = \\frac{1}{1+1} = 0.5$。\n\n接下来，计算得分向量 $U(\\beta^{(0)})$：\n我们需要以下总和：\n$\\sum_{i=1}^6 y_i = 0+0+0+1+1+0 = 2$。\n$\\sum_{i=1}^6 x_i = -2-1+0+1+2+0 = 0$。\n$\\sum_{i=1}^6 x_i y_i = (-2)(0) + (-1)(0) + (0)(0) + (1)(1) + (2)(1) + (0)(0) = 3$。\n得分向量的分量是：\n$U_0^{(0)} = \\sum_{i=1}^6 (y_i - p_i^{(0)}) = \\sum y_i - \\sum p_i^{(0)} = 2 - 6 \\times 0.5 = 2 - 3 = -1$。\n$U_1^{(0)} = \\sum_{i=1}^6 x_i (y_i - p_i^{(0)}) = \\sum x_i y_i - p_i^{(0)} \\sum x_i = 3 - 0.5 \\times 0 = 3$。\n所以，$U(\\beta^{(0)}) = \\begin{pmatrix} -1 \\\\ 3 \\end{pmatrix}$。\n\n接下来，计算雅可比矩阵 $J(\\beta^{(0)})$：\n对于所有 $i$，$p_i^{(0)}(1-p_i^{(0)}) = 0.5 \\times (1-0.5) = 0.25$。\n我们需要 $x_i$ 的平方和：\n$\\sum_{i=1}^6 x_i^2 = (-2)^2 + (-1)^2 + 0^2 + 1^2 + 2^2 + 0^2 = 4+1+0+1+4+0 = 10$。\n雅可比矩阵的元素是：\n$J_{00}^{(0)} = -\\sum p_i^{(0)}(1-p_i^{(0)}) = -6 \\times 0.25 = -1.5$。\n$J_{01}^{(0)} = -\\sum x_i p_i^{(0)}(1-p_i^{(0)}) = -0.25 \\sum x_i = -0.25 \\times 0 = 0$。\n$J_{11}^{(0)} = -\\sum x_i^2 p_i^{(0)}(1-p_i^{(0)}) = -0.25 \\sum x_i^2 = -0.25 \\times 10 = -2.5$。\n所以，$J(\\beta^{(0)}) = \\begin{pmatrix} -1.5  0 \\\\ 0  -2.5 \\end{pmatrix}$。\n\n最后，执行更新以找到 $\\beta^{(1)} = (\\beta_0^{(1)}, \\beta_1^{(1)})^{\\top}$：\n$$\n\\beta^{(1)} = \\beta^{(0)} - [J(\\beta^{(0)})]^{-1} U(\\beta^{(0)})\n$$\n对角雅可比矩阵的逆矩阵是：\n$$\n[J(\\beta^{(0)})]^{-1} = \\begin{pmatrix} 1/(-1.5)  0 \\\\ 0  1/(-2.5) \\end{pmatrix} = \\begin{pmatrix} -2/3  0 \\\\ 0  -2/5 \\end{pmatrix}\n$$\n现在，我们计算 $\\beta^{(1)}$：\n$$\n\\beta^{(1)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} -2/3  0 \\\\ 0  -2/5 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 3 \\end{pmatrix} = -\\begin{pmatrix} (-2/3)(-1) + (0)(3) \\\\ (0)(-1) + (-2/5)(3) \\end{pmatrix} = -\\begin{pmatrix} 2/3 \\\\ -6/5 \\end{pmatrix} = \\begin{pmatrix} -2/3 \\\\ 6/5 \\end{pmatrix}\n$$\n用小数形式表示，这是 $\\beta^{(1)} = \\begin{pmatrix} -0.6666... \\\\ 1.2 \\end{pmatrix}$。\n将每个条目四舍五入到四位有效数字，我们得到：\n$\\beta_0^{(1)} \\approx -0.6667$\n$\\beta_1^{(1)} = 1.200$\n\n更新后的参数向量为 $\\beta^{(1)} \\approx (-0.6667, 1.200)^{\\top}$。按要求表示为一个 $1 \\times 2$ 的行矩阵：\n$\\begin{pmatrix} -0.6667  1.200 \\end{pmatrix}$。", "answer": "$$\n\\boxed{\\begin{pmatrix} -0.6667  1.200 \\end{pmatrix}}\n$$", "id": "4969348"}]}