{"hands_on_practices": [{"introduction": "极大似然估计（MLE）是一种强大而普遍的参数估计方法，但其结果未必总是完美的。这个练习将通过估计方差这一常见任务，引导你探索估计量的一个关键性质——偏差。理解并修正这种偏差，对于在神经科学等领域准确解释数据至关重要。[@problem_id:4159922]", "problem": "一位神经科学家正在量化来自一个皮层区域的诱发局部场电位（LFP）振幅的逐次试验变异性。在 $n$ 次独立试验中，振幅测量值被建模为来自未知均值 $\\mu$ 和未知方差 $\\sigma^{2}$ 的高斯分布的独立同分布样本 $y_{1},\\dots,y_{n}$，即，对于 $i=1,\\dots,n$，$y_{i} \\sim \\mathcal{N}(\\mu,\\sigma^{2})$ 独立地。令 $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_{i}$ 表示样本均值。\n\n从高斯模型的似然函数和标量参数 $\\theta$ 的估计量 $\\hat{\\theta}$ 的偏差定义 $\\operatorname{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$ 出发，完成以下任务：\n\n1. 在均值 $\\mu$ 未知的假设下，推导方差 $\\sigma^{2}$ 的最大似然估计量（MLE），并用 $y_{i}$ 和 $\\bar{y}$ 表示。\n2. 使用独立高斯随机变量的期望和方差的核心性质，推导 $\\sigma^{2}$ 的 MLE 的期望，并得到其偏差，表示为 $n$ 和 $\\sigma^{2}$ 的函数。你的推导应仅依赖于基本原理，如独立性、期望的线性性以及高斯数据样本均值的方差。\n3. 基于用 $\\bar{y}$ 构成的残差平方和，提出一个 $\\sigma^{2}$ 的无偏估计量，并证明其是无偏的。\n\n报告 $\\sigma^{2}$ 的 MLE 偏差的闭式解析表达式，用 $n$ 和 $\\sigma^{2}$ 表示，作为你的最终答案。不需要四舍五入。最终答案必须是单一表达式。", "solution": "该问题被认为是有效的，因为它具有科学依据、问题设定良好、客观且内容完整。它在神经科学数据分析的背景下，提出了一个估计理论中标准的、可解的问题。我们将进行完整的推导。\n\n该问题要求我们完成三项任务，这些任务与从一组 $n$ 个独立同分布（i.i.d.）样本 $y_1, \\dots, y_n$ 中估计高斯分布 $\\mathcal{N}(\\mu, \\sigma^2)$ 的方差 $\\sigma^2$ 相关，其中 $\\mu$ 和 $\\sigma^2$ 均为未知。\n\n**1. $\\sigma^2$ 的最大似然估计量（MLE）的推导**\n\n从均值为 $\\mu$、方差为 $\\sigma^2$ 的高斯分布中抽取的单个观测值 $y_i$ 的概率密度函数（PDF）为：\n$$f(y_i \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right)$$\n鉴于样本 $y_1, \\dots, y_n$ 是独立同分布的，似然函数 $L(\\mu, \\sigma^2 \\mid y_1, \\dots, y_n)$ 是各个 PDF 的乘积：\n$$L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} f(y_i \\mid \\mu, \\sigma^2) = \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2\\right)$$\n为了找到最大似然估计量，使用对数似然函数 $\\ell(\\mu, \\sigma^2) = \\ln(L(\\mu, \\sigma^2))$ 更为方便：\n$$\\ell(\\mu, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} (y_i - \\mu)^2$$\n我们通过求 $\\ell$ 关于 $\\mu$ 和 $\\sigma^2$ 的偏导数并将其设为零来找到 MLE。\n\n首先，对于均值 $\\mu$：\n$$\\frac{\\partial \\ell}{\\partial \\mu} = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} 2(y_i - \\mu)(-1) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)$$\n将其设为零可得 $\\sum_{i=1}^{n} (y_i - \\mu) = 0$，这意味着 $\\sum_{i=1}^{n} y_i - n\\mu = 0$。因此，$\\mu$ 的 MLE 是：\n$$\\hat{\\mu}_{\\text{MLE}} = \\frac{1}{n}\\sum_{i=1}^{n} y_i = \\bar{y}$$\n接下来，对于方差 $\\sigma^2$。为方便起见，我们设 $\\theta = \\sigma^2$。\n$$\\frac{\\partial \\ell}{\\partial \\theta} = -\\frac{n}{2\\theta} + \\frac{1}{2\\theta^2}\\sum_{i=1}^{n} (y_i - \\mu)^2$$\n为了找到似然函数的联合最大值，我们将 $\\mu$ 的 MLE $\\hat{\\mu}_{\\text{MLE}} = \\bar{y}$ 代入关于 $\\theta$ 的导数中，并将其设为零：\n$$\\left. \\frac{\\partial \\ell}{\\partial \\theta} \\right|_{\\mu=\\bar{y}, \\theta=\\hat{\\theta}_{\\text{MLE}}} = -\\frac{n}{2\\hat{\\theta}_{\\text{MLE}}} + \\frac{1}{2\\hat{\\theta}_{\\text{MLE}}^2}\\sum_{i=1}^{n} (y_i - \\bar{y})^2 = 0$$\n乘以 $2\\hat{\\theta}_{\\text{MLE}}^2$ 得到：\n$$-n\\hat{\\theta}_{\\text{MLE}} + \\sum_{i=1}^{n} (y_i - \\bar{y})^2 = 0$$\n求解 $\\hat{\\theta}_{\\text{MLE}}$，我们得到方差 $\\sigma^2$ 的 MLE：\n$$\\hat{\\sigma}^2_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\bar{y})^2$$\n\n**2. $\\sigma^2$ 的 MLE 的偏差**\n\n参数 $\\theta$ 的估计量 $\\hat{\\theta}$ 的偏差定义为 $\\operatorname{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$。我们必须计算 $\\hat{\\sigma}^2_{\\text{MLE}}$ 的期望。\n$$\\mathbb{E}[\\hat{\\sigma}^2_{\\text{MLE}}] = \\mathbb{E}\\left[ \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\right] = \\frac{1}{n} \\mathbb{E}\\left[ \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\right]$$\n我们通过加上和减去真实均值 $\\mu$ 来分析平方和项：\n$$\\sum_{i=1}^{n} (y_i - \\bar{y})^2 = \\sum_{i=1}^{n} ((y_i - \\mu) - (\\bar{y} - \\mu))^2$$\n展开平方：\n$$= \\sum_{i=1}^{n} \\left[ (y_i - \\mu)^2 - 2(y_i - \\mu)(\\bar{y} - \\mu) + (\\bar{y} - \\mu)^2 \\right]$$\n$$= \\sum_{i=1}^{n} (y_i - \\mu)^2 - 2(\\bar{y} - \\mu) \\sum_{i=1}^{n} (y_i - \\mu) + \\sum_{i=1}^{n} (\\bar{y} - \\mu)^2$$\n认识到 $\\sum_{i=1}^{n} (y_i - \\mu) = n(\\bar{y} - \\mu)$，表达式变为：\n$$= \\sum_{i=1}^{n} (y_i - \\mu)^2 - 2(\\bar{y} - \\mu) [n(\\bar{y} - \\mu)] + n(\\bar{y} - \\mu)^2$$\n$$= \\sum_{i=1}^{n} (y_i - \\mu)^2 - 2n(\\bar{y} - \\mu)^2 + n(\\bar{y} - \\mu)^2 = \\sum_{i=1}^{n} (y_i - \\mu)^2 - n(\\bar{y} - \\mu)^2$$\n现在，我们利用期望的线性性来求这个表达式的期望：\n$$\\mathbb{E}\\left[ \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\right] = \\mathbb{E}\\left[ \\sum_{i=1}^{n} (y_i - \\mu)^2 \\right] - n\\mathbb{E}\\left[ (\\bar{y} - \\mu)^2 \\right]$$\n我们分别计算每一项。\n根据定义，$y_i$ 的方差是 $\\operatorname{Var}(y_i) = \\mathbb{E}[(y_i - \\mathbb{E}[y_i])^2] = \\mathbb{E}[(y_i - \\mu)^2] = \\sigma^2$。因此：\n$$\\mathbb{E}\\left[ \\sum_{i=1}^{n} (y_i - \\mu)^2 \\right] = \\sum_{i=1}^{n} \\mathbb{E}[(y_i - \\mu)^2] = \\sum_{i=1}^{n} \\sigma^2 = n\\sigma^2$$\n项 $\\mathbb{E}[(\\bar{y} - \\mu)^2]$ 是样本均值的方差 $\\operatorname{Var}(\\bar{y})$，因为 $\\mathbb{E}[\\bar{y}] = \\mu$。我们从第一性原理推导它。由于样本是独立的：\n$$\\operatorname{Var}(\\bar{y}) = \\operatorname{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n} y_i\\right) = \\frac{1}{n^2} \\operatorname{Var}\\left(\\sum_{i=1}^{n} y_i\\right) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\operatorname{Var}(y_i) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\sigma^2 = \\frac{n\\sigma^2}{n^2} = \\frac{\\sigma^2}{n}$$\n将这些结果代回：\n$$\\mathbb{E}\\left[ \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\right] = n\\sigma^2 - n\\left(\\frac{\\sigma^2}{n}\\right) = n\\sigma^2 - \\sigma^2 = (n-1)\\sigma^2$$\n现在我们可以计算 MLE 的期望：\n$$\\mathbb{E}[\\hat{\\sigma}^2_{\\text{MLE}}] = \\frac{1}{n} \\mathbb{E}\\left[ \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\right] = \\frac{1}{n}(n-1)\\sigma^2 = \\frac{n-1}{n}\\sigma^2$$\n最后，$\\sigma^2$ 的 MLE 的偏差是：\n$$\\operatorname{Bias}(\\hat{\\sigma}^2_{\\text{MLE}}) = \\mathbb{E}[\\hat{\\sigma}^2_{\\text{MLE}}] - \\sigma^2 = \\frac{n-1}{n}\\sigma^2 - \\sigma^2 = \\left(\\frac{n-1-n}{n}\\right)\\sigma^2 = -\\frac{1}{n}\\sigma^2$$\n\n**3. $\\sigma^2$ 的无偏估计量**\n\n我们已经证明了 $\\hat{\\sigma}^2_{\\text{MLE}}$ 是一个有偏估计量。我们可以通过缩放 $\\hat{\\sigma}^2_{\\text{MLE}}$ 来构造一个无偏估计量。设无偏估计量为 $s^2$。我们希望 $\\mathbb{E}[s^2] = \\sigma^2$。\n我们知道 $\\mathbb{E}[\\hat{\\sigma}^2_{\\text{MLE}}] = \\frac{n-1}{n}\\sigma^2$。让我们定义 $s^2 = c \\cdot \\hat{\\sigma}^2_{\\text{MLE}}$，其中 $c$ 是某个常数。\n$$\\mathbb{E}[s^2] = c \\cdot \\mathbb{E}[\\hat{\\sigma}^2_{\\text{MLE}}] = c \\left(\\frac{n-1}{n}\\right)\\sigma^2$$\n为了使其无偏，我们必须有 $c \\left(\\frac{n-1}{n}\\right) = 1$，这意味着 $c = \\frac{n}{n-1}$。\n因此，基于残差平方和 $\\sum_{i=1}^{n} (y_i - \\bar{y})^2$ 提出的无偏估计量是：\n$$s^2 = \\frac{n}{n-1} \\hat{\\sigma}^2_{\\text{MLE}} = \\frac{n}{n-1} \\left( \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\right) = \\frac{1}{n-1} \\sum_{i=1}^{n} (y_i - \\bar{y})^2$$\n这是众所周知的样本方差。为了证明它是无偏的，我们计算它的期望：\n$$\\mathbb{E}[s^2] = \\mathbb{E}\\left[ \\frac{1}{n-1} \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\right] = \\frac{1}{n-1} \\mathbb{E}\\left[ \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\right]$$\n使用第 2 部分的结果，$\\mathbb{E}\\left[ \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\right] = (n-1)\\sigma^2$。\n$$\\mathbb{E}[s^2] = \\frac{1}{n-1} (n-1)\\sigma^2 = \\sigma^2$$\n由于 $\\mathbb{E}[s^2] - \\sigma^2 = 0$，估计量 $s^2$ 是无偏的。\n\n问题要求给出 $\\sigma^2$ 的 MLE 偏差的闭式表达式。这在分析的第二部分已经推导出。", "answer": "$$\\boxed{-\\frac{1}{n}\\sigma^2}$$", "id": "4159922"}, {"introduction": "在掌握了估计量的基本性质后，我们来系统地实践一下极大似然估计（MLE）的完整推导过程。本练习将以指数分布为例，从第一性原理出发构建其率参数的估计量。指数分布是生物统计学中为事件发生时间建模的基石，因此熟练掌握其参数估计是至关重要的技能。[@problem_id:4937856]", "problem": "在一项关于稀有生物事件到达间隔时间的研究中，假设您观察到一个来自率参数为 $\\theta$（其中 $\\theta0$）的指数模型的独立同分布样本 $X_{1}, X_{2}, \\ldots, X_{n}$。其概率密度函数为 $f(x\\mid\\theta)=\\theta \\exp(-\\theta x)$，其中 $x0$。请仅使用独立观测的似然函数定义以及对其关于参数最大化的原则，根据所述假设，从第一性原理出发，推导出 $\\theta$ 的最大似然估计量 $\\hat{\\theta}$。请仔细证明您的解确实是参数空间 $\\{\\theta0\\}$ 上的最大化子。请将您的最终答案表示为关于样本均值 $\\bar{X}$ 的单一闭式表达式，其中 $\\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$。不需要数值近似，也不涉及单位。", "solution": "该问题要求基于一个独立同分布（i.i.d.）样本 $X_{1}, X_{2}, \\ldots, X_{n}$，推导指数分布率参数 $\\theta$ 的最大似然估计量（MLE）。推导必须从第一性原理出发，并且必须严格证明该解是参数空间 $\\theta  0$ 上的一个最大值。\n\n首先，我们建立似然函数 $L(\\theta)$。对于一个独立同分布样本，似然函数是在每个样本点上评估的单个概率密度函数（PDF）的乘积。概率密度函数给出为 $f(x_i \\mid \\theta) = \\theta \\exp(-\\theta x_i)$，其中 $x_i  0$。\n因此，似然函数为：\n$$L(\\theta) = \\prod_{i=1}^{n} f(x_i \\mid \\theta) = \\prod_{i=1}^{n} \\left[ \\theta \\exp(-\\theta x_i) \\right]$$\n根据乘积和指数的性质，这可以简化为：\n$$L(\\theta) = \\theta^n \\exp\\left(-\\theta \\sum_{i=1}^{n} x_i\\right)$$\n\n为了找到使 $L(\\theta)$ 最大化的 $\\theta$ 值，计算上更方便的是最大化似然函数的自然对数，即对数似然函数 $\\ell(\\theta) = \\ln(L(\\theta))$。由于自然对数是一个严格递增函数，使 $\\ell(\\theta)$ 最大化的 $\\theta$ 值也将使 $L(\\theta)$ 最大化。\n对数似然函数为：\n$$\\ell(\\theta) = \\ln\\left( \\theta^n \\exp\\left(-\\theta \\sum_{i=1}^{n} x_i\\right) \\right)$$\n使用对数性质 $\\ln(ab) = \\ln(a) + \\ln(b)$ 和 $\\ln(a^b) = b\\ln(a)$，我们得到：\n$$\\ell(\\theta) = \\ln(\\theta^n) + \\ln\\left(\\exp\\left(-\\theta \\sum_{i=1}^{n} x_i\\right)\\right)$$\n$$\\ell(\\theta) = n \\ln(\\theta) - \\theta \\sum_{i=1}^{n} x_i$$\n该函数定义在 $\\theta  0$ 上，这是指定的参数空间。\n\n为了找到最大值，我们首先通过求 $\\ell(\\theta)$ 关于 $\\theta$ 的一阶导数并将其设为零来找到临界点。这个导数通常被称为得分函数。\n$$\\frac{d\\ell(\\theta)}{d\\theta} = \\frac{d}{d\\theta}\\left( n \\ln(\\theta) - \\theta \\sum_{i=1}^{n} x_i \\right) = \\frac{n}{\\theta} - \\sum_{i=1}^{n} x_i$$\n将导数设为零以找到候选估计量，我们记为 $\\hat{\\theta}$：\n$$\\frac{n}{\\hat{\\theta}} - \\sum_{i=1}^{n} x_i = 0$$\n$$\\frac{n}{\\hat{\\theta}} = \\sum_{i=1}^{n} x_i$$\n求解 $\\hat{\\theta}$ 得：\n$$\\hat{\\theta} = \\frac{n}{\\sum_{i=1}^{n} x_i}$$\n问题要求答案用样本均值 $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i$ 来表示。我们可以将分母重写为 $\\sum_{i=1}^{n} x_i = n \\bar{x}$。将此代入 $\\hat{\\theta}$ 的表达式中：\n$$\\hat{\\theta} = \\frac{n}{n\\bar{x}} = \\frac{1}{\\bar{x}}$$\n因此，$\\theta$ 的最大似然估计量是样本均值的倒数，$\\hat{\\theta} = \\frac{1}{\\bar{X}}$。\n\n接下来，我们必须证明这个临界点对应一个最大值。我们使用二阶导数检验。对数似然函数的二阶导数为：\n$$\\frac{d^2\\ell(\\theta)}{d\\theta^2} = \\frac{d}{d\\theta}\\left( \\frac{n}{\\theta} - \\sum_{i=1}^{n} x_i \\right) = -\\frac{n}{\\theta^2}$$\n$\\theta$ 的参数空间是 $\\{\\theta \\mid \\theta  0\\}$。样本大小 $n$ 是一个正整数。因此，对于参数空间中的任何 $\\theta$，都有 $\\theta^2  0$ 和 $n  0$，这意味着二阶导数总是负的：\n$$\\frac{d^2\\ell(\\theta)}{d\\theta^2} = -\\frac{n}{\\theta^2}  0$$\n在整个定义域上为负的二阶导数表明对数似然函数 $\\ell(\\theta)$ 是严格凹的。一个严格凹函数最多只有一个临界点，如果存在，则该点必为唯一的全局最大值点。我们的计算找到了这个唯一的临界点位于 $\\hat{\\theta} = \\frac{1}{\\bar{X}}$。\n为了完全严谨，我们还检查 $\\ell(\\theta)$ 在参数空间 $(0, \\infty)$ 边界处的行为。\n当 $\\theta \\to 0^+$ 时，项 $n\\ln(\\theta) \\to -\\infty$，所以 $\\ell(\\theta) \\to -\\infty$。\n当 $\\theta \\to \\infty$ 时，我们分析 $\\ell(\\theta) = n \\ln(\\theta) - \\theta \\sum x_i$。在此极限下，负的线性项 $-\\theta \\sum x_i$ 主导对数项 $n\\ln(\\theta)$，导致 $\\ell(\\theta) \\to -\\infty$。（这假设 $\\sum x_i  0$，对于一个非平凡的到达间隔时间样本，其中每个 $x_i  0$，这必须为真）。\n由于该函数在定义域的两个边界处都趋于 $-\\infty$，并且在函数为凹的定义域内有唯一的临界点，因此该临界点必须是全局最大值点。\n因此，$\\theta$ 的最大似然估计量确实是 $\\hat{\\theta} = \\frac{1}{\\bar{X}}$。", "answer": "$$\\boxed{\\frac{1}{\\bar{X}}}$$", "id": "4937856"}, {"introduction": "虽然无偏性是一个理想的性质，但在实践中，我们往往追求整体误差最小化，而不仅仅是零偏差。本练习引入了均方误差（MSE）作为评估估计量優劣的更全面指标，并通过一个“收缩估计量”的例子，具体展示了统计学中核心的“偏差-方差权衡”思想。[@problem_id:4937858]", "problem": "一个公共卫生实验室进行 $n$ 次独立检测，每次检测使用来自同一人群的不同血样，以估计二元生物标志物（存在与否）的患病率 $p$。设 $X_{1},\\dots,X_{n}$ 是独立同分布 (I.I.D.) 的，服从 $X_{i}\\sim \\mathrm{Bernoulli}(p)$，其中 $X_{i}=1$ 表示存在，$X_{i}=0$ 表示不存在，样本比例为 $\\hat{p}=\\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$。考虑收缩估计量 $\\tilde{p}=\\frac{n\\bar{X}+\\alpha}{n+\\beta}$，其中 $\\alpha\\ge 0$ 和 $\\beta\\ge 0$ 是不依赖于数据的固定常数，并编码了伪计数正则化。\n\n从期望、方差、偏差和均方误差 (MSE) 的定义出发，其中估计量 $\\hat{\\theta}$ 对参数 $\\theta$ 的均方误差 (MSE) 为 $\\mathrm{MSE}(\\hat{\\theta})= \\mathrm{Var}(\\hat{\\theta}) + \\left(\\mathrm{Bias}(\\hat{\\theta})\\right)^{2}$，偏差为 $\\mathrm{Bias}(\\hat{\\theta})=E[\\hat{\\theta}]-\\theta$，推导：\n- $\\hat{p}$ 的偏差和方差，\n- $\\tilde{p}$ 的偏差和方差，\n- 作为 $p$ 的函数的 MSE $\\mathrm{MSE}(\\hat{p})$ 和 $\\mathrm{MSE}(\\tilde{p})$。\n\n然后，通过将差值 $\\mathrm{MSE}(\\tilde{p})-\\mathrm{MSE}(\\hat{p})$ 简化为关于 $p$、$n$、$\\alpha$ 和 $\\beta$ 的单一闭式解析表达式来比较 MSE。作为最终答案，报告 $\\mathrm{MSE}(\\tilde{p})-\\mathrm{MSE}(\\hat{p})$ 的简化表达式。不需要四舍五入。", "solution": "问题陈述已经过严格验证，被认为是有效的。它在科学上基于生物统计学和点估计理论的既定原则。问题提法恰当、客观、自洽，为得出唯一且有意义的解提供了所有必要的定义和数据。未检测到科学、逻辑或结构上的缺陷。\n\n问题要求推导伯努利比例 $p$ 的两个估计量的偏差、方差和均方误差 (MSE)，然后求出它们 MSE 之间的简化差值。设 $X_{1},\\dots,X_{n}$ 是来自 $\\mathrm{Bernoulli}(p)$ 分布的独立同分布 (I.I.D.) 随机变量。我们知道对于每个 $X_i$，其期望为 $E[X_i] = p$，方差为 $\\mathrm{Var}(X_i) = p(1-p)$。设 $S_n = \\sum_{i=1}^{n}X_{i}$。由于 I.I.D. 性质，$S_n$ 服从二项分布，$S_n \\sim \\mathrm{Binomial}(n, p)$，其期望为 $E[S_n] = np$，方差为 $\\mathrm{Var}(S_n) = np(1-p)$。\n\n首先，我们分析标准样本比例 $\\hat{p} = \\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n}X_{i} = \\frac{S_n}{n}$。\n\n$\\hat{p}$ 的期望是：\n$$E[\\hat{p}] = E\\left[\\frac{S_n}{n}\\right] = \\frac{1}{n}E[S_n] = \\frac{1}{n}(np) = p$$\n估计量 $\\hat{\\theta}$ 对参数 $\\theta$ 的偏差定义为 $\\mathrm{Bias}(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta$。因此，$\\hat{p}$ 的偏差是：\n$$\\mathrm{Bias}(\\hat{p}) = E[\\hat{p}] - p = p - p = 0$$\n估计量 $\\hat{p}$ 是无偏的。\n\n$\\hat{p}$ 的方差是：\n$$\\mathrm{Var}(\\hat{p}) = \\mathrm{Var}\\left(\\frac{S_n}{n}\\right) = \\frac{1}{n^2}\\mathrm{Var}(S_n) = \\frac{1}{n^2}(np(1-p)) = \\frac{p(1-p)}{n}$$\nMSE 定义为 $\\mathrm{MSE}(\\hat{\\theta}) = \\mathrm{Var}(\\hat{\\theta}) + (\\mathrm{Bias}(\\hat{\\theta}))^2$。对于 $\\hat{p}$，MSE 是：\n$$\\mathrm{MSE}(\\hat{p}) = \\mathrm{Var}(\\hat{p}) + (\\mathrm{Bias}(\\hat{p}))^2 = \\frac{p(1-p)}{n} + 0^2 = \\frac{p(1-p)}{n}$$\n\n接下来，我们分析收缩估计量 $\\tilde{p} = \\frac{n\\bar{X}+\\alpha}{n+\\beta} = \\frac{n\\hat{p}+\\alpha}{n+\\beta}$。\n\n$\\tilde{p}$ 的期望是：\n$$E[\\tilde{p}] = E\\left[\\frac{n\\hat{p}+\\alpha}{n+\\beta}\\right] = \\frac{1}{n+\\beta}E[n\\hat{p}+\\alpha] = \\frac{1}{n+\\beta}(nE[\\hat{p}]+\\alpha) = \\frac{np+\\alpha}{n+\\beta}$$\n$\\tilde{p}$ 的偏差是：\n$$\\mathrm{Bias}(\\tilde{p}) = E[\\tilde{p}] - p = \\frac{np+\\alpha}{n+\\beta} - p = \\frac{np+\\alpha - p(n+\\beta)}{n+\\beta} = \\frac{np+\\alpha - np - p\\beta}{n+\\beta} = \\frac{\\alpha - \\beta p}{n+\\beta}$$\n$\\tilde{p}$ 的方差是：\n$$\\mathrm{Var}(\\tilde{p}) = \\mathrm{Var}\\left(\\frac{n\\hat{p}+\\alpha}{n+\\beta}\\right) = \\left(\\frac{n}{n+\\beta}\\right)^2 \\mathrm{Var}(\\hat{p}) = \\frac{n^2}{(n+\\beta)^2}\\left(\\frac{p(1-p)}{n}\\right) = \\frac{np(1-p)}{(n+\\beta)^2}$$\n$\\tilde{p}$ 的 MSE 是其方差与偏差平方的和：\n$$\\mathrm{MSE}(\\tilde{p}) = \\mathrm{Var}(\\tilde{p}) + (\\mathrm{Bias}(\\tilde{p}))^2 = \\frac{np(1-p)}{(n+\\beta)^2} + \\left(\\frac{\\alpha - \\beta p}{n+\\beta}\\right)^2 = \\frac{np(1-p) + (\\alpha - \\beta p)^2}{(n+\\beta)^2}$$\n\n最后，我们被要求求出差值 $\\mathrm{MSE}(\\tilde{p}) - \\mathrm{MSE}(\\hat{p})$ 并将其简化为单一的闭式表达式。\n$$\\mathrm{MSE}(\\tilde{p}) - \\mathrm{MSE}(\\hat{p}) = \\frac{np(1-p) + (\\alpha - \\beta p)^2}{(n+\\beta)^2} - \\frac{p(1-p)}{n}$$\n为了合并这些项，我们使用公分母 $n(n+\\beta)^2$。\n$$\\mathrm{MSE}(\\tilde{p}) - \\mathrm{MSE}(\\hat{p}) = \\frac{n\\left[np(1-p) + (\\alpha - \\beta p)^2\\right] - (n+\\beta)^2 p(1-p)}{n(n+\\beta)^2}$$\n我们来分析分子。我们可以将包含 $p(1-p)$ 的项分组：\n$$ \\text{分子} = n^2 p(1-p) - (n+\\beta)^2 p(1-p) + n(\\alpha - \\beta p)^2 $$\n$$ = \\left[n^2 - (n+\\beta)^2\\right]p(1-p) + n(\\alpha - \\beta p)^2 $$\n$$ = \\left[n^2 - (n^2 + 2n\\beta + \\beta^2)\\right]p(1-p) + n(\\alpha - \\beta p)^2 $$\n$$ = (-2n\\beta - \\beta^2)p(1-p) + n(\\alpha - \\beta p)^2 $$\n现在我们展开余下的项，并根据 $p$ 的幂次收集系数：\n$$ \\text{分子} = -\\beta(2n+\\beta)(p-p^2) + n(\\alpha^2 - 2\\alpha\\beta p + \\beta^2 p^2) $$\n$$ = - (2n\\beta + \\beta^2)p + (2n\\beta + \\beta^2)p^2 + n\\alpha^2 - 2n\\alpha\\beta p + n\\beta^2 p^2 $$\n收集 $p^2$ 的系数：\n$$(2n\\beta + \\beta^2 + n\\beta^2)p^2 = (n\\beta^2 + 2n\\beta + \\beta^2)p^2$$\n收集 $p$ 的系数：\n$$(-(2n\\beta + \\beta^2) - 2n\\alpha\\beta)p = -(2n\\alpha\\beta + 2n\\beta + \\beta^2)p$$\n常数项是 $n\\alpha^2$。\n\n将分子组合起来得到关于 $p$ 的多项式：\n$$ \\text{分子} = (n\\beta^2 + 2n\\beta + \\beta^2)p^2 - (2n\\alpha\\beta + 2n\\beta + \\beta^2)p + n\\alpha^2 $$\n因此，MSE 的简化差值为：\n$$ \\mathrm{MSE}(\\tilde{p}) - \\mathrm{MSE}(\\hat{p}) = \\frac{(n\\beta^2 + 2n\\beta + \\beta^2)p^2 - (2n\\alpha\\beta + 2n\\beta + \\beta^2)p + n\\alpha^2}{n(n+\\beta)^2} $$", "answer": "$$\n\\boxed{\\frac{(n\\beta^2 + 2n\\beta + \\beta^2)p^2 - (2n\\alpha\\beta + 2n\\beta + \\beta^2)p + n\\alpha^2}{n(n+\\beta)^2}}\n$$", "id": "4937858"}]}