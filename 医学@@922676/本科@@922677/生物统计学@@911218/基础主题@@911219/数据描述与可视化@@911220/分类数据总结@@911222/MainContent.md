## 引言
在生物统计学、流行病学及日益增多的数据驱动学科中，对[分类数据](@entry_id:202244)进行准确的总结与分析是一项基本而关键的技能。无论是评估一种新疗法的有效性、监测人群中的疾病患病率，还是构建预测模型，我们都离不开对类别、计数和比例的严谨处理。然而，与连续数据不同，[分类数据](@entry_id:202244)的汇总隐藏着独特的挑战和陷阱；不恰当的方法不仅会丢失关键信息，甚至可能引出完全错误的科学结论。

本文旨在系统性地解决这一知识需求，为读者提供一个从理论基础到前沿应用的全面指南。通过学习本文，你将不仅掌握计算比例和风险比的基础知识，更能理解这些统计量背后的原理、它们在复杂数据场景中的局限性，以及如何将它们应用于解决真实的科学问题。

文章将通过三个核心章节展开：第一章“**原理与机制**”将深入剖析[分类数据](@entry_id:202244)总结的基石，包括测量尺度、核心汇总统计量、关联性度量（如风险比与优势比），并直面混杂（如辛普森悖论）、[缺失数据](@entry_id:271026)和聚集性数据等复杂挑战。第二章“**应用与跨学科联系**”将把这些理论置于实践之中，展示它们在公共卫生、临床研究、循证医学乃至人工智能等领域的具体应用，从调整患病率估算到评估AI模型的增量价值。最后，第三章“**动手实践**”将提供一系列精心设计的问题，让你有机会亲手应用所学知识，巩固理解。让我们首先从[分类数据](@entry_id:202244)总结的核心原理与机制开始探索。

## 原理与机制

在生物统计学中，对[分类数据](@entry_id:202244)的精确总结是描述性分析和推断性分析的基石。与连续数据不同，[分类数据](@entry_id:202244)的值代表了不同的类别或标签，其汇总和解释需要一套专门的方法。本章将深入探讨总结[分类数据](@entry_id:202244)的核心原理与机制，从数据的[基本类](@entry_id:158335)型出发，逐步扩展到关联性度量、混杂因素的挑战以及现实世界数据中常见的复杂情况。

### [分类数据](@entry_id:202244)的[基本类](@entry_id:158335)型：测量的尺度

对变量进行汇总的第一步是识别其测量尺度，因为这决定了哪些统计量是有意义的。不恰当的汇总方法不仅会丢失信息，甚至可能导致错误的结论。

#### 名义变量 (Nominal Variables)

**名义变量**的类别之间没有内在的顺序或等级。这些类别仅仅是标签，用于区分不同的组。例如，血型（A、B、AB、O）或样本来源的病原体种类（如金黄色葡萄球菌 vs. [大肠杆菌](@entry_id:265676)）都是名义变量。对这类变量进行汇总时，核心任务是描述各类别中观察对象的分布情况。

最主要的汇总统计量是每个类别的**频数**（frequency，即计数）和**比例**（proportion）。描述集中趋势的唯一适用指标是**众数**（mode），即最常见的类别。对名义变量的类别进行数值编码（例如，A=1, B=2）并计算均值或中位数是毫无意义的，因为这些数值编码是任意的，不具备代数性质。[@problem_id:4955364]

#### 有序变量 (Ordinal Variables)

**有序变量**的类别具有明确的内在顺序或等级，但类别之间的间隔不一定是均等的。例如，自我报告的疼痛程度（无、轻微、中度、剧烈）或纽约心脏协会（NYHA）心力衰竭分级（I、II、III、IV）都是有序变量。我们可以确定“剧烈”疼痛比“轻微”疼痛更严重，但无法量化其严重程度的差异是“中度”与“轻微”之间差异的多少倍。

由于存在顺序，除了频数和比例外，**累积比例**（cumulative proportions）成为一个非常有用的汇总统计量。例如，在急诊科的分诊严重性评估中，一个三级量表包含“危重”、“紧急”和“非紧急”三个有序类别。假设一周内分诊了200名患者，其中危重30人，紧急90人，非紧急80人。我们可以计算出处于“紧急”或更严重级别的患者比例为 $(30+90)/200 = 0.60$。

对于集中趋势，**[中位数](@entry_id:264877)**（median）是描述有[序数](@entry_id:150084)据的首选指标，它代表了累积比例达到或超过0.50的最小类别。在上例中，危重类别的累积比例为 $30/200 = 0.15$，紧急类别的累积比例为 $(30+90)/200 = 0.60$。因此，中位数类别是“紧急”。同样，我们也可以定义**[四分位数](@entry_id:167370)**等位置度量。将有序类别错误地视为名义变量，会丢失其固有的等级结构，使中位数和累积比例等概念变得无从谈起。反之，若强行给类别赋分（如危重=1, 紧急=2, 非紧急=3）并计算均值和标准差，则无理地假设了类别间的距离是相等的，这同样是错误的。[@problem_id:4955396]

#### 二元变量 (Binary Variables)

**[二元变量](@entry_id:162761)**是只有两个可能类别的特殊情况，例如生存/死亡、疫苗接种后[血清转化](@entry_id:195698)（是/否）。这类变量可以被看作是名义变量或有序变量。由于只有两个互补的类别，我们通常只需报告其中一个类别的频数和比例。一个重要的特性是，如果我们将两个类别编码为 $1$ 和 $0$（例如，$Y=1$ 代表事件发生，$Y=0$ 代表事件未发生），那么样本均值 $\bar{Y}$ 在数值上就等于事件发生的样本比例。这为在某些模型（如线性回归）中处理[二元变量](@entry_id:162761)提供了便利。[@problem_id:4955364]

#### 与离散计数数据的区别

[分类数据](@entry_id:202244)必须与**离散计数数据**（discrete count data）明确区分。离散计数数据是定量的，代表事件发生的次数，取值为非负整数（$0, 1, 2, \dots$）。例如，患者在一年内的急诊次数或培养皿上的菌落数量。这些数值具有真正的数量意义，数值间的差异是恒定的（例如，2次与1次的差异和1次与0次的差异相同）。其汇总统计量包括均值、方差、中位数等，并且当计数是在一定时间或空间内观察到时，通常会计算**率**（rate，如每人年事件发生率）。[@problem_id:4955364]

### 单个[分类变量](@entry_id:637195)的汇总与推断

在确定了数据类型后，下一步是进行准确的量化总结，并理解这些总结如何作为对总体参数的估计。

#### 频数、比例、百分比与患病率

这些术语在日常和科学语境中经常互换使用，但它们在统计学上具有精确的含义。[@problem_id:4955339]

-   **频数 (Frequency)**：指一个类别中观察到的原始**计数**，例如样本中出现某健康状况的病例数 $F$。频数本身依赖于样本量，通常不是一个好的跨研究比较指标。它不是总体病例总数 $T_X$ 的[无偏估计](@entry_id:756289)，除非样本就是总体本身。

-   **比例 (Proportion)**：是频数除以样本总数 $n$ 的结果，即 $\hat{p} = F/n$。比例是一个介于 $0$ 和 $1$ 之间的值，它标准化了样本量。在概率抽样（如简单[随机抽样](@entry_id:175193)）下，样本比例 $\hat{p}$ 是总体比例（或概率）$P_X$ 的一个**无偏**和**一致**的估计量。这意味着，在[重复抽样](@entry_id:274194)的想象实验中，样本比例的[期望值](@entry_id:150961)等于真实的总体比例，即 $\mathbb{E}[\hat{p}] = P_X$。

-   **百分比 (Percentage)**：是比例乘以100的结果，即 $100 \times \hat{p}$。它只是比例的一种呈现方式，将其从 $(0,1)$ [区间映射](@entry_id:194829)到 $(0,100)$ 区间。百分比与比例的统计性质（如无偏性）是相同的，只是单位不同，并不会因为它更直观而具有更好的[统计稳健性](@entry_id:165428)。

-   **患病率 (Prevalence)**：这是一个流行病学术语，特指在特定时间点，人群中存在某种疾病或状况的**比例**。因此，在横断面研究或队列研究的基线时刻，样本比例 $\hat{p}$ 就是对总体患病率 $P_X$ 的估计。

值得注意的是，这些美好的统计性质依赖于**概率抽样**。在非概率抽样（如[便利抽样](@entry_id:175175)）中，即使样本量很大，“[大数定律](@entry_id:140915)”也只能保证样本比例收敛到被抽样的那个特定子群体的比例，而不能保证其收敛到我们真正感兴趣的目标总体的比例。代表性源于抽样设计，而非样本量本身。[@problem_id:4955339]

#### Logit 变换

比例 $\pi$ 的取值范围是 $(0,1)$，这在许多[统计模型](@entry_id:755400)中会带来不便，例如，[线性模型](@entry_id:178302)预测的值可能超出这个范围。因此，我们常常需要一个变换 $g(\pi)$，将其映射到整个[实数轴](@entry_id:148276) $(-\infty, \infty)$。一个理想的变换应满足以下几个性质：[@problem_id:4955382]

1.  **范围映射**：$g$ 将 $(0,1)$ 映射到 $(-\infty, \infty)$。
2.  **对称性**：如果交换“成功”和“失败”的标签，变换后数值的绝对值应不变，仅符号相反，即 $g(1-\pi) = -g(\pi)$。
3.  **可加性**：将优势（odds）的乘法关系转换为加法关系。

**优势**定义为事件发生的概率与不发生的概率之比，即 $O = \pi / (1-\pi)$。第三个性质要求变换能将优势的乘积（例如，一个优势是另一个的两倍）变成尺度上的加法，这是对数函数的核心特征。因此，该变换必然是对数的函数，即 $g(\pi) = \ln(O) = \ln(\frac{\pi}{1-\pi})$。这个函数被称为 **logit** 变换，或对数优势。

我们可以验证 logit 变换满足所有性质：
-   当 $\pi \to 0$ 时，$\text{logit}(\pi) \to -\infty$；当 $\pi \to 1$ 时，$\text{logit}(\pi) \to +\infty$。
-   $\text{logit}(1-\pi) = \ln(\frac{1-\pi}{1-(1-\pi)}) = \ln(\frac{1-\pi}{\pi}) = -\ln(\frac{\pi}{1-\pi}) = -\text{logit}(\pi)$。

在实际应用中，我们计算**经验 logit**。为避免在样本中出现 $x=0$ 或 $x=n$ 的情况（此时比例为0或1，logit无定义），通常采用**小样本调整**，例如 Haldane-Anscombe 修正，即给成功和失败的计数都加上 $0.5$。对于 $n=80$ 的样本中观察到 $x=12$ 个事件，调整后的计数为 $x_{adj}=12.5$ 和 $(n-x)_{adj}=68.5$。经验 logit 为：
$$ \text{logit}(\hat{p}_{adj}) = \ln\left(\frac{12.5}{68.5}\right) \approx -1.701 $$
其近似标准误可通过Delta方法推导，一个简单的估计公式是：
$$ \text{SE}(\text{logit}(\hat{p}_{adj})) \approx \sqrt{\frac{1}{x_{adj}} + \frac{1}{(n-x)_{adj}}} = \sqrt{\frac{1}{12.5} + \frac{1}{68.5}} \approx 0.3076 $$
Logit 变换是逻辑回归等广义线性模型的核心，它将[二元结果](@entry_id:173636)的概率与线性预测变量联系起来。[@problem_id:4955382]

### 比较两组：关联性度量

生物统计学中的一个核心任务是比较不同组（如治疗组 vs. 安慰剂组）的事件发生风险。对于[二元结果](@entry_id:173636)，我们通常使用 $2 \times 2$ 表来总结数据，并计算关联性度量。

设 $\pi_1$ 和 $\pi_0$ 分别为暴露组（$X=1$）和非暴露组（$X=0$）发生事件（$Y=1$）的风险（概率）。三个主要的关联性度量是：

-   **风险差 (Risk Difference, RD)**: $\mathrm{RD} = \pi_1 - \pi_0$。它衡量了暴露带来的绝对风险变化。
-   **风险比 (Risk Ratio, RR)**: $\mathrm{RR} = \pi_1 / \pi_0$。它衡量了暴露使风险增加或减少的倍数，也称为相对风险。
-   **优势比 (Odds Ratio, OR)**: $\mathrm{OR} = o_1 / o_0$，其中优势 $o_x = \pi_x / (1 - \pi_x)$。它衡量了暴露组中事件发生的优势相对于非暴露组中事件发生优势的倍数。

一个有趣且重要的性质是这些度量在**结果重新编码**时的表现。假设我们不关注不良事件（$Y=1$），而是关注良好结果（$Y'=1-Y=1$）。新的风险为 $\pi'_x = 1 - \pi_x$。新的关联度量 $\mathrm{RD}', \mathrm{RR}', \mathrm{OR}'$ 会如何变化？[@problem_id:4955337]

-   **风险差**: $\mathrm{RD}' = \pi'_1 - \pi'_0 = (1-\pi_1) - (1-\pi_0) = -(\pi_1 - \pi_0) = -\mathrm{RD}$。风险差的绝对值不变，但符号相反。
-   **优势比**: 新的优势 $o'_x = \pi'_x / (1-\pi'_x) = (1-\pi_x) / \pi_x = 1/o_x$。因此，$\mathrm{OR}' = o'_1 / o'_0 = (1/o_1)/(1/o_0) = o_0/o_1 = 1/\mathrm{OR}$。优势比变为其倒数。
-   **风险比**: $\mathrm{RR}' = \pi'_1 / \pi'_0 = (1-\pi_1) / (1-\pi_0)$。这个新值与原始的 $\mathrm{RR} = \pi_1/\pi_0$ 之间没有简单的倒数或相反数关系。

OR的这种对称性（即对事件或非事件的OR互为倒数）是其在统计建模（特别是逻辑回归）中备受青睐的数学特性之一。无论我们将“成功”还是“失败”定义为结果 $Y=1$，关联的强度（以对数尺度看，$\ln(\mathrm{OR}') = -\ln(\mathrm{OR})$）是相同的。

### 混杂的挑战与调整方法

在[观察性研究](@entry_id:174507)中，暴露与结果之间的表面关联可能被第三个变量——**混杂因素**（confounder）——所扭曲。混杂因素既与暴露相关，也与结果相关。

#### 辛普森悖论 (Simpson's Paradox)

辛普森悖论是一个极端但经典的例子，说明了忽略混杂因素的危险。在这个悖论中，在一个合并的总体中观察到的关联方向，在按混杂因素分层的每个亚组中都完全相反。

考虑一个分析两种疗法（$A$ 和 $B$）对康复（$Y=1$）效果的研究，同时根据患者的临床风险组（低风险/高风险）进行分层。数据如下：[@problem_id:4955384]
-   **低风险组**：疗法A下，19人康复，1人不康复；疗法B下，490人康复，60人不康复。
-   **高风险组**：疗法A下，80人康复，120人不康复；疗法B下，9人康复，16人不康复。

我们计算每个分层的条件优势比 (conditional OR)：
-   低风险组：$OR_{XY|Z=\text{low}} = (19 \times 60) / (1 \times 490) \approx 2.33$。
-   高风险组：$OR_{XY|Z=\text{high}} = (80 \times 16) / (120 \times 9) \approx 1.19$。

在两个风险组内，疗法A的康复优势都高于疗法B（$OR > 1$）。现在，让我们忽略风险分层，将数据合并：
-   疗法A：总计 $19+80=99$ 人康复，$1+120=121$ 人不康复。
-   疗法B：总计 $490+9=499$ 人康复，$60+16=76$ 人不康复。

计算合并后的边际优势比 (marginal OR)：
$$ OR_M = (99 \times 76) / (121 \times 499) \approx 0.1246 $$
令人惊讶的是，合并后的结果显示疗法A的康复优势远低于疗法B（$OR \ll 1$）。这就是辛普森悖论。悖论的产生是因为风险组是一个混杂因素：它与疗法（暴露）相关（疗法A更多地被用于高风险患者），也与康复（结果）相关（高风险患者的总体康复率更低）。分层分析控制了这种混杂，揭示了真实的关联。

#### 标准化 (Standardization)

为了在存在混杂的情况下得出一个单一的、经过调整的效应估计，**标准化**是一种重要方法。其核心思想是：计算在一个“标准”的协变量分布下，不同暴露组的预期风险会是多少。

标准化风险是通过对分层风险 $P(Y=1 | A=a, L=l)$ 进行加权平均得到的，权重来自预先指定的标准协变量分布 $p^\star(L)$。其公式是**[全概率定律](@entry_id:268479)**的应用：
$$ R_a^\star = \sum_l P(Y=1 | A=a, L=l) \cdot p^\star(l) $$
这个计算产生了一个可比的指标：它告诉我们，如果所有暴露组都具有与标准人群完全相同的协变量构成，他们的风险会是多少。[@problem_id:4955404]

为了使标准化风险具有**因果解释**（即，它估计的是将整个人群的暴露水平设为 $a$ 时的潜在结果风险 $P(Y^a=1)$），必须满足三个关键假设：
1.  **一致性 (Consistency)**：个体的实际观测结果等于其在该暴露水平下的潜在结果。
2.  **条件[可交换性](@entry_id:263314) (Conditional Exchangeability)**：在给定的协变量分层内，[潜在结果](@entry_id:753644)与实际接受的暴露无关。这等价于“无未测量的混杂因素”。
3.  **正性 (Positivity)**：在每个协变量分层中，接受每种暴露水平的概率都大于零。这确保了所有分层的风险都是可以从数据中估计的。

#### 关联的可视化：马赛克图

**马赛克图**（Mosaic Plot）是可视化两个或多个[分类变量](@entry_id:637195)之间关系的强大工具。对于一个 $2 \times 2$ 的[列联表](@entry_id:162738)，马赛克图从一个单位面积的正方形开始。首先，根据一个变量（如吸烟状况）的边际比例，将正方形水平分割。然后，在每个水平条带内，根据另一个变量（如呼吸系统疾病）在该亚组内的条件比例，进行垂直分割。[@problem_id:4955356]

这样一来，每个小矩形（瓦片）的**面积**就精确地等于其对应单元格的[联合概率](@entry_id:266356)（或样本中的联合比例 $n_{ij}/n$）。瓦片的形状则揭示了[条件概率](@entry_id:151013)。

为了评估观察到的关联是否超出偶然，马赛克图经常使用**残差着色**。在独立性原假设下，单元格的[期望计数](@entry_id:162854)为 $E_{ij} = n_{i\cdot} n_{\cdot j} / n$。**[标准化残差](@entry_id:634169)**定义为 $r_{ij} = (n_{ij} - E_{ij}) / \sqrt{E_{ij}}$。这个值近似服从标准正态分布。着色规则通常是：
-   **色调**：表示残差的符号（例如，蓝色表示正残差，即观测值大于[期望值](@entry_id:150961)；红色表示负残差）。
-   **饱和度/亮度**：表示残差的绝对值大小 $|r_{ij}|$。

$|r_{ij}|$ 的值如果大于2或3，通常被认为是从独立性假设的显著偏离。通过这种方式，马赛克图不仅展示了数据的结构，还直观地突出了关联最强的部分。[@problem_id:4955356]

### 数据汇总中的高级课题

现实世界的数据分析常常面临更复杂的挑战，如数据缺失和数据结构相关。

#### [缺失数据机制](@entry_id:173251)

在许多研究中，部分受试者的结果变量 $Y$ 可能会缺失。处理缺失数据的策略取决于数据为何会缺失。我们用一个[指示变量](@entry_id:266428) $R$ 来表示 $Y$ 是否被观察到（$R=1$ 为观测到，$R=0$ 为缺失）。缺失机制通常分为三类：[@problem_id:4955353]

-   **[完全随机缺失](@entry_id:170286) (Missing Completely At Random, MCAR)**：缺失的概率与任何变量（包括结果 $Y$ 本身和所有协变量 $X$）都无关。形式化地， $R \perp (Y, X)$。在这种理想情况下，仅使用完整数据（即 $R=1$ 的受试者）进行分析（称为**完整案例分析**）得到的结果是无偏的。

-   **[随机缺失](@entry_id:168632) (Missing At Random, MAR)**：缺失的概率可能依赖于**观测到**的协变量 $X$，但在给定 $X$ 的条件下，与**未观测到**的结果 $Y$ 本身无关。形式化地，$R \perp Y | X$。在这种情况下，简单的完整案例分析通常是有偏的，因为完整案例子集不再是总体的随机样本。然而，由于缺失机制可以由观测数据 $X$ 来解释，我们可以通过分层、加权等方法得到无偏估计，前提是满足正性假设（即在每个 $X$ 的层级中都有观测到 $Y$ 的个体）。

-   **[非随机缺失](@entry_id:163489) (Missing Not At Random, MNAR)**：缺失的概率即使在控制了所有观测到的协变量 $X$ 之后，仍然依赖于结果 $Y$ 本身。形式化地，$R \not\perp Y | X$。例如，病情更严重的患者可能更不愿意完成随访，导致他们的结果数据缺失。在这种情况下，缺失的信息无法从观测数据中恢复，任何分析都依赖于无法被数据检验的额外假设。

#### 聚集性数据

在许多研究设计中，数据天然地以“簇”（cluster）的形式出现，例如，来自同一临床中心的多名患者，或来自同一家庭的多名成员。同一簇内的个体往往比来自不同簇的个体更相似，这种现象导致了数据间的相关性。

这种相关性由**组内[相关系数](@entry_id:147037) (Intracluster Correlation Coefficient, ICC)**，记为 $\rho$，来量化。$\rho$ 衡量了来自同一簇的两个随机选择的个体其结果之间的相关程度。当 $\rho > 0$ 时，忽略这种聚集性会带来严重后果。

考虑一个均衡设计，有 $K$ 个簇，每个簇有 $n$ 个受试者。对于一个[二元结果](@entry_id:173636)，样本比例 $\hat{p}$ 的方差不再是简单的 $\frac{p(1-p)}{Kn}$。由于簇内相关性，真实的方差被放大了。这个放大因子被称为**设计效应 (Design Effect, DEFF)**，其表达式为：
$$ \text{DEFF} = 1 + (n-1)\rho $$
因此，聚集性数据的真实方差为：
$$ \text{Var}(\hat{p}) = \frac{p(1-p)}{Kn} \times [1 + (n-1)\rho] $$
例如，在一个有 $n=18$ 个受试者/簇的研究中，如果ICC为 $\rho = 0.073$，那么设计效应为 $1 + (18-1) \times 0.073 \approx 2.24$。这意味着，由于聚集性，[方差膨胀](@entry_id:756433)到了原来的2.24倍，或者说，有效样本量远小于总样本量 $Kn$。在总结这[类数](@entry_id:156164)据或进行统计推断时，必须使用能够处理这种相关性的专门方法（如广义估计方程 GEE 或混合效应模型）。[@problem_id:4955383]

此外，对于大小不等的簇，**主体层面**的汇总（按每个簇的大小进行加权平均）和**簇层面**的汇总（对每个簇的比例进行简单平均）会得出不同的结果。前者反映了所有个体的平均状况，而后者则赋予每个簇平等的权重。选择哪种汇总方式取决于研究问题的具体目标。[@problem_id:4955383]