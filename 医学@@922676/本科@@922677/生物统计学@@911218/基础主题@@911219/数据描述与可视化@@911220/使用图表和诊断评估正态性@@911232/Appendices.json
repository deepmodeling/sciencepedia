{"hands_on_practices": [{"introduction": "许多统计方法都假设数据服从正态分布，但真实世界的生物统计数据往往是偏态的。Box-Cox变换是一种强大的工具，可以通过对数据进行幂变换来使其更接近正态分布。本练习将指导您完成一个完整的数据分析流程：通过最大化似然函数来自动寻找最佳的变换参数 $\\hat{\\lambda}$，对数据进行变换，然后使用定量指标（Q-Q相关系数）和正式的假设检验（Shapiro-Wilk检验）来评估变换后数据的正态性 [@problem_id:4894199]。这个过程是数据预处理中一项至关重要的实践技能。", "problem": "给定四个严格为正的生物标志物数据集。您的任务是实现一个端到端的正态性评估流程，该流程对每个数据集计算最优的 Box–Cox 变换参数 $\\hat{\\lambda}$（通过最大化正态剖面对数似然），使用估计的 $\\hat{\\lambda}$ 对数据进行变换，然后使用分位数-分位数（Q–Q, quantile–quantile）相关性摘要和 Shapiro–Wilk（SW, Shapiro–Wilk）检验来评估正态性。\n\n基本定义与假设：\n- 对于严格为正的测量值 $x_1,\\dots,x_n$，Box–Cox 变换定义为\n$$\nT_{\\lambda}(x)=\n\\begin{cases}\n\\dfrac{x^{\\lambda}-1}{\\lambda},  \\lambda \\neq 0,\\\\\n\\log(x),  \\lambda = 0.\n\\end{cases}\n$$\n- 假设存在一个 $\\lambda$，使得 $Z_i = T_{\\lambda}(x_i)$ 作为均值为 $\\mu$、方差为 $\\sigma^2$ 的正态分布，是独立同分布的。\n- 在此模型下，最优的 $\\hat{\\lambda}$ 是关于 $\\lambda$ 的正态剖面对数似然的最大化者。\n\n在不绘图的情况下量化 Q–Q 线性度：\n- 令 $z_{(1)} \\le \\dots \\le z_{(n)}$ 表示变换后样本 $z_i = T_{\\hat{\\lambda}}(x_i)$ 的顺序统计量。\n- 定义 Blom 标绘点 $p_i = \\dfrac{i - 0.375}{n + 0.25}$（对于 $i=1,\\dots,n$），并令 $q_i = \\Phi^{-1}(p_i)$，其中 $\\Phi^{-1}$ 是标准正态分布的反累积分布函数。\n- 计算向量 $(z_{(1)},\\dots,z_{(n)})$ 和 $(q_1,\\dots,q_n)$ 之间的 Pearson 相关性；将此相关性记为 $r_{\\text{QQ}}$。$r_{\\text{QQ}}$ 的值越接近 1，表示 Q–Q 关系中的线性度越强。\n\n正态性检验：\n- 对变换后的数据 $\\{z_i\\}$ 应用 Shapiro–Wilk 检验并记录检验的 $p$ 值。使用显著性水平 $\\alpha = 0.05$，如果 $p$ 值大于或等于 $\\alpha$，则声明为“正态”。\n\n您的程序必须：\n- 对于每个数据集，通过在有界区间上最大化关于 $\\lambda$ 的正态剖面对数似然来计算 $\\hat{\\lambda}$，用 $T_{\\hat{\\lambda}}$ 变换数据，计算 $r_{\\text{QQ}}$，计算 Shapiro–Wilk 的 $p$ 值，最后使用 $\\alpha = 0.05$ 报告一个布尔值的正态性决策。\n- 将每个实值输出（$\\hat{\\lambda}$，$p$ 值，$r_{\\text{QQ}}$）四舍五入到 4 位小数。\n\n测试套件（数据集）：\n所有数据集均使用 $n = 30$ 个点，其中 $i \\in \\{1,\\dots,30\\}$ 且 $p_i^{(u)} = \\dfrac{i - 0.5}{n}$。令 $\\Phi^{-1}$ 表示标准正态分布的反累积分布函数，令 $G^{-1}_{k,\\theta}$ 表示形状参数为 $k$、尺度参数为 $\\theta$ 的伽马分布的反累积分布函数。\n\n- 数据集 $\\mathcal{D}_1$（类对数正态）：$x_i^{(1)} = \\exp\\!\\big(2.0 + 0.5 \\cdot \\Phi^{-1}(p_i^{(u)})\\big)$。\n- 数据集 $\\mathcal{D}_2$（近似正态但为正）：$x_i^{(2)} = 10.0 + 1.2 \\cdot \\Phi^{-1}(p_i^{(u)})$。\n- 数据集 $\\mathcal{D}_3$（类伽马分布偏态）：$x_i^{(3)} = G^{-1}_{2.0,\\,3.0}(p_i^{(u)})$。\n- 数据集 $\\mathcal{D}_4$（小数值正数）：$x_i^{(4)} = 0.02 + \\exp\\!\\big(-2.0 + 0.5 \\cdot \\Phi^{-1}(p_i^{(u)})\\big)$。\n\n输出规范：\n- 对于每个数据集 $\\mathcal{D}_j$（其中 $j \\in \\{1,2,3,4\\}$），返回一个列表 $[\\hat{\\lambda}_j, p\\_j, r_{\\text{QQ},j}, \\text{normal}_j]$，其中 $\\hat{\\lambda}_j$ 是估计的 Box–Cox 参数，$p\\_j$ 是变换后数据的 Shapiro–Wilk $p$ 值，$r_{\\text{QQ},j}$ 是 Q–Q 相关性，而 $\\text{normal}_j$ 是一个布尔值，如果 $p\\_j \\ge 0.05$ 则为真，否则为假。\n- 将 $\\hat{\\lambda}_j$、$p\\_j$ 和 $r_{\\text{QQ},j}$ 四舍五入到 4 位小数。\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。该行必须具有以下格式：\n$[\\,[\\hat{\\lambda}_1,p_1,r_{\\text{QQ},1},\\text{normal}_1],[\\hat{\\lambda}_2,p_2,r_{\\text{QQ},2},\\text{normal}_2],[\\hat{\\lambda}_3,p_3,r_{\\text{QQ},3},\\text{normal}_3],[\\hat{\\lambda}_4,p_4,r_{\\text{QQ},4},\\text{normal}_4]\\,]$.", "solution": "用户提供了一个有效的、定义明确的生物统计学问题。任务是构建并应用一个统计流程，以评估四个给定数据集的正态性，其中包含一个初步的 Box-Cox 变换步骤以改善正态性。解决方案首先详细阐述所需方法的理论基础，然后通过计算实现它们。\n\n### 1. 理论框架\n\n#### 1.1. Box-Cox 变换与正态性假设\n给定一组严格为正的数据点 $x_1, x_2, \\dots, x_n$，Box-Cox 变换是一个由 $\\lambda$ 参数化的幂变换。其定义为：\n$$\nT_{\\lambda}(x) =\n\\begin{cases}\n\\frac{x^{\\lambda}-1}{\\lambda},  \\text{if } \\lambda \\neq 0, \\\\\n\\log(x),  \\text{if } \\lambda = 0.\n\\end{cases}\n$$\n$\\lambda=0$ 的情况是当 $\\lambda \\to 0$ 时 $\\lambda \\neq 0$ 表达式的极限，这可以使用 L'Hôpital's rule 来证明。该程序的核心假设是，对于某个 $\\lambda$ 值，变换后的变量 $Z_i = T_{\\lambda}(x_i)$ 是来自正态分布 $N(\\mu, \\sigma^2)$ 的独立同分布（i.i.d.）样本。\n\n#### 1.2. 最优参数 $\\hat{\\lambda}$ 的估计\n最优参数 $\\hat{\\lambda}$ 是通过最大似然法确定的。原始数据 $x_i$ 的概率密度函数可以通过使用变量替换公式从变换后数据 $z_i$ 的密度中找到。给定参数 $\\mu$、$\\sigma^2$ 和 $\\lambda$ 时，原始观测值 $\\mathbf{x} = (x_1, \\dots, x_n)$ 的似然函数为：\n$$\nL(\\mu, \\sigma^2, \\lambda | \\mathbf{x}) = \\left( \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(T_\\lambda(x_i) - \\mu)^2}{2\\sigma^2}\\right) \\right) \\cdot J(\\lambda; \\mathbf{x})\n$$\n项 $J(\\lambda; \\mathbf{x})$ 是从 $\\mathbf{x}$ 到 $\\mathbf{z}$ 变换的雅可比行列式：\n$$\nJ(\\lambda; \\mathbf{x}) = \\prod_{i=1}^{n} \\left| \\frac{d T_\\lambda(x_i)}{d x_i} \\right| = \\prod_{i=1}^{n} x_i^{\\lambda-1}\n$$\n因此，对数似然函数为：\n$$\n\\ell(\\mu, \\sigma^2, \\lambda | \\mathbf{x}) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} (T_\\lambda(x_i) - \\mu)^2 + (\\lambda-1)\\sum_{i=1}^{n} \\log(x_i)\n$$\n对于一个固定的 $\\lambda$，$\\mu$ 和 $\\sigma^2$ 的最大似然估计（MLEs）是变换后数据 $z_i(\\lambda) = T_\\lambda(x_i)$ 的样本均值和方差：\n$$\n\\hat{\\mu}(\\lambda) = \\frac{1}{n} \\sum_{i=1}^{n} z_i(\\lambda) = \\bar{z}(\\lambda)\n$$\n$$\n\\hat{\\sigma}^2(\\lambda) = \\frac{1}{n} \\sum_{i=1}^{n} (z_i(\\lambda) - \\bar{z}(\\lambda))^2\n$$\n将这些代回到对数似然函数中，得到关于 $\\lambda$ 的剖面对数似然：\n$$\n\\ell_p(\\lambda | \\mathbf{x}) = -\\frac{n}{2}\\log(2\\pi) -\\frac{n}{2}\\log(\\hat{\\sigma}^2(\\lambda)) - \\frac{n}{2} + (\\lambda-1)\\sum_{i=1}^{n} \\log(x_i)\n$$\n为了找到最优的 $\\hat{\\lambda}$，我们相对于 $\\lambda$ 最大化 $\\ell_p(\\lambda | \\mathbf{x})$。由于项 $-\\frac{n}{2}\\log(2\\pi)$ 和 $-\\frac{n}{2}$ 相对于 $\\lambda$ 是常数，这等价于最大化简化后的函数：\n$$\nL_{\\text{profile}}(\\lambda) = -\\frac{n}{2}\\log(\\hat{\\sigma}^2(\\lambda)) + (\\lambda-1)\\sum_{i=1}^{n} \\log(x_i)\n$$\n这个最大化将通过最小化该函数的负数，在一个有界区间（例如 $\\lambda \\in [-5, 5]$）上进行数值计算。\n\n#### 1.3. 变换后数据的正态性评估\n\n一旦找到 $\\hat{\\lambda}$，数据就被变换为 $z_i = T_{\\hat{\\lambda}}(x_i)$。然后使用两种方法评估这个新样本 $\\{z_i\\}$ 的正态性。\n\n**分位数-分位数（Q-Q）相关性，$r_{\\text{QQ}}$：**\n这个度量标准量化了 Q-Q 图的线性度。它涉及将排序后的数据与标准正态分布的理论分位数进行比较。\n1. 计算变换后数据的顺序统计量：$z_{(1)} \\le z_{(2)} \\le \\dots \\le z_{(n)}$。\n2. 计算 Blom 标绘点：$p_i = \\frac{i - 0.375}{n + 0.25}$，对于 $i=1, \\dots, n$。\n3. 从标准正态分布中确定相应的理论分位数：$q_i = \\Phi^{-1}(p_i)$，其中 $\\Phi^{-1}$ 是 $N(0, 1)$ 的反累积分布函数（CDF）。\n4. 计算顺序统计量向量 $\\mathbf{z}_{\\text{ord}} = (z_{(1)}, \\dots, z_{(n)})$ 和理论分位数向量 $\\mathbf{q} = (q_1, \\dots, q_n)$ 之间的 Pearson 相关系数。此相关性记为 $r_{\\text{QQ}}$。$r_{\\text{QQ}}$ 的值接近 1 表示存在强线性关系，支持正态性假设。\n\n**Shapiro-Wilk (SW) 检验：**\nShapiro-Wilk 检验是一种用于检验正态性的正式假设检验。零假设（$H_0$）是数据样本来自一个正态分布的总体。计算检验统计量并将其转换为一个 $p$ 值。\n- 如果 $p$ 值小于选定的显著性水平 $\\alpha$（此处为 $\\alpha=0.05$），则拒绝 $H_0$，数据被认为不服从正态分布。\n- 如果 $p$ 值大于或等于 $\\alpha$，则没有足够的证据拒绝 $H_0$，为了本问题的目的，数据被声明为与正态分布一致。\n\n### 2. 计算步骤\n\n对提供的四个数据集中的每一个执行以下步骤：\n\n1.  **生成数据集**：根据指定的公式为每个数据集 $\\mathcal{D}_j$ 生成数据点。\n2.  **估计 $\\hat{\\lambda}$**：将剖面对数似然的负数 $-L_{\\text{profile}}(\\lambda)$ 定义为目标函数。使用数值优化例程（`scipy.optimize.minimize_scalar`）在如 $[-5, 5]$ 的有界区间内找到使该函数最小化的值 $\\hat{\\lambda}$。\n3.  **变换数据**：使用估计的 $\\hat{\\lambda}$ 将原始数据集 $x_i$ 变换为 $z_i = T_{\\hat{\\lambda}}(x_i)$。\n4.  **计算 $r_{\\text{QQ}}$**：如第 1.3 节所述，为变换后的数据 $\\{z_i\\}$ 计算 Q-Q 相关性。\n5.  **执行 Shapiro-Wilk 检验**：对变换后的数据 $\\{z_i\\}$ 应用 Shapiro-Wilk 检验以获得 $p$ 值。\n6.  **报告结果**：基于 $p$ 值是否 $\\ge 0.05$ 做出关于正态性的布尔决策。收集数据集的最终结果——$\\hat{\\lambda}$、SW $p$ 值、$r_{\\text{QQ}}$ 和正态性决策。实数值四舍五入到四位小数。\n7.  **最终输出**：将所有四个数据集的收集结果组合成一个列表的列表，并以指定格式打印。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats, optimize\n\ndef solve():\n    \"\"\"\n    Main function to perform normality assessment on four datasets.\n    \"\"\"\n\n    def generate_datasets():\n        \"\"\"Generates the four datasets as specified in the problem.\"\"\"\n        n = 30\n        i = np.arange(1, n + 1)\n        p_u = (i - 0.5) / n\n        norm_quantiles = stats.norm.ppf(p_u)\n\n        # Dataset D1 (log-normal-like)\n        d1 = np.exp(2.0 + 0.5 * norm_quantiles)\n\n        # Dataset D2 (approximately Normal but positive)\n        d2 = 10.0 + 1.2 * norm_quantiles\n\n        # Dataset D3 (Gamma-like skew)\n        gamma_quantiles = stats.gamma.ppf(p_u, a=2.0, scale=3.0)\n        d3 = gamma_quantiles\n\n        # Dataset D4 (small-magnitude positive)\n        d4 = 0.02 + np.exp(-2.0 + 0.5 * norm_quantiles)\n\n        return [d1, d2, d3, d4]\n\n    def boxcox_transform(x, lam):\n        \"\"\"Applies the Box-Cox transformation.\"\"\"\n        if lam == 0:\n            return np.log(x)\n        else:\n            return (x**lam - 1) / lam\n\n    def profile_log_likelihood(lam, x):\n        \"\"\"\n        Computes the profile log-likelihood for a given lambda.\n        Note: The function returns the *negative* for minimization purposes.\n        \"\"\"\n        n = len(x)\n        z = boxcox_transform(x, lam)\n        # Using n-ddof=0 for MLE of variance (division by n)\n        log_lik = -n / 2 * np.log(np.var(z, ddof=0)) + (lam - 1) * np.sum(np.log(x))\n        return -log_lik\n\n    def analyze_dataset(x):\n        \"\"\"\n        Performs the full analysis pipeline for a single dataset.\n        \"\"\"\n        # 1. Compute optimal lambda\n        # We use a bounded search for robustness, as is common practice.\n        res = optimize.minimize_scalar(\n            profile_log_likelihood, \n            args=(x,), \n            bounds=(-5, 5), \n            method='bounded'\n        )\n        lambda_hat = res.x\n\n        # 2. Transform the data\n        z_transformed = boxcox_transform(x, lambda_hat)\n\n        # 3. Compute Q-Q correlation (r_QQ)\n        n = len(z_transformed)\n        z_ordered = np.sort(z_transformed)\n        i = np.arange(1, n + 1)\n        p_blom = (i - 0.375) / (n + 0.25)\n        q_theoretical = stats.norm.ppf(p_blom)\n        r_qq = np.corrcoef(z_ordered, q_theoretical)[0, 1]\n\n        # 4. Compute Shapiro-Wilk test p-value\n        _, p_value = stats.shapiro(z_transformed)\n\n        # 5. Make normality decision\n        alpha = 0.05\n        is_normal = p_value >= alpha\n\n        # 6. Format results\n        return [\n            round(lambda_hat, 4),\n            round(p_value, 4),\n            round(r_qq, 4),\n            is_normal\n        ]\n\n    datasets = generate_datasets()\n    all_results = [analyze_dataset(d) for d in datasets]\n    \n    # Final print statement in the exact required format.\n    # The str() representation of a list includes spaces after commas,\n    # which matches the visual style of the problem's output format example.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n\n```", "id": "4894199"}, {"introduction": "在统计软件中，我们通常只需一行代码就能调用正态性检验，但其背后的原理是什么？特别是当总体的均值和方差未知时，检验过程会变得更加复杂。Lilliefors检验（Kolmogorov-Smirnov检验的一种修正）正是为了解决这个问题而设计的，它通过蒙特卡洛模拟来计算p值。通过从零开始实现这个检验 [@problem_id:4894176]，您将不仅学会如何应用一个检验，更将深入理解拟合优度检验的本质、参数估计对假设检验的影响，以及模拟方法在现代统计学中的强大作用。", "problem": "您需要编写一个完整的、自包含的程序，通过计算单样本Kolmogorov-Smirnov (KS) 统计量并通过蒙特卡洛模拟近似Lilliefors p值，来评估位置和尺度参数未知的样本对正态分布的拟合优度。正态性评估是生物统计学中的一项核心诊断，当参数未知时，近似方法必须考虑到参数估计的步骤。\n\n请使用以下基础来设计您的方法：根据观测样本构建的经验累积分布函数、标准正态分布的累积分布函数，以及累积分布函数之间点态差异的上确界概念。当参数未知时，将它们视为讨厌参数，并使用从样本中导出的一致估计量。蒙特卡洛方法必须在正态性原假设下复现完全相同的估计过程，以近似这种复合情况下KS统计量的抽样分布。\n\n每个测试用例需要实现的任务：\n- 从输入样本中，使用除数为$(n-1)$的无偏样本标准差来估计未知的均值和标准差，其中$n$是样本大小。\n- 使用估计出的参数对数据进行标准化，并通过计算标准化数据的经验累积分布函数与标准正态分布的累积分布函数之间绝对差的上确界，来计算用于正态性检验的单样本Kolmogorov-Smirnov统计量。\n- 通过在原假设下进行蒙特卡洛模拟来近似Lilliefors p值。具体方法是：从标准正态分布中抽取$M$个独立样本，每个样本大小为$n$；对每个模拟样本使用相同的步骤重新估计均值和标准差；以与处理观测数据相同的方式为每个模拟样本重新计算KS统计量；最后计算模拟的KS统计量中大于或等于观测KS统计量的比例。这个比例就是以小数表示的蒙特卡洛p值。\n\n模拟可复现性要求：\n- 所有模拟均使用固定的伪随机数生成器种子$123456$，以确保不同运行间的输出具有确定性。\n\n最终输出要求：\n- 对每个测试用-例，输出一个列表$[D, p]$，其中$D$是KS统计量，$p$是近似的Lilliefors p值，两者都四舍五入到六位小数并以小数形式表示（无百分号）。\n- 您的程序应生成单行输出，其中包含所有测试用例的结果，形式为一个用方括号括起来的逗号分隔列表。预期的最终输出格式为$[[D_1,p_1],[D_2,p_2],[D_3,p_3]]$，其中每个$D_i$和$p_i$都四舍五入到六位小数。\n\n测试套件：\n- 案例A（中等样本，近似正态）：样本$x_A$，其中$n=30$，数据为$[8.6, 9.8, 12.1, 10.5, 7.9, 11.3, 9.2, 10.1, 12.5, 8.8, 9.9, 10.4, 11.0, 8.3, 10.9, 11.7, 9.0, 10.2, 12.0, 7.5, 8.1, 9.7, 10.8, 11.5, 9.4, 10.0, 12.3, 8.7, 9.1, 10.6]$，$M=4000$。\n- 案例B（偏态样本，非正态）：样本$x_B$，其中$n=25$，数据为$[5.1, 5.3, 5.4, 5.6, 5.7, 6.0, 6.2, 6.5, 7.0, 7.2, 7.6, 8.4, 5.2, 5.8, 5.9, 6.1, 6.3, 6.9, 7.5, 8.1, 9.0, 9.5, 10.8, 11.3, 12.7]$，$M=4000$。\n- 案例C（小样本，临界情况）：样本$x_C$，其中$n=8$，数据为$[-0.2, 0.1, -1.1, 0.7, 1.0, -0.5, 0.3, -0.8]$，$M=4000$。\n\n您的程序必须精确地产生一行格式为$[[D_1,p_1],[D_2,p_2],[D_3,p_3]]$的输出，其中每个浮点数都使用指定的种子四舍五入到六位小数，且无其他输出。此问题不涉及物理单位或角度；所有值都是无单位的，p值必须以小数形式表示。", "solution": "该问题要求实现用于正态性检验的Lilliefors检验，这是Kolmogorov-Smirnov (KS) 拟合优度检验的一种特定应用，其中假设的正态分布的参数（均值和标准差）是未知的，必须从数据中估计。由于参数的估计会改变统计量的原分布，因此检验统计量的p值需要通过蒙特卡洛模拟来近似。\n\n验证过程确认了该问题在科学上是合理的、提法得当、客观，并包含了获得唯一且可验证解所需的所有信息。它准确地描述了生物统计学及相关领域的标准流程。\n\n解决方案首先定义检验统计量，然后概述用于p值近似的蒙特卡洛方法。\n\n设观测样本为$X = \\{x_1, x_2, \\ldots, x_n\\}$。原假设$H_0$主张该样本来自一个正态分布$\\mathcal{N}(\\mu, \\sigma^2)$，其中均值$\\mu$和标准差$\\sigma$未知。\n\n第一步是从数据中估计这些讨厌参数。样本均值$\\bar{x}$作为$\\mu$的估计，无偏样本标准差$s$作为$\\sigma$的估计。它们的计算公式如下：\n$$ \\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i $$\n$$ s = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2} $$\n使用除数$n-1$可以提供对总体方差的无偏估计。\n\n接下来，使用这些估计值对观测数据进行标准化，以产生一组新的值$Z = \\{z_1, z_2, \\ldots, z_n\\}$：\n$$ z_i = \\frac{x_i - \\bar{x}}{s} $$\n在原假设下，如果$\\bar{x}$和$s$是真实的总体参数$\\mu$和$\\sigma$，那么标准化后的数据$Z$将服从标准正态分布$\\mathcal{N}(0, 1)$。\n\nLilliefors检验统计量$D$是在此标准化数据上计算的单样本KS统计量。它被定义为标准化样本的经验累积分布函数（ECDF），记为$F_n(z)$，与标准正态分布的累积分布函数（CDF），记为$\\Phi(z)$，之间绝对差的上确界。\n$$ D = \\sup_{z} |F_n(z) - \\Phi(z)| $$\n为了计算$D$，我们首先对标准化数据进行排序，得到顺序统计量$z_{(1)} \\le z_{(2)} \\le \\ldots \\le z_{(n)}$。ECDF $F_n(z)$是一个阶跃函数，在每个$z_{(i)}$处增加$1/n$。差值$|F_n(z) - \\Phi(z)|$的上确界将出现在某个观测数据点上。因此，可以使用以下公式高效地计算$D$值，该公式评估了ECDF在每个阶跃之前和之时的差值：\n$$ D = \\max_{i=1, \\ldots, n} \\left( \\max \\left( \\frac{i}{n} - \\Phi(z_{(i)}), \\Phi(z_{(i)}) - \\frac{i-1}{n} \\right) \\right) $$\n\n由于参数$\\mu$和$\\sigma$是从数据中估计的，因此在$H_0$下统计量$D$的分布与标准的KS分布不同。数据经过其自身样本矩的中心化和尺度变换后，往往比来自标准正态分布的真实随机样本更贴合正态分布。因此，Lilliefors检验的临界值小于标准KS检验的临界值。我们通过蒙特卡洛模拟来近似正确的原分布和相应的p值。\n\n蒙特卡洛过程如下：\n1.  计算原始观测数据的检验统计量，记为$D_{obs}$。\n2.  进行大量的重复， $M$次（此处，$M=4000$），执行以下步骤来模拟原假设：\n    a. 从标准正态分布$\\mathcal{N}(0, 1)$中生成一个大小为$n$的合成样本$\\{y_1, y_2, \\ldots, y_n\\}$。\n    b. 对此合成样本执行与观测数据完全相同的估计和标准化过程。计算其样本均值$\\bar{y}$和样本标准差$s_y$，然后将其标准化得到$\\{w_1, w_2, \\ldots, w_n\\}$，其中$w_j = (y_j - \\bar{y})/s_y$。\n    c. 计算此合成样本$\\{w_j\\}$的Lilliefors统计量$D_{sim}$。\n3.  此过程产生一个包含$M$个模拟统计量的集合$\\{D_{sim,1}, D_{sim,2}, \\ldots, D_{sim,M}\\}$。这个集合可作为样本大小为$n$时统计量$D$的原分布的经验近似。\n4.  然后，p值被估计为模拟统计量中大于或等于观测统计量的比例：\n    $$ p \\approx \\frac{\\sum_{j=1}^{M} \\mathbb{I}(D_{sim,j} \\ge D_{obs})}{M} $$\n其中$\\mathbb{I}(\\cdot)$是指示函数。为伪随机数生成器设置一个固定的种子可以确保此模拟的可复现性。\n\n实现将遵循此逻辑。将设计一个辅助函数来计算任何给定数据样本的$D$统计量。主程序将遍历所提供的测试用例。对于每个用例，它将计算$D_{obs}$，然后运行蒙特卡洛循环以生成原分布并计算p值。每个用例的最终结果$[D_{obs}, p]$将四舍五入到六位小数，并格式化为指定的输出字符串。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the Lilliefors test statistic for normality and its p-value via\n    Monte Carlo simulation for a series of test cases.\n    \"\"\"\n\n    def lilliefors_statistic(data):\n        \"\"\"\n        Calculates the Lilliefors statistic for a given data sample.\n\n        This involves:\n        1. Estimating the mean and standard deviation from the data.\n        2. Standardizing the data using these estimates.\n        3. Computing the Kolmogorov-Smirnov statistic against a standard normal distribution.\n\n        Args:\n            data (np.ndarray): A 1D array of sample data.\n\n        Returns:\n            float: The computed Lilliefors test statistic D.\n        \"\"\"\n        n = len(data)\n        if n == 0:\n            return 0.0\n\n        # Estimate parameters: sample mean and unbiased sample standard deviation\n        mu_hat = np.mean(data)\n        sigma_hat = np.std(data, ddof=1)\n\n        # Handle degenerate case of zero standard deviation\n        if sigma_hat == 0:\n            return 1.0\n\n        # Standardize the data and sort it\n        sorted_data = np.sort(data)\n        z_scores = (sorted_data - mu_hat) / sigma_hat\n\n        # Calculate the CDF of the standard normal distribution at each z-score\n        cdf_values = norm.cdf(z_scores)\n\n        # The ECDF steps are at i/n\n        i = np.arange(1, n + 1)\n\n        # Compute the statistic D = max(D+, D-)\n        # D+ = max_i (i/n - Phi(z_i))\n        # D- = max_i (Phi(z_i) - (i-1)/n)\n        d_plus = np.max(i / n - cdf_values)\n        d_minus = np.max(cdf_values - (i - 1) / n)\n\n        return np.max([d_plus, d_minus])\n\n    # Simulation reproducibility requirement\n    seed = 123456\n    rng = np.random.default_rng(seed)\n\n    # Test suite\n    test_cases = [\n        {\n            \"data\": np.array([8.6, 9.8, 12.1, 10.5, 7.9, 11.3, 9.2, 10.1, 12.5, 8.8, 9.9, 10.4, 11.0, 8.3, 10.9, 11.7, 9.0, 10.2, 12.0, 7.5, 8.1, 9.7, 10.8, 11.5, 9.4, 10.0, 12.3, 8.7, 9.1, 10.6]),\n            \"M\": 4000\n        },\n        {\n            \"data\": np.array([5.1, 5.3, 5.4, 5.6, 5.7, 6.0, 6.2, 6.5, 7.0, 7.2, 7.6, 8.4, 5.2, 5.8, 5.9, 6.1, 6.3, 6.9, 7.5, 8.1, 9.0, 9.5, 10.8, 11.3, 12.7]),\n            \"M\": 4000\n        },\n        {\n            \"data\": np.array([-0.2, 0.1, -1.1, 0.7, 1.0, -0.5, 0.3, -0.8]),\n            \"M\": 4000\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        observed_data = case[\"data\"]\n        n = len(observed_data)\n        M = case[\"M\"]\n\n        # 1. Compute the observed KS statistic\n        d_obs = lilliefors_statistic(observed_data)\n\n        # 2. Approximate p-value through Monte Carlo simulation\n        simulated_stats = np.zeros(M)\n        for i in range(M):\n            # a. Draw a sample of size n from N(0,1)\n            sim_sample = rng.normal(loc=0.0, scale=1.0, size=n)\n            # b, c, d. Calculate KS statistic for this simulated sample\n            simulated_stats[i] = lilliefors_statistic(sim_sample)\n\n        # 3. Compute the p-value\n        p_value = np.sum(simulated_stats >= d_obs) / M\n\n        # 4. Store rounded results\n        all_results.append([round(d_obs, 6), round(p_value, 6)])\n\n    # Final print statement in the exact required format\n    # Example: [[0.123456,0.123456],[0.234567,0.234567],[0.345678,0.345678]]\n    result_str = \"[\" + \",\".join([f\"[{d},{p}]\" for d, p in all_results]) + \"]\"\n    print(result_str)\n\nsolve()\n```", "id": "4894176"}, {"introduction": "在回归分析中，检查残差的正态性是一项标准的诊断步骤，但这远不止是例行公事。残差的非正态性往往不是问题本身，而是模型设定存在更深层次问题的“症状”，例如模型形式错误、误差方差不恒定（异方差性）或遗漏了重要的解释变量。本综合性模拟练习 [@problem_id:4894203] 让你扮演一名统计侦探的角色：您将亲眼见证不同的模型设定错误如何破坏残差的正态性，并学习如何应用加权最小二乘法（WLS）或稳健回归等修正方法来“治愈”模型，从而恢复统计推断的有效性。", "problem": "您必须编写一个完整、可运行的程序，该程序模拟多个生物统计学上真实的场景，以在模型设定错误的情况下评估模型残差的正态性，然后应用纠正措施。目标是使用源于经典线性模型第一性原理的诊断方法来评估残差的正态性，并就具体的纠正措施如何以及为何能改善正态性进行推理。\n\n请从以下核心定义和经过充分检验的事实出发。假设经典线性模型，其响应向量为 $y \\in \\mathbb{R}^n$，设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，参数向量为 $\\beta \\in \\mathbb{R}^p$，误差向量为 $\\varepsilon \\in \\mathbb{R}^n$，满足 $y = X \\beta + \\varepsilon$。残差定义为 $r = y - X \\hat{\\beta}$，其中 $\\hat{\\beta}$ 是使残差平方和最小化的最小二乘估计。在模型设定正确且误差独立同分布于正态分布的情况下，残差是正态向量的线性变换，因此也服从正态分布。偏离正态性的情况可能由均值函数设定错误、方差函数设定错误、重尾误差或遗漏变量引起。残差的正态性可以使用夏皮罗-威尔克（Shapiro-Wilk）检验进行评估，该检验在正态性原假设下提供一个 $p$ 值；标准化残差计算公式为 $z_i = r_i / \\hat{\\sigma}$，其中 $\\hat{\\sigma}^2 = \\sum_{i=1}^n r_i^2 / (n - p)$。\n\n您必须纯粹以数学和逻辑术语实现以下模拟协议：\n\n1. 对于每个测试用例，根据指定的数据生成过程生成合成数据，使用普通最小二乘法 (OLS) 拟合一个基准的设定错误模型，计算标准化残差，并计算这些残差的夏皮罗-威尔克（Shapiro-Wilk）检验的 $p$ 值。\n\n2. 根据模型设定错误的类型应用特定的纠正措施并重新拟合模型：\n   - 均值设定错误：使用适当的非线性项丰富均值函数。\n   - 方差设定错误：使用加权最小二乘法 (WLS)，其权重根据方差函数的理论推导得出。\n   - 重尾误差：使用带有 Huber 权重的迭代重加权最小二乘法 (IRLS)，以减少异常值和重尾的影响。\n   - 遗漏变量：包含遗漏的预测变量以正确设定均值函数。\n\n3. 纠正后重新计算标准化残差和夏皮罗-威尔克（Shapiro-Wilk）检验的 $p$ 值。报告基准和纠正后的 $p$ 值及其差异，以量化改善程度。\n\n您的程序必须通过正规方程从第一性原理实现 OLS 和 WLS。对于带 Huber 权重的 IRLS，请使用由权重 $w_i = 1$（如果 $|r_i| \\leq k s$）和 $w_i = k s / |r_i|$（其他情况）定义的 Huber 影响函数，其中 $s$ 是一个稳健的尺度参数，$k$ 是一个调节常数。使用无偏残差标准差 $\\hat{\\sigma} = \\sqrt{\\sum r_i^2 / (n - p)}$ 来标准化残差。\n\n测试套件规范：\n您必须实现以下五个测试用例，每个用例的参数涵盖不同方面（一般情况、边界情况和边缘情况）。所有随机性必须使用固定的种子以确保可复现性。此问题不涉及任何物理单位。\n\n- 案例 1 (正确设定，理想路径)：生成 $n = 200$ 个观测值，其中 $x \\sim \\mathrm{Uniform}(-3, 3)$，参数 $\\beta_0 = 2$, $\\beta_1 = 0.5$，同方差误差 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ 且 $\\sigma = 1$。拟合正确的线性模型 $y = \\beta_0 + \\beta_1 x + \\varepsilon$。使用种子 $314159$。\n\n- 案例 2 (均值设定错误)：生成 $n = 200$ 个观测值，其中 $x \\sim \\mathrm{Uniform}(-2, 2)$，参数 $\\beta_0 = 1$, $\\beta_1 = 1$, $\\beta_2 = 0.8$，$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ 且 $\\sigma = 1$。真实均值为 $y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\varepsilon$，但拟合时省略 $x^2$ 的基准线性模型。纠正措施：包含 $x^2$。使用种子 $271828$。\n\n- 案例 3 (方差设定错误，异方差性)：生成 $n = 300$ 个观测值，其中 $x \\sim \\mathrm{Uniform}(0, 3)$，参数 $\\beta_0 = 0$, $\\beta_1 = 1.5$，基础尺度 $\\sigma_0 = 0.5$，异方差方差 $\\mathrm{Var}(\\varepsilon \\mid x) = \\sigma_0^2 (1 + c x^2)$ 且 $c = 2$。拟合基准 OLS，并通过 WLS 进行纠正，权重为 $w_i = 1 / (1 + c x_i^2)$。使用种子 $161803$。\n\n- 案例 4 (重尾误差，边缘情况)：生成 $n = 250$ 个观测值，其中 $x \\sim \\mathrm{Uniform}(-2, 2)$，参数 $\\beta_0 = 0$, $\\beta_1 = 1$，自由度 $df = 3$，尺度 $\\sigma = 1$。误差 $\\varepsilon$ 来自自由度为 $df$ 的学生t分布，并经过缩放以使方差为 $\\sigma^2$（即，将标准 $t$ 分布乘以 $\\sqrt{(df - 2)/df} \\cdot \\sigma$）。基准模型使用 OLS；纠正措施使用带 Huber 权重和调节常数 $k = 1.345$ 的 IRLS。使用种子 $101$。\n\n- 案例 5 (遗漏变量，相关预测变量)：生成 $n = 200$ 个观测值，其中 $x_1 \\sim \\mathrm{Uniform}(-2, 2)$ 且 $x_2 = \\rho x_1 + \\eta$，其中 $\\rho = 0.7$ 且 $\\eta \\sim \\mathcal{N}(0, \\sigma_\\eta^2)$ 且 $\\sigma_\\eta = 0.3$。参数为 $\\beta_0 = 0$, $\\beta_1 = 1$, $\\beta_2 = 1$，同方差 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ 且 $\\sigma = 0.5$。真实均值为 $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon$，但拟合时省略 $x_2$ 的基准模型。纠正措施：包含 $x_2$。使用种子 $2024$。\n\n对于每个案例，计算：\n- 基准标准化残差的夏皮罗-威尔克（Shapiro-Wilk）$p$ 值，\n- 纠正后标准化残差的夏皮罗-威尔克（Shapiro-Wilk）$p$ 值，\n- 差值 $d = p_{\\mathrm{corrected}} - p_{\\mathrm{baseline}}$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。列表中的每个元素是包含三个浮点数 $[p_{\\mathrm{baseline}}, p_{\\mathrm{corrected}}, d]$ 的列表，顺序与测试用例相同，不含空格。例如，生成形如 $[[p_{1b},p_{1c},d_1],[p_{2b},p_{2c},d_2],\\dots,[p_{5b},p_{5c},d_5]]$ 的输出。\n\n所有输出必须是浮点数。此问题不涉及物理单位、角度单位或百分比，因此不需要进行单位转换。", "solution": "评估统计模型的有效性问题是所有经验科学学科的核心。经典线性模型的一项关键诊断程序是检查残差，即观测数据与模型预测值之间的差异。在模型的理想假设下——具体来说，即误差独立且同分布于正态分布——模型的残差也应呈正态分布。残差中的正态性偏离通常表明模型的一个或多个基本假设已被违反。本模拟研究旨在展示不同类型的模型设定错误如何表现为残差的非正态性，以及有针对性的纠正措施如何能够恢复正态性（通过夏皮罗-威尔克检验来衡量）。\n\n经典线性模型的一般形式由以下方程给出：\n$$ y = X \\beta + \\varepsilon $$\n其中 $y$ 是观测值的 $n \\times 1$ 向量，$X$ 是预测变量的 $n \\times p$ 设计矩阵（包括一个截距项），$\\beta$ 是未知参数的 $p \\times 1$ 向量，而 $\\varepsilon$ 是不可观测的随机误差的 $n \\times 1$ 向量。我们假设 $\\mathrm{E}[\\varepsilon] = 0$ 且 $\\mathrm{Var}(\\varepsilon) = \\sigma^2 I_n$，其中 $I_n$ 是 $n \\times n$ 的单位矩阵。\n\n$\\beta$ 的普通最小二乘 (OLS) 估计量（记作 $\\hat{\\beta}$）是通过最小化残差平方和 $\\|y - X\\beta\\|^2$ 得到的。这产生了著名的正规方程解：\n$$ \\hat{\\beta}_{OLS} = (X^T X)^{-1} X^T y $$\n然后，残差计算为 $r = y - X \\hat{\\beta}$。为诊断目的，通常使用标准化残差，本问题将其定义为 $z_i = r_i / \\hat{\\sigma}$，其中 $\\hat{\\sigma}$ 是误差标准差的无偏估计量：\n$$ \\hat{\\sigma} = \\sqrt{\\frac{\\sum_{i=1}^n r_i^2}{n - p}} $$\n此处，$n$ 是观测数量，$p$ 是参数数量（即 $X$ 中的列数）。这些标准化残差的正态性使用夏皮罗-威尔克（Shapiro-Wilk）检验进行评估，该检验的原假设是数据来自正态分布。一个小的 $p$ 值（通常  0.05）提供了反对正态性的证据。\n\n该模拟协议处理了几种常见的模型设定错误：\n\n$1$. **均值设定错误**：预测变量与响应之间关系的函数形式不正确。例如，当真实关系为二次方时，却拟合了线性关系。这种设定错误通常会在残差中引入系统性模式，可能导致其分布偏离正态性。纠正措施是通过向设计矩阵 $X$ 添加适当的非线性项（例如 $x^2$）来丰富模型。\n\n$2$. **方差设定错误（异方差性）**：误差方差恒定（同方差性）的假设被违反。误差的方差取决于预测变量的值。OLS 估计量虽然仍是无偏的，但不再是最佳线性无偏估计量 (BLUE)。残差将不具有恒定的方差，这可能导致正态性检验失败。纠正措施是使用加权最小二乘法 (WLS)，它在这种情况下是 BLUE。WLS 估计量为：\n$$ \\hat{\\beta}_{WLS} = (X^T W X)^{-1} X^T W y $$\n其中 $W$ 是一个权重对角矩阵，$W = \\mathrm{diag}(w_1, w_2, \\dots, w_n)$，每个权重 $w_i$ 与该观测的误差方差成反比，即 $w_i \\propto 1/\\mathrm{Var}(\\varepsilon_i)$。\n\n$3$. **重尾误差**：误差来自比正态分布具有更重尾部的分布，例如自由度较小的学生t分布。这会导致异常值的出现，这些异常值会严重影响 OLS 拟合结果，并导致残差分布非正态。纠正措施涉及稳健回归，特别是迭代重加权最小二乘法 (IRLS)。IRLS 是一种迭代执行加权最小二乘法的算法，其中权重由前一次迭代的残差确定。此过程会降低具有大残差（异常值）的观测值的影响。对于 Huber M-估计，权重定义为：\n$$ w_i = \\begin{cases} 1  \\text{if } |r_i| \\le k s \\\\ \\frac{k s}{|r_i|}  \\text{if } |r_i| > k s \\end{cases} $$\n这里，$k$ 是一个调节常数（指定为 $k=1.345$，以在正态数据上达到约 $95\\%$ 的效率），$s$ 是对残差尺度的稳健估计。一个常见的选择是采用初始 OLS 残差的缩放后的中位数绝对偏差 (MAD)：$s = \\text{median}(|r_{\\text{initial}}|) / 0.6745$。迭代过程从 OLS 拟合开始，一直持续到估计的系数收敛为止。\n\n$4$. **遗漏变量**：一个相关的预测变量被排除在模型之外。如果遗漏的变量与模型中包含的预测变量相关，那么所包含变量的估计系数将是有偏的（遗漏变量偏差）。这种偏差会传递到残差中，残差将包含缺失变量的影响，通常导致非正态性。直接的纠正措施是将遗漏的变量包含在模型设定中。\n\n算法的流程是为五个指定的测试用例中的每一个模拟数据。对于每个案例，它首先拟合规定的基准（设定错误的）模型，计算标准化残差，并计算夏皮罗-威尔克 $p$ 值（$p_{\\mathrm{baseline}}$）。然后，它应用指定的纠正措施，重新拟合模型，并重新计算标准化残差及相应的夏皮罗-威尔克 $p$ 值（$p_{\\mathrm{corrected}}$）。差值 $d = p_{\\mathrm{corrected}} - p_{\\mathrm{baseline}}$ 量化了残差正态性的改善程度。所有计算，包括 OLS、WLS 和 IRLS，均按照问题要求从第一性原理实现。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef fit_ols(X, y):\n    \"\"\"\n    Fits an Ordinary Least Squares (OLS) model using the normal equations.\n    \"\"\"\n    try:\n        # beta_hat = (X'X)^-1 * X'y\n        XtX_inv = np.linalg.inv(X.T @ X)\n        XtY = X.T @ y\n        beta = XtX_inv @ XtY\n        return beta\n    except np.linalg.LinAlgError:\n        return np.linalg.lstsq(X, y, rcond=None)[0]\n\ndef fit_wls(X, y, w):\n    \"\"\"\n    Fits a Weighted Least Squares (WLS) model.\n    \"\"\"\n    W = np.diag(w)\n    try:\n        # beta_hat = (X'WX)^-1 * X'Wy\n        XtWX_inv = np.linalg.inv(X.T @ W @ X)\n        XtWY = X.T @ W @ y\n        beta = XtWX_inv @ XtWY\n        return beta\n    except np.linalg.LinAlgError:\n        # Fallback for singular matrix\n        sqrt_W = np.sqrt(W)\n        return np.linalg.lstsq(sqrt_W @ X, sqrt_W @ y, rcond=None)[0]\n\ndef fit_irls_huber(X, y, k=1.345, max_iter=100, tol=1e-6):\n    \"\"\"\n    Fits a robust regression model using Iteratively Reweighted Least Squares (IRLS)\n    with Huber T weights.\n    \"\"\"\n    # 1. Initial fit\n    beta = fit_ols(X, y)\n    \n    # 2. Calculate robust scale s (MAD, scaled for asymptotic normality)\n    residuals_initial = y - X @ beta\n    # MAD is median(|res - median(res)|). median(res) is close to 0 for a good fit.\n    mad = np.median(np.abs(residuals_initial - np.median(residuals_initial)))\n    s = mad / 0.6745  # Scale for consistency\n    if s  1e-10: s = 1.0 # Protect against zero scale\n\n    for _ in range(max_iter):\n        beta_old = beta\n        \n        # 3. Compute residuals\n        r = y - X @ beta\n        \n        # 4. Compute Huber weights\n        abs_r = np.abs(r)\n        weights = np.ones_like(r)\n        idx = abs_r > k * s\n        weights[idx] = (k * s) / abs_r[idx]\n        \n        # 5. WLS step\n        beta = fit_wls(X, y, weights)\n        \n        # 6. Check for convergence\n        if np.linalg.norm(beta - beta_old)  tol:\n            break\n            \n    return beta\n\ndef calculate_diagnostics(X, y, beta):\n    \"\"\"\n    Calculates residuals, standardized residuals, and Shapiro-Wilk p-value.\n    \"\"\"\n    n, p = X.shape\n    r = y - X @ beta\n    \n    # Unbiased estimate of error variance\n    sigma_sq_hat = np.sum(r**2) / (n - p)\n    sigma_hat = np.sqrt(sigma_sq_hat)\n    \n    # Avoid division by zero if residuals are all zero\n    if sigma_hat  1e-10:\n        return 1.0 # Perfect fit, residuals are constant, not normal, but p-value=1 is reasonable\n    \n    # Standardized residuals as per problem statement\n    z = r / sigma_hat\n    \n    # Shapiro-Wilk test\n    if len(z)  3: return np.nan # Shapiro-Wilk needs at least 3 samples\n    p_value = stats.shapiro(z).pvalue\n    return p_value\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases_params = [\n        # Case 1: Correct specification\n        {'seed': 314159, 'n': 200},\n        # Case 2: Mean misspecification\n        {'seed': 271828, 'n': 200},\n        # Case 3: Variance misspecification\n        {'seed': 161803, 'n': 300},\n        # Case 4: Heavy-tailed errors\n        {'seed': 101, 'n': 250},\n        # Case 5: Omitted variable\n        {'seed': 2024, 'n': 200},\n    ]\n\n    results = []\n\n    # --- Case 1: Correct Specification ---\n    spec = test_cases_params[0]\n    rng = np.random.default_rng(spec['seed'])\n    n = spec['n']\n    x = rng.uniform(-3, 3, n)\n    X_base = np.c_[np.ones(n), x]\n    beta_true = np.array([2, 0.5])\n    eps = rng.normal(0, 1, n)\n    y = X_base @ beta_true + eps\n    \n    # Baseline (which is also the correct model here)\n    beta_base = fit_ols(X_base, y)\n    p_base = calculate_diagnostics(X_base, y, beta_base)\n    \n    # Correction (same model, so same result)\n    p_corr = p_base\n    results.append([p_base, p_corr, p_corr - p_base])\n\n    # --- Case 2: Mean Misspecification ---\n    spec = test_cases_params[1]\n    rng = np.random.default_rng(spec['seed'])\n    n = spec['n']\n    x = rng.uniform(-2, 2, n)\n    X_true = np.c_[np.ones(n), x, x**2]\n    beta_true = np.array([1, 1, 0.8])\n    eps = rng.normal(0, 1, n)\n    y = X_true @ beta_true + eps\n    \n    # Baseline\n    X_base = np.c_[np.ones(n), x]\n    beta_base = fit_ols(X_base, y)\n    p_base = calculate_diagnostics(X_base, y, beta_base)\n    \n    # Correction\n    X_corr = X_true\n    beta_corr = fit_ols(X_corr, y)\n    p_corr = calculate_diagnostics(X_corr, y, beta_corr)\n    results.append([p_base, p_corr, p_corr - p_base])\n    \n    # --- Case 3: Variance Misspecification ---\n    spec = test_cases_params[2]\n    rng = np.random.default_rng(spec['seed'])\n    n = spec['n']\n    c = 2.0\n    sigma0 = 0.5\n    x = rng.uniform(0, 3, n)\n    X_base = np.c_[np.ones(n), x]\n    beta_true = np.array([0, 1.5])\n    error_std_dev = sigma0 * np.sqrt(1 + c * x**2)\n    eps = rng.normal(0, 1, n) * error_std_dev\n    y = X_base @ beta_true + eps\n    \n    # Baseline\n    beta_base = fit_ols(X_base, y)\n    p_base = calculate_diagnostics(X_base, y, beta_base)\n    \n    # Correction\n    weights = 1 / (1 + c * x**2)\n    beta_corr = fit_wls(X_base, y, weights)\n    p_corr = calculate_diagnostics(X_base, y, beta_corr)\n    results.append([p_base, p_corr, p_corr - p_base])\n\n    # --- Case 4: Heavy-Tailed Errors ---\n    spec = test_cases_params[3]\n    rng = np.random.default_rng(spec['seed'])\n    n = spec['n']\n    df = 3\n    sigma = 1\n    x = rng.uniform(-2, 2, n)\n    X_base = np.c_[np.ones(n), x]\n    beta_true = np.array([0, 1])\n    scale_factor = sigma * np.sqrt((df - 2) / df)\n    eps = stats.t.rvs(df=df, size=n, random_state=rng) * scale_factor\n    y = X_base @ beta_true + eps\n    \n    # Baseline\n    beta_base = fit_ols(X_base, y)\n    p_base = calculate_diagnostics(X_base, y, beta_base)\n    \n    # Correction\n    beta_corr = fit_irls_huber(X_base, y, k=1.345)\n    p_corr = calculate_diagnostics(X_base, y, beta_corr)\n    results.append([p_base, p_corr, p_corr - p_base])\n\n    # --- Case 5: Omitted Variable ---\n    spec = test_cases_params[4]\n    rng = np.random.default_rng(spec['seed'])\n    n = spec['n']\n    rho = 0.7\n    sigma_eta = 0.3\n    sigma_eps = 0.5\n    x1 = rng.uniform(-2, 2, n)\n    eta = rng.normal(0, sigma_eta, n)\n    x2 = rho * x1 + eta\n    X_true = np.c_[np.ones(n), x1, x2]\n    beta_true = np.array([0, 1, 1])\n    eps = rng.normal(0, sigma_eps, n)\n    y = X_true @ beta_true + eps\n\n    # Baseline\n    X_base = np.c_[np.ones(n), x1]\n    beta_base = fit_ols(X_base, y)\n    p_base = calculate_diagnostics(X_base, y, beta_base)\n    \n    # Correction\n    X_corr = X_true\n    beta_corr = fit_ols(X_corr, y)\n    p_corr = calculate_diagnostics(X_corr, y, beta_corr)\n    results.append([p_base, p_corr, p_corr - p_base])\n\n    # Final print statement in the exact required format.\n    # Convert list of lists to string, then remove spaces\n    output_str = str(results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```", "id": "4894203"}]}