## 引言
在任何依赖数据的科学领域，将原始、复杂的定量数据提炼为简洁、有意义的摘要，是进行分析和交流的第一步，也是最关键的一步。无论我们面对的是物理实验的测量结果、经济市场的波动指标，还是环境监测的读数，有效的数据汇总都能够揭示其中心趋势、离散程度和分布形状，为后续的[假设检验](@entry_id:142556)、模型构建和科学发现奠定基础。然而，简单地计算一个均值或绘制一个[直方图](@entry_id:178776)往往是不够的。现实世界的数据常常伴随着异常值、[偏态分布](@entry_id:175811)或信息不完整等复杂情况，这要求我们掌握一个更系统、更严谨的方法论框架，以选择最合适的统计量和可视化工具。

本文旨在填补这一知识鸿沟，带领读者超越基础计算，深入理解定量数据汇总的“道”与“术”。我们将探讨为何在某些情境下中位数比均值更优越，标准差的解释力有何局限，以及如何利用图形工具直观地评估[数据质量](@entry_id:185007)和分布假设。通过本文的学习，你将能够：
1.  在“原理与机制”一章中，掌握从统计泛函理论到稳健统计量的核心概念，理解均值、方差、[分位数](@entry_id:178417)、[箱形图](@entry_id:177433)及[Q-Q图](@entry_id:174944)背后的统计学原理。
2.  在“应用与跨学科联系”一章中，看到这些汇总方法如何在流行病学、临床研究、药物警戒和高维数据分析等真实场景中发挥作用，解决从风险标准化到[生存数据](@entry_id:165675)解读的实际问题。
3.  最后，在“动手实践”部分，通过具体的编程练习，将理论知识转化为解决实际问题的能力。

让我们从数据汇总的基石——其核心原理与机制——开始我们的探索之旅。

## 原理与机制

在“导论”章节之后，我们现在深入探讨总结定量数据的核心原理与机制。本章的目标是超越简单的计算，建立一个严谨的框架来理解我们为何以及如何选择特定的数值和图形方法来揭示数据中的模式。我们将从基本度量（如均值和方差）的理论基础开始，逐步扩展到能够描述整个分布的非参数方法，并最终讨论在面对现实世界数据（如存在异常值、缺失或删失）的复杂性时，如何选择和解释这些摘要。

### 统计泛函与“即插即用”估计原理

思考如何总结一个数据集最深刻的方式，是从其所代表的潜在“总体”分布入手的。我们可以将任何描述性统计量（如均值、[中位数](@entry_id:264877)或方差）看作是作用于一个概率[分布函数](@entry_id:145626) $F$ 并返回一个实数值的映射。在统计学中，这种映射被称为**统计泛函**，记为 $T(F)$。

例如，一个分布的**均值 (mean)** 定义为[期望值](@entry_id:150961)泛函：
$$
T(F) = \mathbb{E}_F[X] = \int x \, \mathrm{d}F(x)
$$
其中积分是针对分布 $F$ 进行的。

在实际应用中，我们无法观测到真实的总体分布 $F$。我们拥有的是一个从 $F$ 中抽取的样本，例如 $n$ 个独立同分布 (i.i.d.) 的观测值 $\{X_1, \dots, X_n\}$。一个强大且直观的估计 $T(F)$ 的方法是使用所谓的**即插即用原理 (plug-in principle)**。该原理指出，我们可以通过将未知的总体分布 $F$ 替换为其经验估计量——**[经验累积分布函数](@entry_id:167083) (Empirical Cumulative Distribution Function, ECDF)**，记为 $\hat{F}_n$，来获得 $T(F)$ 的一个估计量。

ECDF $\hat{F}_n(x)$ 将等概率质量 $1/n$ 赋予样本中的每一个观测值 $X_i$。因此，对 $\hat{F}_n$ 求[期望值](@entry_id:150961)就简化为对样本值求算术平均。将 $\hat{F}_n$ “插入”到均值泛函中，我们得到均值的即插即用估计量 [@problem_id:4955540]：
$$
T(\hat{F}_n) = \int x \, \mathrm{d}\hat{F}_n(x) = \sum_{i=1}^{n} X_i \cdot \frac{1}{n} = \bar{X}
$$
这正是我们所熟知的**样本均值 (sample mean)**。这个框架不仅为我们熟悉的统计量提供了坚实的理论基础，也为定义和理解更复杂的统计量（如稳健统计量和非参数摘要）铺平了道路。

### 基本摘要统计量及其变换性质

定量数据的两个最核心的特征是其**中心趋势 (central tendency)** 和**离散程度 (dispersion)**。

#### 中心趋势与[离散程度的度量](@entry_id:178320)

**均值 ($\mu$)** 是最常用的中心趋势度量，如上所述，其泛函为 $\mu(F) = \mathbb{E}_F[X]$。

对于离散程度，最核心的度量是**方差 (variance)**，其泛函定义为二阶[中心矩](@entry_id:270177)：
$$
\operatorname{Var}(F) = \mathbb{E}_F[(X - \mu_F)^2] = \int (x - \mu_F)^2 \, \mathrm{d}F(x)
$$
其中 $\mu_F = \mathbb{E}_F[X]$。方差的即插即用估计量是：
$$
\operatorname{Var}(\hat{F}_n) = \frac{1}{n} \sum_{i=1}^{n} (X_i - \bar{X})^2
$$
值得注意的是，这个即插即用估计量（有时称为总体式样本方差）与用于推断的**无偏样本方差 (unbiased sample variance)** $s^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i - \bar{X})^2$ 略有不同。后者对分母进行了修正（[贝塞尔校正](@entry_id:169538), Bessel's correction）以消除对总体[方差估计](@entry_id:268607)的偏差，但这并非直接源于即插即用原理 [@problem_id:4955540]。

由于方差的单位是原始数据单位的平方，这使得解释变得困难。因此，我们通常使用其平方根——**标准差 (standard deviation)**，记为 $\sigma$：
$$
\sigma(F) = \sqrt{\operatorname{Var}(F)}
$$
标准差的单位与原始数据相同，使其成为一个更直观的离散程度度量。

#### 变换下的行为：[等变性](@entry_id:636671)与不变性

在生物统计学实践中，数据经常会因为[单位转换](@entry_id:136593)（例如，从毫克到微克）或基线调整而被[线性变换](@entry_id:143080)。理解摘要统计量在这些变换下的行为至关重要。考虑一个[仿射变换](@entry_id:144885) $Y = aX + b$，其中 $a > 0$ 代表尺度（单位）变换，$b$ 代[表位](@entry_id:181551)置（基线）平移。

- **均值**：均值是**位置等变 (location equivariant)** 和**尺度等变 (scale equivariant)** 的。这意味着 $E[aX+b] = aE[X] + b$。如果将所有测量值增加 $b$ 个单位，均值也会增加 $b$ 个单位；如果将所有测量值乘以因子 $a$，均值也会乘以 $a$ [@problem_id:4955515]。

- **方差**：方差是**位置不变 (location invariant)** 的，但以二次方的方式**尺度等变**。这意味着 $\operatorname{Var}(aX+b) = a^2\operatorname{Var}(X)$。给所有数据点加上一个常数并不会改变它们的离散程度，因此方差不变。然而，将数据乘以 $a$ 会使方差乘以 $a^2$ [@problem_id:4955515]。

- **标准差**：作为方差的平方根，标准差也是**位置不变**的，并且是**尺度等变**的。$\sigma(aX+b) = |a|\sigma(X) = a\sigma(X)$ (因为 $a>0$)。这使得标准差在解释上非常方便：如果数据尺度加倍，其[离散程度的度量](@entry_id:178320)也加倍。

#### 相对变异性：[变异系数](@entry_id:272423)

当我们想要比较不同尺度（即不同均值）的数据集的相对离散程度时，标准差本身可能具有误导性。一个均值为 $1000$、标准差为 $10$ 的数据集，其相对变异性远小于一个均值为 $5$、标准差也是 $10$ 的数据集。为了解决这个问题，我们使用**变异系数 (coefficient of variation, CV)**，它被定义为标准差与均值的比率（对于严格为正的随机变量）：
$$
\operatorname{CV} = \frac{\sigma}{\mu}
$$
CV 是一个无量纲的量。其最重要的特性是**[尺度不变性](@entry_id:180291) (scale invariance)**。对于 $a > 0$，$\operatorname{CV}(aX) = \frac{\sigma(aX)}{\mu(aX)} = \frac{a\sigma(X)}{a\mu(X)} = \frac{\sigma(X)}{\mu(X)} = \operatorname{CV}(X)$。这意味着无论我们使用何种单位（例如，米或厘米）来测量一个物理量，其 CV 保持不变。然而，CV 不是位置不变的，因此它不适用于包含任意基线平移的数据 [@problem_id:4955515]。

### 稳健性与分布的完整描述

均值和标准差虽然是描述正态分布数据的黄金标准，但它们对异常值（outliers）和数据的偏态（skewness）极为敏感。一个极端值就可以极大地扭曲均值和方差，使其不再能代表数据的主体部分。

#### 稳健性问题与柴比雪夫不等式

例如，对于一组血液生物标志物浓度测量值 $\{3.5, 4.0, 4.0, 4.1, 4.1, 4.2, 4.3, 20.0\}$，样本均值为 $6.025$，这远高于数据中绝大多数点的值。这个结果被一个极端的观测值 $20.0$ 严重拉高 [@problem_id:4955540]。这说明均值是一个**非稳健 (non-robust)** 的统计量。同样，[高阶矩](@entry_id:266936)（如方差）对异常值更加敏感 [@problem_id:4955540]。

此外，标准差的解释能力也严重依赖于分布的形状。对于正态分布，我们有著名的“68-95-99.7”经验法则，即大约 $68\%$、$95\%$ 和 $99.7\%$ 的数据分别落在均值的 $1\sigma$、$2\sigma$ 和 $3\sigma$ 范围内。但对于未知或[重尾](@entry_id:274276)（heavy-tailed）分布，这个法则完全不适用。我们能做的最强的、不依赖于分布形式的声明来自**柴比雪夫不等式 (Chebyshev’s inequality)**：
$$
P(|X - \mu| \ge k\sigma) \le \frac{1}{k^2}
$$
这等价于说，对于任何分布（只要其方差有限），落在均值 $k$ 个标准差**之内**的数据比例**至少**为 $1 - 1/k^2$。

- 对于 $k=2$，至少有 $1 - 1/2^2 = 0.75$（即 $75\%$）的数据落在 $\mu \pm 2\sigma$ 范围内。
- 对于 $k=3$，至少有 $1 - 1/3^2 \approx 0.889$（即 $88.9\%$）的数据落在 $\mu \pm 3\sigma$ 范围内。

柴比雪夫不等式揭示了标准差在不作任何分布假设时的局限性。存在一些分布，其 $3\sigma$ 范围内的数据比例确实可能低于 $90\%$ [@problem_id:4955537]。这强调了在处理非正态数据，尤其是重尾数据时，仅依赖均值和标准差可能导致对数据离散程度的误解。

#### [经验累积分布函数](@entry_id:167083) (ECDF)

为了克服这些局限，我们需要能够总结整个分布而不仅仅是其前两个矩的方法。最基本的非参数工具是**[经验累积分布函数](@entry_id:167083) (ECDF)**，$\hat{F}_n(x)$。它被定义为样本中小于或等于 $x$ 的观测值所占的比例 [@problem_id:4955565]：
$$
\hat{F}_n(x) = \frac{1}{n} \sum_{i=1}^{n} \mathbf{1}\{X_i \le x\}
$$
其中 $\mathbf{1}\{\cdot\}$ 是指示函数。$\hat{F}_n(x)$ 是一个阶梯函数，它在每个观测值 $X_i$ 处“跳跃”，跳跃的高度为 $1/n$（或 $k/n$，如果值 $X_i$ 出现 $k$ 次）。根据 Glivenko-Cantelli 定理，当样本量 $n$ 增大时，$\hat{F}_n(x)$ 会一致收敛于真实的总体CDF $F(x)$。因此，ECDF 本身就是对整个分布的一个完整、非参数的“摘要”。

#### 基于分位数的稳健摘要与可视化

从 ECDF 的概念，我们可以自然地导出**分位数 (quantiles)**。一个分布的 $p$-[分位数](@entry_id:178417)是这样一个值，它将分布中较低的 $p$ 部分与较高的 $1-p$ 部分分开。形式上，**[分位数函数](@entry_id:271351) (quantile function)** $Q(p)$ 是 CDF 的[广义逆](@entry_id:140762)函数 [@problem_id:4955552]：
$$
Q(p) = \inf\{x: F(x) \ge p\}
$$
将此定义应用于 ECDF，我们得到**样本[分位数](@entry_id:178417) (sample quantile)**：
$$
\hat{Q}_n(p) = \inf\{x: \hat{F}_n(x) \ge p\}
$$
这意味着样本 $p$-分位数是使得 ECDF 首次达到或超过 $p$ 的最小观测值。

基于分位数的摘要统计量具有天然的稳健性，因为它们依赖于数据的排序位置而非其具体数值。
- **中位数 (Median)**：$Q(0.5)$，即第 $50$ 百分位数。它是一个高度稳健的中心趋势度量。对于上面有异常值的数据集，中位数是 $4.1$，这显然比均值 $6.025$ 更能代表数据的中心 [@problem_id:4955540]。
- **[四分位数](@entry_id:167370) (Quartiles)**：包括下[四分位数](@entry_id:167370) $Q_1 = Q(0.25)$ 和上[四分位数](@entry_id:167370) $Q_3 = Q(0.75)$。
- **[四分位距](@entry_id:169909) (Interquartile Range, IQR)**：$\text{IQR} = Q_3 - Q_1$。IQR 描述了数据中间 $50\%$ 的散布范围，是一个稳健的离散程度度量，不受两端异常值的影响 [@problem_id:4955540]。

**[箱形图](@entry_id:177433) (Boxplot)**，由 John W. Tukey 发明，是基于这些稳健统计量的一个绝佳可视化工具。一个标准的[箱形图](@entry_id:177433)由以下部分构成 [@problem_id:4955534]：
1.  一个从 $Q_1$ 到 $Q_3$ 的“箱子”，代表了数据的中间 $50\%$。
2.  箱子内部的一条线，表示中位数 $Q_2$。
3.  从箱子两端延伸出的“胡须” (whiskers)。胡须的末端通常延伸到所谓的“内篱笆” (inner fences) 之内的最远数据点。内篱笆的位置定义为 $Q_1 - 1.5 \times \text{IQR}$ 和 $Q_3 + 1.5 \times \text{IQR}$。
4.  任何落在内篱笆之外的数据点都被标记为**潜在的异常值 (potential outliers)**。

例如，对于样本 $\{3.2, ..., 7.2, 10.5\}$，排序后我们计算出[中位数](@entry_id:264877)为 $5.1$，$Q_1=4.1$，$Q_3=6.2$，因此 $\text{IQR} = 2.1$。上内篱笆为 $6.2 + 1.5 \times 2.1 = 9.35$。由于 $10.5 > 9.35$，它被识别为一个异常值，而上胡须则延伸到 $9.35$ 之内的最大值 $7.2$ [@problem_id:4955534]。[箱形图](@entry_id:177433)提供了一个关于数据中心、散布、[偏态](@entry_id:178163)和异常值的简洁而稳健的视觉摘要。

#### [直方图](@entry_id:178776)作为[密度估计](@entry_id:634063)

**直方图 (Histogram)** 是另一种常见的可视化工具，它通过将数据分箱并计算每个箱内的观测数量来近似数据的分布形状。更正式地，直方图可以被看作一个**分段常数[密度估计](@entry_id:634063)器 (piecewise-constant density estimator)** [@problem_id:4955528]。
$$
\hat{f}(x) = \frac{1}{nh} \sum_{i=1}^{n} \mathbf{1}\{X_i \in \text{bin}(x)\}
$$
这里，$h$ 是箱子的宽度，$n$ 是样本量，$\text{bin}(x)$ 是包含点 $x$ 的那个箱子。这个公式确保了[直方图](@entry_id:178776)的总面积（即 $\int \hat{f}(x) dx$）为 $1$，使其成为一个合法的概率密度函数（PDF）估计 [@problem_id:4955528]。

[直方图](@entry_id:178776)的形状对两个关键参数非常敏感：
1.  **箱宽 ($h$)**：这是一个平滑参数。较小的 $h$ 会产生一个“锯齿状”但偏差较小、方差较大的直方图；较大的 $h$ 会产生一个更平滑但可能隐藏分布细节（偏差较大）、方差较小的[直方图](@entry_id:178776)。
2.  **箱子的起点**：即使箱宽相同，移动箱子的边界（起点）也可能导致数据点在箱之间移动，从而显著改变直方图的外观。

由于其对这些任意设计选择的敏感性，直方图被认为是一个相对“粗糙”的摘要 [@problem_id:4955528]。更现代的方法，如[核密度估计](@entry_id:167724) (Kernel Density Estimation)，通过更平滑的方式估计密度，克服了其中一些问题。

### 高级主题与特殊[数据结构](@entry_id:262134)

#### 数据变换的影响

在生物统计学中，我们经常对数据进行变换（如[对数变换](@entry_id:267035)）以改善其对称性或稳定其方差。理解摘要统计量在**单调变换 (monotone transformation)** $g(X)$ 下的行为至关重要 [@problem_id:4955544]。

- **[分位数](@entry_id:178417)**：分位数在单调变换下表现出优美的**[等变性](@entry_id:636671)**。
    - 如果 $g$ 是严格递增的，则 $Y=g(X)$ 的 $p$-分位数等于 $X$ 的 $p$-[分位数](@entry_id:178417)经 $g$ 变换后的值：$Q_Y(p) = g(Q_X(p))$。
    - 如果 $g$ 是严格递减的，则 $p$-[分位数](@entry_id:178417)与 $(1-p)$-分位数发生关联：$Q_Y(p) = g(Q_X(1-p))$。
  这意味着中位数（和所有其他[分位数](@entry_id:178417)）可以直接在变换后的尺度上计算，然后变换回原始尺度进行解释。

- **均值与方差**：均值和方差在非线性变换下没有简单的等变关系。特别是，**期望的变换不等于变换的期望**，即 $E[g(X)] \neq g(E[X])$ （除非 $g$ 是线性的）。这是**[詹森不等式](@entry_id:144269) (Jensen's inequality)** 的一个体现。
  一个重要的例子是[对数变换](@entry_id:267035) $g(x) = \ln(x)$。$\exp(E[\ln X])$ 定义了 $X$ 的**[几何平均数](@entry_id:275527) (geometric mean)**。由于对数函数是[凹函数](@entry_id:274100)，[詹森不等式](@entry_id:144269)表明[几何平均数](@entry_id:275527)小于或等于算术平均数 $E[X]$。因此，在对数尺度上计算的均值，当变换回原始尺度时，得到的是[几何平均数](@entry_id:275527)，它通常是描述[右偏态](@entry_id:275130)数据（如浓度）中心趋势的一个更好的度量 [@problem_id:4955544]。

#### 分布假设的评估：[Q-Q图](@entry_id:174944)

许多[统计模型](@entry_id:755400)都依赖于数据服从特定分布（如正态分布）的假设。**[分位数-分位数图](@entry_id:174944) (Quantile-Quantile plot, Q-Q plot)** 是评估这种假设的强大图形工具 [@problem_id:4955563]。其构造步骤如下：
1.  将样本数据排序得到[顺序统计量](@entry_id:266649) $X_{(1)} \le \dots \le X_{(n)}$，它们是样本分位数的估计。
2.  为每个 $X_{(i)}$ 确定一个对应的理论累积概率，称为**绘图位置 (plotting position)** $p_i$（例如，$p_i = (i-0.5)/n$ 或 $p_i = i/(n+1)$）。
3.  计算在这些概率 $p_i$ 下的理论分位数 $Q_0(p_i)$，其中 $Q_0$ 是我们想要比较的参考分布（如[标准正态分布](@entry_id:184509)）的[分位数函数](@entry_id:271351)。
4.  绘制点对 $(Q_0(p_i), X_{(i)})$。

如果样本数据确实来自参考分布（或其[线性变换](@entry_id:143080)），那么[Q-Q图](@entry_id:174944)上的点将近似地落在一条直线上。偏离这条直线则揭示了分布失配的类型：
- **[S形曲线](@entry_id:167614)**：如果样本的尾部比理论分布更“重”（即极端值更多），则图的左下角点会落在参考线下方，右上角点会落在参考线上方 [@problem_id:4955563]。
- **弓形曲线**：表明样本分布的偏度与理论分布不同。
- **斜率不为1**：如果[直线的斜率](@entry_id:165209)大于 $1$，说明样本的离散程度（如标准差）大于理论分布；斜率小于 $1$ 则相反。

#### 双变量关联的度量

当处理成对的定量数据 $(X, Y)$ 时，我们常常需要总结它们之间的关联强度和方向。
- **皮尔逊积矩[相关系数](@entry_id:147037) (Pearson's $r$)**：这是衡量**线性**关联的标准度量。然而，它对异常值非常敏感，并且无法捕捉非线性的关系。
- **秩[相关系数](@entry_id:147037) (Rank Correlation Coefficients)**：当关系是单调但非线性，或者当数据中存在异常值或来自[重尾分布](@entry_id:142737)时，基于秩的度量是更稳健和合适的选择 [@problem_id:4955529]。
    - **[斯皮尔曼等级相关](@entry_id:755150)系数 ($\rho$)**：通过计算 $X$ 和 $Y$ 的等级的[皮尔逊相关系数](@entry_id:270276)得到。它完美地量化了任何单调关系的强度。
    - **[肯德尔等级相关系数](@entry_id:750989) ($\tau$)**：通过计算数据中“一致对”（concordant pairs）和“[不一致对](@entry_id:166371)”（discordant pairs）的比例得到。
  由于秩相关系数仅依赖于数据的排序，它们对于任意的严格单调递增变换都是不变的，并且对极端异常值的幅度不敏感。此外，当数据的[总体矩](@entry_id:170482)（如方差）可能不存在时（例如对于[柯西分布](@entry_id:266469)），[皮尔逊相关](@entry_id:260880)在理论上是无定义的，而秩[相关系数](@entry_id:147037)依然保持良好定义和稳定 [@problem_id:4955529]。

#### 现实世界数据的挑战：缺失与删失

在生物统计学研究中，数据很少是完美和完整的。
- **[缺失数据](@entry_id:271026) (Missing Data)**：当某些观测值由于某种原因未能记录时，就会出现缺失数据。处理[缺失数据](@entry_id:271026)的正确方法取决于数据缺失的机制 [@problem_id:4955545]：
    - **[完全随机缺失](@entry_id:170286) (MCAR)**：缺失的概率与任何观测到的或未观测到的变量都无关。在这种（非常强的）假设下，仅对可用数据进行分析（称为“完整案例分析”）可以得到对总体参数的无偏估计。
    - **[随机缺失](@entry_id:168632) (MAR)**：缺失的概率可以依赖于**观测到**的变量，但不能依赖于**未观测到**的变量本身。在这种情况下，简单的完整案例分析通常会导致有偏估计，因为观测到的样本不再是总体的随机代表。需要使用更复杂的方法（如加权或插补）来校正偏差。
    - **[非随机缺失](@entry_id:163489) (MNAR)**：缺失的概率依赖于未观测到的变量本身（例如，病情最严重的患者更有可能退出研究）。在这种情况下，即使是复杂的统计调整也可能无法消除偏差，因为缺失机制是不可识别的。

- **删失与[截断数据](@entry_id:163004) (Censored and Truncated Data)**：在生存分析中尤其常见。
    - **[右删失](@entry_id:164686) (Right-Censoring)**：当研究结束或患者失访时，我们只知道其生存时间**大于**某个观测到的时间，但不知道确切的事件时间。
    - **左截断 (Left-Truncation)**：当研究只招募在某个时间点之后仍然“存活”的个体时发生，排除了早期发生事件的个体。
  在这些情况下，使用标准的摘要统计（如样本均值）是严重错误的。例如，简单地将被删失的观测时间当作真实事件时间来计算平均值，会系统性地低估真实的平均生存时间，因为我们用已知的下限替换了未知的、更大的值 [@problem_id:4955555]。这[类数](@entry_id:156164)据需要专门的生存分析方法（如Kaplan-Meier估计）来正确地进行总结和推断。