{"hands_on_practices": [{"introduction": "我们将从卡方分布（$\\chi^2$ 分布）的概率密度函数（PDF）这一根本出发点开始实践。通过寻找分布的峰值（即众数），我们可以直观地理解其特有的形态和右偏性，特别是这些性质如何随自由度的变化而改变。这是一项旨在深入理解 $\\chi^2$ 分布几何特征的基础性练习。[@problem_id:4958326]", "problem": "一位生物统计学家正在分析列联表数据的拟合优度检验。在原假设下，Pearson检验统计量 $X$ 服从自由度为 $k$ 的卡方分布，记为 $\\chi^{2}_{k}$。从卡方分布的基本概率密度函数（PDF）和伽马分布的标准均值出发，通过直接在其支撑集上优化其PDF来确定 $\\chi^{2}_{k}$ 分布的众数作为 $k$ 的函数，并适当考虑在 $x=0$ 处的边界情况。然后，对于 $k \\ge 2$ 的情况，解释该众数的位置与均值的比较，并根据分布的偏度来解释这一比较。将您的最终答案表示为关于众数的单个闭式解析表达式；无需四舍五入。", "solution": "该问题要求推导自由度为 $k$ 的卡方分布（记为 $\\chi^2_k$）的众数，并随后在 $k \\ge 2$ 的情况下将此众数与分布的均值进行比较，以解释其偏度。\n\n首先，我们确定 $\\chi^2_k$ 分布的均值。$\\chi^2_k$ 分布是伽马分布的一个特例。一个服从形状参数为 $\\alpha$、率参数为 $\\beta$ 的伽马分布的随机变量 $X$，记为 $X \\sim \\text{Gamma}(\\alpha, \\beta)$，其均值为 $E[X] = \\frac{\\alpha}{\\beta}$。$\\chi^2_k$ 分布对应于参数为 $\\alpha = \\frac{k}{2}$ 和 $\\beta = \\frac{1}{2}$ 的伽马分布。因此，$\\chi^2_k$ 分布的均值为 $\\mu = E[X] = \\frac{k/2}{1/2} = k$。\n\n接下来，我们通过最大化 $\\chi^2_k$ 分布的概率密度函数（PDF）来确定众数。服从 $\\chi^2_k$ 分布的随机变量 $X$ 的PDF由下式给出：\n$$ f(x; k) = \\frac{1}{2^{k/2} \\Gamma(k/2)} x^{k/2 - 1} \\exp(-x/2) $$\n对于 $x  0$，其中 $k$ 是表示自由度的正整数，$\\Gamma(\\cdot)$ 是伽马函数。该分布的支撑集是区间 $(0, \\infty)$。众数是在此支撑集上使 $f(x; k)$ 最大化的 $x$ 的值。\n\n为了简化最大化过程，我们可以处理PDF的自然对数 $\\ln f(x; k)$，因为对数函数是严格递增函数。最大化 $\\ln f(x; k)$ 将得到与最大化 $f(x; k)$ 相同的 $x$ 值。\n$$ \\ln f(x; k) = \\ln\\left(\\frac{1}{2^{k/2} \\Gamma(k/2)}\\right) + \\ln\\left(x^{k/2 - 1}\\right) + \\ln\\left(\\exp(-x/2)\\right) $$\n$$ \\ln f(x; k) = -\\frac{k}{2}\\ln(2) - \\ln\\left(\\Gamma(k/2)\\right) + \\left(\\frac{k}{2} - 1\\right)\\ln(x) - \\frac{x}{2} $$\n为了找到最大值，我们计算关于 $x$ 的一阶导数并将其设为零。不含 $x$ 的项在微分时是常数。\n$$ \\frac{d}{dx} \\ln f(x; k) = \\frac{d}{dx} \\left[ \\left(\\frac{k}{2} - 1\\right)\\ln(x) - \\frac{x}{2} \\right] $$\n$$ \\frac{d}{dx} \\ln f(x; k) = \\left(\\frac{k}{2} - 1\\right)\\frac{1}{x} - \\frac{1}{2} $$\n将导数设为零以寻找临界点：\n$$ \\left(\\frac{k}{2} - 1\\right)\\frac{1}{x} - \\frac{1}{2} = 0 $$\n$$ \\frac{k-2}{2x} = \\frac{1}{2} $$\n假设 $x \\neq 0$（对于支撑集这是成立的），我们找到临界点：\n$$ x = k - 2 $$\n现在我们必须根据 $k$ 的值来分析这个结果，同时考虑到 $k$ 是一个正整数。\n\n情况1：$k  2$。\n对于 $k  2$，临界点 $x=k-2$ 是正数，因此它位于支撑集 $(0, \\infty)$ 内。为了确定这是否是一个最大值，我们考察二阶导数：\n$$ \\frac{d^2}{dx^2} \\ln f(x; k) = \\frac{d}{dx} \\left[ \\left(\\frac{k-2}{2}\\right)x^{-1} - \\frac{1}{2} \\right] = -\\left(\\frac{k-2}{2}\\right)x^{-2} = -\\frac{k-2}{2x^2} $$\n由于 $k  2$，项 $k-2$ 是正的。对于任何 $x  0$，$2x^2$ 也是正的。因此，对于支撑集中的所有 $x$，二阶导数都是负的。这证实了对数PDF是凹的，并且临界点 $x=k-2$ 是唯一的最大值点。众数是 $k-2$。\n\n情况2：$k \\le 2$。由于 $k$ 必须是正整数，这种情况包括 $k=1$ 和 $k=2$。\n如果 $k=2$，一阶导数变为：\n$$ \\frac{d}{dx} \\ln f(x; 2) = \\left(\\frac{2-2}{2}\\right)\\frac{1}{x} - \\frac{1}{2} = 0 - \\frac{1}{2} = -\\frac{1}{2} $$\n由于对于 $x0$ 导数恒为负，函数 $f(x; 2)$ 在其整个支撑集上是严格递减的。当 $x$ 趋近于支撑集的左边界，即 $x \\to 0^+$ 时，函数值趋近于最大值。因此，众数位于 $x=0$。注意，公式 $k-2$ 给出 $2-2=0$。\n\n如果 $k=1$，一阶导数变为：\n$$ \\frac{d}{dx} \\ln f(x; 1) = \\left(\\frac{1-2}{2}\\right)\\frac{1}{x} - \\frac{1}{2} = -\\frac{1}{2x} - \\frac{1}{2} $$\n对于 $x  0$，$-\\frac{1}{2x}$ 和 $-\\frac{1}{2}$ 都是负数，所以导数是严格为负的。同样，函数 $f(x; 1)$ 是严格递减的。PDF $f(x;1) = (\\sqrt{2\\pi})^{-1} x^{-1/2} \\exp(-x/2)$ 在 $x=0$ 处有一条垂直渐近线。函数的上确界在 $x=0$ 处，因此众数是 $0$。在这种情况下，临界点公式 $k-2$ 给出 $1-2 = -1$，它在支撑集之外，这证实了最大值不是一个内部点。\n\n总结众数的结果：\n- 如果 $k  2$，众数是 $k-2$。\n- 如果 $k \\in \\{1, 2\\}$，众数是 $0$。\n\n这可以表示为 $k$ 的单个函数：$\\text{众数} = \\max(0, k-2)$。\n\n问题接着要求在 $k \\ge 2$ 的情况下比较众数与均值，并根据偏度进行解释。\n均值为 $\\mu = k$。\n众数为 $\\text{众数} = \\max(0, k-2)$。\n\n对于 $k \\ge 2$：\n- 如果 $k=2$，均值 $= 2$，众数 $= \\max(0, 2-2) = 0$。此时，均值 $$ 众数。\n- 如果 $k2$，均值 $= k$，众数 $= \\max(0, k-2) = k-2$。显然，$k  k-2$，所以均值 $$ 众数。\n\n在所有 $k \\ge 2$ 的情况下，卡方分布的均值都严格大于其众数。\n\n这种均值 $$ 众数的比较，是右偏或正偏分布的一个典型特征。对于单峰分布，这个不等式表明大部分概率质量集中在较低的值处，而一个较长的尾部延伸向较高的值。均值被尾部中这些较大但不那么频繁的值向右（朝向更高的值）拉动。$\\chi^2_k$ 分布的偏度的正式度量是 $\\gamma_1 = \\frac{2\\sqrt{2}}{\\sqrt{k}}$，对于 $k0$ 它始终为正，这证实了该分布总是右偏的。随着 $k$ 的增加，偏度减小，均值（$k$）和众数（$k-2$）变得更接近，这反映了该分布向对称的正态分布收敛。\n\n所要求的最终答案是众数作为 $k$ 的函数的单个闭式表达式。", "answer": "$$ \\boxed{\\max(0, k-2)} $$", "id": "4958326"}, {"introduction": "卡方分布（$\\chi^2$ 分布）在关于总体方差的推断中扮演着核心角色。本练习将这一经典结论与现代统计学中的稳健性概念联系起来，分析单个异常值如何显著影响样本方差估计量。通过影响函数这一工具，我们将揭示为何与 $\\chi^2$ 分布相关的估计量可能很“脆弱”，以及为何有时需要稳健的替代方法。[@problem_id:4958322]", "problem": "一个生物统计学实验室分析一个连续的生物标志物，其基线分布可以很好地用正态分布来近似。设 $X$ 表示一个个体测量值，其分布为 $F$，均值为 $\\mu$，方差为 $\\sigma^{2}$。考虑方差泛函 $T(F) = \\mathrm{Var}_{F}(X)$。$\\sigma^{2}$ 的一个经典估计量是无偏样本方差 $S^{2} = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_{i} - \\bar{X})^{2}$，在正态性假设下，统计量 $(n-1) S^{2} / \\sigma^{2}$ 服从自由度为 $n-1$ 的卡方分布。\n\n为了研究离群值的影响，考虑污染分布 $F_{\\varepsilon} = (1 - \\varepsilon) F + \\varepsilon \\Delta_{z}$，其中 $0  \\varepsilon \\ll 1$ 且 $\\Delta_{z}$ 是在 $z$ 处的点质量。统计泛函 $T$ 在 $F$ 处、点 $z$ 上的影响函数定义为\n$$\n\\mathrm{IF}(z; T, F) \\equiv \\lim_{\\varepsilon \\to 0^{+}} \\frac{T\\big((1 - \\varepsilon) F + \\varepsilon \\Delta_{z}\\big) - T(F)}{\\varepsilon}.\n$$\n仅从方差的定义 $T(F) = \\mathbb{E}_{F}[X^{2}] - \\big(\\mathbb{E}_{F}[X]\\big)^{2}$ 和上述影响函数的定义出发，完成以下任务：\n\n1) 推导 $\\mathrm{IF}(z; T, F)$ 关于 $z$、$\\mu$ 和 $\\sigma^{2}$ 的闭式表达式。\n\n2) 将您的结果特化到基线正态模型 $F = \\mathcal{N}(0, 1)$，并对在 $z = c$ 处的单个污染，获得一阶近似式\n$$\nT(F_{\\varepsilon}) \\approx T(F) + \\varepsilon \\,\\mathrm{IF}(z; T, F)\n$$\n\n3) 将您的结果与卡方框架联系起来，具体如下。设 $n = 20$，并在基线尺度 $\\sigma^{2} = 1$ 下定义 $Q = (n-1) S^{2}$。利用对于任何具有有限二阶矩的分布，都有 $\\mathbb{E}[S^{2}] = \\mathrm{Var}(X)$ 这一事实，来写出在污染模型（$\\varepsilon = 0.01$，$c = 10$）下 $\\mathbb{E}[Q]$ 的一阶近似。\n\n报告 $\\mathbb{E}[Q]$ 近似值的计算结果，四舍五入到四位有效数字。\n\n用文字简要评论为什么中位数绝对偏差（MAD），在经过适当缩放以在正态性假设下估计 $\\sigma$ 后，是此场景中一个更稳健的替代方案，但不要计算它。\n\n您的最终答案必须是一个实数。", "solution": "该问题陈述经核实具有科学依据、问题提出得当且客观。它植根于稳健统计学的标准框架，特别是使用影响函数来分析统计估计量对污染的敏感性。所有定义、常数和任务都明确指定，构成了一个自洽且逻辑一致的问题。没有矛盾、模糊或事实错误。因此，我们可以进行完整解答。\n\n根据问题陈述的要求，解答分为三个部分，最后附有简要评论。\n\n### 第1部分：方差泛函影响函数的推导\n\n方差泛函由 $T(F) = \\mathrm{Var}_{F}(X)$ 给出。我们从其期望的定义开始：\n$$\nT(F) = \\mathbb{E}_{F}[X^{2}] - \\big(\\mathbb{E}_{F}[X]\\big)^{2}\n$$\n污染分布为 $F_{\\varepsilon} = (1 - \\varepsilon) F + \\varepsilon \\Delta_{z}$。为了求得 $T(F_{\\varepsilon})$，我们首先需要计算关于 $F_{\\varepsilon}$ 的前两阶矩。\n\n$X$ 在 $F_{\\varepsilon}$ 下的期望为：\n$$\n\\mathbb{E}_{F_{\\varepsilon}}[X] = \\int x \\, dF_{\\varepsilon}(x) = (1 - \\varepsilon) \\int x \\, dF(x) + \\varepsilon \\int x \\, d\\Delta_{z}(x)\n$$\n根据定义，$\\int x \\, dF(x) = \\mathbb{E}_{F}[X] = \\mu$ 且 $\\int x \\, d\\Delta_{z}(x) = z$。因此，\n$$\n\\mathbb{E}_{F_{\\varepsilon}}[X] = (1 - \\varepsilon)\\mu + \\varepsilon z\n$$\n\n$X^{2}$ 在 $F_{\\varepsilon}$ 下的期望为：\n$$\n\\mathbb{E}_{F_{\\varepsilon}}[X^{2}] = \\int x^{2} \\, dF_{\\varepsilon}(x) = (1 - \\varepsilon) \\int x^{2} \\, dF(x) + \\varepsilon \\int x^{2} \\, d\\Delta_{z}(x)\n$$\n根据定义，$\\int x^{2} \\, dF(x) = \\mathbb{E}_{F}[X^{2}]$ 且 $\\int x^{2} \\, d\\Delta_{z}(x) = z^{2}$。我们知道 $\\sigma^{2} = \\mathbb{E}_{F}[X^{2}] - \\mu^{2}$，这意味着 $\\mathbb{E}_{F}[X^{2}] = \\sigma^{2} + \\mu^{2}$。因此，\n$$\n\\mathbb{E}_{F_{\\varepsilon}}[X^{2}] = (1 - \\varepsilon)(\\sigma^{2} + \\mu^{2}) + \\varepsilon z^{2}\n$$\n\n现在我们可以写出 $T(F_{\\varepsilon})$：\n$$\nT(F_{\\varepsilon}) = \\mathbb{E}_{F_{\\varepsilon}}[X^{2}] - \\big(\\mathbb{E}_{F_{\\varepsilon}}[X]\\big)^{2} = \\left[(1 - \\varepsilon)(\\sigma^{2} + \\mu^{2}) + \\varepsilon z^{2}\\right] - \\left[(1 - \\varepsilon)\\mu + \\varepsilon z\\right]^{2}\n$$\n我们展开这个表达式，保留 $\\varepsilon$ 的一阶项，因为之后将取 $\\varepsilon \\to 0$ 的极限：\n$$\n(1 - \\varepsilon)\\mu + \\varepsilon z = \\mu + \\varepsilon(z - \\mu)\n$$\n$$\n\\left[(1 - \\varepsilon)\\mu + \\varepsilon z\\right]^{2} = \\left[\\mu + \\varepsilon(z - \\mu)\\right]^{2} = \\mu^{2} + 2\\varepsilon\\mu(z-\\mu) + O(\\varepsilon^{2})\n$$\n$T(F_{\\varepsilon})$ 的第一项是：\n$$\n(1 - \\varepsilon)(\\sigma^{2} + \\mu^{2}) + \\varepsilon z^{2} = \\sigma^{2} + \\mu^{2} - \\varepsilon(\\sigma^{2} + \\mu^{2}) + \\varepsilon z^{2}\n$$\n将这些合并，我们得到：\n$$\nT(F_{\\varepsilon}) = \\left[\\sigma^{2} + \\mu^{2} - \\varepsilon(\\sigma^{2} + \\mu^{2}) + \\varepsilon z^{2}\\right] - \\left[\\mu^{2} + 2\\varepsilon\\mu(z-\\mu) + O(\\varepsilon^{2})\\right]\n$$\n$$\nT(F_{\\varepsilon}) = (\\sigma^{2} + \\mu^{2} - \\mu^{2}) + \\varepsilon(-(\\sigma^{2} + \\mu^{2}) + z^{2} - 2\\mu(z-\\mu)) + O(\\varepsilon^{2})\n$$\n$$\nT(F_{\\varepsilon}) = \\sigma^{2} + \\varepsilon(- \\sigma^{2} - \\mu^{2} + z^{2} - 2\\mu z + 2\\mu^{2}) + O(\\varepsilon^{2})\n$$\n$$\nT(F_{\\varepsilon}) = \\sigma^{2} + \\varepsilon(z^{2} - 2\\mu z + \\mu^{2} - \\sigma^{2}) + O(\\varepsilon^{2})\n$$\n识别出完全平方，我们有：\n$$\nT(F_{\\varepsilon}) = \\sigma^{2} + \\varepsilon \\big( (z-\\mu)^{2} - \\sigma^{2} \\big) + O(\\varepsilon^{2})\n$$\n现在我们应用影响函数的定义，注意到 $T(F) = \\sigma^{2}$：\n$$\n\\mathrm{IF}(z; T, F) = \\lim_{\\varepsilon \\to 0^{+}} \\frac{T(F_{\\varepsilon}) - T(F)}{\\varepsilon} = \\lim_{\\varepsilon \\to 0^{+}} \\frac{\\left[\\sigma^{2} + \\varepsilon \\big( (z-\\mu)^{2} - \\sigma^{2} \\big) + O(\\varepsilon^{2})\\right] - \\sigma^{2}}{\\varepsilon}\n$$\n$$\n\\mathrm{IF}(z; T, F) = \\lim_{\\varepsilon \\to 0^{+}} \\frac{\\varepsilon \\big( (z-\\mu)^{2} - \\sigma^{2} \\big) + O(\\varepsilon^{2})}{\\varepsilon} = \\lim_{\\varepsilon \\to 0^{+}} \\left[ (z-\\mu)^{2} - \\sigma^{2} + O(\\varepsilon) \\right]\n$$\n这就给出了方差影响函数的闭式表达式：\n$$\n\\mathrm{IF}(z; T, F) = (z-\\mu)^{2} - \\sigma^{2}\n$$\n\n### 第2部分：特化与一阶近似\n\n对于基线正态模型 $F = \\mathcal{N}(0, 1)$，参数为 $\\mu = 0$ 和 $\\sigma^{2} = 1$。将这些代入影响函数的一般表达式，我们得到：\n$$\n\\mathrm{IF}(z; T, \\mathcal{N}(0, 1)) = (z - 0)^{2} - 1 = z^{2} - 1\n$$\n泛函 $T(F_{\\varepsilon})$ 的一阶近似由 $T(F_{\\varepsilon}) \\approx T(F) + \\varepsilon \\, \\mathrm{IF}(z; T, F)$ 给出。对于在 $z = c$ 处的单个污染，这变为：\n$$\nT(F_{\\varepsilon}) \\approx T(\\mathcal{N}(0, 1)) + \\varepsilon \\, \\mathrm{IF}(c; T, \\mathcal{N}(0, 1))\n$$\n代入具体值 $T(\\mathcal{N}(0, 1)) = \\sigma^2 = 1$ 和推导出的影响函数，我们得到：\n$$\nT(F_{\\varepsilon}) \\approx 1 + \\varepsilon (c^{2} - 1)\n$$\n\n### 第3部分：与卡方框架的联系及计算\n\n我们已知 $n=20$ 和统计量 $Q = (n-1) S^{2}$。我们需要在污染模型下找到 $\\mathbb{E}[Q]$ 的一阶近似。利用期望的线性性质以及对于任何具有有限二阶矩的分布都有 $\\mathbb{E}[S^{2}] = \\mathrm{Var}(X)$ 这一已知事实，我们可以写出：\n$$\n\\mathbb{E}[Q] = \\mathbb{E}[(n-1)S^{2}] = (n-1) \\mathbb{E}[S^{2}]\n$$\n在污染分布 $F_{\\varepsilon}$ 下，样本方差的期望是该分布的真实方差，即 $\\mathrm{Var}_{F_{\\varepsilon}}(X) = T(F_{\\varepsilon})$。\n$$\n\\mathbb{E}[Q] = (n-1) T(F_{\\varepsilon})\n$$\n使用第2部分中推导出的 $T(F_{\\varepsilon})$ 的一阶近似，我们有：\n$$\n\\mathbb{E}[Q] \\approx (n-1) \\left[ T(F) + \\varepsilon \\, \\mathrm{IF}(c; T, F) \\right]\n$$\n问题指定了基线尺度为 $\\sigma^{2} = 1$，且上下文（来自第2部分）暗示基线均值为 $\\mu=0$。因此，$T(F) = 1$ 且 $\\mathrm{IF}(c; T, F) = c^2 - 1$。\n$\\mathbb{E}[Q]$ 的近似值为：\n$$\n\\mathbb{E}[Q] \\approx (n-1) [1 + \\varepsilon (c^{2} - 1)]\n$$\n我们现在代入给定的数值：$n=20$，$\\varepsilon=0.01$ 和 $c=10$。\n$$\n\\mathbb{E}[Q] \\approx (20-1) [1 + 0.01 (10^{2} - 1)]\n$$\n$$\n\\mathbb{E}[Q] \\approx 19 [1 + 0.01 (100 - 1)]\n$$\n$$\n\\mathbb{E}[Q] \\approx 19 [1 + 0.01 \\times 99]\n$$\n$$\n\\mathbb{E}[Q] \\approx 19 [1 + 0.99]\n$$\n$$\n\\mathbb{E}[Q] \\approx 19 \\times 1.99\n$$\n$$\n\\mathbb{E}[Q] \\approx 37.81\n$$\n在 $\\mathcal{N}(0,1)$ 模型下 $Q$ 的基线期望是 $\\mathbb{E}[Q] = n-1=19$，因为 $Q/\\sigma^2 \\sim \\chi^2_{n-1}$。计算表明，在10个标准差处的1%污染几乎使 $Q$ 的期望值翻倍，这突显了样本方差对离群值的极端敏感性。\n结果 $37.81$ 已经是四位有效数字。\n\n### 关于MAD稳健性的评论\n\n中位数绝对偏差（MAD）是比样本方差更稳健的尺度估计量。这是因为它基于中位数，而中位数本身就是稳健的位置估计量。一个估计量的影响函数量化了单个离群值对估计值的影响。对于方差，我们发现 $\\mathrm{IF}(z; T, F) = (z-\\mu)^{2} - \\sigma^{2}$，当 $z \\to \\infty$ 时，它是无界的。这意味着单个任意大的离群值可以对样本方差产生任意大的影响。相比之下，中位数的影响函数是有界的。因此，MAD的影响函数也是有界的。有界的影响函数意味着单个离群值的影响是有限的，无论其大小如何。这个属性使MAD成为一个稳健的统计量，意味着它对数据中一小部分严重错误或离群值不敏感。", "answer": "$$\\boxed{37.81}$$", "id": "4958322"}, {"introduction": "卡方分布（$\\chi^2$ 分布）的一个关键应用是拟合优度检验，其中检验统计量仅在样本量趋于无穷时*渐近*服从 $\\chi^2$ 分布。这项计算练习将通过模拟皮尔逊检验统计量，并观察其分布如何随着样本量的增加而收敛于理论 $\\chi^2$ 分布，从而将这个抽象的定理变得鲜活起来。该实践展示了在验证统计理论方面，模拟方法的强大威力。[@problem_id:2405617]", "problem": "您将通过模拟研究 Pearson 卡方拟合优度检验统计量的渐近零分布，并量化其收敛到理论卡方分布的情况。考虑一个具有 $K$ 个类别和概率向量 $\\mathbf{p} = (p_{1},\\ldots,p_{K})$ 的分类模型，其中每个 $p_{i} \\in (0,1)$ 且 $\\sum_{i=1}^{K} p_{i} = 1$。对于大小为 $n$ 的样本，令 $\\mathbf{O} = (O_{1},\\ldots,O_{K})$ 表示类别计数，假设在零假设下服从多项分布，并令 $\\mathbf{E} = (E_{1},\\ldots,E_{K})$ 为期望计数，其中对于每个 $i \\in \\{1,\\ldots,K\\}$，有 $E_{i} = n p_{i}$。定义 Pearson 卡方统计量\n$$\nQ_{n} \\;=\\; \\sum_{i=1}^{K} \\frac{(O_{i}-E_{i})^{2}}{E_{i}}.\n$$\n已知在零假设下，当 $K$ 固定且所有 $p_{i} \\in (0,1)$ 时，随着 $n \\to \\infty$，$Q_{n}$ 的分布收敛到自由度为 $K-1$ 的卡方分布。\n\n您的任务是编写一个基于模拟的确定性程序，该程序对每个指定的测试用例，使用蒙特卡洛复制来近似 $Q_{n}$ 的分布，然后报告 $Q_{n}$ 的经验分布与自由度为 $K-1$ 的理论卡方分布之间的 Kolmogorov-Smirnov 距离。Kolmogorov-Smirnov (KS) 距离是两个累积分布函数 (CDF) 之间绝对差的上确界：\n$$\nD \\;=\\; \\sup_{x \\in \\mathbb{R}} \\left| \\widehat{F}_{Q_{n}}(x) \\;-\\; F_{\\chi^{2}_{K-1}}(x) \\right|.\n$$\n您的程序必须是确定性的，通过在每个测试用例中使用提供的种子来控制伪随机性。\n\n使用以下测试套件。对于每个测试用例，给定 $(K,\\mathbf{p},n,R,s)$，其中 $K$ 是类别数，$\\mathbf{p}$ 是概率向量，$n$ 是样本量，$R$ 是蒙特卡洛复制次数，$s$ 是伪随机数生成器的种子。\n\n- 测试用例 1（中等 $n$ 的一般情况）：\n  - $K = 5$\n  - $\\mathbf{p} = (0.1,\\,0.2,\\,0.3,\\,0.25,\\,0.15)$\n  - $n = 50$\n  - $R = 20000$\n  - $s = 12345$\n- 测试用例 2（相同模型，更大的 $n$）：\n  - $K = 5$\n  - $\\mathbf{p} = (0.1,\\,0.2,\\,0.3,\\,0.25,\\,0.15)$\n  - $n = 200$\n  - $R = 20000$\n  - $s = 12345$\n- 测试用例 3（相同模型，远大于 $n$）：\n  - $K = 5$\n  - $\\mathbf{p} = (0.1,\\,0.2,\\,0.3,\\,0.25,\\,0.15)$\n  - $n = 2000$\n  - $R = 20000$\n  - $s = 12345$\n- 测试用例 4（$n$ 较小且类别数相对于 $n$ 较多的边缘情况）：\n  - $K = 8$\n  - $\\mathbf{p} = (1/8,\\,1/8,\\,1/8,\\,1/8,\\,1/8,\\,1/8,\\,1/8,\\,1/8)$\n  - $n = 16$\n  - $R = 20000$\n  - $s = 67890$\n- 测试用例 5（更高维度的模型）：\n  - $K = 20$\n  - $\\mathbf{p} = (1/20,\\,\\ldots,\\,1/20)$\n  - $n = 1000$\n  - $R = 15000$\n  - $s = 13579$\n\n您的程序必须为每个测试用例计算一个实数，该实数等于 $Q_{n}$ 的经验分布（基于 $R$ 次复制）与自由度为 $K-1$ 的卡方分布之间的 Kolmogorov-Smirnov 距离 $D$。答案必须是精确到 $6$ 位小数的实数。\n\n最终输出格式：您的程序应生成一行内容，包含五个测试用例的结果，形式为方括号括起来的逗号分隔列表（例如，$[d_{1},d_{2},d_{3},d_{4},d_{5}]$），其中每个 $d_{j}$ 是测试用例 $j$ 的四舍五入后的 Kolmogorov-Smirnov 距离。", "solution": "所提出的问题是计算统计学中一个明确定义的练习，并且被认为是有效的。它在科学上基于已建立的统计理论，即 Pearson 卡方检验及其渐近性质。每个测试用例的参数都是完整、一致的，并允许一个唯一、可验证的解决方案。该任务要求对一个基本极限定理进行数值研究，这是量化领域中一个标准且有意义的程序。\n\n解决方案如下。我们处理量化 Pearson 卡方统计量 $Q_n$ 收敛到其渐近 $\\chi^2$ 分布的问题。\n\n首先，我们建立理论基础。在零假设下，从具有 $K$ 个类别和概率向量 $\\mathbf{p}=(p_1, \\ldots, p_K)$ 的分类分布中抽取的样本量为 $n$ 的观测计数 $\\mathbf{O} = (O_{1},\\ldots,O_{K})$ 服从多项分布，记为 $\\text{Multinomial}(n, \\mathbf{p})$。类别 $i$ 的期望计数是 $E_i=np_i$。Pearson 卡方统计量由下式给出\n$$\nQ_{n} = \\sum_{i=1}^{K} \\frac{(O_{i}-E_{i})^{2}}{E_{i}}.\n$$\n统计学中的一个基本结果，即 Pearson 定理，指出随着样本量 $n$ 趋于无穷大，$Q_n$ 的分布收敛到自由度为 $K-1$ 的卡方分布，记为 $\\chi^2_{K-1}$。这种收敛是我们研究的主题。收敛速度取决于 $n$、$K$ 和具体的概率 $p_i$。当所有期望计数 $E_i$ 都足够大时（通常 $E_i \\ge 5$），该近似被普遍认为是可靠的。\n\n为了在有限的 $n$ 下数值评估此近似的质量，我们采用蒙特卡洛模拟。对于由参数 $(K, \\mathbf{p}, n, R, s)$ 定义的每个测试用例，过程如下：\n\n1.  **初始化**：我们将伪随机数生成器的种子固定为指定值 $s$。这确保了模拟是确定性的，其结果是完全可复现的。\n\n2.  **$Q_n$ 的模拟**：我们生成 $R$ 个统计量 $Q_n$ 的独立样本。这通过执行 $R$ 次以下步骤的复制来实现：\n    a. 从 $\\text{Multinomial}(n, \\mathbf{p})$ 分布中抽取一个观测计数向量 $\\mathbf{O} = (O_1, \\ldots, O_K)$。使用向量化实现，我们可以在单次操作中生成所有 $R$ 个计数向量，得到一个 $R \\times K$ 的观测计数矩阵。\n    b. 对于 $R$ 个观测计数向量中的每一个，我们计算相应的 $Q_n$ 值。期望计数 $\\mathbf{E} = n\\mathbf{p}$ 在所有复制中是恒定的。此计算也进行向量化，以高效地计算一个包含 $R$ 个 $Q_n$ 统计量值的数组。\n\n3.  **分布距离的量化**：生成 $R$ 个 $Q_n$ 样本后，我们得到一个经验累积分布函数 (CDF)，记为 $\\widehat{F}_{Q_n}(x)$。我们必须测量这个经验 CDF 与目标分布的理论 CDF $F_{\\chi^2_{K-1}}(x)$ 之间的“距离”。问题指定使用 Kolmogorov-Smirnov (KS) 距离，定义为两个 CDF 之间绝对差的上确界：\n    $$\n    D = \\sup_{x \\in \\mathbb{R}} \\left| \\widehat{F}_{Q_{n}}(x) - F_{\\chi^{2}_{K-1}}(x) \\right|.\n    $$\n    较小的 $D$ 值表示 $Q_n$ 的经验分布与理论 $\\chi^2_{K-1}$ 分布之间的拟合更好。此计算使用 `scipy.stats` 库中的 `kstest` 函数执行，该函数直接将生成的 $Q_n$ 值样本与自由度为 $K-1$ 的理论卡方分布进行比较。\n\n将这整个过程应用于五个测试用例中的每一个。每个案例的最终输出是计算出的 KS 距离 $D$，四舍五入到 $6$ 位小数。测试用例的结果说明了渐近近似的性质：距离 $D$ 预计会随着 $n$ 的增加而减小（案例 1-3），而当期望单元格计数较小时，近似效果预计会很差（案例 4）。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import kstest\n\ndef solve():\n    \"\"\"\n    Computes the Kolmogorov-Smirnov distance between the empirical distribution\n    of the Pearson chi-square statistic and the theoretical chi-square\n    distribution for a suite of test cases.\n    \"\"\"\n\n    def compute_ks_distance(K, p_vec, n, R, seed):\n        \"\"\"\n        Runs a Monte Carlo simulation to compute the KS distance for one test case.\n\n        Args:\n            K (int): Number of categories.\n            p_vec (list or np.ndarray): Probability vector.\n            n (int): Sample size.\n            R (int): Number of Monte Carlo replications.\n            seed (int): Seed for the pseudorandom number generator.\n\n        Returns:\n            float: The computed KS distance, rounded to 6 decimal places.\n        \"\"\"\n        # 1. Initialize the pseudorandom number generator for deterministic results.\n        rng = np.random.default_rng(seed)\n\n        # 2. Define model parameters and calculate theoretical expected counts.\n        p_vec = np.array(p_vec)\n        expected_counts = n * p_vec\n        df = K - 1  # Degrees of freedom for the chi-square distribution.\n\n        # 3. Generate R sets of observed counts from the multinomial distribution.\n        # This is a vectorized operation, creating an (R, K) array.\n        observed_counts = rng.multinomial(n, p_vec, size=R)\n\n        # 4. Calculate the Pearson chi-square statistic for each of the R replicates.\n        # This calculation is also vectorized for efficiency.\n        # We sum over the K categories (axis=1).\n        q_n_samples = np.sum((observed_counts - expected_counts)**2 / expected_counts, axis=1)\n\n        # 5. Compute the Kolmogorov-Smirnov statistic.\n        # This compares the empirical distribution of the simulated Q_n values\n        # against the theoretical chi-square CDF with K-1 degrees of freedom.\n        ks_statistic, _ = kstest(q_n_samples, 'chi2', args=(df,), N=R)\n\n        # 6. Return the result rounded to the specified precision.\n        return round(ks_statistic, 6)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (K, p_vector, n, R, seed)\n        (5, [0.1, 0.2, 0.3, 0.25, 0.15], 50, 20000, 12345),\n        (5, [0.1, 0.2, 0.3, 0.25, 0.15], 200, 20000, 12345),\n        (5, [0.1, 0.2, 0.3, 0.25, 0.15], 2000, 20000, 12345),\n        (8, [1/8] * 8, 16, 20000, 67890),\n        (20, [1/20] * 20, 1000, 15000, 13579)\n    ]\n\n    results = []\n    for case in test_cases:\n        K, p, n, R, s = case\n        result = compute_ks_distance(K, p, n, R, s)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2405617"}]}