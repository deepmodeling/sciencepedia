{"hands_on_practices": [{"introduction": "构建分类树是一个贪婪的递归过程，其核心在于不断地分裂节点。在每一步，算法都必须选择能最大程度提升子节点“纯度”的分割方式。本练习提供了一个具体场景，让您亲手实践如何使用基尼不纯度（Gini impurity）这一基本指标来量化分割所带来的纯度提升，这是理解CART算法的关键。", "problem": "在一次使用分类与回归树（CART）的生物统计分析中，对于一个具有类别 $1$ 和 $2$ 的二元结果，考虑一个父节点，其包含 $n_1=20$ 个类别 $1$ 的观测值和 $n_2=30$ 个类别 $2$ 的观测值。一个候选划分将此父节点分割为左子节点（类别 $(1,2)$ 的计数为 $(15,5)$）和右子节点（类别 $(1,2)$ 的计数为 $(5,25)$）。使用基尼不纯度作为节点不纯度的度量，并采用标准的加权不纯度减少量作为评估划分的准则，计算该划分所实现的不纯度减少量。请将最终答案表示为精确分数，无需四舍五入。", "solution": "用户提供了一个生物统计学领域的问题，具体涉及分类与回归树（CART）。任务是使用基尼不纯度度量，计算将一个父节点划分为两个子节点的特定划分所导致的不纯度减少量。\n\n### 第一步：问题验证\n\n首先，我必须验证问题陈述。\n\n**提取已知条件：**\n*   **方法**：分类与回归树（CART）\n*   **输出**：二元的，类别标记为 $1$ 和 $2$。\n*   **父节点构成**：包含 $n_1 = 20$ 个类别 $1$ 的观测值和 $n_2 = 30$ 个类别 $2$ 的观测值。\n*   **划分定义**：\n    *   **左子节点构成**：$n_{L,1} = 15$（类别 $1$），$n_{L,2} = 5$（类别 $2$）。\n    *   **右子节点构成**：$n_{R,1} = 5$（类别 $1$），$n_{R,2} = 25$（类别 $2$）。\n*   **不纯度度量**：基尼不纯度。\n*   **评估准则**：加权不纯度减少量。\n\n**根据准则进行验证：**\n1.  **科学基础**：该问题在决策树理论中有坚实的基础，这是机器学习和生物统计学中的一个标准课题。基尼不纯度是分类树中广泛使用的一个度量标准。\n2.  **适定性**：该问题提供了计算唯一数值答案所需的所有数据和定义。对“精确分数”的要求是明确的。\n3.  **客观性**：问题以精确的、定量的术语陈述，没有任何主观性。\n4.  **一致性检查**：\n    *   父节点中的总观测值：$N_P = n_1 + n_2 = 20 + 30 = 50$。\n    *   左子节点中的总观测值：$N_L = n_{L,1} + n_{L,2} = 15 + 5 = 20$。\n    *   右子节点中的总观测值：$N_R = n_{R,1} + n_{R,2} = 5 + 25 = 30$。\n    *   观测值守恒：$N_L + N_R = 20 + 30 = 50 = N_P$。\n    *   类别 1 守恒：$n_{L,1} + n_{R,1} = 15 + 5 = 20 = n_1$。\n    *   类别 2 守恒：$n_{L,2} + n_{R,2} = 5 + 25 = 30 = n_2$。\n    数据是自洽的。\n\n**结论**：该问题是有效的、科学上合理的、适定的和内部一致的。我现在将继续进行求解。\n\n### 第二步：求解推导\n\n一个划分的不纯度减少量（也称为基尼增益）的计算方法是，父节点的不纯度减去子节点不纯度的加权平均值。\n不纯度减少量 $\\Delta I_{Gini}$ 的公式是：\n$$ \\Delta I_{Gini} = I_{Gini}(\\text{parent}) - \\left[ \\frac{N_L}{N_P} I_{Gini}(\\text{left}) + \\frac{N_R}{N_P} I_{Gini}(\\text{right}) \\right] $$\n其中 $I_{Gini}$ 是一个节点的基尼不纯度，$N_P$，$N_L$，$N_R$ 分别是父节点、左子节点和右子节点中的总观测值数量。\n\n对于一个有 $K$ 个类别的节点 $m$，其基尼不纯度定义为：\n$$ I_{Gini}(m) = 1 - \\sum_{k=1}^{K} p_{mk}^2 $$\n其中 $p_{mk}$ 是节点 $m$ 中类别 $k$ 的观测值所占的比例。对于这个二分类问题（$K=2$），公式为：\n$$ I_{Gini}(m) = 1 - (p_{m1}^2 + p_{m2}^2) $$\n\n**1. 计算父节点的基尼不纯度 ($I_{Gini}(\\text{parent})$)**\n父节点有 $N_P = 20 + 30 = 50$ 个观测值。\n各类别所占的比例为：\n*   $p_{P,1} = \\frac{n_1}{N_P} = \\frac{20}{50} = \\frac{2}{5}$\n*   $p_{P,2} = \\frac{n_2}{N_P} = \\frac{30}{50} = \\frac{3}{5}$\n\n父节点的基尼不纯度为：\n$$ I_{Gini}(\\text{parent}) = 1 - \\left[ \\left(\\frac{2}{5}\\right)^2 + \\left(\\frac{3}{5}\\right)^2 \\right] = 1 - \\left[ \\frac{4}{25} + \\frac{9}{25} \\right] = 1 - \\frac{13}{25} = \\frac{12}{25} $$\n\n**2. 计算左子节点的基尼不纯度 ($I_{Gini}(\\text{left})$)**\n左子节点有 $N_L = 15 + 5 = 20$ 个观测值。\n各类别所占的比例为：\n*   $p_{L,1} = \\frac{n_{L,1}}{N_L} = \\frac{15}{20} = \\frac{3}{4}$\n*   $p_{L,2} = \\frac{n_{L,2}}{N_L} = \\frac{5}{20} = \\frac{1}{4}$\n\n左子节点的基尼不纯度为：\n$$ I_{Gini}(\\text{left}) = 1 - \\left[ \\left(\\frac{3}{4}\\right)^2 + \\left(\\frac{1}{4}\\right)^2 \\right] = 1 - \\left[ \\frac{9}{16} + \\frac{1}{16} \\right] = 1 - \\frac{10}{16} = 1 - \\frac{5}{8} = \\frac{3}{8} $$\n\n**3. 计算右子节点的基尼不纯度 ($I_{Gini}(\\text{right})$)**\n右子节点有 $N_R = 5 + 25 = 30$ 个观测值。\n各类别所占的比例为：\n*   $p_{R,1} = \\frac{n_{R,1}}{N_R} = \\frac{5}{30} = \\frac{1}{6}$\n*   $p_{R,2} = \\frac{n_{R,2}}{N_R} = \\frac{25}{30} = \\frac{5}{6}$\n\n右子节点的基尼不纯度为：\n$$ I_{Gini}(\\text{right}) = 1 - \\left[ \\left(\\frac{1}{6}\\right)^2 + \\left(\\frac{5}{6}\\right)^2 \\right] = 1 - \\left[ \\frac{1}{36} + \\frac{25}{36} \\right] = 1 - \\frac{26}{36} = 1 - \\frac{13}{18} = \\frac{5}{18} $$\n\n**4. 计算不纯度减少量 ($\\Delta I_{Gini}$)**\n子节点的权重为：\n*   左子节点的权重：$\\frac{N_L}{N_P} = \\frac{20}{50} = \\frac{2}{5}$\n*   右子节点的权重：$\\frac{N_R}{N_P} = \\frac{30}{50} = \\frac{3}{5}$\n\n现在，将计算出的值代入不纯度减少量公式：\n$$ \\Delta I_{Gini} = I_{Gini}(\\text{parent}) - \\left[ \\frac{N_L}{N_P} I_{Gini}(\\text{left}) + \\frac{N_R}{N_P} I_{Gini}(\\text{right}) \\right] $$\n$$ \\Delta I_{Gini} = \\frac{12}{25} - \\left[ \\left(\\frac{2}{5}\\right) \\left(\\frac{3}{8}\\right) + \\left(\\frac{3}{5}\\right) \\left(\\frac{5}{18}\\right) \\right] $$\n$$ \\Delta I_{Gini} = \\frac{12}{25} - \\left[ \\frac{6}{40} + \\frac{15}{90} \\right] $$\n简化括号内的分数：\n$$ \\Delta I_{Gini} = \\frac{12}{25} - \\left[ \\frac{3}{20} + \\frac{1}{6} \\right] $$\n为了对分数进行减法运算，需要找到 $25$、$20$ 和 $6$ 的公分母。最小公倍数是 $\\text{lcm}(25, 20, 6) = \\text{lcm}(5^2, 2^2 \\cdot 5, 2 \\cdot 3) = 2^2 \\cdot 3 \\cdot 5^2 = 4 \\cdot 3 \\cdot 25 = 300$。\n首先，计算括号中的项：\n$$ \\frac{3}{20} + \\frac{1}{6} = \\frac{3 \\cdot 15}{20 \\cdot 15} + \\frac{1 \\cdot 50}{6 \\cdot 50} = \\frac{45}{300} + \\frac{50}{300} = \\frac{95}{300} $$\n或者，$\\text{lcm}(20, 6) = 60$。\n$$ \\frac{3}{20} + \\frac{1}{6} = \\frac{9}{60} + \\frac{10}{60} = \\frac{19}{60} $$\n现在将此结果代回主方程：\n$$ \\Delta I_{Gini} = \\frac{12}{25} - \\frac{19}{60} $$\n使用公分母 $300$：\n$$ \\Delta I_{Gini} = \\frac{12 \\cdot 12}{25 \\cdot 12} - \\frac{19 \\cdot 5}{60 \\cdot 5} = \\frac{144}{300} - \\frac{95}{300} = \\frac{144 - 95}{300} $$\n$$ \\Delta I_{Gini} = \\frac{49}{300} $$\n这个分数不能再化简，因为 $49 = 7^2$ 且 $300 = 3 \\cdot 100$，它们没有共同的因子。结果就是不纯度减少量的精确分数值。", "answer": "$$\\boxed{\\frac{49}{300}}$$", "id": "4962711"}, {"introduction": "除了计算不纯度的降低，理解树模型所创建的决策边界的几何性质也至关重要。本练习展示了一个经典的例子——异或（XOR）问题，它无法通过单次标准的、与坐标轴平行的分割来完美解决。通过解决这个问题，您将直观地理解标准CART模型的内在局限性，并体会到为何有时需要更复杂的分割或更深的决策树。", "problem": "一位生物统计研究员正在分类与回归树 (CART) 的框架下构建一个二元分类树，以使用两个连续的生物标志物 $x_{1}$ 和 $x_{2}$ 来区分两种表型（类别 $1$ 和类别 $0$）。经验训练集恰好由四个观测值组成：坐标为 $(0,0)$ 和 $(1,1)$ 的观测值被标记为类别 $1$，坐标为 $(0,1)$ 和 $(1,0)$ 的观测值被标记为类别 $0$。考虑一个单次分裂树，它将特征空间划分为两个终端节点，然后通过每个节点内的经验多数投票为该终端节点分配一个类别标签。经验错分率定义为：其真实类别标签与被分配到的终端节点标签不同的训练观测值所占的比例。\n\n正在考虑两种分裂类型：\n\n- 轴对齐分裂：对于某个特征索引 $j \\in \\{1,2\\}$ 和阈值 $s \\in \\mathbb{R}$，谓词形式为 $x_{j} \\leq s$。\n- 斜向分裂：对于不全为零的系数 $(a_{1},a_{2}) \\in \\mathbb{R}^{2}$ 和阈值 $t \\in \\mathbb{R}$，谓词形式为 $a_{1} x_{1} + a_{2} x_{2} \\leq t$。\n\n从 CART 分裂和经验错分风险的核心定义出发，通过对观测值的几何结构和这些分裂的结构进行推理，来解决以下问题：\n\n(i) 判断是否存在任何轴对齐分裂，能对该数据集产生零经验错分。\n\n(ii) 在所有上述形式的斜向分裂中，确定在通过经验多数投票为终端节点分配标签后可实现的最小经验错分率。\n\n请将你的最终答案表示为最佳斜向分裂下的最小经验错分率，形式为一个单独的数字，以精确分数表示（不带百分号），无需四舍五入。", "solution": "首先评估问题陈述的有效性。\n\n### 第 1 步：提取已知条件\n-   **主题**：基于树的方法：生物统计学中的分类与回归树 (CART)。\n-   **任务**：二元分类。\n-   **类别**：类别 $1$ 和类别 $0$。\n-   **特征**：两个连续的生物标志物，$x_{1}$ 和 $x_{2}$。\n-   **训练集**：四个观测值。\n    -   类别 $1$：坐标为 $(0,0)$ 和 $(1,1)$ 的点。\n    -   类别 $0$：坐标为 $(0,1)$ 和 $(1,0)$ 的点。\n-   **模型**：一个单次分裂树，将特征空间划分为两个终端节点。\n-   **节点标记规则**：每个终端节点内的经验多数投票。\n-   **评估指标**：经验错分率，定义为被终端节点标签错分的训练观测值的比例。\n-   **分裂类型**：\n    1.  轴对齐：对于 $j \\in \\{1,2\\}$ 和 $s \\in \\mathbb{R}$，$x_{j} \\leq s$。\n    2.  斜向：对于不全为零的 $(a_{1},a_{2}) \\in \\mathbb{R}^{2}$ 和 $t \\in \\mathbb{R}$，$a_{1} x_{1} + a_{2} x_{2} \\leq t$。\n-   **问题**：\n    -   (i) 判断是否有任何轴对齐分裂能实现零经验错分。\n    -   (ii) 确定所有斜向分裂中的最小经验错分率。\n-   **要求的最终答案**：最佳斜向分裂下的最小经验错分率。\n\n### 第 2 步：使用提取的已知条件进行验证\n1.  **科学依据**：该问题是机器学习和计算统计学中的一个典型例子，说明了简单线性分类器的局限性，以及与轴对齐决策树相比，斜向分裂决策树具有更强的表达能力。它基于明确定义的 CART 理论。\n2.  **良构性**：该问题是良构的。数据集是有限且明确的。分类规则和误差度量有精确的定义。在一个明确定义的函数集（线性划分）上搜索最小值是一个标准的优化问题，在这种离散情况下，保证存在一个可达到的最小值。\n3.  **客观性**：问题陈述使用了精确、客观的数学语言。\n4.  **缺陷**：该问题没有列出的任何缺陷。它不是不健全、不完整、矛盾、不切实际或提法不当的。它代表了在指定领域内的一个标准的、可形式化的问题（异或问题）。\n\n### 第 3 步：结论与行动\n问题是**有效的**。将提供完整解答。\n\n### 解答推导\n\n设训练数据集为 $D = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^4$，其中特征向量 $\\mathbf{x}_i \\in \\mathbb{R}^2$，类别标签 $y_i \\in \\{0, 1\\}$。四个数据点如下：\n-   $(\\mathbf{x}_1, y_1) = ((0,0), 1)$\n-   $(\\mathbf{x}_2, y_2) = ((1,1), 1)$\n-   $(\\mathbf{x}_3, y_3) = ((0,1), 0)$\n-   $(\\mathbf{x}_4, y_4) = ((1,0), 0)$\n总观测数为 $N=4$。经验错分率是错分点的数量除以 $N$。单次分裂树基于一个谓词将特征空间划分为两个区域 $R_L$ 和 $R_R$。每个区域中的点被赋予该区域的多数类别标签。\n\n**(i) 轴对齐分裂分析**\n\n轴对齐分裂由形式为 $x_j \\leq s$ 的谓词定义，其中 $j \\in \\{1, 2\\}$。这对应于特征空间中的一条垂直或水平线。\n\n让我们考虑对特征 $x_1$ 使用阈值 $s$ 进行分裂。数据点的 $x_1$ 坐标为 $0$ 和 $1$。唯一有意义的分裂发生在阈值 $s$ 介于 $0$ 和 $1$ 之间时，即 $s \\in [0, 1)$。对于任何这样的 $s$，例如 $s=0.5$，分裂线 $x_1 = s$ 将数据划分如下：\n-   区域 $R_L$ ($x_1 \\leq s$)：包含点 $(0,0)$（类别 $1$）和 $(0,1)$（类别 $0$）。\n-   区域 $R_R$ ($x_1  s$)：包含点 $(1,0)$（类别 $0$）和 $(1,1)$（类别 $1$）。\n\n在区域 $R_L$ 中，有一个类别 $1$ 的点和一个类别 $0$ 的点。计数为 $N_{L,1}=1$ 和 $N_{L,0}=1$。多数投票结果不明确。任何确定性的平局打破规则（例如，分配较小编号的类别标签，即类别 $0$）都将导致一个错分。例如，如果我们为 $R_L$ 分配标签 $0$，点 $(0,0)$ 就会被错分。$R_L$ 中的错分数量为 $1$。\n\n同样，在区域 $R_R$ 中，有一个类别 $1$ 的点和一个类别 $0$ 的点。计数为 $N_{R,1}=1$ 和 $N_{R,0}=1$。同样，多数投票将导致该区域出现一个错分。$R_R$ 中的错分数量为 $1$。\n\n对于任何在 $x_1$ 上的此类轴对齐分裂，总错分数量为 $1 + 1 = 2$。经验错分率为 $\\frac{2}{4} = \\frac{1}{2}$。\n根据对称性，对特征 $x_2$ 使用阈值 $s \\in [0, 1)$ 进行分裂也将导致两个混合类别的节点，每个节点包含一个类别 $1$ 的点和一个类别 $0$ 的点，总共导致 $2$ 个错分，错分率为 $\\frac{1}{2}$。\n\n阈值在数据范围之外的分裂（例如，对 $x_1$ 进行分裂时 $s  0$ 或 $s \\ge 1$）会将所有四个点都分到同一个区域。该区域包含两个类别 $1$ 的点和两个类别 $0$ 的点。任何多数投票标签都会错分两个点，得到错分率为 $\\frac{2}{4} = \\frac{1}{2}$。\n\n因此，没有轴对齐分裂可以实现零经验错分。轴对齐分裂的最小错分率为 $\\frac{1}{2}$。\n\n**(ii) 斜向分裂分析**\n\n斜向分裂由形式为 $a_1 x_1 + a_2 x_2 \\leq t$ 的谓词定义。这对应于一条任意方向的直线。\n\n首先，我们确定是否可以实现 $0$ 的错分率。错分率为 $0$ 意味着类别 $1$ 的数据点可以被一条直线与类别 $0$ 的数据点完全分开。这个性质被称为线性可分性。两个有限点集是线性可分的，当且仅当它们的凸包不相交。\n-   类别 $1$ 点集的凸包 $\\text{conv}(\\{(0,0), (1,1)\\})$ 是连接 $(0,0)$ 和 $(1,1)$ 的线段。\n-   类别 $0$ 点集的凸包 $\\text{conv}(\\{(0,1), (1,0)\\})$ 是连接 $(0,1)$ 和 $(1,0)$ 的线段。\n这两条线段相交于点 $(0.5, 0.5)$。由于凸包相交，所以这两个类别不是线性可分的。因此，没有斜向分裂可以实现 $0$ 的错分率。\n\n由于错分率 $0$ 是不可能的，对于大小为 $N=4$ 的数据集，最小可能的非零错分率为 $\\frac{1}{4}$（对应 $1$ 个错分点）。我们现在研究这个错分率是否可以实现。为了恰好实现一个错分，我们需要找到一个分裂，它创建的终端节点内的错分点总数为 $1$。\n\n考虑将四个数据点划分为一个大小为 $1$ 的集合和一个大小为 $3$ 的集合。这样的划分可以通过一个能隔离单个数据点的斜向分裂来实现。让我们构建一个隔离点 $(0,0)$（类别 $1$）的分裂。\n考虑谓词 $x_1 + x_2 \\leq c$，其中 $c$ 是某个小的正常数，例如 $c=0.1$。\n-   对于 $(0,0)$：$0+0 = 0 \\leq 0.1$。这个点在区域 $R_L$ 中。\n-   对于 $(1,0)$：$1+0 = 1  0.1$。这个点在区域 $R_R$ 中。\n-   对于 $(0,1)$：$0+1 = 1  0.1$。这个点在区域 $R_R$ 中。\n-   对于 $(1,1)$：$1+1 = 2  0.1$。这个点在区域 $R_R$ 中。\n\n该分裂将数据划分如下：\n-   区域 $R_L$ ($x_1+x_2 \\leq 0.1$)：只包含点 $(0,0)$（类别 $1$）。该节点是纯的。多数投票标签为 $1$。$R_L$ 中的错分数量为 $0$。\n-   区域 $R_R$ ($x_1+x_2  0.1$)：包含其余三个点：$(1,0)$（类别 $0$），$(0,1)$（类别 $0$）和 $(1,1)$（类别 $1$）。在这个区域中，有两个类别 $0$ 的点和一个类别 $1$ 的点。多数类别是 $0$。为 $R_R$ 分配的标签是 $0$。点 $(1,1)$ 被错分。$R_R$ 中的错分数量为 $1$。\n\n对于这个斜向分裂，总错分数量为 $0 + 1 = 1$。\n经验错分率为 $\\frac{1}{4}$。\n\n既然我们已经证明了错分率 $0$ 是不可能的，而错分率 $\\frac{1}{4}$ 是可以实现的，那么斜向分裂的最小经验错分率必定是 $\\frac{1}{4}$。任何其他划分类型，例如将点分成两组各两个，都会导致更高的错分率 $\\frac{1}{2}$，正如在轴对齐情况下所示。\n\n最终答案是可实现的最小错分率，即 $\\frac{1}{4}$。", "answer": "$$\\boxed{\\frac{1}{4}}$$", "id": "4962677"}, {"introduction": "一个完全生长的决策树通常过于复杂，容易对训练数据产生过拟合，导致其在预测新数据时表现不佳。剪枝是解决该问题的关键。本练习将指导您完成一个常见且实用的剪枝策略：利用交叉验证的结果和“一倍标准误规则”（1-SE rule）来选择最终模型。这项技术旨在模型的预测性能与简约性之间找到关键的平衡点，这也是统计建模的核心主题之一。", "problem": "一个生物统计学团队正在设计一个分类与回归树（CART）模型，以根据电子健康记录的特征来预测$30$天内重症监护室的死亡率。该模型使用成本复杂度剪枝进行修剪，其调整参数为$\\alpha$，较大的$\\alpha$值会产生更简单、终端节点更少的树。为了估计每个$\\alpha$的预测风险，该团队执行了$5$折交叉验证（CV），并记录了每个候选$\\alpha \\in \\{0, 0.005, 0.01, 0.02\\}$值的平均交叉验证风险及其估计的标准误差。\n\n对于按相同顺序排列的$\\alpha$，观测到的$5$折交叉验证风险为$\\{0.18, 0.16, 0.155, 0.158\\}$，相应的标准误差为$\\{0.01, 0.012, 0.013, 0.014\\}$。使用一倍标准误（1-SE）规则，选择要部署的$\\alpha$值。将选定的$\\alpha$值报告为单个数字。无需四舍五入。", "solution": "该问题是有效的。这是一个在生物统计学和机器学习领域中表述清晰、有科学依据的问题，提供了所有必要的数据，且没有内部矛盾。它要求应用一个标准的统计程序，即一倍标准误规则，来选择模型的调整参数。\n\n任务是使用一倍标准误（1-SE）规则，为CART模型选择成本复杂度调整参数$\\alpha$的最优值。问题提供了对四个候选$\\alpha$值进行$5$折交叉验证程序的结果。\n\n给定的数据如下：\n-   $\\alpha$的候选值：$\\{0, 0.005, 0.01, 0.02\\}$\n-   相应的平均交叉验证风险，我们记为$R(\\alpha)$：$\\{0.18, 0.16, 0.155, 0.158\\}$\n-   相应的平均风险标准误差，我们记为$SE(\\alpha)$：$\\{0.01, 0.012, 0.013, 0.014\\}$\n\n一倍标准误（1-SE）规则是一种启发式方法，用于选择一个简约模型，其性能与绝对最佳性能的模型几乎一样好。该规则包括以下步骤：\n\n1.  找出产生最小交叉验证风险$R_{min}$的模型。\n2.  将与此最小风险相对应的标准误差加到最小风险本身，以计算性能阈值。该阈值为$R_{target} = R_{min} + SE(R_{min})$。\n3.  选择最简单的模型（在本例中是$\\alpha$值最大的模型），其交叉验证风险小于或等于此阈值$R_{target}$。其前提是，较大的$\\alpha$会导致更简单、修剪程度更高的树，这通常是为了避免过拟合而更受青睐的选择。\n\n让我们将这些步骤应用于所提供的数据。\n\n首先，我们为每个$\\alpha$整理数据对$(R(\\alpha), SE(\\alpha))$：\n-   对于 $\\alpha = 0$：$R(0) = 0.18$, $SE(0) = 0.01$\n-   对于 $\\alpha = 0.005$：$R(0.005) = 0.16$, $SE(0.005) = 0.012$\n-   对于 $\\alpha = 0.01$：$R(0.01) = 0.155$, $SE(0.01) = 0.013$\n-   对于 $\\alpha = 0.02$：$R(0.02) = 0.158$, $SE(0.02) = 0.014$\n\n**第一步：找到最小的交叉验证风险。**\n通过检查风险值$\\{0.18, 0.16, 0.155, 0.158\\}$，最小风险为：\n$$R_{min} = 0.155$$\n这个最小风险出现在调整参数值为$\\alpha = 0.01$时。\n\n**第二步：计算目标风险阈值。**\n与最小风险模型相对应的标准误差是与$\\alpha = 0.01$相关联的那个，即$SE(0.01) = 0.013$。\n1-SE规则的阈值计算如下：\n$$R_{target} = R_{min} + SE(R_{min}) = 0.155 + 0.013 = 0.168$$\n\n**第三步：选择风险低于阈值的最简单模型。**\n我们必须找到$\\alpha$值最大（最简单）的模型，使其风险$R(\\alpha)$小于或等于$R_{target} = 0.168$。我们按复杂度递减（即$\\alpha$递增）的顺序检查候选模型。\n\n-   对于$\\alpha = 0$，风险为$R(0) = 0.18$。因为$0.18  0.168$，所以不选择此模型。\n-   对于$\\alpha = 0.005$，风险为$R(0.005) = 0.16$。因为$0.16 \\le 0.168$，所以此模型是一个候选模型。\n-   对于$\\alpha = 0.01$，风险为$R(0.01) = 0.155$。因为$0.155 \\le 0.168$，所以此模型是一个候选模型。\n-   对于$\\alpha = 0.02$，风险为$R(0.02) = 0.158$。因为$0.158 \\le 0.168$，所以此模型是一个候选模型。\n\n满足条件$R(\\alpha) \\le 0.168$的模型的$\\alpha$值集合为$\\{0.005, 0.01, 0.02\\}$。\n1-SE规则要求从此集合中选择最简单的模型。在成本复杂度剪枝中，较大的$\\alpha$对应于对复杂度的更高惩罚，从而产生更简单的树。因此，我们从候选集合中选择最大的$\\alpha$值。\n$$\n\\alpha_{selected} = \\max\\{0.005, 0.01, 0.02\\} = 0.02\n$$\n因此，根据一倍标准误规则，要部署的$\\alpha$值为$0.02$。这一选择倾向于一个更简单的模型，其性能仍在最佳性能模型的一倍标准误差范围内，这对于更好地泛化到新数据是理想的。", "answer": "$$\\boxed{0.02}$$", "id": "4962656"}]}