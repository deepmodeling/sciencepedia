{"hands_on_practices": [{"introduction": "统计软件使得拟合像多项逻辑回归这样的复杂模型看起来是瞬时完成的。这个练习旨在揭开这层抽象的面纱[@problem_id:4929771]。通过手动执行牛顿-拉弗森（Newton-Raphson）算法的一个步骤，你将具体理解模型是如何通过迭代找到最大似然估计的，并对得分向量和海森矩阵等核心概念建立起直观的认识。", "problem": "考虑一个基线类别多项式逻辑回归模型，该模型应用于一个玩具生物统计数据集，其中结果类别为 $K=3$ 个，预测变量为 $p=2$ 个。令类别 $3$ 为基线类别。对于具有预测向量 $x_i \\in \\mathbb{R}^2$ 的观测值 $i$，定义类别 $k=1,2$ 的线性预测变量为 $\\eta_{ik} = x_i^{\\top}\\beta_k$，其中 $\\beta_k \\in \\mathbb{R}^2$，类别概率由下式给出\n$$\np_{ik} = \\frac{\\exp(\\eta_{ik})}{1 + \\exp(\\eta_{i1}) + \\exp(\\eta_{i2})}, \\quad k=1,2, \\quad \\text{和} \\quad p_{i3} = \\frac{1}{1 + \\exp(\\eta_{i1}) + \\exp(\\eta_{i2})}.\n$$\n令 $y_{ik} \\in \\{0,1\\}$ 表示观测值 $i$ 属于类别 $k$ 的指示变量，其中 $k=1,2,3$，且 $\\sum_{k=1}^{3} y_{ik} = 1$。对于 $n$ 个观测值，其对数似然函数为\n$$\n\\ell(\\beta_1,\\beta_2) = \\sum_{i=1}^{n} \\sum_{k=1}^{3} y_{ik} \\ln(p_{ik}).\n$$\n从最大似然和上述模型的定义出发，推导一步牛顿-拉弗森（NR）或迭代重加权最小二乘（IRLS）更新所需的得分向量和负海森矩阵。然后，在初始参数值 $\\beta_1 = (0,0)^{\\top}$ 和 $\\beta_2 = (0,0)^{\\top}$ 下，对以下包含 $n=3$ 个观测值的玩具数据集实施此一步更新：\n- 预测变量：$x_1 = (1,0)^{\\top}$, $x_2 = (0,1)^{\\top}$, $x_3 = (1,1)^{\\top}$。\n- 观测类别：$y_1$ 是类别 $1$，$y_2$ 是类别 $2$，$y_3$ 是类别 $3$。\n\n在初始参数下，明确计算：\n- 每个观测值 $i$ 的工作残差 $r_i = y_i^{*} - p_i$，其中 $y_i^{*} = (y_{i1}, y_{i2})^{\\top}$ 且 $p_i = (p_{i1}, p_{i2})^{\\top}$。\n- 每个观测值 $i$ 的权重矩阵 $W_i = \\operatorname{diag}(p_i) - p_i p_i^{\\top}$。\n使用这些量，组装块负海森矩阵和得分向量，求解满足 $H \\Delta = U$ 的牛顿步长 $\\Delta = (\\Delta\\beta_1^{\\top}, \\Delta\\beta_2^{\\top})^{\\top} \\in \\mathbb{R}^{4}$，并精确报告 $\\Delta$ 的四个分量。无需四舍五入。将您的最终答案以单个 $1 \\times 4$ 行矩阵的形式提供。", "solution": "用户要求我解决一个涉及多项式逻辑回归的生物统计学问题。\n\n我的计划如下：\n\n1.  **验证问题：**\n    *   **第 1 步：提取已知条件：** 我将列出所有给定的公式、定义、数据点和初始条件。\n        *   模型：基线类别多项式逻辑回归。\n        *   类别数：$K=3$。\n        *   预测变量数：$p=2$。\n        *   基线类别：类别 $3$。\n        *   线性预测变量：$\\eta_{ik} = x_i^{\\top}\\beta_k$ 对于 $k=1,2$。\n        *   概率：\n            *   $p_{ik} = \\frac{\\exp(\\eta_{ik})}{1 + \\exp(\\eta_{i1}) + \\exp(\\eta_{i2})}$ 对于 $k=1,2$。\n            *   $p_{i3} = \\frac{1}{1 + \\exp(\\eta_{i1}) + \\exp(\\eta_{i2})}$。\n        *   结果指示变量：$y_{ik} \\in \\{0,1\\}$，$\\sum_{k=1}^{3} y_{ik} = 1$。\n        *   对数似然：$\\ell(\\beta_1,\\beta_2) = \\sum_{i=1}^{n} \\sum_{k=1}^{3} y_{ik} \\ln(p_{ik})$。\n        *   任务：推导得分向量和负海森矩阵，然后执行一步牛顿-拉弗森更新。\n        *   初始参数：$\\beta_1 = (0,0)^{\\top}$，$\\beta_2 = (0,0)^{\\top}$。\n        *   数据集 ($n=3$)：\n            *   预测变量：$x_1 = (1,0)^{\\top}$，$x_2 = (0,1)^{\\top}$，$x_3 = (1,1)^{\\top}$。\n            *   结果：$y_1$ 是类别 1，$y_2$ 是类别 2，$y_3$ 是类别 3。这意味着：\n                *   对于 $i=1$：$y_{11}=1, y_{12}=0, y_{13}=0$。\n                *   对于 $i=2$：$y_{21}=0, y_{22}=1, y_{23}=0$。\n                *   对于 $i=3$：$y_{31}=0, y_{32}=0, y_{33}=1$。\n        *   在初始参数下需要计算的具体量：\n            *   工作残差 $r_i = y_i^{*} - p_i$，其中 $y_i^{*} = (y_{i1}, y_{i2})^{\\top}$ 和 $p_i = (p_{i1}, p_{i2})^{\\top}$。\n            *   权重矩阵 $W_i = \\operatorname{diag}(p_i) - p_i p_i^{\\top}$。\n        *   目标：求解 $H \\Delta = U$ 得到 $\\Delta = (\\Delta\\beta_1^{\\top}, \\Delta\\beta_2^{\\top})^{\\top} \\in \\mathbb{R}^{4}$ 并报告 $\\Delta$ 的四个分量。\n\n    *   **第 2 步：使用提取的已知条件进行验证：**\n        *   科学性：是的，多项式逻辑回归是一种标准的统计模型。概率、对数似然、得分和海森矩阵的公式在广义线性模型理论中是成熟的。该任务是牛顿-拉弗森算法在寻找最大似然估计中的一个标准应用。\n        *   良态性：是的，该问题提供了执行一步牛顿-拉弗森算法所需的所有必要数据和初始条件。只要海森矩阵是可逆的，这一步解的存在性和唯一性就得到保证。我需要检查这一点。\n        *   目标明确：是的，语言精确，需要计算的量定义清晰。\n        *   未检测到缺陷。问题是有效的。\n\n    *   **第 3 步：结论和行动：** 问题是有效的。我将继续进行求解。\n\n2.  **解决问题：**\n\n    *   **第 1 部分：一般推导（得分向量和海森矩阵）**\n        *   令 $\\boldsymbol{\\beta} = (\\beta_1^{\\top}, \\beta_2^{\\top})^{\\top}$ 为所有参数的向量。它是一个 $4 \\times 1$ 的向量。\n        *   对数似然可以重写为。\n            $$ \\ell = \\sum_{i=1}^n \\left( y_{i1} \\eta_{i1} + y_{i2} \\eta_{i2} - \\ln(1 + \\exp(\\eta_{i1}) + \\exp(\\eta_{i2})) \\right) $$\n            这是通过将 $p_{ik}$ 的表达式代入对数似然得到的：\n            $$ \\sum_{k=1}^3 y_{ik} \\ln(p_{ik}) = y_{i1} (\\eta_{i1} - D_i) + y_{i2} (\\eta_{i2} - D_i) + y_{i3} (0 - D_i) $$\n            其中 $D_i = \\ln(1 + \\exp(\\eta_{i1}) + \\exp(\\eta_{i2}))$。\n            使用 $\\sum_k y_{ik} = 1$，这可以简化为：\n            $$ y_{i1} \\eta_{i1} + y_{i2} \\eta_{i2} - (y_{i1} + y_{i2} + y_{i3})D_i = y_{i1} \\eta_{i1} + y_{i2} \\eta_{i2} - D_i $$\n            这与我重写的形式相匹配。\n        *   现在，我需要找到得分向量 $U$，即 $\\ell$ 关于 $\\boldsymbol{\\beta}$ 的梯度。得分向量有两个块分量，$U_1 = \\frac{\\partial \\ell}{\\partial \\beta_1}$ 和 $U_2 = \\frac{\\partial \\ell}{\\partial \\beta_2}$。\n        *   我们来求 $\\ell$ 关于 $\\beta_{1j}$ 和 $\\beta_{2j}$（$\\beta_1$ 和 $\\beta_2$ 的第 $j$ 个分量）的导数。\n        *   我们有 $\\eta_{i1} = x_i^{\\top}\\beta_1$ 和 $\\eta_{i2} = x_i^{\\top}\\beta_2$。\n        *   $\\frac{\\partial \\eta_{i1}}{\\partial \\beta_1} = x_i$ 且 $\\frac{\\partial \\eta_{i1}}{\\partial \\beta_2} = 0$。\n        *   $\\frac{\\partial \\eta_{i2}}{\\partial \\beta_1} = 0$ 且 $\\frac{\\partial \\eta_{i2}}{\\partial \\beta_2} = x_i$。\n        *   还需要 $D_i$ 的导数：\n            *   $\\frac{\\partial D_i}{\\partial \\eta_{i1}} = \\frac{\\exp(\\eta_{i1})}{1+\\dots} = p_{i1}$。\n            *   $\\frac{\\partial D_i}{\\partial \\eta_{i2}} = \\frac{\\exp(\\eta_{i2})}{1+\\dots} = p_{i2}$。\n        *   现在来看得分分量：\n            *   $\\frac{\\partial \\ell}{\\partial \\beta_1} = \\sum_{i=1}^n \\left( y_{i1} \\frac{\\partial \\eta_{i1}}{\\partial \\beta_1} - \\frac{\\partial D_i}{\\partial \\eta_{i1}}\\frac{\\partial \\eta_{i1}}{\\partial \\beta_1} - \\frac{\\partial D_i}{\\partial \\eta_{i2}}\\frac{\\partial \\eta_{i2}}{\\partial \\beta_1} \\right) = \\sum_{i=1}^n (y_{i1} x_i - p_{i1} x_i - p_{i2} \\cdot 0) = \\sum_{i=1}^n (y_{i1} - p_{i1})x_i$。\n            *   $\\frac{\\partial \\ell}{\\partial \\beta_2} = \\sum_{i=1}^n \\left( y_{i2} \\frac{\\partial \\eta_{i2}}{\\partial \\beta_2} - \\frac{\\partial D_i}{\\partial \\eta_{i1}}\\frac{\\partial \\eta_{i1}}{\\partial \\beta_2} - \\frac{\\partial D_i}{\\partial \\eta_{i2}}\\frac{\\partial \\eta_{i2}}{\\partial \\beta_2} \\right) = \\sum_{i=1}^n (y_{i2} x_i - p_{i1} \\cdot 0 - p_{i2} x_i) = \\sum_{i=1}^n (y_{i2} - p_{i2})x_i$。\n        *   所以得分向量 $U$ 是 $U = \\begin{pmatrix} U_1 \\\\ U_2 \\end{pmatrix} = \\begin{pmatrix} \\sum_i (y_{i1} - p_{i1})x_i \\\\ \\sum_i (y_{i2} - p_{i2})x_i \\end{pmatrix}$。这可以写成矩阵形式。令 $X$ 为设计矩阵（$n \\times p$），但在这里它更复杂。我们使用给定的符号 $r_i = y_i^* - p_i$。那么 $U = \\sum_{i=1}^n \\begin{pmatrix} r_{i1} x_i \\\\ r_{i2} x_i \\end{pmatrix}$。这可以写成 $\\sum_i (x_i \\otimes r_i)$，其中 $\\otimes$ 是克罗内克积。\n        *   更简洁地，如果我们为每个观测值定义一个设计矩阵 $X_i = \\operatorname{diag}(x_i, x_i) = I_2 \\otimes x_i$，这是一个 $2p \\times 2$ 矩阵（这里是 $4 \\times 2$），那么 $U = \\sum_i X_i^{\\top} r_i$ 并不完全正确，因为矩阵维度不匹配。$X_i$ 可能应该是 $I_{K-1} \\otimes x_i^{\\top}$。我们还是坚持使用求和形式。$U = \\sum_i \\begin{pmatrix} x_i(y_{i1}-p_{i1}) \\\\ x_i(y_{i2}-p_{i2}) \\end{pmatrix}$。\n\n        *   接下来是海森矩阵 $\\mathcal{H}$。它是二阶导数矩阵。负海森矩阵是费雪信息矩阵，$H = -\\mathcal{H}$。\n        *   $\\mathcal{H} = \\begin{pmatrix} \\frac{\\partial^2 \\ell}{\\partial \\beta_1 \\partial \\beta_1^{\\top}} & \\frac{\\partial^2 \\ell}{\\partial \\beta_1 \\partial \\beta_2^{\\top}} \\\\ \\frac{\\partial^2 \\ell}{\\partial \\beta_2 \\partial \\beta_1^{\\top}} & \\frac{\\partial^2 \\ell}{\\partial \\beta_2 \\partial \\beta_2^{\\top}} \\end{pmatrix}$。\n        *   我们来求得分分量的导数。\n            *   $\\frac{\\partial}{\\partial \\beta_1^{\\top}} (\\sum_i (y_{i1} - p_{i1})x_i) = -\\sum_i x_i \\frac{\\partial p_{i1}}{\\partial \\beta_1^{\\top}}$。\n            *   我们需要 $\\frac{\\partial p_{i1}}{\\partial \\beta_1^{\\top}}$。$p_{i1} = \\exp(\\eta_{i1}) / D_i'$，其中 $D_i' = 1 + \\exp(\\eta_{i1}) + \\exp(\\eta_{i2})$。\n            *   $\\frac{\\partial p_{i1}}{\\partial \\eta_{i1}} = \\frac{\\exp(\\eta_{i1})D_i' - \\exp(\\eta_{i1})\\exp(\\eta_{i1})}{(D_i')^2} = p_{i1}(1-p_{i1})$。\n            *   $\\frac{\\partial p_{i1}}{\\partial \\eta_{i2}} = \\frac{-\\exp(\\eta_{i1})\\exp(\\eta_{i2})}{(D_i')^2} = -p_{i1}p_{i2}$。\n            *   所以，$\\frac{\\partial p_{i1}}{\\partial \\beta_1^{\\top}} = \\frac{\\partial p_{i1}}{\\partial \\eta_{i1}} \\frac{\\partial \\eta_{i1}}{\\partial \\beta_1^{\\top}} + \\frac{\\partial p_{i1}}{\\partial \\eta_{i2}} \\frac{\\partial \\eta_{i2}}{\\partial \\beta_1^{\\top}} = p_{i1}(1-p_{i1}) x_i^{\\top}$。\n            *   $\\frac{\\partial^2 \\ell}{\\partial \\beta_1 \\partial \\beta_1^{\\top}} = -\\sum_i x_i (p_{i1}(1-p_{i1})) x_i^{\\top} = -\\sum_i p_{i1}(1-p_{i1}) x_i x_i^{\\top}$。\n            *   现在来看非对角块：$\\frac{\\partial^2 \\ell}{\\partial \\beta_1 \\partial \\beta_2^{\\top}} = \\frac{\\partial}{\\partial \\beta_2^{\\top}} (\\sum_i (y_{i1} - p_{i1})x_i) = -\\sum_i x_i \\frac{\\partial p_{i1}}{\\partial \\beta_2^{\\top}}$。\n            *   $\\frac{\\partial p_{i1}}{\\partial \\beta_2^{\\top}} = \\frac{\\partial p_{i1}}{\\partial \\eta_{i1}} \\frac{\\partial \\eta_{i1}}{\\partial \\beta_2^{\\top}} + \\frac{\\partial p_{i1}}{\\partial \\eta_{i2}} \\frac{\\partial \\eta_{i2}}{\\partial \\beta_2^{\\top}} = -p_{i1}p_{i2} x_i^{\\top}$。\n            *   $\\frac{\\partial^2 \\ell}{\\partial \\beta_1 \\partial \\beta_2^{\\top}} = -\\sum_i x_i (-p_{i1}p_{i2}) x_i^{\\top} = \\sum_i p_{i1}p_{i2} x_i x_i^{\\top}$。\n            *   根据对称性，$\\frac{\\partial^2 \\ell}{\\partial \\beta_2 \\partial \\beta_1^{\\top}} = (\\frac{\\partial^2 \\ell}{\\partial \\beta_1 \\partial \\beta_2^{\\top}})^{\\top} = \\sum_i p_{i1}p_{i2} x_i x_i^{\\top}$。\n            *   最后，最后一个块：$\\frac{\\partial^2 \\ell}{\\partial \\beta_2 \\partial \\beta_2^{\\top}} = \\frac{\\partial}{\\partial \\beta_2^{\\top}} (\\sum_i (y_{i2} - p_{i2})x_i) = -\\sum_i x_i \\frac{\\partial p_{i2}}{\\partial \\beta_2^{\\top}}$。\n            *   $\\frac{\\partial p_{i2}}{\\partial \\eta_{i1}} = -p_{i1}p_{i2}$。\n            *   $\\frac{\\partial p_{i2}}{\\partial \\eta_{i2}} = p_{i2}(1-p_{i2})$。\n            *   $\\frac{\\partial p_{i2}}{\\partial \\beta_2^{\\top}} = \\frac{\\partial p_{i2}}{\\partial \\eta_{i1}} \\frac{\\partial \\eta_{i1}}{\\partial \\beta_2^{\\top}} + \\frac{\\partial p_{i2}}{\\partial \\eta_{i2}} \\frac{\\partial \\eta_{i2}}{\\partial \\beta_2^{\\top}} = p_{i2}(1-p_{i2}) x_i^{\\top}$。\n            *   $\\frac{\\partial^2 \\ell}{\\partial \\beta_2 \\partial \\beta_2^{\\top}} = -\\sum_i x_i (p_{i2}(1-p_{i2})) x_i^{\\top} = -\\sum_i p_{i2}(1-p_{i2}) x_i x_i^{\\top}$。\n        *   所以海森矩阵是 $\\mathcal{H} = \\sum_i \\begin{pmatrix} -p_{i1}(1-p_{i1}) x_i x_i^{\\top} & p_{i1}p_{i2} x_i x_i^{\\top} \\\\ p_{i1}p_{i2} x_i x_i^{\\top} & -p_{i2}(1-p_{i2}) x_i x_i^{\\top} \\end{pmatrix}$。\n        *   负海森矩阵 $H = -\\mathcal{H}$ 是：\n            $$ H = \\sum_i \\begin{pmatrix} p_{i1}(1-p_{i1}) x_i x_i^{\\top} & -p_{i1}p_{i2} x_i x_i^{\\top} \\\\ -p_{i1}p_{i2} x_i x_i^{\\top} & p_{i2}(1-p_{i2}) x_i x_i^{\\top} \\end{pmatrix} $$\n        *   这可以用权重矩阵 $W_i$ 来写。问题定义了 $W_i = \\operatorname{diag}(p_i) - p_i p_i^{\\top}$，其中 $p_i = (p_{i1}, p_{i2})^{\\top}$。\n        *   我们来看一下 $W_i$ 的具体形式：$W_i = \\begin{pmatrix} p_{i1} & 0 \\\\ 0 & p_{i2} \\end{pmatrix} - \\begin{pmatrix} p_{i1} \\\\ p_{i2} \\end{pmatrix} \\begin{pmatrix} p_{i1} & p_{i2} \\end{pmatrix} = \\begin{pmatrix} p_{i1} - p_{i1}^2 & -p_{i1}p_{i2} \\\\ -p_{i1}p_{i2} & p_{i2} - p_{i2}^2 \\end{pmatrix} = \\begin{pmatrix} p_{i1}(1-p_{i1}) & -p_{i1}p_{i2} \\\\ -p_{i1}p_{i2} & p_{i2}(1-p_{i2}) \\end{pmatrix}$。\n        *   这正是 $H$ 的求和式中的矩阵，以分块方式前乘和后乘 $x_i$。\n        *   $H$ 求和式中的分块矩阵是 $\\begin{pmatrix} (W_i)_{11} x_i x_i^{\\top} & (W_i)_{12} x_i x_i^{\\top} \\\\ (W_i)_{21} x_i x_i^{\\top} & (W_i)_{22} x_i x_i^{\\top} \\end{pmatrix}$。这是克罗内克积 $W_i \\otimes (x_i x_i^{\\top})$。\n        *   所以，$H = \\sum_i W_i \\otimes (x_i x_i^{\\top})$。这证实了一般形式。\n\n    *   **第 2 部分：一步 NR 更新的计算**\n        *   需要求解的方程是 $H \\Delta = U$。\n        *   更新公式为 $\\boldsymbol{\\beta}^{(1)} = \\boldsymbol{\\beta}^{(0)} + \\Delta$。我们需要求出 $\\Delta$。\n        *   **步骤 2a：在初始参数处求值**\n            *   给定 $\\beta_1^{(0)} = (0,0)^{\\top}$ 和 $\\beta_2^{(0)} = (0,0)^{\\top}$。\n            *   首先，为每个观测值 $i=1,2,3$ 计算 $\\eta_{ik}$ 和 $p_{ik}$。\n            *   对于任何 $i$，$\\eta_{i1} = x_i^{\\top}\\beta_1^{(0)} = x_i^{\\top}(0,0)^{\\top} = 0$。\n            *   对于任何 $i$，$\\eta_{i2} = x_i^{\\top}\\beta_2^{(0)} = x_i^{\\top}(0,0)^{\\top} = 0$。\n            *   所以，对于所有 $i$，分母是 $1 + \\exp(0) + \\exp(0) = 1+1+1=3$。\n            *   概率：\n                *   $p_{i1} = \\exp(0)/3 = 1/3$。\n                *   $p_{i2} = \\exp(0)/3 = 1/3$。\n                *   $p_{i3} = 1/3$。\n            *   所以，对于所有 $i$，$p_i = (p_{i1}, p_{i2})^{\\top} = (1/3, 1/3)^{\\top}$。\n\n        *   **步骤 2b：计算工作残差 $r_i$**\n            *   $r_i = y_i^* - p_i$。\n            *   数据：\n                *   $i=1$：类别 1。$y_1^* = (1, 0)^{\\top}$。\n                *   $i=2$：类别 2。$y_2^* = (0, 1)^{\\top}$。\n                *   $i=3$：类别 3。$y_3^* = (0, 0)^{\\top}$。\n            *   残差：\n                *   $r_1 = (1, 0)^{\\top} - (1/3, 1/3)^{\\top} = (2/3, -1/3)^{\\top}$。\n                *   $r_2 = (0, 1)^{\\top} - (1/3, 1/3)^{\\top} = (-1/3, 2/3)^{\\top}$。\n                *   $r_3 = (0, 0)^{\\top} - (1/3, 1/3)^{\\top} = (-1/3, -1/3)^{\\top}$。\n\n        *   **步骤 2c：计算权重矩阵 $W_i$**\n            *   $W_i = \\operatorname{diag}(p_i) - p_i p_i^{\\top}$。\n            *   由于对于所有 $i$ 都有 $p_i = (1/3, 1/3)^{\\top}$，所以权重矩阵 $W_i$ 对所有 $i$ 都是相同的。我们称之为 $W$。\n            *   $p_i p_i^{\\top} = \\begin{pmatrix} 1/3 \\\\ 1/3 \\end{pmatrix} \\begin{pmatrix} 1/3 & 1/3 \\end{pmatrix} = \\begin{pmatrix} 1/9 & 1/9 \\\\ 1/9 & 1/9 \\end{pmatrix}$。\n            *   $\\operatorname{diag}(p_i) = \\begin{pmatrix} 1/3 & 0 \\\\ 0 & 1/3 \\end{pmatrix}$。\n            *   $W = \\begin{pmatrix} 1/3 & 0 \\\\ 0 & 1/3 \\end{pmatrix} - \\begin{pmatrix} 1/9 & 1/9 \\\\ 1/9 & 1/9 \\end{pmatrix} = \\begin{pmatrix} 2/9 & -1/9 \\\\ -1/9 & 2/9 \\end{pmatrix} = \\frac{1}{9}\\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix}$。\n            *   所以 $W_1 = W_2 = W_3 = W = \\frac{1}{9}\\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix}$。\n\n        *   **步骤 2d：组装得分向量 $U$**\n            *   $U = \\sum_{i=1}^3 \\begin{pmatrix} (y_{i1}-p_{i1})x_i \\\\ (y_{i2}-p_{i2})x_i \\end{pmatrix} = \\sum_{i=1}^3 \\begin{pmatrix} r_{i1} x_i \\\\ r_{i2} x_i \\end{pmatrix}$。\n            *   预测变量：$x_1 = (1,0)^{\\top}$, $x_2 = (0,1)^{\\top}$, $x_3 = (1,1)^{\\top}$。\n            *   对于 $i=1$ 的项：$r_1 = (2/3, -1/3)^{\\top}$。\n                *   $\\begin{pmatrix} (2/3)x_1 \\\\ (-1/3)x_1 \\end{pmatrix} = \\begin{pmatrix} 2/3 (1,0)^{\\top} \\\\ -1/3 (1,0)^{\\top} \\end{pmatrix} = (2/3, 0, -1/3, 0)^{\\top}$。\n            *   对于 $i=2$ 的项：$r_2 = (-1/3, 2/3)^{\\top}$。\n                *   $\\begin{pmatrix} (-1/3)x_2 \\\\ (2/3)x_2 \\end{pmatrix} = \\begin{pmatrix} -1/3 (0,1)^{\\top} \\\\ 2/3 (0,1)^{\\top} \\end{pmatrix} = (0, -1/3, 0, 2/3)^{\\top}$。\n            *   对于 $i=3$ 的项：$r_3 = (-1/3, -1/3)^{\\top}$。\n                *   $\\begin{pmatrix} (-1/3)x_3 \\\\ (-1/3)x_3 \\end{pmatrix} = \\begin{pmatrix} -1/3 (1,1)^{\\top} \\\\ -1/3 (1,1)^{\\top} \\end{pmatrix} = (-1/3, -1/3, -1/3, -1/3)^{\\top}$。\n            *   将它们相加：\n                *   $U = (2/3, 0, -1/3, 0)^{\\top} + (0, -1/3, 0, 2/3)^{\\top} + (-1/3, -1/3, -1/3, -1/3)^{\\top}$。\n                *   $U_1 = 2/3 + 0 - 1/3 = 1/3$。\n                *   $U_2 = 0 - 1/3 - 1/3 = -2/3$。\n                *   $U_3 = -1/3 + 0 - 1/3 = -2/3$。\n                *   $U_4 = 0 + 2/3 - 1/3 = 1/3$。\n                *   $U = (1/3, -2/3, -2/3, 1/3)^{\\top}$。\n\n        *   **步骤 2e：组装负海森矩阵 $H$**\n            *   $H = \\sum_{i=1}^3 W_i \\otimes (x_i x_i^{\\top})$。\n            *   $W_i = W = \\frac{1}{9}\\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix}$。\n            *   预测变量外积：\n                *   $x_1 x_1^{\\top} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}$。\n                *   $x_2 x_2^{\\top} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}$。\n                *   $x_3 x_3^{\\top} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix}$。\n            *   对于 $i=1$ 的项：$W \\otimes (x_1 x_1^{\\top})$。\n                *   $\\frac{1}{9}\\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix} \\otimes \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} = \\frac{1}{9} \\begin{pmatrix} 2\\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} & -1\\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\\\ -1\\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} & 2\\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\end{pmatrix} = \\frac{1}{9} \\begin{pmatrix} 2 & 0 & -1 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ -1 & 0 & 2 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}$。\n            *   对于 $i=2$ 的项：$W \\otimes (x_2 x_2^{\\top})$。\n                *   $\\frac{1}{9}\\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix} \\otimes \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\frac{1}{9} \\begin{pmatrix} 2\\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} & -1\\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} \\\\ -1\\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} & 2\\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} \\end{pmatrix} = \\frac{1}{9} \\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ 0 & 2 & 0 & -1 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & -1 & 0 & 2 \\end{pmatrix}$。\n            *   对于 $i=3$ 的项：$W \\otimes (x_3 x_3^{\\top})$。\n                *   $\\frac{1}{9}\\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix} \\otimes \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\frac{1}{9} \\begin{pmatrix} 2\\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} & -1\\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} \\\\ -1\\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} & 2\\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} \\end{pmatrix} = \\frac{1}{9} \\begin{pmatrix} 2 & 2 & -1 & -1 \\\\ 2 & 2 & -1 & -1 \\\\ -1 & -1 & 2 & 2 \\\\ -1 & -1 & 2 & 2 \\end{pmatrix}$。\n            *   将它们相加得到 $H$：\n                *   $H = \\frac{1}{9} \\left[ \\begin{pmatrix} 2 & 0 & -1 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ -1 & 0 & 2 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix} + \\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ 0 & 2 & 0 & -1 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & -1 & 0 & 2 \\end{pmatrix} + \\begin{pmatrix} 2 & 2 & -1 & -1 \\\\ 2 & 2 & -1 & -1 \\\\ -1 & -1 & 2 & 2 \\\\ -1 & -1 & 2 & 2 \\end{pmatrix} \\right]$\n                *   $H = \\frac{1}{9} \\begin{pmatrix} 2+0+2 & 0+0+2 & -1+0-1 & 0+0-1 \\\\ 0+0+2 & 0+2+2 & 0+0-1 & 0-1-1 \\\\ -1+0-1 & 0+0-1 & 2+0+2 & 0+0+2 \\\\ 0+0-1 & 0-1-1 & 0+0+2 & 0+2+2 \\end{pmatrix} = \\frac{1}{9} \\begin{pmatrix} 4 & 2 & -2 & -1 \\\\ 2 & 4 & -1 & -2 \\\\ -2 & -1 & 4 & 2 \\\\ -1 & -2 & 2 & 4 \\end{pmatrix}$。\n            *   我们来检查一下 $H$ 的结构。它应该是 $\\begin{pmatrix} H_{11} & H_{12} \\\\ H_{21} & H_{22} \\end{pmatrix}$。\n                *   $H_{11} = \\sum_i p_{i1}(1-p_{i1}) x_i x_i^{\\top} = \\frac{2}{9} \\sum_i x_i x_i^{\\top}$。\n                *   $H_{22} = \\sum_i p_{i2}(1-p_{i2}) x_i x_i^{\\top} = \\frac{2}{9} \\sum_i x_i x_i^{\\top}$。\n                *   $H_{12} = -\\sum_i p_{i1}p_{i2} x_i x_i^{\\top} = -\\frac{1}{9} \\sum_i x_i x_i^{\\top}$。\n                *   $H_{21} = H_{12}^{\\top} = H_{12}$ 因为 $x_i x_i^{\\top}$ 是对称的。\n                *   $\\sum_i x_i x_i^{\\top} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}$。\n                *   $H_{11} = H_{22} = \\frac{2}{9}\\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} = \\frac{1}{9}\\begin{pmatrix} 4 & 2 \\\\ 2 & 4 \\end{pmatrix}$。\n                *   $H_{12} = H_{21} = -\\frac{1}{9}\\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} = \\frac{1}{9}\\begin{pmatrix} -2 & -1 \\\\ -1 & -2 \\end{pmatrix}$。\n                *   组合起来：\n                    $H = \\frac{1}{9} \\begin{pmatrix} 4 & 2 & -2 & -1 \\\\ 2 & 4 & -1 & -2 \\\\ -2 & -1 & 4 & 2 \\\\ -1 & -2 & 2 & 4 \\end{pmatrix}$。这与我之前的计算相符。很好。\n\n        *   **步骤 2f：求解线性系统 $H \\Delta = U$**\n            *   $\\frac{1}{9} \\begin{pmatrix} 4 & 2 & -2 & -1 \\\\ 2 & 4 & -1 & -2 \\\\ -2 & -1 & 4 & 2 \\\\ -1 & -2 & 2 & 4 \\end{pmatrix} \\begin{pmatrix} \\Delta_1 \\\\ \\Delta_2 \\\\ \\Delta_3 \\\\ \\Delta_4 \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} 1 \\\\ -2 \\\\ -2 \\\\ 1 \\end{pmatrix}$。\n            *   两边乘以 9：\n                $\\begin{pmatrix} 4 & 2 & -2 & -1 \\\\ 2 & 4 & -1 & -2 \\\\ -2 & -1 & 4 & 2 \\\\ -1 & -2 & 2 & 4 \\end{pmatrix} \\begin{pmatrix} \\Delta_1 \\\\ \\Delta_2 \\\\ \\Delta_3 \\\\ \\Delta_4 \\end{pmatrix} = 3 \\begin{pmatrix} 1 \\\\ -2 \\\\ -2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -6 \\\\ -6 \\\\ 3 \\end{pmatrix}$。\n            *   我们称该矩阵为 $\\tilde{H}$。$\\tilde{H} \\Delta = \\tilde{U}$。\n            *   注意 $\\tilde{H}$ 的结构。$\\tilde{H} = \\begin{pmatrix} A & B \\\\ B & A \\end{pmatrix}$，其中 $A = \\begin{pmatrix} 4 & 2 \\\\ 2 & 4 \\end{pmatrix}$ 和 $B = \\begin{pmatrix} -2 & -1 \\\\ -1 & -2 \\end{pmatrix}$。而且 $B = - \\frac{1}{2} A$。我们来验证一下：$-\\frac{1}{2} \\begin{pmatrix} 4 & 2 \\\\ 2 & 4 \\end{pmatrix} = \\begin{pmatrix} -2 & -1 \\\\ -1 & -2 \\end{pmatrix}$。是的。\n            *   所以系统是：\n                $A \\Delta_{1,2} + B \\Delta_{3,4} = \\tilde{U}_{1,2}$\n                $B \\Delta_{1,2} + A \\Delta_{3,4} = \\tilde{U}_{3,4}$\n                其中 $\\Delta = (\\Delta_{1,2}^{\\top}, \\Delta_{3,4}^{\\top})^{\\top}$。\n            *   注意 $\\tilde{U}_{3,4} = \\begin{pmatrix} -6 \\\\ 3 \\end{pmatrix}$ 和 $\\tilde{U}_{1,2} = \\begin{pmatrix} 3 \\\\ -6 \\end{pmatrix}$。让我重新检查一下我的 $U$。\n                *   $U = (1/3, -2/3, -2/3, 1/3)^{\\top}$。\n                *   所以 $\\tilde{U}$ 应该是 $(3, -6, -6, 3)^\\top$。我再检查一遍。\n                *   $U = [ (2/3, 0) + (0, -1/3) + (-1/3, -1/3) ]_1 + [ (-1/3, 0) + (0, 2/3) + (-1/3, -1/3) ]_2$。这个写法不太对。\n                *   我们重新计算 $U$ 的分量。\n                    *   $U_1 = (2/3)(1) + (-1/3)(0) + (-1/3)(1) = 2/3 - 1/3 = 1/3$。\n                    *   $U_2 = (2/3)(0) + (-1/3)(1) + (-1/3)(1) = -1/3 - 1/3 = -2/3$。\n                    *   $U_3 = (-1/3)(1) + (2/3)(0) + (-1/3)(1) = -1/3 - 1/3 = -2/3$。\n                    *   $U_4 = (-1/3)(0) + (2/3)(1) + (-1/3)(1) = 2/3 - 1/3 = 1/3$。\n                *   所以 $U = (1/3, -2/3, -2/3, 1/3)^{\\top}$ 是正确的。\n                *   并且 $\\tilde{U} = (3, -6, -6, 3)^{\\top}$ 是正确的。\n                *   我们来检查一下 $\\tilde{U}$ 的结构。令 $\\tilde{U}_{1,2} = (3, -6)^{\\top}$ 和 $\\tilde{U}_{3,4} = (-6, 3)^{\\top}$。\n                *   注意 $\\tilde{U}_{3,4}$ 不等于 $\\tilde{U}_{1,2}$。\n            *   我们尝试用另一种方法求解。注意到系统中的对称性。\n                *   (1) $4\\Delta_1 + 2\\Delta_2 - 2\\Delta_3 - \\Delta_4 = 3$\n                *   (2) $2\\Delta_1 + 4\\Delta_2 - \\Delta_3 - 2\\Delta_4 = -6$\n                *   (3) $-2\\Delta_1 - \\Delta_2 + 4\\Delta_3 + 2\\Delta_4 = -6$\n                *   (4) $-\\Delta_1 - 2\\Delta_2 + 2\\Delta_3 + 4\\Delta_4 = 3$\n            *   将 (1) 和 (4) 相加：$3\\Delta_1 + 0\\Delta_2 + 0\\Delta_3 + 3\\Delta_4 = 6 \\implies \\Delta_1 + \\Delta_4 = 2$。\n            *   将 (2) 和 (3) 相加：$0\\Delta_1 + 3\\Delta_2 + 3\\Delta_3 + 0\\Delta_4 = -12 \\implies \\Delta_2 + \\Delta_3 = -4$。\n            *   这很有帮助。我们看看是否能找到更多关系。\n            *   将 (1) 和 (3) 相加：$2\\Delta_1+\\Delta_2+2\\Delta_3+\\Delta_4 = -3$。\n            *   将 (2) 和 (4) 相加：$\\Delta_1+2\\Delta_2+ \\Delta_3+2\\Delta_4 = -3$。\n            *   我们使用我找到的两个关系。$\\Delta_4 = 2 - \\Delta_1$ 和 $\\Delta_3 = -4 - \\Delta_2$。\n            *   将它们代入 (1) 和 (2)。\n            *   代入 (1)：\n                $4\\Delta_1 + 2\\Delta_2 - 2(-4 - \\Delta_2) - (2 - \\Delta_1) = 3$\n                $4\\Delta_1 + 2\\Delta_2 + 8 + 2\\Delta_2 - 2 + \\Delta_1 = 3$\n                $5\\Delta_1 + 4\\Delta_2 + 6 = 3$\n                $5\\Delta_1 + 4\\Delta_2 = -3$ (方程 A)\n            *   代入 (2)：\n                $2\\Delta_1 + 4\\Delta_2 - (-4 - \\Delta_2) - 2(2 - \\Delta_1) = -6$\n                $2\\Delta_1 + 4\\Delta_2 + 4 + \\Delta_2 - 4 + 2\\Delta_1 = -6$\n                $4\\Delta_1 + 5\\Delta_2 = -6$ (方程 B)\n            *   现在我有一个关于 $\\Delta_1$ 和 $\\Delta_2$ 的 $2 \\times 2$ 系统：\n                $5\\Delta_1 + 4\\Delta_2 = -3$\n                $4\\Delta_1 + 5\\Delta_2 = -6$\n            *   第一个方程乘以 5，第二个方程乘以 4：\n                $25\\Delta_1 + 20\\Delta_2 = -15$\n                $16\\Delta_1 + 20\\Delta_2 = -24$\n            *   第一个方程减去第二个方程：\n                $9\\Delta_1 = -15 - (-24) = 9 \\implies \\Delta_1 = 1$。\n            *   将 $\\Delta_1=1$ 代入方程 A：\n                $5(1) + 4\\Delta_2 = -3 \\implies 4\\Delta_2 = -8 \\implies \\Delta_2 = -2$。\n            *   现在求 $\\Delta_3$ 和 $\\Delta_4$：\n                *   $\\Delta_3 = -4 - \\Delta_2 = -4 - (-2) = -2$。\n                *   $\\Delta_4 = 2 - \\Delta_1 = 2 - 1 = 1$。\n            *   所以解是 $\\Delta = (1, -2, -2, 1)^{\\top}$。\n\n        *   **步骤 2g：验证解**\n            *   我们将 $\\Delta = (1, -2, -2, 1)^{\\top}$ 代回原始系统 $\\tilde{H} \\Delta = \\tilde{U}$。\n            *   $\\tilde{H} \\Delta = \\begin{pmatrix} 4 & 2 & -2 & -1 \\\\ 2 & 4 & -1 & -2 \\\\ -2 & -1 & 4 & 2 \\\\ -1 & -2 & 2 & 4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -2 \\\\ -2 \\\\ 1 \\end{pmatrix}$\n            *   第 1 行：$4(1) + 2(-2) - 2(-2) - 1(1) = 4 - 4 + 4 - 1 = 3$。正确。\n            *   第 2 行：$2(1) + 4(-2) - 1(-2) - 2(1) = 2 - 8 + 2 - 2 = -6$。正确。\n            *   第 3 行：$-2(1) - 1(-2) + 4(-2) + 2(1) = -2 + 2 - 8 + 2 = -6$。正确。\n            *   第 4 行：$-1(1) - 2(-2) + 2(-2) + 4(1) = -1 + 4 - 4 + 4 = 3$。正确。\n            *   解是正确的。$\\Delta = (\\Delta\\beta_{11}, \\Delta\\beta_{12}, \\Delta\\beta_{21}, \\Delta\\beta_{22})^{\\top} = (1, -2, -2, 1)^{\\top}$。\n\n3.  **最终格式化：**\n\n    *   解题过程需要逻辑清晰地写出，从一般推导开始，然后将其应用于具体数据。\n    *   所有数学实体必须使用 LaTeX。\n    *   最终答案必须放在一个框中，形式为 $1 \\times 4$ 的行矩阵。\n    *   最终答案是 $\\Delta\\beta_1 = (1, -2)^{\\top}$ 和 $\\Delta\\beta_2 = (-2, 1)^{\\top}$。\n    *   所以 $\\Delta = (1, -2, -2, 1)^{\\top}$。\n    *   最终答案的行矩阵形式为 $\\begin{pmatrix} 1 & -2 & -2 & 1 \\end{pmatrix}$。\n\n    再次检查问题陈述：‘精确报告 $\\Delta$ 的四个分量’。‘将您的最终答案以单个 $1 \\times 4$ 行矩阵的形式提供’。这些都吻合。\n    无需四舍五入，所以整数结果是完美的。\n    所有步骤看起来都是正确且逻辑合理的。得分向量和海森矩阵的推导是多项式逻辑回归的标准步骤。计算是直接的算术和矩阵运算。最后的线性系统求解也是标准的。\n    我对结果有信心。", "answer": "$$ \\boxed{ \\begin{pmatrix} 1 & -2 & -2 & 1 \\end{pmatrix} } $$", "id": "4929771"}, {"introduction": "在掌握了模型拟合的基本原理后，我们将焦点转向模型解释。本练习关注比例优势模型（proportional odds model），这是生物统计学中处理有序数据的常用工具[@problem_id:4929834]。通过这个实践，你将学会如何使用估计出的系数（阈值和斜率）来计算有意义的预测——具体来说，就是一个个体落入每个有序类别的概率。", "problem": "一个定序结果 $Y \\in \\{1,2,3,4,5\\}$ 在一个5级量表上记录疾病的严重程度，数值越高表示严重程度越差。考虑一个带有单个协变量 $x$（例如，一个标准化的生物标志物）的比例优势累积 logit 模型。该比例优势模型假设，对于每个切点 $j \\in \\{1,2,3,4\\}$，累积对数优势与 $x$ 呈线性关系，具有特定类别的截距（阈值）和一个公共斜率：\n$$\n\\ln\\!\\left(\\frac{\\Pr(Y \\le j \\mid x)}{1 - \\Pr(Y \\le j \\mid x)}\\right) \\;=\\; \\theta_j \\;-\\; \\beta\\, x.\n$$\n假设一项研究估计出阈值为 $\\theta_1=-2.1$, $\\theta_2=-0.8$, $\\theta_3=0.6$, $\\theta_4=1.8$，斜率为 $\\beta=0.9$。对于一个协变量值为 $x=1.6$ 的新个体：\n- 从 logit 链接函数及其逆 logistic 函数的定义出发，计算 $j=1,2,3,4$ 的预测累积概率 $\\gamma_j(x)=\\Pr(Y \\le j \\mid x)$。\n- 然后计算 $k=1,2,3,4,5$ 的类别概率 $\\pi_k(x)=\\Pr(Y=k \\mid x)$。\n- 简要陈述在实践中评估这些概率时相关的至少两个数值稳定性考量。\n\n将所有概率表示为小数。您最终报告的量必须是预测概率 $\\Pr(Y=4 \\mid x=1.6)$，并四舍五入到四位有效数字。", "solution": "用户提供了一个问题陈述，在尝试解决之前需要进行验证。\n\n### 第一步：提取已知条件\n-   结果变量是一个定序变量 $Y \\in \\{1,2,3,4,5\\}$，表示疾病的严重程度。\n-   模型是一个带有单个协变量 $x$ 的比例优势累积 logit 模型。\n-   模型方程为：$\\ln\\!\\left(\\frac{\\Pr(Y \\le j \\mid x)}{1 - \\Pr(Y \\le j \\mid x)}\\right) \\;=\\; \\theta_j \\;-\\; \\beta\\, x$，对于 $j \\in \\{1,2,3,4\\}$。\n-   估计的阈值为：$\\theta_1=-2.1$, $\\theta_2=-0.8$, $\\theta_3=0.6$, $\\theta_4=1.8$。\n-   估计的斜率为：$\\beta=0.9$。\n-   新个体的协变量值为：$x=1.6$。\n-   任务1：计算 $j=1,2,3,4$ 的累积概率 $\\gamma_j(x)=\\Pr(Y \\le j \\mid x)$。\n-   任务2：计算 $k=1,2,3,4,5$ 的类别概率 $\\pi_k(x)=\\Pr(Y=k \\mid x)$。\n-   任务3：陈述至少两个数值稳定性考量。\n-   任务4：报告最终量 $\\Pr(Y=4 \\mid x=1.6)$，四舍五入到四位有效数字。\n\n### 第二步：使用提取的已知条件进行验证\n1.  **科学或事实合理性**：该问题描述了一个比例优势模型，这是一种在生物统计学和其他领域用于分析定序数据的标准且广泛使用的统计方法。模型方程陈述正确。给定的参数是合理的。一个有效的比例优势模型的一个必要条件是阈值必须严格递增，即 $\\theta_1 < \\theta_2 < \\dots < \\theta_{J-1}$。给定的阈值 $\\theta_1=-2.1$, $\\theta_2=-0.8$, $\\theta_3=0.6$, $\\theta_4=1.8$ 满足此条件（$-2.1 < -0.8 < 0.6 < 1.8$）。这确保了累积概率 $\\Pr(Y \\le j \\mid x)$ 将随 j 非递减，这是一个逻辑上的必然要求。该问题在科学上和数学上都是合理的。\n2.  **适定性**：该问题是适定的。它提供了计算所要求的概率所需的所有必要信息——模型形式、所有参数值（$\\theta_j$ 和 $\\beta$）以及协变量 $x$ 的具体值。任务定义清晰，并能导出一个唯一解。\n3.  **客观性**：该问题以精确、客观和定量的术语陈述。没有主观或模棱两可的语言。\n\n### 第三步：结论和行动\n该问题有效。它是一个标准统计模型的适定、有科学依据的应用。我将继续进行解答。\n\n比例优势模型由累积概率的 logit 链接函数定义：\n$$\n\\text{logit}(\\Pr(Y \\le j \\mid x)) = \\ln\\left(\\frac{\\Pr(Y \\le j \\mid x)}{1 - \\Pr(Y \\le j \\mid x)}\\right) = \\theta_j - \\beta x\n$$\n为了找到累积概率 $\\gamma_j(x) = \\Pr(Y \\le j \\mid x)$，我们必须应用 logit 链接函数的逆函数，即标准 logistic 函数 $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$。\n$$\n\\gamma_j(x) = \\sigma(\\theta_j - \\beta x) = \\frac{1}{1 + \\exp(-(\\theta_j - \\beta x))}\n$$\n给定参数 $\\theta_1=-2.1$, $\\theta_2=-0.8$, $\\theta_3=0.6$, $\\theta_4=1.8$ 和 $\\beta=0.9$。我们需要为协变量值为 $x=1.6$ 的个体评估概率。\n\n首先，我们计算线性预测项 $\\beta x$：\n$$\n\\beta x = 0.9 \\times 1.6 = 1.44\n$$\n现在，我们计算每个累积概率的 logits：\n$$\n\\text{logit}(\\gamma_1(1.6)) = \\theta_1 - \\beta x = -2.1 - 1.44 = -3.54\n$$\n$$\n\\text{logit}(\\gamma_2(1.6)) = \\theta_2 - \\beta x = -0.8 - 1.44 = -2.24\n$$\n$$\n\\text{logit}(\\gamma_3(1.6)) = \\theta_3 - \\beta x = 0.6 - 1.44 = -0.84\n$$\n$$\n\\text{logit}(\\gamma_4(1.6)) = \\theta_4 - \\beta x = 1.8 - 1.44 = 0.36\n$$\n接下来，我们将 logistic 函数应用于这些 logit 值，以获得累积概率 $\\gamma_j(x)$：\n$$\n\\gamma_1(1.6) = \\Pr(Y \\le 1 \\mid x=1.6) = \\frac{1}{1 + \\exp(-(-3.54))} = \\frac{1}{1 + \\exp(3.54)} \\approx 0.02819\n$$\n$$\n\\gamma_2(1.6) = \\Pr(Y \\le 2 \\mid x=1.6) = \\frac{1}{1 + \\exp(-(-2.24))} = \\frac{1}{1 + \\exp(2.24)} \\approx 0.09621\n$$\n$$\n\\gamma_3(1.6) = \\Pr(Y \\le 3 \\mid x=1.6) = \\frac{1}{1 + \\exp(-(-0.84))} = \\frac{1}{1 + \\exp(0.84)} \\approx 0.30154\n$$\n$$\n\\gamma_4(1.6) = \\Pr(Y \\le 4 \\mid x=1.6) = \\frac{1}{1 + \\exp(-(0.36))} = \\frac{1}{1 + \\exp(-0.36)} \\approx 0.58904\n$$\n计算出累积概率后，我们可以找到各个类别概率 $\\pi_k(x) = \\Pr(Y=k \\mid x)$。关系如下：\n$$\n\\pi_1(x) = \\Pr(Y=1 \\mid x) = \\Pr(Y \\le 1 \\mid x) = \\gamma_1(x)\n$$\n$$\n\\pi_k(x) = \\Pr(Y=k \\mid x) = \\Pr(Y \\le k \\mid x) - \\Pr(Y \\le k-1 \\mid x) = \\gamma_k(x) - \\gamma_{k-1}(x) \\text{ for } k=2, 3, 4\n$$\n$$\n\\pi_5(x) = \\Pr(Y=5 \\mid x) = 1 - \\Pr(Y \\le 4 \\mid x) = 1 - \\gamma_4(x)\n$$\n使用我们为 $x=1.6$ 计算的值：\n$$\n\\pi_1(1.6) = \\gamma_1(1.6) \\approx 0.02819\n$$\n$$\n\\pi_2(1.6) = \\gamma_2(1.6) - \\gamma_1(1.6) \\approx 0.09621 - 0.02819 = 0.06802\n$$\n$$\n\\pi_3(1.6) = \\gamma_3(1.6) - \\gamma_2(1.6) \\approx 0.30154 - 0.09621 = 0.20533\n$$\n$$\n\\pi_4(1.6) = \\gamma_4(1.6) - \\gamma_3(1.6) \\approx 0.58904 - 0.30154 = 0.28750\n$$\n$$\n\\pi_5(1.6) = 1 - \\gamma_4(1.6) \\approx 1 - 0.58904 = 0.41096\n$$\n作为核对，这些概率的总和是 $0.02819 + 0.06802 + 0.20533 + 0.28750 + 0.41096 = 1.00000$。\n\n最后，我们讨论数值稳定性考量。\n1.  **logistic 函数 $\\sigma(z) = 1/(1 + \\exp(-z))$ 的求值**：对于参数 $z$ 的大的负值，项 $-z$ 变为大的正值，导致 $\\exp(-z)$ 超出标准浮点表示的范围（上溢）。例如，如果 $z=-1000$，$\\exp(1000)$ 在计算上是无穷大。一个数值稳定的实现对负参数使用一种替代的代数形式：$\\sigma(z) = \\exp(z) / (1 + \\exp(z))$。当 $z$ 是大的负数时，$\\exp(z)$ 下溢到 $0$，从而产生稳定且正确的结果 $0$。一种常见的做法是使用条件求值：如果 $z < 0$，计算 $\\exp(z)/(1+\\exp(z))$；否则，计算 $1/(1+\\exp(-z))$。许多统计软件包也在对数尺度上工作，使用像 `log1p(x)`（它能为小的 $x$ 精确计算 $\\ln(1+x)$）这样的函数，以避免在计算对数概率时出现下溢和上溢问题（即“log-sum-exp”技巧）。\n2.  **概率差中的灾难性抵消**：对于 $k \\in \\{2, 3, 4\\}$，类别概率 $\\pi_k$ 是通过减去连续的累积概率来计算的：$\\pi_k(x) = \\gamma_k(x) - \\gamma_{k-1}(x)$。如果阈值 $\\theta_k$ 和 $\\theta_{k-1}$ 非常接近，它们对应的 logits 也会很接近，导致 $\\gamma_k(x)$ 和 $\\gamma_{k-1}(x)$ 几乎相等。将两个几乎相等的浮点数相减会导致灾难性抵消，即相对精度的严重损失。例如，如果 $\\gamma_k(x) = 0.51234567$ 而 $\\gamma_{k-1}(x) = 0.51234500$，它们的差是 $0.00000067$，其有效数字位数远少于原始数字。这可能导致对小的类别概率的估计高度不准确。在这种情况下，可能需要更稳定的数值例程来精确计算这个差值。\n\n所要求的最终量是 $\\Pr(Y=4 \\mid x=1.6)$，即 $\\pi_4(1.6)$。根据我们的计算，$\\pi_4(1.6) \\approx 0.28750$。四舍五入到四位有效数字得到 $0.2875$。", "answer": "$$\\boxed{0.2875}$$", "id": "4929834"}, {"introduction": "最后，我们通过一个关键的概念性练习，从“如何做”转向“为何”与“何时”做。这个思想实验利用一个经典场景，揭示了多项逻辑回归模型的一个关键假设：无关备择选项的独立性（Independence of Irrelevant Alternatives, IIA）[@problem_id:4929809]。理解这一特性及其潜在的缺陷，对于批判性地评估模型是否适用于特定的研究问题、避免得出误导性结论至关重要。", "problem": "一个医院系统正在一个同质子群中分析患者在不同抗高血压治疗类别中的选择，观察到的选择是在血管紧张素转换酶抑制剂 (ACEI) 和血管紧张素受体阻断剂 (ARB) 之间。在一个只有这两个类别可用的初始队列中，选择 ACEI 的概率为 $3/5$，选择 ARB 的概率为 $2/5$。随后，引入了第二个 ARB 品牌，该品牌在系统效用层面（同类别、给药方案、价格和副作用特征相同）与原始 ARB 临床上无法区分，因此它是原始 ARB 类别的一个近乎重复的选项。假设患者的选择由一个具有无关备择项独立性 (IIA) 的多项 Logit (MNL) 模型建模，该模型源于一个在不同选项间具有独立同分布随机成分的随机效用模型。\n\n哪个选项正确地构建了一个反例，表明增加一个近乎重复的类别会违反合理的替代模式，并正确解释了为什么具有 IIA 特性的 MNL 模型在这种情况下是设定错误的？\n\nA. 因为 IIA 通过系统效用差异固定了 ACEI 与任何特定 ARB 的选择比，增加一个近乎重复的 ARB 会迫使概率变为 ACEI $=3/7$，ARB $=2/7$ 和新 ARB $=2/7$。这使得 ACEI 失去了市场份额，尽管新类别仅与 ARB 竞争。合理的替代模式会使 ACEI 的份额保持在 $3/5$ 附近，并瓜分 ARB 的份额，因此当类别具有相关的未观察成分时，具有 IIA 的 MNL 模型是设定错误的。\n\nB. 因为 IIA 适用，增加一个近乎重复的 ARB 会使 ACEI 的份额保持在 $3/5$ 不变，同时将 ARB 的份额均分为 $1/5$ 和 $1/5$，这符合合理的替代模式；因此，具有 IIA 的 MNL 模型在这里是设定正确的。\n\nC. 增加一个近乎重复的 ARB 会因冗余选项导致分母收缩，从而使 ACEI 的份额增加到 $4/5$；这表明 MNL 捕捉到了强烈的同类偏好，因此是设定良好的。\n\nD. 这里的任何违背都仅仅是由于遗漏了关于品牌标签的可观察协变量；一旦包含该协变量，IIA 确保在增加一个完全的近乎重复项时不会产生设定错误，因此 MNL 模型无需进一步更改即可保持有效。", "solution": "用户希望我扮演一名 STEM 教授，解决一个与多项逻辑回归相关的问题。\n\n**1. 问题验证**\n\n首先，我将按要求验证问题陈述。\n\n**步骤 1：提取已知条件**\n\n- **背景：** 患者在抗高血压治疗方案之间的选择。\n- **初始选择集（2个备择项）：** 血管紧张素转换酶抑制剂 (ACEI) 和血管紧张素受体阻断剂 (ARB)。\n- **初始概率：** $P(\\text{ACEI}) = 3/5$，$P(\\text{ARB}) = 2/5$。\n- **新备择项：** 引入了第二个 ARB 品牌 (ARB2)。\n- **新备择项的属性：** 在系统效用层面（同类别、给药方案、价格、副作用特征相同）与原始 ARB (ARB1) 临床上无法区分；它是一个近乎重复的选项。\n- **建模假设：** 患者选择由一个具有无关备择项独立性 (IIA) 特性的多项 Logit (MNL) 模型建模，该模型源于一个具有独立同分布 (IID) 随机成分的随机效用模型。\n- **问题：** 找出正确构建反例的选项，该反例表明具有 IIA 的 MNL 模型设定错误，并解释原因。\n\n**步骤 2：使用提取的已知条件进行验证**\n\n-   **科学性（关键）：** 该问题牢固地植根于离散选择模型的统计理论，这是生物统计学、计量经济学和交通科学的核心课题。所描述的情景是典型的“红巴士/蓝巴士”问题，这是说明多项 Logit 模型及其 IIA 特性局限性的经典例子。随机效用、系统效用、MNL 和 IIA 的概念都是公认的。该问题在科学上和数学上都是合理的。\n-   **定义明确：** 该问题是定义明确的。它提供了一个清晰的、定量的设置，并要求基于指定模型的属性得出演绎结论。它旨在测试一个特定的、明确定义的概​​念，并且可以从前提中推导出唯一的正确答案。\n-   **客观性（关键）：** 该问题以客观、正式的语言陈述。它使用统计学的标准术语，不包含主观或基于意见的陈述。\n\n**步骤 3：结论与行动**\n\n问题陈述是**有效的**。这是一个应用统计学中的标准、定义明确的问题，准确地代表了一个常用统计模型的已知局限性。我现在将开始解决这个问题。\n\n**2. 解题推导**\n\n该问题要求在特定情景下分析多项 Logit (MNL) 模型及其无关备择项独立性 (IIA) 特性。\n\n**多项 Logit 模型**\n\n从随机效用最大化框架推导出的 MNL 模型指出，个体从选择集 $C$ 中选择备择项 $j$ 的概率由下式给出：\n$$P(j) = \\frac{\\exp(V_j)}{\\sum_{k \\in C} \\exp(V_k)}$$\n其中 $V_j$ 是备择项 $j$ 的系统效用或代表性效用。该模型的一个关键特性是 IIA，它意味着选择两个备择项 $i$ 和 $j$ 的概率之比与任何其他备择项 $k$ 是否存在无关。\n$$\\frac{P(i)}{P(j)} = \\frac{\\exp(V_i) / \\sum_{k \\in C} \\exp(V_k)}{\\exp(V_j) / \\sum_{k \\in C} \\exp(V_k)} = \\frac{\\exp(V_i)}{\\exp(V_j)}$$\n\n**初始情景：两个备择项**\n\n设备择项 $1$ 为 ACEI，备择项 $2$ 为 ARB。选择集为 $C_1 = \\{\\text{ACEI, ARB}\\}$。\n我们已知 $P(\\text{ACEI}) = 3/5$ 和 $P(\\text{ARB}) = 2/5$。\n选择 ACEI 相对于 ARB 的优势比是：\n$$\\frac{P(\\text{ACEI})}{P(\\text{ARB})} = \\frac{3/5}{2/5} = \\frac{3}{2}$$\n根据 IIA 特性，这个比率由系统效用决定：\n$$\\frac{\\exp(V_{\\text{ACEI}})}{\\exp(V_{\\text{ARB}})} = \\frac{3}{2}$$\n为便于分析，我们可以通过将一个备择项的效用设为零来进行归一化。我们设 $V_{\\text{ARB}} = 0$。那么，$\\exp(V_{\\text{ACEI}}) = 3/2$。\n\n**第二情景：增加一个近乎重复的备择项**\n\n引入了一个新的备择项，第二个 ARB（我们称之为 ARB2，原始的 ARB 现在是 ARB1）。新的选择集是 $C_2 = \\{\\text{ACEI, ARB1, ARB2}\\}$。\n问题陈述，在系统效用方面，ARB2 是 ARB1 的一个近乎重复的选项。因此，$V_{\\text{ARB2}} = V_{\\text{ARB1}}$。\n使用我们的归一化方法，$V_{\\text{ARB1}} = V_{\\text{ARB2}} = 0$。ACEI 的效用 $V_{\\text{ACEI}}$ 保持不变，所以 $\\exp(V_{\\text{ACEI}}) = 3/2$。\n\n现在，我们将 MNL 公式应用于新的选择集 $C_2$：\n概率公式的分母是所有可用备择项的指数化效用之和：\n$$\\sum_{k \\in C_2} \\exp(V_k) = \\exp(V_{\\text{ACEI}}) + \\exp(V_{\\text{ARB1}}) + \\exp(V_{\\text{ARB2}}) = \\frac{3}{2} + 1 + 1 = \\frac{7}{2}$$\n\n新的概率是：\n$P(\\text{ACEI}) = \\frac{\\exp(V_{\\text{ACEI}})}{\\sum_{k \\in C_2} \\exp(V_k)} = \\frac{3/2}{7/2} = \\frac{3}{7}$\n$P(\\text{ARB1}) = \\frac{\\exp(V_{\\text{ARB1}})}{\\sum_{k \\in C_2} \\exp(V_k)} = \\frac{1}{7/2} = \\frac{2}{7}$\n$P(\\text{ARB2}) = \\frac{\\exp(V_{\\text{ARB2}})}{\\sum_{k \\in C_2} \\exp(V_k)} = \\frac{1}{7/2} = \\frac{2}{7}$\n\n**结果分析与模型设定错误**\n\nMNL 模型预测新的概率为 $P(\\text{ACEI}) = 3/7$，$P(\\text{ARB1}) = 2/7$ 和 $P(\\text{ARB2}) = 2/7$。\n让我们将其与初始情况进行比较：\n- 选择 ACEI 的概率从 $3/5 = 0.6$ 下降到 $3/7 \\approx 0.429$。\n- 选择 ARB 的总概率从 $2/5 = 0.4$ 增加到 $P(\\text{ARB1}) + P(\\text{ARB2}) = 2/7 + 2/7 = 4/7 \\approx 0.571$。\n\n这个结果违反了合理的替代模式。新的 ARB2 是 ARB1 的近乎完美替代品，而不是 ACEI 的替代品。一个理性的决策者可能会将选择视为在 {ACEI} 和 {ARBs} 组之间进行。引入一个新的 ARB 主要应该从现有的 ARB 中吸引份额，而不是从不同的 ACEI 类别中。一个更合理的结果会是选择 ACEI 的概率保持在 $3/5$ 附近，而 ARB 的 $2/5$ 概率在 ARB1 和 ARB2 之间分配（例如，每个 $1/5$）。\n\n这种设定错误的原因在于基础的随机效用模型 $U_j = V_j + \\epsilon_j$ 中随机误差项 ($\\epsilon_j$) 的 IID 假设中的“独立性”。通过假设误差是独立的，模型无法解释这样一个事实：使患者偏爱 ARB1 的未观察因素（例如，对 ARB 药物类别的特定未观察到的遗传反应）也会使他们偏爱 ARB2。实际上，ARB1 和 ARB2 的误差项是高度相关的。MNL 模型由于其构造，无法表示这种相关性，从而导致了不切实际的 IIA 特性及错误的预测。\n\n**3. 逐项评估选项**\n\n**A. 因为 IIA 通过系统效用差异固定了 ACEI 与任何特定 ARB 的选择比，增加一个近乎重复的 ARB 会迫使概率变为 ACEI $=3/7$，ARB $=2/7$ 和新 ARB $=2/7$。这使得 ACEI 失去了市场份额，尽管新类别仅与 ARB 竞争。合理的替代模式会使 ACEI 的份额保持在 $3/5$ 附近，并瓜分 ARB 的份额，因此当类别具有相关的未观察成分时，具有 IIA 的 MNL 模型是设定错误的。**\n- 计算出的概率（$3/7$、$2/7$、$2/7$）是正确的。\n- 关于 ACEI 以违反合理替代模式的方式失去份额的解释是正确的。\n- 对合理替代模式的描述（ACEI 份额保持稳定，ARB 份额被瓜分）是正确的。\n- 设定错误的根本原因（相关的未观察成分违反了独立性假设）被正确地识别出来。\n- **结论：正确。**\n\n**B. 因为 IIA 适用，增加一个近乎重复的 ARB 会使 ACEI 的份额保持在 $3/5$ 不变，同时将 ARB 的份额均分为 $1/5$ 和 $1/5$，这符合合理的替代模式；因此，具有 IIA 的 MNL 模型在这里是设定正确的。**\n- 该选项错误地陈述了应用 IIA 特性的结果。如计算所示，选择 ACEI 的概率并未保持在 $3/5$；它变成了 $3/7$。该选项描述了合理的结果，但错误地将其归因于 MNL 模型。\n- **结论：错误。**\n\n**C. 增加一个近乎重复的 ARB 会因冗余选项导致分母收缩，从而使 ACEI 的份额增加到 $4/5$；这表明 MNL 捕捉到了强烈的同类偏好，因此是设定良好的。**\n- ACEI 份额增加到 $4/5$ 的说法在数值上是错误的；它减少到了 $3/7$。\n- 关于“分母收缩”的推理是无稽之谈。在选择集中增加一个备择项会严格增加 MNL 概率公式的分母。\n- **结论：错误。**\n\n**D. 这里的任何违背都仅仅是由于遗漏了关于品牌标签的可观察协变量；一旦包含该协变量，IIA 确保在增加一个完全的近乎重复项时不会产生设定错误，因此 MNL 模型无需进一步更改即可保持有效。**\n- 这个选项误诊了问题。问题不在于缺少一个可观察的协变量，而在于模型对独立误差项的假设存在结构性缺陷。即使我们控制了品牌，两种 ARB 药物的未观察效用成分仍然会相关。IIA 特性是问题的根源，而不是确保正确性的功能。解决这个问题需要“进一步的模型更改”，例如转向嵌套 Logit 或混合 Logit 模型，这些模型放宽了 IIA 假设。\n- **结论：错误。**", "answer": "$$\\boxed{A}$$", "id": "4929809"}]}