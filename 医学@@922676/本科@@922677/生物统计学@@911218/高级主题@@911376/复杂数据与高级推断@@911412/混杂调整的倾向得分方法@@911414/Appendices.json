{"hands_on_practices": [{"introduction": "在我们能够调整混淆之前，我们必须首先识别它。本练习利用有向无环图（DAGs）这一强大的因果假设可视化工具，来精确定位产生混淆的后门路径，并确定为得到无偏估计所需的最小调整变量集。掌握这项技能是构建一个有效的倾向性得分模型的基础，也是从理论走向实践的第一步 [@problem_id:4943138]。", "problem": "考虑一项观察性研究，评估二元处理 $A$ 对连续结果 $Y$ 的因果效应。设 $X$ 表示一个已测量的处理前协变量，$U$ 表示一个未测量的患者因素。假设数据生成过程由以下有向无环图（DAG）表示：存在有向边 $U \\to A$、$U \\to X$、$X \\to A$、$X \\to Y$ 和 $A \\to Y$，且无其他箭头。假设满足倾向性得分方法的标准因果可识别性条件，包括正性（positivity）和一致性（consistency）。使用DAG上的 $d$-分离规则来推断混杂，其中从 $A$ 到 $Y$ 的后门路径是指任何以指向 $A$ 的箭头开始，并且在给定空条件集的情况下保持开放的路径。识别此DAG中从 $A$ 到 $Y$ 的所有开放的后门路径，并指定一个由观测变量组成的最小调整集，该调整集能阻断所有这些后门路径，同时不阻断因果路径 $A \\to Y$ 或通过对撞节点打开新路径。您的答案应适用于倾向性得分（PS）模型。\n\n哪个选项正确列出了所有开放的后门路径，并给出了一个能阻断它们的最小调整集？\n\nA. 后门路径：仅 $A \\leftarrow X \\to Y$；最小调整集 $\\{X\\}$。\n\nB. 后门路径：$A \\leftarrow U \\to Y$ 和 $A \\leftarrow X \\to Y$；最小调整集 $\\{U,X\\}$。\n\nC. 后门路径：$A \\leftarrow U \\to X \\to Y$ 和 $A \\leftarrow X \\to Y$；最小调整集 $\\{X\\}$。\n\nD. 无后门路径；最小调整集 $\\varnothing$。\n\nE. 后门路径：仅 $A \\leftarrow U \\to X \\to Y$；最小调整集 $\\{U\\}$。", "solution": "## 问题验证\n\n### 步骤 1：提取已知信息\n-   **研究类型**：观察性研究。\n-   **变量**：\n    -   $A$：二元处理。\n    -   $Y$：连续结果。\n    -   $X$：已测量的处理前协变量。\n    -   $U$：未测量的患者因素。\n-   **数据生成过程（DAG）**：该有向无环图包含以下有向边：\n    1.  $U \\to A$\n    2.  $U \\to X$\n    3.  $X \\to A$\n    4.  $X \\to Y$\n    5.  $A \\to Y$\n    -   不存在其他箭头。\n-   **假设**：标准的因果可识别性条件，包括正性和一致性。\n-   **定义**：从 $A$ 到 $Y$ 的后门路径定义为“任何以指向 $A$ 的箭头开始，并且在给定空条件集的情况下保持开放的路径”。\n-   **目标**：识别从 $A$ 到 $Y$ 的所有开放后门路径，并指定一个由观测变量组成的最小调整集，该调整集能阻断所有此类路径，且适用于倾向性得分（PS）模型。\n\n### 步骤 2：使用提取的已知信息进行验证\n根据验证标准对问题陈述进行分析：\n\n-   **科学基础**：该问题在因果推断理论中有充分的依据，该理论使用有向无环图（DAGs），这是生物统计学、流行病学和计算机科学中的一个标准且严谨的框架。混杂、d-分离、后门路径和调整集等概念都是该框架的核心原则。\n-   **良态问题**：该问题是良态的（well-posed）。DAG结构有明确定义，目标清晰，d-分离规则为识别路径和确定条件化效应提供了确定性的方法。存在唯一且有意义的解。\n-   **客观性**：问题以精确、客观、技术性的语言陈述，没有歧义或主观论断。\n-   **完整与一致**：该问题提供了所有必要信息：变量、它们之间的关系（DAG）、已测量（$X$）和未测量（$U$）变量之间的区别，以及具体目标（找到路径和由观测变量组成的最小调整集）。前提条件是内部一致的。\n-   **现实与可行**：该场景代表了观察性研究中一种典型的混杂结构，其中一个已测量协变量（$X$）和一个未测量协变量（$U$）都是处理和结果（或结果路径上的变量）的共同原因。在应用研究中，区分已测量和未测量变量是一个关键且现实的约束。\n-   **其他缺陷**：该问题不是无意义的、同义反复的、病态的或无法验证的。它要求正确应用已建立的因果推断原则。\n\n### 步骤 3：结论与行动\n该问题是**有效的**。可以继续进行分析。\n\n## 解题推导\n\n目标是识别从处理 $A$ 到结果 $Y$ 的所有开放后门路径，然后找到一个由观测变量组成的最小充分调整集来阻断这些路径。我们关注的因果路径是直接边 $A \\to Y$。后门路径是 $A$ 和 $Y$ 之间的一条非因果路径，它会产生伪关联。\n\n该DAG由以下边定义：$U \\to A$、$U \\to X$、$X \\to A$、$X \\to Y$ 和 $A \\to Y$。\n\n**1. 识别后门路径**\n\n从 $A$ 到 $Y$ 的后门路径是指以指向 $A$ 的箭头（即 $A \\leftarrow \\dots$）开始的路径。我们必须追踪所有连接 $A$ 和 $Y$ 但不包含边 $A \\to Y$ 的此类路径。如果一条路径上没有对撞节点，或者路径上的每个对撞节点都在条件集中（目前为空集），则该路径是“开放的”。\n\n-   **路径 1**：考虑通过协变量 $X$ 的路径。该路径为 $A \\leftarrow X \\to Y$。\n    -   它以指向 $A$ 的箭头开始：$A \\leftarrow X$。\n    -   它连接了 $A$ 和 $Y$。\n    -   这条路径是以 $X$ 为中心的分叉（fork）结构。此路径上没有对撞节点。\n    -   因此，在给定空条件集的情况下，$A \\leftarrow X \\to Y$ 是一条开放的后门路径。\n\n-   **路径 2**：考虑涉及未测量因素 $U$ 的路径。\n    -   一条路径以 $A \\leftarrow U$ 开始。为了从 $U$ 到达 $Y$，我们观察到路径 $U \\to X \\to Y$。\n    -   将它们组合起来得到路径：$A \\leftarrow U \\to X \\to Y$。\n    -   它以指向 $A$ 的箭头开始：$A \\leftarrow U$。\n    -   它通过 $U$ 和 $X$ 连接了 $A$ 和 $Y$。\n    -   这条路径是一个链式（chain）结构（$U \\to X \\to Y$）。此路径上没有对撞节点。\n    -   因此，在给定空条件集的情况下，$A \\leftarrow U \\to X \\to Y$ 是一条开放的后门路径。\n\n-   **其他潜在路径**：还有其他路径吗？我们检查一下涉及对撞节点的路径。\n    -   节点 $A$ 是路径 $U \\to A \\leftarrow X$ 上的一个对撞节点。这条路径连接了 $U$ 和 $X$，而不是 $A$ 和 $Y$。它不是从 $A$ 到 $Y$ 的后门路径。\n    -   节点 $Y$ 是路径 $X \\to Y \\leftarrow A$ 上的一个对撞节点，但后门路径不能穿过结果变量。\n\n因此，从 $A$ 到 $Y$ 恰好有两条开放的后门路径：\n1.  $A \\leftarrow X \\to Y$\n2.  $A \\leftarrow U \\to X \\to Y$\n\n**2. 识别最小充分调整集**\n\n为了估计 $A$ 对 $Y$ 的因果效应，我们必须通过对一组观测协变量进行条件化来阻断这两条后门路径。唯一的观测协变量是 $X$。因素 $U$ 是未测量的，不能包含在倾向性得分模型的调整集中。\n\n-   **阻断路径 1 ($A \\leftarrow X \\to Y$)**：这条路径在 $X$ 处是一个分叉。根据d-分离规则，对分叉变量 $X$ 进行条件化会阻断这条路径。\n-   **阻断路径 2 ($A \\leftarrow U \\to X \\to Y$)**：这条路径是一个链。对链中的任何中间变量进行条件化都会阻断该路径。$X$ 是这条路径上的一个中间变量。因此，对 $X$ 进行条件化也会阻断这条路径。\n\n对集合 $\\{X\\}$ 进行条件化足以阻断所有开放的后门路径。\n\n现在，我们必须验证 $\\{X\\}$ 是否为最小集，以及对它进行条件化是否会引发任何新问题。\n-   **最小性**：唯一更小的集合是空集 $\\varnothing$。对 $\\varnothing$ 进行条件化会使两条路径都保持开放。因此，$\\{X\\}$ 是一个最小充分调整集。\n-   **条件化的有效性**：我们必须确保对 $X$ 进行条件化不会阻断因果路径或打开新的偏倚路径。\n    -   $X$ 不是因果路径 $A \\to Y$ 上的中介变量，因此对它进行条件化不会阻断因果效应。\n    -   如果一个变量是某条路径上的对撞节点，那么对该变量进行条件化会打开该路径。系统中除结果变量外唯一的对撞节点是路径 $U \\to A \\leftarrow X$ 上的 $A$。对 $X$ 进行条件化不会打开这条路径（只有对 $A$ 或其后代进行条件化才会）。\n    \n因此，$\\{X\\}$ 是由观测变量组成的正确的最小充分调整集。\n\n## 逐项分析选项\n\n-   **A. 后门路径：仅 $A \\leftarrow X \\to Y$；最小调整集 $\\{X\\}$。**\n    -   所识别的后门路径列表不完整；它遗漏了路径 $A \\leftarrow U \\to X \\to Y$。虽然最小调整集是正确的，但其选择的前提是有缺陷的。\n    -   **结论**：不正确。\n\n-   **B. 后门路径：$A \\leftarrow U \\to Y$ 和 $A \\leftarrow X \\to Y$；最小调整集 $\\{U,X\\}$。**\n    -   在指定的DAG中，路径 $A \\leftarrow U \\to Y$ 不存在；从 $U$ 到 $Y$ 的路径由 $X$ 介导。正确的路径是 $A \\leftarrow U \\to X \\to Y$。调整集 $\\{U,X\\}$ 是无效的，因为 $U$ 是未测量的，在实践中无法对其进行条件化。此外，该集合不是最小的，因为 $\\{X\\}$ 就足够了。\n    -   **结论**：不正确。\n\n-   **C. 后门路径：$A \\leftarrow U \\to X \\to Y$ 和 $A \\leftarrow X \\to Y$；最小调整集 $\\{X\\}$。**\n    -   此选项正确地识别了两条开放的后门路径：$A \\leftarrow X \\to Y$ 和 $A \\leftarrow U \\to X \\to Y$。\n    -   它也正确地将 $\\{X\\}$ 识别为由观测变量组成的最小充分调整集，该集合能阻断这两条路径。\n    -   **结论**：正确。\n\n-   **D. 无后门路径；最小调整集 $\\varnothing$。**\n    -   这是不正确的。如推导所示，存在两条开放的后门路径造成了混杂。空集 $\\varnothing$ 不是一个充分的调整集。\n    -   **结论**：不正确。\n\n-   **E. 后门路径：仅 $A \\leftarrow U \\to X \\to Y$；最小调整集 $\\{U\\}$。**\n    -   后门路径列表不完整；它遗漏了路径 $A \\leftarrow X \\to Y$。建议的调整集 $\\{U\\}$ 是不正确的，原因有二：(1) $U$ 是未测量的，(2) 仅对 $U$ 进行条件化并不能阻断路径 $A \\leftarrow X \\to Y$。\n    -   **结论**：不正确。", "answer": "$$\\boxed{C}$$", "id": "4943138"}, {"introduction": "现在，我们转向一个更复杂且在现实世界中常见的场景，即处理受时间影响的混淆变量——这类变量既受过去处理措施的影响，又反过来影响未来的处理决策和最终结果。这项综合性编程练习将指导您完成整个流程：模拟此类纵向数据，计算并应用稳定的逆概率权重，检验关键的平衡性假设是否在每个时间点都得以满足，并最终使用边际结构模型（MSM）来估计因果效应。这项实践将所有核心概念融会贯通，形成一个强大的分析应用 [@problem_id:4943123]。", "problem": "您的任务是构建一个完整的模拟研究，以演示如何使用倾向性得分方法对受先前处理影响的时依性混杂因素进行混杂调整，并通过带有稳定化处理概率倒数权重的边际结构模型（MSM）估计因果效应。您必须将模拟、加权、平衡性验证和MSM估计完全作为一个程序来实现。\n\n使用的基本原理和定义：\n- 时依性混杂因素是一个变量，其在时间 $t$ 的值可能受到先前处理的影响，也可能影响未来的处理和结局。设时间 $t$ 的处理为 $A_t \\in \\{0,1\\}$，混杂因素为 $L_t \\in \\mathbb{R}$，最终结局为 $Y \\in \\mathbb{R}$。截至时间 $t$ 的处理史为 $\\bar{A}_t = (A_1,\\dots,A_t)$，截至时间 $t$ 的混杂因素史为 $\\bar{L}_t = (L_0,\\dots,L_t)$，基线为 $L_0$。\n- 时间 $t$ 的倾向性得分是在给定 $\\bar{A}_{t-1}$ 和 $\\bar{L}_t$ 的条件下，$A_t = a$ 的条件概率 $P(A_t = a \\mid \\bar{A}_{t-1}, \\bar{L}_t)$，其中 $a \\in \\{0,1\\}$。\n- 稳定化处理概率倒数权重定义为在所有时间点上，一个依赖于处理史的分子模型与一个同时依赖于处理史和混杂因素史的分母模型之比的乘积。对于个体 $i$，其稳定化权重为\n$$\nSW_i = \\prod_{t=1}^T \\frac{P(A_{it} = a \\mid \\bar{A}_{i,t-1})}{P(A_{it} = a \\mid \\bar{A}_{i,t-1}, \\bar{L}_{it})},\n$$\n其中 $a$ 表示个体 $i$ 在时间 $t$ 实际接受的处理。\n- 这里的边际结构模型（MSM）是一个关于 $Y$ 的边际均值与累积处理 $K_i = \\sum_{t=1}^T A_{it}$ 之间关系的线性模型，即\n$$\nE[Y_i \\mid K_i] = \\gamma_0 + \\gamma_1 K_i,\n$$\n其中 $\\gamma_1$ 是我们感兴趣的因果参数。\n- 必须在每个时间点 $t$ 计算平衡性诊断指标，以验证在由 $SW_i$ 诱导的加权伪群体中，$A_t$ 和 $L_t$ 之间的关联已降至最低。使用 $A_t = 1$ 组和 $A_t = 0$ 组之间的绝对标准化均数差（SMD）：\n$$\n\\text{SMD}_t = \\frac{\\left|\\mu_{t,1} - \\mu_{t,0}\\right|}{\\sqrt{\\frac{1}{2}\\left(s^2_{t,1} + s^2_{t,0}\\right)}},\n$$\n其中 $\\mu_{t,g}$ 和 $s^2_{t,g}$ 分别是在 $A_t = g$ 组中，使用权重 $SW_i$ 计算的 $L_t$ 的加权均值和加权方差。如果在时间 $t$ 时 $\\text{SMD}_t \\le 0.1$，则认为在该时间点达到了可接受的平衡；如果对于所有 $t \\in \\{1,\\dots,T\\}$ 该条件都成立，则认为总体平衡是可接受的。\n\n模拟设置：\n- 生成基线混杂因素 $L_{i0} \\sim \\mathcal{N}(0,1)$。\n- 对所有 $i$ 设 $A_{i0} = 0$。\n- 对每个时间点 $t \\in \\{1,\\dots,T\\}$，依次生成混杂因素和处理：\n    - 受先前处理影响的混杂因素演变：\n    $$\n    L_{it} = c_1 L_{i,t-1} + c_2 A_{i,t-1} + \\varepsilon_{it}, \\quad \\varepsilon_{it} \\sim \\mathcal{N}(0,\\sigma_L^2).\n    $$\n    - 依赖于先前处理和当前混杂因素的处理分配模型（逻辑斯蒂）：\n    $$\n    \\Pr(A_{it} = 1 \\mid A_{i,t-1}, L_{it}) = \\text{expit}\\left(\\alpha_0 + \\alpha_1 A_{i,t-1} + \\alpha_2 L_{it}\\right),\n    $$\n    其中 $\\text{expit}(x) = \\frac{1}{1 + e^{-x}}$。\n- 最终结局生成方式如下\n$$\nY_i = y_0 + \\gamma_1 \\sum_{t=1}^T A_{it} + y_2 L_{iT} + \\eta_i, \\quad \\eta_i \\sim \\mathcal{N}(0,\\sigma_Y^2).\n$$\n\n需要实现的估计任务：\n1. 在每个时间点 $t$ 拟合分子处理模型：仅对截距项和 $A_{t-1}$ 进行 $A_t$ 的逻辑斯蒂回归，以估计 $P(A_t \\mid A_{t-1})$。\n2. 在每个时间点 $t$ 拟合分母处理模型：对截距项、$A_{t-1}$ 和 $L_t$ 进行 $A_t$ 的逻辑斯蒂回归，以估计 $P(A_t \\mid A_{t-1}, L_t)$。\n3. 对每个个体，通过将每个时间点上实际发生的 $A_t$ 的概率比率相乘来计算稳定化权重。为保证数值稳定性，将所有估计的概率截断在区间 $[\\epsilon, 1 - \\epsilon]$ 内，其中 $\\epsilon = 10^{-6}$。\n4. 在每个时间点 $t$，通过使用稳定化权重计算 $A_t = 1$ 和 $A_t = 0$ 组之间 $L_t$ 的绝对标准化均数差 $\\text{SMD}_t$ 来验证平衡性。计算所有时间点中的最大值 $\\max_t \\text{SMD}_t$，以及一个指示符（如果所有 $\\text{SMD}_t \\le 0.1$ 则为 $1$，否则为 $0$）。\n5. 使用 $SW_i$ 通过加权最小二乘法拟合 MSM 模型 $Y = \\gamma_0 + \\gamma_1 K + \\text{error}$，并报告估计值 $\\hat{\\gamma}_1$。\n\n您的程序必须实现以上步骤，并为以下参数集的测试套件生成输出（每个案例包括样本量、时间点数和数据生成参数）：\n- 案例1：$n = 5000$, $T = 3$, $\\alpha_0 = -0.2$, $\\alpha_1 = 0.3$, $\\alpha_2 = 1.0$, $c_1 = 0.5$, $c_2 = 0.8$, $\\sigma_L = 1.0$, $\\sigma_Y = 1.0$, $y_0 = 0.0$, $y_2 = 0.7$, $\\gamma_1 = 1.5$, 随机种子 $42$。\n- 案例2：$n = 3000$, $T = 3$, $\\alpha_0 = -0.1$, $\\alpha_1 = 0.2$, $\\alpha_2 = 2.0$, $c_1 = 0.5$, $c_2 = 0.8$, $\\sigma_L = 1.0$, $\\sigma_Y = 1.0$, $y_0 = 0.0$, $y_2 = 0.7$, $\\gamma_1 = 1.5$, 随机种子 $123$。\n- 案例3：$n = 800$, $T = 2$, $\\alpha_0 = -0.3$, $\\alpha_1 = 0.4$, $\\alpha_2 = 0.8$, $c_1 = 0.5$, $c_2 = 0.8$, $\\sigma_L = 0.8$, $\\sigma_Y = 1.2$, $y_0 = 0.0$, $y_2 = 0.7$, $\\gamma_1 = 1.5$, 随机种子 $7$。\n\n答案规格：\n- 对每个测试案例，计算并返回一个包含三项的列表：MSM 估计值 $\\hat{\\gamma}_1$（浮点数，保留 $6$ 位小数）、所有时间点中的最大绝对 SMD $\\max_t \\text{SMD}_t$（浮点数，保留 $6$ 位小数）和平衡指示符（如果所有 $\\text{SMD}_t \\le 0.1$ 则为整数 $1$，否则为 $0$）。\n- 您的程序应生成单行输出，其中包含所有测试案例的结果，格式为一个由方括号括起来的逗号分隔列表，其中每个元素本身也是一个由方括号括起来的逗号分隔列表。例如： \"[$[x_1,y_1,z_1],[x_2,y_2,z_2],[x_3,y_3,z_3]$]\"。不要包含任何其他文本。", "solution": "用户在因果推断和生物统计学领域提供了一个有效、适定且具有科学依据的问题陈述。任务是实现一个模拟研究，演示如何使用通过稳定化处理概率倒数权重（IPTW）估计的边际结构模型（MSM），对时依性混杂因素进行混杂调整。问题陈述是完整的，提供了所有必要的模型、参数和程序。\n\n该方法解决的核心挑战是时依性混杂，这种情况在纵向研究中很普遍，即一个变量的值随时间变化，对后续的处理-结局关系起到混杂作用，并且其本身也受到先前处理的影响。设 $A_t$ 为时间 $t$ 的处理，$L_t$ 为时依性混杂因素，$Y$ 为最终结局。指定的因果结构是 $A_{t-1} \\to L_t \\to A_t$，其中 $A_t$ 和 $L_t$（通过 $L_T$）都会影响结局 $Y$。对 $L_t$ 进行标准的回归调整是不恰当的，因为控制 $L_t$ 会阻断 $A_{t-1}$ 对 $Y$ 的部分因果效应，这部分效应是通过 $L_t$ 介导的。\n\n指定的解决方案，即用稳定化权重估计的边际结构模型，是解决该问题的经典方法。其核心思想是创建一个伪群体，在这个伪群体中，从混杂因素到处理的因果路径被切断。这是通过为每个受试者赋予一个权重来实现的，该权重等于他们在观测到的混杂因素史条件下，实际接受的处理的概率的倒数。\n\n个体 $i$ 的稳定化权重 $SW_i$ 定义为：\n$$\nSW_i = \\prod_{t=1}^T \\frac{P(A_{it} = a_t \\mid \\bar{A}_{i,t-1})}{P(A_{it} = a_t \\mid \\bar{A}_{i,t-1}, \\bar{L}_{it})}\n$$\n其中 $a_t$ 是个体 $i$ 在时间 $t$ 实际接受的处理。分母 $P(A_{it} = a_t \\mid \\bar{A}_{i,t-1}, \\bar{L}_{it})$ 通过对个体加权来解释混杂。例如，一个在给定其混杂因素值的情况下接受了“不太可能”的处理（即分母概率很小）的个体，会获得一个较大的权重。分子 $P(A_{it} = a_t \\mid \\bar{A}_{i,t-1})$ 用于减小权重的方差，从而得到更稳定和高效的估计，而不会重新引入偏倚。\n\n实现过程分为四个不同阶段：\n\n**1. 数据模拟**\n首先，我们根据指定的因果结构生成一个纵向数据集。对于一个包含 $n$ 个个体和 $T$ 个时间点的样本：\n- 基线混杂因素 $L_{i0}$ 从标准正态分布 $\\mathcal{N}(0,1)$ 中抽取。基线处理 $A_{i0}$ 固定为 $0$。\n- 对于每个后续时间步 $t \\in \\{1,\\dots,T\\}$，我们对系统的演变进行建模：\n    - 混杂因素 $L_{it}$ 由一个依赖于其先前值 $L_{i,t-1}$ 和先前处理 $A_{i,t-1}$ 的线性模型生成：$L_{it} = c_1 L_{i,t-1} + c_2 A_{i,t-1} + \\varepsilon_{it}$。这建立了有问题的关联 $A_{t-1} \\to L_t$。\n    - 处理 $A_{it}$ 通过一个以过去处理 $A_{i,t-1}$ 和当前混杂因素值 $L_{it}$ 为条件的逻辑斯蒂模型分配：$\\Pr(A_{it} = 1) = \\text{expit}(\\alpha_0 + \\alpha_1 A_{i,t-1} + \\alpha_2 L_{it})$。这建立了混杂关联 $L_t \\to A_t$。\n- 最后，结局 $Y_i$ 被生成为累积处理 $K_i = \\sum_{t=1}^T A_{it}$ 和最终混杂因素值 $L_{iT}$ 的函数。此模型 $Y_i = y_0 + \\gamma_1 K_i + y_2 L_{iT} + \\eta_i$ 中的参数 $\\gamma_1$ 是我们感兴趣的真实因果效应。$y_2 L_{iT}$ 项在分析 $K_i$ 与 $Y_i$ 之间的朴素关联时引入了混杂。\n\n**2. 权重估计**\n由于在现实世界场景中，权重公式中的真实概率是未知的，因此必须从数据中进行估计。问题指定在每个时间点 $t$ 拟合逻辑斯蒂回归模型：\n- **分子概率：** 通过将 $A_t$ 对 $A_{t-1}$ 作为预测变量进行逻辑斯蒂回归来估计 $P(A_{it} \\mid A_{i,t-1})$。\n- **分母概率：** 通过将 $A_t$ 对 $A_{t-1}$ 和 $L_{t}$ 作为预测变量进行逻辑斯蒂回归来估计 $P(A_{it} \\mid A_{i,t-1}, L_{it})$。\n对于每个个体，从这些拟合的模型中计算出他们实际接受的处理的估计概率。为了防止因个别个体的概率接近零而产生极端权重，估计的概率被截断在一个小区间 $[\\epsilon, 1-\\epsilon]$ 内。每个个体的最终稳定化权重 $SW_i$ 是这些逐时间点概率比率的乘积。\n\n**3. 平衡性验证**\n一个关键的诊断步骤是验证计算出的权重是否成功创建了一个伪群体，其中在每个时间点 $t$，混杂因素 $L_t$ 不再与处理 $A_t$ 相关联。这通过计算两个处理组（$A_t = 1$ 和 $A_t=0$）之间 $L_t$ 的绝对标准化均数差（SMD）来评估：\n$$\n\\text{SMD}_t = \\frac{\\left|\\mu_{t,1} - \\mu_{t,0}\\right|}{\\sqrt{\\frac{1}{2}\\left(s^2_{t,1} + s^2_{t,0}\\right)}}\n$$\n此处，$\\mu_{t,g}$ 和 $s^2_{t,g}$ 是在处理组 $g \\in \\{0,1\\}$ 内，使用最终稳定化权重 $SW_i$ 计算的 $L_t$ 的均值和方差。通常认为SMD值小于或等于 $0.1$ 表示达到了可接受的平衡。此检查在所有时间点进行。\n\n**4. 因果效应估计**\n在确认平衡性之后，我们可以估计边际结构模型 $E[Y_i \\mid K_i] = \\gamma_0 + \\gamma_1 K_i$ 的因果参数 $\\gamma_1$。这是通过拟合一个加权最小二乘（WLS）回归模型来实现的，该模型以结局 $Y_i$ 为因变量，以截距项和累积处理次数 $K_i = \\sum_{t=1}^T A_{it}$ 为自变量。估计的稳定化权重 $SW_i$ 被用作 WLS 过程中的权重。得到的 $K_i$ 的系数即为因果效应的估计值 $\\hat{\\gamma}_1$。\n\n以下程序实现了这整个流程，为每个指定的参数集执行模拟和估计。必要的统计模型（逻辑斯蒂回归，WLS）使用 `numpy` 和 `scipy.optimize` 从基本原理实现。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# A constant for probability clipping\nEPSILON = 1e-6\n\ndef _expit(x):\n    \"\"\"Numerically stable expit (logistic sigmoid) function.\"\"\"\n    return 1. / (1. + np.exp(-x))\n\ndef _logistic_regression(X, y):\n    \"\"\"\n    Fits a logistic regression model using scipy's optimizer.\n\n    Args:\n        X (np.ndarray): Design matrix with an intercept column.\n        y (np.ndarray): Binary outcome vector.\n\n    Returns:\n        np.ndarray: Fitted model coefficients (beta).\n    \"\"\"\n    def neg_log_likelihood(beta, X, y):\n        logits = X @ beta\n        # The log-likelihood for a Bernoulli-distributed outcome is sum(y*logits - log(1+exp(logits)))\n        # Using a stable computation for log(1+exp(x))\n        log_likelihood = np.sum(y * logits - np.logaddexp(0, logits))\n        return -log_likelihood\n\n    def grad_neg_log_likelihood(beta, X, y):\n        probs = _expit(X @ beta)\n        gradient = X.T @ (probs - y)\n        return gradient\n\n    initial_beta = np.zeros(X.shape[1])\n    result = minimize(\n        neg_log_likelihood,\n        initial_beta,\n        args=(X, y),\n        method='BFGS',\n        jac=grad_neg_log_likelihood,\n    )\n    if not result.success:\n        # This fallback is unlikely to be needed with the given problem setup\n        # but is good practice.\n        print(\"Warning: Logistic regression did not converge.\")\n\n    return result.x\n\ndef _weighted_mean(x, weights):\n    \"\"\"Computes the weighted mean.\"\"\"\n    sum_weights = np.sum(weights)\n    if sum_weights == 0:\n        return np.mean(x) if len(x)  0 else 0.0\n    return np.sum(weights * x) / sum_weights\n\ndef _weighted_var(x, weights):\n    \"\"\"Computes the weighted variance.\"\"\"\n    sum_weights = np.sum(weights)\n    if sum_weights == 0:\n        return 0.0\n    mean_w = _weighted_mean(x, weights)\n    return np.sum(weights * (x - mean_w)**2) / sum_weights\n\ndef run_simulation_case(n, T, alpha0, alpha1, alpha2, c1, c2, sigma_L, sigma_Y, y0, y2, gamma1_true, seed):\n    \"\"\"\n    Runs a single simulation case from data generation to final estimation.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # --- Data Generation ---\n    L = np.zeros((n, T + 1))\n    A = np.zeros((n, T + 1), dtype=int)\n\n    L[:, 0] = rng.normal(0, 1, size=n)\n    A[:, 0] = 0\n\n    for t in range(1, T + 1):\n        eps_L = rng.normal(0, sigma_L, size=n)\n        L[:, t] = c1 * L[:, t - 1] + c2 * A[:, t - 1] + eps_L\n        logits_A = alpha0 + alpha1 * A[:, t - 1] + alpha2 * L[:, t]\n        probs_A = _expit(logits_A)\n        A[:, t] = rng.binomial(1, probs_A, size=n)\n\n    K = np.sum(A[:, 1:], axis=1)\n    eta = rng.normal(0, sigma_Y, size=n)\n    Y = y0 + gamma1_true * K + y2 * L[:, T] + eta\n\n    # --- Estimation of Stabilized Weights (SW) ---\n    stabilized_weights = np.ones(n)\n\n    for t in range(1, T + 1):\n        X_num = np.c_[np.ones(n), A[:, t - 1]]\n        beta_num = _logistic_regression(X_num, A[:, t])\n        p_num = _expit(X_num @ beta_num)\n\n        X_den = np.c_[np.ones(n), A[:, t - 1], L[:, t]]\n        beta_den = _logistic_regression(X_den, A[:, t])\n        p_den = _expit(X_den @ beta_den)\n\n        p_num = np.clip(p_num, EPSILON, 1 - EPSILON)\n        p_den = np.clip(p_den, EPSILON, 1 - EPSILON)\n\n        p_num_obs = np.where(A[:, t] == 1, p_num, 1 - p_num)\n        p_den_obs = np.where(A[:, t] == 1, p_den, 1 - p_den)\n\n        stabilized_weights *= (p_num_obs / p_den_obs)\n\n    # --- Balance Diagnostics (SMD) ---\n    smds = []\n    for t in range(1, T + 1):\n        is_treated = (A[:, t] == 1)\n        is_control = (A[:, t] == 0)\n\n        L_t_treated, L_t_control = L[is_treated, t], L[is_control, t]\n        sw_treated, sw_control = stabilized_weights[is_treated], stabilized_weights[is_control]\n\n        if len(L_t_treated) == 0 or len(L_t_control) == 0:\n            smds.append(np.nan)\n            continue\n            \n        mu_t1 = _weighted_mean(L_t_treated, sw_treated)\n        mu_t0 = _weighted_mean(L_t_control, sw_control)\n        s2_t1 = _weighted_var(L_t_treated, sw_treated)\n        s2_t0 = _weighted_var(L_t_control, sw_control)\n\n        pooled_sd = np.sqrt(0.5 * (s2_t1 + s2_t0))\n        smd = np.abs(mu_t1 - mu_t0) / pooled_sd if pooled_sd > 0 else 0.0\n        smds.append(smd)\n\n    max_smd = np.nanmax(smds) if smds else 0.0\n    balance_indicator = 1 if max_smd = 0.1 else 0\n\n    # --- MSM Estimation ---\n    X_msm = np.c_[np.ones(n), K]\n    \n    # Weighted Least Squares: beta = (X'WX)^-1 * (X'WY)\n    XT_W = X_msm.T * stabilized_weights\n    XT_W_X = XT_W @ X_msm\n    XT_W_Y = XT_W @ Y\n\n    try:\n        beta_msm = np.linalg.solve(XT_W_X, XT_W_Y)\n        gamma1_hat = beta_msm[1]\n    except np.linalg.LinAlgError:\n        gamma1_hat = np.nan\n\n    return [round(gamma1_hat, 6), round(max_smd, 6), balance_indicator]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # n, T, alpha0, alpha1, alpha2, c1, c2, sigma_L, sigma_Y, y0, y2, gamma1, seed\n        (5000, 3, -0.2, 0.3, 1.0, 0.5, 0.8, 1.0, 1.0, 0.0, 0.7, 1.5, 42),\n        (3000, 3, -0.1, 0.2, 2.0, 0.5, 0.8, 1.0, 1.0, 0.0, 0.7, 1.5, 123),\n        (800, 2, -0.3, 0.4, 0.8, 0.5, 0.8, 0.8, 1.2, 0.0, 0.7, 1.5, 7),\n    ]\n\n    results = []\n    for case in test_cases:\n        params = {\n            \"n\": case[0], \"T\": case[1], \"alpha0\": case[2], \"alpha1\": case[3],\n            \"alpha2\": case[4], \"c1\": case[5], \"c2\": case[6], \"sigma_L\": case[7],\n            \"sigma_Y\": case[8], \"y0\": case[9], \"y2\": case[10],\n            \"gamma1_true\": case[11], \"seed\": case[12]\n        }\n        result = run_simulation_case(**params)\n        results.append(result)\n\n    formatted_results = \",\".join([f\"[{','.join(map(str, r))}]\" for r in results])\n    print(f\"[{formatted_results}]\")\n\nsolve()\n```", "id": "4943123"}]}