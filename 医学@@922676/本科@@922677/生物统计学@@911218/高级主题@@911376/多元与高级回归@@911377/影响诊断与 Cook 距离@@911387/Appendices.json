{"hands_on_practices": [{"introduction": "要真正理解影响诊断，最好从动手计算开始。本练习将引导您为一个包含重复数据点的小型特定数据集计算杠杆值和库克距离，通过这个具体的例子，您将亲身体会到预测变量的结构（例如数据点的重复）如何直接影响其潜在的影响力 [@problem_id:4916300]。", "problem": "考虑一个包含截距项和单个协变量的简单线性回归模型，该模型通过普通最小二乘法 (OLS) 进行拟合。设设计矩阵为\n$$\nX \\;=\\; \\begin{pmatrix}\n1  0 \\\\\n1  1 \\\\\n1  2 \\\\\n1  2 \\\\\n1  3 \\\\\n1  4\n\\end{pmatrix},\n$$\n因此，观测值 $i=3$ 和 $i=4$ 是具有相同协变量向量的重复设计点。假设 OLS 的经典线性模型假设成立。设响应向量为\n$$\ny \\;=\\; \\begin{pmatrix}\n1 \\\\\n2 \\\\\n3 + d \\\\\n3 \\\\\n4 \\\\\n5\n\\end{pmatrix},\n$$\n其中 $d$ 是一个实常数。仅从 OLS 作为到 $X$ 列空间的正交投影的核心定义以及标准影响诊断框架出发，按以下步骤进行：\n\n- 使用投影框架计算杠杆值 $h_{33}$ 和 $h_{44}$，并证明它们是相同的。简要解释这种重复设计点在概念上如何影响杠杆值和影响诊断。\n\n- 然后，推导并计算此模型中观测值 $i=3$ 的库克距离。您的推导应从第一性原理（OLS 投影和通过删除案例的影响定义）开始，并得出观测值 $i=3$ 的库克距离的闭式数值，以给定的 $X$ 和 $y$ 表示。\n\n将观测值 $i=3$ 的最终库克距离以单个精确数字的形式给出。无需四舍五入。", "solution": "根据指定标准对问题进行验证。\n\n### 第 1 步：提取已知条件\n- **模型：** 简单线性回归，$y = \\beta_0 + \\beta_1 x + \\epsilon$，通过普通最小二乘法 (OLS) 拟合。\n- **设计矩阵, $X$**：\n$$\nX \\;=\\; \\begin{pmatrix}\n1  0 \\\\\n1  1 \\\\\n1  2 \\\\\n1  2 \\\\\n1  3 \\\\\n1  4\n\\end{pmatrix}\n$$\n- **响应向量, $y$**：\n$$\ny \\;=\\; \\begin{pmatrix}\n1 \\\\\n2 \\\\\n3 + d \\\\\n3 \\\\\n4 \\\\\n5\n\\end{pmatrix}\n$$\n- **参数：** $n=6$ 个观测值，$p=2$ 个回归系数（截距项和斜率）。$d$ 是一个实常数。\n- **假设：** OLS 的经典线性模型假设成立。\n- **任务：**\n    1. 计算杠杆值 $h_{33}$ 和 $h_{44}$ 并证明它们相同。\n    2. 解释重复设计点的影响。\n    3. 从第一性原理推导并计算观测值 $i=3$ 的库克距离 $D_3$。\n    4. 将 $D_3$ 以单个精确数字的形式给出。\n\n### 第 2 步：使用提取的已知条件进行验证\n1.  **科学依据：** 该问题设置在线性回归理论的标准框架内，特别是影响诊断。所有概念（OLS、杠杆值、库克距离）在统计学中都有明确的定义并且是基础性的。该问题在科学上是合理的。\n2.  **适定性：** 设计矩阵 $X$ 有两列。第一列是全为 1 的向量。第二列是 $[0, 1, 2, 2, 3, 4]^T$。这两个向量是线性无关的，因此矩阵 $X$ 具有满列秩。这确保了矩阵 $X^T X$ 是可逆的，并且 OLS 估计是唯一的。问题提供了所有必要的信息。符号常数 $d$ 的存在可能暗示结果依赖于它，但如下所示，它在库克距离的最终计算中被消除了。因此，要求提供“单个精确数字”与问题的结构是一致的。该问题是适定的。\n3.  **客观性：** 问题使用精确的数学定义和客观的语言陈述。没有主观或基于意见的陈述。\n\n### 第 3 步：结论与行动\n该问题是**有效的**。将提供完整的解答。\n\n### 解答\n\n普通最小二乘法 (OLS) 框架旨在找到系数向量 $\\hat{\\beta}$，以最小化残差平方和 $S(\\beta) = (y-X\\beta)^T(y-X\\beta)$。这个最小化问题的解在几何上被解释为响应向量 $y$ 到设计矩阵 $X$ 的列空间上的正交投影。拟合值向量 $\\hat{y}$ 就是这个投影：\n$$\n\\hat{y} = P_X y = X(X^T X)^{-1} X^T y\n$$\n矩阵 $H = X(X^T X)^{-1} X^T$ 被称为“帽子矩阵”，因为它将 $y$ 映射到 $\\hat{y}$。该矩阵的对角线元素 $h_{ii}$ 被称为观测值的杠杆值。\n\n**第 1 部分：杠杆值计算与解释**\n\n第 $i$ 个观测值的杠杆值由 $h_{ii} = x_i^T (X^T X)^{-1} x_i$ 给出，其中 $x_i^T$ 是设计矩阵 $X$ 的第 $i$ 行。为了计算杠杆值，我们首先需要计算 $(X^T X)^{-1}$。\n\n设计矩阵为 $X = \\begin{pmatrix} 1  0 \\\\ 1  1 \\\\ 1  2 \\\\ 1  2 \\\\ 1  3 \\\\ 1  4 \\end{pmatrix}$。\n其转置为 $X^T = \\begin{pmatrix} 1  1  1  1  1  1 \\\\ 0  1  2  2  3  4 \\end{pmatrix}$。\n\n乘积 $X^T X$ 是：\n$$\nX^T X = \\begin{pmatrix} 6  \\sum_{i=1}^6 x_{i2} \\\\ \\sum_{i=1}^6 x_{i2}  \\sum_{i=1}^6 x_{i2}^2 \\end{pmatrix} = \\begin{pmatrix} 6  0+1+2+2+3+4 \\\\ 0+1+2+2+3+4  0^2+1^2+2^2+2^2+3^2+4^2 \\end{pmatrix} = \\begin{pmatrix} 6  12 \\\\ 12  34 \\end{pmatrix}\n$$\n行列式为 $\\det(X^T X) = (6)(34) - (12)(12) = 204 - 144 = 60$。\n逆矩阵是：\n$$\n(X^T X)^{-1} = \\frac{1}{60} \\begin{pmatrix} 34  -12 \\\\ -12  6 \\end{pmatrix} = \\begin{pmatrix} \\frac{34}{60}  -\\frac{12}{60} \\\\ -\\frac{12}{60}  \\frac{6}{60} \\end{pmatrix} = \\begin{pmatrix} \\frac{17}{30}  -\\frac{1}{5} \\\\ -\\frac{1}{5}  \\frac{1}{10} \\end{pmatrix}\n$$\n现在，我们计算观测值 $i=3$ 的杠杆值。协变量向量是 $x_3^T = \\begin{pmatrix} 1  2 \\end{pmatrix}$。\n$$\nh_{33} = x_3^T (X^T X)^{-1} x_3 = \\begin{pmatrix} 1  2 \\end{pmatrix} \\begin{pmatrix} \\frac{17}{30}  -\\frac{1}{5} \\\\ -\\frac{1}{5}  \\frac{1}{10} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n$$\n$$\nh_{33} = \\begin{pmatrix} 1 \\cdot \\frac{17}{30} + 2 \\cdot (-\\frac{1}{5})  1 \\cdot (-\\frac{1}{5}) + 2 \\cdot \\frac{1}{10} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{17}{30} - \\frac{12}{30}  -\\frac{1}{5} + \\frac{1}{5} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{30}  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n$$\n$$\nh_{33} = \\frac{5}{30} \\cdot 1 + 0 \\cdot 2 = \\frac{5}{30} = \\frac{1}{6}\n$$\n对于观测值 $i=4$，协变量向量是 $x_4^T = \\begin{pmatrix} 1  2 \\end{pmatrix}$，与 $x_3^T$ 相同。因此，$h_{44}$ 的计算是相同的：\n$$\nh_{44} = x_4^T (X^T X)^{-1} x_4 = x_3^T (X^T X)^{-1} x_3 = h_{33} = \\frac{1}{6}\n$$\n因此，$h_{33}$ 和 $h_{44}$ 是相同的，其值为 $\\frac{1}{6}$。\n\n从概念上讲，杠杆值 $h_{ii}$ 衡量响应值 $y_i$ 对其自身拟合值 $\\hat{y}_i$ 的影响，因为 $\\hat{y}_i = \\sum_j h_{ij} y_j$。它是在预测变量空间中一个观测值的“偏远性”或“潜在”影响力的度量。由于杠杆值仅取决于设计矩阵 $X$，任何具有相同协变量向量的两个观测值（重复设计点）在预测变量空间中将具有相同的位置，因此必须具有相同的杠杆值。它们的影响潜力，由它们在 $X$ 空间中与数据云中心的距离决定，是相同的。\n\n**第 2 部分：库克距离的推导与计算**\n\n观测值 $i$ 的库克距离是当删除第 $i$ 个观测值时，估计系数总体变化的度量。推导从这个案例删除原则开始。其正式定义为：\n$$\nD_i = \\frac{(\\hat{\\beta} - \\hat{\\beta}_{(i)})^T (X^T X) (\\hat{\\beta} - \\hat{\\beta}_{(i)})}{p s^2}\n$$\n其中 $\\hat{\\beta}$ 是来自完整数据集的 OLS 估计，$\\hat{\\beta}_{(i)}$ 是移除观测值 $i$ 后的估计，$p$ 是参数数量，$s^2$ 是来自完整模型的均方误差。我们需要计算 $D_3$。\n\n1.  **计算完整模型的估计值 $\\hat{\\beta}$：**\n    首先，我们计算 $X^T y$：\n    $$\n    X^T y = \\begin{pmatrix} 1  1  1  1  1  1 \\\\ 0  1  2  2  3  4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3+d \\\\ 3 \\\\ 4 \\\\ 5 \\end{pmatrix} = \\begin{pmatrix} 1+2+3+d+3+4+5 \\\\ 0(1)+1(2)+2(3+d)+2(3)+3(4)+4(5) \\end{pmatrix} = \\begin{pmatrix} 18+d \\\\ 46+2d \\end{pmatrix}\n    $$\n    然后，$\\hat{\\beta} = (X^T X)^{-1} X^T y$：\n    $$\n    \\hat{\\beta} = \\begin{pmatrix} \\hat{\\beta}_0 \\\\ \\hat{\\beta}_1 \\end{pmatrix} = \\begin{pmatrix} \\frac{17}{30}  -\\frac{1}{5} \\\\ -\\frac{1}{5}  \\frac{1}{10} \\end{pmatrix} \\begin{pmatrix} 18+d \\\\ 46+2d \\end{pmatrix}\n    $$\n    $$\n    \\hat{\\beta}_0 = \\frac{17}{30}(18+d) - \\frac{1}{5}(46+2d) = \\frac{17(18)+17d - 6(46+2d)}{30} = \\frac{306+17d-276-12d}{30} = \\frac{30+5d}{30} = 1 + \\frac{d}{6}\n    $$\n    $$\n    \\hat{\\beta}_1 = -\\frac{1}{5}(18+d) + \\frac{1}{10}(46+2d) = \\frac{-2(18+d) + (46+2d)}{10} = \\frac{-36-2d+46+2d}{10} = \\frac{10}{10} = 1\n    $$\n    所以，$\\hat{\\beta} = \\begin{pmatrix} 1 + d/6 \\\\ 1 \\end{pmatrix}$。\n\n2.  **计算删除案例后的估计值 $\\hat{\\beta}_{(3)}$：**\n    我们移除第 3 个观测值。简化后的数据集是 $(X_{(3)}, y_{(3)})$。\n    $$\n    X_{(3)} = \\begin{pmatrix} 1  0 \\\\ 1  1 \\\\ 1  2 \\\\ 1  3 \\\\ 1  4 \\end{pmatrix}, \\quad y_{(3)} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{pmatrix}\n    $$\n    对于这个简化后的数据集，很明显 $y_i = x_{i2} + 1$。OLS 拟合将是完美的。因此，$\\hat{\\beta}_{(3)} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$。为了完整起见，我们验证一下：\n    $$\n    X_{(3)}^T X_{(3)} = \\begin{pmatrix} 5  10 \\\\ 10  30 \\end{pmatrix}, \\quad X_{(3)}^T y_{(3)} = \\begin{pmatrix} 15 \\\\ 40 \\end{pmatrix}\n    $$\n    $$\n    (X_{(3)}^T X_{(3)})^{-1} = \\frac{1}{50} \\begin{pmatrix} 30  -10 \\\\ -10  5 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{5}  -\\frac{1}{5} \\\\ -\\frac{1}{5}  \\frac{1}{10} \\end{pmatrix}\n    $$\n    $$\n    \\hat{\\beta}_{(3)} = \\begin{pmatrix} \\frac{3}{5}  -\\frac{1}{5} \\\\ -\\frac{1}{5}  \\frac{1}{10} \\end{pmatrix} \\begin{pmatrix} 15 \\\\ 40 \\end{pmatrix} = \\begin{pmatrix} 9-8 \\\\ -3+4 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n    $$\n\n3.  **计算 $D_3$ 的分子：**\n    系数的差异是 $\\hat{\\beta} - \\hat{\\beta}_{(3)} = \\begin{pmatrix} 1 + d/6 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} d/6 \\\\ 0 \\end{pmatrix}$。\n    分子是 $(\\hat{\\beta} - \\hat{\\beta}_{(3)})^T (X^T X) (\\hat{\\beta} - \\hat{\\beta}_{(3)})$：\n    $$\n    \\begin{pmatrix} \\frac{d}{6}  0 \\end{pmatrix} \\begin{pmatrix} 6  12 \\\\ 12  34 \\end{pmatrix} \\begin{pmatrix} d/6 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{d}{6} \\cdot 6 + 0 \\cdot 12  \\frac{d}{6} \\cdot 12 + 0 \\cdot 34 \\end{pmatrix} \\begin{pmatrix} d/6 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} d  2d \\end{pmatrix} \\begin{pmatrix} d/6 \\\\ 0 \\end{pmatrix} = d \\cdot \\frac{d}{6} = \\frac{d^2}{6}\n    $$\n\n4.  **计算 $D_3$ 的分母：**\n    分母是 $p s^2$。这里 $p=2$。我们需要 $s^2$，即完整模型的均方误差。\n    $s^2 = \\frac{SSE}{n-p} = \\frac{e^T e}{6-2}$。\n    首先，找到拟合值 $\\hat{y} = X\\hat{\\beta}$：\n    $$\n    \\hat{y} = \\begin{pmatrix} 1  0 \\\\ 1  1 \\\\ 1  2 \\\\ 1  2 \\\\ 1  3 \\\\ 1  4 \\end{pmatrix} \\begin{pmatrix} 1+d/6 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1+d/6 \\\\ 2+d/6 \\\\ 3+d/6 \\\\ 3+d/6 \\\\ 4+d/6 \\\\ 5+d/6 \\end{pmatrix}\n    $$\n    现在，找到残差 $e = y - \\hat{y}$：\n    $$\n    e = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3+d \\\\ 3 \\\\ 4 \\\\ 5 \\end{pmatrix} - \\begin{pmatrix} 1+d/6 \\\\ 2+d/6 \\\\ 3+d/6 \\\\ 3+d/6 \\\\ 4+d/6 \\\\ 5+d/6 \\end{pmatrix} = \\begin{pmatrix} -d/6 \\\\ -d/6 \\\\ d-d/6 \\\\ -d/6 \\\\ -d/6 \\\\ -d/6 \\end{pmatrix} = \\begin{pmatrix} -d/6 \\\\ -d/6 \\\\ 5d/6 \\\\ -d/6 \\\\ -d/6 \\\\ -d/6 \\end{pmatrix}\n    $$\n    残差平方和是 $SSE = e^T e$：\n    $$\n    SSE = 5 \\cdot \\left(-\\frac{d}{6}\\right)^2 + \\left(\\frac{5d}{6}\\right)^2 = 5 \\cdot \\frac{d^2}{36} + \\frac{25d^2}{36} = \\frac{30d^2}{36} = \\frac{5d^2}{6}\n    $$\n    均方误差是：\n    $$\n    s^2 = \\frac{SSE}{n-p} = \\frac{5d^2/6}{4} = \\frac{5d^2}{24}\n    $$\n    $D_3$ 的分母是 $p s^2 = 2 \\cdot \\frac{5d^2}{24} = \\frac{10d^2}{24} = \\frac{5d^2}{12}$。\n\n5.  **组合 $D_3$ 的最终值：**\n    $$\n    D_3 = \\frac{\\text{分子}}{\\text{分母}} = \\frac{d^2/6}{5d^2/12}\n    $$\n    假设 $d \\neq 0$（否则 $y$ 拟合成一条完美的直线，所有残差和 $D_3$ 都为 $0$），$d^2$ 项相互抵消：\n    $$\n    D_3 = \\frac{1/6}{5/12} = \\frac{1}{6} \\cdot \\frac{12}{5} = \\frac{12}{30} = \\frac{2}{5}\n    $$\n观测值 $i=3$ 的库克距离是 $\\frac{2}{5}$。", "answer": "$$\\boxed{\\frac{2}{5}}$$", "id": "4916300"}, {"introduction": "库克距离衡量的是整体影响，但我们常常需要了解单个观测点如何影响某个特定系数，例如一个关键的临床处理效应。本练习要求您从第一性原理出发推导 DFBETAS 统计量，从而更深入地理解观测点的杠杆值和残差如何共同作用以改变单个参数的估计值 [@problem_id:4916346]。", "problem": "一个生物统计学团队正在拟合一个多元线性回归模型，以评估一项心血管研究中具有临床意义的治疗效果。结果变量是收缩压，预测变量包括一个截距、一个二元治疗指示变量和以十年为单位的年龄。该分析使用普通最小二乘法（OLS）。设设计矩阵表示为 $X \\in \\mathbb{R}^{n \\times p}$，参数向量表示为 $\\beta \\in \\mathbb{R}^{p}$，OLS 估计值表示为 $\\hat{\\beta}$。设 $C = (X^{\\top}X)^{-1}$，并设第 $i$ 个受试者的协变量行向量为 $x_{i}^{\\top}$，OLS 残差为 $e_{i} = y_{i} - x_{i}^{\\top}\\hat{\\beta}$。考虑对具有临床意义的治疗系数的标准化删除效应，对于系数索引 $j = 2$（治疗效应）和特定受试者 $i$，该效应表示为 $\\mathrm{DFBETAS}_{j(i)}$。\n\n从 OLS 的标准线性模型恒等式（即模型 $y = X\\beta + \\varepsilon$、正规方程、投影（帽子）矩阵的定义，以及残差和残差平方和的定义）出发，推导 $\\mathrm{DFBETAS}_{j(i)}$ 关于 $x_{i}$、$e_{i}$ 和 $C$ 的显式表达式，以及任何可以用这些量表示的所需量。利用此推导解释，为什么对于一个具有临床意义的系数，一个大的 $|\\mathrm{DFBETAS}_{j(i)}|$ 值对应于一个会实质性改变其推断的案例。您的推导不得假设或引用任何现成的影响力公式；它应遵循 OLS 估计量的第一性原理和删除一个案例的效果。\n\n然后，对于协变量向量为\n$$\nx_{i^{\\ast}}^{\\top} = \\begin{pmatrix} 1  1  6 \\end{pmatrix},\n$$\n的特定受试者 $i^{\\ast}$，给定\n$$\nC = \\begin{pmatrix}\n0.02  0  0 \\\\\n0  0.05  0.002 \\\\\n0  0.002  0.01\n\\end{pmatrix}, \\quad n = 50, \\quad p = 3, \\quad s = 2, \\quad e_{i^{\\ast}} = 3,\n$$\n使用您推导出的公式计算 $\\mathrm{DFBETAS}_{2(i^{\\ast})}$ 的数值。这里 $s$ 表示来自完整模型拟合的残差标准差。将您的最终数值答案四舍五入到三位有效数字。最终值不需要单位。", "solution": "问题要求从普通最小二乘（OLS）回归的第一性原理出发，推导 $\\mathrm{DFBETAS}_{j(i)}$ 的表达式，解释其含义，并对一个具体案例进行数值计算。\n\n线性模型由 $y = X\\beta + \\varepsilon$ 给出，其中 $y \\in \\mathbb{R}^{n}$ 是结果向量，$X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$\\beta \\in \\mathbb{R}^{p}$ 是参数向量，$\\varepsilon \\in \\mathbb{R}^{n}$ 是误差向量，满足 $\\mathbb{E}[\\varepsilon] = 0$ 和 $\\mathrm{Var}(\\varepsilon) = \\sigma^2 I$。$\\beta$ 的 OLS 估计值通过求解正规方程 $X^{\\top}X \\hat{\\beta} = X^{\\top}y$ 得到，即 $\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y$。设 $C = (X^{\\top}X)^{-1}$。\n\n$\\mathrm{DFBETAS}_{j(i)}$ 这个量度量了当从数据集中移除第 $i$ 个观测值时，第 $j$ 个系数 $\\hat{\\beta}_j$ 的变化，并用 $\\hat{\\beta}_j$ 的标准误差估计值进行缩放。标准定义为：\n$$\n\\mathrm{DFBETAS}_{j(i)} = \\frac{\\hat{\\beta}_j - \\hat{\\beta}_{j(i)}}{\\mathrm{SE}_{(i)}(\\hat{\\beta}_j)}\n$$\n其中 $\\hat{\\beta}_{(i)}$ 是在没有观测值 $i$ 的情况下计算出的 $\\beta$ 的 OLS 估计值，而 $\\mathrm{SE}_{(i)}(\\hat{\\beta}_j)$ 是从删除了观测值 $i$ 的数据集中计算出的 $\\hat{\\beta}_j$ 的标准误差估计值。按照惯例，这个标准误差取为 $s_{(i)}\\sqrt{C_{jj}}$，其中 $s_{(i)}$ 是在没有观测值 $i$ 的情况下模型拟合的残差标准误差，而 $C_{jj}$ 是来自完整数据拟合的 $C = (X^{\\top}X)^{-1}$ 的第 $(j,j)$ 个元素。\n\n推导过程分为三个主要步骤：\n1. 找到差值 $\\hat{\\beta} - \\hat{\\beta}_{(i)}$ 的表达式。\n2. 找到缩放因子 $s_{(i)}$ 的表达式。\n3. 将这些结合起来，形成 $\\mathrm{DFBETAS}_{j(i)}$ 的最终表达式。\n\n**1. $\\hat{\\beta} - \\hat{\\beta}_{(i)}$ 的推导**\n\n设 $X_{(i)}$ 和 $y_{(i)}$ 是移除了第 $i$ 行（观测值）的设计矩阵和响应向量。从这个简化的数据集中得到的 OLS 估计值为 $\\hat{\\beta}_{(i)} = (X_{(i)}^{\\top}X_{(i)})^{-1}X_{(i)}^{\\top}y_{(i)}$。\n\n完整数据矩阵可以表示为简化数据矩阵与第 $i$ 个观测值（协变量向量为 $x_i^{\\top}$，结果为 $y_i$）的贡献之和：\n$$\nX^{\\top}X = \\sum_{k=1}^{n} x_k x_k^{\\top} = X_{(i)}^{\\top}X_{(i)} + x_i x_i^{\\top}\n$$\n$$\nX^{\\top}y = \\sum_{k=1}^{n} x_k y_k = X_{(i)}^{\\top}y_{(i)} + x_i y_i\n$$\n从第一个恒等式，我们有 $X_{(i)}^{\\top}X_{(i)} = X^{\\top}X - x_i x_i^{\\top}$。为了求其逆，我们使用 Sherman-Morrison-Woodbury 公式进行秩-1 更新：$(A - uv^{\\top})^{-1} = A^{-1} + \\frac{A^{-1}uv^{\\top}A^{-1}}{1 - v^{\\top}A^{-1}u}$。\n令 $A = X^{\\top}X$ 和 $u=v=x_i$，我们得到：\n$$\n(X_{(i)}^{\\top}X_{(i)})^{-1} = (X^{\\top}X - x_i x_i^{\\top})^{-1} = (X^{\\top}X)^{-1} + \\frac{(X^{\\top}X)^{-1}x_i x_i^{\\top}(X^{\\top}X)^{-1}}{1 - x_i^{\\top}(X^{\\top}X)^{-1}x_i}\n$$\n设 $C = (X^{\\top}X)^{-1}$ 并定义第 $i$ 个观测值的杠杆值为 $h_{ii} = x_i^{\\top}Cx_i$。表达式简化为：\n$$\n(X_{(i)}^{\\top}X_{(i)})^{-1} = C + \\frac{Cx_i x_i^{\\top}C}{1 - h_{ii}}\n$$\n现在，使用 $X_{(i)}^{\\top}y_{(i)} = X^{\\top}y - x_i y_i$，我们可以写出 $\\hat{\\beta}_{(i)}$：\n$$\n\\hat{\\beta}_{(i)} = \\left(C + \\frac{Cx_i x_i^{\\top}C}{1 - h_{ii}}\\right) (X^{\\top}y - x_i y_i)\n$$\n展开这个乘积：\n$$\n\\hat{\\beta}_{(i)} = C(X^{\\top}y) - C x_i y_i + \\frac{Cx_i x_i^{\\top}C(X^{\\top}y)}{1 - h_{ii}} - \\frac{Cx_i x_i^{\\top}C x_i y_i}{1 - h_{ii}}\n$$\n我们认识到 $\\hat{\\beta} = C(X^{\\top}y)$，所以 $x_i^{\\top}\\hat{\\beta} = x_i^{\\top}C(X^{\\top}y)$。并且，$x_i^{\\top}C x_i = h_{ii}$。代入这些可得：\n$$\n\\hat{\\beta}_{(i)} = \\hat{\\beta} - C x_i y_i + \\frac{Cx_i(x_i^{\\top}\\hat{\\beta})}{1 - h_{ii}} - \\frac{Cx_i(h_{ii})y_i}{1 - h_{ii}}\n$$\n合并具有公因子 $Cx_i/(1 - h_{ii})$ 的项：\n$$\n\\hat{\\beta}_{(i)} = \\hat{\\beta} - \\frac{Cx_i}{1 - h_{ii}} \\left( (1-h_{ii})y_i - x_i^{\\top}\\hat{\\beta} + h_{ii}y_i \\right)\n$$\n$$\n\\hat{\\beta}_{(i)} = \\hat{\\beta} - \\frac{Cx_i}{1 - h_{ii}} (y_i - h_{ii}y_i - x_i^{\\top}\\hat{\\beta} + h_{ii}y_i)\n$$\n$$\n\\hat{\\beta}_{(i)} = \\hat{\\beta} - \\frac{Cx_i(y_i - x_i^{\\top}\\hat{\\beta})}{1 - h_{ii}}\n$$\n项 $y_i - x_i^{\\top}\\hat{\\beta}$ 是第 $i$ 个观测值的 OLS 残差 $e_i$。因此，我们得到了这个优美的结果：\n$$\n\\hat{\\beta} - \\hat{\\beta}_{(i)} = \\frac{C x_i e_i}{1 - h_{ii}}\n$$\n这个向量差的第 $j$ 个分量是：\n$$\n\\hat{\\beta}_j - \\hat{\\beta}_{j(i)} = \\frac{(C x_i)_j e_i}{1 - h_{ii}}\n$$\n其中 $(Cx_i)_j$ 是向量 $Cx_i$ 的第 $j$ 个元素。\n\n**2. $s_{(i)}$ 的推导**\n\n来自完整模型的残差标准误差是 $s = \\sqrt{\\frac{\\mathrm{RSS}}{n-p}}$，其中 $\\mathrm{RSS} = \\sum_{k=1}^{n} e_k^2$。从删除了观测值 $i$ 的模型得到的残差标准误差是 $s_{(i)} = \\sqrt{\\frac{\\mathrm{RSS}_{(i)}}{n-p-1}}$。一个标准的线性模型恒等式关联了这两个模型的残差平方和：\n$$\n\\mathrm{RSS}_{(i)} = \\mathrm{RSS} - \\frac{e_i^2}{1-h_{ii}}\n$$\n代入 $\\mathrm{RSS} = (n-p)s^2$：\n$$\n(n-p-1)s_{(i)}^2 = (n-p)s^2 - \\frac{e_i^2}{1-h_{ii}}\n$$\n这使我们能够根据完整模型拟合中可用的量来计算 $s_{(i)}$。\n\n**3. $\\mathrm{DFBETAS}_{j(i)}$ 的最终表达式**\n\n将推导出的分子表达式和分母的定义代入 $\\mathrm{DFBETAS}_{j(i)}$ 的定义中：\n$$\n\\mathrm{DFBETAS}_{j(i)} = \\frac{\\hat{\\beta}_j - \\hat{\\beta}_{j(i)}}{s_{(i)}\\sqrt{C_{jj}}} = \\frac{\\frac{(C x_i)_j e_i}{1 - h_{ii}}}{s_{(i)}\\sqrt{C_{jj}}} = \\frac{(C x_i)_j e_i}{(1-h_{ii})s_{(i)}\\sqrt{C_{jj}}}\n$$\n这就是所要求的显式表达式。\n\n**解释：**\n一个大的 $|\\mathrm{DFBETAS}_{j(i)}|$ 值表明观测值 $i$ 对系数 $\\hat{\\beta}_j$ 具有影响力。该公式表明，这种影响力主要来自两个来源：\n- **残差 ($e_i$)：** 模型预测不佳的观测值（大的 $|e_i|$）具有潜在的影响力。\n- **杠杆值 ($h_{ii}$):** 具有不寻常协变量模式的观测值（大的 $h_{ii}$，接近 $1$）其影响力将被因子 $1/(1-h_{ii})$ 放大。这样的点被称为高杠杆点。\n项 $(C x_i)_j$ 将观测值 $i$ 的特定协变量值与估计值的方差-协方差结构联系起来。一个大的 $|\\mathrm{DFBETAS}_{j(i)}|$（常见的阈值是 $1$ 或 $2/\\sqrt{n}$）表明，移除观测值 $i$ 将会相对于其标准误差显著改变估计值 $\\hat{\\beta}_j$。对于一个具有临床意义的系数，这意味着关于治疗效果的结论（例如，其大小、符号或统计显著性）可能依赖于这单个观测值，因此需要进一步调查。\n\n**数值计算：**\n对于受试者 $i^{\\ast}$，我们已知以下信息：\n- $j=2$（治疗系数）\n- $x_{i^{\\ast}}^{\\top} = \\begin{pmatrix} 1  1  6 \\end{pmatrix}$\n- $C = \\begin{pmatrix} 0.02  0  0 \\\\ 0  0.05  0.002 \\\\ 0  0.002  0.01 \\end{pmatrix}$\n- $e_{i^{\\ast}} = 3$\n- $s = 2$\n- $n = 50$\n- $p = 3$\n\n我们需要计算 $\\mathrm{DFBETAS}_{2(i^{\\ast})}$。让我们计算必要的组成部分。\n\n首先，杠杆值 $h_{i^{\\ast}i^{\\ast}}$：\n$h_{i^{\\ast}i^{\\ast}} = x_{i^{\\ast}}^{\\top} C x_{i^{\\ast}} = \\begin{pmatrix} 1  1  6 \\end{pmatrix} \\begin{pmatrix} 0.02  0  0 \\\\ 0  0.05  0.002 \\\\ 0  0.002  0.01 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 6 \\end{pmatrix}$\n$C x_{i^{\\ast}} = \\begin{pmatrix} 0.02(1) + 0(1) + 0(6) \\\\ 0(1) + 0.05(1) + 0.002(6) \\\\ 0(1) + 0.002(1) + 0.01(6) \\end{pmatrix} = \\begin{pmatrix} 0.02 \\\\ 0.05 + 0.012 \\\\ 0.002 + 0.06 \\end{pmatrix} = \\begin{pmatrix} 0.02 \\\\ 0.062 \\\\ 0.062 \\end{pmatrix}$\n$h_{i^{\\ast}i^{\\ast}} = \\begin{pmatrix} 1  1  6 \\end{pmatrix} \\begin{pmatrix} 0.02 \\\\ 0.062 \\\\ 0.062 \\end{pmatrix} = 1(0.02) + 1(0.062) + 6(0.062) = 0.02 + 0.062 + 0.372 = 0.454$.\n所以，$1-h_{i^{\\ast}i^{\\ast}} = 1 - 0.454 = 0.546$。\n\n其次，对于 $j=2$ 的项 $(C x_{i^{\\ast}})_j$：\n这是向量 $C x_{i^{\\ast}}$ 的第二个元素，即 $0.062$。\n\n第三，留一法残差标准误差 $s_{(i^{\\ast})}$：\n$s_{(i^{\\ast})}^2 = \\frac{(n-p)s^2 - e_{i^{\\ast}}^2/(1-h_{i^{\\ast}i^{\\ast}})}{n-p-1}$\n当 $n=50$, $p=3$, $s=2$, $e_{i^{\\ast}}=3$, 且 $h_{i^{\\ast}i^{\\ast}}=0.454$ 时：\n$s_{(i^{\\ast})}^2 = \\frac{(50-3)(2^2) - 3^2/(1-0.454)}{50-3-1} = \\frac{47(4) - 9/0.546}{46} = \\frac{188 - 16.483516...}{46} = \\frac{171.516483...}{46} = 3.728619...$\n$s_{(i^{\\ast})} = \\sqrt{3.728619...} \\approx 1.930963...$\n\n第四，对于 $j=2$ 的项 $\\sqrt{C_{jj}}$：\n$C_{22} = 0.05$，所以 $\\sqrt{C_{22}} = \\sqrt{0.05} \\approx 0.223606...$\n\n最后，我们组合 $\\mathrm{DFBETAS}_{2(i^{\\ast})}$：\n$$\n\\mathrm{DFBETAS}_{2(i^{\\ast})} = \\frac{(C x_{i^{\\ast}})_2 e_{i^{\\ast}}}{(1-h_{i^{\\ast}i^{\\ast}})s_{(i^{\\ast})}\\sqrt{C_{22}}} = \\frac{(0.062)(3)}{(0.546)(1.930963...)(0.223606...)}\n$$\n$$\n\\mathrm{DFBETAS}_{2(i^{\\ast})} = \\frac{0.186}{0.235741...} \\approx 0.788998...\n$$\n四舍五入到三位有效数字，我们得到 $0.789$。这个值低于常见的阈值 $1$，表明虽然这个案例有不可忽略的影响力，但其本身可能不被认为是高度有问题的。", "answer": "$$\n\\boxed{0.789}\n$$", "id": "4916346"}, {"introduction": "在现代数据分析中，尤其是在基因组学等领域，我们需要一种系统性的方法来筛选众多样本中的影响点。本计算实践将指导您构建一个完整的分析流程，从计算库克距离到使用置换检验和控制错误发现率（FDR），最终实现对大数据集中影响点的可靠识别 [@problem_id:4916287]。", "problem": "给定一个具有多个预测变量的经典正态线性模型，该模型代表了基因表达的场景，其中每个样本（行）是一个经过分析的生物标本，预测变量（列）编码了技术和生物协变量。该模型假设 $y = X \\beta + \\varepsilon$，其中 $y \\in \\mathbb{R}^n$，$X \\in \\mathbb{R}^{n \\times p}$，$\\beta \\in \\mathbb{R}^p$，以及 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$。从这个基础模型出发，推导计算样本杠杆值和库克距离所需的数学表达式，解释为什么杠杆值会改变残差对库克距离的贡献，并设计一个使用库克距离作为检验统计量来控制错误发现率 (FDR) 的筛选规则。然后，在一个程序中实现这个规则，并用它来处理下面的测试套件。\n\n你的算法任务如下，所有任务都必须基于普通最小二乘法 (OLS) 估计量和线性模型恒等式来实现，不得依赖本说明中给出的任何快捷公式：\n- 计算 OLS 拟合。\n- 计算残差和残差均方误差。\n- 计算帽子矩阵的对角线元素（杠杆值）。\n- 计算每个样本的库克距离。\n- 对每个样本，使用基于置换的零假设（其中响应 $y$ 被随机置换，而预测变量 $X$ 保持不变）为库克距离计算一个经验 p 值。使用 $B = 800$ 次置换，并为每个测试用例指定固定的伪随机数生成器种子以确保确定性。\n- 将 Benjamini–Hochberg (BH) 程序应用于这组 p 值，水平为 $\\alpha = 0.10$，以识别有影响的样本，从而控制被标记样本集中的 FDR。\n\n筛选规则要求：\n- 筛选规则必须是：“如果一个样本基于置换的库克距离 p 值被 Benjamini–Hochberg (BH) 程序在 $\\alpha = 0.10$ 水平下选中，则标记该样本。” 你必须在你的解决方案中证明此规则的合理性，并用代码实现它。\n\n每个测试用例的数据生成协议：\n- 对于每个用例，按如下方式生成 $X$：抽取 $n \\times p$ 个独立的标准正态分布项，对列进行中心化使其均值为 $0$，并将每列缩放至标准差为 $1$；然后附加一个全为 1 的截距列，形成一个包含 $p+1$ 列的设计矩阵。为了在特定样本中产生高杠杆值，将指定行（仅非截距列）乘以给定的放大因子。\n- 生成 $y$ 为 $y = X \\beta + \\eta$，其中 $\\eta$ 是均值为 $0$、标准差为 $\\sigma$ 的独立正态噪声；然后通过将给定值加到 $y$ 的指定条目上，施加指定的残差冲击。\n- 对样本使用基于零的索引表示所有量。\n- 确保 $n  p+1$，以便残差自由度严格为正。\n\n测试套件：\n- 情况 1（“理想路径”，具有单个极端概况）：\n  - 维度：$n = 30$, $p = 8$（不包括截距）。\n  - 随机种子：预测变量种子 $11$，响应种子 $17$。\n  - 系数：$\\beta \\in \\mathbb{R}^{9}$ 由 $\\beta = [0.5,\\, 0.9,\\,-0.5,\\,0.0,\\,0.7,\\,0.0,\\,-0.4,\\,0.2,\\,0.0]$ 给出，其中第一个条目是截距系数。\n  - 噪声标准差：$\\sigma = 0.4$。\n  - 高杠杆值：将行索引 $5$（仅前 $p$ 列，不包括截距）乘以因子 $4.0$。\n  - 残差冲击：将 $3.0$ 加到 $y[5]$。\n  - 置换种子：$1017$。\n- 情况 2（“边界情况”，无极端值）：\n  - 维度：$n = 25$, $p = 5$。\n  - 随机种子：预测变量种子 $19$，响应种子 $23$。\n  - 系数：$\\beta \\in \\mathbb{R}^{6}$ 由 $\\beta = [0.5,\\,0.6,\\,0.0,\\,-0.5,\\,0.3,\\,0.0]$ 给出。\n  - 噪声标准差：$\\sigma = 0.5$。\n  - 高杠杆值：无。\n  - 残差冲击：无。\n  - 置换种子：$2023$。\n- 情况 3（“边缘情况”，具有多个预测变量和多个极端值）：\n  - 维度：$n = 18$, $p = 10$。\n  - 随机种子：预测变量种子 $29$，响应种子 $31$。\n  - 系数：$\\beta \\in \\mathbb{R}^{11}$ 由 $\\beta = [0.5,\\,0.8,\\,-0.3,\\,0.0,\\,0.7,\\,0.0,\\,-0.2,\\,0.4,\\,0.0,\\,0.2,\\,-0.1]$ 给出。\n  - 噪声标准差：$\\sigma = 0.6$。\n  - 高杠杆值：将行索引 $2$ 乘以因子 $3.0$，并将行索引 $10$ 乘以因子 $2.5$（同样，仅前 $p$ 列）。\n  - 残差冲击：将 $2.4$ 加到 $y[2]$，并从 $y[15]$ 中减去 $2.4$。\n  - 置换种子：$3031$。\n\n实现说明：\n- 在整个过程中对样本使用基于零的索引。\n- 所有线性代数运算必须严格按照 OLS 框架的含义执行。除了置换方法所要求的外，不要使用任何外部近似。\n- 角度单位和物理单位不适用；输出是无单位的。\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，列表中的每个元素本身是针对每个测试用例，由 BH 程序标记出的基于零的索引列表，并按顺序排列。例如，一个有效的输出格式是 $[[i_1,i_2],[],[j_1]]$（这只是一个示例格式；你的实际索引必须根据上述数据计算得出）。", "solution": "该问题要求推导并实现一个统计筛选规则，用于在线性回归模型中识别有影响的数据点。这涉及回归诊断和多重假设检验中的几个基本概念。我将首先介绍理论推导和理由，然后是实现细节。\n\n分析基于正态线性模型：\n$$ y = X \\beta + \\varepsilon $$\n其中 $y \\in \\mathbb{R}^n$ 是响应向量，$X \\in \\mathbb{R}^{n \\times p_{eff}}$ 是秩为 $p_{eff}$ 的设计矩阵，$\\beta \\in \\mathbb{R}^{p_{eff}}$ 是未知系数向量，$\\varepsilon \\in \\mathbb{R}^n$ 是误差向量，假设其独立同分布为 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$。问题指定构建一个包含 $p$ 个预测变量和一个截距的设计矩阵，因此要估计的参数总数为 $p_{eff} = p+1$。我们需要 $n  p_{eff}$ 以确保模型是可识别的。\n\n**1. 普通最小二乘法 (OLS) 估计**\nOLS 估计量 $\\hat{\\beta}$ 的选择是为了最小化残差平方和 (RSS)，即 $RSS(\\beta) = (y - X\\beta)^T(y - X\\beta)$。为了找到最小值，我们将关于 $\\beta$ 的梯度设为零：\n$$ \\nabla_{\\beta} RSS = -2X^T(y - X\\beta) = 0 $$\n$$ X^T X \\beta = X^T y $$\n假设 $X$ 具有满列秩，则矩阵 $X^T X$ 是可逆的。因此，$\\beta$ 的 OLS 估计量是唯一的，并由下式给出：\n$$ \\hat{\\beta} = (X^T X)^{-1} X^T y $$\n\n**2. 帽子矩阵与杠杆值**\n通过应用估计的系数可以获得拟合值向量 $\\hat{y}$：\n$$ \\hat{y} = X\\hat{\\beta} = X(X^T X)^{-1} X^T y $$\n这引入了“帽子矩阵”$H$，定义为：\n$$ H = X(X^T X)^{-1} X^T $$\n帽子矩阵是一个 $n \\times n$ 的投影矩阵，它将响应向量 $y$ 映射到拟合值 $\\hat{y} = Hy$。该矩阵的对角线元素 $h_{ii} = [H]_{ii}$ 称为杠杆值。第 $i$ 个观测值的杠杆值 $h_{ii}$ 可以写为 $h_{ii} = x_i^T (X^T X)^{-1} x_i$，其中 $x_i^T$ 是 $X$ 的第 $i$ 行。它衡量了预测变量向量 $x_i$ 与预测变量空间中数据云中心的距离。高杠杆值表示观测值 $i$ 具有不寻常的预测变量值组合，因此对回归拟合有很大的潜在影响。杠杆值的主要性质是 $0 \\le h_{ii} \\le 1$ 且 $\\sum_{i=1}^n h_{ii} = \\operatorname{tr}(H) = p_{eff}$。\n\n**3. 残差与库克距离**\n残差是观测值与拟合值之间的差异：\n$$ e = y - \\hat{y} = (I - H)y $$\n误差方差 $\\sigma^2$ 的一个无偏估计量是均方误差 (MSE)，也记作 $\\hat{\\sigma}^2$：\n$$ \\hat{\\sigma}^2 = \\frac{e^T e}{n - p_{eff}} = \\frac{RSS}{n - p_{eff}} $$\n库克距离 $D_i$ 是衡量第 $i$ 个数据点影响力的一个指标。它量化了删除观测值 $i$ 对所有拟合值集合的影响。其定义为：\n$$ D_i = \\frac{(\\hat{y} - \\hat{y}_{(i)})^T (\\hat{y} - \\hat{y}_{(i)})}{p_{eff} \\hat{\\sigma}^2} = \\frac{\\sum_{j=1}^n (\\hat{y}_j - \\hat{y}_{j(i)})^2}{p_{eff} \\hat{\\sigma}^2} $$\n其中 $\\hat{y}_{(i)}$ 是从不包含观测值 $i$ 的回归拟合中获得的预测值向量。直接使用此定义进行计算会成本高昂，需要进行 $n$ 次单独的回归。一个标准的代数恒等式提供了一个更高效的公式。该恒等式将系数向量的变化与被排除点的属性联系起来：$\\hat{\\beta} - \\hat{\\beta}_{(i)} = (X^T X)^{-1} x_i \\frac{e_i}{1 - h_{ii}}$。利用这一点，拟合值的变化是：\n$$ \\hat{y} - \\hat{y}_{(i)} = X(\\hat{\\beta} - \\hat{\\beta}_{(i)}) = X(X^T X)^{-1} x_i \\frac{e_i}{1-h_{ii}} $$\n该向量的欧几里得范数平方是：\n$$ (\\hat{y} - \\hat{y}_{(i)})^T (\\hat{y} - \\hat{y}_{(i)}) = \\left(\\frac{e_i}{1-h_{ii}}\\right)^2 \\left(X(X^T X)^{-1} x_i\\right)^T \\left(X(X^T X)^{-1} x_i\\right) $$\n$$ = \\left(\\frac{e_i}{1-h_{ii}}\\right)^2 x_i^T (X^T X)^{-1} X^T X (X^T X)^{-1} x_i = \\left(\\frac{e_i}{1-h_{ii}}\\right)^2 x_i^T (X^T X)^{-1} x_i = \\frac{e_i^2 h_{ii}}{(1-h_{ii})^2} $$\n将此代入 $D_i$ 的定义，得到计算公式：\n$$ D_i = \\frac{e_i^2}{p_{eff} \\hat{\\sigma}^2} \\frac{h_{ii}}{(1-h_{ii})^2} $$\n这个表达式揭示了库克距离是两个组成部分的函数：残差平方 $e_i^2$ 和杠杆值 $h_{ii}$。第 $i$ 个标准化残差是 $r_i = e_i / (\\hat{\\sigma}\\sqrt{1-h_{ii}})$。库克距离可以写成 $D_i = \\frac{r_i^2}{p_{eff}} \\frac{h_{ii}}{1-h_{ii}}$。这种形式清楚地表明 $D_i$ 是残差大小（标准化的）和点的杠杆值的乘积。如果一个样本具有大残差、高杠杆值或两者的结合，它就可能具有影响力（大的 $D_i$）。项 $h_{ii}/(1-h_{ii})^2$ 表明，杠杆值非线性地放大了残差对影响得分的影响。当 $h_{ii} \\to 1$ 时，这个放大因子无界增长，意味着一个杠杆值非常高的点即使只有一个中等大小的残差，也可能具有极大的影响力。\n\n**4. 影响力的统计筛选规则**\n为了系统地识别有影响的样本，我们为每个样本构建一个假设检验。检验统计量是其库克距离 $D_i$。我们需要该统计量的零分布来评估其显著性。\n\n零假设 ($H_0$) 是预测变量 $X$ 和响应 $y$ 之间没有关系。在此假设下， $y$ 的元素任何置换都是等可能的。这构成了置换检验的基础。通过重复置换 $y$ 并为生成的零数据集重新计算库克距离，我们可以为 $D_i$ 生成一个经验零分布。\n\n程序如下：\n1.  根据原始数据计算观测到的库克距离 $D_i^{obs}$，其中 $i=1, \\dots, n$。\n2.  对于 $b=1, \\dots, B$，通过随机打乱 $y$ 生成一个置换后的响应向量 $y^{(b)}$。\n3.  对于每个 $y^{(b)}$，拟合模型 $y^{(b)} \\sim X$ 并计算全套库克距离 $\\{D_j^{(b)}\\}_{j=1}^n$。请注意，帽子矩阵 $H$ 和杠杆值 $h_{jj}$ 仅依赖于 $X$，在置换中保持不变。\n4.  汇集所有计算出的零统计量，形成一个大小为 $N_{null} = n \\times B$ 的单一零分布：$\\mathcal{D}_{null} = \\{D_j^{(b)} \\mid j=1, \\dots, n; b=1, \\dots, B\\}$。\n5.  每个观测统计量 $D_i^{obs}$ 的经验 p 值是至少与观测值一样极端的零统计量的比例：\n    $$ p_i = \\frac{1 + |\\{D \\in \\mathcal{D}_{null} \\mid D \\ge D_i^{obs}\\}|}{1 + N_{null}} $$\n    在分子和分母上加 $1$ 是一种标准做法，用于处理 $D_i^{obs}$ 大于零分布中任何值的情况，并避免 p 值恰好为零。\n\n**5. 错误发现率 (FDR) 控制**\n由于我们同时进行 $n$ 个检验（每个样本一个），控制族系误差率（例如 Bonferroni 校正）会过于保守。对于筛选问题，更合适的方法是控制错误发现率 (FDR)，定义为所有发现中假阳性所占的期望比例。Benjamini-Hochberg (BH) 程序可以实现这一目标。\n\n给定一组 $n$ 个 p 值 $p_1, \\dots, p_n$ 和一个期望的 FDR 水平 $\\alpha$：\n1.  将 p 值从小到大排序：$p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(n)}$。\n2.  找到最大的整数 $k$ 使得 $p_{(k)} \\le \\frac{k}{n} \\alpha$。\n3.  如果存在这样的 $k$，则拒绝与 p 值 $p_{(1)}, \\dots, p_{(k)}$ 对应的零假设。否则，不拒绝任何假设。\n\n**6. 最终筛选规则的理由**\n问题要求设计一个筛选规则来标记有影响的样本，同时将 FDR 控制在 $\\alpha=0.10$。该规则是：“如果一个样本基于置换的库克距离 p 值被 Benjamini–Hochberg (BH) 程序在 $\\alpha = 0.10$ 水平下选中，则标记该样本。” 这个规则是合理的：\n- 它使用了有原则的影响力度量——库克距离，该度量综合了来自残差和杠杆值的信息。\n- 它采用一种非参数、数据驱动的方法（置换检验）来确定观测到的影响得分的统计显著性，这种方法是稳健的，并且不依赖于对检验统计量本身的强分布假设。\n- 它正确地解决了筛选所有 $n$ 个样本时固有的多重检验问题，为被标记的样本集提供了严格的统计保证（FDR 控制）。这对于确保在像基因组学这样的高维场景中研究结果的可靠性至关重要。\n\n下面的实现为所提供的测试用例执行了这套完整的程序。", "answer": "```python\nimport numpy as np\n\ndef analyze_case(n, p, predictor_seed, response_seed, beta, sigma, leverage_mods, residual_shocks, perm_seed, B=800, alpha=0.10):\n    \"\"\"\n    Performs influence diagnostics and screening for a single test case.\n    \"\"\"\n    # === 1. Data Generation ===\n    p_eff = p + 1\n    \n    # Generate predictors X\n    rng_pred = np.random.default_rng(predictor_seed)\n    X_raw = rng_pred.standard_normal(size=(n, p))\n    \n    # Center and scale predictors\n    X_centered = X_raw - X_raw.mean(axis=0)\n    # Use ddof=1 for sample standard deviation, a common statistical convention\n    X_scaled = X_centered / X_raw.std(axis=0, ddof=1)\n    \n    # Prepend intercept column\n    X = np.c_[np.ones(n), X_scaled]\n    \n    # Apply high-leverage modifications\n    if leverage_mods:\n        for row_idx, factor in leverage_mods:\n            X[row_idx, 1:] *= factor\n            \n    # Generate response y\n    rng_resp = np.random.default_rng(response_seed)\n    noise = rng_resp.normal(0, sigma, size=n)\n    y = X @ np.array(beta) + noise\n    \n    # Apply residual shocks\n    if residual_shocks:\n        for row_idx, shock in residual_shocks:\n            y[row_idx] += shock\n\n    # === 2. Calculate Observed Statistics ===\n    # Pre-compute X^T*X and its inverse\n    XTX = X.T @ X\n    XTX_inv = np.linalg.inv(XTX)\n    \n    # OLS fit\n    beta_hat = XTX_inv @ X.T @ y\n    \n    # Hat matrix diagonal (leverages)\n    H = X @ XTX_inv @ X.T\n    h = np.diag(H)\n    \n    # Residuals and MSE\n    y_hat = X @ beta_hat\n    residuals = y - y_hat\n    rss = residuals.T @ residuals\n    df_resid = n - p_eff\n    mse = rss / df_resid\n    \n    # Observed Cook's Distances\n    # D_i = (e_i^2 / (p_eff * mse)) * (h_ii / (1 - h_ii)^2)\n    D_obs = (residuals**2 / (p_eff * mse)) * (h / (1 - h)**2)\n\n    # === 3. Permutation Test to Generate Null Distribution ===\n    rng_perm = np.random.default_rng(perm_seed)\n    # The number of null statistics will be n * B\n    null_cooks_distances = np.zeros(n * B)\n    y_perm = y.copy()\n\n    for b in range(B):\n        rng_perm.shuffle(y_perm)\n        \n        # OLS on permuted data\n        beta_hat_perm = XTX_inv @ X.T @ y_perm\n        \n        # Residuals and MSE for permuted data\n        y_hat_perm = X @ beta_hat_perm\n        residuals_perm = y_perm - y_hat_perm\n        rss_perm = residuals_perm.T @ residuals_perm\n        mse_perm = rss_perm / df_resid\n        \n        # Cook's distances for permuted data\n        # Note: h and p_eff are constant\n        D_perm = (residuals_perm**2 / (p_eff * mse_perm)) * (h / (1 - h)**2)\n        \n        null_cooks_distances[b*n:(b+1)*n] = D_perm\n\n    # === 4. Calculate P-values ===\n    p_values = np.zeros(n)\n    num_null_samples = len(null_cooks_distances)\n    \n    for i in range(n):\n        D_i_obs = D_obs[i]\n        num_exceeding = np.sum(null_cooks_distances >= D_i_obs)\n        p_values[i] = (1 + num_exceeding) / (1 + num_null_samples)\n\n    # === 5. Apply Benjamini-Hochberg (BH) Procedure ===\n    sorted_indices = np.argsort(p_values)\n    sorted_p_values = p_values[sorted_indices]\n    \n    bh_thresholds = (np.arange(1, n + 1) / n) * alpha\n    \n    is_significant = sorted_p_values = bh_thresholds\n    \n    if np.any(is_significant):\n        # Find the largest k such that p_(k) = (k/n) * alpha\n        k = np.where(is_significant)[0].max() + 1\n        significant_indices = sorted_indices[:k]\n    else:\n        significant_indices = np.array([], dtype=int)\n        \n    return sorted(significant_indices.tolist())\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        {\n            \"n\": 30, \"p\": 8,\n            \"predictor_seed\": 11, \"response_seed\": 17,\n            \"beta\": [0.5, 0.9, -0.5, 0.0, 0.7, 0.0, -0.4, 0.2, 0.0],\n            \"sigma\": 0.4,\n            \"leverage_mods\": [(5, 4.0)],\n            \"residual_shocks\": [(5, 3.0)],\n            \"perm_seed\": 1017\n        },\n        {\n            \"n\": 25, \"p\": 5,\n            \"predictor_seed\": 19, \"response_seed\": 23,\n            \"beta\": [0.5, 0.6, 0.0, -0.5, 0.3, 0.0],\n            \"sigma\": 0.5,\n            \"leverage_mods\": None,\n            \"residual_shocks\": None,\n            \"perm_seed\": 2023\n        },\n        {\n            \"n\": 18, \"p\": 10,\n            \"predictor_seed\": 29, \"response_seed\": 31,\n            \"beta\": [0.5, 0.8, -0.3, 0.0, 0.7, 0.0, -0.2, 0.4, 0.0, 0.2, -0.1],\n            \"sigma\": 0.6,\n            \"leverage_mods\": [(2, 3.0), (10, 2.5)],\n            \"residual_shocks\": [(2, 2.4), (15, -2.4)],\n            \"perm_seed\": 3031\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        flagged_indices = analyze_case(\n            case[\"n\"], case[\"p\"],\n            case[\"predictor_seed\"], case[\"response_seed\"],\n            case[\"beta\"], case[\"sigma\"],\n            case[\"leverage_mods\"], case[\"residual_shocks\"],\n            case[\"perm_seed\"]\n        )\n        results.append(flagged_indices)\n    \n    # Format the final output string exactly as requested\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n\n```", "id": "4916287"}]}