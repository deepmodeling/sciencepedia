{"hands_on_practices": [{"introduction": "在深入研究正则化方法之前，我们首先需要理解它们旨在解决的问题。这项练习 [@problem_id:4947371] 模拟了一个生物统计学中的常见情景：当预测变量高度相关时，标准的普通最小二乘（OLS）回归会变得不稳定。通过亲手计算系数估计的方差，你将直观地感受到多重共线性是如何“放大”不确定性的，从而为学习正则化技术奠定坚实的基础。", "problem": "一个生物统计学团队正在使用两种已知其测定结果高度相关的实验室生物标志物来为一个连续的临床结局 $y$ 建模。$n=5$ 名患者和 $p=2$ 种生物标志物的数据被整合到设计矩阵 $X$ 中（其列已被缩放到可比较的单位）：\n$$\nX=\\begin{pmatrix}\n1  1.002 \\\\\n2  2.001 \\\\\n3  3.000 \\\\\n4  4.001 \\\\\n5  5.002\n\\end{pmatrix}.\n$$\n假设标准线性模型为 $y = X\\beta + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$，$\\mathcal{N}$ 表示正态分布，$I$ 是单位矩阵。假设先前的测量研究提供了残差方差的可靠估计值 $\\sigma^2 = 0.25$。仅使用这些模型定义和普通最小二乘法 (OLS) 的性质，推导第一个系数 $\\beta_1$ 的 OLS 估计量的抽样方差，然后计算其在给定 $X$ 和 $\\sigma^2$ 下的数值。将最终数值答案四舍五入到四位有效数字。将最终答案表示为一个不带单位的实数。此外，在共线性的背景下解释此方差的大小，并解释为什么岭回归和最小绝对收缩和选择算子 (LASSO) 等正则化方法在此处是相关的。", "solution": "该问题要求推导和计算普通最小二乘法 (OLS) 系数估计的抽样方差，然后结合多重共线性和正则化技术的相关性对结果进行解释。\n\n**第 1 部分：OLS 估计量抽样方差的推导**\n\n标准线性模型由 $y = X\\beta + \\varepsilon$ 给出，其中 $y$ 是 $n \\times 1$ 的结局向量，$X$ 是 $n \\times p$ 的设计矩阵，$\\beta$ 是 $p \\times 1$ 的系数向量，$\\varepsilon$ 是 $n \\times 1$ 的误差向量。OLS 估计量 $\\hat{\\beta}$ 是通过最小化残差平方和 $SSR = (y - X\\beta)^T(y - X\\beta)$ 推导出来的。这得到了正规方程 $X^T X \\hat{\\beta} = X^T y$。假设 $X^T X$ 是可逆的，则 OLS 估计量为：\n$$\n\\hat{\\beta} = (X^T X)^{-1} X^T y\n$$\n为了找到 $\\hat{\\beta}$ 的抽样分布，我们将真实模型 $y = X\\beta + \\varepsilon$ 代入 $\\hat{\\beta}$ 的表达式中：\n$$\n\\hat{\\beta} = (X^T X)^{-1} X^T (X\\beta + \\varepsilon) = (X^T X)^{-1} (X^T X) \\beta + (X^T X)^{-1} X^T \\varepsilon = \\beta + (X^T X)^{-1} X^T \\varepsilon\n$$\n问题指出误差服从正态分布 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$。这意味着误差向量的期望值为 $E[\\varepsilon] = 0$，误差的协方差矩阵为 $E[\\varepsilon \\varepsilon^T] = \\sigma^2 I$。\n估计量的期望值为：\n$$\nE[\\hat{\\beta}] = E[\\beta + (X^T X)^{-1} X^T \\varepsilon] = \\beta + (X^T X)^{-1} X^T E[\\varepsilon] = \\beta + 0 = \\beta\n$$\n这表明 OLS 估计量是无偏的。$\\hat{\\beta}$ 的协方差矩阵定义为 $\\text{Cov}(\\hat{\\beta}) = E[(\\hat{\\beta} - E[\\hat{\\beta}])(\\hat{\\beta} - E[\\hat{\\beta}])^T]$。由于 $E[\\hat{\\beta}] = \\beta$，我们有 $\\hat{\\beta} - \\beta = (X^T X)^{-1} X^T \\varepsilon$。因此：\n$$\n\\text{Cov}(\\hat{\\beta}) = E[((X^T X)^{-1} X^T \\varepsilon)((X^T X)^{-1} X^T \\varepsilon)^T]\n$$\n使用性质 $(AB)^T = B^T A^T$，上式变为：\n$$\n\\text{Cov}(\\hat{\\beta}) = E[(X^T X)^{-1} X^T \\varepsilon \\varepsilon^T X ((X^T X)^{-1})^T]\n$$\n由于 $X$ 被视为固定的（非随机的），我们可以将其移到期望的外面：\n$$\n\\text{Cov}(\\hat{\\beta}) = (X^T X)^{-1} X^T E[\\varepsilon \\varepsilon^T] X ((X^T X)^{-1})^T\n$$\n代入 $E[\\varepsilon \\varepsilon^T] = \\sigma^2 I$：\n$$\n\\text{Cov}(\\hat{\\beta}) = (X^T X)^{-1} X^T (\\sigma^2 I) X ((X^T X)^{-1})^T = \\sigma^2 (X^T X)^{-1} X^T X ((X^T X)^{-1})^T\n$$\n这可以简化为 $\\sigma^2 (X^T X)^{-1} I ((X^T X)^{-1})^T$。由于 $X^T X$ 是对称的，其逆矩阵 $(X^T X)^{-1}$ 也是对称的，所以 $((X^T X)^{-1})^T = (X^T X)^{-1}$。OLS 估计量协方差矩阵的最终表达式是：\n$$\n\\text{Cov}(\\hat{\\beta}) = \\sigma^2 (X^T X)^{-1}\n$$\n第一个系数 $\\hat{\\beta}_1$ 的估计量的抽样方差是该协方差矩阵的第一个对角线元素。如果我们记 $C = (X^T X)^{-1}$，那么：\n$$\n\\text{Var}(\\hat{\\beta}_1) = \\sigma^2 C_{11} = \\sigma^2 [(X^T X)^{-1}]_{11}\n$$\n这是我们将用于计算的通用公式。\n\n**第 2 部分：数值计算**\n\n给定的数据是 $n=5$, $p=2$, $\\sigma^2 = 0.25$，以及设计矩阵：\n$$\nX = \\begin{pmatrix}\n1  1.002 \\\\\n2  2.001 \\\\\n3  3.000 \\\\\n4  4.001 \\\\\n5  5.002\n\\end{pmatrix}\n$$\n首先，我们计算矩阵 $X^T X$：\n$$\nX^T X = \\begin{pmatrix}\n1  2  3  4  5 \\\\\n1.002  2.001  3.000  4.001  5.002\n\\end{pmatrix}\n\\begin{pmatrix}\n1  1.002 \\\\\n2  2.001 \\\\\n3  3.000 \\\\\n4  4.001 \\\\\n5  5.002\n\\end{pmatrix}\n$$\n其元素为：\n$(X^T X)_{11} = 1^2 + 2^2 + 3^2 + 4^2 + 5^2 = 1+4+9+16+25 = 55$\n$(X^T X)_{12} = (X^T X)_{21} = 1(1.002) + 2(2.001) + 3(3.000) + 4(4.001) + 5(5.002) = 1.002 + 4.002 + 9 + 16.004 + 25.010 = 55.018$\n$(X^T X)_{22} = 1.002^2 + 2.001^2 + 3.000^2 + 4.001^2 + 5.002^2 = 1.004004 + 4.004001 + 9 + 16.008001 + 25.020004 = 55.03601$\n所以，该矩阵为：\n$$\nX^T X = \\begin{pmatrix} 55  55.018 \\\\ 55.018  55.03601 \\end{pmatrix}\n$$\n接下来，我们计算这个 $2 \\times 2$ 矩阵的逆。矩阵 $\\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$ 的逆是 $\\frac{1}{ad-bc} \\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}$。\n行列式为：\n$$\n\\det(X^T X) = (55)(55.03601) - (55.018)^2 = 3026.98055 - 3026.980324 = 0.000226\n$$\n逆矩阵为：\n$$\n(X^T X)^{-1} = \\frac{1}{0.000226} \\begin{pmatrix} 55.03601  -55.018 \\\\ -55.018  55 \\end{pmatrix}\n$$\n我们需要这个矩阵的 $(1,1)$ 元素，即：\n$$\n[(X^T X)^{-1}]_{11} = \\frac{55.03601}{0.000226}\n$$\n现在我们可以使用 $\\sigma^2 = 0.25$ 来计算 $\\hat{\\beta}_1$ 的方差：\n$$\n\\text{Var}(\\hat{\\beta}_1) = \\sigma^2 [(X^T X)^{-1}]_{11} = 0.25 \\times \\frac{55.03601}{0.000226} \\approx 60880.542\n$$\n四舍五入到四位有效数字，我们得到 $60880$。用科学记数法表示，这是 $6.088 \\times 10^4$。\n\n**第 3 部分：解释和正则化的相关性**\n\n计算出的方差 $\\text{Var}(\\hat{\\beta}_1) \\approx 6.088 \\times 10^4$ 是一个异常大的值。系数估计量的高方差意味着该估计非常不精确和不稳定。如果我们从同一总体中收集另一组不同的样本数据，$\\hat{\\beta}_1$ 的估计值可能会发生巨大变化。这使得该系数估计对于推断或预测是不可靠的。\n\n这种高方差的根本原因是严重的多重共线性。当回归模型中的预测变量高度相关时，就会发生多重共线性。在这个问题中，设计矩阵 $X$ 的两列几乎完全相同，表明存在极高的相关性。两种生物标志物测量值之间的相关系数几乎为 $1$。当预测变量高度相关时，矩阵 $X^T X$ 会变得近奇异（即其行列式接近于零）。在我们的案例中，$\\det(X^T X) = 0.000226$，这是一个非常小的数。对一个近奇异矩阵求逆的过程在数值上是不稳定的，并会导致逆矩阵中出现非常大的元素。由于 OLS 估计量的方差与该逆矩阵的对角线元素成正比（如公式 $\\text{Cov}(\\hat{\\beta}) = \\sigma^2(X^T X)^{-1}$ 所示），方差被“膨胀”到非常大的值。这种现象通常用方差膨胀因子 (VIF) 来量化。\n\n正则化方法是专门为解决多重共线性问题而设计的。OLS 在这种情况下会失效，因为它寻求无偏估计量，而这是以巨大的方差为代价的。正则化方法在估计中引入少量偏差，以实现方差的大幅减少，从而导致总体均方误差 ($MSE = \\text{方差} + \\text{偏差}^2$) 大大降低。\n\n- **岭回归**：岭回归在 OLS 目标函数中增加了一个惩罚项，最小化 $\\sum (y_i - X_i\\beta)^2 + \\lambda \\sum \\beta_j^2$。得到的估计量是 $\\hat{\\beta}_{\\text{ridge}} = (X^T X + \\lambda I)^{-1} X^T y$，其中 $\\lambda  0$ 是一个调整参数。在 $X^T X$ 中加入 $\\lambda I$ 项，使得待求逆的矩阵变为良态且非奇异的，即使当 $X^T X$ 不是良态时也是如此。这稳定了逆矩阵的计算，并产生方差小得多的系数估计。\n- **LASSO (最小绝对收缩和选择算子)**：Lasso 最小化 $\\sum (y_i - X_i\\beta)^2 + \\lambda \\sum |\\beta_j|$。与岭回归一样，它将系数向零收缩以减少方差。然而，$L_1$ 惩罚项具有将某些系数收缩到恰好为零的独特属性。在这种两个高度相关的预测变量的情况下，Lasso 很可能会选择其中一个生物标志物并为其分配一个非零系数，同时通过将其系数设置为零来消除另一个。这通过执行自动变量选择，提供了一个更简单、更易于解释的模型。\n\n总之，OLS 估计量的极端方差凸显了其在存在严重多重共线性时的失效。像岭回归和 LASSO 这样的正则化技术是至关重要的统计工具，可以在此种常见的生物统计学场景中提供稳定、可靠且更易于解释的模型。", "answer": "$$\\boxed{6.088 \\times 10^4}$$", "id": "4947371"}, {"introduction": "了解了问题的根源后，我们来剖析一种有效的解决方案：岭回归。本练习 [@problem_id:4947422] 将引导你推导岭回归估计器及其对应的“帽子矩阵”$H_{\\lambda}$。通过这个过程，你不仅能理解岭回归如何通过对奇异值进行缩放来稳定系数，还将推导出模型的“有效自由度”，这是衡量模型复杂度和进行模型比较的关键概念。", "problem": "考虑一个生物统计线性回归情景，其中一个中心化的响应向量 $y \\in \\mathbb{R}^{n}$ 使用一个中心化的生物预测变量设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$（例如，标准化的基因表达或临床协变量）进行建模，因此不需要截距项。为减轻多重共线性和过拟合，岭回归估计量 $\\hat{\\beta}^{\\text{ridge}} \\in \\mathbb{R}^{p}$ 被定义为惩罚最小二乘目标的唯一最小化子\n$$\n\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{2}^{2},\n$$\n其中惩罚参数 $\\lambda  0$。从这个定义出发，并仅使用线性代数的基本性质，如导数性质、对称性、正交对角化和奇异值分解（SVD）的存在性，按以下步骤进行：\n\n1. 推导刻画 $\\hat{\\beta}^{\\text{ridge}}$ 的正规方程，并获得用 $X$、$y$ 和 $\\lambda$ 表示的 $\\hat{\\beta}^{\\text{ridge}}$ 的显式表达式。\n2. 推导拟合值 $\\hat{y} = X \\hat{\\beta}^{\\text{ridge}}$，并证明存在一个线性映射 $H_{\\lambda} \\in \\mathbb{R}^{n \\times n}$ 使得 $\\hat{y} = H_{\\lambda} y$。明确地用 $X$ 和 $\\lambda$ 表示 $H_{\\lambda}$。\n3. 令 $r = \\operatorname{rank}(X)$，并设 $X$ 的奇异值分解（SVD）为 $X = U \\Sigma V^{\\top}$，其中 $U \\in \\mathbb{R}^{n \\times n}$ 和 $V \\in \\mathbb{R}^{p \\times p}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{n \\times p}$ 的主对角线上有非零奇异值 $\\sigma_{1}, \\dots, \\sigma_{r}  0$。仅使用这些定义和迹的基本性质，推导 $\\operatorname{tr}(H_{\\lambda})$ 作为 $\\lambda$ 和非零奇异值 $\\{\\sigma_{j}\\}_{j=1}^{r}$ 的函数的闭式解析表达式。\n\n以 $\\lambda$ 和 $\\{\\sigma_{j}\\}_{j=1}^{r}$ 表示的 $\\operatorname{tr}(H_{\\lambda})$ 的单一闭式解析表达式的形式提供最终答案。不需要数值近似或四舍五入。", "solution": "所述问题是有效的。它在科学上基于正则化线性模型的既定理论，特别是岭回归，这是现代生物统计学和机器学习的基石。该问题是适定的，提供了所有必要的条件和定义以推导出唯一且有意义的解。它是客观的，没有任何模糊不清或事实不准确之处。我们可以开始求解。\n\n该问题分为三部分。我们将按顺序进行处理。\n\n第1部分：推导正规方程和 $\\hat{\\beta}^{\\text{ridge}}$ 的表达式。\n\n岭回归的目标函数由下式给出\n$$L(\\beta) = \\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{2}^{2}$$\n其中 $y \\in \\mathbb{R}^{n}$ 是中心化的响应向量，$X \\in \\mathbb{R}^{n \\times p}$ 是中心化的设计矩阵，$\\beta \\in \\mathbb{R}^{p}$ 是系数向量，$\\lambda  0$ 是正则化参数。我们可以使用向量转置来表示欧几里得范数的平方：\n$$L(\\beta) = (y - X\\beta)^{\\top}(y - X\\beta) + \\lambda \\beta^{\\top}\\beta$$\n展开第一项，我们得到：\n$$L(\\beta) = y^{\\top}y - y^{\\top}X\\beta - \\beta^{\\top}X^{\\top}y + \\beta^{\\top}X^{\\top}X\\beta + \\lambda \\beta^{\\top}\\beta$$\n由于 $y^{\\top}X\\beta$ 是一个标量（$1 \\times 1$ 矩阵），它等于其转置 $(\\beta^{\\top}X^{\\top}y)$。这使我们可以合并两个交叉乘积项：\n$$L(\\beta) = y^{\\top}y - 2\\beta^{\\top}X^{\\top}y + \\beta^{\\top}X^{\\top}X\\beta + \\lambda \\beta^{\\top}I\\beta$$\n我们可以提出因子 $\\beta^{\\top}$ 和 $\\beta$，将函数写成 $\\beta$ 的二次型：\n$$L(\\beta) = y^{\\top}y - 2\\beta^{\\top}X^{\\top}y + \\beta^{\\top}(X^{\\top}X + \\lambda I)\\beta$$\n为了找到最小化 $L(\\beta)$ 的系数向量 $\\hat{\\beta}^{\\text{ridge}}$，我们计算 $L(\\beta)$ 关于 $\\beta$ 的梯度，并将其设为零向量。对于 $\\lambda  0$，目标函数是严格凸的，这保证了唯一的全局最小值。使用向量微积分的法则（对于对称矩阵 $A$，有 $\\frac{\\partial}{\\partial x} c^{\\top}x = c$ 和 $\\frac{\\partial}{\\partial x} x^{\\top}Ax = 2Ax$），梯度为：\n$$\\frac{\\partial L(\\beta)}{\\partial \\beta} = -2X^{\\top}y + 2(X^{\\top}X + \\lambda I)\\beta$$\n在 $\\beta = \\hat{\\beta}^{\\text{ridge}}$ 处将梯度设为零，得到条件：\n$$-2X^{\\top}y + 2(X^{\\top}X + \\lambda I)\\hat{\\beta}^{\\text{ridge}} = 0$$\n除以2并重新整理，得到岭回归的正规方程：\n$$(X^{\\top}X + \\lambda I)\\hat{\\beta}^{\\text{ridge}} = X^{\\top}y$$\n为了获得 $\\hat{\\beta}^{\\text{ridge}}$ 的显式表达式，我们必须对矩阵 $(X^{\\top}X + \\lambda I)$ 求逆。矩阵 $X^{\\top}X$ 是半正定的。由于 $\\lambda  0$，矩阵 $\\lambda I$ 是正定的。一个半正定矩阵和一个正定矩阵的和是正定的，因此总是可逆的。左乘 $(X^{\\top}X + \\lambda I)$ 的逆矩阵，得到显式解：\n$$\\hat{\\beta}^{\\text{ridge}} = (X^{\\top}X + \\lambda I)^{-1}X^{\\top}y$$\n\n第2部分：推导拟合值 $\\hat{y}$ 和帽子矩阵 $H_{\\lambda}$。\n\n拟合值，记作 $\\hat{y}$，是由模型使用估计的系数预测的：\n$$\\hat{y} = X\\hat{\\beta}^{\\text{ridge}}$$\n代入第1部分中 $\\hat{\\beta}^{\\text{ridge}}$ 的表达式：\n$$\\hat{y} = X \\left( (X^{\\top}X + \\lambda I)^{-1}X^{\\top}y \\right)$$\n利用矩阵乘法的结合律，我们可以重新组合各项，将 $\\hat{y}$ 表示为 $y$ 的线性变换：\n$$\\hat{y} = \\left( X(X^{\\top}X + \\lambda I)^{-1}X^{\\top} \\right) y$$\n这表明存在一个线性映射 $H_{\\lambda} \\in \\mathbb{R}^{n \\times n}$ 使得 $\\hat{y} = H_{\\lambda} y$。通过观察，这个矩阵，被称为岭回归的帽子矩阵或平滑矩阵，是：\n$$H_{\\lambda} = X(X^{\\top}X + \\lambda I)^{-1}X^{\\top}$$\n\n第3部分：推导 $\\operatorname{tr}(H_{\\lambda})$ 的闭式表达式。\n\n我们的目标是计算 $\\operatorname{tr}(H_{\\lambda}) = \\operatorname{tr}\\left(X(X^{\\top}X + \\lambda I)^{-1}X^{\\top}\\right)$。我们使用迹的循环性质，即对于矩阵 $A \\in \\mathbb{R}^{n \\times p}$ 和 $B \\in \\mathbb{R}^{p \\times n}$，有 $\\operatorname{tr}(AB) = \\operatorname{tr}(BA)$。令 $A = X$ 且 $B = (X^{\\top}X + \\lambda I)^{-1}X^{\\top}$。那么，\n$$\\operatorname{tr}(H_{\\lambda}) = \\operatorname{tr}\\left((X^{\\top}X + \\lambda I)^{-1}X^{\\top}X\\right)$$\n我们现在使用 $X$ 的奇异值分解（SVD），给定为 $X = U \\Sigma V^{\\top}$。这里，$U \\in \\mathbb{R}^{n \\times n}$ 和 $V \\in \\mathbb{R}^{p \\times p}$ 是正交矩阵，而 $\\Sigma \\in \\mathbb{R}^{n \\times p}$ 是一个矩形对角矩阵，其上有 $r = \\operatorname{rank}(X)$ 个非零奇异值 $\\sigma_1, \\dots, \\sigma_r  0$。首先，我们用SVD表示 $X^{\\top}X$：\n$$X^{\\top}X = (U\\Sigma V^{\\top})^{\\top}(U\\Sigma V^{\\top}) = V\\Sigma^{\\top}U^{\\top}U\\Sigma V^{\\top} = V\\Sigma^{\\top}I_n\\Sigma V^{\\top} = V(\\Sigma^{\\top}\\Sigma) V^{\\top}$$\n矩阵 $\\Sigma^{\\top}\\Sigma \\in \\mathbb{R}^{p \\times p}$ 是一个对角矩阵，其对角线元素为 $(\\sigma_1^2, \\dots, \\sigma_r^2, 0, \\dots, 0)$。现在将此代入迹的表达式中。首先，考虑项 $(X^{\\top}X + \\lambda I)$：\n$$X^{\\top}X + \\lambda I = V(\\Sigma^{\\top}\\Sigma) V^{\\top} + \\lambda I = V(\\Sigma^{\\top}\\Sigma) V^{\\top} + \\lambda VV^{\\top} = V(\\Sigma^{\\top}\\Sigma + \\lambda I)V^{\\top}$$\n那么其逆矩阵是：\n$$(X^{\\top}X + \\lambda I)^{-1} = (V(\\Sigma^{\\top}\\Sigma + \\lambda I)V^{\\top})^{-1} = V(\\Sigma^{\\top}\\Sigma + \\lambda I)^{-1}V^{\\top}$$\n现在我们组合迹内部的项：\n$$(X^{\\top}X + \\lambda I)^{-1}X^{\\top}X = \\left(V(\\Sigma^{\\top}\\Sigma + \\lambda I)^{-1}V^{\\top}\\right) \\left(V(\\Sigma^{\\top}\\Sigma) V^{\\top}\\right)$$\n$$= V(\\Sigma^{\\top}\\Sigma + \\lambda I)^{-1}(V^{\\top}V)(\\Sigma^{\\top}\\Sigma) V^{\\top} = V\\left((\\Sigma^{\\top}\\Sigma + \\lambda I)^{-1}(\\Sigma^{\\top}\\Sigma)\\right)V^{\\top}$$\n令 $M = (\\Sigma^{\\top}\\Sigma + \\lambda I)^{-1}(\\Sigma^{\\top}\\Sigma)$。对 $\\operatorname{tr}(H_{\\lambda}) = \\operatorname{tr}(VMV^{\\top})$ 使用迹的循环性质：\n$$\\operatorname{tr}(H_{\\lambda}) = \\operatorname{tr}(V^{\\top}VM) = \\operatorname{tr}(I_pM) = \\operatorname{tr}(M)$$\n矩阵 $M$ 是两个可交换的对角矩阵的乘积，因此它本身也是一个对角矩阵。其第 $j$ 个对角元素 $M_{jj}$ 是 $(\\Sigma^{\\top}\\Sigma + \\lambda I)^{-1}$ 和 $\\Sigma^{\\top}\\Sigma$ 相应对角元素的乘积：\n$$M_{jj} = \\frac{1}{(\\Sigma^{\\top}\\Sigma)_{jj} + \\lambda} \\times (\\Sigma^{\\top}\\Sigma)_{jj}$$\n对于 $j = 1, \\dots, r$，$\\Sigma^{\\top}\\Sigma$ 的第 $j$ 个对角元素是 $\\sigma_j^2$。因此，\n$$M_{jj} = \\frac{\\sigma_j^2}{\\sigma_j^2 + \\lambda} \\quad \\text{for } j = 1, \\dots, r$$\n对于 $j = r+1, \\dots, p$，第 $j$ 个对角元素是 $0$。因此，\n$$M_{jj} = \\frac{0}{0 + \\lambda} = 0 \\quad \\text{for } j = r+1, \\dots, p$$\n$M$ 的迹是其对角元素的和：\n$$\\operatorname{tr}(M) = \\sum_{j=1}^{p} M_{jj} = \\sum_{j=1}^{r} \\frac{\\sigma_j^2}{\\sigma_j^2 + \\lambda} + \\sum_{j=r+1}^{p} 0$$\n因此，$H_{\\lambda}$ 的迹的最终闭式解析表达式为：\n$$\\operatorname{tr}(H_{\\lambda}) = \\sum_{j=1}^{r} \\frac{\\sigma_{j}^{2}}{\\sigma_{j}^{2} + \\lambda}$$\n这个量也被称为岭回归拟合的有效自由度。", "answer": "$$\\boxed{\\sum_{j=1}^{r} \\frac{\\sigma_{j}^{2}}{\\sigma_{j}^{2} + \\lambda}}$$", "id": "4947422"}, {"introduction": "理论的最后一步是实践。在现实世界中，正则化模型是如何被计算出来的？这项练习 [@problem_id:4947418] 将带你从数学公式走向算法实现。你将亲手编写坐标下降算法——这是许多现代统计软件包背后的核心引擎——并将其应用于岭回归和Lasso回归，从而在最根本的层面上巩固你对这两种方法工作原理的理解。", "problem": "考虑惩罚线性回归中的单坐标更新，这是一种在生物统计学中广泛使用的方法，用于在高维生物数据中控制方差和提高预测精度。设有 $n$ 个观测值。令响应向量为 $y \\in \\mathbb{R}^n$，并假设当前拟合模型的残差向量为 $r \\in \\mathbb{R}^n$，定义为 $r = y - X \\beta$，其中 $X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$\\beta \\in \\mathbb{R}^p$ 是当前系数向量。关注对应于一个坐标的单个预测变量列 $x \\in \\mathbb{R}^n$，其当前系数为 $b \\in \\mathbb{R}$。一次单步坐标更新将 $b$ 改变一个量 $d \\in \\mathbb{R}$，得到新系数 $b_{\\text{new}} = b + d$，从而产生新的残差向量 $r' = r - x d$ 和新的残差平方和（RSS），由 $RSS' = \\lVert r' \\rVert_2^2$ 给出。\n\n仅从最小二乘法和凸惩罚的定义出发：\n- 残差平方和为 $RSS(\\beta) = \\lVert y - X \\beta \\rVert_2^2$。\n- 岭回归目标函数为 $\\tfrac{1}{2} \\lVert y - X \\beta \\rVert_2^2 + \\tfrac{\\alpha}{2} \\lVert \\beta \\rVert_2^2$，其中惩罚参数 $\\alpha \\ge 0$。\n- Lasso 目标函数为 $\\tfrac{1}{2} \\lVert y - X \\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1$，其中惩罚参数 $\\lambda \\ge 0$。\n\n你的任务是实现对 $b$ 的单坐标更新，并计算残差平方和随该更新的精确变化量。该更新必须通过最小化相应惩罚目标函数相对于单个坐标来确定，同时保持所有其他坐标固定。在概念上使用偏残差 $r_j = y - X_{-j} \\beta_{-j}$，但在计算中，你被给予 $r$ 和 $b$，因此你应该使用 $r_j = r + x b$ 来构造 $r_j$，以消除对未知子模型的依赖。然后对以下惩罚项之一执行更新：\n- 岭回归：使用惩罚参数 $\\alpha$。\n- Lasso：使用惩罚参数 $\\lambda$。\n\n在计算出 $b_{\\text{new}}$ 后，定义 $d = b_{\\text{new}} - b$，并计算残差平方和的精确变化量 $\\Delta RSS = RSS' - RSS$，作为 $d$、$x$ 和 $r$ 的函数。\n\n实现一个程序，对下面的每个测试用例，执行：\n- 构造 $r_j = r + x b$。\n- 计算单步坐标更新 $b_{\\text{new}}$，以最小化所选惩罚目标函数在坐标 $b$ 上的值。\n- 计算 $d = b_{\\text{new}} - b$。\n- 计算将 $d$ 应用于 $b$ 后（不改变任何其他坐标）残差平方和的精确变化量 $\\Delta RSS$。\n\n对每个测试用例，将 $\\Delta RSS$ 报告为浮点数。\n\n测试套件（每个案例给出 $n$、$x$、$r$、$b$ 和惩罚规范）：\n1. 岭回归正常路径：\n   - $n = 5$\n   - $x = [1.2, -0.3, 0.5, 0.0, 2.1]$\n   - $r = [0.8, -1.1, 0.3, 0.0, 2.0]$\n   - $b = 0.4$\n   - 岭回归，$\\alpha = 0.7$。\n2. Lasso 正常路径（预期非零更新）：\n   - $n = 6$\n   - $x = [0.5, -1.0, 0.3, 1.2, -0.7, 0.4]$\n   - $r = [1.0, 0.5, -0.2, 0.3, -1.5, 0.1]$\n   - $b = -0.1$\n   - Lasso，$\\lambda = 0.2$。\n3. Lasso 阈值化为零（边缘情况）：\n   - $n = 4$\n   - $x = [0.2, -0.1, 0.05, 0.0]$\n   - $r = [0.01, -0.02, 0.03, -0.04]$\n   - $b = 0.5$\n   - Lasso，$\\lambda = 0.2$。\n4. 岭回归预测变量列为零（边界情况）：\n   - $n = 5$\n   - $x = [0.0, 0.0, 0.0, 0.0, 0.0]$\n   - $r = [1.0, -1.0, 0.5, -0.5, 2.0]$\n   - $b = 3.0$\n   - 岭回归，$\\alpha = 1.0$。\n5. Lasso 预测变量数值极小（数值稳定性情况）：\n   - $n = 3$\n   - $x = [10^{-8}, -2 \\cdot 10^{-8}, 3 \\cdot 10^{-8}]$\n   - $r = [0.0, 0.0, 0.0]$\n   - $b = 1.0$\n   - Lasso，$\\lambda = 0.1$。\n\n最终输出格式：\n- 你的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表，例如 $[result_1,result_2,\\dots]$，其顺序与上面列出的测试用例完全一致。", "solution": "该问题要求推导并实现岭回归和 Lasso 正则化线性回归的单坐标更新，并计算由此产生的残差平方和 ($RSS$) 的变化。该问题具有科学依据，是良定的并且是自洽的。所提供的定义和测试用例是一致且有效的。因此，我们可以着手解决。\n\n惩罚线性回归的一般目标函数是：\n$$L(\\beta) = \\frac{1}{2}\\lVert y - X \\beta \\rVert_2^2 + P(\\beta)$$\n其中 $P(\\beta)$ 是一个惩罚函数。我们的目标是更新单个系数，记为 $b$，对应于预测变量列 $x$。所有其他系数都保持固定。新系数将是 $b_{\\text{new}}$。目标函数，仅看作是 $b_{\\text{new}}$ 的函数，可以用偏残差 $r_j = y - \\sum_{k \\neq j} x_k \\beta_k$ 来表示。按照规定，这可以从当前的总残差 $r = y - X\\beta$ 计算得出，即 $r_j = r + xb$。\n\n目标函数的 $RSS$ 部分可以写成：\n$$\\frac{1}{2}\\lVert r_j - x b_{\\text{new}} \\rVert_2^2 = \\frac{1}{2}(r_j - x b_{\\text{new}})^T(r_j - x b_{\\text{new}}) = \\frac{1}{2} (\\lVert r_j \\rVert_2^2 - 2 b_{\\text{new}} (r_j^T x) + b_{\\text{new}}^2 \\lVert x \\rVert_2^2)$$\n这是 $b_{\\text{new}}$ 的一个二次函数。项 $\\frac{1}{2}\\lVert r_j \\rVert_2^2$ 相对于 $b_{\\text{new}}$ 是一个常数，在最小化过程中可以忽略。\n\n**1. 岭回归更新**\n\n对于岭回归，惩罚项是 $P(\\beta) = \\frac{\\alpha}{2} \\lVert \\beta \\rVert_2^2 = \\frac{\\alpha}{2} \\sum_k \\beta_k^2$。单个坐标 $b_{\\text{new}}$ 的目标函数是：\n$$L_{\\text{ridge}}(b_{\\text{new}}) = \\frac{1}{2} \\lVert r_j - x b_{\\text{new}} \\rVert_2^2 + \\frac{\\alpha}{2} b_{\\text{new}}^2 + \\text{const}$$\n该函数是可微且凸的。为了找到最小值，我们将其关于 $b_{\\text{new}}$ 的导数设为零：\n$$\\frac{\\partial L_{\\text{ridge}}}{\\partial b_{\\text{new}}} = -r_j^T x + b_{\\text{new}} \\lVert x \\rVert_2^2 + \\alpha b_{\\text{new}} = 0$$\n求解 $b_{\\text{new}}$：\n$$b_{\\text{new}} (\\lVert x \\rVert_2^2 + \\alpha) = r_j^T x$$\n$$b_{\\text{new}} = \\frac{r_j^T x}{\\lVert x \\rVert_2^2 + \\alpha}$$\n代入 $r_j = r + xb$：\n$$r_j^T x = (r + xb)^T x = r^T x + b(x^T x) = r^T x + b \\lVert x \\rVert_2^2$$\n因此，新系数的更新规则是：\n$$b_{\\text{new}} = \\frac{r^T x + b \\lVert x \\rVert_2^2}{\\lVert x \\rVert_2^2 + \\alpha}$$\n如果预测变量列 $x$ 是零向量，则 $\\lVert x \\rVert_2^2 = 0$ 且 $r^T x = 0$，导致 $b_{\\text{new}} = 0$。由于 $\\alpha \\ge 0$，如果 $\\alpha  0$ 或 $\\lVert x \\rVert_2^2  0$，分母总是正的。\n\n**2. Lasso 回归更新**\n\n对于 Lasso 回归，惩罚项是 $P(\\beta) = \\lambda \\lVert \\beta \\rVert_1 = \\lambda \\sum_k |\\beta_k|$。单个坐标 $b_{\\text{new}}$ 的目标函数是：\n$$L_{\\text{lasso}}(b_{\\text{new}}) = \\frac{1}{2} \\lVert r_j - x b_{\\text{new}} \\rVert_2^2 + \\lambda |b_{\\text{new}}| + \\text{const}$$\n这个目标函数是凸的，但在 $b_{\\text{new}} = 0$ 处不可微。我们使用次梯度优化。该目标函数的次梯度是：\n$$\\partial L_{\\text{lasso}}(b_{\\text{new}}) = -r_j^T x + b_{\\text{new}} \\lVert x \\rVert_2^2 + \\lambda \\cdot \\partial |b_{\\text{new}}|$$\n其中 $\\partial |u|$ 是绝对值函数的次梯度：当 $u \\neq 0$ 时为 $\\text{sgn}(u)$，当 $u=0$ 时为区间 $[-1, 1]$。\n在最小值点，次梯度集合必须包含 $0$：\n$$0 \\in -r_j^T x + b_{\\text{new}} \\lVert x \\rVert_2^2 + \\lambda \\cdot \\text{sgn}(b_{\\text{new}})$$\n令 $z = r_j^T x = r^T x + b \\lVert x \\rVert_2^2$ 并假设 $\\lVert x \\rVert_2^2  0$。条件变为：\n$$z - b_{\\text{new}} \\lVert x \\rVert_2^2 \\in \\lambda \\cdot \\text{sgn}(b_{\\text{new}})$$\n这就得出了解决方案：\n- 如果 $z  \\lambda$，则 $b_{\\text{new}}  0$：$z - b_{\\text{new}} \\lVert x \\rVert_2^2 = \\lambda \\implies b_{\\text{new}} = (z - \\lambda) / \\lVert x \\rVert_2^2$。\n- 如果 $z  -\\lambda$，则 $b_{\\text{new}}  0$：$z - b_{\\text{new}} \\lVert x \\rVert_2^2 = -\\lambda \\implies b_{\\text{new}} = (z + \\lambda) / \\lVert x \\rVert_2^2$。\n- 如果 $|z| \\le \\lambda$，则 $b_{\\text{new}} = 0$。\n\n这是软阈值函数，$S(z, \\lambda) = \\text{sgn}(z) \\max(|z|-\\lambda, 0)$。更新规则是：\n$$b_{\\text{new}} = \\frac{S(r^T x + b \\lVert x \\rVert_2^2, \\lambda)}{\\lVert x \\rVert_2^2}$$\n如果 $\\lVert x \\rVert_2^2 = 0$，要最小化的目标函数就是简单的 $\\lambda |b_{\\text{new}}|$（加上一个常数），它在 $b_{\\text{new}} = 0$ 处取最小值。这种情况在实现中必须单独处理。\n\n**3. 残差平方和的变化量 ($\\Delta RSS$)**\n\n问题要求计算 $RSS$ 的精确变化量，定义为 $\\Delta RSS = RSS' - RSS$。\n当前的 $RSS$ 是 $\\lVert r \\rVert_2^2$。\n新的残差是 $r' = r - xd$，其中 $d = b_{\\text{new}} - b$ 是系数的变化量。\n新的 $RSS'$ 是：\n$$RSS' = \\lVert r' \\rVert_2^2 = \\lVert r - xd \\rVert_2^2 = (r - xd)^T(r - xd)$$\n$$RSS' = r^T r - (xd)^T r - r^T(xd) + (xd)^T(xd)$$\n由于 $r^T(xd) = d(r^T x)$ 是一个标量，它等于其转置 $(xd)^T r = d(x^T r)$。\n$$RSS' = \\lVert r \\rVert_2^2 - 2d(r^T x) + d^2(x^T x) = RSS - 2d(r^T x) + d^2 \\lVert x \\rVert_2^2$$\n因此，$RSS$ 的变化量是：\n$$\\Delta RSS = RSS' - RSS = d^2 \\lVert x \\rVert_2^2 - 2d(r^T x)$$\n这个公式允许我们直接从更新量 $d$、预测变量 $x$ 和当前残差 $r$ 计算 $\\Delta RSS$，而无需计算新的残差向量 $r'$。\n\n**算法摘要**\n对于每个测试用例：\n1.  给定 $x, r, b$ 和惩罚参数。\n2.  计算标量：$r^T x$ 和 $\\lVert x \\rVert_2^2$。\n3.  根据惩罚类型（岭回归或 Lasso），使用推导出的公式计算更新后的系数 $b_{\\text{new}}$。处理 $\\lVert x \\rVert_2^2 = 0$ 的特殊情况。\n4.  计算系数的变化量：$d = b_{\\text{new}} - b$。\n5.  计算 $RSS$ 的变化量：$\\Delta RSS = d^2 \\lVert x \\rVert_2^2 - 2d(r^T x)$。\n6.  报告 $\\Delta RSS$ 的值。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a series of single-coordinate update problems for Ridge and Lasso\n    regression and computes the change in Residual Sum of Squares (RSS).\n    \"\"\"\n\n    test_cases = [\n        # Case 1: Ridge happy path\n        {'x': [1.2, -0.3, 0.5, 0.0, 2.1],\n         'r': [0.8, -1.1, 0.3, 0.0, 2.0],\n         'b': 0.4,\n         'penalty': 'ridge',\n         'param': 0.7},\n\n        # Case 2: Lasso happy path (nonzero update expected)\n        {'x': [0.5, -1.0, 0.3, 1.2, -0.7, 0.4],\n         'r': [1.0, 0.5, -0.2, 0.3, -1.5, 0.1],\n         'b': -0.1,\n         'penalty': 'lasso',\n         'param': 0.2},\n\n        # Case 3: Lasso thresholding to zero (edge case)\n        {'x': [0.2, -0.1, 0.05, 0.0],\n         'r': [0.01, -0.02, 0.03, -0.04],\n         'b': 0.5,\n         'penalty': 'lasso',\n         'param': 0.2},\n\n        # Case 4: Ridge with zero predictor column (boundary case)\n        {'x': [0.0, 0.0, 0.0, 0.0, 0.0],\n         'r': [1.0, -1.0, 0.5, -0.5, 2.0],\n         'b': 3.0,\n         'penalty': 'ridge',\n         'param': 1.0},\n\n        # Case 5: Lasso with numerically tiny predictor (numerical stability case)\n        {'x': [1e-8, -2e-8, 3e-8],\n         'r': [0.0, 0.0, 0.0],\n         'b': 1.0,\n         'penalty': 'lasso',\n         'param': 0.1}\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        x_vec = np.array(case['x'], dtype=np.float64)\n        r_vec = np.array(case['r'], dtype=np.float64)\n        b = case['b']\n        penalty_type = case['penalty']\n        param = case['param']\n        \n        b_new = 0.0\n\n        # Compute scalar quantities needed for the updates\n        x_sq_norm = np.dot(x_vec, x_vec) # Corresponds to ||x||_2^2\n        r_dot_x = np.dot(r_vec, x_vec)   # Corresponds to r^T x\n\n        if penalty_type == 'ridge':\n            # Ridge update: b_new = (r^T x + b ||x||^2) / (||x||^2 + alpha)\n            alpha = param\n            numerator = r_dot_x + b * x_sq_norm\n            denominator = x_sq_norm + alpha\n            # Denominator is guaranteed to be non-zero since alpha = 0 and we\n            # handle the pure ||x||^2 = 0 case implicitly. If ||x||^2 is 0,\n            # numerator is 0, so b_new becomes 0 correctly.\n            if denominator != 0:\n                b_new = numerator / denominator\n            else:\n                 # This case only happens if x_sq_norm=0 and alpha=0,\n                 # which is unpenalized OLS and ill-defined for a zero predictor.\n                 # With alpha0, this will not occur.\n                 b_new = 0.0\n\n        elif penalty_type == 'lasso':\n            # Lasso update: b_new = S(r^T x + b ||x||^2, lambda) / ||x||^2\n            # where S is the soft-thresholding operator.\n            lmbda = param\n            \n            # Handle the case where the predictor is a zero vector.\n            # In this case, the coordinate has no effect on RSS, so the\n            # L1 penalty lambda * |b_new| is minimized at b_new = 0.\n            if x_sq_norm == 0:\n                b_new = 0.0\n            else:\n                z = r_dot_x + b * x_sq_norm\n                if z  lmbda:\n                    b_new = (z - lmbda) / x_sq_norm\n                elif z  -lmbda:\n                    b_new = (z + lmbda) / x_sq_norm\n                else: # |z| = lmbda\n                    b_new = 0.0\n        \n        # Compute the change in coefficient\n        d = b_new - b\n        \n        # Compute the exact change in RSS: Delta_RSS = d^2 ||x||^2 - 2d (r^T x)\n        delta_rss = d**2 * x_sq_norm - 2 * d * r_dot_x\n        \n        results.append(delta_rss)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4947418"}]}