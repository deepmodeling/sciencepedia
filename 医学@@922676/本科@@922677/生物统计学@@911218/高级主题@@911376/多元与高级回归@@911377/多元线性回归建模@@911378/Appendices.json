{"hands_on_practices": [{"introduction": "在构建回归模型时，我们经常遇到以不同单位（如年龄以年计，钠摄入量以毫克计）测量的预测变量。虽然普通最小二乘法（OLS）在数学上不要求对这些变量进行缩放，但中心化和标准化等预处理步骤可以极大地提高模型系数的可解释性，并在引入交互项或多项式项时改善模型的数值稳定性。这项练习将帮助您辨析这些变换如何影响您的模型结果，澄清常见的误解 ([@problem_id:4930773])。", "problem": "一位生物统计学家使用多元线性回归模型来模拟一个连续的临床结果 $Y$（例如，收缩压）。该模型包含一个截距项和三个预测变量：年龄 $A$（单位：年）、身体质量指数 $B$（单位：$\\mathrm{kg}/\\mathrm{m}^2$）和膳食钠摄入量 $S$（单位：$\\mathrm{mg}/\\mathrm{day}$）。在误差向量均值为 $0$ 且方差恒定的标准假设下，使用普通最小二乘法（OLS）进行拟合。由于预测变量的测量尺度不同，她考虑对预测变量列（而非对 $Y$）应用两种常见的预处理步骤：中心化（减去每个预测变量的样本均值，使其均值为 $0$）和标准化（减去每个预测变量的样本均值并除以其样本标准差，使其均值为 $0$，方差为 $1$）。她拟合了只包含线性主效应的模型，并且也考虑了增加交互作用项和多项式项（例如，$A \\times S$ 和 $B^2$）的模型，这些项可能是在中心化之后构成的。\n\n假设使用方差膨胀因子（VIF）来评估多重共线性，VIF 通过将一个预测变量对剩余预测变量进行回归所得到的 $R^2$ 来定义。关于对预测变量矩阵 $X$ 进行中心化和标准化的操作，对回归系数 $\\beta$ 的解释以及对多重共线性的影响，下列哪些陈述是正确的？选择所有适用项。\n\nA. 在一个只包含线性主效应的模型中，对 $A$、$B$ 和 $S$ 进行中心化，使每个变量的均值为 $0$，这会使 $A$、$B$ 和 $S$ 的斜率估计值在数值上保持不变，但会改变截距项，使其估计的是当 $A$、$B$ 和 $S$ 处于其样本均值时 $Y$ 的期望值。\n\nB. 对 $A$、$B$ 和 $S$ 进行标准化（均值为 $0$，方差为 $1$）会改变拟合值和残差，因为预测变量的尺度不同。\n\nC. 在一个只包含线性主效应的模型中，进行标准化后，每个斜率可以解释为相应预测变量增加 $1$ 个标准差时 $Y$ 的期望变化量，并且其数值上等于原始的未标准化斜率乘以该预测变量的样本标准差。\n\nD. 仅对线性预测变量进行中心化或标准化会降低这些预测变量的 VIF，因为它降低了预测变量之间的相关性。\n\nE. 在创建交互作用项或多项式项之前对预测变量进行中心化（例如，在形成 $(A - \\bar{A}) \\times (S - \\bar{S})$ 或 $(B - \\bar{B})^2$ 之前，用 $A - \\bar{A}$ 替换 $A$）倾向于减少主效应与这些高阶项之间的多重共线性，而不改变拟合值。\n\nF. 标准化预测变量可以改善用于计算 OLS 估计值的正规方程矩阵的数值条件，尽管基于 $R^2$（和 VIF）的统计多重共线性度量不受此类对原始线性预测变量的线性重缩放的影响。", "solution": "该问题陈述是理解多元线性回归建模性质的一个有效练习。它在科学上基于已建立的统计理论，提法恰当且客观。其中没有矛盾、模糊之处或缺失信息会妨碍对这些断言进行严谨的分析。\n\n多元线性回归模型的一般形式为 $Y = X\\beta + \\epsilon$，其中 $Y$ 是 $n \\times 1$ 的结果向量，$X$ 是 $n \\times (p+1)$ 的设计矩阵（其中一列全为1，作为截距项，另有 $p$ 个预测变量列），$\\beta$ 是 $(p+1) \\times 1$ 的系数向量，$\\epsilon$ 是 $n \\times 1$ 的误差向量。$\\beta$ 的普通最小二乘（OLS）估计为 $\\hat{\\beta} = (X^T X)^{-1} X^T Y$。\n\n预测变量是年龄（$A$）、身体质量指数（$B$）和膳食钠摄入量（$S$）。样本均值记为 $\\bar{A}$、$\\bar{B}$、$\\bar{S}$，样本标准差记为 $s_A$、$s_B$、$s_S$。\n\n对预测变量 $A$ 进行中心化得到新的预测变量 $A_c = A - \\bar{A}$。\n对预测变量 $A$ 进行标准化得到新的预测变量 $A_s = (A - \\bar{A}) / s_A$。\n\n我们现在来评估每个陈述。\n\nA. 在一个只包含线性主效应的模型中，对 $A$、$B$ 和 $S$ 进行中心化，使每个变量的均值为 $0$，这会使 $A$、$B$ 和 $S$ 的斜率估计值在数值上保持不变，但会改变截距项，使其估计的是当 $A$、$B$ 和 $S$ 处于其样本均值时 $Y$ 的期望值。\n\n设原始模型为 $Y_i = \\beta_0 + \\beta_A A_i + \\beta_B B_i + \\beta_S S_i + \\epsilon_i$。\n中心化后的模型为 $Y_i = \\beta_0^* + \\beta_A^* (A_i - \\bar{A}) + \\beta_B^* (B_i - \\bar{B}) + \\beta_S^* (S_i - \\bar{S}) + \\epsilon_i$。\n\n根据 Frisch-Waugh-Lovell 定理，多元回归中给定预测变量的系数，等价于将因变量对所有其他预测变量回归得到的残差，对该预测变量对所有其他预测变量回归得到的残差进行回归所得到的系数。对所有预测变量进行中心化等价于对它们相对于截距项进行残差化。在有截距项的模型中，斜率系数衡量的是在考虑了截距项之后每个预测变量的影响。因此，向一个已经包含截距项的模型中添加一个中心化的截距项（通过中心化预测变量实现）并不会改变估计的斜率系数。因此，$\\hat{\\beta}_A = \\hat{\\beta}_A^*$，$\\hat{\\beta}_B = \\hat{\\beta}_B^*$，以及 $\\hat{\\beta}_S = \\hat{\\beta}_S^*$。斜率估计值保持不变。\n\n对于中心化模型中的截距项，由于设计矩阵的预测变量列的和（以及均值）为 $0$，所以这些列与截距列（一个全为1的向量）是正交的。这种块正交性简化了 OLS 估计，截距估计值变为 $\\hat{\\beta}_0^* = \\bar{Y}$。这是当中心化的预测变量都为 $0$ 时的 $Y$ 的预测值，这发生在 $A_i = \\bar{A}$，$B_i = \\bar{B}$ 和 $S_i = \\bar{S}$ 的时候。\n原始截距 $\\hat{\\beta}_0$ 与新截距的关系如下：\n$\\hat{\\beta}_0^* + \\hat{\\beta}_A (A - \\bar{A}) = \\hat{\\beta}_0 + \\hat{\\beta}_A A$\n$\\hat{\\beta}_0 = \\hat{\\beta}_0^* - \\hat{\\beta}_A \\bar{A}$。\n对于所有预测变量，$\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_A \\bar{A} - \\hat{\\beta}_B \\bar{B} - \\hat{\\beta}_S \\bar{S}$。这是当 $A=0$，$B=0$ 和 $S=0$ 时 $Y$ 的预测值，通常与 $\\bar{Y}$ 不同。该陈述在所有方面都是正确的。\n\n结论：**正确**。\n\nB. 对 $A$、$B$ 和 $S$ 进行标准化（均值为 $0$，方差为 $1$）会改变拟合值和残差，因为预测变量的尺度不同。\n\n设原始设计矩阵为 $X = [\\mathbf{1}, X_A, X_B, X_S]$。\n设标准化后的设计矩阵为 $X_{std} = [\\mathbf{1}, X_{A,s}, X_{B,s}, X_{S,s}]$，其中 $X_{A,s} = (X_A - \\bar{A}\\mathbf{1})/s_A$。\n$X_{std}$ 的每一列都是 $X$ 的列的线性组合。例如，$X_{A,s}$ 是 $X_A$ 和 $\\mathbf{1}$ 的线性组合。反之，$X$ 的每一列也是 $X_{std}$ 的列的线性组合。例如，$X_A = s_A X_{A,s} + \\bar{A}\\mathbf{1}$。\n这意味着 $X$ 的列空间与 $X_{std}$ 的列空间相同，即 $\\text{Col}(X) = \\text{Col}(X_{std})$。\n拟合值 $\\hat{Y}$ 是结果向量 $Y$ 在设计矩阵列空间上的正交投影。投影矩阵（或帽子矩阵）由 $H = X(X^T X)^{-1} X^T$ 给出。由于列空间不变，投影矩阵也不变。因此，拟合值 $\\hat{Y} = HY$ 对两个模型是相同的。\n残差由 $e = Y - \\hat{Y}$ 给出。由于 $Y$ 和 $\\hat{Y}$ 都不变，残差也相同。\n\n结论：**不正确**。\n\nC. 在一个只包含线性主效应的模型中，进行标准化后，每个斜率可以解释为相应预测变量增加 $1$ 个标准差时 $Y$ 的期望变化量，并且其数值上等于原始的未标准化斜率乘以该预测变量的样本标准差。\n\n设标准化模型为 $Y = \\beta_0^{std} + \\beta_A^{std} A_s + \\dots + \\epsilon$。预测变量为 $A_s = (A - \\bar{A})/s_A$。\n系数 $\\beta_A^{std}$ 表示在保持其他标准化预测变量不变的情况下，$A_s$ 每增加 $1$ 个单位时 $E[Y]$ 的变化量。$A_s$ 增加 $1$ 个单位对应于原始预测变量 $A$ 增加 $s_A$ 个单位。因此，该解释是正确的。\n\n现在，我们来找出数值关系。设未标准化的模型为 $Y = \\beta_0 + \\beta_A A + \\dots + \\epsilon$。\n我们从(A)中知道，中心化不会改变斜率估计值。因此，如果我们将 $Y$ 对中心化预测变量 $A_c = A - \\bar{A}$ 进行回归，斜率估计值仍然是 $\\hat{\\beta}_A$。\n中心化和标准化预测变量之间的关系是 $A_c = s_A A_s$。\n将此代入中心化模型方程：\n$Y = ... + \\hat{\\beta}_A A_c + ... = ... + \\hat{\\beta}_A (s_A A_s) + ...$\n将此与标准化模型形式 $Y = ... + \\hat{\\beta}_A^{std} A_s + ...$ 进行比较，我们必然有：\n$\\hat{\\beta}_A^{std} = \\hat{\\beta}_A s_A$。\n标准化斜率是原始未标准化斜率乘以预测变量的样本标准差。该陈述完全正确。\n\n结论：**正确**。\n\nD. 仅对线性预测变量进行中心化或标准化会降低这些预测变量的 VIF，因为它降低了预测变量之间的相关性。\n\n预测变量 $X_j$ 的方差膨胀因子为 $\\text{VIF}_j = 1/(1 - R_j^2)$，其中 $R_j^2$ 是将 $X_j$ 对其他 $p-1$ 个预测变量进行回归得到的决定系数。\n两个变量 $U$ 和 $V$ 之间的皮尔逊相关系数对缩放和平移是不变的，即对于符号相同的 $a, c$，有 $\\text{Corr}(aU+b, cV+d) = \\text{Corr}(U,V)$。中心化（$A_c = A - \\bar{A}$）和标准化（$A_s = (A-\\bar{A})/s_A$）都是线性变换。\n因此，任意两个预测变量 $A$ 和 $B$ 之间的相关性，在对它们进行中心化或标准化后保持不变：\n$\\text{Corr}(A, B) = \\text{Corr}(A_c, B_c) = \\text{Corr}(A_s, B_s)$。\n$R_j^2$ 的值取决于预测变量之间的相关性。由于这些相关性没有改变，$R_j^2$ 也没有改变。因此，$\\text{VIF}_j$ 也没有改变。前提“因为它降低了预测变量之间的相关性”是错误的，结论“降低了 VIF”也是错误的。\n\n结论：**不正确**。\n\nE. 在创建交互作用项或多项式项之前对预测变量进行中心化（例如，在形成 $(A - \\bar{A}) \\times (S - \\bar{S})$ 或 $(B - \\bar{B})^2$ 之前，用 $A - \\bar{A}$ 替换 $A$）倾向于减少主效应与这些高阶项之间的多重共线性，而不改变拟合值。\n\n考虑一个包含 $A$ 和 $A^2$ 的模型。$A$ 和 $A^2$ 之间的相关性通常很高。现在考虑包含中心化预测变量 $A_c = A - \\bar{A}$ 及其平方 $A_c^2 = (A - \\bar{A})^2$ 的模型。协方差为 $E[A_c \\cdot A_c^2] - E[A_c]E[A_c^2] = E[(A-\\bar{A})^3]$，这与 $A$ 的偏度有关。对于对称分布，该相关性为 $0$。对于许多真实世界的分布，$\\text{Corr}(A_c, A_c^2)$ 显著小于 $\\text{Corr}(A, A^2)$。类似地，主效应 $A_c$ 与交互作用项 $A_c S_c$ 之间的相关性通常远低于 $A$ 与 $AS$ 之间的相关性。因此，中心化确实倾向于减少这种“结构性”多重共线性。\n\n现在，考虑拟合值是否改变。让我们比较模型1（$Y \\sim 1 + A + S + AS$）和模型2（$Y \\sim 1 + A_c + S_c + A_c S_c$）。\n模型2中的预测变量是 $1$、$A - \\bar{A}$、$S - \\bar{S}$ 和 $(A - \\bar{A})(S - \\bar{S}) = AS - \\bar{A}S - A\\bar{S} + \\bar{A}\\bar{S}$。\n这些预测变量中的每一个都是模型1中预测变量（即 $1$、$A$、$S$ 和 $AS$）的线性组合。例如，$A-\\bar{A} = 1 \\cdot A - \\bar{A} \\cdot 1$。\n反之，我们也可以将模型1的预测变量表示为模型2中预测变量的线性组合。例如，$A = (A-\\bar{A}) + \\bar{A} \\cdot 1$。并且 $AS = (A_c S_c) + \\bar{S}A_c + \\bar{A}S_c + \\bar{A}\\bar{S} \\cdot 1$。\n由于每个模型的预测变量集都可以由另一个模型的预测变量集的线性组合构成，因此它们的设计矩阵张成相同的向量空间。正如在(B)的分析中所确立的，如果列空间相同，则投影矩阵 $H$ 相同，因此拟合值 $\\hat{Y}=HY$ 和残差 $e=Y-\\hat{Y}$ 也相同。\n\n结论：**正确**。\n\nF. 标准化预测变量可以改善用于计算 OLS 估计值的正规方程矩阵的数值条件，尽管基于 $R^2$（和 VIF）的统计多重共线性度量不受此类对原始线性预测变量的线性重缩放的影响。\n\n在 OLS 中要求逆的矩阵是 $X^T X$。这个求逆过程的数值稳定性与该矩阵的条件数 $\\kappa(X^T X)$ 有关。高条件数表示矩阵是病态的，容易产生数值误差。\n如果预测变量的尺度差异巨大（例如，年龄以十为单位，钠摄入量以千为单位），那么 $X^T X$ 的元素（即平方和与交叉乘积）的量级也会有巨大差异。这通常会导致高的条件数。\n当预测变量被标准化后，它们的均值都为 $0$，方差都为 $1$。设计矩阵的预测变量部分 $X_p$ 的各列具有大致相等的范数。矩阵 $(X_{p,std})^T X_{p,std}$ 是预测变量相关矩阵的 $(n-1)$ 倍。其对角元素都相等（为 $(n-1)$），且非对角元素是有界的。这使得矩阵的元素处于相似的尺度，这是一种对矩阵进行预处理的标准技术。它通常会显著降低 $X^T X$ 的条件数，从而提高数值稳定性。\n该陈述的第二部分，“尽管基于 $R^2$（和 VIF）的统计多重共线性度量不受此类对原始线性预测变量的线性重缩放的影响”，是对(D)中关于仅含主效应模型的发现的重述。正如所证，VIF 确实没有改变。该陈述正确地将数值上的好处与未改变的共线性统计度量进行了对比。\n\n结论：**正确**。", "answer": "$$\\boxed{ACEF}$$", "id": "4930773"}, {"introduction": "决定系数 $R^2$ 是衡量模型拟合优度的常用指标，但它有一个众所周知的缺陷：每当向模型中添加新的预测变量时，无论该变量是否有用，$R^2$ 的值几乎总会增加。为了更公允地比较含有不同数量预测变量的模型，我们引入了“调整后的 $R^2$”，它通过对模型复杂性施加“惩罚”来修正这一问题。通过从第一性原理出发推导调整后 $R^2$ 的公式，本练习将让您深刻理解其如何平衡拟合优度与模型简约性 ([@problem_id:4930824])。", "problem": "一位生物统计学家正在建立一个关于空腹血糖（用响应变量 $Y$ 表示）的多元线性回归模型，使用的预测变量为年龄 $X_{1}$、身体质量指数 $X_{2}$ 和睡眠时长 $X_{3}$。设线性模型为 $Y = X\\beta + \\varepsilon$，其中 $X$ 包含一个截距列和 $p$ 个预测变量列（因此回归系数的总数为 $p+1$），样本量为 $n$，$\\varepsilon$ 是一个均值为 $0$ 且方差恒定的随机误差项。决定系数 $R^{2}$ 由总平方和（TSS）和残差平方和（RSS）定义为 $R^{2} = 1 - \\frac{\\text{RSS}}{\\text{TSS}}$，其中 $\\text{TSS} = \\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}$ 且 $\\text{RSS} = \\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^{2}$。从 $\\text{TSS}$ 和 $\\text{RSS}$ 的基本定义出发，并根据方差的无偏估计量是通过将平方和除以其相应的自由度得到的这一原理，推导调整后决定系数的闭式表达式，该表达式应以 $R^{2}$、$n$ 和 $p$（其中 $p$ 为不包括截距的预测变量个数）表示。然后，使用您推导出的表达式，用文字解释为什么当增加一个并未实质性改善模型拟合度的预测变量时，调整后决定系数可能会下降，并说明自由度的作用。您的最终答案必须是调整后决定系数的单一解析表达式。", "solution": "问题陈述是有效的。这是一个在生物统计学领域中定义明确、有科学依据的问题，要求从第一性原理推导一个标准统计公式——调整后决定系数（$R^2_{\\text{adj}}$），并解释其性质。所有给定的定义和条件都是标准且一致的。\n\n目标是推导调整后决定系数 $R^2_{\\text{adj}}$ 的表达式，并解释其行为。推导始于这样一个原理：调整后的度量指标应使用方差的无偏估计量，而这些估计量是通过将平方和除以其相应的自由度得到的。\n\n决定系数 $R^2$ 定义为 $R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}}$，其中 $\\text{TSS}$ 是总平方和，$\\text{RSS}$ 是残差平方和。该指标衡量了响应变量中可由预测变量预测的方差比例。然而，每当向模型中添加一个新的预测变量时，无论该预测变量的实际解释能力如何，$R^2$ 都会机械地增加或保持不变。这是因为最小二乘法优化总是会减少或保持 $\\text{RSS}$ 不变，即使这仅仅是偶然的。调整后 $R^2$ 通过惩罚包含非信息性预测变量的做法来纠正这一点。\n\n我们从响应变量总方差 $\\sigma^2_Y$ 和模型随机误差项方差 $\\sigma^2_{\\varepsilon}$ 的无偏估计量开始。\n\n总平方和 $\\text{TSS} = \\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}$ 具有 $n-1$ 个自由度，因为在估计样本均值 $\\bar{y}$ 时损失了一个自由度。因此，$Y$ 的总方差的一个无偏估计量是总均方（Mean Square Total, $\\text{MST}$）：\n$$ \\text{MST} = \\frac{\\text{TSS}}{n-1} $$\n\n残差平方和 $\\text{RSS} = \\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^{2}$ 具有 $n-(p+1)$ 个自由度。这是因为在多元线性回归模型中估计了 $p+1$ 个参数：$p$ 个预测变量的系数和一个截距项。误差方差 $\\sigma^2_{\\varepsilon}$ 的一个无偏估计量是均方误差（Mean Square Error, $\\text{MSE}$）：\n$$ \\text{MSE} = \\frac{\\text{RSS}}{n-p-1} $$\n\n调整后决定系数 $R^2_{\\text{adj}}$ 的定义与 $R^2$ 类似，但它使用的是这些无偏方差估计量的比率，而不是平方和的比率：\n$$ R^2_{\\text{adj}} = 1 - \\frac{\\text{MSE}}{\\text{MST}} $$\n代入 $\\text{MSE}$ 和 $\\text{MST}$ 的表达式，我们得到：\n$$ R^2_{\\text{adj}} = 1 - \\frac{\\frac{\\text{RSS}}{(n-p-1)}}{\\frac{\\text{TSS}}{(n-1)}} $$\n这个表达式可以重排为：\n$$ R^2_{\\text{adj}} = 1 - \\left(\\frac{\\text{RSS}}{\\text{TSS}}\\right) \\left(\\frac{n-1}{n-p-1}\\right) $$\n根据标准决定系数的定义 $R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}}$，我们可以用 $R^2$ 来表示比率 $\\frac{\\text{RSS}}{\\text{TSS}}$：\n$$ \\frac{\\text{RSS}}{\\text{TSS}} = 1 - R^2 $$\n将此代入我们的 $R^2_{\\text{adj}}$ 方程，得到以 $R^2$、$n$ 和 $p$ 表示的最终表达式：\n$$ R^2_{\\text{adj}} = 1 - (1 - R^2)\\left(\\frac{n-1}{n-p-1}\\right) $$\n\n现在，我们用这个表达式来解释为什么当增加一个并未实质性改善模型拟合度的预测变量时，$R^2_{\\text{adj}}$ 可能会下降。当向模型中添加一个新的预测变量时，预测变量的数量 $p$ 增加 1。这对 $R^2_{\\text{adj}}$ 的公式有两个相互竞争的影响：\n\n1.  **对 $R^2$ 的影响**：添加任何预测变量（无论有用与否）都会导致标准 $R^2$ 增加，或者在极少数情况下保持不变。这意味着 $(1 - R^2)$ 项会减少或保持不变。表达式的这一部分会推动 $R^2_{\\text{adj}}$ 增加。\n\n2.  **对惩罚因子的影响**：项 $\\left(\\frac{n-1}{n-p-1}\\right)$ 是对模型复杂度的惩罚。当 $p$ 增加到 $p+1$ 时，分母 $(n-p-1)$ 减小，导致整个惩罚因子增加。对于 $p \\ge 0$，该因子总是大于或等于 1。表达式的这一部分会推动 $R^2_{\\text{adj}}$ 减小。\n\n$R^2_{\\text{adj}}$ 的总体变化取决于这两种效应之间的平衡。如果新的预测变量“并未实质性改善模型拟合度”，这意味着由此导致的 $R^2$ 增量非常小。$(1 - R^2)$ 项的相应减少是轻微的。然而，随着新预测变量的加入，惩罚因子 $\\left(\\frac{n-1}{n-p-1}\\right)$ 会严格增加。如果 $(1 - R^2)$ 的微小减少不足以抵消惩罚因子的增加，它们的乘积 $(1 - R^2)\\left(\\frac{n-1}{n-p-1}\\right)$ 将会增加。由于这个乘积是从 1 中减去的，其值的增加将导致 $R^2_{\\text{adj}}$ 下降。\n\n本质上，只有当拟合度的改善（$R^2$ 的增加）足够大，足以证明为新预测变量花费一个额外自由度的“成本”是合理的时，调整后 $R^2$ 才会增加。这种机制确保了 $R^2_{\\text{adj}}$ 是一个更诚实的模型质量度量，因为它在奖励拟合优度的同时，也惩罚了模型的复杂度。", "answer": "$$\n\\boxed{1 - (1 - R^2) \\frac{n-1}{n-p-1}}\n$$", "id": "4930824"}, {"introduction": "在生物统计学的观察性研究中，一个核心挑战是区分相关性与因果关系。一个暴露变量 $X$ 与一个结局变量 $Y$ 之间的观察到的关联，可能被第三个变量 $Z$（即混杂因素）所扭曲。本练习通过一个假设但符合科学情境的案例，探讨了当混杂因素被忽略时，回归系数可能产生的偏移，甚至可能出现效应方向完全反转的“辛普森悖论”现象 ([@problem_id:4930822])。", "problem": "一位生物医学研究者将一个连续结果变量 $Y$（例如，血浆生物标志物浓度）建模为暴露变量 $X$（例如，营养摄入量）和协变量 $Z$（例如，年龄）的线性函数，并带有加性噪声。数据生成机制指定为\n$$\nY \\;=\\; \\alpha \\;+\\; \\beta_X X \\;+\\; \\beta_Z Z \\;+\\; \\varepsilon,\n$$\n其中 $\\varepsilon$ 满足 $\\mathrm{E}[\\varepsilon \\mid X,Z] = 0$，且 $\\varepsilon$ 独立于 $(X,Z)$。系数 $\\beta_X$ 表示在同时包含 $X$ 和 $Z$ 时，$X$ 的多元回归系数。在整个问题中，假设 $\\mathrm{E}[X]=0$ 和 $\\mathrm{E}[Z]=0$。\n\n考虑两种科学上现实的研究设计：\n\n研究 A（随机暴露）：$X$ 是随机化的且独立于 $Z$，因此 $X \\perp Z$。$(X,Z)$ 的联合分布满足 $\\operatorname{Var}(X)>0$ 和 $\\operatorname{Var}(Z)>0$。\n\n研究 B（具有混杂结构的观察性研究）：$X$ 和 $Z$ 相关，且 $\\operatorname{Cov}(X,Z) \\ne 0$。为具体起见，假设 $\\operatorname{Var}(X) = 1$，$\\operatorname{Var}(Z) = 1$，$\\operatorname{Cov}(X,Z) = -0.6$，并且 $\\beta_X = 0.5$，$\\beta_Z = 2.0$。\n\n$X$ 的系数关于 $Z$ 的可折叠性（collapsibility）意味着，仅通过将 $Y$ 对 $X$ 进行回归所获得的 $X$ 的系数（即简单回归系数）等于将 $Y$ 对 $(X,Z)$ 进行回归时获得的多元回归系数 $\\beta_X$。\n\n以下哪个陈述是正确的？选择所有适用的选项。\n\nA. 在研究 A 中，根据所述模型，仅将 $Y$ 对 $X$ 回归得到的 $X$ 的简单回归系数，等于将 $Y$ 对 $(X,Z)$ 回归得到的 $X$ 的多元回归系数。\n\nB. 在研究 A 中，即使 $X$ 独立于 $Z$，$X$ 的简单回归系数通常也会与其多元回归系数不同，除非 $\\beta_Z = 0$。\n\nC. 在研究 B 中，只要 $\\operatorname{Cov}(X,Z)  0$ 且 $\\beta_Z > 0$ 且其幅度足够大，即使多元回归系数 $\\beta_X$ 为正，$X$ 的简单回归系数也可能为负。\n\nD. 辛普森悖论（Simpson’s paradox）指的是当 $X$ 和 $Z$ 相关且 $Z$ 影响 $Y$ 时，在对 $Z$ 进行调整后，$X$ 和 $Y$ 之间关联的方向可能发生逆转的现象。\n\nE. 如果 $\\operatorname{Cov}(X,Z) = 0$，那么无论 $\\beta_Z$ 的大小如何，$X$ 的简单回归系数都等于其多元回归系数 $\\beta_X$。", "solution": "用户希望我验证并解决一个关于简单线性回归系数和多元线性回归系数之间关系的问题，该问题涉及对预测变量协方差结构的不同假设。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n\n问题提供了以下信息：\n-   **数据生成模型**：$Y \\;=\\; \\alpha \\;+\\; \\beta_X X \\;+\\; \\beta_Z Z \\;+\\; \\varepsilon$。\n-   **结果变量**：$Y$ 是一个连续变量。\n-   **预测变量**：$X$（暴露）和 $Z$（协变量）是变量。\n-   **误差项**：$\\varepsilon$ 是一个随机误差，满足 $\\mathrm{E}[\\varepsilon \\mid X,Z] = 0$ 且 $\\varepsilon$ 独立于 $(X,Z)$。\n-   **目标系数**：$\\beta_X$ 是 $X$ 的多元回归系数。\n-   **假设**：$\\mathrm{E}[X]=0$ 和 $\\mathrm{E}[Z]=0$。\n-   **研究 A（随机暴露）**：$X$ 独立于 $Z$（$X \\perp Z$）。此外，$\\operatorname{Var}(X)0$ 和 $\\operatorname{Var}(Z)0$。\n-   **研究 B（有混杂的观察性研究）**：$X$ 和 $Z$ 相关。具体来说，$\\operatorname{Var}(X) = 1$，$\\operatorname{Var}(Z) = 1$，$\\operatorname{Cov}(X,Z) = -0.6$，$\\beta_X = 0.5$，$\\beta_Z = 2.0$。\n-   **可折叠性定义**：$X$ 的简单回归系数（仅将 $Y$ 对 $X$ 回归得到）等于多元回归系数 $\\beta_X$。\n\n**步骤 2：使用提取的已知条件进行验证**\n\n-   **科学性**：该问题基于多元线性回归的基本原理，这是生物统计学和许多其他科学领域的核心课题。混杂、可折叠性、随机化和观察性研究等概念是统计建模和因果推断的核心。指定的模型是标准线性模型。所有方面都具有科学合理性。\n-   **适定性**：问题陈述清晰。它要求在明确定义的条件下评估 $X$ 的简单回归系数和多元回归系数之间的关系。所提供的信息足以推导出必要的公式并评估每个陈述。存在一个唯一的、稳定的、有意义的解。\n-   **客观性**：术语精确且为统计学标准术语（例如，“多元回归系数”、“协方差”、“独立”）。问题没有主观或含糊不清的语言。研究 B 中的数值是一致的；$(X, Z)$ 的指定协方差矩阵为 $\\begin{pmatrix} 1  -0.6 \\\\ -0.6  1 \\end{pmatrix}$，该矩阵是正定的，因为其行列式为 $1 - (-0.6)^2 = 1 - 0.36 = 0.64 > 0$。\n\n**步骤 3：结论与行动**\n\n问题陈述是有效的。它科学合理、适定、客观，并包含足够的信息以进行严谨的求解。我将继续推导解答。\n\n### 解答推导\n\n问题的核心是将模型 $Y = \\alpha + \\beta_X X + \\beta_Z Z + \\varepsilon$ 中的多元回归系数 $\\beta_X$ 与省略 $Z$ 的模型中获得的简单回归系数进行比较。\n\n设简单线性回归模型为 $Y = \\alpha' + \\beta_{X, \\text{simple}} X + \\eta$。\n总体系数 $\\beta_{X, \\text{simple}}$ 由以下公式给出：\n$$\n\\beta_{X, \\text{simple}} = \\frac{\\operatorname{Cov}(X,Y)}{\\operatorname{Var}(X)}\n$$\n我们必须使用真实的数据生成模型 $Y = \\alpha + \\beta_X X + \\beta_Z Z + \\varepsilon$ 来计算 $\\operatorname{Cov}(X,Y)$。\n\n使用协方差的性质：\n$$\n\\operatorname{Cov}(X,Y) = \\operatorname{Cov}(X, \\alpha + \\beta_X X + \\beta_Z Z + \\varepsilon)\n$$\n$$\n\\operatorname{Cov}(X,Y) = \\operatorname{Cov}(X, \\alpha) + \\operatorname{Cov}(X, \\beta_X X) + \\operatorname{Cov}(X, \\beta_Z Z) + \\operatorname{Cov}(X, \\varepsilon)\n$$\n由于 $\\alpha$ 和 $\\beta_X, \\beta_Z$ 是常数：\n$$\n\\operatorname{Cov}(X,Y) = 0 + \\beta_X \\operatorname{Cov}(X,X) + \\beta_Z \\operatorname{Cov}(X,Z) + \\operatorname{Cov}(X, \\varepsilon)\n$$\n$$\n\\operatorname{Cov}(X,Y) = \\beta_X \\operatorname{Var}(X) + \\beta_Z \\operatorname{Cov}(X,Z) + \\operatorname{Cov}(X, \\varepsilon)\n$$\n问题陈述 $\\varepsilon$ 独立于 $(X,Z)$，这意味着 $\\operatorname{Cov}(X, \\varepsilon) = 0$。因此：\n$$\n\\operatorname{Cov}(X,Y) = \\beta_X \\operatorname{Var}(X) + \\beta_Z \\operatorname{Cov}(X,Z)\n$$\n将此代入 $\\beta_{X, \\text{simple}}$ 的公式中：\n$$\n\\beta_{X, \\text{simple}} = \\frac{\\beta_X \\operatorname{Var}(X) + \\beta_Z \\operatorname{Cov}(X,Z)}{\\operatorname{Var}(X)}\n$$\n$$\n\\beta_{X, \\text{simple}} = \\beta_X + \\beta_Z \\frac{\\operatorname{Cov}(X,Z)}{\\operatorname{Var}(X)}\n$$\n这个基本方程将简单回归系数与多元回归系数联系起来。项 $\\beta_Z \\frac{\\operatorname{Cov}(X,Z)}{\\operatorname{Var}(X)}$ 被称为遗漏变量偏误。\n当此偏误项为零时，即可实现可折叠性，即 $\\beta_{X, \\text{simple}} = \\beta_X$。这种情况当且仅当以下条件之一成立时发生：\n1.  $\\beta_Z = 0$（在多元回归模型中，被遗漏的变量 $Z$ 对结果变量 $Y$ 没有影响）。\n2.  $\\operatorname{Cov}(X,Z) = 0$（暴露变量 $X$ 与被遗漏的变量 $Z$ 不相关）。\n\n现在我们评估每个选项。\n\n### 逐项分析\n\n**A. 在研究 A 中，根据所述模型，仅将 $Y$ 对 $X$ 回归得到的 $X$ 的简单回归系数，等于将 $Y$ 对 $(X,Z)$ 回归得到的 $X$ 的多元回归系数。**\n\n在研究 A 中，给定 $X$ 独立于 $Z$（$X \\perp Z$）。独立性的一个结果是协方差为零，即 $\\operatorname{Cov}(X,Z) = 0$。将此代入我们推导的公式：\n$$\n\\beta_{X, \\text{simple}} = \\beta_X + \\beta_Z \\frac{0}{\\operatorname{Var}(X)} = \\beta_X\n$$\n由于 $\\beta_{X, \\text{simple}}$ 等于 $\\beta_X$，该陈述是正确的。这是随机化的关键优势：它通过打破暴露与其他协变量之间的关联来消除混杂。\n**结论：正确。**\n\n**B. 在研究 A 中，即使 $X$ 独立于 $Z$，$X$ 的简单回归系数通常也会与其多元回归系数不同，除非 $\\beta_Z = 0$。**\n\n该陈述与选项 A 的分析相矛盾。如上所示，在研究 A 中，条件 $X \\perp Z$ 意味着 $\\operatorname{Cov}(X,Z) = 0$。这使得偏误项 $\\beta_Z \\frac{\\operatorname{Cov}(X,Z)}{\\operatorname{Var}(X)}$ 对于 *任何* $\\beta_Z$ 的值都等于零。简单回归系数和多元回归系数将相等。声称除非 $\\beta_Z = 0$，否则它们通常会不同的说法是错误的。\n**结论：不正确。**\n\n**C. 在研究 B 中，只要 $\\operatorname{Cov}(X,Z)  0$ 且 $\\beta_Z > 0$ 且其幅度足够大，即使多元回归系数 $\\beta_X$ 为正，$X$ 的简单回归系数也可能为负。**\n\n让我们使用公式 $\\beta_{X, \\text{simple}} = \\beta_X + \\beta_Z \\frac{\\operatorname{Cov}(X,Z)}{\\operatorname{Var}(X)}$。\n问题提供了研究 B 的具体数值：$\\beta_X = 0.5$，$\\beta_Z = 2.0$，$\\operatorname{Cov}(X,Z) = -0.6$，以及 $\\operatorname{Var}(X) = 1$。让我们计算 $\\beta_{X, \\text{simple}}$：\n$$\n\\beta_{X, \\text{simple}} = 0.5 + (2.0) \\frac{-0.6}{1} = 0.5 - 1.2 = -0.7\n$$\n在这种情况下，多元回归系数 $\\beta_X$ 是正的 ($0.5$)，但简单回归系数 $\\beta_{X, \\text{simple}}$ 是负的 ($-0.7$)。发生这种情况是因为选项中指定的条件（$\\operatorname{Cov}(X,Z)  0$ 和 $\\beta_Z > 0$）产生了一个负的偏误项。如果这个偏误的绝对值 ($|-1.2|=1.2$) 大于真实系数 $\\beta_X$ ($0.5$)，符号就会逆转。这证实了这种可能性。\n**结论：正确。**\n\n**D. 辛普森悖论（Simpson’s paradox）指的是当 $X$ 和 $Z$ 相关且 $Z$ 影响 $Y$ 时，在对 $Z$ 进行调整后，$X$ 和 $Y$ 之间关联的方向可能发生逆转的现象。**\n\n这是一个在多元回归背景下描述辛普森悖论现象的定性陈述。“关联的方向”对应于回归系数的符号。未调整的关联由 $\\beta_{X, \\text{simple}}$ 度量，而调整后的关联由 $\\beta_X$ 度量。当 $\\mathrm{sign}(\\beta_{X, \\text{simple}}) \\neq \\mathrm{sign}(\\beta_X)$ 时，悖论就发生了。如选项 C 的分析所示，当一个变量 $Z$ 从模型中被省略，并且 $Z$ 与 $X$ 相关（$\\operatorname{Cov}(X,Z) \\neq 0$）且 $Z$ 是 $Y$ 的一个预测变量（$\\beta_Z \\neq 0$）时，这种情况就可能发生。该陈述正确地描述了这些条件及其导致的现象。\n**结论：正确。**\n\n**E. 如果 $\\operatorname{Cov}(X,Z) = 0$，那么无论 $\\beta_Z$ 的大小如何，$X$ 的简单回归系数都等于其多元回归系数 $\\beta_X$。**\n\n该陈述提出了可折叠性的两个一般条件之一。使用我们推导的公式：\n$$\n\\beta_{X, \\text{simple}} = \\beta_X + \\beta_Z \\frac{\\operatorname{Cov}(X,Z)}{\\operatorname{Var}(X)}\n$$\n如果我们设置 $\\operatorname{Cov}(X,Z) = 0$，偏误项变为 $\\beta_Z \\frac{0}{\\operatorname{Var}(X)} = 0$。这对于任何有限的 $\\beta_Z$ 值都成立（前提是 $\\operatorname{Var}(X) \\neq 0$，这本身就是估计系数的必要条件）。因此，在 $\\operatorname{Cov}(X,Z) = 0$ 的条件下，$\\beta_{X, \\text{simple}} = \\beta_X$ 总是成立的。\n**结论：正确。**", "answer": "$$\\boxed{ACDE}$$", "id": "4930822"}]}