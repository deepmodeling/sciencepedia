{"hands_on_practices": [{"introduction": "评估算法公平性的一个基本方法是衡量其决策结果是否在不同群体间公平分配。“差异性影响”（Disparate Impact）是一个核心指标，常用于法律和监管审查，用以判断AI系统的推荐是否存在偏向。本练习将指导你如何利用混淆矩阵——评估分类模型的标准工具——来计算不同群体的“选择率”（即获得积极预测的比例），并量化其间的差异。通过这个实践，你将掌握一个具体的方法来回答关键问题：“AI系统为不同群体的个体提供建议的频率是否相当？” [@problem_id:4849763]", "problem": "一家医院部署了一个监督式人工智能 (AI) 系统，该系统基于电子健康记录 (EHR) 数据，为疑似心力衰竭的患者推荐加速临床审查。预测结果 $\\hat{Y}=1$ 对应于推荐加速审查，而 $\\hat{Y}=0$ 对应于不推荐加速审查。令 $G \\in \\{A,B\\}$ 表示为审计公平性而记录的二元群体成员属性。该模型的性能在一个留出的验证集上，通过按群体分层的两个混淆矩阵（真阳性、假阳性、假阴性和真阴性的计数）进行总结：\n- 对于 $G=A$：真阳性 $=180$，假阳性 $=120$，假阴性 $=70$，真阴性 $=430$。\n- 对于 $G=B$：真阳性 $=210$，假阳性 $=90$，假阴性 $=40$，真阴性 $=260$。\n\n仅使用条件概率和混淆矩阵条目的定义，计算差异性影响比率，该比率定义为群体 $G=A$ 的选择率（阳性预测率）与群体 $G=B$ 的选择率之比。然后，使用以 $G=B$ 为参照群体的五分之四（$0.8$）规则，确定该模型对于群体 $G=A$ 是否满足该规则。\n\n将您的最终数值答案表示为差异性影响比率（一个无单位的小数），并四舍五入到四位有效数字。", "solution": "该问题陈述清晰，具有科学依据，并为完整解答提供了所有必要信息。混淆矩阵和差异性影响的概念是评估机器学习模型的标准方法。\n\n主要任务是计算差异性影响比率 (DIR)，它被定义为两个群体的选择率之比。给定群体的选择率是该群体中获得阳性预测的个体比例。预测 $\\hat{Y}=1$ 对应于阳性预测（推荐加速审查）。\n\n令 $g$ 代表一个群体，其中 $g \\in \\{A, B\\}$。每个群体的混淆矩阵的组成部分如下：\n- 真阳性 ($TP_g$)：被正确预测为阳性的实例。\n- 假阳性 ($FP_g$)：被错误预测为阳性的实例。\n- 假阴性 ($FN_g$)：被错误预测为阴性的实例。\n- 真阴性 ($TN_g$)：被正确预测为阴性的实例。\n\n群体 $g$ 中的个体总数用 $N_g$ 表示，是混淆矩阵中所有结果的总和：\n$$N_g = TP_g + FP_g + FN_g + TN_g$$\n\n群体 $g$ 的阳性预测总数是真阳性和假阳性的总和：\n$$\\text{Positive Predictions}_g = TP_g + FP_g$$\n\n群体 $g$ 的选择率，记作 $SR_g$，是在给定属于群体 $g$ 的条件下获得阳性预测的条件概率，即 $P(\\hat{Y}=1 | G=g)$。它计算为阳性预测数量与该群体中个体总数的比率：\n$$SR_g = \\frac{TP_g + FP_g}{TP_g + FP_g + FN_g + TN_g}$$\n\n首先，我们计算群体 $A$ 的选择率。给定数据如下：\n- $TP_A = 180$\n- $FP_A = 120$\n- $FN_A = 70$\n- $TN_A = 430$\n\n群体 $A$ 中的个体总数是：\n$$N_A = 180 + 120 + 70 + 430 = 800$$\n群体 $A$ 的阳性预测数量是：\n$$\\text{Positive Predictions}_A = 180 + 120 = 300$$\n因此，群体 $A$ 的选择率是：\n$$SR_A = \\frac{300}{800} = \\frac{3}{8} = 0.375$$\n\n接下来，我们计算群体 $B$ 的选择率。给定数据如下：\n- $TP_B = 210$\n- $FP_B = 90$\n- $FN_B = 40$\n- $TN_B = 260$\n\n群体 $B$ 中的个体总数是：\n$$N_B = 210 + 90 + 40 + 260 = 600$$\n群体 $B$ 的阳性预测数量是：\n$$\\text{Positive Predictions}_B = 210 + 90 = 300$$\n因此，群体 $B$ 的选择率是：\n$$SR_B = \\frac{300}{600} = \\frac{1}{2} = 0.5$$\n\n差异性影响比率 (DIR) 定义为群体 $A$ 的选择率与参照群体 $B$ 的选择率之比：\n$$DIR = \\frac{SR_A}{SR_B}$$\n代入计算出的值：\n$$DIR = \\frac{0.375}{0.5} = \\frac{3/8}{1/2} = \\frac{3}{8} \\times \\frac{2}{1} = \\frac{6}{8} = \\frac{3}{4} = 0.75$$\n\n该问题还要求使用五分之四规则进行评估。此规则是用于评估差异性影响的常用准则。该规则指出，受保护群体（在此案例中为群体 $A$）的选择率应至少为选择率最高群体（参照群体 $B$）的 $80\\%$（或 $0.8$）。在数学上，如果满足以下条件，则模型符合该规则：\n$$\\frac{SR_A}{SR_B} \\ge 0.8$$\n我们计算出的差异性影响比率为 $DIR = 0.75$。将此值与阈值进行比较：\n$$0.75  0.8$$\n这个不等式表明模型不满足五分之四规则，因为群体 $A$ 的选择率仅为群体 $B$ 选择率的 $75\\%$，低于 $80\\%$ 的阈值。\n\n问题要求差异性影响比率的数值，并四舍五入到四位有效数字。\n$$DIR = 0.7500$$", "answer": "$$\\boxed{0.7500}$$", "id": "4849763"}, {"introduction": "除了关注AI系统做出推荐的*频率*，我们还必须关心这些推荐的*可靠性*。如果一个AI警报对于某个群体的准确性远低于另一个群体，那么即使警报频率相同，也可能导致临床应用中的不公。本练习探讨了“预测均等”（Predictive Parity）这一概念，它衡量了AI的阳性预测对于不同群体是否具有相同的意义和准确性。通过计算阳性预测值（Positive Predictive Value, PPV）的差异，你将理解这种偏差如何导致临床医生对某些群体的警报产生“警报疲劳”，并最终造成AI工具临床效用的不平等。[@problem_id:4849697]", "problem": "一家三级医院在急诊科部署了一款人工智能 (AI) 临床决策支持工具，用于标记有败血症风险的患者。评估团队审计了该工具在两个在临床上具有可比性的人口统计学群体（A组和B组）之间的公平性。设阳性预测值 (PPV) 定义为 $\\text{PPV} = \\mathbb{P}(Y=1 \\mid \\hat{Y}=1)$，其中 $Y$ 是真实的临床结局（48小时内发生败血症，$Y=1$ 表示发生败血症），而 $\\hat{Y}$ 是模型的二元警报（$\\hat{Y}=1$ 表示发出警报）。预测均等性是指 PPV 在不同群体间相等的条件。预测均等性差异 (PPD) 是各组之间阳性预测值的有符号差异，计算方法为A组的值减去B组的值。\n\n在为期一个月的审计中，该团队估计A组的 $\\text{PPV}$ 为 $0.60$，B组的 $\\text{PPV}$ 为 $0.45$。仅使用 $\\text{PPV}$ 和预测均等性的核心定义，计算预测均等性差异。请用小数（而非百分比）表示您的答案。无需四舍五入。然后，仅根据该差异的符号和大小，简要说明其主要的临床意义，即A组和B组之间警报的正确性有何不同。", "solution": "经评估，问题陈述有效。它在科学上基于既定的模型评估统计原则，问题提法得当，提供了所有必要信息，且语言客观。该场景是医疗人工智能伦理中的一个现实应用，数据具有一致性和合理性。\n\n该问题要求计算预测均等性差异 (PPD) 并解释其临床意义。\n\n设 $Y$ 是表示真实临床结局的随机变量，其中 $Y=1$ 表示在48小时内出现败血症，$Y=0$ 表示未出现败血症。设 $\\hat{Y}$ 是人工智能模型二元警报的随机变量，其中 $\\hat{Y}=1$ 表示发出警报，$\\hat{Y}=0$ 表示无警报。\n\n阳性预测值 (PPV) 定义为在模型预测为阳性的条件下，真实结局为阳性的条件概率：\n$$ \\text{PPV} = \\mathbb{P}(Y=1 \\mid \\hat{Y}=1) $$\n该值量化了AI工具发出的警报能正确识别出将要发生败血症的患者的概率。\n\n题目给出了两个不同人口统计学群体（A组和B组）的估计 PPV：\n$$ \\text{PPV}_{\\text{A}} = 0.60 $$\n$$ \\text{PPV}_{\\text{B}} = 0.45 $$\n\n预测均等性差异 (PPD) 定义为A组的 PPV 与 B组的 PPV 之间的差值：\n$$ \\text{PPD} = \\text{PPV}_{\\text{A}} - \\text{PPV}_{\\text{B}} $$\n\n将给定值代入定义中：\n$$ \\text{PPD} = 0.60 - 0.45 $$\n$$ \\text{PPD} = 0.15 $$\n\n预测均等性差异为 $0.15$。\n\n问题的第二部分要求阐述这一结果的主要临床意义。\nPPD 的符号为正，这表明 $\\text{PPV}_{\\text{A}}  \\text{PPV}_{\\text{B}}$。这意味着，在人工智能发出警报的情况下，患者实际患有败血症的概率对于A组患者 ($60\\%$) 高于B组患者 ($45\\%$)。\n\n其主要的临床意义是，对于B组患者，AI的警报可靠性较低。与针对A组患者的警报相比，针对B组患者的警报是假阳性（即“虚假警报”）的概率更高。因此，临床医生在监测B组患者时可能会经历更严重的警报疲劳，并且B组患者更有可能因不正确的AI警报而接受不必要的后续诊断程序或治疗。这代表了一种算法偏见，即该工具的性能和临床效用在不同人口统计学群体之间是不公平的。", "answer": "$$\\boxed{0.15}$$", "id": "4849697"}, {"introduction": "ROC曲线下面积（Area Under the ROC Curve, AUC）是衡量模型整体区分能力的常用指标，一个高AUC值通常被认为是模型性能优越的标志。然而，仅仅依靠AUC来判断算法的公平性可能会产生误导。本练习设定了一个场景，其中两个群体的AUC值存在差异，并引导你深入思考公平性的真正含义。这个概念性练习旨在挑战“单一性能分数足以保证公平性”的简单看法，帮助你认识到为何决策阈值的选择、疾病流行率和模型校准度等因素对于全面的伦理分析至关重要，从而让你对AI评估建立更深刻的理解。[@problem_id:4849739]", "problem": "一家医院评估一个基于人工智能（AI）的风险预测系统，用于早期检测败血症。该模型输出一个连续的风险评分 $s \\in [0,1]$，临床医生应用一个决策阈值 $\\tau$ 来触发早期干预。对于两个患者亚群，A组和B组，医院计算了受试者工作特征（ROC）曲线，该曲线定义为在所有可能的阈值 $\\tau$ 下，真阳性率（TPR）对假阳性率（FPR）的图。真阳性率定义为 $\\mathrm{TPR} = \\frac{\\mathrm{TP}}{\\mathrm{P}}$，其中 $\\mathrm{TP}$ 是真阳性（true positives）的数量，$\\mathrm{P}$ 是实际阳性（actual positives）的数量；假阳性率定义为 $\\mathrm{FPR} = \\frac{\\mathrm{FP}}{\\mathrm{N}}$，其中 $\\mathrm{FP}$ 是假阳性（false positives）的数量，$\\mathrm{N}$ 是实际阴性（actual negatives）的数量。曲线下面积（AUC）定义为 ROC 曲线下的面积，等效地定义为模型将一个随机选择的阳性案例的排序高于一个随机选择的阴性案例的概率，即 $\\mathrm{AUC} = \\mathbb{P}\\big(s(x^{+})  s(x^{-})\\big)$。\n\n假设医院发现A组的 $\\mathrm{AUC}_A = 0.90$，B组的 $\\mathrm{AUC}_B = 0.80$。考虑一个定义为组间 AUC 绝对差异的均等性度量，$\\Delta_{\\mathrm{AUC}} = \\left|\\mathrm{AUC}_A - \\mathrm{AUC}_B\\right|$，以及 AUC 比率 $R_{\\mathrm{AUC}} = \\frac{\\mathrm{AUC}_B}{\\mathrm{AUC}_A}$。使用上述核心定义以及公正（公平对待）、有利（促进患者福祉）和不伤害（避免伤害）的伦理原则，确定 AUC 均等性值，并选择关于 AUC 上的均等性是否足以确保临床部署中的公平性的最准确陈述。\n\n哪个选项最正确？\n\nA. $\\Delta_{\\mathrm{AUC}} = 0.10$ 且 $R_{\\mathrm{AUC}} \\approx 0.89$。仅靠 AUC 均等性并不能确保公平，因为阈值选择、校准和患病率差异可能导致不相等的错误率和下游伤害，即使在 AUC 相等的情况下也是如此。\n\nB. $\\Delta_{\\mathrm{AUC}} = 0.00$ 且 $R_{\\mathrm{AUC}} = 1.00$。相等的 AUC 总是保证在任何临床选择的阈值下都有相等的假阳性率和假阴性率，因此 AUC 均等性确保了公平性。\n\nC. $\\Delta_{\\mathrm{AUC}} = 0.10$ 且 $R_{\\mathrm{AUC}} \\approx 0.89$。任何非零的 AUC 差距都明确证明该模型违反了公正的伦理原则，必须不经进一步分析就予以拒绝。\n\nD. $R_{\\mathrm{AUC}} = \\frac{0.80}{0.90} \\approx 0.89$。AUC 上的均等性是足够的，因为 AUC 总结了所有临床相关的性能；如果 AUC 相等，那么无论校准或阈值如何，模型都是公平的。\n\nE. $\\Delta_{\\mathrm{AUC}} = 0.10$。因为 AUC 与阈值无关，患病率较低的组在决策阈值下必然具有更高的阳性预测值，因此均衡 AUC 可以实现跨组的公平资源分配。", "solution": "首先将验证问题陈述的科学合理性、清晰度和完整性。\n\n### 步骤1：提取已知条件\n\n-   一个基于 AI 的败血症风险预测系统输出一个连续的风险评分 $s \\in [0,1]$。\n-   对评分应用一个决策阈值 $\\tau$ 来触发干预。\n-   考虑两个患者亚群：A组和B组。\n-   受试者工作特征（ROC）曲线定义为真阳性率（TPR）对假阳性率（FPR）的图。\n-   真阳性率定义为 $\\mathrm{TPR} = \\frac{\\mathrm{TP}}{\\mathrm{P}}$，其中 $\\mathrm{TP}$ 是真阳性的数量，$\\mathrm{P}$ 是实际阳性的数量。\n-   假阳性率定义为 $\\mathrm{FPR} = \\frac{\\mathrm{FP}}{\\mathrm{N}}$，其中 $\\mathrm{FP}$ 是假阳性的数量，$\\mathrm{N}$ 是实际阴性的数量。\n-   曲线下面积（AUC）定义为 ROC 曲线下的面积。\n-   AUC 的一个等效定义是概率 $\\mathrm{AUC} = \\mathbb{P}\\big(s(x^{+})  s(x^{-})\\big)$，其中 $x^{+}$ 是一个随机选择的阳性案例，$x^{-}$ 是一个随机选择的阴性案例。\n-   对于A组，测得的 AUC 是 $\\mathrm{AUC}_A = 0.90$。\n-   对于B组，测得的 AUC 是 $\\mathrm{AUC}_B = 0.80$。\n-   一个均等性度量，即 AUC 的绝对差异，定义为 $\\Delta_{\\mathrm{AUC}} = \\left|\\mathrm{AUC}_A - \\mathrm{AUC}_B\\right|$。\n-   一个均等性度量，即 AUC 比率，定义为 $R_{\\mathrm{AUC}} = \\frac{\\mathrm{AUC}_B}{\\mathrm{AUC}_A}$。\n-   任务是计算 $\\Delta_{\\mathrm{AUC}}$ 和 $R_{\\mathrm{AUC}}$，并基于公正、有利和不伤害的伦理原则，评估 AUC 均等性是否足以确保公平。\n\n### 步骤2：使用提取的已知条件进行验证\n\n-   **科学依据：** 该问题使用了统计学和机器学习中标准的、定义明确的度量（$\\mathrm{TPR}$、$\\mathrm{FPR}$、$\\mathrm{ROC}$、$\\mathrm{AUC}$）。评估 AI 模型在不同群体间的性能差异是算法公平性和医学伦理领域的一个核心课题。所有定义和总体背景都具有科学合理性。\n-   **适定性：** 该问题提供了计算所需度量的所有必要数值。关于 AUC 是否足以保证公平性的概念性问题是该领域的标准问题（尽管复杂），并且在科学文献中已有公认的共识答案。该问题是适定的。\n-   **客观性：** 该问题使用精确、客观和技术性的语言陈述。提到的伦理原则（公正、有利、不伤害）是医学伦理的基础，并提供了标准的评估框架。\n-   **完整性和一致性：** 提供的数据（$\\mathrm{AUC}_A = 0.90$, $\\mathrm{AUC}_B = 0.80$）对于计算是一致且充分的。定义是标准的，没有矛盾。\n-   **现实性和可行性：** 该场景非常现实。医疗 AI 模型在不同人口统计或临床群体间的性能差异是一个重要的现实问题。给出的 AUC 值对于一个良好到优秀的预测模型来说是典型的。\n\n### 步骤3：结论和行动\n\n问题陈述在科学上是合理的、适定的、客观的和完整的。它被验证为可以进行分析的有效问题。现在开始求解过程。\n\n### 解题推导\n\n首先，我们使用给定值计算指定的均等性度量。\n已知：\n$\\mathrm{AUC}_A = 0.90$\n$\\mathrm{AUC}_B = 0.80$\n\nAUC 的绝对差异是：\n$$ \\Delta_{\\mathrm{AUC}} = \\left|\\mathrm{AUC}_A - \\mathrm{AUC}_B\\right| = |0.90 - 0.80| = 0.10 $$\n\nAUC 的比率是：\n$$ R_{\\mathrm{AUC}} = \\frac{\\mathrm{AUC}_B}{\\mathrm{AUC}_A} = \\frac{0.80}{0.90} = \\frac{8}{9} \\approx 0.888... $$\n四舍五入到两位小数，得到 $R_{\\mathrm{AUC}} \\approx 0.89$。\n\n接下来，我们处理核心概念问题：AUC 上的均等性是否足以确保临床部署中的公平性？\n\nAUC 是衡量模型区分能力的一个指标，具体来说是其将随机阳性案例排在随机阴性案例之上的能力。它是一个聚合度量，总结了所有可能的决策阈值 $\\tau$ 下的性能。然而，临床效用和公平性是由在*特定、选定*的阈值下做出的决策后果决定的。有利（行善）、不伤害（避免伤害）和公正（公平分配利益和负担）的伦理原则与患者的真实世界结果相关联，而这些结果取决于操作阈值。\n\nAUC 均等性不足以保证公平，原因有几个关键点：\n1.  **伤害的阈值依赖性：** 临床系统在单个阈值 $\\tau$ 下运行。两个组的 AUC 相等并不保证在该特定 $\\tau$ 下性能相等。两条不同的 ROC 曲线可以围成相同的面积（$\\mathrm{AUC}_A = \\mathrm{AUC}_B$），但它们可以相互交叉。对于给定的阈值，A组可能有较低的 $\\mathrm{FPR}$ 和较高的 $\\mathrm{TPR}$，而B组可能有较高的 $\\mathrm{FPR}$ 和较低的 $\\mathrm{TPR}$。这将导致在谁会收到假警报（可能导致不必要的、昂贵的或有害的后续检查；违反不伤害原则）和谁会被系统漏诊（可能导致疾病未得到治疗；违反有利原则）方面存在差异。这构成了伤害和利益的不公平分配，违反了公正原则。\n2.  **校准：** AUC 衡量的是区分能力（排序能力），而不是校准。校准指的是模型的输出风险评分 $s$ 是否与结果的真实概率相对应。例如，一个校准良好的模型将确保在所有被给予评分 $s=0.2$ 的患者中，大约有 $20\\%$ 的人确实患有该病。一个模型可以有很高的 AUC，但校准得很差，并且这种校准不良在不同组之间可能存在差异。例如，评分 $s=0.7$ 对A组可能意味着 $70\\%$ 的风险，但对B组仅意味着 $50\\%$ 的风险。对两个组使用相同的阈值将导致相对于其实际风险对B组进行系统性的过度治疗，这既低效又可能有害。\n3.  **患病率差异：** 某种疾病的患病率（人群中阳性案例的比例）在不同组之间可能有所不同。假阳性率和假阴性率（$\\mathrm{FPR}$ 和 $\\mathrm{FNR} = 1-\\mathrm{TPR}$）不能直接转化为给定警报正确的可能性。这个可能性是阳性预测值（$\\mathrm{PPV} = \\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FP}}$）。根据贝叶斯定理，$\\mathrm{PPV}$ 是 $\\mathrm{TPR}$、$\\mathrm{FPR}$ 和患病率（$\\pi$）的函数。即使一个模型在各组之间实现了相等的 $\\mathrm{TPR}$ 和 $\\mathrm{FPR}$（一个被称为均等化几率的强公平性条件），患病率的差异（$\\pi_A \\ne \\pi_B$）也会导致不同的 $\\mathrm{PPV}$（$\\mathrm{PPV}_A \\ne \\mathrm{PPV}_B$）。这意味着临床团队对两个组的阳性警报的信任度会不同，可能导致注意力和资源的差异化分配，这是一个公正性问题。\n\n因此，仅仅依赖像 AUC 这样的聚合的、与阈值无关的度量，不足以进行全面的公平性评估。彻底的评估必须考虑特定阈值的错误率（$\\mathrm{TPR}$、$\\mathrm{FPR}$）、模型校准以及患病率对预测值的影响。\n\n### 逐项分析选项\n\n**A. $\\Delta_{\\mathrm{AUC}} = 0.10$ 且 $R_{\\mathrm{AUC}} \\approx 0.89$。仅靠 AUC 均等性并不能确保公平，因为阈值选择、校准和患病率差异可能导致不相等的错误率和下游伤害，即使在 AUC 相等的情况下也是如此。**\n-   **计算：** 计算出的值 $\\Delta_{\\mathrm{AUC}} = 0.10$ 和 $R_{\\mathrm{AUC}} \\approx 0.89$ 是正确的。\n-   **推理：** 关于 AUC 均等性因阈值选择、校准和患病率问题而不足以保证公平的解释，正是对 AUC 作为公平性度量局限性的正确而细致的理解。它正确地指出了这些因素可能导致不平等的错误率和伤害。\n-   **结论：** **正确**。\n\n**B. $\\Delta_{\\mathrm{AUC}} = 0.00$ 且 $R_{\\mathrm{AUC}} = 1.00$。相等的 AUC 总是保证在任何临床选择的阈值下都有相等的假阳性率和假阴性率，因此 AUC 均等性确保了公平性。**\n-   **计算：** 问题陈述为 $\\mathrm{AUC}_A = 0.90$ 和 $\\mathrm{AUC}_B = 0.80$，所以 $\\Delta_{\\mathrm{AUC}} = 0.10$ 且 $R_{\\mathrm{AUC}} \\approx 0.89$。此选项中的数值根据问题陈述是事实错误的。\n-   **推理：** 声称相等的 AUC 保证在任何阈值下都有相等的 $\\mathrm{FPR}$ 和 $\\mathrm{FNR}$（其中 $\\mathrm{FNR}=1-\\mathrm{TPR}$）是根本上错误的。如上所述，不同的 ROC 曲线可以有相同的面积。\n-   **结论：** **错误**。\n\n**C. $\\Delta_{\\mathrm{AUC}} = 0.10$ 且 $R_{\\mathrm{AUC}} \\approx 0.89$。任何非零的 AUC 差距都明确证明该模型违反了公正的伦理原则，必须不经进一步分析就予以拒绝。**\n-   **计算：** 计算出的值是正确的。\n-   **推理：** 这种说法过于绝对。虽然这样的性能差距是一个重大关切，确实*牵涉到*公正原则，但这并不意味着必须“不经进一步分析”就拒绝该模型。医学中的伦理决策涉及权衡各种原则。如果该模型尽管存在差距，但相对于现状（例如没有模型）能为*两个*群体都带来实质性的净收益，那么断然拒绝它可能会违反所有患者的有利原则和不伤害原则。正确的做法是对权衡利弊进行更深入的分析，而不是草率拒绝。\n-   **结论：** **错误**。\n\n**D. $R_{\\mathrm{AUC}} = \\frac{0.80}{0.90} \\approx 0.89$。AUC 上的均等性是足够的，因为 AUC 总结了所有临床相关的性能；如果 AUC 相等，那么无论校准或阈值如何，模型都是公平的。**\n-   **计算：** $R_{\\mathrm{AUC}}$ 的计算是正确的。\n-   **推理：** 其核心主张“AUC 上的均等性是足够的”以及它“总结了所有临床相关的性能”是错误的。这种说法体现了一种常见的误解，即认为 AUC 是衡量模型质量的包罗万象的指标。正如在推导中详细说明的，它忽略了关键的、临床相关的方面，如校准和特定阈值的性能。\n-   **结论：** **错误**。\n\n**E. $\\Delta_{\\mathrm{AUC}} = 0.10$。因为 AUC 与阈值无关，患病率较低的组在决策阈值下必然具有更高的阳性预测值，因此均衡 AUC 可以实现跨组的公平资源分配。**\n-   **计算：** $\\Delta_{\\mathrm{AUC}}$ 的值是正确的。\n-   **推理：** 该陈述包含多个错误。AUC 的阈值无关性与 PPV 之间的联系是虚假的。更关键的是，声称较低的患病率会导致*更高*的 PPV 是错误的。在其他条件相同的情况下，较低的患病率（$\\pi$）会导致*较低*的 PPV。这可以从 PPV 公式 $\\mathrm{PPV}(\\pi) = \\frac{\\mathrm{TPR} \\cdot \\pi}{\\mathrm{TPR} \\cdot \\pi + \\mathrm{FPR} \\cdot (1-\\pi)}$ 看出，该公式是 $\\pi$ 的增函数。因此，关于公平资源分配的结论是基于有缺陷的前提。\n-   **结论：** **错误**。\n\n基于此分析，选项 A 是唯一一个在计算和对 AUC 在公平性评估中作用的深入理解上都正确的选项。", "answer": "$$\\boxed{A}$$", "id": "4849739"}]}