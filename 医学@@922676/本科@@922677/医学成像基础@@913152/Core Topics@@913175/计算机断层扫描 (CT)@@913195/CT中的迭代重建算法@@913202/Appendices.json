{"hands_on_practices": [{"introduction": "在计算机断层扫描（CT）重建中，由于数据不完整或含有噪声，逆问题往往是“不适定”的（ill-posed）。本练习通过一个具体可解的例子，介绍了吉洪诺夫正则化（Tikhonov regularization），这是一种通过惩罚相邻像素之间的剧烈变化来稳定解的基本技术。通过亲手计算，您将理解正则化项如何修正标准最小二乘解，从而在有噪声的数据中获得一个更物理上合理的估计。", "problem": "考虑一个简化的平行束计算机断层扫描（CT）正向模型，其中有两个未知的、空间均匀的像素衰减系数 $x \\in \\mathbb{R}^{2}$，表示一个 $2 \\times 1$ 的列像素物体。测量了三条射线：射线1仅穿过像素1，路径长度为 $1\\,\\mathrm{cm}$；射线2仅穿过像素2，路径长度为 $1\\,\\mathrm{cm}$；射线3穿过两个像素，每个像素的路径长度均为 $1\\,\\mathrm{cm}$。将探测到的光子计数进行对数转换得到线积分后，测量数据向量为 $b \\in \\mathbb{R}^{3}$，其分量为 $b_{1} = 0.10\\,\\mathrm{cm}^{-1}$、$b_{2} = 0.20\\,\\mathrm{cm}^{-1}$ 和 $b_{3} = 0.25\\,\\mathrm{cm}^{-1}$。系统矩阵 $A \\in \\mathbb{R}^{3 \\times 2}$ 对应于这些路径长度，并由上述几何结构隐式给出。假设射线间的噪声是独立同分布的高斯噪声，因此加权最小二乘法（WLS）的加权矩阵为 $W = I_{3}$。为稳定重建结果，应用二次吉洪诺夫正则化，使用一阶有限差分算子 $L \\in \\mathbb{R}^{1 \\times 2}$（定义为 $L = \\begin{bmatrix}1  -1\\end{bmatrix}$）和正则化参数 $\\lambda = 0.50$。\n\n从线性正向模型 $b = A x + n$ 和WLS吉洪诺夫目标函数\n$$\nJ(x) = \\frac{1}{2}\\,\\|W^{1/2}(A x - b)\\|_{2}^{2} + \\frac{\\lambda}{2}\\,\\|L x\\|_{2}^{2},\n$$\n出发，推导最小化解必须满足的条件，然后使用给定的 $A$、$b$、$W$、$L$ 和 $\\lambda$ 计算最小化解 $x^{\\star}$。将最终重建的衰减系数以倒数厘米（$\\mathrm{cm}^{-1}$）表示。以单行向量 $\\begin{pmatrix}\\cdot  \\cdot\\end{pmatrix}$ 的格式提供您的最终答案。无需四舍五入；报告精确值。", "solution": "用户希望求解从3次CT测量中获得的2像素物体的正则化加权最小二乘估计。\n\n### 第1步：提取已知条件\n-   物体向量：$x \\in \\mathbb{R}^{2}$，表示一个 $2 \\times 1$ 的物体，其像素衰减系数未知且空间均匀。\n-   测量数据向量：$b \\in \\mathbb{R}^{3}$，其分量为 $b_{1} = 0.10\\,\\mathrm{cm}^{-1}$、$b_{2} = 0.20\\,\\mathrm{cm}^{-1}$ 和 $b_{3} = 0.25\\,\\mathrm{cm}^{-1}$。\n-   系统矩阵：$A \\in \\mathbb{R}^{3 \\times 2}$，由几何结构定义：\n    -   射线1：仅穿过像素1，路径长度为 $1\\,\\mathrm{cm}$。\n    -   射线2：仅穿过像素2，路径长度为 $1\\,\\mathrm{cm}$。\n    -   射线3：穿过像素1的路径长度为 $1\\,\\mathrm{cm}$，穿过像素2的路径长度为 $1\\,\\mathrm{cm}$。\n-   加权矩阵：$W = I_{3}$（$3 \\times 3$ 单位矩阵）。\n-   正则化算子：$L = \\begin{bmatrix}1  -1\\end{bmatrix}$。\n-   正则化参数：$\\lambda = 0.50$。\n-   线性正向模型：$b = A x + n$。\n-   WLS吉洪诺夫目标函数：$J(x) = \\frac{1}{2}\\,\\|W^{1/2}(A x - b)\\|_{2}^{2} + \\frac{\\lambda}{2}\\,\\|L x\\|_{2}^{2}$。\n\n### 第2步：使用提取的已知条件进行验证\n根据验证标准评估问题：\n-   **科学依据**：该问题描述了一个吉洪诺夫正则化的线性最小二乘估计，这是一种解决不适定反问题的标准和基本技术，尤其是在像CT这样的医学成像重建算法的背景下。该模型是离散物体拉东变换的一个简化但正确的表示。所有方面都与线性代数和图像重建理论的既定原则一致。\n-   **适定性**：该问题是适定的。目标函数 $J(x)$ 是严格凸的，因为项 $(A^T W A + \\lambda L^T L)$ 是正定的（因为 $A$ 具有满列秩且 $\\lambda > 0$），这确保了唯一最小化解的存在。为找到这个唯一解提供了所有必要的数据和参数。\n-   **客观性**：该问题以精确、客观的数学语言陈述。所有术语都已定义，任务明确。\n-   **完整性和一致性**：该问题是自洽的。测量次数（3）大于未知数数量（2），使得非正则化系统是超定的。即使在这种情况下，正则化也常用于稳定解以抵抗噪声，并融入先验知识（在本例中是由有限差分算子 $L$ 强制执行的平滑性）。所有数值和矩阵维度都是一致的。\n\n### 第3步：结论与行动\n该问题是有效的，因为它代表了医学成像领域中一个标准的、定义明确的数学任务。将提供解答。\n\n### 解题推导\n目标是找到向量 $x \\in \\mathbb{R}^2$，使其最小化吉洪诺夫正则化加权最小二乘目标函数：\n$$\nJ(x) = \\frac{1}{2}\\,\\|W^{1/2}(A x - b)\\|_{2}^{2} + \\frac{\\lambda}{2}\\,\\|L x\\|_{2}^{2}\n$$\n平方 $L_2$-范数可以用转置表示：$\\|v\\|_2^2 = v^T v$。\n$$\nJ(x) = \\frac{1}{2}\\,(A x - b)^T W (A x - b) + \\frac{\\lambda}{2}\\,(L x)^T (L x)\n$$\n展开各项，我们得到：\n$$\nJ(x) = \\frac{1}{2}\\,(x^T A^T - b^T) W (A x - b) + \\frac{\\lambda}{2}\\, x^T L^T L x\n$$\n$$\nJ(x) = \\frac{1}{2}\\,(x^T A^T W A x - x^T A^T W b - b^T W A x + b^T W b) + \\frac{\\lambda}{2}\\, x^T L^T L x\n$$\n由于 $x^T A^T W b$ 是一个标量，它等于其转置，$(x^T A^T W b)^T = b^T W^T A x$。因为 $W$ 是一个加权矩阵，所以它是对称的（$W^T = W$），因此 $b^T W A x = x^T A^T W b$。\n$$\nJ(x) = \\frac{1}{2}\\,x^T (A^T W A) x - x^T (A^T W b) + \\frac{1}{2}\\,b^T W b + \\frac{1}{2}\\, x^T (\\lambda L^T L) x\n$$\n这是关于 $x$ 的二次函数。为了找到最小值，我们计算 $J(x)$ 关于 $x$ 的梯度，并将其设为零向量。\n$$\n\\nabla_{x} J(x) = \\frac{1}{2}\\,(2 A^T W A x) - A^T W b + \\frac{1}{2}\\,(2 \\lambda L^T L x)\n$$\n$$\n\\nabla_{x} J(x) = A^T W A x - A^T W b + \\lambda L^T L x\n$$\n将梯度设为零，得到最小化解 $x^{\\star}$ 的条件：\n$$\nA^T W A x^{\\star} + \\lambda L^T L x^{\\star} = A^T W b\n$$\n提出 $x^{\\star}$，我们得到吉洪诺夫正则化WLS的法方程：\n$$\n(A^T W A + \\lambda L^T L) x^{\\star} = A^T W b\n$$\n因此，解为：\n$$\nx^{\\star} = (A^T W A + \\lambda L^T L)^{-1} A^T W b\n$$\n\n现在，我们代入给定的值。首先，我们根据给定的几何结构为 $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ 构建系统矩阵 $A$：\n-   射线1：$1 \\cdot x_1 + 0 \\cdot x_2 = b_1$。这给出 $A$ 的第一行为 $\\begin{bmatrix} 1  0 \\end{bmatrix}$。\n-   射线2：$0 \\cdot x_1 + 1 \\cdot x_2 = b_2$。这给出第二行为 $\\begin{bmatrix} 0  1 \\end{bmatrix}$。\n-   射线3：$1 \\cdot x_1 + 1 \\cdot x_2 = b_3$。这给出第三行为 $\\begin{bmatrix} 1  1 \\end{bmatrix}$。\n所以，系统矩阵为：\n$$\nA = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{pmatrix}\n$$\n数据向量为 $b = \\begin{pmatrix} 0.10 \\\\ 0.20 \\\\ 0.25 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{10} \\\\ \\frac{2}{10} \\\\ \\frac{25}{100} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{10} \\\\ \\frac{1}{5} \\\\ \\frac{1}{4} \\end{pmatrix}$。\n加权矩阵为 $W = I_3$，因此公式简化为：\n$$\nx^{\\star} = (A^T A + \\lambda L^T L)^{-1} A^T b\n$$\n我们计算所需的矩阵：\n$A^T = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix}$\n$A^T A = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{pmatrix} = \\begin{pmatrix} 1+0+1  0+0+1 \\\\ 0+0+1  0+1+1 \\end{pmatrix} = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix}$\n$L = \\begin{bmatrix} 1  -1 \\end{bmatrix}$，所以 $L^T = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$\n$L^T L = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\begin{bmatrix} 1  -1 \\end{bmatrix} = \\begin{pmatrix} 1  -1 \\\\ -1  1 \\end{pmatrix}$\n正则化参数为 $\\lambda = 0.50 = \\frac{1}{2}$。\n现在，我们计算项 $(A^T A + \\lambda L^T L)$：\n$$\nA^T A + \\lambda L^T L = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1  -1 \\\\ -1  1 \\end{pmatrix} = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} + \\begin{pmatrix} \\frac{1}{2}  -\\frac{1}{2} \\\\ -\\frac{1}{2}  \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{2}  \\frac{1}{2} \\\\ \\frac{1}{2}  \\frac{5}{2} \\end{pmatrix}\n$$\n接下来，我们计算它的逆矩阵。行列式为 $\\det(\\cdot) = (\\frac{5}{2})(\\frac{5}{2}) - (\\frac{1}{2})(\\frac{1}{2}) = \\frac{25}{4} - \\frac{1}{4} = \\frac{24}{4} = 6$。\n逆矩阵为：\n$$\n(A^T A + \\lambda L^T L)^{-1} = \\frac{1}{6} \\begin{pmatrix} \\frac{5}{2}  -\\frac{1}{2} \\\\ -\\frac{1}{2}  \\frac{5}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{12}  -\\frac{1}{12} \\\\ -\\frac{1}{12}  \\frac{5}{12} \\end{pmatrix}\n$$\n接下来，我们计算项 $A^T b$：\n$$\nA^T b = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{10} \\\\ \\frac{1}{5} \\\\ \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} 1(\\frac{1}{10}) + 0(\\frac{1}{5}) + 1(\\frac{1}{4}) \\\\ 0(\\frac{1}{10}) + 1(\\frac{1}{5}) + 1(\\frac{1}{4}) \\end{pmatrix} = \\begin{pmatrix} \\frac{2+5}{20} \\\\ \\frac{4+5}{20} \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{20} \\\\ \\frac{9}{20} \\end{pmatrix}\n$$\n最后，我们计算解 $x^{\\star}$：\n$$\nx^{\\star} = (A^T A + \\lambda L^T L)^{-1} (A^T b) = \\begin{pmatrix} \\frac{5}{12}  -\\frac{1}{12} \\\\ -\\frac{1}{12}  \\frac{5}{12} \\end{pmatrix} \\begin{pmatrix} \\frac{7}{20} \\\\ \\frac{9}{20} \\end{pmatrix}\n$$\n$$\nx^{\\star} = \\begin{pmatrix} \\frac{5}{12} \\cdot \\frac{7}{20} - \\frac{1}{12} \\cdot \\frac{9}{20} \\\\ -\\frac{1}{12} \\cdot \\frac{7}{20} + \\frac{5}{12} \\cdot \\frac{9}{20} \\end{pmatrix} = \\begin{pmatrix} \\frac{35 - 9}{240} \\\\ \\frac{-7 + 45}{240} \\end{pmatrix} = \\begin{pmatrix} \\frac{26}{240} \\\\ \\frac{38}{240} \\end{pmatrix}\n$$\n化简分数：\n$$\nx^{\\star} = \\begin{pmatrix} x_1^{\\star} \\\\ x_2^{\\star} \\end{pmatrix} = \\begin{pmatrix} \\frac{13}{120} \\\\ \\frac{19}{120} \\end{pmatrix}\n$$\n重建的衰减系数为 $x_1^{\\star} = \\frac{13}{120}\\,\\mathrm{cm}^{-1}$ 和 $x_2^{\\star} = \\frac{19}{120}\\,\\mathrm{cm}^{-1}$。", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{13}{120}  \\frac{19}{120} \\end{pmatrix}}\n$$", "id": "4895893"}, {"introduction": "对于大规模的真实CT系统，直接解析求解是不现实的，因此必须采用迭代方法。本练习将引导您实现并比较两种经典的迭代算法——代数重建技术（ART）和应用于正规方程的高斯-赛德尔（Gauss-Seidel）方法。通过这个过程，您能深入理解它们在处理含噪声数据时的不同表现、收敛特性和实际应用中的权衡 [@problem_id:3135124]。", "problem": "您正在对计算机断层扫描（CT）中的迭代图像重建进行建模，这是一个由线性系统 $A x = b$ 描述的线性逆问题，其中 $A \\in \\mathbb{R}^{m \\times n}$ 是投影（系统）矩阵，$x \\in \\mathbb{R}^{n}$ 是未知图像向量（例如，像素值），$b \\in \\mathbb{R}^{m}$ 是测量的线积分。在存在测量噪声的情况下，该系统通常是不相容的，因此需要寻找一个能在收敛性和噪声放大之间取得平衡的最小二乘解。任务是从基本原理出发，设计并实现两种迭代求解器：应用于正规方程组的高斯-赛德尔迭代法和代数重建技术（ART）。计算机断层扫描（CT）和代数重建技术（ART）在首次使用时必须进行定义。计算机断层扫描（CT）是一种利用X射线投影重建横截面图像的成像模式。代数重建技术（ART）是一种迭代方法，通过将图像依次投影到由线性方程定义的超平面上来更新图像。\n\n从最小二乘解是最小化平方误差目标函数 $$\\min_{x \\in \\mathbb{R}^{n}} \\ \\lVert A x - b \\rVert_2^2,$$ 并且其驻点满足正规方程组 $$A^{\\mathsf{T}} A x = A^{\\mathsf{T}} b$$ 这一基本出发点开始。对于ART，利用每个线性方程 $a_i^{\\mathsf{T}} x = b_i$ 在 $\\mathbb{R}^{n}$ 中定义一个超平面这一几何事实，通过向这些超平面进行序贯正交投影来驱动迭代解趋向可行解。通过将系数矩阵分解为下三角和上三角部分，并要求新迭代解的每个分量都利用最新的可用分量精确满足相应的标量方程，来推导用于正规方程组的高斯-赛德尔迭代法。通过使用内积和欧几里得范数表示对一个超平面的正交投影，来推导ART的更新规则。不要使用未从这些原理推导出的简化公式。\n\n为以下合成的CT测试套件实现这两种方法，以从 $A$ 和 $b$ 重建 $x$。该系统模拟一个 $2 \\times 2$ 像素的图像（$n = 4$），包含 $m = 6$ 个射线和：两行、两列和两条对角线。像素按 $[x_1, x_2, x_3, x_4]^{\\mathsf{T}}$ 排序，对应位置 $[(1,1), (1,2), (2,1), (2,2)]$。每个测试用例的投影矩阵和数据如下：\n\n- 测试用例1（理想情况，相容且良态）：\n  $$A_1 = \\begin{bmatrix}\n  1  1  0  0 \\\\\n  0  0  1  1 \\\\\n  1  0  1  0 \\\\\n  0  1  0  1 \\\\\n  1  0  0  1 \\\\\n  0  1  1  0\n  \\end{bmatrix}, \\quad x_{\\text{true},1} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix}, \\quad \\eta_1 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad b_1 = A_1 x_{\\text{true},1} + \\eta_1.$$\n  在正规方程组上使用 $N_{\\mathrm{GS},1} = 20$ 次高斯-赛德尔迭代，并使用松弛参数 $\\lambda_1 = 1.0$ 对所有行进行 $N_{\\mathrm{ART},1} = 20$ 次ART的完整扫描。\n\n- 测试用例2（中度噪声，相同几何结构）：\n  $$A_2 = A_1, \\quad x_{\\text{true},2} = \\begin{bmatrix} 1 \\\\ 0.5 \\\\ 1.5 \\\\ 1 \\end{bmatrix}, \\quad \\eta_2 = \\begin{bmatrix} 0.02 \\\\ -0.015 \\\\ 0.01 \\\\ -0.005 \\\\ 0.0 \\\\ 0.025 \\end{bmatrix}, \\quad b_2 = A_2 x_{\\text{true},2} + \\eta_2.$$\n  使用 $N_{\\mathrm{GS},2} = 40$ 次高斯-赛德尔迭代，并使用松弛参数 $\\lambda_2 = 1.0$ 进行 $N_{\\mathrm{ART},2} = 40$ 次完整扫描。\n\n- 测试用例3（近冗余射线，较高噪声，测试稳定性）：\n  $$A_3 = \\begin{bmatrix}\n  1  1  0  0 \\\\\n  0  0  1  1 \\\\\n  1  0  1  0 \\\\\n  0  1  0  1 \\\\\n  0.99  0.99  0  0 \\\\\n  0  0  1.01  1.01\n  \\end{bmatrix}, \\quad x_{\\text{true},3} = \\begin{bmatrix} 1.2 \\\\ 0.8 \\\\ 1.0 \\\\ 1.5 \\end{bmatrix}, \\quad \\eta_3 = \\begin{bmatrix} 0.2 \\\\ -0.15 \\\\ 0.1 \\\\ -0.05 \\\\ 0.0 \\\\ 0.25 \\end{bmatrix}, \\quad b_3 = A_3 x_{\\text{true},3} + \\eta_3.$$\n  使用 $N_{\\mathrm{GS},3} = 80$ 次高斯-赛德尔迭代，并使用松弛参数 $\\lambda_3 = 1.2$ 进行 $N_{\\mathrm{ART},3} = 80$ 次完整扫描。\n\n对于所有情况，使用零向量 $x^{(0)} = \\mathbf{0}$ 作为初始猜测。为每个测试用例，计算并报告两种方法的以下可量化指标：\n- 数据空间中的最终残差范数，定义为 $$\\rho = \\lVert A x_{\\text{final}} - b \\rVert_2,$$ 以浮点数形式报告。\n- 噪声放大因子，定义为 $$\\alpha = \\frac{\\lVert x_{\\text{final}} - x_{\\text{true}} \\rVert_2}{\\max\\big(\\lVert \\eta \\rVert_2, \\varepsilon\\big)},$$ 其中 $\\varepsilon = 10^{-12}$ 是一个小的稳定项。对于无噪声数据（即 $\\lVert \\eta \\rVert_2 = 0$），根据定义设置 $\\alpha = 0$。以浮点数形式报告 $\\alpha$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表必须按\n$$[\\rho_{\\mathrm{GS},1}, \\ \\rho_{\\mathrm{ART},1}, \\ \\alpha_{\\mathrm{GS},1}, \\ \\alpha_{\\mathrm{ART},1}, \\ \\rho_{\\mathrm{GS},2}, \\ \\rho_{\\mathrm{ART},2}, \\ \\alpha_{\\mathrm{GS},2}, \\ \\alpha_{\\mathrm{ART},2}, \\ \\rho_{\\mathrm{GS},3}, \\ \\rho_{\\mathrm{ART},3}, \\ \\alpha_{\\mathrm{GS},3}, \\ \\alpha_{\\mathrm{ART},3}],$$\n的顺序排列，每个浮点数四舍五入到6位小数。本问题不涉及物理单位。未使用角度。不得使用百分比；只应打印指定的浮点数。", "solution": "该问题要求推导并实现两种迭代算法——应用于正规方程组的高斯-赛德尔法和代数重建技术（ART），以求解模拟简化计算机断层扫描（CT）重建问题的线性系统 $A x = b$。计算机断层扫描（CT）是一种利用X射线投影重建横截面图像的成像模式。向量$x$表示未知的像素值，$A$是将像素值映射到线积分的系统矩阵，$b$是测量的线积分向量，可能受到噪声$\\eta$的干扰。\n\n解决方法是首先按照指定的基本原理推导出两种方法的迭代更新规则。然后，将实现这些方法并将其应用于三个不同的测试用例，以根据最终残差范数$\\rho$和噪声放大因子$\\alpha$来评估其性能。\n\n首先，我们建立数学基础。对于一个不相容的系统 $A x = b$，我们寻求一个最小二乘解，该解能最小化残差的平方欧几里得范数，由以下目标函数定义：\n$$ \\min_{x \\in \\mathbb{R}^{n}} \\ \\lVert A x - b \\rVert_2^2 $$\n该目标函数的最小值在关于$x$的梯度为零的驻点处取得。目标函数可以写成 $f(x) = (Ax-b)^{\\mathsf{T}}(Ax-b) = x^{\\mathsf{T}}A^{\\mathsf{T}}Ax - 2b^{\\mathsf{T}}Ax + b^{\\mathsf{T}}b$。梯度为 $\\nabla_x f(x) = 2A^{\\mathsf{T}}Ax - 2A^{\\mathsf{T}}b$。将梯度设为零，得到正规方程组：\n$$ A^{\\mathsf{T}} A x = A^{\\mathsf{T}} b $$\n这就构成了一个对称半正定线性系统，我们将其记为 $Cx = d$，其中 $C = A^{\\mathsf{T}}A$ 且 $d = A^{\\mathsf{T}}b$。如果$A$具有满列秩，$C$就是正定的。\n\n**正规方程组的高斯-赛德尔法推导**\n\n高斯-赛德尔法是求解方形线性系统$Cx=d$的一种迭代技术。它通过首先将矩阵$C$分解为其对角（$D$）、严格下三角（$L$）和严格上三角（$U$）部分来推导，使得$C = L + D + U$。该系统$Cx = d$可以写成：\n$$ (L + D + U)x = d $$\n该迭代方法生成一个向量序列$x^{(k)}$，理想情况下它会收敛到真实解$x$。高斯-赛德尔的核心思想是重排方程，利用最新计算出的值来求解下一个迭代解$x^{(k+1)}$：\n$$ (L + D) x^{(k+1)} = d - U x^{(k)} $$\n这可以用分量形式表示。对于系统中的第$j$个方程，我们有：\n$$ \\sum_{i=1}^{n} C_{ji} x_i = d_j $$\n展开此和式可得：\n$$ \\sum_{i=1}^{j-1} C_{ji} x_i + C_{jj} x_j + \\sum_{i=j+1}^{n} C_{ji} x_i = d_j $$\n高斯-赛德尔法在第$k+1$次迭代中更新解向量的第$j$个分量$x_j$时，使用了当前迭代中已经更新的分量$x_1^{(k+1)}, \\dots, x_{j-1}^{(k+1)}$以及上一次迭代中的旧分量$x_{j+1}^{(k)}, \\dots, x_n^{(k)}$。我们求解$x_j^{(k+1)}$：\n$$ C_{jj} x_j^{(k+1)} = d_j - \\sum_{i=1}^{j-1} C_{ji} x_i^{(k+1)} - \\sum_{i=j+1}^{n} C_{ji} x_i^{(k)} $$\n第$k+1$次迭代中第$j$个分量的最终更新规则是：\n$$ x_j^{(k+1)} = \\frac{1}{C_{jj}} \\left( d_j - \\sum_{i=1}^{j-1} C_{ji} x_i^{(k+1)} - \\sum_{i=j+1}^{n} C_{ji} x_i^{(k)} \\right) $$\n对$j = 1, \\dots, n$执行此迭代，以完成从$x^{(k)}$到$x^{(k+1)}$的一次完整更新。该过程从一个初始猜测$x^{(0)}$（在本问题中为零向量）开始，并重复指定的迭代次数。为使此方法收敛，矩阵$C$必须是对称正定或严格对角占优的。由于$C=A^{\\mathsf{T}}A$，并且所提供的矩阵$A$在前两种情况下具有满列秩，在第三种情况下也接近满列秩，因此$C$是对称正定的，从而保证了收敛性。\n\n**代数重建技术（ART）的推导**\n\n代数重建技术（ART）是Kaczmarz方法的一种特定应用，它逐行处理原始系统$Ax=b$。代数重建技术（ART）是一种迭代方法，通过将图像依次投影到由线性方程定义的超平面上来更新图像。系统的每一行，$a_i^{\\mathsf{T}} x = b_i$（对于$i=1, \\dots, m$），在$\\mathbb{R}^n$中定义了一个超平面$H_i$。其目标是将当前的解估计值迭代地投影到这些超平面上。\n\n设$x^{(k)}$为当前估计值。我们希望通过将$x^{(k)}$正交投影到超平面$H_i$上来找到下一个估计值$x^{(k+1)}$。连接$x^{(k)}$与其在$H_i$上投影的向量必须与超平面的法向量$a_i$平行。因此，更新具有以下形式：\n$$ x^{(k+1)} = x^{(k)} + c \\cdot a_i $$\n对于某个标量$c$。由于$x^{(k+1)}$必须位于超平面$H_i$上，它必须满足方程$a_i^{\\mathsf{T}} x^{(k+1)} = b_i$。将更新形式代入该方程可得：\n$$ a_i^{\\mathsf{T}} (x^{(k)} + c \\cdot a_i) = b_i $$\n$$ a_i^{\\mathsf{T}} x^{(k)} + c \\cdot (a_i^{\\mathsf{T}} a_i) = b_i $$\n项$a_i^{\\mathsf{T}} a_i$是向量$a_i$的欧几里得范数的平方，记作$\\lVert a_i \\rVert_2^2$。求解$c$：\n$$ c \\cdot \\lVert a_i \\rVert_2^2 = b_i - a_i^{\\mathsf{T}} x^{(k)} $$\n$$ c = \\frac{b_i - a_i^{\\mathsf{T}} x^{(k)}}{\\lVert a_i \\rVert_2^2} $$\n这里假设$\\lVert a_i \\rVert_2 \\neq 0$，对于给定的系统矩阵这是成立的。将$c$代回更新形式，得到正交投影更新：\n$$ x^{(k+1)} = x^{(k)} + \\frac{b_i - a_i^{\\mathsf{T}} x^{(k)}}{\\lVert a_i \\rVert_2^2} a_i $$\n问题要求使用带松弛参数$\\lambda$的版本。该参数控制投影的步长，其中$\\lambda=1$对应于完全正交投影。广义更新规则为：\n$$ x^{(k+1)} = x^{(k)} + \\lambda \\frac{b_i - a_i^{\\mathsf{T}} x^{(k)}}{\\lVert a_i \\rVert_2^2} a_i $$\nART的一次完整扫描包括对每一行$i=1, \\dots, m$依次应用此更新。也就是说，投影到$H_i$上的结果成为投影到$H_{i+1}$上的起始点。\n\n**实现与度量指标**\n\n将实现的两种推导方法应用于三个测试用例。对于每种方法和每个用例，使用最终解向量$x_{\\text{final}}$计算两个度量指标：\n1. 最终残差范数：$\\rho = \\lVert A x_{\\text{final}} - b \\rVert_2$。\n2. 噪声放大因子：$\\alpha = \\frac{\\lVert x_{\\text{final}} - x_{\\text{true}} \\rVert_2}{\\max\\big(\\lVert \\eta \\rVert_2, \\varepsilon\\big)}$，其中$\\varepsilon = 10^{-12}$。根据给定定义，如果$\\lVert \\eta \\rVert_2=0$，则$\\alpha = 0$。\n\n实现将遵循推导出的公式，从初始猜测$x^{(0)} = \\mathbf{0}$开始，进行指定次数的迭代或扫描。然后收集最终结果并按要求格式化。", "answer": "```python\nimport numpy as np\n\ndef run_gauss_seidel(A, b, n_iters):\n    \"\"\"\n    Solves the normal equations Ax=b using the Gauss-Seidel method.\n    The system being solved is (A^T A)x = (A^T b).\n    \"\"\"\n    m, n = A.shape\n    x_gs = np.zeros(n)\n    \n    C = A.T @ A\n    d = A.T @ b\n    \n    for _ in range(n_iters):\n        for j in range(n):\n            # Sum for already updated components in this iteration\n            sum_new = C[j, :j] @ x_gs[:j]\n            # Sum for components from the previous iteration\n            sum_old = C[j, j+1:] @ x_gs[j+1:]\n            \n            # Avoid division by zero, though C[j, j] should be positive\n            diag_element = C[j, j]\n            if abs(diag_element)  1e-12:\n                diag_element = 1e-12\n\n            x_gs[j] = (d[j] - sum_new - sum_old) / diag_element\n            \n    return x_gs\n\ndef run_art(A, b, n_sweeps, lam):\n    \"\"\"\n    Solves Ax=b using the Algebraic Reconstruction Technique (ART).\n    \"\"\"\n    m, n = A.shape\n    x_art = np.zeros(n)\n    \n    # Pre-calculate the squared L2 norm of each row of A\n    a_row_norm_sq = np.sum(A**2, axis=1)\n    \n    for _ in range(n_sweeps):\n        for i in range(m):\n            a_i = A[i, :]\n            b_i = b[i]\n            \n            norm_sq = a_row_norm_sq[i]\n            if norm_sq  1e-12:\n                # This row is all zeros, no information, skip update\n                continue\n            \n            # ART update rule\n            current_projection = a_i @ x_art\n            update_factor = lam * (b_i - current_projection) / norm_sq\n            x_art = x_art + update_factor * a_i\n            \n    return x_art\n\ndef calculate_alpha(x_final, x_true, eta):\n    \"\"\"\n    Calculates the noise amplification factor alpha.\n    \"\"\"\n    eps = 1e-12\n    norm_eta = np.linalg.norm(eta)\n    \n    if norm_eta == 0:\n        return 0.0\n    \n    error_norm = np.linalg.norm(x_final - x_true)\n    alpha = error_norm / max(norm_eta, eps)\n    return alpha\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # Test Case 1\n    A1 = np.array([\n        [1, 1, 0, 0],\n        [0, 0, 1, 1],\n        [1, 0, 1, 0],\n        [0, 1, 0, 1],\n        [1, 0, 0, 1],\n        [0, 1, 1, 0]\n    ], dtype=float)\n    x_true1 = np.array([1, 2, 3, 4], dtype=float)\n    eta1 = np.zeros(6, dtype=float)\n    N_GS1, N_ART1, lambda1 = 20, 20, 1.0\n\n    # Test Case 2\n    A2 = A1\n    x_true2 = np.array([1, 0.5, 1.5, 1], dtype=float)\n    eta2 = np.array([0.02, -0.015, 0.01, -0.005, 0.0, 0.025], dtype=float)\n    N_GS2, N_ART2, lambda2 = 40, 40, 1.0\n\n    # Test Case 3\n    A3 = np.array([\n        [1.00, 1.00, 0.00, 0.00],\n        [0.00, 0.00, 1.00, 1.00],\n        [1.00, 0.00, 1.00, 0.00],\n        [0.00, 1.00, 0.00, 1.00],\n        [0.99, 0.99, 0.00, 0.00],\n        [0.00, 0.00, 1.01, 1.01]\n    ], dtype=float)\n    x_true3 = np.array([1.2, 0.8, 1.0, 1.5], dtype=float)\n    eta3 = np.array([0.2, -0.15, 0.1, -0.05, 0.0, 0.25], dtype=float)\n    N_GS3, N_ART3, lambda3 = 80, 80, 1.2\n    \n    test_cases = [\n        (A1, x_true1, eta1, N_GS1, N_ART1, lambda1),\n        (A2, x_true2, eta2, N_GS2, N_ART2, lambda2),\n        (A3, x_true3, eta3, N_GS3, N_ART3, lambda3),\n    ]\n\n    results = []\n    for A, x_true, eta, N_GS, N_ART, lam in test_cases:\n        b = A @ x_true + eta\n\n        # Run Gauss-Seidel for Normal Equations\n        x_final_gs = run_gauss_seidel(A, b, N_GS)\n        rho_gs = np.linalg.norm(A @ x_final_gs - b)\n        alpha_gs = calculate_alpha(x_final_gs, x_true, eta)\n\n        # Run ART\n        x_final_art = run_art(A, b, N_ART, lam)\n        rho_art = np.linalg.norm(A @ x_final_art - b)\n        alpha_art = calculate_alpha(x_final_art, x_true, eta)\n\n        results.extend([rho_gs, rho_art, alpha_gs, alpha_art])\n\n    # Format and print the final output\n    print(f\"[{','.join(f'{x:.6f}' for x in results)}]\")\n\nsolve()\n```", "id": "3135124"}, {"introduction": "现代CT重建越来越依赖于能够融入图像先验知识的先进正则化技术，例如稀疏性。本练习深入探讨了受压缩感知启发的重建方法，展示了如何利用 $\\ell_1$ 范数在变换域（如小波变换）中促进稀疏性。您将推导出如迭代收缩阈值算法（ISTA）及其加速版本（FISTA）这类邻近梯度算法，以求解由此产生的非光滑优化问题 [@problem_id:4913497]。", "problem": "在平行束计算机断层扫描 (CT) 系统中，连续 Radon 变换被离散化以得到一个线性正演模型。设未知图像由向量 $\\boldsymbol{x} \\in \\mathbb{R}^{n}$ 表示，系统矩阵 $\\boldsymbol{A} \\in \\mathbb{R}^{m \\times n}$ 编码沿射线的线积分。测量数据为 $\\boldsymbol{b} \\in \\mathbb{R}^{m}$。为了引入图像在标准正交小波基中是稀疏的这一先验知识，考虑一个标准正交小波变换 $\\boldsymbol{W} \\in \\mathbb{R}^{n \\times n}$ (因此 $\\boldsymbol{W}^{\\top}\\boldsymbol{W}=\\boldsymbol{I}$) 和凸优化问题\n$$\n\\min_{\\boldsymbol{x}\\in\\mathbb{R}^{n}} \\ \\frac{1}{2}\\|\\boldsymbol{A}\\boldsymbol{x}-\\boldsymbol{b}\\|_{2}^{2} \\ + \\ \\lambda \\|\\boldsymbol{W}\\boldsymbol{x}\\|_{1},\n$$\n其中 $\\lambda0$ 是一个正则化参数。\n\n仅从以下基本依据出发：\n(i) 断层扫描的线性正演模型，\n(ii) 光滑函数梯度的定义，以及\n(iii) 闭的、正常的、凸函数的近端算子的定义，\n完成以下任务：\n\n1) 解释为什么项 $\\lambda\\|\\boldsymbol{W}\\boldsymbol{x}\\|_{1}$ 能促进小波系数的稀疏性，以及这与在 CT 重建中抑制类噪声结构有何关系。\n\n2) 通过将近端梯度下降应用于光滑的数据保真项和非光滑的小波 $\\ell_{1}$ 项，推导上述问题的迭代收缩阈值算法 (ISTA; Iterative Shrinkage-Thresholding Algorithm) 的更新步骤。假设步长选择为 $t \\in (0, 2/L)$，其中 $L$ 是数据保真项梯度的 Lipschitz 常数。\n\n3) 推导此问题的快速迭代收缩阈值算法 (FISTA; Fast Iterative Shrinkage-Thresholding Algorithm) 加速，并清楚地指明动量序列和外推迭代值。\n\n4) 对于以下具体的二维测试用例，评估推导出的更新：\n$$\n\\boldsymbol{A}=\\begin{pmatrix}1  0 \\\\ 0  1\\end{pmatrix}, \\quad\n\\boldsymbol{W}=\\frac{1}{\\sqrt{2}}\\begin{pmatrix}1  1 \\\\ 1  -1\\end{pmatrix}, \\quad\n\\lambda=\\frac{3}{5}, \\quad \\boldsymbol{x}^{0}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix},\n$$\n和\n$$\n\\boldsymbol{b}=\\boldsymbol{W}\\begin{pmatrix}1 \\\\ \\frac{1}{5}\\end{pmatrix}.\n$$\n使用经典选择 $t=1/L$，其中 $L$ 是此情况下数据保真项梯度的 Lipschitz 常数。从 $\\boldsymbol{x}^{0}$ 开始，计算第一个 ISTA 迭代结果 $\\boldsymbol{x}^{1}_{\\text{ISTA}}$ 和第二个 FISTA 迭代结果 $\\boldsymbol{x}^{2}_{\\text{FISTA}}$。将最终数值结果表示为一个包含四个条目 $\\big(x^{1}_{\\text{ISTA},1}, x^{1}_{\\text{ISTA},2}, x^{2}_{\\text{FISTA},1}, x^{2}_{\\text{FISTA},2}\\big)$ 的单行向量。无需四舍五入；报告精确值。", "solution": "该问题是适定的，具有科学依据，并包含唯一解所需的所有信息。该公式是计算成像和逆问题领域的标准公式。所有常数和矩阵都已正确定义，任务在程序上是清晰的。因此，该问题被认为是有效的。\n\n优化问题是\n$$\n\\min_{\\boldsymbol{x}\\in\\mathbb{R}^{n}} \\ F(\\boldsymbol{x}) \\quad \\text{其中} \\quad F(\\boldsymbol{x}) = \\underbrace{\\frac{1}{2}\\|\\boldsymbol{A}\\boldsymbol{x}-\\boldsymbol{b}\\|_{2}^{2}}_{f(\\boldsymbol{x})} \\ + \\ \\underbrace{\\lambda \\|\\boldsymbol{W}\\boldsymbol{x}\\|_{1}}_{g(\\boldsymbol{x})}.\n$$\n这是一个复合凸优化问题，其中 $f(\\boldsymbol{x})$ 是一个光滑的、凸的数据保真项，而 $g(\\boldsymbol{x})$ 是一个非光滑的、凸的正则化项。\n\n**1) 稀疏性促进与噪声抑制**\n\n项 $\\lambda\\|\\boldsymbol{W}\\boldsymbol{x}\\|_{1}$ 作为一个正则化惩罚项。向量 $\\boldsymbol{\\alpha} = \\boldsymbol{W}\\boldsymbol{x}$ 表示图像 $\\boldsymbol{x}$ 在标准正交小波基 $\\boldsymbol{W}$ 中的系数。$\\ell_1$-范数 $\\|\\boldsymbol{\\alpha}\\|_1 = \\sum_{i} |\\alpha_i|$ 被最小化，并由正则化参数 $\\lambda  0$ 缩放。\n\n$\\ell_1$-范数是衡量向量中非零元素个数的非凸 $\\ell_0$ 伪范数的一个凸代理。众所周知，最小化 $\\ell_1$-范数可以促进稀疏性，这意味着它鼓励许多系数 $\\alpha_i$ 精确为零的解。从几何上看，$\\ell_1$-范数的水平集是超菱形（在 $\\mathbb{R}^2$ 中是旋转了 $45^\\circ$ 的正方形），其尖锐的“角”位于坐标轴上。在寻求总目标函数 $F(\\boldsymbol{x})$ 的最小值时，解通常在这些角点上找到，在这些角点上一个或多个系数为零。\n\n在 CT 成像的背景下，由于两个经验事实，这一点非常有效：\ni. 自然图像虽然在像素域中不稀疏，但在小波基中通常是稀疏或“可压缩”的。这意味着它们的结构可以用少量的大幅度小波系数精确表示，而其余的则接近于零。\nii. CT 中的测量噪声在转换到小波域时，其能量倾向于或多或少均匀地分布在所有系数上，从而产生大量的小幅度系数。\n\n通过最小化小波系数的 $\\ell_1$-范数，优化过程优先将小系数收缩到零，同时保留大系数。这样做可以消除噪声的贡献（表现为小系数），同时保留图像的基本结构信息（由大系数捕获）。因此，项 $\\lambda\\|\\boldsymbol{W}\\boldsymbol{x}\\|_{1}$ 作为一个强大的先验，可以抑制类噪声结构，并鼓励与自然图像预期特征一致的重建。\n\n**2) 迭代收缩阈值算法 (ISTA) 的推导**\n\nISTA 是近端梯度下降算法的一个实例，专为形如 $\\min_{\\boldsymbol{x}} f(\\boldsymbol{x}) + g(\\boldsymbol{x})$ 的问题而设计。迭代更新为：\n$$\n\\boldsymbol{x}^{k+1} = \\text{prox}_{t g}(\\boldsymbol{x}^k - t \\nabla f(\\boldsymbol{x}^k)),\n$$\n其中 $t  0$ 是步长。\n\n首先，我们计算光滑项 $f(\\boldsymbol{x}) = \\frac{1}{2}\\|\\boldsymbol{A}\\boldsymbol{x}-\\boldsymbol{b}\\|_{2}^{2} = \\frac{1}{2}(\\boldsymbol{A}\\boldsymbol{x}-\\boldsymbol{b})^{\\top}(\\boldsymbol{A}\\boldsymbol{x}-\\boldsymbol{b})$ 的梯度。\n$$\n\\nabla f(\\boldsymbol{x}) = \\boldsymbol{A}^{\\top}(\\boldsymbol{A}\\boldsymbol{x}-\\boldsymbol{b}).\n$$\n对 $f(\\boldsymbol{x})$ 的梯度下降步骤是 $\\boldsymbol{x}^k - t \\nabla f(\\boldsymbol{x}^k)$。\n\n接下来，我们推导 $g(\\boldsymbol{x}) = \\lambda\\|\\boldsymbol{W}\\boldsymbol{x}\\|_1$ 的近端算子。近端算子定义为：\n$$\n\\text{prox}_{t g}(\\boldsymbol{y}) = \\arg\\min_{\\boldsymbol{z} \\in \\mathbb{R}^n} \\left( \\frac{1}{2}\\|\\boldsymbol{z}-\\boldsymbol{y}\\|_{2}^{2} + t\\lambda\\|\\boldsymbol{W}\\boldsymbol{z}\\|_1 \\right).\n$$\n由于 $\\boldsymbol{W}$ 是一个标准正交矩阵，$\\boldsymbol{W}^{\\top}\\boldsymbol{W} = \\boldsymbol{I}$，并且它保持欧几里得范数，即 $\\|\\boldsymbol{u}\\|_2 = \\|\\boldsymbol{W}\\boldsymbol{u}\\|_2$。设 $\\boldsymbol{\\alpha} = \\boldsymbol{W}\\boldsymbol{z}$ 和 $\\boldsymbol{\\beta} = \\boldsymbol{W}\\boldsymbol{y}$。那么 $\\boldsymbol{z} = \\boldsymbol{W}^{\\top}\\boldsymbol{\\alpha}$，且 $\\|\\boldsymbol{z}-\\boldsymbol{y}\\|_2^2 = \\|\\boldsymbol{W}^{\\top}(\\boldsymbol{\\alpha}-\\boldsymbol{\\beta})\\|_2^2 = \\|\\boldsymbol{\\alpha}-\\boldsymbol{\\beta}\\|_2^2$。最小化问题可以改写为关于 $\\boldsymbol{\\alpha}$ 的形式：\n$$\n\\arg\\min_{\\boldsymbol{\\alpha} \\in \\mathbb{R}^n} \\left( \\frac{1}{2}\\|\\boldsymbol{\\alpha}-\\boldsymbol{\\beta}\\|_2}^{2} + t\\lambda\\|\\boldsymbol{\\alpha}\\|_1 \\right).\n$$\n这是 $\\ell_1$-范数的近端算子的定义，即软阈值算子 $S_{t\\lambda}(\\cdot)$。解为 $\\boldsymbol{\\alpha}^{*} = S_{t\\lambda}(\\boldsymbol{\\beta})$。软阈值算子是逐元素应用的：$(S_{\\tau}(u))_i = \\text{sign}(u_i)\\max(|u_i|-\\tau, 0)$。\n\n转换回原始变量 $\\boldsymbol{z}$，我们得到 $\\boldsymbol{z}^{*} = \\boldsymbol{W}^{\\top}\\boldsymbol{\\alpha}^{*} = \\boldsymbol{W}^{\\top}S_{t\\lambda}(\\boldsymbol{\\beta}) = \\boldsymbol{W}^{\\top}S_{t\\lambda}(\\boldsymbol{W}\\boldsymbol{y})$。\n结合这些部分，ISTA 更新是一个两步过程：\n1. 梯度步：$\\boldsymbol{y}^k = \\boldsymbol{x}^k - t \\boldsymbol{A}^{\\top}(\\boldsymbol{A}\\boldsymbol{x}^k-\\boldsymbol{b})$\n2. 近端步：$\\boldsymbol{x}^{k+1} = \\boldsymbol{W}^{\\top}S_{t\\lambda}(\\boldsymbol{W}\\boldsymbol{y}^k)$\n\n**3) 快速迭代收缩阈值算法 (FISTA) 的推导**\n\nFISTA 通过引入一个动量项来加速 ISTA。它维持一个外推点 $\\boldsymbol{z}^k$ 的辅助序列。算法流程如下：\n初始化：选择 $\\boldsymbol{x}^0 \\in \\mathbb{R}^n$，设置 $\\boldsymbol{z}^0 = \\boldsymbol{x}^0$ 和 $\\theta_0=1$。\n对于 $k=0, 1, 2, \\dots$：\n1. 在外推点 $\\boldsymbol{z}^k$ 处执行梯度和近端步骤：\n   a. 梯度步：$\\boldsymbol{y}^k = \\boldsymbol{z}^k - t_k \\boldsymbol{A}^{\\top}(\\boldsymbol{A}\\boldsymbol{z}^k - \\boldsymbol{b})$\n   b. 近端步：$\\boldsymbol{x}^{k+1} = \\boldsymbol{W}^{\\top}S_{t_k\\lambda}(\\boldsymbol{W}\\boldsymbol{y}^k)$\n2. 更新动量参数 $\\theta_k$：\n   $$\n   \\theta_{k+1} = \\frac{1 + \\sqrt{1 + 4\\theta_k^2}}{2}\n   $$\n3. 通过当前和先前迭代值的线性组合计算下一个外推点 $\\boldsymbol{z}^{k+1}$：\n   $$\n   \\boldsymbol{z}^{k+1} = \\boldsymbol{x}^{k+1} + \\frac{\\theta_k - 1}{\\theta_{k+1}}(\\boldsymbol{x}^{k+1} - \\boldsymbol{x}^k)\n   $$\n步长 $t_k$ 通常是常数，$t_k=t \\in (0, 2/L)$，其中 $L$ 是 $\\nabla f$ 的 Lipschitz 常数。\n\n**4) 具体测试用例的评估**\n\n我们已知：\n$\\boldsymbol{A}=\\begin{pmatrix}1  0 \\\\ 0  1\\end{pmatrix} = \\boldsymbol{I}$，\n$\\boldsymbol{W}=\\frac{1}{\\sqrt{2}}\\begin{pmatrix}1  1 \\\\ 1  -1\\end{pmatrix}$，\n$\\lambda=\\frac{3}{5}$，\n$\\boldsymbol{x}^{0}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$，\n和 $\\boldsymbol{b}=\\boldsymbol{W}\\begin{pmatrix}1 \\\\ \\frac{1}{5}\\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1  1 \\\\ 1  -1\\end{pmatrix}\\begin{pmatrix}1 \\\\ \\frac{1}{5}\\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1+\\frac{1}{5} \\\\ 1-\\frac{1}{5}\\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}\\frac{6}{5} \\\\ \\frac{4}{5}\\end{pmatrix}$。\n\n$\\nabla f(\\boldsymbol{x})=\\boldsymbol{A}^{\\top}(\\boldsymbol{A}\\boldsymbol{x}-\\boldsymbol{b})$ 的 Lipschitz 常数是 $L = \\|\\boldsymbol{A}^{\\top}\\boldsymbol{A}\\|_2$。这里，$\\boldsymbol{A}=\\boldsymbol{I}$，所以 $\\boldsymbol{A}^{\\top}\\boldsymbol{A} = \\boldsymbol{I}^{\\top}\\boldsymbol{I} = \\boldsymbol{I}$。单位矩阵的谱范数为 1，所以 $L=1$。\n给定的步长是 $t=1/L = 1/1 = 1$。\n软阈值参数为 $\\tau = t\\lambda = 1 \\cdot \\frac{3}{5} = \\frac{3}{5}$。\n\n注意到 $\\boldsymbol{W}$ 是对称的 ($\\boldsymbol{W}^{\\top}=\\boldsymbol{W}$) 并且也满足 $\\boldsymbol{W}^2 = \\frac{1}{2}\\begin{pmatrix}1  1 \\\\ 1  -1\\end{pmatrix}\\begin{pmatrix}1  1 \\\\ 1  -1\\end{pmatrix} = \\frac{1}{2}\\begin{pmatrix}2  0 \\\\ 0  2\\end{pmatrix} = \\boldsymbol{I}$。\n\n**$\\boldsymbol{x}^{1}_{\\text{ISTA}}$ 的计算：**\n从 $\\boldsymbol{x}^0 = \\begin{pmatrix}0\\\\0\\end{pmatrix}$ 开始。\n1. 梯度步：\n   $\\boldsymbol{y}^0 = \\boldsymbol{x}^0 - t \\boldsymbol{A}^{\\top}(\\boldsymbol{A}\\boldsymbol{x}^0 - \\boldsymbol{b}) = \\begin{pmatrix}0\\\\0\\end{pmatrix} - 1 \\cdot \\boldsymbol{I}^{\\top}(\\boldsymbol{I}\\begin{pmatrix}0\\\\0\\end{pmatrix} - \\boldsymbol{b}) = -(-\\boldsymbol{b}) = \\boldsymbol{b}$。\n2. 近端步：\n   $\\boldsymbol{x}^1_{\\text{ISTA}} = \\boldsymbol{W}^{\\top}S_{\\tau}(\\boldsymbol{W}\\boldsymbol{y}^0) = \\boldsymbol{W}S_{3/5}(\\boldsymbol{W}\\boldsymbol{b})$。\n   首先，我们计算阈值函数的参数：\n   $\\boldsymbol{W}\\boldsymbol{b} = \\boldsymbol{W}\\left(\\boldsymbol{W}\\begin{pmatrix}1 \\\\ 1/5\\end{pmatrix}\\right) = \\boldsymbol{W}^2 \\begin{pmatrix}1 \\\\ 1/5\\end{pmatrix} = \\boldsymbol{I}\\begin{pmatrix}1 \\\\ 1/5\\end{pmatrix} = \\begin{pmatrix}1 \\\\ 1/5\\end{pmatrix}$。\n   接下来，应用软阈值 $\\tau=3/5$：\n   $S_{3/5}\\begin{pmatrix}1 \\\\ 1/5\\end{pmatrix} = \\begin{pmatrix} \\text{sign}(1)\\max(|1|-\\frac{3}{5}, 0) \\\\ \\text{sign}(\\frac{1}{5})\\max(|\\frac{1}{5}|-\\frac{3}{5}, 0) \\end{pmatrix} = \\begin{pmatrix} \\max(\\frac{2}{5}, 0) \\\\ \\max(-\\frac{2}{5}, 0) \\end{pmatrix} = \\begin{pmatrix}\\frac{2}{5} \\\\ 0\\end{pmatrix}$。\n   最后，变换回来：\n   $\\boldsymbol{x}^1_{\\text{ISTA}} = \\boldsymbol{W}\\begin{pmatrix}\\frac{2}{5} \\\\ 0\\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1  1 \\\\ 1  -1\\end{pmatrix}\\begin{pmatrix}\\frac{2}{5} \\\\ 0\\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}\\frac{2}{5} \\\\ \\frac{2}{5}\\end{pmatrix} = \\frac{2}{5\\sqrt{2}}\\begin{pmatrix}1 \\\\ 1\\end{pmatrix} = \\frac{\\sqrt{2}}{5}\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$。\n   所以，$\\boldsymbol{x}^{1}_{\\text{ISTA}} = \\begin{pmatrix} \\frac{\\sqrt{2}}{5} \\\\ \\frac{\\sqrt{2}}{5} \\end{pmatrix}$。\n\n**$\\boldsymbol{x}^{2}_{\\text{FISTA}}$ 的计算：**\n初始化：$\\boldsymbol{x}^0 = \\begin{pmatrix}0\\\\0\\end{pmatrix}$，$\\boldsymbol{z}^0 = \\boldsymbol{x}^0$，$\\theta_0=1$。步长 $t=1$。\n\n*迭代 1 (k=0):*\n1. 在 $\\boldsymbol{z}^0$ 处的梯度和近端步骤：\n   $\\boldsymbol{y}^0 = \\boldsymbol{z}^0 - t\\boldsymbol{A}^{\\top}(\\boldsymbol{A}\\boldsymbol{z}^0-\\boldsymbol{b}) = \\begin{pmatrix}0\\\\0\\end{pmatrix} - 1\\cdot(\\begin{pmatrix}0\\\\0\\end{pmatrix}-\\boldsymbol{b}) = \\boldsymbol{b}$。\n   $\\boldsymbol{x}^1 = \\boldsymbol{W}^{\\top}S_{\\tau}(\\boldsymbol{W}\\boldsymbol{y}^0) = \\boldsymbol{W}S_{3/5}(\\boldsymbol{W}\\boldsymbol{b}) = \\boldsymbol{x}^1_{\\text{ISTA}} = \\frac{\\sqrt{2}}{5}\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$。\n2. 更新 $\\theta_0$：\n   $\\theta_1 = \\frac{1+\\sqrt{1+4\\theta_0^2}}{2} = \\frac{1+\\sqrt{5}}{2}$。\n3. 更新外推点：\n   $\\boldsymbol{z}^1 = \\boldsymbol{x}^1 + \\frac{\\theta_0-1}{\\theta_1}(\\boldsymbol{x}^1-\\boldsymbol{x}^0) = \\boldsymbol{x}^1 + \\frac{1-1}{\\theta_1}(\\boldsymbol{x}^1-\\boldsymbol{x}^0) = \\boldsymbol{x}^1$。\n   所以，$\\boldsymbol{z}^1 = \\frac{\\sqrt{2}}{5}\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$。\n\n*迭代 2 (k=1):*\n1. 在 $\\boldsymbol{z}^1$ 处的梯度和近端步骤：\n   $\\boldsymbol{y}^1 = \\boldsymbol{z}^1 - t\\boldsymbol{A}^{\\top}(\\boldsymbol{A}\\boldsymbol{z}^1-\\boldsymbol{b}) = \\boldsymbol{z}^1 - 1\\cdot(\\boldsymbol{z}^1-\\boldsymbol{b}) = \\boldsymbol{b}$。\n   近端算子的参数同样是 $\\boldsymbol{b}$。\n   $\\boldsymbol{x}^2 = \\boldsymbol{W}^{\\top}S_{\\tau}(\\boldsymbol{W}\\boldsymbol{y}^1) = \\boldsymbol{W}S_{3/5}(\\boldsymbol{W}\\boldsymbol{b}) = \\boldsymbol{x}^1 = \\frac{\\sqrt{2}}{5}\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$。\n   所以，$\\boldsymbol{x}^2_{\\text{FISTA}} = \\begin{pmatrix} \\frac{\\sqrt{2}}{5} \\\\ \\frac{\\sqrt{2}}{5} \\end{pmatrix}$。\n\n$\\boldsymbol{A}=\\boldsymbol{I}$ 和 $t=1/L$ 的特殊情况导致算法在单次迭代中找到精确解。因此，FISTA 的迭代结果 $\\boldsymbol{x}^2$ 与 $\\boldsymbol{x}^1$ 相同。\n\n最终结果是行向量 $\\big(x^{1}_{\\text{ISTA},1}, x^{1}_{\\text{ISTA},2}, x^{2}_{\\text{FISTA},1}, x^{2}_{\\text{FISTA},2}\\big)$。\n$x^{1}_{\\text{ISTA},1} = \\frac{\\sqrt{2}}{5}$，$x^{1}_{\\text{ISTA},2} = \\frac{\\sqrt{2}}{5}$。\n$x^{2}_{\\text{FISTA},1} = \\frac{\\sqrt{2}}{5}$，$x^{2}_{\\text{FISTA},2} = \\frac{\\sqrt{2}}{5}$。\n最终向量是 $\\left(\\frac{\\sqrt{2}}{5}, \\frac{\\sqrt{2}}{5}, \\frac{\\sqrt{2}}{5}, \\frac{\\sqrt{2}}{5}\\right)$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\sqrt{2}}{5}  \\frac{\\sqrt{2}}{5}  \\frac{\\sqrt{2}}{5}  \\frac{\\sqrt{2}}{5}\n\\end{pmatrix}\n}\n$$", "id": "4913497"}]}