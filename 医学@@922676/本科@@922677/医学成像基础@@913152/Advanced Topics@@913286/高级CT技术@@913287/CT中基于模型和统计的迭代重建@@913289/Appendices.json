{"hands_on_practices": [{"introduction": "许多基于模型的迭代重建（MBIR）方法的核心是求解一个庞大的线性方程组。该方程组被称为正规方程，源于对一个惩罚加权最小二乘（PWLS）目标函数的最小化，该目标函数旨在平衡数据一致性与图像的先验知识。本练习 [@problem_id:4900883] 将挑战你实现共轭梯度（CG）算法——一种强大的迭代求解器——来解决这个核心问题，并探索预处理如何加速其收敛，从而让你对CT图像重建背后的计算引擎有基本的了解。", "problem": "考虑在计算机断层扫描（CT）中基于模型和统计的迭代重建所产生的二次惩罚加权最小二乘（PWLS）子问题。目标函数由下式给出\n$$\n\\min_{\\mathbf{x} \\in \\mathbb{R}^n} \\ \\frac{1}{2}\\,\\lVert \\mathbf{y} - A\\mathbf{x} \\rVert_{W}^2 \\ + \\ \\frac{1}{2}\\,\\beta\\,\\lVert C\\mathbf{x} \\rVert_2^2,\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$ 是将图像系数映射到线积分的系统矩阵，$\\mathbf{y} \\in \\mathbb{R}^m$ 是测量向量，$W \\in \\mathbb{R}^{m \\times m}$ 是一个对角加权矩阵，其非负项反映了测量方差，$C \\in \\mathbb{R}^{p \\times n}$ 是一个线性正则化算子，$\\beta \\in \\mathbb{R}_{>0}$ 是正则化参数。加权范数定义为 $\\lVert \\mathbf{v} \\rVert_W^2 = \\mathbf{v}^{\\top} W \\mathbf{v}$。\n\n从凸二次最小化的基本定义和对称正定（SPD）矩阵的性质出发，推导最小化子的正规方程组，并证明解 $\\mathbf{x}^\\star$ 满足\n$$\nH\\,\\mathbf{x}^\\star = \\mathbf{b},\n$$\n其中\n$$\nH = A^{\\top} W A + \\beta\\, C^{\\top} C, \\qquad \\mathbf{b} = A^{\\top} W \\mathbf{y}.\n$$\n设计一个共轭梯度（CG）方案，仅使用与 $A$、$W$、$C$ 和 $C^{\\top}$ 的矩阵向量乘积来求解线性系统 $H\\,\\mathbf{x} = \\mathbf{b}$，而无需显式地构建 $H$。讨论并实现使用对角矩阵\n$$\nM = \\operatorname{diag}\\!\\left(A^{\\top} W A + \\beta\\, C^{\\top} C\\right),\n$$\n的左预处理，并使用 $M^{-1}$ 作为预处理器。解释为什么 $M$ 是一个有效的预处理器，以及它如何近似 $H$。\n\n您的程序必须为系统 $H\\,\\mathbf{x} = \\mathbf{b}$ 实现非预处理CG和对角预处理CG，并遵循以下规范：\n- 初始迭代点必须为 $\\mathbf{x}_0 = \\mathbf{0}$。\n- 对残差使用欧几里得范数。\n- 停止准则必须是相对残差阈值\n$$\n\\frac{\\lVert \\mathbf{r}_k \\rVert_2}{\\lVert \\mathbf{r}_0 \\rVert_2} \\leq \\varepsilon,\n$$\n其中 $\\varepsilon = 10^{-8}$，且 $\\mathbf{r}_k = \\mathbf{b} - H\\mathbf{x}_k$。\n- 最大迭代次数必须为 $5n$。\n\n通过\n$$\nH\\mathbf{v} = A^{\\top}\\left(W(A\\mathbf{v})\\right) + \\beta\\, C^{\\top}(C\\mathbf{v}),\n$$\n实现矩阵向量乘积，并通过\n$$\n\\operatorname{diag}(H)_j = \\sum_{i=1}^{m} W_{ii}\\,A_{ij}^2 + \\beta \\sum_{k=1}^{p} C_{kj}^2,\n$$\n实现对角预处理器，约定任何零对角项都用一个微小正常数替换，以避免在 $M^{-1}$ 中出现除以零的情况。\n\n测试套件：\n为保证可复现性，请使用以下四个测试用例。在每个用例中，构建 $A$、$W$、$C$、$\\beta$、一个真实值向量 $\\mathbf{x}_{\\text{true}}$，以及测量值 $\\mathbf{y} = A\\mathbf{x}_{\\text{true}}$。然后设置 $\\mathbf{b} = A^{\\top} W \\mathbf{y}$。\n\n- 用例 1（一般良态情况）：$n=6$, $m=12$。使用种子为1的伪随机正态生成器生成 $A$ 和 $\\mathbf{x}_{\\text{true}}$。生成对角矩阵 $W$，其对角项从 $[0.5, 2.0]$ 中均匀抽取。将 $C$ 设置为 $\\mathbb{R}^n$ 上大小为 $(n-1)\\times n$ 的一阶有限差分算子，并设置 $\\beta = 0.1$。\n\n- 用例 2（通过零权重实现部分数据缺失）：$n=6$, $m=12$。使用种子为2的伪随机正态生成器生成 $A$ 和 $\\mathbf{x}_{\\text{true}}$。生成 $W$ 的初始对角项，在 $[0.5, 2.0]$ 上均匀分布，然后将索引为 $2$、$3$、$6$ 和 $9$ 的四个项设置为零（使用从零开始的索引）。设置 $C = I_n$（$n \\times n$ 单位矩阵）和 $\\beta = 0.5$。\n\n- 用例 3（病态前向模型）：$n=6$, $m=10$。使用种子为3的伪随机正态生成器生成一个向量 $\\mathbf{u} \\in \\mathbb{R}^m$；构建 $A$，使其第一列等于 $\\mathbf{u}$，每个后续列等于 $\\mathbf{u} + 10^{-4}\\,\\mathbf{v}_j$，其中每个 $\\mathbf{v}_j$ 从标准正态分布中独立同分布地抽取。在 $[0.8, 1.2]$ 上均匀抽取 $W$ 的对角项。将 $C$ 设置为 $\\mathbb{R}^n$ 上的一阶有限差分算子，并设置 $\\beta = 10^{-4}$。\n\n- 用例 4（强正则化）：$n=8$, $m=12$。使用种子为4的伪随机正态生成器生成 $A$ 和 $\\mathbf{x}_{\\text{true}}$。在 $[0.2, 0.5]$ 上均匀生成 $W$ 的对角项。将 $C$ 设置为 $\\mathbb{R}^n$ 上的一阶有限差分算子，并设置 $\\beta = 100$。\n\n角度单位不适用。输出中没有物理单位。\n\n输出规范：\n对于每个测试用例，运行非预处理CG和对角预处理CG，并记录达到停止准则所需的迭代次数。最终程序输出必须是单行，包含一个用方括号括起来的逗号分隔列表，其中包含八个整数，顺序如下：\n$$\n[\\text{iters\\_unpre\\_case1},\\text{iters\\_pre\\_case1},\\text{iters\\_unpre\\_case2},\\text{iters\\_pre\\_case2},\\text{iters\\_unpre\\_case3},\\text{iters\\_pre\\_case3},\\text{iters\\_unpre\\_case4},\\text{iters\\_pre\\_case4}].\n$$\n您的程序必须是自包含的、确定性的，并精确地生成这一单行作为其唯一输出。", "solution": "问题陈述已经过验证，被认为是合理、完整且适定的。我们可以继续进行推导和求解。\n\n该问题要求解在医学成像重建中常见的惩罚加权最小二乘（PWLS）最小化问题。目标函数由下式给出：\n$$\nf(\\mathbf{x}) = \\frac{1}{2}\\,\\lVert \\mathbf{y} - A\\mathbf{x} \\rVert_{W}^2 + \\frac{1}{2}\\,\\beta\\,\\lVert C\\mathbf{x} \\rVert_2^2\n$$\n其中 $\\mathbf{x} \\in \\mathbb{R}^n$ 是待估计的图像系数向量，$\\mathbf{y} \\in \\mathbb{R}^m$ 是测量向量，$A \\in \\mathbb{R}^{m \\times n}$ 是系统矩阵，$W \\in \\mathbb{R}^{m \\times m}$ 是具有非负项的对角加权矩阵，$C \\in \\mathbb{R}^{p \\times n}$ 是正则化算子，$\\beta \\in \\mathbb{R}_{>0}$ 是正则化参数。加权范数定义为 $\\lVert \\mathbf{v} \\rVert_W^2 = \\mathbf{v}^{\\top} W \\mathbf{v}$。\n\n**1. 正规方程组的推导**\n\n函数 $f(\\mathbf{x})$ 是关于 $\\mathbf{x}$ 的二次函数。为找到最小化 $f(\\mathbf{x})$ 的向量 $\\mathbf{x}^\\star$，我们必须找到 $f(\\mathbf{x})$ 相对于 $\\mathbf{x}$ 的梯度为零的点。首先，我们展开目标函数：\n$$\nf(\\mathbf{x}) = \\frac{1}{2} (\\mathbf{y} - A\\mathbf{x})^{\\top} W (\\mathbf{y} - A\\mathbf{x}) + \\frac{1}{2} \\beta (C\\mathbf{x})^{\\top}(C\\mathbf{x})\n$$\n展开第一项：\n$$\n(\\mathbf{y} - A\\mathbf{x})^{\\top} W (\\mathbf{y} - A\\mathbf{x}) = (\\mathbf{y}^{\\top} - \\mathbf{x}^{\\top}A^{\\top})W(\\mathbf{y} - A\\mathbf{x}) = \\mathbf{y}^{\\top}W\\mathbf{y} - \\mathbf{y}^{\\top}WA\\mathbf{x} - \\mathbf{x}^{\\top}A^{\\top}W\\mathbf{y} + \\mathbf{x}^{\\top}A^{\\top}WA\\mathbf{x}\n$$\n由于 $\\mathbf{x}^{\\top}A^{\\top}W\\mathbf{y}$ 是一个标量，它等于其转置 $(\\mathbf{x}^{\\top}A^{\\top}W\\mathbf{y})^{\\top} = \\mathbf{y}^{\\top}W^{\\top}A\\mathbf{x}$。因为 $W$ 是对角矩阵，所以它是对称的（$W=W^\\top$），因此 $\\mathbf{x}^{\\top}A^{\\top}W\\mathbf{y} = \\mathbf{y}^{\\top}WA\\mathbf{x}$。该项因此变为：\n$$\n\\mathbf{y}^{\\top}W\\mathbf{y} - 2\\mathbf{y}^{\\top}WA\\mathbf{x} + \\mathbf{x}^{\\top}A^{\\top}WA\\mathbf{x}\n$$\n$f(\\mathbf{x})$ 的第二项是 $\\frac{1}{2} \\beta \\mathbf{x}^{\\top}C^{\\top}C\\mathbf{x}$。合并所有部分，目标函数为：\n$$\nf(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^{\\top} (A^{\\top}WA + \\beta C^{\\top}C) \\mathbf{x} - (A^{\\top}W\\mathbf{y})^{\\top}\\mathbf{x} + \\frac{1}{2}\\mathbf{y}^{\\top}W\\mathbf{y}\n$$\n这是一个标准二次型 $f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^{\\top}H\\mathbf{x} - \\mathbf{b}^{\\top}\\mathbf{x} + \\text{常数}$，其中：\n$$\nH = A^{\\top}WA + \\beta C^{\\top}C\n$$\n$$\n\\mathbf{b} = A^{\\top}W\\mathbf{y}\n$$\n$f(\\mathbf{x})$ 相对于 $\\mathbf{x}$ 的梯度是 $\\nabla_{\\mathbf{x}}f(\\mathbf{x}) = H\\mathbf{x} - \\mathbf{b}$。矩阵 $H$ 是对称的，因为 $W$ 是对称的，因此 $(A^{\\top}WA)^{\\top} = A^{\\top}W^{\\top}A = A^{\\top}WA$，并且 $(C^{\\top}C)^{\\top} = C^{\\top}C$。$f(\\mathbf{x})$ 的海森矩阵是 $\\nabla^2_{\\mathbf{x}}f(\\mathbf{x}) = H$。由于 $W$ 具有非负项，$A^{\\top}WA$ 是半正定的。类似地，$C^{\\top}C$ 是半正定的。当 $\\beta > 0$ 时，它们的和 $H$ 也是半正定的。在典型条件下，即 $\\ker(A) \\cap \\ker(C) = \\{\\mathbf{0}\\}$，$H$ 是正定的，这使得 $f(\\mathbf{x})$ 是严格凸函数，并确保存在唯一的最小化子。\n\n将梯度设为零以找到最小值 $\\mathbf{x}^\\star$：\n$$\n\\nabla_{\\mathbf{x}}f(\\mathbf{x}^\\star) = H \\mathbf{x}^\\star - \\mathbf{b} = \\mathbf{0}\n$$\n这就得到了被称为正规方程组的线性方程组：\n$$\nH \\mathbf{x}^\\star = \\mathbf{b}\n$$\n这证实了问题陈述中提供的结构。\n\n**2. 共轭梯度法和预处理**\n\n共轭梯度（CG）算法是一种迭代方法，非常适合求解像 $H\\mathbf{x} = \\mathbf{b}$ 这样的大规模、稀疏、对称正定（SPD）线性系统。其一个关键优势是它只需要一个计算矩阵向量乘积 $H\\mathbf{v}$ 的函数，从而避免了显式构建和存储矩阵 $H$。该乘积实现为：\n$$\nH\\mathbf{v} = (A^{\\top}WA + \\beta C^{\\top}C)\\mathbf{v} = A^{\\top}(W(A\\mathbf{v})) + \\beta C^{\\top}(C\\mathbf{v})\n$$\nCG的收敛速度取决于 $H$ 的条件数。预处理是一种将系统转换为具有更有利条件数的系统的技术。我们使用一个预处理器矩阵 $M$，它近似于 $H$ 并且易于求逆。预处理共轭梯度（PCG）方法修改了标准CG算法以整合 $M^{-1}$。\n\n所选的预处理器是对角矩阵 $M = \\operatorname{diag}(H)$。这是一种雅可比预处理。\n- **作为预处理器的有效性**：要使 $M$ 成为一个有效的预处理器，它必须是对称和正定的。$M$ 是对角矩阵，因此是对称的。由于 $H$ 是对称正定（在温和条件下），其所有对角项 $H_{jj} = \\mathbf{e}_j^\\top H \\mathbf{e}_j$ 都是正的，其中 $\\mathbf{e}_j$ 是第 $j$ 个标准基向量。因此，$M$ 是正定的。如果任何对角项为零，则用一个微小正常数替换，以保持可逆性和正定性。\n- **$H$ 的近似**：$M$ 通过保留其对角元素而丢弃所有非对角线信息来近似 $H$。这是一个简单但通常有效的策略，特别是当 $H$ 具有一定程度的对角占优时。\n- **可逆性**：作为一个对角矩阵，$M^{-1}$ 的计算非常简单；其对角项是 $M$ 对角项的倒数。\n\n$H = A^{\\top}WA + \\beta C^{\\top}C$ 的第 $j$ 个对角项计算如下：\n$$\nM_{jj} = H_{jj} = (A^{\\top}WA)_{jj} + \\beta (C^{\\top}C)_{jj}\n$$\n项 $(A^{\\top}WA)_{jj}$ 是 $\\sum_{i=1}^m (A^\\top)_{ji} (WA)_{ij} = \\sum_{i=1}^m A_{ij} \\sum_{k=1}^m W_{ik} A_{kj}$。由于 $W$ 是对角的（当 $i \\neq k$ 时 $W_{ik}=0$），这可以简化为 $\\sum_{i=1}^m A_{ij} W_{ii} A_{ij} = \\sum_{i=1}^m W_{ii} A_{ij}^2$。项 $(C^{\\top}C)_{jj}$ 是 $\\sum_{k=1}^p (C^\\top)_{jk} C_{kj} = \\sum_{k=1}^p C_{kj}^2$。将这些结合起来，得到指定的公式：\n$$\nM_{jj} = \\sum_{i=1}^{m} W_{ii}\\,A_{ij}^2 + \\beta \\sum_{k=1}^{p} C_{kj}^2\n$$\n\n实现将遵循标准的PCG算法，其中步骤 $M\\mathbf{z}_k = \\mathbf{r}_k$ 通过按元素相除 $\\mathbf{z}_k = \\mathbf{r}_k ./ \\operatorname{diag}(M)$ 来求解。\n\n算法将以初始猜测 $\\mathbf{x}_0 = \\mathbf{0}$、相对残差停止准则 $\\lVert \\mathbf{r}_k \\rVert_2 / \\lVert \\mathbf{r}_0 \\rVert_2 \\leq 10^{-8}$ 以及最大迭代次数 $5n$ 来实现。", "answer": "```python\nimport numpy as np\n\ndef H_matvec(v, A, W_diag, C, beta):\n    \"\"\"Computes the matrix-vector product H*v without forming H explicitly.\"\"\"\n    # H*v = (A.T @ W @ A + beta * C.T @ C) @ v\n    #     = A.T @ (W @ (A @ v)) + beta * C.T @ (C @ v)\n    # Since W is diagonal, W @ u is just W_diag * u\n    return A.T @ (W_diag * (A @ v)) + beta * (C.T @ (C @ v))\n\ndef compute_M_diag(A, W_diag, C, beta):\n    \"\"\"Computes the diagonal of H.\"\"\"\n    # diag(H)_j = sum_i(W_ii * A_ij^2) + beta * sum_k(C_kj^2)\n    # Vectorized computation:\n    # Part 1: diag(A.T @ W @ A) = einsum('i,ij->j', W_diag, A**2)\n    # Part 2: diag(C.T @ C) = sum(C**2, axis=0)\n    diag_H = np.einsum('i,ij->j', W_diag, A**2) + beta * np.sum(C**2, axis=0)\n    \n    # Replace zero entries with a small positive constant to ensure invertibility\n    diag_H[diag_H == 0] = 1e-12\n    return diag_H\n\ndef cg_unpreconditioned(A, W_diag, C, beta, b, tol=1e-8, max_iter=None):\n    \"\"\"Implements the unpreconditioned Conjugate Gradient algorithm.\"\"\"\n    n = b.shape[0]\n    if max_iter is None:\n        max_iter = 5 * n\n\n    x = np.zeros(n)\n    r = b - H_matvec(x, A, W_diag, C, beta)\n    p = r.copy()\n    \n    r0_norm = np.linalg.norm(r)\n    if r0_norm == 0:\n        return 0\n\n    rs_old = np.dot(r, r)\n\n    for k in range(max_iter):\n        Hp = H_matvec(p, A, W_diag, C, beta)\n        alpha = rs_old / np.dot(p, Hp)\n        \n        x = x + alpha * p\n        r = r - alpha * Hp\n        \n        r_norm = np.linalg.norm(r)\n        if r_norm / r0_norm = tol:\n            return k + 1\n\n        rs_new = np.dot(r, r)\n        gamma = rs_new / rs_old\n        p = r + gamma * p\n        rs_old = rs_new\n        \n    return max_iter\n\ndef cg_preconditioned(A, W_diag, C, beta, b, tol=1e-8, max_iter=None):\n    \"\"\"Implements the diagonally preconditioned Conjugate Gradient algorithm.\"\"\"\n    n = b.shape[0]\n    if max_iter is None:\n        max_iter = 5 * n\n\n    M_diag = compute_M_diag(A, W_diag, C, beta)\n    \n    x = np.zeros(n)\n    r = b - H_matvec(x, A, W_diag, C, beta)\n    \n    r0_norm = np.linalg.norm(r)\n    if r0_norm == 0:\n        return 0\n\n    z = r / M_diag\n    p = z.copy()\n    rz_old = np.dot(r, z)\n\n    for k in range(max_iter):\n        Hp = H_matvec(p, A, W_diag, C, beta)\n        alpha = rz_old / np.dot(p, Hp)\n        \n        x = x + alpha * p\n        r = r - alpha * Hp\n        \n        r_norm = np.linalg.norm(r)\n        if r_norm / r0_norm = tol:\n            return k + 1\n\n        z = r / M_diag\n        rz_new = np.dot(r, z)\n        gamma = rz_new / rz_old\n        p = z + gamma * p\n        rz_old = rz_new\n        \n    return max_iter\n\ndef setup_case(case_params):\n    \"\"\"Sets up the matrices and vectors for a given test case.\"\"\"\n    n, m, beta, seed, case_type = case_params\n    rng = np.random.default_rng(seed)\n\n    # Generate A and x_true\n    if case_type == 'ill-conditioned':\n        A = np.zeros((m, n))\n        u = rng.normal(size=m)\n        A[:, 0] = u\n        for j in range(1, n):\n            v_j = rng.normal(size=m)\n            A[:, j] = u + 1e-4 * v_j\n        x_true = rng.normal(size=n)\n    else:\n        A = rng.normal(size=(m, n))\n        x_true = rng.normal(size=n)\n\n    # Generate W\n    if case_type == 'general':\n        W_diag = rng.uniform(0.5, 2.0, size=m)\n    elif case_type == 'missing-data':\n        W_diag = rng.uniform(0.5, 2.0, size=m)\n        W_diag[[2, 3, 6, 9]] = 0.0\n    elif case_type == 'ill-conditioned':\n        W_diag = rng.uniform(0.8, 1.2, size=m)\n    elif case_type == 'strong-reg':\n        W_diag = rng.uniform(0.2, 0.5, size=m)\n    else:\n        raise ValueError(\"Unknown case type\")\n\n    # Generate C\n    if case_type == 'missing-data':\n        C = np.eye(n)\n    else: # first-order finite difference operator\n        C = np.eye(n - 1, n, k=1) - np.eye(n - 1, n)\n\n    y = A @ x_true\n    b = A.T @ (W_diag * y)\n    \n    return A, W_diag, C, beta, b\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    test_cases = [\n        # n, m, beta, seed, case_type\n        (6, 12, 0.1, 1, 'general'),\n        (6, 12, 0.5, 2, 'missing-data'),\n        (6, 10, 1e-4, 3, 'ill-conditioned'),\n        (8, 12, 100.0, 4, 'strong-reg')\n    ]\n\n    results = []\n    \n    for params in test_cases:\n        n = params[0]\n        max_iter = 5 * n\n        A, W_diag, C, beta, b = setup_case(params)\n        \n        iters_unpre = cg_unpreconditioned(A, W_diag, C, beta, b, max_iter=max_iter)\n        iters_pre = cg_preconditioned(A, W_diag, C, beta, b, max_iter=max_iter)\n        \n        results.extend([iters_unpre, iters_pre])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "4900883"}, {"introduction": "虽然共轭梯度法是有效的，但对于大规模CT问题，其收敛速度可能很慢，因为每次迭代都需要遍历整个数据集。为了克服这一点，从业者使用有序子集（OS）等加速技术，该技术每次仅使用一小部分数据来更新图像。本练习 [@problem_id:4900906] 邀请你为PWLS目标函数推导OS更新的数学公式，并分析其收敛特性，揭示速度与准确性之间的权衡。", "problem": "您正在为透射计算机断层扫描 (CT) 设计一种迭代图像重建算法，该算法使用一个凸二次惩罚加权最小二乘 (PWLS) 目标函数。系统模型是线性的，包含前向投影矩阵 $A \\in \\mathbb{R}^{M \\times N}$、图像向量 $x \\in \\mathbb{R}^{N}$、正弦图数据 $y \\in \\mathbb{R}^{M}$ 以及对角且正定的统计权重矩阵 $W \\in \\mathbb{R}^{M \\times M}$。正则化器是二次的，其矩阵 $R \\in \\mathbb{R}^{N \\times N}$ 是半正定的。PWLS 目标函数为\n$$\nf(x) \\triangleq \\tfrac{1}{2}\\lVert y - A x \\rVert_{W}^{2} + \\tfrac{1}{2} x^{\\mathsf{T}} R x,\n$$\n其中 $\\lVert v \\rVert_{W}^{2} \\triangleq v^{\\mathsf{T}} W v$。您考虑通过有序子集 (OS) 方法来加速预条件梯度法。您将 $M$ 条射线划分为 $S$ 个大小近似相等的不相交子集 $\\{\\mathcal{I}_{s}\\}_{s=1}^{S}$，并通过将 $A$、$W$ 和 $y$ 限制在 $\\mathcal{I}_{s}$ 中的行来定义每个子集的量 $A_{s} \\in \\mathbb{R}^{|\\mathcal{I}_{s}| \\times N}$、$W_{s} \\in \\mathbb{R}^{|\\mathcal{I}_{s}| \\times |\\mathcal{I}_{s}|}$ 和 $y_{s} \\in \\mathbb{R}^{|\\mathcal{I}_{s}|}$。设 $D \\in \\mathbb{R}^{N \\times N}$ 是 Hessian 矩阵 $H \\triangleq A^{\\mathsf{T}} W A + R$ 的任意固定对角正定替代（例如，一个優化对角阵），并设 $\\alpha \\!\\! 0$ 表示一个固定步长。\n\n从第一性原理出发，基于 $f(x)$ 的定义和矩阵微积分的基本规则，推理 OS 是如何通过用缩放后的子集梯度来近似全数据梯度而构建的。然后，仔细地形式化一个在迭代 $k$ 次时使用循环子集索引 $s_{k} \\in \\{1,\\dots,S\\}$（采用典型的循环方案 $s_{k} = ((k \\bmod S) + 1)$）的单个 OS 步骤，并解释在何种机制下，即使 $f(x)$ 是凸二次的，OS 也可能无法精确收敛，而是表现出极限环。\n\n选择一个选项，该选项正确地指明了：\n- 一个数学上一致的、从此 PWLS 设置的子集划分构造和适当缩放中得出的 OS 更新步骤，以及\n- 对于固定的 $\\alpha$ 和 $S \\ge 2$，OS 为何会引入极限环的正确推理。\n\nA. 射线被划分为 $S$ 个不相交的子集。在 $D \\succ 0$ 是对角且固定的情况下，使用子集 $s_{k}$ 的一个 OS 步骤是\n$$\nx^{k+1} = x^{k} - \\alpha D^{-1} \\Big( R x^{k} + S\\, A_{s_{k}}^{\\mathsf{T}} W_{s_{k}} \\big( A_{s_{k}} x^{k} - y_{s_{k}} \\big) \\Big).\n$$\n这使用了按 $S$ 缩放的数据项的子集梯度（以便子集上的平均值与全数据梯度匹配），并在每一步中都包含了完整的正则化器梯度。OS 会产生极限环，因为每次迭代的梯度是全梯度的一个确定性的、循环的近似；该迭代应用了一系列依赖于子集的线性映射的乘积，对于固定的 $\\alpha$，这不一定是一个收缩映射，因此迭代值可能会在真实最小值点周围循环而不稳定下来，除非减小 $\\alpha$ 或使用其他阻尼方法。\n\nB. 射线被划分为 $S$ 个子集。在 $D \\succ 0$ 的情况下，OS 步骤是\n$$\nx^{k+1} = x^{k} - \\alpha D^{-1} \\Big( S\\, R x^{k} + S\\, A_{s_{k}}^{\\mathsf{T}} W_{s_{k}} \\big( A_{s_{k}} x^{k} - y_{s_{k}} \\big) \\Big).\n$$\n同时将数据和正则化梯度乘以 $S$ 可以保持期望梯度。由于 $f(x)$ 是凸二次的，对于任何固定的 $\\alpha \\in (0,2)$，OS 保证 $f(x)$ 的单调递减和无环的全局收敛。\n\nC. 射线被划分为 $S$ 个子集。在 $D \\succ 0$ 的情况下，OS 步骤是\n$$\nx^{k+1} = x^{k} - \\alpha D^{-1} \\Big( R x^{k} + A_{s_{k}}^{\\mathsf{T}} W_{s_{k}} \\big( A_{s_{k}} x^{k} - y_{s_{k}} \\big) \\Big).\n$$\n不需要按 $S$ 缩放，因为子集梯度已经近似了全梯度。由于 Hessian 矩阵 $H$ 是常数，对于任何固定的 $\\alpha$，OS 不会产生极限环。\n\nD. 射线被划分为 $S$ 个子集。在迭代 $k$ 时，精确求解子集 $s_{k}$ 的正规方程，\n$$\n\\big( A_{s_{k}}^{\\mathsf{T}} W_{s_{k}} A_{s_{k}} + R \\big) x^{k+1} = A_{s_{k}}^{\\mathsf{T}} W_{s_{k}} y_{s_{k}},\n$$\n并循环遍历子集。因为每个子集都得到精确求解，这相当于在 $S$ 次迭代中顺序求解完整问题，因此不会表现出极限环。\n\nE. 射线被划分为 $S$ 个子集。在 $D \\succ 0$ 的情况下，一个 OS 步骤是\n$$\nx^{k+1} = x^{k} - \\alpha D^{-1} \\Big( R x^{k} + S\\, A_{s_{k}}^{\\mathsf{T}} W_{s_{k}} \\big( A_{s_{k}} x^{k} - y_{s_{k}} \\big) \\Big).\n$$\nOS 只有在惩罚项非二次时才会显示极限环；对于凸二次的 $f(x)$，常数 Hessian 矩阵使得对于任何固定的 $\\alpha$，极限环都不可能出现。", "solution": "用户提供了一个关于计算机断层扫描 (CT) 中惩罚加权最小二乘 (PWLS) 目标函数的有序子集 (OS) 方法的构建和收敛性质的问题。\n\n### 问题验证\n\n**第 1 步：提取已知条件**\n- **目标函数**：$f(x) \\triangleq \\tfrac{1}{2}\\lVert y - A x \\rVert_{W}^{2} + \\tfrac{1}{2} x^{\\mathsf{T}} R x$。\n- **向量/矩阵定义**：\n    - $x \\in \\mathbb{R}^{N}$ 是图像向量。\n    - $y \\in \\mathbb{R}^{M}$ 是正弦图数据。\n    - $A \\in \\mathbb{R}^{M \\times N}$ 是前向投影矩阵。\n    - $W \\in \\mathbb{R}^{M \\times M}$ 是一个对角、正定的统计权重矩阵。\n    - $R \\in \\mathbb{R}^{N \\times N}$ 是一个半正定正则化矩阵。\n- **范数定义**：$\\lVert v \\rVert_{W}^{2} \\triangleq v^{\\mathsf{T}} W v$。\n- **有序子集 (OS) 划分**：\n    - $M$ 条射线被划分为 $S$ 个不相交的子集 $\\{\\mathcal{I}_{s}\\}_{s=1}^{S}$。\n    - 通过将 $A$、$W$ 和 $y$ 的行限制在索引集 $\\mathcal{I}_{s}$ 中，来定义每个子集的量 $A_{s}$、$W_{s}$ 和 $y_{s}$。\n- **迭代法参数**：\n    - $D \\in \\mathbb{R}^{N \\times N}$ 是 Hessian 矩阵 $H \\triangleq A^{\\mathsf{T}} W A + R$ 的一个固定的、对角的、正定的替代。\n    - $\\alpha  0$ 是一个固定步长。\n    - 子集索引 $s_{k}$ 是循环的，例如，$s_{k} = ((k \\bmod S) + 1)$。\n\n**第 2 步：使用提取的已知条件进行验证**\n- **科学上合理**：该问题是在统计迭代重建 CT 的标准数学框架内提出的。PWLS 是一个广泛使用的目标函数，有序子集是一种典型的加速技术。所有概念在医学成像和数值优化领域都是公认的。该问题在科学上是合理的。\n- **适定的**：该问题要求推导一个标准算法并解释其已知的收敛行为。PWLS 目标函数是凸函数，因为它是两个凸函数（一个二次数据保真度项和一个二次正则化项）的和。该问题陈述清晰，基于已建立的理论有确定且无歧义的解。\n- **客观性**：语言技术性强、精确，且无主观内容。\n\n**第 3 步：结论和行动**\n问题陈述有效。这是一个医学图像重建领域中适定且有科学依据的问题。我将继续进行推导和求解。\n\n### 推导与分析\n\n**1. 目标函数的梯度**\n\nPWLS 目标函数由以下公式给出：\n$$\nf(x) = \\tfrac{1}{2}\\lVert y - A x \\rVert_{W}^{2} + \\tfrac{1}{2} x^{\\mathsf{T}} R x = \\tfrac{1}{2}(y - Ax)^{\\mathsf{T}} W (y - Ax) + \\tfrac{1}{2}x^{\\mathsf{T}} R x\n$$\n为了求梯度，我们使用标准的矩阵微积分法则。\n$$\n\\nabla f(x) = \\frac{d}{dx} \\left( \\tfrac{1}{2}(y^{\\mathsf{T}}Wy - 2x^{\\mathsf{T}}A^{\\mathsf{T}}Wy + x^{\\mathsf{T}}A^{\\mathsf{T}}WAx) + \\tfrac{1}{2}x^{\\mathsf{T}} R x \\right)\n$$\n$$\n\\nabla f(x) = -A^{\\mathsf{T}}Wy + A^{\\mathsf{T}}WAx + Rx\n$$\n$$\n\\nabla f(x) = A^{\\mathsf{T}}W(Ax - y) + Rx\n$$\n一个标准的预条件梯度下降迭代将是：\n$$\nx^{k+1} = x^{k} - \\alpha D^{-1} \\nabla f(x^k) = x^{k} - \\alpha D^{-1} \\Big( A^{\\mathsf{T}}W(Ax^k - y) + Rx^k \\Big)\n$$\n此更新在每次迭代中都使用整个数据集 $(A, y, W)$，这在计算上可能是禁止的。\n\n**2. 有序子集 (OS) 更新的构造**\n\nOS 方法通过在每次迭代中仅使用数据的一个子集来加速此过程。梯度的完整数据保真度部分 $A^{\\mathsf{T}}W(Ax - y)$ 可以表示为不相交子集的和：\n$$\nA^{\\mathsf{T}}W(Ax - y) = \\sum_{s=1}^{S} A_{s}^{\\mathsf{T}} W_{s} (A_{s} x - y_{s})\n$$\nOS 的核心思想是用求和中的单个项（经过放大以具有相似的量级）来近似这个完整的求和。假设选择的子集大小和贡献大致相等，则使用缩放因子 $S$。\n$$\n\\sum_{s=1}^{S} A_{s}^{\\mathsf{T}} W_{s} (A_{s} x - y_{s}) \\approx S \\cdot A_{s_{k}}^{\\mathsf{T}} W_{s_{k}} (A_{s_{k}} x - y_{s_{k}})\n$$\n对于使用子集 $s_k$ 的子迭代 $k$。\n\n正则化项的梯度 $Rx$ 不依赖于数据子集。它通常在每个子迭代中完整计算，因为它在计算上通常比数据项梯度便宜。因此，全梯度 $\\nabla f(x^k)$ 的 OS 近似为：\n$$\n\\nabla_{\\text{OS}} f(x^k) \\triangleq S\\, A_{s_{k}}^{\\mathsf{T}} W_{s_{k}} (A_{s_{k}} x^{k} - y_{s_{k}}) + R x^{k}\n$$\n将此近似梯度代入预条件梯度下降公式，得到 OS 更新规则：\n$$\nx^{k+1} = x^{k} - \\alpha D^{-1} \\Big( S\\, A_{s_{k}}^{\\mathsf{T}} W_{s_{k}} (A_{s_{k}} x^{k} - y_{s_{k}}) + R x^{k} \\Big)\n$$\n\n**3. OS 收敛性和极限环分析**\n\n如果一个点 $x^*$ 是迭代映射的一个不动点，那么迭代算法就收敛到该点。$f(x)$ 的全局最小值点 $x^*$ 由条件 $\\nabla f(x^*) = 0$ 定义：\n$$\n\\nabla f(x^*) = \\sum_{s=1}^{S} A_{s}^{\\mathsf{T}} W_{s} (A_{s} x^* - y_{s}) + Rx^* = 0\n$$\n现在，我们来检查这个最小值点 $x^*$ 是否是给定子集 $s_k$ 的 OS 更新的不动点。要使 $x^*$ 成为不动点，更新项必须为零，即 $\\nabla_{\\text{OS}}f(x^*) = 0$。\n$$\n\\nabla_{\\text{OS}} f(x^*) = S\\, A_{s_{k}}^{\\mathsf{T}} W_{s_{k}} (A_{s_{k}} x^* - y_{s_{k}}) + R x^* \\stackrel{?}{=} 0\n$$\n通常，这个条件不满足。真实的最小值点 $x^*$ 仅保证子集梯度分量（加上正则化梯度）的*和*为零。它不保证每个单独的（缩放后的）子集梯度分量（加上正则化）都为零。这种情况只在“一致性数据”的情况下发生，即对于所有的 $s$，$A_s^\\mathsf{T} W_s (A_s x^* - y_s)$ 都相同，这对于真实的（含噪声的）CT 数据是不成立的。\n\n因为 $x^*$ 不是每个子迭代映射 $x \\mapsto x - \\alpha D^{-1} \\nabla_{\\text{OS}} f(x)$ 的不动点，迭代值 $x^k$ 在每一步都会被“拉”离真实解。随着算法循环遍历子集 $s=1, \\dots, S$，这种“拉力”的方向也随之循环。迭代是仿射映射的复合，每个子集对应一个映射。对于固定的步长 $\\alpha  0$ 和 $S \\ge 2$，这个确定性的、有偏的更新序列会阻止迭代值在真实最小值点 $x^*$ 处稳定下来。相反，它们通常会收敛到 $x^*$ 周围的一个稳定轨道，即“极限环”。\n\n目标函数 $f(x)$ 是凸二次的（因此具有常数 Hessian 矩阵 $H = A^{\\mathsf{T}} W A + R$）这一事实与此现象无关。极限环是迭代过程本身的结果，它不是对 $f(x)$ 的真正梯度下降，而是一系列步骤，每个步骤都是在不同的辅助目标函数上的下降。OS 方法要收敛到真实最小值点，通常需要一个递减的步长，即 $\\alpha_k \\to 0$。\n\n### 逐项分析\n\n**A. 射线被划分为 $S$ 个不相交的子集。在 $D \\succ 0$ 是对角且固定的情况下，使用子集 $s_{k}$ 的一个 OS 步骤是...**\n- **更新规则**：$x^{k+1} = x^{k} - \\alpha D^{-1} \\Big( R x^{k} + S\\, A_{s_{k}}^{\\mathsf{T}} W_{s_{k}} \\big( A_{s_{k}} x^{k} - y_{s_{k}} \\big) \\Big)$。这与正确推导出的 OS-PWLS 更新规则相符。\n- **推理**：它正确地指出，OS 使用一个缩放的、循环的、确定性的梯度近似。它正确地识别出，依赖于子集的映射的复合可能不是一个收缩映射，并可能导致围绕最小值点的轨道。它正确地提到，需要像递减步长这样的补救措施才能实现精确收敛。这是一个完整而准确的解释。\n- **结论**：**正确**。\n\n**B. 射线被划分为 $S$ 个子集。在 $D \\succ 0$ 的情况下，OS 步骤是...**\n- **更新规则**：$x^{k+1} = x^{k} - \\alpha D^{-1} \\Big( S\\, R x^{k} + S\\, A_{s_{k}}^{\\mathsf{T}} W_{s_{k}} \\big( A_{s_{k}} x^{k} - y_{s_{k}} \\big) \\Big)$。此更新错误地将正则化梯度 $Rx^k$ 乘以 $S$。这将导致在一个周期内正则化被过度调整 $S$ 倍。\n- **推理**：它声称对于凸二次函数，OS 在固定步长下保证全局收敛而无极限环。如上文推导所述，这从根本上是错误的。极限环是具有固定步长的 OS 的一个标志性特征。\n- **结论**：**错误**。\n\n**C. 射线被划分为 $S$ 个子集。在 $D \\succ 0$ 的情况下，OS 步骤是...**\n- **更新规则**：$x^{k+1} = x^{k} - \\alpha D^{-1} \\Big( R x^{k} + A_{s_{k}}^{\\mathsf{T}} W_{s_{k}} \\big( A_{s_{k}} x^{k} - y_{s_{k}} \\big) \\Big)$。此更新未能将数据保真度子集梯度按 $S$ 缩放。由此产生的梯度近似值太小，导致收敛非常缓慢。这不是标准的 OS 算法。\n- **推理**：它声称常数 Hessian 矩阵能防止极限环。这是错误的。极限环的来源是不一致的梯度近似，而不是 Hessian 矩阵的任何性质或目标函数的非线性。\n- **结论**：**错误**。\n\n**D. 射线被划分为 $S$ 个子集。在迭代 $k$ 时，精确求解子集 $s_{k}$ 的正规方程...**\n- **更新规则**：这描述了一种块迭代方法，其中每一步都完全替换解，即 $x^{k+1}$ 是 $\\tfrac{1}{2}\\lVert y_{s_k} - A_{s_k} x \\rVert_{W_{s_k}}^{2} + \\tfrac{1}{2} x^{\\mathsf{T}} R x$ 的精确最小值点，但这并不是在 $x^k$ 的基础上构建的。这是另一类算法，而不是从 $f(x)$ 推导出的基于梯度的 OS 方法。\n- **推理**：它声称此过程不会出现极限环。这是不正确的。循环求解不同子问题的精确解是极限环行为的典型范例，因为解将在不同子问题的最小值之间跳跃，围绕全局最小值运行。\n- **结论**：**错误**。\n\n**E. 射线被划分为 $S$ 个子集。在 $D \\succ 0$ 的情况下，一个 OS 步骤是...**\n- **更新规则**：$x^{k+1} = x^{k} - \\alpha D^{-1} \\Big( R x^{k} + S\\, A_{s_{k}}^{\\mathsf{T}} W_{s_{k}} \\big( A_{s_{k}} x^{k} - y_{s_{k}} \\big) \\Big)$。更新规则本身是正确的，与选项 A 相同。\n- **推理**：它声称极限环只在惩罚项非二次时出现，而对于凸二次的 $f(x)$，由于常数 Hessian 矩阵，极限环不可能出现。这个推理与选项 C 中的错误逻辑相同，从根本上是错误的。目标函数的凸性或二次性并不能阻止 OS 极限环的出现。\n- **结论**：**错误**。", "answer": "$$\\boxed{A}$$", "id": "4900906"}, {"introduction": "有序子集（OS）方法显著加快了重建速度，但这种加速并非没有微妙之处。使用固定步长的OS的一个关键特征是它无法收敛到精确解，而是会陷入一个“极限环”——一个围绕真实图像最小化点的轨道。这个动手实践问题 [@problem_id:4900879] 将通过一个具体的数值反例来揭示这种行为的神秘面纱，让你能够明确地计算出极限环，并精确地理解数据子集之间的不一致性是如何导致这一重要现象的。", "problem": "考虑一个用于双参数图像模型 $\\mathbf{x} \\in \\mathbb{R}^{2}$ 的程式化计算机断层扫描（CT）逆问题，该模型具有三个理想化的投影射线。数据保真度由最小二乘目标函数建模\n$$\nf(\\mathbf{x}) = \\frac{1}{2} \\| A \\mathbf{x} - \\mathbf{b} \\|_{2}^{2},\n$$\n其中 $A \\in \\mathbb{R}^{3 \\times 2}$ 堆叠了单位范数射线方向，而 $\\mathbf{b} \\in \\mathbb{R}^{3}$ 是测量的线积分向量。设三个射线方向为\n$$\n\\mathbf{u}_{1} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}, \\quad \\mathbf{u}_{2} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}, \\quad \\mathbf{u}_{3} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix}1 \\\\ 1\\end{pmatrix},\n$$\n并将测量矩阵和数据划分为两个有序子集，用于有序子集（OS）方法（在每个轮次中，先处理第一个子集，然后处理第二个子集）：\n$$\nA_{1} = \\begin{pmatrix} \\mathbf{u}_{1}^{\\top} \\\\ \\mathbf{u}_{2}^{\\top} \\end{pmatrix} \\in \\mathbb{R}^{2 \\times 2}, \\quad \\mathbf{b}_{1} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}, \\qquad\nA_{2} = \\begin{pmatrix} \\mathbf{u}_{3}^{\\top} \\end{pmatrix} \\in \\mathbb{R}^{1 \\times 2}, \\quad b_{2} = \\frac{1}{2}.\n$$\n因此 $A = \\begin{pmatrix} A_{1} \\\\ A_{2} \\end{pmatrix}$ 且 $\\mathbf{b} = \\begin{pmatrix} \\mathbf{b}_{1} \\\\ b_{2} \\end{pmatrix}$。考虑使用恒定步长 $\\alpha = \\frac{1}{2}$ 的 OS 梯度下降法，其对轮次 $k$ 的定义为从 $\\mathbf{x}_{k,1}$ 开始并执行\n$$\n\\mathbf{x}_{k,2} = \\mathbf{x}_{k,1} - \\alpha \\, A_{1}^{\\top} \\big( A_{1}\\mathbf{x}_{k,1} - \\mathbf{b}_{1} \\big), \\qquad\n\\mathbf{x}_{k,3} = \\mathbf{x}_{k,2} - \\alpha \\, A_{2}^{\\top} \\big( A_{2}\\mathbf{x}_{k,2} - b_{2} \\big),\n$$\n轮次更新为 $\\mathbf{x}_{k+1,1} \\equiv \\mathbf{x}_{k+1} := \\mathbf{x}_{k,3}$。\n\n你将需要证明 $f$ 是强凸的，推导出得到的仿射轮次映射，并证明在稳态下 $\\mathbf{x}_{k,1}$ 和 $\\mathbf{x}_{k,2}$ 之间存在一个非平凡的两点极限环。具体而言：\n\n- 从以上定义出发，通过分析海森矩阵 $H := A^{\\top}A$ 来验证 $f$ 的强凸性。\n- 用 $H_{1} := A_{1}^{\\top}A_{1}$、$H_{2} := A_{2}^{\\top}A_{2}$、$\\mathbf{g}_{1} := A_{1}^{\\top}\\mathbf{b}_{1}$ 和 $\\mathbf{g}_{2} := A_{2}^{\\top} b_{2}$ 表示，推导出仿射轮次算子 $\\mathbf{x}_{k+1} = T \\, \\mathbf{x}_{k} + \\mathbf{h}$。\n- 求解轮次算子的不动点 $\\mathbf{x}_{\\mathrm{OS}}$，并证明子集内部的迭代收敛到一个稳定的两点循环 $\\mathbf{x}_{\\mathrm{OS}} \\leftrightarrow \\mathbf{y}_{\\mathrm{OS}}$，其中 $\\mathbf{y}_{\\mathrm{OS}} := \\mathbf{x}_{\\mathrm{OS}} - \\alpha\\big(H_{1}\\mathbf{x}_{\\mathrm{OS}} - \\mathbf{g}_{1}\\big)$。\n- 根据子集偏差，解释为什么 $\\big(H_{1}\\mathbf{x}_{\\mathrm{OS}} - \\mathbf{g}_{1}\\big) \\neq \\mathbf{0}$ 以及这如何导致非零的循环振幅。\n- 计算给定数值实例的稳态循环振幅的欧几里得范数 $\\|\\mathbf{y}_{\\mathrm{OS}} - \\mathbf{x}_{\\mathrm{OS}}\\|_{2}$。请将你的最终答案以单个实数形式给出。无需四舍五入。该量为无量纲量。", "solution": "该问题是有效的，因为它在医学成像迭代算法领域提出了一个适定的、有科学依据的问题。我们将进行完整的推导和求解。\n\n该问题要求对用于一个简化的计算机断层扫描（CT）逆问题的有序子集（OS）梯度下降算法进行分析。我们将按顺序处理问题的每个部分。\n\n首先，我们验证目标函数 $f(\\mathbf{x}) = \\frac{1}{2} \\| A \\mathbf{x} - \\mathbf{b} \\|_{2}^{2}$ 的强凸性。如果一个函数的海森矩阵是正定的，那么该函数是强凸的。该目标函数是一个二次型：\n$$f(\\mathbf{x}) = \\frac{1}{2} (A\\mathbf{x} - \\mathbf{b})^{\\top}(A\\mathbf{x} - \\mathbf{b}) = \\frac{1}{2} (\\mathbf{x}^{\\top}A^{\\top}A\\mathbf{x} - 2\\mathbf{b}^{\\top}A\\mathbf{x} + \\mathbf{b}^{\\top}\\mathbf{b})$$\n$f$ 相对于 $\\mathbf{x}$ 的梯度是 $\\nabla f(\\mathbf{x}) = A^{\\top}A\\mathbf{x} - A^{\\top}\\mathbf{b}$。\n海森矩阵是二阶导数，即 $H = \\nabla^2 f(\\mathbf{x}) = A^{\\top}A$。\n让我们从其子集 $A_1$ 和 $A_2$ 构建完整的系统矩阵 $A$。\n$$A_1 = \\begin{pmatrix} \\mathbf{u}_{1}^{\\top} \\\\ \\mathbf{u}_{2}^{\\top} \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}, \\quad A_2 = \\begin{pmatrix} \\mathbf{u}_{3}^{\\top} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}} \\end{pmatrix}$$\n$$A = \\begin{pmatrix} A_1 \\\\ A_2 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}} \\end{pmatrix}$$\n现在，我们计算海森矩阵 $H$：\n$$H = A^{\\top}A = \\begin{pmatrix} 1  0  \\frac{1}{\\sqrt{2}} \\\\ 0  1  \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} 1\\cdot1 + 0\\cdot0 + \\frac{1}{\\sqrt{2}}\\frac{1}{\\sqrt{2}}  1\\cdot0 + 0\\cdot1 + \\frac{1}{\\sqrt{2}}\\frac{1}{\\sqrt{2}} \\\\ 0\\cdot1 + 1\\cdot0 + \\frac{1}{\\sqrt{2}}\\frac{1}{\\sqrt{2}}  0\\cdot0 + 1\\cdot1 + \\frac{1}{\\sqrt{2}}\\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2}  \\frac{1}{2} \\\\ \\frac{1}{2}  \\frac{3}{2} \\end{pmatrix}$$\n为了检查正定性，我们通过求解 $\\det(H - \\lambda I) = 0$ 来找到 $H$ 的特征值 $\\lambda$：\n$$\\det \\begin{pmatrix} \\frac{3}{2} - \\lambda  \\frac{1}{2} \\\\ \\frac{1}{2}  \\frac{3}{2} - \\lambda \\end{pmatrix} = \\left(\\frac{3}{2} - \\lambda\\right)^2 - \\left(\\frac{1}{2}\\right)^2 = 0$$\n$$ \\left(\\frac{3}{2} - \\lambda - \\frac{1}{2}\\right) \\left(\\frac{3}{2} - \\lambda + \\frac{1}{2}\\right) = 0 $$\n$$ (1-\\lambda)(2-\\lambda) = 0 $$\n特征值为 $\\lambda_1 = 1$ 和 $\\lambda_2 = 2$。由于两个特征值都严格为正，海森矩阵 $H$ 是正定的，因此目标函数 $f(\\mathbf{x})$ 是强凸的。\n\n接下来，我们推导仿射轮次算子 $\\mathbf{x}_{k+1} = T \\mathbf{x}_{k} + \\mathbf{h}$。对于从 $\\mathbf{x}_k \\equiv \\mathbf{x}_{k,1}$ 开始的一个轮次，其更新由以下两个连续步骤给出：\n$$ \\mathbf{x}_{k,2} = \\mathbf{x}_{k,1} - \\alpha A_{1}^{\\top} ( A_{1}\\mathbf{x}_{k,1} - \\mathbf{b}_{1} ) $$\n$$ \\mathbf{x}_{k+1} = \\mathbf{x}_{k,2} - \\alpha A_{2}^{\\top} ( A_{2}\\mathbf{x}_{k,2} - b_{2} ) $$\n使用定义 $H_i = A_i^{\\top}A_i$ 以及 $\\mathbf{g}_1 = A_1^{\\top}\\mathbf{b}_1$、$\\mathbf{g}_2 = A_2^{\\top}b_2$，更新步骤为：\n$$ \\mathbf{x}_{k,2} = (I - \\alpha H_1) \\mathbf{x}_{k,1} + \\alpha \\mathbf{g}_1 $$\n$$ \\mathbf{x}_{k+1} = (I - \\alpha H_2) \\mathbf{x}_{k,2} + \\alpha \\mathbf{g}_2 $$\n将 $\\mathbf{x}_{k,2}$ 的表达式代入第二个方程：\n$$ \\mathbf{x}_{k+1} = (I - \\alpha H_2) \\left[ (I - \\alpha H_1) \\mathbf{x}_{k,1} + \\alpha \\mathbf{g}_1 \\right] + \\alpha \\mathbf{g}_2 $$\n$$ \\mathbf{x}_{k+1} = (I - \\alpha H_2)(I - \\alpha H_1) \\mathbf{x}_{k,1} + \\alpha(I - \\alpha H_2)\\mathbf{g}_1 + \\alpha \\mathbf{g}_2 $$\n这是一个仿射映射 $\\mathbf{x}_{k+1} = T\\mathbf{x}_k + \\mathbf{h}$，其算子 $T$ 和偏移量 $\\mathbf{h}$ 由下式给出：\n$$ T = (I - \\alpha H_2)(I - \\alpha H_1) $$\n$$ \\mathbf{h} = \\alpha(I - \\alpha H_2)\\mathbf{g}_1 + \\alpha \\mathbf{g}_2 $$\n\n在稳态下，轮次开始的迭代序列收敛到一个不动点，$\\mathbf{x}_k \\to \\mathbf{x}_{\\mathrm{OS}}$。子集内部的迭代随后形成一个极限环。设当 $k \\to \\infty$ 时，$\\mathbf{x}_{k,1} \\to \\mathbf{x}_{\\mathrm{OS}}$ 且 $\\mathbfx_{k,2} \\to \\mathbf{y}_{\\mathrm{OS}}$。稳态更新变为：\n$$ \\mathbf{y}_{\\mathrm{OS}} = \\mathbf{x}_{\\mathrm{OS}} - \\alpha (H_1 \\mathbf{x}_{\\mathrm{OS}} - \\mathbf{g}_1) $$\n$$ \\mathbf{x}_{\\mathrm{OS}} = \\mathbf{y}_{\\mathrm{OS}} - \\alpha (H_2 \\mathbf{y}_{\\mathrm{OS}} - \\mathbf{g}_2) $$\n这两个方程定义了两点极限环 $\\mathbf{x}_{\\mathrm{OS}} \\leftrightarrow \\mathbf{y}_{\\mathrm{OS}}$。通过将第一个方程代入第二个方程，可以找到轮次算子的不动点 $\\mathbf{x}_{\\mathrm{OS}}$，得到 $ (H_1 + H_2 - \\alpha H_2 H_1) \\mathbf{x}_{\\mathrm{OS}} = \\mathbf{g}_1 + \\mathbf{g}_2 - \\alpha H_2 \\mathbf{g}_1 $。这证实了 $\\mathbf{x}_{\\mathrm{OS}}$ 是算子 $T$ 的一个不动点。\n\n极限环的振幅非零，即 $\\|\\mathbf{y}_{\\mathrm{OS}} - \\mathbf{x}_{\\mathrm{OS}}\\|_2 \\neq 0$，因为在循环中的任何一点，子集目标函数的梯度不会同时为零。循环振幅向量为 $\\mathbf{y}_{\\mathrm{OS}} - \\mathbf{x}_{\\mathrm{OS}} = -\\alpha (H_1 \\mathbf{x}_{\\mathrm{OS}} - \\mathbf{g}_1)$。其范数仅当 $H_1 \\mathbf{x}_{\\mathrm{OS}} - \\mathbf{g}_1 = \\mathbf{0}$ 时为零，这意味着第一个子集的代价函数在 $\\mathbf{x}_{\\mathrm{OS}}$ 处的梯度为零。如果这是真的，那么 $\\mathbf{y}_{\\mathrm{OS}} = \\mathbf{x}_{\\mathrm{OS}}$。将此代入第二个稳态方程将意味着 $H_2 \\mathbf{x}_{\\mathrm{OS}} - \\mathbf{g}_2 = \\mathbf{0}$。因此，零振幅循环（收敛到单一点）要求该点同时使两个子集代价函数的梯度都为零。这种情况仅当存在一个点 $\\mathbf{x}^*$ 使得 $H_1\\mathbf{x}^*=\\mathbf{g}_1$ 且 $H_2\\mathbf{x}^*=\\mathbf{g}_2$ 时才会发生。这等价于 $\\mathbf{x}^*$ 是完整问题的最小二乘解 $\\mathbf{x}_{\\mathrm{LS}}$，并且该解恰好也最小化了每个子集的代价函数。通常情况下，子集之间的数据是不一致的，意味着 $f_1(\\mathbf{x})$ 的最小化点不同于 $f_2(\\mathbf{x})$ 的最小化点。这被称为子集偏差。因此，OS算法收敛到围绕一个点 $\\mathbf{x}_{\\mathrm{OS}}$ 的极限环，该点不是真正的最小二乘解，并且在该点子集梯度不为零。非零梯度 $H_1\\mathbf{x}_{\\mathrm{OS}} - \\mathbf{g}_1$ 是导致非零循环振幅的直接原因。\n\n最后，我们计算给定实例的稳态循环振幅的欧几里得范数 $\\|\\mathbf{y}_{\\mathrm{OS}} - \\mathbf{x}_{\\mathrm{OS}}\\|_2$。\n我们有 $\\alpha = 1/2$。我们首先计算必要的矩阵和向量：\n$$ H_1 = A_1^{\\top} A_1 = I^{\\top}I = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} $$\n$$ \\mathbf{g}_1 = A_1^{\\top} \\mathbf{b}_1 = I \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\n$$ H_2 = A_2^{\\top} A_2 = \\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\right) \\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1  1 \\end{pmatrix}\\right) = \\frac{1}{2} \\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix} $$\n$$ \\mathbf{g}_2 = A_2^{\\top} b_2 = \\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\right) \\frac{1}{2} = \\frac{1}{2\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\n我们使用 $(H_1 + H_2 - \\alpha H_2 H_1) \\mathbf{x}_{\\mathrm{OS}} = \\mathbf{g}_1 + \\mathbf{g}_2 - \\alpha H_2 \\mathbf{g}_1$ 求解不动点 $\\mathbf{x}_{\\mathrm{OS}}$。\n项 $\\alpha H_2 \\mathbf{g}_1$ 简化为：\n$$ \\alpha H_2 \\mathbf{g}_1 = \\frac{1}{2} \\left( \\frac{1}{2} \\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix} \\right) \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix} 1-1 \\\\ 1-1 \\end{pmatrix} = \\mathbf{0} $$\n用于求解 $\\mathbf{x}_{\\mathrm{OS}}$ 的系统变为 $(H_1 + H_2 - \\alpha H_2 H_1) \\mathbf{x}_{\\mathrm{OS}} = \\mathbf{g}_1 + \\mathbf{g}_2$。\n左边的矩阵是：\n$$ H_1 + H_2 - \\frac{1}{2} H_2 I = \\begin{pmatrix} \\frac{3}{2}  \\frac{1}{2} \\\\ \\frac{1}{2}  \\frac{3}{2} \\end{pmatrix} - \\frac{1}{2} \\left( \\frac{1}{2} \\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix} \\right) = \\begin{pmatrix} \\frac{3}{2}-\\frac{1}{4}  \\frac{1}{2}-\\frac{1}{4} \\\\ \\frac{1}{2}-\\frac{1}{4}  \\frac{3}{2}-\\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{4}  \\frac{1}{4} \\\\ \\frac{1}{4}  \\frac{5}{4} \\end{pmatrix} $$\n右边的向量是：\n$$ \\mathbf{g}_1 + \\mathbf{g}_2 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} + \\frac{1}{2\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 + \\frac{\\sqrt{2}}{4} \\\\ -1 + \\frac{\\sqrt{2}}{4} \\end{pmatrix} $$\n我们求解 $\\frac{1}{4}\\begin{pmatrix} 5  1 \\\\ 1  5 \\end{pmatrix} \\mathbf{x}_{\\mathrm{OS}} = \\begin{pmatrix} 1 + \\frac{\\sqrt{2}}{4} \\\\ -1 + \\frac{\\sqrt{2}}{4} \\end{pmatrix}$，即 $\\begin{pmatrix} 5  1 \\\\ 1  5 \\end{pmatrix} \\mathbf{x}_{\\mathrm{OS}} = \\begin{pmatrix} 4 + \\sqrt{2} \\\\ -4 + \\sqrt{2} \\end{pmatrix}$。\n$\\begin{pmatrix} 5  1 \\\\ 1  5 \\end{pmatrix}$ 的逆是 $\\frac{1}{24}\\begin{pmatrix} 5  -1 \\\\ -1  5 \\end{pmatrix}$。\n$$ \\mathbf{x}_{\\mathrm{OS}} = \\frac{1}{24} \\begin{pmatrix} 5  -1 \\\\ -1  5 \\end{pmatrix} \\begin{pmatrix} 4 + \\sqrt{2} \\\\ -4 + \\sqrt{2} \\end{pmatrix} = \\frac{1}{24} \\begin{pmatrix} 5(4+\\sqrt{2}) - (-4+\\sqrt{2}) \\\\ -(4+\\sqrt{2}) + 5(-4+\\sqrt{2}) \\end{pmatrix} = \\frac{1}{24} \\begin{pmatrix} 24 + 4\\sqrt{2} \\\\ -24 + 4\\sqrt{2} \\end{pmatrix} = \\begin{pmatrix} 1 + \\frac{\\sqrt{2}}{6} \\\\ -1 + \\frac{\\sqrt{2}}{6} \\end{pmatrix} $$\n循环振幅向量为 $\\mathbf{y}_{\\mathrm{OS}} - \\mathbf{x}_{\\mathrm{OS}} = -\\alpha (H_1 \\mathbf{x}_{\\mathrm{OS}} - \\mathbf{g}_1)$。由于 $H_1=I$：\n$$ \\mathbf{y}_{\\mathrm{OS}} - \\mathbf{x}_{\\mathrm{OS}} = -\\frac{1}{2} (\\mathbf{x}_{\\mathrm{OS}} - \\mathbf{g}_1) = -\\frac{1}{2} \\left( \\begin{pmatrix} 1 + \\frac{\\sqrt{2}}{6} \\\\ -1 + \\frac{\\sqrt{2}}{6} \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\right) = -\\frac{1}{2} \\begin{pmatrix} \\frac{\\sqrt{2}}{6} \\\\ \\frac{\\sqrt{2}}{6} \\end{pmatrix} = \\begin{pmatrix} -\\frac{\\sqrt{2}}{12} \\\\ -\\frac{\\sqrt{2}}{12} \\end{pmatrix} $$\n该向量的欧几里得范数是：\n$$ \\|\\mathbf{y}_{\\mathrm{OS}} - \\mathbf{x}_{\\mathrm{OS}}\\|_{2} = \\sqrt{\\left(-\\frac{\\sqrt{2}}{12}\\right)^2 + \\left(-\\frac{\\sqrt{2}}{12}\\right)^2} = \\sqrt{\\frac{2}{144} + \\frac{2}{144}} = \\sqrt{\\frac{4}{144}} = \\sqrt{\\frac{1}{36}} = \\frac{1}{6} $$\n稳态循环振幅的欧几里得范数为 $\\frac{1}{6}$。", "answer": "$$\\boxed{\\frac{1}{6}}$$", "id": "4900879"}]}