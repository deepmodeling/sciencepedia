## 应用与跨学科连接

在前面的章节中，我们已经深入探讨了[卷积神经网络](@entry_id:178973)（CNN）的核心原理与机制。我们理解了卷积、池化、[非线性激活](@entry_id:635291)以及[反向传播](@entry_id:199535)等基本构件如何协同工作，以从图像数据中学习层次化的特征表示。现在，我们将从“如何工作”转向“在何处以及为何有效”的问题。本章的目的是展示这些核心原理在多样化的真实世界和跨学科背景下的实际应用，从而揭示CNN在现代科学与工程，尤其是在[医学影像](@entry_id:269649)分析领域中的强大效用。

在我们深入具体应用之前，有必要从一个更高的视角理解CNN所带来的范式转变。在传统（或称为“经典”）的机器学习图像分析流程中，任务通常被分解为两个独立的阶段：首先，由领域专家依据先验知识设计一套“手工特征”（hand-crafted features）提取算法，将原始图像[数据转换](@entry_id:170268)为一个固定长度的特征向量；然后，将这些特征向量输入到一个独立的预测模型（如[支持向量机](@entry_id:172128)或随机森林）中进行训练。这种方法的有效性在很大程度上取决于[特征工程](@entry_id:174925)的质量，而手工设计的特征可能无法捕捉到与任务相关的全部复杂模式。与之相对，CNN引领的“端到端”（end-to-end）学习范式将特征提取和模型预测整合到一个统一、可微的框架中。整个网络，从输入像素到最终输出，都通过最小化任务[损失函数](@entry_id:136784)进行联合优化。这种方法允许模型直接从原始数据中自主学习与任务最相关的分层特征，从而避免了手工设计特征的局限性。[CNN架构](@entry_id:635079)本身所包含的[归纳偏置](@entry_id:137419)（inductive biases），例如通过卷积实现的局部性和[平移等变性](@entry_id:636340)，使其尤其擅长处理图像数据，这种架构性先验知识不同于经典放射组学中基于特定纹理或形态学假设的先验知识。正是这种端到端的学习能力和为图像数据量身定制的[归纳偏置](@entry_id:137419)，构成了CNN在众多应用中取得成功的基石 [@problem_id:4534170]。

### 核心临床应用：[医学图像分割](@entry_id:636215)

[医学图像分割](@entry_id:636215)是CNN最成功和最具影响力的应用领域之一，其目标是在像素或体素层面为图像的不同区域分配类别标签（例如，区分肿瘤与健康组织）。这项任务是定量分析、疾病诊断、治疗规划和手术导航的基础。

#### 任务定义与架构选择

首先，必须明确分割（segmentation）与其他相关任务的区别。分割是一个逐像素的标记问题，而图像配准（registration）则是寻找一个空间变换，将一幅图像与另一幅对齐。例如，在一项脑肿瘤治疗流程中，术前勾画肿瘤区域是一项分割任务，而将术前扫描与术中扫描对齐则是一项配准任务 [@problem_id:4857503]。

对于[医学图像分割](@entry_id:636215)，[U-Net](@entry_id:635895)及其变体已成为事实上的标准架构。[U-Net](@entry_id:635895)采用了一种对称的[编码器-解码器](@entry_id:637839)结构。编码器（或称收缩路径）通过一系列[卷积和](@entry_id:263238)[下采样](@entry_id:265757)（如[最大池化](@entry_id:636121)）操作，逐步减少空间维度并增加通道数，从而捕捉图像的上下文信息和多尺度特征。解码器（或称扩张路径）则通过[上采样](@entry_id:275608)（如[转置卷积](@entry_id:636519)）和卷积操作，逐步恢复空间分辨率，最终生成与输入图像大小相同的分割图。[U-Net](@entry_id:635895)设计的点睛之笔在于“[跳跃连接](@entry_id:637548)”（skip connections），它将编码器中相应层级的高分辨率特征图直接拼接到解码器的[上采样](@entry_id:275608)输出上。这种特征融合机制使得解码器能够同时利用深层的语义信息和浅层的精细空间细节，从而在精确勾画复杂结构边界时表现出色。这一架构特性对于需要保留空间细节的密集预测任务至关重要 [@problem_id:4354073] [@problem_id:4857503]。

#### 处理三维容积数据

[医学影像](@entry_id:269649)本质上常常是三维的（例如CT和MRI扫描），这为[CNN架构](@entry_id:635079)设计带来了新的考量。最直接的方法是使用三维CNN，它将[二维卷积](@entry_id:275218)核扩展为三维（例如，$3 \times 3 \times 3$），直接在$x, y, z$三个空间轴上进[行运算](@entry_id:149765)。这种方法能够完整地捕捉三维空间上下文，因为网络的感受野（receptive field）在所有三个维度上都会随[网络深度](@entry_id:635360)的增加而增长。

然而，三维CNN的计算和内存开销巨大。一种替代方案是采用二维CNN，逐个处理三维容积的每个二维切片（例如，逐个处理轴向切片）。这种方法的计算效率更高，但其核心局限在于，每个切片被独立处理，网络无法直接学习切片之间的（即穿透平面，through-plane）空间依赖关系。对于一个纯粹的二维网络，其在$z$轴上的感受野始终为1，除非采用特殊设计（如输入多通道切片）来引入跨切片信息 [@problem_id:4491608]。

在处理各向异性（anisotropic）数据时，架构设计需要更加精细。例如，临床[CT扫描](@entry_id:747639)通常具有很高的平面内分辨率（如$s_x = s_y = 0.8 \, \mathrm{mm}$）但较低的穿透平面分辨率（即层厚较大，如$s_z = 5.0 \, \mathrm{mm}$）。在这种情况下，直接使用各向同性的$3 \times 3 \times 3$[卷积核](@entry_id:635097)可能不合适，因为它在物理空间上覆盖的$z$轴范围远大于$x, y$轴。一个有效的策略是在网络的早期层使用各向异性的卷积核，例如$3 \times 3 \times 1$。这种卷积核只在平面内进行[信息聚合](@entry_id:137588)，而不跨越物理距离较远的切片。这不仅可以减少参数数量和计算量（一个$3 \times 3 \times 1$卷积层的参数量仅为$3 \times 3 \times 3$卷积层的三分之一），还能使网络首先学习稳健的二维特征，避免被层间伪影过度影响。随后，在网络的更深层级再引入$3 \times 3 \times 3$的卷积核，以在更高层次的[特征图](@entry_id:637719)上聚合三维上下文信息。值得注意的是，网络的[感受野大小](@entry_id:634995)取决于卷积核的尺寸和步长，而非通道数。即使为了维持参数预算而将$3 \times 3 \times 1$卷积层的输出通道数增加三倍，也无法恢复因使用$k_z=1$而损失的穿透平面感受野 [@problem_id:4897453]。

#### 训练与评估策略

在分割任务中，目标区域（如肿瘤）往往只占整个图像的一小部分，这导致了严重的类别不平衡问题。如果使用标准的[交叉熵损失](@entry_id:141524)函数，模型很容易通过将所有像素预测为背景来获得很高的准确率，从而完全忽略了前景目标。为了解决这个问题，基于区域重叠的[损失函数](@entry_id:136784)被广泛采用，其中最著名的是Dice损失。Dice损失直接优化预测掩码与真实掩码之间的Dice相似系数（Dice Similarity Coefficient, DSC），这是一个衡量体积重叠度的指标。由于Dice系数是一个比值，它对前景和背景的大小不敏感，因此能有效应对[类别不平衡](@entry_id:636658) [@problem_id:4857503]。

评估[分割模](@entry_id:138050)型的性能同样需要精心选择指标。除了在训练中使用的Dice系数外，还有一系列基于[混淆矩阵](@entry_id:635058)的指标，如灵敏度（sensitivity，又称召回率recall或[真阳性率](@entry_id:637442)），特异性（specificity，真阴性率），精确率（precision，阳性预测值）和准确率（accuracy）。它们的定义如下：
- 灵敏度/召回率 = $\dfrac{TP}{TP+FN}$，衡量所有真实正例中被正确识别的比例。
- 特异性 = $\dfrac{TN}{TN+FP}$，衡量所有真实负例中被正确识别的比例。
- 精确率 = $\dfrac{TP}{TP+FP}$，衡量所有预测为正例的样本中真正为正例的比例。
- 准确率 = $\dfrac{TP+TN}{TP+FP+TN+FN}$，衡量所有样本中被正确分类的比例 [@problem_id:4897430]。

在许多临床场景中，仅仅衡量体积重叠度是不够的。例如，在脑肿瘤分割中，神经外科医生可能特别关注避免在功能关键区（eloquent cortex）附近出现大的局部边界误差。Dice系数对此类局部误差不敏感。为了捕捉这种最坏情况下的边界偏差，需要使用基于距离的度量。[豪斯多夫距离](@entry_id:152367)（Hausdorff distance）是其中一个关键指标。它被定义为两个点集之间相互的最大最小距离，即$H(S,T)=\max\{\sup_{\mathbf{x}\in S} d(\mathbf{x},T), \sup_{\mathbf{y}\in T} d(\mathbf{y},S)\}$，其中$d(\mathbf{x}, A)$表示点$\mathbf{x}$到集合$A$的最短距离。[豪斯多夫距离](@entry_id:152367)能够有效地量化两个边界表面的最大不匹配程度。与之相对，平均表面距离（Average Surface Distance, ASD）则计算所有[边界点](@entry_id:176493)到对方表面的平均距离。在一个几乎完美的分割中，单个孤立的异常值（outlier）会使[豪斯多夫距离](@entry_id:152367)的值等于该异常值到真实表面的距离，而ASD则会将这个误差在所有[边界点](@entry_id:176493)上平均掉，因此其值会小得多。这表明，[豪斯多夫距离](@entry_id:152367)对局部大误差高度敏感，而ASD则更为稳健。因此，一个全面的评估方案通常会同时报告重叠度量（如Dice系数）和边界[距离度量](@entry_id:636073)（如[豪斯多夫距离](@entry_id:152367)），以从不同角度评价分割质量 [@problem_id:4857503] [@problem_id:4897452]。

### 超越分割：分类与放射基因组学

除了分割，CNN在图像级分类和更复杂的跨[模态分析](@entry_id:163921)中也发挥着核心作用。

#### 图像级分类与病理学应用

在许多诊断任务中，目标是为整个图像或容积分配一个标签，例如在[光学相干断层扫描](@entry_id:173275)（OCT）容积中检测是否存在年龄相关性黄斑变性（AMD）[@problem_id:4650585]。在这种情况下，CNN通常被设计为输出一个或多个代表类别概率的标量值。

数字病理学是CNN应用的另一个重要领域。组织学图像（如苏木精-伊红（H [@problem_id:4354073]。

然而，数字病理学中的一个独特挑战是全切片图像（Whole-Slide Image, WSI）的巨大尺寸，其分辨率可达数亿甚至数十亿像素。直接将这样的图像输入CNN是不现实的。此外，诊断标签通常是切片级别的，而判别性特征（如少数癌细胞）可能只出现在切片的稀疏、微小的区域。这种“包”（bag，即整个切片）和“实例”（instance，即切片中的小块区域）之间的关系，正是多实例学习（Multiple Instance Learning, MIL）框架所要解决的问题。在这种情况下，传统的[CNN架构](@entry_id:635079)面临挑战，而像视觉变换器（Vision Transformer, ViT）这样的模型则显示出巨大潜力。ViT将[图像分割](@entry_id:263141)成一系[列图像](@entry_id:150789)块（patch）或“令牌”（token），并通过[自注意力](@entry_id:635960)（self-attention）机制来建模这些图像块之间的长距离依赖关系。这种全局信息整合能力使其天然适合于从众多实例中聚合证据以做出包级别的预测。此外，ViT的弱[归纳偏置](@entry_id:137419)可以通过在大量无标签图像块上进行自监督预训练来弥补，使其成为WSI分析等[弱监督](@entry_id:176812)、长距离依赖任务的有力候选者 [@problem_id:5184434]。

#### 放射基因组学与多模态融合

放射基因组学（Radiogenomics）是一个新兴的交叉学科领域，旨在建立医学影像特征与基因组数据（如[基因突变](@entry_id:166469)状态、基因表达谱）之间的联系。在这个领域，[机器学习模型](@entry_id:262335)扮演着桥梁的角色。除了直接使用监督学习的CNN从图像$x$预测基因组标签$y$之外，还有其他重要的范式。

一种是利用[无监督学习](@entry_id:160566)进行[特征提取](@entry_id:164394)。例如，自编码器（Autoencoder）通过训练一个编码器$g_{\phi}$和一个解码器$h_{\psi}$来最小化图像的重建误差，即$\mathbb{E}_{x}[\|x - h_{\psi}(g_{\phi}(x))\|^{2}]$。这个过程不使用基因组标签$y$，但其目标是学习一个能够捕捉图像主要变化模式的低维潜在表示$z = g_{\phi}(x)$。这个潜在表示$z$随后可以作为特征，输入到一个独立的、更简单的分类器中来预测$y$。然而，纯粹基于重建损失学习到的表示可能主要捕捉了与基因组标签无关的“无关变异”（nuisance variation）。为了解决这个问题，一种有效的实践是在训练自编码器时加入一个辅助的监督损失项，即同时优化重建损失和预测$y$的[分类损失](@entry_id:634133)。这种半监督或[多任务学习](@entry_id:634517)方法可以引导模型学习到既能很好地重建图像，又对下游任务具有判别力的特征表示。

另一种更强大的范式是多模态融合（Multimodal Fusion）。现实世界中的决策往往依赖于多种信息来源。在放射基因组学中，模型可以被设计为同时接收图像数据$x$和非图像数据（如临床协变量$u$），以联合预测基因组标签$y$。融合策略多种多样，例如，“早期融合”在特征层面将来自不同模态的表示（而非原始数据）连接起来，再送入后续的预测网络；而“晚期融合”则为每个模态训练一个独立的预测器，最后再整合它们的预测结果。通过有效融合多种互补信息，多模态模型通常能达到比任何单一模态模型更好的性能 [@problem_id:4557668]。

### 真实世界部署的实践考量

将一个在精心整理的数据集上表现良好的CNN模型部署到复杂的、异构的临床环境中，会面临一系列独特的挑战。

#### [数据预处理](@entry_id:197920)与[迁移学习](@entry_id:178540)

来自不同医院、不同设备的医学图像通常存在显著的异质性，例如不同的体素间距、不同的重建算法和不同的强度尺度，这构成了“[协变量偏移](@entry_id:636196)”（covariate shift）问题。为了使模型能够泛化到不同的数据源，一个标准化的预处理流水线至关重要。例如，在处理多中心CT数据时，一个稳健的流程通常包括：(1) **Hounsfield单位（HU）窗宽窗位调整**，将强度值裁剪到感兴趣的组织范围内（如用于观察肺结节的肺窗），以抑制无关信号；(2) **几何重采样**，将所有容积插值到统一的各向同性体素间距（如$1 \times 1 \times 1 \, \mathrm{mm}$），以确保卷积核在物理空间中具有一致的感受野；(3) **强度归一化**，例如对每个容积进行z-score归一化，使其均值为0，标准差为1，以稳定网络激活值的分布。这些步骤的正确顺序和实施对于降低数据异质性、提高[模型泛化](@entry_id:174365)能力至关重要 [@problem_id:4568514]。

[迁移学习](@entry_id:178540)（Transfer Learning）是应对数据稀缺问题的常用策略，即将在大规模数据集（如自然图像）上预训练的模型的权重作为新任务的初始化。研究表明，CNN的这种可迁移性并非在所有层级上都相同。网络的早期卷积层主要学习通用的、低级的特征，如边缘、角点和纹理。这些基本结构在不同类型的图像中是共有的。例如，在不同模态（如CT和MRI）之间，尽管[强度分布](@entry_id:163068)截然不同，但解剖结构的边缘在经过局部单调的强度重映射后仍然存在。因此，早期层的特征是高度可迁移的。相比之下，网络的深层则学习更高级、更抽象、更依赖于特定任务的语义特征（例如，自然图像中的“车轮”或“眼睛”）。这些特征在新的医学影像领域中往往没有直接对应物，因此其可迁移性较差 [@problem_id:4568450]。

#### 内存限制与推理策略

处理高分辨率医学图像（尤其是三维容积）时，GPU内存往往成为瓶颈。当无法将整个图像一次性送入网络时，一种常见的策略是进行“基于图像块的训练”（patch-based training），即从大图中随机或系统地裁剪出小图像块进行训练。这种方法虽然解决了内存问题，但也引入了新的挑战：模型在训练时看到的人工边界远多于在处理整张图时的情况，并且缺乏长距离上下文信息。

在推理阶段，为了得到整张图的预测结果，必须采用一种拼接策略。简单地将无重叠的图像块的预测结果拼接在一起，会在块与块的边界处产生明显的“接缝伪影”。这是因为在每个图像块的边缘区域，其预测依赖于网络输入端的[零填充](@entry_id:637925)（zero-padding），因此是不可靠的。一个严谨的解决方案是“重叠-切片”策略（overlap-tile strategy）。该策略的核心思想是，每次预测一个图像块，但只保留其中心“有效区域”的预测结果，并丢弃边缘的不可靠部分。一个输出像素的“有效性”取决于其整个[感受野](@entry_id:636171)是否完全落在输入的图像块内部。为了无缝覆盖整个大图，相邻图像块之间必须有足够的重叠，以确保一个图像块的无效边缘区域能被相邻图像块的有效中心区域所覆盖。所需的最小重叠量可以直接由[网络架构](@entry_id:268981)的[感受野大小](@entry_id:634995)推导出来。例如，对于一个[感受野大小](@entry_id:634995)为$r$的网络，其输出的单侧无效边界宽度为$R=(r-1)/2$，因此相邻图像块的最小重叠应为$O=2R=r-1$ [@problem_id:4897417]。

#### 对[域漂移](@entry_id:637840)的稳健性

即使经过了标准化预处理，不同扫描仪之间的细微差异仍然可能导致模型性能下降，这种现象称为“[域漂移](@entry_id:637840)”（domain shift）。批归一化（Batch Normalization, BN）层是导致这种问题的一个常见根源。BN层在训练期间使用当前批次数据的均值和方差来归一化激活值，并维护一个用于推理的全局“运行统计量”。当一个在A中心数据上训练的模型（存储了A中心的运行统计量$\mu_A, \sigma_A$）被用于B中心的数据时，如果B中心的数据激活值分布（均值为$\mu_B$，标准差为$\sigma_B$）与A中心不同，使用不匹配的统计量进行归一化会导致严重的性能下降。

为了解决这个问题，可以在测试时采取一些自适应策略。一种有效的方法是“测试时归一化”（Test-Time Normalization），即在对单个测试样本进行推理时，使用该样本自身（或一个小批次测试样本）的统计量$(\mu_{\text{test}}, \sigma_{\text{test}})$来代替存储的训练时运行统计量。另一种在数学上等价的方法是，保持使用$\mu_A, \sigma_A$进行归一化，但动态地调整BN层的仿射变换参数$(\gamma, \beta)$以补偿分布的差异。这两种方法都能在理论上完美地校正线性的[域漂移](@entry_id:637840)，并且由于它们只使用当前测试样本的信息，避免了从整个测试集中“数据泄露”的问题，是符合实际部署规范的稳健性增强技术 [@problem_id:4897405]。

### 迈向可信AI：可解释性与不确定性

在医疗等高风险领域，模型的预测准确率本身并不足够。为了让临床医生信任并负责任地使用AI系统，模型还需要提供可解释的洞见，并量化其预测的不确定性。

#### [模型可解释性](@entry_id:171372)

[模型可解释性](@entry_id:171372)旨在阐明模型做出特定预测的原因。一种基本方法是生成“[显著性图](@entry_id:635441)”（saliency map），即一张热力图，高亮显示输入图像中对模型输出贡献最大的区域。这与传统[特征工程](@entry_id:174925)形成对比：[特征工程](@entry_id:174925)是学习前的步骤，它约束了模型能看到的信息；而显著性分析是学习后的步骤，它解释一个已经训练好的模型 [@problem_id:4650585]。

最简单的[显著性图](@entry_id:635441)是基于模型输出对输入像素的梯度来计算的。然而，这种简单的梯度图存在严重缺陷。其一，由于网络中存在[饱和非线性](@entry_id:271106)（如Sigmoid或ReLU），当一个特征的激活值已经很高时，其梯度可能趋于零，导致重要的像素在梯度图中“消失”，这称为“饱和问题”。其二，深度网络对于输入的微小扰动可能产生剧烈变化的梯度，导致梯度图充满视觉噪声。

为了获得更稳定和可靠的归因，研究者们提出了更先进的方法。例如，**SmoothGrad**通过在输入图像周围添加少量高斯噪声，计算多次带噪输入的梯度并取其平均值，从而平滑掉高频噪声。**[积分梯度](@entry_id:637152)（Integrated Gradients, IG）**则提供了一种更具原则性的方法。它将特征的归因定义为从一个信息量为零的“基线”（baseline）图像（如全黑图像）到当前输入图像的直线路徑上，梯度沿该路径的积分。IG方法满足“完备性”公理，即所有像素的归因值之和等于模型输出与基线输出之差，并且通[过积分](@entry_id:753033)的方式有效缓解了梯度饱和问题 [@problem_id:4897414]。

#### [量化不确定性](@entry_id:272064)

一个可信的模型不仅应该给出预测，还应该知道自己“何时不知道”。预测不确定性可以分解为两种类型：
- **[认知不确定性](@entry_id:149866)（Epistemic Uncertainty）**：源于模型本身知识的局限，例如训练数据不足或存在偏差。这种不确定性是模型相关的，可以通过增加训练数据来降低。
- **[偶然不确定性](@entry_id:154011)（Aleatoric Uncertainty）**：源于数据本身的内在噪声和模糊性，例如由于部分容积效应导致的组织边界不清，或图像采集过程中的随机噪声。这种不确定性是数据相关的，即使模型完美也无法消除。

在[贝叶斯深度学习](@entry_id:633961)框架下，这两种不确定性可以被明确地建模和分离。**认知不确定性**可以通过对模型参数的后验分布进行[近似推断](@entry_id:746496)来估计。[蒙特卡洛丢弃](@entry_id:636300)（[Monte Carlo Dropout](@entry_id:636300), MC Dropout）是一种实用的近似方法。通过在测试时保持Dropout层激活，并进行多次随机前向传播，可以得到一组不同的预测结果。这组预测结果的方差（或熵、互信息等）就反映了模型对其参数的不确定性，即[认知不确定性](@entry_id:149866)。**[偶然不确定性](@entry_id:154011)**则可以通过让模型直接预测一个与输入相关的方差来建模，这被称为异方差（heteroscedastic）模型。例如，在分割任务中，网络可以被设计为对每个体素同时输出预测的类别概率和预测的“噪声水平”或方差。总的预测不确定性是这两部分不确定性的组合，其关系可以通过全变异数定律（Law of Total Variance）来形式化地描述 [@problem_id:4897419]。

### 新兴架构与未来展望

CNN领域在不断发展，新的架构正在涌现，挑战着传统卷积的主导地位。

#### CNN vs. 视觉变换器 (ViT)

近年来，最初为自然语言处理设计的[Transformer架构](@entry_id:635198)，经改造后以视觉变换器（ViT）的形式在计算机视觉领域取得了巨大成功。与CNN相比，ViT具有根本不同的[归纳偏置](@entry_id:137419)。CNN通过局部[卷积核](@entry_id:635097)和层级结构内在地编码了局部性和[平移等变性](@entry_id:636340)，这使其在数据量较少时更为高效。而ViT将图像视为一系[列图像](@entry_id:150789)块（token），并通过[自注意力机制](@entry_id:638063)直接建模所有图像块之间的全局关系，其局部性[归纳偏置](@entry_id:137419)要弱得多，因此通常需要更大规模的数据集进行训练才能达到或超过CNN的性能。

在[医学影像](@entry_id:269649)领域，选择CNN还是ViT并非一成不变，而应取决于具体任务的特性：
-   对于**数据量小、需要强局部先验**的任务，如从少量样本（例如$n=80$）进行三维MRI脑肿瘤分割，传统的3D CNN（如3D [U-Net](@entry_id:635895)）因其强大的[归纳偏置](@entry_id:137419)和更低的计算/内存开销而通常是更合适的选择。
-   对于**涉及长距离依赖、[弱监督](@entry_id:176812)**的任务，如基于全切片图像（WSI）的病理分类，ViT的全局[注意力机制](@entry_id:636429)和对多实例学习（MIL）框架的天然适配性使其成为理想选择，特别是当有大量无标签数据可用于自监督预训练时。
-   对于**数据量大、且病理特征跨越多个尺度**的任务，如从数十万张眼底照片中进行糖尿病视网膜病变分级（其中既有微小的微动脉瘤，也有广泛的血管改变），纯粹的CNN或ViT可能都不是最优的。一种强大的策略是构建**混合架构**，例如，在模型前端使用一个“卷积主干”（convolutional stem）来高效地提取局部[特征和](@entry_id:189446)生成高质量的图像块表示，然后将这些表示送入后续的Transformer模块中进行全局关系建模。这种混合模型结合了CNN的局部[特征提取](@entry_id:164394)能力和ViT的全局上下文建模能力，往往能在准确性、数据效率和对微小病灶的敏感性之间取得最佳平衡 [@problem_id:5184434]。

本章通过一系列具体的应用案例，展示了[卷积神经网络](@entry_id:178973)的核心原理如何在[医学影像](@entry_id:269649)分析的各个层面——从核心的分割、[分类任务](@entry_id:635433)，到处理[多模态数据](@entry_id:635386)、应对实际部署挑战，再到追求模型的可信度——发挥作用。我们看到，一个成功的应用不仅需要深刻理解CNN的理论基础，还需要根据具体问题的物理特性、数据属性和临床需求，进行精巧的架构设计、严谨的训练与评估，以及对前沿方法的批判性吸收。随着模型、数据和计算能力的不断进步，CNN及其后继者必将在推动精准医疗和生物医学研究中扮演越来越重要的角色。