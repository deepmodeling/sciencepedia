{"hands_on_practices": [{"introduction": "卷积神经网络（CNN）由许多层堆叠而成，而其核心构建单元便是卷积层。在设计和评估一个网络架构时，理解每一层的计算开销和参数数量至关重要，因为这直接影响模型的训练速度和内存占用。这项实践将引导你从第一性原理出发，推导出一个标准二维卷积层的可学习参数总量和计算复杂度，为你优化网络设计打下坚实基础。[@problem_id:4897442]", "problem": "考虑一个卷积神经网络（CNN）中的二维卷积层，应用于磁共振成像（MRI）切片以进行组织分割。该层接收一个具有 $C_{\\text{in}}$ 个通道的输入特征图，并通过与 $C_{\\text{out}}$ 个可学习的滤波器进行卷积，生成 $C_{\\text{out}}$ 个输出通道，每个滤波器的空间尺寸为 $k \\times k$。假设如下：\n- 该操作是标准的稠密卷积（无分组、无深度可分离、无膨胀）。\n- 步幅为 $1$，且填充不影响每个输出像素的操作计数。\n- 每个滤波器都有一个标量偏置，在其对应的输出通道的卷积求和之后添加。\n- 一次乘加运算（MAC）定义为一次乘法紧随其后的一次累加，将结果累积到一个运行总和中。\n从离散卷积的定义以及将滤波器参数化为在 $k \\times k$ 空间支持域上连接输入通道与输出通道的权重出发，推导以下两个量：\n- 该层中可学习参数的总数。\n- 计算单个空间位置（即所有输出通道上的每个输出像素）上所有 $C_{\\text{out}}$ 个输出所需的乘加运算（MAC）次数。仅计算与权重应用相关的 MAC 次数；不要将偏置加法计入 MAC 次数。\n用 $C_{\\text{in}}$、$C_{\\text{out}}$ 和 $k$ 表示的闭式符号表达式来表达您的最终结果。这些是无量纲的计数；请以无单位的解析表达式报告您的答案。不需要进行数值评估或四舍五入。", "solution": "问题陈述经过验证。\n\n### 步骤 1：提取已知条件\n- 考虑一个CNN中的二维卷积层。\n- 输入特征图有 $C_{\\text{in}}$ 个通道。\n- 输出特征图有 $C_{\\text{out}}$ 个通道。\n- 该层使用 $C_{\\text{out}}$ 个可学习的滤波器。\n- 每个滤波器的空间尺寸为 $k \\times k$。\n- 操作是标准的稠密卷积。\n- 步幅为 $1$。\n- 填充不影响每个输出像素的操作计数。\n- 每个滤波器为其对应的输出通道提供一个标量偏置。\n- 一次乘加（MAC）运算是一次乘法和一次累加。\n- 目标是推导两个量：可学习参数的总数和每个输出像素的MACs数量。\n- 偏置加法不计入MAC计数。\n- 最终表达式应以 $C_{\\text{in}}$、$C_{\\text{out}}$ 和 $k$ 表示。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题具有科学依据，描述了标准的二维卷积，这是机器学习和计算神经科学中的一个基本操作。问题提法清晰，提供了所有必要的变量和约束（$C_{\\text{in}}$、$C_{\\text{out}}$、$k$、标准卷积、MAC定义），以推导出唯一且有意义的符号表达式。语言客观而精确。问题是自洽的，没有矛盾。诸如步幅为 $1$ 和对填充的澄清等假设，旨在将问题简化为其核心计算，而不失对单点计算的一般性，这是一种标准的教学方法。该问题不违反任何无效性标准。\n\n### 步骤 3：结论与行动\n问题有效。现在将提供所要求数量的严格推导。\n\n推导分两部分进行，分别解决每个要求的量。\n\n#### 第 1 部分：可学习参数的总数\n标准卷积层中的可学习参数包括两个组成部分：滤波器权重和偏置项。\n\n1.  **滤波器权重**：该层旨在将一个具有 $C_{\\text{in}}$ 个通道的输入转换为一个具有 $C_{\\text{out}}$ 个通道的输出。这需要 $C_{\\text{out}}$ 个不同的滤波器。每个滤波器必须处理输入特征图的整个深度。因此，单个滤波器不仅仅是一个 $k \\times k$ 的矩阵，而是一个维度为 $k \\times k \\times C_{\\text{in}}$ 的三维张量。该张量包含的权重，用于将所有 $C_{\\text{in}}$ 个输入通道上的一个 $k \\times k$ 空间区域连接到其中一个输出通道中的单个值。\n    这样一个滤波器中的权重参数数量是其维度的乘积：\n    $$\n    \\text{每个滤波器的参数} = k \\times k \\times C_{\\text{in}} = k^2 C_{\\text{in}}\n    $$\n    因为有 $C_{\\text{out}}$ 个独立的滤波器来生成 $C_{\\text{out}}$ 个输出通道，所以总权重参数数量是滤波器数量与每个滤波器参数数量的乘积：\n    $$\n    \\text{总权重参数} = C_{\\text{out}} \\times (\\text{每个滤波器的参数}) = C_{\\text{out}} \\times (k^2 C_{\\text{in}}) = k^2 C_{\\text{in}} C_{\\text{out}}\n    $$\n\n2.  **偏置项**：问题陈述指出，每个滤波器都有一个标量偏置，加到其对应的输出通道上。由于有 $C_{\\text{out}}$ 个输出通道，每个通道由一个不同的滤波器产生，因此总共必须有 $C_{\\text{out}}$ 个偏置项。\n    $$\n    \\text{总偏置参数} = C_{\\text{out}}\n    $$\n\n3.  **总参数**：可学习参数的总数是总权重参数和总偏置参数之和。\n    $$\n    \\text{总参数} = (\\text{总权重参数}) + (\\text{总偏置参数}) = k^2 C_{\\text{in}} C_{\\text{out}} + C_{\\text{out}}\n    $$\n    这个表达式可以因式分解为更紧凑的形式：\n    $$\n    \\text{总参数} = C_{\\text{out}} (k^2 C_{\\text{in}} + 1)\n    $$\n\n#### 第 2 部分：每个输出像素的乘加（MAC）运算次数\n我们被要求计算在输出特征图的单个空间位置上，计算所有 $C_{\\text{out}}$ 个通道的值所需的MACs数量。问题明确指出，偏置加法不计为MACs。\n\n1.  **单个输出通道的MACs**：让我们首先考虑单个输出通道中单个像素的计算，比如通道 $j$。该像素的值是第 $j$ 个滤波器与输入特征图上一个对应的 $k \\times k$ 空间区域进行卷积运算的结果。\n    输入区域的维度为 $k \\times k \\times C_{\\text{in}}$。第 $j$ 个滤波器的维度也为 $k \\times k \\times C_{\\text{in}}$。此位置的离散卷积是展平的滤波器张量与展平的输入区域张量之间的点积。这涉及将 $k^2 C_{\\text{in}}$ 个滤波器权重与对应的 $k^2 C_{\\text{in}}$ 个输入值进行逐元素相乘，然后将所有这些乘积求和。\n    因此，乘法的总次数为 $k^2 C_{\\text{in}}$。这些乘积随后被累加成一个总和。MAC运算定义为一次乘法和一次累加，这与该过程完美契合。因此，计算一个输出通道中一个像素的激活前值需要 $k^2 C_{\\text{in}}$ 次MAC运算。\n\n2.  **所有输出通道的MACs**：$C_{\\text{out}}$ 个输出通道中每一个的计算都是独立的。为了找到单个输出空间位置的总MACs数量，我们必须计算所有 $C_{\\text{out}}$ 个通道的贡献。由于每个输出通道需要 $k^2 C_{\\text{in}}$ 次MACs，总MACs数量是每个通道的MACs数量与通道数的乘积。\n    $$\n    \\text{每个输出像素的总MACs} = (\\text{每个通道的MACs}) \\times (\\text{输出通道数})\n    $$\n    $$\n    \\text{每个输出像素的总MACs} = (k^2 C_{\\text{in}}) \\times C_{\\text{out}} = k^2 C_{\\text{in}} C_{\\text{out}}\n    $$\n推导到此结束。所要求的两个量是：可学习参数的总数，$C_{\\text{out}} (k^2 C_{\\text{in}} + 1)$，以及每个输出像素的MACs数量，$k^2 C_{\\text{in}} C_{\\text{out}}$。", "answer": "$$\\boxed{\\begin{pmatrix} C_{\\text{out}}(k^2 C_{\\text{in}} + 1)  k^2 C_{\\text{in}} C_{\\text{out}} \\end{pmatrix}}$$", "id": "4897442"}, {"introduction": "模型通过学习来最小化一个“损失函数”，而学习的过程则依赖于梯度下降算法，梯度的计算是其核心。本练习将深入探讨在医学图像分割任务中广泛使用的 Soft Dice 损失函数，并让你亲手计算其对于网络预测输出的梯度。通过这个具体的计算过程，你将直观地理解反向传播算法如何将误差信号传递给模型参数，从而驱动模型的优化。[@problem_id:4897433]", "problem": "在医学成像基础中的一个二元分割任务中，一个卷积神经网络（CNN）对从磁共振成像（MRI）切片中提取的一个小图像块的每个像素预测其类别概率。考虑一个 $2 \\times 2$ 的图像块，其真实标签为 $y_1 = 1$、$y_2 = 1$、$y_3 = 0$、$y_4 = 0$，其预测的前景概率为 $p_1 = \\frac{1}{2}$、$p_2 = \\frac{3}{4}$、$p_3 = \\frac{1}{4}$、$p_4 = \\frac{1}{8}$。两个二元集合 $A$ 和 $B$ 之间的 Dice 相似系数定义为 $D_{\\text{set}} = \\frac{2|A \\cap B|}{|A| + |B|}$。为了使用概率进行可微训练，一种常见的松弛方法是将集合的基数替换为像素值的总和，将此图像块上的软 Dice 系数定义为 $D = \\frac{2 \\sum_{i=1}^{4} p_i y_i}{\\sum_{i=1}^{4} p_i + \\sum_{i=1}^{4} y_i}$，软 Dice 损失定义为 $L = 1 - D$。使用标准微积分法则，并将 $y_i$ 视为常数，计算此图像块的梯度向量 $\\nabla_{\\mathbf{p}} L = \\left( \\frac{\\partial L}{\\partial p_1}, \\frac{\\partial L}{\\partial p_2}, \\frac{\\partial L}{\\partial p_3}, \\frac{\\partial L}{\\partial p_4} \\right)$。将你的最终答案表示为精确的有理数，并将各分量以单个行向量的形式呈现。", "solution": "该问题定义明确，科学上合理，并提供了计算所要求梯度的所有必要信息。这是微积分在用于医学图像分割的机器学习中一种常见损失函数上的标准应用。因此，该问题是有效的，并将提供解答。\n\n软 Dice 损失 $L$ 定义为 $L = 1 - D$，其中 $D$ 是软 Dice 系数。根据一个4像素图像块的定义，我们有：\n$$\nD = \\frac{2 \\sum_{i=1}^{4} p_i y_i}{\\sum_{i=1}^{4} p_i + \\sum_{i=1}^{4} y_i}\n$$\n因此，损失函数为：\n$$\nL(\\mathbf{p}) = 1 - \\frac{2 \\sum_{i=1}^{4} p_i y_i}{\\sum_{i=1}^{4} p_i + \\sum_{i=1}^{4} y_i}\n$$\n我们需要计算梯度向量 $\\nabla_{\\mathbf{p}} L$，其分量是对于 $j \\in \\{1, 2, 3, 4\\}$ 的偏导数 $\\frac{\\partial L}{\\partial p_j}$。\n\n利用微分的线性性质，损失函数关于单个预测概率 $p_j$ 的偏导数为：\n$$\n\\frac{\\partial L}{\\partial p_j} = \\frac{\\partial}{\\partial p_j} \\left( 1 - D \\right) = - \\frac{\\partial D}{\\partial p_j}\n$$\n为了简化对 $D$ 的微分，我们将分子和分母定义为独立的函数：\n令 $N = 2 \\sum_{i=1}^{4} p_i y_i$。\n令 $M = \\sum_{i=1}^{4} p_i + \\sum_{i=1}^{4} y_i$。\n所以，$D = \\frac{N}{M}$。\n\n我们应用微分的商法则：\n$$\n\\frac{\\partial D}{\\partial p_j} = \\frac{\\frac{\\partial N}{\\partial p_j} M - N \\frac{\\partial M}{\\partial p_j}}{M^2}\n$$\n现在，我们计算 $N$ 和 $M$ 关于 $p_j$ 的偏导数。真实标签 $y_i$ 被视为常数。\n$$\n\\frac{\\partial N}{\\partial p_j} = \\frac{\\partial}{\\partial p_j} \\left( 2 \\sum_{i=1}^{4} p_i y_i \\right) = 2 y_j\n$$\n$$\n\\frac{\\partial M}{\\partial p_j} = \\frac{\\partial}{\\partial p_j} \\left( \\sum_{i=1}^{4} p_i + \\sum_{i=1}^{4} y_i \\right) = 1\n$$\n将这些代入商法则公式中：\n$$\n\\frac{\\partial D}{\\partial p_j} = \\frac{(2y_j)M - N(1)}{M^2} = \\frac{2y_j \\left(\\sum_{i=1}^{4} p_i + \\sum_{i=1}^{4} y_i\\right) - 2\\sum_{i=1}^{4} p_i y_i}{\\left(\\sum_{i=1}^{4} p_i + \\sum_{i=1}^{4} y_i\\right)^2}\n$$\n因此，损失 $L$ 的偏导数为：\n$$\n\\frac{\\partial L}{\\partial p_j} = - \\frac{\\partial D}{\\partial p_j} = \\frac{2\\sum_{i=1}^{4} p_i y_i - 2y_j \\left(\\sum_{i=1}^{4} p_i + \\sum_{i=1}^{4} y_i\\right)}{\\left(\\sum_{i=1}^{4} p_i + \\sum_{i=1}^{4} y_i\\right)^2}\n$$\n接下来，我们将给定的数值代入此表达式的各项中。\n真实标签为 $y_1 = 1$、$y_2 = 1$、$y_3 = 0$、$y_4 = 0$。\n预测概率为 $p_1 = \\frac{1}{2}$、$p_2 = \\frac{3}{4}$、$p_3 = \\frac{1}{4}$、$p_4 = \\frac{1}{8}$。\n\n我们计算必要的总和：\n$$\n\\sum_{i=1}^{4} y_i = y_1 + y_2 + y_3 + y_4 = 1 + 1 + 0 + 0 = 2\n$$\n$$\n\\sum_{i=1}^{4} p_i = p_1 + p_2 + p_3 + p_4 = \\frac{1}{2} + \\frac{3}{4} + \\frac{1}{4} + \\frac{1}{8} = \\frac{4}{8} + \\frac{6}{8} + \\frac{2}{8} + \\frac{1}{8} = \\frac{13}{8}\n$$\n$$\n\\sum_{i=1}^{4} p_i y_i = p_1 y_1 + p_2 y_2 + p_3 y_3 + p_4 y_4 = \\left(\\frac{1}{2}\\right)(1) + \\left(\\frac{3}{4}\\right)(1) + \\left(\\frac{1}{4}\\right)(0) + \\left(\\frac{1}{8}\\right)(0) = \\frac{1}{2} + \\frac{3}{4} = \\frac{2}{4} + \\frac{3}{4} = \\frac{5}{4}\n$$\n现在，我们计算导数公式中的常数项：\n$2 \\sum_{i=1}^{4} p_i y_i = 2 \\times \\frac{5}{4} = \\frac{5}{2}$\n分母项为 $\\sum_{i=1}^{4} p_i + \\sum_{i=1}^{4} y_i = \\frac{13}{8} + 2 = \\frac{13}{8} + \\frac{16}{8} = \\frac{29}{8}$。\n分母的平方为 $\\left(\\frac{29}{8}\\right)^2 = \\frac{841}{64}$。\n\n现在我们可以计算梯度的每个分量：\n\n对于 $j=1$，$y_1=1$：\n$$\n\\frac{\\partial L}{\\partial p_1} = \\frac{\\frac{5}{2} - 2(1) \\left(\\frac{29}{8}\\right)}{\\frac{841}{64}} = \\frac{\\frac{5}{2} - \\frac{29}{4}}{\\frac{841}{64}} = \\frac{\\frac{10}{4} - \\frac{29}{4}}{\\frac{841}{64}} = \\frac{-\\frac{19}{4}}{\\frac{841}{64}} = -\\frac{19}{4} \\times \\frac{64}{841} = -\\frac{19 \\times 16}{841} = -\\frac{304}{841}\n$$\n\n对于 $j=2$，$y_2=1$：\n由于 $y_2=1$，其计算与 $j=1$ 的情况相同。\n$$\n\\frac{\\partial L}{\\partial p_2} = -\\frac{304}{841}\n$$\n\n对于 $j=3$，$y_3=0$：\n$$\n\\frac{\\partial L}{\\partial p_3} = \\frac{\\frac{5}{2} - 2(0) \\left(\\frac{29}{8}\\right)}{\\frac{841}{64}} = \\frac{\\frac{5}{2} - 0}{\\frac{841}{64}} = \\frac{\\frac{5}{2}}{\\frac{841}{64}} = \\frac{5}{2} \\times \\frac{64}{841} = \\frac{5 \\times 32}{841} = \\frac{160}{841}\n$$\n\n对于 $j=4$，$y_4=0$：\n由于 $y_4=0$，其计算与 $j=3$ 的情况相同。\n$$\n\\frac{\\partial L}{\\partial p_4} = \\frac{160}{841}\n$$\n\n因此，梯度向量 $\\nabla_{\\mathbf{p}} L$ 为：\n$$\n\\nabla_{\\mathbf{p}} L = \\left( -\\frac{304}{841}, -\\frac{304}{841}, \\frac{160}{841}, \\frac{160}{841} \\right)\n$$\n这是最终答案，以精确有理数的形式表示为一个行向量。", "answer": "$$\n\\boxed{\\begin{pmatrix} -\\frac{304}{841}  -\\frac{304}{841}  \\frac{160}{841}  \\frac{160}{841} \\end{pmatrix}}\n$$", "id": "4897433"}, {"introduction": "深度网络中的神经元并非孤立地处理信息，它的输出依赖于输入图像的一个特定区域，这个区域被称为“感受野”。感受野的大小决定了网络能够利用多大范围的上下文信息来进行决策，是理解网络行为的关键。这项实践将带你分析一个完整的三维 U-Net 编码器，系统地计算其最深层神经元的感受野大小，从而揭示深层网络如何获得对图像的全局视角。[@problem_id:4897471]", "problem": "考虑一个用于三维医学图像分割的U-Net家族的三维卷积神经网络（Convolutional Neural Network, CNN）。我们只关注其编码器（收缩）路径，该路径作用于体素网格，并被组织成由$0,1,2,3,4$索引的$5$个分辨率阶段。每个阶段$i\\in\\{0,1,2,3,4\\}$都包含恰好两次三维卷积，其卷积核大小为$3\\times 3\\times 3$，步幅为$1$，无膨胀，并选择零填充以保持空间维度（即常用的“same”填充）。在连续阶段之间，阶段$0,1,2,3$的两次卷积之后会应用一次核大小为$2\\times 2\\times 2$、步幅为$2$的三维最大池化；阶段$4$没有池化操作。因此，总共有$4$次因子为$2$的下采样操作。\n\n使用感受野的基本定义，即特征图中一个单元的感受野是能够通过局部操作的复合作用影响到该单元的输入体素集合。在一个空间维度上，核宽度为$k$的卷积会聚合其输入的$k$个相邻位置，而步幅为$s$的池化操作会将一个单元所依赖的位置之间的间距增加$s$倍。假设各向同性，因此感受野是立方体形状，并且可以通过沿任一空间轴的单个边长来表征。\n\n计算阶段$4$中第二次卷积输出端单个单元相对于原始输入体积的有效感受野的边长$R$（以体素为单位，沿单个空间轴）。将$R$报告为单个整数。无需四舍五入。在最终答案框中以纯数字形式表示最终答案，不带单位。", "solution": "感受野的边长$R$和累积步幅$J$可以通过迭代计算得出。我们追踪这两个量穿过网络的各个层。对于一个核大小为$k_i$、步幅为$s_i$的层$i$（卷积或池化），更新规则如下：\n$R_{i} = R_{i-1} + (k_i - 1) J_{i-1}$\n$J_{i} = J_{i-1} s_i$\n其中$R_{i-1}$和$J_{i-1}$是进入该层之前的值。\n\n我们从输入体积开始，其中单个体素是其自身的感受野。因此，初始状态为$R_0 = 1$和$J_0 = 1$。\n\n对于此计算，我们将核大小为$k_p=2$、步幅为$s_p=2$的最大池化层视为一个参数为$k=2$和$s=2$的层。我们现在逐个阶段地追踪编码器路径中$R$和$J$的值。\n\n**初始状态：** $R = 1, J = 1$。\n\n**经过阶段 0 ($i=0$)：**\n*   卷积 1 ($k=3, s=1$): $R = 1 + (3-1) \\times 1 = 3$。$J = 1 \\times 1 = 1$。\n*   卷积 2 ($k=3, s=1$): $R = 3 + (3-1) \\times 1 = 5$。$J = 1 \\times 1 = 1$。\n*   池化 ($k=2, s=2$): $R = 5 + (2-1) \\times 1 = 6$。$J = 1 \\times 2 = 2$。\n\n**经过阶段 1 ($i=1$)：**\n*   卷积 1 ($k=3, s=1$): $R = 6 + (3-1) \\times 2 = 10$。$J = 2 \\times 1 = 2$。\n*   卷积 2 ($k=3, s=1$): $R = 10 + (3-1) \\times 2 = 14$。$J = 2 \\times 1 = 2$。\n*   池化 ($k=2, s=2$): $R = 14 + (2-1) \\times 2 = 16$。$J = 2 \\times 2 = 4$。\n\n**经过阶段 2 ($i=2$)：**\n*   卷积 1 ($k=3, s=1$): $R = 16 + (3-1) \\times 4 = 24$。$J = 4 \\times 1 = 4$。\n*   卷积 2 ($k=3, s=1$): $R = 24 + (3-1) \\times 4 = 32$。$J = 4 \\times 1 = 4$。\n*   池化 ($k=2, s=2$): $R = 32 + (2-1) \\times 4 = 36$。$J = 4 \\times 2 = 8$。\n\n**经过阶段 3 ($i=3$)：**\n*   卷积 1 ($k=3, s=1$): $R = 36 + (3-1) \\times 8 = 52$。$J = 8 \\times 1 = 8$。\n*   卷积 2 ($k=3, s=1$): $R = 52 + (3-1) \\times 8 = 68$。$J = 8 \\times 1 = 8$。\n*   池化 ($k=2, s=2$): $R = 68 + (2-1) \\times 8 = 76$。$J = 8 \\times 2 = 16$。\n\n**经过阶段 4 ($i=4$)：**\n*   卷积 1 ($k=3, s=1$): $R = 76 + (3-1) \\times 16 = 108$。$J = 16 \\times 1 = 16$。\n*   卷积 2 ($k=3, s=1$): $R = 108 + (3-1) \\times 16 = 140$。$J = 16 \\times 1 = 16$。\n\n计算在阶段4的第二次卷积输出处结束。最终的有效感受野边长为$R=140$体素。", "answer": "$$ \\boxed{140} $$", "id": "4897471"}]}