{"hands_on_practices": [{"introduction": "要训练一个深度学习模型，我们必须定义一个“损失函数”来衡量其表现，并计算该函数相对于模型参数的梯度。在低剂量CT中，主要的噪声来源遵循泊松分布，而非简单的均匀噪声。这个练习 [@problem_id:4875558] 将指导你为一个基于物理现实的损失函数（泊松负对数似然）推导出梯度，从而将光子计数的物理原理与深度学习优化的数学核心直接联系起来。", "problem": "考虑一个透射计算机断层扫描（CT）系统，其中每个射线束单元的探测器光子计数被建模为独立的泊松随机变量。对于由 $i$ 索引的给定射线束单元，观测到的原始计数为 $y_i \\in \\mathbb{N}$，相应的期望光子计数为 $\\lambda_i > 0$。一个深度神经网络直接在原始计数 $y_i$ 上进行训练，以预测每个单元的期望计数。为保证正性，网络输出一个无约束的实数 $s_i \\in \\mathbb{R}$，并且预测的期望计数定义为 $\\hat{\\lambda}_i = \\exp(s_i)$。假设各单元间的测量是独立的，训练目标是泊松模型下的负对数似然（NLL），对所有单元求和，并忽略不依赖于网络输出的加性常数。\n\n从泊松概率质量函数 $p(y_i \\mid \\lambda_i) = \\exp(-\\lambda_i)\\,\\lambda_i^{y_i}/y_i!$ 和负对数似然的定义出发，推导总 NLL 相对于任意单元 $i$ 的网络输出 $s_i$ 的梯度，并仅用 $s_i$ 和 $y_i$ 表示。请以单一闭式解析表达式的形式给出最终梯度。不需要数值近似，也不需要单位。", "solution": "该问题陈述已经过验证，被认为是科学上合理的、适定的和客观的。它基于标准的统计建模（用于光子计数的泊松分布）、医学成像中机器学习的常规做法（负对数似然损失）以及确保参数正性的标准方法（指数链接函数）。所有推导所需梯度的必要信息均已提供。\n\n目标是推导总负对数似然（NLL）相对于单个射线束单元 $i$ 的无约束网络输出 $s_i$ 的梯度。最终表达式应只用 $s_i$ 和观测到的计数 $y_i$ 来表示。\n\n设总 NLL 为 $L$。问题陈述表明，这是对每个单元（由 $j$ 索引）的 NLL 的总和。\n$$\nL = \\sum_j L_j\n$$\n其中 $L_j$ 是单元 $j$ 的 NLL。\n\n总 NLL 相对于特定单元 $i$ 的网络输出 $s_i$ 的梯度由偏导数 $\\frac{\\partial L}{\\partial s_i}$ 给出。由于当 $j \\neq i$ 时，$L_j$ 项不依赖于 $s_i$（因为各单元间的测量是独立的），它们相对于 $s_i$ 的导数为零。\n$$\n\\frac{\\partial L}{\\partial s_i} = \\frac{\\partial}{\\partial s_i} \\left( \\sum_j L_j \\right) = \\sum_j \\frac{\\partial L_j}{\\partial s_i} = \\frac{\\partial L_i}{\\partial s_i}\n$$\n因此，我们只需要计算单元 $i$ 的 NLL 的导数。\n\n对于给定的预测均值 $\\hat{\\lambda}_i$，观测到计数 $y_i$ 的似然由泊松概率质量函数（PMF）决定：\n$$\nP(y_i \\mid \\hat{\\lambda}_i) = \\frac{\\exp(-\\hat{\\lambda}_i) \\hat{\\lambda}_i^{y_i}}{y_i!}\n$$\n对数似然 $\\ell_i$ 是 PMF 的自然对数：\n$$\n\\ell_i(\\hat{\\lambda}_i) = \\ln \\left( \\frac{\\exp(-\\hat{\\lambda}_i) \\hat{\\lambda}_i^{y_i}}{y_i!} \\right) = \\ln(\\exp(-\\hat{\\lambda}_i)) + \\ln(\\hat{\\lambda}_i^{y_i}) - \\ln(y_i!)\n$$\n$$\n\\ell_i(\\hat{\\lambda}_i) = -\\hat{\\lambda}_i + y_i \\ln(\\hat{\\lambda}_i) - \\ln(y_i!)\n$$\n负对数似然 $L_i$ 就是 $-\\ell_i$：\n$$\nL_i(\\hat{\\lambda}_i) = \\hat{\\lambda}_i - y_i \\ln(\\hat{\\lambda}_i) + \\ln(y_i!)\n$$\n项 $\\ln(y_i!)$ 相对于网络参数是一个常数，因为 $y_i$ 是一个固定的观测值。在优化时我们可以忽略它，但为了完整性，我们在此保留它；在对任何网络参数求导时，它都会消失。\n\n为了求出 $L_i$ 相对于 $s_i$ 的梯度，我们必须使用链式法则，因为 $L_i$ 依赖于 $\\hat{\\lambda}_i$，而 $\\hat{\\lambda}_i$ 又依赖于 $s_i$：\n$$\n\\frac{\\partial L_i}{\\partial s_i} = \\frac{\\partial L_i}{\\partial \\hat{\\lambda}_i} \\frac{\\partial \\hat{\\lambda}_i}{\\partial s_i}\n$$\n首先，我们计算 $L_i$ 相对于 $\\hat{\\lambda}_i$ 的导数：\n$$\n\\frac{\\partial L_i}{\\partial \\hat{\\lambda}_i} = \\frac{\\partial}{\\partial \\hat{\\lambda}_i} \\left( \\hat{\\lambda}_i - y_i \\ln(\\hat{\\lambda}_i) + \\ln(y_i!) \\right) = 1 - y_i \\frac{1}{\\hat{\\lambda}_i} + 0 = 1 - \\frac{y_i}{\\hat{\\lambda}_i}\n$$\n接着，我们计算 $\\hat{\\lambda}_i$ 相对于 $s_i$ 的导数。问题中定义了关系式 $\\hat{\\lambda}_i = \\exp(s_i)$。\n$$\n\\frac{\\partial \\hat{\\lambda}_i}{\\partial s_i} = \\frac{\\partial}{\\partial s_i} (\\exp(s_i)) = \\exp(s_i)\n$$\n根据所给定义，我们也有 $\\exp(s_i) = \\hat{\\lambda}_i$。因此：\n$$\n\\frac{\\partial \\hat{\\lambda}_i}{\\partial s_i} = \\hat{\\lambda}_i\n$$\n现在，我们将这两个分量代回链式法则的表达式中：\n$$\n\\frac{\\partial L_i}{\\partial s_i} = \\left( 1 - \\frac{y_i}{\\hat{\\lambda}_i} \\right) \\cdot \\hat{\\lambda}_i = \\hat{\\lambda}_i - y_i\n$$\n最后一步是将此梯度完全用网络输出 $s_i$ 和观测数据 $y_i$ 来表示。我们将 $\\hat{\\lambda}_i = \\exp(s_i)$ 代入梯度表达式中。\n$$\n\\frac{\\partial L}{\\partial s_i} = \\frac{\\partial L_i}{\\partial s_i} = \\exp(s_i) - y_i\n$$\n这就是总 NLL 相对于任意单元 $i$ 的网络输出 $s_i$ 的梯度的最终闭式解析表达式。", "answer": "$$\n\\boxed{\\exp(s_i) - y_i}\n$$", "id": "4875558"}, {"introduction": "许多用于CT重建的深度学习方法都基于“展开”的迭代算法，其中包含了重复应用正向投影和反向投影的操作。为了使优化过程正确无误，反向投影算子在数学上必须是正向投影算子的“伴随”算子。这项实践 [@problem_id:4875608] 旨在探讨在实际应用中，使用不匹配的算子对所带来的微妙但至关重要的影响，强调了在构建复杂模型时数学严谨性的重要性。", "problem": "在离散计算机断层扫描 (CT) 中，将图像向量 $x \\in \\mathbb{R}^{n}$ 映射到投影数据 $y \\in \\mathbb{R}^{m}$ 的前向模型通常被建模为一个线性算子 $A \\in \\mathbb{R}^{m \\times n}$，使得 $y \\approx A x$。考虑一个端到端训练的重建架构，其包含一个数据保真度损失 $\\mathcal{L}(x) = \\tfrac{1}{2}\\|A x - y\\|_{2}^{2}$，并在训练期间使用一个反投影器 $B \\in \\mathbb{R}^{n \\times m}$ 将残差 $(A x - y)$ 传播到图像空间。在许多实际的工具链中，所实现的投影器和反投影器在标准欧几里得内积下并非彼此的精确伴随算子，例如，当使用 Siddon 射线驱动投影器作为 $A$ 并结合使用体素驱动反投影器作为 $B$ 时。从希尔伯特空间中伴随算子的定义以及梯度作为选定内积下 Fréchet 导数的 Riesz 表示的定义出发，推断 $B \\neq A^{\\top}$ 对端到端训练中使用的梯度的准确性以及对最终的优化动力学的影响。\n\n选择所有正确的陈述。\n\nA. 在标准欧几里得内积下，$\\mathcal{L}(x)$ 的真实梯度是 $A^{\\top}(A x - y)$。如果 $\\tilde{g}(x) = B(A x - y)$ 且 $B \\neq A^{\\top}$，那么 $\\tilde{g}(x)$ 通常是一个有偏近似，不一定是 $\\mathcal{L}$ 在 $x$ 处的下降方向。因此，使用 $\\tilde{g}(x)$ 会导致端到端训练优化一个与 $\\mathcal{L}$ 不同的目标。\n\nB. 对于任何线性算子 $B$，总存在一个对称正定 (SPD) 矩阵 $M \\in \\mathbb{R}^{n \\times n}$，使得对于所有残差 $(A x - y)$，$\\tilde{g}(x)$ 等于 $\\mathcal{L}$ 关于 $M$ 加权内积的梯度，这意味着它等同于对 $\\mathcal{L}$ 的预条件梯度下降。\n\nC. 检验伴随算子正确性的一个实用数值测试是，随机抽取 $x \\in \\mathbb{R}^{n}$ 和 $z \\in \\mathbb{R}^{m}$，并在欧几里得内积下检查 $\\langle A x, z \\rangle = \\langle x, B z \\rangle$ 是否成立；系统性偏差表明存在实现上的不匹配，这将破坏梯度。\n\nD. 因为残差 $(A x - y)$ 在训练过程中会波动，$\\tilde{g}(x)$ 与真实梯度之间的夹角在多次迭代后会平均为零，这使得使用 $B \\neq A^{\\top}$ 在期望上是有效无偏的，并且对收敛无害。\n\nE. 即使当 $B \\neq A^{\\top}$ 时，只要 $A$ 是列满秩的，将学习率减小到足够小就能保证收敛到 $\\mathcal{L}$ 的最小值点。\n\nF. 在一个端到端网络中，如果重建模块内部包含 $A$ 和 $B$，并且损失在图像域计算，那么学习到的参数可能会部分补偿 $B \\neq A^{\\top}$ 的问题，但训练信号对应的是由 $B$ 引导的代理目标，而不是原始的物理数据保真度。", "solution": "题目要求分析在训练端到端重建网络时，使用一个非投影矩阵 $A$ 精确伴随算子的反投影矩阵 $B$（即 $B \\neq A^{\\top}$）所带来的影响。数据保真度损失函数为 $\\mathcal{L}(x) = \\tfrac{1}{2}\\|A x - y\\|_{2}^{2}$。\n\n### 步骤 1：提取已知条件\n- 图像向量：$x \\in \\mathbb{R}^{n}$\n- 投影数据：$y \\in \\mathbb{R}^{m}$\n- 前向模型 (投影器)：$A \\in \\mathbb{R}^{m \\times n}$\n- 反投影器：$B \\in \\mathbb{R}^{n \\times m}$\n- 数据保真度损失函数：$\\mathcal{L}(x) = \\tfrac{1}{2}\\|A x - y\\|_{2}^{2}$\n- 关注的条件：所实现的反投影器 $B$ 在标准欧几里得内积下不是投影器 $A$ 的精确伴随算子，即 $B \\neq A^{\\top}$。\n- 实现的梯度近似：在训练中，使用 $\\tilde{g}(x) = B(A x - y)$ 来传播残差。\n\n### 步骤 2：使用提取的已知条件进行验证\n- **科学上成立**：这个问题在计算医学成像和数值优化的原理上有很好的基础。线性模型 $y = Ax$ 是 CT 的标准模型。损失函数是标准的最小二乘数据保真度项。数学上的伴随算子 ($A^\\top$) 和计算上实现的反投影器 ($B$)（可能不是一个完美的伴随算子，例如 Siddon 投影器与体素驱动反投影器）之间的区别是该领域一个已知的实际问题。关于其对基于梯度的优化的影响的问题是高度相关的。\n- **问题定义良好**：这个问题定义良好。它设定了一个清晰的数学场景 ($B \\neq A^\\top$)，并要求基于线性代数和微积分的基本原理，分析其对优化动力学的影响。\n- **客观性**：这个问题是用精确、客观的数学语言陈述的。\n\n### 步骤 3：结论与行动\n题目陈述有效。我将继续进行解答。\n\n### 基于原理的真实梯度推导\n泛函的梯度是根据其 Fréchet 导数和所选的内积来定义的。我们在希尔伯特空间 $\\mathbb{R}^n$ 中，使用标准欧几里得内积 $\\langle u, v \\rangle = u^{\\top}v$。损失函数为 $\\mathcal{L}(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} = \\frac{1}{2}\\langle A x - y, A x - y \\rangle$。\n\n为了求梯度，我们首先计算 Fréchet 导数 $D\\mathcal{L}(x)[h]$，它是 $\\mathcal{L}$ 对于一个微小扰动 $h \\in \\mathbb{R}^n$ 变化的线性部分。\n$$\n\\mathcal{L}(x+h) = \\frac{1}{2}\\langle A(x+h) - y, A(x+h) - y \\rangle \\\\\n= \\frac{1}{2}\\langle (A x - y) + Ah, (A x - y) + Ah \\rangle \\\\\n= \\frac{1}{2} \\left( \\langle A x - y, A x - y \\rangle + 2\\langle A x - y, Ah \\rangle + \\langle Ah, Ah \\rangle \\right) \\\\\n= \\mathcal{L}(x) + \\langle A x - y, Ah \\rangle + O(\\|h\\|^2)\n$$\n根据伴随算子 $A^{\\top}$ 的定义，我们有 $\\langle A x - y, Ah \\rangle = \\langle A^{\\top}(A x - y), h \\rangle$。\n所以，Fréchet 导数是 $D\\mathcal{L}(x)[h] = \\langle A^{\\top}(A x - y), h \\rangle$。\n\n梯度 $\\nabla \\mathcal{L}(x)$ 被定义为 Fréchet 导数的 Riesz 表示，它是满足 $D\\mathcal{L}(x)[h] = \\langle \\nabla \\mathcal{L}(x), h \\rangle$ 对所有 $h$ 成立的唯一向量。\n通过直接比较，$\\mathcal{L}(x)$ 相对于欧几里得内积的真实梯度是：\n$$\n\\nabla \\mathcal{L}(x) = A^{\\top}(A x - y)\n$$\n题目指出，实际使用的是一个近似 $\\tilde{g}(x) = B(A x - y)$，其中 $B \\neq A^{\\top}$。这意味着优化算法（例如，梯度下降）使用的是一个不正确的更新方向。\n\n### 逐项分析\n\n**A. 在标准欧几里得内积下，$\\mathcal{L}(x)$ 的真实梯度是 $A^{\\top}(A x - y)$。如果 $\\tilde{g}(x) = B(A x - y)$ 且 $B \\neq A^{\\top}$，那么 $\\tilde{g}(x)$ 通常是一个有偏近似，不一定是 $\\mathcal{L}$ 在 $x$ 处的下降方向。因此，使用 $\\tilde{g}(x)$ 会导致端到端训练优化一个与 $\\mathcal{L}$ 不同的目标。**\n\n- 上述推导证实了真实梯度确实是 $\\nabla \\mathcal{L}(x) = A^{\\top}(A x - y)$。\n- 由于 $B \\neq A^{\\top}$，所提供的“梯度” $\\tilde{g}(x)$ 系统地不同于真实梯度。这是一种偏倚（一种系统性误差，而非统计误差）。\n- 如果 $\\langle \\nabla \\mathcal{L}(x), -d \\rangle > 0$，则方向 $d$ 是 $\\mathcal{L}$ 在 $x$ 处的下降方向。对于使用 $\\tilde{g}(x)$ 的梯度下降步，这意味着我们需要 $\\langle \\nabla \\mathcal{L}(x), \\tilde{g}(x) \\rangle > 0$。令 $r = A x - y$。此条件为 $\\langle A^{\\top}r, B r \\rangle = (A^{\\top}r)^{\\top}(Br) = r^{\\top}A B r > 0$。一般情况下，不能保证矩阵 $A B$ 是正定的，因此不能保证对于所有非零残差 $r$，都有 $r^{\\top}A B r > 0$。因此，$\\tilde{g}(x)$ 可能不是一个下降方向。\n- 使用基于 $\\tilde{g}(x)$ 的更新的优化算法将（如果收敛）收敛到一个点 $x^*$，使得 $\\tilde{g}(x^*) = B(A x^* - y) = 0$。相比之下，$\\mathcal{L}(x)$ 的真正最小值点满足正规方程 $\\nabla \\mathcal{L}(x) = A^{\\top}(A x^* - y) = 0$。由于 $B \\neq A^{\\top}$，这两个条件定义了不同的解。因此，训练过程实际上是在为一个不同于最小化 $\\mathcal{L}(x)$ 的目标进行优化。\n\n**结论：正确。**\n\n**B. 对于任何线性算子 $B$，总存在一个对称正定 (SPD) 矩阵 $M \\in \\mathbb{R}^{n \\times n}$，使得对于所有残差 $(A x - y)$，$\\tilde{g}(x)$ 等于 $\\mathcal{L}$ 关于 $M$ 加权内积的梯度，这意味着它等同于对 $\\mathcal{L}$ 的预条件梯度下降。**\n\n- $\\mathcal{L}(x)$ 相对于由 SPD 矩阵 $M$ 定义的内积 $\\langle u, v \\rangle_M = u^\\top M v$ 的梯度由 $\\nabla_M \\mathcal{L}(x) = M^{-1} \\nabla \\mathcal{L}(x) = M^{-1} A^{\\top}(A x - y)$ 给出。这是使用预条件子 $M$ 的预条件梯度下降中的更新方向。\n- 该陈述断言，对于任何 $B$，都存在一个 SPD 矩阵 $M$，使得 $\\tilde{g}(x) = \\nabla_M \\mathcal{L}(x)$。这意味着对于所有可能的残差 $r = Ax-y$，必须有 $B(A x - y) = M^{-1} A^{\\top}(A x - y)$。\n- 这意味着 $B = M^{-1} A^{\\top}$，或 $M = A^{\\top} B^{-1}$。\n- 这个方程有几个问题。首先，矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 和 $B \\in \\mathbb{R}^{n \\times m}$ 通常不是方阵。如果 $n \\neq m$，$B$ 不是可逆的，所以 $B^{-1}$ 没有定义。即使 $n = m$，$B$ 也可能是奇异的。此外，即使 $B$ 是可逆的，也不能保证得到的矩阵 $M = A^{\\top} B^{-1}$ 是对称和正定的。因此，该陈述是错误的。\n\n**结论：不正确。**\n\n**C. 检验伴随算子正确性的一个实用数值测试是，随机抽取 $x \\in \\mathbb{R}^{n}$ 和 $z \\in \\mathbb{R}^{m}$，并在欧几里得内积下检查 $\\langle A x, z \\rangle = \\langle x, B z \\rangle$ 是否成立；系统性偏差表明存在实现上的不匹配，这将破坏梯度。**\n\n- 根据定义，实内积空间之间的线性算子 $A$ 的伴随算子 $A^{\\top}$ 是满足 $\\langle Ax, z \\rangle = \\langle x, A^{\\top}z \\rangle$ 对所有向量 $x$ 和 $z$ 成立的唯一算子。\n- 该陈述建议通过检查随机向量的 $\\langle Ax, z \\rangle = \\langle x, B z \\rangle$ 是否成立来测试 $B$ 是否为 $A$ 的伴随算子。这是被称为“点积测试”的标准数值验证方法。\n- 这个测试的系统性失败（即偏差超出浮点精度）证明了 $B \\neq A^{\\top}$。\n- 正如已确立的，如果 $B \\neq A^{\\top}$，则实现的梯度 $\\tilde{g}(x)$ 不是真实梯度 $\\nabla \\mathcal{L}(x)$。使用不正确的梯度进行优化当然可以被认为是“破坏”了梯度。\n\n**结论：正确。**\n\n**D. 因为残差 $(A x - y)$ 在训练过程中会波动，$\\tilde{g}(x)$ 与真实梯度之间的夹角在多次迭代后会平均为零，这使得使用 $B \\neq A^{\\top}$ 在期望上是有效无偏的，并且对收敛无害。**\n\n- 实现的梯度与真实梯度之间的差异是一个误差项 $\\epsilon(x) = \\tilde{g}(x) - \\nabla\\mathcal{L}(x) = (B - A^{\\top})(A x - y)$。\n- 算子 $(B - A^{\\top})$ 是一个固定的非零矩阵。残差 $(A x - y)$ 是当前迭代 $x$ 的一个确定性函数，而不是一个零均值的随机变量。它的“波动”是优化所采取的结构化路径的一部分，而不是随机噪声。\n- 没有任何数学原理能使误差向量 $\\epsilon(x)$ 或 $\\tilde{g}(x)$ 与 $\\nabla\\mathcal{L}(x)$ 之间的夹角在优化轨迹上平均为零。该误差是系统性的，并取决于不匹配 $(B - A^{\\top})$ 的具体结构和图像 $x$ 的内容。\n- 声称这在“期望上是无偏的”是错误地将随机梯度下降（其中小批量噪声是无偏的）中的一个概念应用于一个确定性的、系统性的误差。声称它“对收敛无害”是错误的，因为它可能阻止收敛或导致收敛到错误的解。\n\n**结论：不正确。**\n\n**E. 即使当 $B \\neq A^{\\top}$ 时，只要 $A$ 是列满秩的，将学习率减小到足够小就能保证收敛到 $\\mathcal{L}$ 的最小值点。**\n\n- 优化更新为 $x_{k+1} = x_k - \\eta \\tilde{g}(x_k) = x_k - \\eta B(Ax_k - y)$。\n- 如果这个迭代收敛，它会收敛到一个不动点 $x^*$，使得 $x^* = x^* - \\eta B(Ax^* - y)$，这意味着 $B(Ax^* - y) = 0$。\n- $\\mathcal{L}(x)$ 的最小值点，我们称之为 $x_{LS}$，满足正规方程 $A^{\\top}(A x_{LS} - y) = 0$。（$A$ 是列满秩的条件确保了该最小值点是唯一的）。\n- 由于 $B \\neq A^{\\top}$，条件 $B(Ax - y) = 0$ 通常不等同于 $A^{\\top}(Ax - y) = 0$。因此，迭代的不动点 $x^*$ 与损失函数的最小值点 $x_{LS}$ 是不相同的。\n- 减小学习率 $\\eta$ 对于确保 *迭代过程* 的稳定性和收敛可能是必要的（例如，如果 $I - \\eta BA$ 的谱半径小于1），但它不会改变迭代收敛到的不动点的位置。该算法将尽职地收敛到错误的答案，只是步长更小而已。\n\n**结论：不正确。**\n\n**F. 在一个端到端网络中，如果重建模块内部包含 $A$ 和 $B$，并且损失在图像域计算，那么学习到的参数可能会部分补偿 $B \\neq A^{\\top}$ 的问题，但训练信号对应的是由 $B$ 引导的代理目标，而不是原始的物理数据保真度。**\n\n- 该陈述在更广泛的深度学习背景下描述了这种情况。假设数据保真度项 $\\mathcal{L}(x)$ 是用于训练具有参数 $\\theta$ 的网络的总损失函数的一个组成部分。该部分相对于网络输出图像 $x$ 的梯度被错误地计算为 $\\tilde{g}(x) = B(Ax-y)$。\n- 这个不正确的梯度随后通过网络的其余部分进行反向传播，以更新参数 $\\theta$。网络的学习过程是由这个错误的信号驱动的。\n- 因此，网络参数 $\\theta$ 并非为了最小化真实的损失函数而被优化，而是为了最小化一个其稳定点由算子 $B$ 决定的代理目标。网络学会了生成与训练期间使用的特定算子对 $(A, B)$ 配合良好的输出。\n- 学习到的参数可以“部分补偿”这种不匹配。例如，神经网络正则化器可能会学会抑制由特定 $B \\neq A^\\top$ 不匹配引起的特征性伪影。\n- 关键点是，训练从根本上是相对于一个不同于由 $\\mathcal{L}(x)$ 和欧几里得内积数学上指定的目标进行优化。由 $\\mathcal{L}(x)$ 描述的物理保真度对应于梯度 $\\nabla \\mathcal{L}(x)$，但训练使用的是来自 $\\tilde{g}(x)$ 的信号，而后者是由 $B$ 定义的。“由 $B$ 引导的代理目标”这个短语准确地捕捉了这种情况。（注意：“损失在图像域计算”这个说法可能有些不精确，因为给定的 $\\mathcal{L}(x)$ 是在数据域的，但该陈述关于训练信号和代理目标的核心论点是合理且切中要害的）。\n\n**结论：正确。**", "answer": "$$\\boxed{ACF}$$", "id": "4875608"}, {"introduction": "当我们构建并训练好模型后，评估其性能是至关重要的一步。虽然像峰值信噪比（PSNR）和结构相似性指数（SSIM）这样的指标易于计算，但它们并不总能反映放射科医生真正需要的信息。这个练习 [@problem_id:4875595] 要求你计算这些标准指标，并批判性地思考为何更高的分数未必意味着图像在临床上更有用，从而强调了基于任务的图像质量才是最终目标。", "problem": "一位研究人员评估了一个小的计算机断层扫描（CT）图像块的两种深度学习重建结果。像素强度被归一化到范围 $[0,1]$。真实图像块 $G \\in \\mathbb{R}^{4 \\times 4}$ 和两种重建结果 $R_1, R_2 \\in \\mathbb{R}^{4 \\times 4}$ 由下式给出\n$$\nG \\;=\\;\n\\begin{bmatrix}\n0  0  1  1 \\\\\n0  0  1  1 \\\\\n0  0  1  1 \\\\\n0  0  1  1\n\\end{bmatrix},\\quad\nR_1 \\;=\\;\n\\begin{bmatrix}\n0.2  0.2  0.8  0.8 \\\\\n0.2  0.2  0.8  0.8 \\\\\n0.2  0.2  0.8  0.8 \\\\\n0.2  0.2  0.8  0.8\n\\end{bmatrix},\\quad\nR_2 \\;=\\;\n\\begin{bmatrix}\n0    0.2  1    0.8 \\\\\n0    0.2  1    0.8 \\\\\n0    0.2  1    0.8 \\\\\n0    0.2  1    0.8\n\\end{bmatrix}.\n$$\n假设峰值信号值为 $L=1$。在所有 $N=16$ 个像素上使用均方误差（MSE）的标准定义，以及峰值信噪比（PSNR）的标准定义。对于结构相似性指数（SSIM），在整个 $4 \\times 4$ 图像块上使用单窗口计算，并采用总体统计量（计算方差和协方差时除以 $N$），常数取 $C_1=(0.01)^2$ 和 $C_2=(0.03)^2$。\n\n基于对 $R_1$ 和 $R_2$ 的 PSNR 和 SSIM 的正确计算，以及基于用于临床决策（例如，检测微小、高对比度结构）的、以任务为导向的图像质量的推理，以下哪个陈述是正确的？\n\nA. $R_2$ 的峰值信噪比（PSNR）约为 $17\\,\\mathrm{dB}$，超过了 $R_1$ 的PSNR（约为 $14\\,\\mathrm{dB}$）。\n\nB. $R_1$ 的结构相似性指数（SSIM）超过了 $R_2$ 的 SSIM，因为 $R_1$ 更好地匹配了全局亮度和对比度。\n\nC. 尽管 PSNR 和 SSIM 存在差异，但对于任何深度学习重建结果，单凭 PSNR 或 SSIM 任何一个指标都足以保证更好的临床病灶检测性能。\n\nD. 对于对高频特征敏感的临床任务，像 $R_1$ 这样的重建结果的性能可能会不如 $R_2$，即使它们的 PSNR 相同，因为平滑处理会降低与任务相关的结构。\n\nE. 使用给定的常数，$R_1$ 的 SSIM 约为 $0.88$，$R_2$ 的 SSIM 约为 $0.95$。", "solution": "我们从标准定义开始。图像 $X$ 和真实图像 $G$ 之间在 $N$ 个像素上的均方误差（MSE）为\n$$\n\\mathrm{MSE}(X,G) \\;=\\; \\frac{1}{N} \\sum_{i=1}^{N} \\left(X_i - G_i\\right)^2.\n$$\n对于峰值 $L$，以分贝（dB）为单位的峰值信噪比（PSNR）为\n$$\n\\mathrm{PSNR}(X,G) \\;=\\; 10 \\log_{10}\\!\\left(\\frac{L^2}{\\mathrm{MSE}(X,G)}\\right).\n$$\n在整个图像块上使用总体统计量计算的 $X$ 和 $G$ 之间的结构相似性指数（SSIM）为\n$$\n\\mathrm{SSIM}(X,G) \\;=\\; \\frac{\\left(2 \\mu_X \\mu_G + C_1\\right)\\left(2 \\sigma_{XG} + C_2\\right)}{\\left(\\mu_X^2 + \\mu_G^2 + C_1\\right)\\left(\\sigma_X^2 + \\sigma_G^2 + C_2\\right)},\n$$\n其中 $\\mu_X$ 和 $\\mu_G$ 是均值，$\\sigma_X^2$ 和 $\\sigma_G^2$ 是方差（计算时除以 $N$），$\\sigma_{XG}$ 是协方差（计算时除以 $N$）。我们取 $C_1=(0.01)^2=0.0001$ 和 $C_2=(0.03)^2=0.0009$，以及 $L=1$。\n\n第1步：计算 MSE 和 PSNR。\n\n- 对于 $R_1$ 与 $G$：$G$ 中的每个 $0$ 在 $R_1$ 中被近似为 $0.2$（误差 $0.2$），每个 $1$ 被近似为 $0.8$（误差 $-0.2$）。对于所有 $N=16$ 个像素，每个像素的平方误差都是 $0.04$。因此\n$$\n\\mathrm{MSE}(R_1,G) \\;=\\; \\frac{16 \\cdot 0.04}{16} \\;=\\; 0.04.\n$$\n那么\n$$\n\\mathrm{PSNR}(R_1,G) \\;=\\; 10 \\log_{10}\\!\\left(\\frac{1}{0.04}\\right) \\;=\\; 10 \\log_{10}(25) \\;\\approx\\; 13.98\\,\\mathrm{dB}.\n$$\n\n- 对于 $R_2$ 与 $G$：八个像素完全匹配（$0 \\rightarrow 0$ 和 $1 \\rightarrow 1$，平方误差为 $0$），另外八个像素的误差为 $\\pm 0.2$（$0 \\rightarrow 0.2$ 和 $1 \\rightarrow 0.8$，平方误差为 $0.04$）。因此\n$$\n\\mathrm{MSE}(R_2,G) \\;=\\; \\frac{8 \\cdot 0 + 8 \\cdot 0.04}{16} \\;=\\; 0.02,\n$$\n并且\n$$\n\\mathrm{PSNR}(R_2,G) \\;=\\; 10 \\log_{10}\\!\\left(\\frac{1}{0.02}\\right) \\;=\\; 10 \\log_{10}(50) \\;\\approx\\; 16.99\\,\\mathrm{dB}.\n$$\n\n因此，$\\mathrm{PSNR}(R_2,G) \\approx 16.99\\,\\mathrm{dB}$ 超过了 $\\mathrm{PSNR}(R_1,G) \\approx 13.98\\,\\mathrm{dB}$。\n\n第2步：计算 SSIM 的各个组成部分。\n\n首先，计算真实图像的统计量。对于 $G$，有八个零和八个一：\n- 均值：\n$$\n\\mu_G \\;=\\; \\frac{8 \\cdot 0 + 8 \\cdot 1}{16} \\;=\\; 0.5.\n$$\n- 方差（总体）：\n$$\n\\sigma_G^2 \\;=\\; \\frac{8 \\cdot (0-0.5)^2 + 8 \\cdot (1-0.5)^2}{16} \\;=\\; \\frac{8 \\cdot 0.25 + 8 \\cdot 0.25}{16} \\;=\\; 0.25.\n$$\n\n现在计算 $R_1$：有八个值为 $0.2$，八个值为 $0.8$。\n- 均值：\n$$\n\\mu_{R_1} \\;=\\; \\frac{8 \\cdot 0.2 + 8 \\cdot 0.8}{16} \\;=\\; 0.5.\n$$\n- 方差：\n$$\n\\sigma_{R_1}^2 \\;=\\; \\frac{8 \\cdot (0.2-0.5)^2 + 8 \\cdot (0.8-0.5)^2}{16} \\;=\\; \\frac{8 \\cdot 0.09 + 8 \\cdot 0.09}{16} \\;=\\; 0.09.\n$$\n- 与 $G$ 的协方差：\n对于每个零值位置，$(G-\\mu_G) = -0.5$ 和 $(R_1-\\mu_{R_1}) = -0.3$，乘积为 $0.15$（共八次）；对于每个一值位置，$(G-\\mu_G) = +0.5$ 和 $(R_1-\\mu_{R_1}) = +0.3$，乘积为 $0.15$（共八次）。因此\n$$\n\\sigma_{R_1,G} \\;=\\; \\frac{8 \\cdot 0.15 + 8 \\cdot 0.15}{16} \\;=\\; 0.15.\n$$\n现在计算 $\\mathrm{SSIM}(R_1,G)$：\n- 亮度项：\n$$\n\\frac{2 \\mu_{R_1} \\mu_G + C_1}{\\mu_{R_1}^2 + \\mu_G^2 + C_1} \\;=\\; \\frac{2 \\cdot 0.5 \\cdot 0.5 + 0.0001}{0.5^2 + 0.5^2 + 0.0001} \\;=\\; \\frac{0.5001}{0.5001} \\;=\\; 1.\n$$\n- 对比度-结构项：\n$$\n\\frac{2 \\sigma_{R_1,G} + C_2}{\\sigma_{R_1}^2 + \\sigma_G^2 + C_2} \\;=\\; \\frac{2 \\cdot 0.15 + 0.0009}{0.09 + 0.25 + 0.0009} \\;=\\; \\frac{0.3009}{0.3409} \\;\\approx\\; 0.883.\n$$\n因此\n$$\n\\mathrm{SSIM}(R_1,G) \\;\\approx\\; 0.883.\n$$\n\n对于 $R_2$：其值分别为四个 $0$、四个 $0.2$、四个 $0.8$ 和四个 $1.0$。\n- 均值：\n$$\n\\mu_{R_2} \\;=\\; \\frac{4 \\cdot 0 + 4 \\cdot 0.2 + 4 \\cdot 0.8 + 4 \\cdot 1}{16} \\;=\\; 0.5.\n$$\n- 方差：\n$$\n\\sigma_{R_2}^2 \\;=\\; \\frac{4 \\cdot (0-0.5)^2 + 4 \\cdot (0.2-0.5)^2 + 4 \\cdot (0.8-0.5)^2 + 4 \\cdot (1-0.5)^2}{16}\n$$\n$$\n\\;=\\; \\frac{4 \\cdot 0.25 + 4 \\cdot 0.09 + 4 \\cdot 0.09 + 4 \\cdot 0.25}{16} \\;=\\; \\frac{2.72}{16} \\;=\\; 0.17.\n$$\n- 与 $G$ 的协方差：\n乘积 $(G-\\mu_G)(R_2-\\mu_{R_2})$ 对于完全匹配的情况（$0$ 与 $0$，$1$ 与 $1$）为 $0.25$，对于有扰动的情况（$0$ 与 $0.2$，$1$ 与 $0.8$）为 $0.15$。这四种组合各出现四次。因此\n$$\n\\sigma_{R_2,G} \\;=\\; \\frac{4 \\cdot 0.25 + 4 \\cdot 0.15 + 4 \\cdot 0.25 + 4 \\cdot 0.15}{16} \\;=\\; \\frac{3.2}{16} \\;=\\; 0.2.\n$$\n现在计算 $\\mathrm{SSIM}(R_2,G)$：\n- 亮度项同样为\n$$\n\\frac{2 \\mu_{R_2} \\mu_G + C_1}{\\mu_{R_2}^2 + \\mu_G^2 + C_1} \\;=\\; 1.\n$$\n- 对比度-结构项：\n$$\n\\frac{2 \\sigma_{R_2,G} + C_2}{\\sigma_{R_2}^2 + \\sigma_G^2 + C_2} \\;=\\; \\frac{2 \\cdot 0.2 + 0.0009}{0.17 + 0.25 + 0.0009} \\;=\\; \\frac{0.4009}{0.4209} \\;\\approx\\; 0.952.\n$$\n因此\n$$\n\\mathrm{SSIM}(R_2,G) \\;\\approx\\; 0.952.\n$$\n\n第3步：将指标与临床任务性能联系起来。\n\n医学成像中基于任务的图像质量，强调在特定观察者任务（例如，检测微小、高对比度的病灶）上的性能。检测理论的基本考量表明，可检测性取决于任务相关信号分量的保留程度以及跨空间频率的噪声统计特性。平滑操作会减少高频内容，可能会削弱对于检测至关重要的微小结构的能量，即使逐像素的误差指标得以保持或改善。因此，更高的 PSNR 或 SSIM 本身并不能保证更优的临床任务性能。\n\n逐项分析：\n\n- 选项A：根据计算值，$\\mathrm{PSNR}(R_1,G) \\approx 13.98\\,\\mathrm{dB}$ 且 $\\mathrm{PSNR}(R_2,G) \\approx 16.99\\,\\mathrm{dB}$。因此 $R_2$ 的 PSNR 比 $R_1$ 高出约 $3\\,\\mathrm{dB}$。此陈述正确。\n\n- 选项B：我们计算得出 $\\mathrm{SSIM}(R_1,G) \\approx 0.883$ 且 $\\mathrm{SSIM}(R_2,G) \\approx 0.952$。尽管均值相等且对比度对齐相似，但由于 $R_2$ 相对于 $G$ 具有更强的协方差和更适宜的方差，因此其 SSIM 更高。声称 $R_1$ 的 SSIM 超过 $R_2$ 是不正确的。结论：不正确。\n\n- 选项C：没有任何单一的保真度指标（PSNR 或 SSIM）能够保证在所有重建和任务中都获得更好的临床病灶检测性能；需要进行基于任务的评估。这种保证的说法是错误的。结论：不正确。\n\n- 选项D：平滑处理会降低与检测微小结构相关的高频内容；即使在 PSNR 相同的情况下，一个经过平滑处理的图像在处理高频检测任务时，其性能也可能不如一个平滑程度较低的图像。这与基于任务的原则相符。结论：正确。\n\n- 选项E：计算出的 $R_1$ 的 SSIM 约为 $0.88$，$R_2$ 的 SSIM 约为 $0.95$，这与我们的计算结果一致。结论：正确。\n\n因此，正确的陈述是 A、D 和 E。", "answer": "$$\\boxed{ADE}$$", "id": "4875595"}]}