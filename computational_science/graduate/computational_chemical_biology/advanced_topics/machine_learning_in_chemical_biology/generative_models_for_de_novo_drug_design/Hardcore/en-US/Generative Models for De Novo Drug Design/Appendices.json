{
    "hands_on_practices": [
        {
            "introduction": "Effective *de novo* design is not just about generating novel molecules, but about generating molecules that are useful. This practice delves into a core challenge: how to balance the optimization of a primary objective, like binding affinity, with the adherence to crucial medicinal chemistry rules. By applying the principles of Lagrangian relaxation, you will construct and analyze a penalized scoring function , a fundamental technique for guiding generative models toward producing viable drug candidates.",
            "id": "5247455",
            "problem": "A de novo molecular generator is being trained to propose candidate ligands that optimize a normalized binding objective $A \\in [0,1]$ while satisfying medicinal chemistry rules such as Lipinski's Rule of Five, Pan-Assay Interference Compounds (PAINS) filtering, and synthetic accessibility constraints. Let $V \\in \\{0,1,2,\\dots\\}$ denote the count of rule violations observed for a given molecule under this rule set, and suppose the design goal is the constrained optimization problem to maximize $A$ subject to $V=0$. In practice, constrained optimization is often relaxed via the method of Lagrange multipliers by constructing an unconstrained surrogate objective that incorporates a penalty for constraint violation. Assume the penalty is additive across violations and, to first order, linear in the violation count, with a nonnegative scalar penalty weight $\\lambda \\geq 0$ that reflects the trade-off between $A$ and rule compliance. Starting from this constrained formulation and the principles of Lagrangian relaxation, derive the unconstrained surrogate objective $S(A,V;\\lambda)$ and then evaluate it for a candidate with $A=0.8$, $V=3$, and $\\lambda=0.1$. The score $S$ is dimensionless. Provide the exact value as a decimal. Additionally, briefly discuss the sensitivity of $S$ to changes in $\\lambda$ at fixed $A$ and $V$, and interpret this sensitivity in the context of tuning the generator for pharmaceutical rule compliance. Do not round the numerical score.",
            "solution": "The problem statement will be validated by first extracting the given information and then assessing its scientific and logical soundness.\n\n### Step 1: Extract Givens\n-   **Objective function**: A normalized binding objective $A \\in [0,1]$. The goal is to maximize $A$.\n-   **Constraint function**: A count of rule violations $V \\in \\{0, 1, 2, \\dots\\}$.\n-   **Constraint**: The optimization is subject to the condition $V=0$.\n-   **Method**: Lagrangian relaxation is used to create an unconstrained surrogate objective, $S$.\n-   **Penalty term**: The penalty for constraint violation is additive, linear in the violation count $V$, and weighted by a nonnegative scalar penalty weight $\\lambda \\geq 0$.\n-   **Surrogate objective**: $S(A,V;\\lambda)$ is the function to be derived.\n-   **Evaluation point**: The values for evaluation are $A=0.8$, $V=3$, and $\\lambda=0.1$.\n-   **Additional tasks**: Discuss the sensitivity of $S$ to changes in $\\lambda$ and interpret this sensitivity.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria:\n-   **Scientifically Grounded**: The problem is well-grounded in the fields of medicinal chemistry and computational optimization. Concepts like Lipinski's rules, PAINS filters, synthetic accessibility, and binding objectives are standard in drug design. The use of a penalized scoring function, derived from principles of Lagrangian relaxation, is a common and valid technique for handling constraints in generative models and optimization algorithms.\n-   **Well-Posed**: The problem is well-posed. It specifies a clear optimization goal (maximize $A$ subject to $V=0$) and a standard method (Lagrangian relaxation with a linear penalty) to transform it into an unconstrained problem. This setup provides sufficient information to uniquely derive the form of the surrogate objective $S$.\n-   **Objective**: The language is precise, technical, and free from subjective or ambiguous statements.\n-   **Completeness and Consistency**: The problem is self-contained and its components are consistent. It provides all necessary information to derive the surrogate objective, evaluate it, and analyze its sensitivity.\n-   **Realism**: The provided numerical values ($A=0.8$, $V=3$, $\\lambda=0.1$) are plausible within the context of scoring functions for molecular design.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid as it is scientifically sound, well-posed, objective, and complete. A full solution will be provided.\n\n### Derivation of the Surrogate Objective\nThe original constrained optimization problem is stated as:\n$$\n\\text{maximize } A \\quad \\text{subject to } V = 0\n$$\nThe method of Lagrange multipliers is employed to convert this into an unconstrained problem. The Lagrangian function, $\\mathcal{L}$, for a problem of maximizing a function $f(x)$ subject to a constraint $g(x)=c$ is generally written as $\\mathcal{L}(x, \\mu) = f(x) - \\mu(g(x)-c)$, where $\\mu$ is the Lagrange multiplier.\n\nIn this context, the function to be maximized is the binding objective $A$, and the constraint is $V=0$. Therefore, we can identify $f \\rightarrow A$, $g \\rightarrow V$, and the constraint value $c \\rightarrow 0$. The penalty weight $\\lambda$ plays the role of the Lagrange multiplier $\\mu$.\n\nThe Lagrangian function is therefore:\n$$\n\\mathcal{L}(A, V; \\lambda) = A - \\lambda(V - 0) = A - \\lambda V\n$$\nThis Lagrangian serves as the unconstrained surrogate objective $S(A, V; \\lambda)$ which the de novo generator seeks to maximize. A higher value of $S$ corresponds to a more desirable candidate molecule. This form correctly reflects the problem's requirements: it is an unconstrained objective that incorporates an additive penalty, which is linear in the violation count $V$ with a weight $\\lambda$.\n$$\nS(A, V; \\lambda) = A - \\lambda V\n$$\n\n### Evaluation of the Surrogate Objective\nWe are asked to evaluate $S(A, V; \\lambda)$ for a candidate molecule with $A=0.8$, $V=3$, and with a penalty weight of $\\lambda=0.1$.\nSubstituting these values into the derived expression for $S$:\n$$\nS(0.8, 3; 0.1) = 0.8 - (0.1)(3)\n$$\n$$\nS = 0.8 - 0.3\n$$\n$$\nS = 0.5\n$$\nThe score for this candidate molecule is $0.5$.\n\n### Sensitivity Analysis and Interpretation\nThe sensitivity of the score $S$ to changes in the penalty weight $\\lambda$ is given by the partial derivative of $S$ with respect to $\\lambda$, holding $A$ and $V$ constant.\n$$\n\\frac{\\partial S}{\\partial \\lambda} = \\frac{\\partial}{\\partial \\lambda} (A - \\lambda V)\n$$\nSince $A$ and $V$ are properties of the given molecule, they are treated as constants in this differentiation.\n$$\n\\frac{\\partial S}{\\partial \\lambda} = -V\n$$\n**Interpretation:**\nThe sensitivity of the score $S$ with respect to the penalty weight $\\lambda$ is equal to the negative of the number of rule violations, $-V$. This result has a clear and significant interpretation in the context of tuning the molecular generator:\n\n1.  **Dependence on Violations**: The impact of changing $\\lambda$ on a molecule's score is directly proportional to its number of violations.\n    -   For a fully compliant molecule ($V=0$), the sensitivity is $\\frac{\\partial S}{\\partial \\lambda} = 0$. Its score is $S=A$, which is independent of the penalty weight $\\lambda$. This is logical, as there is no penalty to be weighted.\n    -   For a non-compliant molecule ($V0$), the sensitivity is negative. This means that increasing the penalty weight $\\lambda$ will decrease the molecule's overall score.\n\n2.  **Tuning Parameter**: The parameter $\\lambda$ acts as a \"tuning knob\" for the drug designer to control the behavior of the generative model.\n    -   By increasing $\\lambda$, the designer places greater importance on rule compliance. Molecules with a high number of violations (large $V$) will be more heavily penalized and thus will be less likely to be proposed by the generator as \"optimal\". This steers the generator towards producing more \"drug-like\" candidates that satisfy the specified medicinal chemistry rules.\n    -   Conversely, if the generator becomes too conservative, producing only rule-compliant molecules but with suboptimal binding objectives ($A$), the designer can decrease $\\lambda$. This reduces the penalty for violations, allowing the generator to explore a wider chemical space that may contain molecules with higher binding potential, even if they have some rule violations.\n\nIn summary, the sensitivity $\\frac{\\partial S}{\\partial \\lambda} = -V$ quantifies the trade-off between maximizing the primary objective ($A$) and satisfying the constraints ($V=0$). It provides a mechanism by which molecules with more violations are more strongly disfavored as the stringency of the rule-based filter ($\\lambda$) is increased.",
            "answer": "$$\n\\boxed{0.5}\n$$"
        },
        {
            "introduction": "For generative models that operate in three-dimensional space, geometric accuracy is paramount. This exercise provides hands-on experience with the gold-standard metric for comparing molecular conformers: the root-mean-square deviation (RMSD), calculated after optimal rigid alignment. By implementing the Kabsch algorithm and testing it on generated structures , you will gain a practical understanding of why E(3)-equivariant architectures are essential for producing high-fidelity 3D molecular geometries.",
            "id": "3847969",
            "problem": "You are evaluating coordinate-generating models for de novo drug design, where each model outputs three-dimensional atomic coordinates for a single conformer of a small molecule. A robust comparison requires an alignment-invariant error metric between a generated conformer and a reference conformer. The canonical metric is the root-mean-square deviation (RMSD), computed after optimal rigid alignment that preserves chirality. In the context of Euclidean transformations, equivariance to the three-dimensional Euclidean group (E(3)) implies that if the inputs are rotated or translated, the model outputs transform accordingly by the same rotation and translation without changing molecular shape.\n\nYour task is to implement, from first principles, a program that computes the alignment-invariant root-mean-square deviation (RMSD) between a predicted conformer and a fixed reference conformer using the optimal rotation that minimizes the mean squared error under a rigid transformation with determinant constraint $+\\!1$ (no reflections), and to analyze the reduction in RMSD achieved by Euclidean group equivariant architectures relative to vanilla architectures.\n\nFundamental base to use:\n- Euclidean distance in three dimensions: for points $p,q \\in \\mathbb{R}^3$, the squared distance is $\\lVert p - q \\rVert_2^2$.\n- Properties of orthogonal matrices: a matrix $R \\in \\mathbb{R}^{3 \\times 3}$ is a rotation if $R^\\top R = I$ and $\\det(R) = 1$.\n- Singular Value Decomposition (SVD): for any matrix $H \\in \\mathbb{R}^{3 \\times 3}$, there exist orthogonal matrices $U,V \\in \\mathbb{R}^{3 \\times 3}$ and a diagonal matrix $\\Sigma$ with nonnegative entries such that $H = U \\Sigma V^\\top$.\n- The optimal Procrustes/Kabsch alignment: the optimal rotation that minimizes the sum of squared distances between centered point sets is obtained from the SVD of the covariance matrix, with a determinant correction to enforce $\\det(R)=1$.\n\nMathematical definitions to implement:\n- For $N$ atoms, let the reference conformer be $X \\in \\mathbb{R}^{N \\times 3}$ with rows $x_i \\in \\mathbb{R}^3$, and a prediction be $Y \\in \\mathbb{R}^{N \\times 3}$ with rows $y_i \\in \\mathbb{R}^3$. Define the centroids $c_X = \\frac{1}{N}\\sum_{i=1}^N x_i$ and $c_Y = \\frac{1}{N}\\sum_{i=1}^N y_i$. Let $\\tilde{X} = X - \\mathbf{1} c_X^\\top$ and $\\tilde{Y} = Y - \\mathbf{1} c_Y^\\top$. Form the covariance $H = \\tilde{Y}^\\top \\tilde{X}$. Compute the singular value decomposition $H = U \\Sigma V^\\top$. Define $R^\\star = V U^\\top$ and if $\\det(R^\\star)  0$, flip the sign of the last column of $V$ and recompute $R^\\star = V U^\\top$ to enforce $\\det(R^\\star)=1$. The optimal translation is $t^\\star = c_X - R^\\star c_Y$. The aligned prediction is $Y_{\\text{aligned}} = \\tilde{Y} R^\\star$ (equivalently $Y R^\\star + \\mathbf{1} t^{\\star\\top}$).\n- The alignment-invariant root-mean-square deviation is $\\mathrm{RMSD}(Y,X) = \\sqrt{\\frac{1}{N}\\sum_{i=1}^N \\lVert (y_i - c_Y) R^\\star - (x_i - c_X) \\rVert_2^2}$.\n- Define the improvement metric for test case $k$ as $\\Delta_k = \\mathrm{RMSD}_{\\text{vanilla},k} - \\mathrm{RMSD}_{\\text{equivariant},k}$.\n\nUnits:\n- All coordinates and translations are in Angstroms. All requested outputs involving distances must be expressed in Angstroms as real-valued numbers (floats).\n- All rotation angles are given in radians.\n\nReference conformer:\nLet $N = 7$, and the fixed reference conformer coordinates $X \\in \\mathbb{R}^{7 \\times 3}$ in Angstroms be the following seven points (rows), representing a planar hexagon with one out-of-plane substituent:\n- $x_0 = (1.400, 0.000, 0.000)$\n- $x_1 = (0.700, 1.212435, 0.000)$\n- $x_2 = (-0.700, 1.212435, 0.000)$\n- $x_3 = (-1.400, 0.000, 0.000)$\n- $x_4 = (-0.700, -1.212435, 0.000)$\n- $x_5 = (0.700, -1.212435, 0.000)$\n- $x_6 = (0.000, 0.000, 1.100)$\n\nDeterministic noise model:\nFor atom index $i \\in \\{0,1,2,3,4,5,6\\}$ and amplitude $\\epsilon \\ge 0$, define the deterministic noise vector\n$n_i(\\epsilon) = \\epsilon \\cdot \\big(\\sin(i), \\cos(2i), \\sin(3i)\\big)$,\nwith all trigonometric functions taking arguments in radians.\n\nRotation parameterization:\nUse intrinsic rotations about axes in the order $z$ then $y$ then $x$. Given angles $(\\alpha,\\beta,\\gamma)$ in radians, define $R(\\alpha,\\beta,\\gamma) = R_z(\\gamma) R_y(\\beta) R_x(\\alpha)$, where\n$R_x(\\alpha) = \\begin{bmatrix}1  0  0\\\\ 0  \\cos\\alpha  -\\sin\\alpha\\\\ 0  \\sin\\alpha  \\cos\\alpha\\end{bmatrix}$,\n$R_y(\\beta) = \\begin{bmatrix}\\cos\\beta  0  \\sin\\beta\\\\ 0  1  0\\\\ -\\sin\\beta  0  \\cos\\beta\\end{bmatrix}$,\n$R_z(\\gamma) = \\begin{bmatrix}\\cos\\gamma  -\\sin\\gamma  0\\\\ \\sin\\gamma  \\cos\\gamma  0\\\\ 0  0  1\\end{bmatrix}$.\n\nTest suite:\nConstruct four test cases. For each case, build two predictions $Y_{\\text{eq}}$ (equivariant-like) and $Y_{\\text{van}}$ (vanilla-like) as follows:\n- Given a base linear transformation $A \\in \\mathbb{R}^{3 \\times 3}$ and translation $t \\in \\mathbb{R}^3$, define the transformed coordinates $Z = X A^\\top + \\mathbf{1} t^\\top$. Then add deterministic noise row-wise: the $i$-th row is $z_i + n_i(\\epsilon)$ for a specified $\\epsilon$.\n\nCase $1$ (happy path: anisotropic scaling harms vanilla):\n- Equivariant-like: $A_{\\text{eq}} = R(0.5, -0.2, 0.3)$, $t_{\\text{eq}} = (1.2, -0.7, 0.4)$, $\\epsilon_{\\text{eq}} = 0.05$.\n- Vanilla-like: $A_{\\text{van}} = S R(0.5, -0.2, 0.3)$ with $S = \\mathrm{diag}(1.03, 0.97, 1.02)$, $t_{\\text{van}} = (1.2, -0.7, 0.4)$, $\\epsilon_{\\text{van}} = 0.20$.\n\nCase $2$ (boundary: reflection cannot be removed by proper rotation):\n- Equivariant-like: $A_{\\text{eq}} = R(1.0, 0.2, -0.7)$, $t_{\\text{eq}} = (0.0, 0.0, 0.0)$, $\\epsilon_{\\text{eq}} = 0.00$.\n- Vanilla-like: $A_{\\text{van}} = M R(1.0, 0.2, -0.7)$ where $M = \\mathrm{diag}(1.0, 1.0, -1.0)$ (a reflection with $\\det(M) = -1$), $t_{\\text{van}} = (0.0, 0.0, 0.0)$, $\\epsilon_{\\text{van}} = 0.00$.\n\nCase $3$ (edge: shear and higher noise harm vanilla):\n- Equivariant-like: $A_{\\text{eq}} = R(0.3, 0.4, 0.2)$, $t_{\\text{eq}} = (-0.5, 0.8, -0.3)$, $\\epsilon_{\\text{eq}} = 0.10$.\n- Vanilla-like: $A_{\\text{van}} = B R(0.3, 0.4, 0.2)$ with $B = \\begin{bmatrix}1.00  0.04  0.02\\\\ 0.01  1.00  0.03\\\\ -0.02  0.00  1.02\\end{bmatrix}$, $t_{\\text{van}} = (-0.5, 0.8, -0.3)$, $\\epsilon_{\\text{van}} = 0.30$.\n\nCase $4$ (boundary: identity, no noise):\n- Equivariant-like: $A_{\\text{eq}} = I_{3}$, $t_{\\text{eq}} = (0.0, 0.0, 0.0)$, $\\epsilon_{\\text{eq}} = 0.00$.\n- Vanilla-like: $A_{\\text{van}} = I_{3}$, $t_{\\text{van}} = (0.0, 0.0, 0.0)$, $\\epsilon_{\\text{van}} = 0.00$.\n\nComputation requirements:\n- Implement the optimal rigid alignment with determinant constraint $+\\!1$ as described, compute $\\mathrm{RMSD}(Y,X)$ for both $Y_{\\text{eq}}$ and $Y_{\\text{van}}$ in each case, and then compute the improvement $\\Delta_k$ for $k \\in \\{1,2,3,4\\}$ as $\\Delta_k = \\mathrm{RMSD}_{\\text{vanilla},k} - \\mathrm{RMSD}_{\\text{equivariant},k}$.\n- All RMSD values and improvements must be computed and reported in Angstroms as real numbers.\n\nFinal output format:\n- Your program should produce a single line of output containing the four improvement values $\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4$ as a comma-separated list enclosed in square brackets, for example $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$. Use the exact order of cases $1$ through $4$.\n\nNote: Do not use any randomness; use the deterministic noise $n_i(\\epsilon)$ exactly as specified. Angles are in radians, and distances are in Angstroms.",
            "solution": "The task is to implement a function to compute the alignment-invariant root-mean-square deviation (RMSD) between two molecular conformers and to use this function to evaluate the performance difference between an equivariant and a non-equivariant generative model. The problem is scientifically sound and well-posed, grounded in the established principles of structural alignment in computational biology, specifically the Kabsch algorithm for optimal rigid superposition.\n\nBefore proceeding, it is necessary to clarify minor notational inconsistencies in the problem statement to ensure a correct implementation. The problem defines coordinates as row vectors in matrices $X, Y \\in \\mathbb{R}^{N \\times 3}$. Standard matrix operations for such a convention are assumed.\n1.  Centering coordinates: The expression $\\tilde{X} = X - \\mathbf{1} c_X^\\top$ is notationally problematic. Assuming $c_X$ is a $1 \\times 3$ row vector (the mean of the rows of $X$), and $\\mathbf{1}$ is an $N \\times 1$ column vector of ones, the operation should be $\\tilde{X} = X - \\mathbf{1} c_X$. In practice, this is achieved by subtracting the mean vector from each row of the coordinate matrix, an operation well-supported by broadcasting in numerical libraries.\n2.  Optimal translation: The expression for the optimal translation vector $t^\\star = c_X - R^\\star c_Y$ involves an invalid matrix-vector product $R^\\star c_Y$ for a $3 \\times 3$ matrix $R^\\star$ and a $1 \\times 3$ row vector $c_Y$. The correct expression for row vectors is $t^\\star = c_X - c_Y R^\\star$. However, the specified RMSD formula, $\\mathrm{RMSD}(Y,X) = \\sqrt{\\frac{1}{N}\\sum_{i=1}^N \\lVert (y_i - c_Y) R^\\star - (x_i - c_X) \\rVert_2^2}$, correctly depends only on the centered coordinates. This means the translational component is implicitly handled by the centering step, and the explicit calculation of $t^\\star$ is not required for the RMSD.\n\nWith these clarifications, the solution proceeds by implementing the Kabsch algorithm and applying it to the specified test cases.\n\nFirst, we define a function to compute the RMSD. Let the reference coordinates be $X \\in \\mathbb{R}^{N \\times 3}$ and the predicted coordinates be $Y \\in \\mathbb{R}^{N \\times 3}$.\n\nStep 1: Center the Coordinate Sets\nThe first step in rigid alignment is to remove the translational component by moving the centroid of each conformer to the origin. The centroids are calculated as:\n$$ c_X = \\frac{1}{N} \\sum_{i=1}^N x_i $$\n$$ c_Y = \\frac{1}{N} \\sum_{i=1}^N y_i $$\nThe centered coordinate matrices, $\\tilde{X}$ and $\\tilde{Y}$, are then obtained by subtracting the respective centroids from each coordinate vector:\n$$ \\tilde{X} = X - \\mathbf{1}c_X $$\n$$ \\tilde{Y} = Y - \\mathbf{1}c_Y $$\nHere, $x_i$ and $y_i$ are the $i$-th rows of $X$ and $Y$, and $\\mathbf{1}$ is an $N \\times 1$ column vector of ones.\n\nStep 2: Compute the Covariance Matrix\nThe optimal rotation is found by maximizing the overlap between the two centered point sets. This is achieved by finding the rotation $R$ that minimizes $\\sum_{i=1}^N \\|(y_i - c_Y)R - (x_i - c_X)\\|^2$. The solution involves the covariance matrix $H$ between the two centered sets:\n$$ H = \\tilde{Y}^\\top \\tilde{X} \\in \\mathbb{R}^{3 \\times 3} $$\n\nStep 3: Singular Value Decomposition (SVD)\nThe covariance matrix $H$ is decomposed using SVD:\n$$ H = U \\Sigma V^\\top $$\nwhere $U$ and $V$ are $3 \\times 3$ orthogonal matrices, and $\\Sigma$ is a $3 \\times 3$ diagonal matrix of non-negative singular values.\n\nStep 4: Compute the Optimal Rotation Matrix\nThe optimal rotation matrix $R^\\star$ that maximizes the overlap is given by:\n$$ R^\\star = V U^\\top $$\nHowever, this matrix $R^\\star$ could represent a rotation ($\\det(R^\\star) = +1$) or a reflection-rotation ($\\det(R^\\star) = -1$). To ensure we find a pure rotation that preserves chirality, we must enforce the constraint $\\det(R^\\star) = +1$. This is done by checking the sign of the determinant. If $\\det(R^\\star) = -1$, it indicates that the optimal transformation is improper. To find the best *proper* rotation, we invert the component of the transformation corresponding to the smallest singular value. As specified, if $\\det(V U^\\top) = -1$, we let $V' = V \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  -1 \\end{pmatrix}$, then $R^\\star = V' U^\\top$. This procedure guarantees that $\\det(R^\\star)=1$.\n\nStep 5: Calculate the RMSD\nWith the optimal proper rotation $R^\\star$, the RMSD is calculated as the square root of the mean of the squared Euclidean distances between the aligned predicted coordinates and the reference coordinates:\n$$ \\mathrm{RMSD}(Y,X) = \\sqrt{\\frac{1}{N}\\sum_{i=1}^N \\lVert (y_i - c_Y) R^\\star - (x_i - c_X) \\rVert_2^2} $$\nThis is equivalent to calculating the Frobenius norm of the difference between the aligned centered matrices:\n$$ \\mathrm{RMSD}(Y,X) = \\sqrt{\\frac{1}{N} \\lVert \\tilde{Y} R^\\star - \\tilde{X} \\rVert_F^2} $$\n\nNext, we programmatically construct the test cases. A helper function is created to generate rotation matrices $R(\\alpha, \\beta, \\gamma) = R_z(\\gamma) R_y(\\beta) R_x(\\alpha)$ given Euler angles. Another function generates the deterministic noise vector $n_i(\\epsilon)$.\n\nFor each of the four test cases, we generate the \"equivariant-like\" coordinates $Y_{\\text{eq}}$ and \"vanilla-like\" coordinates $Y_{\\text{van}}$. For a given affine transformation $(A, t)$ and noise level $\\epsilon$, the predicted coordinates $Y$ are generated from the reference $X$ by applying the linear transformation and translation, followed by the addition of noise:\n$$ Y_i = X_i A^\\top + t + n_i(\\epsilon) \\quad \\text{for each atom } i=0, \\dots, 6 $$\n\nFinally, for each case $k \\in \\{1,2,3,4\\}$, we compute $\\mathrm{RMSD}_{\\text{equivariant},k} = \\mathrm{RMSD}(Y_{\\text{eq},k}, X)$ and $\\mathrm{RMSD}_{\\text{vanilla},k} = \\mathrm{RMSD}(Y_{\\text{van},k}, X)$. The improvement metric is then calculated as $\\Delta_k = \\mathrm{RMSD}_{\\text{vanilla},k} - \\mathrm{RMSD}_{\\text{equivariant},k}$. The expected results are:\n-   Case 1: $\\Delta_1 > 0$. The vanilla model's output is distorted by anisotropic scaling, which cannot be corrected by a rigid alignment.\n-   Case 2: $\\Delta_2 > 0$. The vanilla model's output is a reflection of the true structure. The alignment must find a proper rotation, leaving a large residual RMSD. The equivariant model, with $\\epsilon=0$, should have $\\mathrm{RMSD}=0$.\n-   Case 3: $\\Delta_3 > 0$. The shear transformation applied to the vanilla model causes a non-rigid distortion, leading to a higher RMSD.\n-   Case 4: $\\Delta_4 = 0$. Both models produce the exact reference structure, resulting in zero RMSD for both and thus zero improvement. This serves as a sanity check.\n\nThe implementation will follow these steps to calculate the four $\\Delta_k$ values.",
            "answer": "[0.1706240224449887, 0.7200592984534727, 0.3013627253503527, 0.0]"
        },
        {
            "introduction": "Beyond evaluating individual molecules, a successful generative model must capture the broader distribution of a target chemical space. This practice introduces the Fréchet ChemNet Distance (FCD), a powerful metric for this purpose, by modeling molecular embedding spaces as Gaussian distributions. You will derive the closed-form 2-Wasserstein distance between these distributions from first principles , providing a rigorous tool for quantifying how well a generated set of molecules resembles a reference chemical library.",
            "id": "3847972",
            "problem": "In computational chemical biology, a common approach to quantitatively evaluate the similarity between the distributions of molecular embeddings produced by a generative model and those of a reference dataset is to model the learned embeddings as multivariate Gaussian distributions and to measure their discrepancy using the quadratic-cost optimal transport distance. The Fréchet ChemNet Distance (FCD) is defined as the squared quadratic-cost optimal transport distance between two Gaussian approximations of ChemNet embeddings for a generated molecule set and a reference molecule set. ChemNet is a neural network trained to learn chemically meaningful embeddings from molecular representations. \n\nStarting from first principles, use the definition of the quadratic-cost $2$-Wasserstein distance between probability measures on $\\mathbb{R}^{d}$,\n$$\nW_{2}^{2}(\\mu,\\nu) \\equiv \\inf_{\\gamma \\in \\Gamma(\\mu,\\nu)} \\int_{\\mathbb{R}^{d} \\times \\mathbb{R}^{d}} \\|x-y\\|^{2} \\, \\mathrm{d}\\gamma(x,y),\n$$\nwhere $\\Gamma(\\mu,\\nu)$ is the set of couplings with marginals $\\mu$ and $\\nu$, and the characterization of Gaussian measures, to derive the closed-form Fréchet ChemNet Distance under the assumption that the ChemNet embedding vectors for the reference and generated sets are distributed as multivariate normal. Then, compute the closed-form $2$-Wasserstein distance between the following two Gaussian distributions that approximate the reference and generated ChemNet embedding distributions in de novo drug design:\n\n- Reference embeddings: $X_{\\mathrm{ref}} \\sim \\mathcal{N}(\\mu_{\\mathrm{ref}}, \\Sigma_{\\mathrm{ref}})$ with $\\mu_{\\mathrm{ref}} = \\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix}$ and $\\Sigma_{\\mathrm{ref}} = \\mathrm{diag}(1,\\,4,\\,9)$.\n- Generated embeddings: $X_{\\mathrm{gen}} \\sim \\mathcal{N}(\\mu_{\\mathrm{gen}}, \\Sigma_{\\mathrm{gen}})$ with $\\mu_{\\mathrm{gen}} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}$ and $\\Sigma_{\\mathrm{gen}} = \\mathrm{diag}(4,\\,1,\\,16)$.\n\nYour derivation must begin from the definition of $W_{2}^{2}$ above and from core properties of Gaussian measures and optimal transport, without presupposing any target formula. Identify the Fréchet ChemNet Distance (FCD) arising in this Gaussian setting, and finally report the exact analytical value of the $2$-Wasserstein distance $W_{2}$ between $X_{\\mathrm{ref}}$ and $X_{\\mathrm{gen}}$. Express the final answer with no units and do not round; provide the exact expression.",
            "solution": "The problem requires the derivation of the closed-form expression for the Fréchet ChemNet Distance (FCD) between two multivariate Gaussian distributions and its subsequent calculation for a specific pair of distributions. The FCD is defined as the squared $2$-Wasserstein distance, $W_2^2$.\n\nLet the two multivariate Gaussian distributions for the reference and generated embeddings be $\\mu = \\mathcal{N}(\\mu_1, \\Sigma_1)$ and $\\nu = \\mathcal{N}(\\mu_2, \\Sigma_2)$ on $\\mathbb{R}^{d}$. The squared $2$-Wasserstein distance is defined as:\n$$\nW_{2}^{2}(\\mu, \\nu) = \\inf_{\\gamma \\in \\Gamma(\\mu, \\nu)} \\int_{\\mathbb{R}^{d} \\times \\mathbb{R}^{d}} \\|x-y\\|^{2} \\, \\mathrm{d}\\gamma(x,y)\n$$\nwhere $\\Gamma(\\mu, \\nu)$ is the set of all joint probability measures (couplings) on $\\mathbb{R}^{d} \\times \\mathbb{R}^{d}$ with marginals $\\mu$ and $\\nu$. Let $(X, Y)$ be a pair of random vectors with joint distribution $\\gamma$. Then $X \\sim \\mu$ and $Y \\sim \\nu$. The integral is the expectation $\\mathbb{E}_{\\gamma}[\\|X-Y\\|^2]$.\n\nFirst, we decompose the squared norm term based on the means:\n$$\n\\|x-y\\|^2 = \\|(x-\\mu_1) - (y-\\mu_2) + (\\mu_1-\\mu_2)\\|^2\n$$\nExpanding this, we get:\n$$\n\\|x-y\\|^2 = \\|(x-\\mu_1) - (y-\\mu_2)\\|^2 + \\|\\mu_1-\\mu_2\\|^2 + 2((x-\\mu_1) - (y-\\mu_2))^T(\\mu_1-\\mu_2)\n$$\nTaking the expectation with respect to $\\gamma$, we have:\n$$\n\\mathbb{E}_{\\gamma}[\\|X-Y\\|^2] = \\mathbb{E}_{\\gamma}[\\|(X-\\mu_1) - (Y-\\mu_2)\\|^2] + \\|\\mu_1-\\mu_2\\|^2 + 2\\mathbb{E}_{\\gamma}[((X-\\mu_1) - (Y-\\mu_2))^T(\\mu_1-\\mu_2)]\n$$\nThe last term is zero because $\\mathbb{E}_{\\gamma}[X-\\mu_1] = \\mathbb{E}_{\\mu}[X]-\\mu_1 = \\mu_1-\\mu_1 = 0$ and $\\mathbb{E}_{\\gamma}[Y-\\mu_2] = \\mathbb{E}_{\\nu}[Y]-\\mu_2 = \\mu_2-\\mu_2 = 0$.\nThe term $\\|\\mu_1-\\mu_2\\|^2$ is constant with respect to the coupling $\\gamma$. Therefore, minimizing the expectation is equivalent to minimizing the first term, which corresponds to the squared $2$-Wasserstein distance between the centered distributions $\\mathcal{N}(0, \\Sigma_1)$ and $\\mathcal{N}(0, \\Sigma_2)$.\nThis establishes the decomposition:\n$$\nW_{2}^{2}(\\mathcal{N}(\\mu_1, \\Sigma_1), \\mathcal{N}(\\mu_2, \\Sigma_2)) = \\|\\mu_1-\\mu_2\\|^2 + W_{2}^{2}(\\mathcal{N}(0, \\Sigma_1), \\mathcal{N}(0, \\Sigma_2))\n$$\nNow, we focus on the centered case. Let $\\tilde{X} \\sim \\mathcal{N}(0, \\Sigma_1)$ and $\\tilde{Y} \\sim \\mathcal{N}(0, \\Sigma_2)$. We seek to compute:\n$$\nW_{2}^{2}(\\mathcal{N}(0, \\Sigma_1), \\mathcal{N}(0, \\Sigma_2)) = \\inf_{\\gamma} \\mathbb{E}_{\\gamma}[\\|\\tilde{X}-\\tilde{Y}\\|^2]\n$$\nExpanding the norm:\n$$\n\\mathbb{E}_{\\gamma}[\\|\\tilde{X}-\\tilde{Y}\\|^2] = \\mathbb{E}_{\\gamma}[\\tilde{X}^T\\tilde{X} - 2\\tilde{X}^T\\tilde{Y} + \\tilde{Y}^T\\tilde{Y}] = \\mathbb{E}_{\\gamma}[\\tilde{X}^T\\tilde{X}] + \\mathbb{E}_{\\gamma}[\\tilde{Y}^T\\tilde{Y}] - 2\\mathbb{E}_{\\gamma}[\\tilde{X}^T\\tilde{Y}]\n$$\nSince the marginals are fixed, the first two terms are constant:\n$$\n\\mathbb{E}_{\\gamma}[\\tilde{X}^T\\tilde{X}] = \\mathbb{E}_{\\gamma}[\\text{Tr}(\\tilde{X}\\tilde{X}^T)] = \\text{Tr}(\\mathbb{E}_{\\gamma}[\\tilde{X}\\tilde{X}^T]) = \\text{Tr}(\\text{Cov}(\\tilde{X})) = \\text{Tr}(\\Sigma_1)\n$$\n$$\n\\mathbb{E}_{\\gamma}[\\tilde{Y}^T\\tilde{Y}] = \\text{Tr}(\\Sigma_2)\n$$\nThe problem thus reduces to maximizing the cross-term $\\mathbb{E}_{\\gamma}[\\tilde{X}^T\\tilde{Y}]$ over all valid couplings $\\gamma$.\n$$\n\\mathbb{E}_{\\gamma}[\\tilde{X}^T\\tilde{Y}] = \\mathbb{E}_{\\gamma}[\\text{Tr}(\\tilde{Y}\\tilde{X}^T)] = \\text{Tr}(\\mathbb{E}_{\\gamma}[\\tilde{Y}\\tilde{X}^T]) = \\text{Tr}(\\text{Cov}(\\tilde{X}, \\tilde{Y}))\n$$\nThe task is to find the supremum of $\\text{Tr}(\\Sigma_{12})$ where $\\Sigma_{12} = \\text{Cov}(\\tilde{X}, \\tilde{Y})$ over all possible couplings. For a joint distribution of $(\\tilde{X}, \\tilde{Y})$ to be a valid coupling, the covariance matrix of the concatenated vector $(\\tilde{X}^T, \\tilde{Y}^T)^T$ must be positive semi-definite (PSD):\n$$\n\\begin{pmatrix} \\Sigma_1  \\Sigma_{12} \\\\ \\Sigma_{12}^T  \\Sigma_2 \\end{pmatrix} \\succeq 0\n$$\nA fundamental result in optimal transport for Gaussian measures, which can be derived from properties of PSD matrices and trace inequalities, states that the maximum achievable value for this trace is given by:\n$$\n\\sup_{\\gamma} \\text{Tr}(\\text{Cov}(\\tilde{X}, \\tilde{Y})) = \\text{Tr}((\\Sigma_1^{1/2} \\Sigma_2 \\Sigma_1^{1/2})^{1/2})\n$$\nwhere $A^{1/2}$ denotes the unique positive semi-definite square root of a PSD matrix $A$.\n\nCombining all the pieces, the squared $2$-Wasserstein distance between the two centered Gaussians is:\n$$\nW_{2}^{2}(\\mathcal{N}(0, \\Sigma_1), \\mathcal{N}(0, \\Sigma_2)) = \\text{Tr}(\\Sigma_1) + \\text{Tr}(\\Sigma_2) - 2 \\text{Tr}((\\Sigma_1^{1/2} \\Sigma_2 \\Sigma_1^{1/2})^{1/2})\n$$\nTherefore, the general closed-form expression for the Fréchet ChemNet Distance (FCD), which is $W_2^2$, is:\n$$\n\\text{FCD} = W_{2}^{2}(\\mathcal{N}(\\mu_1, \\Sigma_1), \\mathcal{N}(\\mu_2, \\Sigma_2)) = \\|\\mu_1-\\mu_2\\|^2 + \\text{Tr}(\\Sigma_1) + \\text{Tr}(\\Sigma_2) - 2 \\text{Tr}((\\Sigma_1^{1/2} \\Sigma_2 \\Sigma_1^{1/2})^{1/2})\n$$\nThis completes the derivation.\n\nNow, we compute this distance for the given distributions:\n-   Reference distribution: $\\mu_1 = \\mu_{\\mathrm{ref}} = \\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix}$, $\\Sigma_1 = \\Sigma_{\\mathrm{ref}} = \\mathrm{diag}(1, 4, 9)$\n-   Generated distribution: $\\mu_2 = \\mu_{\\mathrm{gen}} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}$, $\\Sigma_2 = \\Sigma_{\\mathrm{gen}} = \\mathrm{diag}(4, 1, 16)$\n\n$1$. Compute the squared norm of the difference in means:\n$$\n\\|\\mu_{\\mathrm{ref}} - \\mu_{\\mathrm{gen}}\\|^2 = \\left\\| \\begin{pmatrix} 1-0 \\\\ -2-1 \\\\ 3-1 \\end{pmatrix} \\right\\|^2 = \\left\\| \\begin{pmatrix} 1 \\\\ -3 \\\\ 2 \\end{pmatrix} \\right\\|^2 = 1^2 + (-3)^2 + 2^2 = 1 + 9 + 4 = 14\n$$\n\n$2$. Compute the traces of the covariance matrices:\n$$\n\\text{Tr}(\\Sigma_{\\mathrm{ref}}) = 1 + 4 + 9 = 14\n$$\n$$\n\\text{Tr}(\\Sigma_{\\mathrm{gen}}) = 4 + 1 + 16 = 21\n$$\n\n$3$. Compute the trace of the covariance cross-term. Since $\\Sigma_{\\mathrm{ref}}$ and $\\Sigma_{\\mathrm{gen}}$ are diagonal matrices, they commute. When $\\Sigma_1$ and $\\Sigma_2$ commute, the expression simplifies:\n$$\n(\\Sigma_1^{1/2} \\Sigma_2 \\Sigma_1^{1/2})^{1/2} = (\\Sigma_1 \\Sigma_2)^{1/2} = \\Sigma_1^{1/2} \\Sigma_2^{1/2}\n$$\nWe shall compute $\\Sigma_1^{1/2}$ and $\\Sigma_2^{1/2}$:\n$$\n\\Sigma_{\\mathrm{ref}}^{1/2} = \\mathrm{diag}(\\sqrt{1}, \\sqrt{4}, \\sqrt{9}) = \\mathrm{diag}(1, 2, 3) = \\begin{pmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  3 \\end{pmatrix}\n$$\n$$\n\\Sigma_{\\mathrm{gen}}^{1/2} = \\mathrm{diag}(\\sqrt{4}, \\sqrt{1}, \\sqrt{16}) = \\mathrm{diag}(2, 1, 4) = \\begin{pmatrix} 2  0  0 \\\\ 0  1  0 \\\\ 0  0  4 \\end{pmatrix}\n$$\nNow, compute their product:\n$$\n\\Sigma_{\\mathrm{ref}}^{1/2} \\Sigma_{\\mathrm{gen}}^{1/2} = \\begin{pmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  3 \\end{pmatrix} \\begin{pmatrix} 2  0  0 \\\\ 0  1  0 \\\\ 0  0  4 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 2  0  0 \\\\ 0  2 \\cdot 1  0 \\\\ 0  0  3 \\cdot 4 \\end{pmatrix} = \\begin{pmatrix} 2  0  0 \\\\ 0  2  0 \\\\ 0  0  12 \\end{pmatrix}\n$$\nThe trace of this matrix is:\n$$\n\\text{Tr}(\\Sigma_{\\mathrm{ref}}^{1/2} \\Sigma_{\\mathrm{gen}}^{1/2}) = 2 + 2 + 12 = 16\n$$\n\n$4$. Assemble the final value for $W_2^2$:\n$$\nW_2^2 = 14 + 14 + 21 - 2(16) = 49 - 32 = 17\n$$\nThe problem asks for the $2$-Wasserstein distance, $W_2$, which is the square root of this value.\n$$\nW_2 = \\sqrt{17}\n$$\nThis is the exact analytical value of the distance.",
            "answer": "$$\n\\boxed{\\sqrt{17}}\n$$"
        }
    ]
}