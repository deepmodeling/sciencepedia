{
    "hands_on_practices": [
        {
            "introduction": "评估蛋白质结构预测的准确性是计算结构生物学中的一项基本任务。最广泛使用的指标之一是均方根偏差（Root-Mean-Square Deviation, RMSD），它量化了预测坐标与实验确定的真实坐标之间的平均距离。然而，由于模型的输出可能与真实结构存在任意的全局旋转和平移，直接计算坐标差异是无意义的。本练习将指导您计算未对齐和最佳叠合后的RMSD，通过一个具体案例来深刻理解刚体变换对结构评估的决定性影响。",
            "id": "3842224",
            "problem": "在计算化学生物学中，一个基于图神经网络的预测器输出一个短肽段的四个α-碳原子的笛卡尔坐标。由于该网络使用旋转和平移不变的损失函数进行训练，模型输出可能与实验坐标因全局刚体运动而不同。给定以埃（angstrom）为单位的实验测定的目标坐标和模型预测的四个对应原子的坐标（在两组坐标中索引一致）。坐标如下：\n目标坐标（实验）：$t_{1} = (0,\\,0,\\,0)$，$t_{2} = (1,\\,0,\\,0)$，$t_{3} = (0,\\,2,\\,0)$，$t_{4} = (1,\\,2,\\,0)$。\n预测坐标（模型）：$p_{1} = (3,\\,-1,\\,0)$，$p_{2} = (3,\\,0,\\,0)$，$p_{3} = (1,\\,-1,\\,0)$，$p_{4} = (1,\\,0,\\,0)$。\n\n仅使用均方根偏差（RMSD）的基本定义（即对应原子间欧几里得距离平方的均值的平方根），以及最优叠合是包含一个旋转（三维正交矩阵）和一个平移的刚体变换这一物理约束，完成以下任务：\n1. 计算预测坐标和目标坐标之间未经任何对齐的RMSD（“未对齐RMSD”），即直接从$\\{p_{i}\\}$和$\\{t_{i}\\}$计算。\n2. 在最优刚体叠合下计算最小RMSD（“对齐RMSD”），即找到使RMSD最小化的旋转和平移，并评估该最小RMSD。\n3. 给出未对齐RMSD和对齐RMSD之间的数值差异。\n\n将所有三个报告量四舍五入至四位有效数字。以埃为单位表示RMSD。在你的最终数值答案中，不要包含单位。",
            "solution": "我们从计算化学生物学中蛋白质结构比较相关的基本定义和约束开始。两组对应的笛卡尔坐标 $\\{t_{i}\\}_{i=1}^{N}$ 和 $\\{p_{i}\\}_{i=1}^{N}$ 之间的均方根偏差（RMSD）定义为\n$$\n\\mathrm{RMSD} = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N} \\|p_{i} - t_{i}\\|^{2}},\n$$\n其中 $\\|\\cdot\\|$ 表示欧几里得范数。除非应用了最优刚体叠合，否则该定义是依赖于坐标系的。刚体叠合是一种变换，包括一个满足 $R^{\\top} R = I$ 和 $\\det(R)=1$ 的旋转矩阵 $R \\in \\mathbb{R}^{3\\times 3}$，以及一个平移向量 $d \\in \\mathbb{R}^{3}$。变换后的预测坐标为 $p_{i}^{\\mathrm{aligned}} = R p_{i} + d$。对齐后的RMSD为\n$$\n\\mathrm{RMSD}_{\\mathrm{aligned}} = \\min_{R,\\,d} \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N} \\|R p_{i} + d - t_{i}\\|^{2}},\n$$\n约束条件为 $R^{\\top}R = I$ 和 $\\det(R)=1$。\n\n第1部分：未对齐RMSD。我们直接计算欧几里得距离平方的均值。对于每个索引 $i$，平方距离为 $\\|p_{i}-t_{i}\\|^{2}$。\n- 对于 $i=1$：$p_{1}-t_{1} = (3,\\,-1,\\,0) - (0,\\,0,\\,0) = (3,\\,-1,\\,0)$，所以 $\\|p_{1}-t_{1}\\|^{2} = 3^{2} + (-1)^{2} + 0^{2} = 9 + 1 + 0 = 10$。\n- 对于 $i=2$：$p_{2}-t_{2} = (3,\\,0,\\,0) - (1,\\,0,\\,0) = (2,\\,0,\\,0)$，所以 $\\|p_{2}-t_{2}\\|^{2} = 2^{2} + 0^{2} + 0^{2} = 4$。\n- 对于 $i=3$：$p_{3}-t_{3} = (1,\\,-1,\\,0) - (0,\\,2,\\,0) = (1,\\,-3,\\,0)$，所以 $\\|p_{3}-t_{3}\\|^{2} = 1^{2} + (-3)^{2} + 0^{2} = 1 + 9 + 0 = 10$。\n- 对于 $i=4$：$p_{4}-t_{4} = (1,\\,0,\\,0) - (1,\\,2,\\,0) = (0,\\,-2,\\,0)$，所以 $\\|p_{4}-t_{4}\\|^{2} = 0^{2} + (-2)^{2} + 0^{2} = 4$。\n因此，距离平方的均值为\n$$\n\\frac{1}{4}(10 + 4 + 10 + 4) = \\frac{28}{4} = 7,\n$$\n未对齐RMSD为\n$$\n\\mathrm{RMSD}_{\\mathrm{unaligned}} = \\sqrt{7}。\n$$\n\n第2部分：通过最优刚体叠合计算对齐RMSD。刚体变换保持成对距离不变，并且可以解释方向和位置上的全局差异。为了确定预测坐标是否是目标坐标的刚体变换，我们寻找 $R$ 和 $d$ 使得对于所有 $i$ 都有 $p_{i} = R t_{i} + d$。考虑绕 $z$ 轴旋转角度 $\\theta = \\frac{\\pi}{2}$。其矩阵为\n$$\nR_{z}\\!\\left(\\frac{\\pi}{2}\\right) =\n\\begin{pmatrix}\n0 & -1 & 0 \\\\\n1 & \\phantom{-}0 & 0 \\\\\n0 & \\phantom{-}0 & 1\n\\end{pmatrix}。\n$$\n设平移为\n$$\nd = \\begin{pmatrix} 3 \\\\ -1 \\\\ 0 \\end{pmatrix}。\n$$\n我们对每个目标点验证该映射：\n- $R_{z}\\!\\left(\\frac{\\pi}{2}\\right) t_{1} + d = R_{z}\\!\\left(\\frac{\\pi}{2}\\right) (0,\\,0,\\,0)^{\\top} + (3,\\,-1,\\,0)^{\\top} = (3,\\,-1,\\,0)^{\\top} = p_{1}$。\n- $R_{z}\\!\\left(\\frac{\\pi}{2}\\right) t_{2} + d = R_{z}\\!\\left(\\frac{\\pi}{2}\\right) (1,\\,0,\\,0)^{\\top} + (3,\\,-1,\\,0)^{\\top} = (0,\\,1,\\,0)^{\\top} + (3,\\,-1,\\,0)^{\\top} = (3,\\,0,\\,0)^{\\top} = p_{2}$。\n- $R_{z}\\!\\left(\\frac{\\pi}{2}\\right) t_{3} + d = R_{z}\\!\\left(\\frac{\\pi}{2}\\right) (0,\\,2,\\,0)^{\\top} + (3,\\,-1,\\,0)^{\\top} = (-2,\\,0,\\,0)^{\\top} + (3,\\,-1,\\,0)^{\\top} = (1,\\,-1,\\,0)^{\\top} = p_{3}$。\n- $R_{z}\\!\\left(\\frac{\\pi}{2}\\right) t_{4} + d = R_{z}\\!\\left(\\frac{\\pi}{2}\\right) (1,\\,2,\\,0)^{\\top} + (3,\\,-1,\\,0)^{\\top} = (-2,\\,1,\\,0)^{\\top} + (3,\\,-1,\\,0)^{\\top} = (1,\\,0,\\,0)^{\\top} = p_{4}$。\n因此，预测坐标正是目标坐标的一个刚体变换，旋转矩阵为 $R = R_{z}\\!\\left(\\frac{\\pi}{2}\\right)$，平移向量为 $d = (3,\\,-1,\\,0)^{\\top}$。最优叠合问题的最小值是通过逆变换将预测坐标集对齐回目标坐标集来实现的，即应用 $R^{\\top}$ 和适当的平移，使得对于对齐质心的平移 $d'$ 有 $R^{\\top}(R t_{i} + d - d') = t_{i}$（等效于撤销 $R$ 和 $d$）。由于存在一个精确的映射，最小化的距离平方和为零：\n$$\n\\sum_{i=1}^{4} \\|R p_{i} + d' - t_{i}\\|^{2} = 0 \\quad \\Rightarrow \\quad \\mathrm{RMSD}_{\\mathrm{aligned}} = 0.\n$$\n这个结论与 Kabsch 算法一致，该算法通过最大化 $R$ 与互协方差矩阵乘积的迹来找到旋转矩阵；在这里，存在精确刚体映射意味着 Kabsch 解能够恢复 $R$（在数值精度范围内），从而产生零残差。\n\n第3部分：未对齐RMSD和对齐RMSD之差。这个差值为\n$$\n\\Delta = \\mathrm{RMSD}_{\\mathrm{unaligned}} - \\mathrm{RMSD}_{\\mathrm{aligned}} = \\sqrt{7} - 0 = \\sqrt{7}。\n$$\n\n数值和四舍五入至四位有效数字：\n- $\\sqrt{7} \\approx 2.64575131\\ldots \\Rightarrow 2.646$（四位有效数字）。\n- $0$ 仍然是 $0$。\n- $\\Delta \\approx 2.646$（四位有效数字）。\n\n在深度学习蛋白质结构预测背景下的解释：巨大的未对齐RMSD源于全局坐标系（旋转和平移）的差异，而非实际的结构差异。最优叠合消除了坐标系的差异，揭示了预测器精确地复现了目标几何构型，其对齐RMSD为零。这说明了为什么用于三维分子预测的训练目标和评估指标通常包含刚体不变性，以确保损失函数反映的是真实的几何不匹配，而不是任意的坐标系选择。",
            "answer": "$$\\boxed{\\begin{pmatrix}2.646 & 0 & 2.646\\end{pmatrix}}$$"
        },
        {
            "introduction": "现代蛋白质结构预测模型，如AlphaFold2，通过端到端（end-to-end）的深度学习进行训练，这意味着模型的输出（原子坐标）可以直接用于计算与真实结构的偏差，并指导模型参数的优化。这一过程要求损失函数必须是可微的，以便通过梯度下降进行学习。本练习将引导您实现局部距离差异检验（lDDT）——一个比RMSD更精细的评估指标，并构建其可微的“软”版本，最终推导并计算用于反向传播的梯度。这是理解现代结构预测模型如何学习生成精确结构的核心实践。",
            "id": "4554883",
            "problem": "给定两组三维坐标集，分别代表蛋白质中残基的α-碳原子（Cα）的预测位置和真实位置。您的任务有两部分：首先，使用标准的基于指示函数的定义，计算每个残基的局部距离差异检验（lDDT-Cα）分数；其次，构建lDDT的可微代理，并执行反向传播，以计算标量损失相对于预测坐标的梯度。所有距离均以埃（ångström）为单位（缩写为Å），最终的lDDT分数是位于区间$[0,1]$内的无量纲值。\n\n基本定义和基础：\n- 局部距离差异检验（lDDT）是一种基于接触的度量标准，用于评估局部几何一致性。对于每个残基 $i$，定义其邻居集为$$\\mathcal{N}_i = \\{ j \\in \\{1,\\dots,N\\} \\setminus \\{i\\} \\mid d^{\\mathrm{true}}_{ij} \\le R \\},$$ 其中 $d^{\\mathrm{true}}_{ij}$ 是残基 $i$ 和 $j$ 的真实Cα坐标之间的欧几里得距离，而 $R$ 是一个以Å为单位的固定邻居半径。每个残基的lDDT是其邻居中，预测距离误差在指定阈值内的邻居所占的比例。给定阈值集合 $\\mathcal{T} = \\{0.5, 1.0, 2.0, 4.0\\}$（单位均为Å），定义基于硬指示函数的每个残基lDDT分数为$$\\mathrm{lDDT}_i^{\\mathrm{hard}} = \\begin{cases} \\dfrac{1}{|\\mathcal{N}_i|} \\sum\\limits_{j \\in \\mathcal{N}_i} \\left( \\dfrac{1}{|\\mathcal{T}|} \\sum\\limits_{t \\in \\mathcal{T}} \\mathbf{1}\\left( \\left| d^{\\mathrm{pred}}_{ij} - d^{\\mathrm{true}}_{ij} \\right|  t \\right) \\right),  |\\mathcal{N}_i| > 0 \\\\[8pt] 0,  |\\mathcal{N}_i| = 0 \\end{cases}$$ 其中 $d^{\\mathrm{pred}}_{ij}$ 是残基 $i$ 和 $j$ 的预测Cα坐标之间的欧几里得距离，$\\mathbf{1}(\\cdot)$ 是指示函数。\n- 可微代理用平滑函数替代不可微的绝对值和指示函数。令 $$e_{ij} = d^{\\mathrm{pred}}_{ij} - d^{\\mathrm{true}}_{ij}, \\quad a_{ij} = \\sqrt{\\varepsilon + e_{ij}^2},$$ 其中 $\\varepsilon > 0$ 是一个小的平滑参数。对于每个阈值 $t \\in \\mathcal{T}$，定义软满足分数为 $$s_{ij}(t) = \\sigma\\!\\left( -\\dfrac{a_{ij} - t}{\\beta} \\right),$$ 其中 $\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$ 是logistic函数，$\\beta > 0$ 是一个以Å为单位的软度标度。平滑的每个残基lDDT分数为 $$\\mathrm{lDDT}_i^{\\mathrm{soft}} = \\begin{cases} \\dfrac{1}{|\\mathcal{N}_i|} \\sum\\limits_{j \\in \\mathcal{N}_i} \\left( \\dfrac{1}{|\\mathcal{T}|} \\sum\\limits_{t \\in \\mathcal{T}} s_{ij}(t) \\right),  |\\mathcal{N}_i| > 0 \\\\[8pt] 0,  |\\mathcal{N}_i| = 0 \\end{cases}$$\n- 定义标量损失 $$\\mathcal{L} = 1 - \\dfrac{1}{N} \\sum_{i=1}^N \\mathrm{lDDT}_i^{\\mathrm{soft}},$$ 并计算其相对于每个预测坐标向量 $\\mathbf{x}_i^{\\mathrm{pred}} \\in \\mathbb{R}^3$ 的梯度。使用欧几里得范数定义 $$d^{\\mathrm{pred}}_{ij} = \\left\\| \\mathbf{x}_i^{\\mathrm{pred}} - \\mathbf{x}_j^{\\mathrm{pred}} \\right\\|_2,$$ 并应用链式法则，注意对于 $d^{\\mathrm{pred}}_{ij} > 0$，\n$$\\dfrac{\\partial d^{\\mathrm{pred}}_{ij}}{\\partial \\mathbf{x}_i^{\\mathrm{pred}}} = \\dfrac{\\mathbf{x}_i^{\\mathrm{pred}} - \\mathbf{x}_j^{\\mathrm{pred}}}{d^{\\mathrm{pred}}_{ij}}, \\quad \\dfrac{\\partial d^{\\mathrm{pred}}_{ij}}{\\partial \\mathbf{x}_j^{\\mathrm{pred}}} = \\dfrac{\\mathbf{x}_j^{\\mathrm{pred}} - \\mathbf{x}_i^{\\mathrm{pred}}}{d^{\\mathrm{pred}}_{ij}}.$$\n当 $d^{\\mathrm{pred}}_{ij} = 0$ 时，将导数设为零向量以避免除以零。logistic函数的导数为 $\\sigma'(z) = \\sigma(z)\\left(1 - \\sigma(z)\\right)$。\n\n您的程序必须实现以下功能：\n1. 计算所有残基 $i$ 的 $\\mathrm{lDDT}_i^{\\mathrm{hard}}$，然后计算平均硬lDDT $$\\overline{\\mathrm{lDDT}}^{\\mathrm{hard}} = \\dfrac{1}{N} \\sum_{i=1}^N \\mathrm{lDDT}_i^{\\mathrm{hard}}.$$\n2. 计算所有残基 $i$ 的 $\\mathrm{lDDT}_i^{\\mathrm{soft}}$，然后计算平均软lDDT $$\\overline{\\mathrm{lDDT}}^{\\mathrm{soft}} = \\dfrac{1}{N} \\sum_{i=1}^N \\mathrm{lDDT}_i^{\\mathrm{soft}}.$$\n3. 计算上面定义的标量损失 $\\mathcal{L}$。\n4. 进行反向传播以计算梯度 $\\nabla_{\\mathbf{X}^{\\mathrm{pred}}} \\mathcal{L}$，其中 $\\mathbf{X}^{\\mathrm{pred}} \\in \\mathbb{R}^{N \\times 3}$ 堆叠了所有预测坐标。报告该梯度的欧几里得范数 $$\\left\\| \\nabla_{\\mathbf{X}^{\\mathrm{pred}}} \\mathcal{L} \\right\\|_2 = \\sqrt{ \\sum_{i=1}^N \\left\\| \\dfrac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}_i^{\\mathrm{pred}}} \\right\\|_2^2 }.$$\n\n测试套件：\n使用以下五个测试案例。在所有案例中，距离和阈值的单位为Å，角度不使用。输出必须表示为十进制浮点数（输出中不含百分号或其他单位）。\n\n- 案例1（完美匹配，典型半径）：$N = 5$，真实坐标 $$\\mathbf{X}^{\\mathrm{true}} = \\begin{bmatrix} 0  0  0 \\\\ 3.8  0  0 \\\\ 7.6  0  0 \\\\ 11.4  0  0 \\\\ 15.2  0  0 \\end{bmatrix},$$ 预测坐标 $$\\mathbf{X}^{\\mathrm{pred}} = \\mathbf{X}^{\\mathrm{true}},$$ 邻居半径 $R = 10.0$，阈值 $\\mathcal{T} = \\{0.5, 1.0, 2.0, 4.0\\}$，软度 $\\beta = 0.5$，平滑参数 $\\varepsilon = 10^{-6}$。\n- 案例2（小扰动）：与案例1相同的 $\\mathbf{X}^{\\mathrm{true}}$、$R$、阈值、$\\beta$、$\\varepsilon$。预测坐标 $$\\mathbf{X}^{\\mathrm{pred}} = \\mathbf{X}^{\\mathrm{true}} + \\Delta,$$ 其中 $$\\Delta = \\begin{bmatrix} 0.1  -0.2  0.05 \\\\ -0.05  0.15  -0.1 \\\\ 0.2  0.0  0.2 \\\\ -0.1  -0.15  0.0 \\\\ 0.05  0.05  -0.2 \\end{bmatrix}.$$\n- 案例3（平移不变性）：与案例1相同的 $\\mathbf{X}^{\\mathrm{true}}$、$R$、阈值、$\\beta$、$\\varepsilon$。预测坐标 $$\\mathbf{X}^{\\mathrm{pred}} = \\mathbf{X}^{\\mathrm{true}} + \\mathbf{t}, \\quad \\mathbf{t} = \\begin{bmatrix} 10.0  -5.0  3.0 \\end{bmatrix},$$ 应用于每个残基。\n- 案例4（无邻居的边界情况）：与案例1相同的 $\\mathbf{X}^{\\mathrm{true}}$。预测坐标 $$\\mathbf{X}^{\\mathrm{pred}} = \\mathbf{X}^{\\mathrm{true}},$$ 邻居半径 $R = 2.0$，阈值 $\\mathcal{T} = \\{0.5, 1.0, 2.0, 4.0\\}$，软度 $\\beta = 0.5$，平滑参数 $\\varepsilon = 10^{-6}$。\n- 案例5（全局缩放失真）：与案例1相同的 $\\mathbf{X}^{\\mathrm{true}}$、$R$、阈值、$\\beta$、$\\varepsilon$。预测坐标 $$\\mathbf{X}^{\\mathrm{pred}} = 2.0 \\cdot \\mathbf{X}^{\\mathrm{true}}.$$\n\n最终输出格式：\n您的程序应生成单行输出，其中包含所有测试案例的结果，形式为一个逗号分隔的列表的列表。每个内部列表对应一个测试案例，包含四个浮点数 $[\\overline{\\mathrm{lDDT}}^{\\mathrm{hard}}, \\overline{\\mathrm{lDDT}}^{\\mathrm{soft}}, \\mathcal{L}, \\left\\| \\nabla_{\\mathbf{X}^{\\mathrm{pred}}} \\mathcal{L} \\right\\|_2]$。例如，输出应如下所示：$$\\left[ [r_{11}, r_{12}, r_{13}, r_{14}], [r_{21}, r_{22}, r_{23}, r_{24}], [r_{31}, r_{32}, r_{33}, r_{34}], [r_{41}, r_{42}, r_{43}, r_{44}], [r_{51}, r_{52}, r_{53}, r_{54}] \\right],$$ 其中每个 $r_{ij}$ 都是一个十进制浮点数。",
            "solution": "该问题定义明确，具有科学依据，并为获得唯一解提供了所有必要信息。因此，该问题被认为是有效的。解决方案要求实现用于蛋白质结构评估的局部距离差异检验（lDDT-Cα）分数、其可微代理，以及从该代理派生的损失函数的梯度。\n\n解决方案主要分为四个步骤：\n1.  计算标准的、基于指示函数的硬lDDT分数。\n2.  使用平滑近似计算可微的软lDDT分数。\n3.  基于软lDDT计算标量损失 $\\mathcal{L}$。\n4.  推导并计算损失的梯度 $\\nabla_{\\mathbf{X}^{\\mathrm{pred}}} \\mathcal{L}$，该梯度是相对于预测坐标的。\n\n此问题的核心在于仔细应用向量微积分，特别是链式法则，以计算用于反向传播的梯度。所有计算都在提供的测试案例上执行。\n\n### 1. 准备工作：逐对距离和邻居集\n\n两种lDDT变体的基础是逐对欧几里得距离的集合。给定真实坐标 $\\mathbf{X}^{\\mathrm{true}} \\in \\mathbb{R}^{N \\times 3}$ 和预测坐标 $\\mathbf{X}^{\\mathrm{pred}} \\in \\mathbb{R}^{N \\times 3}$，我们计算两个距离矩阵 $D^{\\mathrm{true}}$ 和 $D^{\\mathrm{pred}}$，其中 $D_{ij} = \\| \\mathbf{x}_i - \\mathbf{x}_j \\|_2$。\n\n对于每个残基 $i$，邻居集 $\\mathcal{N}_i$ 是根据真实距离和一个固定半径 $R$ 确定的：\n$$\n\\mathcal{N}_i = \\{ j \\in \\{1,\\dots,N\\} \\setminus \\{i\\} \\mid d^{\\mathrm{true}}_{ij} \\le R \\}\n$$\n该集合的大小 $|\\mathcal{N}_i|$ 用于归一化。\n\n### 2. 硬lDDT分数 ($\\overline{\\mathrm{lDDT}}^{\\mathrm{hard}}$)\n\n残基 $i$ 的硬lDDT分数 $\\mathrm{lDDT}_i^{\\mathrm{hard}}$，是在特定误差阈值内得以保持的局部原子距离的比例。一对 $(i, j)$ 的距离误差是 $|d^{\\mathrm{pred}}_{ij} - d^{\\mathrm{true}}_{ij}|$。对于每个邻居 $j \\in \\mathcal{N}_i$，我们计算有多少个阈值 $t \\in \\mathcal{T} = \\{0.5, 1.0, 2.0, 4.0\\}$ 被满足，即误差小于 $t$。这由指示函数 $\\mathbf{1}(\\cdot)$ 正式地捕捉。\n\n公式为：\n$$\n\\mathrm{lDDT}_i^{\\mathrm{hard}} = \\begin{cases} \\dfrac{1}{|\\mathcal{N}_i|} \\sum\\limits_{j \\in \\mathcal{N}_i} \\left( \\dfrac{1}{|\\mathcal{T}|} \\sum\\limits_{t \\in \\mathcal{T}} \\mathbf{1}\\left( \\left| d^{\\mathrm{pred}}_{ij} - d^{\\mathrm{true}}_{ij} \\right|  t \\right) \\right),  |\\mathcal{N}_i| > 0 \\\\ 0,  |\\mathcal{N}_i| = 0 \\end{cases}\n$$\n平均硬lDDT分数是所有残基的平均值：$\\overline{\\mathrm{lDDT}}^{\\mathrm{hard}} = \\frac{1}{N} \\sum_{i=1}^N \\mathrm{lDDT}_i^{\\mathrm{hard}}$。\n\n### 3. 软lDDT分数 ($\\overline{\\mathrm{lDDT}}^{\\mathrm{soft}}$) 与损失 ($\\mathcal{L}$)\n\n为了可微性，非平滑的绝对值和指示函数被平滑代理所替代。距离误差为 $e_{ij} = d^{\\mathrm{pred}}_{ij} - d^{\\mathrm{true}}_{ij}$。\n- 绝对值 $|e_{ij}|$ 被替换为 $a_{ij} = \\sqrt{\\varepsilon + e_{ij}^2}$，其中 $\\varepsilon$ 是一个小的正常数，以防止在 $e_{ij}=0$ 处导数无定义。\n- 指示函数 $\\mathbf{1}(|e_{ij}|  t)$，等价于 $\\mathbf{1}(t - |e_{ij}| > 0)$，被logistic函数 $\\sigma(z) = (1 + e^{-z})^{-1}$ 替代。阈值 $t$ 的软满足分数为 $s_{ij}(t) = \\sigma\\left( \\frac{t - a_{ij}}{\\beta} \\right) = \\sigma\\left( -\\frac{a_{ij} - t}{\\beta} \\right)$，其中 $\\beta$ 控制过渡的“软度”。\n\n残基 $i$ 的软lDDT分数则为：\n$$\n\\mathrm{lDDT}_i^{\\mathrm{soft}} = \\begin{cases} \\dfrac{1}{|\\mathcal{N}_i|} \\sum\\limits_{j \\in \\mathcal{N}_i} \\left( \\dfrac{1}{|\\mathcal{T}|} \\sum\\limits_{t \\in \\mathcal{T}} s_{ij}(t) \\right),  |\\mathcal{N}_i| > 0 \\\\ 0,  |\\mathcal{N}_i| = 0 \\end{cases}\n$$\n平均软lDDT为 $\\overline{\\mathrm{lDDT}}^{\\mathrm{soft}} = \\frac{1}{N} \\sum_{i=1}^N \\mathrm{lDDT}_i^{\\mathrm{soft}}$。标量损失定义为 $\\mathcal{L} = 1 - \\overline{\\mathrm{lDDT}}^{\\mathrm{soft}}$。\n\n### 4. 梯度计算 ($\\nabla_{\\mathbf{X}^{\\mathrm{pred}}} \\mathcal{L}$)\n\n为了计算损失 $\\mathcal{L}$ 相对于预测坐标 $\\mathbf{X}^{\\mathrm{pred}}$ 的梯度，我们系统地应用链式法则。单个坐标向量 $\\mathbf{x}_k^{\\mathrm{pred}}$ 的梯度为 $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}_k^{\\mathrm{pred}}}$。\n\n损失 $\\mathcal{L}$ 依赖于每个 $\\mathrm{lDDT}_i^{\\mathrm{soft}}$，而后者又依赖于距离 $d^{\\mathrm{pred}}_{ij}$。每个距离 $d^{\\mathrm{pred}}_{ij}$ 依赖于坐标 $\\mathbf{x}_i^{\\mathrm{pred}}$ 和 $\\mathbf{x}_j^{\\mathrm{pred}}$。最直接的方法是找到损失相对于每个逐对距离 $d^{\\mathrm{pred}}_{ij}$ 的导数，然后将此导数传播回坐标。\n\n$\\mathcal{L}$ 相对于单个距离 $d^{\\mathrm{pred}}_{ij}$ （$i \\neq j$）的导数，从 $\\mathrm{lDDT}_i^{\\mathrm{soft}}$ （其中 $d^{\\mathrm{pred}}_{ij}$ 出现在对邻居的求和中）和 $\\mathrm{lDDT}_j^{\\mathrm{soft}}$ （其中 $d^{\\mathrm{pred}}_{ji} = d^{\\mathrm{pred}}_{ij}$ 出现）接收贡献。这仅在 $j \\in \\mathcal{N}_i$ （这意味着 $i \\in \\mathcal{N}_j$）时才非零。\n\n让我们追踪导数：\n1.  $\\dfrac{\\partial \\mathcal{L}}{\\partial \\mathrm{lDDT}_i^{\\mathrm{soft}}} = -\\dfrac{1}{N}$\n2.  $\\dfrac{\\partial \\mathrm{lDDT}_i^{\\mathrm{soft}}}{\\partial s_{ij}(t)} = \\dfrac{1}{|\\mathcal{N}_i| |\\mathcal{T}|}$ （如果 $|\\mathcal{N}_i|0$，否则为 $0$）\n3.  $\\dfrac{\\partial s_{ij}(t)}{\\partial a_{ij}} = \\sigma'\\!\\left(-\\dfrac{a_{ij}-t}{\\beta}\\right) \\cdot \\left(-\\dfrac{1}{\\beta}\\right)$，其中 $\\sigma'(z) = \\sigma(z)(1-\\sigma(z))$。\n4.  $\\dfrac{\\partial a_{ij}}{\\partial e_{ij}} = \\dfrac{e_{ij}}{\\sqrt{\\varepsilon + e_{ij}^2}} = \\dfrac{e_{ij}}{a_{ij}}$\n5.  $\\dfrac{\\partial e_{ij}}{\\partial d^{\\mathrm{pred}}_{ij}} = 1$\n\n结合这些，$\\mathcal{L}$ 关于 $d^{\\mathrm{pred}}_{ij}$ 的导数为：\n$$\n\\dfrac{\\partial \\mathcal{L}}{\\partial d^{\\mathrm{pred}}_{ij}} = \\dfrac{\\partial \\mathcal{L}}{\\partial \\mathrm{lDDT}_i^{\\mathrm{soft}}} \\dfrac{\\partial \\mathrm{lDDT}_i^{\\mathrm{soft}}}{\\partial d^{\\mathrm{pred}}_{ij}} + \\dfrac{\\partial \\mathcal{L}}{\\partial \\mathrm{lDDT}_j^{\\mathrm{soft}}} \\dfrac{\\partial \\mathrm{lDDT}_j^{\\mathrm{soft}}}{\\partial d^{\\mathrm{pred}}_{ij}}\n$$\n其中\n$$\n\\dfrac{\\partial \\mathrm{lDDT}_i^{\\mathrm{soft}}}{\\partial d^{\\mathrm{pred}}_{ij}} = \\dfrac{1}{|\\mathcal{N}_i||\\mathcal{T}|} \\sum_{t \\in \\mathcal{T}} \\dfrac{\\partial s_{ij}(t)}{\\partial d^{\\mathrm{pred}}_{ij}} = \\dfrac{1}{|\\mathcal{N}_i||\\mathcal{T}|} \\sum_{t \\in \\mathcal{T}} \\left( \\sigma'\\!\\left(-\\tfrac{a_{ij}-t}{\\beta}\\right) \\left(-\\tfrac{1}{\\beta}\\right) \\tfrac{e_{ij}}{a_{ij}} \\right)\n$$\n令 $S'_{ij} = \\sum_{t \\in \\mathcal{T}} \\sigma'\\!\\left(-\\frac{a_{ij}-t}{\\beta}\\right)$。\n$$\n\\dfrac{\\partial \\mathcal{L}}{\\partial d^{\\mathrm{pred}}_{ij}} = \\left(-\\dfrac{1}{N}\\right) \\left[ \\left(-\\dfrac{e_{ij}}{a_{ij} |\\mathcal{N}_i| |\\mathcal{T}| \\beta} S'_{ij}\\right) + \\left(-\\dfrac{e_{ji}}{a_{ji} |\\mathcal{N}_j| |\\mathcal{T}| \\beta} S'_{ji}\\right) \\right]\n$$\n由于 $e_{ij}=e_{ji}$，$a_{ij}=a_{ji}$，且 $S'_{ij}=S'_{ji}$，这可以简化为：\n$$\n\\dfrac{\\partial \\mathcal{L}}{\\partial d^{\\mathrm{pred}}_{ij}} = \\dfrac{e_{ij} S'_{ij}}{N a_{ij} |\\mathcal{T}| \\beta} \\left( \\dfrac{1}{|\\mathcal{N}_i|} + \\dfrac{1}{|\\mathcal{N}_j|} \\right)\n$$\n对于每对 $j \\in \\mathcal{N}_i$ 的 $(i, j)$，计算这个标量导数。如果邻居集为空，则包含 $1/|\\mathcal{N}|$ 的项设为零。\n\n最后，我们使用 $\\frac{\\partial d^{\\mathrm{pred}}_{ij}}{\\partial \\mathbf{x}_i^{\\mathrm{pred}}} = \\frac{\\mathbf{x}_i^{\\mathrm{pred}} - \\mathbf{x}_j^{\\mathrm{pred}}}{d^{\\mathrm{pred}}_{ij}}$ （如果 $d^{\\mathrm{pred}}_{ij}0$，否则为 $\\mathbf{0}$）将其传播到坐标。\n坐标 $\\mathbf{x}_k^{\\mathrm{pred}}$ 的梯度是所有涉及它的距离的贡献的累加：\n$$\n\\dfrac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}_k^{\\mathrm{pred}}} = \\sum_{j \\neq k, j \\in \\mathcal{N}_k} \\dfrac{\\partial \\mathcal{L}}{\\partial d^{\\mathrm{pred}}_{kj}} \\dfrac{\\partial d^{\\mathrm{pred}}_{kj}}{\\partial \\mathbf{x}_k^{\\mathrm{pred}}} = \\sum_{j \\in \\mathcal{N}_k} \\dfrac{\\partial \\mathcal{L}}{\\partial d^{\\mathrm{pred}}_{kj}} \\left( \\dfrac{\\mathbf{x}_k^{\\mathrm{pred}} - \\mathbf{x}_j^{\\mathrm{pred}}}{d^{\\mathrm{pred}}_{kj}} \\right)\n$$\n通过为每个 $k \\in \\{1, \\dots, N\\}$ 计算这个向量，可以组装出完整的梯度矩阵 $\\nabla_{\\mathbf{X}^{\\mathrm{pred}}} \\mathcal{L} \\in \\mathbb{R}^{N \\times 3}$。最终报告的值是其欧几里得（Frobenius）范数：$\\|\\nabla_{\\mathbf{X}^{\\mathrm{pred}}} \\mathcal{L}\\|_2$。\n\n这个完整的分析框架允许计算给定测试案例的所有必需量。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the lDDT problem for a suite of test cases.\n    \"\"\"\n    \n    # Test case definitions\n    X_true_base = np.array([\n        [0.0, 0.0, 0.0],\n        [3.8, 0.0, 0.0],\n        [7.6, 0.0, 0.0],\n        [11.4, 0.0, 0.0],\n        [15.2, 0.0, 0.0]\n    ])\n\n    delta = np.array([\n        [0.1, -0.2, 0.05],\n        [-0.05, 0.15, -0.1],\n        [0.2, 0.0, 0.2],\n        [-0.1, -0.15, 0.0],\n        [0.05, 0.05, -0.2]\n    ])\n    \n    t_vec = np.array([10.0, -5.0, 3.0])\n\n    test_cases = [\n        # Case 1: Perfect match\n        (X_true_base, X_true_base.copy(), 10.0, {0.5, 1.0, 2.0, 4.0}, 0.5, 1e-6),\n        # Case 2: Small perturbation\n        (X_true_base, X_true_base + delta, 10.0, {0.5, 1.0, 2.0, 4.0}, 0.5, 1e-6),\n        # Case 3: Translation invariance\n        (X_true_base, X_true_base + t_vec, 10.0, {0.5, 1.0, 2.0, 4.0}, 0.5, 1e-6),\n        # Case 4: No neighbors edge case\n        (X_true_base, X_true_base.copy(), 2.0, {0.5, 1.0, 2.0, 4.0}, 0.5, 1e-6),\n        # Case 5: Global scaling distortion\n        (X_true_base, 2.0 * X_true_base, 10.0, {0.5, 1.0, 2.0, 4.0}, 0.5, 1e-6)\n    ]\n\n    results = []\n    for params in test_cases:\n        case_result = _calculate_all_metrics(*params)\n        results.append(case_result)\n\n    # Format output exactly as specified\n    inner_list_strs = [f\"[{','.join(map(str, np.round(res, 8)))}]\" for res in results]\n    final_str = f\"[{','.join(inner_list_strs)}]\"\n    print(final_str.replace(\" \", \"\"))\n\ndef _calculate_all_metrics(X_true, X_pred, R, T_set, beta, eps):\n    \"\"\"\n    Computes all required metrics for a single test case.\n    \"\"\"\n    N = X_true.shape[0]\n    T = np.array(sorted(list(T_set)))\n    num_T = len(T)\n\n    # Pairwise distances\n    diff_true = X_true[:, np.newaxis, :] - X_true[np.newaxis, :, :]\n    d_true = np.sqrt(np.sum(diff_true**2, axis=-1))\n    \n    diff_pred = X_pred[:, np.newaxis, :] - X_pred[np.newaxis, :, :]\n    d_pred = np.sqrt(np.sum(diff_pred**2, axis=-1))\n\n    # Neighbor sets\n    neighbor_mask = (d_true = R)  (d_true > 0)\n    N_i_sizes = np.sum(neighbor_mask, axis=1)\n\n    # 1. Hard lDDT\n    dist_err = np.abs(d_pred - d_true)\n    lddt_hard_per_res = np.zeros(N)\n    for i in range(N):\n        if N_i_sizes[i] > 0:\n            neighbors_j = np.where(neighbor_mask[i])[0]\n            score_sum = 0\n            for j in neighbors_j:\n                satisfied_count = np.sum(dist_err[i, j]  T)\n                score_sum += satisfied_count / num_T\n            lddt_hard_per_res[i] = score_sum / N_i_sizes[i]\n    mean_lddt_hard = np.mean(lddt_hard_per_res)\n\n    # 2. Soft lDDT\n    e_ij = d_pred - d_true\n    a_ij = np.sqrt(eps + e_ij**2)\n    \n    def sigma(z):\n        return 1.0 / (1.0 + np.exp(-z))\n\n    lddt_soft_per_res = np.zeros(N)\n    s_ij_t_sum = np.zeros((N, N))\n    for t in T:\n        s_ij_t = sigma(-(a_ij - t) / beta)\n        s_ij_t_sum += s_ij_t\n\n    for i in range(N):\n        if N_i_sizes[i] > 0:\n            neighbors_j = np.where(neighbor_mask[i])[0]\n            per_neighbor_scores = (1/num_T) * s_ij_t_sum[i, neighbors_j]\n            lddt_soft_per_res[i] = np.sum(per_neighbor_scores) / N_i_sizes[i]\n\n    mean_lddt_soft = np.mean(lddt_soft_per_res)\n\n    # 3. Loss\n    loss = 1.0 - mean_lddt_soft\n\n    # 4. Gradient\n    grad = np.zeros_like(X_pred)\n\n    def sigma_prime(z):\n        s = sigma(z)\n        return s * (1.0 - s)\n        \n    # Sum of sigma' over thresholds\n    S_prime_ij = np.zeros((N, N))\n    for t in T:\n        z = -(a_ij - t) / beta\n        S_prime_ij += sigma_prime(z)\n        \n    # Pre-calculate factors to avoid recomputing in loop\n    inv_Ni_sizes = np.zeros(N)\n    inv_Ni_sizes[N_i_sizes > 0] = 1.0 / N_i_sizes[N_i_sizes > 0]\n\n    # Iterate over unique pairs (i, j)\n    for i in range(N):\n        for j in range(i + 1, N):\n            if neighbor_mask[i, j]:\n                # Scalar derivative part\n                # dL/dd_pred_ij\n                if a_ij[i, j] > 1e-9: # Avoid division by zero\n                    common_factor = (e_ij[i, j] * S_prime_ij[i, j]) / (N * a_ij[i, j] * num_T * beta)\n                else:\n                    common_factor = 0.0\n\n                dL_dd = common_factor * (inv_Ni_sizes[i] + inv_Ni_sizes[j])\n                \n                # Vector part\n                # dd_pred_ij/dx_i\n                if d_pred[i, j] > 1e-9: # Avoid division by zero\n                    unit_vec = diff_pred[i, j, :] / d_pred[i, j]\n                else:\n                    unit_vec = np.zeros(3)\n\n                # Accumulate gradients\n                grad[i, :] += dL_dd * unit_vec\n                grad[j, :] -= dL_dd * unit_vec\n    \n    grad_norm = np.linalg.norm(grad)\n\n    return [mean_lddt_hard, mean_lddt_soft, loss, grad_norm]\n\nif __name__ == '__main__':\n    solve()\n\n```"
        },
        {
            "introduction": "一个强大的预测模型不仅应提供准确的预测，还应能评估自身预测的置信度。当真实结构未知时，这种自我评估能力至关重要。AlphaFold2引入的pLDDT分数就是一个典范，它为每个残基的预测精度提供了可靠的估计。本练习将模拟从模型的原始误差预测中计算类似pLDDT的置信度分数的过程。更进一步，您将学习使用期望校准误差（Expected Calibration Error, ECE）来检验这些置信度分数是否“校准良好”，即模型的自信程度是否与其真实表现相符。",
            "id": "4554904",
            "problem": "给定来自蛋白质结构预测深度学习模型的逐残基预测误差分布。每个残基的预测是一个离散概率质量函数，通过固定的区间中点表示绝对距离误差的大小。您将计算一个类似于预测局部距离差异检验 (Predicted Local Distance Difference Test, pLDDT) 的逐残基置信度分数，然后通过预期校准误差 (Expected Calibration Error, ECE) 来评估其校准性。\n\n定义与基本原理：\n- 一个残基的局部距离差异检验 (Local Distance Difference Test, LDDT) 分数定义为在指定的容差阈值内的局部成对距离的比例。我们将使用此比例的概率期望作为置信度的代理指标。设容差阈值集合为 $\\{\\tau_k\\}_{k=1}^K$（单位为埃，记作 Å）。\n- 对于一个给定的残基，令随机变量 $E$ 表示绝对距离误差的大小（单位为 Å）。对于每个阈值 $\\tau_k$，如果误差低于该阈值，则指示函数 $\\mathbf{1}(E  \\tau_k)$ 等于 $1$，否则等于 $0$。\n- 逐残基的类 pLDDT 置信度定义为满足的阈值比例的期望值：$C = 100 \\times \\frac{1}{K} \\sum_{k=1}^{K} \\mathbb{E}\\left[\\mathbf{1}(E  \\tau_k)\\right]$。这将产生一个在 $[0,100]$ 区间内的值。\n- 校准性通过比较预测置信度与经验准确度来评估。令 $c \\in [0,1]$ 为归一化置信度 $C/100$，令 $t \\in [0,1]$ 为根据真实误差 $e$ 计算出的经验归一化 LDDT 分数，$t = \\frac{1}{K} \\sum_{k=1}^{K} \\mathbf{1}(e  \\tau_k)$。\n- 使用 $M$ 个分箱的预期校准误差 (ECE) 将区间 $[0,1]$ 划分为 $M$ 个不相交的区间。对于分箱 $m$，令 $B_m$ 为其预测置信度 $c$ 落入该分箱的残基集合。令 $\\bar{c}_m$ 为 $B_m$ 上 $c$ 的均值，$\\bar{t}_m$ 为 $B_m$ 上 $t$ 的均值。设总残基数为 $N$，则 ECE 为 $$\\mathrm{ECE} = \\sum_{m=1}^{M} \\frac{|B_m|}{N} \\left| \\bar{c}_m - \\bar{t}_m \\right|.$$ 如果 $B_m$ 为空，则其贡献为零。\n\n离散误差分布的近似方法：\n- 每个残基的预测分布由区间中点 $\\{m_i\\}_{i=1}^{I}$（单位为 Å）和概率 $\\{p_i\\}_{i=1}^{I}$ 指定，其中 $\\sum_{i=1}^{I} p_i = 1$。我们通过对中点低于阈值的区间的概率求和来近似 $P(E  \\tau_k)$：$$P(E  \\tau_k) \\approx \\sum_{i=1}^{I} p_i \\, \\mathbf{1}(m_i  \\tau_k).$$\n\n您的任务：\n- 对于每个测试用例，使用上述定义和离散近似方法计算每个残基的 $C$ 值和归一化值 $c = C/100$。然后，使用真实误差 $e$ 和给定的阈值计算每个残基的 $t$ 值。最后，在 $[0,1]$ 区间上使用 $M$ 个等宽分箱计算 ECE。\n\n单位与输出：\n- 所有距离值和阈值都必须以埃（Å）为单位处理。\n- 每个测试用例的最终输出是一个浮点数，等于 ECE 的值，四舍五入到六位小数（十进制格式，不带百分号）。\n- 您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表（例如：`[result1,result2,result3]`）。\n\n测试套件：\n使用以下参数集。在每种情况下，$K=4$ 个阈值 $\\tau = [0.5, 1.0, 2.0, 4.0]$ Å，$I=6$ 个区间中点 $m = [0.25, 0.75, 1.5, 3.0, 6.0, 10.0]$ Å，以及 $M=10$ 个校准分箱。\n\n- 情况1（通用，接近校准）：\n    - 残基数：$5$\n    - 每个残基的预测概率（每个列表总和为 $1$）：\n        - 残基 $1$：$[0.35, 0.25, 0.20, 0.10, 0.06, 0.04]$\n        - 残基 $2$：$[0.15, 0.20, 0.25, 0.20, 0.10, 0.10]$\n        - 残基 $3$：$[0.05, 0.10, 0.15, 0.25, 0.25, 0.20]$\n        - 残基 $4$：$[0.40, 0.30, 0.15, 0.10, 0.04, 0.01]$\n        - 残基 $5$：$[0.10, 0.15, 0.25, 0.25, 0.15, 0.10]$\n    - 真实误差（Å）：$[0.6, 1.2, 3.5, 0.4, 2.5]$\n\n- 情况2（过度自信，校准不良）：\n    - 残基数：$4$\n    - 每个残基的预测概率（所有残基相同）：$[0.60, 0.20, 0.10, 0.05, 0.03, 0.02]$\n    - 真实误差（Å）：$[5.5, 6.5, 8.0, 7.0]$\n\n- 情况3（均匀预测，混合真实误差）：\n    - 残基数：$6$\n    - 每个残基的预测概率（所有残基相同）：$[1/6, 1/6, 1/6, 1/6, 1/6, 1/6]$\n    - 真实误差（Å）：$[0.1, 0.8, 1.9, 2.1, 3.9, 10.0]$\n\n最终输出规范：\n- 您的程序应计算这三种情况中每一种的 ECE，并打印一行带有方括号的逗号分隔列表。每个值必须四舍五入到六位小数，并以十进制表示（例如，$0.123456$）。输出必须具有确切的格式：$[x_1,x_2,x_3]$，其中 $x_j$ 是情况 $j$ 的 ECE。",
            "solution": "问题陈述已被验证为科学上合理、表述清晰且完整。它基于机器学习模型评估中，特别是在蛋白质结构预测领域的既定原则，提出了一个明确、可形式化的任务。解决方案的方法论直接源自所提供的定义。计算涉及三个主要阶段：确定逐残基的预测置信度，计算逐残基的经验准确度，以及汇总这些值以计算预期校准误差 (ECE)。\n\n目标是计算模型校准的统计度量，即 ECE，这需要比较模型的预测置信度分数与观测到的正确率。该问题基于局部距离差异检验 (LDDT) 框架，为置信度和正确性提供了代理指标。\n\n步骤1：计算逐残基预测置信度 ($c_j$)\n对于给定的残基 $j$，预测的类 pLDDT 置信度分数 $C_j$ 被定义为满足距离容差阈值的比例的缩放期望值。这组 $K$ 个阈值表示为 $\\{\\tau_k\\}_{k=1}^K$。置信度由以下公式给出：\n$$\nC_j = 100 \\times \\frac{1}{K} \\sum_{k=1}^{K} \\mathbb{E}\\left[\\mathbf{1}(E_j  \\tau_k)\\right]\n$$\n其中 $E_j$ 是残基 $j$ 的绝对误差的随机变量。指示函数的期望值 $\\mathbb{E}\\left[\\mathbf{1}(E_j  \\tau_k)\\right]$ 等同于概率 $P(E_j  \\tau_k)$。问题指定使用离散误差分布，由 $I$ 个区间中点 $\\{m_i\\}_{i=1}^I$ 及其对每个残基 $j$ 的相关概率 $\\{p_{j,i}\\}_{i=1}^I$ 定义。概率 $P(E_j  \\tau_k)$ 通过对所有区间中点严格小于阈值 $\\tau_k$ 的区间的概率求和来近似：\n$$\nP(E_j  \\tau_k) \\approx \\sum_{i=1}^{I} p_{j,i} \\, \\mathbf{1}(m_i  \\tau_k)\n$$\nECE 计算所需的归一化置信度 $c_j \\in [0, 1]$ 由 $c_j = C_j / 100$ 给出。因此，对于每个残基 $j$，我们计算：\n$$\nc_j = \\frac{1}{K} \\sum_{k=1}^{K} \\left( \\sum_{i=1}^{I} p_{j,i} \\, \\mathbf{1}(m_i  \\tau_k) \\right)\n$$\n\n步骤2：计算逐残基经验准确度 ($t_j$)\n残基 $j$ 的经验准确度（表示为 $t_j$）用作与预测置信度 $c_j$ 进行比较的基准真相。它被定义为使用已知的真实误差 $e_j$ 计算出的实际满足阈值的比例。\n$$\nt_j = \\frac{1}{K} \\sum_{k=1}^{K} \\mathbf{1}(e_j  \\tau_k)\n$$\n该计算是确定性的：对于每个残基 $j$，我们计算大于其真实误差 $e_j$ 的阈值 $\\tau_k$ 的数量，并将此计数除以总阈值数 $K$。\n\n步骤3：计算预期校准误差 (ECE)\nECE 量化了预测置信度与经验准确度之间的差异。该过程涉及将置信度范围 $[0, 1]$ 划分为 $M$ 个不相交的等宽分箱。对于此问题，$M=10$，因此分箱为 $[0, 0.1), [0.1, 0.2), \\dots, [0.9, 1.0]$。\n$1$. $N$ 个残基中的每一个都根据其预测置信度值 $c_j$ 被分配到一个分箱 $B_m$ 中。\n$2$. 对于每个非空分箱 $B_m$，我们计算该分箱内残基的平均置信度 $\\bar{c}_m$ 和平均经验准确度 $\\bar{t}_m$。\n   $$\n   \\bar{c}_m = \\frac{1}{|B_m|} \\sum_{j \\in B_m} c_j\n   $$\n   $$\n   \\bar{t}_m = \\frac{1}{|B_m|} \\sum_{j \\in B_m} t_j\n   $$\n$3$. ECE 是所有分箱中平均置信度和平均准确度之间绝对差的加权平均值。每个分箱的权重是其相对大小 $|B_m|/N$。\n   $$\n   \\mathrm{ECE} = \\sum_{m=1}^{M} \\frac{|B_m|}{N} \\left| \\bar{c}_m - \\bar{t}_m \\right|\n   $$\n如果分箱 $B_m$ 为空，其对总和的贡献为 $0$。\n\n此程序系统地应用于三个测试用例中的每一个，使用指定的参数：$K=4$ 个阈值 $\\tau = [0.5, 1.0, 2.0, 4.0]$ Å，$I=6$ 个区间中点 $m = [0.25, 0.75, 1.5, 3.0, 6.0, 10.0]$ Å，以及 $M=10$ 个校准分箱。每个用例的最终结果是计算出的 ECE，四舍五入到六位小数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_ece(probs, true_errors, thresholds, bin_midpoints, num_ece_bins):\n    \"\"\"\n    Computes the Expected Calibration Error (ECE) for a set of residues.\n\n    Args:\n        probs (list of list of float): Predicted probabilities for each residue.\n        true_errors (list of float): True errors for each residue.\n        thresholds (list of float): Tolerance thresholds for LDDT score.\n        bin_midpoints (list of float): Midpoints of the error distribution bins.\n        num_ece_bins (int): Number of bins for ECE calculation.\n\n    Returns:\n        float: The computed Expected Calibration Error.\n    \"\"\"\n    num_residues = len(true_errors)\n    num_thresholds = len(thresholds)\n\n    # Convert to numpy arrays for efficient computation\n    np_probs = np.array(probs)\n    np_true_errors = np.array(true_errors)\n    np_thresholds = np.array(thresholds)\n    np_bin_midpoints = np.array(bin_midpoints)\n\n    predicted_confidences = []\n    empirical_accuracies = []\n\n    # Step 1  2: Calculate predicted confidence (c) and empirical accuracy (t) for each residue\n    for j in range(num_residues):\n        p_j = np_probs[j]\n        e_j = np_true_errors[j]\n        \n        # Calculate P(E  tau_k) for each threshold to get expected indicators\n        expected_indicators = []\n        for tau_k in np_thresholds:\n            # Sum probabilities of bins where midpoint is less than the threshold\n            mask = np_bin_midpoints  tau_k\n            prob_less_than_tau = np.sum(p_j[mask])\n            expected_indicators.append(prob_less_than_tau)\n        \n        # Calculate normalized confidence c_j (mean of expected indicators)\n        c_j = np.mean(expected_indicators)\n        predicted_confidences.append(c_j)\n        \n        # Calculate empirical accuracy t_j\n        satisfied_thresholds = np.sum(e_j  np_thresholds)\n        t_j = satisfied_thresholds / num_thresholds\n        empirical_accuracies.append(t_j)\n    \n    np_confidences = np.array(predicted_confidences)\n    np_accuracies = np.array(empirical_accuracies)\n\n    # Step 3: Compute ECE\n    ece = 0.0\n    \n    # Assign each residue to an ECE bin based on its confidence.\n    # Bins are [0, 0.1), [0.1, 0.2), ..., [0.9, 1.0].\n    # A confidence of 1.0 should go into the last bin.\n    bin_indices = np.floor(np_confidences * num_ece_bins)\n    bin_indices = np.clip(bin_indices, 0, num_ece_bins - 1).astype(int)\n\n    for m in range(num_ece_bins):\n        # Find residues in the current bin\n        in_bin_mask = (bin_indices == m)\n        num_in_bin = np.sum(in_bin_mask)\n\n        if num_in_bin > 0:\n            # Get confidences and accuracies for residues in this bin\n            bin_confidences = np_confidences[in_bin_mask]\n            bin_accuracies = np_accuracies[in_bin_mask]\n            \n            # Calculate average confidence and average accuracy in the bin\n            avg_confidence_in_bin = np.mean(bin_confidences)\n            avg_accuracy_in_bin = np.mean(bin_accuracies)\n            \n            # Add weighted difference to ECE\n            weight = num_in_bin / num_residues\n            ece += weight * np.abs(avg_confidence_in_bin - avg_accuracy_in_bin)\n            \n    return ece\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Common parameters\n    thresholds = [0.5, 1.0, 2.0, 4.0]\n    bin_midpoints = [0.25, 0.75, 1.5, 3.0, 6.0, 10.0]\n    num_ece_bins = 10\n\n    test_cases = [\n        # Case 1\n        {\n            \"probs\": [\n                [0.35, 0.25, 0.20, 0.10, 0.06, 0.04],\n                [0.15, 0.20, 0.25, 0.20, 0.10, 0.10],\n                [0.05, 0.10, 0.15, 0.25, 0.25, 0.20],\n                [0.40, 0.30, 0.15, 0.10, 0.04, 0.01],\n                [0.10, 0.15, 0.25, 0.25, 0.15, 0.10]\n            ],\n            \"true_errors\": [0.6, 1.2, 3.5, 0.4, 2.5]\n        },\n        # Case 2\n        {\n            \"probs\": [[0.60, 0.20, 0.10, 0.05, 0.03, 0.02]] * 4,\n            \"true_errors\": [5.5, 6.5, 8.0, 7.0]\n        },\n        # Case 3\n        {\n            \"probs\": [[1/6, 1/6, 1/6, 1/6, 1/6, 1/6]] * 6,\n            \"true_errors\": [0.1, 0.8, 1.9, 2.1, 3.9, 10.0]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Main logic to calculate the result for one case goes here.\n        ece_result = calculate_ece(\n            probs=case[\"probs\"],\n            true_errors=case[\"true_errors\"],\n            thresholds=thresholds,\n            bin_midpoints=bin_midpoints,\n            num_ece_bins=num_ece_bins\n        )\n        results.append(ece_result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}