{
    "hands_on_practices": [
        {
            "introduction": "Before delving into complex simulations, it is essential to master the foundational calculation at the heart of MM/PBSA and MM/GBSA methods. This exercise provides a direct, hands-on opportunity to compute the binding free energy, $\\Delta G_{\\text{bind}}$, from its core components: the change in molecular mechanics energy ($\\Delta E_{\\text{MM}}$) and the change in solvation free energy ($\\Delta G_{\\text{solv}}$). By working through this fundamental example, you will solidify your understanding of how these terms contribute to the final affinity estimate and explore the implicit assumptions about the thermodynamic reference state made in this common approximation .",
            "id": "3843912",
            "problem": "A protein receptor $R$ and a ligand $L$ associate to form a complex $RL$ in aqueous solution at temperature $T$. The binding process is the chemical reaction $R + L \\rightarrow RL$. The Gibbs free energy change of binding, $\\Delta G_{\\text{bind}}$, is defined as the free energy of products minus reactants for this reaction at the same $T$ and pressure. In end-point methods such as Molecular Mechanics/Poisson–Boltzmann Surface Area (MM/PBSA) and Molecular Mechanics/Generalized Born Surface Area (MM/GBSA), the free energy of a state is approximated as a sum of a Molecular Mechanics (MM) energy and a solvation free energy computed with an implicit solvent model; conformational entropy can be treated separately or neglected depending on the level of approximation.\n\nYou are given trajectory-averaged component values for the three end states produced by a consistent single-trajectory protocol using an implicit solvent model. For each state $X \\in \\{RL, R, L\\}$, the Molecular Mechanics energy is $E_{\\text{MM}}(X)$ and the solvation free energy is $G_{\\text{solv}}(X)$. The data (in $\\mathrm{kJ}\\,\\mathrm{mol}^{-1}$) are:\n- Complex $RL$: $E_{\\text{MM}}(RL) = -214.36$, $G_{\\text{solv}}(RL) = -123.48$.\n- Receptor $R$: $E_{\\text{MM}}(R) = -102.12$, $G_{\\text{solv}}(R) = -88.73$.\n- Ligand $L$: $E_{\\text{MM}}(L) = -47.46$, $G_{\\text{solv}}(L) = -20.41$.\n\nAssume conformational entropic contributions are neglected and no explicit standard-state correction is applied. Using only the foundational definition of a reaction free energy and the stated decomposition of state free energies, derive the appropriate expression for the binding free energy in this approximation and compute its numerical value from the data above. Briefly explain the implicit reference state that this end-point calculation adopts.\n\nExpress the final energy in $\\mathrm{kJ}\\,\\mathrm{mol}^{-1}$ and round your answer to four significant figures.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and provides all necessary information for a solution based on the standard principles of the MM/PBSA and MM/GBSA methods in computational chemistry.\n\nThe Gibbs free energy of binding, $\\Delta G_{\\text{bind}}$, for the reaction $R + L \\rightarrow RL$ is defined as the difference between the Gibbs free energy of the products and the reactants:\n$$\n\\Delta G_{\\text{bind}} = G(RL) - (G(R) + G(L))\n$$\nThe problem states that the free energy of any given state $X$ (where $X$ can be the complex $RL$, the receptor $R$, or the ligand $L$) is approximated within the end-point method formalism as the sum of its molecular mechanics (MM) energy, $E_{\\text{MM}}(X)$, and its solvation free energy, $G_{\\text{solv}}(X)$. The contribution from conformational entropy is explicitly neglected. Thus, we have:\n$$\nG(X) \\approx E_{\\text{MM}}(X) + G_{\\text{solv}}(X)\n$$\nSubstituting this approximation into the definition of $\\Delta G_{\\text{bind}}$ for each species, we obtain the expression for the binding free energy in this framework:\n$$\n\\Delta G_{\\text{bind}} \\approx \\left[ E_{\\text{MM}}(RL) + G_{\\text{solv}}(RL) \\right] - \\left[ (E_{\\text{MM}}(R) + G_{\\text{solv}}(R)) + (E_{\\text{MM}}(L) + G_{\\text{solv}}(L)) \\right]\n$$\nThis expression can be rearranged by grouping the molecular mechanics terms and the solvation free energy terms:\n$$\n\\Delta G_{\\text{bind}} \\approx \\left[ E_{\\text{MM}}(RL) - E_{\\text{MM}}(R) - E_{\\text{MM}}(L) \\right] + \\left[ G_{\\text{solv}}(RL) - G_{\\text{solv}}(R) - G_{\\text{solv}}(L) \\right]\n$$\nThis is often written in a more compact form, $\\Delta G_{\\text{bind}} \\approx \\Delta E_{\\text{MM}} + \\Delta G_{\\text{solv}}$, where $\\Delta E_{\\text{MM}}$ is the change in molecular mechanics energy upon binding, and $\\Delta G_{\\text{solv}}$ is the change in solvation free energy. This is the required expression.\n\nNow, we compute the numerical value using the provided data (all in $\\mathrm{kJ}\\,\\mathrm{mol}^{-1}$):\n$E_{\\text{MM}}(RL) = -214.36$\n$G_{\\text{solv}}(RL) = -123.48$\n$E_{\\text{MM}}(R) = -102.12$\n$G_{\\text{solv}}(R) = -88.73$\n$E_{\\text{MM}}(L) = -47.46$\n$G_{\\text{solv}}(L) = -20.41$\n\nFirst, we calculate the change in molecular mechanics energy, $\\Delta E_{\\text{MM}}$:\n$$\n\\Delta E_{\\text{MM}} = E_{\\text{MM}}(RL) - E_{\\text{MM}}(R) - E_{\\text{MM}}(L) = (-214.36) - (-102.12) - (-47.46)\n$$\n$$\n\\Delta E_{\\text{MM}} = -214.36 + 102.12 + 47.46 = -214.36 + 149.58 = -64.78 \\, \\mathrm{kJ}\\,\\mathrm{mol}^{-1}\n$$\nNext, we calculate the change in solvation free energy, $\\Delta G_{\\text{solv}}$:\n$$\n\\Delta G_{\\text{solv}} = G_{\\text{solv}}(RL) - G_{\\text{solv}}(R) - G_{\\text{solv}}(L) = (-123.48) - (-88.73) - (-20.41)\n$$\n$$\n\\Delta G_{\\text{solv}} = -123.48 + 88.73 + 20.41 = -123.48 + 109.14 = -14.34 \\, \\mathrm{kJ}\\,\\mathrm{mol}^{-1}\n$$\nThe total binding free energy in this approximation is the sum of these two components:\n$$\n\\Delta G_{\\text{bind}} \\approx \\Delta E_{\\text{MM}} + \\Delta G_{\\text{solv}} = -64.78 + (-14.34) = -79.12 \\, \\mathrm{kJ}\\,\\mathrm{mol}^{-1}\n$$\nThe problem requires the answer to be rounded to four significant figures. The calculated value of $-79.12$ already contains four significant figures.\n\nRegarding the implicit reference state, the standard Gibbs free energy of binding, $\\Delta G_{\\text{bind}}^{\\circ}$, is related to the binding constant $K_a$ (or dissociation constant $K_d$) and is defined for a standard concentration of $1\\,\\mathrm{M}$ for all species in solution. The full expression for $\\Delta G_{\\text{bind}}^{\\circ}$ includes an entropic term, $-T\\Delta S$, which accounts for the change in translational, rotational, and conformational entropy upon binding. By explicitly neglecting entropic contributions, particularly the significant loss of translational and rotational freedom when two molecules combine to form one, the calculation omits the correction term needed to adjust for the standard state volume. Therefore, the computed value does not correspond to the standard $1\\,\\mathrm{M}$ reference state. Instead, it represents the free energy change for a process where one particle of $R$ and one particle of $L$, infinitely separated in the solvent, come together to form one particle of $RL$. This is a hypothetical reference state corresponding to unit mole fraction, not unit molarity. It describes the intrinsic affinity of the molecules for each other in the solvent environment, stripped of the statistical thermodynamic effects of concentration.",
            "answer": "$$\n\\boxed{-79.12}\n$$"
        },
        {
            "introduction": "The reliability of any MM/PBSA or MM/GBSA calculation is fundamentally limited by the quality of the conformational ensemble from which the energies are averaged. This problem moves from the static energy formula to the dynamic challenge of molecular dynamics (MD) sampling . You will be tasked with making a defensible choice for a complete simulation protocol, balancing the need to achieve stationarity, obtain statistically independent snapshots, and meet a target precision, all while working within realistic computational constraints. This exercise hones the critical skills required to design robust and efficient MD simulations for free energy estimation.",
            "id": "3843949",
            "problem": "A flexible enzyme–ligand complex is to be analyzed with the end-point Molecular Mechanics/Generalized Born Surface Area (MM/GBSA) method to estimate the mean binding free energy from a Molecular Dynamics trajectory. The estimator is the ensemble average of energy components computed on snapshots of the complex, receptor, and ligand, and is only valid under stationarity of the sampled ensemble. In a pilot trajectory of length $60\\,\\mathrm{ns}$, the following observables were measured after the system relaxed: the slowest collective mode (a loop rearrangement) has a relaxation time $\\tau_{\\mathrm{slow}} \\approx 10\\,\\mathrm{ns}$; the time series of the MM/GBSA end-point energy (excluding any configurational entropy correction) has a post-equilibration standard deviation $\\sigma \\approx 2.4\\,\\mathrm{kcal\\,mol^{-1}}$ and an integrated autocorrelation time $\\tau_{\\mathrm{int}} \\approx 0.15\\,\\mathrm{ns}$. The study requires the standard error of the mean binding energy to be below $0.20\\,\\mathrm{kcal\\,mol^{-1}}$. If a configurational entropy estimate by normal-mode analysis is included, the downstream workflow can handle at most $300$ frames for tractability. You must choose a single uniform snapshot interval $\\Delta t$, a total simulation length $T$, and a discarded initial segment $t_{\\mathrm{eq}}$.\n\nWhich option provides the most defensible and internally consistent choice of $\\Delta t$, $T$, and $t_{\\mathrm{eq}}$ for this MM/GBSA workflow, given the constraints and the measured pilot statistics?\n\nA. $\\Delta t = 0.001\\,\\mathrm{ns}$, $T = 50\\,\\mathrm{ns}$, $t_{\\mathrm{eq}} = 5\\,\\mathrm{ns}$. Rationale: an extremely dense snapshot frequency ensures many frames and thus a small standard error; a brief initial discard suffices because the running average stabilizes quickly.\n\nB. $\\Delta t = 0.20\\,\\mathrm{ns}$, $T = 100\\,\\mathrm{ns}$, $t_{\\mathrm{eq}} = 20\\,\\mathrm{ns}$. Rationale: a snapshot spacing comparable to the measured integrated autocorrelation time yields largely independent frames; discarding twice the slow-mode relaxation time ensures stationarity while retaining ample production data.\n\nC. $\\Delta t = 0.50\\,\\mathrm{ns}$, $T = 150\\,\\mathrm{ns}$, $t_{\\mathrm{eq}} = 30\\,\\mathrm{ns}$. Rationale: a snapshot spacing several times the integrated autocorrelation time promotes near-independence; discarding three times the slow-mode relaxation time is conservative for stationarity; the total frames remain within the downstream limit while enabling the target precision.\n\nD. $\\Delta t = 5.0\\,\\mathrm{ns}$, $T = 200\\,\\mathrm{ns}$, $t_{\\mathrm{eq}} = 50\\,\\mathrm{ns}$. Rationale: very sparse sampling ensures independence; a long discard addresses slow relaxation comprehensively; the production segment is sufficiently long to provide representative sampling of microstates.",
            "solution": "The problem asks for the most defensible simulation protocol based on statistical requirements and practical constraints. We must evaluate each option against a set of criteria derived from the provided data: $\\tau_{\\mathrm{slow}} \\approx 10\\,\\mathrm{ns}$, $\\sigma \\approx 2.4\\,\\mathrm{kcal\\,mol^{-1}}$, $\\tau_{\\mathrm{int}} \\approx 0.15\\,\\mathrm{ns}$, a target $\\mathrm{SEM}  0.20\\,\\mathrm{kcal\\,mol^{-1}}$, and a maximum of $300$ frames.\n\n1.  **Equilibration Time ($t_{\\mathrm{eq}}$):** To ensure the system has reached stationarity, the discarded equilibration segment should be at least 2-3 times the slowest relaxation time, $\\tau_{\\mathrm{slow}}$. Thus, we need $t_{\\mathrm{eq}} \\ge 2 \\times 10\\,\\mathrm{ns} = 20\\,\\mathrm{ns}$.\n    *   Options B, C, and D satisfy this. Option A ($t_{\\mathrm{eq}} = 5\\,\\mathrm{ns}$) is insufficient.\n\n2.  **Production Time ($T_{\\mathrm{prod}}$):** The production time must be long enough to achieve the target precision. The number of effective (independent) samples, $N_{\\mathrm{eff}}$, required is given by $N_{\\mathrm{eff}} = (\\sigma / \\mathrm{SEM})^2 = (2.4 / 0.20)^2 = 144$. The required production time is $T_{\\mathrm{prod}} = N_{\\mathrm{eff}} \\times (2\\tau_{\\mathrm{int}}) = 144 \\times (2 \\times 0.15\\,\\mathrm{ns}) = 43.2\\,\\mathrm{ns}$.\n    *   Option A: $T_{\\mathrm{prod}} = 50 - 5 = 45\\,\\mathrm{ns}$ (Sufficient, but fails on equilibration).\n    *   Option B: $T_{\\mathrm{prod}} = 100 - 20 = 80\\,\\mathrm{ns}$ (Sufficient).\n    *   Option C: $T_{\\mathrm{prod}} = 150 - 30 = 120\\,\\mathrm{ns}$ (Sufficient).\n    *   Option D: $T_{\\mathrm{prod}} = 200 - 50 = 150\\,\\mathrm{ns}$ (Sufficient).\n\n3.  **Number of Frames ($N$):** The number of snapshots collected for analysis, $N = T_{\\mathrm{prod}} / \\Delta t$, must not exceed 300.\n    *   Option A: $45 / 0.001 = 45000 \\gg 300$. Fails.\n    *   Option B: $80 / 0.20 = 400 > 300$. Fails.\n    *   Option C: $120 / 0.50 = 240 \\le 300$. Passes.\n    *   Option D: $150 / 5.0 = 30 \\le 300$. Passes.\n\n4.  **Snapshot Interval ($\\Delta t$):** The interval should be efficient (not too small compared to $\\tau_{\\mathrm{int}}$) and safe (not so large as to risk aliasing important motions like $\\tau_{\\mathrm{slow}}$).\n    *   Option C: $\\Delta t = 0.50\\,\\mathrm{ns}$. This is a few times $\\tau_{\\mathrm{int}}$ (ensuring largely decorrelated frames) and much smaller than $\\tau_{\\mathrm{slow}}$ (safely sampling the slow loop dynamics). This is an excellent choice.\n    *   Option D: $\\Delta t = 5.0\\,\\mathrm{ns}$. This is dangerously large, equaling half of $\\tau_{\\mathrm{slow}}$. Such sparse sampling risks misrepresenting the conformational ensemble associated with the slow motion. It is not a robust or defensible choice.\n\n**Conclusion:**\nOption C is the only one that satisfies all criteria simultaneously. It provides a conservative equilibration time, more than sufficient production time for the target precision, respects the computational limit on the number of frames, and uses a scientifically sound sampling interval.",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "While MM/GBSA is a powerful tool for understanding binding, its raw outputs often exhibit systematic biases and may not correlate perfectly with experimental measurements. This advanced practice addresses this real-world challenge by guiding you through the process of building and validating a statistical correction model . You will use ordinary least squares to create a linear mapping between computed and experimental affinities and, more importantly, employ rigorous hypothesis testing to determine if this correction is statistically justified and transferable to new data. This exercise equips you with the data analysis skills necessary to transform raw computational results into a validated, predictive tool.",
            "id": "3843932",
            "problem": "You are given paired data sets of computed end-point binding free energies from Molecular Mechanics/Generalized Born Surface Area (MM/GBSA) and experimental binding free energies measured in solution for several hypothetical protein–ligand complexes. End-point methods such as Molecular Mechanics/Poisson–Boltzmann Surface Area (MM/PBSA) and Molecular Mechanics/Generalized Born Surface Area (MM/GBSA) approximate the binding free energy as differences of molecular mechanical energies plus solvation and non-polar terms. In practice, computed values often exhibit systematic bias relative to experiment due to approximations in the solvent model and entropic contributions. A common post-processing step is to apply a linear correction model that maps computed values to experimental values.\n\nStarting from first principles, consider a linear mapping from computed to experimental values with a model of the form $y = a + b x + \\varepsilon$, where $x$ denotes the computed MM/GBSA binding free energy in $\\mathrm{kcal/mol}$, $y$ denotes the experimental binding free energy in $\\mathrm{kcal/mol}$, $a$ is an intercept in $\\mathrm{kcal/mol}$, $b$ is a dimensionless slope, and $\\varepsilon$ is a zero-mean random error. You must derive and implement the ordinary least squares estimator by minimizing the sum of squared residuals, then assess whether the linear correction is statistically justified against the null hypothesis that no correction is needed (that is, the identity mapping $y = x$), and finally evaluate whether the learned correction is transferable to a separate test set.\n\nScientific and statistical requirements:\n- Use ordinary least squares to estimate $a$ and $b$ on a training set by minimizing $\\sum_i (y_i - (a + b x_i))^2$.\n- To assess whether the correction is statistically justified, perform a joint significance test of the linear restrictions $a = 0$ and $b = 1$ using a valid model comparison that contrasts the unrestricted linear fit against the restricted identity model $y = x$. Use a significance level of $\\alpha = 0.05$ and return the corresponding $p$-value for the joint test.\n- To assess transferability, evaluate the corrected model and the identity mapping on a held-out test set. Compute the root-mean-square error on the test set for both mappings. Then perform a one-sided paired test on the difference in absolute prediction errors between the corrected and identity mappings on the test set, with the null hypothesis that the mean difference is $0$ and the alternative that the corrected mapping has a smaller mean absolute error. Use the same significance level $\\alpha = 0.05$. Declare the correction transferable if it achieves a strictly smaller test root-mean-square error and the one-sided test is significant at $\\alpha$.\n\nAll free energies must be treated and reported in $\\mathrm{kcal/mol}$. The final program must operate on the following four test cases. Each test case consists of a training set and a test set, each provided as parallel lists of computed and experimental values. Each list entry is in $\\mathrm{kcal/mol}$.\n\nCase $1$ (good correlation with systematic bias):\n- Training computed: $x_\\text{train}^{(1)} = [-7.5, -6.1, -8.0, -5.2, -9.3, -7.1, -6.8, -5.9, -8.7, -7.9]$\n- Training experimental: $y_\\text{train}^{(1)} = [2 + 0.8(-7.5) + 0.15,\\; 2 + 0.8(-6.1) - 0.22,\\; 2 + 0.8(-8.0) + 0.05,\\; 2 + 0.8(-5.2) + 0.30,\\; 2 + 0.8(-9.3) - 0.10,\\; 2 + 0.8(-7.1) + 0.12,\\; 2 + 0.8(-6.8) - 0.05,\\; 2 + 0.8(-5.9) + 0.18,\\; 2 + 0.8(-8.7) - 0.20,\\; 2 + 0.8(-7.9) + 0.00]$\n- Test computed: $x_\\text{test}^{(1)} = [-6.5, -5.0, -8.2, -7.0, -9.0, -5.5, -6.0]$\n- Test experimental: $y_\\text{test}^{(1)} = [2 + 0.8(-6.5) + 0.05,\\; 2 + 0.8(-5.0) - 0.10,\\; 2 + 0.8(-8.2) + 0.12,\\; 2 + 0.8(-7.0) - 0.08,\\; 2 + 0.8(-9.0) + 0.00,\\; 2 + 0.8(-5.5) + 0.20,\\; 2 + 0.8(-6.0) - 0.15]$\n\nCase $2$ (identity mapping holds):\n- Training computed: $x_\\text{train}^{(2)} = [-7.0, -6.5, -8.0, -5.5, -9.2, -7.3, -6.0, -5.8]$\n- Training experimental: $y_\\text{train}^{(2)} = [-7.0 + 0.05,\\; -6.5 - 0.08,\\; -8.0 + 0.02,\\; -5.5 - 0.01,\\; -9.2 - 0.03,\\; -7.3 + 0.00,\\; -6.0 + 0.04,\\; -5.8 - 0.02]$\n- Test computed: $x_\\text{test}^{(2)} = [-7.1, -6.2, -8.3, -5.2, -9.0]$\n- Test experimental: $y_\\text{test}^{(2)} = [-7.1 + 0.02,\\; -6.2 - 0.04,\\; -8.3 + 0.01,\\; -5.2 - 0.02,\\; -9.0 + 0.03]$\n\nCase $3$ (small-sample boundary):\n- Training computed: $x_\\text{train}^{(3)} = [-7.0, -5.0, -9.0]$\n- Training experimental: $y_\\text{train}^{(3)} = [1.5 + 0.9(-7.0) + 0.4,\\; 1.5 + 0.9(-5.0) - 0.3,\\; 1.5 + 0.9(-9.0) + 0.1]$\n- Test computed: $x_\\text{test}^{(3)} = [-6.0, -8.0]$\n- Test experimental: $y_\\text{test}^{(3)} = [1.5 + 0.9(-6.0) + 0.0,\\; 1.5 + 0.9(-8.0) + 0.2]$\n\nCase $4$ (non-transferable correction due to distribution shift):\n- Training computed: $x_\\text{train}^{(4)} = [-10.0, -8.5, -7.2, -6.8, -5.5, -9.1, -7.7]$\n- Training experimental: $y_\\text{train}^{(4)} = [2 + 0.8(-10.0) + 0.10,\\; 2 + 0.8(-8.5) - 0.05,\\; 2 + 0.8(-7.2) + 0.20,\\; 2 + 0.8(-6.8) - 0.10,\\; 2 + 0.8(-5.5) + 0.15,\\; 2 + 0.8(-9.1) - 0.08,\\; 2 + 0.8(-7.7) + 0.00]$\n- Test computed: $x_\\text{test}^{(4)} = [-9.5, -8.0, -6.0, -5.0, -7.5]$\n- Test experimental: $y_\\text{test}^{(4)} = [-9.5 + 0.05,\\; -8.0 - 0.02,\\; -6.0 + 0.03,\\; -5.0 - 0.01,\\; -7.5 + 0.00]$\n\nAlgorithmic tasks:\n- For each case, estimate $a$ and $b$ on the training set using ordinary least squares, perform a joint test of $a = 0$ and $b = 1$ against the unrestricted linear model to obtain a $p$-value at $\\alpha = 0.05$, compute the test-set root-mean-square error for the corrected mapping and for the identity mapping, and perform a one-sided paired test on the difference in absolute errors to evaluate transferability.\n- For the one-sided paired test, use the alternative hypothesis that the corrected mapping yields smaller mean absolute error.\n\nYour program must produce a single line of output containing one list per case, aggregated in a single bracketed list. Each per-case list must contain exactly seven entries in the following order:\n$[b,\\; a,\\; p_\\text{joint},\\; \\text{justified},\\; \\text{RMSE}_\\text{corr},\\; \\text{RMSE}_\\text{ident},\\; \\text{transferable}]$\nwhere $b$ and $a$ are the slope and intercept, $p_\\text{joint}$ is the $p$-value of the joint test for $a = 0$ and $b = 1$, $\\text{justified}$ and $\\text{transferable}$ are boolean values determined at $\\alpha = 0.05$, and $\\text{RMSE}_\\text{corr}$ and $\\text{RMSE}_\\text{ident}$ are the root-mean-square errors on the test set for the corrected and identity mappings, respectively. Format the floats with $6$ decimal places for $b$, $a$, and $p_\\text{joint}$, and with $3$ decimal places for the two root-mean-square errors. The final output should be a single line of text exactly in the format:\n$[[\\text{case1}],[\\text{case2}],[\\text{case3}],[\\text{case4}]]$\nwith comma-separated entries and no additional text. All energies are in $\\mathrm{kcal/mol}$ and no units should be printed in the output.",
            "solution": "The problem is valid as it is scientifically grounded in computational biophysics, well-posed, and provides all necessary information for a unique solution. We will proceed by first deriving the necessary statistical formulas from first principles and then applying them to the provided data.\n\nThe overall goal is to evaluate a linear correction model for computed binding free energies. The model is given by:\n$$y = a + b x + \\varepsilon$$\nwhere $y$ is the experimental binding free energy, $x$ is the computed binding free energy, $a$ and $b$ are the intercept and slope coefficients, and $\\varepsilon$ is a zero-mean error term. The analysis involves three main steps: parameter estimation, model justification, and assessment of transferability.\n\n**1. Parameter Estimation via Ordinary Least Squares (OLS)**\n\nThe coefficients $a$ and $b$ are estimated by minimizing the sum of squared residuals (SSR) on a training set of $n$ data points $\\{ (x_i, y_i) \\}_{i=1}^n$. The SSR, denoted by $S(a, b)$, is:\n$$S(a, b) = \\sum_{i=1}^{n} (y_i - (a + b x_i))^2$$\nTo find the minimum, we take the partial derivatives of $S(a, b)$ with respect to $a$ and $b$ and set them to zero. This gives the normal equations:\n$$\n\\frac{\\partial S}{\\partial a} = -2\\sum_{i=1}^{n}(y_i - a - b x_i) = 0 \\implies \\sum_{i=1}^{n} y_i = n a + b \\sum_{i=1}^{n} x_i\n$$\n$$\n\\frac{\\partial S}{\\partial b} = -2\\sum_{i=1}^{n} x_i(y_i - a - b x_i) = 0 \\implies \\sum_{i=1}^{n} x_i y_i = a \\sum_{i=1}^{n} x_i + b \\sum_{i=1}^{n} x_i^2\n$$\nDividing the first equation by $n$ gives an expression for the OLS estimator $\\hat{a}$ in terms of the sample means $\\bar{y}$ and $\\bar{x}$:\n$$\\bar{y} = \\hat{a} + \\hat{b} \\bar{x} \\implies \\hat{a} = \\bar{y} - \\hat{b} \\bar{x}$$\nSubstituting this into the second normal equation and solving for the OLS estimator $\\hat{b}$ yields:\n$$\n\\hat{b} = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} = \\frac{\\text{Cov}(x, y)}{\\text{Var}(x)}\n$$\nThese formulas provide the optimal estimates for the slope $\\hat{b}$ and intercept $\\hat{a}$ that minimize the SSR.\n\n**2. Joint Hypothesis Test for Statistical Justification**\n\nWe must assess if the linear correction is statistically justified. This is framed as a hypothesis test against the null hypothesis $H_0$ that no correction is needed, which corresponds to the identity mapping $y=x$. This implies the joint linear restrictions $a=0$ and $b=1$. The alternative hypothesis $H_A$ is that at least one of these restrictions does not hold.\n\nWe use an F-test to compare the unrestricted model ($y = a + b x$) with the restricted model ($y = x$). The F-statistic is constructed from the sum of squared residuals of both models:\n$$\nF = \\frac{(\\text{SSR}_R - \\text{SSR}_U) / q}{\\text{SSR}_U / (n - k_U)}\n$$\nwhere:\n- $\\text{SSR}_U = \\sum_{i=1}^{n} (y_i - (\\hat{a} + \\hat{b} x_i))^2$ is the SSR for the unrestricted model, with $\\hat{a}$ and $\\hat{b}$ being the OLS estimates.\n- $\\text{SSR}_R = \\sum_{i=1}^{n} (y_i - (0 + 1 \\cdot x_i))^2 = \\sum_{i=1}^{n} (y_i - x_i)^2$ is the SSR for the restricted model.\n- $q=2$ is the number of linear restrictions ($a=0$, $b=1$).\n- $k_U=2$ is the number of parameters in the unrestricted model ($a$, $b$).\n- $n$ is the number of data points in the training set.\n\nUnder the null hypothesis, this statistic follows an F-distribution with $q$ and $n-k_U$ degrees of freedom, i.e., $F \\sim F(2, n-2)$. We calculate the $p$-value from this distribution. The correction is deemed justified if this $p$-value is less than the significance level $\\alpha = 0.05$.\n\n**3. Transferability Assessment on a Test Set**\n\nThe final step is to evaluate whether the learned correction model generalizes to new data. This is assessed on a held-out test set of $m$ points $\\{ (x_{\\text{test},j}, y_{\\text{test},j}) \\}_{j=1}^m$.\n\nFirst, we compute the Root-Mean-Square Error (RMSE) for both the corrected model and the identity mapping on the test set:\n$$\n\\text{RMSE}_\\text{corr} = \\sqrt{\\frac{1}{m} \\sum_{j=1}^{m} (y_{\\text{test},j} - (\\hat{a} + \\hat{b} x_{\\text{test},j}))^2}\n$$\n$$\n\\text{RMSE}_\\text{ident} = \\sqrt{\\frac{1}{m} \\sum_{j=1}^{m} (y_{\\text{test},j} - x_{\\text{test},j})^2}\n$$\nNext, a one-sided paired t-test is performed on the absolute prediction errors. Let $d_j$ be the difference in absolute errors for the $j$-th test point:\n$$\nd_j = |y_{\\text{test}, j} - x_{\\text{test}, j}| - |y_{\\text{test}, j} - (\\hat{a} + \\hat{b} x_{\\text{test}, j})|\n$$\nThe null hypothesis is $H_0: \\mu_d = 0$, where $\\mu_d$ is the mean of the differences. The alternative hypothesis is $H_A: \\mu_d > 0$, which posits that the corrected model has a smaller mean absolute error. The t-statistic is:\n$$\nt = \\frac{\\bar{d}}{s_d / \\sqrt{m}}\n$$\nwhere $\\bar{d}$ and $s_d$ are the sample mean and sample standard deviation of the differences $d_j$. This statistic follows a Student's t-distribution with $m-1$ degrees of freedom.\n\nThe correction is considered transferable if two conditions are met:\n1. The corrected model has a strictly smaller test RMSE: $\\text{RMSE}_\\text{corr}  \\text{RMSE}_\\text{ident}$.\n2. The one-sided paired t-test is significant at $\\alpha = 0.05$.\n\nThe following implementation will carry out these calculations for each provided case.",
            "answer": "[[0.801692,1.986751,0.000003,True,0.141,4.401,True],[1.002821,-0.021051,0.999986,False,0.033,0.033,False],[0.866667,1.833333,0.518519,False,0.158,2.500,True],[0.798131,1.967290,0.000021,True,0.032,1.968,False]]"
        }
    ]
}