{
    "hands_on_practices": [
        {
            "introduction": "Before using experimental data to build or test a computational model, we must first ensure the data itself is reliable and internally consistent. This practice demonstrates how to leverage a fundamental principle of thermodynamics, Hess's Law, to cross-validate multiple, independent measurements of binding free energies. By constructing a thermodynamic cycle, you will learn to assess statistical consistency and combine redundant data to infer missing parameters with confidence .",
            "id": "3845656",
            "problem": "A protein $P$ binds two ligands $L_{1}$ and $L_{2}$ in sequence to form the complexes $PL_{1}$, $PL_{2}$, and $PL_{1}L_{2}$ under standard state ($1\\,\\mathrm{M}$) conditions at $T=298.15\\,\\mathrm{K}$. The standard Gibbs free energies of binding measured by Isothermal Titration Calorimetry (ITC) and Surface Plasmon Resonance (SPR) are provided with their estimated standard uncertainties (one standard deviation), and an additional three-component calorimetric experiment provides the overall standard free energy to form $PL_{1}L_{2}$ directly from $P+L_{1}+L_{2}$. The measurements are:\n- $P + L_{1} \\rightarrow PL_{1}$: $\\Delta G_{A\\to B} = -35.4\\,\\mathrm{kJ\\, mol^{-1}}$ with uncertainty $\\sigma_{AB} = 0.7\\,\\mathrm{kJ\\, mol^{-1}}$ (ITC).\n- $PL_{1} + L_{2} \\rightarrow PL_{1}L_{2}$: $\\Delta G_{B\\to D} = -18.6\\,\\mathrm{kJ\\, mol^{-1}}$ with uncertainty $\\sigma_{BD} = 0.9\\,\\mathrm{kJ\\, mol^{-1}}$ (ITC).\n- $P + L_{2} \\rightarrow PL_{2}$: $\\Delta G_{A\\to C} = -14.2\\,\\mathrm{kJ\\, mol^{-1}}$ with uncertainty $\\sigma_{AC} = 0.5\\,\\mathrm{kJ\\, mol^{-1}}$ (SPR).\n- $P + L_{1} + L_{2} \\rightarrow PL_{1}L_{2}$: $\\Delta G_{A\\to D} = -53.0\\,\\mathrm{kJ\\, mol^{-1}}$ with uncertainty $\\sigma_{AD} = 1.4\\,\\mathrm{kJ\\, mol^{-1}}$ (three-component calorimetry).\n\nAssume the Gibbs free energy is a state function and that Hess’s law applies to any thermodynamic cycle formed by these states. Treat the reported uncertainties as independent and Gaussian, and use inverse-variance weighting to impose experimental constraints on a computational model of the binding network.\n\nFirst, use the state-function property to assess whether the direct measurement $\\Delta G_{A\\to D}$ is statistically consistent with the two-step path $A\\to B\\to D$. Then, using Hess’s law to close the $A\\to C\\to D$ path, infer the missing standard Gibbs free energy $\\Delta G_{C\\to D}$ that is most consistent with all measurements under a weighted least-squares adjustment that enforces $A\\to B\\to D$ to match $A\\to D$. Express the final inferred $\\Delta G_{C\\to D}$ in $\\mathrm{kJ\\, mol^{-1}}$ and round your answer to four significant figures.",
            "solution": "The problem requires the analysis of a thermodynamic network for protein-ligand binding and the inference of a missing thermodynamic quantity using a set of redundant, uncertain experimental measurements. The analysis will proceed in two parts as requested: first, an assessment of the statistical consistency of the provided data, and second, the calculation of the most consistent value for the unknown Gibbs free energy change, $\\Delta G_{C\\to D}$, using a weighted least-squares adjustment.\n\nThe binding scheme describes a thermodynamic cycle connecting four states:\n- State A: $P + L_{1} + L_{2}$\n- State B: $PL_{1} + L_{2}$\n- State C: $PL_{2} + L_{1}$\n- State D: $PL_{1}L_{2}$\n\nThe provided standard Gibbs free energy changes ($\\Delta G^\\circ$) and their standard uncertainties ($\\sigma$) are:\n- $\\Delta G_{A\\to B} = -35.4\\,\\mathrm{kJ\\, mol^{-1}}$ with $\\sigma_{AB} = 0.7\\,\\mathrm{kJ\\, mol^{-1}}$\n- $\\Delta G_{B\\to D} = -18.6\\,\\mathrm{kJ\\, mol^{-1}}$ with $\\sigma_{BD} = 0.9\\,\\mathrm{kJ\\, mol^{-1}}$\n- $\\Delta G_{A\\to C} = -14.2\\,\\mathrm{kJ\\, mol^{-1}}$ with $\\sigma_{AC} = 0.5\\,\\mathrm{kJ\\, mol^{-1}}$\n- $\\Delta G_{A\\to D} = -53.0\\,\\mathrm{kJ\\, mol^{-1}}$ with $\\sigma_{AD} = 1.4\\,\\mathrm{kJ\\, mol^{-1}}$\n\nThe fundamental principle governing this system is Hess's law, which states that because Gibbs free energy is a state function, the total change in free energy between two states is independent of the path taken. This implies that for any a closed cycle, the net change in free energy must be zero. For the square cycle $A \\to B \\to D \\to C \\to A$, this means the true (error-free) values must satisfy $\\Delta G_{A\\to B} + \\Delta G_{B\\to D} + \\Delta G_{D\\to C} + \\Delta G_{C\\to A} = 0$. Since $\\Delta G_{D\\to C} = -\\Delta G_{C\\to D}$ and $\\Delta G_{C\\to A} = -\\Delta G_{A\\to C}$, this is equivalent to the path-independence condition:\n$$ \\Delta G_{A\\to B} + \\Delta G_{B\\to D} = \\Delta G_{A\\to C} + \\Delta G_{C\\to D} $$\nThis equation also implies that the total free energy change from state A to state D, $\\Delta G_{A\\to D}$, must be the same regardless of the path:\n$$ \\Delta G_{A\\to D} = \\Delta G_{A\\to B} + \\Delta G_{B\\to D} $$\n$$ \\Delta G_{A\\to D} = \\Delta G_{A\\to C} + \\Delta G_{C\\to D} $$\nOur experimental data provide multiple, independent pathways to determine the same quantity, a situation known as overdetermination.\n\nFirst, we assess the statistical consistency between the direct measurement of $\\Delta G_{A\\to D}$ and the value derived from the two-step path $A \\to B \\to D$.\nThe free energy change for the indirect path is the sum of the steps:\n$$ \\Delta G_{A\\to B\\to D} = \\Delta G_{A\\to B} + \\Delta G_{B\\to D} = -35.4\\,\\mathrm{kJ\\, mol^{-1}} + (-18.6\\,\\mathrm{kJ\\, mol^{-1}}) = -54.0\\,\\mathrm{kJ\\, mol^{-1}} $$\nSince the measurements are independent, the uncertainties add in quadrature (i.e., the variances add). The variance for the indirect path is:\n$$ \\sigma_{A\\to B\\to D}^2 = \\sigma_{AB}^2 + \\sigma_{BD}^2 = (0.7\\,\\mathrm{kJ\\, mol^{-1}})^2 + (0.9\\,\\mathrm{kJ\\, mol^{-1}})^2 = 0.49\\,\\mathrm{kJ^2\\, mol^{-2}} + 0.81\\,\\mathrm{kJ^2\\, mol^{-2}} = 1.30\\,\\mathrm{kJ^2\\, mol^{-2}} $$\nWe compare this result, $\\Delta G_{A\\to B\\to D} = -54.0\\,\\mathrm{kJ\\, mol^{-1}}$, with the direct measurement, $\\Delta G_{A\\to D} = -53.0\\,\\mathrm{kJ\\, mol^{-1}}$. The difference, $\\delta$, is:\n$$ \\delta = \\Delta G_{A\\to B\\to D} - \\Delta G_{A\\to D} = -54.0\\,\\mathrm{kJ\\, mol^{-1}} - (-53.0\\,\\mathrm{kJ\\, mol^{-1}}) = -1.0\\,\\mathrm{kJ\\, mol^{-1}} $$\nThe variance of this difference is the sum of the variances of the two quantities, as they are derived from independent sets of experiments:\n$$ \\sigma_{\\delta}^2 = \\sigma_{A\\to B\\to D}^2 + \\sigma_{AD}^2 = 1.30\\,\\mathrm{kJ^2\\, mol^{-2}} + (1.4\\,\\mathrm{kJ\\, mol^{-1}})^2 = 1.30\\,\\mathrm{kJ^2\\, mol^{-2}} + 1.96\\,\\mathrm{kJ^2\\, mol^{-2}} = 3.26\\,\\mathrm{kJ^2\\, mol^{-2}} $$\nThe standard deviation of the difference is $\\sigma_{\\delta} = \\sqrt{3.26}\\,\\mathrm{kJ\\, mol^{-1}} \\approx 1.806\\,\\mathrm{kJ\\, mol^{-1}}$.\nTo assess statistical consistency, we compute the z-score, which measures the difference in units of its standard deviation:\n$$ z = \\frac{|\\delta|}{\\sigma_{\\delta}} = \\frac{|-1.0\\,\\mathrm{kJ\\, mol^{-1}}|}{1.806\\,\\mathrm{kJ\\, mol^{-1}}} \\approx 0.55 $$\nSince $z \\ll 2$, the difference between the direct and indirect measurements of $\\Delta G_{A\\to D}$ is not statistically significant. The measurements are consistent with each other within their reported uncertainties.\n\nNext, we infer the value of $\\Delta G_{C\\to D}$ that is most consistent with all measurements. This requires combining the redundant information to obtain a single best estimate for $\\Delta G_{A\\to D}$, which we will denote $\\Delta G_{A\\to D}^*$. This is the \"weighted least-squares adjustment that enforces $A\\to B\\to D$ to match $A\\to D$\". For independent Gaussian uncertainties, the optimal combination is an inverse-variance weighted average, which is the maximum likelihood estimator.\n\nWe have two independent estimates for $\\Delta G_{A\\to D}$:\n1. From path $A \\to B \\to D$: $x_1 = \\Delta G_{A\\to B\\to D} = -54.0\\,\\mathrm{kJ\\, mol^{-1}}$, with variance $v_1 = \\sigma_{A\\to B\\to D}^2 = 1.30\\,\\mathrm{kJ^2\\, mol^{-2}}$.\n2. From the direct measurement: $x_2 = \\Delta G_{A\\to D} = -53.0\\,\\mathrm{kJ\\, mol^{-1}}$, with variance $v_2 = \\sigma_{AD}^2 = 1.96\\,\\mathrm{kJ^2\\, mol^{-2}}$.\n\nThe inverse-variance weighted average is given by:\n$$ \\Delta G_{A\\to D}^* = \\frac{\\frac{x_1}{v_1} + \\frac{x_2}{v_2}}{\\frac{1}{v_1} + \\frac{1}{v_2}} $$\nPlugging in the values:\n$$ \\Delta G_{A\\to D}^* = \\frac{\\frac{-54.0}{1.30} + \\frac{-53.0}{1.96}}{\\frac{1}{1.30} + \\frac{1}{1.96}} = \\frac{-41.53846 - 27.04082}{0.76923 + 0.51020}\\,\\mathrm{kJ\\, mol^{-1}} = \\frac{-68.57928}{1.27943}\\,\\mathrm{kJ\\, mol^{-1}} \\approx -53.6014\\,\\mathrm{kJ\\, mol^{-1}} $$\nThis reconciled value, $\\Delta G_{A\\to D}^*$, represents the most probable true free energy change for the overall $A \\to D$ process, given all the relevant measurements.\n\nFinally, we use this reconciled value and Hess's law for the other path, $A \\to C \\to D$, to infer the value of $\\Delta G_{C\\to D}$. The governing thermodynamic relationship is:\n$$ \\Delta G_{A\\to D}^* = \\Delta G_{A\\to C} + \\Delta G_{C\\to D} $$\nWe solve for $\\Delta G_{C\\to D}$, using our best estimate for $\\Delta G_{A\\to D}^*$ and the experimental measurement for $\\Delta G_{A\\to C}$, as it is the only information available for that leg of the cycle.\n$$ \\Delta G_{C\\to D, \\text{inf}} = \\Delta G_{A\\to D}^* - \\Delta G_{A\\to C} $$\n$$ \\Delta G_{C\\to D, \\text{inf}} = -53.6014\\,\\mathrm{kJ\\, mol^{-1}} - (-14.2\\,\\mathrm{kJ\\, mol^{-1}}) = -39.4014\\,\\mathrm{kJ\\, mol^{-1}} $$\nThe problem requires the answer to be rounded to four significant figures.\n$$ \\Delta G_{C\\to D, \\text{inf}} \\approx -39.40\\,\\mathrm{kJ\\, mol^{-1}} $$\nThis value is the Gibbs free energy change for the reaction $PL_{2} + L_{1} \\to PL_{1}L_{2}$ that is most consistent with the entire set of provided experimental data under the assumption of a valid thermodynamic cycle.",
            "answer": "$$ \\boxed{-39.40} $$"
        },
        {
            "introduction": "A central task in computational modeling is to determine whether a model's predictions are statistically compatible with experimental evidence. This hands-on coding exercise will guide you through the implementation of the chi-square ($\\chi^2$) goodness-of-fit test, a robust statistical method for this purpose. You will learn to compute standardized residuals and use the $\\chi^2$ distribution to obtain a p-value that quantifies the model-data agreement, providing an objective criterion for model validation .",
            "id": "3845717",
            "problem": "You are given multiple independent datasets of experimental constraints for a fixed computational chemical biology model that predicts measurable quantities. Each dataset comprises observed values, model-predicted values, and estimated measurement uncertainties. Assume that each measurement error is independent and normally distributed, and that some datasets have had a subset of model parameters tuned using their data. Under these assumptions, standardized residuals follow a standard normal distribution and their sum of squares follows a chi-square distribution. Your task is to implement a complete program that computes per-dataset residuals, chi-square test statistics, and goodness-of-fit decisions, and also aggregates these into a global goodness-of-fit across datasets.\n\nUse the following foundational base:\n- Under independent measurement errors modeled as normally distributed with known standard deviations, if the model predicts latent mean values, then the standardized residuals $$r_i = \\frac{y_i - m_i}{\\sigma_i}$$ are approximately independent draws from a standard normal distribution.\n- By the Central Limit Theorem (CLT) and properties of the normal distribution, the sum $$X^2 = \\sum_{i=1}^{n} r_i^2$$ follows a chi-square distribution with degrees of freedom (DoF) $$\\nu = n - k,$$ where $$n$$ is the number of independent measurements in the dataset and $$k$$ is the number of model parameters effectively fit using that dataset.\n- For a chi-square random variable $$X^2 \\sim \\chi^2_{\\nu}$$, the tail probability (goodness-of-fit p-value) equals $$p = 1 - F_{\\chi^2_{\\nu}}(X^2),$$ where $$F_{\\chi^2_{\\nu}}$$ denotes the cumulative distribution function (CDF) of the chi-square distribution.\n\nProgram requirements:\n- For each dataset, compute the residual vector $$\\mathbf{r} = \\left[\\frac{y_1 - m_1}{\\sigma_1}, \\ldots, \\frac{y_n - m_n}{\\sigma_n}\\right],$$ the chi-square statistic $$X^2 = \\sum_{i=1}^{n} r_i^2,$$ the degrees of freedom $$\\nu = n - k,$$ the p-value $$p = 1 - F_{\\chi^2_{\\nu}}(X^2),$$ and the test decision at significance level $$\\alpha$$, defined as $$\\text{pass} = (p \\ge \\alpha).$$\n- Independently across datasets, aggregate a global goodness-of-fit by summing chi-square statistics and degrees of freedom: $$X^2_{\\text{global}} = \\sum_{d} X^2_d,$$ $$\\nu_{\\text{global}} = \\sum_{d} \\nu_d,$$ and $$p_{\\text{global}} = 1 - F_{\\chi^2_{\\nu_{\\text{global}}}}(X^2_{\\text{global}}),$$ with $$\\text{pass}_{\\text{global}} = (p_{\\text{global}} \\ge \\alpha).$$\n- Round all floating-point outputs (residuals, chi-square statistics, and p-values) to six decimal places.\n\nInput specification embedded in the program:\n- There is no external input. Your program must internally define the following test suite of cases, each containing multiple datasets. The measurement units are consistent and the outputs (residuals, chi-square statistics, and p-values) are dimensionless. The arrays are given in the order $[y_1, \\ldots, y_n]$, $[m_1, \\ldots, m_n]$, and $[\\sigma_1, \\ldots, \\sigma_n]$ with $k$ indicating the number of fitted parameters subtracted from the degrees of freedom.\n\nTest Suite:\n- Case 1 (general consistency, homogeneous uncertainties, moderate residuals):\n  - Significance level: $$\\alpha = 0.05.$$\n  - Dataset A:\n    - Observed: $$\\mathbf{y}_A = [1.0, 0.8, 1.2, 0.9, 1.1].$$\n    - Predicted: $$\\mathbf{m}_A = [1.0, 0.75, 1.15, 0.95, 1.05].$$\n    - Uncertainties: $$\\boldsymbol{\\sigma}_A = [0.1, 0.1, 0.1, 0.1, 0.1].$$\n    - Fitted parameters: $$k_A = 1.$$\n  - Dataset B:\n    - Observed: $$\\mathbf{y}_B = [2.0, 1.8, 2.1, 1.9].$$\n    - Predicted: $$\\mathbf{m}_B = [2.05, 1.85, 2.05, 1.95].$$\n    - Uncertainties: $$\\boldsymbol{\\sigma}_B = [0.1, 0.1, 0.1, 0.1, 0.1].$$\n    - Fitted parameters: $$k_B = 1.$$\n- Case 2 (heteroscedastic uncertainties and mixed residual magnitudes):\n  - Significance level: $$\\alpha = 0.05.$$\n  - Dataset C:\n    - Observed: $$\\mathbf{y}_C = [5.0, 5.3, 4.8, 5.2].$$\n    - Predicted: $$\\mathbf{m}_C = [5.1, 5.2, 5.0, 5.1].$$\n    - Uncertainties: $$\\boldsymbol{\\sigma}_C = [0.2, 0.3, 0.2, 0.4].$$\n    - Fitted parameters: $$k_C = 2.$$\n  - Dataset D:\n    - Observed: $$\\mathbf{y}_D = [3.0, 3.2, 2.9].$$\n    - Predicted: $$\\mathbf{m}_D = [3.1, 3.0, 3.0].$$\n    - Uncertainties: $$\\boldsymbol{\\sigma}_D = [0.2, 0.2, 0.1].$$\n    - Fitted parameters: $$k_D = 1.$$\n- Case 3 (strong inconsistency, small uncertainties leading to large residuals):\n  - Significance level: $$\\alpha = 0.05.$$\n  - Dataset E:\n    - Observed: $$\\mathbf{y}_E = [0.9, 1.1, 0.95, 1.05, 1.0].$$\n    - Predicted: $$\\mathbf{m}_E = [1.2, 1.2, 1.2, 1.2, 1.2].$$\n    - Uncertainties: $$\\boldsymbol{\\sigma}_E = [0.05, 0.05, 0.05, 0.05, 0.05].$$\n    - Fitted parameters: $$k_E = 0.$$\n  - Dataset F:\n    - Observed: $$\\mathbf{y}_F = [10.0, 9.5, 10.5, 9.8].$$\n    - Predicted: $$\\mathbf{m}_F = [10.0, 10.0, 10.0, 10.0].$$\n    - Uncertainties: $$\\boldsymbol{\\sigma}_F = [0.2, 0.2, 0.2, 0.2].$$\n    - Fitted parameters: $$k_F = 0.$$\n\nOutput specification:\n- For each case, output a list comprising one entry per dataset and one final global entry. Each dataset entry must be a list $$[\\mathbf{r}, X^2, p, \\text{pass}]$$ where $$\\mathbf{r}$$ is the residual vector rounded to six decimals, $$X^2$$ is the chi-square statistic rounded to six decimals, $$p$$ is the p-value rounded to six decimals, and $$\\text{pass}$$ is a boolean. The final global entry must be a list $$[X^2_{\\text{global}}, p_{\\text{global}}, \\text{pass}_{\\text{global}}]$$ with the same rounding for floating values.\n- Your program should produce a single line of output containing the results for all cases as a comma-separated list enclosed in square brackets, with no spaces anywhere. That is, the top-level output has the form $$[\\text{case}_1,\\text{case}_2,\\text{case}_3]$$ where each $$\\text{case}_i$$ is itself a list constructed as specified above.",
            "solution": "The starting point is the probabilistic model of experimental constraints for a computational chemical biology system with a fixed parameterization. Let $$y_i$$ denote the observed measurement and $$m_i$$ denote the corresponding model-predicted mean for the same experimental condition, with $$\\sigma_i$$ denoting the known or estimated standard deviation of the measurement error. We assume independent, normally distributed errors so that $$y_i = m_i + \\varepsilon_i$$ with $$\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_i^2)$$ and errors across different measurements and datasets are independent. This assumption is justified when measurement protocols are independent and replicate variability is well characterized, which is a common scenario within computational chemical biology when integrating orthogonal assays.\n\nUnder this model, each standardized residual $$r_i = \\frac{y_i - m_i}{\\sigma_i}$$ follows a standard normal distribution $$\\mathcal{N}(0,1)$$ and, due to independence, the vector $$\\mathbf{r}$$ is composed of independent standard normal entries. A core statistical fact is that the sum of squares of independent standard normal variables follows a chi-square distribution. Specifically, the statistic\n$$\nX^2 = \\sum_{i=1}^{n} r_i^2\n$$\nhas a chi-square distribution with degrees of freedom $$\\nu = n - k$$, where $$n$$ is the number of measurements in the dataset and $$k$$ is the number of effectively fitted model parameters used to tune predictions with that dataset. The subtraction by $$k$$ accounts for the loss of degrees of freedom incurred by parameter estimation, a standard correction in goodness-of-fit testing.\n\nTo assess goodness-of-fit, we compute the p-value as the upper-tail probability of the chi-square distribution:\n$$\np = \\mathbb{P}\\left(\\chi^2_{\\nu} \\ge X^2\\right) = 1 - F_{\\chi^2_{\\nu}}(X^2),\n$$\nwhere $$F_{\\chi^2_{\\nu}}$$ is the cumulative distribution function (CDF) of the chi-square distribution with $$\\nu$$ degrees of freedom. Given a significance level $$\\alpha$$, the test decision is\n$$\n\\text{pass} =\n\\begin{cases}\n\\text{True}, & \\text{if } p \\ge \\alpha, \\\\\n\\text{False}, & \\text{if } p < \\alpha.\n\\end{cases}\n$$\n\nWhen combining datasets that are experimentally independent, the global goodness-of-fit statistic is the sum of individual chi-square statistics, and the global degrees of freedom is the sum of individual degrees of freedom:\n$$\nX^2_{\\text{global}} = \\sum_{d} X^2_d, \\quad \\nu_{\\text{global}} = \\sum_{d} \\nu_d.\n$$\nBy the additivity property of chi-square variables derived from sums of independent squared standard normals, $$X^2_{\\text{global}} \\sim \\chi^2_{\\nu_{\\text{global}}}$$ under the model assumptions. Consequently,\n$$\np_{\\text{global}} = 1 - F_{\\chi^2_{\\nu_{\\text{global}}}}(X^2_{\\text{global}})\n$$\nwith the same decision rule $$\\text{pass}_{\\text{global}} = (p_{\\text{global}} \\ge \\alpha)$$.\n\nAlgorithmic design:\n1. For each dataset, read $$\\mathbf{y}, \\mathbf{m}, \\boldsymbol{\\sigma}, k$$ and $$\\alpha$$. Compute the residual vector as $$\\mathbf{r} = (\\mathbf{y} - \\mathbf{m}) \\oslash \\boldsymbol{\\sigma}$$, where $$\\oslash$$ denotes element-wise division.\n2. Compute $$X^2 = \\sum r_i^2$$ and $$\\nu = n - k$$. Check that $$\\nu > 0$$ to ensure validity of the chi-square goodness-of-fit test.\n3. Compute the p-value $$p = 1 - F_{\\chi^2_{\\nu}}(X^2)$$ using a reliable numerical routine for the chi-square survival function, which directly computes upper-tail probabilities to maintain numerical stability.\n4. Compare $$p$$ against $$\\alpha$$ and set $$\\text{pass}$$ accordingly.\n5. Aggregate across datasets by summing $$X^2$$ and $$\\nu$$ to obtain $$X^2_{\\text{global}}$$ and $$\\nu_{\\text{global}}$$, then compute $$p_{\\text{global}}$$ and $$\\text{pass}_{\\text{global}}$$.\n6. Round all floating outputs to six decimals, as required, and construct the output structure per the specification.\n\nApplication to the test suite:\n- Case 1 uses homogeneous uncertainties $$\\boldsymbol{\\sigma}_A = [0.1, 0.1, 0.1, 0.1, 0.1]$$ and $$\\boldsymbol{\\sigma}_B = [0.1, 0.1, 0.1, 0.1]$$ with moderate residuals, and fitted parameters $$k_A = 1$$, $$k_B = 1$$. The computed $$X^2$$ values are small relative to their respective $$\\nu$$, yielding large p-values and passing the test at $$\\alpha = 0.05$$.\n- Case 2 demonstrates heteroscedastic uncertainties and mixed residuals, with $$k_C = 2$$, $$k_D = 1$$. The per-dataset $$X^2$$ are moderate and the aggregated global $$X^2_{\\text{global}}$$ with $$\\nu_{\\text{global}}$$ yields a non-small p-value, passing at $$\\alpha = 0.05$$.\n- Case 3 has small uncertainties and large discrepancies, leading to large $$X^2$$ for both datasets and a very small global p-value; the test fails at $$\\alpha = 0.05$$.\n\nThe program implements these steps and prints a single line containing the list of results for all cases. Each dataset entry is of the form $$[\\mathbf{r}, X^2, p, \\text{pass}]$$ and each global entry is $$[X^2_{\\text{global}}, p_{\\text{global}}, \\text{pass}_{\\text{global}}]$$, all rounded to six decimals. The top-level list contains the three cases in order, formatted with commas and brackets without spaces, enabling automated parsing and verification.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef compute_dataset_metrics(y, m, s, k, alpha):\n    \"\"\"\n    Compute residuals, chi-square statistic, p-value, and pass/fail for one dataset.\n    y, m, s are numpy arrays of equal length; k is integer; alpha is float.\n    \"\"\"\n    # Standardized residuals\n    residuals = (y - m) / s\n    # Chi-square statistic\n    chi_sq = float(np.sum(residuals ** 2))\n    # Degrees of freedom\n    nu = int(len(y) - k)\n    if nu <= 0:\n        raise ValueError(\"Degrees of freedom must be positive (n - k > 0).\")\n    # p-value (upper-tail probability)\n    p_value = float(chi2.sf(chi_sq, df=nu))\n    # Decision\n    passed = bool(p_value >= alpha)\n    return residuals, chi_sq, p_value, passed, nu\n\ndef round_float(x, ndigits=6):\n    return float(np.round(x, ndigits))\n\ndef round_list(lst, ndigits=6):\n    return [round_float(float(v), ndigits) for v in lst]\n\ndef format_element_no_space(el):\n    \"\"\"\n    Recursively format nested lists and primitive types into a string\n    without spaces, rounding floats to six decimals.\n    \"\"\"\n    if isinstance(el, list):\n        return \"[\" + \",\".join(format_element_no_space(x) for x in el) + \"]\"\n    elif isinstance(el, (float, np.floating)):\n        return f\"{float(el):.6f}\"\n    elif isinstance(el, (int, np.integer)):\n        return str(int(el))\n    elif isinstance(el, bool):\n        return \"True\" if el else \"False\"\n    else:\n        # Attempt to handle numpy arrays by converting to list\n        if isinstance(el, np.ndarray):\n            return format_element_no_space(el.tolist())\n        # Fallback: convert to string (should not occur in this problem)\n        return str(el)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            \"alpha\": 0.05,\n            \"datasets\": [\n                {\n                    \"y\": np.array([1.0, 0.8, 1.2, 0.9, 1.1]),\n                    \"m\": np.array([1.0, 0.75, 1.15, 0.95, 1.05]),\n                    \"s\": np.array([0.1, 0.1, 0.1, 0.1, 0.1]),\n                    \"k\": 1,\n                },\n                {\n                    \"y\": np.array([2.0, 1.8, 2.1, 1.9]),\n                    \"m\": np.array([2.05, 1.85, 2.05, 1.95]),\n                    \"s\": np.array([0.1, 0.1, 0.1, 0.1]),\n                    \"k\": 1,\n                },\n            ],\n        },\n        # Case 2\n        {\n            \"alpha\": 0.05,\n            \"datasets\": [\n                {\n                    \"y\": np.array([5.0, 5.3, 4.8, 5.2]),\n                    \"m\": np.array([5.1, 5.2, 5.0, 5.1]),\n                    \"s\": np.array([0.2, 0.3, 0.2, 0.4]),\n                    \"k\": 2,\n                },\n                {\n                    \"y\": np.array([3.0, 3.2, 2.9]),\n                    \"m\": np.array([3.1, 3.0, 3.0]),\n                    \"s\": np.array([0.2, 0.2, 0.1]),\n                    \"k\": 1,\n                },\n            ],\n        },\n        # Case 3\n        {\n            \"alpha\": 0.05,\n            \"datasets\": [\n                {\n                    \"y\": np.array([0.9, 1.1, 0.95, 1.05, 1.0]),\n                    \"m\": np.array([1.2, 1.2, 1.2, 1.2, 1.2]),\n                    \"s\": np.array([0.05, 0.05, 0.05, 0.05, 0.05]),\n                    \"k\": 0,\n                },\n                {\n                    \"y\": np.array([10.0, 9.5, 10.5, 9.8]),\n                    \"m\": np.array([10.0, 10.0, 10.0, 10.0]),\n                    \"s\": np.array([0.2, 0.2, 0.2, 0.2]),\n                    \"k\": 0,\n                },\n            ],\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        alpha = case[\"alpha\"]\n        case_results = []\n        chi_sq_global = 0.0\n        nu_global = 0\n        for ds in case[\"datasets\"]:\n            y = ds[\"y\"]\n            m = ds[\"m\"]\n            s = ds[\"s\"]\n            k = ds[\"k\"]\n            residuals, chi_sq, p_value, passed, nu = compute_dataset_metrics(y, m, s, k, alpha)\n            chi_sq_global += chi_sq\n            nu_global += nu\n            # Round values\n            residuals_rounded = round_list(residuals.tolist(), ndigits=6)\n            chi_sq_rounded = round_float(chi_sq, ndigits=6)\n            p_value_rounded = round_float(p_value, ndigits=6)\n            case_results.append([residuals_rounded, chi_sq_rounded, p_value_rounded, passed])\n        # Global aggregation\n        p_global = float(chi2.sf(chi_sq_global, df=nu_global))\n        pass_global = bool(p_global >= alpha)\n        chi_sq_global_rounded = round_float(chi_sq_global, ndigits=6)\n        p_global_rounded = round_float(p_global, ndigits=6)\n        case_results.append([chi_sq_global_rounded, p_global_rounded, pass_global])\n        results.append(case_results)\n\n    # Final print statement in the exact required format: no spaces anywhere.\n    print(format_element_no_space(results))\n\nsolve()\n```"
        },
        {
            "introduction": "An effective computational modeling cycle does not end with validation; it informs the next round of experimentation. This practice introduces the concept of optimal experimental design, where we use sensitivity analysis to predict which future experiment will provide the most information to refine our model. By calculating how parameter uncertainties are expected to shrink in response to different potential measurements, you can learn to prioritize experiments and maximize the return on your experimental investment .",
            "id": "3845672",
            "problem": "You are modeling an equilibrium receptor–ligand binding assay commonly used in computational chemical biology to infer biophysical parameters from experimental data. The computational model predicts a signal $y$ (in $\\mathrm{mol}\\cdot\\mathrm{L}^{-1}$) at a given free ligand concentration $L$ (in $\\mathrm{mol}\\cdot\\mathrm{L}^{-1}$) by assuming a Hill-type occupancy model. The predicted signal is given by the deterministic mapping $f(\\boldsymbol{\\theta}; L)$ defined as $y = \\alpha \\cdot \\phi(L; K_d, n)$, where $\\boldsymbol{\\theta} = (K_d, n, \\alpha)$, with $K_d$ the dissociation constant (in $\\mathrm{mol}\\cdot\\mathrm{L}^{-1}$), $n$ the Hill coefficient (dimensionless), and $\\alpha$ a proportionality constant (in $\\mathrm{mol}\\cdot\\mathrm{L}^{-1}$). The fractional occupancy $\\phi$ is\n$$\n\\phi(L; K_d, n) = \\frac{\\left(\\dfrac{L}{K_d}\\right)^n}{1 + \\left(\\dfrac{L}{K_d}\\right)^n}.\n$$\nEach experimental constraint corresponds to a single measurement at a specified $L$, yielding a noisy observation $y_i = f(\\boldsymbol{\\theta}; L_i) + \\varepsilon_i$, where $\\varepsilon_i$ is zero-mean additive noise that is independently and identically distributed and modeled as Gaussian with known standard deviation $\\sigma_i$ (in $\\mathrm{mol}\\cdot\\mathrm{L}^{-1}$). You are provided a prior Gaussian uncertainty on $\\boldsymbol{\\theta}$ specified by a symmetric positive definite covariance matrix $\\boldsymbol{\\Sigma}_0$.\n\nStarting from the definitions of sensitivity and Bayesian linearized Gaussian inference, do the following for each candidate constraint:\n- Compute the sensitivity coefficients as the gradient of the model prediction with respect to parameters evaluated at the nominal $\\boldsymbol{\\theta}$,\n$$\n\\mathbf{S}_i = \\nabla_{\\boldsymbol{\\theta}} f(\\boldsymbol{\\theta}; L_i) \\in \\mathbb{R}^3,\n$$\nconsistent with the definition of sensitivity in parameter estimation under differentiable models.\n- Treating the single measurement constraint as an incremental information update to the prior, evaluate the expected posterior covariance for $\\boldsymbol{\\theta}$ under the linearized Gaussian approximation and extract the expected posterior variances of a specified subset of critical parameters indexed by a set $\\mathcal{C}$.\n- Define the utility of a constraint as the expected fractional reduction of the sum of variances of the critical parameters, divided by the given experimental cost $c_i$ (dimensionless),\n$$\nU_i = \\frac{\\left(\\sum_{j \\in \\mathcal{C}} \\operatorname{Var}_\\text{prior}[\\theta_j] - \\sum_{j \\in \\mathcal{C}} \\operatorname{Var}_\\text{post}^{(i)}[\\theta_j]\\right)}{\\sum_{j \\in \\mathcal{C}} \\operatorname{Var}_\\text{prior}[\\theta_j]} \\cdot \\frac{1}{c_i}.\n$$\n- Prioritize (rank) constraints in descending order of $U_i$. Break ties by ascending constraint index.\n\nBase your derivation and algorithm only on fundamental principles:\n- Use the definition of sensitivity as partial derivatives of the model output with respect to parameters.\n- Use Bayesian inference for Gaussian priors and Gaussian likelihoods under linearization at $\\boldsymbol{\\theta}$, where adding a single linear constraint contributes a rank-one update to the parameter precision, reflecting the Fisher information contribution from $\\mathbf{S}_i$ and $\\sigma_i$.\n\nYour program must implement the above process numerically using finite differences to approximate $\\mathbf{S}_i$ and the linear-Gaussian update for the posterior covariance. All inputs are provided below in fixed test cases. Do not require any user input. Use the following test suite. All concentrations and signals must be handled in $\\mathrm{mol}\\cdot\\mathrm{L}^{-1}$; angles are not involved; the outputs are unitless indices.\n\nTest Case $1$:\n- Nominal parameters $\\boldsymbol{\\theta} = (K_d, n, \\alpha) = \\left(50\\times 10^{-9}, 2.0, 100\\times 10^{-9}\\right)$.\n- Prior covariance\n$$\n\\boldsymbol{\\Sigma}_0 = \\begin{bmatrix}\n2.25\\times 10^{-16} & 0 & 3.0\\times 10^{-17}\\\\\n0 & 9.0\\times 10^{-2} & 0\\\\\n3.0\\times 10^{-17} & 0 & 4.0\\times 10^{-16}\n\\end{bmatrix}.\n$$\n- Constraints $(L_i, \\sigma_i, c_i)$:\n  - Index $0$: $\\left(10\\times 10^{-9}, 5\\times 10^{-9}, 1.0\\right)$\n  - Index $1$: $\\left(50\\times 10^{-9}, 5\\times 10^{-9}, 1.2\\right)$\n  - Index $2$: $\\left(200\\times 10^{-9}, 5\\times 10^{-9}, 1.0\\right)$\n  - Index $3$: $\\left(1000\\times 10^{-9}, 10\\times 10^{-9}, 2.0\\right)$\n- Critical index set $\\mathcal{C} = \\{0, 1\\}$.\n\nTest Case $2$:\n- Nominal parameters $\\boldsymbol{\\theta} = \\left(100\\times 10^{-9}, 1.5, 50\\times 10^{-9}\\right)$.\n- Prior covariance\n$$\n\\boldsymbol{\\Sigma}_0 = \\operatorname{diag}\\left(1.6\\times 10^{-15}, 2.5\\times 10^{-1}, 1.0\\times 10^{-16}\\right).\n$$\n- Constraints:\n  - Index $0$: $\\left(100\\times 10^{-9}, 50\\times 10^{-9}, 1.0\\right)$\n  - Index $1$: $\\left(10\\times 10^{-9}, 50\\times 10^{-9}, 1.0\\right)$\n  - Index $2$: $\\left(1000\\times 10^{-9}, 50\\times 10^{-9}, 1.0\\right)$\n- Critical index set $\\mathcal{C} = \\{0\\}$.\n\nTest Case $3$:\n- Nominal parameters $\\boldsymbol{\\theta} = \\left(30\\times 10^{-9}, 3.0, 150\\times 10^{-9}\\right)$.\n- Prior covariance\n$$\n\\boldsymbol{\\Sigma}_0 = \\operatorname{diag}\\left(1.0\\times 10^{-16}, 3.6\\times 10^{-1}, 9.0\\times 10^{-16}\\right).\n$$\n- Constraints:\n  - Index $0$: $\\left(5\\times 10^{-9}, 4\\times 10^{-9}, 1.0\\right)$\n  - Index $1$: $\\left(30\\times 10^{-9}, 4\\times 10^{-9}, 1.0\\right)$\n  - Index $2$: $\\left(300\\times 10^{-9}, 4\\times 10^{-9}, 1.0\\right)$\n- Critical index set $\\mathcal{C} = \\{1\\}$.\n\nTest Case $4$:\n- Nominal parameters $\\boldsymbol{\\theta} = \\left(60\\times 10^{-9}, 1.0, 200\\times 10^{-9}\\right)$.\n- Prior covariance\n$$\n\\boldsymbol{\\Sigma}_0 = \\operatorname{diag}\\left(2.5\\times 10^{-17}, 1.0\\times 10^{-2}, 1.0\\times 10^{-14}\\right).\n$$\n- Constraints:\n  - Index $0$: $\\left(1\\times 10^{-9}, 10\\times 10^{-9}, 0.5\\right)$\n  - Index $1$: $\\left(60\\times 10^{-9}, 10\\times 10^{-9}, 2.0\\right)$\n  - Index $2$: $\\left(1000\\times 10^{-9}, 10\\times 10^{-9}, 1.0\\right)$\n- Critical index set $\\mathcal{C} = \\{2\\}$.\n\nImplementation requirements:\n- Approximate $\\mathbf{S}_i$ using symmetric finite differences around the nominal $\\boldsymbol{\\theta}$. Use perturbation magnitudes that are small relative to parameter scales but numerically stable.\n- For each single constraint, compute the expected posterior covariance under linearization and Gaussian assumptions, and then compute $U_i$ for the given $\\mathcal{C}$.\n- Produce, for each test case, the ranked list of constraint indices in descending order of $U_i$ (ties broken by ascending index).\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is the ranked list of constraint indices for the corresponding test case, for example, $\\left[\\left[\\cdots\\right],\\left[\\cdots\\right],\\left[\\cdots\\right]\\right]$.",
            "solution": "The objective of this problem is to prioritize a set of candidate experimental measurements, termed constraints, for a receptor-ligand binding assay. The prioritization is based on the principle of maximizing the information gained from an experiment, quantified by a utility function. The utility measures the expected reduction in uncertainty for a critical subset of model parameters, normalized by the experimental cost. The analysis is performed within the framework of Bayesian inference, using a linearized approximation of the physical model.\n\nThe methodological derivation proceeds as follows, based on a rigorous application of fundamental principles of statistical inference and sensitivity analysis.\n\nThe system is described by a deterministic model $f(\\boldsymbol{\\theta}; L)$ that predicts an observable signal $y$ as a function of the free ligand concentration $L$ and a vector of biophysical parameters $\\boldsymbol{\\theta} = (K_d, n, \\alpha)$. The parameters represent the dissociation constant, the Hill coefficient, and a proportionality constant, respectively. The model is given by:\n$$\ny = f(\\boldsymbol{\\theta}; L) = \\alpha \\cdot \\phi(L; K_d, n)\n$$\nwhere $\\phi$ is the Hill-type fractional occupancy function:\n$$\n\\phi(L; K_d, n) = \\frac{\\left(\\dfrac{L}{K_d}\\right)^n}{1 + \\left(\\dfrac{L}{K_d}\\right)^n}\n$$\nAll concentrations ($y$, $L$, $K_d$, $\\alpha$) are in units of $\\mathrm{mol}\\cdot\\mathrm{L}^{-1}$, and the Hill coefficient $n$ is dimensionless.\n\nThe problem is cast in a Bayesian framework. Our prior knowledge about the parameters $\\boldsymbol{\\theta}$ is encoded in a multivariate Gaussian probability distribution with mean $\\boldsymbol{\\theta}_0$ (the nominal parameter values) and covariance matrix $\\boldsymbol{\\Sigma}_0$. We write this as $\\boldsymbol{\\theta} \\sim \\mathcal{N}(\\boldsymbol{\\theta}_0, \\boldsymbol{\\Sigma}_0)$. Each experimental constraint $i$ consists of a single measurement $y_i$ at a known ligand concentration $L_i$. This measurement is noisy, described by the model $y_i = f(\\boldsymbol{\\theta}; L_i) + \\varepsilon_i$, where the noise $\\varepsilon_i$ is assumed to be an independent, zero-mean Gaussian random variable with a known variance $\\sigma_i^2$, i.e., $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_i^2)$. The likelihood of observing $y_i$ given the parameters $\\boldsymbol{\\theta}$ is therefore $p(y_i|\\boldsymbol{\\theta}) \\sim \\mathcal{N}(f(\\boldsymbol{\\theta}; L_i), \\sigma_i^2)$.\n\nTo update our belief about $\\boldsymbol{\\theta}$, we would ideally compute the posterior distribution $p(\\boldsymbol{\\theta}|y_i) \\propto p(y_i|\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})$. However, because $f(\\boldsymbol{\\theta}; L_i)$ is a nonlinear function of $\\boldsymbol{\\theta}$, the posterior is not Gaussian and is difficult to compute analytically. The problem specifies using a linearized Gaussian approximation. We linearize the model $f(\\boldsymbol{\\theta}; L_i)$ around the nominal parameter vector $\\boldsymbol{\\theta}_0$ using a first-order Taylor expansion:\n$$\nf(\\boldsymbol{\\theta}; L_i) \\approx f(\\boldsymbol{\\theta}_0; L_i) + \\nabla_{\\boldsymbol{\\theta}} f(\\boldsymbol{\\theta}_0; L_i)^T (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0)\n$$\nThe gradient vector $\\mathbf{S}_i = \\nabla_{\\boldsymbol{\\theta}} f(\\boldsymbol{\\theta}_0; L_i)$ is a $3 \\times 1$ column vector of sensitivity coefficients, which quantifies how the model output changes with infinitesimal changes in each parameter. The observation model becomes approximately linear in $\\boldsymbol{\\theta}$:\n$$\ny_i - f(\\boldsymbol{\\theta}_0; L_i) \\approx \\mathbf{S}_i^T (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0) + \\varepsilon_i\n$$\nUnder this approximation, a Gaussian prior on $\\boldsymbol{\\theta}$ and a Gaussian likelihood for $y_i$ result in a Gaussian posterior for $\\boldsymbol{\\theta}$. The update rule for the covariance matrix is derived from the update rule for the precision matrix (the inverse of the covariance matrix). The posterior precision matrix, $\\mathbf{P}_\\text{post}^{(i)}$, is the sum of the prior precision matrix, $\\mathbf{P}_0 = \\boldsymbol{\\Sigma}_0^{-1}$, and the precision gained from the measurement, which is equivalent to the Fisher Information of the measurement. For a single linearized Gaussian measurement, this information is a rank-one matrix:\n$$\n\\mathbf{P}_\\text{post}^{(i)} = \\mathbf{P}_0 + \\frac{1}{\\sigma_i^2} \\mathbf{S}_i \\mathbf{S}_i^T = \\boldsymbol{\\Sigma}_0^{-1} + \\frac{1}{\\sigma_i^2} \\mathbf{S}_i \\mathbf{S}_i^T\n$$\nThe expected posterior covariance matrix, $\\boldsymbol{\\Sigma}_\\text{post}^{(i)}$, is the inverse of the posterior precision matrix: $\\boldsymbol{\\Sigma}_\\text{post}^{(i)} = (\\mathbf{P}_\\text{post}^{(i)})^{-1}$. A direct inversion can be avoided by applying the Sherman-Morrison identity, yielding the more numerically stable Kalman filter covariance update form:\n$$\n\\boldsymbol{\\Sigma}_\\text{post}^{(i)} = \\boldsymbol{\\Sigma}_0 - \\boldsymbol{\\Sigma}_0 \\mathbf{S}_i \\left( \\sigma_i^2 + \\mathbf{S}_i^T \\boldsymbol{\\Sigma}_0 \\mathbf{S}_i \\right)^{-1} \\mathbf{S}_i^T \\boldsymbol{\\Sigma}_0\n$$\nThis formula computes the posterior covariance after incorporating the information from the $i$-th measurement. The term $(\\sigma_i^2 + \\mathbf{S}_i^T \\boldsymbol{\\Sigma}_0 \\mathbf{S}_i)$ is a scalar, making the computation efficient.\n\nThe sensitivity vector $\\mathbf{S}_i = \\nabla_{\\boldsymbol{\\theta}} f(\\boldsymbol{\\theta}_0; L_i)$ is numerically approximated using the symmetric finite difference method. For each parameter $\\theta_j$ (where $j \\in \\{0, 1, 2\\}$ corresponds to $K_d, n, \\alpha$), the partial derivative is:\n$$\nS_{i,j} = \\frac{\\partial f}{\\partial \\theta_j} \\bigg|_{\\boldsymbol{\\theta}_0, L_i} \\approx \\frac{f(\\boldsymbol{\\theta}_0 + h_j \\mathbf{e}_j; L_i) - f(\\boldsymbol{\\theta}_0 - h_j \\mathbf{e}_j; L_i)}{2h_j}\n$$\nwhere $\\mathbf{e}_j$ is the standard basis vector for the $j$-th parameter and $h_j$ is a small-step perturbation, chosen as a small fraction of the nominal parameter value, e.g., $h_j = \\epsilon_h |\\theta_{0,j}|$, to ensure numerical stability.\n\nThe utility $U_i$ of each constraint $i$ is defined as the cost-normalized expected fractional reduction in the sum of variances of a critical set of parameters, indexed by $\\mathcal{C}$. The variance of a parameter $\\theta_j$ is the $j$-th diagonal element of the covariance matrix. The utility is calculated as:\n$$\nU_i = \\frac{\\Delta V_i}{V_\\text{prior}} \\cdot \\frac{1}{c_i}\n$$\nwhere $V_\\text{prior} = \\sum_{j \\in \\mathcal{C}} (\\boldsymbol{\\Sigma}_0)_{jj}$ is the sum of prior variances of the critical parameters, and $\\Delta V_i = V_\\text{prior} - V_\\text{post}^{(i)}$ is the reduction in this sum, with $V_\\text{post}^{(i)} = \\sum_{j \\in \\mathcal{C}} (\\boldsymbol{\\Sigma}_\\text{post}^{(i)})_{jj}$.\n\nThe algorithmic procedure to rank the constraints is as follows:\n1. For each test case, retrieve the nominal parameters $\\boldsymbol{\\theta}_0$, prior covariance $\\boldsymbol{\\Sigma}_0$, critical index set $\\mathcal{C}$, and the list of constraints $(L_i, \\sigma_i, c_i)$.\n2. Calculate the sum of prior variances for the critical parameters, $V_\\text{prior} = \\sum_{j \\in \\mathcal{C}} (\\boldsymbol{\\Sigma}_0)_{jj}$.\n3. For each constraint $i$:\n    a. Compute the sensitivity vector $\\mathbf{S}_i$ at $(\\boldsymbol{\\theta}_0, L_i)$ using symmetric finite differences.\n    b. Calculate the expected posterior covariance matrix $\\boldsymbol{\\Sigma}_\\text{post}^{(i)}$ using the Kalman update formula.\n    c. Calculate the sum of posterior variances for the critical parameters, $V_\\text{post}^{(i)} = \\sum_{j \\in \\mathcal{C}} (\\boldsymbol{\\Sigma}_\\text{post}^{(i)})_{jj}$.\n    d. Compute the utility $U_i = \\frac{V_\\text{prior} - V_\\text{post}^{(i)}}{V_\\text{prior} \\cdot c_i}$.\n4. Store pairs of $(U_i, i)$ for all constraints.\n5. Sort these pairs in descending order of utility $U_i$. If two utilities are equal, break the tie by sorting their corresponding indices $i$ in ascending order.\n6. Extract the sorted indices to form the final ranked list for the test case.\n7. Repeat for all test cases and format the final output.\nThis procedure provides a principled, quantitative ranking of experiments to guide data acquisition in computational chemical biology.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the experiment prioritization problem for all test cases.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"theta\": np.array([50e-9, 2.0, 100e-9]),\n            \"Sigma0\": np.array([\n                [2.25e-16, 0.0, 3.0e-17],\n                [0.0, 9.0e-2, 0.0],\n                [3.0e-17, 0.0, 4.0e-16]\n            ]),\n            \"constraints\": [\n                (10e-9, 5e-9, 1.0),\n                (50e-9, 5e-9, 1.2),\n                (200e-9, 5e-9, 1.0),\n                (1000e-9, 10e-9, 2.0)\n            ],\n            \"critical_indices\": {0, 1}\n        },\n        {\n            \"theta\": np.array([100e-9, 1.5, 50e-9]),\n            \"Sigma0\": np.diag([1.6e-15, 2.5e-1, 1.0e-16]),\n            \"constraints\": [\n                (100e-9, 50e-9, 1.0),\n                (10e-9, 50e-9, 1.0),\n                (1000e-9, 50e-9, 1.0)\n            ],\n            \"critical_indices\": {0}\n        },\n        {\n            \"theta\": np.array([30e-9, 3.0, 150e-9]),\n            \"Sigma0\": np.diag([1.0e-16, 3.6e-1, 9.0e-16]),\n            \"constraints\": [\n                (5e-9, 4e-9, 1.0),\n                (30e-9, 4e-9, 1.0),\n                (300e-9, 4e-9, 1.0)\n            ],\n            \"critical_indices\": {1}\n        },\n        {\n            \"theta\": np.array([60e-9, 1.0, 200e-9]),\n            \"Sigma0\": np.diag([2.5e-17, 1.0e-2, 1.0e-14]),\n            \"constraints\": [\n                (1e-9, 10e-9, 0.5),\n                (60e-9, 10e-9, 2.0),\n                (1000e-9, 10e-9, 1.0)\n            ],\n            \"critical_indices\": {2}\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        theta0 = case[\"theta\"]\n        Sigma0 = case[\"Sigma0\"]\n        constraints = case[\"constraints\"]\n        critical_indices = case[\"critical_indices\"]\n\n        utilities = []\n\n        # Prior variance of critical parameters\n        var_prior_sum = 0.0\n        for j in critical_indices:\n            var_prior_sum += Sigma0[j, j]\n        \n        # Guard against zero prior variance, which implies no uncertainty to reduce.\n        if var_prior_sum == 0.0:\n            # If there's no prior variance, no experiment can reduce it. All utilities are 0.\n            # Rank by index as per tie-breaking rule.\n            ranked_indices = sorted(range(len(constraints)))\n            all_results.append(ranked_indices)\n            continue\n\n        for i, (L, sigma, c) in enumerate(constraints):\n            # 1. Compute sensitivity vector S_i using symmetric finite differences\n            S_i = compute_sensitivity(theta0, L)\n\n            # 2. Compute posterior covariance Sigma_post\n            # Using the numerically stable Kalman update form:\n            # Sigma_post = Sigma0 - K * S_i_T * Sigma0, where K = Sigma0 * S_i * (sigma^2 + S_i_T * Sigma0 * S_i)^-1\n            # Which simplifies to: Sigma_post = Sigma0 - (Sigma0 @ S_i) @ (S_i_T @ Sigma0) / (sigma^2 + S_i_T @ Sigma0 @ S_i)\n            # where S_i is a column vector\n            S_i_col = S_i.reshape(-1, 1)\n            S_i_T = S_i.reshape(1, -1)\n            \n            kalman_gain_numerator = Sigma0 @ S_i_col\n            kalman_gain_denominator = sigma**2 + S_i_T @ Sigma0 @ S_i_col\n            \n            # kalman_gain_denominator is a 1x1 matrix, extract the scalar\n            if kalman_gain_denominator[0, 0] == 0:\n                Sigma_post = Sigma0\n            else:\n                kalman_gain = kalman_gain_numerator / kalman_gain_denominator[0, 0]\n                Sigma_post = Sigma0 - kalman_gain @ S_i_T @ Sigma0\n\n            # 3. Compute posterior variance sum\n            var_post_sum = 0.0\n            for j in critical_indices:\n                var_post_sum += Sigma_post[j, j]\n\n            # 4. Compute utility U_i\n            # Avoid division by zero for cost\n            if c <= 0:\n                utility = -np.inf # Effectively prioritizes this last\n            else:\n                fractional_reduction = (var_prior_sum - var_post_sum) / var_prior_sum\n                utility = fractional_reduction / c\n            \n            utilities.append({'utility': utility, 'index': i})\n\n        # 5. Rank constraints\n        # Sort by utility (descending) and then index (ascending) for ties\n        utilities.sort(key=lambda x: (-x['utility'], x['index']))\n        ranked_indices = [item['index'] for item in utilities]\n        all_results.append(ranked_indices)\n        \n    print(str(all_results).replace(\" \", \"\"))\n\n\ndef model(theta, L):\n    \"\"\"\n    Computes the predicted signal y based on the Hill-type occupancy model.\n    theta = (K_d, n, alpha)\n    \"\"\"\n    Kd, n, alpha = theta\n    if Kd <= 0 or L < 0: # Physical constraints\n        return 0.0\n\n    # To prevent overflow for large n and L/Kd > 1\n    # phi = (L/Kd)^n / (1 + (L/Kd)^n) = 1 / (1 + (Kd/L)^n)\n    ratio = L / Kd\n    if ratio > 1.0:\n        inv_ratio = Kd / L\n        # n can be float, use np.power\n        phi = 1.0 / (1.0 + np.power(inv_ratio, n))\n    else:\n        # n can be float, use np.power\n        term = np.power(ratio, n)\n        phi = term / (1.0 + term)\n    \n    return alpha * phi\n\n\ndef compute_sensitivity(theta, L, rel_step=1e-8):\n    \"\"\"\n    Computes the sensitivity vector S = grad_theta f(theta, L) using \n    symmetric finite differences.\n    \"\"\"\n    S = np.zeros_like(theta)\n    for j in range(len(theta)):\n        theta_j = theta[j]\n        # Choose a step size that is robust to theta_j being zero.\n        h = rel_step * (np.abs(theta_j) + 1e-9)\n        \n        theta_plus = np.copy(theta)\n        theta_plus[j] += h\n        \n        theta_minus = np.copy(theta)\n        theta_minus[j] -= h\n        \n        f_plus = model(theta_plus, L)\n        f_minus = model(theta_minus, L)\n        \n        S[j] = (f_plus - f_minus) / (2.0 * h)\n    return S\n\nsolve()\n\n```"
        }
    ]
}