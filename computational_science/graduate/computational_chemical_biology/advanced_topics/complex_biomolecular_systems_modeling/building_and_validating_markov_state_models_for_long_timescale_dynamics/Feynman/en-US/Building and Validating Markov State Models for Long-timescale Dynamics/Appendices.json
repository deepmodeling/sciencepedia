{
    "hands_on_practices": [
        {
            "introduction": "Before building and validating complex models, it is essential to understand the fundamental mathematical properties of the core object: the transition matrix $T(\\tau)$. This exercise will guide you through implementing, from first principles, the verification of key characteristics such as row-stochasticity, stationarity, and long-term convergence. Mastering these concepts ensures you can perform basic sanity checks on any Markov State Model you encounter or build .",
            "id": "3838051",
            "problem": "Consider a finite-state Markov state model (MSM) constructed for conformational dynamics in computational chemical biology at a fixed lag time $\\tau$, represented by a row-stochastic transition matrix $T(\\tau)$ of size $n \\times n$ with nonnegative entries. By definition, each row of $T(\\tau)$ sums to $1$, all entries are nonnegative, and the evolution of state probabilities is given by $p(t+\\tau) = p(t) T(\\tau)$ for a row vector $p(t)$. A stationary distribution $\\pi$ is a row vector satisfying $\\pi T(\\tau) = \\pi$ and $\\sum_{i=1}^{n} \\pi_i = 1$, with $\\pi_i \\ge 0$. For an irreducible and aperiodic chain (primitive $T(\\tau)$), the Perron–Frobenius theorem guarantees the existence of a unique stationary distribution with strictly positive components and the convergence \n$$\n\\lim_{k \\to \\infty} T(\\tau)^k = \\mathbf{1} \\pi,\n$$\nwhere $\\mathbf{1}$ is the column vector of ones and $\\pi$ is the stationary row vector.\n\nUsing only these core definitions and facts, write a complete program that, for each test case below, computes the stationary distribution $\\pi$ from $T(\\tau)$, verifies (i) row normalization, (ii) entrywise nonnegativity (positivity), (iii) stationarity, and (iv) convergence of $T(\\tau)^k$ toward $\\mathbf{1}\\pi$ for a specified power $k$. Your program must implement the following steps from first principles:\n- Compute $\\pi$ by solving the linear system $(T(\\tau)^\\top - I)\\,x = 0$ with the normalization constraint $\\sum_i x_i = 1$.\n- Verify row normalization by checking that each row sum is within a tolerance $\\varepsilon_{\\text{norm}}$ of $1$ in the maximum absolute deviation sense.\n- Verify positivity by checking that the minimum entry of $T(\\tau)$ is at least $-\\varepsilon_{\\text{pos}}$.\n- Verify stationarity by computing the residual $\\lVert \\pi T(\\tau) - \\pi \\rVert_{\\infty}$ and comparing against $\\varepsilon_{\\text{stat}}$.\n- Verify convergence by computing $\\lVert T(\\tau)^k - \\mathbf{1}\\pi \\rVert_{\\infty}$ and comparing against a case-specific convergence tolerance $\\varepsilon_{\\text{conv}}$.\n\nUse the following numerical tolerances that are shared across all cases unless otherwise specified:\n- Row normalization tolerance $\\varepsilon_{\\text{norm}} = 10^{-12}$.\n- Positivity tolerance $\\varepsilon_{\\text{pos}} = 10^{-15}$.\n- Stationarity tolerance $\\varepsilon_{\\text{stat}} = 10^{-10}$.\n- Convergence tolerances $\\varepsilon_{\\text{conv}}$ are specified per test case below.\n\nTest suite (all numbers are real and dimensionless):\n1. Case A (a reversible $3$-state MSM with strong self-retention):\n   - Matrix\n     $$\n     T_A = \\begin{bmatrix}\n     0.86 & 0.10 & 0.04 \\\\\n     0.16666666666666666 & 0.7333333333333334 & 0.10 \\\\\n     0.10 & 0.15 & 0.75\n     \\end{bmatrix}.\n     $$\n   - Power $k_A = 200$.\n   - Convergence tolerance $\\varepsilon_{\\text{conv},A} = 10^{-6}$.\n2. Case B (a $2$-state MSM with slow mixing due to weak coupling):\n   - Matrix\n     $$\n     T_B = \\begin{bmatrix}\n     0.995 & 0.005 \\\\\n     0.02 & 0.98\n     \\end{bmatrix}.\n     $$\n   - Power $k_B = 800$.\n   - Convergence tolerance $\\varepsilon_{\\text{conv},B} = 10^{-8}$.\n3. Case C (a $4$-state MSM with two weakly coupled basins):\n   - Matrix\n     $$\n     T_C = \\begin{bmatrix}\n     0.94 & 0.05 & 0.01 & 0.0 \\\\\n     0.04 & 0.93 & 0.0 & 0.03 \\\\\n     0.005 & 0.0 & 0.985 & 0.01 \\\\\n     0.0 & 0.02 & 0.03 & 0.95\n     \\end{bmatrix}.\n     $$\n   - Power $k_C = 2000$.\n   - Convergence tolerance $\\varepsilon_{\\text{conv},C} = 10^{-8}$.\n\nYour program must:\n- For each case, compute $\\pi$, then evaluate the four boolean checks: row normalization, positivity, stationarity, and convergence, each returning true if the respective criterion is satisfied within its tolerance.\n- Aggregate the results across the three cases into a single flat list of $12$ boolean values, in the order:\n  $[\\text{norm}_A, \\text{pos}_A, \\text{stat}_A, \\text{conv}_A, \\text{norm}_B, \\text{pos}_B, \\text{stat}_B, \\text{conv}_B, \\text{norm}_C, \\text{pos}_C, \\text{stat}_C, \\text{conv}_C]$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[{\\rm true},{\\rm false},\\dots]$ using the programming language’s boolean literals). No other text should be printed.",
            "solution": "The problem requires a numerical validation of key properties of several Markov state models (MSMs) represented by their transition matrices $T(\\tau)$. The validation process involves computing the stationary distribution $\\pi$ and then verifying four fundamental properties: row normalization, non-negativity, stationarity of $\\pi$, and the convergence of high powers of the transition matrix to its theoretical limit. The entire procedure is to be implemented from first principles as a self-contained program.\n\nThe core of the problem rests on the properties of a row-stochastic transition matrix $T$ for an irreducible and aperiodic Markov chain. The Perron-Frobenius theorem guarantees a unique stationary distribution $\\pi$, which is a row vector with strictly positive entries ($\\pi_i > 0$) that satisfies the following conditions:\n1. Stationarity: $\\pi T = \\pi$\n2. Normalization: $\\sum_i \\pi_i = 1$\n\nThe theorem also guarantees the convergence of the matrix power $T^k$ to a rank-one matrix formed by the stationary distribution: $$ \\lim_{k \\to \\infty} T^k = \\mathbf{1}\\pi, $$ where $\\mathbf{1}$ is a column vector of ones. Our task is to numerically verify these properties.\n\nThe methodological steps are as follows:\n\nFirst, we compute the stationary distribution $\\pi$. The stationarity-eigenvalue equation $\\pi T = \\pi$ can be transposed to $T^\\top \\pi^\\top = \\pi^\\top$. Let $x = \\pi^\\top$ be the column vector representation of the stationary distribution. The equation becomes $T^\\top x = x$, or, rearranged, $(T^\\top - I)x = 0$, where $I$ is the identity matrix. This is a homogeneous linear system. For an irreducible Markov chain, the eigenvalue $1$ has a geometric multiplicity of $1$, meaning the null space of $(T^\\top - I)$ is one-dimensional. Consequently, the system $(T^\\top - I)x = 0$ has infinitely many solutions, all of which are scalar multiples of a single eigenvector. To find the unique solution corresponding to the stationary distribution, we must impose the normalization constraint $\\sum_i x_i = \\sum_i \\pi_i = 1$. A standard numerical technique to solve this is to augment the system of equations. We can replace one of the linearly dependent equations in $(T^\\top - I)x = 0$ with the normalization equation. Let $A = T^\\top - I$. We construct a new matrix $A'$ by replacing the last row of $A$ with a row of all ones. We also construct a new right-hand side vector $b'$ which is zero everywhere except for a one in the last position. The resulting linear system $A'x=b'$ is non-singular and can be solved uniquely for $x = \\pi^\\top$.\n\nSecond, we perform the four specified verification checks for each given transition matrix $T$, its corresponding power $k$, and a set of numerical tolerances.\n\n(i) Row Normalization Verification: A matrix $T$ is row-stochastic if the sum of elements in each row is $1$. This reflects the principle that from any given state, the system must transition to some other state in the model, so the total probability of transitioning out of a state is $1$. We check this by computing the maximum absolute deviation of the row sums from $1$: $\\max_i |\\sum_j T_{ij} - 1|$. This value must be less than or equal to the tolerance $\\varepsilon_{\\text{norm}} = 10^{-12}$.\n\n(ii) Positivity Verification: The entries $T_{ij}$ of a transition matrix represent probabilities and must therefore be non-negative. To account for potential floating-point inaccuracies, we verify this by checking that the minimum entry in the matrix is not substantially negative. The condition is $\\min_{i,j} T_{ij} \\ge -\\varepsilon_{\\text{pos}}$, with $\\varepsilon_{\\text{pos}} = 10^{-15}$.\n\n(iii) Stationarity Verification: This check confirms that the numerically computed distribution $\\pi$ is indeed a stationary distribution for the given matrix $T$. We compute the residual vector $\\pi T - \\pi$. A perfect stationary distribution would yield a zero vector. We quantify the deviation by computing the infinity norm of this residual, $\\lVert \\pi T - \\pi \\rVert_{\\infty} = \\max_i |(\\pi T)_i - \\pi_i|$, and checking if it is less than or equal to the stationarity tolerance $\\varepsilon_{\\text{stat}} = 10^{-10}$.\n\n(iv) Convergence Verification: This test validates the long-time convergence property. For a large power $k$, the matrix $T^k$ should be close to the limiting matrix $\\mathbf{1}\\pi$. We compute the discrepancy between the $k$-th power of the matrix, $T^k$, and the theoretical limit matrix. The error is measured as the maximum absolute difference between the elements of the two matrices, $\\max_{i,j} |(T^k)_{ij} - (\\mathbf{1}\\pi)_{ij}|$. This is a common interpretation of the matrix infinity norm in the context of element-wise convergence. This error must be less than or equal to the case-specific convergence tolerance $\\varepsilon_{\\text{conv}}$.\n\nThese steps are systematically applied to each test case, and the boolean outcomes of the four checks are aggregated into a single list as per the problem's requirement.",
            "answer": "```python\nimport numpy as np\n\ndef solve_case(T, k, eps_norm, eps_pos, eps_stat, eps_conv):\n    \"\"\"\n    Computes the stationary distribution of T and performs four validation checks.\n\n    Args:\n        T (np.ndarray): The n x n transition matrix.\n        k (int): The power to raise T to for the convergence check.\n        eps_norm (float): Tolerance for row normalization check.\n        eps_pos (float): Tolerance for positivity check.\n        eps_stat (float): Tolerance for stationarity check.\n        eps_conv (float): Tolerance for convergence check.\n\n    Returns:\n        tuple: A tuple of four booleans for (normalization, positivity, stationarity, convergence).\n    \"\"\"\n    n = T.shape[0]\n\n    # Step 1: Compute the stationary distribution pi\n    # We need to solve (T.T - I)x = 0 subject to sum(x) = 1.\n    # This is done by replacing the last row of (T.T - I) with a row of ones,\n    # and setting the corresponding element in the RHS vector to 1.\n    A = T.T - np.identity(n)\n    A[-1, :] = 1.0\n    b = np.zeros(n)\n    b[-1] = 1.0\n\n    try:\n        # pi_vec is a column vector, but numpy's 1-D array handles this flexibly.\n        pi_vec = np.linalg.solve(A, b)\n    except np.linalg.LinAlgError:\n        # Should not happen for a valid irreducible, aperiodic MSM\n        return (False, False, False, False)\n\n    # Step 2: Perform the four verification checks\n\n    # Check (i): Row normalization\n    row_sums = np.sum(T, axis=1)\n    norm_err = np.max(np.abs(row_sums - 1.0))\n    is_normalized = norm_err <= eps_norm\n\n    # Check (ii): Entrywise nonnegativity (positivity)\n    min_val = np.min(T)\n    is_positive = min_val >= -eps_pos\n\n    # Check (iii): Stationarity\n    # pi_vec is broadcasted as a row vector in this operation\n    residual_vec = pi_vec @ T - pi_vec\n    stat_err = np.max(np.abs(residual_vec))\n    is_stationary = stat_err <= eps_stat\n\n    # Check (iv): Convergence\n    T_k = np.linalg.matrix_power(T, k)\n    # limit_matrix is the outer product of a column vector of ones and the row vector pi\n    limit_matrix = np.ones((n, 1)) @ pi_vec.reshape(1, -1)\n    conv_err = np.max(np.abs(T_k - limit_matrix))\n    is_converged = conv_err <= eps_conv\n\n    return (is_normalized, is_positive, is_stationary, is_converged)\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # Shared numerical tolerances\n    eps_norm = 1e-12\n    eps_pos = 1e-15\n    eps_stat = 1e-10\n\n    # Test suite\n    test_cases = [\n        {\n            \"T\": np.array([\n                [0.86, 0.10, 0.04],\n                [1/6, 11/15, 0.10], # Using fractions for precision\n                [0.10, 0.15, 0.75]\n            ]),\n            \"k\": 200,\n            \"eps_conv\": 1e-6\n        },\n        {\n            \"T\": np.array([\n                [0.995, 0.005],\n                [0.02, 0.98]\n            ]),\n            \"k\": 800,\n            \"eps_conv\": 1e-8\n        },\n        {\n            \"T\": np.array([\n                [0.94, 0.05, 0.01, 0.0],\n                [0.04, 0.93, 0.0, 0.03],\n                [0.005, 0.0, 0.985, 0.01],\n                [0.0, 0.02, 0.03, 0.95]\n            ]),\n            \"k\": 2000,\n            \"eps_conv\": 1e-8\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        T = case[\"T\"]\n        # The first case in the problem description used floating point approximations.\n        # Let's ensure the matrix matches the problem's floats exactly for reproducibility.\n        if T.shape[0] == 3:\n            T[1,0] = 0.16666666666666666\n            T[1,1] = 0.7333333333333334\n        \n        results = solve_case(T, case[\"k\"], eps_norm, eps_pos, eps_stat, case[\"eps_conv\"])\n        all_results.extend(results)\n\n    # Format the final output as a single-line string\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The central claim of any MSM is that the system's dynamics are Markovian, meaning the future depends only on the present. The Chapman-Kolmogorov (C-K) test provides a direct and statistically rigorous method to validate this core assumption by comparing model predictions against empirical data at longer timescales. This practice will challenge you to implement the C-K test, including the crucial step of quantifying statistical uncertainty, to determine if a model is truly predictive .",
            "id": "3838019",
            "problem": "You are asked to implement and apply the Chapman–Kolmogorov test to assess the validity of a Markov State Model (MSM) constructed from synthetic molecular dynamics-like trajectories. You will formalize the test in purely mathematical terms and quantify whether the MSM is consistent with the Markov property within statistical uncertainties. The test must be applied across several lag steps and on multiple synthetic systems to probe both valid and invalid scenarios.\n\nUse the following foundational definitions and facts.\n\nA discrete-time Markov chain on a finite state space has a row-stochastic transition matrix at a given lag time, denoted by $T(\\tau)$, where each element $T(\\tau)_{ij}$ satisfies $T(\\tau)_{ij} \\ge 0$ and $\\sum_{j} T(\\tau)_{ij} = 1$. For an integer multiple $k$ of the base lag $\\tau$, the Chapman–Kolmogorov property states\n$$\nT(k \\tau) = \\left(T(\\tau)\\right)^k,\n$$\nwhere the superscript denotes standard matrix multiplication. This implication underpins the MSM validity at longer time scales.\n\nGiven a trajectory of states $\\{x_t\\}_{t=0}^{N-1}$ sampled at uniform discrete time steps of base lag $\\tau$, define the lag-$L$ transition count matrix $C^{(L)}$ by\n$$\nC_{ij}^{(L)} = \\left|\\left\\{ t \\in \\{0,\\dots,N-L-1\\} : x_t = i,\\, x_{t+L} = j \\right\\}\\right|,\n$$\nwith row sums $n_i^{(L)} = \\sum_{j} C_{ij}^{(L)}$. The maximum likelihood estimate of the lag-$L$ transition matrix is the row-normalized counts,\n$$\n\\hat{T}_{ij}^{(L)} = \n\\begin{cases}\n\\dfrac{C_{ij}^{(L)}}{n_i^{(L)}}, & n_i^{(L)} > 0, \\\\\n0, & n_i^{(L)} = 0.\n\\end{cases}\n$$\n\nAssume a Dirichlet prior with symmetric concentration parameters $\\alpha_0$ for each outcome in a row, leading to a Dirichlet posterior $\\text{Dir}(\\alpha_0 + \\mathbf{n}_i)$ for row $i$, where $\\mathbf{n}_i$ denotes the vector of row counts. The marginal posterior for element $p_{ij}^{(L)}$ is a Beta distribution,\n$$\np_{ij}^{(L)} \\sim \\text{Beta}\\left(C_{ij}^{(L)} + \\alpha_0,\\, n_i^{(L)} - C_{ij}^{(L)} + (m-1)\\alpha_0\\right),\n$$\nwhere $m$ is the number of states. A $100\\gamma\\%$ credible interval for $p_{ij}^{(L)}$ is obtained via the $\\gamma$-level central quantiles of the Beta distribution.\n\nThe Chapman–Kolmogorov test proceeds as follows for a set of integer lag multipliers $\\{k\\}$:\n1. Estimate the base-lag matrix $\\hat{T}^{(1)}$ from $C^{(1)}$.\n2. For each $k$, compute the Chapman–Kolmogorov prediction $\\left(\\hat{T}^{(1)}\\right)^k$, and estimate the empirical lag-$k$ matrix $\\hat{T}^{(k)}$ from $C^{(k)}$.\n3. For each row $i$ with $n_i^{(k)} \\ge n_{\\min}$ and for each column $j$, compute the posterior credible interval $[L_{ij}^{(k)}, U_{ij}^{(k)}]$ for $p_{ij}^{(k)}$ using the Beta marginal and test whether the predicted entry $\\left(\\hat{T}^{(1)}\\right)^k_{ij}$ lies inside this interval.\n4. Define the coverage fraction at lag $k$ as\n$$\n\\text{cov}(k) = \\frac{\\text{number of covered entries}}{\\text{number of tested entries}},\n$$\nwhere tested entries are those with $n_i^{(k)} \\ge n_{\\min}$. The MSM passes at lag $k$ if $\\text{cov}(k) \\ge \\theta$, where $\\theta$ is a coverage threshold matching the chosen credible level. The MSM is declared valid for the test suite if it passes for all $k$ in the set.\n\nYour task is to implement a program that:\n- Generates synthetic trajectories from specified ground-truth transition matrices.\n- Constructs MSMs at a base lag, performs Chapman–Kolmogorov predictions for several $k$, computes empirical lag-$k$ transition matrices, constructs posterior credible intervals under a symmetric Dirichlet prior, and evaluates coverage.\n\nYour program must process the following test suite of parameterized cases.\n\nTest case A (valid MSM, long trajectory):\n- Number of states $m = 4$.\n- Ground-truth base-lag transition matrix $T^{\\text{true}}$ is\n$$\n\\begin{bmatrix}\n0.90 & 0.07 & 0.02 & 0.01 \\\\\n0.06 & 0.88 & 0.05 & 0.01 \\\\\n0.03 & 0.05 & 0.88 & 0.04 \\\\\n0.01 & 0.02 & 0.06 & 0.91\n\\end{bmatrix}.\n$$\n- Trajectory length $N = 200000$ steps.\n- Lag multipliers $\\{k\\} = \\{2,\\,5,\\,10\\}$.\n- Symmetric Dirichlet prior concentration $\\alpha_0 = 0.5$ (Jeffreys prior).\n- Credible level $\\gamma = 0.95$.\n- Minimum row count threshold $n_{\\min} = 50$.\n- Coverage threshold $\\theta = 0.95$.\n\nTest case B (valid MSM, shorter trajectory):\n- Number of states $m = 4$.\n- Ground-truth base-lag transition matrix $T^{\\text{true}}$ identical to Test case A.\n- Trajectory length $N = 10000$ steps.\n- Lag multipliers $\\{k\\} = \\{2,\\,10,\\,20\\}$.\n- Symmetric Dirichlet prior concentration $\\alpha_0 = 0.5$.\n- Credible level $\\gamma = 0.95$.\n- Minimum row count threshold $n_{\\min} = 50$.\n- Coverage threshold $\\theta = 0.95$.\n\nTest case C (invalid MSM under projection, non-Markov coarse-graining):\n- Underlying number of microstates $m_{\\text{micro}} = 3$, labeled $A$, $B$, $C$.\n- Ground-truth base-lag transition matrix on microstates is\n$$\n\\begin{bmatrix}\n0.75 & 0.25 & 0.00 \\\\\n0.10 & 0.80 & 0.10 \\\\\n0.00 & 0.005 & 0.995\n\\end{bmatrix}.\n$$\n- Observed coarse-graining maps microstates into two macrostates $X$ and $Y$ via $(A \\rightarrow X)$, $(B \\rightarrow Y)$, $(C \\rightarrow X)$, yielding an observed $m = 2$ state process that is not Markovian due to memory in $X$ arising from distinct escape propensities of $A$ and $C$.\n- Trajectory length $N = 200000$ steps on microstates, subsequently mapped to observed macrostates.\n- Lag multipliers $\\{k\\} = \\{2,\\,5,\\,10\\}$.\n- Symmetric Dirichlet prior concentration $\\alpha_0 = 0.5$.\n- Credible level $\\gamma = 0.95$.\n- Minimum row count threshold $n_{\\min} = 50$.\n- Coverage threshold $\\theta = 0.95$.\n\nImplementation requirements:\n- Use a fixed random seed for reproducibility.\n- For each test case, simulate a trajectory from the specified ground-truth transition matrix at base lag $1$, estimate $\\hat{T}^{(1)}$, compute predicted $\\left(\\hat{T}^{(1)}\\right)^k$, compute empirical $\\hat{T}^{(k)}$, form posterior credible intervals based on $$\\text{Beta}\\left(C_{ij}^{(k)} + \\alpha_0,\\, n_i^{(k)} - C_{ij}^{(k)} + (m-1)\\alpha_0\\right)$$ at level $\\gamma$, and evaluate $\\text{cov}(k)$ for each $k$. The test case returns a boolean indicating whether $\\text{cov}(k) \\ge \\theta$ for all $k$ in the specified set.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[resultA,resultB,resultC]\"), where each entry is a boolean corresponding to the pass or fail outcome for the respective test case.",
            "solution": "The problem is valid. It presents a well-defined computational task grounded in the standard theory of Markov State Model (MSM) validation from computational statistical mechanics. It is scientifically sound, objective, and complete. All parameters and procedures are explicitly specified.\n\nThe core of the problem is to implement and apply the Chapman–Kolmogorov (C-K) test for MSM validation. An MSM is a useful approximation if the underlying dynamics are Markovian on the chosen state space. The C-K property, $T(k\\tau) = [T(\\tau)]^k$, provides a necessary condition for this Markovianity. It asserts that the transition probabilities for a long lag time $k\\tau$ can be predicted by simply propagating the transition probabilities of a short base lag time $\\tau$.\n\nThe validation procedure involves comparing this prediction with an empirical estimate of the transition matrix at the longer lag time. If the system is truly Markovian, the prediction should agree with the empirical measurement within statistical uncertainty. The procedure is as follows:\n\n1.  **Synthetic Data Generation**: For each test case, a discrete-time trajectory of states $\\{x_t\\}_{t=0}^{N-1}$ is simulated. This is achieved by starting in a random state (e.g., state $0$) and iteratively drawing the next state $x_{t+1}$ from the multinomial distribution defined by row $x_t$ of the ground-truth transition matrix, $T^{\\text{true}}$. For Test Case C, this simulation is performed on the underlying $3$-state microstate space, and the resulting trajectory is then mapped to the observed $2$-state macrostate space according to the specified coarse-graining rule: state $A \\to X$, $B \\to Y$, $C \\to X$. This coarse-graining is known to induce non-Markovian dynamics in the observed process.\n\n2.  **Estimation of Transition Matrices**:\n    -   From the generated trajectory, we first estimate the base-lag transition matrix, $\\hat{T}^{(1)}$, where the lag is $\\tau=1$ time step. This is done by first computing the transition count matrix $C^{(1)}$, where $C_{ij}^{(1)}$ is the number of observed transitions from state $i$ to state $j$ in one step. The maximum likelihood estimate (MLE) is then the row-normalized count matrix: $\\hat{T}_{ij}^{(1)} = C_{ij}^{(1)} / \\sum_k C_{ik}^{(1)}$.\n    -   For each lag multiplier $k$ in the test set, we compute the C-K prediction, which is the base-lag MLE matrix raised to the $k$-th power: $T^{\\text{pred}}(k) = (\\hat{T}^{(1)})^k$.\n    -   We also compute the empirical transition matrix at lag $k$, $\\hat{T}^{(k)}$, by directly counting transitions over $k$ steps in the trajectory to form $C^{(k)}$ and then row-normalizing.\n\n3.  **Statistical Comparison via Credible Intervals**: The key step is to determine if the difference between the prediction $T^{\\text{pred}}(k)$ and the empirical estimate $\\hat{T}^{(k)}$ is statistically significant. We treat the empirical estimate not as a single point but as a distribution reflecting finite-sampling uncertainty.\n    -   Using a Bayesian framework with a symmetric Dirichlet prior (parameterized by $\\alpha_0$), the posterior distribution for each transition probability $p_{ij}^{(k)}$ is a Beta distribution: $p_{ij}^{(k)} \\sim \\text{Beta}(C_{ij}^{(k)} + \\alpha_0, n_i^{(k)} - C_{ij}^{(k)} + (m-1)\\alpha_0)$, where $n_i^{(k)} = \\sum_j C_{ij}^{(k)}$ is the total number of transitions starting from state $i$ at lag $k$, and $m$ is the number of states.\n    -   For each element $(i,j)$ in a row $i$ with sufficient statistics ($n_i^{(k)} \\ge n_{\\min}$), we compute a $100\\gamma\\%$ credible interval $[L_{ij}^{(k)}, U_{ij}^{(k)}]$. This interval is defined by the lower and upper quantiles of the Beta posterior distribution, specifically the $(\\frac{1-\\gamma}{2})$ and $(1 - \\frac{1-\\gamma}{2})$ quantiles.\n    -   We then check if the predicted value, $(T^{\\text{pred}}(k))_{ij}$, falls within this interval: $L_{ij}^{(k)} \\le (T^{\\text{pred}}(k))_{ij} \\le U_{ij}^{(k)}$.\n\n4.  **Verdict Formulation**:\n    -   The coverage fraction at lag $k$, $\\text{cov}(k)$, is calculated as the fraction of tested matrix elements for which the prediction falls inside the credible interval.\n    -   The MSM is considered to have passed the test at lag $k$ if this coverage fraction is greater than or equal to a threshold $\\theta$, i.e., $\\text{cov}(k) \\ge \\theta$. The threshold $\\theta$ is typically set equal to the credible level $\\gamma$, reflecting the expectation that, for a valid model, approximately $100\\gamma\\%$ of the predictions should lie within their corresponding $100\\gamma\\%$ credible intervals.\n    -   The final result for a given test case is a single boolean value: `True` if the MSM passes the test for all specified lag multipliers $\\{k\\}$, and `False` otherwise.\n\n-   **Test Case A (Valid MSM, Long Trajectory)**: The underlying process is Markovian and the long trajectory provides good statistics. We expect the C-K prediction to be accurate and the coverage fraction to be high (close to $\\gamma=0.95$), thus passing the test.\n-   **Test Case B (Valid MSM, Shorter Trajectory)**: The process is still Markovian, but the shorter trajectory leads to greater statistical uncertainty. This means the credible intervals for $\\hat{T}^{(k)}$ will be wider, but the initial estimate $\\hat{T}^{(1)}$ will also be less precise, leading to larger errors in the prediction $(\\hat{T}^{(1)})^k$. The test result depends on which of these effects dominates. Typically, the increased width of the credible intervals is sufficient to maintain high coverage. We expect this case to pass.\n-   **Test Case C (Invalid MSM, Coarse-Graining)**: The coarse-graining introduces memory into the observed dynamics, violating the Markov assumption. For example, the probability of transitioning from macrostate $X$ to $Y$ depends on whether the system is in microstate $A$ or $C$. The MSM, which averages over these microstates, cannot capture this. The C-K prediction, based on the faulty Markov assumption, will systematically deviate from the true long-term dynamics. We expect the coverage fraction to fall significantly below the threshold $\\theta$, causing the test to fail.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import beta\n\ndef solve():\n    \"\"\"\n    Main function to run the Chapman-Kolmogorov test suite.\n    \"\"\"\n    # Set a fixed random seed for reproducibility.\n    np.random.seed(42)\n\n    # Define the test cases from the problem statement.\n    T_true_A_B = np.array([\n        [0.90, 0.07, 0.02, 0.01],\n        [0.06, 0.88, 0.05, 0.01],\n        [0.03, 0.05, 0.88, 0.04],\n        [0.01, 0.02, 0.06, 0.91]\n    ])\n\n    T_true_C_micro = np.array([\n        [0.75, 0.25, 0.00],\n        [0.10, 0.80, 0.10],\n        [0.00, 0.005, 0.995]\n    ])\n\n    test_cases = [\n        {\n            \"name\": \"A\",\n            \"m\": 4,\n            \"T_true\": T_true_A_B,\n            \"N\": 200000,\n            \"lags\": [2, 5, 10],\n            \"alpha0\": 0.5,\n            \"gamma\": 0.95,\n            \"n_min\": 50,\n            \"theta\": 0.95\n        },\n        {\n            \"name\": \"B\",\n            \"m\": 4,\n            \"T_true\": T_true_A_B,\n            \"N\": 10000,\n            \"lags\": [2, 10, 20],\n            \"alpha0\": 0.5,\n            \"gamma\": 0.95,\n            \"n_min\": 50,\n            \"theta\": 0.95\n        },\n        {\n            \"name\": \"C\",\n            \"m\": 2, # Number of macrostates\n            \"m_micro\": 3,\n            \"T_true\": T_true_C_micro,\n            \"N\": 200000,\n            \"lags\": [2, 5, 10],\n            \"alpha0\": 0.5,\n            \"gamma\": 0.95,\n            \"n_min\": 50,\n            \"theta\": 0.95,\n            \"coarse_grain_map\": np.array([0, 1, 0]) # A->X(0), B->Y(1), C->X(0)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        case_result = run_ck_test(case)\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The map to str is required for Python's bool capitalization (True vs true)\n    print(f\"[{','.join(map(lambda x: str(x), results))}]\")\n\n\ndef generate_trajectory(T_true, N, start_state=0):\n    \"\"\"\n    Generates a trajectory from a discrete-time Markov chain.\n    \"\"\"\n    num_states = T_true.shape[0]\n    traj = np.zeros(N, dtype=int)\n    traj[0] = start_state\n    for t in range(N - 1):\n        next_state_probs = T_true[traj[t], :]\n        traj[t+1] = np.random.choice(num_states, p=next_state_probs)\n    return traj\n\n\ndef get_count_matrix(trajectory, m, lag):\n    \"\"\"\n    Computes the transition count matrix for a given lag time.\n    \"\"\"\n    C = np.zeros((m, m), dtype=int)\n    for t in range(len(trajectory) - lag):\n        i = trajectory[t]\n        j = trajectory[t+lag]\n        C[i, j] += 1\n    return C\n\n\ndef get_mle_t_matrix(C):\n    \"\"\"\n    Computes the MLE transition matrix from a count matrix.\n    \"\"\"\n    m = C.shape[0]\n    T_mle = np.zeros((m, m))\n    row_sums = C.sum(axis=1)\n    # Using np.where to avoid division by zero\n    T_mle = np.where(row_sums[:, None] > 0, C / row_sums[:, None], 0)\n    return T_mle\n\n\ndef run_ck_test(params):\n    \"\"\"\n    Runs the full Chapman-Kolmogorov test for a single parameter set.\n    \"\"\"\n    m = params[\"m\"]\n    N = params[\"N\"]\n    T_true = params[\"T_true\"]\n    lags = params[\"lags\"]\n    alpha0 = params[\"alpha0\"]\n    gamma = params[\"gamma\"]\n    n_min = params[\"n_min\"]\n    theta = params[\"theta\"]\n\n    # --- 1. Generate Trajectory ---\n    if \"coarse_grain_map\" in params:\n        micro_traj = generate_trajectory(T_true, N)\n        trajectory = params[\"coarse_grain_map\"][micro_traj]\n    else:\n        trajectory = generate_trajectory(T_true, N)\n\n    # --- 2. Estimate Base-Lag Matrix ---\n    C1 = get_count_matrix(trajectory, m, lag=1)\n    T_hat_1 = get_mle_t_matrix(C1)\n\n    case_passes = True\n    for k in lags:\n        # --- 3. C-K Prediction ---\n        T_pred_k = np.linalg.matrix_power(T_hat_1, k)\n\n        # --- 4. Empirical Lag-k Matrix ---\n        Ck = get_count_matrix(trajectory, m, lag=k)\n        \n        # --- 5. Statistical Comparison ---\n        covered_entries = 0\n        tested_entries = 0\n        \n        row_sums_k = Ck.sum(axis=1)\n        \n        for i in range(m):\n            if row_sums_k[i] < n_min:\n                continue\n            \n            for j in range(m):\n                tested_entries += 1\n                \n                # Beta posterior parameters\n                alpha_post = Ck[i, j] + alpha0\n                beta_post = row_sums_k[i] - Ck[i, j] + (m - 1) * alpha0\n                \n                # Credible interval\n                q_low = (1.0 - gamma) / 2.0\n                q_high = 1.0 - q_low\n                \n                lower_bound = beta.ppf(q_low, alpha_post, beta_post)\n                upper_bound = beta.ppf(q_high, alpha_post, beta_post)\n\n                # Check if prediction is covered\n                prediction = T_pred_k[i, j]\n                if lower_bound <= prediction <= upper_bound:\n                    covered_entries += 1\n        \n        # --- 6. Verdict ---\n        if tested_entries > 0:\n            coverage_fraction = covered_entries / tested_entries\n        else: # No rows met n_min, technically passes as no violations found.\n            coverage_fraction = 1.0\n\n        if coverage_fraction < theta:\n            case_passes = False\n            break # Fail fast, no need to check other lags for this case\n\n    return case_passes\n\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "A powerful diagnostic for MSM quality is the analysis of its implied timescales, which connect the model's eigenvalues to the physical relaxation processes of the system. For a well-constructed MSM, these timescales should be constant with respect to the lag time $\\tau$ used for analysis, indicating that the model has successfully captured intrinsic system properties. This exercise will have you compute implied timescales for models built at different lag times and implement a quantitative test to assess their convergence .",
            "id": "3838048",
            "problem": "You are given the task of computing implied timescales for an example transition probability matrix as a function of lag time and assessing whether the slowest timescales have converged, in the context of building and validating Markov state models (MSMs) for long-timescale dynamics in computational chemical biology. Begin from the following definitions and facts. A Markov state model (MSM) represents the dynamics of a system projected onto a finite set of discrete states by a transition probability matrix $T(\\tau)$ that advances state probabilities by a lag time $\\tau$. A transition probability matrix $T(\\tau)$ is a row-stochastic matrix with nonnegative entries and row sums equal to $1$; its largest eigenvalue equals $1$. In continuous-time kinetics, a rate matrix (generator) $K$ has nonnegative off-diagonal elements $K_{ij}$ for $i \\neq j$ and diagonal elements $K_{ii} = - \\sum_{j \\neq i} K_{ij}$. For a continuous-time Markov chain (CTMC), the corresponding lagged transition probability matrix is $T(\\tau) = \\exp(\\tau K)$, where $\\exp$ denotes the matrix exponential. The implied relaxation timescales $t_i(\\tau)$ of $T(\\tau)$ are defined via its eigenvalues $\\lambda_i(\\tau)$ (ordered by decreasing magnitude), excluding the trivial stationary eigenvalue $\\lambda_1(\\tau)=1$, by the formula $t_i(\\tau) = - \\tau / \\ln |\\lambda_i(\\tau)|$. A slow implied timescale is one associated with the largest nontrivial eigenvalue magnitude $|\\lambda_2(\\tau)|$. Convergence of the slowest implied timescale across increasing $\\tau$ is expected for a well-constructed MSM and indicates that the model captures approximate Markovian kinetics at those lag times. Your program must compute $t_2(\\tau)$ for multiple lag times and assess convergence over the larger-lag subset by a relative range criterion.\n\nScientific realism requirement: You will construct $T(\\tau)$ matrices from physically plausible continuous-time rate matrices $K$ via $T(\\tau) = \\exp(\\tau K)$, and then add small deterministic perturbations to mimic finite-sample estimation noise. The perturbations must preserve row-stochasticity and nonnegativity by clipping negative entries to $0$ and renormalizing rows to sum to $1$.\n\nUnits: All lag times $\\tau$ and implied timescales $t_i(\\tau)$ must be expressed in microseconds. Your program’s final numerical outputs must be in microseconds.\n\nAlgorithm to implement:\n- For each test case and each lag time $\\tau$, form the unperturbed $T_0(\\tau) = \\exp(\\tau K)$.\n- Add a deterministic perturbation matrix $\\Delta(\\tau) = \\delta(\\tau) N$, where $N$ is a fixed off-diagonal pattern matrix with $N_{ij} = 1$ for $i \\neq j$ and $N_{ii} = 0$, and $\\delta(\\tau)$ is a small nonnegative scalar provided per test case.\n- Set $\\tilde{T}(\\tau) = T_0(\\tau) + \\Delta(\\tau)$, then project $\\tilde{T}(\\tau)$ back to the set of row-stochastic matrices by elementwise clipping to nonnegativity and row normalization: for each row $i$, set $\\tilde{T}_{ij}(\\tau) \\leftarrow \\max\\{ \\tilde{T}_{ij}(\\tau), 0 \\}$, then divide the row by its sum.\n- Compute all eigenvalues $\\lambda_i(\\tau)$ of $\\tilde{T}(\\tau)$, sort them by descending magnitude $|\\lambda_i(\\tau)|$, exclude the trivial eigenvalue closest to $1$, and compute the slowest implied timescale $t_2(\\tau) = - \\tau / \\ln |\\lambda_2(\\tau)|$. If $|\\lambda_2(\\tau)| = 0$, define $t_2(\\tau) = 0$.\n- Convergence assessment: For the set of slowest implied timescales $\\{ t_2(\\tau_k) \\}$ across all lag times sorted in increasing $\\tau_k$, consider only the larger-lag subset consisting of the last half of the values (rounding down the split index), compute the mean $\\bar{t}$ of this subset, its maximum $t_{\\max}$ and minimum $t_{\\min}$, and declare convergence if $(t_{\\max} - t_{\\min}) / \\bar{t} \\leq \\epsilon$, where $\\epsilon$ is a tolerance specified per test case. If $\\bar{t} = 0$, declare convergence if and only if $t_{\\max} = t_{\\min}$.\n\nYour program should produce a single line of output containing the results aggregated across all test cases as a comma-separated list enclosed in square brackets. For each test case, output a list with two elements: the list of slowest implied timescales $[t_2(\\tau_1), t_2(\\tau_2), \\dots]$ in microseconds rounded to six decimal places, and the boolean value of the convergence assessment for that test case. The final output line must therefore look like $[[[t_{21}, t_{22}, \\dots], b_1], [[t_{31}, t_{32}, \\dots], b_2], \\dots]$ where each $t_{ij}$ is a float in microseconds and each $b_i$ is a boolean.\n\nTest suite specification with explicit parameters:\n\n- Test case $1$ (happy path, reversible symmetric clusters with decreasing perturbation):\n  - Number of states: $4$.\n  - Rate matrix $K$ defined by two clusters $\\{0,1\\}$ and $\\{2,3\\}$ with strong intra-cluster rates and weak inter-cluster rates: for $i \\neq j$, set $K_{ij} = 6.0$ if $i,j \\in \\{0,1\\}$ or $i,j \\in \\{2,3\\}$, and set $K_{ij} = 0.2$ otherwise. Set each $K_{ii} = - \\sum_{j \\neq i} K_{ij}$.\n  - Lag times (microseconds): $\\tau \\in \\{ 1.0, 2.0, 5.0, 10.0 \\}$.\n  - Perturbation magnitudes: $\\delta(\\tau) \\in \\{ 0.02, 0.01, 0.005, 0.002 \\}$ respectively.\n  - Convergence tolerance: $\\epsilon = 0.1$.\n\n- Test case $2$ (edge case, $3$-state asymmetric ring with nondecreasing perturbation leading to nonconvergence under tight tolerance):\n  - Number of states: $3$.\n  - Rate matrix $K$ defined by an asymmetric ring: $K_{01} = 1.5$, $K_{12} = 1.5$, $K_{20} = 1.5$, $K_{10} = 1.2$, $K_{21} = 1.2$, $K_{02} = 0.8$, with $K_{ii} = - \\sum_{j \\neq i} K_{ij}$.\n  - Lag times (microseconds): $\\tau \\in \\{ 0.5, 1.0, 2.0, 4.0 \\}$.\n  - Perturbation magnitudes: $\\delta(\\tau) \\in \\{ 0.03, 0.03, 0.03, 0.03 \\}$.\n  - Convergence tolerance: $\\epsilon = 0.05$.\n\n- Test case $3$ (boundary condition, $5$-state with a very slow inter-cluster bridge and small lags including very small $\\tau$):\n  - Number of states: $5$.\n  - Rate matrix $K$ defined by clusters $\\{0,1,2\\}$ and $\\{3,4\\}$ with strong intra-cluster rates and very weak inter-cluster rates: for $i \\neq j$, set $K_{ij} = 4.0$ if $i,j \\in \\{0,1,2\\}$ or $i,j \\in \\{3,4\\}$, and set $K_{ij} = 0.05$ otherwise. Set each $K_{ii} = - \\sum_{j \\neq i} K_{ij}$.\n  - Lag times (microseconds): $\\tau \\in \\{ 0.1, 0.5, 1.0, 2.0 \\}$.\n  - Perturbation magnitudes: $\\delta(\\tau) \\in \\{ 0.02, 0.01, 0.005, 0.003 \\}$.\n  - Convergence tolerance: $\\epsilon = 0.15$.\n\nRequired final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element in this list corresponds to one test case and must be a two-element list whose first element is the list of slowest implied timescales for the specified lag times (in microseconds, rounded to six decimal places) and whose second element is a boolean indicating whether the slowest timescales have converged according to the criterion above. For example, the output should look like $[[[t_{11},t_{12},t_{13},t_{14}],b_1],[[t_{21},t_{22},t_{23},t_{24}],b_2],[[t_{31},t_{32},t_{33},t_{34}],b_3]]$ with numerical values replacing the $t_{ij}$ placeholders.",
            "solution": "The user has provided a problem that requires the computation and analysis of implied timescales for several Markov State Models (MSMs). I will first validate the problem statement according to the specified criteria.\n\n### Step 1: Extract Givens\n\n**Definitions and Core Concepts:**\n- A Markov State Model (MSM) is represented by a transition probability matrix $T(\\tau)$ that advances state probabilities by a lag time $\\tau$.\n- A transition probability matrix $T(\\tau)$ is a row-stochastic matrix with nonnegative entries and row sums equal to $1$. Its largest eigenvalue equals $1$.\n- A continuous-time rate matrix (generator) $K$ has nonnegative off-diagonal elements $K_{ij}$ for $i \\neq j$ and diagonal elements $K_{ii} = - \\sum_{j \\neq i} K_{ij}$.\n- The relationship between $K$ and $T(\\tau)$ for a continuous-time Markov chain (CTMC) is $T(\\tau) = \\exp(\\tau K)$, where $\\exp$ is the matrix exponential.\n- The implied relaxation timescales $t_i(\\tau)$ are defined by the eigenvalues $\\lambda_i(\\tau)$ of $T(\\tau)$ (ordered by decreasing magnitude $|\\lambda_i(\\tau)|$) as $t_i(\\tau) = - \\tau / \\ln |\\lambda_i(\\tau)|$, for all $i$ where $\\lambda_i(\\tau)$ is not the trivial stationary eigenvalue $\\lambda_1(\\tau)=1$.\n- The slowest implied timescale is $t_2(\\tau)$, associated with the largest nontrivial eigenvalue magnitude $|\\lambda_2(\\tau)|$.\n- If $|\\lambda_2(\\tau)| = 0$, then $t_2(\\tau)$ is defined as $0$.\n\n**Algorithmic Steps:**\n1.  For each test case and lag time $\\tau$, form the unperturbed transition matrix $T_0(\\tau) = \\exp(\\tau K)$.\n2.  Define a perturbation matrix $\\Delta(\\tau) = \\delta(\\tau) N$, where $N$ is a fixed off-diagonal pattern matrix with $N_{ij} = 1$ for $i \\neq j$ and $N_{ii} = 0$. $\\delta(\\tau)$ is a given small nonnegative scalar.\n3.  Construct the perturbed matrix $\\tilde{T}(\\tau) = T_0(\\tau) + \\Delta(\\tau)$.\n4.  Project $\\tilde{T}(\\tau)$ back to a row-stochastic matrix:\n    a. Element-wise clipping: $\\tilde{T}_{ij}(\\tau) \\leftarrow \\max\\{ \\tilde{T}_{ij}(\\tau), 0 \\}$.\n    b. Row normalization: Divide each row by its sum.\n5.  Compute eigenvalues $\\lambda_i(\\tau)$ of $\\tilde{T}(\\tau)$, sort by descending magnitude, and identify the second largest magnitude, $|\\lambda_2(\\tau)|$.\n6.  Compute the slowest implied timescale $t_2(\\tau) = - \\tau / \\ln |\\lambda_2(\\tau)|$.\n7.  Assess convergence: For the set of timescales $\\{ t_2(\\tau_k) \\}$ over all lag times, consider the latter half of the values (rounding down the split index). Compute the mean $\\bar{t}$, maximum $t_{\\max}$, and minimum $t_{\\min}$ of this subset. Convergence is declared if $(t_{\\max} - t_{\\min}) / \\bar{t} \\leq \\epsilon$. If $\\bar{t} = 0$, convergence is declared if and only if $t_{\\max} = t_{\\min}$.\n\n**Units and Formatting:**\n- All time units ($\\tau$, $t_i(\\tau)$) are in microseconds.\n- Final numerical output for timescales must be rounded to six decimal places.\n- The final output is a single line: a string representation of a list of lists, where each inner list contains a list of timescales and a boolean convergence flag. Format: `[[[t_11, t_12,...], b_1], [[t_21, t_22,...], b_2], ...]`.\n\n**Test Suite Parameters:**\n\n*   **Test Case 1:**\n    *   States: $4$.\n    *   Rate matrix $K$: $K_{ij} = 6.0$ for intra-cluster transitions ($\\{0,1\\}$ and $\\{2,3\\}$) and $K_{ij} = 0.2$ for inter-cluster, for $i \\neq j$. $K_{ii} = - \\sum_{j \\neq i} K_{ij}$.\n    *   Lag times $\\tau$ (μs): $\\{ 1.0, 2.0, 5.0, 10.0 \\}$.\n    *   Perturbations $\\delta(\\tau)$: $\\{ 0.02, 0.01, 0.005, 0.002 \\}$.\n    *   Tolerance $\\epsilon$: $0.1$.\n*   **Test Case 2:**\n    *   States: $3$.\n    *   Rate matrix $K$: Asymmetric ring with $K_{01} = 1.5, K_{12} = 1.5, K_{20} = 1.5, K_{10} = 1.2, K_{21} = 1.2, K_{02} = 0.8$. $K_{ii} = - \\sum_{j \\neq i} K_{ij}$.\n    *   Lag times $\\tau$ (μs): $\\{ 0.5, 1.0, 2.0, 4.0 \\}$.\n    *   Perturbations $\\delta(\\tau)$: $\\{ 0.03, 0.03, 0.03, 0.03 \\}$.\n    *   Tolerance $\\epsilon$: $0.05$.\n*   **Test Case 3:**\n    *   States: $5$.\n    *   Rate matrix $K$: $K_{ij} = 4.0$ for intra-cluster transitions ($\\{0,1,2\\}$ and $\\{3,4\\}$) and $K_{ij} = 0.05$ for inter-cluster, for $i \\neq j$. $K_{ii} = - \\sum_{j \\neq i} K_{ij}$.\n    *   Lag times $\\tau$ (μs): $\\{ 0.1, 0.5, 1.0, 2.0 \\}$.\n    *   Perturbations $\\delta(\\tau)$: $\\{ 0.02, 0.01, 0.005, 0.003 \\}$.\n    *   Tolerance $\\epsilon$: $0.15$.\n\n### Step 2: Validate Using Extracted Givens\n\n1.  **Scientific or Factual Unsoundness:** The problem is firmly grounded in the theory of continuous-time Markov chains and their application in constructing Markov State Models, a standard and powerful technique in computational chemistry and biophysics. The definitions of the rate matrix $K$, transition matrix $T(\\tau)$, matrix exponential relation $T(\\tau) = \\exp(\\tau K)$, and the formula for implied timescales are all standard and correct. The concept of implied timescale convergence is a cornerstone of MSM validation. The problem is scientifically sound.\n2.  **Non-Formalizable or Irrelevant:** The problem is highly formalizable. It provides a precise, step-by-step algorithm and is directly relevant to the specified topic of *building and validating Markov state models (MSMs) for long-timescale dynamics* in computational chemical biology.\n3.  **Incomplete or Contradictory Setup:** The problem is self-contained. All necessary parameters for each test case—number of states, rules for constructing the rate matrix $K$, lists of lag times $\\tau$ and perturbation magnitudes $\\delta(\\tau)$, and the convergence tolerance $\\epsilon$—are explicitly provided. There are no contradictions.\n4.  **Unrealistic or Infeasible:** The rate matrices are constructed to represent physically plausible scenarios, such as kinetically distinct clusters of states with fast intra-cluster dynamics and slow inter-cluster transitions. The introduction of a small, state-dependent perturbation is a reasonable simplification to model finite-sampling noise in transition count estimation from molecular dynamics trajectories. The parameter values are within realistic ranges.\n5.  **Ill-Posed or Poorly Structured:** The problem is well-posed. The algorithm is deterministic and, given the inputs, will produce a unique, meaningful output. The definitions are clear and unambiguous. The edge case of a zero eigenvalue is explicitly handled.\n6.  **Pseudo-Profound, Trivial, or Tautological:** The problem requires the application of non-trivial mathematical concepts and numerical methods, including matrix exponentiation and eigenvalue decomposition. It is a substantive computational task that encapsulates a key part of the MSM validation workflow, not a trivial exercise.\n7.  **Outside Scientific Verifiability:** The problem is purely computational. The results can be independently verified by implementing the same specified algorithm with the same inputs.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. It is scientifically sound, well-posed, and complete. I will proceed with generating the solution.\n\nThe solution requires implementing the specified algorithm to compute the slowest implied timescale, $t_2(\\tau)$, for a series of lag times $\\tau$ across three distinct test cases. Each test case is defined by a rate matrix $K$, which describes the underlying continuous-time kinetics. The process involves several key steps rooted in the theory of Markov processes.\n\nFirst, for each test case, we construct the rate matrix $K$. The elements $K_{ij}$ for $i \\neq j$ represent the rate of transitioning from state $i$ to state $j$. The diagonal elements $K_{ii}$ are fixed by the condition that each row of the rate matrix must sum to zero, i.e., $K_{ii} = -\\sum_{j \\neq i} K_{ij}$. This ensures conservation of probability.\n\nNext, for each specified lag time $\\tau$, we compute the \"ideal\" transition probability matrix $T_0(\\tau)$. For a continuous-time Markov process described by $K$, the probability of transitioning from state $i$ to state $j$ in a time interval $\\tau$ is given by the matrix element $[T(\\tau)]_{ij}$. This matrix is calculated via the matrix exponential, $T(\\tau) = \\exp(\\tau K)$.\n\nThe problem introduces a realistic complication: noise. In practice, transition probabilities are estimated from finite simulation data and are subject to statistical error. To mimic this, we add a deterministic perturbation $\\Delta(\\tau) = \\delta(\\tau) N$ to the ideal matrix $T_0(\\tau)$, where $N$ is a matrix of ones on the off-diagonal and zeros on the diagonal, and $\\delta(\\tau)$ is a small, lag-dependent magnitude. This yields a perturbed matrix $\\tilde{T}(\\tau) = T_0(\\tau) + \\Delta(\\tau)$.\n\nThis perturbed matrix $\\tilde{T}(\\tau)$ is not guaranteed to be a valid row-stochastic matrix; its entries may be negative, and its rows may not sum to $1$. Therefore, we must project it back onto the space of valid stochastic matrices. This is achieved in two steps: first, all negative entries are clipped to zero, $\\tilde{T}_{ij}(\\tau) \\leftarrow \\max\\{\\tilde{T}_{ij}(\\tau), 0\\}$. Second, each row is normalized by dividing its elements by the row sum. This two-step process ensures both non-negativity and the row-sum-to-one property.\n\nWith a valid, albeit noisy, transition matrix $\\tilde{T}(\\tau)$ for each lag time, we proceed to compute its implied timescales. This involves calculating the eigenvalues $\\lambda_i(\\tau)$ of $\\tilde{T}(\\tau)$. The eigenvalues of a stochastic matrix have magnitudes less than or equal to $1$. The largest eigenvalue is always $\\lambda_1(\\tau) = 1$, corresponding to the stationary distribution. The other eigenvalues, $|\\lambda_i(\\tau)| < 1$ for $i > 1$, describe the timescales of relaxation toward this stationary distribution. The implied timescale $t_i(\\tau)$ associated with eigenvalue $\\lambda_i(\\tau)$ is given by the fundamental relation $t_i(\\tau) = -\\tau / \\ln|\\lambda_i(\\tau)|$. The slowest process in the system, beyond reaching stationarity, is captured by the largest non-unit eigenvalue magnitude, $|\\lambda_2(\\tau)|$. We therefore calculate $t_2(\\tau)$ for each $\\tau$. Per the problem, if $|\\lambda_2(\\tau)| = 0$, we define $t_2(\\tau)=0$.\n\nFinally, a crucial part of MSM validation is to check if the computed implied timescales are constant with respect to the choice of lag time $\\tau$. If an MSM is a good approximation of the underlying dynamics (i.e., it is Markovian), the physical relaxation timescales of the system should not depend on the lag time used to build the model, provided $\\tau$ is large enough to resolve the slow processes but not so large as to average them out. We assess this convergence by examining the set of slowest timescales $\\{ t_2(\\tau_k) \\}$. We focus on the latter half of these values, corresponding to larger lag times where Markovianity is more likely to hold. We compute the relative range of this subset, $(t_{\\max} - t_{\\min})/\\bar{t}$, and compare it to a given tolerance $\\epsilon$. If the relative range is within the tolerance, we conclude that the slowest timescale has converged. A special case is handled where the mean timescale $\\bar{t}$ is zero.\n\nThis entire procedure is applied to each of the three test cases, and the results are compiled into the required output format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import expm\n\ndef solve():\n    \"\"\"\n    Computes and validates implied timescales for Markov state models.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1 (happy path, reversible symmetric clusters)\n        {\n            \"n_states\": 4,\n            \"k_rules\": lambda i, j: 6.0 if (i in {0, 1} and j in {0, 1}) or (i in {2, 3} and j in {2, 3}) else 0.2,\n            \"lag_times\": [1.0, 2.0, 5.0, 10.0],\n            \"perturbations\": [0.02, 0.01, 0.005, 0.002],\n            \"epsilon\": 0.1,\n        },\n        # Test case 2 (edge case, 3-state asymmetric ring)\n        {\n            \"n_states\": 3,\n            \"k_rules\": {\n                (0, 1): 1.5, (1, 2): 1.5, (2, 0): 1.5,\n                (1, 0): 1.2, (2, 1): 1.2, (0, 2): 0.8\n            },\n            \"lag_times\": [0.5, 1.0, 2.0, 4.0],\n            \"perturbations\": [0.03, 0.03, 0.03, 0.03],\n            \"epsilon\": 0.05,\n        },\n        # Test case 3 (boundary condition, 5-state with slow bridge)\n        {\n            \"n_states\": 5,\n            \"k_rules\": lambda i, j: 4.0 if (i in {0, 1, 2} and j in {0, 1, 2}) or (i in {3, 4} and j in {3, 4}) else 0.05,\n            \"lag_times\": [0.1, 0.5, 1.0, 2.0],\n            \"perturbations\": [0.02, 0.01, 0.005, 0.003],\n            \"epsilon\": 0.15,\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        n = case[\"n_states\"]\n        k_rules = case[\"k_rules\"]\n        lag_times = case[\"lag_times\"]\n        perturbations = case[\"perturbations\"]\n        epsilon = case[\"epsilon\"]\n        \n        # 1. Construct the rate matrix K\n        K = np.zeros((n, n), dtype=float)\n        if callable(k_rules):\n            for i in range(n):\n                for j in range(n):\n                    if i != j:\n                        K[i, j] = k_rules(i, j)\n        else: # Dictionary-based rules for Test Case 2\n            for (i, j), val in k_rules.items():\n                K[i, j] = val\n        \n        for i in range(n):\n            K[i, i] = -np.sum(K[i, :])\n\n        # Define the off-diagonal pattern matrix N\n        N = np.ones((n, n)) - np.eye(n)\n        \n        slowest_implied_timescales = []\n        \n        for i, tau in enumerate(lag_times):\n            delta_tau = perturbations[i]\n            \n            # 2. Form T0(tau) = exp(tau * K)\n            T0_tau = expm(tau * K)\n            \n            # 3. Add perturbation\n            Delta_tau = delta_tau * N\n            T_tilde_tau = T0_tau + Delta_tau\n            \n            # 4. Project back to row-stochastic\n            # a. Clip to non-negativity\n            T_tilde_tau = np.maximum(T_tilde_tau, 0)\n            # b. Row normalize\n            row_sums = T_tilde_tau.sum(axis=1)\n            # Avoid division by zero for rows that are all zero.\n            non_zero_rows = row_sums > 0\n            T_tilde_tau[non_zero_rows] /= row_sums[non_zero_rows, np.newaxis]\n\n            # 5. Compute eigenvalues and find |lambda_2(tau)|\n            eigenvalues = np.linalg.eigvals(T_tilde_tau)\n            # Sort by descending magnitude\n            sorted_magnitudes = sorted(np.abs(eigenvalues), reverse=True)\n            \n            # The second largest magnitude eigenvalue\n            lambda2_mag = sorted_magnitudes[1]\n            \n            # 6. Compute slowest implied timescale t_2(tau)\n            if lambda2_mag > 0 and lambda2_mag < 1.0: # Avoid log(0) and log(1)\n                t2_tau = -tau / np.log(lambda2_mag)\n            else:\n                # As per problem spec, if |lambda_2| = 0, t_2 = 0.\n                # If |lambda_2| = 1, timescale is infinite, but this indicates an issue. Let's return a large number or 0.\n                # The problem handles log(0) by setting t=0. Let's handle log(1) by setting t to a large value, but let's stick to the problem\n                t2_tau = 0.0\n            \n            slowest_implied_timescales.append(t2_tau)\n        \n        # 7. Assess convergence on the latter half of the timescales\n        split_index = len(slowest_implied_timescales) // 2\n        later_timescales = np.array(slowest_implied_timescales[split_index:])\n        \n        is_converged = False\n        if len(later_timescales) > 0:\n            t_mean = np.mean(later_timescales)\n            if t_mean > 1e-9: # Check for non-zero mean to avoid division by zero\n                t_max = np.max(later_timescales)\n                t_min = np.min(later_timescales)\n                relative_range = (t_max - t_min) / t_mean\n                if relative_range <= epsilon:\n                    is_converged = True\n            else: # t_mean is effectively 0\n                # Convergence if and only if all values are identical (and thus 0)\n                if np.max(later_timescales) == np.min(later_timescales):\n                    is_converged = True\n\n        rounded_timescales = [round(t, 6) for t in slowest_implied_timescales]\n        all_results.append([rounded_timescales, is_converged])\n\n    # Final print statement in the exact required format.\n    # The default string representation of a list in Python matches the required format.\n    print(str(all_results).replace(\"'\", \"\").replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}