{
    "hands_on_practices": [
        {
            "introduction": "Once a Principal Component Analysis model has been constructed from a training ensemble, a primary application is to map new conformations into the learned low-dimensional space. This allows for the rapid classification of novel structures, such as those from a different simulation or a proposed drug-bound state. This fundamental exercise guides you through the mathematical derivation of this projection and underscores the critical importance of applying an identical preprocessing pipeline to ensure the scientific validity of your results .",
            "id": "3859043",
            "problem": "Consider a molecular dynamics ensemble of protein backbone conformations represented by a $d$-dimensional feature vector $x \\in \\mathbb{R}^{d}$ constructed from internal coordinate descriptors (e.g., torsion-angle derived features) after a fixed featurization and alignment procedure. Principal Component Analysis (PCA) is performed on a training set of $n$ conformations $\\{x_i\\}_{i=1}^n$ using the standard mean-centering preprocessing, where the training mean is $\\bar{x} \\in \\mathbb{R}^{d}$ and the empirical covariance matrix is defined by\n$$\nS \\equiv \\frac{1}{n-1} \\sum_{i=1}^{n} (x_{i} - \\bar{x})(x_{i} - \\bar{x})^{\\top}.\n$$\nLet $S$ have the orthonormal eigen-decomposition $S = V \\Lambda V^{\\top}$ with $V \\in \\mathbb{R}^{d \\times d}$ containing orthonormal eigenvectors as columns and $\\Lambda \\in \\mathbb{R}^{d \\times d}$ diagonal with eigenvalues ordered nonincreasingly. The first $k$ columns of $V$ are called the loading vectors for the leading $k$ principal components.\n\nStarting strictly from the above definitions and properties of orthonormal bases, derive the expression for the coefficients that represent a new conformation $x_{\\text{new}} \\in \\mathbb{R}^{d}$ in the $k$-dimensional principal component basis associated with the training covariance $S$. Then, explain why applying the identical preprocessing pipeline (including featurization, alignment, and mean-centering with the training mean $\\bar{x}$) to $x_{\\text{new}}$ is necessary to obtain scientifically meaningful projections.\n\nFinally, evaluate the derived expression for the following concrete case with mean-centering only (no scaling):\n- Dimension $d = 4$ and number of principal components $k = 2$.\n- The loading matrix for the first two principal components is\n$$\nV_{(2)} = \\begin{pmatrix}\n\\frac{1}{2} & \\frac{1}{2} \\\\\n\\frac{1}{2} & -\\frac{1}{2} \\\\\n\\frac{1}{2} & \\frac{1}{2} \\\\\n\\frac{1}{2} & -\\frac{1}{2}\n\\end{pmatrix}.\n$$\n- The training mean vector is\n$$\n\\bar{x} = \\begin{pmatrix}\n0.2 \\\\ -0.1 \\\\ 0.3 \\\\ 0.0\n\\end{pmatrix}.\n$$\n- The new conformation is\n$$\nx_{\\text{new}} = \\begin{pmatrix}\n0.3 \\\\ 0.1 \\\\ 0.0 \\\\ -0.1\n\\end{pmatrix}.\n$$\n\nCompute the two principal component coefficients for $x_{\\text{new}}$ using the derived expression and present the pair of values as a single row vector. Round your final numerical values to four significant figures. No physical units are required for the final numerical answer.",
            "solution": "The problem statement is analyzed and validated before proceeding to a solution.\n\n### Step 1: Problem Validation\n\nThe givens are extracted verbatim:\n- A $d$-dimensional feature vector $x \\in \\mathbb{R}^{d}$ represents a protein backbone conformation.\n- The training set is $\\{x_{i}\\}_{i=1}^{n}$.\n- Preprocessing is mean-centering with training mean $\\bar{x} \\in \\mathbb{R}^{d}$.\n- The empirical covariance matrix is $S \\equiv \\frac{1}{n-1} \\sum_{i=1}^{n} (x_{i} - \\bar{x})(x_{i} - \\bar{x})^{\\top}$.\n- The eigen-decomposition of $S$ is $S = V \\Lambda V^{\\top}$, with $V \\in \\mathbb{R}^{d \\times d}$ having orthonormal eigenvectors as columns and $\\Lambda$ being a diagonal matrix of non-increasing eigenvalues.\n- The first $k$ columns of $V$ are the loading vectors.\n- A new conformation is given by $x_{\\text{new}} \\in \\mathbb{R}^{d}$.\n- For the numerical case:\n  - $d = 4$ and $k = 2$.\n  - Loading matrix for the first two components: $V_{(2)} = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} \\\\ \\frac{1}{2} & -\\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{1}{2} \\\\ \\frac{1}{2} & -\\frac{1}{2} \\end{pmatrix}$.\n  - Training mean vector: $\\bar{x} = \\begin{pmatrix} 0.2 \\\\ -0.1 \\\\ 0.3 \\\\ 0.0 \\end{pmatrix}$.\n  - New conformation: $x_{\\text{new}} = \\begin{pmatrix} 0.3 \\\\ 0.1 \\\\ 0.0 \\\\ -0.1 \\end{pmatrix}$.\n\nThe problem is subjected to validation:\n- **Scientifically Grounded**: The problem describes a standard and fundamental application of Principal Component Analysis (PCA) to conformational data from molecular dynamics, a core technique in computational biophysics. The definitions of the covariance matrix, its eigen-decomposition, and principal components are standard and correct.\n- **Well-Posed**: The problem is well-posed. It asks for a standard derivation, a conceptual explanation, and a concrete calculation for which all necessary data are provided. The existence of a unique and meaningful solution is guaranteed.\n- **Objective**: The problem is stated in precise mathematical terms, free of ambiguity or subjectivity.\n- **Consistency**: The provided loading matrix $V_{(2)}$ consists of two columns. Let these be $v_1$ and $v_2$. We check for orthonormality: $v_1^\\top v_1 = (\\frac{1}{2})^2 \\times 4 = 1$, $v_2^\\top v_2 = (\\frac{1}{2})^2 + (-\\frac{1}{2})^2 + (\\frac{1}{2})^2 + (-\\frac{1}{2})^2 = 1$, and $v_1^\\top v_2 = \\frac{1}{4} - \\frac{1}{4} + \\frac{1}{4} - \\frac{1}{4} = 0$. The columns are indeed orthonormal, confirming the internal consistency of the problem statement.\n\nThe problem is deemed valid as it is scientifically sound, self-contained, consistent, and well-posed.\n\n### Step 2: Solution\n\n#### Derivation of the Principal Component Coefficients\n\nThe principal components, which are the columns of the matrix $V$, form an orthonormal basis for the space $\\mathbb{R}^d$. Let these column vectors be denoted by $\\{v_j\\}_{j=1}^d$. The core idea of PCA is to describe the data's variance relative to its mean, $\\bar{x}$. Therefore, any new data point, $x_{\\text{new}}$, must first be transformed into this centered coordinate system. The centered vector is:\n$$\nx'_{\\text{new}} = x_{\\text{new}} - \\bar{x}\n$$\nWe wish to express this centered vector, $x'_{\\text{new}}$, as a linear combination of the first $k$ principal component basis vectors. The full representation in the $d$-dimensional basis is:\n$$\nx'_{\\text{new}} = \\sum_{j=1}^{d} c_j v_j\n$$\nwhere $c_j$ is the coefficient, or score, corresponding to the $j$-th principal component $v_j$.\n\nTo find the value of a specific coefficient, say $c_i$, we utilize the orthonormality of the basis vectors. Taking the dot product (inner product) of both sides of the equation with the vector $v_i$:\n$$\nv_i^{\\top} x'_{\\text{new}} = v_i^{\\top} \\left( \\sum_{j=1}^{d} c_j v_j \\right)\n$$\nBy the linearity of the dot product, we can move $v_i^{\\top}$ inside the summation:\n$$\nv_i^{\\top} x'_{\\text{new}} = \\sum_{j=1}^{d} c_j (v_i^{\\top} v_j)\n$$\nSince the basis $\\{v_j\\}$ is orthonormal, the inner product $v_i^{\\top} v_j$ is equal to the Kronecker delta, $\\delta_{ij}$, which is $1$ if $i=j$ and $0$ if $i \\neq j$.\n$$\nv_i^{\\top} x'_{\\text{new}} = \\sum_{j=1}^{d} c_j \\delta_{ij}\n$$\nThe sum on the right-hand side collapses to a single term, $c_i$:\n$$\nc_i = v_i^{\\top} x'_{\\text{new}} = v_i^{\\top} (x_{\\text{new}} - \\bar{x})\n$$\nThis expression gives the coefficient for the $i$-th principal component. To find the set of coefficients for the first $k$ components, we collect the coefficients $\\{c_1, c_2, \\dots, c_k\\}$. This can be expressed in matrix form. Let $V_{(k)} \\in \\mathbb{R}^{d \\times k}$ be the matrix whose columns are the first $k$ eigenvectors, $v_1, \\dots, v_k$. Let $c \\in \\mathbb{R}^k$ be the column vector of coefficients. Then:\n$$\nc = \\begin{pmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_k \\end{pmatrix} = \\begin{pmatrix} v_1^{\\top} \\\\ v_2^{\\top} \\\\ \\vdots \\\\ v_k^{\\top} \\end{pmatrix} (x_{\\text{new}} - \\bar{x}) = V_{(k)}^{\\top} (x_{\\text{new}} - \\bar{x})\n$$\nThis is the derived expression for the coefficients that represent the new conformation in the $k$-dimensional principal component basis.\n\n#### Justification for Preprocessing Pipeline\n\nThe application of an identical preprocessing pipeline is a critical requirement for scientific validity.\n1.  **Shared Frame of Reference**: PCA identifies a new coordinate system (the principal components) whose origin is the mean of the training data, $\\bar{x}$, and whose axes are aligned with the directions of maximum variance within that training data. The covariance matrix $S$ and its eigenvectors $V$ are defined entirely with respect to this statistical context. Projecting a new data point $x_{\\text{new}}$ onto this basis is only meaningful if $x_{\\text{new}}$ is first expressed in the same coordinate system. Subtracting the *training mean* $\\bar{x}$ from $x_{\\text{new}}$ translates the new point into this shared frame of reference. Failure to do so would be analogous to measuring a location relative to one coordinate origin (e.g., the global origin) but plotting it on a map centered elsewhere, yielding a meaningless position.\n2.  **Consistent Feature Space**: The featurization and alignment procedures are even more fundamental. They define the very vector space $\\mathbb{R}^d$ in which the analysis takes place. A conformation is not inherently a vector; it is transformed into one by a specific set of rules (e.g., calculating all $\\phi/\\psi$ torsion angles). If $x_{\\text{new}}$ were featurized or aligned differently from the training set, it would exist in a different feature space altogether. Projecting a vector from one space onto a basis defined in another is a mathematically and scientifically invalid operation. Therefore, to compare or represent $x_{\\text{new}}$ in the context of the conformational landscape learned from the training set, it must undergo the exact same sequence of transformations.\n\n#### Numerical Evaluation\n\nWe are given:\n- Dimension $d = 4$ and number of principal components $k = 2$.\n- The loading matrix $V_{(2)} = \\begin{pmatrix} 0.5 & 0.5 \\\\ 0.5 & -0.5 \\\\ 0.5 & 0.5 \\\\ 0.5 & -0.5 \\end{pmatrix}$.\n- The training mean $\\bar{x} = \\begin{pmatrix} 0.2 \\\\ -0.1 \\\\ 0.3 \\\\ 0.0 \\end{pmatrix}$.\n- The new conformation $x_{\\text{new}} = \\begin{pmatrix} 0.3 \\\\ 0.1 \\\\ 0.0 \\\\ -0.1 \\end{pmatrix}$.\n\nFirst, we compute the centered vector $x'_{\\text{new}} = x_{\\text{new}} - \\bar{x}$:\n$$\nx'_{\\text{new}} = \\begin{pmatrix} 0.3 \\\\ 0.1 \\\\ 0.0 \\\\ -0.1 \\end{pmatrix} - \\begin{pmatrix} 0.2 \\\\ -0.1 \\\\ 0.3 \\\\ 0.0 \\end{pmatrix} = \\begin{pmatrix} 0.3 - 0.2 \\\\ 0.1 - (-0.1) \\\\ 0.0 - 0.3 \\\\ -0.1 - 0.0 \\end{pmatrix} = \\begin{pmatrix} 0.1 \\\\ 0.2 \\\\ -0.3 \\\\ -0.1 \\end{pmatrix}\n$$\nNext, we compute the coefficient vector $c \\in \\mathbb{R}^2$ using the derived expression $c = V_{(2)}^{\\top} x'_{\\text{new}}$. The transpose of $V_{(2)}$ is:\n$$\nV_{(2)}^{\\top} = \\begin{pmatrix} 0.5 & 0.5 & 0.5 & 0.5 \\\\ 0.5 & -0.5 & 0.5 & -0.5 \\end{pmatrix}\n$$\nNow we perform the matrix-vector multiplication:\n$$\nc = \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix} = \\begin{pmatrix} 0.5 & 0.5 & 0.5 & 0.5 \\\\ 0.5 & -0.5 & 0.5 & -0.5 \\end{pmatrix} \\begin{pmatrix} 0.1 \\\\ 0.2 \\\\ -0.3 \\\\ -0.1 \\end{pmatrix}\n$$\nThe first coefficient $c_1$ is:\n$$\nc_1 = (0.5)(0.1) + (0.5)(0.2) + (0.5)(-0.3) + (0.5)(-0.1) = 0.05 + 0.10 - 0.15 - 0.05 = -0.05\n$$\nThe second coefficient $c_2$ is:\n$$\nc_2 = (0.5)(0.1) + (-0.5)(0.2) + (0.5)(-0.3) + (-0.5)(-0.1) = 0.05 - 0.10 - 0.15 + 0.05 = -0.15\n$$\nThe resulting coefficient vector is $c = \\begin{pmatrix} -0.05 \\\\ -0.15 \\end{pmatrix}$. The problem requires the answer as a single row vector, with values rounded to four significant figures.\n- $-0.05$ becomes $-0.05000$.\n- $-0.15$ becomes $-0.1500$.\nThe final row vector is $\\begin{pmatrix} -0.05000 & -0.1500 \\end{pmatrix}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-0.05000 & -0.1500\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Beyond analyzing existing data, Principal Component Analysis can serve as a powerful generative tool to explore a system's conformational landscape more efficiently. By drawing samples from the low-dimensional distribution defined by the principal components and back-mapping them to Cartesian coordinates, we can generate novel structures. This hands-on coding practice walks you through this generative process, including the crucial final step of filtering out physically implausible structures via steric checks to ensure the biological relevance of the generated ensemble .",
            "id": "3859035",
            "problem": "You are given a task that integrates principal component analysis (PCA) with conformational sampling for a coarse-grained peptide backbone model in computational chemical biology. The goal is to derive, implement, and validate a method to generate synthetic structures by sampling in principal component space and back-mapping to Cartesian coordinates, followed by filtering physically implausible conformations using simple steric checks. All molecular distances are to be treated in Ångströms, and any random number generation must be deterministic via a fixed pseudo-random seed.\n\nStart from the following fundamental base, which must guide your derivation and algorithmic design:\n- The input conformational ensemble is represented as a set of mean-free coordinate fluctuations of a molecular system, where each conformation is a vector of Cartesian coordinates of $N$ atoms, resulting in a vector of length $3N$.\n- The ensemble covariance matrix is defined from the mean-free data matrix as a symmetric, positive semi-definite matrix, and its eigen-decomposition yields orthonormal eigenvectors and non-negative eigenvalues.\n- Principal component analysis (PCA) is defined as the orthogonal transformation that diagonalizes the covariance matrix, projecting mean-free coordinates onto a basis where coordinate variances are ordered and decoupled.\n\nYou must implement the following, deriving the necessary transformations strictly from the base above:\n1. Construct a deterministic training ensemble of $T$ conformations for a backbone motif with $N = 6$ atoms labeled as $\\{N, CA, C, N, CA, C\\}$ arranged linearly along a backbone graph with bonds between successive atoms. Use a fixed pseudo-random seed, apply small random rigid rotations about the $z$-axis and independent Gaussian positional noise to a physically plausible mean structure, with all distances in Ångströms. The backbone graph is a simple chain such that the graph distance between atom indices $i$ and $j$ is $|i - j|$.\n2. Compute the $3N \\times 3N$ covariance matrix from the mean-free flattened coordinate ensemble and perform an eigen-decomposition to obtain ordered principal components. Use the ordering of eigenvalues from largest to smallest.\n3. Sample synthetic structures by drawing Gaussian coefficients in the principal component space of dimension $k$, with zero mean and component-wise variances proportional to the corresponding top $k$ eigenvalues, scaled by a dimensionless factor $s$. Back-map these samples to the full $3N$-dimensional Cartesian coordinate space using the top $k$ principal component directions and re-add the mean.\n4. Filter unphysical conformations using a simple steric check: for all atom pairs with graph distance greater than or equal to $3$ (i.e., $|i - j| \\ge 3$), reject a conformation if any Euclidean interatomic distance is strictly less than $\\alpha \\cdot (r_i + r_j)$, where $r_i$ and $r_j$ are the van der Waals radii of atoms $i$ and $j$ respectively, with $r_{\\mathrm{N}} = 1.55$ Å and $r_{\\mathrm{C}} = 1.70$ Å and $r_{\\mathrm{CA}} = r_{\\mathrm{C}}$. The angle unit for any rotation must be radians. Report an acceptance fraction defined as the number of sampled conformations that pass the steric filter divided by the total number of samples, expressed as a decimal.\n\nTest suite and required outputs:\n- Use a single, deterministic training ensemble with $T = 1000$ conformations generated from the mean structure with Gaussian positional noise of standard deviation $0.1$ Å and rotation angle standard deviation $0.05$ radians applied about the $z$-axis. All randomness must be controlled by a fixed pseudo-random seed.\n- Apply the method to the following four test cases, each specified as a tuple $(k, M, s, \\alpha)$, where $k$ is the number of principal components retained, $M$ is the number of synthetic samples to draw, $s$ is the dimensionless scaling of component variances, and $\\alpha$ is the steric scaling factor:\n    - Case $1$: $(k = 2, M = 200, s = 1.0, \\alpha = 1.0)$.\n    - Case $2$: $(k = 1, M = 200, s = 0.5, \\alpha = 1.0)$.\n    - Case $3$: $(k = 18, M = 200, s = 0.0, \\alpha = 1.0)$, where $18 = 3N$.\n    - Case $4$: $(k = 4, M = 200, s = 3.0, \\alpha = 1.0)$.\n- For each case, compute the acceptance fraction as a decimal float. Your program should produce a single line of output containing the results as a comma-separated list of four decimal numbers rounded to three decimal places and enclosed in square brackets (e.g., $[x_1,x_2,x_3,x_4]$).\n\nYour implementation must be a complete, runnable program that performs the ensemble generation, PCA, sampling, back-mapping, steric filtering, and aggregation of acceptance fractions for the four test cases, and prints the final single-line output in the specified format.",
            "solution": "The posed problem is valid as it is scientifically grounded, well-posed, objective, and contains all necessary information to derive a unique, verifiable solution. It outlines a standard procedure in computational biophysics for analyzing and generating molecular structures using principal component analysis (PCA), a cornerstone of statistical data analysis. We shall now proceed with a formal derivation and solution.\n\nThe overarching goal is to generate a set of synthetic molecular conformations from a low-dimensional latent space defined by PCA and to filter these conformations for physical plausibility. The methodology involves four primary stages: 1. Generation of a training conformational ensemble. 2. Application of PCA to identify the principal modes of structural variation. 3. Stochastic sampling in the principal component space and back-mapping to Cartesian coordinates. 4. Steric filtering of the generated structures.\n\nLet the system consist of $N$ atoms. A single conformation is represented by a vector $\\mathbf{x} \\in \\mathbb{R}^{3N}$ containing the flattened Cartesian coordinates $(x_1, y_1, z_1, x_2, y_2, z_2, \\dots, x_N, y_N, z_N)$. All coordinates and distances are in units of Ångströms (Å).\n\n1.  **Ensemble Generation and Covariance Analysis**\n\nWe begin by generating a training ensemble of $T$ conformations, denoted as $\\{\\mathbf{x}^{(m)}\\}_{m=1}^T$. This ensemble is created by applying stochastic perturbations to a reference mean structure. The coordinates of the $m$-th conformation form the vector $\\mathbf{x}^{(m)}$. These vectors are collected as rows of a data matrix $X$ of size $T \\times 3N$.\n\nThe first step in PCA is to center the data. We compute the mean conformation $\\bar{\\mathbf{x}}$ over the ensemble:\n$$\n\\bar{\\mathbf{x}} = \\frac{1}{T} \\sum_{m=1}^{T} \\mathbf{x}^{(m)}\n$$\nThe mean-free (or centered) conformations are given by $\\bar{\\mathbf{x}}^{(m)} = \\mathbf{x}^{(m)} - \\bar{\\mathbf{x}}$. The sample covariance matrix $C$, a $3N \\times 3N$ symmetric positive semi-definite matrix, is then computed from these centered vectors:\n$$\nC = \\frac{1}{T-1} \\sum_{m=1}^{T} \\bar{\\mathbf{x}}^{(m)} (\\bar{\\mathbf{x}}^{(m)})^\\top\n$$\nThe covariance matrix quantifies the correlated fluctuations between all pairs of atomic coordinates.\n\n2.  **Principal Component Analysis**\n\nPCA is performed by finding the eigenvalues and eigenvectors of the covariance matrix $C$. This is an eigen-decomposition problem:\n$$\nC \\mathbf{v}_i = \\lambda_i \\mathbf{v}_i, \\quad i = 1, \\dots, 3N\n$$\nHere, $\\lambda_i$ are the eigenvalues and $\\mathbf{v}_i$ are the corresponding eigenvectors. The eigenvectors, called principal components (PCs), form an orthonormal basis that diagonalizes the covariance matrix. Each eigenvalue $\\lambda_i$ represents the variance of the data projected onto the corresponding eigenvector $\\mathbf{v}_i$. The PCs are ordered according to their corresponding eigenvalues in descending order, $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_{3N} \\ge 0$. The first few PCs with the largest eigenvalues capture the dominant, large-amplitude motions within the conformational ensemble. Let $V$ be the orthogonal matrix whose columns are the ordered eigenvectors, $V = [\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_{3N}]$.\n\nAny mean-free conformation $\\bar{\\mathbf{x}}^{(m)}$ can be exactly represented as a linear combination of the PCs:\n$$\n\\bar{\\mathbf{x}}^{(m)} = \\sum_{i=1}^{3N} p_{i}^{(m)} \\mathbf{v}_i = V \\mathbf{p}^{(m)}\n$$\nwhere $\\mathbf{p}^{(m)}$ is the vector of principal coordinates for conformation $m$, obtained by projection: $\\mathbf{p}^{(m)} = V^\\top \\bar{\\mathbf{x}}^{(m)}$.\n\n3.  **Synthetic Conformation Generation**\n\nWe can generate new, synthetic conformations by sampling in the low-dimensional space spanned by the top $k$ principal components, where $k \\le 3N$. A new vector of principal coordinates, $\\mathbf{p}_{\\text{synth}}$, is generated. Its first $k$ components, $\\{c_i\\}_{i=1}^k$, are drawn from independent Gaussian distributions, while the remaining $3N-k$ components are set to zero. The distribution for each component $c_i$ is chosen to mimic the variance observed in the training data:\n$$\nc_i \\sim \\mathcal{N}(\\mu=0, \\sigma^2=s \\lambda_i), \\quad \\text{for } i=1, \\dots, k\n$$\nHere, $s$ is a dimensionless scaling factor that allows modulation of the sampled variance. The full vector of synthetic principal coordinates is $\\mathbf{p}_{\\text{synth}} = (c_1, c_2, \\dots, c_k, 0, \\dots, 0)^T$.\n\nThe synthetic conformation is then reconstructed in Cartesian coordinates via back-mapping. First, the mean-free synthetic conformation $\\bar{\\mathbf{x}}_{\\text{synth}}$ is obtained:\n$$\n\\bar{\\mathbf{x}}_{\\text{synth}} = \\sum_{i=1}^{k} c_i \\mathbf{v}_i\n$$\nThen, the mean conformation is added back to yield the final structure:\n$$\n\\mathbf{x}_{\\text{synth}} = \\bar{\\mathbf{x}}_{\\text{synth}} + \\bar{\\mathbf{x}}\n$$\n\n4.  **Steric Filtering**\n\nThe generated structures, particularly when sampled with large variance ($s > 1$) or from a limited number of PCs, may not be physically realistic. A common issue is steric clashes, where non-bonded atoms are unphysically close. We apply a filter to reject such structures. A conformation is deemed invalid if the Euclidean distance $d_{ij}$ between any pair of atoms $(i, j)$ that are not closely connected in the molecular graph violates a distance criterion.\n\nFor the given backbone with $N=6$ atoms, the graph distance is $|i-j|$. The check is performed for all pairs with $|i-j| \\ge 3$. A conformation is rejected if for any such pair:\n$$\nd_{ij} < \\alpha \\cdot (r_i + r_j)\n$$\nwhere $r_i$ and $r_j$ are the van der Waals radii of atoms $i$ and $j$, and $\\alpha$ is a scaling factor. The specified radii are $r_{\\mathrm{N}} = 1.55$ Å and $r_{\\mathrm{C}} = r_{\\mathrm{CA}} = 1.70$ Å. The acceptance fraction is the ratio of conformations that pass this filter to the total number of generated samples, $M$.\n\nFor the specific case where the variance scaling factor $s=0$, all generated coefficients $c_i$ will be exactly $0$. Consequently, $\\bar{\\mathbf{x}}_{\\text{synth}}$ will be the zero vector, and every synthetic conformation $\\mathbf{x}_{\\text{synth}}$ will be identical to the mean conformation $\\bar{\\mathbf{x}}$. The acceptance fraction will then be $1.0$ if the mean structure itself is sterically valid, and $0.0$ otherwise. Given that the mean structure is an average of physically plausible conformations, it is expected to be valid, leading to an acceptance fraction of $1.0$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs PCA-based conformational sampling and steric filtering for a coarse-grained peptide model.\n    \"\"\"\n    #\n    # --- GLOBAL PARAMETERS AND CONSTANTS ---\n    #\n    RANDOM_SEED = 0  # Fixed seed for deterministic randomness\n    \n    # Ensemble generation parameters\n    T = 1000  # Number of training conformations\n    N = 6     # Number of atoms\n    POS_NOISE_STD = 0.1  # Std dev for Gaussian positional noise (Å)\n    ROT_NOISE_STD = 0.05  # Std dev for z-axis rotation noise (radians)\n\n    # Atom properties\n    ATOM_LABELS = ['N', 'CA', 'C', 'N', 'CA', 'C']\n    VDW_RADII_MAP = {'N': 1.55, 'C': 1.70, 'CA': 1.70}\n    ATOM_RADII = np.array([VDW_RADII_MAP[label] for label in ATOM_LABELS])\n\n    # Bond lengths for mean structure (Å)\n    BOND_N_CA = 1.46\n    BOND_CA_C = 1.52\n    BOND_C_N_PEPTIDE = 1.33\n\n    # Test cases: (k, M, s, alpha)\n    test_cases = [\n        (2, 200, 1.0, 1.0),\n        (1, 200, 0.5, 1.0),\n        (18, 200, 0.0, 1.0),\n        (4, 200, 3.0, 1.0),\n    ]\n\n    #\n    # --- STEP 1: GENERATE TRAINING ENSEMBLE ---\n    #\n    # Define a simple, reproducible mean structure (linear along x-axis)\n    mean_structure_coords = np.zeros((N, 3))\n    x_pos = 0.0\n    mean_structure_coords[1] = [x_pos := x_pos + BOND_N_CA, 0, 0]\n    mean_structure_coords[2] = [x_pos := x_pos + BOND_CA_C, 0, 0]\n    mean_structure_coords[3] = [x_pos := x_pos + BOND_C_N_PEPTIDE, 0, 0]\n    mean_structure_coords[4] = [x_pos := x_pos + BOND_N_CA, 0, 0]\n    mean_structure_coords[5] = [x_pos := x_pos + BOND_CA_C, 0, 0]\n    \n    rng_ensemble = np.random.default_rng(RANDOM_SEED)\n\n    ensemble_matrix_X = np.zeros((T, N * 3))\n\n    for i in range(T):\n        # Generate random z-rotation\n        theta = rng_ensemble.normal(0, ROT_NOISE_STD)\n        cos_t, sin_t = np.cos(theta), np.sin(theta)\n        rotation_matrix = np.array([\n            [cos_t, -sin_t, 0],\n            [sin_t, cos_t, 0],\n            [0, 0, 1]\n        ])\n        \n        # Apply rotation to the mean structure\n        rotated_coords = mean_structure_coords @ rotation_matrix.T\n        \n        # Add independent Gaussian positional noise\n        noise = rng_ensemble.normal(0, POS_NOISE_STD, size=(N, 3))\n        perturbed_coords = rotated_coords + noise\n        \n        # Store flattened coordinates\n        ensemble_matrix_X[i, :] = perturbed_coords.flatten()\n\n    #\n    # --- STEP 2: PERFORM PRINCIPAL COMPONENT ANALYSIS ---\n    #\n    # Compute mean conformation and mean-free data\n    mean_conformation = np.mean(ensemble_matrix_X, axis=0)\n    mean_free_X = ensemble_matrix_X - mean_conformation\n\n    # Compute covariance matrix\n    # Using T-1 for the unbiased sample covariance estimator\n    covariance_matrix = (mean_free_X.T @ mean_free_X) / (T - 1)\n    \n    # Eigen-decomposition of the symmetric covariance matrix\n    # np.linalg.eigh returns eigenvalues in ascending order, so we reverse them\n    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n    \n    # Sort eigenvalues and eigenvectors in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n\n    results = []\n    \n    rng_sampling = np.random.default_rng(RANDOM_SEED + 1) # Separate RNG for sampling\n\n    #\n    # --- STEPS 3 and 4: SAMPLING, BACK-MAPPING, AND FILTERING LOOP ---\n    #\n    for k, M, s, alpha in test_cases:\n        passed_count = 0\n        for _ in range(M):\n            # Step 3: Sample in PC space and back-map\n            \n            # Sample coefficients for the top k components\n            # Handle s=0 case where variance is 0, avoiding sqrt of negative tiny numbers\n            stdevs = np.sqrt(np.maximum(0, s * sorted_eigenvalues[:k]))\n            coeffs = rng_sampling.normal(0, stdevs)\n\n            # Reconstruct mean-free conformation\n            mean_free_synth = sorted_eigenvectors[:, :k] @ coeffs\n            \n            # Add mean to get full conformation\n            synth_conformation_flat = mean_free_synth + mean_conformation\n            synth_conformation = synth_conformation_flat.reshape((N, 3))\n\n            # Step 4: Steric filtering\n            is_valid = True\n            for i in range(N):\n                for j in range(i + 1, N):\n                    # Check only pairs with graph distance >= 3\n                    if abs(i - j) >= 3:\n                        dist_sq = np.sum((synth_conformation[i] - synth_conformation[j])**2)\n                        min_dist_sq = (alpha * (ATOM_RADII[i] + ATOM_RADII[j]))**2\n                        \n                        if dist_sq  min_dist_sq:\n                            is_valid = False\n                            break\n                if not is_valid:\n                    break\n            \n            if is_valid:\n                passed_count += 1\n        \n        acceptance_fraction = passed_count / M\n        results.append(acceptance_fraction)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.3f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The success of any PCA application hinges on a meaningful numerical representation of the physical system, a process known as featurization. A classic pitfall in analyzing molecular simulations is the naive use of raw dihedral angles, whose periodic nature is incompatible with PCA's linear assumptions. This thought experiment uses a simple two-state model to provide a clear, quantitative demonstration of why this approach fails and how the standard sine-cosine embedding correctly preserves the circular topology of angular data, leading to a meaningful low-dimensional representation .",
            "id": "3859111",
            "problem": "A single backbone torsion angle $\\,\\phi\\,$ from a peptide exhibits two metastable conformational basins of equal population, one centered at $\\,\\phi=0\\,$ and the other at $\\,\\phi=\\pi\\,$. Assume the thermal spread within each basin is negligible relative to the separation between basins, so that the empirical distribution can be modeled as a mixture of two point masses at $\\,\\phi=0\\,$ and $\\,\\phi=\\pi\\,$ with equal probability $\\,\\tfrac{1}{2}\\,$ each. Angles are measured in radians.\n\nPrincipal Component Analysis (PCA) is defined as the orthogonal linear transform that diagonalizes the covariance matrix of mean-centered data; the principal component variances are the eigenvalues of the covariance matrix. Using only the core definitions of covariance and PCA, consider two encodings of the same conformational ensemble:\n\n- Raw-angle encoding: the scalar variable $\\,x=\\phi\\,$.\n- Sine-cosine embedding: the vector variable $\\,\\mathbf{y}=\\big(\\cos\\phi,\\sin\\phi\\big)^{\\top}$.\n\nFor each encoding, construct the mean-centered covariance matrix of the random variable under the given two-state model, and determine the largest eigenvalue (the variance along the first principal component). Then, compute the exact ratio\n$$\nr \\;=\\; \\frac{\\lambda_{\\text{raw}}}{\\lambda_{\\text{sincos}}}\\,,\n$$\nwhere $\\,\\lambda_{\\text{raw}}\\,$ is the largest eigenvalue for the raw-angle encoding and $\\,\\lambda_{\\text{sincos}}\\,$ is the largest eigenvalue for the sine-cosine embedding. Provide your final answer as an exact symbolic expression. No rounding is required.",
            "solution": "The problem is validated as scientifically sound, well-posed, and self-contained, presenting a classic comparative analysis in feature engineering for molecular simulations.\n\n### Part 1: Raw-Angle Encoding\n\nLet the random variable for the raw-angle encoding be $x = \\phi$. The distribution of $x$ consists of two point masses:\n$$\nx = \\begin{cases} 0  \\text{with probability } 1/2 \\\\ \\pi  \\text{with probability } 1/2 \\end{cases}\n$$\nThe mean (expected value) of $x$ is:\n$$\nE[x] = \\frac{1}{2}(0) + \\frac{1}{2}(\\pi) = \\frac{\\pi}{2}\n$$\nSince this is a scalar variable, the $1 \\times 1$ covariance matrix is simply the variance, $\\text{Var}(x)$. The variance is given by $\\text{Var}(x) = E[x^2] - (E[x])^2$.\nFirst, we compute the expected value of $x^2$:\n$$\nE[x^2] = \\frac{1}{2}(0^2) + \\frac{1}{2}(\\pi^2) = \\frac{\\pi^2}{2}\n$$\nNow, we can calculate the variance, which is the sole eigenvalue $\\lambda_{\\text{raw}}$:\n$$\n\\lambda_{\\text{raw}} = \\text{Var}(x) = E[x^2] - (E[x])^2 = \\frac{\\pi^2}{2} - \\left(\\frac{\\pi}{2}\\right)^2 = \\frac{\\pi^2}{2} - \\frac{\\pi^2}{4} = \\frac{\\pi^2}{4}\n$$\n\n### Part 2: Sine-Cosine Embedding\n\nLet the random vector for the sine-cosine embedding be $\\mathbf{y} = (\\cos\\phi, \\sin\\phi)^{\\top}$. We evaluate $\\mathbf{y}$ for the two states of $\\phi$:\n- If $\\phi=0$, then $\\mathbf{y}_1 = (\\cos 0, \\sin 0)^{\\top} = (1, 0)^{\\top}$.\n- If $\\phi=\\pi$, then $\\mathbf{y}_2 = (\\cos \\pi, \\sin \\pi)^{\\top} = (-1, 0)^{\\top}$.\n\nThe distribution of $\\mathbf{y}$ is:\n$$\n\\mathbf{y} = \\begin{cases} (1, 0)^{\\top}  \\text{with probability } 1/2 \\\\ (-1, 0)^{\\top}  \\text{with probability } 1/2 \\end{cases}\n$$\nThe mean vector of $\\mathbf{y}$ is:\n$$\nE[\\mathbf{y}] = \\frac{1}{2}\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\frac{1}{2}\\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThe data is already mean-centered. The $2 \\times 2$ covariance matrix is given by $\\mathbf{C}_\\mathbf{y} = E[\\mathbf{y}\\mathbf{y}^{\\top}] - E[\\mathbf{y}]E[\\mathbf{y}]^{\\top}$. Since $E[\\mathbf{y}]$ is the zero vector, this simplifies to $\\mathbf{C}_\\mathbf{y} = E[\\mathbf{y}\\mathbf{y}^{\\top}]$.\nWe calculate the outer products for each state:\n- For $\\mathbf{y}_1$: $\\mathbf{y}_1\\mathbf{y}_1^{\\top} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1  0 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}$.\n- For $\\mathbf{y}_2$: $\\mathbf{y}_2\\mathbf{y}_2^{\\top} = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} -1  0 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}$.\n\nThe expected value of the outer product is:\n$$\n\\mathbf{C}_\\mathbf{y} = E[\\mathbf{y}\\mathbf{y}^{\\top}] = \\frac{1}{2} \\left( \\mathbf{y}_1\\mathbf{y}_1^{\\top} \\right) + \\frac{1}{2} \\left( \\mathbf{y}_2\\mathbf{y}_2^{\\top} \\right) = \\frac{1}{2}\\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} + \\frac{1}{2}\\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}\n$$\nThe covariance matrix is a diagonal matrix. Its eigenvalues are the diagonal elements, which are $1$ and $0$. The largest eigenvalue is:\n$$\n\\lambda_{\\text{sincos}} = 1\n$$\n\n### Part 3: Ratio Calculation\n\nFinally, we compute the ratio $r$:\n$$\nr = \\frac{\\lambda_{\\text{raw}}}{\\lambda_{\\text{sincos}}} = \\frac{\\pi^2/4}{1} = \\frac{\\pi^2}{4}\n$$",
            "answer": "$$\n\\boxed{\\frac{\\pi^2}{4}}\n$$"
        }
    ]
}