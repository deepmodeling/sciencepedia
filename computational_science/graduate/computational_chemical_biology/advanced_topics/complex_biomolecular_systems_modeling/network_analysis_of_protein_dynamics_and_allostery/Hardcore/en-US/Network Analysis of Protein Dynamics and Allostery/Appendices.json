{
    "hands_on_practices": [
        {
            "introduction": "A foundational decision in modeling a protein as a network is the choice of nodes. Do we represent the system at the fine-grained level of individual atoms, or do we use a 'coarse-grained' view where each amino acid residue is a single node? This choice profoundly impacts the resulting network's topology and our subsequent biological interpretation. This practice guides you through constructing both atom-level and residue-level networks from the same molecular structure to quantitatively compare how this representational choice affects fundamental properties like degree distribution, clustering, and path lengths .",
            "id": "3855794",
            "problem": "You are given a single three-dimensional structural model for a short polypeptide segment approximating an ideal alpha helix. From this structure, construct two undirected, simple graphs: an atom-level graph and a residue-level graph. The atom-level graph has one node for each heavy atom, and the residue-level graph has one node for each amino acid residue. For both graphs, edges represent spatial contacts inferred from Euclidean distances. Your program must compute quantitative differences between the two graphs in terms of degree distributions, clustering, and path lengths for a specified set of distance cutoffs. All distances must be treated in Angstrom (Å), and graph-theoretic quantities are dimensionless.\n\nFundamental base and definitions to use:\n- The Euclidean distance between two points with Cartesian coordinates is defined by $d(\\mathbf{x},\\mathbf{y}) = \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + (x_3 - y_3)^2}$.\n- A simple undirected graph $G = (V,E)$ has a set of nodes $V$ and an edge set $E$ with no self-loops or multi-edges.\n- The adjacency matrix $A \\in \\{0,1\\}^{n \\times n}$ of $G$ is defined by $A_{ij} = 1$ if there is an edge between nodes $i$ and $j$, and $A_{ij} = 0$ otherwise, with $A_{ii} = 0$.\n- The degree $k_i$ of node $i$ is defined as $k_i = \\sum_{j=1}^{n} A_{ij}$.\n- The local clustering coefficient $C_i$ for node $i$ with degree $k_i \\ge 2$ is $C_i = \\dfrac{2 \\, m_i}{k_i (k_i - 1)}$, where $m_i$ is the number of edges among the neighbors of node $i$. For $k_i  2$, define $C_i = 0$.\n- The average clustering coefficient $\\bar{C}$ is defined as $\\bar{C} = \\dfrac{1}{n} \\sum_{i=1}^{n} C_i$.\n- The shortest path length between two nodes is the minimum number of edges required to connect them, computed on the largest connected component of the graph. If the largest component has size $s  2$, define the average shortest path length as $0$.\n- The empirical degree distribution $P(k)$ is defined by $P(k) = \\dfrac{1}{n} \\sum_{i=1}^{n} \\mathbf{1}_{\\{k_i = k\\}}$ for integer $k \\ge 0$. The total variation distance between two discrete distributions $P$ and $Q$ supported on $\\{0,1,\\dots,K_{\\max}\\}$ is $D_{\\mathrm{TV}}(P,Q) = \\dfrac{1}{2} \\sum_{k=0}^{K_{\\max}} |P(k) - Q(k)|$.\n\nStructure generation:\n- Consider $N_{\\mathrm{res}} = 12$ residues arranged as an ideal alpha helix. The position of the alpha-carbon (CA) of residue $i$ is parameterized by an axial rise per residue $h = 1.5$ Å, a helical radius $R = 2.3$ Å, and a rotation per residue $\\Delta\\theta = 100^\\circ$ in radians, such that $\\theta_i = i \\times \\Delta\\theta$ and $z_i = i \\times h$, with $i = 0,1,\\dots,N_{\\mathrm{res}}-1$. The CA Cartesian coordinates are defined by $x_i = R \\cos(\\theta_i)$, $y_i = R \\sin(\\theta_i)$, and $z_i$ as above.\n- For each residue, define heavy atom positions for the amide nitrogen (N), the alpha carbon (CA), the carbonyl carbon (C), and the carbonyl oxygen (O). Let $\\mathbf{r}_{\\mathrm{CA},i}$ denote the CA position. Define a local tangent unit vector $\\mathbf{t}_i$ using finite differences on CA positions: for interior residues, $\\mathbf{t}_i$ is the normalized vector $\\mathbf{r}_{\\mathrm{CA},i+1} - \\mathbf{r}_{\\mathrm{CA},i-1}$; for the endpoints, use forward or backward differences respectively. Then place atoms as follows:\n    - $\\mathbf{r}_{\\mathrm{N},i} = \\mathbf{r}_{\\mathrm{CA},i} - \\ell_{\\mathrm{N-CA}} \\, \\mathbf{t}_i$ with $\\ell_{\\mathrm{N-CA}} = 1.47$ Å,\n    - $\\mathbf{r}_{\\mathrm{C},i} = \\mathbf{r}_{\\mathrm{CA},i} + \\ell_{\\mathrm{CA-C}} \\, \\mathbf{t}_i$ with $\\ell_{\\mathrm{CA-C}} = 1.53$ Å,\n    - $\\mathbf{r}_{\\mathrm{O},i} = \\mathbf{r}_{\\mathrm{C},i} + \\ell_{\\mathrm{C-O}} \\, \\mathbf{n}_i$ with $\\ell_{\\mathrm{C-O}} = 1.24$ Å,\nwhere $\\mathbf{n}_i$ is a unit vector perpendicular to both $\\mathbf{t}_i$ and the helix axis $\\mathbf{z} = (0,0,1)$, defined by $\\mathbf{n}_i = \\dfrac{\\mathbf{t}_i \\times \\mathbf{z}}{\\|\\mathbf{t}_i \\times \\mathbf{z}\\|}$ if $\\|\\mathbf{t}_i \\times \\mathbf{z}\\| > 0$, and otherwise $\\mathbf{n}_i = (1,0,0)$.\n\nGraph construction rules:\n- Atom-level graph $G_{\\mathrm{atom}}$: nodes correspond to all heavy atoms $\\{\\mathrm{N}, \\mathrm{CA}, \\mathrm{C}, \\mathrm{O}\\}$ per residue. Place an undirected edge between two atoms $a$ and $b$ if $d(\\mathbf{r}_a,\\mathbf{r}_b) \\le d_{\\mathrm{atom}}$, where $d_{\\mathrm{atom}}$ is the atom-level distance cutoff in Å.\n- Residue-level graph $G_{\\mathrm{res}}$: nodes correspond to residues $i=0,\\dots,N_{\\mathrm{res}}-1$. Place an undirected edge between residues $i$ and $j$ if the minimum interatomic distance between any heavy atom of residue $i$ and any heavy atom of residue $j$ is less than or equal to $d_{\\mathrm{res}}$, where $d_{\\mathrm{res}}$ is the residue-level distance cutoff in Å.\n\nQuantities to compute for each test case:\n- Compute the average degree $\\bar{k}_{\\mathrm{atom}}$ and $\\bar{k}_{\\mathrm{res}}$ for $G_{\\mathrm{atom}}$ and $G_{\\mathrm{res}}$, respectively.\n- Compute the empirical degree distributions $P_{\\mathrm{atom}}(k)$ and $P_{\\mathrm{res}}(k)$ for $k \\in \\{0,1,\\dots,K_{\\max}\\}$ with $K_{\\max} = \\max\\{\\max_i k^{\\mathrm{atom}}_i, \\max_j k^{\\mathrm{res}}_j\\}$, and the total variation distance $D_{\\mathrm{TV}}(P_{\\mathrm{atom}}, P_{\\mathrm{res}})$.\n- Compute the average clustering coefficients $\\bar{C}_{\\mathrm{atom}}$ and $\\bar{C}_{\\mathrm{res}}$ for the two graphs.\n- Compute the average shortest path lengths $\\bar{L}_{\\mathrm{atom}}$ and $\\bar{L}_{\\mathrm{res}}$ on the largest connected component of each graph. Define the average shortest path length $\\bar{L}$ as the mean of all pairwise shortest path lengths over distinct node pairs within the largest component; if the largest component has fewer than $2$ nodes, set $\\bar{L} = 0$.\n\nFinal outputs per test case:\n- For each test case, return a list of $9$ floating-point numbers in the following order:\n    1. $\\bar{k}_{\\mathrm{atom}}$,\n    2. $\\bar{k}_{\\mathrm{res}}$,\n    3. $D_{\\mathrm{TV}}(P_{\\mathrm{atom}}, P_{\\mathrm{res}})$,\n    4. $\\bar{C}_{\\mathrm{atom}}$,\n    5. $\\bar{C}_{\\mathrm{res}}$,\n    6. $\\Delta \\bar{C} = \\bar{C}_{\\mathrm{atom}} - \\bar{C}_{\\mathrm{res}}$,\n    7. $\\bar{L}_{\\mathrm{atom}}$,\n    8. $\\bar{L}_{\\mathrm{res}}$,\n    9. $\\Delta \\bar{L} = \\bar{L}_{\\mathrm{atom}} - \\bar{L}_{\\mathrm{res}}$.\n\nTest suite:\n- Use the following test cases for $(d_{\\mathrm{atom}}, d_{\\mathrm{res}})$ in Å:\n    1. $(4.5, 7.0)$,\n    2. $(3.0, 5.0)$,\n    3. $(6.0, 10.0)$,\n    4. $(4.0, 4.0)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of the nine floats in the order specified above (e.g., $[\\,[r_{1,1},\\dots,r_{1,9}],\\,[r_{2,1},\\dots,r_{2,9}],\\,[r_{3,1},\\dots,r_{3,9}],\\,[r_{4,1},\\dots,r_{4,9}]\\,]$). The output is dimensionless except for the use of Å in defining the input cutoffs, which are internal to the computation and not printed. No angles are used in the output; any angles used internally are in radians.",
            "solution": "The user-provided problem is valid as it is scientifically grounded, well-posed, and contains all necessary information for a deterministic solution. It involves the construction and analysis of network representations of a biomolecule, a standard and important task in computational chemical biology. We will proceed with a step-by-step solution.\n\nThe overall methodology comprises four main stages: (1) algorithmic generation of the atomic coordinates for an idealized alpha helix; (2) construction of two distinct graph representations—an atom-level graph $G_{\\mathrm{atom}}$ and a residue-level graph $G_{\\mathrm{res}}$—based on distance cutoffs; (3) computation of a suite of quantitative graph-theoretic metrics for both graphs; and (4) comparison of these metrics to evaluate the differences between the two levels of representation.\n\n### Part 1: Generation of the Atomic Structure\n\nThe foundation of the analysis is a three-dimensional model of a polypeptide. We are tasked with generating the coordinates of all heavy atoms for a $N_{\\mathrm{res}} = 12$ residue segment in an ideal alpha-helical conformation.\n\nFirst, we generate the positions of the alpha-carbon atoms ($\\mathrm{CA}$), which define the helical backbone trace. The position of the $\\mathrm{CA}$ atom of residue $i$ (for $i = 0, 1, \\dots, N_{\\mathrm{res}}-1$) is given by a parametric representation in Cartesian coordinates $(x_i, y_i, z_i)$. The parameters are the radius of the helix $R = 2.3$ Å, the axial rise per residue $h = 1.5$ Å, and the rotation per residue $\\Delta\\theta = 100^\\circ$. The angle must be converted to radians for calculations: $\\Delta\\theta_{\\mathrm{rad}} = 100 \\cdot (\\pi/180)$. The coordinates of the $i$-th $\\mathrm{CA}$ atom, $\\mathbf{r}_{\\mathrm{CA},i}$, are then:\n$$\n\\begin{cases}\nx_i = R \\cos(i \\cdot \\Delta\\theta_{\\mathrm{rad}}) \\\\\ny_i = R \\sin(i \\cdot \\Delta\\theta_{\\mathrm{rad}}) \\\\\nz_i = i \\cdot h\n\\end{cases}\n$$\n\nWith the $\\mathrm{CA}$ positions established, we place the other backbone heavy atoms: the amide nitrogen ($\\mathrm{N}$), the carbonyl carbon ($\\mathrm{C}$), and the carbonyl oxygen ($\\mathrm{O}$). Their positions are defined relative to the $\\mathrm{CA}$ atom using a local coordinate system. This requires a local tangent vector $\\mathbf{t}_i$ along the backbone. We approximate this vector using finite differences on the $\\mathrm{CA}$ positions:\n- For interior residues ($1  i  N_{\\mathrm{res}}-1$): $\\mathbf{t}_i = \\frac{\\mathbf{r}_{\\mathrm{CA},i+1} - \\mathbf{r}_{\\mathrm{CA},i-1}}{\\|\\mathbf{r}_{\\mathrm{CA},i+1} - \\mathbf{r}_{\\mathrm{CA},i-1}\\|}$ (central difference).\n- For the N-terminus ($i=0$): $\\mathbf{t}_0 = \\frac{\\mathbf{r}_{\\mathrm{CA},1} - \\mathbf{r}_{\\mathrm{CA},0}}{\\|\\mathbf{r}_{\\mathrm{CA},1} - \\mathbf{r}_{\\mathrm{CA},0}\\|}$ (forward difference).\n- For the C-terminus ($i=N_{\\mathrm{res}}-1$): $\\mathbf{t}_{N_{\\mathrm{res}}-1} = \\frac{\\mathbf{r}_{\\mathrm{CA},N_{\\mathrm{res}}-1} - \\mathbf{r}_{\\mathrm{CA},N_{\\mathrm{res}}-2}}{\\|\\mathbf{r}_{\\mathrm{CA},N_{\\mathrm{res}}-1} - \\mathbf{r}_{\\mathrm{CA},N_{\\mathrm{res}}-2}\\|}$ (backward difference).\n\nThe $\\mathrm{N}$ and $\\mathrm{C}$ atoms are placed along this tangent vector:\n$$\n\\mathbf{r}_{\\mathrm{N},i} = \\mathbf{r}_{\\mathrm{CA},i} - \\ell_{\\mathrm{N-CA}} \\, \\mathbf{t}_i, \\quad \\text{with} \\quad \\ell_{\\mathrm{N-CA}} = 1.47 \\, \\text{Å}\n$$\n$$\n\\mathbf{r}_{\\mathrm{C},i} = \\mathbf{r}_{\\mathrm{CA},i} + \\ell_{\\mathrm{CA-C}} \\, \\mathbf{t}_i, \\quad \\text{with} \\quad \\ell_{\\mathrm{CA-C}} = 1.53 \\, \\text{Å}\n$$\n\nThe placement of the carbonyl oxygen ($\\mathrm{O}$) requires an additional direction vector, $\\mathbf{n}_i$, which should be perpendicular to both the local tangent $\\mathbf{t}_i$ and the main helical axis $\\mathbf{z} = (0,0,1)$. This vector is defined by the normalized cross product:\n$$\n\\mathbf{n}_i = \\frac{\\mathbf{t}_i \\times \\mathbf{z}}{\\|\\mathbf{t}_i \\times \\mathbf{z}\\|}\n$$\nThe problem specifies a fallback $\\mathbf{n}_i = (1,0,0)$ if $\\|\\mathbf{t}_i \\times \\mathbf{z}\\| = 0$, which occurs if $\\mathbf{t}_i$ is parallel to the $\\mathbf{z}$ axis (a physically unrealistic scenario for a helix). The oxygen atom is then placed as:\n$$\n\\mathbf{r}_{\\mathrm{O},i} = \\mathbf{r}_{\\mathrm{C},i} + \\ell_{\\mathrm{C-O}} \\, \\mathbf{n}_i, \\quad \\text{with} \\quad \\ell_{\\mathrm{C-O}} = 1.24 \\, \\text{Å}\n$$\nThis procedure yields the coordinates for all $N_{\\mathrm{atom}} = N_{\\mathrm{res}} \\times 4 = 12 \\times 4 = 48$ heavy atoms.\n\n### Part 2: Graph Construction\n\nFrom the atomic coordinates, we construct two undirected, simple graphs.\n\n**Atom-level graph ($G_{\\mathrm{atom}}$)**: This graph provides a high-resolution view of atomic contacts. The set of nodes $V_{\\mathrm{atom}}$ consists of all $N_{\\mathrm{atom}}=48$ heavy atoms. An edge exists between two distinct atoms $a$ and $b$ if their Euclidean distance $d(\\mathbf{r}_a, \\mathbf{r}_b)$ is less than or equal to a specified cutoff $d_{\\mathrm{atom}}$. The adjacency matrix $A^{\\mathrm{atom}}$ is defined as:\n$$\nA^{\\mathrm{atom}}_{ab} = \\begin{cases} 1  \\text{if } 0  d(\\mathbf{r}_a, \\mathbf{r}_b) \\le d_{\\mathrm{atom}} \\\\ 0  \\text{otherwise} \\end{cases}\n$$\n\n**Residue-level graph ($G_{\\mathrm{res}}$)**: This is a coarse-grained representation where each node corresponds to an entire amino acid residue. The set of nodes $V_{\\mathrm{res}}$ consists of the $N_{\\mathrm{res}}=12$ residues. An edge exists between two distinct residues $i$ and $j$ if the minimum Euclidean distance between any heavy atom of residue $i$ and any heavy atom of residue $j$ is less than or equal to a specified cutoff $d_{\\mathrm{res}}$. Let $\\mathcal{H}_i$ be the set of heavy atoms in residue $i$. The adjacency matrix $A^{\\mathrm{res}}$ is defined as:\n$$\nA^{\\mathrm{res}}_{ij} = \\begin{cases} 1  \\text{if } i \\ne j \\text{ and } \\min_{a \\in \\mathcal{H}_i, b \\in \\mathcal{H}_j} d(\\mathbf{r}_a, \\mathbf{r}_b) \\le d_{\\mathrm{res}} \\\\ 0  \\text{otherwise} \\end{cases}\n$$\n\n### Part 3: Computation of Graph-Theoretic Metrics\n\nFor each graph, we compute a set of standard network metrics to characterize its topology. For a generic graph $G$ with $n$ nodes and adjacency matrix $A$:\n\n- **Average Degree ($\\bar{k}$)**: The degree of a node $i$ is $k_i = \\sum_{j=1}^{n} A_{ij}$. The average degree is the mean of all node degrees: $\\bar{k} = \\frac{1}{n} \\sum_{i=1}^{n} k_i$.\n\n- **Degree Distribution ($P(k)$) and Total Variation Distance ($D_{\\mathrm{TV}}$)**: The empirical degree distribution $P(k)$ measures the fraction of nodes in the graph having degree $k$. The total variation distance between the distributions for the atom-level and residue-level graphs, $P_{\\mathrm{atom}}$ and $P_{\\mathrm{res}}$, provides a single value quantifying their difference. It is computed as $D_{\\mathrm{TV}}(P_{\\mathrm{atom}}, P_{\\mathrm{res}}) = \\frac{1}{2} \\sum_{k=0}^{K_{\\max}} |P_{\\mathrm{atom}}(k) - P_{\\mathrm{res}}(k)|$, where $K_{\\max}$ is the maximum degree observed across both graphs.\n\n- **Average Clustering Coefficient ($\\bar{C}$)**: The local clustering coefficient $C_i$ for a node $i$ quantifies how well its neighbors are connected to each other. For a node $i$ with degree $k_i \\ge 2$, it is the ratio of the number of edges between its neighbors ($m_i$) to the maximum possible number of such edges, $\\binom{k_i}{2}$.\n$$\nC_i = \\frac{2 m_i}{k_i (k_i - 1)}\n$$\nFor $k_i  2$, $C_i = 0$. The number of triangles passing through node $i$ is $\\frac{1}{2}(A^3)_{ii}$. Thus, an efficient way to compute $C_i$ is $C_i = \\frac{(A^3)_{ii}}{k_i(k_i - 1)}$. The average clustering coefficient $\\bar{C}$ is the mean of the local coefficients over all nodes: $\\bar{C} = \\frac{1}{n} \\sum_{i=1}^{n} C_i$.\n\n- **Average Shortest Path Length ($\\bar{L}$)**: This metric measures the typical separation between nodes in a graph. First, we identify the largest connected component (LCC) of the graph. If the size of the LCC is $s  2$, $\\bar{L}$ is defined as $0$. Otherwise, we compute the shortest path length (number of edges in the shortest path) for all $\\frac{s(s-1)}{2}$ pairs of distinct nodes within the LCC. The average shortest path length $\\bar{L}$ is the mean of these path lengths. This can be computed by running a Breadth-First Search (BFS) from each node in the LCC or by using the Floyd-Warshall algorithm on the LCC's adjacency matrix.\n\n### Part 4: Synthesis and Comparison\n\nThe final step is to apply these calculations for each test case, which is a pair of distance cutoffs $(d_{\\mathrm{atom}}, d_{\\mathrm{res}})$. For each case, we compute the metrics for both $G_{\\mathrm{atom}}$ and $G_{\\mathrm{res}}$ and then calculate their differences. The nine output values are:\n1.  $\\bar{k}_{\\mathrm{atom}}$: Average degree of the atom-level graph.\n2.  $\\bar{k}_{\\mathrm{res}}$: Average degree of the residue-level graph.\n3.  $D_{\\mathrm{TV}}(P_{\\mathrm{atom}}, P_{\\mathrm{res}})$: Total variation distance between degree distributions.\n4.  $\\bar{C}_{\\mathrm{atom}}$: Average clustering coefficient of the atom-level graph.\n5.  $\\bar{C}_{\\mathrm{res}}$: Average clustering coefficient of the residue-level graph.\n6.  $\\Delta \\bar{C} = \\bar{C}_{\\mathrm{atom}} - \\bar{C}_{\\mathrm{res}}$: Difference in average clustering.\n7.  $\\bar{L}_{\\mathrm{atom}}$: Average shortest path length of the atom-level graph.\n8.  $\\bar{L}_{\\mathrm{res}}$: Average shortest path length of the residue-level graph.\n9.  $\\Delta \\bar{L} = \\bar{L}_{\\mathrm{atom}} - \\bar{L}_{\\mathrm{res}}$: Difference in average path length.\n\nThis comprehensive analysis will reveal how the choice of representation (atomic vs. residue-based) impacts key topological properties of the protein contact network.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse.csgraph import connected_components, shortest_path\n\ndef generate_helix_structure(n_res, h, R, d_theta_deg):\n    \"\"\"\n    Generates atomic coordinates for an ideal alpha helix.\n    Returns a numpy array of shape (n_res, 4, 3) for N, CA, C, O atoms.\n    \"\"\"\n    d_theta_rad = np.deg2rad(d_theta_deg)\n    \n    # Bond lengths\n    l_n_ca = 1.47\n    l_ca_c = 1.53\n    l_c_o = 1.24\n\n    # Atom types map: 0:N, 1:CA, 2:C, 3:O\n    coords = np.zeros((n_res, 4, 3))\n    \n    # 1. Generate CA coordinates\n    ca_coords = np.zeros((n_res, 3))\n    for i in range(n_res):\n        theta_i = i * d_theta_rad\n        ca_coords[i, 0] = R * np.cos(theta_i)\n        ca_coords[i, 1] = R * np.sin(theta_i)\n        ca_coords[i, 2] = i * h\n    coords[:, 1, :] = ca_coords\n\n    # 2. Generate other heavy atoms\n    z_axis = np.array([0., 0., 1.])\n    \n    for i in range(n_res):\n        # Calculate tangent vector t_i\n        if i == 0:\n            t_i = ca_coords[1] - ca_coords[0]\n        elif i == n_res - 1:\n            t_i = ca_coords[i] - ca_coords[i-1]\n        else:\n            t_i = ca_coords[i+1] - ca_coords[i-1]\n        t_i /= np.linalg.norm(t_i)\n\n        # Place N and C atoms\n        coords[i, 0, :] = ca_coords[i] - l_n_ca * t_i\n        coords[i, 2, :] = ca_coords[i] + l_ca_c * t_i\n\n        # Calculate normal vector n_i and place O atom\n        t_cross_z = np.cross(t_i, z_axis)\n        norm_t_cross_z = np.linalg.norm(t_cross_z)\n        if norm_t_cross_z > 1e-9:\n            n_i = t_cross_z / norm_t_cross_z\n        else:\n            n_i = np.array([1., 0., 0.])\n        \n        coords[i, 3, :] = coords[i, 2, :] + l_c_o * n_i\n        \n    return coords\n\ndef analyze_graph(A):\n    \"\"\"\n    Analyzes a graph given its adjacency matrix A.\n    Returns avg_degree, degree_dist, avg_clustering, avg_path_length.\n    \"\"\"\n    n = A.shape[0]\n    if n == 0:\n        return 0.0, np.array([1.0]), 0.0, 0.0\n        \n    # Degrees and average degree\n    degrees = A.sum(axis=1)\n    avg_degree = np.mean(degrees)\n    \n    # Degree distribution\n    max_k = int(np.max(degrees)) if degrees.size > 0 else 0\n    k_counts = np.zeros(max_k + 1)\n    for k in degrees:\n        k_counts[int(k)] += 1\n    degree_dist = k_counts / n\n\n    # Average clustering coefficient\n    A3 = np.linalg.matrix_power(A, 3)\n    diagonals = np.diag(A3)\n    c_i = np.zeros(n)\n    for i in range(n):\n        ki = degrees[i]\n        if ki >= 2:\n            c_i[i] = diagonals[i] / (ki * (ki - 1))\n    avg_clustering = np.mean(c_i)\n\n    # Average shortest path length\n    n_components, labels = connected_components(A, directed=False)\n    if n_components > 0:\n        component_sizes = np.bincount(labels)\n        largest_comp_label = np.argmax(component_sizes)\n        lcc_nodes = np.where(labels == largest_comp_label)[0]\n        s = len(lcc_nodes)\n    else:\n        s = 0\n\n    if s  2:\n        avg_path_length = 0.0\n    else:\n        lcc_A = A[np.ix_(lcc_nodes, lcc_nodes)]\n        path_matrix = shortest_path(lcc_A, directed=False, unweighted=True)\n        # Sum of upper triangle (excluding diagonal)\n        total_path_length = np.sum(np.triu(path_matrix, k=1))\n        num_pairs = s * (s - 1) / 2\n        avg_path_length = total_path_length / num_pairs\n        \n    return avg_degree, degree_dist, avg_clustering, avg_path_length\n\ndef solve():\n    # Define constants from the problem\n    N_RES = 12\n    H = 1.5\n    R = 2.3\n    D_THETA = 100.0\n    \n    test_cases = [\n        (4.5, 7.0),\n        (3.0, 5.0),\n        (6.0, 10.0),\n        (4.0, 4.0),\n    ]\n\n    # Generate structure once\n    structure_coords = generate_helix_structure(N_RES, H, R, D_THETA)\n    \n    all_results = []\n\n    for d_atom, d_res in test_cases:\n        # 1. Build atom-level graph\n        n_atoms = N_RES * 4\n        atom_coords_flat = structure_coords.reshape(n_atoms, 3)\n        diffs = atom_coords_flat[:, np.newaxis, :] - atom_coords_flat[np.newaxis, :, :]\n        dist_matrix_atom = np.sqrt(np.sum(diffs**2, axis=-1))\n        A_atom = (dist_matrix_atom = d_atom).astype(int)\n        np.fill_diagonal(A_atom, 0)\n\n        # 2. Build residue-level graph\n        dist_matrix_res_min = np.full((N_RES, N_RES), np.inf)\n        for i in range(N_RES):\n            for j in range(i + 1, N_RES):\n                res_i_coords = structure_coords[i, :, :]\n                res_j_coords = structure_coords[j, :, :]\n                \n                # Pairwise distances between all atoms of residue i and j\n                res_diffs = res_i_coords[:, np.newaxis, :] - res_j_coords[np.newaxis, :, :]\n                res_dists = np.sqrt(np.sum(res_diffs**2, axis=-1))\n                min_dist = np.min(res_dists)\n                dist_matrix_res_min[i, j] = min_dist\n                dist_matrix_res_min[j, i] = min_dist\n        \n        A_res = (dist_matrix_res_min = d_res).astype(int)\n        np.fill_diagonal(A_res, 0)\n        \n        # 3. Analyze graphs\n        k_atom, p_atom, c_atom, l_atom = analyze_graph(A_atom)\n        k_res, p_res, c_res, l_res = analyze_graph(A_res)\n\n        # 4. Compute TV distance and deltas\n        len_p_atom = len(p_atom)\n        len_p_res = len(p_res)\n        k_max = max(len_p_atom, len_p_res)\n        \n        p_atom_padded = np.pad(p_atom, (0, k_max - len_p_atom), 'constant')\n        p_res_padded = np.pad(p_res, (0, k_max - len_p_res), 'constant')\n        \n        d_tv = 0.5 * np.sum(np.abs(p_atom_padded - p_res_padded))\n        delta_c = c_atom - c_res\n        delta_l = l_atom - l_res\n        \n        case_results = [\n            k_atom, k_res, d_tv, c_atom, c_res, delta_c, l_atom, l_res, delta_l\n        ]\n        all_results.append(case_results)\n\n    # Format the final output string without extra spaces\n    outer_parts = []\n    for inner_list in all_results:\n        inner_parts_str = [f\"{val:.10f}\" for val in inner_list]\n        outer_parts.append(f\"[{','.join(inner_parts_str)}]\")\n    final_output_str = f\"[{','.join(outer_parts)}]\"\n    print(final_output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Proteins are not static; their function arises from dynamic, correlated motions. We can capture these collective movements by building a network where edges represent the correlation between residue fluctuations from a molecular dynamics simulation. Since correlation is a continuous measure, a critical step is applying a threshold $\\theta$ to define the presence of an edge. This exercise demonstrates how to systematically analyze network connectivity as this threshold is varied, allowing you to identify critical transitions where the network suddenly coalesces into a large, connected 'giant component'—a process analogous to percolation that highlights the emergence of long-range communication pathways .",
            "id": "3855795",
            "problem": "Consider a residue interaction network constructed from a protein molecular dynamics simulation by thresholding an inter-residue correlation matrix. Let $N$ denote the number of residues. For each pair of residues $i$ and $j$ with $i \\neq j$, suppose a real-valued correlation $C_{ij}$ is defined as the Pearson correlation coefficient between mean-centered displacement time series $x_i(t)$ and $x_j(t)$, where the Pearson correlation coefficient (PCC) for a pair $(i,j)$ is defined by\n$$\nC_{ij} \\equiv \\frac{\\sum_{t=1}^{T} \\left(x_i(t) - \\bar{x}_i\\right)\\left(x_j(t) - \\bar{x}_j\\right)}{\\sqrt{\\sum_{t=1}^{T}\\left(x_i(t) - \\bar{x}_i\\right)^2}\\sqrt{\\sum_{t=1}^{T}\\left(x_j(t) - \\bar{x}_j\\right)^2}},\n$$\nwith $\\bar{x}_i$ denoting the time average of $x_i(t)$. Define an undirected, unweighted graph $G(\\theta)$ with node set $\\{1,\\dots,N\\}$ and adjacency rule\n$$\nA_{ij}(\\theta) = \\begin{cases}\n1  \\text{if } i \\neq j \\text{ and } |C_{ij}| \\ge \\theta,\\\\\n0  \\text{otherwise.}\n\\end{cases}\n$$\nEdges represent dynamically correlated residue pairs above threshold $\\theta$. Let $E(\\theta)$ denote the set of undirected edges in $G(\\theta)$, and let $M \\equiv \\frac{N(N-1)}{2}$ denote the maximum possible number of undirected edges on $N$ nodes. The edge density is defined as\n$$\np(\\theta) \\equiv \\frac{|E(\\theta)|}{M}.\n$$\nLet the largest connected component (the giant component) of $G(\\theta)$ have size $S_{\\max}(\\theta)$, and define its fractional size as\n$$\ng(\\theta) \\equiv \\frac{S_{\\max}(\\theta)}{N}.\n$$\nDefine the average shortest path length on the giant component, $\\ell(\\theta)$, as the mean of geodesic distances over all unordered node pairs belonging to the giant component of $G(\\theta)$, with each edge having unit length. If $S_{\\max}(\\theta)  2$, define $\\ell(\\theta) \\equiv 0$.\n\nA percolation-like transition may be detected by observing abrupt changes in connectivity measures as $\\theta$ varies. Given a non-increasing sequence of thresholds $\\{\\theta_k\\}_{k=1}^{K}$, define the discrete change in fractional giant component size across adjacent thresholds as\n$$\n\\Delta g_k \\equiv g(\\theta_{k+1}) - g(\\theta_k), \\quad \\text{for } k \\in \\{1,\\dots,K-1\\}.\n$$\nDefine a percolation-like transition point by the threshold $\\theta^\\star$ corresponding to the largest positive discrete change, i.e.,\n$$\n\\theta^\\star \\equiv \\theta_{k^\\star}, \\quad \\text{where } k^\\star = \\arg\\max_{k \\in \\{1,\\dots,K-1\\}} \\Delta g_k.\n$$\nIf multiple $k$ achieve the maximum, choose the smallest such $k$. Additionally, define a connectivity flip threshold $\\theta^{\\mathrm{conn}}$ as the earliest threshold in the sequence at which the whole graph becomes connected (i.e., $g(\\theta) = 1$). If the graph is never fully connected across the sequence, set $\\theta^{\\mathrm{conn}} \\equiv -1$. Also define a half-fraction onset threshold $\\theta^{1/2}$ as the earliest threshold at which $g(\\theta) \\ge 0.5$, with $\\theta^{1/2} \\equiv -1$ if this never occurs.\n\nYour task is to implement a program that, for each specified test case, constructs $G(\\theta)$ at each threshold in the sequence, computes $p(\\theta)$, $g(\\theta)$, $\\ell(\\theta)$, and identifies $\\theta^\\star$, $\\theta^{\\mathrm{conn}}$, and $\\theta^{1/2}$.\n\nBase your reasoning on the following well-tested definitions and facts:\n- Pearson correlation coefficient as given above.\n- Graph connectivity and connected components defined via paths in undirected graphs.\n- Shortest path length in unweighted graphs as the minimum number of edges between nodes.\n- Edge density as the fraction of actual edges among all possible undirected edges.\n\nUse absolute correlations $|C_{ij}|$ for thresholding, with $C_{ii} = 1$ for all $i$. All graphs are undirected and simple.\n\nTest Suite:\n- Case $1$ (modular with a bridge):\n  - $N = 10$ residues labeled $0$ through $9$.\n  - Partition residues into two modules $\\{0,1,2,3,4\\}$ and $\\{5,6,7,8,9\\}$.\n  - Define $C_{ij}$ for $i \\neq j$ as follows: within the first module, set $C_{ij} = 0.80$; within the second module, set $C_{ij} = 0.85$; between modules, set $C_{ij} = 0.20$ except for a single bridging pair $(4,5)$ with $C_{45} = C_{54} = 0.60$; set $C_{ii} = 1.00$.\n  - Thresholds $\\{\\theta_k\\}$ are $\\left[0.90, 0.85, 0.80, 0.60, 0.40, 0.20, 0.10, 0.00\\right]$.\n- Case $2$ (ring-lattice decay):\n  - $N = 12$ residues labeled $0$ through $11$ arranged on a ring. Let the ring distance $d(i,j)$ be the minimum of $|i-j|$ and $N - |i-j|$.\n  - Define $C_{ij} = \\exp\\left(-\\frac{d(i,j)}{2}\\right)$ for $i \\neq j$, and $C_{ii} = 1.00$.\n  - Thresholds $\\{\\theta_k\\}$ are $\\left[0.70, 0.61, 0.6065, 0.50, 0.37, 0.20, 0.10\\right]$.\n- Case $3$ (uniformly weak):\n  - $N = 8$ residues labeled $0$ through $7$.\n  - Define $C_{ij} = 0.05$ for $i \\neq j$, and $C_{ii} = 1.00$.\n  - Thresholds $\\{\\theta_k\\}$ are $\\left[0.50, 0.10, 0.05, 0.00\\right]$.\n\nFor each test case, compute:\n- The list of edge densities $\\left[p(\\theta_1), p(\\theta_2), \\dots, p(\\theta_K)\\right]$.\n- The list of giant component fractional sizes $\\left[g(\\theta_1), g(\\theta_2), \\dots, g(\\theta_K)\\right]$.\n- The list of average shortest path lengths on the giant component $\\left[\\ell(\\theta_1), \\ell(\\theta_2), \\dots, \\ell(\\theta_K)\\right]$.\n- The percolation-like transition $\\theta^\\star$.\n- The connectivity flip threshold $\\theta^{\\mathrm{conn}}$.\n- The half-fraction onset threshold $\\theta^{1/2}$.\n\nFinal Output Format:\nYour program should produce a single line of output containing a list of results, one per test case, where each test case result is itself a list in the following order:\n$\\left[\\left[p(\\theta_k)\\right]_{k=1}^{K}, \\left[g(\\theta_k)\\right]_{k=1}^{K}, \\left[\\ell(\\theta_k)\\right]_{k=1}^{K}, \\theta^\\star, \\theta^{\\mathrm{conn}}, \\theta^{1/2}\\right]$.\nThe entire output should be printed as a single Python-style list on one line, for example: $\\left[\\text{case1\\_result}, \\text{case2\\_result}, \\text{case3\\_result}\\right]$. All numerical values are unitless real numbers. Where a threshold is undefined by the above rules, return $-1.0$ as specified.",
            "solution": "The problem statement has been critically examined and is determined to be valid. The problem is scientifically grounded in the established field of network science as applied to computational biology, specifically in the analysis of protein dynamics. All terms are mathematically well-defined, and the test cases are self-contained and unambiguous. The problem is well-posed, objective, and provides a clear computational task that admits a unique, verifiable solution.\n\nThe solution to this problem requires implementing a computational pipeline that, for each given test case, systematically analyzes a series of graphs generated at different correlation thresholds. The overall algorithm proceeds as follows:\n\nFirst, for each test case, the correlation matrix $C$ of size $N \\times N$ is constructed according to the specified rules. The provided sequence of $K$ non-increasing thresholds, $\\{\\theta_k\\}_{k=1}^{K}$, is then processed sequentially. For each threshold $\\theta_k$, an unweighted, undirected graph $G(\\theta_k)$ is constructed and its properties are computed.\n\nThe core steps for analyzing the graph at a single threshold $\\theta$ are:\n\n1.  **Graph Construction**: The adjacency matrix $A(\\theta)$ of the graph $G(\\theta)$ is constructed from the $N \\times N$ correlation matrix $C$. An edge exists between two distinct residues (nodes) $i$ and $j$ if the absolute value of their correlation, $|C_{ij}|$, meets or exceeds the threshold $\\theta$. The elements of the adjacency matrix are thus given by:\n    $$\n    A_{ij}(\\theta) = \\begin{cases}\n    1  \\text{if } i \\neq j \\text{ and } |C_{ij}| \\ge \\theta, \\\\\n    0  \\text{otherwise.}\n    \\end{cases}\n    $$\n    Note that $A_{ii}(\\theta) = 0$ for all $i$, as there are no self-loops.\n\n2.  **Edge Density $p(\\theta)$**: The number of edges in the graph, $|E(\\theta)|$, is half the sum of all elements in the adjacency matrix, $|E(\\theta)| = \\frac{1}{2}\\sum_{i,j} A_{ij}(\\theta)$. The edge density $p(\\theta)$ is the ratio of the actual number of edges to the maximum possible number of edges, $M = \\frac{N(N-1)}{2}$.\n    $$\n    p(\\theta) = \\frac{\\sum_{ij} A_{ij}(\\theta)}{\\frac{N(N-1)}{2}}\n    $$\n\n3.  **Connected Components and Fractional Giant Component Size $g(\\theta)$**: To determine the connectivity of $G(\\theta)$, its nodes are partitioned into connected components. This is accomplished using a standard graph traversal algorithm such as Breadth-First Search (BFS) or by using a Union-Find data structure. After identifying all components, the size of each component (i.e., the number of nodes it contains) is counted. The size of the largest connected component, or giant component, is denoted $S_{\\max}(\\theta)$. The fractional giant component size is then calculated as:\n    $$\n    g(\\theta) = \\frac{S_{\\max}(\\theta)}{N}\n    $$\n\n4.  **Average Shortest Path Length $\\ell(\\theta)$**: This metric is computed only for the giant component.\n    - If the giant component contains fewer than two nodes ($S_{\\max}(\\theta)  2$), the average shortest path length is defined to be zero: $\\ell(\\theta) \\equiv 0$.\n    - Otherwise, we consider the subgraph induced by the $S_{\\max}(\\theta)$ nodes of the giant component. The shortest path (geodesic distance) between every unordered pair of nodes within this component is calculated. Since the graph is unweighted, this is equivalent to running a BFS starting from each node in the component to find its distance to all other nodes in the same component.\n    - The average shortest path length $\\ell(\\theta)$ is the sum of these geodesic distances divided by the total number of unique pairs of nodes in the giant component, which is $\\frac{S_{\\max}(\\theta)(S_{\\max}(\\theta)-1)}{2}$.\n\nAfter iterating through all thresholds $\\theta_k$ for a given test case, the stored lists of results—$[p(\\theta_k)]_{k=1}^K$, $[g(\\theta_k)]_{k=1}^K$, and $[\\ell(\\theta_k)]_{k=1}^K$—are used to compute the final summary statistics.\n\n5.  **Percolation-like Transition Threshold $\\theta^\\star$**: The discrete change in the fractional giant component size is computed for each adjacent pair of thresholds: $\\Delta g_k = g(\\theta_{k+1}) - g(\\theta_k)$ for $k \\in \\{1, \\dots, K-1\\}$. The index $k^\\star$ corresponding to the largest positive change is found:\n    $$\n    k^\\star = \\arg\\max_{k \\in \\{1, \\dots, K-1\\}} \\Delta g_k\n    $$\n    In case of a tie for the maximum value, the smallest index $k$ is chosen. The percolation threshold is then defined as the threshold at the beginning of this interval of maximum change: $\\theta^\\star = \\theta_{k^\\star}$. If all $\\Delta g_k \\le 0$, $k^{\\star}$ corresponds to the index of the largest (least non-positive) change.\n\n6.  **Connectivity Flip Threshold $\\theta^{\\mathrm{conn}}$**: This is identified by finding the first threshold $\\theta_k$ in the non-increasing sequence for which the entire graph becomes a single connected component, i.e., $g(\\theta_k)=1$. If this condition is never met, $\\theta^{\\mathrm{conn}}$ is set to $-1$.\n\n7.  **Half-Fraction Onset Threshold $\\theta^{1/2}$**: This is identified by finding the first threshold $\\theta_k$ in the non-increasing sequence for which the giant component comprises at least half of the total nodes, i.e., $g(\\theta_k) \\ge 0.5$. If this condition is never met, $\\theta^{1/2}$ is set to $-1$.\n\nThe implementation will leverage the `numpy` library for efficient numerical and matrix operations and `scipy.sparse.csgraph` for robust and optimized implementations of graph algorithms, namely for finding connected components and computing all-pairs shortest paths. This ensures both correctness and computational efficiency.",
            "answer": "```python\nimport numpy as np\nfrom scipy.sparse import csgraph\n\ndef get_case1_C():\n    \"\"\"Generates the correlation matrix for Case 1.\"\"\"\n    N = 10\n    C = np.full((N, N), 0.20)\n    module1 = np.arange(5)\n    module2 = np.arange(5, 10)\n    \n    # Within-module correlations\n    C[np.ix_(module1, module1)] = 0.80\n    C[np.ix_(module2, module2)] = 0.85\n    \n    # Bridge correlation\n    C[4, 5] = C[5, 4] = 0.60\n    \n    np.fill_diagonal(C, 1.0)\n    return C\n\ndef get_case2_C():\n    \"\"\"Generates the correlation matrix for Case 2.\"\"\"\n    N = 12\n    C = np.zeros((N, N))\n    for i in range(N):\n        for j in range(i, N):\n            if i == j:\n                C[i, j] = 1.0\n            else:\n                dist = min(abs(i - j), N - abs(i - j))\n                val = np.exp(-dist / 2.0)\n                C[i, j] = C[j, i] = val\n    return C\n\ndef get_case3_C():\n    \"\"\"Generates the correlation matrix for Case 3.\"\"\"\n    N = 8\n    C = np.full((N, N), 0.05)\n    np.fill_diagonal(C, 1.0)\n    return C\n\ndef analyze_network(C, thresholds):\n    \"\"\"\n    Analyzes the network properties for a given correlation matrix C\n    across a sequence of thresholds.\n    \"\"\"\n    N = C.shape[0]\n    M = N * (N - 1) / 2 if N > 1 else 0\n\n    p_list, g_list, l_list = [], [], []\n\n    for theta in thresholds:\n        # 1. Graph Construction\n        adj = (np.abs(C) >= theta).astype(int)\n        np.fill_diagonal(adj, 0)\n\n        # 2. Edge Density p(theta)\n        num_edges = np.sum(adj) / 2\n        p = num_edges / M if M > 0 else 0.0\n        p_list.append(p)\n\n        # 3. Connected Components and g(theta)\n        if N == 0:\n            s_max = 0\n        elif num_edges == 0:\n            s_max = 1 if N > 0 else 0\n        else:\n            _, labels = csgraph.connected_components(adj, directed=False, return_labels=True)\n            if labels.size > 0:\n                component_sizes = np.bincount(labels)\n                s_max = np.max(component_sizes)\n            else: # Should not happen for N > 0\n                s_max = 0\n        g = s_max / N if N > 0 else 0.0\n        g_list.append(g)\n\n        # 4. Average Shortest Path Length l(theta)\n        if s_max  2:\n            l_list.append(0.0)\n        else:\n            _, labels = csgraph.connected_components(adj, directed=False, return_labels=True)\n            giant_comp_idx = np.argmax(np.bincount(labels))\n            giant_comp_nodes = np.where(labels == giant_comp_idx)[0]\n            \n            giant_adj = adj[np.ix_(giant_comp_nodes, giant_comp_nodes)]\n            \n            dist_matrix = csgraph.shortest_path(giant_adj, directed=False, unweighted=True)\n            \n            num_pairs = s_max * (s_max - 1) / 2\n            total_dist = np.sum(dist_matrix) / 2\n            \n            l = total_dist / num_pairs if num_pairs > 0 else 0.0\n            l_list.append(l)\n\n    # 5. Percolation Threshold theta_star\n    g_array = np.array(g_list)\n    delta_g = g_array[1:] - g_array[:-1]\n    \n    theta_star = -1.0\n    if len(delta_g) > 0:\n        k_star_idx = np.argmax(delta_g)\n        # Check if max change is positive, as per interpretation.\n        # If all changes are =0, `argmax` will give the first one.\n        # The problem asks for \"largest positive discrete change\",\n        # but argmax works on the whole array. We follow argmax strictly.\n        theta_star = thresholds[k_star_idx]\n\n    # 6. Connectivity Flip Threshold theta_conn\n    theta_conn = -1.0\n    for i, g_val in enumerate(g_list):\n        if g_val == 1.0:\n            theta_conn = thresholds[i]\n            break\n\n    # 7. Half-Fraction Onset Threshold theta_half\n    theta_half = -1.0\n    for i, g_val in enumerate(g_list):\n        if g_val >= 0.5:\n            theta_half = thresholds[i]\n            break\n            \n    return [p_list, g_list, l_list, theta_star, theta_conn, theta_half]\n    \n\ndef solve():\n    test_cases = [\n        {\n            \"C_func\": get_case1_C,\n            \"thresholds\": [0.90, 0.85, 0.80, 0.60, 0.40, 0.20, 0.10, 0.00]\n        },\n        {\n            \"C_func\": get_case2_C,\n            \"thresholds\": [0.70, 0.61, 0.6065, 0.50, 0.37, 0.20, 0.10]\n        },\n        {\n            \"C_func\": get_case3_C,\n            \"thresholds\": [0.50, 0.10, 0.05, 0.00]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        C = case[\"C_func\"]()\n        thresholds = case[\"thresholds\"]\n        result = analyze_network(C, thresholds)\n        results.append(result)\n\n    # Format numbers for consistent output representation\n    # This part is for display only, to match a typical float representation.\n    # The computation is done with full precision.\n    formatted_results = []\n    for res_case in results:\n        p, g, l, ts, tc, th = res_case\n        # The problem doesn't specify precision, so we will not artificially round.\n        # The conversion to string will handle standard representation.\n        formatted_results.append([p, g, l, ts, tc, th])\n        \n    # The required output is a string representation of the list of lists.\n    # Python's `str()` on a list already provides the desired `[...]` format.\n    print(f\"[{','.join(map(str, formatted_results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "A primary goal of analyzing allosteric networks is to pinpoint the specific residues and interactions that act as key conduits for communication across the protein. One powerful way to reveal these critical links is to assess the network's vulnerability to their removal. This practice contrasts the effects of removing edges at random with a targeted strategy that removes edges with the highest betweenness centrality—a measure of how many shortest communication paths an edge lies on. By quantifying the network's fragmentation, you can identify 'vulnerable bridges' whose integrity is disproportionately important for global connectivity, providing strong hypotheses for key allosteric pathways .",
            "id": "3855936",
            "problem": "You are given the task of assessing whether an allosteric residue–residue interaction network contains vulnerable communication bridges by comparing the impact of random edge removal versus targeted removal based on edge betweenness centrality. Model the protein network as an undirected simple graph $G = (V,E)$ where nodes represent residues and edges represent effective dynamical couplings. From fundamental graph-theoretic definitions, the edge betweenness centrality $B_e$ of an edge $e \\in E$ is the sum, over all ordered source–target pairs, of the fraction of geodesic (shortest) paths that traverse $e$: $$B_e \\equiv \\sum_{s \\in V}\\sum_{\\substack{t \\in V\\\\ t \\neq s}} \\frac{\\sigma_{s t}(e)}{\\sigma_{s t}},$$ where $\\sigma_{s t}$ is the number of shortest paths between $s$ and $t$, and $\\sigma_{s t}(e)$ is the number of such paths that include edge $e$. Consider edge removal strategies that either (i) remove $k$ edges with highest $B_e$ (targeted), or (ii) remove $k$ edges sampled uniformly at random without replacement (random). After removing a set of edges $R \\subseteq E$, define the fraction of nodes in the largest connected component (Largest Connected Component (LCC)) as $$f_{\\mathrm{LCC}}(G\\setminus R) \\equiv \\frac{\\lvert C_{\\max}(G\\setminus R)\\rvert}{\\lvert V\\rvert},$$ where $C_{\\max}(G\\setminus R)$ denotes the largest connected component of the residual graph. For the random strategy, estimate the mean LCC fraction by averaging $f_{\\mathrm{LCC}}$ over $S$ independent random draws of $k$ edges. Define the vulnerability score $$V \\equiv \\overline{f}_{\\mathrm{rand}} - f_{\\mathrm{target}},$$ where $\\overline{f}_{\\mathrm{rand}}$ is the sample mean of $f_{\\mathrm{LCC}}$ under random removal and $f_{\\mathrm{target}}$ is the LCC fraction under targeted removal. Large positive $V$ indicates the presence of edges acting as vulnerable bridges that disproportionately support geodesic communication in the network.\n\nYour program must:\n- Construct the specified undirected graphs and compute edge betweenness centrality $B_e$ directly from the path-counting definition for unweighted graphs, using first-principles algorithms (for example, Breadth-First Search (BFS) to enumerate distances and path counts).\n- For each test case, compute $f_{\\mathrm{target}}$, compute $\\overline{f}_{\\mathrm{rand}}$ using $S$ random trials, and return $V$ for that test case as a float.\n- Use a fixed pseudorandom number generator seed so that $\\overline{f}_{\\mathrm{rand}}$ is deterministic.\n- Express the final output as a single line containing the vulnerability scores $V$ for all test cases as a comma-separated list enclosed in square brackets, with each float rounded to $3$ decimal places, for example $[0.123,0.000,0.456]$.\n\nTest suite specification (all graphs are unweighted and undirected):\n- Test case $1$ (two dense modules with a single inter-module bridge): $V$ has $10$ nodes, edges are the union of two cliques on nodes $\\{0,1,2,3,4\\}$ and $\\{5,6,7,8,9\\}$ plus a single bridging edge $(4,5)$. Explicitly, edges are $$\\{(0,1),(0,2),(0,3),(0,4),(1,2),(1,3),(1,4),(2,3),(2,4),(3,4),(5,6),(5,7),(5,8),(5,9),(6,7),(6,8),(6,9),(7,8),(7,9),(8,9),(4,5)\\}.$$ Use $k=1$ and $S=200$.\n- Test case $2$ (complete graph): $V$ has $6$ nodes with all pairwise edges, i.e., edges $$\\{(0,1),(0,2),(0,3),(0,4),(0,5),(1,2),(1,3),(1,4),(1,5),(2,3),(2,4),(2,5),(3,4),(3,5),(4,5)\\}.$$ Use $k=1$ and $S=200$.\n- Test case $3$ (linear chain): $V$ has $6$ nodes with edges $$\\{(0,1),(1,2),(2,3),(3,4),(4,5)\\}.$$ Use $k=1$ and $S=200$.\n- Test case $4$ (redundant inter-module connections): $V$ has $8$ nodes; two cliques on $\\{0,1,2,3\\}$ and $\\{4,5,6,7\\}$ plus $3$ inter-module edges $(0,4),(1,5),(2,6)$. Explicitly, edges are $$\\{(0,1),(0,2),(0,3),(1,2),(1,3),(2,3),(4,5),(4,6),(4,7),(5,6),(5,7),(6,7),(0,4),(1,5),(2,6)\\}.$$ Use $k=2$ and $S=200$.\n- Test case $5$ (boundary case $k=0$): reuse the graph of test case $1$. Use $k=0$ and $S=50$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, i.e., $[V_1,V_2,V_3,V_4,V_5]$, where each $V_i$ is rounded to $3$ decimal places.",
            "solution": "The user's request is to validate and solve a problem concerning the vulnerability of a network, modeled as a graph, to different edge removal strategies.\n\n### Problem Validation\nThe problem is first assessed for validity.\n\n**Step 1: Extract Givens**\n- **Graph Model**: An undirected simple graph $G = (V,E)$, where $V$ are residues (nodes) and $E$ are dynamical couplings (edges).\n- **Edge Betweenness Centrality ($B_e$)**: $B_e \\equiv \\sum_{s \\in V}\\sum_{\\substack{t \\in V\\\\ t \\neq s}} \\frac{\\sigma_{s t}(e)}{\\sigma_{s t}}$, where $\\sigma_{st}$ is the number of shortest paths between nodes $s$ and $t$, and $\\sigma_{st}(e)$ is the number of such paths passing through edge $e$.\n- **Edge Removal Strategies**:\n    1.  **Targeted**: Removal of $k$ edges with the highest $B_e$.\n    2.  **Random**: Removal of $k$ edges sampled uniformly at random without replacement.\n- **Evaluation Metric (LCC Fraction)**: $f_{\\mathrm{LCC}}(G\\setminus R) \\equiv \\frac{\\lvert C_{\\max}(G\\setminus R)\\rvert}{\\lvert V\\rvert}$, where $C_{\\max}$ is the largest connected component of the graph after removing edge set $R$.\n- **Vulnerability Score ($V$)**: $V \\equiv \\overline{f}_{\\mathrm{rand}} - f_{\\mathrm{target}}$, where $\\overline{f}_{\\mathrm{rand}}$ is the mean $f_{\\mathrm{LCC}}$ over $S$ random trials and $f_{\\mathrm{target}}$ is the $f_{\\mathrm{LCC}}$ from the single targeted trial.\n- **Procedural Requirements**:\n    - $B_e$ must be computed from its first-principles definition.\n    - A fixed pseudorandom number generator seed must be used.\n- **Test Suite**:\n    - **Case 1**: Graph of two 5-node cliques joined by one edge; $k=1$, $S=200$.\n    - **Case 2**: Complete graph on 6 nodes; $k=1$, $S=200$.\n    - **Case 3**: Linear chain of 6 nodes; $k=1$, $S=200$.\n    - **Case 4**: Graph of two 4-node cliques joined by three edges; $k=2$, $S=200$.\n    - **Case 5**: Graph from Case 1; $k=0$, $S=50$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem statement is analyzed against the validation criteria.\n- **Scientific/Factual Soundness**: The problem is well-grounded in network science and graph theory. The concepts of betweenness centrality, connected components, and network robustness analysis are standard in physics, computer science, and computational biology. The application to protein interaction networks is a well-established research area.\n- **Well-Posedness**: The problem is mathematically well-posed. The definitions are precise and unambiguous for simple, unweighted graphs. The inputs are clearly specified, and the required output, the vulnerability score $V$, is a uniquely defined quantity (deterministic due to the fixed random seed).\n- **Objectivity**: The problem is stated in objective, formal language without subjective or non-scientific claims.\n- **Completeness and Consistency**: The problem provides all necessary information, including the exact structure of the graphs and the parameters ($k$, $S$) for each test case. There are no internal contradictions. The requirement to implement from \"first principles\" is a specific instruction on methodology, not a flaw.\n- **Feasibility**: The specified graphs are small, making the computation of edge betweenness centrality and the simulation of edge removals computationally feasible within a reasonable time frame.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. It is a well-defined computational task based on established scientific principles. The solution process may proceed.\n\n### Solution\n\nThe solution involves developing a set of algorithms to compute the vulnerability score $V$ for each test case. The overall process for each case is to (1) compute edge betweenness centrality for all edges, (2) simulate targeted removal and compute $f_{\\mathrm{target}}$, (3) simulate random removals and compute $\\overline{f}_{\\mathrm{rand}}$, and (4) calculate $V$.\n\n**1. Algorithmic Core: Edge Betweenness Centrality ($B_e$)**\n\nThe central algorithmic challenge is the computation of edge betweenness centrality, $B_e$, from its fundamental definition without relying on a pre-built library function. The provided definition is $B_e \\equiv \\sum_{s \\in V}\\sum_{t \\in V, t \\neq s} \\frac{\\sigma_{s t}(e)}{\\sigma_{s t}}$. A direct enumeration of all paths for all pairs of nodes is computationally prohibitive. A more efficient method, based on the algorithm by Ulrik Brandes, is employed. This method calculates the sum of dependencies over all source nodes.\n\nThe algorithm proceeds as follows:\nInitialize the betweenness score $B_e$ to $0$ for all edges $e \\in E$. Then, for each node $s \\in V$ as a potential source:\n\n- **Phase 1: Shortest Path Counting**\n  A Breadth-First Search (BFS) is executed starting from $s$. During the traversal, we compute two quantities for every other node $v \\in V$:\n  1. The length of the shortest path(s) from $s$ to $v$, denoted $d(s,v)$.\n  2. The number of such shortest paths, denoted $\\sigma_{sv}$.\n  The BFS naturally explores the graph in layers of increasing distance from $s$. When moving from a node $u$ to a neighbor $v$, if $v$ is on a shortest path from $s$ (i.e., $d(s,v) = d(s,u) + 1$), the number of paths is updated: $\\sigma_{sv} = \\sigma_{sv} + \\sigma_{su}$. The algorithm also records the set of predecessors $P_s(v)$ for each node $v$, which are the neighbors of $v$ that lie on a shortest path from $s$. The collection of nodes and edges involved in shortest paths from $s$ forms a Directed Acyclic Graph (DAG).\n\n- **Phase 2: Dependency Accumulation**\n  After the BFS completes, the nodes are processed in reverse order of their distance from $s$. This is achieved by using a stack populated during the BFS. For each node $w$, a dependency score $\\delta_s(w)$ is calculated. This score represents the fraction of shortest paths from $s$ to nodes further away that pass through $w$. The update rule is derived from the DAG structure. For each predecessor $v \\in P_s(w)$, the contribution to the betweenness of the edge $(v,w)$ is given by $\\frac{\\sigma_{sv}}{\\sigma_{sw}}(1+\\delta_s(w))$. This value is added to $B_{(v,w)}$, and the dependency of the predecessor node is updated: $\\delta_s(v) \\leftarrow \\delta_s(v) + \\frac{\\sigma_{sv}}{\\sigma_{sw}}(1+\\delta_s(w))$.\n\nBy iterating through all nodes $s \\in V$ as sources and accumulating these contributions, we compute the final betweenness score for every edge. Since the given formula is a sum over ordered pairs $(s,t)$, and the Brandes algorithm correctly accumulates contributions for paths $s \\to t$ and $t \\to s$ separately (when run for sources $s$ and $t$ respectively), no final division by $2$ is required.\n\n**2. Network Analysis and Vulnerability Calculation**\n\nWith the ability to compute $B_e$, the vulnerability score $V$ is determined through the following procedure for each test case $(G, k, S)$:\n\n- **Step A: Compute LCC Fraction**\n  A helper function is required to determine the size of the largest connected component (LCC) of a given graph $G' = (V, E')$. This is implemented using a standard graph traversal algorithm, such as BFS or DFS. The function iterates through all nodes. If an unvisited node is found, a traversal is initiated to find all nodes in its component, marking them as visited and counting them. The maximum component size found across all such traversals is the LCC size, $\\lvert C_{\\max}(G')\\rvert$. The fraction is then $\\lvert C_{\\max}(G')\\rvert / \\lvert V\\rvert$.\n\n- **Step B: Targeted Removal**\n  1. Compute $B_e$ for all edges in the original graph $G$ using the algorithm described above.\n  2. Sort the edges in descending order of their $B_e$ values.\n  3. Select the top $k$ edges for removal, forming the set $R_{\\mathrm{target}}$.\n  4. Construct the residual graph $G_{\\mathrm{target}} = G \\setminus R_{\\mathrm{target}}$.\n  5. Calculate $f_{\\mathrm{target}} = f_{\\mathrm{LCC}}(G_{\\mathrm{target}})$.\n\n- **Step C: Random Removal**\n  1. Initialize an accumulator for the total LCC fraction, $\\Sigma f = 0$.\n  2. For $i=1, \\dots, S$:\n     a. From the original set of edges $E$, sample a set $R_{\\mathrm{rand}, i}$ of $k$ edges uniformly at random without replacement. A fixed-seed pseudorandom number generator ensures reproducibility.\n     b. Construct the residual graph $G_{\\mathrm{rand},i} = G \\setminus R_{\\mathrm{rand}, i}$.\n     c. Calculate $f_i = f_{\\mathrm{LCC}}(G_{\\mathrm{rand},i})$ and add it to the accumulator: $\\Sigma f \\leftarrow \\Sigma f + f_i$.\n  3. Compute the average LCC fraction: $\\overline{f}_{\\mathrm{rand}} = \\frac{1}{S} \\Sigma f$.\n\n- **Step D: Vulnerability Score**\n  The final vulnerability score is the difference between the average random impact and the targeted impact: $V = \\overline{f}_{\\mathrm{rand}} - f_{\\mathrm{target}}$. A special case for $k=0$ is handled, where no edges are removed, resulting in $V = 1.0 - 1.0 = 0.0$.\n\nThis comprehensive procedure is applied to each of the five test cases specified in the problem statement.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom collections import deque\n\ndef get_lcc_size(num_nodes, adj):\n    \"\"\"Calculates the size of the largest connected component using BFS.\"\"\"\n    if num_nodes == 0:\n        return 0\n    \n    visited = [False] * num_nodes\n    max_size = 0\n    for i in range(num_nodes):\n        if not visited[i]:\n            current_size = 0\n            q = deque([i])\n            visited[i] = True\n            while q:\n                u = q.popleft()\n                current_size += 1\n                if u in adj:\n                    for v in adj[u]:\n                        if not visited[v]:\n                            visited[v] = True\n                            q.append(v)\n            max_size = max(max_size, current_size)\n    return max_size\n\ndef calculate_edge_betweenness(num_nodes, edges):\n    \"\"\"\n    Computes edge betweenness centrality from first principles (Brandes' algorithm).\n    The graph is unweighted and undirected.\n    \"\"\"\n    adj = {i: [] for i in range(num_nodes)}\n    for u, v in edges:\n        adj[u].append(v)\n        adj[v].append(u)\n    \n    # Use canonical representation for undirected edges\n    edge_betweenness = {tuple(sorted(e)): 0.0 for e in edges}\n    \n    for s in range(num_nodes):\n        # Phase 1: Shortest path counting (BFS)\n        stack = []\n        predecessors = {i: [] for i in range(num_nodes)}\n        sigma = np.zeros(num_nodes)\n        distance = np.full(num_nodes, -1, dtype=int)\n        \n        sigma[s] = 1.0\n        distance[s] = 0\n        q = deque([s])\n        \n        while q:\n            v = q.popleft()\n            stack.append(v)\n            for w in adj[v]:\n                if distance[w]  0:\n                    q.append(w)\n                    distance[w] = distance[v] + 1\n                if distance[w] == distance[v] + 1:\n                    sigma[w] += sigma[v]\n                    predecessors[w].append(v)\n\n        # Phase 2: Dependency accumulation\n        delta = np.zeros(num_nodes)\n        while stack:\n            w = stack.pop()\n            for v in predecessors[w]:\n                # The division sigma[v] / sigma[w] can be zero if sigma[w] is zero, but\n                # in a connected component, sigma[w] > 0\n                if sigma[w] > 0:\n                    c = (sigma[v] / sigma[w]) * (1.0 + delta[w])\n                    edge = tuple(sorted((v, w)))\n                    edge_betweenness[edge] += c\n                    delta[v] += c\n                \n    return edge_betweenness\n\ndef solve_case(nodes, edges, k, S, rng):\n    \"\"\"\n    Solves a single test case for the vulnerability score V.\n    \"\"\"\n    num_nodes = len(nodes)\n    \n    if k == 0:\n        return 0.0\n\n    # Step 1: Compute edge betweenness centrality for the original graph\n    edge_betweenness = calculate_edge_betweenness(num_nodes, edges)\n    \n    # Step 2: Targeted removal\n    # Sort edges by betweenness centrality in descending order\n    sorted_edges_by_b = sorted(edge_betweenness.items(), key=lambda item: item[1], reverse=True)\n    \n    # Identify k edges with highest betweenness\n    edges_to_remove_target = {e[0] for e in sorted_edges_by_b[:k]}\n    \n    # Build adjacency list for the targeted residual graph\n    adj_target = {i: [] for i in range(num_nodes)}\n    for u, v in edges:\n        if tuple(sorted((u, v))) not in edges_to_remove_target:\n            adj_target[u].append(v)\n            adj_target[v].append(u)\n            \n    # Compute LCC fraction for the targeted case\n    lcc_target_size = get_lcc_size(num_nodes, adj_target)\n    f_target = lcc_target_size / num_nodes\n    \n    # Step 3: Random removal\n    total_f_rand = 0.0\n    canonical_edges = list(edge_betweenness.keys())\n    edge_indices = np.arange(len(canonical_edges))\n\n    for _ in range(S):\n        # Choose k distinct edges to remove at random\n        indices_to_remove = rng.choice(edge_indices, size=k, replace=False)\n        edges_to_remove_rand = {canonical_edges[i] for i in indices_to_remove}\n        \n        # Build adjacency list for the random residual graph\n        adj_rand = {i: [] for i in range(num_nodes)}\n        for u, v in edges:\n            if tuple(sorted((u, v))) not in edges_to_remove_rand:\n                adj_rand[u].append(v)\n                adj_rand[v].append(u)\n\n        # Compute LCC fraction and add to sum\n        lcc_rand_size = get_lcc_size(num_nodes, adj_rand)\n        total_f_rand += lcc_rand_size / num_nodes\n        \n    f_rand_avg = total_f_rand / S\n    \n    # Step 4: Compute vulnerability score\n    vulnerability = f_rand_avg - f_target\n    return vulnerability\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run simulations, and print results.\n    \"\"\"\n    # Test cases specification\n    case1_edges = [(0,1),(0,2),(0,3),(0,4),(1,2),(1,3),(1,4),(2,3),(2,4),(3,4),(5,6),(5,7),(5,8),(5,9),(6,7),(6,8),(6,9),(7,8),(7,9),(8,9),(4,5)]\n    case2_edges = [(i,j) for i in range(6) for j in range(i+1, 6)]\n    case3_edges = [(i, i+1) for i in range(5)]\n    case4_edges = [(0,1),(0,2),(0,3),(1,2),(1,3),(2,3),(4,5),(4,6),(4,7),(5,6),(5,7),(6,7),(0,4),(1,5),(2,6)]\n    \n    test_cases = [\n        (list(range(10)), case1_edges, 1, 200),\n        (list(range(6)), case2_edges, 1, 200),\n        (list(range(6)), case3_edges, 1, 200),\n        (list(range(8)), case4_edges, 2, 200),\n        (list(range(10)), case1_edges, 0, 50),\n    ]\n\n    # Use a fixed seed for the random number generator for reproducibility\n    rng = np.random.default_rng(seed=42)\n    \n    results = []\n    for nodes, edges, k, S in test_cases:\n        v_score = solve_case(nodes, edges, k, S, rng)\n        results.append(v_score)\n\n    # Format the final output string as specified\n    output_str = f\"[{','.join([f'{v:.3f}' for v in results])}]\"\n    print(output_str)\n\n# Execute the solver\nsolve()\n```"
        }
    ]
}