{
    "hands_on_practices": [
        {
            "introduction": "The foundation of classical QSAR lies in correlating physicochemical properties of molecules with their biological activity. This exercise walks you through a cornerstone of the field: the Hansch analysis. You will calculate two of the most influential descriptors—the substituent hydrophobicity constant ($\\pi$) and the electronic constant ($\\sigma$)—from fundamental data, and then use them to build a predictive model using multiple linear regression . This practice is invaluable for understanding how distinct chemical properties contribute to a molecule's overall biological effect.",
            "id": "3860378",
            "problem": "You are given a scenario in quantitative structure–activity relationships (QSAR) where substituent hydrophobicity and electronic effects are hypothesized to jointly influence a biological activity measured on a logarithmic scale. The fundamental bases are: the definition of the octanol–water partition coefficient $P$ and its logarithm $\\log_{10} P$ (hydrophobicity), and the Hammett equation relating substituent electronic effects to reaction or equilibrium constants. The substituent hydrophobicity constant $\\pi_i$ for substituent $i$ is defined as the change in $\\log_{10} P$ relative to the parent (hydrogen) substituent on the same aromatic scaffold, and the electronic constant $\\sigma_i$ is defined using the Hammett relation with a reaction constant $\\rho$ and the acid dissociation constant $K_a$ expressed via $pK_a$.\n\nStarting from these bases:\n- Octanol–water partition coefficient $P_i$ is the ratio of equilibrium concentrations in octanol and water phases, and hydrophobicity is quantified by $x_i = \\log_{10} P_i$ (dimensionless).\n- Substituent hydrophobicity constant is $ \\pi_i = x_i - x_H $, where $x_H = \\log_{10} P_H$ is the parent hydrophobicity (dimensionless).\n- The Hammett relation is $ \\log_{10}(K_i / K_0) = \\rho \\sigma_i $, where $K_i = 10^{-pK_{a,i}}$ and $K_0 = 10^{-pK_{a,0}}$. This implies $ \\sigma_i = -\\dfrac{pK_{a,i} - pK_{a,0}}{\\rho} $ (dimensionless).\n\nYou must:\n1. For each test case, compute $ \\pi_i $ and $ \\sigma_i $ for each substituent in the series from the provided $\\log_{10} P_i$, $\\log_{10} P_H$, $pK_{a,i}$, $pK_{a,0}$, and $\\rho$.\n2. Fit a multiple linear regression model to the provided activity values $A_i$ (dimensionless, log scale) of the form\n$$\nA_i = \\beta_0 + \\beta_{\\pi} \\pi_i + \\beta_{\\sigma} \\sigma_i + \\varepsilon_i,\n$$\nwhere $ \\beta_0 $, $ \\beta_{\\pi} $, and $ \\beta_{\\sigma} $ are regression coefficients and $ \\varepsilon_i $ is the residual. Use ordinary least squares to estimate $ \\beta_0 $, $ \\beta_{\\pi} $, and $ \\beta_{\\sigma} $. Compute the coefficient of determination\n$$\nR^2 = 1 - \\frac{\\sum_i (A_i - \\hat{A}_i)^2}{\\sum_i (A_i - \\bar{A})^2},\n$$\nwhere $ \\hat{A}_i $ are fitted values and $ \\bar{A} $ is the mean of the $A_i$.\n\n3. For each test case, output the list $[\\beta_0, \\beta_{\\pi}, \\beta_{\\sigma}, R^2]$ with each value rounded to six decimal places.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a bracketed four-element list in order. For example, the output should look like:\n[[_case1_b0_,_case1_bpi_,_case1_bsigma_,_case1_R2_],[_case2_b0_,_case2_bpi_,_case2_bsigma_,_case2_R2_],[_case3_b0_,_case3_bpi_,_case3_bsigma_,_case3_R2_]]\nwith numerical values substituted and no spaces.\n\nUse the following test suite. All quantities are dimensionless.\n\nTest Case 1 (general case):\n- Parent hydrophobicity $ x_H = \\log_{10} P_H = 2.13 $.\n- Parent acidity $ pK_{a,0} = 4.20 $.\n- Reaction constant $ \\rho = 1.00 $.\n- Substituent data arrays:\n    - $ x_i = [2.73, 2.18, 2.84, 2.11, 0.90, 3.00, 1.46] $.\n    - $ pK_{a,i} = [4.34, 3.45, 3.98, 4.47, 4.80, 3.60, 4.54] $.\n    - $ A_i = [0.812, 1.44, 1.332, 0.16, -1.204, 1.916, -0.444] $.\n\nTest Case 2 (boundary condition includes a case with $ \\pi_i = 0 $):\n- Parent hydrophobicity $ x_H = \\log_{10} P_H = 2.13 $.\n- Parent acidity $ pK_{a,0} = 4.20 $.\n- Reaction constant $ \\rho = 0.90 $.\n- Substituent data arrays:\n    - $ x_i = [2.73, 2.13, 2.84, 2.11, 0.90, 2.95, 1.46] $.\n    - $ pK_{a,i} = [4.30, 3.50, 4.00, 4.50, 4.85, 3.70, 4.52] $.\n    - $ A_i = [0.8288888889, 1.1777777778, 1.2612222222, 0.0486666667, -1.4292222222, 1.6935555556, -0.5585555556] $.\n\nTest Case 3 (edge case with stronger electronic weighting and mixed hydrophobicity extremes):\n- Parent hydrophobicity $ x_H = \\log_{10} P_H = 2.13 $.\n- Parent acidity $ pK_{a,0} = 4.20 $.\n- Reaction constant $ \\rho = 1.20 $.\n- Substituent data arrays:\n    - $ x_i = [2.73, 2.13, 3.05, 0.90, 1.46, 2.70] $.\n    - $ pK_{a,i} = [4.36, 3.40, 3.60, 4.86, 4.60, 3.95] $.\n    - $ A_i = [0.84, 0.9, 1.762, -1.548, -0.737, 1.1145] $.\n\nFinal Output Format Requirement:\n- Your program must print exactly one line: a single bracketed list of three bracketed lists, one per test case, each containing $[\\beta_0, \\beta_{\\pi}, \\beta_{\\sigma}, R^2]$ rounded to six decimal places, with commas and no spaces.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of quantitative structure-activity relationships (QSAR), specifically the Hansch analysis. It is well-posed, objective, and provides all necessary information to compute a unique solution.\n\nThe problem requires a multiple linear regression analysis for three distinct test cases. For each case, we are to determine the coefficients of a linear model and its coefficient of determination, $R^2$. The model relates biological activity, $A_i$, to two physicochemical properties of molecular substituents: the hydrophobicity constant, $\\pi_i$, and the electronic constant, $\\sigma_i$.\n\nThe procedure is as follows:\nFirst, for each substituent $i$ in a given test series, we must calculate the independent variables (descriptors) $\\pi_i$ and $\\sigma_i$ from the provided data.\n\nThe substituent hydrophobicity constant, $\\pi_i$, is defined as the difference between the logarithm of the octanol-water partition coefficient of the substituted molecule, $x_i = \\log_{10} P_i$, and that of the parent compound, $x_H = \\log_{10} P_H$. The formula is:\n$$\n\\pi_i = x_i - x_H\n$$\n\nThe substituent electronic constant, $\\sigma_i$, is derived from the Hammett equation, which relates reaction rates and equilibrium constants of substituted aromatic compounds. The definition provided is:\n$$\n\\sigma_i = -\\frac{pK_{a,i} - pK_{a,0}}{\\rho}\n$$\nwhere $pK_{a,i}$ is the negative base-$10$ logarithm of the acid dissociation constant for the substituted compound, $pK_{a,0}$ is that for the parent compound, and $\\rho$ is the reaction constant that characterizes the sensitivity of the reaction to electronic effects.\n\nSecond, with the descriptors $\\pi_i$ and $\\sigma_i$ calculated for each substituent, we fit the specified linear model:\n$$\nA_i = \\beta_0 + \\beta_{\\pi} \\pi_i + \\beta_{\\sigma} \\sigma_i + \\varepsilon_i\n$$\nHere, $A_i$ is the measured biological activity, $\\beta_0$ is the intercept, $\\beta_{\\pi}$ and $\\beta_{\\sigma}$ are the regression coefficients for the hydrophobicity and electronic effects respectively, and $\\varepsilon_i$ is the residual error.\n\nTo find the coefficients, we use the method of ordinary least squares (OLS). The problem can be expressed in matrix form as $\\mathbf{y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\varepsilon}$, where:\n- $\\mathbf{y}$ is an $n \\times 1$ column vector of the observed activities $A_i$.\n- $\\mathbf{X}$ is the $n \\times 3$ design matrix, where each row $i$ is $[1, \\pi_i, \\sigma_i]$. The first column of ones corresponds to the intercept term $\\beta_0$.\n- $\\mathbf{\\beta}$ is the $3 \\times 1$ column vector of coefficients to be estimated: $[\\beta_0, \\beta_{\\pi}, \\beta_{\\sigma}]^T$.\n- $\\mathbf{\\varepsilon}$ is an $n \\times 1$ column vector of the residuals.\n\nThe OLS estimate for $\\mathbf{\\beta}$ that minimizes the sum of squared residuals, $\\sum_i \\varepsilon_i^2$, is given by the normal equations:\n$$\n\\mathbf{\\hat{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n$$\nwhere $\\mathbf{\\hat{\\beta}}$ is the vector of estimated coefficients. This system is solved numerically for each test case.\n\nThird, after estimating the coefficients, we evaluate the goodness of fit of the model using the coefficient of determination, $R^2$. First, we calculate the predicted activities, $\\hat{A}_i$, for each substituent using the fitted model:\n$$\n\\hat{A}_i = \\hat{\\beta}_0 + \\hat{\\beta}_{\\pi} \\pi_i + \\hat{\\beta}_{\\sigma} \\sigma_i\n$$\nThis can be written in matrix form as $\\mathbf{\\hat{y}} = \\mathbf{X}\\mathbf{\\hat{\\beta}}$.\n\n$R^2$ is then computed as:\n$$\nR^2 = 1 - \\frac{\\text{SSR}}{\\text{SST}}\n$$\nwhere:\n- The Residual Sum of Squares (SSR) is the sum of the squared differences between the observed and predicted activities: $\\text{SSR} = \\sum_{i=1}^{n} (A_i - \\hat{A}_i)^2$.\n- The Total Sum of Squares (SST) is the sum of the squared differences between the observed activities and their mean, $\\bar{A} = \\frac{1}{n}\\sum_{i=1}^{n} A_i$: $\\text{SST} = \\sum_{i=1}^{n} (A_i - \\bar{A})^2$.\n\nFinally, for each test case, the resulting values $[\\hat{\\beta}_0, \\hat{\\beta}_{\\pi}, \\hat{\\beta}_{\\sigma}, R^2]$ are rounded to six decimal places and formatted as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the QSAR problem for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1\n        {\n            \"x_H\": 2.13,\n            \"pKa_0\": 4.20,\n            \"rho\": 1.00,\n            \"x_i\": np.array([2.73, 2.18, 2.84, 2.11, 0.90, 3.00, 1.46]),\n            \"pKa_i\": np.array([4.34, 3.45, 3.98, 4.47, 4.80, 3.60, 4.54]),\n            \"A_i\": np.array([0.812, 1.44, 1.332, 0.16, -1.204, 1.916, -0.444]),\n        },\n        # Test Case 2\n        {\n            \"x_H\": 2.13,\n            \"pKa_0\": 4.20,\n            \"rho\": 0.90,\n            \"x_i\": np.array([2.73, 2.13, 2.84, 2.11, 0.90, 2.95, 1.46]),\n            \"pKa_i\": np.array([4.30, 3.50, 4.00, 4.50, 4.85, 3.70, 4.52]),\n            \"A_i\": np.array([0.8288888889, 1.1777777778, 1.2612222222, 0.0486666667, -1.4292222222, 1.6935555556, -0.5585555556]),\n        },\n        # Test Case 3\n        {\n            \"x_H\": 2.13,\n            \"pKa_0\": 4.20,\n            \"rho\": 1.20,\n            \"x_i\": np.array([2.73, 2.13, 3.05, 0.90, 1.46, 2.70]),\n            \"pKa_i\": np.array([4.36, 3.40, 3.60, 4.86, 4.60, 3.95]),\n            \"A_i\": np.array([0.84, 0.9, 1.762, -1.548, -0.737, 1.1145]),\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        # Extract data for the current case\n        x_H = case[\"x_H\"]\n        pKa_0 = case[\"pKa_0\"]\n        rho = case[\"rho\"]\n        x_i = case[\"x_i\"]\n        pKa_i = case[\"pKa_i\"]\n        A_i = case[\"A_i\"]\n\n        # 1. Compute pi and sigma descriptors\n        pi_vals = x_i - x_H\n        sigma_vals = -(pKa_i - pKa_0) / rho\n\n        # 2. Fit a multiple linear regression model\n        # Construct the design matrix X\n        n_obs = len(A_i)\n        X = np.c_[np.ones(n_obs), pi_vals, sigma_vals]\n        \n        # Use ordinary least squares to estimate beta coefficients\n        # lstsq solves X * beta = A_i\n        beta, residuals, rank, s = np.linalg.lstsq(X, A_i, rcond=None)\n        beta_0, beta_pi, beta_sigma = beta[0], beta[1], beta[2]\n\n        # 3. Compute the coefficient of determination R^2\n        # Calculate predicted activities A_hat\n        A_hat = X @ beta\n        \n        # Calculate Total Sum of Squares (SST)\n        A_mean = np.mean(A_i)\n        sst = np.sum((A_i - A_mean)**2)\n        \n        # The 'residuals' output from lstsq is the Residual Sum of Squares (SSR)\n        # It's a 1-element array, so we access it with [0]\n        # This is valid only if sst > 0, which is true for non-constant A_i\n        ssr = residuals[0]\n        \n        # Handle the case where sst is zero (all A_i values are the same)\n        if sst == 0:\n            # If ssr is also zero, the fit is perfect. If ssr>0, the fit is meaningless.\n            # Conventionally, R^2 is set to 1.0 if SSR is also 0, otherwise undefined or 0.\n            R2 = 1.0 if ssr < 1e-9 else 0.0\n        else:\n            R2 = 1 - (ssr / sst)\n\n        # Format results for the current case\n        case_result_str = f\"[{beta_0:.6f},{beta_pi:.6f},{beta_sigma:.6f},{R2:.6f}]\"\n        all_results.append(case_result_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While QSAR models provide a global view of structure-activity relationships, medicinal chemists are often interested in \"activity cliffs\"—pairs of structurally similar molecules with surprisingly large differences in activity. Understanding these cliffs is key to navigating the SAR landscape. In this exercise, you will derive the Structure-Activity Landscape Index (SALI) from first principles, starting with the Tanimoto similarity of molecular fingerprints . Calculating this index provides a quantitative tool to identify and analyze these critical regions of the activity landscape, guiding rational drug design.",
            "id": "3860335",
            "problem": "Consider the modeling of Quantitative Structure-Activity Relationships (QSAR) where molecules are represented by high-dimensional binary fingerprints and biological activities are real-valued responses. An activity cliff is informally understood as a pair of molecules that are structurally very similar yet exhibit a large difference in biological activity. In order to quantify the steepness of local structure-activity relationships, construct an index from first principles that captures the ratio of activity change to structural distance in fingerprint space, using the following foundations: binary molecular fingerprints as sets of on-bits, the Jaccard or Tanimoto similarity between two sets, and the induced notion of a distance in the space of molecular representations.\n\nUse the following fundamental base without introducing any shortcut formulas not derived from it:\n- Binary fingerprints as sets: each molecule is mapped to a set of features (on-bits). Let molecule $i$ have a set $F_{i}$ and molecule $j$ have a set $F_{j}$.\n- The Tanimoto (Jaccard) similarity between two sets $F_{i}$ and $F_{j}$ is $S_{ij}$, defined as the ratio of size of intersection to size of union of the sets.\n- A distance in representation space can be induced from similarity via subtraction from $1$.\n\nFrom these primitives, derive an index that measures the local steepness of the activity landscape between two molecules as a discrete gradient in activity with respect to fingerprint distance, and state the resulting expression. Then apply your derived index to the following pair of molecules:\n- Molecule $i$ has an Extended Connectivity FingerPrint (ECFP) with $a = 128$ on-bits.\n- Molecule $j$ has an ECFP with $b = 142$ on-bits.\n- The size of the intersection of on-bits is $c = 110$.\n- The activity is the negative base-$10$ logarithm of inhibitory concentration, $pIC_{50}$, which is dimensionless. The measured activities are $A_{i} = 7.4$ and $A_{j} = 5.8$.\n\nCompute the value of the index for this pair. Round your answer to four significant figures. Express your final answer as a unitless quantity.",
            "solution": "The problem requires the construction of an index from first principles to quantify the local steepness of a structure-activity relationship, followed by its application to a specific pair of molecules. The index is defined as the ratio of the change in biological activity to the structural distance in fingerprint space.\n\nLet the two molecules be denoted by $i$ and $j$. Their measured biological activities are $A_i$ and $A_j$, respectively. The change in activity is the absolute difference between these values, $|\\Delta A| = |A_i - A_j|$.\n\nThe molecules are represented by binary fingerprints, which are sets of on-bits, denoted as $F_i$ and $F_j$. The structural relationship between them is first quantified by the Tanimoto similarity, $S_{ij}$. As per the problem statement, this is defined as the Jaccard similarity between the sets $F_i$ and $F_j$:\n$$S_{ij} = \\frac{|F_i \\cap F_j|}{|F_i \\cup F_j|}$$\nwhere $|...|$ denotes the cardinality (size) of a set.\n\nThe size of the union of two sets can be expressed using the principle of inclusion-exclusion:\n$$|F_i \\cup F_j| = |F_i| + |F_j| - |F_i \\cap F_j|$$\nSubstituting this into the expression for Tanimoto similarity gives:\n$$S_{ij} = \\frac{|F_i \\cap F_j|}{|F_i| + |F_j| - |F_i \\cap F_j|}$$\n\nThe structural distance, $D_{ij}$, is induced from the similarity by subtraction from $1$:\n$$D_{ij} = 1 - S_{ij}$$\nSubstituting the expression for $S_{ij}$:\n$$D_{ij} = 1 - \\frac{|F_i \\cap F_j|}{|F_i| + |F_j| - |F_i \\cap F_j|}$$\nTo simplify, we find a common denominator:\n$$D_{ij} = \\frac{(|F_i| + |F_j| - |F_i \\cap F_j|) - |F_i \\cap F_j|}{|F_i| + |F_j| - |F_i \\cap F_j|} = \\frac{|F_i| + |F_j| - 2|F_i \\cap F_j|}{|F_i| + |F_j| - |F_i \\cap F_j|}$$\nThis expression for $D_{ij}$ is also known as the Jaccard distance.\n\nThe index for the local steepness of the activity landscape, which we will denote as $I_{ij}$, is the discrete gradient defined as the ratio of activity change to structural distance:\n$$I_{ij} = \\frac{|A_i - A_j|}{D_{ij}}$$\nSubstituting the derived expression for $D_{ij}$:\n$$I_{ij} = \\frac{|A_i - A_j|}{1 - S_{ij}} = \\frac{|A_i - A_j|}{1 - \\frac{|F_i \\cap F_j|}{|F_i| + |F_j| - |F_i \\cap F_j|}}$$\nThis is the required index derived from the given first principles. It is a formulation of the Structure-Activity Landscape Index (SALI).\n\nNow, we apply this index to the provided data:\n-   Size of on-bits for molecule $i$: $|F_i| = a = 128$\n-   Size of on-bits for molecule $j$: $|F_j| = b = 142$\n-   Size of the intersection of on-bits: $|F_i \\cap F_j| = c = 110$\n-   Activity of molecule $i$: $A_i = 7.4$\n-   Activity of molecule $j$: $A_j = 5.8$\n\nFirst, we calculate the structural distance $D_{ij}$. We begin by computing the Tanimoto similarity $S_{ij}$. The size of the union of the fingerprint sets is:\n$$|F_i \\cup F_j| = |F_i| + |F_j| - |F_i \\cap F_j| = a + b - c = 128 + 142 - 110 = 270 - 110 = 160$$\nThe Tanimoto similarity is:\n$$S_{ij} = \\frac{|F_i \\cap F_j|}{|F_i \\cup F_j|} = \\frac{c}{160} = \\frac{110}{160} = \\frac{11}{16} = 0.6875$$\nThe structural distance is therefore:\n$$D_{ij} = 1 - S_{ij} = 1 - 0.6875 = 0.3125$$\n\nNext, we calculate the absolute difference in activity:\n$$|A_i - A_j| = |7.4 - 5.8| = 1.6$$\n\nFinally, we compute the index $I_{ij}$:\n$$I_{ij} = \\frac{|A_i - A_j|}{D_{ij}} = \\frac{1.6}{0.3125}$$\nTo perform the division:\n$$I_{ij} = \\frac{1.6}{0.3125} = \\frac{1.6}{1/3.2} = 1.6 \\times 3.2 = 5.12$$\nAlternatively, using fractions:\n$$D_{ij} = 0.3125 = \\frac{3125}{10000} = \\frac{5}{16}$$\n$$I_{ij} = \\frac{1.6}{5/16} = \\frac{16 \\times 1.6}{5} = \\frac{25.6}{5} = 5.12$$\n\nThe problem requires the answer to be rounded to four significant figures. The calculated value $5.12$ has three significant figures. To express this with four significant figures, we write it as $5.120$.",
            "answer": "$$\\boxed{5.120}$$"
        },
        {
            "introduction": "A predictive model is only as good as its validation. In this final practice, we address the critical task of evaluating a QSAR model's performance, paying close attention to the realities of experimental biological data, which often includes outliers. You will compute three different error metrics—Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and Median Absolute Error—for a set of predictions and analyze how a single outlier drastically affects them . This exercise will equip you to choose the most appropriate metric to robustly assess your model's true predictive power, avoiding overly pessimistic or optimistic conclusions.",
            "id": "3860364",
            "problem": "A Quantitative Structure–Activity Relationships (QSAR) model is trained to predict inhibitory potency reported as $pIC_{50}$ for a chemical series. The experimental endpoint $pIC_{50}$ is dimensionless. For a held-out test set of $ n = 20 $ compounds, the model produces predicted values $ \\hat{y}_i $ and corresponding residuals $ r_i = y_i - \\hat{y}_i $ given by\n$$\n\\{0.07,\\,-0.12,\\,0.09,\\,-0.05,\\,0.11,\\,-0.08,\\,0.10,\\,0.06,\\,-0.07,\\,0.13,\\,-0.04,\\,0.15,\\,0.14,\\,-0.06,\\,0.05,\\,-0.09,\\,0.08,\\,-0.03,\\,0.12,\\,-1.50\\}.\n$$\nAssume residuals are independent and identically distributed across compounds, and that the measurement noise in $pIC_{50}$ is bounded for the majority of compounds but occasional annotation errors may induce rare large-magnitude residuals. Using foundational definitions of error aggregation in predictive modeling, compute the root mean squared error (RMSE), the mean absolute error (MAE), and the median absolute error for this test set. Then, justify from first principles when each metric is robust to outliers or skewed residuals in QSAR datasets, identifying the error distributional assumptions under which each metric is preferable. Round all three numerical answers to four significant figures. Express each metric in the same units as $pIC_{50}$ (dimensionless).",
            "solution": "The problem statement is evaluated as scientifically grounded, well-posed, and objective. All necessary data and definitions for the computation of standard statistical error metrics are provided. The context is a valid application of predictive modeling in computational chemical biology. The problem is therefore deemed valid and a solution will be provided.\n\nThe task is to compute three error metrics for a given set of residuals from a Quantitative Structure–Activity Relationships (QSAR) model and to provide a justification for the use of each metric based on its statistical properties. The endpoint, $pIC_{50}$, is dimensionless, and therefore the residuals and all calculated error metrics will also be dimensionless.\n\nThe set of $ n = 20 $ residuals is given by $ R = \\{r_i\\}_{i=1}^{20} $:\n$$\n\\{0.07,\\,-0.12,\\,0.09,\\,-0.05,\\,0.11,\\,-0.08,\\,0.10,\\,0.06,\\,-0.07,\\,0.13,\\,-0.04,\\,0.15,\\,0.14,\\,-0.06,\\,0.05,\\,-0.09,\\,0.08,\\,-0.03,\\,0.12,\\,-1.50\\}\n$$\nThe outlier is the residual $r_{20} = -1.50$.\n\nFirst, we define and compute the requested metrics.\n\n**1. Root Mean Squared Error (RMSE)**\n\nThe RMSE is the square root of the mean of the squared errors. It is defined as:\n$$\n\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} r_i^2}\n$$\nWe first compute the sum of the squares of the residuals:\n\\begin{align*}\n\\sum_{i=1}^{20} r_i^2 &= (0.07)^2 + (-0.12)^2 + (0.09)^2 + (-0.05)^2 + (0.11)^2 + (-0.08)^2 + (0.10)^2 + (0.06)^2 + (-0.07)^2 \\\\\n&\\quad + (0.13)^2 + (-0.04)^2 + (0.15)^2 + (0.14)^2 + (-0.06)^2 + (0.05)^2 + (-0.09)^2 + (0.08)^2 + (-0.03)^2 \\\\\n&\\quad + (0.12)^2 + (-1.50)^2 \\\\\n&= 0.0049 + 0.0144 + 0.0081 + 0.0025 + 0.0121 + 0.0064 + 0.0100 + 0.0036 + 0.0049 + 0.0169 \\\\\n&\\quad + 0.0016 + 0.0225 + 0.0196 + 0.0036 + 0.0025 + 0.0081 + 0.0064 + 0.0009 + 0.0144 + 2.25 \\\\\n&= 0.1634 + 2.25 = 2.4134\n\\end{align*}\nThe mean squared error (MSE) is:\n$$\n\\text{MSE} = \\frac{2.4134}{20} = 0.12067\n$$\nThe RMSE is the square root of the MSE:\n$$\n\\text{RMSE} = \\sqrt{0.12067} \\approx 0.347375876...\n$$\nRounding to four significant figures, we get $\\text{RMSE} = 0.3474$.\n\n**2. Mean Absolute Error (MAE)**\n\nThe MAE is the mean of the absolute values of the errors. It is defined as:\n$$\n\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |r_i|\n$$\nWe first compute the sum of the absolute values of the residuals:\n\\begin{align*}\n\\sum_{i=1}^{20} |r_i| &= |0.07| + |-0.12| + |0.09| + |-0.05| + |0.11| + |-0.08| + |0.10| + |0.06| + |-0.07| + |0.13| \\\\\n&\\quad + |-0.04| + |0.15| + |0.14| + |-0.06| + |0.05| + |-0.09| + |0.08| + |-0.03| + |0.12| + |-1.50| \\\\\n&= 0.07 + 0.12 + 0.09 + 0.05 + 0.11 + 0.08 + 0.10 + 0.06 + 0.07 + 0.13 + 0.04 + 0.15 + 0.14 \\\\\n&\\quad + 0.06 + 0.05 + 0.09 + 0.08 + 0.03 + 0.12 + 1.50 \\\\\n&= 1.64 + 1.50 = 3.14\n\\end{align*}\nThe MAE is:\n$$\n\\text{MAE} = \\frac{3.14}{20} = 0.157\n$$\nRounding to four significant figures, we get $\\text{MAE} = 0.1570$.\n\n**3. Median Absolute Error**\n\nThe Median Absolute Error is the median of the absolute values of the errors. It is defined as:\n$$\n\\text{MedianAE} = \\text{median}(\\{|r_i|\\}_{i=1}^n)\n$$\nFirst, we list the absolute values of the residuals:\n$$\n\\{0.07,\\,0.12,\\,0.09,\\,0.05,\\,0.11,\\,0.08,\\,0.10,\\,0.06,\\,0.07,\\,0.13,\\,0.04,\\,0.15,\\,0.14,\\,0.06,\\,0.05,\\,0.09,\\,0.08,\\,0.03,\\,0.12,\\,1.50\\}\n$$\nNext, we sort this list in ascending order:\n$$\n\\{0.03,\\,0.04,\\,0.05,\\,0.05,\\,0.06,\\,0.06,\\,0.07,\\,0.07,\\,0.08,\\,{\\bf 0.08},\\,{\\bf 0.09},\\,0.09,\\,0.10,\\,0.11,\\,0.12,\\,0.12,\\,0.13,\\,0.14,\\,0.15,\\,1.50\\}\n$$\nSince there are $n=20$ values (an even number), the median is the average of the two middle values, which are the 10th and 11th values in the sorted list.\n$$\n\\text{MedianAE} = \\frac{0.08 + 0.09}{2} = \\frac{0.17}{2} = 0.085\n$$\nRounding to four significant figures, we get $\\text{MedianAE} = 0.08500$.\n\n**Justification of Metric Choice**\n\nThe choice of an error metric depends on the assumed distribution of the errors and the desired properties of the assessment, particularly robustness to outliers.\n\n**Root Mean Squared Error (RMSE)** is associated with the $L_2$ norm. Minimizing the sum of squared errors is equivalent to finding the maximum likelihood estimate for a model assuming the errors are independent and identically distributed (i.i.d.) according to a Gaussian (normal) distribution, $r_i \\sim \\mathcal{N}(0, \\sigma^2)$. The squaring operation gives disproportionately high weight to large errors. In this dataset, the residual $-1.50$ contributes $(-1.50)^2 = 2.25$ to the sum of squares, which is $2.25 / 0.1634 \\approx 13.8$ times the sum of all other $19$ squared residuals. This extreme sensitivity makes RMSE a non-robust metric. It is preferable only when large errors are considered exceptionally detrimental and must be heavily penalized, and when the underlying error distribution is believed to be Gaussian, without heavy tails or outliers. In the context of QSAR, if an outlier is a genuine, albeit rare, instance of poor model performance, RMSE would rightly flag it. However, if outliers stem from data annotation or experimental errors, as suggested in the problem, RMSE provides a distorted and pessimistic view of the model's typical performance.\n\n**Mean Absolute Error (MAE)** is associated with the $L_1$ norm. Minimizing the sum of absolute errors is equivalent to finding the maximum likelihood estimate assuming the errors are i.i.d. according to a Laplace distribution, $r_i \\sim \\text{Laplace}(0, b)$. This distribution has heavier tails than the Gaussian distribution. MAE gives a linear penalty to errors, meaning its value scales directly with the magnitude of the error. The contribution of the outlier $-1.50$ to the sum of absolute errors is $|-1.50|=1.50$, which is less dominant than in the RMSE case. MAE is therefore considered more robust to outliers than RMSE. It is preferable when the error distribution is suspected to be heavy-tailed or when the dataset contains outliers that should not disproportionately influence the overall error measure. This aligns well with the problem description where occasional large-magnitude residuals due to annotation errors are expected.\n\n**Median Absolute Error**, as a non-parametric statistic, does not rely on a specific parametric error distribution assumption. The median is a robust measure of central tendency, as it is defined by the rank order of the data, not the magnitude of extreme values. In our dataset, the median of the absolute errors is determined by the 10th and 11th values in the sorted list, and the extreme value of $1.50$ has no more influence on the result than if it had been, for example, $0.16$. This makes the Median Absolute Error exceptionally robust to outliers. It is the most suitable metric when the primary goal is to understand the typical performance of the model on the majority of the compounds, while intentionally ignoring the influence of rare, large errors that may be artifacts of the data collection process. Given the problem states that \"occasional annotation errors may induce rare large-magnitude residuals\", the Median Absolute Error provides the most faithful assessment of the model's predictive power for the bulk of the chemical series.\n\nIn summary, for this specific QSAR problem, the high value of RMSE ($0.3474$) compared to MAE ($0.1570$) is driven almost entirely by the single outlier. The Median Absolute Error ($0.08500$) is much smaller still, indicating that for at least half of the compounds, the model's prediction error is less than $0.085$ $pIC_{50}$ units. This suggests the model performs well on typical compounds, a fact obscured by RMSE.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.3474 & 0.1570 & 0.08500 \\end{pmatrix}}\n$$"
        }
    ]
}