{
    "hands_on_practices": [
        {
            "introduction": "We begin by exploring one of the most widely used 2D descriptors in drug discovery: the Topological Polar Surface Area (TPSA). This practice demonstrates the power of fragment-based approaches, where a complex molecular property is estimated by summing pre-calculated contributions from its constituent polar atoms . By performing this straightforward calculation, you will gain insight into how simple structural features are used to predict a molecule's likelihood of crossing biological membranes, a critical aspect of Quantitative Structure–Activity Relationship (QSAR) modeling.",
            "id": "3854295",
            "problem": "A central goal in Quantitative Structure–Activity Relationship (QSAR) modeling is to relate molecular structure to biological properties via interpretable descriptors. One widely used descriptor is the topological polar surface area (TPSA), which approximates the polar surface area using a fragment-additive scheme over heteroatom environments. In the fragment-based approach originally put forward by Ertl and co-workers, TPSA is computed as an additive sum over predefined fragment contributions associated with specific nitrogen and oxygen atom environments on the molecular graph. Assume a molecule whose only polar contributors in this fragment scheme are one amide nitrogen (fragment contribution $12.03$), one tertiary amine nitrogen (fragment contribution $3.24$), and one pyridine ring nitrogen (fragment contribution $12.89$). Using the principle that fragment-based TPSA is additive over the contributing heteroatom fragments on the molecular graph, compute the total TPSA. Then, considering widely used permeability heuristics that associate lower TPSA with increased likelihood of passive membrane permeability and central nervous system exposure (with rough heuristic thresholds near $90$ for blood–brain barrier penetration and $140$ for oral bioavailability), briefly interpret whether the computed TPSA suggests a molecule that is likely to be highly permeable. Express the final TPSA in square ångströms ($\\mathrm{\\AA^{2}}$) and round your numeric answer to $4$ significant figures.",
            "solution": "The problem is valid. It is scientifically grounded in the principles of Quantitative Structure–Activity Relationship (QSAR) modeling, specifically the concept of Topological Polar Surface Area (TPSA) as a molecular descriptor. The data provided are consistent and sufficient for a unique solution.\n\nThe problem states that the total Topological Polar Surface Area (TPSA) is computed as an additive sum of contributions from predefined polar fragments. The molecule in question has three such contributing fragments. Let the contribution from the amide nitrogen be denoted as $c_{amide}$, the contribution from the tertiary amine nitrogen as $c_{tert-amine}$, and the contribution from the pyridine ring nitrogen as $c_{pyridine}$. The values provided are:\n$$\nc_{amide} = 12.03~\\mathrm{\\AA^{2}}\n$$\n$$\nc_{tert-amine} = 3.24~\\mathrm{\\AA^{2}}\n$$\n$$\nc_{pyridine} = 12.89~\\mathrm{\\AA^{2}}\n$$\nAccording to the principle of additivity, the total TPSA, denoted as $TPSA_{total}$, is the sum of these individual contributions:\n$$\nTPSA_{total} = c_{amide} + c_{tert-amine} + c_{pyridine}\n$$\nSubstituting the given numerical values into the equation:\n$$\nTPSA_{total} = 12.03 + 3.24 + 12.89\n$$\nPerforming the summation:\n$$\nTPSA_{total} = 28.16~\\mathrm{\\AA^{2}}\n$$\nThe problem requires the answer to be rounded to $4$ significant figures. The calculated value of $28.16$ already consists of $4$ significant figures, so no further rounding is necessary.\n\nThe second part of the task is to interpret this result in the context of membrane permeability and central nervous system (CNS) exposure. The provided heuristics state rough thresholds for these properties.\nThe heuristic for blood–brain barrier (BBB) penetration suggests that molecules with a TPSA below approximately $90~\\mathrm{\\AA^{2}}$ are more likely to be CNS-permeable. Our calculated value is $TPSA_{total} = 28.16~\\mathrm{\\AA^{2}}$. Since $28.16  90$, the molecule is predicted to have a high likelihood of crossing the blood–brain barrier.\n\nThe heuristic for oral bioavailability suggests that molecules with a TPSA below approximately $140~\\mathrm{\\AA^{2}}$ are more likely to be well-absorbed. Our calculated value of $28.16~\\mathrm{\\AA^{2}}$ is also well below this threshold, as $28.16  140$. This suggests the molecule is likely to have good oral bioavailability.\n\nIn summary, the computed TPSA of $28.16~\\mathrm{\\AA^{2}}$ is a low value that falls significantly below the common heuristic thresholds for both BBB penetration and oral bioavailability. Therefore, based on this single descriptor, the molecule is predicted to be highly permeable.",
            "answer": "$$\\boxed{28.16}$$"
        },
        {
            "introduction": "Moving from simple descriptors to more complex structural representations, we now delve into the generation of molecular fingerprints. This exercise provides a step-by-step walkthrough of the Extended-Connectivity Fingerprint (ECFP) algorithm, one of the most successful and widely used fingerprinting methods in cheminformatics . By manually tracing the iterative hashing process, you will understand how local atomic environments are encoded into features and see precisely how stereochemical information can alter the resulting fingerprint, a key consideration for modeling stereoselective biological interactions.",
            "id": "3854395",
            "problem": "Consider the Extended-Connectivity Fingerprint (ECFP) algorithm used in Quantitative Structure–Activity Relationship (QSAR) modeling, which constructs features by iteratively hashing atom-centered neighborhoods in a molecular graph. In this problem, use a simplified and deterministic instantiation of ECFP with a radius of $R=1$ (equivalently, ECFP2) to quantify the effect of including chirality at a stereocenter. The molecule is 2-butanol, with the heavy-atom graph and implicit hydrogen counts specified as follows: atom $1$ is a terminal carbon ($\\mathrm{C}$) in $\\mathrm{CH}_{3}$ attached to atom $2$; atom $2$ is the chiral carbon ($\\mathrm{C}$) attached to atoms $1$, $3$, and $4$; atom $3$ is oxygen ($\\mathrm{O}$) in $\\mathrm{OH}$ attached to atom $2$; atom $4$ is a methylene carbon ($\\mathrm{C}$) attached to atoms $2$ and $5$; atom $5$ is the terminal carbon ($\\mathrm{C}$) in $\\mathrm{CH}_{3}$ attached to atom $4$. The implicit hydrogen counts on the heavy atoms are: $h_{1}=3$, $h_{2}=1$, $h_{3}=1$, $h_{4}=2$, $h_{5}=3$. All atoms are non-aromatic and not in rings.\n\nDefine the initial atom invariant for atom $i$ as\n$$\nv_{i} \\;=\\; Z_{i} \\;+\\; 10\\,d_{i} \\;+\\; 100\\,h_{i} \\;+\\; 1000\\,r_{i},\n$$\nwhere $Z_{i}$ is the atomic number, $d_{i}$ is the heavy-atom degree, $h_{i}$ is the implicit hydrogen count, and $r_{i}\\in\\{0,1\\}$ indicates ring membership ($0$ for non-ring). Incorporate chirality for the stereocenter at atom $2$ by adding a chirality term $\\chi_{2}$, where $\\chi_{2}=+5$ for the $R$ configuration and $\\chi_{2}=0$ when chirality is not used. For all other atoms $i\\neq 2$, set $\\chi_{i}=0$. Thus, with chirality, use $v_{2}^{\\text{(chiral)}}=v_{2}+\\chi_{2}$; without chirality, use $v_{2}^{\\text{(no)}}=v_{2}$.\n\nLet the fingerprint length be $m=16$. Construct features at radius $0$ and radius $1$ as follows:\n- Radius $0$: for each atom $i$, compute the bit index $b^{(0)}_{i} \\equiv v_{i} \\bmod m$ and set that bit in the fingerprint.\n- Radius $1$: for each atom $i$, define the neighborhood-aggregated value\n$$\nu_{i} \\;=\\; v_{i} \\;+\\; \\sum_{j \\in N(i)} v_{j},\n$$\nwhere $N(i)$ is the set of heavy-atom neighbors of $i$, and compute the bit index $b^{(1)}_{i} \\equiv u_{i} \\bmod m$; set that bit in the fingerprint.\n\nCompute the two fingerprints for the $R$ enantiomer: one with chirality (using $\\chi_{2}=+5$) and one without chirality (using $\\chi_{2}=0$). Quantify the effect of including chirality by computing the Hamming distance between the two bit vectors (the number of positions at which the corresponding bits differ). Express your final answer as an integer with no units. No rounding is required.",
            "solution": "The solution requires calculating two separate fingerprints using the specified ECFP algorithm—one without considering chirality and one with—and then finding the Hamming distance between the resulting bit vectors.\n\n**1. Base Atom Invariants**\n\nFirst, we calculate the base invariant $v_i$ for each heavy atom using the provided formula, $v_{i} = Z_{i} + 10\\,d_{i} + 100\\,h_{i} + 1000\\,r_{i}$. The properties for each atom are:\n-   Atom 1 (C): $Z_1=6, d_1=1, h_1=3, r_1=0 \\implies v_1 = 316$.\n-   Atom 2 (C): $Z_2=6, d_2=3, h_2=1, r_2=0 \\implies v_2 = 136$.\n-   Atom 3 (O): $Z_3=8, d_3=1, h_3=1, r_3=0 \\implies v_3 = 118$.\n-   Atom 4 (C): $Z_4=6, d_4=2, h_4=2, r_4=0 \\implies v_4 = 226$.\n-   Atom 5 (C): $Z_5=6, d_5=1, h_5=3, r_5=0 \\implies v_5 = 316$.\nThe heavy-atom neighborhoods are: $N(1)=\\{2\\}$, $N(2)=\\{1, 3, 4\\}$, $N(3)=\\{2\\}$, $N(4)=\\{2, 5\\}$, and $N(5)=\\{4\\}$.\n\n**2. Fingerprint Without Chirality**\n\nUsing the base invariants $\\{316, 136, 118, 226, 316\\}$ and fingerprint length $m=16$, we generate features.\n-   **Radius 0 Features**: The bit indices are $v_i \\bmod 16$.\n    -   $v_1=316 \\equiv 12 \\pmod{16}$\n    -   $v_2=136 \\equiv 8 \\pmod{16}$\n    -   $v_3=118 \\equiv 6 \\pmod{16}$\n    -   $v_4=226 \\equiv 2 \\pmod{16}$\n    -   $v_5=316 \\equiv 12 \\pmod{16}$\n    The set of unique radius-0 bits is $S^{(0)}_{\\text{no}} = \\{2, 6, 8, 12\\}$.\n-   **Radius 1 Features**: The bit indices are $(v_i + \\sum_{j \\in N(i)} v_j) \\bmod 16$.\n    -   Atom 1: $(316 + 136) = 452 \\equiv 4 \\pmod{16}$\n    -   Atom 2: $(136 + 316 + 118 + 226) = 796 \\equiv 12 \\pmod{16}$\n    -   Atom 3: $(118 + 136) = 254 \\equiv 14 \\pmod{16}$\n    -   Atom 4: $(226 + 136 + 316) = 678 \\equiv 6 \\pmod{16}$\n    -   Atom 5: $(316 + 226) = 542 \\equiv 14 \\pmod{16}$\n    The set of unique radius-1 bits is $S^{(1)}_{\\text{no}} = \\{4, 6, 12, 14\\}$.\nThe complete set of bits for the non-chiral fingerprint is $S_{\\text{no}} = S^{(0)}_{\\text{no}} \\cup S^{(1)}_{\\text{no}} = \\{2, 4, 6, 8, 12, 14\\}$.\n\n**3. Fingerprint With Chirality**\n\nFor the chiral fingerprint, the invariant for the stereocenter (atom 2) is updated: $v'_{2} = v_2 + \\chi_2 = 136 + 5 = 141$.\n-   **Radius 0 Features**: Only the feature for atom 2 changes.\n    -   $v'_{2}=141 \\equiv 13 \\pmod{16}$.\n    The set of unique radius-0 bits is $S^{(0)}_{\\text{chiral}} = \\{2, 6, 12, 13\\}$.\n-   **Radius 1 Features**: Features for atom 2 and its neighbors (1, 3, 4) change.\n    -   Atom 1: $(316 + 141) = 457 \\equiv 9 \\pmod{16}$\n    -   Atom 2: $(141 + 316 + 118 + 226) = 801 \\equiv 1 \\pmod{16}$\n    -   Atom 3: $(118 + 141) = 259 \\equiv 3 \\pmod{16}$\n    -   Atom 4: $(226 + 141 + 316) = 683 \\equiv 11 \\pmod{16}$\n    -   Atom 5: Unchanged, $542 \\equiv 14 \\pmod{16}$.\n    The set of unique radius-1 bits is $S^{(1)}_{\\text{chiral}} = \\{1, 3, 9, 11, 14\\}$.\nThe complete set of bits for the chiral fingerprint is $S_{\\text{chiral}} = S^{(0)}_{\\text{chiral}} \\cup S^{(1)}_{\\text{chiral}} = \\{1, 2, 3, 6, 9, 11, 12, 13, 14\\}$.\n\n**4. Hamming Distance**\n\nThe Hamming distance is the number of positions at which the two fingerprints differ, which is the size of the symmetric difference of their sets of enabled bits.\n-   Bits in $S_{\\text{no}}$ but not in $S_{\\text{chiral}}$: $\\{4, 8\\}$ (2 bits).\n-   Bits in $S_{\\text{chiral}}$ but not in $S_{\\text{no}}$: $\\{1, 3, 9, 11, 13\\}$ (5 bits).\nThe total Hamming distance is $2 + 5 = 7$.",
            "answer": "$$\n\\boxed{7}\n$$"
        },
        {
            "introduction": "Having learned to generate descriptors for individual molecules, the final step is to analyze the resulting high-dimensional data for an entire compound library. This practice guides you through implementing Principal Component Analysis (PCA) from first principles, the cornerstone technique for reducing the dimensionality of a descriptor space . By computing the eigendecomposition of the covariance matrix and projecting data onto the principal components, you will learn to uncover hidden patterns, interpret descriptor importance, and visualize the chemical space of a dataset, which are essential skills for building robust QSAR models.",
            "id": "3854316",
            "problem": "You are given small, standardized molecular descriptor matrices representing collections of compounds used in Quantitative Structure–Activity Relationship (QSAR). Principal Component Analysis (PCA) is to be performed entirely from first principles by computing the eigendecomposition of the sample covariance of the standardized descriptors, projecting the samples onto the first two principal components, and interpreting component loadings.\n\nBase definitions and facts to use:\n- Standardized descriptors are arranged in a matrix $Z \\in \\mathbb{R}^{n \\times p}$ whose columns (descriptors) have zero mean across samples. The sample covariance matrix is defined as $$C = \\frac{1}{n-1} Z^\\top Z.$$\n- Principal Component Analysis (PCA) seeks orthonormal directions (principal axes) $v_1, v_2, \\dots, v_p \\in \\mathbb{R}^p$ that maximize the variance of the projected data under unit-length constraints. For the first principal component direction $v_1$, this is equivalent to maximizing the Rayleigh quotient $v^\\top C v$ subject to $\\|v\\|_2 = 1$, which yields that $v_1$ is the eigenvector of $C$ corresponding to the largest eigenvalue $\\lambda_1$. Subsequent principal components $v_2, \\dots$ follow similarly under orthogonality constraints.\n- Given the eigen-decomposition $$C = V \\Lambda V^\\top,$$ with $V$ orthonormal and $\\Lambda = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_p)$ sorted so that $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_p \\ge 0$, the projection (scores) of samples onto the first two principal components is $$T_{(2)} = Z \\, V_{(2)},$$ where $V_{(2)}$ contains the first two eigenvectors as columns. The component loadings for the first two components are defined as $$L_{(2)} = V_{(2)} \\, \\mathrm{diag}\\!\\big(\\sqrt{\\lambda_1}, \\sqrt{\\lambda_2}\\big).$$\n- The explained variance ratio for principal component $j$ is $$r_j = \\frac{\\lambda_j}{\\sum_{k=1}^{p} \\lambda_k}.$$\n\nYour program must:\n1. For each provided test case, compute the sample covariance $C$, its eigenvalues and eigenvectors, sort eigenvalues in descending order with their eigenvectors accordingly, obtain the first two eigenvalues $[\\lambda_1, \\lambda_2]$, the first two eigenvectors $V_{(2)}$, the scores $T_{(2)}$, and the loadings $L_{(2)}$.\n2. Interpret the loadings by identifying, for each of the first two principal components, the index (using $0$-based indexing) of the descriptor with the maximum absolute loading, i.e., for component $j \\in \\{1,2\\}$, return $$\\arg\\max_{i \\in \\{0,\\dots,p-1\\}} |L_{(2)}[i,j-1]|.$$\n3. Compute the explained variance ratios $[r_1, r_2]$.\n4. Return, for each test case, a nested list containing:\n   - The first two eigenvalues $[\\lambda_1, \\lambda_2]$,\n   - The explained variance ratios $[r_1, r_2]$,\n   - The indices of descriptors with maximum absolute loadings for the first two components $[\\mathrm{idx}_1, \\mathrm{idx}_2]$,\n   - The first sample’s two-dimensional projection (scores) $[t_{1,1}, t_{1,2}]$ where $t_{1,j}$ is the score of the first sample on component $j$.\n\nAll floating-point outputs must be rounded to $6$ decimal places. All descriptor indices must be integers via $0$-based indexing. The final output must be a single line that aggregates the results for all test cases into one list, with no spaces, e.g., $$\\big[\\text{case}_1,\\text{case}_2,\\text{case}_3\\big],$$ where each $\\text{case}_k$ is the nested list described above.\n\nTest suite (each $Z^{(k)}$ is standardized to zero mean per descriptor):\n- Case $1$: $$Z^{(1)} = \\begin{bmatrix}\n-1.2  -1.0  \\phantom{-}0.8  \\phantom{-}0.5 \\\\\n\\phantom{-}0.4  \\phantom{-}0.3  -0.6  -0.2 \\\\\n\\phantom{-}0.8  \\phantom{-}0.9  -0.7  \\phantom{-}0.1 \\\\\n-0.5  -0.7  \\phantom{-}0.9  -0.4 \\\\\n\\phantom{-}0.3  \\phantom{-}0.4  -0.2  \\phantom{-}0.0 \\\\\n\\phantom{-}0.2  \\phantom{-}0.1  -0.2  \\phantom{-}0.0 \\\\\n\\end{bmatrix}.$$\n- Case $2$ (boundary condition with perfect collinearity between the first two descriptors): $$Z^{(2)} = \\begin{bmatrix}\n-1.5  -3.0  \\phantom{-}0.6 \\\\\n-0.5  -1.0  -0.2 \\\\\n\\phantom{-}0.5  \\phantom{-}1.0  -0.1 \\\\\n\\phantom{-}1.0  \\phantom{-}2.0  -0.2 \\\\\n\\phantom{-}0.5  \\phantom{-}1.0  -0.1 \\\\\n\\end{bmatrix}.$$\n- Case $3$ (edge case with a zero-variance descriptor): $$Z^{(3)} = \\begin{bmatrix}\n\\phantom{-}0.5  -0.3  \\phantom{-}0.0  \\phantom{-}0.7 \\\\\n-0.5  \\phantom{-}0.3  \\phantom{-}0.0  -0.7 \\\\\n\\phantom{-}0.4  -0.2  \\phantom{-}0.0  \\phantom{-}0.6 \\\\\n-0.4  \\phantom{-}0.2  \\phantom{-}0.0  -0.6 \\\\\n\\end{bmatrix}.$$\n\nYour program should produce a single line of output containing a list of results for the three cases, with each case represented as a nested list $[\\,[\\lambda_1,\\lambda_2],[r_1,r_2],[\\mathrm{idx}_1,\\mathrm{idx}_2],[t_{1,1},t_{1,2}]\\,]$, all floats rounded to $6$ decimal places and no spaces anywhere in the line. Angles are not involved, so no angle units are required, and the explained variance ratios must be expressed as decimals.",
            "solution": "The task is to perform Principal Component Analysis (PCA) on given standardized molecular descriptor matrices ($Z$) from first principles. This involves computing the sample covariance matrix ($C$), performing its eigendecomposition to find the principal components, and then calculating derived quantities such as scores, loadings, and explained variance ratios.\n\nThe core of PCA lies in finding a new set of orthogonal axes, called principal components, that align with the directions of maximum variance in the data. For a data matrix $Z \\in \\mathbb{R}^{n \\times p}$ with $n$ samples and $p$ descriptors, where each descriptor (column) is centered to have zero mean, the sample covariance matrix is given by:\n$$C = \\frac{1}{n-1} Z^\\top Z$$\nThe principal components are the eigenvectors of this covariance matrix $C$. Let the eigendecomposition of $C$ be $C = V \\Lambda V^\\top$, where $V$ is an orthonormal matrix whose columns $v_j$ are the eigenvectors, and $\\Lambda$ is a diagonal matrix of the corresponding eigenvalues $\\lambda_j$. The eigenvalues are sorted in descending order, $\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p \\geq 0$, such that the first eigenvector $v_1$ corresponds to the direction of maximum variance in the data.\n\nThe amount of variance captured by each principal component $j$ is given by its eigenvalue $\\lambda_j$. The explained variance ratio, $r_j$, normalizes this value by the total variance in the data, which is the sum of all eigenvalues (equal to the trace of $C$):\n$$r_j = \\frac{\\lambda_j}{\\sum_{k=1}^{p} \\lambda_k}$$\n\nThe original data can be projected onto the new coordinate system defined by the principal components. The resulting coordinates are called scores. The scores for the first two principal components are calculated by projecting the data matrix $Z$ onto the first two eigenvectors, $V_{(2)} = [v_1, v_2]$:\n$$T_{(2)} = Z V_{(2)}$$\n\nThe component loadings, which measure the correlation between the original descriptors and the principal components, are calculated for the first two components using the formula:\n$$L_{(2)} = V_{(2)} \\mathrm{diag}(\\sqrt{\\lambda_1}, \\sqrt{\\lambda_2})$$\nThe magnitude of a loading value indicates the importance of the corresponding original descriptor in defining that principal component. Therefore, for each of the first two components, we identify the descriptor with the maximum absolute loading to interpret which original variable contributes most to it.\n\nThe overall algorithm to be implemented for each test case is as follows:\n1.  Given the standardized descriptor matrix $Z$ with $n$ samples and $p$ descriptors.\n2.  Compute the $p \\times p$ sample covariance matrix $C = \\frac{1}{n-1}Z^\\top Z$.\n3.  Compute the eigenvalues and eigenvectors of $C$. A numerical routine for symmetric matrices, such as `numpy.linalg.eigh`, is appropriate, as it guarantees real eigenvalues and orthonormal eigenvectors.\n4.  Sort the eigenvalues in descending order and arrange the corresponding eigenvectors accordingly. Let the sorted eigenvalues be $[\\lambda_1, \\lambda_2, \\dots, \\lambda_p]$ and the corresponding matrix of eigenvectors be $V=[v_1, v_2, \\dots, v_p]$.\n5.  Extract the first two eigenvalues $[\\lambda_1, \\lambda_2]$ and the first two eigenvectors into a matrix $V_{(2)} = [v_1, v_2]$.\n6.  Calculate the explained variance ratios $[r_1, r_2]$ using the total variance $\\sum_{k=1}^{p} \\lambda_k$.\n7.  Calculate the component loadings $L_{(2)} = V_{(2)} \\mathrm{diag}(\\sqrt{\\lambda_1}, \\sqrt{\\lambda_2})$.\n8.  For each of the two components, find the $0$-based index of the descriptor with the maximum absolute loading: $[\\arg\\max_i |L_{(2)}[i,0]|, \\arg\\max_i |L_{(2)}[i,1]|]$.\n9.  Calculate the scores of the first sample (the first row of $Z$) on the first two components by computing the first row of the matrix product $T_{(2)} = Z V_{(2)}$.\n10. Aggregate these results—$[\\lambda_1, \\lambda_2]$, $[r_1, r_2]$, the two indices, and the two scores for the first sample—into a nested list. All floating-point values are rounded to $6$ decimal places.\n\nThis procedure will be applied to all provided test cases. The special cases of perfect collinearity (Case $2$) and a zero-variance descriptor (Case $3$) are handled correctly by the eigendecomposition, which will yield zero eigenvalues, reflecting the reduced rank of the covariance matrix. The final output is a single-line string representing a list of the results from all test cases, with no extraneous characters or spacing.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        np.array([\n            [-1.2, -1.0, 0.8, 0.5],\n            [0.4, 0.3, -0.6, -0.2],\n            [0.8, 0.9, -0.7, 0.1],\n            [-0.5, -0.7, 0.9, -0.4],\n            [0.3, 0.4, -0.2, 0.0],\n            [0.2, 0.1, -0.2, 0.0]\n        ]),\n        np.array([\n            [-1.5, -3.0, 0.6],\n            [-0.5, -1.0, -0.2],\n            [0.5, 1.0, -0.1],\n            [1.0, 2.0, -0.2],\n            [0.5, 1.0, -0.1]\n        ]),\n        np.array([\n            [0.5, -0.3, 0.0, 0.7],\n            [-0.5, 0.3, 0.0, -0.7],\n            [0.4, -0.2, 0.0, 0.6],\n            [-0.4, 0.2, 0.0, -0.6]\n        ])\n    ]\n\n    results = []\n    for Z in test_cases:\n        case_result = perform_pca(Z)\n        results.append(case_result)\n    \n    # Format the final output string to remove all spaces as per the requirement.\n    # str(list) creates a string representation like '[item1, item2, ...]'\n    # replace(' ', '') removes the spaces after commas and inside lists.\n    final_output_string = str(results).replace(' ', '')\n    print(final_output_string)\n\ndef perform_pca(Z):\n    \"\"\"\n    Performs PCA on a given standardized data matrix Z.\n\n    Args:\n        Z (np.ndarray): The n x p standardized descriptor matrix.\n\n    Returns:\n        list: A nested list containing PCA results for the first two components.\n              [[lambda1, lambda2], [r1, r2], [idx1, idx2], [t11, t12]]\n    \"\"\"\n    n, p = Z.shape\n    \n    # 1. Compute the sample covariance matrix C\n    # The definition is C = 1/(n-1) * Z^T * Z\n    C = (Z.T @ Z) / (n - 1)\n\n    # 2. Perform eigendecomposition of C\n    # np.linalg.eigh is for Hermitian (symmetric) matrices and returns eigenvalues\n    # in ascending order.\n    eigenvalues, eigenvectors = np.linalg.eigh(C)\n\n    # 3. Sort eigenvalues and eigenvectors in descending order\n    sort_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sort_indices]\n    sorted_eigenvectors = eigenvectors[:, sort_indices]\n\n    # 4. Extract first two eigenvalues and eigenvectors\n    lambda12 = sorted_eigenvalues[0:2]\n    V2 = sorted_eigenvectors[:, 0:2]\n\n    # 5. Compute explained variance ratios\n    total_variance = np.sum(sorted_eigenvalues)\n    # Handle division by zero for cases where total variance is zero\n    if total_variance > 0:\n        ratios = lambda12 / total_variance\n    else:\n        ratios = np.zeros(2)\n\n    # 6. Compute scores for the first sample\n    # T2 = Z @ V2\n    # first_sample_scores = T2[0, :]\n    first_sample_scores = Z[0, :] @ V2\n\n    # 7. Compute loadings and find indices of max absolute loading\n    # L2 = V2 @ diag(sqrt(lambda12))\n    # We must handle potentially negative eigenvalues from numerical errors, although\n    # covariance matrices are positive semidefinite. Taking abs() before sqrt is safe.\n    sqrt_lambda12 = np.sqrt(np.abs(lambda12))\n    L2 = V2 @ np.diag(sqrt_lambda12)\n    \n    max_loadings_indices = [int(np.argmax(np.abs(L2[:, 0]))), int(np.argmax(np.abs(L2[:, 1])))]\n\n    # 8. Assemble results and round floats to 6 decimal places\n    result = [\n        [round(val, 6) for val in lambda12],\n        [round(val, 6) for val in ratios],\n        max_loadings_indices,\n        [round(val, 6) for val in first_sample_scores]\n    ]\n\n    return result\n\nsolve()\n```"
        }
    ]
}