{
    "hands_on_practices": [
        {
            "introduction": "分子描述符将化学结构转化为可用于定量构效关系（QSAR）模型的数值。拓扑极性表面积（Topological Polar Surface Area, TPSA）是一个经典的二维描述符，它易于计算，并且对膜渗透性等药代动力学特性具有很高的预测能力。该练习  将指导你使用基于片段的方法计算 TPSA 并解释其数值，这是药物早期设计阶段的一项关键技能。",
            "id": "3854295",
            "problem": "在定量构效关系（QSAR）建模中，一个核心目标是通过可解释的描述符将分子结构与生物性质联系起来。一个广泛使用的描述符是拓扑极性表面积（TPSA），它通过对杂原子环境使用片段加和方案来近似计算极性表面积。在Ertl及其同事最初提出的基于片段的方法中，TPSA是分子图上与特定氮和氧原子环境相关的预定义片段贡献的加和。假设一个分子，在该片段方案中，其唯一的极性贡献者是一个酰胺氮（片段贡献为$12.03$）、一个叔胺氮（片段贡献为$3.24$）和一个吡啶环氮（片段贡献为$12.89$）。利用基于片段的TPSA是分子图上贡献杂原子片段的加和这一原理，计算总TPSA。然后，考虑到广泛使用的渗透性启发式规则，即较低的TPSA与较高的被动膜渗透性和中枢神经系统暴露可能性相关（血脑屏障穿透的粗略启发式阈值约为$90$，口服生物利用度的阈值约为$140$），简要解释计算出的TPSA是否表明该分子可能具有高渗透性。以平方埃（$\\mathrm{\\AA^{2}}$）表示最终的TPSA，并将您的数值答案四舍五入到$4$位有效数字。",
            "solution": "该问题是有效的。它科学地基于定量构效关系（QSAR）建模的原理，特别是拓扑极性表面积（TPSA）作为分子描述符的概念。所提供的数据是一致且充分的，足以得出一个唯一的解。\n\n问题指出，总拓扑极性表面积（TPSA）是预定义极性片段贡献的加和。所讨论的分子有三个这样的贡献片段。设酰胺氮的贡献表示为$c_{amide}$，叔胺氮的贡献表示为$c_{tert-amine}$，吡啶环氮的贡献表示为$c_{pyridine}$。提供的值如下：\n$$\nc_{amide} = 12.03~\\mathrm{\\AA^{2}}\n$$\n$$\nc_{tert-amine} = 3.24~\\mathrm{\\AA^{2}}\n$$\n$$\nc_{pyridine} = 12.89~\\mathrm{\\AA^{2}}\n$$\n根据加和性原理，总TPSA（表示为$TPSA_{total}$）是这些单个贡献的总和：\n$$\nTPSA_{total} = c_{amide} + c_{tert-amine} + c_{pyridine}\n$$\n将给定的数值代入方程：\n$$\nTPSA_{total} = 12.03 + 3.24 + 12.89\n$$\n进行求和：\n$$\nTPSA_{total} = 28.16~\\mathrm{\\AA^{2}}\n$$\n问题要求答案四舍五入到$4$位有效数字。计算值$28.16$已经包含$4$位有效数字，因此无需进一步四舍五入。\n\n任务的第二部分是在膜渗透性和中枢神经系统（CNS）暴露的背景下解释这一结果。提供的启发式规则给出了这些性质的粗略阈值。\n血脑屏障（BBB）穿透的启发式规则表明，TPSA低于约$90~\\mathrm{\\AA^{2}}$的分子更可能具有中枢神经系统渗透性。我们计算出的值为$TPSA_{total} = 28.16~\\mathrm{\\AA^{2}}$。由于$28.16  90$，预测该分子有很高的可能性穿过血脑屏障。\n\n口服生物利用度的启发式规则表明，TPSA低于约$140~\\mathrm{\\AA^{2}}$的分子更可能被良好吸收。我们计算出的值$28.16~\\mathrm{\\AA^{2}}$也远低于此阈值，因为$28.16  140$。这表明该分子可能具有良好的口服生物利用度。\n\n总之，计算出的TPSA值为$28.16~\\mathrm{\\AA^{2}}$，这是一个低值，远低于血脑屏障穿透和口服生物利用度的常见启发式阈值。因此，基于这单一的描述符，预测该分子具有高渗透性。",
            "answer": "$$\\boxed{28.16}$$"
        },
        {
            "introduction": "在使用 TPSA 等性质描述一个分子后，我们常常需要将其与其他分子进行比较。分子指纹将结构特征编码为二进制向量，而相似性度量则量化了它们之间的重叠程度。本练习  聚焦于 Tanimoto 和 Dice 系数这两个基本度量，帮助你理解它们的数学定义，以及它们在虚拟筛选中如何因细微差异而影响相似性评估。",
            "id": "3854355",
            "problem": "考虑用于定量构效关系 (QSAR) 模型的两个小分子，每个分子都被编码为一个 $1024$ 位的扩展连通性指纹 (ECFP)。设分子 A 中比特位置为 $1$ 的集合记为 $A \\subset \\{1,\\dots,1024\\}$，分子 B 中比特位置为 $1$ 的集合记为 $B \\subset \\{1,\\dots,1024\\}$。根据经验，观察到以下基数：$|A|=a=100$，$|B|=b=120$，以及 $|A \\cap B|=c=80$。在化学信息学中，二进制指纹之间的基于集合的相似性度量是根据集合论的第一性原理定义的。Tanimoto 相似度（也称为 Jaccard 指数）定义为交集基数与并集基数之比，而 Dice 相似度（也称为 Sørensen–Dice 系数）定义为交集基数的两倍除以各个集合基数之和。\n\n从集合论恒等式 $|A \\cup B|=|A|+|B|-|A \\cap B|$ 出发，推导出用 $a$、$b$ 和 $c$ 表示的 Tanimoto 相似度和 Dice 相似度的表达式，用给定值计算它们的值，然后计算差值 $D - T$。请将 $D - T$ 的最终答案以单个简化的解析表达式的精确形式表示。此外，请在推导中解释这两种度量为何不同，以及在 QSAR 的二进制指纹背景下，这种差异是如何从它们的定义中产生的。",
            "solution": "我们从用于定量构效关系 (QSAR) 建模中二进制分子指纹的相似性度量的基础集合论定义开始。每个分子的指纹是一个二进制向量，值为 $1$ 的比特位置对应于分子中存在的特征。集合 $A$ 和 $B$ 分别代表分子 A 和 B 中置位比特的索引。交集 $A \\cap B$ 代表共享的特征，并集 $A \\cup B$ 代表至少存在于一个分子中的特征。\n\nTanimoto 相似度，即 Jaccard 指数，根据第一性原理定义为共享特征数占总独特特征数的分数。用集合论的术语来说，即\n$$\nT \\equiv \\frac{|A \\cap B|}{|A \\cup B|}.\n$$\n使用恒等式\n$$\n|A \\cup B| = |A| + |B| - |A \\cap B|,\n$$\n我们可以用基数 $a$、$b$ 和 $c$ 来表示 $T$ 为\n$$\nT = \\frac{c}{a + b - c}.\n$$\n\nDice 相似度，也称为 Sørensen–Dice 系数，通过将交集相对于两个集合的总大小进行缩放来强调交集。其集合论定义为\n$$\nD \\equiv \\frac{2|A \\cap B|}{|A| + |B|},\n$$\n用 $a$、$b$ 和 $c$ 表示则为\n$$\nD = \\frac{2c}{a + b}.\n$$\n\n现在我们代入给定值 $a=100$，$b=120$ 和 $c=80$。\n\n对于 Tanimoto 相似度，\n$$\nT = \\frac{c}{a + b - c} = \\frac{80}{100 + 120 - 80} = \\frac{80}{140} = \\frac{4}{7}.\n$$\n\n对于 Dice 相似度，\n$$\nD = \\frac{2c}{a + b} = \\frac{160}{220} = \\frac{16}{22} = \\frac{8}{11}.\n$$\n\n所要求的最终量是差值 $D - T$。以精确形式计算这个差值：\n$$\nD - T = \\frac{8}{11} - \\frac{4}{7}.\n$$\n使用公分母进行简化：\n$$\n\\frac{8}{11} - \\frac{4}{7} = \\frac{8 \\cdot 7}{77} - \\frac{4 \\cdot 11}{77} = \\frac{56 - 44}{77} = \\frac{12}{77}.\n$$\n因此，该差值的精确简化表达式为\n$$\nD - T = \\frac{12}{77}.\n$$\n\n从定义推导出的差异解释：Tanimoto 相似度 $T$ 通过并集 $|A \\cup B|$ 来归一化交集 $|A \\cap B|$，这通过将不匹配的特征以 $a + b - c$ 的形式包含在分母中来对其进行惩罚。Dice 相似度 $D$ 通过和 $a + b$ 来归一化交集，并将交集加倍，从而有效地相对于总特征赋予共享特征更大的权重。因为当 $c>0$ 时，$|A \\cup B| = a + b - c$ 严格小于 $a + b$，Dice 的分母较大，而其分子为 $2c$，这使得对于非平凡的重叠，$D$ 通常大于 $T$。在这种情况下，差值 $D - T = \\frac{12}{77}$ 是由于 Dice 通过因子 $2$ 对交集给予了额外的强调，并且分母中没有减去 $c$ 所致。在采用二进制指纹的 QSAR 筛选中，这意味着 Dice 对共享比特更敏感，并且可能将具有显著重叠的分子对的排名排在 Tanimoto 之前，这可能会影响命中富集和阈值选择，具体取决于在奖励重叠和惩罚不匹配之间所期望的平衡。",
            "answer": "$$\\boxed{\\frac{12}{77}}$$"
        },
        {
            "introduction": "QSAR 分析很少只依赖单一描述符；相反，我们通常使用多个描述符来捕捉分子的复杂性，从而创建一个高维描述符空间。主成分分析（Principal Component Analysis, PCA）是一种强大的技术，可用于降低维度并揭示数据集中的最重要趋势。这项动手实践  将指导你从第一性原理出发执行 PCA，使你能够可视化复杂数据，并识别出哪些描述符驱动了一组化合物中的主要变异。",
            "id": "3854316",
            "problem": "给定您一些小型的、标准化的分子描述符矩阵，这些矩阵代表了用于定量构效关系 (QSAR) 研究的化合物集合。您需要完全从第一性原理出发执行主成分分析 (PCA)，通过计算标准化描述符的样本协方差的特征分解，将样本投影到前两个主成分上，并解释成分载荷。\n\n使用的基本定义和事实：\n- 标准化的描述符排列在一个矩阵 $Z \\in \\mathbb{R}^{n \\times p}$ 中，其各列（描述符）在样本间的均值为零。样本协方差矩阵定义为 $$C = \\frac{1}{n-1} Z^\\top Z.$$\n- 主成分分析 (PCA) 旨在寻找一组标准正交方向（主轴）$v_1, v_2, \\dots, v_p \\in \\mathbb{R}^p$，这些方向在单位长度约束下最大化投影数据的方差。对于第一个主成分方向 $v_1$，这等同于在 $\\|v\\|_2 = 1$ 的约束下最大化瑞利商 $v^\\top C v$，这得出 $v_1$ 是 $C$ 对应于最大特征值 $\\lambda_1$ 的特征向量。后续的主成分 $v_2, \\dots$ 在正交性约束下类似地得出。\n- 给定特征分解 $$C = V \\Lambda V^\\top,$$ 其中 $V$ 是标准正交矩阵，$\\Lambda = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_p)$ 经过排序以使 $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_p \\ge 0$，样本在前两个主成分上的投影（得分）为 $$T_{(2)} = Z \\, V_{(2)},$$ 其中 $V_{(2)}$ 的列是前两个特征向量。前两个主成分的成分载荷定义为 $$L_{(2)} = V_{(2)} \\, \\mathrm{diag}\\!\\big(\\sqrt{\\lambda_1}, \\sqrt{\\lambda_2}\\big).$$\n- 主成分 $j$ 的方差解释率为 $$r_j = \\frac{\\lambda_j}{\\sum_{k=1}^{p} \\lambda_k}.$$\n\n您的程序必须：\n1. 对于每个提供的测试用例，计算样本协方差 $C$、其特征值和特征向量，将特征值按降序排序并相应地排列其特征向量，获取前两个特征值 $[\\lambda_1, \\lambda_2]$、前两个特征向量 $V_{(2)}$、得分 $T_{(2)}$ 和载荷 $L_{(2)}$。\n2. 解释载荷，即对于前两个主成分中的每一个，识别出具有最大绝对载荷的描述符的索引（使用从 $0$ 开始的索引），即，对于成分 $j \\in \\{1,2\\}$，返回 $$\\arg\\max_{i \\in \\{0,\\dots,p-1\\}} |L_{(2)}[i,j-1]|.$$\n3. 计算方差解释率 $[r_1, r_2]$。\n4. 为每个测试用例返回一个嵌套列表，其中包含：\n   - 前两个特征值 $[\\lambda_1, \\lambda_2]$，\n   - 方差解释率 $[r_1, r_2]$，\n   - 前两个主成分中具有最大绝对载荷的描述符的索引 $[\\mathrm{idx}_1, \\mathrm{idx}_2]$，\n   - 第一个样本的二维投影（得分）$[t_{1,1}, t_{1,2}]$，其中 $t_{1,j}$ 是第一个样本在成分 $j$ 上的得分。\n\n所有浮点输出必须四舍五入到 $6$ 位小数。所有描述符索引必须是通过从 $0$ 开始的索引得到的整数。最终输出必须是单行，将所有测试用例的结果聚合到一个列表中，不含任何空格，例如 $$\\big[\\text{case}_1,\\text{case}_2,\\text{case}_3\\big],$$ 其中每个 $\\text{case}_k$ 是上述的嵌套列表。\n\n测试套件（每个 $Z^{(k)}$ 都已按描述符标准化为零均值）：\n- 情况 1： $$Z^{(1)} = \\begin{bmatrix}\n-1.2  -1.0  \\phantom{-}0.8  \\phantom{-}0.5 \\\\\n\\phantom{-}0.4  \\phantom{-}0.3  -0.6  -0.2 \\\\\n\\phantom{-}0.8  \\phantom{-}0.9  -0.7  \\phantom{-}0.1 \\\\\n-0.5  -0.7  \\phantom{-}0.9  -0.4 \\\\\n\\phantom{-}0.3  \\phantom{-}0.4  -0.2  \\phantom{-}0.0 \\\\\n\\phantom{-}0.2  \\phantom{-}0.1  -0.2  \\phantom{-}0.0 \\\\\n\\end{bmatrix}.$$\n- 情况 2（边界条件，前两个描述符之间存在完全共线性）： $$Z^{(2)} = \\begin{bmatrix}\n-1.5  -3.0  \\phantom{-}0.6 \\\\\n-0.5  -1.0  -0.2 \\\\\n\\phantom{-}0.5  \\phantom{-}1.0  -0.1 \\\\\n\\phantom{-}1.0  \\phantom{-}2.0  -0.2 \\\\\n\\phantom{-}0.5  \\phantom{-}1.0  -0.1 \\\\\n\\end{bmatrix}.$$\n- 情况 3（边缘情况，存在一个零方差描述符）： $$Z^{(3)} = \\begin{bmatrix}\n\\phantom{-}0.5  -0.3  \\phantom{-}0.0  \\phantom{-}0.7 \\\\\n-0.5  \\phantom{-}0.3  \\phantom{-}0.0  -0.7 \\\\\n\\phantom{-}0.4  -0.2  \\phantom{-}0.0  \\phantom{-}0.6 \\\\\n-0.4  \\phantom{-}0.2  \\phantom{-}0.0  -0.6 \\\\\n\\end{bmatrix}.$$\n\n您的程序应生成单行输出，其中包含三个测试用例的结果列表，每个用例表示为一个嵌套列表 $[\\,[\\lambda_1,\\lambda_2],[r_1,r_2],[\\mathrm{idx}_1,\\mathrm{idx}_2],[t_{1,1},t_{1,2}]\\,]$，所有浮点数四舍五入到 $6$ 位小数，并且行内任何地方都没有空格。不涉及角度，因此不需要角度单位，方差解释率必须以小数形式表示。",
            "solution": "问题陈述已经过分析并确定是有效的。它在科学上基于线性代数和多元统计的原理，特别是主成分分析 (PCA)，并将这些概念应用于计算化学生物学 (QSAR) 中的一个相关问题。该问题定义良好，提供了所有必要的数据和定义以计算出唯一且可验证的解。测试用例包括标准、共线性和零方差情景，这些都适合评估一个稳健的实现。\n\n任务是从第一性原理出发，对给定的标准化分子描述符矩阵 ($Z$) 执行 PCA。这包括计算样本协方差矩阵 ($C$)，对其进行特征分解以找到主成分，然后计算得分、载荷和方差解释率等派生量。\n\nPCA 的核心在于找到一组新的正交轴，称为主成分，这些轴与数据中方差最大的方向对齐。对于一个包含 $n$ 个样本和 $p$ 个描述符的数据矩阵 $Z \\in \\mathbb{R}^{n \\times p}$，其中每个描述符（列）都中心化为零均值，样本协方差矩阵由以下公式给出：\n$$C = \\frac{1}{n-1} Z^\\top Z$$\n主成分是此协方差矩阵 $C$ 的特征向量。设 $C$ 的特征分解为 $C = V \\Lambda V^\\top$，其中 $V$ 是一个标准正交矩阵，其列 $v_j$ 是特征向量，$\\Lambda$ 是相应特征值 $\\lambda_j$ 组成的对角矩阵。特征值按降序排序，$\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p \\geq 0$，使得第一个特征向量 $v_1$ 对应于数据中方差最大的方向。\n\n每个主成分 $j$ 捕获的方差量由其特征值 $\\lambda_j$ 给出。方差解释率 $r_j$ 通过将此值除以数据中的总方差（即所有特征值的总和，等于 $C$ 的迹）来进行归一化：\n$$r_j = \\frac{\\lambda_j}{\\sum_{k=1}^{p} \\lambda_k}$$\n\n原始数据可以投影到由主成分定义的新坐标系上。得到的坐标称为得分。前两个主成分的得分通过将数据矩阵 $Z$ 投影到前两个特征向量 $V_{(2)} = [v_1, v_2]$ 上来计算：\n$$T_{(2)} = Z V_{(2)}$$\n\n成分载荷衡量了原始描述符与主成分之间的相关性，前两个主成分的成分载荷使用以下公式计算：\n$$L_{(2)} = V_{(2)} \\mathrm{diag}(\\sqrt{\\lambda_1}, \\sqrt{\\lambda_2})$$\n载荷值的大小表示相应原始描述符在定义该主成分时的重要性。因此，对于前两个主成分中的每一个，我们通过识别具有最大绝对载荷的描述符来解释哪个原始变量对其贡献最大。\n\n为每个测试用例实现的总体算法如下：\n1.  给定具有 $n$ 个样本和 $p$ 个描述符的标准化描述符矩阵 $Z$。\n2.  计算 $p \\times p$ 的样本协方差矩阵 $C = \\frac{1}{n-1}Z^\\top Z$。\n3.  计算 $C$ 的特征值和特征向量。适用于对称矩阵的数值例程，如 `numpy.linalg.eigh`，是合适的，因为它能保证得到实数特征值和标准正交的特征向量。\n4.  将特征值按降序排序，并相应地排列对应的特征向量。设排序后的特征值为 $[\\lambda_1, \\lambda_2, \\dots, \\lambda_p]$，对应的特征向量矩阵为 $V=[v_1, v_2, \\dots, v_p]$。\n5.  提取前两个特征值 $[\\lambda_1, \\lambda_2]$ 和前两个特征向量构成矩阵 $V_{(2)} = [v_1, v_2]$。\n6.  使用总方差 $\\sum_{k=1}^{p} \\lambda_k$ 计算方差解释率 $[r_1, r_2]$。\n7.  计算成分载荷 $L_{(2)} = V_{(2)} \\mathrm{diag}(\\sqrt{\\lambda_1}, \\sqrt{\\lambda_2})$。\n8.  对于这两个成分中的每一个，找到具有最大绝对载荷的描述符的从 $0$ 开始的索引：$[\\arg\\max_i |L_{(2)}[i,0]|, \\arg\\max_i |L_{(2)}[i,1]|]$。\n9.  通过计算矩阵乘积 $T_{(2)} = Z V_{(2)}$ 的第一行，来计算第一个样本（$Z$ 的第一行）在前两个主成分上的得分。\n10. 将这些结果——$[\\lambda_1, \\lambda_2]$、$[r_1, r_2]$、两个索引以及第一个样本的两个得分——聚合到一个嵌套列表中。所有浮点值都四舍五入到 $6$ 位小数。\n\n此过程将应用于所有提供的测试用例。完全共线性（情况 2）和零方差描述符（情况 3）的特殊情况会由特征分解正确处理，这将产生零特征值，反映协方差矩阵的秩降低。最终输出是一个单行字符串，表示所有测试用例结果的列表，不含多余字符或空格。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        np.array([\n            [-1.2, -1.0, 0.8, 0.5],\n            [0.4, 0.3, -0.6, -0.2],\n            [0.8, 0.9, -0.7, 0.1],\n            [-0.5, -0.7, 0.9, -0.4],\n            [0.3, 0.4, -0.2, 0.0],\n            [0.2, 0.1, -0.2, 0.0]\n        ]),\n        np.array([\n            [-1.5, -3.0, 0.6],\n            [-0.5, -1.0, -0.2],\n            [0.5, 1.0, -0.1],\n            [1.0, 2.0, -0.2],\n            [0.5, 1.0, -0.1]\n        ]),\n        np.array([\n            [0.5, -0.3, 0.0, 0.7],\n            [-0.5, 0.3, 0.0, -0.7],\n            [0.4, -0.2, 0.0, 0.6],\n            [-0.4, 0.2, 0.0, -0.6]\n        ])\n    ]\n\n    results = []\n    for Z in test_cases:\n        case_result = perform_pca(Z)\n        results.append(case_result)\n    \n    # Format the final output string to remove all spaces as per the requirement.\n    # str(list) creates a string representation like '[item1, item2, ...]'\n    # replace(' ', '') removes the spaces after commas and inside lists.\n    final_output_string = str(results).replace(' ', '')\n    print(final_output_string)\n\ndef perform_pca(Z):\n    \"\"\"\n    Performs PCA on a given standardized data matrix Z.\n\n    Args:\n        Z (np.ndarray): The n x p standardized descriptor matrix.\n\n    Returns:\n        list: A nested list containing PCA results for the first two components.\n              [[lambda1, lambda2], [r1, r2], [idx1, idx2], [t11, t12]]\n    \"\"\"\n    n, p = Z.shape\n    \n    # 1. Compute the sample covariance matrix C\n    # The definition is C = 1/(n-1) * Z^T * Z\n    C = (Z.T @ Z) / (n - 1)\n\n    # 2. Perform eigendecomposition of C\n    # np.linalg.eigh is for Hermitian (symmetric) matrices and returns eigenvalues\n    # in ascending order.\n    eigenvalues, eigenvectors = np.linalg.eigh(C)\n\n    # 3. Sort eigenvalues and eigenvectors in descending order\n    sort_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sort_indices]\n    sorted_eigenvectors = eigenvectors[:, sort_indices]\n\n    # 4. Extract first two eigenvalues and eigenvectors\n    lambda12 = sorted_eigenvalues[0:2]\n    V2 = sorted_eigenvectors[:, 0:2]\n\n    # 5. Compute explained variance ratios\n    total_variance = np.sum(sorted_eigenvalues)\n    # Handle division by zero for cases where total variance is zero\n    if total_variance > 0:\n        ratios = lambda12 / total_variance\n    else:\n        ratios = np.zeros(2)\n\n    # 6. Compute scores for the first sample\n    # T2 = Z @ V2\n    # first_sample_scores = T2[0, :]\n    first_sample_scores = Z[0, :] @ V2\n\n    # 7. Compute loadings and find indices of max absolute loading\n    # L2 = V2 @ diag(sqrt(lambda12))\n    # We must handle potentially negative eigenvalues from numerical errors, although\n    # covariance matrices are positive semidefinite. Taking abs() before sqrt is safe.\n    sqrt_lambda12 = np.sqrt(np.abs(lambda12))\n    L2 = V2 @ np.diag(sqrt_lambda12)\n    \n    max_loadings_indices = [int(np.argmax(np.abs(L2[:, 0]))), int(np.argmax(np.abs(L2[:, 1])))]\n\n    # 8. Assemble results and round floats to 6 decimal places\n    result = [\n        [round(val, 6) for val in lambda12],\n        [round(val, 6) for val in ratios],\n        max_loadings_indices,\n        [round(val, 6) for val in first_sample_scores]\n    ]\n\n    return result\n\nsolve()\n```"
        }
    ]
}