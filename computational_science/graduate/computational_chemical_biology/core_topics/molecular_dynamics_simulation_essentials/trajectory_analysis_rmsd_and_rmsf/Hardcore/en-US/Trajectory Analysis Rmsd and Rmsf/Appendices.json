{
    "hands_on_practices": [
        {
            "introduction": "Before we can interpret atomic fluctuations, we must understand their physical origin. The Root-Mean-Square Fluctuation (RMSF) is not merely a descriptive statistic; it is deeply rooted in the principles of statistical mechanics, connecting macroscopic temperature to microscopic motion. This foundational exercise () will guide you through a first-principles derivation of the RMSF for a particle in a simple harmonic potential, revealing the direct relationship between fluctuation, temperature, and the local stiffness of the potential.",
            "id": "3868675",
            "problem": "In computational chemical biology, root-mean-square fluctuation (RMSF) is used to quantify the amplitude of per-atom positional fluctuations along a trajectory, while root-mean-square deviation (RMSD) aggregates deviations for many atoms against a reference structure. Consider a single Cartesian coordinate $x$ of a protein atom that fluctuates near an equilibrium position $x_{0}$ under a one-dimensional harmonic potential $U(x) = \\frac{1}{2} k \\left(x - x_{0}\\right)^{2}$, where $k$ is a force constant. The system is at absolute temperature $T$ and exchanges energy with a thermal reservoir, so it is described by the canonical ensemble. A long, ergodic molecular dynamics trajectory provides samples $x(t)$ such that time averages equal ensemble averages.\n\nStarting from the canonical distribution and the definition of RMSF as the square root of the mean-squared fluctuation about the mean position, derive a closed-form analytic expression for the RMSF in terms of the Boltzmann constant $k_{B}$, the temperature $T$, and the force constant $k$. Your final answer must be a single simplified analytic expression. If you perform any intermediate integrations, justify each step from first principles of equilibrium statistical mechanics. Express your final answer in the same length units as $x$ (no numerical evaluation is required).",
            "solution": "The problem requires the derivation of a closed-form expression for the root-mean-square fluctuation (RMSF) of a single Cartesian coordinate, $x$, of an atom. The atom is subject to a one-dimensional harmonic potential, $U(x)$, and is in thermal equilibrium with a reservoir at temperature $T$. This corresponds to a canonical ensemble in statistical mechanics.\n\nThe problem statement provides the following:\n- Potential energy: $U(x) = \\frac{1}{2} k (x - x_0)^2$, where $k$ is the force constant and $x_0$ is the equilibrium position.\n- The system is described by the canonical ensemble at absolute temperature $T$.\n- The Boltzmann constant is $k_B$.\n- The assumption of ergodicity implies that time averages from a long trajectory are equivalent to ensemble averages.\n\nThe RMSF is defined as the square root of the mean-squared fluctuation of the coordinate about its mean position. Mathematically, this is expressed as:\n$$\n\\text{RMSF} = \\sqrt{\\langle (x - \\langle x \\rangle)^2 \\rangle}\n$$\nwhere $\\langle \\cdot \\rangle$ denotes the ensemble average.\n\nIn the canonical ensemble, the probability density function for the coordinate $x$ follows the Boltzmann distribution:\n$$\nP(x) = \\frac{1}{Z} \\exp(-\\beta U(x))\n$$\nwhere $\\beta = \\frac{1}{k_B T}$ and $Z$ is the partition function, which acts as a normalization constant. The partition function is given by the integral of the Boltzmann factor over all possible states (positions):\n$$\nZ = \\int_{-\\infty}^{\\infty} \\exp(-\\beta U(x)) \\, dx = \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{\\beta k}{2} (x - x_0)^2\\right) \\, dx\n$$\n\nThe derivation proceeds in two main steps: first, we determine the mean position $\\langle x \\rangle$, and second, we calculate the mean-squared fluctuation about this mean.\n\n**Step 1: Calculation of the Mean Position $\\langle x \\rangle$**\n\nThe mean position $\\langle x \\rangle$ is the expectation value of $x$, calculated as:\n$$\n\\langle x \\rangle = \\int_{-\\infty}^{\\infty} x P(x) \\, dx = \\frac{\\int_{-\\infty}^{\\infty} x \\exp\\left(-\\frac{\\beta k}{2} (x - x_0)^2\\right) \\, dx}{\\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{\\beta k}{2} (x - x_0)^2\\right) \\, dx}\n$$\nTo evaluate these integrals, we perform a change of variables. Let $y = x - x_0$, which implies $x = y + x_0$ and $dx = dy$. The limits of integration from $-\\infty$ to $\\infty$ for $x$ remain the same for $y$.\n$$\n\\langle x \\rangle = \\frac{\\int_{-\\infty}^{\\infty} (y + x_0) \\exp\\left(-\\frac{\\beta k}{2} y^2\\right) \\, dy}{\\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{\\beta k}{2} y^2\\right) \\, dy}\n$$\nWe can split the numerator into two parts:\n$$\n\\langle x \\rangle = \\frac{\\int_{-\\infty}^{\\infty} y \\exp\\left(-\\frac{\\beta k}{2} y^2\\right) \\, dy + \\int_{-\\infty}^{\\infty} x_0 \\exp\\left(-\\frac{\\beta k}{2} y^2\\right) \\, dy}{\\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{\\beta k}{2} y^2\\right) \\, dy}\n$$\nThe first integral in the numerator, $\\int_{-\\infty}^{\\infty} y \\exp(-\\frac{\\beta k}{2} y^2) \\, dy$, has an integrand that is an odd function (a product of an odd function, $y$, and an even function, $\\exp(-\\text{const} \\cdot y^2)$). The integral of an odd function over a symmetric interval $(-\\infty, \\infty)$ is zero.\nThe second integral in the numerator is $x_0 \\int_{-\\infty}^{\\infty} \\exp(-\\frac{\\beta k}{2} y^2) \\, dy$.\nTherefore, the expression for $\\langle x \\rangle$ simplifies to:\n$$\n\\langle x \\rangle = \\frac{0 + x_0 \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{\\beta k}{2} y^2\\right) \\, dy}{\\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{\\beta k}{2} y^2\\right) \\, dy} = x_0\n$$\nThis confirms the physically intuitive result that the average position is the equilibrium position $x_0$ for a symmetric harmonic potential.\n\n**Step 2: Calculation of the Mean-Squared Fluctuation $\\langle (x - \\langle x \\rangle)^2 \\rangle$**\n\nWith $\\langle x \\rangle = x_0$, the mean-squared fluctuation is $\\langle (x - x_0)^2 \\rangle$. This is the variance of the position, $\\sigma_x^2$.\n$$\n\\langle (x - x_0)^2 \\rangle = \\frac{\\int_{-\\infty}^{\\infty} (x - x_0)^2 \\exp\\left(-\\frac{\\beta k}{2} (x - x_0)^2\\right) \\, dx}{\\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{\\beta k}{2} (x - x_0)^2\\right) \\, dx}\n$$\nUsing the same substitution $y = x - x_0$:\n$$\n\\langle (x - x_0)^2 \\rangle = \\frac{\\int_{-\\infty}^{\\infty} y^2 \\exp\\left(-\\frac{\\beta k}{2} y^2\\right) \\, dy}{\\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{\\beta k}{2} y^2\\right) \\, dy}\n$$\nLet's define the constant $\\alpha = \\frac{\\beta k}{2}$. The expression becomes:\n$$\n\\langle (x - x_0)^2 \\rangle = \\frac{\\int_{-\\infty}^{\\infty} y^2 \\exp(-\\alpha y^2) \\, dy}{\\int_{-\\infty}^{\\infty} \\exp(-\\alpha y^2) \\, dy}\n$$\nThese are standard forms of Gaussian integrals. Let's denote them as $I_2 = \\int_{-\\infty}^{\\infty} y^2 \\exp(-\\alpha y^2) \\, dy$ and $I_0 = \\int_{-\\infty}^{\\infty} \\exp(-\\alpha y^2) \\, dy$.\nThe value of the standard Gaussian integral $I_0$ is:\n$$\nI_0 = \\sqrt{\\frac{\\pi}{\\alpha}}\n$$\nThe integral $I_2$ can be obtained from $I_0$ by differentiating with respect to the parameter $\\alpha$:\n$$\n\\frac{dI_0}{d\\alpha} = \\frac{d}{d\\alpha} \\int_{-\\infty}^{\\infty} \\exp(-\\alpha y^2) \\, dy = \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial\\alpha} \\exp(-\\alpha y^2) \\, dy = \\int_{-\\infty}^{\\infty} (-y^2) \\exp(-\\alpha y^2) \\, dy = -I_2\n$$\nSo, $I_2 = -\\frac{dI_0}{d\\alpha}$. Differentiating the expression for $I_0$:\n$$\nI_2 = - \\frac{d}{d\\alpha} \\left(\\sqrt{\\pi} \\alpha^{-1/2}\\right) = - \\sqrt{\\pi} \\left(-\\frac{1}{2} \\alpha^{-3/2}\\right) = \\frac{\\sqrt{\\pi}}{2} \\alpha^{-3/2} = \\frac{1}{2\\alpha} \\sqrt{\\frac{\\pi}{\\alpha}} = \\frac{I_0}{2\\alpha}\n$$\nNow we can compute the ratio $\\frac{I_2}{I_0}$:\n$$\n\\langle (x - x_0)^2 \\rangle = \\frac{I_2}{I_0} = \\frac{I_0 / (2\\alpha)}{I_0} = \\frac{1}{2\\alpha}\n$$\nSubstituting back the expression for $\\alpha = \\frac{\\beta k}{2}$:\n$$\n\\langle (x - x_0)^2 \\rangle = \\frac{1}{2 \\left(\\frac{\\beta k}{2}\\right)} = \\frac{1}{\\beta k}\n$$\nFinally, substituting $\\beta = \\frac{1}{k_B T}$:\n$$\n\\langle (x - x_0)^2 \\rangle = \\frac{k_B T}{k}\n$$\nThis result is consistent with the equipartition theorem, which states that each quadratic degree of freedom in the energy contributes an average of $\\frac{1}{2} k_B T$ to the total energy. For the potential energy $U(x)$, we have $\\langle U(x) \\rangle = \\langle \\frac{1}{2} k (x-x_0)^2 \\rangle = \\frac{1}{2} k \\langle (x-x_0)^2 \\rangle = \\frac{1}{2} k_B T$, which directly yields $\\langle (x-x_0)^2 \\rangle = \\frac{k_B T}{k}$.\n\n**Step 3: Final Calculation of RMSF**\n\nThe RMSF is the square root of the mean-squared fluctuation:\n$$\n\\text{RMSF} = \\sqrt{\\langle (x - x_0)^2 \\rangle} = \\sqrt{\\frac{k_B T}{k}}\n$$\nThis is the final closed-form analytic expression for the RMSF of a coordinate fluctuating under a harmonic potential at temperature $T$. The units are consistent, as $k_B T$ has units of energy (e.g., Joules) and $k$ has units of energy per length squared (e.g., Joules/meter$^2$), so the ratio has units of length squared, and its square root has units of length, as required.",
            "answer": "$$\n\\boxed{\\sqrt{\\frac{k_B T}{k}}}\n$$"
        },
        {
            "introduction": "Molecular dynamics simulations often employ Periodic Boundary Conditions (PBC) to approximate an infinite system, a technique essential for minimizing edge effects. However, this computational convenience can introduce significant artifacts into the raw trajectory data, such as molecules appearing to be broken or jumping across the simulation box. This hands-on exercise () addresses this critical challenge by requiring you to implement an 'unwrapping' algorithm, a crucial pre-processing step to restore a physically coherent molecular structure before any meaningful RMSD or RMSF analysis can be performed.",
            "id": "3868658",
            "problem": "You are given a scenario from computational chemical biology in which molecular dynamics trajectories are recorded under periodic boundary conditions. The simulated system is a three-dimensional orthorhombic periodic box, and recorded coordinates are \"wrapped\" into the primary image of the unit cell. You must explain using first principles why periodic boundary conditions necessitate unwrapping or imaging corrections before aligning coordinates and computing Root Mean Square Deviation (RMSD) and Root Mean Square Fluctuation (RMSF), and then implement a robust unwrapping algorithm. Finally, you must compute RMSD and RMSF for a small test suite of wrapped trajectories and demonstrate the effect of unwrapping.\n\nBegin from the following foundational base:\n- The periodic box is a lattice defined by side lengths $\\mathbf{L} = (L_x, L_y, L_z)$ in nanometers; coordinates obey equivalence modulo $\\mathbf{L}$, i.e., two positions $\\mathbf{r}$ and $\\mathbf{r} + n_x L_x \\hat{\\mathbf{x}} + n_y L_y \\hat{\\mathbf{y}} + n_z L_z \\hat{\\mathbf{z}}$ for integers $n_x, n_y, n_z$ represent the same physical point in the periodic sense.\n- The Euclidean distance in three dimensions is defined by $\\|\\mathbf{a} - \\mathbf{b}\\| = \\sqrt{(a_x - b_x)^2 + (a_y - b_y)^2 + (a_z - b_z)^2}$.\n- The best-fit rigid alignment (rotation and translation) between two sets of points minimizes the sum of squared Euclidean distances between corresponding points, under the constraint that the transformation is a rotation in special orthogonal group $\\mathrm{SO}(3)$ followed by a translation.\n- The Root Mean Square Deviation (RMSD) between two conformations with $N$ atoms is the square root of the mean of squared distances between corresponding atoms after optimal alignment.\n- The Root Mean Square Fluctuation (RMSF) of one atom over a trajectory is the square root of the mean of squared distances of that atom from its time-averaged position (after removing global rotations and translations by alignment).\n\nYour tasks:\n1. Explain, starting from the above fundamental base, why wrapped coordinates under periodic boundary conditions can cause erroneous distances and misalignment if unwrapping or imaging corrections are not applied prior to RMSD and RMSF computation.\n2. Propose and implement an algorithm that robustly unwraps coordinates frame-by-frame, preserving continuity and internal geometry. The algorithm must use connectivity (a bonded graph between atoms) to propagate imaging corrections across the molecule and use the minimum image convention to choose the nearest periodic image for continuity.\n3. Align each trajectory frame to the first frame using an optimal rigid rotation and translation derived from minimizing the sum of squared distances, and compute:\n   - The mean RMSD over frames $t = 1,\\dots,T-1$ relative to the reference frame $t = 0$.\n   - The mean RMSF over atoms, computed from aligned coordinates across all frames $t = 0,\\dots,T-1$.\n4. Perform the above computations once using raw wrapped coordinates (no unwrapping), and once using your unwrapped coordinates. Report both sets of results for each test case.\n\nPhysical units:\n- All coordinates and box lengths are in nanometers. RMSD and RMSF must be reported in nanometers, as floating-point numbers.\n\nAngle unit:\n- Any periodic modulations you use for synthetic vibrations must use radians in trigonometric functions.\n\nTest suite:\nImplement your program to use the following three test cases of synthetic trajectories. Each test case defines the number of atoms $N$, the number of frames $T$, the box lengths $\\mathbf{L}$, an initial configuration $\\mathbf{r}_i(0)$ for atom index $i$, a constant drift per frame $\\mathbf{d}$, and small per-frame vibrations $\\boldsymbol{\\epsilon}_{i}(t)$. The unwrapped coordinates are defined by\n$$\n\\mathbf{r}_i(t) = \\mathbf{r}_i(0) + t\\,\\mathbf{d} + \\boldsymbol{\\epsilon}_{i}(t),\n$$\nand the wrapped coordinates recorded are\n$$\n\\mathbf{w}_i(t) = \\mathbf{r}_i(t) \\bmod \\mathbf{L},\n$$\nwhere the modulo operation is applied component-wise to map positions into the interval $[0,L_\\alpha)$ for each axis $\\alpha \\in \\{x,y,z\\}$.\n\nPer-frame vibrations are defined by\n$$\n\\boldsymbol{\\epsilon}_{i}(t) = \\left(0,\\; 0.01 \\sin\\left(\\frac{2\\pi t}{T} + i\\right),\\; 0.005 \\cos\\left(\\frac{2\\pi t}{T} + \\frac{i}{2}\\right)\\right),\n$$\nwith all quantities in nanometers and radians.\n\nFor each test, provide the bonded connectivity as an undirected edge list that forms a chain.\n\n- Test Case A (happy path drift across $x$-boundary):\n  - $N = 3$, $T = 6$, $\\mathbf{L} = (3.0, 3.0, 3.0)$.\n  - Initial positions: $\\mathbf{r}_0(0) = (0.40, 0.40, 0.20)$, $\\mathbf{r}_1(0) = (0.52, 0.40, 0.20)$, $\\mathbf{r}_2(0) = (0.64, 0.40, 0.20)$.\n  - Drift per frame: $\\mathbf{d} = (0.80, 0.00, 0.00)$.\n  - Connectivity: edges $(0,1)$, $(1,2)$.\n\n- Test Case B (drift across $y$-boundary with mild internal vibrations):\n  - $N = 5$, $T = 8$, $\\mathbf{L} = (2.5, 2.5, 2.5)$.\n  - Initial positions: $\\mathbf{r}_0(0) = (0.30, 1.80, 0.30)$, $\\mathbf{r}_1(0) = (0.30, 1.90, 0.30)$, $\\mathbf{r}_2(0) = (0.30, 2.00, 0.30)$, $\\mathbf{r}_3(0) = (0.30, 2.10, 0.30)$, $\\mathbf{r}_4(0) = (0.30, 2.20, 0.30)$.\n  - Drift per frame: $\\mathbf{d} = (0.00, -0.60, 0.00)$.\n  - Connectivity: edges $(0,1)$, $(1,2)$, $(2,3)$, $(3,4)$.\n\n- Test Case C (near half-box jump in $x$ to probe rounding robustness):\n  - $N = 4$, $T = 7$, $\\mathbf{L} = (3.0, 3.0, 3.0)$.\n  - Initial positions: $\\mathbf{r}_0(0) = (0.10, 0.50, 0.10)$, $\\mathbf{r}_1(0) = (0.23, 0.50, 0.10)$, $\\mathbf{r}_2(0) = (0.36, 0.50, 0.10)$, $\\mathbf{r}_3(0) = (0.49, 0.50, 0.10)$.\n  - Drift per frame: $\\mathbf{d} = (1.49, 0.00, 0.00)$.\n  - Connectivity: edges $(0,1)$, $(1,2)$, $(2,3)$.\n\nRequired computations:\n- For each test case, compute four floats in nanometers:\n  1. Mean RMSD using wrapped coordinates, aligned to the wrapped frame $t=0$.\n  2. Mean RMSD using unwrapped coordinates, aligned to the unwrapped frame $t=0$.\n  3. Mean RMSF (averaged over atoms) using wrapped coordinates after alignment.\n  4. Mean RMSF (averaged over atoms) using unwrapped coordinates after alignment.\n\nOutput format:\n- Your program should produce a single line of output containing the results for the three test cases as a comma-separated list of lists enclosed in square brackets; each inner list must be ordered as specified above for the four floats. For example: \"[[a,b,c,d],[e,f,g,h],[i,j,k,l]]\". All values must be floats in nanometers.\n\nNo external input:\n- Your program must be self-contained, generate the test cases as specified, and print the result line exactly as described, with no additional text. The alignment must be performed by minimizing the sum of squared distances under a rotation in $\\mathrm{SO}(3)$ and a translation, and the unwrapping must use connectivity and the minimum image convention to enforce continuity and internal geometry.\n\nExpress units in nanometers for all reported quantities. Do not include percentage signs; any ratios must be reported as decimal numbers if present.",
            "solution": "The problem presented is valid, scientifically grounded in the principles of computational biophysics, and well-posed with sufficient information for a unique solution. We will proceed with a detailed, step-by-step resolution.\n\nThe core of this problem lies in the disconnect between the mathematical formalism of structural comparison metrics like Root Mean Square Deviation (RMSD) and Root Mean Square Fluctuation (RMSF), which assume a contiguous object in Euclidean space, and the computational convenience of periodic boundary conditions (PBC) in simulations, which \"wrap\" coordinates into a primary box, thus breaking this contiguity.\n\n### 1. The Necessity of Unwrapping for RMSD and RMSF Calculation\n\nLet us begin from the foundational definitions. RMSD and RMSF are measures of structural similarity and atomic mobility, respectively. Both rely fundamentally on the Euclidean distance, $\\|\\mathbf{a} - \\mathbf{b}\\|$, between atomic positions.\n\nThe RMSD between two conformations, $A$ and $B$, each with $N$ atoms, is calculated after finding an optimal rigid-body transformation (rotation $R \\in \\mathrm{SO}(3)$ and translation $\\mathbf{v}$) that minimizes the sum of squared distances between corresponding atoms. The RMSD is then:\n$$\n\\text{RMSD}(A, B) = \\min_{R, \\mathbf{v}} \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} \\| (R\\mathbf{r}_{i,A} + \\mathbf{v}) - \\mathbf{r}_{i,B} \\|^2}\n$$\nThe RMSF of an atom $i$ over a trajectory of $T$ frames is the square root of its mean squared displacement from its average position, after the entire trajectory has been aligned to a common reference frame to remove global translation and rotation:\n$$\n\\text{RMSF}_i = \\sqrt{\\frac{1}{T} \\sum_{t=0}^{T-1} \\| \\mathbf{r}'_i(t) - \\langle \\mathbf{r}'_i \\rangle \\|^2}\n$$\nwhere $\\mathbf{r}'_i(t)$ are the coordinates of atom $i$ in frame $t$ after alignment, and $\\langle\\mathbf{r}'_i\\rangle$ is its time-averaged position.\n\nNow, consider a molecular dynamics simulation employing a periodic box of side lengths $\\mathbf{L} = (L_x, L_y, L_z)$. When an atom crosses a boundary, its coordinates are \"wrapped\" back into the primary box, typically in the range $[0, L_\\alpha)$ for each axis $\\alpha \\in \\{x, y, z\\}$. For example, an atom moving from $x=L_x - \\delta$ to $x=L_x + \\delta$ (where $\\delta$ is small) will have its recorded coordinate jump from $L_x - \\delta$ to $\\delta$.\n\nThis wrapping procedure has two catastrophic consequences for RMSD and RMSF calculations if not corrected:\n\n1.  **Distortion of Internal Geometry**: Consider two bonded atoms, $i$ and $j$, with a small bond length. If the molecule drifts such that atom $i$ is at $x_i \\approx 0$ and atom $j$ has crossed the boundary to $x_j \\approx L_x$, their true separation is small. However, the naive Euclidean distance calculated from their wrapped coordinates, $\\|\\mathbf{w}_i - \\mathbf{w}_j\\|$, will be large, approaching $L_x$. This artifactually breaks the molecule's apparent structure. The alignment algorithm, which seeks to minimize the sum of these squared distances, will be presented with what appears to be a grotesquely distorted structure. It will fail to find a meaningful superposition, leading to an exceptionally large and incorrect RMSD value that reflects properties of the box size rather than true conformational change.\n\n2.  **Corruption of Temporal Averages**: For RMSF, an atom's coordinates over time are averaged. If an atom's true trajectory is continuous but crosses a boundary, its wrapped coordinates will exhibit large, instantaneous jumps (e.g., from $L_y-\\delta$ to $\\delta$). The average of these jumping coordinates, $\\langle \\mathbf{w}_i \\rangle$, will not represent a physically meaningful central position. Instead, it will be biased towards the center of the box. Consequently, the calculated fluctuations of $\\mathbf{w}_i(t)$ around this erroneous average, $\\|\\mathbf{w}_i(t) - \\langle \\mathbf{w}_i \\rangle\\|^2$, will be enormous and dominated by the artificial wrapping events, not by the true physical vibrations of the atom.\n\nTherefore, it is an absolute prerequisite to \"unwrap\" the trajectory, or at least apply imaging corrections, to restore a physically contiguous representation of the molecule in Euclidean space before any alignment, RMSD, or RMSF analysis can be performed.\n\n### 2. An Algorithm for Coordinate Unwrapping\n\nA robust unwrapping algorithm must achieve two goals: (1) maintain the internal geometric integrity of the molecular structure, and (2) preserve the temporal continuity of the molecule's overall motion. The following algorithm uses the molecule's bonded connectivity and the minimum image convention (MIC) to achieve this.\n\nThe MIC states that the true vector between two points $\\mathbf{a}$ and $\\mathbf{b}$ in a periodic box of lengths $\\mathbf{L}$ is the one connecting $\\mathbf{a}$ to the closest periodic image of $\\mathbf{b}$. This vector, $\\Delta\\mathbf{r}_{\\text{mic}}$, is calculated as:\n$$\n\\Delta\\mathbf{r}_{\\text{mic}}(\\mathbf{a}, \\mathbf{b}) = (\\mathbf{a} - \\mathbf{b}) - \\mathbf{L} \\odot \\text{round}\\left(\\frac{\\mathbf{a} - \\mathbf{b}}{\\mathbf{L}}\\right)\n$$\nwhere $\\odot$ denotes component-wise multiplication and the division and rounding are also component-wise.\n\nThe unwrapping proceeds frame-by-frame, using the previously unwrapped frame as a reference for the current one. Let $\\mathbf{W}(t)$ be the wrapped trajectory and $\\mathbf{R}(t)$ be the unwrapped trajectory to be constructed.\n\n**Algorithm: Frame-by-Frame Unwrapping**\n1.  **Initialization**: The first frame requires no unwrapping. Set $\\mathbf{R}_i(0) = \\mathbf{W}_i(0)$ for all atoms $i=0, \\dots, N-1$.\n2.  **Iteration**: For each subsequent frame $t=1, \\dots, T-1$:\n    a.  Let $\\mathbf{R}_{\\text{prev}}$ be the unwrapped coordinates from frame $t-1$, and $\\mathbf{W}_{\\text{curr}}$ be the wrapped coordinates from frame $t$.\n    b.  **Seed Atom Continuity**: Select a single \"seed\" atom (e.g., atom $0$). Its unwrapped position in the new frame is determined by assuming its motion from the previous frame was continuous. We calculate the minimum image of its displacement vector:\n        -   Displacement in wrapped space: $\\Delta\\mathbf{w}_0 = \\mathbf{W}_0(t) - \\mathbf{R}_0(t-1)$.\n        -   Minimum image displacement: $\\Delta\\mathbf{r}'_0 = \\Delta\\mathbf{w}_0 - \\mathbf{L} \\odot \\text{round}(\\Delta\\mathbf{w}_0 / \\mathbf{L})$.\n        -   Set the seed atom's new unwrapped position: $\\mathbf{R}_0(t) = \\mathbf{R}_0(t-1) + \\Delta\\mathbf{r}'_0$.\n    c.  **Propagate via Connectivity**: Reconstruct the rest of the molecule around the newly placed seed atom. This is done by traversing the bonded connectivity graph (e.g., using a Breadth-First Search or Depth-First Search starting from the seed atom).\n        -   Initialize a queue for the traversal with the seed atom and a set of visited atoms.\n        -   While the queue is not empty, dequeue an atom $i$ whose position $\\mathbf{R}_i(t)$ is known.\n        -   For each bonded neighbor $j$ of $i$ that has not been visited:\n            i.  Calculate the vector between $j$ and $i$ in the current wrapped frame: $\\Delta\\mathbf{w}_{ji} = \\mathbf{W}_j(t) - \\mathbf{W}_i(t)$.\n            ii. Apply MIC to find the correct bond vector, connecting to the nearest image of atom $j$: $\\Delta\\mathbf{r}'_{ji} = \\Delta\\mathbf{w}_{ji} - \\mathbf{L} \\odot \\text{round}(\\Delta\\mathbf{w}_{ji} / \\mathbf{L})$.\n            iii. Set the unwrapped position of atom $j$: $\\mathbf{R}_j(t) = \\mathbf{R}_i(t) + \\Delta\\mathbf{r}'_{ji}$.\n            iv. Mark $j$ as visited and add it to the queue.\n3.  The resulting sequence of coordinates $\\mathbf{R}(t)$ constitutes the fully unwrapped trajectory, representing a continuous path in an infinite Euclidean space, with all internal covalent geometry preserved.\n\n### 3. Alignment and Analysis Procedure\n\nWith a valid (unwrapped) trajectory, we can proceed with the analysis.\n\n**Alignment**: The optimal rigid-body rotation is found using the Kabsch algorithm, which is based on Singular Value Decomposition (SVD). To align a \"mobile\" set of coordinates $A$ to a \"reference\" set $B$:\n1.  Translate both sets of coordinates to their respective centroids: $A' = A - \\mathbf{c}_A$ and $B' = B - \\mathbf{c}_B$.\n2.  Compute the $3 \\times 3$ covariance matrix $H = (A')^T B'$.\n3.  Perform SVD on $H$: $H = U S V^T$.\n4.  The optimal rotation matrix is $R = V U^T$. A check must be performed to ensure $R$ is a proper rotation ($\\det(R)=+1$) and not a reflection. If $\\det(R)=-1$, the sign of the column of $V$ corresponding to the smallest singular value is flipped before recomputing $R$.\n5.  The mobile coordinates are then rotated: $A'' = A' R$.\n\n**RMSD Calculation**:\n- For each frame $t \\in \\{1, \\dots, T-1\\}$, the coordinates $\\mathbf{X}(t)$ are aligned to the reference frame $\\mathbf{X}(0)$.\n- The centered coordinates are computed: $\\mathbf{X}'(t) = \\mathbf{X}(t) - \\mathbf{c}(t)$ and $\\mathbf{X}'(0) = \\mathbf{X}(0) - \\mathbf{c}(0)$.\n- The rotation matrix $R_t$ is found via the Kabsch algorithm.\n- The RMSD for that frame is $\\text{RMSD}(t) = \\sqrt{\\frac{1}{N} \\sum_{i=0}^{N-1} \\| (\\mathbf{X}'(t)R_t)_i - \\mathbf{X}'_i(0) \\|^2}$.\n- The final reported value is the mean of these values: $\\langle \\text{RMSD} \\rangle = \\frac{1}{T-1}\\sum_{t=1}^{T-1}\\text{RMSD}(t)$.\n\n**RMSF Calculation**:\n- An entire trajectory, $\\mathbf{X}(0), \\dots, \\mathbf{X}(T-1)$, is aligned to the single reference frame $\\mathbf{X}(0)$. This produces a trajectory of aligned structures, $\\mathbf{X}''(0), \\dots, \\mathbf{X}''(T-1)$, where each frame has been centered and optimally rotated.\n- For each atom $i$, its time-averaged position is computed from this aligned trajectory: $\\langle \\mathbf{x}''_i \\rangle = \\frac{1}{T}\\sum_{t=0}^{T-1} \\mathbf{x}''_i(t)$.\n- The RMSF for atom $i$ is then $\\text{RMSF}_i = \\sqrt{\\frac{1}{T}\\sum_{t=0}^{T-1} \\| \\mathbf{x}''_i(t) - \\langle \\mathbf{x}''_i \\rangle \\|^2}$.\n- The final reported value is the mean RMSF over all atoms: $\\langle \\text{RMSF} \\rangle = \\frac{1}{N}\\sum_{i=0}^{N-1}\\text{RMSF}_i$.\n\nThe implementation will perform these calculations twice for each test case: once using the raw, wrapped coordinates and once using the coordinates produced by our unwrapping algorithm. As hypothesized, the results will demonstrate a drastic difference, highlighting the critical importance of the unwrapping step.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom collections import deque\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"N\": 3, \"T\": 6, \"L\": np.array([3.0, 3.0, 3.0]),\n            \"r0\": np.array([\n                [0.40, 0.40, 0.20],\n                [0.52, 0.40, 0.20],\n                [0.64, 0.40, 0.20]\n            ]),\n            \"d\": np.array([0.80, 0.00, 0.00]),\n            \"connectivity\": [(0, 1), (1, 2)],\n        },\n        {\n            \"N\": 5, \"T\": 8, \"L\": np.array([2.5, 2.5, 2.5]),\n            \"r0\": np.array([\n                [0.30, 1.80, 0.30],\n                [0.30, 1.90, 0.30],\n                [0.30, 2.00, 0.30],\n                [0.30, 2.10, 0.30],\n                [0.30, 2.20, 0.30]\n            ]),\n            \"d\": np.array([0.00, -0.60, 0.00]),\n            \"connectivity\": [(0, 1), (1, 2), (2, 3), (3, 4)],\n        },\n        {\n            \"N\": 4, \"T\": 7, \"L\": np.array([3.0, 3.0, 3.0]),\n            \"r0\": np.array([\n                [0.10, 0.50, 0.10],\n                [0.23, 0.50, 0.10],\n                [0.36, 0.50, 0.10],\n                [0.49, 0.50, 0.10]\n            ]),\n            \"d\": np.array([1.49, 0.00, 0.00]),\n            \"connectivity\": [(0, 1), (1, 2), (2, 3)],\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        N, T, L, r0, d, conn_edges = case[\"N\"], case[\"T\"], case[\"L\"], case[\"r0\"], case[\"d\"], case[\"connectivity\"]\n\n        # 1. Generate trajectories\n        unwrapped_traj = np.zeros((T, N, 3))\n        wrapped_traj = np.zeros((T, N, 3))\n        \n        t_vals = np.arange(T)[:, np.newaxis]\n        i_vals = np.arange(N)[np.newaxis, :]\n        \n        eps_y = 0.01 * np.sin(2 * np.pi * t_vals / T + i_vals)\n        eps_z = 0.005 * np.cos(2 * np.pi * t_vals / T + i_vals / 2)\n        epsilon = np.stack([np.zeros((T, N)), eps_y, eps_z], axis=2)\n\n        unwrapped_traj = r0[np.newaxis, :, :] + t_vals[:, :, np.newaxis] * d[np.newaxis, np.newaxis, :] + epsilon\n        wrapped_traj = unwrapped_traj % L\n\n        # 2. Re-create unwrapped trajectory from wrapped one to test the algorithm\n        re_unwrapped_traj = unwrap_trajectory(wrapped_traj, L, conn_edges)\n\n        # 3. Analyze trajectories\n        mean_rmsd_w, mean_rmsf_w = analyze_trajectory(wrapped_traj)\n        mean_rmsd_u, mean_rmsf_u = analyze_trajectory(re_unwrapped_traj)\n\n        all_results.append([mean_rmsd_w, mean_rmsd_u, mean_rmsf_w, mean_rmsf_u])\n\n    # Final print statement in the exact required format.\n    inner_strings = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    print(f\"[{','.join(inner_strings)}]\")\n\ndef kabsch_align(mobile_coords, ref_coords):\n    \"\"\"\n    Aligns mobile_coords to ref_coords using the Kabsch algorithm and returns the\n    rotated (but not translated) mobile coordinates.\n    \"\"\"\n    # Center coordinates\n    mobile_centroid = np.mean(mobile_coords, axis=0)\n    ref_centroid = np.mean(ref_coords, axis=0)\n    mobile_c = mobile_coords - mobile_centroid\n    ref_c = ref_coords - ref_centroid\n\n    # Covariance matrix\n    H = mobile_c.T @ ref_c\n\n    # SVD\n    U, S, Vt = np.linalg.svd(H)\n    V = Vt.T\n\n    # Calculate rotation matrix, checking for reflection\n    R = V @ U.T\n    if np.linalg.det(R)  0:\n        V[:, -1] *= -1\n        R = V @ U.T\n    \n    # Apply rotation\n    aligned_mobile_c = mobile_c @ R\n    \n    return aligned_mobile_c, ref_c\n\n\ndef analyze_trajectory(traj):\n    \"\"\"\n    Computes mean RMSD and mean RMSF for a given trajectory.\n    \"\"\"\n    T, N, _ = traj.shape\n    ref_frame = traj[0]\n\n    # Mean RMSD calculation (over frames t=1...T-1)\n    rmsd_values = []\n    if T > 1:\n        for t in range(1, T):\n            mobile_frame = traj[t]\n            aligned_mobile_c, ref_c = kabsch_align(mobile_frame, ref_frame)\n            \n            # Sum of squared distances\n            ssd = np.sum((aligned_mobile_c - ref_c)**2)\n            rmsd = np.sqrt(ssd / N)\n            rmsd_values.append(rmsd)\n        mean_rmsd = np.mean(rmsd_values) if rmsd_values else 0.0\n    else:\n        mean_rmsd = 0.0\n\n    # Mean RMSF calculation (over all atoms)\n    aligned_traj_c = np.zeros_like(traj)\n    _, ref_c_for_all = kabsch_align(ref_frame, ref_frame)\n    aligned_traj_c[0] = ref_c_for_all\n    \n    for t in range(1, T):\n        aligned_mobile_c, _ = kabsch_align(traj[t], ref_frame)\n        aligned_traj_c[t] = aligned_mobile_c\n        \n    avg_pos = np.mean(aligned_traj_c, axis=0)\n    sq_displacements = np.sum((aligned_traj_c - avg_pos)**2, axis=2) # Sum over x,y,z\n    msd_per_atom = np.mean(sq_displacements, axis=0) # Average over time\n    rmsf_values = np.sqrt(msd_per_atom)\n    mean_rmsf = np.mean(rmsf_values)\n\n    return mean_rmsd, mean_rmsf\n\n\ndef unwrap_trajectory(wrapped_traj, L, connectivity_edges):\n    \"\"\"\n    Unwraps a trajectory using connectivity and the minimum image convention.\n    \"\"\"\n    T, N, _ = wrapped_traj.shape\n    unwrapped_traj = np.zeros_like(wrapped_traj)\n    unwrapped_traj[0] = wrapped_traj[0]\n\n    # Build adjacency list for connectivity\n    adj = [[] for _ in range(N)]\n    for i, j in connectivity_edges:\n        adj[i].append(j)\n        adj[j].append(i)\n\n    for t in range(1, T):\n        prev_unwrapped = unwrapped_traj[t-1]\n        curr_wrapped = wrapped_traj[t]\n        curr_unwrapped = np.zeros_like(curr_wrapped)\n        \n        visited = [False] * N\n        q = deque()\n\n        # 1. Place seed atom (atom 0) based on temporal continuity\n        seed_atom = 0\n        delta = curr_wrapped[seed_atom] - prev_unwrapped[seed_atom]\n        shift = L * np.round(delta / L)\n        delta_mic = delta - shift\n        curr_unwrapped[seed_atom] = prev_unwrapped[seed_atom] + delta_mic\n        \n        visited[seed_atom] = True\n        q.append(seed_atom)\n\n        # 2. Propagate unwrapping through the molecule via BFS\n        while q:\n            i = q.popleft()\n            for j in adj[i]:\n                if not visited[j]:\n                    delta_ij = curr_wrapped[j] - curr_wrapped[i]\n                    shift_ij = L * np.round(delta_ij / L)\n                    delta_ij_mic = delta_ij - shift_ij\n                    curr_unwrapped[j] = curr_unwrapped[i] + delta_ij_mic\n                    \n                    visited[j] = True\n                    q.append(j)\n        \n        unwrapped_traj[t] = curr_unwrapped\n        \n    return unwrapped_traj\n\n\nif __name__ == \"__main__\":\n    solve()\n\n```"
        },
        {
            "introduction": "A value computed from a simulation, such as a mean RMSD, is a statistical estimate, not an exact truth. To report our findings with scientific rigor, we must also report the uncertainty in that estimate, a task complicated by the fact that data points in a time series from a molecular dynamics simulation are inherently correlated. This advanced practice () introduces the gold-standard method for this task: using block analysis, guided by the integrated autocorrelation time, to calculate a robust standard error for the mean.",
            "id": "3868639",
            "problem": "You are given time series representing Root Mean Square Deviation (RMSD) values of a macromolecule relative to a fixed reference structure, sampled uniformly in time. Consider the RMSD series as a realization of a stationary stochastic process $\\{r_t\\}_{t=1}^N$ measured in angstroms, where $t$ indexes frames and $N$ is the number of frames. The objective is to construct a block-analysis method to estimate the standard error of the mean RMSD and to select block sizes using the integrated autocorrelation time.\n\nStart from core definitions of a stationary process: expectation, variance, autocovariance function, and autocorrelation function, and use these as the base for your derivation and algorithm. Your program must:\n\n- Estimate the normalized autocorrelation function of the centered RMSD series and use it to compute the integrated autocorrelation time in frames.\n- Choose a block size in frames by scaling the integrated autocorrelation time by a constant factor so that blocks are longer than the correlation time, while ensuring at least two blocks exist.\n- Partition the series into contiguous non-overlapping blocks of equal length, compute block means, and use these to estimate the standard error of the overall mean RMSD.\n- Handle edge cases such as zero variance series and very short series in a scientifically consistent way.\n\nConcrete requirements:\n\n1. Given a real-valued time series $\\{r_t\\}_{t=1}^N$:\n   - Center the series by subtracting its sample mean to form $\\{x_t\\}_{t=1}^N$ with $x_t = r_t - \\bar{r}$.\n   - Compute the sample autocorrelation function $\\rho(k)$ for lags $k \\in \\{0,1,\\dots,N-1\\}$ using the normalized autocovariance of $\\{x_t\\}$, where $\\rho(0) = 1$ by definition.\n   - Estimate the integrated autocorrelation time $\\tau_{\\mathrm{int}}$ in frames by summing the positive-lag autocorrelation values until a cutoff defined by the first non-positive $\\rho(k)$; if no non-positive values occur within the available lags, continue summing until the lag at which $\\rho(k)$ falls below the threshold $1/\\sqrt{N}$, or else up to the maximum available lag. Include the conventional $\\frac{1}{2}$ term associated with the zero-lag contribution when forming the integrated autocorrelation time.\n   - If the sample variance of $\\{x_t\\}$ is zero, set $\\tau_{\\mathrm{int}} = 0$.\n\n2. Choose the block size $B$ in frames as the smallest integer greater than or equal to $c \\,\\tau_{\\mathrm{int}}$ with $c = 5$, subject to the constraints $B \\ge 1$ and that the number of blocks $M = \\left\\lfloor \\frac{N}{B} \\right\\rfloor$ satisfies $M \\ge 2$. If the choice $B = \\lceil c \\,\\tau_{\\mathrm{int}} \\rceil$ yields $M  2$, then set $B = \\left\\lfloor \\frac{N}{2} \\right\\rfloor$ to guarantee at least two blocks. For $\\tau_{\\mathrm{int}} = 0$ use $B = 1$.\n\n3. Form $M$ contiguous non-overlapping blocks of length $B$ by truncating any trailing frames that do not complete a block. Compute the mean of each block and use these block means to estimate the standard error of the mean RMSD as the square root of the unbiased sample variance of the block means divided by $M$.\n\n4. Express the standard error in angstroms. Round all final reported floats to six decimal places.\n\n5. Test Suite and Data Generation:\n   Implement your method on the following four test cases. For reproducibility, use the specified pseudorandom seeds. All RMSD values are in angstroms. For autoregressive processes, use a Gaussian innovation $\\epsilon_t \\sim \\mathcal{N}(0,1)$ scaled by a noise amplitude.\n\n   - Case $1$ (approximately uncorrelated): White noise around a mean RMSD\n     - Parameters: $N = 4096$, $\\mu = 2.0$, $\\sigma = 0.1$, seed $= 123$.\n     - Generation: $r_t = \\mu + \\sigma \\,\\epsilon_t$ with independent $\\epsilon_t$.\n   - Case $2$ (strongly autocorrelated): First-order autoregressive process\n     - Parameters: $N = 4096$, $\\mu = 2.0$, $\\phi = 0.9$, $\\sigma = 0.05$, seed $= 456$.\n     - Generation: $r_1 = \\mu$, and for $t \\ge 2$, $r_t = \\mu + \\phi \\,(r_{t-1} - \\mu) + \\sigma \\,\\epsilon_t$.\n   - Case $3$ (moderately autocorrelated and shorter): First-order autoregressive process\n     - Parameters: $N = 1024$, $\\mu = 3.0$, $\\phi = 0.5$, $\\sigma = 0.2$, seed $= 789$.\n     - Generation: $r_1 = \\mu$, and for $t \\ge 2$, $r_t = \\mu + \\phi \\,(r_{t-1} - \\mu) + \\sigma \\,\\epsilon_t$.\n   - Case $4$ (degenerate): Constant series\n     - Parameters: $N = 128$, $\\mu = 1.5$.\n     - Generation: $r_t = \\mu$ for all $t$.\n\n6. Required final output format:\n   Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets. Each test case result must be a list of three values in the order $[\\tau_{\\mathrm{int}} \\text{ in frames}, B \\text{ in frames}, \\text{standard error in angstrom}]$. For example, a valid output format using placeholder values is \"[[$12.000000$,$64$,$0.004321$],[$8.500000$,$43$,$0.002100$],[$1.000000$,$5$,$0.010000$],[$0.000000$,$1$,$0.000000$]]\". Your program must implement the generation of the four test cases exactly as specified and compute the corresponding outputs using your method.",
            "solution": "The problem asks for the implementation of a block-averaging method to estimate the standard error of the mean for a correlated time series, specifically a Root Mean Square Deviation (RMSD) series from a molecular simulation trajectory. The method's design is dictated by concepts from the statistical mechanics of time series analysis.\n\nA time series $\\{r_t\\}_{t=1}^N$ drawn from a stationary stochastic process is characterized by a time-independent mean $\\mu = E[r_t]$ and variance $\\sigma^2 = E[(r_t - \\mu)^2]$. The correlation between observations at different times is described by the autocovariance function, $\\gamma(k) = E[(r_t - \\mu)(r_{t+k} - \\mu)]$, and its normalized counterpart, the autocorrelation function (ACF), $\\rho(k) = \\gamma(k) / \\gamma(0)$. By definition, $\\rho(0) = 1$.\n\nThe primary objective is to estimate the standard error of the sample mean, $\\bar{r} = \\frac{1}{N}\\sum_{t=1}^N r_t$. For a series of $N$ independent observations, the variance of the sample mean is $\\text{Var}(\\bar{r}) = \\sigma^2/N$. However, when the data points are correlated, this formula is incorrect and typically underestimates the true variance. The correct variance for a correlated series is given by:\n$$\n\\text{Var}(\\bar{r}) = \\frac{\\sigma^2}{N} \\left( 1 + 2\\sum_{k=1}^{N-1} \\left(1 - \\frac{k}{N}\\right) \\rho(k) \\right)\n$$\nFor large $N$, this approximates to $\\text{Var}(\\bar{r}) \\approx \\frac{\\sigma^2}{N} g$, where $g$ is the statistical inefficiency, defined as:\n$$\ng = 1 + 2\\sum_{k=1}^{\\infty} \\rho(k)\n$$\nThe quantity $g$ can be interpreted as the number of correlated samples that contain the same amount of information as one independent sample. The integrated autocorrelation time $\\tau_{\\mathrm{int}}$ is related to $g$ and provides a measure of the correlation time in the series. The problem specifies a particular definition for its estimation:\n$$\n\\tau_{\\mathrm{int}} = \\frac{1}{2} + \\sum_{k=1}^{k_{\\mathrm{max}}} \\rho(k)\n$$\nThis is related to the statistical inefficiency by $g \\approx 1 + 2\\tau_{\\mathrm{int}}$ for large sums, though this specific formula links to other conventions. We will strictly follow the provided definition.\n\nThe block-averaging method provides a robust way to estimate $\\text{Var}(\\bar{r})$ without explicitly calculating the full ACF. The core idea is to partition the time series into blocks of a length $B$ that is much larger than the correlation time $\\tau_{\\mathrm{int}}$. If $B \\gg \\tau_{\\mathrm{int}}$, the means of these blocks can be treated as approximately independent and identically distributed random variables. The standard statistical formulas for i.i.d. data can then be applied to this new, smaller sample of block means.\n\nThe algorithm proceeds as follows:\n\n**Step 1: Estimation of the Integrated Autocorrelation Time $\\tau_{\\mathrm{int}}$**\nFirst, the input time series $\\{r_t\\}_{t=1}^N$ is centered by subtracting its sample mean $\\bar{r} = \\frac{1}{N}\\sum_{t=1}^N r_t$, yielding a new series $\\{x_t = r_t - \\bar{r}\\}_{t=1}^N$ with a mean of zero.\n\nNext, the sample autocorrelation function $\\hat{\\rho}(k)$ is computed. A common (biased) estimator for the autocovariance is $\\hat{\\gamma}(k) = \\frac{1}{N} \\sum_{t=1}^{N-k} x_t x_{t+k}$. The sample ACF is then $\\hat{\\rho}(k) = \\hat{\\gamma}(k) / \\hat{\\gamma}(0)$. $\\hat{\\gamma}(0)$ is simply the sample variance of $\\{x_t\\}$.\n\nWith the estimated ACF, $\\hat{\\rho}(k)$, the integrated autocorrelation time $\\tau_{\\mathrm{int}}$ is calculated using the formula and summation cutoff procedure stipulated:\n$$\n\\tau_{\\mathrm{int}} = \\frac{1}{2} + \\sum_{k=1}^{k_{\\mathrm{max}}} \\hat{\\rho}(k)\n$$\nThe upper summation limit $k_{\\mathrm{max}}$ is determined by finding the first lag $k  0$ for which $\\hat{\\rho}(k) \\le 0$. If all $\\hat{\\rho}(k)$ for $k0$ are positive, the summation continues until $\\hat{\\rho}(k)$ first falls below a noise-level threshold of $1/\\sqrt{N}$. If this threshold is never crossed, the sum includes all available lags up to $N-1$.\nIn the special case where the variance of the series is zero (i.e., a constant series), $\\tau_{\\mathrm{int}}$ is defined to be $0$.\n\n**Step 2: Selection of Block Size $B$**\nThe block size $B$ must be large enough to ensure that the block means are approximately uncorrelated, i.e., $B$ should be significantly larger than $\\tau_{\\mathrm{int}}$. A scaling factor $c$ is introduced for this purpose. The block size $B$ is chosen as:\n$$\nB = \\lceil c \\cdot \\tau_{\\mathrm{int}} \\rceil\n$$\nwith $c=5$. Additionally, two constraints must be met: the block size must be at least $1$ frame ($B \\ge 1$), and there must be at least two blocks ($M \\ge 2$), where $M = \\lfloor N/B \\rfloor$ is the number of blocks. If the initial choice of $B$ results in $M  2$, the block size is adjusted to $B = \\lfloor N/2 \\rfloor$ to guarantee $M \\ge 2$ for any series of length $N \\ge 2$. For the case $\\tau_{\\mathrm{int}}=0$, $B$ is set to $1$.\n\n**Step 3: Standard Error Estimation**\nThe original time series of length $N$ is divided into $M = \\lfloor N/B \\rfloor$ non-overlapping blocks of length $B$. Any trailing data points that do not form a full block are discarded.\nFor each block $j \\in \\{1, \\dots, M\\}$, the block mean $\\bar{r}_{\\text{block}, j}$ is calculated:\n$$\n\\bar{r}_{\\text{block}, j} = \\frac{1}{B} \\sum_{i=1}^{B} r_{(j-1)B + i}\n$$\nThis produces a new, smaller time series of block means $\\{\\bar{r}_{\\text{block}, j}\\}_{j=1}^M$. Since these block means are assumed to be approximately independent, the standard error of their mean can be calculated. The unbiased sample variance of the block means is:\n$$\n\\hat{\\sigma}^2_{\\text{blocks}} = \\frac{1}{M-1} \\sum_{j=1}^{M} (\\bar{r}_{\\text{block}, j} - \\bar{\\bar{r}}_{\\text{block}})^2\n$$\nwhere $\\bar{\\bar{r}}_{\\text{block}}$ is the mean of the block means. The square root of the variance of the mean of block means gives the final estimate for the standard error of the mean (SEM) of the original series:\n$$\n\\text{SEM} = \\sqrt{\\frac{\\hat{\\sigma}^2_{\\text{blocks}}}{M}}\n$$\nThis procedure is applied to the four test cases specified, involving the generation of synthetic RMSD data from white noise, autoregressive processes, and a constant series, to demonstrate the method's application across different correlation structures. All final floating-point values are reported to six decimal places.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the block analysis on specified test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1 (approximately uncorrelated): White noise\n        {'type': 'noise', 'N': 4096, 'mu': 2.0, 'sigma': 0.1, 'seed': 123},\n        # Case 2 (strongly autocorrelated): AR(1) process\n        {'type': 'ar1', 'N': 4096, 'mu': 2.0, 'phi': 0.9, 'sigma': 0.05, 'seed': 456},\n        # Case 3 (moderately autocorrelated and shorter): AR(1) process\n        {'type': 'ar1', 'N': 1024, 'mu': 3.0, 'phi': 0.5, 'sigma': 0.2, 'seed': 789},\n        # Case 4 (degenerate): Constant series\n        {'type': 'const', 'N': 128, 'mu': 1.5}\n    ]\n\n    results = []\n    for params in test_cases:\n        # Generate the time series data for the current case\n        N = params['N']\n        mu = params['mu']\n        rng = np.random.default_rng(params.get('seed'))\n\n        if params['type'] == 'noise':\n            sigma = params['sigma']\n            r = mu + sigma * rng.normal(size=N)\n        elif params['type'] == 'ar1':\n            phi = params['phi']\n            sigma = params['sigma']\n            r = np.zeros(N)\n            r[0] = mu\n            innovations = sigma * rng.normal(size=N)\n            for t in range(1, N):\n                r[t] = mu + phi * (r[t-1] - mu) + innovations[t]\n        elif params['type'] == 'const':\n            r = np.full(N, mu)\n        \n        # --- Start of analysis algorithm ---\n\n        # 1. Center the series and compute variance\n        r_mean = np.mean(r)\n        x = r - r_mean\n        r_var = np.var(x)\n\n        if r_var == 0:\n            tau_int = 0.0\n        else:\n            # Compute normalized autocorrelation function (ACF)\n            # np.correlate with 'full' gives unnormalized autocovariance sequence\n            # The second half (from N-1) corresponds to non-negative lags\n            autocov = np.correlate(x, x, mode='full')[N-1:]\n            acf = autocov / autocov[0]\n            \n            # Estimate integrated autocorrelation time (tau_int)\n            # Sum ACF until it becomes non-positive\n            positive_acf_indices = np.where(acf[1:] = 0)[0]\n            if positive_acf_indices.size > 0:\n                k_max = positive_acf_indices[0] + 1\n            else:\n                # If ACF is always positive, sum until it's below a threshold\n                threshold = 1 / np.sqrt(N)\n                threshold_indices = np.where(acf[1:]  threshold)[0]\n                if threshold_indices.size > 0:\n                    k_max = threshold_indices[0] + 1\n                else:\n                    # If still not found, sum over all available lags\n                    k_max = N - 1\n            \n            # The sum includes lags from 1 to k_max\n            tau_int = 0.5 + np.sum(acf[1:k_max + 1])\n\n        # 2. Choose the block size B\n        if tau_int == 0:\n            B = 1\n        else:\n            c = 5.0\n            B = int(np.ceil(c * tau_int))\n            B = max(1, B) # Ensure B is at least 1\n            \n            # Ensure at least two blocks exist\n            if N // B  2:\n                B = N // 2\n        \n        # 3. Compute block means and standard error\n        if N  2: # Cannot form blocks\n            M = 0\n            sem = np.nan\n        else:\n            M = N // B\n            \n        if M  2: # Handle cases that slip through, although logic for B should prevent this\n            sem = np.nan\n        else:\n            # Truncate series to fit an integer number of blocks\n            r_truncated = r[:M * B]\n            # Reshape into blocks and compute means\n            blocks = r_truncated.reshape((M, B))\n            block_means = np.mean(blocks, axis=1)\n            \n            # Compute SEM from block means\n            # Unbiased sample variance of block means (ddof=1)\n            if M > 1:\n                var_block_means = np.var(block_means, ddof=1)\n                sem = np.sqrt(var_block_means / M)\n            else: # Should not happen with M>=2 guarantee\n                sem = 0.0\n        \n        if r_var == 0:\n           sem = 0.0 # For constant series, SEM is exactly 0.\n\n        results.append([tau_int, B, sem])\n\n    # 4. Format and print the final output\n    case_strings = []\n    for res in results:\n        tau_str = f\"{res[0]:.6f}\"\n        b_str = str(res[1])\n        sem_str = f\"{res[2]:.6f}\"\n        case_strings.append(f\"[{tau_str},{b_str},{sem_str}]\")\n    \n    final_output = f\"[{','.join(case_strings)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}