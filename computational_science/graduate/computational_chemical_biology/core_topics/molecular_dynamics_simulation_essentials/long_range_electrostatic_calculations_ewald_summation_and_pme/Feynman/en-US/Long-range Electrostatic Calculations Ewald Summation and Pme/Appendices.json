{
    "hands_on_practices": [
        {
            "introduction": "The Ewald summation method introduces a splitting parameter, $\\alpha$, as a mathematical device to partition the Coulomb potential into short-range and long-range components. For the method to be physically sound, the final calculated energy must not depend on this arbitrary choice. This practice guides you through a formal proof of this invariance, providing a rigorous mathematical exercise that solidifies the foundational principles of Ewald summation .",
            "id": "3851277",
            "problem": "Consider a system of $N$ point charges $\\{q_{i}\\}_{i=1}^{N}$ at positions $\\{\\mathbf{r}_{i}\\}_{i=1}^{N}$ in a cubic periodic box of side length $L$ and volume $V=L^{3}$, with overall charge neutrality $\\sum_{i=1}^{N} q_{i} = 0$. The electrostatic interaction is evaluated under periodic boundary conditions using the Ewald splitting with a Gaussian screening parameter $\\alpha>0$. The total electrostatic energy $U(\\alpha)$ is composed of a real-space term $U_{\\mathrm{r}}(\\alpha)$, a reciprocal-space term $U_{\\mathrm{k}}(\\alpha)$, a self-energy term $U_{\\mathrm{self}}(\\alpha)$, and, depending on the boundary condition at infinity, a surface term $U_{\\mathrm{surf}}$. For a conducting (tin-foil) boundary condition, $U_{\\mathrm{surf}}$ is zero and has no dependence on $\\alpha$. Assume the Coulomb prefactor is unity to focus on the $\\alpha$-scaling. Define the real-space contribution using the Ewald-split pair potential\n$$\n\\phi_{\\mathrm{r}}(r;\\alpha) = \\frac{\\operatorname{erfc}(\\alpha r)}{r},\n$$\nwhere $\\operatorname{erfc}$ is the complementary error function, and let the reciprocal-space contribution be defined through the Fourier-space representation with reciprocal lattice vectors $\\mathbf{k} = \\frac{2\\pi}{L} \\mathbf{m}$, $\\mathbf{m}\\in\\mathbb{Z}^{3}$, and the structure factor $S(\\mathbf{k}) = \\sum_{j=1}^{N} q_{j} \\exp(- i \\mathbf{k}\\cdot \\mathbf{r}_{j})$. The Particle Mesh Ewald (PME) method evaluates $U_{\\mathrm{k}}(\\alpha)$ numerically through charge assignment and a mesh-based Fast Fourier Transform, but the exact Ewald sum corresponds to the formal reciprocal-space series over nonzero $\\mathbf{k}$.\n\nStarting from the above definitions and using only fundamental properties of Gaussians and the Poisson summation formula, analyze the $\\alpha$-scaling of each contribution and compute the exact derivative $\\frac{\\partial U(\\alpha)}{\\partial \\alpha}$ for the neutral system under conducting boundary conditions. You must explicitly account for the self-energy and surface terms in your analysis and justify any cancellation of $\\alpha$-dependences that you claim. Express your final answer as a single real number. No units are required. No rounding is required.",
            "solution": "The task is to compute the derivative of the total Ewald electrostatic energy $U(\\alpha)$ with respect to the Gaussian screening parameter $\\alpha$. The total energy in Ewald summation is constructed to be independent of $\\alpha$. Therefore, its derivative with respect to $\\alpha$ must be zero. The problem requires a formal proof of this result by analyzing the $\\alpha$-dependence of each term in the Ewald energy expression for a specific case.\n\nThe total electrostatic energy $U(\\alpha)$ for a system of $N$ point charges $\\{q_i\\}$ at positions $\\{\\mathbf{r}_i\\}$ in a cubic box of volume $V=L^3$ under periodic boundary conditions, with overall charge neutrality ($\\sum_i q_i = 0$) and conducting boundary conditions at infinity, is given by the sum of four terms:\n$$\nU(\\alpha) = U_{\\mathrm{r}}(\\alpha) + U_{\\mathrm{k}}(\\alpha) + U_{\\mathrm{self}}(\\alpha) + U_{\\mathrm{surf}}\n$$\n\nThe problem specifies that the Coulomb prefactor is unity (i.e., $1/(4\\pi\\epsilon_0) = 1$) and that for conducting boundary conditions, $U_{\\mathrm{surf}} = 0$. The other three terms are given by the standard Ewald formulas, consistent with the provided real-space potential.\n\n1.  **Real-space term ($U_{\\mathrm{r}}$):** This term sums the short-range screened interactions between all pairs of charges and their periodic images. The prime on the summation indicates exclusion of the $i=j$ self-interaction term within the primary cell ($\\mathbf{n}=0$).\n    $$\n    U_{\\mathrm{r}}(\\alpha) = \\frac{1}{2} \\sum_{i,j=1}^{N} \\sum_{\\mathbf{n}\\in\\mathbb{Z}^3}{'} q_i q_j \\frac{\\operatorname{erfc}(\\alpha |\\mathbf{r}_{ij} + L\\mathbf{n}|)}{|\\mathbf{r}_{ij} + L\\mathbf{n}|}\n    $$\n    where $\\mathbf{r}_{ij} = \\mathbf{r}_i - \\mathbf{r}_j$ and $\\operatorname{erfc}(x)$ is the complementary error function.\n\n2.  **Reciprocal-space term ($U_{\\mathrm{k}}$):** This term accounts for the long-range part of the interaction via a sum in Fourier space over reciprocal lattice vectors $\\mathbf{k} = \\frac{2\\pi}{L}\\mathbf{m}$ with $\\mathbf{m} \\in \\mathbb{Z}^3$. The $\\mathbf{k}=\\mathbf{0}$ term is excluded.\n    $$\n    U_{\\mathrm{k}}(\\alpha) = \\frac{2\\pi}{V} \\sum_{\\mathbf{k}\\neq \\mathbf{0}} \\frac{|S(\\mathbf{k})|^2}{k^2} \\exp\\left(-\\frac{k^2}{4\\alpha^2}\\right)\n    $$\n    where $S(\\mathbf{k}) = \\sum_{j=1}^{N} q_j \\exp(-i\\mathbf{k}\\cdot\\mathbf{r}_j)$ is the structure factor. The prefactor $2\\pi/V$ is consistent with setting $1/(4\\pi\\epsilon_0)=1$.\n\n3.  **Self-energy term ($U_{\\mathrm{self}}$):** This term subtracts the artificial interaction of each charge with its own screening Gaussian charge cloud.\n    $$\n    U_{\\mathrm{self}}(\\alpha) = -\\frac{\\alpha}{\\sqrt{\\pi}} \\sum_{i=1}^{N} q_i^2\n    $$\n\nWe will now compute the derivative of each term with respect to $\\alpha$.\n\n**Derivative of the real-space term $\\frac{\\partial U_{\\mathrm{r}}}{\\partial \\alpha}$:**\nWe first need the derivative of the screening function. Using the chain rule and the identity $\\frac{d}{dx}\\operatorname{erfc}(x) = -\\frac{2}{\\sqrt{\\pi}}\\exp(-x^2)$, we have:\n$$\n\\frac{\\partial}{\\partial \\alpha} \\left( \\frac{\\operatorname{erfc}(\\alpha r)}{r} \\right) = \\frac{1}{r} \\left( \\frac{\\partial}{\\partial \\alpha} \\operatorname{erfc}(\\alpha r) \\right) = \\frac{1}{r} \\left( -\\frac{2}{\\sqrt{\\pi}}\\exp(-(\\alpha r)^2) \\cdot r \\right) = -\\frac{2}{\\sqrt{\\pi}}\\exp(-\\alpha^2 r^2)\n$$\nApplying this to $U_{\\mathrm{r}}(\\alpha)$, we get:\n$$\n\\frac{\\partial U_{\\mathrm{r}}}{\\partial \\alpha} = \\frac{1}{2} \\sum_{i,j,\\mathbf{n}}{'} q_i q_j \\left( -\\frac{2}{\\sqrt{\\pi}}\\exp(-\\alpha^2 |\\mathbf{r}_{ij} + L\\mathbf{n}|^2) \\right) = -\\frac{1}{\\sqrt{\\pi}} \\sum_{i,j,\\mathbf{n}}{'} q_i q_j \\exp(-\\alpha^2 |\\mathbf{r}_{ij} + L\\mathbf{n}|^2)\n$$\nThe primed sum $\\sum'$ is over all pairs $(i,j)$ and lattice vectors $\\mathbf{n}$, excluding the case where $i=j$ and $\\mathbf{n}=\\mathbf{0}$ simultaneously. We can rewrite this by taking the full sum and subtracting the excluded term:\n$$\n\\sum_{i,j,\\mathbf{n}}{'} (\\dots) = \\sum_{i,j,\\mathbf{n}} (\\dots) - \\sum_{i=1}^N \\left( q_i^2 \\exp(-\\alpha^2 |\\mathbf{r}_{ii} + L\\mathbf{0}|^2) \\right) = \\sum_{i,j,\\mathbf{n}} (\\dots) - \\sum_{i=1}^N q_i^2\n$$\nSo, the derivative of the real-space term is:\n$$\n\\frac{\\partial U_{\\mathrm{r}}}{\\partial \\alpha} = -\\frac{1}{\\sqrt{\\pi}} \\left( \\sum_{i,j,\\mathbf{n}} q_i q_j \\exp(-\\alpha^2 |\\mathbf{r}_{ij} + L\\mathbf{n}|^2) \\right) + \\frac{1}{\\sqrt{\\pi}} \\sum_{i=1}^{N} q_i^2\n$$\n\n**Derivative of the reciprocal-space term $\\frac{\\partial U_{\\mathrm{k}}}{\\partial \\alpha}$:**\nWe differentiate the exponential term with respect to $\\alpha$:\n$$\n\\frac{\\partial}{\\partial \\alpha} \\exp\\left(-\\frac{k^2}{4\\alpha^2}\\right) = \\exp\\left(-\\frac{k^2}{4\\alpha^2}\\right) \\cdot \\left( -\\frac{k^2}{4} \\right) \\cdot \\left( -2\\alpha^{-3} \\right) = \\frac{k^2}{2\\alpha^3} \\exp\\left(-\\frac{k^2}{4\\alpha^2}\\right)\n$$\nSubstituting this into the expression for $U_{\\mathrm{k}}(\\alpha)$:\n$$\n\\frac{\\partial U_{\\mathrm{k}}}{\\partial \\alpha} = \\frac{2\\pi}{V} \\sum_{\\mathbf{k}\\neq \\mathbf{0}} \\frac{|S(\\mathbf{k})|^2}{k^2} \\left[ \\frac{k^2}{2\\alpha^3} \\exp\\left(-\\frac{k^2}{4\\alpha^2}\\right) \\right] = \\frac{\\pi}{V\\alpha^3} \\sum_{\\mathbf{k}\\neq \\mathbf{0}} |S(\\mathbf{k})|^2 \\exp\\left(-\\frac{k^2}{4\\alpha^2}\\right)\n$$\n\n**Derivative of the self-energy term $\\frac{\\partial U_{\\mathrm{self}}}{\\partial \\alpha}$:**\nThis derivative is straightforward:\n$$\n\\frac{\\partial U_{\\mathrm{self}}}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha} \\left( -\\frac{\\alpha}{\\sqrt{\\pi}} \\sum_{i=1}^{N} q_i^2 \\right) = -\\frac{1}{\\sqrt{\\pi}} \\sum_{i=1}^{N} q_i^2\n$$\n\n**Total Derivative $\\frac{\\partial U}{\\partial \\alpha}$:**\nNow, we sum the derivatives of all components. The derivative of $U_{\\mathrm{surf}}$ is $0$.\n$$\n\\frac{\\partial U}{\\partial \\alpha} = \\frac{\\partial U_{\\mathrm{r}}}{\\partial \\alpha} + \\frac{\\partial U_{\\mathrm{k}}}{\\partial \\alpha} + \\frac{\\partial U_{\\mathrm{self}}}{\\partial \\alpha}\n$$\n$$\n\\frac{\\partial U}{\\partial \\alpha} = \\left[ -\\frac{1}{\\sqrt{\\pi}} \\sum_{i,j,\\mathbf{n}} q_i q_j e^{-\\alpha^2 |\\mathbf{r}_{ij} + L\\mathbf{n}|^2} + \\frac{1}{\\sqrt{\\pi}} \\sum_i q_i^2 \\right] + \\left[ \\frac{\\pi}{V\\alpha^3} \\sum_{\\mathbf{k}\\neq\\mathbf{0}} |S(\\mathbf{k})|^2 e^{-\\frac{k^2}{4\\alpha^2}} \\right] + \\left[ -\\frac{1}{\\sqrt{\\pi}} \\sum_i q_i^2 \\right]\n$$\nThe self-energy derivative cancels exactly with the second part of the real-space derivative:\n$$\n\\frac{\\partial U}{\\partial \\alpha} = -\\frac{1}{\\sqrt{\\pi}} \\sum_{i,j,\\mathbf{n}} q_i q_j e^{-\\alpha^2 |\\mathbf{r}_{ij} + L\\mathbf{n}|^2} + \\frac{\\pi}{V\\alpha^3} \\sum_{\\mathbf{k}\\neq\\mathbf{0}} |S(\\mathbf{k})|^2 e^{-\\frac{k^2}{4\\alpha^2}}\n$$\nTo show that the remaining sum is zero, we use the Poisson summation formula, which connects a sum over a real-space lattice to a sum over the corresponding reciprocal-space lattice. The relevant identity for a Gaussian function is:\n$$\n\\sum_{\\mathbf{n}\\in\\mathbb{Z}^3} \\exp(-a |\\mathbf{x} + L\\mathbf{n}|^2) = \\frac{1}{L^3} \\left(\\frac{\\pi}{a}\\right)^{3/2} \\sum_{\\mathbf{k}} \\exp\\left(-\\frac{k^2}{4a}\\right) \\exp(i\\mathbf{k}\\cdot\\mathbf{x})\n$$\nWe apply this to the first term, letting $a = \\alpha^2$, $\\mathbf{x} = \\mathbf{r}_{ij} = \\mathbf{r}_i - \\mathbf{r}_j$, and $V=L^3$:\n$$\n\\sum_{\\mathbf{n}} \\exp(-\\alpha^2 |\\mathbf{r}_{ij} + L\\mathbf{n}|^2) = \\frac{\\pi^{3/2}}{V\\alpha^3} \\sum_{\\mathbf{k}} \\exp\\left(-\\frac{k^2}{4\\alpha^2}\\right) \\exp(i\\mathbf{k}\\cdot\\mathbf{r}_{ij})\n$$\nSubstituting this back into the first term of $\\frac{\\partial U}{\\partial \\alpha}$:\n$$\n-\\frac{1}{\\sqrt{\\pi}} \\sum_{i,j} q_i q_j \\left[ \\frac{\\pi^{3/2}}{V\\alpha^3} \\sum_{\\mathbf{k}} e^{-\\frac{k^2}{4\\alpha^2}} e^{i\\mathbf{k}\\cdot(\\mathbf{r}_i - \\mathbf{r}_j)} \\right] = -\\frac{\\pi}{V\\alpha^3} \\sum_{\\mathbf{k}} e^{-\\frac{k^2}{4\\alpha^2}} \\sum_{i,j} q_i q_j e^{i\\mathbf{k}\\cdot\\mathbf{r}_i} e^{-i\\mathbf{k}\\cdot\\mathbf{r}_j}\n$$\nThe double sum over $i,j$ can be factorized:\n$$\n\\sum_{i,j} q_i q_j e^{i\\mathbf{k}\\cdot\\mathbf{r}_i} e^{-i\\mathbf{k}\\cdot\\mathbf{r}_j} = \\left(\\sum_i q_i e^{i\\mathbf{k}\\cdot\\mathbf{r}_i}\\right) \\left(\\sum_j q_j e^{-i\\mathbf{k}\\cdot\\mathbf{r}_j}\\right) = S(-\\mathbf{k}) S(\\mathbf{k}) = |S(\\mathbf{k})|^2\n$$\nSo the first term becomes:\n$$\n-\\frac{\\pi}{V\\alpha^3} \\sum_{\\mathbf{k}} |S(\\mathbf{k})|^2 \\exp\\left(-\\frac{k^2}{4\\alpha^2}\\right)\n$$\nNow, substitute this back into the expression for the total derivative:\n$$\n\\frac{\\partial U}{\\partial \\alpha} = -\\frac{\\pi}{V\\alpha^3} \\sum_{\\mathbf{k}} |S(\\mathbf{k})|^2 e^{-\\frac{k^2}{4\\alpha^2}} + \\frac{\\pi}{V\\alpha^3} \\sum_{\\mathbf{k}\\neq\\mathbf{0}} |S(\\mathbf{k})|^2 e^{-\\frac{k^2}{4\\alpha^2}}\n$$\nThe two sums are identical except that the first sum includes the $\\mathbf{k}=\\mathbf{0}$ term, while the second does not. Their difference is the negative of the $\\mathbf{k}=\\mathbf{0}$ term from the first sum:\n$$\n\\frac{\\partial U}{\\partial \\alpha} = -\\frac{\\pi}{V\\alpha^3} |S(\\mathbf{0})|^2 \\exp(0) = -\\frac{\\pi}{V\\alpha^3} |S(\\mathbf{0})|^2\n$$\nThe structure factor at $\\mathbf{k}=\\mathbf{0}$ is defined as $S(\\mathbf{0}) = \\sum_{j=1}^{N} q_j \\exp(-i\\mathbf{0}\\cdot\\mathbf{r}_j) = \\sum_{j=1}^{N} q_j$.\nThe problem states that the system has overall charge neutrality, so $\\sum_{j=1}^{N} q_j = 0$.\nThis implies that $S(\\mathbf{0}) = 0$.\nTherefore, the total derivative is:\n$$\n\\frac{\\partial U(\\alpha)}{\\partial \\alpha} = 0\n$$\nThis demonstrates that the total Ewald energy is independent of the arbitrary splitting parameter $\\alpha$, which is a fundamental property of the method. The final answer is the numerical value of this derivative.",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "While the total Ewald energy is independent of the splitting parameter $\\alpha$, the computational cost is highly sensitive to its value. This practice explores the fundamental trade-off at the heart of the method: how $\\alpha$ shifts the computational burden between the real-space sum (which is fast for large $\\alpha$) and the reciprocal-space sum (which is fast for small $\\alpha$). Understanding this balance is the first step toward optimizing any Ewald-based simulation for maximum efficiency .",
            "id": "3851319",
            "problem": "Consider a cubic periodic simulation cell of side length $L$ containing $N$ point charges with number density $\\rho = N/L^3$. In Ewald summation, the Coulomb kernel is split by a Gaussian screening of width parameter $\\alpha$ into a short-range real-space term and a long-range reciprocal-space term. The real-space sum is truncated at a cutoff $r_c$, and the reciprocal-space sum is truncated at a maximum wavevector magnitude $k_{\\max}$ or evaluated via Particle Mesh Ewald (PME) on a mesh whose effective resolution corresponds to $k_{\\max}$. Assume a homogeneous distribution for error estimates and consider a target root-mean-square (RMS) force error tolerance $\\varepsilon$.\n\nStarting from the facts that (i) the complementary error function $\\operatorname{erfc}(x)$ decays like a Gaussian for large $x$, and (ii) the reciprocal-space Ewald kernel is damped by a Gaussian factor in $k$-space, reason from first principles to establish how $\\alpha$ controls the spatial decay of the real-space kernel and the spectral decay of the reciprocal-space kernel. Use these decays to bound the truncation errors and to deduce how $r_c$ and $k_{\\max}$ must scale with $\\alpha$ in order to maintain a fixed tolerance $\\varepsilon$. Then use these relationships to infer the qualitative scaling of computational cost with $\\alpha$ for the two parts, under the following standard cost models: real-space cost per particle scales as $\\mathcal{O}(\\rho r_c^3)$ due to neighbor counts within $r_c$, and reciprocal-space cost scales as the number of $k$-modes with $|\\mathbf{k}| \\le k_{\\max}$, i.e., $\\mathcal{O}(k_{\\max}^3)$ for classical Ewald or monotonically with that count for PME.\n\nWhich of the following statements are correct in light of your derivation? Select all that apply.\n\nA. For a fixed error tolerance $\\varepsilon$ and fixed density $\\rho$, decreasing $\\alpha$ forces $r_c$ to grow roughly like $\\alpha^{-1}$ to keep the real-space truncation error small, so the real-space computational cost scales up as $\\alpha^{-3}$, while the reciprocal-space mode count needed scales down as $\\alpha^{3}$ because the Gaussian damping in $k$-space becomes stronger.\n\nB. For fixed $r_c$, the real-space computational cost is independent of $\\alpha$, but the real-space truncation error increases as $\\alpha$ decreases, with a leading dependence that is approximately $\\propto \\exp(-\\alpha^2 r_c^2)$, implying that choosing $\\alpha$ too small is inadmissible for a given tolerance even if the reciprocal-space grid is refined arbitrarily.\n\nC. For fixed $r_c$, decreasing $\\alpha$ increases the reciprocal-space workload because the Gaussian damping factor in $k$-space decays more slowly, so more $k$-modes are required.\n\nD. At a fixed accuracy target $\\varepsilon$, there exists an optimal $\\alpha$ that balances real-space and reciprocal-space costs, and as $r_c$ is decreased (e.g., due to memory or neighbor-list constraints), the optimal $\\alpha$ shifts to larger values, shifting more work to reciprocal space.",
            "solution": "The problem asks for an analysis of the Ewald summation method, specifically focusing on how the Ewald parameter $\\alpha$ influences the computational cost of the real-space and reciprocal-space parts of the calculation for a fixed error tolerance $\\varepsilon$.\n\n**Derivation from First Principles**\n\nThe Ewald summation method splits the Coulomb potential $V(r) = 1/r$ for each pair of charges into a short-range and a long-range component using the identity $1 = \\operatorname{erfc}(\\alpha r) + \\operatorname{erf}(\\alpha r)$. The parameter $\\alpha$ controls the width of the screening Gaussian charge distribution used for this split.\n\nThe potential energy is split into:\n$$ V(r) = \\underbrace{\\frac{\\operatorname{erfc}(\\alpha r)}{r}}_{\\text{Real space}} + \\underbrace{\\frac{\\operatorname{erf}(\\alpha r)}{r}}_{\\text{Reciprocal space}} $$\n\n**1. Real-Space Analysis**\nThe real-space term is $\\phi_{rs}(r) = q_i q_j \\frac{\\operatorname{erfc}(\\alpha r)}{r}$. This term decays rapidly due to the complementary error function. The force is the negative gradient of the potential. The magnitude of the force contribution is $|F_{rs}(r)| = | - \\frac{d\\phi_{rs}}{dr} | = |q_i q_j| \\left| \\frac{\\operatorname{erfc}(\\alpha r)}{r^2} + \\frac{2\\alpha}{\\sqrt{\\pi}} \\frac{e^{-\\alpha^2 r^2}}{r} \\right|$.\n\nThe real-space sum is truncated at a cutoff $r_c$. The error introduced by this truncation is dominated by the magnitude of the force term at $r=r_c$. For a sufficiently large product $\\alpha r_c$, the term involving the Gaussian $e^{-\\alpha^2 r_c^2}$ dominates the decay. A standard result for the RMS force error, $\\varepsilon_{rs}$, is that it scales approximately as:\n$$ \\varepsilon_{rs} \\propto \\frac{e^{-\\alpha^2 r_c^2}}{r_c} $$\nTo maintain a constant error $\\varepsilon$, the exponential term must be kept roughly constant. This means we must have $\\alpha^2 r_c^2 \\approx C_1$, where $C_1$ is a constant determined by the target tolerance $\\varepsilon$. This leads to the scaling relationship:\n$$ r_c \\propto \\frac{1}{\\alpha} $$\nThe real-space computational cost is given as $\\mathcal{O}(\\rho r_c^3)$. Substituting the scaling for $r_c$:\n$$ \\text{Cost}_{rs} \\propto \\rho r_c^3 \\propto \\rho \\left(\\frac{1}{\\alpha}\\right)^3 = \\rho \\alpha^{-3} $$\nThus, as $\\alpha$ decreases, $r_c$ must increase, and the real-space cost grows as $\\alpha^{-3}$.\n\n**2. Reciprocal-Space Analysis**\nThe long-range term, $\\frac{\\operatorname{erf}(\\alpha r)}{r}$, is slowly varying and is handled by Fourier transformation. The Fourier transform of the associated screening charge distribution (a Gaussian of width controlled by $\\alpha$) is also a Gaussian. The coefficients of the reciprocal-space sum are modulated by a factor of $\\exp(-k^2 / (4\\alpha^2))$, where $k$ is the magnitude of the wavevector $\\mathbf{k}$.\n\nThe truncation of the reciprocal-space sum at $k_{\\max}$ introduces an error, $\\varepsilon_{recip}$. The error in the force scales approximately as:\n$$ \\varepsilon_{recip} \\propto k_{\\max}^c \\exp\\left(-\\frac{k_{\\max}^2}{4\\alpha^2}\\right) $$\nwhere $c$ is a small positive power. To maintain a constant error, the exponential term must again be kept roughly constant. This requires $k_{\\max}^2 / (4\\alpha^2) \\approx C_2$, leading to the scaling relationship:\n$$ k_{\\max} \\propto \\alpha $$\nThe reciprocal-space cost is given as $\\mathcal{O}(k_{\\max}^3)$, representing the number of wavevectors to sum over. Substituting the scaling for $k_{\\max}$:\n$$ \\text{Cost}_{recip} \\propto k_{\\max}^3 \\propto \\alpha^3 $$\nThus, as $\\alpha$ decreases, $k_{\\max}$ can be decreased, and the reciprocal-space cost shrinks as $\\alpha^3$.\n\n**3. Total Cost and Optimization**\nThe total computational cost is the sum of the real-space and reciprocal-space costs:\n$$ \\text{Cost}_{total} \\approx C_{rs} \\alpha^{-3} + C_{recip} \\alpha^3 $$\nThis function has a minimum with respect to $\\alpha$, which occurs when the two terms are of comparable magnitude, i.e., when the computational load is balanced between real and reciprocal space. This defines an optimal $\\alpha$ for a given problem and desired accuracy.\n\n**Evaluation of Options**\n\n**A. For a fixed error tolerance $\\varepsilon$ and fixed density $\\rho$, decreasing $\\alpha$ forces $r_c$ to grow roughly like $\\alpha^{-1}$ to keep the real-space truncation error small, so the real-space computational cost scales up as $\\alpha^{-3}$, while the reciprocal-space mode count needed scales down as $\\alpha^{3}$ because the Gaussian damping in $k$-space becomes stronger.**\n- \"decreasing $\\alpha$ forces $r_c$ to grow roughly like $\\alpha^{-1}$\": Our derivation shows $r_c \\propto 1/\\alpha$. This is correct.\n- \"so the real-space computational cost scales up as $\\alpha^{-3}$\": Our derivation shows $\\text{Cost}_{rs} \\propto \\alpha^{-3}$. This is correct.\n- \"the reciprocal-space mode count needed scales down as $\\alpha^{3}$\": The mode count is proportional to $k_{\\max}^3$. Our derivation shows $k_{\\max} \\propto \\alpha$, so the mode count scales as $\\alpha^3$. As $\\alpha$ decreases, this count decreases. This is correct.\n- \"because the Gaussian damping in $k$-space becomes stronger\": The damping factor is $\\exp(-k^2/(4\\alpha^2))$. As $\\alpha$ decreases, the denominator $4\\alpha^2$ becomes smaller, so the negative exponent grows more rapidly in magnitude with $k$. This corresponds to a stronger (faster) decay. This reasoning is correct.\nThe statement is entirely accurate.\n**Verdict: Correct**\n\n**B. For fixed $r_c$, the real-space computational cost is independent of $\\alpha$, but the real-space truncation error increases as $\\alpha$ decreases, with a leading dependence that is approximately $\\propto \\exp(-\\alpha^2 r_c^2)$, implying that choosing $\\alpha$ too small is inadmissible for a given tolerance even if the reciprocal-space grid is refined arbitrarily.**\n- \"For fixed $r_c$, the real-space computational cost is independent of $\\alpha$\": The cost model is $\\mathcal{O}(\\rho r_c^3)$. If $r_c$ is fixed, the cost is constant. This is correct.\n- \"the real-space truncation error increases as $\\alpha$ decreases\": The error scales with terms like $\\exp(-\\alpha^2 r_c^2)$. For fixed $r_c$, as $\\alpha$ decreases, $\\alpha^2 r_c^2$ decreases, so $-\\alpha^2 r_c^2$ increases, and the exponential term grows. The error increases. This is correct.\n- \"with a leading dependence that is approximately $\\propto \\exp(-\\alpha^2 r_c^2)$\": This is the dominant factor in the error formula, as established in our initial analysis. Correct.\n- \"implying that choosing $\\alpha$ too small is inadmissible for a given tolerance even if the reciprocal-space grid is refined arbitrarily\": If $\\alpha$ is too small for a given $r_c$, the real-space error $\\varepsilon_{rs}$ may exceed the total allowed error $\\varepsilon$. Refining the reciprocal-space calculation only reduces $\\varepsilon_{recip}$; it cannot compensate for an already too-large $\\varepsilon_{rs}$. Thus, the choice of $\\alpha$ is constrained by $r_c$ and $\\varepsilon$. This is a key practical consideration in using Ewald methods. Correct.\nThe statement is entirely accurate.\n**Verdict: Correct**\n\n**C. For fixed $r_c$, decreasing $\\alpha$ increases the reciprocal-space workload because the Gaussian damping factor in $k$-space decays more slowly, so more $k$-modes are required.**\n- This statement analyzes the consequence of decreasing $\\alpha$. As established in our reciprocal-space analysis, decreasing $\\alpha$ leads to a faster decay of the reciprocal-space terms (the damping factor $\\exp(-k^2 / (4\\alpha^2))$ decays more rapidly).\n- This means fewer $k$-modes are required to achieve a given accuracy, so $k_{\\max}$ can be smaller. A smaller $k_{\\max}$ leads to a a *decreased*, not increased, reciprocal-space workload.\n- The statement claims the workload increases because the damping factor \"decays more slowly\", which is also factually incorrect. Decreasing $\\alpha$ makes the decay faster.\nThe statement is incorrect in both its conclusion and its reasoning.\n**Verdict: Incorrect**\n\n**D. At a fixed accuracy target $\\varepsilon$, there exists an optimal $\\alpha$ that balances real-space and reciprocal-space costs, and as $r_c$ is decreased (e.g., due to memory or neighbor-list constraints), the optimal $\\alpha$ shifts to larger values, shifting more work to reciprocal space.**\n- \"there exists an optimal $\\alpha$ that balances real-space and reciprocal-space costs\": Our analysis of the total cost function $\\text{Cost}_{total} \\approx C_{rs} \\alpha^{-3} + C_{recip} \\alpha^3$ shows that a minimum exists, typically where the two costs are balanced. Correct.\n- \"as $r_c$ is decreased ... the optimal $\\alpha$ shifts to larger values\": If $r_c$ is constrained to be smaller, we must ensure the real-space error $\\varepsilon_{rs} \\propto \\exp(-\\alpha^2 r_c^2)$ remains small. With a smaller $r_c$, the only way to keep the argument $\\alpha^2 r_c^2$ large is to increase $\\alpha$. So, a smaller $r_c$ forces a larger $\\alpha$. Correct.\n- \"shifting more work to reciprocal space\": A larger $\\alpha$ makes the real-space interaction $\\operatorname{erfc}(\\alpha r)/r$ more short-ranged, justifying the smaller $r_c$ and reducing the real-space cost. Conversely, a larger $\\alpha$ makes the reciprocal-space components decay more slowly in $k$-space (since $\\exp(-k^2/(4\\alpha^2))$ decays more slowly as $\\alpha$ increases), requiring a larger $k_{\\max}$ and thus a higher reciprocal-space cost ($\\propto \\alpha^3$). The balance of work is shifted from real space to reciprocal space. Correct.\nThe statement is entirely accurate.\n**Verdict: Correct**",
            "answer": "$$\\boxed{ABD}$$"
        },
        {
            "introduction": "Moving from the general Ewald framework to its most common implementation, the Particle Mesh Ewald (PME) method, introduces new parameters to optimize: the grid spacing $h$ and the B-spline interpolation order $p$. This exercise delves into the practical trade-off between the cost of the Fast Fourier Transform, which depends on the grid size, and the cost of spreading charges to the grid, which depends on the spline order. Mastering this balance is essential for configuring PME calculations to achieve a desired accuracy at the lowest possible computational cost .",
            "id": "3851278",
            "problem": "A solvated biomolecular system of size $N$ interacts via long-range Coulomb forces that are computed with the Particle Mesh Ewald (PME) method, which is a grid-based realization of Ewald summation. In PME, the reciprocal-space contribution is obtained by assigning charges to a mesh using $p$-th order cardinal $B$-splines, performing a three-dimensional Fast Fourier Transform (FFT), multiplying by the lattice Greenâ€™s function, and interpolating the field back to particle positions. Let the simulation cell have linear dimension $L$ and a uniform mesh with spacing $h$, so that the number of grid points is $M \\approx (L/h)^3$. The Ewald splitting parameter is $\\kappa$, and the real-space contribution is truncated at a cutoff $r_c$.\n\nFrom Fourier analysis of charge assignment, the Fourier transform of a $p$-th order cardinal $B$-spline has a $\\mathrm{sinc}$ envelope, so that the amplitude of aliasing from unresolved lattice images is suppressed by a factor that decays with increasing $p$. For sufficiently smooth target fields, this mesh-interpolation error in the forces decreases rapidly with $p$ and, for small $h$, behaves as a high-order power of $h$ determined by $p$. The dominant PME computational costs are: charge spreading and gathering, which scale with the particle count and the stencil size, and the $3$-dimensional FFT, which scales as $M \\log M$.\n\nYou are tasked with achieving a target relative root-mean-square force error $\\varepsilon_{\\text{target}}$ at minimal computational cost by choosing the interpolation order $p$ and grid spacing $h$, while the Ewald parameter $\\kappa$ and cutoff $r_c$ are already tuned so that real-space truncation error is smaller than $\\varepsilon_{\\text{target}}$. Based on first principles of Fourier error suppression by $B$-spline assignment and asymptotic cost scaling of PME components, which guideline for choosing $p$ is most appropriate to achieve the target error with minimal cost?\n\nA. Choose very large interpolation order $p$ (for example, $p \\ge 10$) so that the grid can be made coarse, i.e., take $h$ as large as possible, because the FFT cost $M \\log M$ dominates; increasing $p$ hardly changes the total runtime.\n\nB. Choose the smallest possible interpolation order $p$ (for example, $p=3$) and reduce the grid spacing $h$ until the mesh-interpolation error becomes smaller than $\\varepsilon_{\\text{target}}$, because decreasing $h$ is always cheaper than increasing $p$.\n\nC. Choose a moderate interpolation order $p$ (for example, $p$ in the range $4$ to $6$) and then set the grid spacing $h$ just fine enough that the mesh-interpolation error matches $\\varepsilon_{\\text{target}}$, balancing the per-particle spreading/gathering cost, which grows with the stencil volume of order $p^3$, against the FFT cost, which grows as $(L/h)^3 \\log\\big((L/h)^3\\big)$.\n\nD. Keep the grid spacing $h$ fixed by the box dimension $L$ and let the interpolation order $p$ increase until the mesh-interpolation error is negligible, because the PME error is primarily controlled by $p$ and $h$ is not an effective tuning parameter.",
            "solution": "The problem asks for the most appropriate guideline to choose the interpolation order $p$ and grid spacing $h$ to achieve a target root-mean-square force error $\\varepsilon_{\\text{target}}$ at minimal computational cost, for the Particle Mesh Ewald (PME) method. The real-space error contribution is assumed to be already below the target error.\n\nFirst, we must formalize the objective. The goal is to minimize the total computational cost, $C_{\\text{total}}$, subject to the constraint that the mesh-interpolation error, $\\varepsilon_{\\text{mesh}}$, equals the target error, $\\varepsilon_{\\text{target}}$. We set $\\varepsilon_{\\text{mesh}} = \\varepsilon_{\\text{target}}$ because any smaller error would imply a higher computational cost (e.g., from a finer grid) for no added benefit.\n\nThe total computational cost, $C_{\\text{total}}$, is dominated by two main components as described in the problem statement:\n1.  The charge spreading and gathering cost, $C_{\\text{sg}}$. This part of the calculation involves distributing the partial charge of each of the $N$ particles onto the surrounding grid points and, after the FFT part of the calculation, interpolating the potential or field from the grid back to the particle positions. For a $p$-th order cardinal $B$-spline, the support of the spline function covers $p$ grid intervals. Therefore, the computational stencil for each particle is a cube of side length $p$ grid points, and the cost scales with the number of particles, $N$, and the volume of this stencil, $p^3$.\n    $$C_{\\text{sg}} \\propto N p^3$$\n2.  The Fast Fourier Transform (FFT) cost, $C_{\\text{FFT}}$. The reciprocal-space calculation involves a three-dimensional FFT on a grid with $M$ points. The cost of a 3D FFT scales as $M \\log M$. Given a simulation cell of linear dimension $L$ and a grid spacing of $h$, the number of grid points is $M \\approx (L/h)^3$.\n    $$C_{\\text{FFT}} \\propto M \\log M \\approx \\left(\\frac{L}{h}\\right)^3 \\log\\left(\\left(\\frac{L}{h}\\right)^3\\right)$$\n    This simplifies to $C_{\\text{FFT}} \\propto (L/h)^3 \\log(L/h)$.\n\nThe total cost to be minimized is the sum of these two components:\n$$C_{\\text{total}}(p, h) = k_1 N p^3 + k_2 \\left(\\frac{L}{h}\\right)^3 \\log\\left(\\frac{L}{h}\\right)$$\nwhere $k_1$ and $k_2$ are constants of proportionality.\n\nThe constraint is given by the mesh-interpolation error. For a sufficiently smooth charge distribution and potential, the error in the force calculated using $p$-th order $B$-spline interpolation on a grid with spacing $h$ scales as:\n$$\\varepsilon_{\\text{mesh}}(p, h) \\propto h^p$$\nMore precisely, the error is a function of the product of the Ewald splitting parameter $\\kappa$ and the grid spacing $h$, so $\\varepsilon_{\\text{mesh}}(p, h) \\approx C(p) (\\kappa h)^p$, where $C(p)$ is a coefficient that varies with $p$. To meet the target error, we set:\n$$\\varepsilon_{\\text{target}} = C(p) (\\kappa h)^p$$\nThis equation establishes a relationship between $h$ and $p$ for a fixed target error. We can express $h$ as a function of $p$:\n$$h(p) = \\frac{1}{\\kappa} \\left(\\frac{\\varepsilon_{\\text{target}}}{C(p)}\\right)^{1/p}$$\nThis shows that to maintain a constant error $\\varepsilon_{\\text{target}}$, increasing the interpolation order $p$ allows one to use a coarser grid (larger $h$).\n\nNow we can analyze the cost as a function of $p$ alone by substituting $h(p)$ into the expression for $C_{\\text{total}}$:\n$$C_{\\text{total}}(p) = k_1 N p^3 + k_2 \\left(\\frac{\\kappa L}{\\left(\\frac{\\varepsilon_{\\text{target}}}{C(p)}\\right)^{1/p}}\\right)^3 \\log\\left(\\frac{\\kappa L}{\\left(\\frac{\\varepsilon_{\\text{target}}}{C(p)}\\right)^{1/p}}\\right)$$\nLet's analyze the behavior of the two cost terms as a function of $p$:\n-   The spreading/gathering cost, $C_{\\text{sg}}(p) = k_1 N p^3$, is a monotonically and rapidly increasing function of $p$.\n-   The FFT cost, $C_{\\text{FFT}}(p)$, is a decreasing function of $p$. This is because as $p$ increases, $h(p)$ increases, which means the number of grid points $M \\approx (L/h(p))^3$ decreases, thereby reducing the FFT cost.\n\nThe total cost is the sum of a rapidly increasing function ($C_{\\text{sg}}$) and a decreasing function ($C_{\\text{FFT}}$). The optimal choice of $p$ will be at a value that minimizes this sum. This minimum will not be at the extremes (very small or very large $p$) but at a moderate, intermediate value where the two costs are balanced. If $p$ is too small, $h$ must be very small to meet the accuracy target, making the FFT cost prohibitive. If $p$ is too large, the $p^3$ dependence of the spreading/gathering cost becomes dominant and prohibitive. Therefore, the optimal strategy is to choose a moderate $p$ and then determine the corresponding $h$ that satisfies the error constraint.\n\nNow we evaluate each option.\n\n**A. Choose very large interpolation order $p$ (for example, $p \\ge 10$) so that the grid can be made coarse, i.e., take $h$ as large as possible, because the FFT cost $M \\log M$ dominates; increasing $p$ hardly changes the total runtime.**\nThis option correctly recognizes that a large $p$ allows for a coarse grid, reducing FFT cost. However, it incorrectly claims that increasing $p$ \"hardly changes the total runtime\". It completely neglects the spreading/gathering cost, which scales as $p^3$. For large $p$, this cost becomes very large and can easily dominate the total cost. For instance, increasing $p$ from a moderate value like $5$ to a large value like $10$ would increase this cost component by a factor of $(10/5)^3 = 8$, which is a substantial change. This strategy is therefore suboptimal.\n**Verdict: Incorrect.**\n\n**B. Choose the smallest possible interpolation order $p$ (for example, $p=3$) and reduce the grid spacing $h$ until the mesh-interpolation error becomes smaller than $\\varepsilon_{\\text{target}}$, because decreasing $h$ is always cheaper than increasing $p$.**\nThis option suggests using a minimal $p$, such as $p=3$ (cubic splines). To achieve high accuracy with a low-order spline, one must use a very fine grid (very small $h$). This leads to a very large number of grid points $M \\approx (L/h)^3$, causing the FFT cost, $C_{\\text{FFT}} \\propto M \\log M$, to become extremely large. The assertion that \"decreasing $h$ is always cheaper than increasing $p$\" is fundamentally false. It ignores the significant cost savings in the FFT component that can be achieved by using a slightly larger $p$, which allows for a much coarser grid. This strategy is also suboptimal, especially for high-accuracy calculations.\n**Verdict: Incorrect.**\n\n**C. Choose a moderate interpolation order $p$ (for example, $p$ in the range $4$ to $6$) and then set the grid spacing $h$ just fine enough that the mesh-interpolation error matches $\\varepsilon_{\\text{target}}$, balancing the per-particle spreading/gathering cost, which grows with the stencil volume of order $p^3$, against the FFT cost, which grows as $(L/h)^3 \\log\\big((L/h)^3\\big)$.**\nThis option correctly frames the problem as an optimization task to balance two competing costs. It accurately identifies that the spreading/gathering cost increases with $p$ (as $p^3$), while the FFT cost decreases with $p$ (for a fixed accuracy, since $h$ can be made larger). The logical conclusion is that an optimal, moderate value of $p$ exists that minimizes the sum of these two costs. Once this moderate $p$ is selected, $h$ is determined by the accuracy requirement. The proposed range of $p$ from $4$ to $6$ (quartic to sextic splines) is consistent with what is known from empirical studies and is commonly used in practice for an optimal balance. This statement provides a complete and correct picture of the optimization guideline.\n**Verdict: Correct.**\n\n**D. Keep the grid spacing $h$ fixed by the box dimension $L$ and let the interpolation order $p$ increase until the mesh-interpolation error is negligible, because the PME error is primarily controlled by $p$ and $h$ is not an effective tuning parameter.**\nThis option is flawed because it removes one of the key tuning parameters, $h$, from the optimization process. The grid spacing $h$ is a highly effective parameter for tuning accuracy and cost, as the error scales as $h^p$ and the FFT cost scales as $(1/h)^3$. Fixing $h$ arbitrarily would lead to a suboptimal solution. If the fixed $h$ is too large, an impractically large $p$ would be needed, leading to excessive spreading/gathering costs. If the fixed $h$ is too small, the FFT cost would be unnecessarily high. The premise that \"$h$ is not an effective tuning parameter\" is factually incorrect.\n**Verdict: Incorrect.**\n\nThe analysis confirms that the most appropriate guideline is to recognize the trade-off between the spreading/gathering cost and the FFT cost, leading to the selection of a moderate interpolation order $p$ and a corresponding grid spacing $h$.",
            "answer": "$$\\boxed{C}$$"
        }
    ]
}