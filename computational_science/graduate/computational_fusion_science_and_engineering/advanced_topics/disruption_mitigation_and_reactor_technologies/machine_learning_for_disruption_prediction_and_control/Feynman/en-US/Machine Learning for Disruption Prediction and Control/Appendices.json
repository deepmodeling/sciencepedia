{
    "hands_on_practices": [
        {
            "introduction": "Many machine learning classifiers, such as Support Vector Machines or Gradient Boosted Trees, produce output scores that are not true probabilities. To be useful for risk assessment and control, these scores must be reliably mapped to the actual likelihood of an event. This exercise  introduces the crucial distinction between a model's ability to rank events (discrimination) and the trustworthiness of its probability estimates (calibration), guiding you through the implementation of isotonic regression to transform raw model outputs into well-calibrated probabilities.",
            "id": "4003844",
            "problem": "Consider a binary disruption warning model in magnetic confinement fusion that outputs a real-valued score for each pulse, intended to represent the probability of a disruption. Let the model output be denoted by the random variable $S \\in [0,1]$ and the ground truth disruption indicator be $Y \\in \\{0,1\\}$, where $Y=1$ denotes that a disruption occurred. The following concepts are fundamental.\n\n1. Probability calibration: A model is perfectly calibrated if for all $s \\in [0,1]$, it satisfies the conditional identity $\\mathbb{P}(Y=1 \\mid S=s) = s$. Calibration concerns the absolute probabilistic correctness of the outputs. Equivalently, for any set $A \\subseteq [0,1]$, one requires $\\mathbb{E}[Y \\mid S \\in A] \\approx \\mathbb{E}[S \\mid S \\in A]$, and perfect calibration achieves equality for any measurable $A$. Empirically, calibration is assessed by comparing observed frequencies of $Y=1$ with predicted scores $S$ at comparable $S$ values.\n\n2. Discrimination: Discrimination addresses relative ranking performance, not absolute probabilities. A model discriminates well if, for two pulses with different outcomes, it tends to assign the higher score to the disrupted pulse. This is often quantified by metrics such as the Area Under the Receiver Operating Characteristic (AUROC), defined as $\\mathrm{AUROC} = \\mathbb{P}(S^{+} > S^{-})$ where $S^{+}$ is the score for a disrupted pulse and $S^{-}$ is the score for a non-disrupted pulse. Discrimination does not imply calibration, and calibration does not imply discrimination; they address distinct properties.\n\nTo recalibrate a model, one can fit a nondecreasing function $g:[0,1]\\to[0,1]$ such that the calibrated output $g(S)$ better approximates $\\mathbb{P}(Y=1 \\mid S)$. A principled nonparametric approach is isotonic regression, which solves the weighted least squares problem\n$$\n\\min_{f \\in \\mathcal{F}} \\sum_{i=1}^{n} w_i \\left(f(s_i) - y_i\\right)^2,\\quad \\text{subject to } f \\text{ nondecreasing on } [0,1],\n$$\nwhere $(s_i,y_i)$ are validation pairs, $w_i>0$ are sample weights (e.g., counts for tied scores), and $\\mathcal{F}$ is the set of all nondecreasing functions. The isotonic regression solution is a piecewise constant nondecreasing function that can be constructed by the Pool-Adjacent-Violators Algorithm (PAVA), which merges adjacent score bins whenever the nondecreasing constraint is violated, replacing their fitted values by the weighted average.\n\nAlgorithmic specification for this problem:\n- Group the validation data by identical scores to obtain unique sorted abscissae $\\{x_k\\}_{k=1}^{m}$, weights $\\{W_k\\}_{k=1}^{m}$ (the counts within each group), and group means $\\{\\bar{y}_k\\}_{k=1}^{m}$ where $\\bar{y}_k = \\frac{1}{W_k}\\sum_{i: s_i = x_k} y_i$.\n- Apply the Pool-Adjacent-Violators Algorithm to $(\\bar{y}_k,W_k)$ in ascending order of $x_k$ to produce blocks. Each block $B$ is a contiguous index set with a single fitted value $v_B$ equal to the weighted mean across the block and total weight $W_B=\\sum_{k\\in B} W_k$. Let $r_B$ denote the maximum $x_k$ inside block $B$ (the right edge of the block along the score axis). The sequence $\\{v_B\\}$ is nondecreasing and the sequence $\\{r_B\\}$ is strictly increasing.\n- Define the calibrated step function $g$ by right-continuous steps: for any $t \\in \\mathbb{R}$, find the smallest block $B$ such that $t \\le r_B$, and set $g(t) = v_B$. If $t < \\min_k x_k$, use the first block’s value; if $t > \\max_k x_k$, use the last block’s value. All outputs must lie in $[0,1]$.\n\nYou must implement isotonic regression using the Pool-Adjacent-Violators Algorithm from first principles, without external calibration libraries. Then, for each test case below, fit $g$ on the provided validation set and compute the recalibrated probabilities $g(t_j)$ for the specified test scores.\n\nTest suite:\n- Case A (general monotonic): Validation $(s_i,y_i)$ with $s_i = [0.02, 0.10, 0.18, 0.26, 0.34, 0.42, 0.50, 0.58, 0.66, 0.74, 0.82, 0.90]$ and $y_i = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]$. Test scores $t = [0.15, 0.45, 0.85]$.\n- Case B (constant scores boundary): Validation $s_i = [0.40, 0.40, 0.40, 0.40, 0.40, 0.40, 0.40, 0.40, 0.40, 0.40]$, $y_i = [0,1,0,1,0,1,0,1,0,1]$. Test scores $t = [0.40, 0.10, 0.90]$.\n- Case C (anti-monotonic edge): Validation $s_i = [0.05, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95]$, $y_i = [1,1,1,1,0,0,0,0,0,0]$. Test scores $t = [0.20, 0.60, 0.80]$.\n- Case D (ties, partial violations, out-of-range): Validation $s_i = [0.10, 0.10, 0.20, 0.20, 0.20, 0.40, 0.40, 0.60, 0.70, 0.90]$, $y_i = [0,1,0,0,1,0,1,1,1,1]$. Test scores $t = [0.05, 0.15, 0.30, 0.50, 0.80, 1.00]$.\n\nAngle units are not applicable. No physical units are involved. All probabilities must be reported as decimals in $[0,1]$. Your program must output a single line with the recalibrated probabilities for each case, formatted as a comma-separated list of lists, enclosed in square brackets, with each probability rounded to six decimal places. For example, a valid output format is $[[0.123456,0.654321],[0.500000,0.500000]]$ for two cases. In this problem, produce four inner lists corresponding to the four cases in the test suite, preserving order Case A through Case D.",
            "solution": "The user-provided problem has been analyzed and is determined to be valid. It is scientifically sound, well-posed, objective, and contains all necessary information to derive a unique solution. The problem requires the implementation of a standard statistical algorithm, isotonic regression via the Pool-Adjacent-Violators Algorithm (PAVA), to a series of well-defined test cases.\n\nThe overarching goal is to recalibrate the output scores $S$ of a binary classification model to better represent true probabilities. This is achieved by finding a non-decreasing function $g: [0,1] \\to [0,1]$ such that for a new score $S$, the recalibrated score $g(S)$ is a more accurate estimate of the conditional probability $\\mathbb{P}(Y=1 \\mid S)$. Isotonic regression provides a non-parametric method to find the best-fitting non-decreasing function in a weighted least-squares sense.\n\nThe solution proceeds in three main stages for each test case: data preprocessing, application of the Pool-Adjacent-Violators Algorithm (PAVA), and computation of calibrated probabilities for test scores using the resulting step function.\n\n### Stage 1: Data Preprocessing\n\nThe raw validation data consists of pairs $(s_i, y_i)$, where $s_i$ is the model score and $y_i \\in \\{0,1\\}$ is the ground-truth outcome. The PAVA algorithm operates on a reduced dataset of unique scores. We first group the data by identical scores.\n\nLet the set of unique scores, sorted in ascending order, be $\\{x_k\\}_{k=1}^{m}$. For each unique score $x_k$, we compute two quantities:\n1.  The weight $W_k$, which is the number of times the score $x_k$ appears in the validation set. Mathematically, $W_k = \\sum_{i=1}^{n} \\mathbf{1}_{s_i=x_k}$, where $\\mathbf{1}$ is the indicator function.\n2.  The group mean $\\bar{y}_k$, which is the observed frequency of disruptions ($Y=1$) for all pulses with score $x_k$. This is the empirical estimate of $\\mathbb{P}(Y=1 \\mid S=x_k)$. Mathematically, $\\bar{y}_k = \\frac{1}{W_k} \\sum_{i: s_i = x_k} y_i$.\n\nThe PAVA is then applied to the sequence of pairs $(\\bar{y}_k, W_k)$ ordered by $x_k$.\n\n### Stage 2: The Pool-Adjacent-Violators Algorithm (PAVA)\n\nPAVA is an iterative algorithm that finds the non-decreasing sequence $\\{\\hat{y}_k\\}_{k=1}^{m}$ that is closest to $\\{\\bar{y}_k\\}_{k=1}^{m}$ in the weighted least-squares sense. It operates by identifying and resolving \"violations\" of the non-decreasing constraint, where $\\bar{y}_k < \\bar{y}_{k-1}$.\n\nThe algorithm can be efficiently implemented by maintaining a set of active \"blocks.\" A block is a contiguous group of the initial points that are assigned a single, common fitted value.\n\n1.  **Initialization:** Start with an empty set of active blocks.\n2.  **Iteration:** Process the points $(\\bar{y}_k, W_k)$ for $k=1, \\dots, m$. For each point $k$, create a new block containing only this point. Its initial value is $\\bar{y}_k$ and its weight is $W_k$.\n3.  **Violation Check and Pooling:** After adding the new block, compare its value with the value of the block immediately preceding it. If the new block's value is smaller, a monotonicity violation has occurred.\n    -   Merge (pool) the new block with the preceding block.\n    -   The value of the newly merged block is the weighted average of the values of its constituent points: $v_{\\text{new}} = \\frac{\\sum_{j \\in \\text{merged block}} \\bar{y}_j W_j}{\\sum_{j \\in \\text{merged block}} W_j}$.\n    -   The weight of the merged block is the sum of the constituent weights.\n4.  **Cascading Merge:** This merged block is now the new last block. Repeat the violation check with the block that now precedes it. Continue pooling backwards until the non-decreasing constraint is satisfied or the beginning of the sequence is reached.\n5.  **Termination:** After all $m$ points are processed, the sequence of block values will be non-decreasing. This sequence of values, expanded back to the original length $m$, forms the calibrated isotonic solution $\\{\\hat{y}_k\\}_{k=1}^m$.\n\n### Stage 3: Constructing and Applying the Calibrated Function $g(t)$\n\nThe output of PAVA is a piecewise-constant, non-decreasing sequence $\\{\\hat{y}_k\\}_{k=1}^{m}$, where $\\hat{y}_k$ is the calibrated probability corresponding to the unique score $x_k$. This defines a step function $g(t)$.\n\nThe problem specifies a right-continuous definition for $g(t)$. The PAVA process partitions the set of unique scores $\\{x_k\\}$ into a set of disjoint, contiguous blocks $\\{B_j\\}_{j=1}^{J}$. Each block $B_j$ has a single fitted value $v_{B_j}$ and a rightmost score $r_{B_j} = \\max_{k \\in B_j} x_k$. The sequences $\\{v_{B_j}\\}$ and $\\{r_{B_j}\\}$ are strictly increasing.\n\nFor any test score $t$, the calibrated probability $g(t)$ is determined as follows:\n1.  Find the smallest block index $j$ such that $t \\le r_{B_j}$. The calibrated value is $g(t) = v_{B_j}$.\n2.  **Extrapolation:**\n    -   If $t$ is less than the minimum score in the validation set ($t < \\min_k x_k$), use the value of the first block, $g(t) = v_{B_1}$.\n    -   If $t$ is greater than the maximum score ($t > \\max_k x_k$), use the value of the last block, $g(t) = v_{B_J}$.\n\nThis procedure is applied to each test case.\n\n- **Case A (general monotonic):** The initial means $\\bar{y}_k$ are already non-decreasing ($[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]$). PAVA makes no changes. The function $g(t)$ is a step function with a jump from $0$ to $1$ between score $0.42$ and $0.50$.\n- **Case B (constant scores boundary):** There is only one unique score $x_1=0.40$ with weight $W_1=10$ and mean $\\bar{y}_1=5/10=0.5$. The PAVA solution is a single block with value $0.5$. Thus, $g(t) = 0.5$ for all $t$.\n- **Case C (anti-monotonic edge):** The initial means $\\bar{y}_k$ are non-increasing ($[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]$). PAVA will pool all points into a single block. The value of this block is the global weighted average, which is $\\frac{4 \\times 1 + 6 \\times 0}{10} = 0.4$. Thus, $g(t) = 0.4$ for all $t$.\n- **Case D (ties, partial violations, out-of-range):**\n    - Preprocessing yields unique scores $x_k = [0.1, 0.2, 0.4, 0.6, 0.7, 0.9]$, weights $W_k = [2, 3, 2, 1, 1, 1]$, and means $\\bar{y}_k = [0.5, 1/3, 0.5, 1, 1, 1]$.\n    - PAVA identifies a violation between $\\bar{y}_1=0.5$ and $\\bar{y}_2=1/3$. They are pooled into a new block with value $(0.5 \\times 2 + 1/3 \\times 3)/(2+3) = 2/5 = 0.4$.\n    - The resulting PAVA sequence of means is $[0.4, 0.4, 0.5, 1, 1, 1]$.\n    - This defines three blocks: Block 1 with value $0.4$ and right edge $0.2$; Block 2 with value $0.5$ and right edge $0.4$; Block 3 with value $1.0$ and right edge $0.9$.\n    - The test scores are mapped accordingly using the right-continuous rule.",
            "answer": "```python\nimport numpy as np\n\ndef pool_adjacent_violators_algorithm(y_means, weights):\n    \"\"\"\n    Implementation of the Pool-Adjacent-Violators Algorithm (PAVA).\n    \n    Args:\n        y_means (np.ndarray): Array of initial mean values, ordered by their corresponding x-coordinates.\n        weights (np.ndarray): Array of weights for each mean value.\n        \n    Returns:\n        np.ndarray: The isotonic (non-decreasing) sequence of fitted values.\n    \"\"\"\n    n = len(y_means)\n    if n == 0:\n        return np.array([])\n    \n    # calibrated_y will store the final solution\n    calibrated_y = np.copy(y_means)\n    \n    # PAVA is often implemented with a list of active blocks\n    active_blocks = []\n    \n    for i in range(n):\n        # Add the current point as a new block\n        active_blocks.append({'value': y_means[i], 'weight': weights[i], 'indices': [i]})\n        \n        # Pool backwards if a violation occurs\n        while len(active_blocks) > 1:\n            last_block = active_blocks[-1]\n            second_last_block = active_blocks[-2]\n            \n            if last_block['value'] < second_last_block['value']:\n                # Violation found, merge the last two blocks\n                merged_weight = last_block['weight'] + second_last_block['weight']\n                merged_value = (last_block['value'] * last_block['weight'] + \n                                second_last_block['value'] * second_last_block['weight']) / merged_weight\n                merged_indices = second_last_block['indices'] + last_block['indices']\n                \n                # Replace the last two blocks with the new merged block\n                active_blocks.pop()\n                active_blocks.pop()\n                active_blocks.append({'value': merged_value, 'weight': merged_weight, 'indices': merged_indices})\n            else:\n                # No violation, stop pooling for this point\n                break\n\n    # Reconstruct the solution vector from the final blocks\n    for block in active_blocks:\n        for i in block['indices']:\n            calibrated_y[i] = block['value']\n            \n    return calibrated_y\n\ndef solve():\n    \"\"\"\n    Main function to solve the isotonic regression problem for all test cases.\n    \"\"\"\n    test_suite = {\n        'A': {\n            's': np.array([0.02, 0.10, 0.18, 0.26, 0.34, 0.42, 0.50, 0.58, 0.66, 0.74, 0.82, 0.90]),\n            'y': np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]),\n            't': np.array([0.15, 0.45, 0.85]),\n        },\n        'B': {\n            's': np.array([0.40, 0.40, 0.40, 0.40, 0.40, 0.40, 0.40, 0.40, 0.40, 0.40]),\n            'y': np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1]),\n            't': np.array([0.40, 0.10, 0.90]),\n        },\n        'C': {\n            's': np.array([0.05, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95]),\n            'y': np.array([1, 1, 1, 1, 0, 0, 0, 0, 0, 0]),\n            't': np.array([0.20, 0.60, 0.80]),\n        },\n        'D': {\n            's': np.array([0.10, 0.10, 0.20, 0.20, 0.20, 0.40, 0.40, 0.60, 0.70, 0.90]),\n            'y': np.array([0, 1, 0, 0, 1, 0, 1, 1, 1, 1]),\n            't': np.array([0.05, 0.15, 0.30, 0.50, 0.80, 1.00]),\n        }\n    }\n    \n    all_results = []\n    \n    for case_id in sorted(test_suite.keys()):\n        case_data = test_suite[case_id]\n        s_val, y_val, t_test = case_data['s'], case_data['y'], case_data['t']\n        \n        # Step 1: Preprocessing - group data by unique scores\n        unique_scores, inverse_indices, counts = np.unique(s_val, return_inverse=True, return_counts=True)\n        \n        # Calculate sums of y for each unique score, then the means\n        y_sums = np.bincount(inverse_indices, weights=y_val)\n        y_means = y_sums / counts\n        \n        # Step 2: Apply PAVA\n        calibrated_y = pool_adjacent_violators_algorithm(y_means, counts)\n        \n        # Step 3: Construct the calibrated function g(t) and apply it\n        # Find the unique block values and their rightmost edges\n        if unique_scores.size > 0:\n            block_vals, first_indices = np.unique(calibrated_y, return_index=True)\n            # The right edge of a block is the x_k value at the end of the block\n            block_indices_ends = np.concatenate((first_indices[1:] - 1, [len(unique_scores) - 1]))\n            block_right_edges = unique_scores[block_indices_ends]\n        else: # Handle empty validation set if needed, though not in test cases\n            block_vals, block_right_edges = np.array([]), np.array([])\n        \n        case_results = []\n        for t_score in t_test:\n            if unique_scores.size == 0:\n                # No data to fit, cannot predict. Could be NaN, 0.5, etc.\n                # Problem does not specify. Assume 0.5 as neutral.\n                recal_prob = 0.5 \n            elif unique_scores.size == 1:\n                # Only one block, constant function\n                recal_prob = block_vals[0]\n            else:\n                # Find the block where t_score belongs using the right-continuous rule\n                # 'side=left' finds the first index i where edge[i] >= t_score\n                block_idx = np.searchsorted(block_right_edges, t_score, side='left')\n                \n                if block_idx >= len(block_vals):\n                    # t_score is greater than the largest edge, extrapolate with last block value\n                    recal_prob = block_vals[-1]\n                else:\n                    recal_prob = block_vals[block_idx]\n            \n            case_results.append(recal_prob)\n\n        all_results.append(case_results)\n\n    # Format the final output string\n    output_str = '['\n    for i, res_list in enumerate(all_results):\n        formatted_list = [f\"{val:.6f}\" for val in res_list]\n        output_str += '[' + ','.join(formatted_list) + ']'\n        if i < len(all_results) - 1:\n            output_str += ','\n    output_str += ']'\n    \n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "With a calibrated model providing a reliable disruption probability, the next critical step is to translate this information into an action. This requires balancing the severe consequences of a missed disruption (a false negative) with the operational costs of an unnecessary mitigation action (a false positive). This practice problem  walks you through the derivation of an optimal decision threshold from the principles of Bayes risk, demonstrating how to systematically incorporate asymmetric costs into your prediction and control logic.",
            "id": "4003916",
            "problem": "In a tokamak disruption prediction and control system, a machine learning classifier outputs a calibrated posterior probability $q(\\mathbf{x}) = P(y=1 \\mid \\mathbf{x})$ that the plasma will disrupt within a short horizon $\\Delta t$, where $y=1$ denotes an imminent disruption and $y=0$ denotes no imminent disruption. The control decision is a binary action: trigger mitigation ($a=1$) or do not trigger mitigation ($a=0$). A false positive (FP) occurs when $a=1$ and $y=0$, incurring cost $C_{\\mathrm{FP}}$, and a false negative (FN) occurs when $a=0$ and $y=1$, incurring cost $C_{\\mathrm{FN}}$. Assume the classifier probabilities are well-calibrated under the data distribution with prior disruption rate $p = P(y=1)$.\n\nStarting from risk minimization principles and the definition of conditional expected loss (Bayes risk) for each $\\mathbf{x}$, derive the optimal threshold $\\tau^{\\star}$ such that the mitigation action is triggered if and only if $q(\\mathbf{x}) \\ge \\tau^{\\star}$ under asymmetric costs $C_{\\mathrm{FP}}$ and $C_{\\mathrm{FN}}$. Then, compute $\\tau^{\\star}$ numerically for a system with prior $p = 0.15$, $C_{\\mathrm{FP}} = 1.7$ (normalized cost units), and $C_{\\mathrm{FN}} = 9.3$ (normalized cost units), assuming the probabilities $q(\\mathbf{x})$ are calibrated posteriors with respect to the same prior $p$. Express the threshold as a decimal with no units and round your answer to four significant figures.",
            "solution": "The posed problem is valid. It is a well-defined application of Bayesian decision theory to a binary classification task with asymmetric costs, which is a standard and scientifically grounded problem in machine learning and its applications. The context of tokamak disruption prediction is a realistic and important area of research in computational fusion science. All necessary parameters are provided, and the objective is clearly stated.\n\nWe begin by establishing the framework for minimizing the expected loss, also known as the Bayes risk. The decision to trigger mitigation ($a=1$) or not ($a=0$) is made for each specific plasma state, represented by the feature vector $\\mathbf{x}$. The optimal decision rule chooses the action that minimizes the conditional expected loss, given the observation $\\mathbf{x}$.\n\nThe classifier provides the posterior probability of an imminent disruption, $q(\\mathbf{x}) = P(y=1 \\mid \\mathbf{x})$, where $y=1$ signifies a disruption and $y=0$ signifies stable operation. The probability of no disruption is therefore $P(y=0 \\mid \\mathbf{x}) = 1 - q(\\mathbf{x})$.\n\nThe problem defines a cost structure for incorrect decisions. Let $L(a, y)$ be the loss incurred for taking action $a$ when the true outcome is $y$. Based on the problem statement:\n\\begin{itemize}\n    \\item Cost of a False Positive (FP): Action is $a=1$ (mitigate), but the true state is $y=0$ (no disruption). The loss is $L(1, 0) = C_{\\mathrm{FP}}$.\n    \\item Cost of a False Negative (FN): Action is $a=0$ (do not mitigate), but the true state is $y=1$ (disruption). The loss is $L(0, 1) = C_{\\mathrm{FN}}$.\n\\end{itemize}\nThe costs for correct decisions are implicitly zero, which is the standard convention for focusing on the loss due to errors.\n\\begin{itemize}\n    \\item Cost of a True Positive (TP): Action is $a=1$, true state is $y=1$. The loss is $L(1, 1) = 0$.\n    \\item Cost of a True Negative (TN): Action is $a=0$, true state is $y=0$. The loss is $L(0, 0) = 0$.\n\\end{itemize}\n\nThe conditional expected loss, or risk, $R(a \\mid \\mathbf{x})$, for each action $a$ is the sum of the possible losses for that action, weighted by their conditional probabilities.\n\nFor the action $a=1$ (trigger mitigation), the conditional expected loss is:\n$$R(a=1 \\mid \\mathbf{x}) = L(1, 1)P(y=1 \\mid \\mathbf{x}) + L(1, 0)P(y=0 \\mid \\mathbf{x})$$\nSubstituting the known values and probabilities:\n$$R(a=1 \\mid \\mathbf{x}) = (0) \\cdot q(\\mathbf{x}) + C_{\\mathrm{FP}} \\cdot (1 - q(\\mathbf{x})) = C_{\\mathrm{FP}}(1 - q(\\mathbf{x}))$$\n\nFor the action $a=0$ (do not trigger mitigation), the conditional expected loss is:\n$$R(a=0 \\mid \\mathbf{x}) = L(0, 1)P(y=1 \\mid \\mathbf{x}) + L(0, 0)P(y=0 \\mid \\mathbf{x})$$\nSubstituting the known values and probabilities:\n$$R(a=0 \\mid \\mathbf{x}) = C_{\\mathrm{FN}} \\cdot q(\\mathbf{x}) + (0) \\cdot (1 - q(\\mathbf{x})) = C_{\\mathrm{FN}}q(\\mathbf{x})$$\n\nThe Bayes-optimal decision rule is to select the action with the minimum conditional expected loss. We should trigger mitigation ($a=1$) if the risk of doing so is less than or equal to the risk of not doing so. The problem specifies the decision rule as triggering for $q(\\mathbf{x}) \\ge \\tau^{\\star}$, which implies assigning the boundary case (equal risk) to action $a=1$. Therefore, we trigger mitigation if:\n$$R(a=1 \\mid \\mathbf{x}) \\le R(a=0 \\mid \\mathbf{x})$$\nSubstituting the expressions for the risks:\n$$C_{\\mathrm{FP}}(1 - q(\\mathbf{x})) \\le C_{\\mathrm{FN}}q(\\mathbf{x})$$\nWe now solve this inequality for $q(\\mathbf{x})$ to find the condition under which mitigation is the optimal action.\n$$C_{\\mathrm{FP}} - C_{\\mathrm{FP}}q(\\mathbf{x}) \\le C_{\\mathrm{FN}}q(\\mathbf{x})$$\n$$C_{\\mathrm{FP}} \\le C_{\\mathrm{FN}}q(\\mathbf{x}) + C_{\\mathrm{FP}}q(\\mathbf{x})$$\n$$C_{\\mathrm{FP}} \\le (C_{\\mathrm{FP}} + C_{\\mathrm{FN}})q(\\mathbf{x})$$\nAssuming $C_{\\mathrm{FP}} + C_{\\mathrm{FN}} > 0$, which is physically required as costs cannot be negative, we can divide without changing the inequality's direction:\n$$q(\\mathbf{x}) \\ge \\frac{C_{\\mathrm{FP}}}{C_{\\mathrm{FP}} + C_{\\mathrm{FN}}}$$\nThis inequality defines the decision rule. The mitigation action $a=1$ is chosen if and only if the classifier's output probability $q(\\mathbf{x})$ exceeds a certain threshold. By comparing this to the desired decision rule form, $q(\\mathbf{x}) \\ge \\tau^{\\star}$, we can directly identify the optimal threshold $\\tau^{\\star}$.\n$$\\tau^{\\star} = \\frac{C_{\\mathrm{FP}}}{C_{\\mathrm{FP}} + C_{\\mathrm{FN}}}$$\nIt is noteworthy that this optimal threshold depends only on the costs of misclassification and is independent of the prior probability of disruption, $p=P(y=1)$. The prior is relevant for training and calibrating the classifier, such that $q(\\mathbf{x})$ is a correct posterior, but it does not enter the threshold calculation itself.\n\nNow, we compute the numerical value for $\\tau^{\\star}$ using the provided costs: $C_{\\mathrm{FP}} = 1.7$ and $C_{\\mathrm{FN}} = 9.3$.\n$$\\tau^{\\star} = \\frac{1.7}{1.7 + 9.3}$$\n$$\\tau^{\\star} = \\frac{1.7}{11.0}$$\n$$\\tau^{\\star} = 0.154545\\overline{45}...$$\nThe problem requires the answer to be rounded to four significant figures.\n$$\\tau^{\\star} \\approx 0.1545$$\nThis is the optimal threshold for triggering the mitigation system. The action is $a=1$ if $q(\\mathbf{x}) \\ge 0.1545$ and $a=0$ otherwise.",
            "answer": "$$\n\\boxed{0.1545}\n$$"
        },
        {
            "introduction": "Framing disruption prediction as a binary classification problem within a fixed time window is a useful simplification, but a more powerful approach models the time-to-disruption directly. This allows for a dynamic forecast of the evolving risk. This exercise  challenges you to move beyond simple classification and implement a time-to-event model using the Cox Proportional Hazards framework, a cornerstone of survival analysis, and to compute its partial log-likelihood for time-varying plasma parameters.",
            "id": "4003911",
            "problem": "You are tasked with formalizing and computing the partial log-likelihood of a hazard-based machine learning model for time-to-disruption prediction in magnetic confinement fusion plasmas (tokamaks). The model is the Cox Proportional Hazards (CPH) model with time-varying covariates. Starting from first principles, you must use the definitions of hazard rate, survival function, and the proportional hazards assumption to derive the partial log-likelihood under right-censoring and then implement it algorithmically for small, piecewise-constant covariate trajectories. All time quantities must be expressed in seconds, and any computed numerical outputs must be dimensionless real numbers.\n\nDefinitions and assumptions to use:\n- The instantaneous hazard rate $h_i(t)$ for shot $i$ is the limit of the conditional event rate as $t$ advances, defined by $h_i(t) = \\lim_{\\Delta t \\to 0^+} \\frac{\\mathbb{P}(T_i \\in [t, t + \\Delta t) \\mid T_i \\ge t)}{\\Delta t}$, where $T_i$ is the event time (disruption time) or censoring time.\n- The survival function $S_i(t)$ satisfies $S_i(t) = \\mathbb{P}(T_i \\ge t)$ and is related to the hazard by $S_i(t) = \\exp\\left(-\\int_0^t h_i(u) \\, du\\right)$.\n- The proportional hazards assumption postulates $h_i(t) = h_0(t) \\, \\exp\\left(\\boldsymbol{\\beta}^\\top \\mathbf{x}_i(t)\\right)$, where $h_0(t)$ is an unspecified baseline hazard common to all shots, $\\boldsymbol{\\beta}$ is a constant coefficient vector, and $\\mathbf{x}_i(t)$ is the shot-specific, possibly time-varying covariate vector.\n- Covariates are piecewise constant in time. For any event (disruption) occurring at time $t_j$, the covariate value used for shot $i$ must be the left-limit $\\mathbf{x}_i(t_j^-)$, i.e., the value just before $t_j$.\n- Right-censoring is present: if a shot is censored at time $c_i$, it contributes to the risk sets up to times $t < c_i$ but does not produce an event contribution.\n\nYour tasks:\n- Derive the partial log-likelihood that depends only on the ordering of observed event times and the covariates via the proportional hazards structure, eliminating $h_0(t)$ by conditioning on the risk sets at each event time.\n- Implement a program that computes the partial log-likelihood for given $\\boldsymbol{\\beta}$ and a dataset of shots, where each shot has a censor or event time and a sequence of piecewise-constant covariate segments. Use the correct left-limit covariate evaluation at each event time and construct the risk set at each event time $t_j$ as all shots $i$ with $t_j \\le T_i$ (event times counted as at risk just prior to $t_j$).\n- You must treat all times in seconds and produce dimensionless partial log-likelihood values.\n\nCovariates to use:\n- The covariate vector $\\mathbf{x}_i(t)$ is three-dimensional, $\\mathbf{x}_i(t) = \\left[x_{1i}(t), x_{2i}(t), x_{3i}(t)\\right]$. Interpret $x_{1i}(t)$ as a normalized plasma current ratio (dimensionless), $x_{2i}(t)$ as a locked-mode radial magnetic perturbation amplitude in Tesla, and $x_{3i}(t)$ as a line-averaged density in units of $10^{19}\\,\\mathrm{m}^{-3}$ (dimensionless with respect to this scale). Units should be consistently treated but the final computed likelihood is dimensionless.\n\nTest suite:\n- Dataset A (mixed events and censoring, time-varying covariates):\n    - Shot $1$: event at $3.0$ s; segments:\n        - $[0.0, 1.0)$ s: $\\mathbf{x} = (1.20, 1.0\\times 10^{-4}, 0.60)$\n        - $[1.0, 2.5)$ s: $\\mathbf{x} = (1.30, 2.0\\times 10^{-4}, 0.58)$\n        - $[2.5, 3.0)$ s: $\\mathbf{x} = (1.40, 4.0\\times 10^{-4}, 0.55)$\n    - Shot $2$: censored at $4.0$ s; segments:\n        - $[0.0, 2.0)$ s: $\\mathbf{x} = (1.00, 1.0\\times 10^{-4}, 0.62)$\n        - $[2.0, 4.0)$ s: $\\mathbf{x} = (1.05, 1.5\\times 10^{-4}, 0.60)$\n    - Shot $3$: event at $2.2$ s; segments:\n        - $[0.0, 1.0)$ s: $\\mathbf{x} = (1.50, 3.0\\times 10^{-4}, 0.50)$\n        - $[1.0, 2.2)$ s: $\\mathbf{x} = (1.60, 5.0\\times 10^{-4}, 0.49)$\n    - Shot $4$: censored at $3.5$ s; segments:\n        - $[0.0, 1.5)$ s: $\\mathbf{x} = (0.90, 1.0\\times 10^{-4}, 0.65)$\n        - $[1.5, 3.5)$ s: $\\mathbf{x} = (1.00, 2.5\\times 10^{-4}, 0.64)$\n- Dataset B (all censored):\n    - Shot $1$: censored at $2.0$ s; segments:\n        - $[0.0, 2.0)$ s: $\\mathbf{x} = (1.10, 2.0\\times 10^{-4}, 0.60)$\n    - Shot $2$: censored at $3.0$ s; segments:\n        - $[0.0, 3.0)$ s: $\\mathbf{x} = (1.30, 1.0\\times 10^{-4}, 0.55)$\n- Dataset C (event at a covariate change boundary; ensure left-limit is used):\n    - Shot $1$: event at $1.0$ s; segments:\n        - $[0.0, 1.0)$ s: $\\mathbf{x} = (1.00, 2.0\\times 10^{-4}, 0.60)$\n        - $[1.0, 2.0)$ s: $\\mathbf{x} = (1.20, 5.0\\times 10^{-4}, 0.60)$\n    - Shot $2$: censored at $2.0$ s; segments:\n        - $[0.0, 2.0)$ s: $\\mathbf{x} = (1.00, 1.0\\times 10^{-4}, 0.60)$\n- Dataset D (single subject at risk at event time):\n    - Shot $1$: event at $2.0$ s; segments:\n        - $[0.0, 2.0)$ s: $\\mathbf{x} = (1.10, 2.0\\times 10^{-4}, 0.60)$\n    - Shot $2$: censored at $1.0$ s; segments:\n        - $[0.0, 1.0)$ s: $\\mathbf{x} = (0.90, 1.0\\times 10^{-4}, 0.70)$\n\nCoefficient vectors to evaluate:\n- Case $1$: $\\boldsymbol{\\beta} = (0.50, 3000.0, -0.50)$\n- Case $2$: $\\boldsymbol{\\beta} = (0.70, 4000.0, -0.90)$\n- Case $3$: $\\boldsymbol{\\beta} = (0.00, 0.00, 0.00)$\n\nComputational requirements:\n- For each dataset and coefficient vector, compute the partial log-likelihood value as a single real number using the correct construction of risk sets and left-limit covariate evaluation.\n- Use natural logarithms.\n- No ties in event times are present in the provided datasets; if an event occurs exactly at a covariate segment boundary $t_b$, use the covariate value from the segment immediately preceding $t_b$ (the left-limit).\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as follows:\n    - Dataset A with Case $1$, Dataset A with Case $2$, Dataset A with Case $3$, Dataset B with Case $1$, Dataset C with Case $2$, Dataset D with Case $1$.\n- Express each result as a float rounded to $6$ decimal places.\n- Example formatting: `[r_1,r_2,r_3,r_4,r_5,r_6]`, where each $r_k$ is a dimensionless real number.",
            "solution": "The problem requires the derivation and implementation of the partial log-likelihood for a Cox Proportional Hazards (CPH) model with time-varying covariates. The derivation begins from the fundamental definitions of hazard rates and survival functions and culminates in an expression that is independent of the unspecified baseline hazard.\n\nLet there be $N$ independent shots (subjects). For each shot $i \\in \\{1, \\dots, N\\}$, the observed data is a triplet $(T_i, \\delta_i, \\mathbf{x}_i(\\cdot))$, where $T_i$ is the observed time (either an event time or a censoring time), $\\delta_i$ is an event indicator such that $\\delta_i=1$ for an event (disruption) and $\\delta_i=0$ for right-censoring, and $\\mathbf{x}_i(t)$ is the vector of covariates for shot $i$ at time $t$.\n\nThe instantaneous hazard rate for shot $i$ at time $t$, $h_i(t)$, is defined as the instantaneous rate of event occurrence at time $t$, given survival up to time $t$:\n$$h_i(t) = \\lim_{\\Delta t \\to 0^+} \\frac{\\mathbb{P}(T_i \\in [t, t + \\Delta t) \\mid T_i \\ge t)}{\\Delta t}$$\nThe survival function, $S_i(t) = \\mathbb{P}(T_i \\ge t)$, is related to the hazard rate via $S_i(t) = \\exp\\left(-\\int_0^t h_i(u) \\, du\\right)$. The probability density function of an event at time $t$ is $f_i(t) = h_i(t)S_i(t)$.\n\nThe full likelihood for the observed data across all $N$ shots is the product of the probability densities for the shots that experience an event and the survival probabilities for the shots that are censored:\n$$ L = \\prod_{i=1}^N [f_i(T_i)]^{\\delta_i} [S_i(T_i)]^{1-\\delta_i} = \\prod_{i=1}^N [h_i(T_i)S_i(T_i)]^{\\delta_i} [S_i(T_i)]^{1-\\delta_i} = \\prod_{i=1}^N [h_i(T_i)]^{\\delta_i} S_i(T_i) $$\n\nThe Cox Proportional Hazards model assumes that the hazard rate for shot $i$ can be factored into a common baseline hazard function $h_0(t)$ and a term dependent on the covariates:\n$$ h_i(t) = h_0(t) \\exp\\left(\\boldsymbol{\\beta}^\\top \\mathbf{x}_i(t)\\right) $$\nwhere $\\boldsymbol{\\beta}$ is a vector of coefficients. Substituting this into the full likelihood expression results in a formula that depends on the unknown function $h_0(t)$.\n\nTo eliminate this dependency, Sir David Cox proposed the use of a partial likelihood. This is constructed by considering the set of distinct event times, ordered as $t_{(1)} < t_{(2)} < \\dots < t_{(D)}$, where $D$ is the total number of observed events. At each event time $t_{(j)}$, we define the risk set $\\mathcal{R}(t_{(j)})$ as the set of all shots $i$ that are still at risk of an event, i.e., their event or censoring time $T_i$ is at or after $t_{(j)}$: $\\mathcal{R}(t_{(j)}) = \\{i \\mid T_i \\ge t_{(j)}\\}$.\n\nLet $i_j$ be the index of the shot that uniquely fails at time $t_{(j)}$. The partial likelihood considers the conditional probability that it is specifically shot $i_j$ that fails at $t_{(j)}$, given that exactly one failure occurs at this time among all shots in the risk set $\\mathcal{R}(t_{(j)})$. In a small time interval $[t_{(j)}, t_{(j)}+\\Delta t)$, the probability of shot $k$ failing is approximately $h_k(t_{(j)}) \\Delta t$. The conditional probability is then:\n$$ \\mathbb{P}(\\text{shot } i_j \\text{ fails} \\mid \\text{one failure in } \\mathcal{R}(t_{(j)})) = \\frac{h_{i_j}(t_{(j)}) \\Delta t}{\\sum_{k \\in \\mathcal{R}(t_{(j)})} h_k(t_{(j)}) \\Delta t} = \\frac{h_{i_j}(t_{(j)})}{\\sum_{k \\in \\mathcal{R}(t_{(j)})} h_k(t_{(j)})} $$\n\nSubstituting the proportional hazards assumption into this expression:\n$$ \\frac{h_0(t_{(j)}) \\exp\\left(\\boldsymbol{\\beta}^\\top \\mathbf{x}_{i_j}(t_{(j)})\\right)}{\\sum_{k \\in \\mathcal{R}(t_{(j)})} h_0(t_{(j)}) \\exp\\left(\\boldsymbol{\\beta}^\\top \\mathbf{x}_k(t_{(j)})\\right)} = \\frac{\\exp\\left(\\boldsymbol{\\beta}^\\top \\mathbf{x}_{i_j}(t_{(j)})\\right)}{\\sum_{k \\in \\mathcal{R}(t_{(j)})} \\exp\\left(\\boldsymbol{\\beta}^\\top \\mathbf{x}_k(t_{(j)})\\right)} $$\nThe baseline hazard $h_0(t_{(j)})$ cancels. For time-varying covariates, the value used is the left-limit $\\mathbf{x}_k(t_{(j)}^-)$, i.e., the value just before the event.\n\nThe total partial likelihood $L_p(\\boldsymbol{\\beta})$ is the product of these conditional probabilities over all $D$ event times (assuming no ties in event times):\n$$ L_p(\\boldsymbol{\\beta}) = \\prod_{j=1}^{D} \\frac{\\exp\\left(\\boldsymbol{\\beta}^\\top \\mathbf{x}_{i_j}(t_{(j)}^-)\\right)}{\\sum_{k \\in \\mathcal{R}(t_{(j)})} \\exp\\left(\\boldsymbol{\\beta}^\\top \\mathbf{x}_k(t_{(j)}^-)\\right)} $$\n\nThe partial log-likelihood $\\ell_p(\\boldsymbol{\\beta})$ is the natural logarithm of $L_p(\\boldsymbol{\\beta})$:\n$$ \\ell_p(\\boldsymbol{\\beta}) = \\ln\\left(L_p(\\boldsymbol{\\beta})\\right) = \\sum_{j=1}^{D} \\left( \\boldsymbol{\\beta}^\\top \\mathbf{x}_{i_j}(t_{(j)}^-) - \\ln\\left[\\sum_{k \\in \\mathcal{R}(t_{(j)})} \\exp\\left(\\boldsymbol{\\beta}^\\top \\mathbf{x}_k(t_{(j)}^-)\\right)\\right] \\right) $$\nIf no events occur ($D=0$), the sum is empty, and the product is over an empty set, which is $1$. Thus, $\\ell_p(\\boldsymbol{\\beta}) = \\ln(1) = 0$.\n\nThe algorithm to compute this value is as follows:\n1.  Represent the data for each shot $i$ as $(T_i, \\delta_i, \\text{segments}_i)$, where $\\text{segments}_i$ is a list of tuples `(start_time, covariate_vector)`.\n2.  Extract all shots with $\\delta_i=1$. Let their event times be $\\{t_{(1)}, t_{(2)}, \\dots, t_{(D)}\\}$, sorted in increasing order. Let the corresponding shot indices be $\\{i_1, i_2, \\dots, i_D\\}$.\n3.  If $D=0$, the partial log-likelihood is $0.0$.\n4.  Initialize a total log-likelihood variable, $\\ell_{total}$, to $0.0$.\n5.  Iterate through each event $j$ from $1$ to $D$, with event time $t_{(j)}$ and subject index $i_j$:\n    a. Construct the risk set $\\mathcal{R}(t_{(j)}) = \\{k \\mid T_k \\ge t_{(j)}\\}$.\n    b. For each shot $k \\in \\mathcal{R}(t_{(j)})$, determine its covariate vector at the left-limit time $t_{(j)}^-$. For piecewise-constant covariates defined by start times, this corresponds to finding the segment for shot $k$ with the largest start time $t_s$ such that $t_s < t_{(j)}$.\n    c. Calculate the risk score $\\eta_k = \\boldsymbol{\\beta}^\\top \\mathbf{x}_k(t_{(j)}^-)$ for each shot $k \\in \\mathcal{R}(t_{(j)})$.\n    d. Compute the sum of the exponentials of the risk scores over the risk set: $S_j = \\sum_{k \\in \\mathcal{R}(t_{(j)})} \\exp(\\eta_k)$.\n    e. Identify the risk score of the shot that had the event, $\\eta_{i_j}$.\n    f. The contribution to the log-likelihood from this event is $\\eta_{i_j} - \\ln(S_j)$.\n    g. Add this contribution to $\\ell_{total}$.\n6.  The final value $\\ell_{total}$ is the partial log-likelihood. All calculations use floating-point arithmetic with the natural logarithm. The problem-specified units for covariates and $\\boldsymbol{\\beta}$ ensure that the term $\\boldsymbol{\\beta}^\\top \\mathbf{x}$ is dimensionless, making the final log-likelihood dimensionless.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the partial log-likelihood for the Cox Proportional Hazards model\n    for several test cases, as specified in the problem statement.\n    \"\"\"\n\n    # Data structure for shots:\n    # Each shot is a tuple: (shot_id, T, delta, segments)\n    # T: Observed time (event or censor)\n    # delta: Event indicator (1 for event, 0 for censor)\n    # segments: List of tuples (start_time, covariate_vector)\n\n    datasets = {\n        'A': [\n            (1, 3.0, 1, [(0.0, [1.20, 1.0e-4, 0.60]), (1.0, [1.30, 2.0e-4, 0.58]), (2.5, [1.40, 4.0e-4, 0.55])]),\n            (2, 4.0, 0, [(0.0, [1.00, 1.0e-4, 0.62]), (2.0, [1.05, 1.5e-4, 0.60])]),\n            (3, 2.2, 1, [(0.0, [1.50, 3.0e-4, 0.50]), (1.0, [1.60, 5.0e-4, 0.49])]),\n            (4, 3.5, 0, [(0.0, [0.90, 1.0e-4, 0.65]), (1.5, [1.00, 2.5e-4, 0.64])])\n        ],\n        'B': [\n            (1, 2.0, 0, [(0.0, [1.10, 2.0e-4, 0.60])]),\n            (2, 3.0, 0, [(0.0, [1.30, 1.0e-4, 0.55])])\n        ],\n        'C': [\n            (1, 1.0, 1, [(0.0, [1.00, 2.0e-4, 0.60]), (1.0, [1.20, 5.0e-4, 0.60])]),\n            (2, 2.0, 0, [(0.0, [1.00, 1.0e-4, 0.60])])\n        ],\n        'D': [\n            (1, 2.0, 1, [(0.0, [1.10, 2.0e-4, 0.60])]),\n            (2, 1.0, 0, [(0.0, [0.90, 1.0e-4, 0.70])])\n        ]\n    }\n\n    betas = {\n        1: np.array([0.50, 3000.0, -0.50]),\n        2: np.array([0.70, 4000.0, -0.90]),\n        3: np.array([0.00, 0.00, 0.00])\n    }\n    \n    test_cases = [\n        ('A', 1), ('A', 2), ('A', 3),\n        ('B', 1),\n        ('C', 2),\n        ('D', 1)\n    ]\n\n    def get_covariates_at_left_limit(segments, time):\n        \"\"\"\n        Finds the covariate vector for a shot at a time's left-limit.\n        This means finding the value from the time interval active just before 'time'.\n        \"\"\"\n        applicable_segments = []\n        for start_time, x_vec in segments:\n            if start_time < time:\n                 applicable_segments.append((start_time, x_vec))\n        \n        if not applicable_segments:\n            # This case might occur if event time is at or before the first segment start time.\n            # Assuming first segment start_time is <= event time.\n            # If t_event = 0, no segments will be found. The problem data does not have this.\n            # Let's check segments at exactly time=start_time.\n            for start_time, x_vec in segments:\n                if start_time == time:\n                    # this is an edge case, but we need the segment *before* this one.\n                    # the logic `start_time < time` handles this correctly as per problem spec\n                    # \"use the covariate value from the segment immediately preceding tb\"\n                    pass\n            # if we get here, it implies an event time <= the minimal start time.\n            # The problem data start at 0, so this shouldn't be an issue.\n            # For a more robust implementation, one may need domain knowledge on how\n            # to handle t=0 events. For this problem, we rely on the data given.\n            # Based on test case C, event at 1.0 with segments starting at 0.0 and 1.0,\n            # must use the 0.0 segment, which `start_time < time` does.\n            raise ValueError(f\"No covariate history found for time {time}\")\n\n        # Find the segment with the latest start time before the event time\n        best_start_time = -1.0\n        covariate_vector = None\n        for start_time, x_vec in applicable_segments:\n            if start_time > best_start_time:\n                best_start_time = start_time\n                covariate_vector = x_vec\n        \n        return np.array(covariate_vector)\n\n\n    def calculate_log_likelihood(dataset, beta):\n        \"\"\"\n        Calculates the partial log-likelihood for a given dataset and beta vector.\n        \"\"\"\n        \n        # Identify event times and the corresponding shots\n        events = []\n        for shot_id, T, delta, segments in dataset:\n            if delta == 1:\n                events.append({'time': T, 'shot_id': shot_id})\n        \n        # If no events, log-likelihood is 0\n        if not events:\n            return 0.0\n\n        # Sort events by time\n        events.sort(key=lambda e: e['time'])\n        \n        total_log_likelihood = 0.0\n\n        for event in events:\n            event_time = event['time']\n            event_shot_id = event['shot_id']\n\n            # Construct risk set\n            risk_set_shots = []\n            for shot in dataset:\n                if shot[1] >= event_time:\n                    risk_set_shots.append(shot)\n            \n            # Calculate risk scores for all subjects in the risk set\n            risk_scores_exp = []\n            event_shot_risk_score__eta = None\n\n            for shot in risk_set_shots:\n                shot_id, T, delta, segments = shot\n                \n                # Get covariate vector at event_time^-\n                x_vec = get_covariates_at_left_limit(segments, event_time)\n                \n                # Calculate risk score eta = beta.T * x\n                eta = np.dot(beta, x_vec)\n                risk_scores_exp.append(np.exp(eta))\n\n                if shot_id == event_shot_id:\n                    event_shot_risk_score_eta = eta\n\n            # Sum of exponentiated risk scores\n            risk_set_sum = np.sum(risk_scores_exp)\n\n            # Add term to log-likelihood: eta_i - log(sum(exp(eta_j)))\n            if risk_set_sum > 0: # Avoid log(0) if all risk scores are -inf\n                term = event_shot_risk_score_eta - np.log(risk_set_sum)\n                total_log_likelihood += term\n\n        return total_log_likelihood\n\n    results = []\n    for d_key, b_key in test_cases:\n        dataset = datasets[d_key]\n        beta = betas[b_key]\n        result = calculate_log_likelihood(dataset, beta)\n        results.append(result)\n\n    # Format output as specified\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}