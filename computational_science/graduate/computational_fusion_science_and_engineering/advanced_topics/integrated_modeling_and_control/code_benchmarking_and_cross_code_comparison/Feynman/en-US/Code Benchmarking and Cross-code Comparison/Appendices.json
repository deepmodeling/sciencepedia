{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of code verification is the mesh refinement study, which confirms that a simulation's numerical error decreases at the expected rate as the grid resolution increases. This practice provides a concrete application of this principle, guiding you through the derivation and use of Richardson extrapolation . By analyzing hypothetical data from a magnetohydrodynamics (MHD) simulation, you will learn how to formally estimate a code's observed order of accuracy, $p$, and produce a more accurate estimate of the true continuum solution.",
            "id": "3956960",
            "problem": "A cross-code benchmarking exercise in computational fusion science and engineering considers the linear tearing mode in Resistive Magnetohydrodynamics (MHD) in a two-dimensional periodic slab. The physical quantity of interest is the dimensionless mode growth rate, denoted by $\\gamma$, measured in $\\mathrm{s}^{-1}$ under a fixed normalization. Two production codes implement finite-volume spatial discretizations and explicit time stepping with the same physics model parameters (constant resistivity, identical boundary conditions, identical equilibrium profiles, and identical normalization), but potentially different numerical implementations. To enable a rigorous mesh refinement study for cross-code comparison, you will design and use the following procedure:\n\n- Fix all physical and numerical parameters except the uniform grid spacing $h$ and the time step $\\Delta t$.\n- Refine the grid by a constant ratio $r$, so that $2h$ and $4h$ are coarser levels and $h$ is the finest level. Use the same refinement ratio $r$ across both codes.\n- Choose $\\Delta t$ based on the Courant–Friedrichs–Lewy (CFL) condition so that $\\Delta t \\propto h$ with a fixed CFL number, ensuring temporal errors remain commensurate across refinements.\n- For each grid level, extract a single scalar diagnostic of interest $u_h$ from the saturated linear regime, here $u_h \\equiv \\gamma_h$.\n- Assume the discrete solution obeys an asymptotic error expansion of the form $u_h = u^\\star + C h^{p} + \\mathcal{O}(h^{p+1})$ for sufficiently small $h$, where $u^\\star$ is the continuum quantity, $C$ is a constant, and $p$ is the formal order of accuracy with respect to $h$.\n\nStarting from this asymptotic expansion and fundamental definitions, derive a formal estimator for $u^\\star$ that uses two mesh levels with ratio $r$ and the leading-order term characterized by $p$. Then, estimate $p$ from three mesh levels using only the measured discrete solutions. Finally, apply the estimator to produce a continuum estimate of the growth rate using the two finest levels.\n\nFor Code A, the measured growth rates at three mesh levels are:\n$$\\gamma_{4h}^{(A)} = 0.1010\\ \\mathrm{s}^{-1},\\quad \\gamma_{2h}^{(A)} = 0.10025\\ \\mathrm{s}^{-1},\\quad \\gamma_{h}^{(A)} = 0.1000625\\ \\mathrm{s}^{-1},$$\nwith refinement ratio $r = 2$. Using these data:\n1. Derive an expression for $p$ in terms of $\\{u_h, u_{2h}, u_{4h}\\}$ and $r$ only.\n2. Derive the Richardson extrapolation estimator for $u^\\star$ using two grid levels with ratio $r$ and known $p$.\n3. Evaluate the estimator for Code A using the two finest levels, and express the final continuum estimate of the growth rate in $\\mathrm{s}^{-1}$.\n\nRound your final numerical answer for the continuum growth rate to four significant figures.",
            "solution": "The problem statement has been critically validated and is deemed valid. It is scientifically grounded in the principles of numerical analysis and computational physics, specifically the method of Richardson extrapolation for code verification. The problem is well-posed, objective, and provides all necessary information to derive the requested estimators and compute the final value.\n\nThe task is to derive estimators for the order of accuracy $p$ and the continuum solution $u^\\star$ from a given asymptotic error expansion, and then apply these to experimental data from a numerical simulation.\n\nThe assumed asymptotic error expansion for the discrete solution $u_h$ at a grid spacing $h$ is given by:\n$$u_h = u^\\star + C h^{p} + \\mathcal{O}(h^{p+1})$$\nwhere $u^\\star$ is the exact continuum solution, $C$ is a constant independent of $h$, and $p$ is the formal order of accuracy. We will neglect the higher-order terms $\\mathcal{O}(h^{p+1})$ for this analysis, which is a standard practice when $h$ is sufficiently small.\n\n**1. Derivation of the Order of Accuracy, $p$**\n\nWe are given three mesh levels with a constant refinement ratio $r$. Let the grid spacings be $h$ (finest), $rh$ (medium), and $r^2h$ (coarsest). The corresponding discrete solutions are denoted $u_h$, $u_{rh}$, and $u_{r^2h}$. In the context of the problem data, these levels correspond to $h$, $2h$, and $4h$ with $r=2$.\n\nFrom the leading-order asymptotic expansion, we write the equations for the three levels:\n$$u_h \\approx u^\\star + C h^{p} \\quad (1)$$\n$$u_{rh} \\approx u^\\star + C (rh)^{p} = u^\\star + C r^{p} h^{p} \\quad (2)$$\n$$u_{r^2h} \\approx u^\\star + C (r^2h)^{p} = u^\\star + C r^{2p} h^{p} \\quad (3)$$\n\nTo find $p$, we must eliminate the unknowns $u^\\star$ and $C$. First, we eliminate $u^\\star$ by taking differences between successive equations:\nSubtracting equation $(1)$ from $(2)$:\n$$u_{rh} - u_h \\approx (u^\\star + C r^{p} h^{p}) - (u^\\star + C h^{p}) = C h^{p} (r^{p} - 1) \\quad (4)$$\nSubtracting equation $(2)$ from $(3)$:\n$$u_{r^2h} - u_{rh} \\approx (u^\\star + C r^{2p} h^{p}) - (u^\\star + C r^{p} h^{p}) = C h^{p} (r^{2p} - r^{p}) = C h^{p} r^{p} (r^{p} - 1) \\quad (5)$$\n\nNow, we eliminate $C$ and $h$ by taking the ratio of equation $(5)$ to equation $(4)$:\n$$\\frac{u_{r^2h} - u_{rh}}{u_{rh} - u_h} \\approx \\frac{C h^{p} r^{p} (r^{p} - 1)}{C h^{p} (r^{p} - 1)} = r^p$$\n\nTo solve for $p$, we take the natural logarithm of both sides:\n$$\\ln\\left(\\frac{u_{r^2h} - u_{rh}}{u_{rh} - u_h}\\right) \\approx \\ln(r^p) = p \\ln(r)$$\nThus, the estimator for the order of accuracy $p$ is:\n$$p \\approx \\frac{\\ln\\left(\\frac{u_{r^2h} - u_{rh}}{u_{rh} - u_h}\\right)}{\\ln(r)}$$\nFor the specific notation requested with levels $h$, $2h$, and $4h$, this corresponds to $r=2$, and the expression is $p \\approx \\frac{\\ln\\left(\\frac{u_{4h} - u_{2h}}{u_{2h} - u_h}\\right)}{\\ln(2)}$.\n\n**2. Derivation of the Richardson Extrapolation Estimator for $u^\\star$**\n\nWe use two mesh levels, $h$ and $rh$, with corresponding solutions $u_h$ and $u_{rh}$. The equations are:\n$$u_h \\approx u^\\star + C h^{p} \\quad (6)$$\n$$u_{rh} \\approx u^\\star + C r^{p} h^{p} \\quad (7)$$\n\nOur goal is to find an estimate for $u^\\star$ by eliminating the leading-order error term, which is proportional to $C h^p$. We can achieve this by creating a linear combination of equations $(6)$ and $(7)$. Multiply equation $(6)$ by $r^p$:\n$$r^{p} u_h \\approx r^{p} u^\\star + r^{p} C h^{p} \\quad (8)$$\n\nNow, subtract equation $(7)$ from equation $(8)$:\n$$r^{p} u_h - u_{rh} \\approx (r^{p} u^\\star + r^{p} C h^{p}) - (u^\\star + C r^{p} h^{p})$$\n$$r^{p} u_h - u_{rh} \\approx r^{p} u^\\star - u^\\star = u^\\star(r^{p} - 1)$$\n\nSolving for $u^\\star$, we obtain the Richardson extrapolation estimator:\n$$u^\\star \\approx \\frac{r^{p} u_h - u_{rh}}{r^{p} - 1}$$\nThis formula provides a higher-order estimate of the continuum solution by canceling the leading-order error term.\n\n**3. Application to Code A Data**\n\nWe are given the following growth rates for Code A with a refinement ratio $r=2$:\n$$\\gamma_{h}^{(A)} = u_h = 0.1000625\\ \\mathrm{s}^{-1}$$\n$$\\gamma_{2h}^{(A)} = u_{2h} = 0.10025\\ \\mathrm{s}^{-1}$$\n$$\\gamma_{4h}^{(A)} = u_{4h} = 0.1010\\ \\mathrm{s}^{-1}$$\n\nFirst, we calculate the observed order of accuracy $p$ using the derived formula with $r=2$:\n$$p = \\frac{\\ln\\left(\\frac{u_{4h} - u_{2h}}{u_{2h} - u_h}\\right)}{\\ln(2)} = \\frac{\\ln\\left(\\frac{0.1010 - 0.10025}{0.10025 - 0.1000625}\\right)}{\\ln(2)}$$\n$$p = \\frac{\\ln\\left(\\frac{0.00075}{0.0001875}\\right)}{\\ln(2)} = \\frac{\\ln(4)}{\\ln(2)} = \\frac{2\\ln(2)}{\\ln(2)} = 2$$\nThe observed order of accuracy is exactly $p=2$, indicating the numerical method is second-order accurate and well into its asymptotic convergence regime.\n\nNext, we evaluate the estimator for the continuum growth rate $u^\\star$ using the two finest levels ($h$ and $2h$). With $p=2$ and $r=2$, the Richardson extrapolation formula becomes:\n$$u^\\star = \\frac{r^{p} u_h - u_{rh}}{r^{p} - 1} = \\frac{2^{2} u_h - u_{2h}}{2^{2} - 1} = \\frac{4 u_h - u_{2h}}{3}$$\n\nSubstituting the values for $u_h$ and $u_{2h}$:\n$$u^\\star = \\frac{4(0.1000625) - 0.10025}{3} = \\frac{0.40025 - 0.10025}{3}$$\n$$u^\\star = \\frac{0.30000}{3} = 0.1$$\n\nThe problem requires the final answer to be rounded to four significant figures. Therefore, the continuum estimate of the growth rate is $0.1000\\ \\mathrm{s}^{-1}$.",
            "answer": "$$\\boxed{0.1000}$$"
        },
        {
            "introduction": "Once a code's numerical correctness is established, the focus shifts to performance. The roofline model provides an intuitive yet powerful way to understand whether a computational kernel is limited by the processor's raw compute speed, $P_{\\text{peak}}$, or by the speed at which it can access data from memory, $I B$. In this exercise , you will construct a roofline analysis for a sparse matrix-vector multiply kernel, a ubiquitous operation in fusion codes, and quantify how a seemingly small implementation choice—the precision of index data types—impacts its performance bound.",
            "id": "3957002",
            "problem": "In computational fusion science and engineering, cross-code comparison requires performance models grounded in first principles that apply across implementations and architectures. Consider the sparse matrix-vector multiply kernel used in a gyrokinetic Poisson operator within a drift-kinetic solver. The operator is stored in Compressed Sparse Row (CSR) format and applied as $\\mathbf{y} \\leftarrow \\mathbf{A}\\mathbf{x}$, where $\\mathbf{A}$ is sparse, $\\mathbf{x}$ and $\\mathbf{y}$ are dense vectors, and the arithmetic on all floating-point quantities is in double precision.\n\nTwo code implementations handle indexing differently:\n- Code $\\mathcal{A}$ uses $64$-bit indices for the CSR column indices and $64$-bit row pointers.\n- Code $\\mathcal{B}$ uses $32$-bit indices for the CSR column indices and $32$-bit row pointers.\n\nAssume the following scientifically realistic properties of the kernel on a large matrix representative of a nonlinear gyrokinetic discretization:\n- The number of unknowns is $n = 10^{7}$, and the average number of nonzeros per row is $k = 32$, with irregular sparsity that prevents reuse of $\\mathbf{x}$ elements across nonzeros in the memory hierarchy.\n- One nonzero contributes one floating-point multiply and one floating-point add, for a total of $2$ floating-point operations per nonzero.\n- For each nonzero, the data items read/written to main memory are:\n  - The matrix value: $8$ bytes.\n  - The CSR column index: $8$ bytes in Code $\\mathcal{A}$ and $4$ bytes in Code $\\mathcal{B}$.\n  - The input vector value $\\mathbf{x}[\\text{col}]$: $8$ bytes.\n  - The output accumulation for $\\mathbf{y}[\\text{row}]$ can be modeled as one $8$-byte read and one $8$-byte write per row, amortized uniformly across the $k$ nonzeros in the row, yielding an $16/k$-byte cost per nonzero.\n  - The CSR row pointer access, amortized as $8/k$ bytes in Code $\\mathcal{A}$ and $4/k$ bytes in Code $\\mathcal{B}$ per nonzero.\n\nLet the target hardware be a Graphics Processing Unit (GPU) equipped with High Bandwidth Memory 2 (HBM2), for which a sustained streaming memory bandwidth measurement yields $B = 1.6 \\times 10^{12}$ bytes per second, and the vendor peak double-precision throughput is $P_{\\text{peak}} = 9.7 \\times 10^{12}$ floating-point operations per second.\n\nStarting only from the definitions of arithmetic intensity and the roofline bound,\n- Arithmetic intensity: $I = \\frac{\\text{floating-point operations}}{\\text{bytes transferred to/from main memory}}$,\n- Roofline bound: $P \\le \\min\\!\\big(P_{\\text{peak}},\\, I B\\big)$,\n\nderive the arithmetic intensity for Code $\\mathcal{A}$ and Code $\\mathcal{B}$, determine the roofline performance bound for each code on the specified GPU, and then compute the predicted speedup ratio $S$ of Code $\\mathcal{B}$ over Code $\\mathcal{A}$ on this GPU, defined by $S = \\frac{P_{\\text{bound}, \\mathcal{B}}}{P_{\\text{bound}, \\mathcal{A}}}$. Express the final speedup ratio as a dimensionless number rounded to four significant figures. No units are required in the final answer.",
            "solution": "The problem statement is first validated against the required criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Kernel**: Sparse matrix-vector multiply, $\\mathbf{y} \\leftarrow \\mathbf{A}\\mathbf{x}$.\n- **Matrix representation**: Compressed Sparse Row (CSR).\n- **Precision**: Double precision ($8$ bytes per floating-point quantity).\n- **Code $\\mathcal{A}$ indexing**: $64$-bit ($8$-byte) column indices and row pointers.\n- **Code $\\mathcal{B}$ indexing**: $32$-bit ($4$-byte) column indices and row pointers.\n- **Matrix properties**: Number of unknowns $n = 10^{7}$, average nonzeros per row $k = 32$. Sparsity is irregular, preventing cache reuse of $\\mathbf{x}$.\n- **Operations**: $2$ floating-point operations (one multiply, one add) per nonzero.\n- **Memory traffic per nonzero**:\n  - Matrix value ($A_{ij}$): $8$ bytes.\n  - CSR column index: $8$ bytes for Code $\\mathcal{A}$, $4$ bytes for Code $\\mathcal{B}$.\n  - Input vector value ($x_j$): $8$ bytes.\n  - Output vector value ($y_i$): $16/k$ bytes (amortized read/write).\n  - CSR row pointer: $8/k$ bytes for Code $\\mathcal{A}$, $4/k$ bytes for Code $\\mathcal{B}$ (amortized).\n- **Hardware specifications**:\n  - Sustained memory bandwidth $B = 1.6 \\times 10^{12}$ bytes/s.\n  - Peak double-precision throughput $P_{\\text{peak}} = 9.7 \\times 10^{12}$ FLOPS/s.\n- **Definitions**:\n  - Arithmetic intensity: $I = \\frac{\\text{floating-point operations}}{\\text{bytes transferred to/from main memory}}$.\n  - Roofline bound: $P \\le \\min\\!\\big(P_{\\text{peak}},\\, I B\\big)$.\n- **Objective**: Compute the speedup ratio $S = \\frac{P_{\\text{bound}, \\mathcal{B}}}{P_{\\text{bound}, \\mathcal{A}}}$, rounded to four significant figures.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is scientifically grounded, well-posed, and objective. It describes a standard performance modeling scenario (sparse matrix-vector multiply on a GPU) using the well-established roofline model. The parameters provided for the matrix size, sparsity, hardware bandwidth, and peak performance are realistic for contemporary high-performance computing in computational science. The distinction between $32$-bit and $64$-bit indexing is a common and practical consideration in scientific code optimization. The problem is self-contained, with all necessary data and definitions provided. There are no contradictions, ambiguities, or scientifically unsound premises. The number of matrix rows $n=10^7$ and total non-zeros $n \\times k = 3.2 \\times 10^8$ are both addressable with $32$-bit integers (which can represent up to $2^{32}-1 \\approx 4.29 \\times 10^9$), making the premise of Code $\\mathcal{B}$ valid.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. A complete solution will be provided.\n\n### Derivation of Solution\n\nThe objective is to compute the predicted speedup ratio $S = \\frac{P_{\\text{bound}, \\mathcal{B}}}{P_{\\text{bound}, \\mathcal{A}}}$. This requires determining the roofline performance bound for each code implementation. The roofline model provides an upper bound on performance $P$ given by:\n$$\nP \\le \\min(P_{\\text{peak}}, I B)\n$$\nwhere $P_{\\text{peak}}$ is the peak floating-point throughput, $B$ is the sustained memory bandwidth, and $I$ is the arithmetic intensity. The arithmetic intensity is defined as the ratio of floating-point operations to bytes of data transferred between the processor and main memory. We will first compute the arithmetic intensity for Code $\\mathcal{A}$ and Code $\\mathcal{B}$.\n\nThe unit of computation is the processing of one nonzero element of the matrix $\\mathbf{A}$.\nThe number of floating-point operations per nonzero is given as $2$ FLOPS.\n\nThe total bytes transferred from/to main memory per nonzero, $W$, is the sum of the sizes of all data items accessed.\n\nFor Code $\\mathcal{A}$ (using $64$-bit indices):\nThe memory traffic per nonzero, $W_{\\mathcal{A}}$, is the sum of:\n- Matrix value: $8$ bytes\n- CSR column index: $8$ bytes\n- Input vector element $\\mathbf{x}[\\text{col}]$: $8$ bytes\n- Amortized output vector element $\\mathbf{y}[\\text{row}]$ traffic: $\\frac{16}{k}$ bytes\n- Amortized CSR row pointer traffic: $\\frac{8}{k}$ bytes\n\nSo, the total bytes per nonzero for Code $\\mathcal{A}$ is:\n$$\nW_{\\mathcal{A}} = 8 + 8 + 8 + \\frac{16}{k} + \\frac{8}{k} = 24 + \\frac{24}{k}\n$$\nGiven the average number of nonzeros per row $k = 32$:\n$$\nW_{\\mathcal{A}} = 24 + \\frac{24}{32} = 24 + 0.75 = 24.75 \\text{ bytes/nonzero}\n$$\nThe arithmetic intensity for Code $\\mathcal{A}$ is:\n$$\nI_{\\mathcal{A}} = \\frac{\\text{FLOPS per nonzero}}{W_{\\mathcal{A}}} = \\frac{2}{24.75} \\text{ FLOPS/byte}\n$$\n\nFor Code $\\mathcal{B}$ (using $32$-bit indices):\nThe memory traffic per nonzero, $W_{\\mathcal{B}}$, is the sum of:\n- Matrix value: $8$ bytes\n- CSR column index: $4$ bytes\n- Input vector element $\\mathbf{x}[\\text{col}]$: $8$ bytes\n- Amortized output vector element $\\mathbf{y}[\\text{row}]$ traffic: $\\frac{16}{k}$ bytes\n- Amortized CSR row pointer traffic: $\\frac{4}{k}$ bytes\n\nSo, the total bytes per nonzero for Code $\\mathcal{B}$ is:\n$$\nW_{\\mathcal{B}} = 8 + 4 + 8 + \\frac{16}{k} + \\frac{4}{k} = 20 + \\frac{20}{k}\n$$\nWith $k = 32$:\n$$\nW_{\\mathcal{B}} = 20 + \\frac{20}{32} = 20 + 0.625 = 20.625 \\text{ bytes/nonzero}\n$$\nThe arithmetic intensity for Code $\\mathcal{B}$ is:\n$$\nI_{\\mathcal{B}} = \\frac{\\text{FLOPS per nonzero}}{W_{\\mathcal{B}}} = \\frac{2}{20.625} \\text{ FLOPS/byte}\n$$\n\nNext, we determine the performance bound for each code. We must compare the memory-bound performance, $I B$, with the peak performance, $P_{\\text{peak}}$.\nGiven $B = 1.6 \\times 10^{12}$ bytes/s and $P_{\\text{peak}} = 9.7 \\times 10^{12}$ FLOPS/s.\n\nFor Code $\\mathcal{A}$:\n$$\nI_{\\mathcal{A}} B = \\frac{2}{24.75} \\times (1.6 \\times 10^{12}) \\approx 0.0808 \\times 1.6 \\times 10^{12} \\approx 0.129 \\times 10^{12} \\text{ FLOPS/s}\n$$\nSince $I_{\\mathcal{A}} B \\approx 0.129 \\times 10^{12} \\text{ FLOPS/s} \\ll 9.7 \\times 10^{12} \\text{ FLOPS/s} = P_{\\text{peak}}$, the performance is limited by memory bandwidth.\nThus, the performance bound for Code $\\mathcal{A}$ is $P_{\\text{bound}, \\mathcal{A}} = I_{\\mathcal{A}} B$.\n\nFor Code $\\mathcal{B}$:\n$$\nI_{\\mathcal{B}} B = \\frac{2}{20.625} \\times (1.6 \\times 10^{12}) \\approx 0.0969 \\times 1.6 \\times 10^{12} \\approx 0.155 \\times 10^{12} \\text{ FLOPS/s}\n$$\nSimilarly, $I_{\\mathcal{B}} B \\approx 0.155 \\times 10^{12} \\text{ FLOPS/s} \\ll 9.7 \\times 10^{12} \\text{ FLOPS/s} = P_{\\text{peak}}$, so performance is also memory-bandwidth-bound.\nThus, the performance bound for Code $\\mathcal{B}$ is $P_{\\text{bound}, \\mathcal{B}} = I_{\\mathcal{B}} B$.\n\nThe predicted speedup ratio $S$ of Code $\\mathcal{B}$ over Code $\\mathcal{A}$ is:\n$$\nS = \\frac{P_{\\text{bound}, \\mathcal{B}}}{P_{\\text{bound}, \\mathcal{A}}} = \\frac{I_{\\mathcal{B}} B}{I_{\\mathcal{A}} B} = \\frac{I_{\\mathcal{B}}}{I_{\\mathcal{A}}}\n$$\nSubstituting the expressions for $I_{\\mathcal{A}}$ and $I_{\\mathcal{B}}$:\n$$\nS = \\frac{2/W_{\\mathcal{B}}}{2/W_{\\mathcal{A}}} = \\frac{W_{\\mathcal{A}}}{W_{\\mathcal{B}}}\n$$\nUsing the calculated values for $W_{\\mathcal{A}}$ and $W_{\\mathcal{B}}$:\n$$\nS = \\frac{24.75}{20.625}\n$$\nTo compute this value exactly:\n$$\nS = \\frac{24 + 3/4}{20 + 5/8} = \\frac{99/4}{165/8} = \\frac{99}{4} \\times \\frac{8}{165} = \\frac{99 \\times 2}{165} = \\frac{198}{165}\n$$\nDividing numerator and denominator by their greatest common divisor, which is $33$:\n$$\nS = \\frac{198 / 33}{165 / 33} = \\frac{6}{5} = 1.2\n$$\nThe problem requires the answer to be rounded to four significant figures.\n$$\nS = 1.200\n$$",
            "answer": "$$\\boxed{1.200}$$"
        },
        {
            "introduction": "Modern fusion simulations run on massively parallel supercomputers, making an understanding of scaling behavior essential. This hands-on practice explores weak scaling, where the goal is to solve a proportionally larger problem with more processors in roughly the same amount of time. You will build an analytic performance model from first principles  to predict the weak scaling efficiency of a simulation, accounting for the interplay between computation, nearest-neighbor communication, and global collective operations.",
            "id": "3956942",
            "problem": "A fusion-relevant computational scenario is to compare weak scaling behavior of two simulation classes used in Computational Fusion Science and Engineering: a gyrokinetic Particle-In-Cell (PIC) code and a pseudo-spectral Magnetohydrodynamics (MHD) code. Weak scaling is defined as increasing the total number of processes $N$ while keeping the workload per process fixed. For physically meaningful comparison, the benchmark must preserve physics-relevant resolution constraints as $N$ increases, such as resolving ion Larmor-radius-scale fluctuations in gyrokinetics and Alfvénic cascades in MHD.\n\nAssume the following fundamental base for time-to-solution per time step on a parallel machine:\n- For compute, each process updates a fixed number of cells $M$ with cost proportional to the number of cells, modeled as $T_{\\text{comp}} = c_f M$, where $c_f$ is the compute time per cell per time step.\n- For nearest-neighbor communication, each process exchanges halo data with $6$ neighbors on a three-dimensional domain-decomposed Cartesian grid arranged as $p \\times p \\times p$ with $p = N^{1/3}$, using the Hockney latency–bandwidth model for point-to-point communication, $T_{\\text{msg}} = \\alpha + m/B$, where $\\alpha$ is the latency, $m$ is the message size (in bytes), and $B$ is the asymptotic bandwidth (in bytes per second).\n- For global diagnostics, each time step performs $c_{\\text{red}}$ instances of all-reduce operations of payload $r$ bytes, with cost modeled as $T_{\\text{red}} = c_{\\text{red}} \\left[ \\alpha \\log_{2}(N) + \\frac{r}{B} \\log_{2}(N) \\right]$.\n\nThe benchmark definitions must maintain physics-relevant resolution as $N$ increases:\n- For gyrokinetics, require perpendicular grid spacing $\\Delta x$ such that $k_{\\perp} \\rho_i$ is resolvable in the interval $[0.1, 1.0]$, and maintain a fixed number of particles per cell sufficient to resolve distribution function moments. Here $\\rho_i$ is the ion Larmor radius and $k_{\\perp}$ is the perpendicular wavenumber.\n- For MHD, require that the maximum resolved wavenumber $k_{\\max}$ satisfies $k_{\\max} \\eta \\gtrsim 1$ at the dissipative scale, for resistivity $\\eta$, and maintain a fixed number of grid points per process that resolves Alfvénic fluctuations and the inertial range. Under weak scaling, the per-process grid spacing and number of modes/cells remain fixed while the global domain size grows with $N$.\n\nConsider a single benchmark configuration common to both codes in terms of data movement and grid topology, with the following parameters:\n- Per-process cell count $M = 128^{3}$, per-cell compute time $c_f = 2.0 \\times 10^{-9}$ s.\n- Halo width $h = 2$ cells, doubles per cell $v = 10$ variables, bytes per variable $8$, so bytes per cell $b = 8 v$. Each face message size is $m_{\\text{face}} = h \\, b \\, n_{\\text{face}}$, where $n_{\\text{face}} = 128^{2}$ is the number of cells on a face. There are $6$ faces per process.\n- Latency $\\alpha = 2.5 \\times 10^{-6}$ s, bandwidth $B = 25 \\times 10^{9}$ bytes/s.\n- All-reduce payload $r = 64$ bytes, with $c_{\\text{red}} = 3$ reductions per time step.\n\nStarting from the above base and constraints, derive an analytic expression for the weak scaling efficiency as the ratio of the single-process runtime to the $N$-process runtime with fixed per-process workload, and then evaluate it for $N = 512$ using the parameters provided. Round your final numerical answer to four significant figures. Express the efficiency as a pure number (unitless).\n\nExplain, in words, how the benchmark definitions stated maintain physics-relevant resolution for both codes as $N$ increases, and justify why the chosen communication model components are appropriate for cross-code comparison under weak scaling. Do not use any shortcut formulas that are not justified from the provided base; construct any intermediate expressions from first principles as needed.",
            "solution": "The problem statement has been evaluated and is found to be valid. It is scientifically grounded in computational plasma physics and high-performance computing, well-posed with all necessary parameters defined, and objective in its formulation. We may proceed with the solution.\n\nThe objective is to derive an expression for the weak scaling efficiency, $\\epsilon_W(N)$, and evaluate it for $N=512$, based on a provided performance model for fusion simulation codes. Weak scaling is characterized by a fixed workload per process, where the total problem size grows with the number of processes, $N$.\n\nFirst, we establish the total time-to-solution per time step for $N$ processes, denoted as $T(N)$. This time is the sum of the computation time, $T_{\\text{comp}}$, and the communication time, $T_{\\text{comm}}$.\n$$T(N) = T_{\\text{comp}}(N) + T_{\\text{comm}}(N)$$\nIn a weak scaling scenario, the amount of computation per process is constant. The problem gives the compute time per process as $T_{\\text{comp}} = c_f M$, where $M$ is the fixed number of cells per process and $c_f$ is the compute time per cell. Thus, the compute time is independent of $N$:\n$$T_{\\text{comp}}(N) = c_f M = \\text{constant}$$\n\nThe communication time, $T_{\\text{comm}}(N)$, consists of two components: nearest-neighbor halo exchange, $T_{\\text{halo}}(N)$, and global reductions, $T_{\\text{red}}(N)$.\n$$T_{\\text{comm}}(N) = T_{\\text{halo}}(N) + T_{\\text{red}}(N)$$\n\nFor nearest-neighbor communication, each process exchanges data with its $6$ neighbors on a $p \\times p \\times p$ Cartesian grid, where $p = N^{1/3}$. The time for a single point-to-point message is given by the Hockney model, $T_{\\text{msg}} = \\alpha + m/B$. The problem does not specify if the $6$ halo exchanges can be overlapped. A standard and conservative assumption, which we will adopt, is that the total time is the sum of the times for each of the $6$ messages. Since the per-process data block is constant in weak scaling ($M=128^3$), the face size ($n_{\\text{face}}=128^2$) and halo depth ($h=2$) are also constant. This means the message size for a face, $m_{\\text{face}}$, is also constant.\nThe size of a message for one face is $m_{\\text{face}} = h \\cdot b \\cdot n_{\\text{face}}$, where $b=8v$ is the number of bytes per cell. Since $m_{\\text{face}}$ is constant, the time for each of the $6$ messages is the same.\n$$T_{\\text{halo}}(N) = 6 \\times T_{\\text{msg}} = 6 \\left( \\alpha + \\frac{m_{\\text{face}}}{B} \\right)$$\nFor $N=1$, there are no neighbors, so $T_{\\text{halo}}(1)=0$. Thus, this term should only be included for $N  1$.\n\nFor global diagnostics, the time for $c_{\\text{red}}$ all-reduce operations is given as:\n$$T_{\\text{red}}(N) = c_{\\text{red}} \\left[ \\alpha \\log_{2}(N) + \\frac{r}{B} \\log_{2}(N) \\right] = c_{\\text{red}} \\left( \\alpha + \\frac{r}{B} \\right) \\log_{2}(N)$$\nThis model intrinsically yields $T_{\\text{red}}(1) = 0$ since $\\log_{2}(1)=0$.\n\nCombining these components, the total time for $N$ processes is:\n$$T(N) = c_f M + 6 \\left( \\alpha + \\frac{m_{\\text{face}}}{B} \\right) \\cdot I(N1) + c_{\\text{red}} \\left( \\alpha + \\frac{r}{B} \\right) \\log_{2}(N)$$\nwhere $I(N1)$ is an indicator function that is $1$ if $N  1$ and $0$ if $N=1$.\n\nThe time for a single process, $T(1)$, is the purely computational part, as communication overhead is zero:\n$$T(1) = c_f M$$\n\nWeak scaling efficiency, $\\epsilon_W(N)$, is defined as the ratio of the single-process runtime to the $N$-process runtime.\n$$\\epsilon_W(N) = \\frac{T(1)}{T(N)}$$\nFor $N1$, the analytic expression for the efficiency is:\n$$\\epsilon_W(N) = \\frac{c_f M}{c_f M + 6 \\left( \\alpha + \\frac{m_{\\text{face}}}{B} \\right) + c_{\\text{red}} \\left( \\alpha + \\frac{r}{B} \\right) \\log_{2}(N)}$$\n\nNext, we evaluate this expression for $N = 512$ using the provided parameters.\nFirst, we calculate the intermediary values:\n- Per-process cell count: $M = 128^3 = 2,097,152$.\n- Compute time: $T_{\\text{comp}} = T(1) = c_f M = (2.0 \\times 10^{-9} \\text{ s}) \\times 128^3 = 4.194304 \\times 10^{-3} \\text{ s}$.\n- Bytes per cell: $b = 8v = 8 \\times 10 = 80$ bytes.\n- Cells on a face: $n_{\\text{face}} = 128^2 = 16,384$.\n- Face message size: $m_{\\text{face}} = h \\cdot b \\cdot n_{\\text{face}} = 2 \\times 80 \\times 128^2 = 2,621,440$ bytes.\n- Given parameters: $\\alpha = 2.5 \\times 10^{-6}$ s, $B = 25 \\times 10^9$ bytes/s, $c_{\\text{red}} = 3$, $r = 64$ bytes.\n- $N = 512$, so $\\log_{2}(N) = \\log_{2}(512) = 9$.\n\nNow we calculate the communication time components for $N=512$:\n- Halo exchange time:\n$$T_{\\text{halo}}(512) = 6 \\left( \\alpha + \\frac{m_{\\text{face}}}{B} \\right) = 6 \\left( 2.5 \\times 10^{-6} + \\frac{2,621,440}{25 \\times 10^9} \\right) \\text{ s}$$\n$$T_{\\text{halo}}(512) = 6 \\left( 2.5 \\times 10^{-6} + 1.048576 \\times 10^{-4} \\right) \\text{ s} = 6 (1.073576 \\times 10^{-4}) \\text{ s} = 6.441456 \\times 10^{-4} \\text{ s}$$\n- Global reduction time:\n$$T_{\\text{red}}(512) = c_{\\text{red}} \\left( \\alpha + \\frac{r}{B} \\right) \\log_{2}(512) = 3 \\left( 2.5 \\times 10^{-6} + \\frac{64}{25 \\times 10^9} \\right) \\times 9 \\text{ s}$$\n$$T_{\\text{red}}(512) = 27 \\left( 2.5 \\times 10^{-6} + 2.56 \\times 10^{-9} \\right) \\text{ s} = 27 (2.50256 \\times 10^{-6}) \\text{ s} = 6.756912 \\times 10^{-5} \\text{ s}$$\n\nThe total time for $N=512$ processes is the sum of all components:\n$$T(512) = T_{\\text{comp}} + T_{\\text{halo}}(512) + T_{\\text{red}}(512)$$\n$$T(512) = (4.194304 \\times 10^{-3} + 6.441456 \\times 10^{-4} + 6.756912 \\times 10^{-5}) \\text{ s}$$\n$$T(512) = (0.004194304 + 0.0006441456 + 0.00006756912) \\text{ s} = 0.00490601872 \\text{ s}$$\n\nFinally, the weak scaling efficiency is:\n$$\\epsilon_W(512) = \\frac{T(1)}{T(512)} = \\frac{4.194304 \\times 10^{-3}}{0.00490601872} \\approx 0.8549285$$\nRounding to four significant figures, the efficiency is $0.8549$.\n\n**Explanation of Physics Resolution and Communication Model Appropriateness:**\n\nThe problem requires an explanation of how the benchmark definitions maintain physics-relevant resolution and a justification for the chosen communication models.\n\n1.  **Maintaining Physics-Relevant Resolution:**\n    -   **Gyrokinetics (GK):** Gyrokinetic simulations study micro-instabilities and turbulence in fusion plasmas, where the characteristic scale length is the ion Larmor radius, $\\rho_i$. The physics is governed by interactions at perpendicular wavenumbers $k_{\\perp}$ such that $k_{\\perp}\\rho_i \\approx 1$. The benchmark requirement to resolve the range $k_{\\perp} \\rho_i \\in [0.1, 1.0]$ ensures that the simulation grid spacing, $\\Delta x$, is fine enough to capture the unstable modes that drive turbulence. In weak scaling, the number of cells per process ($M$) is fixed. This implies that the grid spacing $\\Delta x$ per process is also fixed. As the total number of processes $N$ increases, the total physical volume of the simulation grows ($L_{total} \\propto N^{1/3}$), but the resolution of the small, physically important scales (on the order of $\\rho_i$) is maintained everywhere. The fixed number of particles per cell is also critical as it keeps the statistical noise inherent to PIC methods at a constant, controlled level relative to the physical signals.\n    -   **Magnetohydrodynamics (MHD):** MHD simulations often focus on larger-scale phenomena and turbulence. In MHD turbulence, energy cascades from large scales to small scales, where it is dissipated by resistivity $\\eta$. The condition $k_{\\max} \\eta \\gtrsim 1$ ensures that the simulation's maximum resolved wavenumber, $k_{\\max} \\sim \\pi/\\Delta x$, is sufficient to capture the physics at the dissipative scale. As with the GK case, the weak scaling setup fixes the number of grid points per process, thus fixing $\\Delta x$ and $k_{\\max}$. As $N$ increases, the simulation resolves the same finest scales, but in a larger box. This allows for a larger inertial range (the range of scales between energy injection and dissipation), which is crucial for studying the properties of fully developed turbulence.\n\n2.  **Justification of Communication Models:**\n    -   **Nearest-Neighbor (Hockney Model):** Scientific codes using domain decomposition on a grid, such as the GK and MHD codes described, must exchange boundary information between adjacent subdomains. This is known as halo or ghost-cell exchange. This process is a classic nearest-neighbor communication pattern. The Hockney latency-bandwidth model, $T_{\\text{msg}} = \\alpha + m/B$, is a fundamental performance model for point-to-point messaging that captures the two dominant costs: a fixed start-up cost (latency, $\\alpha$) and a data transfer cost proportional to message size (bandwidth, $B$). It is an appropriate model for cross-code comparison because it represents the essential performance characteristics of the underlying hardware and network without being overly complex. In 3D weak scaling, the per-process compute volume is fixed, which means the surface area for halo exchange is also fixed. The model correctly reflects that the message size $m_{\\text{face}}$ and thus the halo exchange cost per process (excluding logarithmic terms that may arise from network contention, which are not modeled here) remain largely constant as $N$ increases.\n    -   **Global Reductions (Logarithmic Model):** Both GK and MHD simulations require periodic global operations for diagnostics (e.g., calculating total conserved quantities like energy) or numerical stability (e.g., finding a global maximum to determine the time step). These are implemented with collective operations like `MPI_Allreduce`. The cost of efficient parallel reduction algorithms on most supercomputer interconnects scales as $\\log(N)$. The model $T_{\\text{red}} \\propto \\log_2(N)$ correctly captures this scaling behavior. It includes a latency term scaling with the number of communication steps in the reduction tree ($\\log_2(N)$) and a bandwidth term for transferring the data payload through the stages. This makes it a realistic and suitable model for the global communication overhead that inevitably grows as the machine size $N$ increases, even in a weak scaling context.",
            "answer": "$$\\boxed{0.8549}$$"
        }
    ]
}