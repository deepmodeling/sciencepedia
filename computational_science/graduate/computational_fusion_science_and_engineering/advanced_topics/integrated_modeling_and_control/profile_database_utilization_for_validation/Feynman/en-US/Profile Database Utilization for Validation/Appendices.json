{
    "hands_on_practices": [
        {
            "introduction": "The first step in validating a computational model is to establish a fair comparison with experimental data. However, profile databases rarely provide measurements on a clean, uniform grid; diagnostic limitations often lead to irregular spacing and data gaps. This exercise  guides you through a principled method to handle this challenge by constructing quadrature weights based on a physical measure, ensuring that your validation metrics accurately reflect the profile's behavior over the entire measured domain.",
            "id": "4032744",
            "problem": "In computational fusion science and engineering, profile databases aggregate diagnostics that sample plasma profiles on a normalized minor-radius coordinate. Due to diagnostic gaps, the available measurement radii form an irregular radial grid: contiguous coverage exists only over certain sub-intervals, and the union of those sub-intervals excludes regions without data. Consider a normalized radial coordinate $r \\in [0,1]$ (dimensionless). Let the coverage set be a finite union of disjoint closed intervals $C = \\bigcup_{k=1}^{K} [a_k,b_k] \\subseteq [0,1]$, where gaps are the complement $[0,1] \\setminus C$. Within each coverage interval $[a_k,b_k]$, a diagnostic provides sample centers $\\{r_{k,i}\\}$, which are strictly increasing and satisfy $a_k  r_{k,1}  \\dots  r_{k,n_k}  b_k$. There are no samples in gaps. The task is to align a model output profile to these irregular grids and compute an unbiased validation metric using quadrature weights derived from a physically motivated measure.\n\nStart from the following fundamental base: validation of a scalar profile $q(r)$ over the covered set $C$ should reflect integration with respect to an appropriate measure. For a normalized minor-radius proxy, a commonly used axisymmetric cross-sectional area element is $J(r)\\,\\mathrm{d}r$, with $J(r)$ proportional to $r$; take $J(r)=r$ (dimensionless) as the measure density for this problem. For any integrable function $q(r)$, the continuous aggregate over coverage is\n$$\n\\mathcal{I}[q] \\equiv \\int_{C} q(r)\\,J(r)\\,\\mathrm{d}r = \\sum_{k=1}^{K} \\int_{a_k}^{b_k} q(r)\\,r\\,\\mathrm{d}r.\n$$\nTo construct an unbiased discrete approximation, partition each coverage interval $[a_k,b_k]$ into control volumes by assigning to each sample center $r_{k,i}$ the sub-interval bounded by midpoints to its neighbors, clamped to interval endpoints. Specifically, define the left and right faces as\n$$\n\\ell_{k,i} = \\begin{cases}\na_k,  i=1,\\\\[4pt]\n\\dfrac{r_{k,i-1}+r_{k,i}}{2},  i1,\n\\end{cases}\n\\qquad\nu_{k,i} = \\begin{cases}\n\\dfrac{r_{k,i}+r_{k,i+1}}{2},  in_k,\\\\[4pt]\nb_k,  i=n_k.\n\\end{cases}\n$$\nDefine the quadrature weight for sample $r_{k,i}$ as the exact measure of its control volume under $J(r)=r$,\n$$\nw_{k,i} \\equiv \\int_{\\ell_{k,i}}^{u_{k,i}} r\\,\\mathrm{d}r = \\dfrac{1}{2}\\left(u_{k,i}^{2}-\\ell_{k,i}^{2}\\right).\n$$\nThis construction is unbiased for constant profiles because it reproduces the exact integral of $q(r)=\\text{const}$ over $C$.\n\nYou are given an analytic target profile $t(r)$ and a model profile $m(r)$. All angles within trigonometric functions are in radians. Define:\n$$\nt(r) = \\exp(-3r^{2}) + 0.1r\n$$\n$$\nm(r) = \\exp(-2.7r^{2}) + 0.1r + 0.02\\sin(12r)\n$$\nBoth $t(r)$ and $m(r)$ are dimensionless. Using the quadrature weights $\\{w_{k,i}\\}$ and the aligned pointwise differences evaluated at sample centers $\\{r_{k,i}\\}$, compute the weighted root-mean-square error (RMS error) over $C$,\n$$\n\\mathrm{RMS} \\equiv \\sqrt{\\dfrac{\\sum_{k=1}^{K}\\sum_{i=1}^{n_k} w_{k,i}\\,\\left(m(r_{k,i})-t(r_{k,i})\\right)^{2}}{\\sum_{k=1}^{K}\\sum_{i=1}^{n_k} w_{k,i}}}.\n$$\nThis $\\mathrm{RMS}$ is dimensionless.\n\nImplement a program that constructs $\\{\\ell_{k,i},u_{k,i},w_{k,i}\\}$ from $\\{[a_k,b_k]\\}$ and $\\{r_{k,i}\\}$ as above, aligns $m(r)$ and $t(r)$ on the given $\\{r_{k,i}\\}$, and outputs the $\\mathrm{RMS}$ for each of the following test suites. All numeric values are dimensionless.\n\nTest Suite:\n- Case $1$ (happy path, no gaps, irregular spacing): $K=1$ with coverage interval $[a_1,b_1]=[\\,0.0,\\,1.0\\,]$, and samples $\\{r_{1,i}\\} = [\\,0.05,\\,0.12,\\,0.20,\\,0.33,\\,0.50,\\,0.72,\\,0.90\\,]$.\n- Case $2$ (two coverage segments with a significant diagnostic gap): $K=2$ with $[a_1,b_1]=[\\,0.0,\\,0.4\\,]$, $[a_2,b_2]=[\\,0.7,\\,1.0\\,]$, and samples $\\{r_{1,i}\\} = [\\,0.05,\\,0.18,\\,0.35\\,]$, $\\{r_{2,i}\\} = [\\,0.72,\\,0.88,\\,0.96\\,]$.\n- Case $3$ (edge coverage, narrow segments, single-sample segment): $K=3$ with $[a_1,b_1]=[\\,0.0,\\,0.15\\,]$, $[a_2,b_2]=[\\,0.4,\\,0.42\\,]$, $[a_3,b_3]=[\\,0.95,\\,0.99\\,]$, and samples $\\{r_{1,i}\\} = [\\,0.02,\\,0.14\\,]$, $\\{r_{2,i}\\} = [\\,0.41\\,]$, $\\{r_{3,i}\\} = [\\,0.955,\\,0.98\\,]$.\n\nYour program must:\n- Construct control-volume faces $\\ell_{k,i}$ and $u_{k,i}$ and weights $w_{k,i}$ as specified.\n- Evaluate $t(r)$ and $m(r)$ at each $r_{k,i}$ and compute the dimensionless $\\mathrm{RMS}$ for each case using the formula above.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Cases $1$, $2$, $3$. Each value should be formatted as a decimal number rounded to exactly six digits after the decimal point (for example, $0.012345$).\n\nNo physical units are involved beyond the dimensionless specification. All trigonometric arguments are in radians. The final output format must be exactly of the form $[x_1,x_2,x_3]$ where each $x_j$ is a decimal with six digits after the decimal point.",
            "solution": "The problem requires the calculation of a weighted root-mean-square (RMS) error between a model profile $m(r)$ and a target profile $t(r)$ for a physical quantity in fusion plasma analysis. The validation is performed over an irregular grid of measurement points. The design of the solution is based on the principle that a discrete validation metric should be a verifiably unbiased approximation of a continuous, physically motivated integral.\n\nThe foundational principle is that the comparison should be weighted by an appropriate measure reflecting the geometry of the system. For a normalized minor-radius coordinate $r \\in [0,1]$, a natural choice for the axisymmetric cross-sectional area element is proportional to $r\\,\\mathrm{d}r$. We are given the measure density $J(r)=r$. The total value of a profile $q(r)$ over the measurement coverage set $C = \\bigcup_{k=1}^{K} [a_k, b_k]$ is thus defined by the continuous integral:\n$$\n\\mathcal{I}[q] \\equiv \\int_{C} q(r)\\,J(r)\\,\\mathrm{d}r = \\sum_{k=1}^{K} \\int_{a_k}^{b_k} q(r)\\,r\\,\\mathrm{d}r\n$$\nTo create a discrete numerical approximation of this integral, we employ a finite-volume method. Each measurement point $r_{k,i}$ within a coverage interval $[a_k, b_k]$ becomes the center of a unique control volume. The boundaries of these control volumes are defined by the midpoints between adjacent sample centers. This partitioning ensures that the entire coverage interval is divided without overlap. Specifically, for a sample $r_{k,i}$ in the set $\\{r_{k,1}, \\dots, r_{k,n_k}\\}$, the left and right boundaries of its control volume, $\\ell_{k,i}$ and $u_{k,i}$, are given by:\n$$\n\\ell_{k,i} = \\begin{cases}\na_k,  i=1 \\\\[4pt]\n\\dfrac{r_{k,i-1}+r_{k,i}}{2},  i > 1\n\\end{cases}\n\\qquad\nu_{k,i} = \\begin{cases}\n\\dfrac{r_{k,i}+r_{k,i+1}}{2},  i  n_k \\\\[4pt]\nb_k,  i = n_k\n\\end{cases}\n$$\nThe quadrature weight $w_{k,i}$ assigned to the sample point $r_{k,i}$ is defined as the exact integral of the measure density $J(r)=r$ over its corresponding control volume $[\\ell_{k,i}, u_{k,i}]$:\n$$\nw_{k,i} \\equiv \\int_{\\ell_{k,i}}^{u_{k,i}} r\\,\\mathrm{d}r = \\left[ \\frac{1}{2}r^2 \\right]_{\\ell_{k,i}}^{u_{k,i}} = \\frac{1}{2}\\left(u_{k,i}^{2}-\\ell_{k,i}^{2}\\right)\n$$\nThis construction is unbiased for any profile that is constant over each control volume. In particular, for a globally constant profile $q(r)=c$, the discrete sum $\\sum_{k,i} c \\cdot w_{k,i}$ exactly equals the continuous integral $\\int_C c \\cdot r\\,\\mathrm{d}r$, because the control volumes form a perfect partition of the coverage set $C$, and thus $\\sum_{k,i} w_{k,i} = \\sum_{k,i} \\int_{\\ell_{k,i}}^{u_{k,i}} r\\,\\mathrm{d}r = \\int_C r\\,\\mathrm{d}r$.\n\nThe specific target and model profiles are given as:\n$$\nt(r) = \\exp(-3r^{2}) + 0.1r\n$$\n$$\nm(r) = \\exp(-2.7r^{2}) + 0.1r + 0.02\\sin(12r)\n$$\nThe squared difference at each sample point, $(m(r_{k,i})-t(r_{k,i}))^2$, is weighted by $w_{k,i}$. The final validation metric is the weighted root-mean-square error, which normalizes the weighted sum of squared errors by the total weight. This ensures the metric represents an average error over the geometric measure of the coverage domain. The formula is:\n$$\n\\mathrm{RMS} \\equiv \\sqrt{\\dfrac{\\sum_{k=1}^{K}\\sum_{i=1}^{n_k} w_{k,i}\\,\\left(m(r_{k,i})-t(r_{k,i})\\right)^{2}}{\\sum_{k=1}^{K}\\sum_{i=1}^{n_k} w_{k,i}}}\n$$\nThe algorithm proceeds by first receiving the test case data, which specifies the coverage intervals $[a_k, b_k]$ and the sample points $\\{r_{k,i}\\}$ for each segment $k$. It then iterates through each segment and each sample point within that segment. In each step, it calculates the control volume boundaries $\\ell_{k,i}$ and $u_{k,i}$, computes the corresponding weight $w_{k,i}$, evaluates the profiles $t(r_{k,i})$ and $m(r_{k,i})$, and accumulates the weighted squared error and the total weight. After processing all points in all segments for a given case, it computes the final RMS error using the formula above. This process is repeated for each of the $3$ test cases, and the results are formatted as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the profile validation problem for the given test suites.\n    \"\"\"\n    \n    # Define the analytic target and model profiles as functions.\n    # All angles are in radians.\n    def t(r):\n        \"\"\"Target profile t(r).\"\"\"\n        return np.exp(-3.0 * r**2) + 0.1 * r\n\n    def m(r):\n        \"\"\"Model profile m(r).\"\"\"\n        return np.exp(-2.7 * r**2) + 0.1 * r + 0.02 * np.sin(12.0 * r)\n        \n    # Define the test suites from the problem statement.\n    # Structure: list of cases. Each case is a list of segments.\n    # Each segment is a tuple (a_k, b_k, r_samples_k).\n    test_cases = [\n        # Case 1 (happy path, no gaps, irregular spacing)\n        [\n            (0.0, 1.0, [0.05, 0.12, 0.20, 0.33, 0.50, 0.72, 0.90]),\n        ],\n        # Case 2 (two coverage segments with a significant diagnostic gap)\n        [\n            (0.0, 0.4, [0.05, 0.18, 0.35]),\n            (0.7, 1.0, [0.72, 0.88, 0.96]),\n        ],\n        # Case 3 (edge coverage, narrow segments, single-sample segment)\n        [\n            (0.0, 0.15, [0.02, 0.14]),\n            (0.4, 0.42, [0.41]),\n            (0.95, 0.99, [0.955, 0.98]),\n        ],\n    ]\n\n    results = []\n    \n    # Process each test case\n    for case_data in test_cases:\n        total_weighted_sq_err = 0.0\n        total_weight = 0.0\n\n        # Iterate over each coverage segment in the case\n        for segment in case_data:\n            a_k, b_k, r_samples = segment\n            n_k = len(r_samples)\n\n            if n_k == 0:\n                continue\n\n            # Iterate over each sample point in the segment\n            for i in range(n_k):\n                r_ki = r_samples[i]\n\n                # Define the left face of the control volume\n                if i == 0:\n                    l_ki = a_k\n                else:\n                    l_ki = (r_samples[i - 1] + r_ki) / 2.0\n\n                # Define the right face of the control volume\n                if i == n_k - 1:\n                    u_ki = b_k\n                else:\n                    u_ki = (r_ki + r_samples[i + 1]) / 2.0\n                \n                # Calculate the quadrature weight w_ki\n                w_ki = 0.5 * (u_ki**2 - l_ki**2)\n                \n                # Evaluate profiles at r_ki and calculate squared difference\n                diff = m(r_ki) - t(r_ki)\n                \n                # Accumulate weighted squared error and total weight\n                total_weighted_sq_err += w_ki * (diff**2)\n                total_weight += w_ki\n\n        # Calculate the final RMS error for the case\n        if total_weight > 0:\n            rms_error = np.sqrt(total_weighted_sq_err / total_weight)\n        else:\n            rms_error = 0.0 # Should not happen based on problem description\n\n        results.append(rms_error)\n\n    # Format the results for the final output\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\n# Execute the solver\nsolve()\n```"
        },
        {
            "introduction": "Once we have a method for aligning model and data, the next critical step is to rigorously quantify their agreement in the presence of measurement uncertainty. A common pitfall is to ignore systematic errors that affect multiple data points simultaneously, leading to incorrect statistical conclusions. In this practice , you will confront this issue by deriving the full error covariance matrix and using it to compute a correct $\\chi^2$ statistic, a cornerstone of robust model validation.",
            "id": "4032708",
            "problem": "A fusion device profile database is used to validate a core transport model by comparing stored electron temperature profiles against model predictions. Consider a single discharge entry in the database in which electron temperature, denoted by $x(r)$, is measured at $N=3$ normalized flux radii $r_1$, $r_2$, and $r_3$. The measurements suffer from two discharge-wide systematic calibration errors: a multiplicative absolute calibration factor error and an additive background offset error, in addition to pointwise statistical noise. Assume the following stochastic measurement model for each profile point:\n- The measurement at $r_i$ satisfies $x(r_i) = (1 + k)\\,x_{\\mathrm{true}}(r_i) + b + \\epsilon_i$, where $k$ is a zero-mean multiplicative calibration error, $b$ is a zero-mean additive offset error, and $\\epsilon_i$ is a zero-mean statistical noise term.\n- The variables $k$, $b$, and the vector $\\boldsymbol{\\epsilon} = (\\epsilon_1,\\epsilon_2,\\epsilon_3)^{\\mathsf{T}}$ are mutually independent, with $k \\sim \\mathcal{N}(0,\\sigma_k^2)$, $b \\sim \\mathcal{N}(0,\\sigma_b^2)$, and $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma_i^2)$.\n- You may assume the small-error regime and linearize about the model prediction so that wherever $x_{\\mathrm{true}}(r_i)$ appears in a variance or covariance, you may substitute $x_{\\mathrm{model}}(r_i)$.\n\nThe database entry provides the following for this discharge:\n- The model predictions at the three radii are $x_{\\mathrm{model}}(r_1) = 2.0$, $x_{\\mathrm{model}}(r_2) = 1.5$, $x_{\\mathrm{model}}(r_3) = 1.0$ (in $\\mathrm{keV}$).\n- The corresponding measurements are $x(r_1) = 2.1$, $x(r_2) = 1.6$, $x(r_3) = 0.9$ (in $\\mathrm{keV}$).\n- The pointwise statistical standard deviations are $\\sigma_1 = 0.1$, $\\sigma_2 = 0.1$, $\\sigma_3 = 0.1$ (in $\\mathrm{keV}$).\n- The calibration metadata reports a fractional absolute calibration uncertainty $\\sigma_k = 0.05$ and an additive background uncertainty $\\sigma_b = 0.05$ (in $\\mathrm{keV}$).\n\nTasks:\n1) Starting from the definition of covariance and the properties of independent zero-mean Gaussian variables together with linearization in the small-error regime, derive the covariance structure $C_{ij} = \\mathrm{Cov}\\!\\left[x(r_i), x(r_j)\\right]$ in terms of $\\delta_{ij}$, $\\sigma_i$, $\\sigma_k$, $\\sigma_b$, and $x_{\\mathrm{model}}(r_i)$.\n2) Using the derived covariance, construct the full covariance matrix $\\mathbf{C}$ for this discharge and compute the quadratic form associated with the chi-square statistic $\\chi^2 = \\boldsymbol{d}^{\\mathsf{T}} \\mathbf{C}^{-1} \\boldsymbol{d}$, where the residual vector is $\\boldsymbol{d} = \\big(x(r_1)-x_{\\mathrm{model}}(r_1),\\,x(r_2)-x_{\\mathrm{model}}(r_2),\\,x(r_3)-x_{\\mathrm{model}}(r_3)\\big)^{\\mathsf{T}}$.\n3) For comparison, compute the naive uncorrelated chi-square $\\chi^2_{\\mathrm{naive}}$ that ignores shared calibration errors by treating the uncertainties as uncorrelated with variance $\\sigma_i^2$ only.\n4) Define the overestimation factor $R = \\chi^2_{\\mathrm{naive}} / \\chi^2$ that quantifies the impact of shared systematics on chi-square-based validation decisions. Provide $R$ as your final answer in exact form as a reduced fraction. No units are required. Do not round.",
            "solution": "The problem statement has been validated and is deemed to be scientifically grounded, well-posed, objective, and internally consistent. It presents a standard problem in statistical data analysis and model validation, common in experimental physics and engineering disciplines. All necessary data and definitions are provided to arrive at a unique, meaningful solution.\n\nThe solution proceeds by addressing the four tasks in sequence.\n\nTask 1: Derivation of the covariance structure $C_{ij}$.\n\nThe measurement at a normalized flux radius $r_i$ is given by the stochastic model:\n$$x(r_i) = (1 + k)\\,x_{\\mathrm{true}}(r_i) + b + \\epsilon_i$$\nHere, $k$, $b$, and $\\epsilon_i$ are independent, zero-mean Gaussian random variables with variances $\\sigma_k^2$, $\\sigma_b^2$, and $\\sigma_i^2$, respectively. Let's denote $x_i = x(r_i)$ and $x_{i, \\mathrm{true}} = x_{\\mathrm{true}}(r_i)$.\n\nThe covariance between two measurements $x_i$ and $x_j$ is defined as $C_{ij} = \\mathrm{Cov}(x_i, x_j) = E[(x_i - E[x_i])(x_j - E[x_j])]$. First, we find the expectation value of $x_i$:\n$$E[x_i] = E[(1 + k)x_{i, \\mathrm{true}} + b + \\epsilon_i] = E[x_{i, \\mathrm{true}} + k x_{i, \\mathrm{true}} + b + \\epsilon_i]$$\nUsing the linearity of expectation and the fact that $E[k] = 0$, $E[b] = 0$, and $E[\\epsilon_i] = 0$:\n$$E[x_i] = x_{i, \\mathrm{true}} + E[k]x_{i, \\mathrm{true}} + E[b] + E[\\epsilon_i] = x_{i, \\mathrm{true}}$$\nThe deviation of a measurement from its expected (true) value is:\n$$x_i - E[x_i] = x_i - x_{i, \\mathrm{true}} = k x_{i, \\mathrm{true}} + b + \\epsilon_i$$\nNow we compute the covariance:\n$$C_{ij} = E[(x_i - x_{i, \\mathrm{true}})(x_j - x_{j, \\mathrm{true}})] = E[(k x_{i, \\mathrm{true}} + b + \\epsilon_i)(k x_{j, \\mathrm{true}} + b + \\epsilon_j)]$$\nExpanding the product:\n$$C_{ij} = E[k^2 x_{i, \\mathrm{true}} x_{j, \\mathrm{true}} + k b (x_{i, \\mathrm{true}} + x_{j, \\mathrm{true}}) + b^2 + k(x_{i, \\mathrm{true}}\\epsilon_j + x_{j, \\mathrm{true}}\\epsilon_i) + b(\\epsilon_i + \\epsilon_j) + \\epsilon_i \\epsilon_j]$$\nDue to the mutual independence and zero-mean property of $k$, $b$, and $\\epsilon_i$, the expectation of any cross-term is zero (e.g., $E[kb] = E[k]E[b]=0$). The only non-zero expectations are for the squared terms: $E[k^2] = \\sigma_k^2$, $E[b^2] = \\sigma_b^2$, and $E[\\epsilon_i \\epsilon_j]$. The statistical noise terms $\\epsilon_i$ are independent for different points, so $E[\\epsilon_i \\epsilon_j] = 0$ for $i \\neq j$, and $E[\\epsilon_i^2] = \\sigma_i^2$. This can be written compactly using the Kronecker delta, $\\delta_{ij}$, as $E[\\epsilon_i \\epsilon_j] = \\sigma_i^2 \\delta_{ij}$.\n\nSubstituting these into the expanded expression for $C_{ij}$:\n$$C_{ij} = \\sigma_k^2 x_{i, \\mathrm{true}} x_{j, \\mathrm{true}} + \\sigma_b^2 + \\sigma_i^2 \\delta_{ij}$$\nThe problem permits linearization in the small-error regime, allowing the substitution of the true values $x_{i, \\mathrm{true}}$ with the model predictions $x_{\\mathrm{model}}(r_i)$, which we denote as $x_{i, \\mathrm{model}}$. Thus, the final expression for the covariance structure is:\n$$C_{ij} = \\sigma_k^2 x_{i, \\mathrm{model}} x_{j, \\mathrm{model}} + \\sigma_b^2 + \\sigma_i^2 \\delta_{ij}$$\n\nTask 2: Construction of the covariance matrix $\\mathbf{C}$ and computation of $\\chi^2$.\n\nWe are given the following numerical values:\n- Model predictions: $\\mathbf{x}_{\\mathrm{model}} = (x_{1, \\mathrm{model}}, x_{2, \\mathrm{model}}, x_{3, \\mathrm{model}})^{\\mathsf{T}} = (2.0, 1.5, 1.0)^{\\mathsf{T}}$.\n- Measurements: $\\mathbf{x} = (x_1, x_2, x_3)^{\\mathsf{T}} = (2.1, 1.6, 0.9)^{\\mathsf{T}}$.\n- Statistical standard deviations: $\\sigma_1 = \\sigma_2 = \\sigma_3 = 0.1$, so $\\sigma_i^2 = 0.01$ for all $i$.\n- Calibration uncertainties: $\\sigma_k = 0.05$ (so $\\sigma_k^2 = 0.0025$) and $\\sigma_b = 0.05$ (so $\\sigma_b^2 = 0.0025$).\n\nThe residual vector is $\\mathbf{d} = \\mathbf{x} - \\mathbf{x}_{\\mathrm{model}} = (0.1, 0.1, -0.1)^{\\mathsf{T}}$.\n\nWe compute the elements of the $3 \\times 3$ covariance matrix $\\mathbf{C}$:\n$C_{11} = (0.05)^2 (2.0)^2 + (0.05)^2 + (0.1)^2 = 0.0025 \\cdot 4 + 0.0025 + 0.01 = 0.01 + 0.0025 + 0.01 = 0.0225$.\n$C_{22} = (0.05)^2 (1.5)^2 + (0.05)^2 + (0.1)^2 = 0.0025 \\cdot 2.25 + 0.0025 + 0.01 = 0.005625 + 0.0025 + 0.01 = 0.018125$.\n$C_{33} = (0.05)^2 (1.0)^2 + (0.05)^2 + (0.1)^2 = 0.0025 \\cdot 1 + 0.0025 + 0.01 = 0.005 + 0.01 = 0.015$.\n$C_{12} = C_{21} = (0.05)^2 (2.0)(1.5) + (0.05)^2 = 0.0025 \\cdot 3 + 0.0025 = 0.01$.\n$C_{13} = C_{31} = (0.05)^2 (2.0)(1.0) + (0.05)^2 = 0.0025 \\cdot 2 + 0.0025 = 0.0075$.\n$C_{23} = C_{32} = (0.05)^2 (1.5)(1.0) + (0.05)^2 = 0.0025 \\cdot 1.5 + 0.0025 = 0.00625$.\n\nThe full covariance matrix $\\mathbf{C}$ is:\n$$ \\mathbf{C} = \\begin{pmatrix} 0.0225  0.01  0.0075 \\\\ 0.01  0.018125  0.00625 \\\\ 0.0075  0.00625  0.015 \\end{pmatrix} $$\nTo compute $\\chi^2 = \\mathbf{d}^{\\mathsf{T}} \\mathbf{C}^{-1} \\mathbf{d}$, we first find the inverse of $\\mathbf{C}$. It is computationally advantageous to work with fractions.\n$\\sigma_k^2 = (\\frac{1}{20})^2=\\frac{1}{400}$, $\\sigma_b^2 = \\frac{1}{400}$, $\\sigma_i^2 = (\\frac{1}{10})^2=\\frac{1}{100}$. $x_{1, \\mathrm{model}}=2, x_{2, \\mathrm{model}}=\\frac{3}{2}, x_{3, \\mathrm{model}}=1$.\n$C_{11} = \\frac{4}{400}+\\frac{1}{400}+\\frac{4}{400}=\\frac{9}{400}$. $C_{22} = \\frac{9/4}{400}+\\frac{1}{400}+\\frac{4}{400}=\\frac{9}{1600}+\\frac{4}{1600}+\\frac{16}{1600}=\\frac{29}{1600}$. $C_{33} = \\frac{1}{400}+\\frac{1}{400}+\\frac{4}{400}=\\frac{6}{400}$.\n$C_{12} = \\frac{3}{400}+\\frac{1}{400}=\\frac{4}{400}$. $C_{13} = \\frac{2}{400}+\\frac{1}{400}=\\frac{3}{400}$. $C_{23} = \\frac{3/2}{400}+\\frac{1}{400}=\\frac{5}{800}$.\n$$ \\mathbf{C} = \\frac{1}{1600} \\begin{pmatrix} 36  16  12 \\\\ 16  29  10 \\\\ 12  10  24 \\end{pmatrix} $$\nLet $\\mathbf{M}$ be the inner matrix. The determinant of $\\mathbf{M}$ is $\\det(\\mathbf{M}) = 14976$. The inverse matrix is $\\mathbf{C}^{-1} = 1600 \\cdot \\mathbf{M}^{-1} = 1600 \\frac{\\mathrm{adj}(\\mathbf{M})}{\\det(\\mathbf{M})}$.\n$$ \\mathrm{adj}(\\mathbf{M}) = \\begin{pmatrix} 596  -264  -188 \\\\ -264  720  -168 \\\\ -188  -168  788 \\end{pmatrix} $$\n$$ \\mathbf{C}^{-1} = \\frac{1600}{14976} \\mathrm{adj}(\\mathbf{M}) = \\frac{25}{234} \\mathrm{adj}(\\mathbf{M}) $$\nThe chi-square statistic is $\\chi^2 = \\mathbf{d}^{\\mathsf{T}} \\mathbf{C}^{-1} \\mathbf{d}$:\n$$ \\chi^2 = (0.1, 0.1, -0.1) \\frac{25}{234} \\begin{pmatrix} 596  -264  -188 \\\\ -264  720  -168 \\\\ -188  -168  788 \\end{pmatrix} \\begin{pmatrix} 0.1 \\\\ 0.1 \\\\ -0.1 \\end{pmatrix} $$\n$$ \\chi^2 = \\frac{0.01 \\times 25}{234} (1, 1, -1) \\begin{pmatrix} 596  -264  -188 \\\\ -264  720  -168 \\\\ -188  -168  788 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix} $$\nThe quadratic form evaluates to:\n$$ (1, 1, -1) \\begin{pmatrix} 596-264+188 \\\\ -264+720+168 \\\\ -188-168-788 \\end{pmatrix} = (1, 1, -1) \\begin{pmatrix} 520 \\\\ 624 \\\\ -1144 \\end{pmatrix} = 520+624+1144 = 2288 $$\n$$ \\chi^2 = \\frac{0.25}{234} \\times 2288 = \\frac{1}{4} \\frac{2288}{234} = \\frac{572}{234} = \\frac{286}{117} = \\frac{22 \\times 13}{9 \\times 13} = \\frac{22}{9} $$\n\nTask 3: Computation of the naive uncorrelated chi-square $\\chi^2_{\\mathrm{naive}}$.\n\nThis calculation ignores the shared systematic errors, treating the measurements as independent. The covariance matrix becomes diagonal, with entries being only the statistical variances $\\sigma_i^2$.\n$$ \\mathbf{C}_{\\mathrm{naive}} = \\mathrm{diag}(\\sigma_1^2, \\sigma_2^2, \\sigma_3^2) = \\mathrm{diag}(0.01, 0.01, 0.01) $$\nThe chi-square is calculated as:\n$$ \\chi^2_{\\mathrm{naive}} = \\sum_{i=1}^3 \\frac{(x_i - x_{i, \\mathrm{model}})^2}{\\sigma_i^2} = \\frac{d_1^2}{\\sigma_1^2} + \\frac{d_2^2}{\\sigma_2^2} + \\frac{d_3^2}{\\sigma_3^2} $$\n$$ \\chi^2_{\\mathrm{naive}} = \\frac{(0.1)^2}{(0.1)^2} + \\frac{(0.1)^2}{(0.1)^2} + \\frac{(-0.1)^2}{(0.1)^2} = 1 + 1 + 1 = 3 $$\n\nTask 4: Computation of the overestimation factor $R$.\n\nThe factor $R$ quantifies the ratio of the naive chi-square to the correct one.\n$$ R = \\frac{\\chi^2_{\\mathrm{naive}}}{\\chi^2} = \\frac{3}{22/9} = 3 \\times \\frac{9}{22} = \\frac{27}{22} $$\nThe naive calculation overestimates the statistical significance of the model-data mismatch by ignoring the positive correlations induced by shared systematic errors.",
            "answer": "$$\\boxed{\\frac{27}{22}}$$"
        },
        {
            "introduction": "The ultimate promise of large-scale profile databases is the ability to discover and validate physical laws that hold across different operating conditions and even different machines. This capstone practice  puts you in the role of a data scientist synthesizing information from a multi-device database to derive an empirical scaling law for pedestal temperature. You will move beyond simple regression to account for both within-device and between-device variability, a sophisticated technique essential for building truly predictive models.",
            "id": "4032768",
            "problem": "A computational fusion science and engineering validation study aims to quantify how the pedestal electron temperature height scales with the toroidal magnetic field and the external heating power, using a multi-device profile database. The goal is to derive a cross-device scaling relation that is both scientifically grounded and testable, and then use it to adjust validation expectations for target devices by accounting for between-device variability.\n\nStarting point and scientific basis: The pedestal energy balance is governed by conservation of energy and transport. The externally injected heating power is balanced by cross-field heat transport out of the pedestal region. By dimensional reasoning and well-tested empirical observations in fusion experiments, the pedestal electron temperature $T_{e,\\text{ped}}$ often exhibits a power-law dependence on the toroidal magnetic field $B_T$ and on the total heating power $P_{\\text{heat}}$. This motivates the model\n$$\nT_{e,\\text{ped}} \\propto B_T^{a}\\, P_{\\text{heat}}^{b},\n$$\nwith exponents $a$ and $b$ to be inferred. A log-transformation yields a linear relation,\n$$\n\\ln T_{e,\\text{ped}} = \\ln C + a \\ln B_T + b \\ln P_{\\text{heat}},\n$$\nwhere $C$ is a constant prefactor. Across devices, systematic offsets may exist due to geometry, wall conditions, or operational specifics. These can be represented statistically as device-level random effects that broaden expectations across devices, compared to within-device variability.\n\nDatabase: Use the following multi-device dataset of pedestal electron temperature heights. Each tuple is $(B_T, P_{\\text{heat}}, T_{e,\\text{ped}})$ with $B_T$ in Tesla, $P_{\\text{heat}}$ in megawatts, and $T_{e,\\text{ped}}$ in electron-volts. Devices are labeled $\\text{D1}$, $\\text{D2}$, $\\text{D3}$, $\\text{D4}$.\n\nDevice $\\text{D1}$:\n- $(\\,$ $2.0$, $5.0$, $410.0$ $\\,)$\n- $(\\,$ $2.5$, $8.0$, $535.0$ $\\,)$\n- $(\\,$ $2.5$, $10.0$, $561.0$ $\\,)$\n- $(\\,$ $3.0$, $5.0$, $493.0$ $\\,)$\n- $(\\,$ $3.0$, $8.0$, $597.0$ $\\,)$\n- $(\\,$ $3.5$, $10.0$, $672.0$ $\\,)$\n\nDevice $\\text{D2}$:\n- $(\\,$ $2.0$, $6.0$, $373.0$ $\\,)$\n- $(\\,$ $2.5$, $4.0$, $354.0$ $\\,)$\n- $(\\,$ $3.0$, $12.0$, $559.0$ $\\,)$\n- $(\\,$ $3.0$, $8.0$, $500.0$ $\\,)$\n- $(\\,$ $3.5$, $6.0$, $498.0$ $\\,)$\n- $(\\,$ $4.0$, $10.0$, $612.0$ $\\,)$\n\nDevice $\\text{D3}$:\n- $(\\,$ $1.5$, $5.0$, $329.0$ $\\,)$\n- $(\\,$ $2.0$, $8.0$, $435.0$ $\\,)$\n- $(\\,$ $3.0$, $10.0$, $581.0$ $\\,)$\n- $(\\,$ $3.5$, $12.0$, $676.0$ $\\,)$\n- $(\\,$ $4.0$, $5.0$, $511.0$ $\\,)$\n- $(\\,$ $5.0$, $8.0$, $708.0$ $\\,)$\n\nDevice $\\text{D4}$:\n- $(\\,$ $2.0$, $3.0$, $271.0$ $\\,)$\n- $(\\,$ $2.5$, $6.0$, $394.0$ $\\,)$\n- $(\\,$ $3.0$, $4.0$, $372.0$ $\\,)$\n- $(\\,$ $3.5$, $8.0$, $501.0$ $\\,)$\n- $(\\,$ $4.0$, $6.0$, $499.0$ $\\,)$\n- $(\\,$ $5.0$, $10.0$, $660.0$ $\\,)$\n\nTasks:\n1. Infer the scaling exponents $a$ and $b$ and the prefactor $C$ by minimizing the sum of squared errors in the log-transformed space for the model $\\ln T_{e,\\text{ped}} = \\ln C + a \\ln B_T + b \\ln P_{\\text{heat}}$ over the entire database.\n2. Compute residuals $r_i = \\ln T_{e,\\text{ped},i} - (\\ln C + a \\ln B_{T,i} + b \\ln P_{\\text{heat},i})$ and decompose variability into a between-device component and a within-device component. Let $m_d$ be the mean residual for device $d$, and let $w_i = r_i - m_{d(i)}$ be the within-device residual for datum $i$. Estimate the within-device variance $\\sigma_e^2$ from $\\{w_i\\}$ and the between-device variance $\\sigma_d^2$ from $\\{m_d\\}$, accounting for the finite-sample correction due to $\\sigma_e^2 / n_d$, where $n_d$ is the number of data points for device $d$. Use $D = 4$ devices and the total number of data points $N = 24$.\n3. For validation across devices, define an adjusted confidence interval for a predicted $\\ln T_{e,\\text{ped}}$ at confidence level $0.95$ as follows:\n   - For a new device not present in the database, use the total standard deviation $\\sqrt{\\sigma_d^2 + \\sigma_e^2}$.\n   - For an existing device present in the database, use only the within-device standard deviation $\\sqrt{\\sigma_e^2}$.\n   Let $z$ be the two-sided quantile such that the coverage is $0.95$ for a standard normal distribution, and form the interval in log-space as $[\\hat{y} - z s, \\hat{y} + z s]$. Map it back to electron-volts via the exponential function to obtain $[T_{\\text{low}}, T_{\\text{high}}]$.\n4. Apply the scaling to the test suite below and evaluate whether the measured pedestal electron temperature is consistent with the adjusted expectation for each case. For each case, compute the predicted pedestal electron temperature $T_{e,\\text{pred}}$ in electron-volts and, when a measured value is provided, a boolean indicating whether the measured value falls within the adjusted confidence interval.\n\nTest suite:\n- Case $1$ (new device): $B_T = 2.5$ (Tesla), $P_{\\text{heat}} = 10.0$ (megawatts), measured $T_{e,\\text{ped}} = 560.0$ (electron-volts).\n- Case $2$ (new device, boundary): $B_T = 1.5$ (Tesla), $P_{\\text{heat}} = 3.0$ (megawatts), measured $T_{e,\\text{ped}} = 210.0$ (electron-volts).\n- Case $3$ (new device, edge): $B_T = 5.0$ (Tesla), $P_{\\text{heat}} = 8.0$ (megawatts), measured $T_{e,\\text{ped}} = 750.0$ (electron-volts).\n- Case $4$ (existing device $\\text{D2}$): $B_T = 3.0$ (Tesla), $P_{\\text{heat}} = 12.0$ (megawatts), measured $T_{e,\\text{ped}} = 520.0$ (electron-volts).\n- Case $5$ (prediction only): $B_T = 2.0$ (Tesla), $P_{\\text{heat}} = 5.0$ (megawatts), no measured value; report only $T_{e,\\text{pred}}$.\n\nAnswer specification:\n- Physical units: Express all pedestal electron temperatures in electron-volts.\n- Confidence level: Use $0.95$ with its corresponding two-sided normal quantile $z$.\n- Output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must be $[b_1,b_2,b_3,b_4,t_5]$ where $b_k$ are booleans for Cases $1$ to $4$ (true if the measured value is within the adjusted interval, false otherwise), and $t_5$ is the predicted pedestal electron temperature for Case $5$ as a float rounded to one decimal place. For example, a valid output would look like `[True,False,True,False,362.0]`.",
            "solution": "The objective is to construct a physically motivated, statistically calibrated, and algorithmically implementable cross-device scaling for the pedestal electron temperature height as a function of toroidal magnetic field and total heating power, using a multi-device profile database, and then use it to adjust validation expectations across devices.\n\nPrincipled derivation:\n\n1. Physical foundation and scaling form. In the pedestal region, conservation of energy requires that the external heating power deposited in the plasma is balanced by transport losses. The cross-field heat flux $q$ at the pedestal is determined by transport coefficients that depend on magnetic field strength and thermodynamic gradients. For a wide class of microinstability-limited transport regimes, dimensionless analysis and empirical observation support a power-law relation between the electron temperature height and control parameters such as $B_T$ and $P_{\\text{heat}}$. This leads to the hypothesis\n$$\nT_{e,\\text{ped}} \\propto B_T^{a} P_{\\text{heat}}^{b},\n$$\nwith unknown exponents $a$ and $b$ to be estimated from data. This model is universal enough to be testable across devices while grounded in the physics that stronger magnetic fields and increased heating tend to raise the temperature.\n\n2. Linearization and estimation. To estimate the parameters, apply the logarithm to both sides to obtain\n$$\n\\ln T_{e,\\text{ped}} = \\ln C + a \\ln B_T + b \\ln P_{\\text{heat}},\n$$\nwhere $C$ is a prefactor to be determined. This is a linear model in the variables $\\ln B_T$ and $\\ln P_{\\text{heat}}$. Define $y_i = \\ln T_{e,\\text{ped},i}$, $x_{1,i} = \\ln B_{T,i}$, and $x_{2,i} = \\ln P_{\\text{heat},i}$. Stack the data into a design matrix\n$$\nX = \\begin{bmatrix}\n1  x_{1,1}  x_{2,1} \\\\\n\\vdots  \\vdots  \\vdots \\\\\n1  x_{1,N}  x_{2,N}\n\\end{bmatrix}, \\quad y = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_N \\end{bmatrix}.\n$$\nEstimate $\\beta = [\\ln C, a, b]^T$ by minimizing the sum of squared residuals $\\sum_{i=1}^{N} (y_i - X_i \\beta)^2$, where $X_i$ is the $i$-th row of $X$. The least-squares solution is\n$$\n\\hat{\\beta} = (X^T X)^{-1} X^T y,\n$$\nyielding $\\widehat{\\ln C}$, $\\hat{a}$, and $\\hat{b}$. The predicted log-temperature for a new $(B_T,P_{\\text{heat}})$ pair is\n$$\n\\hat{y} = \\widehat{\\ln C} + \\hat{a} \\ln B_T + \\hat{b} \\ln P_{\\text{heat}}, \\quad T_{e,\\text{pred}} = \\exp(\\hat{y}).\n$$\n\n3. Variability decomposition across devices. Because the database spans multiple devices, systematic device-level offsets can arise. Let $r_i = y_i - X_i \\hat{\\beta}$ be residuals. For each device $d$ with index set $I_d$ and sample count $n_d$, define the device mean residual\n$$\nm_d = \\frac{1}{n_d} \\sum_{i \\in I_d} r_i,\n$$\nand the within-device residuals\n$$\nw_i = r_i - m_{d(i)},\n$$\nwhere $d(i)$ denotes the device for datum $i$. The within-device variance can be estimated by the unbiased estimator\n$$\n\\hat{\\sigma}_e^2 = \\frac{1}{N - D} \\sum_{d=1}^{D} \\sum_{i \\in I_d} w_i^2,\n$$\nwhere $D$ is the number of devices. The variation of the device means $\\{m_d\\}$ estimates the between-device variance, but each $m_d$ contains a contribution from the within-device noise $\\sigma_e^2$ scaled by $n_d^{-1}$. A corrected estimator can be obtained by subtracting the average within-device contribution:\n$$\n\\operatorname{Var}_{\\text{unb}}(m_d) = \\frac{1}{D - 1} \\sum_{d=1}^{D} \\left(m_d - \\overline{m}\\right)^2, \\quad \\overline{m} = \\frac{1}{D} \\sum_{d=1}^{D} m_d,\n$$\n$$\n\\hat{\\sigma}_d^2 = \\max\\left(0,\\ \\operatorname{Var}_{\\text{unb}}(m_d) - \\frac{1}{D} \\sum_{d=1}^{D} \\frac{\\hat{\\sigma}_e^2}{n_d} \\right).\n$$\nThe maximum with $0$ ensures a nonnegative variance estimate.\n\n4. Adjusted validation expectations. For cross-device validation at confidence level $0.95$, use a normal approximation in log-space. Let $z$ be the two-sided quantile such that the coverage of the interval $[-z,z]$ is $0.95$ for a standard normal random variable. Then define the standard deviation used in prediction as\n$$\ns_{\\text{new}} = \\sqrt{\\hat{\\sigma}_d^2 + \\hat{\\sigma}_e^2} \\quad \\text{for a new device},\n$$\n$$\ns_{\\text{existing}} = \\sqrt{\\hat{\\sigma}_e^2} \\quad \\text{for an existing device present in the database}.\n$$\nThe adjusted confidence interval in log-space is $[\\hat{y} - z s, \\hat{y} + z s]$ with $s$ chosen appropriately, and the interval in electron-volts is $[T_{\\text{low}}, T_{\\text{high}}] = [\\exp(\\hat{y} - z s), \\exp(\\hat{y} + z s)]$. A measured pedestal electron temperature $T_{e,\\text{meas}}$ is considered consistent if $T_{\\text{low}} \\le T_{e,\\text{meas}} \\le T_{\\text{high}}$.\n\nAlgorithmic implementation details:\n- Construct the database vectors $B_T$, $P_{\\text{heat}}$, $T_{e,\\text{ped}}$, and device labels.\n- Build $X$ and $y$ in log-space and solve for $\\hat{\\beta}$ using least squares.\n- Compute residuals $r_i$, group by device to compute $m_d$, $w_i$, and then $\\hat{\\sigma}_e^2$ and $\\hat{\\sigma}_d^2$ using the formulas above.\n- For each test case, compute $\\hat{y}$, select $s$ based on whether the device is new or existing, form the interval bounds $T_{\\text{low}}$ and $T_{\\text{high}}$, and compare to the measured $T_{e,\\text{meas}}$ when provided. Also compute $T_{e,\\text{pred}} = \\exp(\\hat{y})$.\n- Report booleans for Cases $1$ through $4$ and the predicted pedestal electron temperature for Case $5$ rounded to one decimal place, in a single-line list format as specified, with all temperatures in electron-volts.\n\nScientific realism: The database values span plausible pedestal electron temperatures in electron-volts across a range of toroidal magnetic fields in Tesla and heating powers in megawatts. The regression and variance decomposition are standard and well-tested for empirical scaling analysis in fusion profile databases, and the adjusted expectations are grounded in device-level variability modeling. The confidence level is specified as a decimal probability $0.95$, avoiding percent notation.\n\nThis principled approach integrates physics-motivated scaling, statistical estimation in log-space, and a validation framework that adapts expectations across devices by explicitly accounting for between-device and within-device variability.",
            "answer": "```python\nimport numpy as np\n\ndef fit_log_linear_scaling(BT, Pheat, Te):\n    \"\"\"\n    Fit ln(Te) = beta0 + a*ln(BT) + b*ln(Pheat) via least squares.\n    Returns beta0, a, b and residuals.\n    \"\"\"\n    y = np.log(Te)\n    X = np.column_stack([np.ones_like(BT), np.log(BT), np.log(Pheat)])\n    beta, *_ = np.linalg.lstsq(X, y, rcond=None)\n    residuals = y - X @ beta\n    return beta, residuals\n\ndef variance_components(residuals, devices):\n    \"\"\"\n    Compute within-device (sigma_e^2) and between-device (sigma_d^2) variance\n    components from residuals using device means and within-device residuals.\n    \"\"\"\n    # Map devices to indices\n    unique_devices = np.unique(devices)\n    D = len(unique_devices)\n    N = len(residuals)\n    m_d = []\n    n_d = []\n    device_mean = {}\n    # Compute mean residual per device\n    for d in unique_devices:\n        idx = np.where(devices == d)[0]\n        r_d = residuals[idx]\n        md = np.mean(r_d)\n        device_mean[d] = md\n        m_d.append(md)\n        n_d.append(len(idx))\n    m_d = np.array(m_d)\n    n_d = np.array(n_d)\n    # Within-device residuals\n    w = np.zeros_like(residuals)\n    for i in range(N):\n        w[i] = residuals[i] - device_mean[devices[i]]\n    # Unbiased within-device variance\n    sigma_e2 = np.sum(w**2) / (N - D)\n    # Unbiased variance of device means (unweighted)\n    m_bar = np.mean(m_d)\n    var_m_unb = np.sum((m_d - m_bar)**2) / (D - 1)\n    # Correction for finite within-device noise in means\n    avg_noise = np.mean(sigma_e2 / n_d)\n    sigma_d2 = max(0.0, var_m_unb - avg_noise)\n    return sigma_e2, sigma_d2\n\ndef predict_log(beta, BT, Pheat):\n    \"\"\"\n    Predict ln(Te) given beta and inputs.\n    \"\"\"\n    return beta[0] + beta[1]*np.log(BT) + beta[2]*np.log(Pheat)\n\ndef solve():\n    # Construct database\n    # Each entry: (device, BT [T], P_heat [MW], Te_ped [eV])\n    data = [\n        ('D1', 2.0, 5.0, 410.0),\n        ('D1', 2.5, 8.0, 535.0),\n        ('D1', 2.5, 10.0, 561.0),\n        ('D1', 3.0, 5.0, 493.0),\n        ('D1', 3.0, 8.0, 597.0),\n        ('D1', 3.5, 10.0, 672.0),\n\n        ('D2', 2.0, 6.0, 373.0),\n        ('D2', 2.5, 4.0, 354.0),\n        ('D2', 3.0, 12.0, 559.0),\n        ('D2', 3.0, 8.0, 500.0),\n        ('D2', 3.5, 6.0, 498.0),\n        ('D2', 4.0, 10.0, 612.0),\n\n        ('D3', 1.5, 5.0, 329.0),\n        ('D3', 2.0, 8.0, 435.0),\n        ('D3', 3.0, 10.0, 581.0),\n        ('D3', 3.5, 12.0, 676.0),\n        ('D3', 4.0, 5.0, 511.0),\n        ('D3', 5.0, 8.0, 708.0),\n\n        ('D4', 2.0, 3.0, 271.0),\n        ('D4', 2.5, 6.0, 394.0),\n        ('D4', 3.0, 4.0, 372.0),\n        ('D4', 3.5, 8.0, 501.0),\n        ('D4', 4.0, 6.0, 499.0),\n        ('D4', 5.0, 10.0, 660.0),\n    ]\n    devices = np.array([d for d, _, _, _ in data])\n    BT = np.array([bt for _, bt, _, _ in data], dtype=float)\n    Pheat = np.array([p for _, _, p, _ in data], dtype=float)\n    Te = np.array([te for _, _, _, te in data], dtype=float)\n\n    # Fit scaling\n    beta, residuals = fit_log_linear_scaling(BT, Pheat, Te)\n\n    # Variance components\n    sigma_e2, sigma_d2 = variance_components(residuals, devices)\n\n    # Confidence level 0.95 -> z approx 1.96 for two-sided normal\n    z = 1.96\n\n    # Test cases:\n    # Case 1: new device\n    case1 = {'type': 'new', 'BT': 2.5, 'P': 10.0, 'Te_meas': 560.0}\n    # Case 2: new device, boundary\n    case2 = {'type': 'new', 'BT': 1.5, 'P': 3.0, 'Te_meas': 210.0}\n    # Case 3: new device, edge\n    case3 = {'type': 'new', 'BT': 5.0, 'P': 8.0, 'Te_meas': 750.0}\n    # Case 4: existing device D2\n    case4 = {'type': 'existing', 'device': 'D2', 'BT': 3.0, 'P': 12.0, 'Te_meas': 520.0}\n    # Case 5: prediction only\n    case5 = {'type': 'prediction', 'BT': 2.0, 'P': 5.0}\n\n    test_cases = [case1, case2, case3, case4, case5]\n\n    results = []\n    for case in test_cases:\n        if case['type'] == 'prediction':\n            y_pred = predict_log(beta, case['BT'], case['P'])\n            Te_pred = float(np.exp(y_pred))\n            results.append(round(Te_pred, 1))\n        else:\n            y_pred = predict_log(beta, case['BT'], case['P'])\n            if case['type'] == 'new':\n                s = np.sqrt(sigma_d2 + sigma_e2)\n            elif case['type'] == 'existing':\n                # Use within-device variance only for existing device\n                s = np.sqrt(sigma_e2)\n            else:\n                s = np.sqrt(sigma_d2 + sigma_e2)\n            lower = np.exp(y_pred - z * s)\n            upper = np.exp(y_pred + z * s)\n            Te_meas = case['Te_meas']\n            ok = (Te_meas >= lower) and (Te_meas = upper)\n            results.append(ok)\n\n    # Final output format: single line, comma-separated list enclosed in brackets\n    formatted_results = [str(r) for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}