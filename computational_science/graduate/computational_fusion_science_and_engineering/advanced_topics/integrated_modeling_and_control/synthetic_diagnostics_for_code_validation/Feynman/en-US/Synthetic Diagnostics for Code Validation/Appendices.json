{
    "hands_on_practices": [
        {
            "introduction": "A fundamental task in creating a synthetic diagnostic for an imaging system is to model how the instrument blurs the true emission pattern from the plasma. This exercise  explores this through the concept of the Point-Spread Function (PSF), treating the imaging process as a convolution. By analyzing the system's response in the frequency domain via the Modulation Transfer Function (MTF), you will develop a quantitative understanding of how instrument characteristics define the achievable resolution.",
            "id": "4051667",
            "problem": "A synthetic camera diagnostic is used in computational fusion science and engineering to validate radiation transport and imaging codes by comparing synthetic images to experimental observations. Assume the instrument can be modeled as linear and shift-invariant in the image plane, so that the image of a spatially localized input depends only on the relative displacement between input and output positions. Let the emissivity field on the image plane be a scalar function $\\epsilon(\\mathbf{x})$ with $\\mathbf{x} = (x,y)$, and let the instrument response to a unit impulsive emissivity (a Dirac delta at a point) define the instrument Point-Spread Function (PSF), denoted $\\mathrm{PSF}(\\mathbf{x})$. The Optical Transfer Function (OTF) is defined as the Fourier transform of $\\mathrm{PSF}(\\mathbf{x})$, and the Modulation Transfer Function (MTF) is the magnitude of the OTF. Assume the instrument is energy conserving in the sense that $\\int \\mathrm{PSF}(\\mathbf{x})\\, d^2\\mathbf{x} = 1$.\n\nConsider an anisotropic Gaussian PSF,\n$$\n\\mathrm{PSF}(\\mathbf{x}) = \\frac{1}{2\\pi \\sigma_x \\sigma_y}\\exp\\!\\left(-\\frac{x^2}{2\\sigma_x^2} - \\frac{y^2}{2\\sigma_y^2}\\right),\n$$\nwith $\\sigma_x > 0$ and $\\sigma_y > 0$. Consider the following emissivity test patterns for synthetic diagnostic characterization:\n- A point source $\\epsilon(\\mathbf{x}) = \\delta(x)\\delta(y)$.\n- A sinusoidal grating $\\epsilon(\\mathbf{x}) = \\epsilon_0\\left[1 + C\\cos(k_x x + k_y y)\\right]$ with $0  C \\ll 1$ and spatial frequency vector $\\mathbf{k} = (k_x,k_y)$.\n\nUsing the definitions above and the fundamental properties of linear, shift-invariant imaging and Fourier analysis, determine which statements are correct regarding the synthetic image formation and the impact of PSF anisotropy on resolution metrics. Select all correct options.\n\nA. For a linear, shift-invariant instrument, the synthetic image is given by the convolution of the emissivity with the PSF: $I(\\mathbf{x}) = \\int \\epsilon(\\mathbf{x}')\\, \\mathrm{PSF}(\\mathbf{x}-\\mathbf{x}')\\, d^2\\mathbf{x}'$.\n\nB. For the point-source emissivity $\\epsilon(\\mathbf{x}) = \\delta(x)\\delta(y)$, the synthetic image equals the PSF: $I(\\mathbf{x}) = \\mathrm{PSF}(\\mathbf{x})$.\n\nC. For the sinusoidal grating $\\epsilon(\\mathbf{x}) = \\epsilon_0\\left[1 + C\\cos(k_x x + k_y y)\\right]$, the image contrast (the amplitude of the cosine component relative to the direct-current level) is $C$ times the Modulation Transfer Function evaluated at $\\mathbf{k}$, and for the anisotropic Gaussian PSF this equals $\\exp\\!\\left[-\\tfrac{1}{2}\\left(\\sigma_x^2 k_x^2 + \\sigma_y^2 k_y^2\\right)\\right]$.\n\nD. The Full Width at Half Maximum (FWHM) of the image of a point source, measured along the $x$ axis and $y$ axis, is $2\\sqrt{2\\ln 2}\\,\\sigma_x$ and $2\\sqrt{2\\ln 2}\\,\\sigma_y$, respectively.\n\nE. When the PSF is anisotropic, a single isotropic resolution metric constructed by angle-averaging the Modulation Transfer Function over direction at fixed $|\\mathbf{k}|$ generally overestimates the resolution in the poorest-resolving direction.\n\nF. Anisotropy in the PSF does not affect the magnitude of the Optical Transfer Function, only its phase, so the Modulation Transfer Function remains isotropic.\n\nG. For the anisotropic Gaussian PSF, the minus three decibel spatial frequencies along the principal axes (defined by the wave numbers where the Modulation Transfer Function equals $1/2$ for one-dimensional patterns aligned with the axes) are $k_x^{(-3\\mathrm{dB})} = \\sqrt{2\\ln 2}/\\sigma_x$ and $k_y^{(-3\\mathrm{dB})} = \\sqrt{2\\ln 2}/\\sigma_y$, respectively, demonstrating direction-dependent resolution limits.",
            "solution": "The problem statement has been validated and found to be scientifically grounded, well-posed, objective, and internally consistent. It presents a standard problem in linear systems theory and Fourier optics applied to the characterization of an imaging system. All definitions and provided data are standard and sufficient for a rigorous evaluation of the presented statements. We may, therefore, proceed with the analysis.\n\nA. For a linear, shift-invariant instrument, the synthetic image is given by the convolution of the emissivity with the PSF: $I(\\mathbf{x}) = \\int \\epsilon(\\mathbf{x}')\\, \\mathrm{PSF}(\\mathbf{x}-\\mathbf{x}')\\, d^2\\mathbf{x}'$.\n\nThis statement is the mathematical definition of the output of a linear, shift-invariant (LSI) system. In imaging science, the object's property being imaged (here, emissivity $\\epsilon(\\mathbf{x})$) is the input signal. The instrument is characterized by its response to a point impulse, which is the Point-Spread Function ($\\mathrm{PSF}$). For an LSI system, the image $I(\\mathbf{x})$ is the convolution of the input signal with the system's impulse response. The integral presented, $I(\\mathbf{x}) = (\\epsilon * \\mathrm{PSF})(\\mathbf{x}) = \\int \\epsilon(\\mathbf{x}')\\, \\mathrm{PSF}(\\mathbf{x}-\\mathbf{x}')\\, d^2\\mathbf{x}'$, is the correct mathematical expression for this convolution operation in two dimensions.\n\n**Verdict: Correct.**\n\nB. For the point-source emissivity $\\epsilon(\\mathbf{x}) = \\delta(x)\\delta(y)$, the synthetic image equals the PSF: $I(\\mathbf{x}) = \\mathrm{PSF}(\\mathbf{x})$.\n\nUsing the convolution integral from statement A with the point-source emissivity $\\epsilon(\\mathbf{x}) = \\delta(\\mathbf{x})$, we compute the image $I(\\mathbf{x})$:\n$$\nI(\\mathbf{x}) = \\int \\delta(\\mathbf{x}')\\, \\mathrm{PSF}(\\mathbf{x}-\\mathbf{x}')\\, d^2\\mathbf{x}'\n$$\nThis integral is an application of the sifting property of the Dirac delta function, which states that $\\int f(\\mathbf{z}) \\delta(\\mathbf{z}-\\mathbf{a})\\,d\\mathbf{z} = f(\\mathbf{a})$. In our case, the integration is over $\\mathbf{x}'$, the delta function is centered at $\\mathbf{x}'=0$, and the function being sifted is $\\mathrm{PSF}(\\mathbf{x}-\\mathbf{x}')$.\n$$\nI(\\mathbf{x}) = \\mathrm{PSF}(\\mathbf{x}-0) = \\mathrm{PSF}(\\mathbf{x})\n$$\nThis result is consistent with the definition of the PSF as the image of a point source.\n\n**Verdict: Correct.**\n\nC. For the sinusoidal grating $\\epsilon(\\mathbf{x}) = \\epsilon_0\\left[1 + C\\cos(k_x x + k_y y)\\right]$, the image contrast (the amplitude of the cosine component relative to the direct-current level) is $C$ times the Modulation Transfer Function evaluated at $\\mathbf{k}$, and for the anisotropic Gaussian PSF this equals $\\exp\\!\\left[-\\tfrac{1}{2}\\left(\\sigma_x^2 k_x^2 + \\sigma_y^2 k_y^2\\right)\\right]$.\n\nIn the Fourier domain, convolution becomes multiplication: $\\mathcal{F}\\{I(\\mathbf{x})\\} = \\mathcal{F}\\{\\epsilon(\\mathbf{x})\\} \\cdot \\mathcal{F}\\{\\mathrm{PSF}(\\mathbf{x})\\}$, or $\\hat{I}(\\mathbf{k}) = \\hat{\\epsilon}(\\mathbf{k}) \\cdot \\mathrm{OTF}(\\mathbf{k})$.\nThe input emissivity is $\\epsilon(\\mathbf{x}) = \\epsilon_0 + \\epsilon_0 C \\cos(\\mathbf{k}_0 \\cdot \\mathbf{x})$, where $\\mathbf{k}_0 = (k_x, k_y)$ is the fixed spatial frequency of the grating. Using the identity $\\cos(\\theta) = \\frac{1}{2}(e^{i\\theta} + e^{-i\\theta})$, the Fourier transform of $\\epsilon(\\mathbf{x})$ is:\n$$\n\\hat{\\epsilon}(\\mathbf{k}) = \\epsilon_0 (2\\pi)^2 \\delta(\\mathbf{k}) + \\frac{\\epsilon_0 C (2\\pi)^2}{2} \\left[ \\delta(\\mathbf{k}-\\mathbf{k}_0) + \\delta(\\mathbf{k}+\\mathbf{k}_0) \\right]\n$$\nThe Fourier transform of the image is then:\n$$\n\\hat{I}(\\mathbf{k}) = \\hat{\\epsilon}(\\mathbf{k}) \\mathrm{OTF}(\\mathbf{k}) = \\epsilon_0 (2\\pi)^2 \\delta(\\mathbf{k})\\mathrm{OTF}(0) + \\frac{\\epsilon_0 C (2\\pi)^2}{2} \\left[ \\delta(\\mathbf{k}-\\mathbf{k}_0)\\mathrm{OTF}(\\mathbf{k}_0) + \\delta(\\mathbf{k}+\\mathbf{k}_0)\\mathrm{OTF}(-\\mathbf{k}_0) \\right]\n$$\nThe problem states $\\int \\mathrm{PSF}(\\mathbf{x})\\, d^2\\mathbf{x} = 1$, which means $\\mathrm{OTF}(0) = 1$. Since $\\mathrm{PSF}(\\mathbf{x})$ is a real function, $\\mathrm{OTF}(-\\mathbf{k}) = \\mathrm{OTF}^*(\\mathbf{k})$. Taking the inverse Fourier transform gives the image:\n$$\nI(\\mathbf{x}) = \\epsilon_0 \\mathrm{OTF}(0) + \\frac{\\epsilon_0 C}{2} \\left[ \\mathrm{OTF}(\\mathbf{k}_0)e^{i\\mathbf{k}_0 \\cdot \\mathbf{x}} + \\mathrm{OTF}^*(\\mathbf{k}_0)e^{-i\\mathbf{k}_0 \\cdot \\mathbf{x}} \\right]\n$$\nLetting $\\mathrm{OTF}(\\mathbf{k}_0) = \\mathrm{MTF}(\\mathbf{k}_0)e^{i\\phi(\\mathbf{k}_0)}$, this simplifies to:\n$$\nI(\\mathbf{x}) = \\epsilon_0 \\left[ 1 + C \\cdot \\mathrm{MTF}(\\mathbf{k}_0) \\cos(\\mathbf{k}_0 \\cdot \\mathbf{x} + \\phi(\\mathbf{k}_0)) \\right]\n$$\nThe image contrast is the amplitude of the oscillatory part, $C \\cdot \\mathrm{MTF}(\\mathbf{k}_0)$, relative to the DC level ($1$). So the contrast is $C \\cdot \\mathrm{MTF}(\\mathbf{k}_0)$.\nNow we calculate the OTF for the given Gaussian PSF: $\\mathrm{PSF}(\\mathbf{x}) = \\frac{1}{2\\pi \\sigma_x \\sigma_y}\\exp(-\\frac{x^2}{2\\sigma_x^2} - \\frac{y^2}{2\\sigma_y^2})$. This is a separable product of two 1D Gaussians. The Fourier transform of $\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-z^2/(2\\sigma^2)}$ is $e^{-k^2\\sigma^2/2}$. Thus,\n$$\n\\mathrm{OTF}(\\mathbf{k}) = \\mathcal{F}_x\\left\\{\\frac{1}{\\sqrt{2\\pi}\\sigma_x}e^{-x^2/(2\\sigma_x^2)}\\right\\} \\mathcal{F}_y\\left\\{\\frac{1}{\\sqrt{2\\pi}\\sigma_y}e^{-y^2/(2\\sigma_y^2)}\\right\\} = e^{-k_x^2\\sigma_x^2/2} e^{-k_y^2\\sigma_y^2/2} = \\exp\\left[-\\frac{1}{2}(k_x^2\\sigma_x^2 + k_y^2\\sigma_y^2)\\right]\n$$\nThis OTF is real and positive, so $\\mathrm{MTF}(\\mathbf{k}) = |\\mathrm{OTF}(\\mathbf{k})| = \\mathrm{OTF}(\\mathbf{k})$. The statement asserts that the image contrast is $C$ times the MTF, and that this MTF is given by the derived expression. Both parts are correct.\n\n**Verdict: Correct.**\n\nD. The Full Width at Half Maximum (FWHM) of the image of a point source, measured along the $x$ axis and $y$ axis, is $2\\sqrt{2\\ln 2}\\,\\sigma_x$ and $2\\sqrt{2\\ln 2}\\,\\sigma_y$, respectively.\n\nFrom B, the image of a point source is $I(\\mathbf{x}) = \\mathrm{PSF}(\\mathbf{x})$. The maximum intensity is at $\\mathbf{x}=0$, $I_{max} = \\frac{1}{2\\pi\\sigma_x\\sigma_y}$. We seek the width where the intensity is $I_{max}/2$.\nAlong the $x$-axis ($y=0$), we solve $I(x,0) = I_{max}/2$:\n$$\n\\frac{1}{2\\pi \\sigma_x \\sigma_y} \\exp\\left(-\\frac{x^2}{2\\sigma_x^2}\\right) = \\frac{1}{2} \\left(\\frac{1}{2\\pi \\sigma_x \\sigma_y}\\right)\n$$\n$$\n\\exp\\left(-\\frac{x^2}{2\\sigma_x^2}\\right) = \\frac{1}{2} \\implies -\\frac{x^2}{2\\sigma_x^2} = \\ln\\left(\\frac{1}{2}\\right) = -\\ln 2\n$$\n$$\nx^2 = 2\\sigma_x^2 \\ln 2 \\implies x = \\pm \\sigma_x \\sqrt{2\\ln 2}\n$$\nThe Full Width is the distance between these two points: $\\mathrm{FWHM}_x = \\sigma_x \\sqrt{2\\ln 2} - (-\\sigma_x \\sqrt{2\\ln 2}) = 2\\sigma_x\\sqrt{2\\ln 2}$.\nBy symmetry, the calculation for the $y$-axis yields $\\mathrm{FWHM}_y = 2\\sigma_y\\sqrt{2\\ln 2}$. The statement is accurate.\n\n**Verdict: Correct.**\n\nE. When the PSF is anisotropic, a single isotropic resolution metric constructed by angle-averaging the Modulation Transfer Function over direction at fixed $|\\mathbf{k}|$ generally overestimates the resolution in the poorest-resolving direction.\n\nFor the anisotropic Gaussian PSF, with $\\sigma_x \\neq \\sigma_y$, the MTF is $\\mathrm{MTF}(\\mathbf{k}) = \\exp[-\\frac{1}{2}(k_x^2\\sigma_x^2 + k_y^2\\sigma_y^2)]$. Resolution is better when the MTF remains high at larger spatial frequencies $|\\mathbf{k}|$. The \"poorest-resolving direction\" is the one where the MTF falls off most rapidly. Let's assume $\\sigma_x  \\sigma_y$. The PSF is widest along the $x$-axis, so resolution is poorest in that direction. In k-space, this corresponds to the $k_x$-axis $(\\theta=0)$, where the MTF is $\\exp[-\\frac{1}{2}|\\mathbf{k}|^2\\sigma_x^2]$. The best resolution is along the $y$-axis, corresponding to the $k_y$-axis $(\\theta=\\pi/2)$, with MTF equal to $\\exp[-\\frac{1}{2}|\\mathbf{k}|^2\\sigma_y^2]$.\nThe angle-averaged MTF, $\\langle\\mathrm{MTF}\\rangle (|\\mathbf{k}|)$, is the average of $\\mathrm{MTF}(|\\mathbf{k}|, \\theta)$ over all angles $\\theta$. Since the function being averaged is not constant, its average must lie strictly between its minimum and maximum values. For any $|\\mathbf{k}|  0$:\n$$\n\\mathrm{MTF}_{poor}  \\langle\\mathrm{MTF}\\rangle  \\mathrm{MTF}_{best}\n$$\nwhere $\\mathrm{MTF}_{poor}$ is the MTF in the poorest-resolving direction. Since a higher MTF value corresponds to better resolution, the resolution metric based on $\\langle\\mathrm{MTF}\\rangle$ will be higher than the actual resolution in the worst direction. Therefore, it \"overestimates the resolution in the poorest-resolving direction.\"\n\n**Verdict: Correct.**\n\nF. Anisotropy in the PSF does not affect the magnitude of the Optical Transfer Function, only its phase, so the Modulation Transfer Function remains isotropic.\n\nThis statement is fundamentally incorrect. Anisotropy in the PSF ($\\mathrm{PSF}(\\mathbf{x})$ is not a function of $|\\mathbf{x}|$) directly causes anisotropy in the OTF and MTF. As derived in C, the MTF for our anisotropic Gaussian PSF is:\n$$\n\\mathrm{MTF}(\\mathbf{k}) = \\exp\\left[-\\frac{1}{2}(\\sigma_x^2 k_x^2 + \\sigma_y^2 k_y^2)\\right]\n$$\nIf $\\sigma_x \\neq \\sigma_y$, this function is not solely a function of $|\\mathbf{k}| = \\sqrt{k_x^2+k_y^2}$, and is therefore anisotropic. Its level sets are ellipses, not circles. For instance, along the $k_x$-axis, $\\mathrm{MTF}(k,0) = e^{-k^2\\sigma_x^2/2}$, while along the $k_y$-axis, $\\mathrm{MTF}(0,k) = e^{-k^2\\sigma_y^2/2}$. These are different, proving the MTF is anisotropic. The premise that anisotropy only affects the phase is also false; for a symmetric real PSF like the one given, the OTF is purely real (zero phase), and all anisotropy is in its magnitude.\n\n**Verdict: Incorrect.**\n\nG. For the anisotropic Gaussian PSF, the minus three decibel spatial frequencies along the principal axes (defined by the wave numbers where the Modulation Transfer Function equals $1/2$ for one-dimensional patterns aligned with the axes) are $k_x^{(-3\\mathrm{dB})} = \\sqrt{2\\ln 2}/\\sigma_x$ and $k_y^{(-3\\mathrm{dB})} = \\sqrt{2\\ln 2}/\\sigma_y$, respectively, demonstrating direction-dependent resolution limits.\n\nThe statement provides an explicit definition for the \"minus three decibel\" frequency: the wavenumber $k$ where $\\mathrm{MTF}(k) = 1/2$. While the standard definition of $-3\\,\\mathrm{dB}$ corresponds to an amplitude ratio of $1/\\sqrt{2}$, we must follow the definition given in the problem.\nWe need to find the frequency $k_x$ along the $x$-axis ($k_y=0$) where the MTF equals $1/2$.\n$$\n\\mathrm{MTF}(k_x, 0) = \\exp\\left(-\\frac{1}{2}\\sigma_x^2 k_x^2\\right) = \\frac{1}{2}\n$$\nTaking the natural logarithm of both sides:\n$$\n-\\frac{1}{2}\\sigma_x^2 k_x^2 = \\ln\\left(\\frac{1}{2}\\right) = -\\ln 2\n$$\n$$\n\\sigma_x^2 k_x^2 = 2\\ln 2 \\implies k_x^2 = \\frac{2\\ln 2}{\\sigma_x^2}\n$$\nTaking the positive root for the spatial frequency, we get $k_x^{(-3\\mathrm{dB})} = \\frac{\\sqrt{2\\ln 2}}{\\sigma_x}$.\nThe calculation for the $y$-axis is analogous and yields $k_y^{(-3\\mathrm{dB})} = \\frac{\\sqrt{2\\ln 2}}{\\sigma_y}$.\nThe derived expressions match the statement perfectly. Since $\\sigma_x$ and $\\sigma_y$ can be different, these frequency limits are direction-dependent, as the statement correctly concludes.\n\n**Verdict: Correct.**",
            "answer": "$$\\boxed{ABCDEG}$$"
        },
        {
            "introduction": "Moving from static imaging to time-dependent measurements, this practice requires you to build a synthetic foil bolometer from the ground up . You will translate physical principles—energy conservation, thermal inertia, and electronic filtering—into a system of ordinary differential equations. By implementing a numerical solution, you will simulate the full response of the instrument to various radiation signals, a key step in creating a realistic virtual diagnostic.",
            "id": "4051674",
            "problem": "You are tasked with constructing a synthetic diagnostic for a Foil Bolometer (FB) that includes thermal inertia, suitable for validating a computational radiation transport code. A foil bolometer measures incident radiated power by converting absorbed radiation into heat, changing the foil temperature, which is subsequently converted into an electrical signal by an electronic readout chain. Your program must model the foil temperature evolution and the measured voltage using a physically grounded approach that starts from first principles and widely accepted definitions, without relying on shortcut formulas.\n\nBegin from fundamental bases:\n- Energy conservation: the rate of change of internal energy in the foil equals the absorbed power minus the thermal loss to the environment.\n- Definition of heat capacity and thermal resistance: the thermal inertial response of the foil depends on its heat capacity, and its cooling depends on a thermal resistance to the environment.\n- A first-order low-pass filter (LPF) representation for electronics: the electronic readout chain is well approximated by a first-order linear filter.\n\nUse the following physically consistent and realistic constants for the foil material:\n- Gold foil thickness $d = 10\\times 10^{-6}\\,\\mathrm{m}$,\n- Foil area $A = 1\\times 10^{-4}\\,\\mathrm{m}^2$,\n- Gold mass density $\\rho = 19300\\,\\mathrm{kg/m^3}$,\n- Specific heat capacity $c = 129\\,\\mathrm{J/(kg\\cdot K)}$.\n\nThe environment baseline temperature is fixed at $T_0 = 300\\,\\mathrm{K}$ for all tests.\n\nModel requirements to be derived and implemented:\n- Compute the foil heat capacity $C$ from the material properties.\n- Derive the governing ordinary differential equation (ODE) for foil temperature $T(t)$ using energy conservation with a linear thermal loss term.\n- Derive and implement the transfer from temperature to measured voltage using a first-order electronic filter with time constant $\\tau_e$ and a temperature-to-voltage sensitivity $k_T$.\n- The absorbed power is $P_{\\text{abs}}(t) = \\eta P_{\\text{rad}}(t)$, where $\\eta$ is a dimensionless absorption factor and $P_{\\text{rad}}(t)$ is the incident radiated power in $\\mathrm{W}$.\n\nNumerical requirements:\n- Use a fixed time step for explicit time-stepping and propagate the coupled ODEs for $T(t)$ and the measured voltage $v(t)$ over the specified duration.\n- Initial conditions: $T(0)=T_0$ and $v(0)=0$.\n- For each test case, compute three metrics:\n    1. $v_{\\text{final}}$: the measured voltage at the final simulation time, in volts,\n    2. $v_{\\text{peak}}$: the maximum measured voltage over the simulation interval, in volts,\n    3. $t_{90}$: the first time in seconds when the measured voltage reaches $0.9$ times $v_{\\text{final}}$. If $v_{\\text{final}}\\le 0$, report $t_{90}=-1.0$ seconds.\n\nExpress all voltages in volts and all times in seconds. Angles are not used. Do not attach units to numeric outputs; they are implied by this specification.\n\nTest suite:\nImplement the following four test cases, each fully specified by a radiated power profile $P_{\\text{rad}}(t)$, absorption factor $\\eta$, thermal resistance $R_{\\text{th}}$, electronic time constant $\\tau_e$, sensitivity $k_T$, time step $\\Delta t$, and end time $T_{\\text{end}}$.\n\n- Case $1$ (step input, nominal):\n    - $P_{\\text{rad}}(t) = P_0\\, H(t-t_s)$, where $H$ is the Heaviside step, $P_0 = 2.0\\,\\mathrm{W}$, $t_s = 0.2\\,\\mathrm{s}$.\n    - $\\eta = 0.35$, $R_{\\text{th}} = 120\\,\\mathrm{K/W}$, $\\tau_e = 0.05\\,\\mathrm{s}$, $k_T = 7.0\\times 10^{-3}\\,\\mathrm{V/K}$.\n    - $\\Delta t = 1.0\\times 10^{-3}\\,\\mathrm{s}$, $T_{\\text{end}} = 2.0\\,\\mathrm{s}$.\n\n- Case $2$ (ramp input):\n    - $P_{\\text{rad}}(t)$ increases linearly from $0\\,\\mathrm{W}$ at $t=0.2\\,\\mathrm{s}$ to $1.0\\,\\mathrm{W}$ at $t=1.2\\,\\mathrm{s}$, stays at $1.0\\,\\mathrm{W}$ thereafter; prior to $t=0.2\\,\\mathrm{s}$ it is $0\\,\\mathrm{W}$.\n    - $\\eta = 0.35$, $R_{\\text{th}} = 80\\,\\mathrm{K/W}$, $\\tau_e = 0.02\\,\\mathrm{s}$, $k_T = 5.0\\times 10^{-3}\\,\\mathrm{V/K}$.\n    - $\\Delta t = 1.0\\times 10^{-3}\\,\\mathrm{s}$, $T_{\\text{end}} = 2.0\\,\\mathrm{s}$.\n\n- Case $3$ (Gaussian pulse):\n    - $P_{\\text{rad}}(t) = P_0 \\exp\\!\\left(-\\frac{1}{2}\\left(\\frac{t-t_c}{\\sigma}\\right)^2\\right)$ with $P_0 = 5.0\\,\\mathrm{W}$, $t_c = 0.5\\,\\mathrm{s}$, $\\sigma = 0.02\\,\\mathrm{s}$.\n    - $\\eta = 0.30$, $R_{\\text{th}} = 200\\,\\mathrm{K/W}$, $\\tau_e = 0.10\\,\\mathrm{s}$, $k_T = 1.0\\times 10^{-2}\\,\\mathrm{V/K}$.\n    - $\\Delta t = 5.0\\times 10^{-4}\\,\\mathrm{s}$, $T_{\\text{end}} = 1.5\\,\\mathrm{s}$.\n\n- Case $4$ (zero input baseline):\n    - $P_{\\text{rad}}(t) = 0$ for all $t$.\n    - $\\eta = 0.30$, $R_{\\text{th}} = 100\\,\\mathrm{K/W}$, $\\tau_e = 0.05\\,\\mathrm{s}$, $k_T = 5.0\\times 10^{-3}\\,\\mathrm{V/K}$.\n    - $\\Delta t = 1.0\\times 10^{-3}\\,\\mathrm{s}$, $T_{\\text{end}} = 1.0\\,\\mathrm{s}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a sub-list of three floats $[v_{\\text{final}}, v_{\\text{peak}}, t_{90}]$. For example, output in the form $[[x_1,y_1,z_1],[x_2,y_2,z_2],[x_3,y_3,z_3],[x_4,y_4,z_4]]$.",
            "solution": "The problem requires the construction of a synthetic diagnostic for a foil bolometer, starting from fundamental physical principles. This involves deriving and numerically solving a system of coupled ordinary differential equations (ODEs) that model the evolution of the bolometer foil's temperature and the resulting measured voltage.\n\nThe derivation and numerical solution strategy are as follows:\n\nFirst, we establish the physical constants of the system. The foil's heat capacity, $C$, is a crucial parameter for its thermal response. It is derived from the given material properties: gold foil thickness $d = 10 \\times 10^{-6}\\,\\mathrm{m}$, area $A = 1 \\times 10^{-4}\\,\\mathrm{m}^2$, mass density $\\rho = 19300\\,\\mathrm{kg/m^3}$, and specific heat capacity $c = 129\\,\\mathrm{J/(kg\\cdot K)}$.\nThe volume of the foil is $V = A d$.\n$V = (1 \\times 10^{-4}\\,\\mathrm{m}^2) \\times (10 \\times 10^{-6}\\,\\mathrm{m}) = 1 \\times 10^{-9}\\,\\mathrm{m}^3$.\nThe mass of the foil is $m = \\rho V$.\n$m = (19300\\,\\mathrm{kg/m^3}) \\times (1 \\times 10^{-9}\\,\\mathrm{m}^3) = 1.93 \\times 10^{-5}\\,\\mathrm{kg}$.\nThe heat capacity $C$ is the product of mass and specific heat capacity, $C = m c$.\n$C = (1.93 \\times 10^{-5}\\,\\mathrm{kg}) \\times (129\\,\\mathrm{J/(kg\\cdot K)}) = 2.4897 \\times 10^{-3}\\,\\mathrm{J/K}$.\n\nNext, we derive the governing ODE for the foil temperature, $T(t)$, based on the principle of energy conservation. The rate of change of the foil's internal energy, $C \\frac{dT(t)}{dt}$, is equal to the net power absorbed. This net power is the difference between the absorbed radiated power, $P_{\\text{abs}}(t)$, and the power lost to the surrounding environment, $P_{\\text{loss}}(t)$.\nThe absorbed power is given by $P_{\\text{abs}}(t) = \\eta P_{\\text{rad}}(t)$, where $\\eta$ is the absorption factor and $P_{\\text{rad}}(t)$ is the incident radiated power.\nThe thermal loss is modeled using a linear thermal resistance $R_{\\text{th}}$, analogous to Ohm's law, where the heat flow (power) is the temperature difference divided by the resistance: $P_{\\text{loss}}(t) = \\frac{T(t) - T_0}{R_{\\text{th}}}$, where $T_0 = 300\\,\\mathrm{K}$ is the constant environment temperature.\nThe energy balance equation is:\n$$C \\frac{dT(t)}{dt} = P_{\\text{abs}}(t) - P_{\\text{loss}}(t) = \\eta P_{\\text{rad}}(t) - \\frac{T(t) - T_0}{R_{\\text{th}}}$$\nRearranging this into the standard form for a first-order ODE gives:\n$$\\frac{dT(t)}{dt} = \\frac{\\eta P_{\\text{rad}}(t)}{C} - \\frac{T(t) - T_0}{R_{\\text{th}} C}$$\nWe can define the thermal time constant of the foil as $\\tau_{\\text{th}} = R_{\\text{th}} C$. The ODE for temperature becomes:\n$$\\frac{dT(t)}{dt} = \\frac{T_0 - T(t)}{\\tau_{\\text{th}}} + \\frac{\\eta P_{\\text{rad}}(t)}{C}$$\n\nThe second part of the model describes the electronic readout chain, which converts the foil temperature into a measured voltage, $v(t)$. This process is modeled as a first-order low-pass filter (LPF). The \"ideal\" voltage signal, which is directly proportional to the temperature rise of the foil above the baseline, is $v_{\\text{ideal}}(t) = k_T (T(t) - T_0)$, where $k_T$ is the temperature-to-voltage sensitivity in $\\mathrm{V/K}$.\nA first-order LPF with an electronic time constant $\\tau_e$ relates the output voltage $v(t)$ to the input voltage $v_{\\text{ideal}}(t)$ by the following ODE:\n$$\\tau_e \\frac{dv(t)}{dt} + v(t) = v_{\\text{ideal}}(t) = k_T (T(t) - T_0)$$\nRearranging for the derivative of the measured voltage gives the second ODE:\n$$\\frac{dv(t)}{dt} = \\frac{k_T (T(t) - T_0) - v(t)}{\\tau_e}$$\n\nWe now have a system of two coupled first-order ODEs for the state variables $T(t)$ and $v(t)$:\n1. $\\frac{dT}{dt} = \\frac{T_0 - T}{\\tau_{\\text{th}}} + \\frac{\\eta P_{\\text{rad}}(t)}{C}$\n2. $\\frac{dv}{dt} = \\frac{k_T (T - T_0) - v}{\\tau_e}$\n\nThe problem specifies initial conditions $T(0) = T_0 = 300\\,\\mathrm{K}$ and $v(0) = 0\\,\\mathrm{V}$.\n\nTo solve this system numerically, we employ the explicit Forward Euler method with a fixed time step $\\Delta t$. Let $t_n = n \\Delta t$ be the time at the $n$-th step, with $T_n = T(t_n)$ and $v_n = v(t_n)$. The values at the next time step, $t_{n+1} = (n+1) \\Delta t$, are calculated as $T_{n+1} = T_n + \\Delta t \\left( \\frac{T_0 - T_n}{\\tau_{\\text{th}}} + \\frac{\\eta P_{\\text{rad}}(t_n)}{C} \\right)$ and $v_{n+1} = v_n + \\Delta t \\left( \\frac{k_T (T_n - T_0) - v_n}{\\tau_e} \\right)$. This iterative process is repeated from $t=0$ to $t=T_{\\text{end}}$ to generate the time series for $T(t)$ and $v(t)$.\n\nFinally, from the computed voltage time series $v(t)$, we extract the required metrics for each test case:\n1.  $v_{\\text{final}}$: The voltage at the final time step, $v(T_{\\text{end}})$.\n2.  $v_{\\text{peak}}$: The maximum voltage value attained over the entire simulation interval, $\\max_{t \\in [0, T_{\\text{end}}]} v(t)$.\n3.  $t_{90}$: The first time $t$ at which the voltage reaches or exceeds $0.9$ times the final voltage, i.e., $v(t) \\ge 0.9 v_{\\text{final}}$. If $v_{\\text{final}} \\le 0$, $t_{90}$ is defined as $-1.0\\,\\mathrm{s}$. The numerical implementation will search for the first time step where this condition is met.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the foil bolometer synthetic diagnostic problem for all test cases.\n    \"\"\"\n\n    # Physical constants for the gold foil\n    d = 10e-6  # Foil thickness in m\n    A = 1e-4   # Foil area in m^2\n    rho = 19300  # Gold mass density in kg/m^3\n    c = 129    # Specific heat capacity in J/(kg*K)\n\n    # Derived constant: foil heat capacity\n    C_foil = rho * A * d * c  # in J/K\n\n    # Environment baseline temperature\n    T0 = 300.0  # in K\n\n    test_cases = [\n        # Case 1 (step input, nominal)\n        {\n            \"id\": 1,\n            \"power_profile\": lambda t: 2.0 if t = 0.2 else 0.0,\n            \"eta\": 0.35, \"R_th\": 120.0, \"tau_e\": 0.05, \"k_T\": 7.0e-3,\n            \"dt\": 1.0e-3, \"T_end\": 2.0,\n        },\n        # Case 2 (ramp input)\n        {\n            \"id\": 2,\n            \"power_profile\": lambda t: 0.0 if t  0.2 else (1.0 * (t - 0.2) if t  1.2 else 1.0),\n            \"eta\": 0.35, \"R_th\": 80.0, \"tau_e\": 0.02, \"k_T\": 5.0e-3,\n            \"dt\": 1.0e-3, \"T_end\": 2.0,\n        },\n        # Case 3 (Gaussian pulse)\n        {\n            \"id\": 3,\n            \"power_profile\": lambda t: 5.0 * np.exp(-0.5 * ((t - 0.5) / 0.02)**2),\n            \"eta\": 0.30, \"R_th\": 200.0, \"tau_e\": 0.10, \"k_T\": 1.0e-2,\n            \"dt\": 5.0e-4, \"T_end\": 1.5,\n        },\n        # Case 4 (zero input baseline)\n        {\n            \"id\": 4,\n            \"power_profile\": lambda t: 0.0,\n            \"eta\": 0.30, \"R_th\": 100.0, \"tau_e\": 0.05, \"k_T\": 5.0e-3,\n            \"dt\": 1.0e-3, \"T_end\": 1.0,\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        # Unpack parameters\n        P_rad_func = case[\"power_profile\"]\n        eta = case[\"eta\"]\n        R_th = case[\"R_th\"]\n        tau_e = case[\"tau_e\"]\n        k_T = case[\"k_T\"]\n        dt = case[\"dt\"]\n        T_end = case[\"T_end\"]\n\n        # Calculate thermal time constant\n        tau_th = R_th * C_foil\n\n        # Set up simulation time grid and state vectors\n        num_steps = int(T_end / dt)\n        times = np.linspace(0, T_end, num_steps + 1)\n        T = np.zeros(num_steps + 1)\n        v = np.zeros(num_steps + 1)\n\n        # Initial conditions\n        T[0] = T0\n        v[0] = 0.0\n\n        # Perform explicit time-stepping (Forward Euler)\n        for i in range(num_steps):\n            t_i = times[i]\n            T_i = T[i]\n            v_i = v[i]\n            \n            P_rad_i = P_rad_func(t_i)\n            \n            # ODE for temperature\n            dT_dt = (T0 - T_i) / tau_th + (eta * P_rad_i) / C_foil\n            \n            # ODE for measured voltage\n            dv_dt = (k_T * (T_i - T0) - v_i) / tau_e\n            \n            # Update states\n            T[i+1] = T_i + dt * dT_dt\n            v[i+1] = v_i + dt * dv_dt\n\n        # Calculate metrics\n        v_final = v[-1]\n        v_peak = np.max(v)\n        \n        t_90 = -1.0\n        if v_final  0:\n            v_90_target = 0.9 * v_final\n            # Find first index where voltage = target\n            indices = np.where(v = v_90_target)[0]\n            if indices.size  0:\n                first_index = indices[0]\n                t_90 = times[first_index]\n\n        results.append([v_final, v_peak, t_90])\n    \n    # Final print statement in the exact required format\n    # Using map and join to correctly format the list of lists as a string\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The ultimate goal of a synthetic diagnostic is to enable a quantitative comparison between simulation and experiment. This practice  focuses on the statistical heart of this comparison: the chi-square ($ \\chi^2 $) test. You will explore how the $ \\chi^2 $ statistic is derived from a Gaussian likelihood and, crucially, how to generalize it for the common and complex case of correlated measurement errors, ensuring a statistically rigorous validation.",
            "id": "4051633",
            "problem": "A synthetic diagnostic pipeline in computational fusion science and engineering produces predicted measurement vectors $ \\mu(\\theta) \\in \\mathbb{R}^N $ from a simulation state parameterized by $ \\theta $, to be compared with measured diagnostic data $ y \\in \\mathbb{R}^N $. Suppose measurement errors arise from instrumentation and modeling imperfections and, in general, may be correlated across channels (for example, due to common-mode calibration drift and shared line-of-sight geometry in line-integrated diagnostics). Starting from the first-principles definition of the Gaussian probability density function (PDF) for measurement errors and the construction of a likelihood for $ y $ given $ \\mu(\\theta) $, select all statements that correctly characterize the chi-square statistic $ \\chi^2 $ used for code validation, its assumptions, and its limitations when errors are correlated.\n\nA. Under independent Gaussian errors with known per-channel standard deviations $ \\sigma_i $, the negative log-likelihood (up to an additive constant) reduces to a weighted sum of squared normalized residuals, and the corresponding $ \\chi^2 $ equals $ \\sum_{i=1}^{N} \\left[ \\dfrac{y_i - \\mu_i}{\\sigma_i} \\right]^2 $.\n\nB. If the errors have known covariance matrix $ \\Sigma \\in \\mathbb{R}^{N \\times N} $, the appropriate generalization is $ \\chi^2 = (y - \\mu)^{\\top} \\Sigma (y - \\mu) $, i.e., weight by the covariance directly, and independence is recovered by choosing $ \\Sigma $ diagonal with entries $ \\sigma_i^2 $.\n\nC. When errors are correlated, using the diagonal-$ \\sigma_i $ formula for $ \\chi^2 $ tends to understate uncertainty in $ \\chi^2 $ and can make a model look spuriously bad; the correct remedy is to include $ \\Sigma^{-1} $ in the quadratic form and also account for the normalization term $ \\ln |\\Sigma| $ when comparing models with different implied covariances or when estimating $ \\Sigma $ from the data.\n\nD. In the presence of a common-mode calibration drift modeled as $ \\epsilon = \\delta c\\, s + \\eta $, with $ s \\in \\mathbb{R}^N $ fixed, $ \\delta c \\sim \\mathcal{N}(0, \\sigma_c^2) $, and independent channel noise $ \\eta \\sim \\mathcal{N}(0, \\mathrm{diag}(\\sigma_{\\eta,i}^2)) $, the covariance is $ \\Sigma = \\sigma_c^2\\, s\\, s^{\\top} + \\mathrm{diag}(\\sigma_{\\eta,i}^2) $. In this case, one can whiten the residuals by finding a factor $ W $ such that $ W^{\\top} W = \\Sigma^{-1} $ and then compute $ \\chi^2 = \\| W (y - \\mu) \\|_2^2 $.\n\nE. The usual reduced $ \\chi^2 $ defined as $ \\chi^2 / (N - p) $ remains a meaningful absolute goodness-of-fit measure under strongly correlated errors, provided $ p $ is the number of model parameters in $ \\mu(\\theta) $, regardless of whether hyperparameters in $ \\Sigma $ were estimated from $ y $.\n\nF. If the error distribution is heavy-tailed (for example, Student’s $ t $), one should still use a quadratic $ \\chi^2 $ as above, because it is the maximum-likelihood estimator for the mean under symmetric errors.\n\nSelect all choices that are correct.",
            "solution": "The user has provided a problem statement regarding the chi-square statistic in the context of synthetic diagnostics for computational fusion science. I will first validate the problem statement and then proceed to a detailed solution if it is deemed valid.\n\n### Step 1: Extract Givens\n- Predicted measurement vectors: $ \\mu(\\theta) \\in \\mathbb{R}^N $\n- Simulation state parameters: $ \\theta $\n- Measured diagnostic data: $ y \\in \\mathbb{R}^N $\n- Nature of measurement errors: Arise from instrumentation and modeling imperfections, may be correlated across channels.\n- Examples of correlated errors: Common-mode calibration drift, shared line-of-sight geometry.\n- Foundational principles: The analysis must start from the first-principles definition of the Gaussian probability density function (PDF) for measurement errors and the construction of a likelihood for $ y $ given $ \\mu(\\theta) $.\n- Objective: Select all statements that correctly characterize the chi-square statistic $ \\chi^2 $, its assumptions, and its limitations when errors are correlated.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is set in the context of computational fusion science and engineering, a valid scientific discipline. It deals with standard, fundamental concepts of statistical data analysis, specifically likelihood-based inference, Gaussian error models, covariance matrices, and the chi-square statistic. These concepts are central to model validation and parameter estimation in all quantitative sciences. The problem is scientifically and mathematically sound.\n- **Well-Posed**: The problem is a multiple-choice question asking for the evaluation of several technical statements based on a clearly defined theoretical framework (Gaussian likelihood). The premise is sufficient to allow for a rigorous and unambiguous evaluation of each option. A unique set of correct options can be determined.\n- **Objective**: The problem statement uses precise, unbiased, and standard terminology from statistics and physics. There are no subjective or opinion-based claims.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. It is a well-posed, scientifically grounded, and objective question concerning the application of statistical methods in a STEM field. I will proceed with deriving the solution.\n\n### Derivation of the Chi-Square Statistic\nThe foundation of the analysis is the probability of observing the data $y$ given the model prediction $ \\mu(\\theta) $. The measurement error vector is $ \\epsilon = y - \\mu(\\theta) $. The problem specifies a Gaussian distribution for this error. For a multivariate case with $N$ measurements, the probability density function (PDF) for $ \\epsilon $ is the multivariate Gaussian distribution:\n$$ p(\\epsilon) = \\frac{1}{(2\\pi)^{N/2} \\det(\\Sigma)^{1/2}} \\exp \\left( -\\frac{1}{2} \\epsilon^{\\top} \\Sigma^{-1} \\epsilon \\right) $$\nwhere $ \\Sigma $ is the $ N \\times N $ covariance matrix of the errors, $ \\Sigma_{ij} = \\mathbb{E}[\\epsilon_i \\epsilon_j] $, and $ \\Sigma^{-1} $ is its inverse, the precision matrix.\n\nThe likelihood of observing the data $ y $ for a given model $ \\mu(\\theta) $ is $ \\mathcal{L}(\\theta | y) = p(y | \\theta) $. Since $ y = \\mu(\\theta) + \\epsilon $, the distribution of $ y $ is a Gaussian centered at $ \\mu(\\theta) $ with the same covariance $ \\Sigma $.\n$$ p(y | \\theta) = \\frac{1}{(2\\pi)^{N/2} \\det(\\Sigma)^{1/2}} \\exp \\left( -\\frac{1}{2} (y - \\mu(\\theta))^{\\top} \\Sigma^{-1} (y - \\mu(\\theta)) \\right) $$\nIn the context of model fitting and validation, the chi-square statistic, $ \\chi^2 $, is defined from the exponent of the Gaussian likelihood:\n$$ \\chi^2(\\theta) = (y - \\mu(\\theta))^{\\top} \\Sigma^{-1} (y - \\mu(\\theta)) $$\nMaximizing the likelihood with respect to $ \\theta $ (for a fixed, known $ \\Sigma $) is equivalent to minimizing $ \\chi^2(\\theta) $. The negative log-likelihood (NLL) is often used, as it is computationally more convenient:\n$$ -\\ln \\mathcal{L}(\\theta | y) = \\frac{N}{2}\\ln(2\\pi) + \\frac{1}{2}\\ln(\\det(\\Sigma)) + \\frac{1}{2} \\chi^2(\\theta) $$\nThis expression forms the basis for evaluating the given options.\n\n### Option-by-Option Analysis\n\n**A. Under independent Gaussian errors with known per-channel standard deviations $ \\sigma_i $, the negative log-likelihood (up to an additive constant) reduces to a weighted sum of squared normalized residuals, and the corresponding $ \\chi^2 $ equals $ \\sum_{i=1}^{N} \\left( \\frac{y_i - \\mu_i}{\\sigma_i} \\right)^2 $.**\n\nIf the measurement errors are independent, the covariance matrix $ \\Sigma $ is diagonal. The diagonal elements are the variances of each channel, $ \\Sigma_{ii} = \\sigma_i^2 $. All off-diagonal elements are zero.\n$$ \\Sigma = \\mathrm{diag}(\\sigma_1^2, \\sigma_2^2, \\ldots, \\sigma_N^2) $$\nThe inverse of this diagonal matrix is also diagonal, with elements $ (\\Sigma^{-1})_{ii} = 1/\\sigma_i^2 $.\n$$ \\Sigma^{-1} = \\mathrm{diag}(1/\\sigma_1^2, 1/\\sigma_2^2, \\ldots, 1/\\sigma_N^2) $$\nSubstituting this into the general formula for $ \\chi^2 $:\n$$ \\chi^2 = (y - \\mu)^{\\top} \\Sigma^{-1} (y - \\mu) = \\sum_{i=1}^{N} \\sum_{j=1}^{N} (y_i - \\mu_i) (\\Sigma^{-1})_{ij} (y_j - \\mu_j) $$\nSince $ \\Sigma^{-1} $ is diagonal, this simplifies to:\n$$ \\chi^2 = \\sum_{i=1}^{N} (y_i - \\mu_i) (\\Sigma^{-1})_{ii} (y_i - \\mu_i) = \\sum_{i=1}^{N} (y_i - \\mu_i) \\left( \\frac{1}{\\sigma_i^2} \\right) (y_i - \\mu_i) = \\sum_{i=1}^{N} \\left( \\frac{y_i - \\mu_i}{\\sigma_i} \\right)^2 $$\nThe negative log-likelihood becomes:\n$$ -\\ln \\mathcal{L} = \\frac{N}{2}\\ln(2\\pi) + \\frac{1}{2}\\ln\\left(\\prod_{i=1}^{N} \\sigma_i^2\\right) + \\frac{1}{2} \\sum_{i=1}^{N} \\left( \\frac{y_i - \\mu_i}{\\sigma_i} \\right)^2 $$\nSince the standard deviations $ \\sigma_i $ are assumed to be known constants, the first two terms are constant. Thus, up to an additive constant, the NLL is proportional to the sum of squared normalized residuals. The statement correctly identifies both the form of the NLL and the resulting $ \\chi^2 $ statistic for this special case.\n\n**Verdict: Correct.**\n\n**B. If the errors have known covariance matrix $ \\Sigma \\in \\mathbb{R}^{N \\times N} $, the appropriate generalization is $ \\chi^2 = (y - \\mu)^{\\top} \\Sigma (y - \\mu) $, i.e., weight by the covariance directly, and independence is recovered by choosing $ \\Sigma $ diagonal with entries $ \\sigma_i^2 $.**\n\nThis statement proposes $ \\chi^2 = (y - \\mu)^{\\top} \\Sigma (y - \\mu) $. As derived from first principles, the correct exponent in the Gaussian likelihood contains the inverse covariance matrix, $ \\Sigma^{-1} $, not $ \\Sigma $. The matrix $ \\Sigma^{-1} $, the precision matrix, correctly assigns lower weight to channels with higher variance (uncertainty). Weighting by $ \\Sigma $ would do the opposite, assigning higher weight to more uncertain measurements, which is statistically incorrect. For the special case of independent errors with $ \\Sigma = \\mathrm{diag}(\\sigma_i^2) $, the proposed formula would yield $ \\sum_{i=1}^{N} (y_i - \\mu_i)^2 \\sigma_i^2 $, which is dimensionally incorrect (if $y$ has units, this sum has units of $[y]^4$) and statistically meaningless as a goodness-of-fit metric. The correct formula, as shown in A, is $ \\sum_i [(y_i - \\mu_i)/\\sigma_i]^2 $.\n\n**Verdict: Incorrect.**\n\n**C. When errors are correlated, using the diagonal-$ \\sigma_i $ formula for $ \\chi^2 $ tends to understate uncertainty in $ \\chi^2 $ and can make a model look spuriously bad; the correct remedy is to include $ \\Sigma^{-1} $ in the quadratic form and also account for the normalization term $ \\ln |\\Sigma| $ when comparing models with different implied covariances or when estimating $ \\Sigma $ from the data.**\n\nThis statement contains three parts.\n1.  *Consequences of ignoring correlations*: If errors are positively correlated (e.g., all channels tend to drift up or down together), a residual vector $ y - \\mu $ where all components have the same sign might be highly probable. However, the diagonal formula $ \\sum_i ((y_i - \\mu_i)/\\sigma_i)^2 $ would treat these as $N$ independent deviations, leading to a large $ \\chi^2 $ value and making the model seem to fit poorly (\"spuriously bad\"). This is because the simplified formula effectively assumes $N$ independent pieces of information, while in reality, due to correlations, there are fewer effective degrees of freedom. This miscalculation of a model's performance can be interpreted as an issue related to the uncertainty of the $ \\chi^2 $ statistic itself.\n2.  *The correct quadratic form*: As derived, the correct statistical weight is provided by the precision matrix $ \\Sigma^{-1} $. The correct $ \\chi^2 $ is indeed $ (y - \\mu)^{\\top} \\Sigma^{-1} (y - \\mu) $.\n3.  *The role of the determinant*: The full NLL is $ -\\ln \\mathcal{L} \\propto \\ln(\\det(\\Sigma)) + (y - \\mu)^{\\top} \\Sigma^{-1} (y - \\mu) $. When comparing different models that might have different underlying error structures (and thus different implied $ \\Sigma $ matrices), or when hyperparameters within $ \\Sigma $ are estimated alongside $ \\theta $, one cannot simply compare the $ \\chi^2 $ values. The full likelihood, which includes the $ \\ln(\\det(\\Sigma)) $ term, must be used. This term penalizes models that achieve a good fit by positing very large uncertainties (large $ \\det(\\Sigma) $). All parts of the statement are correct and crucial for proper statistical inference with correlated errors.\n\n**Verdict: Correct.**\n\n**D. In the presence of a common-mode calibration drift modeled as $ \\epsilon = \\delta c\\, s + \\eta $, with $ s \\in \\mathbb{R}^N $ fixed, $ \\delta c \\sim \\mathcal{N}(0, \\sigma_c^2) $, and independent channel noise $ \\eta \\sim \\mathcal{N}(0, \\mathrm{diag}(\\sigma_{\\eta,i}^2)) $, the covariance is $ \\Sigma = \\sigma_c^2 s s^{\\top} + \\mathrm{diag}(\\sigma_{\\eta,i}^2) $. In this case, one can whiten the residuals by finding a factor $ W $ such that $ W^{\\top} W = \\Sigma^{-1} $ and then compute $ \\chi^2 = \\| W (y - \\mu) \\|_2^2 $.**\n\nFirst, let's verify the covariance matrix. The error vector is $ \\epsilon = \\delta c\\, s + \\eta $. Assuming $ \\delta c $ and $ \\eta $ are independent and have zero mean, the covariance matrix is:\n$$ \\Sigma = \\mathbb{E}[\\epsilon \\epsilon^{\\top}] = \\mathbb{E}[(\\delta c\\, s + \\eta)(\\delta c\\, s + \\eta)^{\\top}] = \\mathbb{E}[\\delta c^2 s s^{\\top} + \\delta c s \\eta^{\\top} + \\eta s^{\\top} \\delta c + \\eta \\eta^{\\top}] $$\nUsing linearity of expectation and independence ($ \\mathbb{E}[\\delta c]=0, \\mathbb{E}[\\eta]=0 $):\n$$ \\Sigma = \\mathbb{E}[\\delta c^2] s s^{\\top} + 0 + 0 + \\mathbb{E}[\\eta \\eta^{\\top}] $$\nGiven $ \\delta c \\sim \\mathcal{N}(0, \\sigma_c^2) $, we have $ \\mathbb{E}[\\delta c^2] = \\sigma_c^2 $. The covariance of the independent noise $ \\eta $ is $ \\mathbb{E}[\\eta \\eta^{\\top}] = \\mathrm{diag}(\\sigma_{\\eta,i}^2) $. Thus,\n$$ \\Sigma = \\sigma_c^2 s s^{\\top} + \\mathrm{diag}(\\sigma_{\\eta,i}^2) $$\nThe first part of the statement is correct.\nSecond, let's analyze the whitening procedure. The $ \\chi^2 $ statistic is $ \\chi^2 = (y - \\mu)^{\\top} \\Sigma^{-1} (y - \\mu) $. Since $ \\Sigma $ is a real, symmetric, positive-definite matrix, so is $ \\Sigma^{-1} $. Therefore, $ \\Sigma^{-1} $ admits a matrix factorization, such as a Cholesky decomposition $ \\Sigma^{-1} = L L^{\\top} $ or a symmetric square root $ S^2 = \\Sigma^{-1} $. The statement proposes a factorization $ W^{\\top} W = \\Sigma^{-1} $. Let's assume such a matrix $ W $ exists (e.g., $ W=L^{\\top} $ from Cholesky).\nSubstituting this into the $ \\chi^2 $ expression:\n$$ \\chi^2 = (y - \\mu)^{\\top} (W^{\\top} W) (y - \\mu) = [W(y - \\mu)]^{\\top} [W(y - \\mu)] $$\nThis is the dot product of the vector $ z = W(y - \\mu) $ with itself, which is the squared Euclidean norm of $ z $: $ \\|z\\|_2^2 $.\n$$ \\chi^2 = \\| W(y - \\mu) \\|_2^2 $$\nThis procedure is known as whitening, because the transformed residual vector $ z $ has an identity covariance matrix: $ \\mathbb{E}[zz^{\\top}] = W \\mathbb{E}[\\epsilon\\epsilon^{\\top}] W^{\\top} = W \\Sigma W^{\\top} $. (Note: For this to be exactly $I$, one must choose $W$ such that $W\\Sigma W^T=I$, e.g., $W=L^{-1}$ where $\\Sigma=LL^T$. The expression in the statement is for $\\chi^2$ and is correct regardless of this detail). The calculation of $ \\chi^2 $ is correct.\n\n**Verdict: Correct.**\n\n**E. The usual reduced $ \\chi^2 $ defined as $ \\chi^2 / (N - p) $ remains a meaningful absolute goodness-of-fit measure under strongly correlated errors, provided $ p $ is the number of model parameters in $ \\mu(\\theta) $, regardless of whether hyperparameters in $ \\Sigma $ were estimated from $ y $.**\n\nThe reduced chi-square, $ \\chi^2_{\\text{red}} = \\chi^2/\\nu $, where $ \\nu $ is the number of degrees of freedom, is expected to be approximately $ 1 $ for a good fit. The degrees of freedom $ \\nu $ are the number of data points minus the number of fitted parameters. The statement claims $ \\nu = N - p $, where $ p $ is the number of parameters in the physics model $ \\mu(\\theta) $. However, it critically adds \"regardless of whether hyperparameters in $ \\Sigma $ were estimated from $ y $\". If the parameters of the covariance matrix itself (e.g., $ \\sigma_c $ and $ \\sigma_{\\eta,i} $ in option D) are not known a priori and are estimated from the data $ y $, they also \"consume\" degrees of freedom. If $ k $ such hyperparameters are estimated, the correct number of degrees of freedom is $ \\nu = N - p - k $. Using $ \\nu=N-p $ overestimates the degrees of freedom, which will cause $ \\chi^2_{\\text{red}} $ to be systematically underestimated. Therefore, it ceases to be a meaningful *absolute* goodness-of-fit measure, as its expected value is no longer $ 1 $. The \"regardless\" clause makes this statement false.\n\n**Verdict: Incorrect.**\n\n**F. If the error distribution is heavy-tailed (for example, Student’s $ t $), one should still use a quadratic $ \\chi^2 $ as above, because it is the maximum-likelihood estimator for the mean under symmetric errors.**\n\nThe quadratic $ \\chi^2 $ statistic arises directly from the exponent of a Gaussian PDF. If the error distribution is not Gaussian, but some other form like a Student's t-distribution, then the principle of maximum likelihood dictates using a different objective function. For a Student's t-distribution, the negative log-likelihood involves minimizing a sum of logarithms, $ \\sum_i \\ln(1 + (\\text{residual}_i/\\text{scale})^2/\\nu_{df}) $, which is not quadratic and is known to be more robust to outliers (a key feature of heavy-tailed distributions). The justification provided is also false: minimizing a quadratic form (least squares) is the maximum-likelihood estimator for the mean *only if the errors are Gaussian*. For a general symmetric error distribution, the MLE is different. For example, for a Laplace distribution, the MLE corresponds to minimizing the sum of absolute deviations.\n\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{ACD}$$"
        }
    ]
}