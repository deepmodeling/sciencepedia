{
    "hands_on_practices": [
        {
            "introduction": "A core challenge in designing efficient Asynchronous Many-Task (AMT) applications is determining the optimal task \"granularity.\" If tasks are too small, the runtime system's scheduling overhead can dominate the computation, leading to poor performance. This exercise provides a hands-on opportunity to model this fundamental trade-off by analyzing the relationship between task size, kernel execution time, and system overhead to calculate the parallel efficiency . By working through this problem, you will develop a first-principles understanding of how to size tasks to maximize computational throughput.",
            "id": "3951831",
            "problem": "An asynchronous many-task (AMT) runtime in a computational fusion science code schedules fine-grained tasks that each apply a localized operator on a block of grid data of size $b$. Empirical microbenchmarking on the target platform shows that the runtime incurs a constant per-batch scheduling overhead of $o = 15~\\mu\\text{s}$, while the useful kernel time for a batch scales as $t(b) = 1.2\\,b^{3}~\\mu\\text{s}$ due to dense-local operations with cubic complexity. Assume there is a large fixed problem consisting of $M$ identical batches with $M \\gg P$, and that load balancing is perfect across $P = 64$ identical workers with sufficient task parallelism to keep all workers busy. Assume the AMT runtime executes each batch’s overhead and useful kernel time additively for that batch, with no overlap between $o$ and $t(b)$, and that communication and synchronization costs beyond $o$ are negligible and do not introduce additional serialization in the steady-state.\n\nStarting from first principles, define parallel efficiency as $E = \\frac{T_{\\text{ideal}}}{T_{\\text{actual}}}$, where $T_{\\text{ideal}}$ is the parallel runtime ignoring runtime overhead, and $T_{\\text{actual}}$ is the measured parallel runtime including the per-batch overhead $o$. Use these definitions to derive a condition on $b$ that guarantees at least $0.90$ efficiency on $P = 64$ workers for the given large fixed problem. Report the smallest integer $b$ that satisfies this condition. Express your final answer as a dimensionless integer.",
            "solution": "The problem asks for the smallest integer block size, $b$, that ensures a parallel efficiency of at least $0.90$. We begin by analyzing the problem from first principles as requested.\n\nThe total problem consists of $M$ identical batches to be processed by $P$ workers.\nThe work associated with a single batch has two components:\n1.  Useful kernel work, with duration $t(b) = 1.2\\,b^{3}~\\mu\\text{s}$.\n2.  Runtime scheduling overhead, with duration $o = 15~\\mu\\text{s}$.\n\nThe problem states that for each batch, the overhead and useful kernel time are additive. Therefore, the total time to process one batch on a single worker is $t_{batch}(b) = t(b) + o$.\n\nFor the entire problem of $M$ batches, we can define the total useful work and the total overhead work. Let's denote the total amount of useful computational work as $W_{\\text{useful}}$ and the total overhead as $W_{\\text{overhead}}$.\n$$ W_{\\text{useful}} = M \\cdot t(b) = M \\cdot (1.2\\,b^{3}) $$\n$$ W_{\\text{overhead}} = M \\cdot o = M \\cdot 15 $$\nThe total actual work, $W_{\\text{actual}}$, is the sum of the useful work and the overhead.\n$$ W_{\\text{actual}} = W_{\\text{useful}} + W_{\\text{overhead}} = M \\cdot (t(b) + o) $$\n\nThe problem specifies key assumptions: perfect load balancing, sufficient task parallelism to keep all $P=64$ workers busy, and negligible communication and synchronization costs beyond the per-batch overhead $o$. Under these assumptions, the total work is distributed evenly among the $P$ workers.\n\nWe can now define the parallel runtimes, $T_{\\text{ideal}}$ and $T_{\\text{actual}}$.\n$T_{\\text{ideal}}$ is the parallel runtime if there were no overhead ($o=0$). It is the total useful work divided by the number of workers.\n$$ T_{\\text{ideal}} = \\frac{W_{\\text{useful}}}{P} = \\frac{M \\cdot t(b)}{P} $$\n$T_{\\text{actual}}$ is the measured parallel runtime including the overhead. It is the total actual work divided by the number of workers.\n$$ T_{\\text{actual}} = \\frac{W_{\\text{actual}}}{P} = \\frac{M \\cdot (t(b) + o)}{P} $$\n\nParallel efficiency, $E$, is defined as the ratio of the ideal parallel runtime to the actual parallel runtime.\n$$ E = \\frac{T_{\\text{ideal}}}{T_{\\text{actual}}} $$\nSubstituting the expressions for $T_{\\text{ideal}}$ and $T_{\\text{actual}}$:\n$$ E = \\frac{\\frac{M \\cdot t(b)}{P}}{\\frac{M \\cdot (t(b) + o)}{P}} $$\nThe terms $M$ and $P$ cancel out. This shows that for this model, the efficiency is independent of the total problem size and the number of workers, and is instead determined by the \"granularity\" of a single task, i.e., the ratio of its useful work to its overhead.\n$$ E = \\frac{t(b)}{t(b) + o} $$\nThis expression represents the fraction of time spent doing useful computation.\n\nWe are given the condition that the efficiency must be at least $0.90$.\n$$ E \\ge 0.90 $$\n$$ \\frac{t(b)}{t(b) + o} \\ge 0.90 $$\n\nNow, we substitute the given expressions for $t(b)$ and $o$. The units ($\\mu\\text{s}$) are consistent and will cancel, so we can work with the numerical values.\n$t(b) = 1.2\\,b^3$\n$o = 15$\n$$ \\frac{1.2\\,b^3}{1.2\\,b^3 + 15} \\ge 0.90 $$\nTo solve for $b$, we rearrange the inequality. Since $b$ is a block size, $b > 0$, which means $1.2\\,b^3 + 15$ is a positive quantity, so we can multiply both sides by it without changing the direction of the inequality.\n$$ 1.2\\,b^3 \\ge 0.90 \\cdot (1.2\\,b^3 + 15) $$\n$$ 1.2\\,b^3 \\ge 0.90 \\cdot 1.2\\,b^3 + 0.90 \\cdot 15 $$\n$$ 1.2\\,b^3 \\ge 1.08\\,b^3 + 13.5 $$\nNow, we isolate the term with $b^3$.\n$$ 1.2\\,b^3 - 1.08\\,b^3 \\ge 13.5 $$\n$$ 0.12\\,b^3 \\ge 13.5 $$\n$$ b^3 \\ge \\frac{13.5}{0.12} $$\n$$ b^3 \\ge \\frac{1350}{12} $$\nSimplifying the fraction:\n$$ b^3 \\ge \\frac{450}{4} = \\frac{225}{2} = 112.5 $$\nTo find the condition on $b$, we take the cube root of both sides.\n$$ b \\ge \\sqrt[3]{112.5} $$\nWe need to find the smallest integer $b$ that satisfies this condition. We can estimate the value by checking integer cubes:\n$4^3 = 64$\n$5^3 = 125$\nSince $64  112.5  125$, we know that $4  \\sqrt[3]{112.5}  5$.\nThe inequality $b \\ge \\sqrt[3]{112.5}$ requires $b$ to be greater than a number between $4$ and $5$. As $b$ must be an integer, the smallest integer value that satisfies this condition is $5$.\n\nTo verify, let's check the efficiency for $b=4$ and $b=5$.\nFor $b=4$:\n$t(4) = 1.2 \\cdot 4^3 = 1.2 \\cdot 64 = 76.8$.\n$E(4) = \\frac{76.8}{76.8 + 15} = \\frac{76.8}{91.8} \\approx 0.8366$, which is less than $0.90$.\nFor $b=5$:\n$t(5) = 1.2 \\cdot 5^3 = 1.2 \\cdot 125 = 150$.\n$E(5) = \\frac{150}{150 + 15} = \\frac{150}{165} = \\frac{10}{11} \\approx 0.9091$, which is greater than $0.90$.\nThus, the smallest integer value for $b$ that meets the required efficiency target is $5$.",
            "answer": "$$ \\boxed{5} $$"
        },
        {
            "introduction": "While task granularity is crucial, the overall speed of an asynchronous computation is often limited by dependencies between tasks. These relationships form a Directed Acyclic Graph (DAG), and the longest path through this graph, known as the \"critical path,\" determines the minimum possible execution time, regardless of how many processors are available. This practice problem allows you to model a simple computational pipeline, map out its dependencies, and calculate the critical path makespan, a foundational skill for predicting and reasoning about the performance of complex, task-based workflows .",
            "id": "3951863",
            "problem": "A computational fusion science control loop employs an Asynchronous Many-Task (AMT) runtime, where computation is organized as tasks linked by dependency futures. Consider a single pipeline invocation in which four computational stages represent distinct physics modules: stage $1$ (e.g., a fast equilibrium preconditioner), stage $2$ (e.g., a magnetohydrodynamics update), stage $3$ (e.g., a radio-frequency heating kernel), and stage $4$ (e.g., diagnostics assimilation). The AMT runtime uses futures to chain tasks so that a task becomes eligible to start the instant all of its input futures are satisfied, and ready tasks can be executed concurrently if resources are available. Assume an idealized machine model with sufficient workers to run any number of ready tasks concurrently and negligible scheduling overhead.\n\nThe stage durations are $2$, $3$, $1$, and $4$ milliseconds for stages $1$, $2$, $3$, and $4$, respectively. The dependency structure is as follows: stage $2$ depends on stage $1$; stage $4$ depends on stage $3$; stages $1$ and $3$ are independent. There are no other dependencies. The pipeline processes a single input, and the AMT runtime schedules tasks immediately upon dependency satisfaction.\n\nUsing only first-principles reasoning from the core definitions of AMT task scheduling and dependency-respecting execution (for example, the notion that ready tasks can start at time $0$ and that a task’s earliest start time is determined by the completion of its predecessors), determine the minimum makespan, defined as the earliest time at which all four stages have completed, under the given assumptions. Express your final answer in milliseconds (ms). No rounding is required; provide the exact value as a single real number.",
            "solution": "The problem asks for the minimum makespan of a set of four computational tasks with specified durations and dependencies, executed by an idealized Asynchronous Many-Task (AMT) runtime. The makespan is the total time elapsed from the start of the first task to the completion of the last task. This problem is equivalent to finding the length of the critical path in the task dependency graph.\n\nLet $S_i$ be the start time and $C_i$ be the completion time for stage $i$, where $i \\in \\{1, 2, 3, 4\\}$. The duration of stage $i$ is denoted by $T_i$. The relationship between these quantities is $C_i = S_i + T_i$.\n\nAccording to the rules of AMT scheduling with futures:\n1.  A task with no dependencies can start at time $t=0$.\n2.  A task with dependencies can start only after all of its prerequisite tasks have completed. Under the assumption of immediate scheduling, its start time is the maximum of the completion times of all its direct predecessors.\n\nThe problem specifies two independent chains of computation:\n-   Chain A: Stage $1 \\rightarrow$ Stage $2$\n-   Chain B: Stage $3 \\rightarrow$ Stage $4$\n\nSince stages $1$ and $3$ are independent and have no predecessors, they can both start at time $t=0$. The idealized machine model with sufficient workers allows these two chains to execute concurrently.\n\nLet's analyze Chain A:\n-   Stage $1$ has no predecessors. Its start time is $S_1 = 0$ ms.\n-   The duration of stage $1$ is $T_1 = 2$ ms.\n-   The completion time of stage $1$ is $C_1 = S_1 + T_1 = 0 + 2 = 2$ ms.\n-   Stage $2$ depends on stage $1$. Therefore, its earliest start time is the completion time of stage $1$.\n-   The start time of stage $2$ is $S_2 = C_1 = 2$ ms.\n-   The duration of stage $2$ is $T_2 = 3$ ms.\n-   The completion time of stage $2$ is $C_2 = S_2 + T_2 = 2 + 3 = 5$ ms.\nThe total time to complete Chain A is $C_2 = 5$ ms.\n\nNow, let's analyze Chain B, which runs in parallel with Chain A:\n-   Stage $3$ has no predecessors. Its start time is $S_3 = 0$ ms.\n-   The duration of stage $3$ is $T_3 = 1$ ms.\n-   The completion time of stage $3$ is $C_3 = S_3 + T_3 = 0 + 1 = 1$ ms.\n-   Stage $4$ depends on stage $3$. Therefore, its earliest start time is the completion time of stage $3$.\n-   The start time of stage $4$ is $S_4 = C_3 = 1$ ms.\n-   The duration of stage $4$ is $T_4 = 4$ ms.\n-   The completion time of stage $4$ is $C_4 = S_4 + T_4 = 1 + 4 = 5$ ms.\nThe total time to complete Chain B is $C_4 = 5$ ms.\n\nThe makespan is defined as the earliest time at which *all* four stages have completed. This is the maximum of the completion times of all tasks.\nMakespan $= \\max(C_1, C_2, C_3, C_4)$.\nSubstituting the calculated values:\nMakespan $= \\max(2 \\text{ ms}, 5 \\text{ ms}, 1 \\text{ ms}, 5 \\text{ ms})$.\nThe maximum of these values is $5$ ms.\n\nThe critical path is the longest path through the dependency graph. In this case, there are two critical paths of equal length: $1 \\rightarrow 2$ and $3 \\rightarrow 4$. The length of path $1 \\rightarrow 2$ is $T_1 + T_2 = 2+3 = 5$ ms. The length of path $3 \\rightarrow 4$ is $T_3 + T_4 = 1+4 = 5$ ms. The makespan is determined by the length of the longest path, which is $5$ ms.\nTherefore, the minimum makespan is $5$ ms.",
            "answer": "$$\n\\boxed{5}\n$$"
        },
        {
            "introduction": "Theoretical models like critical path analysis provide an ideal lower bound on execution time, but understanding real-world performance requires analyzing execution traces. These traces capture the complex interplay between task dependencies, scheduler decisions, and resource contention. In this exercise, you will act as a performance analyst, dissecting a given execution trace to compute key metrics like makespan, worker utilization, and scheduling imbalance . This practice is essential for diagnosing performance bottlenecks and understanding the gap between ideal and actual performance in AMT systems.",
            "id": "3951867",
            "problem": "You are given traces of asynchronous many-task executions represented as directed acyclic graphs with explicit task start and finish times, worker assignments, and dependencies. Each task is a node $i$ with duration $d_i$ measured in seconds, defined by $d_i = t_i^{\\mathrm{finish}} - t_i^{\\mathrm{start}}$, where $t_i^{\\mathrm{start}}$ and $t_i^{\\mathrm{finish}}$ are the start and finish times in seconds. Each directed edge $(j \\rightarrow i)$ means that task $i$ depends on task $j$ and therefore cannot begin until task $j$ has finished. The workers are identical resources and no worker executes more than one task at the same time. Your program must, for each test case, reconstruct the critical path length of the dependency graph from the durations, compute the average utilization of workers over the makespan, and quantify temporal imbalance periods according to a precise criterion.\n\nThe fundamental base is as follows.\n\n- The dependency structure is a directed acyclic graph (DAG). The earliest finish time $E_i$ of a task $i$ under unlimited resources is defined recursively by\n$$\nE_i = d_i + \\max_{j \\in \\mathrm{pred}(i)} E_j,\n$$\nwhere $\\mathrm{pred}(i)$ is the set of immediate predecessors of $i$. If $\\mathrm{pred}(i) = \\emptyset$, then $E_i = d_i$. The critical path length $L_{\\mathrm{cp}}$ is\n$$\nL_{\\mathrm{cp}} = \\max_{i} E_i,\n$$\nwhich represents the minimum possible completion time under infinite resources and the given dependencies.\n\n- Let $W$ denote the total number of workers, and let $B(t)$ be the number of tasks running at time $t$ (tasks with $t_i^{\\mathrm{start}} \\le t  t_i^{\\mathrm{finish}}$). The utilization at time $t$ is $U(t) = \\frac{B(t)}{W}$. Let the makespan be $T = \\max_i t_i^{\\mathrm{finish}} - \\min_i t_i^{\\mathrm{start}}$. The average utilization is defined by\n$$\n\\bar{U} = \\frac{1}{T} \\int_{0}^{T} \\frac{B(t)}{W} \\, dt,\n$$\nwith time measured in seconds and $\\bar{U}$ a dimensionless fraction.\n\n- Define the readiness of a task at time $t$ by completion of all its dependencies at or before $t$. A task $i$ is ready-but-not-running at time $t$ if $\\max_{j \\in \\mathrm{pred}(i)} t_j^{\\mathrm{finish}} \\le t$ and $t_i^{\\mathrm{start}} > t$. Let $R(t)$ denote the number of ready-but-not-running tasks at time $t$. Define the imbalance indicator by\n$$\nI(t) = \n\\begin{cases}\n1,  \\text{if } R(t)  0 \\text{ and } W - B(t)  0, \\\\\n0,  \\text{otherwise}.\n\\end{cases}\n$$\nThe imbalance time fraction relative to the makespan is\n$$\n\\phi = \\frac{1}{T} \\int_{0}^{T} I(t) \\, dt,\n$$\nwhich is a dimensionless fraction.\n\nYour task is to implement a program that, for each test case, computes:\n- The critical path length $L_{\\mathrm{cp}}$ in seconds.\n- The makespan $T$ in seconds.\n- The average utilization $\\bar{U}$ as a dimensionless fraction.\n- The imbalance time fraction $\\phi$ as a dimensionless fraction.\n\nThe computations are to be performed exactly using the piecewise-constant nature over the event times induced by $t_i^{\\mathrm{start}}$ and $t_i^{\\mathrm{finish}}$; that is, if the sorted unique event times are $t_0  t_1  \\cdots  t_K$, then the integrals reduce to sums\n$$\n\\bar{U} = \\frac{1}{T} \\sum_{k=0}^{K-1} \\frac{B(t_k)}{W} \\left( t_{k+1} - t_k \\right), \\quad\n\\phi = \\frac{1}{T} \\sum_{k=0}^{K-1} I(t_k) \\left( t_{k+1} - t_k \\right).\n$$\n\nAll times must be handled in seconds. All outputs must be floats. Angle units are not applicable.\n\nTest Suite (all times in seconds, workers indexed by integers starting at $0$):\n\n- Test Case $1$ (balanced execution):\n    - Workers: $W = 2$.\n    - Tasks:\n        - $T1$: $t^{\\mathrm{start}} = 0$, $t^{\\mathrm{finish}} = 2$, worker $0$, dependencies {$\\emptyset$}.\n        - $T2$: $t^{\\mathrm{start}} = 0$, $t^{\\mathrm{finish}} = 2$, worker $1$, dependencies {$\\emptyset$}.\n        - $T3$: $t^{\\mathrm{start}} = 2$, $t^{\\mathrm{finish}} = 4$, worker $0$, dependencies $\\{T1\\}$.\n        - $T4$: $t^{\\mathrm{start}} = 2$, $t^{\\mathrm{finish}} = 4$, worker $1$, dependencies $\\{T2\\}$.\n\n- Test Case $2$ (dependency-limited phase with no imbalance):\n    - Workers: $W = 3$.\n    - Tasks:\n        - $A$: $t^{\\mathrm{start}} = 0$, $t^{\\mathrm{finish}} = 3$, worker $0$, dependencies {$\\emptyset$}.\n        - $B$: $t^{\\mathrm{start}} = 0$, $t^{\\mathrm{finish}} = 3$, worker $1$, dependencies {$\\emptyset$}.\n        - $C$: $t^{\\mathrm{start}} = 0$, $t^{\\mathrm{finish}} = 3$, worker $2$, dependencies {$\\emptyset$}.\n        - $D$: $t^{\\mathrm{start}} = 3$, $t^{\\mathrm{finish}} = 6$, worker $0$, dependencies $\\{A,B,C\\}$.\n\n- Test Case $3$ (scheduling inefficiency creating imbalance):\n    - Workers: $W = 2$.\n    - Tasks:\n        - $E$: $t^{\\mathrm{start}} = 0$, $t^{\\mathrm{finish}} = 3$, worker $0$, dependencies {$\\emptyset$}.\n        - $F$: $t^{\\mathrm{start}} = 1$, $t^{\\mathrm{finish}} = 4$, worker $1$, dependencies {$\\emptyset$}.\n        - $H$: $t^{\\mathrm{start}} = 3.5$, $t^{\\mathrm{finish}} = 5$, worker $0$, dependencies {$\\emptyset$}.\n\n- Test Case $4$ (single worker serial chain):\n    - Workers: $W = 1$.\n    - Tasks:\n        - $P$: $t^{\\mathrm{start}} = 0$, $t^{\\mathrm{finish}} = 2$, worker $0$, dependencies {$\\emptyset$}.\n        - $Q$: $t^{\\mathrm{start}} = 2$, $t^{\\mathrm{finish}} = 5$, worker $0$, dependencies $\\{P\\}$.\n        - $R$: $t^{\\mathrm{start}} = 5$, $t^{\\mathrm{finish}} = 6$, worker $0$, dependencies $\\{Q\\}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a list of four floats in the order $[L_{\\mathrm{cp}}, T, \\bar{U}, \\phi]$. For example, a valid output format is $[[x_1,y_1,u_1,\\phi_1],[x_2,y_2,u_2,\\phi_2],\\ldots]$, where each symbol is a float. No additional text should be printed.",
            "solution": "The core of the solution lies in processing the provided task execution trace, which is a directed acyclic graph (DAG) where nodes represent tasks and edges represent dependencies. The analysis involves graph traversal for the critical path and a time-domain analysis for utilization and imbalance.\n\nFirst, let us formalize the data structure. For each test case, we have a set of tasks. Each task $i$ can be represented by its properties: start time $t_i^{\\mathrm{start}}$, finish time $t_i^{\\mathrm{finish}}$, and its set of predecessors $\\mathrm{pred}(i)$. The duration is $d_i = t_i^{\\mathrm{finish}} - t_i^{\\mathrm{start}}$. The number of workers is $W$.\n\n**1. Makespan ($T$) Computation**\nThe makespan $T$ is the total duration of the execution trace, from the start of the very first task to the completion of the very last one. As per the definition provided:\n$$\nT = \\max_i t_i^{\\mathrm{finish}} - \\min_i t_i^{\\mathrm{start}}\n$$\nThis is computed by finding the minimum of all task start times and the maximum of all task finish times across the entire set of tasks in the trace.\n\n**2. Critical Path Length ($L_{\\mathrm{cp}}$) Computation**\nThe critical path length $L_{\\mathrm{cp}}$ represents the longest path in the dependency graph, weighted by task durations. It establishes the theoretical minimum execution time if infinite resources were available. The calculation is based on the recursive definition for the earliest finish time $E_i$ of each task $i$:\n$$\nE_i = d_i + \\max_{j \\in \\mathrm{pred}(i)} E_j\n$$\nFor a source task $i$ with no predecessors ($\\mathrm{pred}(i) = \\emptyset$), this simplifies to $E_i = d_i$. The overall critical path length is the maximum of these earliest finish times over all tasks:\n$$\nL_{\\mathrm{cp}} = \\max_i E_i\n$$\nTo implement this, we can use a recursive function with memoization (a form of dynamic programming) to avoid recomputing $E_j$ for the same task multiple times. The calculation for $E_i$ for a given task $i$ requires the values of $E_j$ for all its predecessors $j \\in \\mathrm{pred}(i)$. This naturally maps to a traversal of the DAG. A cache or memoization dictionary stores the computed $E_i$ values. The algorithm iterates through all tasks, recursively computes their earliest finish times if not already cached, and then finds the maximum value among them.\n\n**3. Average Utilization ($\\bar{U}$) Computation**\nThe average utilization $\\bar{U}$ measures the fraction of time that workers are busy, averaged over the entire makespan. The formal definition is an integral:\n$$\n\\bar{U} = \\frac{1}{T} \\int_{0}^{T} \\frac{B(t)}{W} \\, dt\n$$\nwhere $B(t)$ is the number of tasks running at time $t$. While the problem suggests a numerical integration by summing over intervals between event points, a more direct and computationally efficient method exists. The integral of the busy function, $\\int_{0}^{T} B(t) \\, dt$, represents the total worker-seconds, which is simply the sum of all individual task durations.\n$$\n\\int_{0}^{T} B(t) \\, dt = \\sum_i \\int_{0}^{T} \\mathbb{I}(t_i^{\\mathrm{start}} \\le t  t_i^{\\mathrm{finish}}) \\, dt = \\sum_i (t_i^{\\mathrm{finish}} - t_i^{\\mathrm{start}}) = \\sum_i d_i\n$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. Therefore, the average utilization can be computed directly as:\n$$\n\\bar{U} = \\frac{\\sum_{i} d_i}{W \\cdot T}\n$$\nThis formula is mathematically equivalent to the piecewise summation and simpler to implement.\n\n**4. Imbalance Time Fraction ($\\phi$) Computation**\nThe imbalance time fraction $\\phi$ quantifies periods of inefficiency where workers are idle, but there are tasks ready to be executed. The imbalance indicator $I(t)$ is $1$ if and only if there is at least one ready-but-not-running task ($R(t) > 0$) AND there is at least one idle worker ($W - B(t) > 0$).\n$$\n\\phi = \\frac{1}{T} \\int_{0}^{T} I(t) \\, dt\n$$\nUnlike utilization, this integral does not simplify to a simple sum over task properties. We must use the specified piecewise-constant evaluation method. The algorithm is as follows:\na. Collect all unique task start times ($t_i^{\\mathrm{start}}$) and finish times ($t_i^{\\mathrm{finish}}$) into a single list of event points. Sort these points to get a timeline $t_0  t_1  \\cdots  t_K$.\nb. These event points define a set of disjoint time intervals $[t_k, t_{k+1})$. The functions $B(t)$, $R(t)$, and thus $I(t)$ are constant within each open interval.\nc. We iterate from $k=0$ to $K-1$. For each interval, we pick a sample point (e.g., the start $t_k$) and evaluate the imbalance indicator $I(t_k)$. The duration of this interval is $\\Delta t_k = t_{k+1} - t_k$.\nd. At each $t_k$, we compute:\n    i. $B(t_k)$: The number of tasks $i$ for which $t_i^{\\mathrm{start}} \\le t_k  t_i^{\\mathrm{finish}}$.\n    ii. $R(t_k)$: The number of tasks $i$ that are ready-but-not-running, i.e., $t_i^{\\mathrm{start}} > t_k$ and for all predecessors $j \\in \\mathrm{pred}(i)$, $t_j^{\\mathrm{finish}} \\le t_k$.\n    iii. $I(t_k) = 1$ if $R(t_k) > 0$ and $W - B(t_k) > 0$; otherwise, $I(t_k) = 0$.\ne. The total imbalance time is the sum of the durations of the imbalanced intervals: $\\sum_{k=0}^{K-1} I(t_k) \\cdot \\Delta t_k$.\nf. Finally, $\\phi$ is this total imbalance time divided by the makespan $T$.\n\nBy implementing these four algorithms, we can systematically analyze the provided execution traces and compute the required performance metrics for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        {\n            \"W\": 2,\n            \"tasks\": {\n                \"T1\": {\"start\": 0.0, \"finish\": 2.0, \"deps\": []},\n                \"T2\": {\"start\": 0.0, \"finish\": 2.0, \"deps\": []},\n                \"T3\": {\"start\": 2.0, \"finish\": 4.0, \"deps\": [\"T1\"]},\n                \"T4\": {\"start\": 2.0, \"finish\": 4.0, \"deps\": [\"T2\"]},\n            }\n        },\n        {\n            \"W\": 3,\n            \"tasks\": {\n                \"A\": {\"start\": 0.0, \"finish\": 3.0, \"deps\": []},\n                \"B\": {\"start\": 0.0, \"finish\": 3.0, \"deps\": []},\n                \"C\": {\"start\": 0.0, \"finish\": 3.0, \"deps\": []},\n                \"D\": {\"start\": 3.0, \"finish\": 6.0, \"deps\": [\"A\", \"B\", \"C\"]},\n            }\n        },\n        {\n            \"W\": 2,\n            \"tasks\": {\n                \"E\": {\"start\": 0.0, \"finish\": 3.0, \"deps\": []},\n                \"F\": {\"start\": 1.0, \"finish\": 4.0, \"deps\": []},\n                \"H\": {\"start\": 3.5, \"finish\": 5.0, \"deps\": []},\n            }\n        },\n        {\n            \"W\": 1,\n            \"tasks\": {\n                \"P\": {\"start\": 0.0, \"finish\": 2.0, \"deps\": []},\n                \"Q\": {\"start\": 2.0, \"finish\": 5.0, \"deps\": [\"P\"]},\n                \"R\": {\"start\": 5.0, \"finish\": 6.0, \"deps\": [\"Q\"]},\n            }\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        results.append(process_case(case))\n    \n    # The str() of a list includes spaces, which we should remove for the exact format.\n    # The final format is [[a,b,c,d],[e,f,g,h]], without spaces after commas.\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef process_case(case_data):\n    \"\"\"\n    Computes the four metrics for a single test case.\n    \"\"\"\n    W = case_data[\"W\"]\n    tasks_data = case_data[\"tasks\"]\n    task_ids = list(tasks_data.keys())\n\n    # --- 1. Compute Makespan (T) and durations ---\n    min_start_time = float('inf')\n    max_finish_time = float('-inf')\n    all_durations = {}\n    \n    for tid, tinfo in tasks_data.items():\n        min_start_time = min(min_start_time, tinfo[\"start\"])\n        max_finish_time = max(max_finish_time, tinfo[\"finish\"])\n        all_durations[tid] = tinfo[\"finish\"] - tinfo[\"start\"]\n\n    makespan_T = max_finish_time - min_start_time\n    # Handle the edge case of no tasks or zero makespan\n    if makespan_T == 0:\n        return [0.0, 0.0, 0.0, 0.0]\n\n    # --- 2. Compute Critical Path Length (L_cp) ---\n    memo_E = {}\n    def get_earliest_finish(tid):\n        if tid in memo_E:\n            return memo_E[tid]\n        \n        task_info = tasks_data[tid]\n        duration = all_durations[tid]\n        \n        max_pred_E = 0.0\n        if task_info[\"deps\"]:\n            max_pred_E = max(get_earliest_finish(p) for p in task_info[\"deps\"])\n            \n        result = duration + max_pred_E\n        memo_E[tid] = result\n        return result\n\n    L_cp = 0.0\n    for tid in task_ids:\n        L_cp = max(L_cp, get_earliest_finish(tid))\n\n    # --- 3. Compute Average Utilization (U_bar) ---\n    total_duration_sum = sum(all_durations.values())\n    U_bar = total_duration_sum / (W * makespan_T)\n\n    # --- 4. Compute Imbalance Time Fraction (phi) ---\n    event_points = set()\n    for tinfo in tasks_data.values():\n        event_points.add(tinfo[\"start\"])\n        event_points.add(tinfo[\"finish\"])\n    \n    sorted_events = sorted(list(event_points))\n    \n    total_imbalance_time = 0.0\n    \n    for i in range(len(sorted_events) - 1):\n        t_k = sorted_events[i]\n        t_k_plus_1 = sorted_events[i+1]\n        interval_duration = t_k_plus_1 - t_k\n        \n        if interval_duration == 0:\n            continue\n\n        # Evaluate B(t_k) and R(t_k) at the start of the interval\n        B_tk = 0\n        for tinfo in tasks_data.values():\n            if tinfo[\"start\"] = t_k  tinfo[\"finish\"]:\n                B_tk += 1\n        \n        idle_workers = W - B_tk\n        \n        # If no idle workers, no imbalance possible in this interval\n        if idle_workers = 0:\n            continue\n\n        R_tk = 0\n        for tid, tinfo in tasks_data.items():\n            if tinfo[\"start\"]  t_k: # Not yet running\n                # Check if all predecessors are finished\n                max_pred_finish_time = float('-inf')\n                if tinfo[\"deps\"]:\n                    max_pred_finish_time = max(tasks_data[p][\"finish\"] for p in tinfo[\"deps\"])\n                \n                if max_pred_finish_time = t_k: # Ready\n                    R_tk += 1\n        \n        I_tk = 1 if R_tk  0 else 0\n        \n        total_imbalance_time += I_tk * interval_duration\n\n    phi = total_imbalance_time / makespan_T if makespan_T  0 else 0.0\n    \n    return [L_cp, makespan_T, U_bar, phi]\n\nsolve()\n```"
        }
    ]
}