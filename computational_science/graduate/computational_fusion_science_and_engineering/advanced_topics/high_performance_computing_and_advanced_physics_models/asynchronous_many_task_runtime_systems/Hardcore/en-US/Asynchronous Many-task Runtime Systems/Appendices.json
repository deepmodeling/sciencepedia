{
    "hands_on_practices": [
        {
            "introduction": "The efficiency of any asynchronous many-task (AMT) system hinges on a fundamental balance between useful computation and runtime overhead. This first practice explores the concept of task granularity, which is crucial for performance tuning in complex fusion simulations. By modeling the relationship between a task's computational workload and the fixed overhead incurred by the runtime, you will derive a condition to achieve a target parallel efficiency, providing a quantitative basis for choosing optimal task sizes. ",
            "id": "3951831",
            "problem": "An asynchronous many-task (AMT) runtime in a computational fusion science code schedules fine-grained tasks that each apply a localized operator on a block of grid data of size $b$. Empirical microbenchmarking on the target platform shows that the runtime incurs a constant per-batch scheduling overhead of $o = 15~\\mu\\text{s}$, while the useful kernel time for a batch scales as $t(b) = 1.2\\,b^{3}~\\mu\\text{s}$ due to dense-local operations with cubic complexity. Assume there is a large fixed problem consisting of $M$ identical batches with $M \\gg P$, and that load balancing is perfect across $P = 64$ identical workers with sufficient task parallelism to keep all workers busy. Assume the AMT runtime executes each batch’s overhead and useful kernel time additively for that batch, with no overlap between $o$ and $t(b)$, and that communication and synchronization costs beyond $o$ are negligible and do not introduce additional serialization in the steady-state.\n\nStarting from first principles, define parallel efficiency as $E = \\frac{T_{\\text{ideal}}}{T_{\\text{actual}}}$, where $T_{\\text{ideal}}$ is the parallel runtime ignoring runtime overhead, and $T_{\\text{actual}}$ is the measured parallel runtime including the per-batch overhead $o$. Use these definitions to derive a condition on $b$ that guarantees at least $0.90$ efficiency on $P = 64$ workers for the given large fixed problem. Report the smallest integer $b$ that satisfies this condition. Express your final answer as a dimensionless integer.",
            "solution": "The problem asks for the smallest integer block size, $b$, that ensures a parallel efficiency of at least $0.90$. We begin by analyzing the problem from first principles as requested.\n\nThe total problem consists of $M$ identical batches to be processed by $P$ workers.\nThe work associated with a single batch has two components:\n1.  Useful kernel work, with duration $t(b) = 1.2\\,b^{3}~\\mu\\text{s}$.\n2.  Runtime scheduling overhead, with duration $o = 15~\\mu\\text{s}$.\n\nThe problem states that for each batch, the overhead and useful kernel time are additive. Therefore, the total time to process one batch on a single worker is $t_{batch}(b) = t(b) + o$.\n\nFor the entire problem of $M$ batches, we can define the total useful work and the total overhead work. Let's denote the total amount of useful computational work as $W_{\\text{useful}}$ and the total overhead as $W_{\\text{overhead}}$.\n$$ W_{\\text{useful}} = M \\cdot t(b) = M \\cdot (1.2\\,b^{3}) $$\n$$ W_{\\text{overhead}} = M \\cdot o = M \\cdot 15 $$\nThe total actual work, $W_{\\text{actual}}$, is the sum of the useful work and the overhead.\n$$ W_{\\text{actual}} = W_{\\text{useful}} + W_{\\text{overhead}} = M \\cdot (t(b) + o) $$\n\nThe problem specifies key assumptions: perfect load balancing, sufficient task parallelism to keep all $P=64$ workers busy, and negligible communication and synchronization costs beyond the per-batch overhead $o$. Under these assumptions, the total work is distributed evenly among the $P$ workers.\n\nWe can now define the parallel runtimes, $T_{\\text{ideal}}$ and $T_{\\text{actual}}$.\n$T_{\\text{ideal}}$ is the parallel runtime if there were no overhead ($o=0$). It is the total useful work divided by the number of workers.\n$$ T_{\\text{ideal}} = \\frac{W_{\\text{useful}}}{P} = \\frac{M \\cdot t(b)}{P} $$\n$T_{\\text{actual}}$ is the measured parallel runtime including the overhead. It is the total actual work divided by the number of workers.\n$$ T_{\\text{actual}} = \\frac{W_{\\text{actual}}}{P} = \\frac{M \\cdot (t(b) + o)}{P} $$\n\nParallel efficiency, $E$, is defined as the ratio of the ideal parallel runtime to the actual parallel runtime.\n$$ E = \\frac{T_{\\text{ideal}}}{T_{\\text{actual}}} $$\nSubstituting the expressions for $T_{\\text{ideal}}$ and $T_{\\text{actual}}$:\n$$ E = \\frac{\\frac{M \\cdot t(b)}{P}}{\\frac{M \\cdot (t(b) + o)}{P}} $$\nThe terms $M$ and $P$ cancel out. This shows that for this model, the efficiency is independent of the total problem size and the number of workers, and is instead determined by the \"granularity\" of a single task, i.e., the ratio of its useful work to its overhead.\n$$ E = \\frac{t(b)}{t(b) + o} $$\nThis expression represents the fraction of time spent doing useful computation.\n\nWe are given the condition that the efficiency must be at least $0.90$.\n$$ E \\ge 0.90 $$\n$$ \\frac{t(b)}{t(b) + o} \\ge 0.90 $$\n\nNow, we substitute the given expressions for $t(b)$ and $o$. The units ($\\mu\\text{s}$) are consistent and will cancel, so we can work with the numerical values.\n$t(b) = 1.2\\,b^3$\n$o = 15$\n$$ \\frac{1.2\\,b^3}{1.2\\,b^3 + 15} \\ge 0.90 $$\nTo solve for $b$, we rearrange the inequality. Since $b$ is a block size, $b0$, which means $1.2\\,b^3 + 15$ is a positive quantity, so we can multiply both sides by it without changing the direction of the inequality.\n$$ 1.2\\,b^3 \\ge 0.90 \\cdot (1.2\\,b^3 + 15) $$\n$$ 1.2\\,b^3 \\ge 0.90 \\cdot 1.2\\,b^3 + 0.90 \\cdot 15 $$\n$$ 1.2\\,b^3 \\ge 1.08\\,b^3 + 13.5 $$\nNow, we isolate the term with $b^3$.\n$$ 1.2\\,b^3 - 1.08\\,b^3 \\ge 13.5 $$\n$$ 0.12\\,b^3 \\ge 13.5 $$\n$$ b^3 \\ge \\frac{13.5}{0.12} $$\n$$ b^3 \\ge \\frac{1350}{12} $$\nSimplifying the fraction:\n$$ b^3 \\ge \\frac{450}{4} = \\frac{225}{2} = 112.5 $$\nTo find the condition on $b$, we take the cube root of both sides.\n$$ b \\ge \\sqrt[3]{112.5} $$\nWe need to find the smallest integer $b$ that satisfies this condition. We can estimate the value by checking integer cubes:\n$4^3 = 64$\n$5^3 = 125$\nSince $64  112.5  125$, we know that $4  \\sqrt[3]{112.5}  5$.\nThe inequality $b \\ge \\sqrt[3]{112.5}$ requires $b$ to be greater than a number between $4$ and $5$. As $b$ must be an integer, the smallest integer value that satisfies this condition is $5$.\n\nTo verify, let's check the efficiency for $b=4$ and $b=5$.\nFor $b=4$:\n$t(4) = 1.2 \\cdot 4^3 = 1.2 \\cdot 64 = 76.8$.\n$E(4) = \\frac{76.8}{76.8 + 15} = \\frac{76.8}{91.8} \\approx 0.8366$, which is less than $0.90$.\nFor $b=5$:\n$t(5) = 1.2 \\cdot 5^3 = 1.2 \\cdot 125 = 150$.\n$E(5) = \\frac{150}{150 + 15} = \\frac{150}{165} = \\frac{10}{11} \\approx 0.9091$, which is greater than $0.90$.\nThus, the smallest integer value for $b$ that meets the required efficiency target is $5$.",
            "answer": "$$ \\boxed{5} $$"
        },
        {
            "introduction": "While task granularity sets the efficiency of individual work units, the overall application performance is often constrained by dependencies between tasks. This exercise models a computational pipeline as a Directed Acyclic Graph (DAG) and asks you to determine its makespan, which is governed by the longest dependency chain, or \"critical path\". This analysis is fundamental to understanding inherent algorithmic bottlenecks and identifying opportunities for increasing concurrency in an AMT system. ",
            "id": "3951863",
            "problem": "A computational fusion science control loop employs an Asynchronous Many-Task (AMT) runtime, where computation is organized as tasks linked by dependency futures. Consider a single pipeline invocation in which four computational stages represent distinct physics modules: stage $1$ (e.g., a fast equilibrium preconditioner), stage $2$ (e.g., a magnetohydrodynamics update), stage $3$ (e.g., a radio-frequency heating kernel), and stage $4$ (e.g., diagnostics assimilation). The AMT runtime uses futures to chain tasks so that a task becomes eligible to start the instant all of its input futures are satisfied, and ready tasks can be executed concurrently if resources are available. Assume an idealized machine model with sufficient workers to run any number of ready tasks concurrently and negligible scheduling overhead.\n\nThe stage durations are $2$, $3$, $1$, and $4$ milliseconds for stages $1$, $2$, $3$, and $4$, respectively. The dependency structure is as follows: stage $2$ depends on stage $1$; stage $4$ depends on stage $3$; stages $1$ and $3$ are independent. There are no other dependencies. The pipeline processes a single input, and the AMT runtime schedules tasks immediately upon dependency satisfaction.\n\nUsing only first-principles reasoning from the core definitions of AMT task scheduling and dependency-respecting execution (for example, the notion that ready tasks can start at time $0$ and that a task’s earliest start time is determined by the completion of its predecessors), determine the minimum makespan, defined as the earliest time at which all four stages have completed, under the given assumptions. Express your final answer in milliseconds (ms). No rounding is required; provide the exact value as a single real number.",
            "solution": "The problem asks for the minimum makespan of a set of four computational tasks with specified durations and dependencies, executed by an idealized Asynchronous Many-Task (AMT) runtime. The makespan is the total time elapsed from the start of the first task to the completion of the last task. This problem is equivalent to finding the length of the critical path in the task dependency graph.\n\nLet $S_i$ be the start time and $C_i$ be the completion time for stage $i$, where $i \\in \\{1, 2, 3, 4\\}$. The duration of stage $i$ is denoted by $T_i$. The relationship between these quantities is $C_i = S_i + T_i$.\n\nAccording to the rules of AMT scheduling with futures:\n1.  A task with no dependencies can start at time $t=0$.\n2.  A task with dependencies can start only after all of its prerequisite tasks have completed. Under the assumption of immediate scheduling, its start time is the maximum of the completion times of all its direct predecessors.\n\nThe problem specifies two independent chains of computation:\n-   Chain A: Stage $1 \\rightarrow$ Stage $2$\n-   Chain B: Stage $3 \\rightarrow$ Stage $4$\n\nSince stages $1$ and $3$ are independent and have no predecessors, they can both start at time $t=0$. The idealized machine model with sufficient workers allows these two chains to execute concurrently.\n\nLet's analyze Chain A:\n-   Stage $1$ has no predecessors. Its start time is $S_1 = 0$ ms.\n-   The duration of stage $1$ is $T_1 = 2$ ms.\n-   The completion time of stage $1$ is $C_1 = S_1 + T_1 = 0 + 2 = 2$ ms.\n-   Stage $2$ depends on stage $1$. Therefore, its earliest start time is the completion time of stage $1$.\n-   The start time of stage $2$ is $S_2 = C_1 = 2$ ms.\n-   The duration of stage $2$ is $T_2 = 3$ ms.\n-   The completion time of stage $2$ is $C_2 = S_2 + T_2 = 2 + 3 = 5$ ms.\nThe total time to complete Chain A is $C_2 = 5$ ms.\n\nNow, let's analyze Chain B, which runs in parallel with Chain A:\n-   Stage $3$ has no predecessors. Its start time is $S_3 = 0$ ms.\n-   The duration of stage $3$ is $T_3 = 1$ ms.\n-   The completion time of stage $3$ is $C_3 = S_3 + T_3 = 0 + 1 = 1$ ms.\n-   Stage $4$ depends on stage $3$. Therefore, its earliest start time is the completion time of stage $3$.\n-   The start time of stage $4$ is $S_4 = C_3 = 1$ ms.\n-   The duration of stage $4$ is $T_4 = 4$ ms.\n-   The completion time of stage $4$ is $C_4 = S_4 + T_4 = 1 + 4 = 5$ ms.\nThe total time to complete Chain B is $C_4 = 5$ ms.\n\nThe makespan is defined as the earliest time at which *all* four stages have completed. This is the maximum of the completion times of all tasks.\nMakespan $= \\max(C_1, C_2, C_3, C_4)$.\nSubstituting the calculated values:\nMakespan $= \\max(2 \\text{ ms}, 5 \\text{ ms}, 1 \\text{ ms}, 5 \\text{ ms})$.\nThe maximum of these values is $5$ ms.\n\nThe critical path is the longest path through the dependency graph. In this case, there are two critical paths of equal length: $1 \\rightarrow 2$ and $3 \\rightarrow 4$. The length of path $1 \\rightarrow 2$ is $T_1 + T_2 = 2+3 = 5$ ms. The length of path $3 \\rightarrow 4$ is $T_3 + T_4 = 1+4 = 5$ ms. The makespan is determined by the length of the longest path, which is $5$ ms.\nTherefore, the minimum makespan is $5$ ms.",
            "answer": "$$\n\\boxed{5}\n$$"
        },
        {
            "introduction": "High performance is predicated on correctness, an issue that becomes particularly subtle in asynchronous systems due to relaxed memory consistency models on modern hardware. This practice delves into the critical topic of data races, demonstrating how unsynchronized communication between tasks can lead to incorrect results by observing stale data. You will model this failure and then implement a correct solution using release-acquire semantics, a cornerstone synchronization mechanism that provides the necessary memory ordering guarantees for robust parallel algorithms. ",
            "id": "3951864",
            "problem": "You are given a setting inspired by asynchronous many-task runtime systems used in computational fusion science and engineering, where a global reduction aggregates partial results produced by independent subdomain tasks. Each subdomain $i \\in \\{1,\\dots,n\\}$ produces a partial scalar quantity $x_i^{\\text{new}}$ to replace an old stored value $x_i^{\\text{old}}$. The aggregator must compute the correct total $S^{\\star} = \\sum_{i=1}^{n} x_i^{\\text{new}}$. In a typical unsynchronized design, each producer performs two operations in program order: it first writes $x_i^{\\text{new}}$ to a shared memory location and then sets a readiness flag $f_i \\leftarrow 1$ to signal completion. The aggregator polls flags and, for each $i$ with $f_i = 1$, reads the corresponding shared memory cell to reduce its value. You must construct a counterexample showing that, under a weak memory model without explicit synchronization, this design can introduce a race that violates the intended happens-before relation, allowing the aggregator to observe $f_i = 1$ while still reading a stale $x_i^{\\text{old}}$ rather than $x_i^{\\text{new}}$, thereby producing an incorrect reduction result. Then, you must repair the design using a Single-Producer Single-Consumer (SPSC) channel with release-acquire semantics that eliminates the race and guarantees the correctness of the reduction.\n\nFundamental base and definitions to use: a data race occurs when two operations on the same memory location by different tasks are not ordered by a happens-before relation, at least one is a write, and there is no intervening synchronization. Release-acquire semantics establish a happens-before order such that, for a producer performing a release operation after writing data, and a consumer performing an acquire operation before reading the data, all producer writes prior to the release become visible to the consumer reads after the acquire. The correctness property of the reduction is that the aggregator must compute $S^{\\star} = \\sum_{i=1}^{n} x_i^{\\text{new}}$ rather than any mixture of $x_i^{\\text{old}}$ and $x_i^{\\text{new}}$ values.\n\nYour program must implement a discrete-event model that captures both the unsynchronized flag-based design and the SPSC channel repair. For the unsynchronized model, you should simulate an adversarial interleaving by specifying a set $U \\subseteq \\{1,\\dots,n\\}$ of indices for which the aggregator, after observing $f_i = 1$, still reads $x_i^{\\text{old}}$ due to reordering and lack of synchronization. For the SPSC model, you should implement the following logical semantics: each producer performs a release operation when it sends $x_i^{\\text{new}}$ into the channel, and the aggregator performs an acquire operation when it receives, guaranteeing that $x_i^{\\text{new}}$ is the value observed for reduction. You may implement the channel and the memory model as pure functions without actual threads; however, the logical semantics must respect the release-acquire ordering.\n\nThe test suite is as follows, each case specified by the triple $(n, \\mathbf{x}^{\\text{old}}, \\mathbf{x}^{\\text{new}}, U)$, where $\\mathbf{x}^{\\text{old}}$ and $\\mathbf{x}^{\\text{new}}$ are lists of integers:\n- Case $1$: $n = 4$, $\\mathbf{x}^{\\text{old}} = [1,2,3,4]$, $\\mathbf{x}^{\\text{new}} = [2,3,4,5]$, $U = \\varnothing$.\n- Case $2$: $n = 4$, $\\mathbf{x}^{\\text{old}} = [5,1,7,0]$, $\\mathbf{x}^{\\text{new}} = [6,2,8,3]$, $U = \\{2\\}$ meaning index $2$ uses stale data.\n- Case $3$ (boundary): $n = 0$, $\\mathbf{x}^{\\text{old}} = []$, $\\mathbf{x}^{\\text{new}} = []$, $U = \\varnothing$.\n- Case $4$: $n = 3$, $\\mathbf{x}^{\\text{old}} = [10,20,30]$, $\\mathbf{x}^{\\text{new}} = [11,21,31]$, $U = \\{1,2,3\\}$ meaning all indices use stale data.\n\nFor each case, your program must compute:\n- The expected correct reduction $S^{\\star} = \\sum_{i=1}^{n} x_i^{\\text{new}}$.\n- The unsynchronized reduction $S^{\\text{unsync}} = \\sum_{i \\in \\{1,\\dots,n\\} \\setminus U} x_i^{\\text{new}} + \\sum_{i \\in U} x_i^{\\text{old}}$.\n- The SPSC release-acquire reduction $S^{\\text{ra}} = \\sum_{i=1}^{n} x_i^{\\text{new}}$, which must equal $S^{\\star}$ by construction.\n\nYour program must produce, for each case, a list $[E, C]$ where $E = |S^{\\star} - S^{\\text{unsync}}|$ is the absolute error as an integer, and $C$ is a boolean indicating whether $S^{\\text{ra}} = S^{\\star}$. The final program output must be a single line containing the results for all cases as a comma-separated list of these per-case lists, enclosed in square brackets, for example $[[0,\\text{True}],\\dots]$. No physical units are involved. Angles are not involved.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces, where each element is the per-case list $[E,C]$ exactly as specified above.",
            "solution": "The present problem requires the analysis and modeling of a common data race scenario in parallel computing, specifically within the context of asynchronous many-task runtime systems. We are asked to first model an unsynchronized reduction operation that is flawed due to weak memory consistency effects, and then model a corrected version employing release-acquire semantics.\n\nThe fundamental issue stems from memory models weaker than sequential consistency. In such models, a processor core is permitted to reorder memory operations for performance reasons, provided the local program order on that core appears to be maintained. However, these reorderings can become visible to other cores, leading to violations of intuitive assumptions about the global order of events.\n\n**Part 1: The Unsynchronized Flag-based Design and its Failure Mode**\n\nIn the naive design, for each subdomain $i \\in \\{1, \\dots, n\\}$, a producer task executes two distinct operations in program order:\n$1$. Write the new value: The producer calculates $x_i^{\\text{new}}$ and writes it to a shared memory location, let's call it `data[i]`.\n$2$. Set the flag: The producer sets a readiness flag to signal completion, $f_i \\leftarrow 1$.\n\nAn aggregator task polls these flags. Upon observing $f_i = 1$, it reads `data[i]` and adds it to a global sum. The implicit assumption is that observing $f_i=1$ implies that the write to `data[i]` has also completed and is visible to the aggregator. This establishes a *happens-before* relation: the write of $x_i^{\\text{new}}$ *happens-before* the write to $f_i$, which in turn *happens-before* the aggregator's read of $f_i$, which *happens-before* the aggregator's read of `data[i]`.\n\nHowever, without explicit synchronization instructions (memory fences or barriers), a weakly-ordered architecture is not required to enforce this inter-thread happens-before relationship. The system might reorder the writes from the producer's perspective, or the propagation of these writes through the memory system might be reordered. Consequently, the write to the flag $f_i$ could become visible to the aggregator's core before the write of $x_i^{\\text{new}}$ does. This creates a data race: the aggregator sees $f_i=1$, attempts to read `data[i]`, but because the new data has not yet arrived in its local cache or view of memory, it reads the stale value, $x_i^{\\text{old}}$.\n\nTo model this adversarial interleaving, the problem defines a set $U \\subseteq \\{1, \\dots, n\\}$ containing the indices of tasks for which this race condition manifests. The resulting unsynchronized sum, $S^{\\text{unsync}}$, is therefore a mixture of old and new values:\n$$S^{\\text{unsync}} = \\sum_{i \\in \\{1,\\dots,n\\} \\setminus U} x_i^{\\text{new}} + \\sum_{i \\in U} x_i^{\\text{old}}$$\nThe ideal, correct sum, which would be obtained under a sequentially consistent memory model, is:\n$$S^{\\star} = \\sum_{i=1}^{n} x_i^{\\text{new}}$$\nThe error introduced by the data race is the absolute difference $E = |S^{\\star} - S^{\\text{unsync}}|$.\n\n**Part 2: The Repaired Design with Release-Acquire Semantics**\n\nThe data race is resolved by introducing explicit memory ordering constraints, specifically release-acquire semantics, often implemented using atomic operations or SPSC (Single-Producer, Single-Consumer) queues.\n\n- **Release Semantics (Producer)**: When the producer task has finished all its writes (i.e., computed and stored $x_i^{\\text{new}}$), it performs a *release* operation. A common example is an atomic store with release semantics. This operation acts as a memory fence, ensuring that all memory writes preceding it in program order are made visible to other cores before the release operation itself becomes visible.\n\n- **Acquire Semantics (Consumer)**: The consumer task (our aggregator) performs an *acquire* operation to detect readiness. An example is an atomic load with acquire semantics. This operation also acts as a memory fence, ensuring that the acquire operation completes before any memory reads following it in program order are executed.\n\nWhen a release operation by a producer is paired with an acquire operation by a consumer that observes the result of the release, a happens-before relationship is established between the two threads. All memory writes by the producer before the release are guaranteed to be visible to all memory reads by the consumer after the acquire.\n\nIn our problem, this is modeled as an SPSC channel. The producer sending $x_i^{\\text{new}}$ constitutes a release operation. The aggregator receiving from the channel constitutes an acquire operation. This design guarantees that if the aggregator receives a value for subdomain $i$, it must be $x_i^{\\text{new}}$, as the write of $x_i^{\\text{new}}$ is guaranteed to have happened before the receipt. The stale value $x_i^{\\text{old}}$ can never be observed.\n\nTherefore, the sum computed using the release-acquire model, $S^{\\text{ra}}$, is guaranteed to be correct:\n$$S^{\\text{ra}} = \\sum_{i=1}^{n} x_i^{\\text{new}} = S^{\\star}$$\nThe correctness check, $C$, is defined as the boolean comparison $S^{\\text{ra}} = S^{\\star}$, which, by construction, must always evaluate to true.\n\n**Algorithm Implementation**\n\nThe program will iterate through each test case $(n, \\mathbf{x}^{\\text{old}}, \\mathbf{x}^{\\text{new}}, U)$. For each case:\n$1$. The correct sum $S^{\\star}$ is computed by summing all elements of the $\\mathbf{x}^{\\text{new}}$ vector.\n$2$. The unsynchronized sum $S^{\\text{unsync}}$ is computed. We iterate from $i=0$ to $n-1$. If the corresponding $1$-based index $i+1$ is in the set $U$, we add $\\mathbf{x}^{\\text{old}}[i]$ to the sum; otherwise, we add $\\mathbf{x}^{\\text{new}}[i]$.\n$3$. The release-acquire sum $S^{\\text{ra}}$ is, by definition, equal to $S^{\\star}$.\n$4$. The absolute error $E$ is calculated as $|S^{\\star} - S^{\\text{unsync}}|$.\n$5$. The correctness check $C$ is determined by the boolean expression $S^{\\text{ra}} == S^{\\star}$.\n$6$. The resulting pair $[E, C]$ is stored. Finally, all results are formatted into a single string as specified. For the edge case $n=0$, all sums are correctly evaluated as $0$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by modeling and comparing unsynchronized and synchronized\n    parallel reduction schemes.\n    \"\"\"\n    test_cases = [\n        # Case 1: n = 4, x_old = [1,2,3,4], x_new = [2,3,4,5], U = {}\n        (4, [1, 2, 3, 4], [2, 3, 4, 5], set()),\n        # Case 2: n = 4, x_old = [5,1,7,0], x_new = [6,2,8,3], U = {2}\n        (4, [5, 1, 7, 0], [6, 2, 8, 3], {2}),\n        # Case 3 (boundary): n = 0, x_old = [], x_new = [], U = {}\n        (0, [], [], set()),\n        # Case 4: n = 3, x_old = [10,20,30], x_new = [11,21,31], U = {1,2,3}\n        (3, [10, 20, 30], [11, 21, 31], {1, 2, 3}),\n    ]\n\n    results = []\n    \n    for n, x_old, x_new, U in test_cases:\n        # Calculate the expected correct reduction S_star\n        # S_star = sum(x_i_new for all i)\n        s_star = np.sum(x_new)\n\n        # Calculate the unsynchronized reduction S_unsync\n        # This simulates a race condition where for indices in U, the old value is read.\n        # Note: The problem uses 1-based indexing for U, so we adjust for 0-based list access.\n        s_unsync = 0\n        for i in range(n):\n            if (i + 1) in U:\n                # Adversarial interleaving: read stale data\n                s_unsync += x_old[i]\n            else:\n                # Correct read\n                s_unsync += x_new[i]\n        \n        # Calculate the SPSC release-acquire reduction S_ra\n        # By construction with release-acquire semantics, this must always be correct.\n        s_ra = np.sum(x_new)\n\n        # Compute the absolute error E for the unsynchronized model\n        # E = |S_star - S_unsync|\n        e = abs(s_star - s_unsync)\n        \n        # Compute the correctness check C for the SPSC model\n        # C = (S_ra == S_star)\n        c = (s_ra == s_star)\n        \n        results.append([e, c])\n\n    # Format the final output string exactly as specified: a list of lists,\n    # with no spaces within the sub-lists or between them.\n    # e.g., \"[[0,True],[1,True],[0,True],[3,True]]\"\n    formatted_results = [f\"[{e},{'True' if c else 'False'}]\" for e, c in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}