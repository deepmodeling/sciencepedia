{
    "hands_on_practices": [
        {
            "introduction": "Understanding the classification of a partial differential equation begins with its principal part. This foundational exercise connects the coefficients of a general second-order linear PDE to the geometric notion of characteristic curves, which represent paths of information propagation. By deriving the equation for the slopes of these curves, you will see precisely how the discriminant, $\\Delta = b^2 - ac$, determines whether the PDE is hyperbolic, parabolic, or elliptic, providing a concrete basis for the classification scheme. ",
            "id": "3992010",
            "problem": "In computational fusion science and engineering, two-dimensional reduced models in the poloidal–radial plane often involve second-order linear operators whose leading-order behavior determines wave propagation, diffusion, or mixed behavior. Consider the principal part of a linearized operator acting on a scalar field $u(x,y)$, written locally as\n$$\na(x,y)\\,u_{xx} + 2\\,b(x,y)\\,u_{xy} + c(x,y)\\,u_{yy} \\, ,\n$$\nwhere $a(x,y)$, $b(x,y)$, and $c(x,y)$ are smooth functions determined by the underlying physics, such as the symmetry-reduced metric coefficients of flux-surface coordinates or an anisotropic transport tensor aligned with the magnetic field in Magnetohydrodynamics (MHD). The principal symbol is\n$$\nP(\\boldsymbol{\\xi}) \\equiv a\\,\\xi_x^{2} + 2\\,b\\,\\xi_x\\,\\xi_y + c\\,\\xi_y^{2} \\, .\n$$\nCharacteristic curves are defined by the vanishing of the principal symbol on the normal covector to the curve. Let a characteristic curve be described locally as the level set $F(x,y)=0$, and denote its slope by $m \\equiv \\frac{dy}{dx}$.\n\nStarting from the definition of the principal symbol and the characteristic condition $P(\\nabla F)=0$, derive the quadratic equation governing the slope $m$ of characteristic curves and solve it in closed form to obtain the two characteristic directions $m_{+}(x,y)$ and $m_{-}(x,y)$. Then relate the number of real solutions to the classification of the partial differential equation (hyperbolic, parabolic, or elliptic) in terms of the discriminant. Express your final answer as the ordered pair $\\big(m_{+},\\,m_{-}\\big)$ as analytic expressions in $a$, $b$, and $c$.",
            "solution": "The operator’s leading-order behavior is captured by its principal symbol,\n$$\nP(\\boldsymbol{\\xi}) = a\\,\\xi_x^{2} + 2\\,b\\,\\xi_x\\,\\xi_y + c\\,\\xi_y^{2} \\, ,\n$$\nwhich is a quadratic form in the covector $\\boldsymbol{\\xi} = (\\xi_x,\\xi_y)$. A curve $F(x,y)=0$ has normal covector $\\nabla F = (F_x,F_y)$. By the definition of characteristic curves for second-order linear partial differential equations, the principal symbol vanishes on the normal covector to a characteristic:\n$$\nP(\\nabla F) \\;=\\; a\\,F_x^{2} + 2\\,b\\,F_x\\,F_y + c\\,F_y^{2} \\;=\\; 0 \\, .\n$$\nTo connect this to the slope $m \\equiv \\frac{dy}{dx}$ of the curve $F(x,y)=0$, we use the total derivative along the curve:\n$$\n\\frac{d}{dx} F(x,y(x)) \\;=\\; F_x + F_y\\,\\frac{dy}{dx} \\;=\\; 0 \\;\\;\\Rightarrow\\;\\; m \\equiv \\frac{dy}{dx} \\;=\\; -\\,\\frac{F_x}{F_y} \\, ,\n$$\nprovided $F_y \\neq 0$ (the complementary case is analogous and yields vertical characteristics when $F_y=0$). Substitute $F_x = -\\,m\\,F_y$ into $P(\\nabla F)=0$ and divide by $F_y^{2}$ (which is nonzero along a regular curve):\n$$\na\\,\\left(\\frac{F_x}{F_y}\\right)^{2} + 2\\,b\\,\\left(\\frac{F_x}{F_y}\\right) + c \\;=\\; 0\n\\;\\;\\Rightarrow\\;\\;\na\\,(-m)^{2} + 2\\,b\\,(-m) + c \\;=\\; 0 \\, .\n$$\nSimplifying, we obtain the quadratic equation for the slope $m$:\n$$\na\\,m^{2} - 2\\,b\\,m + c \\;=\\; 0 \\, .\n$$\nSolving this quadratic for $m$ yields the two characteristic directions,\n$$\nm_{\\pm} \\;=\\; \\frac{2\\,b \\,\\pm\\, \\sqrt{(2\\,b)^{2} - 4\\,a\\,c}}{2\\,a}\n\\;=\\;\n\\frac{b \\,\\pm\\, \\sqrt{\\,b^{2} - a\\,c\\,}}{a} \\, ,\n$$\nassuming $a \\neq 0$ at the point of interest. The discriminant controlling the nature of the roots is\n$$\n\\Delta \\;\\equiv\\; b^{2} - a\\,c \\, .\n$$\nThe classification follows directly from the number and nature of the real roots $m_{\\pm}$:\n- If $\\Delta > 0$, there are two distinct real roots $m_{+} \\neq m_{-}$, and the operator is hyperbolic at the point.\n- If $\\Delta = 0$, there is a repeated real root $m_{+} = m_{-}$, and the operator is parabolic at the point.\n- If $\\Delta  0$, there are no real roots (the slopes are complex), and the operator is elliptic at the point.\n\nIn fusion-relevant contexts, an anisotropic but positive-definite transport tensor aligned with the magnetic field typically gives $a>0$, $c>0$, and $a\\,c - b^{2} > 0$, hence $\\Delta  0$ and elliptic behavior (diffusion-dominated). In contrast, linearized wave operators associated with Magnetohydrodynamics (MHD) modes can yield $\\Delta > 0$, corresponding to hyperbolic behavior and real characteristic directions for wave propagation.\n\nThus, the requested analytic expressions for the characteristic slopes are\n$$\nm_{+} \\;=\\; \\frac{b + \\sqrt{\\,b^{2} - a\\,c\\,}}{a}\n\\quad\\text{and}\\quad\nm_{-} \\;=\\; \\frac{b - \\sqrt{\\,b^{2} - a\\,c\\,}}{a} \\, .\n$$",
            "answer": "$$\\boxed{\\left(\\frac{b + \\sqrt{\\,b^{2} - a\\,c\\,}}{a}, \\frac{b - \\sqrt{\\,b^{2} - a\\,c\\,}}{a}\\right)}$$"
        },
        {
            "introduction": "In many physical systems, such as near resonant layers in fusion plasmas, the governing equations can change type within the domain. This practice explores this phenomenon using the canonical Tricomi equation, which transitions from elliptic to hyperbolic. By applying the principles of characteristic analysis, you will derive and integrate the equations for the characteristic curves, explicitly revealing how they are real in the hyperbolic region and complex in the elliptic region, demonstrating a powerful application of the theory to a non-trivial problem. ",
            "id": "3992020",
            "problem": "In computational fusion science and engineering, mixed-type partial differential equations arise in reduced models of transonic magnetohydrodynamic (MHD) equilibria and wave propagation near critical surfaces. Consider the canonical Tricomi equation\n$$\nu_{xx} + x\\,u_{yy} = 0,\n$$\nwhich captures a change of type across the line $x=0$. Starting only from the foundational notions that (i) classification of a linear second-order partial differential equation in two variables is determined by the principal symbol and its sign properties, and (ii) characteristic curves are defined geometrically as those along which the principal symbol vanishes when evaluated on conormals to the curve, do the following:\n\n- Determine, from first principles, the type (elliptic, parabolic, hyperbolic) of the equation in the regions $x0$, $x=0$, and $x0$, and explain in physical terms why a change of type is expected in transonic or critical-surface regimes in magnetized plasmas.\n- Derive the ordinary differential equation for characteristic curves in the $(x,y)$-plane implied by the geometric definition of characteristics in terms of the principal symbol.\n- Integrate this ordinary differential equation to obtain explicit characteristic invariants in the regions $x0$ and $x0$, and identify what happens at $x=0$.\n\nProvide your final answer as the two characteristic invariants written as a single row matrix, with each entry given piecewise on $x0$, $x=0$, and $x0$. No numerical rounding is required. Do not include any units. The final answer must be analytic expressions only, with no explanatory text inside the final answer box.",
            "solution": "A general second-order linear PDE in two independent variables $x$ and $y$ can be written as:\n$$\nA(x,y) u_{xx} + B(x,y) u_{xy} + C(x,y) u_{yy} + D(x,y) u_x + E(x,y) u_y + F(x,y) u = G(x,y)\n$$\nThe classification of the PDE is determined by its principal part, which consists of the highest-order derivatives: $A u_{xx} + B u_{xy} + C u_{yy}$.\n\nPer the problem's first foundational notion, the type is determined by the principal symbol. The principal symbol, $P(x,y,k_x,k_y)$, is a quadratic form obtained by replacing the partial derivatives $\\partial_x$ and $\\partial_y$ with dual variables (conormals) $k_x$ and $k_y$, respectively:\n$$\nP(x,y; k_x,k_y) = A(x,y)k_x^2 + B(x,y)k_x k_y + C(x,y)k_y^2\n$$\nThe sign of this quadratic form determines the equation's type. This is conventionally assessed using the discriminant, $\\Delta = B^2 - 4AC$.\n\nFor the given Tricomi equation, $u_{xx} + x\\,u_{yy} = 0$, we identify the coefficients: $A=1$, $B=0$, and $C=x$.\nThe discriminant is $\\Delta(x) = (0)^2 - 4(1)(x) = -4x$.\n\n**Part 1: Classification and Physical Context**\n\nThe type of the equation depends on the sign of $\\Delta = -4x$, which varies with $x$:\n- **For $x > 0$**: The discriminant $\\Delta = -4x  0$. The principal symbol $k_x^2 + xk_y^2$ is a positive-definite quadratic form. The equation is **elliptic**.\n- **For $x = 0$**: The discriminant $\\Delta = -4(0) = 0$. The principal symbol is $k_x^2$, which is positive semi-definite. The equation is **parabolic**. The line $x=0$ is the parabolic line or sonic line.\n- **For $x  0$**: The discriminant $\\Delta = -4x > 0$. The principal symbol $k_x^2 + xk_y^2$ is an indefinite quadratic form. The equation is **hyperbolic**.\n\n**Physical Explanation**: In computational fusion science, this change of type models phenomena near a critical surface in a magnetized plasma, which is analogous to a sonic transition in fluid dynamics.\n- In the **elliptic region ($x>0$)**, which corresponds to a \"subsonic\" or sub-Alfvénic regime, information about a disturbance propagates in all directions. This requires specifying boundary conditions on a closed contour to determine the solution within, reflecting the global nature of the equilibrium or wave field.\n- In the **hyperbolic region ($x0$)**, corresponding to a \"supersonic\" or super-Alfvénic regime, information propagates along well-defined paths known as characteristics. The solution at a given point is determined by initial data on a non-characteristic open curve, reflecting the directional flow of information within a \"Mach cone\".\n- The **parabolic line ($x=0$)** represents the critical surface itself (e.g., a sonic or Alfvén resonance surface), where the background flow velocity matches a characteristic wave speed. At this transitional boundary, the governing physics changes fundamentally, and the mathematical character of the PDE degenerates.\n\n**Part 2: Derivation of the ODE for Characteristic Curves**\n\nAccording to the second foundational notion, characteristic curves are defined by the condition that the principal symbol vanishes for a conormal vector to the curve. Let a characteristic curve be a level set of a function $\\phi(x,y)$, i.e., $\\phi(x,y) = \\text{constant}$. The conormal vector is proportional to the gradient of $\\phi$, so we can set $(k_x, k_y) = (\\phi_x, \\phi_y)$.\n\nThe principal symbol for the Tricomi equation is $P(k_x, k_y) = k_x^2 + x k_y^2$. The condition for a characteristic curve is $P(\\phi_x, \\phi_y) = 0$:\n$$\n(\\phi_x)^2 + x (\\phi_y)^2 = 0\n$$\nAlong a level curve $\\phi(x,y) = \\text{constant}$, the total differential is zero:\n$$\nd\\phi = \\phi_x dx + \\phi_y dy = 0\n$$\nThis implies that the slope of the curve is given by $\\frac{dy}{dx} = -\\frac{\\phi_x}{\\phi_y}$. From the characteristic condition, assuming $\\phi_y \\neq 0$, we have $(\\frac{\\phi_x}{\\phi_y})^2 = -x$.\nSubstituting this into the expression for the slope, we obtain the ODE for the characteristic curves:\n$$\n\\left(\\frac{dy}{dx}\\right)^2 = -x\n$$\nThis can be written as two separate first-order ODEs:\n$$\n\\frac{dy}{dx} = \\pm \\sqrt{-x}\n$$\n\n**Part 3: Integration and Characteristic Invariants**\n\nWe integrate the ODE $\\frac{dy}{dx} = \\pm\\sqrt{-x}$ to find the families of characteristic curves, which are level sets of the characteristic invariants.\n\n- **In the hyperbolic region ($x0$)**: Here, $-x > 0$, so $\\sqrt{-x}$ is real. The characteristic curves are real.\n$$\n\\int dy = \\pm \\int \\sqrt{-x} \\, dx\n$$\nTo evaluate the integral, we use the substitution $u=-x$, so $du = -dx$.\n$$\ny = \\pm \\int \\sqrt{u} (-du) = \\mp \\frac{u^{3/2}}{3/2} + C = \\mp \\frac{2}{3}(-x)^{3/2} + C\n$$\nThe constants of integration, $C$, are the characteristic invariants. Rearranging, we find the two real invariants:\n$$\n\\zeta_{1,2}(x,y) = y \\pm \\frac{2}{3}(-x)^{3/2}\n$$\n\n- **In the elliptic region ($x>0$)**: Here, $-x  0$, so $\\sqrt{-x}$ is purely imaginary. Let $\\sqrt{-x} = i\\sqrt{x}$. There are no real characteristic curves. However, we can find the complex characteristic invariants by formally integrating the ODE:\n$$\n\\int dy = \\pm \\int i\\sqrt{x} \\, dx\n$$\n$$\ny = \\pm i \\frac{x^{3/2}}{3/2} + C = \\pm i\\frac{2}{3}x^{3/2} + C\n$$\nThe complex invariants are:\n$$\n\\zeta_{1,2}(x,y) = y \\mp i\\frac{2}{3}x^{3/2}\n$$\n\n- **On the parabolic line ($x=0$)**: The ODE becomes $\\frac{dy}{dx} = 0$, which integrates to $y = C$. There is only one family of characteristics, the horizontal lines $y = \\text{constant}$. This is consistent with the limit of the invariants from both the hyperbolic and elliptic regions as $x \\to 0$. As $x \\to 0$, both $(-x)^{3/2}$ and $x^{3/2}$ approach $0$, so both invariants $\\zeta_{1,2}$ collapse to the single invariant $y$.\n\n**Final Piecewise Expressions**:\nTo provide a consistent definition across the domains, we define two invariants, $\\zeta_1$ and $\\zeta_2$. Let's associate $\\zeta_1$ with the solution to $\\frac{dy}{dx} = +\\sqrt{-x}$ and $\\zeta_2$ with $\\frac{dy}{dx} = -\\sqrt{-x}$.\n\nFor $\\frac{dy}{dx} = +\\sqrt{-x}$: The invariant is $C = y - \\int \\sqrt{-x} dx$.\n- If $x0$, $C = y - (-\\frac{2}{3}(-x)^{3/2}) = y + \\frac{2}{3}(-x)^{3/2}$.\n- If $x>0$, $C = y - \\int i\\sqrt{x} dx = y - i\\frac{2}{3}x^{3/2}$.\n\nFor $\\frac{dy}{dx} = -\\sqrt{-x}$: The invariant is $C = y - \\int -\\sqrt{-x} dx = y + \\int \\sqrt{-x} dx$.\n- If $x0$, $C = y + (-\\frac{2}{3}(-x)^{3/2}) = y - \\frac{2}{3}(-x)^{3/2}$.\n- If $x>0$, $C = y + \\int i\\sqrt{x} dx = y + i\\frac{2}{3}x^{3/2}$.\n\nThus, the two characteristic invariants are:\n$$\n\\zeta_1(x,y) =\n\\begin{cases}\ny + \\frac{2}{3}(-x)^{3/2}  \\text{if } x  0 \\\\\ny  \\text{if } x = 0 \\\\\ny - i \\frac{2}{3}x^{3/2}  \\text{if } x > 0\n\\end{cases}\n$$\n$$\n\\zeta_2(x,y) =\n\\begin{cases}\ny - \\frac{2}{3}(-x)^{3/2}  \\text{if } x  0 \\\\\ny  \\text{if } x = 0 \\\\\ny + i \\frac{2}{3}x^{3/2}  \\text{if } x > 0\n\\end{cases}\n$$\nThese expressions are required to be presented as a row matrix for the final answer.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\begin{cases} y + \\frac{2}{3}(-x)^{3/2}  \\text{if } x  0 \\\\ y  \\text{if } x=0 \\\\ y - i \\frac{2}{3}x^{3/2}  \\text{if } x  0 \\end{cases}\n\n\\begin{cases} y - \\frac{2}{3}(-x)^{3/2}  \\text{if } x  0 \\\\ y  \\text{if } x=0 \\\\ y + i \\frac{2}{3}x^{3/2}  \\text{if } x  0 \\end{cases}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "This practice bridges the gap between the continuous theory of PDEs and the discrete world of computational engineering. The choice of a numerical solver is not arbitrary; it is deeply informed by the mathematical nature of the problem. In this coding exercise, you will construct discrete operators for canonical elliptic and hyperbolic problems and analyze their spectral properties, discovering how the PDE classification manifests in the eigenvalue distribution of the system matrix and dictates the appropriate choice between solvers like Conjugate Gradient and GMRES. ",
            "id": "3992007",
            "problem": "You are asked to construct and analyze discrete linear operators that reflect the spectral signatures of elliptic and hyperbolic steady-state Partial Differential Equations (PDEs) relevant to computational fusion science and engineering. Starting from first principles, use the definitions of PDE classification via the principal part and the mathematical properties of standard finite difference semi-discretizations to examine how eigenvalue distributions inform iterative solver choice. Your program must be a single, complete, runnable script that, for each test case provided, builds the matrix, computes spectral diagnostics, forms canonical iteration matrices, and outputs quantitative indicators.\n\nBegin from the following fundamental base:\n- For a second-order linear PDE in two dimensions with constant coefficients, the principal part can be written as $$\\mathcal{L} u = a_{11} \\frac{\\partial^2 u}{\\partial x^2} + 2 a_{12} \\frac{\\partial^2 u}{\\partial x \\partial y} + a_{22} \\frac{\\partial^2 u}{\\partial y^2},$$ with the classification determined by the definiteness of the symmetric matrix $$\\mathbf{A} = \\begin{pmatrix} a_{11}  a_{12} \\\\ a_{12}  a_{22} \\end{pmatrix}.$$ Elliptic type corresponds to $\\mathbf{A}$ positive definite, which for the isotropic diffusion operator reduces to coefficients $a_{11} = k_x$, $a_{22} = k_y$, $a_{12} = 0$ with $k_x > 0$ and $k_y > 0$.\n- For a first-order linear advection PDE in one dimension, the principal part is $$\\mathcal{H} u = a \\frac{\\partial u}{\\partial x},$$ which is hyperbolic and, under periodic boundary conditions with central differences, yields a discrete spatial operator that is skew-symmetric, indicating purely imaginary eigenvalues in the semi-discrete spectrum. To avoid singularity in the steady operator, a small real diagonal shift can be added to represent damping.\n- For a linear system $$\\mathbf{A} \\mathbf{u} = \\mathbf{b},$$ the Jacobi iteration matrix is $$\\mathbf{M}_{J} = \\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A},$$ where $\\mathbf{D}$ is the diagonal of $\\mathbf{A}$. The Gauss–Seidel iteration matrix can be written as $$\\mathbf{M}_{GS} = -(\\mathbf{D} + \\mathbf{L})^{-1} \\mathbf{U},$$ where $\\mathbf{A} = \\mathbf{D} + \\mathbf{L} + \\mathbf{U}$ with $\\mathbf{L}$ strictly lower triangular and $\\mathbf{U}$ strictly upper triangular. Convergence of a stationary iteration is characterized by the spectral radius $$\\rho(\\mathbf{M})  1.$$\n- For symmetric positive definite (SPD) matrices, the Conjugate Gradient method is an optimal Krylov solver. For general nonsymmetric matrices, the Generalized Minimal Residual method (GMRES) is the canonical choice.\n\nTasks to implement for each test case:\n1. Construct the discrete operator matrix $\\mathbf{A}$ as prescribed by the case definition.\n2. Compute the full spectrum $\\{\\lambda_i\\}$ of $\\mathbf{A}$ and the fraction of eigenvalues with nonzero imaginary part (use a tolerance of $10^{-10}$).\n3. Determine whether $\\mathbf{A}$ is symmetric, and whether it is symmetric positive definite (SPD) by symmetry and by checking all eigenvalues are strictly positive (use a positivity tolerance of $10^{-12}$).\n4. Form the Jacobi and Gauss–Seidel iteration matrices $\\mathbf{M}_J$ and $\\mathbf{M}_{GS}$, and compute their spectral radii $\\rho(\\mathbf{M}_J)$ and $\\rho(\\mathbf{M}_{GS})$.\n5. Recommend a Krylov solver based on classification: output $0$ for Conjugate Gradient if SPD, else $1$ for Generalized Minimal Residual.\n\nDiscretizations to use:\n- Elliptic operator (anisotropic diffusion): consider the steady operator $$\\mathcal{L} u = -\\frac{\\partial}{\\partial x}\\left(k_x \\frac{\\partial u}{\\partial x}\\right) - \\frac{\\partial}{\\partial y}\\left(k_y \\frac{\\partial u}{\\partial y}\\right),$$ on a rectangular domain with homogeneous Dirichlet boundary conditions. Use a uniform grid with spacings $h_x$ and $h_y$, and approximate with the standard five-point stencil. For interior node $(i,j)$, the discrete operator has diagonal entry $$a_{ii} = \\frac{2 k_x}{h_x^2} + \\frac{2 k_y}{h_y^2},$$ and nearest-neighbor couplings $$-\\frac{k_x}{h_x^2}$$ in the $\\pm x$ directions and $$-\\frac{k_y}{h_y^2}$$ in the $\\pm y$ directions.\n- Hyperbolic operator (steady advection with periodic boundary conditions): consider $$\\mathcal{H} u = a \\frac{\\partial u}{\\partial x},$$ discretized with central differences on a uniform periodic grid of spacing $h$. The circulant matrix has off-diagonals $$+\\frac{a}{2h}$$ at the $+1$ shift and $$-\\frac{a}{2h}$$ at the $-1$ shift, with wrap-around. Add a small diagonal damping $$\\epsilon \\mathbf{I}$$ to avoid singularity.\n\nTest suite:\n- Case 1 (elliptic, isotropic): two-dimensional grid with $n_x = 4$, $n_y = 4$, $h_x = h_y = 1$, $k_x = 1$, $k_y = 1$.\n- Case 2 (hyperbolic, periodic central difference): one-dimensional periodic grid with $N = 31$, $h = 1/N$, advection speed $a = 1$, diagonal damping $\\epsilon = 10^{-2}$.\n- Case 3 (elliptic, strongly anisotropic): two-dimensional grid with $n_x = 3$, $n_y = 5$, $h_x = h_y = 1$, $k_x = 100$, $k_y = 1$.\n\nRequired outputs for each case as a list in the order: \n- a boolean indicating symmetry of $\\mathbf{A}$,\n- a boolean indicating SPD classification of $\\mathbf{A}$,\n- a float giving the fraction of eigenvalues with nonzero imaginary part,\n- a float for $\\rho(\\mathbf{M}_J)$,\n- a float for $\\rho(\\mathbf{M}_{GS})$,\n- an integer solver recommendation ($0$ for Conjugate Gradient, $1$ for Generalized Minimal Residual).\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the list described above for a test case. For example: $$[\\text{case1\\_list},\\text{case2\\_list},\\text{case3\\_list}]$$\n- No physical units are required; all quantities are dimensionless. Angles do not appear. Percentages should be expressed as decimal fractions.\n\nYour implementation must be fully self-contained and must not require any external input; use only the libraries specified in the execution environment.",
            "solution": "The problem requires a thorough analysis of the spectral properties of discrete linear operators derived from canonical elliptic and hyperbolic Partial Differential Equations (PDEs). The objective is to connect the theoretical classification of the PDE to the practical performance of iterative linear solvers by constructing the operators, analyzing their spectra, and evaluating the convergence characteristics of standard stationary iterative methods. This analysis will be conducted for three specific test cases.\n\nThe methodological approach for each case is as follows:\n1.  **Matrix Construction ($\\mathbf{A}$):** The discrete operator matrix $\\mathbf{A}$ is constructed based on the specified PDE, domain, boundary conditions, and finite difference scheme.\n2.  **Spectral Analysis:** The eigenvalues $\\{\\lambda_i\\}$ of $\\mathbf{A}$ are computed. These are used to determine:\n    - **Symmetry:** The matrix $\\mathbf{A}$ is tested for symmetry, i.e., $\\mathbf{A} = \\mathbf{A}^T$.\n    - **Spectrum Location:** The fraction of eigenvalues with a non-negligible imaginary part ($|\\text{Im}(\\lambda_i)| > 10^{-10}$) is calculated.\n    - **Positive Definiteness (SPD):** A matrix is classified as Symmetric Positive Definite (SPD) if it is symmetric and all its eigenvalues are strictly positive ($\\lambda_i > 10^{-12}$).\n3.  **Iterative Method Analysis:** The Jacobi ($\\mathbf{M}_J$) and Gauss-Seidel ($\\mathbf{M}_{GS}$) iteration matrices are formed. Their spectral radii, $\\rho(\\mathbf{M}_J)$ and $\\rho(\\mathbf{M}_{GS})$, are computed to assess their theoretical convergence rates. An iterative method is convergent if and only if the spectral radius of its iteration matrix is less than $1$.\n    - Jacobi Matrix: $\\mathbf{M}_{J} = \\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A}$\n    - Gauss-Seidel Matrix: $\\mathbf{M}_{GS} = -(\\mathbf{D} + \\mathbf{L})^{-1} \\mathbf{U}$, where $\\mathbf{A} = \\mathbf{D} + \\mathbf{L} + \\mathbf{U}$ is the decomposition of $\\mathbf{A}$ into its diagonal, strictly lower triangular, and strictly upper triangular parts, respectively.\n4.  **Krylov Solver Recommendation:** Based on the properties of $\\mathbf{A}$, a suitable Krylov subspace method is recommended. The Conjugate Gradient (CG) method is the optimal choice for SPD systems ($0$), while the Generalized Minimal Residual (GMRES) method is a robust choice for general, non-symmetric systems ($1$).\n\nThe analysis is performed on each test case as detailed below.\n\n**Case 1: Isotropic Elliptic Operator**\nThis case involves the operator $\\mathcal{L} u = -\\nabla \\cdot (k \\nabla u)$ with isotropic conductivity ($k_x = k_y = 1$) on a $4 \\times 4$ grid of interior points ($n_x=4$, $n_y=4$) with unit grid spacing ($h_x=h_y=1$). Homogeneous Dirichlet boundary conditions are applied. The problem is discretized using a five-point finite difference stencil. The total number of unknowns is $N = n_x \\times n_y = 16$. Using a lexicographical ordering of the grid points, the resulting $16 \\times 16$ matrix $\\mathbf{A}$ is a block tridiagonal matrix.\n-   The diagonal entries are $a_{k,k} = \\frac{2k_x}{h_x^2} + \\frac{2k_y}{h_y^2} = \\frac{2 \\cdot 1}{1^2} + \\frac{2 \\cdot 1}{1^2} = 4$.\n-   Off-diagonal entries corresponding to neighbors in the x- and y-directions are $-\\frac{k_x}{h_x^2} = -1$ and $-\\frac{k_y}{h_y^2} = -1$, respectively.\nThis discrete operator is a finite-dimensional representation of the negative Laplacian, which is a self-adjoint (symmetric) and positive definite operator. Consequently, the matrix $\\mathbf{A}$ is expected to be symmetric with real, positive eigenvalues. This qualifies it for the CG solver. The Jacobi and Gauss-Seidel methods are expected to converge, with $\\rho(\\mathbf{M}_{GS}) \\approx \\rho(\\mathbf{M}_J)^2$ as the matrix has 'Property A' and is consistently ordered.\n\n**Case 2: Hyperbolic Operator**\nThis case examines the steady-state advection equation $\\mathcal{H} u = a \\frac{\\partial u}{\\partial x}$ on a 1D periodic domain with $N=31$ grid points. The parameters are $a=1$, grid spacing $h=1/N$, and a small artificial damping $\\epsilon = 10^{-2}$. The operator is discretized using a central difference scheme, leading to a circulant matrix.\n- The diagonal entries are equal to the damping parameter, $\\epsilon = 10^{-2}$.\n- The entry on the first super-diagonal is $\\frac{a}{2h} = \\frac{1}{2(1/31)} = 15.5$.\n- The entry on the first sub-diagonal is $-\\frac{a}{2h} = -15.5$.\n- Periodicity introduces wrap-around entries: $A_{0,N-1} = -15.5$ and $A_{N-1,0} = 15.5$.\nThe pure central difference operator (without damping) is skew-symmetric ($\\mathbf{A}_{adv}^T = -\\mathbf{A}_{adv}$), resulting in purely imaginary eigenvalues. The addition of the diagonal term $\\epsilon\\mathbf{I}$ shifts the entire spectrum by $\\epsilon$ along the real axis. The resulting matrix $\\mathbf{A} = \\epsilon\\mathbf{I} + \\mathbf{A}_{adv}$ is not symmetric. Its eigenvalues will be complex (of the form $\\epsilon + i\\beta$), with the exception of one eigenvalue which will be purely real. The matrix is not SPD, thus requiring a general-purpose solver like GMRES. For stationary methods, the Jacobi iteration matrix is $\\mathbf{M}_J = \\mathbf{I} - \\mathbf{D}^{-1}\\mathbf{A} = \\mathbf{I} - (\\epsilon\\mathbf{I})^{-1}(\\epsilon\\mathbf{I} + \\mathbf{A}_{adv}) = -\\frac{1}{\\epsilon}\\mathbf{A}_{adv}$. Its spectral radius is $\\rho(\\mathbf{M}_J) = \\frac{1}{\\epsilon} \\rho(\\mathbf{A}_{adv}) = \\frac{1}{\\epsilon} \\frac{a}{h} \\sin(\\frac{\\pi(N-1)}{N}) \\approx \\frac{a}{\\epsilon h} = \\frac{1}{0.01 \\cdot (1/31)} = 3100$. This value is much greater than $1$, indicating rapid divergence.\n\n**Case 3: Strongly Anisotropic Elliptic Operator**\nThis case is similar to the first, but with strong anisotropy in the diffusion coefficients: $k_x = 100$ and $k_y = 1$. The grid of interior points is $n_x=3, n_y=5$, giving $N=15$ unknowns.\n-   The diagonal entries are $a_{k,k} = \\frac{2k_x}{h_x^2} + \\frac{2k_y}{h_y^2} = \\frac{2 \\cdot 100}{1^2} + \\frac{2 \\cdot 1}{1^2} = 202$.\n-   Off-diagonal entries are $-\\frac{k_x}{h_x^2} = -100$ (x-direction) and $-\\frac{k_y}{h_y^2} = -1$ (y-direction).\nLike Case 1, the underlying operator is self-adjoint and positive definite, so the matrix $\\mathbf{A}$ will be SPD with real, positive eigenvalues, making CG the appropriate solver. The strong anisotropy, however, typically leads to a more poorly conditioned matrix compared to the isotropic case. This is reflected in a spectral radius for Jacobi and Gauss-Seidel iterations that is closer to $1$, implying slower convergence for these stationary methods. For this consistently ordered matrix, we again expect $\\rho(\\mathbf{M}_{GS}) \\approx \\rho(\\mathbf{M}_J)^2$.\n\nThese theoretical expectations will be confirmed by the numerical implementation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef analyze_case(case_params):\n    \"\"\"\n    Constructs and analyzes the matrix for a given test case.\n    \"\"\"\n    case_type = case_params['type']\n    \n    if case_type == 'elliptic':\n        nx, ny = case_params['nx'], case_params['ny']\n        hx, hy = case_params['hx'], case_params['hy']\n        kx, ky = case_params['kx'], case_params['ky']\n        \n        N = nx * ny\n        A = np.zeros((N, N))\n        \n        val_diag = (2 * kx / hx**2) + (2 * ky / hy**2)\n        val_x = -kx / hx**2\n        val_y = -ky / hy**2\n        \n        for j in range(ny):\n            for i in range(nx):\n                k = i + j * nx\n                \n                # Diagonal entry\n                A[k, k] = val_diag\n                \n                # Off-diagonal entries (x-direction)\n                if i > 0:\n                    A[k, k - 1] = val_x\n                if i  nx - 1:\n                    A[k, k + 1] = val_x\n                \n                # Off-diagonal entries (y-direction)\n                if j > 0:\n                    A[k, k - nx] = val_y\n                if j  ny - 1:\n                    A[k, k + nx] = val_y\n\n    elif case_type == 'hyperbolic':\n        N = case_params['N']\n        h = case_params['h']\n        a = case_params['a']\n        eps = case_params['eps']\n        \n        A = np.zeros((N, N))\n        val_adv = a / (2 * h)\n        \n        for i in range(N):\n            A[i, i] = eps\n            A[i, (i + 1) % N] = val_adv\n            A[i, (i - 1 + N) % N] = -val_adv\n            \n    else:\n        raise ValueError(\"Unknown case type\")\n\n    # 1. Compute spectrum and related properties\n    eigvals = np.linalg.eigvals(A)\n    \n    # 2. Check symmetry\n    is_symmetric = np.allclose(A, A.T, atol=1e-12)\n    \n    # 3. Check for nonzero imaginary part\n    frac_imag = np.sum(np.abs(np.imag(eigvals)) > 1e-10) / len(eigvals)\n    \n    # 4. Check for SPD\n    # For a real matrix to be SPD, it must be symmetric and have all positive eigenvalues.\n    # If symmetric, eigenvalues are real.\n    is_spd = False\n    if is_symmetric:\n        if np.all(np.real(eigvals) > 1e-12):\n            is_spd = True\n\n    # 5. Form Jacobi and Gauss-Seidel matrices and compute spectral radii\n    D = np.diag(np.diag(A))\n    L = np.tril(A, k=-1)\n    U = np.triu(A, k=1)\n    \n    # Check if D is invertible (no zero diagonal elements)\n    if np.any(np.abs(np.diag(D))  1e-15):\n        rho_J = np.inf # Or handle as an error\n    else:\n        D_inv = np.linalg.inv(D)\n        Mj = np.identity(N) - D_inv @ A\n        eigvals_J = np.linalg.eigvals(Mj)\n        rho_J = np.max(np.abs(eigvals_J))\n\n    D_plus_L = D + L\n    if np.linalg.det(D_plus_L) == 0:\n         rho_GS = np.inf # Or handle as an error\n    else:\n        D_plus_L_inv = np.linalg.inv(D_plus_L)\n        Mgs = -D_plus_L_inv @ U\n        eigvals_GS = np.linalg.eigvals(Mgs)\n        rho_GS = np.max(np.abs(eigvals_GS))\n        \n    # 6. Recommend Krylov solver\n    solver_rec = 0 if is_spd else 1\n\n    return [is_symmetric, is_spd, float(frac_imag), float(rho_J), float(rho_GS), solver_rec]\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            'type': 'elliptic', \n            'nx': 4, 'ny': 4, 'hx': 1.0, 'hy': 1.0, \n            'kx': 1.0, 'ky': 1.0\n        },\n        {\n            'type': 'hyperbolic',\n            'N': 31, 'h': 1.0/31.0, 'a': 1.0, 'eps': 1e-2\n        },\n        {\n            'type': 'elliptic',\n            'nx': 3, 'ny': 5, 'hx': 1.0, 'hy': 1.0, \n            'kx': 100.0, 'ky': 1.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = analyze_case(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The string representation of each inner list is generated automatically by map(str, ...).\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}