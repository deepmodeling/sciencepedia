{
    "hands_on_practices": [
        {
            "introduction": "To write efficient scientific code, one must first understand the fundamental performance limits of the underlying hardware. The Roofline model offers a powerful framework for this analysis by relating a program's computational intensity to the hardware's peak performance and memory bandwidth. This exercise provides a foundational problem in performance modeling, asking you to determine the minimum arithmetic intensity required for a kernel to be compute-bound on a modern GPU . Mastering this concept allows you to quickly diagnose the primary performance bottleneck—computation or memory access—and focus your optimization efforts where they will have the most impact.",
            "id": "3977177",
            "problem": "A fusion magnetohydrodynamic kernel is implemented to run entirely on a single Graphics Processing Unit (GPU) within a heterogeneous exascale node. The node comprises $2$ Central Processing Units (CPUs), each attached to $128$ gigabytes of Double Data Rate (DDR) memory, and $4$ GPUs, each equipped with $80$ gigabytes of High Bandwidth Memory (HBM). Each GPU sustains a peak double-precision performance of $20$ tera Floating-Point Operations per second (TFLOP/s) and a sustained HBM bandwidth of $3$ terabytes per second (TB/s). The kernel streams its data from the GPU-resident HBM with no reuse beyond those streams, and the CPU memory and interconnect are not used in the kernel’s execution. Define arithmetic intensity $I$ as the ratio of floating-point operations executed to data bytes moved from HBM.\n\nUsing only fundamental performance and throughput considerations, determine the minimum arithmetic intensity $I^{\\star}$ (in flops per byte) required for the kernel to be compute-bound on a single GPU, where “compute-bound” means that the attainable performance is limited by the GPU’s peak floating-point throughput rather than by HBM data movement. Express your final answer as a simplified fraction in flops per byte. Do not round.",
            "solution": "The problem requires the determination of the minimum arithmetic intensity, denoted as $I^{\\star}$, for a computational kernel to be compute-bound on a single Graphics Processing Unit (GPU). The problem statement must first be validated for correctness and completeness.\n\nProblem Validation:\n\nStep 1: Extract Givens\n- Node composition: $2$ Central Processing Units (CPUs) and $4$ GPUs.\n- CPU memory: $128$ gigabytes of DDR memory per CPU.\n- GPU memory: $80$ gigabytes of High Bandwidth Memory (HBM) per GPU.\n- Single GPU peak double-precision performance ($P_{peak}$): $20$ tera Floating-Point Operations per second (TFLOP/s).\n- Single GPU sustained HBM bandwidth ($B_{peak}$): $3$ terabytes per second (TB/s).\n- Kernel context: The kernel executes entirely on a single GPU, streaming data from its HBM. CPU memory and interconnects are not used.\n- Definition of Arithmetic Intensity ($I$): The ratio of floating-point operations executed to data bytes moved from HBM, with units of flops/byte.\n- Definition of \"compute-bound\": The attainable performance is limited by the GPU’s peak floating-point throughput, not by HBM data movement.\n\nStep 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, relying on the fundamental principles of high-performance computing performance modeling, specifically the concept known as the Roofline model. The provided data for a modern GPU's performance ($P_{peak} = 20 \\times 10^{12}$ FLOP/s) and memory bandwidth ($B_{peak} = 3 \\times 10^{12}$ B/s) are realistic and dimensionally consistent. The problem is well-posed, providing clear definitions for all necessary terms and sufficient data to arrive at a unique solution. The information regarding the node's CPUs, their memory, and the total number of GPUs is contextual and extraneous to the core question, which focuses on the performance characteristics of a single GPU, but this does not create a contradiction or invalidate the problem. The problem statement is objective and free of ambiguity.\n\nStep 3: Verdict and Action\nThe problem is valid. A solution will be formulated based on the provided data and definitions.\n\nSolution Formulation:\n\nThe execution time of the kernel, $T$, is limited by two distinct factors: the time required to perform the computations ($T_{compute}$) and the time required to move the necessary data from HBM to the processing units ($T_{memory}$). The actual execution time will be the greater of these two, as computation and data movement can often be overlapped, but the overall process cannot complete faster than its slowest component.\n$$T \\ge \\max(T_{compute}, T_{memory})$$\nLet $F$ be the total number of floating-point operations performed by the kernel, and let $D$ be the total number of data bytes moved from HBM.\n\nThe time required for computation is determined by the peak floating-point performance of the GPU, $P_{peak}$:\n$$T_{compute} = \\frac{F}{P_{peak}}$$\nThe time required for data movement is determined by the sustained memory bandwidth of the HBM, $B_{peak}$:\n$$T_{memory} = \\frac{D}{B_{peak}}$$\nThe attainable performance of the kernel, $P_{attainable}$, is the total number of floating-point operations divided by the execution time, $T$. Therefore, the performance is upper-bounded as follows:\n$$P_{attainable} = \\frac{F}{T} \\le \\frac{F}{\\max(T_{compute}, T_{memory})}$$\nSubstituting the expressions for $T_{compute}$ and $T_{memory}$:\n$$P_{attainable} \\le \\frac{F}{\\max\\left(\\frac{F}{P_{peak}}, \\frac{D}{B_{peak}}\\right)} = \\min\\left(\\frac{F}{F/P_{peak}}, \\frac{F}{D/B_{peak}}\\right) = \\min\\left(P_{peak}, \\frac{F}{D} \\cdot B_{peak}\\right)$$\nThe problem defines arithmetic intensity as $I = F/D$. Substituting this definition into the performance bound expression gives:\n$$P_{attainable} \\le \\min(P_{peak}, I \\cdot B_{peak})$$\nThis expression mathematically captures the two performance regimes. If $I \\cdot B_{peak} < P_{peak}$, the kernel is memory-bound, and its performance is limited by the rate at which data can be supplied. If $I \\cdot B_{peak} > P_{peak}$, the kernel is compute-bound, and its performance is limited by the peak computational throughput of the hardware.\n\nThe problem asks for the minimum arithmetic intensity, $I^{\\star}$, required for the kernel to be compute-bound. This transition point occurs when the performance limit imposed by memory bandwidth is equal to the performance limit of the compute units. At this crossover point, the system is perfectly balanced.\n$$I^{\\star} \\cdot B_{peak} = P_{peak}$$\nSolving for $I^{\\star}$ yields:\n$$I^{\\star} = \\frac{P_{peak}}{B_{peak}}$$\nWe are given the following values:\n$P_{peak} = 20$ TFLOP/s $= 20 \\times 10^{12}$ flops/s.\n$B_{peak} = 3$ TB/s $= 3 \\times 10^{12}$ bytes/s.\n\nSubstituting these values into the equation for $I^{\\star}$:\n$$I^{\\star} = \\frac{20 \\times 10^{12} \\text{ flops/s}}{3 \\times 10^{12} \\text{ bytes/s}}$$\nThe factor of $10^{12}$ in the numerator and denominator cancels out:\n$$I^{\\star} = \\frac{20}{3} \\text{ flops/byte}$$\nThis is the minimum arithmetic intensity required to ensure that the kernel's performance is limited by the GPU's computational capacity rather than its memory bandwidth. The result is already a simplified fraction as requested.",
            "answer": "$$\\boxed{\\frac{20}{3}}$$"
        },
        {
            "introduction": "For kernels that are memory-bound, improving performance hinges on maximizing the effective use of available memory bandwidth. On architectures like GPUs, which execute instructions on groups of threads (warps) in a Single Instruction, Multiple Threads (SIMT) fashion, memory access patterns are critical. This practice delves into the crucial choice between Array of Structures (AoS) and Structure of Arrays (SoA) data layouts for particle data . By analytically deriving the performance difference caused by memory coalescing, you will gain a quantitative appreciation for why data layout is a first-order concern in high-performance GPU programming for applications like Particle-In-Cell simulations.",
            "id": "3977148",
            "problem": "A canonical particle-in-cell (PIC) push kernel in a magnetized fusion plasma simulation updates particle positions and velocities using interpolated fields on a Graphics Processing Unit (GPU). Consider two data layouts for particle records: Array of Structures (AoS) and Structure of Arrays (SoA). In the AoS layout, each particle record stores six double-precision scalars, namely position components $x$, $y$, $z$ and velocity components $v_x$, $v_y$, $v_z$, contiguously in memory per particle. In the SoA layout, the same six quantities are stored in six separate arrays, each containing the corresponding component for all particles.\n\nAssume the following hardware and code-path facts that are standard for modern GPUs operating under Single Instruction Multiple Threads (SIMT):\n- The warp size is $W = 32$ threads.\n- Double precision width is $B = 8$ bytes.\n- The global memory transaction granularity is a segment of size $S = 128$ bytes, and a single memory transaction transfers one full segment aligned to $S$-byte boundaries.\n- Coalescing rule: for a given load or store instruction, the memory system issues one transaction per unique $S$-byte segment touched by the addresses referenced by the warp.\n- All particle arrays and AoS records are $S$-byte aligned at their base, and the first element of each field begins at the start of a segment.\n- The PIC push step issues $6$ separate load instructions per particle to read $(x,y,z,v_x,v_y,v_z)$ and $6$ separate store instructions to write the updated $(x,y,z,v_x,v_y,v_z)$. Each of these $12$ instructions is issued warp-wide, with each thread operating on its own particle. There is no cross-field reuse due to register lifetime or cache effects; assume no hardware vectorization beyond the per-thread scalar double loads and stores.\n\nUnder these assumptions:\n- In the SoA layout, for a given field (for example, the $x$ array), the warp’s addresses for that load (or store) are contiguous across threads.\n- In the AoS layout, for a given field (for example, the $x$ member inside each particle record), the warp’s addresses for that load (or store) are separated by the AoS record stride. Let the AoS record size be $6B$ bytes, and assume no padding.\n\nStarting from the above facts about warp coalescing and memory transactions, derive an exact analytic expression for the multiplicative bandwidth-efficiency speedup factor $R$ of SoA over AoS for the entire push step, defined as the ratio of useful bytes per physical bytes transferred under SoA to that under AoS, aggregated over all $12$ memory instructions (the $6$ loads and $6$ stores). Then compute $R$ numerically. Provide the multiplicative speedup $R$ as an exact real number. No rounding is required. Express your final result as a pure number without units.",
            "solution": "The problem asks for the multiplicative bandwidth-efficiency speedup factor, $R$, of a Structure of Arrays (SoA) data layout compared to an Array of Structures (AoS) data layout for a particle push operation on a GPU. This factor is defined as the ratio of the bandwidth efficiencies of the two layouts:\n$$R = \\frac{\\eta_{\\text{SoA}}}{\\eta_{\\text{AoS}}}$$\nThe bandwidth efficiency, $\\eta$, is the ratio of useful bytes transferred to the total physical bytes transferred from memory:\n$$\\eta = \\frac{\\text{Useful Bytes}}{\\text{Physical Bytes}}$$\nThe push step consists of $6$ load and $6$ store instructions, for a total of $12$ memory instructions. For each instruction, a full warp of $W$ threads operates, with each thread accessing one double-precision scalar of size $B$. Thus, the total useful data transferred for the entire push step for one warp is identical for both layouts:\n$$\\text{Total Useful Bytes} = 12 \\times W \\times B$$\nGiven that the useful bytes are the same, the speedup factor $R$ simplifies to the inverse ratio of the total physical bytes transferred:\n$$R = \\frac{\\frac{\\text{Total Useful Bytes}}{(\\text{Total Physical Bytes})_{\\text{SoA}}}}{\\frac{\\text{Total Useful Bytes}}{(\\text{Total Physical Bytes})_{\\text{AoS}}}} = \\frac{(\\text{Total Physical Bytes})_{\\text{AoS}}}{(\\text{Total Physical Bytes})_{\\text{SoA}}}$$\nOur task is to calculate the total physical bytes transferred for a warp executing the full push step for each layout. The physical bytes are determined by the number of memory transactions, where each transaction transfers a segment of size $S$.\n\nThe given parameters are:\n- Warp size: $W = 32$\n- Double-precision scalar width: $B = 8$ bytes\n- Memory segment size: $S = 128$ bytes\n\n**1. Analysis of the SoA (Structure of Arrays) Layout**\n\nIn the SoA layout, each of the six particle components ($x, y, z, v_x, v_y, v_z$) is stored in a separate, contiguous array. Consider a single warp-wide instruction, for example, loading the $x$ positions. The $W$ threads of the warp access $W$ consecutive double-precision values.\nThe total amount of contiguous data accessed by the warp for one instruction is:\n$$\\text{Data span} = W \\times B = 32 \\times 8 = 256 \\text{ bytes}$$\nSince the base of each array is aligned to an $S$-byte boundary, these $256$ bytes of data will perfectly occupy a set of consecutive memory segments. The number of memory transactions, $N_{\\text{SoA}}$, required to transfer this data is the number of unique $S$-byte segments touched.\n$$N_{\\text{SoA}} = \\frac{W \\times B}{S} = \\frac{256}{128} = 2$$\nEach of the $12$ memory instructions (6 loads, 6 stores) in the push step will thus require $2$ memory transactions.\nThe total physical bytes transferred for the entire push step in the SoA case is:\n$$(\\text{Total Physical Bytes})_{\\text{SoA}} = 12 \\times N_{\\text{SoA}} \\times S = 12 \\times 2 \\times 128 = 3072 \\text{ bytes}$$\nThe bandwidth efficiency for the SoA layout is:\n$$\\eta_{\\text{SoA}} = \\frac{12 \\times W \\times B}{(\\text{Total Physical Bytes})_{\\text{SoA}}} = \\frac{12 \\times 32 \\times 8}{3072} = \\frac{3072}{3072} = 1$$\nThis indicates perfect memory coalescing and 100% bandwidth efficiency.\n\n**2. Analysis of the AoS (Array of Structures) Layout**\n\nIn the AoS layout, all six components for a single particle are stored contiguously. The size of one particle record, which is the stride between the same component of consecutive particles, is:\n$$\\sigma = 6 \\times B = 6 \\times 8 = 48 \\text{ bytes}$$\nConsider a single warp-wide instruction, such as loading the $x$ component for all particles processed by the warp. Thread $i$ (where $i \\in \\{0, 1, \\dots, W-1\\}$) accesses the $x$ component of particle $i$. The memory addresses accessed by the warp are strided. The address for thread $i$ is offset by $i \\times \\sigma$ from the base address of the particle array.\nTo find the number of memory transactions, we must count the number of unique $S$-byte segments touched by the warp for a single instruction. Let's analyze the access for an arbitrary component $j \\in \\{0, 1, ..., 5\\}$, where $j=0$ for $x$, $j=1$ for $y$, and so on. The offset of component $j$ within a record is $j \\times B$. Assuming the base of the particle array is at address $0$ for simplicity, the address accessed by thread $i$ for component $j$ is $A(i, j) = i \\times \\sigma + j \\times B$.\nThe segment index for this access is $\\lfloor \\frac{A(i, j)}{S} \\rfloor$. The number of transactions, $N_{\\text{AoS}, j}$, is the number of unique values in the set of segment indices for a given $j$:\n$$N_{\\text{AoS}, j} = \\left| \\left\\{ \\left\\lfloor \\frac{i \\cdot \\sigma + j \\cdot B}{S} \\right\\rfloor \\Big| i \\in \\{0, \\dots, W-1\\} \\right\\} \\right|$$\nSubstituting the given values:\n$$N_{\\text{AoS}, j} = \\left| \\left\\{ \\left\\lfloor \\frac{i \\cdot 48 + j \\cdot 8}{128} \\right\\rfloor \\Big| i \\in \\{0, \\dots, 31\\} \\right\\} \\right| = \\left| \\left\\{ \\left\\lfloor \\frac{i \\cdot 6 + j}{16} \\right\\rfloor \\Big| i \\in \\{0, \\dots, 31\\} \\right\\} \\right|$$\nLet's evaluate this for any $j \\in \\{0, \\dots, 5\\}$. The function $f_j(i) = \\lfloor \\frac{6i+j}{16} \\rfloor$ is monotonically non-decreasing. The difference $f_j(i+1) - f_j(i)$ can be shown to be either $0$ or $1$, so the function does not skip any integer values between its minimum and maximum. The number of unique values is therefore $f_j(W-1) - f_j(0) + 1$.\nThe minimum value (for $i=0$) is:\n$$f_j(0) = \\left\\lfloor \\frac{j}{16} \\right\\rfloor = 0 \\quad (\\text{since } 0 \\le j \\le 5)$$\nThe maximum value (for $i = W-1=31$) is:\n$$f_j(31) = \\left\\lfloor \\frac{6 \\cdot 31 + j}{16} \\right\\rfloor = \\left\\lfloor \\frac{186 + j}{16} \\right\\rfloor$$\nFor $j=0$, this gives $\\lfloor 186/16 \\rfloor = \\lfloor 11.625 \\rfloor = 11$.\nFor $j=5$, this gives $\\lfloor 191/16 \\rfloor = \\lfloor 11.9375 \\rfloor = 11$.\nFor any $j \\in \\{0, \\dots, 5\\}$, the maximum value is $11$.\nTherefore, the number of unique segments for any component $j$ is:\n$$N_{\\text{AoS}, j} = 11 - 0 + 1 = 12$$\nThis means that for each of the $12$ memory instructions, the warp accesses $12$ distinct memory segments. Let $N_{\\text{AoS}} = 12$.\nThe total physical bytes transferred for the entire push step in the AoS case is:\n$$(\\text{Total Physical Bytes})_{\\text{AoS}} = 12 \\times N_{\\text{AoS}} \\times S = 12 \\times 12 \\times 128 = 18432 \\text{ bytes}$$\nThe bandwidth efficiency for the AoS layout is:\n$$\\eta_{\\text{AoS}} = \\frac{12 \\times W \\times B}{(\\text{Total Physical Bytes})_{\\text{AoS}}} = \\frac{3072}{18432} = \\frac{1}{6}$$\n\n**3. Calculation of the Speedup Factor R**\n\nNow we can calculate the speedup factor $R$:\n$$R = \\frac{(\\text{Total Physical Bytes})_{\\text{AoS}}}{(\\text{Total Physical Bytes})_{\\text{SoA}}}$$\nSubstituting the derived expressions for physical bytes:\n$$R = \\frac{12 \\times N_{\\text{AoS}} \\times S}{12 \\times N_{\\text{SoA}} \\times S} = \\frac{N_{\\text{AoS}}}{N_{\\text{SoA}}}$$\nUsing the calculated values for the number of transactions per instruction:\n$$R = \\frac{12}{2} = 6$$\nAlternatively, using the calculated efficiencies:\n$$R = \\frac{\\eta_{\\text{SoA}}}{\\eta_{\\text{AoS}}} = \\frac{1}{1/6} = 6$$\nThe multiplicative bandwidth-efficiency speedup factor of SoA over AoS is $6$.",
            "answer": "$$\\boxed{6}$$"
        },
        {
            "introduction": "Scaling fusion simulations to exascale systems requires distributing the problem domain across numerous interconnected nodes, which introduces communication as a major performance bottleneck. A common technique in structured-grid solvers is to exchange boundary data (halo or ghost cells) to satisfy stencil dependencies. This practice explores a classic trade-off in parallel computing: reducing communication frequency by exchanging larger amounts of data at once, at the cost of performing redundant computations in the wider halo regions . By deriving a performance model and finding the optimal halo exchange frequency, you will develop an essential skill in tuning distributed applications to balance computation and communication costs.",
            "id": "3977170",
            "problem": "A widely used approach in explicit magnetohydrodynamics and gyrokinetic solvers for fusion plasma simulation is structured grid domain decomposition with Message Passing Interface (MPI), in which each MPI rank owns a local brick of the global mesh and maintains halo (ghost) cells for nearest-neighbor stencils. Consider a $3$-dimensional brick of size $n_x \\times n_y \\times n_z$ cells per MPI rank. The numerical update per time step uses a nearest-neighbor stencil of radius $r$ cells per time step in each coordinate direction, so that information propagates at most $r$ cells per time step. To amortize communication, one may exchange a wider halo of width $w$ cells, then perform multiple time steps locally before the next exchange. Let the tunable parameter be the halo width per exchange measured in time steps, namely $h$, and set $w = r h$ so that exactly $h$ explicit time steps can be advanced locally without violating stencil dependencies.\n\nAssume the following performance model, grounded on first principles of work and data movement, for a single MPI rank:\n- Each cell update costs $t_u$ seconds, independent of location, and the rank updates all owned interior cells and all currently available halo cells during each local time step to keep halo data consistent until the next exchange.\n- Each communicated cell carries $b_c$ bytes. The network has per-message latency $\\lambda$ seconds and inverse bandwidth $\\beta$ seconds per byte. Each halo exchange aggregates all $6$ faces and is performed once every $h$ local time steps.\n- For large $n_x$, $n_y$, and $n_z$, neglect edge and corner ghost contributions relative to face contributions. Under this approximation, the number of ghost cells exchanged for halo width $w$ is $2 w (n_y n_z + n_x n_z + n_x n_y)$.\n- Communication is bulk-synchronous: computation and communication do not overlap.\n\nStarting from these assumptions and the definitions of work and data movement per time step and per exchange, first derive the average time per time step $T(h)$ as a function of $h$, reflecting the trade-off that increasing $h$ reduces communication frequency (especially amortizing latency) at the cost of extra redundant computation in the halos. Then, determine the value of $h$ that minimizes $T(h)$ in closed form. Your final answer must be a single analytical expression for the optimal $h$ in terms of $\\lambda$, $t_u$, $r$, $n_x$, $n_y$, and $n_z$. Express the answer as a dimensionless quantity. No numerical evaluation is required.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of parallel computing performance modeling, well-posed with sufficient information for a unique solution, and stated using objective, formal language. We can therefore proceed with the derivation.\n\nThe goal is to find the average time per time step, denoted as $T(h)$, and then find the value of $h$ that minimizes this time. The total time for a cycle of $h$ time steps is the sum of the total computation time and the communication time, as these are assumed to be non-overlapping.\n\nFirst, let us define the key quantities based on the problem statement.\n- The number of interior cells per MPI rank: $V = n_x n_y n_z$.\n- The number of cells on the faces of the local domain, under the approximation of neglecting edges and corners: Half the surface area is $A = n_y n_z + n_x n_z + n_x n_y$.\n- The stencil radius is $r$ cells. The number of time steps between halo exchanges is $h$.\n- The halo width required is $w = r h$ cells.\n\nThe total time for one cycle, $T_{cycle}(h)$, which consists of one communication event and $h$ computational steps, is given by:\n$$T_{cycle}(h) = W_{total}(h) + C(h)$$\nwhere $W_{total}(h)$ is the total computation time over $h$ steps and $C(h)$ is the communication time for one halo exchange.\n\n**1. Communication Cost $C(h)$**\nA halo exchange is performed once every $h$ time steps. The number of ghost cells to be exchanged, $N_{halo}$, for a halo of width $w$ is given as $2w(n_y n_z + n_x n_z + n_x n_y)$.\nUsing our notation:\n$$N_{halo}(w) = 2wA$$\nSubstituting $w=rh$:\n$$N_{halo}(h) = 2(rh)A = 2rhA$$\nEach communicated cell has a size of $b_c$ bytes. The total data volume, $B(h)$, exchanged is:\n$$B(h) = N_{halo}(h) \\times b_c = 2rhAb_c$$\nThe communication cost for one exchange consists of a latency component $\\lambda$ and a bandwidth component. The time is:\n$$C(h) = \\lambda + \\beta B(h) = \\lambda + \\beta (2rhAb_c)$$\n\n**2. Computation Cost $W_{total}(h)$**\nAccording to the problem, during each local time step, the rank updates all its owned interior cells and all the halo cells.\nThe number of cells updated in a single local time step, $N_{update}$, is the sum of interior cells and halo cells:\n$$N_{update}(h) = V + N_{halo}(h) = V + 2rhA$$\nThe cost of a single cell update is $t_u$ seconds. Therefore, the computation time for one local time step, $W_{step}(h)$, is:\n$$W_{step}(h) = N_{update}(h) \\times t_u = (V + 2rhA)t_u$$\nThis computation is performed for $h$ local time steps. The total computation time for the cycle is:\n$$W_{total}(h) = h \\times W_{step}(h) = h(V + 2rhA)t_u = (hV + 2rh^2A)t_u$$\n\n**3. Average Time Per Time Step $T(h)$**\nThe total time for a cycle of $h$ steps is the sum of the total computation and communication costs:\n$$T_{cycle}(h) = W_{total}(h) + C(h) = (hV + 2rh^2A)t_u + (\\lambda + 2rhAb_c\\beta)$$\nThe average time per time step, $T(h)$, is this total cycle time divided by the number of steps, $h$:\n$$T(h) = \\frac{T_{cycle}(h)}{h} = \\frac{(hV + 2rh^2A)t_u + \\lambda + 2rhAb_c\\beta}{h}$$\n$$T(h) = Vt_u + 2rhAt_u + \\frac{\\lambda}{h} + 2rAb_c\\beta$$\nWe can group the terms based on their dependence on $h$:\n$$T(h) = (2rAt_u)h + \\frac{\\lambda}{h} + (Vt_u + 2rAb_c\\beta)$$\nThis expression represents the trade-off. The term $(2rAt_u)h$ represents the cost of redundant computation in the halo, which grows linearly with $h$. The term $\\frac{\\lambda}{h}$ represents the amortized latency, which decreases as $h$ increases. The remaining terms are constant with respect to $h$.\n\n**4. Minimization of $T(h)$**\nTo find the optimal value of $h$ that minimizes $T(h)$, we take the derivative of $T(h)$ with respect to $h$ and set it to zero.\n$$\\frac{dT(h)}{dh} = \\frac{d}{dh} \\left( (2rAt_u)h + \\lambda h^{-1} + (Vt_u + 2rAb_c\\beta) \\right)$$\n$$\\frac{dT(h)}{dh} = 2rAt_u - \\lambda h^{-2} = 2rAt_u - \\frac{\\lambda}{h^2}$$\nSetting the derivative to zero to find the critical point:\n$$2rAt_u - \\frac{\\lambda}{h^2} = 0$$\n$$2rAt_u = \\frac{\\lambda}{h^2}$$\nSolving for $h^2$:\n$$h^2 = \\frac{\\lambda}{2rAt_u}$$\nSince $h$ must be positive, we take the positive square root:\n$$h_{opt} = \\sqrt{\\frac{\\lambda}{2rAt_u}}$$\nTo ensure this is a minimum, we examine the second derivative:\n$$\\frac{d^2T(h)}{dh^2} = \\frac{d}{dh} \\left( 2rAt_u - \\lambda h^{-2} \\right) = -(-2)\\lambda h^{-3} = \\frac{2\\lambda}{h^3}$$\nSince $\\lambda$ (latency) and $h$ (number of steps) are positive physical quantities, the second derivative is always positive. This confirms that our solution for $h$ corresponds to a minimum of $T(h)$.\n\nFinally, we substitute the expression for $A$ back into the equation for $h_{opt}$:\n$$h_{opt} = \\sqrt{\\frac{\\lambda}{2rt_u(n_y n_z + n_x n_z + n_x n_y)}}$$\nThis is the closed-form analytical expression for the optimal number of time steps $h$ that minimizes the average time per step. The quantity is dimensionless as requested.",
            "answer": "$$\\boxed{\\sqrt{\\frac{\\lambda}{2rt_u(n_y n_z + n_x n_z + n_x n_y)}}}$$"
        }
    ]
}