## 引言
随着聚变能源研究迈向预测科学的新时代，百亿亿次级（Exascale）计算已成为解锁等离子体复杂物理过程的关键。通过前所未有的高保真模拟，科学家们能够以前所未有的细节探索和理解[燃烧等离子体](@entry_id:1121942)的行为，从而加速未来聚变反应堆的设计与优化。然而，有效利用这些顶级计算资源并非易事。从根本上说，这不仅仅是拥有更快处理器的问题，而是面临着一个巨大的知识鸿沟：如何驾驭由数百万个[异构计算](@entry_id:750240)单元组成的复杂系统，并使其高效地服务于[聚变模拟](@entry_id:1125419)的独特需求。

本文旨在系统性地填补这一鸿沟，为读者提供一幅清晰的路[线图](@entry_id:264599)，以应对百亿亿次级[聚变模拟](@entry_id:1125419)的挑战。我们将分三个章节逐步深入：首先，在“**原理与机制**”一章中，我们将剖析构成百亿亿次级系统的硬件特性、限制并行扩展的基本定律以及驾驭这些系统所需的软件与算法策略。接着，在“**应用与跨学科连接**”一章中，我们将通过一系列具体的[聚变模拟](@entry_id:1125419)案例，展示这些原理如何与物理洞察相结合，并与数据科学、人工智能等领域交叉融合，以解决实际的科学问题。最后，通过“**动手实践**”部分，读者将有机会将理论知识应用于具体计算问题，加深理解。通过这一结构化的学习路径，本文将引导您从基本原理走向前沿应用，掌握在百亿亿次级时代进行尖端[计算聚变](@entry_id:1122783)研究所需的核心知识。

## 原理与机制

继前一章对百亿亿次级（Exascale）计算在[聚变模拟](@entry_id:1125419)中所扮演角色的宏观介绍之后，本章将深入探讨驱动这些模拟所必须掌握的核心技术原理与底层机制。我们将剖析构成百亿亿次级系统的硬件特性，分析并行计算的基本扩展定律，并探讨应对这些挑战的软件与算法策略。本章旨在为读者构建一个坚实的理论框架，用以理解、设计和优化面向未来[聚变能](@entry_id:138601)源研究的[大规模科学计算](@entry_id:155172)应用。

### 百亿亿次级硬件图景

[百亿亿次级计算](@entry_id:1124720)系统的性能根植于其复杂的硬件架构。理解其基本构成单元——计算节点——的特性，是迈向高效能模拟的第一步。

#### 异构节点架构与内存层次

现代高性能计算节点普遍采用**异构架构 (heterogeneous architecture)**，它将传统的**延迟优化 (latency-optimized)** 的多核中央处理器 (CPU) 与**吞吐量优化 (throughput-oriented)** 的图形处理器 (GPU) 或其他加速器结合在一起。这种设计旨在兼顾不同类型计算任务的需求。

一个典型的[百亿亿次级计算](@entry_id:1124720)节点可能包含多个CPU插槽和多个GPU。例如，考虑一个配置了2个CPU插槽和4个GPU的节点。每个CPU可能提供约 $2\,\text{TFLOP/s}$ 的[双精度](@entry_id:636927)峰值吞吐量，并连接到全节点共享的、容量较大的DDR（双倍数据速率）动态随机存取存储器，例如 $512\,\text{GB}$。与此同时，每个GPU则可能提供高达 $30\,\text{TFLOP/s}$ 的峰值性能，但其连接的是容量较小、带宽却极高的**[高带宽内存](@entry_id:1126106) (High Bandwidth Memory, [HBM](@entry_id:1126106))**，例如每卡 $80\,\text{GB}$。

这种架构带来了一个核心挑战：计算能力与内存容量之间的不平衡。我们可以通过计算节点的**峰值算力与内存容量之比 (compute-to-capacity ratio)** 来量化这一特性。对于上述假设的节点，其总峰值算力为 $P_{\text{peak}} = (2 \times 2) + (4 \times 30) = 124\,\text{TFLOP/s}$，总内存容量为 $M_{\text{cap}} = 512 + (4 \times 80) = 832\,\text{GB}$。其比率 $R = P_{\text{peak}}/M_{\text{cap}} \approx 0.149\,\text{TFLOP/s per GB}$。这个相对较高的比值意味着，要充分发挥节点的计算潜力，算法必须在有限的内存空间内实现极高的数据复用率。

对于像回旋动理学**粒子模拟 (Particle-In-Cell, PIC)** 这样的聚变应用，其状态数据由数以十亿计的粒子和三维网格组成。假设一个[PIC模拟](@entry_id:180612)包含 $N_p = 3 \times 10^9$ 个粒子，每个粒子需要 $56$ 字节存储状态，网格状态数据则需要约 $5\,\text{GB}$。总内存需求约为 $M_{\text{req}} \approx 168 + 5 = 173\,\text{GB}$。虽然这个大小远小于节点的总内存 $832\,\text{GB}$，但关键在于它是否能放入GPU的高速[HBM](@entry_id:1126106)中。在此例中，总需求 $173\,\text{GB}$ 小于4个GPU的总[HBM](@entry_id:1126106)容量 $320\,\text{GB}$，意味着通过数据划分，可以将整个问题的[工作集](@entry_id:756753)驻留在[HBM](@entry_id:1126106)中。这指导了我们的算法设计：必须采用高**[算术强度](@entry_id:746514) (arithmetic intensity)** 的策略，通过数据分块（tiling）等技术，最大化数据在加载到[HBM](@entry_id:1126106)后被重复计算的次数，从而“喂饱”强大的[GPU计算](@entry_id:174918)单元，避免因频繁访问慢速DDR内存而造成的性能瓶颈。

#### 内核与架构的映射：Roofline模型

不同类型的计算内核对延迟优化和[吞吐量](@entry_id:271802)优化的硬件有着截然不同的适应性。**Roofline模型**为我们提供了分析这种映射关系的有力工具。该模型指出，一个计算内核能够达到的性能 $P_{\text{bound}}$ 受限于两个因素：硬件的峰值计算吞吐量 $P_{\max}$ 和由[内存带宽](@entry_id:751847)决定的性能上限。后者的计算方式为内核的[算术强度](@entry_id:746514) $I$ （单位为“[浮点](@entry_id:749453)操作数/字节”）乘以系统的有效[内存带宽](@entry_id:751847) $B_{\text{eff}}$。因此，$P_{\text{bound}} = \min(P_{\max}, I \cdot B_{\text{eff}})$。

让我们通过两个典型的[聚变模拟](@entry_id:1125419)内核来说明这一点：一个是基于[结构化网格](@entry_id:755573)的**磁流体力学 (MHD)** 模板更新，另一个是PIC中的粒子推进与场-粒相互作用。假设一个GPU的 $P_{\text{max,GPU}} = 10\,\text{Tf/s}$，$B_{\text{GPU}} = 1000\,\text{GB/s}$；而一个CPU的 $P_{\text{max,CPU}} = 2\,\text{Tf/s}$，$B_{\text{CPU}} = 200\,\text{GB/s}$。

- **MHD模板计算**：这类计算具有高度的规则性，内存访问是连续的（单位步长），非常适合GPU的**[内存合并](@entry_id:178845)访问 (coalesced memory access)** 机制和CPU的缓存阻塞策略。其[算术强度](@entry_id:746514)可能为 $I_{\text{MHD}} = 0.25\,\text{flops/byte}$。在GPU上，由于访问模式良好，其[有效带宽](@entry_id:748805)几乎能达到峰值，而在CPU上也能高效利用缓存。计算表明，尽管此内核是**[内存带宽](@entry_id:751847)受限 (memory-bound)** 的，但GPU的高带宽使其性能远超CPU（例如 $0.225\,\text{Tf/s}$ vs $0.04\,\text{Tf/s}$）。

- **PIC粒子计算**：这类计算涉及从网格节点**收集 (gather)** 电磁场到粒子位置，以及将粒子电荷/电流**散射 (scatter)** 回网格节点。这些操作通常是间接和不规则的内存访问，导致GPU上的内存访问无法有效合并，并在需要[原子操作](@entry_id:746564)的散射步骤中产生线程竞争。这会显著降低有效[内存带宽](@entry_id:751847)（例如，效率因子 $\alpha$ 可能低至 $0.2$）。其[算术强度](@entry_id:746514)也往往较低，如 $I_{\text{PIC}} = 0.15\,\text{flops/byte}$。在这种情况下，虽然GPU的原始峰值性能和带宽更高，但由于不规则访问带来的巨大折损，其最终交付性能可能与CPU相当，甚至低于CPU（例如 $0.015\,\text{Tf/s}$ vs $0.018\,\text{Tf/s}$）。CPU拥有更强大的缓存系统和更少的线程并发，能更好地处理这类不规则访存模式。

这个分析揭示了一个关键原则：硬件与算法的成功匹配，必须超越简单的峰值性能比较，深入到内存访问模式与架构特性的相互作用层面。

### 并行扩展的基本限制

将模拟从单个节点扩展到成千上万个节点时，我们会遇到更基本的性能瓶颈。这些瓶颈决定了大规模并行计算的效率和可行性。

#### 阿姆达尔定律与串行瓶颈

**[阿姆达尔定律](@entry_id:137397) (Amdahl's Law)** 为我们揭示了一个残酷的现实。该定律指出，一个程序在并行化后所能获得的最大加速比受限于程序中必须串行执行部分的比例。如果一个程序的总工作中，可[并行化](@entry_id:753104)的部分占 $f$，而串行部分占 $1-f$，那么在使用 $P$ 个处理器时，其**强扩展 (strong scaling)** 加速比 $S(P)$ 为：
$$ S(P) = \frac{1}{(1-f) + \frac{f}{P}} $$
当处理器数量 $P \to \infty$ 时，最[大加速](@entry_id:198882)比 $S_{\max}$ 趋近于 $\frac{1}{1-f}$。

这意味着，即使一个程序只有很小一部分是串行的，其可获得的加速比也存在一个严格的上限。例如，一个具有 $f = 0.95$（即 $5\%$ 串行部分）的[聚变模拟](@entry_id:1125419)代码，其理论最大加速比仅为 $S_{\max} = \frac{1}{1-0.95} = 20$。当使用 $P=10^5$ 个节点时，实际加速比约为 $19.996$，已经极其接近理论极限。此时，[并行效率](@entry_id:637464) $\eta(P) = S(P)/P$ 仅为约 $0.02\%$，意味着超过 $99.9\%$ 的计算资源都在等待那 $5\%$ 的串行代码执行完毕。因此，对于固定规模问题的强扩展，根除或极力压缩串行部分是实现大规模[并行效率](@entry_id:637464)的前提。

#### 通信与计算之比：表面积-体积效应

在实践中，[并行性能](@entry_id:636399)的主要限制来自于节点间的**[通信开销](@entry_id:636355) (communication overhead)**。对于使用**区域分解 (domain decomposition)** 的模拟，例如在MHD中，每个MPI进程（rank）负责计算一个子区域。为了计算边界处的物理量，每个进程需要与相邻进程交换一层“**晕轮 (halo)**”或“**幽灵区 (ghost zone)**”数据。

通信与计算的相对重要性可以用**通信计算比 (Communication-to-Computation Ratio, CCR)** 来衡量。对于一个被分解为 $n \times n \times n$ 子区域的3D[模板计算](@entry_id:755436)，其计算量与子区域的体积成正比，即 $\Theta(n^3)$。而其通信量则与子区域的表面积成正比（交换6个面），即 $\Theta(h n^2)$，其中 $h$ 是晕轮区域的厚度。因此，CCR的[标度关系](@entry_id:273705)为：
$$ \mathrm{CCR} = \frac{\Theta(h n^2)}{\Theta(n^3)} = \Theta\left(\frac{h}{n}\right) $$
这被称为**表面积-体积效应 (surface-to-volume effect)**。

这一比例揭示了两种主要扩展策略的行为差异：
- **弱扩展 (Weak scaling)**：保持每个进程的工作量（即 $n$）不变，通过增加进程数来求解更大的问题。在此情况下，CCR保持近似恒定，算法具有良好的可扩展性。
- **强扩展 (Strong scaling)**：保持全局问题规模不变，通过增加进程数（从而减小每个进程的工作量 $n$）来更快地求解问题。随着 $n$ 的减小，CCR会随之增大，[通信开销](@entry_id:636355)逐渐占据主导地位，最终限制加速比的增长。

使用更精确的**alpha-beta通信模型** $T_{\text{msg}} = \alpha + \beta m$（其中 $\alpha$ 是延迟，$\beta$ 是反带宽，$m$ 是消息大小）进行分析，可以更清晰地看到这一点。对于一个在 $P = p^3$ 个进程上进行强扩展的 $N^3$ 全局问题，每个进程的计算时间 $T_{\text{comp}}(P) \propto N^3/P$。而通信时间 $T_{\text{halo}}(P)$ 包含6次消息交换，其时间正比于 $6(\alpha + \beta \cdot \text{消息大小})$。最终，通信计算比 $R(P) = T_{\text{halo}}(P)/T_{\text{comp}}(P)$ 可以被推导为：
$$ R(P) = \frac{6 \alpha P}{\gamma N^{3}} + \frac{6 \beta h P^{1/3}}{\gamma N} $$
其中 $\gamma$ 是单位计算时间的系数。这个表达式明确显示，在强扩展下，随着 $P$ 的增加，与延迟相关的开销（第一项）以 $P$ 的速度增长，与带宽相关的开销（第二项）以 $P^{1/3}$ 的速度增长。这两种效应共同导致[通信开销](@entry_id:636355)的相对占比急剧上升。

### 软件与算法策略

面对硬件的复杂性和并行扩展的物理限制，我们需要精巧的软件和算法策略来驾驭百亿亿次级系统。

#### 异构性与可移植性的编程模型

为异构系统编写可移植且高性能的代码是一大挑战。开发者面临着一个从底层硬件控制到高层抽象的编程模型谱系。

- **[分布式内存](@entry_id:163082)编程**：**[消息传递接口](@entry_id:1128233) (Message Passing Interface, MPI)** 是跨节点[并行编程](@entry_id:753136)的基石。它采用**单程序多数据 (SPMD)** 模型，每个进程拥有独立的地址空间，通过显式消息传递进行通信。

- **共享内存与设备卸载**：
    - **[OpenMP](@entry_id:178590)** 最初是为CPU多核共享内存编程设计的[线程模型](@entry_id:755945)。自4.0版起，它增加了对加速器的**目标卸载 (target offload)** 功能，允许通过指令 (`pragma`) 将计算区域和数据卸载到设备上。
    - **CUDA** 和 **HIP** 分别是NVIDIA和AMD提供的底层[GPU编程模型](@entry_id:749978)。它们都基于**单指令[多线程](@entry_id:752340) (SIMT)** 的内核执行模型，并管理独立的主机和设备内存空间。CUDA是厂商锁定的，而HIP旨在提供与CUD[A相](@entry_id:195484)似的语法，并支持在AMD和NVIDIA的GPU上运行。

- **[性能可移植性](@entry_id:753342)抽象**：为了解决多厂商异构平台的编程难题，出现了一些更高层次的抽象模型。
    - **SYCL** 是一个由Khronos Group制定的开放标准，它允许使用标准的单源C++代码进行异构编程。它通过队列、缓冲区/访问器或统一共享内存 (USM) 来管理任务和数据，并通过不同的后端实现来支持跨厂商的硬件。
    - **Kokkos** 是一个基于C++库的[性能可移植性](@entry_id:753342)框架。它将并行执行模式（如 `parallel_for`）和数据结构（如 `Kokkos::View`）从底层硬件（如[OpenMP](@entry_id:178590), CUDA, HIP, SYCL后端）中抽象出来。开发者编写一份应用代码，在编译时选择合适的后端，从而生成针对目标架构的本地代码。
    - **OpenACC** 是一个基于指令的加速器编程模型，与[OpenMP](@entry_id:178590)的卸载模型类似。它旨在提供一种更简单的方式来标注代码区域以进行卸载，但其跨厂商的可移植性高度依赖于编译器支持的成熟度。

一个典型的[聚变模拟](@entry_id:1125419)应用通常采用混合编程模型，例如使用MPI进行跨节点通信，而在节点内部则使用[OpenMP](@entry_id:178590)、CUDA/HIP或Kokkos/SYCL等模型来利用多核CPU和GPU。

#### 优化[内存层次结构](@entry_id:163622)利用率

如前所述，性能往往受限于数据移动而非计算本身。**[数据布局](@entry_id:1123398) (data layout)** 的选择对缓存和SIMD（单指令多数据）单元的利用率有决定性影响。对于在结构化网格上存储多个物理量的MHD代码，主要有两种布局方式：**[结构数组](@entry_id:755562) (Array of Structures, AoS)** 和 **[数组结构](@entry_id:635205) (Structure of Arrays, SoA)**。

- **AoS**：将一个网格单元的所有物理场（如密度、动量、能量等）连续存储在内存中。这种布局有利于访问同一单元的不同物理场。
- **SoA**：将每个物理场存储在各自独立的数组中。

当需要对一个物理场沿着某个维度（例如 $i$ 轴）进行SIMD矢量化计算时，SoA布局展现出巨大优势。在SoA中，一个物理场在 $i$ 轴上的数据是连续存储的，这使得SIMD加载指令可以一次性抓取一个完整的、由 $W$ 个连续数据组成的矢量，并且这种单位步长的访问模式能完美地利用[硬件预取](@entry_id:750156)器和缓存行（缓存行利用率可达100%）。相比之下，在AoS布局中，同一物理场的相邻数据在内存中被其他物理场隔开，步长为一个完整结构体的大小。这导致SIMD加载需要执行“收集”(gather)操作，触及多个不连续的缓存行，且每个缓存行的利用率极低，从而严重制约性能。因此，对于[模板计算](@entry_id:755436)这类以流式访问单个场为主的内核，SoA是首选布局。

#### [动态负载均衡](@entry_id:748736)

许多[聚变模拟](@entry_id:1125419)，特别是PIC模拟，其计算负载在时间和空间上都不是静态的。例如，在边缘[等离子体湍流模拟](@entry_id:1129816)中，粒子会因物理过程（如鞘层形成）而在某些区域（如材料边界附近）聚集。

这导致了**负载不均衡 (load imbalance)**：最初均匀划分的区域，随着模拟的进行，某些进程的粒子数会远超其他进程。在**体同步并行 (bulk-synchronous parallel)** 执行模型（所有进程在每个时间步结束时同步）下，整个模拟的步进时间由负载最重的那个进程决定。这就好比一个木桶，其容量由最短的那块木板决定。

- **静态负载均衡 (Static load balancing)**：在模拟开始前根据初始状态或预估进行一次区域划分，之后不再改变。这种方法无法适应演化中的负载变化。
- **[动态负载均衡](@entry_id:748736) (Dynamic load balancing)**：在运行时周期性地检测负载不均衡，并通过迁移区域边界或直接迁移粒子等方式重新分配工作，以使得每个进程的计算时间大致相等。虽然重分区本身会带来开销，但这种开销通常可以被后续成百上千个时间步获得的平衡效率所摊销。因此，对于具有动态演化特征的[粒子模拟](@entry_id:144357)，[动态负载均衡](@entry_id:748736)是维持大规模[并行效率](@entry_id:637464)不可或缺的关键技术。

### 跨领域挑战

除了[性能扩展](@entry_id:1129513)和编程模型，[百亿亿次级计算](@entry_id:1124720)还带来了一些影响整个[科学工作流](@entry_id:1131303)的系统性挑战。

#### 数值再现性

**再现性 (Reproducibility)**，即在相同输入和环境下多次运行程序能得到**比特级别完全相同 (bitwise-identical)** 的结果，是软件调试、验证和回归测试的基石。然而，在并行[浮点](@entry_id:749453)计算中，再现性并非理所当然。

其根本原因在于[IEEE 754浮点](@entry_id:750510)数加法的**非[结合性](@entry_id:147258) (non-associativity)**。也就是说，$fl((a+b)+c)$ 不一定等于 $fl(a+(b+c))$，其中 $fl(\cdot)$ 表示[浮点运算](@entry_id:749454)结果。当使用MPI进行全局求和（如计算总能量）时，MPI实现为了优化性能，可能会根据系统拓扑和运行时状态构建不同形状的归约树。这意味着不同运行次序下，全局求和的加法配对顺序可能不同。

一个简单的例子足以说明问题：对三个数 $x_1 = 10^{16}$，$x_2 = 1$，$x_3 = -10^{16}$ 求和。
- 按 `(x1 + x2) + x3` 顺序：$fl(10^{16} + 1)$ 由于精度限制，结果仍为 $10^{16}$（称为“大数吃小数”），再与 $-10^{16}$ 相加得 $0$。
- 按 `(x1 + x3) + x2` 顺序：$fl(10^{16} - 10^{16})$ 结果为 $0$，再与 $1$ 相加得 $1$。

仅仅改变加法顺序，就导致了 $0$ 和 $1$ 的天壤之别。为保证再现性，必须采取特殊策略，例如：强制MPI使用一个固定的、确定性的归约顺序（但这可能牺牲性能），或者使用保证结果与顺序无关的可再现求和算法，例如使用一个不会产生中间舍入误差的超长[累加器](@entry_id:175215)（superaccumulator）。

#### 输入/输出与[数据管理](@entry_id:893478)

[聚变模拟](@entry_id:1125419)产生海量数据，高效的**输入/输出 (I/O)** 是另一个严峻挑战，尤其是对于需要频繁**检查点 (checkpointing)** 的长时间模拟。当成千上万个进程同时向一个共享[文件系统](@entry_id:749324)写入TB级别的数据时，就会发生**并行I/O争用 (parallel I/O contention)**。

我们可以将I/O系统视为一个由多个串联组件构成的流水线，其整体性能由最慢的那个组件（即“瓶颈”）决定。这些组件包括：应用的I/O聚合器带宽、节点间的网络带宽、[并行文件系统](@entry_id:1129315)的总带宽、以及单个文件所跨越的存储设备（如对象存储目标，OSTs）的**条带化 (striping)** 带宽。

考虑一个场景：一个模拟需要在 $60$ 秒内写入一个 $4\,\mathrm{TiB}$ 的HDF5检查点文件。这要求系统的有效I/O吞吐量至少为 $4096\,\mathrm{GiB} / 60\,\mathrm{s} \approx 68.3\,\mathrm{GiB/s}$。然而，如果该文件仅被条带化到 $32$ 个OST上，每个OST提供 $1\,\mathrm{GiB/s}$ 的带宽，那么该文件能获得的最大带宽仅为 $32\,\mathrm{GiB/s}$。由于需求速率（$68.3\,\mathrm{GiB/s}$）远超服务速率（$32\,\mathrm{GiB/s}$），系统将发生严重拥塞，导致I/O操作无法按时完成，最终拖慢整个模拟进程。这个例子表明，即便是拥有极高总带宽的[并行文件系统](@entry_id:1129315)，不恰当的文件条带化策略也会使单个文件的I/O性能成为整个应用的瓶颈。

#### 功耗与能量效率

最后，百亿亿次级系统的运行受限于巨大的功耗预算。**解题能耗 (energy-to-solution)**，即完成一次模拟所消耗的总能量，已成为与求解时间同等重要的性能指标。

现代处理器支持**动态电压与频率调节 (DVFS)** 技术。处理器动态功耗 $P_{\text{dyn}} \propto V^2 f$，其中 $V$ 是电压，$f$ 是频率。为了在更高频率下稳定工作，电压也必须相应提高。总能量 $E_{\text{sol}}$ 是总功率在运行时间 $T_{\text{sol}}$ 上的积分，即 $E_{\text{sol}} = P_{\text{avg}} T_{\text{sol}}$。

一个普遍的误解是，要最小化能耗，就应该尽快完成计算（即最小化 $T_{\text{sol}}$）。然而，事实并非如此。
- 对于**计算受限 (compute-bound)** 的任务，其运行时间 $T \propto 1/f$。完成固定数量周期的动态能耗 $E_{\text{dyn}} \propto V^2$。当为了缩短时间而提高频率 $f$ 时，电压 $V$ 随之上升，导致 $E_{\text{dyn}}$ 上升。同时，静态泄漏功耗 $P_{\text{leak}}$ 也随电压急剧增加。这两种效应叠加，可能导致总能耗 $E_{\text{sol}}$ 反而增加。因此，能耗最优的频率点往往低于最高频率。
- 对于**内存受限 (memory-bound)** 的任务，处理器大量时间处于等待内存的停顿状态，其运行时间对CPU频率 $f$ 不敏感。在这种情况下，适度降低频率 $f$ 和电压 $V$ 可以在不显著增加运行时间 $T_{\text{sol}}$ 的前提下，大幅降低处理器的动态和[静态功耗](@entry_id:174547)，从而显著减少总能耗 $E_{\text{sol}}$。

因此，智能的能耗管理策略需要根据应用在不同阶段的计算或访存特性，动态调节频率，以在性能和能耗之间取得最佳平衡。