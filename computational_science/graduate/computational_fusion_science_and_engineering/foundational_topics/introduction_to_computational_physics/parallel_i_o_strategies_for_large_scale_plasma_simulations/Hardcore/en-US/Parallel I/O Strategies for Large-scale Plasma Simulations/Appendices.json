{
    "hands_on_practices": [
        {
            "introduction": "Choosing the right parallel I/O strategy is often the most critical factor in achieving good performance. This exercise moves beyond simple rules of thumb, challenging you to develop a principled decision framework for selecting between independent and collective I/O. By considering the underlying mechanics of a parallel file system, such as byte-range locking and metadata contention, you will learn to evaluate the trade-offs and identify the optimal strategy based on your application's data access pattern .",
            "id": "4026041",
            "problem": "A large-scale plasma turbulence code in computational fusion science writes full-state checkpoints to a single shared file on a parallel file system using the Message Passing Interface Input/Output (MPI-IO). The file is striped across data servers with stripe unit $u$ and stripe count $s$. There are $p$ application processes (ranks); each rank produces a contiguous payload of size $r$ bytes per checkpoint. Two write modes are available: independent I/O (each rank issues its own write calls) and collective I/O (ranks perform a two-phase I/O with designated aggregators that coalesce and align I/O).\n\nThe lock manager on each data server enforces byte-range locks; any overlapping lock requests among clients are serialized, causing contention. Metadata updates are handled by a metadata server and include extent allocations, stripe target selection, and commit records; small and fragmented writes inflate the number of metadata operations. In collective I/O, the two-phase algorithm includes a data redistribution (gather/scatter) among ranks to aggregators, and the aggregators issue large, aligned writes that can reduce lock and metadata traffic. In independent I/O, ranks directly issue their writes; if their file regions are disjoint and aligned to stripe units, lock and metadata contention is minimized.\n\nAssume that:\n- Alignment to the stripe unit $u$ ensures that each lock request covers an integral number of stripe units.\n- Misalignment and interleaving across ranks increase the probability of lock overlap $\\pi$ and the number of locks and metadata operations per byte.\n- Collective I/O adds a data shuffle cost across the interconnect and internal coordination overhead among ranks but can reduce lock and metadata contention by coalescing small requests into large aligned extents.\n- For large contiguous writes, per-byte device bandwidth dominates over fixed per-lock and per-metadata costs when there is no contention; for small or irregular writes, fixed costs and contention can dominate.\n\nSelect the most principled decision rule that determines when to use independent I/O versus collective I/O for this checkpoint, and justify it based on lock and metadata contention considerations and the amortization of coordination costs. Which option most accurately captures the correct rule?\n\nA. Use independent I/O if $r \\gg u$, writes are aligned to $u$, and per-rank file regions are disjoint so $\\pi \\approx 0$; otherwise prefer collective I/O when writes are small or irregular (misaligned or interleaved), because aggregators coalesce requests into large aligned extents that reduce lock and metadata contention at the cost of shuffle overhead.\n\nB. Always prefer collective I/O when writes are aligned to $u$ to maximize stripe utilization; reserve independent I/O for misaligned writes, since collective I/O cannot handle misalignment without violating byte-range lock semantics.\n\nC. Choose independent I/O whenever the number of ranks $p$ is large, because collective coordination scales poorly with $p$; choose collective I/O when $p$ is small, regardless of $r$, $u$, alignment, or disjointness.\n\nD. Prefer collective I/O if the stripe count $s$ exceeds the number of aggregators, $s > a$, to avoid overloading data servers; otherwise prefer independent I/O, independent of $r$, $u$, alignment, or $\\pi$.\n\nE. Use independent I/O when $r \\ll u$ to avoid collective coordination overhead on small messages; use collective I/O when $r \\gg u$, because aggregators always outperform independent I/O on large writes irrespective of overlap or alignment.",
            "solution": "The core of this problem is a trade-off analysis between two I/O strategies, independent and collective, based on the access pattern and its interaction with the parallel file system.\n\nThe total time for an I/O operation can be modeled conceptually as the sum of several components: data transfer time, coordination overhead, and contention/metadata penalties.\n$T_{total} = T_{transfer} + T_{overhead} + T_{contention}$\n\nLet's analyze the behavior of each I/O mode.\n\n1.  **Independent I/O**: Each of the $p$ ranks writes its $r$-byte payload directly to the file.\n    -   $T_{transfer}$: In the absence of contention, this is related to the payload size $r$ and the available bandwidth. For large $r$, this term dominates.\n    -   $T_{overhead}$: This is the fixed per-operation cost (initiating the system call, metadata updates, lock acquisition). If each rank issues one write, there are $p$ operations.\n    -   $T_{contention}$: This cost arises from lock conflicts and file system serialization. The problem states this is high when writes are misaligned or overlapping (i.e., $\\pi$ is high).\n\n    The ideal case for independent I/O is when $T_{contention} \\approx 0$. This occurs when:\n    -   **Writes are disjoint**: Each rank writes to a unique file offset range, so no two ranks request locks on the same byte range. This leads to the lock overlap probability $\\pi \\approx 0$.\n    -   **Writes are large**: The payload $r$ is large, ideally a multiple of the stripe unit $u$ ($r \\ge u$). This amortizes the fixed $T_{overhead}$ over many bytes, making the I/O bandwidth-bound. The problem statement refers to this as the case where \"per-byte device bandwidth dominates\".\n    -   **Writes are aligned**: Each write begins on a stripe unit boundary. This ensures each write operation interacts cleanly with the underlying file striping, minimizing the number of locks and data servers involved.\n\n    In this ideal scenario, independent I/O is highly efficient as it avoids any inter-process communication overhead.\n\n2.  **Collective I/O**: Ranks coordinate, data is shuffled to aggregators, and aggregators perform the actual writes.\n    -   $T_{transfer}$: The total amount of data written to disk is the same ($p \\times r$), but it is now done by a smaller number of aggregators in larger, coalesced chunks.\n    -   $T_{overhead}$: This now consists of two parts: the internal coordination/shuffle cost ($T_{shuffle}$) and the I/O operation cost. The key benefit is that aggregators issue a much smaller number of large, aligned I/O operations. This dramatically reduces the number of metadata updates and lock requests, minimizing the fixed-cost component of the I/O phase.\n    -   $T_{contention}$: Because aggregators can write large, aligned, and often contiguous extents, the probability of lock contention is significantly reduced, even if the original application access pattern was small, misaligned, or interleaved.\n\nThe trade-off is clear: Collective I/O introduces a $T_{shuffle}$ cost to drastically reduce $T_{contention}$ and the per-operation part of $T_{overhead}$.\n\nThe decision rule is therefore:\n-   If the access pattern is already \"good\" for the file system (large, aligned, disjoint writes), then $T_{contention}$ for independent I/O is already near zero. Paying the $T_{shuffle}$ cost of collective I/O provides no benefit and only adds overhead. Thus, **use independent I/O**.\n-   If the access pattern is \"bad\" (small, misaligned, or interleaved writes), then $T_{contention}$ for independent I/O will be very large and likely dominate performance. Paying the $T_{shuffle}$ cost is a good investment because it allows the aggregators to transform the \"bad\" pattern into a \"good\" one, making the subsequent I/O phase's contention negligible. Thus, **use collective I/O**.\n\nThe condition \"$r \\gg u$\" is a good proxy for \"large\" writes relative to the file system's geometry. \"Small or irregular\" corresponds to cases where $r$ is small or writes are misaligned/interleaved.\n\n### Option-by-Option Analysis\n\n**A. Use independent I/O if $r \\gg u$, writes are aligned to $u$, and per-rank file regions are disjoint so $\\pi \\approx 0$; otherwise prefer collective I/O when writes are small or irregular (misaligned or interleaved), because aggregators coalesce requests into large aligned extents that reduce lock and metadata contention at the cost of shuffle overhead.**\n\n-   **Analysis**: This statement perfectly encapsulates the derived decision rule. It correctly identifies the ideal conditions for independent I/O: large ($r \\gg u$), aligned, and disjoint ($\\pi \\approx 0$) writes. It correctly identifies the alternative scenario—small or irregular writes—as the domain of collective I/O. It also provides the correct justification: collective I/O amortizes its shuffle overhead by coalescing requests to reduce lock and metadata contention.\n-   **Verdict**: **Correct**.\n\n**B. Always prefer collective I/O when writes are aligned to $u$ to maximize stripe utilization; reserve independent I/O for misaligned writes, since collective I/O cannot handle misalignment without violating byte-range lock semantics.**\n\n-   **Analysis**: This statement is fundamentally incorrect. Firstly, if writes are large, aligned, and disjoint, independent I/O is superior as it avoids collective overhead. Secondly, and more critically, it claims collective I/O cannot handle misalignment. This is false; handling misaligned and small/scattered access patterns is the primary purpose and strength of two-phase collective I/O. Independent I/O is what performs poorly with misalignment.\n-   **Verdict**: **Incorrect**.\n\n**C. Choose independent I/O whenever the number of ranks $p$ is large, because collective coordination scales poorly with $p$; choose collective I/O when $p$ is small, regardless of $r$, $u$, alignment, or disjointness.**\n\n-   **Analysis**: This rule is a gross oversimplification. While scalability with $p$ is a concern for collective operations, the performance impact of I/O contention from a poor access pattern at large scale is typically far more severe. For a large number of ranks performing small, misaligned writes, independent I/O would cripple the file system, whereas collective I/O is designed to solve this exact problem. The decision cannot be made \"regardless of $r$, $u$, alignment, or disjointness,\" as these are the primary factors determining contention.\n-   **Verdict**: **Incorrect**.\n\n**D. Prefer collective I/O if the stripe count $s$ exceeds the number of aggregators, $s > a$, to avoid overloading data servers; otherwise prefer independent I/O, independent of $r$, $u$, alignment, or $\\pi$.**\n\n-   **Analysis**: The relationship between stripe count ($s$) and number of aggregators ($a$, which is not given but implied) is a secondary tuning parameter for optimizing collective I/O itself, not the primary criterion for choosing between independent and collective modes. The fundamental choice is dictated by the access pattern ($r$, alignment, $\\pi$). Ignoring these dominant factors makes this decision rule invalid.\n-   **Verdict**: **Incorrect**.\n\n**E. Use independent I/O when $r \\ll u$ to avoid collective coordination overhead on small messages; use collective I/O when $r \\gg u$, because aggregators always outperform independent I/O on large writes irrespective of overlap or alignment.**\n\n-   **Analysis**: This statement presents the exact opposite of the correct logic. The case $r \\ll u$ (many small writes) is the worst-case for independent I/O due to high metadata and lock overhead, and it's the primary use case for collective I/O's coalescing feature. Conversely, the case $r \\gg u$ (assuming alignment and disjointness) is the ideal case for independent I/O, where adding collective overhead is detrimental.\n-   **Verdict**: **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "After establishing a conceptual framework for choosing an I/O strategy, it is essential to quantify the expected performance gains. This practice guides you through a first-principles performance model to compare independent and collective I/O in a scenario dominated by a high number of small write requests . By calculating the total write time for each strategy, you will gain a concrete understanding of how request aggregation mitigates latency overhead and can dramatically improve wall-clock time.",
            "id": "4026071",
            "problem": "A gyrokinetic turbulence code for fusion plasma evolves a distribution on a structured mesh and periodically writes a global checkpoint to a Parallel File System (PFS). The job uses $P=512$ Message Passing Interface (MPI) processes. The global checkpoint size is $S=2\\,\\mathrm{TB}$, written as fixed-size chunks of size $r=64\\,\\mathrm{KB}$ per write system call. Assume the following I/O system characteristics: each Input/Output (I/O) request incurs a fixed per-request overhead of $\\alpha=100\\,\\mu\\mathrm{s}$ at the storage interface due to metadata and Remote Procedure Call processing, and the sustained end-to-end PFS bandwidth observed by the job is $b=300\\,\\mathrm{MB/s}$ when the job saturates the storage servers. Further assume that per-request overheads serialize at the storage interface (i.e., they do not overlap) while bulk data transfer proceeds at the sustained bandwidth $b$; interconnect and aggregation overheads on the compute side are negligible compared to the storage-side costs; the storage system is saturated so that the aggregate data transfer time depends only on $S$ and $b$; and that collective I/O reduces the number of storage I/O requests by an aggregation factor $A=8$ by perfectly coalescing $A$ adjacent chunk writes into one storage request without changing the total data volume.\n\nModel two strategies:\n1. Independent I/O: each process issues its own $r$-sized writes directly to the PFS.\n2. Collective I/O: two-phase I/O with perfect aggregation factor $A=8$ at the aggregators.\n\nInterpret $\\mathrm{KB}$, $\\mathrm{MB}$, and $\\mathrm{TB}$ as binary units (kibibyte, mebibyte, tebibyte). Using only the assumptions above and first principles about fixed per-request overhead and sustained bulk bandwidth, derive the total wall-clock write time under independent I/O and under collective I/O. Then compute the ratio of the independent I/O time to the collective I/O time. Round your final ratio to four significant figures and express it as a pure number with no units.",
            "solution": "The total wall-clock time $T$ to write the checkpoint is modeled as the sum of two components: the total overhead time due to per-request latencies, $T_{overhead}$, and the total time for bulk data transfer, $T_{transfer}$.\n$$T = T_{overhead} + T_{transfer}$$\nThe problem states that \"per-request overheads serialize at the storage interface\". This implies that the total overhead time is the product of the number of requests, $N_{req}$, and the per-request overhead, $\\alpha$.\n$$T_{overhead} = N_{req} \\alpha$$\nThe problem also states that \"the aggregate data transfer time depends only on $S$ and $b$\". This means the bulk data transfer time is the total data size, $S$, divided by the sustained bandwidth, $b$. This component is independent of the I/O strategy (independent vs. collective) because the total volume of data being transferred to a saturated system is the same in both cases.\n$$T_{transfer} = \\frac{S}{b}$$\nTherefore, the general model for the total write time is:\n$$T = N_{req} \\alpha + \\frac{S}{b}$$\nWe first convert the given quantities to a consistent set of base units (bytes for size, seconds for time). The problem specifies that $\\mathrm{KB}$, $\\mathrm{MB}$, and $\\mathrm{TB}$ are binary units (kibibyte, mebibyte, tebibyte).\n- Global checkpoint size: $S = 2\\,\\mathrm{TiB} = 2 \\times (2^{10})^4\\,\\text{bytes} = 2 \\times 2^{40}\\,\\text{bytes} = 2^{41}\\,\\text{bytes}$.\n- Chunk size: $r = 64\\,\\mathrm{KiB} = 64 \\times 2^{10}\\,\\text{bytes} = 2^6 \\times 2^{10}\\,\\text{bytes} = 2^{16}\\,\\text{bytes}$.\n- Per-request overhead: $\\alpha = 100\\,\\mu\\mathrm{s} = 100 \\times 10^{-6}\\,\\mathrm{s} = 10^{-4}\\,\\mathrm{s}$.\n- Sustained bandwidth: $b = 300\\,\\mathrm{MiB/s} = 300 \\times (2^{10})^2\\,\\text{bytes/s} = 300 \\times 2^{20}\\,\\text{bytes/s}$.\n- Aggregation factor: $A = 8 = 2^3$.\nThe number of MPI processes, $P=512$, does not directly enter the calculation because the overheads are assumed to serialize at the storage interface, making the total overhead time dependent only on the total number of requests generated by the application, irrespective of how they are distributed among processes.\n\nFirst, we calculate the total number of chunks, $N_{chunks}$, that constitute the global checkpoint.\n$$N_{chunks} = \\frac{S}{r} = \\frac{2^{41}\\,\\text{bytes}}{2^{16}\\,\\text{bytes}} = 2^{41-16} = 2^{25}$$\n\nNext, we analyze the two I/O strategies.\n\nStrategy 1: Independent I/O\nIn this strategy, each process issues its own writes. With no aggregation, each $r$-sized chunk write corresponds to one storage I/O request. The total number of requests, $N_{req, ind}$, is therefore equal to the total number of chunks.\n$$N_{req, ind} = N_{chunks} = 2^{25}$$\nThe total time for independent I/O, $T_{ind}$, is:\n$$T_{ind} = N_{req, ind} \\alpha + \\frac{S}{b} = \\left(\\frac{S}{r}\\right)\\alpha + \\frac{S}{b}$$\n\nStrategy 2: Collective I/O\nIn this strategy, the collective I/O mechanism coalesces $A$ chunk writes into a single, larger storage request. This reduces the total number of requests sent to the storage system by a factor of $A$.\n$$N_{req, coll} = \\frac{N_{chunks}}{A} = \\frac{2^{25}}{8} = \\frac{2^{25}}{2^3} = 2^{22}$$\nThe total time for collective I/O, $T_{coll}$, is:\n$$T_{coll} = N_{req, coll} \\alpha + \\frac{S}{b} = \\left(\\frac{S}{rA}\\right)\\alpha + \\frac{S}{b}$$\n\nWe are asked to compute the ratio of the independent I/O time to the collective I/O time.\n$$ \\text{Ratio} = \\frac{T_{ind}}{T_{coll}} = \\frac{(S/r)\\alpha + S/b}{(S/rA)\\alpha + S/b} $$\nLet's now substitute the numerical values. First, we compute the two time components: the overhead component for independent I/O and the common transfer component.\n$$ T_{overhead, ind} = N_{req, ind} \\alpha = 2^{25} \\times 10^{-4}\\,\\mathrm{s} = 33,554,432 \\times 10^{-4}\\,\\mathrm{s} = 3355.4432\\,\\mathrm{s} $$\n$$ T_{transfer} = \\frac{S}{b} = \\frac{2^{41}\\,\\text{bytes}}{300 \\times 2^{20}\\,\\text{bytes/s}} = \\frac{2^{21}}{300}\\,\\mathrm{s} = \\frac{2,097,152}{300}\\,\\mathrm{s} \\approx 6990.5067\\,\\mathrm{s} $$\nThe overhead component for collective I/O is:\n$$ T_{overhead, coll} = N_{req, coll} \\alpha = 2^{22} \\times 10^{-4}\\,\\mathrm{s} = 4,194,304 \\times 10^{-4}\\,\\mathrm{s} = 419.4304\\,\\mathrm{s} $$\nNote that $T_{overhead, coll} = T_{overhead, ind} / A = 3355.4432 / 8 = 419.4304\\,\\mathrm{s}$.\n\nNow we can compute the total times for each strategy.\n$$T_{ind} = T_{overhead, ind} + T_{transfer} \\approx 3355.4432\\,\\mathrm{s} + 6990.5067\\,\\mathrm{s} \\approx 10345.9499\\,\\mathrm{s}$$\n$$T_{coll} = T_{overhead, coll} + T_{transfer} \\approx 419.4304\\,\\mathrm{s} + 6990.5067\\,\\mathrm{s} \\approx 7409.9371\\,\\mathrm{s}$$\nFinally, we compute the ratio:\n$$ \\text{Ratio} = \\frac{T_{ind}}{T_{coll}} \\approx \\frac{10345.9499}{7409.9371} \\approx 1.3962260 $$\nRounding the result to four significant figures gives $1.396$.",
            "answer": "$$\\boxed{1.396}$$"
        },
        {
            "introduction": "Beyond optimizing the I/O access pattern itself, reducing the total data volume is another powerful strategy for accelerating large-scale writes. This exercise explores the use of online, in-situ data compression as a way to alleviate pressure on the file system . You will analyze a perfectly overlapped pipeline of compression and writing, identifying the performance bottleneck and quantifying the trade-off between CPU work and I/O time.",
            "id": "4026026",
            "problem": "A direct numerical simulation of magnetized plasma turbulence produces a single three-dimensional scalar field of size $S=4\\,\\mathrm{TB}$ at the end of a time window. The field is written to a Parallel File System (PFS) with sustained aggregate bandwidth $6\\,\\mathrm{GB/s}$. To reduce I/O volume, an online lossless compressor is applied on the compute node prior to writing, achieving a compression ratio $\\rho=8$ and a sustained end-to-end compression throughput of $2\\,\\mathrm{GB/s}$ on the central processing unit (CPU). Assume a two-stage streaming pipeline with double-buffering that allows perfect overlap between compression and PFS writing in the steady state, and assume metadata and synchronization latencies are negligible compared to bulk transfer.\n\nUsing only the fundamental definitions that (i) throughput is bytes per unit time, (ii) a compression ratio $\\rho$ maps input size $S$ to output size $S/\\rho$, and (iii) in a perfectly overlapped two-stage pipeline the wall-clock completion time is governed by the slower stage for sufficiently large $S$, derive the wall-clock time to complete the write when compression is enabled. Then, define the CPU overhead as the fraction of the pipeline wall-clock time during which CPU compression cannot be hidden by concurrent PFS writing, and compute this fraction.\n\nUse base-10 units where $1\\,\\mathrm{TB}=10^{12}\\,\\mathrm{bytes}$ and $1\\,\\mathrm{GB/s}=10^{9}\\,\\mathrm{bytes/s}$. Express the wall-clock time in seconds and the overhead as a dimensionless decimal. Round both quantities to four significant figures. Report your final answer as a row matrix $\\begin{pmatrix} T_{\\mathrm{total}} & O \\end{pmatrix}$, where $T_{\\mathrm{total}}$ is the wall-clock time with compression and $O$ is the CPU overhead fraction as defined above.",
            "solution": "First, we convert all given quantities to base units of bytes and bytes/second to ensure consistency in calculations.\nThe uncompressed data size is $S = 4\\,\\mathrm{TB} = 4 \\times 10^{12}\\,\\mathrm{bytes}$.\nThe compression throughput is $B_c = 2\\,\\mathrm{GB/s} = 2 \\times 10^{9}\\,\\mathrm{bytes/s}$.\nThe PFS write bandwidth is $B_w = 6\\,\\mathrm{GB/s} = 6 \\times 10^{9}\\,\\mathrm{bytes/s}$.\nThe compression ratio is a dimensionless quantity, $\\rho = 8$.\n\nThe process is modeled as a two-stage pipeline:\nStage 1: Compression. This stage processes the original data of size $S$ at a throughput of $B_c$. The time required for this stage to complete, if run in isolation, is $T_c$.\n$$T_c = \\frac{S}{B_c}$$\nStage 2: Writing. This stage processes the compressed data. The size of the compressed data is $S' = S/\\rho$. This stage operates at the PFS write bandwidth $B_w$. The time required for this stage to complete, if run in isolation, is $T_w$.\n$$T_w = \\frac{S'}{B_w} = \\frac{S/\\rho}{B_w} = \\frac{S}{\\rho B_w}$$\n\nAccording to the problem statement, for a perfectly overlapped two-stage pipeline, the total wall-clock time, $T_{\\mathrm{total}}$, is determined by the bottleneck, which is the slower of the two stages.\n$$T_{\\mathrm{total}} = \\max(T_c, T_w)$$\n\nWe now substitute the numerical values to determine the duration of each stage and identify the bottleneck.\n$$T_c = \\frac{4 \\times 10^{12}\\,\\mathrm{bytes}}{2 \\times 10^{9}\\,\\mathrm{bytes/s}} = 2000\\,\\mathrm{s}$$\n$$T_w = \\frac{4 \\times 10^{12}\\,\\mathrm{bytes}}{8 \\times (6 \\times 10^{9}\\,\\mathrm{bytes/s})} = \\frac{4 \\times 10^{12}\\,\\mathrm{bytes}}{48 \\times 10^{9}\\,\\mathrm{bytes/s}} = \\frac{1}{12} \\times 10^3\\,\\mathrm{s} \\approx 83.33\\,\\mathrm{s}$$\n\nComparing the two times, we find that $T_c > T_w$ ($2000\\,\\mathrm{s} > 83.33\\,\\mathrm{s}$). Therefore, the compression stage is the bottleneck. The total wall-clock time to complete the write is governed by the compression time.\n$$T_{\\mathrm{total}} = T_c = 2000\\,\\mathrm{s}$$\nThe problem requires this value to be rounded to four significant figures. To express $2000$ with four significant figures, we use scientific notation: $2.000 \\times 10^3\\,\\mathrm{s}$.\n\nNext, we compute the CPU overhead fraction, $O$. This is defined as the fraction of the total pipeline time during which CPU compression is not hidden by concurrent PFS writing. The total time the CPU is active for compression is $T_c$. The total time the PFS is active for writing is $T_w$. Due to the pipeline overlap, the amount of CPU time that is \"hidden\" by concurrent I/O activity is equal to the duration of the faster stage, which is $T_w$. The amount of \"unhidden\" CPU time is the time the CPU works while the I/O system is idle, which is the difference $T_c - T_w$ (since $T_c > T_w$).\n\nThe overhead fraction $O$ is the ratio of this unhidden time to the total pipeline wall-clock time $T_{\\mathrm{total}}$.\n$$O = \\frac{\\text{unhidden CPU time}}{T_{\\mathrm{total}}} = \\frac{T_c - T_w}{T_{\\mathrm{total}}}$$\nSince the compression stage is the bottleneck, $T_{\\mathrm{total}} = T_c$.\n$$O = \\frac{T_c - T_w}{T_c} = 1 - \\frac{T_w}{T_c}$$\nWe can substitute the symbolic expressions for $T_c$ and $T_w$ to find a general formula for the overhead in this bottleneck regime.\n$$O = 1 - \\frac{S/(\\rho B_w)}{S/B_c} = 1 - \\frac{B_c}{\\rho B_w}$$\nSubstituting the numerical values:\n$$O = 1 - \\frac{2 \\times 10^{9}\\,\\mathrm{bytes/s}}{8 \\times (6 \\times 10^{9}\\,\\mathrm{bytes/s})} = 1 - \\frac{2}{48} = 1 - \\frac{1}{24} = \\frac{23}{24}$$\nAs a decimal, this is $O \\approx 0.958333...$. Rounding to four significant figures gives:\n$$O = 0.9583$$\n\nThe final answer consists of the wall-clock time $T_{\\mathrm{total}}$ and the overhead fraction $O$, reported as a row matrix.\n$T_{\\mathrm{total}} = 2.000 \\times 10^3$ seconds.\n$O = 0.9583$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2.000 \\times 10^{3} & 0.9583\n\\end{pmatrix}\n}\n$$"
        }
    ]
}