{
    "hands_on_practices": [
        {
            "introduction": "Many critical problems in computational fusion, such as resistive Magnetohydrodynamics (MHD), result in large, block-coupled linear systems. This exercise  offers a practical opportunity to implement the Block Gauss-Seidel method, a foundational iterative solver that leverages this structure. By applying it to a model MHD system, you will directly investigate how the solver's convergence is critically dependent on the chosen ordering of the physical variable blocks, a key consideration in designing robust numerical schemes.",
            "id": "3967020",
            "problem": "Consider a resistive Magnetohydrodynamics (MHD) linearization around a uniform background state leading to a block-coupled algebraic system for the increments of velocity, magnetic field, and pressure. Let the unknown be the vector $x = [u \\;|\\; p \\;|\\; b]^{\\top}$, where $u \\in \\mathbb{R}^{n_u}$ is the discrete velocity, $p \\in \\mathbb{R}^{n_p}$ is the discrete pressure, and $b \\in \\mathbb{R}^{n_b}$ is the discrete magnetic field. The linear system has the block structure\n$$\nA x = f, \\quad \nA = \n\\begin{bmatrix}\nA_{uu}  G^{\\top}  A_{ub} \\\\\nG  -\\beta I_{p}  0 \\\\\nA_{bu}  0  A_{bb}\n\\end{bmatrix},\n$$\nwhere $A_{uu}$ is the discrete viscous operator, $A_{bb}$ is the discrete resistive (magnetic diffusion) operator, $G$ is the discrete gradient/divergence coupling, $A_{ub}$ represents the Lorentz force coupling from magnetic field to momentum, and $A_{bu}$ represents the induction coupling from velocity to magnetic field. Here, $I_p$ denotes the identity matrix of size $n_p \\times n_p$, and $\\beta > 0$ is a pressure stabilization parameter ensuring invertibility of the block associated with $p$. This system is dimensionless for the purposes of this computational exercise.\n\nYou will construct the blocks as follows:\n- Let $n_u = 4$, $n_p = 2$, $n_b = 4$, and $n = n_u + n_p + n_b$.\n- Define the one-dimensional Dirichlet discrete Laplacian $L_m \\in \\mathbb{R}^{m \\times m}$ by\n$$\nL_m = \\frac{1}{h^2}\n\\begin{bmatrix}\n2  -1   \\\\\n-1  2  -1  \\\\\n \\ddots  \\ddots  \\ddots \\\\\n  -1  2\n\\end{bmatrix}, \\quad h = \\frac{1}{m+1}.\n$$\n- Define $A_{uu} = \\mu I_{u} + \\nu L_{n_u}$ and $A_{bb} = \\sigma I_{b} + \\eta L_{n_b}$, where $I_u$ and $I_b$ are identity matrices of sizes $n_u \\times n_u$ and $n_b \\times n_b$, respectively. The parameters $\\mu, \\nu, \\sigma, \\eta > 0$ reflect mass-like regularization and diffusion coefficients.\n- Define a fixed discrete gradient/divergence coupling $G \\in \\mathbb{R}^{n_p \\times n_u}$ by\n$$\nG =\n\\begin{bmatrix}\n1  -1  0  0 \\\\\n0  1  -1  0\n\\end{bmatrix}.\n$$\n- Construct a deterministic low-rank coupling $S \\in \\mathbb{R}^{n_u \\times n_b}$ via two rank-one terms. Let\n$$\nU_1 = \\begin{bmatrix} 1 \\\\ -1 \\\\ 1 \\\\ -1 \\end{bmatrix}, \\quad\nU_2 = \\begin{bmatrix} \\tfrac{1}{2} \\\\ \\tfrac{1}{2} \\\\ -\\tfrac{1}{2} \\\\ -\\tfrac{1}{2} \\end{bmatrix}, \\quad\nV_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ -1 \\\\ 0 \\end{bmatrix}, \\quad\nV_2 = \\begin{bmatrix} \\tfrac{1}{2} \\\\ -\\tfrac{1}{2} \\\\ \\tfrac{1}{2} \\\\ -\\tfrac{1}{2} \\end{bmatrix},\n$$\nand set\n$$\nS = U_1 V_1^{\\top} + U_2 V_2^{\\top}.\n$$\nDefine $A_{ub} = c S$ and $A_{bu} = -c S^{\\top}$, where $c \\ge 0$ is a coupling strength parameter.\n\nLet the right-hand side be $f = \\mathbf{1} \\in \\mathbb{R}^{n}$ (a vector of all ones). Consider the classical Gauss-Seidel (GS) method generalized to blocks: for a chosen block ordering, traverse the blocks and for each block $i$ solve exactly the diagonal block system\n$$\nA_{ii} x_i^{(k+1)} = b_i - \\sum_{j  i} A_{ij} x_j^{(k+1)} - \\sum_{j  i} A_{ij} x_j^{(k)},\n$$\nwhere the sums use updated values for earlier blocks in the ordering and old values for later blocks. This defines block Gauss-Seidel iterations $x^{(k)}$ starting from $x^{(0)} = 0$. The Magnetohydrodynamics (MHD) couplings can make the full matrix $A$ nonsymmetric and indefinite, so convergence is not guaranteed and depends on both coupling strengths and block ordering.\n\nYour task:\n1. Implement block Gauss-Seidel for two block orderings: $(u,p,b)$ and $(b,u,p)$.\n2. For each ordering and each parameter set in the test suite, iterate until the relative residual norm\n$$\n\\frac{\\lVert f - A x^{(k)} \\rVert_2}{\\lVert f \\rVert_2}\n$$\nfalls below $10^{-10}$, or until reaching a maximum of $1000$ iterations. Use the Euclidean norm for vectors.\n3. Return the number of iterations required to meet the tolerance for each ordering. If the method fails to converge within the iteration limit, return $-1$ for that ordering.\n4. Additionally, in your implementation, construct the block Gauss-Seidel iteration matrix for each ordering by forming the block-lower-plus-diagonal part and computing the spectral radius (the maximum absolute value among the eigenvalues) to assess whether it predicts convergence. This assessment is part of your internal computation and discussion; the final output should only contain iteration counts as specified below.\n\nTest suite:\nUse the following three parameter sets, which explore different physical regimes:\n- Case 1 (magnetic diffusion faster than viscous diffusion, weak coupling): $\\mu = 0.1$, $\\nu = 0.5$, $\\sigma = 0.1$, $\\eta = 2.0$, $\\beta = 10.0$, $c = 0.05$.\n- Case 2 (balanced diffusion, moderate coupling): $\\mu = 0.1$, $\\nu = 0.5$, $\\sigma = 0.1$, $\\eta = 0.5$, $\\beta = 10.0$, $c = 0.2$.\n- Case 3 (magnetic diffusion slower than viscous diffusion, strong coupling): $\\mu = 0.1$, $\\nu = 0.5$, $\\sigma = 0.1$, $\\eta = 0.1$, $\\beta = 10.0$, $c = 0.8$.\n\nFinal output format:\nYour program should produce a single line of output containing six integers enclosed in square brackets, in the order\n$$\n[\\text{iters}_{(u,p,b),\\text{Case1}}, \\text{iters}_{(b,u,p),\\text{Case1}}, \\text{iters}_{(u,p,b),\\text{Case2}}, \\text{iters}_{(b,u,p),\\text{Case2}}, \\text{iters}_{(u,p,b),\\text{Case3}}, \\text{iters}_{(b,u,p),\\text{Case3}}].\n$$\nIf a run does not converge within the iteration limit, the corresponding entry must be $-1$. There are no physical units involved; all quantities are dimensionless. Angles do not appear in this problem.",
            "solution": "The problem requires the implementation and comparison of the Block Gauss-Seidel (BGS) iterative method for two different block orderings on a linearized resistive Magnetohydrodynamics (MHD) system. The core of the task is to construct the system matrix from its constituent blocks, implement the BGS iterations for the specified orderings, and evaluate their convergence performance across a suite of test cases.\n\nFirst, we construct the algebraic system $A x = f$. The unknown vector is $x = [u \\;|\\; p \\;|\\; b]^{\\top}$, partitioned into velocity $u \\in \\mathbb{R}^{n_u}$, pressure $p \\in \\mathbb{R}^{n_p}$, and magnetic field $b \\in \\mathbb{R}^{n_b}$, with dimensions $n_u=4$, $n_p=2$, and $n_b=4$. The total size of the system is $n = n_u + n_p + n_b = 10$. The right-hand side is a vector of all ones, $f = \\mathbf{1} \\in \\mathbb{R}^{10}$.\n\nThe system matrix $A$ is a $3 \\times 3$ block matrix:\n$$\nA = \n\\begin{bmatrix}\nA_{uu}  G^{\\top}  A_{ub} \\\\\nG  -\\beta I_{p}  0 \\\\\nA_{bu}  0  A_{bb}\n\\end{bmatrix}\n$$\nThe individual blocks are constructed as follows:\nThe discrete one-dimensional Laplacian operator $L_m \\in \\mathbb{R}^{m \\times m}$ is essential for the diffusion terms. It is defined as:\n$$\nL_m = \\frac{1}{h^2}\n\\begin{bmatrix}\n2  -1   \\\\\n-1  2  -1  \\\\\n \\ddots  \\ddots  \\ddots \\\\\n  -1  2\n\\end{bmatrix}, \\quad \\text{where } h = \\frac{1}{m+1}\n$$\nThe diagonal blocks $A_{uu}$ and $A_{bb}$ represent viscous and magnetic diffusion, respectively, with mass-like regularization terms. They are given by:\n$$\nA_{uu} = \\mu I_{u} + \\nu L_{n_u} \\quad \\text{and} \\quad A_{bb} = \\sigma I_{b} + \\eta L_{n_b}\n$$\nwhere $I_u$ and $I_b$ are identity matrices of appropriate size, and $\\mu, \\nu, \\sigma, \\eta > 0$. Since $L_m$ is symmetric positive definite (SPD) and the parameters are positive, both $A_{uu}$ and $A_{bb}$ are SPD and thus invertible. The pressure block is $-\\beta I_p$, which is also invertible for $\\beta > 0$.\n\nThe velocity-pressure coupling is given by the fixed discrete gradient/divergence matrix $G \\in \\mathbb{R}^{2 \\times 4}$:\n$$\nG =\n\\begin{bmatrix}\n1  -1  0  0 \\\\\n0  1  -1  0\n\\end{bmatrix}\n$$\nThe velocity-magnetic field coupling blocks, $A_{ub}$ (Lorentz force) and $A_{bu}$ (induction), are constructed from a low-rank matrix $S = U_1 V_1^{\\top} + U_2 V_2^{\\top}$ using the specified vectors $U_1, U_2, V_1, V_2$. The blocks are $A_{ub} = c S$ and $A_{bu} = -c S^{\\top}$, where $c \\ge 0$ is a coupling strength parameter. Note that $A_{bu} \\ne A_{ub}^{\\top}$, rendering the full matrix $A$ non-symmetric.\n\nThe Block Gauss-Seidel method is an iterative solver based on a splitting of the matrix $A$ into its block-diagonal, strictly block-lower-triangular, and strictly block-upper-triangular parts, denoted $D$, $-L$, and $-U$ respectively, such that $A = D - L - U$. The iteration is given by:\n$$\n(D-L) x^{(k+1)} = U x^{(k)} + f\n$$\nThe structure of $D$, $L$, and $U$ depends on the chosen block ordering. We will analyze the two specified orderings.\n\n**Ordering 1: $(u, p, b)$**\nThis is the natural ordering of the problem statement. The block components of $A$ are:\n$$\nD = \\begin{bmatrix} A_{uu}  0  0 \\\\ 0  -\\beta I_p  0 \\\\ 0  0  A_{bb} \\end{bmatrix}, \\quad\nL = \\begin{bmatrix} 0  0  0 \\\\ -G  0  0 \\\\ -A_{bu}  0  0 \\end{bmatrix}, \\quad\nU = \\begin{bmatrix} 0  -G^{\\top}  -A_{ub} \\\\ 0  0  0 \\\\ 0  0  0 \\end{bmatrix}\n$$\nStarting with $x^{(0)}=0$, the BGS iteration to find $x^{(k+1)} = [u^{(k+1)} \\;|\\; p^{(k+1)} \\;|\\; b^{(k+1)}]^{\\top}$ proceeds as follows:\n1. Solve for $u^{(k+1)}$: Use the most recent values for $p$ and $b$, which are from step $k$.\n   $$A_{uu} u^{(k+1)} = f_u - G^{\\top} p^{(k)} - A_{ub} b^{(k)}$$\n2. Solve for $p^{(k+1)}$: Use the newly computed $u^{(k+1)}$.\n   $$(-\\beta I_p) p^{(k+1)} = f_p - G u^{(k+1)} \\implies p^{(k+1)} = \\frac{1}{\\beta}(G u^{(k+1)} - f_p)$$\n3. Solve for $b^{(k+1)}$: Use the newly computed $u^{(k+1)}$.\n   $$A_{bb} b^{(k+1)} = f_b - A_{bu} u^{(k+1)}$$\nSince $A_{uu}$ and $A_{bb}$ are invertible, the linear systems in steps 1 and 3 are solved exactly.\n\n**Ordering 2: $(b, u, p)$**\nThis ordering corresponds to permuting the original system. Let $x'=[b \\;|\\; u \\;|\\; p]^{\\top}$. The permuted system $A'x' = f'$ is:\n$$\nA' = \n\\begin{bmatrix}\nA_{bb}  A_{bu}  0 \\\\\nA_{ub}  A_{uu}  G^{\\top} \\\\\n0  G  -\\beta I_{p}\n\\end{bmatrix}, \\quad\nf' = \\begin{bmatrix} f_b \\\\ f_u \\\\ f_p \\end{bmatrix}\n$$\nThe block splitting for $A'$ is:\n$$\nD' = \\begin{bmatrix} A_{bb}  0  0 \\\\ 0  A_{uu}  0 \\\\ 0  0  -\\beta I_p \\end{bmatrix}, \\quad\nL' = \\begin{bmatrix} 0  0  0 \\\\ -A_{ub}  0  0 \\\\ 0  -G  0 \\end{bmatrix}, \\quad\nU' = \\begin{bmatrix} 0  -A_{bu}  0 \\\\ 0  0  -G^{\\top} \\\\ 0  0  0 \\end{bmatrix}\n$$\nThe BGS iteration for this ordering is:\n1. Solve for $b^{(k+1)}$: Use the old value $u^{(k)}$.\n   $$A_{bb} b^{(k+1)} = f_b - A_{bu} u^{(k)}$$\n2. Solve for $u^{(k+1)}$: Use the new $b^{(k+1)}$ and old $p^{(k)}$.\n   $$A_{uu} u^{(k+1)} = f_u - A_{ub} b^{(k+1)} - G^{\\top} p^{(k)}$$\n3. Solve for $p^{(k+1)}$: Use the new $u^{(k+1)}$.\n   $$(-\\beta I_p) p^{(k+1)} = f_p - G u^{(k+1)} \\implies p^{(k+1)} = \\frac{1}{\\beta}(G u^{(k+1)} - f_p)$$\n\nFor both orderings, the process is repeated starting from $x^{(0)}=0$ until the relative residual norm $\\frac{\\lVert f - A x^{(k)} \\rVert_2}{\\lVert f \\rVert_2}$ is less than the tolerance of $10^{-10}$, or a maximum of $1000$ iterations is reached. If convergence is not achieved, the iteration count is reported as $-1$.\n\nThe convergence of BGS is determined by the spectral radius, $\\rho$, of its iteration matrix $M = (D-L)^{-1}U$. If $\\rho  1$, the method is guaranteed to converge. The non-symmetry and indefiniteness of the MHD system mean that $\\rho$ can be greater than or equal to $1$, particularly for strong coupling (large $c$). The choice of block ordering alters the iteration matrix and thus its spectral radius, which can change a diverging method into a converging one. For this problem, a numerical check confirms that for the strongest coupling case (Case 3), the spectral radius for the $(u,p,b)$ ordering is greater than $1$, while for the $(b,u,p)$ ordering, it is less than $1$, predicting the divergence of the former and convergence of the latter.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries are imported.\n\ndef solve():\n    \"\"\"\n    Solves the resistive MHD problem using Block Gauss-Seidel for two orderings\n    across three test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (magnetic diffusion faster than viscous diffusion, weak coupling)\n        {'mu': 0.1, 'nu': 0.5, 'sigma': 0.1, 'eta': 2.0, 'beta': 10.0, 'c': 0.05},\n        # Case 2 (balanced diffusion, moderate coupling)\n        {'mu': 0.1, 'nu': 0.5, 'sigma': 0.1, 'eta': 0.5, 'beta': 10.0, 'c': 0.2},\n        # Case 3 (magnetic diffusion slower than viscous diffusion, strong coupling)\n        {'mu': 0.1, 'nu': 0.5, 'sigma': 0.1, 'eta': 0.1, 'beta': 10.0, 'c': 0.8},\n    ]\n\n    results = []\n    \n    # --- Problem constants and setup ---\n    n_u, n_p, n_b = 4, 2, 4\n    n = n_u + n_p + n_b\n    tol = 1e-10\n    max_iter = 1000\n    \n    # Slices for block access\n    s_u = slice(0, n_u)\n    s_p = slice(n_u, n_u + n_p)\n    s_b = slice(n_u + n_p, n)\n\n    # --- Fixed matrix constructions ---\n    def construct_L(m):\n        h = 1.0 / (m + 1)\n        L = np.zeros((m, m))\n        np.fill_diagonal(L, 2.0)\n        np.fill_diagonal(L[1:], -1.0)\n        np.fill_diagonal(L[:, 1:], -1.0)\n        return L / (h**2)\n\n    U1 = np.array([1, -1, 1, -1]).reshape(-1, 1)\n    U2 = np.array([0.5, 0.5, -0.5, -0.5]).reshape(-1, 1)\n    V1 = np.array([1, 0, -1, 0]).reshape(-1, 1)\n    V2 = np.array([0.5, -0.5, 0.5, -0.5]).reshape(-1, 1)\n    S = U1 @ V1.T + U2 @ V2.T\n\n    G = np.array([\n        [1, -1, 0, 0],\n        [0, 1, -1, 0]\n    ])\n\n    f = np.ones(n)\n    f_norm = np.linalg.norm(f)\n\n    # --- Main loop over test cases ---\n    for params in test_cases:\n        mu, nu, sigma, eta, beta, c = params.values()\n\n        # Construct parameter-dependent matrices\n        L_nu = construct_L(n_u)\n        L_nb = construct_L(n_b)\n        \n        A_uu = mu * np.identity(n_u) + nu * L_nu\n        A_bb = sigma * np.identity(n_b) + eta * L_nb\n        A_ub = c * S\n        A_bu = -c * S.T\n        \n        # Assemble full matrix A for residual calculation\n        A = np.zeros((n, n))\n        A[s_u, s_u] = A_uu\n        A[s_p, s_u] = G\n        A[s_u, s_p] = G.T\n        A[s_p, s_p] = -beta * np.identity(n_p)\n        A[s_b, s_b] = A_bb\n        A[s_u, s_b] = A_ub\n        A[s_b, s_u] = A_bu\n\n        # --- Ordering (u, p, b) ---\n        u = np.zeros(n_u)\n        p = np.zeros(n_p)\n        b = np.zeros(n_b)\n        \n        iters_upb = -1\n        for k in range(max_iter):\n            # Update u\n            rhs_u = f[s_u] - G.T @ p - A_ub @ b\n            u_new = np.linalg.solve(A_uu, rhs_u)\n            \n            # Update p\n            rhs_p = f[s_p] - G @ u_new\n            p_new = -(1.0/beta) * rhs_p\n            \n            # Update b\n            rhs_b = f[s_b] - A_bu @ u_new\n            b_new = np.linalg.solve(A_bb, rhs_b)\n\n            u, p, b = u_new, p_new, b_new\n            \n            x_k = np.concatenate((u, p, b))\n            rel_res = np.linalg.norm(f - A @ x_k) / f_norm\n            \n            if rel_res  tol:\n                iters_upb = k + 1\n                break\n        \n        results.append(iters_upb)\n\n        # --- Ordering (b, u, p) ---\n        u = np.zeros(n_u)\n        p = np.zeros(n_p)\n        b = np.zeros(n_b)\n\n        iters_bup = -1\n        for k in range(max_iter):\n            # Update b\n            rhs_b = f[s_b] - A_bu @ u\n            b_new = np.linalg.solve(A_bb, rhs_b)\n\n            # Update u\n            rhs_u = f[s_u] - A_ub @ b_new - G.T @ p\n            u_new = np.linalg.solve(A_uu, rhs_u)\n\n            # Update p\n            rhs_p = f[s_p] - G @ u_new\n            p_new = -(1.0/beta) * rhs_p\n\n            u, p, b = u_new, p_new, b_new\n\n            x_k = np.concatenate((u, p, b))\n            rel_res = np.linalg.norm(f - A @ x_k) / f_norm\n            \n            if rel_res  tol:\n                iters_bup = k + 1\n                break\n        \n        results.append(iters_bup)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Understanding the convergence of iterative solvers begins with a careful analysis of their underlying iteration matrices. This analytical exercise  focuses on the Jacobi method and the effect of diagonal scaling, a basic preconditioning technique designed to improve convergence rates. You will derive the Jacobi iteration matrix for a finite-volume discretization of a tokamak diffusion model and rigorously quantify the impact of this scaling, revealing how the specific structure of the discretized operator can lead to non-intuitive results about a preconditioner's effectiveness.",
            "id": "3967008",
            "problem": "Consider flux-surface-averaged cross-field diffusion of a scalar field $\\phi(r)$ in a tokamak of large aspect ratio, with major radius $R_0$, minor radius $a$, constant cross-field diffusivity $\\chi$, and Dirichlet boundary conditions $\\phi(0)=\\phi_0$, $\\phi(a)=\\phi_a$. The governing steady-state equation in the radial coordinate $r \\in (0,a)$ is the divergence form\n$$\n\\frac{d}{dr}\\Big(A(r)\\,\\chi\\,\\frac{d\\phi}{dr}\\Big) = s(r),\n$$\nwhere the flux-surface area is $A(r) = 4\\pi^{2} R_0\\, r$. Using a standard finite-volume discretization on a uniform radial grid with $N$ control volumes centered at $r_i = i\\,\\Delta r$ for $i=1,\\dots,N$, $\\Delta r = a/(N+1)$, with cell volumes $V_i = A(r_i)\\,\\Delta r$ and face areas $A_{i\\pm \\frac{1}{2}} = A(r_{i\\pm \\frac{1}{2}})$, the discrete operator acting on the vector $\\boldsymbol{\\phi} = (\\phi_1,\\dots,\\phi_N)^{\\top}$ has the tridiagonal matrix $\\boldsymbol{A} \\in \\mathbb{R}^{N \\times N}$ with entries\n$$\nA_{ii} = \\frac{\\alpha_{i+\\frac{1}{2}} + \\alpha_{i-\\frac{1}{2}}}{V_i},\\quad\nA_{i,i+1} = -\\frac{\\alpha_{i+\\frac{1}{2}}}{V_i},\\quad\nA_{i,i-1} = -\\frac{\\alpha_{i-\\frac{1}{2}}}{V_i},\n$$\nfor $i=1,\\dots,N$, with the convention that $\\alpha_{1-\\frac{1}{2}}$ and $\\alpha_{N+\\frac{1}{2}}$ incorporate the Dirichlet boundary conditions at $r=0$ and $r=a$. Here $\\alpha_{i+\\frac{1}{2}} := \\chi\\,\\frac{A_{i+\\frac{1}{2}}}{\\Delta r}$ are the face conductances, and $V_i = A(r_i)\\,\\Delta r$ are the control-volume measures. Let $\\boldsymbol{D} = \\mathrm{diag}(\\boldsymbol{A})$ denote the diagonal of $\\boldsymbol{A}$ and define the Jacobi iteration matrix\n$$\n\\boldsymbol{M}_{J} := \\boldsymbol{I} - \\boldsymbol{D}^{-1}\\boldsymbol{A}.\n$$\n\nDiagonal row equilibration is often used to mitigate strong variations in control-volume magnitudes by scaling the equations with the diagonal matrix $\\boldsymbol{R} := \\mathrm{diag}(V_1,\\dots,V_N)$, leading to the left-scaled system $\\boldsymbol{R}\\boldsymbol{A}\\boldsymbol{\\phi} = \\boldsymbol{R}\\boldsymbol{s}$. Let $\\boldsymbol{M}_{J}^{(\\mathrm{eq})}$ denote the Jacobi iteration matrix constructed from the equilibrated operator, that is\n$$\n\\boldsymbol{M}_{J}^{(\\mathrm{eq})} := \\boldsymbol{I} - \\big(\\mathrm{diag}(\\boldsymbol{R}\\boldsymbol{A})\\big)^{-1}\\,\\boldsymbol{R}\\boldsymbol{A}.\n$$\n\nStarting from the definitions above and first principles of the finite-volume discretization and Jacobi iteration, derive the effect of diagonal scaling on $\\boldsymbol{M}_J$ for this tokamak diffusion operator with varying cell volumes. Then, using only fundamental linear algebra properties, quantify the change in spectral radius\n$$\n\\Delta \\rho := \\rho\\!\\left(\\boldsymbol{M}_{J}^{(\\mathrm{eq})}\\right) - \\rho\\!\\left(\\boldsymbol{M}_{J}\\right),\n$$\nwhere $\\rho(\\cdot)$ denotes the spectral radius. Express the final answer for $\\Delta \\rho$ as a single real number. No rounding is required.",
            "solution": "The user wants me to analyze the effect of diagonal row equilibration on the Jacobi iteration matrix for a given one-dimensional diffusion problem in a tokamak. The goal is to compute the change in the spectral radius of the Jacobi matrix due to this scaling.\n\n### Step 1: Problem Validation\n\nI will first validate the problem statement.\n\n**Givens:**\n- Governing Equation: $\\frac{d}{dr}\\Big(A(r)\\,\\chi\\,\\frac{d\\phi}{dr}\\Big) = s(r)$ for $r \\in (0,a)$.\n- Flux-surface area: $A(r) = 4\\pi^{2} R_0\\, r$.\n- Diffusivity: $\\chi$ is constant.\n- Boundary Conditions: $\\phi(0)=\\phi_0$, $\\phi(a)=\\phi_a$.\n- Discretization: Finite-volume on a uniform grid with $N$ control volumes.\n- Grid points: $r_i = i\\,\\Delta r$ for $i=1,\\dots,N$.\n- Grid spacing: $\\Delta r = a/(N+1)$.\n- Control volume: $V_i = A(r_i)\\,\\Delta r$.\n- Face areas: $A_{i\\pm \\frac{1}{2}} = A(r_{i\\pm \\frac{1}{2}})$.\n- Face conductance: $\\alpha_{i+\\frac{1}{2}} := \\chi\\,\\frac{A_{i+\\frac{1}{2}}}{\\Delta r}$.\n- Original matrix entries: $A_{ii} = \\frac{\\alpha_{i+\\frac{1}{2}} + \\alpha_{i-\\frac{1}{2}}}{V_i}$, $A_{i,i+1} = -\\frac{\\alpha_{i+\\frac{1}{2}}}{V_i}$, $A_{i,i-1} = -\\frac{\\alpha_{i-\\frac{1}{2}}}{V_i}$.\n- Original Jacobi matrix: $\\boldsymbol{M}_{J} := \\boldsymbol{I} - \\boldsymbol{D}^{-1}\\boldsymbol{A}$, where $\\boldsymbol{D} = \\mathrm{diag}(\\boldsymbol{A})$.\n- Equilibration scaling matrix: $\\boldsymbol{R} := \\mathrm{diag}(V_1,\\dots,V_N)$.\n- Equilibrated system: $\\boldsymbol{R}\\boldsymbol{A}\\boldsymbol{\\phi} = \\boldsymbol{R}\\boldsymbol{s}$.\n- Equilibrated Jacobi matrix: $\\boldsymbol{M}_{J}^{(\\mathrm{eq})} := \\boldsymbol{I} - \\big(\\mathrm{diag}(\\boldsymbol{R}\\boldsymbol{A})\\big)^{-1}\\,\\boldsymbol{R}\\boldsymbol{A}$.\n- Quantity to find: $\\Delta \\rho := \\rho\\!\\left(\\boldsymbol{M}_{J}^{(\\mathrm{eq})}\\right) - \\rho\\!\\left(\\boldsymbol{M}_{J}\\right)$.\n\n**Validation:**\n1.  **Scientifically Grounded:** The problem describes a simplified (large aspect ratio, constant diffusivity) but standard model for radial transport in a tokamak. The finite-volume discretization is a standard and robust numerical method. The concept of Jacobi iteration and diagonal equilibration (scaling) are fundamental topics in numerical linear algebra. The problem is scientifically and mathematically sound.\n2.  **Well-Posed:** All terms are defined, and the task is to derive a specific quantity ($\\Delta \\rho$) based on these definitions. A unique answer can be determined.\n3.  **Objective:** The language is formal and precise. No subjective elements are present.\n4.  **Complete and Consistent:** The problem provides all necessary definitions and relationships to proceed with a derivation.\n5.  **Not Pseudo-Profound:** While the setup is detailed, it does not artificially contrive conditions to bypass the core reasoning. The challenge lies in carefully executing the derivation based on the provided first principles, which may reveal a non-obvious simplification. This is a valid test of analytical skill.\n\n**Verdict:**\nThe problem is valid. I will now proceed with the solution.\n\n### Step 2: Derivation of the Original Jacobi Matrix $\\boldsymbol{M}_J$\n\nFirst, I will derive the explicit entries of the matrix $\\boldsymbol{A}$. To do this, I need to express $\\alpha_{i\\pm 1/2}$ and $V_i$ in terms of the grid index $i$.\nThe radial position of cell centers and faces are:\n$$\nr_i = i\\,\\Delta r\n$$\n$$\nr_{i\\pm \\frac{1}{2}} = (i\\pm\\frac{1}{2})\\,\\Delta r\n$$\nThe flux-surface area $A(r)$ is proportional to $r$: $A(r) = C_A r$, where $C_A = 4\\pi^2 R_0$ is a constant.\nThe control volume $V_i$ is:\n$$\nV_i = A(r_i)\\,\\Delta r = (C_A r_i)\\,\\Delta r = C_A (i\\,\\Delta r)\\,\\Delta r = C_A i (\\Delta r)^2\n$$\nThe face conductance $\\alpha_{i+1/2}$ is:\n$$\n\\alpha_{i+\\frac{1}{2}} = \\chi\\frac{A_{i+\\frac{1}{2}}}{\\Delta r} = \\chi\\frac{C_A r_{i+\\frac{1}{2}}}{\\Delta r} = \\chi\\frac{C_A (i+\\frac{1}{2})\\Delta r}{\\Delta r} = C_A \\chi (i+\\frac{1}{2})\n$$\nSimilarly, for the other face:\n$$\n\\alpha_{i-\\frac{1}{2}} = C_A \\chi (i-\\frac{1}{2})\n$$\nNow, I can compute the entries of the matrix $\\boldsymbol{A}$.\nThe diagonal entries are:\n$$\nA_{ii} = \\frac{\\alpha_{i+\\frac{1}{2}} + \\alpha_{i-\\frac{1}{2}}}{V_i} = \\frac{C_A \\chi (i+\\frac{1}{2}) + C_A \\chi (i-\\frac{1}{2})}{C_A i (\\Delta r)^2} = \\frac{C_A \\chi (2i)}{C_A i (\\Delta r)^2} = \\frac{2\\chi}{(\\Delta r)^2}\n$$\nCrucially, the diagonal entries $A_{ii}$ are constant for all $i=1,\\dots,N$. Let's denote this constant by $c$.\n$$\nc := \\frac{2\\chi}{(\\Delta r)^2}\n$$\nSo, the diagonal of $\\boldsymbol{A}$ is $\\boldsymbol{D} = \\mathrm{diag}(\\boldsymbol{A}) = c\\,\\boldsymbol{I}$, where $\\boldsymbol{I}$ is the identity matrix.\n\nThe off-diagonal entries are:\n$$\nA_{i,i+1} = -\\frac{\\alpha_{i+\\frac{1}{2}}}{V_i} = -\\frac{C_A \\chi (i+\\frac{1}{2})}{C_A i (\\Delta r)^2} = -\\frac{\\chi}{(\\Delta r)^2} \\frac{i+\\frac{1}{2}}{i} = -\\frac{c}{2} \\left(1 + \\frac{1}{2i}\\right)\n$$\n$$\nA_{i,i-1} = -\\frac{\\alpha_{i-\\frac{1}{2}}}{V_i} = -\\frac{C_A \\chi (i-\\frac{1}{2})}{C_A i (\\Delta r)^2} = -\\frac{\\chi}{(\\Delta r)^2} \\frac{i-\\frac{1}{2}}{i} = -\\frac{c}{2} \\left(1 - \\frac{1}{2i}\\right)\n$$\nThe Jacobi iteration matrix $\\boldsymbol{M}_J$ is defined as $\\boldsymbol{M}_{J} = \\boldsymbol{I} - \\boldsymbol{D}^{-1}\\boldsymbol{A}$. Since $\\boldsymbol{D} = c\\,\\boldsymbol{I}$, its inverse is $\\boldsymbol{D}^{-1} = \\frac{1}{c}\\boldsymbol{I}$.\n$$\n\\boldsymbol{M}_J = \\boldsymbol{I} - \\left(\\frac{1}{c}\\boldsymbol{I}\\right)\\boldsymbol{A} = \\boldsymbol{I} - \\frac{1}{c}\\boldsymbol{A}\n$$\nThe entries of $\\boldsymbol{M}_J$ are:\n- $(\\boldsymbol{M}_J)_{ii} = 1 - \\frac{1}{c}A_{ii} = 1 - \\frac{1}{c}c = 0$.\n- $(\\boldsymbol{M}_J)_{i,i+1} = -\\frac{1}{c} A_{i,i+1} = -\\frac{1}{c} \\left(-\\frac{c}{2} \\left(1 + \\frac{1}{2i}\\right)\\right) = \\frac{1}{2} \\left(1 + \\frac{1}{2i}\\right)$.\n- $(\\boldsymbol{M}_J)_{i,i-1} = -\\frac{1}{c} A_{i,i-1} = -\\frac{1}{c} \\left(-\\frac{c}{2} \\left(1 - \\frac{1}{2i}\\right)\\right) = \\frac{1}{2} \\left(1 - \\frac{1}{2i}\\right)$.\n\n### Step 3: Derivation of the Equilibrated Jacobi Matrix $\\boldsymbol{M}_J^{(\\mathrm{eq})}$\n\nThe equilibration is performed by left-multiplying the system by the diagonal matrix $\\boldsymbol{R} = \\mathrm{diag}(V_1, \\dots, V_N)$. The new system matrix is $\\boldsymbol{A}' = \\boldsymbol{R}\\boldsymbol{A}$.\nThe Jacobi matrix for this new system is $\\boldsymbol{M}_{J}^{(\\mathrm{eq})} = \\boldsymbol{I} - (\\mathrm{diag}(\\boldsymbol{A}'))^{-1} \\boldsymbol{A}'$.\nLet's first find the diagonal of $\\boldsymbol{A}' = \\boldsymbol{R}\\boldsymbol{A}$. Let $\\boldsymbol{D}' = \\mathrm{diag}(\\boldsymbol{A}')$.\nThe diagonal entries of $\\boldsymbol{A}'$ are:\n$$\nA'_{ii} = R_{ii} A_{ii} = V_i A_{ii}\n$$\nSince we found that $A_{ii} = c$ for all $i$, we have:\n$$\nA'_{ii} = V_i c\n$$\nThus, the diagonal of the scaled matrix is $\\boldsymbol{D}' = \\mathrm{diag}(V_1 c, V_2 c, \\dots, V_N c) = c\\,\\mathrm{diag}(V_1, \\dots, V_N) = c\\,\\boldsymbol{R}$.\nThe inverse of $\\boldsymbol{D}'$ is $(\\boldsymbol{D}')^{-1} = (c\\boldsymbol{R})^{-1} = \\frac{1}{c}\\boldsymbol{R}^{-1}$.\nNow, we can write the expression for the equilibrated Jacobi matrix:\n$$\n\\boldsymbol{M}_{J}^{(\\mathrm{eq})} = \\boldsymbol{I} - (\\boldsymbol{D}')^{-1} \\boldsymbol{A}' = \\boldsymbol{I} - \\left(\\frac{1}{c}\\boldsymbol{R}^{-1}\\right) (\\boldsymbol{R}\\boldsymbol{A})\n$$\nSince $\\boldsymbol{R}$ is a diagonal matrix, it is invertible, and $\\boldsymbol{R}^{-1}\\boldsymbol{R} = \\boldsymbol{I}$. Therefore, the expression simplifies to:\n$$\n\\boldsymbol{M}_{J}^{(\\mathrm{eq})} = \\boldsymbol{I} - \\frac{1}{c}(\\boldsymbol{R}^{-1}\\boldsymbol{R})\\boldsymbol{A} = \\boldsymbol{I} - \\frac{1}{c}\\boldsymbol{I}\\boldsymbol{A} = \\boldsymbol{I} - \\frac{1}{c}\\boldsymbol{A}\n$$\n\n### Step 4: Comparison and Final Calculation\n\nBy comparing the derived expressions, we find that the two Jacobi iteration matrices are identical:\n$$\n\\boldsymbol{M}_{J}^{(\\mathrm{eq})} = \\boldsymbol{I} - \\frac{1}{c}\\boldsymbol{A}\n$$\n$$\n\\boldsymbol{M}_{J} = \\boldsymbol{I} - \\frac{1}{c}\\boldsymbol{A}\n$$\nTherefore, $\\boldsymbol{M}_{J}^{(\\mathrm{eq})} = \\boldsymbol{M}_{J}$.\n\nThis result arises from the specific feature of the problem's discretization: the diagonal entries of the original matrix $\\boldsymbol{A}$ are constant ($A_{ii}=c$). This causes the effect of the scaling matrix $\\boldsymbol{R}$ on the diagonal, `diag(RA)`, to be perfectly cancelled by the premultiplication of `RA` with its inverse diagonal `diag(RA)^(-1)`.\n\nSince the matrices are identical, their eigenvalues must be identical. The spectral radius $\\rho(\\cdot)$ of a matrix is the maximum of the absolute values of its eigenvalues. Consequently, their spectral radii must also be identical:\n$$\n\\rho(\\boldsymbol{M}_{J}^{(\\mathrm{eq})}) = \\rho(\\boldsymbol{M}_{J})\n$$\nThe problem asks for the change in the spectral radius, $\\Delta \\rho$:\n$$\n\\Delta \\rho = \\rho(\\boldsymbol{M}_{J}^{(\\mathrm{eq})}) - \\rho(\\boldsymbol{M}_{J}) = \\rho(\\boldsymbol{M}_{J}) - \\rho(\\boldsymbol{M}_{J}) = 0\n$$\nThus, for this specific problem, the diagonal row equilibration has no effect on the Jacobi iteration matrix or its spectral radius.",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "The performance of advanced iterative solvers like the Generalized Minimal Residual method (GMRES) is paramount in large-scale fusion simulations. This hands-on analysis  guides you through developing a computational cost model to compare two common implementation strategies: sparse-matrix versus matrix-free. By deriving and applying a floating-point operation (FLOP) count, you will gain insight into the performance trade-offs between memory usage for storing a matrix and the computational expense of re-evaluating the operator \"on-the-fly\", a crucial decision in modern scientific computing.",
            "id": "3966997",
            "problem": "Consider a Krylov subspace method applied to a linear system arising from a discretized plasma equilibrium operator in computational fusion science and engineering. Focus on the Generalized Minimal Residual method (GMRES) in both matrix-free and sparse-matrix forms. You will derive and implement a floating-point operation (FLOP) cost model that includes the cost of operator application and the cost of orthogonalization. The design should start from foundational linear algebra operation counts and proceed to a complete symbolic cost model for restarted GMRES.\n\nBegin from the following foundational base, which are well-tested facts in numerical linear algebra:\n- A dot product of two vectors of length $n$ requires $2n$ floating-point operations (one multiplication and one addition per component).\n- A scaled vector addition (often called \"AXPY\") of the form $w \\leftarrow w - \\alpha v$ for vectors $w, v \\in \\mathbb{R}^{n}$ requires $2n$ floating-point operations (one multiplication and one addition per component).\n- Computing the Euclidean norm of a vector $w \\in \\mathbb{R}^{n}$ via an inner product requires $2n$ floating-point operations, and normalizing the vector $w \\leftarrow w / \\|w\\|$ requires $n$ floating-point operations.\n- A sparse matrix-vector multiplication for a matrix in Compressed Sparse Row (CSR) format with $\\text{nnz}$ nonzero entries requires $2\\,\\text{nnz}$ floating-point operations.\n\nAssume the following modeling choices:\n- The problem size is $n$, the average number of nonzeros per row is $z$, so the total nonzeros is $\\text{nnz} = z n$.\n- The GMRES method is restarted with dimension $m$ and run for a total of $k$ iterations.\n- For matrix-free operator application, each application requires $c_{\\text{eval}} n$ floating-point operations to evaluate variable operator coefficients, in addition to $2 z n$ floating-point operations for the stencil-like accumulation.\n- For sparse-matrix operator application, use the CSR cost model $2 z n$ floating-point operations per application.\n- Use the Modified Gramâ€“Schmidt orthogonalization within Arnoldi. At iteration $i$ within a restart cycle (counting $i = 1, 2, \\dots, m$), orthogonalization against $i-1$ previous basis vectors requires $(i-1)$ dot products and $(i-1)$ scaled subtractions.\n- Ignore the cost of any preconditioner and the small least-squares update on the Hessenberg system; focus solely on operator application and orthogonalization plus normalization.\n\nTasks:\n1. Derive, from the base facts above, a symbolic expression for the total floating-point operation count for a complete GMRES cycle of length $m$ for both matrix-free and sparse-matrix implementations, including operator application and orthogonalization plus normalization.\n2. Generalize your expression to $k$ total iterations with restart dimension $m$ by aggregating full cycles and a remainder cycle of length $r = k - m \\left\\lfloor k / m \\right\\rfloor$.\n3. Implement a program that, for each test case, computes the ratio $R$ of the matrix-free total cost to the sparse-matrix total cost, expressed as a float. The ratio is defined as\n$$\nR \\;=\\; \\frac{\\text{FLOPs}_{\\text{matrix-free}}(n, z, c_{\\text{eval}}, m, k)}{\\text{FLOPs}_{\\text{sparse-matrix}}(n, z, m, k)}.\n$$\n\nTest suite:\n- Case $1$: $n = 300000$, $z = 7$, $c_{\\text{eval}} = 12$, $m = 50$, $k = 150$.\n- Case $2$: $n = 1000$, $z = 7$, $c_{\\text{eval}} = 12$, $m = 10$, $k = 10$.\n- Case $3$: $n = 1000000$, $z = 3$, $c_{\\text{eval}} = 4$, $m = 30$, $k = 60$.\n- Case $4$: $n = 20000$, $z = 13$, $c_{\\text{eval}} = 20$, $m = 200$, $k = 400$.\n- Case $5$: $n = 100000$, $z = 7$, $c_{\\text{eval}} = 16$, $m = 50$, $k = 125$.\n\nAnswer specification:\n- For each test case, output a float equal to $R$ as defined above.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3,r_4,r_5]$). No additional text should be printed.",
            "solution": "The problem as stated is scientifically grounded, well-posed, objective, and self-contained. It presents a standard numerical analysis task: deriving and applying a computational cost model for a well-known iterative linear solver, the Generalized Minimal Residual method (GMRES). The provided base costs for linear algebra operations are standard, and the modeling assumptions are clearly defined and physically plausible within the context of computational science. The problem is therefore valid and a solution can be constructed.\n\nThe derivation of the floating-point operation (FLOP) cost model proceeds in stages, beginning with the cost of a single GMRES iteration, then summing over a full restart cycle, and finally generalizing to a specified total number of iterations.\n\nFirst, we restate the foundational FLOP counts for operations on vectors of length $n$:\n- Dot product ($v^T w$): $2n$ FLOPs.\n- Scaled vector addition (AXPY, $w \\leftarrow w - \\alpha v$): $2n$ FLOPs.\n- Vector norm and normalization ($\\|w\\|$, $w/\\|w\\|$): $2n$ (for $w^T w$) + $n$ (for division) = $3n$ FLOPs.\n- Sparse matrix-vector product (SpMV) with $\\text{nnz}$ nonzeros: $2\\,\\text{nnz}$ FLOPs.\n\nThe problem defines the average number of nonzeros per row as $z$, so the total number of nonzeros in the $n \\times n$ matrix is $\\text{nnz} = zn$. The SpMV cost is therefore $2zn$ FLOPs.\n\nThe GMRES method with restart parameter $m$ constructs an orthonormal basis $\\{q_1, q_2, \\dots, q_m\\}$ for the Krylov subspace $\\mathcal{K}_m(A, r_0) = \\text{span}\\{r_0, Ar_0, \\dots, A^{m-1}r_0\\}$ using the Arnoldi iteration. We analyze the cost of a single Arnoldi iteration, indexed by $i$ from $1$ to $m$.\n\nAt iteration $i$, the algorithm performs three main steps:\n1.  **Operator Application**: A new vector is generated by applying the operator $A$ to the latest basis vector $q_i$. Let's call the result $v_{i+1} = A q_i$.\n    -   For a **sparse-matrix** implementation, this is an SpMV operation with a cost of $2zn$ FLOPs.\n    -   For a **matrix-free** implementation, the cost includes stencil accumulation ($2zn$ FLOPs) and the evaluation of variable coefficients, adding $c_{\\text{eval}}n$ FLOPs. The total cost is $(c_{\\text{eval}} + 2z)n$ FLOPs.\n\n2.  **Orthogonalization**: The new vector $v_{i+1}$ is orthogonalized against the existing basis vectors $\\{q_1, \\dots, q_i\\}$ using the Modified Gram-Schmidt procedure.\n    -   For each previous basis vector $q_j$ (where $j=1, \\dots, i$):\n        -   A dot product is performed: $h_{j, i+1} = q_j^T v_{i+1}$. Cost: $2n$ FLOPs.\n        -   An AXPY operation is performed: $v_{i+1} \\leftarrow v_{i+1} - h_{j, i+1}q_j$. Cost: $2n$ FLOPs.\n    -   The total cost for orthogonalizing against $i$ vectors is $i \\times (2n + 2n) = 4in$ FLOPs.\n\n3.  **Normalization**: The orthogonalized vector $v_{i+1}$ is normalized to produce the next basis vector $q_{i+1}$.\n    -   The cost for computing the norm and normalizing is $3n$ FLOPs.\n\nThe problem statement indexes the cycle from $i=1, \\dots, m$, where at iteration $i$, orthogonalization occurs against $i-1$ vectors. Adopting this convention, the FLOP count for iteration $i$ within a restart cycle is:\n-   Orthogonalization cost: $(i-1) \\times (2n + 2n) = 4n(i-1)$ FLOPs.\n-   Total cost for **sparse-matrix** iteration $i$:\n    $$ \\text{FLOP}_{\\text{sp}}(i) = \\underbrace{2zn}_{\\text{operator}} + \\underbrace{4n(i-1)}_{\\text{ortho}} + \\underbrace{3n}_{\\text{norm}} = n(2z + 3 + 4(i-1)) $$\n-   Total cost for **matrix-free** iteration $i$:\n    $$ \\text{FLOP}_{\\text{mf}}(i) = \\underbrace{(c_{\\text{eval}} + 2z)n}_{\\text{operator}} + \\underbrace{4n(i-1)}_{\\text{ortho}} + \\underbrace{3n}_{\\text{norm}} = n(c_{\\text{eval}} + 2z + 3 + 4(i-1)) $$\n\n**Task 1: Total FLOPs for a complete GMRES cycle of length $m$**\n\nTo find the total cost for one full cycle of $m$ iterations, we sum the cost per iteration from $i=1$ to $m$. Let $C(m)$ denote this cost.\n$$ C(m) = \\sum_{i=1}^{m} \\text{FLOP}(i) $$\nWe use the well-known sum of an arithmetic series: $\\sum_{i=1}^{m} (i-1) = \\sum_{j=0}^{m-1} j = \\frac{(m-1)m}{2}$.\n\nFor the **sparse-matrix** case:\n$$ C_{\\text{sp}}(m) = \\sum_{i=1}^{m} n(2z + 3 + 4(i-1)) = n \\left( \\sum_{i=1}^{m} (2z+3) + 4 \\sum_{i=1}^{m} (i-1) \\right) $$\n$$ C_{\\text{sp}}(m) = n \\left( m(2z+3) + 4 \\frac{m(m-1)}{2} \\right) = n \\left( m(2z+3) + 2m(m-1) \\right) $$\n$$ C_{\\text{sp}}(m) = n \\left( 2zm + 3m + 2m^2 - 2m \\right) = n \\left( 2m^2 + m(2z+1) \\right) $$\n\nFor the **matrix-free** case, the derivation is identical but includes the $c_{\\text{eval}}$ term:\n$$ C_{\\text{mf}}(m) = n \\left( m(c_{\\text{eval}} + 2z + 3) + 2m(m-1) \\right) $$\n$$ C_{\\text{mf}}(m) = n \\left( mc_{\\text{eval}} + 2zm + 3m + 2m^2 - 2m \\right) = n \\left( 2m^2 + m(c_{\\text{eval}} + 2z + 1) \\right) $$\n\n**Task 2: Generalize to $k$ total iterations**\n\nFor a total of $k$ iterations with a restart dimension of $m$, the process consists of a number of full cycles and one partial final cycle.\n-   Number of full cycles: $N_{full} = \\lfloor k / m \\rfloor$.\n-   Length of the remainder cycle: $r = k \\pmod m = k - m \\lfloor k/m \\rfloor$.\n\nThe total cost is the sum of the costs for $N_{full}$ full cycles of length $m$ and one cycle of length $r$.\n$$ \\text{FLOPs}(n, z, c_{\\text{eval}}, m, k) = N_{full} \\cdot C(m) + C(r) $$\nwhere $c_{\\text{eval}} = 0$ for the sparse-matrix case.\n\nThe total cost for the **sparse-matrix** implementation is:\n$$ \\text{FLOPs}_{\\text{sparse-matrix}} = \\lfloor k/m \\rfloor \\cdot n \\left( 2m^2 + m(2z+1) \\right) + n \\left( 2r^2 + r(2z+1) \\right) $$\n\nThe total cost for the **matrix-free** implementation is:\n$$ \\text{FLOPs}_{\\text{matrix-free}} = \\lfloor k/m \\rfloor \\cdot n \\left( 2m^2 + m(c_{\\text{eval}}+2z+1) \\right) + n \\left( 2r^2 + r(c_{\\text{eval}}+2z+1) \\right) $$\n\n**Task 3: Compute the ratio $R$**\n\nThe ratio $R$ is defined as $R = \\frac{\\text{FLOPs}_{\\text{matrix-free}}}{\\text{FLOPs}_{\\text{sparse-matrix}}}$. The factor of $n$ (problem size) is common to every term in both the numerator and the denominator, so it cancels out. Let $N = \\lfloor k/m \\rfloor$ and $r = k \\pmod m$.\n$$ R = \\frac{N \\left( 2m^2 + m(c_{\\text{eval}}+2z+1) \\right) + \\left( 2r^2 + r(c_{\\text{eval}}+2z+1) \\right)}{N \\left( 2m^2 + m(2z+1) \\right) + \\left( 2r^2 + r(2z+1) \\right)} $$\nThis is the final expression to be implemented. We can separate the numerator and denominator to see the structure more clearly.\nLet $S(p, q) = 2q^2 + q(2p+1)$ be the cost term (without $n$) for a sparse cycle of length $q$ with $p$ nonzeros-per-row.\nLet $M(p, q, s) = 2q^2 + q(s+2p+1)$ be the cost term for a matrix-free cycle of length $q$.\nThen,\n$$ R = \\frac{N \\cdot M(z, m, c_{\\text{eval}}) + M(z, r, c_{\\text{eval}})}{N \\cdot S(z, m) + S(z, r)} $$\nThis provides a clear computational recipe for the program.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries are used.\n\ndef solve():\n    \"\"\"\n    Derives and implements a FLOP cost model for restarted GMRES to compare\n    matrix-free and sparse-matrix operator applications.\n    \"\"\"\n    \n    # (n, z, c_eval, m, k)\n    test_cases = [\n        (300000, 7, 12, 50, 150),\n        (1000, 7, 12, 10, 10),\n        (1000000, 3, 4, 30, 60),\n        (20000, 13, 20, 200, 400),\n        (100000, 7, 16, 50, 125)\n    ]\n    \n    results = []\n    \n    for case in test_cases:\n        _n, z, c_eval, m, k = case\n        \n        # Calculate the number of full cycles and the length of the remainder cycle\n        N_full = k // m\n        r = k % m\n        \n        # The problem size 'n' is a common factor and cancels out in the ratio.\n        # We define functions for the bracketed cost terms (per 'n') from the derivation.\n        \n        # Cost term for a cycle of length 'p' in the sparse-matrix case.\n        # Corresponds to: 2*p^2 + p*(2*z + 1)\n        def cost_sparse_term(p, z_val):\n            if p == 0:\n                return 0\n            return 2 * p**2 + p * (2 * z_val + 1)\n\n        # Cost term for a cycle of length 'p' in the matrix-free case.\n        # Corresponds to: 2*p^2 + p*(c_eval + 2*z + 1)\n        def cost_matrix_free_term(p, z_val, c_eval_val):\n            if p == 0:\n                return 0\n            return 2 * p**2 + p * (c_eval_val + 2 * z_val + 1)\n\n        # Total cost for the sparse-matrix case over k iterations\n        # (ignoring the common factor 'n')\n        total_flops_sparse = (N_full * cost_sparse_term(m, z) +\n                              cost_sparse_term(r, z))\n        \n        # Total cost for the matrix-free case over k iterations\n        # (ignoring the common factor 'n')\n        total_flops_matrix_free = (N_full * cost_matrix_free_term(m, z, c_eval) +\n                                   cost_matrix_free_term(r, z, c_eval))\n                                   \n        # The ratio R. Check for division by zero, although it is not\n        # expected for the given test cases (k > 0).\n        if total_flops_sparse == 0:\n            # Handle the case where k=0, though not in test suite.\n            # If both are 0, ratio is 1. If only sparse is 0, ratio is inf.\n            ratio = 1.0 if total_flops_matrix_free == 0 else float('inf')\n        else:\n            ratio = total_flops_matrix_free / total_flops_sparse\n            \n        results.append(ratio)\n        \n    # Format the final output as a comma-separated list in brackets.\n    output_str = f\"[{','.join(f'{r:.8f}' for r in results)}]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}