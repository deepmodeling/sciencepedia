{
    "hands_on_practices": [
        {
            "introduction": "Many problems in fusion science, such as Magnetohydrodynamics (MHD), involve the coupling of multiple physical fields, leading to large, block-structured linear systems. While modern Krylov methods are often used, understanding classical block-iterative methods like Block Gauss-Seidel is fundamental. This hands-on exercise  demonstrates that for such coupled, non-symmetric systems, convergence can be highly sensitive to the order in which the variable blocks are updated, a choice that often has a direct physical interpretation.",
            "id": "3967020",
            "problem": "Consider a resistive Magnetohydrodynamics (MHD) linearization around a uniform background state leading to a block-coupled algebraic system for the increments of velocity, magnetic field, and pressure. Let the unknown be the vector $x = [u \\;|\\; p \\;|\\; b]^{\\top}$, where $u \\in \\mathbb{R}^{n_u}$ is the discrete velocity, $p \\in \\mathbb{R}^{n_p}$ is the discrete pressure, and $b \\in \\mathbb{R}^{n_b}$ is the discrete magnetic field. The linear system has the block structure\n$$\nA x = f, \\quad \nA = \n\\begin{bmatrix}\nA_{uu} & G^{\\top} & A_{ub} \\\\\nG & -\\beta I_{p} & 0 \\\\\nA_{bu} & 0 & A_{bb}\n\\end{bmatrix},\n$$\nwhere $A_{uu}$ is the discrete viscous operator, $A_{bb}$ is the discrete resistive (magnetic diffusion) operator, $G$ is the discrete gradient/divergence coupling, $A_{ub}$ represents the Lorentz force coupling from magnetic field to momentum, and $A_{bu}$ represents the induction coupling from velocity to magnetic field. Here, $I_p$ denotes the identity matrix of size $n_p \\times n_p$, and $\\beta > 0$ is a pressure stabilization parameter ensuring invertibility of the block associated with $p$. This system is dimensionless for the purposes of this computational exercise.\n\nYou will construct the blocks as follows:\n- Let $n_u = 4$, $n_p = 2$, $n_b = 4$, and $n = n_u + n_p + n_b$.\n- Define the one-dimensional Dirichlet discrete Laplacian $L_m \\in \\mathbb{R}^{m \\times m}$ by\n$$\nL_m = \\frac{1}{h^2}\n\\begin{bmatrix}\n2 & -1 & & \\\\\n-1 & 2 & -1 & \\\\\n& \\ddots & \\ddots & \\ddots \\\\\n& & -1 & 2\n\\end{bmatrix}, \\quad h = \\frac{1}{m+1}.\n$$\n- Define $A_{uu} = \\mu I_{u} + \\nu L_{n_u}$ and $A_{bb} = \\sigma I_{b} + \\eta L_{n_b}$, where $I_u$ and $I_b$ are identity matrices of sizes $n_u \\times n_u$ and $n_b \\times n_b$, respectively. The parameters $\\mu, \\nu, \\sigma, \\eta > 0$ reflect mass-like regularization and diffusion coefficients.\n- Define a fixed discrete gradient/divergence coupling $G \\in \\mathbb{R}^{n_p \\times n_u}$ by\n$$\nG =\n\\begin{bmatrix}\n1 & -1 & 0 & 0 \\\\\n0 & 1 & -1 & 0\n\\end{bmatrix}.\n$$\n- Construct a deterministic low-rank coupling $S \\in \\mathbb{R}^{n_u \\times n_b}$ via two rank-one terms. Let\n$$\nU_1 = \\begin{bmatrix} 1 \\\\ -1 \\\\ 1 \\\\ -1 \\end{bmatrix}, \\quad\nU_2 = \\begin{bmatrix} \\tfrac{1}{2} \\\\ \\tfrac{1}{2} \\\\ -\\tfrac{1}{2} \\\\ -\\tfrac{1}{2} \\end{bmatrix}, \\quad\nV_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ -1 \\\\ 0 \\end{bmatrix}, \\quad\nV_2 = \\begin{bmatrix} \\tfrac{1}{2} \\\\ -\\tfrac{1}{2} \\\\ \\tfrac{1}{2} \\\\ -\\tfrac{1}{2} \\end{bmatrix},\n$$\nand set\n$$\nS = U_1 V_1^{\\top} + U_2 V_2^{\\top}.\n$$\nDefine $A_{ub} = c S$ and $A_{bu} = -c S^{\\top}$, where $c \\ge 0$ is a coupling strength parameter.\n\nLet the right-hand side be $f = \\mathbf{1} \\in \\mathbb{R}^{n}$ (a vector of all ones). Consider the classical Gauss-Seidel (GS) method generalized to blocks: for a chosen block ordering, traverse the blocks and for each block $i$ solve exactly the diagonal block system\n$$\nA_{ii} x_i^{(k+1)} = b_i - \\sum_{j < i} A_{ij} x_j^{(k+1)} - \\sum_{j > i} A_{ij} x_j^{(k)},\n$$\nwhere the sums use updated values for earlier blocks in the ordering and old values for later blocks. This defines block Gauss-Seidel iterations $x^{(k)}$ starting from $x^{(0)} = 0$. The Magnetohydrodynamics (MHD) couplings can make the full matrix $A$ nonsymmetric and indefinite, so convergence is not guaranteed and depends on both coupling strengths and block ordering.\n\nYour task:\n1. Implement block Gauss-Seidel for two block orderings: $(u,p,b)$ and $(b,u,p)$.\n2. For each ordering and each parameter set in the test suite, iterate until the relative residual norm\n$$\n\\frac{\\lVert f - A x^{(k)} \\rVert_2}{\\lVert f \\rVert_2}\n$$\nfalls below $10^{-10}$, or until reaching a maximum of $1000$ iterations. Use the Euclidean norm for vectors.\n3. Return the number of iterations required to meet the tolerance for each ordering. If the method fails to converge within the iteration limit, return $-1$ for that ordering.\n4. Additionally, in your implementation, construct the block Gauss-Seidel iteration matrix for each ordering by forming the block-lower-plus-diagonal part and computing the spectral radius (the maximum absolute value among the eigenvalues) to assess whether it predicts convergence. This assessment is part of your internal computation and discussion; the final output should only contain iteration counts as specified below.\n\nTest suite:\nUse the following three parameter sets, which explore different physical regimes:\n- Case 1 (magnetic diffusion faster than viscous diffusion, weak coupling): $\\mu = 0.1$, $\\nu = 0.5$, $\\sigma = 0.1$, $\\eta = 2.0$, $\\beta = 10.0$, $c = 0.05$.\n- Case 2 (balanced diffusion, moderate coupling): $\\mu = 0.1$, $\\nu = 0.5$, $\\sigma = 0.1$, $\\eta = 0.5$, $\\beta = 10.0$, $c = 0.2$.\n- Case 3 (magnetic diffusion slower than viscous diffusion, strong coupling): $\\mu = 0.1$, $\\nu = 0.5$, $\\sigma = 0.1$, $\\eta = 0.1$, $\\beta = 10.0$, $c = 0.8$.\n\nFinal output format:\nYour program should produce a single line of output containing six integers enclosed in square brackets, in the order\n$$\n[\\text{iters}_{(u,p,b),\\text{Case1}}, \\text{iters}_{(b,u,p),\\text{Case1}}, \\text{iters}_{(u,p,b),\\text{Case2}}, \\text{iters}_{(b,u,p),\\text{Case2}}, \\text{iters}_{(u,p,b),\\text{Case3}}, \\text{iters}_{(b,u,p),\\text{Case3}}].\n$$\nIf a run does not converge within the iteration limit, the corresponding entry must be $-1$. There are no physical units involved; all quantities are dimensionless. Angles do not appear in this problem.",
            "solution": "The problem requires the implementation and comparison of the Block Gauss-Seidel (BGS) iterative method for two different block orderings on a linearized resistive Magnetohydrodynamics (MHD) system. The core of the task is to construct the system matrix from its constituent blocks, implement the BGS iterations for the specified orderings, and evaluate their convergence performance across a suite of test cases.\n\nFirst, we construct the algebraic system $A x = f$. The unknown vector is $x = [u \\;|\\; p \\;|\\; b]^{\\top}$, partitioned into velocity $u \\in \\mathbb{R}^{n_u}$, pressure $p \\in \\mathbb{R}^{n_p}$, and magnetic field $b \\in \\mathbb{R}^{n_b}$, with dimensions $n_u=4$, $n_p=2$, and $n_b=4$. The total size of the system is $n = n_u + n_p + n_b = 10$. The right-hand side is a vector of all ones, $f = \\mathbf{1} \\in \\mathbb{R}^{10}$.\n\nThe system matrix $A$ is a $3 \\times 3$ block matrix:\n$$\nA = \n\\begin{bmatrix}\nA_{uu} & G^{\\top} & A_{ub} \\\\\nG & -\\beta I_{p} & 0 \\\\\nA_{bu} & 0 & A_{bb}\n\\end{bmatrix}\n$$\nThe individual blocks are constructed as follows:\nThe discrete one-dimensional Laplacian operator $L_m \\in \\mathbb{R}^{m \\times m}$ is essential for the diffusion terms. It is defined as:\n$$\nL_m = \\frac{1}{h^2}\n\\begin{bmatrix}\n2 & -1 & & \\\\\n-1 & 2 & -1 & \\\\\n& \\ddots & \\ddots & \\ddots \\\\\n& & -1 & 2\n\\end{bmatrix}, \\quad \\text{where } h = \\frac{1}{m+1}\n$$\nThe diagonal blocks $A_{uu}$ and $A_{bb}$ represent viscous and magnetic diffusion, respectively, with mass-like regularization terms. They are given by:\n$$\nA_{uu} = \\mu I_{u} + \\nu L_{n_u} \\quad \\text{and} \\quad A_{bb} = \\sigma I_{b} + \\eta L_{n_b}\n$$\nwhere $I_u$ and $I_b$ are identity matrices of appropriate size, and $\\mu, \\nu, \\sigma, \\eta > 0$. Since $L_m$ is symmetric positive definite (SPD) and the parameters are positive, both $A_{uu}$ and $A_{bb}$ are SPD and thus invertible. The pressure block is $-\\beta I_p$, which is also invertible for $\\beta > 0$.\n\nThe velocity-pressure coupling is given by the fixed discrete gradient/divergence matrix $G \\in \\mathbb{R}^{2 \\times 4}$:\n$$\nG =\n\\begin{bmatrix}\n1 & -1 & 0 & 0 \\\\\n0 & 1 & -1 & 0\n\\end{bmatrix}\n$$\nThe velocity-magnetic field coupling blocks, $A_{ub}$ (Lorentz force) and $A_{bu}$ (induction), are constructed from a low-rank matrix $S = U_1 V_1^{\\top} + U_2 V_2^{\\top}$ using the specified vectors $U_1, U_2, V_1, V_2$. The blocks are $A_{ub} = c S$ and $A_{bu} = -c S^{\\top}$, where $c \\ge 0$ is a coupling strength parameter. Note that $A_{bu} \\ne A_{ub}^{\\top}$, rendering the full matrix $A$ non-symmetric.\n\nThe Block Gauss-Seidel method is an iterative solver based on a splitting of the matrix $A$ into its block-diagonal, strictly block-lower-triangular, and strictly block-upper-triangular parts, denoted $D$, $-L$, and $-U$ respectively, such that $A = D - L - U$. The iteration is given by:\n$$\n(D-L) x^{(k+1)} = U x^{(k)} + f\n$$\nThe structure of $D$, $L$, and $U$ depends on the chosen block ordering. We will analyze the two specified orderings.\n\n**Ordering 1: $(u, p, b)$**\nThis is the natural ordering of the problem statement. The block components of $A$ are:\n$$\nD = \\begin{bmatrix} A_{uu} & 0 & 0 \\\\ 0 & -\\beta I_p & 0 \\\\ 0 & 0 & A_{bb} \\end{bmatrix}, \\quad\nL = \\begin{bmatrix} 0 & 0 & 0 \\\\ -G & 0 & 0 \\\\ -A_{bu} & 0 & 0 \\end{bmatrix}, \\quad\nU = \\begin{bmatrix} 0 & -G^{\\top} & -A_{ub} \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}\n$$\nStarting with $x^{(0)}=0$, the BGS iteration to find $x^{(k+1)} = [u^{(k+1)} \\;|\\; p^{(k+1)} \\;|\\; b^{(k+1)}]^{\\top}$ proceeds as follows:\n1. Solve for $u^{(k+1)}$: Use the most recent values for $p$ and $b$, which are from step $k$.\n   $$A_{uu} u^{(k+1)} = f_u - G^{\\top} p^{(k)} - A_{ub} b^{(k)}$$\n2. Solve for $p^{(k+1)}$: Use the newly computed $u^{(k+1)}$.\n   $$(-\\beta I_p) p^{(k+1)} = f_p - G u^{(k+1)} \\implies p^{(k+1)} = \\frac{1}{\\beta}(G u^{(k+1)} - f_p)$$\n3. Solve for $b^{(k+1)}$: Use the newly computed $u^{(k+1)}$.\n   $$A_{bb} b^{(k+1)} = f_b - A_{bu} u^{(k+1)}$$\nSince $A_{uu}$ and $A_{bb}$ are invertible, the linear systems in steps 1 and 3 are solved exactly.\n\n**Ordering 2: $(b, u, p)$**\nThis ordering corresponds to permuting the original system. Let $x'=[b \\;|\\; u \\;|\\; p]^{\\top}$. The permuted system $A'x' = f'$ is:\n$$\nA' = \n\\begin{bmatrix}\nA_{bb} & A_{bu} & 0 \\\\\nA_{ub} & A_{uu} & G^{\\top} \\\\\n0 & G & -\\beta I_{p}\n\\end{bmatrix}, \\quad\nf' = \\begin{bmatrix} f_b \\\\ f_u \\\\ f_p \\end{bmatrix}\n$$\nThe block splitting for $A'$ is:\n$$\nD' = \\begin{bmatrix} A_{bb} & 0 & 0 \\\\ 0 & A_{uu} & 0 \\\\ 0 & 0 & -\\beta I_p \\end{bmatrix}, \\quad\nL' = \\begin{bmatrix} 0 & 0 & 0 \\\\ -A_{ub} & 0 & 0 \\\\ 0 & -G & 0 \\end{bmatrix}, \\quad\nU' = \\begin{bmatrix} 0 & -A_{bu} & 0 \\\\ 0 & 0 & -G^{\\top} \\\\ 0 & 0 & 0 \\end{bmatrix}\n$$\nThe BGS iteration for this ordering is:\n1. Solve for $b^{(k+1)}$: Use the old value $u^{(k)}$.\n   $$A_{bb} b^{(k+1)} = f_b - A_{bu} u^{(k)}$$\n2. Solve for $u^{(k+1)}$: Use the new $b^{(k+1)}$ and old $p^{(k)}$.\n   $$A_{uu} u^{(k+1)} = f_u - A_{ub} b^{(k+1)} - G^{\\top} p^{(k)}$$\n3. Solve for $p^{(k+1)}$: Use the new $u^{(k+1)}$.\n   $$(-\\beta I_p) p^{(k+1)} = f_p - G u^{(k+1)} \\implies p^{(k+1)} = \\frac{1}{\\beta}(G u^{(k+1)} - f_p)$$\n\nFor both orderings, the process is repeated starting from $x^{(0)}=0$ until the relative residual norm $\\frac{\\lVert f - A x^{(k)} \\rVert_2}{\\lVert f \\rVert_2}$ is less than the tolerance of $10^{-10}$, or a maximum of $1000$ iterations is reached. If convergence is not achieved, the iteration count is reported as $-1$.\n\nThe convergence of BGS is determined by the spectral radius, $\\rho$, of its iteration matrix $M = (D-L)^{-1}U$. If $\\rho < 1$, the method is guaranteed to converge. The non-symmetry and indefiniteness of the MHD system mean that $\\rho$ can be greater than or equal to $1$, particularly for strong coupling (large $c$). The choice of block ordering alters the iteration matrix and thus its spectral radius, which can change a diverging method into a converging one. For this problem, a numerical check confirms that for the strongest coupling case (Case 3), the spectral radius for the $(u,p,b)$ ordering is greater than $1$, while for the $(b,u,p)$ ordering, it is less than $1$, predicting the divergence of the former and convergence of the latter.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries are imported.\n\ndef solve():\n    \"\"\"\n    Solves the resistive MHD problem using Block Gauss-Seidel for two orderings\n    across three test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (magnetic diffusion faster than viscous diffusion, weak coupling)\n        {'mu': 0.1, 'nu': 0.5, 'sigma': 0.1, 'eta': 2.0, 'beta': 10.0, 'c': 0.05},\n        # Case 2 (balanced diffusion, moderate coupling)\n        {'mu': 0.1, 'nu': 0.5, 'sigma': 0.1, 'eta': 0.5, 'beta': 10.0, 'c': 0.2},\n        # Case 3 (magnetic diffusion slower than viscous diffusion, strong coupling)\n        {'mu': 0.1, 'nu': 0.5, 'sigma': 0.1, 'eta': 0.1, 'beta': 10.0, 'c': 0.8},\n    ]\n\n    results = []\n    \n    # --- Problem constants and setup ---\n    n_u, n_p, n_b = 4, 2, 4\n    n = n_u + n_p + n_b\n    tol = 1e-10\n    max_iter = 1000\n    \n    # Slices for block access\n    s_u = slice(0, n_u)\n    s_p = slice(n_u, n_u + n_p)\n    s_b = slice(n_u + n_p, n)\n\n    # --- Fixed matrix constructions ---\n    def construct_L(m):\n        h = 1.0 / (m + 1)\n        L = np.zeros((m, m))\n        np.fill_diagonal(L, 2.0)\n        np.fill_diagonal(L[1:], -1.0)\n        np.fill_diagonal(L[:, 1:], -1.0)\n        return L / (h**2)\n\n    U1 = np.array([1, -1, 1, -1]).reshape(-1, 1)\n    U2 = np.array([0.5, 0.5, -0.5, -0.5]).reshape(-1, 1)\n    V1 = np.array([1, 0, -1, 0]).reshape(-1, 1)\n    V2 = np.array([0.5, -0.5, 0.5, -0.5]).reshape(-1, 1)\n    S = U1 @ V1.T + U2 @ V2.T\n\n    G = np.array([\n        [1, -1, 0, 0],\n        [0, 1, -1, 0]\n    ])\n\n    f = np.ones(n)\n    f_norm = np.linalg.norm(f)\n\n    # --- Main loop over test cases ---\n    for params in test_cases:\n        mu, nu, sigma, eta, beta, c = params.values()\n\n        # Construct parameter-dependent matrices\n        L_nu = construct_L(n_u)\n        L_nb = construct_L(n_b)\n        \n        A_uu = mu * np.identity(n_u) + nu * L_nu\n        A_bb = sigma * np.identity(n_b) + eta * L_nb\n        A_ub = c * S\n        A_bu = -c * S.T\n        \n        # Assemble full matrix A for residual calculation\n        A = np.zeros((n, n))\n        A[s_u, s_u] = A_uu\n        A[s_p, s_u] = G\n        A[s_u, s_p] = G.T\n        A[s_p, s_p] = -beta * np.identity(n_p)\n        A[s_b, s_b] = A_bb\n        A[s_u, s_b] = A_ub\n        A[s_b, s_u] = A_bu\n\n        # --- Ordering (u, p, b) ---\n        u = np.zeros(n_u)\n        p = np.zeros(n_p)\n        b = np.zeros(n_b)\n        \n        iters_upb = -1\n        for k in range(max_iter):\n            # Update u\n            rhs_u = f[s_u] - G.T @ p - A_ub @ b\n            u_new = np.linalg.solve(A_uu, rhs_u)\n            \n            # Update p\n            rhs_p = f[s_p] - G @ u_new\n            p_new = -(1.0/beta) * rhs_p\n            \n            # Update b\n            rhs_b = f[s_b] - A_bu @ u_new\n            b_new = np.linalg.solve(A_bb, rhs_b)\n\n            u, p, b = u_new, p_new, b_new\n            \n            x_k = np.concatenate((u, p, b))\n            rel_res = np.linalg.norm(f - A @ x_k) / f_norm\n            \n            if rel_res < tol:\n                iters_upb = k + 1\n                break\n        \n        results.append(iters_upb)\n\n        # --- Ordering (b, u, p) ---\n        u = np.zeros(n_u)\n        p = np.zeros(n_p)\n        b = np.zeros(n_b)\n\n        iters_bup = -1\n        for k in range(max_iter):\n            # Update b\n            rhs_b = f[s_b] - A_bu @ u\n            b_new = np.linalg.solve(A_bb, rhs_b)\n\n            # Update u\n            rhs_u = f[s_u] - A_ub @ b_new - G.T @ p\n            u_new = np.linalg.solve(A_uu, rhs_u)\n\n            # Update p\n            rhs_p = f[s_p] - G @ u_new\n            p_new = -(1.0/beta) * rhs_p\n\n            u, p, b = u_new, p_new, b_new\n\n            x_k = np.concatenate((u, p, b))\n            rel_res = np.linalg.norm(f - A @ x_k) / f_norm\n            \n            if rel_res < tol:\n                iters_bup = k + 1\n                break\n        \n        results.append(iters_bup)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The Conjugate Gradient (CG) method is the premier solver for the symmetric positive-definite (SPD) systems that arise from discretized self-adjoint operators, such as pure diffusion. However, in realistic fusion scenarios, extreme physical or geometric anisotropy can lead to severe ill-conditioning, crippling CG's performance. This practice problem  requires you to analyze this common issue and apply your knowledge of preconditioning to select a strategy that restores efficient convergence by clustering the system's eigenvalues.",
            "id": "3967060",
            "problem": "A toroidal magnetic confinement simulation in computational fusion science generates a linear system $A x = b$ from a curvilinear finite element discretization of an anisotropic diffusion operator. The physical diffusion tensor is $D(\\mathbf{x}) = \\chi_{\\parallel} \\,\\mathbf{b}(\\mathbf{x}) \\mathbf{b}(\\mathbf{x})^{\\top} + \\chi_{\\perp} \\,(\\mathbf{I} - \\mathbf{b}(\\mathbf{x}) \\mathbf{b}(\\mathbf{x})^{\\top})$, where $\\mathbf{b}(\\mathbf{x})$ is the unit magnetic field direction, $\\chi_{\\parallel}/\\chi_{\\perp} \\approx 10^{6}$, and the mesh mapping $\\mathbf{x}(\\boldsymbol{\\xi})$ has metric tensor $G(\\mathbf{x}) = J(\\boldsymbol{\\xi})^{\\top} J(\\boldsymbol{\\xi})$ with eigenvalue ratio $\\lambda_{\\max}(G)/\\lambda_{\\min}(G) \\approx 10^{3}$. The assembled stiffness matrix $A$ is symmetric positive definite (SPD) and has entries\n$$\nA_{ij} = \\int_{\\Omega} \\left(\\nabla \\phi_{i}(\\mathbf{x})\\right)^{\\top} D(\\mathbf{x}) \\,\\nabla \\phi_{j}(\\mathbf{x}) \\, d\\Omega,\n$$\nwhere $\\{\\phi_{i}\\}$ are standard finite element basis functions. Because the mesh exhibits strong metric anisotropy aligned with $\\mathbf{b}(\\mathbf{x})$, the spectrum of $A$ is widely spread and the Conjugate Gradient (CG) method converges slowly without preconditioning.\n\nStarting from core facts about SPD matrices, eigenvalues, and the relationship between spectral condition number and CG convergence, analyze how anisotropy and metric coefficients affect the eigenvalue distribution of $A$. Then, select the strategy that most directly produces eigenvalue clustering via scaling, improves convergence by reducing the effective condition number, and preserves the mathematical requirements of the Conjugate Gradient (CG) method.\n\nWhich option is the most appropriate?\n\nA. Apply symmetric diagonal scaling using $H = \\operatorname{diag}(A)$ and form the scaled system $\\tilde{A} = H^{-1/2} A H^{-1/2}$, solving for $\\tilde{x}$ with CG and mapping back by $x = H^{-1/2} \\tilde{x}$.\n\nB. Apply only left row scaling using $S = \\operatorname{diag}(A)$ to obtain $\\tilde{A} = S^{-1} A$, and run CG directly on $\\tilde{A}$.\n\nC. Globally scale by the parallel conductivity, replacing $A$ with $\\tilde{A} = \\chi_{\\parallel}^{-1} A$ before running CG.\n\nD. Use an incomplete LU factorization with zero fill-in (ILU($0$)) as a preconditioner in CG, since LU factors equilibrate the system and cluster eigenvalues.",
            "solution": "The problem concerns the solution of a linear system $A x = b$, where the matrix $A$ is symmetric positive definite (SPD) and arises from a finite element discretization of an anisotropic diffusion operator $-\\nabla \\cdot (D(\\mathbf{x}) \\nabla u)$. The convergence rate of the Conjugate Gradient (CG) method for such a system is primarily determined by the spectral condition number of the matrix, $\\kappa(A) = \\lambda_{\\max}(A) / \\lambda_{\\min}(A)$, where $\\lambda_{\\max}$ and $\\lambda_{\\min}$ are the largest and smallest eigenvalues of $A$, respectively. The number of iterations required to reach a certain tolerance is approximately proportional to $\\sqrt{\\kappa(A)}$. A large condition number, corresponding to a wide spread of eigenvalues, leads to slow convergence.\n\nThe problem states that the ill-conditioning of $A$ stems from two sources:\n1.  **Physical Anisotropy**: The diffusion tensor $D(\\mathbf{x})$ has eigenvalues $\\chi_{\\parallel}$ and $\\chi_{\\perp}$ with a ratio $\\chi_{\\parallel}/\\chi_{\\perp} \\approx 10^{6}$. This extreme anisotropy in the physical operator is inherited by the stiffness matrix $A$.\n2.  **Geometric Anisotropy**: The mesh cells are stretched, with the metric tensor $G$ having an eigenvalue ratio $\\lambda_{\\max}(G)/\\lambda_{\\min}(G) \\approx 10^{3}$.\n\nThese factors cause the diagonal entries of the stiffness matrix, $A_{ii} = \\int_{\\Omega} (\\nabla\\phi_i)^\\top D (\\nabla\\phi_i) d\\Omega$, to have widely varying magnitudes. This large variance in the diagonal is a major contributor to the large condition number.\n\nThe task is to select a strategy that (1) uses scaling to cluster eigenvalues, (2) improves convergence by reducing the effective condition number, and (3) preserves the mathematical requirements for CG. The fundamental requirement for the standard CG algorithm is that the system matrix must be symmetric positive definite.\n\nPreconditioning aims to transform the system $A x = b$ into an equivalent one, $\\tilde{A} \\tilde{x} = \\tilde{b}$, where $\\tilde{A}$ has a much smaller condition number. For CG, we must ensure $\\tilde{A}$ remains SPD. A symmetric preconditioner $M$ (which is itself SPD) can be used to construct a symmetrically preconditioned system:\n$$\nM^{-1/2} A M^{-1/2} z = M^{-1/2} b, \\quad x = M^{-1/2} z\n$$\nThe new system matrix is $\\tilde{A} = M^{-1/2} A M^{-1/2}$, which is guaranteed to be SPD if $A$ and $M$ are SPD.\n\nWe now analyze the given options based on these principles.\n\n### Option-by-Option Analysis\n\n**A. Apply symmetric diagonal scaling using $H = \\operatorname{diag}(A)$ and form the scaled system $\\tilde{A} = H^{-1/2} A H^{-1/2}$, solving for $\\tilde{x}$ with CG and mapping back by $x = H^{-1/2} \\tilde{x}$.**\n\nThis method uses a diagonal matrix $H = \\operatorname{diag}(A)$ as the preconditioner $M$. Since $A$ is SPD, its diagonal entries $A_{ii}$ are all positive, so $H$ is SPD and its square root $H^{1/2} = \\operatorname{diag}(\\sqrt{A_{ii}})$ is well-defined and real. The preconditioned matrix is $\\tilde{A} = H^{-1/2} A H^{-1/2}$. This is a symmetric preconditioning scheme. The resulting matrix $\\tilde{A}$ is SPD and thus suitable for the CG method.\nThe entries of $\\tilde{A}$ are $\\tilde{A}_{ij} = A_{ij} / \\sqrt{A_{ii} A_{jj}}$. Crucially, the diagonal entries are $\\tilde{A}_{ii} = A_{ii} / \\sqrt{A_{ii} A_{ii}} = 1$. This method, known as Jacobi preconditioning, directly addresses the problem of large variations in the diagonal entries of $A$ by normalizing them to unity. It is a \"scaling\" method that attempts to cluster eigenvalues around $1$, thereby reducing the effective condition number and improving CG convergence. It satisfies all three requirements of the prompt: it's a scaling method, it improves the condition number, and it preserves the SPD property for CG.\n- Verdict: **Correct**.\n\n**B. Apply only left row scaling using $S = \\operatorname{diag}(A)$ to obtain $\\tilde{A} = S^{-1} A$, and run CG directly on $\\tilde{A}$.**\n\nThis method transforms the system to $S^{-1} A x = S^{-1} b$. The new matrix is $\\tilde{A} = S^{-1} A$. While $A$ is symmetric, $S^{-1}$ (a diagonal matrix) and $A$ do not generally commute. Therefore, the product $\\tilde{A} = S^{-1} A$ is, in general, **not symmetric**. The standard Conjugate Gradient algorithm is defined for symmetric positive definite systems. Applying it to a non-symmetric matrix is mathematically invalid and will likely fail to converge or produce the correct solution. To solve a non-symmetric system, one would need to use a different iterative method, such as GMRES or BiCGSTAB. This option explicitly violates the requirement to \"preserve the mathematical requirements of the Conjugate Gradient (CG) method\".\n- Verdict: **Incorrect**.\n\n**C. Globally scale by the parallel conductivity, replacing $A$ with $\\tilde{A} = \\chi_{\\parallel}^{-1} A$ before running CG.**\n\nThis action scales the entire matrix $A$ by a single constant scalar value $c = \\chi_{\\parallel}^{-1}$. The new matrix is $\\tilde{A} = cA$. If $\\lambda$ is an eigenvalue of $A$, then $c\\lambda$ is an eigenvalue of $\\tilde{A}$. The condition number of the new matrix is:\n$$\n\\kappa(\\tilde{A}) = \\frac{\\lambda_{\\max}(\\tilde{A})}{\\lambda_{\\min}(\\tilde{A})} = \\frac{c \\cdot \\lambda_{\\max}(A)}{c \\cdot \\lambda_{\\min}(A)} = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)} = \\kappa(A)\n$$\nGlobal scalar multiplication does not change the condition number. It merely scales the entire spectrum of eigenvalues, leaving their relative spacing unchanged. Therefore, it has no effect on the convergence rate of the CG method, which depends on the eigenvalue distribution and condition number. This strategy fails to \"improve convergence by reducing the effective condition number\".\n- Verdict: **Incorrect**.\n\n**D. Use an incomplete LU factorization with zero fill-in (ILU($0$)) as a preconditioner in CG, since LU factors equilibrate the system and cluster eigenvalues.**\n\nILU factorization produces an approximation $M = \\tilde{L}\\tilde{U} \\approx A$, where $\\tilde{L}$ is unit lower triangular and $\\tilde{U}$ is upper triangular. If used as a left preconditioner, the system matrix becomes $M^{-1}A = (\\tilde{L}\\tilde{U})^{-1}A$, which is generally not symmetric. This, like option B, makes it unsuitable for the standard CG method. For SPD matrices, the corresponding factorization is the Incomplete Cholesky (IC) factorization, which finds $\\tilde{L}$ such that $A \\approx \\tilde{L}\\tilde{L}^T$. This would provide a symmetric preconditioner $M = \\tilde{L}\\tilde{L}^T$. However, the option explicitly specifies \"incomplete LU\". Furthermore, the prompt asks for a strategy that *most directly* produces clustering *via scaling*. ILU is an approximate inverse method, not a scaling method in the spirit of diagonal scaling. While it is a more powerful preconditioning technique in many cases, it does not fit the specific criteria of the question as well as option A. Standard ILU($0$) is also known to perform poorly for strongly anisotropic problems where the grid is not aligned with the anisotropy.\n- Verdict: **Incorrect**.\n\n### Conclusion\nOption A is the only one that satisfies all the stated criteria. It is a scaling method, it is designed to reduce the condition number by balancing the diagonal, and it uses a symmetric preconditioning approach which maintains the SPD property required by the Conjugate Gradient algorithm.",
            "answer": "$$\n\\boxed{A}\n$$"
        },
        {
            "introduction": "Advection-dominated phenomena, like the crucial $\\mathbf{E}\\times\\mathbf{B}$ drift in tokamak plasmas, lead to linear systems with highly non-normal matrices where simple eigenvalue analysis fails to predict solver behavior. The Generalized Minimal Residual (GMRES) method is the standard tool for these systems, but its practical, restarted form—GMRES($m$)—can suffer from severe convergence stagnation. This exercise  delves into the reasons for this stagnation, connecting it to the matrix's pseudospectrum and exploring advanced strategies to overcome it.",
            "id": "3966993",
            "problem": "In an implicit time advance of a drift-kinetic model for magnetized plasma in a tokamak edge, suppose one must solve a preconditioned linear system at each step,\n$$\nM^{-1} A x = M^{-1} b,\n$$\nwhere $A = I + \\Delta t\\,L$, $I$ is the identity, $\\Delta t > 0$ is the time step, and $L$ is a discrete operator dominated by electric-cross-magnetic ($\\mathbf{E}\\times\\mathbf{B}$) drift advection along flux surfaces, with weak perpendicular collisional diffusion. Due to upwinding, grid metric couplings, and boundary conditions, $L$ is strongly non-normal. The preconditioner $M$ is block-diagonal and physics-informed, clustering the eigenvalues of $M^{-1}A$ near $1$, but $M^{-1}A$ remains non-normal with a broad $\\varepsilon$-pseudospectrum that approaches $0$.\n\nYou are asked to use Generalized Minimal Residual (GMRES) with restarts, denoted GMRES($m$). Recall the following foundational principles and definitions:\n- GMRES constructs the $j$-step Krylov subspace $\\mathcal{K}_j(M^{-1}A,r_0) = \\mathrm{span}\\{r_0,(M^{-1}A)r_0,\\dots,(M^{-1}A)^{j-1}r_0\\}$, where $r_0 = M^{-1}(b - A x_0)$, and picks $x_j$ that minimizes the residual norm over $x_0 + \\mathcal{K}_j$.\n- Equivalently, $r_j = p_j(M^{-1}A)\\,r_0$ for some polynomial $p_j$ of degree at most $j$ with $p_j(0)=1$.\n- For non-normal matrices, eigenvalues alone do not determine convergence; the behavior of $\\|p(M^{-1}A)\\|$ can be governed by the field of values and the pseudospectrum, allowing transient amplification of certain directions even when eigenvalues are favorably located.\n\nConsider the effect of restarting on convergence in this setting and the problem of choosing $m$ to balance memory and convergence under a fixed memory budget for storing Krylov basis vectors. Select all statements that are most accurate.\n\nA. Restarting GMRES($m$) on $M^{-1}A$ that is highly non-normal (as in discrete $\\mathbf{E}\\times\\mathbf{B}$ advection) can cause severe stagnation because each cycle restricts the residual to a polynomial of degree at most $m$; the loss of the long polynomial filter across restarts allows transiently amplified components to re-enter. Increasing $m$ reduces this effect up to the point allowed by memory.\n\nB. For such problems, convergence of GMRES($m$) is governed solely by the location of the eigenvalues of $M^{-1}A$; if they lie in the open right half-plane, rapid convergence occurs even for small $m$ regardless of non-normality.\n\nC. A balanced choice of $m$ can be made adaptively: estimate the per-cycle residual reduction factor $\\|r^{(j+1)}\\|/\\|r^{(j)}\\|$ during the first cycle, and increase $m$ until this factor falls below a target $\\rho<1$ while ensuring that storing $m$ Krylov vectors of length $n$ under a memory budget $B$ bytes (approximately $8 n m$ bytes in double precision) is respected; if the target is not met at the budget limit, augment with deflated restarts or stronger preconditioning.\n\nD. Because discrete $\\mathbf{E}\\times\\mathbf{B}$ advection is nearly skew-symmetric, GMRES without restart would risk breakdown, so restarting is required to maintain stability and actually improves convergence when $m$ is smaller.\n\nE. When non-normality is strong, carrying over $k$ harmonic Ritz vectors across restarts (deflated restarting) preserves approximate invariant subspaces associated with slowly damped modes; thus a moderate $m$ with $k \\ll m$ can achieve good convergence subject to the same memory budget.\n\nF. The orthogonalization cost per iteration in GMRES($m$) is independent of $m$, so increasing $m$ improves convergence without affecting per-iteration cost; hence one should always choose the largest possible $m$.",
            "solution": "The core of the problem lies in the behavior of the GMRES algorithm when applied to a highly non-normal matrix $C = M^{-1}A$. The convergence of GMRES is dictated by its ability to find a low-degree polynomial $p_j$ (with $p_j(0)=1$) that minimizes the norm of the residual $r_j = p_j(C)r_0$. The residual norm is bounded by $\\|r_j\\| \\le \\|p_j(C)\\| \\|r_0\\|$.\n\nFor a normal matrix, the operator norm of the polynomial is simply the maximum value of the polynomial on the spectrum: $\\|p_j(C)\\| = \\max_{\\lambda \\in \\Lambda(C)} |p_j(\\lambda)|$. Since the problem states eigenvalues of $C$ are clustered near $1$, a low-degree polynomial could easily be constructed to be small in this cluster, ensuring rapid convergence.\n\nHowever, $C$ is strongly non-normal. For such matrices, $\\|p_j(C)\\|$ can be vastly larger than $\\max_{\\lambda \\in \\Lambda(C)} |p_j(\\lambda)|$. The behavior of $\\|p_j(C)\\|$ is better characterized by the pseudospectrum, $\\Lambda_{\\varepsilon}(C)$. The problem states that $\\Lambda_{\\varepsilon}(C)$ is broad and approaches $0$. This means there exists a direction (a vector $v$) that is amplified significantly by $C$ initially, even though all eigenvalues might have magnitudes near $1$. This phenomenon is known as transient growth. For GMRES to converge, the polynomial $p_j(z)$ must be small not just on the eigenvalues, but over a significant portion of the pseudospectrum. Suppressing a function over a large, complex region of the complex plane generally requires a high-degree polynomial.\n\nRestarted GMRES, GMRES($m$), limits the polynomial degree to at most $m$. If $m$ is too small, the polynomial $p_m(z)$ may not be able to effectively damp the components of the residual corresponding to the pseudospectrum far from the eigenvalues. After the restart, the search for a good polynomial begins anew from a limited $m$ dimensional space, and the transiently amplified components can re-emerge. This can lead to stagnation, where each restart cycle achieves only a minor reduction in the residual norm.\n\n### Option-by-Option Analysis\n\n**A. Restarting GMRES($m$) on $M^{-1}A$ that is highly non-normal (as in discrete $\\mathbf{E}\\times\\mathbf{B}$ advection) can cause severe stagnation because each cycle restricts the residual to a polynomial of degree at most $m$; the loss of the long polynomial filter across restarts allows transiently amplified components to re-enter. Increasing $m$ reduces this effect up to the point allowed by memory.**\n\nThis statement is a precise and accurate description of the failure mode of restarted GMRES for non-normal problems. The key concepts are correctly identified: the limitation of the polynomial degree to $m$, the loss of information (the \"long polynomial filter\") at restart, the re-emergence of transiently amplified components, and the resulting convergence stagnation. It also correctly states that increasing $m$ is the primary way to combat this, as it allows for a higher-degree polynomial filter within each cycle.\n- Verdict: **Correct**\n\n**B. For such problems, convergence of GMRES($m$) is governed solely by the location of the eigenvalues of $M^{-1}A$; if they lie in the open right half-plane, rapid convergence occurs even for small $m$ regardless of non-normality.**\n\nThis statement is fundamentally flawed. A central theme in modern numerical linear algebra is that for non-normal matrices, eigenvalue information alone is insufficient and often misleading for predicting iterative solver convergence. The problem statement itself notes this: \"For non-normal matrices, eigenvalues alone do not determine convergence\". The phenomenon of a well-conditioned spectrum coexisting with poor GMRES convergence is a classic example of non-normality at play.\n- Verdict: **Incorrect**\n\n**C. A balanced choice of $m$ can be made adaptively: estimate the per-cycle residual reduction factor $\\|r^{(j+1)}\\|/\\|r^{(j)}\\|$ during the first cycle, and increase $m$ until this factor falls below a target $\\rho<1$ while ensuring that storing $m$ Krylov vectors of length $n$ under a memory budget $B$ bytes (approximately $8 n m$ bytes in double precision) is respected; if the target is not met at the budget limit, augment with deflated restarts or stronger preconditioning.**\n\nThis statement describes a completely reasonable and practical strategy for dealing with the challenges of GMRES($m$). It correctly identifies the trade-off between convergence and resources. It proposes an adaptive method for choosing $m$ based on observed convergence rate. The memory cost is correctly estimated as being linear in $m$ and the vector size $n$ (a double-precision floating-point number is $8$ bytes, so $m$ vectors of size $n$ require $8nm$ bytes). Finally, it correctly suggests advanced strategies like deflated restarts or improving the preconditioner when simply increasing $m$ is not feasible. This is a sound engineering approach to the problem.\n- Verdict: **Correct**\n\n**D. Because discrete $\\mathbf{E}\\times\\mathbf{B}$ advection is nearly skew-symmetric, GMRES without restart would risk breakdown, so restarting is required to maintain stability and actually improves convergence when $m$ is smaller.**\n\nThis statement is incorrect for several reasons. First, a breakdown in the Arnoldi process (the core of GMRES) is not a generic risk that necessitates restarting; it is a specific event that occurs when an exact solution is found within the Krylov subspace. Restarting is primarily motivated by memory and computational cost limitations, not stability against breakdown. Second, and more importantly, restarting *harms* convergence for strongly non-normal problems, it does not improve it. This directly contradicts the principle explained in the analysis for option A. A larger $m$ (leading to full GMRES in the limit) is almost always better for convergence speed, albeit more expensive.\n- Verdict: **Incorrect**\n\n**E. When non-normality is strong, carrying over $k$ harmonic Ritz vectors across restarts (deflated restarting) preserves approximate invariant subspaces associated with slowly damped modes; thus a moderate $m$ with $k \\ll m$ can achieve good convergence subject to the same memory budget.**\n\nThis statement accurately describes an advanced technique, often called GMRES-DR, specifically designed to address the stagnation of restarted GMRES. The \"slowly damped modes\" correspond to the parts of the pseudospectrum near the origin that require a high-degree polynomial to suppress. Harmonic Ritz vectors are good at approximating the eigenvectors associated with eigenvalues of smallest magnitude. By augmenting the Krylov subspace with these vectors from previous cycles, the algorithm retains crucial information and does not need to \"re-learn\" how to handle these problematic components in every cycle. This allows for good convergence with a smaller restart parameter $m$, making it a very effective strategy.\n- Verdict: **Correct**\n\n**F. The orthogonalization cost per iteration in GMRES($m$) is independent of $m$, so increasing $m$ improves convergence without affecting per-iteration cost; hence one should always choose the largest possible $m$.**\n\nThe premise of this statement is false. In the Arnoldi process within GMRES, at iteration $j$ (for $j=1, \\dots, m$), the newly generated vector must be orthogonalized against all $j$ previous basis vectors. Using an algorithm like Modified Gram-Schmidt, this requires $j$ inner products and $j$ vector updates, a cost of $O(jn)$, where $n$ is the vector size. Thus, the per-iteration cost is *not* independent of $m$; it grows linearly as the cycle progresses. The total cost of one full GMRES($m$) cycle is $O(m^2n)$ for the orthogonalizations, plus the cost of $m$ matrix-vector products. Therefore, increasing $m$ significantly increases the cost per cycle and the memory usage. The choice of $m$ is a trade-off between convergence rate and resource consumption.\n- Verdict: **Incorrect**",
            "answer": "$$\n\\boxed{ACE}\n$$"
        }
    ]
}