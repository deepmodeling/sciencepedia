{
    "hands_on_practices": [
        {
            "introduction": "To correct magnetic error fields, we must first accurately measure them. In toroidal fusion devices, these fields are inherently periodic in the poloidal and toroidal directions, making Fourier analysis the natural language for their description. This first exercise  provides hands-on practice in bridging the gap between the continuous mathematical definition of a Fourier harmonic and its practical estimation from a finite set of discrete, and potentially noisy, sensor measurements. You will implement and compare estimators for both uniformly-gridded data and randomly-spaced data, building a foundational skill for experimental data analysis.",
            "id": "3976225",
            "problem": "Consider a toroidal magnetic confinement device with a rational surface at radius $r_s$ such that the safety factor $q(r_s)$ satisfies $q(r_s)=2/1$. On this surface, the radial magnetic field perturbation $b_r(\\theta,\\phi;r_s)$ is a doubly periodic function of the poloidal angle $\\theta\\in[0,2\\pi)$ and the toroidal angle $\\phi\\in[0,2\\pi)$, both specified in radians. The resonant complex Fourier amplitude associated with the $m/n=2/1$ helical symmetry, denoted $b_{21}(r_s)$, is defined in terms of the doubly periodic function $b_r(\\theta,\\phi;r_s)$ using the fundamental complex Fourier series over the poloidal-toroidal angle domain. The discrete magnetic probe measurements are uniformly distributed in $\\theta$ and $\\phi$ on that surface, and you must compute $b_{21}(r_s)$ directly from those discrete samples without any external inputs. Use a derivation that starts from the foundational definition of complex Fourier coefficients for periodic functions over a two-dimensional angular domain, and apply a scientifically sound discretization consistent with uniform sampling.\n\nYour task is to write a self-contained program that synthesizes magnetic probe data for three specified test cases, computes the complex resonant Fourier amplitude $b_{21}(r_s)$ for each case from the synthesized discrete measurements, and outputs, for each case, the magnitude $\\lvert b_{21}(r_s)\\rvert$ in Tesla and the phase $\\arg\\left(b_{21}(r_s)\\right)$ in radians. The final program must produce these results as a single line containing a comma-separated list enclosed in square brackets, with the sequence ordered as $[\\lvert b_{21}\\rvert_1,\\arg(b_{21})_1,\\lvert b_{21}\\rvert_2,\\arg(b_{21})_2,\\lvert b_{21}\\rvert_3,\\arg(b_{21})_3]$.\n\nThe synthesized data and test suite are as follows. In all cases, angles are in radians and magnetic field units are Tesla.\n\n- Test case $1$ (general case, uniform grid, single harmonic):\n  - Grid resolution: $N_\\theta=64$, $N_\\phi=64$.\n  - Field model: $b_r(\\theta,\\phi;r_s)=A_{21}\\cos(m\\theta-n\\phi+\\delta_{21})$ with $m=2$ and $n=1$.\n  - Amplitude and phase: $A_{21}=5\\times 10^{-5}\\,\\mathrm{T}$, $\\delta_{21}=1.2$.\n  - No other harmonics or noise are present.\n\n- Test case $2$ (mixture of harmonics with measurement noise, uniform grid):\n  - Grid resolution: $N_\\theta=48$, $N_\\phi=32$.\n  - Field model: $b_r(\\theta,\\phi;r_s)=A_{21}\\cos(2\\theta-\\phi+\\delta_{21})+A_{11}\\cos(1\\theta-1\\phi+\\delta_{11})+\\eta(\\theta,\\phi)$, where $\\eta$ is zero-mean Gaussian noise.\n  - Amplitudes and phases: $A_{21}=8\\times 10^{-6}\\,\\mathrm{T}$, $\\delta_{21}=-0.5$; $A_{11}=3\\times 10^{-5}\\,\\mathrm{T}$, $\\delta_{11}=2.0$.\n  - Noise standard deviation: $\\sigma_\\eta=1\\times 10^{-6}\\,\\mathrm{T}$.\n  - Use a fixed random seed $12345$ for generating $\\eta$.\n\n- Test case $3$ (boundary sampling case, random uniform points):\n  - Number of probes: $N=12$.\n  - Probe positions: $\\theta_k$ and $\\phi_k$ independently and uniformly distributed in $[0,2\\pi)$.\n  - Field model: $b_r(\\theta,\\phi;r_s)=A_{21}\\cos(2\\theta-\\phi+\\delta_{21})$ with $A_{21}=4\\times 10^{-5}\\,\\mathrm{T}$ and $\\delta_{21}=-2.3$.\n  - Use a fixed random seed $2024$ for generating the random probe angles.\n\nScientific and numerical requirements:\n- Base your computation on the fundamental complex Fourier coefficient definition for a two-dimensional periodic function on the domain $\\theta\\in[0,2\\pi)$, $\\phi\\in[0,2\\pi)$, and use a discretization consistent with uniform sampling in $\\theta$ and $\\phi$.\n- Ensure the estimator is unbiased under uniform sampling and discuss any aliasing constraints in your derivation.\n- Output the magnitude in $\\mathrm{T}$ and the phase in radians for each test case as real numbers.\n\nFinal output format specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[\\lvert b_{21}\\rvert_1,\\arg(b_{21})_1,\\lvert b_{21}\\rvert_2,\\arg(b_{21})_2,\\lvert b_{21}\\rvert_3,\\arg(b_{21})_3]$, where each entry is a float. No other text should be printed.",
            "solution": "The problem requires the computation of the complex resonant Fourier amplitude $b_{21}(r_s)$ of a radial magnetic field perturbation $b_r(\\theta, \\phi; r_s)$ on a rational magnetic surface in a toroidal device. The indices $(m,n)=(2,1)$ correspond to the poloidal ($m$) and toroidal ($n$) mode numbers, respectively. The computation must be derived from the fundamental definition of a complex Fourier series and applied to discrete data sets.\n\n### Theoretical Foundation: Continuous Fourier Series\n\nA doubly periodic function $f(\\theta, \\phi)$ on the domain $(\\theta, \\phi) \\in [0, 2\\pi) \\times [0, 2\\pi)$ can be represented by a complex Fourier series. In the context of helical structures in toroidal plasmas, it is conventional to use basis functions of the form $e^{i(m\\theta - n\\phi)}$. The corresponding complex Fourier coefficient, herein denoted $b_{mn}$, is defined by the projection integral:\n$$\nb_{mn} = \\frac{1}{(2\\pi)^2} \\int_0^{2\\pi} \\int_0^{2\\pi} b_r(\\theta, \\phi) e^{-i(m\\theta - n\\phi)} \\, d\\theta \\, d\\phi\n$$\nThis definition ensures the orthogonality of the basis functions over the specified domain.\n\nTo establish a ground truth for the test cases, we evaluate this integral for a single, pure helical mode given by $b_r(\\theta, \\phi) = A_{mn} \\cos(m\\theta - n\\phi + \\delta_{mn})$. Using Euler's formula, $\\cos(x) = \\frac{1}{2}(e^{ix} + e^{-ix})$, we substitute this into the integral:\n$$\nb_{mn} = \\frac{1}{(2\\pi)^2} \\int_0^{2\\pi} \\int_0^{2\\pi} \\left[ \\frac{A_{mn}}{2} \\left( e^{i(m\\theta - n\\phi + \\delta_{mn})} + e^{-i(m\\theta - n\\phi + \\delta_{mn})} \\right) \\right] e^{-i(m\\theta - n\\phi)} \\, d\\theta \\, d\\phi\n$$\n$$\nb_{mn} = \\frac{A_{mn}}{2(2\\pi)^2} \\int_0^{2\\pi} \\int_0^{2\\pi} \\left( e^{i\\delta_{mn}} + e^{-i(2(m\\theta - n\\phi) + \\delta_{mn})} \\right) \\, d\\theta \\, d\\phi\n$$\nDue to the orthogonality property of complex exponentials, the integral of the second term over the periodic domain is zero (for non-zero $m, n$). The first term is a constant, leading to:\n$$\nb_{mn} = \\frac{A_{mn}}{2(2\\pi)^2} e^{i\\delta_{mn}} \\int_0^{2\\pi} \\int_0^{2\\pi} d\\theta \\, d\\phi = \\frac{A_{mn}}{2(2\\pi)^2} e^{i\\delta_{mn}} (2\\pi)^2 = \\frac{A_{mn}}{2} e^{i\\delta_{mn}}\n$$\nThus, for a pure cosine mode of amplitude $A_{mn}$ and phase $\\delta_{mn}$, the corresponding complex Fourier coefficient has a magnitude of $|b_{mn}| = A_{mn}/2$ and a phase of $\\arg(b_{mn}) = \\delta_{mn}$. This provides the expected theoretical result.\n\n### Numerical Estimation from Discrete Data\n\nIn practice, $b_r$ is known only at a finite number of discrete points. The continuous integral must be approximated by a discrete sum. The form of this approximation depends on the sampling strategy.\n\n#### Case 1 & 2: Uniform Grid Sampling\n\nFor measurements taken on a uniform grid of $N_\\theta \\times N_\\phi$ points, where $\\theta_j = j \\frac{2\\pi}{N_\\theta}$ for $j \\in \\{0, \\dots, N_\\theta-1\\}$ and $\\phi_k = k \\frac{2\\pi}{N_\\phi}$ for $k \\in \\{0, \\dots, N_\\phi-1\\}$, the integral can be approximated by a Riemann sum. The differential area element $d\\theta d\\phi$ is replaced by the discrete area element $\\Delta\\theta \\Delta\\phi = \\frac{2\\pi}{N_\\theta} \\frac{2\\pi}{N_\\phi} = \\frac{(2\\pi)^2}{N_\\theta N_\\phi}$.\n\nSubstituting this into the definition of $b_{mn}$:\n$$\nb_{mn} \\approx \\frac{1}{(2\\pi)^2} \\sum_{j=0}^{N_\\theta-1} \\sum_{k=0}^{N_\\phi-1} b_r(\\theta_j, \\phi_k) e^{-i(m\\theta_j - n\\phi_k)} \\frac{(2\\pi)^2}{N_\\theta N_\\phi}\n$$\nThis expression simplifies to the formula for the 2D Discrete Fourier Transform (DFT), scaled appropriately:\n$$\nb_{mn} \\approx \\frac{1}{N_\\theta N_\\phi} \\sum_{j=0}^{N_\\theta-1} \\sum_{k=0}^{N_\\phi-1} b_r(\\theta_j, \\phi_k) e^{-i(m\\theta_j - n\\phi_k)}\n$$\nThis estimator is exact for band-limited signals if the sampling rates satisfy the Nyquist-Shannon sampling theorem, which requires $N_\\theta > 2|m_{\\text{max}}|$ and $N_\\phi > 2|n_{\\text{max}}|$. For Case 1, we have $(m,n)=(2,1)$ with $N_\\theta=64, N_\\phi=64$, which satisfies $64 > 4$ and $64 > 2$. For Case 2, the modes are $(2,1)$ and $(1,1)$, so $|m_{\\text{max}}|=2, |n_{\\text{max}}|=1$. The grid $N_\\theta=48, N_\\phi=32$ also satisfies the criterion ($48 > 4$, $32 > 2$), so aliasing is not an issue. The orthogonality of the discrete Fourier basis ensures that the $(1,1)$ mode does not interfere with the calculation of the $(2,1)$ coefficient. The zero-mean noise term in Case 2 will introduce a statistical error into the estimate.\n\n#### Case 3: Random Uniform Sampling\n\nFor $N$ measurements taken at positions $(\\theta_k, \\phi_k)$ drawn independently from a uniform random distribution over $[0, 2\\pi) \\times [0, 2\\pi)$, we employ a Monte Carlo integration approach. The integral defining $b_{mn}$ can be interpreted as an expectation of the function $g(\\theta, \\phi) = b_r(\\theta, \\phi) e^{-i(m\\theta - n\\phi)}$ over the domain, scaled by the domain's area, which is $(2\\pi)^2$.\n$$\nb_{mn} = \\frac{1}{(2\\pi)^2} \\int_0^{2\\pi} \\int_0^{2\\pi} g(\\theta, \\phi) \\,d\\theta d\\phi = E[g(\\theta, \\phi)]\n$$\nwhere the expectation is taken with respect to the uniform probability measure on the domain. By the law of large numbers, this expectation can be estimated by the sample mean of the function evaluated at the $N$ random points:\n$$\nb_{mn} \\approx \\frac{1}{N} \\sum_{k=1}^{N} g(\\theta_k, \\phi_k) = \\frac{1}{N} \\sum_{k=1}^{N} b_r(\\theta_k, \\phi_k) e^{-i(m\\theta_k - n\\phi_k)}\n$$\nThis Monte Carlo estimator is unbiased, meaning its expected value is the true $b_{mn}$. However, for a finite number of samples $N$, especially a small one like $N=12$, the estimate will have a significant variance, and thus a statistical error compared to the true value.\n\n### Application to Test Cases\n\nThe program will implement these estimators for $(m,n)=(2,1)$.\n- **Test Case 1:** A pure $(2,1)$ mode on a fine uniform grid. The uniform grid estimator is used. The result should be extremely close to the theoretical value $|b_{21}| = A_{21}/2 = 2.5 \\times 10^{-5}$ T and $\\arg(b_{21}) = \\delta_{21} = 1.2$ rad.\n- **Test Case 2:** A mixture of $(2,1)$ and $(1,1)$ modes with noise on a uniform grid. The uniform grid estimator is used. Due to discrete orthogonality, the $(1,1)$ mode does not contribute. The result will be close to the theoretical $(2,1)$ value ($|b_{21}| = A_{21}/2 = 4.0 \\times 10^{-6}$ T, $\\arg(b_{21}) = \\delta_{21} = -0.5$ rad), with a small deviation caused by the realization of the random noise.\n- **Test Case 3:** A pure $(2,1)$ mode sampled at a small number of random points. The random sample estimator is used. The result will approximate the theoretical value ($|b_{21}| = A_{21}/2 = 2.0 \\times 10^{-5}$ T, $\\arg(b_{21}) = \\delta_{21} = -2.3$ rad), but with a noticeable statistical error due to the small sample size $N=12$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the complex resonant Fourier amplitude b_21 for three test cases.\n    \"\"\"\n    m, n = 2, 1\n    results = []\n\n    # --- Test Case 1: General case, uniform grid, single harmonic ---\n    N_theta_1, N_phi_1 = 64, 64\n    A_21_1, delta_21_1 = 5e-5, 1.2\n    \n    # Generate uniform angular grids\n    theta_1_vals = np.linspace(0, 2 * np.pi, N_theta_1, endpoint=False)\n    phi_1_vals = np.linspace(0, 2 * np.pi, N_phi_1, endpoint=False)\n    theta_1, phi_1 = np.meshgrid(theta_1_vals, phi_1_vals, indexing='ij')\n\n    # Synthesize magnetic field data\n    b_r_1 = A_21_1 * np.cos(m * theta_1 - n * phi_1 + delta_21_1)\n    \n    # Compute the complex Fourier coefficient b_21\n    # Estimator: (1/N_theta*N_phi) * sum(b_r * exp(-i*(m*theta - n*phi)))\n    # This is equivalent to np.mean(b_r * kernel)\n    kernel_1 = np.exp(-1j * (m * theta_1 - n * phi_1))\n    b_21_1 = np.mean(b_r_1 * kernel_1)\n\n    mag_1 = np.abs(b_21_1)\n    phase_1 = np.angle(b_21_1)\n    results.extend([mag_1, phase_1])\n    \n    # --- Test Case 2: Mixture of harmonics with noise, uniform grid ---\n    N_theta_2, N_phi_2 = 48, 32\n    A_21_2, delta_21_2 = 8e-6, -0.5\n    A_11_2, delta_11_2 = 3e-5, 2.0\n    sigma_eta_2, seed_2 = 1e-6, 12345\n    \n    # Generate uniform angular grids\n    theta_2_vals = np.linspace(0, 2 * np.pi, N_theta_2, endpoint=False)\n    phi_2_vals = np.linspace(0, 2 * np.pi, N_phi_2, endpoint=False)\n    theta_2, phi_2 = np.meshgrid(theta_2_vals, phi_2_vals, indexing='ij')\n\n    # Synthesize magnetic field data with two modes and noise\n    b_r_2_mode1 = A_21_2 * np.cos(m * theta_2 - n * phi_2 + delta_21_2)\n    b_r_2_mode2 = A_11_2 * np.cos(1 * theta_2 - 1 * phi_2 + delta_11_2)\n    \n    # Generate reproducible Gaussian noise\n    rng_2 = np.random.default_rng(seed_2)\n    noise_2 = rng_2.normal(0, sigma_eta_2, size=(N_theta_2, N_phi_2))\n    \n    b_r_2 = b_r_2_mode1 + b_r_2_mode2 + noise_2\n    \n    # Compute the complex Fourier coefficient b_21\n    kernel_2 = np.exp(-1j * (m * theta_2 - n * phi_2))\n    b_21_2 = np.mean(b_r_2 * kernel_2)\n    \n    mag_2 = np.abs(b_21_2)\n    phase_2 = np.angle(b_21_2)\n    results.extend([mag_2, phase_2])\n\n    # --- Test Case 3: Boundary sampling case, random uniform points ---\n    N_3 = 12\n    A_21_3, delta_21_3 = 4e-5, -2.3\n    seed_3 = 2024\n\n    # Generate random uniform probe positions\n    rng_3 = np.random.default_rng(seed_3)\n    theta_3 = rng_3.uniform(0, 2 * np.pi, N_3)\n    phi_3 = rng_3.uniform(0, 2 * np.pi, N_3)\n\n    # Synthesize magnetic field data at random points\n    b_r_3 = A_21_3 * np.cos(m * theta_3 - n * phi_3 + delta_21_3)\n    \n    # Compute the complex Fourier coefficient b_21 using Monte Carlo estimator\n    # Estimator: (1/N) * sum(b_r * exp(-i*(m*theta - n*phi)))\n    kernel_3 = np.exp(-1j * (m * theta_3 - n * phi_3))\n    b_21_3 = np.mean(b_r_3 * kernel_3)\n    \n    mag_3 = np.abs(b_21_3)\n    phase_3 = np.angle(b_21_3)\n    results.extend([mag_3, phase_3])\n\n    # Print the final output in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once we can measure error fields, the next step is to build a predictive model of how our control coils affect the magnetic landscape. This exercise  guides you through the process of constructing a linear response model, a cornerstone of modern feedback control systems in fusion devices. You will move beyond simple model fitting to rigorously assess your model's ability to generalize to new, unseen data using the crucial technique of $K$-fold cross-validation. This practice will build your intuition for diagnosing and preventing overfitting, ensuring the development of robust and reliable predictive models.",
            "id": "3976233",
            "problem": "You are tasked with building a complete program that estimates linear response operators for magnetic error field correction in a fusion-relevant setting and evaluates generalization performance through cross-validation. The setting is as follows. In a set of independent experiments indexed by $k \\in \\{1,\\dots,N\\}$, a sensor array of size $S$ records a vector $y_k \\in \\mathbb{R}^S$ of dimensionless magnetic perturbation measurements arising from two sources: applied control coil currents $u_k \\in \\mathbb{R}^A$ and applied proxy error-field patterns $w_k \\in \\mathbb{R}^M$. The relationship is assumed linear by superposition of fields for small perturbations, consistent with Maxwell's equations in the magnetoquasistatic regime, so that\n$$\ny_k = R\\,u_k + G\\,w_k + \\varepsilon_k,\n$$\nwhere $R \\in \\mathbb{R}^{S \\times A}$ and $G \\in \\mathbb{R}^{S \\times M}$ are unknown response operators to be identified, and $\\varepsilon_k \\sim \\mathcal{N}(0,\\sigma^2 I_S)$ is independent Gaussian measurement noise with known variance $\\sigma^2$ and identity matrix $I_S$. All quantities in this problem are dimensionless.\n\nYou must implement the following computational tasks purely in mathematical terms:\n\n1. Data generation model. For a given random seed $s$, generate $R_{\\text{true}} \\in \\mathbb{R}^{S \\times A}$ and $G_{\\text{true}} \\in \\mathbb{R}^{S \\times M}$ by sampling independent Gaussian entries with zero mean and standard deviations $1/\\sqrt{A}$ and $1/\\sqrt{M}$ respectively. Then, for $k=1,\\dots,N$, generate $u_k \\sim \\mathcal{N}(0, I_A)$ and $w_k \\sim \\mathcal{N}(0, I_M)$ independently, and set\n$$\ny_k = R_{\\text{true}} u_k + G_{\\text{true}} w_k + \\varepsilon_k,\\quad \\varepsilon_k \\sim \\mathcal{N}(0,\\sigma^2 I_S).\n$$\nStack the inputs $x_k \\in \\mathbb{R}^{A+M}$ as $x_k = \\begin{bmatrix} u_k \\\\ w_k \\end{bmatrix}$ to form $X \\in \\mathbb{R}^{(A+M)\\times N}$ with columns $x_k$, and stack the outputs to form $Y \\in \\mathbb{R}^{S \\times N}$ with columns $y_k$. This corresponds to the multi-output linear model\n$$\nY \\approx B X,\\quad B = \\begin{bmatrix} R & G \\end{bmatrix} \\in \\mathbb{R}^{S \\times (A+M)}.\n$$\n\n2. Estimation by Tikhonov regularization. Given a regularization parameter $\\lambda \\ge 0$, estimate $B$ from a training subset by solving the ridge regression (also called Tikhonov regularization) problem that minimizes the Frobenius-norm penalized least-squares objective\n$$\n\\min_{B \\in \\mathbb{R}^{S \\times (A+M)}} \\sum_{k \\in \\mathcal{T}} \\lVert y_k - B x_k \\rVert_2^2 + \\lambda \\lVert B \\rVert_F^2,\n$$\nwhere $\\mathcal{T}$ indexes the training columns. Implement the closed-form solution\n$$\n\\widehat{B} = Y_{\\mathcal{T}} X_{\\mathcal{T}}^{\\top}\\, \\left(X_{\\mathcal{T}} X_{\\mathcal{T}}^{\\top} + \\lambda I_{A+M}\\right)^{-1},\n$$\nwith $I_{A+M}$ the identity matrix of size $(A+M)\\times(A+M)$ and $X_{\\mathcal{T}}$ and $Y_{\\mathcal{T}}$ the submatrices of $X$ and $Y$ containing the training columns.\n\n3. $K$-fold cross-validation. Partition the $N$ experiments into $K$ folds of as equal size as possible using a deterministic permutation determined by the seed $s$. For each fold $f \\in \\{1,\\dots,K\\}$, let the validation set $\\mathcal{V}_f$ be the $f$-th fold and the training set $\\mathcal{T}_f$ be its complement. Fit $\\widehat{B}_f$ on $\\mathcal{T}_f$ using the formula above. Compute the mean squared error on the training and validation sets as\n$$\n\\operatorname{MSE}_{\\text{train},f} = \\frac{1}{|\\mathcal{T}_f|\\,S} \\sum_{k \\in \\mathcal{T}_f} \\lVert y_k - \\widehat{B}_f x_k \\rVert_2^2,\\quad\n\\operatorname{MSE}_{\\text{val},f} = \\frac{1}{|\\mathcal{V}_f|\\,S} \\sum_{k \\in \\mathcal{V}_f} \\lVert y_k - \\widehat{B}_f x_k \\rVert_2^2.\n$$\nReport the cross-validated mean squared error as\n$$\n\\overline{\\operatorname{MSE}}_{\\text{val}} = \\frac{1}{K} \\sum_{f=1}^{K} \\operatorname{MSE}_{\\text{val},f}.\n$$\nAlso compute the average training error $\\overline{\\operatorname{MSE}}_{\\text{train}} = \\frac{1}{K} \\sum_{f=1}^{K} \\operatorname{MSE}_{\\text{train},f}$.\n\n4. Overfitting risk indicator. Define the parameter count $P = S\\,(A+M)$, the average number of training samples per fold $N_{\\text{train,avg}} = N \\,(K-1)/K$, and the complexity ratio\n$$\nr = \\frac{P}{N_{\\text{train,avg}}\\,S} = \\frac{S\\,(A+M)}{N \\,(K-1)/K \\cdot S} = \\frac{K\\,(A+M)}{N\\,(K-1)}.\n$$\nClassify an overfitting risk indicator $O \\in \\{\\text{True},\\text{False}\\}$ according to the rule\n$$\nO = \\left(\\frac{\\overline{\\operatorname{MSE}}_{\\text{val}}}{\\overline{\\operatorname{MSE}}_{\\text{train}}} \\ge 1.5\\right) \\land \\left(r \\ge 0.6\\right),\n$$\nwhere $\\land$ denotes logical conjunction.\n\n5. Output format. Your program must aggregate the results for each test case into a single line of output, formatted as a list of two-element lists, one per test case, with each inner list containing the cross-validated mean squared error as a floating-point number and the overfitting indicator as a boolean. For example, the output must look like\n$$\n[[m_1,b_1],[m_2,b_2],[m_3,b_3]],\n$$\nwhere $m_i$ are numeric floats and $b_i \\in \\{\\mathrm{True},\\mathrm{False}\\}$.\n\nTest suite. Implement and run your program on the following three test cases using the exact parameters provided. All quantities are dimensionless. Each case uses $K=5$ folds.\n\n- Case $1$: $N=200$, $S=8$, $A=3$, $M=2$, $\\sigma=0.05$, $\\lambda=10^{-6}$, seed $s=1$.\n- Case $2$: $N=30$, $S=8$, $A=10$, $M=8$, $\\sigma=0.08$, $\\lambda=10^{-6}$, seed $s=2$.\n- Case $3$: $N=15$, $S=6$, $A=8$, $M=8$, $\\sigma=0.05$, $\\lambda=10^{-3}$, seed $s=3$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no extraneous whitespace inside inner lists, in the exact format\n$$\n[[m_1,b_1],[m_2,b_2],[m_3,b_3]].\n$$",
            "solution": "This problem involves implementing a computational workflow to estimate linear response operators for magnetic error field correction in a simplified fusion device setting. This entails simulating experimental data, fitting a model using Tikhonov regularization (ridge regression), and evaluating its generalization performance via $K$-fold cross-validation. The core of the problem lies in solving a linear inverse problem, a common task in experimental science where an underlying system model must be inferred from noisy measurements.\n\nThe physical model is given by the linear relationship $y_k = R\\,u_k + G\\,w_k + \\varepsilon_k$ for $k \\in \\{1, \\dots, N\\}$ experiments. Here, $y_k$ is the vector of sensor measurements, $u_k$ is the vector of applied control currents, and $w_k$ is a vector representing intrinsic error fields. The matrices $R$ and $G$ are the linear response operators we wish to identify. The term $\\varepsilon_k$ represents measurement noise. By concatenating the inputs into $x_k$ and the operators into $B$, the model for the entire dataset can be expressed in a compact matrix form: $Y \\approx B X$. Our goal is to find an estimate $\\widehat{B}$ for the true operator $B_{\\text{true}}$.\n\nThe solution is implemented through a series of logical steps:\n\n1.  **Data Generation**: For each test case, we first synthesize a \"ground truth\" dataset according to the specified statistical properties. A deterministic random number generator, seeded by the parameter $s$, ensures reproducibility. The true response operators $R_{\\text{true}}$ and $G_{\\text{true}}$ are generated with entries drawn from a normal distribution. The input vectors $u_k$ and $w_k$ are drawn from standard normal distributions, simulating a typical experimental scenario where diverse inputs are applied to probe the system's response. The measurement vectors $y_k$ are then computed using the linear model, with Gaussian noise $\\varepsilon_k$ added.\n\n2.  **$K$-Fold Cross-Validation**: To robustly assess the model's ability to predict on new, unseen data, we employ $K$-fold cross-validation. The $N$ data samples are deterministically permuted and partitioned into $K$ folds. For each fold $f \\in \\{1, \\dots, K\\}$, we designate it as the validation set ($\\mathcal{V}_f$) and the remaining $K-1$ folds as the training set ($\\mathcal{T}_f$). This process is repeated $K$ times, with each fold serving as the validation set exactly once.\n\n3.  **Tikhonov Regularization (Ridge Regression)**: For each fold, we estimate the operator $\\widehat{B}_f$ using only the training data ($\\mathcal{T}_f$). The estimation is performed by solving the regularized least-squares problem:\n    $$\n    \\min_{B \\in \\mathbb{R}^{S \\times (A+M)}} \\sum_{k \\in \\mathcal{T}_f} \\lVert y_k - B x_k \\rVert_2^2 + \\lambda \\lVert B \\rVert_F^2\n    $$\n    The first term measures fidelity to the training data. The second term is a penalty on the squared Frobenius norm of the operator $B$, controlled by the regularization parameter $\\lambda \\ge 0$. This penalty is crucial for stabilizing the solution when the problem is underdetermined or ill-conditioned. The closed-form solution is given by:\n    $$\n    \\widehat{B}_f = Y_{\\mathcal{T}_f} X_{\\mathcal{T}_f}^{\\top}\\, \\left(X_{\\mathcal{T}_f} X_{\\mathcal{T}_f}^{\\top} + \\lambda I_{A+M}\\right)^{-1}\n    $$\n    where $X_{\\mathcal{T}_f}$ and $Y_{\\mathcal{T}_f}$ are the data matrices for the training set.\n\n4.  **Error Evaluation**: After obtaining $\\widehat{B}_f$, we evaluate its performance by calculating the mean squared error (MSE), normalized per sensor per sample. This is done separately for the training data and the held-out validation data. After iterating through all $K$ folds, we compute the average training MSE, $\\overline{\\operatorname{MSE}}_{\\text{train}}$, and the average validation MSE, $\\overline{\\operatorname{MSE}}_{\\text{val}}$. The latter serves as our primary metric for the model's generalization performance.\n\n5.  **Overfitting Assessment**: Overfitting occurs when a model learns the noise and specific artifacts of the training data, leading to poor performance on unseen data. This is typically characterized by a much lower training error compared to the validation error. We implement a specific heuristic to flag a high risk of overfitting. The indicator $O$ is set to true if two conditions are met simultaneously: the ratio of validation to training error is significant ($\\ge 1.5$), and the problem is in a regime susceptible to overfitting, as measured by a high complexity ratio $r$. This ratio compares the number of model parameters to the effective number of data points available for training.\n\nThe final program structure consists of a main function `solve` that iterates through the provided test cases. For each case, a helper function `run_single_case` executes the described workflow and returns the resulting $\\overline{\\operatorname{MSE}}_{\\text{val}}$ and the overfitting indicator $O$. The results are then aggregated and printed in the specified format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_single_case(N, S, A, M, sigma, lam, K, seed):\n    \"\"\"\n    Executes the full simulation and analysis for a single test case.\n    \n    Args:\n        N (int): Number of experiments.\n        S (int): Size of the sensor array.\n        A (int): Number of control coil currents.\n        M (int): Number of proxy error-field patterns.\n        sigma (float): Standard deviation of measurement noise.\n        lam (float): Tikhonov regularization parameter.\n        K (int): Number of folds for cross-validation.\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        tuple[float, bool]: A tuple containing the cross-validated mean squared \n                            error and the boolean overfitting risk indicator.\n    \"\"\"\n    # Use a dedicated Random Number Generator for reproducibility\n    rng = np.random.default_rng(seed)\n\n    # 1. Data generation\n    # Generate true response operators R and G, then stack them into B_true\n    R_true = rng.normal(loc=0.0, scale=1/np.sqrt(A), size=(S, A))\n    G_true = rng.normal(loc=0.0, scale=1/np.sqrt(M), size=(S, M))\n    B_true = np.hstack((R_true, G_true))  # Shape: S x (A+M)\n\n    # Generate stacked inputs X (u and w) and noise Epsilon\n    X = rng.normal(loc=0.0, scale=1.0, size=(A + M, N))\n    Epsilon = rng.normal(loc=0.0, scale=sigma, size=(S, N))\n\n    # Compute sensor measurements Y\n    Y = B_true @ X + Epsilon  # Shape: S x N\n\n    # 3. K-fold cross-validation setup\n    # Create a deterministic permutation of data indices\n    indices = rng.permutation(N)\n    # Split indices into K folds\n    folds = np.array_split(indices, K)\n    \n    all_mse_train = []\n    all_mse_val = []\n\n    for f in range(K):\n        # Determine training and validation sets for the current fold\n        val_indices = folds[f]\n        train_indices_list = [folds[i] for i in range(K) if i != f]\n        train_indices = np.concatenate(train_indices_list)\n        \n        X_train, Y_train = X[:, train_indices], Y[:, train_indices]\n        X_val, Y_val = X[:, val_indices], Y[:, val_indices]\n\n        num_train_samples = X_train.shape[1]\n        num_val_samples = X_val.shape[1]\n        \n        # 2. Estimation by Tikhonov regularization\n        # Calculate B_hat for the current training set\n        XXT = X_train @ X_train.T\n        YXT = Y_train @ X_train.T\n        identity_mat = np.identity(A + M)\n        \n        # Closed-form solution: B_hat = Y*X^T * (X*X^T + lambda*I)^-1\n        try:\n            inv_term = np.linalg.inv(XXT + lam * identity_mat)\n            B_hat = YXT @ inv_term\n        except np.linalg.LinAlgError:\n            # Fallback to pseudo-inverse if inversion fails, though unlikely with lambda > 0\n            inv_term = np.linalg.pinv(XXT + lam * identity_mat)\n            B_hat = YXT @ inv_term\n\n        # 3. Compute MSE for the current fold\n        # Predictions\n        Y_train_pred = B_hat @ X_train\n        Y_val_pred = B_hat @ X_val\n\n        # Sum of squared errors\n        train_error_sum_sq = np.sum((Y_train - Y_train_pred)**2)\n        val_error_sum_sq = np.sum((Y_val - Y_val_pred)**2)\n\n        # Normalized MSE\n        mse_train_f = train_error_sum_sq / (num_train_samples * S)\n        \n        if num_val_samples > 0:\n            mse_val_f = val_error_sum_sq / (num_val_samples * S)\n        else:\n            mse_val_f = 0.0\n\n        all_mse_train.append(mse_train_f)\n        all_mse_val.append(mse_val_f)\n\n    # Average MSEs over all folds\n    avg_mse_train = np.mean(all_mse_train)\n    avg_mse_val = np.mean(all_mse_val)\n\n    # 4. Overfitting risk indicator\n    # Calculate complexity ratio r\n    r = (K * (A + M)) / (N * (K - 1))\n    \n    # Calculate ratio of validation to training MSE.\n    # Handle the unlikely case of zero training error to avoid division by zero.\n    ratio_mse = np.inf if avg_mse_train == 0 else avg_mse_val / avg_mse_train\n    \n    # Apply the logical AND condition for the overfitting indicator\n    overfitting_risk = (ratio_mse >= 1.5) and (r >= 0.6)\n\n    return avg_mse_val, overfitting_risk\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results in the required format.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, S, A, M, sigma, lam, K, seed)\n        (200, 8, 3, 2, 0.05, 1e-6, 5, 1),\n        (30, 8, 10, 8, 0.08, 1e-6, 5, 2),\n        (15, 6, 8, 8, 0.05, 1e-3, 5, 3),\n    ]\n\n    results = []\n    for case in test_cases:\n        N, S, A, M, sigma, lam, K, seed = case\n        m_val, b_risk = run_single_case(N, S, A, M, sigma, lam, K, seed)\n        results.append([m_val, b_risk])\n\n    # Final print statement in the exact required format.\n    # e.g., [[0.0025,False],[0.015,True],[0.018,True]]\n    # str(bool_value) correctly produces 'True' or 'False'.\n    inner_parts = [f\"[{m},{b}]\" for m, b in results]\n    print(f\"[{','.join(inner_parts)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Operating a fusion device often involves balancing competing priorities. For instance, we may want to simultaneously minimize error fields in the plasma core to maintain confinement while also applying specific magnetic perturbations at the edge to control instabilities. This final practice  tackles this challenge directly through the framework of multi-objective optimization. You will use a linear response model, like the one you might build in the previous exercise, to trace the trade-off curve (Pareto front) between conflicting goals and use a quantitative method to select an optimal, balanced operating point.",
            "id": "3976162",
            "problem": "You are given a linearized, dimensionless model of coil current actuation for magnetic error fields in a toroidal device. The decision variable is the coil current vector $\\mathbf{x} \\in \\mathbb{R}^n$. The core error field residual is represented by a linear mapping $\\mathbf{C} \\in \\mathbb{R}^{m_c \\times n}$ and a desired core cancellation vector $\\mathbf{d} \\in \\mathbb{R}^{m_c}$, giving a core residual cost $J_c(\\mathbf{x}) = \\lVert \\mathbf{C}\\mathbf{x} - \\mathbf{d} \\rVert_2^2$. The edge Resonant Magnetic Perturbation (RMP) strength target is represented by a linear mapping $\\mathbf{E} \\in \\mathbb{R}^{m_e \\times n}$ with a desired RMP vector $\\mathbf{r} \\in \\mathbb{R}^{m_e}$, giving an edge tracking cost $J_e(\\mathbf{x}) = \\lVert \\mathbf{E}\\mathbf{x} - \\mathbf{r} \\rVert_2^2$. To encode engineering limits and avoid ill-conditioning, a Tikhonov regularization term $\\mu \\lVert \\mathbf{x} \\rVert_2^2$ with $\\mu > 0$ is included. All quantities are dimensionless and normalized, so no physical units are required.\n\nThe Pareto optimization is performed using a weighted-sum scalarization parameter $w \\in [0,1]$ over the set $\\mathcal{W} = \\{\\, 0,\\, 0.1,\\, 0.25,\\, 0.5,\\, 0.75,\\, 0.9,\\, 1 \\,\\}$, for a fixed $\\mu$. For each $w \\in \\mathcal{W}$, consider the convex quadratic program\n$$\n\\min_{\\mathbf{x} \\in \\mathbb{R}^n} \\; F_w(\\mathbf{x}) = w \\, J_c(\\mathbf{x}) + (1-w) \\, J_e(\\mathbf{x}) + \\mu \\lVert \\mathbf{x} \\rVert_2^2,\n$$\nand denote by $\\mathbf{x}_w$ the unique minimizer. Define the realized core residual $J_c(\\mathbf{x}_w)$, the edge tracking residual $J_e(\\mathbf{x}_w)$, and the achieved edge RMP strength $S(\\mathbf{x}_w) = \\lVert \\mathbf{E} \\mathbf{x}_w \\rVert_2$.\n\nSelect the operating point by identifying the discrete “knee” of the Pareto trade-off curve using the following procedure: construct the sequence of points $\\big( \\log_{10} J_c(\\mathbf{x}_w), \\log_{10} J_e(\\mathbf{x}_w) \\big)$ in the order of increasing $w \\in \\mathcal{W}$, then for every triple of consecutive points compute the Menger curvature $\\kappa = \\dfrac{4 A}{a b c}$, where $a$, $b$, and $c$ are the side lengths of the triangle formed by the triple, and $A$ is the triangle area. Choose the $w^\\star \\in \\mathcal{W}$ corresponding to the maximum curvature over interior triples (endpoints are excluded). Return the triple $\\big[ w^\\star,\\, J_c(\\mathbf{x}_{w^\\star}),\\, S(\\mathbf{x}_{w^\\star}) \\big]$.\n\nUse the regularization $\\mu = 10^{-3}$ and solve the following three independent test cases. Each case defines $\\mathbf{C}$, $\\mathbf{d}$, $\\mathbf{E}$, and $\\mathbf{r}$ explicitly.\n\nTest case $1$:\n$$\n\\mathbf{C}_1 = \\begin{pmatrix}\n1 & 0.5 & 0 \\\\\n0 & 1 & 0.5\n\\end{pmatrix}, \\quad\n\\mathbf{d}_1 = \\begin{pmatrix} 0.1 \\\\ -0.2 \\end{pmatrix}, \\quad\n\\mathbf{E}_1 = \\begin{pmatrix}\n0.2 & -0.5 & 1.0 \\\\\n0 & 0.3 & 0.4\n\\end{pmatrix}, \\quad\n\\mathbf{r}_1 = \\begin{pmatrix} 0.3 \\\\ 0.0 \\end{pmatrix}.\n$$\n\nTest case $2$:\n$$\n\\mathbf{C}_2 = \\begin{pmatrix}\n1.0 & 2.0 & -1.0 \\\\\n0.0 & 1.0 & 1.0 \\\\\n1.0 & -1.0 & 0.5\n\\end{pmatrix}, \\quad\n\\mathbf{d}_2 = \\begin{pmatrix} 0.05 \\\\ -0.1 \\\\ 0.2 \\end{pmatrix}, \\quad\n\\mathbf{E}_2 = \\begin{pmatrix}\n-0.3 & 1.0 & 0.7 \\\\\n0.6 & -0.2 & 0.1\n\\end{pmatrix}, \\quad\n\\mathbf{r}_2 = \\begin{pmatrix} 0.4 \\\\ 0.2 \\end{pmatrix}.\n$$\n\nTest case $3$:\n$$\n\\mathbf{C}_3 = \\begin{pmatrix}\n10^{-2} & 2 \\cdot 10^{-2} & -10^{-2} \\\\\n2 \\cdot 10^{-2} & 4 \\cdot 10^{-2} & -2 \\cdot 10^{-2}\n\\end{pmatrix}, \\quad\n\\mathbf{d}_3 = \\begin{pmatrix} 10^{-3} \\\\ -10^{-3} \\end{pmatrix}, \\quad\n\\mathbf{E}_3 = \\begin{pmatrix}\n1.0 & 0.0 & 0.0 \\\\\n0.0 & 1.0 & 0.0\n\\end{pmatrix}, \\quad\n\\mathbf{r}_3 = \\begin{pmatrix} 0.2 \\\\ -0.1 \\end{pmatrix}.\n$$\n\nRequirements:\n- For each test case $i \\in \\{1,2,3\\}$ and each $w \\in \\mathcal{W}$, solve for $\\mathbf{x}_w$ and compute $J_c(\\mathbf{x}_w)$, $J_e(\\mathbf{x}_w)$, and $S(\\mathbf{x}_w)$ as defined above. Use $\\mu = 10^{-3}$.\n- Determine $w^\\star$ by maximizing the discrete Menger curvature on the $\\big( \\log_{10} J_c, \\log_{10} J_e \\big)$ curve over interior triples.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where the list contains three elements corresponding to the three test cases, each element being a list of the form $[w^\\star, J_c(\\mathbf{x}_{w^\\star}), S(\\mathbf{x}_{w^\\star})]$. For example, an output of the form `[[\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot]]`.\n\nAll computations are purely numerical and dimensionless; no unit conversions are required. Angles are not used. The outputs must be real numbers.",
            "solution": "This problem requires the solution of a series of regularized linear least-squares problems to trace a Pareto front, followed by a geometric analysis to identify an optimal operating point. The principles of multi-objective optimization, linear algebra, and numerical geometry are central to the solution.\n\nFirst, we address the core optimization problem for a fixed weight $w \\in [0,1]$. The objective function to minimize is\n$$\nF_w(\\mathbf{x}) = w \\lVert \\mathbf{C}\\mathbf{x} - \\mathbf{d} \\rVert_2^2 + (1-w) \\lVert \\mathbf{E}\\mathbf{x} - \\mathbf{r} \\rVert_2^2 + \\mu \\lVert \\mathbf{x} \\rVert_2^2.\n$$\nThis is a convex quadratic function of the decision variable $\\mathbf{x}$. The minimizer $\\mathbf{x}_w$ is found by setting the gradient with respect to $\\mathbf{x}$ to zero. This yields a system of linear equations known as the normal equations:\n$$\n\\left( w \\mathbf{C}^T \\mathbf{C} + (1-w) \\mathbf{E}^T \\mathbf{E} + \\mu \\mathbf{I} \\right) \\mathbf{x}_w = w \\mathbf{C}^T \\mathbf{d} + (1-w) \\mathbf{E}^T \\mathbf{r}.\n$$\nLet us define $\\mathbf{A}_w = w \\mathbf{C}^T \\mathbf{C} + (1-w) \\mathbf{E}^T \\mathbf{E} + \\mu \\mathbf{I}$ and $\\mathbf{b}_w = w \\mathbf{C}^T \\mathbf{d} + (1-w) \\mathbf{E}^T \\mathbf{r}$. The system is $\\mathbf{A}_w \\mathbf{x}_w = \\mathbf{b}_w$. The matrix $\\mathbf{A}_w$ is guaranteed to be invertible because it is the sum of positive semi-definite matrices and a positive definite matrix ($\\mu \\mathbf{I}$ for $\\mu > 0$). Thus, a unique solution $\\mathbf{x}_w = \\mathbf{A}_w^{-1} \\mathbf{b}_w$ exists for each $w$.\n\nThe overall solution procedure is as follows:\n1. For each test case, iterate through each weight $w$ in the specified set $\\mathcal{W}$.\n2. For each $w$, construct the matrix $\\mathbf{A}_w$ and vector $\\mathbf{b}_w$ as defined above.\n3. Solve the linear system $\\mathbf{A}_w \\mathbf{x}_w = \\mathbf{b}_w$ to find the optimal coil current vector $\\mathbf{x}_w$.\n4. Using $\\mathbf{x}_w$, compute the realized core and edge residuals, $J_c(\\mathbf{x}_w)$ and $J_e(\\mathbf{x}_w)$.\n5. Construct a sequence of points representing the Pareto trade-off curve in log-space: $P_w = \\left( \\log_{10} J_c(\\mathbf{x}_w), \\log_{10} J_e(\\mathbf{x}_w) \\right)$, ordered by increasing $w$.\n6. To find the \"knee\" of this curve, analyze interior triples of consecutive points. For each interior point $P_i$ and its neighbors $P_{i-1}$ and $P_{i+1}$, compute the Menger curvature $\\kappa_i$.\n7. The Menger curvature is given by $\\kappa = \\frac{4A}{abc}$, where $a$, $b$, and $c$ are the lengths of the sides of the triangle formed by the three points, and $A$ is the triangle's area. The area $A$ can be computed robustly using the vector cross product magnitude.\n8. Identify the weight $w^\\star$ that corresponds to the triple yielding the maximum curvature. This $w^\\star$ is our selected operating point, representing the best compromise between the competing objectives.\n9. Finally, retrieve the corresponding core residual $J_c(\\mathbf{x}_{w^\\star})$ and calculate the achieved edge RMP strength $S(\\mathbf{x}_{w^\\star}) = \\lVert \\mathbf{E} \\mathbf{x}_{w^\\star} \\rVert_2$. The required output is the triple $[w^\\star, J_c(\\mathbf{x}_{w^\\star}), S(\\mathbf{x}_{w^\\star})]$.\n\nThis procedure is systematically applied to each of the three test cases using the provided numerical data.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the three test cases for the error field correction problem.\n    \"\"\"\n    mu = 1e-3\n    W = [0.0, 0.1, 0.25, 0.5, 0.75, 0.9, 1.0]\n\n    # Test case 1 data\n    C1 = np.array([[1.0, 0.5, 0.0],\n                   [0.0, 1.0, 0.5]])\n    d1 = np.array([0.1, -0.2])\n    E1 = np.array([[0.2, -0.5, 1.0],\n                   [0.0, 0.3, 0.4]])\n    r1 = np.array([0.3, 0.0])\n\n    # Test case 2 data\n    C2 = np.array([[1.0, 2.0, -1.0],\n                   [0.0, 1.0, 1.0],\n                   [1.0, -1.0, 0.5]])\n    d2 = np.array([0.05, -0.1, 0.2])\n    E2 = np.array([[-0.3, 1.0, 0.7],\n                   [0.6, -0.2, 0.1]])\n    r2 = np.array([0.4, 0.2])\n\n    # Test case 3 data\n    C3 = np.array([[1e-2, 2e-2, -1e-2],\n                   [2e-2, 4e-2, -2e-2]])\n    d3 = np.array([1e-3, -1e-3])\n    E3 = np.array([[1.0, 0.0, 0.0],\n                   [0.0, 1.0, 0.0],\n                   [0.0, 0.0, 1.0]]) # Dimension corrected to match x (n=3)\n    r3 = np.array([0.2, -0.1, 0.0]) # Dimension corrected for n=3, E3 is not 2x2 but 3x3 if C is 2x3.\n    # The problem description for case 3 has a dimension mismatch.\n    # C3 is 2x3, implying x is in R^3. E3 is given as 2x2. This is impossible.\n    # A reasonable assumption is that E3 is meant to access the first two components of x.\n    # Let's redefine E3 to be 2x3 to match x.\n    E3_corrected = np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]])\n    r3_corrected = np.array([0.2, -0.1])\n    \n    test_cases = [\n        (C1, d1, E1, r1),\n        (C2, d2, E2, r2),\n        (C3, d3, E3_corrected, r3_corrected)\n    ]\n\n    results = []\n    for case in test_cases:\n        C, d, E, r = case\n        result = solve_case(C, d, E, r, mu, W)\n        results.append(result)\n\n    # Format the final output string as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef solve_case(C, d, E, r, mu, W):\n    \"\"\"\n    Solves a single test case of the optimization problem.\n\n    Args:\n        C (np.ndarray): Core error field mapping matrix.\n        d (np.ndarray): Desired core cancellation vector.\n        E (np.ndarray): Edge RMP strength mapping matrix.\n        r (np.ndarray): Desired RMP vector.\n        mu (float): Tikhonov regularization parameter.\n        W (list of float): List of weights for Pareto optimization.\n\n    Returns:\n        list: A list containing [w_star, Jc(x_w_star), S(x_w_star)].\n    \"\"\"\n    n = C.shape[1]\n    I = np.identity(n)\n    \n    pareto_points = []\n    results_for_w = {}\n\n    for w in W:\n        # Formulate and solve the linear system A_w * x_w = b_w\n        A_w = w * (C.T @ C) + (1 - w) * (E.T @ E) + mu * I\n        b_w = w * (C.T @ d) + (1 - w) * (E.T @ r)\n        x_w = np.linalg.solve(A_w, b_w)\n\n        # Calculate performance metrics\n        Jc_xw = np.linalg.norm(C @ x_w - d)**2\n        Je_xw = np.linalg.norm(E @ x_w - r)**2\n        S_xw = np.linalg.norm(E @ x_w)\n\n        # Store data for Pareto analysis\n        pareto_points.append((np.log10(Jc_xw), np.log10(Je_xw)))\n        results_for_w[w] = {\n            'Jc': Jc_xw,\n            'S': S_xw,\n        }\n    \n    # Find the knee of the Pareto curve using Menger curvature\n    curvatures = []\n    # Iterate over interior triples of points on the curve\n    for i in range(1, len(W) - 1):\n        p_prev = np.array(pareto_points[i-1])\n        p_curr = np.array(pareto_points[i])\n        p_next = np.array(pareto_points[i+1])\n\n        # Calculate side lengths of the triangle formed by the triple\n        a = np.linalg.norm(p_next - p_curr)\n        b = np.linalg.norm(p_next - p_prev)\n        c = np.linalg.norm(p_curr - p_prev)\n\n        # Check for collinearity or degenerate triangles\n        if a * b * c == 0.0 or np.isclose(a + c, b) or np.isclose(b + c, a) or np.isclose(a + b, c):\n            curvatures.append(0.0)\n            continue\n        \n        # Calculate triangle area using the 2D cross product magnitude for numerical stability\n        vec1 = p_prev - p_curr\n        vec2 = p_next - p_curr\n        area = 0.5 * np.abs(vec1[0] * vec2[1] - vec1[1] * vec2[0])\n\n        # Menger curvature formula\n        kappa = (4 * area) / (a * b * c)\n        curvatures.append(kappa)\n\n    # The curvature list corresponds to the interior weights W[1] through W[len(W)-2]\n    # Find the index of the maximum curvature\n    max_curvature_idx = np.argmax(curvatures)\n    # The index in the original weight list W is offset by 1\n    w_star_index = max_curvature_idx + 1\n    w_star = W[w_star_index]\n\n    # Retrieve the final results for the optimal weight w_star\n    final_Jc = results_for_w[w_star]['Jc']\n    final_S = results_for_w[w_star]['S']\n    \n    return [w_star, final_Jc, final_S]\n\nif __name__ == '__main__':\n    # In test case 3, there was a dimension mismatch in the problem spec.\n    # C3 is 2x3, implying x is in R^3. E3 was given as 2x2.\n    # The most logical interpretation is that E3 should be 2x3 to match x.\n    # This correction is implemented in the main `solve` function.\n    solve()\n```"
        }
    ]
}