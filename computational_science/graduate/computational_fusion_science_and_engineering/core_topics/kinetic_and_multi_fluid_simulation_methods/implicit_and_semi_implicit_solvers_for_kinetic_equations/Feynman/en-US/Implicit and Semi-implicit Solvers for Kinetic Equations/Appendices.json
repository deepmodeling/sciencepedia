{
    "hands_on_practices": [
        {
            "introduction": "To build a solid foundation, we begin with the fundamental mechanics of a semi-implicit solver. This exercise  walks you through a single time step of a semi-Lagrangian scheme for a kinetic model of the form $\\partial_t f + v \\partial_x f = C(f)$. By explicitly tracing particle characteristics to handle the advection term and then applying an implicit update for the collisional relaxation, you will gain hands-on experience with the core components of operator-splitting methods.",
            "id": "3992583",
            "problem": "Consider the one-dimensional Bhatnagar–Gross–Krook (BGK) advection–relaxation kinetic equation\n$$\n\\frac{\\partial f}{\\partial t} + v\\,\\frac{\\partial f}{\\partial x} = \\nu \\left(f_{\\mathrm{eq}} - f\\right),\n$$\nwith constant advecting velocity $v$ and constant collision frequency $\\nu$, posed on a periodic domain of length $L$ with coordinate $x \\in [0,L)$. A semi-Lagrangian time step traces characteristics backward in space over one time increment to obtain an advected value from the previous time level and then advances the relaxation term implicitly in time. The backward characteristic from an Eulerian grid point $x_i$ at time $t^{n+1}$ reaches the departure point $x_{\\mathrm{d}}$ at time $t^n$, where $x_{\\mathrm{d}}$ is determined using the constant velocity characteristic.\n\nYou are given a uniform grid with $N=8$ points, $L=1$, grid spacing $\\Delta x = L/N$, grid points $x_i = i\\,\\Delta x$ for $i=0,1,\\dots,7$, and periodic boundary conditions. The equilibrium is uniform $f_{\\mathrm{eq}}(x)=0.25$. The time step is $\\Delta t = 0.125$, the constant velocity is $v = 1.6$, and the collision frequency is $\\nu = 4.0$. The discrete profile at time level $t^n$ is defined by sampling $f^n(x)=\\sin(2\\pi x)$ on the grid, so that\n- $f^n(x_0)=0$,\n- $f^n(x_1)=\\frac{\\sqrt{2}}{2}$,\n- $f^n(x_2)=1$,\n- $f^n(x_3)=\\frac{\\sqrt{2}}{2}$,\n- $f^n(x_4)=0$,\n- $f^n(x_5)=-\\frac{\\sqrt{2}}{2}$,\n- $f^n(x_6)=-1$,\n- $f^n(x_7)=-\\frac{\\sqrt{2}}{2}$.\n\nStarting from the fundamental definition of characteristics for the advective term and an implicit (backward Euler) time discretization for the relaxation term along the characteristic, perform one semi-Lagrangian implicit time step. Specifically:\n- Trace the characteristic backward from $x_3$ over $\\Delta t$ to obtain the departure point $x_{\\mathrm{d}}$.\n- Evaluate $f^n$ at $x_{\\mathrm{d}}$ using first-order linear interpolation between the two nearest grid points on the uniform grid.\n- Advance the relaxation term implicitly at $x_3$ over the time interval $\\Delta t$.\n\nCompute $f^{n+1}(x_3)$ and express your final answer as a single exact analytical expression. Do not approximate or round the result; no units are required for the final value.",
            "solution": "The problem is well-posed, scientifically sound, and contains all necessary information to compute a unique solution. We can therefore proceed with the calculation.\n\nThe one-dimensional Bhatnagar–Gross–Krook (BGK) advection–relaxation kinetic equation is given by:\n$$\n\\frac{\\partial f}{\\partial t} + v\\,\\frac{\\partial f}{\\partial x} = \\nu \\left(f_{\\mathrm{eq}} - f\\right)\n$$\nThe left-hand side represents the total derivative of the distribution function $f$ along the characteristic curves defined by $\\frac{dx}{dt} = v$. Let's denote this total derivative as $\\frac{Df}{Dt}$. The equation can be rewritten as:\n$$\n\\frac{Df}{Dt} = \\nu \\left(f_{\\mathrm{eq}} - f\\right)\n$$\nA semi-Lagrangian implicit scheme discretizes this equation. The advection part is handled by tracing characteristics backward in time. The relaxation (source) term is handled with an implicit time-stepping method. To find the value $f^{n+1}(x_i)$ at a grid point $x_i$ and time $t^{n+1} = t^n + \\Delta t$, we first identify the departure point $x_d$ of the characteristic that arrives at $x_i$ at time $t^{n+1}$. The value of $f$ at the previous time step at this departure point, $f^n(x_d)$, is used as the advected value. Applying a backward Euler (fully implicit) scheme for the relaxation term, we get:\n$$\n\\frac{f(x_i, t^{n+1}) - f(x_d, t^n)}{\\Delta t} = \\nu \\left(f_{\\mathrm{eq}}(x_i) - f(x_i, t^{n+1})\\right)\n$$\nLet $f_i^{n+1} = f(x_i, t^{n+1})$ and $f^n(x_d)$ be the value of the distribution function at the departure point at time $t^n$. The equation for $f_i^{n+1}$ is:\n$$\n\\frac{f_i^{n+1} - f^n(x_d)}{\\Delta t} = \\nu \\left(f_{\\mathrm{eq},i} - f_i^{n+1}\\right)\n$$\nwhere $f_{\\mathrm{eq},i} = f_{\\mathrm{eq}}(x_i)$. Rearranging to solve for $f_i^{n+1}$:\n$$\nf_i^{n+1} - f^n(x_d) = \\nu \\Delta t f_{\\mathrm{eq},i} - \\nu \\Delta t f_i^{n+1}\n$$\n$$\nf_i^{n+1} (1 + \\nu \\Delta t) = f^n(x_d) + \\nu \\Delta t f_{\\mathrm{eq},i}\n$$\n$$\nf_i^{n+1} = \\frac{f^n(x_d) + \\nu \\Delta t f_{\\mathrm{eq},i}}{1 + \\nu \\Delta t}\n$$\nThe problem requires computing $f^{n+1}(x_3)$. The calculation proceeds in three steps:\n\n**Step 1: Determine the departure point $x_d$**\nThe characteristic equation is $\\frac{dx}{dt} = v$. For a constant velocity $v$, the characteristic line arriving at $x_3$ at time $t^{n+1}$ is given by $x(t) = x_d + v(t - t^n)$. At $t=t^{n+1}$, we have $x(t^{n+1}) = x_3$.\n$$\nx_3 = x_d + v(t^{n+1} - t^n) = x_d + v \\Delta t\n$$\nThus, the departure point at time $t^n$ is:\n$$\nx_d = x_3 - v \\Delta t\n$$\nThe given values are:\n- Grid point $x_3 = 3\\,\\Delta x = 3 \\times \\frac{L}{N} = 3 \\times \\frac{1}{8} = \\frac{3}{8}$.\n- Velocity $v = 1.6 = \\frac{16}{10} = \\frac{8}{5}$.\n- Time step $\\Delta t = 0.125 = \\frac{1}{8}$.\nSubstituting these values:\n$$\nx_d = \\frac{3}{8} - \\left(\\frac{8}{5}\\right) \\left(\\frac{1}{8}\\right) = \\frac{3}{8} - \\frac{1}{5} = \\frac{15 - 8}{40} = \\frac{7}{40}\n$$\nSo, the departure point is $x_d = \\frac{7}{40} = 0.175$.\n\n**Step 2: Evaluate $f^n$ at $x_d$ using linear interpolation**\nThe departure point $x_d = 0.175$ lies between the grid points $x_1$ and $x_2$.\n- $x_1 = 1\\,\\Delta x = \\frac{1}{8} = 0.125$.\n- $x_2 = 2\\,\\Delta x = \\frac{2}{8} = 0.25$.\nThe values of $f^n$ at these points are given:\n- $f_1^n = f^n(x_1) = \\frac{\\sqrt{2}}{2}$.\n- $f_2^n = f^n(x_2) = 1$.\nFirst-order linear interpolation is defined as:\n$$\nf^n(x_d) = f_1^n + \\frac{x_d - x_1}{x_2 - x_1} (f_2^n - f_1^n)\n$$\nThe interpolation weight is $\\alpha = \\frac{x_d - x_1}{x_2 - x_1} = \\frac{0.175 - 0.125}{0.25 - 0.125} = \\frac{0.05}{0.125} = \\frac{50}{125} = \\frac{2}{5}$.\nThe interpolated value is:\n$$\nf^n(x_d) = (1-\\alpha) f_1^n + \\alpha f_2^n = \\left(1-\\frac{2}{5}\\right) \\frac{\\sqrt{2}}{2} + \\frac{2}{5}(1) = \\frac{3}{5}\\frac{\\sqrt{2}}{2} + \\frac{2}{5}\n$$\n$$\nf^n(x_d) = \\frac{3\\sqrt{2}}{10} + \\frac{4}{10} = \\frac{3\\sqrt{2} + 4}{10}\n$$\n\n**Step 3: Advance the relaxation term implicitly**\nWe now use the update formula to compute $f^{n+1}(x_3) = f_3^{n+1}$:\n$$\nf_3^{n+1} = \\frac{f^n(x_d) + \\nu \\Delta t f_{\\mathrm{eq}}(x_3)}{1 + \\nu \\Delta t}\n$$\nThe remaining parameters are:\n- Collision frequency $\\nu = 4.0 = 4$.\n- Equilibrium distribution $f_{\\mathrm{eq}}(x_3) = 0.25 = \\frac{1}{4}$.\nThe term $\\nu \\Delta t$ is:\n$$\n\\nu \\Delta t = 4 \\times 0.125 = 4 \\times \\frac{1}{8} = \\frac{1}{2}\n$$\nSubstituting all values into the update formula:\n$$\nf_3^{n+1} = \\frac{\\left(\\frac{3\\sqrt{2} + 4}{10}\\right) + \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{4}\\right)}{1 + \\frac{1}{2}} = \\frac{\\frac{3\\sqrt{2} + 4}{10} + \\frac{1}{8}}{\\frac{3}{2}}\n$$\nWe simplify the numerator by finding a common denominator, which is $40$:\n$$\n\\frac{3\\sqrt{2} + 4}{10} + \\frac{1}{8} = \\frac{4(3\\sqrt{2} + 4)}{40} + \\frac{5}{40} = \\frac{12\\sqrt{2} + 16 + 5}{40} = \\frac{12\\sqrt{2} + 21}{40}\n$$\nNow, we perform the division:\n$$\nf_3^{n+1} = \\frac{\\frac{12\\sqrt{2} + 21}{40}}{\\frac{3}{2}} = \\frac{12\\sqrt{2} + 21}{40} \\times \\frac{2}{3} = \\frac{12\\sqrt{2} + 21}{60}\n$$\nFinally, we can factor out a $3$ from the numerator:\n$$\nf_3^{n+1} = \\frac{3(4\\sqrt{2} + 7)}{60} = \\frac{4\\sqrt{2} + 7}{20}\n$$\nThis is the final exact analytical expression for $f^{n+1}(x_3)$.",
            "answer": "$$\n\\boxed{\\frac{7 + 4\\sqrt{2}}{20}}\n$$"
        },
        {
            "introduction": "While linear models are instructive, most kinetic collision operators $C[f]$ are nonlinear, requiring iterative methods like Newton's method to solve the implicit equations. This practice  confronts a critical challenge: ensuring that the distribution function $f$ remains physically positive during the iteration. You will analyze several strategies for \"globalizing\" the Newton solve, learning to distinguish between ad-hoc fixes and theoretically sound methods that guarantee both positivity and rapid convergence.",
            "id": "3992632",
            "problem": "Consider a nonlinear kinetic equation for the single-species velocity-space distribution function $f(v,t)$ used in computational fusion science and engineering, with collisions modeled by a conservative, entropy-dissipating operator $C[f]$ that satisfies the Boltzmann $H$-theorem: for any nonnegative $f$, the functional $H(f) = \\int f \\ln f \\, dv$ is nonincreasing under the evolution $\\partial_t f = C[f]$, and $C[f]$ preserves mass. An implicit time update of backward Euler type seeks $f^{n+1}$ from\n$$\nf^{n+1} - f^n = \\Delta t \\, C[f^{n+1}],\n$$\nwhich can be written as a residual equation $R(f^{n+1}) = f^{n+1} - f^n - \\Delta t \\, C[f^{n+1}] = 0$. A Newton iteration constructs a sequence $f^{(k)}$ by solving the linearized system\n$$\nJ(f^{(k)}) \\, \\delta f^{(k)} = - R(f^{(k)}), \\quad f^{(k+1)} = f^{(k)} + \\delta f^{(k)},\n$$\nwhere $J(f)$ is the Fréchet derivative of $R$ at $f$.\n\nIn many physically consistent discretizations of $C[f]$ used for fusion-relevant collisional dynamics (for example, discretizations of the Fokker–Planck or Landau operators), coefficients such as diffusion tensors and friction terms depend functionally on $f$ and are well-defined only for $f \\ge 0$. Furthermore, discrete entropy functionals include terms like $f \\ln f$, which are undefined for $f \\le 0$. It is observed that undamped Newton steps can produce iterates $f^{(k)}$ with negative components in the discrete representation, leading to such failure modes as:\n- violation of positivity constraints, causing coefficients to become unphysical (e.g., negative diffusion) and the Jacobian $J(f^{(k)})$ to lose monotonicity or become indefinite;\n- breakdown of discrete entropy dissipation because $H(f^{(k)})$ ceases to be defined as soon as any component of $f^{(k)}$ is nonpositive;\n- divergence or stagnation of Newton iterations due to leaving the domain on which $R$ and $J$ are smoothly defined.\n\nAssume a velocity grid with $N$ cells, a mass-conserving discretization of $C[f]$, and a time step $\\Delta t$ that is large enough to require implicit treatment of collisions but small enough to expect convergence of a well-designed nonlinear solver. You are asked to analyze methods to enforce positivity during the nonlinear solve and recover convergence with high accuracy.\n\nWhich of the following modifications to the Newton iteration are well-founded approaches to enforce nonnegativity of $f$ and recover convergence while preserving, in the asymptotic regime, the rapid local convergence of Newton’s method to the physically admissible solution?\n\nA. Introduce a backtracking line search that enforces a feasibility condition and a descent condition. Replace $f^{(k+1)} = f^{(k)} + \\delta f^{(k)}$ by $f^{(k+1)} = f^{(k)} + \\lambda^{(k)} \\delta f^{(k)}$, where $\\lambda^{(k)} \\in (0,1]$ is chosen so that $f^{(k)} + \\lambda^{(k)} \\delta f^{(k)} \\ge \\epsilon$ componentwise for a small $\\epsilon > 0$ and such that an Armijo-type condition on the residual norm or discrete entropy dissipation holds.\n\nB. Perform a nonlinear change of variables to an entropy variable $u = \\ln f$, so that $f = e^{u} \\ge 0$ is guaranteed. Rewrite the residual as $\\widehat{R}(u) = R(e^{u})$ and apply Newton’s method in $u$, i.e., solve $\\widehat{J}(u^{(k)}) \\, \\delta u^{(k)} = - \\widehat{R}(u^{(k)})$ with $u^{(k+1)} = u^{(k)} + \\delta u^{(k)}$, and then set $f^{(k+1)} = e^{u^{(k+1)}}$.\n\nC. After each Newton update, clip negative components by setting $(f^{(k+1)})_i = \\max\\{(f^{(k+1)})_i,0\\}$, leaving the Jacobian and residual evaluation unchanged in subsequent iterations.\n\nD. Reduce the time step $\\Delta t$ until the undamped Newton method no longer produces negative components, without any change to the update rule or residual definition; accept the resulting solution even if $\\Delta t$ is orders of magnitude smaller than originally intended.\n\nE. Project the tentative update $f^{(k)} + \\delta f^{(k)}$ onto the feasible set that enforces discrete nonnegativity and mass conservation by solving the convex optimization problem\n$$\n\\min_{g \\in \\mathbb{R}^N} \\frac{1}{2} \\| g - (f^{(k)} + \\delta f^{(k)}) \\|_{M}^2 \\quad \\text{subject to} \\quad g_i \\ge \\epsilon \\ \\text{for all} \\ i, \\quad \\sum_{i=1}^{N} w_i g_i = \\sum_{i=1}^{N} w_i f^{n},\n$$\nwhere $M$ is a symmetric positive definite mass matrix associated with the discretization and $w_i$ are quadrature weights. Then set $f^{(k+1)} = g$ and recompute the residual and Jacobian at $f^{(k+1)}$.\n\nSelect all that apply.",
            "solution": "It presents a well-posed, scientifically grounded, and objective problem in the field of computational science and engineering, specifically addressing the numerical solution of nonlinear kinetic equations. The described challenge — enforcing positivity in implicit Newton-based solvers for distribution functions — is a standard and significant issue in this domain. The problem is self-contained and provides sufficient context to evaluate the proposed methods.\n\nThe task is to identify which of the proposed modifications to a Newton iteration for solving $R(f) = f - f^n - \\Delta t \\, C[f] = 0$ represent well-founded approaches to enforce nonnegativity ($f \\ge 0$), ensure convergence, and asymptotically preserve the quadratic convergence rate of Newton's method.\n\nLet's analyze each option based on established principles of numerical analysis for nonlinear equations and constrained optimization.\n\n**A. Introduce a backtracking line search...**\n\nThis option proposes a standard globalization strategy for Newton's method, known as a line search. The update formula is modified from $f^{(k+1)} = f^{(k)} + \\delta f^{(k)}$ to $f^{(k+1)} = f^{(k)} + \\lambda^{(k)} \\delta f^{(k)}$, where $\\lambda^{(k)} \\in (0,1]$ is a step-length parameter.\n\n1.  **Enforcement of Nonnegativity:** The step length $\\lambda^{(k)}$ is chosen to satisfy a feasibility condition. If the current iterate $f^{(k)}$ is positive, but the tentative update $f^{(k)} + \\delta f^{(k)}$ has negative components, a smaller $\\lambda^{(k)}$ can be found to ensure positivity. For any component $i$ where $(\\delta f^{(k)})_i < 0$, the condition $(f^{(k)})_i + \\lambda^{(k)} (\\delta f^{(k)})_i \\ge \\epsilon$ must hold. This implies $\\lambda^{(k)} \\le \\frac{(f^{(k)})_i - \\epsilon}{-(\\delta f^{(k)})_i}$. A suitable $\\lambda^{(k)}$ can be found by taking the minimum of $1$ and these upper bounds (multiplied by a safety factor $< 1$) over all components where the update points towards the boundary. This systematically prevents the iterates from becoming non-positive.\n\n2.  **Recovery of Convergence:** The Newton direction $\\delta f^{(k)}$ is a descent direction for the residual norm $\\|R(f)\\|$ near a solution. The Armijo-type condition, often written as $\\|R(f^{(k)} + \\lambda^{(k)} \\delta f^{(k)})\\|^2 \\le (1 - 2\\alpha\\lambda^{(k)})\\|R(f^{(k)})\\|^2$ for some $\\alpha \\in (0, 1/2)$, guarantees that each step provides a sufficient decrease in the residual. This prevents the iteration from diverging and globalizes the convergence from initial guesses far from the solution. Using a discrete entropy as a merit function is also a physically motivated and powerful approach.\n\n3.  **Preservation of Asymptotic Convergence:** As the iterates $f^{(k)}$ approach the solution, the full Newton step ($\\delta f^{(k)}$) becomes smaller and less likely to violate positivity. Consequently, the line search will begin to accept the full step, $\\lambda^{(k)} \\rightarrow 1$. When $\\lambda^{(k)} = 1$, the method reverts to the pure Newton's method, which possesses quadratic local convergence.\n\nThis is a theoretically sound and widely used method.\n\n**Verdict: Correct**\n\n**B. Perform a nonlinear change of variables to an entropy variable $u = \\ln f$...**\n\nThis approach reformulates the problem in terms of a new variable $u$ related to $f$ by $f = e^u$.\n\n1.  **Enforcement of Nonnegativity:** The exponential function's range is $(0, \\infty)$. By representing the distribution function as $f=e^u$, nonnegativity ($f>0$, in fact) is automatically and unconditionally satisfied for any real-valued iterate $u^{(k)}$. This elegantly transforms the constrained problem on $f$ into an unconstrained problem on $u$.\n\n2.  **Recovery of Convergence:** The Newton iteration is applied to the transformed residual equation $\\widehat{R}(u) = R(e^u) = 0$. The new Jacobian $\\widehat{J}(u)$ is related to the original Jacobian $J(f)$ by the chain rule: $\\widehat{J}_{ij}(u) = \\sum_l \\frac{\\partial R_i}{\\partial f_l}\\Big|_{f=e^u} \\frac{\\partial f_l}{\\partial u_j}$. Since $f_l = e^{u_l}$, we have $\\frac{\\partial f_l}{\\partial u_j} = \\delta_{lj} e^{u_l} = \\delta_{lj} f_l$. Thus, $\\widehat{J}(u)=J(e^u) \\text{diag}(e^u)$. The Newton step is well-defined as long as $J(e^u)$ is invertible. The transformation to logarithmic/entropy variables is known to often improve the conditioning and linearity of the problem, aiding convergence.\n\n3.  **Preservation of Asymptotic Convergence:** Newton's method applied to the unconstrained problem $\\widehat{R}(u)=0$ will exhibit quadratic convergence in the $u$ variable, i.e., $\\|u^{(k+1)} - u^*\\| = O(\\|u^{(k)}-u^*\\|^2)$, where $u^*$ is the solution. Since the transformation $f=e^u$ is smooth and locally invertible for $f>0$, quadratic convergence in $u$ implies quadratic convergence in $f$. Near the solution $f^* = e^{u^*}$, we have $\\|f^{(k+1)} - f^*\\| \\approx \\|e^{u^*} (u^{(k+1)} - u^*)\\| = O(\\|u^{(k+1)} - u^*\\|)$, which means the convergence rate is preserved.\n\nThis is a very elegant, well-founded, and physically motivated technique, sometimes referred to as solving in \"entropy variables\".\n\n**Verdict: Correct**\n\n**C. After each Newton update, clip negative components...**\n\nThis option proposes an ad-hoc fix. After computing the standard Newton update $f_{temp} = f^{(k)} + \\delta f^{(k)}$, one simply sets $f^{(k+1)}_i = \\max\\{(f_{temp})_i, 0\\}$.\n\n1.  **Enforcement of Nonnegativity:** By definition, this procedure ensures that all components of $f^{(k+1)}$ are non-negative.\n\n2.  **Recovery of Convergence:** This is the critical weakness. The clipping operation is a nonlinear and non-differentiable projection. The resulting vector $f^{(k+1)}$ is, in general, no longer related to the solution of the linearized system $J(f^{(k)}) \\delta f^{(k)} = -R(f^{(k)})$. The direction $f^{(k+1)} - f^{(k)}$ is not the Newton direction and is not guaranteed to be a descent direction for the residual. This can severely hinder or completely stall the convergence of the iteration. It is a well-known \"fix\" that often performs very poorly in practice.\n\n3.  **Preservation of Asymptotic Convergence:** The method is no longer Newton's method. As soon as clipping becomes active (which might happen even close to a solution if it lies on the boundary), the update is tampered with. This modification destroys the theoretical basis for quadratic convergence. At best, one can hope for linear convergence, and often not even that.\n\nThis approach is not considered \"well-founded\" in numerical analysis because it lacks a sound theoretical basis for convergence.\n\n**Verdict: Incorrect**\n\n**D. Reduce the time step $\\Delta t$ until the undamped Newton method...**\n\nThis option suggests changing the problem itself.\n\n1.  **Enforcement of Nonnegativity:** For a sufficiently small time step $\\Delta t$, the backward Euler solution $f^{n+1}$ is very close to $f^n$. If $f^n$ is a valid distribution function, it becomes an excellent initial guess for the Newton iteration. The Newton steps $\\delta f^{(k)}$ will be small, and the undamped method $f^{(k+1)} = f^{(k)} + \\delta f^{(k)}$ is likely to converge without violating positivity. So, this procedure can lead to a positive solution.\n\n2.  **Recovery of Convergence:** This method does not \"recover convergence\" for the original problem. The problem statement explicitly assumes \"a time step $\\Delta t$ that is large enough to require implicit treatment\". Reducing $\\Delta t$ by orders of magnitude means abandoning the original, challenging problem and solving an easier one. The entire purpose of developing robust implicit solvers is to take large time steps that are determined by accuracy requirements, not by the stability limits of an explicit method or the convergence domain of a simple Newton solver.\n\n3.  **Preservation of Asymptotic Convergence:** For the *new*, small $\\Delta t$, the method is just the standard Newton method and would converge quadratically. However, it fails to solve the intended problem.\n\nThis is a method of evasion, not a solution technique for the problem as posed. It sacrifices the primary efficiency advantage of the implicit scheme.\n\n**Verdict: Incorrect**\n\n**E. Project the tentative update $f^{(k)} + \\delta f^{(k)}$ onto the feasible set...**\n\nThis option describes a projected Newton method. It formalizes the crude clipping in option C into a principled, constrained optimization step.\n\n1.  **Enforcement of Nonnegativity:** The solution $g$ to the minimization problem is explicitly constrained by $g_i \\ge \\epsilon$ for all $i$. Setting $f^{(k+1)} = g$ therefore guarantees that all iterates are positive. Furthermore, this approach can simultaneously enforce other crucial physical constraints like mass conservation ($\\sum w_i g_i = \\text{const}$), which is a significant advantage.\n\n2.  **Recovery of Convergence:** This is a well-established technique in constrained nonlinear optimization. Solving the quadratic program (QP) to find the projection ensures that each iterate remains in the physically valid domain where the residual $R(f)$ and Jacobian $J(f)$ are well-defined. This prevents solver failure and acts as a powerful globalization strategy. The problem is a convex QP (minimizing a quadratic function over a convex set), which can be solved efficiently.\n\n3.  **Preservation of Asymptotic Convergence:** Such projection methods are designed to retain fast local convergence. If the solution $f^{n+1}$ lies in the interior of the feasible set (i.e., $(f^{n+1})_i > \\epsilon$ for all $i$), then for iterates $f^{(k)}$ sufficiently close to $f^{n+1}$, the unprojected Newton step $f^{(k)} + \\delta f^{(k)}$ will also be in the feasible set. In this case, the projection is the identity operator ($g = f^{(k)} + \\delta f^{(k)}$), and the method becomes the standard Newton's method, yielding quadratic convergence. Even when the solution lies on the boundary of the feasible set, these methods can, under suitable conditions, achieve superlinear or quadratic convergence.\n\nThis is a sophisticated and robust approach, constituting a state-of-the-art technique for such problems.\n\n**Verdict: Correct**",
            "answer": "$$\\boxed{ABE}$$"
        },
        {
            "introduction": "The true power of modern implicit solvers lies in their ability to handle multiscale phenomena, which are ubiquitous in fusion plasmas. This final practice  challenges you to implement a complete Asymptotic-Preserving (AP) solver for a kinetic equation with a stiffness parameter $\\epsilon \\ll 1$. By correctly formulating an Implicit-Explicit (IMEX) scheme, you will build a code that remains accurate and stable even with a time step $\\Delta t \\gg \\epsilon$, demonstrating the solver's ability to efficiently capture the slow macroscopic evolution.",
            "id": "3992572",
            "problem": "Consider the spatially homogeneous one-dimensional kinetic Bhatnagar-Gross-Krook (BGK) model for a magnetized fusion plasma in dimensionless units, where the particle velocity is denoted by $v \\in \\mathbb{R}$ and the distribution function is $f(v,t) \\ge 0$. The BGK model with a stiff collision operator and an externally applied isotropic heating source is\n$$\n\\partial_t f(v,t) \\;=\\; \\frac{1}{\\epsilon}\\,\\big(M[n(t),T(t)](v) - f(v,t)\\big) \\;+\\; s(v,t),\n$$\nwhere $\\epsilon > 0$ is the (dimensionless) stiffness parameter, $M[n,T](v)$ is the one-dimensional Maxwellian with density $n$ and temperature $T$, and $s(v,t)$ is a source term that injects energy without changing the density. The Maxwellian is defined as\n$$\nM[n,T](v) \\;=\\; \\frac{n}{\\sqrt{2\\pi T}}\\,\\exp\\!\\left(-\\frac{v^2}{2T}\\right).\n$$\nThe macroscopic moments are defined by the velocity integrals\n$$\nn(t) \\;=\\; \\int_{\\mathbb{R}} f(v,t)\\,dv, \\quad E(t) \\;=\\; \\int_{\\mathbb{R}} v^2\\,f(v,t)\\,dv, \\quad T(t) \\;=\\; \\frac{E(t)}{n(t)}.\n$$\nAssume the energy-injecting source has the specific form\n$$\ns(v,t) \\;=\\; \\frac{q(t)}{2\\,T(t)^2}\\,\\big(v^2 - T(t)\\big)\\,M[n(t),T(t)](v),\n$$\nwith a prescribed scalar heating rate $q(t)$ that is independent of $\\epsilon$. This source satisfies two properties that are consistent with physical modeling for fusion heating: it preserves density, since $\\int_{\\mathbb{R}} s(v,t)\\,dv = 0$, and it injects energy at rate $n(t)\\,q(t)$, since $\\int_{\\mathbb{R}} v^2\\,s(v,t)\\,dv = n(t)\\,q(t)$.\n\nYou must apply an Asymptotic-Preserving Implicit-Explicit (AP IMEX) time discretization to the stiff kinetic equation with $\\epsilon = 10^{-6}$, using a time step $\\Delta t = O(1)$, so that the macroscopic evolution of density and temperature is correctly captured while the kinetic error remains bounded uniformly in $\\epsilon$. Use the following first-order IMEX Euler scheme: treat the collision term implicitly and the source term explicitly. Denote $f^n(v) \\approx f(v, t^n)$ with $t^n = n\\,\\Delta t$. The scheme to be implemented must be based only on the BGK model and the definitions given above, without introducing any unphysical approximations.\n\nYour program must discretize the velocity integral using a uniform grid on $[-V_{\\max}, V_{\\max}]$ with at least $2001$ grid points, trapezoidal quadrature, and dimensionless units. The angle unit is not applicable. The output quantities must be dimensionless. Initialize the distribution with a slightly perturbed Maxwellian that preserves both density and energy,\n$$\nf^0(v) \\;=\\; M[n_0, T_0](v)\\,\\big(1 + \\delta\\,P_4(v;T_0)\\big),\n$$\nwhere $P_4(v;T) = \\frac{v^4 - 6 T v^2 + 3 T^2}{T^2}$ and $\\delta$ is a small dimensionless amplitude chosen so that $f^0(v) \\ge 0$.\n\nFrom first principles, derive the macroscopic update implied by the AP IMEX scheme and implement it consistently with the kinetic update. Quantify the kinetic error at each time step by the $L^2$ norm\n$$\n\\mathcal{E}^n \\;=\\; \\left(\\int_{\\mathbb{R}} \\big(f^n(v) - M[n^n,T^n](v)\\big)^2\\,dv\\right)^{1/2},\n$$\ncomputed numerically on the same velocity grid. Report the maximum kinetic error over all time steps, $\\max_{0 \\le n \\le N}\\mathcal{E}^n$, for each test.\n\nTest Suite:\nImplement the solver for the following parameter sets:\n- Case A (happy path): $\\epsilon = 10^{-6}$, $\\Delta t = 0.5$, $N = 10$ steps, $n_0 = 1.0$, $T_0 = 1.0$, $q(t) \\equiv 0.1$, $\\delta = 10^{-3}$, $V_{\\max} = 20$.\n- Case B (large time step): $\\epsilon = 10^{-6}$, $\\Delta t = 1.5$, $N = 10$ steps, $n_0 = 1.0$, $T_0 = 0.5$, $q(t) \\equiv 0.2$, $\\delta = 10^{-3}$, $V_{\\max} = 20$.\n- Case C (zero heating, edge case): $\\epsilon = 10^{-6}$, $\\Delta t = 2.0$, $N = 10$ steps, $n_0 = 1.0$, $T_0 = 1.2$, $q(t) \\equiv 0.0$, $\\delta = 10^{-3}$, $V_{\\max} = 20$.\n\nFor each case, run the AP IMEX scheme for $N$ steps and compute:\n- The final macroscopic temperature $T^{N}$ in dimensionless units.\n- The maximum kinetic error $\\max_{0 \\le n \\le N}\\mathcal{E}^n$ in dimensionless units.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[T^{N}_{\\text{A}}, \\max \\mathcal{E}_{\\text{A}}, T^{N}_{\\text{B}}, \\max \\mathcal{E}_{\\text{B}}, T^{N}_{\\text{C}}, \\max \\mathcal{E}_{\\text{C}}]$.",
            "solution": "The problem is valid as it presents a well-posed, scientifically grounded task in computational physics. It requires the implementation of an Asymptotic-Preserving (AP) Implicit-Explicit (IMEX) numerical scheme for the Bhatnagar-Gross-Krook (BGK) kinetic equation, a standard model in plasma physics. All parameters, initial conditions, and evaluation metrics are clearly defined, and the problem is free of contradictions or ambiguities.\n\nHerein, we derive the specified AP IMEX scheme and detail its implementation. The governing kinetic equation is:\n$$\n\\partial_t f(v,t) \\;=\\; \\frac{1}{\\epsilon}\\,\\big(M[n(t),T(t)](v) - f(v,t)\\big) \\;+\\; s(v,t)\n$$\nwhere $f(v,t)$ is the particle distribution function, $\\epsilon$ is the stiffness parameter, $M[n,T](v)$ is the Maxwellian distribution, and $s(v,t)$ is an energy-injecting source term. The Maxwellian is defined as:\n$$\nM[n,T](v) \\;=\\; \\frac{n}{\\sqrt{2\\pi T}}\\,\\exp\\!\\left(-\\frac{v^2}{2T}\\right)\n$$\nThe macroscopic density $n(t)$ and temperature $T(t)$ are moments of the distribution function:\n$$\nn(t) \\;=\\; \\int_{\\mathbb{R}} f(v,t)\\,dv, \\quad T(t) \\;=\\; \\frac{1}{n(t)}\\int_{\\mathbb{R}} v^2\\,f(v,t)\\,dv\n$$\nThe source term $s(v,t)$ is given by:\n$$\ns(v,t) \\;=\\; \\frac{q(t)}{2\\,T(t)^2}\\,\\big(v^2 - T(t)\\big)\\,M[n(t),T(t)](v)\n$$\nThis source has the properties $\\int_{\\mathbb{R}} s(v,t) \\,dv = 0$ and $\\int_{\\mathbb{R}} v^2 s(v,t) \\,dv = n(t)q(t)$.\n\nWe are required to apply a first-order IMEX Euler scheme, treating the stiff collision term implicitly and the source term explicitly. Let $f^n(v) \\approx f(v, t^n)$ where $t^n = n\\Delta t$. The discretized equation is:\n$$\n\\frac{f^{n+1}(v) - f^n(v)}{\\Delta t} \\;=\\; \\frac{1}{\\epsilon}\\,\\big(M^{n+1}(v) - f^{n+1}(v)\\big) \\;+\\; s^n(v)\n$$\nwhere $s^n(v) = s(v, t^n; n^n, T^n)$ and $M^{n+1}(v) = M[n^{n+1}, T^{n+1}](v)$. The moments $n^{n+1}$ and $T^{n+1}$ are computed from $f^{n+1}(v)$, making the scheme implicit. To proceed efficiently, we solve for $f^{n+1}(v)$ algebraically first:\n$$\nf^{n+1}(v) \\left(1 + \\frac{\\Delta t}{\\epsilon}\\right) \\;=\\; f^n(v) + \\Delta t \\, s^n(v) + \\frac{\\Delta t}{\\epsilon} M^{n+1}(v)\n$$\n$$\nf^{n+1}(v) \\;=\\; \\frac{f^n(v) + \\Delta t \\, s^n(v) + \\frac{\\Delta t}{\\epsilon} M^{n+1}(v)}{1 + \\frac{\\Delta t}{\\epsilon}}\n$$\nThis is a nonlinear equation for $f^{n+1}(v)$ since $M^{n+1}(v)$ depends on its own moments. The key to an AP scheme is to derive macroscopic update equations by taking moments of this expression.\n\nFirst, we integrate over $v$ to find the update for density, $n^{n+1} = \\int_{\\mathbb{R}} f^{n+1}(v) \\,dv$. Using the known moment properties $\\int M^{n+1} dv = n^{n+1}$, $\\int s^n dv = 0$, and $\\int f^n dv = n^n$:\n$$\nn^{n+1} \\;=\\; \\frac{n^n + \\Delta t (0) + \\frac{\\Delta t}{\\epsilon} n^{n+1}}{1 + \\frac{\\Delta t}{\\epsilon}}\n$$\n$$\nn^{n+1} \\left(1 + \\frac{\\Delta t}{\\epsilon}\\right) \\;=\\; n^n + \\frac{\\Delta t}{\\epsilon} n^{n+1} \\quad\\implies\\quad n^{n+1} = n^n\n$$\nThe scheme exactly conserves density, a crucial property. This simplifies the problem, as $n^{n+1}$ is known from the previous step.\n\nNext, we find the update for energy by integrating $\\int_{\\mathbb{R}} v^2 (\\cdot) \\,dv$. Let $E^n = n^n T^n$. The moment of the source is $\\int v^2 s^n dv = n^n q^n$. The moment of the Maxwellian is $\\int v^2 M^{n+1} dv = n^{n+1}T^{n+1}$. Since $n^{n+1}=n^n$, this is $n^n T^{n+1}$. The energy update is:\n$$\nE^{n+1} \\;=\\; \\frac{E^n + \\Delta t (n^n q^n) + \\frac{\\Delta t}{\\epsilon} (n^n T^{n+1})}{1 + \\frac{\\Delta t}{\\epsilon}}\n$$\nSubstituting $E^{n+1} = n^{n+1}T^{n+1} = n^n T^{n+1}$ and $E^n=n^n T^n$, and dividing by $n^n$ (which is non-zero):\n$$\nT^{n+1} \\;=\\; \\frac{T^n + \\Delta t \\, q^n + \\frac{\\Delta t}{\\epsilon} T^{n+1}}{1 + \\frac{\\Delta t}{\\epsilon}}\n$$\n$$\nT^{n+1} \\left(1 + \\frac{\\Delta t}{\\epsilon}\\right) \\;=\\; T^n + \\Delta t \\, q^n + \\frac{\\Delta t}{\\epsilon} T^{n+1} \\quad\\implies\\quad T^{n+1} = T^n + \\Delta t \\, q^n\n$$\nThis provides a simple, explicit update rule for the temperature.\n\nThe AP IMEX algorithm is as follows:\n1.  Initialize $f^0(v) = M[n_0, T_0](v)\\,\\big(1 + \\delta\\,P_4(v;T_0)\\big)$.\n2.  For each time step $n = 0, 1, \\dots, N-1$:\n    a. Given $f^n(v)$, compute its moments $n^n = \\int f^n dv$ and $T^n = (\\int v^2 f^n dv) / n^n$.\n    b. Predict the macroscopic quantities for the next step: $n^{n+1} = n^n$ and $T^{n+1} = T^n + \\Delta t \\, q(t^n)$.\n    c. Construct the Maxwellian $M^{n+1}(v) = M[n^{n+1}, T^{n+1}](v)$ based on the predicted moments.\n    d. Construct the source term $s^n(v) = s(v, t^n)$ based on the moments at step $n$.\n    e. Update the distribution function using the now-explicit formula:\n       $$\n       f^{n+1}(v) \\;=\\; \\frac{f^n(v) + \\Delta t \\, s^n(v) + \\frac{\\Delta t}{\\epsilon} M^{n+1}(v)}{1 + \\frac{\\Delta t}{\\epsilon}}\n       $$\n3.  After the loop, the final temperature $T^N$ is computed from the moments of the final distribution $f^N(v)$.\n\nThe kinetic error at each step $n$ is calculated as the $L^2$ norm of the deviation from the corresponding Maxwellian:\n$$\n\\mathcal{E}^n \\;=\\; \\left(\\int_{\\mathbb{R}} \\big(f^n(v) - M[n^n,T^n](v)\\big)^2\\,dv\\right)^{1/2}\n$$\nThe maximum of these values over $n = 0, \\dots, N$ is reported for each test case. Velocity integrals are computed numerically using the trapezoidal rule on a uniform grid over $[-V_{\\max}, V_{\\max}]$ with $2001$ points.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the BGK equation using an Asymptotic-Preserving IMEX scheme\n    for the specified test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case A (happy path)\n        {'eps': 1e-6, 'dt': 0.5, 'N_steps': 10, 'n0': 1.0, 'T0': 1.0, \n         'q_func': lambda t: 0.1, 'delta': 1e-3, 'Vmax': 20},\n        # Case B (large time step)\n        {'eps': 1e-6, 'dt': 1.5, 'N_steps': 10, 'n0': 1.0, 'T0': 0.5,\n         'q_func': lambda t: 0.2, 'delta': 1e-3, 'Vmax': 20},\n        # Case C (zero heating, edge case)\n        {'eps': 1e-6, 'dt': 2.0, 'N_steps': 10, 'n0': 1.0, 'T0': 1.2,\n         'q_func': lambda t: 0.0, 'delta': 1e-3, 'Vmax': 20},\n    ]\n\n    results = []\n    \n    # Use 2001 velocity grid points as specified\n    Nv = 2001\n\n    def maxwellian(v, n, T):\n        \"\"\"Computes the 1D Maxwellian distribution.\"\"\"\n        # Handle T=0 case to avoid division by zero, although not expected here\n        if T = 0:\n            # A delta function at v=0, approximated here\n            M = np.zeros_like(v)\n            center_idx = np.argmin(np.abs(v))\n            dv = v[1] - v[0]\n            M[center_idx] = n / dv\n            return M\n        return n / np.sqrt(2 * np.pi * T) * np.exp(-v**2 / (2 * T))\n\n    def p4_poly(v, T):\n        \"\"\"Computes the P4 polynomial.\"\"\"\n        if T = 0: return np.zeros_like(v)\n        return (v**4 - 6 * T * v**2 + 3 * T**2) / T**2\n\n    def source_term(v, n, T, q):\n        \"\"\"Computes the energy-injecting source term.\"\"\"\n        if T = 0: return np.zeros_like(v)\n        M = maxwellian(v, n, T)\n        return (q / (2 * T**2)) * (v**2 - T) * M\n\n    for case in test_cases:\n        eps = case['eps']\n        dt = case['dt']\n        N_steps = case['N_steps']\n        n0 = case['n0']\n        T0 = case['T0']\n        q_func = case['q_func']\n        delta = case['delta']\n        Vmax = case['Vmax']\n\n        # Set up the velocity grid\n        v = np.linspace(-Vmax, Vmax, Nv)\n\n        # Initialize the distribution function\n        f = maxwellian(v, n0, T0) * (1 + delta * p4_poly(v, T0))\n\n        kinetic_errors = []\n        \n        # Main time-stepping loop\n        for n in range(N_steps + 1):\n            # 1. Compute moments and kinetic error for the current distribution f^n\n            current_n = np.trapz(f, v)\n            current_E = np.trapz(v**2 * f, v)\n            current_T = current_E / current_n\n            \n            M_current = maxwellian(v, current_n, current_T)\n            error_sq = np.trapz((f - M_current)**2, v)\n            kinetic_errors.append(np.sqrt(error_sq))\n\n            # Stop after computing the final state at n = N_steps\n            if n == N_steps:\n                final_T = current_T\n                break\n\n            # 2. Predict macroscopic quantities for the next step\n            t_n = n * dt\n            q_n = q_func(t_n)\n            \n            next_n = current_n\n            next_T = current_T + dt * q_n\n            \n            # 3. Construct terms for the update\n            M_next = maxwellian(v, next_n, next_T)\n            S_n = source_term(v, current_n, current_T, q_n)\n            \n            # 4. Update the distribution function f to f^{n+1}\n            dt_over_eps = dt / eps\n            f = (f + dt * S_n + dt_over_eps * M_next) / (1 + dt_over_eps)\n\n        max_kinetic_error = max(kinetic_errors)\n        results.extend([final_T, max_kinetic_error])\n\n    print(f\"[{','.join(f'{r:.6e}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}