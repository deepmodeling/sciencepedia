{
    "hands_on_practices": [
        {
            "introduction": "A fundamental requirement for any physically meaningful interatomic potential is that the forces it predicts must be conservative, meaning they can be derived as the negative gradient of a scalar potential energy function, $\\mathbf{F} = -\\nabla U$. This physical law implies that the force field must be curl-free. This exercise provides a hands-on look at how this constraint can be enforced within a machine learning framework through the use of a regularization term that penalizes non-conservative components of the learned force field.",
            "id": "4250465",
            "problem": "Consider a two-dimensional force field model used in a Machine Learning Interatomic Potential (MLIP) for an electrochemical system at equilibrium, where the predicted force is linear in the coordinates. Let the model be defined by a matrix parameter $\\mathbf{W} \\in \\mathbb{R}^{2 \\times 2}$ with predicted force $\\hat{\\mathbf{F}}(\\mathbf{x}) = \\mathbf{W}\\mathbf{x}$ for $\\mathbf{x} \\in \\mathbb{R}^2$. The ground-truth force is assumed to be conservative, arising from a twice-differentiable scalar potential $U(\\mathbf{x})$, so that $\\mathbf{F}(\\mathbf{x}) = -\\nabla U(\\mathbf{x})$. In this setting, a conservative field implies zero curl on a simply connected domain.\n\nThe training objective combines a force-matching term and a curl-penalizing regularizer. Let the force-matching loss over independent and identically distributed samples $\\{\\mathbf{x}_n, \\mathbf{F}_n\\}_{n=1}^N$ be\n$$\n\\mathcal{L}_{\\mathrm{FM}}(\\mathbf{W}) = \\frac{1}{N}\\sum_{n=1}^{N} \\left\\| \\mathbf{W}\\mathbf{x}_n - \\mathbf{F}_n \\right\\|_2^2.\n$$\nLet the curl penalty be the mean-squared scalar curl in two dimensions. For the linear model, the scalar curl is constant in space and equal to\n$$\n\\omega = \\frac{\\partial \\hat{F}_y}{\\partial x} - \\frac{\\partial \\hat{F}_x}{\\partial y} = W_{21} - W_{12}.\n$$\nDefine the regularizer as\n$$\n\\mathcal{R}_{\\mathrm{curl}}(\\mathbf{W}) = \\omega^2 = \\left(W_{21} - W_{12}\\right)^2.\n$$\nThe total loss is\n$$\n\\mathcal{L}(\\mathbf{W}) = \\mathcal{L}_{\\mathrm{FM}}(\\mathbf{W}) + \\lambda \\,\\mathcal{R}_{\\mathrm{curl}}(\\mathbf{W}),\n$$\nwith regularization weight $\\lambda \\ge 0$.\n\nYou are to show, from first principles, how the curl penalty enforces conservativity and to compute its effect on the training dynamics under gradient descent with learning rate $\\eta > 0$. Assume:\n- The sample covariance of inputs is isotropic, $\\boldsymbol{\\Sigma} = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top] = \\sigma^2 \\mathbf{I}_2$ for some $\\sigma^2 > 0$.\n- The ground-truth force is conservative with a symmetric matrix $\\mathbf{W}_{\\star}$, i.e., $\\mathbf{F}(\\mathbf{x}) = \\mathbf{W}_{\\star}\\mathbf{x}$ with $\\mathbf{W}_{\\star}^\\top = \\mathbf{W}_{\\star}$.\n\nIntroduce the symmetric and antisymmetric components of $\\mathbf{W}$:\n$$\n\\mathbf{C} = \\frac{1}{2}\\left(\\mathbf{W} + \\mathbf{W}^\\top\\right), \\quad \\mathbf{S} = \\frac{1}{2}\\left(\\mathbf{W} - \\mathbf{W}^\\top\\right).\n$$\nNote that the curl depends only on $\\mathbf{S}$. Derive the gradient descent update for $\\mathbf{S}$ under one training step and show that, under the isotropic covariance assumption, the antisymmetric part is multiplied by a scalar contraction factor\n$$\n\\phi_{\\lambda} = 1 - 2\\eta\\left(\\sigma^2 + 2\\lambda\\right)\n$$\nper step, while with no regularizer it is multiplied by\n$$\n\\phi_{0} = 1 - 2\\eta \\sigma^2.\n$$\nLet $a_0$ denote the initial amplitude of the antisymmetric component (for example, the off-diagonal entry $S_{12}$ in the basis $\\{\\mathbf{e}_1,\\mathbf{e}_2\\}$). After $K$ steps, the amplitude scales as $|a_K| = |a_0|\\,|\\phi|^K$.\n\nYour task is to implement a program that, given a small test suite of parameter sets $(\\eta, \\sigma^2, \\lambda, a_0, K)$, computes for each test case the ratio\n$$\nr = \\frac{|\\phi_{\\lambda}|^K}{|\\phi_0|^K},\n$$\nwhich quantifies the relative decay (or growth) of the antisymmetric component with the curl penalty versus without it. This ratio is unitless.\n\nUse the following test suite, designed to cover a general case, a boundary case, an edge case showing slower contraction due to the fixed learning rate, and a critically damped case:\n- Test $1$: $(\\eta, \\sigma^2, \\lambda, a_0, K) = (0.05, 1.0, 0.5, 1.0, 10)$.\n- Test $2$: $(\\eta, \\sigma^2, \\lambda, a_0, K) = (0.05, 1.0, 0.0, 1.0, 10)$.\n- Test $3$: $(\\eta, \\sigma^2, \\lambda, a_0, K) = (0.45, 1.0, 0.2, 1.0, 10)$.\n- Test $4$: $(\\eta, \\sigma^2, \\lambda, a_0, K) = (0.4, 0.5, 0.375, 1.0, 1)$.\n\nYour program must:\n- Compute $r$ for each test case exactly as defined above.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces, in the order of the tests, for example, $[r_1,r_2,r_3,r_4]$.",
            "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the principles of computational physics and machine learning, well-posed with a clear objective, and internally consistent.\n\nThe task is to derive the gradient descent dynamics for the antisymmetric component of a weight matrix $\\mathbf{W}$ in a linear force field model and then compute a ratio quantifying the effect of a curl-based regularizer.\n\nFirst, we establish the theoretical framework. The total loss function is given by:\n$$\n\\mathcal{L}(\\mathbf{W}) = \\mathcal{L}_{\\mathrm{FM}}(\\mathbf{W}) + \\lambda \\,\\mathcal{R}_{\\mathrm{curl}}(\\mathbf{W})\n$$\nwhere $\\mathcal{L}_{\\mathrm{FM}}$ is the force-matching loss and $\\mathcal{R}_{\\mathrm{curl}}$ is the curl regularization term. In the large-sample limit ($N \\to \\infty$), we can replace the summation in $\\mathcal{L}_{\\mathrm{FM}}$ with an expectation over the data distribution:\n$$\n\\mathcal{L}_{\\mathrm{FM}}(\\mathbf{W}) = \\mathbb{E}_{\\mathbf{x}} \\left[ \\left\\| \\mathbf{W}\\mathbf{x} - \\mathbf{F}(\\mathbf{x}) \\right\\|_2^2 \\right]\n$$\nGiven the ground-truth force is $\\mathbf{F}(\\mathbf{x}) = \\mathbf{W}_{\\star}\\mathbf{x}$, this becomes:\n$$\n\\mathcal{L}_{\\mathrm{FM}}(\\mathbf{W}) = \\mathbb{E}_{\\mathbf{x}} \\left[ \\left\\| (\\mathbf{W} - \\mathbf{W}_{\\star})\\mathbf{x} \\right\\|_2^2 \\right]\n$$\nLet $\\Delta\\mathbf{W} = \\mathbf{W} - \\mathbf{W}_{\\star}$. The expectation can be written as:\n$$\n\\mathbb{E}_{\\mathbf{x}} \\left[ \\mathbf{x}^\\top (\\Delta\\mathbf{W})^\\top (\\Delta\\mathbf{W}) \\mathbf{x} \\right]\n$$\nUsing the property $\\mathbb{E}[\\mathbf{z}^\\top \\mathbf{A} \\mathbf{z}] = \\text{Tr}(\\mathbf{A} \\mathbb{E}[\\mathbf{z}\\mathbf{z}^\\top])$, we have:\n$$\n\\mathcal{L}_{\\mathrm{FM}}(\\mathbf{W}) = \\text{Tr}\\left( (\\Delta\\mathbf{W})^\\top (\\Delta\\mathbf{W}) \\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top] \\right)\n$$\nUsing the assumption of isotropic input covariance, $\\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top] = \\sigma^2 \\mathbf{I}_2$:\n$$\n\\mathcal{L}_{\\mathrm{FM}}(\\mathbf{W}) = \\text{Tr}\\left( (\\Delta\\mathbf{W})^\\top (\\Delta\\mathbf{W}) \\sigma^2 \\mathbf{I}_2 \\right) = \\sigma^2 \\text{Tr}\\left( (\\Delta\\mathbf{W})^\\top \\Delta\\mathbf{W} \\right) = \\sigma^2 \\|\\mathbf{W} - \\mathbf{W}_{\\star}\\|_F^2\n$$\nwhere $\\|\\cdot\\|_F$ denotes the Frobenius norm.\n\nNext, we decompose the weight matrix $\\mathbf{W}$ and the error $\\Delta\\mathbf{W}$ into symmetric and antisymmetric components. Let $\\mathbf{W} = \\mathbf{C} + \\mathbf{S}$ and $\\mathbf{W}_{\\star} = \\mathbf{C}_{\\star}$, where $\\mathbf{C} = \\frac{1}{2}(\\mathbf{W} + \\mathbf{W}^\\top)$ and $\\mathbf{C}_{\\star} = \\mathbf{W}_{\\star}$ are symmetric, and $\\mathbf{S} = \\frac{1}{2}(\\mathbf{W} - \\mathbf{W}^\\top)$ is antisymmetric. The true weight matrix $\\mathbf{W}_{\\star}$ is symmetric, so its antisymmetric part is zero. The error is $\\Delta\\mathbf{W} = (\\mathbf{C} - \\mathbf{C}_{\\star}) + \\mathbf{S}$.\nThe Frobenius norm of a matrix is the square root of the sum of squares of its elements. For a matrix that is a sum of a symmetric and an antisymmetric matrix, the squared Frobenius norm decouples due to orthogonality:\n$$\n\\|\\Delta\\mathbf{W}\\|_F^2 = \\|\\mathbf{C} - \\mathbf{C}_{\\star}\\|_F^2 + \\|\\mathbf{S}\\|_F^2\n$$\nThus, the force-matching loss is:\n$$\n\\mathcal{L}_{\\mathrm{FM}}(\\mathbf{W}) = \\sigma^2 \\left( \\|\\mathbf{C} - \\mathbf{C}_{\\star}\\|_F^2 + \\|\\mathbf{S}\\|_F^2 \\right)\n$$\nThe curl regularizer is $\\mathcal{R}_{\\mathrm{curl}}(\\mathbf{W}) = (W_{21} - W_{12})^2$. The antisymmetric part $\\mathbf{S}$ has components $S_{11}=S_{22}=0$, $S_{21} = \\frac{1}{2}(W_{21}-W_{12})$, and $S_{12} = \\frac{1}{2}(W_{12}-W_{21}) = -S_{21}$.\nThe regularizer can be expressed in terms of the components of $\\mathbf{S}$:\n$$\n\\mathcal{R}_{\\mathrm{curl}}(\\mathbf{W}) = (2 S_{21})^2 = 4S_{21}^2 = 2 (S_{21}^2 + S_{12}^2) = 2 \\|\\mathbf{S}\\|_F^2\n$$\nThe total loss function is therefore decoupled with respect to the symmetric and antisymmetric components:\n$$\n\\mathcal{L}(\\mathbf{W}) = \\sigma^2 \\|\\mathbf{C} - \\mathbf{C}_{\\star}\\|_F^2 + \\sigma^2 \\|\\mathbf{S}\\|_F^2 + \\lambda (2 \\|\\mathbf{S}\\|_F^2)\n$$\n$$\n\\mathcal{L}(\\mathbf{W}) = \\sigma^2 \\|\\mathbf{C} - \\mathbf{C}_{\\star}\\|_F^2 + (\\sigma^2 + 2\\lambda) \\|\\mathbf{S}\\|_F^2\n$$\nThe gradient of the loss with respect to the matrix $\\mathbf{W}$ is required for the gradient descent update. The general derivative rules are $\\nabla_{\\mathbf{X}} \\|\\mathbf{X}-\\mathbf{A}\\|_F^2 = 2(\\mathbf{X}-\\mathbf{A})$ and $\\nabla_{\\mathbf{X}} \\|\\mathbf{S}_{\\mathbf{X}}\\|_F^2 = 2\\mathbf{S}_{\\mathbf{X}}$, where $\\mathbf{S}_{\\mathbf{X}}$ is the antisymmetric part of $\\mathbf{X}$. Applying these:\n$$\n\\nabla_{\\mathbf{W}} \\mathcal{L}(\\mathbf{W}) = \\nabla_{\\mathbf{W}} \\left( \\sigma^2 \\|\\mathbf{W} - \\mathbf{W}_{\\star}\\|_F^2 \\right) + \\nabla_{\\mathbf{W}} \\left( 2\\lambda \\|\\mathbf{S}\\|_F^2 \\right)\n$$\nThe first term gives $2\\sigma^2(\\mathbf{W} - \\mathbf{W}_{\\star})$. The gradient of the regularizer is $\\nabla_{\\mathbf{W}}(2\\lambda \\|\\mathbf{S}\\|_F^2) = 4\\lambda \\mathbf{S}$. Combining these:\n$$\n\\nabla_{\\mathbf{W}} \\mathcal{L}(\\mathbf{W}) = 2\\sigma^2 (\\mathbf{W} - \\mathbf{W}_{\\star}) + 4\\lambda \\mathbf{S}\n$$\nSubstituting $\\mathbf{W} = \\mathbf{C} + \\mathbf{S}$ and $\\mathbf{W}_{\\star} = \\mathbf{C}_{\\star}$:\n$$\n\\nabla_{\\mathbf{W}} \\mathcal{L}(\\mathbf{W}) = 2\\sigma^2 ((\\mathbf{C} - \\mathbf{C}_{\\star}) + \\mathbf{S}) + 4\\lambda \\mathbf{S} = 2\\sigma^2 (\\mathbf{C} - \\mathbf{C}_{\\star}) + (2\\sigma^2 + 4\\lambda) \\mathbf{S}\n$$\nThe gradient descent update rule for $\\mathbf{W}$ at step $k$ is $\\mathbf{W}_{k+1} = \\mathbf{W}_k - \\eta \\nabla_{\\mathbf{W}} \\mathcal{L}(\\mathbf{W}_k)$. The update for the antisymmetric component $\\mathbf{S}$ is the antisymmetric part of the update for $\\mathbf{W}$:\n$$\n\\mathbf{S}_{k+1} = \\text{antisymm}(\\mathbf{W}_{k+1}) = \\mathbf{S}_k - \\eta \\, \\text{antisymm}\\left( \\nabla_{\\mathbf{W}} \\mathcal{L}(\\mathbf{W}_k) \\right)\n$$\nThe gradient's symmetric part is $2\\sigma^2 (\\mathbf{C}_k - \\mathbf{C}_{\\star})$ and its antisymmetric part is $(2\\sigma^2 + 4\\lambda) \\mathbf{S}_k$. Thus, the update for $\\mathbf{S}$ is:\n$$\n\\mathbf{S}_{k+1} = \\mathbf{S}_k - \\eta (2\\sigma^2 + 4\\lambda) \\mathbf{S}_k = (1 - 2\\eta(\\sigma^2 + 2\\lambda)) \\mathbf{S}_k\n$$\nThis demonstrates that the antisymmetric component $\\mathbf{S}$ is multiplied by a scalar contraction factor $\\phi_{\\lambda} = 1 - 2\\eta(\\sigma^2 + 2\\lambda)$ at each step. In the absence of the regularizer ($\\lambda=0$), the factor is $\\phi_0 = 1 - 2\\eta\\sigma^2$. The regularizer increases the rate of decay of the non-conservative component $\\mathbf{S}$ towards zero, provided the learning rate $\\eta$ is chosen appropriately to ensure $|\\phi_{\\lambda}| < 1$.\n\nThe ratio $r$ that quantifies the relative decay after $K$ steps is:\n$$\nr = \\frac{|\\phi_{\\lambda}|^K}{|\\phi_0|^K} = \\left( \\frac{|\\phi_{\\lambda}|}{|\\phi_0|} \\right)^K\n$$\nThis ratio is computed for the provided test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the relative decay ratio of the antisymmetric component of a \n    weight matrix in a linear MLIP model, with and without a curl penalty.\n    \"\"\"\n    \n    # Test suite of parameters: (eta, sigma^2, lambda, a_0, K)\n    # a_0 is not needed for the calculation of the ratio r.\n    test_cases = [\n        # Test 1: General case\n        (0.05, 1.0, 0.5, 1.0, 10),\n        # Test 2: Boundary case (lambda = 0)\n        (0.05, 1.0, 0.0, 1.0, 10),\n        # Test 3: Edge case (slower contraction)\n        (0.45, 1.0, 0.2, 1.0, 10),\n        # Test 4: Critically damped case\n        (0.4, 0.5, 0.375, 1.0, 1),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        eta, sigma_sq, lam, a0, K = case\n        \n        # Contraction factor with the curl regularizer\n        # phi_lambda = 1 - 2*eta*(sigma^2 + 2*lambda)\n        phi_lambda = 1 - 2 * eta * (sigma_sq + 2 * lam)\n        \n        # Contraction factor without the regularizer (lambda = 0)\n        # phi_0 = 1 - 2*eta*sigma^2\n        phi_0 = 1 - 2 * eta * sigma_sq\n        \n        # The ratio r quantifies the relative decay (or growth) of the \n        # antisymmetric component after K steps.\n        # r = (|phi_lambda|^K) / (|phi_0|^K)\n        # Handle the case where phi_0 is zero to avoid division by zero.\n        # This case is not present in the test suite but is good practice.\n        if abs(phi_0) == 0:\n            # If phi_0 is 0, the unregularized component vanishes in one step.\n            # If phi_lambda is also 0, the ratio is indeterminate, but 1.0 is a\n            # reasonable interpretation (both vanish instantly).\n            # If phi_lambda is not 0, the regularized component does not vanish,\n            # so the relative decay is infinite. For the scope of this problem,\n            # we assume phi_0 is non-zero.\n            if abs(phi_lambda) == 0:\n                r = 1.0\n            else:\n                r = float('inf')\n        else:\n            r = (abs(phi_lambda)**K) / (abs(phi_0)**K)\n            \n        results.append(r)\n\n    # Format the final output as a comma-separated list in brackets,\n    # with no spaces.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "State-of-the-art machine learning potentials are built upon the principle of equivariance, which ensures that the model's predictions transform consistently with the physical symmetries of the system. For atomistic systems, this means respecting the translations and rotations of the Euclidean group, E(3). In this practice, you will construct a simple E(3)-equivariant message-passing layer, providing a foundational understanding of how features representing scalars, vectors, and tensors are propagated while preserving these critical symmetries.",
            "id": "4250488",
            "problem": "You are tasked with constructing and validating a Euclidean group in three dimensions (E(3))-equivariant message-passing architecture tailored for ionic liquids within the framework of computational electrochemistry. The goal is to implement an algorithmic pipeline that demonstrates the propagation of irreducible tensor features under spatial rotations. The Euclidean group in three dimensions (E(3)) consists of rotations and translations acting on three-dimensional space. Rotational equivariance is defined by the property that rotating the input configuration by a rotation matrix $\\mathbf{R} \\in \\mathrm{SO}(3)$ corresponds to rotating the output features by a consistent representation of $\\mathbf{R}$, while translational invariance is enforced by constructing operations from relative position vectors. You must formulate a message-passing scheme over a graph where nodes correspond to ions and edges correspond to neighbor relations determined by a distance cutoff.\n\nYour architecture must use irreducible feature types that transform under rotations according to the following rules:\n- Scalars (spherical harmonic degree $l=0$): features $s_i$ satisfy $s_i' = s_i$ under a rotation $\\mathbf{R}$.\n- Vectors (spherical harmonic degree $l=1$): features $\\mathbf{v}_i$ satisfy $\\mathbf{v}_i' = \\mathbf{R}\\mathbf{v}_i$ under a rotation $\\mathbf{R}$.\n- Symmetric traceless second-order tensors (spherical harmonic degree $l=2$): features $\\mathbf{T}_i$ satisfy $\\mathbf{T}_i' = \\mathbf{R}\\mathbf{T}_i\\mathbf{R}^{\\top}$ under a rotation $\\mathbf{R}$.\n\nStart from the following conceptual bases:\n- The Euclidean group action via rotations $\\mathbf{R}$ and translations $\\mathbf{a}$ on positions $\\mathbf{r}_i$ uses rotated positions $\\mathbf{r}_i' = \\mathbf{R}\\mathbf{r}_i + \\mathbf{a}$. Enforce translational invariance by using only relative positions $\\mathbf{r}_{ij} = \\mathbf{r}_j - \\mathbf{r}_i$ and unit direction vectors $\\hat{\\mathbf{r}}_{ij} = \\mathbf{r}_{ij}/\\|\\mathbf{r}_{ij}\\|$.\n- Rotational equivariance requires that any constructed operation built from $\\hat{\\mathbf{r}}_{ij}$ and isotropic (radial) functions of $\\|\\mathbf{r}_{ij}\\|$ transforms consistently under $\\mathbf{R}$. Isotropic functions of $\\|\\mathbf{r}_{ij}\\|$ are rotation-invariant scalars.\n- The symmetric traceless projection of a second-order tensor $\\mathbf{M}$ is defined as $\\mathrm{STT}(\\mathbf{M}) = \\tfrac{1}{2}(\\mathbf{M} + \\mathbf{M}^{\\top}) - \\tfrac{\\mathrm{tr}(\\mathbf{M})}{3}\\mathbf{I}$, where $\\mathbf{I}$ is the identity matrix.\n\nImplement a single message-passing layer that:\n- Initializes node features $s_i$ (scalar), $\\mathbf{v}_i$ (vector), and $\\mathbf{T}_i$ (symmetric traceless tensor) using sums over neighbors with isotropic radial functions of $\\|\\mathbf{r}_{ij}\\|$ multiplied by appropriate geometric covariants constructed from $\\hat{\\mathbf{r}}_{ij}$ and ionic charges $q_j$.\n- Updates features $s_i$, $\\mathbf{v}_i$, and $\\mathbf{T}_i$ by aggregating messages from neighbors $j$ using only rotationally consistent contractions and projections derived from $\\hat{\\mathbf{r}}_{ij}$, $s_j$, $\\mathbf{v}_j$, and $\\mathbf{T}_j$. The update must preserve E(3)-equivariance: under a global rotation by $\\mathbf{R}$, the updated features must transform exactly as $s_i'$, $\\mathbf{v}_i'$, and $\\mathbf{T}_i'$ above.\n- Uses isotropic scalar weights (constants) in the update to avoid introducing any preferred directions, ensuring rotational symmetry.\n\nDemonstrate rotational equivariance numerically by performing the following verification for each test case:\n- Compute updated features for an original configuration of positions and charges.\n- Apply a rotation $\\mathbf{R}$ to all positions, recompute updated features, and separately rotate the originally computed updated features via the appropriate representation for each irreducible type ($l=0$, $l=1$, $l=2$).\n- Report the maximum discrepancy across nodes between the recomputed features and the rotated features: for scalars the absolute difference, for vectors the Euclidean norm of the difference, and for tensors the Frobenius norm of the difference. The discrepancy must be expressed as dimensionless floats.\n\nPhysical and numerical conventions:\n- Positions are given in nanometers (nm).\n- Rotation angles must be interpreted in radians.\n- All distances are computed in nanometers, and radial functions must depend only on $\\|\\mathbf{r}_{ij}\\|$ measured in nanometers.\n- The final outputs are dimensionless floats representing numerical residuals; do not include physical units in the output.\n\nTest Suite:\nUse the following parameter sets as the input configurations. For each case, the program must apply a single rotation specified by a rotation axis and angle, and compute the residuals as described.\n\nCase $1$ (general ionic liquid cluster, happy path):\n- Positions (nm): $[\\,[0.0,\\,0.0,\\,0.0],\\,[0.4,\\,0.1,\\,-0.2],\\,[-0.3,\\,0.5,\\,0.2],\\,[0.2,\\,-0.4,\\,0.3],\\,[-0.5,\\,-0.1,\\,-0.4],\\,[0.1,\\,0.6,\\,-0.3]\\,]$\n- Charges (elementary charge units): $[\\,1.0,\\,-1.0,\\,1.0,\\,-1.0,\\,1.0,\\,-1.0\\,]$\n- Cutoff (nm): $0.8$\n- Rotation axis: $[\\,0.3,\\,0.7,\\,0.6\\,]$\n- Rotation angle (radians): $0.6$\n\nCase $2$ (colinear neighbors, structural degeneracy):\n- Positions (nm): $[\\,[\\,-0.5,\\,0.0,\\,0.0],\\,[\\,0.0,\\,0.0,\\,0.0],\\,[\\,0.5,\\,0.0,\\,0.0]\\,]$\n- Charges (elementary charge units): $[\\,1.0,\\,-1.0,\\,1.0\\,]$\n- Cutoff (nm): $1.0$\n- Rotation axis: $[\\,0.0,\\,0.0,\\,1.0\\,]$\n- Rotation angle (radians): $\\pi/3$\n\nCase $3$ (no neighbors due to very small cutoff, edge case):\n- Positions (nm): $[\\,[0.0,\\,0.0,\\,0.0],\\,[0.4,\\,0.1,\\,-0.2],\\,[-0.3,\\,0.5,\\,0.2],\\,[0.2,\\,-0.4,\\,0.3],\\,[-0.5,\\,-0.1,\\,-0.4],\\,[0.1,\\,0.6,\\,-0.3]\\,]$\n- Charges (elementary charge units): $[\\,1.0,\\,-1.0,\\,1.0,\\,-1.0,\\,1.0,\\,-1.0\\,]$\n- Cutoff (nm): $0.05$\n- Rotation axis: $[\\,0.2,\\,-0.4,\\,0.9\\,]$\n- Rotation angle (radians): $1.2$\n\nCase $4$ (zero rotation angle, boundary condition):\n- Positions (nm): $[\\,[0.2,\\,0.2,\\,0.2],\\,[\\,-0.2,\\,-0.2,\\,-0.2],\\,[\\,0.3,\\,-0.1,\\,0.4]\\,]$\n- Charges (elementary charge units): $[\\,1.0,\\,-1.0,\\,1.0\\,]$\n- Cutoff (nm): $0.5$\n- Rotation axis: $[\\,1.0,\\,0.0,\\,0.0\\,]$\n- Rotation angle (radians): $0.0$\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a three-element list of floats corresponding to $[\\,\\text{scalar\\_error},\\,\\text{vector\\_error},\\,\\text{tensor\\_error}\\,]$. For example: $[[e_{s,1},e_{v,1},e_{T,1}],[e_{s,2},e_{v,2},e_{T,2}],[e_{s,3},e_{v,3},e_{T,3}],[e_{s,4},e_{v,4},e_{T,4}]]$.",
            "solution": "The problem is valid. It is scientifically grounded in the principles of E(3)-equivariant deep learning, specifically concerning the construction of message-passing layers that respect Euclidean symmetries. The problem is well-posed, with all necessary data and definitions provided for a unique and verifiable solution. The language is objective and formal.\n\nThe core of the problem is to implement a single message-passing layer for a graph of ions and to numerically verify its rotational equivariance. The nodes of the graph are ions, characterized by their positions $\\mathbf{r}_i$ and charges $q_i$. The edges are defined by a distance cutoff $r_{\\text{cut}}$. The features on each node $i$ are an irreducible representation of the rotation group $\\mathrm{SO}(3)$, consisting of a scalar $s_i$ (type $l=0$), a vector $\\mathbf{v}_i$ (type $l=1$), and a symmetric traceless second-order tensor $\\mathbf{T}_i$ (type $l=2$).\n\nUnder a rotation $\\mathbf{R} \\in \\mathrm{SO}(3)$, these features must transform as:\n- $s_i' = s_i$\n- $\\mathbf{v}_i' = \\mathbf{R}\\mathbf{v}_i$\n- $\\mathbf{T}_i' = \\mathbf{R}\\mathbf{T}_i\\mathbf{R}^{\\top}$\n\nTranslational invariance is guaranteed by constructing all operations from relative position vectors $\\mathbf{r}_{ij} = \\mathbf{r}_j - \\mathbf{r}_i$. Rotational equivariance is achieved by ensuring that all operations combining geometric information (derived from $\\mathbf{r}_{ij}$) with node features are valid tensor products in the group-theoretical sense.\n\nThe process involves two main stages: feature initialization and feature update.\n\n**Step 1: Feature Initialization**\n\nFirst, initial features ($s_i^{(0)}, \\mathbf{v}_i^{(0)}, \\mathbf{T}_i^{(0)}$) are computed for each ion $i$ by summing contributions from its neighbors $j$ (where $\\|\\mathbf{r}_{ij}\\| < r_{\\text{cut}}$). These initial features are constructed from the most fundamental equivariant quantities available: the scalar charges $q_j$ and the vectorial unit direction vectors $\\hat{\\mathbf{r}}_{ij} = \\mathbf{r}_{ij} / \\|\\mathbf{r}_{ij}\\|$.\n\n- **Scalar Initialization ($l=0$):** A scalar can be formed by summing other scalars. We use the neighbor charges $q_j$.\n$$s_i^{(0)} = \\sum_{j \\in \\mathcal{N}(i)} q_j$$\nSince $q_j$ is a scalar, $s_i^{(0)}$ is also a scalar and transforms correctly (i.e., is invariant).\n\n- **Vector Initialization ($l=1$):** A vector can be formed by multiplying a scalar ($q_j$) by a vector ($\\hat{\\mathbf{r}}_{ij}$).\n$$\\mathbf{v}_i^{(0)} = \\sum_{j \\in \\mathcal{N}(i)} q_j \\hat{\\mathbf{r}}_{ij}$$\nUnder rotation, $\\hat{\\mathbf{r}}_{ij} \\rightarrow \\mathbf{R}\\hat{\\mathbf{r}}_{ij}$, so $\\mathbf{v}_i^{(0)} \\rightarrow \\mathbf{R}\\mathbf{v}_i^{(0)}$, which is the correct transformation for a vector.\n\n- **Tensor Initialization ($l=2$):** A symmetric traceless second-order tensor can be constructed from the outer product of the unit vector with itself, $\\hat{\\mathbf{r}}_{ij} \\hat{\\mathbf{r}}_{ij}^{\\top}$, followed by a projection. The symmetric traceless part of $\\hat{\\mathbf{r}}_{ij} \\hat{\\mathbf{r}}_{ij}^{\\top}$ is $\\hat{\\mathbf{r}}_{ij} \\hat{\\mathbf{r}}_{ij}^{\\top} - \\frac{1}{3}\\mathbf{I}$, since $\\mathrm{tr}(\\hat{\\mathbf{r}}_{ij} \\hat{\\mathbf{r}}_{ij}^{\\top}) = \\|\\hat{\\mathbf{r}}_{ij}\\|^2 = 1$.\n$$\\mathbf{T}_i^{(0)} = \\sum_{j \\in \\mathcal{N}(i)} q_j \\left( \\hat{\\mathbf{r}}_{ij} \\hat{\\mathbf{r}}_{ij}^{\\top} - \\frac{1}{3}\\mathbf{I} \\right)$$\nThis construction correctly transforms as a type $l=2$ tensor.\n\n**Step 2: Feature Update**\n\nNext, we define a message-passing update rule to compute the next layer of features ($s_i^{(1)}, \\mathbf{v}_i^{(1)}, \\mathbf{T}_i^{(1)}$) based on the initial features of the neighboring nodes. The update for each feature type must be a sum of terms that all transform according to that feature's type. This is achieved by forming valid contractions (tensor products) between features from a neighbor node $j$ and geometric quantities derived from $\\hat{\\mathbf{r}}_{ij}$. For simplicity and demonstration, we choose one or two representative interaction terms for each update.\n\n- **Scalar Update ($l=0 \\leftarrow (l=0 \\otimes l=0) \\oplus (l=1 \\otimes l=1)$):**\nA scalar can be formed from a neighbor's scalar $s_j^{(0)}$ (a trivial product) or by the dot product of two vectors, $\\mathbf{v}_j^{(0)}$ and $\\hat{\\mathbf{r}}_{ij}$.\n$$s_i^{(1)} = \\sum_{j \\in \\mathcal{N}(i)} \\left( s_j^{(0)} + \\mathbf{v}_j^{(0)} \\cdot \\hat{\\mathbf{r}}_{ij} \\right)$$\n\n- **Vector Update ($l=1 \\leftarrow (l=0 \\otimes l=1) \\oplus (l=1 \\otimes l=0) \\oplus (l=2 \\otimes l=1)$):**\nA vector can be formed by multiplying a scalar by a vector (e.g., $s_j^{(0)}\\hat{\\mathbf{r}}_{ij}$), a vector by a scalar (e.g., $\\mathbf{v}_j^{(0)}$), or by contracting a tensor with a vector (e.g., $\\mathbf{T}_j^{(0)}\\hat{\\mathbf{r}}_{ij}$).\n$$\\mathbf{v}_i^{(1)} = \\sum_{j \\in \\mathcal{N}(i)} \\left( s_j^{(0)}\\hat{\\mathbf{r}}_{ij} + \\mathbf{v}_j^{(0)} + \\mathbf{T}_j^{(0)}\\hat{\\mathbf{r}}_{ij} \\right)$$\n\n- **Tensor Update ($l=2 \\leftarrow (l=0 \\otimes l=2) \\oplus (l=1 \\otimes l=1)$):**\nA type $l=2$ tensor can be formed by multiplying a scalar by a tensor (e.g., $s_j^{(0)}(\\hat{\\mathbf{r}}_{ij}\\hat{\\mathbf{r}}_{ij}^\\top - \\frac{1}{3}\\mathbf{I})$) or from the symmetric traceless part of the outer product of two vectors.\n$$\\mathbf{T}_i^{(1)} = \\sum_{j \\in \\mathcal{N}(i)} \\left( s_j^{(0)}\\left(\\hat{\\mathbf{r}}_{ij}\\hat{\\mathbf{r}}_{ij}^\\top - \\frac{1}{3}\\mathbf{I}\\right) + \\mathrm{STT}\\left(\\mathbf{v}_j^{(0)}\\hat{\\mathbf{r}}_{ij}^\\top + \\hat{\\mathbf{r}}_{ij}(\\mathbf{v}_j^{(0)})^\\top\\right) \\right)$$\nwhere $\\mathrm{STT}(\\mathbf{M}) = \\frac{1}{2}(\\mathbf{M} + \\mathbf{M}^{\\top}) - \\frac{\\mathrm{tr}(\\mathbf{M})}{3}\\mathbf{I}$ is the symmetric traceless projection operator.\n\n**Step 3: Numerical Verification**\n\nTo verify equivariance, we perform the following procedure:\n1.  Compute the updated features $\\{s_i^{(1)}, \\mathbf{v}_i^{(1)}, \\mathbf{T}_i^{(1)}\\}_{\\text{orig}}$ for the original ion positions.\n2.  Construct a rotation matrix $\\mathbf{R}$ from the given axis and angle.\n3.  Apply the rotation to the original positions: $\\mathbf{r}_i' = \\mathbf{R}\\mathbf{r}_i$.\n4.  Recompute the updated features $\\{s_i^{(1)}, \\mathbf{v}_i^{(1)}, \\mathbf{T}_i^{(1)}\\}_{\\text{recomputed}}$ using these new positions.\n5.  Separately, transform the original results using the defined transformation rules:\n    - $s_{i, \\text{transformed}}^{(1)} = s_{i, \\text{orig}}^{(1)}$\n    - $\\mathbf{v}_{i, \\text{transformed}}^{(1)} = \\mathbf{R} \\mathbf{v}_{i, \\text{orig}}^{(1)}$\n    - $\\mathbf{T}_{i, \\text{transformed}}^{(1)} = \\mathbf{R} \\mathbf{T}_{i, \\text{orig}}^{(1)} \\mathbf{R}^{\\top}$\n6.  The E(3)-equivariant property dictates that the recomputed features must equal the transformed features, i.e., $\\{h\\}_{\\text{recomputed}} = \\{h\\}_{\\text{transformed}}$. Due to floating-point arithmetic, we check this by calculating the maximum discrepancy over all ions for each feature type. The errors are calculated as the absolute difference for scalars, the Euclidean norm of the difference for vectors, and the Frobenius norm of the difference for tensors.\n\nThe provided Python code implements this entire pipeline. The `rotation_matrix_from_axis_angle` function generates the rotation matrix $\\mathbf{R}$ using Rodrigues' formula. The `equivariant_pipeline` function encapsulates the feature initialization and update steps. The main `solve` function iterates through the test cases, performs the numerical verification, and formats the resulting error values as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef rotation_matrix_from_axis_angle(axis: np.ndarray, angle: float) -> np.ndarray:\n    \"\"\"\n    Computes a 3x3 rotation matrix from a rotation axis and angle using Rodrigues' formula.\n    \"\"\"\n    axis = np.asarray(axis)\n    if np.isclose(angle, 0.0):\n        return np.eye(3)\n    \n    # Normalize the rotation axis\n    norm = np.linalg.norm(axis)\n    if np.isclose(norm, 0.0):\n        # A zero axis with non-zero angle is ill-defined; return identity.\n        return np.eye(3)\n    axis = axis / norm\n    \n    # Skew-symmetric cross-product matrix\n    K = np.array([[0, -axis[2], axis[1]],\n                  [axis[2], 0, -axis[0]],\n                  [-axis[1], axis[0], 0]])\n    \n    I = np.eye(3)\n    # Rodrigues' rotation formula\n    R = I + np.sin(angle) * K + (1 - np.cos(angle)) * (K @ K)\n    \n    return R\n\ndef stt_projection(M: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the Symmetric Traceless Tensor (STT) projection of a 3x3 matrix.\n    STT(M) = 1/2(M + M^T) - tr(M)/3 * I\n    \"\"\"\n    return 0.5 * (M + M.T) - (np.trace(M) / 3.0) * np.eye(3)\n\ndef equivariant_pipeline(positions: np.ndarray, charges: np.ndarray, cutoff: float) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Executes one layer of the E(3)-equivariant message passing architecture.\n    \"\"\"\n    num_atoms = positions.shape[0]\n    I = np.eye(3)\n    epsilon = 1e-9 # For numerical stability when normalizing vectors\n\n    # Step 1: Find neighbors for each atom\n    adj = []\n    for i in range(num_atoms):\n        neighbors = []\n        for j in range(num_atoms):\n            if i == j:\n                continue\n            dist = np.linalg.norm(positions[i] - positions[j])\n            if dist < cutoff:\n                neighbors.append(j)\n        adj.append(neighbors)\n\n    # Step 2: Initialize features (h_0)\n    s_0 = np.zeros(num_atoms)\n    v_0 = np.zeros((num_atoms, 3))\n    T_0 = np.zeros((num_atoms, 3, 3))\n\n    for i in range(num_atoms):\n        if not adj[i]: continue\n        for j in adj[i]:\n            r_ij = positions[j] - positions[i]\n            dist_ij = np.linalg.norm(r_ij)\n            hat_r_ij = r_ij / (dist_ij + epsilon)\n            \n            s_0[i] += charges[j]\n            v_0[i] += charges[j] * hat_r_ij\n            outer_prod = np.outer(hat_r_ij, hat_r_ij)\n            T_0[i] += charges[j] * (outer_prod - (1.0 / 3.0) * I)\n\n    # Step 3: Update features (h_1) by message passing\n    s_1 = np.zeros(num_atoms)\n    v_1 = np.zeros((num_atoms, 3))\n    T_1 = np.zeros((num_atoms, 3, 3))\n\n    for i in range(num_atoms):\n        if not adj[i]: continue\n        for j in adj[i]:\n            r_ij = positions[j] - positions[i]\n            dist_ij = np.linalg.norm(r_ij)\n            hat_r_ij = r_ij / (dist_ij + epsilon)\n\n            # Scalar update\n            s_1[i] += s_0[j] + np.dot(v_0[j], hat_r_ij)\n\n            # Vector update\n            v_1[i] += s_0[j] * hat_r_ij + v_0[j] + T_0[j] @ hat_r_ij\n\n            # Tensor update\n            term1_T = s_0[j] * (np.outer(hat_r_ij, hat_r_ij) - (1.0 / 3.0) * I)\n            \n            v_outer_r = np.outer(v_0[j], hat_r_ij)\n            M_vj_rij = v_outer_r + v_outer_r.T\n            term2_T = stt_projection(M_vj_rij) # Simplified from STT(v_j*r_ij_T + r_ij*v_j_T)\n            \n            T_1[i] += term1_T + term2_T\n\n    return s_1, v_1, T_1\n\ndef format_nested_list(data: list) -> str:\n    \"\"\"\n    Recursively formats a nested list into a string without spaces after commas.\n    \"\"\"\n    if isinstance(data, list):\n        return f\"[{','.join(format_nested_list(item) for item in data)}]\"\n    else:\n        return f\"{item:.15e}\" if isinstance(item, float) else str(item)\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and verify equivariance.\n    \"\"\"\n    test_cases = [\n        {\n            \"positions\": [[0.0, 0.0, 0.0], [0.4, 0.1, -0.2], [-0.3, 0.5, 0.2], [0.2, -0.4, 0.3], [-0.5, -0.1, -0.4], [0.1, 0.6, -0.3]],\n            \"charges\": [1.0, -1.0, 1.0, -1.0, 1.0, -1.0],\n            \"cutoff\": 0.8,\n            \"rot_axis\": [0.3, 0.7, 0.6],\n            \"rot_angle\": 0.6\n        },\n        {\n            \"positions\": [[-0.5, 0.0, 0.0], [0.0, 0.0, 0.0], [0.5, 0.0, 0.0]],\n            \"charges\": [1.0, -1.0, 1.0],\n            \"cutoff\": 1.0,\n            \"rot_axis\": [0.0, 0.0, 1.0],\n            \"rot_angle\": np.pi / 3\n        },\n        {\n            \"positions\": [[0.0, 0.0, 0.0], [0.4, 0.1, -0.2], [-0.3, 0.5, 0.2], [0.2, -0.4, 0.3], [-0.5, -0.1, -0.4], [0.1, 0.6, -0.3]],\n            \"charges\": [1.0, -1.0, 1.0, -1.0, 1.0, -1.0],\n            \"cutoff\": 0.05,\n            \"rot_axis\": [0.2, -0.4, 0.9],\n            \"rot_angle\": 1.2\n        },\n        {\n            \"positions\": [[0.2, 0.2, 0.2], [-0.2, -0.2, -0.2], [0.3, -0.1, 0.4]],\n            \"charges\": [1.0, -1.0, 1.0],\n            \"cutoff\": 0.5,\n            \"rot_axis\": [1.0, 0.0, 0.0],\n            \"rot_angle\": 0.0\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        positions_orig = np.array(case[\"positions\"], dtype=float)\n        charges = np.array(case[\"charges\"], dtype=float)\n        cutoff = float(case[\"cutoff\"])\n        rot_axis = np.array(case[\"rot_axis\"], dtype=float)\n        rot_angle = float(case[\"rot_angle\"])\n\n        # 1. Compute updated features for the original configuration\n        s_orig, v_orig, T_orig = equivariant_pipeline(positions_orig, charges, cutoff)\n\n        # 2. Build rotation matrix and rotate positions\n        R = rotation_matrix_from_axis_angle(rot_axis, rot_angle)\n        positions_rot = (R @ positions_orig.T).T\n\n        # 3. Compute updated features for the rotated configuration\n        s_recomputed, v_recomputed, T_recomputed = equivariant_pipeline(positions_rot, charges, cutoff)\n\n        # 4. Transform the original results using the rotation matrix\n        s_transformed = s_orig\n        v_transformed = (R @ v_orig.T).T\n        T_transformed = np.zeros_like(T_orig)\n        for i in range(T_orig.shape[0]):\n            T_transformed[i] = R @ T_orig[i] @ R.T\n\n        # 5. Calculate maximum discrepancy (error)\n        num_atoms = positions_orig.shape[0]\n        if num_atoms == 0:\n            scalar_error, vector_error, tensor_error = 0.0, 0.0, 0.0\n        else:\n            scalar_error = np.max(np.abs(s_recomputed - s_transformed)) if s_orig.size > 0 else 0.0\n            vector_error = np.max(np.linalg.norm(v_recomputed - v_transformed, axis=1)) if v_orig.size > 0 else 0.0\n            tensor_error = np.max(np.linalg.norm(T_recomputed - T_transformed, axis=(1, 2))) if T_orig.size > 0 else 0.0\n\n        all_results.append([scalar_error, vector_error, tensor_error])\n    \n    # Custom formatter to match the output format specification precisely\n    def format_results(results_list):\n        outer_parts = []\n        for inner_list in results_list:\n            inner_parts = [f\"{x:.6e}\" for x in inner_list]\n            outer_parts.append(f\"[{','.join(inner_parts)}]\")\n        return f\"[{','.join(outer_parts)}]\"\n\n    print(format_results(all_results))\n\nsolve()\n```"
        },
        {
            "introduction": "A unique challenge in applying machine learning potentials to electrochemical systems is capturing the explicit dependence of energies and forces on the electrode potential, $U$, a thermodynamic variable rather than a geometric one. Standard MLPs, which are functions of atomic positions, often fail to describe this behavior accurately. This exercise uses a simplified model to quantitatively demonstrate the shortcomings of a potential-unaware MLP and shows how a hybrid Quantum Mechanics/Machine Learning (QM/ML) embedding approach can begin to correct for these potential-dependent errors.",
            "id": "4250456",
            "problem": "Consider an adsorbate interacting with an electrochemical interface modeled at constant electrode potential. The adsorption energy is defined by the fundamental thermodynamic difference $$E_{\\mathrm{ads}}(U) = E_{\\mathrm{tot}}^{\\mathrm{slab+ads}}(U) - E_{\\mathrm{tot}}^{\\mathrm{slab}}(U) - \\mu_{\\mathrm{adsorbate}}(U),$$ where $U$ is the electrode potential, $E_{\\mathrm{tot}}^{\\mathrm{slab+ads}}(U)$ is the total energy of the slab with the adsorbate, $E_{\\mathrm{tot}}^{\\mathrm{slab}}(U)$ is the total energy of the clean slab, and $\\mu_{\\mathrm{adsorbate}}(U)$ is the chemical potential of the adsorbate, all expressed in electronvolts (eV). Under constant potential conditions, the interfacial energy response to the electric field can be represented to leading orders by a linear term and a quadratic polarization term, giving a reference adsorption energy model $$E_{\\mathrm{ref}}(U) = E_0 + \\alpha U + \\gamma U^2,$$ where $E_0$ is the baseline adsorption energy at $U=0$ in eV, $\\alpha$ (in eV/V) collects effective linear coupling related to charge transfer and dipole changes, and $\\gamma$ (in eV/V$^2$) represents capacitive or nonlinear polarization contributions from the interfacial environment.\n\nA pure Machine Learning (ML) potential for the interface that neglects explicit electrostatic coupling predicts $$E_{\\mathrm{ML}}(U) = E_0 + b_{\\mathrm{ML}},$$ where $b_{\\mathrm{ML}}$ is a systematic bias in eV. A Quantum Mechanics/Machine Learning (QM/ML) embedding model combines a quantum mechanical region for chemically active sites with a machine learning environment and captures a fraction of the electrostatic response, $$E_{\\mathrm{QM/ML}}(U) = E_0 + b_{\\mathrm{QM/ML}} + s\\,\\alpha\\,U + t\\,\\gamma\\,U^2,$$ where $b_{\\mathrm{QM/ML}}$ is a smaller systematic bias in eV, $s \\in [0,1]$ is the fraction of the linear response recovered, and $t \\in [0,1]$ is the fraction of the quadratic polarization recovered.\n\nFor a set of electrode potentials $\\{U_i\\}_{i=1}^N$, define the prediction errors $$\\varepsilon_{\\mathrm{ML},i} = E_{\\mathrm{ML}}(U_i) - E_{\\mathrm{ref}}(U_i), \\quad \\varepsilon_{\\mathrm{QM/ML},i} = E_{\\mathrm{QM/ML}}(U_i) - E_{\\mathrm{ref}}(U_i).$$ The bias (mean error) for a model is $$\\mathrm{Bias} = \\frac{1}{N}\\sum_{i=1}^N \\varepsilon_i,$$ the root-mean-square error (RMSE) is $$\\mathrm{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{i=1}^N \\varepsilon_i^2},$$ and the least-squares slope of error versus potential, which quantifies systematic potential-dependent discrepancy, is $$m = \\frac{\\sum_{i=1}^N \\left(U_i - \\bar{U}\\right)\\left(\\varepsilon_i - \\bar{\\varepsilon}\\right)}{\\sum_{i=1}^N \\left(U_i - \\bar{U}\\right)^2},$$ where $\\bar{U} = \\frac{1}{N}\\sum_{i=1}^N U_i$ and $\\bar{\\varepsilon} = \\frac{1}{N}\\sum_{i=1}^N \\varepsilon_i$. The slope $m$ has units of eV/V.\n\nImplement a program that, for each test case described below, constructs $E_{\\mathrm{ref}}(U)$, $E_{\\mathrm{ML}}(U)$, and $E_{\\mathrm{QM/ML}}(U)$ over the specified potential grid, computes the error metrics $\\mathrm{RMSE}$, $\\mathrm{Bias}$, and $m$ for both the ML and QM/ML models, and outputs the results.\n\nAll energies must be in eV, potentials $U$ in volts (V), and slopes in eV/V. The final output must be a single line containing a comma-separated list enclosed in square brackets, formed by concatenating, for each test case in order, the sequence $$[\\mathrm{RMSE}_{\\mathrm{ML}}, \\mathrm{RMSE}_{\\mathrm{QM/ML}}, \\mathrm{Bias}_{\\mathrm{ML}}, \\mathrm{Bias}_{\\mathrm{QM/ML}}, m_{\\mathrm{ML}}, m_{\\mathrm{QM/ML}}],$$ with each float rounded to $6$ decimal places.\n\nUse the following test suite:\n\n- Test Case $1$ (general case with moderate linear and quadratic response): $E_0 = -0.60$ eV, $\\alpha = 0.30$ eV/V, $\\gamma = 0.10$ eV/V$^2$, $b_{\\mathrm{ML}} = 0.04$ eV, $b_{\\mathrm{QM/ML}} = 0.01$ eV, $s = 0.80$, $t = 0.50$, potentials $U \\in \\{-0.40, -0.20, 0.00, 0.20, 0.40\\}$ V.\n- Test Case $2$ (near potential of zero charge with negligible linear response): $E_0 = -0.30$ eV, $\\alpha = 0.00$ eV/V, $\\gamma = 0.02$ eV/V$^2$, $b_{\\mathrm{ML}} = 0.03$ eV, $b_{\\mathrm{QM/ML}} = 0.015$ eV, $s = 0.80$, $t = 0.50$, potentials $U \\in \\{-0.20, 0.00, 0.20\\}$ V.\n- Test Case $3$ (high-field with strong linear and quadratic response): $E_0 = -0.85$ eV, $\\alpha = 0.50$ eV/V, $\\gamma = 0.20$ eV/V$^2$, $b_{\\mathrm{ML}} = 0.05$ eV, $b_{\\mathrm{QM/ML}} = 0.02$ eV, $s = 0.70$, $t = 0.30$, potentials $U \\in \\{-0.60, -0.30, 0.00, 0.30, 0.60\\}$ V.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[x_1,x_2,\\dots,x_M]$), in the exact order specified above for all test cases concatenated. Each numeric entry must be rounded to $6$ decimal places.",
            "solution": "The problem requires the computation of several error metrics for two different computational models of adsorption energy at an electrochemical interface—a pure Machine Learning (ML) model and a hybrid Quantum Mechanics/Machine Learning (QM/ML) model—against a defined reference model. The solution involves a systematic, step-by-step application of the provided mathematical formulas for a given set of parameters and electrode potentials.\n\nFirst, we establish the analytical expressions for the prediction errors, $\\varepsilon_i$, for each model at a specific potential $U_i$. The error is defined as the difference between the model's prediction and the reference energy, $E_{\\mathrm{ref}}(U_i)$.\n\nThe reference model for adsorption energy is given by a quadratic function of the electrode potential $U$:\n$$E_{\\mathrm{ref}}(U) = E_0 + \\alpha U + \\gamma U^2$$\nwhere $E_0$ is the energy at zero potential, $\\alpha$ is the linear coupling coefficient, and $\\gamma$ is the quadratic polarization coefficient.\n\nThe pure ML model provides a potential-independent prediction:\n$$E_{\\mathrm{ML}}(U) = E_0 + b_{\\mathrm{ML}}$$\nwhere $b_{\\mathrm{ML}}$ is a systematic bias. The prediction error for the ML model, $\\varepsilon_{\\mathrm{ML},i}$, at a potential $U_i$ is therefore:\n$$\\varepsilon_{\\mathrm{ML},i} = E_{\\mathrm{ML}}(U_i) - E_{\\mathrm{ref}}(U_i) = (E_0 + b_{\\mathrm{ML}}) - (E_0 + \\alpha U_i + \\gamma U_i^2)$$\nSimplifying this expression, we find that the baseline energy $E_0$ cancels out:\n$$\\varepsilon_{\\mathrm{ML},i} = b_{\\mathrm{ML}} - \\alpha U_i - \\gamma U_i^2$$\n\nThe QM/ML embedding model incorporates a partial description of the electrostatic response:\n$$E_{\\mathrm{QM/ML}}(U) = E_0 + b_{\\mathrm{QM/ML}} + s\\,\\alpha\\,U + t\\,\\gamma\\,U^2$$\nwhere $b_{\\mathrm{QM/ML}}$ is the QM/ML bias, and $s$ and $t$ are the fractions of the linear and quadratic responses recovered, respectively. The prediction error for the QM/ML model, $\\varepsilon_{\\mathrm{QM/ML},i}$, is:\n$$\\varepsilon_{\\mathrm{QM/ML},i} = E_{\\mathrm{QM/ML}}(U_i) - E_{\\mathrm{ref}}(U_i) = (E_0 + b_{\\mathrm{QM/ML}} + s\\alpha U_i + t\\gamma U_i^2) - (E_0 + \\alpha U_i + \\gamma U_i^2)$$\nAgain, $E_0$ cancels. By grouping terms, we obtain:\n$$\\varepsilon_{\\mathrm{QM/ML},i} = b_{\\mathrm{QM/ML}} - (1-s)\\alpha U_i - (1-t)\\gamma U_i^2$$\n\nWith these analytical forms for the error at each potential $U_i$, we can proceed to calculate the required statistical metrics: Bias, Root-Mean-Square Error (RMSE), and the slope of error versus potential ($m$). For each test case, we are given a set of $N$ discrete potentials $\\{U_i\\}_{i=1}^N$. We first compute the corresponding error vectors $\\boldsymbol{\\varepsilon}_{\\mathrm{ML}} = \\{\\varepsilon_{\\mathrm{ML},i}\\}_{i=1}^N$ and $\\boldsymbol{\\varepsilon}_{\\mathrm{QM/ML}} = \\{\\varepsilon_{\\mathrm{QM/ML},i}\\}_{i=1}^N$.\n\nThe metrics are then calculated as follows:\n1.  **Bias**: This is the mean error, $\\bar{\\varepsilon}$. For a generic error vector $\\boldsymbol{\\varepsilon}$, it is calculated as:\n    $$\\mathrm{Bias} = \\bar{\\varepsilon} = \\frac{1}{N}\\sum_{i=1}^N \\varepsilon_i$$\n\n2.  **Root-Mean-Square Error (RMSE)**: This metric quantifies the magnitude of the errors. It is the square root of the mean of the squared errors:\n    $$\\mathrm{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{i=1}^N \\varepsilon_i^2}$$\n\n3.  **Slope of Error vs. Potential ($m$)**: This metric measures the systematic potential-dependent trend in the error. It is calculated as the slope of the best-fit line for a plot of $\\varepsilon_i$ versus $U_i$:\n    $$m = \\frac{\\sum_{i=1}^N \\left(U_i - \\bar{U}\\right)\\left(\\varepsilon_i - \\bar{\\varepsilon}\\right)}{\\sum_{i=1}^N \\left(U_i - \\bar{U}\\right)^2}$$\n    where $\\bar{U}$ is the mean of the potentials, $\\bar{U} = \\frac{1}{N}\\sum_{i=1}^N U_i$, and $\\bar{\\varepsilon}$ is the mean of the errors (the Bias). This calculation is equivalent to finding the covariance of $U$ and $\\varepsilon$ divided by the variance of $U$.\n\nThe algorithmic procedure for each test case is as follows:\n1.  Define the set of potentials $\\{U_i\\}$ as a numerical array.\n2.  Using the test case parameters ($\\alpha, \\gamma, b_{\\mathrm{ML}}, b_{\\mathrm{QM/ML}}, s, t$), compute the error arrays $\\boldsymbol{\\varepsilon}_{\\mathrm{ML}}$ and $\\boldsymbol{\\varepsilon}_{\\mathrm{QM/ML}}$ using the derived simplified expressions.\n3.  For each model (ML and QM/ML), use its error array and the potential array to compute the three metrics: $\\mathrm{RMSE}$, $\\mathrm{Bias}$, and $m$.\n4.  Store the six calculated values in the specified order: $[\\mathrm{RMSE}_{\\mathrm{ML}}, \\mathrm{RMSE}_{\\mathrm{QM/ML}}, \\mathrm{Bias}_{\\mathrm{ML}}, \\mathrm{Bias}_{\\mathrm{QM/ML}}, m_{\\mathrm{ML}}, m_{\\mathrm{QM/ML}}]$.\n\nThis process is repeated for all test cases, and the resulting lists of metrics are concatenated to form the final output, with each value formatted to six decimal places. The implementation uses the `numpy` library for efficient array-based computations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates error metrics for ML and QM/ML models of adsorption energy\n    at an electrochemical interface for a suite of test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test Case 1: general case\n        {\n            'params': {'alpha': 0.30, 'gamma': 0.10, 'b_ml': 0.04, 'b_qmml': 0.01, 's': 0.80, 't': 0.50},\n            'potentials': [-0.40, -0.20, 0.00, 0.20, 0.40]\n        },\n        # Test Case 2: near potential of zero charge\n        {\n            'params': {'alpha': 0.00, 'gamma': 0.02, 'b_ml': 0.03, 'b_qmml': 0.015, 's': 0.80, 't': 0.50},\n            'potentials': [-0.20, 0.00, 0.20]\n        },\n        # Test Case 3: high-field case\n        {\n            'params': {'alpha': 0.50, 'gamma': 0.20, 'b_ml': 0.05, 'b_qmml': 0.02, 's': 0.70, 't': 0.30},\n            'potentials': [-0.60, -0.30, 0.00, 0.30, 0.60]\n        },\n    ]\n\n    def calculate_metrics(potentials_arr, errors_arr):\n        \"\"\"\n        Computes RMSE, Bias, and slope m for a given set of errors and potentials.\n        \n        Args:\n            potentials_arr (np.ndarray): Array of electrode potentials U.\n            errors_arr (np.ndarray): Array of prediction errors.\n            \n        Returns:\n            tuple: A tuple containing (RMSE, Bias, m).\n        \"\"\"\n        # Bias (mean error)\n        bias = np.mean(errors_arr)\n        \n        # Root-Mean-Square Error\n        rmse = np.sqrt(np.mean(errors_arr**2))\n        \n        # Least-squares slope of error vs. potential\n        u_mean = np.mean(potentials_arr)\n        denominator = np.sum((potentials_arr - u_mean)**2)\n        \n        # Handle the case where all potentials are the same to avoid division by zero\n        if np.isclose(denominator, 0):\n            slope_m = 0.0\n        else:\n            # Note: err_mean is the same as bias\n            err_mean = np.mean(errors_arr)\n            numerator = np.sum((potentials_arr - u_mean) * (errors_arr - err_mean))\n            slope_m = numerator / denominator\n            \n        return rmse, bias, slope_m\n\n    all_results = []\n    for case in test_cases:\n        params = case['params']\n        U = np.array(case['potentials'])\n        \n        alpha, gamma = params['alpha'], params['gamma']\n        b_ml, b_qmml = params['b_ml'], params['b_qmml']\n        s, t = params['s'], params['t']\n\n        # Calculate error vectors for ML and QM/ML models\n        # eps_ml = b_ml - alpha*U - gamma*U^2\n        errors_ml = b_ml - alpha * U - gamma * U**2\n        \n        # eps_qmml = b_qmml - (1-s)*alpha*U - (1-t)*gamma*U^2\n        errors_qmml = b_qmml - (1 - s) * alpha * U - (1 - t) * gamma * U**2\n        \n        # Calculate metrics for the ML model\n        rmse_ml, bias_ml, m_ml = calculate_metrics(U, errors_ml)\n        \n        # Calculate metrics for the QM/ML model\n        rmse_qmml, bias_qmml, m_qmml = calculate_metrics(U, errors_qmml)\n        \n        # Extend the final list with results for the current case in the specified order\n        all_results.extend([rmse_ml, rmse_qmml, bias_ml, bias_qmml, m_ml, m_qmml])\n\n    # Format the final output string as a comma-separated list\n    # with each number rounded to 6 decimal places.\n    formatted_results = [f\"{x:.6f}\" for x in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}