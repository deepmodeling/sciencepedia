{
    "hands_on_practices": [
        {
            "introduction": "The primary goal of a metadynamics simulation is to reconstruct the free energy surface, $F(s)$. Unlike standard metadynamics, in the well-tempered variant the final bias potential, $V_b(s)$, is not the simple negative of $F(s)$. This practice guides you through the fundamental derivation that connects the converged bias potential to the true free energy, showing how this relationship depends critically on the bias factor $\\gamma$. Understanding this scaling is the first step toward correctly analyzing any WTMetaD simulation output. ",
            "id": "4244349",
            "problem": "In a constant-potential molecular dynamics study of a proton-coupled electron transfer at an electrified interface, you wish to reconstruct the free energy surface $F(s)$ along a collective variable $s$ (for example, a charge-transfer coordinate) using Well-Tempered Metadynamics (WTMetaD). The system is thermostatted at temperature $T$ and the WTMetaD bias $V_b(s,t)$ is built by depositing Gaussian hills with an instantaneous rate that is modulated according to the WTMetaD prescription. Assume the following foundational facts:\n\n- Statistical mechanics of biased ensembles implies that the stationary distribution of the collective variable under the applied bias is $P_b(s) \\propto \\exp\\!\\left(-\\beta\\,[F(s) + V_b(s)]\\right)$, where $\\beta = 1/(k_B T)$, $k_B$ is the Boltzmann constant, and $T$ is the physical temperature.\n- In Well-Tempered Metadynamics (WTMetaD), the instantaneous bias deposition rate at a given $s$ is proportional to $\\exp\\!\\left(-V_b(s,t)/(k_B \\Delta T)\\right)$, where $\\Delta T$ is the bias temperature. In the infinite-time quasi-stationary regime, the expected rate of bias accumulation is uniform across $s$, which implies $P_b(s)\\,\\exp\\!\\left(-V_b(s)/(k_B \\Delta T)\\right)$ is independent of $s$.\n- The WTMetaD bias factor $\\gamma$ is defined by $\\gamma = (T + \\Delta T)/T$.\n\nUsing only these facts, derive the infinite-time relation that connects $V_b(s)$ and $F(s)$ and determine the scaling factor $c$ such that $F(s) = c\\,V_b(s)$ up to an additive constant. Then, for a simulation conducted at $T = 300\\ \\mathrm{K}$ with bias factor $\\gamma = 10$, compute the bias temperature $\\Delta T$ (express your answer in $\\mathrm{K}$) and the dimensionless scaling factor $c$ in the infinite-time limit. Provide your final answer as a row matrix $\\begin{pmatrix}\\Delta T & c\\end{pmatrix}$. Do not round your answer.",
            "solution": "The problem requires the derivation of the relationship between the free energy surface $F(s)$ and the converged Well-Tempered Metadynamics (WTMetaD) bias potential $V_b(s)$, and the calculation of specific parameters for a given system. The derivation will be based on the foundational principles of biased statistical mechanical ensembles and WTMetaD, as provided in the problem statement.\n\nThe first principle states that the stationary probability distribution of the collective variable $s$ under the influence of both the intrinsic free energy $F(s)$ and the applied bias $V_b(s)$ is given by:\n$$P_b(s) \\propto \\exp\\left(-\\beta[F(s) + V_b(s)]\\right)$$\nwhere $\\beta = 1/(k_B T)$, $k_B$ is the Boltzmann constant, and $T$ is the temperature of the system. We can write this with an explicit normalization constant $C_1$:\n$$P_b(s) = C_1 \\exp\\left(-\\beta F(s) - \\beta V_b(s)\\right)$$\n\nThe second principle describes the condition for the quasi-stationary state in WTMetaD, achieved in the infinite-time limit. It states that the effective sampling rate is uniform across the space of the collective variable $s$. This is mathematically expressed as the product of the biased probability $P_b(s)$ and a tempering factor being constant:\n$$P_b(s) \\exp\\left(-\\frac{V_b(s)}{k_B \\Delta T}\\right) = C_2$$\nwhere $\\Delta T$ is the bias temperature and $C_2$ is a constant. From this expression, we can isolate $P_b(s)$:\n$$P_b(s) = C_2 \\exp\\left(\\frac{V_b(s)}{k_B \\Delta T}\\right)$$\n\nWe now have two expressions for the same stationary distribution $P_b(s)$. Equating them yields:\n$$C_1 \\exp\\left(-\\beta F(s) - \\beta V_b(s)\\right) = C_2 \\exp\\left(\\frac{V_b(s)}{k_B \\Delta T}\\right)$$\nTo solve for $F(s)$, we take the natural logarithm of both sides:\n$$\\ln(C_1) - \\beta F(s) - \\beta V_b(s) = \\ln(C_2) + \\frac{V_b(s)}{k_B \\Delta T}$$\nNext, we rearrange the terms to isolate $\\beta F(s)$:\n$$-\\beta F(s) = \\beta V_b(s) + \\frac{V_b(s)}{k_B \\Delta T} + \\ln(C_2) - \\ln(C_1)$$\nWe can factor out $V_b(s)$ from the first two terms on the right-hand side and combine the logarithmic terms into a single constant, $C' = \\ln(C_2) - \\ln(C_1)$:\n$$-\\beta F(s) = V_b(s)\\left(\\beta + \\frac{1}{k_B \\Delta T}\\right) + C'$$\nSolving for $F(s)$ by dividing by $-\\beta$:\n$$F(s) = -\\frac{1}{\\beta} V_b(s)\\left(\\beta + \\frac{1}{k_B \\Delta T}\\right) - \\frac{C'}{\\beta}$$\nLetting $C_{add} = -C'/\\beta$ be the new additive constant, we simplify the term in the parenthesis:\n$$F(s) = -V_b(s)\\left(1 + \\frac{1}{\\beta k_B \\Delta T}\\right) + C_{add}$$\nSubstituting the definition $\\beta = 1/(k_B T)$:\n$$F(s) = -V_b(s)\\left(1 + \\frac{k_B T}{k_B \\Delta T}\\right) + C_{add} = -V_b(s)\\left(1 + \\frac{T}{\\Delta T}\\right) + C_{add}$$\nThis is the sought-after relationship between $F(s)$ and $V_b(s)$. The problem asks for this relationship to be expressed in terms of the bias factor $\\gamma$, defined as $\\gamma = (T + \\Delta T)/T$. Let us manipulate the scaling term:\n$$1 + \\frac{T}{\\Delta T} = \\frac{\\Delta T + T}{\\Delta T}$$\nFrom the definition of $\\gamma$, we have $T + \\Delta T = \\gamma T$ and $\\Delta T = \\gamma T - T = T(\\gamma-1)$. Substituting these into the scaling term gives:\n$$\\frac{\\Delta T + T}{\\Delta T} = \\frac{\\gamma T}{T(\\gamma-1)} = \\frac{\\gamma}{\\gamma-1}$$\nThus, the final relation between the free energy and the converged bias potential is:\n$$F(s) = -\\frac{\\gamma}{\\gamma-1}V_b(s) + C_{add}$$\nThis equation shows that the free energy surface $F(s)$ is linearly proportional to the converged WTMetaD bias potential $V_b(s)$, up to an additive constant.\n\nThe problem defines a scaling factor $c$ such that $F(s) = c\\,V_b(s)$ up to an additive constant. By direct comparison with our derived relation, we find:\n$$c = -\\frac{\\gamma}{\\gamma-1}$$\n\nNow, we proceed to the numerical calculation part of the problem. We are given the system temperature $T = 300\\ \\mathrm{K}$ and the bias factor $\\gamma = 10$.\n\nFirst, we calculate the bias temperature $\\Delta T$. Using the rearranged definition of $\\gamma$:\n$$\\Delta T = T(\\gamma - 1)$$\nSubstituting the given values:\n$$\\Delta T = 300 \\cdot (10 - 1) = 300 \\cdot 9 = 2700\\ \\mathrm{K}$$\n\nSecond, we calculate the dimensionless scaling factor $c$:\n$$c = -\\frac{\\gamma}{\\gamma-1}$$\nSubstituting the value of $\\gamma$:\n$$c = -\\frac{10}{10-1} = -\\frac{10}{9}$$\n\nThe final answer is required as a row matrix $\\begin{pmatrix}\\Delta T & c\\end{pmatrix}$. The values are $\\Delta T = 2700$ and $c = -10/9$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2700 & -\\frac{10}{9}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The choice of the bias factor, $\\gamma$, is arguably the most critical parameter in setting up a Well-Tempered Metadynamics simulation. This parameter dictates a fundamental trade-off: a $\\gamma$ that is too small may fail to enhance sampling across high energy barriers, while one that is too large can slow convergence and introduce statistical noise. This exercise challenges you to reason through these trade-offs and apply a practical heuristic to select a near-optimal $\\gamma$ for a system with a known energy barrier, a key skill for designing efficient and reliable simulations. ",
            "id": "4244364",
            "problem": "In an atomistic simulation of an electrified aqueous interface relevant to computational electrochemistry, you plan to accelerate sampling of a rare interfacial ion-transfer event described by a one-dimensional collective variable $s$ using Well-Tempered Metadynamics (WTMetaD). Let the equilibrium free energy surface along $s$ be $F(s)$ at temperature $T$, with a dominant activation barrier of height $\\Delta F_b$ separating two basins. In WTMetaD, the bias factor $\\gamma$ controls how much the deposited bias potential modifies the stationary sampling along $s$.\n\nYou estimate from preliminary umbrella sampling that $\\Delta F_b \\approx 0.60\\ \\mathrm{eV}$ at $T=298\\ \\mathrm{K}$ (Boltzmann constant $k_B \\approx 8.617\\times 10^{-5}\\ \\mathrm{eV/K}$). From first principles of statistical mechanics and the definition of WTMetaD, reason qualitatively about how $\\gamma$ modifies the effective barrier and the stationary distribution along $s$, and choose the option that correctly explains why choosing $\\gamma$ too small leads to underfilling of basins, choosing $\\gamma$ too large leads to slow convergence of free-energy estimates, and proposes a quantitatively reasonable heuristic for selecting $\\gamma$ based on $\\Delta F_b$ for this system. Your answer must include a defensible numerical value of $\\gamma$ for the given $\\Delta F_b$ and $T$.\n\nA. In WTMetaD, the stationary distribution along $s$ becomes broader as $\\gamma$ increases, so the effective barrier is reduced approximately to $\\Delta F_b/\\gamma$. If $\\gamma$ is too close to $1$, the deposited bias cannot compensate the barrier, transitions remain infrequent (underfilling), and free-energy reconstruction is poor. If $\\gamma$ is very large, the stationary distribution becomes nearly flat, the system spends substantial time diffusing through high free-energy regions, and reweighting suffers from large variance, leading to slow statistical convergence. A practical heuristic is to choose $\\gamma \\approx \\Delta F_b/(m k_B T)$ with $m$ in the range $3$–$5$ to reduce the effective barrier to a few $k_B T$. For $\\Delta F_b=0.60\\ \\mathrm{eV}$ and $T=298\\ \\mathrm{K}$, taking $m=4$ gives $\\gamma \\approx 0.60/\\left(4\\times 8.617\\times 10^{-5}\\times 298\\right)\\approx 5.8$, i.e., $\\gamma \\approx 6$.\n\nB. In WTMetaD, the tempering suppresses bias growth at large times, so taking $\\gamma$ small strongly damps the bias and prevents overshoot, which leads to faster convergence. Large $\\gamma$ overfills wells and causes hysteresis. A robust heuristic is to pick $\\gamma \\approx 2$ regardless of $\\Delta F_b$, because this halves any barrier. For $\\Delta F_b=0.60\\ \\mathrm{eV}$ and $T=298\\ \\mathrm{K}$ this choice is optimal.\n\nC. In WTMetaD, larger $\\gamma$ always increases the bias and thus always improves convergence because barriers vanish in the limit of large $\\gamma$. The best heuristic is to make the effective barrier negligible by choosing $\\gamma \\gg \\Delta F_b/(k_B T)$; for the given system a safe choice is $\\gamma \\approx 40$.\n\nD. The most efficient sampling is obtained when the biased barrier equals approximately $k_B T$, which minimizes variance in reweighting. Therefore set $\\gamma \\approx \\Delta F_b/(k_B T)$, so the barrier is $\\Delta F_b/\\gamma \\approx k_B T$. For $\\Delta F_b=0.60\\ \\mathrm{eV}$ and $T=298\\ \\mathrm{K}$ this gives $\\gamma \\approx 23$, which ensures both rapid transitions and minimal noise because larger $\\gamma$ strengthens tempering and thus reduces fluctuations.",
            "solution": "This problem requires a qualitative and quantitative understanding of the role of the bias factor $\\gamma$ in Well-Tempered Metadynamics (WTMetaD). The correct choice of $\\gamma$ is crucial for balancing efficient barrier crossing with the statistical quality and convergence speed of the free energy reconstruction.\n\n### The Role of the Bias Factor $\\gamma$\n\nIn WTMetaD, the long-time stationary distribution of the collective variable $s$ is not uniform (as in ideal standard metadynamics) but is a \"tempered\" version of the original Boltzmann distribution:\n$$P_{\\text{biased}}(s) \\propto [P_{\\text{unbiased}}(s)]^{1/\\gamma} \\propto \\exp\\left(-\\frac{\\beta F(s)}{\\gamma}\\right)$$\nThis means the system samples an effective (or flattened) free energy surface, $F_{\\text{eff}}(s) = F(s)/\\gamma$. Consequently, an energy barrier of height $\\Delta F_b$ in the original landscape is reduced to an effective barrier of height $\\Delta F_b / \\gamma$ in the biased simulation.\n\n### The Trade-Off in Choosing $\\gamma$\n\nThe choice of $\\gamma$ involves a critical trade-off:\n1.  **If $\\gamma$ is too small (e.g., $\\gamma \\to 1$):** The effective barrier $\\Delta F_b / \\gamma$ remains high, and the simulation will not be significantly accelerated. The system may fail to cross the barrier frequently, leading to \"underfilling\" of the free energy landscape and an inaccurate reconstruction.\n2.  **If $\\gamma$ is too large (e.g., $\\gamma \\to \\infty$):** The effective barrier $\\Delta F_b / \\gamma \\to 0$, and the landscape becomes excessively flat. This approaches the behavior of standard metadynamics. While barrier crossings are frequent, the bias potential must grow very large to cancel the original free energy, slowing down the convergence of the free energy estimate. Furthermore, the reweighting factors, $\\exp[\\beta V_b(s,t)]$, can become enormous, leading to high variance in reweighted averages and poor statistical efficiency.\n\n### Heuristic for an Optimal $\\gamma$\n\nA practical and widely used heuristic is to choose $\\gamma$ such that the effective barrier is reduced to a few multiples of the thermal energy, $k_B T$. This ensures that barrier crossings are frequent enough for efficient sampling but not so frequent that the system diffuses uncontrollably and convergence is hampered. A reasonable target for the effective barrier is:\n$$\\frac{\\Delta F_b}{\\gamma} \\approx m k_B T, \\quad \\text{where } m \\text{ is a small integer, typically } 3-5.$$\nThis leads to the heuristic for choosing $\\gamma$:\n$$\\gamma \\approx \\frac{\\Delta F_b}{m k_B T}$$\n\n### Calculation for the Given System\n\n- Barrier height: $\\Delta F_b = 0.60 \\text{ eV}$\n- Temperature: $T = 298 \\text{ K}$\n- Boltzmann constant: $k_B \\approx 8.617 \\times 10^{-5} \\text{ eV/K}$\n- Thermal energy: $k_B T \\approx 8.617 \\times 10^{-5} \\text{ eV/K} \\times 298 \\text{ K} \\approx 0.02568 \\text{ eV}$\n- Barrier height in units of thermal energy: $\\frac{\\Delta F_b}{k_B T} \\approx \\frac{0.60}{0.02568} \\approx 23.4$\n\nUsing the heuristic with $m=4$:\n$$\\gamma \\approx \\frac{23.4}{4} \\approx 5.85$$\nA choice of $\\gamma \\approx 6$ is therefore a well-justified and reasonable selection.\n\n### Analysis of Options\n\n*   **A. Correct.** This option correctly describes the trade-offs of choosing $\\gamma$. It notes that small $\\gamma$ leads to underfilling and large $\\gamma$ leads to slow convergence and reweighting issues. It proposes the correct heuristic of reducing the effective barrier to a few $k_B T$ and performs the calculation correctly, arriving at a reasonable value of $\\gamma \\approx 6$.\n*   **B. Incorrect.** This option incorrectly claims small $\\gamma$ leads to faster convergence and proposes a fixed heuristic ($\\gamma \\approx 2$) that ignores the system-specific barrier height, which is not optimal.\n*   **C. Incorrect.** This option incorrectly claims that larger $\\gamma$ is always better. This ignores the convergence and statistical problems associated with overly aggressive biasing, which are the very issues WTMetaD was designed to solve.\n*   **D. Incorrect.** Setting the effective barrier to exactly $1 k_B T$ is generally too aggressive. It leads to a very large $\\gamma \\approx 23$, which makes the landscape extremely flat and can harm convergence speed and statistical efficiency. The reasoning that larger $\\gamma$ strengthens tempering is also confused; larger $\\gamma$ means *less* tempering, as the bias hill heights are attenuated more slowly.\n\nTherefore, option A provides the most complete and accurate reasoning.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "A calculated free energy profile is incomplete without a robust estimate of its statistical uncertainty. In metadynamics, this is a non-trivial task due to strong temporal correlations in the trajectory and the time-dependent nature of the applied bias. This practice introduces the block bootstrap method, a powerful and principled approach for uncertainty quantification in biased simulations. By evaluating different protocols, you will learn to correctly account for autocorrelation and non-stationarity, enabling you to produce scientifically defensible error bars for your reconstructed free energy surfaces. ",
            "id": "4244420",
            "problem": "A computational electrochemistry group performs Well-Tempered Metadynamics (WTMetaD) Molecular Dynamics (MD) to estimate the free energy profile $F(s)$ along a collective variable $s$ that quantifies the progress of an interfacial electron-transfer event at a metal electrode held at constant potential. The trajectory is biased by a time-dependent metadynamics potential $V(s,t)$ and recorded at a sampling interval $\\Delta t$. The group wants a principled protocol to estimate statistical uncertainties in the reconstructed $F(s)$ using a bootstrap approach that resamples time blocks from the biased trajectory.\n\nStarting from first principles, recall that the unbiased free energy is defined via the unbiased probability density $P(s)$ as $F(s) = -k_{\\mathrm{B}} T \\ln P(s) + C$, where $k_{\\mathrm{B}}$ is the Boltzmann constant, $T$ is the absolute temperature, and $C$ is an arbitrary constant. At a given time $t$, the biased sampling density for $s$ is proportional to $\\pi_{t}(s) \\propto \\exp\\left[-\\beta \\left(F(s) + V(s,t)\\right)\\right]$, with $\\beta = 1/(k_{\\mathrm{B}} T)$. Importance sampling implies that an unbiased histogram for $s$ can be formed by assigning to each snapshot at time $t$ the weight $w(t)$ proportional to the ratio $P(s(t))/\\pi_{t}(s(t))$, which does not require knowledge of $F(s)$ but depends on $V(s,t)$.\n\nBecause MD time series are strongly autocorrelated, naive independent resampling underestimates the variance. Let $X_{t}^{(k)} = w(t) \\,\\mathbf{1}\\{s(t) \\in \\mathcal{B}_{k}\\}$ denote the reweighted bin-count contribution to bin $\\mathcal{B}_{k}$ at time $t$, and define its normalized autocorrelation function $\\rho_{X^{(k)}}(\\tau)$ for lag $\\tau \\in \\mathbb{N}$. The integrated autocorrelation time is $\\tau_{\\mathrm{int}}^{(k)} = 1 + 2 \\sum_{\\tau=1}^{\\infty} \\rho_{X^{(k)}}(\\tau)$, and a conservative block length $L$ should satisfy $L \\gtrsim c \\,\\max_{k} \\tau_{\\mathrm{int}}^{(k)}$ for some $c$ in the range $[5,10]$ to approximate independence across blocks. Moreover, because $V(s,t)$ is time dependent, one should avoid mixing early non-stationary bias-deposition segments with late quasi-stationary segments. A practical way is to define a stationarity onset time $t^{\\ast}$ by monitoring the average magnitude of bias increments and requiring it to be much smaller than $k_{\\mathrm{B}} T$ per deposition.\n\nConsider the following candidate protocols to bootstrap uncertainties in $F(s)$ from a single long WTMetaD trajectory. Which protocol is asymptotically consistent and scientifically justified?\n\nA. Build a reweighted histogram using $w(t) = \\exp\\left[\\beta V\\left(s(t),t\\right)\\right]$ for each recorded frame. Restrict the resampling pool to times $t \\ge t^{\\ast}$ where the average bias increment per deposition obeys $\\langle |\\Delta V| \\rangle \\le \\varepsilon k_{\\mathrm{B}} T$ with a small $\\varepsilon$, ensuring quasi-stationarity. Perform a nonoverlapping block bootstrap by partitioning the post-$t^{\\ast}$ time series into contiguous blocks of length $L$ chosen as $L = c \\,\\max_{k} \\tau_{\\mathrm{int}}^{(k)}$ with $c \\in [5,10]$, where $\\tau_{\\mathrm{int}}^{(k)}$ is computed from $X_{t}^{(k)}$. Resample blocks with replacement to build bootstrap replicates, reconstruct $F^{\\ast}(s) = -k_{\\mathrm{B}} T \\ln H^{\\ast}(s) + C^{\\prime}$ for each replicate from the reweighted histogram $H^{\\ast}(s)$, and report uncertainties from the spread (for example, standard deviation or percentile intervals) across replicates.\n\nB. Build the histogram using $w(t) = \\exp\\left[-\\beta V\\left(s(t),t\\right)\\right]$ and apply a standard i.i.d. bootstrap by resampling individual frames regardless of temporal order. Use all frames from the full trajectory, because block resampling is unnecessary once reweighting is applied, and choose $L = \\Delta t$ to maximize the number of resampled units.\n\nC. Choose the block length by the heuristic $L = \\sigma_{s}^{2}/D_{s}$, where $\\sigma_{s}$ is the kernel width in $s$ used to deposit Gaussians and $D_{s}$ is the diffusion coefficient of $s$ estimated from the biased trajectory, and set $w(t) = 1$ because the well-tempered bias makes the sampled distribution stationary. Resample blocks of length $L$ from the entire trajectory and compute $F(s)$ by direct logarithm of the biased histogram.\n\nD. Use overlapping blocks of length equal to one bias-deposition stride and set $w(t) = \\exp\\left[\\beta V\\left(s(t),t\\right)/\\gamma\\right]$, where $\\gamma$ is the well-tempered bias factor. Combine early and late trajectory segments to maximize sample size, and estimate uncertainties from the variance of $F(s)$ across $M$ bootstrap replicates without checking autocorrelation.\n\nE. Exploit the stationary distribution of WTMetaD, $p_{\\mathrm{WT}}(s) \\propto \\exp\\left[-\\beta F(s)/\\gamma\\right]$, to avoid reweighting and construct $F(s)$ directly from the biased histogram. Use short blocks of length $L \\approx \\tau_{\\mathrm{int}}/2$ to increase the number of blocks, and include the entire trajectory because the WTMetaD bias ensures rapid decorrelation.\n\nSelect the correct option(s).",
            "solution": "Begin from the definition of the unbiased free energy along a collective variable. The free energy $F(s)$ is defined by the unbiased probability density $P(s)$ via $F(s) = -k_{\\mathrm{B}} T \\ln P(s) + C$. In a biased simulation with time-dependent bias $V(s,t)$, the instantaneous sampling density for $s$ at time $t$ is proportional to\n$$\n\\pi_{t}(s) \\propto \\exp\\left[-\\beta\\left(F(s) + V(s,t)\\right)\\right],\n$$\nassuming the dynamical propagator preserves detailed balance with respect to the biased potential and neglecting Jacobian corrections for $s$.\n\nImportance sampling states that, for any observable $g(s)$, the unbiased expectation can be recovered from biased samples $\\{s(t)\\}$ by\n$$\n\\langle g(s)\\rangle_{\\text{unbiased}} = \\frac{\\left\\langle g\\left(s(t)\\right)\\, w(t)\\right\\rangle_{\\text{biased}}}{\\left\\langle w(t)\\right\\rangle_{\\text{biased}}},\n$$\nwhere the weight $w(t)$ is proportional to the ratio of the target density to the sampling density evaluated at $s(t)$. For the target density $P(s) \\propto \\exp[-\\beta F(s)]$ and the sampling density $\\pi_{t}(s) \\propto \\exp[-\\beta(F(s)+V(s,t))]$, the ratio is\n$$\nw(t) \\propto \\frac{\\exp\\left[-\\beta F\\left(s(t)\\right)\\right]}{\\exp\\left[-\\beta\\left(F\\left(s(t)\\right)+V\\left(s(t),t\\right)\\right)\\right]} = \\exp\\left[\\beta V\\left(s(t),t\\right)\\right].\n$$\nAny time-dependent normalization factor cancels in ratios or can be absorbed into a constant that does not affect relative bin weights in a histogram. Therefore, a consistent reweighted histogram estimator for $P(s)$ is\n$$\nH(s) = \\sum_{t} w(t)\\,\\delta\\left(s - s(t)\\right), \\quad \\text{with} \\quad w(t) = \\exp\\left[\\beta V\\left(s(t),t\\right)\\right],\n$$\nand the corresponding free energy estimator is $\\hat{F}(s) = -k_{\\mathrm{B}} T \\ln H(s) + C^{\\prime}$, where $C^{\\prime}$ enforces a chosen reference.\n\nNext, address statistical uncertainty under autocorrelation. For a time series $Y_{t}$ with normalized autocorrelation function $\\rho_{Y}(\\tau)$, the integrated autocorrelation time is\n$$\n\\tau_{\\mathrm{int}} = 1 + 2 \\sum_{\\tau=1}^{\\infty} \\rho_{Y}(\\tau),\n$$\nwhich quantifies the factor by which correlations inflate the variance relative to independent samples. For a reweighted histogram, a natural choice is $Y_{t} = X_{t}^{(k)} = w(t)\\,\\mathbf{1}\\{s(t)\\in \\mathcal{B}_{k}\\}$ for each bin $\\mathcal{B}_{k}$, because the variance of bin counts directly enters the uncertainty of $\\hat{F}(s)$ via propagation through the logarithm. To capture the slowest timescale among bins, use $\\max_{k} \\tau_{\\mathrm{int}}^{(k)}$.\n\nBlock bootstrap resamples contiguous segments of the time series to preserve intra-block correlation structures while approximating independence between blocks if the block length $L$ exceeds the correlation length. A principled choice is $L \\gtrsim c \\,\\max_{k}\\tau_{\\mathrm{int}}^{(k)}$ with $c$ in $[5,10]$, which reduces bias in bootstrap variance estimates for strongly correlated series. Because $V(s,t)$ is time dependent, the sampling density changes during the run; hence, mixing early times when $V(s,t)$ grows rapidly with later times when $V(s,t)$ fluctuates around a quasi-stationary value can introduce non-stationarity into resampling. A practical remedy is to define a stationarity onset time $t^{\\ast}$ by monitoring bias increments per deposition,\n$$\n\\Delta V_{n} = V\\left(s(t_{n}),t_{n}\\right) - V\\left(s(t_{n^{-}}),t_{n^{-}}\\right),\n$$\nwhere $t_{n}$ enumerates deposition events. When $\\langle |\\Delta V_{n}| \\rangle \\le \\varepsilon k_{\\mathrm{B}} T$ with a small $\\varepsilon$ (for example, $\\varepsilon \\in [0.05,0.2]$), subsequent segments can be regarded as approximately stationary for reweighting and block resampling.\n\nWith these principles in place, evaluate each option:\n\nOption A: This protocol uses $w(t) = \\exp\\left[\\beta V\\left(s(t),t\\right)\\right]$, which is the correct importance sampling weight derived above. It restricts resampling to a quasi-stationary regime defined by a threshold on average bias increments per deposition, which is a scientifically sound criterion to mitigate non-stationarity arising from time-dependent bias. For block length, it computes $\\tau_{\\mathrm{int}}^{(k)}$ from $X_{t}^{(k)}$ and sets $L = c \\,\\max_{k} \\tau_{\\mathrm{int}}^{(k)}$ with $c$ in $[5,10]$, which is a conservative choice that aligns with the need to exceed the longest correlation time across bins. It then resamples nonoverlapping blocks and reconstructs $F^{\\ast}(s)$ for each bootstrap replicate from reweighted histograms. This is asymptotically consistent and scientifically justified for estimating uncertainties in $\\hat{F}(s)$. Verdict — Correct.\n\nOption B: The weight choice $w(t) = \\exp\\left[-\\beta V\\left(s(t),t\\right)\\right]$ is the inverse of the correct ratio. From the derivation, the correct weight increases with $V$ as $\\exp(+\\beta V)$; using $\\exp(-\\beta V)$ would reweight in the wrong direction and bias the estimator for $P(s)$ and thus $F(s)$. Furthermore, applying an i.i.d. bootstrap at the frame level ignores autocorrelation, which leads to underestimation of uncertainties for correlated MD data. Choosing $L = \\Delta t$ does not address correlation. Verdict — Incorrect.\n\nOption C: The block length heuristic $L=\\sigma_{s}^{2}/D_{s}$ is dimensionally inconsistent unless accompanied by a specific stochastic model, and it does not follow from the integrated autocorrelation time of the reweighted observables that determine uncertainty in $\\hat{F}(s)$. In addition, setting $w(t)=1$ ignores the need to reweight from the biased sampling density back to the unbiased target; while WTMetaD can yield a modified stationary distribution $p_{\\mathrm{WT}}(s) \\propto \\exp\\left[-\\beta F(s)/\\gamma\\right]$, directly using the biased histogram without reweighting does not recover $P(s)$. This violates the importance sampling principle derived above. Verdict — Incorrect.\n\nOption D: Overlapping blocks are sometimes used in block bootstrap variants to reduce variance of bootstrap estimates, but choosing a block length equal to one deposition stride is not justified by the integrated autocorrelation time of the reweighted bin counts and is typically far too short to ensure approximate independence between blocks. The weight $w(t) = \\exp\\left[\\beta V(s(t),t)/\\gamma\\right]$ is not the correct importance sampling ratio for recovering the unbiased distribution; the derivation shows that the ratio relative to the biased density is $\\exp\\left[\\beta V(s(t),t)\\right]$ without division by $\\gamma$. Combining early and late segments without checking stationarity can inject non-stationarity into resampling and distort uncertainty estimates. Verdict — Incorrect.\n\nOption E: While WTMetaD achieves a stationary modified distribution $p_{\\mathrm{WT}}(s) \\propto \\exp\\left[-\\beta F(s)/\\gamma\\right]$, computing $F(s)$ from the biased histogram without reweighting yields the scaled free energy $F(s)/\\gamma$ and does not recover the unbiased $F(s)$ unless additional calibration is performed; the problem explicitly seeks uncertainties in $F(s)$, not $F(s)/\\gamma$. Choosing $L \\approx \\tau_{\\mathrm{int}}/2$ is too short to decorrelate blocks and will underestimate uncertainty. Including the entire trajectory, including non-stationary early deposition, ignores the time dependence of $V(s,t)$ and can bias both the estimator and its uncertainty. Verdict — Incorrect.\n\nTherefore, the scientifically grounded and asymptotically consistent protocol is given in Option A.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}