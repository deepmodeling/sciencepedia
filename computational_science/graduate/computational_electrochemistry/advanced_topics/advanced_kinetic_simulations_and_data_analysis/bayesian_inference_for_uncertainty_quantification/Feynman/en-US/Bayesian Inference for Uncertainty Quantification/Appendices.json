{
    "hands_on_practices": [
        {
            "introduction": "This first practice is a foundational exercise in Bayesian analysis. Here, we will use the Laplace approximation to combine a prior belief about a parameter with information from experimental data, encapsulated in the likelihood. This exercise provides a clear, analytical demonstration of how prior knowledge and evidence are mathematically blended to form an updated, more informed posterior distribution for the logarithm of the exchange current density.",
            "id": "4237021",
            "problem": "Consider a computational electrochemistry inference task in which the exchange current density $i_{0}$ of a charge-transfer reaction, as inferred from Electrochemical Impedance Spectroscopy (EIS), is modeled as lognormal. Define $\\theta \\equiv \\ln i_{0}$. The prior on $\\theta$ is Gaussian with mean $\\mu_{0}$ and variance $\\sigma_{0}^{2}$, i.e., $\\theta \\sim \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2})$, which is equivalent to a lognormal prior on $i_{0}$. An EIS data set $y$ is fit by nonlinear least squares to a mechanistic impedance model, and the resulting log-likelihood for $\\theta$ is locally Gaussian around its maximum likelihood estimate $\\hat{\\theta}_{L}$ with observed curvature $\\kappa>0$, meaning that a second-order Taylor expansion yields\n$$\n\\ln p(y \\mid \\theta) \\approx \\ln p(y \\mid \\hat{\\theta}_{L}) - \\frac{1}{2}\\,\\kappa\\,(\\theta - \\hat{\\theta}_{L})^{2}.\n$$\nUsing Bayes’ theorem and the Laplace approximation about the Maximum A Posteriori (MAP) estimator, derive a Gaussian approximation to the posterior $p(\\theta \\mid y)$ and report:\n- the approximate posterior mean (equal to the MAP under the Laplace approximation), and\n- the approximate posterior variance (the inverse of the negative second derivative of the log-posterior at the MAP),\nas closed-form analytic expressions in terms of $\\mu_{0}$, $\\sigma_{0}^{2}$, $\\hat{\\theta}_{L}$, and $\\kappa$. Express your final answer as a row matrix containing the mean and the variance, in that order. No numerical evaluation is required. The final answer must be a single analytic expression without units.",
            "solution": "The problem asks for the mean and variance of a Gaussian approximation to the posterior probability distribution $p(\\theta \\mid y)$, where $\\theta \\equiv \\ln i_{0}$ and $i_{0}$ is the exchange current density. The derivation will be based on Bayes' theorem and the provided Gaussian approximations for the prior and likelihood.\n\nThe validation of the problem statement is the first critical step.\nThe givens are:\n1.  The parameter of interest is $\\theta \\equiv \\ln i_{0}$.\n2.  The prior distribution is Gaussian: $p(\\theta) = \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2})$.\n3.  The log-likelihood function is approximated by a quadratic form: $\\ln p(y \\mid \\theta) \\approx \\ln p(y \\mid \\hat{\\theta}_{L}) - \\frac{1}{2}\\kappa(\\theta - \\hat{\\theta}_{L})^{2}$, where $\\kappa > 0$.\n\nThe problem is scientifically and mathematically sound. It describes a standard Bayesian inference task, which is a common application in computational electrochemistry for parameter estimation and uncertainty quantification. The use of a Gaussian prior for a log-transformed parameter (which implies a lognormal prior for the original strictly positive parameter $i_0$) is a standard and valid technique. The quadratic approximation of the log-likelihood (the Laplace approximation) is a well-established method in statistics for approximating posterior distributions. All terms are clearly defined, and the problem is self-contained and well-posed, admitting a unique analytical solution. Thus, the problem is deemed valid and a solution will be formulated.\n\nAccording to Bayes' theorem, the posterior probability density function $p(\\theta \\mid y)$ is proportional to the product of the likelihood $p(y \\mid \\theta)$ and the prior $p(\\theta)$:\n$$\np(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta)\n$$\nIt is more convenient to work with the logarithm of the posterior, the log-posterior:\n$$\n\\ln p(\\theta \\mid y) = \\ln p(y \\mid \\theta) + \\ln p(\\theta) + C\n$$\nwhere $C$ is a constant of proportionality that does not depend on $\\theta$.\n\nThe prior distribution for $\\theta$ is Gaussian, $\\theta \\sim \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2})$. Its probability density function is:\n$$\np(\\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma_{0}^{2}}} \\exp\\left(-\\frac{(\\theta - \\mu_{0})^{2}}{2\\sigma_{0}^{2}}\\right)\n$$\nThe log-prior is therefore:\n$$\n\\ln p(\\theta) = -\\frac{1}{2}\\ln(2\\pi\\sigma_{0}^{2}) - \\frac{(\\theta - \\mu_{0})^{2}}{2\\sigma_{0}^{2}} = C_{\\text{prior}} - \\frac{(\\theta - \\mu_{0})^{2}}{2\\sigma_{0}^{2}}\n$$\nwhere $C_{\\text{prior}}$ is a constant.\n\nThe problem provides the log-likelihood approximation:\n$$\n\\ln p(y \\mid \\theta) \\approx \\ln p(y \\mid \\hat{\\theta}_{L}) - \\frac{1}{2}\\kappa(\\theta - \\hat{\\theta}_{L})^{2} = C_{\\text{like}} - \\frac{1}{2}\\kappa(\\theta - \\hat{\\theta}_{L})^{2}\n$$\nwhere $C_{\\text{like}}$ is a constant with respect to $\\theta$.\n\nCombining these, the log-posterior $L(\\theta) = \\ln p(\\theta \\mid y)$ is:\n$$\nL(\\theta) \\approx \\left(C_{\\text{prior}} + C_{\\text{like}}\\right) - \\frac{(\\theta - \\mu_{0})^{2}}{2\\sigma_{0}^{2}} - \\frac{\\kappa}{2}(\\theta - \\hat{\\theta}_{L})^{2}\n$$\nThis expression is a quadratic function of $\\theta$. A probability distribution whose logarithm is a quadratic function is a Gaussian distribution. Therefore, the resulting posterior approximation is Gaussian, and we can find its mean and variance by analyzing this quadratic form.\n\nThe Laplace approximation states that the posterior can be approximated by a Gaussian distribution centered at the Maximum A Posteriori (MAP) estimate, $\\hat{\\theta}_{\\text{MAP}}$. The MAP estimate is the value of $\\theta$ that maximizes $L(\\theta)$. We find it by setting the first derivative of $L(\\theta)$ with respect to $\\theta$ to zero:\n$$\n\\frac{dL}{d\\theta} = -\\frac{\\theta - \\mu_{0}}{\\sigma_{0}^{2}} - \\kappa(\\theta - \\hat{\\theta}_{L}) = 0\n$$\nSolving for $\\theta$ gives the MAP estimate, which we denote as $\\mu_{\\text{post}}$:\n$$\n-\\frac{\\mu_{\\text{post}}}{\\sigma_{0}^{2}} + \\frac{\\mu_{0}}{\\sigma_{0}^{2}} - \\kappa\\mu_{\\text{post}} + \\kappa\\hat{\\theta}_{L} = 0\n$$\n$$\n\\mu_{\\text{post}}\\left(\\frac{1}{\\sigma_{0}^{2}} + \\kappa\\right) = \\frac{\\mu_{0}}{\\sigma_{0}^{2}} + \\kappa\\hat{\\theta}_{L}\n$$\n$$\n\\mu_{\\text{post}} = \\frac{\\frac{\\mu_{0}}{\\sigma_{0}^{2}} + \\kappa\\hat{\\theta}_{L}}{\\frac{1}{\\sigma_{0}^{2}} + \\kappa}\n$$\nMultiplying the numerator and denominator by $\\sigma_{0}^{2}$ yields the final expression for the posterior mean:\n$$\n\\mu_{\\text{post}} = \\frac{\\mu_{0} + \\kappa \\sigma_{0}^{2} \\hat{\\theta}_{L}}{1 + \\kappa \\sigma_{0}^{2}}\n$$\nThis shows that the posterior mean is a precision-weighted average of the prior mean $\\mu_{0}$ and the maximum-likelihood estimate $\\hat{\\theta}_{L}$. The prior precision is $1/\\sigma_{0}^{2}$ and the likelihood precision (from the data) is $\\kappa$.\n\nThe variance of the Gaussian approximation, $\\sigma_{\\text{post}}^{2}$, is given by the inverse of the negative second derivative of the log-posterior evaluated at the MAP estimate. Let's compute the second derivative of $L(\\theta)$:\n$$\n\\frac{d^{2}L}{d\\theta^{2}} = \\frac{d}{d\\theta}\\left(-\\frac{\\theta - \\mu_{0}}{\\sigma_{0}^{2}} - \\kappa(\\theta - \\hat{\\theta}_{L})\\right) = -\\frac{1}{\\sigma_{0}^{2}} - \\kappa\n$$\nSince the second derivative is constant, we do not need to evaluate it at $\\mu_{\\text{post}}$. The negative of the second derivative is the posterior precision:\n$$\n\\frac{1}{\\sigma_{\\text{post}}^{2}} = -\\frac{d^{2}L}{d\\theta^{2}} = \\frac{1}{\\sigma_{0}^{2}} + \\kappa\n$$\nThis demonstrates that the posterior precision is the sum of the prior precision and the likelihood precision. The posterior variance, $\\sigma_{\\text{post}}^{2}$, is the reciprocal of this quantity:\n$$\n\\sigma_{\\text{post}}^{2} = \\left(\\frac{1}{\\sigma_{0}^{2}} + \\kappa\\right)^{-1} = \\frac{1}{\\frac{1 + \\kappa\\sigma_{0}^{2}}{\\sigma_{0}^{2}}} = \\frac{\\sigma_{0}^{2}}{1 + \\kappa\\sigma_{0}^{2}}\n$$\nThe approximate posterior distribution is thus $p(\\theta \\mid y) \\approx \\mathcal{N}(\\mu_{\\text{post}}, \\sigma_{\\text{post}}^{2})$. The required quantities are the posterior mean, $\\mu_{\\text{post}}$, and the posterior variance, $\\sigma_{\\text{post}}^{2}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\mu_{0} + \\kappa \\sigma_{0}^{2} \\hat{\\theta}_{L}}{1 + \\kappa \\sigma_{0}^{2}} & \\frac{\\sigma_{0}^{2}}{1 + \\kappa \\sigma_{0}^{2}} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Many physical parameters in electrochemistry, such as exchange current density ($i_0$) and diffusion coefficients ($D$), are inherently positive. This practice addresses the crucial technique of using a logarithmic transformation to handle these constraints, a common step for improving the stability and efficiency of computational algorithms. You will derive the Jacobian adjustment factor, which is essential for ensuring that the probability density is correctly transformed when changing variables, thereby preserving the integrity of the posterior distribution.",
            "id": "4237097",
            "problem": "An electrochemical kinetic model based on the Butler–Volmer relation is used to interpret transient current data from a chronoamperometry experiment for a single-electron outer-sphere redox couple. The exchange current density $i_0$ and the diffusion coefficient $D$ enter the model only through the likelihood and are known to be strictly positive physical parameters. A Bayesian calibration is formulated with data vector $\\mathbf{y}$, likelihood $L(\\mathbf{y}\\mid i_0, D)$, and a proper joint prior density $p(i_0, D)$ supported on $(0,\\infty)\\times(0,\\infty)$. The posterior density in the original parameterization is\n$$\n\\pi_{I_0,D}(i_0,D\\mid \\mathbf{y}) \\propto L(\\mathbf{y}\\mid i_0, D)\\,p(i_0, D).\n$$\nTo enforce positivity and improve numerical stability in Markov chain Monte Carlo (MCMC), you introduce the log-transform $(\\xi,\\eta) = (\\ln i_0, \\ln D)$ and perform inference in $(\\xi,\\eta)\\in \\mathbb{R}^2$ instead. Using only the change-of-variables theorem for probability densities as the fundamental starting point, derive the exact multiplicative Jacobian adjustment that must be applied to correctly express the posterior density in the transformed variables. Express your final answer as a single closed-form analytic expression for this Jacobian adjustment term as a function of $(\\xi,\\eta)$ only. Do not include any units. If you derive intermediate expressions, you must still report a single final expression. No rounding is required.",
            "solution": "The problem as stated is scientifically grounded, well-posed, objective, and contains sufficient information for a unique and meaningful solution. The context is a standard application of Bayesian parameter estimation for an electrochemical model, and the task requires the derivation of the Jacobian adjustment term for a change of variables, a fundamental procedure in probability theory. I will therefore proceed with the derivation.\n\nThe problem requires the derivation of the multiplicative Jacobian adjustment for transforming the posterior probability density from the original parameterization $(i_0, D)$ to the log-transformed parameterization $(\\xi, \\eta)$. The starting point is the change-of-variables theorem for multivariate probability densities.\n\nLet the original random variables be represented by the vector $\\mathbf{v} = \\begin{pmatrix} i_0 \\\\ D \\end{pmatrix}$ and the transformed variables by the vector $\\mathbf{u} = \\begin{pmatrix} \\xi \\\\ \\eta \\end{pmatrix}$. The transformation is given by:\n$$\n\\mathbf{u} = g(\\mathbf{v}) \\implies \\begin{pmatrix} \\xi \\\\ \\eta \\end{pmatrix} = \\begin{pmatrix} \\ln i_0 \\\\ \\ln D \\end{pmatrix}\n$$\nThe domain for the original parameters is $(i_0, D) \\in (0, \\infty) \\times (0, \\infty)$, and the domain for the transformed parameters is $(\\xi, \\eta) \\in \\mathbb{R}^2$. The respective posterior densities are denoted $\\pi_{I_0,D}(i_0, D \\mid \\mathbf{y})$ and $\\pi_{\\Xi,\\text{H}}(\\xi, \\eta \\mid \\mathbf{y})$.\n\nThe change-of-variables theorem states that the density in the new variables is related to the density in the old variables by:\n$$\n\\pi_{\\Xi,\\text{H}}(\\mathbf{u} \\mid \\mathbf{y}) = \\pi_{I_0,D}(g^{-1}(\\mathbf{u}) \\mid \\mathbf{y}) \\left| \\det(J_{g^{-1}}(\\mathbf{u})) \\right|\n$$\nwhere $g^{-1}$ is the inverse transformation and $J_{g^{-1}}(\\mathbf{u})$ is the Jacobian matrix of the inverse transformation, evaluated at $\\mathbf{u}$. The term $\\left| \\det(J_{g^{-1}}(\\mathbf{u})) \\right|$ is the multiplicative Jacobian adjustment that we must find.\n\nFirst, we determine the inverse transformation, $g^{-1}$, which maps $(\\xi, \\eta)$ back to $(i_0, D)$.\nFrom the forward transformation, we have:\n$$\n\\xi = \\ln i_0 \\implies i_0 = \\exp(\\xi)\n$$\n$$\n\\eta = \\ln D \\implies D = \\exp(\\eta)\n$$\nThus, the inverse transformation $\\mathbf{v} = g^{-1}(\\mathbf{u})$ is:\n$$\n\\begin{pmatrix} i_0 \\\\ D \\end{pmatrix} = \\begin{pmatrix} \\exp(\\xi) \\\\ \\exp(\\eta) \\end{pmatrix}\n$$\n\nNext, we compute the Jacobian matrix of this inverse transformation, $J_{g^{-1}}$. The elements of this matrix are the partial derivatives of the components of $g^{-1}$ with respect to the components of $\\mathbf{u}$:\n$$\nJ_{g^{-1}}(\\xi, \\eta) = \\frac{\\partial(i_0, D)}{\\partial(\\xi, \\eta)} = \\begin{pmatrix} \\frac{\\partial i_0}{\\partial \\xi} & \\frac{\\partial i_0}{\\partial \\eta} \\\\ \\frac{\\partial D}{\\partial \\xi} & \\frac{\\partial D}{\\partial \\eta} \\end{pmatrix}\n$$\nWe calculate the individual partial derivatives:\n$$\n\\frac{\\partial i_0}{\\partial \\xi} = \\frac{\\partial}{\\partial \\xi} \\left( \\exp(\\xi) \\right) = \\exp(\\xi)\n$$\n$$\n\\frac{\\partial i_0}{\\partial \\eta} = \\frac{\\partial}{\\partial \\eta} \\left( \\exp(\\xi) \\right) = 0\n$$\n$$\n\\frac{\\partial D}{\\partial \\xi} = \\frac{\\partial}{\\partial \\xi} \\left( \\exp(\\eta) \\right) = 0\n$$\n$$\n\\frac{\\partial D}{\\partial \\eta} = \\frac{\\partial}{\\partial \\eta} \\left( \\exp(\\eta) \\right) = \\exp(\\eta)\n$$\nSubstituting these into the Jacobian matrix gives:\n$$\nJ_{g^{-1}}(\\xi, \\eta) = \\begin{pmatrix} \\exp(\\xi) & 0 \\\\ 0 & \\exp(\\eta) \\end{pmatrix}\n$$\n\nNow, we compute the determinant of this matrix:\n$$\n\\det(J_{g^{-1}}(\\xi, \\eta)) = \\left( \\exp(\\xi) \\right) \\left( \\exp(\\eta) \\right) - (0)(0) = \\exp(\\xi)\\exp(\\eta)\n$$\n\nFinally, the Jacobian adjustment factor is the absolute value of this determinant. Since the exponential function $\\exp(x)$ is strictly positive for any real argument $x$, both $\\exp(\\xi)$ and $\\exp(\\eta)$ are positive. Therefore, their product is also strictly positive.\n$$\n\\left| \\det(J_{g^{-1}}(\\xi, \\eta)) \\right| = \\left| \\exp(\\xi)\\exp(\\eta) \\right| = \\exp(\\xi)\\exp(\\eta)\n$$\nThis can be written in the more compact form $\\exp(\\xi + \\eta)$.\n\nThis expression is the required multiplicative Jacobian adjustment. It is a function of only the transformed variables $(\\xi, \\eta)$, as required. The full expression for the posterior in the transformed space is:\n$$\n\\pi_{\\Xi,\\text{H}}(\\xi, \\eta \\mid \\mathbf{y}) \\propto L(\\mathbf{y} \\mid i_0=\\exp(\\xi), D=\\exp(\\eta)) \\, p(i_0=\\exp(\\xi), D=\\exp(\\eta)) \\, \\exp(\\xi+\\eta)\n$$",
            "answer": "$$\\boxed{\\exp(\\xi+\\eta)}$$"
        },
        {
            "introduction": "Beyond estimating parameters from existing data, Bayesian inference offers powerful tools for prospectively designing better experiments. This coding exercise challenges you to investigate parameter identifiability in a porous electrode model, exploring how different experimental measurements affect our ability to distinguish between tortuosity and conductivity. By computing sensitivity metrics and posterior correlations, you will see how this framework can quantify the information content of an experiment before it is even performed, guiding you toward designs that most effectively reduce uncertainty.",
            "id": "4237034",
            "problem": "Consider a one-dimensional (1D) porous electrode-electrolyte system under a steady-state galvanostatic experiment in computational electrochemistry. The effective ionic conduction through the electrolyte in the porous structure is modeled using the tortuosity-adjusted Ohm's law and a simple diffusion time scaling. Let the unknown parameters be the porous electrode tortuosity $\\tau$ (dimensionless) and the bulk electrolyte conductivity $\\kappa$ (in $\\mathrm{S}/\\mathrm{m}$). The effective ionic conductivity in the porous medium is $\\kappa_{\\mathrm{eff}} = \\kappa / \\tau$. The electrolyte thickness is $L$ (in $\\mathrm{m}$), and the applied current density is $j$ (in $\\mathrm{A}/\\mathrm{m}^2$).\n\nFundamental base:\n- Ohm's law in one dimension implies a potential drop $V$ (in $\\mathrm{V}$) across the electrolyte satisfying $V = (L/\\kappa_{\\mathrm{eff}})\\, j$. Using $\\kappa_{\\mathrm{eff}} = \\kappa/\\tau$, this becomes $V = L\\, (\\tau/\\kappa)\\, j$.\n- A simple diffusion time scaling for concentration relaxation in porous media implies a characteristic time $T$ (in $\\mathrm{s}$) of the form $T = (\\tau L^2)/D$, where $D$ (in $\\mathrm{m}^2/\\mathrm{s}$) is the effective electrolyte diffusivity treated as known.\n\nBayesian uncertainty quantification setup:\n- Define the transformed parameters $s = \\ln \\tau$ and $r = \\ln \\kappa$.\n- Assume independent Gaussian priors on $s$ and $r$: $s \\sim \\mathcal{N}(\\mu_s,\\sigma_s^2)$ and $r \\sim \\mathcal{N}(\\mu_r,\\sigma_r^2)$.\n- Measurements are modeled with independent Gaussian noise. For voltage measurements $V_k$ at current densities $j_k$, the likelihood is $V_k \\sim \\mathcal{N}(f_V(s,r;j_k),\\sigma_V^2)$ with $f_V(s,r;j) = L\\, e^{s-r}\\, j$. For diffusion time measurements $T \\sim \\mathcal{N}(f_T(s),\\sigma_T^2)$ with $f_T(s) = e^{s}\\, L^2/D$.\n\nLocal sensitivity and identifiability:\n- The whitened sensitivity (Jacobian) matrix $\\mathbf{J}_w$ is formed by stacking the gradients of each measurement with respect to $(s,r)$, each divided by its noise standard deviation. For a voltage measurement at $j_k$, the gradient row is $\\left[\\partial f_V/\\partial s,\\, \\partial f_V/\\partial r\\right]/\\sigma_V = \\left[f_V(s,r;j_k),\\, -f_V(s,r;j_k)\\right]/\\sigma_V$. For a diffusion time measurement, the gradient row is $\\left[\\partial f_T/\\partial s,\\, \\partial f_T/\\partial r\\right]/\\sigma_T = \\left[f_T(s),\\, 0\\right]/\\sigma_T$.\n- The data Fisher information matrix is $\\mathbf{F} = \\mathbf{J}_w^\\top \\mathbf{J}_w$. The Laplace approximation to the posterior around the maximum a posteriori point adds the prior precision, yielding a Hessian $\\mathbf{H} = \\mathbf{F} + \\boldsymbol{\\Lambda}$, where $\\boldsymbol{\\Lambda} = \\mathrm{diag}(1/\\sigma_s^2, 1/\\sigma_r^2)$ in the $(s,r)$ coordinates. The approximate posterior covariance is $\\boldsymbol{\\Sigma} \\approx \\mathbf{H}^{-1}$.\n- Identifiability can be probed via (i) the smallest singular value of $\\mathbf{J}_w$, a dimensionless number indicating rank deficiency when it is near zero, and (ii) the posterior correlation coefficient between $s$ and $r$, computed as $\\rho = \\boldsymbol{\\Sigma}_{sr}/\\sqrt{\\boldsymbol{\\Sigma}_{ss}\\,\\boldsymbol{\\Sigma}_{rr}}$, which is dimensionless and approaches $\\pm 1$ under strong non-identifiability.\n\nTask:\n- Using the above base, set up the joint posterior over $(s,r)$ under three experimental designs. For all designs, use the same ground-truth parameters $\\tau_{\\mathrm{true}}$ and $\\kappa_{\\mathrm{true}}$, and construct the sensitivity and posterior metrics at $(s,r) = (\\ln \\tau_{\\mathrm{true}}, \\ln \\kappa_{\\mathrm{true}})$.\n- Use the following fixed physical constants and priors:\n  - $L = 1.0\\times 10^{-4}\\ \\mathrm{m}$, $D = 1.5\\times 10^{-9}\\ \\mathrm{m}^2/\\mathrm{s}$.\n  - $\\tau_{\\mathrm{true}} = 2.5$ (dimensionless), $\\kappa_{\\mathrm{true}} = 1.0\\ \\mathrm{S}/\\mathrm{m}$.\n  - Priors on $s$ and $r$: $\\mu_s = \\ln(2.5)$, $\\mu_r = \\ln(1.0)$, with $\\sigma_s = 2.0$ and $\\sigma_r = 2.0$.\n- Experimental designs (each is a test case):\n  1. Ohmic-only design: three voltage measurements at current densities $j = [100, 200, 300]\\ \\mathrm{A}/\\mathrm{m}^2$, with voltage noise standard deviation $\\sigma_V = 1.0\\times 10^{-3}\\ \\mathrm{V}$. No diffusion time measurement.\n  2. Combined design: the same three voltage measurements as in test case 1 with $\\sigma_V = 1.0\\times 10^{-3}\\ \\mathrm{V}$, plus one diffusion time measurement with $\\sigma_T = 1.0\\times 10^{-2}\\ \\mathrm{s}$.\n  3. Weak time-information design: the same voltage measurements with $\\sigma_V = 1.0\\times 10^{-3}\\ \\mathrm{V}$, plus one diffusion time measurement with larger noise $\\sigma_T = 1.0\\ \\mathrm{s}$.\n- For each test case, compute:\n  - The posterior correlation coefficient $\\rho$ between $s$ and $r$ (dimensionless).\n  - The smallest singular value $s_{\\min}$ of the whitened sensitivity matrix $\\mathbf{J}_w$ (dimensionless).\n- Required final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"). In this problem, the single-line output must be \"[\\rho_1,s_{\\min,1},\\rho_2,s_{\\min,2},\\rho_3,s_{\\min,3}]\", where the index $1,2,3$ corresponds to test cases 1, 2, and 3. All outputs are floats. There are no angles and no percentages in this problem.\n\nEnsure scientific realism by using the specified units: $L$ in $\\mathrm{m}$, $D$ in $\\mathrm{m}^2/\\mathrm{s}$, $\\kappa$ in $\\mathrm{S}/\\mathrm{m}$, $\\tau$ dimensionless, $j$ in $\\mathrm{A}/\\mathrm{m}^2$, $V$ in $\\mathrm{V}$, and $T$ in $\\mathrm{s}$. The computed metrics $\\rho$ and $s_{\\min}$ are unitless floats. Your program must be self-contained and must not require any input. It should aggregate results over the three test cases exactly as specified.",
            "solution": "The problem is assessed to be valid. It is scientifically sound, self-contained, and well-posed, providing all necessary physical constants, mathematical models, and experimental conditions to perform the requested analysis on parameter identifiability in a computational electrochemistry context.\n\nThe objective is to quantify parameter identifiability for a simplified porous electrode model under three different experimental designs. The unknown parameters are the tortuosity $\\tau$ and the bulk electrolyte conductivity $\\kappa$. We work with their natural logarithms, $s = \\ln \\tau$ and $r = \\ln \\kappa$. The analysis is centered at the ground-truth parameter values, which coincide with the prior means. We will compute two metrics for each design: the smallest singular value $s_{\\min}$ of the whitened sensitivity matrix $\\mathbf{J}_w$, and the posterior correlation coefficient $\\rho$ between $s$ and $r$ derived from the Laplace approximation.\n\nFirst, we establish the constants and ground-truth values.\nThe physical constants are:\n- Electrolyte thickness, $L = 1.0 \\times 10^{-4}\\ \\mathrm{m}$\n- Effective electrolyte diffusivity, $D = 1.5 \\times 10^{-9}\\ \\mathrm{m}^2/\\mathrm{s}$\n\nThe ground-truth parameters are:\n- Tortuosity, $\\tau_{\\mathrm{true}} = 2.5$\n- Conductivity, $\\kappa_{\\mathrm{true}} = 1.0\\ \\mathrm{S}/\\mathrm{m}$\n\nThe analysis is performed at the corresponding transformed parameter point:\n- $s_{\\mathrm{true}} = \\ln(\\tau_{\\mathrm{true}}) = \\ln(2.5)$\n- $r_{\\mathrm{true}} = \\ln(\\kappa_{\\mathrm{true}}) = \\ln(1.0) = 0$\n\nThe independent Gaussian priors on $s$ and $r$ are given by $\\mu_s = \\ln(2.5)$, $\\mu_r = \\ln(1.0)$, and standard deviations $\\sigma_s = 2.0$ and $\\sigma_r = 2.0$. The prior precision matrix $\\boldsymbol{\\Lambda}$ is diagonal with entries $1/\\sigma^2$, which is constant for all test cases:\n$$\n\\boldsymbol{\\Lambda} = \\begin{pmatrix} 1/\\sigma_s^2 & 0 \\\\ 0 & 1/\\sigma_r^2 \\end{pmatrix} = \\begin{pmatrix} 1/2.0^2 & 0 \\\\ 0 & 1/2.0^2 \\end{pmatrix} = \\begin{pmatrix} 0.25 & 0 \\\\ 0 & 0.25 \\end{pmatrix}\n$$\n\nThe model functions evaluated at the ground-truth point $(s_{\\mathrm{true}}, r_{\\mathrm{true}})$ are:\n- Voltage model: $f_V(s,r;j) = L e^{s-r} j$. At the ground truth, $f_V(s_{\\mathrm{true}}, r_{\\mathrm{true}}; j) = L e^{s_{\\mathrm{true}}-r_{\\mathrm{true}}} j = L (\\tau_{\\mathrm{true}}/\\kappa_{\\mathrm{true}}) j = (1.0 \\times 10^{-4}\\ \\mathrm{m}) (2.5 / 1.0\\ \\mathrm{S}/\\mathrm{m}) j = (2.5 \\times 10^{-4})\\, j$.\n- Time model: $f_T(s) = (L^2/D) e^s$. At the ground truth, $f_T(s_{\\mathrm{true}}) = (L^2/D) e^{s_{\\mathrm{true}}} = (L^2/D) \\tau_{\\mathrm{true}} = \\frac{(1.0 \\times 10^{-4}\\ \\mathrm{m})^2}{1.5 \\times 10^{-9}\\ \\mathrm{m}^2/\\mathrm{s}} \\times 2.5 = \\frac{1.0 \\times 10^{-8}}{1.5 \\times 10^{-9}} \\times 2.5\\ \\mathrm{s} = \\frac{10}{1.5} \\times 2.5\\ \\mathrm{s} = \\frac{50}{3}\\ \\mathrm{s}$.\n\nThe gradient rows for the whitened sensitivity matrix $\\mathbf{J}_w$ are derived from the model functions:\n- For a voltage measurement $V_k$ at current density $j_k$:\n$$\n\\mathbf{g}_{V,k} = \\frac{1}{\\sigma_V} \\left[ \\frac{\\partial f_V}{\\partial s}, \\frac{\\partial f_V}{\\partial r} \\right] = \\frac{1}{\\sigma_V} [f_V, -f_V]\n$$\n- For a diffusion time measurement $T$:\n$$\n\\mathbf{g}_{T} = \\frac{1}{\\sigma_T} \\left[ \\frac{\\partial f_T}{\\partial s}, \\frac{\\partial f_T}{\\partial r} \\right] = \\frac{1}{\\sigma_T} [f_T, 0]\n$$\n\nWe now proceed with each test case.\n\n**Test Case 1: Ohmic-only Design**\nThis design involves three voltage measurements at $j = [100, 200, 300]\\ \\mathrm{A}/\\mathrm{m}^2$ with $\\sigma_V = 1.0 \\times 10^{-3}\\ \\mathrm{V}$. There is no diffusion time measurement.\n\nThe model predictions $f_V(j_k)$ are:\n- $f_V(100) = (2.5 \\times 10^{-4}) \\times 100 = 0.025\\ \\mathrm{V}$\n- $f_V(200) = (2.5 \\times 10^{-4}) \\times 200 = 0.050\\ \\mathrm{V}$\n- $f_V(300) = (2.5 \\times 10^{-4}) \\times 300 = 0.075\\ \\mathrm{V}$\n\nThe rows of the whitened sensitivity matrix $\\mathbf{J}_w$ are:\n- $\\mathbf{g}_{V,1} = \\frac{1}{1.0 \\times 10^{-3}} [0.025, -0.025] = [25, -25]$\n- $\\mathbf{g}_{V,2} = \\frac{1}{1.0 \\times 10^{-3}} [0.050, -0.050] = [50, -50]$\n- $\\mathbf{g}_{V,3} = \\frac{1}{1.0 \\times 10^{-3}} [0.075, -0.075] = [75, -75]$\n\nSo, the sensitivity matrix is:\n$$\n\\mathbf{J}_{w,1} = \\begin{pmatrix} 25 & -25 \\\\ 50 & -50 \\\\ 75 & -75 \\end{pmatrix}\n$$\nThe columns of $\\mathbf{J}_{w,1}$ are linearly dependent, meaning the matrix is rank-deficient (rank $1$). Consequently, its smallest singular value is $s_{\\min,1} = 0$. This indicates that from voltage data alone, the parameters $s$ and $r$ are non-identifiable, as they only influence the measurement through the combination $s-r$.\n\nThe data Fisher Information Matrix (FIM) is $\\mathbf{F}_1 = \\mathbf{J}_{w,1}^\\top \\mathbf{J}_{w,1}$:\n$$\n\\mathbf{F}_1 = \\begin{pmatrix} 25 & 50 & 75 \\\\ -25 & -50 & -75 \\end{pmatrix} \\begin{pmatrix} 25 & -25 \\\\ 50 & -50 \\\\ 75 & -75 \\end{pmatrix} = \\begin{pmatrix} 8750 & -8750 \\\\ -8750 & 8750 \\end{pmatrix}\n$$\nThe Hessian is $\\mathbf{H}_1 = \\mathbf{F}_1 + \\boldsymbol{\\Lambda}$:\n$$\n\\mathbf{H}_1 = \\begin{pmatrix} 8750 & -8750 \\\\ -8750 & 8750 \\end{pmatrix} + \\begin{pmatrix} 0.25 & 0 \\\\ 0 & 0.25 \\end{pmatrix} = \\begin{pmatrix} 8750.25 & -8750 \\\\ -8750 & 8750.25 \\end{pmatrix}\n$$\nThe posterior covariance is $\\boldsymbol{\\Sigma}_1 = \\mathbf{H}_1^{-1}$:\n$$\n\\boldsymbol{\\Sigma}_1 = \\frac{1}{8750.25^2 - (-8750)^2} \\begin{pmatrix} 8750.25 & 8750 \\\\ 8750 & 8750.25 \\end{pmatrix} \\approx \\frac{1}{4375.125} \\begin{pmatrix} 8750.25 & 8750 \\\\ 8750 & 8750.25 \\end{pmatrix}\n$$\nThe posterior correlation coefficient is $\\rho_1 = \\boldsymbol{\\Sigma}_{1,sr} / \\sqrt{\\boldsymbol{\\Sigma}_{1,ss} \\boldsymbol{\\Sigma}_{1,rr}}$:\n$$\n\\rho_1 = \\frac{8750 / 4375.125}{\\sqrt{(8750.25 / 4375.125) \\times (8750.25 / 4375.125)}} = \\frac{8750}{8750.25} \\approx 0.999971\n$$\nThis value, extremely close to $1$, confirms the strong non-identifiability. The only reason it is not exactly $1$ is the regularizing effect of the prior.\n\n**Test Case 2: Combined Design**\nThis design adds one diffusion time measurement with $\\sigma_T = 1.0 \\times 10^{-2}\\ \\mathrm{s}$ to the measurements from Case $1$.\nThe model prediction for time is $f_T = 50/3\\ \\mathrm{s}$. The corresponding gradient row is:\n$$\n\\mathbf{g}_{T} = \\frac{1}{1.0 \\times 10^{-2}} [50/3, 0] = [5000/3, 0] \\approx [1666.67, 0]\n$$\nThe new sensitivity matrix $\\mathbf{J}_{w,2}$ has an additional row:\n$$\n\\mathbf{J}_{w,2} = \\begin{pmatrix} 25 & -25 \\\\ 50 & -50 \\\\ 75 & -75 \\\\ 5000/3 & 0 \\end{pmatrix}\n$$\nThis matrix is now full rank (rank $2$), as the new row breaks the linear dependence between the columns. The smallest singular value $s_{\\min,2}$ will be non-zero.\nThe FIM is $\\mathbf{F}_2 = \\mathbf{J}_{w,2}^\\top \\mathbf{J}_{w,2} = \\mathbf{F}_1 + \\mathbf{g}_{T}^\\top \\mathbf{g}_{T}$:\n$$\n\\mathbf{F}_2 = \\begin{pmatrix} 8750 & -8750 \\\\ -8750 & 8750 \\end{pmatrix} + \\begin{pmatrix} (5000/3)^2 & 0 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 8750 + 25000000/9 & -8750 \\\\ -8750 & 8750 \\end{pmatrix}\n$$\nThe Hessian is $\\mathbf{H}_2 = \\mathbf{F}_2 + \\boldsymbol{\\Lambda}$:\n$$\n\\mathbf{H}_2 = \\begin{pmatrix} 8750.25 + 25000000/9 & -8750 \\\\ -8750 & 8750.25 \\end{pmatrix} \\approx \\begin{pmatrix} 2786528.03 & -8750 \\\\ -8750 & 8750.25 \\end{pmatrix}\n$$\nInverting $\\mathbf{H}_2$ to get $\\boldsymbol{\\Sigma}_2$ and then computing $\\rho_2 = \\boldsymbol{\\Sigma}_{2,sr} / \\sqrt{\\boldsymbol{\\Sigma}_{2,ss} \\boldsymbol{\\Sigma}_{2,rr}}$ will yield a significantly lower correlation, as the time measurement provides unique information about $s$.\n\n**Test Case 3: Weak Time-information Design**\nThis case is identical to Case $2$, but the noise on the time measurement is much larger: $\\sigma_T = 1.0\\ \\mathrm{s}$.\nThe gradient row for the time measurement is now:\n$$\n\\mathbf{g}_{T} = \\frac{1}{1.0} [50/3, 0] = [50/3, 0] \\approx [16.67, 0]\n$$\nThe sensitivity matrix is:\n$$\n\\mathbf{J}_{w,3} = \\begin{pmatrix} 25 & -25 \\\\ 50 & -50 \\\\ 75 & -75 \\\\ 50/3 & 0 \\end{pmatrix}\n$$\nThe matrix is still full rank, but the magnitude of the new row is much smaller than in Case $2$, meaning it provides less information to distinguish $s$ and $r$. Thus, we expect $s_{\\min,3}$ to be smaller than $s_{\\min,2}$ but greater than $s_{\\min,1}$.\nThe FIM is $\\mathbf{F}_3 = \\mathbf{F}_1 + \\mathbf{g}_{T}^\\top \\mathbf{g}_{T}$:\n$$\n\\mathbf{F}_3 = \\begin{pmatrix} 8750 & -8750 \\\\ -8750 & 8750 \\end{pmatrix} + \\begin{pmatrix} (50/3)^2 & 0 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 8750 + 2500/9 & -8750 \\\\ -8750 & 8750 \\end{pmatrix}\n$$\nThe Hessian is $\\mathbf{H}_3 = \\mathbf{F}_3 + \\boldsymbol{\\Lambda}$:\n$$\n\\mathbf{H}_3 = \\begin{pmatrix} 8750.25 + 2500/9 & -8750 \\\\ -8750 & 8750.25 \\end{pmatrix} \\approx \\begin{pmatrix} 9028.03 & -8750 \\\\ -8750 & 8750.25 \\end{pmatrix}\n$$\nSince the added information is weaker, we expect the resulting correlation $\\rho_3$ to be lower than $\\rho_1$ but significantly higher than $\\rho_2$.\n\nThe final numerical values will be computed by the implementing program.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes parameter identifiability metrics for a porous electrode model\n    under three different experimental designs.\n    \"\"\"\n    # Define fixed physical constants and ground-truth parameters\n    L = 1.0e-4  # m\n    D = 1.5e-9  # m^2/s\n    tau_true = 2.5  # dimensionless\n    kappa_true = 1.0  # S/m\n\n    # Define prior distribution parameters\n    sigma_s = 2.0\n    sigma_r = 2.0\n\n    # Test cases representing the three experimental designs\n    test_cases = [\n        {\n            \"name\": \"Ohmic-only\",\n            \"j_values\": np.array([100.0, 200.0, 300.0]),\n            \"sigma_V\": 1.0e-3,\n            \"sigma_T\": None  # No diffusion time measurement\n        },\n        {\n            \"name\": \"Combined\",\n            \"j_values\": np.array([100.0, 200.0, 300.0]),\n            \"sigma_V\": 1.0e-3,\n            \"sigma_T\": 1.0e-2\n        },\n        {\n            \"name\": \"Weak time-information\",\n            \"j_values\": np.array([100.0, 200.0, 300.0]),\n            \"sigma_V\": 1.0e-3,\n            \"sigma_T\": 1.0\n        }\n    ]\n\n    results = []\n\n    # Transformed ground-truth parameters\n    s_true = np.log(tau_true)\n    r_true = np.log(kappa_true)\n\n    # Prior precision matrix (same for all cases)\n    prior_precision = np.diag([1.0/sigma_s**2, 1.0/sigma_r**2])\n\n    for case in test_cases:\n        # --- Step 1: Construct the whitened sensitivity matrix J_w ---\n        \n        jw_rows = []\n\n        # Add rows for voltage measurements\n        if case[\"j_values\"] is not None and case[\"sigma_V\"] is not None:\n            sigma_V = case[\"sigma_V\"]\n            for j in case[\"j_values\"]:\n                # Model function for voltage\n                f_V = L * np.exp(s_true - r_true) * j\n                # Whitened gradient row: [dfV/ds, dfV/dr] / sigma_V\n                grad_row_v = np.array([f_V, -f_V]) / sigma_V\n                jw_rows.append(grad_row_v)\n\n        # Add row for diffusion time measurement\n        if case[\"sigma_T\"] is not None:\n            sigma_T = case[\"sigma_T\"]\n            # Model function for time\n            f_T = (L**2 / D) * np.exp(s_true)\n            # Whitened gradient row: [dfT/ds, dfT/dr] / sigma_T\n            grad_row_t = np.array([f_T, 0.0]) / sigma_T\n            jw_rows.append(grad_row_t)\n\n        J_w = np.array(jw_rows)\n\n        # --- Step 2: Compute the smallest singular value of J_w ---\n        \n        if J_w.shape[0]  J_w.shape[1]:\n            # This case won't happen here, but for completeness\n            # pad with zero rows to make it square or tall\n            zero_padding = np.zeros((J_w.shape[1] - J_w.shape[0], J_w.shape[1]))\n            J_w_svd = np.vstack((J_w, zero_padding))\n        else:\n            J_w_svd = J_w\n\n        singular_values = np.linalg.svd(J_w_svd, compute_uv=False)\n        s_min = np.min(singular_values)\n\n        # --- Step 3: Compute the posterior correlation coefficient rho ---\n\n        # Data Fisher Information Matrix\n        F = J_w.T @ J_w\n        \n        # Hessian of the negative log-posterior\n        H = F + prior_precision\n\n        # Posterior covariance matrix (inverse of Hessian)\n        Sigma = np.linalg.inv(H)\n\n        Sigma_ss = Sigma[0, 0]\n        Sigma_rr = Sigma[1, 1]\n        Sigma_sr = Sigma[0, 1]\n        \n        # Posterior correlation coefficient\n        rho = Sigma_sr / np.sqrt(Sigma_ss * Sigma_rr)\n\n        results.extend([rho, s_min])\n    \n    # Format and print the final output string\n    print(f\"[{','.join(f'{x:.8f}' for x in results)}]\")\n\nsolve()\n```"
        }
    ]
}