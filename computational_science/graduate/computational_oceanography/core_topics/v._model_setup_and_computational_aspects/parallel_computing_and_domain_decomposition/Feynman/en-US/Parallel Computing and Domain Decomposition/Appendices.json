{
    "hands_on_practices": [
        {
            "introduction": "The efficiency of a parallel algorithm is often dictated by the ratio of computation to communication. For domain decomposition strategies, this translates to the \"volume-to-surface\" ratio of the subdomains. This first exercise  provides a foundational tool for performance analysis by asking you to derive an analytical expression for the total communication volume in a standard 2D block decomposition. Mastering this type of derivation is the first step toward building predictive performance models and understanding how algorithm scalability is tied to decomposition geometry.",
            "id": "3806486",
            "problem": "Consider a structured, logically rectangular, two-dimensional ($2$-D) ocean model grid of size $N_{x} \\times N_{y}$ cells, where $N_{x}$ is the number of cells in the zonal ($x$) direction and $N_{y}$ is the number of cells in the meridional ($y$) direction. The grid is decomposed into $P_{x} \\times P_{y}$ non-overlapping rectangular subdomains using a standard Cartesian block decomposition, with $P_{x}$ partitions along the $x$ direction and $P_{y}$ partitions along the $y$ direction. Assume $N_{x}$ is divisible by $P_{x}$ and $N_{y}$ is divisible by $P_{y}$, so each subdomain has interior size $n_{x} = N_{x}/P_{x}$ by $n_{y} = N_{y}/P_{y}$ cells. The external physical boundary of the global domain does not require communication, and the halo exchanges occur only across inter-partition interfaces.\n\nThe time-stepping scheme employs a finite-difference stencil that requires a fixed halo width of $w$ cells on each face of a subdomain. At each time step, each pair of neighboring subdomains exchanges halo data across their shared interface: a strip of $w$ cells of thickness normal to the interface and of length equal to the interface length. This exchange is symmetric, so each inter-partition interface induces two messages (one in each direction), each carrying exactly $w$ layers times the interface length times the per-cell state size. Let the per-cell state size be $b$ bytes (for example, $b = 8S$ for $S$ double-precision prognostic scalars). Corner (diagonal) halo exchanges are neglected; only face-adjacent exchanges are performed.\n\nUsing the fundamental definition of edge cut in graph partitioning (the number of edges that connect vertices assigned to different partitions) and the geometric structure of the block decomposition, derive:\n- the total number of inter-partition faces (the edge cut) as a function of $P_{x}$ and $P_{y}$, and\n- a closed-form expression for the total bytes communicated per time step across all processes, counting each transmitted byte once, in terms of $N_{x}$, $N_{y}$, $P_{x}$, $P_{y}$, $w$, and $b$.\n\nExpress your final answer as a single analytical expression for the total bytes communicated per time step in bytes. No numerical evaluation is required, and no rounding should be performed.",
            "solution": "The problem as stated is scientifically grounded, well-posed, objective, and contains sufficient information for a unique analytical solution. It represents a standard analysis of communication costs in parallel computing on structured grids. Therefore, a solution will be derived.\n\nThe problem asks for two quantities: the total number of inter-partition faces (the edge cut) and the total bytes communicated per time step. The latter will be the final answer.\n\n**Part 1: Derivation of the Total Number of Inter-Partition Faces (Edge Cut)**\n\nThe global computational grid of size $N_{x} \\times N_{y}$ cells is decomposed into a logical grid of $P_{x} \\times P_{y}$ subdomains. An inter-partition face, or interface, is the boundary shared between two adjacent subdomains. We can find the total number of such interfaces by counting the vertical and horizontal interfaces separately.\n\n1.  **Vertical Interfaces:** The $P_{x} \\times P_{y}$ arrangement of subdomains has $P_{x}$ columns. Between any two adjacent columns, there is a line of vertical interfaces. There are $P_{x}-1$ such lines of interfaces. Each of these lines runs across all $P_{y}$ rows of subdomains. Therefore, the number of vertical interfaces, $F_{v}$, is the product of the number of columns of interfaces and the number of rows of subdomains:\n    $$F_{v} = (P_{x}-1) P_{y}$$\n\n2.  **Horizontal Interfaces:** Similarly, the arrangement has $P_{y}$ rows of subdomains. This creates $P_{y}-1$ lines of horizontal interfaces between adjacent rows. Each of these lines runs across all $P_{x}$ columns of subdomains. Therefore, the number of horizontal interfaces, $F_{h}$, is:\n    $$F_{h} = (P_{y}-1) P_{x}$$\n\nThe total number of inter-partition faces, $F_{total}$, which corresponds to the edge cut of the decomposition graph, is the sum of the vertical and horizontal interfaces:\n$$F_{total} = F_{v} + F_{h} = P_{y}(P_{x}-1) + P_{x}(P_{y}-1)$$\nThis can be expanded to $P_{x}P_{y} - P_{y} + P_{x}P_{y} - P_{x}$, which simplifies to $2P_{x}P_{y} - P_{x} - P_{y}$. This expression provides the total number of internal boundaries in the decomposition.\n\n**Part 2: Derivation of Total Communication Volume**\n\nAt each time step, every internal interface requires a halo exchange. The problem states that the exchange is symmetric, inducing two messages per interface (one in each direction), and that we must count all transmitted bytes. This means for each interface, we calculate the size of the data transferred in one direction and multiply by $2$.\n\nThe size of a single message is determined by the interface length (in cells), the halo width $w$ (in cells), and the per-cell state size $b$ (in bytes).\n\n1.  **Communication across Vertical Interfaces:**\n    *   The length of a single vertical interface is the number of cells in the meridional ($y$) direction within one subdomain. This is given by $n_{y} = N_{y}/P_{y}$.\n    *   The data to be exchanged is a halo region of width $w$ and length $n_{y}$. The number of cells in this halo region is $w \\times n_{y}$.\n    *   The size of a single message (in one direction) across a vertical interface is $C_{v,1} = w \\cdot n_{y} \\cdot b = w \\cdot \\frac{N_{y}}{P_{y}} \\cdot b$ bytes.\n    *   There are $F_{v} = P_{y}(P_{x}-1)$ vertical interfaces. For each, there are two messages.\n    *   The total bytes communicated across all vertical interfaces, $B_{v}$, is:\n        $$B_{v} = F_{v} \\times C_{v,1} \\times 2 = P_{y}(P_{x}-1) \\times \\left(w \\frac{N_{y}}{P_{y}} b \\right) \\times 2$$\n    *   The term $P_{y}$ cancels out, simplifying the expression to:\n        $$B_{v} = 2wb N_{y} (P_{x}-1)$$\n\n2.  **Communication across Horizontal Interfaces:**\n    *   The length of a single horizontal interface is the number of cells in the zonal ($x$) direction within one subdomain, given by $n_{x} = N_{x}/P_{x}$.\n    *   The number of cells in the corresponding halo region is $w \\times n_{x}$.\n    *   The size of a single message across a horizontal interface is $C_{h,1} = w \\cdot n_{x} \\cdot b = w \\cdot \\frac{N_{x}}{P_{x}} \\cdot b$ bytes.\n    *   There are $F_{h} = P_{x}(P_{y}-1)$ horizontal interfaces.\n    *   The total bytes communicated across all horizontal interfaces, $B_{h}$, is:\n        $$B_{h} = F_{h} \\times C_{h,1} \\times 2 = P_{x}(P_{y}-1) \\times \\left(w \\frac{N_{x}}{P_{x}} b \\right) \\times 2$$\n    *   The term $P_{x}$ cancels out, simplifying to:\n        $$B_{h} = 2wb N_{x} (P_{y}-1)$$\n\n3.  **Total Communication Volume:**\n    The total bytes communicated per time step, $B_{total}$, is the sum of the bytes communicated across all vertical and horizontal interfaces.\n    $$B_{total} = B_{v} + B_{h} = 2wb N_{y} (P_{x}-1) + 2wb N_{x} (P_{y}-1)$$\n    Factoring out the common term $2wb$ yields the final closed-form expression:\n    $$B_{total} = 2wb \\left( N_{x}(P_{y}-1) + N_{y}(P_{x}-1) \\right)$$\nThis expression represents the total communication volume per time step under the specified conditions. It is dependent on the global grid dimensions ($N_{x}, N_{y}$), the partitioning scheme ($P_{x}, P_{y}$), the halo width ($w$), and the data size per cell ($b$).",
            "answer": "$$\\boxed{2wb\\left(N_{x}(P_{y}-1) + N_{y}(P_{x}-1)\\right)}$$"
        },
        {
            "introduction": "While analytical models help predict performance, correct and efficient implementation is paramount to achieving it. This practice  moves from theory to implementation, focusing on the critical task of performing a halo exchange using nonblocking MPI operations. You will explore how to overlap communication with computation to hide latency and, most importantly, how to structure your code to avoid the pernicious problem of deadlock, a common pitfall in parallel programming.",
            "id": "3806415",
            "problem": "Consider a three-dimensional finite-volume discretization of a passive tracer transport equation used in computational oceanography, where the computational domain is decomposed into subdomains arranged on a Cartesian grid. Each subdomain is assigned to a distinct process, and the processes communicate through the Message Passing Interface (MPI). To update fluxes with a stencil that depends on nearest neighbors, each process must exchange halo data with up to $6$ face-adjacent neighbors (in $+\\hat{x}$, $-\\hat{x}$, $+\\hat{y}$, $-\\hat{y}$, $+\\hat{z}$, $-\\hat{z}$ directions) with a halo thickness of $1$ cell. You are asked to derive, from first principles, a correct and deadlock-free sequence of nonblocking MPI operations that achieves a $3\\text{D}$ halo exchange while overlapping computation and communication, and to formalize the conditions under which deadlocks can occur if ordering is violated.\n\nBase your derivation on the following fundamental considerations:\n- The $3\\text{D}$ domain decomposition defines a set of processes arranged as a grid with dimensions $(N_x,N_y,N_z)$, where each process has coordinates $(i,j,k)$ with $0 \\le i < N_x$, $0 \\le j < N_y$, and $0 \\le k < N_z$. Face-adjacent neighbor relations follow from grid adjacency.\n- Nonblocking MPI operations, such as $\\mathrm{Irecv}$ and $\\mathrm{Isend}$, initiate communication without blocking the caller. Completion requires a matching peer operation and is detected by $\\mathrm{Wait}$ or $\\mathrm{Waitall}$.\n- In ocean model stencils, interior updates that do not require halo values can be computed while halo messages are in transit, enabling overlap between communication and computation.\n- Matching in MPI is determined by communicator, source or destination rank, and message tag; correctness requires symmetric agreements on these.\n\nYour derivation must:\n- Explicitly define the sequence of operations per process in terms of $\\mathrm{Irecv}$, $\\mathrm{Isend}$, and $\\mathrm{Waitall}$, and justify why posting receives before sends is sufficient to avoid deadlock while enabling overlap of computation and communication.\n- State conditions under which deadlocks can occur if ordering or matching is violated, such as cyclic waits and tag mismatches.\n- Provide a formal description of neighbor sets and tags sufficient for a simulator to determine whether a given global schedule completes.\n\nThen, implement a simulator that models this nonblocking halo exchange at a logical level (no actual MPI calls). Represent processes arranged in a grid of size $(N_x,N_y,N_z)$, with each process posting a set of nonblocking operations according to a chosen scheduling policy. The simulator must progress communications by matching posted sends and receives; a $\\mathrm{Waitall}$ succeeds only when all posted operations by that process are matched. If progress stalls with unmatched operations and some processes are waiting, the system is in deadlock.\n\nDefine the following scheduling policies:\n- \"recv_then_send\": a process posts all $\\mathrm{Irecv}$ to its existing face neighbors, then posts all $\\mathrm{Isend}$, performs overlapped computation on interior cells, and finally calls $\\mathrm{Waitall}$ on all posted operations.\n- \"send_then_wait_no_recv\": a process posts $\\mathrm{Isend}$ to all existing face neighbors and immediately calls $\\mathrm{Waitall}$ without ever posting $\\mathrm{Irecv}$.\n- \"recv_then_send_tag_mismatch_X\": like \"recv_then_send\" but the process uses an incorrect tag for messages on the $x$-faces (violating tag matching for those faces); correct tags are used for $y$ and $z$ faces.\n\nAssume:\n- Messages match only if both sender and receiver agree on the peer ranks and the axis tag ($x$, $y$, or $z$). A direction $+\\hat{x}$ or $-\\hat{x}$ maps to the axis tag $x$, similarly for $y$ and $z$.\n- A nonblocking send does not complete until the matching receive is posted; a process calling $\\mathrm{Waitall}$ with unmatched operations remains waiting.\n\nYour program must simulate the following test suite and output, for each case, a boolean indicating whether the global execution is deadlock-free (i.e., all processes complete their $\\mathrm{Waitall}$). The test suite is:\n\n- Case $1$: $(N_x,N_y,N_z) = (2,2,2)$, all processes use \"recv_then_send\".\n- Case $2$: $(N_x,N_y,N_z) = (2,1,1)$, all processes use \"send_then_wait_no_recv\".\n- Case $3$: $(N_x,N_y,N_z) = (2,2,1)$, all processes use \"recv_then_send\" except the process at $(0,0,0)$ which uses \"recv_then_send_tag_mismatch_X\".\n- Case $4$: $(N_x,N_y,N_z) = (1,1,1)$, all processes use \"recv_then_send\".\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[result_1,result_2,result_3,result_4]$), where each $result_i$ is the boolean $\\mathrm{True}$ if the case is deadlock-free and $\\mathrm{False}$ otherwise.\n\nNo physical units are involved. Angles are not used. Percentages are not used.",
            "solution": "The problem asks for a first-principles derivation of a deadlock-free halo exchange algorithm using nonblocking Message Passing Interface (MPI) operations, an analysis of deadlock conditions, and the implementation of a simulator to verify the behavior of different communication policies.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Domain:** A three-dimensional computational domain for a passive tracer transport equation.\n- **Discretization:** Finite-volume method.\n- **Decomposition:** The domain is decomposed into a Cartesian grid of subdomains.\n- **Parallelism:** One MPI process per subdomain, with processes arranged in a grid of size $(N_x, N_y, N_z)$.\n- **Process Coordinates:** Each process is identified by coordinates $(i, j, k)$ where $0 \\le i < N_x$, $0 \\le j < N_y$, and $0 \\le k < N_z$.\n- **Communication:** Processes communicate with up to $6$ face-adjacent neighbors ($\\pm\\hat{x}, \\pm\\hat{y}, \\pm\\hat{z}$) to exchange halo data.\n- **Halo Thickness:** $1$ cell.\n- **MPI Operations:** Nonblocking operations $\\mathrm{Irecv}$, $\\mathrm{Isend}$, and completion check with $\\mathrm{Waitall}$.\n- **Overlap:** Interior computations (not requiring halo data) can be overlapped with communication.\n- **MPI Matching Rule:** A send and receive match based on communicator, source/destination rank, and message tag.\n- **Tagging Convention:** Communications on the $x$-axis (in either $+\\hat{x}$ or $-\\hat{x}$ direction) use tag $x$. Similarly for $y$ and $z$ axes.\n- **Scheduling Policies:**\n    1.  `\"recv_then_send\"`: Post all $\\mathrm{Irecv}$s, then all $\\mathrm{Isend}$s, then compute, then $\\mathrm{Waitall}$.\n    2.  `\"send_then_wait_no_recv\"`: Post all $\\mathrm{Isend}$s, then $\\mathrm{Waitall}$, without posting any $\\mathrm{Irecv}$s.\n    3.  `\"recv_then_send_tag_mismatch_X\"`: Like `\"recv_then_send\"`, but uses an incorrect tag for sends on the $x$-faces.\n- **Simulation Goal:** For a given test suite, determine if the global execution is deadlock-free (i.e., all processes complete their $\\mathrm{Waitall}$).\n- **Test Suite:**\n    1.  $(N_x, N_y, N_z) = (2, 2, 2)$, policy: `\"recv_then_send\"`.\n    2.  $(N_x, N_y, N_z) = (2, 1, 1)$, policy: `\"send_then_wait_no_recv\"`.\n    3.  $(N_x, N_y, N_z) = (2, 2, 1)$, policy `\"recv_then_send\"` for all, except `\"recv_then_send_tag_mismatch_X\"` for process $(0, 0, 0)$.\n    4.  $(N_x, N_y, N_z) = (1, 1, 1)$, policy: `\"recv_then_send\"`.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem statement is scientifically and algorithmically sound.\n-   **Scientifically Grounded:** The scenario described—domain decomposition, halo exchange for finite-volume/difference methods on a Cartesian grid—is a canonical and fundamental problem in high-performance scientific computing. It is a standard model for analyzing the performance of parallel numerical solvers for partial differential equations.\n-   **Well-Posed:** The problem provides a clear objective (simulate and detect deadlocks), well-defined initial conditions (grid sizes and policies), and precise rules for process interaction and communication matching. This establishes a framework where a unique and meaningful outcome (deadlock or no deadlock) exists for each test case.\n-   **Objective:** The language is formal and unambiguous. The policies and matching rules are defined with sufficient precision for a deterministic simulation.\n-   **Completeness:** The problem is self-contained. All necessary information to build the simulator and evaluate the test cases is provided. The abstraction of MPI logic to tags and ranks is appropriate and sufficient.\n-   **No Flaws:** The problem does not violate any scientific principles, is not metaphorical or trivial, and is completely formalizable. The constraints are realistic within the context of MPI programming models.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n### Derivation of a Deadlock-Free Halo Exchange Algorithm\n\n**1. Formalizing the System**\nLet the set of processes be $P = \\{ p_{i,j,k} \\mid 0 \\le i < N_x, 0 \\le j < N_y, 0 \\le k < N_z \\}$. Each process $p_{i,j,k}$ has a unique rank, which can be computed via a mapping like $r(i,j,k) = i + j \\cdot N_x + k \\cdot N_x \\cdot N_y$.\n\nA process $p_{i,j,k}$ has up to $6$ neighbors. For example, its neighbor in the $+\\hat{x}$ direction is $p_{i+1,j,k}$, provided $i+1 < N_x$. Communication with this neighbor occurs along the $x$-axis. We assign unique integer tags to each axis, for instance, tag $0$ for the $x$-axis, tag $1$ for the $y$-axis, and tag $2$ for the $z$-axis.\n\nTo update cell values at the boundary of a subdomain, a process needs data from its neighbors. This is achieved by exchanging \"halo\" or \"ghost\" cells. For a halo of thickness $1$, each process sends a layer of its boundary cells to its neighbor and receives a corresponding layer into its halo region.\n\n**2. The \"Receive-Then-Send\" Strategy**\nThe use of nonblocking operations, $\\mathrm{Irecv}$ and $\\mathrm{Isend}$, is key to performance. They allow the program to initiate a communication operation and continue with other work, such as computation, while the data transfer occurs in the background. The $\\mathrm{Waitall}$ operation is used to ensure all initiated communications are complete before the program proceeds to use the received data.\n\nA robust and widely adopted strategy to perform halo exchange without deadlock is to **post all receives before posting any sends**. The sequence for each process is as follows:\n\n1.  **Post All Receives:** For each neighboring process from which data is expected, post a nonblocking receive operation: $\\mathrm{Irecv}(source\\_rank, tag, ...)$. This informs the MPI runtime library that the process is ready to accept incoming data into a specified buffer.\n2.  **Post All Sends:** For each neighboring process to which data must be sent, post a nonblocking send operation: $\\mathrm{Isend}(destination\\_rank, tag, ...)$. Since the corresponding $\\mathrm{Irecv}$ has already been posted by the neighbor (as all processes follow the same algorithm), the MPI library can immediately start the data transfer.\n3.  **Overlap Computation:** Perform computations on the *interior* of the local subdomain. These are calculations that do not depend on the halo data currently in transit. This step is the source of performance gain, as computation is overlapped with communication.\n4.  **Wait for Completion:** Call $\\mathrm{Waitall}$ on the set of all request handles from the posted $\\mathrm{Irecv}$ and $\\mathrm{Isend}$ operations. This call will block until all sends and receives for the calling process have completed.\n5.  **Boundary Computation:** Once $\\mathrm{Waitall}$ returns, the halo regions are filled with fresh data. The process can now safely perform computations on the boundary cells of its subdomain, which depend on this halo data.\n\nThe reason this strategy is sufficient to avoid deadlock is that it breaks any potential resource-dependent cyclic dependencies. A common cause of deadlock in simpler blocking communication schemes (or even in nonblocking schemes under heavy load) is when two processes both try to send to each other simultaneously. If system buffers are finite, both sends might block, waiting for the other process to post a receive, which it never will because it's also blocked on a send. By posting all receives first, every process signals its readiness to accept data, ensuring that a subsequent send will always find a matching receive already posted, thus guaranteeing that the communication can progress.\n\n**3. Conditions for Deadlock**\nDeadlocks arise when a cyclic dependency forms in which a group of processes are all waiting for an event that can only be triggered by another process within the same group. In the context of MPI halo exchange, this can happen primarily in two ways:\n\n*   **Incorrect Operation Ordering (Cyclic Wait):** Consider a policy where processes post sends and then wait for them to complete before posting receives. For example, process $p_A$ executes $\\mathrm{Isend}(p_B); \\mathrm{Wait}(...)$ and process $p_B$ executes $\\mathrm{Isend}(p_A); \\mathrm{Wait}(...)$. The `Isend` from $p_A$ to $p_B$ will not complete until $p_B$ posts a matching `Irecv`. However, $p_B$ is blocked in its own `Wait` call, waiting for its send to $p_A$ to complete, and will never reach the point of posting a receive. $p_A$ is in a symmetric situation. This creates a deadlock. The `\"send_then_wait_no_recv\"` policy is a perfect example of this flaw; since no receives are ever posted by any process, no sends can ever complete, and all processes will wait indefinitely.\n\n*   **Parameter Mismatch:** MPI requires a precise match between the parameters of a send and a receive operation for them to be paired. A mismatch in the source rank, destination rank, or message tag will cause the operations to be ignored by the matching engine. For instance, suppose process $p_A$ wants to communicate with $p_B$ along the $x$-axis (tag $0$).\n    -   $p_A$ posts $\\mathrm{Irecv}(source=p_B, tag=0)$.\n    -   $p_B$ posts $\\mathrm{Isend}(dest=p_A, tag=99)$.\n    The tags do not match. The send from $p_B$ and the receive from $p_A$ are for the same pair of processes but are considered unrelated communications. If both processes subsequently call $\\mathrm{Waitall}$, they will block forever. $p_A$ waits for a message from $p_B$ with tag $0$ that will never be sent, and $p_B$ waits for its sent message with tag $99$ to be received, but $p_A$ is not expecting a message with that tag. This is simulated by the `\"recv_then_send_tag_mismatch_X\"` policy.\n\n**4. Formalism for Simulation**\nThe simulator can model this system by tracking the state of posted operations.\n-   **Processes and Ranks:** A grid of $N_x \\times N_y \\times N_z$ processes is created. Each process $(i,j,k)$ is assigned a unique rank $r$.\n-   **Operations:** Communication operations are represented as objects or tuples holding their essential matching information: `(type, rank, peer_rank, tag)`, where `type` is `SEND` or `RECV`.\n-   **Execution Model:**\n    1.  **Operation Posting:** According to the specified policy for each process, all intended operations (`Irecv` and `Isend`) are generated and placed into two global lists: `pending_sends` and `pending_recvs`. The ordering of policies (e.g., all `Irecv`s are posted before any `Isend`s) is respected.\n    2.  **Matching Engine:** The simulator repeatedly scans the `pending_sends` and `pending_recvs` lists. If it finds a send operation `(SEND, r1, r2, tag)` and a receive operation `(RECV, r2, r1, tag)`, this constitutes a match. The two matched operations are removed from their respective lists. This process continues until no more matches can be found in a full pass over the lists.\n    3.  **Deadlock Detection:** After the matching engine terminates, the system is in a stable state. If both the `pending_sends` and `pending_recvs` lists are empty, it means every posted send was matched with a corresponding receive. All processes can therefore complete their `Waitall` call successfully. The system is deadlock-free. If either list is non-empty, it means there are outstanding communication operations that will never be matched. The processes that posted these operations will block indefinitely in `Waitall`, and the system is in a state of deadlock.",
            "answer": "```python\nimport numpy as np\nfrom collections import namedtuple\n\n# Define a simple structure for communication operations\nOperation = namedtuple('Operation', ['type', 'rank', 'peer', 'tag'])\n\ndef get_neighbors(coords, dims):\n    \"\"\"Finds face-adjacent neighbors for a process.\"\"\"\n    i, j, k = coords\n    Nx, Ny, Nz = dims\n    neighbors = []\n    # Directions: +x, -x, +y, -y, +z, -z\n    # Corresponding axes/tags: 0, 0, 1, 1, 2, 2\n    directions = [(1, 0, 0), (-1, 0, 0), (0, 1, 0), (0, -1, 0), (0, 0, 1), (0, 0, -1)]\n    tags = [0, 0, 1, 1, 2, 2]\n\n    for idx, (di, dj, dk) in enumerate(directions):\n        ni, nj, nk = i + di, j + dj, k + dk\n        if 0 <= ni < Nx and 0 <= nj < Ny and 0 <= nk < Nz:\n            neighbors.append({\n                'coords': (ni, nj, nk),\n                'tag': tags[idx]\n            })\n    return neighbors\n\ndef simulate_halo_exchange(dims, policies):\n    \"\"\"\n    Simulates the MPI halo exchange and detects deadlocks.\n\n    Args:\n        dims (tuple): Grid dimensions (Nx, Ny, Nz).\n        policies (dict): A mapping from process coordinates to policy strings.\n\n    Returns:\n        bool: True if the exchange is deadlock-free, False otherwise.\n    \"\"\"\n    Nx, Ny, Nz = dims\n    num_procs = Nx * Ny * Nz\n    if num_procs == 0:\n        return True\n\n    # Map coordinates to a unique rank and back\n    coord_to_rank = {\n        (i, j, k): i + j * Nx + k * Nx * Ny\n        for k in range(Nz) for j in range(Ny) for i in range(Nx)\n    }\n\n    # Global pools of posted operations\n    pending_recvs = []\n    pending_sends = []\n\n    # --- Step 1: Post Receives ---\n    # According to the \"recv_then_send\" logic, post all receives first.\n    for k in range(Nz):\n        for j in range(Ny):\n            for i in range(Nx):\n                coords = (i, j, k)\n                rank = coord_to_rank[coords]\n                policy = policies[coords]\n\n                if policy in [\"recv_then_send\", \"recv_then_send_tag_mismatch_X\"]:\n                    neighbors = get_neighbors(coords, dims)\n                    for neighbor in neighbors:\n                        peer_coords = neighbor['coords']\n                        peer_rank = coord_to_rank[peer_coords]\n                        tag = neighbor['tag']\n                        pending_recvs.append(Operation('RECV', rank, peer_rank, tag))\n\n    # --- Step 2: Post Sends ---\n    # Now, all processes post their sends.\n    for k in range(Nz):\n        for j in range(Ny):\n            for i in range(Nx):\n                coords = (i, j, k)\n                rank = coord_to_rank[coords]\n                policy = policies[coords]\n\n                if policy in [\"recv_then_send\", \"send_then_wait_no_recv\", \"recv_then_send_tag_mismatch_X\"]:\n                    neighbors = get_neighbors(coords, dims)\n                    for neighbor in neighbors:\n                        peer_coords = neighbor['coords']\n                        peer_rank = coord_to_rank[peer_coords]\n                        tag = neighbor['tag']\n                        \n                        # Apply tag mismatch if the policy requires it\n                        if policy == \"recv_then_send_tag_mismatch_X\" and tag == 0: # tag 0 is x-axis\n                            tag = 99 # Mismatched tag\n\n                        pending_sends.append(Operation('SEND', rank, peer_rank, tag))\n\n    # --- Step 3: Matching Engine ---\n    # Repeatedly scan and match sends and receives until no more matches can be found.\n    while True:\n        matched_in_pass = False\n        \n        # To allow removing items while iterating, we iterate over a copy of one list\n        # and search the other.\n        sends_to_remove = set()\n        recvs_to_remove = set()\n        \n        # Use indices to handle list removal properly\n        recv_indices = list(range(len(pending_recvs)))\n        recv_matched = [False] * len(pending_recvs)\n\n        for i, s_op in enumerate(pending_sends):\n            for j_idx, r_op in enumerate(pending_recvs):\n                # Check if recv is already matched in this pass\n                if recv_matched[j_idx]:\n                    continue\n                \n                # Check for a match: sender->peer == receiver and receiver->peer == sender and tags match\n                if (s_op.peer == r_op.rank and s_op.rank == r_op.peer and s_op.tag == r_op.tag):\n                    sends_to_remove.add(i)\n                    recvs_to_remove.add(j_idx)\n                    recv_matched[j_idx] = True\n                    matched_in_pass = True\n                    break # A send can only match one receive\n        \n        if not matched_in_pass:\n            break\n\n        # Remove matched items from the lists in reverse index order to avoid shifting issues\n        for i in sorted(list(sends_to_remove), reverse=True):\n            del pending_sends[i]\n        for j in sorted(list(recvs_to_remove), reverse=True):\n            del pending_recvs[j]\n\n    # --- Step 4: Deadlock Detection ---\n    # If any operations remain unmatched, it's a deadlock.\n    # All policies imply a Waitall, so any pending op causes a block.\n    return len(pending_sends) == 0 and len(pending_recvs) == 0\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for the specified test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1: (2,2,2), all processes use \"recv_then_send\".\n        {\n            \"dims\": (2, 2, 2),\n            \"default_policy\": \"recv_then_send\",\n            \"exceptions\": {}\n        },\n        # Case 2: (2,1,1), all processes use \"send_then_wait_no_recv\".\n        {\n            \"dims\": (2, 1, 1),\n            \"default_policy\": \"send_then_wait_no_recv\",\n            \"exceptions\": {}\n        },\n        # Case 3: (2,2,1), all processes use \"recv_then_send\" except (0,0,0).\n        {\n            \"dims\": (2, 2, 1),\n            \"default_policy\": \"recv_then_send\",\n            \"exceptions\": { (0, 0, 0): \"recv_then_send_tag_mismatch_X\" }\n        },\n        # Case 4: (1,1,1), all processes use \"recv_then_send\".\n        {\n            \"dims\": (1, 1, 1),\n            \"default_policy\": \"recv_then_send\",\n            \"exceptions\": {}\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        dims = case[\"dims\"]\n        Nx, Ny, Nz = dims\n        \n        # Build the policy map for the current case\n        policies = {}\n        for k in range(Nz):\n            for j in range(Ny):\n                for i in range(Nx):\n                    coords = (i, j, k)\n                    if coords in case[\"exceptions\"]:\n                        policies[coords] = case[\"exceptions\"][coords]\n                    else:\n                        policies[coords] = case[\"default_policy\"]\n\n        is_deadlock_free = simulate_halo_exchange(dims, policies)\n        results.append(is_deadlock_free)\n\n    # Format the final output string\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The ultimate goal of performance modeling and efficient parallel programming is to make optimal choices for real-world problems. In computational oceanography, workloads are rarely uniform, with factors like bathymetry creating significant load imbalance. This final exercise  synthesizes the concepts of computational cost and communication overhead into a practical optimization problem. By creating and using a workload estimator, you will determine the optimal domain partition that minimizes the total time-to-solution, demonstrating how these foundational skills directly inform high-level strategy in scientific computing.",
            "id": "3806433",
            "problem": "Consider a baroclinic time step in a hydrostatic primitive-equation ocean model discretized on a structured horizontal grid with a single row and $I=8$ columns indexed by $i \\in \\{1,2,\\ldots,8\\}$. The vertical column depths (number of active $z$-levels) are given by\n$$\nN_{z}(i) \\in \\{12,14,18,22,28,34,38,40\\} \\quad \\text{for} \\quad i=1,2,\\ldots,8,\n$$\nordered from shallow nearshore ($i=1$) to deep offshore ($i=8$). The number of active tracers is $N_{t}=6$.\n\nIn a High-Performance Computing (HPC) setting with one-dimensional domain decomposition into contiguous slabs along $i$, assume the following modeling assumptions grounded in first principles of operation counting and latency-hiding for explicit baroclinic updates:\n\n- The per-column computational work for tracer advection–diffusion scales linearly with the number of vertical levels and the number of tracers, contributing $C_{\\text{tr}}(i)=\\alpha\\,N_{z}(i)\\,N_{t}$ work units.\n- The baroclinic pressure-gradient and related baroclinic work scales linearly with the number of vertical levels, contributing $C_{\\text{pg}}(i)=\\beta\\,N_{z}(i)$ work units.\n- Lateral mixing and equation-of-state evaluations add a per-column cost proportional to the number of tracers, contributing $C_{\\text{lat}}(i)=\\delta\\,N_{t}$ work units.\n- A vertical mixing parameterization is triggered in shallow water, with additional cost $C_{\\text{vmix}}(i)=\\gamma\\,(N_{\\text{crit}}-N_{z}(i))\\,N_{t}$ if $N_{z}(i)<N_{\\text{crit}}$, and $C_{\\text{vmix}}(i)=0$ otherwise.\n\nTake the constants to be $\\alpha=4$, $\\beta=2$, $\\delta=5$, $\\gamma=3$, and $N_{\\text{crit}}=20$. The total per-column work estimator is therefore\n$$\nw(i) \\equiv C_{\\text{tr}}(i)+C_{\\text{pg}}(i)+C_{\\text{lat}}(i)+C_{\\text{vmix}}(i).\n$$\n\nNow, decompose the domain into $2$ contiguous subdomains by placing a single split between columns $k$ and $k+1$, where $k \\in \\{1,2,\\ldots,7\\}$. Under the maximum-subdomain-time model for the makespan of a synchronized explicit step, and assuming one nearest-neighbor halo exchange across the internal interface per baroclinic step, model the additional per-step communication cost as\n$$\nC_{\\text{halo}}(k)=\\epsilon\\,N_{t}\\,\\overline{N_{z}}(k), \\quad \\text{with} \\quad \\overline{N_{z}}(k)=\\frac{N_{z}(k)+N_{z}(k+1)}{2},\n$$\nand $\\epsilon=1$. The predicted step time for a split at $k$ is then\n$$\nT(k)=\\max\\left(\\sum_{i=1}^{k}w(i),\\ \\sum_{i=k+1}^{8}w(i)\\right)+C_{\\text{halo}}(k).\n$$\n\nUsing only the above foundational modeling assumptions and the provided data, determine the split index $k$ that minimizes $T(k)$. Report the optimal $k$ as an integer. No rounding is required, and no physical units are to be included in the final answer.",
            "solution": "The problem will first be validated against the specified criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Grid Dimensions**: A single row with $I=8$ columns, indexed by $i \\in \\{1, 2, \\ldots, 8\\}$.\n- **Vertical Levels**: The number of vertical levels for each column is given by the ordered set $N_{z}(i) \\in \\{12, 14, 18, 22, 28, 34, 38, 40\\}$ for $i=1, 2, \\ldots, 8$.\n- **Number of Tracers**: $N_t = 6$.\n- **Cost Model Components**:\n  - Tracer advection-diffusion cost: $C_{\\text{tr}}(i)=\\alpha\\,N_{z}(i)\\,N_{t}$\n  - Baroclinic pressure-gradient cost: $C_{\\text{pg}}(i)=\\beta\\,N_{z}(i)$\n  - Lateral physics cost: $C_{\\text{lat}}(i)=\\delta\\,N_{t}$\n  - Vertical mixing cost: $C_{\\text{vmix}}(i)=\\gamma\\,(N_{\\text{crit}}-N_{z}(i))\\,N_{t}$ if $N_{z}(i)<N_{\\text{crit}}$, and $C_{\\text{vmix}}(i)=0$ otherwise.\n- **Cost Model Constants**: $\\alpha=4$, $\\beta=2$, $\\delta=5$, $\\gamma=3$, $N_{\\text{crit}}=20$.\n- **Total Per-Column Work**: $w(i) \\equiv C_{\\text{tr}}(i)+C_{\\text{pg}}(i)+C_{\\text{lat}}(i)+C_{\\text{vmix}}(i)$.\n- **Domain Decomposition**: The domain is split into $2$ contiguous subdomains between columns $k$ and $k+1$, where $k \\in \\{1, 2, \\ldots, 7\\}$.\n- **Communication Cost**: $C_{\\text{halo}}(k)=\\epsilon\\,N_{t}\\,\\overline{N_{z}}(k)$, with $\\overline{N_{z}}(k)=\\frac{N_{z}(k)+N_{z}(k+1)}{2}$ and $\\epsilon=1$.\n- **Predicted Step Time (Objective Function)**: $T(k)=\\max\\left(\\sum_{i=1}^{k}w(i),\\ \\sum_{i=k+1}^{8}w(i)\\right)+C_{\\text{halo}}(k)$.\n- **Objective**: Find the integer index $k$ that minimizes $T(k)$.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded**: The problem is a simplified but standard exercise in performance modeling for parallel scientific computing. The concepts of domain decomposition, load balancing, communication overhead (halo exchange), and component-wise cost estimation are fundamental principles in High-Performance Computing, particularly for models like those used in computational oceanography. The linear scaling assumptions are common first-order approximations. The problem is scientifically sound within its specified domain.\n- **Well-Posed**: The problem asks to find the minimum of a function $T(k)$ over a finite, discrete domain of integers $k \\in \\{1, 2, \\ldots, 7\\}$. Since the domain is finite, a minimum value is guaranteed to exist. All functions and parameters are explicitly defined, leading to a unique and computable value for $T(k)$ for each $k$.\n- **Objective**: The problem is stated using precise mathematical definitions and numerical values. There are no subjective or ambiguous terms.\n- **Completeness and Consistency**: The problem is self-contained. All necessary variables, constants, and formulas are provided. There are no apparent contradictions in the problem setup. The range for the split index $k$ is consistent with partitioning $8$ columns into two non-empty sets.\n- **No Other Flaws**: The problem is not unrealistic (within the context of a modeling exercise), ill-posed, trivial, or unverifiable. It is a straightforward optimization problem based on a clearly defined cost model.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. A complete solution will be provided.\n\n### Solution\n\nThe goal is to find the integer $k \\in \\{1, 2, \\ldots, 7\\}$ that minimizes the predicted step time $T(k)$. This requires a systematic evaluation of $T(k)$ for each possible value of $k$.\n\nFirst, we calculate the per-column computational work, $w(i)$, for each column $i=1, \\ldots, 8$. The formula for $w(i)$ is:\n$$\nw(i) = C_{\\text{tr}}(i)+C_{\\text{pg}}(i)+C_{\\text{lat}}(i)+C_{\\text{vmix}}(i)\n$$\nSubstituting the given expressions for each component:\n$$\nw(i) = \\alpha N_{z}(i) N_{t} + \\beta N_{z}(i) + \\delta N_{t} + C_{\\text{vmix}}(i)\n$$\nPlugging in the constant values $\\alpha=4$, $\\beta=2$, $\\delta=5$, and $N_t=6$:\n$$\nw(i) = (4) N_{z}(i) (6) + (2) N_{z}(i) + (5) (6) + C_{\\text{vmix}}(i) = 24 N_{z}(i) + 2 N_{z}(i) + 30 + C_{\\text{vmix}}(i) = 26 N_{z}(i) + 30 + C_{\\text{vmix}}(i)\n$$\nThe vertical mixing cost $C_{\\text{vmix}}(i)$ depends on whether $N_{z}(i) < N_{\\text{crit}}$. With $N_{\\text{crit}}=20$, $\\gamma=3$, and $N_t=6$:\n$$\nC_{\\text{vmix}}(i) = \\begin{cases} (3)(20 - N_{z}(i))(6) = 18(20 - N_{z}(i)) & \\text{if } N_{z}(i) < 20 \\\\ 0 & \\text{if } N_{z}(i) \\ge 20 \\end{cases}\n$$\nThis gives two forms for $w(i)$:\n1. If $N_{z}(i) < 20$: $w(i) = 26 N_{z}(i) + 30 + 18(20 - N_{z}(i)) = 26 N_{z}(i) + 30 + 360 - 18 N_{z}(i) = 8 N_{z}(i) + 390$.\n2. If $N_{z}(i) \\ge 20$: $w(i) = 26 N_{z}(i) + 30 + 0 = 26 N_{z}(i) + 30$.\n\nThe given vertical levels are $N_{z}(1)=12$, $N_{z}(2)=14$, $N_{z}(3)=18$, $N_{z}(4)=22$, $N_{z}(5)=28$, $N_{z}(6)=34$, $N_{z}(7)=38$, $N_{z}(8)=40$. We can now compute the work $w(i)$ for each column:\n- $w(1)$: $N_z(1) = 12 < 20 \\implies w(1) = 8(12) + 390 = 96 + 390 = 486$.\n- $w(2)$: $N_z(2) = 14 < 20 \\implies w(2) = 8(14) + 390 = 112 + 390 = 502$.\n- $w(3)$: $N_z(3) = 18 < 20 \\implies w(3) = 8(18) + 390 = 144 + 390 = 534$.\n- $w(4)$: $N_z(4) = 22 \\ge 20 \\implies w(4) = 26(22) + 30 = 572 + 30 = 602$.\n- $w(5)$: $N_z(5) = 28 \\ge 20 \\implies w(5) = 26(28) + 30 = 728 + 30 = 758$.\n- $w(6)$: $N_z(6) = 34 \\ge 20 \\implies w(6) = 26(34) + 30 = 884 + 30 = 914$.\n- $w(7)$: $N_z(7) = 38 \\ge 20 \\implies w(7) = 26(38) + 30 = 988 + 30 = 1018$.\n- $w(8)$: $N_z(8) = 40 \\ge 20 \\implies w(8) = 26(40) + 30 = 1040 + 30 = 1070$.\n\nNext, we evaluate $T(k)$ for each possible split $k \\in \\{1, \\ldots, 7\\}$. The formula is $T(k)=\\max(W_1(k), W_2(k))+C_{\\text{halo}}(k)$, where $W_1(k) = \\sum_{i=1}^{k}w(i)$ and $W_2(k) = \\sum_{i=k+1}^{8}w(i)$. The total work is $\\sum_{i=1}^{8}w(i) = 486+502+534+602+758+914+1018+1070 = 5884$.\nThe communication cost is $C_{\\text{halo}}(k) = \\epsilon N_{t} \\frac{N_{z}(k)+N_{z}(k+1)}{2} = (1)(6)\\frac{N_{z}(k)+N_{z}(k+1)}{2} = 3(N_{z}(k)+N_{z}(k+1))$.\n\nLet's compute $T(k)$ for each $k$:\n\n- For $k=1$:\n  - $W_1(1) = w(1) = 486$.\n  - $W_2(1) = 5884 - 486 = 5398$.\n  - $C_{\\text{halo}}(1) = 3(N_z(1)+N_z(2)) = 3(12+14) = 3(26) = 78$.\n  - $T(1) = \\max(486, 5398) + 78 = 5398 + 78 = 5476$.\n\n- For $k=2$:\n  - $W_1(2) = w(1)+w(2) = 486+502 = 988$.\n  - $W_2(2) = 5884 - 988 = 4896$.\n  - $C_{\\text{halo}}(2) = 3(N_z(2)+N_z(3)) = 3(14+18) = 3(32) = 96$.\n  - $T(2) = \\max(988, 4896) + 96 = 4896 + 96 = 4992$.\n\n- For $k=3$:\n  - $W_1(3) = 988+w(3) = 988+534 = 1522$.\n  - $W_2(3) = 5884 - 1522 = 4362$.\n  - $C_{\\text{halo}}(3) = 3(N_z(3)+N_z(4)) = 3(18+22) = 3(40) = 120$.\n  - $T(3) = \\max(1522, 4362) + 120 = 4362 + 120 = 4482$.\n\n- For $k=4$:\n  - $W_1(4) = 1522+w(4) = 1522+602 = 2124$.\n  - $W_2(4) = 5884 - 2124 = 3760$.\n  - $C_{\\text{halo}}(4) = 3(N_z(4)+N_z(5)) = 3(22+28) = 3(50) = 150$.\n  - $T(4) = \\max(2124, 3760) + 150 = 3760 + 150 = 3910$.\n\n- For $k=5$:\n  - $W_1(5) = 2124+w(5) = 2124+758 = 2882$.\n  - $W_2(5) = 5884 - 2882 = 3002$.\n  - $C_{\\text{halo}}(5) = 3(N_z(5)+N_z(6)) = 3(28+34) = 3(62) = 186$.\n  - $T(5) = \\max(2882, 3002) + 186 = 3002 + 186 = 3188$.\n\n- For $k=6$:\n  - $W_1(6) = 2882+w(6) = 2882+914 = 3796$.\n  - $W_2(6) = 5884 - 3796 = 2088$.\n  - $C_{\\text{halo}}(6) = 3(N_z(6)+N_z(7)) = 3(34+38) = 3(72) = 216$.\n  - $T(6) = \\max(3796, 2088) + 216 = 3796 + 216 = 4012$.\n\n- For $k=7$:\n  - $W_1(7) = 3796+w(7) = 3796+1018 = 4814$.\n  - $W_2(7) = w(8) = 1070$.\n  - $C_{\\text{halo}}(7) = 3(N_z(7)+N_z(8)) = 3(38+40) = 3(78) = 234$.\n  - $T(7) = \\max(4814, 1070) + 234 = 4814 + 234 = 5048$.\n\nFinally, we compare the values of $T(k)$ to find the minimum:\n$T(1) = 5476$\n$T(2) = 4992$\n$T(3) = 4482$\n$T(4) = 3910$\n$T(5) = 3188$\n$T(6) = 4012$\n$T(7) = 5048$\n\nThe minimum value is $3188$, which occurs for the split index $k=5$.",
            "answer": "$$\\boxed{5}$$"
        }
    ]
}