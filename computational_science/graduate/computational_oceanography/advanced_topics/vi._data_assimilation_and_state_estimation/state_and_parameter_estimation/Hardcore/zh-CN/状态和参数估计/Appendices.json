{
    "hands_on_practices": [
        {
            "introduction": "四维变分（`4D-Var`）资料同化的核心在于最小化一个代价函数，而这需要精确计算该函数相对于控制变量（例如初始状态和模型参数）的梯度。本练习将指导您完成一个关键的实践步骤：为一个简化的海洋动力学模型推导并实现其伴随模型，用以计算梯度。通过梯度检验（将伴随模型的计算结果与有限差分近似进行比较）来验证其正确性，这是开发任何变分同化系统时确保代码可靠性的黄金标准。",
            "id": "3811253",
            "problem": "考虑一个简化的二维线性模型，用于描述受线性阻尼和科里奥利力影响的水平洋流。令状态为 $x_k \\in \\mathbb{R}^2$，其分量 $x_k = [u_k, v_k]^\\top$ 代表在离散时间索引 $k \\in \\{0,1,\\dots,K\\}$ 时的纬向和经向速度。离散时间动力学通过对一个线性系统进行前向欧拉离散化得到，其时间步长为 $\\Delta t$，参数为 $\\theta = [\\alpha, f]^\\top$，其中 $\\alpha$ 是线性阻尼率，$f$ 是科里奥利参数。动力学方程为\n$$\nx_{k+1} = M(\\theta)\\, x_k, \\quad M(\\theta) = I + \\Delta t\\, F(\\theta), \\quad F(\\theta) = \\begin{bmatrix} -\\alpha & f \\\\ -f & -\\alpha \\end{bmatrix},\n$$\n对所有 $k \\in \\{0,1,\\dots,K-1\\}$ 成立，其中 $I$ 是大小为 2 的单位矩阵。\n\n观测是在每个时间 $k$ 对纬向速度 $u_k$ 的标量测量，其观测算子为 $H = [1,0]$。令 $y_k \\in \\mathbb{R}$ 表示在时间 $k$ 的观测值。考虑定义如下的四维变分 (4D-Var) 数据同化代价函数\n$$\nJ(x_0,\\theta) = \\tfrac{1}{2}(x_0 - x_b)^\\top B^{-1} (x_0 - x_b) + \\tfrac{1}{2}\\sum_{k=0}^K \\left(H x_k - y_k\\right)^\\top R^{-1} \\left(H x_k - y_k\\right) + \\tfrac{1}{2}(\\theta - \\theta_b)^\\top P^{-1} (\\theta - \\theta_b),\n$$\n其中 $x_b \\in \\mathbb{R}^2$ 和 $\\theta_b \\in \\mathbb{R}^2$ 是先验均值，$B \\in \\mathbb{R}^{2\\times 2}$ 和 $P \\in \\mathbb{R}^{2\\times 2}$ 是先验协方差，$R \\in \\mathbb{R}$ 是观测误差方差（因此 $R^{-1}$ 是一个标量）。状态轨迹 $\\{x_k\\}_{k=0}^K$ 受上述动力学方程的约束。\n\n你的任务是构建并实现一个梯度检验，该检验将 $J$ 的有限差分方向导数与伴随推导的方向导数进行比较。梯度检验必须在方向 $(p_{x_0}, p_{\\theta})$ 上使用中心有限差分近似，步长为 $h>0$，\n$$\nD_{\\mathrm{FD}} J(x_0,\\theta; p_{x_0}, p_{\\theta}, h) = \\frac{J(x_0 + h\\, p_{x_0},\\ \\theta + h\\, p_{\\theta}) - J(x_0 - h\\, p_{x_0},\\ \\theta - h\\, p_{\\theta})}{2h},\n$$\n并且必须将其与伴随推导的方向导数进行比较\n$$\nD_{\\mathrm{ADJ}} J(x_0,\\theta; p_{x_0}, p_{\\theta}) = \\nabla_{x_0} J(x_0,\\theta)^\\top p_{x_0} + \\nabla_{\\theta} J(x_0,\\theta)^\\top p_{\\theta},\n$$\n其中 $\\nabla_{x_0} J$ 和 $\\nabla_{\\theta} J$ 分别是 $J$ 关于 $x_0$ 和 $\\theta$ 的梯度，通过线性模型的离散伴随方法和链式法则获得。你必须计算相对误差\n$$\n\\mathrm{relerr}(h, p_{x_0}, p_{\\theta}) = \\frac{\\left| D_{\\mathrm{FD}} J(x_0,\\theta; p_{x_0}, p_{\\theta}, h) - D_{\\mathrm{ADJ}} J(x_0,\\theta; p_{x_0}, p_{\\theta}) \\right|}{\\max\\left(1,\\ \\left| D_{\\mathrm{FD}} J(x_0,\\theta; p_{x_0}, p_{\\theta}, h) \\right|\\right)},\n$$\n并当 $\\mathrm{relerr} \\leq \\tau$ 时，宣布测试通过，其中 $\\tau$ 为一个预设的容差。容差 $\\tau$ 将在下面精确指定，并统一用于所有测试。\n\n你必须实现一个完整的、可运行的程序，在以下科学一致的测试套件上执行梯度检验。以下所有数学量都必须用使其方程量纲一致的单位来解释；然而，最终要求的输出是无量纲的布尔值，因此不需要单位。\n\n模型设置和数据生成：\n- 时间步长参数：$K = 10$，$\\Delta t = 900\\ \\mathrm{s}$。\n- 用于生成观测的真实参数：$\\alpha^\\star = 2\\times 10^{-6}\\ \\mathrm{s}^{-1}$，$f^\\star = 1\\times 10^{-4}\\ \\mathrm{s}^{-1}$。\n- 用于生成观测的真实初始状态：$x_0^\\star = [0.1,\\ 0.05]^\\top$。\n- 观测：$y_k = H x_k^\\star$，其中 $\\{x_k^\\star\\}$ 是通过用 $(x_0^\\star,\\ \\theta^\\star)$ 积分动力学方程得到的轨迹。\n- 先验均值：$x_b = [0,\\ 0]^\\top$，$\\theta_b = [1\\times 10^{-6},\\ 0.8\\times 10^{-4}]^\\top$。\n- 先验协方差和观测误差方差：\n  - $B = \\mathrm{diag}([\\sigma_{b,u}^2,\\ \\sigma_{b,v}^2])$，其中 $\\sigma_{b,u} = 0.2$，$\\sigma_{b,v} = 0.2$。\n  - $R = \\sigma_R^2$，其中 $\\sigma_R = 0.01$。\n  - $P = \\mathrm{diag}([\\sigma_{\\alpha}^2,\\ \\sigma_{f}^2])$，其中 $\\sigma_{\\alpha} = 1\\times 10^{-6}$，$\\sigma_{f} = 0.2\\times 10^{-4}$。\n- 梯度检验的评估点（非真实值）：$x_0 = [0.05,\\ 0.01]^\\top$，$\\theta = [1.5\\times 10^{-6},\\ 0.9\\times 10^{-4}]^\\top$。\n\n要求的梯度构建：\n- 推导并实现离散伴随方法，以获得在评估点 $(x_0,\\ \\theta)$ 的 $\\nabla_{x_0} J$ 和 $\\nabla_{\\theta} J$。\n\n方向导数比较和容差：\n- 使用上面定义的中心有限差分近似，并与伴随推导的方向导数进行比较。\n- 对每个测试用例使用统一的容差 $\\tau = 1\\times 10^{-6}$ 来判断通过或失败。\n\n测试套件：\n- 仅状态方向测试，使用 $p_{x_0} = [1.0,\\ -0.5]^\\top$，$p_{\\theta} = [0,\\ 0]^\\top$，以及步长 $h \\in \\{1\\times 10^{-16},\\ 1\\times 10^{-5},\\ 1\\times 10^{-1}\\}$。\n- 仅参数方向测试，使用 $p_{x_0} = [0,\\ 0]^\\top$，$p_{\\theta} = [1.0,\\ -1.0]^\\top$，以及步长 $h \\in \\{1\\times 10^{-16},\\ 1\\times 10^{-5},\\ 1\\times 10^{-1}\\}$。\n- 混合方向测试，使用 $p_{x_0} = [0.3,\\ 0.7]^\\top$，$p_{\\theta} = [0.5,\\ 0.2]^\\top$，以及步长 $h \\in \\{1\\times 10^{-5},\\ 1\\times 10^{-1}\\}$。\n\n对于上述八个测试中的每一个，计算 $\\mathrm{relerr}$ 并返回一个布尔值，指示测试是否通过（$\\mathrm{relerr} \\leq \\tau$）。你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，`[{\\tt True},{\\tt False},\\dots]`），其顺序与上面列出的测试完全一致。",
            "solution": "用户提供的问题已经过验证，被认为是一个定义明确、具有科学依据的计算数据同化练习。本解答将为所提供的四维变分 (4D-Var) 代价函数推导并实现一个梯度检验。\n\n任务的核心是计算代价函数 $J(x_0, \\theta)$ 相对于控制变量（即初始状态 $x_0$ 和模型参数 $\\theta$）的梯度。这个梯度是伴随推导的方向导数所必需的。代价函数是\n$$\nJ(x_0,\\theta) = \\underbrace{\\tfrac{1}{2}(x_0 - x_b)^\\top B^{-1} (x_0 - x_b)}_{J_b(x_0)} + \\underbrace{\\tfrac{1}{2}\\sum_{k=0}^K \\left(H x_k - y_k\\right)^\\top R^{-1} \\left(H x_k - y_k\\right)}_{J_o(x_0, \\theta)} + \\underbrace{\\tfrac{1}{2}(\\theta - \\theta_b)^\\top P^{-1} (\\theta - \\theta_b)}_{J_p(\\theta)}.\n$$\n对于 $k>0$，状态 $x_k$ 是控制变量的函数，由模型动力学方程 $x_{k+1} = M(\\theta) x_k$ 隐式定义。$J$ 的梯度是其各分量梯度的总和：$\\nabla J = \\nabla J_b + \\nabla J_o + \\nabla J_p$。\n\n背景项 $J_b$ 和 $J_p$ 的梯度很简单：\n$$\n\\nabla_{x_0} J_b = B^{-1}(x_0 - x_b)\n$$\n$$\n\\nabla_{\\theta} J_p = P^{-1}(\\theta - \\theta_b)\n$$\n观测项 $J_o$ 的梯度由于状态 $\\{x_k\\}$ 对控制变量的递归依赖性而更为复杂。我们采用离散伴随方法，这是拉格朗日乘子技术的一种专门应用，来高效地计算此梯度。\n\n我们定义一个伴随变量序列 $\\{\\lambda_k\\}_{k=0}^K$，它们是 $\\mathbb{R}^2$ 中的向量。伴随模型是通过确保拉格朗日函数（即代价函数加上模型约束）相对于状态变量 $x_k$（对于 $k > 0$）的梯度为零来推导的。这导出了一个方程组，可以通过时间反向积分来求解伴随变量。\n\n计算梯度分量 $\\nabla_{x_0} J_o$ 和 $\\nabla_{\\theta} J_o$ 的步骤如下：\n\n1.  **前向积分**：给定控制空间中的一个点 $(x_0, \\theta)$，从 $k=0$ 到 $K-1$ 积分前向模型 $x_{k+1} = M(\\theta) x_k$，并存储整个状态轨迹 $\\{x_k\\}_{k=0}^K$。矩阵 $M(\\theta)$ 由 $M(\\theta) = I + \\Delta t\\, F(\\theta)$ 给出，其中 $F(\\theta) = \\begin{bmatrix} -\\alpha & f \\\\ -f & -\\alpha \\end{bmatrix}$。\n\n2.  **反向（伴随）积分**：\n    a. 初始化最终的伴随变量 $\\lambda_K$。这对应于代价函数关于最终状态 $x_K$ 的梯度：\n    $$\n    \\lambda_K = \\frac{\\partial J_o}{\\partial x_K} = H^\\top R^{-1} (H x_K - y_K)\n    $$\n    b. 从 $k=K-1$ 向下反向积分到 $k=0$。步骤 $k$ 的伴随变量由步骤 $k+1$ 的伴随变量、前向模型传播矩阵的转置 $M(\\theta)^\\top$ 以及来自时间 $k$ 观测的一个额外强迫项计算得出：\n    $$\n    \\lambda_k = M(\\theta)^\\top \\lambda_{k+1} + H^\\top R^{-1} (H x_k - y_k)\n    $$\n    计算出的最终伴随变量 $\\lambda_0$ 正是观测代价函数关于初始状态的梯度：$\\nabla_{x_0} J_o = \\lambda_0$。\n\n3.  **参数梯度累积**：关于参数 $\\theta$ 的梯度可以在反向积分期间计算。代价函数对参数 $\\theta_j$ 的敏感度由下式给出\n    $$\n    \\nabla_{\\theta_j} J_o = \\sum_{k=0}^{K-1} \\left( \\frac{\\partial x_{k+1}}{\\partial \\theta_j} \\right)^\\top \\lambda_{k+1}\n    $$\n    其中括号中的量是沿前向轨迹计算的。由于 $x_{k+1} = M(\\theta) x_k$，我们有 $\\frac{\\partial x_{k+1}}{\\partial \\theta_j} = \\left(\\frac{\\partial M(\\theta)}{\\partial \\theta_j}\\right) x_k$。所需的矩阵导数为：\n    $$\n    \\frac{\\partial M}{\\partial \\alpha} = \\Delta t \\begin{bmatrix} -1 & 0 \\\\ 0 & -1 \\end{bmatrix} = -\\Delta t I\n    $$\n    $$\n    \\frac{\\partial M}{\\partial f} = \\Delta t \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix}\n    $$\n    在从 $k=K-1, \\dots, 0$ 的反向扫描过程中，我们累积对参数梯度的贡献：\n    $$\n    \\nabla_{\\alpha} J_o \\leftarrow \\nabla_{\\alpha} J_o + \\left( (-\\Delta t I) x_k \\right)^\\top \\lambda_{k+1} = \\nabla_{\\alpha} J_o - \\Delta t \\, x_k^\\top \\lambda_{k+1}\n    $$\n    $$\n    \\nabla_{f} J_o \\leftarrow \\nabla_{f} J_o + \\left( \\left(\\Delta t \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix}\\right) x_k \\right)^\\top \\lambda_{k+1} = \\nabla_{f} J_o + \\Delta t \\, x_k^\\top \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix} \\lambda_{k+1}\n    $$\n\n最后，通过加上来自背景项的贡献来组合得到总梯度：\n$$\n\\nabla_{x_0} J = B^{-1}(x_0 - x_b) + \\lambda_0\n$$\n$$\n\\nabla_{\\theta} J = P^{-1}(\\theta - \\theta_b) + \\nabla_{\\theta} J_o\n$$\n有了这些梯度，就可以计算伴随推导的方向导数 $D_{\\mathrm{ADJ}} J$。然后将其与中心有限差分近似 $D_{\\mathrm{FD}} J$ 进行比较，以计算每个测试用例的相对误差。如果此误差低于容差 $\\tau = 1 \\times 10^{-6}$，则测试通过。\n\n测试套件旨在探究实现的正确性和数值稳定性。步长非常小（$h=1 \\times 10^{-16}$）的测试预计会由于浮点运算中的灾难性抵消而失败。步长较大（$h=1 \\times 10^{-1}$）的测试预计会由于有限差分近似中的巨大截断误差而失败。步长中等（$h=1 \\times 10^{-5}$）的测试预计会通过，从而证明伴随推导梯度的准确性。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and implements a gradient test for a 4D-Var data assimilation problem.\n    \"\"\"\n\n    # --- Model setup and data generation ---\n\n    # Time-stepping parameters\n    K = 10\n    dt = 900.0\n\n    # True parameters and initial state for generating observations\n    alpha_star = 2e-6\n    f_star = 1e-4\n    theta_star = np.array([alpha_star, f_star])\n    x0_star = np.array([0.1, 0.05])\n\n    # Prior means\n    x_b = np.array([0.0, 0.0])\n    theta_b = np.array([1e-6, 0.8e-4])\n\n    # Standard deviations for covariances\n    sigma_b_u = 0.2\n    sigma_b_v = 0.2\n    sigma_R = 0.01\n    sigma_alpha = 1e-6\n    sigma_f = 0.2e-4\n\n    # Inverse covariance matrices and observation error variance\n    B_inv = np.diag([1/sigma_b_u**2, 1/sigma_b_v**2])\n    R_inv = 1/sigma_R**2\n    P_inv = np.diag([1/sigma_alpha**2, 1/sigma_f**2])\n    \n    # Observation operator\n    H = np.array([1.0, 0.0])\n\n    # Generate synthetic observations\n    def get_M(theta):\n        alpha, f = theta\n        F = np.array([[-alpha, f], [-f, -alpha]])\n        return np.identity(2) + dt * F\n\n    M_star = get_M(theta_star)\n    x_star_traj = np.zeros((K + 1, 2))\n    x_star_traj[0] = x0_star\n    for k in range(K):\n        x_star_traj[k + 1] = M_star @ x_star_traj[k]\n\n    ys = np.array([H @ xk for xk in x_star_traj])\n\n    # --- Core Functions ---\n    def cost_function_J(x0, theta):\n        \"\"\"Computes the 4D-Var cost function J(x0, theta).\"\"\"\n        # Trajectory calculation\n        M = get_M(theta)\n        x_traj = np.zeros((K + 1, 2))\n        x_traj[0] = x0\n        for k in range(K):\n            x_traj[k + 1] = M @ x_traj[k]\n\n        # Background term for x0\n        j_b = 0.5 * (x0 - x_b).T @ B_inv @ (x0 - x_b)\n\n        # Observation term\n        j_o = 0.0\n        for k in range(K + 1):\n            mismatch = H @ x_traj[k] - ys[k]\n            j_o += 0.5 * mismatch * R_inv * mismatch\n        \n        # Background term for theta\n        j_p = 0.5 * (theta - theta_b).T @ P_inv @ (theta - theta_b)\n\n        return j_b + j_o + j_p\n\n    def adjoint_gradient(x0, theta):\n        \"\"\"Computes gradients of J w.r.t. x0 and theta using the adjoint method.\"\"\"\n        # 1. Forward model run to get trajectory\n        M = get_M(theta)\n        x_traj = np.zeros((K + 1, 2))\n        x_traj[0] = x0\n        for k in range(K):\n            x_traj[k + 1] = M @ x_traj[k]\n\n        # 2. Backward (adjoint) integration\n        grad_theta_o = np.zeros(2)\n        \n        # Initialize adjoint variable at final time K\n        forcing_K = H.T * R_inv * (H @ x_traj[K] - ys[K])\n        lambda_next = forcing_K # This is lambda_{K+1} for the first backward step, starting with lambda_K\n\n        # Loop from k = K-1 down to 0\n        for k in range(K - 1, -1, -1):\n            # Accumulate parameter gradient contribution using x_k and lambda_{k+1}\n            # Contribution to dJ_o/d_alpha\n            grad_theta_o[0] += (-dt * x_traj[k]) @ lambda_next\n            # Contribution to dJ_o/d_f\n            dMdf_x_k_T = dt * np.array([x_traj[k, 1], -x_traj[k, 0]]) # (dM/df * x_k)^T\n            grad_theta_o[1] += dMdf_x_k_T @ lambda_next\n\n            # Forcing term at time k\n            # Note: The problem formulation sums observations from k=0 to K.\n            # So every time step has a forcing term.\n            forcing_k = H.T * R_inv * (H @ x_traj[k] - ys[k])\n            \n            # Update adjoint state: lambda_k = M^T @ lambda_{k+1} + forcing_k\n            lambda_k = M.T @ lambda_next + forcing_k\n            \n            # Update for next iteration\n            lambda_next = lambda_k\n\n        # After the loop, lambda_next holds lambda_0\n        lambda_0 = lambda_next\n\n        # 3. Assemble full gradients\n        grad_x0 = B_inv @ (x0 - x_b) + lambda_0\n        grad_theta = P_inv @ (theta - theta_b) + grad_theta_o\n\n        return grad_x0, grad_theta\n\n    # --- Gradient Test ---\n    # Evaluation point for the gradient test\n    x0_eval = np.array([0.05, 0.01])\n    theta_eval = np.array([1.5e-6, 0.9e-4])\n\n    # Test suite\n    test_cases = [\n        # State-only direction tests\n        {'p_x0': np.array([1.0, -0.5]), 'p_theta': np.array([0.0, 0.0]), 'h': 1e-16},\n        {'p_x0': np.array([1.0, -0.5]), 'p_theta': np.array([0.0, 0.0]), 'h': 1e-5},\n        {'p_x0': np.array([1.0, -0.5]), 'p_theta': np.array([0.0, 0.0]), 'h': 1e-1},\n        # Parameter-only direction tests\n        {'p_x0': np.array([0.0, 0.0]), 'p_theta': np.array([1.0, -1.0]), 'h': 1e-16},\n        {'p_x0': np.array([0.0, 0.0]), 'p_theta': np.array([1.0, -1.0]), 'h': 1e-5},\n        {'p_x0': np.array([0.0, 0.0]), 'p_theta': np.array([1.0, -1.0]), 'h': 1e-1},\n        # Mixed direction tests\n        {'p_x0': np.array([0.3, 0.7]), 'p_theta': np.array([0.5, 0.2]), 'h': 1e-5},\n        {'p_x0': np.array([0.3, 0.7]), 'p_theta': np.array([0.5, 0.2]), 'h': 1e-1},\n    ]\n\n    tau = 1e-6\n    results = []\n    \n    # Pre-compute adjoint gradient at the evaluation point\n    grad_x0_adj, grad_theta_adj = adjoint_gradient(x0_eval, theta_eval)\n\n    for case in test_cases:\n        p_x0, p_theta, h = case['p_x0'], case['p_theta'], case['h']\n\n        # Adjoint-derived directional derivative\n        D_adj = grad_x0_adj.T @ p_x0 + grad_theta_adj.T @ p_theta\n\n        # Finite-difference directional derivative (central difference)\n        J_plus = cost_function_J(x0_eval + h * p_x0, theta_eval + h * p_theta)\n        J_minus = cost_function_J(x0_eval - h * p_x0, theta_eval - h * p_theta)\n        \n        # Avoid division by zero if h is extremely small\n        if 2*h == 0:\n            D_fd = 0.0\n        else:\n            D_fd = (J_plus - J_minus) / (2 * h)\n\n        # Relative error calculation\n        abs_diff = np.abs(D_fd - D_adj)\n        denominator = np.max([1.0, np.abs(D_fd)])\n        rel_err = abs_diff / denominator\n        \n        # Test pass/fail\n        results.append(rel_err = tau)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在 `4D-Var` 中，一旦我们能够计算代价函数的梯度，下一个挑战就是如何高效地求解相应的最小化问题。本练习聚焦于 `4D-Var` 增量式方法的“内循环”，即求解一个由背景和观测误差协方差定义的大型线性系统。您将通过实施控制变量变换（一种强大的预条件技术）并使用共轭梯度法，来学习如何将理论上的最小化问题转化为一个在数值上稳定且可快速求解的实际算法。",
            "id": "3811214",
            "problem": "考虑一个用于线性化海洋状态估计问题的增量四维变分数据同化（$4$D-Var）的内循环。其目标是确定状态增量 $\\delta x \\in \\mathbb{R}^n$，以最小化一个从线性高斯估计框架导出的二次代价函数。背景误差协方差为 $B_0 \\in \\mathbb{R}^{n \\times n}$（对称正定），观测误差协方差为 $R \\in \\mathbb{R}^{m \\times m}$（对称正定），线性化观测算子为 $H \\in \\mathbb{R}^{m \\times n}$，新息向量（观测空间中的观测值减去背景值）为 $d \\in \\mathbb{R}^m$。在无偏高斯误差和围绕当前轨迹进行线性化的假设下，增量代价函数可以视为背景不匹配项和观测不匹配项之和。\n\n仅从线性高斯估计的变分原理的基本定义出发，内循环的解由代价泛函的驻点给出，从而得到一个关于状态增量的对称正定线性系统。为了构造一个高效的求解器，使用 $B_0$ 的矩阵平方根进行控制变量变换，并采用预处理共轭梯度法求解变换后的系统。请基于 $B_0$ 的平方根来论证预条件子的选择，讨论变换后变量中的对称性、正定性以及条件数效应。\n\n您的任务是实现一个完整的程序，该程序能够：\n- 使用控制变量变换 $\\delta x = B_0^{1/2} v$（其中 $B_0^{1/2}$ 表示 $B_0$ 的对称矩阵平方根），从变分原理推导出内循环的对称正定线性系统。\n- 在控制变量 $v$ 上使用共轭梯度法求解变换后的线性系统，收敛容差为 $\\varepsilon = 10^{-10}$，最大迭代次数为 $200$ 次。\n- 从 $v$ 恢复 $\\delta x$，并使用精确的二次型在原始变量中计算最小化后的代价值。\n- 全程使用双精度算术。\n\n程序必须运行以下测试套件，并为每个案例输出最小化代价值 $J(\\delta x^\\star)$（作为浮点数）。所有输出都是无量纲的。将每个浮点数四舍五入到 $6$ 位小数。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果（例如，$[r_1,r_2,r_3,r_4]$）。\n\n使用以下测试套件，其中每个案例指定了 $B_0$、$H$、$R$ 和 $d$。所有矩阵均为实值，在指定处为对称或对角矩阵，并具有兼容的维度。\n\n案例1（良态背景，完全观测）：\n$$\nB_0^{(1)} = \\begin{bmatrix}\n1.0  0.2  0.0  0.0  0.0 \\\\\n0.2  0.8  0.1  0.0  0.0 \\\\\n0.0  0.1  1.2  0.3  0.0 \\\\\n0.0  0.0  0.3  1.0  0.2 \\\\\n0.0  0.0  0.0  0.2  0.9\n\\end{bmatrix}, \\quad\nH^{(1)} = \\begin{bmatrix}\n1  0  0  0  0 \\\\\n0  1  0  0  0 \\\\\n0  0  1  0  0 \\\\\n0  0  0  1  0 \\\\\n0  0  0  0  1 \\\\\n1  1  0  0  0\n\\end{bmatrix},\n$$\n$$\nR^{(1)} = \\operatorname{diag}(0.04, 0.05, 0.03, 0.04, 0.05, 0.02), \\quad\nd^{(1)} = \\begin{bmatrix} 0.5 \\\\ -0.3 \\\\ 0.2 \\\\ -0.1 \\\\ 0.4 \\\\ 0.0 \\end{bmatrix}.\n$$\n\n案例2（病态背景，部分聚合观测）：\n$$\nB_0^{(2)} = \\begin{bmatrix}\n10.0  0.0  0.0  0.0  0.0 \\\\\n0.0  0.1  0.0  0.0  0.0 \\\\\n0.0  0.0  5.0  0.0  0.0 \\\\\n0.0  0.0  0.0  0.05  0.0 \\\\\n0.0  0.0  0.0  0.0  2.0\n\\end{bmatrix}, \\quad\nH^{(2)} = \\begin{bmatrix}\n1  0  0  0  0 \\\\\n0  1  0  0  0 \\\\\n0  0  1  0  0 \\\\\n0  0  0  1  1\n\\end{bmatrix},\n$$\n$$\nR^{(2)} = \\operatorname{diag}(0.5, 0.5, 0.5, 0.1), \\quad\nd^{(2)} = \\begin{bmatrix} 1.0 \\\\ -0.5 \\\\ 0.3 \\\\ -0.2 \\end{bmatrix}.\n$$\n\n案例3（秩亏观测，相关背景块）：\n$$\nB_0^{(3)} = \\begin{bmatrix}\n1.5  0.6  0.0  0.0 \\\\\n0.6  1.2  0.0  0.0 \\\\\n0.0  0.0  0.7  0.3 \\\\\n0.0  0.0  0.3  0.9\n\\end{bmatrix}, \\quad\nH^{(3)} = \\begin{bmatrix}\n1  1  0  0 \\\\\n0  0  1  0\n\\end{bmatrix},\n$$\n$$\nR^{(3)} = \\operatorname{diag}(0.1, 0.2), \\quad\nd^{(3)} = \\begin{bmatrix} 0.2 \\\\ -0.1 \\end{bmatrix}.\n$$\n\n案例4（标量状态，重复观测）：\n$$\nB_0^{(4)} = \\begin{bmatrix} 0.5 \\end{bmatrix}, \\quad\nH^{(4)} = \\begin{bmatrix}\n1 \\\\\n1\n\\end{bmatrix}, \\quad\nR^{(4)} = \\operatorname{diag}(0.2, 0.3), \\quad\nd^{(4)} = \\begin{bmatrix} 0.1 \\\\ -0.2 \\end{bmatrix}.\n$$\n\n实现要求：\n- 通过 $B_0$ 的谱分解计算其对称平方根 $B_0^{1/2}$。\n- 在控制变量 $v$ 中构建变换后的对称正定算子，使用共轭梯度法求解 $v$，初始猜测为 $v_0 = 0$，容差为 $\\varepsilon = 10^{-10}$，最大迭代次数为 $200$。\n- 映射回 $\\delta x = B_0^{1/2} v$，并使用线性高斯估计的变分原理所蕴含的精确二次型 $J(\\delta x)$ 来计算代价 $J(\\delta x)$。\n- 对每个案例，输出四舍五入到 $6$ 位小数的最小化代价值 $J(\\delta x^\\star)$。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，例如 $[r_1,r_2,r_3,r_4]$，其中每个 $r_i$ 是案例 $i$ 指定的四舍五入后的浮点数。",
            "solution": "该问题要求在增量四维变分数据同化（$4$D-Var）的背景下，确定一个最优的状态增量 $\\delta x$。解决方案必须从线性高斯估计的第一性原理出发推导，表述为一个变换后的线性系统，并使用共轭梯度法进行数值求解。\n\n这个问题的基础是最小化一个二次代价函数 $J(\\delta x)$，该函数衡量了估计状态与先验（背景）估计和一组观测值之间的差异。在误差是无偏且呈高斯分布的标准假设下，代价函数是背景项和观测项这两项之和。\n\n背景项惩罚状态增量 $\\delta x$ 与其期望值（为 $0$）之间的偏差，并由背景误差协方差矩阵 $B_0 \\in \\mathbb{R}^{n \\times n}$ 的逆进行加权。观测项惩罚投影到状态空间的观测值与状态增量之间的不匹配，这种不匹配由新息向量 $d \\in \\mathbb{R}^m$ 表示。这种不匹配 $H \\delta x - d$ 由观测误差协方差矩阵 $R \\in \\mathbb{R}^{m \\times m}$ 的逆加权，其中 $H \\in \\mathbb{R}^{m \\times n}$ 是线性化观测算子。因此，代价函数 $J(\\delta x)$ 由下式给出：\n$$\nJ(\\delta x) = \\frac{1}{2} \\delta x^T B_0^{-1} \\delta x + \\frac{1}{2} (H \\delta x - d)^T R^{-1} (H \\delta x - d)\n$$\n矩阵 $B_0$ 和 $R$ 被指定为对称正定（SPD），这确保了它们的逆存在，并且 $J(\\delta x)$ 是一个具有唯一最小值的严格凸函数。这个最小值对应于最优状态增量 $\\delta x^\\star$。\n\n最小值在代价函数相对于 $\\delta x$ 的梯度为零的驻点处找到：\n$$\n\\nabla_{\\delta x} J(\\delta x) = B_0^{-1} \\delta x + H^T R^{-1} (H \\delta x - d) = 0\n$$\n这就得到了称为分析方程或正规方程的线性系统：\n$$\n(B_0^{-1} + H^T R^{-1} H) \\delta x = H^T R^{-1} d\n$$\nHessian 矩阵 $\\mathcal{H} = B_0^{-1} + H^T R^{-1} H$ 在实际应用中通常规模庞大且病态，使得直接求逆在计算上不可行且数值上不稳定。首选迭代求解器，但其收敛速度取决于 $\\mathcal{H}$ 的条件数。\n\n为了改善条件数，引入了控制变量变换。这是一种预处理形式。令 $\\delta x = B_0^{1/2} v$，其中 $v \\in \\mathbb{R}^n$ 是新的控制变量，$B_0^{1/2}$ 是 $B_0$ 唯一的对称正定平方根。因为 $B_0$ 是对称正定的，所以 $B_0^{1/2}$ 的存在性和唯一性得到了保证。这种变换将问题从物理空间（$\\delta x$ 的空间）映射到控制空间（$v$ 的空间），在控制空间中，背景误差协方差等效于单位矩阵。\n\n将该变换代入代价函数 $J(\\delta x)$，得到新的代价函数 $\\tilde{J}(v)$：\n$$\n\\tilde{J}(v) = J(B_0^{1/2} v) = \\frac{1}{2} (B_0^{1/2} v)^T B_0^{-1} (B_0^{1/2} v) + \\frac{1}{2} (H B_0^{1/2} v - d)^T R^{-1} (H B_0^{1/2} v - d)\n$$\n背景项显著简化：\n$$\n(B_0^{1/2} v)^T B_0^{-1} (B_0^{1/2} v) = v^T (B_0^{1/2})^T (B_0^{1/2} B_0^{1/2})^{-1} B_0^{1/2} v = v^T B_0^{1/2} (B_0^{1/2})^{-1} (B_0^{1/2})^{-1} B_0^{1/2} v = v^T v\n$$\n变换后的代价函数变为：\n$$\n\\tilde{J}(v) = \\frac{1}{2} v^T v + \\frac{1}{2} (H B_0^{1/2} v - d)^T R^{-1} (H B_0^{1/2} v - d)\n$$\n相对于控制变量 $v$ 的梯度是：\n$$\n\\nabla_v \\tilde{J}(v) = v + (H B_0^{1/2})^T R^{-1} (H B_0^{1/2} v - d)\n$$\n将梯度设为零以求最小值，得到关于 $v$ 的线性系统：\n$$\nv + B_0^{1/2} H^T R^{-1} H B_0^{1/2} v = B_0^{1/2} H^T R^{-1} d\n$$\n这可以写成 $A_v v = b_v$，其中：\n$$\nA_v = I + B_0^{1/2} H^T R^{-1} H B_0^{1/2} \\quad \\text{and} \\quad b_v = B_0^{1/2} H^T R^{-1} d\n$$\n矩阵 $A_v$ 是 $\\tilde{J}(v)$ 的 Hessian 矩阵。它是对称正定的，使得该系统适合使用共轭梯度（CG）法求解。\n\n将此控制变量变换用作预条件子的理由在于，与原始 Hessian 矩阵 $\\mathcal{H}$ 相比，$A_v$ 的谱特性得到了改善。$\\mathcal{H}$ 中的 $B_0^{-1}$ 项通常会引入大范围的尺度，导致条件数很大。相比之下，$A_v$ 是单位矩阵 $I$ 被一个半正定矩阵扰动后的结果。单位矩阵是完美条件的（条件数为 $1$），因此 $A_v$ 的条件数通常比 $\\mathcal{H}$（或者更准确地说，预处理后的 Hessian 矩阵 $B_0\\mathcal{H}$）的条件数更接近 $1$。这使得共轭梯度算法的收敛速度显著加快。\n\n数值实现过程如下：\n$1$. 对每个测试案例，定义矩阵 $B_0$、$H$、$R$ 和向量 $d$。\n$2$. 通过 $B_0$ 的谱分解计算对称平方根 $B_0^{1/2}$。如果 $B_0 = U \\Lambda U^T$，其中 $U$ 是特征向量构成的正交矩阵，$\\Lambda$ 是特征值构成的对角矩阵，则 $B_0^{1/2} = U \\Lambda^{1/2} U^T$。\n$3$. 计算观测误差协方差的逆 $R^{-1}$。由于 $R$ 是对角矩阵，其逆矩阵就是对角元素取倒数后形成的对角矩阵。\n$4$. 构建系统矩阵 $A_v$ 和右端向量 $b_v$。\n$5$. 使用共轭梯度算法求解线性系统 $A_v v = b_v$ 以得到最优控制变量 $v^\\star$，初始猜测为 $v_0=0$，收敛容差为 $\\varepsilon = 10^{-10}$，最大迭代次数为 $200$ 次。\n$6$. 获得 $v^\\star$ 后，使用数值上稳定的形式 $\\tilde{J}(v^\\star)$ 计算最小代价值。这避免了直接计算 $J(\\delta x^\\star)$ 时所需的对 $B_0^{-1}$ 的显式计算。\n$7$. 按要求将最终代价值四舍五入到 $6$ 位小数。对所有测试案例重复此过程。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import sqrtm\n\ndef conjugate_gradient(A, b, tol=1e-10, max_iter=200):\n    \"\"\"\n    Solves the symmetric positive definite linear system Ax = b using the\n    conjugate gradient method.\n    \"\"\"\n    n = len(b)\n    x = np.zeros(n, dtype=np.float64)  # Initial guess x0 = 0\n    r = b - A @ x  # Initial residual for x0 = 0 is b\n    p = r.copy()\n    rs_old = np.dot(r, r)\n\n    if np.sqrt(rs_old)  tol:\n        return x\n\n    for _ in range(max_iter):\n        Ap = A @ p\n        alpha = rs_old / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rs_new = np.dot(r, r)\n\n        if np.sqrt(rs_new)  tol:\n            break\n\n        p = r + (rs_new / rs_old) * p\n        rs_old = rs_new\n\n    return x\n\ndef solve_case(B0, H, R, d):\n    \"\"\"\n    Solves a single case of the 4D-Var inner-loop problem.\n    \"\"\"\n    # Ensure all inputs are double-precision numpy arrays\n    B0 = np.array(B0, dtype=np.float64)\n    H = np.array(H, dtype=np.float64)\n    R = np.array(R, dtype=np.float64)\n    d = np.array(d, dtype=np.float64)\n\n    # 1. Compute necessary matrices\n    # Since R is diagonal, its inverse is trivial to compute.\n    R_inv = np.diag(1.0 / np.diag(R))\n\n    # Compute the symmetric matrix square root of B0.\n    # We use scipy.linalg.sqrtm which is robust.\n    # Result for a real symmetric matrix can have negligible imaginary parts due to\n    # numerical precision, so we take the real part.\n    B0_sqrt = sqrtm(B0).real\n\n    # 2. Form the transformed system A_v * v = b_v\n    # The control variable transform is dx = B0_sqrt @ v\n    # The system matrix for v is A_v = I + B0_sqrt.T @ H.T @ R_inv @ H @ B0_sqrt\n    # Since B0_sqrt is symmetric, B0_sqrt.T = B0_sqrt\n    n = B0.shape[0]\n    A_v = np.identity(n) + B0_sqrt @ H.T @ R_inv @ H @ B0_sqrt\n    b_v = B0_sqrt @ H.T @ R_inv @ d\n\n    # 3. Solve for v using the conjugate gradient method\n    v_star = conjugate_gradient(A_v, b_v, tol=1e-10, max_iter=200)\n\n    # 4. Evaluate the minimized cost J(dx_star) = J_tilde(v_star)\n    # J_tilde(v) = 0.5 * v.T @ v + 0.5 * (H @ B0_sqrt @ v - d).T @ R_inv @ (H @ B0_sqrt @ v - d)\n    # This form is numerically preferred as it avoids using B0_inv.\n    term1 = 0.5 * np.dot(v_star, v_star)\n    residual_obs_space = H @ B0_sqrt @ v_star - d\n    term2 = 0.5 * np.dot(residual_obs_space.T, R_inv @ residual_obs_space)\n    min_cost = term1 + term2\n\n    return min_cost\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        (\n            [[1.0, 0.2, 0.0, 0.0, 0.0],\n             [0.2, 0.8, 0.1, 0.0, 0.0],\n             [0.0, 0.1, 1.2, 0.3, 0.0],\n             [0.0, 0.0, 0.3, 1.0, 0.2],\n             [0.0, 0.0, 0.0, 0.2, 0.9]],\n            [[1, 0, 0, 0, 0],\n             [0, 1, 0, 0, 0],\n             [0, 0, 1, 0, 0],\n             [0, 0, 0, 1, 0],\n             [0, 0, 0, 0, 1],\n             [1, 1, 0, 0, 0]],\n            np.diag([0.04, 0.05, 0.03, 0.04, 0.05, 0.02]),\n            [0.5, -0.3, 0.2, -0.1, 0.4, 0.0]\n        ),\n        # Case 2\n        (\n            np.diag([10.0, 0.1, 5.0, 0.05, 2.0]),\n            [[1, 0, 0, 0, 0],\n             [0, 1, 0, 0, 0],\n             [0, 0, 1, 0, 0],\n             [0, 0, 0, 1, 1]],\n            np.diag([0.5, 0.5, 0.5, 0.1]),\n            [1.0, -0.5, 0.3, -0.2]\n        ),\n        # Case 3\n        (\n            [[1.5, 0.6, 0.0, 0.0],\n             [0.6, 1.2, 0.0, 0.0],\n             [0.0, 0.0, 0.7, 0.3],\n             [0.0, 0.0, 0.3, 0.9]],\n            [[1, 1, 0, 0],\n             [0, 0, 1, 0]],\n            np.diag([0.1, 0.2]),\n            [0.2, -0.1]\n        ),\n        # Case 4\n        (\n            [[0.5]],\n            [[1],\n             [1]],\n            np.diag([0.2, 0.3]),\n            [0.1, -0.2]\n        )\n    ]\n\n    results = []\n    for B0, H, R, d in test_cases:\n        min_cost = solve_case(B0, H, R, d)\n        results.append(f\"{min_cost:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "与一次性求解整个时间窗口的变分方法不同，顺序估计方法（如集合卡尔曼滤波器，`EnKF`）会随着新观测的到来逐步更新状态估计。一个关键问题是：我们如何判断滤波器是否正常工作？本练习介绍了一种重要的诊断工具——归一化新息平方（`NIS`）统计量，它为检验滤波器关于误差的假设是否与观测一致提供了一个有原则的框架。您将推导 `NIS` 的理论分布，并将其应用于一个实际场景，即调整集合协方差膨胀因子，这是确保 `EnKF` 性能稳健的关键步骤。",
            "id": "3811231",
            "problem": "考虑一个海洋数据同化循环，其中海面温度由位于 $m$ 个位置的浮标阵列观测，并使用集合卡尔曼滤波器 (Ensemble Kalman Filter, EnKF) 同化到一个原始方程海洋模型中。设观测算子在观测分量上是线性的，且等于单位阵，因此 $H = I_{m}$。假设观测分量的预报误差是无偏的，服从协方差为 $P^{f} \\in \\mathbb{R}^{m \\times m}$ 的高斯分布；观测误差也是无偏的，服从协方差为 $R \\in \\mathbb{R}^{m \\times m}$ 的高斯分布，且与预报误差在统计上独立。定义分析时刻的新息为 $d = y - H x^{f} \\in \\mathbb{R}^{m}$，其中 $y$ 是观测向量，$x^{f}$ 是预报状态向量。EnKF 使用的预报观测误差协方差为 $S = H P^{f} H^{\\top} + R \\in \\mathbb{R}^{m \\times m}$。归一化新息平方统计量定义为 $\\text{NIS} = d^{\\top} S^{-1} d$。\n\n任务 A（分布推导）：从上述高斯假设出发，推导归一化新息平方统计量的分布，并阐明观测维度 $m$ 在该分布中的作用。论证在何种一致性条件下，该统计量为 $S$ 的保真度提供了一个有原则的检验。\n\n任务 B（各向同性设置下的实用调优）：将问题具体化到一个对计算海洋学而言科学上现实的各向同性设置中，其中 $H = I_{m}$，预报方差由集合估计为 $P^{f} = \\hat{\\sigma}_{f}^{2} I_{m}$（其中 $\\hat{\\sigma}_{f}^{2}  0$），观测误差协方差为 $R = \\sigma_{r}^{2} I_{m}$（其中已知传感器噪声方差 $\\sigma_{r}^{2}  0$）。假设将一个乘法膨胀因子 $\\lambda  0$ 应用于集合预报协方差，使得滤波器中使用的预报协方差为 $S(\\lambda) = \\lambda \\hat{\\sigma}_{f}^{2} I_{m} + \\sigma_{r}^{2} I_{m}$。在统计平稳状态下的 $K$ 个连续循环中，定义新息范数平方的样本均值为\n$$\n\\bar{q} = \\frac{1}{K} \\sum_{k=1}^{K} \\| d_{k} \\|^{2},\n$$\n其中 $d_{k}$ 表示第 $k$ 个循环的新息。在所述假设下，通过用可观测的估计量 $\\bar{q}$ 替代真实预报协方差的未知迹，推导膨胀因子 $\\lambda$ 的一个闭式表达式，该表达式在期望意义上强制满足一致性条件 $\\mathbb{E}[\\text{NIS}] = m$。你的最终答案必须是 $\\lambda$ 关于 $\\bar{q}$、$m$、$\\hat{\\sigma}_{f}^{2}$ 和 $\\sigma_{r}^{2}$ 的单个解析表达式。不需要进行数值计算。",
            "solution": "该问题分为两个任务。任务 A 要求推导归一化新息平方 (NIS) 统计量的分布，并解释其如何用作一致性检验。任务 B 要求在特定的各向同性设置下，推导膨胀因子 $\\lambda$ 的闭式表达式。\n\n任务 A：归一化新息平方统计量的分布\n\n设观测空间中的真实状态向量为 $H x^t$，其中 $x^t$ 是真实状态。观测向量 $y \\in \\mathbb{R}^m$ 由 $y = H x^t + v$ 给出，其中 $v \\sim \\mathcal{N}(0, R)$ 是观测误差向量，假设其服从均值为零、协方差矩阵为 $R \\in \\mathbb{R}^{m \\times m}$ 的高斯分布。预报状态向量 $x^f$ 与真实状态的关系为 $x^f = x^t + e^f$，其中 $e^f$ 是预报误差。观测空间中的预报误差为 $H e^f$。问题陈述该误差是无偏的，服从协方差为 $P^f \\in \\mathbb{R}^{m \\times m}$ 的高斯分布。这是一个非传统的记法，因为 $P^f$ 通常表示 $x^f$ 的协方差，而不是 $H x^f$ 的。然而，后续 $H = I_m$ 的假设澄清了我们正在考虑的是整个状态都被观测的情况，因此状态空间和观测空间的维度都是 $m$。当 $H = I_m$ 时，$P^f$ 就是预报误差 $e^f$ 的协方差。\n\n新息向量 $d \\in \\mathbb{R}^m$ 定义为 $d = y - H x^f$。代入 $y$ 和 $x^f$ 的表达式：\n$$\nd = (H x^t + v) - H(x^t + e^f) = v - H e^f\n$$\n由于 $v$ 和 $e^f$ 都是零均值高斯随机向量，并假设它们在统计上是独立的，因此它们的线性组合 $d$ 也是一个零均值高斯随机向量。\nd 的均值为：\n$$\n\\mathbb{E}[d] = \\mathbb{E}[v - H e^f] = \\mathbb{E}[v] - H \\mathbb{E}[e^f] = 0 - H(0) = 0\n$$\nd 的协方差为：\n$$\n\\text{Cov}(d) = \\mathbb{E}[d d^\\top] = \\mathbb{E}[(v - H e^f)(v - H e^f)^\\top] = \\mathbb{E}[v v^\\top - v (H e^f)^\\top - H e^f v^\\top + H e^f (H e^f)^\\top]\n$$\n由于 $v$ 和 $e^f$ 的独立性，交叉项的期望为零：$\\mathbb{E}[v (H e^f)^\\top] = \\mathbb{E}[v] \\mathbb{E}[(H e^f)^\\top] = 0$。\n因此，d 的协方差是各自协方差之和：\n$$\n\\text{Cov}(d) = \\mathbb{E}[v v^\\top] + \\mathbb{E}[H e^f (e^f)^\\top H^\\top] = R + H \\mathbb{E}[e^f (e^f)^\\top] H^\\top = R + H P^f H^\\top\n$$\n问题将预报观测误差协方差（或新息协方差）定义为 $S = H P^f H^\\top + R$。因此，如果滤波器使用的协方差 $P^f$ 和 $R$ 是真实的协方差，那么新息向量 $d$ 服从均值为 0、协方差为 $S$ 的多元正态分布：\n$$\nd \\sim \\mathcal{N}(0, S)\n$$\n归一化新息平方 (NIS) 统计量定义为 $\\text{NIS} = d^\\top S^{-1} d$。为了找到它的分布，我们定义一个变换后的随机向量 $z \\in \\mathbb{R}^m$ 为：\n$$\nz = S^{-1/2} d\n$$\n其中 $S^{1/2}$ 是正定矩阵 $S$ 的一个矩阵平方根，满足 $S = S^{1/2} (S^{1/2})^\\top$。因此，$S^{-1} = (S^{-1/2})^\\top S^{-1/2}$。\nz 的均值为 $\\mathbb{E}[z] = S^{-1/2} \\mathbb{E}[d] = 0$。\nz 的协方差为：\n$$\n\\text{Cov}(z) = \\mathbb{E}[z z^\\top] = \\mathbb{E}[S^{-1/2} d d^\\top (S^{-1/2})^\\top] = S^{-1/2} \\mathbb{E}[d d^\\top] (S^{-1/2})^\\top = S^{-1/2} S (S^{-1/2})^\\top\n$$\n代入 $S = S^{1/2} (S^{1/2})^\\top$：\n$$\n\\text{Cov}(z) = S^{-1/2} S^{1/2} (S^{1/2})^\\top (S^{-1/2})^\\top = I_m I_m^\\top = I_m\n$$\n其中 $I_m$ 是 $m \\times m$ 的单位矩阵。因此，向量 $z$ 服从标准多元正态分布，$z \\sim \\mathcal{N}(0, I_m)$。这意味着 $z$ 的分量，记为 $z_i$（$i=1, \\dots, m$），是独立同分布的标准正态随机变量，$z_i \\sim \\mathcal{N}(0,1)$。\n\n现在我们可以用 $z$ 来表示 NIS 统计量：\n$$\n\\text{NIS} = d^\\top S^{-1} d = d^\\top (S^{-1/2})^\\top S^{-1/2} d = (S^{-1/2}d)^\\top(S^{-1/2}d) = z^\\top z = \\sum_{i=1}^m z_i^2\n$$\n根据定义，$m$ 个独立标准正态随机变量的平方和服从自由度为 $m$ 的卡方分布。因此，NIS 统计量的分布是：\n$$\n\\text{NIS} \\sim \\chi^2_m\n$$\n观测维度 $m$ 的作用至关重要：它决定了这个卡方分布的自由度。\n\n此推导成立的条件是，用于归一化新息的新息协方差 $S = H P^f H^\\top + R$ 确实是新息向量 $d$ 的真实协方差。这就是一致性条件。如果滤波器关于预报误差协方差 $P^f$ 和观测误差协方差 $R$ 的假设是正确的，那么新息将与一个 $\\mathcal{N}(0, S)$ 过程在统计上是一致的。NIS 统计量为这种一致性提供了一个有原则的检验。卡方分布的一个关键性质是其期望值等于其自由度，即 $\\mathbb{E}[\\chi^2_m] = m$。因此，通过监测 NIS 值的长期平均值，可以检验 $S$ 的保真度。如果 NIS 的样本均值显著偏离 $m$，则意味着滤波器的误差协方差假设不正确，$S$ 要么过大（$\\mathbb{E}[\\text{NIS}]  m$），要么过小（$\\mathbb{E}[\\text{NIS}]  m$）。\n\n任务 B：膨胀因子 $\\lambda$ 的推导\n\n我们现在将问题具体化到各向同性设置，其中 $H = I_m$，集合估计的预报误差协方差为 $\\hat{P}^f = \\hat{\\sigma}_f^2 I_m$，观测误差协方差为 $R = \\sigma_r^2 I_m$。一个乘法膨胀因子 $\\lambda  0$ 被应用于预报误差协方差。滤波器中使用的最终预报观测误差协方差为：\n$$\nS(\\lambda) = \\lambda \\hat{P}^f + R = \\lambda \\hat{\\sigma}_f^2 I_m + \\sigma_r^2 I_m = (\\lambda \\hat{\\sigma}_f^2 + \\sigma_r^2) I_m\n$$\n该协方差矩阵的逆为：\n$$\nS(\\lambda)^{-1} = \\frac{1}{\\lambda \\hat{\\sigma}_f^2 + \\sigma_r^2} I_m\n$$\n对于给定的分析循环 $k$，其 NIS 统计量为 $\\text{NIS}_k = d_k^\\top S(\\lambda)^{-1} d_k$。代入 $S(\\lambda)^{-1}$ 的表达式：\n$$\n\\text{NIS}_k = d_k^\\top \\left( \\frac{1}{\\lambda \\hat{\\sigma}_f^2 + \\sigma_r^2} I_m \\right) d_k = \\frac{d_k^\\top d_k}{\\lambda \\hat{\\sigma}_f^2 + \\sigma_r^2} = \\frac{\\|d_k\\|^2}{\\lambda \\hat{\\sigma}_f^2 + \\sigma_r^2}\n$$\n任务是找到一个 $\\lambda$ 值，以强制满足一致性条件 $\\mathbb{E}[\\text{NIS}] = m$。在统计平稳状态下，对于任何循环 $k$，期望都是相同的。\n$$\n\\mathbb{E}[\\text{NIS}_k] = \\mathbb{E}\\left[ \\frac{\\|d_k\\|^2}{\\lambda \\hat{\\sigma}_f^2 + \\sigma_r^2} \\right] = \\frac{\\mathbb{E}[\\|d_k\\|^2]}{\\lambda \\hat{\\sigma}_f^2 + \\sigma_r^2}\n$$\n令该期望等于 $m$：\n$$\nm = \\frac{\\mathbb{E}[\\|d_k\\|^2]}{\\lambda \\hat{\\sigma}_f^2 + \\sigma_r^2}\n$$\n这个方程将未知参数 $\\lambda$ 与新息的期望平方范数 $\\mathbb{E}[\\|d_k\\|^2]$ 联系起来。这个期望代表了真实新息协方差矩阵的迹，即 $\\mathbb{E}[\\|d_k\\|^2] = \\text{tr}(\\text{Cov}(d_k))$。这个真实值是未知的。然而，问题为它提供了一个可观测的估计量，即 $K$ 个循环中新息范数平方的样本均值：\n$$\n\\bar{q} = \\frac{1}{K} \\sum_{k=1}^{K} \\|d_k\\|^2\n$$\n对于大的 $K$，$\\bar{q}$ 是 $\\mathbb{E}[\\|d_k\\|^2]$ 的一个估计量。按照问题的指示，我们在一致性方程中用其可观测的估计量 $\\bar{q}$ 来代替 $\\mathbb{E}[\\|d_k\\|^2]$：\n$$\nm = \\frac{\\bar{q}}{\\lambda \\hat{\\sigma}_f^2 + \\sigma_r^2}\n$$\n我们现在求解这个方程以得到膨胀因子 $\\lambda$：\n$$\n\\lambda \\hat{\\sigma}_f^2 + \\sigma_r^2 = \\frac{\\bar{q}}{m}\n$$\n$$\n\\lambda \\hat{\\sigma}_f^2 = \\frac{\\bar{q}}{m} - \\sigma_r^2\n$$\n$$\n\\lambda = \\frac{1}{\\hat{\\sigma}_f^2} \\left( \\frac{\\bar{q}}{m} - \\sigma_r^2 \\right)\n$$\n这就是膨胀因子 $\\lambda$ 的闭式表达式，它基于由 $\\bar{q}$ 总结的观测新息统计量，调整滤波器的内部协方差模型，以使其平均满足一致性准则 $\\mathbb{E}[\\text{NIS}]=m$。",
            "answer": "$$\n\\boxed{\\lambda = \\frac{1}{\\hat{\\sigma}_{f}^{2}} \\left( \\frac{\\bar{q}}{m} - \\sigma_{r}^{2} \\right)}\n$$"
        }
    ]
}