{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of effective data assimilation is the ability to diagnose whether the system is functioning as intended. This exercise explores the Normalized Innovation Squared (NIS) statistic, a powerful tool for validating the statistical assumptions within a Kalman filter framework. By deriving its theoretical distribution, you will gain insight into how to quantitatively assess filter performance and tune critical parameters like covariance inflation, a common and essential task in practical ocean modeling .",
            "id": "3811231",
            "problem": "Consider an ocean data assimilation cycle in which Sea Surface Temperature is observed by an array of buoys at $m$ locations, and assimilated into a primitive-equation ocean model using the Ensemble Kalman Filter (EnKF). Let the observation operator be linear and equal to the identity on the observed components, so $H = I_{m}$. Assume the forecast error for the observed components is unbiased and Gaussian with covariance $P^f \\in \\mathbb{R}^{m \\times m}$, and the observation error is unbiased and Gaussian with covariance $R \\in \\mathbb{R}^{m \\times m}$, statistically independent of the forecast error. Define the innovation at an analysis time by $d = y - H x^f \\in \\mathbb{R}^{m}$, where $y$ is the observation vector and $x^f$ is the forecast state vector. The predicted observation error covariance used by the EnKF is $S = H P^f H^{\\top} + R \\in \\mathbb{R}^{m \\times m}$. The normalized innovation squared statistic is defined by $\\text{NIS} = d^{\\top} S^{-1} d$.\n\nTask A (distributional derivation): Starting from the above Gaussian assumptions, derive the distribution of the normalized innovation squared statistic and identify the role of the observation dimension $m$ in this distribution. Justify under what consistency condition this statistic provides a principled check of the fidelity of $S$.\n\nTask B (practical tuning in an isotropic setting): Specialize to a scientifically realistic isotropic setting for computational oceanography where $H = I_{m}$, $P^f = \\hat{\\sigma}_{f}^{2} I_{m}$ with some ensemble-estimated forecast variance $\\hat{\\sigma}_{f}^{2} > 0$, and $R = \\sigma_{r}^{2} I_{m}$ with known sensor noise variance $\\sigma_{r}^{2} > 0$. Suppose a multiplicative inflation factor $\\lambda > 0$ is applied to the ensemble forecast covariance so that the predicted covariance used in the filter is $S(\\lambda) = \\lambda \\hat{\\sigma}_{f}^{2} I_{m} + \\sigma_{r}^{2} I_{m}$. Across $K$ successive cycles at statistical stationarity, define the sample mean of the squared innovation norm by\n$$\n\\bar{q} = \\frac{1}{K} \\sum_{k=1}^{K} \\| d_{k} \\|^{2},\n$$\nwhere $d_k$ denotes the innovation at cycle $k$. Under the stated assumptions, derive a closed-form expression for the inflation factor $\\lambda$ that enforces the consistency condition $\\mathbb{E}[\\text{NIS}] = m$ in expectation by replacing the unknown trace of the true predicted covariance with the observable estimator $\\bar{q}$. Your final answer must be a single analytic expression for $\\lambda$ in terms of $\\bar{q}$, $m$, $\\hat{\\sigma}_{f}^{2}$, and $\\sigma_{r}^{2}$. No numerical evaluation is required.",
            "solution": "The problem is divided into two tasks. Task A requires the derivation of the distribution of the normalized innovation squared (NIS) statistic and an explanation of its use as a consistency check. Task B requires the derivation of a closed-form expression for an inflation factor $\\lambda$ in a specialized isotropic setting.\n\nTask A: Distribution of the Normalized Innovation Squared Statistic\n\nLet the true state vector in observation space be $H x^t$, where $x^t$ is the true state. The observation vector $y \\in \\mathbb{R}^m$ is given by $y = H x^t + v$, where $v \\sim \\mathcal{N}(0, R)$ is the observation error vector, assumed to be Gaussian with zero mean and covariance matrix $R \\in \\mathbb{R}^{m \\times m}$. The forecast state vector $x^f$ is related to the true state by $x^f = x^t + e^f$, where $e^f$ is the forecast error. The forecast error in observation space is $H e^f$. The problem states that this error is unbiased and Gaussian with covariance $P^f$. This is an unconventional notation, as $P^f$ typically denotes the covariance of $x^f$, not $H x^f$. However, the subsequent assumption that $H = I_m$ clarifies that we are considering a case where the entire state is observed, so the state space and observation space dimension are both $m$. With $H = I_m$, we have $P^f$ as the covariance of the forecast error $e^f$.\n\nThe innovation vector $d \\in \\mathbb{R}^m$ is defined as $d = y - H x^f$. Substituting the expressions for $y$ and $x^f$:\n$$\nd = (H x^t + v) - H(x^t + e^f) = v - H e^f\n$$\nSince both $v$ and $e^f$ are zero-mean Gaussian random vectors and are assumed to be statistically independent, their linear combination $d$ is also a zero-mean Gaussian random vector.\nThe mean of $d$ is:\n$$\n\\mathbb{E}[d] = \\mathbb{E}[v - H e^f] = \\mathbb{E}[v] - H \\mathbb{E}[e^f] = 0 - H(0) = 0\n$$\nThe covariance of $d$ is:\n$$\n\\text{Cov}(d) = \\mathbb{E}[d d^\\top] = \\mathbb{E}[(v - H e^f)(v - H e^f)^\\top] = \\mathbb{E}[v v^\\top - v (H e^f)^\\top - H e^f v^\\top + H e^f (H e^f)^\\top]\n$$\nDue to the independence of $v$ and $e^f$, the cross-terms have zero expectation: $\\mathbb{E}[v (H e^f)^\\top] = \\mathbb{E}[v] \\mathbb{E}[(H e^f)^\\top] = 0$.\nThus, the covariance of $d$ is the sum of the individual covariances:\n$$\n\\text{Cov}(d) = \\mathbb{E}[v v^\\top] + \\mathbb{E}[H e^f (e^f)^\\top H^\\top] = R + H \\mathbb{E}[e^f (e^f)^\\top] H^\\top = R + H P^f H^\\top\n$$\nThe problem defines the predicted observation error covariance (or innovation covariance) as $S = H P^f H^\\top + R$. Therefore, if the covariances $P^f$ and $R$ used by the filter are the true covariances, then the innovation vector $d$ follows a multivariate normal distribution with mean $0$ and covariance $S$:\n$$\nd \\sim \\mathcal{N}(0, S)\n$$\nThe Normalized Innovation Squared (NIS) statistic is defined as $\\text{NIS} = d^\\top S^{-1} d$. To find its distribution, we define a transformed random vector $z \\in \\mathbb{R}^m$ as:\n$$\nz = S^{-1/2} d\n$$\nwhere $S^{1/2}$ is a matrix square root of the positive definite matrix $S$, such that $S = S^{1/2} (S^{1/2})^\\top$. Consequently, $S^{-1} = (S^{-1/2})^\\top S^{-1/2}$.\nThe mean of $z$ is $\\mathbb{E}[z] = S^{-1/2} \\mathbb{E}[d] = 0$.\nThe covariance of $z$ is:\n$$\n\\text{Cov}(z) = \\mathbb{E}[z z^\\top] = \\mathbb{E}[S^{-1/2} d d^\\top (S^{-1/2})^\\top] = S^{-1/2} \\mathbb{E}[d d^\\top] (S^{-1/2})^\\top = S^{-1/2} S (S^{-1/2})^\\top\n$$\nSubstituting $S = S^{1/2} (S^{1/2})^\\top$:\n$$\n\\text{Cov}(z) = S^{-1/2} S^{1/2} (S^{1/2})^\\top (S^{-1/2})^\\top = I_m I_m^\\top = I_m\n$$\nwhere $I_m$ is the $m \\times m$ identity matrix. Thus, the vector $z$ follows a standard multivariate normal distribution, $z \\sim \\mathcal{N}(0, I_m)$. This means that the components of $z$, denoted $z_i$ for $i=1, \\dots, m$, are independent and identically distributed standard normal random variables, $z_i \\sim \\mathcal{N}(0,1)$.\n\nNow we can express the NIS statistic in terms of $z$:\n$$\n\\text{NIS} = d^\\top S^{-1} d = d^\\top (S^{-1/2})^\\top S^{-1/2} d = (S^{-1/2}d)^\\top(S^{-1/2}d) = z^\\top z = \\sum_{i=1}^m z_i^2\n$$\nBy definition, the sum of the squares of $m$ independent standard normal random variables follows a chi-squared distribution with $m$ degrees of freedom. Therefore, the distribution of the NIS statistic is:\n$$\n\\text{NIS} \\sim \\chi^2_m\n$$\nThe role of the observation dimension $m$ is critical: it determines the number of degrees of freedom of this chi-squared distribution.\n\nThis derivation holds under the condition that the innovation covariance $S = H P^f H^\\top + R$ used to normalize the innovations is indeed the true covariance of the innovation vector $d$. This is the consistency condition. If the filter's assumptions about the forecast error covariance $P^f$ and observation error covariance $R$ are correct, the innovations will be statistically consistent with a $\\mathcal{N}(0, S)$ process. The NIS statistic provides a principled check of this consistency. A key property of the chi-squared distribution is that its expected value is equal to its degrees of freedom, $\\mathbb{E}[\\chi^2_m] = m$. Therefore, by monitoring the long-term average of the NIS values, one can check the fidelity of $S$. If the sample mean of NIS is significantly different from $m$, it implies that the filter's error covariance assumptions are incorrect, and $S$ is either too large ($\\mathbb{E}[\\text{NIS}] < m$) or too small ($\\mathbb{E}[\\text{NIS}] > m$).\n\nTask B: Derivation of the Inflation Factor $\\lambda$\n\nWe now specialize to the isotropic setting where $H = I_m$, the ensemble-estimated forecast error covariance is $\\hat{P}^f = \\hat{\\sigma}_f^2 I_m$, and the observation error covariance is $R = \\sigma_r^2 I_m$. A multiplicative inflation factor $\\lambda > 0$ is applied to the forecast error covariance. The resulting predicted observation error covariance used in the filter is:\n$$\nS(\\lambda) = \\lambda \\hat{P}^f + R = \\lambda \\hat{\\sigma}_f^2 I_m + \\sigma_r^2 I_m = (\\lambda \\hat{\\sigma}_f^2 + \\sigma_r^2) I_m\n$$\nThe inverse of this covariance matrix is:\n$$\nS(\\lambda)^{-1} = \\frac{1}{\\lambda \\hat{\\sigma}_f^2 + \\sigma_r^2} I_m\n$$\nThe NIS statistic for a given analysis cycle $k$ is $\\text{NIS}_k = d_k^\\top S(\\lambda)^{-1} d_k$. Substituting the expression for $S(\\lambda)^{-1}$:\n$$\n\\text{NIS}_k = d_k^\\top \\left( \\frac{1}{\\lambda \\hat{\\sigma}_f^2 + \\sigma_r^2} I_m \\right) d_k = \\frac{d_k^\\top d_k}{\\lambda \\hat{\\sigma}_f^2 + \\sigma_r^2} = \\frac{\\|d_k\\|^2}{\\lambda \\hat{\\sigma}_f^2 + \\sigma_r^2}\n$$\nThe task is to find a value of $\\lambda$ that enforces the consistency condition $\\mathbb{E}[\\text{NIS}] = m$. At statistical stationarity, the expectation is the same for any cycle $k$.\n$$\n\\mathbb{E}[\\text{NIS}_k] = \\mathbb{E}\\left[ \\frac{\\|d_k\\|^2}{\\lambda \\hat{\\sigma}_f^2 + \\sigma_r^2} \\right] = \\frac{\\mathbb{E}[\\|d_k\\|^2]}{\\lambda \\hat{\\sigma}_f^2 + \\sigma_r^2}\n$$\nSetting this expectation equal to $m$:\n$$\nm = \\frac{\\mathbb{E}[\\|d_k\\|^2]}{\\lambda \\hat{\\sigma}_f^2 + \\sigma_r^2}\n$$\nThis equation relates the unknown parameter $\\lambda$ to the expected squared norm of the innovation, $\\mathbb{E}[\\|d_k\\|^2]$. This expectation represents the trace of the true innovation covariance matrix, $\\mathbb{E}[\\|d_k\\|^2] = \\text{tr}(\\text{Cov}(d_k))$. This true value is unknown. However, the problem provides an observable estimator for it, the sample mean of the squared innovation norm over $K$ cycles:\n$$\n\\bar{q} = \\frac{1}{K} \\sum_{k=1}^{K} \\|d_k\\|^2\n$$\nFor large $K$, $\\bar{q}$ is an estimator for $\\mathbb{E}[\\|d_k\\|^2]$. Following the problem's instruction, we replace $\\mathbb{E}[\\|d_k\\|^2]$ with its observable estimator $\\bar{q}$ in the consistency equation:\n$$\nm = \\frac{\\bar{q}}{\\lambda \\hat{\\sigma}_f^2 + \\sigma_r^2}\n$$\nWe now solve this equation for the inflation factor $\\lambda$:\n$$\n\\lambda \\hat{\\sigma}_f^2 + \\sigma_r^2 = \\frac{\\bar{q}}{m}\n$$\n$$\n\\lambda \\hat{\\sigma}_f^2 = \\frac{\\bar{q}}{m} - \\sigma_r^2\n$$\n$$\n\\lambda = \\frac{1}{\\hat{\\sigma}_f^2} \\left( \\frac{\\bar{q}}{m} - \\sigma_r^2 \\right)\n$$\nThis is the closed-form expression for the inflation factor $\\lambda$ that, based on the observed innovation statistics summarized by $\\bar{q}$, tunes the filter's internal covariance model to satisfy the consistency criterion $\\mathbb{E}[\\text{NIS}]=m$ on average.",
            "answer": "$$\n\\boxed{\\frac{1}{\\hat{\\sigma}_{f}^{2}} \\left( \\frac{\\bar{q}}{m} - \\sigma_{r}^{2} \\right)}\n$$"
        },
        {
            "introduction": "Variational data assimilation methods, such as 4D-Var, rely on gradient-based optimization to find the best estimate of the ocean state. The gradient is computed efficiently by a corresponding adjoint model, but verifying its implementation is a critical and non-trivial step. This practice guides you through the construction of a gradient test, the gold standard for adjoint model verification, providing indispensable hands-on experience in the development and debugging of variational assimilation systems .",
            "id": "3811253",
            "problem": "Consider a simplified two-dimensional linear model for horizontal ocean currents influenced by linear damping and the Coriolis force. Let the state be $x_k \\in \\mathbb{R}^2$ with components $x_k = [u_k, v_k]^\\top$ representing zonal and meridional velocities at discrete time index $k \\in \\{0,1,\\dots,K\\}$. The discrete-time dynamics are obtained from forward Euler discretization of a linear system with time step $\\Delta t$ and parameters $\\theta = [\\alpha, f]^\\top$, where $\\alpha$ is a linear damping rate and $f$ is the Coriolis parameter. The dynamics are\n$$\nx_{k+1} = M(\\theta)\\, x_k, \\quad M(\\theta) = I + \\Delta t\\, F(\\theta), \\quad F(\\theta) = \\begin{bmatrix} -\\alpha & f \\\\ -f & -\\alpha \\end{bmatrix},\n$$\nfor all $k \\in \\{0,1,\\dots,K-1\\}$, where $I$ is the identity matrix of size $2$.\n\nObservations are scalar measurements of the zonal velocity $u_k$ at each time $k$, with measurement operator $H = [1,0]$. Let $y_k \\in \\mathbb{R}$ denote the observation at time $k$. Consider the Four-Dimensional Variational (4D-Var) data assimilation cost function defined as\n$$\nJ(x_0,\\theta) = \\tfrac{1}{2}(x_0 - x_b)^\\top B^{-1} (x_0 - x_b) + \\tfrac{1}{2}\\sum_{k=0}^K \\left(H x_k - y_k\\right)^\\top R^{-1} \\left(H x_k - y_k\\right) + \\tfrac{1}{2}(\\theta - \\theta_b)^\\top P^{-1} (\\theta - \\theta_b),\n$$\nwhere $x_b \\in \\mathbb{R}^2$ and $\\theta_b \\in \\mathbb{R}^2$ are prior means, $B \\in \\mathbb{R}^{2\\times 2}$ and $P \\in \\mathbb{R}^{2\\times 2}$ are prior covariances, and $R \\in \\mathbb{R}$ is the observation error variance (so that $R^{-1}$ is a scalar). The state trajectory $\\{x_k\\}_{k=0}^K$ is constrained by the dynamics given above.\n\nYour task is to construct and implement a gradient test that compares finite-difference directional derivatives of $J$ with adjoint-derived directional derivatives. The gradient test must use the central finite-difference approximation in a direction $(p_{x_0}, p_{\\theta})$, with step size $h>0$,\n$$\nD_{\\mathrm{FD}} J(x_0,\\theta; p_{x_0}, p_{\\theta}, h) = \\frac{J(x_0 + h\\, p_{x_0},\\ \\theta + h\\, p_{\\theta}) - J(x_0 - h\\, p_{x_0},\\ \\theta - h\\, p_{\\theta})}{2h},\n$$\nand it must compare this to the adjoint-derived directional derivative\n$$\nD_{\\mathrm{ADJ}} J(x_0,\\theta; p_{x_0}, p_{\\theta}) = \\nabla_{x_0} J(x_0,\\theta)^\\top p_{x_0} + \\nabla_{\\theta} J(x_0,\\theta)^\\top p_{\\theta},\n$$\nwhere $\\nabla_{x_0} J$ and $\\nabla_{\\theta} J$ are the gradients of $J$ with respect to $x_0$ and $\\theta$, respectively, obtained via the discrete adjoint of the linear model and the chain rule. You must compute the relative error\n$$\n\\mathrm{relerr}(h, p_{x_0}, p_{\\theta}) = \\frac{\\left| D_{\\mathrm{FD}} J(x_0,\\theta; p_{x_0}, p_{\\theta}, h) - D_{\\mathrm{ADJ}} J(x_0,\\theta; p_{x_0}, p_{\\theta}) \\right|}{\\max\\left(1,\\ \\left| D_{\\mathrm{FD}} J(x_0,\\theta; p_{x_0}, p_{\\theta}, h) \\right|\\right)},\n$$\nand declare a test to pass if $\\mathrm{relerr} \\leq \\tau$, for a prescribed tolerance $\\tau$. The tolerance $\\tau$ is to be specified precisely below and used uniformly for all tests.\n\nYou must implement a complete, runnable program that performs the gradient test over the following scientifically consistent test suite. All mathematical quantities below must be interpreted with the units that make the equations dimensionally consistent; however, the final outputs required are dimensionless boolean values and thus do not require units.\n\nModel setup and data generation:\n- Time-stepping parameters: $K = 10$, $\\Delta t = 900\\ \\mathrm{s}$.\n- True parameters for generating observations: $\\alpha^\\star = 2\\times 10^{-6}\\ \\mathrm{s}^{-1}$, $f^\\star = 1\\times 10^{-4}\\ \\mathrm{s}^{-1}$.\n- True initial state for generating observations: $x_0^\\star = [0.1,\\ 0.05]^\\top$.\n- Observations: $y_k = H x_k^\\star$, where $\\{x_k^\\star\\}$ is the trajectory obtained by integrating the dynamics with $(x_0^\\star,\\ \\theta^\\star)$.\n- Prior means: $x_b = [0,\\ 0]^\\top$, $\\theta_b = [1\\times 10^{-6},\\ 0.8\\times 10^{-4}]^\\top$.\n- Prior covariances and observation error variance:\n  - $B = \\mathrm{diag}([\\sigma_{b,u}^2,\\ \\sigma_{b,v}^2])$ with $\\sigma_{b,u} = 0.2$, $\\sigma_{b,v} = 0.2$.\n  - $R = \\sigma_R^2$ with $\\sigma_R = 0.01$.\n  - $P = \\mathrm{diag}([\\sigma_{\\alpha}^2,\\ \\sigma_{f}^2])$ with $\\sigma_{\\alpha} = 1\\times 10^{-6}$, $\\sigma_{f} = 0.2\\times 10^{-4}$.\n- Evaluation point for the gradient test (not the truth): $x_0 = [0.05,\\ 0.01]^\\top$, $\\theta = [1.5\\times 10^{-6},\\ 0.9\\times 10^{-4}]^\\top$.\n\nRequired gradient construction:\n- Derive and implement the discrete adjoint to obtain $\\nabla_{x_0} J$ and $\\nabla_{\\theta} J$ at the evaluation point $(x_0,\\ \\theta)$.\n\nDirectional derivative comparisons and tolerances:\n- Use the central finite-difference approximation defined above and compare to the adjoint-derived directional derivative.\n- Use a uniform tolerance $\\tau = 1\\times 10^{-6}$ to declare pass or fail for each test case.\n\nTest suite:\n- State-only direction tests with $p_{x_0} = [1.0,\\ -0.5]^\\top$, $p_{\\theta} = [0,\\ 0]^\\top$, and step sizes $h \\in \\{1\\times 10^{-16},\\ 1\\times 10^{-5},\\ 1\\times 10^{-1}\\}$.\n- Parameter-only direction tests with $p_{x_0} = [0,\\ 0]^\\top$, $p_{\\theta} = [1.0,\\ -1.0]^\\top$, and step sizes $h \\in \\{1\\times 10^{-16},\\ 1\\times 10^{-5},\\ 1\\times 10^{-1}\\}$.\n- Mixed direction tests with $p_{x_0} = [0.3,\\ 0.7]^\\top$, $p_{\\theta} = [0.5,\\ 0.2]^\\top$, and step sizes $h \\in \\{1\\times 10^{-5},\\ 1\\times 10^{-1}\\}$.\n\nFor each of the above eight tests, compute $\\mathrm{relerr}$ and return a boolean indicating whether the test passes ($\\mathrm{relerr} \\leq \\tau$). Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[{\\tt True},{\\tt False},\\dots]$), in the exact order that the tests are listed above.",
            "solution": "The user-provided problem has been validated and is deemed a well-posed, scientifically-grounded exercise in computational data assimilation. This solution will present the derivation and implementation of a gradient test for the provided Four-Dimensional Variational (4D-Var) cost function.\n\nThe core of the task is to compute the gradient of the cost function $J(x_0, \\theta)$ with respect to the control variables, which are the initial state $x_0$ and the model parameters $\\theta$. This gradient is required for the adjoint-derived directional derivative. The cost function is\n$$\nJ(x_0,\\theta) = \\underbrace{\\tfrac{1}{2}(x_0 - x_b)^\\top B^{-1} (x_0 - x_b)}_{J_b(x_0)} + \\underbrace{\\tfrac{1}{2}\\sum_{k=0}^K \\left(H x_k - y_k\\right)^\\top R^{-1} \\left(H x_k - y_k\\right)}_{J_o(x_0, \\theta)} + \\underbrace{\\tfrac{1}{2}(\\theta - \\theta_b)^\\top P^{-1} (\\theta - \\theta_b)}_{J_p(\\theta)}.\n$$\nThe state $x_k$ for $k>0$ is a function of the control variables, implicitly defined by the model dynamics $x_{k+1} = M(\\theta) x_k$. The gradient of $J$ is the sum of the gradients of its components: $\\nabla J = \\nabla J_b + \\nabla J_o + \\nabla J_p$.\n\nThe gradients of the background terms, $J_b$ and $J_p$, are straightforward:\n$$\n\\nabla_{x_0} J_b = B^{-1}(x_0 - x_b)\n$$\n$$\n\\nabla_{\\theta} J_p = P^{-1}(\\theta - \\theta_b)\n$$\nThe gradient of the observation term $J_o$ is more complex due to the recursive dependency of the states $\\{x_k\\}$ on the control variables. We employ the discrete adjoint method, a specialized application of the Lagrange multiplier technique, to compute this gradient efficiently.\n\nWe define a sequence of adjoint variables, $\\{\\lambda_k\\}_{k=0}^K$, which are vectors in $\\mathbb{R}^2$. The adjoint model is derived by ensuring that the gradient of a Lagrangian (the cost function augmented with the model constraints) with respect to the state variables $x_k$ (for $k > 0$) is zero. This leads to a system of equations that can be solved for the adjoint variables by integrating backward in time.\n\nThe procedure to compute the gradient components $\\nabla_{x_0} J_o$ and $\\nabla_{\\theta} J_o$ is as follows:\n\n1.  **Forward Integration**: Given a point in the control space $(x_0, \\theta)$, integrate the forward model $x_{k+1} = M(\\theta) x_k$ from $k=0$ to $K-1$, storing the entire state trajectory $\\{x_k\\}_{k=0}^K$. The matrix $M(\\theta)$ is given by $M(\\theta) = I + \\Delta t\\, F(\\theta)$ with $F(\\theta) = \\begin{bmatrix} -\\alpha & f \\\\ -f & -\\alpha \\end{bmatrix}$.\n\n2.  **Backward (Adjoint) Integration**:\n    a. Initialize the final adjoint variable $\\lambda_K$. This corresponds to the gradient of the cost function with respect to the final state $x_K$:\n    $$\n    \\lambda_K = \\frac{\\partial J_o}{\\partial x_K} = H^\\top R^{-1} (H x_K - y_K)\n    $$\n    b. Integrate backward from $k=K-1$ down to $k=0$. The adjoint variable at step $k$ is computed from the adjoint variable at step $k+1$ using the transpose of the forward model's propagator matrix, $M(\\theta)^\\top$, and an additional forcing term from the observations at time $k$:\n    $$\n    \\lambda_k = M(\\theta)^\\top \\lambda_{k+1} + H^\\top R^{-1} (H x_k - y_k)\n    $$\n    The final adjoint variable computed, $\\lambda_0$, is precisely the gradient of the observation cost function with respect to the initial state: $\\nabla_{x_0} J_o = \\lambda_0$.\n\n3.  **Parameter Gradient Accumulation**: The gradient with respect to the parameters $\\theta$ can be computed during the backward integration. The sensitivity of the cost function to a parameter $\\theta_j$ is given by\n    $$\n    \\nabla_{\\theta_j} J_o = \\sum_{k=0}^{K-1} \\left( \\frac{\\partial x_{k+1}}{\\partial \\theta_j} \\right)^\\top \\lambda_{k+1}\n    $$\n    where the quantity in the parenthesis is evaluated along the forward trajectory. Since $x_{k+1} = M(\\theta) x_k$, we have $\\frac{\\partial x_{k+1}}{\\partial \\theta_j} = \\left(\\frac{\\partial M(\\theta)}{\\partial \\theta_j}\\right) x_k$. The required matrix derivatives are:\n    $$\n    \\frac{\\partial M}{\\partial \\alpha} = \\Delta t \\begin{bmatrix} -1 & 0 \\\\ 0 & -1 \\end{bmatrix} = -\\Delta t I\n    $$\n    $$\n    \\frac{\\partial M}{\\partial f} = \\Delta t \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix}\n    $$\n    During the backward sweep for $k=K-1, \\dots, 0$, we accumulate the contributions to the parameter gradient:\n    $$\n    \\nabla_{\\alpha} J_o \\leftarrow \\nabla_{\\alpha} J_o + \\left( (-\\Delta t I) x_k \\right)^\\top \\lambda_{k+1} = \\nabla_{\\alpha} J_o - \\Delta t \\, x_k^\\top \\lambda_{k+1}\n    $$\n    $$\n    \\nabla_{f} J_o \\leftarrow \\nabla_{f} J_o + \\left( \\left(\\Delta t \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix}\\right) x_k \\right)^\\top \\lambda_{k+1} = \\nabla_{f} J_o + \\Delta t \\, x_k^\\top \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix} \\lambda_{k+1}\n    $$\n\nFinally, the total gradients are assembled by adding the contributions from the background terms:\n$$\n\\nabla_{x_0} J = B^{-1}(x_0 - x_b) + \\lambda_0\n$$\n$$\n\\nabla_{\\theta} J = P^{-1}(\\theta - \\theta_b) + \\nabla_{\\theta} J_o\n$$\nWith these gradients, the adjoint-derived directional derivative, $D_{\\mathrm{ADJ}} J$, can be computed. This is then compared to the central finite-difference approximation, $D_{\\mathrm{FD}} J$, to compute the relative error for each test case. The test passes if this error is below the tolerance $\\tau = 1 \\times 10^{-6}$.\n\nThe test suite is designed to probe the correctness and numerical stability of the implementation. The tests with a very small step size ($h=1 \\times 10^{-16}$) are expected to fail due to catastrophic cancellation in floating-point arithmetic. The tests with a large step size ($h=1 \\times 10^{-1}$) are expected to fail due to large truncation errors in the finite-difference approximation. The tests with an intermediate step size ($h=1 \\times 10^{-5}$) are expected to pass, demonstrating the accuracy of the adjoint-derived gradient.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and implements a gradient test for a 4D-Var data assimilation problem.\n    \"\"\"\n\n    # --- Model setup and data generation ---\n\n    # Time-stepping parameters\n    K = 10\n    dt = 900.0\n\n    # True parameters and initial state for generating observations\n    alpha_star = 2e-6\n    f_star = 1e-4\n    theta_star = np.array([alpha_star, f_star])\n    x0_star = np.array([0.1, 0.05])\n\n    # Prior means\n    x_b = np.array([0.0, 0.0])\n    theta_b = np.array([1e-6, 0.8e-4])\n\n    # Standard deviations for covariances\n    sigma_b_u = 0.2\n    sigma_b_v = 0.2\n    sigma_R = 0.01\n    sigma_alpha = 1e-6\n    sigma_f = 0.2e-4\n\n    # Inverse covariance matrices and observation error variance\n    B_inv = np.diag([1/sigma_b_u**2, 1/sigma_b_v**2])\n    R_inv = 1/sigma_R**2\n    P_inv = np.diag([1/sigma_alpha**2, 1/sigma_f**2])\n    \n    # Observation operator\n    H = np.array([1.0, 0.0])\n\n    # Generate synthetic observations\n    def get_M(theta):\n        alpha, f = theta\n        F = np.array([[-alpha, f], [-f, -alpha]])\n        return np.identity(2) + dt * F\n\n    M_star = get_M(theta_star)\n    x_star_traj = np.zeros((K + 1, 2))\n    x_star_traj[0] = x0_star\n    for k in range(K):\n        x_star_traj[k + 1] = M_star @ x_star_traj[k]\n\n    ys = np.array([H @ xk for xk in x_star_traj])\n\n    # --- Core Functions ---\n    def cost_function_J(x0, theta):\n        \"\"\"Computes the 4D-Var cost function J(x0, theta).\"\"\"\n        # Trajectory calculation\n        M = get_M(theta)\n        x_traj = np.zeros((K + 1, 2))\n        x_traj[0] = x0\n        for k in range(K):\n            x_traj[k + 1] = M @ x_traj[k]\n\n        # Background term for x0\n        j_b = 0.5 * (x0 - x_b).T @ B_inv @ (x0 - x_b)\n\n        # Observation term\n        j_o = 0.0\n        for k in range(K + 1):\n            mismatch = H @ x_traj[k] - ys[k]\n            j_o += 0.5 * mismatch * R_inv * mismatch\n        \n        # Background term for theta\n        j_p = 0.5 * (theta - theta_b).T @ P_inv @ (theta - theta_b)\n\n        return j_b + j_o + j_p\n\n    def adjoint_gradient(x0, theta):\n        \"\"\"Computes gradients of J w.r.t. x0 and theta using the adjoint method.\"\"\"\n        # 1. Forward model run to get trajectory\n        M = get_M(theta)\n        x_traj = np.zeros((K + 1, 2))\n        x_traj[0] = x0\n        for k in range(K):\n            x_traj[k + 1] = M @ x_traj[k]\n\n        # 2. Backward (adjoint) integration\n        grad_theta_o = np.zeros(2)\n        \n        # Initialize adjoint variable at final time K\n        forcing_K = H.T * R_inv * (H @ x_traj[K] - ys[K])\n        lambda_next = forcing_K # This is lambda_{k+1}, starting with lambda_K\n\n        # Loop from k = K-1 down to 0\n        for k in range(K - 1, -1, -1):\n            # Accumulate parameter gradient contribution using x_k and lambda_{k+1}\n            # Contribution to dJ_o/d_alpha\n            grad_theta_o[0] += (-dt * x_traj[k]) @ lambda_next\n            # Contribution to dJ_o/d_f\n            dMdf_x_k_T = dt * np.array([x_traj[k, 1], -x_traj[k, 0]]) # (dM/df * x_k)^T\n            grad_theta_o[1] += dMdf_x_k_T @ lambda_next\n\n            # Forcing term at time k\n            forcing_k = H.T * R_inv * (H @ x_traj[k] - ys[k])\n            \n            # Update adjoint state: lambda_k = M^T @ lambda_{k+1} + forcing_k\n            lambda_k = M.T @ lambda_next + forcing_k\n            \n            # Update for next iteration\n            lambda_next = lambda_k\n\n        lambda_0 = lambda_next\n\n        # 3. Assemble full gradients\n        grad_x0 = B_inv @ (x0 - x_b) + lambda_0\n        grad_theta = P_inv @ (theta - theta_b) + grad_theta_o\n\n        return grad_x0, grad_theta\n\n    # --- Gradient Test ---\n    # Evaluation point for the gradient test\n    x0_eval = np.array([0.05, 0.01])\n    theta_eval = np.array([1.5e-6, 0.9e-4])\n\n    # Test suite\n    test_cases = [\n        # State-only direction tests\n        {'p_x0': np.array([1.0, -0.5]), 'p_theta': np.array([0.0, 0.0]), 'h': 1e-16},\n        {'p_x0': np.array([1.0, -0.5]), 'p_theta': np.array([0.0, 0.0]), 'h': 1e-5},\n        {'p_x0': np.array([1.0, -0.5]), 'p_theta': np.array([0.0, 0.0]), 'h': 1e-1},\n        # Parameter-only direction tests\n        {'p_x0': np.array([0.0, 0.0]), 'p_theta': np.array([1.0, -1.0]), 'h': 1e-16},\n        {'p_x0': np.array([0.0, 0.0]), 'p_theta': np.array([1.0, -1.0]), 'h': 1e-5},\n        {'p_x0': np.array([0.0, 0.0]), 'p_theta': np.array([1.0, -1.0]), 'h': 1e-1},\n        # Mixed direction tests\n        {'p_x0': np.array([0.3, 0.7]), 'p_theta': np.array([0.5, 0.2]), 'h': 1e-5},\n        {'p_x0': np.array([0.3, 0.7]), 'p_theta': np.array([0.5, 0.2]), 'h': 1e-1},\n    ]\n\n    tau = 1e-6\n    results = []\n    \n    # Pre-compute adjoint gradient at the evaluation point\n    grad_x0_adj, grad_theta_adj = adjoint_gradient(x0_eval, theta_eval)\n\n    for case in test_cases:\n        p_x0, p_theta, h = case['p_x0'], case['p_theta'], case['h']\n\n        # Adjoint-derived directional derivative\n        D_adj = grad_x0_adj.T @ p_x0 + grad_theta_adj.T @ p_theta\n\n        # Finite-difference directional derivative (central difference)\n        J_plus = cost_function_J(x0_eval + h * p_x0, theta_eval + h * p_theta)\n        J_minus = cost_function_J(x0_eval - h * p_x0, theta_eval - h * p_theta)\n        \n        # Avoid division by zero if h is extremely small\n        if 2*h == 0:\n            D_fd = 0.0\n        else:\n            D_fd = (J_plus - J_minus) / (2 * h)\n\n        # Relative error calculation\n        abs_diff = np.abs(D_fd - D_adj)\n        denominator = np.max([1.0, np.abs(D_fd)])\n        rel_err = abs_diff / denominator\n        \n        # Test pass/fail\n        results.append(rel_err = tau)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond estimating the state of a system, data assimilation is often used to infer unknown model parameters, such as friction or forcing coefficients. However, the information contained in observations may be insufficient to distinguish between different parameter combinations, a fundamental challenge known as non-identifiability or confounding. This exercise uses a classic oceanographic scenario to explore this limitation analytically, challenging you to determine not only why the problem exists but also how an improved observational strategy could resolve it .",
            "id": "3811287",
            "problem": "Consider a depth-averaged, barotropic, one-dimensional along-channel flow in a rectangular channel of length $L$ with periodic boundary conditions, uniform depth $H$, and constant reference density $\\rho_{0}$. The along-channel momentum equation for the spatially uniform mode is governed by the fundamental balance between the barotropic pressure gradient, wind stress, and linear bottom drag,\n$$\n\\rho_{0} H \\frac{dU}{dt} \\;=\\; - \\rho_{0} g H \\frac{\\partial \\eta}{\\partial x} \\;+\\; \\tau_{x} \\;-\\; \\rho_{0} H r U,\n$$\nwhere $U$ is the depth-averaged along-channel velocity, $g$ is the gravitational acceleration, $\\eta$ is the free-surface elevation, $\\tau_{x}$ is the spatially uniform wind stress (assumed constant in time for the steady-state analysis), and $r$ is a linear bottom drag coefficient. Assume that the flow reaches a steady and spatially uniform state and that, for the uniform mode under periodic boundary conditions, the mean barotropic pressure gradient can be taken to be negligible for this particular mode (i.e., the spatially uniform branch of the solution has $\\partial \\eta/\\partial x$ effectively zero in the mean for the steady uniform mode).\n\nYou are performing state and parameter estimation using only a single steady-state observation of $U$ at one time. Denote the parameter vector by $\\boldsymbol{\\theta} = (\\tau_{x}, r)$ and the observational operator by $h(\\boldsymbol{\\theta}) = U(\\tau_{x}, r)$ evaluated at steady state under the assumptions above. Treat $U$ as the scalar state variable observed by the data assimilation system.\n\nTasks:\n- Starting from the momentum equation and the stated assumptions, derive the steady-state relation linking $U$, $\\tau_{x}$, and $r$ for the uniform mode. Use this to show that there exists a one-dimensional family of parameter pairs $(\\tau_{x}, r)$ that produce the same steady-state $U$, hence demonstrating confounding (non-identifiability) between wind stress amplitude and bottom drag when only $U$ is observed.\n- Formally compute the sensitivity (Jacobian) of $h(\\boldsymbol{\\theta})$ with respect to $\\boldsymbol{\\theta}$ and determine the one-dimensional nullspace direction in parameter space, i.e., the nonzero parameter-increment direction $\\delta \\boldsymbol{\\theta}$ that yields zero change in $U$ at first order. Report this direction normalized to have unit Euclidean norm.\n- Propose an additional observation type or experimental design that would break this degeneracy and justify mathematically which additional parameter dependence it introduces that yields local identifiability of $\\tau_{x}$ and $r$ (i.e., a full-rank sensitivity with respect to $\\boldsymbol{\\theta}$), under physically plausible oceanographic conditions.\n\nAnswer specification:\n- The final answer must be the single, closed-form analytical expression giving the unit-norm nullspace direction vector in parameter space (expressed as a row vector). Do not include any units in the final answer. No rounding is required.",
            "solution": "The problem is first validated against the specified criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Governing Equation**: $\\rho_{0} H \\frac{dU}{dt} \\;=\\; - \\rho_{0} g H \\frac{\\partial \\eta}{\\partial x} \\;+\\; \\tau_{x} \\;-\\; \\rho_{0} H r U$\n- **System**: Depth-averaged, barotropic, one-dimensional along-channel flow.\n- **Geometry  Boundaries**: Rectangular channel of length $L$, uniform depth $H$, periodic boundary conditions.\n- **Variables**: $U$ (depth-averaged along-channel velocity), $\\eta$ (free-surface elevation).\n- **Parameters**: $\\tau_{x}$ (spatially uniform, constant wind stress), $r$ (linear bottom drag coefficient). The parameter vector is $\\boldsymbol{\\theta} = (\\tau_{x}, r)$.\n- **Constants**: $\\rho_{0}$ (constant reference density), $g$ (gravitational acceleration).\n- **Assumptions**: \n    1. The flow reaches a steady state.\n    2. The flow is spatially uniform (considering the spatially uniform mode).\n    3. For the uniform mode, the mean barotropic pressure gradient is negligible, i.e., $\\frac{\\partial \\eta}{\\partial x}$ is effectively zero.\n- **Observation Scheme**: A single steady-state observation of $U$.\n- **Observational Operator**: $h(\\boldsymbol{\\theta}) = U(\\tau_{x}, r)$ evaluated at steady state.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is based on a simplified form of the shallow water momentum equation, a cornerstone of geophysical fluid dynamics. The balance of forces (wind stress vs. bottom drag) is a fundamental concept in modeling wind-driven circulation. The model is a common and valid idealization used in oceanography.\n- **Well-Posed**: The problem is well-posed. It provides a clear governing equation and a set of simplifying assumptions, from which a unique steady-state relationship can be derived. The tasks—deriving this relation, analyzing parameter sensitivity, and proposing a method to resolve non-identifiability—are standard procedures in inverse modeling and data assimilation.\n- **Objective**: The problem is stated in precise, objective, and standard scientific language. All terms are clearly defined within the context of physical oceanography.\n\n**Step 3: Verdict and Action**\nThe problem is scientifically sound, well-posed, and objective. It is a valid problem that can be solved as stated.\n\n### Solution\n\n**Part 1: Steady-State Relation and Confounding**\n\nWe begin with the provided one-dimensional along-channel momentum equation:\n$$\n\\rho_{0} H \\frac{dU}{dt} \\;=\\; - \\rho_{0} g H \\frac{\\partial \\eta}{\\partial x} \\;+\\; \\tau_{x} \\;-\\; \\rho_{0} H r U\n$$\nTo find the steady-state relation, we apply the specified assumptions.\nFirst, we assume the flow reaches a steady state, which implies that all time derivatives are zero. Thus, $\\frac{dU}{dt} = 0$.\nSecond, for the spatially uniform mode under periodic boundary conditions, the mean barotropic pressure gradient term is assumed to be negligible, so $\\frac{\\partial \\eta}{\\partial x} = 0$.\n\nSubstituting these conditions into the momentum equation, we get:\n$$\n\\rho_{0} H (0) \\;=\\; - \\rho_{0} g H (0) \\;+\\; \\tau_{x} \\;-\\; \\rho_{0} H r U\n$$\nThis simplifies to a balance between the wind stress forcing and the bottom drag:\n$$\n0 \\;=\\; \\tau_{x} \\;-\\; \\rho_{0} H r U\n$$\nSolving for the steady-state velocity $U$ gives the explicit form of the observational operator, $h(\\boldsymbol{\\theta})$:\n$$\nU(\\tau_{x}, r) = \\frac{\\tau_{x}}{\\rho_{0} H r}\n$$\nThis relationship demonstrates that the steady-state velocity $U$ is determined not by $\\tau_{x}$ or $r$ individually, but by their ratio $\\frac{\\tau_{x}}{r}$. If a specific steady-state velocity $U_{obs}$ is observed, any parameter pair $(\\tau_{x}, r)$ that satisfies the equation\n$$\nU_{obs} = \\frac{\\tau_{x}}{\\rho_{0} H r} \\quad \\implies \\quad \\tau_{x} = (\\rho_{0} H U_{obs}) r\n$$\nis a valid solution. Since $\\rho_{0}$, $H$, and $U_{obs}$ are constants, this equation describes a line passing through the origin in the $(\\tau_{x}, r)$ parameter space. This constitutes a one-dimensional family of solutions, meaning that from a single observation of steady-state $U$, we cannot uniquely determine the two parameters $\\tau_{x}$ and $r$. This is a classic case of parameter confounding, or non-identifiability.\n\n**Part 2: Sensitivity (Jacobian) and Nullspace Direction**\n\nThe sensitivity of the observation $U$ to the parameters $\\boldsymbol{\\theta} = (\\tau_x, r)$ is given by the Jacobian of the observational operator $h(\\boldsymbol{\\theta}) = U(\\tau_{x}, r)$. The Jacobian is a row vector in this case:\n$$\n\\mathbf{J} = \\nabla_{\\boldsymbol{\\theta}} h = \\begin{pmatrix} \\frac{\\partial U}{\\partial \\tau_{x}}  \\frac{\\partial U}{\\partial r} \\end{pmatrix}\n$$\nWe compute the partial derivatives of $U(\\tau_{x}, r) = \\frac{\\tau_{x}}{\\rho_{0} H r}$:\n$$\n\\frac{\\partial U}{\\partial \\tau_{x}} = \\frac{\\partial}{\\partial \\tau_{x}} \\left( \\frac{\\tau_{x}}{\\rho_{0} H r} \\right) = \\frac{1}{\\rho_{0} H r}\n$$\n$$\n\\frac{\\partial U}{\\partial r} = \\frac{\\partial}{\\partial r} \\left( \\frac{\\tau_{x}}{\\rho_{0} H} r^{-1} \\right) = \\frac{\\tau_{x}}{\\rho_{0} H} (-1) r^{-2} = -\\frac{\\tau_{x}}{\\rho_{0} H r^{2}}\n$$\nThus, the Jacobian is:\n$$\n\\mathbf{J} = \\begin{pmatrix} \\frac{1}{\\rho_{0} H r}  -\\frac{\\tau_{x}}{\\rho_{0} H r^{2}} \\end{pmatrix}\n$$\nA parameter-increment direction $\\delta\\boldsymbol{\\theta} = (\\delta\\tau_x, \\delta r)$ lies in the nullspace of the Jacobian if its application results in zero change in the observation at first order. Mathematically, this is expressed as $\\mathbf{J} \\cdot \\delta\\boldsymbol{\\theta}^{T} = 0$:\n$$\n\\begin{pmatrix} \\frac{1}{\\rho_{0} H r}  -\\frac{\\tau_{x}}{\\rho_{0} H r^{2}} \\end{pmatrix} \\begin{pmatrix} \\delta\\tau_x \\\\ \\delta r \\end{pmatrix} = 0\n$$\n$$\n\\frac{1}{\\rho_{0} H r} \\delta\\tau_x - \\frac{\\tau_{x}}{\\rho_{0} H r^{2}} \\delta r = 0\n$$\nMultiplying the entire equation by $\\rho_{0} H r^{2}$ (assuming $r \\neq 0$) simplifies the relationship:\n$$\nr \\, \\delta\\tau_x - \\tau_{x} \\, \\delta r = 0 \\quad \\implies \\quad r \\, \\delta\\tau_x = \\tau_{x} \\, \\delta r\n$$\nA simple non-zero vector that satisfies this relation is $\\delta\\boldsymbol{\\theta} = (\\tau_{x}, r)$, since $r(\\tau_{x}) - \\tau_{x}(r) = 0$. This vector, let's call it $\\mathbf{v} = (\\tau_{x}, r)$, spans the one-dimensional nullspace.\n\nTo find the unit-norm nullspace direction, we normalize this vector by its Euclidean norm $\\|\\mathbf{v}\\| = \\sqrt{\\tau_{x}^{2} + r^{2}}$. The normalized direction $\\hat{\\mathbf{v}}$ is:\n$$\n\\hat{\\mathbf{v}} = \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|} = \\left( \\frac{\\tau_{x}}{\\sqrt{\\tau_{x}^{2} + r^{2}}}, \\frac{r}{\\sqrt{\\tau_{x}^{2} + r^{2}}} \\right)\n$$\nThis is the required unit-norm nullspace direction vector.\n\n**Part 3: Breaking the Degeneracy**\n\nTo break the degeneracy and enable unique identification of both $\\tau_{x}$ and $r$, we need an additional, independent observation that depends on the parameters in a different way. The current problem arises because we only observe the steady state. Observing the system's transient (time-dependent) evolution provides the necessary independent information.\n\nLet's propose an experiment where we observe the spin-up of the channel from a state of rest, i.e., $U(t=0)=0$. The governing equation for the uniform mode is a first-order ordinary differential equation:\n$$\n\\rho_{0} H \\frac{dU}{dt} = \\tau_{x} - \\rho_{0} H r U \\quad \\implies \\quad \\frac{dU}{dt} + rU = \\frac{\\tau_{x}}{\\rho_{0} H}\n$$\nThe solution to this ODE with the initial condition $U(0)=0$ is:\n$$\nU(t) = \\frac{\\tau_{x}}{\\rho_{0} H r} \\left( 1 - \\exp(-rt) \\right)\n$$\nThis solution reveals that the system evolves towards its steady state with an e-folding time scale of $T_{efold} = \\frac{1}{r}$. This time scale depends only on $r$, not on $\\tau_{x}$.\n\nAn experimental design that could break the degeneracy would be to measure the velocity $U(t)$ at a time $t0$ during the transient phase, in addition to the steady-state velocity $U_{ss} = \\lim_{t\\to\\infty} U(t)$. For instance, we can form an augmented observation vector $\\mathbf{y}$ consisting of two independent pieces of information: the steady-state velocity $U_{ss}$ and the adjustment timescale $T_{efold}$.\n\nThe new observation operator $\\mathbf{h}(\\boldsymbol{\\theta})$ would be:\n$$\n\\mathbf{h}(\\boldsymbol{\\theta}) = \\begin{pmatrix} h_{1}(\\tau_x, r) \\\\ h_{2}(\\tau_x, r) \\end{pmatrix} = \\begin{pmatrix} U_{ss} \\\\ T_{efold} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\tau_{x}}{\\rho_{0} H r} \\\\ \\frac{1}{r} \\end{pmatrix}\n$$\nThe new sensitivity matrix (Jacobian) is a $2 \\times 2$ matrix:\n$$\n\\mathbf{J}_{\\text{new}} = \\begin{pmatrix} \\frac{\\partial h_{1}}{\\partial \\tau_{x}}  \\frac{\\partial h_{1}}{\\partial r} \\\\ \\frac{\\partial h_{2}}{\\partial \\tau_{x}}  \\frac{\\partial h_{2}}{\\partial r} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\rho_{0} H r}  -\\frac{\\tau_{x}}{\\rho_{0} H r^{2}} \\\\ 0  -\\frac{1}{r^{2}} \\end{pmatrix}\n$$\nFor the parameters to be locally identifiable, this Jacobian matrix must have full rank (i.e., rank $2$), which can be verified by checking if its determinant is non-zero.\n$$\n\\det(\\mathbf{J}_{\\text{new}}) = \\left( \\frac{1}{\\rho_{0} H r} \\right) \\left( -\\frac{1}{r^{2}} \\right) - \\left( -\\frac{\\tau_{x}}{\\rho_{0} H r^{2}} \\right) (0) = -\\frac{1}{\\rho_{0} H r^{3}}\n$$\nUnder physically plausible oceanographic conditions, $\\rho_{0}  0$, $H  0$, and the drag coefficient $r  0$. Therefore, $\\det(\\mathbf{J}_{\\text{new}})$ is non-zero. A non-zero determinant implies that the matrix is invertible and has full rank. This proves that with the addition of an observation related to the transient dynamics (such as the adjustment timescale), the parameters $\\tau_{x}$ and $r$ become locally identifiable.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\tau_x}{\\sqrt{\\tau_x^2 + r^2}}  \\frac{r}{\\sqrt{\\tau_x^2 + r^2}}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}