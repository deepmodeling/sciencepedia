{
    "hands_on_practices": [
        {
            "introduction": "This first exercise is the foundational building block for understanding data assimilation. By working through a simple scalar (one-dimensional) case, you will derive the core Kalman filter equations directly from Bayes' theorem. This practice illuminates how a prior forecast is optimally combined with a new observation to yield a more accurate analysis, revealing the fundamental roles of the Kalman gain and the reduction of uncertainty. ",
            "id": "3795500",
            "problem": "In a single-grid-point setting that represents a nondimensionalized sea surface temperature anomaly in a data assimilation cycle for a one-step persistence model, suppose the prognostic state is scalar and the dynamics are linear and deterministic over the cycle. The prior (also called forecast or background) distribution of the state is Gaussian with mean $x^{f}$ and variance $P^{f}$. A single, collocated observation is taken that is linearly related to the state with observation operator $H$ and independent Gaussian observational error with variance $R$. Assume all variables are nondimensional, and that the prior and likelihood are independent and jointly Gaussian. Using Bayes' theorem for linear-Gaussian models and completing the square, derive the posterior (also called analysis) Gaussian distribution and explicitly identify the Kalman gain, the analysis mean, and the analysis variance in terms of $x^{f}$, $P^{f}$, $H$, $R$, and the observation $y$.\n\nThen evaluate your expressions for the specific case $F=\\begin{bmatrix}1\\end{bmatrix}$, $H=\\begin{bmatrix}1\\end{bmatrix}$, $P^{f}=2$, $R=1$, $x^{f}=0$, and $y=1$. Provide the final numerical values as exact rational numbers.\n\nExpress your final answer as a row of three entries, in the order: Kalman gain, analysis mean, analysis variance. No units are required. Do not round; provide exact values.",
            "solution": "The problem asks for the derivation of the posterior (analysis) distribution for a scalar state in a linear-Gaussian data assimilation setting, and then for the evaluation of the key quantities for a specific case. The analysis will be performed by applying Bayes' theorem and completing the square, as requested.\n\n### Step 1: Problem Validation\n\nThe problem is first validated against the required criteria.\n\n**1.1. Extracted Givens:**\n- The prognostic state, $x$, is a scalar.\n- The prior distribution of the state is Gaussian with mean $x^f$ and variance $P^f$.\n- The observation, $y$, is a single, collocated measurement.\n- The observation operator, $H$, is linear.\n- The observational error is from an independent Gaussian distribution with zero mean and variance $R$.\n- The prior and likelihood are independent and jointly Gaussian.\n- The task is to use Bayes' theorem and the method of completing the square to derive the posterior (analysis) distribution.\n- Identify the Kalman gain ($K$), analysis mean ($x^a$), and analysis variance ($P^a$).\n- Evaluate these quantities for the case: $F=\\begin{bmatrix}1\\end{bmatrix}$, $H=\\begin{bmatrix}1\\end{bmatrix}$, $P^{f}=2$, $R=1$, $x^{f}=0$, and $y=1$.\n\n**1.2. Validation:**\n- **Scientific Grounding:** The problem is a standard, fundamental exercise in data assimilation theory, specifically the derivation of the Kalman filter analysis step. It is based on established principles of Bayesian statistics and probability theory. It is scientifically sound.\n- **Well-Posedness:** The problem provides all necessary information to derive the general expressions and to compute the specific numerical results. The state and observation models are clearly defined. The mention of the model operator $F$ provides context about the data assimilation cycle (a persistence forecast) but is not required for the analysis calculation itself, as the forecast state $x^f$ is given. This does not constitute a contradiction or an underspecified setup.\n- **Objectivity:** The problem is stated in precise, objective mathematical language.\n\n**1.3. Verdict:**\nThe problem is valid as it is scientifically grounded, well-posed, and objective. There are no flaws that would prevent a rigorous solution. We may proceed.\n\n### Step 2: Derivation of the Analysis Distribution\n\nAccording to Bayes' theorem, the posterior probability density function (PDF), $p(x|y)$, is proportional to the product of the likelihood PDF, $p(y|x)$, and the prior PDF, $p(x)$.\n$$p(x|y) \\propto p(y|x) p(x)$$\nGiven the problem statement, both the prior and the likelihood are Gaussian. For a scalar state $x$, the PDFs are:\n\nThe prior distribution is $x \\sim \\mathcal{N}(x^f, P^f)$:\n$$p(x) \\propto \\exp\\left( -\\frac{1}{2} \\frac{(x - x^f)^2}{P^f} \\right)$$\nThe likelihood, based on the observation model $y = Hx + \\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0, R)$, is:\n$$p(y|x) \\propto \\exp\\left( -\\frac{1}{2} \\frac{(y - Hx)^2}{R} \\right)$$\n\nThe posterior PDF is then the product of these two:\n$$p(x|y) \\propto \\exp\\left( -\\frac{1}{2} \\frac{(x - x^f)^2}{P^f} \\right) \\exp\\left( -\\frac{1}{2} \\frac{(y - Hx)^2}{R} \\right)$$\n$$p(x|y) \\propto \\exp\\left( -\\frac{1}{2} \\left[ \\frac{(x - x^f)^2}{P^f} + \\frac{(y - Hx)^2}{R} \\right] \\right)$$\nThe product of two Gaussian distributions is another Gaussian distribution. Therefore, the posterior distribution for $x$ will be Gaussian, say $x \\sim \\mathcal{N}(x^a, P^a)$, with its PDF having the form:\n$$p(x|y) \\propto \\exp\\left( -\\frac{1}{2} \\frac{(x - x^a)^2}{P^a} \\right)$$\nTo find the analysis mean $x^a$ and variance $P^a$, we equate the arguments of the exponential functions. Let $J(x)$ be the cost function in the exponent of the posterior PDF:\n$$J(x) = \\frac{(x - x^a)^2}{P^a} + \\text{const.} = \\frac{(x - x^f)^2}{P^f} + \\frac{(y - Hx)^2}{R}$$\nWe now expand the terms on the right-hand side and collect terms based on powers of $x$:\n$$J(x) = \\frac{1}{P^f}(x^2 - 2xx^f + (x^f)^2) + \\frac{1}{R}(y^2 - 2yHx + H^2x^2)$$\n$$J(x) = x^2 \\left(\\frac{1}{P^f} + \\frac{H^2}{R}\\right) - 2x \\left(\\frac{x^f}{P^f} + \\frac{yH}{R}\\right) + \\left(\\frac{(x^f)^2}{P^f} + \\frac{y^2}{R}\\right)$$\nThis quadratic in $x$ must be equivalent to the form $\\frac{1}{P^a}(x^2 - 2xx^a + (x^a)^2) + \\text{const.}$. By comparing the coefficients of the $x^2$ and $x$ terms (\"completing the square\"), we can identify $P^a$ and $x^a$.\n\nComparing the coefficients of the $x^2$ term:\n$$\\frac{1}{P^a} = \\frac{1}{P^f} + \\frac{H^2}{R}$$\nThis equation shows that the precision (inverse variance) of the analysis is the sum of the precisions of the forecast and the observation. Solving for the analysis variance $P^a$:\n$$P^a = \\left( \\frac{R + H^2 P^f}{P^f R} \\right)^{-1} = \\frac{P^f R}{R + H^2 P^f}$$\n\nComparing the coefficients of the $-2x$ term:\n$$\\frac{x^a}{P^a} = \\frac{x^f}{P^f} + \\frac{yH}{R}$$\nSolving for the analysis mean $x^a$:\n$$x^a = P^a \\left( \\frac{x^f}{P^f} + \\frac{yH}{R} \\right) = \\frac{P^f R}{R + H^2 P^f} \\left( \\frac{x^f R + yH P^f}{P^f R} \\right) = \\frac{x^f R + yH P^f}{R + H^2 P^f}$$\nTo express this in the standard form involving the Kalman gain, we manipulate the expression for $x^a$:\n$$x^a = \\frac{x^f(R + H^2 P^f) - x^f H^2 P^f + yH P^f}{R + H^2 P^f}$$\n$$x^a = x^f + \\frac{yH P^f - x^f H^2 P^f}{R + H^2 P^f} = x^f + \\left( \\frac{P^f H}{R + H^2 P^f} \\right) (y - Hx^f)$$\nThis is the canonical Kalman update equation, $x^a = x^f + K(y - Hx^f)$. From this, we identify the Kalman gain $K$:\n$$K = \\frac{P^f H}{H^2 P^f + R}$$\nThe analysis variance $P^a$ can also be written in terms of $K$. From our expression for $P^a$:\n$$P^a = \\frac{P^f R}{R + H^2 P^f} = P^f \\frac{R + H^2 P^f - H^2 P^f}{R + H^2 P^f} = P^f \\left( 1 - \\frac{H^2 P^f}{R + H^2 P^f} \\right)$$\n$$P^a = P^f \\left( 1 - H \\cdot \\frac{H P^f}{R + H^2 P^f} \\right) = P^f(1 - HK)$$\nSince all quantities are scalars, this is equivalent to $P^a = (1-KH)P^f$.\n\n### Step 3: Evaluation for the Specific Case\n\nWe are given the following values:\n- Forecast mean: $x^f = 0$\n- Forecast variance: $P^f = 2$\n- Observation operator: $H = 1$ (from $H=\\begin{bmatrix}1\\end{bmatrix}$)\n- Observation error variance: $R = 1$\n- Observation: $y = 1$\n\nWe now substitute these values into the derived expressions.\n\n**1. Kalman Gain ($K$):**\n$$K = \\frac{P^f H}{H^2 P^f + R} = \\frac{2 \\cdot 1}{1^2 \\cdot 2 + 1} = \\frac{2}{2 + 1} = \\frac{2}{3}$$\n\n**2. Analysis Mean ($x^a$):**\n$$x^a = x^f + K(y - Hx^f) = 0 + \\frac{2}{3}(1 - 1 \\cdot 0) = \\frac{2}{3}(1) = \\frac{2}{3}$$\n\n**3. Analysis Variance ($P^a$):**\nUsing the form $P^a = (1 - KH)P^f$:\n$$P^a = \\left( 1 - \\frac{2}{3} \\cdot 1 \\right) \\cdot 2 = \\left( 1 - \\frac{2}{3} \\right) \\cdot 2 = \\frac{1}{3} \\cdot 2 = \\frac{2}{3}$$\nAlternatively, using the direct formula:\n$$P^a = \\frac{P^f R}{R + H^2 P^f} = \\frac{2 \\cdot 1}{1 + 1^2 \\cdot 2} = \\frac{2}{1 + 2} = \\frac{2}{3}$$\nThe results are consistent.\n\nThe final values, expressed as exact rational numbers, are: Kalman gain $K = \\frac{2}{3}$, analysis mean $x^a = \\frac{2}{3}$, and analysis variance $P^a = \\frac{2}{3}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{2}{3} & \\frac{2}{3} & \\frac{2}{3} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Building on the scalar case, we now move to a more realistic scenario with a multidimensional state. This practice demonstrates one of the most powerful features of data assimilation: the ability to update unobserved state variables by observing related ones. You will see how the forecast error covariance matrix, $P^{f}$, encodes the relationships between state components, allowing the Kalman gain to intelligently spread the information from a single observation across the entire system. ",
            "id": "3795551",
            "problem": "In computational oceanography, consider a linear-Gaussian single-assimilation step for a two-dimensional state representing sea surface temperature anomalies at two nearby grid points in a regional ocean model. Let the state be denoted by the random vector $x \\in \\mathbb{R}^{2}$ with forecast (prior) mean $x^{f}$ and forecast covariance $P^{f} \\in \\mathbb{R}^{2 \\times 2}$. A single in situ observation $y \\in \\mathbb{R}$ measures the first state component through a linear observation operator $H \\in \\mathbb{R}^{1 \\times 2}$, with additive measurement noise $\\varepsilon$ that is independent of $x$ and satisfies $\\varepsilon \\sim \\mathcal{N}(0, R)$ where $R \\in \\mathbb{R}$ is the observation-error variance. The observation model is $y = H x + \\varepsilon$. Assume the joint distribution of $(x, y)$ is Gaussian, and consider the Best Linear Unbiased Estimator (BLUE), also known as the Kalman Filter (KF) analysis update, which yields an analysis state $x^{a}$ and analysis covariance $P^{a}$.\n\nStarting from the definitions of linear estimation in the Gaussian setting and the requirement that the estimator minimizes the analysis error variance subject to unbiasedness, derive the expression for the Kalman gain $K \\in \\mathbb{R}^{2 \\times 1}$ and the analysis covariance $P^{a} \\in \\mathbb{R}^{2 \\times 2}$ in terms of $P^{f}$, $H$, and $R$. Then, for the specific case\n$$\nP^{f} = \\begin{bmatrix} 1 & 0.5 \\\\ 0.5 & 1 \\end{bmatrix}, \\quad\nH = \\begin{bmatrix} 1 & 0 \\end{bmatrix}, \\quad\nR = 0.25,\n$$\ncompute the numerical values of $K$ and $P^{a}$ exactly.\n\nExpress your final answer as a single row matrix using the following ordering: list the two entries of $K$ (top to bottom), followed by the four entries of $P^{a}$ in row-major order (first row left to right, then second row left to right). No rounding is required, and no physical units are involved. The final answer must be a calculation.",
            "solution": "The problem asks for the derivation of the Kalman gain $K$ and the analysis covariance $P^{a}$ for a linear-Gaussian estimation problem, followed by a numerical calculation for a specific case.\n\nThe analysis state $x^{a}$ is a linear estimator, which combines the forecast (prior) state $x^{f}$ and the new observation $y$. The general form of a linear estimator is:\n$$\nx^{a} = A x^{f} + B y\n$$\nwhere $A$ and $B$ are matrices. For the standard Kalman filter, we seek an update to the forecast based on the new information from the observation. This is expressed as correcting the forecast with the \"innovation\" or \"misfit\" $(y - Hx^{f})$, weighted by a gain matrix $K$.\n$$\nx^{a} = x^{f} + K(y - Hx^{f})\n$$\nThis form can be rewritten as $x^{a} = (I - KH)x^{f} + Ky$, which corresponds to the general linear form with $A=I-KH$ and $B=K$.\n\nThe estimator must be unbiased and minimize the analysis error variance.\n\n**1. Unbiasedness Condition**\nThe analysis error is defined as $e^{a} = x - x^{a}$. An unbiased estimator requires that the expected value of the analysis error is zero, i.e., $E[e^{a}] = 0$.\nSubstituting the expression for $x^{a}$ and the observation model $y = Hx + \\varepsilon$:\n$$\ne^{a} = x - [x^{f} + K(y - Hx^{f})]\n$$\n$$\ne^{a} = x - x^{f} - K(Hx + \\varepsilon - Hx^{f})\n$$\nLet the forecast error be $e^{f} = x - x^{f}$. Then:\n$$\ne^{a} = e^{f} - K(H(x-x^{f}) + \\varepsilon)\n$$\n$$\ne^{a} = e^{f} - K(He^{f} + \\varepsilon)\n$$\n$$\ne^{a} = (I - KH)e^{f} - K\\varepsilon\n$$\nThe expected value of the analysis error is:\n$$\nE[e^{a}] = E[(I - KH)e^{f} - K\\varepsilon] = (I - KH)E[e^{f}] - KE[\\varepsilon]\n$$\nThe forecast is assumed to be unbiased, so $E[x^{f}] = E[x]$ which implies $E[e^{f}] = E[x - x^{f}] = 0$. The measurement noise is also assumed to have zero mean, $E[\\varepsilon] = 0$. Therefore:\n$$\nE[e^{a}] = (I - KH)(0) - K(0) = 0\n$$\nThe chosen form of the estimator is inherently unbiased, regardless of the choice of $K$.\n\n**2. Minimization of Analysis Error Variance**\nThe analysis error covariance matrix is defined as $P^{a} = E[e^{a}(e^{a})^T]$. We aim to find the Kalman gain $K$ that minimizes a scalar measure of this matrix, typically its trace, which represents the total variance of the state estimate.\n$$\nP^{a} = E[((I - KH)e^{f} - K\\varepsilon)((I - KH)e^{f} - K\\varepsilon)^T]\n$$\nExpanding the transpose:\n$$\nP^{a} = E[((I - KH)e^{f} - K\\varepsilon)((e^{f})^T(I - KH)^T - \\varepsilon^T K^T)]\n$$\n$$\nP^{a} = E[(I - KH)e^{f}(e^{f})^T(I - KH)^T - (I - KH)e^{f}\\varepsilon^T K^T - K\\varepsilon(e^{f})^T(I - KH)^T + K\\varepsilon\\varepsilon^T K^T]\n$$\nTaking the expectation term by term and using the fact that the forecast error $e^f$ and observation noise $\\varepsilon$ are uncorrelated, i.e., $E[e^{f}\\varepsilon^T] = 0$ and $E[\\varepsilon(e^{f})^T] = 0$:\n$$\nP^{a} = (I - KH)E[e^{f}(e^{f})^T](I - KH)^T + K E[\\varepsilon\\varepsilon^T] K^T\n$$\nBy definition, the forecast error covariance is $P^{f} = E[e^{f}(e^{f})^T]$ and the observation error covariance is $R = E[\\varepsilon\\varepsilon^T]$ (since $y$ is a scalar, $R$ is a scalar variance).\n$$\nP^{a} = (I - KH)P^{f}(I - KH)^T + KRK^T\n$$\nTo minimize the analysis error variance, we minimize the trace of $P^{a}$, $J(K) = \\text{tr}(P^{a})$.\n$$\nJ(K) = \\text{tr}((I - KH)P^{f}(I - KH)^T + KRK^T)\n$$\nExpanding the first term:\n$$\n(I - KH)P^{f}(I - KH)^T = (P^{f} - KHP^{f})(I - H^T K^T) = P^{f} - P^{f}H^T K^T - KHP^{f} + KHP^{f}H^T K^T\n$$\nSo,\n$$\nJ(K) = \\text{tr}(P^{f}) - \\text{tr}(P^{f}H^T K^T) - \\text{tr}(KHP^{f}) + \\text{tr}(KHP^{f}H^T K^T) + \\text{tr}(KRK^T)\n$$\nUsing the cyclic property of the trace, $\\text{tr}(ABC) = \\text{tr}(CAB)$, we have $\\text{tr}(P^{f}H^T K^T) = \\text{tr}(K^T P^{f}H^T)$. Since the trace of a matrix is equal to the trace of its transpose, $\\text{tr}(K^T P^{f}H^T) = \\text{tr}((P^{f}H^T)^T K) = \\text{tr}(HP^{f}K)$. But $P^f$ is symmetric ($P^f = (P^f)^T$), so this is $\\text{tr}(HP^f K)$. Also, $\\text{tr}(KHP^f)$ is a scalar, so it equals its transpose $\\text{tr}((KHP^f)^T) = \\text{tr}((P^f)^T H^T K^T) = \\text{tr}(P^f H^T K^T)$. A more direct way is to notice that $\\text{tr}(KHP^f) = \\text{tr}(P^f H^T K^T)$ for real matrices.\nThus, $\\text{tr}(P^{f}H^T K^T) = \\text{tr}(KHP^{f})$.\n$$\nJ(K) = \\text{tr}(P^{f}) - 2 \\text{tr}(KHP^{f}) + \\text{tr}(K(HP^{f}H^T + R)K^T)\n$$\nTo find the minimum, we differentiate $J(K)$ with respect to $K$ and set the result to zero. Using standard matrix derivative identities $\\frac{d}{dX}\\text{tr}(AXB) = A^T B^T$ and $\\frac{d}{dX}\\text{tr}(AXA^T) = 2AX$:\n$$\n\\frac{d J(K)}{dK} = -2\\frac{d}{dK}\\text{tr}(KHP^{f}) + \\frac{d}{dK}\\text{tr}(K(HP^{f}H^T + R)K^T) = 0\n$$\n$$\n-2(HP^{f})^T + 2K(HP^{f}H^T + R) = 0\n$$\n$$\n-2 P^{f}H^T + 2K(HP^{f}H^T + R) = 0\n$$\n$$\nK(HP^{f}H^T + R) = P^{f}H^T\n$$\nSolving for $K$, we get the expression for the optimal Kalman gain:\n$$\nK = P^{f}H^T(HP^{f}H^T + R)^{-1}\n$$\nNow we derive the simplified expression for the analysis covariance $P^{a}$. Starting from $P^{a} = (I - KH)P^{f} - (I - KH)P^{f}H^T K^T + KRK^T$:\n$$\nP^{a} = P^{f} - KHP^{f} - P^{f}H^T K^T + KHP^{f}H^T K^T + KRK^T\n$$\n$$\nP^{a} = P^{f} - KHP^{f} - P^{f}H^T K^T + K(HP^{f}H^T + R)K^T\n$$\nFrom the derivation of $K$, we have $K(HP^{f}H^T + R) = P^{f}H^T$. Substituting this into the last term:\n$$\nP^{a} = P^{f} - KHP^{f} - P^{f}H^T K^T + (P^{f}H^T)K^T\n$$\nThe last two terms cancel, leaving:\n$$\nP^{a} = P^{f} - KHP^{f}\n$$\nSince $KHP^f$ is not necessarily symmetric, a more robust form is $P^a = (I - KH)P^f$. This form preserves the symmetry of $P^a$ if $P^f$ is symmetric, which we will see in the calculation.\n\n**3. Numerical Calculation**\nWe are given:\n$$\nP^{f} = \\begin{bmatrix} 1 & 0.5 \\\\ 0.5 & 1 \\end{bmatrix}, \\quad\nH = \\begin{bmatrix} 1 & 0 \\end{bmatrix}, \\quad\nR = 0.25 = \\frac{1}{4}\n$$\nFirst, calculate the terms needed for the Kalman gain $K$.\nThe transpose of $H$ is $H^T = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\nThe term $P^{f}H^T$ is:\n$$\nP^{f}H^T = \\begin{bmatrix} 1 & 0.5 \\\\ 0.5 & 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1 \\cdot 1 + 0.5 \\cdot 0 \\\\ 0.5 \\cdot 1 + 1 \\cdot 0 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0.5 \\end{bmatrix}\n$$\nThe term $HP^{f}H^T$ is:\n$$\nHP^{f}H^T = \\begin{bmatrix} 1 & 0 \\end{bmatrix} \\left( \\begin{bmatrix} 1 & 0.5 \\\\ 0.5 & 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\right) = \\begin{bmatrix} 1 & 0 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0.5 \\end{bmatrix} = 1 \\cdot 1 + 0 \\cdot 0.5 = 1\n$$\nThe innovation covariance is $S = HP^{f}H^T + R$:\n$$\nS = 1 + 0.25 = 1.25 = \\frac{5}{4}\n$$\nSince this is a scalar, its inverse is $S^{-1} = \\frac{1}{1.25} = \\frac{4}{5} = 0.8$.\nNow, calculate the Kalman gain $K = P^{f}H^T S^{-1}$:\n$$\nK = \\begin{bmatrix} 1 \\\\ 0.5 \\end{bmatrix} \\cdot 0.8 = \\begin{bmatrix} 0.8 \\\\ 0.4 \\end{bmatrix} = \\begin{bmatrix} \\frac{4}{5} \\\\ \\frac{2}{5} \\end{bmatrix}\n$$\nNext, calculate the analysis covariance $P^{a}$ using $P^{a} = (I-KH)P^{f}$.\nFirst, compute the matrix $KH$:\n$$\nKH = \\begin{bmatrix} 0.8 \\\\ 0.4 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\end{bmatrix} = \\begin{bmatrix} 0.8 \\cdot 1 & 0.8 \\cdot 0 \\\\ 0.4 \\cdot 1 & 0.4 \\cdot 0 \\end{bmatrix} = \\begin{bmatrix} 0.8 & 0 \\\\ 0.4 & 0 \\end{bmatrix}\n$$\nThen, compute $I - KH$:\n$$\nI - KH = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} - \\begin{bmatrix} 0.8 & 0 \\\\ 0.4 & 0 \\end{bmatrix} = \\begin{bmatrix} 1 - 0.8 & 0 - 0 \\\\ 0 - 0.4 & 1 - 0 \\end{bmatrix} = \\begin{bmatrix} 0.2 & 0 \\\\ -0.4 & 1 \\end{bmatrix}\n$$\nFinally, compute $P^{a} = (I - KH)P^{f}$:\n$$\nP^{a} = \\begin{bmatrix} 0.2 & 0 \\\\ -0.4 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0.5 \\\\ 0.5 & 1 \\end{bmatrix}\n$$\n$$\nP^{a} = \\begin{bmatrix} (0.2)(1) + (0)(0.5) & (0.2)(0.5) + (0)(1) \\\\ (-0.4)(1) + (1)(0.5) & (-0.4)(0.5) + (1)(1) \\end{bmatrix}\n$$\n$$\nP^{a} = \\begin{bmatrix} 0.2 & 0.1 \\\\ 0.1 & -0.2 + 1 \\end{bmatrix} = \\begin{bmatrix} 0.2 & 0.1 \\\\ 0.1 & 0.8 \\end{bmatrix}\n$$\nLet's express these results as exact fractions:\n$K = \\begin{bmatrix} 4/5 \\\\ 2/5 \\end{bmatrix}$\n$P^{a} = \\begin{bmatrix} 1/5 & 1/10 \\\\ 1/10 & 4/5 \\end{bmatrix}$\n\nThe problem requires a single row matrix of the entries of $K$ (top to bottom) followed by the entries of $P^{a}$ (row-major order).\nThe entries are: $K_1 = \\frac{4}{5}$, $K_2 = \\frac{2}{5}$, $P^{a}_{11} = \\frac{1}{5}$, $P^{a}_{12} = \\frac{1}{10}$, $P^{a}_{21} = \\frac{1}{10}$, $P^{a}_{22} = \\frac{4}{5}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{4}{5} & \\frac{2}{5} & \\frac{1}{5} & \\frac{1}{10} & \\frac{1}{10} & \\frac{4}{5} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Data assimilation methods can appear in different mathematical forms, most notably as sequential filters or as variational (optimization) problems. This advanced exercise connects these two major perspectives by showing their fundamental equivalence under linear and Gaussian assumptions. By deriving the analysis from both a 3D-Var cost function and the Kalman filter equations, you will prove that they are two sides of the same Bayesian coin, yielding the exact same optimal estimate. ",
            "id": "3795564",
            "problem": "Consider a single analysis step in computational oceanography for a two-dimensional linear state whose components represent two prognostic variables at a single horizontal grid point. Let the unknown true state be $x \\in \\mathbb{R}^{2}$ and suppose the background (prior) state $x_b \\in \\mathbb{R}^{2}$ and its error covariance $B \\in \\mathbb{R}^{2 \\times 2}$ are given, and the observation $y \\in \\mathbb{R}$ is related to the state by a known linear operator $H \\in \\mathbb{R}^{1 \\times 2}$ with observational error covariance $R \\in \\mathbb{R}^{1 \\times 1}$. Assume the prior and observational errors are independent, unbiased, and Gaussian.\n\nStarting only from Bayesâ€™ theorem and the assumptions of linearity and Gaussian error statistics, derive the analysis state that minimizes the negative log of the posterior density, which is equivalent to the Three-Dimensional Variational data assimilation (3D-Var) analysis. Independently, starting from the definition of the optimal linear estimator under Gaussian errors, derive the single-step Kalman Filter (KF) analysis for this same setup. Then, for the specific case\n$$\nx_b = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}, \\quad\nB = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad\nH = \\begin{pmatrix} 1 & 1 \\end{pmatrix}, \\quad\nR = \\begin{pmatrix} 1 \\end{pmatrix}, \\quad\ny = 0.5,\n$$\ncompute both the 3D-Var analysis and the single-step KF analysis explicitly, and report the Euclidean norm of the difference between these two analysis states.\n\nExpress the final answer as a real number with no units. No rounding is required.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of data assimilation, is well-posed with a unique and meaningful solution, and is expressed in objective, formal language. It is a standard problem in computational oceanography and statistical estimation.\n\nThe problem requires two independent derivations for an analysis state $x_a \\in \\mathbb{R}^2$ and a final numerical calculation. First, we derive the Three-Dimensional Variational (3D-Var) analysis. Second, we derive the single-step Kalman Filter (KF) analysis. Finally, we show that for the linear and Gaussian case, these two methods are equivalent, which directly implies the final answer.\n\n**Part 1: Derivation of the 3D-Var Analysis**\n\nThe 3D-Var analysis seeks the state $x$ that maximizes the posterior probability density function (PDF), $p(x|y)$. According to Bayes' theorem, the posterior is proportional to the product of the likelihood and the prior:\n$$p(x|y) \\propto p(y|x) p(x)$$\nThe problem states that the prior and observational errors are unbiased and Gaussian.\n\nThe prior information is the background state $x_b$ with error covariance $B$. The prior PDF, $p(x)$, is therefore a Gaussian distribution centered at $x_b$ with covariance $B$:\n$$p(x) = \\mathcal{N}(x_b, B) \\propto \\exp\\left(-\\frac{1}{2}(x - x_b)^T B^{-1} (x - x_b)\\right)$$\nThe observation $y$ is related to the true state $x$ by the linear operator $H$, with observational error $\\epsilon_o$ having zero mean and covariance $R$. Thus, $y = Hx + \\epsilon_o$. The likelihood of observing $y$ given a state $x$ is also a Gaussian distribution, $p(y|x)$, centered at $Hx$ with covariance $R$:\n$$p(y|x) = \\mathcal{N}(Hx, R) \\propto \\exp\\left(-\\frac{1}{2}(y - Hx)^T R^{-1} (y - Hx)\\right)$$\nCombining these, the posterior PDF is:\n$$p(x|y) \\propto \\exp\\left(-\\frac{1}{2}\\left[ (x - x_b)^T B^{-1} (x - x_b) + (y - Hx)^T R^{-1} (y - Hx) \\right]\\right)$$\nMaximizing the posterior PDF $p(x|y)$ is equivalent to minimizing its negative logarithm. We define the 3D-Var cost function $J(x)$ as twice the negative exponent:\n$$J(x) = (x - x_b)^T B^{-1} (x - x_b) + (y - Hx)^T R^{-1} (y - Hx)$$\nTo find the state $x_a$ that minimizes $J(x)$, we compute the gradient of $J(x)$ with respect to $x$ and set it to zero. Using matrix calculus identities, the gradient is:\n$$\\nabla_x J(x) = 2B^{-1}(x - x_b) - 2H^T R^{-1}(y - Hx)$$\nSetting $\\nabla_x J(x) = 0$ at $x = x_a$:\n$$B^{-1}(x_a - x_b) - H^T R^{-1}(y - Hx_a) = 0$$\nRearranging the terms to solve for $x_a$:\n$$B^{-1}x_a - B^{-1}x_b - H^T R^{-1}y + H^T R^{-1}Hx_a = 0$$\n$$(B^{-1} + H^T R^{-1}H)x_a = B^{-1}x_b + H^T R^{-1}y$$\nThe 3D-Var analysis state is therefore:\n$$x_{a, \\text{3DVar}} = (B^{-1} + H^T R^{-1}H)^{-1} (B^{-1}x_b + H^T R^{-1}y)$$\n\n**Part 2: Derivation of the Kalman Filter Analysis**\n\nThe Kalman Filter analysis update provides the optimal linear unbiased estimate of the state. The analysis state $x_a$ is expressed as a linear correction to the background state $x_b$ based on the innovation, which is the difference between the observation $y$ and its background estimate $Hx_b$:\n$$x_a = x_b + K(y - Hx_b)$$\nHere, $K$ is the Kalman gain matrix, which is chosen to minimize the variance of the analysis error, i.e., minimize the trace of the analysis error covariance matrix $P_a = E[(x_a-x)(x_a-x)^T]$.\n\nThe analysis error $x_a-x$ can be written as:\n$$x_a - x = x_b + K(y - Hx_b) - x = x_b + K(Hx + \\epsilon_o - Hx_b) - x$$\nwhere $\\epsilon_o$ is the observational error. Let $\\epsilon_b = x_b - x$ be the background error.\n$$x_a - x = (x_b - x) + K(H(x-x_b) + \\epsilon_o) = \\epsilon_b - KH\\epsilon_b + K\\epsilon_o = (I - KH)\\epsilon_b + K\\epsilon_o$$\nNow, we compute the analysis error covariance $P_a$. Since the background error $\\epsilon_b$ and observation error $\\epsilon_o$ are independent, $E[\\epsilon_b \\epsilon_o^T] = 0$ and $E[\\epsilon_o \\epsilon_b^T] = 0$.\n$$P_a = E[((I - KH)\\epsilon_b + K\\epsilon_o)((I - KH)\\epsilon_b + K\\epsilon_o)^T]$$\n$$P_a = (I - KH) E[\\epsilon_b\\epsilon_b^T] (I - KH)^T + K E[\\epsilon_o\\epsilon_o^T] K^T$$\nUsing $E[\\epsilon_b\\epsilon_b^T] = B$ and $E[\\epsilon_o\\epsilon_o^T] = R$:\n$$P_a = (I - KH)B(I - KH)^T + KRK^T$$\n$$P_a = B - KHB - BH^T K^T + KHBH^TK^T + KRK^T$$\nTo minimize the trace of $P_a$ with respect to $K$, we set its derivative to zero:\n$$\\frac{\\partial}{\\partial K} \\mathrm{tr}(P_a) = -2BH^T + 2K(HBH^T + R) = 0$$\nSolving for $K$ yields the optimal Kalman gain:\n$$K = BH^T(HBH^T + R)^{-1}$$\nThe Kalman Filter analysis state is thus:\n$$x_{a, \\text{KF}} = x_b + BH^T(HBH^T + R)^{-1}(y - Hx_b)$$\n\n**Equivalence and Final Calculation**\n\nFor a linear system with Gaussian errors, the 3D-Var analysis and the Kalman Filter analysis are identical. This can be shown by applying the Woodbury matrix identity to the 3D-Var analysis expression. The identity states $(A+UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1}+VA^{-1}U)^{-1}VA^{-1}$. Let $A = B^{-1}$, $U=H^T$, $C=R^{-1}$, and $V=H$.\n$$(B^{-1} + H^T R^{-1}H)^{-1} = B - BH^T(R + HBH^T)^{-1}HB$$\nSubstituting this into the 3D-Var formula for $x_a$:\n$$x_{a, \\text{3DVar}} = (B - BH^T(HBH^T + R)^{-1}HB)(B^{-1}x_b + H^T R^{-1}y)$$\n$$x_{a, \\text{3DVar}} = x_b + BH^T R^{-1}y - BH^T(HBH^T + R)^{-1}HB B^{-1}x_b - BH^T(HBH^T + R)^{-1}HBH^TR^{-1}y$$\n$$x_{a, \\text{3DVar}} = x_b - BH^T(HBH^T + R)^{-1}Hx_b + BH^T[I - (HBH^T + R)^{-1}HBH^T]R^{-1}y$$\nThe term in the square brackets simplifies: $I - (HBH^T+R)^{-1}HBH^T = (HBH^T+R)^{-1}[(HBH^T+R) - HBH^T] = (HBH^T+R)^{-1}R$.\n$$x_{a, \\text{3DVar}} = x_b - BH^T(HBH^T + R)^{-1}Hx_b + BH^T(HBH^T + R)^{-1}R R^{-1}y$$\n$$x_{a, \\text{3DVar}} = x_b + BH^T(HBH^T + R)^{-1}(y - Hx_b) = x_{a, \\text{KF}}$$\nSince the two methods produce analytically identical results, the difference between their numerical outputs must be a zero vector, and the Euclidean norm of this difference must be zero.\n\nFor completeness, we compute the analysis state for the given numerical values using the Kalman Filter formulation.\nGiven:\n$$\nx_b = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}, \\quad\nB = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad\nH = \\begin{pmatrix} 1 & 1 \\end{pmatrix}, \\quad\nR = 1, \\quad\ny = 0.5\n$$\nFirst, compute the components of the Kalman gain $K = BH^T(HBH^T + R)^{-1}$:\n$$H^T = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\n$$BH^T = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$$\n$$HBH^T = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 1(2)+1(1) = 3$$\nThe term $(HBH^T + R)^{-1}$ is a scalar:\n$$(3 + 1)^{-1} = 4^{-1} = \\frac{1}{4}$$\nThe Kalman gain is:\n$$K = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} \\frac{1}{4} = \\begin{pmatrix} 1/2 \\\\ 1/4 \\end{pmatrix}$$\nNext, compute the innovation term $y - Hx_b$:\n$$Hx_b = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = 1(1) + 1(-1) = 0$$\n$$y - Hx_b = 0.5 - 0 = 0.5 = \\frac{1}{2}$$\nFinally, compute the analysis state $x_a$:\n$$x_a = x_b + K(y - Hx_b) = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} + \\begin{pmatrix} 1/2 \\\\ 1/4 \\end{pmatrix} \\left(\\frac{1}{2}\\right) = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} + \\begin{pmatrix} 1/4 \\\\ 1/8 \\end{pmatrix} = \\begin{pmatrix} 5/4 \\\\ -7/8 \\end{pmatrix}$$\nBoth $x_{a, \\text{3DVar}}$ and $x_{a, \\text{KF}}$ are equal to $\\begin{pmatrix} 5/4 \\\\ -7/8 \\end{pmatrix}$.\nThe difference is $x_{a, \\text{3DVar}} - x_{a, \\text{KF}} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\nThe Euclidean norm of the difference is:\n$$\\left\\| \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\right\\|_2 = \\sqrt{0^2 + 0^2} = 0$$",
            "answer": "$$\\boxed{0}$$"
        }
    ]
}