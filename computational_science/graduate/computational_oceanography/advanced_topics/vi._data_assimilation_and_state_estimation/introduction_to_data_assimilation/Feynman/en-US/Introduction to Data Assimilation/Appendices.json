{
    "hands_on_practices": [
        {
            "introduction": "This first exercise is fundamental to understanding data assimilation from first principles. By considering the simplest possible scenario—a single scalar state and one observation—you will derive the core analysis equations directly from Bayes' theorem . This process of 'completing the square' makes the statistical fusion of a forecast and an observation explicit, providing a clear intuition for the role of the Kalman gain and how uncertainties are reduced.",
            "id": "3795500",
            "problem": "In a single-grid-point setting that represents a nondimensionalized sea surface temperature anomaly in a data assimilation cycle for a one-step persistence model, suppose the prognostic state is scalar and the dynamics are linear and deterministic over the cycle. The prior (also called forecast or background) distribution of the state is Gaussian with mean $x^{f}$ and variance $P^{f}$. A single, collocated observation is taken that is linearly related to the state with observation operator $H$ and independent Gaussian observational error with variance $R$. Assume all variables are nondimensional, and that the prior and likelihood are independent and jointly Gaussian. Using Bayes' theorem for linear-Gaussian models and completing the square, derive the posterior (also called analysis) Gaussian distribution and explicitly identify the Kalman gain, the analysis mean, and the analysis variance in terms of $x^{f}$, $P^{f}$, $H$, $R$, and the observation $y$.\n\nThen evaluate your expressions for the specific case $F=\\begin{bmatrix}1\\end{bmatrix}$, $H=\\begin{bmatrix}1\\end{bmatrix}$, $P^{f}=2$, $R=1$, $x^{f}=0$, and $y=1$. Provide the final numerical values as exact rational numbers.\n\nExpress your final answer as a row of three entries, in the order: Kalman gain, analysis mean, analysis variance. No units are required. Do not round; provide exact values.",
            "solution": "The problem asks for the derivation of the posterior (analysis) distribution for a scalar state in a linear-Gaussian data assimilation setting, and then for the evaluation of the key quantities for a specific case. The analysis will be performed by applying Bayes' theorem and completing the square, as requested.\n\n### Step 1: Problem Validation\n\nThe problem is first validated against the required criteria.\n\n**1.1. Extracted Givens:**\n- The prognostic state, $x$, is a scalar.\n- The prior distribution of the state is Gaussian with mean $x^f$ and variance $P^f$.\n- The observation, $y$, is a single, collocated measurement.\n- The observation operator, $H$, is linear.\n- The observational error is from an independent Gaussian distribution with zero mean and variance $R$.\n- The prior and likelihood are independent and jointly Gaussian.\n- The task is to use Bayes' theorem and the method of completing the square to derive the posterior (analysis) distribution.\n- Identify the Kalman gain ($K$), analysis mean ($x^a$), and analysis variance ($P^a$).\n- Evaluate these quantities for the case: $F=\\begin{bmatrix}1\\end{bmatrix}$, $H=\\begin{bmatrix}1\\end{bmatrix}$, $P^{f}=2$, $R=1$, $x^{f}=0$, and $y=1$.\n\n**1.2. Validation:**\n- **Scientific Grounding:** The problem is a standard, fundamental exercise in data assimilation theory, specifically the derivation of the Kalman filter analysis step. It is based on established principles of Bayesian statistics and probability theory. It is scientifically sound.\n- **Well-Posedness:** The problem provides all necessary information to derive the general expressions and to compute the specific numerical results. The state and observation models are clearly defined. The mention of the model operator $F$ provides context about the data assimilation cycle (a persistence forecast) but is not required for the analysis calculation itself, as the forecast state $x^f$ is given. This does not constitute a contradiction or an underspecified setup.\n- **Objectivity:** The problem is stated in precise, objective mathematical language.\n\n**1.3. Verdict:**\nThe problem is valid as it is scientifically grounded, well-posed, and objective. There are no flaws that would prevent a rigorous solution. We may proceed.\n\n### Step 2: Derivation of the Analysis Distribution\n\nAccording to Bayes' theorem, the posterior probability density function (PDF), $p(x|y)$, is proportional to the product of the likelihood PDF, $p(y|x)$, and the prior PDF, $p(x)$.\n$$p(x|y) \\propto p(y|x) p(x)$$\nGiven the problem statement, both the prior and the likelihood are Gaussian. For a scalar state $x$, the PDFs are:\n\nThe prior distribution is $x \\sim \\mathcal{N}(x^f, P^f)$:\n$$p(x) \\propto \\exp\\left( -\\frac{1}{2} \\frac{(x - x^f)^2}{P^f} \\right)$$\nThe likelihood, based on the observation model $y = Hx + \\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0, R)$, is:\n$$p(y|x) \\propto \\exp\\left( -\\frac{1}{2} \\frac{(y - Hx)^2}{R} \\right)$$\n\nThe posterior PDF is then the product of these two:\n$$p(x|y) \\propto \\exp\\left( -\\frac{1}{2} \\frac{(x - x^f)^2}{P^f} \\right) \\exp\\left( -\\frac{1}{2} \\frac{(y - Hx)^2}{R} \\right)$$\n$$p(x|y) \\propto \\exp\\left( -\\frac{1}{2} \\left[ \\frac{(x - x^f)^2}{P^f} + \\frac{(y - Hx)^2}{R} \\right] \\right)$$\nThe product of two Gaussian distributions is another Gaussian distribution. Therefore, the posterior distribution for $x$ will be Gaussian, say $x \\sim \\mathcal{N}(x^a, P^a)$, with its PDF having the form:\n$$p(x|y) \\propto \\exp\\left( -\\frac{1}{2} \\frac{(x - x^a)^2}{P^a} \\right)$$\nTo find the analysis mean $x^a$ and variance $P^a$, we equate the arguments of the exponential functions. Let $J(x)$ be the cost function in the exponent of the posterior PDF:\n$$J(x) = \\frac{(x - x^a)^2}{P^a} + \\text{const.} = \\frac{(x - x^f)^2}{P^f} + \\frac{(y - Hx)^2}{R}$$\nWe now expand the terms on the right-hand side and collect terms based on powers of $x$:\n$$J(x) = \\frac{1}{P^f}(x^2 - 2xx^f + (x^f)^2) + \\frac{1}{R}(y^2 - 2yHx + H^2x^2)$$\n$$J(x) = x^2 \\left(\\frac{1}{P^f} + \\frac{H^2}{R}\\right) - 2x \\left(\\frac{x^f}{P^f} + \\frac{yH}{R}\\right) + \\left(\\frac{(x^f)^2}{P^f} + \\frac{y^2}{R}\\right)$$\nThis quadratic in $x$ must be equivalent to the form $\\frac{1}{P^a}(x^2 - 2xx^a + (x^a)^2) + \\text{const.}$. By comparing the coefficients of the $x^2$ and $x$ terms (\"completing the square\"), we can identify $P^a$ and $x^a$.\n\nComparing the coefficients of the $x^2$ term:\n$$\\frac{1}{P^a} = \\frac{1}{P^f} + \\frac{H^2}{R}$$\nThis equation shows that the precision (inverse variance) of the analysis is the sum of the precisions of the forecast and the observation. Solving for the analysis variance $P^a$:\n$$P^a = \\left( \\frac{R + H^2 P^f}{P^f R} \\right)^{-1} = \\frac{P^f R}{R + H^2 P^f}$$\n\nComparing the coefficients of the $-2x$ term:\n$$\\frac{x^a}{P^a} = \\frac{x^f}{P^f} + \\frac{yH}{R}$$\nSolving for the analysis mean $x^a$:\n$$x^a = P^a \\left( \\frac{x^f}{P^f} + \\frac{yH}{R} \\right) = \\frac{P^f R}{R + H^2 P^f} \\left( \\frac{x^f R + yH P^f}{P^f R} \\right) = \\frac{x^f R + yH P^f}{R + H^2 P^f}$$\nTo express this in the standard form involving the Kalman gain, we manipulate the expression for $x^a$:\n$$x^a = \\frac{x^f(R + H^2 P^f) - x^f H^2 P^f + yH P^f}{R + H^2 P^f}$$\n$$x^a = x^f + \\frac{yH P^f - x^f H^2 P^f}{R + H^2 P^f} = x^f + \\left( \\frac{P^f H}{R + H^2 P^f} \\right) (y - Hx^f)$$\nThis is the canonical Kalman update equation, $x^a = x^f + K(y - Hx^f)$. From this, we identify the Kalman gain $K$:\n$$K = \\frac{P^f H}{H^2 P^f + R}$$\nThe analysis variance $P^a$ can also be written in terms of $K$. From our expression for $P^a$:\n$$P^a = \\frac{P^f R}{R + H^2 P^f} = P^f \\frac{R + H^2 P^f - H^2 P^f}{R + H^2 P^f} = P^f \\left( 1 - \\frac{H^2 P^f}{R + H^2 P^f} \\right)$$\n$$P^a = P^f \\left( 1 - H \\cdot \\frac{H P^f}{R + H^2 P^f} \\right) = P^f(1 - HK)$$\nSince all quantities are scalars, this is equivalent to $P^a = (1-KH)P^f$.\n\n### Step 3: Evaluation for the Specific Case\n\nWe are given the following values:\n- Forecast mean: $x^f = 0$\n- Forecast variance: $P^f = 2$\n- Observation operator: $H = 1$ (from $H=\\begin{bmatrix}1\\end{bmatrix}$)\n- Observation error variance: $R = 1$\n- Observation: $y = 1$\n\nWe now substitute these values into the derived expressions.\n\n**1. Kalman Gain ($K$):**\n$$K = \\frac{P^f H}{H^2 P^f + R} = \\frac{2 \\cdot 1}{1^2 \\cdot 2 + 1} = \\frac{2}{2 + 1} = \\frac{2}{3}$$\n\n**2. Analysis Mean ($x^a$):**\n$$x^a = x^f + K(y - Hx^f) = 0 + \\frac{2}{3}(1 - 1 \\cdot 0) = \\frac{2}{3}(1) = \\frac{2}{3}$$\n\n**3. Analysis Variance ($P^a$):**\nUsing the form $P^a = (1 - KH)P^f$:\n$$P^a = \\left( 1 - \\frac{2}{3} \\cdot 1 \\right) \\cdot 2 = \\left( 1 - \\frac{2}{3} \\right) \\cdot 2 = \\frac{1}{3} \\cdot 2 = \\frac{2}{3}$$\nAlternatively, using the direct formula:\n$$P^a = \\frac{P^f R}{R + H^2 P^f} = \\frac{2 \\cdot 1}{1 + 1^2 \\cdot 2} = \\frac{2}{1 + 2} = \\frac{2}{3}$$\nThe results are consistent.\n\nThe final values, expressed as exact rational numbers, are: Kalman gain $K = \\frac{2}{3}$, analysis mean $x^a = \\frac{2}{3}$, and analysis variance $P^a = \\frac{2}{3}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{2}{3} & \\frac{2}{3} & \\frac{2}{3} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Data assimilation techniques are often categorized into two major families: sequential methods like the Kalman filter and variational methods like 3D-Var. This practice reveals a profound connection between them, demonstrating that for linear systems with Gaussian statistics, these distinct approaches yield an identical result . By deriving the analysis state from both a cost function minimization (3D-Var) and an optimal linear estimation framework (Kalman filter), you will solidify your understanding of the unified Bayesian foundation that underpins modern data assimilation.",
            "id": "3795564",
            "problem": "Consider a single analysis step in computational oceanography for a two-dimensional linear state whose components represent two prognostic variables at a single horizontal grid point. Let the unknown true state be $x \\in \\mathbb{R}^{2}$ and suppose the background (prior) state $x_b \\in \\mathbb{R}^{2}$ and its error covariance $B \\in \\mathbb{R}^{2 \\times 2}$ are given, and the observation $y \\in \\mathbb{R}$ is related to the state by a known linear operator $H \\in \\mathbb{R}^{1 \\times 2}$ with observational error covariance $R \\in \\mathbb{R}^{1 \\times 1}$. Assume the prior and observational errors are independent, unbiased, and Gaussian.\n\nStarting only from Bayes’ theorem and the assumptions of linearity and Gaussian error statistics, derive the analysis state that minimizes the negative log of the posterior density, which is equivalent to the Three-Dimensional Variational data assimilation (3D-Var) analysis. Independently, starting from the definition of the optimal linear estimator under Gaussian errors, derive the single-step Kalman Filter (KF) analysis for this same setup. Then, for the specific case\n$$\nx_b = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}, \\quad\nB = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad\nH = \\begin{pmatrix} 1 & 1 \\end{pmatrix}, \\quad\nR = \\begin{pmatrix} 1 \\end{pmatrix}, \\quad\ny = 0.5,\n$$\ncompute both the 3D-Var analysis and the single-step KF analysis explicitly, and report the Euclidean norm of the difference between these two analysis states.\n\nExpress the final answer as a real number with no units. No rounding is required.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of data assimilation, is well-posed with a unique and meaningful solution, and is expressed in objective, formal language. It is a standard problem in computational oceanography and statistical estimation.\n\nThe problem requires two independent derivations for an analysis state $x_a \\in \\mathbb{R}^2$ and a final numerical calculation. First, we derive the Three-Dimensional Variational (3D-Var) analysis. Second, we derive the single-step Kalman Filter (KF) analysis. Finally, we show that for the linear and Gaussian case, these two methods are equivalent, which directly implies the final answer.\n\n**Part 1: Derivation of the 3D-Var Analysis**\n\nThe 3D-Var analysis seeks the state $x$ that maximizes the posterior probability density function (PDF), $p(x|y)$. According to Bayes' theorem, the posterior is proportional to the product of the likelihood and the prior:\n$$p(x|y) \\propto p(y|x) p(x)$$\nThe problem states that the prior and observational errors are unbiased and Gaussian.\n\nThe prior information is the background state $x_b$ with error covariance $B$. The prior PDF, $p(x)$, is therefore a Gaussian distribution centered at $x_b$ with covariance $B$:\n$$p(x) = \\mathcal{N}(x_b, B) \\propto \\exp\\left(-\\frac{1}{2}(x - x_b)^T B^{-1} (x - x_b)\\right)$$\nThe observation $y$ is related to the true state $x$ by the linear operator $H$, with observational error $\\epsilon_o$ having zero mean and covariance $R$. Thus, $y = Hx + \\epsilon_o$. The likelihood of observing $y$ given a state $x$ is also a Gaussian distribution, $p(y|x)$, centered at $Hx$ with covariance $R$:\n$$p(y|x) = \\mathcal{N}(Hx, R) \\propto \\exp\\left(-\\frac{1}{2}(y - Hx)^T R^{-1} (y - Hx)\\right)$$\nCombining these, the posterior PDF is:\n$$p(x|y) \\propto \\exp\\left(-\\frac{1}{2}\\left[ (x - x_b)^T B^{-1} (x - x_b) + (y - Hx)^T R^{-1} (y - Hx) \\right]\\right)$$\nMaximizing the posterior PDF $p(x|y)$ is equivalent to minimizing its negative logarithm. We define the 3D-Var cost function $J(x)$ as twice the negative exponent:\n$$J(x) = (x - x_b)^T B^{-1} (x - x_b) + (y - Hx)^T R^{-1} (y - Hx)$$\nTo find the state $x_a$ that minimizes $J(x)$, we compute the gradient of $J(x)$ with respect to $x$ and set it to zero. Using matrix calculus identities, the gradient is:\n$$\\nabla_x J(x) = 2B^{-1}(x - x_b) - 2H^T R^{-1}(y - Hx)$$\nSetting $\\nabla_x J(x) = 0$ at $x = x_a$:\n$$B^{-1}(x_a - x_b) - H^T R^{-1}(y - Hx_a) = 0$$\nRearranging the terms to solve for $x_a$:\n$$B^{-1}x_a - B^{-1}x_b - H^T R^{-1}y + H^T R^{-1}Hx_a = 0$$\n$$(B^{-1} + H^T R^{-1}H)x_a = B^{-1}x_b + H^T R^{-1}y$$\nThe 3D-Var analysis state is therefore:\n$$x_{a, \\text{3DVar}} = (B^{-1} + H^T R^{-1}H)^{-1} (B^{-1}x_b + H^T R^{-1}y)$$\n\n**Part 2: Derivation of the Kalman Filter Analysis**\n\nThe Kalman Filter analysis update provides the optimal linear unbiased estimate of the state. The analysis state $x_a$ is expressed as a linear correction to the background state $x_b$ based on the innovation, which is the difference between the observation $y$ and its background estimate $Hx_b$:\n$$x_a = x_b + K(y - Hx_b)$$\nHere, $K$ is the Kalman gain matrix, which is chosen to minimize the variance of the analysis error, i.e., minimize the trace of the analysis error covariance matrix $P_a = E[(x_a-x)(x_a-x)^T]$.\n\nThe analysis error $x_a-x$ can be written as:\n$$x_a - x = x_b + K(y - Hx_b) - x = x_b + K(Hx + \\epsilon_o - Hx_b) - x$$\nwhere $\\epsilon_o$ is the observational error. Let $\\epsilon_b = x_b - x$ be the background error.\n$$x_a - x = (x_b - x) + K(H(x-x_b) + \\epsilon_o) = \\epsilon_b - KH\\epsilon_b + K\\epsilon_o = (I - KH)\\epsilon_b + K\\epsilon_o$$\nNow, we compute the analysis error covariance $P_a$. Since the background error $\\epsilon_b$ and observation error $\\epsilon_o$ are independent, $E[\\epsilon_b \\epsilon_o^T] = 0$ and $E[\\epsilon_o \\epsilon_b^T] = 0$.\n$$P_a = E[((I - KH)\\epsilon_b + K\\epsilon_o)((I - KH)\\epsilon_b + K\\epsilon_o)^T]$$\n$$P_a = (I - KH) E[\\epsilon_b\\epsilon_b^T] (I - KH)^T + K E[\\epsilon_o\\epsilon_o^T] K^T$$\nUsing $E[\\epsilon_b\\epsilon_b^T] = B$ and $E[\\epsilon_o\\epsilon_o^T] = R$:\n$$P_a = (I - KH)B(I - KH)^T + KRK^T$$\n$$P_a = B - KHB - BH^T K^T + KHBH^TK^T + KRK^T$$\nTo minimize the trace of $P_a$ with respect to $K$, we set its derivative to zero:\n$$\\frac{\\partial}{\\partial K} \\mathrm{tr}(P_a) = -2BH^T + 2K(HBH^T + R) = 0$$\nSolving for $K$ yields the optimal Kalman gain:\n$$K = BH^T(HBH^T + R)^{-1}$$\nThe Kalman Filter analysis state is thus:\n$$x_{a, \\text{KF}} = x_b + BH^T(HBH^T + R)^{-1}(y - Hx_b)$$\n\n**Equivalence and Final Calculation**\n\nFor a linear system with Gaussian errors, the 3D-Var analysis and the Kalman Filter analysis are identical. This can be shown by applying the Woodbury matrix identity to the 3D-Var analysis expression. The identity states $(A+UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1}+VA^{-1}U)^{-1}VA^{-1}$. Let $A = B^{-1}$, $U=H^T$, $C=R^{-1}$, and $V=H$.\n$$(B^{-1} + H^T R^{-1}H)^{-1} = B - BH^T(R + HBH^T)^{-1}HB$$\nSubstituting this into the 3D-Var formula for $x_a$:\n$$x_{a, \\text{3DVar}} = (B - BH^T(HBH^T + R)^{-1}HB)(B^{-1}x_b + H^T R^{-1}y)$$\n$$x_{a, \\text{3DVar}} = x_b + BH^T R^{-1}y - BH^T(HBH^T + R)^{-1}HB B^{-1}x_b - BH^T(HBH^T + R)^{-1}HBH^TR^{-1}y$$\n$$x_{a, \\text{3DVar}} = x_b - BH^T(HBH^T + R)^{-1}Hx_b + BH^T[I - (HBH^T + R)^{-1}HBH^T]R^{-1}y$$\nThe term in the square brackets simplifies: $I - (HBH^T+R)^{-1}HBH^T = (HBH^T+R)^{-1}[(HBH^T+R) - HBH^T] = (HBH^T+R)^{-1}R$.\n$$x_{a, \\text{3DVar}} = x_b - BH^T(HBH^T + R)^{-1}Hx_b + BH^T(HBH^T + R)^{-1}R R^{-1}y$$\n$$x_{a, \\text{3DVar}} = x_b + BH^T(HBH^T + R)^{-1}(y - Hx_b) = x_{a, \\text{KF}}$$\nSince the two methods produce analytically identical results, the difference between their numerical outputs must be a zero vector, and the Euclidean norm of this difference must be zero.\n\nFor completeness, we compute the analysis state for the given numerical values using the Kalman Filter formulation.\nGiven:\n$$\nx_b = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}, \\quad\nB = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad\nH = \\begin{pmatrix} 1 & 1 \\end{pmatrix}, \\quad\nR = 1, \\quad\ny = 0.5\n$$\nFirst, compute the components of the Kalman gain $K = BH^T(HBH^T + R)^{-1}$:\n$$H^T = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\n$$BH^T = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$$\n$$HBH^T = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 1(2)+1(1) = 3$$\nThe term $(HBH^T + R)^{-1}$ is a scalar:\n$$(3 + 1)^{-1} = 4^{-1} = \\frac{1}{4}$$\nThe Kalman gain is:\n$$K = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} \\frac{1}{4} = \\begin{pmatrix} 1/2 \\\\ 1/4 \\end{pmatrix}$$\nNext, compute the innovation term $y - Hx_b$:\n$$Hx_b = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = 1(1) + 1(-1) = 0$$\n$$y - Hx_b = 0.5 - 0 = 0.5 = \\frac{1}{2}$$\nFinally, compute the analysis state $x_a$:\n$$x_a = x_b + K(y - Hx_b) = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} + \\begin{pmatrix} 1/2 \\\\ 1/4 \\end{pmatrix} \\left(\\frac{1}{2}\\right) = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} + \\begin{pmatrix} 1/4 \\\\ 1/8 \\end{pmatrix} = \\begin{pmatrix} 5/4 \\\\ -7/8 \\end{pmatrix}$$\nBoth $x_{a, \\text{3DVar}}$ and $x_{a, \\text{KF}}$ are equal to $\\begin{pmatrix} 5/4 \\\\ -7/8 \\end{pmatrix}$.\nThe difference is $x_{a, \\text{3DVar}} - x_{a, \\text{KF}} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\nThe Euclidean norm of the difference is:\n$$\\left\\| \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\right\\|_2 = \\sqrt{0^2 + 0^2} = 0$$",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "The power of the Kalman filter and variational methods is built on assumptions of linearity and Gaussianity, which are often violated in complex geophysical systems. This practice introduces a powerful alternative for nonlinear problems: the particle filter . You will implement the core mechanics of a sequential Monte Carlo method, including the calculation of importance weights for a nonlinear observation model and the crucial resampling step, providing a hands-on look at how to approximate complex probability distributions in data assimilation.",
            "id": "3795560",
            "problem": "Consider a basic particle-based data assimilation setting for a one-dimensional state in computational oceanography. Let the hidden state be a scalar $x \\in \\mathbb{R}$ with a Gaussian prior and a nonlinear observation model given by $y = x^2 + \\epsilon$, where $\\epsilon$ is Gaussian noise. Specifically, observations follow the conditional law $y \\mid x \\sim \\mathcal{N}(x^2, s^2)$, where $s > 0$ is the known observation noise standard deviation. In a bootstrap particle filter, the proposal distribution equals the prior distribution, so the importance weight for each particle is proportional to the observation likelihood. The goal is to compute normalized importance weights proportional to the Probability Density Function (PDF) $p(y \\mid x)$ and then perform a deterministic systematic resampling based on these weights.\n\nStarting from Bayes’ rule and the definition of importance sampling, and assuming the proposal equals the prior (so the prior term cancels in the importance ratio), derive the formula for normalized importance weights in terms of the observation likelihood under the Gaussian noise model. Then, implement the following deterministic Systematic Resampling (SR) procedure: given normalized weights $\\{w_i\\}_{i=1}^N$ with $\\sum_{i=1}^N w_i = 1$ and $N$ particles, compute the cumulative distribution function $C_i = \\sum_{j=1}^i w_j$ and select $N$ resampled indices by placing $N$ equally spaced points on $[0,1)$ starting at $u_0 = \\frac{1}{2N}$, that is, using points $u_k = \\frac{k + 0.5}{N}$ for $k = 0, 1, \\dots, N-1$, and for each $u_k$ selecting the smallest index $i$ such that $C_i \\ge u_k$. The resampled particle values are those corresponding to the selected indices.\n\nYour program must:\n- Use the Gaussian likelihood for $p(y \\mid x)$, where $y \\mid x \\sim \\mathcal{N}(x^2, s^2)$. You must compute normalized importance weights $w_i$ that are proportional to $p(y \\mid x_i)$.\n- Perform the deterministic Systematic Resampling (SR) with $u_0 = \\frac{1}{2N}$ as specified.\n- For each test case, output the resampled particle values (not the indices) in the order they are selected by the SR procedure.\n\nTest suite. Use $N = 5$ particles for each case, and the particle set is fixed as $[-2.0,-1.0,0.0,1.0,2.0]$. The observation noise standard deviation $s$ and the observation $y$ vary per test case as follows:\n- Test case $1$: particles $[-2.0,-1.0,0.0,1.0,2.0]$, observation $y = 3.5$, noise standard deviation $s = 0.5$.\n- Test case $2$: particles $[-2.0,-1.0,0.0,1.0,2.0]$, observation $y = 0.0$, noise standard deviation $s = 0.1$.\n- Test case $3$: particles $[-2.0,-1.0,0.0,1.0,2.0]$, observation $y = -0.25$, noise standard deviation $s = 0.3$.\n- Test case $4$: particles $[-2.0,-1.0,0.0,1.0,2.0]$, observation $y = 1.0$, noise standard deviation $s = 3.0$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result must be a list of the resampled particle values formatted with one decimal place. For example, the output should look like $[[a_{11},a_{12},a_{13},a_{14},a_{15}],[a_{21},a_{22},a_{23},a_{24},a_{25}],[a_{31},a_{32},a_{33},a_{34},a_{35}],[a_{41},a_{42},a_{43},a_{44},a_{45}]]$, where each $a_{ij}$ is a floating-point number written with one decimal place. The output must be printed exactly as a single line with no additional whitespace outside or inside the enclosing brackets beyond the commas separating elements.",
            "solution": "The problem requires the derivation and implementation of a single step of a bootstrap particle filter, which involves calculating importance weights and performing systematic resampling for a one-dimensional state with a nonlinear observation model.\n\nThe analysis proceeds in two main parts: first, the derivation of the formula for normalized importance weights, and second, the description and implementation of the deterministic systematic resampling algorithm.\n\n**1. Derivation of Normalized Importance Weights**\n\nIn a particle filter, we approximate a target probability distribution, the posterior $p(x|y)$, with a set of $N$ weighted particles $\\{x_i, w_i\\}_{i=1}^N$. The state $x$ is the hidden variable, and $y$ is the observation. The core idea of importance sampling is to draw samples $x_i$ from a proposal distribution $q(x)$ and assign them weights that correct for the difference between the proposal and the target distribution. The importance weight $w_i$ for a particle $x_i$ is given by:\n$$\nw_i \\propto \\frac{p(x_i|y)}{q(x_i)}\n$$\nThe problem specifies a bootstrap particle filter, where the proposal distribution $q(x)$ is chosen to be the prior distribution $p(x)$. In this context, the prior refers to the distribution of the state before the observation $y$ is incorporated. Thus, $q(x_i) = p(x_i)$.\n\nThe target distribution is the posterior $p(x_i|y)$. Using Bayes' rule, we can express the posterior in terms of the likelihood $p(y|x_i)$ and the prior $p(x_i)$:\n$$\np(x_i|y) = \\frac{p(y|x_i)p(x_i)}{p(y)}\n$$\nwhere $p(y)$ is the marginal likelihood, acting as a normalization constant. Substituting the posterior and the proposal into the importance weight formula gives:\n$$\nw_i \\propto \\frac{p(y|x_i)p(x_i)/p(y)}{p(x_i)} = \\frac{p(y|x_i)}{p(y)}\n$$\nSince $p(y)$ is constant for all particles, it can be absorbed into the proportionality constant. Therefore, the importance weights are directly proportional to the likelihood of the observation given the particle's state:\n$$\nw_i \\propto p(y|x_i)\n$$\nThe problem defines the observation model as $y = x^2 + \\epsilon$, where the noise $\\epsilon$ is Gaussian with mean $0$ and standard deviation $s$. This implies that the conditional distribution of $y$ given $x$, which is the likelihood function, is a Gaussian distribution with mean $x^2$ and standard deviation $s$:\n$$\ny \\mid x \\sim \\mathcal{N}(x^2, s^2)\n$$\nThe probability density function (PDF) for a Gaussian distribution $\\mathcal{N}(\\mu, \\sigma^2)$ is $f(z) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(z-\\mu)^2}{2\\sigma^2}\\right)$. Thus, the likelihood for a particle $x_i$ is:\n$$\np(y|x_i) = \\frac{1}{\\sqrt{2\\pi s^2}} \\exp\\left(-\\frac{(y - x_i^2)^2}{2s^2}\\right)\n$$\nThe un-normalized weights, denoted $\\tilde{w}_i$, can be taken as any quantity proportional to $p(y|x_i)$. We can drop the constant multiplicative factor $\\frac{1}{\\sqrt{2\\pi s^2}}$:\n$$\n\\tilde{w}_i = \\exp\\left(-\\frac{(y - x_i^2)^2}{2s^2}\\right)\n$$\nTo obtain the final normalized weights $w_i$, we divide each un-normalized weight by their sum:\n$$\nw_i = \\frac{\\tilde{w}_i}{\\sum_{j=1}^N \\tilde{w}_j}\n$$\nThe set of normalized weights $\\{w_i\\}_{i=1}^N$ satisfies $\\sum_{i=1}^N w_i = 1$ and represents a discrete probability distribution over the particle set.\n\n**2. Systematic Resampling Procedure**\n\nSystematic resampling is an efficient algorithm to draw $N$ new particles from the current set of $N$ particles according to their importance weights. It mitigates the particle degeneracy problem by selecting more copies of particles with high weights and fewer or no copies of particles with low weights. The deterministic procedure specified is as follows:\n\n1.  **Compute the Cumulative Distribution Function (CDF)**: Given the normalized weights $\\{w_i\\}_{i=1}^N$, calculate the cumulative sum for each particle index $i$:\n    $$\n    C_i = \\sum_{j=1}^i w_j\n    $$\n    with $C_N = 1$. This creates a set of contiguous intervals on $[0,1)$, where the length of the $i$-th interval is $w_i$.\n\n2.  **Generate Deterministic Pointers**: A set of $N$ equally spaced points is generated on the interval $[0,1)$. The starting point is $u_0 = \\frac{1}{2N}$, and subsequent points are spaced by $\\frac{1}{N}$. The $k$-th pointer is given by:\n    $$\n    u_k = \\frac{k + 0.5}{N} \\quad \\text{for } k = 0, 1, \\dots, N-1\n    $$\n    This choice of starting point ensures that the pointers are located at the midpoints of the $N$ equal sub-intervals of $[0,1)$.\n\n3.  **Select Resampled Indices**: For each pointer $u_k$, we locate the particle it falls into. This is achieved by finding the smallest index $i$ such that the cumulative weight $C_i$ is greater than or equal to the pointer value $u_k$:\n    $$\n    \\text{Selected index for } u_k = \\min\\{i \\mid C_i \\ge u_k\\}\n    $$\n    This step is repeated for all $N$ pointers, yielding a new set of $N$ particle indices. The resampled particle set is then constructed by taking the particle values corresponding to these new indices. This process effectively inverts the empirical CDF defined by the weights.\n\nThe combination of these two steps—weight calculation and systematic resampling—forms the core of the update step in the specified particle filtering problem. The entire procedure is deterministic, leading to a unique set of resampled particles for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef systematic_resampling(particles, y, s):\n    \"\"\"\n    Performs one step of a bootstrap particle filter: weight calculation\n    and systematic resampling.\n\n    Args:\n        particles (list or np.ndarray): The set of particle values.\n        y (float): The observation value.\n        s (float): The standard deviation of the observation noise.\n\n    Returns:\n        list: A list of resampled particle values.\n    \"\"\"\n    N = len(particles)\n    particles = np.array(particles, dtype=np.float64)\n\n    # Step 1: Compute importance weights based on the Gaussian likelihood.\n    # The observation model is y = x^2 + noise, so the mean of the likelihood\n    # is the particle value squared.\n    x_sq = particles**2\n    \n    # Calculate un-normalized weights as exp(- (y - x^2)^2 / (2s^2)).\n    # Using a numerically stable approach by first computing log-weights.\n    # We subtract the max log-likelihood before exponentiating to prevent\n    # overflow/underflow issues.\n    log_likelihood = -((y - x_sq)**2) / (2 * s**2)\n    max_log_likelihood = np.max(log_likelihood)\n    weights_un = np.exp(log_likelihood - max_log_likelihood)\n\n    # Normalize the weights.\n    weights_sum = np.sum(weights_un)\n    if weights_sum  1e-15:  # Handle case where all weights are numerically zero\n        weights_norm = np.full(N, 1.0 / N)\n    else:\n        weights_norm = weights_un / weights_sum\n\n    # Step 2: Perform deterministic systematic resampling.\n    # a) Compute the cumulative distribution function (CDF) of the weights.\n    cdf = np.cumsum(weights_norm)\n\n    # b) Generate N equally spaced points, starting at 1/(2N).\n    # u_k = (k + 0.5) / N for k = 0, ..., N-1\n    u = (np.arange(N) + 0.5) / N\n\n    # c) Find the indices of the resampled particles by inverting the CDF.\n    # np.searchsorted finds the insertion points for `u` in `cdf` to maintain order.\n    # This is equivalent to finding the first index `i` such that `cdf[i] >= u_k`.\n    resampled_indices = np.searchsorted(cdf, u)\n    \n    # d) Construct the new set of particles based on the selected indices.\n    resampled_particles = particles[resampled_indices]\n\n    return resampled_particles.tolist()\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the final output.\n    \"\"\"\n    # Fixed parameters for all test cases\n    N = 5\n    particles = [-2.0, -1.0, 0.0, 1.0, 2.0]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'y': 3.5, 's': 0.5},   # Test case 1\n        {'y': 0.0, 's': 0.1},   # Test case 2\n        {'y': -0.25, 's': 0.3}, # Test case 3\n        {'y': 1.0, 's': 3.0}    # Test case 4\n    ]\n\n    all_results = []\n    for case in test_cases:\n        y = case['y']\n        s = case['s']\n        \n        # Perform the resampling for the current test case\n        resampled_particles = systematic_resampling(particles, y, s)\n        \n        # Format the result list for this case\n        formatted_case_result = [f\"{val:.1f}\" for val in resampled_particles]\n        all_results.append(f\"[{','.join(formatted_case_result)}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[[{','.join(all_results)}]]\")\n\nsolve()\n```"
        }
    ]
}