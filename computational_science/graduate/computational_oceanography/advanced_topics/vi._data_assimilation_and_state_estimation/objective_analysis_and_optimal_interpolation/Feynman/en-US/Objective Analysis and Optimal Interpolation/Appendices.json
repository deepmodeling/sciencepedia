{
    "hands_on_practices": [
        {
            "introduction": "The core idea of Optimal Interpolation (OI) is to blend background information with new observations to produce an improved estimate, or \"analysis.\" A key strength of this method is its ability to quantify the uncertainty of this final analysis. This first exercise  walks you through the fundamental calculation of the posterior analysis error variance, $\\sigma_a^2$. By working through a simple case with just two observations, you will see exactly how the prior uncertainty, the observation errors, and their spatial arrangement combine to reduce the error in our final estimate.",
            "id": "3805220",
            "problem": "Consider a zero-mean, stationary Gaussian random field representing sea surface temperature anomaly, denoted by $x(\\mathbf{x})$, with isotropic covariance function $C(r) = \\sigma^{2}\\exp(-r/L)$, where $r$ is the horizontal separation, $\\sigma^{2}$ is the prior variance, and $L$ is the correlation length scale. You wish to estimate $x(\\mathbf{x}_0)$ at an analysis location $\\mathbf{x}_0$ using linear Objective Analysis (Optimal Interpolation), which is the Best Linear Unbiased Estimator of $x(\\mathbf{x}_0)$ constructed from a set of point observations.\n\nTwo independent scalar observations $y_1$ and $y_2$ of the same anomaly field are available at locations $\\mathbf{x}_1$ and $\\mathbf{x}_2$, respectively. Each observation is contaminated by additive, independent, zero-mean measurement noise with known variances $\\mathrm{Var}(\\epsilon_1) = r_1$ and $\\mathrm{Var}(\\epsilon_2) = r_2$, and no cross-covariance between measurement errors. Assume the errors are uncorrelated with the signal $x(\\mathbf{x})$.\n\nYou are given the following scientifically reasonable parameters:\n- Prior variance $\\sigma^{2} = 1.2$ in $^{\\circ}\\mathrm{C}^{2}$.\n- Correlation length $L = 60\\,\\mathrm{km}$.\n- Distances from the analysis point: $r_1 = 30\\,\\mathrm{km}$ and $r_2 = 60\\,\\mathrm{km}$.\n- Separation between the two observation locations: $d_{12} = 90\\,\\mathrm{km}$.\n- Measurement error variances: $r_1 = r_2 = 0.09$ in $^{\\circ}\\mathrm{C}^{2}$.\n\nStarting from the definition of a Best Linear Unbiased Estimator for a Gaussian random field with additive measurement noise, derive the expression for the posterior analysis error variance at $\\mathbf{x}_0$ produced by Optimal Interpolation in terms of the prior covariance and the data covariance. Then, evaluate this posterior variance numerically for the configuration given above. Clearly indicate all intermediate quantities you compute (such as the signal–data covariance vector and the data covariance matrix) based on the stated covariance model and distances.\n\nExpress your final answer as a single real number in $^{\\circ}\\mathrm{C}^{2}$ and round your result to four significant figures. Briefly interpret this quantity physically as a posterior uncertainty at the analysis location after assimilating the two observations.",
            "solution": "The problem is scientifically grounded, well-posed, objective, and contains sufficient information for a unique solution. The notational ambiguity wherein the symbols $r_1$ and $r_2$ are used to denote both distances and error variances is noted; however, the intended meaning is clear from the physical units and context. To prevent confusion in the derivation, the distances from the analysis point $\\mathbf{x}_0$ to the observation points $\\mathbf{x}_1$ and $\\mathbf{x}_2$ will be denoted by $d_{01}$ and $d_{02}$, respectively. The measurement error variances will be denoted as per the problem statement by $r_1$ and $r_2$.\n\nThe objective is to find the posterior analysis error variance, denoted $\\sigma_a^2$, at the location $\\mathbf{x}_0$. For a Best Linear Unbiased Estimator (BLUE), which is the basis of Optimal Interpolation (OI), the analysis error variance is given by the expression:\n$$\n\\sigma_a^2 = \\sigma_b^2 - \\mathbf{k}^T \\mathbf{C}_{yy}^{-1} \\mathbf{k}\n$$\nHere, $\\sigma_b^2$ is the prior (background) error variance at the analysis location, $\\mathbf{k}$ is the covariance vector between the true state at the analysis location and the observations, and $\\mathbf{C}_{yy}$ is the total covariance matrix of the observations.\n\nWe will now determine each component of this equation.\n\n1.  **Prior Variance ($\\sigma_b^2$)**\n    The prior variance at the analysis location $\\mathbf{x}_0$ is the variance of the field itself, evaluated at zero separation, as there is no other prior information.\n    $$\n    \\sigma_b^2 = \\mathrm{Var}(x(\\mathbf{x}_0)) = C(0) = \\sigma^2 \\exp(-0/L) = \\sigma^2\n    $$\n    Given $\\sigma^2 = 1.2\\,^{\\circ}\\mathrm{C}^2$, we have $\\sigma_b^2 = 1.2$.\n\n2.  **Signal-Data Covariance Vector ($\\mathbf{k}$)**\n    This vector contains the covariances between the true field value at the analysis point $x(\\mathbf{x}_0)$ and each of the observations, $y_1$ and $y_2$. The $i$-th element of $\\mathbf{k}$ is $k_i = \\mathrm{Cov}(x(\\mathbf{x}_0), y_i)$.\n    Since $y_i = x(\\mathbf{x}_i) + \\epsilon_i$ and the measurement noise $\\epsilon_i$ is uncorrelated with the signal $x$, this simplifies to:\n    $$\n    k_i = \\mathrm{Cov}(x(\\mathbf{x}_0), x(\\mathbf{x}_i) + \\epsilon_i) = \\mathrm{Cov}(x(\\mathbf{x}_0), x(\\mathbf{x}_i)) + \\mathrm{Cov}(x(\\mathbf{x}_0), \\epsilon_i) = C(d_{0i}) + 0\n    $$\n    where $d_{0i}$ is the distance between $\\mathbf{x}_0$ and $\\mathbf{x}_i$. The vector $\\mathbf{k}$ is thus:\n    $$\n    \\mathbf{k} = \\begin{pmatrix} C(d_{01}) \\\\ C(d_{02}) \\end{pmatrix} = \\begin{pmatrix} \\sigma^2 \\exp(-d_{01}/L) \\\\ \\sigma^2 \\exp(-d_{02}/L) \\end{pmatrix}\n    $$\n    Using the given values $d_{01} = 30\\,\\mathrm{km}$, $d_{02} = 60\\,\\mathrm{km}$, $L = 60\\,\\mathrm{km}$, and $\\sigma^2 = 1.2$:\n    $$\n    k_1 = 1.2 \\exp(-30/60) = 1.2 \\exp(-0.5)\n    $$\n    $$\n    k_2 = 1.2 \\exp(-60/60) = 1.2 \\exp(-1)\n    $$\n    Numerically, $k_1 \\approx 0.727837\\,^{\\circ}\\mathrm{C}^2$ and $k_2 \\approx 0.441455\\,^{\\circ}\\mathrm{C}^2$.\n\n3.  **Data Covariance Matrix ($\\mathbf{C}_{yy}$)**\n    This matrix represents the total covariance of the observation vector $\\mathbf{y} = (y_1, y_2)^T$. It is the sum of the signal covariance matrix $\\mathbf{C}_{xx}$ and the measurement error covariance matrix $\\mathbf{R}$:\n    $$\n    \\mathbf{C}_{yy} = \\mathbf{C}_{xx} + \\mathbf{R}\n    $$\n    The elements of the signal covariance matrix are $(\\mathbf{C}_{xx})_{ij} = \\mathrm{Cov}(x(\\mathbf{x}_i), x(\\mathbf{x}_j)) = C(d_{ij})$, where $d_{ij}$ is the distance between observation points $\\mathbf{x}_i$ and $\\mathbf{x}_j$.\n    $$\n    \\mathbf{C}_{xx} = \\begin{pmatrix} C(0) & C(d_{12}) \\\\ C(d_{21}) & C(0) \\end{pmatrix} = \\begin{pmatrix} \\sigma^2 & \\sigma^2 \\exp(-d_{12}/L) \\\\ \\sigma^2 \\exp(-d_{12}/L) & \\sigma^2 \\end{pmatrix}\n    $$\n    The measurement errors are independent, so their covariance matrix $\\mathbf{R}$ is diagonal:\n    $$\n    \\mathbf{R} = \\begin{pmatrix} \\mathrm{Var}(\\epsilon_1) & 0 \\\\ 0 & \\mathrm{Var}(\\epsilon_2) \\end{pmatrix} = \\begin{pmatrix} r_1 & 0 \\\\ 0 & r_2 \\end{pmatrix}\n    $$\n    Combining these gives:\n    $$\n    \\mathbf{C}_{yy} = \\begin{pmatrix} \\sigma^2 + r_1 & \\sigma^2 \\exp(-d_{12}/L) \\\\ \\sigma^2 \\exp(-d_{12}/L) & \\sigma^2 + r_2 \\end{pmatrix}\n    $$\n    Using the given values $d_{12} = 90\\,\\mathrm{km}$, $L=60\\,\\mathrm{km}$, $\\sigma^2=1.2$, and $r_1 = r_2 = 0.09$:\n    - Diagonal elements: $\\sigma^2 + r_1 = 1.2 + 0.09 = 1.29$.\n    - Off-diagonal elements: $\\sigma^2 \\exp(-90/60) = 1.2 \\exp(-1.5)$.\n    So, the matrix is:\n    $$\n    \\mathbf{C}_{yy} = \\begin{pmatrix} 1.29 & 1.2 \\exp(-1.5) \\\\ 1.2 \\exp(-1.5) & 1.29 \\end{pmatrix}\n    $$\n    Numerically, $1.2 \\exp(-1.5) \\approx 0.267756$, so $\\mathbf{C}_{yy} \\approx \\begin{pmatrix} 1.29 & 0.267756 \\\\ 0.267756 & 1.29 \\end{pmatrix}$.\n\n4.  **Final Calculation**\n    We need to compute the variance reduction term, $\\Delta\\sigma^2 = \\mathbf{k}^T \\mathbf{C}_{yy}^{-1} \\mathbf{k}$.\n    First, we find the inverse of the $2 \\times 2$ matrix $\\mathbf{C}_{yy}$.\n    The determinant is $\\det(\\mathbf{C}_{yy}) = (1.29)^2 - (1.2 \\exp(-1.5))^2 = 1.6641 - (1.44 \\exp(-3))$.\n    $\\det(\\mathbf{C}_{yy}) \\approx 1.6641 - 1.44 \\times 0.049787 = 1.6641 - 0.071693 = 1.592407$.\n    The inverse is:\n    $$\n    \\mathbf{C}_{yy}^{-1} = \\frac{1}{\\det(\\mathbf{C}_{yy})} \\begin{pmatrix} 1.29 & -1.2 \\exp(-1.5) \\\\ -1.2 \\exp(-1.5) & 1.29 \\end{pmatrix}\n    $$\n    The quadratic form $\\Delta\\sigma^2 = \\mathbf{k}^T \\mathbf{C}_{yy}^{-1} \\mathbf{k}$ is:\n    $$\n    \\Delta\\sigma^2 = \\frac{1}{\\det(\\mathbf{C}_{yy})} \\left[ 1.29(k_1^2 + k_2^2) - 2 \\cdot (1.2 \\exp(-1.5)) \\cdot k_1 k_2 \\right]\n    $$\n    Substituting the expressions for $k_1$ and $k_2$:\n    $k_1^2 = (1.2 \\exp(-0.5))^2 = 1.44 \\exp(-1)$\n    $k_2^2 = (1.2 \\exp(-1))^2 = 1.44 \\exp(-2)$\n    $k_1 k_2 = (1.2 \\exp(-0.5)) (1.2 \\exp(-1)) = 1.44 \\exp(-1.5)$\n    Numerator of $\\Delta\\sigma^2$:\n    $1.29 \\left( 1.44 \\exp(-1) + 1.44 \\exp(-2) \\right) - 2 \\cdot (1.2 \\exp(-1.5)) \\cdot (1.44 \\exp(-1.5))$\n    $= 1.8576 (\\exp(-1) + \\exp(-2)) - 3.456 \\exp(-3)$\n    Using numerical values:\n    $k_1 \\approx 0.727837$, $k_2 \\approx 0.441455$, $1.2 \\exp(-1.5) \\approx 0.267756$.\n    $k_1^2 \\approx 0.529727$, $k_2^2 \\approx 0.194883$.\n    $k_1 k_2 \\approx 0.321303$.\n    Numerator $\\approx 1.29(0.529727 + 0.194883) - 2(0.267756)(0.321303)$\n    $\\approx 1.29(0.724610) - 0.172061 \\approx 0.934747 - 0.172061 = 0.762686$.\n    $\\Delta\\sigma^2 \\approx \\frac{0.762686}{1.592407} \\approx 0.479002 \\,^{\\circ}\\mathrm{C}^2$.\n\n    Finally, the posterior analysis error variance is:\n    $$\n    \\sigma_a^2 = \\sigma_b^2 - \\Delta\\sigma^2 \\approx 1.2 - 0.479002 = 0.720998 \\,^{\\circ}\\mathrm{C}^2\n    $$\n    Rounding to four significant figures, we get $\\sigma_a^2 = 0.7210 \\,^{\\circ}\\mathrm{C}^2$.\n\nThe physical interpretation of this result is that the initial uncertainty in the sea surface temperature anomaly at the analysis location, represented by the prior variance $\\sigma_b^2 = 1.2\\,^{\\circ}\\mathrm{C}^2$, has been reduced by assimilating the two observations. The posterior variance $\\sigma_a^2 \\approx 0.7210\\,^{\\circ}\\mathrm{C}^2$ is the expected squared error of the optimal estimate. It quantifies the remaining uncertainty after the information from the data has been incorporated. The square root of this value, $\\sqrt{\\sigma_a^2} \\approx 0.849\\,^{\\circ}\\mathrm{C}$, is the expected root-mean-square error of the final analysis.",
            "answer": "$$\n\\boxed{0.7210}\n$$"
        },
        {
            "introduction": "How far does the influence of a single observation extend? The answer is crucial for understanding how data assimilation systems work. This practice  focuses on deriving and visualizing the spatial \"influence function,\" $\\phi(r)$, which shows how the weight given to an observation's information decays with distance. You will explore how parameters like the background correlation length scale, $L$, and the observation error variance, $R$, control the size and shape of an observation's footprint on the final analysis.",
            "id": "4070623",
            "problem": "You are given a single scalar point observation of a geophysical field embedded in a background state used in numerical weather prediction and climate modeling. The task is to derive and implement the spatial influence function for Objective Analysis using Optimal Interpolation (OI), for a homogeneous and isotropic Gaussian background-error covariance. The scenario is as follows.\n\nAssume a scalar field $x(\\mathbf{r})$ defined over a two-dimensional horizontal domain, with background estimate $x^{b}(\\mathbf{r})$ and background error $e^{b}(\\mathbf{r}) = x(\\mathbf{r}) - x^{b}(\\mathbf{r})$. Assume $e^{b}(\\mathbf{r})$ is a zero-mean, second-order stationary random field characterized by variance $\\sigma_{b}^{2}$ and correlation function $C(r; L)$ that depends only on radial separation $r = \\|\\mathbf{r} - \\mathbf{r}'\\|$. Let the correlation be Gaussian with correlation length scale $L$, i.e., $C(r; L)$ is monotonically decreasing with $r$ and encodes spatial homogeneity and isotropy. There is a single point observation $y$ at location $\\mathbf{r}_{o}$ with observation operator $H$ extracting the true field value at $\\mathbf{r}_{o}$ and an independent observation error $\\epsilon$ that is Gaussian, zero-mean, and has variance $R$, so that $y = H x + \\epsilon$. All variables $x$, $x^{b}$, $y$, $e^{b}$, and $\\epsilon$ are assumed jointly Gaussian. The domain is horizontal only; vertical structure is not considered.\n\nStarting from the Best Linear Unbiased Estimator (BLUE) under Gaussian assumptions and the definition of Optimal Interpolation, derive the spatial influence function $\\phi(r)$ for a single observation located at the origin, defined as the dimensionless weight that multiplies the scalar innovation $d = y - H x^{b}$ to produce the analysis increment at a grid point at radial distance $r$ from the observation. Express $\\phi(r)$ in terms of the background-error variance $\\sigma_{b}^{2}$, the observation-error variance $R$, the correlation function $C(r; L)$, and the correlation length $L$. Then, implement a program to compute $\\phi(r)$ numerically for a prescribed set of radii.\n\nAssume the Gaussian correlation function is $C(r; L) = \\exp\\!\\left( -\\dfrac{r^{2}}{2 L^{2}} \\right)$. Let the background-error variance $\\sigma_{b}^{2}$ and the observation-error variance $R$ be constant scalars. Distances must be treated in kilometers and reported in kilometers (km). The influence $\\phi(r)$ is a dimensionless number. Additionally, derive the $e$-folding radius $r_{e}$ of the influence function defined by the condition $\\phi(r_{e}) = \\phi(0)/e$, and compute $r_{e}$ for each test case. Express $r_{e}$ in kilometers (km).\n\nYour program must compute, for each test case, the values of the influence function $\\phi(r)$ at the five radii $r \\in \\{0, L, 2L, 4L, 10L\\}$ (all in km), followed by the $e$-folding radius $r_{e}$ (in km). The program must aggregate the results from all test cases into a single line of output formatted as a comma-separated list enclosed in square brackets, where each test case’s result is itself a list of six floats in the order $[\\phi(0), \\phi(L), \\phi(2L), \\phi(4L), \\phi(10L), r_{e}]$.\n\nUse the following test suite that explores different parameter regimes:\n\n- Test Case $1$ (general happy path): $\\sigma_{b}^{2} = 4.0$, $L = 100.0$ km, $R = 1.0$.\n- Test Case $2$ (large observation-error variance): $\\sigma_{b}^{2} = 4.0$, $L = 100.0$ km, $R = 100.0$.\n- Test Case $3$ (short correlation length): $\\sigma_{b}^{2} = 4.0$, $L = 20.0$ km, $R = 1.0$.\n- Test Case $4$ (zero observation-error variance boundary): $\\sigma_{b}^{2} = 4.0$, $L = 50.0$ km, $R = 0.0$.\n\nFor all radii, distances must be in kilometers (km). The output influence values $\\phi(r)$ must be dimensionless floats. The $e$-folding radius $r_{e}$ must be reported in kilometers (km). Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case result as a list of six floats in the specified order.",
            "solution": "The problem requires the derivation and implementation of the spatial influence function for Optimal Interpolation (OI) in the context of a single scalar observation. The derivation begins with the fundamental equation of the Best Linear Unbiased Estimator (BLUE), which defines the analysis state $x^a$ as an update to a background (or first guess) state $x^b$.\n\nThe analysis update at a specific location $\\mathbf{r}$ in the domain is given by:\n$$\nx^a(\\mathbf{r}) = x^b(\\mathbf{r}) + \\delta x^a(\\mathbf{r})\n$$\nwhere $\\delta x^a(\\mathbf{r})$ is the analysis increment. The core of OI is to compute this increment as a linear combination of the innovations (observed values minus their background counterparts). For a single observation $y$ at location $\\mathbf{r}_o$, the innovation is $d = y - \\mathbf{H} x^b$. The operator $\\mathbf{H}$ maps the state field to the observation space. In this problem, $\\mathbf{H}$ simply extracts the value of the field at $\\mathbf{r}_o$, so $\\mathbf{H} x^b = x^b(\\mathbf{r}_o)$. The analysis increment at point $\\mathbf{r}$ is then:\n$$\n\\delta x^a(\\mathbf{r}) = K(\\mathbf{r}) (y - x^b(\\mathbf{r}_o))\n$$\nThe term $K(\\mathbf{r})$ is the optimal weight, or gain, that minimizes the analysis error variance. The general formula for the gain matrix $\\mathbf{K}$ in a multi-variable context is:\n$$\n\\mathbf{K} = \\mathbf{B} \\mathbf{H}^T (\\mathbf{H} \\mathbf{B} \\mathbf{H}^T + \\mathbf{R})^{-1}\n$$\nHere, $\\mathbf{B}$ is the background-error covariance matrix and $\\mathbf{R}$ is the observation-error covariance matrix. We must adapt this matrix equation to our specific case of a continuous field and a single scalar observation.\n\n1.  The term $\\mathbf{B} \\mathbf{H}^T$ represents the covariance of the background error $e^b$ between the analysis grid points and the background error at the observation location. For a single analysis point $\\mathbf{r}$ and a single observation at $\\mathbf{r}_o$, this term becomes the scalar covariance $\\mathbb{E}[e^b(\\mathbf{r}) e^b(\\mathbf{r}_o)]$. The problem states that the background error is characterized by a variance $\\sigma_b^2$ and a correlation function $C(r; L)$, where $r = \\|\\mathbf{r} - \\mathbf{r}_o\\|$. By definition, covariance is variance times correlation, so:\n    $$\n    \\mathbb{E}[e^b(\\mathbf{r}) e^b(\\mathbf{r}_o)] = \\sigma_b^2 C(r; L)\n    $$\n\n2.  The term $\\mathbf{H} \\mathbf{B} \\mathbf{H}^T$ represents the background-error variance in observation space. For a single observation at $\\mathbf{r}_o$, this is the variance of the background error at that specific point: $\\mathbb{E}[e^b(\\mathbf{r}_o) e^b(\\mathbf{r}_o)]$. This is simply the background-error variance $\\sigma_b^2$, since the correlation of a point with itself, $C(0; L)$, is unity.\n    $$\n    \\mathbf{H} \\mathbf{B} \\mathbf{H}^T = \\sigma_b^2\n    $$\n\n3.  The term $\\mathbf{R}$ is the observation-error covariance matrix. For a single, independent observation, this is simply the scalar observation-error variance, $R$.\n\nSubstituting these components back into the gain formula, we find the scalar weight $K(\\mathbf{r})$ for the analysis point at $\\mathbf{r}$. The problem defines the spatial influence function $\\phi(r)$ as this dimensionless weight. Therefore:\n$$\n\\phi(r) = \\frac{\\sigma_b^2 C(r; L)}{\\sigma_b^2 + R}\n$$\nThis function describes how the influence of the single observation's innovation decays spatially from its location. The magnitude of the influence is scaled by the ratio of background-error variance to the total error variance (background plus observation) in observation space, reflecting the relative confidence in the background versus the observation. The spatial structure of the influence is dictated entirely by the background-error correlation function.\n\nThe problem specifies a Gaussian correlation function:\n$$\nC(r; L) = \\exp\\left( -\\frac{r^2}{2 L^2} \\right)\n$$\nSubstituting this into the expression for $\\phi(r)$, we obtain the final form of the influence function:\n$$\n\\phi(r) = \\frac{\\sigma_b^2}{\\sigma_b^2 + R} \\exp\\left(-\\frac{r^2}{2 L^2}\\right)\n$$\nNext, we derive the $e$-folding radius $r_e$, which is defined by the condition $\\phi(r_e) = \\phi(0)/e$. First, we evaluate $\\phi(r)$ at $r=0$:\n$$\n\\phi(0) = \\frac{\\sigma_b^2}{\\sigma_b^2 + R} \\exp(0) = \\frac{\\sigma_b^2}{\\sigma_b^2 + R}\n$$\nNow we set up the equation for $r_e$:\n$$\n\\phi(r_e) = \\frac{1}{e} \\phi(0)\n$$\n$$\n\\frac{\\sigma_b^2}{\\sigma_b^2 + R} \\exp\\left(-\\frac{r_e^2}{2 L^2}\\right) = \\frac{1}{e} \\left( \\frac{\\sigma_b^2}{\\sigma_b^2 + R} \\right)\n$$\nAssuming $\\sigma_b^2 > 0$, we can cancel the pre-factor $\\frac{\\sigma_b^2}{\\sigma_b^2 + R}$ from both sides. Using the identity $1/e = e^{-1}$:\n$$\n\\exp\\left(-\\frac{r_e^2}{2 L^2}\\right) = e^{-1}\n$$\nTaking the natural logarithm of both sides gives:\n$$\n-\\frac{r_e^2}{2 L^2} = -1\n$$\n$$\nr_e^2 = 2 L^2\n$$\nSolving for $r_e$ (which must be non-negative):\n$$\nr_e = \\sqrt{2} L\n$$\nThis result demonstrates that the $e$-folding radius of the influence function is directly proportional to the correlation length scale $L$ of the background-error covariance function, with a proportionality constant of $\\sqrt{2}$. It is independent of the error variances $\\sigma_b^2$ and $R$.\n\nWith these two derived formulas, we can proceed to the numerical implementation.\n- Influence Function: $\\phi(r) = \\frac{\\sigma_b^2}{\\sigma_b^2 + R} \\exp\\left(-\\frac{r^2}{2 L^2}\\right)$\n- $e$-folding Radius: $r_e = \\sqrt{2} L$\nFor the special case where $R=0$ and $\\sigma_b^2 > 0$, the pre-factor becomes $\\frac{\\sigma_b^2}{\\sigma_b^2} = 1$, and $\\phi(r) = C(r; L)$. This represents the case of a perfect observation, where the analysis at the observation location exactly matches the observation value.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and implements the spatial influence function for Objective Analysis\n    using Optimal Interpolation (OI) for a single observation.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (sigma_b^2, L, R)\n    test_cases = [\n        # Test Case 1: general happy path\n        (4.0, 100.0, 1.0),\n        # Test Case 2: large observation-error variance\n        (4.0, 100.0, 100.0),\n        # Test Case 3: short correlation length\n        (4.0, 20.0, 1.0),\n        # Test Case 4: zero observation-error variance boundary\n        (4.0, 50.0, 0.0),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        sigma_b_sq, L, R = case\n        \n        case_results = []\n\n        # Define the influence function phi(r) based on the derived formula\n        def influence_function(r, sigma_b_sq, L, R):\n            \"\"\"\n            Computes the spatial influence function phi(r).\n            phi(r) = (sigma_b^2 / (sigma_b^2 + R)) * exp(-r^2 / (2 * L^2))\n            \"\"\"\n            # Handle the case R=0 where sigma_b_sq > 0. The pre-factor is 1.\n            # Avoids potential issues with large numbers but is robust either way.\n            if sigma_b_sq + R == 0:\n                # This case (sigma_b_sq=0 and R=0) is not in the test suite\n                # but represents no information from background or observation.\n                # Influence would be ill-defined or zero.\n                return 0.0\n            \n            prefactor = sigma_b_sq / (sigma_b_sq + R)\n            exponent = - (r**2) / (2 * L**2)\n            return prefactor * np.exp(exponent)\n\n        # Radii at which to compute the influence function\n        radii = [0 * L, 1 * L, 2 * L, 4 * L, 10 * L]\n        \n        # Calculate phi(r) for each radius\n        for r in radii:\n            phi_val = influence_function(r, sigma_b_sq, L, R)\n            case_results.append(phi_val)\n        \n        # Calculate the e-folding radius r_e based on the derived formula\n        # r_e = sqrt(2) * L\n        r_e = np.sqrt(2) * L\n        case_results.append(r_e)\n\n        all_results.append(case_results)\n\n    # Format the final output as a string representation of a list of lists.\n    # e.g., [[val1, val2, ...], [val7, val8, ...]]\n    # Python's default list-to-string conversion adds spaces, which is fine.\n    # The prompt's example f\"[{','.join(map(str, results))}]\" correctly constructs\n    # the string representation of a list of lists.\n    output_str = f\"[{','.join(map(str, all_results))}]\"\n\n    # Final print statement in the exact required format.\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Now we bring all the pieces together in a realistic oceanographic scenario. This final practice  challenges you to perform a complete Objective Analysis, mapping sea surface temperature from sparse, irregular ship-track data onto a regular grid. More importantly, you will compute the corresponding analysis error variance map, which is a critical product that tells us where our mapped field is most (and least) reliable. This exercise demonstrates how OI is used in practice to create not just an estimate, but also a spatially explicit characterization of its uncertainty.",
            "id": "3805211",
            "problem": "You are provided with a scenario in computational oceanography involving Objective Analysis and Optimal Interpolation (OI). A set of irregular sea surface temperature (SST) observations are collected along two ship tracks within a square domain. Assume the SST anomalies form a second-order stationary Gaussian random field with isotropic covariance specified by $C(r)=\\sigma^2\\exp\\left(-\\dfrac{r}{L}\\right)$, where $r$ is the Euclidean separation in $\\mathrm{km}$, $\\sigma^2$ is the prior variance in $(\\text{°C})^2$, and $L$ is the covariance length scale in $\\mathrm{km}$. The measurement error is independent, Gaussian, and spatially uncorrelated with known variance $\\epsilon^2$ in $(\\text{°C})^2$.\n\nYou must implement Optimal Interpolation (OI) to map SST onto a regular grid from the irregular ship-track observations and compute the corresponding analysis error variance map. The Objective Analysis (OA) and Optimal Interpolation (OI) should be derived and implemented from first principles, beginning with a Gaussian random field assumption and the definition of the linear unbiased estimator that minimizes mean squared error.\n\nThe computational domain is a square with coordinates in $\\mathrm{km}$: $x\\in[0,200]$ and $y\\in[0,200]$. The grid for mapping consists of $21\\times 21$ points with $10\\,\\mathrm{km}$ spacing in both $x$ and $y$. The ship tracks are defined by the following observation locations (in $\\mathrm{km}$):\n- Track A: $(10,50)$, $(40,50)$, $(70,50)$, $(100,50)$, $(130,50)$, $(160,50)$, $(190,50)$.\n- Track B: $(120,20)$, $(120,60)$, $(120,100)$, $(120,140)$, $(120,180)$.\nThese yield $12$ observations. The corresponding observed SST values in $^\\circ\\mathrm{C}$ must be treated as given constants in your program, constructed by evaluating a smooth, physically plausible true SST anomaly field and adding small, fixed measurement noise for reproducibility. The analysis error variance mapping must depend only on the geometry and the covariance parameters $L$, $\\sigma^2$, and $\\epsilon^2$.\n\nFrom the Gaussian random field fundamentals and the Best Linear Unbiased Estimator (BLUE) principles, derive the OI weights and the analysis error variance as functions of the covariance model and the observation network. Implement the OI mapping on the specified grid and compute the analysis error variance map across the grid in $(\\text{°C})^2$.\n\nYour program must evaluate the following test suite of parameter sets, each specified by $(L,\\sigma^2,\\epsilon^2)$ with units $L$ in $\\mathrm{km}$ and $\\sigma^2,\\epsilon^2$ in $(\\text{°C})^2$:\n1. $(50,1.0,0.04)$: a typical mesoscale-dominant case with moderate measurement error.\n2. $(10,1.0,0.01)$: a short length-scale case where observations have highly localized influence.\n3. $(200,1.0,0.25)$: a long length-scale case with relatively high measurement error.\n4. $(80,1.0,4.0)$: an edge case with very high measurement error approaching prior uncertainty.\n\nFor each parameter set, compute the domain-mean analysis error variance across the grid in $(\\text{°C})^2$. Report each domain-mean value as a float rounded to six decimal places.\n\nAngle units do not apply in this problem. All distances must be in $\\mathrm{km}$, and all variances must be in $(\\text{°C})^2$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces, in the order of the parameter sets listed above, for example: $[v_1,v_2,v_3,v_4]$, where each $v_i$ is the domain-mean analysis error variance in $(\\text{°C})^2$ rounded to six decimals.\n\nEnsure scientific realism and internal consistency. The mapping and analysis error must be computed using the OI equations derived from the stated covariance and measurement error assumptions, without using shortcut formulas that bypass those derivations.",
            "solution": "The problem requires the implementation of Optimal Interpolation (OI) to estimate a sea surface temperature (SST) anomaly field and its corresponding analysis error variance. The derivation must proceed from first principles, assuming the SST anomalies constitute a second-order stationary Gaussian random field.\n\nLet the true SST anomaly at a location $\\mathbf{x}$ be denoted by the random variable $t(\\mathbf{x})$. We are given a set of $N=12$ observations at locations $\\mathbf{x}_i$ for $i=1, \\dots, N$. The vector of observations is $\\mathbf{d}^o$, where each observation $d^o_i$ is the sum of the true field value and a measurement error $\\eta_i$:\n$$d^o_i = t(\\mathbf{x}_i) + \\eta_i$$\nIn vector form, this is $\\mathbf{d}^o = \\mathbf{t}^o + \\boldsymbol{\\eta}$, where $\\mathbf{t}^o$ is the vector of true SST anomalies at the observation locations.\n\nThe statistical properties of the field and errors are specified:\n1.  The field $t(\\mathbf{x})$ and error $\\boldsymbol{\\eta}$ are zero-mean Gaussian random variables: $E[t(\\mathbf{x})] = 0$ and $E[\\boldsymbol{\\eta}] = \\mathbf{0}$.\n2.  The covariance of the true field between two points $\\mathbf{x}_i$ and $\\mathbf{x}_j$ is given by the isotropic function $C(r_{ij}) = \\sigma^2\\exp\\left(-\\frac{r_{ij}}{L}\\right)$, where $r_{ij} = ||\\mathbf{x}_i - \\mathbf{x}_j||$ is the Euclidean distance. The matrix of these covariances between all pairs of observation points is the prior signal covariance matrix, $\\mathbf{P}$. Its elements are $P_{ij} = E[t(\\mathbf{x}_i) t(\\mathbf{x}_j)] = C(r_{ij})$.\n3.  The measurement errors are spatially uncorrelated and independent of the signal, with a constant variance $\\epsilon^2$. The measurement error covariance matrix, $\\mathbf{R}$, is therefore diagonal: $R_{ij} = E[\\eta_i \\eta_j] = \\epsilon^2 \\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta.\n\nThe goal is to find an estimate, or analysis, $t_g^a$ of the true value $t_g = t(\\mathbf{x}_g)$ at a grid point $\\mathbf{x}_g$. We seek the Best Linear Unbiased Estimator (BLUE).\n\nThe estimator is defined as a linear combination of the observations:\n$$t_g^a = \\mathbf{w}^T \\mathbf{d}^o = \\sum_{i=1}^{N} w_i d^o_i$$\nwhere $\\mathbf{w}$ is a vector of weights to be determined.\n\nThe estimator must be unbiased, meaning its expected value equals the expected value of the true quantity: $E[t_g^a] = E[t_g]$.\nGiven the zero-mean assumption for both the field and the observations, we have $E[t_g] = 0$ and $E[t_g^a] = E[\\mathbf{w}^T \\mathbf{d}^o] = \\mathbf{w}^T E[\\mathbf{d}^o] = 0$. The unbiased condition is thus satisfied for any choice of weights $\\mathbf{w}$.\n\nThe \"best\" estimator is the one that minimizes the mean squared error, which is the analysis error variance $\\sigma_a^2$:\n$$\\sigma_a^2 = J(\\mathbf{w}) = E[(t_g^a - t_g)^2]$$\nSubstituting the expressions for $t_g^a$ and $\\mathbf{d}^o$:\n$$\\sigma_a^2 = E\\left[ \\left(\\mathbf{w}^T (\\mathbf{t}^o + \\boldsymbol{\\eta}) - t_g\\right)^2 \\right]$$\nExpanding and taking the expectation, we use the fact that the signal and noise are uncorrelated ($E[\\mathbf{t}^o \\boldsymbol{\\eta}^T]=\\mathbf{0}$, $E[t_g \\boldsymbol{\\eta}^T]=\\mathbf{0}$):\n$$\\sigma_a^2 = E\\left[ \\mathbf{w}^T \\mathbf{t}^o (\\mathbf{t}^o)^T \\mathbf{w} + \\mathbf{w}^T \\boldsymbol{\\eta} \\boldsymbol{\\eta}^T \\mathbf{w} - 2\\mathbf{w}^T \\mathbf{t}^o t_g + t_g^2 \\right]$$\n$$\\sigma_a^2 = \\mathbf{w}^T E[\\mathbf{t}^o (\\mathbf{t}^o)^T] \\mathbf{w} + \\mathbf{w}^T E[\\boldsymbol{\\eta} \\boldsymbol{\\eta}^T] \\mathbf{w} - 2\\mathbf{w}^T E[\\mathbf{t}^o t_g] + E[t_g^2]$$\n\nWe identify the expectation terms based on the covariance definitions:\n- $E[\\mathbf{t}^o (\\mathbf{t}^o)^T] = \\mathbf{P}$, the $N \\times N$ prior signal covariance matrix at observation points.\n- $E[\\boldsymbol{\\eta} \\boldsymbol{\\eta}^T] = \\mathbf{R}$, the $N \\times N$ measurement error covariance matrix.\n- $E[t_g^2] = C(0) = \\sigma^2\\exp(0) = \\sigma^2$, the prior variance of the field.\n- $E[\\mathbf{t}^o t_g]$ is an $N \\times 1$ column vector whose $i$-th element is $E[t(\\mathbf{x}_i) t(\\mathbf{x}_g)] = C(\\mathbf{x}_i, \\mathbf{x}_g)$. We denote this vector by $\\mathbf{p}_g$.\n\nSubstituting these into the expression for $\\sigma_a^2$ yields the cost function:\n$$\\sigma_a^2(\\mathbf{w}) = \\mathbf{w}^T (\\mathbf{P} + \\mathbf{R}) \\mathbf{w} - 2\\mathbf{w}^T \\mathbf{p}_g + \\sigma^2$$\nTo minimize this quadratic function with respect to $\\mathbf{w}$, we compute its gradient and set it to zero:\n$$\\nabla_{\\mathbf{w}} \\sigma_a^2 = 2(\\mathbf{P} + \\mathbf{R})\\mathbf{w} - 2\\mathbf{p}_g = \\mathbf{0}$$\nThis gives the normal equations for the optimal weights:\n$$(\\mathbf{P} + \\mathbf{R})\\mathbf{w} = \\mathbf{p}_g$$\nThe solution for the optimal weight vector $\\mathbf{w}$ is:\n$$\\mathbf{w} = (\\mathbf{P} + \\mathbf{R})^{-1} \\mathbf{p}_g$$\nThe matrix $\\mathbf{P} + \\mathbf{R}$ is the observation covariance matrix, which is invertible as long as $\\mathbf{R}$ has positive diagonal elements (i.e., $\\epsilon^2 > 0$), which is true here.\n\nWith the optimal weights, we can write the final OI equations.\nThe analysis field at grid point $\\mathbf{x}_g$ is:\n$$t_g^a = \\mathbf{w}^T \\mathbf{d}^o = \\mathbf{p}_g^T (\\mathbf{P} + \\mathbf{R})^{-1} \\mathbf{d}^o$$\nThe analysis error variance is found by substituting the optimal $\\mathbf{w}$ back into the cost function. A simpler form is obtained by noting that $\\mathbf{p}_g^T = \\mathbf{w}^T (\\mathbf{P}+\\mathbf{R})$:\n$$\\sigma_a^2 = \\sigma^2 - \\mathbf{p}_g^T \\mathbf{w} = \\sigma^2 - \\mathbf{p}_g^T (\\mathbf{P} + \\mathbf{R})^{-1} \\mathbf{p}_g$$\nThis final equation is central to the problem. It shows that the analysis error variance $\\sigma_a^2$ at any point $\\mathbf{x}_g$ is the prior variance $\\sigma^2$ reduced by an amount that depends on the geometry of the observations relative to the grid point and the statistical parameters $(L, \\sigma^2, \\epsilon^2)$. Crucially, it does not depend on the specific observation values in $\\mathbf{d}^o$.\n\nTo solve the problem, we will implement this formula for each parameter set. The algorithm is as follows:\n1.  Define the fixed observation locations $\\mathbf{x}_i$ and generate the set of grid point locations $\\mathbf{x}_g$.\n2.  For each parameter set $(L, \\sigma^2, \\epsilon^2)$:\n    a. Construct the $N \\times N$ observation covariance matrix $\\mathbf{M} = \\mathbf{P} + \\mathbf{R}$. The elements are $M_{ij} = \\sigma^2\\exp(-||\\mathbf{x}_i - \\mathbf{x}_j||/L) + \\epsilon^2\\delta_{ij}$.\n    b. Compute the inverse $\\mathbf{M}^{-1}$. This is done once per parameter set.\n    c. For each of the $21 \\times 21 = 441$ grid points $\\mathbf{x}_g$:\n        i. Construct the $N \\times 1$ grid-to-observation covariance vector $\\mathbf{p}_g$, with elements $p_{g,i} = \\sigma^2\\exp(-||\\mathbf{x}_g - \\mathbf{x}_i||/L)$.\n        ii. Calculate the analysis error variance: $\\sigma_{a,g}^2 = \\sigma^2 - \\mathbf{p}_g^T \\mathbf{M}^{-1} \\mathbf{p}_g$.\n    d. Compute the arithmetic mean of all $441$ values of $\\sigma_{a,g}^2$ to obtain the domain-mean analysis error variance.\n3.  Report the results for all parameter sets, formatted as requested.",
            "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef solve():\n    \"\"\"\n    Solves the Optimal Interpolation problem for the given test cases.\n    \"\"\"\n\n    # --- Define problem geometry and constants ---\n    # Observation locations in km\n    obs_locs = np.array([\n        [10., 50.], [40., 50.], [70., 50.], [100., 50.], [130., 50.], [160., 50.], [190., 50.],\n        [120., 20.], [120., 60.], [120., 100.], [120., 140.], [120., 180.]\n    ])\n    num_obs = obs_locs.shape[0]\n\n    # Grid definition in km\n    grid_dim = 21\n    grid_coords = np.linspace(0., 200., grid_dim)\n    gx, gy = np.meshgrid(grid_coords, grid_coords)\n    grid_locs = np.vstack([gx.ravel(), gy.ravel()]).T\n\n    # Test cases: (L, sigma^2, epsilon^2)\n    # L in km, sigma^2 and epsilon^2 in (degC)^2\n    test_cases = [\n        (50.0, 1.0, 0.04),\n        (10.0, 1.0, 0.01),\n        (200.0, 1.0, 0.25),\n        (80.0, 1.0, 4.0),\n    ]\n\n    # As per the problem, observed SST values are constants, though not used\n    # for error variance calculation. We define them here for completeness.\n    def true_field(x, y):\n        # A simple, physically plausible smooth field for anomalies\n        return 1.5 * np.exp(-((x - 100.)**2 + (y - 100.)**2) / (2. * 60.**2))\n\n    true_values_at_obs = np.array([true_field(x, y) for x, y in obs_locs])\n    # Reproducible noise with a small, fixed standard deviation\n    rng = np.random.RandomState(seed=123)\n    noise = rng.normal(0, np.sqrt(0.0225), size=num_obs)\n    # This d_obs vector is a constant, as required by the problem statement.\n    d_obs = true_values_at_obs + noise\n\n    def compute_mean_error_variance(L, sigma_sq, epsilon_sq, obs_locs, grid_locs):\n        \"\"\"\n        Computes the domain-mean analysis error variance for a given set of parameters.\n        \n        Args:\n            L (float): Covariance length scale in km.\n            sigma_sq (float): Prior variance in (degC)^2.\n            epsilon_sq (float): Measurement error variance in (degC)^2.\n            obs_locs (np.ndarray): Array of observation locations (N_obs, 2).\n            grid_locs (np.ndarray): Array of grid locations (N_grid, 2).\n\n        Returns:\n            float: The domain-mean analysis error variance.\n        \"\"\"\n        n_obs = obs_locs.shape[0]\n\n        # 1. Construct observation covariance matrix M = P + R\n        # P: prior signal covariance matrix between observations\n        dist_obs_obs = cdist(obs_locs, obs_locs, 'euclidean')\n        P = sigma_sq * np.exp(-dist_obs_obs / L)\n        \n        # R: measurement error covariance matrix (diagonal)\n        R = np.eye(n_obs) * epsilon_sq\n        \n        M = P + R\n        \n        # 2. Invert the matrix M\n        try:\n            M_inv = np.linalg.inv(M)\n        except np.linalg.LinAlgError:\n            # Add a small nugget for stability if matrix is singular, though\n            # epsilon_sq > 0 should prevent this.\n            M += np.eye(n_obs) * 1e-9\n            M_inv = np.linalg.inv(M)\n\n        # 3. Compute analysis error variance for each grid point\n        # p_g: grid-to-observation covariance vector\n        dist_grid_obs = cdist(grid_locs, obs_locs, 'euclidean')\n        P_go = sigma_sq * np.exp(-dist_grid_obs / L) # Shape: (n_grid, n_obs)\n        \n        # Variance reduction term: p_g^T * M^-1 * p_g for each grid point\n        # This can be vectorized efficiently.\n        # einsum 'ik,kl,lj->i' does (P_go @ M_inv @ P_go.T) and takes the diagonal.\n        # A more direct way is sum( (P_go @ M_inv) * P_go, axis=1)\n        variance_reduction = np.sum((P_go @ M_inv) * P_go, axis=1)\n        \n        # Analysis error variance map: sigma_a^2 = sigma^2 - reduction\n        analysis_error_variance_map = sigma_sq - variance_reduction\n        \n        # 4. Compute domain-mean of the analysis error variance\n        mean_error_variance = np.mean(analysis_error_variance_map)\n        \n        return mean_error_variance\n\n    results = []\n    for case in test_cases:\n        L, sigma_sq, epsilon_sq = case\n        mean_var = compute_mean_error_variance(L, sigma_sq, epsilon_sq, obs_locs, grid_locs)\n        results.append(f\"{mean_var:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}