{
    "hands_on_practices": [
        {
            "introduction": "The adjoint model is the computational engine of 4D-Var, enabling the efficient calculation of the cost function's gradient with respect to the initial state. Before deploying this model within the larger assimilation system, it is imperative to verify its correctness. This exercise  provides a fundamental consistency check, ensuring that the implemented discrete adjoint operator is indeed the true adjoint of the tangent-linear model with respect to a chosen inner product, a critical property for the validity of the entire 4D-Var process.",
            "id": "3793618",
            "problem": "Consider the role of the adjoint model in Four-Dimensional Variational Data Assimilation (4D-Var) in computational oceanography. The numerical adjoint of the tangent-linear operator must satisfy the discrete adjoint property under a chosen inner product for any state perturbations. Let the inner product on $\\mathbb{R}^N$ be defined by $\\langle \\mathbf{x}, \\mathbf{y} \\rangle_{\\mathbf{W}} = \\mathbf{x}^\\top \\mathbf{W} \\mathbf{y}$, where $\\mathbf{W} \\in \\mathbb{R}^{N \\times N}$ is symmetric positive definite. For a linear operator $\\mathbf{L} \\in \\mathbb{R}^{N \\times N}$, its discrete adjoint under this inner product, denoted $\\mathbf{L}^\\dagger$, is defined by $\\langle \\mathbf{L}\\mathbf{x}, \\mathbf{y} \\rangle_{\\mathbf{W}} = \\langle \\mathbf{x}, \\mathbf{L}^\\dagger \\mathbf{y} \\rangle_{\\mathbf{W}}$ for all $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^N$.\n\nStart from a physically meaningful one-dimensional tracer transport model commonly used in oceanography, consisting of advection, diffusion, and a quadratic local reaction. The continuous equation is\n$$\n\\frac{\\partial u}{\\partial t} + c \\frac{\\partial u}{\\partial x} = K \\frac{\\partial^2 u}{\\partial x^2} + \\gamma u^2,\n$$\non the domain $x \\in [0,1]$, with either periodic boundary conditions or homogeneous Neumann boundary conditions (zero flux at both ends). Let $u$ be discretized on a uniform grid with $N$ points and spacing $h$ (with $h = \\frac{1}{N}$ for periodic and $h = \\frac{1}{N-1}$ for Neumann boundary conditions). Use second-order centered differences for interior points. For periodic boundaries, use wrap-around indexing. For homogeneous Neumann boundaries, enforce zero-gradient via mirrored ghost points for the second derivative and set the first derivative to zero at the boundaries.\n\nDefine a single explicit Euler time step mapping $F: \\mathbb{R}^N \\to \\mathbb{R}^N$ by\n$$\nF(\\mathbf{u}) = \\mathbf{u} + \\Delta t \\left( -c \\mathbf{D}_x \\mathbf{u} + K \\mathbf{D}_{xx} \\mathbf{u} + \\gamma \\, \\mathbf{u} \\odot \\mathbf{u} \\right),\n$$\nwhere $\\mathbf{D}_x$ is the discrete first-derivative matrix, $\\mathbf{D}_{xx}$ is the discrete second-derivative (Laplacian) matrix, $\\Delta t$ is the time step, $c$ is the advection speed, $K$ is the diffusion coefficient, $\\gamma$ is the reaction coefficient, and $\\odot$ denotes element-wise multiplication. The linearized forward operator (tangent-linear) about a background state $\\bar{\\mathbf{u}}$ is\n$$\n\\mathbf{L} = \\mathbf{I} + \\Delta t \\left( -c \\mathbf{D}_x + K \\mathbf{D}_{xx} + 2 \\gamma \\, \\operatorname{diag}(\\bar{\\mathbf{u}}) \\right),\n$$\nwhere $\\operatorname{diag}(\\bar{\\mathbf{u}})$ is the diagonal matrix with entries of $\\bar{\\mathbf{u}}$ on the diagonal.\n\nUnder the weighted inner product $\\langle \\cdot, \\cdot \\rangle_{\\mathbf{W}}$, the discrete adjoint is\n$$\n\\mathbf{L}^\\dagger = \\mathbf{W}^{-1} \\mathbf{L}^\\top \\mathbf{W}.\n$$\n\nYour task is to implement a complete, runnable program that:\n- Constructs $\\mathbf{D}_x$ and $\\mathbf{D}_{xx}$ according to the specifications above for both periodic and homogeneous Neumann boundary conditions on a uniform grid.\n- Constructs $\\mathbf{L}$ for a given set of parameters $(N, \\Delta t, c, K, \\gamma, \\text{boundary}, \\bar{\\mathbf{u}})$.\n- Constructs $\\mathbf{W}$ as a diagonal matrix:\n  - For uniform weighting, set $w_i = h$ for all $i$.\n  - For a nonuniform test, set $w_i = h\\left(1 + 0.5 \\sin\\left(\\frac{2\\pi i}{N}\\right)\\right)$ for $i = 0, 1, \\ldots, N-1$, which is strictly positive.\n- Verifies the discrete adjoint property by computing the scalar difference\n$$\n\\delta = \\left| \\langle \\mathbf{L}\\mathbf{x}, \\mathbf{y} \\rangle_{\\mathbf{W}} - \\langle \\mathbf{x}, \\mathbf{L}^\\dagger \\mathbf{y} \\rangle_{\\mathbf{W}} \\right|\n$$\nfor randomly generated $\\mathbf{x}, \\mathbf{y}$ and checking whether $\\delta \\le \\tau$, with tolerance $\\tau = 10^{-10}$. Use a deterministic pseudo-random number generator with a specified integer seed $s$ and draw $\\bar{\\mathbf{u}}, \\mathbf{x}, \\mathbf{y}$ independently from a standard normal distribution $\\mathcal{N}(0,1)$.\n\nImplement the following test suite of parameter sets, each defined by $(N, \\Delta t, c, K, \\gamma, \\text{boundary}, \\text{weights}, s)$:\n- Test $1$ (general advection–diffusion–reaction, periodic, uniform weights): $(N=64, \\Delta t=10^{-3}, c=0.4, K=10^{-2}, \\gamma=0.5, \\text{periodic}, \\text{uniform}, s=1)$.\n- Test $2$ (advection-dominated, periodic, uniform weights): $(N=128, \\Delta t=2\\times 10^{-3}, c=1.0, K=0.0, \\gamma=0.0, \\text{periodic}, \\text{uniform}, s=2)$.\n- Test $3$ (diffusion-dominated, Neumann, uniform weights): $(N=50, \\Delta t=5\\times 10^{-3}, c=0.0, K=5\\times 10^{-2}, \\gamma=0.0, \\text{neumann}, \\text{uniform}, s=3)$.\n- Test $4$ (nonlinear reaction with nonuniform weights, periodic): $(N=60, \\Delta t=10^{-3}, c=0.2, K=10^{-2}, \\gamma=1.0, \\text{periodic}, \\text{nonuniform}, s=4)$.\n- Test $5$ (small time step edge case, Neumann, uniform weights): $(N=32, \\Delta t=10^{-6}, c=0.3, K=2\\times 10^{-2}, \\gamma=0.7, \\text{neumann}, \\text{uniform}, s=5)$.\n- Test $6$ (small grid, periodic, uniform weights): $(N=5, \\Delta t=10^{-3}, c=0.5, K=0.0, \\gamma=0.0, \\text{periodic}, \\text{uniform}, s=6)$.\n\nFor each test case, your program must:\n- Build $\\mathbf{D}_x$, $\\mathbf{D}_{xx}$, $\\mathbf{L}$, and $\\mathbf{W}$ using the specified parameters.\n- Generate $\\bar{\\mathbf{u}}, \\mathbf{x}, \\mathbf{y}$ with the given seed $s$.\n- Compute the boolean result of the consistency check defined by $\\delta \\le \\tau$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets (for example, $[\\text{result}_1,\\text{result}_2,\\ldots,\\text{result}_6]$), where each entry is the boolean value for the corresponding test case, in the order listed above.",
            "solution": "The objective of this problem is to verify the discrete adjoint property for a tangent-linear model derived from a one-dimensional advection-diffusion-reaction equation, which is a fundamental task in developing 4D-Var data assimilation systems. The verification is conducted by numerically testing the defining relation of the discrete adjoint under a specified weighted inner product.\n\nThe continuous physical model is a tracer transport equation on the domain $x \\in [0,1]$:\n$$\n\\frac{\\partial u}{\\partial t} + c \\frac{\\partial u}{\\partial x} = K \\frac{\\partial^2 u}{\\partial x^2} + \\gamma u^2\n$$\nHere, $u(x,t)$ is the tracer concentration, $c$ is the advection velocity, $K$ is the diffusion coefficient, and $\\gamma$ is a quadratic reaction coefficient. The state of the system is discretized on a uniform grid of $N$ points, represented by a vector $\\mathbf{u} \\in \\mathbb{R}^N$. A single time step using the explicit Euler method defines a nonlinear forward model $F: \\mathbb{R}^N \\to \\mathbb{R}^N$:\n$$\nF(\\mathbf{u}) = \\mathbf{u} + \\Delta t \\left( -c \\mathbf{D}_x \\mathbf{u} + K \\mathbf{D}_{xx} \\mathbf{u} + \\gamma \\, \\mathbf{u} \\odot \\mathbf{u} \\right)\n$$\nwhere $\\mathbf{D}_x$ and $\\mathbf{D}_{xx}$ are matrix representations of the first and second spatial derivatives, respectively, $\\Delta t$ is the time step, and $\\odot$ is the element-wise product.\n\nThe core of 4D-Var involves the tangent-linear model, which describes the evolution of small perturbations. This is obtained by linearizing $F$ around a background trajectory state $\\bar{\\mathbf{u}}$. The resulting tangent-linear operator $\\mathbf{L}$ is:\n$$\n\\mathbf{L} = \\frac{\\partial F}{\\partial \\mathbf{u}}\\bigg|_{\\bar{\\mathbf{u}}} = \\mathbf{I} + \\Delta t \\left( -c \\mathbf{D}_x + K \\mathbf{D}_{xx} + 2 \\gamma \\, \\operatorname{diag}(\\bar{\\mathbf{u}}) \\right)\n$$\nwhere $\\mathbf{I}$ is the identity matrix and $\\operatorname{diag}(\\bar{\\mathbf{u}})$ is a diagonal matrix with the elements of $\\bar{\\mathbf{u}}$ on its diagonal.\n\nThe adjoint model is central to efficiently computing the gradient of the cost function in 4D-Var. The adjoint operator, $\\mathbf{L}^\\dagger$, is defined with respect to an inner product. For this problem, a weighted inner product $\\langle \\mathbf{x}, \\mathbf{y} \\rangle_{\\mathbf{W}} = \\mathbf{x}^\\top \\mathbf{W} \\mathbf{y}$ is used, where $\\mathbf{W}$ is a symmetric positive-definite weight matrix. The defining property of the adjoint is $\\langle \\mathbf{L}\\mathbf{x}, \\mathbf{y} \\rangle_{\\mathbf{W}} = \\langle \\mathbf{x}, \\mathbf{L}^\\dagger \\mathbf{y} \\rangle_{\\mathbf{W}}$ for all vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^N$. From this definition, it can be mathematically derived that the matrix representation of the adjoint is $\\mathbf{L}^\\dagger = \\mathbf{W}^{-1} \\mathbf{L}^\\top \\mathbf{W}$.\n\nThe implementation proceeds by first constructing the discrete derivative operators $\\mathbf{D}_x$ and $\\mathbf{D}_{xx}$. For both periodic and Neumann boundary conditions, these operators are constructed using second-order centered differences.\n- For periodic boundaries on $N$ grid points with spacing $h = 1/N$, indices are wrapped around (e.g., the neighbors of node $i$ are $(i-1+N)\\%N$ and $(i+1)\\%N$). This results in circulant matrices.\n- For homogeneous Neumann boundaries on $N$ grid points with spacing $h = 1/(N-1)$, the conditions are enforced as follows:\n  - For the first derivative $\\mathbf{D}_x$, the gradient is set to zero at the boundaries ($x=0$ and $x=1$). This is implemented by setting the first and last rows of the $\\mathbf{D}_x$ matrix to zero.\n  - For the second derivative $\\mathbf{D}_{xx}$, a zero-flux condition is implemented using mirrored ghost points (e.g., $u_{-1}=u_1$ at the left boundary). This modifies the first and last rows of the $\\mathbf{D}_{xx}$ matrix. For node $i=0$, the stencil becomes $(2u_1 - 2u_0)/h^2$, and for node $i=N-1$, it becomes $(2u_{N-2} - 2u_{N-1})/h^2$.\n\nNext, the diagonal weight matrix $\\mathbf{W}$ is constructed. For uniform weighting, the diagonal elements are all $w_i = h$. For the specified non-uniform case, they are $w_i = h(1 + 0.5 \\sin(2\\pi i/N))$.\n\nWith all component matrices defined ($\\mathbf{D}_x$, $\\mathbf{D}_{xx}$, $\\mathbf{W}$), and given a random background state $\\bar{\\mathbf{u}}$, the tangent-linear operator $\\mathbf{L}$ is assembled. Subsequently, its discrete adjoint $\\mathbf{L}^\\dagger$ is computed using the formula $\\mathbf{L}^\\dagger = \\mathbf{W}^{-1} \\mathbf{L}^\\top \\mathbf{W}$.\n\nThe final step is the verification, often called the \"adjoint check.\" For randomly generated perturbation vectors $\\mathbf{x}$ and $\\mathbf{y}$, the two sides of the adjoint identity are computed:\n- Left-hand side: $\\text{LHS} = \\langle \\mathbf{L}\\mathbf{x}, \\mathbf{y} \\rangle_{\\mathbf{W}} = (\\mathbf{L}\\mathbf{x})^\\top \\mathbf{W} \\mathbf{y}$\n- Right-hand side: $\\text{RHS} = \\langle \\mathbf{x}, \\mathbf{L}^\\dagger \\mathbf{y} \\rangle_{\\mathbf{W}} = \\mathbf{x}^\\top \\mathbf{W} (\\mathbf{L}^\\dagger \\mathbf{y})$\nThe numerical validity of the implemented adjoint is confirmed by checking if the absolute difference $\\delta = |\\text{LHS} - \\text{RHS}|$ is smaller than a prescribed numerical tolerance, $\\tau = 10^{-10}$. A deterministic pseudo-random number generator, seeded for reproducibility, is used to generate the vectors $\\bar{\\mathbf{u}}$, $\\mathbf{x}$, and $\\mathbf{y}$. The program iterates through the specified test cases, performing this verification for each and reporting the boolean outcome.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef build_derivative_matrices(N, h, boundary_type):\n    \"\"\"\n    Constructs discrete first and second derivative matrices for a 1D grid.\n\n    Args:\n        N (int): Number of grid points.\n        h (float): Grid spacing.\n        boundary_type (str): 'periodic' or 'neumann'.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: The Dx and Dxx matrices.\n    \"\"\"\n    Dx = np.zeros((N, N))\n    Dxx = np.zeros((N, N))\n\n    if boundary_type == 'periodic':\n        # Second-order centered differences with wrap-around indexing\n        for i in range(N):\n            im1 = (i - 1 + N) % N\n            ip1 = (i + 1) % N\n            Dx[i, ip1] = 1.0 / (2.0 * h)\n            Dx[i, im1] = -1.0 / (2.0 * h)\n            Dxx[i, ip1] = 1.0 / (h * h)\n            Dxx[i, i] = -2.0 / (h * h)\n            Dxx[i, im1] = 1.0 / (h * h)\n    elif boundary_type == 'neumann':\n        # Interior points use second-order centered differences\n        for i in range(1, N - 1):\n            Dx[i, i + 1] = 1.0 / (2.0 * h)\n            Dx[i, i - 1] = -1.0 / (2.0 * h)\n            Dxx[i, i + 1] = 1.0 / (h * h)\n            Dxx[i, i] = -2.0 / (h * h)\n            Dxx[i, i - 1] = 1.0 / (h * h)\n        \n        # Dx boundaries are zero to enforce zero gradient\n        # This is handled by initializing Dx to zeros and only filling the interior.\n\n        # Dxx boundaries use mirrored ghost points for zero flux\n        # At i = 0: (u_1 - 2u_0 + u_{-1})/h^2 with u_{-1}=u_1\n        Dxx[0, 1] = 2.0 / (h * h)\n        Dxx[0, 0] = -2.0 / (h * h)\n        # At i = N-1: (u_{N} - 2u_{N-1} + u_{N-2})/h^2 with u_{N}=u_{N-2}\n        Dxx[N - 1, N - 2] = 2.0 / (h * h)\n        Dxx[N - 1, N - 1] = -2.0 / (h * h)\n    else:\n        raise ValueError(f\"Invalid boundary type specified: {boundary_type}\")\n        \n    return Dx, Dxx\n\ndef run_adjoint_test(N, dt, c, K, gamma, boundary, weights_type, s):\n    \"\"\"\n    Runs a single adjoint test case.\n    \"\"\"\n    # Use a deterministic pseudo-random number generator with the specified seed\n    rng = np.random.default_rng(s)\n    \n    # Generate random state vectors from a standard normal distribution\n    u_bar = rng.standard_normal(N)\n    x = rng.standard_normal(N)\n    y = rng.standard_normal(N)\n    \n    # Determine grid spacing h based on boundary conditions\n    if boundary == 'periodic':\n        h = 1.0 / N\n    elif boundary == 'neumann':\n        h = 1.0 / (N - 1)\n    else:\n        raise ValueError(f\"Invalid boundary type: {boundary}\")\n\n    # Build derivative matrices\n    Dx, Dxx = build_derivative_matrices(N, h, boundary)\n    \n    # Build diagonal weighting matrix W\n    if weights_type == 'uniform':\n        w_diag = h * np.ones(N)\n    elif weights_type == 'nonuniform':\n        i = np.arange(N)\n        w_diag = h * (1.0 + 0.5 * np.sin(2.0 * np.pi * i / N))\n    else:\n        raise ValueError(f\"Invalid weights type: {weights_type}\")\n    \n    W = np.diag(w_diag)\n    \n    # Build tangent-linear operator L\n    L = np.identity(N) + dt * (-c * Dx + K * Dxx + 2.0 * gamma * np.diag(u_bar))\n    \n    # Build discrete adjoint L_dagger using the formula L_dagger = W^-1 * L^T * W\n    W_inv = np.diag(1.0 / w_diag)\n    L_dagger = W_inv @ L.T @ W\n    \n    # Compute the two sides of the adjoint property using the inner product <a, b>_W = a^T W b\n    # Left-hand side: <L*x, y>_W\n    lhs = (L @ x).T @ W @ y\n    \n    # Right-hand side: <x, L_dagger*y>_W\n    rhs = x.T @ W @ (L_dagger @ y)\n    \n    # Calculate the absolute difference\n    delta = np.abs(lhs - rhs)\n    \n    # Check if the difference is within the specified tolerance\n    tau = 1e-10\n    \n    return delta = tau\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, dt, c, K, gamma, boundary, weights_type, s)\n        (64, 1e-3, 0.4, 1e-2, 0.5, 'periodic', 'uniform', 1),\n        (128, 2e-3, 1.0, 0.0, 0.0, 'periodic', 'uniform', 2),\n        (50, 5e-3, 0.0, 5e-2, 0.0, 'neumann', 'uniform', 3),\n        (60, 1e-3, 0.2, 1e-2, 1.0, 'periodic', 'nonuniform', 4),\n        (32, 1e-6, 0.3, 2e-2, 0.7, 'neumann', 'uniform', 5),\n        (5, 1e-3, 0.5, 0.0, 0.0, 'periodic', 'uniform', 6),\n    ]\n\n    results = []\n    for case_params in test_cases:\n        # Run the adjoint test for the current set of parameters\n        result = run_adjoint_test(*case_params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once the adjoint model is verified, the next step is to assemble the full gradient of the 4D-Var cost function. A subtle error in either the model code, the adjoint code, or their integration can lead to an incorrect gradient and failed minimization. This hands-on practice  introduces the 'gradient check,' a non-negotiable step in developing assimilation systems, which validates the analytical gradient by comparing it to a finite-difference approximation.",
            "id": "3793675",
            "problem": "You must implement a rigorous gradient check for the cost function of Four-Dimensional Variational Data Assimilation (4D-Var) in computational oceanography. Begin from the definition of the 4D-Var cost function and a linear, one-dimensional periodic tracer model. You will compare finite-difference directional derivatives against adjoint-based gradients along random directions and decide acceptance using clear tolerances.\n\nSet up the following purely mathematical and numerically self-consistent problem in non-dimensional units. Consider a one-dimensional periodic domain with $n$ grid points and grid spacing $\\Delta x = 1/n$. Let $\\Delta t$ be the time step, $K$ the number of time steps, $c$ a constant advection velocity, and $\\nu$ a constant diffusion coefficient. The linear forward model updates the tracer state $x_k \\in \\mathbb{R}^n$ via\n$$\nx_{k+1} = M x_k,\n$$\nwhere the model matrix $M \\in \\mathbb{R}^{n \\times n}$ is defined by the explicit Euler discretization of advection-diffusion with periodic boundary conditions:\n$$\nM = I + \\Delta t \\left( A_{\\mathrm{adv}} + A_{\\mathrm{diff}} \\right),\n$$\nwith\n$$\nA_{\\mathrm{adv}} = -\\frac{c}{\\Delta x}\\left(I - S_{-}\\right), \\quad A_{\\mathrm{diff}} = \\frac{\\nu}{\\Delta x^2}\\left(S_{+} + S_{-} - 2I\\right).\n$$\nHere $I$ is the identity matrix in $\\mathbb{R}^{n \\times n}$, $S_{-}$ is the circulant shift matrix such that $(S_{-} x)_i = x_{i-1}$, and $S_{+}$ is the circulant shift matrix such that $(S_{+} x)_i = x_{i+1}$, with indices taken modulo $n$.\n\nObservations at each time $k \\in \\{1,\\dots,K\\}$ are defined by a linear observation operator $H \\in \\mathbb{R}^{m \\times n}$ that selects $m$ grid-point values from the state, and observation vectors $y_k \\in \\mathbb{R}^m$. The background (prior) state is $x_b \\in \\mathbb{R}^n$. Assume diagonal covariance matrices for background and observation errors: $B = \\sigma_b^2 I$ and $R = \\sigma_r^2 I$, so that $B^{-1} = \\sigma_b^{-2} I$ and $R^{-1} = \\sigma_r^{-2} I$.\n\nThe four-dimensional variational (4D-Var) cost function is\n$$\nJ(x_0) = \\frac{1}{2}(x_0 - x_b)^\\top B^{-1}(x_0 - x_b)\n+ \\frac{1}{2} \\sum_{k=1}^K \\left(H x_k - y_k\\right)^\\top R^{-1} \\left(H x_k - y_k\\right),\n$$\nwith $x_k = M^k x_0$.\n\nYour task is to perform a gradient check of $J$ with respect to $x_0$ by:\n- Computing the adjoint-based gradient of $J$ with respect to $x_0$ derived from first principles.\n- Drawing random directions $v \\in \\mathbb{R}^n$ with unit Euclidean norm $\\|v\\|_2 = 1$.\n- Computing the finite-difference directional derivative using the central difference formula:\n$$\nD_{\\mathrm{FD}}(v) = \\frac{J(x_0 + \\varepsilon v) - J(x_0 - \\varepsilon v)}{2\\varepsilon}.\n$$\n- Computing the adjoint-based directional derivative:\n$$\nD_{\\mathrm{ADJ}}(v) = \\nabla J(x_0)^\\top v.\n$$\n- For each direction, evaluating the absolute error $E_{\\mathrm{abs}} = |D_{\\mathrm{FD}}(v) - D_{\\mathrm{ADJ}}(v)|$ and the relative error\n$$\nE_{\\mathrm{rel}} = \\frac{|D_{\\mathrm{FD}}(v) - D_{\\mathrm{ADJ}}(v)|}{\\max\\left(1, |D_{\\mathrm{FD}}(v)|, |D_{\\mathrm{ADJ}}(v)|\\right)}.\n$$\n- Accepting the gradient check along a direction if either $E_{\\mathrm{rel}} \\le \\text{tol}_{\\mathrm{rel}}$ or $E_{\\mathrm{abs}} \\le \\text{tol}_{\\mathrm{abs}}$.\n\nConstruct $y_k$ and $x_b$ as follows for scientific realism:\n- Generate a \"truth\" initial condition $x_0^{\\mathrm{true}}$ with independent standard normal entries, propagate with the same $M$ to obtain $x_k^{\\mathrm{true}}$, and set $y_k = H x_k^{\\mathrm{true}} + \\sigma_r \\eta_k$ with independent standard normal $\\eta_k$.\n- Set $x_b = x_0^{\\mathrm{true}} + \\sigma_b \\xi$ with independent standard normal $\\xi$.\n- Evaluate the gradient check at $x_0 = x_b$.\n\nUse dimensionless units throughout. Angles are not involved. No percentages are involved.\n\nImplement a program that, for the parameter sets listed below, performs the above gradient check using independent random directions and returns, for each parameter set, a single boolean that is true if all tested directions in that set are accepted and false otherwise. The final output must be a single line containing the results across all parameter sets as a comma-separated list enclosed in square brackets.\n\nTest suite (each line is one parameter set; use an independent random seed per set to ensure reproducibility):\n- Case $1$: $n=32$, $K=6$, $\\Delta t=10^{-2}$, $c=2\\times 10^{-1}$, $\\nu=5\\times 10^{-3}$, $m=16$, $\\sigma_b=5\\times 10^{-1}$, $\\sigma_r=10^{-1}$, $\\varepsilon=10^{-6}$, $\\text{tol}_{\\mathrm{rel}}=10^{-7}$, $\\text{tol}_{\\mathrm{abs}}=10^{-10}$, number of directions $=5$.\n- Case $2$: $n=32$, $K=6$, $\\Delta t=10^{-2}$, $c=2\\times 10^{-1}$, $\\nu=5\\times 10^{-3}$, $m=16$, $\\sigma_b=5\\times 10^{-1}$, $\\sigma_r=10^{-1}$, $\\varepsilon=10^{-10}$, $\\text{tol}_{\\mathrm{rel}}=10^{-3}$, $\\text{tol}_{\\mathrm{abs}}=10^{-8}$, number of directions $=5$.\n- Case $3$: $n=32$, $K=6$, $\\Delta t=10^{-2}$, $c=2\\times 10^{-1}$, $\\nu=5\\times 10^{-3}$, $m=8$, $\\sigma_b=5\\times 10^{-1}$, $\\sigma_r=10^{2}$, $\\varepsilon=10^{-6}$, $\\text{tol}_{\\mathrm{rel}}=10^{-7}$, $\\text{tol}_{\\mathrm{abs}}=10^{-10}$, number of directions $=5$.\n- Case $4$: $n=40$, $K=10$, $\\Delta t=5\\times 10^{-3}$, $c=5\\times 10^{-1}$, $\\nu=5\\times 10^{-3}$, $m=40$, $\\sigma_b=2\\times 10^{-1}$, $\\sigma_r=5\\times 10^{-2}$, $\\varepsilon=10^{-6}$, $\\text{tol}_{\\mathrm{rel}}=10^{-7}$, $\\text{tol}_{\\mathrm{abs}}=10^{-10}$, number of directions $=5$.\n- Case $5$: $n=24$, $K=5$, $\\Delta t=10^{-2}$, $c=3\\times 10^{-1}$, $\\nu=4\\times 10^{-3}$, $m=12$, $\\sigma_b=10^{-2}$, $\\sigma_r=2\\times 10^{-1}$, $\\varepsilon=10^{-6}$, $\\text{tol}_{\\mathrm{rel}}=10^{-7}$, $\\text{tol}_{\\mathrm{abs}}=10^{-10}$, number of directions $=5$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3,r_4,r_5]$ where each $r_i$ is a boolean for Case $i$).",
            "solution": "The user-provided problem is assessed to be **valid**. It is a well-posed, scientifically grounded problem in the field of computational data assimilation that is free of ambiguity, contradiction, and factual error. The task is to implement a standard numerical procedure, a gradient check, for a 4D-Var data assimilation system. We will proceed with a full solution.\n\nOur objective is to derive the analytical gradient of the 4D-Var cost function $J(x_0)$ with respect to the initial state $x_0$. This analytical gradient, computed using the adjoint method, will then be compared against a finite-difference approximation to verify its correctness.\n\nThe cost function is given by:\n$$\nJ(x_0) = \\frac{1}{2}(x_0 - x_b)^\\top B^{-1}(x_0 - x_b)\n+ \\frac{1}{2} \\sum_{k=1}^K \\left(H x_k - y_k\\right)^\\top R^{-1} \\left(H x_k - y_k\\right)\n$$\nwhere the state $x_k$ at time step $k$ is related to the initial state $x_0$ by the linear forward model $x_k = M^k x_0$. The cost function consists of two terms: a background term $J_b(x_0)$ that measures the distance to a prior estimate $x_b$, and an observation term $J_o(x_0)$ that measures the misfit to observations $y_k$ over a time window.\n\n$$\nJ_b(x_0) = \\frac{1}{2}(x_0 - x_b)^\\top B^{-1}(x_0 - x_b)\n$$\n$$\nJ_o(x_0) = \\frac{1}{2} \\sum_{k=1}^K \\left(H M^k x_0 - y_k\\right)^\\top R^{-1} \\left(H M^k x_0 - y_k\\right)\n$$\n\nThe gradient $\\nabla J(x_0)$ is the sum of the gradients of these two terms: $\\nabla J(x_0) = \\nabla J_b(x_0) + \\nabla J_o(x_0)$.\n\n1.  **Gradient of the Background Term**\n    The background term $J_b(x_0)$ is a standard quadratic form. Its gradient with respect to $x_0$ is straightforward to compute:\n    $$\n    \\nabla J_b(x_0) = B^{-1}(x_0 - x_b)\n    $$\n    Given $B = \\sigma_b^2 I$, the inverse is $B^{-1} = \\sigma_b^{-2} I$, so this term simplifies to $\\sigma_b^{-2}(x_0 - x_b)$.\n\n2.  **Gradient of the Observation Term (Adjoint Method)**\n    To find $\\nabla J_o(x_0)$, we use the method of Lagrange multipliers, which gives rise to the adjoint model. We treat the model equations $x_{k+1} = M x_k$ for $k=0, \\dots, K-1$ as constraints. The Lagrangian $\\mathcal{L}$ is:\n    $$\n    \\mathcal{L}(\\{x_k\\}_{k=0}^K, \\{\\lambda_k\\}_{k=1}^K) = J(\\{x_k\\}) + \\sum_{k=0}^{K-1} \\lambda_{k+1}^\\top (M x_k - x_{k+1})\n    $$\n    Here, the cost function is expressed in terms of the full state trajectory $\\{x_k\\}$. The vectors $\\lambda_k$ are the Lagrange multipliers, also known as the adjoint variables. At an optimal point, the gradient of the Lagrangian with respect to all its variables is zero. The gradient of $J(x_0)$ is equivalent to the total derivative of the constrained cost function, $\\frac{dJ}{dx_0}$. We can find this by evaluating $\\frac{\\partial \\mathcal{L}}{\\partial x_0}$ after imposing the conditions $\\frac{\\partial \\mathcal{L}}{\\partial x_k} = 0$ for $k=1, \\dots, K$.\n\n    Taking the partial derivative of $\\mathcal{L}$ with respect to a state $x_k$ for $k \\in \\{1, \\dots, K\\}$ yields:\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial x_k} = \\frac{\\partial J}{\\partial x_k} + M^\\top \\lambda_{k+1} - \\lambda_k = H^\\top R^{-1} (H x_k - y_k) + M^\\top \\lambda_{k+1} - \\lambda_k\n    $$\n    Setting this to zero gives the adjoint equations. For $k=K$, the term $\\lambda_{K+1}$ is not present in the sum, so we define $\\lambda_{K+1}=0$. The derivative with respect to $x_K$ is:\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial x_K} = H^\\top R^{-1} (H x_K - y_K) - \\lambda_K = 0 \\implies \\lambda_K = H^\\top R^{-1} (H x_K - y_K)\n    $$\n    For $k \\in \\{1, \\dots, K-1\\}$, we have:\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial x_k} = 0 \\implies \\lambda_k = M^\\top \\lambda_{k+1} + H^\\top R^{-1} (H x_k - y_k)\n    $$\n    These equations define a backward recurrence for the adjoint variables, starting from $\\lambda_K$ and propagating backward in time to $\\lambda_1$. Note that since $R = \\sigma_r^2 I$, the term $H^\\top R^{-1} (H x_k - y_k)$ simplifies to $\\sigma_r^{-2} H^\\top (H x_k - y_k)$.\n\n    Finally, the gradient of the cost function with respect to the initial state $x_0$ is obtained by differentiating $\\mathcal{L}$ with respect to $x_0$:\n    $$\n    \\nabla J(x_0) = \\frac{d\\mathcal{L}}{dx_0} = \\frac{\\partial\\mathcal{L}}{\\partial x_0} = \\frac{\\partial J_b}{\\partial x_0} + M^\\top \\lambda_1 = B^{-1}(x_0 - x_b) + M^\\top \\lambda_1\n    $$\n\n3.  **Algorithmic Summary for Gradient Calculation**\n    The procedure to calculate $\\nabla J(x_0)$ is as follows:\n    a.  **Forward Run**: Starting with $x_0$, integrate the forward model $x_{k+1} = M x_k$ for $k = 0, \\dots, K-1$ and store the entire state trajectory $\\{x_k\\}_{k=1}^K$.\n    b.  **Backward (Adjoint) Run**:\n        i.  Initialize the adjoint variable at the final time $K$: $\\lambda_K = \\sigma_r^{-2} H^\\top (H x_K - y_K)$.\n        ii. Integrate the adjoint model backward in time from $k=K-1$ down to $1$: $\\lambda_k = M^\\top \\lambda_{k+1} + \\sigma_r^{-2} H^\\top (H x_k - y_k)$.\n    c.  **Gradient Assembly**: Compute the final gradient using the result from the backward run: $\\nabla J(x_0) = \\sigma_b^{-2}(x_0 - x_b) + M^\\top \\lambda_1$.\n\n4.  **Gradient Check Procedure**\n    The gradient check validates the analytical gradient by comparing its projection along a random direction $v$ with a finite-difference approximation of the directional derivative.\n    -   The adjoint-based directional derivative is $D_{\\mathrm{ADJ}}(v) = \\nabla J(x_0)^\\top v$.\n    -   The finite-difference approximation using a central difference scheme is $D_{\\mathrm{FD}}(v) = \\frac{J(x_0 + \\varepsilon v) - J(x_0 - \\varepsilon v)}{2\\varepsilon}$.\n    The implementation will compute both quantities for several random unit vectors $v$, calculate the absolute error $E_{\\mathrm{abs}}$ and relative error $E_{\\mathrm{rel}}$, and accept the gradient if the errors are within the specified tolerances $\\text{tol}_{\\mathrm{abs}}$ and $\\text{tol}_{\\mathrm{rel}}$. The overall check for a parameter set passes only if all tested random directions are accepted.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the gradient check for all test cases and print the results.\n    \"\"\"\n\n    def run_gradient_check(n, K, dt, c, nu, m, sigma_b, sigma_r, epsilon, tol_rel, tol_abs, num_directions, seed):\n        \"\"\"\n        Performs a gradient check for a 4D-Var cost function for a single parameter set.\n        \"\"\"\n        # 1. Setup\n        np.random.seed(seed)\n        dx = 1.0 / n\n\n        # 2. Build matrices M, and define H operation\n        I = np.eye(n)\n        # (S- @ x)_i = x_{i-1}. Matrix equivalent is np.roll(I, 1, axis=0)\n        S_minus = np.roll(I, 1, axis=0)\n        # (S+ @ x)_i = x_{i+1}. Matrix equivalent is np.roll(I, -1, axis=0)\n        S_plus = np.roll(I, -1, axis=0)\n        \n        A_adv = -(c / dx) * (I - S_minus)\n        A_diff = (nu / dx**2) * (S_plus + S_minus - 2 * I)\n        M = I + dt * (A_adv + A_diff)\n        M_T = M.T\n        \n        obs_indices = np.linspace(0, n - 1, num=m, dtype=int)\n\n        # 3. Generate data (truth, obs, background)\n        x0_true = np.random.randn(n)\n        \n        y_obs_list = []\n        x_k_true = np.copy(x0_true)\n        for _ in range(K):\n            x_k_true = M @ x_k_true\n            obs_noise = np.random.randn(m)\n            y_k = x_k_true[obs_indices] + sigma_r * obs_noise\n            y_obs_list.append(y_k)\n            \n        bg_noise = np.random.randn(n)\n        x_b = x0_true + sigma_b * bg_noise\n        \n        # 4. Define cost and gradient functions\n        inv_sigma_b2 = 1.0 / sigma_b**2\n        inv_sigma_r2 = 1.0 / sigma_r**2\n\n        def calculate_J(x0):\n            j_b = 0.5 * inv_sigma_b2 * np.sum((x0 - x_b)**2)\n            \n            j_o = 0.0\n            x_k = np.copy(x0)\n            for k in range(K):\n                x_k = M @ x_k\n                innov = x_k[obs_indices] - y_obs_list[k]\n                j_o += 0.5 * inv_sigma_r2 * np.sum(innov**2)\n                \n            return j_b + j_o\n\n        def calculate_grad_J(x0):\n            # Forward run: store trajectory\n            x_traj = [x0]\n            x_k = np.copy(x0)\n            for _ in range(K):\n                x_k = M @ x_k\n                x_traj.append(x_k)\n\n            # Backward run: compute adjoint variable\n            # Initialize lambda_K\n            x_K = x_traj[K]\n            y_K = y_obs_list[K-1] # y_obs_list is 0-indexed for k=1..K\n            innov_K = x_K[obs_indices] - y_K\n            forcing_term_K = np.zeros(n)\n            forcing_term_K[obs_indices] = inv_sigma_r2 * innov_K\n            lambda_next = forcing_term_K # This is lambda_K\n\n            # Loop for k from K-1 down to 1\n            for k in range(K - 1, 0, -1):\n                x_k = x_traj[k]\n                y_k = y_obs_list[k-1]\n                \n                innov_k = x_k[obs_indices] - y_k\n                forcing_term_k = np.zeros(n)\n                forcing_term_k[obs_indices] = inv_sigma_r2 * innov_k\n                \n                lambda_current = M_T @ lambda_next + forcing_term_k\n                lambda_next = lambda_current\n\n            # After loop, lambda_next is lambda_1\n            lambda_1 = lambda_next\n            \n            grad_b = inv_sigma_b2 * (x0 - x_b)\n            grad_o = M_T @ lambda_1\n            \n            return grad_b + grad_o\n\n        # 5. Perform the gradient check at x0 = x_b\n        x0_eval = x_b\n        grad_adj = calculate_grad_J(x0_eval)\n        \n        for i in range(num_directions):\n            v = np.random.randn(n)\n            v /= np.linalg.norm(v)\n\n            J_plus = calculate_J(x0_eval + epsilon * v)\n            J_minus = calculate_J(x0_eval - epsilon * v)\n\n            D_fd = (J_plus - J_minus) / (2.0 * epsilon)\n            D_adj = grad_adj.T @ v\n\n            E_abs = np.abs(D_fd - D_adj)\n            denom = max(1.0, np.abs(D_fd), np.abs(D_adj))\n            E_rel = E_abs / denom\n\n            if not (E_rel = tol_rel or E_abs = tol_abs):\n                return False # Failed test for this direction\n        \n        return True # All directions passed\n\n    test_cases = [\n        # n, K, dt, c, nu, m, sigma_b, sigma_r, epsilon, tol_rel, tol_abs, num_directions\n        (32, 6, 1e-2, 2e-1, 5e-3, 16, 5e-1, 1e-1, 1e-6, 1e-7, 1e-10, 5),\n        (32, 6, 1e-2, 2e-1, 5e-3, 16, 5e-1, 1e-1, 1e-10, 1e-3, 1e-8, 5),\n        (32, 6, 1e-2, 2e-1, 5e-3, 8, 5e-1, 1e2, 1e-6, 1e-7, 1e-10, 5),\n        (40, 10, 5e-3, 5e-1, 5e-3, 40, 2e-1, 5e-2, 1e-6, 1e-7, 1e-10, 5),\n        (24, 5, 1e-2, 3e-1, 4e-3, 12, 1e-2, 2e-1, 1e-6, 1e-7, 1e-10, 5)\n    ]\n\n    results = []\n    for i, case_params in enumerate(test_cases):\n        # Use an independent random seed for each test case for reproducibility\n        result = run_gradient_check(*case_params, seed=i)\n        results.append(result)\n\n    # Format output as a lowercase boolean list: [true,false,...]\n    print(f\"[{','.join(map(lambda b: str(b).lower(), results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Possessing a correct gradient allows us to search for the minimum of the cost function, which corresponds to the optimal analysis. This minimization is a sophisticated numerical task, for which methods like the trust-region algorithm provide robustness and efficiency. In this final practice , you will implement a single step of this powerful optimization technique, gaining insight into how the gradient and Hessian information are used to iteratively and safely approach the best estimate of the ocean state.",
            "id": "3793624",
            "problem": "Consider the linear four-dimensional variational data assimilation cost function defined on a state vector $\\mathbf{x} \\in \\mathbb{R}^n$ by\n$$\nJ(\\mathbf{x}) = \\tfrac{1}{2}(\\mathbf{x}-\\mathbf{x}_b)^\\top \\mathbf{B}^{-1}(\\mathbf{x}-\\mathbf{x}_b) + \\tfrac{1}{2}\\sum_{t=1}^{T}\\left(\\mathbf{H}\\mathbf{M}_t\\mathbf{x} - \\mathbf{y}_t\\right)^\\top \\mathbf{R}^{-1}\\left(\\mathbf{H}\\mathbf{M}_t\\mathbf{x} - \\mathbf{y}_t\\right),\n$$\nwhere $\\mathbf{x}_b \\in \\mathbb{R}^n$ is the background state, $\\mathbf{B} \\in \\mathbb{R}^{n \\times n}$ is the background error covariance matrix, $\\mathbf{M}_t \\in \\mathbb{R}^{n \\times n}$ is the linear model propagator at time index $t$, $\\mathbf{H} \\in \\mathbb{R}^{m \\times n}$ is the linear observation operator, $\\mathbf{y}_t \\in \\mathbb{R}^m$ is the observation vector at time index $t$, and $\\mathbf{R} \\in \\mathbb{R}^{m \\times m}$ is the observation error covariance matrix. For given $\\mathbf{x}_k$, define the quadratic model\n$$\nm(\\mathbf{p}) = J(\\mathbf{x}_k) + \\mathbf{g}^\\top \\mathbf{p} + \\tfrac{1}{2}\\mathbf{p}^\\top \\mathbf{H}_{\\text{approx}} \\mathbf{p},\n$$\nwhere $\\mathbf{g} = \\nabla J(\\mathbf{x}_k)$ and $\\mathbf{H}_{\\text{approx}}$ is a positive definite approximation to the Hessian of $J$ at $\\mathbf{x}_k$. The trust-region subproblem seeks\n$$\n\\min_{\\mathbf{p} \\in \\mathbb{R}^n} \\quad \\tfrac{1}{2}\\mathbf{p}^\\top \\mathbf{H}_{\\text{approx}} \\mathbf{p} + \\mathbf{g}^\\top \\mathbf{p} \\quad \\text{subject to} \\quad \\|\\mathbf{p}\\|_2 \\le \\Delta,\n$$\nfor a given trust-region radius $\\Delta  0$. The predicted reduction is defined as\n$$\n\\text{pred} = m(\\mathbf{0}) - m(\\mathbf{p}) = -\\left(\\mathbf{g}^\\top \\mathbf{p} + \\tfrac{1}{2}\\mathbf{p}^\\top \\mathbf{H}_{\\text{approx}} \\mathbf{p}\\right),\n$$\nand the actual reduction is\n$$\n\\text{ared} = J(\\mathbf{x}_k) - J(\\mathbf{x}_k + \\mathbf{p}).\n$$\nThe ratio $\\rho$ monitoring convergence is \n$$\n\\rho = \\frac{\\text{ared}}{\\text{pred}},\n$$\nwith the convention that if $\\text{pred}$ is numerically zero (specified below), the reported $\\rho$ must be $0.0$.\n\nStarting from the definitions above and the linearity of $\\mathbf{M}_t$ and $\\mathbf{H}$, it follows that the gradient and the exact Hessian for $J$ at any $\\mathbf{x}$ are\n$$\n\\mathbf{g}(\\mathbf{x}) = \\mathbf{B}^{-1}(\\mathbf{x}-\\mathbf{x}_b) + \\sum_{t=1}^{T}\\mathbf{M}_t^\\top \\mathbf{H}^\\top \\mathbf{R}^{-1}\\left(\\mathbf{H}\\mathbf{M}_t\\mathbf{x} - \\mathbf{y}_t\\right),\n\\quad\n\\mathbf{H}_{\\text{true}} = \\mathbf{B}^{-1} + \\sum_{t=1}^{T}\\mathbf{M}_t^\\top \\mathbf{H}^\\top \\mathbf{R}^{-1}\\mathbf{H}\\mathbf{M}_t.\n$$\nIn this task, set the Hessian approximation to \n$$\n\\mathbf{H}_{\\text{approx}} = \\mathbf{H}_{\\text{true}} + \\alpha \\mathbf{I},\n$$\nfor a given scalar $\\alpha \\ge 0$, where $\\mathbf{I}$ is the identity matrix, to emulate Gauss-Newton damping or regularization. The trust-region step $\\mathbf{p}$ must be computed by solving the first-order optimality conditions\n$$\n(\\mathbf{H}_{\\text{approx}} + \\lambda \\mathbf{I})\\mathbf{p} = -\\mathbf{g}, \\quad \\lambda \\ge 0, \\quad \\|\\mathbf{p}\\|_2 \\le \\Delta, \\quad \\lambda(\\|\\mathbf{p}\\|_2 - \\Delta) = 0,\n$$\nusing a robust algorithm based on scalar root-finding for $\\lambda$ and linear solves, ensuring numerical stability for positive definite $\\mathbf{H}_{\\text{approx}}$. Then evaluate $\\text{pred}$ and $\\text{ared}$ and compute $\\rho$. If $\\text{pred}  \\epsilon$ for tolerance $\\epsilon = 10^{-12}$, report $\\rho = 0.0$.\n\nImplement a program that carries out this procedure for the following test suite. All matrices and vectors are specified in $\\mathbb{R}$ and all numbers are to be treated as dimensionless. Use the Euclidean norm for $\\|\\cdot\\|_2$.\n\nBase definitions common to all test cases:\n- Dimension $n = 3$, observation dimension $m = 2$, and number of time steps $T = 2$.\n- Background state $\\mathbf{x}_b = [\\,0.5,\\,-0.2,\\,0.1\\,]^\\top$.\n- Background covariance $\\mathbf{B} = \\operatorname{diag}([\\,0.4^2,\\,0.3^2,\\,0.5^2\\,])$ so that $\\mathbf{B}^{-1} = \\operatorname{diag}([\\,6.25,\\,11.\\overline{1},\\,4.0\\,])$.\n- Observation covariance $\\mathbf{R} = \\operatorname{diag}([\\,0.2^2,\\,0.3^2\\,])$ so that $\\mathbf{R}^{-1} = \\operatorname{diag}([\\,25.0,\\,11.\\overline{1}\\,])$.\n- Observation operator\n$$\n\\mathbf{H} = \\begin{bmatrix}\n1.0  0.0  0.0 \\\\\n0.0  1.0  0.3\n\\end{bmatrix}.\n$$\n- Linear model propagators\n$$\n\\mathbf{M}_1 = \\begin{bmatrix}\n1.0  0.1  0.0 \\\\\n0.0  0.9  0.2 \\\\\n0.0  0.0  1.0\n\\end{bmatrix},\n\\quad\n\\mathbf{M}_2 = \\begin{bmatrix}\n0.9  0.0  0.1 \\\\\n0.1  1.0  0.0 \\\\\n0.0  0.2  0.95\n\\end{bmatrix}.\n$$\n- Nominal observations for general cases:\n$$\n\\mathbf{y}_1 = \\begin{bmatrix} 0.7 \\\\ -0.15 \\end{bmatrix}, \\quad\n\\mathbf{y}_2 = \\begin{bmatrix} 0.6 \\\\ -0.05 \\end{bmatrix}.\n$$\n\nDefine the four test cases as follows:\n- Case $1$ (happy path): $\\mathbf{x}_k = [\\,0.6,\\,-0.25,\\,0.0\\,]^\\top$, trust-region radius $\\Delta = 1.5$, damping $\\alpha = 0.0$, observations $\\mathbf{y}_1$ and $\\mathbf{y}_2$ as specified above.\n- Case $2$ (gradient nearly zero): $\\mathbf{x}_k = \\mathbf{x}_b$, trust-region radius $\\Delta = 1.0$, damping $\\alpha = 0.0$, and observations chosen consistent with the background, i.e., $\\mathbf{y}_t = \\mathbf{H}\\mathbf{M}_t\\mathbf{x}_b$ for $t \\in \\{1,2\\}$.\n- Case $3$ (tight trust region with damping): $\\mathbf{x}_k = [\\,0.0,\\,0.0,\\,0.0\\,]^\\top$, trust-region radius $\\Delta = 0.1$, damping $\\alpha = 0.5$, observations $\\mathbf{y}_1$ and $\\mathbf{y}_2$ as specified above.\n- Case $4$ (strong damping): $\\mathbf{x}_k = [\\,-0.3,\\,0.4,\\,-0.1\\,]^\\top$, trust-region radius $\\Delta = 0.8$, damping $\\alpha = 2.0$, observations $\\mathbf{y}_1$ and $\\mathbf{y}_2$ as specified above.\n\nProgram requirements:\n- Compute $\\mathbf{g}$ and $\\mathbf{H}_{\\text{true}}$ using the definitions above.\n- Form $\\mathbf{H}_{\\text{approx}} = \\mathbf{H}_{\\text{true}} + \\alpha \\mathbf{I}$.\n- Compute the trust-region step $\\mathbf{p}$ by solving $(\\mathbf{H}_{\\text{approx}} + \\lambda \\mathbf{I})\\mathbf{p} = -\\mathbf{g}$ with $\\lambda \\ge 0$ such that $\\|\\mathbf{p}\\|_2 \\le \\Delta$ and $\\lambda(\\|\\mathbf{p}\\|_2 - \\Delta) = 0$ via a robust scalar search for $\\lambda$.\n- Evaluate $\\text{pred}$ and $\\text{ared}$ and compute $\\rho = \\text{ared}/\\text{pred}$, with the convention that if $\\text{pred}  \\epsilon$ for $\\epsilon = 10^{-12}$, report $\\rho = 0.0$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, i.e., $[\\,\\rho_1,\\rho_2,\\rho_3,\\rho_4\\,]$, where $\\rho_i$ corresponds to Case $i$.\n\nAll computations are purely mathematical and dimensionless; no physical units are required. Angles are not used. Percentages must not be used; any ratios must be reported as decimal floating-point numbers as specified.",
            "solution": "The user has provided a well-defined computational problem in the domain of four-dimensional variational data assimilation (4D-Var), a subfield of computational oceanography and numerical weather prediction. The problem is scientifically sound, mathematically consistent, and all necessary data and definitions are provided. It is a valid problem.\n\nThe task is to compute the ratio $\\rho = \\text{ared} / \\text{pred}$ for four different test cases. This ratio is a standard metric in trust-region optimization methods, used to assess the quality of a quadratic model of an objective function. The procedure involves several steps: defining the cost function and its derivatives, solving a trust-region subproblem to find an optimal step, and then evaluating the actual and predicted reduction in the cost function.\n\n### Step 1: Definition of the Cost Function and its Derivatives\n\nThe 4D-Var cost function $J(\\mathbf{x})$ is a quadratic function of the state vector $\\mathbf{x} \\in \\mathbb{R}^n$:\n$$\nJ(\\mathbf{x}) = \\tfrac{1}{2}(\\mathbf{x}-\\mathbf{x}_b)^\\top \\mathbf{B}^{-1}(\\mathbf{x}-\\mathbf{x}_b) + \\tfrac{1}{2}\\sum_{t=1}^{T}\\left(\\mathbf{H}\\mathbf{M}_t\\mathbf{x} - \\mathbf{y}_t\\right)^\\top \\mathbf{R}^{-1}\\left(\\mathbf{H}\\mathbf{M}_t\\mathbf{x} - \\mathbf{y}_t\\right)\n$$\nAll matrices and vectors are defined in the problem statement. Given that $J(\\mathbf{x})$ is quadratic, its gradient $\\mathbf{g}(\\mathbf{x}) = \\nabla J(\\mathbf{x})$ is a linear function of $\\mathbf{x}$, and its Hessian $\\mathbf{H}_{\\text{true}} = \\nabla^2 J(\\mathbf{x})$ is a constant matrix. The expressions provided are correct:\n\nThe gradient at a point $\\mathbf{x}$ is:\n$$\n\\mathbf{g}(\\mathbf{x}) = \\mathbf{B}^{-1}(\\mathbf{x}-\\mathbf{x}_b) + \\sum_{t=1}^{T}\\mathbf{M}_t^\\top \\mathbf{H}^\\top \\mathbf{R}^{-1}\\left(\\mathbf{H}\\mathbf{M}_t\\mathbf{x} - \\mathbf{y}_t\\right)\n$$\n\nThe exact Hessian is constant for all $\\mathbf{x}$:\n$$\n\\mathbf{H}_{\\text{true}} = \\mathbf{B}^{-1} + \\sum_{t=1}^{T}\\mathbf{M}_t^\\top \\mathbf{H}^\\top \\mathbf{R}^{-1}\\mathbf{H}\\mathbf{M}_t\n$$\n\nFor each test case, we first compute the gradient $\\mathbf{g} = \\mathbf{g}(\\mathbf{x}_k)$ at the given iterate $\\mathbf{x}_k$. The Hessian approximation is then formed as $\\mathbf{H}_{\\text{approx}} = \\mathbf{H}_{\\text{true}} + \\alpha \\mathbf{I}$, where $\\mathbf{H}_{\\text{true}}$ can be pre-computed as it is independent of $\\mathbf{x}_k$ and the observations $\\mathbf{y}_t$.\n\n### Step 2: Solving the Trust-Region Subproblem\n\nThe core of the task is to solve the trust-region subproblem for the step $\\mathbf{p}$:\n$$\n\\min_{\\mathbf{p} \\in \\mathbb{R}^n} \\quad \\tfrac{1}{2}\\mathbf{p}^\\top \\mathbf{H}_{\\text{approx}} \\mathbf{p} + \\mathbf{g}^\\top \\mathbf{p} \\quad \\text{subject to} \\quad \\|\\mathbf{p}\\|_2 \\le \\Delta\n$$\nThe solution $\\mathbf{p}$ must satisfy the Karush-Kuhn-Tucker (KKT) conditions:\n$$\n(\\mathbf{H}_{\\text{approx}} + \\lambda \\mathbf{I})\\mathbf{p} = -\\mathbf{g}, \\quad \\lambda \\ge 0, \\quad \\|\\mathbf{p}\\|_2 \\le \\Delta, \\quad \\lambda(\\|\\mathbf{p}\\|_2 - \\Delta) = 0\n$$\nSince $\\mathbf{B}$ and $\\mathbf{R}$ are covariance matrices, they are symmetric positive definite, and so are their inverses. This guarantees that $\\mathbf{H}_{\\text{true}}$ is positive definite. With $\\alpha \\ge 0$, $\\mathbf{H}_{\\text{approx}}$ is also positive definite. This simplifies the solution procedure, as we do not need to consider the so-called \"hard case\" of the trust-region subproblem.\n\nThe solution algorithm is as follows:\n1.  **Check for trivial solution**: If the gradient norm $\\|\\mathbf{g}\\|_2$ is close to zero, the current point $\\mathbf{x}_k$ is already optimal. The step is $\\mathbf{p} = \\mathbf{0}$. This applies to Case 2.\n2.  **Interior Solution**: First, attempt to find the unconstrained minimizer by setting $\\lambda=0$. This gives the Newton step $\\mathbf{p}_N = -\\mathbf{H}_{\\text{approx}}^{-1}\\mathbf{g}$. If $\\|\\mathbf{p}_N\\|_2 \\le \\Delta$, then $\\mathbf{p}=\\mathbf{p}_N$ is the solution, and the trust-region constraint is inactive.\n3.  **Boundary Solution**: If $\\|\\mathbf{p}_N\\|_2  \\Delta$, the solution must lie on the boundary of the trust region, i.e., $\\|\\mathbfp\\|_2 = \\Delta$. This requires finding a $\\lambda  0$ that satisfies the secular equation $\\|\\mathbf{p}(\\lambda)\\|_2 = \\Delta$, where $\\mathbf{p}(\\lambda) = -(\\mathbf{H}_{\\text{approx}} + \\lambda\\mathbf{I})^{-1}\\mathbf{g}$. The function $\\phi(\\lambda) = \\|\\mathbf{p}(\\lambda)\\|_2 - \\Delta$ is a monotonically decreasing function for $\\lambda \\ge 0$. We can find the unique root $\\lambda^*  0$ of $\\phi(\\lambda) = 0$ using a numerical root-finding algorithm, such as the Brent-Dekker method (`brentq` in SciPy). The search for $\\lambda^*$ is performed on an interval $[a, b]$ where $\\phi(a)0$ and $\\phi(b)0$. We know $\\phi(0)0$, and a sufficiently large value can be found for $b$.\n\n### Step 3: Calculation of $\\rho$\n\nOnce the step $\\mathbf{p}$ is determined, the predicted and actual reductions are calculated.\n\nThe predicted reduction is based on the quadratic model of $J$:\n$$\n\\text{pred} = -\\left(\\mathbf{g}^\\top \\mathbf{p} + \\tfrac{1}{2}\\mathbf{p}^\\top \\mathbf{H}_{\\text{approx}} \\mathbf{p}\\right)\n$$\n\nThe actual reduction is based on the true cost function $J$:\n$$\n\\text{ared} = J(\\mathbf{x}_k) - J(\\mathbf{x}_k + \\mathbf{p})\n$$\n\nFinally, the ratio $\\rho$ is computed:\n$$\n\\rho = \\frac{\\text{ared}}{\\text{pred}}\n$$\nAs per the problem specification, if $|\\text{pred}|  10^{-12}$, we set $\\rho = 0.0$ to avoid division by a near-zero number. This is relevant for Case 2, where $\\mathbf{g}=\\mathbf{0}$ leads to $\\mathbf{p}=\\mathbf{0}$ and $\\text{pred}=0$.\n\nThis entire procedure is implemented for each of the four test cases, yielding four values for $\\rho$. The implementation uses `numpy` for linear algebra operations and `scipy.optimize.brentq` for the scalar root-finding. All calculations are performed using double-precision floating-point arithmetic.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Solves the 4D-Var trust-region problem for the four specified test cases.\n    \"\"\"\n\n    # 1. Define constants and common matrices\n    n, m, T = 3, 2, 2\n    xb = np.array([0.5, -0.2, 0.1])\n    Binv = np.diag([6.25, 100.0 / 9.0, 4.0])\n    Rinv = np.diag([25.0, 100.0 / 9.0])\n    I = np.eye(n)\n\n    H_op = np.array([[1.0, 0.0, 0.0],\n                     [0.0, 1.0, 0.3]])\n\n    M1 = np.array([[1.0, 0.1, 0.0],\n                   [0.0, 0.9, 0.2],\n                   [0.0, 0.0, 1.0]])\n\n    M2 = np.array([[0.9, 0.0, 0.1],\n                   [0.1, 1.0, 0.0],\n                   [0.0, 0.2, 0.95]])\n\n    # 2. Pre-compute constant matrices\n    C1 = H_op @ M1\n    C2 = H_op @ M2\n    C1T = C1.T\n    C2T = C2.T\n\n    # H_true = B_inv + C1^T R_inv C1 + C2^T R_inv C2\n    H_true = Binv + C1T @ Rinv @ C1 + C2T @ Rinv @ C2\n\n    # 3. Define helper functions that capture the constants from the outer scope\n    def J_cost(x, y1, y2):\n        term_b = 0.5 * (x - xb).T @ Binv @ (x - xb)\n        term_o1 = 0.5 * (C1 @ x - y1).T @ Rinv @ (C1 @ x - y1)\n        term_o2 = 0.5 * (C2 @ x - y2).T @ Rinv @ (C2 @ x - y2)\n        return term_b + term_o1 + term_o2\n\n    def gradient(x, y1, y2):\n        term_b = Binv @ (x - xb)\n        term_o1 = C1T @ Rinv @ (C1 @ x - y1)\n        term_o2 = C2T @ Rinv @ (C2 @ x - y2)\n        return term_b + term_o1 + term_o2\n\n    def solve_trust_region_subproblem(H_approx, g, delta):\n        g_norm = np.linalg.norm(g)\n        if g_norm  1e-15:\n            return np.zeros_like(g)\n\n        # Try interior solution (lambda = 0)\n        try:\n            p0 = np.linalg.solve(H_approx, -g)\n            if np.linalg.norm(p0) = delta:\n                return p0\n        except np.linalg.LinAlgError:\n            # H_approx is guaranteed to be PD in this problem, but this is good practice.\n            pass\n\n        # Boundary solution: find lambda > 0 such that ||p(lambda)||_2 = delta\n        def phi(lmbda):\n            mat = H_approx + lmbda * I\n            try:\n                p_lambda = np.linalg.solve(mat, -g)\n                return np.linalg.norm(p_lambda) - delta\n            except np.linalg.LinAlgError:\n                # Should not happen for lambda >= 0 as H_approx is PD\n                return 1e9\n\n        lambda_lower = 0.0\n        # Establish an upper bound for the root finding search\n        lambda_upper = g_norm / delta if delta > 0 else g_norm\n        if lambda_upper == 0: lambda_upper = 1.0 # Avoid zero upper bound\n        \n        while phi(lambda_upper) > 0:\n            lambda_upper *= 2\n\n        lambda_sol = brentq(phi, lambda_lower, lambda_upper)\n        p = np.linalg.solve(H_approx + lambda_sol * I, -g)\n        return p\n\n    # 4. Define and process test cases\n    y1_nom = np.array([0.7, -0.15])\n    y2_nom = np.array([0.6, -0.05])\n    \n    y1_case2 = C1 @ xb\n    y2_case2 = C2 @ xb\n\n    test_cases = [\n        {'xk': np.array([0.6, -0.25, 0.0]), 'delta': 1.5, 'alpha': 0.0, 'y1': y1_nom, 'y2': y2_nom},\n        {'xk': xb, 'delta': 1.0, 'alpha': 0.0, 'y1': y1_case2, 'y2': y2_case2},\n        {'xk': np.array([0.0, 0.0, 0.0]), 'delta': 0.1, 'alpha': 0.5, 'y1': y1_nom, 'y2': y2_nom},\n        {'xk': np.array([-0.3, 0.4, -0.1]), 'delta': 0.8, 'alpha': 2.0, 'y1': y1_nom, 'y2': y2_nom},\n    ]\n\n    results = []\n    pred_tolerance = 1e-12\n\n    for case in test_cases:\n        xk, delta, alpha = case['xk'], case['delta'], case['alpha']\n        y1, y2 = case['y1'], case['y2']\n\n        H_approx = H_true + alpha * I\n        g = gradient(xk, y1, y2)\n        \n        p = solve_trust_region_subproblem(H_approx, g, delta)\n        \n        pred = -(g.T @ p + 0.5 * p.T @ H_approx @ p)\n        \n        if pred  pred_tolerance:\n            rho = 0.0\n        else:\n            ared = J_cost(xk, y1, y2) - J_cost(xk + p, y1, y2)\n            rho = ared / pred\n            \n        results.append(rho)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}