{
    "hands_on_practices": [
        {
            "introduction": "This first exercise goes back to the basics, exploring the statistical heart of variational data assimilation. By working through a simple scalar problem, you will see how the analysis state and its uncertainty can be derived directly from Bayesian principles . This practice is crucial for building intuition on how 3D-Var optimally blends background information (a forecast) with new observations, weighting each by its respective confidence.",
            "id": "3864732",
            "problem": "Consider a single-grid-cell environmental state variable $x$ representing a column-averaged tracer concentration at analysis time in a remote sensing and environmental modeling system. You are performing Three-Dimensional Variational (3D-Var) data assimilation, which is the time-independent limit of Four-Dimensional Variational (4D-Var) data assimilation, using one satellite observation. Assume a linear observation operator with $H=1$, so the observation model is $y=Hx+\\epsilon$, where $\\epsilon$ is additive zero-mean Gaussian instrument noise. The prior (background) state is modeled as a Gaussian random variable with mean $x_b$ and variance $B$. The observation error is Gaussian with variance $R$. Under these assumptions, the Maximum A Posteriori (MAP) estimate equals the minimizer of the quadratic 3D-Var cost function derived from Bayes’ theorem for linear-Gaussian models.\n\nStarting from the definitions of a Gaussian prior $p(x)$ and a Gaussian likelihood $p(y\\mid x)$, and using Bayes’ theorem to form the posterior $p(x\\mid y)$, derive the scalar MAP estimator $x_a$ and the associated analysis variance $\\sigma_a^2$ for $H=1$. Then evaluate your expressions for the case $x_b=2$, $B=4$, $y=5$, $H=1$, and $R=1$.\n\nReport the final answer as two entries in a single row matrix $\\begin{pmatrix}x_a & \\sigma_a^2\\end{pmatrix}$. Express the values exactly; no rounding is required.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. It represents a standard application of Bayesian inference in the context of data assimilation.\n\nThe objective is to derive the Maximum A Posteriori (MAP) estimate, denoted as the analysis state $x_a$, and the corresponding analysis variance $\\sigma_a^2$. The derivation begins with Bayes' theorem, which relates the posterior probability of the state $x$ given an observation $y$ to the prior probability of the state and the likelihood of the observation given the state:\n\n$$p(x \\mid y) = \\frac{p(y \\mid x) p(x)}{p(y)}$$\n\nThe term $p(y)$ is a normalization constant, independent of $x$. Therefore, for the purpose of finding the value of $x$ that maximizes the posterior, we can write:\n\n$$p(x \\mid y) \\propto p(y \\mid x) p(x)$$\n\nThe problem specifies a Gaussian prior distribution for the state variable $x$, with mean $x_b$ (the background state) and variance $B$. The probability density function (PDF) is:\n\n$$p(x) = \\frac{1}{\\sqrt{2\\pi B}} \\exp\\left(-\\frac{1}{2} \\frac{(x - x_b)^2}{B}\\right)$$\n\nThe observation model is given as $y = Hx + \\epsilon$, where $H=1$ and the observation error $\\epsilon$ is drawn from a zero-mean Gaussian distribution with variance $R$. This defines the likelihood function $p(y \\mid x)$ as a Gaussian distribution for $y$ centered at $Hx=x$ with variance $R$:\n\n$$p(y \\mid x) = \\frac{1}{\\sqrt{2\\pi R}} \\exp\\left(-\\frac{1}{2} \\frac{(y - x)^2}{R}\\right)$$\n\nSubstituting the PDFs for the prior and the likelihood into the proportionality for the posterior gives:\n\n$$p(x \\mid y) \\propto \\left[ \\frac{1}{\\sqrt{2\\pi R}} \\exp\\left(-\\frac{1}{2} \\frac{(y - x)^2}{R}\\right) \\right] \\left[ \\frac{1}{\\sqrt{2\\pi B}} \\exp\\left(-\\frac{1}{2} \\frac{(x - x_b)^2}{B}\\right) \\right]$$\n\nCombining the exponential terms and neglecting the constant coefficients, we obtain:\n\n$$p(x \\mid y) \\propto \\exp\\left(-\\frac{1}{2} \\left[ \\frac{(x - x_b)^2}{B} + \\frac{(y - x)^2}{R} \\right]\\right)$$\n\nThe MAP estimate $x_a$ is the value of $x$ that maximizes this posterior probability. Maximizing $p(x \\mid y)$ is equivalent to minimizing the negative of its natural logarithm. This defines the 3D-Var cost function $J(x)$:\n\n$$J(x) = \\frac{1}{2} \\left[ \\frac{(x - x_b)^2}{B} + \\frac{(y - x)^2}{R} \\right]$$\n\nTo find the minimum of $J(x)$, we compute its first derivative with respect to $x$ and set it to zero:\n\n$$\\frac{dJ}{dx} = \\frac{1}{2} \\left[ \\frac{2(x - x_b)}{B} + \\frac{2(y - x)(-1)}{R} \\right] = \\frac{x - x_b}{B} - \\frac{y - x}{R}$$\n\nSetting the derivative to zero at $x = x_a$:\n\n$$\\frac{x_a - x_b}{B} - \\frac{y - x_a}{R} = 0$$\n\n$$\\frac{x_a - x_b}{B} = \\frac{y - x_a}{R}$$\n\nSolving for $x_a$:\n\n$$R(x_a - x_b) = B(y - x_a)$$\n$$Rx_a - Rx_b = By - Bx_a$$\n$$Rx_a + Bx_a = By + Rx_b$$\n$$x_a(R + B) = By + Rx_b$$\n\nThis yields the MAP estimator for the analysis state $x_a$:\n\n$$x_a = \\frac{By + Rx_b}{B+R}$$\n\nThe posterior distribution $p(x \\mid y)$ is itself Gaussian, as it is proportional to the product of two Gaussian functions. The variance of this posterior distribution is the analysis variance, $\\sigma_a^2$. In variational data assimilation, the analysis (posterior) covariance is the inverse of the Hessian of the cost function evaluated at the minimum. For this scalar problem, the analysis variance is the inverse of the second derivative of $J(x)$.\n\nThe second derivative of the cost function is:\n\n$$\\frac{d^2 J}{dx^2} = \\frac{d}{dx} \\left( \\frac{x - x_b}{B} - \\frac{y - x}{R} \\right) = \\frac{1}{B} + \\frac{1}{R}$$\n\nThe analysis variance $\\sigma_a^2$ is the inverse of this expression:\n\n$$\\sigma_a^2 = \\left(\\frac{d^2 J}{dx^2}\\right)^{-1} = \\left(\\frac{1}{B} + \\frac{1}{R}\\right)^{-1} = \\left(\\frac{R + B}{BR}\\right)^{-1} = \\frac{BR}{B+R}$$\n\nNow we evaluate these expressions using the given numerical values: $x_b=2$, $B=4$, $y=5$, and $R=1$.\n\nThe analysis state $x_a$ is:\n\n$$x_a = \\frac{(4)(5) + (1)(2)}{4+1} = \\frac{20 + 2}{5} = \\frac{22}{5}$$\n\nThe analysis variance $\\sigma_a^2$ is:\n\n$$\\sigma_a^2 = \\frac{(4)(1)}{4+1} = \\frac{4}{5}$$\n\nThe final answer is the pair $(x_a, \\sigma_a^2)$, presented as a row matrix.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{22}{5} & \\frac{4}{5} \\end{pmatrix}}$$"
        },
        {
            "introduction": "Real-world ocean models are inherently non-linear, a complexity that simple variational methods do not address directly. Advanced methods like incremental 4D-Var tackle this by repeatedly linearizing the model and observation operators, but this is only an approximation. This exercise provides a hands-on method to quantify the error introduced by this so-called 'tangent linear hypothesis', a critical skill for assessing the validity of the assimilation system for a given model state .",
            "id": "3864651",
            "problem": "In incremental three-dimensional variational assimilation (3D-Var) and four-dimensional variational assimilation (4D-Var), the observation operator is linearized around a current iterate to build the inner-loop quadratic subproblem. Consider a scalar state variable $x$ (dimensionless) and a scalar observation operator $\\mathcal{H}(x)$ representing a simplified nonlinearity akin to saturation in a remote sensing radiance model. Let $\\mathcal{H}(x) = \\sin(x)$, where angles are in radians. At the current iterate $x^{i} = 0.2$, suppose the true state is offset by a known increment $\\delta x = 0.5$, so that the true state is $x^{i} + \\delta x$. Using fundamental principles (Taylor’s theorem and the definition of the tangent linear approximation used in variational data assimilation), compute the absolute linearization error of the observation operator around $x^{i}$ for the increment $\\delta x$. Assume an observational error standard deviation of $\\sigma_{o} = 0.05$ (dimensionless) and use the criterion “the tangent linear hypothesis is acceptable if the absolute linearization error is smaller than $\\sigma_{o}$” to assess acceptability. Round your final answer (the absolute linearization error) to four significant figures, and express it in dimensionless units.",
            "solution": "The problem statement is analyzed for validity.\n\n**Step 1: Extract Givens**\n- Scalar state variable: $x$ (dimensionless)\n- Scalar observation operator: $\\mathcal{H}(x) = \\sin(x)$\n- Current iterate of the state variable: $x^{i} = 0.2$\n- Increment to the state variable: $\\delta x = 0.5$\n- True state: $x^{i} + \\delta x$\n- Observational error standard deviation: $\\sigma_{o} = 0.05$\n- Criterion for acceptability of tangent linear hypothesis: absolute linearization error $< \\sigma_{o}$\n- Required calculation: Compute the absolute linearization error and round to four significant figures.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is grounded in the fundamental principles of variational data assimilation, specifically the linearization of a nonlinear operator using a Taylor series expansion, which forms the basis of the tangent linear model in incremental 3D-Var and 4D-Var. The operator $\\mathcal{H}(x) = \\sin(x)$ is a standard example of a simple, smooth, nonlinear function used for pedagogical purposes.\n- **Well-Posed**: The problem is well-posed. It provides all necessary data and definitions to compute a unique numerical answer. The objective is clearly stated.\n- **Objective**: The problem is formulated in precise, objective, and quantitative terms.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid as it is scientifically sound, self-contained, and well-posed. A solution will be derived.\n\nThe core task is to compute the absolute linearization error. The linearization error is defined as the difference between the full nonlinear observation operator evaluated at the true state and its tangent linear approximation expanded around the current iterate.\n\nLet the true state be $x_{t} = x^{i} + \\delta x$. The true observation is $\\mathcal{H}(x_{t}) = \\mathcal{H}(x^{i} + \\delta x)$.\n\nThe tangent linear approximation of $\\mathcal{H}(x)$ around the point $x^{i}$ is given by the first-order Taylor expansion:\n$$ \\mathcal{H}(x^{i} + \\delta x) \\approx \\mathcal{H}(x^{i}) + \\mathbf{H} \\delta x $$\nwhere $\\mathbf{H}$ is the tangent linear operator, defined as the derivative of $\\mathcal{H}$ with respect to $x$, evaluated at the point of linearization $x^{i}$:\n$$ \\mathbf{H} = \\frac{d\\mathcal{H}}{dx}\\bigg|_{x=x^{i}} $$\nThe linearization error, which we denote as $\\epsilon_{L}$, is the difference between the true value and the approximated value:\n$$ \\epsilon_{L} = \\mathcal{H}(x^{i} + \\delta x) - \\left( \\mathcal{H}(x^{i}) + \\mathbf{H} \\delta x \\right) $$\nThe problem asks for the absolute linearization error, $|\\epsilon_{L}|$.\n\nWe are given the specific observation operator $\\mathcal{H}(x) = \\sin(x)$.\nFirst, we compute the tangent linear operator $\\mathbf{H}$:\n$$ \\mathbf{H} = \\frac{d}{dx}(\\sin(x))\\bigg|_{x=x^{i}} = \\cos(x^{i}) $$\nSubstituting the given value $x^{i} = 0.2$:\n$$ \\mathbf{H} = \\cos(0.2) $$\nNow, we can write the expression for the linearization error $\\epsilon_{L}$ using the given operator and values:\n$$ \\epsilon_{L} = \\sin(x^{i} + \\delta x) - \\left( \\sin(x^{i}) + \\cos(x^{i}) \\delta x \\right) $$\nSubstituting the numerical values $x^{i} = 0.2$ and $\\delta x = 0.5$:\n$$ \\epsilon_{L} = \\sin(0.2 + 0.5) - \\left( \\sin(0.2) + \\cos(0.2) \\times 0.5 \\right) $$\n$$ \\epsilon_{L} = \\sin(0.7) - \\sin(0.2) - 0.5 \\cos(0.2) $$\nWe now compute the numerical value of this expression, ensuring the calculator is in radian mode:\n$$ \\sin(0.7) \\approx 0.6442176872 $$\n$$ \\sin(0.2) \\approx 0.1986693308 $$\n$$ \\cos(0.2) \\approx 0.9800665778 $$\nSubstituting these values into the expression for $\\epsilon_{L}$:\n$$ \\epsilon_{L} \\approx 0.6442176872 - (0.1986693308 + 0.5 \\times 0.9800665778) $$\n$$ \\epsilon_{L} \\approx 0.6442176872 - (0.1986693308 + 0.4900332889) $$\n$$ \\epsilon_{L} \\approx 0.6442176872 - 0.6887026197 $$\n$$ \\epsilon_{L} \\approx -0.0444849325 $$\nThe absolute linearization error is the absolute value of $\\epsilon_{L}$:\n$$ |\\epsilon_{L}| = |-0.0444849325| = 0.0444849325 $$\nThe problem requires this value to be rounded to four significant figures.\n$$ |\\epsilon_{L}| \\approx 0.04448 $$\nFinally, we assess the acceptability of the tangent linear hypothesis using the given criterion: absolute linearization error $< \\sigma_{o}$.\nWe are given $\\sigma_{o} = 0.05$.\nWe check if $0.04448 < 0.05$. This inequality is true. Therefore, based on the provided criterion, the tangent linear hypothesis is acceptable for this increment and state.\n\nThe final answer is the computed absolute linearization error, rounded as requested.",
            "answer": "$$\\boxed{0.04448}$$"
        },
        {
            "introduction": "The efficiency of 4D-Var hinges on the adjoint model, a powerful tool for computing the gradient of a cost function with respect to a model's initial conditions over a time window. However, implementing an adjoint model is notoriously error-prone. This final practice introduces the 'gradient check,' an essential verification technique to ensure your adjoint code is correct by comparing its output to a finite-difference approximation .",
            "id": "3793675",
            "problem": "You must implement a rigorous gradient check for the cost function of Four-Dimensional Variational Data Assimilation (4D-Var) in computational oceanography. Begin from the definition of the 4D-Var cost function and a linear, one-dimensional periodic tracer model. You will compare finite-difference directional derivatives against adjoint-based gradients along random directions and decide acceptance using clear tolerances.\n\nSet up the following purely mathematical and numerically self-consistent problem in non-dimensional units. Consider a one-dimensional periodic domain with $n$ grid points and grid spacing $\\Delta x = 1/n$. Let $\\Delta t$ be the time step, $K$ the number of time steps, $c$ a constant advection velocity, and $\\nu$ a constant diffusion coefficient. The linear forward model updates the tracer state $x_k \\in \\mathbb{R}^n$ via\n$$\nx_{k+1} = M x_k,\n$$\nwhere the model matrix $M \\in \\mathbb{R}^{n \\times n}$ is defined by the explicit Euler discretization of advection-diffusion with periodic boundary conditions:\n$$\nM = I + \\Delta t \\left( A_{\\mathrm{adv}} + A_{\\mathrm{diff}} \\right),\n$$\nwith\n$$\nA_{\\mathrm{adv}} = -\\frac{c}{\\Delta x}\\left(I - S_{-}\\right), \\quad A_{\\mathrm{diff}} = \\frac{\\nu}{\\Delta x^2}\\left(S_{+} + S_{-} - 2I\\right).\n$$\nHere $I$ is the identity matrix in $\\mathbb{R}^{n \\times n}$, $S_{-}$ is the circulant shift matrix such that $(S_{-} x)_i = x_{i-1}$, and $S_{+}$ is the circulant shift matrix such that $(S_{+} x)_i = x_{i+1}$, with indices taken modulo $n$.\n\nObservations at each time $k \\in \\{1,\\dots,K\\}$ are defined by a linear observation operator $H \\in \\mathbb{R}^{m \\times n}$ that selects $m$ grid-point values from the state, and observation vectors $y_k \\in \\mathbb{R}^m$. The background (prior) state is $x_b \\in \\mathbb{R}^n$. Assume diagonal covariance matrices for background and observation errors: $B = \\sigma_b^2 I$ and $R = \\sigma_r^2 I$, so that $B^{-1} = \\sigma_b^{-2} I$ and $R^{-1} = \\sigma_r^{-2} I$.\n\nThe four-dimensional variational (4D-Var) cost function is\n$$\nJ(x_0) = \\frac{1}{2}(x_0 - x_b)^\\top B^{-1}(x_0 - x_b)\n+ \\frac{1}{2} \\sum_{k=1}^K \\left(H x_k - y_k\\right)^\\top R^{-1} \\left(H x_k - y_k\\right),\n$$\nwith $x_k = M^k x_0$.\n\nYour task is to perform a gradient check of $J$ with respect to $x_0$ by:\n- Computing the adjoint-based gradient of $J$ with respect to $x_0$ derived from first principles.\n- Drawing random directions $v \\in \\mathbb{R}^n$ with unit Euclidean norm $\\|v\\|_2 = 1$.\n- Computing the finite-difference directional derivative using the central difference formula:\n$$\nD_{\\mathrm{FD}}(v) = \\frac{J(x_0 + \\varepsilon v) - J(x_0 - \\varepsilon v)}{2\\varepsilon}.\n$$\n- Computing the adjoint-based directional derivative:\n$$\nD_{\\mathrm{ADJ}}(v) = \\nabla J(x_0)^\\top v.\n$$\n- For each direction, evaluating the absolute error $E_{\\mathrm{abs}} = |D_{\\mathrm{FD}}(v) - D_{\\mathrm{ADJ}}(v)|$ and the relative error\n$$\nE_{\\mathrm{rel}} = \\frac{|D_{\\mathrm{FD}}(v) - D_{\\mathrm{ADJ}}(v)|}{\\max\\left(1, |D_{\\mathrm{FD}}(v)|, |D_{\\mathrm{ADJ}}(v)|\\right)}.\n$$\n- Accepting the gradient check along a direction if either $E_{\\mathrm{rel}} \\le \\text{tol}_{\\mathrm{rel}}$ or $E_{\\mathrm{abs}} \\le \\text{tol}_{\\mathrm{abs}}$.\n\nConstruct $y_k$ and $x_b$ as follows for scientific realism:\n- Generate a \"truth\" initial condition $x_0^{\\mathrm{true}}$ with independent standard normal entries, propagate with the same $M$ to obtain $x_k^{\\mathrm{true}}$, and set $y_k = H x_k^{\\mathrm{true}} + \\sigma_r \\eta_k$ with independent standard normal $\\eta_k$.\n- Set $x_b = x_0^{\\mathrm{true}} + \\sigma_b \\xi$ with independent standard normal $\\xi$.\n- Evaluate the gradient check at $x_0 = x_b$.\n\nUse dimensionless units throughout. Angles are not involved. No percentages are involved.\n\nImplement a program that, for the parameter sets listed below, performs the above gradient check using independent random directions and returns, for each parameter set, a single boolean that is true if all tested directions in that set are accepted and false otherwise. The final output must be a single line containing the results across all parameter sets as a comma-separated list enclosed in square brackets.\n\nTest suite (each line is one parameter set; use an independent random seed per set to ensure reproducibility):\n- Case $1$: $n=32$, $K=6$, $\\Delta t=10^{-2}$, $c=2\\times 10^{-1}$, $\\nu=5\\times 10^{-3}$, $m=16$, $\\sigma_b=5\\times 10^{-1}$, $\\sigma_r=10^{-1}$, $\\varepsilon=10^{-6}$, $\\text{tol}_{\\mathrm{rel}}=10^{-7}$, $\\text{tol}_{\\mathrm{abs}}=10^{-10}$, number of directions $=5$.\n- Case $2$: $n=32$, $K=6$, $\\Delta t=10^{-2}$, $c=2\\times 10^{-1}$, $\\nu=5\\times 10^{-3}$, $m=16$, $\\sigma_b=5\\times 10^{-1}$, $\\sigma_r=10^{-1}$, $\\varepsilon=10^{-10}$, $\\text{tol}_{\\mathrm{rel}}=10^{-3}$, $\\text{tol}_{\\mathrm{abs}}=10^{-8}$, number of directions $=5$.\n- Case $3$: $n=32$, $K=6$, $\\Delta t=10^{-2}$, $c=2\\times 10^{-1}$, $\\nu=5\\times 10^{-3}$, $m=8$, $\\sigma_b=5\\times 10^{-1}$, $\\sigma_r=10^{2}$, $\\varepsilon=10^{-6}$, $\\text{tol}_{\\mathrm{rel}}=10^{-7}$, $\\text{tol}_{\\mathrm{abs}}=10^{-10}$, number of directions $=5$.\n- Case $4$: $n=40$, $K=10$, $\\Delta t=5\\times 10^{-3}$, $c=5\\times 10^{-1}$, $\\nu=5\\times 10^{-3}$, $m=40$, $\\sigma_b=2\\times 10^{-1}$, $\\sigma_r=5\\times 10^{-2}$, $\\varepsilon=10^{-6}$, $\\text{tol}_{\\mathrm{rel}}=10^{-7}$, $\\text{tol}_{\\mathrm{abs}}=10^{-10}$, number of directions $=5$.\n- Case $5$: $n=24$, $K=5$, $\\Delta t=10^{-2}$, $c=3\\times 10^{-1}$, $\\nu=4\\times 10^{-3}$, $m=12$, $\\sigma_b=10^{-2}$, $\\sigma_r=2\\times 10^{-1}$, $\\varepsilon=10^{-6}$, $\\text{tol}_{\\mathrm{rel}}=10^{-7}$, $\\text{tol}_{\\mathrm{abs}}=10^{-10}$, number of directions $=5$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3,r_4,r_5]$ where each $r_i$ is a boolean for Case $i$).",
            "solution": "The user-provided problem is assessed to be **valid**. It is a well-posed, scientifically grounded problem in the field of computational data assimilation that is free of ambiguity, contradiction, and factual error. The task is to implement a standard numerical procedure, a gradient check, for a 4D-Var data assimilation system. We will proceed with a full solution.\n\nOur objective is to derive the analytical gradient of the 4D-Var cost function $J(x_0)$ with respect to the initial state $x_0$. This analytical gradient, computed using the adjoint method, will then be compared against a finite-difference approximation to verify its correctness.\n\nThe cost function is given by:\n$$\nJ(x_0) = \\frac{1}{2}(x_0 - x_b)^\\top B^{-1}(x_0 - x_b)\n+ \\frac{1}{2} \\sum_{k=1}^K \\left(H x_k - y_k\\right)^\\top R^{-1} \\left(H x_k - y_k\\right)\n$$\nwhere the state $x_k$ at time step $k$ is related to the initial state $x_0$ by the linear forward model $x_k = M^k x_0$. The cost function consists of two terms: a background term $J_b(x_0)$ that measures the distance to a prior estimate $x_b$, and an observation term $J_o(x_0)$ that measures the misfit to observations $y_k$ over a time window.\n\n$$\nJ_b(x_0) = \\frac{1}{2}(x_0 - x_b)^\\top B^{-1}(x_0 - x_b)\n$$\n$$\nJ_o(x_0) = \\frac{1}{2} \\sum_{k=1}^K \\left(H M^k x_0 - y_k\\right)^\\top R^{-1} \\left(H M^k x_0 - y_k\\right)\n$$\n\nThe gradient $\\nabla J(x_0)$ is the sum of the gradients of these two terms: $\\nabla J(x_0) = \\nabla J_b(x_0) + \\nabla J_o(x_0)$.\n\n1.  **Gradient of the Background Term**\n    The background term $J_b(x_0)$ is a standard quadratic form. Its gradient with respect to $x_0$ is straightforward to compute:\n    $$\n    \\nabla J_b(x_0) = B^{-1}(x_0 - x_b)\n    $$\n    Given $B = \\sigma_b^2 I$, the inverse is $B^{-1} = \\sigma_b^{-2} I$, so this term simplifies to $\\sigma_b^{-2}(x_0 - x_b)$.\n\n2.  **Gradient of the Observation Term (Adjoint Method)**\n    To find $\\nabla J_o(x_0)$, we use the method of Lagrange multipliers, which gives rise to the adjoint model. We treat the model equations $x_{k+1} = M x_k$ for $k=0, \\dots, K-1$ as constraints. The Lagrangian $\\mathcal{L}$ is:\n    $$\n    \\mathcal{L}(\\{x_k\\}_{k=0}^K, \\{\\lambda_k\\}_{k=1}^K) = J(\\{x_k\\}) + \\sum_{k=0}^{K-1} \\lambda_{k+1}^\\top (M x_k - x_{k+1})\n    $$\n    Here, the cost function is expressed in terms of the full state trajectory $\\{x_k\\}$. The vectors $\\lambda_k$ are the Lagrange multipliers, also known as the adjoint variables. At an optimal point, the gradient of the Lagrangian with respect to all its variables is zero. The gradient of $J(x_0)$ is equivalent to the total derivative of the constrained cost function, $\\frac{dJ}{dx_0}$. We can find this by evaluating $\\frac{\\partial \\mathcal{L}}{\\partial x_0}$ after imposing the conditions $\\frac{\\partial \\mathcal{L}}{\\partial x_k} = 0$ for $k=1, \\dots, K$.\n\n    Taking the partial derivative of $\\mathcal{L}$ with respect to a state $x_k$ for $k \\in \\{1, \\dots, K\\}$ yields:\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial x_k} = \\frac{\\partial J}{\\partial x_k} + M^\\top \\lambda_{k+1} - \\lambda_k = H^\\top R^{-1} (H x_k - y_k) + M^\\top \\lambda_{k+1} - \\lambda_k\n    $$\n    Setting this to zero gives the adjoint equations. For $k=K$, the term $\\lambda_{K+1}$ is not present in the sum, so we define $\\lambda_{K+1}=0$. The derivative with respect to $x_K$ is:\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial x_K} = H^\\top R^{-1} (H x_K - y_K) - \\lambda_K = 0 \\implies \\lambda_K = H^\\top R^{-1} (H x_K - y_K)\n    $$\n    For $k \\in \\{1, \\dots, K-1\\}$, we have:\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial x_k} = 0 \\implies \\lambda_k = M^\\top \\lambda_{k+1} + H^\\top R^{-1} (H x_k - y_k)\n    $$\n    These equations define a backward recurrence for the adjoint variables, starting from $\\lambda_K$ and propagating backward in time to $\\lambda_1$. Note that since $R = \\sigma_r^2 I$, the term $H^\\top R^{-1} (H x_k - y_k)$ simplifies to $\\sigma_r^{-2} H^\\top (H x_k - y_k)$.\n\n    Finally, the gradient of the cost function with respect to the initial state $x_0$ is obtained by differentiating $\\mathcal{L}$ with respect to $x_0$:\n    $$\n    \\nabla J(x_0) = \\frac{d\\mathcal{L}}{dx_0} = \\frac{\\partial\\mathcal{L}}{\\partial x_0} = \\frac{\\partial J_b}{\\partial x_0} + M^\\top \\lambda_1 = B^{-1}(x_0 - x_b) + M^\\top \\lambda_1\n    $$\n\n3.  **Algorithmic Summary for Gradient Calculation**\n    The procedure to calculate $\\nabla J(x_0)$ is as follows:\n    a.  **Forward Run**: Starting with $x_0$, integrate the forward model $x_{k+1} = M x_k$ for $k = 0, \\dots, K-1$ and store the entire state trajectory $\\{x_k\\}_{k=1}^K$.\n    b.  **Backward (Adjoint) Run**:\n        i.  Initialize the adjoint variable at the final time $K$: $\\lambda_K = \\sigma_r^{-2} H^\\top (H x_K - y_K)$.\n        ii. Integrate the adjoint model backward in time from $k=K-1$ down to $1$: $\\lambda_k = M^\\top \\lambda_{k+1} + \\sigma_r^{-2} H^\\top (H x_k - y_k)$.\n    c.  **Gradient Assembly**: Compute the final gradient using the result from the backward run: $\\nabla J(x_0) = \\sigma_b^{-2}(x_0 - x_b) + M^\\top \\lambda_1$.\n\n4.  **Gradient Check Procedure**\n    The gradient check validates the analytical gradient by comparing its projection along a random direction $v$ with a finite-difference approximation of the directional derivative.\n    -   The adjoint-based directional derivative is $D_{\\mathrm{ADJ}}(v) = \\nabla J(x_0)^\\top v$.\n    -   The finite-difference approximation using a central difference scheme is $D_{\\mathrm{FD}}(v) = \\frac{J(x_0 + \\varepsilon v) - J(x_0 - \\varepsilon v)}{2\\varepsilon}$.\n    The implementation will compute both quantities for several random unit vectors $v$, calculate the absolute error $E_{\\mathrm{abs}}$ and relative error $E_{\\mathrm{rel}}$, and accept the gradient if the errors are within the specified tolerances $\\text{tol}_{\\mathrm{abs}}$ and $\\text{tol}_{\\mathrm{rel}}$. The overall check for a parameter set passes only if all tested random directions are accepted.\n\nThe following Python code implements this complete procedure.\n```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_gradient_check(n, K, dt, c, nu, m, sigma_b, sigma_r, epsilon, tol_rel, tol_abs, num_directions, seed):\n    \"\"\"\n    Performs a gradient check for a 4D-Var cost function for a single parameter set.\n    \"\"\"\n    # 1. Setup\n    np.random.seed(seed)\n    dx = 1.0 / n\n\n    # 2. Build matrices M, and define H operation\n    I = np.eye(n)\n    # (S- @ x)_i = x_{i-1}. Matrix equivalent is np.roll(I, 1, axis=0)\n    S_minus = np.roll(I, 1, axis=0)\n    # (S+ @ x)_i = x_{i+1}. Matrix equivalent is np.roll(I, -1, axis=0)\n    S_plus = np.roll(I, -1, axis=0)\n    \n    A_adv = -(c / dx) * (I - S_minus)\n    A_diff = (nu / dx**2) * (S_plus + S_minus - 2 * I)\n    M = I + dt * (A_adv + A_diff)\n    M_T = M.T\n    \n    obs_indices = np.linspace(0, n - 1, num=m, dtype=int)\n\n    # 3. Generate data (truth, obs, background)\n    x0_true = np.random.randn(n)\n    \n    y_obs_list = []\n    x_k_true = np.copy(x0_true)\n    for _ in range(K):\n        x_k_true = M @ x_k_true\n        obs_noise = np.random.randn(m)\n        y_k = x_k_true[obs_indices] + sigma_r * obs_noise\n        y_obs_list.append(y_k)\n        \n    bg_noise = np.random.randn(n)\n    x_b = x0_true + sigma_b * bg_noise\n    \n    # 4. Define cost and gradient functions\n    inv_sigma_b2 = 1.0 / sigma_b**2\n    inv_sigma_r2 = 1.0 / sigma_r**2\n\n    def calculate_J(x0):\n        j_b = 0.5 * inv_sigma_b2 * np.sum((x0 - x_b)**2)\n        \n        j_o = 0.0\n        x_k = np.copy(x0)\n        for k in range(K):\n            x_k = M @ x_k\n            innov = x_k[obs_indices] - y_obs_list[k]\n            j_o += 0.5 * inv_sigma_r2 * np.sum(innov**2)\n            \n        return j_b + j_o\n\n    def calculate_grad_J(x0):\n        # Forward run: store trajectory\n        x_traj = [x0]\n        x_k = np.copy(x0)\n        for _ in range(K):\n            x_k = M @ x_k\n            x_traj.append(x_k)\n\n        # Backward run: compute adjoint variable\n        # Initialize lambda_K\n        x_K = x_traj[K]\n        y_K = y_obs_list[K-1] # y_obs_list is 0-indexed for k=1..K\n        innov_K = x_K[obs_indices] - y_K\n        forcing_term_K = np.zeros(n)\n        forcing_term_K[obs_indices] = inv_sigma_r2 * innov_K\n        lambda_next = forcing_term_K # This is lambda_K\n\n        # Loop for k from K-1 down to 1\n        for k in range(K - 1, 0, -1):\n            x_k = x_traj[k]\n            y_k = y_obs_list[k-1]\n            \n            innov_k = x_k[obs_indices] - y_k\n            forcing_term_k = np.zeros(n)\n            forcing_term_k[obs_indices] = inv_sigma_r2 * innov_k\n            \n            lambda_current = M_T @ lambda_next + forcing_term_k\n            lambda_next = lambda_current\n\n        # After loop, lambda_next is lambda_1\n        lambda_1 = lambda_next\n        \n        grad_b = inv_sigma_b2 * (x0 - x_b)\n        grad_o = M_T @ lambda_1\n        \n        return grad_b + grad_o\n\n    # 5. Perform the gradient check at x0 = x_b\n    x0_eval = x_b\n    grad_adj = calculate_grad_J(x0_eval)\n    \n    for i in range(num_directions):\n        v = np.random.randn(n)\n        v /= np.linalg.norm(v)\n\n        J_plus = calculate_J(x0_eval + epsilon * v)\n        J_minus = calculate_J(x0_eval - epsilon * v)\n\n        D_fd = (J_plus - J_minus) / (2.0 * epsilon)\n        D_adj = grad_adj.T @ v\n\n        E_abs = np.abs(D_fd - D_adj)\n        denom = max(1.0, np.abs(D_fd), np.abs(D_adj))\n        E_rel = E_abs / denom\n\n        if not (E_rel <= tol_rel or E_abs <= tol_abs):\n            return False # Failed test for this direction\n    \n    return True # All directions passed\n```",
            "answer": "$$\\boxed{\\texttt{[true,true,true,true,true]}}$$"
        }
    ]
}