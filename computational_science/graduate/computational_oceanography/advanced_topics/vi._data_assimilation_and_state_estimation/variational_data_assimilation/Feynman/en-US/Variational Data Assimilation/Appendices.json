{
    "hands_on_practices": [
        {
            "introduction": "Variational data assimilation is fundamentally an application of Bayesian inference. This first exercise provides a concrete link between the abstract theory and the practical 3D-Var cost function by focusing on a simple scalar system . By deriving the optimal state estimate, or analysis, and its corresponding uncertainty from first principles, you will gain a core intuition for how background knowledge and new observations are optimally blended.",
            "id": "3864732",
            "problem": "Consider a single-grid-cell environmental state variable $x$ representing a column-averaged tracer concentration at analysis time in a remote sensing and environmental modeling system. You are performing Three-Dimensional Variational (3D-Var) data assimilation, which is the time-independent limit of Four-Dimensional Variational (4D-Var) data assimilation, using one satellite observation. Assume a linear observation operator with $H=1$, so the observation model is $y=Hx+\\epsilon$, where $\\epsilon$ is additive zero-mean Gaussian instrument noise. The prior (background) state is modeled as a Gaussian random variable with mean $x_b$ and variance $B$. The observation error is Gaussian with variance $R$. Under these assumptions, the Maximum A Posteriori (MAP) estimate equals the minimizer of the quadratic 3D-Var cost function derived from Bayes’ theorem for linear-Gaussian models.\n\nStarting from the definitions of a Gaussian prior $p(x)$ and a Gaussian likelihood $p(y\\mid x)$, and using Bayes’ theorem to form the posterior $p(x\\mid y)$, derive the scalar MAP estimator $x_a$ and the associated analysis variance $\\sigma_a^2$ for $H=1$. Then evaluate your expressions for the case $x_b=2$, $B=4$, $y=5$, $H=1$, and $R=1$.\n\nReport the final answer as two entries in a single row matrix $\\begin{pmatrix}x_a  \\sigma_a^2\\end{pmatrix}$. Express the values exactly; no rounding is required.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. It represents a standard application of Bayesian inference in the context of data assimilation.\n\nThe objective is to derive the Maximum A Posteriori (MAP) estimate, denoted as the analysis state $x_a$, and the corresponding analysis variance $\\sigma_a^2$. The derivation begins with Bayes' theorem, which relates the posterior probability of the state $x$ given an observation $y$ to the prior probability of the state and the likelihood of the observation given the state:\n\n$$p(x \\mid y) = \\frac{p(y \\mid x) p(x)}{p(y)}$$\n\nThe term $p(y)$ is a normalization constant, independent of $x$. Therefore, for the purpose of finding the value of $x$ that maximizes the posterior, we can write:\n\n$$p(x \\mid y) \\propto p(y \\mid x) p(x)$$\n\nThe problem specifies a Gaussian prior distribution for the state variable $x$, with mean $x_b$ (the background state) and variance $B$. The probability density function (PDF) is:\n\n$$p(x) = \\frac{1}{\\sqrt{2\\pi B}} \\exp\\left(-\\frac{1}{2} \\frac{(x - x_b)^2}{B}\\right)$$\n\nThe observation model is given as $y = Hx + \\epsilon$, where $H=1$ and the observation error $\\epsilon$ is drawn from a zero-mean Gaussian distribution with variance $R$. This defines the likelihood function $p(y \\mid x)$ as a Gaussian distribution for $y$ centered at $Hx=x$ with variance $R$:\n\n$$p(y \\mid x) = \\frac{1}{\\sqrt{2\\pi R}} \\exp\\left(-\\frac{1}{2} \\frac{(y - x)^2}{R}\\right)$$\n\nSubstituting the PDFs for the prior and the likelihood into the proportionality for the posterior gives:\n\n$$p(x \\mid y) \\propto \\left[ \\frac{1}{\\sqrt{2\\pi R}} \\exp\\left(-\\frac{1}{2} \\frac{(y - x)^2}{R}\\right) \\right] \\left[ \\frac{1}{\\sqrt{2\\pi B}} \\exp\\left(-\\frac{1}{2} \\frac{(x - x_b)^2}{B}\\right) \\right]$$\n\nCombining the exponential terms and neglecting the constant coefficients, we obtain:\n\n$$p(x \\mid y) \\propto \\exp\\left(-\\frac{1}{2} \\left[ \\frac{(x - x_b)^2}{B} + \\frac{(y - x)^2}{R} \\right]\\right)$$\n\nThe MAP estimate $x_a$ is the value of $x$ that maximizes this posterior probability. Maximizing $p(x \\mid y)$ is equivalent to minimizing the negative of its natural logarithm. This defines the 3D-Var cost function $J(x)$:\n\n$$J(x) = \\frac{1}{2} \\left[ \\frac{(x - x_b)^2}{B} + \\frac{(y - x)^2}{R} \\right]$$\n\nTo find the minimum of $J(x)$, we compute its first derivative with respect to $x$ and set it to zero:\n\n$$\\frac{dJ}{dx} = \\frac{1}{2} \\left[ \\frac{2(x - x_b)}{B} + \\frac{2(y - x)(-1)}{R} \\right] = \\frac{x - x_b}{B} - \\frac{y - x}{R}$$\n\nSetting the derivative to zero at $x = x_a$:\n\n$$\\frac{x_a - x_b}{B} - \\frac{y - x_a}{R} = 0$$\n\n$$\\frac{x_a - x_b}{B} = \\frac{y - x_a}{R}$$\n\nSolving for $x_a$:\n\n$$R(x_a - x_b) = B(y - x_a)$$\n$$Rx_a - Rx_b = By - Bx_a$$\n$$Rx_a + Bx_a = By + Rx_b$$\n$$x_a(R + B) = By + Rx_b$$\n\nThis yields the MAP estimator for the analysis state $x_a$:\n\n$$x_a = \\frac{By + Rx_b}{B+R}$$\n\nThe posterior distribution $p(x \\mid y)$ is itself Gaussian, as it is proportional to the product of two Gaussian functions. The variance of this posterior distribution is the analysis variance, $\\sigma_a^2$. In variational data assimilation, the analysis (posterior) covariance is the inverse of the Hessian of the cost function evaluated at the minimum. For this scalar problem, the analysis variance is the inverse of the second derivative of $J(x)$.\n\nThe second derivative of the cost function is:\n\n$$\\frac{d^2 J}{dx^2} = \\frac{d}{dx} \\left( \\frac{x - x_b}{B} - \\frac{y - x}{R} \\right) = \\frac{1}{B} + \\frac{1}{R}$$\n\nThe analysis variance $\\sigma_a^2$ is the inverse of this expression:\n\n$$\\sigma_a^2 = \\left(\\frac{d^2 J}{dx^2}\\right)^{-1} = \\left(\\frac{1}{B} + \\frac{1}{R}\\right)^{-1} = \\left(\\frac{R + B}{BR}\\right)^{-1} = \\frac{BR}{B+R}$$\n\nNow we evaluate these expressions using the given numerical values: $x_b=2$, $B=4$, $y=5$, and $R=1$.\n\nThe analysis state $x_a$ is:\n\n$$x_a = \\frac{(4)(5) + (1)(2)}{4+1} = \\frac{20 + 2}{5} = \\frac{22}{5}$$\n\nThe analysis variance $\\sigma_a^2$ is:\n\n$$\\sigma_a^2 = \\frac{(4)(1)}{4+1} = \\frac{4}{5}$$\n\nThe final answer is the pair $(x_a, \\sigma_a^2)$, presented as a row matrix.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{22}{5}  \\frac{4}{5} \\end{pmatrix}}$$"
        },
        {
            "introduction": "Real-world systems like oceans and atmospheres are described by vast state vectors, not single scalars. This practice extends the principles from the scalar case to a simple multivariate system, introducing the critical roles of the background error covariance matrix $B$ and the observation operator $H$ . Solving this problem demonstrates how these matrices control the distribution of information from an observation to update the different components of the state vector, a key feature of operational data assimilation.",
            "id": "3864622",
            "problem": "Consider a two-component environmental state vector $x \\in \\mathbb{R}^{2}$ representing spatially aggregated quantities to be analyzed from a single satellite-derived observation in a linear setting. Assume a three-dimensional variational (3D-Var) data assimilation framework derived from Gaussian error statistics and a linear observation operator, where the analysis $x^{a}$ minimizes the quadratic cost function built from the background and observation misfits. Let the background error covariance be $B=\\begin{bmatrix}1  0 \\\\ 0  4\\end{bmatrix}$, the observation operator be $H=\\begin{bmatrix}1  1\\end{bmatrix}$, the observation error covariance be $R=1$, the background (also called prior) be $x_{b}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$, and the observation be $y=3$.\n\nStarting from the definition of the 3D-Var cost function that combines the background and observation misfits under linear-Gaussian assumptions, derive the first-order optimality condition (normal equation) that characterizes the minimum. Then, using the given numerical values, compute the analysis increment $\\delta x^{a}=x^{a}-x_{b}$ and the analysis state $x^{a}$. Express your final result as a single row matrix in the order $\\big[\\delta x^{a}_{1},\\,\\delta x^{a}_{2},\\,x^{a}_{1},\\,x^{a}_{2}\\big]$.\n\nNo units are required. Provide the exact values; no rounding is necessary.",
            "solution": "The 3D-Var analysis state $x^a$ is the state vector $x$ that minimizes the cost function $J(x)$. The cost function measures the misfit to the background state and the observations, weighted by their respective error covariances. For a linear-Gaussian system, it is given by:\n$$J(x) = \\frac{1}{2}(x - x_b)^T B^{-1} (x - x_b) + \\frac{1}{2}(y - Hx)^T R^{-1} (y - Hx)$$\nTo find the minimum, we compute the gradient of $J(x)$ with respect to $x$ and set it to zero. The gradient $\\nabla_x J(x)$ is:\n$$\\nabla_x J(x) = B^{-1}(x - x_b) - H^T R^{-1} (y - Hx)$$\nSetting the gradient to zero at $x=x^a$ gives the first-order optimality condition, also known as the normal equation:\n$$B^{-1}(x^a - x_b) - H^T R^{-1} (y - Hx^a) = 0$$\nWe are asked to find the analysis increment, defined as $\\delta x^a = x^a - x_b$. We substitute $x^a = x_b + \\delta x^a$ into the normal equation:\n$$B^{-1}(\\delta x^a) - H^T R^{-1} (y - H(x_b + \\delta x^a)) = 0$$\n$$B^{-1}(\\delta x^a) - H^T R^{-1} (y - Hx_b - H\\delta x^a) = 0$$\nRearranging the terms to solve for $\\delta x^a$:\n$$B^{-1}(\\delta x^a) + H^T R^{-1} H\\delta x^a = H^T R^{-1} (y - Hx_b)$$\n$$(B^{-1} + H^T R^{-1} H) \\delta x^a = H^T R^{-1} (y - Hx_b)$$\nThis equation can be solved for the analysis increment $\\delta x^a$.\n\nNow, we substitute the given numerical values:\n-   $x_b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$\n-   $B = \\begin{bmatrix} 1  0 \\\\ 0  4 \\end{bmatrix} \\implies B^{-1} = \\begin{bmatrix} 1  0 \\\\ 0  \\frac{1}{4} \\end{bmatrix}$\n-   $H = \\begin{bmatrix} 1  1 \\end{bmatrix} \\implies H^T = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$\n-   $R = 1 \\implies R^{-1} = 1$\n-   $y = 3$\n\nFirst, let's compute the components of the equation for $\\delta x^a$.\nThe term $(y - Hx_b)$ is the innovation:\n$$y - Hx_b = 3 - \\begin{bmatrix} 1  1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} = 3 - 0 = 3$$\nThe right-hand side of the equation is:\n$$H^T R^{-1} (y - Hx_b) = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} (1) (3) = \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix}$$\nThe matrix on the left-hand side is the Hessian of the cost function:\n$$B^{-1} + H^T R^{-1} H = \\begin{bmatrix} 1  0 \\\\ 0  \\frac{1}{4} \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} (1) \\begin{bmatrix} 1  1 \\end{bmatrix}$$\n$$= \\begin{bmatrix} 1  0 \\\\ 0  \\frac{1}{4} \\end{bmatrix} + \\begin{bmatrix} 1  1 \\\\ 1  1 \\end{bmatrix} = \\begin{bmatrix} 2  1 \\\\ 1  1 + \\frac{1}{4} \\end{bmatrix} = \\begin{bmatrix} 2  1 \\\\ 1  \\frac{5}{4} \\end{bmatrix}$$\nTo solve for $\\delta x^a$, we need to invert this matrix. The determinant is:\n$$\\det\\left(\\begin{bmatrix} 2  1 \\\\ 1  \\frac{5}{4} \\end{bmatrix}\\right) = (2)\\left(\\frac{5}{4}\\right) - (1)(1) = \\frac{5}{2} - 1 = \\frac{3}{2}$$\nThe inverse is:\n$$\\begin{bmatrix} 2  1 \\\\ 1  \\frac{5}{4} \\end{bmatrix}^{-1} = \\frac{1}{\\frac{3}{2}} \\begin{bmatrix} \\frac{5}{4}  -1 \\\\ -1  2 \\end{bmatrix} = \\frac{2}{3} \\begin{bmatrix} \\frac{5}{4}  -1 \\\\ -1  2 \\end{bmatrix} = \\begin{bmatrix} \\frac{5}{6}  -\\frac{2}{3} \\\\ -\\frac{2}{3}  \\frac{4}{3} \\end{bmatrix}$$\nNow we can solve for $\\delta x^a$:\n$$\\delta x^a = \\begin{bmatrix} \\frac{5}{6}  -\\frac{2}{3} \\\\ -\\frac{2}{3}  \\frac{4}{3} \\end{bmatrix} \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} \\frac{5}{6}(3) - \\frac{2}{3}(3) \\\\ -\\frac{2}{3}(3) + \\frac{4}{3}(3) \\end{bmatrix} = \\begin{bmatrix} \\frac{5}{2} - 2 \\\\ -2 + 4 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2} \\\\ 2 \\end{bmatrix}$$\nSo, the components of the analysis increment are $\\delta x^a_1 = \\frac{1}{2}$ and $\\delta x^a_2 = 2$.\n\nFinally, we compute the analysis state $x^a$:\n$$x^a = x_b + \\delta x^a = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} \\frac{1}{2} \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2} \\\\ 2 \\end{bmatrix}$$\nThe components of the analysis state are $x^a_1 = \\frac{1}{2}$ and $x^a_2 = 2$.\n\nThe final result is requested as a single row matrix in the order $\\big[\\delta x^{a}_{1},\\,\\delta x^{a}_{2},\\,x^{a}_{1},\\,x^{a}_{2}\\big]$. This gives:\n$$\\begin{pmatrix} \\frac{1}{2}  2  \\frac{1}{2}  2 \\end{pmatrix}$$",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{2}  2  \\frac{1}{2}  2 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Most geophysical models and observation operators are nonlinear, which means the corresponding cost function is non-quadratic and difficult to minimize. This challenge is typically addressed using an incremental approach built upon the tangent linear approximation, which is the focus of this exercise . By calculating the error introduced by linearization for a simple nonlinear operator, you will gain a practical understanding of the validity and limitations of this crucial technique used in 4D-Var.",
            "id": "3864651",
            "problem": "In incremental three-dimensional variational assimilation (3D-Var) and four-dimensional variational assimilation (4D-Var), the observation operator is linearized around a current iterate to build the inner-loop quadratic subproblem. Consider a scalar state variable $x$ (dimensionless) and a scalar observation operator $\\mathcal{H}(x)$ representing a simplified nonlinearity akin to saturation in a remote sensing radiance model. Let $\\mathcal{H}(x) = \\sin(x)$, where angles are in radians. At the current iterate $x^{i} = 0.2$, suppose the true state is offset by a known increment $\\delta x = 0.5$, so that the true state is $x^{i} + \\delta x$. Using fundamental principles (Taylor’s theorem and the definition of the tangent linear approximation used in variational data assimilation), compute the absolute linearization error of the observation operator around $x^{i}$ for the increment $\\delta x$. Assume an observational error standard deviation of $\\sigma_{o} = 0.05$ (dimensionless) and use the criterion “the tangent linear hypothesis is acceptable if the absolute linearization error is smaller than $\\sigma_{o}$” to assess acceptability. Round your final answer (the absolute linearization error) to four significant figures, and express it in dimensionless units.",
            "solution": "The core task is to compute the absolute linearization error. The linearization error is defined as the difference between the full nonlinear observation operator evaluated at the true state and its tangent linear approximation expanded around the current iterate.\n\nLet the true state be $x_{t} = x^{i} + \\delta x$. The true observation is $\\mathcal{H}(x_{t}) = \\mathcal{H}(x^{i} + \\delta x)$.\n\nThe tangent linear approximation of $\\mathcal{H}(x)$ around the point $x^{i}$ is given by the first-order Taylor expansion:\n$$ \\mathcal{H}(x^{i} + \\delta x) \\approx \\mathcal{H}(x^{i}) + \\mathbf{H} \\delta x $$\nwhere $\\mathbf{H}$ is the tangent linear operator, defined as the derivative of $\\mathcal{H}$ with respect to $x$, evaluated at the point of linearization $x^{i}$:\n$$ \\mathbf{H} = \\frac{d\\mathcal{H}}{dx}\\bigg|_{x=x^{i}} $$\nThe linearization error, which we denote as $\\epsilon_{L}$, is the difference between the true value and the approximated value:\n$$ \\epsilon_{L} = \\mathcal{H}(x^{i} + \\delta x) - \\left( \\mathcal{H}(x^{i}) + \\mathbf{H} \\delta x \\right) $$\nThe problem asks for the absolute linearization error, $|\\epsilon_{L}|$.\n\nWe are given the specific observation operator $\\mathcal{H}(x) = \\sin(x)$.\nFirst, we compute the tangent linear operator $\\mathbf{H}$:\n$$ \\mathbf{H} = \\frac{d}{dx}(\\sin(x))\\bigg|_{x=x^{i}} = \\cos(x^{i}) $$\nSubstituting the given value $x^{i} = 0.2$:\n$$ \\mathbf{H} = \\cos(0.2) $$\nNow, we can write the expression for the linearization error $\\epsilon_{L}$ using the given operator and values:\n$$ \\epsilon_{L} = \\sin(x^{i} + \\delta x) - \\left( \\sin(x^{i}) + \\cos(x^{i}) \\delta x \\right) $$\nSubstituting the numerical values $x^{i} = 0.2$ and $\\delta x = 0.5$:\n$$ \\epsilon_{L} = \\sin(0.2 + 0.5) - \\left( \\sin(0.2) + \\cos(0.2) \\times 0.5 \\right) $$\n$$ \\epsilon_{L} = \\sin(0.7) - \\sin(0.2) - 0.5 \\cos(0.2) $$\nWe now compute the numerical value of this expression, ensuring the calculator is in radian mode:\n$$ \\sin(0.7) \\approx 0.6442176872 $$\n$$ \\sin(0.2) \\approx 0.1986693308 $$\n$$ \\cos(0.2) \\approx 0.9800665778 $$\nSubstituting these values into the expression for $\\epsilon_{L}$:\n$$ \\epsilon_{L} \\approx 0.6442176872 - (0.1986693308 + 0.5 \\times 0.9800665778) $$\n$$ \\epsilon_{L} \\approx 0.6442176872 - (0.1986693308 + 0.4900332889) $$\n$$ \\epsilon_{L} \\approx 0.6442176872 - 0.6887026197 $$\n$$ \\epsilon_{L} \\approx -0.0444849325 $$\nThe absolute linearization error is the absolute value of $\\epsilon_{L}$:\n$$ |\\epsilon_{L}| = |-0.0444849325| = 0.0444849325 $$\nThe problem requires this value to be rounded to four significant figures.\n$$ |\\epsilon_{L}| \\approx 0.04448 $$\nFinally, we assess the acceptability of the tangent linear hypothesis using the given criterion: absolute linearization error $ \\sigma_{o}$.\nWe are given $\\sigma_{o} = 0.05$.\nWe check if $0.04448  0.05$. This inequality is true. Therefore, based on the provided criterion, the tangent linear hypothesis is acceptable for this increment and state.\n\nThe final answer is the computed absolute linearization error, rounded as requested.",
            "answer": "$$\\boxed{0.04448}$$"
        }
    ]
}