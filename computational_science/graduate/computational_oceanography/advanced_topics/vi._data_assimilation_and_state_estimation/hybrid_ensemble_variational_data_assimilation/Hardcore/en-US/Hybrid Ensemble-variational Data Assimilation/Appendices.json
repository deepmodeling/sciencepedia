{
    "hands_on_practices": [
        {
            "introduction": "To build a solid understanding of hybrid data assimilation, we begin with its statistical foundation: the Bayesian update. This first exercise simplifies the scenario to a single point in the ocean, allowing you to focus on the core mechanism of how a background forecast and a new observation are weighted and combined to produce an improved analysis. By working through this scalar example, you will gain a concrete feel for how the uncertainties of the background and the observation dictate the final result, a principle that underpins all advanced data assimilation systems .",
            "id": "3795131",
            "problem": "Consider a $1$-dimensional ocean surface temperature analysis at a single grid point using Hybrid Ensemble-Variational (EnVar) Data Assimilation, where the hybrid background covariance is represented by an effective scalar variance. Assume a linear observation model and Gaussian errors, consistent with the linear-Gaussian limit in which the hybrid method reduces to a Bayesian update with an effective background covariance. Specifically, let the true state $x$ (temperature in degrees Celsius) have a Gaussian prior with mean $x_b$ and variance $B$, and let the observation operator be $H$ so that the observation $y$ satisfies $y = H x + \\varepsilon$, where the observation error $\\varepsilon$ is Gaussian with zero mean and variance $R$. The prior and likelihood are thus consistent with the foundational assumptions of linear Gaussian Bayesian estimation.\n\nFor a single direct thermometer observation with $H = 1$, use the following physically plausible parameters: $x_b = 10$ (degrees Celsius), $B = 4$ (degrees Celsius squared), $y = 12$ (degrees Celsius), and $R = 1$ (degrees Celsius squared). Starting from the definitions of Gaussian prior and likelihood and the application of Bayes' theorem, derive the posterior distribution for $x$ given $y$ in the linear-Gaussian setting, and then compute the posterior mean $x_a$ (analysis temperature) and posterior variance $P_a$ (analysis uncertainty).\n\nExpress the posterior mean $x_a$ in degrees Celsius and the posterior variance $P_a$ in degrees Celsius squared. Your final numerical values should be exact (no rounding). Report your final answer as the ordered pair $\\left(x_a, P_a\\right)$.",
            "solution": "The problem statement has been critically validated and is deemed valid. It is scientifically grounded, well-posed, and objective. The problem provides a self-contained, consistent set of parameters and asks for a standard derivation in Bayesian statistics, which is a cornerstone of data assimilation.\n\nThe problem asks for the posterior probability distribution $p(x|y)$ of a state variable $x$ (temperature) given a prior distribution and an observation $y$. According to Bayes' theorem, the posterior is proportional to the product of the likelihood and the prior:\n$$\np(x|y) \\propto p(y|x) p(x)\n$$\nThe prior distribution of the state $x$ is given as a Gaussian distribution with mean $x_b$ and variance $B$:\n$$\np(x) = \\mathcal{N}(x; x_b, B) = \\frac{1}{\\sqrt{2\\pi B}} \\exp\\left(-\\frac{1}{2} \\frac{(x-x_b)^2}{B}\\right)\n$$\nThe observation model is $y = Hx + \\varepsilon$, where the observation error $\\varepsilon$ is drawn from a Gaussian distribution with mean $0$ and variance $R$. This implies that the likelihood function, which is the probability of observing $y$ given the state $x$, is also a Gaussian distribution with mean $Hx$ and variance $R$:\n$$\np(y|x) = \\mathcal{N}(y; Hx, R) = \\frac{1}{\\sqrt{2\\pi R}} \\exp\\left(-\\frac{1}{2} \\frac{(y-Hx)^2}{R}\\right)\n$$\nSubstituting these into Bayes' theorem:\n$$\np(x|y) \\propto \\exp\\left(-\\frac{1}{2} \\frac{(y-Hx)^2}{R}\\right) \\exp\\left(-\\frac{1}{2} \\frac{(x-x_b)^2}{B}\\right)\n$$\n$$\np(x|y) \\propto \\exp\\left(-\\frac{1}{2} \\left[ \\frac{(y-Hx)^2}{R} + \\frac{(x-x_b)^2}{B} \\right]\\right)\n$$\nThe term in the square brackets is the variational cost function, often denoted as $J(x)$. The posterior distribution is a Gaussian, as the product of two Gaussians is an unnormalized Gaussian. To find its parameters (mean $x_a$ and variance $P_a$), we need to rearrange the exponent into the canonical quadratic form $-\\frac{1}{2} \\frac{(x-x_a)^2}{P_a}$.\nLet's analyze the exponent term $J(x)$:\n$$\nJ(x) = \\frac{(y-Hx)^2}{R} + \\frac{(x-x_b)^2}{B}\n$$\nWe expand the squared terms:\n$$\nJ(x) = \\frac{y^2 - 2yHx + H^2x^2}{R} + \\frac{x^2 - 2xx_b + x_b^2}{B}\n$$\nNow, we collect terms with respect to powers of $x$:\n$$\nJ(x) = x^2 \\left(\\frac{H^2}{R} + \\frac{1}{B}\\right) - 2x \\left(\\frac{Hy}{R} + \\frac{x_b}{B}\\right) + \\left(\\frac{y^2}{R} + \\frac{x_b^2}{B}\\right)\n$$\nTo complete the square for $x$, we identify the form $\\frac{1}{P_a}(x-x_a)^2 = \\frac{1}{P_a}(x^2 - 2xx_a + x_a^2)$.\nComparing the coefficient of $x^2$, we find the inverse of the posterior variance $P_a$:\n$$\n\\frac{1}{P_a} = \\frac{H^2}{R} + \\frac{1}{B}\n$$\nComparing the coefficient of the $-2x$ term, we have:\n$$\n\\frac{x_a}{P_a} = \\frac{Hy}{R} + \\frac{x_b}{B}\n$$\nFrom this, we can solve for the posterior mean $x_a$:\n$$\nx_a = P_a \\left(\\frac{Hy}{R} + \\frac{x_b}{B}\\right)\n$$\nSubstituting the expression for $P_a^{-1}$:\n$$\nx_a = \\left(\\frac{H^2}{R} + \\frac{1}{B}\\right)^{-1} \\left(\\frac{Hy}{R} + \\frac{x_b}{B}\\right)\n$$\nThe problem specifies a direct observation, so the observation operator $H$ is the identity, $H=1$. The formulas simplify to:\n$$\n\\frac{1}{P_a} = \\frac{1}{R} + \\frac{1}{B} = \\frac{B+R}{BR} \\implies P_a = \\frac{BR}{B+R}\n$$\nAnd for the posterior mean $x_a$:\n$$\nx_a = P_a \\left(\\frac{y}{R} + \\frac{x_b}{B}\\right) = \\frac{BR}{B+R} \\left(\\frac{yB + x_bR}{BR}\\right) = \\frac{yB + x_bR}{B+R}\n$$\nThis expression for $x_a$ can be interpreted as a weighted average of the background mean $x_b$ and the observation $y$:\n$$\nx_a = \\left(\\frac{R}{B+R}\\right)x_b + \\left(\\frac{B}{B+R}\\right)y\n$$\nThe weight for the observation is $K = \\frac{B}{B+R}$, known as the Kalman gain in this scalar case.\n\nNow, we substitute the given numerical values:\n- Background mean: $x_b = 10$\n- Background variance: $B = 4$\n- Observation: $y = 12$\n- Observation error variance: $R = 1$\n\nWe first calculate the posterior variance $P_a$:\n$$\nP_a = \\frac{BR}{B+R} = \\frac{4 \\times 1}{4+1} = \\frac{4}{5}\n$$\nNext, we calculate the posterior mean $x_a$:\n$$\nx_a = \\frac{yB + x_bR}{B+R} = \\frac{(12)(4) + (10)(1)}{4+1} = \\frac{48 + 10}{5} = \\frac{58}{5}\n$$\nThus, the posterior distribution is Gaussian with mean $x_a = \\frac{58}{5}$ and variance $P_a = \\frac{4}{5}$.\nThe analysis temperature is $x_a = 11.6$ degrees Celsius, and the analysis uncertainty (variance) is $P_a = 0.8$ degrees Celsius squared. The problem asks for the ordered pair $(x_a, P_a)$.\n\nThe final answer is the ordered pair $\\left(x_a, P_a\\right) = \\left(\\frac{58}{5}, \\frac{4}{5}\\right)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{58}{5} & \\frac{4}{5}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Moving from a single point to a spatial domain, this practice introduces the mechanics of the incremental variational approach, a cornerstone of many operational assimilation systems. You will work with a simple two-point state to explicitly construct a hybrid background error covariance matrix by blending static and ensemble-derived components. This exercise  illuminates how spatial relationships are represented in the covariance matrix and how the variational cost function is minimized iteratively to find the optimal update to the ocean state.",
            "id": "3795136",
            "problem": "Consider a two-point horizontal coastal transect for Sea Surface Temperature (SST), represented by the state vector $\\boldsymbol{x} \\in \\mathbb{R}^{2}$ with components $\\boldsymbol{x} = (x_{1}, x_{2})^{\\top}$ in K. You are given a single satellite SST observation that measures the footprint-average of the two grid cells. The observational operator is the linear row vector $\\boldsymbol{H} = \\begin{pmatrix} 0.5 & 0.5 \\end{pmatrix}$, and the observed value is $y = 300.6$ K. The background state is $\\boldsymbol{x}_{b} = \\begin{pmatrix} 300.0 \\\\ 301.0 \\end{pmatrix}$ K. The observation error covariance is $\\boldsymbol{R} = \\begin{pmatrix} 0.09 \\end{pmatrix}$ in $\\text{K}^{2}$.\n\nThe background error covariance $\\boldsymbol{B}$ is hybrid, constructed as a convex combination of a static covariance and an ensemble-derived covariance, as used in hybrid ensemble-variational (EnVar) methods. Specifically,\n$$\n\\boldsymbol{B} = \\beta\\,\\boldsymbol{B}_{\\text{stat}} + (1-\\beta)\\,\\boldsymbol{B}_{\\text{ens}},\n$$\nwith $\\beta = 0.6$, $\\boldsymbol{B}_{\\text{stat}} = \\begin{pmatrix} 1 & 0.4 \\\\ 0.4 & 1 \\end{pmatrix}$ in $\\text{K}^{2}$, and $\\boldsymbol{B}_{\\text{ens}} = \\begin{pmatrix} 1 & 0.8 \\\\ 0.8 & 1 \\end{pmatrix}$ in $\\text{K}^{2}$. This yields\n$$\n\\boldsymbol{B} = \\begin{pmatrix} 1 & 0.56 \\\\ 0.56 & 1 \\end{pmatrix} \\quad \\text{in } \\text{K}^{2}.\n$$\n\nApply the incremental three-dimensional variational (3D-Var) formulation to assimilate the observation into the background. Use the control-variable transform $\\delta \\boldsymbol{x} = \\boldsymbol{L}\\,\\boldsymbol{v}$ where $\\boldsymbol{B} = \\boldsymbol{L}\\,\\boldsymbol{L}^{\\top}$ is the Cholesky factorization with $\\boldsymbol{L}$ lower triangular. Perform one steepest-descent iteration in control space $\\boldsymbol{v}$, starting from $\\boldsymbol{v}_{0} = \\boldsymbol{0}$, and choose the step length by exact line minimization of the quadratic cost along the steepest-descent direction. Let $\\delta \\boldsymbol{x}_{(1)}$ denote the resulting first-iteration state increment.\n\nUsing only the principles of the incremental variational cost function and the control-variable transform, compute the first-iteration increment at the first grid point, i.e., the value of $\\delta x_{1,(1)}$, in K. Round your final numerical answer to four significant figures and express the answer in kelvin. The final answer must be a single real number.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- State vector: $\\boldsymbol{x} = (x_{1}, x_{2})^{\\top} \\in \\mathbb{R}^{2}$ in units of kelvin (K).\n- Observational operator: $\\boldsymbol{H} = \\begin{pmatrix} 0.5 & 0.5 \\end{pmatrix}$.\n- Observation value: $y = 300.6$ K.\n- Background state: $\\boldsymbol{x}_{b} = \\begin{pmatrix} 300.0 \\\\ 301.0 \\end{pmatrix}$ K.\n- Observation error covariance: $\\boldsymbol{R} = \\begin{pmatrix} 0.09 \\end{pmatrix}$ in K$^2$.\n- Hybrid background error covariance: $\\boldsymbol{B} = \\beta\\,\\boldsymbol{B}_{\\text{stat}} + (1-\\beta)\\,\\boldsymbol{B}_{\\text{ens}}$.\n- Hybridization coefficient: $\\beta = 0.6$.\n- Static background error covariance: $\\boldsymbol{B}_{\\text{stat}} = \\begin{pmatrix} 1 & 0.4 \\\\ 0.4 & 1 \\end{pmatrix}$ K$^2$.\n- Ensemble-derived background error covariance: $\\boldsymbol{B}_{\\text{ens}} = \\begin{pmatrix} 1 & 0.8 \\\\ 0.8 & 1 \\end{pmatrix}$ K$^2$.\n- The problem provides the resulting hybrid covariance matrix: $\\boldsymbol{B} = \\begin{pmatrix} 1 & 0.56 \\\\ 0.56 & 1 \\end{pmatrix}$ K$^2$. A check confirms this: $0.6 \\times 0.4 + (1-0.6) \\times 0.8 = 0.24 + 0.4 \\times 0.8 = 0.24 + 0.32 = 0.56$. The diagonal elements are $0.6 \\times 1 + 0.4 \\times 1 = 1$. The provided matrix is correct.\n- Assimilation method: Incremental three-dimensional variational (3D-Var) formulation.\n- Control-variable transform: $\\delta \\boldsymbol{x} = \\boldsymbol{L}\\,\\boldsymbol{v}$, where $\\boldsymbol{B} = \\boldsymbol{L}\\,\\boldsymbol{L}^{\\top}$ is the Cholesky factorization of $\\boldsymbol{B}$ with $\\boldsymbol{L}$ being a lower triangular matrix.\n- Optimization procedure: One steepest-descent iteration in the control space of $\\boldsymbol{v}$, starting from $\\boldsymbol{v}_{0} = \\boldsymbol{0}$.\n- Step length: To be determined by exact line minimization of the cost function.\n- Quantity to be computed: The first component of the state increment after one iteration, denoted $\\delta x_{1,(1)}$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is firmly rooted in the established principles of data assimilation, specifically hybrid ensemble-variational (EnVar) methods like 3D-Var. All terms, matrices, and procedures are standard in computational oceanography and atmospheric science.\n- **Well-Posed:** All necessary data are provided. The background error covariance matrix $\\boldsymbol{B}$ is symmetric and positive definite, as its determinant is $1 - (0.56)^2 = 1 - 0.3136 = 0.6864 > 0$ and its trace is $2 > 0$. The observation error covariance $\\boldsymbol{R}$ is a positive scalar. The resulting cost function is quadratic and convex, guaranteeing a unique minimum. The task is a single, well-defined step of an optimization algorithm.\n- **Objective:** The problem is stated in precise, quantitative, and unbiased mathematical language.\n\n### Step 3: Verdict and Action\nThe problem is **valid** as it is self-contained, scientifically sound, and well-posed. A solution will be derived.\n\n### Solution Derivation\n\nThe incremental 3D-Var cost function for the state increment $\\delta\\boldsymbol{x}$ is given by:\n$$\nJ(\\delta\\boldsymbol{x}) = \\frac{1}{2} \\delta\\boldsymbol{x}^{\\top} \\boldsymbol{B}^{-1} \\delta\\boldsymbol{x} + \\frac{1}{2} (\\boldsymbol{H}\\delta\\boldsymbol{x} - \\boldsymbol{d})^{\\top} \\boldsymbol{R}^{-1} (\\boldsymbol{H}\\delta\\boldsymbol{x} - \\boldsymbol{d})\n$$\nwhere $\\boldsymbol{d} = y - \\boldsymbol{H}\\boldsymbol{x}_b$ is the innovation vector (a scalar in this case).\n\nUsing the control-variable transform $\\delta\\boldsymbol{x} = \\boldsymbol{L}\\boldsymbol{v}$, and its consequence $\\delta\\boldsymbol{x}^{\\top}\\boldsymbol{B}^{-1}\\delta\\boldsymbol{x} = \\boldsymbol{v}^{\\top}\\boldsymbol{L}^{\\top}(\\boldsymbol{L}\\boldsymbol{L}^{\\top})^{-1}\\boldsymbol{L}\\boldsymbol{v} = \\boldsymbol{v}^{\\top}\\boldsymbol{v}$, the cost function in terms of the control variable $\\boldsymbol{v}$ becomes:\n$$\nJ(\\boldsymbol{v}) = \\frac{1}{2} \\boldsymbol{v}^{\\top}\\boldsymbol{v} + \\frac{1}{2} (\\boldsymbol{H}\\boldsymbol{L}\\boldsymbol{v} - \\boldsymbol{d})^{\\top} \\boldsymbol{R}^{-1} (\\boldsymbol{H}\\boldsymbol{L}\\boldsymbol{v} - \\boldsymbol{d})\n$$\nThis is a quadratic function of $\\boldsymbol{v}$. The gradient of the cost function with respect to $\\boldsymbol{v}$ is:\n$$\n\\nabla J(\\boldsymbol{v}) = \\boldsymbol{v} + (\\boldsymbol{H}\\boldsymbol{L})^{\\top} \\boldsymbol{R}^{-1} (\\boldsymbol{H}\\boldsymbol{L}\\boldsymbol{v} - \\boldsymbol{d})\n$$\nThe steepest-descent method updates the control variable iteratively:\n$$\n\\boldsymbol{v}_{k+1} = \\boldsymbol{v}_{k} - \\alpha_k \\nabla J(\\boldsymbol{v}_{k})\n$$\nWe start from $\\boldsymbol{v}_0 = \\boldsymbol{0}$. The gradient at this point, $\\boldsymbol{g}_0 = \\nabla J(\\boldsymbol{v}_0)$, is:\n$$\n\\boldsymbol{g}_0 = \\boldsymbol{0} + (\\boldsymbol{H}\\boldsymbol{L})^{\\top} \\boldsymbol{R}^{-1} (\\boldsymbol{H}\\boldsymbol{L}\\boldsymbol{0} - \\boldsymbol{d}) = -(\\boldsymbol{H}\\boldsymbol{L})^{\\top} \\boldsymbol{R}^{-1} \\boldsymbol{d}\n$$\nThe step length $\\alpha_0$ is determined by exact line minimization, which for a quadratic cost function has a closed-form solution:\n$$\n\\alpha_0 = \\frac{\\boldsymbol{g}_{0}^{\\top}\\boldsymbol{g}_{0}}{\\boldsymbol{g}_{0}^{\\top}\\boldsymbol{A}\\boldsymbol{g}_{0}}\n$$\nwhere $\\boldsymbol{A}$ is the Hessian of $J(\\boldsymbol{v})$, which is constant:\n$$\n\\boldsymbol{A} = \\frac{\\partial^2 J}{\\partial \\boldsymbol{v} \\partial \\boldsymbol{v}^{\\top}} = \\boldsymbol{I} + (\\boldsymbol{H}\\boldsymbol{L})^{\\top} \\boldsymbol{R}^{-1} (\\boldsymbol{H}\\boldsymbol{L})\n$$\nThe first iteration update is therefore $\\boldsymbol{v}_1 = \\boldsymbol{v}_0 - \\alpha_0 \\boldsymbol{g}_0 = -\\alpha_0 \\boldsymbol{g}_0$. Once $\\boldsymbol{v}_1$ is found, the state increment is computed as $\\delta \\boldsymbol{x}_{(1)} = \\boldsymbol{L}\\boldsymbol{v}_1$.\n\nWe now proceed with the numerical calculations.\n\n1.  **Innovation vector $\\boldsymbol{d}$**:\n    $$\n    \\boldsymbol{H}\\boldsymbol{x}_b = \\begin{pmatrix} 0.5 & 0.5 \\end{pmatrix} \\begin{pmatrix} 300.0 \\\\ 301.0 \\end{pmatrix} = 0.5 \\times 300.0 + 0.5 \\times 301.0 = 150.0 + 150.5 = 300.5\n    $$\n    $$\n    \\boldsymbol{d} = y - \\boldsymbol{H}\\boldsymbol{x}_b = 300.6 - 300.5 = 0.1\n    $$\n\n2.  **Cholesky factorization of $\\boldsymbol{B}$**:\n    We need to find a lower triangular matrix $\\boldsymbol{L} = \\begin{pmatrix} l_{11} & 0 \\\\ l_{21} & l_{22} \\end{pmatrix}$ such that $\\boldsymbol{L}\\boldsymbol{L}^{\\top} = \\boldsymbol{B} = \\begin{pmatrix} 1 & 0.56 \\\\ 0.56 & 1 \\end{pmatrix}$.\n    $l_{11}^2 = 1 \\implies l_{11} = 1$.\n    $l_{21}l_{11} = 0.56 \\implies l_{21} = 0.56$.\n    $l_{21}^2 + l_{22}^2 = 1 \\implies (0.56)^2 + l_{22}^2 = 1 \\implies 0.3136 + l_{22}^2 = 1$.\n    $l_{22}^2 = 1 - 0.3136 = 0.6864 \\implies l_{22} = \\sqrt{0.6864}$.\n    So, $\\boldsymbol{L} = \\begin{pmatrix} 1 & 0 \\\\ 0.56 & \\sqrt{0.6864} \\end{pmatrix}$.\n\n3.  **Intermediate terms**:\n    $\\boldsymbol{R}^{-1} = (0.09)^{-1} = \\frac{1}{0.09} = \\frac{100}{9}$.\n    $\\boldsymbol{H}\\boldsymbol{L} = \\begin{pmatrix} 0.5 & 0.5 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0.56 & \\sqrt{0.6864} \\end{pmatrix} = \\begin{pmatrix} 0.5(1) + 0.5(0.56) & 0.5(\\sqrt{0.6864}) \\end{pmatrix} = \\begin{pmatrix} 0.78 & 0.5\\sqrt{0.6864} \\end{pmatrix}$.\n\n4.  **Gradient at $\\boldsymbol{v}_0$**:\n    $$\n    \\boldsymbol{g}_0 = -(\\boldsymbol{H}\\boldsymbol{L})^{\\top} \\boldsymbol{R}^{-1} \\boldsymbol{d} = - \\begin{pmatrix} 0.78 \\\\ 0.5\\sqrt{0.6864} \\end{pmatrix} \\left(\\frac{100}{9}\\right) (0.1) = -\\frac{10}{9} \\begin{pmatrix} 0.78 \\\\ 0.5\\sqrt{0.6864} \\end{pmatrix}\n    $$\n\n5.  **Step length $\\alpha_0$**:\n    For this specific problem structure where there is a single observation, the gradient $\\boldsymbol{g}_0$ is an eigenvector of the Hessian $\\boldsymbol{A}$. This means the first step of steepest descent with exact line search converges to the true minimum. The step length is the reciprocal of the corresponding eigenvalue, $\\alpha_0 = 1/\\lambda$.\n    The eigenvalue is $\\lambda = 1 + \\boldsymbol{R}^{-1}(\\boldsymbol{H}\\boldsymbol{L})(\\boldsymbol{H}\\boldsymbol{L})^{\\top}$.\n    $$\n    (\\boldsymbol{H}\\boldsymbol{L})(\\boldsymbol{H}\\boldsymbol{L})^{\\top} = (0.78)^2 + (0.5\\sqrt{0.6864})^2 = 0.6084 + 0.25(0.6864) = 0.6084 + 0.1716 = 0.78\n    $$\n    Note that this is equivalent to $\\boldsymbol{H}\\boldsymbol{B}\\boldsymbol{H}^{\\top} = \\begin{pmatrix} 0.5 & 0.5 \\end{pmatrix} \\begin{pmatrix} 1 & 0.56 \\\\ 0.56 & 1 \\end{pmatrix} \\begin{pmatrix} 0.5 \\\\ 0.5 \\end{pmatrix} = 0.78$.\n    The eigenvalue is:\n    $$\n    \\lambda = 1 + \\left(\\frac{100}{9}\\right) (0.78) = 1 + \\frac{78}{9} = 1 + \\frac{26}{3} = \\frac{29}{3}\n    $$\n    The step length is:\n    $$\n    \\alpha_0 = \\frac{1}{\\lambda} = \\frac{3}{29}\n    $$\n\n6.  **Control variable update $\\boldsymbol{v}_1$**:\n    $$\n    \\boldsymbol{v}_1 = -\\alpha_0 \\boldsymbol{g}_0 = -\\left(\\frac{3}{29}\\right) \\left(-\\frac{10}{9} \\begin{pmatrix} 0.78 \\\\ 0.5\\sqrt{0.6864} \\end{pmatrix}\\right) = \\frac{10}{87} \\begin{pmatrix} 0.78 \\\\ 0.5\\sqrt{0.6864} \\end{pmatrix}\n    $$\n\n7.  **State increment $\\delta \\boldsymbol{x}_{(1)}$**:\n    $$\n    \\delta \\boldsymbol{x}_{(1)} = \\boldsymbol{L}\\boldsymbol{v}_1 = \\begin{pmatrix} 1 & 0 \\\\ 0.56 & \\sqrt{0.6864} \\end{pmatrix} \\frac{10}{87} \\begin{pmatrix} 0.78 \\\\ 0.5\\sqrt{0.6864} \\end{pmatrix}\n    $$\n    $$\n    \\delta \\boldsymbol{x}_{(1)} = \\frac{10}{87} \\begin{pmatrix} 1 \\times 0.78 + 0 \\\\ 0.56 \\times 0.78 + \\sqrt{0.6864} \\times 0.5\\sqrt{0.6864} \\end{pmatrix}\n    $$\n    $$\n    \\delta \\boldsymbol{x}_{(1)} = \\frac{10}{87} \\begin{pmatrix} 0.78 \\\\ 0.4368 + 0.5 \\times 0.6864 \\end{pmatrix} = \\frac{10}{87} \\begin{pmatrix} 0.78 \\\\ 0.4368 + 0.3432 \\end{pmatrix} = \\frac{10}{87} \\begin{pmatrix} 0.78 \\\\ 0.78 \\end{pmatrix}\n    $$\n\n8.  **Target component $\\delta x_{1,(1)}$**:\n    The first component of the increment is:\n    $$\n    \\delta x_{1,(1)} = \\frac{10}{87} \\times 0.78 = \\frac{7.8}{87} = \\frac{78}{870} = \\frac{13}{145}\n    $$\n    Converting to a decimal value:\n    $$\n    \\delta x_{1,(1)} = \\frac{13}{145} \\approx 0.08965517...\n    $$\n    Rounding to four significant figures, we get $0.08966$. The units are kelvin.",
            "answer": "$$\n\\boxed{0.08966}\n$$"
        },
        {
            "introduction": "This final exercise demonstrates one of the most powerful capabilities of hybrid data assimilation: the generation of multivariate \"balanced\" updates. By assimilating a temperature observation, you will compute the resulting update not only for temperature but also for salinity, a variable that was not directly observed. This practice  highlights how ensemble-derived cross-covariances capture physically-meaningful relationships between different state variables, allowing the information from a single observation type to be spread intelligently across the full ocean state.",
            "id": "3795180",
            "problem": "In a single grid cell of a primitive-equation ocean model, consider the two-component state vector composed of in-situ temperature and practical salinity, denoted by $x = \\begin{pmatrix} T \\\\ S \\end{pmatrix}$. A Hybrid Ensemble-Variational (EnVar) Data Assimilation system is used to form a background error covariance by linearly combining an ensemble-estimated covariance and a static climatological covariance. Observations provide only the local temperature. The hybrid combination weight is $\\,\\beta\\,$ with $\\,0 < \\beta < 1\\,$.\n\nYou are provided with the following second-moment statistics at this grid cell:\n- Ensemble-estimated second moments: $\\operatorname{var}_{e}(T) = 0.6$, $\\operatorname{var}_{e}(S) = 0.25$, and $\\operatorname{cov}_{e}(S,T) = -0.09$.\n- Static climatological second moments: $\\operatorname{var}_{s}(T) = 0.4$, $\\operatorname{var}_{s}(S) = 0.3$, and $\\operatorname{cov}_{s}(S,T) = -0.06$.\n\nThe hybrid weight is $\\beta = 0.65$. The temperature observation operator is $H = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$, and the observation error variance is $R = 0.05$. The observed-minus-background temperature innovation at this cell is $d = +0.3$.\n\nAssume linear-Gaussian dynamics and measurement error, and use multivariate linear regression consistent with Hybrid EnVar to compute the balanced salinity analysis increment, $\\delta S$, produced at this grid cell by assimilating the temperature observation. Express your final answer in practical salinity units (psu) and round your result to four significant figures.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of oceanographic data assimilation, well-posed with a complete and consistent set of parameters, and stated objectively. We can therefore proceed to a solution.\n\nThe goal is to compute the salinity analysis increment, $\\delta S$, resulting from the assimilation of a temperature observation. The state vector is $x = \\begin{pmatrix} T \\\\ S \\end{pmatrix}$, where $T$ is the in-situ temperature and $S$ is the practical salinity. The analysis increment vector, $\\delta x = \\begin{pmatrix} \\delta T \\\\ \\delta S \\end{pmatrix}$, is given by the standard linear-Gaussian update equation:\n$$ \\delta x = K d $$\nwhere $d$ is the innovation (observed-minus-background) and $K$ is the Kalman gain matrix.\n\nThe Kalman gain $K$ is defined as:\n$$ K = B H^T (H B H^T + R)^{-1} $$\nHere, $B$ is the background error covariance matrix, $H$ is the observation operator, and $R$ is the observation error variance.\n\nFirst, we must construct the hybrid background error covariance matrix $B$. The hybrid covariance matrix $B$ is constructed as:\n$$ B = (1-\\beta) B_s + \\beta B_e $$\nwhere the given weight $\\beta=0.65$ is for the ensemble component.\n\nThe problem provides the necessary second-moment statistics to construct $B_s$ and $B_e$.\nThe state vector is ordered as $\\begin{pmatrix} T \\\\ S \\end{pmatrix}$. The covariance matrices are of the form $\\begin{pmatrix} \\operatorname{var}(T) & \\operatorname{cov}(T,S) \\\\ \\operatorname{cov}(S,T) & \\operatorname{var}(S) \\end{pmatrix}$.\n\nFrom the givens:\nThe ensemble-estimated covariance matrix $B_e$ is:\n$$ B_e = \\begin{pmatrix} \\operatorname{var}_{e}(T) & \\operatorname{cov}_{e}(T,S) \\\\ \\operatorname{cov}_{e}(S,T) & \\operatorname{var}_{e}(S) \\end{pmatrix} = \\begin{pmatrix} 0.6 & -0.09 \\\\ -0.09 & 0.25 \\end{pmatrix} $$\nThe static climatological covariance matrix $B_s$ is:\n$$ B_s = \\begin{pmatrix} \\operatorname{var}_{s}(T) & \\operatorname{cov}_{s}(T,S) \\\\ \\operatorname{cov}_{s}(S,T) & \\operatorname{var}_{s}(S) \\end{pmatrix} = \\begin{pmatrix} 0.4 & -0.06 \\\\ -0.06 & 0.3 \\end{pmatrix} $$\n\nThe hybrid weight is given as $\\beta = 0.65$, so $1-\\beta = 1 - 0.65 = 0.35$.\nNow we compute the hybrid covariance matrix $B$:\n$$ B = 0.35 \\begin{pmatrix} 0.4 & -0.06 \\\\ -0.06 & 0.3 \\end{pmatrix} + 0.65 \\begin{pmatrix} 0.6 & -0.09 \\\\ -0.09 & 0.25 \\end{pmatrix} $$\n$$ B = \\begin{pmatrix} 0.35 \\times 0.4 & 0.35 \\times (-0.06) \\\\ 0.35 \\times (-0.06) & 0.35 \\times 0.3 \\end{pmatrix} + \\begin{pmatrix} 0.65 \\times 0.6 & 0.65 \\times (-0.09) \\\\ 0.65 \\times (-0.09) & 0.65 \\times 0.25 \\end{pmatrix} $$\n$$ B = \\begin{pmatrix} 0.14 & -0.021 \\\\ -0.021 & 0.105 \\end{pmatrix} + \\begin{pmatrix} 0.39 & -0.0585 \\\\ -0.0585 & 0.1625 \\end{pmatrix} $$\n$$ B = \\begin{pmatrix} 0.14+0.39 & -0.021-0.0585 \\\\ -0.021-0.0585 & 0.105+0.1625 \\end{pmatrix} = \\begin{pmatrix} 0.53 & -0.0795 \\\\ -0.0795 & 0.2675 \\end{pmatrix} $$\n\nNext, we calculate the terms in the Kalman gain formula. The observation operator is $H = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$, and its transpose is $H^T = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$. The observation error variance is $R = 0.05$.\n\nCalculate $B H^T$:\n$$ B H^T = \\begin{pmatrix} 0.53 & -0.0795 \\\\ -0.0795 & 0.2675 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0.53 \\\\ -0.0795 \\end{pmatrix} $$\n\nCalculate $H B H^T$:\n$$ H B H^T = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0.53 \\\\ -0.0795 \\end{pmatrix} = 0.53 $$\nThis term represents the background error variance projected into the observation space.\n\nNow, calculate the denominator of the Kalman gain, which is a scalar in this case:\n$$ H B H^T + R = 0.53 + 0.05 = 0.58 $$\n\nWe can now assemble the Kalman gain matrix $K$:\n$$ K = B H^T (H B H^T + R)^{-1} = \\begin{pmatrix} 0.53 \\\\ -0.0795 \\end{pmatrix} (0.58)^{-1} = \\begin{pmatrix} \\frac{0.53}{0.58} \\\\ \\frac{-0.0795}{0.58} \\end{pmatrix} $$\n\nFinally, we compute the analysis increment vector $\\delta x$ using the given innovation $d = 0.3$:\n$$ \\delta x = \\begin{pmatrix} \\delta T \\\\ \\delta S \\end{pmatrix} = K d = \\begin{pmatrix} \\frac{0.53}{0.58} \\\\ \\frac{-0.0795}{0.58} \\end{pmatrix} (0.3) $$\n\nThe problem asks for the salinity analysis increment, $\\delta S$, which is the second component of the vector $\\delta x$:\n$$ \\delta S = \\frac{-0.0795}{0.58} \\times 0.3 $$\n$$ \\delta S = \\frac{-0.02385}{0.58} \\approx -0.0411206896... $$\n\nRounding the result to four significant figures, we get:\n$$ \\delta S \\approx -0.04112 $$\nThis value is in practical salinity units (psu), consistent with the problem context. The negative sign is physically consistent with the negative temperature-salinity covariance; a positive temperature innovation (warmer observation) leads to a negative salinity increment (fresher analysis).",
            "answer": "$$\\boxed{-0.04112}$$"
        }
    ]
}