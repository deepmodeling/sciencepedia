{
    "hands_on_practices": [
        {
            "introduction": "This practice lays the mathematical foundation of the adjoint method. Before implementing any numerical model, it is crucial to derive the continuous adjoint equations to understand their structure and properties. This exercise  guides you through this fundamental derivation for a tracer advection-diffusion equation, demonstrating how the principle of integration by parts reveals the adjoint operator and its backward-in-time nature.",
            "id": "3813441",
            "problem": "Consider a bounded, simply connected ocean domain $\\Omega \\subset \\mathbb{R}^{3}$ with smooth boundary $\\partial \\Omega$ and outward unit normal $\\mathbf{n}$. Let the tracer concentration $T(\\mathbf{x},t)$ evolve on the time interval $t \\in [0,t_{f}]$ according to the linear advection–diffusion Partial Differential Equation (PDE)\n$$\nT_{t} + \\mathbf{u}(\\mathbf{x},t) \\cdot \\nabla T - \\nabla \\cdot \\big( \\mathbf{K}(\\mathbf{x},t) \\nabla T \\big) = 0,\n$$\nwhere $\\mathbf{u}(\\mathbf{x},t)$ is a prescribed divergence-free velocity field satisfying $\\nabla \\cdot \\mathbf{u} = 0$ and $\\mathbf{u} \\cdot \\mathbf{n} = 0$ on $\\partial \\Omega$, and $\\mathbf{K}(\\mathbf{x},t)$ is a symmetric, uniformly positive definite diffusivity tensor. Impose homogeneous no-flux boundary conditions $(\\mathbf{K} \\nabla T) \\cdot \\mathbf{n} = 0$ on $\\partial \\Omega$ and a given initial condition $T(\\mathbf{x},0) = T_{0}(\\mathbf{x})$.\n\nDefine the terminal-data mismatch cost functional\n$$\n\\mathcal{J}(T) = \\frac{1}{2} \\int_{\\Omega} \\big( T(\\mathbf{x},t_{f}) - T_{d}(\\mathbf{x}) \\big)^{2} \\, d\\mathbf{x},\n$$\nwhere $T_{d}(\\mathbf{x})$ is a prescribed target tracer field at time $t_{f}$. Using the $L^{2}(\\Omega)$ inner product as the fundamental pairing and only the divergence theorem and integration by parts as your core tools, derive the continuous adjoint formulation for the Lagrange multiplier field $\\psi(\\mathbf{x},t)$ associated with the forward PDE constraint, under the same homogeneous boundary conditions $(\\mathbf{K} \\nabla \\psi) \\cdot \\mathbf{n} = 0$ and $\\mathbf{u} \\cdot \\mathbf{n} = 0$ on $\\partial \\Omega$.\n\nYour derivation must:\n- Start from the augmented Lagrangian constructed from $\\mathcal{J}(T)$ and the forward PDE constraint with multiplier $\\psi(\\mathbf{x},t)$.\n- Systematically apply integration by parts in time and space to move all derivatives off of $\\delta T$ and onto $\\psi$, clearly identifying the interior adjoint operator and all boundary contributions.\n- Use only the stated properties (divergence-free $\\mathbf{u}$, symmetry and positive definiteness of $\\mathbf{K}$, and the homogeneous boundary conditions) to eliminate boundary terms.\n- Show that the adjoint field evolves backward in time with a terminal condition at $t_{f}$ that arises from the variation of $\\mathcal{J}$.\n\nReport, as your final answer, the analytic expression for the continuous adjoint operator acting on $\\psi$ (the interior operator that multiplies $\\delta T$ in the volume integral after all integrations by parts). Provide only the operator expression, not an equation, and do not include any units. If you obtain multiple equivalent forms, choose the form expressed in divergence notation rather than expanded product-rule form. No rounding is required.",
            "solution": "The problem has been validated and is a well-posed, scientifically grounded exercise in the derivation of a continuous adjoint model. The derivation proceeds using the method of Lagrange multipliers.\n\nFirst, we define the forward model constraint as the PDE being equal to zero:\n$$\nG(T) = T_{t} + \\mathbf{u}(\\mathbf{x},t) \\cdot \\nabla T - \\nabla \\cdot \\big( \\mathbf{K}(\\mathbf{x},t) \\nabla T \\big) = 0\n$$\nThe augmented Lagrangian, $\\mathcal{L}$, is constructed by adding the cost functional $\\mathcal{J}$ to the inner product of the constraint $G(T)$ and the Lagrange multiplier field $\\psi(\\mathbf{x}, t)$. The inner product is taken over the spacetime domain $\\Omega \\times [0,t_{f}]$.\n$$\n\\mathcal{L}(T, \\psi) = \\mathcal{J}(T) + \\int_{0}^{t_{f}} \\int_{\\Omega} \\psi G(T) \\, d\\mathbf{x} \\, dt\n$$\nSubstituting the expressions for $\\mathcal{J}(T)$ and $G(T)$:\n$$\n\\mathcal{L}(T, \\psi) = \\frac{1}{2} \\int_{\\Omega} \\big( T(\\mathbf{x},t_{f}) - T_{d}(\\mathbf{x}) \\big)^{2} \\, d\\mathbf{x} + \\int_{0}^{t_{f}} \\int_{\\Omega} \\psi \\left( T_{t} + \\mathbf{u} \\cdot \\nabla T - \\nabla \\cdot (\\mathbf{K} \\nabla T) \\right) \\, d\\mathbf{x} \\, dt\n$$\nTo find the adjoint equations, we compute the first variation of $\\mathcal{L}$ with respect to $T$, denoted $\\delta \\mathcal{L}$, by considering a small perturbation $\\delta T$. The adjoint equations are found by requiring $\\delta \\mathcal{L} = 0$ for all admissible perturbations $\\delta T$.\nThe variation is:\n$$\n\\delta \\mathcal{L} = \\int_{\\Omega} \\big( T(\\mathbf{x},t_{f}) - T_{d}(\\mathbf{x}) \\big) \\delta T(\\mathbf{x},t_{f}) \\, d\\mathbf{x} + \\int_{0}^{t_{f}} \\int_{\\Omega} \\psi \\left( \\delta T_{t} + \\mathbf{u} \\cdot \\nabla(\\delta T) - \\nabla \\cdot (\\mathbf{K} \\nabla(\\delta T)) \\right) \\, d\\mathbf{x} \\, dt\n$$\nThe core of the method is to use integration by parts to transfer all derivative operators from the perturbation $\\delta T$ to the multiplier field $\\psi$. We analyze each term within the spacetime integral.\n\n1.  The time derivative term: $\\int_{0}^{t_{f}} \\int_{\\Omega} \\psi \\delta T_{t} \\, d\\mathbf{x} \\, dt$.\n    Integrating by parts with respect to time $t$:\n    $$\n    \\int_{0}^{t_{f}} \\psi \\delta T_{t} \\, dt = [\\psi \\delta T]_{0}^{t_{f}} - \\int_{0}^{t_{f}} \\psi_t \\delta T \\, dt = \\psi(t_{f}) \\delta T(t_{f}) - \\psi(0) \\delta T(0) - \\int_{0}^{t_{f}} \\psi_t \\delta T \\, dt\n    $$\n    Since the initial condition $T(\\mathbf{x},0) = T_0(\\mathbf{x})$ is fixed, any admissible perturbation must have $\\delta T(\\mathbf{x},0) = 0$. Thus, the term becomes:\n    $$\n    \\int_{0}^{t_{f}} \\int_{\\Omega} \\psi \\delta T_{t} \\, d\\mathbf{x} \\, dt = \\int_{\\Omega} \\psi(t_{f}) \\delta T(t_{f}) \\, d\\mathbf{x} - \\int_{0}^{t_{f}} \\int_{\\Omega} \\psi_t \\delta T \\, d\\mathbf{x} \\, dt\n    $$\n\n2.  The advection term: $\\int_{0}^{t_{f}} \\int_{\\Omega} \\psi (\\mathbf{u} \\cdot \\nabla(\\delta T)) \\, d\\mathbf{x} \\, dt$.\n    We use the vector identity $\\nabla \\cdot (a \\mathbf{V}) = (\\nabla a) \\cdot \\mathbf{V} + a (\\nabla \\cdot \\mathbf{V})$. Applying this with $a = \\psi \\delta T$ and $\\mathbf{V} = \\mathbf{u}$, and given that $\\nabla \\cdot \\mathbf{u} = 0$, we have $\\nabla \\cdot ((\\psi \\delta T)\\mathbf{u}) = \\nabla(\\psi \\delta T) \\cdot \\mathbf{u} = (\\psi \\nabla(\\delta T) + \\delta T \\nabla \\psi) \\cdot \\mathbf{u}$. Rearranging gives:\n    $$\n    \\psi (\\mathbf{u} \\cdot \\nabla(\\delta T)) = \\nabla \\cdot((\\psi \\delta T)\\mathbf{u}) - \\delta T(\\mathbf{u} \\cdot \\nabla \\psi)\n    $$\n    Integrating over $\\Omega$ and applying the divergence theorem:\n    $$\n    \\int_{\\Omega} \\psi (\\mathbf{u} \\cdot \\nabla(\\delta T)) \\, d\\mathbf{x} = \\int_{\\partial \\Omega} (\\psi \\delta T) \\mathbf{u} \\cdot \\mathbf{n} \\, dS - \\int_{\\Omega} \\delta T(\\mathbf{u} \\cdot \\nabla \\psi) \\, d\\mathbf{x}\n    $$\n    The boundary integral vanishes because of the given boundary condition $\\mathbf{u} \\cdot \\mathbf{n} = 0$ on $\\partial \\Omega$. Therefore, we have:\n    $$\n    \\int_{0}^{t_{f}} \\int_{\\Omega} \\psi (\\mathbf{u} \\cdot \\nabla(\\delta T)) \\, d\\mathbf{x} \\, dt = - \\int_{0}^{t_{f}} \\int_{\\Omega} (\\mathbf{u} \\cdot \\nabla \\psi) \\delta T \\, d\\mathbf{x} \\, dt\n    $$\n\n3.  The diffusion term: $\\int_{0}^{t_{f}} \\int_{\\Omega} \\psi \\left( - \\nabla \\cdot (\\mathbf{K} \\nabla(\\delta T)) \\right) \\, d\\mathbf{x} \\, dt$.\n    We perform integration by parts twice. First, using Green's first identity on the spatial integral:\n    $$\n    - \\int_{\\Omega} \\psi \\nabla \\cdot (\\mathbf{K} \\nabla(\\delta T)) \\, d\\mathbf{x} = - \\int_{\\partial \\Omega} \\psi (\\mathbf{K} \\nabla(\\delta T)) \\cdot \\mathbf{n} \\, dS + \\int_{\\Omega} \\nabla \\psi \\cdot (\\mathbf{K} \\nabla(\\delta T)) \\, d\\mathbf{x}\n    $$\n    The boundary term is zero because perturbations must satisfy the same homogeneous no-flux condition as the forward variable, i.e., $(\\mathbf{K} \\nabla(\\delta T)) \\cdot \\mathbf{n} = 0$ on $\\partial \\Omega$. The remaining term is $\\int_{\\Omega} \\nabla \\psi \\cdot (\\mathbf{K} \\nabla(\\delta T)) \\, d\\mathbf{x}$. Since $\\mathbf{K}$ is symmetric, this is equal to $\\int_{\\Omega} (\\mathbf{K} \\nabla \\psi) \\cdot \\nabla(\\delta T) \\, d\\mathbf{x}$. We apply integration by parts a second time:\n    $$\n    \\int_{\\Omega} (\\mathbf{K} \\nabla \\psi) \\cdot \\nabla(\\delta T) \\, d\\mathbf{x} = \\int_{\\partial \\Omega} \\delta T (\\mathbf{K} \\nabla \\psi) \\cdot \\mathbf{n} \\, dS - \\int_{\\Omega} \\delta T \\nabla \\cdot (\\mathbf{K} \\nabla \\psi) \\, d\\mathbf{x}\n    $$\n    The boundary integral vanishes due to the prescribed boundary condition for the adjoint field, $(\\mathbf{K} \\nabla \\psi) \\cdot \\mathbf{n} = 0$ on $\\partial \\Omega$. Combining these steps, the diffusion term becomes:\n    $$\n    - \\int_{0}^{t_{f}} \\int_{\\Omega} \\psi \\nabla \\cdot (\\mathbf{K} \\nabla(\\delta T)) \\, d\\mathbf{x} \\, dt = - \\int_{0}^{t_{f}} \\int_{\\Omega} \\nabla \\cdot (\\mathbf{K} \\nabla \\psi) \\delta T \\, d\\mathbf{x} \\, dt\n    $$\n\nNow, we substitute these transformed terms back into the expression for $\\delta \\mathcal{L}$:\n$$\n\\delta \\mathcal{L} = \\int_{\\Omega} \\big( T(t_{f}) - T_{d} \\big) \\delta T(t_{f}) \\, d\\mathbf{x} + \\int_{\\Omega} \\psi(t_{f}) \\delta T(t_{f}) \\, d\\mathbf{x} - \\int_{0}^{t_{f}} \\int_{\\Omega} \\psi_t \\delta T \\, d\\mathbf{x} \\, dt - \\int_{0}^{t_{f}} \\int_{\\Omega} (\\mathbf{u} \\cdot \\nabla \\psi) \\delta T \\, d\\mathbf{x} \\, dt - \\int_{0}^{t_{f}} \\int_{\\Omega} \\nabla \\cdot (\\mathbf{K} \\nabla \\psi) \\delta T \\, d\\mathbf{x} \\, dt\n$$\nGrouping the terms by the type of integral:\n$$\n\\delta \\mathcal{L} = \\int_{\\Omega} \\left[ T(t_{f}) - T_{d} + \\psi(t_{f}) \\right] \\delta T(t_{f}) \\, d\\mathbf{x} + \\int_{0}^{t_{f}} \\int_{\\Omega} \\left[ - \\psi_t - \\mathbf{u} \\cdot \\nabla \\psi - \\nabla \\cdot (\\mathbf{K} \\nabla \\psi) \\right] \\delta T \\, d\\mathbf{x} \\, dt\n$$\nFor $\\delta \\mathcal{L}$ to be zero for any arbitrary admissible perturbation $\\delta T$, the expressions in the square brackets must be zero. This yields the adjoint system. The terminal condition for $\\psi$ at $t=t_{f}$ is:\n$$\n\\psi(\\mathbf{x}, t_{f}) = T_d(\\mathbf{x}) - T(\\mathbf{x}, t_{f})\n$$\nThe adjoint PDE, which must hold for $t \\in [0, t_{f})$, is obtained by setting the integrand of the spacetime integral to zero:\n$$\n- \\psi_t - \\mathbf{u} \\cdot \\nabla \\psi - \\nabla \\cdot (\\mathbf{K} \\nabla \\psi) = 0\n$$\nThis equation describes the backward-in-time evolution of the adjoint field $\\psi$. The problem asks for the analytic expression for the continuous adjoint operator acting on $\\psi$, which is precisely the left-hand side of this equation. This is the interior operator that multiplies $\\delta T$ in the volume integral after all manipulations.",
            "answer": "$$\n\\boxed{- \\psi_{t} - \\mathbf{u} \\cdot \\nabla \\psi - \\nabla \\cdot (\\mathbf{K} \\nabla \\psi)}\n$$"
        },
        {
            "introduction": "Building on the basic formulation, this practice addresses a key challenge in real-world data assimilation: ill-posedness. To ensure stable and physically plausible results, we often incorporate prior knowledge about the control parameters through regularization. This exercise  demonstrates how to add a Tikhonov regularization term to the cost functional and correctly derive the modified gradient, a simple yet powerful technique essential for practical inverse modeling.",
            "id": "3813433",
            "problem": "Consider a Four-Dimensional Variational (4D-Var) data assimilation problem in a regional ocean model for sea surface temperature, where the prognostic tracer field $T(\\mathbf{x},t)$ satisfies a well-posed partial differential equation with appropriate boundary conditions, and depends on a spatially distributed parameter field $p \\in \\mathbb{R}^{m}$ that controls a linearized surface heat flux term of the form $q(\\mathbf{x},t;p) = p(\\mathbf{x})\\left(T(\\mathbf{x},t) - T_{\\text{air}}(\\mathbf{x},t)\\right)$. Let $\\{y_{i}\\}_{i=1}^{N}$ denote observational data with known observation operators $\\{H_{i}\\}_{i=1}^{N}$ and times $\\{t_{i}\\}_{i=1}^{N}$, and define the baseline least-squares misfit cost functional\n$$\nJ_{\\text{misfit}}(p) = \\frac{1}{2} \\sum_{i=1}^{N} \\left\\| H_{i} T(\\cdot,t_{i};p) - y_{i} \\right\\|^{2},\n$$\nwhere $\\|\\cdot\\|$ denotes the Euclidean norm on the observation space. Assume the model dynamics and observation operators are sufficiently smooth so that the adjoint method yields a well-defined gradient $\\nabla_{p} J_{\\text{misfit}}(p) \\in \\mathbb{R}^{m}$ with respect to $p$.\n\nTo stabilize the inversion and encode prior information $p_{\\text{ref}} \\in \\mathbb{R}^{m}$, implement Tikhonov regularization by augmenting the cost functional with a quadratic penalty\n$$\nJ_{\\text{reg}}(p) = J_{\\text{misfit}}(p) + \\frac{\\alpha}{2} \\left\\| p - p_{\\text{ref}} \\right\\|^{2},\n$$\nwhere $\\alpha > 0$ is a prescribed regularization parameter and $\\|\\cdot\\|$ is the Euclidean norm on $\\mathbb{R}^{m}$. Starting from the fundamental definitions of the Gâteaux derivative and the inner-product representation of gradients in Euclidean spaces, derive the modified gradient $\\nabla_{p} J_{\\text{reg}}(p)$ that includes the contribution of the regularization term. Express your final answer as a single closed-form analytic expression in terms of $\\nabla_{p} J_{\\text{misfit}}(p)$, $\\alpha$, $p$, and $p_{\\text{ref}}$. No numerical evaluation is required, and no units are to be reported in the final expression.",
            "solution": "The problem statement has been validated and is deemed to be scientifically grounded, well-posed, and objective. It presents a standard Tikhonov regularization problem within the context of 4D-Var data assimilation, a core topic in computational oceanography. All components are clearly defined and consistent. We may therefore proceed with the derivation.\n\nThe objective is to derive the gradient of the regularized cost functional, $\\nabla_{p} J_{\\text{reg}}(p)$, with respect to the parameter vector $p \\in \\mathbb{R}^{m}$. The regularized cost functional is given by\n$$\nJ_{\\text{reg}}(p) = J_{\\text{misfit}}(p) + \\frac{\\alpha}{2} \\left\\| p - p_{\\text{ref}} \\right\\|^{2}\n$$\nwhere $J_{\\text{misfit}}(p)$ is the data misfit term, $\\alpha > 0$ is a scalar regularization parameter, $p_{\\text{ref}}$ is a given reference parameter vector, and $\\|\\cdot\\|$ signifies the Euclidean norm on the parameter space $\\mathbb{R}^{m}$.\n\nThe gradient operator is linear. Therefore, the gradient of the sum is the sum of the gradients:\n$$\n\\nabla_{p} J_{\\text{reg}}(p) = \\nabla_{p} \\left( J_{\\text{misfit}}(p) + \\frac{\\alpha}{2} \\left\\| p - p_{\\text{ref}} \\right\\|^{2} \\right) = \\nabla_{p} J_{\\text{misfit}}(p) + \\nabla_{p} \\left( \\frac{\\alpha}{2} \\left\\| p - p_{\\text{ref}} \\right\\|^{2} \\right)\n$$\nThe problem statement provides that $\\nabla_{p} J_{\\text{misfit}}(p)$ is well-defined and computable via the adjoint method. Our task reduces to finding the gradient of the regularization (or penalty) term, which we shall denote as $J_{\\text{penalty}}(p)$:\n$$\nJ_{\\text{penalty}}(p) = \\frac{\\alpha}{2} \\left\\| p - p_{\\text{ref}} \\right\\|^{2}\n$$\nTo derive the gradient, we begin with the fundamental definition of the Gâteaux derivative. The Gâteaux derivative of a functional $F: \\mathbb{R}^{m} \\to \\mathbb{R}$ at a point $p$ in the direction of a vector $\\delta p \\in \\mathbb{R}^{m}$ is defined as:\n$$\nD F(p; \\delta p) = \\lim_{\\epsilon \\to 0} \\frac{F(p + \\epsilon \\delta p) - F(p)}{\\epsilon}\n$$\nThe gradient, $\\nabla_{p} F(p)$, is the unique vector in $\\mathbb{R}^{m}$ that represents the Gâteaux derivative as an inner product for all directions $\\delta p$, according to the Riesz representation theorem. For the Euclidean space $\\mathbb{R}^{m}$ with the standard inner product $\\langle \\cdot, \\cdot \\rangle$, this relationship is:\n$$\nD F(p; \\delta p) = \\langle \\nabla_{p} F(p), \\delta p \\rangle\n$$\nWe now apply this formalism to $J_{\\text{penalty}}(p)$. Let's evaluate the functional at the perturbed point $p + \\epsilon \\delta p$:\n$$\nJ_{\\text{penalty}}(p + \\epsilon \\delta p) = \\frac{\\alpha}{2} \\left\\| (p + \\epsilon \\delta p) - p_{\\text{ref}} \\right\\|^{2} = \\frac{\\alpha}{2} \\left\\| (p - p_{\\text{ref}}) + \\epsilon \\delta p \\right\\|^{2}\n$$\nThe squared Euclidean norm can be expressed using the inner product: $\\|v\\|^2 = \\langle v, v \\rangle$. Expanding the norm gives:\n$$\n\\left\\| (p - p_{\\text{ref}}) + \\epsilon \\delta p \\right\\|^{2} = \\langle (p - p_{\\text{ref}}) + \\epsilon \\delta p, (p - p_{\\text{ref}}) + \\epsilon \\delta p \\rangle\n$$\nBy the bilinearity of the inner product, this expands to:\n$$\n= \\langle p - p_{\\text{ref}}, p - p_{\\text{ref}} \\rangle + 2\\epsilon \\langle p - p_{\\text{ref}}, \\delta p \\rangle + \\epsilon^2 \\langle \\delta p, \\delta p \\rangle\n$$\n$$\n= \\| p - p_{\\text{ref}} \\|^2 + 2\\epsilon \\langle p - p_{\\text{ref}}, \\delta p \\rangle + \\epsilon^2 \\| \\delta p \\|^2\n$$\nSubstituting this back into the expression for $J_{\\text{penalty}}(p + \\epsilon \\delta p)$:\n$$\nJ_{\\text{penalty}}(p + \\epsilon \\delta p) = \\frac{\\alpha}{2} \\left( \\| p - p_{\\text{ref}} \\|^2 + 2\\epsilon \\langle p - p_{\\text{ref}}, \\delta p \\rangle + \\epsilon^2 \\| \\delta p \\|^2 \\right)\n$$\n$$\nJ_{\\text{penalty}}(p + \\epsilon \\delta p) = \\frac{\\alpha}{2} \\| p - p_{\\text{ref}} \\|^2 + \\alpha \\epsilon \\langle p - p_{\\text{ref}}, \\delta p \\rangle + \\frac{\\alpha}{2} \\epsilon^2 \\| \\delta p \\|^2\n$$\nRecognizing that the first term is $J_{\\text{penalty}}(p)$, we have:\n$$\nJ_{\\text{penalty}}(p + \\epsilon \\delta p) = J_{\\text{penalty}}(p) + \\alpha \\epsilon \\langle p - p_{\\text{ref}}, \\delta p \\rangle + O(\\epsilon^2)\n$$\nNow we compute the Gâteaux derivative:\n$$\nD J_{\\text{penalty}}(p; \\delta p) = \\lim_{\\epsilon \\to 0} \\frac{ \\left( J_{\\text{penalty}}(p) + \\alpha \\epsilon \\langle p - p_{\\text{ref}}, \\delta p \\rangle + O(\\epsilon^2) \\right) - J_{\\text{penalty}}(p)}{\\epsilon}\n$$\n$$\nD J_{\\text{penalty}}(p; \\delta p) = \\lim_{\\epsilon \\to 0} \\frac{\\alpha \\epsilon \\langle p - p_{\\text{ref}}, \\delta p \\rangle + O(\\epsilon^2)}{\\epsilon} = \\lim_{\\epsilon \\to 0} \\left( \\alpha \\langle p - p_{\\text{ref}}, \\delta p \\rangle + O(\\epsilon) \\right)\n$$\n$$\nD J_{\\text{penalty}}(p; \\delta p) = \\alpha \\langle p - p_{\\text{ref}}, \\delta p \\rangle\n$$\nWe equate this result with the definition of the gradient in terms of the inner product:\n$$\n\\langle \\nabla_{p} J_{\\text{penalty}}(p), \\delta p \\rangle = \\alpha \\langle p - p_{\\text{ref}}, \\delta p \\rangle = \\langle \\alpha(p - p_{\\text{ref}}), \\delta p \\rangle\n$$\nSince this equality must hold for all possible directional vectors $\\delta p \\in \\mathbb{R}^{m}$, the vectors themselves must be equal:\n$$\n\\nabla_{p} J_{\\text{penalty}}(p) = \\alpha (p - p_{\\text{ref}})\n$$\nFinally, we substitute this result back into the expression for the gradient of the total regularized cost functional:\n$$\n\\nabla_{p} J_{\\text{reg}}(p) = \\nabla_{p} J_{\\text{misfit}}(p) + \\nabla_{p} J_{\\text{penalty}}(p)\n$$\nThis yields the final expression for the modified gradient:\n$$\n\\nabla_{p} J_{\\text{reg}}(p) = \\nabla_{p} J_{\\text{misfit}}(p) + \\alpha (p - p_{\\text{ref}})\n$$\nThis expression demonstrates that the gradient of the Tikhonov-regularized functional is the sum of the original misfit gradient (computed via the adjoint model) and a simple term that penalizes deviations of the control parameter vector $p$ from the prior reference state $p_{\\text{ref}}$.",
            "answer": "$$\n\\boxed{\\nabla_{p} J_{\\text{misfit}}(p) + \\alpha (p - p_{\\text{ref}})}\n$$"
        },
        {
            "introduction": "The ultimate test of an adjoint model lies in its correct implementation. This hands-on coding practice  introduces the \"gradient check,\" the definitive method for verifying that your adjoint code computes the exact gradient of your discrete forward model. You will implement both a forward model and its discrete adjoint, then use a finite-difference approximation to validate your gradient calculation, exploring the critical trade-offs between truncation and round-off error in the process.",
            "id": "3813416",
            "problem": "Consider a one-dimensional, periodic, linear tracer evolution model representative of a depth-averaged coastal channel. The tracer concentration field $c(x,t)$ is governed by the advection-diffusion equation with constant velocity and diffusivity, expressed as $$\\frac{\\partial c}{\\partial t} = -u \\frac{\\partial c}{\\partial x} + \\kappa \\frac{\\partial^2 c}{\\partial x^2},$$ where $u$ is the depth-averaged along-channel velocity in $\\mathrm{m/s}$ and $\\kappa$ is the eddy diffusivity in $\\mathrm{m^2/s}$. The parameter vector is defined as $p = (p_1, p_2)$ where $p_1 = u$ and $p_2 = \\beta$ with $\\kappa = \\exp(\\beta)$ enforcing strictly positive diffusivity.\n\nDiscretize the spatial domain of length $L$ into $N_x$ uniformly spaced grid points with spacing $\\Delta x = L/N_x$, and impose periodic boundary conditions. Use a first-order upwind finite-difference operator for advection assuming $u > 0$, and a second-order central finite-difference operator for diffusion:\n- The upwind difference operator $D_u$ satisfies $(D_u c)_i = \\frac{c_i - c_{i-1}}{\\Delta x}$, with periodic wrapping of indices.\n- The diffusion operator $D_2$ satisfies $(D_2 c)_i = \\frac{c_{i-1} - 2 c_i + c_{i+1}}{\\Delta x^2}$, again with periodic wrapping.\n\nTime is advanced using the explicit Euler method with a fixed time step $\\Delta t$ and a total time $T$. The discrete forward update for the state $c^k \\in \\mathbb{R}^{N_x}$ at step $k$ is\n$$c^{k+1} = M(p) \\, c^k,\\quad M(p) = I + \\Delta t\\left(-u D_u + \\kappa D_2\\right),$$\nwhere $I$ is the $N_x \\times N_x$ identity matrix. The initial condition $c^0$ is specified as\n$$c^0_i = \\sin\\left(\\frac{2\\pi x_i}{L}\\right) + \\frac{1}{2}\\sin\\left(\\frac{4\\pi x_i}{L}\\right),\\quad x_i = i \\Delta x,$$\nand the target terminal profile is $c^\\star = 0$ (the zero vector). Define the differentiable scalar objective\n$$J(p) = \\frac{1}{2} \\Delta x \\sum_{i=0}^{N_x-1} \\left(c^N_i - c^\\star_i\\right)^2 = \\frac{1}{2} \\Delta x \\left\\|c^N\\right\\|_2^2,$$\nwhere $N$ is the total number of time steps, $N = \\lfloor T/\\Delta t \\rfloor$.\n\nUsing the discrete adjoint method, derive and implement the gradient $\\nabla_p J(p) = \\left(\\frac{\\partial J}{\\partial p_1}, \\frac{\\partial J}{\\partial p_2}\\right)$ at a nominal parameter $p$. For the explicit Euler scheme above, the adjoint recursion for the adjoint variable $\\lambda^k \\in \\mathbb{R}^{N_x}$ is\n$$\\lambda^N = \\Delta x \\left(c^N - c^\\star\\right),\\quad \\lambda^k = M(p)^\\top \\lambda^{k+1},\\quad k = N-1,\\dots,0,$$\nand the parameter gradient components are\n$$\\frac{\\partial J}{\\partial u} = \\sum_{k=0}^{N-1} \\left(\\lambda^{k+1}\\right)^\\top \\left(\\frac{\\partial M}{\\partial u}\\right) c^k,\\quad \\frac{\\partial M}{\\partial u} = -\\Delta t \\, D_u,$$\n$$\\frac{\\partial J}{\\partial \\beta} = \\sum_{k=0}^{N-1} \\left(\\lambda^{k+1}\\right)^\\top \\left(\\frac{\\partial M}{\\partial \\beta}\\right) c^k,\\quad \\frac{\\partial M}{\\partial \\beta} = \\Delta t \\, \\kappa \\, D_2.$$\n\nDesign and implement a finite-difference gradient check using the central difference approximation for each component $i \\in \\{1,2\\}$:\n$$\\mathrm{FD}_i(\\epsilon) = \\frac{J\\left(p + \\epsilon e_i\\right) - J\\left(p - \\epsilon e_i\\right)}{2\\epsilon},$$\nwhere $e_i$ is the $i$-th canonical basis vector in $\\mathbb{R}^2$ and $\\epsilon$ is a scalar step size. Compare $\\mathrm{FD}_i(\\epsilon)$ with the adjoint gradient $\\frac{\\partial J}{\\partial p_i}$ by computing the relative error\n$$E_i(\\epsilon) = \\frac{\\left|\\mathrm{FD}_i(\\epsilon) - \\frac{\\partial J}{\\partial p_i}\\right|}{\\left|\\frac{\\partial J}{\\partial p_i}\\right| + 10^{-14}}.$$\n\nIn the discussion, address step-size selection trade-offs that influence $E_i(\\epsilon)$, including truncation error scaling with $\\epsilon^2$ for the central difference formula and floating-point round-off effects scaling inversely with $\\epsilon$, and the role of machine precision. Relate the choice of $\\Delta t$ to stability and accuracy via the Courant-Friedrichs-Lewy (CFL) condition to ensure that comparisons are meaningful across perturbed parameters.\n\nUse the following scientifically consistent configuration:\n- Domain length $L = 1000$ in $\\mathrm{m}$.\n- Number of grid points $N_x = 128$.\n- Nominal parameters $u = 0.12$ in $\\mathrm{m/s}$ and $\\kappa = 5\\times 10^{-3}$ in $\\mathrm{m^2/s}$, with $\\beta = \\log(\\kappa)$.\n- Fixed time step computed once from the nominal parameters as $$\\Delta t = 0.4 \\times \\min\\left(\\frac{\\Delta x}{u}, \\frac{\\Delta x^2}{2\\kappa}\\right),$$ and total time $T = 2048$ in $\\mathrm{s}$, yielding $N = \\lfloor T/\\Delta t \\rfloor$.\n- Assume strictly positive $u$ so that the upwind stencil definition above holds without switching.\n\nImplement a program that:\n1. Constructs the discrete operators and performs the forward simulation to compute $J(p)$.\n2. Computes the discrete adjoint to evaluate $\\nabla_p J(p)$.\n3. For each test step size $\\epsilon$ in the test suite below, evaluates $\\mathrm{FD}_i(\\epsilon)$ for $i=1,2$, computes the relative errors $E_i(\\epsilon)$, and returns the maximum over components $E_{\\max}(\\epsilon) = \\max\\left(E_1(\\epsilon), E_2(\\epsilon)\\right)$.\n\nTest suite of step sizes (dimensionless scalar added to $u$ and $\\beta$ respectively):\n- $\\epsilon = 10^{-1}$ (coarse, truncation-dominated regime),\n- $\\epsilon = 10^{-3}$ (intermediate),\n- $\\epsilon = 10^{-5}$ (fine),\n- $\\epsilon = 10^{-7}$ (very fine),\n- $\\epsilon = 10^{-9}$ (near the round-off influence),\n- $\\epsilon = 10^{-11}$ (extreme, round-off-dominated regime).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the step sizes listed above, where each entry is $E_{\\max}(\\epsilon)$ as a floating-point number (e.g., \"[r1,r2,r3,r4,r5,r6]\"). All physical quantities used internally must be consistent with the units specified. Angles are not used; if any appear, they must be interpreted as radians by default. The final outputs are unitless relative errors as defined.",
            "solution": "The problem is assessed to be **valid**. It presents a well-posed, scientifically grounded task in computational science, specifically the application of the discrete adjoint method for sensitivity analysis of a one-dimensional advection-diffusion model. All parameters, equations, and numerical methods are explicitly defined and consistent, forming a self-contained and verifiable problem.\n\nThe solution proceeds by first discretizing the governing partial differential equation, then implementing the forward and adjoint models as specified, and finally performing a gradient check using finite differences to verify the adjoint implementation.\n\n### 1. Discretization of the Governing Equation\n\nThe governing one-dimensional advection-diffusion equation is\n$$\n\\frac{\\partial c}{\\partial t} = -u \\frac{\\partial c}{\\partial x} + \\kappa \\frac{\\partial^2 c}{\\partial x^2}\n$$\nThe spatial domain of length $L$ is discretized into $N_x$ grid points $x_i = i \\Delta x$ for $i=0, \\dots, N_x-1$, with grid spacing $\\Delta x = L/N_x$. The tracer concentration $c(x,t)$ is represented by a state vector $c(t) \\in \\mathbb{R}^{N_x}$, where $(c(t))_i = c(x_i, t)$.\n\nThe spatial derivative operators are discretized using finite differences with periodic boundary conditions.\nThe first-order upwind advection operator for $u>0$ is given by $(D_u c)_i = (c_i - c_{i-1}) / \\Delta x$. This can be represented by a circulant matrix $D_u$.\nThe second-order central difference diffusion operator is $(D_2 c)_i = (c_{i-1} - 2c_i + c_{i+1}) / \\Delta x^2$, which is also represented by a circulant matrix $D_2$.\n\n### 2. The Forward Model\n\nTime integration is performed using the explicit Euler method with a fixed time step $\\Delta t$. The discrete state vector $c^k$ at time step $k$ evolves according to the linear map:\n$$\nc^{k+1} = M(p) \\, c^k\n$$\nwhere $p = (u, \\beta)$ is the parameter vector with $\\kappa = \\exp(\\beta)$. The propagator matrix $M(p)$ is defined as:\n$$\nM(p) = I + \\Delta t \\left(-u D_u + \\kappa D_2\\right)\n$$\nwhere $I$ is the $N_x \\times N_x$ identity matrix.\n\nThe simulation starts from a specified initial condition $c^0_i = \\sin(2\\pi x_i/L) + \\frac{1}{2}\\sin(4\\pi x_i/L)$ and runs for $N = \\lfloor T/\\Delta t \\rfloor$ steps to obtain the final state $c^N$. The time step $\\Delta t$ is fixed based on the nominal parameters to ensure stability of the explicit scheme via the Courant-Friedrichs-Lewy (CFL) condition:\n$$\n\\Delta t = 0.4 \\times \\min\\left(\\frac{\\Delta x}{u_{\\text{nom}}}, \\frac{\\Delta x^2}{2\\kappa_{\\text{nom}}}\\right)\n$$\n\nThe objective function to be minimized is a measure of the mismatch between the final state $c^N$ and a target state $c^\\star = 0$:\n$$\nJ(p) = \\frac{1}{2} \\Delta x \\sum_{i=0}^{N_x-1} (c^N_i - c^\\star_i)^2 = \\frac{1}{2} \\Delta x \\|c^N\\|_2^2\n$$\n\n### 3. The Discrete Adjoint Model\n\nThe goal is to compute the gradient of the objective function with respect to the parameters, $\\nabla_p J$. The discrete adjoint method provides an efficient means to do so. We introduce a discrete Lagrangian:\n$$\n\\mathcal{L}(c^{0..N}, \\lambda^{1..N}, p) = J(c^N) - \\sum_{k=0}^{N-1} (\\lambda^{k+1})^\\top \\left( c^{k+1} - M(p) c^k \\right)\n$$\nwhere $\\lambda^k \\in \\mathbb{R}^{N_x}$ are the adjoint variables (Lagrange multipliers). By requiring the gradient of $\\mathcal{L}$ with respect to each state variable $c^k$ to be zero, we derive the adjoint equations.\nFor the final state $c^N$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial c^N} = \\nabla_{c^N} J - \\lambda^N = 0 \\implies \\lambda^N = \\nabla_{c^N} J = \\Delta x \\, c^N\n$$\nFor intermediate states $c^k$ where $k=1, \\dots, N-1$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial c^k} = (\\lambda^{k+1})^\\top M(p) - \\lambda^k = 0 \\implies \\lambda^k = M(p)^\\top \\lambda^{k+1}\n$$\nThese equations define a recurrence relation for the adjoint variables, which are computed by marching backward in time from $\\lambda^N$.\n\nOnce the forward history $c^k$ and the adjoint history $\\lambda^k$ are known, the gradient of the objective function is found by differentiating the Lagrangian with respect to the parameters $p$:\n$$\n\\nabla_p J = \\frac{d\\mathcal{L}}{dp} = \\frac{\\partial \\mathcal{L}}{\\partial p} = \\sum_{k=0}^{N-1} (\\lambda^{k+1})^\\top \\frac{\\partial M(p)}{\\partial p} c^k\n$$\nThe specific components for $p_1=u$ and $p_2=\\beta$ are:\n$$\n\\frac{\\partial J}{\\partial u} = \\sum_{k=0}^{N-1} (\\lambda^{k+1})^\\top \\left(\\frac{\\partial M}{\\partial u}\\right) c^k, \\quad \\text{with} \\quad \\frac{\\partial M}{\\partial u} = -\\Delta t \\, D_u\n$$\n$$\n\\frac{\\partial J}{\\partial \\beta} = \\sum_{k=0}^{N-1} (\\lambda^{k+1})^\\top \\left(\\frac{\\partial M}{\\partial \\beta}\\right) c^k, \\quad \\text{with} \\quad \\frac{\\partial M}{\\partial \\beta} = \\Delta t \\frac{\\partial \\kappa}{\\partial \\beta} D_2 = \\Delta t \\, e^\\beta D_2 = \\Delta t \\, \\kappa \\, D_2\n$$\nThe algorithm therefore consists of:\n1. Running the forward model from $k=0$ to $N$, storing the entire state history $\\{c^k\\}_{k=0}^N$.\n2. Initializing the adjoint variable $\\lambda^N$.\n3. Running the adjoint model backward from $k=N-1$ to $0$, accumulating the gradient contributions at each step using the stored forward states $c^k$ and the computed adjoint variables $\\lambda^{k+1}$.\n\n### 4. Gradient Verification and Step-Size Selection\n\nThe correctness of the adjoint gradient implementation is verified against a finite-difference approximation. A central difference scheme is used for higher accuracy:\n$$\n\\mathrm{FD}_i(\\epsilon) = \\frac{J(p + \\epsilon e_i) - J(p - \\epsilon e_i)}{2\\epsilon}\n$$\nwhere $e_i$ is a canonical basis vector. The relative error $E_i(\\epsilon)$ measures the discrepancy:\n$$\nE_i(\\epsilon) = \\frac{\\left| \\mathrm{FD}_i(\\epsilon) - \\frac{\\partial J}{\\partial p_i} \\right|}{\\left| \\frac{\\partial J}{\\partial p_i} \\right| + \\delta}\n$$\nwhere $\\delta=10^{-14}$ is a small regularization term to prevent division by zero.\n\nThe choice of the step size $\\epsilon$ is critical. The error in the central difference approximation has two main sources:\n1.  **Truncation Error**: Taylor expansion of $J$ shows that the central difference formula has a leading error term proportional to $\\epsilon^2$. This error decreases as $\\epsilon$ becomes smaller.\n2.  **Round-off Error**: When $\\epsilon$ becomes very small, the evaluation of $J(p + \\epsilon e_i) - J(p - \\epsilon e_i)$ suffers from catastrophic cancellation (subtractive cancellation), as the two terms become nearly identical. This loss of precision, combined with division by a very small $2\\epsilon$, causes the round-off error to grow, typically scaling as $1/\\epsilon$.\n\nThe total error is a sum of these two effects. Consequently, the plot of $E_i(\\epsilon)$ versus $\\epsilon$ on a log-log scale typically exhibits a \"V\" shape. For large $\\epsilon$, the error is dominated by truncation error and decreases with $\\epsilon$. For very small $\\epsilon$, it is dominated by round-off error and increases as $\\epsilon$ decreases. The optimal $\\epsilon$ that minimizes the total error lies at the bottom of this \"V\", balancing the two error sources. The test suite of $\\epsilon$ values provided in the problem is designed to sample the error across these regimes, from truncation-dominated ($\\epsilon=10^{-1}$) to round-off-dominated ($\\epsilon=10^{-11}$).",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of computing and verifying the adjoint gradient for a 1D\n    advection-diffusion model.\n    \"\"\"\n    # ------------------\n    # 1. Configuration and Setup\n    # ------------------\n    # Physical and numerical constants\n    L = 1000.0          # Domain length [m]\n    Nx = 128            # Number of grid points\n    u_nom = 0.12        # Nominal velocity [m/s]\n    kappa_nom = 5e-3    # Nominal diffusivity [m^2/s]\n    T = 2048.0          # Total time [s]\n    \n    # Test suite for finite difference step size\n    eps_test_suite = [1e-1, 1e-3, 1e-5, 1e-7, 1e-9, 1e-11]\n\n    # Derived parameters\n    dx = L / Nx\n    x_grid = np.arange(Nx) * dx\n    beta_nom = np.log(kappa_nom)\n    p_nom = np.array([u_nom, beta_nom])\n\n    # Construct discrete operators using dense matrices as Nx is small\n    def get_operators(Nx_in, dx_in):\n        # First-order upwind advection operator D_u for u > 0\n        # (D_u c)_i = (c_i - c_{i-1}) / dx\n        Du = (np.diag(np.ones(Nx_in)) - np.diag(np.ones(Nx_in - 1), k=-1)) / dx_in\n        Du[0, -1] = -1.0 / dx_in  # Periodic boundary condition\n        \n        # Second-order central difference diffusion operator D_2\n        # (D_2 c)_i = (c_{i-1} - 2c_i + c_{i+1}) / dx^2\n        D2 = (np.diag(np.ones(Nx_in - 1), k=-1) - 2 * np.diag(np.ones(Nx_in)) + \n              np.diag(np.ones(Nx_in - 1), k=1)) / dx_in**2\n        D2[0, -1] = 1.0 / dx_in**2  # Periodic BC\n        D2[-1, 0] = 1.0 / dx_in**2  # Periodic BC\n        return Du, D2\n\n    Du, D2 = get_operators(Nx, dx)\n\n    # Calculate fixed time step based on nominal CFL condition\n    cfl_adv_nom = dx / u_nom\n    cfl_diff_nom = dx**2 / (2 * kappa_nom)\n    dt = 0.4 * min(cfl_adv_nom, cfl_diff_nom)\n    N = int(np.floor(T / dt))\n\n    # Initial condition\n    c0 = np.sin(2 * np.pi * x_grid / L) + 0.5 * np.sin(4 * np.pi * x_grid / L)\n\n    # ------------------\n    # 2. Forward and Adjoint Solvers\n    # ------------------\n    # Function to run the forward model and compute objective J\n    def compute_j(p_vec):\n        u_p, beta_p = p_vec\n        kappa_p = np.exp(beta_p)\n        \n        # Forward model propagator matrix M(p)\n        M_p = np.eye(Nx) + dt * (-u_p * Du + kappa_p * D2)\n        \n        # Time-stepping loop\n        c_k = c0.copy()\n        for _ in range(N):\n            c_k = M_p @ c_k\n        c_N = c_k\n        \n        # Objective function J\n        J = 0.5 * dx * np.sum(c_N**2)\n        return J\n\n    # Function to compute the adjoint gradient\n    def compute_adjoint_gradient(p_vec):\n        u_p, beta_p = p_vec\n        kappa_p = np.exp(beta_p)\n        \n        # Propagator matrix M(p) and its transpose\n        M_p = np.eye(Nx) + dt * (-u_p * Du + kappa_p * D2)\n        MT_p = M_p.T\n        \n        # Run forward model and store state history\n        c_hist = np.zeros((N + 1, Nx))\n        c_hist[0] = c0\n        for k in range(N):\n            c_hist[k+1] = M_p @ c_hist[k]\n        c_N = c_hist[-1]\n        \n        # Derivatives of M with respect to parameters\n        dM_du = -dt * Du\n        dM_dbeta = dt * kappa_p * D2\n        \n        # Initialize adjoint variable and gradients\n        lambda_next_k = dx * c_N  # This is lambda^N\n        grad_u = 0.0\n        grad_beta = 0.0\n        \n        # Backward time-stepping for adjoint variable and gradient accumulation\n        for k in range(N - 1, -1, -1):\n            # At step k, lambda_next_k holds lambda^{k+1}\n            c_k = c_hist[k]\n            \n            # Accumulate gradient contributions\n            grad_u += lambda_next_k.T @ dM_du @ c_k\n            grad_beta += lambda_next_k.T @ dM_dbeta @ c_k\n            \n            # Update adjoint variable: lambda^k = M^T lambda^{k+1}\n            lambda_next_k = MT_p @ lambda_next_k\n        \n        return np.array([grad_u, grad_beta])\n\n    # ------------------\n    # 3. Gradient Calculation and Verification\n    # ------------------\n    # Compute adjoint gradient at the nominal parameter values\n    adj_grad = compute_adjoint_gradient(p_nom)\n\n    results = []\n    # Loop through the test suite of epsilons for the finite-difference check\n    for eps in eps_test_suite:\n        # Finite difference for p1 = u\n        p_plus_u = np.array([p_nom[0] + eps, p_nom[1]])\n        p_minus_u = np.array([p_nom[0] - eps, p_nom[1]])\n        J_plus_u = compute_j(p_plus_u)\n        J_minus_u = compute_j(p_minus_u)\n        fd_grad_u = (J_plus_u - J_minus_u) / (2 * eps)\n        \n        # Finite difference for p2 = beta\n        p_plus_beta = np.array([p_nom[0], p_nom[1] + eps])\n        p_minus_beta = np.array([p_nom[0], p_nom[1] - eps])\n        J_plus_beta = compute_j(p_plus_beta)\n        J_minus_beta = compute_j(p_minus_beta)\n        fd_grad_beta = (J_plus_beta - J_minus_beta) / (2 * eps)\n        \n        # Compute relative errors\n        err_u = np.abs(fd_grad_u - adj_grad[0]) / (np.abs(adj_grad[0]) + 1e-14)\n        err_beta = np.abs(fd_grad_beta - adj_grad[1]) / (np.abs(adj_grad[1]) + 1e-14)\n        \n        # Store the maximum relative error for this epsilon\n        E_max = max(err_u, err_beta)\n        results.append(E_max)\n\n    # ------------------\n    # 4. Final Output\n    # ------------------\n    # Format results into a single string as specified.\n    print(f\"[{','.join(f'{r:.8e}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}