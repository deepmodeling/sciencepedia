{
    "hands_on_practices": [
        {
            "introduction": "Empirical Orthogonal Function (EOF) analysis is designed to decompose the variability of a dataset. Therefore, the essential first step is to isolate this variability by removing the time-mean state from the data. This practice focuses on this fundamental preprocessing stage, where you will construct an anomaly matrix by subtracting the temporal mean from a given spatio-temporal field. Mastering this ensures that the subsequent EOF analysis is performed on the fluctuations themselves, which is crucial for obtaining physically meaningful patterns of change rather than patterns of the mean state. ",
            "id": "3792009",
            "problem": "Consider Empirical Orthogonal Function (EOF) analysis, which requires constructing an anomaly matrix with zero temporal mean at each spatial location. Let a Sea Surface Temperature (SST) field be represented by a matrix $S \\in \\mathbb{R}^{N \\times T}$, where $N$ is the number of spatial grid points and $T$ is the number of time steps, with entries $S_{i t}$ denoting the SST (in degrees Celsius) at grid point $i$ at time index $t$. The anomaly matrix $X \\in \\mathbb{R}^{N \\times T}$ is defined by removing the temporal mean at each grid point. Formally, define the temporal mean at grid point $i$ as $\\mu_i = \\frac{1}{T}\\sum_{t=0}^{T-1} S_{i t}$, and define the anomaly matrix entries as $X_{i t} = S_{i t} - \\mu_i$. For numerical verification, define a tolerance $\\tau = 10^{-12}$ and declare a row $i$ to be zero-mean if $\\left|\\frac{1}{T}\\sum_{t=0}^{T-1} X_{i t}\\right| \\le \\tau$. The task is to compute $X$ for each test case and verify that each row of $X$ has zero mean to numerical precision.\n\nYou must construct $S$ for each of the following test cases using the given deterministic formulas. Indices in all formulas are zero-based: $i \\in \\{0,1,\\dots,N-1\\}$ and $t \\in \\{0,1,\\dots,T-1\\}$. All SST values are in degrees Celsius.\n\nTest Suite:\n- Case $1$: $N=4$, $T=5$, $S_{i t} = 20 + 0.5\\,i - 0.3\\,t + 0.5\\,(-1)^t$.\n- Case $2$: $N=3$, $T=1$, $S_{i 0} = 14 + 6\\,i$.\n- Case $3$: $N=2$, $T=8$, $S_{i t} = 10^{7} + 10^{5}\\,i + 1000\\,t + 100\\,(-1)^t$.\n- Case $4$: $N=1$, $T=6$, $S_{0 t} = 18$ for all $t$.\n- Case $5$: $N=5$, $T=7$, $S_{i t} = 10 + 0.2\\,i + 0.01\\,t^2 - 0.5\\,t$.\n\nFor each case:\n1. Compute the temporal mean $\\mu_i$ for each row $i$.\n2. Compute the anomaly matrix $X$ by $X_{i t} = S_{i t} - \\mu_i$.\n3. Verify that the mean of each row of $X$ is within the tolerance, i.e., $\\left|\\frac{1}{T}\\sum_{t=0}^{T-1} X_{i t}\\right| \\le \\tau$ for all rows $i$, using $\\tau = 10^{-12}$.\n4. Return a boolean indicating whether all rows satisfy the zero-mean condition.\n\nYour program should produce a single line of output containing the results for the five cases as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5]\"), where each entry is a boolean (\"True\" or \"False\").",
            "solution": "The user-provided problem has been analyzed and is deemed valid.\n\nThe problem requires the implementation and verification of a fundamental step in Empirical Orthogonal Function (EOF) analysis: the creation of an anomaly matrix. This task is scientifically grounded, well-posed, and objective. It presents a clear computational challenge rooted in the field of computational oceanography. All necessary parameters, definitions, and formulas are provided, creating a self-contained and solvable problem.\n\nThe core of the task is to transform a data matrix $S \\in \\mathbb{R}^{N \\times T}$ into an anomaly matrix $X \\in \\mathbb{R}^{N \\times T}$. Here, $N$ represents the number of spatial locations and $T$ represents the number of time steps. The element $S_{i t}$ is the data value (Sea Surface Temperature) at spatial point $i$ and time $t$.\n\nThe transformation is defined by removing the temporal mean from each spatial point. The temporal mean for a given spatial point $i$ (the $i$-th row of $S$) is given by:\n$$\n\\mu_i = \\frac{1}{T}\\sum_{t=0}^{T-1} S_{i t}\n$$\nThe corresponding anomaly matrix $X$ is then constructed by subtracting this mean from each element in that row:\n$$\nX_{i t} = S_{i t} - \\mu_i\n$$\nA key mathematical property of the anomaly matrix $X$ is that the temporal mean of each of its rows is, by definition, zero. We can demonstrate this analytically:\n$$\n\\text{Mean of row } i \\text{ of } X = \\frac{1}{T}\\sum_{t=0}^{T-1} X_{i t} = \\frac{1}{T}\\sum_{t=0}^{T-1} (S_{i t} - \\mu_i)\n$$\nBy distributing the summation:\n$$\n= \\left(\\frac{1}{T}\\sum_{t=0}^{T-1} S_{i t}\\right) - \\left(\\frac{1}{T}\\sum_{t=0}^{T-1} \\mu_i\\right)\n$$\nThe first term is the definition of $\\mu_i$. Since $\\mu_i$ is a constant with respect to the summation index $t$, the second term simplifies to $\\frac{1}{T}(T \\cdot \\mu_i) = \\mu_i$. Therefore:\n$$\n= \\mu_i - \\mu_i = 0\n$$\nThe problem asks for a numerical verification of this property. In digital computers, floating-point arithmetic can introduce small precision errors. Consequently, a direct comparison to $0$ might fail. The problem specifies a tolerance $\\tau = 10^{-12}$, such that a row $i$ is considered to have a zero mean if the absolute value of its computed mean is within this tolerance:\n$$\n\\left|\\frac{1}{T}\\sum_{t=0}^{T-1} X_{i t}\\right| \\le \\tau\n$$\nThe final result for each test case is a boolean (`True`) if all $N$ rows of its anomaly matrix $X$ satisfy this condition, and `False` otherwise.\n\nThe algorithm to solve the problem for each test case is as follows:\n1.  Initialize an $N \\times T$ matrix, $S$, with floating-point numbers.\n2.  Populate the matrix $S$ according to the specified formula $S_{i t}$ for the given case, iterating through each spatial index $i \\in \\{0, \\dots, N-1\\}$ and temporal index $t \\in \\{0, \\dots, T-1\\}$.\n3.  Compute the temporal mean for each row of $S$. This can be efficiently achieved by calculating the mean along the axis corresponding to time (axis $1$ in a row-major implementation). This yields a vector of means $\\boldsymbol{\\mu} \\in \\mathbb{R}^{N}$.\n4.  Construct the anomaly matrix $X$ by subtracting the mean vector from the data matrix $S$. This operation requires broadcasting the mean of each row across all elements of that row. For an $N \\times T$ matrix $S$ and an $N \\times 1$ column vector of means, the subtraction $X = S - \\boldsymbol{\\mu}$ performs this operation element-wise.\n5.  Compute the temporal mean for each row of the resulting anomaly matrix $X$.\n6.  For each of these newly computed row means, check if its absolute value is less than or equal to the tolerance $\\tau=10^{-12}$.\n7.  The overall result for the test case is `True` if the condition from step 6 holds for all rows, and `False` otherwise.\nThis procedure will be applied to all five test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes anomaly matrices for several SST test cases and verifies\n    if each row has a zero temporal mean to numerical precision.\n    \"\"\"\n\n    # Define the tolerance for numerical verification.\n    tau = 1e-12\n\n    # Define the test cases from the problem statement.\n    # Each case is a dictionary with N, T, and a lambda function for S_it.\n    test_cases = [\n        {\n            \"N\": 4, \"T\": 5,\n            \"formula\": lambda i, t: 20 + 0.5 * i - 0.3 * t + 0.5 * (-1)**t,\n        },\n        {\n            \"N\": 3, \"T\": 1,\n            \"formula\": lambda i, t: 14 + 6 * i,\n        },\n        {\n            \"N\": 2, \"T\": 8,\n            \"formula\": lambda i, t: 10**7 + 10**5 * i + 1000 * t + 100 * (-1)**t,\n        },\n        {\n            \"N\": 1, \"T\": 6,\n            \"formula\": lambda i, t: 18.0,\n        },\n        {\n            \"N\": 5, \"T\": 7,\n            \"formula\": lambda i, t: 10 + 0.2 * i + 0.01 * t**2 - 0.5 * t,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        N = case[\"N\"]\n        T = case[\"T\"]\n        formula = case[\"formula\"]\n\n        # 1. Construct the SST matrix S\n        S = np.zeros((N, T), dtype=np.float64)\n        for i in range(N):\n            for t in range(T):\n                S[i, t] = formula(i, t)\n\n        # 2. Compute the temporal mean mu_i for each row i.\n        #    The shape of mu will be (N, 1) for broadcasting.\n        mu = S.mean(axis=1, keepdims=True)\n\n        # 3. Compute the anomaly matrix X.\n        #    Broadcasting subtracts mu[i, 0] from every element in row i of S.\n        X = S - mu\n\n        # 4. Verify that the mean of each row of X is within the tolerance.\n        #    Compute the mean of each row of the anomaly matrix.\n        X_row_means = X.mean(axis=1)\n\n        # 5. Check if the absolute value of all row means are <= tau.\n        #    np.all returns True if all elements in the iterable are True.\n        all_rows_verified = np.all(np.abs(X_row_means) <= tau)\n        \n        results.append(all_rows_verified)\n\n    # Final print statement in the exact required format.\n    # The map(str, ...) converts boolean True/False to strings \"True\"/\"False\".\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "At its core, EOF analysis is an application of Singular Value Decomposition (SVD), a powerful tool from linear algebra. This exercise takes you into the heart of the method, connecting the abstract mathematical components of SVD to the concrete physical entities of EOF analysis: the spatial patterns (EOFs), the temporal time series (Principal Components or PCs), and their corresponding amplitudes. By performing the decomposition and explicitly verifying the key relationships, such as the link between singular values and the eigenvalues of the covariance matrix, you will build a robust, first-principles understanding of how EOF analysis works. ",
            "id": "3791994",
            "problem": "You are given a set of small, synthetic data matrices representing standardized anomaly fields in computational oceanography. Let $\\tilde{X} \\in \\mathbb{R}^{m \\times n}$ denote an anomaly matrix whose rows index spatial locations and columns index times. In Empirical Orthogonal Function (EOF) analysis, spatial patterns (EOFs) and temporal coefficients (Principal Components, PCs) are obtained by decomposing $\\tilde{X}$ into orthonormal spatial and temporal bases and a set of nonnegative amplitudes. From first principles, the relationship between the squared amplitudes and the eigenvalues of the spatial covariance operator $\\tilde{X}\\tilde{X}^{\\top}$ must be demonstrated. Your task is to build a program that, for each test matrix, computes a singular value decomposition, explicitly constructs Empirical Orthogonal Functions (EOFs) and Principal Components (PCs), and verifies three properties: (i) the squared amplitudes match the eigenvalues of $\\tilde{X}\\tilde{X}^{\\top}$, (ii) the decomposition reconstructs the data, and (iii) the spatial and temporal bases are orthonormal.\n\nDefinitions on first use:\n- Empirical Orthogonal Function (EOF): The spatial basis vectors corresponding to the optimal orthonormal decomposition of space-time anomalies.\n- Principal Component (PC): The temporal coefficients associated with the spatial EOFs.\n- Singular Value Decomposition (SVD): A numerical factorization that yields orthonormal bases and nonnegative amplitudes for any real matrix.\n\nFor each matrix $\\tilde{X}$ in the test suite below, do the following:\n1. Compute a numerical factorization that yields an orthonormal set of spatial vectors (EOFs), an orthonormal set of temporal vectors (PC directions), and nonnegative scalar amplitudes. Denote the spatial EOFs by the columns of $U$, the PCs by the rows of $V^{\\top}$, and the amplitudes by the entries of a nonnegative vector $S$.\n2. Verify that the squared amplitudes $S^2$ match the eigenvalues of $\\tilde{X}\\tilde{X}^{\\top}$ within a tolerance of $10^{-10}$, after sorting both sets in descending order.\n3. Verify that the decomposition reconstructs $\\tilde{X}$ within a relative error tolerance of $10^{-12}$ (use absolute error tolerance $10^{-12}$ if $\\|\\tilde{X}\\|_F=0$).\n4. Verify that the spatial basis and the temporal basis are orthonormal by checking $U^{\\top}U=I$ and $V^{\\top}V=I$ within a tolerance of $10^{-12}$.\n\nTest suite of matrices:\n- Case $1$ (happy path, $3 \\times 4$): $\\tilde{X}_1 = \\begin{bmatrix} 1.2 & -0.3 & 0.5 & -1.4 \\\\ 0.8 & -0.2 & -0.1 & -0.5 \\\\ -0.6 & 0.7 & -0.9 & 0.8 \\end{bmatrix}$.\n- Case $2$ (rank-deficient, $3 \\times 3$): $\\tilde{X}_2 = \\begin{bmatrix} 1 & -1 & 0 \\\\ 2 & -2 & 0 \\\\ -1 & 1 & 0 \\end{bmatrix}$.\n- Case $3$ (boundary, all zeros, $3 \\times 3$): $\\tilde{X}_3 = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$.\n- Case $4$ (repeated amplitudes, $2 \\times 2$): $\\tilde{X}_4 = \\begin{bmatrix} 2 & 0 \\\\ 0 & 2 \\end{bmatrix}$.\n- Case $5$ (rectangular wide, $2 \\times 5$): $\\tilde{X}_5 = \\begin{bmatrix} 3 & -1 & -2 & 0 & 0 \\\\ -2 & 2 & 1 & -1 & 0 \\end{bmatrix}$.\n\nFor each case, your program must compute:\n- A boolean indicating whether $S^2$ equals the eigenvalues of $\\tilde{X}\\tilde{X}^{\\top}$ to within $10^{-10}$, after sorting both sets in descending order.\n- A boolean indicating whether the reconstruction error criterion is satisfied to within $10^{-12}$ (relative, or absolute if $\\|\\tilde{X}\\|_F=0$).\n- A boolean indicating whether both orthonormality conditions $U^{\\top}U=I$ and $V^{\\top}V=I$ are satisfied to within $10^{-12}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results for the five cases as a comma-separated list of lists of booleans with no whitespace, for example: \"[[True,True,True],[True,True,True],[True,True,True],[True,True,True],[True,True,True]]\".",
            "solution": "The problem statement is valid. It presents a well-defined computational task grounded in the established mathematical principles of linear algebra, specifically Singular Value Decomposition (SVD) and its relationship to the eigendecomposition of covariance matrices. This relationship is the theoretical foundation of Empirical Orthogonal Function (EOF) analysis, a standard method in oceanography and climate science. The problem provides all necessary data, definitions, and precise, objective verification criteria, making it a complete and solvable problem.\n\nThe core of EOF analysis is the decomposition of a space-time data matrix $\\tilde{X} \\in \\mathbb{R}^{m \\times n}$, where $m$ represents spatial locations and $n$ represents time points, into a set of orthogonal spatial patterns and their corresponding temporal amplitudes. The Singular Value Decomposition (SVD) provides a natural and numerically robust framework for this task.\n\nThe SVD of the matrix $\\tilde{X}$ is given by:\n$$\n\\tilde{X} = U \\Sigma V^{\\top}\n$$\nwhere:\n- $U \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix whose columns are the left-singular vectors. In the context of EOF analysis, these are the spatial basis vectors, known as Empirical Orthogonal Functions (EOFs).\n- $V \\in \\mathbb{R}^{n \\times n}$ is an orthogonal matrix whose columns are the right-singular vectors. These represent an orthonormal basis for the temporal domain, often called PC directions.\n- $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a rectangular diagonal matrix containing the non-negative singular values, $s_i$, in descending order. These values are the amplitudes of the corresponding modes.\n\nThe Principal Components (PCs), which represent the time series of the spatial patterns, are given by the rows of the matrix product $\\Sigma V^{\\top}$.\n\nThe validation task consists of three parts, each confirming a fundamental property of the SVD in this context.\n\n**Part 1: Verification of Squared Amplitudes and Eigenvalues**\n\nThis verification rests on the connection between the SVD of $\\tilde{X}$ and the eigendecomposition of the spatial covariance matrix, $C_{space} = \\tilde{X}\\tilde{X}^{\\top}$. The spatial covariance matrix measures the covariance between different spatial points over time. Its eigenvectors represent the dominant modes of spatial variability, which are precisely the EOFs.\n\nLet's substitute the SVD of $\\tilde{X}$ into the definition of $C_{space}$:\n$$\nC_{space} = \\tilde{X}\\tilde{X}^{\\top} = (U \\Sigma V^{\\top})(U \\Sigma V^{\\top})^{\\top}\n$$\nUsing the property $(AB)^{\\top} = B^{\\top}A^{\\top}$, we get:\n$$\nC_{space} = (U \\Sigma V^{\\top})(V \\Sigma^{\\top} U^{\\top})\n$$\nSince $V$ is an orthogonal matrix, $V^{\\top}V = I$, where $I$ is the identity matrix. The expression simplifies to:\n$$\nC_{space} = U (\\Sigma \\Sigma^{\\top}) U^{\\top}\n$$\nThis equation is the eigenvalue decomposition of the symmetric matrix $C_{space}$. The columns of $U$ are the eigenvectors (the EOFs), and the diagonal matrix $\\Lambda = \\Sigma \\Sigma^{\\top}$ contains the eigenvalues. The diagonal entries of $\\Lambda$ are the squares of the singular values, $s_i^2$, from the SVD of $\\tilde{X}$. Specifically, if $\\tilde{X}$ is $m \\times n$, then $\\Sigma \\Sigma^{\\top}$ is an $m \\times m$ diagonal matrix whose first $k = \\min(m, n)$ diagonal entries are $s_1^2, s_2^2, \\ldots, s_k^2$, and the remaining $m-k$ entries are zero (if $m > n$).\nTherefore, the eigenvalues of $\\tilde{X}\\tilde{X}^{\\top}$ must be equal to the squared singular values of $\\tilde{X}$ (padded with zeros if necessary). The numerical verification involves computing both sets of numbers, sorting them in descending order, and checking for equality within a specified numerical tolerance, here $10^{-10}$.\n\n**Part 2: Verification of Data Reconstruction**\n\nThe equation $\\tilde{X} = U \\Sigma V^{\\top}$ itself states that the original data matrix can be perfectly reconstructed from its SVD components. In a computational environment with finite precision arithmetic, we expect this reconstruction to be accurate up to a small floating-point error. The verification step confirms this by computing the reconstructed matrix $\\tilde{X}_{rec} = U \\Sigma V^{\\top}$ and measuring the error relative to the original matrix $\\tilde{X}$. The Frobenius norm, $\\|\\cdot\\|_F$, is used for this purpose. The relative error is calculated as:\n$$\n\\text{Error}_{rel} = \\frac{\\|\\tilde{X} - \\tilde{X}_{rec}\\|_F}{\\|\\tilde{X}\\|_F}\n$$\nThis error is checked against a tolerance of $10^{-12}$. For the special case where $\\tilde{X}$ is the zero matrix, $\\|\\tilde{X}\\|_F=0$, and the check reverts to an absolute error $\\|\\tilde{X} - \\tilde{X}_{rec}\\|_F < 10^{-12}$.\n\n**Part 3: Verification of Orthonormality**\n\nA defining property of the SVD is that the matrices $U$ and $V$ are orthogonal. Orthogonality means that the dot product of any two distinct columns is zero, and the dot product of any column with itself is one. This can be expressed concisely in matrix form:\n$$\nU^{\\top}U = I_m \\quad \\text{and} \\quad V^{\\top}V = I_n\n$$\nwhere $I_m$ and $I_n$ are identity matrices of size $m$ and $n$, respectively. From a practical standpoint when using an \"economy\" SVD where $U$ is $m \\times k$ and $V$ is $n \\times k$ (with $k=\\min(m,n)$), the corresponding properties are $U^\\top U = I_k$ and $V^\\top V = I_k$. The verification step checks these identities for the numerically computed matrices, confirming that the spatial basis (EOFs in $U$) and the temporal basis (PC directions in $V$) are indeed orthonormal sets of vectors, within a numerical tolerance of $10^{-12}$. For the implementation, where the SVD routine returns $V^{\\top}$ (denoted `Vh`), the check for $V$ becomes a check on `Vh`, as $(Vh^\\top)^\\top(Vh^\\top) = Vh Vh^\\top = I_k$.\n\nThe program will implement these three verification steps for each provided test matrix, demonstrating the consistency between the theoretical properties of SVD and their numerical implementation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run EOF analysis verification on the test suite of matrices.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: happy path, 3x4\n        np.array([\n            [1.2, -0.3, 0.5, -1.4],\n            [0.8, -0.2, -0.1, -0.5],\n            [-0.6, 0.7, -0.9, 0.8]\n        ]),\n        # Case 2: rank-deficient, 3x3\n        np.array([\n            [1., -1., 0.],\n            [2., -2., 0.],\n            [-1., 1., 0.]\n        ]),\n        # Case 3: boundary, all zeros, 3x3\n        np.array([\n            [0., 0., 0.],\n            [0., 0., 0.],\n            [0., 0., 0.]\n        ]),\n        # Case 4: repeated amplitudes, 2x2\n        np.array([\n            [2., 0.],\n            [0., 2.]\n        ]),\n        # Case 5: rectangular wide, 2x5\n        np.array([\n            [3., -1., -2., 0., 0.],\n            [-2., 2., 1., -1., 0.]\n        ])\n    ]\n\n    results = []\n    for x_tilde in test_cases:\n        results.append(verify_eof_properties(x_tilde))\n\n    # Format the final output string to match the problem specification\n    # e.g., [[True,True,True],[True,True,True],...]\n    formatted_results = []\n    for res in results:\n        # Convert each boolean in the inner list to a string\n        inner_list_str = ','.join(map(str, res))\n        # Enclose in brackets\n        formatted_results.append(f\"[{inner_list_str}]\")\n    \n    # Join the formatted inner lists and enclose in outer brackets\n    final_output = f\"[{','.join(formatted_results)}]\"\n    \n    print(final_output)\n\ndef verify_eof_properties(x_tilde):\n    \"\"\"\n    Performs SVD and verifies the three specified properties for a given matrix.\n\n    Args:\n        x_tilde (np.ndarray): The input anomaly matrix.\n\n    Returns:\n        list: A list of three booleans corresponding to the three verified properties.\n    \"\"\"\n    m, n = x_tilde.shape\n    k = min(m, n)\n\n    # 1. Compute the Singular Value Decomposition.\n    # U: EOFs, s: singular values (amplitudes), Vh: transpose of PC directions\n    # Using full_matrices=False is the standard for economy SVD\n    # and sufficient for these checks.\n    try:\n        U, s, Vh = np.linalg.svd(x_tilde, full_matrices=False)\n    except np.linalg.LinAlgError:\n        # SVD might fail in extreme cases, though unlikely with numpy's robust implementation.\n        # If it fails, all checks are considered False.\n        return [False, False, False]\n    \n    # Property (i): Squared amplitudes match eigenvalues of X*X^T\n    # Calculate spatial covariance matrix and its eigenvalues\n    cov_spatial = x_tilde @ x_tilde.T\n    # eigvalsh is preferred for hermitian (or real symmetric) matrices\n    # and returns eigenvalues in ascending order.\n    eigvals = np.linalg.eigvalsh(cov_spatial)\n    # Sort eigenvalues in descending order to match singular value ordering\n    eigvals_sorted_desc = eigvals[::-1]\n    \n    s_squared = s**2\n    # The number of singular values is k=min(m,n).\n    # The number of eigenvalues is m. If m > n, there are m-n zero eigenvalues.\n    # We must compare the k squared singular values with the k largest eigenvalues.\n    # The remaining m-k eigenvalues should be zero. A robust way is to pad s^2 with zeros.\n    s_squared_padded = np.zeros(m)\n    s_squared_padded[:len(s_squared)] = s_squared\n    \n    is_eig_match = np.allclose(s_squared_padded, eigvals_sorted_desc, atol=1e-10, rtol=0)\n\n    # Property (ii): Decomposition reconstructs the data matrix\n    # Reconstruct the matrix using the SVD components\n    # s is a 1D array, so it needs to be formed into a diagonal matrix.\n    s_diag = np.diag(s)\n    x_reconstructed = U @ s_diag @ Vh\n    \n    norm_x = np.linalg.norm(x_tilde, 'fro')\n    norm_error = np.linalg.norm(x_tilde - x_reconstructed, 'fro')\n    \n    if norm_x == 0:\n        is_reconstructed = norm_error < 1e-12\n    else:\n        is_reconstructed = (norm_error / norm_x) < 1e-12\n\n    # Property (iii): Spatial and temporal bases are orthonormal\n    # Check U^T * U = I\n    identity_U = np.identity(U.shape[1])\n    is_U_orthonormal = np.allclose(U.T @ U, identity_U, atol=1e-12, rtol=0)\n    \n    # Check V^T * V = I. Since we have Vh = V^T, this is Vh * Vh^T = I\n    identity_V = np.identity(Vh.shape[0])\n    is_V_orthonormal = np.allclose(Vh @ Vh.T, identity_V, atol=1e-12, rtol=0)\n    \n    is_orthonormal = is_U_orthonormal and is_V_orthonormal\n    \n    return [is_eig_match, is_reconstructed, is_orthonormal]\n\nsolve()\n```"
        },
        {
            "introduction": "After decomposing a dataset into a spectrum of EOFs and their associated eigenvalues (variances), a critical question arises: which of these modes represent physically robust signals, and which might be artifacts of having a finite data record? This practice introduces a classic statistical test, North's rule of thumb, to address this issue of \"degeneracy.\" You will derive and apply this rule to estimate the sampling error of each eigenvalue, providing a quantitative basis for judging whether adjacent modes in the spectrum are statistically separable or should be considered part of the same physical process. ",
            "id": "3791949",
            "problem": "Consider an Empirical Orthogonal Function (EOF) decomposition of monthly Sea Surface Height (SSH) anomalies over a midlatitude ocean basin. Let the anomaly field at time index $t$ be a $p$-dimensional random vector $\\mathbf{x}_{t}$ with mean zero and true covariance matrix $\\boldsymbol{\\Sigma}$. Assume the anomalies are temporally independent and identically distributed, Gaussian, and stationary across $t=1,\\dots,T$. The sample covariance is defined as\n$$\n\\hat{\\mathbf{C}} \\equiv \\frac{1}{T}\\sum_{t=1}^{T} \\mathbf{x}_{t}\\mathbf{x}_{t}^{\\top}.\n$$\nDenote the ordered eigenvalues of $\\hat{\\mathbf{C}}$ by $\\hat{\\lambda}_{1} \\ge \\hat{\\lambda}_{2} \\ge \\dots \\ge \\hat{\\lambda}_{p} \\ge 0$. The EOF eigenvalue spectrum from a $T=300$ month record yields the leading six empirical eigenvalues (units of $\\text{cm}^{2}$):\n$$\n\\hat{\\lambda}_{1}=120,\\quad \\hat{\\lambda}_{2}=115,\\quad \\hat{\\lambda}_{3}=75,\\quad \\hat{\\lambda}_{4}=74,\\quad \\hat{\\lambda}_{5}=45,\\quad \\hat{\\lambda}_{6}=25.\n$$\nStarting from the definition of the sample covariance and a well-tested fact from multivariate statistics about the sampling distribution of $\\hat{\\mathbf{C}}$ for Gaussian data, derive the leading-order sampling standard deviation of a population eigenvalue $\\lambda_{i}$ for large $T$ in terms of $T$ and $\\lambda_{i}$. Using this leading-order result, formulate a physically motivated decision rule that declares two adjacent empirical eigenvalues $\\hat{\\lambda}_{i}$ and $\\hat{\\lambda}_{i+1}$ as “distinct” if their separation exceeds the sampling standard deviation of the larger mode. Then, apply your rule to the six given empirical eigenvalues to determine how many adjacent pairs $(\\hat{\\lambda}_{i},\\hat{\\lambda}_{i+1})$ with $i=1,\\dots,5$ are distinct.\n\nExpress your final answer as a single integer equal to the number of distinct adjacent pairs. No rounding is required for the integer result. If you compute intermediate numerical values, carry enough precision to make unambiguous comparisons of separations against sampling standard deviations.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in multivariate statistics and its application to geophysical data analysis (EOF analysis). It is well-posed, providing all necessary information and a clear objective. There are no contradictions, ambiguities, or factual errors.\n\nThe solution proceeds in three stages:\n1.  Derivation of the leading-order sampling standard deviation for an eigenvalue of the sample covariance matrix.\n2.  Formulation of the decision rule for distinguishing adjacent eigenvalues based on this standard deviation.\n3.  Application of the rule to the provided data.\n\n**1. Derivation of the Sampling Standard Deviation**\n\nThe problem states that the data vectors $\\mathbf{x}_{t}$ for $t=1, \\dots, T$ are independent and identically distributed samples from a $p$-variate Gaussian distribution with mean zero and true covariance matrix $\\boldsymbol{\\Sigma}$, i.e., $\\mathbf{x}_{t} \\sim \\mathcal{N}_{p}(\\mathbf{0}, \\boldsymbol{\\Sigma})$.\n\nThe sample covariance matrix is defined as:\n$$\n\\hat{\\mathbf{C}} = \\frac{1}{T} \\sum_{t=1}^{T} \\mathbf{x}_{t} \\mathbf{x}_{t}^{\\top}\n$$\nThe matrix $T\\hat{\\mathbf{C}} = \\sum_{t=1}^{T} \\mathbf{x}_{t} \\mathbf{x}_{t}^{\\top}$ is a sum of $T$ outer products of independent multivariate normal vectors. By definition, this matrix follows a Wishart distribution with $T$ degrees of freedom and scale matrix $\\boldsymbol{\\Sigma}$, denoted as $T\\hat{\\mathbf{C}} \\sim W_{p}(T, \\boldsymbol{\\Sigma})$. This is the \"well-tested fact from multivariate statistics\" mentioned in the problem.\n\nLet the eigenvalues of the population covariance matrix $\\boldsymbol{\\Sigma}$ be $\\lambda_1 > \\lambda_2 > \\dots > \\lambda_p > 0$. We assume for this derivation that these are distinct. Let the corresponding sample eigenvalues of $\\hat{\\mathbf{C}}$ be $\\hat{\\lambda}_1 \\ge \\hat{\\lambda}_2 \\ge \\dots \\ge \\hat{\\lambda}_p \\ge 0$.\n\nA fundamental result from asymptotic theory for principal component analysis (e.g., T.W. Anderson, 1963) states that for large $T$, the sample eigenvalues $\\hat{\\lambda}_i$ are asymptotically normal and independent. Specifically, the quantity $\\sqrt{T}(\\hat{\\lambda}_i - \\lambda_i)$ converges in distribution to a normal distribution with mean $0$ and variance $2\\lambda_i^2$:\n$$\n\\sqrt{T}(\\hat{\\lambda}_i - \\lambda_i) \\xrightarrow{d} \\mathcal{N}(0, 2\\lambda_i^2)\n$$\nFrom this asymptotic distribution, we can find the variance of the sample eigenvalue $\\hat{\\lambda}_i$, which is an estimator for the population eigenvalue $\\lambda_i$.\n$$\n\\text{Var}\\left[\\sqrt{T}(\\hat{\\lambda}_i - \\lambda_i)\\right] \\approx 2\\lambda_i^2\n$$\nSince $\\lambda_i$ is a constant, $\\text{Var}[\\hat{\\lambda}_i - \\lambda_i] = \\text{Var}[\\hat{\\lambda}_i]$. Also, $\\text{Var}[c_1 Y] = c_1^2 \\text{Var}[Y]$ for a constant $c_1$ and random variable $Y$. Thus, with $c_1 = \\sqrt{T}$:\n$$\nT \\cdot \\text{Var}[\\hat{\\lambda}_i] \\approx 2\\lambda_i^2\n$$\nSolving for the variance of $\\hat{\\lambda}_i$ gives the leading-order result:\n$$\n\\text{Var}[\\hat{\\lambda}_i] \\approx \\frac{2\\lambda_i^2}{T}\n$$\nThe sampling standard deviation of $\\hat{\\lambda}_i$, denoted $\\sigma(\\hat{\\lambda}_i)$, is the square root of its variance. This is what the problem refers to as the \"sampling standard deviation of a population eigenvalue $\\lambda_i$\".\n$$\n\\sigma(\\hat{\\lambda}_i) \\equiv \\sqrt{\\text{Var}[\\hat{\\lambda}_i]} \\approx \\sqrt{\\frac{2\\lambda_i^2}{T}} = \\lambda_i \\sqrt{\\frac{2}{T}}\n$$\n\n**2. Formulation of the Decision Rule**\n\nThe derived sampling standard deviation depends on the true population eigenvalue $\\lambda_i$, which is unknown. To create a practical rule, we must estimate this standard deviation using the available sample data. The natural estimator for $\\lambda_i$ is the corresponding sample eigenvalue $\\hat{\\lambda}_i$. Substituting $\\hat{\\lambda}_i$ for $\\lambda_i$ in the expression for the standard deviation gives the estimated sampling error:\n$$\n\\hat{\\sigma}(\\hat{\\lambda}_i) \\approx \\hat{\\lambda}_i \\sqrt{\\frac{2}{T}}\n$$\nThe problem specifies a decision rule: two adjacent empirical eigenvalues, $\\hat{\\lambda}_i$ and $\\hat{\\lambda}_{i+1}$, are declared \"distinct\" if their separation is greater than the sampling standard deviation of the larger eigenvalue. Since the eigenvalues are ordered, $\\hat{\\lambda}_i \\ge \\hat{\\lambda}_{i+1}$, the larger eigenvalue is $\\hat{\\lambda}_i$.\n\nThe separation is $\\hat{\\lambda}_i - \\hat{\\lambda}_{i+1}$. The standard deviation of the larger mode is $\\hat{\\sigma}(\\hat{\\lambda}_i)$. Thus, the rule is:\nDeclare the pair $(\\hat{\\lambda}_i, \\hat{\\lambda}_{i+1})$ distinct if $\\hat{\\lambda}_i - \\hat{\\lambda}_{i+1} > \\hat{\\sigma}(\\hat{\\lambda}_i)$.\nSubstituting our derived expression, the condition for distinctness is:\n$$\n\\hat{\\lambda}_i - \\hat{\\lambda}_{i+1} > \\hat{\\lambda}_i \\sqrt{\\frac{2}{T}}\n$$\nThis rule is commonly known as North's rule of thumb.\n\n**3. Application to the Data**\n\nWe are given $T=300$ and the following six leading eigenvalues (in units of $\\text{cm}^2$):\n$\\hat{\\lambda}_{1}=120$, $\\hat{\\lambda}_{2}=115$, $\\hat{\\lambda}_{3}=75$, $\\hat{\\lambda}_{4}=74$, $\\hat{\\lambda}_{5}=45$, $\\hat{\\lambda}_{6}=25$.\n\nWe first calculate the constant factor in the rule:\n$$\n\\sqrt{\\frac{2}{T}} = \\sqrt{\\frac{2}{300}} = \\sqrt{\\frac{1}{150}} \\approx 0.08165\n$$\nWe now apply the decision rule to the five adjacent pairs for $i=1, \\dots, 5$.\n\n*   **Pair 1: $(\\hat{\\lambda}_1, \\hat{\\lambda}_2) = (120, 115)$**\n    *   Separation: $\\hat{\\lambda}_1 - \\hat{\\lambda}_2 = 120 - 115 = 5$.\n    *   Standard deviation estimate: $\\hat{\\sigma}(\\hat{\\lambda}_1) = 120 \\sqrt{\\frac{1}{150}} \\approx 120 \\times 0.08165 = 9.798$.\n    *   Comparison: $5 \\ngtr 9.798$. The pair is **not distinct**.\n\n*   **Pair 2: $(\\hat{\\lambda}_2, \\hat{\\lambda}_3) = (115, 75)$**\n    *   Separation: $\\hat{\\lambda}_2 - \\hat{\\lambda}_3 = 115 - 75 = 40$.\n    *   Standard deviation estimate: $\\hat{\\sigma}(\\hat{\\lambda}_2) = 115 \\sqrt{\\frac{1}{150}} \\approx 115 \\times 0.08165 = 9.390$.\n    *   Comparison: $40 > 9.390$. The pair is **distinct**.\n\n*   **Pair 3: $(\\hat{\\lambda}_3, \\hat{\\lambda}_4) = (75, 74)$**\n    *   Separation: $\\hat{\\lambda}_3 - \\hat{\\lambda}_4 = 75 - 74 = 1$.\n    *   Standard deviation estimate: $\\hat{\\sigma}(\\hat{\\lambda}_3) = 75 \\sqrt{\\frac{1}{150}} \\approx 75 \\times 0.08165 = 6.124$.\n    *   Comparison: $1 \\ngtr 6.124$. The pair is **not distinct**.\n\n*   **Pair 4: $(\\hat{\\lambda}_4, \\hat{\\lambda}_5) = (74, 45)$**\n    *   Separation: $\\hat{\\lambda}_4 - \\hat{\\lambda}_5 = 74 - 45 = 29$.\n    *   Standard deviation estimate: $\\hat{\\sigma}(\\hat{\\lambda}_4) = 74 \\sqrt{\\frac{1}{150}} \\approx 74 \\times 0.08165 = 6.042$.\n    *   Comparison: $29 > 6.042$. The pair is **distinct**.\n\n*   **Pair 5: $(\\hat{\\lambda}_5, \\hat{\\lambda}_6) = (45, 25)$**\n    *   Separation: $\\hat{\\lambda}_5 - \\hat{\\lambda}_6 = 45 - 25 = 20$.\n    *   Standard deviation estimate: $\\hat{\\sigma}(\\hat{\\lambda}_5) = 45 \\sqrt{\\frac{1}{150}} \\approx 45 \\times 0.08165 = 3.674$.\n    *   Comparison: $20 > 3.674$. The pair is **distinct**.\n\nCounting the number of pairs that are declared distinct:\n1.  Not distinct\n2.  Distinct\n3.  Not distinct\n4.  Distinct\n5.  Distinct\n\nThe total number of distinct adjacent pairs is $3$.",
            "answer": "$$\\boxed{3}$$"
        }
    ]
}