{
    "hands_on_practices": [
        {
            "introduction": "To fully appreciate the ingenuity of quasi-Newton methods, we must first understand the limitations of their predecessor, the pure Newton's method. While Newton's method offers rapid quadratic convergence near a solution, its performance can be unreliable on the non-convex energy landscapes often encountered in computational catalysis. This exercise provides a concrete example of how Newton's method can fail when the Hessian matrix is indefinite, generating a search direction that actually increases the objective function. By working through this problem , you will see firsthand why safeguards and Hessian modifications, like those inherent in BFGS methods, are essential for robust optimization.",
            "id": "3897692",
            "problem": "In kinetic parameter estimation for heterogeneous catalysis, one often balances mechanistic constraints with data fit. Suppose an engineer enforces the Brønsted–Evans–Polanyi relation (activation energy versus adsorption energy) as a soft constraint while also incorporating a data-driven regularizer that (erroneously) rewards strong adsorption. The resulting surrogate objective in the two parameters, adsorption energy $x$ and activation energy $y$ (both in electron volts), is\n$$\nf(x,y) \\;=\\; \\big(y - a - b\\,x\\big)^2 \\;-\\; \\gamma\\,x^2,\n$$\nwhere $a>0$ and $b>0$ encode the mechanistic line and $\\gamma>0$ is the (misspecified) regularization weight. Assume $a=0.5$, $b=0.8$, $\\gamma=1.0$, and consider the starting point $x_0=(x,y)=(1.0,\\,1.0)$.\n\nBy the definition of Newton’s method for unconstrained minimization, the Newton direction $p$ at a point satisfies $H(x)p=-\\nabla f(x)$, where $\\nabla f$ and $H$ are the gradient and Hessian of $f$. A direction $p$ is a descent direction if and only if $\\nabla f(x)^{\\top} p  0$. The sufficient decrease (Armijo) rule accepts a step length $\\alpha>0$ only if $f(x+\\alpha p)\\le f(x)+c_1 \\alpha \\nabla f(x)^{\\top} p$ for some $c_1\\in(0,1)$.\n\nWhich of the following statements are correct for this example?\n\nA. At $x_0$, the Hessian is indefinite, the Newton step is $p=(-1.0,\\,-0.5)^{\\top}$, and $\\nabla f(x_0)^{\\top}p>0$, so the unit Newton step causes $f$ to increase from $-0.91$ to $0$.\n\nB. Even with an indefinite Hessian, a nonzero gradient guarantees that the Newton direction is a descent direction, so pure Newton will not diverge.\n\nC. A backtracking line search enforcing the Armijo condition cannot find any $\\alpha\\in(0,1]$ that decreases $f$ along the pure Newton direction at $x_0$, so the method will either stall or require safeguarding.\n\nD. Replacing the exact Hessian with a positive definite approximation updated by the Broyden–Fletcher–Goldfarb–Shanno (BFGS) method, starting from a positive definite initial matrix and using a line search that enforces the curvature condition, yields a descent direction at $x_0$.\n\nE. Adding a Levenberg–Marquardt diagonal shift, i.e., replacing $H(x_0)$ by $H(x_0)+\\lambda I$ with $\\lambda>-\\lambda_{\\min}(H(x_0))$, makes the system positive definite and forces the Newton direction to be a descent direction; for the given numbers, any $\\lambda>1.5$ suffices.\n\nSelect all that apply.",
            "solution": "The problem provides a surrogate objective function for a parameter estimation problem in catalysis, given by $f(x,y) = (y - a - bx)^2 - \\gamma x^2$. The parameters are specified as $a=0.5$, $b=0.8$, and $\\gamma=1.0$. The starting point for an optimization routine is given as $x_0 = (x,y) = (1.0, 1.0)$. We are asked to evaluate several statements about the behavior of Newton's method and its variants at this point.\n\nFirst, we must compute the function value, gradient, and Hessian at the point $x_0$.\nThe objective function with the given parameters is:\n$$\nf(x,y) = (y - 0.5 - 0.8x)^2 - x^2\n$$\n\nThe gradient vector, $\\nabla f(x,y)$, has components:\n$$\n\\frac{\\partial f}{\\partial x} = 2(y - 0.5 - 0.8x)(-0.8) - 2x = -1.6(y - 0.5 - 0.8x) - 2x\n$$\n$$\n\\frac{\\partial f}{\\partial y} = 2(y - 0.5 - 0.8x)(1) = 2(y - 0.5 - 0.8x)\n$$\n\nThe Hessian matrix, $H(x,y)$, which is constant for this quadratic function, has components:\n$$\n\\frac{\\partial^2 f}{\\partial x^2} = \\frac{\\partial}{\\partial x} \\left(-1.6y + 0.8 + 1.28x - 2x\\right) = 1.28 - 2.0 = -0.72\n$$\n$$\n\\frac{\\partial^2 f}{\\partial y^2} = \\frac{\\partial}{\\partial y} \\left(2y - 1.0 - 1.6x\\right) = 2.0\n$$\n$$\n\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{\\partial^2 f}{\\partial y \\partial x} = \\frac{\\partial}{\\partial y} \\left(-1.6y + 0.8 + 1.28x - 2x\\right) = -1.6\n$$\nSo, the Hessian is:\n$$\nH = \\begin{pmatrix} -0.72  -1.6 \\\\ -1.6  2.0 \\end{pmatrix}\n$$\n\nNow, we evaluate these quantities at the specific point $x_0 = (1.0, 1.0)$:\nFunction value:\n$$\nf(1.0, 1.0) = (1.0 - 0.5 - 0.8(1.0))^2 - (1.0)^2 = (-0.3)^2 - 1.0 = 0.09 - 1.0 = -0.91\n$$\nGradient at $x_0$:\n$$\n\\nabla f(x_0) = \\begin{pmatrix} -1.6(-0.3) - 2(1.0) \\\\ 2(-0.3) \\end{pmatrix} = \\begin{pmatrix} 0.48 - 2.0 \\\\ -0.6 \\end{pmatrix} = \\begin{pmatrix} -1.52 \\\\ -0.6 \\end{pmatrix}\n$$\n\nWith these quantities, we can now analyze each option.\n\n### Option-by-Option Analysis\n\n**A. At $x_0$, the Hessian is indefinite, the Newton step is $p=(-1.0,\\,-0.5)^{\\top}$, and $\\nabla f(x_0)^{\\top}p>0$, so the unit Newton step causes $f$ to increase from $-0.91$ to $0$.**\n\n1.  **Hessian indefiniteness**: We check the determinant of the Hessian:\n    $$\n    \\det(H) = (-0.72)(2.0) - (-1.6)^2 = -1.44 - 2.56 = -4.0\n    $$\n    Since $\\det(H)  0$, the eigenvalues have opposite signs, which means the Hessian is indefinite. This part is correct.\n\n2.  **Newton step $p$**: The Newton direction $p$ is found by solving $H p = -\\nabla f(x_0)$.\n    $$\n    \\begin{pmatrix} -0.72  -1.6 \\\\ -1.6  2.0 \\end{pmatrix} \\begin{pmatrix} p_x \\\\ p_y \\end{pmatrix} = -\\begin{pmatrix} -1.52 \\\\ -0.6 \\end{pmatrix} = \\begin{pmatrix} 1.52 \\\\ 0.6 \\end{pmatrix}\n    $$\n    From the second equation, $-1.6 p_x + 2.0 p_y = 0.6$, which simplifies to $p_y = 0.3 + 0.8 p_x$. Substituting into the first equation:\n    $$\n    -0.72 p_x - 1.6(0.3 + 0.8 p_x) = 1.52 \\implies -0.72 p_x - 0.48 - 1.28 p_x = 1.52\n    $$\n    $$\n    -2.0 p_x = 2.0 \\implies p_x = -1.0\n    $$\n    Then, $p_y = 0.3 + 0.8(-1.0) = -0.5$. The Newton step is $p = (-1.0, -0.5)^{\\top}$. This part is correct.\n\n3.  **Directional derivative**: We check if $p$ is a descent direction by computing $\\nabla f(x_0)^{\\top}p$.\n    $$\n    \\nabla f(x_0)^{\\top}p = \\begin{pmatrix} -1.52  -0.6 \\end{pmatrix} \\begin{pmatrix} -1.0 \\\\ -0.5 \\end{pmatrix} = (-1.52)(-1.0) + (-0.6)(-0.5) = 1.52 + 0.30 = 1.82\n    $$\n    Since $\\nabla f(x_0)^{\\top}p = 1.82 > 0$, the Newton direction is an ascent direction, not a descent direction. This part is correct.\n\n4.  **Function value after step**: A unit Newton step ($\\alpha=1$) leads to the point $x_1 = x_0 + p = (1.0, 1.0) + (-1.0, -0.5) = (0.0, 0.5)$. The function value at this new point is:\n    $$\n    f(0.0, 0.5) = (0.5 - 0.5 - 0.8(0.0))^2 - (0.0)^2 = 0^2 - 0 = 0\n    $$\n    The function value increases from $f(x_0) = -0.91$ to $f(x_1) = 0$. This part is correct. In fact, for a quadratic function, one step of Newton's method goes to the unique stationary point, which is a saddle point in this indefinite case.\n\nAll claims in this statement are correct.\n**Verdict: Correct**\n\n**B. Even with an indefinite Hessian, a nonzero gradient guarantees that the Newton direction is a descent direction, so pure Newton will not diverge.**\n\nThis is a general statement about the properties of Newton's method. A direction $p$ is a descent direction if $\\nabla f(x)^{\\top}p  0$. The Newton direction is $p = -H(x)^{-1}\\nabla f(x)$. The directional derivative is $\\nabla f(x)^{\\top}p = -\\nabla f(x)^{\\top}H(x)^{-1}\\nabla f(x)$. This quantity is guaranteed to be negative only if $H(x)^{-1}$ (and thus $H(x)$) is positive definite. If $H(x)$ is indefinite, there is no such guarantee.\nThe specific problem provides a counterexample: at $x_0$, the Hessian is indefinite, the gradient is non-zero, but we found that the Newton direction is an ascent direction ($\\nabla f(x_0)^{\\top}p > 0$). Therefore, the premise of the statement is false.\n**Verdict: Incorrect**\n\n**C. A backtracking line search enforcing the Armijo condition cannot find any $\\alpha\\in(0,1]$ that decreases $f$ along the pure Newton direction at $x_0$, so the method will either stall or require safeguarding.**\n\nLet us analyze the function value along the Newton direction $p = (-1.0, -0.5)^{\\top}$ starting from $x_0=(1.0,1.0)$.\nLet $\\phi(\\alpha) = f(x_0 + \\alpha p) = f(1.0-\\alpha, 1.0-0.5\\alpha)$ for $\\alpha \\ge 0$.\n$$\n\\phi(\\alpha) = ((1.0-0.5\\alpha) - 0.5 - 0.8(1.0-\\alpha))^2 - (1.0-\\alpha)^2\n$$\n$$\n\\phi(\\alpha) = (1.0-0.5\\alpha-0.5-0.8+0.8\\alpha)^2 - (1.0-\\alpha)^2 = (-0.3+0.3\\alpha)^2 - (1.0-\\alpha)^2\n$$\n$$\n\\phi(\\alpha) = 0.09(\\alpha-1.0)^2 - (1.0-\\alpha)^2 = 0.09(1.0-\\alpha)^2 - (1.0-\\alpha)^2 = -0.91(1.0-\\alpha)^2\n$$\nThe initial function value is $f(x_0) = \\phi(0) = -0.91$. We want to check if we can find an $\\alpha \\in (0,1]$ such that $\\phi(\\alpha)  \\phi(0)$, i.e., $-0.91(1.0-\\alpha)^2  -0.91$. This simplifies to $(1.0-\\alpha)^2 > 1.0$. For $\\alpha \\in (0,1]$, we have $1.0-\\alpha \\in [0, 1)$, so $(1.0-\\alpha)^2 \\in [0, 1)$. The inequality $(1.0-\\alpha)^2 > 1.0$ is never satisfied. In fact, $f$ strictly increases for any step in $(0,1)$.\nThe statement that a backtracking line search cannot find an $\\alpha \\in (0,1]$ that *decreases* $f$ is therefore true. Since the Newton direction is an ascent direction, a robust optimization algorithm would not proceed with a line search. It must first apply a \"safeguard\", such as modifying the Hessian to be positive definite or switching to a different direction (like steepest descent). If a naive line search that requires function decrease is used, it will fail to find an acceptable step and will \"stall\". Thus, the entire statement is correct.\n**Verdict: Correct**\n\n**D. Replacing the exact Hessian with a positive definite approximation updated by the Broyden–Fletcher–Goldfarb–Shanno (BFGS) method, starting from a positive definite initial matrix and using a line search that enforces the curvature condition, yields a descent direction at $x_0$.**\n\nThis describes the first step of a quasi-Newton method (BFGS). The search direction at step $k=0$ (our point $x_0$) is given by $p_0 = -B_0^{-1} \\nabla f(x_0)$, where $B_0$ is the initial approximation to the Hessian. The BFGS method requires starting with a positive definite matrix $B_0$ (e.g., $B_0=I$). The directional derivative at $x_0$ along $p_0$ is:\n$$\n\\nabla f(x_0)^{\\top} p_0 = \\nabla f(x_0)^{\\top} (-B_0^{-1} \\nabla f(x_0)) = - \\nabla f(x_0)^{\\top} B_0^{-1} \\nabla f(x_0)\n$$\nSince $B_0$ is positive definite, its inverse $B_0^{-1}$ is also positive definite. For any non-zero vector $v$, the quadratic form $v^{\\top} B_0^{-1} v$ is strictly positive. Our gradient $\\nabla f(x_0)$ is a non-zero vector. Therefore, $\\nabla f(x_0)^{\\top} B_0^{-1} \\nabla f(x_0) > 0$. This implies that $\\nabla f(x_0)^{\\top} p_0  0$. By definition, $p_0$ is a descent direction. The rest of the information (BFGS update, curvature condition) ensures this property is maintained in subsequent steps, but the claim about the direction at $x_0$ is correct based solely on starting with a positive definite $B_0$.\n**Verdict: Correct**\n\n**E. Adding a Levenberg–Marquardt diagonal shift, i.e., replacing $H(x_0)$ by $H(x_0)+\\lambda I$ with $\\lambda>-\\lambda_{\\min}(H(x_0))$, makes the system positive definite and forces the Newton direction to be a descent direction; for the given numbers, any $\\lambda>1.5$ suffices.**\n\n1.  **General Principle**: This describes a modified Newton method. The modified Hessian is $H_{mod} = H + \\lambda I$. The eigenvalues of $H_{mod}$ are $\\mu_i = \\lambda_i + \\lambda$, where $\\lambda_i$ are the eigenvalues of $H$. For $H_{mod}$ to be positive definite, we require all $\\mu_i > 0$, which means $\\lambda_i + \\lambda > 0$ for all $i$. This is equivalent to $\\lambda > -\\lambda_i$ for all $i$, or $\\lambda > -\\min_i(\\lambda_i) = -\\lambda_{\\min}(H)$. If this condition holds, $H_{mod}$ is positive definite. The modified Newton direction $p_{mod} = -H_{mod}^{-1} \\nabla f(x_0)$ is then guaranteed to be a descent direction by the same reasoning as in option D. The general principle stated is correct.\n\n2.  **Specific Calculation**: We need to find $\\lambda_{\\min}(H(x_0))$. The eigenvalues are the roots of the characteristic equation $\\det(H - \\lambda I) = 0$:\n    $$\n    (-0.72 - \\lambda)(2.0 - \\lambda) - (-1.6)^2 = 0\n    $$\n    $$\n    \\lambda^2 - 1.28\\lambda - 1.44 - 2.56 = 0 \\implies \\lambda^2 - 1.28\\lambda - 4.0 = 0\n    $$\n    Using the quadratic formula:\n    $$\n    \\lambda = \\frac{1.28 \\pm \\sqrt{(-1.28)^2 - 4(1)(-4.0)}}{2} = \\frac{1.28 \\pm \\sqrt{1.6384 + 16.0}}{2} = \\frac{1.28 \\pm \\sqrt{17.6384}}{2}\n    $$\n    The minimum eigenvalue is:\n    $$\n    \\lambda_{\\min} = \\frac{1.28 - \\sqrt{17.6384}}{2} \\approx \\frac{1.28 - 4.1998}{2} = \\frac{-2.9198}{2} = -1.4599\n    $$\n    The condition for positive definiteness is $\\lambda > -\\lambda_{\\min} = -(-1.4599) = 1.4599$.\n    The statement claims that any $\\lambda > 1.5$ suffices. Since $1.5 > 1.4599$, this claim is correct.\n\nBoth parts of the statement are verified to be correct.\n**Verdict: Correct**",
            "answer": "$$\\boxed{ACDE}$$"
        },
        {
            "introduction": "At the heart of the most popular quasi-Newton methods is the Broyden–Fletcher–Goldfarb–Shanno (BFGS) update. This elegant formula provides a way to iteratively build an approximation of the Hessian matrix, $B_k$, using only gradient information. The practice problem  offers a direct, hands-on calculation of a single BFGS update step. This exercise will reinforce your understanding of the core mechanics, requiring you to apply the update formula and verify that the resulting matrix $B_{k+1}$ satisfies the fundamental secant condition, $B_{k+1}s_k = y_k$, which anchors the approximation to the most recent gradient behavior.",
            "id": "3897712",
            "problem": "In a two-parameter microkinetic catalytic model, the parameters $x \\in \\mathbb{R}^{2}$ represent a scaled adsorption enthalpy and a scaled activation barrier, both rendered dimensionless by division through appropriate reference magnitudes so that the objective function is dimensionless. The model calibration proceeds by minimizing a twice continuously differentiable least-squares misfit $f(x)$ between predicted and measured turnover frequencies, and the curvature of $f$ is approximated iteratively by a symmetric positive definite matrix $B_{k}$.\n\nSuppose at iteration $k$, the quasi-Newton step $s_{k} = x_{k+1} - x_{k}$ and the corresponding gradient change $y_{k} = \\nabla f(x_{k+1}) - \\nabla f(x_{k})$ are obtained from the calibrated microkinetic simulator with values\n$$\nB_{k} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}, \\quad s_{k} = \\begin{pmatrix} 0.2 \\\\ -0.1 \\end{pmatrix}, \\quad y_{k} = \\begin{pmatrix} 0.3 \\\\ -0.05 \\end{pmatrix}.\n$$\nUsing the Broyden–Fletcher–Goldfarb–Shanno (BFGS) update, which enforces symmetry and the secant condition for the gradient mapping arising from the first-order Taylor expansion of $\\nabla f$, compute the next curvature approximation $B_{k+1}$ numerically and verify the secant condition $B_{k+1} s_{k} = y_{k}$.\n\nReport, as your final answer, the determinant $\\det(B_{k+1})$ as a single dimensionless real number. No rounding is necessary; if you present a rational value, express it in simplest exact form.",
            "solution": "The BFGS update formula for the Hessian approximation, $B_{k+1}$, is given by:\n$$\nB_{k+1} = B_k - \\frac{B_k s_k s_k^T B_k^T}{s_k^T B_k s_k} + \\frac{y_k y_k^T}{y_k^T s_k}\n$$\nGiven that $B_k$ is symmetric, $B_k^T = B_k$, so the formula simplifies to:\n$$\nB_{k+1} = B_k - \\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k} + \\frac{y_k y_k^T}{y_k^T s_k}\n$$\nThe provided values are:\n$$\nB_{k} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}, \\quad s_{k} = \\begin{pmatrix} 0.2 \\\\ -0.1 \\end{pmatrix}, \\quad y_{k} = \\begin{pmatrix} 0.3 \\\\ -0.05 \\end{pmatrix}\n$$\nWe will work with exact fractional representations to maintain precision: $s_k = \\begin{pmatrix} 1/5 \\\\ -1/10 \\end{pmatrix}$ and $y_k = \\begin{pmatrix} 3/10 \\\\ -1/20 \\end{pmatrix}$.\n\nFirst, we verify the curvature condition, $s_k^T y_k > 0$, which is necessary for the update to be well-defined and to preserve the positive definiteness of the Hessian approximation.\n$$\ny_k^T s_k = s_k^T y_k = \\begin{pmatrix} 0.2  -0.1 \\end{pmatrix} \\begin{pmatrix} 0.3 \\\\ -0.05 \\end{pmatrix} = (0.2)(0.3) + (-0.1)(-0.05) = 0.06 + 0.005 = 0.065\n$$\nIn fractional form:\n$$\ns_k^T y_k = \\left(\\frac{1}{5}\\right)\\left(\\frac{3}{10}\\right) + \\left(-\\frac{1}{10}\\right)\\left(-\\frac{1}{20}\\right) = \\frac{3}{50} + \\frac{1}{200} = \\frac{12}{200} + \\frac{1}{200} = \\frac{13}{200}\n$$\nSince $s_k^T y_k = 13/200 > 0$, the curvature condition is satisfied.\n\nNext, we compute the components of the BFGS formula.\nThe denominator of the first correction term is $s_k^T B_k s_k$. Since $B_k = I$ (the identity matrix), this simplifies to $s_k^T s_k$:\n$$\ns_k^T B_k s_k = s_k^T s_k = (0.2)^2 + (-0.1)^2 = 0.04 + 0.01 = 0.05 = \\frac{1}{20}\n$$\nThe first correction term is $-\\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k}$. Since $B_k = I$, this is $-\\frac{s_k s_k^T}{s_k^T s_k}$.\nThe outer product $s_k s_k^T$ is:\n$$\ns_k s_k^T = \\begin{pmatrix} 0.2 \\\\ -0.1 \\end{pmatrix} \\begin{pmatrix} 0.2  -0.1 \\end{pmatrix} = \\begin{pmatrix} 0.04  -0.02 \\\\ -0.02  0.01 \\end{pmatrix} = \\begin{pmatrix} 1/25  -1/50 \\\\ -1/50  1/100 \\end{pmatrix}\n$$\nThe first correction term is therefore:\n$$\n-\\frac{1}{0.05} \\begin{pmatrix} 0.04  -0.02 \\\\ -0.02  0.01 \\end{pmatrix} = -20 \\begin{pmatrix} 0.04  -0.02 \\\\ -0.02  0.01 \\end{pmatrix} = \\begin{pmatrix} -0.8  0.4 \\\\ 0.4  -0.2 \\end{pmatrix} = \\begin{pmatrix} -4/5  2/5 \\\\ 2/5  -1/5 \\end{pmatrix}\n$$\nThe second correction term is $\\frac{y_k y_k^T}{y_k^T s_k}$. The denominator is $y_k^T s_k = 0.065 = 13/200$.\nThe outer product $y_k y_k^T$ is:\n$$\ny_k y_k^T = \\begin{pmatrix} 0.3 \\\\ -0.05 \\end{pmatrix} \\begin{pmatrix} 0.3  -0.05 \\end{pmatrix} = \\begin{pmatrix} 0.09  -0.015 \\\\ -0.015  0.0025 \\end{pmatrix} = \\begin{pmatrix} 9/100  -3/200 \\\\ -3/200  1/400 \\end{pmatrix}\n$$\nThe second correction term is therefore:\n$$\n\\frac{1}{13/200} \\begin{pmatrix} 9/100  -3/200 \\\\ -3/200  1/400 \\end{pmatrix} = \\frac{200}{13} \\begin{pmatrix} 9/100  -3/200 \\\\ -3/200  1/400 \\end{pmatrix} = \\begin{pmatrix} 18/13  -3/13 \\\\ -3/13  1/26 \\end{pmatrix}\n$$\nNow, we assemble $B_{k+1}$:\n$$\nB_{k+1} = B_k - \\frac{s_k s_k^T}{s_k^T s_k} + \\frac{y_k y_k^T}{y_k^T s_k} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} + \\begin{pmatrix} -4/5  2/5 \\\\ 2/5  -1/5 \\end{pmatrix} + \\begin{pmatrix} 18/13  -3/13 \\\\ -3/13  1/26 \\end{pmatrix}\n$$\nCombining the terms:\n$$\nB_{k+1} = \\begin{pmatrix} 1 - 4/5 + 18/13  0 + 2/5 - 3/13 \\\\ 0 + 2/5 - 3/13  1 - 1/5 + 1/26 \\end{pmatrix} = \\begin{pmatrix} 1/5 + 18/13  2/5 - 3/13 \\\\ 2/5 - 3/13  4/5 + 1/26 \\end{pmatrix}\n$$\nWe find a common denominator for each entry:\nEntry (1,1): $\\frac{1}{5} + \\frac{18}{13} = \\frac{13 + 90}{65} = \\frac{103}{65}$\nEntry (1,2) and (2,1): $\\frac{2}{5} - \\frac{3}{13} = \\frac{26 - 15}{65} = \\frac{11}{65}$\nEntry (2,2): $\\frac{4}{5} + \\frac{1}{26} = \\frac{104 + 5}{130} = \\frac{109}{130}$\nSo, the updated Hessian approximation is:\n$$\nB_{k+1} = \\begin{pmatrix} 103/65  11/65 \\\\ 11/65  109/130 \\end{pmatrix} = \\frac{1}{130} \\begin{pmatrix} 206  22 \\\\ 22  109 \\end{pmatrix}\n$$\nNext, we verify the secant condition, $B_{k+1} s_k = y_k$:\n$$\nB_{k+1} s_k = \\frac{1}{130} \\begin{pmatrix} 206  22 \\\\ 22  109 \\end{pmatrix} \\begin{pmatrix} 1/5 \\\\ -1/10 \\end{pmatrix} = \\frac{1}{130} \\begin{pmatrix} 206/5 - 22/10 \\\\ 22/5 - 109/10 \\end{pmatrix}\n$$\n$$\nB_{k+1} s_k = \\frac{1}{130} \\begin{pmatrix} 412/10 - 22/10 \\\\ 44/10 - 109/10 \\end{pmatrix} = \\frac{1}{130} \\begin{pmatrix} 390/10 \\\\ -65/10 \\end{pmatrix} = \\frac{1}{130} \\begin{pmatrix} 39 \\\\ -6.5 \\end{pmatrix}\n$$\n$$\nB_{k+1} s_k = \\begin{pmatrix} 39/130 \\\\ -6.5/130 \\end{pmatrix} = \\begin{pmatrix} 3/10 \\\\ -1/20 \\end{pmatrix} = \\begin{pmatrix} 0.3 \\\\ -0.05 \\end{pmatrix} = y_k\n$$\nThe secant condition is satisfied.\n\nFinally, we compute the determinant of $B_{k+1}$:\n$$\n\\det(B_{k+1}) = \\det \\begin{pmatrix} 103/65  11/65 \\\\ 11/65  109/130 \\end{pmatrix} = \\left(\\frac{103}{65}\\right)\\left(\\frac{109}{130}\\right) - \\left(\\frac{11}{65}\\right)\\left(\\frac{11}{65}\\right)\n$$\n$$\n\\det(B_{k+1}) = \\frac{103 \\times 109}{65 \\times 130} - \\frac{11 \\times 11 \\times 2}{65 \\times 130} = \\frac{11227 - 242}{8450} = \\frac{10985}{8450}\n$$\nTo simplify the fraction, we note that $8450 = 10 \\times 845 = 10 \\times 5 \\times 169 = 50 \\times 13^2$.\nAnd $10985 = 5 \\times 2197 = 5 \\times 13^3$.\n$$\n\\det(B_{k+1}) = \\frac{5 \\times 13^3}{50 \\times 13^2} = \\frac{13}{10}\n$$\nAlternatively, using the BFGS determinant update identity:\n$$\n\\det(B_{k+1}) = \\det(B_k) \\frac{y_k^T s_k}{s_k^T B_k s_k} = \\det(I) \\frac{0.065}{0.05} = 1 \\times \\frac{65}{50} = \\frac{13}{10}\n$$\nBoth methods yield the same result. The determinant of $B_{k+1}$ is $13/10$.",
            "answer": "$$\n\\boxed{\\frac{13}{10}}\n$$"
        },
        {
            "introduction": "In real-world applications, such as calibrating complex microkinetic models, the number of parameters can be very large, making the storage and manipulation of a dense $n \\times n$ Hessian approximation computationally infeasible. This is where the Limited-memory BFGS (L-BFGS) algorithm becomes indispensable. Instead of forming the inverse Hessian $H_k$, L-BFGS computes the search direction vector $p_k = -H_k \\nabla f(x_k)$ directly using only a few of the most recent curvature pairs $(s_i, y_i)$. This advanced exercise  guides you through the celebrated two-loop recursion, the algorithmic marvel that makes L-BFGS both memory-efficient and powerful, and a staple in large-scale optimization.",
            "id": "3897724",
            "problem": "A heterogeneous catalytic reactor model is calibrated by minimizing a regularized least-squares objective function $f(x)$ over a vector of kinetic parameters $x \\in \\mathbb{R}^{n}$. At iteration $k$, a quasi-Newton method constructs a search direction $p_{k} = -H_{k} \\nabla f(x_{k})$, where $H_{k}$ approximates the inverse of the Hessian of $f$ at $x_{k}$, and $\\nabla f(x_{k})$ is the gradient at the current iterate.\n\nIn the Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) method, $H_{k}$ is built implicitly from a small set of stored curvature pairs $(s_{i}, y_{i})$, where $s_{i} = x_{i+1} - x_{i}$ and $y_{i} = \\nabla f(x_{i+1}) - \\nabla f(x_{i})$, with the positive curvature condition $s_{i}^{\\top} y_{i}  0$. The initial inverse-Hessian model is taken as $H_{k}^{0} = \\gamma_{k} I$ with a scalar scaling $\\gamma_{k}  0$ and the identity matrix $I$.\n\nStarting from the quasi-Newton secant condition and the Broyden–Fletcher–Goldfarb–Shanno (BFGS) inverse-Hessian update principle of minimal change subject to satisfying the secant condition and positive definiteness, derive the L-BFGS two-loop recursion that computes $p_{k} = -H_{k} \\nabla f(x_{k})$ using the stored pairs $(s_{i}, y_{i})$ and the initial scaling $\\gamma_{k} I$. Explicitly give the forward loop and the backward loop transformations that map $q = \\nabla f(x_{k})$ to $p_{k}$.\n\nThen, apply your derived recursion to the following chemically plausible calibration subproblem with $n = 2$ and memory size $m = 2$:\n- Stored pairs: $s_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $y_{1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, $s_{2} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, $y_{2} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\n- Current gradient: $\\nabla f(x_{k}) = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$.\n- Initial scaling: $\\gamma_{k} = 0.4$.\n\nCompute the final L-BFGS search direction $p_{k}$ using your two-loop recursion. Express your final vector answer as a row matrix and round each component to four significant figures. The components are dimensionless for this calibration subproblem.",
            "solution": "The problem requires the derivation of the L-BFGS two-loop recursion and its application to a numerical example.\n\n### Part 1: Derivation of the L-BFGS Two-Loop Recursion\n\nThe Broyden–Fletcher–Goldfarb–Shanno (BFGS) method provides an update for the inverse Hessian approximation, $H_k$. Given the current approximation $H_k$ and a new curvature pair $(s_k, y_k)$ where $s_k = x_{k+1}-x_k$ and $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$, a compact form of the update for the inverse Hessian $H_{k+1}$ is:\n$$H_{k+1} = (I - \\rho_k s_k y_k^\\top) H_k (I - \\rho_k y_k s_k^\\top) + \\rho_k s_k s_k^\\top$$\nwhere $\\rho_k = 1/(y_k^\\top s_k)$. The condition $s_k^\\top y_k > 0$ is required for stability.\n\nIn the Limited-memory BFGS (L-BFGS) method, the matrix $H_k$ is not stored explicitly. Instead, at each iteration $k$, it is implicitly constructed by applying the BFGS update formula $m$ times to an initial matrix $H_k^0 = \\gamma_k I$, using the $m$ most recent curvature pairs $(s_i, y_i)$, for $i = k-m, \\dots, k-1$.\n\nThe objective is to compute the product $H_k q$, where $q = \\nabla f(x_k)$, without forming the matrix $H_k$. This is achieved via the L-BFGS two-loop recursion. Let the stored pairs be indexed from oldest to newest: $(s_{k-m}, y_{k-m}), \\dots, (s_{k-1}, y_{k-1})$. The matrix $H_k$ is formed by updating $H_k^0$ with this sequence of pairs.\n\nThe two-loop recursion algorithm is as follows:\nLet $q = \\nabla f(x_k)$.\n1. **Backward Loop:**\n   For $i = k-1$ down to $k-m$:\n   - $\\alpha_i = \\rho_i s_i^\\top q$\n   - $q = q - \\alpha_i y_i$\n   The $\\alpha_i$ values are stored for the second loop.\n\n2. **Initial Hessian Product:**\n   - $r = H_k^0 q = \\gamma_k q$\n\n3. **Forward Loop:**\n   For $i = k-m$ up to $k-1$:\n   - $\\beta = \\rho_i y_i^\\top r$\n   - $r = r + s_i (\\alpha_i - \\beta)$\n\nThe final vector $r$ is the result of $H_k \\nabla f(x_k)$, and the search direction is $p_k = -r$. This algorithm can be derived by recursively applying the compact update formula to the vector $q$ and rearranging terms, which efficiently computes the matrix-vector product with a computational cost of $\\mathcal{O}(nm)$.\n\n### Part 2: Application to the Numerical Subproblem\n\nWe are given the following:\n- Memory size: $m=2$.\n- The pairs are indexed such that pair 2 is the most recent (index $k-1$) and pair 1 is the oldest in memory (index $k-2$).\n- $s_{k-2} = s_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $y_{k-2} = y_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n- $s_{k-1} = s_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, $y_{k-1} = y_2 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\n- Current gradient: $\\nabla f(x_k) = q_{orig} = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$.\n- Initial scaling: $\\gamma_k = 0.4$, so $H_k^0 = 0.4I$.\n\nFirst, we compute the scalars $\\rho_i = 1/(y_i^\\top s_i)$:\n$\\rho_{k-2} = \\rho_1 = \\frac{1}{s_1^\\top y_1} = \\frac{1}{\\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}} = \\frac{1}{1} = 1$.\n$\\rho_{k-1} = \\rho_2 = \\frac{1}{s_2^\\top y_2} = \\frac{1}{\\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}} = \\frac{1}{2} = 0.5$.\n\nNow, we execute the two-loop recursion.\nInitialize $q = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$.\n\n**Backward Loop:**\n- **For $i=k-1$ (pair 2):**\n  $\\alpha_{k-1} = \\alpha_2 = \\rho_2 s_2^\\top q = 0.5 \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} = 0.5 \\times 4 = 2$.\n  $q = q - \\alpha_2 y_2 = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} - 2 \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 3 - 2 \\\\ 4 - 4 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n\n- **For $i=k-2$ (pair 1):**\n  $\\alpha_{k-2} = \\alpha_1 = \\rho_1 s_1^\\top q = 1 \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 1 \\times 1 = 1$.\n  $q = q - \\alpha_1 y_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - 1 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 - 1 \\\\ 0 - 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}$.\n\nWe have stored $\\alpha_1=1$ and $\\alpha_2=2$.\n\n**Initial Hessian Product:**\n$r = H_k^0 q = \\gamma_k q = 0.4 \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -0.4 \\end{pmatrix}$.\n\n**Forward Loop:**\n- **For $i=k-2$ (pair 1):**\n  $\\beta = \\rho_1 y_1^\\top r = 1 \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -0.4 \\end{pmatrix} = -0.4$.\n  $r = r + s_1(\\alpha_1 - \\beta) = \\begin{pmatrix} 0 \\\\ -0.4 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} (1 - (-0.4)) = \\begin{pmatrix} 0 \\\\ -0.4 \\end{pmatrix} + \\begin{pmatrix} 1.4 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1.4 \\\\ -0.4 \\end{pmatrix}$.\n\n- **For $i=k-1$ (pair 2):**\n  $\\beta = \\rho_2 y_2^\\top r = 0.5 \\begin{pmatrix} 1  2 \\end{pmatrix} \\begin{pmatrix} 1.4 \\\\ -0.4 \\end{pmatrix} = 0.5 (1.4 - 0.8) = 0.5 \\times 0.6 = 0.3$.\n  $r = r + s_2(\\alpha_2 - \\beta) = \\begin{pmatrix} 1.4 \\\\ -0.4 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} (2 - 0.3) = \\begin{pmatrix} 1.4 \\\\ -0.4 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1.7 \\end{pmatrix} = \\begin{pmatrix} 1.4 \\\\ 1.3 \\end{pmatrix}$.\n\nThe result of the recursion is $H_k \\nabla f(x_k) = r = \\begin{pmatrix} 1.4 \\\\ 1.3 \\end{pmatrix}$.\nThe search direction is $p_k = -r = \\begin{pmatrix} -1.4 \\\\ -1.3 \\end{pmatrix}$.\n\nThe problem requires the final answer as a row matrix with components rounded to four significant figures.\n$p_k = \\begin{pmatrix} -1.400  -1.300 \\end{pmatrix}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-1.400  -1.300\n\\end{pmatrix}\n}\n$$"
        }
    ]
}