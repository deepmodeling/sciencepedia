{
    "hands_on_practices": [
        {
            "introduction": "At the heart of many geometry optimization algorithms lies the Newton-Raphson method, which provides a powerful way to navigate the potential energy surface. This exercise breaks down the fundamental mechanics of a single Newton step by using a local quadratic model of the energy. By working through this problem , you will gain a concrete understanding of how the gradient and Hessian are used to predict the displacement that leads directly to the minimum of this model, a foundational concept for all second-order optimization methods.",
            "id": "2894209",
            "problem": "In a quantum chemistry geometry optimization performed in scaled, dimensionless internal coordinates, the electronic energy $E(\\mathbf{q})$ in the neighborhood of a current structure $\\mathbf{q}_{0}$ is approximated by the second-order Taylor model\n$$\nm(\\mathbf{s}) \\equiv E(\\mathbf{q}_{0}) + \\mathbf{g}^{\\top}\\mathbf{s} + \\tfrac{1}{2}\\mathbf{s}^{\\top}\\mathbf{H}\\mathbf{s},\n$$\nwhere $\\mathbf{s} = \\mathbf{q} - \\mathbf{q}_{0}$ is the displacement, $\\mathbf{g} = \\nabla E(\\mathbf{q}_{0})$ is the gradient, and $\\mathbf{H} = \\nabla^{2} E(\\mathbf{q}_{0})$ is the Hessian. Both $\\mathbf{g}$ and $\\mathbf{H}$ have units of energy because the coordinates are dimensionless by construction. The Newton step is defined as the displacement that minimizes the quadratic model.\n\nAt $\\mathbf{q}_{0}$, suppose the gradient and Hessian are\n$$\n\\mathbf{g} =\n\\begin{pmatrix}\n0.06 \\\\\n-0.08\n\\end{pmatrix}\n\\ \\text{Hartree}, \n\\qquad\n\\mathbf{H} =\n\\begin{pmatrix}\n1.2 & 0.3 \\\\\n0.3 & 0.9\n\\end{pmatrix}\n\\ \\text{Hartree}.\n$$\nStarting from the second-order Taylor model and the definition of a minimizer, derive the expression for the Newton step $\\mathbf{s}_{N}$ in terms of $\\mathbf{g}$ and $\\mathbf{H}$, compute $\\mathbf{s}_{N}$ explicitly for the data above, and then evaluate the predicted quadratic energy change\n$$\n\\Delta E_{\\text{pred}} \\equiv m(\\mathbf{s}_{N}) - m(\\mathbf{0}).\n$$\nExpress the final energy change in Hartree and round your answer to five significant figures. The final response must be a single real number.",
            "solution": "The problem as stated is scientifically sound, well-posed, and contains all necessary information for a unique solution. The Hessian matrix is positive definite, which guarantees that the quadratic model has a unique minimum. Therefore, I will proceed with the solution.\n\nThe objective is to find the displacement $\\mathbf{s}$ that minimizes the quadratic model of the electronic energy:\n$$\nm(\\mathbf{s}) = E(\\mathbf{q}_{0}) + \\mathbf{g}^{\\top}\\mathbf{s} + \\frac{1}{2}\\mathbf{s}^{\\top}\\mathbf{H}\\mathbf{s}\n$$\nA necessary condition for a minimum of a differentiable function is that its gradient vanishes. We must compute the gradient of $m(\\mathbf{s})$ with respect to the displacement vector $\\mathbf{s}$ and set it to the zero vector.\nThe gradient is given by:\n$$\n\\nabla_{\\mathbf{s}} m(\\mathbf{s}) = \\nabla_{\\mathbf{s}} \\left( E(\\mathbf{q}_{0}) + \\mathbf{g}^{\\top}\\mathbf{s} + \\frac{1}{2}\\mathbf{s}^{\\top}\\mathbf{H}\\mathbf{s} \\right)\n$$\nThe term $E(\\mathbf{q}_{0})$ is a constant with respect to $\\mathbf{s}$, so its derivative is zero. The derivatives of the linear and quadratic terms are standard results from vector calculus:\n$$\n\\nabla_{\\mathbf{s}} (\\mathbf{g}^{\\top}\\mathbf{s}) = \\mathbf{g}\n$$\n$$\n\\nabla_{\\mathbf{s}} \\left(\\frac{1}{2}\\mathbf{s}^{\\top}\\mathbf{H}\\mathbf{s}\\right) = \\mathbf{H}\\mathbf{s}\n$$\nwhere the second result assumes a symmetric Hessian matrix $\\mathbf{H}$, which is true for a second derivative matrix. Combining these, the gradient of the model is:\n$$\n\\nabla_{\\mathbf{s}} m(\\mathbf{s}) = \\mathbf{g} + \\mathbf{H}\\mathbf{s}\n$$\nThe Newton step, denoted $\\mathbf{s}_{N}$, is the specific displacement that minimizes $m(\\mathbf{s})$. To find it, we set the gradient to the zero vector:\n$$\n\\mathbf{g} + \\mathbf{H}\\mathbf{s}_{N} = \\mathbf{0}\n$$\nSolving for $\\mathbf{s}_{N}$ yields the expression for the Newton step:\n$$\n\\mathbf{H}\\mathbf{s}_{N} = -\\mathbf{g}\n$$\n$$\n\\mathbf{s}_{N} = -\\mathbf{H}^{-1}\\mathbf{g}\n$$\nThis requires the Hessian matrix $\\mathbf{H}$ to be invertible. The determinant of the given Hessian is $\\det(\\mathbf{H}) = (1.2)(0.9) - (0.3)(0.3) = 1.08 - 0.09 = 0.99$. Since $\\det(\\mathbf{H}) \\neq 0$, the inverse exists and the Newton step is well-defined.\n\nNow, we compute $\\mathbf{s}_{N}$ using the provided data:\n$$\n\\mathbf{g} =\n\\begin{pmatrix}\n0.06 \\\\\n-0.08\n\\end{pmatrix}\n, \\qquad\n\\mathbf{H} =\n\\begin{pmatrix}\n1.2 & 0.3 \\\\\n0.3 & 0.9\n\\end{pmatrix}\n$$\nWe solve the linear system $\\mathbf{H}\\mathbf{s}_{N} = -\\mathbf{g}$:\n$$\n\\begin{pmatrix}\n1.2 & 0.3 \\\\\n0.3 & 0.9\n\\end{pmatrix}\n\\begin{pmatrix}\ns_1 \\\\\ns_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-0.06 \\\\\n0.08\n\\end{pmatrix}\n$$\nThis corresponds to the system of equations:\n1. $1.2s_1 + 0.3s_2 = -0.06$\n2. $0.3s_1 + 0.9s_2 = 0.08$\n\nMultiplying the first equation by $3$ gives $3.6s_1 + 0.9s_2 = -0.18$. Subtracting the second equation from this result eliminates $s_2$:\n$$\n(3.6s_1 - 0.3s_1) = -0.18 - 0.08\n$$\n$$\n3.3s_1 = -0.26 \\implies s_1 = -\\frac{0.26}{3.3} = -\\frac{26}{330} = -\\frac{13}{165}\n$$\nSubstituting $s_1$ back into the second equation:\n$$\n0.3\\left(-\\frac{13}{165}\\right) + 0.9s_2 = 0.08\n$$\n$$\n-\\frac{3.9}{165} + 0.9s_2 = 0.08 \\implies -\\frac{13}{550} + 0.9s_2 = \\frac{8}{100} = \\frac{2}{25}\n$$\n$$\n0.9s_2 = \\frac{2}{25} + \\frac{13}{550} = \\frac{44}{550} + \\frac{13}{550} = \\frac{57}{550}\n$$\n$$\ns_2 = \\frac{57}{550 \\times 0.9} = \\frac{57}{495} = \\frac{19}{165}\n$$\nSo, the Newton step is $\\mathbf{s}_{N} = \\begin{pmatrix} -13/165 \\\\ 19/165 \\end{pmatrix}$.\n\nNext, we evaluate the predicted quadratic energy change, $\\Delta E_{\\text{pred}}$:\n$$\n\\Delta E_{\\text{pred}} = m(\\mathbf{s}_{N}) - m(\\mathbf{0})\n$$\nSubstituting the definition of $m(\\mathbf{s})$:\n$$\nm(\\mathbf{s}_{N}) = E(\\mathbf{q}_{0}) + \\mathbf{g}^{\\top}\\mathbf{s}_{N} + \\frac{1}{2}\\mathbf{s}_{N}^{\\top}\\mathbf{H}\\mathbf{s}_{N}\n$$\n$$\nm(\\mathbf{0}) = E(\\mathbf{q}_{0})\n$$\nSo,\n$$\n\\Delta E_{\\text{pred}} = \\mathbf{g}^{\\top}\\mathbf{s}_{N} + \\frac{1}{2}\\mathbf{s}_{N}^{\\top}\\mathbf{H}\\mathbf{s}_{N}\n$$\nWe can simplify this expression. From the derivation of the Newton step, we have $\\mathbf{H}\\mathbf{s}_{N} = -\\mathbf{g}$. Substituting this into the quadratic term:\n$$\n\\mathbf{s}_{N}^{\\top}\\mathbf{H}\\mathbf{s}_{N} = \\mathbf{s}_{N}^{\\top}(-\\mathbf{g}) = - \\mathbf{s}_{N}^{\\top}\\mathbf{g}\n$$\nSince $\\mathbf{s}_{N}^{\\top}\\mathbf{g}$ is a scalar, it is equal to its own transpose, $(\\mathbf{s}_{N}^{\\top}\\mathbf{g})^{\\top} = \\mathbf{g}^{\\top}\\mathbf{s}_{N}$. Thus, $\\mathbf{s}_{N}^{\\top}\\mathbf{H}\\mathbf{s}_{N} = -\\mathbf{g}^{\\top}\\mathbf{s}_{N}$.\nThe energy change becomes:\n$$\n\\Delta E_{\\text{pred}} = \\mathbf{g}^{\\top}\\mathbf{s}_{N} + \\frac{1}{2}(-\\mathbf{g}^{\\top}\\mathbf{s}_{N}) = \\frac{1}{2}\\mathbf{g}^{\\top}\\mathbf{s}_{N}\n$$\nNow, we compute this value:\n$$\n\\Delta E_{\\text{pred}} = \\frac{1}{2}\n\\begin{pmatrix}\n0.06 & -0.08\n\\end{pmatrix}\n\\begin{pmatrix}\n-13/165 \\\\\n19/165\n\\end{pmatrix}\n$$\n$$\n\\Delta E_{\\text{pred}} = \\frac{1}{2} \\left[ (0.06)\\left(-\\frac{13}{165}\\right) + (-0.08)\\left(\\frac{19}{165}\\right) \\right]\n$$\n$$\n\\Delta E_{\\text{pred}} = \\frac{1}{2 \\times 165} \\left[ (0.06)(-13) + (-0.08)(19) \\right]\n$$\n$$\n\\Delta E_{\\text{pred}} = \\frac{1}{330} \\left[ -0.78 - 1.52 \\right] = \\frac{-2.3}{330} = -\\frac{23}{3300}\n$$\nPerforming the division:\n$$\n\\Delta E_{\\text{pred}} = -\\frac{23}{3300} \\approx -0.00696969... \\ \\text{Hartree}\n$$\nThe problem requires this value rounded to five significant figures. The first non-zero digit is the $6$ in the thousandths place. The first five significant figures are $6$, $9$, $6$, $9$, $6$. The sixth significant digit is $9$, which is $5$ or greater, so we round up the fifth digit.\n$$\n-0.0069696 \\underline{9}... \\implies -0.0069697\n$$",
            "answer": "$$\\boxed{-0.0069697}$$"
        },
        {
            "introduction": "While the pure Newton-Raphson method is conceptually elegant, computing and inverting the full Hessian matrix at every step can be computationally prohibitive for most chemical systems. This is where quasi-Newton methods, particularly the Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) algorithm, become essential. This practice  demystifies the L-BFGS procedure by guiding you through its famous 'two-loop recursion,' showing how an effective search direction is calculated using only a limited history of previous steps and gradients.",
            "id": "2894174",
            "problem": "You are minimizing the Born–Oppenheimer potential energy surface (PES) $E(\\mathbf{R})$ of a small molecule in mass-weighted Cartesian coordinates using a quasi-Newton geometry optimization. At the current iterate $\\mathbf{R}_{k+1}$, you have the gradient $\\mathbf{g}_{k+1} = \\nabla E(\\mathbf{R}_{k+1})$. The Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) method forms a step $\\mathbf{p}_{k+1} = - H_{k+1} \\mathbf{g}_{k+1}$ by applying an implicit inverse-Hessian approximation $H_{k+1}$, constructed from the past displacement and gradient-difference pairs that satisfy the secant condition. Assume a $2$-dimensional subspace spanned by two internal coordinates (in atomic units), and that the last $3$ pairs are, from oldest to most recent,\n$\\mathbf{s}_{0} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}\\,\\text{bohr}$, $\\mathbf{y}_{0} = \\begin{pmatrix}2 \\\\ 0\\end{pmatrix}\\,\\text{Ha}\\,\\text{bohr}^{-1}$,\n$\\mathbf{s}_{1} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}\\,\\text{bohr}$, $\\mathbf{y}_{1} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}\\,\\text{Ha}\\,\\text{bohr}^{-1}$,\n$\\mathbf{s}_{2} = \\begin{pmatrix}1 \\\\ 1\\end{pmatrix}\\,\\text{bohr}$, $\\mathbf{y}_{2} = \\begin{pmatrix}2 \\\\ 1\\end{pmatrix}\\,\\text{Ha}\\,\\text{bohr}^{-1}$.\nAt $\\mathbf{R}_{k+1}$ the gradient is\n$\\mathbf{g}_{k+1} = \\begin{pmatrix}3 \\\\ 2\\end{pmatrix}\\,\\text{Ha}\\,\\text{bohr}^{-1}$.\nUse the standard two-loop recursion for the L-BFGS inverse-Hessian product with memory $m=3$, initialized with the scalar-multiple identity $H_{0} = \\gamma I$ where $\\gamma = \\dfrac{\\mathbf{s}_{2}^{\\top} \\mathbf{y}_{2}}{\\mathbf{y}_{2}^{\\top} \\mathbf{y}_{2}}$, to compute the step $\\mathbf{p}_{k+1} = -H_{k+1}\\mathbf{g}_{k+1}$. Then, report the Euclidean norm $\\|\\mathbf{p}_{k+1}\\|_{2}$.\nRound your answer to four significant figures and express the final result in bohr.\nYour final answer must be a single real-valued number.",
            "solution": "The problem requires the computation of a search direction, or step, $\\mathbf{p}_{k+1}$ for a geometry optimization using the Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) algorithm. The step is given by $\\mathbf{p}_{k+1} = -H_{k+1}\\mathbf{g}_{k+1}$, where $H_{k+1}$ is the approximate inverse Hessian and $\\mathbf{g}_{k+1}$ is the gradient of the potential energy at the current geometry $\\mathbf{R}_{k+1}$. The calculation is to be performed using the standard L-BFGS two-loop recursion.\n\nFirst, let us verify the validity of the problem statement. The problem provides all necessary information: the gradient vector $\\mathbf{g}_{k+1}$, the memory size $m=3$, the history of the last $m$ displacement vectors $\\mathbf{s}_i$ and gradient difference vectors $\\mathbf{y}_i$, and the specific formula for the initial inverse Hessian approximation $H_0$. The L-BFGS method requires that the curvature condition $\\mathbf{s}_i^\\top \\mathbf{y}_i > 0$ is satisfied for all pairs in memory to ensure that the Hessian approximation remains positive definite. Let us check this condition for the given data.\nFor $i=0$: $\\mathbf{s}_{0}^{\\top} \\mathbf{y}_{0} = \\begin{pmatrix}1 & 0\\end{pmatrix} \\begin{pmatrix}2 \\\\ 0\\end{pmatrix} = 2 > 0$.\nFor $i=1$: $\\mathbf{s}_{1}^{\\top} \\mathbf{y}_{1} = \\begin{pmatrix}0 & 1\\end{pmatrix} \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} = 1 > 0$.\nFor $i=2$: $\\mathbf{s}_{2}^{\\top} \\mathbf{y}_{2} = \\begin{pmatrix}1 & 1\\end{pmatrix} \\begin{pmatrix}2 \\\\ 1\\end{pmatrix} = (1)(2) + (1)(1) = 3 > 0$.\nAll pairs satisfy the curvature condition. The problem is well-posed and scientifically sound. We proceed with the solution.\n\nThe L-BFGS two-loop recursion computes the product $\\mathbf{r} = H_{k+1}\\mathbf{g}_{k+1}$ without forming the matrix $H_{k+1}$ explicitly. The algorithm is as follows, adapted to the problem's indexing ($i=0, 1, 2$ from oldest to most recent):\n\nLet $\\mathbf{g} = \\mathbf{g}_{k+1} = \\begin{pmatrix}3 \\\\ 2\\end{pmatrix}$.\nThe historical pairs are:\n$\\mathbf{s}_{0} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$, $\\mathbf{y}_{0} = \\begin{pmatrix}2 \\\\ 0\\end{pmatrix}$\n$\\mathbf{s}_{1} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$, $\\mathbf{y}_{1} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$\n$\\mathbf{s}_{2} = \\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$, $\\mathbf{y}_{2} = \\begin{pmatrix}2 \\\\ 1\\end{pmatrix}$\n\nWe first compute the scalars $\\rho_i = 1 / (\\mathbf{y}_i^\\top \\mathbf{s}_i)$:\n$\\rho_0 = (\\mathbf{y}_0^\\top \\mathbf{s}_0)^{-1} = (2)^{-1} = \\frac{1}{2}$.\n$\\rho_1 = (\\mathbf{y}_1^\\top \\mathbf{s}_1)^{-1} = (1)^{-1} = 1$.\n$\\rho_2 = (\\mathbf{y}_2^\\top \\mathbf{s}_2)^{-1} = (3)^{-1} = \\frac{1}{3}$.\n\nThe two-loop recursion proceeds as follows:\nInitialize $\\mathbf{q} = \\mathbf{g} = \\begin{pmatrix}3 \\\\ 2\\end{pmatrix}$.\n\n**First loop (from $i=2$ down to $0$):**\nFor $i=2$:\n$\\alpha_2 = \\rho_2 \\mathbf{s}_2^\\top \\mathbf{q} = \\frac{1}{3} \\begin{pmatrix}1 & 1\\end{pmatrix} \\begin{pmatrix}3 \\\\ 2\\end{pmatrix} = \\frac{1}{3}(3+2) = \\frac{5}{3}$.\n$\\mathbf{q} \\leftarrow \\mathbf{q} - \\alpha_2 \\mathbf{y}_2 = \\begin{pmatrix}3 \\\\ 2\\end{pmatrix} - \\frac{5}{3} \\begin{pmatrix}2 \\\\ 1\\end{pmatrix} = \\begin{pmatrix}3 - \\frac{10}{3} \\\\ 2 - \\frac{5}{3}\\end{pmatrix} = \\begin{pmatrix}-\\frac{1}{3} \\\\ \\frac{1}{3}\\end{pmatrix}$.\n\nFor $i=1$:\n$\\alpha_1 = \\rho_1 \\mathbf{s}_1^\\top \\mathbf{q} = 1 \\begin{pmatrix}0 & 1\\end{pmatrix} \\begin{pmatrix}-\\frac{1}{3} \\\\ \\frac{1}{3}\\end{pmatrix} = \\frac{1}{3}$.\n$\\mathbf{q} \\leftarrow \\mathbf{q} - \\alpha_1 \\mathbf{y}_1 = \\begin{pmatrix}-\\frac{1}{3} \\\\ \\frac{1}{3}\\end{pmatrix} - \\frac{1}{3} \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} = \\begin{pmatrix}-\\frac{1}{3} \\\\ 0\\end{pmatrix}$.\n\nFor $i=0$:\n$\\alpha_0 = \\rho_0 \\mathbf{s}_0^\\top \\mathbf{q} = \\frac{1}{2} \\begin{pmatrix}1 & 0\\end{pmatrix} \\begin{pmatrix}-\\frac{1}{3} \\\\ 0\\end{pmatrix} = -\\frac{1}{6}$.\n$\\mathbf{q} \\leftarrow \\mathbf{q} - \\alpha_0 \\mathbf{y}_0 = \\begin{pmatrix}-\\frac{1}{3} \\\\ 0\\end{pmatrix} - (-\\frac{1}{6}) \\begin{pmatrix}2 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}-\\frac{1}{3} + \\frac{2}{6} \\\\ 0\\end{pmatrix} = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$.\n\nAfter the first loop, the vector $\\mathbf{q}$ is $\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$. The computed coefficients are $\\alpha_0 = -1/6$, $\\alpha_1 = 1/3$, $\\alpha_2 = 5/3$.\n\n**Scaling and initial step:**\nThe initial inverse Hessian is $H_0 = \\gamma I$, where $\\gamma = \\frac{\\mathbf{s}_{2}^{\\top} \\mathbf{y}_{2}}{\\mathbf{y}_{2}^{\\top} \\mathbf{y}_{2}}$.\n$\\mathbf{s}_{2}^{\\top} \\mathbf{y}_{2} = 3$.\n$\\mathbf{y}_{2}^{\\top} \\mathbf{y}_{2} = \\begin{pmatrix}2 & 1\\end{pmatrix} \\begin{pmatrix}2 \\\\ 1\\end{pmatrix} = 2^2 + 1^2 = 5$.\nSo, $\\gamma = \\frac{3}{5}$.\nThe initial step vector $\\mathbf{r}$ is calculated as $\\mathbf{r} = H_0 \\mathbf{q} = \\frac{3}{5} I \\begin{pmatrix}0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$.\n\n**Second loop (from $i=0$ up to $2$):**\nInitialize $\\mathbf{r} = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$.\n\nFor $i=0$:\n$\\beta_0 = \\rho_0 \\mathbf{y}_0^\\top \\mathbf{r} = \\frac{1}{2} \\begin{pmatrix}2 & 0\\end{pmatrix} \\begin{pmatrix}0 \\\\ 0\\end{pmatrix} = 0$.\n$\\mathbf{r} \\leftarrow \\mathbf{r} + \\mathbf{s}_0 (\\alpha_0 - \\beta_0) = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix} + \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} (-\\frac{1}{6} - 0) = \\begin{pmatrix}-\\frac{1}{6} \\\\ 0\\end{pmatrix}$.\n\nFor $i=1$:\n$\\beta_1 = \\rho_1 \\mathbf{y}_1^\\top \\mathbf{r} = 1 \\begin{pmatrix}0 & 1\\end{pmatrix} \\begin{pmatrix}-\\frac{1}{6} \\\\ 0\\end{pmatrix} = 0$.\n$\\mathbf{r} \\leftarrow \\mathbf{r} + \\mathbf{s}_1 (\\alpha_1 - \\beta_1) = \\begin{pmatrix}-\\frac{1}{6} \\\\ 0\\end{pmatrix} + \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} (\\frac{1}{3} - 0) = \\begin{pmatrix}-\\frac{1}{6} \\\\ \\frac{1}{3}\\end{pmatrix}$.\n\nFor $i=2$:\n$\\beta_2 = \\rho_2 \\mathbf{y}_2^\\top \\mathbf{r} = \\frac{1}{3} \\begin{pmatrix}2 & 1\\end{pmatrix} \\begin{pmatrix}-\\frac{1}{6} \\\\ \\frac{1}{3}\\end{pmatrix} = \\frac{1}{3}(-\\frac{2}{6} + \\frac{1}{3}) = \\frac{1}{3}(-\\frac{1}{3} + \\frac{1}{3}) = 0$.\n$\\mathbf{r} \\leftarrow \\mathbf{r} + \\mathbf{s}_2 (\\alpha_2 - \\beta_2) = \\begin{pmatrix}-\\frac{1}{6} \\\\ \\frac{1}{3}\\end{pmatrix} + \\begin{pmatrix}1 \\\\ 1\\end{pmatrix} (\\frac{5}{3} - 0) = \\begin{pmatrix}-\\frac{1}{6} + \\frac{5}{3} \\\\ \\frac{1}{3} + \\frac{5}{3}\\end{pmatrix} = \\begin{pmatrix}-\\frac{1}{6} + \\frac{10}{6} \\\\ \\frac{6}{3}\\end{pmatrix} = \\begin{pmatrix}\\frac{9}{6} \\\\ 2\\end{pmatrix} = \\begin{pmatrix}1.5 \\\\ 2\\end{pmatrix}$.\n\nThe result of the two-loop recursion is $\\mathbf{r} = H_{k+1}\\mathbf{g}_{k+1} = \\begin{pmatrix}1.5 \\\\ 2\\end{pmatrix}$.\nThe L-BFGS step is $\\mathbf{p}_{k+1} = -\\mathbf{r} = \\begin{pmatrix}-1.5 \\\\ -2\\end{pmatrix}$. The units of this vector are bohr.\n\nThe final task is to compute the Euclidean norm of this step vector, $\\|\\mathbf{p}_{k+1}\\|_2$.\n$$\n\\|\\mathbf{p}_{k+1}\\|_2 = \\left\\| \\begin{pmatrix}-1.5 \\\\ -2\\end{pmatrix} \\right\\|_2 = \\sqrt{(-1.5)^2 + (-2)^2} = \\sqrt{2.25 + 4} = \\sqrt{6.25} = 2.5\n$$\nThe value is exactly $2.5$. The problem requests the answer to be rounded to four significant figures. Thus, the result is $2.500$.",
            "answer": "$$ \\boxed{2.500} $$"
        },
        {
            "introduction": "A successful geometry optimization run ends when the forces on the atoms are negligible, but this alone does not guarantee you have found a stable structure. It is crucial to characterize the nature of the stationary point to determine if it is a true energy minimum, a transition state, or a higher-order saddle point. This problem  focuses on this vital verification step, demonstrating how to use the eigenvalues of the Hessian matrix to classify a stationary point and outlining the essential diagnostic checks required to confidently report a calculated structure as a local minimum.",
            "id": "2894234",
            "problem": "In a quantum chemistry geometry optimization using a reduced set of internal coordinates of dimension $3$ for a nonlinear molecule (with overall translation and rotation already projected out), a candidate stationary point is obtained with gradient vector $\\mathbf{g} = (0,0,0)^{T}$ and a symmetric Hessian whose eigenvalues in these internal coordinates are $(0.5,\\,1.2,\\,3.0)$ in appropriate energy-curvature units. Which option best captures the correct theoretical conclusion about the nature of this stationary point and the additional diagnostics you should perform to rule out numerical singularities or coordinate pathologies before accepting the structure?\n\nA. Conclude it is a strict local minimum by invoking the second-order characterization of stationary points (positive-definite Hessian implies positive curvature in every direction). As diagnostics, confirm that internal-coordinate projection of external modes was correctly applied, perform a vibrational analysis in mass-weighted coordinates to verify that all nontrivial modes have real positive frequencies (i.e., $3N-6$ for a nonlinear molecule when mapped back), examine the Hessian condition number $\\kappa = \\lambda_{\\max}/\\lambda_{\\min}$ to ensure it is not ill-conditioned, and cross-validate with tight Self-Consistent Field (SCF) convergence and, if available, a finite-difference check of selected Hessian elements.\n\nB. Conclude it is a strict global minimum because all three eigenvalues are positive, and no further checks are necessary since $\\mathbf{g}=\\mathbf{0}$ guarantees optimality and the positivity of the Hessian eliminates any numerical concerns.\n\nC. Use determinant and trace as the decisive criteria: since $\\det(\\mathbf{H}) = 0.5\\times 1.2 \\times 3.0 = 1.8 > 0$ and $\\mathrm{tr}(\\mathbf{H}) = 0.5 + 1.2 + 3.0 = 4.7 > 0$, declare a local minimum; eigenvalue analysis, projection of external modes, and conditioning checks are unnecessary.\n\nD. Reject the minimum because proper characterization requires exactly $6$ zero eigenvalues to account for overall translation and rotation in Cartesian coordinates; the absence of such zero eigenvalues indicates a numerically singular Hessian and precludes concluding minimality.",
            "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- The system is a nonlinear molecule.\n- The geometry optimization is performed in a reduced set of internal coordinates of dimension $3$.\n- Overall translational and rotational degrees of freedom have been projected out from this coordinate system.\n- A stationary point has been located, characterized by a gradient vector $\\mathbf{g} = (0,0,0)^{T}$.\n- The Hessian matrix $\\mathbf{H}$ at this point is symmetric.\n- The eigenvalues of $\\mathbf{H}$ in these internal coordinates are given as $(\\lambda_1, \\lambda_2, \\lambda_3) = (0.5,\\,1.2,\\,3.0)$ in appropriate units.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem describes a standard procedure in computational quantum chemistry for characterizing stationary points on a potential energy surface (PES). The givens are:\n- **Scientifically Grounded:** The concepts of PES, gradient, Hessian, eigenvalues, internal coordinates, and projection of external modes are fundamental to theoretical and computational chemistry. The setup is entirely consistent with established scientific principles.\n- **Well-Posed:** The problem provides sufficient information ($\\mathbf{g} = \\mathbf{0}$ and all Hessian eigenvalues) to classify the stationary point according to standard multivariate calculus. The dimension of the coordinate space ($3$) is precisely specified, corresponding to the dimension of the gradient vector and the number of eigenvalues. This could represent a complete set of internal coordinates for a triatomic molecule ($3N-6 = 3(3)-6=3$), or a constrained optimization in a subspace for a larger molecule. In either context, the mathematical analysis is well-defined.\n- **Objective:** The problem is stated in precise, quantitative, and unambiguous terms. It is free of any subjectivity.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is a well-posed, scientifically sound question from the field of quantum chemistry. A solution will be derived.\n\n**Derivation and Evaluation**\n\nThe characterization of a stationary point on a potential energy surface $E(\\mathbf{q})$ depends on the first and second derivatives of the energy with respect to the coordinates $\\mathbf{q}$.\nThe first-order condition for a stationary point is that the gradient vector, $\\mathbf{g} = \\nabla E$, must be zero. The problem states this is satisfied, as $\\mathbf{g} = (0,0,0)^{T}$.\n\nThe second-order condition determines the nature of the stationary point and depends on the Hessian matrix, $\\mathbf{H}$, with elements $H_{ij} = \\frac{\\partial^2 E}{\\partial q_i \\partial q_j}$. The nature of the point is determined by the signs of the eigenvalues of $\\mathbf{H}$:\n- A strict local minimum requires all eigenvalues to be positive (a positive-definite Hessian).\n- A transition state (a first-order saddle point) has exactly one negative eigenvalue. Higher-order saddle points have more than one negative eigenvalue.\n- A local maximum has all negative eigenvalues.\n\nThe problem states that the Hessian is evaluated in a set of $3$ internal coordinates where translations and rotations have been removed. The eigenvalues are $\\lambda_1 = 0.5$, $\\lambda_2 = 1.2$, and $\\lambda_3 = 3.0$. Since all three eigenvalues are positive, the Hessian matrix is positive-definite. This satisfies the sufficient condition for the stationary point to be a strict local minimum with respect to this three-dimensional coordinate subspace.\n\nHowever, a theoretical conclusion must be supported by practical verification to rule out numerical artifacts or limitations of the model. A thorough diagnostic procedure is mandatory in any competent scientific investigation. These checks include:\n1.  **Vibrational Analysis:** The ultimate physical confirmation of a minimum is a vibrational frequency analysis. This involves computing the Hessian in mass-weighted Cartesian coordinates. For a true minimum, all $3N-6$ vibrational frequencies (for a nonlinear molecule of $N$ atoms) must be real and positive. The positive-definiteness of the internal coordinate Hessian should guarantee this, but performing the analysis serves as a crucial cross-check and confirms the coordinate transformation was handled correctly.\n2.  **Numerical Stability:** The condition number of the Hessian, $\\kappa = \\lambda_{\\max}/\\lambda_{\\min}$, is a measure of numerical sensitivity. Here, $\\kappa = 3.0 / 0.5 = 6.0$, which is a very small number, indicating a well-conditioned problem. A very large $\\kappa$ would suggest a nearly flat PES direction, where numerical precision could be an issue.\n3.  **Convergence Level:** The reliability of the gradient and Hessian depends on the tight convergence of the underlying electronic structure calculation (e.g., the Self-Consistent Field, SCF, procedure) and the geometry optimization algorithm itself. Loose convergence criteria can lead to spurious stationary points.\n4.  **Coordinate System Validity:** One must ensure that the chosen internal coordinates are appropriate for the molecular geometry and do not introduce singularities.\n5.  **Derivative Accuracy:** Finite-difference calculations of gradient or Hessian elements can be used to verify the correctness of the analytical derivative code, although this is more a code-verification step than a routine diagnostic.\n\nWith this formal background, we evaluate the provided options.\n\n**Option-by-Option Analysis**\n\n**A. Conclude it is a strict local minimum by invoking the second-order characterization of stationary points (positive-definite Hessian implies positive curvature in every direction). As diagnostics, confirm that internal-coordinate projection of external modes was correctly applied, perform a vibrational analysis in mass-weighted coordinates to verify that all nontrivial modes have real positive frequencies (i.e., $3N-6$ for a nonlinear molecule when mapped back), examine the Hessian condition number $\\kappa = \\lambda_{\\max}/\\lambda_{\\min}$ to ensure it is not ill-conditioned, and cross-validate with tight Self-Consistent Field (SCF) convergence and, if available, a finite-difference check of selected Hessian elements.**\nThis option correctly concludes that the point is a strict local minimum based on the positive-definite Hessian. It then lists a comprehensive and professionally rigorous set of diagnostic checks that a careful scientist must perform. Each listed diagnostic (vibrational analysis, condition number check, convergence verification, etc.) is standard practice for validating a computed molecular structure. The reference to mapping back to $3N-6$ modes correctly frames the result within the context of the full molecular system. This option is entirely correct.\n**Verdict: Correct**\n\n**B. Conclude it is a strict global minimum because all three eigenvalues are positive, and no further checks are necessary since $\\mathbf{g}=\\mathbf{0}$ guarantees optimality and the positivity of the Hessian eliminates any numerical concerns.**\nThis option is fundamentally flawed. Firstly, second-order conditions only provide information about the *local* neighborhood of a stationary point; they say nothing about it being a *global* minimum. Secondly, the assertion that \"no further checks are necessary\" is scientifically reckless. The positivity of eigenvalues does not eliminate potential numerical problems, such as poor SCF convergence or ill-conditioning in a different case. The statement that $\\mathbf{g}=\\mathbf{0}$ guarantees \"optimality\" is ambiguous and false if interpreted as global optimality.\n**Verdict: Incorrect**\n\n**C. Use determinant and trace as the decisive criteria: since $\\det(\\mathbf{H}) = 0.5\\times 1.2 \\times 3.0 = 1.8 > 0$ and $\\mathrm{tr}(\\mathbf{H}) = 0.5 + 1.2 + 3.0 = 4.7 > 0$, declare a local minimum; eigenvalue analysis, projection of external modes, and conditioning checks are unnecessary.**\nThis option's reasoning is mathematically insufficient. While for a symmetric $2 \\times 2$ matrix, a positive determinant and trace imply positive eigenvalues, this is not true for matrices of dimension $n > 2$. By Sylvester's criterion, a symmetric matrix is positive-definite if and only if all its leading principal minors are positive. For a $3 \\times 3$ matrix, one would need to check three determinants, not just the trace and the full determinant. The most direct and definitive method is eigenvalue analysis itself. Dismissing eigenvalue analysis and other crucial diagnostic checks as \"unnecessary\" is incorrect.\n**Verdict: Incorrect**\n\n**D. Reject the minimum because proper characterization requires exactly $6$ zero eigenvalues to account for overall translation and rotation in Cartesian coordinates; the absence of such zero eigenvalues indicates a numerically singular Hessian and precludes concluding minimality.**\nThis option demonstrates a critical misunderstanding of coordinate systems. The requirement for $6$ zero eigenvalues (for a nonlinear molecule) in the Hessian applies only when calculations are performed in the full set of $3N$ Cartesian coordinates. The problem explicitly states that a \"reduced set of internal coordinates\" is used, where \"overall translation and rotation [are] already projected out\". In such a coordinate system, which represents only the internal degrees of freedom ($3N-6$), a minimum is characterized by having *all* eigenvalues be positive. The absence of zero eigenvalues is not a flaw; it is the *expected* result for a minimum in this coordinate system.\n**Verdict: Incorrect**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}