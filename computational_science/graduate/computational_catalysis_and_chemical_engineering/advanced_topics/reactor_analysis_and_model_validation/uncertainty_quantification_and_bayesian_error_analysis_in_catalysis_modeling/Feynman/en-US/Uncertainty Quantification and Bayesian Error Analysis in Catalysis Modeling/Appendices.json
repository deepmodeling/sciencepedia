{
    "hands_on_practices": [
        {
            "introduction": "In computational catalysis, physical constraints on model parameters are non-negotiable. Rate constants, for example, must always be positive. This exercise  explores a mathematically elegant and widely used technique to enforce such positivity constraints: reparameterization. By setting a rate constant $k$ to be the exponential of an unconstrained variable, $k = \\exp(\\eta)$, and placing a standard Gaussian prior on $\\eta$, you will derive the implied prior distribution on $k$ and understand its fundamental properties.",
            "id": "3904099",
            "problem": "In microkinetic modeling (MKM) of heterogeneous catalysis, each elementary step is represented by a rate constant that must be strictly positive. To impose positivity while enabling uncertainty quantification (UQ) and Bayesian error analysis, consider the reparameterization $k=\\exp(\\eta)$, where $k$ is a rate constant and $\\eta$ is an unconstrained latent variable. Suppose prior epistemic uncertainty in $\\eta$ is represented by a Gaussian distribution, $\\eta \\sim \\mathcal{N}(\\mu_{\\eta}, \\sigma_{\\eta}^{2})$, reflecting additive model error on the logarithmic scale. Starting from the change-of-variables formula for probability densities and standard definitions of moments, derive the implied prior density for $k$ and closed-form expressions for the first two moments expressed in terms of $\\mu_{\\eta}$ and $\\sigma_{\\eta}^{2}$. Your derivation must begin from the transformation law for random variables and must not assume any pre-known form of the resulting distribution. Express your final results as analytic expressions for the prior density $p(k)$, the mean $\\mathbb{E}[k]$, and the variance $\\operatorname{Var}(k)$ in terms of $\\mu_{\\eta}$ and $\\sigma_{\\eta}^{2}$. The final answer must be written as a single row matrix containing these three expressions. No numerical evaluation is required.",
            "solution": "The problem requires the derivation of the probability density function (PDF), mean, and variance for a rate constant $k$, given that its logarithm, $\\eta = \\ln(k)$, follows a normal distribution $\\eta \\sim \\mathcal{N}(\\mu_{\\eta}, \\sigma_{\\eta}^{2})$.\n\n### Derivation of the Probability Density Function $p(k)$\n\nThe derivation begins with the probability density function for the normally distributed variable $\\eta$. Let $p_{\\eta}(\\eta')$ denote this PDF:\n$$p_{\\eta}(\\eta') = \\frac{1}{\\sigma_{\\eta}\\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\eta' - \\mu_{\\eta})^2}{2\\sigma_{\\eta}^2}\\right)$$\nThe transformation relating the random variables is $k = g(\\eta) = \\exp(\\eta)$, which is a one-to-one mapping from $\\eta \\in (-\\infty, \\infty)$ to $k \\in (0, \\infty)$. The inverse transformation is $\\eta = g^{-1}(k) = \\ln(k)$.\n\nThe change-of-variables formula for probability densities states that the PDF of $k$, denoted $p_k(k)$, is given by:\n$$p_k(k) = p_{\\eta}(g^{-1}(k)) \\left| \\frac{d}{dk}g^{-1}(k) \\right|$$\nFirst, we compute the derivative of the inverse transformation with respect to $k$:\n$$\\frac{d}{dk}g^{-1}(k) = \\frac{d}{dk}\\ln(k) = \\frac{1}{k}$$\nSince $k > 0$, the absolute value is $\\left| \\frac{1}{k} \\right| = \\frac{1}{k}$.\n\nSubstituting $g^{-1}(k) = \\ln(k)$ and the derivative into the formula:\n$$p_k(k) = p_{\\eta}(\\ln(k)) \\cdot \\frac{1}{k} = \\left[ \\frac{1}{\\sigma_{\\eta}\\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln(k) - \\mu_{\\eta})^2}{2\\sigma_{\\eta}^2}\\right) \\right] \\cdot \\frac{1}{k}$$\nThis yields the PDF of a log-normal distribution:\n$$p(k) = \\frac{1}{k\\sigma_{\\eta}\\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln(k) - \\mu_{\\eta})^2}{2\\sigma_{\\eta}^2}\\right) \\quad \\text{for } k > 0$$\n\n### Derivation of the Mean $\\mathbb{E}[k]$\n\nThe mean of $k$ is computed using the Law of the Unconscious Statistician:\n$$\\mathbb{E}[k] = \\mathbb{E}[\\exp(\\eta)] = \\int_{-\\infty}^{\\infty} \\exp(\\eta') p_{\\eta}(\\eta') \\, d\\eta' = \\int_{-\\infty}^{\\infty} \\exp(\\eta') \\frac{1}{\\sigma_{\\eta}\\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\eta' - \\mu_{\\eta})^2}{2\\sigma_{\\eta}^2}\\right) \\, d\\eta'$$\nCombining the exponential terms and completing the square in the exponent shows this integral is equivalent to finding the value of the moment-generating function (MGF) of the normal distribution, $M_{\\eta}(t) = \\exp(\\mu_{\\eta}t + \\frac{1}{2}\\sigma_{\\eta}^2t^2)$, evaluated at $t=1$.\nTherefore, the mean of $k$ is:\n$$\\mathbb{E}[k] = \\exp\\left(\\mu_{\\eta} + \\frac{\\sigma_{\\eta}^2}{2}\\right)$$\n\n### Derivation of the Variance $\\operatorname{Var}(k)$\n\nThe variance is $\\operatorname{Var}(k) = \\mathbb{E}[k^2] - (\\mathbb{E}[k])^2$. We first compute the second moment, $\\mathbb{E}[k^2] = \\mathbb{E}[\\exp(2\\eta)]$. This is equivalent to evaluating the MGF at $t=2$:\n$$\\mathbb{E}[k^2] = M_{\\eta}(2) = \\exp\\left(\\mu_{\\eta}(2) + \\frac{1}{2}\\sigma_{\\eta}^2(2)^2\\right) = \\exp(2\\mu_{\\eta} + 2\\sigma_{\\eta}^2)$$\nNow we compute the variance:\n$$\\operatorname{Var}(k) = \\mathbb{E}[k^2] - (\\mathbb{E}[k])^2 = \\exp(2\\mu_{\\eta} + 2\\sigma_{\\eta}^2) - \\left[\\exp\\left(\\mu_{\\eta} + \\frac{\\sigma_{\\eta}^2}{2}\\right)\\right]^2$$\n$$\\operatorname{Var}(k) = \\exp(2\\mu_{\\eta} + 2\\sigma_{\\eta}^2) - \\exp\\left(2\\mu_{\\eta} + \\sigma_{\\eta}^2\\right)$$\nFactoring out the common term gives the final expression:\n$$\\operatorname{Var}(k) = \\left(\\exp(\\sigma_{\\eta}^2) - 1\\right) \\exp(2\\mu_{\\eta} + \\sigma_{\\eta}^2)$$\n\nThe three derived expressions are:\n1.  Prior density $p(k) = \\frac{1}{k\\sigma_{\\eta}\\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln(k) - \\mu_{\\eta})^2}{2\\sigma_{\\eta}^2}\\right)$\n2.  Mean $\\mathbb{E}[k] = \\exp\\left(\\mu_{\\eta} + \\frac{\\sigma_{\\eta}^2}{2}\\right)$\n3.  Variance $\\operatorname{Var}(k) = \\left(\\exp(\\sigma_{\\eta}^2) - 1\\right) \\exp(2\\mu_{\\eta} + \\sigma_{\\eta}^2)$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{k\\sigma_{\\eta}\\sqrt{2\\pi}} \\exp\\left(-\\frac{\\left(\\ln(k) - \\mu_{\\eta}\\right)^2}{2\\sigma_{\\eta}^2}\\right) & \\exp\\left(\\mu_{\\eta} + \\frac{\\sigma_{\\eta}^2}{2}\\right) & \\left(\\exp(\\sigma_{\\eta}^2) - 1\\right) \\exp\\left(2\\mu_{\\eta} + \\sigma_{\\eta}^2\\right)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Determining Arrhenius parameters—the activation energy $E_a$ and the pre-exponential factor $A$—is a central task in kinetic modeling. This practice  demonstrates how to tackle this problem within a complete Bayesian framework. By transforming the Arrhenius equation into a linear form, we can leverage the power of Bayesian linear regression to analytically derive the posterior distribution of the parameters, perfectly blending prior knowledge with experimental evidence in a mathematically tractable case.",
            "id": "3904162",
            "problem": "Consider a microkinetic model of a single elementary surface reaction step whose temperature dependence follows the Arrhenius law. Let the measured turnover frequency be denoted by $r_i$ at temperature $T_i$ for $i=1,\\dots,N$, with the Arrhenius form $r_i = A \\exp\\!\\left(-E_a/(R T_i)\\right)$, where $E_a$ is the activation energy, $A$ is the pre-exponential factor, and $R$ is the universal gas constant. Define the transformed observations $y_i = \\ln(r_i)$, and model the measurement error as additive, independent and identically distributed (i.i.d.) Gaussian noise: $y_i = \\ln A - E_a/(R T_i) + \\epsilon_i$, with $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ and known noise variance $\\sigma^2$.\n\nLet the parameter vector be $\\theta = \\begin{pmatrix}E_a \\\\ \\ln A\\end{pmatrix}$. Assume a Gaussian prior $\\theta \\sim \\mathcal{N}(\\mu_0,\\Sigma_0)$ with prior mean $\\mu_0 = \\begin{pmatrix}m_1 \\\\ m_2\\end{pmatrix}$ and prior covariance $\\Sigma_0$ that is symmetric positive definite, and denote its precision (inverse covariance) as $P_0 = \\Sigma_0^{-1} = \\begin{pmatrix}p_{11} & p_{12} \\\\ p_{12} & p_{22}\\end{pmatrix}$. Define the design matrix $X \\in \\mathbb{R}^{N \\times 2}$ by rows $x_i^{\\top} = \\begin{pmatrix}-\\frac{1}{R T_i} & 1\\end{pmatrix}$ so that $y = X \\theta + \\epsilon$, with $y = \\begin{pmatrix}y_1 \\\\ \\vdots \\\\ y_N\\end{pmatrix}$ and $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2 I)$.\n\nStarting from Bayes’ theorem and the definitions above, do the following:\n- Derive the negative log-posterior up to an additive constant and compute its gradient and Hessian with respect to $\\theta$.\n- Solve for the posterior mode $\\widehat{\\theta}$ that minimizes the negative log-posterior.\n- Using the Laplace approximation (second-order Taylor expansion about the mode), obtain a Gaussian approximation to $p(\\theta \\mid D)$, where $D=\\{(T_i,r_i)\\}_{i=1}^N$ is the dataset, and identify the approximate posterior mean vector and covariance matrix.\n\nTo express your result compactly, define the following summary statistics:\n- $S_{11} = \\sum_{i=1}^{N} \\left(\\frac{1}{R T_i}\\right)^2$, $S_{22} = \\sum_{i=1}^{N} 1 = N$, and $S_{12} = \\sum_{i=1}^{N} \\left(-\\frac{1}{R T_i}\\right)$,\n- $U_1 = \\sum_{i=1}^{N} \\left(-\\frac{1}{R T_i}\\right) y_i$ and $U_2 = \\sum_{i=1}^{N} y_i$,\n- $H = P_0 + \\frac{1}{\\sigma^2} \\begin{pmatrix} S_{11} & S_{12} \\\\ S_{12} & S_{22} \\end{pmatrix}$ with entries $h_{11}$, $h_{12}$, $h_{22}$,\n- $b = P_0 \\mu_0 + \\frac{1}{\\sigma^2} \\begin{pmatrix} U_1 \\\\ U_2 \\end{pmatrix}$ with entries $b_1$ and $b_2$,\n- $\\Delta = h_{11} h_{22} - h_{12}^2$.\n\nReport your final result as the five-entry row matrix containing the two components of the posterior mode and the three unique entries of the approximate posterior covariance matrix: $\\left(\\widehat{E}_a, \\widehat{\\ln A}, \\Sigma_{11}, \\Sigma_{12}, \\Sigma_{22}\\right)$, where $\\Sigma = H^{-1}$. Express the final answer as a closed-form analytical expression in terms of the quantities defined above. Do not include units in your final answer; however, use $E_a$ in joules per mole, $T$ in kelvin, $R$ in joules per mole per kelvin, and treat $\\ln A$ as dimensionless throughout your derivation.",
            "solution": "The goal is to find the posterior distribution for the parameter vector $\\theta = \\begin{pmatrix} E_a \\\\ \\ln A \\end{pmatrix}$ given a set of measurements $D = \\{(T_i, r_i)\\}_{i=1}^N$. According to Bayes' theorem, the posterior probability density function $p(\\theta \\mid D)$ is proportional to the product of the likelihood $p(D \\mid \\theta)$ and the prior $p(\\theta)$:\n$$p(\\theta \\mid D) \\propto p(D \\mid \\theta) p(\\theta)$$\nThe model is given in a linearized form $y = X \\theta + \\epsilon$, where $y_i = \\ln(r_i)$ and the noise is Gaussian, $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$. The likelihood is therefore:\n$$p(D \\mid \\theta) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2} (y - X\\theta)^{\\top}(y - X\\theta) \\right)$$\nThe prior is a multivariate Gaussian, $\\theta \\sim \\mathcal{N}(\\mu_0, \\Sigma_0)$, with precision $P_0 = \\Sigma_0^{-1}$:\n$$p(\\theta) \\propto \\exp\\left( -\\frac{1}{2} (\\theta - \\mu_0)^{\\top}P_0(\\theta - \\mu_0) \\right)$$\nThe negative log-posterior, $L(\\theta) = -\\ln p(\\theta \\mid D)$, is quadratic in $\\theta$ (ignoring constants):\n$$L(\\theta) = \\frac{1}{2\\sigma^2} (y - X\\theta)^{\\top}(y - X\\theta) + \\frac{1}{2} (\\theta - \\mu_0)^{\\top}P_0(\\theta - \\mu_0)$$\nExpanding the terms and collecting powers of $\\theta$ yields:\n$$L(\\theta) = \\frac{1}{2} \\theta^{\\top} \\left( P_0 + \\frac{1}{\\sigma^2}X^{\\top}X \\right) \\theta - \\theta^{\\top} \\left( P_0\\mu_0 + \\frac{1}{\\sigma^2}X^{\\top}y \\right)$$\nThis quadratic form implies the posterior is a Gaussian distribution. We identify its parameters by finding the minimum of $L(\\theta)$, which corresponds to the posterior mode (and mean). The gradient of $L(\\theta)$ with respect to $\\theta$ is:\n$$\\nabla_{\\theta} L(\\theta) = \\left( P_0 + \\frac{1}{\\sigma^2}X^{\\top}X \\right) \\theta - \\left( P_0\\mu_0 + \\frac{1}{\\sigma^2}X^{\\top}y \\right)$$\nUsing the summary statistics defined in the problem, this becomes $\\nabla_{\\theta} L(\\theta) = H\\theta - b$. Setting the gradient to zero gives the posterior mean:\n$$\\widehat{\\theta} = H^{-1} b$$\nThe Hessian of the negative log-posterior is $\\nabla_{\\theta}^2 L(\\theta) = H$. The posterior covariance matrix is the inverse of the Hessian:\n$$\\Sigma_{\\text{post}} = H^{-1}$$\nGiven $\\Sigma = H^{-1}$ and $\\Delta = \\det(H) = h_{11}h_{22} - h_{12}^2$, the inverse of the $2 \\times 2$ matrix $H$ is:\n$$\\Sigma = \\frac{1}{\\Delta} \\begin{pmatrix} h_{22} & -h_{12} \\\\ -h_{12} & h_{11} \\end{pmatrix}$$\nThe components of the posterior covariance matrix $\\Sigma$ are therefore:\n$$\\Sigma_{11} = \\frac{h_{22}}{\\Delta}, \\quad \\Sigma_{12} = -\\frac{h_{12}}{\\Delta}, \\quad \\Sigma_{22} = \\frac{h_{11}}{\\Delta}$$\nThe posterior mean vector is $\\widehat{\\theta} = \\begin{pmatrix} \\widehat{E}_a \\\\ \\widehat{\\ln A} \\end{pmatrix} = H^{-1} b$:\n$$\\begin{pmatrix} \\widehat{E}_a \\\\ \\widehat{\\ln A} \\end{pmatrix} = \\frac{1}{\\Delta} \\begin{pmatrix} h_{22} & -h_{12} \\\\ -h_{12} & h_{11} \\end{pmatrix} \\begin{pmatrix} b_1 \\\\ b_2 \\end{pmatrix} = \\frac{1}{\\Delta} \\begin{pmatrix} h_{22}b_1 - h_{12}b_2 \\\\ h_{11}b_2 - h_{12}b_1 \\end{pmatrix}$$\nThe components of the posterior mode are:\n$$\\widehat{E}_a = \\frac{h_{22}b_1 - h_{12}b_2}{\\Delta}$$\n$$\\widehat{\\ln A} = \\frac{h_{11}b_2 - h_{12}b_1}{\\Delta}$$\nThe final result is the row matrix $(\\widehat{E}_a, \\widehat{\\ln A}, \\Sigma_{11}, \\Sigma_{12}, \\Sigma_{22})$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{h_{22} b_1 - h_{12} b_2}{\\Delta} & \\frac{h_{11} b_2 - h_{12} b_1}{\\Delta} & \\frac{h_{22}}{\\Delta} & -\\frac{h_{12}}{\\Delta} & \\frac{h_{11}}{\\Delta}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Real experimental data is rarely as clean as our models assume; occasional outliers or experimental failures can corrupt a dataset and bias parameter estimates. This exercise  introduces a robust method for handling such imperfections: the mixture error model. You will explore how to formulate a model that explicitly accounts for a small probability of large-error \"contamination\" events, a critical step toward building Bayesian models that are resilient to the complexities of real-world data.",
            "id": "3904125",
            "problem": "A catalytic microkinetic model with parameters $\\boldsymbol{\\theta}$ predicts reaction rates $m_i(\\boldsymbol{\\theta})$ at experimental conditions indexed by $i \\in \\{1,\\dots,N\\}$. Measurements $y_i$ of rates are subject to occasional rare experimental failures (e.g., transient poisoning, miscalibration), which produce unusually large deviations. Define residuals $r_i = y_i - m_i(\\boldsymbol{\\theta})$. To robustly quantify uncertainty and perform Bayesian error analysis, consider a contamination-style mixture error model: with probability $(1-\\epsilon)$, $r_i$ is drawn from a baseline measurement noise distribution, and with probability $\\epsilon$, $r_i$ is drawn from a higher-variance failure distribution. Assume the baseline noise is Gaussian with variance $\\sigma^2$, the failure component is Gaussian with variance $\\kappa\\sigma^2$ where $\\kappa \\gg 1$ is known and fixed, and all residuals $\\{r_i\\}_{i=1}^N$ are conditionally independent given parameters. Let $\\epsilon \\in (0,1)$ denote the mixture weight for the failure component, and let $\\sigma > 0$ denote the baseline noise scale. A Bayesian analysis places a prior on $\\epsilon$ and possibly on $\\sigma$; a convenient hierarchical representation introduces latent failure indicators $z_i \\in \\{0,1\\}$, where $z_i=1$ indicates the failure component was responsible for $r_i$.\n\nFrom first principles (definition of mixture distributions, independence of observations, and Bayes' theorem), determine which of the following statements are correct about the likelihood, posterior structure, decision boundaries, and identifiability under limited data:\n\nA. The likelihood of the data $\\{r_i\\}_{i=1}^N$ under the mixture error model with parameters $(\\epsilon,\\sigma)$ is $\\prod_{i=1}^N\\left[(1-\\epsilon)\\,\\phi_{\\sigma}(r_i) + \\epsilon\\,\\phi_{\\sqrt{\\kappa}\\sigma}(r_i)\\right]$, where $\\phi_{\\sigma}(r) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\!\\left(-\\frac{r^2}{2\\sigma^2}\\right)$.\n\nB. With a prior $\\epsilon \\sim \\text{Beta}(a,b)$ and no latent variables, the posterior for $\\epsilon$ is exactly $\\text{Beta}(a+k,b+N-k)$, where $k$ is the count of residuals satisfying $|r_i| > 3\\sigma$.\n\nC. If latent indicators $\\{z_i\\}$ are introduced with $z_i \\sim \\text{Bernoulli}(\\epsilon)$ independently, and $\\epsilon \\sim \\text{Beta}(a,b)$, then the conditional posterior of $\\epsilon$ given $\\{z_i\\}$ and $\\{r_i\\}$ is $\\text{Beta}\\!\\left(a+\\sum_{i=1}^N z_i,\\; b+N-\\sum_{i=1}^N z_i\\right)$.\n\nD. For fixed $(\\epsilon,\\sigma,\\kappa)$, the boundary in $|r_i|$ at which the posterior responsibility for the failure component equals that of the baseline component (i.e., $\\mathbb{P}(z_i=1 \\mid r_i,\\epsilon,\\sigma,\\kappa) = \\mathbb{P}(z_i=0 \\mid r_i,\\epsilon,\\sigma,\\kappa) = \\tfrac{1}{2}$) is\n$$\n|r_i| \\;=\\; \\sigma\\,\\sqrt{\\,2\\,\\frac{\\kappa}{\\kappa - 1}\\,\\ln\\!\\left(\\frac{1-\\epsilon}{\\epsilon}\\,\\sqrt{\\kappa}\\right)\\,}\\,.\n$$\n\nE. If both $\\epsilon$ and $\\sigma$ are unknown with weakly informative priors and $\\kappa$ is large and fixed, then with limited data and no apparent large residuals, the posterior can exhibit a pronounced ridge in $(\\epsilon,\\sigma)$ where increases in $\\epsilon$ are offset by decreases in $\\sigma$, leading to practical non-identifiability of $\\epsilon$ unless constraints or informative priors are imposed.\n\nF. As $\\kappa \\to \\infty$, the marginal residual distribution of $r_i$ under the contamination mixture converges to a Student’s $t$ distribution with degrees of freedom $2$.\n\nSelect all correct options.",
            "solution": "This problem requires analyzing several statements about a contamination-style mixture error model. Let the PDF of a zero-mean Gaussian with standard deviation $s$ be $\\phi_s(x)$. The baseline noise distribution is $\\mathcal{N}(0, \\sigma^2)$ with PDF $\\phi_{\\sigma}(r_i)$, and the failure component is $\\mathcal{N}(0, \\kappa\\sigma^2)$ with PDF $\\phi_{\\sqrt{\\kappa}\\sigma}(r_i)$.\n\n**A. Likelihood Function**\nThe marginal PDF of a single residual $r_i$ is a mixture:\n$$p(r_i | \\epsilon, \\sigma, \\kappa) = (1-\\epsilon) \\phi_{\\sigma}(r_i) + \\epsilon \\phi_{\\sqrt{\\kappa}\\sigma}(r_i)$$\nSince the residuals $\\{r_i\\}_{i=1}^N$ are conditionally independent, the total likelihood is the product of the individual probability densities:\n$$L(\\epsilon, \\sigma, \\kappa \\mid \\{r_i\\}) = \\prod_{i=1}^N p(r_i | \\epsilon, \\sigma, \\kappa) = \\prod_{i=1}^N \\left[ (1-\\epsilon) \\phi_{\\sigma}(r_i) + \\epsilon \\phi_{\\sqrt{\\kappa}\\sigma}(r_i) \\right]$$\nThe statement is **correct**.\n\n**B. Posterior for $\\epsilon$ without latent variables**\nThe posterior $p(\\epsilon | \\{r_i\\}) \\propto p(\\{r_i\\} | \\epsilon) p(\\epsilon)$. The likelihood, as shown in A, is a polynomial in $\\epsilon$ of degree $N$. When multiplied by the Beta prior, the resulting posterior is not a standard Beta distribution. The Beta distribution is conjugate to the Bernoulli/Binomial likelihood, which does not apply here. A hard thresholding rule is a heuristic approximation, not an exact Bayesian update. The statement is **incorrect**.\n\n**C. Conditional posterior for $\\epsilon$ with latent variables**\nIntroducing latent indicators $z_i \\sim \\text{Bernoulli}(\\epsilon)$, the joint likelihood of data and indicators is $p(\\{r_i\\}, \\{z_i\\} | \\epsilon) = p(\\{r_i\\} | \\{z_i\\}) p(\\{z_i\\} | \\epsilon)$. The term $p(\\{r_i\\} | \\{z_i\\})$ does not depend on $\\epsilon$. The posterior for $\\epsilon$ is therefore $p(\\epsilon | \\{z_i\\}, \\dots) \\propto p(\\{z_i\\} | \\epsilon) p(\\epsilon)$.\nThe likelihood of the indicators is Binomial: $p(\\{z_i\\} | \\epsilon) = \\prod_i \\epsilon^{z_i} (1-\\epsilon)^{1-z_i} = \\epsilon^{\\sum z_i} (1-\\epsilon)^{N-\\sum z_i}$.\nWith a prior $\\epsilon \\sim \\text{Beta}(a,b)$, the posterior is:\n$$p(\\epsilon | \\{z_i\\}) \\propto \\epsilon^{\\sum z_i} (1-\\epsilon)^{N-\\sum z_i} \\cdot \\epsilon^{a-1}(1-\\epsilon)^{b-1} = \\epsilon^{a + \\sum z_i - 1} (1-\\epsilon)^{b + N - \\sum z_i - 1}$$\nThis is the kernel of a $\\text{Beta}(a+\\sum z_i, b+N-\\sum z_i)$ distribution. The statement is **correct**.\n\n**D. Decision Boundary**\nThe posterior responsibilities are equal when $p(r_i|z_i=1)P(z_i=1) = p(r_i|z_i=0)P(z_i=0)$.\n$$\\phi_{\\sqrt{\\kappa}\\sigma}(r_i) \\cdot \\epsilon = \\phi_{\\sigma}(r_i) \\cdot (1-\\epsilon)$$\n$$\\frac{\\epsilon}{\\sqrt{2\\pi\\kappa}\\sigma}\\exp\\left(-\\frac{r_i^2}{2\\kappa\\sigma^2}\\right) = \\frac{1-\\epsilon}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{r_i^2}{2\\sigma^2}\\right)$$\nSolving for $r_i^2$:\n$$\\frac{\\epsilon}{\\sqrt{\\kappa}} = (1-\\epsilon)\\exp\\left(\\frac{r_i^2}{2\\kappa\\sigma^2} - \\frac{r_i^2}{2\\sigma^2}\\right) = (1-\\epsilon)\\exp\\left(-\\frac{r_i^2}{2\\sigma^2}\\frac{\\kappa-1}{\\kappa}\\right)$$\n$$\\ln\\left(\\frac{\\epsilon}{(1-\\epsilon)\\sqrt{\\kappa}}\\right) = -\\frac{r_i^2}{2\\sigma^2}\\frac{\\kappa-1}{\\kappa}$$\n$$r_i^2 = -2\\sigma^2\\frac{\\kappa}{\\kappa-1}\\ln\\left(\\frac{\\epsilon}{(1-\\epsilon)\\sqrt{\\kappa}}\\right) = 2\\sigma^2\\frac{\\kappa}{\\kappa-1}\\ln\\left(\\frac{(1-\\epsilon)\\sqrt{\\kappa}}{\\epsilon}\\right)$$\nTaking the square root gives the expression in the statement. The statement is **correct**.\n\n**E. Identifiability**\nWhen data lacks clear outliers, the model can struggle to distinguish between the two mixture components. The observed variance of the data, $S^2$, can be explained by multiple combinations of $(\\epsilon, \\sigma)$. The mixture variance is $\\text{Var}(r_i) = \\sigma^2(1-\\epsilon+\\epsilon\\kappa)$. The posterior will be high for pairs $(\\epsilon, \\sigma)$ that satisfy $\\sigma^2 \\approx S^2 / (1+\\epsilon(\\kappa-1))$. This creates a ridge of high posterior density where an increase in $\\epsilon$ is compensated by a decrease in $\\sigma$, leading to practical non-identifiability. The statement is **correct**.\n\n**F. Convergence to Student's t distribution**\nA finite mixture of Gaussians will always have tails dominated by its widest Gaussian component, meaning the log-probability decays quadratically ($-\\ln p(x) \\propto x^2$). A Student's t-distribution has heavier, power-law tails ($-\\ln p(x) \\propto \\ln(x)$). The functional forms are fundamentally different, and taking $\\kappa \\to \\infty$ does not transform one into the other. This statement is **incorrect**.",
            "answer": "$$\\boxed{ACDE}$$"
        }
    ]
}