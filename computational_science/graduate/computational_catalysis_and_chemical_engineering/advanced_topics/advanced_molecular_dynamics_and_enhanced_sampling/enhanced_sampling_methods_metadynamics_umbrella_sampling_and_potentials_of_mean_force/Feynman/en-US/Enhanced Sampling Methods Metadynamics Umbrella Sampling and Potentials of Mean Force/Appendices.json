{
    "hands_on_practices": [
        {
            "introduction": "A successful umbrella sampling study hinges on ensuring adequate overlap between the statistical distributions of adjacent simulation windows. Without this overlap, reconstructing a continuous Potential of Mean Force (PMF) is impossible. This exercise provides hands-on practice in quantifying this crucial overlap by deriving two common metrics—the Bhattacharyya coefficient and the Kullback-Leibler divergence—and using them to select an appropriate harmonic force constant, $k$, for a simulation. ",
            "id": "3879960",
            "problem": "Consider umbrella sampling of a one-dimensional collective variable $s$ describing the reaction coordinate of a surface-catalyzed elementary step. You plan to deploy $M$ harmonic bias windows with identical force constants and evenly spaced centers to sample the potential of mean force (PMF). The bias in window $i$ is $V_{i}(s)=\\frac{1}{2}k\\,(s-s_{i})^{2}$, and the centers $\\{s_{i}\\}$ tile the interval $[0,L]$ uniformly. Assume the unbiased PMF is locally flat over the scale of each window so that the biased canonical distribution in each window is dominated by the harmonic term.\n\nStarting from the Boltzmann distribution and the definition of a harmonic potential, derive in full detail the normalized form of the biased distribution $p_{i}(s)$ in window $i$. Using this, derive from first principles the Bhattacharyya coefficient (BC) between neighboring windows $i$ and $i+1$, defined as $\\mathrm{BC}=\\int_{-\\infty}^{\\infty}\\sqrt{p_{i}(s)\\,p_{i+1}(s)}\\,ds$, and the Kullback–Leibler divergence (KLD) $D_{\\mathrm{KL}}(p_{i}\\Vert p_{i+1})=\\int_{-\\infty}^{\\infty}p_{i}(s)\\,\\ln\\!\\big(p_{i}(s)/p_{i+1}(s)\\big)\\,ds$, for the case in which the neighboring distributions are Gaussian with equal variance.\n\nExplain how these overlap measures constrain the choice of the force constant $k$ and center spacing $\\Delta=s_{i+1}-s_{i}$ to achieve a specified target overlap between neighboring windows. Then, using the Bhattacharyya coefficient as the overlap metric, compute the single value of the force constant $k$ required to attain a target $\\mathrm{BC}=b_{0}$ between all neighboring windows when the centers are evenly spaced. Use the following parameters: $L=1.2\\,\\text{nm}$, $M=9$ windows, $b_{0}=0.6$, and $T=300\\,\\text{K}$. Take the ideal gas constant to be $R=8.314462618\\times 10^{-3}\\,\\text{kJ mol}^{-1}\\text{K}^{-1}$. Round your final numerical answer to four significant figures. Express the stiffness $k$ in $\\text{kJ mol}^{-1}\\text{nm}^{-2}$.",
            "solution": "The analysis begins with the principles of statistical mechanics applied to the biased system. The total potential energy in window $i$, denoted $W_i(s)$, is the sum of the unbiased potential of mean force (PMF), $W(s)$, and the harmonic bias potential, $V_i(s)$.\n$$W_{i}(s) = W(s) + V_{i}(s) = W(s) + \\frac{1}{2}k(s-s_{i})^{2}$$\nAccording to the Boltzmann distribution, the probability density function $p_i(s)$ for the collective variable $s$ in window $i$ is proportional to the exponential of the negative potential energy, scaled by the thermal energy. Let $\\beta = (RT)^{-1}$ where $R$ is the ideal gas constant and $T$ is the temperature.\n$$p_i(s) \\propto \\exp(-\\beta W_i(s)) = \\exp\\left(-\\beta \\left(W(s) + \\frac{1}{2}k(s-s_i)^2\\right)\\right)$$\nThe problem states the assumption that the unbiased PMF, $W(s)$, is locally flat. This implies that over the range of $s$ where the Gaussian term $\\exp(-\\frac{1}{2}\\beta k(s-s_i)^2)$ is significant, $W(s)$ can be approximated as a constant, $W(s) \\approx C_i$. This constant can be absorbed into the normalization constant.\n$$p_i(s) \\propto \\exp(-\\beta C_i) \\exp\\left(-\\frac{\\beta k}{2}(s-s_i)^2\\right) \\propto \\exp\\left(-\\frac{\\beta k}{2}(s-s_i)^2\\right)$$\nThis is the functional form of a Gaussian distribution. To find the normalized distribution, we must evaluate the normalization constant $Z_i = \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{\\beta k}{2}(s-s_i)^2\\right) ds$. This is a standard Gaussian integral of the form $\\int_{-\\infty}^{\\infty} \\exp(-ax^2)dx = \\sqrt{\\pi/a}$. With the substitution $x = s-s_i$ and $a = \\frac{\\beta k}{2}$, the integral evaluates to:\n$$Z_i = \\sqrt{\\frac{\\pi}{\\frac{\\beta k}{2}}} = \\sqrt{\\frac{2\\pi}{\\beta k}}$$\nThe normalized biased distribution $p_i(s)$ is therefore:\n$$p_i(s) = \\frac{1}{Z_i} \\exp\\left(-\\frac{\\beta k}{2}(s-s_i)^2\\right) = \\sqrt{\\frac{\\beta k}{2\\pi}} \\exp\\left(-\\frac{\\beta k}{2}(s-s_i)^2\\right)$$\nThis is a normal distribution $N(s|\\mu_i, \\sigma^2)$ with mean $\\mu_i = s_i$ and variance $\\sigma^2 = (\\beta k)^{-1}$.\n\nNext, we derive the Bhattacharyya coefficient (BC) between two neighboring distributions, $p_i(s)$ and $p_{i+1}(s)$. These are two Gaussian distributions with the same variance $\\sigma^2$ and means $s_i$ and $s_{i+1}$, respectively. Let $\\Delta = s_{i+1}-s_i$ be the spacing between centers.\n$$\\mathrm{BC} = \\int_{-\\infty}^{\\infty}\\sqrt{p_{i}(s)p_{i+1}(s)}\\,ds$$\nThe product under the square root is:\n$$p_{i}(s)p_{i+1}(s) = \\left(\\sqrt{\\frac{1}{2\\pi\\sigma^2}}\\right)^2 \\exp\\left(-\\frac{(s-s_i)^2}{2\\sigma^2}\\right) \\exp\\left(-\\frac{(s-s_{i+1})^2}{2\\sigma^2}\\right) = \\frac{1}{2\\pi\\sigma^2}\\exp\\left(-\\frac{(s-s_i)^2 + (s-s_{i+1})^2}{2\\sigma^2}\\right)$$\nTaking the square root:\n$$\\sqrt{p_{i}(s)p_{i+1}(s)} = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(s-s_i)^2 + (s-s_{i+1})^2}{4\\sigma^2}\\right)$$\nWe complete the square for the term in the exponent. Let $\\bar{s} = \\frac{s_i+s_{i+1}}{2}$ be the midpoint.\n$$(s-s_i)^2 + (s-s_{i+1})^2 = 2s^2 - 2s(s_i+s_{i+1}) + s_i^2 + s_{i+1}^2 = 2\\left(s^2 - 2s\\bar{s}\\right) + s_i^2+s_{i+1}^2$$\n$$= 2\\left((s-\\bar{s})^2 - \\bar{s}^2\\right) + s_i^2+s_{i+1}^2 = 2(s-\\bar{s})^2 - 2\\bar{s}^2 + s_i^2+s_{i+1}^2$$\nThe constant term simplifies to:\n$$-2\\left(\\frac{s_i+s_{i+1}}{2}\\right)^2 + s_i^2+s_{i+1}^2 = -\\frac{1}{2}(s_i^2+2s_is_{i+1}+s_{i+1}^2) + s_i^2+s_{i+1}^2 = \\frac{1}{2}(s_i^2-2s_is_{i+1}+s_{i+1}^2) = \\frac{1}{2}(s_{i+1}-s_i)^2 = \\frac{\\Delta^2}{2}$$\nSo the exponent term becomes $2(s-\\bar{s})^2 + \\frac{\\Delta^2}{2}$. The argument of the exponential is $-\\frac{2(s-\\bar{s})^2 + \\Delta^2/2}{4\\sigma^2} = -\\frac{(s-\\bar{s})^2}{2\\sigma^2} - \\frac{\\Delta^2}{8\\sigma^2}$.\nThe integral for the BC is:\n$$\\mathrm{BC} = \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{\\Delta^2}{8\\sigma^2}\\right) \\exp\\left(-\\frac{(s-\\bar{s})^2}{2\\sigma^2}\\right) ds$$\nThe term $\\exp(-\\frac{\\Delta^2}{8\\sigma^2})$ is a constant with respect to $s$. The remaining integral is that of a normalized Gaussian distribution with mean $\\bar{s}$ and variance $\\sigma^2$, which evaluates to $1$.\n$$\\mathrm{BC} = \\exp\\left(-\\frac{\\Delta^2}{8\\sigma^2}\\right)$$\nSubstituting $\\sigma^2 = (\\beta k)^{-1}$, we obtain the final expression for the BC:\n$$\\mathrm{BC} = \\exp\\left(-\\frac{\\beta k \\Delta^2}{8}\\right)$$\n\nNow, we derive the Kullback–Leibler divergence (KLD), $D_{\\mathrm{KL}}(p_{i}\\Vert p_{i+1})$.\n$$D_{\\mathrm{KL}}(p_{i}\\Vert p_{i+1})=\\int_{-\\infty}^{\\infty}p_{i}(s)\\ln\\left(\\frac{p_{i}(s)}{p_{i+1}(s)}\\right)ds$$\nThe logarithmic term is:\n$$\\ln\\left(\\frac{p_{i}(s)}{p_{i+1}(s)}\\right) = \\ln\\left(\\frac{\\exp(-(s-s_i)^2/(2\\sigma^2))}{\\exp(-(s-s_{i+1})^2/(2\\sigma^2))}\\right) = \\frac{1}{2\\sigma^2}\\left((s-s_{i+1})^2 - (s-s_i)^2\\right)$$\n$$= \\frac{1}{2\\sigma^2}(s^2-2ss_{i+1}+s_{i+1}^2 - (s^2-2ss_i+s_i^2)) = \\frac{1}{2\\sigma^2}(2s(s_i-s_{i+1}) + s_{i+1}^2-s_i^2)$$\n$$= \\frac{1}{2\\sigma^2}(-2s\\Delta + (s_{i+1}-s_i)(s_{i+1}+s_i)) = \\frac{\\Delta}{2\\sigma^2}(-2s + s_i+s_{i+1})$$\nThe KLD is the expectation of this quantity with respect to $p_i(s)$:\n$$D_{\\mathrm{KL}}(p_{i}\\Vert p_{i+1}) = E_{p_i}\\left[\\frac{\\Delta}{2\\sigma^2}(-2s + s_i+s_{i+1})\\right] = \\frac{\\Delta}{2\\sigma^2}\\left(-2E_{p_i}[s] + s_i+s_{i+1}\\right)$$\nThe expectation of $s$ under $p_i(s)$ is the mean of the distribution, $E_{p_i}[s] = s_i$.\n$$D_{\\mathrm{KL}}(p_{i}\\Vert p_{i+1}) = \\frac{\\Delta}{2\\sigma^2}(-2s_i + s_i+s_{i+1}) = \\frac{\\Delta}{2\\sigma^2}(s_{i+1}-s_i) = \\frac{\\Delta^2}{2\\sigma^2}$$\nSubstituting $\\sigma^2 = (\\beta k)^{-1}$:\n$$D_{\\mathrm{KL}}(p_{i}\\Vert p_{i+1}) = \\frac{\\beta k \\Delta^2}{2}$$\n\nThese overlap measures constrain the choice of simulation parameters. Both BC and KLD depend only on the dimensionless group $\\beta k \\Delta^2$. To ensure adequate sampling and efficient use of computational resources, there must be sufficient but not excessive overlap between adjacent windows. A target overlap, specified by a value for BC or KLD, fixes the value of $\\beta k \\Delta^2$. For a given temperature $T$ and a chosen window spacing $\\Delta$, this determines the required force constant $k$. High $k$ or $\\Delta$ leads to low overlap (BC $\\to 0$, KLD $\\to \\infty$), making PMF reconstruction difficult. Low $k$ or $\\Delta$ leads to high overlap (BC $\\to 1$, KLD $\\to 0$), which is computationally inefficient.\n\nFinally, we compute the force constant $k$ for the given parameters. The $M=9$ windows tile the interval $[0,L]$ uniformly. This implies the centers are at $s_i = i \\cdot \\Delta$ for $i \\in \\{0, 1, ..., 8\\}$, with $s_0=0$ and $s_8=L$. The spacing $\\Delta$ is:\n$$\\Delta = \\frac{L}{M-1} = \\frac{1.2\\,\\text{nm}}{9-1} = \\frac{1.2\\,\\text{nm}}{8} = 0.15\\,\\text{nm}$$\nThe target Bhattacharyya coefficient is $b_0 = 0.6$. From the derived formula, we have:\n$$b_0 = \\exp\\left(-\\frac{\\beta k \\Delta^2}{8}\\right)$$\nSolving for $k$:\n$$\\ln(b_0) = -\\frac{\\beta k \\Delta^2}{8} \\implies k = -\\frac{8 \\ln(b_0)}{\\beta \\Delta^2} = -8RT \\frac{\\ln(b_0)}{\\Delta^2}$$\nWe are given $T=300\\,\\text{K}$ and $R=8.314462618\\times 10^{-3}\\,\\text{kJ mol}^{-1}\\text{K}^{-1}$.\n$$RT = (8.314462618\\times 10^{-3}\\,\\text{kJ mol}^{-1}\\text{K}^{-1}) \\times (300\\,\\text{K}) \\approx 2.494339\\,\\text{kJ mol}^{-1}$$\nNow, we substitute the values into the expression for $k$:\n$$k = -8 \\times (2.494339\\,\\text{kJ mol}^{-1}) \\frac{\\ln(0.6)}{(0.15\\,\\text{nm})^2}$$\n$$k \\approx -19.95471\\,\\text{kJ mol}^{-1} \\frac{-0.5108256}{0.0225\\,\\text{nm}^2}$$\n$$k \\approx \\frac{10.1935}{0.0225}\\,\\text{kJ mol}^{-1}\\text{nm}^{-2} \\approx 452.991\\,\\text{kJ mol}^{-1}\\text{nm}^{-2}$$\nRounding to four significant figures, the required force constant is $453.0\\,\\text{kJ mol}^{-1}\\text{nm}^{-2}$.",
            "answer": "$$\\boxed{453.0}$$"
        },
        {
            "introduction": "Unlike the static potentials used in umbrella sampling, well-tempered metadynamics employs a dynamic, history-dependent bias that ensures eventual convergence. The key to this method is the bias factor, $\\gamma$, which controls how the simulation temperature is effectively raised to overcome barriers. This problem challenges you to connect the theoretical underpinnings of well-tempered metadynamics to the practical task of selecting the bias factor and initial hill height, $w_0$, to achieve a desired residual free energy in a catalytic system. ",
            "id": "3879967",
            "problem": "A catalytic surface reaction is modeled by a one-dimensional collective variable (CV) $s$ with an unbiased potential of mean force (PMF) $F(s)$ that exhibits two basins corresponding to reactant and product configurations. The molar free energy difference between the basins is $\\Delta F = 20.0~\\text{kJ/mol}$ at temperature $T = 650~\\text{K}$. You will perform well-tempered metadynamics, where the bias factor is defined as $\\gamma = 1 + \\Delta T/T$, with $\\Delta T$ the tempering temperature. At long times, well-tempered metadynamics produces a stationary distribution that rescales basin free energy differences, and for robust catalytic rate estimation you aim to choose parameters so that the residual molar free energy difference between basins is on the order of $k_B T$ per molecule, equivalently $RT$ per mole, where $k_B$ is the Boltzmann constant and $R$ is the universal gas constant.\n\nStarting from the statistical mechanical definition of the potential of mean force and the well-tempered metadynamics stationary distribution, determine the bias factor $\\gamma$ that achieves a residual molar free energy difference of approximately $RT$. Then, using dimensional and scale-matching arguments grounded in the exponential tempering kernel in well-tempered metadynamics, select the initial Gaussian hill height $w_0$ such that the initial hill amplitude matches the molar thermal energy scale at the tempering temperature $\\Delta T$.\n\nTake $R = 8.314462618 \\times 10^{-3}~\\text{kJ/(mol·K)}$. Express your final $w_0$ in $\\text{kJ/mol}$. Round both $\\gamma$ and $w_0$ to four significant figures. Your final answer must be a two-entry row vector containing first $\\gamma$ (dimensionless), then $w_0$ (in $\\text{kJ/mol}$).",
            "solution": "The potential of mean force (PMF), $F(s)$, describes the free energy landscape along a collective variable $s$. In the canonical ensemble, the probability of observing the system at a particular value of $s$ is proportional to $\\exp(-F(s)/(RT))$ for molar quantities, where $R$ is the universal gas constant and $T$ is the temperature.\n\nIn a well-tempered metadynamics simulation, a history-dependent bias potential, $V_b(s, t)$, is added to the system. In the long-time limit, the bias potential converges such that the total, biased PMF becomes a rescaled version of the original:\n$$F_{\\text{biased}}(s) = F(s) + V_b(s) = \\frac{1}{\\gamma} F(s) + C$$\nwhere $\\gamma$ is the bias factor and $C$ is a constant. Consequently, the residual free energy difference between two basins, $\\Delta F_{\\text{res}}$, is scaled by the same factor:\n$$\\Delta F_{\\text{res}} = \\frac{\\Delta F}{\\gamma}$$\nThe problem requires this residual molar free energy difference to be equal to the molar thermal energy, $RT$.\n$$\\Delta F_{\\text{res}} = RT$$\nBy setting these two expressions equal, we can solve for the bias factor $\\gamma$:\n$$\\frac{\\Delta F}{\\gamma} = RT \\implies \\gamma = \\frac{\\Delta F}{RT}$$\nGiven the values $\\Delta F = 20.0~\\text{kJ/mol}$, $T = 650~\\text{K}$, and $R = 8.314462618 \\times 10^{-3}~\\text{kJ/(mol·K)}$, we first calculate $RT$:\n$$RT = (8.314462618 \\times 10^{-3}~\\text{kJ/(mol·K)}) \\times (650~\\text{K}) \\approx 5.4044~\\text{kJ/mol}$$\nNow we can compute $\\gamma$:\n$$\\gamma = \\frac{20.0~\\text{kJ/mol}}{5.4044~\\text{kJ/mol}} \\approx 3.7007$$\nRounding to four significant figures gives $\\gamma \\approx 3.701$.\n\nNext, we determine the initial Gaussian hill height, $w_0$. The bias factor is defined in terms of a tempering temperature, $\\Delta T$:\n$$\\gamma = 1 + \\frac{\\Delta T}{T}$$\nWe can solve for $\\Delta T$:\n$$\\Delta T = (\\gamma - 1)T$$\nUsing our computed value of $\\gamma$:\n$$\\Delta T = (3.7007 - 1) \\times 650~\\text{K} \\approx 1755.5~\\text{K}$$\nThe problem directs us to set the initial hill height $w_0$ to match the molar thermal energy scale at this tempering temperature, which is $R\\Delta T$.\n$$w_0 = R \\Delta T = R(\\gamma-1)T$$\nThis can be simplified by noting that $R(\\gamma-1)T = R\\left(\\frac{\\Delta F}{RT}-1\\right)T = \\Delta F - RT$.\n$$w_0 = 20.0~\\text{kJ/mol} - 5.4044~\\text{kJ/mol} \\approx 14.5956~\\text{kJ/mol}$$\nThis choice ensures the energy scale of the deposited bias hills is commensurate with the energy scale governing the tempering process. Rounding to four significant figures gives $w_0 \\approx 14.60~\\text{kJ/mol}$.\n\nThe final answer is the row vector containing the values for $\\gamma$ and $w_0$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 3.701 & 14.60 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Generating simulation data is only the first step; robustly analyzing it to produce a reliable PMF is a challenge in itself. Methods like the Weighted Histogram Analysis Method (WHAM) and the Multistate Bennett Acceptance Ratio (MBAR) rely on sufficient statistical overlap between windows, a condition that can be violated by poor simulation setup or the presence of unaddressed \"hidden\" slow degrees of freedom. This exercise will test your understanding of these common pitfalls and the diagnostic tools, such as the effective sample size and overlap matrix, used to detect them before drawing incorrect scientific conclusions. ",
            "id": "3879962",
            "problem": "Consider umbrella sampling of a dissociative adsorption reaction on a catalytic surface with a collective variable $s(x)$ representing the reaction coordinate and a hidden slow variable $y(x)$ representing lateral site occupancy. You run $K$ windows, each with a harmonic bias $U_k(x)=\\tfrac{1}{2}\\kappa\\left(s(x)-s_k\\right)^2$ centered at $s_k$, and collect $N_k$ samples per window. Let $\\beta$ denote the inverse thermal energy, and define dimensionless energies $u_0(x)=\\beta U_0(x)$ for the unbiased potential and $u_k(x)=u_0(x)+\\beta U_k(x)$ for window $k$. The unbiased canonical density satisfies $p_0(x)\\propto \\exp\\!\\left[-u_0(x)\\right]$, while the biased density in window $k$ satisfies $p_k(x)\\propto \\exp\\!\\left[-u_k(x)\\right]$.\n\nTo combine windows into a potential of mean force (PMF) $F(s)$ along $s$, you intend to use the Weighted Histogram Analysis Method (WHAM) or the Multistate Bennett Acceptance Ratio (MBAR). In MBAR, a common reweighting identity expresses a weight $w_n^{(r)}$ that maps sample $x_n$ to a reference state $r$ (e.g., the unbiased state $r=0$ or a particular window $r=k$) as a normalized function of the energies across all windows; the effective sample size for a target state $r$ computed from all samples is $N_{\\mathrm{eff}}^{(r)}=\\left(\\sum_n w_n^{(r)}\\right)^2\\Big/\\sum_n\\left(w_n^{(r)}\\right)^2$. If samples are time-correlated within a window, the number of effectively independent samples is reduced by the statistical inefficiency $g\\approx 2\\tau_{\\mathrm{int}}$, where $\\tau_{\\mathrm{int}}$ is the integrated autocorrelation time, so that a correlation-corrected effective sample size can be approximated as $N_{\\mathrm{eff,corr}}^{(r)}\\approx N_{\\mathrm{eff}}^{(r)}/g$.\n\nSuppose adjacent windows along $s$ lack sufficient overlap in some region, and the hidden variable $y$ partitions configurations into two slowly interconverting basins $A$ and $B$ throughout several windows. As a result, some windows primarily sample basin $A$, others primarily sample basin $B$, and a few windows contain both basins but exhibit poor mixing.\n\nUsing the canonical ensemble definitions above and the principle of reweighting between biased ensembles, analyze the consequences for WHAM/MBAR estimators of $F(s)$ when windows lack overlap or contain hidden states, and propose diagnostics using $N_{\\mathrm{eff}}$ and weight distributions. Which of the following statements are correct?\n\nA. In the absence of overlap between adjacent windows along $s$, the Multistate Bennett Acceptance Ratio produces unbiased relative free energies $f_k$ among all windows provided $\\sum_{k=1}^K N_k$ is large, because large sample sizes compensate for missing overlap.\n\nB. A diagnostic for non-overlap is to compute, for each window $k$, an effective sample size $N_{\\mathrm{eff}}^{(k)}$ using reweighting weights $w_n^{(k)}$ for all samples $x_n$ in the dataset; if $N_{\\mathrm{eff}}^{(k)}\\ll 1$ for many adjacent $k$, this flags insufficient overlap and non-identifiability of inter-window free energy differences.\n\nC. If the distribution of $\\log w_n^{(0)}$ across all samples is bimodal or exhibits heavy tails, this indicates hidden orthogonal metastable states, and the PMF $F(s)$ estimated by WHAM/MBAR can be biased unless $y$ is adequately sampled and overlaps are ensured.\n\nD. For the Weighted Histogram Analysis Method, convergence of the iterative estimates of relative free energies $f_k$ is guaranteed whenever each histogram bin has at least $10^2$ counts, because count thresholds ensure sufficient overlap and mixing.\n\nE. A practical diagnostic for hidden states and non-identifiability is to compute the overlap matrix $O_{kl}=\\sum_{n\\in \\mathcal{D}_l} w_n^{(k)}$, where $\\mathcal{D}_l$ denotes samples drawn from window $l$; if the graph with nodes $k$ and edges connecting pairs with non-negligible $O_{kl}$ is disconnected, global free energy differences across components are not identifiable by WHAM/MBAR.\n\nF. If the integrated autocorrelation time $\\tau_{\\mathrm{int}}$ within each window is large, MBAR reweighting still yields $N_{\\mathrm{eff}}^{(0)}$ close to the total number of samples $N=\\sum_{k=1}^K N_k$, because reweighting weights correct for time correlation even without decorrelation or subsampling.\n\nSelect all that apply.",
            "solution": "The problem statement has been validated and is deemed scientifically grounded, well-posed, and objective. It accurately describes a common and complex scenario in computational chemistry involving enhanced sampling simulations (umbrella sampling) and their analysis using the Weighted Histogram Analysis Method (WHAM) or the Multistate Bennett Acceptance Ratio (MBAR). The definitions provided for potentials, probability distributions, and analysis metrics such as the effective sample size are standard and correct. The problem's core challenge—assessing the impact of poor phase space overlap and hidden slow variables on free energy calculations—is a critical topic in the field.\n\nThe analysis of each statement proceeds from the fundamental principles of statistical reweighting, on which both WHAM and MBAR are based. The central idea is that the properties of a target state (e.g., the unbiased state $r=0$) can be estimated from samples drawn from one or more other states (e.g., biased umbrella windows $k=1, ..., K$) provided there is sufficient overlap in the sampled phase space distributions.\n\n**A. In the absence of overlap between adjacent windows along $s$, the Multistate Bennett Acceptance Ratio produces unbiased relative free energies $f_k$ among all windows provided $\\sum_{k=1}^K N_k$ is large, because large sample sizes compensate for missing overlap.**\n\nThis statement is fundamentally incorrect. The ability of MBAR (and WHAM) to determine the relative free energy between two states, say $k$ and $l$, depends critically on the existence of phase space overlap between them. Overlap ensures that configurations sampled in state $k$ have a non-vanishing probability of being observed in state $l$, and vice versa. If there is no overlap, the reweighting integrals that connect the states become ill-defined. No amount of additional sampling in the *non-overlapping* regions can bridge this gap. Increasing the total number of samples $\\sum_{k=1}^K N_k$ will only provide a denser sampling of the already-sampled regions, it will not magically generate the rare events required to connect disconnected parts of the phase space. In the absence of overlap, the relative free energies between disconnected sets of windows are non-identifiable, meaning the solution to the MBAR equations is not unique. Therefore, large sample sizes do not compensate for missing overlap.\n\n**Verdict:** **Incorrect**.\n\n**B. A diagnostic for non-overlap is to compute, for each window $k$, an effective sample size $N_{\\mathrm{eff}}^{(k)}$ using reweighting weights $w_n^{(k)}$ for all samples $x_n$ in the dataset; if $N_{\\mathrm{eff}}^{(k)}\\ll 1$ for many adjacent $k$, this flags insufficient overlap and non-identifiability of inter-window free energy differences.**\n\nThis statement is correct. The effective sample size, defined as $N_{\\mathrm{eff}}^{(r)}=\\left(\\sum_n w_n^{(r)}\\right)^2\\Big/\\sum_n\\left(w_n^{(r)}\\right)^2$, measures how evenly the weights $w_n^{(r)}$ are distributed across all $N = \\sum_k N_k$ samples when estimating properties of a target state $r$. A small value of $N_{\\mathrm{eff}}^{(k)}$ indicates that the estimate for state $k$ is dominated by a very small number of samples, meaning the weights are highly unequal. This occurs when very few samples in the entire dataset are representative of the thermodynamic state of window $k$. A value of $N_{\\mathrm{eff}}^{(k)}\\ll 1$ is an extreme case, indicating that essentially no samples from the combined dataset can effectively contribute to the properties of state $k$. This is a classic symptom of state $k$ being \"isolated\" due to poor phase space overlap with the other windows. If this is observed for a contiguous block of windows, it signals a break in the chain of overlaps, rendering the free energy differences across this gap non-identifiable.\n\n**Verdict:** **Correct**.\n\n**C. If the distribution of $\\log w_n^{(0)}$ across all samples is bimodal or exhibits heavy tails, this indicates hidden orthogonal metastable states, and the PMF $F(s)$ estimated by WHAM/MBAR can be biased unless $y$ is adequately sampled and overlaps are ensured.**\n\nThis statement is correct. The reweighting weights $w_n^{(0)}$ project each sampled configuration $x_n$ onto the unbiased ensemble. If a hidden slow variable $y$ partitions the phase space into distinct basins, $A$ and $B$, that are not resolved by the collective variable $s$, configurations from these two basins can have systematically different unbiased energies $u_0(x)$. This will lead to two distinct families of reweighting weights. The distribution of weights, or log-weights, will consequently be bimodal, with one peak corresponding to samples from basin $A$ and the other to samples from basin $B$. If the sampling does not adequately capture the transitions between $A$ and $B$, or if the umbrella windows do not provide a continuous path that samples both basins, WHAM/MBAR will not have sufficient information to correctly weight their relative contributions to the total free energy. The resulting PMF $F(s)$ will be biased because it averages over an incorrect population of the hidden states. Heavy tails in the weight distribution are also a sign of problematic reweighting, often stemming from poor overlap where a few samples are given enormous weight.\n\n**Verdict:** **Correct**.\n\n**D. For the Weighted Histogram Analysis Method, convergence of the iterative estimates of relative free energies $f_k$ is guaranteed whenever each histogram bin has at least $10^2$ counts, because count thresholds ensure sufficient overlap and mixing.**\n\nThis statement is incorrect. While having a minimum number of counts (e.g., $10^2$) in each histogram bin is a useful heuristic for ensuring that the local density is somewhat reasonably estimated, it does not, by itself, guarantee sufficient overlap for global convergence of the WHAM equations. One can construct a scenario where there are two groups of windows, with excellent overlap and high bin counts *within* each group, but zero overlap *between* the two groups. In this case, the WHAM equations cannot determine the single, unique free energy difference between the two groups. The iterative procedure may fail to converge or converge to an arbitrary solution dependent on initial guesses. The problem is one of global connectivity in phase space, which is not guaranteed by local population statistics. The claim of a \"guarantee\" is too strong and mathematically false.\n\n**Verdict:** **Incorrect**.\n\n**E. A practical diagnostic for hidden states and non-identifiability is to compute the overlap matrix $O_{kl}=\\sum_{n\\in \\mathcal{D}_l} w_n^{(k)}$, where $\\mathcal{D}_l$ denotes samples drawn from window $l$; if the graph with nodes $k$ and edges connecting pairs with non-negligible $O_{kl}$ is disconnected, global free energy differences across components are not identifiable by WHAM/MBAR.**\n\nThis statement is correct. The quantity $O_{kl}$ measures the overlap of the distribution sampled in window $l$ with the target distribution of window $k$. It quantifies how much information samples from window $l$ provide about window $k$. The matrix $O$ of elements $O_{kl}$ is a fundamental diagnostic tool for MBAR and WHAM. If one constructs a graph where the simulation windows $k$ are nodes and an edge connects nodes $k$ and $l$ if their mutual overlap ($O_{kl}$ or $O_{lk}$) is above a small threshold, the connectivity of this graph reflects the flow of statistical information across the entire set of simulations. If this graph is disconnected into two or more components, it means there is no path of overlapping windows to connect these components. Consequently, the relative free energy between any two windows in different components is non-identifiable. This diagnostic robustly detects both explicit lack of overlap along the CV $s$ and implicit lack of overlap due to hidden orthogonal states.\n\n**Verdict:** **Correct**.\n\n**F. If the integrated autocorrelation time $\\tau_{\\mathrm{int}}$ within each window is large, MBAR reweighting still yields $N_{\\mathrm{eff}}^{(0)}$ close to the total number of samples $N=\\sum_{k=1}^K N_k$, because reweighting weights correct for time correlation even without decorrelation or subsampling.**\n\nThis statement is incorrect. It conflates two distinct sources of statistical uncertainty. The effective sample size $N_{\\mathrm{eff}}^{(r)}$ derived from reweighting weights addresses the quality of phase space overlap between windows. It can be high if overlap is good, irrespective of time correlations. The integrated autocorrelation time $\\tau_{\\mathrm{int}}$ quantifies the temporal correlation between samples within a single simulation trajectory. A large $\\tau_{\\mathrm{int}}$ means the samples are not independent, which reduces the true number of statistically independent data points, $N_k/g_k$ where $g_k \\approx 2\\tau_{\\mathrm{int},k}$. The MBAR/WHAM formalisms, in their standard application, treat all provided samples as independent draws from their respective biased distributions. The reweighting weights $w_n^{(r)}$ do not and cannot \"correct\" for time correlation. The high time correlation will manifest as a larger statistical error in the final free energy estimates, an effect that is not captured by $N_{\\mathrm{eff}}^{(r)}$ alone but requires the correlation-corrected quantity $N_{\\mathrm{eff,corr}}^{(r)} \\approx N_{\\mathrm{eff}}^{(r)}/g$.\n\n**Verdict:** **Incorrect**.",
            "answer": "$$\\boxed{BCE}$$"
        }
    ]
}