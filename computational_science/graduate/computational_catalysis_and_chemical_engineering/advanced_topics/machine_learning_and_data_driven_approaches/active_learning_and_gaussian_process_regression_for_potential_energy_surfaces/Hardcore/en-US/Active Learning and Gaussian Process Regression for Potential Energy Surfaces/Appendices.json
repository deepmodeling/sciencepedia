{
    "hands_on_practices": [
        {
            "introduction": "To truly understand Gaussian Process Regression, it is essential to work through the core predictive equations by hand. This exercise guides you through calculating the posterior predictive mean for a new configuration, based on a small set of training data . By performing this fundamental calculation, you will gain a concrete understanding of how the kernel matrix, observed energies, and noise level combine to produce a prediction, moving beyond the abstraction of software packages.",
            "id": "3867315",
            "problem": "In a data-driven exploration of a potential energy surface for an adsorbate on a catalytic slab in computational catalysis, suppose the energy as a function of configuration is modeled by a zero-mean Gaussian process (GP) with covariance given by a positive-definite kernel. Three configurations have been evaluated using Density Functional Theory (DFT), yielding noisy observations modeled by $y = f + \\epsilon$, where $f$ are latent function values and $\\epsilon \\sim \\mathcal{N}(0, \\sigma_{n}^{2} I)$ represents independent and identically distributed Gaussian noise with variance $\\sigma_{n}^{2}$. The Gram matrix over the three training configurations is\n$$\nK \\;=\\; \\begin{bmatrix} 1 & 0.5 & 0.2 \\\\ 0.5 & 1 & 0.3 \\\\ 0.2 & 0.3 & 1 \\end{bmatrix},\n$$\nthe noise variance is $\\sigma_{n}^{2} = 0.01$, the observed energies (in electronvolts) are\n$$\ny \\;=\\; \\begin{bmatrix} -0.1 \\\\ 0.2 \\\\ 0.0 \\end{bmatrix},\n$$\nand the covariance vector between a test configuration and the three training configurations is\n$$\nk_{*} \\;=\\; \\begin{bmatrix} 0.4 \\\\ 0.6 \\\\ 0.1 \\end{bmatrix}.\n$$\nUsing only the conditioning rule for jointly Gaussian variables and the definitions above, compute the Gaussian process posterior predictive mean $\\mu_{*}$ at the test configuration. Express your final answer as the predicted energy in electronvolts and round your result to four significant figures.",
            "solution": "The problem asks for the posterior predictive mean $\\mu_{*}$ at a test configuration, given noisy observations from a zero-mean Gaussian process (GP) model of a potential energy surface. The solution is derived from the conditioning rule for jointly Gaussian random variables.\n\nThe fundamental assumption of GP regression is that any finite collection of function values, including those at training points, $f$, and a test point, $f_{*}$, are jointly Gaussian. Given the problem specifies a zero-mean GP, we have:\n$$\n\\begin{pmatrix} f \\\\ f_{*} \\end{pmatrix}\n\\sim \\mathcal{N} \\left(\n\\begin{pmatrix} \\mathbf{0} \\\\ 0 \\end{pmatrix},\n\\begin{pmatrix} K & k_{*} \\\\ k_{*}^T & k_{**} \\end{pmatrix}\n\\right)\n$$\nwhere $K$ is the covariance matrix (Gram matrix) of the training points, $k_{*}$ is the covariance vector between the test point and the training points, and $k_{**}$ is the self-covariance of the test point.\n\nThe observations $y$ are noisy versions of the true function values $f$, with $y = f + \\epsilon$, where the noise $\\epsilon$ is drawn from an independent and identically distributed Gaussian distribution $\\mathcal{N}(0, \\sigma_{n}^{2}I)$. This implies that the vector of observations $y$ is also Gaussian, with a covariance that includes the noise variance:\n$$\ny \\sim \\mathcal{N}(0, K + \\sigma_{n}^{2}I)\n$$\nThe joint distribution of the observed training outputs $y$ and the latent function value at the test point $f_{*}$ is therefore:\n$$\n\\begin{pmatrix} y \\\\ f_{*} \\end{pmatrix}\n\\sim \\mathcal{N} \\left(\n\\begin{pmatrix} \\mathbf{0} \\\\ 0 \\end{pmatrix},\n\\begin{pmatrix} K + \\sigma_{n}^{2}I & k_{*} \\\\ k_{*}^T & k_{**} \\end{pmatrix}\n\\right)\n$$\nWe seek the posterior predictive mean $\\mu_{*} = \\mathbb{E}[f_{*} | y]$. Using the standard formula for the conditional mean of a partitioned multivariate Gaussian distribution, we have:\n$$\n\\mu_{*} = \\mathbb{E}[f_{*}] + k_{*}^T (K + \\sigma_{n}^{2}I)^{-1} (y - \\mathbb{E}[y])\n$$\nSince the GP has a zero mean, $\\mathbb{E}[f_{*}] = 0$ and $\\mathbb{E}[y] = \\mathbf{0}$. The formula simplifies to:\n$$\n\\mu_{*} = k_{*}^T (K + \\sigma_{n}^{2}I)^{-1} y\n$$\nWe are given the following quantities:\n$$\nK = \\begin{bmatrix} 1 & 0.5 & 0.2 \\\\ 0.5 & 1 & 0.3 \\\\ 0.2 & 0.3 & 1 \\end{bmatrix}, \\quad \\sigma_{n}^{2} = 0.01, \\quad y = \\begin{bmatrix} -0.1 \\\\ 0.2 \\\\ 0.0 \\end{bmatrix}, \\quad k_{*} = \\begin{bmatrix} 0.4 \\\\ 0.6 \\\\ 0.1 \\end{bmatrix}\n$$\nThe calculation proceeds in steps. First, we compute the matrix $K_y = K + \\sigma_{n}^{2}I$:\n$$\nK_y = \\begin{bmatrix} 1 & 0.5 & 0.2 \\\\ 0.5 & 1 & 0.3 \\\\ 0.2 & 0.3 & 1 \\end{bmatrix} + 0.01 \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 1.01 & 0.5 & 0.2 \\\\ 0.5 & 1.01 & 0.3 \\\\ 0.2 & 0.3 & 1.01 \\end{bmatrix}\n$$\nNext, we must find the inverse of $K_y$. The inverse of a matrix $A$ is given by $A^{-1} = \\frac{1}{\\det(A)}\\text{adj}(A)$, where $\\text{adj}(A)$ is the adjugate of $A$.\n\nThe determinant of $K_y$ is:\n$$\n\\det(K_y) = 1.01(1.01^2 - 0.3^2) - 0.5(0.5 \\cdot 1.01 - 0.3 \\cdot 0.2) + 0.2(0.5 \\cdot 0.3 - 1.01 \\cdot 0.2)\n$$\n$$\n\\det(K_y) = 1.01(1.0201 - 0.09) - 0.5(0.505 - 0.06) + 0.2(0.15 - 0.202)\n$$\n$$\n\\det(K_y) = 1.01(0.9301) - 0.5(0.445) + 0.2(-0.052)\n$$\n$$\n\\det(K_y) = 0.939401 - 0.2225 - 0.0104 = 0.706501\n$$\nThe adjugate matrix is the transpose of the cofactor matrix. Since $K_y$ is symmetric, its cofactor matrix is also symmetric, so the adjugate matrix is equal to the cofactor matrix. The cofactors are:\n$$\nC_{11} = 1.01^2 - 0.3^2 = 0.9301\n$$\n$$\nC_{12} = -(0.5 \\cdot 1.01 - 0.3 \\cdot 0.2) = -0.445\n$$\n$$\nC_{13} = 0.5 \\cdot 0.3 - 1.01 \\cdot 0.2 = -0.052\n$$\n$$\nC_{22} = 1.01^2 - 0.2^2 = 0.9801\n$$\n$$\nC_{23} = -(1.01 \\cdot 0.3 - 0.2 \\cdot 0.5) = -0.203\n$$\n$$\nC_{33} = 1.01^2 - 0.5^2 = 0.7701\n$$\nThe cofactor matrix $C$ is:\n$$\nC = \\text{adj}(K_y) = \\begin{bmatrix} 0.9301 & -0.445 & -0.052 \\\\ -0.445 & 0.9801 & -0.203 \\\\ -0.052 & -0.203 & 0.7701 \\end{bmatrix}\n$$\nNow we can compute the vector $\\alpha = (K + \\sigma_{n}^{2}I)^{-1} y$:\n$$\n\\alpha = \\frac{1}{\\det(K_y)} \\text{adj}(K_y) y = \\frac{1}{0.706501} \\begin{bmatrix} 0.9301 & -0.445 & -0.052 \\\\ -0.445 & 0.9801 & -0.203 \\\\ -0.052 & -0.203 & 0.7701 \\end{bmatrix} \\begin{bmatrix} -0.1 \\\\ 0.2 \\\\ 0.0 \\end{bmatrix}\n$$\n$$\n\\alpha = \\frac{1}{0.706501} \\begin{bmatrix} 0.9301(-0.1) - 0.445(0.2) \\\\ -0.445(-0.1) + 0.9801(0.2) \\\\ -0.052(-0.1) - 0.203(0.2) \\end{bmatrix} = \\frac{1}{0.706501} \\begin{bmatrix} -0.09301 - 0.089 \\\\ 0.0445 + 0.19602 \\\\ 0.0052 - 0.0406 \\end{bmatrix} = \\frac{1}{0.706501} \\begin{bmatrix} -0.18201 \\\\ 0.24052 \\\\ -0.0354 \\end{bmatrix}\n$$\nFinally, we compute the posterior mean $\\mu_{*} = k_{*}^T \\alpha$:\n$$\n\\mu_{*} = \\begin{bmatrix} 0.4 & 0.6 & 0.1 \\end{bmatrix} \\left( \\frac{1}{0.706501} \\begin{bmatrix} -0.18201 \\\\ 0.24052 \\\\ -0.0354 \\end{bmatrix} \\right)\n$$\n$$\n\\mu_{*} = \\frac{1}{0.706501} [0.4(-0.18201) + 0.6(0.24052) + 0.1(-0.0354)]\n$$\n$$\n\\mu_{*} = \\frac{1}{0.706501} [-0.072804 + 0.144312 - 0.00354]\n$$\n$$\n\\mu_{*} = \\frac{0.067968}{0.706501} \\approx 0.096203827\n$$\nThe problem requires the result to be rounded to four significant figures. The first significant figure is $9$, the fourth is $0$, and the following digit is $3$. Therefore, we round down.\n$$\n\\mu_{*} \\approx 0.09620\n$$\nThe predicted energy is $0.09620$ eV.",
            "answer": "$$\\boxed{0.09620}$$"
        },
        {
            "introduction": "A robust machine learning model requires careful data preprocessing, and for physical systems, this preprocessing must respect the underlying laws of nature. This problem focuses on the crucial task of normalizing energies and forces for a joint Gaussian Process model . You will derive the correct scaling factor for forces to ensure that the fundamental relationship $\\mathbf{F} = -\\nabla E$ remains invariant, a concept known as energy–force coherence, which is critical for building reliable and physically meaningful potential energy surfaces.",
            "id": "3867259",
            "problem": "You are building a joint energy–force Gaussian Process Regression (GPR) model for a potential energy surface relevant to a catalytic intermediate on a supported metal cluster. The data comprise configurations with Cartesian coordinates $\\mathbf{R} \\in \\mathbb{R}^{3n}$ (with $n$ atoms), scalar energies $E(\\mathbf{R})$, and vector forces $\\mathbf{F}(\\mathbf{R})$, with the physical relation $\\mathbf{F}(\\mathbf{R})=-\\nabla_{\\mathbf{R}}E(\\mathbf{R})$. To stabilize the GPR training and acquisition in an active learning loop, you will standardize energies to zero mean and unit variance and also non-dimensionalize the input coordinates by a characteristic length scale. Specifically, let $\\mu_{E}$ and $\\sigma_{E}>0$ denote the sample mean and standard deviation of the energy values over the current training set, and let $\\ell>0$ be a fixed characteristic length (for example, a representative bond length) used to scale the inputs.\n\nDefine the normalized energy and coordinates by\n- $\\tilde{E}(\\mathbf{R})=\\left(E(\\mathbf{R})-\\mu_{E}\\right)/\\sigma_{E}$,\n- $\\tilde{\\mathbf{R}}=\\mathbf{R}/\\ell$.\n\nYou require that the normalized forces $\\tilde{\\mathbf{F}}$ be defined so that the energy–force coherence is preserved exactly in normalized variables, i.e., $\\tilde{\\mathbf{F}}(\\tilde{\\mathbf{R}})=-\\nabla_{\\tilde{\\mathbf{R}}}\\tilde{E}(\\tilde{\\mathbf{R}})$ holds for all configurations.\n\nWhich of the following normalization protocols achieves this objective?\n\nA. Use $\\tilde{E}(\\mathbf{R})=\\left(E(\\mathbf{R})-\\mu_{E}\\right)/\\sigma_{E}$, $\\tilde{\\mathbf{R}}=\\mathbf{R}/\\ell$, and set $\\tilde{\\mathbf{F}}(\\mathbf{R})=\\left(\\ell/\\sigma_{E}\\right)\\mathbf{F}(\\mathbf{R})$.\n\nB. Use $\\tilde{E}(\\mathbf{R})=\\left(E(\\mathbf{R})-\\mu_{E}\\right)/\\sigma_{E}$, $\\tilde{\\mathbf{R}}=\\mathbf{R}/\\ell$, and set $\\tilde{\\mathbf{F}}(\\mathbf{R})=\\mathbf{F}(\\mathbf{R})/\\sigma_{E}$.\n\nC. Use $\\tilde{E}(\\mathbf{R})=\\left(E(\\mathbf{R})-\\mu_{E}\\right)/\\sigma_{E}$, $\\tilde{\\mathbf{R}}=\\mathbf{R}/\\ell$, and set $\\tilde{\\mathbf{F}}(\\mathbf{R})=\\mathbf{F}(\\mathbf{R})/\\left(\\ell\\,\\sigma_{E}\\right)$.\n\nD. Use $\\tilde{E}(\\mathbf{R})=\\left(E(\\mathbf{R})-\\mu_{E}\\right)/\\sigma_{E}$, $\\tilde{\\mathbf{R}}=\\mathbf{R}/\\ell$, and set $\\tilde{\\mathbf{F}}(\\mathbf{R})=\\left(\\ell/\\sigma_{E}\\right)\\left(\\mathbf{F}(\\mathbf{R})-\\bar{\\mathbf{F}}\\right)$, where $\\bar{\\mathbf{F}}$ is the sample mean of the forces over the training set.\n\nSelect the single best option.",
            "solution": "The user has provided a problem statement regarding the normalization of energies, coordinates, and forces for use in a joint energy-force Gaussian Process Regression (GPR) model for a potential energy surface. The core of the problem is to find the correct transformation for the forces such that the fundamental physical relationship between energy and force is preserved in the normalized coordinate system.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   The system involves Cartesian coordinates $\\mathbf{R} \\in \\mathbb{R}^{3n}$, scalar energies $E(\\mathbf{R})$, and vector forces $\\mathbf{F}(\\mathbf{R})$.\n-   The fundamental physical relation is $\\mathbf{F}(\\mathbf{R})=-\\nabla_{\\mathbf{R}}E(\\mathbf{R})$.\n-   Normalization parameters are the sample energy mean $\\mu_{E}$, the sample energy standard deviation $\\sigma_{E} > 0$, and a characteristic length scale $\\ell > 0$.\n-   Normalized variables are defined as:\n    -   Normalized energy: $\\tilde{E}(\\mathbf{R})=\\left(E(\\mathbf{R})-\\mu_{E}\\right)/\\sigma_{E}$.\n    -   Normalized coordinates: $\\tilde{\\mathbf{R}}=\\mathbf{R}/\\ell$.\n-   The required condition is that the normalized forces $\\tilde{\\mathbf{F}}$ must satisfy the energy-force coherence in the normalized space: $\\tilde{\\mathbf{F}}(\\tilde{\\mathbf{R}})=-\\nabla_{\\tilde{\\mathbf{R}}}\\tilde{E}(\\tilde{\\mathbf{R}})$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem is firmly rooted in the principles of classical mechanics ($\\mathbf{F}=-\\nabla E$) and standard practices in computational chemistry and machine learning (data standardization, GPR for potential energy surfaces). The context is realistic and scientifically sound.\n-   **Well-Posed:** The problem is clearly stated, providing all necessary definitions and a single, unambiguous goal. A unique solution can be derived through mathematical reasoning.\n-   **Objective:** The problem is formulated using precise, objective mathematical and scientific language.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is scientifically sound, well-posed, and objective. I will proceed to derive the solution.\n\n### Derivation\n\nThe objective is to find the transformation for the forces, $\\tilde{\\mathbf{F}}$, such that the relationship $\\tilde{\\mathbf{F}}(\\tilde{\\mathbf{R}}) = -\\nabla_{\\tilde{\\mathbf{R}}}\\tilde{E}(\\tilde{\\mathbf{R}})$ is maintained. To do this, we will compute the right-hand side of this equation, which represents the negative gradient of the normalized energy with respect to the normalized coordinates.\n\nThe normalized coordinates are defined as $\\tilde{\\mathbf{R}} = \\mathbf{R}/\\ell$. This implies a relationship for the original coordinates: $\\mathbf{R} = \\ell\\tilde{\\mathbf{R}}$.\n\nThe normalized energy, $\\tilde{E}$, must be expressed as a function of the normalized coordinates, $\\tilde{\\mathbf{R}}$. Using the given definitions, we have:\n$$\n\\tilde{E}(\\tilde{\\mathbf{R}}) = \\frac{E(\\mathbf{R}) - \\mu_{E}}{\\sigma_{E}} = \\frac{E(\\ell\\tilde{\\mathbf{R}}) - \\mu_{E}}{\\sigma_{E}}\n$$\nHere, $\\mu_{E}$ and $\\sigma_{E}$ are scalar constants for a given training set.\n\nNow, we compute the gradient of $\\tilde{E}(\\tilde{\\mathbf{R}})$ with respect to $\\tilde{\\mathbf{R}}$:\n$$\n\\nabla_{\\tilde{\\mathbf{R}}}\\tilde{E}(\\tilde{\\mathbf{R}}) = \\nabla_{\\tilde{\\mathbf{R}}} \\left( \\frac{E(\\ell\\tilde{\\mathbf{R}}) - \\mu_{E}}{\\sigma_{E}} \\right)\n$$\nSince $\\mu_{E}$ and $\\sigma_{E}$ are constants with respect to the coordinates, we can write:\n$$\n\\nabla_{\\tilde{\\mathbf{R}}}\\tilde{E}(\\tilde{\\mathbf{R}}) = \\frac{1}{\\sigma_{E}} \\nabla_{\\tilde{\\mathbf{R}}} E(\\ell\\tilde{\\mathbf{R}}) - \\nabla_{\\tilde{\\mathbf{R}}} \\left(\\frac{\\mu_{E}}{\\sigma_{E}}\\right) = \\frac{1}{\\sigma_{E}} \\nabla_{\\tilde{\\mathbf{R}}} E(\\ell\\tilde{\\mathbf{R}})\n$$\nTo evaluate $\\nabla_{\\tilde{\\mathbf{R}}} E(\\ell\\tilde{\\mathbf{R}})$, we apply the chain rule for vector calculus. Let the components of $\\mathbf{R}$ be $R_i$ and the components of $\\tilde{\\mathbf{R}}$ be $\\tilde{R}_j$. The relationship is $R_i = \\ell \\tilde{R}_i$ for each component $i$. The $j$-th component of the gradient with respect to $\\tilde{\\mathbf{R}}$ is:\n$$\n\\left[ \\nabla_{\\tilde{\\mathbf{R}}} E(\\ell\\tilde{\\mathbf{R}}) \\right]_j = \\frac{\\partial E}{\\partial \\tilde{R}_j} = \\sum_i \\frac{\\partial E}{\\partial R_i} \\frac{\\partial R_i}{\\partial \\tilde{R}_j}\n$$\nThe partial derivative $\\frac{\\partial R_i}{\\partial \\tilde{R}_j}$ is $\\ell$ if $i=j$ and $0$ if $i \\neq j$. That is, $\\frac{\\partial R_i}{\\partial \\tilde{R}_j} = \\ell \\delta_{ij}$. The sum therefore simplifies to a single term:\n$$\n\\frac{\\partial E}{\\partial \\tilde{R}_j} = \\frac{\\partial E}{\\partial R_j} \\cdot \\ell\n$$\nSince this holds for every component $j$, we can express the full gradient vector as:\n$$\n\\nabla_{\\tilde{\\mathbf{R}}} E(\\ell\\tilde{\\mathbf{R}}) = \\ell \\nabla_{\\mathbf{R}} E(\\mathbf{R})\n$$\nSubstituting this result back into our expression for the gradient of the normalized energy:\n$$\n\\nabla_{\\tilde{\\mathbf{R}}}\\tilde{E}(\\tilde{\\mathbf{R}}) = \\frac{1}{\\sigma_{E}} \\left( \\ell \\nabla_{\\mathbf{R}} E(\\mathbf{R}) \\right) = \\frac{\\ell}{\\sigma_{E}} \\nabla_{\\mathbf{R}} E(\\mathbf{R})\n$$\nThe problem provides the fundamental physical relation $\\mathbf{F}(\\mathbf{R}) = -\\nabla_{\\mathbf{R}}E(\\mathbf{R})$. Substituting this gives:\n$$\n\\nabla_{\\tilde{\\mathbf{R}}}\\tilde{E}(\\tilde{\\mathbf{R}}) = \\frac{\\ell}{\\sigma_{E}} \\left( -\\mathbf{F}(\\mathbf{R}) \\right) = -\\frac{\\ell}{\\sigma_{E}} \\mathbf{F}(\\mathbf{R})\n$$\nThe coherence condition is $\\tilde{\\mathbf{F}}(\\tilde{\\mathbf{R}}) = -\\nabla_{\\tilde{\\mathbf{R}}}\\tilde{E}(\\tilde{\\mathbf{R}})$. Therefore:\n$$\n\\tilde{\\mathbf{F}}(\\tilde{\\mathbf{R}}) = - \\left( -\\frac{\\ell}{\\sigma_{E}} \\mathbf{F}(\\mathbf{R}) \\right) = \\frac{\\ell}{\\sigma_{E}} \\mathbf{F}(\\mathbf{R})\n$$\nThis result gives the rule for transforming the original forces $\\mathbf{F}(\\mathbf{R})$ into the normalized forces $\\tilde{\\mathbf{F}}$ that are consistent with the normalized energies and coordinates. The notation $\\tilde{\\mathbf{F}}(\\mathbf{R})$ used in the options represents this transformation rule applied to the force vector corresponding to the configuration $\\mathbf{R}$.\n\n### Option-by-Option Analysis\n\nBased on the derived relationship $\\tilde{\\mathbf{F}}(\\mathbf{R}) = \\left(\\ell/\\sigma_{E}\\right)\\mathbf{F}(\\mathbf{R})$, we evaluate each option.\n\n*   **A. Use $\\tilde{E}(\\mathbf{R})=\\left(E(\\mathbf{R})-\\mu_{E}\\right)/\\sigma_{E}$, $\\tilde{\\mathbf{R}}=\\mathbf{R}/\\ell$, and set $\\tilde{\\mathbf{F}}(\\mathbf{R})=\\left(\\ell/\\sigma_{E}\\right)\\mathbf{F}(\\mathbf{R})$.**\n    This expression exactly matches our derived result. The normalization factor for the force is correctly identified as $\\ell/\\sigma_{E}$.\n    **Verdict: Correct.**\n\n*   **B. Use $\\tilde{E}(\\mathbf{R})=\\left(E(\\mathbf{R})-\\mu_{E}\\right)/\\sigma_{E}$, $\\tilde{\\mathbf{R}}=\\mathbf{R}/\\ell$, and set $\\tilde{\\mathbf{F}}(\\mathbf{R})=\\mathbf{F}(\\mathbf{R})/\\sigma_{E}$.**\n    This option is missing the factor of $\\ell$ that arises from the scaling of the coordinates. This would only be correct if the coordinates were not scaled (i.e., if $\\ell=1$ in appropriate units). Dimensional analysis also shows this is incorrect: force has units $[E_u]/[L_u]$ (energy units over length units), so $\\mathbf{F}/\\sigma_E$ has units of $1/[L_u]$, whereas the normalized force must be dimensionless.\n    **Verdict: Incorrect.**\n\n*   **C. Use $\\tilde{E}(\\mathbf{R})=\\left(E(\\mathbf{R})-\\mu_{E}\\right)/\\sigma_{E}$, $\\tilde{\\mathbf{R}}=\\mathbf{R}/\\ell$, and set $\\tilde{\\mathbf{F}}(\\mathbf{R})=\\mathbf{F}(\\mathbf{R})/\\left(\\ell\\,\\sigma_{E}\\right)$.**\n    This option has the factor of $\\ell$ in the denominator instead of the numerator. This is the inverse of the correct scaling factor for length.\n    **Verdict: Incorrect.**\n\n*   **D. Use $\\tilde{E}(\\mathbf{R})=\\left(E(\\mathbf{R})-\\mu_{E}\\right)/\\sigma_{E}$, $\\tilde{\\mathbf{R}}=\\mathbf{R}/\\ell$, and set $\\tilde{\\mathbf{F}}(\\mathbf{R})=\\left(\\ell/\\sigma_{E}\\right)\\left(\\mathbf{F}(\\mathbf{R})-\\bar{\\mathbf{F}}\\right)$, where $\\bar{\\mathbf{F}}$ is the sample mean of the forces over the training set.**\n    Our derivation shows that $-\\nabla_{\\tilde{\\mathbf{R}}}\\tilde{E}(\\tilde{\\mathbf{R}}) = (\\ell/\\sigma_{E})\\mathbf{F}(\\mathbf{R})$. For this option to be correct, we would require $(\\ell/\\sigma_{E})\\mathbf{F}(\\mathbf{R}) = (\\ell/\\sigma_{E})(\\mathbf{F}(\\mathbf{R}) - \\bar{\\mathbf{F}})$, which implies $\\bar{\\mathbf{F}}=\\mathbf{0}$. The mean force over a general training set is not guaranteed to be zero. Therefore, subtracting a non-zero mean force $\\bar{\\mathbf{F}}$ would violate the energy-force consistency requirement.\n    **Verdict: Incorrect.**\n\nThe only protocol that correctly preserves the energy-force relationship is presented in option A.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "How can we be confident in our model's performance on truly unseen data? This question is particularly challenging when training on data from molecular dynamics simulations, where configurations are highly correlated in time . This practice challenges you to think critically about train-test splitting, analyzing why simple random splits provide overly optimistic results and identifying validation strategies that respect the temporal structure of the data to avoid information leakage.",
            "id": "3867275",
            "problem": "Consider constructing a generalization evaluation protocol for Gaussian Process (GP) regression of a Potential Energy Surface (PES) in computational catalysis, trained on configurations sampled from Molecular Dynamics (MD). Suppose you have $M=4$ independent MD trajectories, each with $N=2000$ saved configurations (“frames”) sampled at a fixed time interval $\\Delta t_s=0.5\\,\\mathrm{ps}$. Along any single trajectory, the time-autocorrelation function of the PES value is empirically well fit by an exponential decay, $C(\\Delta t)=\\exp\\left(-\\lvert \\Delta t\\rvert/\\tau\\right)$, with correlation time $\\tau=5\\,\\mathrm{ps}$. The GP regression uses a stationary kernel $k(\\mathbf{x},\\mathbf{x}')$ that increases with configuration similarity, so that strong time correlations along an MD trajectory imply strong kernel correlations between nearby frames.\n\nYour evaluation goal is to estimate out-of-sample error (for example, Root Mean Square Error (RMSE)) without information leakage between training and test sets. Assume that leakage occurs when any test frame is highly correlated to training frames along the same trajectory, quantified by the criterion that a train–test split is “leakage-safe” if the minimal temporal separation $s_{\\min}$ between any test frame and any training frame within the same trajectory satisfies $C(s_{\\min})\\le \\epsilon$, where $\\epsilon=0.1$ is a chosen threshold for “negligible correlation.”\n\nStarting from the definition of the autocorrelation function $C(\\Delta t)$ and the GP predictive variance formula $V(\\mathbf{x}_\\star\\mid \\mathcal{D})=k(\\mathbf{x}_\\star,\\mathbf{x}_\\star)-\\mathbf{k}_\\star^{\\top}\\mathbf{K}^{-1}\\mathbf{k}_\\star$, explain why time-nearby train and test frames lead to optimistic test error estimates, and determine the minimum separation $s_{\\min}$ required to meet $C(s_{\\min})\\le \\epsilon$ in this setting. Then, evaluate the following split strategies with respect to both the leakage-safe criterion and practical train–test balance, and select all strategies that are leakage-safe:\n\nA. Randomly shuffle all $M\\times N$ frames and assign $80\\%$ to training and $20\\%$ to testing.\n\nB. Group by trajectory and perform leave-one-trajectory-out evaluation: in each fold, hold out all frames from $1$ trajectory as the test set and train on the remaining $M-1$ trajectories.\n\nC. Within each trajectory, partition frames into contiguous “blocks” of length $L=250$ frames, select approximately $20\\%$ of blocks uniformly at random to form the test set, and enforce a “buffer” of $B=25$ frames around each test block that is excluded from training (i.e., frames within $B$ of any test block boundary are discarded from training).\n\nD. Assign alternating frames to training and testing across all trajectories (e.g., odd indices to training, even indices to testing).\n\nE. Stratify frames by their PES values into energy bins and split each bin $80\\%$–$20\\%$ into training and testing, ignoring trajectory membership, to preserve energy distribution similarity between train and test.\n\nYour answer should justify the selection by deriving $s_{\\min}$ from $C(\\Delta t)$ using first principles, and by explaining the impact of temporal correlations on GP predictive uncertainty. Choose all correct options.",
            "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the principles of statistical mechanics and machine learning as applied to computational chemistry. The scenario described—using Gaussian Process (GP) regression on Molecular Dynamics (MD) data to model a Potential Energy Surface (PES) and the associated challenges of time-series cross-validation—is a standard and important problem in the field. The problem is well-posed, providing all necessary constants and a clear, objective criterion for evaluation ($\\tau=5\\,\\mathrm{ps}$, $\\epsilon=0.1$, etc.). The premises and definitions are internally consistent and physically realistic.\n\nThe core of the problem is to devise a validation protocol that avoids information leakage, where the model is tested on data points that are highly correlated with the training data. This leads to overly optimistic error estimates that do not reflect the model's true generalization performance. We will first explain this phenomenon in the context of GP regression, then determine the minimum safe temporal separation, and finally evaluate each proposed splitting strategy against this criterion.\n\n**Impact of Temporal Correlation on GP Error Estimates**\n\nA Gaussian Process model's prediction for a new input $\\mathbf{x}_\\star$ is not just a single value but a probability distribution, characterized by a mean and a variance. The predictive variance for a test point $\\mathbf{x}_\\star$, given a training dataset $\\mathcal{D}$, is given by the formula:\n$$V(\\mathbf{x}_\\star\\mid \\mathcal{D}) = k(\\mathbf{x}_\\star,\\mathbf{x}_\\star) - \\mathbf{k}_\\star^{\\top}\\mathbf{K}^{-1}\\mathbf{k}_\\star$$\nHere, $k(\\mathbf{x},\\mathbf{x}')$ is the kernel function, $k(\\mathbf{x}_\\star,\\mathbf{x}_\\star)$ is the prior variance at the test point, $\\mathbf{K}$ is the covariance matrix of the training points, and $\\mathbf{k}_\\star$ is the vector of covariances between the test point and each training point.\n\nThe term $\\mathbf{k}_\\star^{\\top}\\mathbf{K}^{-1}\\mathbf{k}_\\star$ represents the reduction in variance (i.e., the increase in certainty) due to the information provided by the training data. The problem states that strong time correlations between MD frames imply strong kernel correlations. This means that if a test frame $\\mathbf{x}_\\star$ is very close in time to a training frame $\\mathbf{x}_i$, their configurations will be similar, and thus the kernel value $k(\\mathbf{x}_\\star, \\mathbf{x}_i)$ will be large. This large value is a component of the vector $\\mathbf{k}_\\star$.\n\nA large $\\mathbf{k}_\\star$ indicates high similarity between the test point and the training data. This generally leads to a large variance reduction term $\\mathbf{k}_\\star^{\\top}\\mathbf{K}^{-1}\\mathbf{k}_\\star$, which in turn makes the predictive variance $V(\\mathbf{x}_\\star\\mid \\mathcal{D})$ small. Small predictive variance signifies high model confidence. In a well-calibrated model, this high confidence correlates with a low prediction error (e.g., Root Mean Square Error, RMSE).\n\nTherefore, if the test set includes frames that are temporally close to frames in the training set, the GP model will make highly confident, low-error predictions for them simply by interpolating from their nearly identical neighbors in the training set. The resulting low test error is \"optimistic\" because it does not measure the model's ability to predict the PES for genuinely new, uncorrelated configurations. This is a classic case of information leakage.\n\n**Derivation of the Minimum Safe Separation ($s_{\\min}$)**\n\nTo prevent such leakage, the problem defines a \"leakage-safe\" criterion: the minimal temporal separation, $s_{\\min}$, between any test frame and any training frame within the same trajectory must be large enough such that their time-autocorrelation $C(s_{\\min})$ is below a threshold $\\epsilon$.\nThe given parameters are:\n- Autocorrelation function: $C(\\Delta t) = \\exp(-\\lvert \\Delta t\\rvert/\\tau)$\n- Correlation time: $\\tau = 5\\,\\mathrm{ps}$\n- Correlation threshold: $\\epsilon = 0.1$\n\nWe must find the minimum $s_{\\min} \\ge 0$ that satisfies $C(s_{\\min}) \\le \\epsilon$. We solve for the boundary case:\n$$C(s_{\\min}) = \\epsilon$$\n$$\\exp(-s_{\\min}/\\tau) = \\epsilon$$\nSubstituting the given values:\n$$\\exp(-s_{\\min}/5) = 0.1$$\nTaking the natural logarithm of both sides:\n$$-s_{\\min}/5 = \\ln(0.1)$$\n$$-s_{\\min}/5 = -\\ln(10)$$\n$$s_{\\min} = 5\\ln(10)\\,\\mathrm{ps}$$\nUsing the approximation $\\ln(10) \\approx 2.3026$, we find:\n$$s_{\\min} \\approx 5 \\times 2.3026\\,\\mathrm{ps} = 11.513\\,\\mathrm{ps}$$\nThus, any valid train-test splitting strategy must ensure that a test frame is separated from any training frame *from the same trajectory* by at least $11.513\\,\\mathrm{ps}$. The sampling interval is $\\Delta t_s = 0.5\\,\\mathrm{ps}$, which corresponds to a separation of $11.513\\,\\mathrm{ps} / (0.5\\,\\mathrm{ps}/\\text{frame}) \\approx 23.026$ frames. Therefore, a gap of at least $24$ frames is required.\n\n**Evaluation of Splitting Strategies**\n\nWe now evaluate each strategy against the criterion $s_{\\min} \\ge 11.513\\,\\mathrm{ps}$.\n\n**A. Randomly shuffle all $M\\times N$ frames and assign $80\\%$ to training and $20\\%$ to testing.**\nThis strategy completely ignores the temporal structure of the data. By randomly shuffling all $8000$ frames, it is virtually certain that for any given test frame with index $i$ in its original trajectory, its neighbors (frame $i-1$ and frame $i+1$) will be assigned to the training set. The minimal temporal separation between a train and test frame will therefore be the sampling interval, $s_{\\min}^{\\text{strategy}} = \\Delta t_s = 0.5\\,\\mathrm{ps}$.\nSince $0.5\\,\\mathrm{ps} \\ll 11.513\\,\\mathrm{ps}$, this strategy results in massive information leakage.\n**Verdict: Incorrect**\n\n**B. Group by trajectory and perform leave-one-trajectory-out evaluation.**\nIn this approach, the training set consists of frames from $M-1=3$ trajectories, and the test set consists of all frames from the remaining $1$ trajectory. The crucial point is that the training and test sets are composed of data from entirely independent MD simulations. There are no two frames, one in training and one in testing, that originate from the same trajectory. The intra-trajectory separation condition is therefore trivially satisfied, as the set of train-test pairs within the same trajectory is empty. This method robustly tests for generalization to different regions of the configuration space.\n**Verdict: Correct**\n\n**C. Within each trajectory, partition frames into contiguous “blocks” of length $L=250$ frames, select approximately $20\\%$ of blocks uniformly at random to form the test set, and enforce a “buffer” of $B=25$ frames around each test block that is excluded from training.**\nThis is a \"blocked\" or \"buffered\" cross-validation strategy. A test frame's closest possible training frame would be one located just outside the buffer zone of its test block. The buffer of $B=25$ frames on each side of a test block ensures a minimum separation. The minimal temporal separation in this case is determined by the size of the buffer:\n$$s_{\\min}^{\\text{strategy}} = B \\times \\Delta t_s = 25\\,\\text{frames} \\times 0.5\\,\\mathrm{ps}/\\text{frame} = 12.5\\,\\mathrm{ps}$$\nWe compare this to our required minimum separation: $12.5\\,\\mathrm{ps} > 11.513\\,\\mathrm{ps}$. The condition is met. The autocorrelation at this separation would be $C(12.5\\,\\mathrm{ps}) = \\exp(-12.5/5) = \\exp(-2.5) \\approx 0.082$, which is less than the threshold $\\epsilon = 0.1$.\n**Verdict: Correct**\n\n**D. Assign alternating frames to training and testing across all trajectories.**\nThis scheme assigns, for example, odd-indexed frames to training and even-indexed frames to testing. For any test frame at index $2k$, its immediate temporal neighbors in the trajectory, frames $2k-1$ and $2k+1$, are assigned to the training set. The minimal temporal separation is again the sampling interval, $s_{\\min}^{\\text{strategy}} = \\Delta t_s = 0.5\\,\\mathrm{ps}$.\nThis is far below the required $11.513\\,\\mathrm{ps}$ and leads to severe information leakage.\n**Verdict: Incorrect**\n\n**E. Stratify frames by their PES values into energy bins and split each bin $80\\%–$20\\% into training and testing, ignoring trajectory membership.**\nWhile well-intentioned to preserve the energy distribution of the dataset, this strategy explicitly \"ignor[es] trajectory membership.\" This means it operates on the global pool of frames, much like strategy A. Within each energy bin, the split is random. Since temporally adjacent frames have very similar energies, they will almost certainly fall into the same energy bin. A random $80\\%/20\\%$ split within that bin will very likely separate these adjacent frames into the train and test sets. The minimal temporal separation is therefore expected to be $s_{\\min}^{\\text{strategy}} = \\Delta t_s = 0.5\\,\\mathrm{ps}$. This fails the leakage-safe criterion.\n**Verdict: Incorrect**\n\nIn summary, only strategies B and C are \"leakage-safe\" based on the provided definition.",
            "answer": "$$\\boxed{BC}$$"
        }
    ]
}