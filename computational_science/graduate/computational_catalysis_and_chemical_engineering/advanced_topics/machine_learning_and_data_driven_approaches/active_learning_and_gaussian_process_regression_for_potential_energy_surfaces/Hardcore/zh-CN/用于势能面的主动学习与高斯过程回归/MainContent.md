## 引言
在[计算催化](@entry_id:165043)与材料科学领域，[势能面](@entry_id:143655)（Potential Energy Surface, PES）是理解和预测化学反应行为的核心。它描绘了系统能量随原子构型变化的复杂景观，其上的极小值点和鞍点分别对应着稳定结构和反应过渡态。然而，通过密度泛函理论（DFT）等[第一性原理方法](@entry_id:1125017)对整个高维[势能面](@entry_id:143655)进行精确计算，其成本之高昂往往令人望而却步，这构成了理论研究中的一个核心瓶颈。如何以更少的数据点，构建一个既能快速评估、又忠于底层物理的代理模型，成为推动该领域发展的关键问题。

本文旨在系统性地介绍一种强大的解决方案：将[主动学习](@entry_id:157812)（Active Learning）与高斯过程回归（Gaussian Process Regression, GPR）相结合，以数据高效的方式构建和探索[势能面](@entry_id:143655)。通过本文的学习，读者将掌握如何利用GPR的[贝叶斯不确定性](@entry_id:901285)量化能力，智能地指导计算资源的投入，从而在保证精度的前提下，最大限度地降低对昂贵DFT计算的依赖。

为实现这一目标，文章分为三个核心部分。首先，在**“原理与机制”**一章中，我们将深入GPR的数学基础，探讨如何通过核函数编码物理对称性，并介绍[主动学习](@entry_id:157812)循环的工作流程。接着，**“应用与交叉学科联系”**一章将展示该框架在真实科研场景中的强大威力，包括寻找反应路径、增强分子动力学模拟的可靠性，以及预测宏观[反应速率](@entry_id:185114)。最后，**“动手实践”**部分提供了一系列精心设计的计算练习，旨在帮助读者巩固理论知识，并将其转化为解决实际问题的能力。让我们从构建这一强大工具所需的理论基石开始。

## 原理与机制

本章旨在深入阐述构建催化[势能面](@entry_id:143655)（Potential Energy Surface, PES）代理模型的核心原理与机制。我们将从[势能面](@entry_id:143655)的物理基础出发，系统介绍作为核心统计工具的[高斯过程回归](@entry_id:276025)（Gaussian Process Regression, GPR），并详细探讨如何通过[主动学习](@entry_id:157812)（Active Learning）策略高效地构建精确的PES模型。本章内容假定读者已对计算催化和[势能面](@entry_id:143655)的基本概念有所了解。

### [势能面](@entry_id:143655)的物理学描述

在深入探讨机器学习方法之前，我们必须首先精确地定义我们的目标——[势能面](@entry_id:143655)，并理解其必须遵循的基本物理对称性。

#### [玻恩-奥本海默近似](@entry_id:146252)

量子力学中的原子和分子系统由原子核和电子组成。由于原子核的质量远大于电子，其运动速度也远慢于电子。**[玻恩-奥本海默近似](@entry_id:146252) (Born-Oppenheimer approximation)** 正是基于这一事实，它将原子核与电子的运动[解耦](@entry_id:160890)。在该近似下，我们可以认为原子核在空间中的位置是固定的，然后求解该[固定核](@entry_id:169539)构型下电子的[定态](@entry_id:137260)薛定谔方程。电子系统的基态能量，再加上原子核之间的[库仑排斥](@entry_id:181876)能，共同构成了在该特定核构型下的总能量。

这个总能量是原子核坐标 $\mathbf{R} \in \mathbb{R}^{3N}$（其中 $N$ 为系统中的原子核总数）的函数，即**[势能面](@entry_id:143655)** $U(\mathbf{R})$。它为原子核的运动提供了一个有效的势场。因此，PES的构建、探索和理解是[理论化学](@entry_id:199050)与催化研究的核心任务，因为它决定了化学反应的路径、过渡态和产物。我们的目标便是利用机器学习方法，根据少数几个构型上的能量（有时还包括力）的昂贵量子[化学计算](@entry_id:155220)结果，构建一个能够精确、快速预测任意构型下能量的代理模型。

#### 催化系统中的[势能面](@entry_id:143655)定义

在[多相催化](@entry_id:139401)研究中，一个典型的系统是吸附物（adsorbate）与周期性金属表面（slab）的相互作用。使用[平面波基组](@entry_id:178287)的[电子结构计算](@entry_id:748901)方法（如[密度泛函理论](@entry_id:139027)，DFT）是模拟这类系统的标准手段。为了模拟一个二维无限延伸的表面，我们构建一个三维的**超胞（supercell）**，它由三个[晶格矢量](@entry_id:161583) $\mathbf{a}_1, \mathbf{a}_2, \mathbf{a}_3$ 定义。其中，$\mathbf{a}_1$ 和 $\mathbf{a}_2$ 位于表面平面内，而 $\mathbf{a}_3$ 垂直于表面。为了避免 slab 在 $\mathbf{a}_3$ 方向上与其周期性镜像发生伪相互作用，通常会在超胞中引入一个足够厚的**真空层（vacuum region）**。因此，整个系统在计算上遵循三维周期性边界条件（Periodic Boundary Conditions, PBC），但在物理上模拟的是一个在 $\mathbf{a}_1$ 和 $\mathbf{a}_2$ 方向上具有周期性，而在 $\mathbf{a}_3$ 方向上孤立的表面。

在一个典型的表面科学模型中，为了降低计算成本，通常会施加一些合理的约束。例如，[晶格矢量](@entry_id:161583) $(\mathbf{a}_1, \mathbf{a}_2, \mathbf{a}_3)$ 通常被固定。此外，由于表面的扰动（如吸附物的存在）对深层衬底原子的影响会迅速衰减，因此通常只允许吸附物和表层（如顶上一至三层）的[原子弛豫](@entry_id:168503)（即自由移动），而将更深层的衬底原子**固定（constrained）**在其体相材料的[平衡位置](@entry_id:272392)上。

因此，为这类系统构建的PES代理模型，其输入坐标需要正确处理这些边界条件和约束。对于允许弛豫的原子，其在平面内（$x, y$ 方向）的位移是周期性的。当计算两个构型之间的距离时，必须采用**[最小镜像约定](@entry_id:142070)（minimum-image convention）**，即考虑一个原子与其在相邻晶胞中的镜像之间的最短距离。而垂直于表面的 $z$ 坐标则被视为[非周期性](@entry_id:275873)的 。

#### [基本对称性](@entry_id:161256)

[势能面](@entry_id:143655)本质上描述的是原子间的相对位置关系所决定的能量，因此它必须遵循一些基本的物理对称性。任何一个有效的PES模型或其输入表示（描述符），都必须内在地尊重这些对称性。在没有外场的情况下，这些对称性包括：

1.  **[平移不变性](@entry_id:195885) (Translational Invariance)**：将整个系统（包括吸附物和表面）在空间中进行任意刚性平移 $\mathbf{t} \in \mathbb{R}^3$，即对所有原子坐标 $\mathbf{r}_i$ 进行 $\mathbf{r}_i \mapsto \mathbf{r}_i + \mathbf{t}$ 的变换，系统的势能保持不变。这是因为势能仅依赖于原子间的相对距离。

2.  **旋转不变性 (Rotational Invariance)**：将整个系统围绕任意原点进行刚性旋转，系统的势能保持不变。一个三维[正常旋转](@entry_id:141831)可以用一个特殊[正交矩阵](@entry_id:169220) $\mathbf{R} \in \mathrm{SO}(3)$ 来描述，该矩阵满足 $\mathbf{R}^\top \mathbf{R} = \mathbf{I}$ 且 $\det(\mathbf{R}) = 1$。变换规则为 $\mathbf{r}_i \mapsto \mathbf{R}\,\mathbf{r}_i$。此变换保持所有原子间距离不变，因此能量不变。

3.  **[置换不变性](@entry_id:753356) (Permutational Invariance)**：同种类的原子是不可区分的。因此，如果我们交换系统中任意两个相同元素的原子（例如，两个金属原子，或一个吸附分子中的两个相同原子）的标签（或索引），系统的总能量必须保持不变。例如，对于吸附在表面上的 $\mathrm{H}_2$ 分子，交换两个氢原子的坐标，得到的构型与原构型在物理上是等价的，其能量也必须相同 。

这些对称性是构建任何PES代理模型时必须遵守的基本法则。我们将在后续章节看到，如何通过精心设计的描述符和[核函数](@entry_id:145324)将这些先验知识融入到高斯过程模型中。

### 作为代理模型的[高斯过程回归](@entry_id:276025)

高斯过程回归（GPR）是一种强大而灵活的非参数贝叶斯方法，非常适合用于建模[势能面](@entry_id:143655)。它不仅能给出能量预测，还能提供预测的不确定性，这对于主动学习至关重要。

#### 从[函数空间](@entry_id:143478)看贝叶斯方法

为了理解GPR的本质，我们首先对比一个更传统的**[参数化](@entry_id:265163)模型（parametric model）**，例如[贝叶斯线性回归](@entry_id:634286)。在一个[参数化](@entry_id:265163)模型中，我们预先假设函数 $V(\mathbf{x})$ 具有一个固定的数学形式，例如一组基函数的[线性组合](@entry_id:154743) $V_{\mathrm{P}}(\mathbf{x}) = \sum_{j=1}^{p} w_j \phi_j(\mathbf{x})$。在这里，模型的复杂性由基函数的数量 $p$ 事先固定。[贝叶斯方法](@entry_id:914731)为权重 $\mathbf{w} \in \mathbb{R}^p$ 设置一个[先验分布](@entry_id:141376)（例如，高斯分布），然后根据观测数据计算其后验分布。该模型的所有函数 realization 都被限制在由这 $p$ 个基函数张成的有限维函数空间内。

与此不同，GPR是一种**非[参数化](@entry_id:265163)模型（nonparametric model）**。它不直接为有限个参数设置先验，而是直接在函数空间上定义一个先验分布。一个**高斯过程（Gaussian Process, GP）**被定义为“任意有限个[随机变量](@entry_id:195330)的集合都服从[联合高斯分布的](@entry_id:636452)[随机变量](@entry_id:195330)的集合”。换句话說，一个GP是对一个函数 $f(\mathbf{x})$ 的先验信念的编码。这个先验由一个**[均值函数](@entry_id:264860)** $m(\mathbf{x})$ 和一个**协方差函数（或核函数）** $k(\mathbf{x}, \mathbf{x}')$ 完全确定，记为：
$$
f(\mathbf{x}) \sim \mathcal{GP}(m(\mathbf{x}), k(\mathbf{x}, \mathbf{x}'))
$$
这里的“非参数”并不意味着模型没有参数（核函数通常有超参数），而是指模型的复杂性不是预先固定的，而是可以随着数据量的增加而增长。例如，许多常用的[核函数](@entry_id:145324)（如[径向基函数核](@entry_id:166868)）对应于一个无限维的[特征空间](@entry_id:638014)。这使得GPR具有极大的灵活性，能够拟合非常复杂的函数 。

有趣的是，任何具有高斯权重先验的[贝叶斯线性回归](@entry_id:634286)模型都可以被看作一个高斯过程。其协方差函数由基函数和权重协方差矩阵确定：$k(\mathbf{x}, \mathbf{x}') = \boldsymbol{\phi}(\mathbf{x})^\top \Sigma_p \boldsymbol{\phi}(\mathbf{x}')$。然而，这个GP的样本路径被限制在由基函数张成的[有限维空间](@entry_id:151571)中，因此它仍然是[参数化](@entry_id:265163)的。只有当核函数对应的[特征空间](@entry_id:638014)是无限维时，GPR才真正体现出其非参数的特性 。

#### GP先验：均值与[协方差函数](@entry_id:265031)

GP先验由[均值函数](@entry_id:264860)和协方差函数共同定义，它们各自扮演着不同的角色：

- **[均值函数](@entry_id:264860) $m(\mathbf{x})$**：它代表了我们对函数 $f(\mathbf{x})$ 在看到任何数据之前的[期望值](@entry_id:150961)，即 $\mathbb{E}[f(\mathbf{x})] = m(\mathbf{x})$。在PES建模中，通常将其设为零，表示我们对能量的绝对值没有先验偏好。然而，如果存在一个廉价的、物理意义明确的基线模型（例如，一个[经验力场](@entry_id:1124410)），我们可以将其用作[均值函数](@entry_id:264860)，让GP专注于学习与基线模型的偏差。重要的是，[均值函数](@entry_id:264860)只影响后验预测的均值，而**不影响后验协方差（即不确定性）**。

- **协方差函数 $k(\mathbf{x}, \mathbf{x}')$**：也称为**核函数（kernel）**，它是GPR的核心。它编码了我们对函数性质的先验信念。
    - **相关性**: $k(\mathbf{x}, \mathbf{x}')$ 度量了函数在两个不同点 $\mathbf{x}$ 和 $\mathbf{x}'$ 处的函数值 $f(\mathbf{x})$ 和 $f(\mathbf{x}')$ 之间的协方差。如果 $\mathbf{x}$ 和 $\mathbf{x}'$ 在[核函数](@entry_id:145324)定义的度量下“接近”，则它们的函数值被认为是高度相关的。
    - **[光滑性](@entry_id:634843)**: 核函数的选择决定了从GP先验中抽样出的函数的光滑程度。例如，[平方指数核](@entry_id:191141)（Squared Exponential, SE）生成的函数是无穷次可微的（非常光滑）。
    - **方差**: 对角线上的值 $k(\mathbf{x}, \mathbf{x})$ 给出了在 $\mathbf{x}$ 处的先验方差，即函数值的变化幅度。
    - **[正定性](@entry_id:149643)**: 一个合法的[协方差函数](@entry_id:265031)必须是**正定（positive definite）**的，即对于任意一组输入点，它生成的协方差矩阵必须是半正定的。

在[主动学习](@entry_id:157812)中，正是这个[协方差函数](@entry_id:265031)决定了信息如何从训练点传播到未知的[构型空间](@entry_id:149531)，并决定了后验不确定性的结构。采集函数依赖于由核函数和训练点位置决定的后验方差来做出决策 。在没有任何数据的情况下，[后验分布](@entry_id:145605)就等于先验分布，即后验均值和协方差分别等于 $m(\mathbf{x})$ 和 $k(\mathbf{x}, \mathbf{x}')$ 。

#### [核函数](@entry_id:145324)：编码物理先验

为PES选择或设计一个合适的[核函数](@entry_id:145324)是至关重要的一步，因为它直接将我们的物理先验知识编码到模型中。

##### 对称性保持描述符

如前所述，PES具有平移、旋转和置換[不变性](@entry_id:140168)。然而，标准的[核函数](@entry_id:145324)（如SE核）通常作用于欧几里得空间中的向量，它们本身并不具备这些不变性。一个标准且强大的解决方案是，不直接将原子坐标输入GPR模型，而是先将原子坐标 $\mathbf{R}$ 映射到一个**描述符（descriptor）**向量 $\mathbf{d}(\mathbf{R})$，该描述符在设计上就满足这些对称性。然后，GPR模型作用于这个描述符空间：$f(\mathbf{d}) \sim \mathcal{GP}(m(\mathbf{d}), k(\mathbf{d}, \mathbf{d}'))$。

一个优秀的描述符需要满足以下条件：
1.  **满足对称性**: 描述符本身对平移、旋转和置换操作是不变的。
2.  **独特性**: 两个物理上不等价的原子环境应映射到不同的描述符向量。
3.  **光滑性**: 描述符 $\mathbf{d}(\mathbf{R})$ 关于原子坐标 $\mathbf{R}$ 的映射应该是连续可微的，这样才能计算出连续的力。

一种被广泛采用的方法是**[Behler-Parrinello](@entry_id:177243)[对称函数](@entry_id:177113)（Symmetry Functions）**。这种方法通过构建径向和角向的函数来描述中心原子周围的化学环境。例如，一个简单的[径向对称](@entry_id:141658)函数可以定义为对中心原子（如吸附物中的C原子）与所有邻近的同种原子（如金属原子M）之间关系的求和：
$$
G_{\mathrm{C},m} = \sum_{i \in \mathcal{N}_{\mathrm{M}}} f_c(r_{i\mathrm{C}})\,\phi_m(r_{i\mathrm{C}})
$$
其中，$\mathcal{N}_{\mathrm{M}}$ 是截断半径 $r_c$ 内的金属原子邻居集合，$r_{i\mathrm{C}}$ 是第 $i$ 个金属原子与碳原子之间的距离。$f_c(r)$ 是一个平滑的截断函数，确保当原子进出[截断半径](@entry_id:136708)时描述符平滑变化。$\phi_m(r)$ 是一组[径向基函数](@entry_id:754004)（如[高斯函数](@entry_id:261394)），用于分辨不同距离的贡献。由于求和操作不依赖于项的顺序，这个构造天然地满足了对金属原子 $i$ 的**置換不变性**。通过组合多种不同类型的径向和角向[对称函数](@entry_id:177113)，可以构建一个足够表达力的描述符向量 $\mathbf{d}$，它能够唯一地表示各种复杂的局部原[子环](@entry_id:154194)境，同时保持[光滑性](@entry_id:634843)和所有必要的对称性 。

##### [核函数](@entry_id:145324)超参数与[自动相关性确定](@entry_id:746592)

一旦我们有了描述符，下一步就是选择作用于描述符的[核函数](@entry_id:145324)。一个常用且功能强大的选择是带有**[自动相关性确定](@entry_id:746592)（Automatic Relevance Determination, ARD）**的平方指数（SE）核：
$$
k(\mathbf{x}, \mathbf{x}') = \sigma_{f}^{2}\exp\left(-\frac{1}{2}\sum_{j=1}^{d}\frac{(x_{j}-x'_{j})^{2}}{\ell_{j}^{2}}\right)
$$
这里的 $\mathbf{x}$ 是 $d$ 维描述符向量。这个[核函数](@entry_id:145324)有两类超参数：
- **信号方差 $\sigma_f^2$**: 控制了函数值的总体变化幅度。
- **长度尺度 $\ell_j$**: 每个描述符分量 $x_j$ 都有其自己独立的长度尺度 $\ell_j$。$\ell_j$ 控制了函数沿该维度变化的快慢。

ARD长度尺度的意义非常深远。一个**较小的 $\ell_j$** 意味着函数对描述符分量 $x_j$ 的变化非常敏感，即使 $x_j$ 只有微小的改变，函数值也可能发生剧烈变化。相反，一个**较大的 $\ell_j$** 意味着函数沿该维度变化平缓，对 $x_j$ 的变化不敏感，表明该描述符分量与能量的相关性较低。实际上，ARD机制可以自动“发现”哪些描述符分量对预测能量是重要的。

这种关系可以被更精确地量化。对于一个SE-ARD核，函数对 $x_j$ 的偏导数的先验方差为 $\mathrm{Var}\! \left(\frac{\partial f}{\partial x_{j}}(\mathbf{x})\right) = \sigma_{f}^{2}/\ell_{j}^{2}$。这清晰地表明，$\ell_j$越小，函数在该方向上的期望斜率（变化率）越大。从几何上看，ARD核相当于在描述符空间中使用了一个对角化的马氏距离度量 $\sum_{j=1}^{d}(x_{j}-x'_{j})^{2}/\ell_{j}^{2}$，它通过学习到的长度尺度来拉伸或压缩空间，从而使GPR模型关注于那些真正重要的维度 。

#### 模型选择与[超参数优化](@entry_id:168477)

我们如何确定[核函数](@entry_id:145324)的超参数 $\boldsymbol{\theta}$（例如 $\sigma_f^2$ 和所有的 $\ell_j$）呢？在贝葉斯框架下，一个原则性的方法是最大化**[边际似然](@entry_id:636856)（marginal likelihood）**，也称为**证据（evidence）**。[边际似然](@entry_id:636856) $p(y \mid X, \boldsymbol{\theta})$ 是在给定模型超参数 $\boldsymbol{\theta}$ 的情况下，观测到数据 $y$ 的概率。它通过对所有可能的函数 $f$ 进行积分得到：$p(y \mid X, \boldsymbol{\theta}) = \int p(y \mid f, X) p(f \mid X, \boldsymbol{\theta}) df$。

对于一个零均值GP模型，其在训练点 $X$ 上的观测值 $y$ 服从一个多元高斯分布 $y \sim \mathcal{N}(0, K_{\boldsymbol{\theta}})$，其中 $K_{\boldsymbol{\theta}}$ 是由核函数和训练点构成的[协方差矩阵](@entry_id:139155)（包含噪声项）。其对数[边际似然](@entry_id:636856)（log marginal likelihood, LML）为：
$$
\log p(y \mid X, \boldsymbol{\theta}) = -\frac{1}{2} y^{\top} K_{\boldsymbol{\theta}}^{-1} y - \frac{1}{2} \log |K_{\boldsymbol{\theta}}| - \frac{n}{2} \log 2\pi
$$
这个表达式由三个项组成，它们之间形成了一种精妙的平衡：
1.  **[数据拟合](@entry_id:149007)项**: $-\frac{1}{2} y^{\top} K_{\boldsymbol{\theta}}^{-1} y$。这一项度量了模型对观测数据的拟合程度。如果模型的预测与真实数据 $y$ 吻合得很好，这一项的值会更接近于零（即惩罚更小）。
2.  **[模型复杂度惩罚](@entry_id:752069)项**: $-\frac{1}{2} \log |K_{\boldsymbol{\theta}}|$。协方差[矩阵的行列式](@entry_id:148198) $|K_{\boldsymbol{\theta}}|$ 可以被看作是先验能够生成的函数空间的“体积”。一个非常灵活、复杂的模型（例如，具有非常大的长度尺度）会有较大的行列式。这一项对模型的复杂度进行惩罚，从而[防止过拟合](@entry_id:635166)。
3.  **[归一化常数](@entry_id:752675)**: $-\frac{n}{2} \log 2\pi$。这一项与超参数 $\boldsymbol{\theta}$ 无关，在优化过程中可以忽略。

最大化LML的过程，本质上是在寻找一组超参数 $\boldsymbol{\theta}$，使得模型在**拟合数据**和**保持简单**之间达到最佳平衡。这体现了**[奥卡姆剃刀](@entry_id:142853)原理（Occam's razor）**：在所有能够同样好地解释数据的模型中，我们应该选择最简单的那一个。通过梯度上升等[优化算法](@entry_id:147840)找到最优的 $\boldsymbol{\theta}$ 后，我们就得到了一个经过“训练”的GP模型，它可以用于后续的预测和[主动学习](@entry_id:157812)决策 。

### 为催化应用增强GP模型

标准的GPR模型已经非常强大，但我们可以通过一些专门的扩展来使其更适用于催化[势能面](@entry_id:143655)的建模，特别是通过引入力的信息和精细化不确定性的理解。

#### 引入力：多输出高斯过程

原子间的力是[势能面](@entry_id:143655)对坐标的负梯度，$\mathbf{F}_i = -\nabla_{\mathbf{r}_i} E(\mathbf{R})$。力信息在PES构建中极为宝贵，因为单次DFT计算通常能同时提供能量和所有原子的力分量，这相当于提供了 $3N+1$ 个数据点。同时利用能量和力的数据进行训练，可以极大地提高模型的的数据效率和准确性。

我们可以将能量和力的预测视为一个**多输出（multi-output）**或**多任务（multi-task）**学习问题。由于力是能量的导数，能量和力这两个任务（或输出）之间存在着严格的物理关联。我们可以利用GP的一个重要性质来构建这样的模型：**高斯过程在線性算子下是封閉的**。如果 $f(\mathbf{x})$ 是一个GP，那么它的导数 $\frac{\partial f}{\partial x_i}$ 也是一个GP。

我们可以将能量和每个力分量看作是一个共享的潜在GP $u(\mathbf{x}) \sim \mathcal{GP}(0, k(\mathbf{x}, \mathbf{x}'))$ 经过不同[线性算子](@entry_id:149003)作用的结果：
- 能量任务: $E(\mathbf{x}) = L_E u(\mathbf{x}) = I u(\mathbf{x})$ ([恒等算子](@entry_id:204623))
- 第 $i$ 个力分量任务: $F_i(\mathbf{x}) = L_{F_i} u(\mathbf{x}) = -\frac{\partial}{\partial x_i} u(\mathbf{x})$

利用GP的导数性质，我们可以推导出不同输出之间的协方差。例如，能量和力分量 $F_j$ 之间的协方差为：
$$
\mathrm{Cov}(E(\mathbf{x}), F_j(\mathbf{x}')) = \mathrm{Cov}(I u(\mathbf{x}), -\frac{\partial}{\partial x'_j} u(\mathbf{x}')) = -\frac{\partial}{\partial x'_j} k(\mathbf{x}, \mathbf{x}')
$$
类似地，两个力分量 $F_i$ 和 $F_j$ 之间的协方差为：
$$
\mathrm{Cov}(F_i(\mathbf{x}), F_j(\mathbf{x}')) = \mathrm{Cov}(-\frac{\partial}{\partial x_i} u(\mathbf{x}), -\frac{\partial}{\partial x'_j} u(\mathbf{x}')) = \frac{\partial^2}{\partial x_i \partial x'_j} k(\mathbf{x}, \mathbf{x}')
$$
通过这种方式，我们可以构建一个包含所有能量-能量、能量-力、力-力协方差的巨大的联合[协方差矩阵](@entry_id:139155)。为了更灵活地控制不同任务（能量、不同方向的力）之间的相关性，可以使用更高级的模型，如**内在共区域化模型（Intrinsic Coregionalization Model, ICM）**。在ICM框架下，能量和每个力分量都被视为独立但相关的输出任务。它们的联合协方差结构不仅包含了梯度关系，还通过一个可学习的**共区域化矩阵（coregionalization matrix）**来调整任务间的相关强度。这允许模型例如学习到能量和力的预测噪声水平不同，或者不同力分量之间的相关性。通过将物理约束（力是能量的梯度）与数据驱动的任务相关性学习相结合，这种方法能更稳健地构建[势能面](@entry_id:143655)，确保了预测的能量和力之间的一致性，并显著提升了学习效率 。

#### 量化与分解不确定性

GPR的一个核心优势是它能够提供预测的不确定性。对于一个测试点 $\mathbf{R}_*$, GP的预测是一个完整的高斯分布 $p(y_* | \mathcal{D}, \mathbf{R}_*) = \mathcal{N}(y_* | \mu_*(\mathbf{R}_*), \sigma_*^2(\mathbf{R}_*))$。这里的预测方差 $\sigma_*^2(\mathbf{R}_*)$ 是总不确定性的度量。然而，这种不确定性可以被分解为两种不同的来源：

1.  **认知不确定性 (Epistemic Uncertainty)**: 也称为**模型不确定性**，它源于我们数据的有限性。在数据稀疏的区域，模型“不知道”函数应该是什么样子，因此不确定性很高。这种不确定性是可以通过增加更多信息丰富的数据点来**减小**的。主动学习的主要目标就是通过智能采样来有效地减小认知不确定性。

2.  **[偶然不确定性](@entry_id:634772) (Aleatoric Uncertainty)**: 也称为**数据不确定性**或**噪声**，它源于数据观测过程本身固有的随机性或噪声。对于DFT计算而言，这可以来自SCF收敛的数值误差、不同的初始猜测等。这种不确定性是**不可约减的**（在固定的计算设置下）。即使在同一个构型上重复进行测量，每次得到的结果也会有微小的差异。

在标准的GP模型中，[偶然不确定性](@entry_id:634772)被建模为一个全局的噪声参数 $\sigma_n^2$。总的预测方差可以概念性地分解为 $\sigma_*^2(\mathbf{R}_*) = \sigma_f^2(\mathbf{R}_*) + \sigma_n^2$，其中 $\sigma_f^2(\mathbf{R}_*)$ 是潜在函数 $f$ 的后验方差（代表认知不确定性），而 $\sigma_n^2$ 是噪声方差（代表[偶然不确定性](@entry_id:634772)）。

区分这两种不确定性对于高效的主动学习至关重要。如果一个区域的总不确定性很高，但主要是由[偶然不确定性](@entry_id:634772)主导，那么在该区域进行更多的采样将是徒劳的，因为不确定性无法被显著降低。相反，我们应该优先在认知不确定性高的区域进行采样。

一个有效的诊断方法是，对于一个模型报告高不确定性的构型 $\mathbf{R}_i$，进行 $m$ 次独立的[DFT计算](@entry_id:1123635)（例如，使用不同的初始[波函数](@entry_id:201714)猜测）。然后，我们可以计算这些重复测量结果 $\\{y_{i,1}, \dots, y_{i,m}\\}$ 的样本方差 $\hat{\sigma}_n^2(\mathbf{R}_i)$。这个值直接估计了在该点的局部[偶然不确定性](@entry_id:634772)。
- 如果 $\hat{\sigma}_n^2(\mathbf{R}_i)$ 很小，但总预测方差 $\sigma_*^2(\mathbf{R}_i)$很大，这表明不确定性主要是**认知性**的。在该点附近增加新的数据点将会显著降低不确定性。
- 如果 $\hat{\sigma}_n^2(\mathbf{R}_i)$ 本身就很大，占据了总预测方差的主要部分，那么不确定性主要是**偶然性**的。

更进一步，这些重复测量的结果可以用来训练一个**异方差（heteroscedastic）GP模型**。在这种模型中，噪声方差 $\sigma_n^2$ 不再是一个全局常数，而是输入构型 $\mathbf{R}$ 的函数，即 $\sigma_n^2(\mathbf{R})$。我们可以用另一个GP来建模这个噪声函数。这样，模型就能够自动地将总[不确定性分解](@entry_id:183314)为位置依赖的认知不确定性和[偶然不确定性](@entry_id:634772)，为[主动学习](@entry_id:157812)提供更精细的指导 。

### [主动学习](@entry_id:157812)引擎

有了功能完备且经过物理约束的GPR模型，我们就可以将其整合到一个自动化的工作流中，以最少的昂贵计算来构建PES，这就是[主动学习](@entry_id:157812)。

#### [主动学习](@entry_id:157812)循环

一个典型的主动学习循环包含以下核心步骤：

1.  **初始化**:
    -   **初始数据集 $\mathcal{D}_0$**: 过程始于一个小的、初始的标记数据集，包含一组构型及其DFT计算的能量（和力）。这些初始点可以随机选择，或者基于化学直觉和已有知识来选取。
    -   **GP模型定义**: 指定GP的[均值函数](@entry_id:264860)、[核函数](@entry_id:145324)（包括其超参数 $\boldsymbol{\theta}$ 的先验）以及噪声模型。
    -   **候选池 $\mathcal{U}_0$**: 生成一个巨大的、未标记的候选构型池。这些构型应该是物理上合理的，可以通过在初步的PES上运行[分子动力学模拟](@entry_id:160737)、进行随机结构搜索或几何扰动等方法获得。
    -   **预算与阈值**: 设定总的计算预算（例如，最多进行 $B$ 次[DFT计算](@entry_id:1123635)）和[停止准则](@entry_id:136282)的阈值。

2.  **迭代循环 (在第 $t$ 步)**:
    -   **模型训练**: 使用当前所有已标记的数据 $\mathcal{D}_t$，通过最大化对数[边际似然](@entry_id:636856)来优化GP的超参数 $\boldsymbol{\theta}$。
    -   **预测**: 在整个候选池 $\mathcal{U}_t$ 中的所有未标记点上，使用训练好的GP模型计算[后验预测分布](@entry_id:167931)（即预测均值 $\mu_t(\mathbf{x})$ 和预测方差 $\sigma_t^2(\mathbf{x})$）。
    -   **查询选择**: 根据预测的均值和方差，为候选池中的每个点计算一个**[采集函数](@entry_id:168889)（acquisition function）** $\alpha(\mathbf{x})$ 的值。采集函数的作用是评估对某个点进行标记的“价值”。
    -   **批次选择**: 选出一个或一批（batch）大小为 $b$ 的、具有最高采集函数值的候选点。在选择批次时，通常需要引入**多样性（diversity）**机制，以避免选出的点在空间上过于聚集，从而最大化每批次查询的[信息量](@entry_id:272315)。
    -   **Oracle查询**: 对选出的批次中的构型进行昂贵的DFT计算，以获得它们的“真实”标签（能量和力）。
    -   **[数据增强](@entry_id:266029)**: 将新标记的数据点加入训练集，形成 $\mathcal{D}_{t+1} = \mathcal{D}_t \cup \\{\text{新标记的点}\\}$。

3.  **终止**:
    -   循环持续进行，直到满足一个或多个**[停止准则](@entry_id:136282)**。这些准则必须是基于已知信息，而不能依赖于未知的真实能量。常见的[停止准则](@entry_id:136282)包括：
        -   计算预算耗尽。
        -   模型性能收敛：例如，在留出[验证集](@entry_id:636445)上的[预测误差](@entry_id:753692)达到稳定或低于某个阈值。
        -   信息增益递减：采集函数的最大值低于某个阈值，表明再进行新的计算所能带来的信息收益已经很小。
        -   全局不确定性降低：整个候选池上的平均或总后验方差低于某个阈值。

这个循环通过迭代地“询问”模型在何处最不确定或最有希望找到重要特征（如能量最低点），并有针对性地进行昂贵的计算，从而能够以远低于暴力网格扫描的成本，构建出一个覆盖所有关键区域的高精度PES模型 。

#### [采集函数](@entry_id:168889)

[采集函数](@entry_id:168889)是主动学习循环的核心决策模块。它指导着“下一步应该在哪里采样”的问题。一个好的[采集函数](@entry_id:168889)能够在**探索（exploration）**和**利用（exploitation）**之间取得平衡。
- **探索**: 在模型不确定性高（即认知不确定性大）的区域进行采样，以减小未知，完善全局模型。
- **利用**: 在模型预测有希望的区域（例如，预测能量低的区域）进行采样，以快速找到[反应路径](@entry_id:163735)上的最小值和过渡态。

常见的[采集函数](@entry_id:168889)包括：
- **最大方差 (Maximum Variance)**: $\alpha(\mathbf{x}) = \sigma_t^2(\mathbf{x})$。这是一种纯粹的探索策略。
- **[期望提升](@entry_id:749168) (Expected Improvement, EI)**: 旨在找到比当前已知的最优值 $y_{best}$ 更好的点。它计算了新采样点的值低于 $y_{best}$ 的期望量。
- **[置信上界](@entry_id:178122) (Upper Confidence Bound, UCB)**: $\alpha(\mathbf{x}) = -\mu_t(\mathbf{x}) + \beta \sigma_t(\mathbf{x})$ (对于最小化问题)。通过[调节参数](@entry_id:756220) $\beta$，可以在利用（最小化 $\mu_t$）和探索（最大化 $\sigma_t$）之间进行权衡。

通过本章的学习，我们已经系统地建立了从第一性原理出发，利用GPR和[主动学习](@entry_id:157812)来高效构建催化[势能面](@entry_id:143655)的理论框架。这个框架不仅在计算上高效，而且在物理上严谨，为现代计算催化研究提供了强大的理论工具。