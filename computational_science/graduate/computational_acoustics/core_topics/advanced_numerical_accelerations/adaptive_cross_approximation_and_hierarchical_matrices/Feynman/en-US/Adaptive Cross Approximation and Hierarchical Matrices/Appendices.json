{
    "hands_on_practices": [
        {
            "introduction": "Before diving into the complex algorithms behind Hierarchical Matrices, it's essential to understand *why* they are a cornerstone of modern computational methods. This practice provides a crucial \"back-of-the-envelope\" calculation to contrast the prohibitive cost of classical dense matrix methods with the efficiency of a hierarchical approach . By working through the memory and computational scaling, you will gain a tangible appreciation for the orders-of-magnitude savings that motivate the development of these advanced data-sparse structures.",
            "id": "4115706",
            "problem": "A boundary integral discretization of the three-dimensional time-harmonic acoustic Helmholtz equation yields a dense linear system with $N=10^{5}$ boundary unknowns. The resulting system matrix is complex-valued, and a single matrix entry is stored in double-precision complex format, occupying $16$ bytes. Consider two assembly-and-storage strategies:\n\n1. Dense assembly and storage:\n   - The matrix is fully assembled by evaluating the Green’s function kernel for all source–target pairs. Use as the fundamental base that a pairwise interaction model yields $\\mathcal{O}(N^{2})$ kernel evaluations.\n   - Storage consists of all $N^{2}$ matrix entries.\n\n2. Hierarchical matrices with Adaptive Cross Approximation (ACA):\n   - The domain is hierarchically partitioned into a balanced binary cluster tree of depth $L=\\lceil \\log_{2} N \\rceil$.\n   - Far-field admissible blocks are compressed using low-rank factorizations of rank $r=30$ via Adaptive Cross Approximation (ACA). In a rank-$r$ factorization of an $m \\times n$ block, $U \\in \\mathbb{C}^{m \\times r}$ and $V \\in \\mathbb{C}^{n \\times r}$ are stored, incurring $r(m+n)$ complex numbers.\n   - Use the well-tested property of balanced binary space partitioning with an admissibility condition that the sum, over all far-field blocks at a fixed level $\\ell$, of $(m+n)$ is bounded by $2N$. Consequently, summing across all $L$ levels, the total count of complex numbers required to store all far-field low-rank factors is bounded by $2 r N \\log_{2} N$. Assume near-field contributions are negligible compared to this term at $N=10^{5}$ and $r=30$.\n   - Assume the ACA assembly cost is proportional to the number of stored low-rank degrees of freedom, so the total number of kernel evaluations for far-field blocks is also proportional to $2 r N \\log_{2} N$. Use this proportionality with unit constant to obtain a concrete count.\n\nTasks:\n- Starting from these bases and assumptions, derive explicit formulas for:\n  - The dense storage in bytes and the dense assembly cost in kernel evaluations.\n  - The hierarchical storage in bytes and the hierarchical assembly cost in kernel evaluations.\n- Then, define the compression factor\n  $$S \\equiv \\frac{\\text{dense storage (bytes)}}{\\text{hierarchical storage (bytes)}}.$$\n  Evaluate $S$ for $N=10^{5}$ and $r=30$.\n- For interpretability, you may also express intermediate memory budgets in gigabytes using decimal units, where $1\\,\\mathrm{GB} = 10^{9}$ bytes. However, your final reported value must be the compression factor $S$ as a pure number.\n\nRound your final answer for $S$ to four significant figures. Do not include any units with your final reported number.",
            "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the established principles of boundary element methods and hierarchical matrix approximations, specifically Adaptive Cross Approximation (ACA), used in computational acoustics. The problem is well-posed, providing all necessary data, explicit assumptions, and clear definitions, ensuring a unique and meaningful solution can be derived. The parameters and context are realistic for a large-scale scientific computing problem.\n\nWe proceed with the derivation and solution.\n\nThe number of boundary unknowns is $N=10^{5}$. A single matrix entry is stored as a double-precision complex number, occupying $16$ bytes.\n\nFirst, we analyze the dense assembly and storage strategy.\nThe system matrix is of size $N \\times N$.\nThe total number of entries is $N^2$.\nThe storage required for the dense matrix, denoted $D_{\\text{storage}}$, is the total number of entries multiplied by the storage per entry:\n$$D_{\\text{storage}} = N^2 \\times 16 \\text{ bytes}$$\nThe assembly cost, defined as the number of kernel evaluations, is based on the pairwise interaction model, resulting in a cost, denoted $D_{\\text{cost}}$, of:\n$$D_{\\text{cost}} = N^2 \\text{ kernel evaluations}$$\n\nNext, we analyze the hierarchical matrix strategy with Adaptive Cross Approximation (ACA).\nThe rank of the low-rank approximations is given as $r=30$.\nThe problem states that the total count of complex numbers required to store all far-field low-rank factors is bounded by $2 r N \\log_{2} N$, and contributions from the near-field are assumed to be negligible.\nTherefore, the storage for the hierarchical matrix, denoted $H_{\\text{storage}}$, is given by this count multiplied by the storage per complex number:\n$$H_{\\text{storage}} = (2 r N \\log_{2} N) \\times 16 \\text{ bytes}$$\nThe assembly cost for the far-field blocks, denoted $H_{\\text{cost}}$, is assumed to be proportional to the storage with a unit constant, and near-field costs are again neglected.\n$$H_{\\text{cost}} = 2 r N \\log_{2} N \\text{ kernel evaluations}$$\n\nNow, we evaluate these quantities and the compression factor $S$ for the given parameters $N=10^5$ and $r=30$.\n\nFor the dense strategy:\n$D_{\\text{storage}} = (10^5)^2 \\times 16 = 10^{10} \\times 16 = 1.6 \\times 10^{11}$ bytes. Using the conversion $1\\,\\mathrm{GB} = 10^9$ bytes, this is equivalent to $160\\,\\mathrm{GB}$.\n$D_{\\text{cost}} = (10^5)^2 = 10^{10}$ kernel evaluations.\n\nFor the hierarchical strategy, we first compute the value of $\\log_{2} N$:\n$$\\log_{2} N = \\log_{2}(10^5) = 5 \\log_{2}(10)$$\nUsing the change of base formula, $\\log_{2}(10) = \\frac{\\ln(10)}{\\ln(2)}$.\nNumerically, $\\log_{2}(10^5) \\approx 5 \\times \\frac{2.302585}{0.693147} \\approx 16.60964$.\nNow we can compute the storage and cost:\n$H_{\\text{storage}} = (2 \\times 30 \\times 10^5 \\times \\log_{2}(10^5)) \\times 16 \\approx (60 \\times 10^5 \\times 16.60964) \\times 16 \\approx 9.965784 \\times 10^7 \\times 16 \\approx 1.5945 \\times 10^9$ bytes. This is approximately $1.595\\,\\mathrm{GB}$.\n$H_{\\text{cost}} = 2 \\times 30 \\times 10^5 \\times \\log_{2}(10^5) \\approx 60 \\times 10^5 \\times 16.60964 \\approx 9.966 \\times 10^7$ kernel evaluations.\n\nThe compression factor $S$ is defined as the ratio of the dense storage to the hierarchical storage:\n$$S \\equiv \\frac{D_{\\text{storage}}}{H_{\\text{storage}}}$$\nSubstituting the derived formulas:\n$$S = \\frac{N^2 \\times 16}{(2 r N \\log_{2} N) \\times 16} = \\frac{N^2}{2 r N \\log_{2} N} = \\frac{N}{2 r \\log_{2} N}$$\nThis formula highlights that the compression factor improves linearly with the problem size $N$ and degrades inversely with the rank $r$ and the logarithm of $N$.\n\nWe now evaluate $S$ for the given values:\n$$S = \\frac{10^5}{2 \\times 30 \\times \\log_{2}(10^5)} = \\frac{10^5}{60 \\times \\log_{2}(10^5)}$$\nUsing the previously calculated value for $\\log_{2}(10^5)$:\n$$S \\approx \\frac{100000}{60 \\times 16.60964} \\approx \\frac{100000}{996.5784} \\approx 100.3433$$\nRounding the result to four significant figures gives $100.3$.\nThis means that the hierarchical matrix approach reduces the storage requirement by a factor of approximately $100.3$ compared to the dense matrix approach for this problem size.",
            "answer": "$$\\boxed{100.3}$$"
        },
        {
            "introduction": "Having established the motivation for H-matrices, we now turn to a fundamental component of their construction: the admissibility condition. This hands-on coding exercise challenges you to implement the geometric test that partitions the matrix into dense near-field blocks and compressible far-field blocks . By programming the logic that governs this critical decision, you will develop a practical understanding of how the hierarchical structure is built and how the separation parameter $\\eta$ controls the trade-off between accuracy and compression.",
            "id": "4115730",
            "problem": "You are given a computational acoustics context where dense boundary integral equation matrices arising from the time-harmonic acoustic Helmholtz problem are compressed using Adaptive Cross Approximation (ACA) within the framework of hierarchical matrices. The physical model is governed by the Helmholtz equation, and the boundary integral formulation leads to matrices populated by the free-space Green's function. The hierarchical matrix structure requires a geometric admissibility classification of blocks into near-field (not compressed) and far-field (eligible for low-rank representation). Your task is to implement a geometric far-field admissibility test parameterized by a user-chosen separation parameter and to compute the fraction of admissible blocks on a sphere discretized with a given number of panels.\n\nStarting from the following fundamental base:\n- The time-harmonic acoustic field is governed by the Helmholtz equation $ \\nabla^2 p(\\boldsymbol{x}) + k^2 p(\\boldsymbol{x}) = 0 $ with wavenumber $ k $.\n- The free-space Green's function $ G(\\boldsymbol{x},\\boldsymbol{y}) $ for the Helmholtz equation is a smooth function of $ \\boldsymbol{x} $ and $ \\boldsymbol{y} $ when the source and target clusters are sufficiently separated in space relative to their sizes, and this smoothness underlies low-rank approximations in far-field interactions.\n- Hierarchical matrices partition the index set into clusters represented by geometric bounding boxes. Blocks between cluster pairs are classified as far-field or near-field using a geometric criterion that is consistent with the principle that two clusters are far-field if their size is small in comparison to their separation.\n\nImplement the following:\n1. Discretize the unit sphere of radius $ R = 1 $ using $ N $ panels, represented by $ N $ quasi-uniform points $ \\{\\boldsymbol{x}_i\\}_{i=1}^N $ on the sphere surface. Use a well-tested deterministic method that yields near-uniform coverage over the sphere (for example, a Fibonacci lattice) to define the panel locations $ \\boldsymbol{x}_i \\in \\mathbb{R}^3 $ satisfying $ \\|\\boldsymbol{x}_i\\|_2 = R $. No physical units are required in the final result because the output is a dimensionless fraction.\n2. Build a binary spatial cluster tree on the set of points by recursively splitting clusters along the longest axis of their axis-aligned bounding box until each leaf cluster contains at most $ m $ panels, where $ m $ is a fixed integer with $ m \\geq 2 $. Each cluster is associated with its axis-aligned bounding box in $ \\mathbb{R}^3 $ defined by the minimum and maximum corner vectors $ \\boldsymbol{b}^{\\min} \\in \\mathbb{R}^3 $ and $ \\boldsymbol{b}^{\\max} \\in \\mathbb{R}^3 $. The cluster size is quantified using a consistent geometric measure derived from the bounding box, and the separation between two clusters is quantified using the Euclidean distance between their axis-aligned bounding boxes.\n3. Construct a block-cluster tree over the same cluster tree for both rows and columns (the same set, since the boundary is the same for sources and targets). Starting from the pair consisting of the root cluster with itself, recursively generate blocks as follows: if a pair of clusters satisfies a geometric condition that formalizes the principle of size being small relative to separation, then declare this block as far-field and do not subdivide further; otherwise, if both clusters are leaves, declare this block as near-field; if not admissible and at least one cluster is not a leaf, subdivide the side with the larger geometric size (if both non-leaf) or the non-leaf side and recurse on child pairs until one of the previous conditions is met. The recursive process must partition the full interaction matrix indices into a disjoint set of blocks.\n4. Using a chosen separation parameter $ \\eta > 0 $, implement a mathematically sound far-field admissibility test consistent with the above principle. Use the axis-aligned bounding box of each cluster to define its geometric size and the Euclidean minimal distance between two axis-aligned bounding boxes to define separation. The test must be parameterized by $ \\eta $ so that increasing $ \\eta $ makes the classification more permissive.\n5. Compute the fraction of admissible blocks for the block-cluster tree, defined as the number of far-field blocks divided by the total number of blocks in the partition.\n\nYour program must:\n- Use $ R = 1 $ and a fixed leaf size $ m = 16 $ panels per leaf cluster.\n- For each test case, build the cluster tree and block-cluster tree and compute the fraction of admissible blocks as a real number in $ [0,1] $ rounded to six decimal places.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n\nTest suite:\n- Case 1: $ N = 64 $, $ \\eta = 0.5 $.\n- Case 2: $ N = 64 $, $ \\eta = 1.0 $.\n- Case 3: $ N = 64 $, $ \\eta = 2.0 $.\n- Case 4: $ N = 8 $, $ \\eta = 0.5 $.\n- Case 5: $ N = 256 $, $ \\eta = 0.5 $.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $ [r_1,r_2,r_3,r_4,r_5] $), where each $ r_i $ is the fraction of admissible blocks for the corresponding test case, rounded to six decimal places.",
            "solution": "The problem is valid. It is a well-defined computational task grounded in the established principles of hierarchical matrices used for accelerating boundary element methods. The parameters and objectives are clear, consistent, and scientifically sound.\n\nThe objective is to compute the fraction of admissible (far-field) blocks in a hierarchical matrix partition for a boundary discretized by points on a sphere. This fraction is a key performance indicator for hierarchical matrix algorithms, as it represents the proportion of matrix blocks that can be compressed, thereby reducing computational complexity. The process involves several standard steps from computational science: geometric discretization, hierarchical data structuring, and recursive partitioning based on a geometric admissibility criterion.\n\nThe solution is constructed through the following sequence of steps:\n1.  Generation of quasi-uniformly distributed points on a sphere to represent the discretized boundary.\n2.  Construction of a spatial cluster tree to hierarchically group these points.\n3.  Definition and implementation of a geometric admissibility criterion to classify pairs of clusters as near-field or far-field.\n4.  Construction of a block-cluster tree that partitions the matrix into near-field and far-field blocks, and computation of the resulting fraction of admissible blocks.\n\n**Step 1: Sphere Discretization**\n\nThe first step is to generate a set of $N$ points, $\\{\\boldsymbol{x}_i\\}_{i=1}^N$, that represent panel locations on the surface of a unit sphere of radius $R=1$. For the results to be deterministic and representative, the points must be distributed in a quasi-uniform manner. A Fibonacci lattice provides an effective method for this. The coordinates for the $i$-th point, where $i \\in \\{0, 1, \\dots, N-1\\}$, are generated using spherical coordinates $(\\rho, \\theta, \\phi)$ where $\\rho$ is the radial distance, $\\theta$ is the azimuthal angle, and $\\phi$ is the polar angle. For a unit sphere, $\\rho=1$. The angles are given by:\n$$ \\phi_i = \\arccos\\left(1 - \\frac{2(i+0.5)}{N}\\right) $$\n$$ \\theta_i = \\frac{2\\pi i}{\\Phi} $$\nwhere $\\Phi = \\frac{1+\\sqrt{5}}{2}$ is the golden ratio. The Cartesian coordinates $\\boldsymbol{x}_i = (x_i, y_i, z_i)$ are then obtained via the standard transformation:\n$$ x_i = R \\sin(\\phi_i) \\cos(\\theta_i) $$\n$$ y_i = R \\sin(\\phi_i) \\sin(\\theta_i) $$\n$$ z_i = R \\cos(\\phi_i) $$\nGiven $R=1$, the points lie on the unit sphere, satisfying $\\|\\boldsymbol{x}_i\\|_2 = 1$. These $N$ points form the index set for our problem.\n\n**Step 2: Cluster Tree Construction**\n\nA cluster tree is a hierarchical data structure that spatially organizes the set of points. It is built recursively. Each node in the tree, called a cluster $\\tau$, represents a subset of the point indices $I_\\tau \\subseteq \\{0, 1, \\dots, N-1\\}$ and is associated with an axis-aligned bounding box (AABB), $\\mathcal{B}_\\tau$, that encloses all points $\\{\\boldsymbol{x}_i\\}_{i \\in I_\\tau}$.\n\nThe tree construction proceeds as follows:\n-   The root cluster contains all $N$ point indices.\n-   A recursive function is applied to the root cluster. For a given cluster $\\tau$:\n    1.  If the number of points in the cluster, $|I_\\tau|$, is less than or equal to the leaf size parameter $m$ (given as $m=16$), the cluster is declared a leaf, and the recursion terminates for this branch.\n    2.  Otherwise, the cluster is split into two child clusters. The split is performed along the longest dimension of the cluster's AABB, $\\mathcal{B}_\\tau$. The points are partitioned into two sets based on the median of their coordinates along this dimension. This ensures a balanced spatial partitioning.\n    3.  Two new child clusters are created from these two sets of points, and the recursive function is called on each child.\n\nThis process results in a binary tree where each leaf cluster contains at most $m$ points.\n\n**Step 3: Geometric Admissibility Criterion**\n\nThe core of the hierarchical matrix method is the admissibility criterion, which determines whether the interaction between two clusters $\\tau$ and $\\sigma$ can be approximated. This is based on the principle that the Green's function is smooth for well-separated domains. A standard geometric condition formalizes this by comparing the clusters' sizes to their separation.\n\nFor two clusters, $\\tau$ and $\\sigma$, we define:\n-   **Bounding Box $\\mathcal{B}$**: The AABB for a cluster $\\tau$ is defined by its minimum and maximum corner vectors, $\\boldsymbol{b}_\\tau^{\\min}$ and $\\boldsymbol{b}_\\tau^{\\max}$.\n-   **Diameter $\\text{diam}(\\mathcal{B}_\\tau)$**: The size of a cluster is quantified by the diameter of its AABB, calculated as the Euclidean norm of the box's diagonal: $\\text{diam}(\\mathcal{B}_\\tau) = \\|\\boldsymbol{b}_\\tau^{\\max} - \\boldsymbol{b}_\\tau^{\\min}\\|_2$.\n-   **Distance $\\text{dist}(\\mathcal{B}_\\tau, \\mathcal{B}_\\sigma)$**: The separation between two clusters is the minimum Euclidean distance between their AABBs. This is computed as $\\text{dist}(\\mathcal{B}_\\tau, \\mathcal{B}_\\sigma) = \\left( \\sum_{j=1}^3 \\max(0, b_{\\tau,j}^{\\min} - b_{\\sigma,j}^{\\max}, b_{\\sigma,j}^{\\min} - b_{\\tau,j}^{\\max})^2 \\right)^{1/2}$.\n\nA block corresponding to the cluster pair $(\\tau, \\sigma)$ is declared **admissible** (far-field) if the following condition holds for a given separation parameter $\\eta > 0$:\n$$ \\max(\\text{diam}(\\mathcal{B}_\\tau), \\text{diam}(\\mathcal{B}_\\sigma)) \\le \\eta \\cdot \\text{dist}(\\mathcal{B}_\\tau, \\mathcal{B}_\\sigma) $$\nIf the bounding boxes touch or overlap, $\\text{dist}(\\mathcal{B}_\\tau, \\mathcal{B}_\\sigma) = 0$, and the condition is not satisfied (unless the clusters are single points, with zero diameter). Increasing $\\eta$ relaxes this condition, allowing larger clusters to be considered \"far-field\" at the same distance, thus making the admissibility more permissive.\n\n**Step 4: Block-Cluster Tree and Fraction Calculation**\n\nThe block-cluster tree represents a partition of the full matrix product space $I \\times I$ into disjoint blocks $(\\tau, \\sigma)$. We do not need to explicitly store this tree; we can traverse it recursively to count the number of admissible (far-field) and inadmissible (near-field) leaf blocks.\n\nThe counting is performed with a recursive algorithm, implemented iteratively using a stack to avoid deep recursion issues:\n1.  Initialize a stack with the initial block, `(root, root)`, where `root` is the root of the cluster tree. Initialize counters $N_{\\text{far}} = 0$ and $N_{\\text{near}} = 0$.\n2.  While the stack is not empty, pop a block $(\\tau, \\sigma)$.\n3.  Check if the block is admissible using the criterion from Step 3.\n    -   If yes, increment $N_{\\text{far}}$ and do not subdivide this block further.\n    -   If no, proceed to the next step.\n4.  Check if both $\\tau$ and $\\sigma$ are leaf clusters.\n    -   If yes, this block is an incompressible near-field block. Increment $N_{\\text{near}}$.\n    -   If no, at least one cluster is not a leaf, and the block must be subdivided.\n5.  To subdivide, select the cluster with the larger diameter for splitting. If one cluster is a leaf, the other (non-leaf) one must be split.\n    -   If $\\tau$ is chosen for splitting, push its child-pairs $(\\tau_1, \\sigma)$ and $(\\tau_2, \\sigma)$ onto the stack.\n    -   If $\\sigma$ is chosen for splitting, push $(\\tau, \\sigma_1)$ and $(\\tau, \\sigma_2)$ onto the stack.\n6.  Repeat until the stack is empty.\n\nFinally, the fraction of admissible blocks is computed as:\n$$ \\mathcal{F} = \\frac{N_{\\text{far}}}{N_{\\text{far}} + N_{\\text{near}}} $$\nThis value is calculated for each test case specified in the problem statement, with results rounded to six decimal places. The special case where $N=8$ and $m=16$ results in a cluster tree with only a single root node (which is also a leaf). The block `(root, root)` is inadmissible and cannot be subdivided, resulting in $N_{\\text{near}}=1$ and $N_{\\text{far}}=0$, yielding a fraction of $0.0$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the fraction of admissible blocks in a hierarchical matrix partition\n    for a sphere discretization.\n    \"\"\"\n\n    class Cluster:\n        \"\"\"Represents a cluster of points and its bounding box.\"\"\"\n        def __init__(self, indices, all_points):\n            self.indices = np.array(indices, dtype=int)\n            if self.indices.size > 0:\n                self.points = all_points[self.indices]\n                self.bbox_min = np.min(self.points, axis=0)\n                self.bbox_max = np.max(self.points, axis=0)\n                self.diameter = np.linalg.norm(self.bbox_max - self.bbox_min)\n            else:\n                self.points = np.empty((0, 3))\n                # Define a non-interfering default for empty clusters\n                self.bbox_min = np.full(3, np.inf)\n                self.bbox_max = np.full(3, -np.inf)\n                self.diameter = 0.0\n            self.children = None\n\n    def generate_sphere_points(N, R):\n        \"\"\"Generates N quasi-uniform points on a sphere of radius R using a Fibonacci lattice.\"\"\"\n        if N == 0:\n            return np.empty((0, 3))\n        \n        indices = np.arange(0, N, dtype=float) + 0.5\n        \n        phi = np.arccos(1 - 2 * indices / N)\n        golden_ratio = (1 + 5**0.5) / 2\n        theta = 2 * np.pi * indices / golden_ratio\n        \n        x = R * np.cos(theta) * np.sin(phi)\n        y = R * np.sin(theta) * np.sin(phi)\n        z = R * np.cos(phi)\n        \n        return np.stack([x, y, z], axis=1)\n\n    def build_cluster_tree(cluster, m, all_points):\n        \"\"\"Recursively builds a spatial cluster tree.\"\"\"\n        if len(cluster.indices) = m:\n            return  # This cluster is a leaf\n\n        # Find the longest dimension of the bounding box to split along\n        dims = cluster.bbox_max - cluster.bbox_min\n        split_dim = np.argmax(dims)\n        \n        # Partition points based on the median coordinate\n        coords = cluster.points[:, split_dim]\n        median = np.median(coords)\n        \n        left_indices = cluster.indices[coords  median]\n        right_indices = cluster.indices[coords >= median]\n\n        # Handle degenerate cases where one child would be empty\n        if len(left_indices) == 0 or len(right_indices) == 0:\n            mid_idx = len(cluster.indices) // 2\n            left_indices = cluster.indices[:mid_idx]\n            right_indices = cluster.indices[mid_idx:]\n\n        cluster.children = [\n            Cluster(left_indices, all_points),\n            Cluster(right_indices, all_points)\n        ]\n        \n        for child in cluster.children:\n            build_cluster_tree(child, m, all_points)\n\n    def is_admissible(c_tau, c_sigma, eta):\n        \"\"\"Checks if the block (c_tau, c_sigma) is admissible.\"\"\"\n        dist_sq = 0.0\n        for i in range(3):\n            d = max(0.0, c_tau.bbox_min[i] - c_sigma.bbox_max[i], c_sigma.bbox_min[i] - c_tau.bbox_max[i])\n            dist_sq += d * d\n        \n        if dist_sq == 0:\n            return False  # Bounding boxes overlap or touch\n        \n        dist = np.sqrt(dist_sq)\n        max_diam = max(c_tau.diameter, c_sigma.diameter)\n        \n        return max_diam = eta * dist\n\n    def count_blocks(root, eta):\n        \"\"\"Counts far-field and near-field blocks in the block-cluster tree.\"\"\"\n        far_count = 0\n        near_count = 0\n        \n        if root is None:\n            return 0, 0\n        \n        stack = [(root, root)]\n        \n        while stack:\n            c_tau, c_sigma = stack.pop()\n            \n            if is_admissible(c_tau, c_sigma, eta):\n                far_count += 1\n                continue\n            \n            # If not admissible, check if we must stop subdividing\n            if c_tau.children is None and c_sigma.children is None:\n                near_count += 1\n                continue\n                \n            # If we can subdivide, determine which cluster to split\n            split_tau = (c_tau.children is not None) and \\\n                        ((c_sigma.children is None) or (c_tau.diameter >= c_sigma.diameter))\n\n            if split_tau:\n                for child_tau in c_tau.children:\n                    stack.append((child_tau, c_sigma))\n            else:  # split_sigma\n                for child_sigma in c_sigma.children:\n                    stack.append((c_tau, child_sigma))\n\n        return far_count, near_count\n\n    R = 1.0\n    m = 16\n    \n    test_cases = [\n        (64, 0.5),\n        (64, 1.0),\n        (64, 2.0),\n        (8, 0.5),\n        (256, 0.5),\n    ]\n\n    results = []\n    for N, eta in test_cases:\n        points = generate_sphere_points(N, R)\n        \n        if N > 0:\n            root = Cluster(range(N), points)\n            build_cluster_tree(root, m, points)\n            far_blocks, near_blocks = count_blocks(root, eta)\n            total_blocks = far_blocks + near_blocks\n        \n            if total_blocks > 0:\n                fraction = far_blocks / total_blocks\n            else:\n                fraction = 0.0\n        else:\n            fraction = 0.0\n\n        results.append(round(fraction, 6))\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The power of H-matrices stems from the low-rank nature of far-field interaction blocks, but what determines this rank? This exercise connects the abstract algebraic property of rank to the underlying physics of the Helmholtz equation . You will implement a heuristic rank estimate based on physical principles—wave oscillation and geometric decay—and compare it against the \"ground truth\" numerical rank from a Singular Value Decomposition (SVD), offering deep insight into the sources of kernel compressibility.",
            "id": "4115752",
            "problem": "Consider the three-dimensional time-harmonic acoustic field governed by the Helmholtz equation with wavenumber $k$. The free-space Green's function is given by $G(\\mathbf{x},\\mathbf{y}) = \\dfrac{e^{\\mathrm{i} k \\|\\mathbf{x} - \\mathbf{y}\\|}}{4 \\pi \\|\\mathbf{x} - \\mathbf{y}\\|}$. In hierarchical matrix methods, a block corresponding to interactions between two well-separated clusters admits a low-rank separable approximation, and Adaptive Cross Approximation (ACA) can recover such low-rank structure. We focus on two axis-aligned cubes $X$ and $Y$ of side length $a$, with centers separated along the $x$-axis by a distance $d$; thus the closest face-to-face gap is $d-a$. The cubes are uniformly discretized into a Cartesian tensor grid to form the dense interaction matrix $A \\in \\mathbb{C}^{n \\times n}$, where $A_{ij} = G(\\mathbf{x}_i,\\mathbf{y}_j)$ and $\\{\\mathbf{x}_i\\}$, $\\{\\mathbf{y}_j\\}$ are the grid points in $X$ and $Y$, respectively.\n\nStarting from the addition theorem for separable expansions of the Helmholtz Green's function and the classical notion of admissibility for hierarchical matrices (well-separated clusters yield convergent degenerate expansions), use the following heuristic to estimate the numerical rank:\n- Let $L$ be the smallest integer satisfying both the separation-tail control and the oscillatory-content resolution constraints:\n  1. Separation-tail control: choose $L$ such that $L \\ge \\left\\lceil \\dfrac{\\ln(1/\\varepsilon)}{\\ln(d/a)} \\right\\rceil$,\n  2. Oscillatory-content resolution: choose $L \\ge \\left\\lceil k a \\right\\rceil$.\n- The separable expansion in spherical harmonics up to order $L$ contains $(L+1)^2$ angular modes; use $R_{\\text{heur}} = (L+1)^2$ as the rank estimate.\n\nHere $\\varepsilon$ denotes the desired relative error tolerance in the Frobenius norm. To verify this heuristic numerically, discretize each cube with $N$ equispaced points per spatial dimension for a total of $n = N^3$ points per cube. Construct the interaction matrix $A$ and compute the minimal integer rank $R_{\\text{svd}}$ obtained by truncating the singular value decomposition (SVD) such that the relative Frobenius error of the truncated approximation $A_R$ satisfies\n$$\n\\frac{\\|A - A_R\\|_F}{\\|A\\|_F} \\le \\varepsilon,\n$$\nwhere $\\|\\cdot\\|_F$ is the Frobenius norm. Use the face-to-face separation $d-a$ and place the cube centers at $\\mathbf{c}_X = \\left(-\\dfrac{d}{2},\\,0,\\,0\\right)$ and $\\mathbf{c}_Y = \\left(\\dfrac{d}{2},\\,0,\\,0\\right)$. Each cube occupies $[-a/2,a/2]^3$ relative to its center.\n\nYour program must implement the heuristic rank estimate and the SVD-based measured minimal rank for the following test suite, all with $N=5$ points per dimension and using the Green's function above:\n- Test case $1$ (happy path): $k a = 2$, $d/a = 6$, $\\varepsilon = 10^{-6}$.\n- Test case $2$ (higher frequency): $k a = 10$, $d/a = 6$, $\\varepsilon = 10^{-6}$.\n- Test case $3$ (near admissibility boundary): $k a = 2$, $d/a = 1.2$, $\\varepsilon = 10^{-6}$.\n- Test case $4$ (low frequency): $k a = 0.5$, $d/a = 6$, $\\varepsilon = 10^{-6}$.\n\nFor all test cases, set $a = 1$ (in meters), so that $k = k a / a = k a$ has units of inverse meters, and $d = (d/a) \\cdot a$ has units of meters. The outputs $R_{\\text{heur}}$ and $R_{\\text{svd}}$ are integers and unitless.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the pair $[R_{\\text{heur}},R_{\\text{svd}}]$ for a test case, in the order listed above. For example, the output format must be exactly\n$$\n\\texttt{[[R\\_heur\\_1,R\\_svd\\_1],[R\\_heur\\_2,R\\_svd\\_2],[R\\_heur\\_3,R\\_svd\\_3],[R\\_heur\\_4,R\\_svd\\_4]]}.\n$$",
            "solution": "The problem requires a comparison between a heuristic estimate and a numerically computed rank for a specific block of a boundary element matrix. This matrix arises from the discretization of the integral operator associated with the Helmholtz equation in three dimensions. The analysis proceeds in two parts: the derivation and calculation of the heuristic rank, $R_{\\text{heur}}$, and the numerical computation of the SVD-based rank, $R_{\\text{svd}}$.\n\nFirst, we address the heuristic rank estimate, $R_{\\text{heur}}$. The foundation for this estimate lies in the physics of wave propagation, specifically the representation of the free-space Green's function, $G(\\mathbf{x},\\mathbf{y}) = \\frac{e^{\\mathrm{i} k \\|\\mathbf{x} - \\mathbf{y}\\|}}{4 \\pi \\|\\mathbf{x} - \\mathbf{y}\\|}$, using separable expansions like the addition theorem for spherical harmonics. For two well-separated clusters of points, $X$ and $Y$, the interaction block $A$ where $A_{ij} = G(\\mathbf{x}_i, \\mathbf{y}_j)$ can be approximated by a low-rank matrix. The rank of this approximation depends on the order of the expansion, $L$. The problem provides two physical constraints to determine a sufficient order $L$.\n\n1.  **Separation-tail control**: This constraint arises from the decay of the singular values of the integral operator, which is related to the separation between the clusters. For clusters of characteristic size $a$ separated by a distance $d$, the multipole expansion error decays geometrically with the ratio $a/d$. To achieve a desired relative accuracy $\\varepsilon$, the order $L$ must be large enough to sufficiently damp the truncated \"tail\" of the expansion. This gives the first condition:\n    $$L \\ge L_{\\text{sep}} = \\left\\lceil \\frac{\\ln(1/\\varepsilon)}{\\ln(d/a)} \\right\\rceil$$\n    This condition dominates for well-separated clusters ($d/a \\gg 1$) at low frequencies.\n\n2.  **Oscillatory-content resolution**: This constraint relates to the Nyquist-Shannon sampling theorem applied to the angular components of the wavefield. To accurately represent a wave with wavenumber $k$ over a region of size $a$, the number of basis functions must be proportional to the dimensionless quantity $ka$. A higher frequency (larger $k$) or larger domain (larger $a$) introduces more oscillations that must be resolved. In the context of spherical harmonics, this leads to the second condition:\n    $$L \\ge L_{\\text{osc}} = \\left\\lceil k a \\right\\rceil$$\n    This condition becomes dominant at high frequencies, where the oscillatory nature of the kernel is the primary factor limiting compressibility.\n\nThe overall required expansion order $L$ must satisfy both constraints, thus $L = \\max(L_{\\text{sep}}, L_{\\text{osc}})$. A spherical harmonics expansion of order $L$ involves $(L+1)^2$ basis functions. This count provides a direct heuristic estimate for the numerical rank:\n$$R_{\\text{heur}} = (L+1)^2$$\n\nSecond, we compute the numerical rank, $R_{\\text{svd}}$, to serve as a ground truth. This involves the following steps:\n1.  **Discretization**: The two cubic domains, $X$ and $Y$, are discretized. Each cube has side length $a=1$ and contains $n=N^3$ points, with $N=5$ points equispaced along each dimension. The centers of the cubes are set to $\\mathbf{c}_X = (-d/2, 0, 0)$ and $\\mathbf{c}_Y = (d/2, 0, 0)$.\n2.  **Matrix Assembly**: The dense $n \\times n$ interaction matrix $A$ is constructed, with entries $A_{ij} = G(\\mathbf{x}_i, \\mathbf{y}_j)$ for all points $\\mathbf{x}_i \\in X$ and $\\mathbf{y}_j \\in Y$.\n3.  **Singular Value Decomposition (SVD)**: The SVD of $A$ is computed, yielding the singular values $\\sigma_0 \\ge \\sigma_1 \\ge \\dots \\ge \\sigma_{n-1} \\ge 0$. The SVD provides the optimal low-rank approximation in the Frobenius norm. A rank-$R$ approximation, $A_R$, is formed by keeping the first $R$ singular values and vectors.\n4.  **Rank Determination**: The error of the rank-$R$ approximation is given by $\\|A - A_R\\|_F = \\sqrt{\\sum_{i=R}^{n-1} \\sigma_i^2}$. The relative Frobenius error is the ratio of this error to the total norm of the matrix, $\\|A\\|_F = \\sqrt{\\sum_{i=0}^{n-1} \\sigma_i^2}$. We seek the minimal integer rank $R_{\\text{svd}}$ such that this relative error is no greater than the specified tolerance $\\varepsilon$:\n    $$\\frac{\\sqrt{\\sum_{i=R_{\\text{svd}}}^{n-1} \\sigma_i^2}}{\\sqrt{\\sum_{i=0}^{n-1} \\sigma_i^2}} \\le \\varepsilon$$\n    This is equivalent to finding the smallest $R$ such that $(\\sum_{i=R}^{n-1} \\sigma_i^2) \\le \\varepsilon^2 (\\sum_{i=0}^{n-1} \\sigma_i^2)$. This value $R$ is determined by iterating through possible ranks from $R=0$ to $n$ and selecting the first one that satisfies the condition.\n\nThe program implements these two procedures for each of the four test cases, providing a comparison between the physics-based heuristic and the exact numerical rank.",
            "answer": "```python\nimport numpy as np\n\ndef calculate_R_heur(ka, da_ratio, epsilon):\n    \"\"\"\n    Calculates the heuristic rank estimate based on physical constraints.\n\n    Args:\n        ka (float): Dimensionless wavenumber.\n        da_ratio (float): Ratio of center-to-center distance to cube side length.\n        epsilon (float): Desired relative error tolerance.\n\n    Returns:\n        int: The heuristic rank estimate R_heur.\n    \"\"\"\n    # Separation-tail control constraint\n    # Note: np.log(da_ratio) is safe since all test cases have da_ratio > 1\n    L_sep = np.ceil(np.log(1.0 / epsilon) / np.log(da_ratio))\n    \n    # Oscillatory-content resolution constraint\n    L_osc = np.ceil(ka)\n    \n    # L is the maximum of the two constraints\n    L = int(np.max([L_sep, L_osc]))\n    \n    # Heuristic rank based on spherical harmonics expansion of order L\n    R_heur = (L + 1)**2\n    return R_heur\n\ndef calculate_R_svd(ka, da_ratio, epsilon, N, a):\n    \"\"\"\n    Constructs the interaction matrix and computes the SVD-based minimal rank.\n\n    Args:\n        ka (float): Dimensionless wavenumber.\n        da_ratio (float): Ratio of center-to-center distance to cube side length.\n        epsilon (float): Desired relative error tolerance.\n        N (int): Number of equispaced points per dimension.\n        a (float): Side length of the cubes.\n\n    Returns:\n        int: The SVD-based minimal rank R_svd.\n    \"\"\"\n    # Derived physical parameters\n    k = ka / a\n    d = da_ratio * a\n    n = N**3\n    \n    # 1. Generate grid points for a cube centered at the origin\n    coords_1d = np.linspace(-a / 2.0, a / 2.0, N)\n    # Use 'ij' indexing for consistency, then reshape to a list of (x,y,z) points\n    grid_rel = np.array(np.meshgrid(coords_1d, coords_1d, coords_1d, indexing='ij')).T.reshape(-1, 3)\n\n    # 2. Translate grid points to absolute positions for cubes X and Y\n    center_X = np.array([-d / 2.0, 0.0, 0.0])\n    center_Y = np.array([d / 2.0, 0.0, 0.0])\n    \n    points_X = grid_rel + center_X\n    points_Y = grid_rel + center_Y\n\n    # 3. Construct the interaction matrix A using broadcasting for efficiency\n    # points_X shape: (n, 3) -> (n, 1, 3)\n    # points_Y shape: (n, 3) -> (1, n, 3)\n    # diff shape: (n, n, 3)\n    diff = points_X[:, np.newaxis, :] - points_Y[np.newaxis, :, :]\n    \n    # dists shape: (n, n)\n    dists = np.linalg.norm(diff, axis=2)\n    \n    # Calculate Green's function for all pairs (Helmholtz kernel)\n    # Note: dists are never zero since cubes are separated in all test cases\n    A = np.exp(1j * k * dists) / (4.0 * np.pi * dists)\n    \n    # 4. Compute singular values (we only need the values, not the vectors)\n    s = np.linalg.svd(A, compute_uv=False)\n    \n    # 5. Find the minimal rank R_svd that satisfies the error tolerance\n    s_sq = s**2\n    total_norm_sq = np.sum(s_sq)\n    \n    # If the matrix is numerically zero, its rank is 0.\n    if total_norm_sq == 0:\n        return 0\n\n    target_error_sq = epsilon**2 * total_norm_sq\n    \n    # Loop through possible ranks R from 0 to n to find the smallest one\n    # that satisfies the error condition.\n    R_svd = n # Default to full rank if no smaller rank suffices\n    for R in range(n):\n        # Error for a rank-R approximation is the sum of squares of trailing singular values\n        error_sq = np.sum(s_sq[R:])\n        \n        if error_sq = target_error_sq:\n            R_svd = R\n            break\n            \n    return R_svd\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define test cases as (ka, d/a, epsilon)\n    test_cases = [\n        (2.0, 6.0, 1e-6),   # Test case 1\n        (10.0, 6.0, 1e-6),  # Test case 2\n        (2.0, 1.2, 1e-6),   # Test case 3\n        (0.5, 6.0, 1e-6),   # Test case 4\n    ]\n\n    # Global parameters from the problem statement\n    N = 5\n    a = 1.0\n\n    final_result_strings = []\n    for ka, da_ratio, epsilon in test_cases:\n        R_heur = calculate_R_heur(ka, da_ratio, epsilon)\n        R_svd = calculate_R_svd(ka, da_ratio, epsilon, N, a)\n        # Format each pair as [R_heur,R_svd] without spaces\n        final_result_strings.append(f\"[{R_heur},{R_svd}]\")\n\n    # Combine all results into the final required format\n    print(f\"[{','.join(final_result_strings)}]\")\n\nsolve()\n```"
        }
    ]
}