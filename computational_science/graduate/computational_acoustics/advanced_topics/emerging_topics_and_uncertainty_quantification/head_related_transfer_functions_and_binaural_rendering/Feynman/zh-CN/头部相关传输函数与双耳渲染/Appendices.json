{
    "hands_on_practices": [
        {
            "introduction": "为了在计算中有效地使用头部相关传递函数（HRTF），我们需要一种紧凑而准确的数学表示。球谐函数为此提供了一个强大的框架，允许我们将定义在球面上的连续方向函数转换为一组离散的系数。本练习将指导您完成将离散的HRTF测量数据转换为球谐函数模型的过程，并分析采样策略对模型精度的影响，这是高级双耳音频处理中的一项基本技能。",
            "id": "4125584",
            "problem": "考虑一个复值头部相关传递函数 (HRTF)，在角频率 $\\omega$ 下建模为单位球面上的函数 $H(\\omega,\\Omega)$，其中 $\\Omega$ 表示球面上的方向，由余纬 $\\theta \\in [0,\\pi]$ 和方位角 $\\phi \\in [0,2\\pi)$ 参数化，两者均以弧度为单位。令 $\\{Y_{n}^{m}(\\theta,\\phi)\\}$ 表示单位球面上的正交归一复球谐函数基，采用 Condon-Shortley 相位约定，其归一化满足 $\\int_{S^{2}} Y_{n}^{m}(\\Omega) Y_{n'}^{m'*}(\\Omega)\\,d\\Omega = \\delta_{nn'}\\delta_{mm'}$，其中 $d\\Omega = \\sin\\theta\\,d\\theta\\,d\\phi$，* 表示复共轭。对于固定的截断阶数 $N$，HRTF 的截断球谐展开为\n$$\nH(\\omega,\\Omega) \\approx \\sum_{n=0}^{N}\\sum_{m=-n}^{n} \\alpha_{n}^{m}(\\omega)\\, Y_{n}^{m}(\\Omega).\n$$\n展开系数 $\\alpha_{n}^{m}(\\omega)$ 由球面积分内积定义\n$$\n\\alpha_{n}^{m}(\\omega) = \\int_{S^{2}} H(\\omega,\\Omega)\\, Y_{n}^{m*}(\\Omega)\\, d\\Omega.\n$$\n您必须从第一性原理出发，推导如何使用近似球面积分的求积权重 $\\{w_{i}\\}_{i=1}^{I}$，从离散方向样本 $\\{H(\\omega,\\Omega_{i})\\}_{i=1}^{I}$ 来近似 $\\alpha_{n}^{m}(\\omega)$。然后，分析当采样未能满足直到 $N$ 阶的求积精度时所产生的误差，包括方位角上的混叠机制和仰角权重的不精确性。您的推导必须从球谐函数的正交归一性和作为曲面积分近似的球面求积的定义开始，并且不得依赖捷径公式。\n\n为了进行数值验证，通过为最高达合成阶数 $K \\leq N$ 的阶数指定一组基准系数 $\\{\\alpha_{n}^{m}(\\omega)\\}$，然后在一组采样方向上通过其球谐展开来评估 $H(\\omega,\\Omega)$，从而构造一个合成的 HRTF $H(\\omega,\\Omega)$。使用以下合成系数模型：\n$$\n\\alpha_{n}^{m}(\\omega) = \\exp\\!\\left(-\\left(\\frac{\\omega}{\\omega_{0}}\\right)^{2}\\right)\\,\\frac{(-1)^{m}}{n+1}\\,\\exp\\!\\big(j\\,0.2\\,(m+n)\\big),\n$$\n其中 $\\omega_{0} = 3000$ $\\mathrm{rad/s}$，$j$ 表示虚数单位。这种选择确保了系数为非平凡复数，且不具有病态对称性。所有角度必须以弧度为单位，所有角频率必须以 $\\mathrm{rad/s}$ 为单位。\n\n通过离散求积近似系数\n$$\n\\widehat{\\alpha}_{n}^{m}(\\omega) = \\sum_{i=1}^{I} w_{i}\\, H(\\omega,\\Omega_{i})\\, Y_{n}^{m*}(\\Omega_{i}),\n$$\n使用两种类型的球面积分乘积求积格式：\n- 余纬上采用高斯-勒让德（Gauss-Legendre）求积，方位角上采用均匀梯形求积（记为“GL”类型）：令 $u=\\cos\\theta \\in [-1,1]$，其高斯-勒让德节点和权重为 $\\{u_{j},w_{j}^{(u)}\\}_{j=1}^{L_{\\theta}}$。令 $\\phi_{k} = \\frac{2\\pi k}{M_{\\phi}}$，其中 $k=0,1,\\dots,M_{\\phi}-1$，方位角步长为均匀的 $\\Delta\\phi = \\frac{2\\pi}{M_{\\phi}}$。采样方向为乘积网格 $\\{(\\theta_{j},\\phi_{k})\\}$，其中 $\\theta_{j}=\\arccos(u_{j})$，对应的权重为 $w_{jk}=w_{j}^{(u)}\\,\\Delta\\phi$。\n- 余纬上采用等角中点求积，方位角上采用均匀梯形求积（记为“EQ”类型）：令 $\\theta_{j} = \\frac{\\pi}{L_{\\theta}}\\left(j+\\frac{1}{2}\\right)$，其中 $j=0,1,\\dots,L_{\\theta}-1$，且 $\\phi_{k} = \\frac{2\\pi k}{M_{\\phi}}$，其中 $k=0,1,\\dots,M_{\\phi}-1$。近似权重为 $w_{jk} = \\sin\\theta_{j}\\,\\Delta\\theta\\,\\Delta\\phi$，其中 $\\Delta\\theta = \\frac{\\pi}{L_{\\theta}}$ 且 $\\Delta\\phi = \\frac{2\\pi}{M_{\\phi}}$。\n\n对于以下每个测试用例，计算在所有阶数 $n \\in \\{0,\\dots,N\\}$ 和次数 $m \\in \\{-n,\\dots,n\\}$ 上的相对均方根 (RMS) 系数误差：\n$$\n\\varepsilon_{\\mathrm{rel}} = \\sqrt{\\frac{\\sum_{n=0}^{N}\\sum_{m=-n}^{n} \\left|\\widehat{\\alpha}_{n}^{m}(\\omega) - \\alpha_{n}^{m}(\\omega)\\right|^{2}}{\\sum_{n=0}^{N}\\sum_{m=-n}^{n} \\left|\\alpha_{n}^{m}(\\omega)\\right|^{2}}}.\n$$\n将每个 $\\varepsilon_{\\mathrm{rel}}$ 报告为无量纲浮点数。\n\n测试套件规范：\n- 用例1（理想情况，旨在达到精确性）：类型 \"GL\"，$N=3$，$K=3$，$L_{\\theta}=4$，$M_{\\phi}=7$，$\\omega=2000$ $\\mathrm{rad/s}$。\n- 用例2（边缘情况，方位角欠采样混叠）：类型 \"GL\"，$N=3$，$K=3$，$L_{\\theta}=4$，$M_{\\phi}=3$，$\\omega=2000$ $\\mathrm{rad/s}$。\n- 用例3（边缘情况，仰角权重不精确性）：类型 \"EQ\"，$N=3$，$K=3$，$L_{\\theta}=4$，$M_{\\phi}=7$，$\\omega=2000$ $\\mathrm{rad/s}$。\n- 用例4（边界情况，单极子）：类型 \"EQ\"，$N=0$，$K=0$，$L_{\\theta}=2$，$M_{\\phi}=3$，$\\omega=2000$ $\\mathrm{rad/s}$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如，\"[result1,result2,result3,result4]\"），其中每个“result”是对应测试用例的 $\\varepsilon_{\\mathrm{rel}}$，通过默认的浮点数到字符串转换进行四舍五入。角度必须以弧度为单位，角频率必须以 $\\mathrm{rad/s}$ 为单位。输出是无量纲的。",
            "solution": "从离散样本中近似球谐系数是声学、地球物理学和计算机图形学等使用球面数据的领域中的一个核心任务。推导始于球谐变换及其通过求积进行离散近似的基本定义。\n\n令 $H(\\omega, \\Omega)$ 为单位球面 $S^2$ 上的一个复值函数，表示在固定角频率 $\\omega$ 下的头部相关传递函数 (HRTF)。方向 $\\Omega$ 由余纬 $\\theta \\in [0, \\pi]$ 和方位角 $\\phi \\in [0, 2\\pi)$ 参数化。函数 $H(\\omega, \\Omega)$ 可以在一组正交归一复球谐函数基 $Y_n^m(\\Omega)$ 中展开为：\n$$\nH(\\omega, \\Omega) = \\sum_{n=0}^{\\infty}\\sum_{m=-n}^{n} \\alpha_{n}^{m}(\\omega)\\, Y_{n}^{m}(\\Omega)\n$$\n展开系数 $\\alpha_{n}^{m}(\\omega)$ 由球面上平方可积函数空间 $L^2(S^2)$ 上的内积确定：\n$$\n\\alpha_{n}^{m}(\\omega) = \\langle H, Y_n^m \\rangle = \\int_{S^2} H(\\omega,\\Omega)\\, Y_{n}^{m*}(\\Omega)\\, d\\Omega\n$$\n其中 $d\\Omega = \\sin\\theta\\,d\\theta\\,d\\phi$ 是曲面面积元，* 表示复共轭。球谐函数的正交归一性表示为：\n$$\n\\langle Y_{n'}^{m'}, Y_n^m \\rangle = \\int_{S^2} Y_{n'}^{m'}(\\Omega)\\, Y_{n}^{m*}(\\Omega)\\, d\\Omega = \\delta_{nn'}\\delta_{mm'}\n$$\n其中 $\\delta_{ij}$ 是克罗内克（Kronecker）δ函数。\n\n在实践中，$H(\\omega, \\Omega)$ 通常只在一组离散的采样方向 $\\{\\Omega_i\\}_{i=1}^I$ 上已知。用于计算 $\\alpha_{n}^{m}(\\omega)$ 的连续积分必须通过离散和来近似。这通过使用球面求积法则来实现，该法则由一组采样点 $\\{\\Omega_i\\}$ 和相应的权重 $\\{w_i\\}$ 定义：\n$$\n\\int_{S^2} f(\\Omega)\\, d\\Omega \\approx \\sum_{i=1}^{I} w_i f(\\Omega_i)\n$$\n将此应用于系数积分，我们得到离散球谐变换，从而得出估计系数 $\\widehat{\\alpha}_{n}^{m}(\\omega)$：\n$$\n\\widehat{\\alpha}_{n}^{m}(\\omega) = \\sum_{i=1}^{I} w_i H(\\omega, \\Omega_i) Y_n^{m*}(\\Omega_i)\n$$\n为了分析此近似中的误差，我们假设真实的 HRTF 是带限的，其合成阶数为 $K$。这意味着其球谐展开是有限的：\n$$\nH(\\omega, \\Omega) = \\sum_{k=0}^{K}\\sum_{l=-k}^{k} \\alpha_{k}^{l}(\\omega)\\, Y_{k}^{l}(\\Omega)\n$$\n我们通过将此展开式代入 $\\widehat{\\alpha}_{n}^{m}$ 的公式中，来计算给定分析阶数 $N$ (通常 $N \\ge K$) 的估计系数 $\\widehat{\\alpha}_{n}^{m}$：\n$$\n\\widehat{\\alpha}_{n}^{m}(\\omega) = \\sum_{i=1}^{I} w_i \\left( \\sum_{k=0}^{K}\\sum_{l=-k}^{k} \\alpha_{k}^{l}(\\omega)\\, Y_{k}^{l}(\\Omega_i) \\right) Y_n^{m*}(\\Omega_i)\n$$\n通过交换求和顺序，我们得到：\n$$\n\\widehat{\\alpha}_{n}^{m}(\\omega) = \\sum_{k=0}^{K}\\sum_{l=-k}^{k} \\alpha_{k}^{l}(\\omega) \\left( \\sum_{i=1}^{I} w_i Y_{k}^{l}(\\Omega_i) Y_n^{m*}(\\Omega_i) \\right)\n$$\n括号中的项是内积 $\\langle Y_k^l, Y_n^m \\rangle$ 基于求积的近似：\n$$\n\\sum_{i=1}^{I} w_i Y_{k}^{l}(\\Omega_i) Y_n^{m*}(\\Omega_i) \\approx \\int_{S^2} Y_{k}^{l}(\\Omega) Y_n^{m*}(\\Omega) d\\Omega = \\delta_{kl}\\delta_{mn}\n$$\n两个球谐函数之积 $Y_{k}^{l}(\\Omega) Y_{n}^{m*}(\\Omega)$ 是一个阶数最高为 $k+n$ 的球面多项式。如果一个求积法则能够精确地对所有阶数最高为 $D_{\\text{max}}$ 的球面多项式进行积分，则称该求积法则的精度达到 $D_{\\text{max}}$ 阶。如果我们的求积法则的精度达到 $K+N$ 阶，那么离散和将精确等于积分，因此：\n$$\n\\sum_{i=1}^{I} w_i Y_{k}^{l}(\\Omega_i) Y_n^{m*}(\\Omega_i) = \\delta_{kl}\\delta_{mn} \\quad \\text{for all } k \\le K, n \\le N\n$$\n在这种理想情况下，估计系数变为：\n$$\n\\widehat{\\alpha}_{n}^{m}(\\omega) = \\sum_{k=0}^{K}\\sum_{l=-k}^{k} \\alpha_{k}^{l}(\\omega)\\, \\delta_{kl}\\delta_{mn} = \\begin{cases} \\alpha_{n}^{m}(\\omega)  \\text{if } n \\le K \\\\ 0  \\text{if } n > K \\end{cases}\n$$\n这表明，如果求积精度达到 $K+N$ 阶，那么直到 $K$ 阶的系数都能被完美恢复。对于 $n > K$ 的系数，由于带限假设，真实的 $\\alpha_n^m$ 为零，因此它们被正确地识别为零。\n\n当求积精度未达到 $K+N$ 阶时，就会产生误差。我们考虑两种乘积求积格式。乘积法则将对 $d\\Omega = d u\\,d\\phi$（其中 $u=\\cos\\theta$）的积分分解为两个一维积分。\n球谐函数的方位角部分是 $e^{im\\phi}$。乘积 $Y_{k}^{l}Y_{n}^{m*}$ 包含形式为 $e^{i(l-m)\\phi}$ 的方位角项。具有 $M_{\\phi}$ 个等距点的梯形法则能对 $|q|  M_{\\phi}$ 的 $e^{iq\\phi}$ 进行精确积分。$|l-m|$ 的最大值是 $K+N$（因为 $|l| \\le K$ 且 $|m| \\le N$）。因此，为了实现精确的方位角积分，我们需要 $M_{\\phi} > K+N$。如果违反此条件（例如 $M_{\\phi} \\le K+N$），就会发生方位角混叠：来自 $k \\ne n$ 或 $l \\ne m$ 的非零贡献会出现，因为方位角基函数的离散傅里叶变换不再是正交的。\n\n余纬部分涉及连带勒让德多项式乘积的积分，这些是关于 $u=\\cos\\theta$ 的多项式。被积函数的多项式阶数最高为 $K+N$。\n- **高斯-勒让德 (GL) 求积：** 一个 $L_{\\theta}$ 点的高斯-勒让德法则是对 $u$ 中最高为 $2L_{\\theta}-1$ 阶的多项式精确的。为确保余纬部分的精确性，我们需要 $2L_{\\theta}-1 \\ge K+N$。\n- **等角 (EQ) 求积：** 此格式使用等距的 $\\theta_j$ 和权重 $w_{jk} \\propto \\sin\\theta_j$。这是积分 $\\int f(\\theta)\\sin\\theta\\,d\\theta$ 的一个简单黎曼和近似，并未为多项式精度而设计。它只是一种一阶方法，对于不很平滑或非恒定的函数会引入显著误差，因为它甚至无法精确积分低阶勒让德多项式。该误差是由于权重和节点对于多项式积分的非最优性造成的，我们称之为仰角权重不精确性。\n\n总而言之，$\\widehat{\\alpha}_{n}^{m}$ 中的总误差是以下因素的组合：\n1.  **方位角混叠**：如果 $M_{\\phi} \\le K+N$ 则发生，导致系数之间的泄漏。\n2.  **仰角权重不精确性**：如果余纬求积的精度未达到 $K+N$ 阶则发生。这在 EQ 格式中是固有的，但在 GL 格式中可以通过足够数量的点来避免（$2L_{\\theta}-1 \\ge K+N$）。\n\n相对均方根误差 $\\varepsilon_{\\mathrm{rel}}$ 的计算为这些效应提供了定量度量。\n$$\n\\varepsilon_{\\mathrm{rel}} = \\sqrt{\\frac{\\sum_{n=0}^{N}\\sum_{m=-n}^{n} \\left|\\widehat{\\alpha}_{n}^{m}(\\omega) - \\alpha_{n}^{m}(\\omega)\\right|^{2}}{\\sum_{n=0}^{N}\\sum_{m=-n}^{n} \\left|\\alpha_{n}^{m}(\\omega)\\right|^{2}}}\n$$\n对于基准系数 $\\alpha_n^m(\\omega)$，任何 $n > K$ 的项都为零。我们的计算必须反映这一点。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import sph_harm, roots_legendre\n\ndef solve():\n    \"\"\"\n    Solves the HRTF spherical harmonic coefficient estimation problem for the given test cases.\n    \"\"\"\n    test_cases = [\n        # (Type, N, K, L_theta, M_phi, omega)\n        (\"GL\", 3, 3, 4, 7, 2000.0),  # Case 1: Happy path, exactness aimed\n        (\"GL\", 3, 3, 4, 3, 2000.0),  # Case 2: Azimuthal undersampling\n        (\"EQ\", 3, 3, 4, 7, 2000.0),  # Case 3: Elevation-weight inexactness\n        (\"EQ\", 0, 0, 2, 3, 2000.0),  # Case 4: Monopole\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        quad_type, N, K, L_theta, M_phi, omega = case\n        omega_0 = 3000.0\n        j_unit = 1j\n\n        # Helper function to get a flat index from (n, m)\n        # sph_harm order is m, n\n        def nm_to_idx(n, m):\n            return n * n + n + m\n\n        # Step 1: Compute ground-truth coefficients alpha_n^m\n        # The problem asks to compute error up to degree N, so we need ground truth up to N.\n        # But the HRTF is synthesized from coeffs up to K. So, alpha_n^m = 0 for n > K.\n        max_deg_for_alpha = max(N, K)\n        num_coeffs_total = (max_deg_for_alpha + 1)**2\n        alpha_true = np.zeros(num_coeffs_total, dtype=np.complex128)\n\n        freq_term = np.exp(-(omega / omega_0)**2)\n        for n_k in range(K + 1):\n            for m_l in range(-n_k, n_k + 1):\n                idx = nm_to_idx(n_k, m_l)\n                alpha_true[idx] = freq_term * ((-1)**m_l / (n_k + 1)) * np.exp(j_unit * 0.2 * (m_l + n_k))\n        \n        # Step 2: Define sampling grid and weights\n        if quad_type == \"GL\":\n            # Gauss-Legendre in colatitude, trapezoidal in azimuth\n            u_nodes, u_weights = roots_legendre(L_theta)  # u = cos(theta)\n            theta_nodes = np.arccos(u_nodes)\n            phi_nodes = np.linspace(0, 2 * np.pi, M_phi, endpoint=False)\n            \n            delta_phi = 2 * np.pi / M_phi\n            weights = u_weights * delta_phi\n            \n            # Create meshgrid\n            phi_grid, theta_grid = np.meshgrid(phi_nodes, theta_nodes)\n            weights_grid = np.meshgrid(np.zeros(M_phi), weights)[1]\n\n        elif quad_type == \"EQ\":\n            # Equiangular in colatitude, trapezoidal in azimuth\n            delta_theta = np.pi / L_theta\n            theta_nodes = delta_theta * (np.arange(L_theta) + 0.5)\n            phi_nodes = np.linspace(0, 2 * np.pi, M_phi, endpoint=False)\n            \n            delta_phi = 2 * np.pi / M_phi\n            \n            # Create meshgrid and weights\n            phi_grid, theta_grid = np.meshgrid(phi_nodes, theta_nodes)\n            weights_grid = np.sin(theta_grid) * delta_theta * delta_phi\n        \n        else:\n            raise ValueError(\"Unknown quadrature type\")\n\n        # Step 3: Synthesize HRTF field H(omega, Omega_i) on the grid\n        H_samples = np.zeros_like(theta_grid, dtype=np.complex128)\n        for n_k in range(K + 1):\n            for m_l in range(-n_k, n_k + 1):\n                idx = nm_to_idx(n_k, m_l)\n                if np.abs(alpha_true[idx]) > 0:\n                    # scipy.special.sph_harm(m, n, azimuth, polar)\n                    Y_kl = sph_harm(m_l, n_k, phi_grid, theta_grid)\n                    H_samples += alpha_true[idx] * Y_kl\n\n        # Step 4: Approximate coefficients alpha_hat via quadrature\n        num_coeffs_analysis = (N + 1)**2\n        alpha_hat = np.zeros(num_coeffs_analysis, dtype=np.complex128)\n        \n        for n in range(N + 1):\n            for m in range(-n, n + 1):\n                # scipy.special.sph_harm(m, n, azimuth, polar)\n                # We need the complex conjugate Y_n^m*\n                Ynm_conj = np.conj(sph_harm(m, n, phi_grid, theta_grid))\n                \n                # Perform the weighted sum\n                integrand = H_samples * Ynm_conj * weights_grid\n                alpha_hat_nm = np.sum(integrand)\n                \n                idx = nm_to_idx(n, m)\n                alpha_hat[idx] = alpha_hat_nm\n\n        # Step 5: Calculate relative RMS error\n        # We need to compare over all degrees up to N\n        # Ground truth coefficients for n > K are zero.\n        alpha_true_for_err = np.zeros(num_coeffs_analysis, dtype=np.complex128)\n        \n        # Copy relevant ground truth coeffs\n        max_deg_for_copy = min(N,K)\n        for n in range(max_deg_for_copy + 1):\n            for m in range(-n, n + 1):\n                idx = nm_to_idx(n, m)\n                alpha_true_for_err[idx] = alpha_true[idx]\n\n        #\n        # E_rel = sqrt( sum(|alpha_hat - alpha_true|^2) / sum(|alpha_true|^2) )\n        # Sums are over n=0..N, m=-n..n\n        #\n        \n        diff_coeffs = alpha_hat - alpha_true_for_err\n        \n        numerator = np.sum(np.abs(diff_coeffs)**2)\n        denominator = np.sum(np.abs(alpha_true_for_err)**2)\n        \n        if denominator  1e-30: # handles cases where true coeffs are all zero\n            if numerator  1e-30:\n                error = 0.0\n            else:\n                error = np.inf\n        else:\n            error = np.sqrt(numerator / denominator)\n        \n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "完整的HRTF数据集通常非常庞大，这给存储和实时处理带来了挑战。本练习探讨了主成分分析（PCA），这是一种用于数据降维的强大技术。您将学习如何降低HRTF数据集的维度，并关键地评估压缩程度与重要感知线索（如定位和音色）保留之间的权衡。",
            "id": "4125598",
            "problem": "考虑一个头相关传递函数（HRTF）数据集，表示为一个实值矩阵 $\\mathbf{X} \\in \\mathbb{R}^{M \\times N}$，其中行索引耳-频率单元，列索引声源方向。假设数据源于线性、时不变的传播，非线性效应可忽略不计，并通过对 $\\mathbf{X}$ 进行奇异值分解（SVD）来应用主成分分析。令 $\\mathbf{X} = \\mathbf{U}\\,\\boldsymbol{\\Sigma}\\,\\mathbf{V}^\\top$ 表示其SVD，其中 $\\mathbf{U} \\in \\mathbb{R}^{M \\times M}$ 和 $\\mathbf{V} \\in \\mathbb{R}^{N \\times N}$ 是标准正交矩阵，$\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{M \\times N}$ 的对角线上有非负奇异值 $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge 0$。将最佳秩-$k$近似 $\\mathbf{X}_k$ 定义为在所有秩-$k$矩阵中，使重构误差的弗罗贝尼乌斯范数最小化的矩阵。从弗罗贝尼乌斯范数的标准正交不变性和SVD分解出发，推导最佳秩-$k$近似的相对重构误差（用奇异值表示），并为指定的$k$计算该误差。\n\n为将重构误差与感知后果联系起来，考虑以下双耳线索和音色度量：\n- 双耳级差（ILD），为每个频率-方向单元定义为 $\\mathrm{ILD} = 20 \\log_{10}\\!\\left(\\frac{|H_L|}{|H_R|}\\right)$，单位为分贝（dB），其中 $|H_L|$ 和 $|H_R|$ 是左耳和右耳的幅度响应（无量纲振幅比）。\n- 每只耳朵的对数谱距离（LSD），定义为所有单元上 $20 \\log_{10}\\!\\left(\\frac{|H_{\\text{ear}}^{\\text{orig}}|}{|H_{\\text{ear}}^{\\text{recon}}|}\\right)$ 的均方根，单位为分贝（dB），然后对左耳和右耳求平均，以获得单一的音色渲染代理指标。\n\n假设以下经过充分测试的感知阈值：\n- ILD恰可察觉差（JND）：$0.5$ dB。\n- LSD渲染可听阈值：$1.0$ dB。\n\n对于每个指定的$k$，计算$\\mathbf{X}_k$相对于$\\mathbf{X}$的相对弗罗贝尼乌斯重构误差，并判断ILD均方根误差和平均LSD是否超过上述阈值。\n\n使用以下合成的、科学上合理的HRTF数据集。频率和方向如下：\n- 频率（单位：赫兹）：$[500, 1500, 3000, 6000]$。\n- 方位角方向（单位：度）：$[-60, -15, 15, 60]$。\n\n通过为每个方向堆叠左耳然后右耳的幅度响应（无量纲线性幅度）来构建 $\\mathbf{X}$。对于每个方向列 $j$，将 $|H_L|(\\text{freq}, j)$ 和 $|H_R|(\\text{freq}, j)$ 定义为：\n- 方向 $-60^\\circ$：\n  - $|H_L|$: $[1.20, 1.30, 1.50, 1.80]$,\n  - $|H_R|$: $[0.80, 0.70, 0.60, 0.50]$.\n- 方向 $-15^\\circ$：\n  - $|H_L|$: $[1.10, 1.15, 1.20, 1.30]$,\n  - $|H_R|$: $[0.95, 0.90, 0.85, 0.80]$.\n- 方向 $15^\\circ$：\n  - $|H_L|$: $[0.95, 0.90, 0.85, 0.80]$,\n  - $|H_R|$: $[1.10, 1.15, 1.20, 1.30]$.\n- 方向 $60^\\circ$：\n  - $|H_L|$: $[0.80, 0.70, 0.60, 0.50]$,\n  - $|H_R|$: $[1.20, 1.30, 1.50, 1.80]$.\n\n因此，$\\mathbf{X} \\in \\mathbb{R}^{8 \\times 4}$，其中每一列等于由相应方向的 $|H_L|$ 四频率向量和 $|H_R|$ 四频率向量串联而成的 $8 \\times 1$ 向量。\n\n对于测试套件中的每个秩 $k$，执行以下计算：\n1. 计算相对重构误差，即 $\\mathbf{X} - \\mathbf{X}_k$ 的弗罗贝尼乌斯范数除以 $\\mathbf{X}$ 的弗罗贝尼乌斯范数，结果为一个无量纲浮点数。\n2. 计算原始ILD与重构ILD在所有频率-方向单元上的均方根误差（单位：dB）；然后确定一个布尔值，指示其是否超过 $0.5$ dB。\n3. 计算所有单元上双耳平均LSD（单位：dB）；然后确定一个布尔值，指示其是否超过 $1.0$ dB。\n\n角度必须以度为单位处理；幅度响应是无量纲线性量；ILD和LSD必须在内部以分贝（dB）为单位计算和报告。最终程序输出必须将所有提供的测试用例的结果聚合为单行，格式为用方括号括起来的逗号分隔列表，其中每个测试用例的结果是一个三元组 $[\\text{relative\\_error}, \\text{ILD\\_exceeds}, \\text{LSD\\_exceeds}]$。相对误差使用浮点数，阈值超出情况使用布尔值。\n\n测试套件：\n- 用例1：$k=0$（边界情况：无分量）。\n- 用例2：$k=1$（强降维）。\n- 用例3：$k=2$（中等降维）。\n- 用例4：$k=4$（边界情况：满秩，等于 $\\min(M,N)$）。\n\n您的程序应生成一行输出，其中包含结果，格式为用方括号括起来的逗号分隔列表（例如：\"[[0.123456,True,False],[...],...]\"）。",
            "solution": "此问题经评估有效。它在科学上基于计算声学和线性代数的原理，特别是将主成分分析（PCA）通过奇异值分解（SVD）应用于头相关传递函数（HRTF）数据。该问题是适定的，提供了所有必要的数据、定义和约束，确保了唯一且可计算的解。语言客观精确，设定无矛盾或歧义。\n\n我们首先为HRTF矩阵$\\mathbf{X}$的降维和重构误差的计算建立理论基础。随后，我们定义感知度量并概述要遵循的计算过程。\n\n**1. 理论框架：PCA与重构误差**\n\n该问题将HRTF数据集建模为一个实值矩阵 $\\mathbf{X} \\in \\mathbb{R}^{M \\times N}$，其中 $M=8$ 是耳-频率单元的数量，$N=4$ 是声源方向的数量。主成分分析（PCA）通过对 $\\mathbf{X}$ 应用奇异值分解（SVD）来执行：\n$$\n\\mathbf{X} = \\mathbf{U}\\,\\boldsymbol{\\Sigma}\\,\\mathbf{V}^\\top\n$$\n这里，$\\mathbf{U} \\in \\mathbb{R}^{M \\times M}$ 和 $\\mathbf{V} \\in \\mathbb{R}^{N \\times N}$ 是标准正交矩阵，其列（主成分或基向量）分别为 $\\mathbf{u}_i$ 和 $\\mathbf{v}_i$。$\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{M \\times N}$ 是一个矩形对角矩阵，按降序包含非负奇异值 $\\sigma_i$，即 $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r > 0$，其中 $r = \\text{rank}(\\mathbf{X})$。\n\nEckart-Young-Miriam定理指出，在所有秩为$k$的矩阵$\\mathbf{A}$中，$\\mathbf{X}$的最佳秩-$k$近似（表示为$\\mathbf{X}_k$），即最小化误差的弗罗贝尼乌斯范数 $\\|\\mathbf{X} - \\mathbf{A}\\|_F$的矩阵，由截断SVD给出：\n$$\n\\mathbf{X}_k = \\sum_{i=1}^k \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^\\top = \\mathbf{U}_k\\,\\boldsymbol{\\Sigma}_k\\,\\mathbf{V}_k^\\top\n$$\n其中$\\mathbf{U}_k$和$\\mathbf{V}_k$分别包含$\\mathbf{U}$和$\\mathbf{V}$的前$k$列，$\\boldsymbol{\\Sigma}_k$是包含前$k$个奇异值的$k \\times k$对角矩阵。\n\n相对重构误差是误差矩阵的弗罗贝尼乌斯范数与原始矩阵的弗罗贝尼乌斯范数之比。弗罗贝尼乌斯范数 $\\|\\mathbf{A}\\|_F$ 在与标准正交矩阵相乘下是不变的。原始矩阵$\\mathbf{X}$的范数是：\n$$\n\\|\\mathbf{X}\\|_F^2 = \\|\\mathbf{U}\\,\\boldsymbol{\\Sigma}\\,\\mathbf{V}^\\top\\|_F^2 = \\|\\boldsymbol{\\Sigma}\\|_F^2 = \\sum_{i=1}^r \\sigma_i^2\n$$\n误差矩阵是 $\\mathbf{E}_k = \\mathbf{X} - \\mathbf{X}_k = \\sum_{i=k+1}^r \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^\\top$。误差的范数是：\n$$\n\\|\\mathbf{X} - \\mathbf{X}_k\\|_F^2 = \\left\\| \\sum_{i=k+1}^r \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^\\top \\right\\|_F^2 = \\sum_{i=k+1}^r \\sigma_i^2\n$$\n因此，相对重构误差由下式给出：\n$$\nE_{\\text{rel}}(k) = \\frac{\\|\\mathbf{X} - \\mathbf{X}_k\\|_F}{\\|\\mathbf{X}\\|_F} = \\sqrt{\\frac{\\sum_{i=k+1}^r \\sigma_i^2}{\\sum_{i=1}^r \\sigma_i^2}}\n$$\n\n**2. 感知度量与评估**\n\n为了评估近似的感知影响，我们评估两个标准度量：\n\n- **双耳级差（ILD）**：对于每个频率-方向单元，ILD以分贝（dB）定义为：\n    $$\n    \\mathrm{ILD} = 20 \\log_{10}\\!\\left(\\frac{|H_L|}{|H_R|}\\right)\n    $$\n    其中$|H_L|$和$|H_R|$是左耳和右耳的幅度响应。感知误差通过原始ILD值与重构ILD值在所有单元上的均方根（RMS）误差来量化。如果此误差超过$0.5$ dB的恰可察觉差（JND），则设置一个布尔标志。\n\n- **对数谱距离（LSD）**：此度量量化音色渲染。对于每只耳朵，它是所有单元上对数谱差异的均方根：\n    $$\n    \\mathrm{LSD}_{\\text{ear}} = \\sqrt{\\text{mean}\\left( \\left( 20 \\log_{10}\\!\\left(\\frac{|H_{\\text{ear}}^{\\text{orig}}|}{|H_{\\text{ear}}^{\\text{recon}}|}\\right) \\right)^2 \\right)}\n    $$\n    最终的LSD度量是左耳和右耳LSD值的平均值。如果此平均值超过$1.0$ dB的可听阈值，则设置一个布尔标志。\n\n当重构幅度$|H^{\\text{recon}}|$为零或负时，对数未定义，会出现一个关键的数值问题。为确保稳定性，在计算ILD和LSD之前，我们将所有重构幅度裁剪到一个小的正底值$\\epsilon$（例如，浮点数的机器ε）。\n\n**3. 算法步骤**\n\n对于每个秩 $k \\in \\{0, 1, 2, 4\\}$，执行以下步骤：\n\n1.  **数据矩阵构建**：从提供的左耳（$|H_L|$）和右耳（$|H_R|$）幅度响应向量组装$8 \\times 4$矩阵$\\mathbf{X}$。$\\mathbf{X}$的每一列对应一个声源方向，由垂直堆叠的4频率$|H_L|$向量和4频率$|H_R|$向量形成。\n\n2.  **SVD计算**：计算$\\mathbf{X}$的SVD，以找到矩阵$\\mathbf{U}$、$\\boldsymbol{\\Sigma}$（表示为奇异值向量$\\boldsymbol{\\sigma}$）和$\\mathbf{V}^\\top$。\n\n3.  **秩-k重构**：构建近似矩阵$\\mathbf{X}_k$。对于$k=0$，$\\mathbf{X}_0$是一个零矩阵。对于$k>0$，使用前$k$个奇异值和相应的奇异向量形成$\\mathbf{X}_k$。对于$k=4$，即给定矩阵的满秩，$\\mathbf{X}_4$将是$\\mathbf{X}$的近乎完美的重构。\n\n4.  **误差计算**：\n    a.  计算相对弗罗贝尼乌斯误差$E_{\\text{rel}}(k)$。\n    b.  将原始矩阵$\\mathbf{X}$和重构矩阵$\\mathbf{X}_k$划分为其左耳和右耳子矩阵：$\\mathbf{H}_L^{\\text{orig}}$、$\\mathbf{H}_R^{\\text{orig}}$、$\\mathbf{H}_L^{\\text{recon}}$和$\\mathbf{H}_R^{\\text{recon}}$。\n    c.  将重构幅度矩阵裁剪为不小于一个小的正常数$\\epsilon$。\n    d.  计算原始ILD和重构ILD矩阵，并找到均方根误差。将其与$0.5$ dB阈值进行比较。\n    e.  计算左耳LSD和右耳LSD，并计算它们的平均值。将其与$1.0$ dB阈值进行比较。\n\n5.  **结果汇总**：为每个$k$值收集计算出的相对误差以及ILD和LSD阈值超出的两个布尔标志。\n\n该过程提供了一个定量评估，说明基于PCA的降维如何影响HRTF表示的数值准确性和感知保真度。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Solves the HRTF analysis problem by performing SVD-based PCA,\n    calculating reconstruction errors, and evaluating perceptual metrics.\n    \"\"\"\n    # Define the HRTF magnitude data as per the problem statement.\n    # Each list corresponds to the 4 specified frequencies.\n    # Directions: -60, -15, 15, 60 degrees.\n    data = {\n        -60: {\n            'L': [1.20, 1.30, 1.50, 1.80],\n            'R': [0.80, 0.70, 0.60, 0.50]\n        },\n        -15: {\n            'L': [1.10, 1.15, 1.20, 1.30],\n            'R': [0.95, 0.90, 0.85, 0.80]\n        },\n        15: {\n            'L': [0.95, 0.90, 0.85, 0.80],\n            'R': [1.10, 1.15, 1.20, 1.30]\n        },\n        60: {\n            'L': [0.80, 0.70, 0.60, 0.50],\n            'R': [1.20, 1.30, 1.50, 1.80]\n        }\n    }\n    \n    directions = [-60, -15, 15, 60]\n    \n    # Construct the M x N data matrix X, where M=8 and N=4.\n    columns = []\n    for az in directions:\n        h_l = np.array(data[az]['L'])\n        h_r = np.array(data[az]['R'])\n        column = np.concatenate((h_l, h_r))\n        columns.append(column)\n    X = np.stack(columns, axis=1)\n\n    # Perform Singular Value Decomposition (SVD).\n    # Since M > N, full_matrices=False is efficient.\n    U, s, Vt = np.linalg.svd(X, full_matrices=False)\n    \n    # Calculate the Frobenius norm of the original matrix.\n    norm_X = np.linalg.norm(X, 'fro')\n\n    # Define perceptual thresholds.\n    ild_jnd = 0.5  # dB\n    lsd_threshold = 1.0  # dB\n    \n    # Small epsilon for numerical stability of log operations.\n    epsilon = np.finfo(float).eps\n\n    # Separate original data into Left and Right ear matrices (4 freqs x 4 dirs).\n    num_freqs = 4\n    X_L_orig = X[:num_freqs, :]\n    X_R_orig = X[num_freqs:, :]\n\n    # Pre-calculate original ILD.\n    # Clip to avoid division by zero, although not expected for original data.\n    X_R_orig_clipped = np.maximum(X_R_orig, epsilon)\n    ILD_orig = 20 * np.log10(X_L_orig / X_R_orig_clipped)\n    \n    test_cases = [0, 1, 2, 4]\n    results = []\n\n    for k in test_cases:\n        # 1. Reconstruct the rank-k approximation matrix X_k.\n        if k == 0:\n            X_k = np.zeros_like(X)\n        else:\n            # Reconstruct from the first k components.\n            U_k = U[:, :k]\n            s_k = s[:k]\n            Vt_k = Vt[:k, :]\n            X_k = (U_k * s_k) @ Vt_k\n\n        # 2. Compute the relative Frobenius reconstruction error.\n        relative_error = np.linalg.norm(X - X_k, 'fro') / norm_X\n        # An alternative using singular values directly:\n        # if k  len(s):\n        #     error_norm_sq = np.sum(s[k:]**2)\n        #     total_norm_sq = np.sum(s**2)\n        #     relative_error = np.sqrt(error_norm_sq / total_norm_sq)\n        # else:\n        #     relative_error = 0.0\n\n        # 3. Compute ILD RMS error.\n        # Separate reconstructed data into Left and Right ear matrices.\n        X_L_recon = X_k[:num_freqs, :]\n        X_R_recon = X_k[num_freqs:, :]\n        \n        # Clip reconstructed magnitudes to avoid log(0) or log(negative).\n        X_L_recon_clipped = np.maximum(X_L_recon, epsilon)\n        X_R_recon_clipped = np.maximum(X_R_recon, epsilon)\n        \n        # Compute reconstructed ILD.\n        ILD_recon = 20 * np.log10(X_L_recon_clipped / X_R_recon_clipped)\n        \n        # Calculate ILD RMS error and check against JND.\n        ild_rms_error = np.sqrt(np.mean((ILD_orig - ILD_recon)**2))\n        ild_exceeds = ild_rms_error > ild_jnd\n\n        # 4. Compute average LSD.\n        # Clip original magnitudes for ratio computation (best practice, though not needed here).\n        X_L_orig_clipped = np.maximum(X_L_orig, epsilon)\n        X_R_orig_clipped = np.maximum(X_R_orig, epsilon)\n        \n        # Calculate Log-Spectral Distance (LSD) for each ear.\n        lsd_L = np.sqrt(np.mean((20 * np.log10(X_L_orig_clipped / X_L_recon_clipped))**2))\n        lsd_R = np.sqrt(np.mean((20 * np.log10(X_R_orig_clipped / X_R_recon_clipped))**2))\n        \n        # Calculate average LSD and check against threshold.\n        avg_lsd = (lsd_L + lsd_R) / 2.0\n        lsd_exceeds = avg_lsd > lsd_threshold\n\n        # Store the results for this k.\n        results.append([relative_error, ild_exceeds, lsd_exceeds])\n\n    # Format and print the final output as a single line.\n    # The map(str, ...) converts each inner list to its string representation.\n    # The join and f-string formatting matches the required output.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "开发新的音频渲染技术只是成功的一半；我们还必须通过听音测试来证明其有效性。本练习将深入探讨实验数据的统计分析。您将学习如何运用假设检验来确定个性化HRTF系统是否比通用系统有统计学上显著的改进，这是进行研究和开发的一项关键技能。",
            "id": "4125608",
            "problem": "给定来自个性化头部相关传输函数 (HRTF) 与通用 HRTF 在双耳渲染中的双盲被试内评估的配对测量数据。在每个配对样本中，第一个值对应于通用 HRTF 的性能，第二个值对应于同一听者和条件下个性化 HRTF 的性能。目标是根据配对差异的正态性评估，为每个指标选择合适的单侧配对假设检验，以评估个性化是否比通用 HRTF 产生统计上显著的改进。\n\n使用以下基本原则进行假设检验和检验选择：\n- 样本 $i$ 的配对差异定义为 $d_i = x_i - y_i$，其中 $x_i$ 和 $y_i$ 分别是通用和个性化条件下的配对测量值，$d_i$ 的符号选择使得改进对应于 $d_i > 0$。\n- 在原假设 $H_0$ 下，对于学生配对 t 检验，差异的均值为零；对于 Wilcoxon 符号秩检验，差异的中位数为零。\n- 当样本量 $n \\ge 8$ 时，首先在显著性水平 $\\alpha_\\text{norm} = 0.05$ 下使用 Shapiro–Wilk 检验评估差异的正态性来选择检验方法。如果正态性未被拒绝（即 Shapiro–Wilk 的 $p$值大于或等于 $\\alpha_\\text{norm}$），则使用配对 t 检验。否则，使用 Wilcoxon 符号秩检验。对于 $n  8$ 的情况，使用 Wilcoxon 符号秩检验。将全为零的差异视为不显著，$p$值为 $1.0$。\n- 使用对应于改进（$d_i > 0$）的单侧备择假设。对于值越低越好的指标（例如，误差或失真），定义 $d_i = x_i - y_i$，以便改进产生正差异。对于值越高越好的指标（例如，质量评分），定义 $d_i = y_i - x_i$，以便改进产生正差异。\n\n对于每个测试用例，计算 $p$值并确定在 $\\alpha = 0.05$ 水平下改进是否具有统计显著性。使用代码 $0$ 表示学生配对 t 检验，使用代码 $1$ 表示 Wilcoxon 符号秩检验来报告所使用的检验。\n\n物理和数值单位：\n- 定位误差角度必须以度为单位处理；在描述数据时以度报告角度（所有提供的角度值都已是度）。\n- 频谱失真必须以分贝为单位处理；在描述数据时以分贝报告值（所有提供的分贝值都已是分贝）。\n- 质量评分是在 $0$ 到 $100$ 范围内的无单位数值。\n- 所有假设检验决策和 $p$值都是无单位的。\n\n您的程序必须处理以下测试套件。对于每个测试用例，程序必须：\n- 以正确的方向（值越低越好或值越高越好）构建配对差异。\n- 当 $n \\ge 8$ 时，对差异应用 Shapiro–Wilk 正态性检验，以选择配对 t 检验或 Wilcoxon 符号秩检验；当 $n  8$ 时，使用 Wilcoxon 符号秩检验。\n- 计算改进的单侧 $p$值。\n- 在 $\\alpha = 0.05$ 水平上判断显著性。\n- 将最终的 $p$值四舍五入到六位小数。\n\n测试套件：\n1. 定位误差（度），值越低越好。通用与个性化对比（每位听者）：\n   - 通用：$\\{12.4, 10.1, 9.8, 14.3, 11.7, 13.0, 10.9, 12.1, 9.5, 15.2, 13.6, 11.3, 12.8, 10.5, 11.9, 9.7, 12.2, 13.1, 10.4, 11.5, 12.0, 14.0, 13.4, 10.8\\}$\n   - 个性化：$\\{8.7, 7.9, 7.5, 10.2, 8.8, 9.3, 7.6, 8.9, 6.8, 11.1, 9.7, 8.3, 9.4, 7.7, 8.6, 7.0, 9.1, 9.8, 7.9, 8.5, 9.2, 10.5, 9.9, 7.8\\}$\n   - 角度单位：度。\n2. 频谱失真（分贝），值越低越好：\n   - 通用：$\\{5.6, 4.9, 6.1, 5.2, 4.7, 5.8, 6.3, 5.0, 5.4, 4.8, 6.0, 5.5\\}$\n   - 个性化：$\\{4.9, 4.4, 5.5, 4.8, 4.3, 5.1, 5.7, 4.7, 5.0, 4.5, 5.4, 5.1\\}$\n   - 单位：分贝。\n3. 外化质量评分（$0$ 到 $100$），值越高越好：\n   - 通用：$\\{62, 58, 70, 65, 60, 72, 68, 61, 64, 59, 66, 67, 63, 71, 69, 60, 62, 64\\}$\n   - 个性化：$\\{75, 72, 82, 78, 74, 85, 80, 73, 76, 70, 79, 81, 77, 84, 83, 73, 76, 78\\}$\n   - 单位：$\\{0, \\dots, 100\\}$ 上的无单位评分。\n4. 定位误差（度），值越低越好，变化极小（边界情况）：\n   - 通用：$\\{10.2, 9.8, 10.0, 9.7, 10.1, 10.3, 9.9, 10.0, 9.6, 10.2\\}$\n   - 个性化：$\\{10.1, 9.9, 10.1, 9.8, 10.2, 10.2, 10.0, 10.1, 9.7, 10.3\\}$\n   - 角度单位：度。\n5. 外化质量评分（$0$ 到 $100$），值越高越好，有平局的极小样本：\n   - 通用：$\\{70, 75, 80\\}$\n   - 个性化：$\\{70, 78, 82\\}$\n   - 单位：$\\{0, \\dots, 100\\}$ 上的无单位评分。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个元素对应一个测试用例，并且本身是一个形式为 $[p\\_value, decision, test\\_code]$ 的三元素列表。这里 $p\\_value$ 是一个四舍五入到六位小数的浮点数，$decision$ 是一个布尔值，指示在 $\\alpha = 0.05$ 水平下改进是否显著，$test\\_code$ 是一个整数，对于学生配对 t 检验等于 $0$，对于 Wilcoxon 符号秩检验等于 $1$。例如，输出应如下所示：$\\big[[p_1, d_1, t_1], [p_2, d_2, t_2], \\dots\\big]$。\n\n所有角度必须解释为度，所有频谱失真解释为分贝，所有评分解释为无单位整数。不应读取任何外部输入；在程序中直接使用提供的测试套件。",
            "solution": "该问题要求进行一系列单侧配对假设检验，以确定个性化头部相关传输函数 (HRTF) 是否比通用 HRTF 提供统计上显著的改进。分析基于被试内实验的配对数据。任务的核心是为每个数据集选择合适的统计检验，执行检验，并解释结果。\n\n基本过程包括以下步骤：\n1.  **构建配对差异**：对于每对测量值 $(x_i, y_i)$（分别代表通用和个性化条件），计算差异 $d_i$。差异的符号定义为正值 $d_i > 0$ 对应于改进。\n    -   对于值越低越好的指标（例如，误差），差异为 $d_i = x_i - y_i$。\n    -   对于值越高越好的指标（例如，质量评分），差异为 $d_i = y_i - x_i$。\n\n2.  **陈述假设**：原假设 $H_0$ 假定个性化没有带来改进。备择假设 $H_A$ 假定有显著改进。\n    -   $H_0$：差异的集中趋势（均值或中位数）为零。\n    -   $H_A$：差异的集中趋势大于零。\n\n3.  **选择统计检验**：在参数化的学生配对 t 检验和非参数化的 Wilcoxon 符号秩检验之间的选择取决于配对差异 $d_i$ 的分布。\n    -   使用 Shapiro-Wilk 检验在显著性水平 $\\alpha_{\\text{norm}} = 0.05$ 下评估差异的正态性。此检验仅在样本量 $n$ 至少为 $8$ 时应用。\n    -   如果 $n \\ge 8$ 且 Shapiro-Wilk 检验的 $p$值大于或等于 $\\alpha_{\\text{norm}}$，则不拒绝正态性的原假设，并选择学生配对 t 检验。检验代码为 $0$。\n    -   如果 $n \\ge 8$ 且 Shapiro-Wilk 检验的 $p$值小于 $\\alpha_{\\text{norm}}$，则数据被认为不服从正态分布，并选择 Wilcoxon 符号秩检验。检验代码为 $1$。\n    -   如果样本量很小，$n  8$，则默认使用 Wilcoxon 符号秩检验。检验代码为 $1$。\n\n4.  **计算 p 值并做出决策**：执行所选的单侧假设检验。将得到的 $p$值与显著性水平 $\\alpha = 0.05$ 进行比较。如果 $p$值小于 $\\alpha$，则拒绝原假设，采纳备择假设，表明存在统计上显著的改进。\n\n一个特殊情况是所有差异 $d_i$ 均为零，此时结果被认为不显著，$p$值为 $1.0$。\n\n我们现在将此方法应用于所提供的五个测试用例中的每一个。\n\n**测试用例 1：定位误差**\n-   **指标**：定位误差（度），一个值越低越好的指标。\n-   **数据**：$n=24$ 对配对测量。\n-   **差异**：$d_i = x_i - y_i$（通用 - 个性化）。差异是根据提供的数据计算得出的。\n-   **检验选择**：样本量为 $n=24 \\ge 8$。我们对差异 $d_i$ 进行 Shapiro-Wilk 检验：\n    $$d = \\{3.7, 2.2, 2.3, 4.1, 2.9, 3.7, 3.3, 3.2, 2.7, 4.1, 3.9, 3.0, 3.4, 2.8, 3.3, 2.7, 3.1, 3.3, 2.5, 3.0, 2.8, 3.5, 3.5, 3.0\\}$$\n    Shapiro-Wilk 检验得出的 $p$值约为 $0.517$。由于 $0.517 \\ge \\alpha_{\\text{norm}} = 0.05$，我们不拒绝正态性的原假设。因此，选择学生配对 t 检验。检验代码为 $0$。\n-   **假设检验**：我们进行单侧配对 t 检验（$H_A: \\mu_d > 0$）。这得出的 $p$值约为 $1.13 \\times 10^{-14}$。\n-   **决策**：$p$值远小于 $\\alpha = 0.05$。改进具有统计显著性。\n-   **结果**：$[0.000000, \\text{True}, 0]$。\n\n**测试用例 2：频谱失真**\n-   **指标**：频谱失真（分贝），一个值越低越好的指标。\n-   **数据**：$n=12$ 对配对测量。\n-   **差异**：$d_i = x_i - y_i$。\n-   **检验选择**：样本量为 $n=12 \\ge 8$。对差异应用 Shapiro-Wilk 检验：\n    $$d = \\{0.7, 0.5, 0.6, 0.4, 0.4, 0.7, 0.6, 0.3, 0.4, 0.3, 0.6, 0.4\\}$$\n    该检验得出的 $p$值约为 $0.106$。由于 $0.106 \\ge 0.05$，我们选择学生配对 t 检验。检验代码为 $0$。\n-   **假设检验**：单侧配对 t 检验得出的 $p$值约为 $1.01 \\times 10^{-7}$。\n-   **决策**：$p$值小于 $\\alpha = 0.05$。改进具有统计显著性。\n-   **结果**：$[0.000001, \\text{True}, 0]$。\n\n**测试用例 3：外化质量评分**\n-   **指标**：质量评分，一个值越高越好的指标。\n-   **数据**：$n=18$ 对配对测量。\n-   **差异**：$d_i = y_i - x_i$（个性化 - 通用）。\n-   **检验选择**：样本量为 $n=18 \\ge 8$。对差异应用 Shapiro-Wilk 检验：\n    $$d = \\{13, 14, 12, 13, 14, 13, 12, 12, 12, 11, 13, 14, 14, 13, 14, 13, 14, 14\\}$$\n    该检验得出的 $p$值约为 $0.002$。由于 $0.002  0.05$，我们拒绝正态性假设，选择非参数化的 Wilcoxon 符号秩检验。检验代码为 $1$。\n-   **假设检验**：单侧 Wilcoxon 符号秩检验得出的 $p$值约为 $3.81 \\times 10^{-6}$。所有差异均为正，这代表了反对原假设的最强可能证据。\n-   **决策**：$p$值小于 $\\alpha = 0.05$。改进具有统计显著性。\n-   **结果**：$[0.000004, \\text{True}, 1]$。\n\n**测试用例 4：定位误差（变化极小）**\n-   **指标**：定位误差（度），一个值越低越好的指标。\n-   **数据**：$n=10$ 对配对测量。\n-   **差异**：$d_i = x_i - y_i$。\n-   **检验选择**：样本量为 $n=10 \\ge 8$。对差异进行 Shapiro-Wilk 检验：\n    $$d = \\{0.1, -0.1, -0.1, -0.1, -0.1, 0.1, -0.1, -0.1, -0.1, -0.1\\}$$\n    得出的 $p$值非常小，约为 $6.0 \\times 10^{-5}$。由于该值小于 $0.05$，我们拒绝正态性，选择 Wilcoxon 符号秩检验。检验代码为 $1$。\n-   **假设检验**：进行单侧 Wilcoxon 符号秩检验以检验差异的中位数是否大于 $0$。数据由两个正差异和八个相同大小的负差异组成。该检验得出的 $p$值约为 $0.967$。\n-   **决策**：$p$值远大于 $\\alpha = 0.05$。没有统计上显著的改进。\n-   **结果**：$[0.966797, \\text{False}, 1]$。\n\n**测试用例 5：外化质量评分（小样本）**\n-   **指标**：质量评分，一个值越高越好的指标。\n-   **数据**：$n=3$ 对配对测量。\n-   **差异**：$d_i = y_i - x_i$。差异为 $d = \\{0, 3, 2\\}$。\n-   **检验选择**：样本量为 $n=3  8$。因此，默认使用 Wilcoxon 符号秩检验。检验代码为 $1$。\n-   **假设检验**：对差异进行 Wilcoxon 检验。该检验通过丢弃零差异来自动处理它，将有效样本量减少到 $2$。剩下的差异是 $\\{3, 2\\}$。两者都为正。单侧检验得出的 $p$值为 $0.25$。\n-   **决策**：$p$值为 $0.25$，大于 $\\alpha=0.05$。改进不具有统计显著性，这对于如此小的样本量是预料之中的。\n-   **结果**：$[0.250000, \\text{False}, 1]$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Solves the hypothesis testing problem for personalized vs. generic HRTFs.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"name\": \"Localization error (degrees)\",\n            \"generic\": [12.4, 10.1, 9.8, 14.3, 11.7, 13.0, 10.9, 12.1, 9.5, 15.2, 13.6, 11.3, 12.8, 10.5, 11.9, 9.7, 12.2, 13.1, 10.4, 11.5, 12.0, 14.0, 13.4, 10.8],\n            \"personalized\": [8.7, 7.9, 7.5, 10.2, 8.8, 9.3, 7.6, 8.9, 6.8, 11.1, 9.7, 8.3, 9.4, 7.7, 8.6, 7.0, 9.1, 9.8, 7.9, 8.5, 9.2, 10.5, 9.9, 7.8],\n            \"improvement_type\": \"lower_is_better\"\n        },\n        {\n            \"name\": \"Spectral distortion (decibels)\",\n            \"generic\": [5.6, 4.9, 6.1, 5.2, 4.7, 5.8, 6.3, 5.0, 5.4, 4.8, 6.0, 5.5],\n            \"personalized\": [4.9, 4.4, 5.5, 4.8, 4.3, 5.1, 5.7, 4.7, 5.0, 4.5, 5.4, 5.1],\n            \"improvement_type\": \"lower_is_better\"\n        },\n        {\n            \"name\": \"Externalization quality rating\",\n            \"generic\": [62, 58, 70, 65, 60, 72, 68, 61, 64, 59, 66, 67, 63, 71, 69, 60, 62, 64],\n            \"personalized\": [75, 72, 82, 78, 74, 85, 80, 73, 76, 70, 79, 81, 77, 84, 83, 73, 76, 78],\n            \"improvement_type\": \"higher_is_better\"\n        },\n        {\n            \"name\": \"Localization error (degrees), minimal change\",\n            \"generic\": [10.2, 9.8, 10.0, 9.7, 10.1, 10.3, 9.9, 10.0, 9.6, 10.2],\n            \"personalized\": [10.1, 9.9, 10.1, 9.8, 10.2, 10.2, 10.0, 10.1, 9.7, 10.3],\n            \"improvement_type\": \"lower_is_better\"\n        },\n        {\n            \"name\": \"Externalization quality rating, small sample\",\n            \"generic\": [70, 75, 80],\n            \"personalized\": [70, 78, 82],\n            \"improvement_type\": \"higher_is_better\"\n        }\n    ]\n\n    alpha = 0.05\n    alpha_norm = 0.05\n    results = []\n\n    for case in test_cases:\n        generic = np.array(case[\"generic\"])\n        personalized = np.array(case[\"personalized\"])\n        \n        if case[\"improvement_type\"] == \"lower_is_better\":\n            differences = generic - personalized\n        else: # higher_is_better\n            differences = personalized - generic\n            \n        n = len(differences)\n        \n        # Determine which test to use initially\n        test_code = 1 # Wilcoxon by default\n        if n = 8:\n            # Only run Shapiro-Wilk if there's variance in the data\n            if np.std(differences)  0:\n                shapiro_p = stats.shapiro(differences).pvalue\n                if shapiro_p = alpha_norm:\n                    test_code = 0 # t-test\n            else: # No variance, so perfectly normal\n                 test_code = 0 # t-test\n        \n        # Handle the special case of all-zero differences\n        if np.all(differences == 0):\n            p_value = 1.0\n            # Test code can be based on the n-based selection logic above\n        else:\n            # Perform the selected test\n            if test_code == 0: # Student's paired t-test\n                p_value = stats.ttest_1samp(differences, 0, alternative='greater').pvalue\n            else: # Wilcoxon signed-rank test\n                # The Wilcoxon test automatically handles zero differences by default\n                # by discarding them. If all differences are zero, it would raise an error,\n                # but we've handled that case above.\n                p_value = stats.wilcoxon(differences, alternative='greater').pvalue\n\n        decision = p_value  alpha\n        rounded_p_value = round(p_value, 6)\n        \n        results.append([rounded_p_value, decision, test_code])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}