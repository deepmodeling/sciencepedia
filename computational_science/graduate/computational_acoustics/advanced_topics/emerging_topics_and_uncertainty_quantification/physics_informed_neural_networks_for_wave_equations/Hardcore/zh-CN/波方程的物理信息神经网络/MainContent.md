## 引言
[物理信息神经网络](@entry_id:145229)（[PINNs](@entry_id:145229)）正迅速成为科学计算领域一种颠覆性的范式，它将深度学习的强大[函数逼近](@entry_id:141329)能力与基础物理定律相结合，为[求解偏微分方程](@entry_id:138485)（PDEs）开辟了新途径。在声学、地震学和众多工程领域，波动方程是描述现象的核心，而利用PINN求解波动方程是当前计算科学的前沿热点。传统数值方法如有限元或[有限差分法](@entry_id:1124968)虽然成熟，但在处理复杂几何、高维空间以及尤其棘手的反演问题时，往往面临网格生成复杂、计算成本高昂等挑战。PINN提供了一种无网格、数据驱动与物理驱动相融合的灵活框架，有望克服这些局限性。

本文旨在为读者提供一个关于使用PINN求解波动方程的全面而深入的指南。在第一章“原理与机制”中，我们将深入剖析PINN的数学基础、核心机制以及在应用于波动方程时特有的训练挑战与解决方案。接着，在第二章“应用与跨学科连接”中，我们将展示PINN如何解决声学和地球物理学中复杂的正向与反演问题，并探讨其在固体力学、量子力学等领域的广泛联系。最后，在第三章“动手实践”中，我们提供了一系列精心设计的问题，旨在将理论知识转化为实践技能。通过这一结构化的学习路径，本文将带领读者从基本原理出发，逐步掌握先进的应用技巧，全面理解PINN在波动现象建模中的强大威力与巨大潜力。

## 原理与机制

本章旨在深入探讨[物理信息神经网络](@entry_id:145229)（PINNs）用于求解[波动方程](@entry_id:139839)的核心科学原理与关键技术机制。在前一章介绍[PINNs](@entry_id:145229)的基本概念之后，本章将系统地剖析[PINNs](@entry_id:145229)的数学基础、实现方法，以及在应用于波动方程这类[双曲型偏微分方程](@entry_id:1126291)时所面临的特有挑战与先进的应对策略。我们将从一个适定数学问题的构建开始，逐步深入到[损失函数](@entry_id:634569)的设计、自动微分的实现细节，最终探讨训练过程中的[病态问题](@entry_id:137067)及其解决方案。

### PINNs的数学问题构建

任何成功的数值方法都始于一个**适定的（well-posed）**数学问题表述。对于PINNs而言，这意味着在将其转化为优化问题之前，必须确保所要解决的物理问题在数学上是完整且唯一可解的。这为神经网络的训练提供了清晰的目标。

#### [波动方程](@entry_id:139839)的导出与适定性条件

[波动方程](@entry_id:139839)是描述声学、电磁学和弹性力学中波动现象的基础。在无损、均匀的声学介质中，声压 $p(\boldsymbol{x}, t)$ 的波动行为可以通过线性化的[质量守恒](@entry_id:204015)和[动量守恒](@entry_id:149964)定律导出。给定平衡质量密度 $\rho_0$ 和声速 $c$，线性化的[动量平衡](@entry_id:1128118)和[质量平衡方程](@entry_id:178786)分别为：

$$
\rho_0 \partial_t \boldsymbol{u} + \nabla p = \boldsymbol{0}
$$
$$
\frac{1}{\rho_0 c^2} \partial_t p + \nabla \cdot \boldsymbol{u} = 0
$$

其中 $\boldsymbol{u}$ 是质点速度。通过对第一个方程求散度，对第二个方程求时间导数，并消去 $\boldsymbol{u}$，我们可以得到齐次[声波方程](@entry_id:746230) ：

$$
\frac{1}{c^2} \frac{\partial^2 p}{\partial t^2} - \Delta p = 0
$$

其中 $\Delta = \nabla \cdot \nabla$ 是拉普拉斯算子。这是一个二阶[双曲型偏微分方程](@entry_id:1126291)。为了确保在有界域 $\Omega \subset \mathbb{R}^d$ 和时间区间 $(0, T)$ 上的解是唯一的，必须提供一套完备的**初始条件（Initial Conditions, ICs）**和**边界条件（Boundary Conditions, BCs）**。

1.  **初始条件**: 由于方程在时间上是二阶的，需要规定系统在初始时刻 $t=0$ 的状态。这包括初始声压场 $p(\boldsymbol{x}, 0) = p_0(\boldsymbol{x})$ 和初始声压场的时间变化率 $\partial_t p(\boldsymbol{x}, 0) = v_0(\boldsymbol{x})$。这两项分别对应于初始位移和初始速度，二者缺一不可。

2.  **边界条件**: 边界条件描述了波在区域边界 $\partial\Omega$ 上的行为。在每个[边界点](@entry_id:176493)上，通常只指定一个条件，否则问题可能变得**超定（over-determined）**而不存在解。常见的边界条件包括 ：
    *   **狄利克雷（Dirichlet）条件**: 直接指定边界上的声压值，即 $p(\boldsymbol{x}, t) = g_D(\boldsymbol{x}, t)$。一个常见的特例是**声压释放边界（pressure-release boundary）**，对应于 $p=0$，物理上可理解为“软”边界，例如与真空的交界面。
    *   **诺伊曼（Neumann）条件**: 指定边界上的法向[质点](@entry_id:186768)速度，这通过[动量平衡](@entry_id:1128118)方程与声压的法向梯度相关。一个**刚性壁（rigid wall）**边界意味着法向速度为零，即 $v_n = \boldsymbol{v} \cdot \boldsymbol{n} = 0$。在时谐声学中，这等价于声压的法向导数为零，$\partial_n p = \nabla p \cdot \boldsymbol{n} = 0$。这代表了一个“硬”边界。
    *   **罗宾（Robin）或阻抗（Impedance）条件**: 这是一个混合条件，将声压与其法向导数[线性组合](@entry_id:154743)起来，形式为 $\alpha p + \beta \partial_n p = g_R$。它在物理上模拟了更复杂的边界行为，如能量吸收或辐射。例如，一个**声阻抗边界（impedance boundary）**将边界上的声压与法向[质点](@entry_id:186768)速度通过声阻抗 $Z_s$ 联系起来：$p = Z_s v_n$。这等价于一个特定的罗宾条件，可以模拟多孔材料或向无限远[空间辐射](@entry_id:1132013)声能的边界。

一个包含波动方程、两个初始条件和一个边界条件（如狄利克雷、诺伊曼或满足[能量耗散](@entry_id:147406)/守恒的[罗宾条件](@entry_id:153384)）的系统，构成了一个适定的[初边值问题](@entry_id:1126514)。这为PINN的损失函数设计提供了完整的数学依据 。

### 核心机制：物理信息[损失函数](@entry_id:634569)

PINN的核心思想是将一个神经网络 $p_\theta(\boldsymbol{x}, t)$ 作为[偏微分](@entry_id:194612)方程（PDE）的解的代理模型，并通过最小化一个精心设计的**复合损失函数**来训练其参数 $\theta$。这个[损失函数](@entry_id:634569)将上文讨论的[适定问题](@entry_id:176268)中的所有数学[约束编码](@entry_id:197822)为可优化的目标 。

典型的复合损失函数 $\mathcal{L}(\theta)$ 是多个分量的加权和：

$$
\mathcal{L}(\theta) = \lambda_{\text{PDE}} \mathcal{L}_{\text{PDE}}(\theta) + \lambda_{\text{IC}} \mathcal{L}_{\text{IC}}(\theta) + \lambda_{\text{BC}} \mathcal{L}_{\text{BC}}(\theta)
$$

各个分量的作用如下：

*   **PDE残差损失 $\mathcal{L}_{\text{PDE}}$**: 该项强制网络在求解域的内部满足物理定律。它通过惩罚在大量**[配置点](@entry_id:169000)（collocation points）** $(\boldsymbol{x}, t) \in \Omega \times (0, T)$ 上的PDE残差的范数（通常是均方误差）来实现。对于[波动方程](@entry_id:139839)，残差定义为 $r_\theta = \frac{1}{c^2} \partial_{tt}p_\theta - \Delta p_\theta - s$，其中 $s$ 是源项。
    $$
    \mathcal{L}_{\text{PDE}}(\theta) = \frac{1}{N_{\text{PDE}}} \sum_{i=1}^{N_{\text{PDE}}} | r_\theta(\boldsymbol{x}_i, t_i) |^2
    $$
    这一项是PINN“物理信息”的来源，它允许网络在没有大量标记数据的情况下学习解。

*   **初始条件损失 $\mathcal{L}_{\text{IC}}$**: 该项确保解在时间上被正确定位。对于[波动方程](@entry_id:139839)，它包含两个部分，分别对应于初始声压和初始声压变化率。
    $$
    \mathcal{L}_{\text{IC}}(\theta) = \frac{1}{N_{\text{IC}}} \sum_{i=1}^{N_{\text{IC}}} \left( |p_\theta(\boldsymbol{x}_i, 0) - p_0(\boldsymbol{x}_i)|^2 + |\partial_t p_\theta(\boldsymbol{x}_i, 0) - v_0(\boldsymbol{x}_i)|^2 \right)
    $$
    这两个条件的满足是保证解唯一性的关键。

*   **边界条件损失 $\mathcal{L}_{\text{BC}}$**: 该项为解提供空间上的约束，确保其在边界上的行为符合物理实际。其具体形式取决于所施加的边界条件类型。
    $$
    \mathcal{L}_{\text{BC}}(\theta) = \frac{1}{N_{\text{BC}}} \sum_{i=1}^{N_{\text{BC}}} | \mathcal{B}[p_\theta](\boldsymbol{x}_i, t_i) - g(\boldsymbol{x}_i, t_i) |^2
    $$
    其中 $\mathcal{B}$ 是[边界算子](@entry_id:160216)（例如，对于[狄利克雷条件](@entry_id:137096)是[恒等算子](@entry_id:204623)，对于[诺伊曼条件](@entry_id:165471)是法向导数算子）。

*   **损失权重 $\lambda_i$**: 这些非负标量权重的作用至关重要。损失函数的不同分量通常具有不同的物理单位和数值尺度（例如，声压的平方 vs. 声压二阶导数的平方）。如果不对它们进行平衡，梯度下降过程可能会被某个梯度范数特别大的项所主导，导致其他约束得不到满足。因此，选择合适的权重（无论是手动调整还是自适应算法）对于训练的稳定性和收敛至物理一致的解至关重要。

#### [强形式与弱形式](@entry_id:1132543)PINNs

上述基于逐点残差的损失函数构建方法被称为**强形式（strong form）**PINN 。它要求网络近似 $p_\theta$ 至少是二次连续可微的（$C^2$），这样才能直接计算波动方程中的二阶导数。这对于标准的神经网络激活函数（如 `[tanh](@entry_id:636446)` 或 `sin`）通常是满足的，但对某些函数（如 `ReLU`）则不然。

另一种方法是**弱形式（weak form）**或**[变分形式](@entry_id:166033)（variational）**PINN。其思想源于有限元方法，即不要求PDE在每一点都精确成立，而是要求它在积分意义下对所有**测试函数（test functions）** $v$ 成立。通过将PDE乘以一个[测试函数](@entry_id:166589)并在整个时空域上积分，然后利用**[分部积分](@entry_id:136350)（integration by parts）**（或[格林公式](@entry_id:173118)），可以将微分算子从解的近似 $p_\theta$ “转移”到测试函数 $v$ 上。

例如，对于[波动方程](@entry_id:139839)，时空弱形式的核心项变为：
$$
\int_0^T \int_{\Omega} \left( -\partial_t p_\theta \partial_t v + c^2 \nabla p_\theta \cdot \nabla v \right) d\boldsymbol{x} dt
$$
在这个形式中，$p_\theta$ 和 $v$ 都只需要[一阶导数](@entry_id:749425)存在且平方可积，即属于[索博列夫空间](@entry_id:141995) $H^1$。这种方法将对网络近似的**正则性（regularity）**要求从二阶降至一阶，使得网络更容易满足约束。此外，[分部积分](@entry_id:136350)会自然地产生边界项，如 $\int_0^T \int_{\partial \Omega} c^2 (\partial_n p_\theta) v \, ds \, dt$，这使得诺伊曼等“自然”边界条件可以非常方便地融入到损失函数中 。

### PINNs的引擎：[自动微分](@entry_id:144512)

无论采用强形式还是弱形式，计算损失函数及其梯度都离不开对神经网络输出 $p_\theta$ 求导。特别是对于波动方程的强形式，需要计算 $\partial_{tt} p_\theta$ 和 $\Delta p_\theta$。这正是**自动微分（Automatic Differentiation, AD）**发挥关键作用的地方。AD是一种精确计算函数导数的技术，它通过在程序执行过程中系统地应用[链式法则](@entry_id:190743)来实现，避免了[符号微分](@entry_id:177213)的复杂表达式膨胀和[数值微分](@entry_id:144452)的[截断误差与舍入误差](@entry_id:164039)。

AD主要有两种模式 ：

*   **前向模式（Forward Mode）**: 从输入到输出，沿[计算图](@entry_id:636350)传播导数值。它非常适合计算一个函数对单个输入变量的导数，或者计算[雅可比-向量积](@entry_id:162748)（Jacobian-Vector Products, JVPs）。对于一个输入维度为 $N$、输出维度为 $M$ 的函数，计算完整的[雅可比矩阵](@entry_id:178326)需要 $N$ 次[前向传播](@entry_id:193086)。

*   **反向模式（Reverse Mode）**: 从输出到输入，沿[计算图](@entry_id:636350)反向传播伴随值（或称敏感度）。它非常适合计算一个标量输出函数对所有输入变量的梯度，只需要一次[前向传播](@entry_id:193086)和一次反向传播。深度学习中的**[反向传播算法](@entry_id:198231)**就是AD反向模式的一个特例。

对于PINN中的二阶导数，通常通过嵌套使用这两种模式来高效计算。例如，[波动方程](@entry_id:139839)的残差计算需要拉普拉斯算子 $\Delta p_\theta = \sum_{i=1}^d \partial_{x_i x_i} p_\theta$。一个实用的计算方法是：
1.  对每个空间维度 $i = 1, \dots, d$，计算**[海森-向量积](@entry_id:635156)（Hessian-Vector Product, HVP）** $H_{\boldsymbol{x}} p_\theta \cdot \boldsymbol{e}_i$，其中 $\boldsymbol{e}_i$ 是第 $i$ 个[标准基向量](@entry_id:152417)。这个HVP的结果是[海森矩阵](@entry_id:139140)的第 $i$ 列。
2.  提取该结果向量的第 $i$ 个分量，即为对角元 $\partial_{x_i x_i} p_\theta$。
3.  将所有对角元求和得到拉普拉斯算子的值。

每个HVP的计算成本大约相当于一次前向传播和一次[反向传播](@entry_id:199535)。因此，计算 $d$ 维空间中的拉普拉斯算子的总[时间复杂度](@entry_id:145062)与空间维度 $d$ **线性相关**。然而，由于这些HVP可以[顺序计算](@entry_id:273887)，其峰值内存需求大致与单次[反向传播](@entry_id:199535)相当，而不会随 $d$ 增长。这在处理高维问题时是一个重要的计算考量 。

### 训练病理学与先进架构

尽管[PINNs](@entry_id:145229)原理上很强大，但在应用于波动方程时，训练过程常常会遇到一些被称为“病态”（pathologies）的挑战。这些挑战主要源于神经网络的内在属性与[波动方程](@entry_id:139839)的高阶、高频特性之间的冲突。

#### [病态问题](@entry_id:137067)一：谱偏差

标准的[深度神经网络](@entry_id:636170)（如使用 `[tanh](@entry_id:636446)` 或 `ReLU` [激活函数](@entry_id:141784)）表现出强烈的**谱偏差（spectral bias）**：它们在训练初期会优先学习[目标函数](@entry_id:267263)的低频分量，而学习高频分量的速度则慢得多 。

这一现象可以通过**[神经正切核](@entry_id:634487)（Neural Tangent Kernel, NTK）**理论来精确描述。在无限宽度的极限下，梯度下降的训练动态等价于一个核回归问题。对于一个空间平移不变的核 $\Theta(x-x')$，其傅里叶模式 $e^{ikx}$ 是核算子的特征函数，对应的特征值为核的谱密度 $\widehat{\Theta}(k)$。训练过程中，第 $k$ 个傅里叶模式的误差 $e_k(\tau)$ 会以 $e^{-\widehat{\Theta}(k)\tau}$ 的速率指数衰减，其中 $\tau$ 是训练时间。

对于 `[tanh](@entry_id:636446)` 等光滑[激活函数](@entry_id:141784)，其对应的NTK谱密度 $\widehat{\Theta}(k)$ 随着波数 $|k|$ 的增大而迅速衰减。这意味着[高频模式](@entry_id:750297)的收敛速率 $\widehat{\Theta}(k)$ 非常小，导致学习过程极其缓慢。当PINN用于求解包含丰富高频信息的波动问题时，这种谱偏差就成了一个巨大的障碍。一个具体的计算示例表明，学习一个波数为 $k_5 = 5\pi/2$ 的模式所需的时间可能是学习基频模式 $k_1 = \pi/2$ 的数十倍之多 。

#### 解决方案一：改进[网络架构](@entry_id:268981)

为了克服谱偏差，研究者们开发了具有更优异频率特性的[网络架构](@entry_id:268981)。

*   **[正弦表示网络](@entry_id:1131718)（SIREN）**: SIREN在所有层都使用正弦函数 $\sigma(z) = \sin(z)$ 作为激活函数。这种架构的**归纳偏置（inductive bias）**与[波动方程](@entry_id:139839)的解的性质天然契合，因为[波动方程](@entry_id:139839)的解本身就是由正弦和余弦函数构成的 。更重要的是，$\sin$ 函数的任意阶导数都是有界的、非饱和的正弦/余弦函数。这保证了PINN在计算[高阶导数](@entry_id:140882)（如 $\partial_{tt}p_\theta$ 和 $\Delta p_\theta$）时，残差信号不会因激活函数导数饱和（趋于零）而消失，从而极大地改善了训练的稳定性和对高频细节的表征能力。相比之下，`ReLU` 网络的二阶导数[几乎处处](@entry_id:146631)为零，从根本上不适合用于求解[二阶PDE](@entry_id:175326) 。

*   **随机傅里叶特征（RFF）**: 另一种有效策略是在将坐标输入网络之前，先通过一个固定的**傅里叶特征映射**进行编码。例如，将一维输入 $x$ 映射为高维向量 $\phi(x) = [\cos(B_j x), \sin(B_j x)]_{j=1}^m$，其中频率 $B_j$ 从某个分布（如高斯分布 $\mathcal{N}(0, \sigma_B^2)$）中随机采样 。这种[预处理](@entry_id:141204)等效于使用一个具有特定谱密度的人造核。通过合理选择[采样分布](@entry_id:269683)，可以使有效NTK的谱密度在很宽的频率范围内保持平坦，从而显著提升网络学习高频分量的能力 。

### 训练病理学与先进训练策略

除了谱偏差，训练过程中的梯度动态本身也可能导致不稳定性。

#### [病态问题](@entry_id:137067)二：梯度病理

[波动方程](@entry_id:139839)的残差包含二阶导数。这会导致损失函数对网络参数的梯度与解的频率之间存在强烈的依赖关系，从而引发[训练不稳定性](@entry_id:634545)。通过一个简化的单模分析，可以证明，对于一个形如 $u_\theta = \theta \sin(kx)\sin(\omega t)$ 的解分量，其PDE[损失函数](@entry_id:634569)对幅度参数 $\theta$ 的梯度大小，在波数 $k$ 很大时，与 $k^4$ 成正比 。这意味着误差中的高频分量会产生巨大的梯度，导致**[梯度爆炸](@entry_id:635825)（exploding gradients）**，使优化步骤过大，从而破坏训练的稳定性。

#### 解决方案二：改进训练策略

针对梯度不稳定性，可以采用多种训练策略进行缓解。

*   **自适应损失权重**: 如前所述，复合损失函数的不同分量（PDE、IC、BC）的梯度范数可能相差几个数量级。一个强大的策略是**自适应地调整损失权重** $\lambda_i$，以平衡各个分量对总梯度的贡献。一种常见的方法是，在每次迭代中，选择 $\lambda_i$ 使得加权后的梯度范数 $\lambda_i \|\nabla_\theta \mathcal{L}_i\|_2$ 对所有分量 $i$ 都相等。在满足归一化约束（如 $\sum \lambda_i = m$）的条件下，可以推导出权重的显式更新规则 ：
    $$
    \lambda_{i}^{(t+1)} = \frac{m}{G_{i}^{(t)} \sum_{j=1}^{m} \frac{1}{G_{j}^{(t)}}}
    $$
    其中 $G_i^{(t)} = \|\nabla_\theta \mathcal{L}_i(\theta^{(t)})\|_2$ 是在第 $t$ 次迭代时计算的梯度范数。

*   **[梯度裁剪](@entry_id:634808)（Gradient Clipping）**: 这是一种简单而实用的启发式方法，它通过设置一个阈值 $\tau$，当[梯度向量](@entry_id:141180) $\boldsymbol{g}$ 的范数超过该阈值时，将其按比例缩回，即 $\tilde{\boldsymbol{g}} = \min\{1, \tau / \|\boldsymbol{g}\|\} \boldsymbol{g}$。这能有效防止单次过大的梯度更新破坏已经学到的网络参数 。

*   **[索博列夫训练](@entry_id:1131829)（Sobolev Training）**: 这是一种更具物理原理的[正则化方法](@entry_id:150559)。它建议在某个**负指数的[索博列夫范数](@entry_id:754999)**（如 $H^{-s}$）中度量PDE残差，而不是标准的 $L^2$ 范数。在傅里叶空间中，$\|r\|_{H^{-s}}^2 = \sum_k (1+k^2)^{-s} |r_k|^2$。这相当于对残差的每个频率分量 $r_k$ 施加一个与频率相关的权重 $(1+k^2)^{-s}$。这个权重会衰减高频分量的贡献。对于波动方程导致的 $k^4$ [梯度爆炸问题](@entry_id:637582)，选择 $s \ge 2$ 可以有效地使梯度范数在高频区域保持有界甚至衰减，从而[稳定训练](@entry_id:635987)过程 。

通过理解并综合运用这些先进的架构和训练策略，研究人员可以显著提升[物理信息神经网络](@entry_id:145229)在求解复杂波动问题时的准确性、稳定性和效率。