## Introduction
Computational acoustics often faces a trade-off between model fidelity and computational cost, making tasks like real-time analysis, [large-scale optimization](@entry_id:168142), and [uncertainty quantification](@entry_id:138597) prohibitively expensive. Machine learning surrogates emerge as a transformative solution, providing near-instantaneous approximations of complex acoustic systems without sacrificing physical consistency. The challenge, however, lies in creating surrogates that are not just fast but also reliable, accurate, and respectful of the underlying physics of wave propagation. This article addresses this need by providing a comprehensive guide to building and applying physics-informed machine learning surrogates.

The reader will embark on a structured journey through this powerful methodology. The "Principles and Mechanisms" chapter will lay the theoretical groundwork, exploring different surrogate architectures and techniques for embedding physical laws. Following this, the "Applications and Interdisciplinary Connections" chapter will demonstrate their real-world utility in engineering design, uncertainty quantification, and across diverse scientific fields from geophysics to cosmology. Finally, "Hands-On Practices" will offer concrete exercises to solidify these concepts. This structured approach begins by dissecting the core principles that enable machine learning models to effectively learn and represent the complex dynamics of acoustic phenomena.

## Principles and Mechanisms

Machine learning surrogates for acoustic models represent a paradigm shift in computational acoustics, offering the potential for rapid, repeated, and even real-time evaluation of complex physical systems. These surrogates are not mere black-box approximators; their power and reliability derive from their ability to learn and respect the underlying physics of wave propagation. This chapter delves into the fundamental principles and mechanisms that govern the design, training, and evaluation of these models. We will first establish a clear taxonomy of surrogate models, then explore the diverse and powerful techniques for embedding physical knowledge, and finally discuss the physical meaning of the mathematical metrics used to measure their success.

### A Taxonomy of Surrogate Models

At the highest level, [surrogate modeling](@entry_id:145866) tasks in acoustics can be categorized into two principal classes, distinguished by the nature of their inputs and outputs. This distinction is crucial as it dictates the appropriate choice of model architecture and training strategy.

#### Field Surrogates: Mapping Parameters to Solutions

The first and most common class of surrogates are **field surrogates**, also known as parameter-to-function maps. These models are designed to learn the relationship between a finite-dimensional vector of input parameters, $\boldsymbol{\theta}$, and the resulting acoustic field. The parameters can represent geometric properties, material characteristics, boundary conditions, or source parameters. The surrogate learns to approximate the solution manifold, which is the set of all possible solutions parameterized by $\boldsymbol{\theta}$.

A [canonical form](@entry_id:140237) for such a surrogate is a model that takes as input both the parameter vector $\boldsymbol{\theta}$ and a spatial coordinate $\boldsymbol{x}$, and outputs the predicted field value at that point, $p_{\text{surrogate}}(\boldsymbol{x}; \boldsymbol{\theta})$ . This approach allows the field to be queried at any location within the domain. An alternative and often more efficient structure is a model that maps the parameters $\boldsymbol{\theta}$ to a set of coefficients in a fixed basis, allowing for reconstruction of the entire field.

A powerful classical technique that falls under this category is the **Polynomial Chaos Expansion (PCE)**. In the context of [uncertainty quantification](@entry_id:138597), where input parameters $\boldsymbol{\xi}$ are random variables, PCE represents a quantity of interest, such as the pressure at a specific point $y(\boldsymbol{\xi}) = p(\mathbf{x}_0; \boldsymbol{\xi})$, as a spectral expansion in polynomials that are orthogonal with respect to the probability measure of the inputs. For a set of independent input variables $\boldsymbol{\xi} = (\xi_1, \dots, \xi_d)$, the multivariate [orthogonal basis](@entry_id:264024) $\{\Psi_j(\boldsymbol{\xi})\}$ is formed by the [tensor product](@entry_id:140694) of univariate polynomial families, where each family is chosen to be orthogonal with respect to the probability distribution of its corresponding input variable . For instance, for a parameter with a $\text{Uniform}$ distribution, Legendre polynomials are the appropriate basis, while for a $\text{Beta}$ distribution, Jacobi polynomials are used . The resulting expansion, $y(\boldsymbol{\xi}) \approx \sum_{j=0}^{P} \hat{y}_j \Psi_j(\boldsymbol{\xi})$, provides a surrogate that is not only computationally efficient but also provides direct access to statistical moments of the output.

#### Operator Surrogates: Mapping Functions to Functions

The second, more general class of surrogates are **operator surrogates**. These models are designed to learn mappings between infinite-dimensional [function spaces](@entry_id:143478). Instead of learning how a field changes with a few parameters, they learn to approximate the underlying solution operator of the governing partial differential equation itself. For example, an operator surrogate might learn the mapping from an arbitrary source distribution function $s(\boldsymbol{x})$ to the resulting pressure field $p(\boldsymbol{x})$, or from a spatially varying boundary condition function to the interior field.

A prominent example of an operator surrogate architecture is the **Deep Operator Network (DeepONet)**. This architecture is composed of two sub-networks: a **branch network** that processes the input function, and a **trunk network** that processes the coordinate of the output function's domain . The input function is typically represented by its values at a [discrete set](@entry_id:146023) of sensor points. The branch network encodes this function into a finite set of coefficients. The trunk network takes a query coordinate (e.g., a spatial location $\boldsymbol{x}$) and outputs a corresponding set of basis functions. The final prediction is the dot product of the branch coefficients and the trunk basis functions.

For instance, consider learning the operator that maps a spatially varying boundary impedance function $Z(\boldsymbol{x})$ on a surface $\Gamma$ to the interior acoustic pressure field $p(\boldsymbol{x})$ in a domain $\Omega$. The branch network would take samples of $Z(\boldsymbol{x})$ on $\Gamma$ as input, while the trunk network would take the interior coordinate $\boldsymbol{x}$ as input. The combined output approximates $p(\boldsymbol{x})$. This operator is fundamentally nonlinear in $Z(\boldsymbol{x})$ due to its appearance in the denominator of the [impedance boundary condition](@entry_id:750536), $\frac{\partial p}{\partial n} \propto \frac{p}{Z}$, making it an ideal candidate for a powerful nonlinear approximator like DeepONet . Other architectures, such as the Fourier Neural Operator (FNO), are also designed for this task of learning "neural operators" by performing key operations in the frequency domain . A classic example of an operator in acoustics is the Dirichlet-to-Neumann map, which relates a function on the boundary (e.g., normal velocity) to another function on the boundary (e.g., pressure), and a machine learning model that approximates this map is a quintessential operator surrogate .

### Encoding Physical Principles in Surrogate Models

The most effective surrogate models are not naive function approximators but are instead imbued with knowledge of the underlying physics. This "physics-informed" approach constrains the learning problem, leading to better accuracy, improved data efficiency, and enhanced generalization to unseen scenarios. Physical principles can be encoded in several ways.

#### Physics-Informed Feature Engineering

One of the most direct methods for incorporating physics is through the careful selection and transformation of the model's inputs and outputs. By non-dimensionalizing variables according to known physical scaling laws, we can factor out trivial dependencies and present the model with a better-conditioned learning problem.

A clear illustration of this principle is in modeling the acoustic modal frequencies of a rectangular room of size $L_x \times L_y \times L_z$. The analytical solution to the wave equation shows that any modal frequency $f$ scales linearly with the sound speed $c$ and inversely with a characteristic length of the room, $L_{\text{char}}$. A naive surrogate might struggle to learn these simple scaling laws from data. A much better approach is to build them in. We can define a characteristic length, for example the [geometric mean](@entry_id:275527) $L_g = (L_x L_y L_z)^{1/3}$, and provide the model with dimensionless inputs representing the room's *shape*, such as $\tilde{L}_i = L_i / L_g$. The model is then trained to predict a dimensionless frequency, $\tilde{f} = f L_g / c$. This transformed problem is independent of the absolute scale of the room and the speed of sound, allowing the neural network to focus solely on learning the complex relationship between the room's aspect ratios and its normalized spectrum. At inference time, the physical frequency is easily recovered via the inverse transformation, $\hat{f} = (c / L_g) \hat{\tilde{f}}$ .

This concept extends to surrogates for [numerical solvers](@entry_id:634411). The error of a numerical scheme, such as **numerical dispersion**, is not an arbitrary function of grid spacing $\Delta x$ and time step $\Delta t$, but is governed by dimensionless quantities like the normalized wavenumber $\kappa = k \Delta x$ and the Courant number $\sigma = c \Delta t / \Delta x$. A surrogate designed to learn and correct for this dispersion will be far more generalizable if it takes $\kappa$ and $\sigma$ as inputs, rather than the raw dimensional parameters .

#### Physics-Informed Priors and Architectures

A more profound way to incorporate physics is to design the model architecture itself to inherently satisfy certain physical laws.

**Causality** is a fundamental property of time-dependent wave propagation: an effect cannot precede its cause. For a time-domain surrogate that evolves a field step-by-step, this means the output at time $t$ must depend only on inputs from times $\tau \le t$. This constraint must be enforced architecturally. Architectures like bidirectional [recurrent neural networks](@entry_id:171248) (RNNs), which use information from both past and future time steps, are inherently non-causal and unsuitable. In contrast, **causal convolutions**, which use one-sided kernels that only look at past data, are a natural choice. For spatiotemporal problems governed by a finite [wave speed](@entry_id:186208), this can be extended to require that the model's [receptive field](@entry_id:634551) at any spacetime point remains within the physical past [light cone](@entry_id:157667). This can be achieved with carefully designed masked or [dilated convolutions](@entry_id:168178) that respect the cone structure, ensuring that information cannot propagate faster than the physical wave speed .

**Passivity** is another critical constraint for acoustic components like terminations or materials. It dictates that the component cannot generate energy. For a linear, [time-invariant system](@entry_id:276427), this property is mathematically equivalent to its complex impedance $Z(s)$ being a **positive-real (PR) function**. This means $Z(s)$ must be analytic in the right-half complex plane and have a non-negative real part there. A surrogate can be forced to be passive by constructing it to always produce a PR function. One method, drawn from classical network synthesis, is to parameterize the impedance using a known PR form, such as a Foster decomposition, and constrain its constituent parameters to be non-negative . An alternative, drawn from control theory, is to represent the impedance as the transfer function of a state-space model and enforce passivity by satisfying the conditions of the Positive-Real Lemma via Linear Matrix Inequality (LMI) constraints during training .

Finally, for probabilistic surrogates like **Gaussian Process Regression (GPR)**, physical knowledge can be encoded in the **prior**, specifically in the choice of the **kernel function**. The kernel defines the covariance between function values at different input points, thereby specifying our prior belief about the function's properties, such as smoothness, periodicity, or length scale. For example, if we are modeling the [transmission loss](@entry_id:1133371) of a panel, which is known to be a [smooth function](@entry_id:158037) of material parameters $\mathbf{m}$ but exhibit quasi-periodic behavior with respect to frequency $f$, we can design a composite kernel to reflect this. A product of a Squared Exponential (SE) kernel over $\mathbf{m}$ (to model smoothness) and a Periodic kernel over $f$ (to model periodicity) is an excellent choice for this task . This allows the GPR to effectively learn the distinct behaviors along different input dimensions.

#### Physics-Informed Loss Functions

Perhaps the most flexible method for informing a surrogate is to add penalty terms to its loss function that measure violations of physical laws. During training, the optimizer will be driven to find parameters that not only fit the training data but also minimize these physical residuals.

A prime example is the enforcement of **symmetry principles**, such as **[acoustic reciprocity](@entry_id:1120710)**. In a linear, time-invariant medium, the acoustic transfer function between a source at $\mathbf{x}_s$ and a receiver at $\mathbf{x}_r$ is identical to the transfer function with their positions swapped: $H(\mathbf{x}_r, \mathbf{x}_s) = H(\mathbf{x}_s, \mathbf{x}_r)$. To encourage a surrogate $h_\theta$ to learn this property, we can add a regularization term to the loss function that penalizes asymmetry: $\mathcal{L}_{\text{recip}} = \lambda \lVert h_\theta(\cdot, \mathbf{x}_s, \mathbf{x}_r) - h_\theta(\cdot, \mathbf{x}_r, \mathbf{x}_s) \rVert^2$ . The model is thus explicitly trained to produce symmetric predictions.

More broadly, the governing partial differential equations (PDEs) themselves can be incorporated into the loss. For any training point $\boldsymbol{x}$, one can evaluate the surrogate's prediction $p_\theta(\boldsymbol{x})$ and its derivatives (computed via automatic differentiation) and plug them into the PDE. The resulting residual—how far the equation is from being satisfied—can be penalized. For example, when training a DeepONet to learn the mapping from impedance $Z(\boldsymbol{x})$ to pressure $p(\boldsymbol{x})$, the loss function can include a term that penalizes the residual of the [impedance boundary condition](@entry_id:750536) on the boundary $\Gamma$ :
$$ \mathcal{L}_\Gamma = \frac{1}{N_\Gamma} \sum_{j=1}^{N_\Gamma} \left| \nabla p_\theta(\mathbf{x}_j) \cdot \mathbf{n}(\mathbf{x}_j) - \frac{i \omega \rho_0}{Z(\mathbf{x}_j)} p_\theta(\mathbf{x}_j) \right|^2 $$
This approach, central to Physics-Informed Neural Networks (PINNs), can guide the model towards physically consistent solutions, especially in regions with sparse data.

### Evaluating Surrogate Performance: Physical Interpretation of Error Norms

The choice of a loss function for training or a metric for evaluation is not merely a mathematical convenience; it has a direct physical interpretation that reflects what aspect of the acoustic field we prioritize matching. The most common metrics are based on [function space](@entry_id:136890) norms, particularly the $L^2$ and $H^1$ norms.

For a surrogate's pressure error field, $e(\mathbf{x}) = p_\theta(\mathbf{x}) - p(\mathbf{x})$, the squared **$L^2$ error norm** is defined as:
$$ \|e\|_{L^2(\Omega)}^2 = \int_\Omega |e(\mathbf{x})|^2 \,\mathrm{d}\mathbf{x} $$
In time-harmonic [linear acoustics](@entry_id:1127264), the time-averaged potential energy density is proportional to the squared magnitude of the pressure, $E_p \propto |p(\mathbf{x})|^2$. Therefore, the squared $L^2$ norm of the error is directly proportional to the total error in the system's potential energy . Training a surrogate by minimizing an $L^2$ loss means we are primarily trying to match the potential energy of the acoustic field.

The **$H^1$ error [seminorm](@entry_id:264573)**, on the other hand, measures the $L^2$ norm of the gradient of the error:
$$ |e|_{H^1(\Omega)}^2 = \int_\Omega \|\nabla e(\mathbf{x})\|^2 \,\mathrm{d}\mathbf{x} $$
From the linearized momentum equation, the particle velocity field $\mathbf{u}(\mathbf{x})$ is proportional to the pressure gradient, $\mathbf{u}(\mathbf{x}) \propto \nabla p(\mathbf{x})$. The time-averaged kinetic energy density is proportional to the squared magnitude of the velocity, $E_k \propto \|\mathbf{u}(\mathbf{x})\|^2$, which implies $E_k \propto \|\nabla p(\mathbf{x})\|^2$. Consequently, the squared $H^1$ [seminorm](@entry_id:264573) of the error is directly proportional to the total error in the system's kinetic energy . Minimizing an $H^1$ loss thus prioritizes matching the velocity field and the kinetic energy distribution. The full **$H^1$ error norm** combines both, $\|e\|_{H^1(\Omega)}^2 = \|e\|_{L^2(\Omega)}^2 + |e|_{H^1(\Omega)}^2$, and corresponds to minimizing the error in the total acoustic energy (potential plus kinetic). Understanding these connections allows for a principled choice of loss function tailored to the specific goals of the acoustic analysis.