{
    "hands_on_practices": [
        {
            "introduction": "Machine learning surrogates are frequently trained on data generated by numerical solvers. However, these solvers are not perfect and introduce their own errors. This foundational exercise explores one of the most common artifacts, numerical dispersion, where the numerical scheme causes waves of different frequencies to travel at incorrect speeds. By deriving the phase velocity error for a simple finite difference scheme , you will gain critical insight into the quality and limitations of the simulation data used to train any acoustic surrogate.",
            "id": "4128308",
            "problem": "Consider the one-dimensional ($1$-D) linear acoustic wave equation for particle displacement $u(x,t)$,\n$$\n\\frac{\\partial^{2} u}{\\partial t^{2}} = c^{2} \\frac{\\partial^{2} u}{\\partial x^{2}},\n$$\nwhere $c$ is the constant sound speed. On a uniform spatial grid with spacing $h$ and a uniform time step $\\Delta t$, discretize the equation using centered second-order finite differences in both space and time on the grid points $x_{j} = j h$ and time levels $t^{n} = n \\Delta t$, with the update\n$$\n\\frac{u_{j}^{n+1} - 2 u_{j}^{n} + u_{j}^{n-1}}{\\Delta t^{2}} = c^{2} \\frac{u_{j+1}^{n} - 2 u_{j}^{n} + u_{j-1}^{n}}{h^{2}}.\n$$\nAssume a plane-wave solution of the form $u_{j}^{n} = A \\exp\\!\\big( \\mathrm{i} (k j h - \\omega n \\Delta t) \\big)$, where $A$ is a complex amplitude, $k$ is the continuous wavenumber, and $\\omega$ is the angular frequency. Starting from this discretization and the plane-wave ansatz, derive the numerical dispersion relation linking $\\omega$ and $k$. Then define the numerical phase velocity $c_{p}^{\\text{num}}(k) = \\omega / k$ and the relative phase velocity error\n$$\n\\varepsilon(k h; r) = \\frac{c_{p}^{\\text{num}}(k)}{c} - 1,\n$$\nwhere the Courant number is $r = c \\Delta t / h$ and the nondimensional wavenumber is $\\xi = k h$. Assume the principal branch for the angular frequency such that $0 \\le \\omega \\Delta t \\le \\pi$ and $0 \\le \\xi \\le \\pi$. Express the final answer as a single closed-form analytic expression for $\\varepsilon(k h; r)$ in terms of $k h$ and $r$. No numerical evaluation is required, and no units are to be reported in the final answer.",
            "solution": "The user wants to derive the relative phase velocity error for a finite difference discretization of the 1D wave equation.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Governing Equation:** The one-dimensional ($1$-D) linear acoustic wave equation for particle displacement $u(x,t)$ is\n$$\n\\frac{\\partial^{2} u}{\\partial t^{2}} = c^{2} \\frac{\\partial^{2} u}{\\partial x^{2}}\n$$\nwhere $c$ is the constant sound speed.\n- **Discretization Scheme:** A centered second-order finite difference scheme is used:\n$$\n\\frac{u_{j}^{n+1} - 2 u_{j}^{n} + u_{j}^{n-1}}{\\Delta t^{2}} = c^{2} \\frac{u_{j+1}^{n} - 2 u_{j}^{n} + u_{j-1}^{n}}{h^{2}}\n$$\nwhere $x_j = j h$, $t^n = n \\Delta t$, $h$ is the spatial grid spacing, and $\\Delta t$ is the time step.\n- **Plane-Wave Ansatz:** The solution is assumed to be of the form:\n$$\nu_{j}^{n} = A \\exp\\!\\big( \\mathrm{i} (k j h - \\omega n \\Delta t) \\big)\n$$\nwhere $A$ is a complex amplitude, $k$ is the continuous wavenumber, and $\\omega$ is the angular frequency.\n- **Definitions:**\n    - Numerical phase velocity: $c_{p}^{\\text{num}}(k) = \\omega / k$.\n    - Relative phase velocity error: $\\varepsilon(k h; r) = \\frac{c_{p}^{\\text{num}}(k)}{c} - 1$.\n    - Courant number: $r = c \\Delta t / h$.\n    - Nondimensional wavenumber: $\\xi = k h$.\n- **Constraints:**\n    - Principal branch for angular frequency: $0 \\le \\omega \\Delta t \\le \\pi$.\n    - Nondimensional wavenumber range: $0 \\le k h \\le \\pi$.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientific Grounding:** The problem is firmly grounded in the field of computational acoustics and the numerical analysis of partial differential equations. The 1D wave equation, its finite difference discretization, and the von Neumann analysis (using a plane-wave ansatz) to derive the numerical dispersion relation are standard, well-established methods.\n- **Well-Posed:** The problem provides all necessary equations, definitions, and constraints to derive a unique analytical expression for the relative phase velocity error. The goal is clearly stated.\n- **Objective:** The problem is formulated in precise, objective mathematical language, free from any subjectivity or ambiguity.\n- **Completeness and Consistency:** The problem is self-contained. All variables ($r$, $kh$) required for the final expression are defined. The constraints on $\\omega \\Delta t$ and $kh$ are consistent and serve to select the principal branch of the solution, ensuring a unique answer.\n- **Other Flaws:** The problem does not violate any of the criteria for invalidity. It is a standard, formalizable, and non-trivial academic exercise in numerical methods.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. A complete solution will be provided.\n\n### Derivation of the Solution\n\nThe objective is to find an expression for the relative phase velocity error $\\varepsilon(k h; r)$. This requires first deriving the numerical dispersion relation, which links the numerical frequency $\\omega$ to the wavenumber $k$.\n\nWe begin by substituting the plane-wave ansatz, $u_{j}^{n} = A \\exp(\\mathrm{i}(k j h - \\omega n \\Delta t))$, into the discretized wave equation. To do this, we express the terms $u_{j}^{n+1}$, $u_{j}^{n-1}$, $u_{j+1}^{n}$, and $u_{j-1}^{n}$ in terms of $u_{j}^{n}$:\n$$\nu_{j}^{n+1} = A \\exp(\\mathrm{i}(k j h - \\omega (n+1) \\Delta t)) = A \\exp(\\mathrm{i}(k j h - \\omega n \\Delta t)) \\exp(-\\mathrm{i} \\omega \\Delta t) = u_{j}^{n} \\exp(-\\mathrm{i} \\omega \\Delta t)\n$$\n$$\nu_{j}^{n-1} = A \\exp(\\mathrm{i}(k j h - \\omega (n-1) \\Delta t)) = u_{j}^{n} \\exp(\\mathrm{i} \\omega \\Delta t)\n$$\n$$\nu_{j+1}^{n} = A \\exp(\\mathrm{i}(k (j+1) h - \\omega n \\Delta t)) = u_{j}^{n} \\exp(\\mathrm{i} k h)\n$$\n$$\nu_{j-1}^{n} = A \\exp(\\mathrm{i}(k (j-1) h - \\omega n \\Delta t)) = u_{j}^{n} \\exp(-\\mathrm{i} k h)\n$$\n\nSubstituting these expressions into the finite difference equation yields:\n$$\n\\frac{u_{j}^{n} \\exp(-\\mathrm{i} \\omega \\Delta t) - 2 u_{j}^{n} + u_{j}^{n} \\exp(\\mathrm{i} \\omega \\Delta t)}{\\Delta t^{2}} = c^{2} \\frac{u_{j}^{n} \\exp(\\mathrm{i} k h) - 2 u_{j}^{n} + u_{j}^{n} \\exp(-\\mathrm{i} k h)}{h^{2}}\n$$\nAssuming a non-trivial solution ($A \\neq 0$, so $u_{j}^{n} \\neq 0$), we can divide the entire equation by $u_{j}^{n}$:\n$$\n\\frac{\\exp(-\\mathrm{i} \\omega \\Delta t) - 2 + \\exp(\\mathrm{i} \\omega \\Delta t)}{\\Delta t^{2}} = c^{2} \\frac{\\exp(\\mathrm{i} k h) - 2 + \\exp(-\\mathrm{i} k h)}{h^{2}}\n$$\nUsing Euler's formula, $\\exp(\\mathrm{i}\\theta) + \\exp(-\\mathrm{i}\\theta) = 2 \\cos(\\theta)$, we can simplify both sides of the equation:\n$$\n\\frac{2 \\cos(\\omega \\Delta t) - 2}{\\Delta t^{2}} = c^{2} \\frac{2 \\cos(k h) - 2}{h^{2}}\n$$\nNext, we apply the half-angle trigonometric identity, $\\cos(\\theta) - 1 = -2 \\sin^{2}(\\theta/2)$:\n$$\n\\frac{-4 \\sin^{2}(\\omega \\Delta t / 2)}{\\Delta t^{2}} = c^{2} \\frac{-4 \\sin^{2}(k h / 2)}{h^{2}}\n$$\nSimplifying by dividing both sides by $-4$:\n$$\n\\frac{\\sin^{2}(\\omega \\Delta t / 2)}{\\Delta t^{2}} = c^{2} \\frac{\\sin^{2}(k h / 2)}{h^{2}}\n$$\nRearranging the terms gives:\n$$\n\\sin^{2}\\left(\\frac{\\omega \\Delta t}{2}\\right) = \\left(\\frac{c \\Delta t}{h}\\right)^{2} \\sin^{2}\\left(\\frac{k h}{2}\\right)\n$$\nBy definition, the Courant number is $r = c \\Delta t / h$. Substituting this into the equation:\n$$\n\\sin^{2}\\left(\\frac{\\omega \\Delta t}{2}\\right) = r^{2} \\sin^{2}\\left(\\frac{k h}{2}\\right)\n$$\nWe take the square root of both sides. Based on the problem constraints, $0 \\le \\omega \\Delta t \\le \\pi$ and $0 \\le k h \\le \\pi$. This implies that $0 \\le \\omega \\Delta t / 2 \\le \\pi/2$ and $0 \\le k h / 2 \\le \\pi/2$. In the interval $[0, \\pi/2]$, the sine function is non-negative. Therefore, we can take the positive square root without ambiguity:\n$$\n\\sin\\left(\\frac{\\omega \\Delta t}{2}\\right) = r \\sin\\left(\\frac{k h}{2}\\right)\n$$\nThis equation is the numerical dispersion relation in its implicit form. To find $\\omega$, we solve for it:\n$$\n\\frac{\\omega \\Delta t}{2} = \\arcsin\\left(r \\sin\\left(\\frac{k h}{2}\\right)\\right)\n$$\n$$\n\\omega = \\frac{2}{\\Delta t} \\arcsin\\left(r \\sin\\left(\\frac{k h}{2}\\right)\\right)\n$$\nNow, we can find the numerical phase velocity, $c_{p}^{\\text{num}} = \\omega / k$:\n$$\nc_{p}^{\\text{num}}(k) = \\frac{1}{k} \\left[ \\frac{2}{\\Delta t} \\arcsin\\left(r \\sin\\left(\\frac{k h}{2}\\right)\\right) \\right] = \\frac{2}{k \\Delta t} \\arcsin\\left(r \\sin\\left(\\frac{k h}{2}\\right)\\right)\n$$\nThe final step is to calculate the relative phase velocity error, $\\varepsilon(k h; r) = c_{p}^{\\text{num}}(k)/c - 1$:\n$$\n\\varepsilon(k h; r) = \\frac{1}{c} \\left[ \\frac{2}{k \\Delta t} \\arcsin\\left(r \\sin\\left(\\frac{k h}{2}\\right)\\right) \\right] - 1\n$$\nTo express this in terms of $r$ and $kh$, we use the definition of the Courant number, $r = c \\Delta t / h$, which can be rearranged to $c = r h / \\Delta t$. Substituting this for $c$:\n$$\n\\varepsilon(k h; r) = \\frac{\\Delta t}{r h} \\left[ \\frac{2}{k \\Delta t} \\arcsin\\left(r \\sin\\left(\\frac{k h}{2}\\right)\\right) \\right] - 1\n$$\nThe $\\Delta t$ terms cancel:\n$$\n\\varepsilon(k h; r) = \\frac{2}{r k h} \\arcsin\\left(r \\sin\\left(\\frac{k h}{2}\\right)\\right) - 1\n$$\nThis is the final closed-form expression for the relative phase velocity error in terms of the nondimensional wavenumber $kh$ and the Courant number $r$.",
            "answer": "$$\n\\boxed{\\frac{2}{r k h} \\arcsin\\left(r \\sin\\left(\\frac{k h}{2}\\right)\\right) - 1}\n$$"
        },
        {
            "introduction": "Acoustic fields are fundamentally described by complex-valued quantities that encode both amplitude and phase information. When training a surrogate, a naive loss function like mean squared error on the real and imaginary components may not capture physically important discrepancies. A more robust approach is to design a loss function that explicitly and independently penalizes errors in amplitude and phase. This practice  challenges you to implement such a physics-aware loss function, a crucial skill for adapting machine learning techniques to the unique structure of acoustics problems.",
            "id": "4128310",
            "problem": "You are training a machine learning surrogate to predict frequency-domain complex acoustic pressure at a fixed angular frequency, modeled as a complex phasor. Let the true complex pressure at a spatial location be denoted by $p \\in \\mathbb{C}$ with Cartesian components $p = x + \\mathrm{i} y$, amplitude $A = \\sqrt{x^2 + y^2}$ in pascals (Pa), and phase $\\phi = \\mathrm{atan2}(y, x)$ in radians. Let the surrogate prediction be $\\hat{p} = \\hat{x} + \\mathrm{i} \\hat{y}$ with amplitude $\\hat{A}$ and phase $\\hat{\\phi}$. You want a loss function for training that balances amplitude and phase errors in a physically meaningful way and has units of squared pascals. The surrogate is trained over $N$ samples indexed by $k \\in \\{1,\\dots,N\\}$.\n\nFrom first principles, consider the amplitude and phase as functions of the real and imaginary parts, and recall that a small phase error of size $\\Delta \\phi$ at amplitude $A$ produces an orthogonal perturbation of magnitude approximately $A \\Delta \\phi$ in the complex plane. Based on this, construct a scalar per-sample loss that is the sum of a weighted squared amplitude discrepancy and a weighted squared phase discrepancy scaled by squared amplitude:\n$$\n\\ell(p, \\hat{p}; w_A, w_\\phi) \\equiv w_A \\left(\\hat{A} - A\\right)^2 + w_\\phi \\, A^2 \\, \\Delta \\phi^2,\n$$\nwhere $w_A \\gt 0$ and $w_\\phi \\gt 0$ are user-specified weights, and $\\Delta \\phi$ is the phase difference wrapped to the principal value interval $(-\\pi, \\pi]$ in radians. The dataset-averaged loss is\n$$\n\\mathcal{L} \\equiv \\frac{1}{N} \\sum_{k=1}^{N} \\ell\\!\\left(p_k, \\hat{p}_k; w_A, w_\\phi\\right).\n$$\nThis $\\mathcal{L}$ must be reported in squared pascals (Pa$^2$), with angles computed in radians.\n\nUsing the change of variables $(x, y) \\mapsto (A, \\phi)$ with $A = \\sqrt{x^2 + y^2}$ and $\\phi = \\mathrm{atan2}(y, x)$, the Jacobian at $p$ is\n$$\nJ(p) \\equiv\n\\begin{bmatrix}\n\\frac{\\partial A}{\\partial x}  \\frac{\\partial A}{\\partial y} \\\\\nA \\frac{\\partial \\phi}{\\partial x}  A \\frac{\\partial \\phi}{\\partial y}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\frac{x}{A}  \\frac{y}{A} \\\\\n-\\frac{y}{A}  \\frac{x}{A}\n\\end{bmatrix},\n$$\nfor $A \\gt 0$. For small errors $\\Delta x = \\hat{x} - x$ and $\\Delta y = \\hat{y} - y$, the linearization gives $[\\Delta A, \\; A \\Delta \\phi]^\\top \\approx J(p) [\\Delta x, \\; \\Delta y]^\\top$. Therefore, the quadratic approximation to $\\ell$ can be written as a weighted form in the real and imaginary parts,\n$$\n\\ell(p, \\hat{p}; w_A, w_\\phi) \\approx\n\\begin{bmatrix} \\Delta x  \\Delta y \\end{bmatrix}\n\\left(J(p)^\\top \\, \\mathrm{diag}(w_A, w_\\phi) \\, J(p)\\right)\n\\begin{bmatrix} \\Delta x \\\\ \\Delta y \\end{bmatrix},\n$$\nwhich defines an amplitude- and phase-aware weighting of the real and imaginary residuals. However, when $A = 0$, the phase is undefined and the phase term should not contribute; in that case the exact loss reduces to $w_A \\hat{A}^2$.\n\nYour task is to implement the exact amplitude-phase balanced loss $\\mathcal{L}$, using the wrapped phase difference in radians and reporting the loss in Pa$^2$. You should also handle the boundary case $A = 0$ robustly via the exact expression above, without relying on the Jacobian.\n\nImplement a program that, for the test suite below, computes the scalar loss $\\mathcal{L}$ for each case and prints them in one single line as a comma-separated list enclosed in square brackets.\n\nAngle unit requirement: all angles must be in radians. Physical unit requirement: the resulting loss must be reported in squared pascals (Pa$^2$). Numerical output: print floating-point values.\n\nTest suite:\n- Case $1$ (amplitude error only): $N = 1$, $p_1 = 1 + \\mathrm{i}\\,0$ Pa, $\\hat{p}_1 = 1.1 + \\mathrm{i}\\,0$ Pa, $w_A = 1$, $w_\\phi = 1$.\n- Case $2$ (phase error only, small angle): $N = 1$, $p_1 = 1 + \\mathrm{i}\\,0$ Pa, $\\hat{p}_1 = \\exp(\\mathrm{i}\\,0.1)$ Pa, $w_A = 1$, $w_\\phi = 1$.\n- Case $3$ (near-zero amplitude, undefined phase): $N = 1$, $p_1 = 0 + \\mathrm{i}\\,0$ Pa, $\\hat{p}_1 = 0.001 + \\mathrm{i}\\,0$ Pa, $w_A = 2$, $w_\\phi = 3$.\n- Case $4$ (mixed amplitude and phase): $N = 1$, $p_1 = 2 \\exp(\\mathrm{i}\\,\\pi/4)$ Pa, $\\hat{p}_1 = 1.8 \\exp(\\mathrm{i}\\,(\\pi/4 + 0.2))$ Pa, $w_A = 0.5$, $w_\\phi = 2$.\n- Case $5$ (batch averaging): $N = 2$, $p_1 = 1 \\exp(\\mathrm{i}\\,0)$ Pa, $\\hat{p}_1 = 1.05 \\exp(\\mathrm{i}\\,0)$ Pa; $p_2 = 3 \\exp(\\mathrm{i}\\,\\pi/3)$ Pa, $\\hat{p}_2 = 3 \\exp(\\mathrm{i}\\,(\\pi/3 + 0.2))$ Pa; $w_A = 1.5$, $w_\\phi = 0.5$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"). Each result corresponds to the scalar loss $\\mathcal{L}$ for the corresponding case, in the same order as the test suite.",
            "solution": "The user has provided a valid problem statement. The task is to compute an amplitude-phase balanced loss function for a set of test cases involving complex acoustic pressure predictions.\n\nThe problem defines the true complex pressure as $p = x + \\mathrm{i} y$ and the surrogate's prediction as $\\hat{p} = \\hat{x} + \\mathrm{i} \\hat{y}$. In polar coordinates, these are represented by amplitude and phase pairs $(A, \\phi)$ and $(\\hat{A}, \\hat{\\phi})$ respectively, where $A = |p|$, $\\phi = \\mathrm{arg}(p)$, $\\hat{A} = |\\hat{p}|$, and $\\hat{\\phi} = \\mathrm{arg}(\\hat{p})$. All angles are in radians.\n\nThe per-sample loss function, $\\ell$, is given by:\n$$ \\ell(p, \\hat{p}; w_A, w_\\phi) = w_A (\\hat{A} - A)^2 + w_\\phi A^2 (\\Delta \\phi)^2 $$\nwhere $w_A  0$ and $w_\\phi  0$ are dimensionless weights. The term $\\Delta \\phi$ represents the phase difference $\\hat{\\phi} - \\phi$, wrapped to the principal interval $(-\\pi, \\pi]$. The problem specifies that for the case $A=0$, where the phase $\\phi$ is undefined, the loss reduces to $\\ell = w_A \\hat{A}^2$. The provided formula is consistent with this requirement, as the term $w_\\phi A^2 (\\Delta \\phi)^2$ becomes zero when $A=0$, irrespective of the value of $\\Delta \\phi$.\n\nThe dataset-averaged loss, $\\mathcal{L}$, over $N$ samples is:\n$$ \\mathcal{L} = \\frac{1}{N} \\sum_{k=1}^{N} \\ell(p_k, \\hat{p}_k; w_A, w_\\phi) $$\nThe final result $\\mathcal{L}$ must be in units of squared pascals (Pa$^2$).\n\nWe will now compute $\\mathcal{L}$ for each test case provided. The calculation involves converting the given complex pressures into their polar components (amplitude and phase), calculating the wrapped phase difference, and applying the loss formula.\n\n**Case 1:** Amplitude error only.\n- Givens: $N=1$, $p_1 = 1 + \\mathrm{i}\\,0$ Pa, $\\hat{p}_1 = 1.1 + \\mathrm{i}\\,0$ Pa, $w_A = 1$, $w_\\phi = 1$.\n- True value: $A_1 = |1 + \\mathrm{i}\\,0| = 1$ Pa, $\\phi_1 = \\mathrm{arg}(1 + \\mathrm{i}\\,0) = 0$ rad.\n- Prediction: $\\hat{A}_1 = |1.1 + \\mathrm{i}\\,0| = 1.1$ Pa, $\\hat{\\phi}_1 = \\mathrm{arg}(1.1 + \\mathrm{i}\\,0) = 0$ rad.\n- Discrepancies: $\\hat{A}_1 - A_1 = 1.1 - 1 = 0.1$ Pa. $\\Delta\\phi_1 = \\hat{\\phi}_1 - \\phi_1 = 0 - 0 = 0$ rad.\n- Per-sample loss: $\\ell_1 = w_A (\\hat{A}_1 - A_1)^2 + w_\\phi A_1^2 (\\Delta\\phi_1)^2 = 1 \\cdot (0.1)^2 + 1 \\cdot (1)^2 \\cdot (0)^2 = 0.01$ Pa$^2$.\n- Average loss: $\\mathcal{L} = \\ell_1 / 1 = 0.01$ Pa$^2$.\n\n**Case 2:** Phase error only.\n- Givens: $N=1$, $p_1 = 1 + \\mathrm{i}\\,0$ Pa, $\\hat{p}_1 = \\exp(\\mathrm{i}\\,0.1)$ Pa, $w_A = 1$, $w_\\phi = 1$.\n- True value: $A_1 = 1$ Pa, $\\phi_1 = 0$ rad.\n- Prediction: $\\hat{A}_1 = |\\exp(\\mathrm{i}\\,0.1)| = 1$ Pa, $\\hat{\\phi}_1 = \\mathrm{arg}(\\exp(\\mathrm{i}\\,0.1)) = 0.1$ rad.\n- Discrepancies: $\\hat{A}_1 - A_1 = 1 - 1 = 0$ Pa. $\\Delta\\phi_1 = 0.1 - 0 = 0.1$ rad. This value is within $(-\\pi, \\pi]$.\n- Per-sample loss: $\\ell_1 = w_A (\\hat{A}_1 - A_1)^2 + w_\\phi A_1^2 (\\Delta\\phi_1)^2 = 1 \\cdot (0)^2 + 1 \\cdot (1)^2 \\cdot (0.1)^2 = 0.01$ Pa$^2$.\n- Average loss: $\\mathcal{L} = \\ell_1 / 1 = 0.01$ Pa$^2$.\n\n**Case 3:** Near-zero amplitude.\n- Givens: $N=1$, $p_1 = 0 + \\mathrm{i}\\,0$ Pa, $\\hat{p}_1 = 0.001 + \\mathrm{i}\\,0$ Pa, $w_A = 2$, $w_\\phi = 3$.\n- True value: $A_1 = |0 + \\mathrm{i}\\,0| = 0$ Pa. The phase $\\phi_1$ is undefined.\n- Prediction: $\\hat{A}_1 = |0.001| = 0.001$ Pa.\n- Per-sample loss: The formula simplifies as the second term becomes zero. $\\ell_1 = w_A (\\hat{A}_1 - A_1)^2 + w_\\phi A_1^2 (\\Delta\\phi_1)^2 = 2 \\cdot (0.001 - 0)^2 + 3 \\cdot (0)^2 \\cdot (\\Delta\\phi_1)^2 = 2 \\cdot (0.001)^2 = 2 \\times 10^{-6}$ Pa$^2$.\n- Average loss: $\\mathcal{L} = \\ell_1 / 1 = 2 \\times 10^{-6}$ Pa$^2$.\n\n**Case 4:** Mixed amplitude and phase error.\n- Givens: $N=1$, $p_1 = 2 \\exp(\\mathrm{i}\\,\\pi/4)$ Pa, $\\hat{p}_1 = 1.8 \\exp(\\mathrm{i}\\,(\\pi/4 + 0.2))$ Pa, $w_A = 0.5$, $w_\\phi = 2$.\n- True value: $A_1 = 2$ Pa, $\\phi_1 = \\pi/4$ rad.\n- Prediction: $\\hat{A}_1 = 1.8$ Pa, $\\hat{\\phi}_1 = \\pi/4 + 0.2$ rad.\n- Discrepancies: $\\hat{A}_1 - A_1 = 1.8 - 2 = -0.2$ Pa. $\\Delta\\phi_1 = (\\pi/4 + 0.2) - (\\pi/4) = 0.2$ rad. This value is within $(-\\pi, \\pi]$.\n- Per-sample loss: $\\ell_1 = w_A (\\hat{A}_1 - A_1)^2 + w_\\phi A_1^2 (\\Delta\\phi_1)^2 = 0.5 \\cdot (-0.2)^2 + 2 \\cdot (2)^2 \\cdot (0.2)^2 = 0.5 \\cdot 0.04 + 2 \\cdot 4 \\cdot 0.04 = 0.02 + 0.32 = 0.34$ Pa$^2$.\n- Average loss: $\\mathcal{L} = \\ell_1 / 1 = 0.34$ Pa$^2$.\n\n**Case 5:** Batch averaging.\n- Givens: $N=2$, $w_A = 1.5$, $w_\\phi = 0.5$.\n- Sample 1: $p_1 = 1 \\exp(\\mathrm{i}\\,0)$ Pa, $\\hat{p}_1 = 1.05 \\exp(\\mathrm{i}\\,0)$ Pa.\n    - $A_1 = 1$ Pa, $\\phi_1 = 0$ rad.\n    - $\\hat{A}_1 = 1.05$ Pa, $\\hat{\\phi}_1 = 0$ rad.\n    - $\\Delta A_1 = 0.05$ Pa, $\\Delta \\phi_1 = 0$ rad.\n    - $\\ell_1 = 1.5 \\cdot (0.05)^2 + 0.5 \\cdot (1)^2 \\cdot (0)^2 = 1.5 \\cdot 0.0025 = 0.00375$ Pa$^2$.\n- Sample 2: $p_2 = 3 \\exp(\\mathrm{i}\\,\\pi/3)$ Pa, $\\hat{p}_2 = 3 \\exp(\\mathrm{i}\\,(\\pi/3 + 0.2))$ Pa.\n    - $A_2 = 3$ Pa, $\\phi_2 = \\pi/3$ rad.\n    - $\\hat{A}_2 = 3$ Pa, $\\hat{\\phi}_2 = \\pi/3 + 0.2$ rad.\n    - $\\Delta A_2 = 0$ Pa, $\\Delta \\phi_2 = 0.2$ rad.\n    - $\\ell_2 = 1.5 \\cdot (0)^2 + 0.5 \\cdot (3)^2 \\cdot (0.2)^2 = 0.5 \\cdot 9 \\cdot 0.04 = 0.18$ Pa$^2$.\n- Average loss: $\\mathcal{L} = \\frac{1}{2}(\\ell_1 + \\ell_2) = \\frac{1}{2}(0.00375 + 0.18) = \\frac{0.18375}{2} = 0.091875$ Pa$^2$.\n\nThe implementation will follow these calculations for each case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the amplitude-phase balanced loss for a suite of test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: amplitude error only\n        {\n            \"samples\": [(1.0 + 0.0j, 1.1 + 0.0j)],\n            \"w_A\": 1.0,\n            \"w_phi\": 1.0,\n        },\n        # Case 2: phase error only, small angle\n        {\n            \"samples\": [(1.0 + 0.0j, np.exp(0.1j))],\n            \"w_A\": 1.0,\n            \"w_phi\": 1.0,\n        },\n        # Case 3: near-zero amplitude, undefined phase\n        {\n            \"samples\": [(0.0 + 0.0j, 0.001 + 0.0j)],\n            \"w_A\": 2.0,\n            \"w_phi\": 3.0,\n        },\n        # Case 4: mixed amplitude and phase\n        {\n            \"samples\": [(2 * np.exp(1j * np.pi / 4), 1.8 * np.exp(1j * (np.pi / 4 + 0.2)))],\n            \"w_A\": 0.5,\n            \"w_phi\": 2.0,\n        },\n        # Case 5: batch averaging\n        {\n            \"samples\": [\n                (1.0 * np.exp(1j * 0), 1.05 * np.exp(1j * 0)),\n                (3.0 * np.exp(1j * np.pi / 3), 3.0 * np.exp(1j * (np.pi / 3 + 0.2)))\n            ],\n            \"w_A\": 1.5,\n            \"w_phi\": 0.5,\n        },\n    ]\n\n    def compute_loss(samples, w_A, w_phi):\n        \"\"\"\n        Calculates the average loss L for a given batch of samples.\n        \"\"\"\n        N = len(samples)\n        if N == 0:\n            return 0.0\n        \n        total_sample_loss = 0.0\n        \n        for p_true, p_hat in samples:\n            # Calculate amplitude and phase for true and predicted values\n            A_true = np.abs(p_true)\n            phi_true = np.angle(p_true)\n            \n            A_hat = np.abs(p_hat)\n            phi_hat = np.angle(p_hat)\n            \n            # Calculate amplitude difference\n            amp_diff = A_hat - A_true\n            \n            # Calculate raw phase difference\n            phase_diff_raw = phi_hat - phi_true\n            \n            # Wrap phase difference to the interval (-pi, pi]\n            phase_diff_wrapped = np.arctan2(np.sin(phase_diff_raw), np.cos(phase_diff_raw))\n            \n            # Calculate per-sample loss l(p, p_hat)\n            # The A_true**2 term naturally handles the case where A_true is 0.\n            sample_loss = w_A * (amp_diff**2) + w_phi * (A_true**2) * (phase_diff_wrapped**2)\n            \n            total_sample_loss += sample_loss\n            \n        # Return the dataset-averaged loss L\n        return total_sample_loss / N\n\n    results = []\n    for case in test_cases:\n        loss = compute_loss(case[\"samples\"], case[\"w_A\"], case[\"w_phi\"])\n        results.append(loss)\n\n    # Format the final output string as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In practice, generating high-quality training data can be the most expensive part of building a surrogate, especially for high-frequency acoustic simulations which are prone to numerical errors. A pragmatic solution is to use a \"multi-fidelity\" approach: generate most data with a fast, low-fidelity solver and strategically \"repair\" the most error-prone samples with a slow, high-fidelity solver. This capstone exercise  guides you through a complete workflow to implement this data curation strategy, demonstrating how to build an accurate surrogate model while balancing computational cost and data quality.",
            "id": "4128285",
            "problem": "You are tasked with constructing and evaluating a machine learning surrogate that predicts an acoustic observable for a one-dimensional cavity governed by the Helmholtz equation, while mitigating high-frequency numerical pollution in the training data by selectively applying high-order and refined mesh finite elements (hp-FEM). The domain is the interval $[0,L]$ with homogeneous Dirichlet boundary conditions for the acoustic pressure field $p(x)$, and the time-harmonic source yields the steady-state Helmholtz model. The fundamental base of the problem is the one-dimensional Helmholtz equation for time-harmonic acoustics: $$\\frac{d^2 p}{dx^2} + k^2 p = f(x),$$ with boundary conditions $$p(0) = 0,\\quad p(L) = 0,$$ where $k$ is the acoustic wavenumber expressed in $\\mathrm{rad/m}$, $L$ is the cavity length expressed in $\\mathrm{m}$, and $f(x)$ is the source distribution. Assume a unit-amplitude modal source $$f(x) = \\sin\\left(\\frac{\\pi x}{L}\\right),$$ and consider the observable defined as the pressure amplitude at the domain midpoint $x_0 = L/2$, which can be taken from the exact modal response of the system. Under these assumptions and consistent with the linearity and orthogonality of modal expansions for the Dirichlet problem, the observable is the exact transfer function value $$A_{\\mathrm{true}}(k,L) = \\frac{\\sin\\left(\\frac{\\pi x_0}{L}\\right)}{k^2 - \\left(\\frac{\\pi}{L}\\right)^2},$$ evaluated at $x_0 = L/2$, noting that $\\sin\\left(\\frac{\\pi}{2}\\right)=1$.\n\nYou must construct a synthetic label generator that represents numerical solutions obtained from low-order finite element method (FEM) on a uniform mesh, which suffer high-frequency dispersion error and thus pollute the training data. Use the well-tested characterization that dispersion error in FEM for the Helmholtz equation grows with the non-dimensional parameter $k h$, where $h$ is the element size, and decreases with the polynomial order $p$ of the FEM basis. Specifically, let the coarse solver label be $$A_{\\mathrm{coarse}}(k,L) = A_{\\mathrm{true}}(k,L) + \\Delta_{\\mathrm{coarse}}(k,h,p),$$ where $$h = \\frac{L}{N_e},$$ $N_e$ is the number of elements, and $\\Delta_{\\mathrm{coarse}}$ is a dispersion-induced bias defined by the deterministic function $$\\Delta_{\\mathrm{coarse}}(k,h,p) = \\gamma_{\\mathrm{c}} \\frac{(k h)^{2p+1}}{1 + (k h)^{2p+1}},$$ which is a smooth increasing function of $k h$ consistent with the asymptotic growth rate $O\\!\\left((k h)^{2p+1}\\right)$. To mitigate this pollution, for a selected fraction of training samples, replace the coarse labels with higher-fidelity hp-FEM labels: $$A_{\\mathrm{hp}}(k,L) = A_{\\mathrm{true}}(k,L) + \\Delta_{\\mathrm{hp}}(k,h_{\\mathrm{hp}},p_{\\mathrm{hp}}),$$ where $$h_{\\mathrm{hp}} = \\frac{L}{N_e^{\\mathrm{hp}}},\\quad \\Delta_{\\mathrm{hp}}(k,h_{\\mathrm{hp}},p_{\\mathrm{hp}}) = \\gamma_{\\mathrm{hp}} \\frac{(k h_{\\mathrm{hp}})^{2p_{\\mathrm{hp}}+1}}{1 + (k h_{\\mathrm{hp}})^{2p_{\\mathrm{hp}}+1}},$$ with $p_{\\mathrm{hp}}  p$, $N_e^{\\mathrm{hp}}  N_e$, and $\\gamma_{\\mathrm{hp}}  \\gamma_{\\mathrm{c}}$.\n\nYou must create a training dataset consisting of input parameter pairs $(k,L)$ and labels produced according to the selection policy described below. Then, fit a kernel ridge regression surrogate with a Gaussian kernel to map $(k,L)$ to the observable. Evaluate its generalization performance on an unseen test set by computing the mean absolute error (MAE) relative to $A_{\\mathrm{true}}(k,L)$.\n\nImplement the following requirements:\n- Use $N_{\\mathrm{train}} = 200$ training samples with $k$ uniformly sampled in $[5,150]$ (in $\\mathrm{rad/m}$) and $L$ uniformly sampled in $[0.8,1.2]$ (in $\\mathrm{m}$).\n- Use $N_{\\mathrm{test}} = 200$ test samples sampled independently as above.\n- For the coarse solver, use $N_e = 20$, $p = 1$, and $\\gamma_{\\mathrm{c}} = 0.08$.\n- For the hp-FEM solver, use $N_e^{\\mathrm{hp}} = 80$, $p_{\\mathrm{hp}} = 4$, and $\\gamma_{\\mathrm{hp}} = 0.005$.\n- Fit kernel ridge regression with a Gaussian kernel $$K(\\mathbf{x},\\mathbf{z}) = \\exp\\left(-\\frac{\\|\\mathbf{x}-\\mathbf{z}\\|^2}{2\\sigma^2}\\right),$$ regularization parameter $\\lambda$, and kernel width $\\sigma$, where $\\mathbf{x} = [k,L]$. Use $\\sigma = 25$ and $\\lambda = 10^{-6}$. Standardize each feature $k$ and $L$ to zero mean and unit variance using statistics from the training set for both training and testing.\n- Selection policy for hp-FEM labels: Given a refinement fraction $r \\in [0,1]$, select $\\lfloor r N_{\\mathrm{train}}\\rfloor$ training samples according to one of two strategies:\n    - \"high-k\": select the samples with the largest values of $k$.\n    - \"random\": select samples uniformly at random. Use a fixed pseudorandom generator with seed $123$ for reproducibility.\n- For all non-selected samples, use the coarse solver labels $A_{\\mathrm{coarse}}(k,L)$; for selected samples, use the hp-FEM labels $A_{\\mathrm{hp}}(k,L)$.\n- Compute the mean absolute error $$\\mathrm{MAE} = \\frac{1}{N_{\\mathrm{test}}} \\sum_{i=1}^{N_{\\mathrm{test}}} \\left| \\widehat{A}(k_i,L_i) - A_{\\mathrm{true}}(k_i,L_i) \\right|,$$ where $\\widehat{A}$ is the surrogate prediction.\n\nDesign a test suite of five parameter cases to quantify the impact on surrogate generalization:\n1. Case $1$: $r = 0.0$, strategy \"none\" (i.e., no hp-FEM is used; all labels are coarse).\n2. Case $2$: $r = 0.1$, strategy \"high-k\".\n3. Case $3$: $r = 0.5$, strategy \"high-k\".\n4. Case $4$: $r = 0.1$, strategy \"random\".\n5. Case $5$: $r = 1.0$, strategy \"high-k\" (i.e., all labels use hp-FEM).\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Cases $1$ through $5$, where each entry is the MAE for the corresponding case, expressed as a floating-point number. The final outputs are dimensionless and must be printed without units. For example, the format must be exactly like $[x_1,x_2,x_3,x_4,x_5]$ where $x_i$ are the computed MAE values for Cases $i=1,\\dots,5$.",
            "solution": "The problem requires the construction and evaluation of a machine learning surrogate model for an acoustic observable derived from the one-dimensional Helmholtz equation. The core of the task is to investigate how selectively refining the quality of training data, a process known as data curation, impacts the surrogate's generalization performance. Specifically, we will use a Kernel Ridge Regression (KRR) model trained on a dataset of mixed fidelity, where some labels are generated by a low-order numerical solver and others by a high-order solver, and evaluate its accuracy against the exact analytical solution.\n\nThe physical system is a one-dimensional acoustic cavity of length $L$ governed by the steady-state Helmholtz equation for the acoustic pressure field $p(x)$:\n$$\n\\frac{d^2 p}{dx^2} + k^2 p = f(x)\n$$\nwhere $k$ is the acoustic wavenumber and $f(x)$ is the time-harmonic source distribution. The domain is $x \\in [0, L]$, with homogeneous Dirichlet boundary conditions $p(0) = 0$ and $p(L) = 0$. The source is specified as a unit-amplitude modal source corresponding to the first eigenmode of the cavity, $f(x) = \\sin\\left(\\frac{\\pi x}{L}\\right)$.\n\nFor this specific source and boundary conditions, the problem has an exact analytical solution. The solution for the pressure field is $p(x) = \\frac{\\sin(\\pi x/L)}{k^2 - (\\pi/L)^2}$, which is valid as long as the wavenumber $k$ does not coincide with the resonant wavenumber $k_1 = \\pi/L$. The observable of interest is the pressure amplitude at the midpoint of the domain, $x_0 = L/2$. The exact value of this observable, which serves as our ground truth, is:\n$$\nA_{\\mathrm{true}}(k,L) = \\frac{\\sin\\left(\\frac{\\pi (L/2)}{L}\\right)}{k^2 - \\left(\\frac{\\pi}{L}\\right)^2} = \\frac{1}{k^2 - \\left(\\frac{\\pi}{L}\\right)^2}\n$$\n\nThe training data for the surrogate model are generated synthetically to mimic numerical solutions from a Finite Element Method (FEM) solver. A key aspect of FEM for the Helmholtz equation is the phenomenon of numerical pollution or dispersion error, which becomes more severe at higher wavenumbers (high frequency). This error's magnitude is known to depend on the non-dimensional parameter $kh$ (wavenumber times element size) and the polynomial order $p$ of the basis functions. We model this behavior with two synthetic solvers.\n\nThe \"coarse\" solver produces labels with significant error:\n$$\nA_{\\mathrm{coarse}}(k,L) = A_{\\mathrm{true}}(k,L) + \\Delta_{\\mathrm{coarse}}(k,h,p)\n$$\nwhere the dispersion-induced bias $\\Delta_{\\mathrm{coarse}}$ is modeled as a function of $k$, element size $h = L/N_e$, and polynomial order $p$. We use the specific form:\n$$\n\\Delta_{\\mathrm{coarse}}(k,h,p) = \\gamma_{\\mathrm{c}} \\frac{(k h)^{2p+1}}{1 + (k h)^{2p+1}}\n$$\nwith parameters $N_e = 20$, $p = 1$, and $\\gamma_{\\mathrm{c}} = 0.08$.\n\nThe \"high-fidelity\" or hp-FEM solver produces more accurate labels with a much smaller error term:\n$$\nA_{\\mathrm{hp}}(k,L) = A_{\\mathrm{true}}(k,L) + \\Delta_{\\mathrm{hp}}(k,h_{\\mathrm{hp}},p_{\\mathrm{hp}})\n$$\nwhere the bias $\\Delta_{\\mathrm{hp}}$ is given by:\n$$\n\\Delta_{\\mathrm{hp}}(k,h_{\\mathrm{hp}},p_{\\mathrm{hp}}) = \\gamma_{\\mathrm{hp}} \\frac{(k h_{\\mathrm{hp}})^{2p_{\\mathrm{hp}}+1}}{1 + (k h_{\\mathrm{hp}})^{2p_{\\mathrm{hp}}+1}}\n$$\nThis solver uses a refined mesh ($h_{\\mathrm{hp}} = L/N_e^{\\mathrm{hp}}$) and higher polynomial order, with parameters $N_e^{\\mathrm{hp}} = 80$, $p_{\\mathrm{hp}} = 4$, and $\\gamma_{\\mathrm{hp}} = 0.005$.\n\nThe surrogate model is a Kernel Ridge Regression (KRR) model that maps input features $\\mathbf{x} = [k, L]$ to the acoustic observable. Before training, the input features are standardized to have zero mean and unit variance, based on the statistics of the training set. This is crucial for distance-based kernel methods. The KRR prediction for a new input $\\mathbf{x}_*$ is given by:\n$$\n\\widehat{A}(\\mathbf{x}_*) = \\sum_{i=1}^{N_{\\mathrm{train}}} \\alpha_i K(\\mathbf{x}_*, \\mathbf{x}_i) = \\mathbf{k}_*^T \\boldsymbol{\\alpha}\n$$\nwhere $\\mathbf{x}_i$ are the $N_{\\mathrm{train}}=200$ training feature vectors, $K(\\cdot, \\cdot)$ is the kernel function, and $\\mathbf{k}_*$ is a vector with elements $K(\\mathbf{x}_*, \\mathbf{x}_i)$. The weight vector $\\boldsymbol{\\alpha}$ is determined by solving the regularized linear system:\n$$\n(\\mathbf{K} + \\lambda I) \\boldsymbol{\\alpha} = \\mathbf{y}_{\\mathrm{train}}\n$$\nHere, $\\mathbf{K}$ is the $N_{\\mathrm{train}} \\times N_{\\mathrm{train}}$ Gram matrix where $K_{ij} = K(\\mathbf{x}_i, \\mathbf{x}_j)$, $\\lambda = 10^{-6}$ is the regularization parameter, $I$ is the identity matrix, and $\\mathbf{y}_{\\mathrm{train}}$ is the vector of training labels. We use a Gaussian kernel:\n$$\nK(\\mathbf{x}, \\mathbf{z}) = \\exp\\left(-\\frac{\\|\\mathbf{x}-\\mathbf{z}\\|^2}{2\\sigma^2}\\right)\n$$\nwith kernel width $\\sigma = 25$.\n\nThe training labels $\\mathbf{y}_{\\mathrm{train}}$ are generated using a mixed-fidelity approach. Given a refinement fraction $r \\in [0,1]$, a total of $\\lfloor r N_{\\mathrm{train}}\\rfloor$ samples are selected from the training set to have their labels computed by the high-fidelity solver, $A_{\\mathrm{hp}}$. The remaining samples are labeled by the coarse solver, $A_{\\mathrm{coarse}}$. Two strategies for selecting samples for refinement are investigated:\n1.  **\"high-k\"**: Select samples with the largest values of the wavenumber $k$, where the coarse solver's error is expected to be largest.\n2.  **\"random\"**: Select samples uniformly at random, using a fixed seed for reproducibility.\n\nThe generalization performance of the trained surrogate is assessed on an independent test set of $N_{\\mathrm{test}}=200$ samples. The performance is quantified by the Mean Absolute Error (MAE) between the surrogate's predictions $\\widehat{A}$ and the ground truth values from the exact analytical solution $A_{\\mathrm{true}}$:\n$$\n\\mathrm{MAE} = \\frac{1}{N_{\\mathrm{test}}} \\sum_{i=1}^{N_{\\mathrm{test}}} \\left| \\widehat{A}(k_i,L_i) - A_{\\mathrm{true}}(k_i,L_i) \\right|\n$$\n\nThe procedure will be executed for five distinct cases to analyze the impact of the data curation strategy on the surrogate's accuracy:\n1.  Case 1: $r = 0.0$ (all coarse labels). This establishes a baseline performance.\n2.  Case 2: $r = 0.1$ with the \"high-k\" strategy. This tests targeted refinement.\n3.  Case 3: $r = 0.5$ with the \"high-k\" strategy. This tests more extensive targeted refinement.\n4.  Case 4: $r = 0.1$ with the \"random\" strategy. This compares random vs. targeted refinement.\n5.  Case 5: $r = 1.0$ (all hp-FEM labels). This establishes an upper bound on performance with high-quality data.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import solve as sp_solve\n\ndef solve():\n    \"\"\"\n    Constructs and evaluates a machine learning surrogate for an acoustic observable,\n    investigating the impact of training data curation on model performance.\n    \"\"\"\n    # --- Problem Constants ---\n    N_TRAIN = 200\n    N_TEST = 200\n    K_RANGE = (5, 150)\n    L_RANGE = (0.8, 1.2)\n    \n    # Coarse solver parameters\n    NE_COARSE = 20\n    P_COARSE = 1\n    GAMMA_C = 0.08\n    \n    # hp-FEM solver parameters\n    NE_HP = 80\n    P_HP = 4\n    GAMMA_HP = 0.005\n    \n    # KRR hyperparameters\n    SIGMA = 25.0\n    LAMBDA = 1e-6\n    \n    # Random seeds for reproducibility\n    DATA_GENERATION_SEED = 42\n    RANDOM_SELECTION_SEED = 123\n\n    # --- Test Cases ---\n    test_cases = [\n        (0.0, \"none\"),    # Case 1\n        (0.1, \"high-k\"),  # Case 2\n        (0.5, \"high-k\"),  # Case 3\n        (0.1, \"random\"),  # Case 4\n        (1.0, \"high-k\")   # Case 5\n    ]\n\n    # --- Helper Functions ---\n\n    def A_true(k, L):\n        \"\"\"Computes the exact analytical observable.\"\"\"\n        return 1.0 / (k**2 - (np.pi / L)**2)\n\n    def delta_error(k, L, Ne, p, gamma):\n        \"\"\"Computes the phenomenological dispersion error term.\"\"\"\n        h = L / Ne\n        kh = k * h\n        kh_pow = kh**(2 * p + 1)\n        return gamma * (kh_pow / (1.0 + kh_pow))\n\n    def A_coarse(k, L):\n        \"\"\"Computes the coarse solver labels.\"\"\"\n        error = delta_error(k, L, NE_COARSE, P_COARSE, GAMMA_C)\n        return A_true(k, L) + error\n\n    def A_hp(k, L):\n        \"\"\"Computes the high-fidelity hp-FEM solver labels.\"\"\"\n        error = delta_error(k, L, NE_HP, P_HP, GAMMA_HP)\n        return A_true(k, L) + error\n\n    def get_sq_dist_matrix(X, Z):\n        \"\"\"Computes the matrix of squared Euclidean distances between two sets of vectors.\"\"\"\n        X_sq = np.sum(X**2, axis=1, keepdims=True)\n        Z_sq = np.sum(Z**2, axis=1, keepdims=True)\n        return X_sq + Z_sq.T - 2 * (X @ Z.T)\n\n    def gaussian_kernel(X, Z, sigma):\n        \"\"\"Computes the Gaussian kernel matrix.\"\"\"\n        sq_dists = get_sq_dist_matrix(X, Z)\n        return np.exp(-sq_dists / (2 * sigma**2))\n\n    def train_krr(X_train, y_train, sigma, lam):\n        \"\"\"Trains the Kernel Ridge Regression model.\"\"\"\n        K = gaussian_kernel(X_train, X_train, sigma)\n        alpha = sp_solve(K + lam * np.eye(len(K)), y_train, assume_a='pos')\n        return alpha\n\n    def predict_krr(X_test, X_train, alpha, sigma):\n        \"\"\"Makes predictions with a trained KRR model.\"\"\"\n        K_test = gaussian_kernel(X_test, X_train, sigma)\n        return K_test @ alpha\n\n    # --- Main Logic ---\n\n    # 1. Generate Training and Test Data\n    data_rng = np.random.default_rng(seed=DATA_GENERATION_SEED)\n    k_train = data_rng.uniform(K_RANGE[0], K_RANGE[1], N_TRAIN)\n    L_train = data_rng.uniform(L_RANGE[0], L_RANGE[1], N_TRAIN)\n    X_train = np.vstack((k_train, L_train)).T\n\n    k_test = data_rng.uniform(K_RANGE[0], K_RANGE[1], N_TEST)\n    L_test = data_rng.uniform(L_RANGE[0], L_RANGE[1], N_TEST)\n    X_test = np.vstack((k_test, L_test)).T\n\n    # 2. Compute Ground Truth for Test Set\n    y_true_test = A_true(k_test, L_test)\n\n    # 3. Standardize Features\n    mean_train = X_train.mean(axis=0)\n    std_train = X_train.std(axis=0)\n    X_train_std = (X_train - mean_train) / std_train\n    X_test_std = (X_test - mean_train) / std_train\n    \n    # 4. Pre-compute solver labels for the entire training set\n    y_coarse_train = A_coarse(k_train, L_train)\n    y_hp_train = A_hp(k_train, L_train)\n\n    results = []\n    for r, strategy in test_cases:\n        # 5. Generate Mixed-Fidelity Training Labels for the current case\n        y_train = y_coarse_train.copy()\n        \n        num_refined = int(np.floor(r * N_TRAIN))\n        \n        if num_refined  0:\n            if strategy == \"high-k\":\n                indices_to_refine = np.argsort(k_train)[-num_refined:]\n            elif strategy == \"random\":\n                selection_rng = np.random.default_rng(seed=RANDOM_SELECTION_SEED)\n                indices_to_refine = selection_rng.choice(N_TRAIN, size=num_refined, replace=False)\n            \n            y_train[indices_to_refine] = y_hp_train[indices_to_refine]\n        \n        # 6. Train KRR Model\n        alpha = train_krr(X_train_std, y_train, SIGMA, LAMBDA)\n        \n        # 7. Make Predictions on Test Set\n        y_pred = predict_krr(X_test_std, X_train_std, alpha, SIGMA)\n        \n        # 8. Calculate and store MAE\n        mae = np.mean(np.abs(y_pred - y_true_test))\n        results.append(mae)\n\n    # 9. Print Final Output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}