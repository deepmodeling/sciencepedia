{
    "hands_on_practices": [
        {
            "introduction": "To accurately model acoustic domains with curved boundaries, we often use isoparametric finite elements. This technique maps a simple reference element, such as a straight-sided triangle, to a curved element in the physical domain. This hands-on exercise  walks you through the fundamental check for a valid mapping: ensuring the Jacobian determinant $J$ remains positive throughout the element, which guarantees the element does not invert or fold over itself.",
            "id": "4128648",
            "problem": "A two-dimensional acoustic cavity is discretized using the Finite Element Method (FEM) with curved, isoparametric quadratic triangular elements to conform to a smooth boundary. Consider a single $6$-node isoparametric quadratic triangle whose mapping from the reference triangle $(\\xi,\\eta)$ (with barycentric coordinate $\\zeta = 1 - \\xi - \\eta$ and domain $0 \\leq \\xi, \\eta, \\xi + \\eta \\leq 1$) to the physical coordinates $(x,y)$ is defined by quadratic Lagrange interpolation at the following physical nodal positions (all coordinates in $\\mathrm{m}$):\n- Vertex nodes: node $1$ at $(x_{1},y_{1}) = (0,0)$, node $2$ at $(x_{2},y_{2}) = (1,0)$, node $3$ at $(x_{3},y_{3}) = (0,1)$.\n- Mid-side nodes: node $4$ on edge $1$–$2$ at $(x_{4},y_{4}) = (0.5, 0.05)$, node $5$ on edge $2$–$3$ at $(x_{5},y_{5}) = (0.5, 0.55)$, node $6$ on edge $3$–$1$ at $(x_{6},y_{6}) = (-0.05, 0.5)$.\n\nStarting from the definition of isoparametric interpolation and the nodal interpolation conditions for quadratic Lagrange polynomials on the reference triangle, derive the quadratic shape functions $N_{i}(\\xi,\\eta)$, $i=1,\\dots,6$, and use them to construct the isoparametric mapping $(x(\\xi,\\eta), y(\\xi,\\eta)) = \\sum_{i=1}^{6} N_{i}(\\xi,\\eta) (x_{i}, y_{i})$. From first principles, compute the Jacobian determinant\n$$\nJ(\\xi,\\eta) = \\det\\!\\left( \\frac{\\partial(x,y)}{\\partial(\\xi,\\eta)} \\right)\n$$\nacross the entire parametric domain $0 \\leq \\xi, \\eta, \\xi + \\eta \\leq 1$, and determine its minimum value over this domain. Conclude, based on this minimum, whether $J(\\xi,\\eta) > 0$ everywhere in the element, and explain the significance of this result for mesh validity in computational acoustics.\n\nExpress the final minimum value of the Jacobian determinant in $\\mathrm{m^{2}}$ and round your answer to four significant figures.",
            "solution": "The problem asks for the minimum value of the Jacobian determinant for a specific $6$-node isoparametric quadratic triangle and to discuss the significance of the result. The solution proceeds in four stages: (1) defining the quadratic shape functions on the reference element, (2) constructing the isoparametric coordinate mapping, (3) calculating the Jacobian determinant, and (4) finding the minimum of the Jacobian determinant over the element domain.\n\nStep 1: Deriving the Quadratic Shape Functions $N_{i}(\\xi,\\eta)$\nThe element is a $6$-node quadratic triangle. The shape functions $N_i(\\xi, \\eta)$ are defined on a reference triangle in the $(\\xi, \\eta)$ plane with vertices at $(0,0)$, $(1,0)$, and $(0,1)$. It is convenient to use barycentric coordinates $\\zeta = 1 - \\xi - \\eta$, $\\xi$, and $\\eta$. The shape functions for a $6$-node Lagrange triangle are given by:\nFor vertex nodes $i=1,2,3$: $N_i = L_i(2L_i-1)$\nFor mid-side nodes $i=4,5,6$: $N_{jk} = 4L_j L_k$\nThe reference coordinates for the nodes are:\nNode 1: $(\\xi,\\eta)=(0,0)$, where $\\zeta=1$.\nNode 2: $(\\xi,\\eta)=(1,0)$, where $\\xi=1$.\nNode 3: $(\\xi,\\eta)=(0,1)$, where $\\eta=1$.\nNode 4 (midpoint of 1-2): $(\\xi,\\eta)=(0.5,0)$.\nNode 5 (midpoint of 2-3): $(\\xi,\\eta)=(0.5,0.5)$.\nNode 6 (midpoint of 3-1): $(\\xi,\\eta)=(0,0.5)$.\n\nUsing the barycentric coordinates $L_1=\\zeta=1-\\xi-\\eta$, $L_2=\\xi$, and $L_3=\\eta$, the shape functions are:\nVertex nodes:\n$N_1(\\xi,\\eta) = \\zeta(2\\zeta-1) = (1-\\xi-\\eta)(2(1-\\xi-\\eta)-1) = (1-\\xi-\\eta)(1-2\\xi-2\\eta)$\n$N_2(\\xi,\\eta) = \\xi(2\\xi-1)$\n$N_3(\\xi,\\eta) = \\eta(2\\eta-1)$\n\nMid-side nodes:\n$N_4(\\xi,\\eta) = 4L_1 L_2 = 4\\zeta\\xi = 4\\xi(1-\\xi-\\eta)$ (for edge 1-2)\n$N_5(\\xi,\\eta) = 4L_2 L_3 = 4\\xi\\eta$ (for edge 2-3)\n$N_6(\\xi,\\eta) = 4L_3 L_1 = 4\\eta\\zeta = 4\\eta(1-\\xi-\\eta)$ (for edge 3-1)\n\nStep 2: Constructing the Isoparametric Mapping\nThe isoparametric mapping relates the physical coordinates $(x,y)$ to the reference coordinates $(\\xi,\\eta)$ using the same shape functions:\n$$x(\\xi,\\eta) = \\sum_{i=1}^{6} N_i(\\xi,\\eta) x_i$$\n$$y(\\xi,\\eta) = \\sum_{i=1}^{6} N_i(\\xi,\\eta) y_i$$\nThe given physical nodal coordinates are:\n$(x_1, y_1) = (0,0)$, $(x_2, y_2) = (1,0)$, $(x_3, y_3) = (0,1)$,\n$(x_4, y_4) = (0.5, 0.05)$, $(x_5, y_5) = (0.5, 0.55)$, $(x_6, y_6) = (-0.05, 0.5)$.\n\nSubstituting the nodal coordinates and shape functions for $x(\\xi,\\eta)$:\n$x(\\xi,\\eta) = x_1 N_1 + x_2 N_2 + x_3 N_3 + x_4 N_4 + x_5 N_5 + x_6 N_6$\n$x(\\xi,\\eta) = (0)N_1 + (1)N_2 + (0)N_3 + (0.5)N_4 + (0.5)N_5 + (-0.05)N_6$\n$x(\\xi,\\eta) = \\xi(2\\xi-1) + 0.5 \\cdot 4\\xi(1-\\xi-\\eta) + 0.5 \\cdot 4\\xi\\eta - 0.05 \\cdot 4\\eta(1-\\xi-\\eta)$\n$x(\\xi,\\eta) = 2\\xi^2-\\xi + 2\\xi(1-\\xi-\\eta) + 2\\xi\\eta - 0.2\\eta(1-\\xi-\\eta)$\n$x(\\xi,\\eta) = 2\\xi^2-\\xi + 2\\xi - 2\\xi^2 - 2\\xi\\eta + 2\\xi\\eta - 0.2\\eta + 0.2\\xi\\eta + 0.2\\eta^2$\n$x(\\xi,\\eta) = \\xi - 0.2\\eta + 0.2\\eta^2 + 0.2\\xi\\eta$\n\nSubstituting for $y(\\xi,\\eta)$:\n$y(\\xi,\\eta) = y_1 N_1 + y_2 N_2 + y_3 N_3 + y_4 N_4 + y_5 N_5 + y_6 N_6$\n$y(\\xi,\\eta) = (0)N_1 + (0)N_2 + (1)N_3 + (0.05)N_4 + (0.55)N_5 + (0.5)N_6$\n$y(\\xi,\\eta) = \\eta(2\\eta-1) + 0.05 \\cdot 4\\xi(1-\\xi-\\eta) + 0.55 \\cdot 4\\xi\\eta + 0.5 \\cdot 4\\eta(1-\\xi-\\eta)$\n$y(\\xi,\\eta) = 2\\eta^2-\\eta + 0.2\\xi-0.2\\xi^2-0.2\\xi\\eta + 2.2\\xi\\eta + 2\\eta-2\\xi\\eta-2\\eta^2$\n$y(\\xi,\\eta) = \\eta + 0.2\\xi - 0.2\\xi^2 + (-0.2+2.2-2.0)\\xi\\eta$\n$y(\\xi,\\eta) = \\eta + 0.2\\xi - 0.2\\xi^2$\n\nStep 3: Calculating the Jacobian Determinant\nThe Jacobian matrix of the transformation is $\\mathbf{J} = \\frac{\\partial(x,y)}{\\partial(\\xi,\\eta)}$. Its components are the partial derivatives of the mapping functions:\n$\\frac{\\partial x}{\\partial \\xi} = 1 + 0.2\\eta$\n$\\frac{\\partial x}{\\partial \\eta} = -0.2 + 0.4\\eta + 0.2\\xi$\n$\\frac{\\partial y}{\\partial \\xi} = 0.2 - 0.4\\xi$\n$\\frac{\\partial y}{\\partial \\eta} = 1$\n\nThe Jacobian determinant is $J(\\xi,\\eta) = \\det(\\mathbf{J}) = \\frac{\\partial x}{\\partial \\xi}\\frac{\\partial y}{\\partial \\eta} - \\frac{\\partial x}{\\partial \\eta}\\frac{\\partial y}{\\partial \\xi}$:\n$J(\\xi,\\eta) = (1 + 0.2\\eta)(1) - (-0.2 + 0.2\\xi + 0.4\\eta)(0.2 - 0.4\\xi)$\n$J(\\xi,\\eta) = 1 + 0.2\\eta - [-0.04 + 0.08\\xi + 0.04\\xi - 0.08\\xi^2 + 0.08\\eta - 0.16\\xi\\eta]$\n$J(\\xi,\\eta) = 1 + 0.2\\eta - [-0.04 + 0.12\\xi - 0.08\\xi^2 + 0.08\\eta - 0.16\\xi\\eta]$\n$J(\\xi,\\eta) = 1 + 0.2\\eta + 0.04 - 0.12\\xi + 0.08\\xi^2 - 0.08\\eta + 0.16\\xi\\eta$\n$J(\\xi,\\eta) = 1.04 - 0.12\\xi + 0.12\\eta + 0.08\\xi^2 + 0.16\\xi\\eta$\n\nStep 4: Minimizing the Jacobian Determinant\nTo find the minimum value of $J(\\xi,\\eta)$ over the domain defined by $0 \\leq \\xi$, $0 \\leq \\eta$, and $\\xi+\\eta \\leq 1$, we first check for critical points in the interior by setting the gradient of $J$ to zero:\n$\\frac{\\partial J}{\\partial \\xi} = -0.12 + 0.16\\xi + 0.16\\eta = 0$\n$\\frac{\\partial J}{\\partial \\eta} = 0.12 + 0.16\\xi = 0$\nFrom the second equation, $0.16\\xi = -0.12$, which gives $\\xi = -0.12/0.16 = -0.75$. This point is outside the domain, so the minimum must lie on the boundary of the reference triangle. The boundary consists of three segments:\n1. Edge $\\eta=0$, for $0 \\leq \\xi \\leq 1$:\n   $J(\\xi,0) = 1.04 - 0.12\\xi + 0.08\\xi^2$.\n   To find the minimum on this segment, we take the derivative with respect to $\\xi$:\n   $\\frac{dJ}{d\\xi} = -0.12 + 0.16\\xi = 0 \\implies \\xi = 0.12/0.16 = 0.75$.\n   This point is in the interval $[0,1]$. The value of the Jacobian at this point is:\n   $J(0.75, 0) = 1.04 - 0.12(0.75) + 0.08(0.75)^2 = 1.04 - 0.09 + 0.08(0.5625) = 0.95 + 0.045 = 0.995$.\n   The values at the endpoints of this segment are $J(0,0)=1.04$ and $J(1,0)=1.04-0.12+0.08=1$.\n\n2. Edge $\\xi=0$, for $0 \\leq \\eta \\leq 1$:\n   $J(0,\\eta) = 1.04 + 0.12\\eta$. This is a linear, increasing function of $\\eta$. Its minimum on $[0,1]$ occurs at $\\eta=0$, giving $J(0,0)=1.04$.\n\n3. Edge $\\eta=1-\\xi$, for $0 \\leq \\xi \\leq 1$:\n   $J(\\xi, 1-\\xi) = 1.04 - 0.12\\xi + 0.12(1-\\xi) + 0.08\\xi^2 + 0.16\\xi(1-\\xi)$\n   $J(\\xi, 1-\\xi) = 1.04 - 0.12\\xi + 0.12 - 0.12\\xi + 0.08\\xi^2 + 0.16\\xi - 0.16\\xi^2$\n   $J(\\xi, 1-\\xi) = 1.16 - 0.08\\xi - 0.08\\xi^2$.\n   The derivative is $\\frac{dJ}{d\\xi} = -0.08 - 0.16\\xi$. Setting this to zero gives $\\xi = -0.5$, which is outside $[0,1]$. Since the derivative is negative for $\\xi \\in [0,1]$, the function is decreasing. The minimum on this segment occurs at $\\xi=1$, which corresponds to the point $(1,0)$. The value is $J(1,0)=1$, as calculated before.\n\nBy comparing all candidate values ($0.995$, $1.04$, and $1$), the minimum value of the Jacobian determinant is $J_{min} = 0.995$.\n\nConclusion and Significance\nThe minimum value of the Jacobian determinant over the entire element is $J_{min} = 0.995$. Since the nodal coordinates are in meters $(\\mathrm{m})$, the Jacobian determinant has units of area, $\\mathrm{m^2}$.\nThe condition $J(\\xi,\\eta) > 0$ is satisfied everywhere in the element. This is a critical requirement for a valid finite element mesh.\nThe significance of a strictly positive Jacobian determinant is as follows:\n1.  **Geometric Validity**: A positive $J$ ensures that the mapping from the reference element to the physical element is one-to-one and orientation-preserving. This means the element does not fold over itself or become inverted. An element with $J \\leq 0$ at any point is considered a distorted or invalid element.\n2.  **Numerical Integration**: In FEM, integrals of functions over the physical element domain $\\Omega_e$ are evaluated by transforming them to the reference domain: $\\int_{\\Omega_e} f(x,y)\\,dx\\,dy = \\int_0^1 \\int_0^{1-\\xi} f(x(\\xi,\\eta), y(\\xi,\\eta)) J(\\xi,\\eta)\\,d\\eta\\,d\\xi$. If $J \\leq 0$, the differential area element $dA=J\\,d\\xi\\,d\\eta$ would be non-positive, which is physically meaningless and would lead to incorrect results.\n3.  **Solver Stability**: The presence of elements with non-positive Jacobians in a mesh typically results in a singular or ill-conditioned global stiffness matrix, causing the numerical solver to fail or produce physically nonsensical results. Therefore, ensuring $J > 0$ for all elements is a fundamental check in mesh generation for any FEM-based simulation, including computational acoustics.\n\nThe minimum value of the Jacobian determinant is $0.995 \\, \\mathrm{m^2}$. Rounded to four significant figures, this is $0.9950 \\, \\mathrm{m^2}$.",
            "answer": "$$\n\\boxed{0.9950}\n$$"
        },
        {
            "introduction": "While a positive Jacobian ensures an element is geometrically valid, it does not guarantee it is of high quality. In three-dimensional meshes, \"sliver\" tetrahedra—elements with nearly coplanar vertices—can severely degrade the conditioning of system matrices and compromise simulation accuracy. This practice  provides a concrete example of calculating a dimensionless quality metric, the normalized volume, to identify these problematic elements.",
            "id": "4128627",
            "problem": "In computational acoustics, the accuracy and stability of discretizations of the acoustic wave equation are sensitive to the quality of tetrahedral meshes. Among pathological elements, slivers are tetrahedra with nearly coplanar vertices that have very small volume relative to their edge lengths, which can degrade the conditioning of finite element method (FEM) matrices and lead to dispersion errors. A dimensionless quality metric often used to detect such slivers is the normalized volume defined as $V/L_{\\max}^{3}$, where $V$ is the tetrahedron volume and $L_{\\max}$ is the maximum edge length among its six edges.\n\nConsider the tetrahedron with vertices $A=(1,0,0)$, $B=(0,1,0)$, $C=(1,1,10^{-3})$, and $D=(0,0,0)$. Using first principles from vector geometry and without invoking any pre-tabulated mesh quality formulas, compute the normalized volume $V/L_{\\max}^{3}$ and classify the element as a sliver using the following rule: declare “sliver” if $V/L_{\\max}^{3}\\tau$ and “non-sliver” otherwise, with the threshold $\\tau=10^{-3}$. Encode the classification as an indicator $I$ where $I=1$ if the element is a sliver and $I=0$ otherwise.\n\nRound the normalized volume $V/L_{\\max}^{3}$ to four significant figures. Express both components of the final answer as dimensionless numbers. Provide the final answer as a two-component row matrix $\\begin{pmatrix}V/L_{\\max}^{3}  I\\end{pmatrix}$.",
            "solution": "The task is to compute the normalized volume $Q = V/L_{\\max}^{3}$ for a given tetrahedron and classify it as a sliver. The vertices of the tetrahedron are given as $A=(1,0,0)$, $B=(0,1,0)$, $C=(1,1,10^{-3})$, and $D=(0,0,0)$.\n\nFirst, we compute the volume $V$ of the tetrahedron. The volume of a tetrahedron with one vertex at the origin $D$ and other vertices at $A$, $B$, and $C$ is given by one-sixth of the absolute value of the scalar triple product of the vectors $\\vec{DA}$, $\\vec{DB}$, and $\\vec{DC}$.\nThe vectors are:\n$\\vec{DA} = A - D = (1,0,0) - (0,0,0) = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$\n$\\vec{DB} = B - D = (0,1,0) - (0,0,0) = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$\n$\\vec{DC} = C - D = (1,1,10^{-3}) - (0,0,0) = \\begin{pmatrix} 1 \\\\ 1 \\\\ 10^{-3} \\end{pmatrix}$\n\nThe scalar triple product can be computed as the determinant of the matrix whose columns (or rows) are these vectors.\n$$V = \\frac{1}{6} |(\\vec{DA} \\times \\vec{DB}) \\cdot \\vec{DC}| = \\frac{1}{6} \\left| \\det \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 0  0  10^{-3} \\end{pmatrix} \\right|$$\nExpanding the determinant along the first column:\n$$ \\det \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 0  0  10^{-3} \\end{pmatrix} = 1 \\cdot \\det \\begin{pmatrix} 1  1 \\\\ 0  10^{-3} \\end{pmatrix} - 0 + 0 = 1 \\cdot (1 \\cdot 10^{-3} - 1 \\cdot 0) = 10^{-3} $$\nThe volume is therefore:\n$$V = \\frac{1}{6} |10^{-3}| = \\frac{1}{6} \\times 10^{-3}$$\n\nNext, we find the maximum edge length $L_{\\max}$ among the six edges of the tetrahedron. The edges are $DA$, $DB$, $DC$, $AB$, $AC$, and $BC$. We compute the square of each edge length to simplify comparison.\nThe vectors corresponding to the edges are:\n$\\vec{DA} = (1,0,0)$\n$\\vec{DB} = (0,1,0)$\n$\\vec{DC} = (1,1,10^{-3})$\n$\\vec{AB} = B - A = (0-1, 1-0, 0-0) = (-1,1,0)$\n$\\vec{AC} = C - A = (1-1, 1-0, 10^{-3}-0) = (0,1,10^{-3})$\n$\\vec{BC} = C - B = (1-0, 1-1, 10^{-3}-0) = (1,0,10^{-3})$\n\nThe squared lengths are the squared magnitudes of these vectors:\n$L_{DA}^2 = 1^2 + 0^2 + 0^2 = 1$\n$L_{DB}^2 = 0^2 + 1^2 + 0^2 = 1$\n$L_{DC}^2 = 1^2 + 1^2 + (10^{-3})^2 = 1 + 1 + 10^{-6} = 2 + 10^{-6}$\n$L_{AB}^2 = (-1)^2 + 1^2 + 0^2 = 1 + 1 = 2$\n$L_{AC}^2 = 0^2 + 1^2 + (10^{-3})^2 = 1 + 10^{-6}$\n$L_{BC}^2 = 1^2 + 0^2 + (10^{-3})^2 = 1 + 10^{-6}$\n\nComparing the squared lengths: $1$, $1$, $2+10^{-6}$, $2$, $1+10^{-6}$, $1+10^{-6}$. The maximum squared length is $L_{\\max}^2 = 2 + 10^{-6}$.\nThus, the maximum edge length is $L_{\\max} = \\sqrt{2 + 10^{-6}}$.\n\nNow, we compute the normalized volume $Q = V/L_{\\max}^{3}$:\n$$Q = \\frac{V}{L_{\\max}^3} = \\frac{\\frac{1}{6} \\times 10^{-3}}{(\\sqrt{2 + 10^{-6}})^3} = \\frac{10^{-3}}{6(2 + 10^{-6})^{3/2}}$$\nTo obtain a numerical value, we evaluate the expression:\n$$(2 + 10^{-6})^{3/2} = (2.000001)^{1.5} \\approx 2.82842924$$\n$$Q \\approx \\frac{10^{-3}}{6 \\times 2.82842924} = \\frac{10^{-3}}{16.97057544} \\approx 0.0589255 \\times 10^{-3}$$\nIn scientific notation, this is $5.89255 \\times 10^{-5}$.\nRounding to four significant figures as required, we get:\n$$Q \\approx 5.893 \\times 10^{-5}$$\n\nFinally, we classify the element. The rule is to classify it as a \"sliver\" if $Q  \\tau$, where the threshold is $\\tau = 10^{-3}$. The classification is encoded by an indicator $I$, where $I=1$ for a sliver and $I=0$ otherwise.\nWe compare the computed normalized volume $Q$ with the threshold $\\tau$:\n$$5.893 \\times 10^{-5}  10^{-3}$$\nThe inequality is true, as $5.893 \\times 10^{-5} = 0.00005893$ and $10^{-3} = 0.001$.\nTherefore, the element is a sliver, and the indicator is $I=1$.\n\nThe final answer is composed of the rounded normalized volume and the indicator $I$, presented as a two-component row matrix $\\begin{pmatrix} Q  I \\end{pmatrix}$.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 5.893 \\times 10^{-5}  1 \\end{pmatrix} } $$"
        },
        {
            "introduction": "Solving large-scale acoustic problems often requires the immense power of parallel computers, and a critical step in this process is domain decomposition, where the mesh is partitioned among many processors. This exercise  delves into the powerful technique of spectral partitioning, which uses the eigenvectors of the graph Laplacian to find a partition that balances computational load while minimizing inter-processor communication. You will implement this foundational algorithm and a multilevel refinement scheme to see how mesh partitioning quality can be systematically improved.",
            "id": "4128632",
            "problem": "You are given weighted graphs that are abstractions of finite element meshes used in computational acoustics for discretizing the Helmholtz equation. Each vertex represents a mesh element, each undirected edge represents an interface between neighboring elements, the edge weight represents a symmetric communication cost between neighboring elements during domain decomposition, and the vertex weight approximates the per-element computational cost of assembling and solving the local discretized equations.\n\nStarting from first principles, you must implement a spectral bipartition driven by the Fiedler vector and evaluate two quality metrics relevant to parallel computational acoustics: load imbalance and edge cut. Then, you must implement a single coarsen-partition-refine multilevel improvement and reevaluate these metrics.\n\nFundamental base for the derivation:\n\n- The discrete graph Laplacian is defined for a weighted, undirected graph with adjacency weights $w_{ij} \\ge 0$ by\n$$\nL = D - W,\n$$\nwhere $W$ is the symmetric weight matrix with entries $W_{ij} = w_{ij}$ for $i \\ne j$ and $W_{ii} = 0$, and $D$ is the diagonal degree matrix with entries $D_{ii} = \\sum_{j} w_{ij}$. For a connected graph, $L$ is symmetric positive semidefinite, with the smallest eigenvalue equal to $0$ and associated eigenvector proportional to the constant vector.\n- The Fiedler vector is the eigenvector $v^{(2)}$ associated with the second smallest eigenvalue $\\lambda_2$ of $L$. Spectral bipartitioning chooses a threshold to split vertices into two sets using the components of $v^{(2)}$ to minimize a relaxation of the cut objective derived from the Rayleigh quotient\n$$\n\\mathcal{R}(x) = \\frac{x^\\top L x}{x^\\top x}.\n$$\n\nDefinitions of metrics:\n\n- Given a bipartition of the vertex set into $A$ and $B$, with vertex weights $\\{m_i\\}_{i=1}^n$, define the total weight $M = \\sum_{i=1}^n m_i$, and partial weights $M_A = \\sum_{i \\in A} m_i$, $M_B = \\sum_{i \\in B} m_i$. The load imbalance (expressed as a decimal, without a percentage sign) is\n$$\n\\mathrm{imbalance} = \\frac{|M_A - M_B|}{M}.\n$$\n- The edge cut is the sum of weights of edges whose endpoints lie in different partitions:\n$$\n\\mathrm{cut} = \\sum_{\\{i,j\\} \\in E, \\, i \\in A, \\, j \\in B} w_{ij}.\n$$\n\nPartitioning method to implement:\n\n1. Build the weighted Laplacian $L$ for the given graph.\n2. Compute the Fiedler vector $v^{(2)}$ (the eigenvector associated with the second smallest eigenvalue of $L$).\n3. Choose a threshold by the weighted median with respect to the vertex weights $\\{m_i\\}$, i.e., sort the vertices by the components of $v^{(2)}$ in ascending order and select the smallest prefix whose cumulative vertex weight is at least $M/2$. Put that prefix in partition $A$ and the rest in partition $B$. This balances the partitions in terms of vertex weight and is appropriate for acoustics where vertex weights reflect computational load.\n4. Compute the load imbalance and edge cut for this spectral partition.\n\nMultilevel improvement to implement:\n\n- Perform a single heavy-edge matching coarsening:\n  - Construct a matching by iterating vertices in descending order of weighted degree and greedily pairing each unmatched vertex $i$ with an unmatched neighbor $j$ that maximizes $w_{ij}$. Unmatched vertices become singleton aggregates.\n  - Collapse matched pairs (and singletons) to build a coarse graph. The coarse vertex weight is the sum of fine vertex weights in the aggregate, and the coarse edge weight between two aggregates is the sum of weights of all fine edges that connect the two aggregates.\n- Compute the coarse Fiedler vector and perform weighted-median thresholding at the coarse level to partition coarse aggregates into two sets.\n- Lift the coarse partition to the fine graph by assigning each fine vertex to the partition of its aggregate.\n- Apply a one-pass local refinement: consider moving individual boundary vertices from one partition to the other if the move strictly decreases the edge cut and keeps the load imbalance below a tolerance of $\\varepsilon = 0.05$ (i.e., enforce $|M_A - M_B|/M \\le 0.05$ after the move). Perform greedy moves in descending order of improvement until no admissible move exists.\n- Compute the load imbalance and edge cut after the multilevel improvement.\n\nAngle units are not applicable. Physical units are not required for the final numerical answers. All outputs must be in pure numbers without units.\n\nTest suite:\n\nYou must implement the program to handle exactly these three parameterized test cases. For each case, construct the graph as specified.\n\n- Case $1$ (happy path): a rectangular grid graph of size $n_x = 4$, $n_y = 4$ with $n = n_x \\cdot n_y = 16$ vertices. Connect each vertex to its $4$-connected neighbors (up, down, left, right within bounds). Use unit vertex weights $m_i = 1$ and unit edge weights $w_{ij} = 1$ for all edges.\n- Case $2$ (weakly connected clusters): two path graphs of sizes $n_1 = 10$ and $n_2 = 10$ connected by a single bridge edge. Within each path graph, connect consecutive vertices with unit edge weight $w_{ij} = 1$; connect the last vertex of the first path to the first vertex of the second path by a bridge of weight $w_{\\mathrm{bridge}} = 0.1$. Use unit vertex weights $m_i = 1$.\n- Case $3$ (anisotropic mesh surrogate): a rectangular grid graph of size $n_x = 6$, $n_y = 3$ with $n = 18$ vertices. Connect $4$-neighbors. Set vertex weights to $m_i = 2$ for vertices in columns $1$ and $2$ (the leftmost third), and $m_i = 1$ otherwise. Set edge weights to $w_{ij} = 3$ for horizontal edges and $w_{ij} = 1$ for vertical edges.\n\nRequired final output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case contributing a list of four numbers $[\\mathrm{imbalance\\_before}, \\mathrm{cut\\_before}, \\mathrm{imbalance\\_after}, \\mathrm{cut\\_after}]$ in the order of the cases described above. For example, the output should look like\n$$\n\\left[\\,[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],[a_3,b_3,c_3,d_3]\\,\\right],\n$$\nwhere each $a_k$, $b_k$, $c_k$, $d_k$ is a float. The program must not print any other text.",
            "solution": "The foundation of spectral partitioning lies in the properties of the graph Laplacian, a matrix that encodes the connectivity of a graph. For a weighted, undirected graph with $n$ vertices, vertex weights $\\{m_i\\}_{i=1}^n$, and a symmetric adjacency weight matrix $W$ (where $W_{ij} = w_{ij}$ is the weight of the edge between vertices $i$ and $j$, and $W_{ii} = 0$), the Laplacian $L$ is defined as:\n$$\nL = D - W\n$$\nHere, $D$ is the diagonal matrix of weighted degrees, with $D_{ii} = \\sum_{j=1}^n w_{ij}$. The Laplacian is symmetric and positive semidefinite. A key insight, due to Miroslav Fiedler, links the algebraic properties of $L$ to the graph's connectivity. Specifically, partitioning the graph to minimize the edge cut is a hard combinatorial problem. It can be relaxed into a continuous optimization problem by minimizing the Rayleigh quotient:\n$$\n\\mathcal{R}(x) = \\frac{x^\\top L x}{x^\\top x} = \\frac{\\sum_{\\{i,j\\} \\in E} w_{ij}(x_i - x_j)^2}{\\sum_{i=1}^n x_i^2}\n$$\nThe vector $x$ that minimizes $\\mathcal{R}(x)$ subject to being orthogonal to the constant vector (which corresponds to the trivial eigenvalue $\\lambda_1=0$) is the eigenvector associated with the second smallest eigenvalue, $\\lambda_2$. This eigenvector is known as the Fiedler vector, $v^{(2)}$. The components of $v^{(2)}$ provide a one-dimensional embedding of the graph's vertices. A cut can be found by choosing a threshold and splitting the vertices based on whether their corresponding component in $v^{(2)}$ is above or below this threshold.\n\nThe problem specifies a particular thresholding strategy designed to balance the computational load, which is represented by the vertex weights $m_i$. The procedure is as follows:\n1.  Construct the Laplacian matrix $L$ from the given graph.\n2.  Compute the eigenvalues and eigenvectors of $L$. The Fiedler vector $v^{(2)}$ is the eigenvector corresponding to the second-smallest eigenvalue $\\lambda_2$.\n3.  Sort the vertices based on the values of their corresponding components in $v^{(2)}$ in ascending order.\n4.  Iterate through this sorted list of vertices, accumulating their weights $m_i$. The partition boundary is placed such that one partition, $A$, is the smallest prefix of vertices whose cumulative weight is at least half of the total graph weight, $M/2 = (\\sum_{i=1}^n m_i)/2$. The remaining vertices form the second partition, $B$.\n5.  With this partition $(A, B)$, the load imbalance and edge cut are calculated using their formal definitions.\n\nThe initial spectral partition is often a good starting point, but it can be improved. Multilevel methods refine partitions by operating on a hierarchy of successively coarser-grained representations of the graph. The prescribed method is a single coarsen-partition-refine cycle.\n\nThe goal of coarsening is to create a smaller, simpler graph that preserves the essential structure of the original (fine) graph. Heavy-Edge Matching (HEM) is used, where vertices are matched with neighbors to which they are most strongly connected. These aggregates become the vertices of the new coarse graph. The coarse graph's properties are derived from the aggregates: the weight of a coarse vertex is the sum of the weights of the fine vertices in its aggregate, and the weight of an edge between two coarse vertices is the sum of the weights of all fine edges connecting the fine vertices in the corresponding aggregates. Once the coarse graph is built, the same spectral partitioning algorithm (Fiedler vector and weighted median split) is applied to it.\n\nThe partition of the coarse graph is projected back to the fine graph. This is a simple \"lifting\" operation: all fine-level vertices belonging to a single coarse-level aggregate are assigned to the partition of that aggregate. This lifted partition is then refined using a greedy local refinement heuristic. Boundary vertices are moved from one partition to the other if the move reduces the edge cut while keeping the load imbalance below a specified tolerance. This process of finding and executing the best admissible move is repeated until no more such moves exist.\n\nFinally, the load imbalance and edge cut are computed for this refined partition. This two-phase process is applied to each of the three test cases to generate the required numerical results.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh\n\ndef build_graph(case_params):\n    \"\"\"Builds the graph (adjacency matrix and vertex weights) for a test case.\"\"\"\n    case_type = case_params['type']\n    if case_type == 1:\n        nx, ny = 4, 4\n        n = nx * ny\n        W = np.zeros((n, n))\n        m = np.ones(n)\n        for r in range(ny):\n            for c in range(nx):\n                idx = r * nx + c\n                if r  0: W[idx, (r-1)*nx + c] = 1 # Up\n                if r  ny - 1: W[idx, (r+1)*nx + c] = 1 # Down\n                if c  0: W[idx, r*nx + c - 1] = 1 # Left\n                if c  nx - 1: W[idx, r*nx + c + 1] = 1 # Right\n    elif case_type == 2:\n        n1, n2 = 10, 10\n        n = n1 + n2\n        W = np.zeros((n, n))\n        m = np.ones(n)\n        for i in range(n1 - 1):\n            W[i, i + 1] = W[i + 1, i] = 1\n        for i in range(n1, n1 + n2 - 1):\n            W[i, i + 1] = W[i + 1, i] = 1\n        W[n1 - 1, n1] = W[n1, n1 - 1] = 0.1\n    elif case_type == 3:\n        nx, ny = 6, 3\n        n = nx * ny\n        W = np.zeros((n, n))\n        m = np.ones(n)\n        for r in range(ny):\n            for c in range(nx):\n                idx = r * nx + c\n                if c  2:\n                    m[idx] = 2\n                if r  0: W[idx, (r-1)*nx + c] = W[(r-1)*nx + c, idx] = 1 # Vertical\n                if c  0: W[idx, r*nx + c - 1] = W[r*nx + c - 1, idx] = 3 # Horizontal\n    return W, m\n\ndef spectral_partition(W, m):\n    \"\"\"Partitions a graph using the Fiedler vector and weighted-median thresholding.\"\"\"\n    n = W.shape[0]\n    if n == 0:\n        return set(), set()\n    \n    D = np.diag(np.sum(W, axis=1))\n    L = D - W\n    \n    try:\n        eigenvalues, eigenvectors = eigh(L)\n        fiedler_vector = eigenvectors[:, 1]\n    except (np.linalg.LinAlgError, IndexError):\n        # Fallback for disconnected or tiny graphs\n        part_A = set(range(n // 2))\n        part_B = set(range(n // 2, n))\n        return part_A, part_B\n\n    sorted_indices = np.argsort(fiedler_vector)\n    \n    total_weight = np.sum(m)\n    half_weight = total_weight / 2.0\n    \n    part_A = set()\n    current_weight = 0.0\n    for idx in sorted_indices:\n        part_A.add(idx)\n        current_weight += m[idx]\n        if current_weight = half_weight:\n            break\n            \n    part_B = set(range(n)) - part_A\n    return part_A, part_B\n\ndef calculate_metrics(W, m, part_A, part_B):\n    \"\"\"Calculates load imbalance and edge cut for a given partition.\"\"\"\n    n = W.shape[0]\n    if n == 0:\n        return 0.0, 0.0\n\n    A_indices = list(part_A)\n    B_indices = list(part_B)\n\n    MA = np.sum(m[A_indices]) if A_indices else 0\n    MB = np.sum(m[B_indices]) if B_indices else 0\n    M = MA + MB\n    imbalance = np.abs(MA - MB) / M if M  0 else 0\n\n    cut = np.sum(W[np.ix_(A_indices, B_indices)])\n\n    return imbalance, cut\n\ndef refine_partition_greedy(W, m, part_A_initial, part_B_initial):\n    \"\"\"Refines a partition using an iterative greedy approach.\"\"\"\n    n = W.shape[0]\n    if n == 0: return set(), set()\n    \n    A_current, B_current = set(part_A_initial), set(part_B_initial)\n    total_weight = np.sum(m)\n    tolerance = 0.05\n    \n    while True:\n        moves = []\n        MA = np.sum(m[list(A_current)]) if A_current else 0\n        MB = np.sum(m[list(B_current)]) if B_current else 0\n\n        boundary_nodes = set()\n        if A_current and B_current:\n            A_indices, B_indices = list(A_current), list(B_current)\n            A_mask = np.zeros(n, dtype=bool); A_mask[A_indices] = True\n            B_mask = np.zeros(n, dtype=bool); B_mask[B_indices] = True\n            \n            boundary_A = {i for i in A_indices if np.any(W[i, B_mask])}\n            boundary_B = {i for i in B_indices if np.any(W[i, A_mask])}\n            boundary_nodes.update(boundary_A)\n            boundary_nodes.update(boundary_B)\n\n        for v in boundary_nodes:\n            if v in A_current:\n                A_indices = list(A_current)\n                B_indices = list(B_current)\n                internal_cost = np.sum(W[v, [i for i in A_indices if i != v]])\n                external_cost = np.sum(W[v, B_indices])\n                gain = external_cost - internal_cost # Gain is reduction in cut\n                \n                MA_new, MB_new = MA - m[v], MB + m[v]\n                new_imbalance = abs(MA_new - MB_new) / total_weight if total_weight  0 else 0\n                \n                if gain  0 and new_imbalance = tolerance:\n                    moves.append((gain, v, 'A_to_B'))\n            else: # v in B_current\n                A_indices = list(A_current)\n                B_indices = list(B_current)\n                internal_cost = np.sum(W[v, [i for i in B_indices if i != v]])\n                external_cost = np.sum(W[v, A_indices])\n                gain = external_cost - internal_cost # Gain is reduction in cut\n\n                MB_new, MA_new = MB - m[v], MA + m[v]\n                new_imbalance = abs(MA_new - MB_new) / total_weight if total_weight  0 else 0\n                \n                if gain  0 and new_imbalance = tolerance: moves.append((gain, v, 'B_to_A'))\n        \n        if not moves:\n            break\n            \n        moves.sort(key=lambda x: (-x[0], x[1])) # Sort by gain (desc), then vertex index (asc)\n        \n        _best_gain, v_to_move, direction = moves[0]\n        \n        if direction == 'A_to_B':\n            A_current.remove(v_to_move)\n            B_current.add(v_to_move)\n        else:\n            B_current.remove(v_to_move)\n            A_current.add(v_to_move)\n\n    return A_current, B_current\n\n\ndef multilevel_improvement(W, m):\n    \"\"\"Executes a single coarsen-partition-refine cycle.\"\"\"\n    n = W.shape[0]\n\n    # 1. Coarsening (Heavy-Edge Matching)\n    weighted_degrees = np.sum(W, axis=1)\n    sorted_vertices = np.lexsort((np.arange(n), -weighted_degrees))\n\n    fine_to_coarse = -np.ones(n, dtype=int)\n    coarse_idx = 0\n    for i in sorted_vertices:\n        if fine_to_coarse[i] != -1: continue\n        \n        neighbors = np.where(W[i, :]  0)[0]\n        unmatched_neighbors = [j for j in neighbors if fine_to_coarse[j] == -1]\n\n        best_neighbor = -1\n        if unmatched_neighbors:\n            candidate_weights = W[i, unmatched_neighbors]\n            max_w = np.max(candidate_weights)\n            best_neighbors_for_max_w = [unmatched_neighbors[k] for k, w in enumerate(candidate_weights) if w == max_w]\n            best_neighbor = min(best_neighbors_for_max_w)\n        \n        fine_to_coarse[i] = coarse_idx\n        if best_neighbor != -1:\n            fine_to_coarse[best_neighbor] = coarse_idx\n        coarse_idx += 1\n    \n    num_coarse_vertices = coarse_idx\n\n    # 2. Coarse Graph Construction\n    m_coarse = np.zeros(num_coarse_vertices)\n    W_coarse = np.zeros((num_coarse_vertices, num_coarse_vertices))\n    for i in range(n):\n        m_coarse[fine_to_coarse[i]] += m[i]\n    \n    for i in range(n):\n        for j in range(i + 1, n):\n            if W[i, j]  0:\n                ci, cj = fine_to_coarse[i], fine_to_coarse[j]\n                if ci != cj:\n                    W_coarse[ci, cj] += W[i, j]\n                    W_coarse[cj, ci] += W[i, j]\n\n    # 3. Partition Coarse Graph\n    coarse_part_A, coarse_part_B = spectral_partition(W_coarse, m_coarse)\n\n    # 4. Lift Partition\n    A_lifted, B_lifted = set(), set()\n    for i in range(n):\n        if fine_to_coarse[i] in coarse_part_A:\n            A_lifted.add(i)\n        else:\n            B_lifted.add(i)\n\n    # 5. Refinement\n    A_final, B_final = refine_partition_greedy(W, m, A_lifted, B_lifted)\n\n    return A_final, B_final\n\n\ndef solve():\n    test_cases = [\n        {'type': 1},\n        {'type': 2},\n        {'type': 3},\n    ]\n\n    results = []\n    for params in test_cases:\n        W, m = build_graph(params)\n        \n        # Before multilevel improvement\n        part_A_before, part_B_before = spectral_partition(W, m)\n        imbalance_before, cut_before = calculate_metrics(W, m, part_A_before, part_B_before)\n\n        # After multilevel improvement\n        part_A_after, part_B_after = multilevel_improvement(W, m)\n        imbalance_after, cut_after = calculate_metrics(W, m, part_A_after, part_B_after)\n        \n        results.append([imbalance_before, cut_before, imbalance_after, cut_after])\n\n    # Final print statement in the exact required format.\n    # The problem asks for float outputs.\n    formatted_results = []\n    for res in results:\n        formatted_results.append(f'[{float(res[0])},{float(res[1])},{float(res[2])},{float(res[3])}]')\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}