{
    "hands_on_practices": [
        {
            "introduction": "The foundation of robustly tracking solution curves through turning points lies in mastering the predictor-corrector algorithm. This practice provides a direct, hands-on implementation of this core mechanic for a chemically reacting system. By working through the exercise , you will build a single predictor-corrector step from first principles, including the calculation of the tangent vector to the solution manifold and the application of Newton's method to the augmented system that corrects back to the curve. This foundational skill is essential for developing any advanced continuation-based analysis tool.",
            "id": "4007867",
            "problem": "A continuous stirred tank reactor (CSTR) sustaining a single irreversible exothermic reaction is modeled at steady state by two dimensionless algebraic balances derived from the conservation of species and energy. The reaction rate is approximated by the Frank-Kamenetskii form, a widely used simplification of the Arrhenius kinetics for thermally driven reactions in combustion, wherein the nondimensional rate grows exponentially with the nondimensional temperature. Let the unknowns be the nondimensional temperature $T$ and the nondimensional reactant concentration $c$, and let the control parameter be the Damköhler number $\\text{Da}$, which scales the residence time and intrinsic kinetics. The reaction rate is $r(T,c) = c \\exp(\\gamma T)$, where $\\gamma > 0$ is the nondimensional activation parameter. The steady-state equations are\n$$\nF_1(T,c,\\text{Da}) = (1 - c) - \\text{Da} \\, c \\exp(\\gamma T) = 0,\n$$\n$$\nF_2(T,c,\\text{Da}) = (1 + h)(1 - T) + \\beta \\, \\text{Da} \\, c \\exp(\\gamma T) = 0,\n$$\nwhere $\\beta > 0$ is the nondimensional heat-release parameter and $h > 0$ is the nondimensional heat-removal coefficient; the feed and coolant temperatures and inlet concentration have been scaled to unity.\n\nArc-length continuation augments these nonlinear equations to track steady-state solution branches across turning points in the parameter $\\text{Da}$. Define the vector of variables $\\mathbf{u} = (T,c,\\text{Da})$ and the residual vector $\\mathbf{F}(\\mathbf{u}) = (F_1, F_2)$. At an operating point $\\mathbf{u}_0 = (T_0, c_0, \\text{Da}_0)$ satisfying $\\mathbf{F}(\\mathbf{u}_0) = \\mathbf{0}$, the tangent direction $\\boldsymbol{\\tau} = (\\tau_T, \\tau_c, \\tau_{\\text{Da}})$ to the solution manifold is determined by differentiating $\\mathbf{F}(\\mathbf{u}) = \\mathbf{0}$ with respect to an arclength parameter $s$ and imposing a normalization. A predictor step advances to $\\mathbf{u}_{\\text{pred}} = \\mathbf{u}_0 + \\Delta s \\, \\boldsymbol{\\tau}$ for a given arclength step size $\\Delta s$. A corrector then solves the augmented system composed of the two reactor residuals and a pseudo-arclength constraint to return to the manifold near $\\mathbf{u}_{\\text{pred}}$.\n\nYour task is to implement the following, starting from the fundamental definitions above and without relying on any pre-supplied formulas:\n1. For each specified operating point, compute a steady-state solution $(T_0,c_0)$ at the given $\\text{Da}_0$ by solving $F_1(T,c,\\text{Da}_0) = 0$ and $F_2(T,c,\\text{Da}_0) = 0$.\n2. At $\\mathbf{u}_0$, calculate the tangent direction $\\boldsymbol{\\tau}$ consistent with a unit Euclidean norm, perform a predictor step with the given $\\Delta s$, and compute the two-norm of the reactor residual $\\|\\mathbf{F}(\\mathbf{u}_{\\text{pred}})\\|_2$.\n3. Carry out a corrector step using Newton's method applied to the augmented system consisting of the two reactor equations and a pseudo-arclength constraint. Report the corrected $(T,\\text{Da})$, the two-norm of the reactor residual $\\|\\mathbf{F}(\\mathbf{u}_{\\text{corr}})\\|_2$, and the two-norm of the augmented residual $\\|\\mathbf{G}(\\mathbf{u}_{\\text{corr}})\\|_2$, where $\\mathbf{G}$ stacks the reactor residuals and the arclength constraint.\n4. All computations use the parameter values $\\gamma = 3$, $\\beta = 2$, $h = 3$. Variables $T$, $c$, and $\\text{Da}$ are nondimensional, so no physical unit conversions are required.\n\nTest Suite:\n- Case A (general case): $\\text{Da}_0 = 0.05$, initial guess $(T,c) = (1.0, 0.9)$, and $\\Delta s = 0.02$.\n- Case B (near turning point detection): scan $\\text{Da}$ uniformly over the interval $[0.01, 0.5]$ with $N = 200$ grid points. At each grid point, solve for $(T,c)$ starting from the previous solution as the initial guess and select the operating point with the smallest singular value of the Jacobian $\\partial \\mathbf{F}/\\partial (T,c)$, then set $\\Delta s = 0.01$. Use $(T,c) = (1.0, 0.8)$ as the starting guess at the first grid point.\n- Case C (small-step edge case): $\\text{Da}_0 = 0.20$, initial guess $(T,c) = (1.1, 0.5)$, and $\\Delta s = 10^{-4}$.\n\nFor each case, the program must return a list of seven floating-point numbers:\n- $T_{\\text{pred}}$, $\\text{Da}_{\\text{pred}}$, $\\|\\mathbf{F}(\\mathbf{u}_{\\text{pred}})\\|_2$, $T_{\\text{corr}}$, $\\text{Da}_{\\text{corr}}$, $\\|\\mathbf{F}(\\mathbf{u}_{\\text{corr}})\\|_2$, $\\|\\mathbf{G}(\\mathbf{u}_{\\text{corr}})\\|_2$.\nRound each number to six decimal places.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is the seven-number list for a test case, in the same order as the test suite, for example, \n$[$$[T_{\\text{pred}}^{(A)},\\text{Da}_{\\text{pred}}^{(A)},\\dots]$$,$$[T_{\\text{pred}}^{(B)},\\text{Da}_{\\text{pred}}^{(B)},\\dots]$$,$$[T_{\\text{pred}}^{(C)},\\text{Da}_{\\text{pred}}^{(C)},\\dots]$$$]$.",
            "solution": "The continuous stirred tank reactor (CSTR) model is built from steady-state species and energy balances. With inlet values scaled so that the feed concentration and feed temperature are unity, and with dimensionless heat removal and release parameters $h$ and $\\beta$, the balances read\n$$\nF_1(T,c,\\text{Da}) = (1 - c) - \\text{Da} \\, c \\exp(\\gamma T) = 0,\n\\quad\nF_2(T,c,\\text{Da}) = (1 + h)(1 - T) + \\beta \\,\\text{Da}\\, c \\exp(\\gamma T) = 0.\n$$\nThese follow from equating inflow minus outflow plus reaction consumption (for species) and cooling minus heating plus reaction heat release (for energy) to zero, after nondimensionalization. The Frank-Kamenetskii approximation, $r(T,c) = c \\exp(\\gamma T)$, is a well-tested simplification of the Arrhenius rate $r \\propto c \\exp(-E/(R T_{\\text{phys}}))$ when expressed in terms of a suitable dimensionless temperature deviation; here $\\gamma$ is an effective activation parameter.\n\nTo perform arc-length continuation, we consider the manifold of solutions $\\{(T,c,\\text{Da}) : \\mathbf{F}(T,c,\\text{Da}) = \\mathbf{0}\\}$ and introduce an arclength parameter $s$ such that $(T,c,\\text{Da}) = \\mathbf{u}(s)$ traces the curve. Differentiating $\\mathbf{F}(\\mathbf{u}(s)) = \\mathbf{0}$ with respect to $s$ produces the linear relation\n$$\n\\frac{\\partial \\mathbf{F}}{\\partial (T,c)} \\begin{bmatrix}\\dfrac{dT}{ds} \\\\ \\dfrac{dc}{ds} \\end{bmatrix} + \\frac{\\partial \\mathbf{F}}{\\partial \\text{Da}} \\, \\frac{d\\,\\text{Da}}{ds} = \\mathbf{0}.\n$$\nAt a solution point $\\mathbf{u}_0 = (T_0,c_0,\\text{Da}_0)$, let $J = \\partial \\mathbf{F}/\\partial (T,c)$ be the $2\\times 2$ Jacobian and $F_{\\text{Da}} = \\partial \\mathbf{F}/\\partial \\text{Da}$ the $2\\times 1$ parameter derivative, both evaluated at $\\mathbf{u}_0$. A practical bordering strategy sets $\\dfrac{d\\,\\text{Da}}{ds} = \\tau_{\\text{Da}} = 1$, yielding the linear system\n$$\nJ \\begin{bmatrix}\\tau_T \\\\ \\tau_c \\end{bmatrix} = - F_{\\text{Da}},\n$$\nwhich can be solved for the $(T,c)$ components of the tangent. The full tangent $\\boldsymbol{\\tau} = (\\tau_T,\\tau_c,\\tau_{\\text{Da}})$ is then normalized to unit Euclidean length:\n$$\n\\boldsymbol{\\tau} \\leftarrow \\frac{\\boldsymbol{\\tau}}{\\|\\boldsymbol{\\tau}\\|_2}.\n$$\nA predictor step advances to\n$$\n\\mathbf{u}_{\\text{pred}} = \\mathbf{u}_0 + \\Delta s \\, \\boldsymbol{\\tau}.\n$$\nThe predictor generally leaves the manifold, so we correct back by solving an augmented system of three equations in the three unknowns $(T,c,\\text{Da})$:\n$$\n\\mathbf{G}(\\mathbf{u}) = \n\\begin{bmatrix}\nF_1(T,c,\\text{Da}) \\\\\nF_2(T,c,\\text{Da}) \\\\\ng(T,c,\\text{Da})\n\\end{bmatrix}\n= \\mathbf{0},\n\\quad\ng(T,c,\\text{Da}) = \\boldsymbol{\\tau}^\\top \\big( \\mathbf{u} - \\mathbf{u}_0 \\big) - \\Delta s,\n$$\nwhich imposes the pseudo-arclength constraint so that the corrected solution lies on the hyperplane orthogonal to the difference between the predictor and the base point while being at arclength $\\Delta s$ from $\\mathbf{u}_0$ along the tangent direction. Newton's method updates $\\mathbf{u}$ via\n$$\n\\mathbf{u}^{(k+1)} = \\mathbf{u}^{(k)} - \\left[ \\frac{\\partial \\mathbf{G}}{\\partial \\mathbf{u}}(\\mathbf{u}^{(k)}) \\right]^{-1} \\mathbf{G}(\\mathbf{u}^{(k)}),\n$$\nwhere the $3\\times 3$ Jacobian of $\\mathbf{G}$ is\n$$\n\\frac{\\partial \\mathbf{G}}{\\partial \\mathbf{u}} =\n\\begin{bmatrix}\n\\frac{\\partial F_1}{\\partial T} & \\frac{\\partial F_1}{\\partial c} & \\frac{\\partial F_1}{\\partial \\text{Da}} \\\\\n\\frac{\\partial F_2}{\\partial T} & \\frac{\\partial F_2}{\\partial c} & \\frac{\\partial F_2}{\\partial \\text{Da}} \\\\\n\\tau_T & \\tau_c & \\tau_{\\text{Da}}\n\\end{bmatrix}.\n$$\nThe derivatives for our model are obtained from the definitions of $F_1$ and $F_2$:\n$$\n\\frac{\\partial F_1}{\\partial T} = - \\text{Da} \\, c \\, \\gamma \\exp(\\gamma T),\n\\quad\n\\frac{\\partial F_1}{\\partial c} = -1 - \\text{Da} \\exp(\\gamma T),\n\\quad\n\\frac{\\partial F_1}{\\partial \\text{Da}} = - c \\exp(\\gamma T),\n$$\n$$\n\\frac{\\partial F_2}{\\partial T} = - (1 + h) + \\beta \\, \\text{Da} \\, c \\, \\gamma \\exp(\\gamma T),\n\\quad\n\\frac{\\partial F_2}{\\partial c} = \\beta \\, \\text{Da} \\exp(\\gamma T),\n\\quad\n\\frac{\\partial F_2}{\\partial \\text{Da}} = \\beta \\, c \\exp(\\gamma T).\n$$\nWe implement Newton's method with a simple backtracking line search to ensure a decrease in the norm $\\|\\mathbf{G}\\|_2$ at each iteration when possible. The correction terminates when $\\|\\mathbf{G}\\|_2$ is below a tolerance, indicating convergence to the manifold under the arclength constraint.\n\nFor each test case:\n1. Solve the steady-state equations to obtain $\\mathbf{u}_0$. In Case B, scanning over $\\text{Da}$ and selecting the point with the smallest singular value of $J$ targets a near-turning point where $J$ is nearly singular, stressing the advantage of arc-length continuation.\n2. Compute the tangent $\\boldsymbol{\\tau}$ by solving the bordered linear system with $\\tau_{\\text{Da}} = 1$ and normalize it.\n3. Predict $\\mathbf{u}_{\\text{pred}}$, and compute $\\|\\mathbf{F}(\\mathbf{u}_{\\text{pred}})\\|_2$.\n4. Correct to $\\mathbf{u}_{\\text{corr}}$ via Newton's method on $\\mathbf{G}$, and compute $\\|\\mathbf{F}(\\mathbf{u}_{\\text{corr}})\\|_2$ and $\\|\\mathbf{G}(\\mathbf{u}_{\\text{corr}})\\|_2$.\n\nFinally, we report, for each case, the seven-number list $[T_{\\text{pred}}, \\text{Da}_{\\text{pred}}, \\|\\mathbf{F}(\\mathbf{u}_{\\text{pred}})\\|_2, T_{\\text{corr}}, \\text{Da}_{\\text{corr}}, \\|\\mathbf{F}(\\mathbf{u}_{\\text{corr}})\\|_2, \\|\\mathbf{G}(\\mathbf{u}_{\\text{corr}})\\|_2]$, rounded to six decimals, as a single line containing three such lists in the order A, B, C. This procedure exercises the core principles of arc-length continuation for turning-point curves in computational combustion, demonstrating the robust tracking of steady solutions of a thermally sensitive reactor even when conventional parameter continuation fails near singular Jacobians.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import root\n\n# Model parameters (dimensionless)\ngamma = 3.0  # activation parameter in Frank-Kamenetskii approximation\nbeta = 2.0   # heat-release parameter\nh = 3.0      # heat-removal coefficient\n\ndef reaction_rate(T, c):\n    return c * np.exp(gamma * T)\n\ndef F_vec(T, c, Da):\n    r = reaction_rate(T, c)\n    F1 = (1.0 - c) - Da * r\n    F2 = (1.0 + h) * (1.0 - T) + beta * Da * r\n    return np.array([F1, F2])\n\ndef J_mat(T, c, Da):\n    r = reaction_rate(T, c)\n    dF1_dT = -Da * gamma * r\n    dF1_dc = -1.0 - Da * np.exp(gamma * T)\n    dF2_dT = -(1.0 + h) + beta * Da * gamma * r\n    dF2_dc = beta * Da * np.exp(gamma * T)\n    return np.array([[dF1_dT, dF1_dc],\n                     [dF2_dT, dF2_dc]])\n\ndef F_lambda(T, c, Da):\n    r = reaction_rate(T, c)\n    dF1_dDa = -r\n    dF2_dDa = beta * r\n    return np.array([dF1_dDa, dF2_dDa])\n\ndef solve_steady(Da, T_guess, c_guess):\n    # First attempt with provided guess\n    def fun(z):\n        return F_vec(z[0], z[1], Da)\n    z0 = np.array([T_guess, c_guess])\n    sol = root(fun, z0, method='hybr')\n    if sol.success and np.linalg.norm(sol.fun) < 1e-8:\n        return sol.x[0], sol.x[1], True\n    # Second attempt: heuristic guess based on species balance ignoring energy coupling\n    T_guess2 = 1.0\n    c_guess2 = 1.0 / (1.0 + Da * np.exp(gamma * T_guess2))\n    z0b = np.array([T_guess2, c_guess2])\n    sol2 = root(fun, z0b, method='hybr')\n    if sol2.success and np.linalg.norm(sol2.fun) < 1e-8:\n        return sol2.x[0], sol2.x[1], True\n    return sol2.x[0], sol2.x[1], False\n\ndef compute_tangent(T0, c0, Da0):\n    J = J_mat(T0, c0, Da0)\n    Fl = F_lambda(T0, c0, Da0)\n    # Set t_lambda = 1 and solve for (t_T, t_c): J * t_x = -F_lambda\n    try:\n        t_x = np.linalg.solve(J, -Fl)\n    except np.linalg.LinAlgError:\n        # Use least squares if singular\n        t_x, *_ = np.linalg.lstsq(J, -Fl, rcond=None)\n    tau_T, tau_c = t_x[0], t_x[1]\n    tau_Da = 1.0\n    tau = np.array([tau_T, tau_c, tau_Da])\n    # Normalize to unit Euclidean norm\n    norm_tau = np.linalg.norm(tau)\n    if norm_tau == 0.0:\n        # Fallback: choose parameter-only direction\n        tau = np.array([0.0, 0.0, 1.0])\n        norm_tau = 1.0\n    tau /= norm_tau\n    return tau\n\ndef predictor(u0, tau, ds):\n    u_pred = u0 + ds * tau\n    T_pred, c_pred, Da_pred = u_pred\n    F_norm = np.linalg.norm(F_vec(T_pred, c_pred, Da_pred))\n    return u_pred, F_norm\n\ndef corrector(u0, u_pred, tau, tol=1e-10, max_iter=25):\n    # Newton corrector for augmented system G = [F; g] = 0 with g = tau^T (u - u0) - ds\n    ds = np.dot(tau, u_pred - u0)  # nominal arc-length step used in constraint\n    u = u_pred.copy()\n    for k in range(max_iter):\n        T, c, Da = u\n        F = F_vec(T, c, Da)\n        g = np.dot(tau, u - u0) - ds\n        G = np.array([F[0], F[1], g])\n        G_norm = np.linalg.norm(G)\n        if G_norm < tol:\n            break\n        # Build Jacobian of G\n        J = J_mat(T, c, Da)\n        Fl = F_lambda(T, c, Da)\n        A = np.zeros((3, 3))\n        A[0, 0:2] = J[0, :]\n        A[1, 0:2] = J[1, :]\n        A[0, 2] = Fl[0]\n        A[1, 2] = Fl[1]\n        A[2, :] = tau\n        # Solve for Newton step\n        try:\n            step = np.linalg.solve(A, G)\n        except np.linalg.LinAlgError:\n            step, *_ = np.linalg.lstsq(A, G, rcond=None)\n        # Backtracking line search to reduce ||G||\n        alpha = 1.0\n        u_new = u - alpha * step\n        G_new = np.array([F_vec(u_new[0], u_new[1], u_new[2])[0],\n                          F_vec(u_new[0], u_new[1], u_new[2])[1],\n                          np.dot(tau, u_new - u0) - ds])\n        while np.linalg.norm(G_new) >= G_norm and alpha > 1e-6:\n            alpha *= 0.5\n            u_new = u - alpha * step\n            G_new = np.array([F_vec(u_new[0], u_new[1], u_new[2])[0],\n                              F_vec(u_new[0], u_new[1], u_new[2])[1],\n                              np.dot(tau, u_new - u0) - ds])\n        u = u_new\n    # Final norms\n    F_corr = F_vec(u[0], u[1], u[2])\n    g_corr = np.dot(tau, u - u0) - ds\n    F_norm = np.linalg.norm(F_corr)\n    G_norm = np.linalg.norm(np.array([F_corr[0], F_corr[1], g_corr]))\n    return u, F_norm, G_norm\n\ndef find_near_turning(Da_min, Da_max, N, T_start, c_start):\n    Das = np.linspace(Da_min, Da_max, N)\n    best = None\n    # Continuation-like scanning to improve robustness: use previous solution as next guess\n    T_guess, c_guess = T_start, c_start\n    prev_sol = None\n    for Da in Das:\n        T_sol, c_sol, ok = solve_steady(Da, T_guess, c_guess)\n        if not ok:\n            # If failed, reset guesses\n            T_guess, c_guess = 1.0, 1.0 / (1.0 + Da * np.exp(gamma * 1.0))\n            T_sol, c_sol, ok = solve_steady(Da, T_guess, c_guess)\n            if not ok:\n                continue\n        # Compute Jacobian singular values\n        J = J_mat(T_sol, c_sol, Da)\n        svals = np.linalg.svd(J, compute_uv=False)\n        smin = np.min(svals)\n        if (best is None) or (smin < best[0]):\n            best = (smin, Da, T_sol, c_sol)\n        # Update guesses for next Da\n        T_guess, c_guess = T_sol, c_sol\n        prev_sol = (T_sol, c_sol)\n    if best is None:\n        # Fallback: pick middle of range and solve\n        Da_mid = 0.5 * (Da_min + Da_max)\n        T_sol, c_sol, ok = solve_steady(Da_mid, T_start, c_start)\n        if not ok:\n            T_sol, c_sol, _ = solve_steady(Da_mid, 1.0, 1.0 / (1.0 + Da_mid * np.exp(gamma)))\n        best = (np.min(np.linalg.svd(J_mat(T_sol, c_sol, Da_mid), compute_uv=False)),\n                Da_mid, T_sol, c_sol)\n    _, Da_sel, T_sel, c_sel = best\n    return Da_sel, T_sel, c_sel\n\ndef run_case_direct(Da0, T_guess, c_guess, ds):\n    # Solve for operating point\n    T0, c0, ok = solve_steady(Da0, T_guess, c_guess)\n    # Compute tangent, predictor, corrector\n    tau = compute_tangent(T0, c0, Da0)\n    u0 = np.array([T0, c0, Da0])\n    u_pred, Fp_norm = predictor(u0, tau, ds)\n    u_corr, Fc_norm, Gc_norm = corrector(u0, u_pred, tau)\n    # Results: [T_pred, Da_pred, ||F_pred||, T_corr, Da_corr, ||F_corr||, ||G_corr||]\n    return [u_pred[0], u_pred[2], Fp_norm, u_corr[0], u_corr[2], Fc_norm, Gc_norm]\n\ndef run_case_scan(Da_min, Da_max, N, T_start, c_start, ds):\n    Da0, T0, c0 = find_near_turning(Da_min, Da_max, N, T_start, c_start)\n    tau = compute_tangent(T0, c0, Da0)\n    u0 = np.array([T0, c0, Da0])\n    u_pred, Fp_norm = predictor(u0, tau, ds)\n    u_corr, Fc_norm, Gc_norm = corrector(u0, u_pred, tau)\n    return [u_pred[0], u_pred[2], Fp_norm, u_corr[0], u_corr[2], Fc_norm, Gc_norm]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\"direct\", {\"Da0\": 0.05, \"T_guess\": 1.0, \"c_guess\": 0.9, \"ds\": 0.02}),\n        (\"scan\",   {\"Da_min\": 0.01, \"Da_max\": 0.5, \"N\": 200, \"T_start\": 1.0, \"c_start\": 0.8, \"ds\": 0.01}),\n        (\"direct\", {\"Da0\": 0.20, \"T_guess\": 1.1, \"c_guess\": 0.5, \"ds\": 1e-4}),\n    ]\n\n    results = []\n    for kind, params in test_cases:\n        if kind == \"direct\":\n            res = run_case_direct(params[\"Da0\"], params[\"T_guess\"], params[\"c_guess\"], params[\"ds\"])\n        else:\n            res = run_case_scan(params[\"Da_min\"], params[\"Da_max\"], params[\"N\"],\n                                params[\"T_start\"], params[\"c_start\"], params[\"ds\"])\n        # Round each float to six decimals\n        res_rounded = [float(f\"{x:.6f}\") for x in res]\n        results.append(res_rounded)\n\n    # Final print statement in the exact required format.\n    # Single line: list of lists with comma-separated floats rounded to six decimals\n    def format_list(lst):\n        return \"[\" + \",\".join(f\"{x:.6f}\" for x in lst) + \"]\"\n    print(\"[\" + \",\".join(format_list(r) for r in results) + \"]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once you can trace a solution curve, the next step is to use this capability to perform targeted analysis, such as precisely locating bifurcation points. This exercise  introduces a powerful two-stage strategy for finding folds: first, use arc-length continuation to approach the turning point, and second, switch to a specialized Newton solver to refine its location with high precision. You will implement diagnostics that monitor the Jacobian's singularity along the path, providing a trigger to initiate the refinement procedure, a workflow that is central to practical bifurcation analysis.",
            "id": "4007925",
            "problem": "Consider a simplified, dimensionless model of a homogeneous reactor relevant to computational combustion, where the steady-state heat balance under the Frank–Kamenetskii approximation leads to the nonlinear residual equation\n$$F(u,\\lambda) = \\lambda e^{u} - u,$$\nwith unknown scalar $u \\in \\mathbb{R}$ representing the dimensionless temperature rise and scalar parameter $\\lambda \\in \\mathbb{R}$ representing a nondimensionalized ratio of heat generation to heat loss.\n\nA turning-point (fold) in the solution curve of $F(u,\\lambda)=0$ satisfies the system\n$$F(u,\\lambda) = 0,\\quad \\frac{\\partial F}{\\partial u}(u,\\lambda) = 0.$$\nArc-length continuation is used to follow solution branches through such folds. The continuation method is based on the fundamental Implicit Function Theorem and the construction of a tangent direction $[t_u, t_\\lambda]^T$ satisfying the orthogonality condition\n$$\\frac{\\partial F}{\\partial u}(u,\\lambda)\\, t_u + \\frac{\\partial F}{\\partial \\lambda}(u,\\lambda)\\, t_\\lambda = 0,$$\nwith normalization $\\sqrt{t_u^2 + t_\\lambda^2} = 1$, followed by a predictor and corrector step enforcing a linearized arc-length constraint.\n\nDevelop a diagnostic mechanism that, along the continuation branch, tracks:\n- the smallest singular value of the Jacobian with respect to $u$, namely the absolute value\n$$\\sigma_{\\min} = \\left|\\frac{\\partial F}{\\partial u}(u,\\lambda)\\right|,$$\nand\n- the scalar quantity\n$$d = w^T \\frac{\\partial F}{\\partial \\lambda}(u,\\lambda),$$\nwhere $w$ is a unit left singular vector associated with the smallest singular value of $\\frac{\\partial F}{\\partial u}(u,\\lambda)$. In the scalar case, this reduces to $w = 1$ and $d = \\frac{\\partial F}{\\partial \\lambda}(u,\\lambda)$.\n\nWhen the branch approaches a fold, $\\sigma_{\\min}$ becomes small. To ensure the fold is simple (nondegenerate), require $|d|$ to remain bounded away from zero. Your task is to implement:\n1. A pseudo arc-length continuation for the scalar residual $F(u,\\lambda)$, starting from a feasible initial solution, using predictor-corrector steps based on the tangent direction and a Newton corrector that enforces both $F(u,\\lambda)=0$ and the linearized arc-length condition.\n2. A diagnostic that triggers refined fold localization when $\\sigma_{\\min}$ falls below a threshold $\\sigma_{\\mathrm{th}}$ and simultaneously $|d| \\ge \\gamma_{\\min}$.\n3. A refinement that solves the fold system\n$$G(u,\\lambda) = \\begin{bmatrix}F(u,\\lambda)\\\\ \\frac{\\partial F}{\\partial u}(u,\\lambda)\\end{bmatrix} = \\begin{bmatrix}\\lambda e^{u} - u\\\\ \\lambda e^{u} - 1\\end{bmatrix} = 0$$\nusing Newton’s method to accurately locate the fold.\n\nUse the following base facts and definitions:\n- The Jacobian entries are $\\frac{\\partial F}{\\partial u}(u,\\lambda) = \\lambda e^{u} - 1$ and $\\frac{\\partial F}{\\partial \\lambda}(u,\\lambda) = e^{u}$.\n- For the arc-length predictor, one valid unit tangent is\n$$\\begin{bmatrix}t_u\\\\ t_\\lambda\\end{bmatrix} = \\frac{1}{\\sqrt{\\left(\\frac{\\partial F}{\\partial u}\\right)^2 + \\left(\\frac{\\partial F}{\\partial \\lambda}\\right)^2}} \\begin{bmatrix} -\\frac{\\partial F}{\\partial \\lambda} \\\\ \\frac{\\partial F}{\\partial u} \\end{bmatrix},$$\nwhich satisfies the orthogonality condition to the gradient of $F$.\n\nYour program must:\n- Initialize at $\\lambda_0 = 0.05$, with an initial guess $u_{\\mathrm{init}} = 0.05$, and solve $F(u,\\lambda_0)=0$ for $u$ using Newton’s method to obtain the starting point on the branch.\n- Perform pseudo arc-length continuation with a fixed step size $\\Delta s$ and a maximum number of steps $N$; at each step, use a predictor based on the tangent and a Newton corrector enforcing both $F(u,\\lambda)=0$ and the linearized arc-length constraint $(u-u_p) t_u + (\\lambda - \\lambda_p) t_\\lambda = 0$, where $(u_p,\\lambda_p)$ is the predictor.\n- At each accepted corrected point, compute $\\sigma_{\\min} = \\left|\\frac{\\partial F}{\\partial u}(u,\\lambda)\\right|$ and $d = w^T \\frac{\\partial F}{\\partial \\lambda}(u,\\lambda)$ with $w=1$ for this scalar case, and trigger fold refinement if $\\sigma_{\\min} \\le \\sigma_{\\mathrm{th}}$ and $|d| \\ge \\gamma_{\\min}$.\n- Upon trigger, solve $G(u,\\lambda)=0$ via Newton’s method starting from the current point to compute a refined estimate of the fold parameter $\\lambda^\\star$.\n\nTest Suite:\nRun the algorithm for the following parameter sets $(\\Delta s, N, \\sigma_{\\mathrm{th}}, \\gamma_{\\min})$:\n1. $(0.05, 60, 10^{-4}, 10^{-6})$.\n2. $(0.2, 30, 10^{-3}, 10^{-6})$.\n3. $(0.05, 60, 10^{-6}, 3.0)$.\n\nFor each case, return:\n- The refined fold parameter $\\lambda^\\star$ as a float if refinement is triggered.\n- Otherwise, return the float $-1.0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"). No physical units are involved because all variables are dimensionless. Angles are not used in this problem. Express all numerical results as floating-point numbers.",
            "solution": "The user has provided a valid problem statement from the field of numerical analysis, specifically concerning the application of pseudo arc-length continuation to locate a turning point in a nonlinear system derived from a model in computational combustion. The problem is scientifically sound, mathematically well-posed, and all necessary components for a numerical solution are provided. The analysis will proceed by first detailing the mathematical and algorithmic framework, followed by the implementation.\n\nThe core of the problem is the nonlinear residual equation:\n$$F(u, \\lambda) = \\lambda e^u - u = 0$$\nwhere $u$ is a dimensionless temperature and $\\lambda$ is a system parameter. We aim to trace the solution curve $(u(\\lambda), \\lambda)$ and accurately locate the turning point (fold), which is defined by the simultaneous satisfaction of two conditions:\n$$F(u,\\lambda) = 0 \\quad \\text{and} \\quad \\frac{\\partial F}{\\partial u}(u, \\lambda) = 0$$\n\nThe numerical procedure is structured into three main stages: initialization, arc-length continuation, and fold refinement.\n\n**1. Initialization: Finding the Starting Point**\nThe continuation process begins from a known point on the solution curve. We are given an initial parameter value $\\lambda_0 = 0.05$ and an initial guess for the state, $u_{\\text{init}} = 0.05$. The first step is to solve the nonlinear equation $F(u, \\lambda_0) = 0$ for $u$ to find a precise starting point $(u_0, \\lambda_0)$. This is accomplished using Newton's method for a single variable. The iterative formula is:\n$$u^{(k+1)} = u^{(k)} - \\frac{F(u^{(k)}, \\lambda_0)}{\\frac{\\partial F}{\\partial u}(u^{(k)}, \\lambda_0)}$$\nwhere $\\frac{\\partial F}{\\partial u}(u, \\lambda) = \\lambda e^u - 1$. The iteration starts with $u^{(0)} = u_{\\text{init}}$ and continues until the change $|u^{(k+1)} - u^{(k)}|$ is below a prescribed tolerance. The converged solution is denoted $u_0$.\n\n**2. Pseudo Arc-Length Continuation**\nStarting from $(u_0, \\lambda_0)$, we trace the solution curve by taking discrete steps of length $\\Delta s$. Each step consists of a predictor and a corrector.\n\n**2.1. Tangent Vector Calculation**\nAt a known point $(u_k, \\lambda_k)$ on the curve, we compute the tangent vector $[t_u, t_\\lambda]^T$. The tangent is orthogonal to the gradient of $F$, $\\nabla F = [\\frac{\\partial F}{\\partial u}, \\frac{\\partial F}{\\partial \\lambda}]^T$. A valid, normalized tangent vector is given by:\n$$\\begin{bmatrix}t_u\\\\ t_\\lambda\\end{bmatrix} = \\frac{1}{\\sqrt{\\left(\\frac{\\partial F}{\\partial u}\\right)^2 + \\left(\\frac{\\partial F}{\\partial \\lambda}\\right)^2}} \\begin{bmatrix} -\\frac{\\partial F}{\\partial \\lambda} \\\\ \\frac{\\partial F}{\\partial u} \\end{bmatrix}$$\nwhere $\\frac{\\partial F}{\\partial u} = \\lambda_k e^{u_k} - 1$ and $\\frac{\\partial F}{\\partial \\lambda} = e^{u_k}$.\nTo ensure the continuation proceeds consistently along the curve, the tangent's orientation must be managed. For the first step from $(u_0, \\lambda_0)$, we require the continuation to move towards increasing $\\lambda$, as the fold is known to occur at $\\lambda^\\star = 1/e \\approx 0.36788 > \\lambda_0$. We thus check the sign of $t_\\lambda$; if $t_\\lambda < 0$, we flip the sign of the entire tangent vector. For all subsequent steps, we ensure the new tangent vector $[t_u, t_\\lambda]_k$ has a positive dot product with the previous tangent vector $[t_u, t_\\lambda]_{k-1}$, flipping its sign if necessary.\n\n**2.2. Predictor Step**\nThe predictor step computes a new point $(u_p, \\lambda_p)$ by moving a distance $\\Delta s$ from the current point $(u_k, \\lambda_k)$ along the tangent direction:\n$$u_p = u_k + \\Delta s \\cdot t_u$$\n$$\\lambda_p = \\lambda_k + \\Delta s \\cdot t_\\lambda$$\n\n**2.3. Corrector Step**\nThe predictor point $(u_p, \\lambda_p)$ is an approximation and does not lie exactly on the solution curve. The corrector step refines this guess to find the next point on the curve, $(u_{k+1}, \\lambda_{k+1})$. This is achieved by solving a $2 \\times 2$ system of nonlinear equations for $(u, \\lambda)$ using Newton's method:\n$$ H(u, \\lambda) = \\begin{bmatrix} F(u,\\lambda) \\\\ N(u,\\lambda) \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} $$\nThe first equation, $F(u, \\lambda) = 0$, ensures the point is on the solution curve. The second equation, $N(u, \\lambda) = (u-u_p) t_u + (\\lambda - \\lambda_p) t_\\lambda = 0$, is the linearized arc-length constraint. It forces the solution to lie on the hyperplane (a line in this 2D case) passing through $(u_p, \\lambda_p)$ and orthogonal to the tangent vector used for the predictor step.\n\nThe Newton iteration for this system starts with the guess $(u_p, \\lambda_p)$ and updates the solution vector $x = [u, \\lambda]^T$ via:\n$$x^{(j+1)} = x^{(j)} - [J_H(x^{(j)})]^{-1} H(x^{(j)})$$\nThe Jacobian of the system $H$ is:\n$$ J_H(u, \\lambda) = \\begin{bmatrix} \\frac{\\partial F}{\\partial u} & \\frac{\\partial F}{\\partial \\lambda} \\\\ t_u & t_\\lambda \\end{bmatrix} = \\begin{bmatrix} \\lambda e^u - 1 & e^u \\\\ t_u & t_\\lambda \\end{bmatrix} $$\nThe iteration continues until the correction becomes smaller than a given tolerance. The converged point is $(u_{k+1}, \\lambda_{k+1})$.\n\n**3. Fold Detection and Refinement**\nAt each successfully computed point $(u_k, \\lambda_k)$, we evaluate diagnostic quantities to detect proximity to the fold.\n\n**3.1. Diagnostic Quantities**\nThe problem specifies tracking two quantities:\n- The smallest singular value of the Jacobian $\\frac{\\partial F}{\\partial u}$, which for this scalar case is $\\sigma_{\\min} = \\left|\\frac{\\partial F}{\\partial u}(u_k,\\lambda_k)\\right| = |\\lambda_k e^{u_k} - 1|$. As the curve approaches a fold, $\\frac{\\partial F}{\\partial u} \\to 0$, so $\\sigma_{\\min} \\to 0$.\n- A scalar $d = w^T \\frac{\\partial F}{\\partial \\lambda}$, where $w$ is the left singular vector corresponding to $\\sigma_{\\min}$. For this scalar problem, $w=1$, so $d = \\frac{\\partial F}{\\partial \\lambda}(u_k,\\lambda_k) = e^{u_k}$. For the fold to be simple (non-degenerate), this value must be bounded away from zero.\n\n**3.2. Refinement Trigger**\nFold refinement is triggered if both of the following conditions are met for a given point $(u_k, \\lambda_k)$:\n$$ \\sigma_{\\min} \\le \\sigma_{\\mathrm{th}} \\quad \\text{and} \\quad |d| \\ge \\gamma_{\\min} $$\nwhere $\\sigma_{\\mathrm{th}}$ and $\\gamma_{\\min}$ are specified thresholds.\n\n**3.3. Fold Refinement**\nUpon triggering, we switch to a more direct method to pinpoint the fold's location $(u^\\star, \\lambda^\\star)$. This involves solving the defining system for the fold using Newton's method:\n$$ G(u, \\lambda) = \\begin{bmatrix} F(u,\\lambda) \\\\ \\frac{\\partial F}{\\partial u}(u,\\lambda) \\end{bmatrix} = \\begin{bmatrix} \\lambda e^u - u \\\\ \\lambda e^u - 1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} $$\nThe initial guess for this Newton solve is the point $(u_k, \\lambda_k)$ that triggered the refinement. The Jacobian of the system $G$ is:\n$$ J_G(u, \\lambda) = \\begin{bmatrix} \\frac{\\partial F}{\\partial u} & \\frac{\\partial F}{\\partial \\lambda} \\\\ \\frac{\\partial^2 F}{\\partial u^2} & \\frac{\\partial^2 F}{\\partial u \\partial \\lambda} \\end{bmatrix} = \\begin{bmatrix} \\lambda e^u - 1 & e^u \\\\ \\lambda e^u & e^u \\end{bmatrix} $$\nThe Newton iteration proceeds until convergence, and the $\\lambda$ component of the solution is the refined fold parameter, $\\lambda^\\star$. If the continuation loop completes its maximum of $N$ steps without triggering refinement, a value of $-1.0$ is returned to indicate failure to locate the fold. Analytically, the fold is at $(u, \\lambda) = (1, 1/e)$, so the refined value $\\lambda^\\star$ should be close to $1/e \\approx 0.36787944$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    test_cases = [\n        (0.05, 60, 1e-4, 1e-6),\n        (0.2, 30, 1e-3, 1e-6),\n        (0.05, 60, 1e-6, 3.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        ds, N, sigma_th, gamma_min = case\n        lambda_fold = run_case(ds, N, sigma_th, gamma_min)\n        results.append(lambda_fold)\n\n    print(f\"[{','.join(f'{r:.7f}' for r in results)}]\")\n\n\ndef run_case(ds, N, sigma_th, gamma_min, u_init=0.05, lambda_init=0.05, tol=1e-9, max_iter=10):\n    \"\"\"\n    Executes the arc-length continuation and fold refinement for a single parameter set.\n    \"\"\"\n    # Helper functions for the residual and its derivatives\n    F = lambda u, lam: lam * np.exp(u) - u\n    F_u = lambda u, lam: lam * np.exp(u) - 1.0\n    F_l = lambda u, lam: np.exp(u)\n\n    # --- Step 1: Find the initial point (u0, lambda0) ---\n    u_curr = u_init\n    for _ in range(max_iter):\n        residual = F(u_curr, lambda_init)\n        if abs(residual) < tol:\n            break\n        u_curr -= residual / F_u(u_curr, lambda_init)\n    \n    lambda_curr = lambda_init\n    \n    # --- Step 2: Arc-length continuation ---\n    tangent_old = None\n    for step in range(N):\n        # --- Tangent calculation ---\n        fu = F_u(u_curr, lambda_curr)\n        fl = F_l(u_curr, lambda_curr)\n        norm = np.sqrt(fu**2 + fl**2)\n        \n        # Guard against division by zero if we land exactly on the fold\n        if norm < tol:\n            break\n            \n        t_u = -fl / norm\n        t_l = fu / norm\n        tangent = np.array([t_u, t_l])\n\n        # --- Tangent orientation ---\n        # For the first step, ensure we move towards increasing lambda\n        if step == 0 and tangent[1] < 0:\n            tangent *= -1.0\n        # For subsequent steps, maintain direction\n        if tangent_old is not None and np.dot(tangent, tangent_old) < 0:\n            tangent *= -1.0\n        \n        tangent_old = tangent\n\n        # --- Predictor step ---\n        u_p = u_curr + ds * tangent[0]\n        lambda_p = lambda_curr + ds * tangent[1]\n\n        # --- Corrector step (Newton's method for 2x2 system) ---\n        u_corr, lambda_corr = u_p, lambda_p\n        for _ in range(max_iter):\n            # Form the residual H and Jacobian J_H\n            h1 = F(u_corr, lambda_corr)\n            h2 = (u_corr - u_p) * tangent[0] + (lambda_corr - lambda_p) * tangent[1]\n            H = np.array([h1, h2])\n\n            jh_11 = F_u(u_corr, lambda_corr)\n            jh_12 = F_l(u_corr, lambda_corr)\n            jh_21 = tangent[0]\n            jh_22 = tangent[1]\n            J_H = np.array([[jh_11, jh_12], [jh_21, jh_22]])\n\n            # Solve J_H * delta = -H\n            try:\n                delta = np.linalg.solve(J_H, -H)\n            except np.linalg.LinAlgError:\n                # Corrector fails, stop continuation\n                return -1.0\n                \n            u_corr += delta[0]\n            lambda_corr += delta[1]\n\n            if np.linalg.norm(delta) < tol:\n                break\n        \n        u_curr, lambda_curr = u_corr, lambda_corr\n\n        # --- Step 3: Fold detection ---\n        sigma_min = abs(F_u(u_curr, lambda_curr))\n        d_val = F_l(u_curr, lambda_curr)\n\n        if sigma_min <= sigma_th and abs(d_val) >= gamma_min:\n            # --- Fold refinement ---\n            u_fold, lambda_fold = u_curr, lambda_curr\n            for _ in range(max_iter):\n                # Form the residual G and Jacobian J_G\n                g1 = F(u_fold, lambda_fold)\n                g2 = F_u(u_fold, lambda_fold)\n                G = np.array([g1, g2])\n\n                # Jacobian of the fold system G\n                jg_11 = F_u(u_fold, lambda_fold)\n                jg_12 = F_l(u_fold, lambda_fold)\n                jg_21 = lambda_fold * np.exp(u_fold) # d(F_u)/du\n                jg_22 = np.exp(u_fold)              # d(F_u)/d_lambda\n                J_G = np.array([[jg_11, jg_12], [jg_21, jg_22]])\n\n                try:\n                    delta_fold = np.linalg.solve(J_G, -G)\n                except np.linalg.LinAlgError:\n                    return -1.0\n\n                u_fold += delta_fold[0]\n                lambda_fold += delta_fold[1]\n\n                if np.linalg.norm(delta_fold) < tol:\n                    return lambda_fold\n            \n            # If refinement Newton loop finishes without converging, return last value\n            return lambda_fold\n\n    # If loop finishes without triggering refinement\n    return -1.0\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "A practical continuation solver must be both robust and efficient, especially when dealing with the sharp turns characteristic of ignition and extinction phenomena. This requires an adaptive algorithm that can adjust its step size, taking small steps in regions of high curvature and larger ones on flatter parts of the curve. This advanced practice  guides you in implementing a proportional-integral (PI) controller to automate step-size selection, a sophisticated technique that targets a constant number of corrector iterations to balance progress and stability. Successfully completing this exercise will equip you with the skills to build a truly autonomous and powerful tool for exploring complex nonlinear systems.",
            "id": "4007914",
            "problem": "Consider a perfectly stirred reacting system (continuous stirred-tank reactor) with an irreversible, first-order, exothermic reaction under constant density and heat capacity approximations. In a nondimensional formulation, the steady-state balances for dimensionless concentration $x$ and dimensionless temperature $y$ at a given Damköhler number $\\lambda$ are modeled by the system of nonlinear algebraic equations\n$$\nF(x,y,\\lambda) =\n\\begin{bmatrix}\n(1 - x) - \\lambda\\,x\\,\\exp(\\gamma\\,y) \\\\\n(1 - y) + \\beta\\,\\lambda\\,x\\,\\exp(\\gamma\\,y) - h\\,(y - y_c)\n\\end{bmatrix} = \\mathbf{0},\n$$\nwhere $\\beta$ is the dimensionless heat-release parameter, $\\gamma$ is a dimensionless activation parameter arising from an Arrhenius dependence, $h$ is a dimensionless heat-loss coefficient, and $y_c$ is the dimensionless coolant temperature. This model, for sufficiently large $\\beta$ and $\\gamma$ and moderate $h$, produces an $S$-shaped equilibrium manifold in $(x,y,\\lambda)$ with turning points (folds) associated with ignition and extinction. Your task is to implement a robust pseudo-arclength continuation algorithm to trace this turning-point curve, and to regulate the arclength step $\\Delta s$ using a proportional-integral controller to target a fixed number of corrector Newton iterations.\n\nStarting from an initial steady state at $\\lambda_0$ found by Newton’s method, use a predictor-corrector pseudo-arclength continuation with the standard hyperplane constraint to advance along the solution curve in $(x,y,\\lambda)$. At each continuation step, perform:\n- A tangent-based predictor using the unit tangent $\\mathbf{t} = \\left[\\dfrac{dx}{ds}, \\dfrac{dy}{ds}, \\dfrac{d\\lambda}{ds}\\right]^T$.\n- A Newton corrector on the augmented system comprising the balances $F(x,y,\\lambda)=\\mathbf{0}$ and the hyperplane constraint defined by the previous tangent.\n\nRegulate the step size $\\Delta s$ with a proportional-integral law to target a fixed number of corrector Newton iterations $n_\\mathrm{targ}$. Let $e_k = n_k - n_\\mathrm{targ}$ be the iteration-count error on accepted step $k$, and let $E_k$ be the running integral of $e_k$. Update the step size multiplicatively as $\\Delta s \\leftarrow \\Delta s \\,\\exp(-k_p\\,e_k - k_i\\,E_k)$ with gains $k_p$ and $k_i$, and bound $\\Delta s$ within $[\\Delta s_{\\min}, \\Delta s_{\\max}]$. If a corrector fails to converge within $n_{\\max}$ iterations or violates a monotone residual decrease safeguard, reject the step, reduce $\\Delta s \\leftarrow r_\\mathrm{fail}\\,\\Delta s$, and retry from the last accepted state, terminating the run early if $\\Delta s < \\Delta s_{\\min}$.\n\nNumerical requirements:\n- Implement analytic Jacobians with respect to $(x,y)$ and $\\lambda$ for $F(x,y,\\lambda)$, and solve the bordered linear systems with a direct linear solver.\n- Use a line-search damping on the corrector update to ensure nonincreasing augmented residual norm, with backtracking by a factor of $1/2$ up to a finite number of trials before declaring failure.\n- Normalize the predictor tangent to unit arclength and maintain consistent orientation by flipping its sign to align with the secant between successive accepted points.\n\nTest suite:\nImplement a single program that executes three cases with the following parameters and reports quantitative metrics. All quantities are dimensionless.\n\nCommon numerical settings across all cases:\n- Initial Damköhler number $\\lambda_0 = 0.01$.\n- Initial guess for the initial steady state $(x,y) = (1,0)$.\n- Newton tolerance for balances $\\|F\\|_2 \\le 10^{-10}$ and for the hyperplane residual $\\le 10^{-12}$.\n- Maximum corrector Newton iterations $n_{\\max} = 20$.\n- Damping backtracking limit $n_{\\mathrm{ls}} = 10$.\n- Initial step $\\Delta s_0 = 0.02$, bounds $\\Delta s_{\\min} = 10^{-4}$ and $\\Delta s_{\\max} = 0.2$.\n- Failure reduction factor $r_{\\mathrm{fail}} = 0.5$.\n- Number of attempted continuation acceptances $N_{\\mathrm{acc}} = 40$ (terminate early if $\\Delta s < \\Delta s_{\\min}$).\n\nCase A (baseline smooth S-curve):\n- Parameters $(\\beta,\\gamma,h,y_c) = (8,6,1,0)$.\n- Controller target $n_\\mathrm{targ} = 4$, gains $(k_p,k_i) = (0.25,0.05)$.\n- No parameter changes during the run.\n- Required output metric $m_A$: the maximum absolute deviation of accepted-step corrector iterations from target,\n$$\nm_A = \\max_{1 \\le k \\le K} \\left| n_k - n_\\mathrm{targ} \\right|,\n$$\nwhere $K$ is the number of accepted steps.\n\nCase B (sudden curvature increase):\n- Start with $(\\beta,\\gamma,h,y_c) = (8,6,1,0)$.\n- Controller target $n_\\mathrm{targ} = 4$, gains $(k_p,k_i) = (0.25,0.05)$.\n- After exactly $k_{\\mathrm{sw}} = 12$ accepted steps, instantaneously change the activation parameter to $\\gamma \\leftarrow 9$ and keep it fixed thereafter; all other parameters unchanged.\n- Required output metric $m_B$: the maximum positive overshoot of iterations above target over the next $K_{\\mathrm{win}} = 8$ accepted steps after the change,\n$$\nm_B = \\max_{k_{\\mathrm{sw}} < k \\le \\min(k_{\\mathrm{sw}}+K_{\\mathrm{win}},K)} \\max\\!\\left(0,\\, n_k - n_\\mathrm{targ} \\right).\n$$\n\nCase C (sharper folds, stricter regulation stress-test):\n- Parameters $(\\beta,\\gamma,h,y_c) = (10,9,0.5,0)$.\n- Controller target $n_\\mathrm{targ} = 3$, gains $(k_p,k_i) = (0.2,0.04)$.\n- No parameter changes during the run.\n- Required output metric $m_C$: the total number of rejected steps due to corrector nonconvergence prior to acceptance or early termination.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[m_A,m_B,m_C]$. Each metric must be printed as a Python literal number, with $m_A$ and $m_B$ as floating-point numbers and $m_C$ as an integer. No other text should be printed.",
            "solution": "The user-provided problem is valid as it constitutes a well-posed, scientifically grounded, and rigorously specified numerical task. It describes the application of a pseudo-arclength continuation method to trace the equilibrium manifold of a classical model for a continuous stirred-tank reactor (CSTR), a standard problem in chemical engineering and bifurcation analysis. All necessary parameters, initial conditions, and numerical specifications are provided without ambiguity or contradiction.\n\nOur objective is to trace the solution curve $\\mathbf{z}(s) = [x(s), y(s), \\lambda(s)]^T$ of the system of nonlinear algebraic equations $F(\\mathbf{z}) = \\mathbf{0}$, where $s$ is the arclength. The system is defined as:\n$$\nF(x,y,\\lambda) =\n\\begin{bmatrix}\nF_1 \\\\\nF_2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n(1 - x) - \\lambda\\,x\\,\\exp(\\gamma\\,y) \\\\\n(1 - y) + \\beta\\,\\lambda\\,x\\,\\exp(\\gamma\\,y) - h\\,(y - y_c)\n\\end{bmatrix} = \\mathbf{0}\n$$\nStandard parameter continuation, where $\\lambda$ is the independent parameter, fails at turning points where the solution branch folds back. At these points, the Jacobian of the system with respect to the state variables $(x, y)$, denoted $J_{F,\\mathbf{v}}$, becomes singular, and the solution is no longer a single-valued function of $\\lambda$. Pseudo-arclength continuation resolves this by parameterizing the curve by arclength $s$ and treating $\\lambda$ as a dependent variable, part of the state vector $\\mathbf{z} = [x, y, \\lambda]^T$. The algorithm proceeds via a sequence of predictor-corrector steps.\n\nFirst, we must find a starting point on the solution curve. For a given initial Damköhler number $\\lambda_0 = 0.01$, we solve the system $F(x, y, \\lambda_0) = \\mathbf{0}$ for $(x_0, y_0)$ using a standard Newton-Raphson method with line-search damping, starting from the guess $(x,y)=(1,0)$. This yields the initial state $\\mathbf{z}_0 = [x_0, y_0, \\lambda_0]^T$.\n\nThe core of the continuation algorithm is an iterative loop. At each accepted step $k$ (at state $\\mathbf{z}_k$), we perform the following:\n\n**1. Predictor Step**\nThe predictor step extrapolates from the current point $\\mathbf{z}_k$ along the curve's tangent direction $\\mathbf{t}_k$ to find a trial point $\\mathbf{z}_{k+1}^p$.\n$$\n\\mathbf{z}_{k+1}^p = \\mathbf{z}_k + \\Delta s_k \\, \\mathbf{t}_k\n$$\nwhere $\\Delta s_k$ is the current step size in arclength. The tangent vector $\\mathbf{t}_k$ is a unit vector spanning the one-dimensional null space of the full Jacobian $J_{F,\\mathbf{z}} = \\left[ \\frac{\\partial F}{\\partial x} \\quad \\frac{\\partial F}{\\partial y} \\quad \\frac{\\partial F}{\\partial \\lambda} \\right]$ evaluated at $\\mathbf{z}_k$.\n$$\nJ_{F,\\mathbf{z}}(\\mathbf{z}_k) \\, \\mathbf{t}_k = \\mathbf{0}, \\quad \\|\\mathbf{t}_k\\|_2 = 1\n$$\nThis vector can be robustly computed using a singular value decomposition (SVD) of $J_{F,\\mathbf{z}}$, for which `scipy.linalg.null_space` provides a convenient implementation. The tangent vector's sign is arbitrary; to ensure consistent traversal of the curve, we enforce an orientation. For the first step ($k=0$), we orient $\\mathbf{t}_0$ such that its $\\lambda$-component is positive ($t_\\lambda > 0$). For all subsequent steps ($k > 0$), we orient $\\mathbf{t}_k$ to have a positive projection onto the secant vector from the previous step, i.e., $\\mathbf{t}_k \\cdot (\\mathbf{z}_k - \\mathbf{z}_{k-1}) > 0$.\n\n**2. Corrector Step**\nThe predicted point $\\mathbf{z}_{k+1}^p$ lies off the true solution manifold. The corrector step finds a nearby point $\\mathbf{z}_{k+1}$ that satisfies the governing equations. This is achieved by solving an augmented system of equations using Newton's method. The system includes the original balance equations and a constraint that forces the corrected point to lie on a hyperplane orthogonal to the tangent vector $\\mathbf{t}_k$ and passing through the predicted point $\\mathbf{z}_{k+1}^p$. The augmented system $G(\\mathbf{z}) = \\mathbf{0}$ is:\n$$\nG(\\mathbf{z}) =\n\\begin{bmatrix}\nF(x,y,\\lambda) \\\\\nN(\\mathbf{z})\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n(1 - x) - \\lambda\\,x\\,\\exp(\\gamma\\,y) \\\\\n(1 - y) + \\beta\\,\\lambda\\,x\\,\\exp(\\gamma\\,y) - h\\,(y - y_c) \\\\\n\\mathbf{t}_k \\cdot (\\mathbf{z} - \\mathbf{z}_{k+1}^p)\n\\end{bmatrix}\n= \\mathbf{0}\n$$\nThis $(3 \\times 3)$ system is solved iteratively for $\\mathbf{z}$, starting from the guess $\\mathbf{z}^{(0)} = \\mathbf{z}_{k+1}^p$. The Jacobian of the augmented system, $J_G$, is a bordered matrix:\n$$\nJ_G =\n\\begin{bmatrix}\nJ_{F,\\mathbf{z}} \\\\\n\\mathbf{t}_k^T\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\frac{\\partial F_1}{\\partial x} & \\frac{\\partial F_1}{\\partial y} & \\frac{\\partial F_1}{\\partial \\lambda} \\\\\n\\frac{\\partial F_2}{\\partial x} & \\frac{\\partial F_2}{\\partial y} & \\frac{\\partial F_2}{\\partial \\lambda} \\\\\nt_x & t_y & t_\\lambda\n\\end{bmatrix}\n$$\nThe Newton update $\\delta \\mathbf{z}^{(j)}$ is found by solving the linear system $J_G(\\mathbf{z}^{(j)}) \\, \\delta \\mathbf{z}^{(j)} = -G(\\mathbf{z}^{(j)})$. To ensure robust convergence, the update is damped with a line search: $\\mathbf{z}^{(j+1)} = \\mathbf{z}^{(j)} + \\alpha \\, \\delta \\mathbf{z}^{(j)}$, where the damping factor $\\alpha \\in (0, 1]$ is chosen by backtracking to ensure a decrease in the norm of the augmented residual, $\\|G(\\mathbf{z}^{(j+1)})\\|_2 < \\|G(\\mathbf{z}^{(j)})\\|_2$. The corrector successfully converges if $\\|F\\|_2$ and $|N|$ fall below their respective tolerances within $n_{\\max}$ iterations.\n\nThe required analytic derivatives for the Jacobian $J_{F,\\mathbf{z}}$ are:\nLetting $R = \\lambda x \\exp(\\gamma y)$, we have:\n$\\frac{\\partial F_1}{\\partial x} = -1 - \\lambda \\exp(\\gamma y)$, $\\frac{\\partial F_1}{\\partial y} = -\\gamma R$, $\\frac{\\partial F_1}{\\partial \\lambda} = -x \\exp(\\gamma y)$\n$\\frac{\\partial F_2}{\\partial x} = \\beta \\lambda \\exp(\\gamma y)$, $\\frac{\\partial F_2}{\\partial y} = -1 + \\beta \\gamma R - h$, $\\frac{\\partial F_2}{\\partial \\lambda} = \\beta x \\exp(\\gamma y)$\n\n**3. Step-Size Control and Failure Handling**\nTo efficiently navigate the solution curve, passing through regions of both high and low curvature, the arclength step size $\\Delta s$ is adapted. A proportional-integral (PI) controller adjusts $\\Delta s$ to maintain a target number of corrector iterations, $n_\\mathrm{targ}$. After a step $k$ is successfully accepted with $n_k$ iterations, the error $e_k = n_k - n_\\mathrm{targ}$ and its running integral $E_k = E_{k-1} + e_k$ are calculated. The next step size is updated multiplicatively:\n$$\n\\Delta s_{k+1} = \\Delta s_k \\exp(-k_p e_k - k_i E_k)\n$$\nThe result is then clamped to the interval $[\\Delta s_{\\min}, \\Delta s_{\\max}]$. If the corrector fails to converge, the step is rejected, the step size is reduced by a factor $r_{\\mathrm{fail}}$, and the step is retried from the last accepted point $\\mathbf{z}_k$. If $\\Delta s$ falls below $\\Delta s_{\\min}$, the continuation is terminated early. This adaptive scheme provides a balance between computational efficiency and robustness.",
            "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main function to execute the three test cases and print the results.\n    \"\"\"\n\n    common_settings = {\n        'lambda0': 0.01,\n        'initial_guess': np.array([1.0, 0.0]),\n        'tol_F': 1e-10,\n        'tol_N': 1e-12,\n        'n_max': 20,\n        'n_ls': 10,\n        'ds0': 0.02,\n        'ds_min': 1e-4,\n        'ds_max': 0.2,\n        'r_fail': 0.5,\n        'N_acc': 40,\n    }\n\n    # Case A\n    case_A_params = {\n        'case_id': 'A',\n        'beta': 8.0, 'gamma': 6.0, 'h': 1.0, 'yc': 0.0,\n        'n_targ': 4, 'kp': 0.25, 'ki': 0.05,\n    }\n    params_A = {**common_settings, **case_A_params}\n    result_A = run_case(params_A)\n    m_A = compute_metric_A(result_A, params_A['n_targ'])\n\n    # Case B\n    case_B_params = {\n        'case_id': 'B',\n        'beta': 8.0, 'gamma': 6.0, 'h': 1.0, 'yc': 0.0,\n        'n_targ': 4, 'kp': 0.25, 'ki': 0.05,\n        'k_sw': 12, 'gamma_new': 9.0, 'K_win': 8,\n    }\n    params_B = {**common_settings, **case_B_params}\n    result_B = run_case(params_B)\n    m_B = compute_metric_B(result_B, params_B['n_targ'], params_B['k_sw'], params_B['K_win'])\n\n    # Case C\n    case_C_params = {\n        'case_id': 'C',\n        'beta': 10.0, 'gamma': 9.0, 'h': 0.5, 'yc': 0.0,\n        'n_targ': 3, 'kp': 0.2, 'ki': 0.04,\n    }\n    params_C = {**common_settings, **case_C_params}\n    result_C = run_case(params_C)\n    m_C = compute_metric_C(result_C)\n\n    print(f\"[{float(m_A)},{float(m_B)},{int(m_C)}]\")\n\ndef F_vec(z, p):\n    \"\"\"Computes the residual vector F of the CSTR equations.\"\"\"\n    x, y, lam = z\n    R = lam * x * np.exp(p['gamma'] * y)\n    F1 = (1.0 - x) - R\n    F2 = (1.0 - y) + p['beta'] * R - p['h'] * (y - p['yc'])\n    return np.array([F1, F2])\n\ndef J_F_z(z, p):\n    \"\"\"Computes the Jacobian of F with respect to z = [x, y, lambda].\"\"\"\n    x, y, lam = z\n    if x == 0.0 or lam == 0.0: # Avoid division by zero\n        # This case is physically unlikely on the solution branch\n        # but good for robustness if the solver strays.\n        exp_gy = np.exp(p['gamma'] * y)\n        R = lam * x * exp_gy\n        dR_dx = lam * exp_gy\n        dR_dy = lam * x * p['gamma'] * exp_gy\n        dR_dlam = x * exp_gy\n    else:\n        R = lam * x * np.exp(p['gamma'] * y)\n        dR_dx = R / x\n        dR_dy = p['gamma'] * R\n        dR_dlam = R / lam\n\n    J11 = -1.0 - dR_dx\n    J12 = -dR_dy\n    J13 = -dR_dlam\n    \n    J21 = p['beta'] * dR_dx\n    J22 = -1.0 + p['beta'] * dR_dy - p['h']\n    J23 = p['beta'] * dR_dlam\n\n    return np.array([[J11, J12, J13], [J21, J22, J23]])\n\ndef find_initial_point(p):\n    \"\"\"Finds the initial steady state (x, y) at lambda0 using Newton's method.\"\"\"\n    lam0 = p['lambda0']\n    v = p['initial_guess'].copy()\n\n    for _ in range(p['n_max']):\n        z_fixed_lam = np.array([v[0], v[1], lam0])\n        F = F_vec(z_fixed_lam, p)\n        if linalg.norm(F) < p['tol_F']:\n            return np.array([v[0], v[1], lam0])\n        \n        J_Fv = J_F_z(z_fixed_lam, p)[:, :2]\n        delta_v = linalg.solve(J_Fv, -F)\n        \n        alpha = 1.0\n        norm_F = linalg.norm(F)\n        for _ in range(p['n_ls']):\n            v_new = v + alpha * delta_v\n            z_new = np.array([v_new[0], v_new[1], lam0])\n            if linalg.norm(F_vec(z_new, p)) < norm_F:\n                v = v_new\n                break\n            alpha /= 2.0\n        else: # Line search failed\n            return None\n    return None\n\ndef run_case(p):\n    \"\"\"Performs the pseudo-arclength continuation for a given set of parameters.\"\"\"\n    z_current = find_initial_point(p)\n    if z_current is None:\n        return {'n_iters_list': [], 'rejected_steps': 0, 'total_accepted': 0}\n\n    # Make a mutable copy of parameters for Case B\n    p_run = p.copy()\n\n    accepted_steps = 0\n    rejected_steps = 0\n    n_iters_list = []\n    \n    ds = p['ds0']\n    E_integral = 0.0\n    z_previous = None\n\n    while accepted_steps < p['N_acc']:\n        if p['case_id'] == 'B' and accepted_steps == p['k_sw']:\n             p_run['gamma'] = p['gamma_new']\n\n        # Retry loop for the current step\n        step_accepted = False\n        while not step_accepted:\n            J_fz = J_F_z(z_current, p_run)\n            tangent = linalg.null_space(J_fz).ravel()\n            \n            # Orient tangent\n            if accepted_steps == 0:\n                if tangent[2] < 0:\n                    tangent = -tangent\n            else:\n                secant = z_current - z_previous\n                if np.dot(tangent, secant) < 0:\n                    tangent = -tangent\n\n            z_pred = z_current + ds * tangent\n            \n            # Newton corrector loop\n            z_corr = z_pred.copy()\n            converged = False\n            n_iter = 0\n            for i in range(p['n_max']):\n                n_iter = i + 1\n                F_res = F_vec(z_corr, p_run)\n                N_res = np.dot(tangent, z_corr - z_pred)\n                \n                if linalg.norm(F_res) < p['tol_F'] and abs(N_res) < p['tol_N']:\n                    converged = True\n                    break\n\n                G_res = np.append(F_res, N_res)\n                J_G = np.vstack([J_F_z(z_corr, p_run), tangent])\n                \n                # Check for singular Jacobian\n                if abs(linalg.det(J_G)) < 1e-12: break\n\n                delta_z = linalg.solve(J_G, -G_res)\n                \n                # Line search\n                alpha = 1.0\n                norm_G = linalg.norm(G_res)\n                ls_success = False\n                for _ in range(p['n_ls']):\n                    z_next = z_corr + alpha * delta_z\n                    G_next = np.append(F_vec(z_next, p_run), np.dot(tangent, z_next - z_pred))\n                    if linalg.norm(G_next) < norm_G:\n                        z_corr = z_next\n                        ls_success = True\n                        break\n                    alpha /= 2.0\n                \n                if not ls_success:\n                    break\n            \n            if converged:\n                step_accepted = True\n                n_iters_list.append(n_iter)\n                \n                z_previous = z_current\n                z_current = z_corr\n                accepted_steps += 1\n                \n                # PI Controller update\n                error = n_iter - p['n_targ']\n                E_integral += error\n                ds_factor = np.exp(-p['kp'] * error - p['ki'] * E_integral)\n                ds = np.clip(ds * ds_factor, p['ds_min'], p['ds_max'])\n            else: # Corrector failed\n                rejected_steps += 1\n                ds *= p['r_fail']\n                if ds < p['ds_min']:\n                    break # Terminate run\n        \n        if ds < p['ds_min']:\n            break # Terminate from outer loop\n\n    return {\n        'n_iters_list': n_iters_list,\n        'rejected_steps': rejected_steps,\n        'total_accepted': accepted_steps,\n    }\n\ndef compute_metric_A(result, n_targ):\n    n_iters = np.array(result['n_iters_list'])\n    if len(n_iters) == 0:\n        return 0.0\n    return np.max(np.abs(n_iters - n_targ))\n\ndef compute_metric_B(result, n_targ, k_sw, K_win):\n    n_iters = result['n_iters_list']\n    K = result['total_accepted']\n    if K <= k_sw:\n        return 0.0\n    \n    window = n_iters[k_sw : min(k_sw + K_win, K)]\n    if not window:\n        return 0.0\n\n    overshoots = np.array(window) - n_targ\n    return np.max(np.maximum(0, overshoots))\n\ndef compute_metric_C(result):\n    return result['rejected_steps']\n\nif __name__ == '__main__':\n    solve()\n\n```"
        }
    ]
}