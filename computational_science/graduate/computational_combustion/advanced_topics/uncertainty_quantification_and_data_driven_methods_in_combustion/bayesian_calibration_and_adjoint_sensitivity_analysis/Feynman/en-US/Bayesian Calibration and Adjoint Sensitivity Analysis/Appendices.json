{
    "hands_on_practices": [
        {
            "introduction": "Before we can leverage the power of gradients in optimization and sampling, we must first master their derivation. This foundational exercise guides you through combining statistical principles with advanced calculus to find the gradient of the log-posterior for a canonical combustion problem. You will see how Bayes' rule defines the objective function and how the continuous adjoint method provides an efficient route to its gradient, particularly for quantities like ignition delay which are defined implicitly by the model's dynamics .",
            "id": "4009554",
            "problem": "Consider a homogeneous, adiabatic, constant-volume reactor model used in computational combustion for predicting ignition delay. Let the reactor state be $y(t;\\theta)\\in\\mathbb{R}^{n}$, collecting temperature and species mass fractions, evolving according to an Ordinary Differential Equation (ODE) of the form\n$$\n\\frac{d y}{d t} \\;=\\; F_{i}\\!\\big(y(t;\\theta),\\theta\\big), \\quad y(0;\\theta)=y_{i0}(\\theta),\n$$\nfor $i=1,\\dots,N$ experimental conditions (each condition has its own right-hand side $F_{i}$ and initial state $y_{i0}$ induced by the mixture composition, pressure, and temperature of that experiment). Reaction rates are modeled with Arrhenius-type kinetics so that $F_{i}$ depends on parameters $\\theta$ through rate constants $k_{r}(T;\\theta)$ of the form $k_{r}(T;\\theta)=A_{r}\\exp\\!\\big(-E_{r}/(R\\,T)\\big)$, where $A_{r}$ and $E_{r}$ are components of $\\theta$, $R$ is the universal gas constant, and $T$ is the temperature component of $y$.\n\nDefine the predicted ignition delay $\\hat{\\tau}_{i}(\\theta)$ for experiment $i$ as the first time $t$ at which an event function $s_{i}\\!\\big(y(t;\\theta),\\theta\\big)$ vanishes:\n$$\ns_{i}\\!\\big(y(\\hat{\\tau}_{i}(\\theta);\\theta),\\theta\\big) \\;=\\; 0,\n$$\nwith $s_{i}$ smooth in both $y$ and $\\theta$. For a chemically realistic criterion, one may take $s_{i}(y,\\theta)=Y_{\\mathrm{OH}}(y)-Y_{\\mathrm{OH}}^{\\star}$, where $Y_{\\mathrm{OH}}$ is the hydroxyl radical mass fraction implicit in $y$ and $Y_{\\mathrm{OH}}^{\\star}$ is a fixed threshold (thus $\\partial s_{i}/\\partial\\theta=0$), although the derivation below holds for general smooth $s_{i}$.\n\nSuppose we have ignition delay measurements $d=\\{\\tau_{i}\\}_{i=1}^{N}$, and adopt a Gaussian error model\n$$\n\\tau_{i} \\;=\\; \\hat{\\tau}_{i}(\\theta) + \\varepsilon_{i}, \\qquad \\varepsilon_{i}\\sim \\mathcal{N}(0,\\sigma^{2}),\n$$\nwith known variance $\\sigma^{2}>0$ and independent errors across $i$. Place a Gaussian prior on $\\theta$,\n$$\n\\theta \\sim \\mathcal{N}(\\theta_{0},\\Sigma_{0}),\n$$\nwith mean $\\theta_{0}$ and positive definite covariance $\\Sigma_{0}$.\n\nUsing Bayes’ rule and the Gaussian likelihood-prior assumptions, derive an explicit expression for the log posterior $\\log p(\\theta\\mid d)$ and compute its gradient $\\nabla_{\\theta}\\log p(\\theta\\mid d)$. For the gradient, you must express $\\nabla_{\\theta}\\hat{\\tau}_{i}(\\theta)$ using adjoint sensitivity analysis for the event time defined implicitly by $s_{i}\\!\\big(y(\\hat{\\tau}_{i}(\\theta);\\theta),\\theta\\big)=0$. Introduce the adjoint $\\lambda_{i}(t)\\in\\mathbb{R}^{n}$ solving\n$$\n-\\frac{d \\lambda_{i}}{d t} \\;=\\; \\left[\\frac{\\partial F_{i}}{\\partial y}\\!\\big(y(t;\\theta),\\theta\\big)\\right]^{\\!\\top} \\lambda_{i}(t), \\qquad \\lambda_{i}\\!\\big(\\hat{\\tau}_{i}(\\theta)\\big) \\;=\\; q_{i},\n$$\nwhere $q_{i} := \\left.\\frac{\\partial s_{i}}{\\partial y}\\!\\big(y,\\theta\\big)\\right|_{y=y(\\hat{\\tau}_{i}(\\theta);\\theta)}$. Your final expressions must be in closed form and may involve integrals of known functions of the forward and adjoint solutions. Express the final answer symbolically. No numerical evaluation is required. If any approximations were required, they would need to be specified in terms of significant figures; however, this problem requires an exact symbolic answer. The log posterior is dimensionless; report the gradient as a dimensionless vector expression.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n-   **State vector**: $y(t;\\theta)\\in\\mathbb{R}^{n}$, representing temperature and species mass fractions.\n-   **Governing ODE**: $\\frac{d y}{d t} = F_{i}(y(t;\\theta),\\theta)$ for $i=1,\\dots,N$ experimental conditions.\n-   **Initial condition**: $y(0;\\theta)=y_{i0}(\\theta)$.\n-   **Parameter dependence**: Reaction rates $k_{r}(T;\\theta)=A_{r}\\exp(-E_{r}/(R\\,T))$ are functions of parameters $\\theta$, where $A_{r}$ and $E_{r}$ are components of $\\theta$.\n-   **Predicted ignition delay**: $\\hat{\\tau}_{i}(\\theta)$ is defined implicitly by the event function $s_{i}(y(\\hat{\\tau}_{i}(\\theta);\\theta),\\theta) = 0$. The function $s_{i}$ is smooth.\n-   **Measurement data**: $d=\\{\\tau_{i}\\}_{i=1}^{N}$.\n-   **Likelihood model**: An independent and identically distributed Gaussian error model: $\\tau_{i} = \\hat{\\tau}_{i}(\\theta) + \\varepsilon_{i}$, with $\\varepsilon_{i}\\sim \\mathcal{N}(0,\\sigma^{2})$ and known variance $\\sigma^{2}>0$.\n-   **Prior model**: A Gaussian prior on the parameters: $\\theta \\sim \\mathcal{N}(\\theta_{0},\\Sigma_{0})$, with mean $\\theta_{0}$ and positive definite covariance $\\Sigma_{0}$.\n-   **Adjoint system**: The adjoint vector $\\lambda_{i}(t)\\in\\mathbb{R}^{n}$ is defined by the ODE $-\\frac{d \\lambda_{i}}{d t} = [\\frac{\\partial F_{i}}{\\partial y}(y(t;\\theta),\\theta)]^{\\top} \\lambda_{i}(t)$ with terminal condition $\\lambda_{i}(\\hat{\\tau}_{i}(\\theta)) = q_{i}$.\n-   **Adjoint terminal condition source**: $q_{i} := \\left.\\frac{\\partial s_{i}}{\\partial y}(y,\\theta)\\right|_{y=y(\\hat{\\tau}_{i}(\\theta);\\theta)}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is a standard formulation of a Bayesian inverse problem for parameter estimation in chemical kinetics, a core topic in computational combustion. The use of homogeneous reactor models, Arrhenius kinetics, and an implicit definition for ignition delay are all standard practice. The application of Bayesian inference with Gaussian priors and likelihoods is a well-established statistical framework. Furthermore, the use of adjoint sensitivity analysis is a state-of-the-art, mathematically rigorous, and computationally efficient technique for computing the gradients required in such problems. The problem is self-contained, with all necessary definitions and models provided. There are no scientific or factual unsoundness, no contradictions, and the problem is objective and well-posed. It directly addresses the topic of *Bayesian calibration and adjoint sensitivity analysis* within *computational combustion*.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full, reasoned solution will be provided.\n\n### Derivation of the Log Posterior and its Gradient\n\nThe goal is to derive the log posterior probability density function $\\log p(\\theta\\mid d)$ and its gradient $\\nabla_{\\theta}\\log p(\\theta\\mid d)$.\n\nAccording to Bayes' rule, the posterior probability is proportional to the product of the likelihood and the prior:\n$$p(\\theta\\mid d) \\propto p(d\\mid\\theta)p(\\theta)$$\nTaking the logarithm, we have:\n$$\\log p(\\theta\\mid d) = \\log p(d\\mid\\theta) + \\log p(\\theta) + C$$\nwhere $C$ is a normalization constant that does not depend on the parameters $\\theta$.\n\n**1. Log-Likelihood Term:**\nThe measurements $\\{\\tau_i\\}$ are assumed to be independent. The likelihood is the product of the probabilities of each measurement:\n$$p(d\\mid\\theta) = \\prod_{i=1}^{N} p(\\tau_i\\mid\\theta)$$\nGiven the error model $\\tau_i \\sim \\mathcal{N}(\\hat{\\tau}_i(\\theta), \\sigma^2)$, the probability density for a single measurement is:\n$$p(\\tau_i\\mid\\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(\\tau_i - \\hat{\\tau}_i(\\theta))^2}{2\\sigma^2}\\right)$$\nThe total log-likelihood is the sum over all $N$ experiments:\n$$\\log p(d\\mid\\theta) = \\sum_{i=1}^{N} \\log p(\\tau_i\\mid\\theta) = \\sum_{i=1}^{N} \\left(-\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\tau_i - \\hat{\\tau}_i(\\theta))^2\\right)$$\nDropping the constant terms, the part of the log-likelihood relevant to $\\theta$ is:\n$$\\log p(d\\mid\\theta) = -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{N}(\\tau_i - \\hat{\\tau}_i(\\theta))^2 + \\text{const.}$$\n\n**2. Log-Prior Term:**\nThe prior on $\\theta$ is given as a multivariate normal distribution, $\\theta \\sim \\mathcal{N}(\\theta_0, \\Sigma_0)$. Its probability density function is:\n$$p(\\theta) = \\frac{1}{\\sqrt{(2\\pi)^k |\\det(\\Sigma_0)|}} \\exp\\left(-\\frac{1}{2}(\\theta - \\theta_0)^{\\top}\\Sigma_0^{-1}(\\theta - \\theta_0)\\right)$$\nwhere $k$ is the dimension of $\\theta$. The log-prior is:\n$$\\log p(\\theta) = -\\frac{1}{2}(\\theta - \\theta_0)^{\\top}\\Sigma_0^{-1}(\\theta - \\theta_0) + \\text{const.}$$\n\n**3. Log-Posterior Expression:**\nCombining the log-likelihood and log-prior, the log-posterior (up to an additive constant) is:\n$$\\log p(\\theta\\mid d) = -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{N}(\\tau_i - \\hat{\\tau}_i(\\theta))^2 - \\frac{1}{2}(\\theta - \\theta_0)^{\\top}\\Sigma_0^{-1}(\\theta - \\theta_0)$$\n\n**4. Gradient of the Log-Posterior:**\nWe now compute the gradient of $\\log p(\\theta\\mid d)$ with respect to $\\theta$:\n$$\\nabla_{\\theta}\\log p(\\theta\\mid d) = \\nabla_{\\theta}\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{N}(\\tau_i - \\hat{\\tau}_i(\\theta))^2\\right) + \\nabla_{\\theta}\\left(-\\frac{1}{2}(\\theta - \\theta_0)^{\\top}\\Sigma_0^{-1}(\\theta - \\theta_0)\\right)$$\nThe gradient of the prior term is a standard result for quadratic forms:\n$$\\nabla_{\\theta}\\left(-\\frac{1}{2}(\\theta - \\theta_0)^{\\top}\\Sigma_0^{-1}(\\theta - \\theta_0)\\right) = -\\Sigma_0^{-1}(\\theta - \\theta_0)$$\nThe gradient of the likelihood term is:\n$$\\nabla_{\\theta}\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{N}(\\tau_i - \\hat{\\tau}_i(\\theta))^2\\right) = -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{N} 2(\\tau_i - \\hat{\\tau}_i(\\theta))(-\\nabla_{\\theta}\\hat{\\tau}_i(\\theta)) = \\frac{1}{\\sigma^2}\\sum_{i=1}^{N}(\\tau_i - \\hat{\\tau}_i(\\theta))\\nabla_{\\theta}\\hat{\\tau}_i(\\theta)$$\nThe remaining task is to find an expression for the sensitivity of the ignition delay, $\\nabla_{\\theta}\\hat{\\tau}_i(\\theta)$.\n\n**5. Adjoint Sensitivity Analysis for $\\nabla_{\\theta}\\hat{\\tau}_i(\\theta)$:**\nThe ignition delay $\\hat{\\tau}_i$ is defined implicitly by $s_{i}(y(\\hat{\\tau}_{i}(\\theta);\\theta),\\theta) = 0$. We differentiate this identity with respect to $\\theta$ using the chain rule:\n$$\\frac{d}{d\\theta}\\left[s_{i}(y(\\hat{\\tau}_{i}(\\theta);\\theta),\\theta)\\right] = 0$$\n$$\\frac{\\partial s_{i}}{\\partial y}\\frac{d y}{d\\theta} + \\frac{\\partial s_{i}}{\\partial\\theta} = 0$$\nThe total derivative of $y$ with respect to $\\theta$ is $\\frac{dy}{d\\theta} = \\frac{\\partial y}{\\partial t}\\frac{d\\hat{\\tau}_i}{d\\theta} + \\frac{\\partial y}{\\partial\\theta}$. As $\\nabla_{\\theta}$ is a row vector operator, this is written for a single component $\\theta_j$ and then generalized. In vector notation:\n$$\\left(\\frac{\\partial s_{i}}{\\partial y}\\right)^{\\!\\top}\\left( \\frac{dy}{dt}\\nabla_{\\theta}\\hat{\\tau}_i + \\nabla_{\\theta}y \\right) + \\left(\\frac{\\partial s_{i}}{\\partial\\theta}\\right)^{\\!\\top}= 0$$\nAll derivatives are evaluated at $t=\\hat{\\tau}_i(\\theta)$. Rearranging to solve for the gradient $\\nabla_{\\theta}\\hat{\\tau}_i$:\n$$\\left(\\left(\\frac{\\partial s_{i}}{\\partial y}\\right)^{\\!\\top}\\frac{dy}{dt}\\right)\\nabla_{\\theta}\\hat{\\tau}_i = -\\left(\\frac{\\partial s_{i}}{\\partial y}\\right)^{\\!\\top}\\nabla_{\\theta}y - \\left(\\frac{\\partial s_{i}}{\\partial\\theta}\\right)^{\\!\\top}$$\nLet $\\dot{s}_{i}(\\hat{\\tau}_{i}) = \\left(\\frac{\\partial s_{i}}{\\partial y}\\right)^{\\!\\top}\\frac{dy}{dt}\\big|_{t=\\hat{\\tau}_i} = q_{i}^{\\top}F_{i}(y(\\hat{\\tau}_i;\\theta),\\theta)$. This is a scalar rate of change of the event function at the event time.\n$$\\nabla_{\\theta}\\hat{\\tau}_i = -\\frac{1}{\\dot{s}_{i}(\\hat{\\tau}_{i})} \\left(q_{i}^{\\top}\\nabla_{\\theta}y(\\hat{\\tau}_i;\\theta) + \\left(\\frac{\\partial s_{i}}{\\partial\\theta}\\right)^{\\!\\top} \\right)$$\nThe term $q_{i}^{\\top}\\nabla_{\\theta}y(\\hat{\\tau}_i;\\theta)$ requires the forward sensitivity matrix $\\nabla_{\\theta}y$, which is computationally expensive to obtain. The adjoint method provides an efficient alternative. The standard adjoint sensitivity identity states that for an adjoint state $\\lambda_i(t)$ satisfying the given ODE and terminal condition:\n$$q_{i}^{\\top}\\nabla_{\\theta}y(\\hat{\\tau}_i;\\theta) = \\lambda_i(\\hat{\\tau}_i)^{\\top}\\nabla_{\\theta}y(\\hat{\\tau}_i;\\theta) = \\lambda_i(0)^{\\top}\\nabla_{\\theta}y_{i0}(\\theta) + \\int_{0}^{\\hat{\\tau}_i(\\theta)} \\lambda_i(t)^{\\top} \\frac{\\partial F_i}{\\partial\\theta}(y(t;\\theta),\\theta) dt$$\nSubstituting this back into the expression for $\\nabla_{\\theta}\\hat{\\tau}_i$:\n$$\\nabla_{\\theta}\\hat{\\tau}_i(\\theta) = -\\frac{1}{\\dot{s}_{i}(\\hat{\\tau}_{i})} \\left( \\left(\\frac{\\partial s_{i}}{\\partial\\theta}\\right)^{\\!\\top} + \\lambda_i(0)^{\\top}\\nabla_{\\theta}y_{i0}(\\theta) + \\int_{0}^{\\hat{\\tau}_i(\\theta)} \\lambda_i(t)^{\\top} \\frac{\\partial F_i}{\\partial\\theta} dt \\right)$$\nFor conciseness and to match vector calculus conventions where gradients are column vectors, we transpose the result:\n$$\\nabla_{\\theta}\\hat{\\tau}_i(\\theta) = -\\frac{1}{\\dot{s}_{i}(\\hat{\\tau}_{i})} \\left( \\frac{\\partial s_{i}}{\\partial\\theta} + (\\nabla_{\\theta}y_{i0}(\\theta))^{\\top}\\lambda_i(0) + \\int_{0}^{\\hat{\\tau}_i(\\theta)} \\left(\\frac{\\partial F_i}{\\partial\\theta}\\right)^{\\!\\top} \\lambda_i(t) dt \\right)$$\n\n**6. Final Gradient Expression:**\nFinally, we substitute the expression for $\\nabla_{\\theta}\\hat{\\tau}_i(\\theta)$ into the gradient of the log-posterior:\n$$\\nabla_{\\theta}\\log p(\\theta\\mid d) = -\\Sigma_0^{-1}(\\theta - \\theta_0) + \\frac{1}{\\sigma^2}\\sum_{i=1}^{N}(\\tau_i - \\hat{\\tau}_i(\\theta))\\nabla_{\\theta}\\hat{\\tau}_i(\\theta)$$\nLet $\\delta_i = \\tau_i - \\hat{\\tau}_i(\\theta)$ be the residual for experiment $i$.\n$$\\nabla_{\\theta}\\log p(\\theta\\mid d) = -\\Sigma_0^{-1}(\\theta - \\theta_0) - \\sum_{i=1}^{N} \\frac{\\delta_i}{\\sigma^2 \\dot{s}_{i}(\\hat{\\tau}_{i})} \\left( \\frac{\\partial s_{i}}{\\partial\\theta} + (\\nabla_{\\theta}y_{i0}(\\theta))^{\\top}\\lambda_i(0) + \\int_{0}^{\\hat{\\tau}_i(\\theta)} \\left(\\frac{\\partial F_i}{\\partial\\theta}\\right)^{\\!\\top} \\lambda_i(t) dt \\right)$$\nThis is the final expression for the gradient of the log-posterior. It is expressed in terms of the forward state solution $y(t;\\theta)$, the backward adjoint solution $\\lambda_i(t)$, and known partial derivatives of the model functions.\nThe term $\\dot{s}_{i}(\\hat{\\tau}_{i})$ is defined as $q_i^{\\top}F_i(y(\\hat{\\tau}_i;\\theta),\\theta)$. Also, $(\\nabla_{\\theta}y_{i0}(\\theta))^{\\top}\\lambda_i(0)$ can be written as $\\lambda_i(0)^{\\top}\\nabla_{\\theta}y_{i0}(\\theta)$, and $(\\frac{\\partial F_i}{\\partial\\theta})^{\\top} \\lambda_i(t)$ as $\\lambda_i(t)^{\\top}\\frac{\\partial F_i}{\\partial\\theta}$ if preferred, depending on layout conventions for the Jacobian $\\frac{\\partial F_i}{\\partial\\theta}$. We will retain the form that naturally arose from the derivation.",
            "answer": "$$\\boxed{-\\Sigma_{0}^{-1}(\\theta - \\theta_{0}) - \\sum_{i=1}^{N} \\frac{\\tau_{i} - \\hat{\\tau}_{i}(\\theta)}{\\sigma^{2} \\left(q_{i}^{\\top}F_{i}(y(\\hat{\\tau}_{i};\\theta),\\theta)\\right)} \\left( \\frac{\\partial s_{i}}{\\partial\\theta} + \\left(\\nabla_{\\theta}y_{i0}(\\theta)\\right)^{\\!\\top}\\lambda_{i}(0) + \\int_{0}^{\\hat{\\tau}_{i}(\\theta)} \\left(\\frac{\\partial F_{i}}{\\partial\\theta}\\right)^{\\!\\top}\\lambda_{i}(t) dt \\right)}$$"
        },
        {
            "introduction": "Gradients do more than just point downhill; they quantify how a model's output responds to parameter changes, which is crucial for understanding what a given dataset can teach us. In this practical coding exercise, you will use sensitivities to construct the Fisher Information Matrix ($I$), a cornerstone of statistical inference and optimal experimental design. By analyzing the eigenvalues of $I$, you will learn to diagnose parameter identifiability and pinpoint which aspects of your model are well-constrained by the data and which are not .",
            "id": "4009502",
            "problem": "You are given a simplified, scientifically consistent surrogate of a one-step, planar, freely propagating premixed flame used in computational combustion. The goal is to compute the Fisher Information Matrix (abbreviated as $I$) for a set of laminar flame speed measurements based on adjoint sensitivities, and to report the eigenvalues of $I$ that indicate parameter identifiability in a Bayesian calibration setting.\n\nFundamental base:\n- Arrhenius law for a one-step reaction: the reaction rate scales as $A \\exp\\!\\left(-\\dfrac{E}{R T}\\right)$, where $A$ is the pre-exponential factor, $E$ is the activation energy, and $R$ is the universal gas constant.\n- Traveling-wave balance between diffusion and reaction in a steady, planar premixed flame leads to the scaling of the laminar flame speed $S_L$ with diffusivity $D$ and reaction kinetics. In the canonical linearized, constant-property approximation, the dispersion relation yields a scaling\n$$\nS_L(\\boldsymbol{\\theta}, T, D) = 2 \\sqrt{D} \\, \\sqrt{A} \\, \\exp\\!\\left(-\\dfrac{E}{2 R T}\\right),\n$$\nwhere $\\boldsymbol{\\theta} = \\left[\\theta_1, \\theta_2\\right] = \\left[\\ln A, E\\right]$ and $T$ is an effective activation temperature representative of the high-temperature reaction zone.\n- Bayesian calibration with Gaussian measurement noise: with independent measurements $\\{y_i\\}$ and model predictions $\\{S_L(\\boldsymbol{\\theta}, T_i, D_i)\\}$ under additive independent, identically distributed Gaussian noise of known standard deviations $\\sigma_i$, the negative log-likelihood is quadratic in residuals. The Fisher Information Matrix in the local (Laplace) approximation is\n$$\nI(\\boldsymbol{\\theta}) = \\sum_{i=1}^N \\dfrac{1}{\\sigma_i^2} \\, \\mathbf{J}_i(\\boldsymbol{\\theta})^\\top \\mathbf{J}_i(\\boldsymbol{\\theta}),\n$$\nwhere $\\mathbf{J}_i(\\boldsymbol{\\theta}) = \\dfrac{\\partial S_L(\\boldsymbol{\\theta}, T_i, D_i)}{\\partial \\boldsymbol{\\theta}}$ is the sensitivity of the laminar flame speed with respect to parameters.\n\nAdjoint sensitivity base:\n- Consider the algebraic constraint for the log-flame-speed $u$ defined by $u = \\ln S_L$,\n$$\ng(u, \\boldsymbol{\\theta}; T, D) \\equiv u - \\left(\\ln\\!\\left(2 \\sqrt{D}\\right) + \\dfrac{\\theta_1}{2} - \\dfrac{\\theta_2}{2 R T}\\right) = 0,\n$$\nand define the quantity of interest $f(u) = \\exp(u) = S_L$. The adjoint variable $\\lambda$ solves\n$$\n\\left(\\dfrac{\\partial g}{\\partial u}\\right)^\\top \\lambda = \\dfrac{\\partial f}{\\partial u},\n$$\nand the adjoint sensitivity gives\n$$\n\\dfrac{\\partial f}{\\partial \\boldsymbol{\\theta}} = \\dfrac{\\partial f}{\\partial \\boldsymbol{\\theta}} - \\lambda^\\top \\dfrac{\\partial g}{\\partial \\boldsymbol{\\theta}} = - \\lambda^\\top \\dfrac{\\partial g}{\\partial \\boldsymbol{\\theta}},\n$$\nwith $\\dfrac{\\partial g}{\\partial u} = 1$ and $\\dfrac{\\partial f}{\\partial u} = S_L$, so $\\lambda = S_L$. This yields the sensitivities\n$$\n\\dfrac{\\partial S_L}{\\partial \\theta_1} = \\dfrac{S_L}{2}, \\qquad \\dfrac{\\partial S_L}{\\partial \\theta_2} = - \\dfrac{S_L}{2 R T}.\n$$\n\nTask:\n- Using the above fundamental and adjoint derivations, implement a program that:\n  1. Computes the laminar flame speed $S_L(\\boldsymbol{\\theta}, T_i, D_i)$ for given $T_i$ (in kelvin) and $D_i$ (in $\\mathrm{m}^2/\\mathrm{s}$), with a fixed parameter vector $\\boldsymbol{\\theta} = [\\ln A, E]$ where $A$ has units $\\mathrm{s}^{-1}$ and $E$ has units $\\mathrm{J}/\\mathrm{mol}$.\n  2. Computes the adjoint sensitivities $\\mathbf{J}_i(\\boldsymbol{\\theta})$ for each measurement.\n  3. Constructs the Fisher Information Matrix $I(\\boldsymbol{\\theta})$ and computes its eigenvalues for each test case.\n- Connection to Bayesian calibration: In the Gaussian-noise model, $I(\\boldsymbol{\\theta})$ is the expected negative Hessian of the log-likelihood and quantifies local identifiability. Larger eigenvalues indicate directions in parameter space that are better informed by the data.\n- Physical and numerical units:\n  - Use $R = 8.314 J/(mol·K)$ for the universal gas constant.\n  - Use $T$ in $\\mathrm{K}$, $D$ in $\\mathrm{m}^2/\\mathrm{s}$, $A$ in $\\mathrm{s}^{-1}$, $E$ in $\\mathrm{J}/\\mathrm{mol}$, and $S_L$ in $\\mathrm{m}/\\mathrm{s}$.\n  - The eigenvalues of $I(\\boldsymbol{\\theta})$ reflect mixed units associated with the parameterization $\\boldsymbol{\\theta} = [\\ln A, E]$; report them as floating-point numbers without unit labels.\n- Parameter values for computation:\n  - Use $\\boldsymbol{\\theta} = [\\ln A, E]$ with $A = 1.0 \\times 10^{18} \\, \\mathrm{s}^{-1}$ and $E = 9.0 \\times 10^{4} \\, \\mathrm{J}/\\mathrm{mol}$.\n- Test suite:\n  - Case 1 (happy path; diverse temperatures): five measurements with\n    - $(T, D, \\sigma)$ equal to\n      - $(1900\\,\\mathrm{K}, 1.5\\times 10^{-5}\\,\\mathrm{m}^2/\\mathrm{s}, 0.05\\,\\mathrm{m}/\\mathrm{s})$,\n      - $(2000\\,\\mathrm{K}, 1.5\\times 10^{-5}\\,\\mathrm{m}^2/\\mathrm{s}, 0.05\\,\\mathrm{m}/\\mathrm{s})$,\n      - $(2100\\,\\mathrm{K}, 1.5\\times 10^{-5}\\,\\mathrm{m}^2/\\mathrm{s}, 0.05\\,\\mathrm{m}/\\mathrm{s})$,\n      - $(2300\\,\\mathrm{K}, 1.5\\times 10^{-5}\\,\\mathrm{m}^2/\\mathrm{s}, 0.05\\,\\mathrm{m}/\\mathrm{s})$,\n      - $(2400\\,\\mathrm{K}, 1.5\\times 10^{-5}\\,\\mathrm{m}^2/\\mathrm{s}, 0.05\\,\\mathrm{m}/\\mathrm{s})$.\n  - Case 2 (boundary; rank deficiency by identical temperature): five measurements with identical $T$ and varying $D$\n    - $(T, D, \\sigma)$ equal to\n      - $(2100\\,\\mathrm{K}, 1.0\\times 10^{-5}\\,\\mathrm{m}^2/\\mathrm{s}, 0.05\\,\\mathrm{m}/\\mathrm{s})$,\n      - $(2100\\,\\mathrm{K}, 8.0\\times 10^{-6}\\,\\mathrm{m}^2/\\mathrm{s}, 0.05\\,\\mathrm{m}/\\mathrm{s})$,\n      - $(2100\\,\\mathrm{K}, 2.0\\times 10^{-5}\\,\\mathrm{m}^2/\\mathrm{s}, 0.05\\,\\mathrm{m}/\\mathrm{s})$,\n      - $(2100\\,\\mathrm{K}, 5.0\\times 10^{-6}\\,\\mathrm{m}^2/\\mathrm{s}, 0.05\\,\\mathrm{m}/\\mathrm{s})$,\n      - $(2100\\,\\mathrm{K}, 1.2\\times 10^{-5}\\,\\mathrm{m}^2/\\mathrm{s}, 0.05\\,\\mathrm{m}/\\mathrm{s})$.\n  - Case 3 (edge; large noise): three measurements with\n    - $(T, D, \\sigma)$ equal to\n      - $(1900\\,\\mathrm{K}, 1.5\\times 10^{-5}\\,\\mathrm{m}^2/\\mathrm{s}, 3.0\\,\\mathrm{m}/\\mathrm{s})$,\n      - $(2300\\,\\mathrm{K}, 1.5\\times 10^{-5}\\,\\mathrm{m}^2/\\mathrm{s}, 3.0\\,\\mathrm{m}/\\mathrm{s})$,\n      - $(2400\\,\\mathrm{K}, 1.5\\times 10^{-5}\\,\\mathrm{m}^2/\\mathrm{s}, 3.0\\,\\mathrm{m}/\\mathrm{s})$.\n- Final output format:\n  - Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is a list of the two eigenvalues of $I(\\boldsymbol{\\theta})$ for each test case, sorted in descending order. For example, the output must look like\n  $$\n  [[\\lambda_{1}^{(1)}, \\lambda_{2}^{(1)}],[\\lambda_{1}^{(2)}, \\lambda_{2}^{(2)}],[\\lambda_{1}^{(3)}, \\lambda_{2}^{(3)}]].\n  $$\n\nAll computations must be self-contained and numerically robust. No external input is permitted. The solution should be implementable in any modern programming language, but you must provide a complete, runnable program that adheres to the specified execution environment.",
            "solution": "The problem statement is evaluated to be **valid**. It is scientifically grounded in simplified but standard models from combustion theory and statistics, is mathematically well-posed, and provides a complete and consistent set of data and definitions required for a unique solution. The numerical values, while leading to physically large flame speeds, are part of a self-consistent \"surrogate\" model and do not impede the mathematical computation or violate the principles being tested. The problem is a formalizable and non-trivial exercise in computational science.\n\nThe procedure to solve the problem is as follows. We are tasked with computing the eigenvalues of the Fisher Information Matrix ($I$) for three different sets of experimental conditions. The parameter vector is $\\boldsymbol{\\theta} = [\\theta_1, \\theta_2] = [\\ln A, E]$.\n\nFirst, we establish the fixed parameters and constants for all computations:\n- Universal gas constant: $R = 8.314 \\, \\mathrm{J}/(\\mathrm{mol}\\,\\mathrm{K})$.\n- Pre-exponential factor: $A = 1.0 \\times 10^{18} \\, \\mathrm{s}^{-1}$.\n- Activation energy: $E = 9.0 \\times 10^{4} \\, \\mathrm{J}/\\mathrm{mol}$.\n\nFrom these, we define the parameter vector $\\boldsymbol{\\theta}$:\n- $\\theta_1 = \\ln A = \\ln(1.0 \\times 10^{18})$.\n- $\\theta_2 = E = 9.0 \\times 10^{4}$.\n\nThe core of the problem involves constructing the Fisher Information Matrix, $I(\\boldsymbol{\\theta})$, for each test case. The formula for $I(\\boldsymbol{\\theta})$ based on $N$ independent measurements is given by:\n$$\nI(\\boldsymbol{\\theta}) = \\sum_{i=1}^N \\dfrac{1}{\\sigma_i^2} \\, \\mathbf{J}_i(\\boldsymbol{\\theta})^\\top \\mathbf{J}_i(\\boldsymbol{\\theta})\n$$\nwhere $\\sigma_i$ is the standard deviation of the $i$-th measurement and $\\mathbf{J}_i(\\boldsymbol{\\theta})$ is the row vector of sensitivities (the Jacobian) of the model prediction with respect to the parameters $\\boldsymbol{\\theta}$, evaluated at the conditions of the $i$-th measurement. The term $\\mathbf{J}_i^\\top \\mathbf{J}_i$ represents the outer product of the sensitivity vector with itself, resulting in a $2 \\times 2$ symmetric, positive semi-definite matrix. The sum of these matrices, weighted by the inverse variance $1/\\sigma_i^2$, gives the total Fisher Information Matrix, which is also a $2 \\times 2$ symmetric, positive semi-definite matrix.\n\nFor each test case, we iterate through its set of measurements $(T_i, D_i, \\sigma_i)$. For each measurement $i$:\n\n1.  **Compute Laminar Flame Speed ($S_{L,i}$):** The model for the laminar flame speed $S_L$ is:\n    $$\n    S_L(\\boldsymbol{\\theta}, T, D) = 2 \\sqrt{D} \\, \\sqrt{A} \\, \\exp\\!\\left(-\\dfrac{E}{2 R T}\\right)\n    $$\n    Substituting $\\theta_1 = \\ln A$ and $\\theta_2 = E$, the formula for computation becomes:\n    $$\n    S_{L,i} = 2 \\sqrt{D_i} \\exp(\\theta_1/2) \\exp(-\\theta_2 / (2 R T_i))\n    $$\n\n2.  **Compute the Sensitivity Vector ($\\mathbf{J}_i$):** The problem provides the adjoint-derived sensitivities of $S_L$ with respect to $\\theta_1$ and $\\theta_2$:\n    $$\n    \\dfrac{\\partial S_L}{\\partial \\theta_1} = \\dfrac{S_L}{2}, \\qquad \\dfrac{\\partial S_L}{\\partial \\theta_2} = - \\dfrac{S_L}{2 R T}\n    $$\n    Thus, the sensitivity vector for the $i$-th measurement is:\n    $$\n    \\mathbf{J}_i(\\boldsymbol{\\theta}) = \\left[ \\dfrac{S_{L,i}}{2}, \\, -\\dfrac{S_{L,i}}{2 R T_i} \\right]\n    $$\n\n3.  **Accumulate the Fisher Information Matrix ($I$):** The contribution of the $i$-th measurement to the $I$ matrix is calculated and added to a running total. We initialize $I$ as a $2 \\times 2$ zero matrix. For each measurement, we compute:\n    $$\n    I \\leftarrow I + \\frac{1}{\\sigma_i^2} \\mathbf{J}_i^\\top \\mathbf{J}_i\n    $$\n    where $\\mathbf{J}_i^\\top \\mathbf{J}_i$ is the outer product:\n    $$\n    \\mathbf{J}_i^\\top \\mathbf{J}_i = \\begin{pmatrix} \\left(\\dfrac{S_{L,i}}{2}\\right)^2 & \\left(\\dfrac{S_{L,i}}{2}\\right) \\left(-\\dfrac{S_{L,i}}{2 R T_i}\\right) \\\\ \\left(\\dfrac{S_{L,i}}{2}\\right) \\left(-\\dfrac{S_{L,i}}{2 R T_i}\\right) & \\left(-\\dfrac{S_{L,i}}{2 R T_i}\\right)^2 \\end{pmatrix}\n    $$\n\nAfter iterating through all measurements for a given test case, the resulting $2 \\times 2$ matrix $I$ is complete.\n\n4.  **Compute Eigenvalues:** The final step for each case is to compute the two eigenvalues of the symmetric matrix $I$. These eigenvalues, denoted $\\lambda_1$ and $\\lambda_2$, are guaranteed to be real and non-negative. They quantify the amount of information the data provides about parameter combinations. Large eigenvalues correspond to well-identifiable parameter directions.\n\n5.  **Sort and Report:** The computed eigenvalues for each case are sorted in descending order and formatted as required.\n\nA special note on Case 2: In this case, the temperature $T_i$ is held constant for all measurements. The sensitivity vector is $\\mathbf{J}_i = [S_{L,i}/2, -S_{L,i}/(2RT_i)]$. Since $S_{L,i}$ is the only term that varies (due to varying $D_i$) and it acts as a scalar multiplier on the vector $[1/2, -1/(2RT_i)]$, all sensitivity vectors $\\mathbf{J}_i$ are collinear. Consequently, the Fisher Information Matrix $I$, being a sum of rank-$1$ matrices with the same null space, will itself be a rank-$1$ matrix. A $2 \\times 2$ rank-$1$ matrix has one non-zero eigenvalue and one zero eigenvalue. This result signifies that the data collected at a single temperature cannot distinguish between the effects of $\\ln A$ and $E$, leading to a non-identifiable parameter direction. The computation is expected to yield one eigenvalue that is numerically zero.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the eigenvalues of the Fisher Information Matrix for three test cases\n    related to a surrogate model of laminar flame speed.\n    \"\"\"\n    \n    # Define physical and numerical constants and parameters\n    R = 8.314  # J/(mol K)\n    A = 1.0e18 # s^-1\n    E = 9.0e4  # J/mol\n\n    # Parameter vector theta = [ln(A), E]\n    theta1 = np.log(A)\n    theta2 = E\n\n    # Define the test suite\n    test_cases = [\n        # Case 1: Happy path; diverse temperatures\n        [\n            (1900.0, 1.5e-5, 0.05),\n            (2000.0, 1.5e-5, 0.05),\n            (2100.0, 1.5e-5, 0.05),\n            (2300.0, 1.5e-5, 0.05),\n            (2400.0, 1.5e-5, 0.05),\n        ],\n        # Case 2: Boundary; rank deficiency by identical temperature\n        [\n            (2100.0, 1.0e-5, 0.05),\n            (2100.0, 8.0e-6, 0.05),\n            (2100.0, 2.0e-5, 0.05),\n            (2100.0, 5.0e-6, 0.05),\n            (2100.0, 1.2e-5, 0.05),\n        ],\n        # Case 3: Edge; large noise\n        [\n            (1900.0, 1.5e-5, 3.0),\n            (2300.0, 1.5e-5, 3.0),\n            (2400.0, 1.5e-5, 3.0),\n        ],\n    ]\n\n    results = []\n\n    for case_measurements in test_cases:\n        # Initialize the 2x2 Fisher Information Matrix for the current case\n        FIM = np.zeros((2, 2))\n\n        for T, D, sigma in case_measurements:\n            # 1. Compute Laminar Flame Speed (S_L)\n            # S_L = 2 * sqrt(D) * sqrt(A) * exp(-E / (2*R*T))\n            # In terms of theta: S_L = 2 * sqrt(D) * exp(theta1/2) * exp(-theta2 / (2*R*T))\n            SL = 2 * np.sqrt(D) * np.exp(theta1 / 2) * np.exp(-theta2 / (2 * R * T))\n\n            # 2. Compute the Sensitivity Vector (Jacobian) J\n            # d(S_L)/d(theta1) = S_L / 2\n            # d(S_L)/d(theta2) = -S_L / (2 * R * T)\n            J = np.array([SL / 2, -SL / (2 * R * T)])\n\n            # 3. Accumulate the Fisher Information Matrix\n            # FIM = sum( (1/sigma_i^2) * J_i^T * J_i )\n            # J_i^T * J_i is the outer product of J with itself\n            J_outer_product = np.outer(J, J)\n            FIM += (1 / sigma**2) * J_outer_product\n\n        # 4. Compute the eigenvalues of the symmetric FIM\n        # np.linalg.eigvalsh is used for Hermitian (or real symmetric) matrices\n        eigenvalues = np.linalg.eigvalsh(FIM)\n        \n        # 5. Sort eigenvalues in descending order and append to results\n        sorted_eigenvalues = sorted(eigenvalues, reverse=True)\n        results.append(sorted_eigenvalues)\n\n    # Final print statement in the exact required format\n    # The default string representation of a list matches the required bracketed format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "The primary motivation for developing complex adjoint solvers is to unlock the performance of modern gradient-based algorithms for high-dimensional problems. This capstone practice brings all the concepts together, tasking you with building a full Bayesian inference pipeline for a time-dependent combustion model. You will implement an adjoint solver to compute gradients and then use them to drive a Hamiltonian Monte Carlo (HMC) sampler, directly comparing its efficiency to a traditional gradient-free method to witness the power of adjoint-based inference .",
            "id": "4009504",
            "problem": "Consider a scalar reduced-combustion mechanism governed by an Arrhenius-type first-order reaction for a normalized progress variable $Y(t) \\in [0,1]$ with dynamics\n$$\n\\frac{dY}{dt} = f(Y,t;\\theta) = -k(t;\\theta)\\,Y(t),\n$$\nwhere the reaction rate\n$$\nk(t;\\theta) = A\\,\\exp\\!\\left(-\\frac{E}{R\\,T(t)}\\right),\n$$\n$A$ is the pre-exponential factor, $E$ is the activation energy, $R$ is the universal gas constant, and $T(t)$ is a prescribed temperature history in Kelvin. Let the initial condition be $Y(0)=1$ and the temperature be a linear ramp,\n$$\nT(t) = T_0 + \\frac{T_f - T_0}{t_f}\\,t,\\quad t\\in[0,t_f],\n$$\nwith $t_f>0$. Observations are available at a discrete set of times $\\{t_i\\}$, given as $y_i = Y(t_i;\\theta_{\\text{true}}) + \\eta_i$, where $\\eta_i$ are independent Gaussian errors with zero mean and standard deviation $\\sigma$. We aim to calibrate the parameters $\\theta=(A,E)$ given these observations under a Bayesian model with a Gaussian prior on the transformed parameters $\\phi = (\\phi_A,\\phi_E) = (\\ln A, \\ln E)$,\n$$\n\\phi \\sim \\mathcal{N}(\\phi_0, \\Sigma_0),\n$$\nwhere $\\phi_0$ and $\\Sigma_0$ are specified.\n\nDefine the negative log-posterior (up to an additive constant) for $\\phi$ by\n$$\nJ(\\phi) = \\frac{1}{2}\\sum_{i} \\frac{\\left(Y(t_i;\\theta(\\phi)) - y_i\\right)^2}{\\sigma^2} + \\frac{1}{2}(\\phi - \\phi_0)^\\top \\Sigma_0^{-1}(\\phi - \\phi_0),\n$$\nwith the transformation $\\theta(\\phi)=(\\exp(\\phi_A),\\exp(\\phi_E))$. Let the forward model $Y(t;\\theta)$ be computed numerically using a stable time-stepping method on a uniform grid.\n\nAdjoint sensitivity analysis is to be used to compute the gradient of the data misfit term with respect to $\\theta$. For the continuous-time scalar adjoint $\\lambda(t)$ corresponding to the discrete-time misfit\n$$\n\\mathcal{J}_{\\text{data}}(\\theta) = \\frac{1}{2}\\sum_i \\frac{\\left(Y(t_i;\\theta)-y_i\\right)^2}{\\sigma^2},\n$$\nthe adjoint satisfies the terminal condition $\\lambda(t_f)=0$, the differential equation\n$$\n\\frac{d\\lambda}{dt} = -\\frac{\\partial f}{\\partial Y}(Y(t),t;\\theta)\\,\\lambda(t),\n$$\nand jump conditions at observation times $t_i$,\n$$\n\\lambda(t_i^-) = \\lambda(t_i^+) + \\frac{Y(t_i;\\theta)-y_i}{\\sigma^2}.\n$$\nThe gradient of the data misfit with respect to parameters is\n$$\n\\frac{\\partial \\mathcal{J}_{\\text{data}}}{\\partial \\theta} = \\int_0^{t_f} \\lambda(t)\\,\\frac{\\partial f}{\\partial \\theta}(Y(t),t;\\theta)\\,dt,\n$$\nand the gradient with respect to $\\phi$ follows by the chain rule:\n$$\n\\frac{\\partial \\mathcal{J}_{\\text{data}}}{\\partial \\phi_A} = A\\,\\frac{\\partial \\mathcal{J}_{\\text{data}}}{\\partial A},\\quad\n\\frac{\\partial \\mathcal{J}_{\\text{data}}}{\\partial \\phi_E} = E\\,\\frac{\\partial \\mathcal{J}_{\\text{data}}}{\\partial E}.\n$$\nThe prior gradient is $\\frac{\\partial \\mathcal{J}_{\\text{prior}}}{\\partial \\phi} = \\Sigma_0^{-1}(\\phi - \\phi_0)$.\n\nTasks:\n- Implement Hamiltonian Monte Carlo (HMC) using the adjoint-derived gradient of $J(\\phi)$ to sample from the posterior over $\\phi$.\n- Implement a gradient-free Random-Walk Metropolis (RWM) sampler on $\\phi$ with Gaussian proposals to sample from the same posterior.\n- Generate synthetic observations $y_i$ using a specified ground truth $\\theta_{\\text{true}}$ and Gaussian noise with standard deviation $\\sigma$.\n\nUse the following constants, units, and discretization:\n- Universal gas constant $R = 8.314 J/(mol·K)$.\n- Time horizon $t_f = 0.02\\ \\text{s}$ uniformly discretized into $N=200$ steps, so $\\Delta t = t_f/N$.\n- Observation times at indices $i \\in \\{20, 60, 100, 140, 180\\}$, corresponding to $t_i = i\\,\\Delta t$.\n- Initial condition $Y(0)=1$.\n- Ground truth parameters $A_{\\text{true}}=10^5\\ \\text{s}^{-1}$ and $E_{\\text{true}}=8.0\\times 10^4\\ \\text{J/mol}$.\n- Prior mean $\\phi_0 = (\\ln(10^5), \\ln(8.0\\times 10^4))$ and prior covariance\n$$\n\\Sigma_0 = \\begin{bmatrix} 0.5^2 & 0 \\\\ 0 & 0.4^2 \\end{bmatrix}.\n$$\n\nDesign three test cases (temperature schedule and noise level):\n- Case 1 (baseline): $T_0=1500\\ \\text{K}$, $T_f=1800\\ \\text{K}$, $\\sigma=0.02$.\n- Case 2 (lower temperature, weaker kinetics): $T_0=1200\\ \\text{K}$, $T_f=1400\\ \\text{K}$, $\\sigma=0.02$.\n- Case 3 (higher temperature, more informative data): $T_0=1800\\ \\text{K}$, $T_f=2000\\ \\text{K}$, $\\sigma=0.005$.\n\nSampling requirements:\n- Use Hamiltonian Monte Carlo (HMC) with identity mass matrix, leapfrog step size $\\epsilon=0.02$, and $L=10$ steps per proposal; draw $150$ samples and discard the first $30$ as burn-in.\n- Use Random-Walk Metropolis (RWM) with Gaussian proposals on $\\phi$ having independent standard deviations $0.05$ for each component; draw $300$ samples and discard the first $60$ as burn-in.\n\nFor each case, estimate the posterior variances of $A$ and $E$ from the HMC samples and from the RWM samples and compute the variance reductions as decimal fractions\n$$\nr_A = 1 - \\frac{\\widehat{\\mathrm{Var}}_{\\mathrm{HMC}}(A)}{\\widehat{\\mathrm{Var}}_{\\mathrm{RWM}}(A)},\\quad\nr_E = 1 - \\frac{\\widehat{\\mathrm{Var}}_{\\mathrm{HMC}}(E)}{\\widehat{\\mathrm{Var}}_{\\mathrm{RWM}}(E)}.\n$$\nThese $r_A$ and $r_E$ are dimensionless.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain six floating-point numbers in the following order:\n$$\n[\\ r_A^{(1)},\\ r_E^{(1)},\\ r_A^{(2)},\\ r_E^{(2)},\\ r_A^{(3)},\\ r_E^{(3)}\\ ],\n$$\nwhere the superscript denotes the case number.\n\nAll physical quantities must be computed in their natural units as specified above (Kelvin for temperature and seconds for time). The final outputs are dimensionless fractions. Angles are not involved in this problem.\n\nThe solution must be a complete, runnable program that implements the forward model, adjoint-based gradient computation, Hamiltonian Monte Carlo (HMC), and Random-Walk Metropolis (RWM), then executes the three test cases and prints the results in the exact format described.",
            "solution": "The user-provided problem has been assessed and is determined to be **valid**. It is scientifically grounded, well-posed, objective, and contains all necessary information to proceed with a unique and verifiable solution.\n\nThe problem requires the implementation and comparison of two Bayesian sampling algorithms—Random-Walk Metropolis (RWM) and Hamiltonian Monte Carlo (HMC)—for parameter inference in a simplified combustion model. The goal is to quantify the efficiency gain of the gradient-based HMC method over the gradient-free RWM method by comparing the variance of the resulting posterior parameter samples. The gradient required for HMC will be computed efficiently using the adjoint sensitivity analysis method.\n\nThe core of the problem is a single-step, first-order chemical reaction whose state is described by a progress variable $Y(t)$. The dynamics are governed by the ordinary differential equation (ODE):\n$$\n\\frac{dY}{dt} = -k(t;\\theta)\\,Y(t)\n$$\nwith the initial condition $Y(0)=1$. The parameters to be inferred are $\\theta = (A, E)$, which are the pre-exponential factor and activation energy in the Arrhenius reaction rate expression:\n$$\nk(t;\\theta) = A\\,\\exp\\!\\left(-\\frac{E}{R\\,T(t)}\\right)\n$$\nThe temperature $T(t)$ follows a prescribed linear ramp, and $R$ is the universal gas constant.\n\nThe solution will be developed through the following steps:\n1.  **Numerical Forward Model**: Discretize and solve the governing ODE for $Y(t)$ given a set of parameters $\\theta$.\n2.  **Bayesian Formulation**: Define the negative log-posterior probability density function, which serves as the target for the MCMC samplers.\n3.  **Adjoint-Based Gradient Calculation**: Implement the adjoint method to compute the gradient of the negative log-posterior with respect to the parameters. This is essential for the HMC sampler.\n4.  **MCMC Samplers**: Implement both RWM and HMC algorithms.\n5.  **Analysis and Comparison**: For each specified test case, generate synthetic data, run both samplers, and compute the required variance reduction ratios.\n\n**1. Numerical Forward Model**\n\nThe time domain $[0, t_f]$ is discretized into $N$ uniform steps of size $\\Delta t = t_f/N$. The ODE for $Y(t)$ is solved on this grid. A simple and stable choice for the numerical integrator is the Forward Euler method. At each time step $t_n = n\\,\\Delta t$, the update rule is:\n$$\nY_{n+1} = Y_n + \\Delta t \\cdot f(Y_n, t_n; \\theta) = Y_n - \\Delta t \\cdot k(t_n;\\theta) Y_n = Y_n (1 - \\Delta t \\cdot k_n)\n$$\nwhere $k_n = k(t_n; \\theta)$. This scheme is numerically stable if $|\\,1 - \\Delta t \\cdot k_n\\,| \\le 1$, which simplifies to $\\Delta t \\cdot k_n \\le 2$ since $k_n > 0$. For the given parameters, this condition is satisfied, making Forward Euler an adequate choice.\n\n**2. Bayesian Formulation**\n\nThe inference is performed on the transformed parameters $\\phi = (\\ln A, \\ln E)$. The posterior distribution is proportional to the likelihood multiplied by the prior. The negative log-posterior, $J(\\phi)$, up to an additive constant, is the sum of a data misfit term (negative log-likelihood) and a prior term (negative log-prior):\n$$\nJ(\\phi) = J_{\\text{data}}(\\phi) + J_{\\text{prior}}(\\phi)\n$$\nThe data misfit term, assuming independent Gaussian observation errors, is:\n$$\nJ_{\\text{data}}(\\phi) = \\frac{1}{2}\\sum_{i} \\frac{\\left(Y(t_i;\\theta(\\phi)) - y_i\\right)^2}{\\sigma^2}\n$$\nwhere $y_i$ are the observations at times $t_i$. The prior on $\\phi$ is Gaussian, $\\phi \\sim \\mathcal{N}(\\phi_0, \\Sigma_0)$, so the prior term is:\n$$\nJ_{\\text{prior}}(\\phi) = \\frac{1}{2}(\\phi - \\phi_0)^\\top \\Sigma_0^{-1}(\\phi - \\phi_0)\n$$\n\n**3. Adjoint-Based Gradient Calculation**\n\nHMC requires the gradient of the potential energy, $\\nabla_\\phi J(\\phi)$. The gradient of the prior term is straightforward: $\\nabla_\\phi J_{\\text{prior}}(\\phi) = \\Sigma_0^{-1}(\\phi - \\phi_0)$. The gradient of the data misfit term is computed using the continuous adjoint method.\n\nThe procedure involves three stages:\na. **Forward Solve**: The state equation for $Y(t)$ is solved forward in time from $t=0$ to $t=t_f$ and the trajectory is stored.\nb. **Backward Adjoint Solve**: The adjoint variable $\\lambda(t)$ is solved backward in time from $t=t_f$ to $t=0$. The adjoint equation is:\n$$\n\\frac{d\\lambda}{dt} = -\\frac{\\partial f}{\\partial Y}(Y(t),t;\\theta)\\,\\lambda(t) = k(t;\\theta)\\,\\lambda(t)\n$$\nwith terminal condition $\\lambda(t_f)=0$. At each observation time $t_i$, the adjoint variable experiences a jump:\n$$\n\\lambda(t_i^-) = \\lambda(t_i^+) + \\frac{Y(t_i;\\theta)-y_i}{\\sigma^2}\n$$\nWe integrate this ODE backward in time. Due to its structure, an explicit Euler scheme would be unstable. We use an implicit (Backward) Euler scheme for unconditional stability. For a time step from $t_n$ to $t_{n-1}$, the update is:\n$$\n\\frac{\\lambda_n - \\lambda_{n-1}}{\\Delta t} = k_{n-1} \\lambda_{n-1} \\implies \\lambda_{n-1} = \\frac{\\lambda_n}{1 + \\Delta t \\cdot k_{n-1}}\n$$\nThe jump condition is applied at each observation index by adding the forcing term to $\\lambda$ before the next integration step.\n\nc. **Gradient Integral**: The gradients with respect to the original parameters $\\theta=(A,E)$ are computed by integrating the product of the adjoint variable and the sensitivity of the right-hand side function $f$ with respect to each parameter:\n$$\n\\frac{\\partial J_{\\text{data}}}{\\partial A} = \\int_0^{t_f} \\lambda(t) \\frac{\\partial f}{\\partial A} dt = \\int_0^{t_f} \\lambda(t) \\left(-\\frac{k(t)}{A} Y(t)\\right) dt\n$$\n$$\n\\frac{\\partial J_{\\text{data}}}{\\partial E} = \\int_0^{t_f} \\lambda(t) \\frac{\\partial f}{\\partial E} dt = \\int_0^{t_f} \\lambda(t) \\left(\\frac{k(t)}{R T(t)} Y(t)\\right) dt\n$$\nThese integrals are computed numerically using the trapezoidal rule over the discretized time grid.\n\nFinally, the chain rule is applied to obtain the gradient with respect to the transformed parameters $\\phi$:\n$$\n\\frac{\\partial J_{\\text{data}}}{\\partial \\phi_A} = \\frac{\\partial J_{\\text{data}}}{\\partial A} \\frac{\\partial A}{\\partial \\phi_A} = A \\frac{\\partial J_{\\text{data}}}{\\partial A}, \\qquad \\frac{\\partial J_{\\text{data}}}{\\partial \\phi_E} = E \\frac{\\partial J_{\\text{data}}}{\\partial E}\n$$\nThe total gradient is then $\\nabla_\\phi J = \\nabla_\\phi J_{\\text{data}} + \\nabla_\\phi J_{\\text{prior}}$.\n\n**4. MCMC Samplers**\n\na. **Random-Walk Metropolis (RWM)**: This gradient-free method generates a new candidate state $\\phi_{\\text{prop}}$ by adding a random perturbation to the current state $\\phi_{\\text{curr}}$, i.e., $\\phi_{\\text{prop}} = \\phi_{\\text{curr}} + \\mathcal{N}(0, \\Sigma_{\\text{prop}})$. The candidate is accepted with probability $\\alpha = \\min\\left(1, \\exp(J(\\phi_{\\text{curr}}) - J(\\phi_{\\text{prop}}))\\right)$.\n\nb. **Hamiltonian Monte Carlo (HMC)**: This gradient-based method improves sampling efficiency by introducing an auxiliary momentum variable $p$. It simulates the dynamics of a fictitious particle on the potential energy surface $U(\\phi) = J(\\phi)$. A proposal is generated by evolving the system $(\\phi, p)$ for a time $L\\epsilon$ using a symplectic integrator, typically the leapfrog method. The proposal is then accepted or rejected based on the change in the total Hamiltonian $H(\\phi, p) = U(\\phi) + K(p)$, where $K(p)$ is the kinetic energy. The use of gradients allows HMC to make large, directed moves in the parameter space, often leading to faster convergence and less correlated samples compared to RWM.\n\n**5. Analysis and Comparison**\n\nFor each of the three test cases, we first generate synthetic data using the true parameters $\\theta_{\\text{true}}$ and specified noise level $\\sigma$. Then, we run both the RWM and HMC samplers to generate a chain of samples from the posterior distribution of $\\phi$. After discarding an initial burn-in period, the remaining samples are transformed back to the original parameter space $(A,E)$ via exponentiation. The sample variances of the $A$ and $E$ posteriors are estimated from both chains. The variance reduction ratio, for a parameter $\\xi \\in \\{A, E\\}$, is computed as:\n$$\nr_\\xi = 1 - \\frac{\\widehat{\\mathrm{Var}}_{\\mathrm{HMC}}(\\xi)}{\\widehat{\\mathrm{Var}}_{\\mathrm{RWM}}(\\xi)}\n$$\nA positive value of $r_\\xi$ indicates that the HMC sampler produced a lower-variance estimate, suggesting greater sampling efficiency. The final output consists of the computed values of $r_A$ and $r_E$ for all three test cases.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the final result.\n    \"\"\"\n    # Set a seed for reproducibility of synthetic data and MCMC runs\n    np.random.seed(42)\n\n    # --- Global Constants and Setup ---\n    R = 8.314  # J/(mol·K)\n    t_f = 0.02  # s\n    N = 200  # Number of time steps\n    dt = t_f / N\n    t_grid = np.linspace(0, t_f, N + 1)\n    \n    # Observation time indices\n    obs_indices = np.array([20, 60, 100, 140, 180])\n\n    # Ground truth parameters\n    A_true = 1e5\n    E_true = 8.0e4\n    phi_true = np.array([np.log(A_true), np.log(E_true)])\n\n    # Prior distribution parameters\n    phi_0 = np.array([np.log(1e5), np.log(8.0e4)])\n    Sigma_0 = np.array([[0.5**2, 0], [0, 0.4**2]])\n    Sigma0_inv = np.linalg.inv(Sigma_0)\n\n    # MCMC settings\n    hmc_params = {'n_samples': 150, 'n_burn': 30, 'epsilon': 0.02, 'L': 10}\n    rwm_params = {'n_samples': 300, 'n_burn': 60, 'proposal_std': np.array([0.05, 0.05])}\n\n    # Test cases\n    test_cases = [\n        {'T0': 1500.0, 'Tf': 1800.0, 'sigma': 0.02},  # Case 1\n        {'T0': 1200.0, 'Tf': 1400.0, 'sigma': 0.02},  # Case 2\n        {'T0': 1800.0, 'Tf': 2000.0, 'sigma': 0.005}, # Case 3\n    ]\n\n    results = []\n\n    # --- Core Functions ---\n\n    def forward_model(phi, T_t):\n        \"\"\"Solves the forward ODE using Forward Euler.\"\"\"\n        A = np.exp(phi[0])\n        E = np.exp(phi[1])\n        \n        Y = np.zeros(N + 1)\n        Y[0] = 1.0\n        \n        k_t = A * np.exp(-E / (R * T_t))\n\n        for n in range(N):\n            Y[n + 1] = Y[n] * (1.0 - dt * k_t[n])\n        return Y, k_t\n\n    def get_log_posterior_and_grad(phi, T_t, y_obs, sigma):\n        \"\"\"\n        Computes the negative log posterior and its gradient using the adjoint method.\n        \"\"\"\n        A = np.exp(phi[0])\n        E = np.exp(phi[1])\n\n        # 1. Forward solve\n        Y_t, k_t = forward_model(phi, T_t)\n        \n        # 2. Compute log posterior value\n        Y_model_obs = Y_t[obs_indices]\n        misfit = 0.5 * np.sum(((Y_model_obs - y_obs) / sigma)**2)\n        prior_term = 0.5 * (phi - phi_0).T @ Sigma0_inv @ (phi - phi_0)\n        log_post_val = misfit + prior_term\n\n        # 3. Adjoint solve (backward in time)\n        lam = np.zeros(N + 1)\n        obs_map = {idx: i for i, idx in enumerate(obs_indices)}\n        \n        for n in range(N, 0, -1):\n            lam_plus = lam[n]\n            if n in obs_map:\n                obs_idx = obs_map[n]\n                lam_plus += (Y_t[n] - y_obs[obs_idx]) / sigma**2\n            \n            lam[n - 1] = lam_plus / (1.0 + dt * k_t[n - 1])\n        \n        # 4. Gradient integral calculation\n        integrand_A = lam * (-k_t / A * Y_t)\n        # Handle potential division by zero if T=0, though not possible in this problem setup\n        T_t_safe = np.maximum(T_t, 1e-6)\n        integrand_E = lam * (k_t / (R * T_t_safe) * Y_t)\n        \n        grad_J_data_A = np.trapz(integrand_A, x=t_grid)\n        grad_J_data_E = np.trapz(integrand_E, x=t_grid)\n\n        # 5. Chain rule for gradient wrt phi\n        grad_J_data_phi_A = A * grad_J_data_A\n        grad_J_data_phi_E = E * grad_J_data_E\n        grad_J_data_phi = np.array([grad_J_data_phi_A, grad_J_data_phi_E])\n        \n        # 6. Add prior gradient\n        grad_J_prior_phi = Sigma0_inv @ (phi - phi_0)\n        \n        total_grad = grad_J_data_phi + grad_J_prior_phi\n        \n        return log_post_val, total_grad\n\n    def hmc_sampler(initial_phi, T_t, y_obs, sigma):\n        \"\"\"Hamiltonian Monte Carlo sampler.\"\"\"\n        n_samples = hmc_params['n_samples']\n        epsilon = hmc_params['epsilon']\n        L = hmc_params['L']\n        \n        samples = np.zeros((n_samples, 2))\n        phi = np.copy(initial_phi)\n        \n        current_U, grad_U = get_log_posterior_and_grad(phi, T_t, y_obs, sigma)\n        \n        for i in range(n_samples):\n            p = np.random.normal(size=2)\n            current_K = 0.5 * np.sum(p**2)\n            \n            phi_prop, p_prop = np.copy(phi), np.copy(p)\n            \n            # Leapfrog integration\n            p_prop -= 0.5 * epsilon * grad_U\n            for _ in range(L):\n                phi_prop += epsilon * p_prop\n                _, grad_U_prop = get_log_posterior_and_grad(phi_prop, T_t, y_obs, sigma)\n                if _  L - 1:\n                    p_prop -= epsilon * grad_U_prop\n            p_prop -= 0.5 * epsilon * grad_U_prop\n            \n            p_prop = -p_prop # Negate momentum for symmetric proposal\n            \n            proposed_U, _ = get_log_posterior_and_grad(phi_prop, T_t, y_obs, sigma)\n            proposed_K = 0.5 * np.sum(p_prop**2)\n            \n            # Acceptance probability\n            delta_H = proposed_U - current_U + proposed_K - current_K\n            if np.log(np.random.rand())  -delta_H:\n                phi = phi_prop\n                current_U = proposed_U\n                grad_U = grad_U_prop\n            \n            samples[i] = phi\n        \n        return samples\n\n    def rwm_sampler(initial_phi, T_t, y_obs, sigma):\n        \"\"\"Random-Walk Metropolis sampler.\"\"\"\n        n_samples = rwm_params['n_samples']\n        proposal_std = rwm_params['proposal_std']\n        \n        samples = np.zeros((n_samples, 2))\n        phi = np.copy(initial_phi)\n        \n        current_log_post, _ = get_log_posterior_and_grad(phi, T_t, y_obs, sigma)\n        \n        for i in range(n_samples):\n            proposal = phi + np.random.normal(loc=0, scale=proposal_std)\n            proposed_log_post, _ = get_log_posterior_and_grad(proposal, T_t, y_obs, sigma)\n            \n            log_alpha = current_log_post - proposed_log_post\n            if np.log(np.random.rand())  log_alpha:\n                phi = proposal\n                current_log_post = proposed_log_post\n            \n            samples[i] = phi\n        \n        return samples\n\n    # --- Main Loop for Test Cases ---\n    for case in test_cases:\n        T0, Tf, sigma = case['T0'], case['Tf'], case['sigma']\n        T_t = T0 + (Tf - T0) / t_f * t_grid\n        \n        # 1. Generate synthetic data\n        Y_true, _ = forward_model(phi_true, T_t)\n        Y_obs_true = Y_true[obs_indices]\n        noise = np.random.normal(0, sigma, size=len(obs_indices))\n        y_obs = Y_obs_true + noise\n\n        # 2. Run samplers\n        initial_phi = np.copy(phi_0)\n        hmc_samples_phi = hmc_sampler(initial_phi, T_t, y_obs, sigma)\n        rwm_samples_phi = rwm_sampler(initial_phi, T_t, y_obs, sigma)\n\n        # 3. Post-processing and analysis\n        hmc_burn = hmc_params['n_burn']\n        rwm_burn = rwm_params['n_burn']\n        \n        hmc_samples_phi_analysis = hmc_samples_phi[hmc_burn:]\n        rwm_samples_phi_analysis = rwm_samples_phi[rwm_burn:]\n\n        # Convert samples from phi=(lnA, lnE) to theta=(A,E)\n        hmc_samples_A = np.exp(hmc_samples_phi_analysis[:, 0])\n        hmc_samples_E = np.exp(hmc_samples_phi_analysis[:, 1])\n        rwm_samples_A = np.exp(rwm_samples_phi_analysis[:, 0])\n        rwm_samples_E = np.exp(rwm_samples_phi_analysis[:, 1])\n        \n        # Calculate sample variances\n        var_A_hmc = np.var(hmc_samples_A, ddof=1)\n        var_E_hmc = np.var(hmc_samples_E, ddof=1)\n        var_A_rwm = np.var(rwm_samples_A, ddof=1)\n        var_E_rwm = np.var(rwm_samples_E, ddof=1)\n        \n        # Calculate variance reduction ratios\n        r_A = 1.0 - var_A_hmc / var_A_rwm\n        r_E = 1.0 - var_E_hmc / var_E_rwm\n        \n        results.extend([r_A, r_E])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}