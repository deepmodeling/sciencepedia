{
    "hands_on_practices": [
        {
            "introduction": "This practice focuses on the fundamental concept of forward uncertainty propagation. We begin with the Arrhenius equation, a cornerstone of chemical kinetics, and consider its parameters—the pre-exponential factor $A$ and activation energy $E$—as uncertain inputs described by probability distributions. The goal is to analytically derive the resulting probability distribution of the reaction rate constant $k$, providing a rigorous understanding of how uncertainties in model parameters translate into uncertainty in model predictions . This exercise builds a crucial foundation for all subsequent UQ analysis.",
            "id": "4075374",
            "problem": "In a single-step irreversible gas-phase reaction, the temperature-dependent rate coefficient is modeled by the Arrhenius law, $k(T)=A\\exp(-E/(R T))$, where $A$ is the pre-exponential factor, $E$ is the activation energy, $R$ is the universal gas constant, and $T$ is the absolute temperature. At a fixed temperature $T>0$, suppose that the uncertainty in $A$ and $E$ arises from unresolved multiscale physics and measurement variability and is modeled as follows: $\\ln A$ is normally distributed with mean $\\mu_A$ and variance $\\sigma_A^2$, and $E$ is normally distributed with mean $\\mu_E$ and variance $\\sigma_E^2$. Assume $A$ and $E$ are statistically independent. Your task is to transform these uncertainties through the Arrhenius relation and deduce the induced uncertainty in $k$ at the fixed temperature.\n\nStarting from the Arrhenius law and first principles of probability transformation, derive the probability density function of $k$ at the fixed temperature. Express your final result in closed form in terms of $\\mu_A$, $\\sigma_A$, $\\mu_E$, $\\sigma_E$, $R$, and $T$. No numerical evaluation is required. Provide the final probability density function for $k$ as a single analytic expression. Do not include units in the final answer. No rounding is required.",
            "solution": "The problem is first validated against the specified criteria.\n\n**Step 1: Extract Givens**\n- The Arrhenius law: $k(T)=A\\exp(-E/(R T))$\n- Temperature $T$ is a fixed positive constant, $T>0$.\n- The Universal Gas Constant is $R$.\n- The logarithm of the pre-exponential factor, $\\ln A$, is a random variable following a normal distribution with mean $\\mu_A$ and variance $\\sigma_A^2$. This can be written as $\\ln A \\sim \\mathcal{N}(\\mu_A, \\sigma_A^2)$.\n- The activation energy, $E$, is a random variable following a normal distribution with mean $\\mu_E$ and variance $\\sigma_E^2$. This can be written as $E \\sim \\mathcal{N}(\\mu_E, \\sigma_E^2)$.\n- The random variables $A$ and $E$ are statistically independent.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem uses the Arrhenius equation, a fundamental principle in chemical kinetics. The modeling of parameter uncertainty using probability distributions (specifically log-normal for $A$ and normal for $E$) is a standard and physically justified practice in uncertainty quantification for combustion and chemical systems. The problem is scientifically sound.\n- **Well-Posed:** The problem provides a clear functional relationship and complete probabilistic descriptions of the input random variables. The request to derive the probability density function (PDF) of the output is a standard problem in probability theory (transformation of random variables) and has a unique solution.\n- **Objective:** The problem is stated in precise, mathematical language, free from any subjectivity or ambiguity.\n- **Consistency and Completeness:** All necessary information is provided. The assumption of statistical independence is crucial and explicitly stated. There are no contradictions. The condition $T>0$ ensures the expression is well-defined.\n- **Feasibility:** The scenario is a realistic representation of uncertainty analysis in computational models.\n\n**Verdict:** The problem is valid.\n\n**Step 3: Derivation of the Solution**\nThe goal is to derive the probability density function (PDF) of the rate coefficient $k$, denoted as $f_k(k)$. The rate coefficient is given by the Arrhenius law:\n$$k = A \\exp\\left(-\\frac{E}{RT}\\right)$$\nTo simplify the product and exponential forms, we take the natural logarithm of $k$:\n$$\\ln k = \\ln\\left(A \\exp\\left(-\\frac{E}{RT}\\right)\\right)$$\nUsing the properties of logarithms, we get:\n$$\\ln k = \\ln A - \\frac{E}{RT}$$\nLet us define a new random variable $Y = \\ln k$. We can express $Y$ as the difference of two random variables, $Y = X_1 - X_2$, where:\n$$X_1 = \\ln A$$\n$$X_2 = \\frac{E}{RT}$$\nWe are given the distribution of $X_1$. Since $\\ln A$ is normally distributed with mean $\\mu_A$ and variance $\\sigma_A^2$, we have:\n$$X_1 \\sim \\mathcal{N}(\\mu_A, \\sigma_A^2)$$\nNext, we determine the distribution of $X_2$. $X_2$ is a linear transformation of the random variable $E$, where the scaling factor is $1/(RT)$. Since $E$ is normally distributed, $E \\sim \\mathcal{N}(\\mu_E, \\sigma_E^2)$, the random variable $X_2$ is also normally distributed. Its mean and variance are:\n$$\\mathbb{E}[X_2] = \\mathbb{E}\\left[\\frac{E}{RT}\\right] = \\frac{1}{RT}\\mathbb{E}[E] = \\frac{\\mu_E}{RT}$$\n$$\\mathrm{Var}(X_2) = \\mathrm{Var}\\left(\\frac{E}{RT}\\right) = \\left(\\frac{1}{RT}\\right)^2 \\mathrm{Var}(E) = \\frac{\\sigma_E^2}{R^2 T^2}$$\nThus, the distribution of $X_2$ is:\n$$X_2 \\sim \\mathcal{N}\\left(\\frac{\\mu_E}{RT}, \\frac{\\sigma_E^2}{R^2 T^2}\\right)$$\nThe problem states that $A$ and $E$ are statistically independent. This implies that any functions of these variables, such as $X_1 = \\ln A$ and $X_2 = E/(RT)$, are also statistically independent.\nThe random variable $Y = \\ln k$ is the difference of two independent normal random variables, $X_1$ and $X_2$. The sum or difference of independent normal random variables is itself a normal random variable. The mean of $Y$, denoted $\\mu_Y$, is the difference of the means of $X_1$ and $X_2$:\n$$\\mu_Y = \\mathbb{E}[Y] = \\mathbb{E}[X_1 - X_2] = \\mathbb{E}[X_1] - \\mathbb{E}[X_2] = \\mu_A - \\frac{\\mu_E}{RT}$$\nThe variance of $Y$, denoted $\\sigma_Y^2$, is the sum of the variances of $X_1$ and $X_2$ (due to their independence):\n$$\\sigma_Y^2 = \\mathrm{Var}(Y) = \\mathrm{Var}(X_1 - X_2) = \\mathrm{Var}(X_1) + \\mathrm{Var}(X_2) = \\sigma_A^2 + \\frac{\\sigma_E^2}{R^2 T^2}$$\nSo, the random variable $Y = \\ln k$ follows a normal distribution:\n$$Y = \\ln k \\sim \\mathcal{N}\\left(\\mu_A - \\frac{\\mu_E}{RT}, \\sigma_A^2 + \\frac{\\sigma_E^2}{R^2 T^2}\\right)$$\nA random variable $k$ whose natural logarithm is normally distributed is said to follow a log-normal distribution. The PDF of a log-normal random variable $k$, for which $\\ln k \\sim \\mathcal{N}(\\mu_Y, \\sigma_Y^2)$, is given by the general formula:\n$$f_k(k) = \\frac{1}{k \\sigma_Y \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln k - \\mu_Y)^2}{2\\sigma_Y^2}\\right), \\quad \\text{for } k>0$$\nSubstituting our derived expressions for $\\mu_Y$ and $\\sigma_Y^2$ into this formula gives the PDF of $k$:\n$$f_k(k) = \\frac{1}{k \\sqrt{2\\pi \\left(\\sigma_A^2 + \\frac{\\sigma_E^2}{R^2 T^2}\\right)}} \\exp\\left(-\\frac{\\left(\\ln k - \\left(\\mu_A - \\frac{\\mu_E}{RT}\\right)\\right)^2}{2\\left(\\sigma_A^2 + \\frac{\\sigma_E^2}{R^2 T^2}\\right)}\\right)$$\nThis can be written more cleanly as:\n$$f_k(k) = \\frac{1}{k \\sqrt{2\\pi\\left(\\sigma_A^2 + \\frac{\\sigma_E^2}{R^2T^2}\\right)}} \\exp\\left( - \\frac{\\left( \\ln k - \\mu_A + \\frac{\\mu_E}{RT} \\right)^2}{2\\left(\\sigma_A^2 + \\frac{\\sigma_E^2}{R^2T^2}\\right)} \\right)$$\nThis is the final closed-form expression for the probability density function of the rate coefficient $k$.",
            "answer": "$$\n\\boxed{\n\\frac{1}{k \\sqrt{2\\pi\\left(\\sigma_A^2 + \\frac{\\sigma_E^2}{R^2T^2}\\right)}} \\exp\\left( - \\frac{\\left( \\ln k - \\mu_A + \\frac{\\mu_E}{RT} \\right)^2}{2\\left(\\sigma_A^2 + \\frac{\\sigma_E^2}{R^2T^2}\\right)} \\right)\n}\n$$"
        },
        {
            "introduction": "After seeing how input uncertainty propagates, we now address the inverse problem: how do we determine the uncertainty in model parameters from experimental data? This practice explores the critical concept of parameter identifiability using the Arrhenius model. By analyzing synthetic experimental data, you will learn how the design of an experiment (e.g., the range of temperatures tested) directly impacts the confidence we can have in the estimated parameters, a crucial consideration for building credible combustion models .",
            "id": "4075321",
            "problem": "Consider a single-step, elementary reaction in a homogeneous mixture modeled by the Arrhenius rate expression. Let the rate constant be $k(T) = A \\exp\\!\\left(-\\frac{E}{R T}\\right)$, where $A$ is the pre-exponential factor, $E$ is the activation energy, $R$ is the universal gas constant with units $\\mathrm{J/(mol\\cdot K)}$, and $T$ is the absolute temperature in $\\mathrm{K}$. To produce dimensionless parameters suitable for statistical inference, define the transformed parameter vector $\\boldsymbol{\\theta} = (\\theta_1, \\theta_2)$ with $\\theta_1 = \\ln A$ and $\\theta_2 = \\dfrac{E}{R T_0}$, where $T_0$ is a fixed reference temperature in $\\mathrm{K}$. Under this reparameterization, the logarithm of the rate constant is\n$$\n\\ln k(T) = \\theta_1 - \\theta_2 \\frac{T_0}{T}.\n$$\nAssume the measurement model in logarithmic space is additive Gaussian noise: for each temperature $T_i$, the observed quantity $y_i$ satisfies\n$$\ny_i = \\ln k(T_i) + \\varepsilon_i,\\quad \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_y^2),\n$$\nwhere $\\sigma_y$ is known and independent of $i$. Adopt a uniform prior on $\\boldsymbol{\\theta}$ so that the posterior is proportional to the likelihood. The goal is to quantify uncertainty in the parameter pair $(\\theta_1,\\theta_2)$ and to demonstrate non-identifiability when the temperature set spans a narrow range.\n\nStarting from the Arrhenius law and the Gaussian noise assumption, derive the linear regression representation in the transformed parameter space and compute the Maximum Likelihood Estimate (MLE) of $\\boldsymbol{\\theta}$. Using the Laplace approximation to the posterior around the MLE, characterize the joint posterior as a bivariate normal distribution with covariance matrix derived from the design matrix of the linear model. From this covariance, construct the $95\\%$ joint confidence (credible) ellipse for $(\\theta_1,\\theta_2)$, and extract the following quantifiable characteristics:\n- the semi-major axis length $a$ and semi-minor axis length $b$ of the $95\\%$ ellipse (both dimensionless),\n- the orientation angle $\\alpha$ in radians of the major axis with respect to the $\\theta_1$ axis (angle unit must be radians),\n- the posterior correlation coefficient $\\rho$ between $\\theta_1$ and $\\theta_2$ (dimensionless),\n- the axis ratio $\\kappa = a/b$ (dimensionless),\n- the condition number $\\mathrm{cond}((X^\\top X))$ of the normal matrix (dimensionless), which serves as a diagnostic for identifiability.\n\nUse $R = 8.314\\,\\mathrm{J/(mol\\cdot K)}$ and $T_0 = 1000\\,\\mathrm{K}$. Generate synthetic, noise-free observations by evaluating $\\ln k(T_i)$ for a given ground truth $(\\theta_1^\\star, \\theta_2^\\star)$ so that the MLE equals the ground truth. The uncertainty characteristics depend on the design matrix and $\\sigma_y$, not on the particular realization of the noise. Use ground truth values corresponding to $A^\\star = 10^{12}\\,\\mathrm{s}^{-1}$ and $E^\\star = 1.5\\times 10^5\\,\\mathrm{J/mol}$. This implies $\\theta_1^\\star = \\ln A^\\star$ and $\\theta_2^\\star = \\dfrac{E^\\star}{R T_0}$.\n\nYour program must compute the above five quantities for each of the following test cases, which explore different temperature ranges and noise levels to assess identifiability:\n\n- Test Case 1 (wide temperature span, moderate noise): temperatures $T_i = [800, 900, 1000, 1200, 1400, 1600, 1800, 2000]$ in $\\mathrm{K}$, $\\sigma_y = 0.1$ (dimensionless).\n- Test Case 2 (narrow temperature span, moderate noise): temperatures $T_i = [1100, 1120, 1140, 1160, 1180, 1200]$ in $\\mathrm{K}$, $\\sigma_y = 0.1$ (dimensionless).\n- Test Case 3 (wide temperature span, higher noise): temperatures $T_i = [800, 900, 1000, 1200, 1400, 1600, 1800, 2000]$ in $\\mathrm{K}$, $\\sigma_y = 0.25$ (dimensionless).\n- Test Case 4 (near-degenerate temperature span, moderate noise): temperatures $T_i = [995, 998, 1000, 1002, 1005]$ in $\\mathrm{K}$, $\\sigma_y = 0.1$ (dimensionless).\n\nFor all computations, treat angles in radians. All reported values must be dimensionless, as they are derived in the $(\\theta_1,\\theta_2)$ space. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must itself be a list in the order $[a, b, \\alpha, \\rho, \\kappa, \\mathrm{cond}]$ with each float rounded to six decimal places, and the overall output must be a list of these per-test-case lists; for example, \"[[a1,b1,alpha1,rho1,kappa1,cond1],[a2,b2,alpha2,rho2,kappa2,cond2],...]\".",
            "solution": "The user has provided a problem statement regarding uncertainty quantification for the parameters of the Arrhenius rate expression in chemical kinetics. The first step is to validate the problem.\n\n### Step 1: Extract Givens\n- **Rate Constant Expression:** $k(T) = A \\exp\\!\\left(-\\frac{E}{R T}\\right)$\n- **Constants:** Universal gas constant $R = 8.314\\,\\mathrm{J/(mol\\cdot K)}$, reference temperature $T_0 = 1000\\,\\mathrm{K}$.\n- **Transformed Parameters:** $\\boldsymbol{\\theta} = (\\theta_1, \\theta_2)$ where $\\theta_1 = \\ln A$ and $\\theta_2 = \\frac{E}{R T_0}$.\n- **Log-Linear Model:** $\\ln k(T) = \\theta_1 - \\theta_2 \\frac{T_0}{T}$.\n- **Measurement Model:** $y_i = \\ln k(T_i) + \\varepsilon_i$, with noise $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_y^2)$. The noise variance $\\sigma_y^2$ is known.\n- **Prior Distribution:** A uniform prior on $\\boldsymbol{\\theta}$, $p(\\boldsymbol{\\theta}) \\propto \\text{constant}$. This implies the posterior distribution is proportional to the likelihood function.\n- **Approximation:** The posterior is to be approximated by a bivariate normal distribution derived from the Laplace approximation centered at the Maximum Likelihood Estimate (MLE).\n- **Data Generation:** Synthetic data is generated without noise, i.e., $y_i = \\ln k(T_i)$ using ground truth parameter values. This ensures the MLE $\\hat{\\boldsymbol{\\theta}}$ is identical to the ground truth $\\boldsymbol{\\theta}^\\star$.\n- **Ground Truth Parameters:** $A^\\star = 10^{12}\\,\\mathrm{s}^{-1}$, $E^\\star = 1.5\\times 10^5\\,\\mathrm{J/mol}$.\n- **Quantities to Compute:**\n    1. $a$: semi-major axis length of the $95\\%$ confidence ellipse.\n    2. $b$: semi-minor axis length of the $95\\%$ confidence ellipse.\n    3. $\\alpha$: orientation angle (in radians) of the major axis relative to the $\\theta_1$ axis.\n    4. $\\rho$: posterior correlation coefficient between $\\theta_1$ and $\\theta_2$.\n    5. $\\kappa = a/b$: axis ratio of the ellipse.\n    6. $\\mathrm{cond}((X^\\top X))$: condition number of the normal matrix.\n- **Test Cases:**\n    1. $T_i = [800, 900, 1000, 1200, 1400, 1600, 1800, 2000]\\,\\mathrm{K}$, $\\sigma_y = 0.1$.\n    2. $T_i = [1100, 1120, 1140, 1160, 1180, 1200]\\,\\mathrm{K}$, $\\sigma_y = 0.1$.\n    3. $T_i = [800, 900, 1000, 1200, 1400, 1600, 1800, 2000]\\,\\mathrm{K}$, $\\sigma_y = 0.25$.\n    4. $T_i = [995, 998, 1000, 1002, 1005]\\,\\mathrm{K}$, $\\sigma_y = 0.1$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is based on the Arrhenius equation, a cornerstone of chemical kinetics, and employs standard statistical methods (linear regression, Maximum Likelihood Estimation, Bayesian inference with Laplace approximation) for uncertainty quantification. The entire setup is scientifically and mathematically sound.\n- **Well-Posed:** The problem is well-posed. It provides all necessary information (model, parameters, data points, noise levels) to uniquely determine the requested statistical quantities. The goal is clearly defined.\n- **Objective:** The problem is stated in precise, objective language. All quantities are mathematically defined, and the task is to compute them, leaving no room for subjective interpretation.\n- **No Flaws Detected:** The problem does not violate any of the invalidity criteria. It is scientifically sound, self-contained, and formalizable. The provided data are consistent, and the required computations, while complex, are feasible and meaningful for assessing parameter identifiability in computational combustion models. The problem is a standard exercise in sensitivity analysis and uncertainty quantification.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n### Solution Derivation\n\nThe analysis proceeds by formulating the problem as a linear regression, from which all desired quantities can be derived.\n\n**1. Linear Model Formulation**\nThe measurement model is given by:\n$$\ny_i = \\theta_1 - \\theta_2 \\frac{T_0}{T_i} + \\varepsilon_i\n$$\nThis can be written in the standard linear model form $\\mathbf{y} = X\\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon}$. For a set of $N$ temperature measurements $\\{T_i\\}_{i=1}^N$, the model is:\n$$\n\\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{pmatrix} = \\begin{pmatrix} 1 & -T_0/T_1 \\\\ 1 & -T_0/T_2 \\\\ \\vdots & \\vdots \\\\ 1 & -T_0/T_N \\end{pmatrix} \\begin{pmatrix} \\theta_1 \\\\ \\theta_2 \\end{pmatrix} + \\begin{pmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_N \\end{pmatrix}\n$$\nHere, $\\mathbf{y}$ is the vector of observations, $X$ is the $N \\times 2$ design matrix, $\\boldsymbol{\\theta}$ is the parameter vector, and $\\boldsymbol{\\varepsilon}$ is the vector of i.i.d. Gaussian noise terms with $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_y^2 I)$, where $I$ is the $N \\times N$ identity matrix.\n\n**2. Posterior Distribution via Laplace Approximation**\nGiven a uniform prior $p(\\boldsymbol{\\theta}) \\propto 1$, the posterior distribution is proportional to the likelihood:\n$$\np(\\boldsymbol{\\theta} | \\mathbf{y}) \\propto L(\\boldsymbol{\\theta}; \\mathbf{y}) \\propto \\exp\\left( -\\frac{1}{2\\sigma_y^2} (\\mathbf{y} - X\\boldsymbol{\\theta})^\\top (\\mathbf{y} - X\\boldsymbol{\\theta}) \\right)\n$$\nThe Laplace approximation states that the posterior can be approximated by a multivariate normal distribution centered at the posterior mode $\\hat{\\boldsymbol{\\theta}}$, with a covariance matrix equal to the inverse of the Hessian of the negative log-posterior evaluated at the mode. For this linear-Gaussian problem, the approximation is exact. The mode $\\hat{\\boldsymbol{\\theta}}$ is the Maximum Likelihood Estimate (MLE), given by the standard least-squares solution:\n$$\n\\hat{\\boldsymbol{\\theta}} = (X^\\top X)^{-1} X^\\top \\mathbf{y}\n$$\nThe Hessian of the negative log-likelihood is $H = \\frac{1}{\\sigma_y^2} X^\\top X$.\nThe covariance matrix of the posterior distribution for $\\boldsymbol{\\theta}$ is therefore:\n$$\n\\Sigma_{\\boldsymbol{\\theta}} = H^{-1} = \\left(\\frac{1}{\\sigma_y^2} X^\\top X\\right)^{-1} = \\sigma_y^2 (X^\\top X)^{-1}\n$$\nThus, the posterior is approximated as $\\boldsymbol{\\theta} | \\mathbf{y} \\sim \\mathcal{N}(\\hat{\\boldsymbol{\\theta}}, \\Sigma_{\\boldsymbol{\\theta}})$.\n\n**3. Characteristics of the Confidence Ellipse**\nThe $95\\%$ confidence (or credible) region for $\\boldsymbol{\\theta}$ is an ellipse defined by the equation:\n$$\n(\\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\theta}})^\\top \\Sigma_{\\boldsymbol{\\theta}}^{-1} (\\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\theta}}) \\le s^2\n$$\nwhere $s^2$ is the $95$-th percentile of the chi-squared distribution with $k=2$ degrees of freedom, i.e., $s^2 = \\chi^2_{2; 0.95} \\approx 5.9915$.\n\n- **Semi-axes ($a, b$) and Axis Ratio ($\\kappa$):** Let $\\lambda_1, \\lambda_2$ be the eigenvalues of the covariance matrix $\\Sigma_{\\boldsymbol{\\theta}}$, with $\\lambda_{\\text{max}} \\ge \\lambda_{\\text{min}}$. The semi-major and semi-minor axes of the ellipse are given by:\n$$\na = \\sqrt{s^2 \\lambda_{\\text{max}}}, \\quad b = \\sqrt{s^2 \\lambda_{\\text{min}}}\n$$\nThe axis ratio is $\\kappa = a/b = \\sqrt{\\lambda_{\\text{max}} / \\lambda_{\\text{min}}}$.\n\n- **Orientation Angle ($\\alpha$):** The orientation of the ellipse is determined by the eigenvectors of $\\Sigma_{\\boldsymbol{\\theta}}$. The major axis is aligned with the eigenvector corresponding to the largest eigenvalue, $\\lambda_{\\text{max}}$. If this eigenvector is $\\mathbf{v}_{\\text{max}} = [v_x, v_y]^\\top$, the angle $\\alpha$ with respect to the $\\theta_1$ axis is:\n$$\n\\alpha = \\mathrm{atan2}(v_y, v_x)\n$$\n\n**4. Derived Statistical Quantities**\n- **Posterior Correlation ($\\rho$):** The correlation coefficient between $\\theta_1$ and $\\theta_2$ is derived from the components of the covariance matrix $\\Sigma_{\\boldsymbol{\\theta}} = \\begin{pmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{pmatrix}$:\n$$\n\\rho = \\frac{\\Sigma_{12}}{\\sqrt{\\Sigma_{11} \\Sigma_{22}}}\n$$\n\n- **Condition Number ($\\mathrm{cond}(X^\\top X)$):** The condition number of the normal matrix $M = X^\\top X$ is a measure of its sensitivity to inversion, which reflects the collinearity of the columns of $X$. For a symmetric positive definite matrix, the $2$-norm condition number is the ratio of its largest to its smallest eigenvalue:\n$$\n\\mathrm{cond}_2(X^\\top X) = \\frac{\\lambda_{\\text{max}}(X^\\top X)}{\\lambda_{\\text{min}}(X^\\top X)}\n$$\nThere is a direct relationship between the axis ratio $\\kappa$ and the condition number:\n$$\n\\kappa^2 = \\frac{\\lambda_{\\text{max}}(\\Sigma_{\\boldsymbol{\\theta}})}{\\lambda_{\\text{min}}(\\Sigma_{\\boldsymbol{\\theta}})} = \\frac{\\sigma_y^2 \\lambda_{\\text{max}}((X^\\top X)^{-1})}{\\sigma_y^2 \\lambda_{\\text{min}}((X^\\top X)^{-1})} = \\frac{1/\\lambda_{\\text{min}}(X^\\top X)}{1/\\lambda_{\\text{max}}(X^\\top X)} = \\frac{\\lambda_{\\text{max}}(X^\\top X)}{\\lambda_{\\text{min}}(X^\\top X)} = \\mathrm{cond}_2(X^\\top X)\n$$\nHence, $\\kappa = \\sqrt{\\mathrm{cond}_2(X^\\top X)}$. This provides a valuable consistency check.\n\n**5. Algorithm**\nFor each test case with a given set of temperatures $\\{T_i\\}$ and noise standard deviation $\\sigma_y$:\n1. Construct the $N \\times 2$ design matrix $X$, where the $i$-th row is $[1, -T_0/T_i]$.\n2. Compute the normal matrix $M = X^\\top X$.\n3. Calculate the condition number of $M$, $\\mathrm{cond}(M)$, using a standard numerical library function.\n4. Calculate the posterior covariance matrix $\\Sigma_{\\boldsymbol{\\theta}} = \\sigma_y^2 M^{-1}$.\n5. Compute the eigenvalues, $\\lambda_{\\text{min}}, \\lambda_{\\text{max}}$, and corresponding eigenvectors of $\\Sigma_{\\boldsymbol{\\theta}}$. Numpy's `linalg.eigh` is suitable as it is designed for symmetric matrices and returns sorted eigenvalues.\n6. Retrieve the critical value $s^2$ from the $\\chi^2_2$ distribution for a $95\\%$ confidence level.\n7. Calculate the semi-axes: $a = \\sqrt{s^2 \\lambda_{\\text{max}}}$ and $b = \\sqrt{s^2 \\lambda_{\\text{min}}}$.\n8. Calculate the axis ratio $\\kappa = a/b$.\n9. Determine the orientation angle $\\alpha$ using `atan2` on the components of the eigenvector associated with $\\lambda_{\\text{max}}$.\n10. Calculate the correlation coefficient $\\rho$ from the elements of $\\Sigma_{\\boldsymbol{\\theta}}$.\n11. Collect the results $[a, b, \\alpha, \\rho, \\kappa, \\mathrm{cond}(M)]$ and format them as required. This procedure is repeated for all specified test cases.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Computes uncertainty quantification metrics for the Arrhenius parameters\n    based on a linear regression model.\n    \"\"\"\n    \n    # Define constants\n    R = 8.314  # J/(mol*K)\n    T0 = 1000.0  # K\n\n    # Test cases: (list of temperatures in K, noise std dev sigma_y)\n    test_cases_data = [\n        (np.array([800, 900, 1000, 1200, 1400, 1600, 1800, 2000]), 0.1),\n        (np.array([1100, 1120, 1140, 1160, 1180, 1200]), 0.1),\n        (np.array([800, 900, 1000, 1200, 1400, 1600, 1800, 2000]), 0.25),\n        (np.array([995, 998, 1000, 1002, 1005]), 0.1),\n    ]\n\n    # Level for 95% confidence interval for a 2D problem\n    s_squared = chi2.ppf(0.95, df=2)\n\n    all_results = []\n\n    for temperatures, sigma_y in test_cases_data:\n        # 1. Construct the design matrix X\n        N = len(temperatures)\n        x_col = -T0 / temperatures\n        # Design matrix X has columns [1, x_col]\n        X = np.vstack((np.ones(N), x_col)).T\n\n        # 2. Compute the normal matrix M = X^T * X\n        M = X.T @ X\n\n        # 3. Calculate the condition number of M\n        cond_M = np.linalg.cond(M)\n\n        # 4. Calculate the posterior covariance matrix Sigma_theta\n        try:\n            M_inv = np.linalg.inv(M)\n        except np.linalg.LinAlgError:\n            # Handle singular matrix case, though not expected for these test cases\n            all_results.append([np.inf] * 6)\n            continue\n            \n        Sigma_theta = sigma_y**2 * M_inv\n\n        # 5. Compute eigenvalues and eigenvectors of Sigma_theta\n        # np.linalg.eigh is for symmetric matrices and returns sorted eigenvalues\n        eigenvalues, eigenvectors = np.linalg.eigh(Sigma_theta)\n        lambda_min, lambda_max = eigenvalues[0], eigenvalues[1]\n        \n        # Eigenvector for lambda_max is the second column\n        v_max = eigenvectors[:, 1]\n\n        # 6. Critical value s_squared is already computed\n        \n        # 7. Calculate semi-axes\n        a = np.sqrt(s_squared * lambda_max)\n        b = np.sqrt(s_squared * lambda_min)\n\n        # 8. Calculate axis ratio\n        kappa = a / b\n\n        # 9. Determine orientation angle alpha\n        # angle of the major axis eigenvector with respect to the theta1 axis\n        alpha = np.arctan2(v_max[1], v_max[0])\n\n        # 10. Calculate correlation coefficient rho\n        Sigma_11 = Sigma_theta[0, 0]\n        Sigma_22 = Sigma_theta[1, 1]\n        Sigma_12 = Sigma_theta[0, 1]\n        rho = Sigma_12 / np.sqrt(Sigma_11 * Sigma_22)\n        \n        # 11. Collect results\n        case_results = [a, b, alpha, rho, kappa, cond_M]\n        all_results.append(case_results)\n\n    # Format output\n    output_str = \"[\"\n    for i, res in enumerate(all_results):\n        formatted_res = [f\"{val:.6f}\" for val in res]\n        output_str += f\"[{','.join(formatted_res)}]\"\n        if i < len(all_results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "This final practice applies UQ techniques to a real-world engineering challenge involving risk assessment and experimental design. Using a surrogate model for ignition delay time—a key safety metric in combustion systems—you will employ Monte Carlo simulation to estimate the probability of an undesirable outcome. The exercise then demonstrates a powerful application of UQ: deciding which future experiment would be most valuable for reducing the uncertainty in our risk estimate, directly linking computational analysis to practical decision-making .",
            "id": "4075234",
            "problem": "Consider a chemically reacting mixture whose ignition delay time is modeled using a single-step Arrhenius-type surrogate commonly employed in computational combustion. Let the ignition delay time be given by the parametric model\n$$\n\\tau_{\\mathrm{ign}} = C \\exp\\left(\\frac{E}{R T_0}\\right)\\left(\\phi\\right)^{\\gamma}\\left(\\frac{p_{\\mathrm{ref}}}{p_0}\\right)^{\\beta},\n$$\nwhere $C$ is a constant parameter with units of milliseconds (ms), $E$ is the effective activation energy in joules per mole, $R$ is the universal gas constant in joules per mole per kelvin, $T_0$ is the initial temperature in kelvin, $\\phi$ is the equivalence ratio (dimensionless), $p_0$ is the initial pressure expressed in atmospheres, $p_{\\mathrm{ref}}$ is a reference pressure in atmospheres, $\\gamma$ and $\\beta$ are dimensionless sensitivity exponents, and $\\tau_{\\mathrm{ign}}$ is the ignition delay time expressed in milliseconds (ms). This surrogate reflects the exponential sensitivity to temperature through the Arrhenius law and monotonic dependencies on the equivalence ratio and pressure.\n\nUncertainty Quantification (UQ) is performed by treating the parameters $C$, $E$, $T_0$, $\\phi$, and $p_0$ as random variables. Specifically, suppose $C$ and $p_0$ are modeled as lognormal random variables via their natural logarithms, and $E$, $T_0$, and $\\phi$ are modeled as normal random variables. The goal is to compute the probability that $\\tau_{\\mathrm{ign}}$ exceeds a specified safety threshold $\\tau_s$ in milliseconds (ms) under the given uncertainties. In addition, design an experiment that reduces that exceedance probability by choosing a single quantity to measure from the set $\\{T_0, E, \\phi\\}$, under additive Gaussian measurement noise. Use Bayesian updating under a normal-normal conjugate model to reduce the variance of the measured quantity: if the prior variance of a normally distributed quantity $X$ is $\\sigma_X^2$ and the measurement noise variance is $\\sigma_{X,\\mathrm{meas}}^2$, then the posterior variance is\n$$\n\\sigma_{X,\\mathrm{post}}^2 = \\left(\\frac{1}{\\sigma_X^2} + \\frac{1}{\\sigma_{X,\\mathrm{meas}}^2}\\right)^{-1}.\n$$\nAssume the expected posterior mean equals the prior mean when averaging over all possible measurement outcomes. Approximate the expected exceedance probability after performing a measurement by replacing the prior variance of the chosen quantity with $\\sigma_{X,\\mathrm{post}}^2$ and sampling the remaining quantities from their priors.\n\nStarting from the ideal gas law and Arrhenius law, the surrogate form above is consistent with $\\tau_{\\mathrm{ign}}$ being inversely related to reaction rates that scale as $\\exp\\left(-E/(R T)\\right)$ and monotonically related to mixture parameters. Use a Monte Carlo estimator to compute the probability of exceedance for the prior model and for each experiment design option. The output must be a single line containing the aggregated results for all test cases, in which each test case result is the list $[\\text{prob\\_prior}, \\text{prob\\_post\\_T}, \\text{prob\\_post\\_E}, \\text{prob\\_post\\_phi}, i]$, where $\\text{prob\\_prior}$ is the prior exceedance probability expressed as a decimal, $\\text{prob\\_post\\_T}$, $\\text{prob\\_post\\_E}$, and $\\text{prob\\_post\\_phi}$ are the expected exceedance probabilities under each respective measurement option, and $i \\in \\{1,2,3\\}$ is the index of the option that minimizes the expected exceedance probability (with ties broken by choosing the smallest index). Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces.\n\nExpress every ignition delay threshold in milliseconds (ms). Express probabilities as decimals without a percentage sign. Angles do not appear in this problem.\n\nUse the following universal constants:\n- $R = 8.314$.\n\nUse the following test suite of parameter sets. For every test case, sampling from priors must be performed with the number of samples $N$ as specified. When sampling a normally distributed $\\phi$, clip the sampled values to the physically reasonable interval $[0.6, 1.4]$ to ensure realism. For the lognormal variables $C$ and $p_0$, sample via their natural logarithms.\n\nTest Case $1$ (happy path):\n- $\\mu_{\\ln C} = \\ln(10^{-6})$, $\\sigma_{\\ln C} = 0.4$.\n- $\\mu_{E} = 140000$, $\\sigma_{E} = 8000$.\n- $\\mu_{T_0} = 1200$, $\\sigma_{T_0} = 60$.\n- $\\mu_{\\phi} = 1.0$, $\\sigma_{\\phi} = 0.05$.\n- $\\mu_{\\ln p_0} = \\ln(10)$, $\\sigma_{\\ln p_0} = 0.2$.\n- $\\beta = 1.0$, $\\gamma = 0.3$, $p_{\\mathrm{ref}} = 1.0$.\n- $\\tau_s = 0.2$ (ms).\n- $N = 50000$.\n- Measurement noises: $\\sigma_{T_0,\\mathrm{meas}} = 10$, $\\sigma_{E,\\mathrm{meas}} = 5000$, $\\sigma_{\\phi,\\mathrm{meas}} = 0.02$.\n\nTest Case $2$ (boundary low threshold):\n- $\\mu_{\\ln C} = \\ln(10^{-6})$, $\\sigma_{\\ln C} = 0.4$.\n- $\\mu_{E} = 140000$, $\\sigma_{E} = 8000$.\n- $\\mu_{T_0} = 1200$, $\\sigma_{T_0} = 60$.\n- $\\mu_{\\phi} = 1.0$, $\\sigma_{\\phi} = 0.05$.\n- $\\mu_{\\ln p_0} = \\ln(10)$, $\\sigma_{\\ln p_0} = 0.2$.\n- $\\beta = 1.0$, $\\gamma = 0.3$, $p_{\\mathrm{ref}} = 1.0$.\n- $\\tau_s = 0.01$ (ms).\n- $N = 50000$.\n- Measurement noises: $\\sigma_{T_0,\\mathrm{meas}} = 10$, $\\sigma_{E,\\mathrm{meas}} = 5000$, $\\sigma_{\\phi,\\mathrm{meas}} = 0.02$.\n\nTest Case $3$ (edge case with high uncertainty):\n- $\\mu_{\\ln C} = \\ln(10^{-6})$, $\\sigma_{\\ln C} = 0.8$.\n- $\\mu_{E} = 140000$, $\\sigma_{E} = 15000$.\n- $\\mu_{T_0} = 1200$, $\\sigma_{T_0} = 120$.\n- $\\mu_{\\phi} = 1.0$, $\\sigma_{\\phi} = 0.10$.\n- $\\mu_{\\ln p_0} = \\ln(10)$, $\\sigma_{\\ln p_0} = 0.3$.\n- $\\beta = 1.0$, $\\gamma = 0.3$, $p_{\\mathrm{ref}} = 1.0$.\n- $\\tau_s = 0.5$ (ms).\n- $N = 50000$.\n- Measurement noises: $\\sigma_{T_0,\\mathrm{meas}} = 10$, $\\sigma_{E,\\mathrm{meas}} = 5000$, $\\sigma_{\\phi,\\mathrm{meas}} = 0.02$.\n\nYour program must implement the following steps for each test case:\n- Sample $C$, $E$, $T_0$, $\\phi$, and $p_0$ from their specified prior distributions and estimate the prior exceedance probability $\\mathbb{P}(\\tau_{\\mathrm{ign}} > \\tau_s)$ using a Monte Carlo estimator.\n- For each experiment option among measuring $T_0$, $E$, or $\\phi$, compute the posterior variance using the normal-normal conjugate formula and estimate the expected exceedance probability by sampling from the updated distribution for the measured quantity (with variance $\\sigma_{X,\\mathrm{post}}^2$ and mean equal to the prior mean) while sampling the remaining quantities from their priors.\n- Select the experiment option index $i \\in \\{1,2,3\\}$ that minimizes the expected exceedance probability, breaking ties by choosing the smallest index.\n\nFinal Output Format:\nYour program should produce a single line of output containing a comma-separated list enclosed in square brackets with no spaces. Each element of the top-level list corresponds to a test case and is formatted as $[\\text{prob\\_prior}, \\text{prob\\_post\\_T}, \\text{prob\\_post\\_E}, \\text{prob\\_post\\_phi}, i]$, where all probabilities are decimals and $i$ is an integer. For example:\n$[[0.123456,0.111111,0.098765,0.110000,2],[\\dots]]$.",
            "solution": "Begin with the Arrhenius law for a reaction rate, which states that the reaction rate $k$ can be written as $k = A \\exp\\left(-\\frac{E}{R T}\\right)$, where $A$ is a pre-exponential factor, $E$ is the activation energy, $R$ is the universal gas constant, and $T$ is the temperature. In ignition phenomena, the ignition delay time $\\tau_{\\mathrm{ign}}$ is inversely related to certain combinations of reaction rates and thermochemical timescales. A widely used surrogate that captures the strong temperature sensitivity and monotonic dependence on mixture parameters is\n$$\n\\tau_{\\mathrm{ign}} = C \\exp\\left(\\frac{E}{R T_0}\\right)\\left(\\phi\\right)^{\\gamma}\\left(\\frac{p_{\\mathrm{ref}}}{p_0}\\right)^{\\beta},\n$$\nwhere $C$ consolidates various scale factors into a time unit, $p_{\\mathrm{ref}}$ is a reference pressure used to form a dimensionless ratio, and $\\gamma$ and $\\beta$ encode sensitivities to equivalence ratio and pressure respectively. The exponential sensitivity in $\\tau_{\\mathrm{ign}}$ arises by inverting the rate's exponential dependence: an increase in $T_0$ reduces $\\frac{E}{R T_0}$, decreasing $\\tau_{\\mathrm{ign}}$, consistent with physical behavior. Similarly, higher $p_0$ reduces $\\tau_{\\mathrm{ign}}$ for positive $\\beta$, representing pressure-accelerated kinetics or reduced ignition delay.\n\nTo quantify uncertainty, we model $C$ and $p_0$ as lognormal via their natural logarithms, meaning $\\ln C \\sim \\mathcal{N}\\left(\\mu_{\\ln C}, \\sigma_{\\ln C}^2\\right)$ and $\\ln p_0 \\sim \\mathcal{N}\\left(\\mu_{\\ln p_0}, \\sigma_{\\ln p_0}^2\\right)$. We model $E$, $T_0$, and $\\phi$ as normal variables: $E \\sim \\mathcal{N}\\left(\\mu_{E}, \\sigma_{E}^2\\right)$, $T_0 \\sim \\mathcal{N}\\left(\\mu_{T_0}, \\sigma_{T_0}^2\\right)$, and $\\phi \\sim \\mathcal{N}\\left(\\mu_{\\phi}, \\sigma_{\\phi}^2\\right)$. To enforce physical realism for the equivalence ratio, we clip sampled $\\phi$ to $[0.6, 1.4]$. The ignition delay threshold $\\tau_s$ is specified in milliseconds (ms). The exceedance probability is defined as $\\mathbb{P}\\left(\\tau_{\\mathrm{ign}} > \\tau_s\\right)$.\n\nMonte Carlo estimation: For a given test case, draw $N$ independent samples of $(C,E,T_0,\\phi,p_0)$ from their distributions. Compute $\\tau_{\\mathrm{ign}}$ for each sample and estimate the probability $\\mathbb{P}\\left(\\tau_{\\mathrm{ign}} > \\tau_s\\right)$ using the empirical fraction of samples for which $\\tau_{\\mathrm{ign}} > \\tau_s$.\n\nExperiment design via Bayesian variance reduction: Suppose we can measure one quantity $X \\in \\{T_0, E, \\phi\\}$ with additive Gaussian noise modeled as $Y = X + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, \\sigma_{X,\\mathrm{meas}}^2)$ is independent of $X$. Under a normal prior $X \\sim \\mathcal{N}(\\mu_X, \\sigma_X^2)$, the normal-normal conjugate posterior for $X$ given a measurement is also normal with variance\n$$\n\\sigma_{X,\\mathrm{post}}^2 = \\left(\\frac{1}{\\sigma_X^2} + \\frac{1}{\\sigma_{X,\\mathrm{meas}}^2}\\right)^{-1},\n$$\nand mean\n$$\n\\mu_{X,\\mathrm{post}}(y) = \\sigma_{X,\\mathrm{post}}^2 \\left(\\frac{\\mu_X}{\\sigma_X^2} + \\frac{y}{\\sigma_{X,\\mathrm{meas}}^2}\\right).\n$$\nWhen averaging over all possible measurement outcomes $y$, the expected posterior mean equals the prior mean: $\\mathbb{E}[\\mu_{X,\\mathrm{post}}(Y)] = \\mu_X$. Therefore, the expected posterior distribution of $X$ after planning a measurement is well approximated by maintaining the prior mean and replacing the variance by $\\sigma_{X,\\mathrm{post}}^2$. To approximate the expected exceedance probability under each measurement plan, we sample $X$ from $\\mathcal{N}(\\mu_X, \\sigma_{X,\\mathrm{post}}^2)$ and the remaining parameters from their priors, compute $\\tau_{\\mathrm{ign}}$ for each sample, and estimate the exceedance probability. This procedure yields a practical approximation to the expected exceedance probability one would obtain after performing a single noisy measurement.\n\nSelection criterion: For each test case, compute the prior exceedance probability and the expected exceedance probabilities under measuring $T_0$, $E$, or $\\phi$. Select the index $i \\in \\{1,2,3\\}$ corresponding to the measurement that minimizes the expected exceedance probability, breaking ties by choosing the smallest index (that is, prefer measuring $T_0$ over $E$ and $\\phi$ if all have equal expected probability, and so on).\n\nAlgorithmic design:\n- Fix $R = 8.314$.\n- For a given test case, draw $N$ samples:\n  - Draw $\\ln C \\sim \\mathcal{N}(\\mu_{\\ln C}, \\sigma_{\\ln C}^2)$ and set $C = \\exp(\\ln C)$.\n  - Draw $E \\sim \\mathcal{N}(\\mu_E, \\sigma_E^2)$.\n  - Draw $T_0 \\sim \\mathcal{N}(\\mu_{T_0}, \\sigma_{T_0}^2)$.\n  - Draw $\\phi \\sim \\mathcal{N}(\\mu_{\\phi}, \\sigma_{\\phi}^2)$ and clip to $[0.6, 1.4]$.\n  - Draw $\\ln p_0 \\sim \\mathcal{N}(\\mu_{\\ln p_0}, \\sigma_{\\ln p_0}^2)$ and set $p_0 = \\exp(\\ln p_0)$.\n- Compute $\\tau_{\\mathrm{ign}} = C \\exp\\left(\\frac{E}{R T_0}\\right)\\left(\\phi\\right)^{\\gamma}\\left(\\frac{p_{\\mathrm{ref}}}{p_0}\\right)^{\\beta}$.\n- Estimate $\\mathbb{P}\\left(\\tau_{\\mathrm{ign}} > \\tau_s\\right)$ using the fraction of samples exceeding $\\tau_s$.\n- For each measurement option $X \\in \\{T_0, E, \\phi\\}$:\n  - Compute $\\sigma_{X,\\mathrm{post}}^2$ using $\\sigma_{X}^2$ and $\\sigma_{X,\\mathrm{meas}}^2$.\n  - Resample with $X \\sim \\mathcal{N}(\\mu_X, \\sigma_{X,\\mathrm{post}}^2)$ and other variables from their priors to estimate the expected exceedance probability.\n- Choose the option index $i$ minimizing the expected exceedance probability, breaking ties by the smallest index.\n\nNumerical considerations: Use sufficiently large $N$ to reduce Monte Carlo sampling error while ensuring computational tractability. All ignition delay thresholds are specified in milliseconds (ms), and computed probabilities are unitless decimals.\n\nThe program will implement the above and produce the required single-line, comma-separated, bracket-enclosed list without spaces containing one list per test case in the format $[\\text{prob\\_prior}, \\text{prob\\_post\\_T}, \\text{prob\\_post\\_E}, \\text{prob\\_post\\_phi}, i]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# Set a global random seed for reproducibility\nnp.random.seed(42)\n\ndef monte_carlo_exceedance(mu_lnC, sigma_lnC,\n                           mu_E, sigma_E,\n                           mu_T, sigma_T,\n                           mu_phi, sigma_phi,\n                           mu_lnp, sigma_lnp,\n                           beta, gamma, p_ref,\n                           tau_s_ms,\n                           N,\n                           override_sigma_T=None,\n                           override_sigma_E=None,\n                           override_sigma_phi=None):\n    \"\"\"\n    Estimate P(tau_ign > tau_s_ms) by Monte Carlo under specified priors.\n    Optionally override the variances of T, E, or phi to represent posterior variance\n    after a measurement (means kept at prior means).\n    \"\"\"\n    # Sample ln C and construct C in ms\n    lnC = np.random.normal(mu_lnC, sigma_lnC, size=N)\n    C = np.exp(lnC)  # ms\n\n    # Sample E (J/mol)\n    sigma_E_use = sigma_E if override_sigma_E is None else override_sigma_E\n    E = np.random.normal(mu_E, sigma_E_use, size=N)\n\n    # Sample T0 (K)\n    sigma_T_use = sigma_T if override_sigma_T is None else override_sigma_T\n    T0 = np.random.normal(mu_T, sigma_T_use, size=N)\n    # Prevent non-physical very small or negative T by clipping to > 1 K\n    T0 = np.clip(T0, 1.0, None)\n\n    # Sample phi (dimensionless) and clip to [0.6, 1.4]\n    sigma_phi_use = sigma_phi if override_sigma_phi is None else override_sigma_phi\n    phi = np.random.normal(mu_phi, sigma_phi_use, size=N)\n    phi = np.clip(phi, 0.6, 1.4)\n\n    # Sample ln p0 and construct p0 in atm\n    lnp0 = np.random.normal(mu_lnp, sigma_lnp, size=N)\n    p0 = np.exp(lnp0)  # atm\n\n    # Universal gas constant R (J/mol/K)\n    R = 8.314\n\n    # Compute ignition delay in ms\n    # tau = C * exp(E/(R*T0)) * phi^gamma * (p_ref/p0)^beta\n    tau_ms = C * np.exp(E / (R * T0)) * (phi ** gamma) * ((p_ref / p0) ** beta)\n\n    # Probability of exceedance tau_ms > tau_s_ms\n    exceed = (tau_ms > tau_s_ms)\n    prob = exceed.mean()\n    return prob\n\ndef posterior_variance_normal(sig_prior, sig_meas):\n    \"\"\"Compute posterior variance under normal-normal conjugacy.\"\"\"\n    return 1.0 / (1.0 / (sig_prior ** 2) + 1.0 / (sig_meas ** 2))\n\ndef format_compact(obj):\n    \"\"\"\n    Format Python lists/numbers to a compact string with no spaces.\n    Floats formatted to 6 decimal places.\n    \"\"\"\n    if isinstance(obj, list):\n        return \"[\" + \",\".join(format_compact(x) for x in obj) + \"]\"\n    elif isinstance(obj, float):\n        return f\"{obj:.6f}\"\n    elif isinstance(obj, (int, np.integer)):\n        return str(int(obj))\n    else:\n        # For numpy floats or other numeric types\n        try:\n            return f\"{float(obj):.6f}\"\n        except Exception:\n            return str(obj)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1 (happy path)\n        {\n            \"mu_lnC\": np.log(1e-6), \"sigma_lnC\": 0.4,\n            \"mu_E\": 140000.0, \"sigma_E\": 8000.0,\n            \"mu_T\": 1200.0, \"sigma_T\": 60.0,\n            \"mu_phi\": 1.0, \"sigma_phi\": 0.05,\n            \"mu_lnp\": np.log(10.0), \"sigma_lnp\": 0.2,\n            \"beta\": 1.0, \"gamma\": 0.3, \"p_ref\": 1.0,\n            \"tau_s_ms\": 0.2, \"N\": 50000,\n            \"sigma_T_meas\": 10.0, \"sigma_E_meas\": 5000.0, \"sigma_phi_meas\": 0.02\n        },\n        # Test Case 2 (boundary low threshold)\n        {\n            \"mu_lnC\": np.log(1e-6), \"sigma_lnC\": 0.4,\n            \"mu_E\": 140000.0, \"sigma_E\": 8000.0,\n            \"mu_T\": 1200.0, \"sigma_T\": 60.0,\n            \"mu_phi\": 1.0, \"sigma_phi\": 0.05,\n            \"mu_lnp\": np.log(10.0), \"sigma_lnp\": 0.2,\n            \"beta\": 1.0, \"gamma\": 0.3, \"p_ref\": 1.0,\n            \"tau_s_ms\": 0.01, \"N\": 50000,\n            \"sigma_T_meas\": 10.0, \"sigma_E_meas\": 5000.0, \"sigma_phi_meas\": 0.02\n        },\n        # Test Case 3 (edge case with high uncertainty)\n        {\n            \"mu_lnC\": np.log(1e-6), \"sigma_lnC\": 0.8,\n            \"mu_E\": 140000.0, \"sigma_E\": 15000.0,\n            \"mu_T\": 1200.0, \"sigma_T\": 120.0,\n            \"mu_phi\": 1.0, \"sigma_phi\": 0.10,\n            \"mu_lnp\": np.log(10.0), \"sigma_lnp\": 0.3,\n            \"beta\": 1.0, \"gamma\": 0.3, \"p_ref\": 1.0,\n            \"tau_s_ms\": 0.5, \"N\": 50000,\n            \"sigma_T_meas\": 10.0, \"sigma_E_meas\": 5000.0, \"sigma_phi_meas\": 0.02\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        mu_lnC = case[\"mu_lnC\"]; sigma_lnC = case[\"sigma_lnC\"]\n        mu_E = case[\"mu_E\"]; sigma_E = case[\"sigma_E\"]\n        mu_T = case[\"mu_T\"]; sigma_T = case[\"sigma_T\"]\n        mu_phi = case[\"mu_phi\"]; sigma_phi = case[\"sigma_phi\"]\n        mu_lnp = case[\"mu_lnp\"]; sigma_lnp = case[\"sigma_lnp\"]\n        beta = case[\"beta\"]; gamma = case[\"gamma\"]; p_ref = case[\"p_ref\"]\n        tau_s_ms = case[\"tau_s_ms\"]; N = case[\"N\"]\n        sigma_T_meas = case[\"sigma_T_meas\"]\n        sigma_E_meas = case[\"sigma_E_meas\"]\n        sigma_phi_meas = case[\"sigma_phi_meas\"]\n\n        # Prior exceedance probability\n        prob_prior = monte_carlo_exceedance(\n            mu_lnC, sigma_lnC,\n            mu_E, sigma_E,\n            mu_T, sigma_T,\n            mu_phi, sigma_phi,\n            mu_lnp, sigma_lnp,\n            beta, gamma, p_ref,\n            tau_s_ms,\n            N\n        )\n\n        # Posterior variances for measurement options (means remain the same)\n        sigma_T_post = np.sqrt(posterior_variance_normal(sigma_T, sigma_T_meas))\n        sigma_E_post = np.sqrt(posterior_variance_normal(sigma_E, sigma_E_meas))\n        sigma_phi_post = np.sqrt(posterior_variance_normal(sigma_phi, sigma_phi_meas))\n\n        # Expected exceedance probabilities after each measurement option\n        prob_post_T = monte_carlo_exceedance(\n            mu_lnC, sigma_lnC,\n            mu_E, sigma_E,\n            mu_T, sigma_T,\n            mu_phi, sigma_phi,\n            mu_lnp, sigma_lnp,\n            beta, gamma, p_ref,\n            tau_s_ms,\n            N,\n            override_sigma_T=sigma_T_post\n        )\n        prob_post_E = monte_carlo_exceedance(\n            mu_lnC, sigma_lnC,\n            mu_E, sigma_E,\n            mu_T, sigma_T,\n            mu_phi, sigma_phi,\n            mu_lnp, sigma_lnp,\n            beta, gamma, p_ref,\n            tau_s_ms,\n            N,\n            override_sigma_E=sigma_E_post\n        )\n        prob_post_phi = monte_carlo_exceedance(\n            mu_lnC, sigma_lnC,\n            mu_E, sigma_E,\n            mu_T, sigma_T,\n            mu_phi, sigma_phi,\n            mu_lnp, sigma_lnp,\n            beta, gamma, p_ref,\n            tau_s_ms,\n            N,\n            override_sigma_phi=sigma_phi_post\n        )\n\n        # Choose best experiment (minimize expected exceedance probability), tie-break by smallest index\n        post_probs = [prob_post_T, prob_post_E, prob_post_phi]\n        min_val = min(post_probs)\n        # Determine first index where min occurs (1-based index)\n        best_index = 1 + post_probs.index(min_val)\n\n        results.append([prob_prior, prob_post_T, prob_post_E, prob_post_phi, best_index])\n\n    # Final print statement in the exact required format (no spaces).\n    print(format_compact(results))\n\nsolve()\n```"
        }
    ]
}