{
    "hands_on_practices": [
        {
            "introduction": "A crucial first step in applying a Physics-Informed Neural Network (PINN) is defining its architecture. This choice determines the model's capacity to represent complex physical fields and directly impacts computational requirements. This practice provides a concrete exercise in analyzing a common PINN design for combustion problems—a shared trunk with specialized heads—and calculating its total number of trainable parameters, a fundamental skill for designing and deploying neural network models .",
            "id": "4049974",
            "problem": "Consider a Physics-Informed Neural Network (PINN) designed to approximate reacting-flow fields in computational combustion under a low-Mach, constant-pressure assumption. The network receives as inputs the spatial coordinates, time, pressure, and equivalence ratio, assembled into the feature vector $\\mathbf{x} = (x, y, z, t, p, \\phi)$ of dimension $d_{\\mathrm{in}} = 6$. The outputs are the temperature field $T(\\mathbf{x})$ and the species mass fractions $\\{Y_i(\\mathbf{x})\\}_{i=1}^{N_s}$ for $N_s = 9$ chemical species. The network uses a shared trunk with species-specific heads and a distinct head for temperature. The architecture is specified as follows:\n\n- Shared trunk: a fully connected Multi-Layer Perceptron (MLP) with three hidden layers of widths $64$, $64$, and $64$, followed by a latent output layer of width $128$. Thus the trunk consists of linear mappings $6 \\to 64$, $64 \\to 64$, $64 \\to 64$, and $64 \\to 128$ in sequence. All linear layers include biases, and the activation functions are parameter-free.\n- Temperature head: a fully connected head mapping the trunk latent vector of width $128$ to the scalar $T$, with two hidden layers of widths $64$ and $32$, followed by an output layer. Thus the temperature head consists of linear mappings $128 \\to 64$, $64 \\to 32$, and $32 \\to 1$, with biases in every linear layer.\n- Species heads: there are $N_s = 9$ species-specific heads, each mapping the trunk latent vector of width $128$ to a scalar $Y_i$ with one hidden layer of width $32$, followed by an output layer. Each species head consists of linear mappings $128 \\to 32$ and $32 \\to 1$, with biases in every linear layer. All $N_s$ heads have identical architectures but distinct parameters.\n\nAssume there are no additional parameterized layers (for example, no batch normalization or residual parameterizations), and that activation functions and any physics operators used in the loss (governing equation residuals, boundary conditions, and integral constraints) introduce no trainable parameters. Compute the total number of trainable parameters in this PINN, counting all weights and biases across the shared trunk and all heads.\n\nExpress your final answer as a single integer. No rounding is required, and no physical units apply.",
            "solution": "The problem requires the calculation of the total number of trainable parameters in a specified Physics-Informed Neural Network (PINN). Trainable parameters in a standard Multi-Layer Perceptron (MLP) consist of weights and biases. For a fully connected linear layer that maps an input of dimension $n_{\\text{in}}$ to an output of dimension $n_{\\text{out}}$, the number of weights is given by the product $n_{\\text{in}} \\times n_{\\text{out}}$, and the number of biases is $n_{\\text{out}}$, as each output neuron has a single bias term. Thus, the total number of parameters for such a layer is $(n_{\\text{in}} \\times n_{\\text{out}}) + n_{\\text{out}}$.\n\nThe network architecture is comprised of three distinct components: a shared trunk, a single temperature head, and $N_s = 9$ species-specific heads. We will calculate the number of parameters for each component sequentially and then sum them to find the total.\n\nLet $N_{\\text{params}}$ denote the number of trainable parameters.\n\n1.  **Shared Trunk Parameters**\n    The shared trunk processes the input vector $\\mathbf{x}$ of dimension $d_{\\text{in}} = 6$. It consists of four sequential fully connected layers.\n    -   Layer $1$: Maps from an input dimension of $n_{\\text{in}} = 6$ to an output dimension of $n_{\\text{out}} = 64$.\n        $N_{\\text{trunk},1} = (6 \\times 64) + 64 = 384 + 64 = 448$.\n    -   Layer $2$: Maps from an input dimension of $n_{\\text{in}} = 64$ to an output dimension of $n_{\\text{out}} = 64$.\n        $N_{\\text{trunk},2} = (64 \\times 64) + 64 = 4096 + 64 = 4160$.\n    -   Layer $3$: Maps from an input dimension of $n_{\\text{in}} = 64$ to an output dimension of $n_{\\text{out}} = 64$.\n        $N_{\\text{trunk},3} = (64 \\times 64) + 64 = 4096 + 64 = 4160$.\n    -   Layer $4$ (latent output layer): Maps from an input dimension of $n_{\\text{in}} = 64$ to an output dimension of $n_{\\text{out}} = 128$.\n        $N_{\\text{trunk},4} = (64 \\times 128) + 128 = 8192 + 128 = 8320$.\n\n    The total number of parameters in the shared trunk, $N_{\\text{trunk}}$, is the sum of the parameters in its layers:\n    $$N_{\\text{trunk}} = N_{\\text{trunk},1} + N_{\\text{trunk},2} + N_{\\text{trunk},3} + N_{\\text{trunk},4} = 448 + 4160 + 4160 + 8320 = 17088$$\n\n2.  **Temperature Head Parameters**\n    The temperature head takes the $128$-dimensional latent vector from the trunk as its input. It has three fully connected layers.\n    -   Layer $1$: Maps from an input dimension of $n_{\\text{in}} = 128$ to an output dimension of $n_{\\text{out}} = 64$.\n        $N_{T,1} = (128 \\times 64) + 64 = 8192 + 64 = 8256$.\n    -   Layer $2$: Maps from an input dimension of $n_{\\text{in}} = 64$ to an output dimension of $n_{\\text{out}} = 32$.\n        $N_{T,2} = (64 \\times 32) + 32 = 2048 + 32 = 2080$.\n    -   Layer $3$ (output layer): Maps from an input dimension of $n_{\\text{in}} = 32$ to a scalar output of dimension $n_{\\text{out}} = 1$.\n        $N_{T,3} = (32 \\times 1) + 1 = 32 + 1 = 33$.\n\n    The total number of parameters in the temperature head, $N_{T}$, is the sum:\n    $$N_{T} = N_{T,1} + N_{T,2} + N_{T,3} = 8256 + 2080 + 33 = 10369$$\n\n3.  **Species Heads Parameters**\n    There are $N_s = 9$ species-specific heads. Each head has an identical architecture but distinct parameters. Each head takes the $128$-dimensional latent vector from the trunk as its input and consists of two fully connected layers. We first find the parameters for a single species head, $N_{Y_i}$.\n    -   Layer $1$: Maps from an input dimension of $n_{\\text{in}} = 128$ to an output dimension of $n_{\\text{out}} = 32$.\n        $N_{Y_i,1} = (128 \\times 32) + 32 = 4096 + 32 = 4128$.\n    -   Layer $2$ (output layer): Maps from an input dimension of $n_{\\text{in}} = 32$ to a scalar output of dimension $n_{\\text{out}} = 1$.\n        $N_{Y_i,2} = (32 \\times 1) + 1 = 32 + 1 = 33$.\n\n    The total number of parameters for a single species head is:\n    $$N_{Y_i} = N_{Y_i,1} + N_{Y_i,2} = 4128 + 33 = 4161$$\n    Since there are $N_s = 9$ such heads, the total number of parameters for all species heads, $N_{\\text{species}}$, is:\n    $$N_{\\text{species}} = N_s \\times N_{Y_i} = 9 \\times 4161 = 37449$$\n\n4.  **Total Network Parameters**\n    The total number of trainable parameters in the entire PINN, $N_{\\text{total}}$, is the sum of the parameters from the trunk, the temperature head, and all species heads.\n    $$N_{\\text{total}} = N_{\\text{trunk}} + N_{T} + N_{\\text{species}}$$\n    $$N_{\\text{total}} = 17088 + 10369 + 37449$$\n    $$N_{\\text{total}} = 64906$$",
            "answer": "$$\\boxed{64906}$$"
        },
        {
            "introduction": "The effectiveness of a PINN hinges on the careful balancing of its composite loss function, which contains terms for both experimental data and physical laws. While often treated as empirical hyperparameters, the weights for these terms can be analyzed systematically through the lens of statistical estimation theory. This exercise explores the fundamental bias-variance trade-off to derive the optimal weighting between a data-misfit term and a physics-residual term, providing a rigorous foundation for data assimilation in hybrid models .",
            "id": "4050006",
            "problem": "Consider training a Physics-Informed Neural Network (PINN) for a single spatial-temporal collocation point in a combustion simulation, where the target scalar state is denoted by $u^{\\ast}$. The training loss combines a data misfit term and a physics residual term. Assume a local linearization of the physics residual around the optimum yields a linear residual with slope $c \\gt 0$, and suppose both the data and physics information are noisy and possibly biased.\n\nModel the data and physics information as follows:\n- The data measurement is $y_{d} = u^{\\ast} + \\varepsilon_{d}$ with $\\mathbb{E}[\\varepsilon_{d}] = 0$ and $\\operatorname{Var}(\\varepsilon_{d}) = \\sigma_{d}^{2}$.\n- The physics anchor is $y_{p} = u^{\\ast} + b_{p} + \\varepsilon_{p}$ with $\\mathbb{E}[\\varepsilon_{p}] = 0$, $\\operatorname{Var}(\\varepsilon_{p}) = \\sigma_{p}^{2}$, and deterministic bias $b_{p} \\in \\mathbb{R}$ that quantifies possible model-form error in the combustion physics (for example, due to an imperfect closure).\n\nThe scalar training loss at this point is\n$$\nL(u) = w_{d}\\,(u - y_{d})^{2} + w_{p}\\,\\big(r(u)\\big)^{2}, \\quad r(u) = c\\,(u - y_{p}),\n$$\nwith weights $w_{d} \\ge 0$ and $w_{p} \\ge 0$ that control the balance between data and physics. All quantities are dimensionless. The minimizer $u^{\\star}$ of $L(u)$ is the PINN estimator at this point.\n\nTask:\n1. Starting from the definitions of bias $\\operatorname{Bias}(u^{\\star}) = \\mathbb{E}[u^{\\star}] - u^{\\ast}$ and variance $\\operatorname{Var}(u^{\\star})$, derive the expected mean squared error $\\operatorname{MSE}(u^{\\star}) = \\operatorname{Bias}(u^{\\star})^{2} + \\operatorname{Var}(u^{\\star})$ as a function of $w_{d}$, $w_{p}$, $c$, $\\sigma_{d}^{2}$, $\\sigma_{p}^{2}$, and $b_{p}$. Show that, for fixed $c \\gt 0$, the optimal ratio\n$$\nr_{\\text{opt}} = \\frac{w_{d}}{w_{p}\\,c^{2}}\n$$\nthat minimizes $\\operatorname{MSE}(u^{\\star})$ satisfies\n$$\nr_{\\text{opt}} = \\frac{b_{p}^{2} + \\sigma_{p}^{2}}{\\sigma_{d}^{2}},\n$$\nand derive the corresponding minimized expected mean squared error $\\operatorname{MSE}_{\\min}$ in closed form.\n\n2. Implement a program that, given a set of test cases, computes $r_{\\text{opt}}$ and $\\operatorname{MSE}_{\\min}$. Use only the assumptions above and do not introduce additional modeling shortcuts.\n\nUse the following test suite, where each test case is the tuple $(\\sigma_{d}^{2}, \\sigma_{p}^{2}, b_{p}, c)$:\n- Test Case 1 (general case): $(0.01, 0.02, 0.05, 1.3)$.\n- Test Case 2 (low data noise, moderate physics bias/noise): $(10^{-4}, 0.02, 0.05, 0.7)$.\n- Test Case 3 (high data noise, unbiased physics): $(1.0, 0.01, 0.0, 2.0)$.\n- Test Case 4 (perfect physics, moderate data noise): $(0.5, 0.0, 0.0, 1.0)$.\n\nAll quantities are dimensionless. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, for each test case in order, first $r_{\\text{opt}}$ and then $\\operatorname{MSE}_{\\min}$. For example, the format should be\n$$\n[ r_{\\text{opt},1},\\, \\operatorname{MSE}_{\\min,1},\\, r_{\\text{opt},2},\\, \\operatorname{MSE}_{\\min,2},\\, r_{\\text{opt},3},\\, \\operatorname{MSE}_{\\min,3},\\, r_{\\text{opt},4},\\, \\operatorname{MSE}_{\\min,4} ].\n$$",
            "solution": "The problem requires the derivation of the optimal weight ratio and the minimum mean squared error for a simplified Physics-Informed Neural Network (PINN) estimator, followed by a numerical implementation.\n\nThe first step is to determine the PINN estimator $u^{\\star}$ by minimizing the loss function $L(u)$. The loss function is given by:\n$$\nL(u) = w_{d}\\,(u - y_{d})^{2} + w_{p}\\,\\big(r(u)\\big)^{2}\n$$\nSubstituting the linearized residual $r(u) = c\\,(u - y_{p})$, the loss function becomes:\n$$\nL(u) = w_{d}\\,(u - y_{d})^{2} + w_{p}\\,c^{2}\\,(u - y_{p})^{2}\n$$\nwhere $w_{d} \\ge 0$, $w_{p} \\ge 0$, and $c > 0$. For a unique minimum to exist, we must have $w_{d} + w_{p}c^2 > 0$. The function $L(u)$ is a quadratic in $u$ and thus has a unique minimum. We find the minimizer $u^{\\star}$ by setting the derivative of $L(u)$ with respect to $u$ to zero:\n$$\n\\frac{dL}{du} = 2\\,w_{d}\\,(u - y_{d}) + 2\\,w_{p}\\,c^{2}\\,(u - y_{p}) = 0\n$$\nSolving for $u$ at the minimum $u=u^{\\star}$:\n$$\nw_{d}\\,(u^{\\star} - y_{d}) + w_{p}\\,c^{2}\\,(u^{\\star} - y_{p}) = 0\n$$\n$$\nu^{\\star}(w_{d} + w_{p}\\,c^{2}) = w_{d}\\,y_{d} + w_{p}\\,c^{2}\\,y_{p}\n$$\n$$\nu^{\\star} = \\frac{w_{d}\\,y_{d} + w_{p}\\,c^{2}\\,y_{p}}{w_{d} + w_{p}\\,c^{2}}\n$$\nTo simplify this expression, we introduce the ratio $r = \\frac{w_{d}}{w_{p}\\,c^{2}}$, which is defined for $w_{p}>0$. Dividing the numerator and denominator by $w_{p}\\,c^{2}$:\n$$\nu^{\\star} = \\frac{r\\,y_{d} + y_{p}}{r + 1}\n$$\nThis shows that the estimator $u^{\\star}$ is a weighted average of the data measurement $y_{d}$ and the physics anchor $y_{p}$.\n\nNext, we derive the statistical properties of $u^{\\star}$. We are given $y_{d} = u^{\\ast} + \\varepsilon_{d}$ and $y_{p} = u^{\\ast} + b_{p} + \\varepsilon_{p}$, with $\\mathbb{E}[\\varepsilon_{d}] = 0$, $\\operatorname{Var}(\\varepsilon_{d}) = \\sigma_{d}^{2}$, $\\mathbb{E}[\\varepsilon_{p}] = 0$, and $\\operatorname{Var}(\\varepsilon_{p}) = \\sigma_{p}^{2}$. The true state $u^{\\ast}$ and the physics bias $b_{p}$ are deterministic.\n\nThe expected value of $u^{\\star}$ is:\n$$\n\\mathbb{E}[u^{\\star}] = \\mathbb{E}\\left[\\frac{r\\,y_{d} + y_{p}}{r + 1}\\right] = \\frac{1}{r+1} \\left( r\\,\\mathbb{E}[y_{d}] + \\mathbb{E}[y_{p}] \\right)\n$$\nThe expectations of $y_{d}$ and $y_{p}$ are:\n$$\n\\mathbb{E}[y_{d}] = \\mathbb{E}[u^{\\ast} + \\varepsilon_{d}] = u^{\\ast} + \\mathbb{E}[\\varepsilon_{d}] = u^{\\ast}\n$$\n$$\n\\mathbb{E}[y_{p}] = \\mathbb{E}[u^{\\ast} + b_{p} + \\varepsilon_{p}] = u^{\\ast} + b_{p} + \\mathbb{E}[\\varepsilon_{p}] = u^{\\ast} + b_{p}\n$$\nSubstituting these into the expression for $\\mathbb{E}[u^{\\star}]$:\n$$\n\\mathbb{E}[u^{\\star}] = \\frac{1}{r+1} \\left( r\\,u^{\\ast} + (u^{\\ast} + b_{p}) \\right) = \\frac{(r+1)u^{\\ast} + b_{p}}{r+1} = u^{\\ast} + \\frac{b_{p}}{r+1}\n$$\nThe bias of the estimator is $\\operatorname{Bias}(u^{\\star}) = \\mathbb{E}[u^{\\star}] - u^{\\ast}$:\n$$\n\\operatorname{Bias}(u^{\\star}) = \\left(u^{\\ast} + \\frac{b_{p}}{r+1}\\right) - u^{\\ast} = \\frac{b_{p}}{r+1}\n$$\nThe squared bias is:\n$$\n\\operatorname{Bias}(u^{\\star})^{2} = \\frac{b_{p}^{2}}{(r+1)^{2}}\n$$\nThe variance of the estimator is $\\operatorname{Var}(u^{\\star}) = \\mathbb{E}[(u^{\\star} - \\mathbb{E}[u^{\\star}])^{2}]$. Assuming the noise terms $\\varepsilon_{d}$ and $\\varepsilon_{p}$ are statistically independent, $y_{d}$ and $y_{p}$ are also independent.\n$$\n\\operatorname{Var}(u^{\\star}) = \\operatorname{Var}\\left(\\frac{r\\,y_{d} + y_{p}}{r + 1}\\right) = \\frac{1}{(r+1)^{2}} \\operatorname{Var}(r\\,y_{d} + y_{p})\n$$\n$$\n\\operatorname{Var}(u^{\\star}) = \\frac{1}{(r+1)^{2}} \\left( r^{2}\\operatorname{Var}(y_{d}) + \\operatorname{Var}(y_{p}) \\right)\n$$\nThe variances of $y_{d}$ and $y_{p}$ are:\n$$\n\\operatorname{Var}(y_{d}) = \\operatorname{Var}(u^{\\ast} + \\varepsilon_{d}) = \\operatorname{Var}(\\varepsilon_{d}) = \\sigma_{d}^{2}\n$$\n$$\n\\operatorname{Var}(y_{p}) = \\operatorname{Var}(u^{\\ast} + b_{p} + \\varepsilon_{p}) = \\operatorname{Var}(\\varepsilon_{p}) = \\sigma_{p}^{2}\n$$\nSubstituting these into the expression for $\\operatorname{Var}(u^{\\star})$:\n$$\n\\operatorname{Var}(u^{\\star}) = \\frac{r^{2}\\sigma_{d}^{2} + \\sigma_{p}^{2}}{(r+1)^{2}}\n$$\nThe expected Mean Squared Error ($\\operatorname{MSE}$) is the sum of the squared bias and the variance:\n$$\n\\operatorname{MSE}(u^{\\star}) = \\operatorname{Bias}(u^{\\star})^{2} + \\operatorname{Var}(u^{\\star}) = \\frac{b_{p}^{2}}{(r+1)^{2}} + \\frac{r^{2}\\sigma_{d}^{2} + \\sigma_{p}^{2}}{(r+1)^{2}}\n$$\n$$\n\\operatorname{MSE}(u^{\\star}) = \\frac{r^{2}\\sigma_{d}^{2} + b_{p}^{2} + \\sigma_{p}^{2}}{(r+1)^{2}}\n$$\nTo find the optimal ratio $r_{\\text{opt}}$ that minimizes the $\\operatorname{MSE}$, we differentiate $\\operatorname{MSE}(u^{\\star})$ with respect to $r$ and set the result to zero. We assume $r \\ge 0$.\n$$\n\\frac{d}{dr}\\operatorname{MSE}(u^{\\star}) = \\frac{ (2r\\sigma_{d}^{2})(r+1)^{2} - (r^{2}\\sigma_{d}^{2} + b_{p}^{2} + \\sigma_{p}^{2})(2(r+1)) }{((r+1)^{2})^{2}} = 0\n$$\nAssuming $r+1 \\neq 0$, we can simplify the numerator:\n$$\n(2r\\sigma_{d}^{2})(r+1) - 2(r^{2}\\sigma_{d}^{2} + b_{p}^{2} + \\sigma_{p}^{2}) = 0\n$$\n$$\nr\\sigma_{d}^{2}(r+1) - (r^{2}\\sigma_{d}^{2} + b_{p}^{2} + \\sigma_{p}^{2}) = 0\n$$\n$$\nr^{2}\\sigma_{d}^{2} + r\\sigma_{d}^{2} - r^{2}\\sigma_{d}^{2} - b_{p}^{2} - \\sigma_{p}^{2} = 0\n$$\n$$\nr\\sigma_{d}^{2} = b_{p}^{2} + \\sigma_{p}^{2}\n$$\nSolving for $r$ gives the optimal ratio $r_{\\text{opt}}$, assuming $\\sigma_{d}^{2} > 0$:\n$$\nr_{\\text{opt}} = \\frac{b_{p}^{2} + \\sigma_{p}^{2}}{\\sigma_{d}^{2}}\n$$\nThis confirms the expression provided in the problem statement.\n\nFinally, we derive the minimized expected mean squared error, $\\operatorname{MSE}_{\\min}$, by substituting $r_{\\text{opt}}$ back into the $\\operatorname{MSE}(u^{\\star})$ formula.\n$$\n\\operatorname{MSE}_{\\min} = \\frac{r_{\\text{opt}}^{2}\\sigma_{d}^{2} + b_{p}^{2} + \\sigma_{p}^{2}}{(r_{\\text{opt}}+1)^{2}}\n$$\nFrom the optimality condition, we have $b_{p}^{2} + \\sigma_{p}^{2} = r_{\\text{opt}}\\sigma_{d}^{2}$. Substituting this into the numerator:\n$$\nr_{\\text{opt}}^{2}\\sigma_{d}^{2} + b_{p}^{2} + \\sigma_{p}^{2} = r_{\\text{opt}}^{2}\\sigma_{d}^{2} + r_{\\text{opt}}\\sigma_{d}^{2} = r_{\\text{opt}}\\sigma_{d}^{2}(r_{\\text{opt}} + 1)\n$$\nSo, the $\\operatorname{MSE}_{\\min}$ becomes:\n$$\n\\operatorname{MSE}_{\\min} = \\frac{r_{\\text{opt}}\\sigma_{d}^{2}(r_{\\text{opt}} + 1)}{(r_{\\text{opt}}+1)^{2}} = \\frac{r_{\\text{opt}}\\sigma_{d}^{2}}{r_{\\text{opt}}+1}\n$$\nTo obtain the closed-form expression in terms of the original parameters, we substitute the expression for $r_{\\text{opt}}$:\n$$\n\\operatorname{MSE}_{\\min} = \\frac{\\left(\\frac{b_{p}^{2} + \\sigma_{p}^{2}}{\\sigma_{d}^{2}}\\right)\\sigma_{d}^{2}}{\\frac{b_{p}^{2} + \\sigma_{p}^{2}}{\\sigma_{d}^{2}}+1} = \\frac{b_{p}^{2} + \\sigma_{p}^{2}}{\\frac{b_{p}^{2} + \\sigma_{p}^{2} + \\sigma_{d}^{2}}{\\sigma_{d}^{2}}}\n$$\n$$\n\\operatorname{MSE}_{\\min} = \\frac{(b_{p}^{2} + \\sigma_{p}^{2})\\sigma_{d}^{2}}{b_{p}^{2} + \\sigma_{p}^{2} + \\sigma_{d}^{2}}\n$$\nThese derived formulas for $r_{\\text{opt}}$ and $\\operatorname{MSE}_{\\min}$ will be used in the implementation.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates the optimal weight ratio and minimum MSE for given PINN parameters.\n    \"\"\"\n    # The test suite, where each tuple is (sigma_d^2, sigma_p^2, b_p, c).\n    # The parameter c is not needed for the final computation but is part of the problem definition.\n    test_cases = [\n        (0.01, 0.02, 0.05, 1.3),\n        (1e-4, 0.02, 0.05, 0.7),\n        (1.0, 0.01, 0.0, 2.0),\n        (0.5, 0.0, 0.0, 1.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        sigma_d_sq, sigma_p_sq, b_p, c = case\n\n        # The sum of squared physics bias and physics variance. This term represents the\n        # total squared error contribution from the physics model.\n        phys_error_sq_sum = b_p**2 + sigma_p_sq\n\n        # 1. Calculate the optimal ratio r_opt.\n        # This ratio balances the data misfit against the physics residual.\n        # It is derived by minimizing the Mean Squared Error (MSE) of the estimator.\n        # r_opt = (b_p^2 + sigma_p^2) / sigma_d^2\n        # The derivation assumes sigma_d_sq > 0, which holds for all test cases.\n        r_opt = phys_error_sq_sum / sigma_d_sq\n\n        # 2. Calculate the corresponding minimized expected mean squared error MSE_min.\n        # This is the minimum achievable MSE for the estimator u_star when the\n        # weights are chosen optimally according to r_opt.\n        # MSE_min = ((b_p^2 + sigma_p^2) * sigma_d^2) / (b_p^2 + sigma_p^2 + sigma_d^2)\n        denominator = phys_error_sq_sum + sigma_d_sq\n        \n        # This condition handles the case where both data and physics are perfect,\n        # resulting in zero error.\n        if denominator == 0:\n            mse_min = 0.0\n        else:\n            mse_min = phys_error_sq_sum * sigma_d_sq / denominator\n\n        results.append(r_opt)\n        results.append(mse_min)\n\n    # Print the final results in the specified single-line format.\n    # The output is a flat list of [r_opt_1, mse_min_1, r_opt_2, mse_min_2, ...].\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the main function.\nsolve()\n```"
        },
        {
            "introduction": "Once a PINN is trained, we must verify that it has learned a valid solution to the governing partial differential equations. The Method of Manufactured Solutions (MMS) is a powerful and standard technique in scientific computing for rigorously assessing the accuracy of a numerical solver. This practice applies MMS to the context of PINNs, demonstrating how to construct a verification test to quantify the network's ability to satisfy the governing physics and boundary conditions .",
            "id": "4050020",
            "problem": "Consider the one-dimensional, steady, constant-property conservation equations for temperature and species mass fractions appropriate to computational combustion. Let $x \\in [0,1]$ be the spatial coordinate in meters. Assume constant density $\\rho$ (kilograms per cubic meter), constant thermal conductivity $k_t$ (watts per meter-kelvin), constant species diffusivities $D_k$ (square meters per second), and constant species specific enthalpies $h_k$ (joules per kilogram). The governing equations are:\n\nEnergy conservation:\n$$ k_t \\frac{d^2 T}{dx^2} + \\sum_{k=1}^K h_k \\, \\omega_k(x) + q_{\\text{ext}}(x) = 0, $$\n\nSpecies conservation for each species $k \\in \\{1,\\dots,K\\}$:\n$$ \\rho D_k \\frac{d^2 Y_k}{dx^2} + \\omega_k(x) = 0. $$\n\nHere, $T(x)$ is the temperature in kelvin, $Y_k(x)$ are species mass fractions (dimensionless), $\\omega_k(x)$ are reaction source terms in kilograms per cubic meter per second, and $q_{\\text{ext}}(x)$ is an external heat source in watts per cubic meter. Boundary conditions are Dirichlet at $x=0$ and $x=1$ for both $T$ and $Y_k$.\n\nThe Method of Manufactured Solutions (MMS) prescribes analytic fields $T_{\\text{true}}(x)$ and $Y_{k,\\text{true}}(x)$ and then computes source terms so that these fields exactly satisfy the governing equations. Specifically, define\n$$ \\omega_k(x) = - \\rho D_k \\frac{d^2 Y_{k,\\text{true}}}{dx^2}, \\qquad q_{\\text{ext}}(x) = - k_t \\frac{d^2 T_{\\text{true}}}{dx^2} - \\sum_{k=1}^K h_k \\, \\omega_k(x). $$\nA Physics-Informed Neural Network (PINN) is expected to recover the manufactured fields by minimizing residuals of the governing equations and boundary conditions. To verify and estimate error, we define predicted fields as $T_{\\text{pred}}(x)$ and $Y_{k,\\text{pred}}(x)$, compute residuals, and quantify error norms.\n\nDefine the PINN residuals for the energy and species equations, evaluated at $N$ uniformly spaced collocation points $\\{x_i\\}_{i=1}^N$ in $[0,1]$, as\n$$ r_T(x_i) = k_t \\frac{d^2 T_{\\text{pred}}}{dx^2}(x_i) + \\sum_{k=1}^K h_k \\, \\omega_k(x_i) + q_{\\text{ext}}(x_i), $$\n$$ r_{Y_k}(x_i) = \\rho D_k \\frac{d^2 Y_{k,\\text{pred}}}{dx^2}(x_i) + \\omega_k(x_i). $$\nDefine the normalized $L_2$ residual norms:\n$$ \\| r_T \\|_{L_2} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} \\left[r_T(x_i)\\right]^2}, \\qquad \\| r_{Y_k} \\|_{L_2} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} \\left[r_{Y_k}(x_i)\\right]^2}. $$\nDefine boundary condition mismatch norms using the values at the endpoints:\n$$ \\mathrm{BC}_T = \\sqrt{\\frac{1}{2} \\left( \\left[T_{\\text{pred}}(0) - T_{\\text{true}}(0)\\right]^2 + \\left[T_{\\text{pred}}(1) - T_{\\text{true}}(1)\\right]^2 \\right)}, $$\n$$ \\mathrm{BC}_{Y_k} = \\sqrt{\\frac{1}{2} \\left( \\left[Y_{k,\\text{pred}}(0) - Y_{k,\\text{true}}(0)\\right]^2 + \\left[Y_{k,\\text{pred}}(1) - Y_{k,\\text{true}}(1)\\right]^2 \\right)}. $$\n\nYour task is to implement a self-contained program that performs manufactured solution tests by prescribing analytic $T_{\\text{true}}(x)$ and $Y_{k,\\text{true}}(x)$, constructing $\\omega_k(x)$ and $q_{\\text{ext}}(x)$ accordingly, and then quantifying the PINN residual norms and boundary mismatch norms for a set of test cases. The PINN \"predicted\" fields will be taken as the manufactured fields plus known analytic perturbations that emulate approximation error.\n\nUnits:\n- Report $\\| r_T \\|_{L_2}$ in watts per cubic meter.\n- Report each $\\| r_{Y_k} \\|_{L_2}$ in kilograms per cubic meter per second.\n- Report $\\mathrm{BC}_T$ in kelvin.\n- Report each $\\mathrm{BC}_{Y_k}$ as dimensionless.\n\nUse $N=200$ collocation points uniformly spaced in $[0,1]$ for all test cases.\n\nTest Suite:\n- Test Case 1 (Happy path, no perturbations, single species):\n  - $K=1$,\n  - $\\rho = 1.0 \\, \\mathrm{kg} \\, \\mathrm{m}^{-3}$, $k_t = 0.1 \\, \\mathrm{W} \\, \\mathrm{m}^{-1} \\, \\mathrm{K}^{-1}$,\n  - $D_1 = 1.0 \\times 10^{-5} \\, \\mathrm{m}^2 \\, \\mathrm{s}^{-1}$, $h_1 = -1.0 \\times 10^{6} \\, \\mathrm{J} \\, \\mathrm{kg}^{-1}$,\n  - $T_{\\text{true}}(x) = 300 + 10 x + 5 \\sin(2 \\pi x)$, $Y_{1,\\text{true}}(x) = 0.2 + 0.1 x + 0.05 \\cos(\\pi x)$,\n  - Predicted fields equal manufactured fields: $T_{\\text{pred}} = T_{\\text{true}}$, $Y_{1,\\text{pred}} = Y_{1,\\text{true}}$.\n- Test Case 2 (Perturbed interior, boundary preserved, single species):\n  - $K=1$,\n  - $\\rho = 1.2 \\, \\mathrm{kg} \\, \\mathrm{m}^{-3}$, $k_t = 0.08 \\, \\mathrm{W} \\, \\mathrm{m}^{-1} \\, \\mathrm{K}^{-1}$,\n  - $D_1 = 2.0 \\times 10^{-5} \\, \\mathrm{m}^2 \\, \\mathrm{s}^{-1}$, $h_1 = -8.0 \\times 10^{5} \\, \\mathrm{J} \\, \\mathrm{kg}^{-1}$,\n  - $T_{\\text{true}}(x) = 290 + 15 x + 3 \\sin(2 \\pi x)$, $Y_{1,\\text{true}}(x) = 0.25 + 0.05 x + 0.02 \\cos(2 \\pi x)$,\n  - Predicted perturbations: $T_{\\text{pred}}(x) = T_{\\text{true}}(x) + \\epsilon_T \\sin(13 \\pi x)$ with $\\epsilon_T = 0.1$ (kelvin), $Y_{1,\\text{pred}}(x) = Y_{1,\\text{true}}(x) + \\epsilon_{Y_1} \\sin(11 \\pi x)$ with $\\epsilon_{Y_1} = 1.0 \\times 10^{-4}$ (dimensionless). These perturbations vanish at $x=0$ and $x=1$, so boundary conditions are preserved.\n- Test Case 3 (Boundary-mismatched perturbations, two species):\n  - $K=2$,\n  - $\\rho = 0.9 \\, \\mathrm{kg} \\, \\mathrm{m}^{-3}$, $k_t = 0.12 \\, \\mathrm{W} \\, \\mathrm{m}^{-1} \\, \\mathrm{K}^{-1}$,\n  - $D_1 = 1.0 \\times 10^{-5} \\, \\mathrm{m}^2 \\, \\mathrm{s}^{-1}$, $D_2 = 5.0 \\times 10^{-6} \\, \\mathrm{m}^2 \\, \\mathrm{s}^{-1}$,\n  - $h_1 = -8.0 \\times 10^{5} \\, \\mathrm{J} \\, \\mathrm{kg}^{-1}$, $h_2 = -1.2 \\times 10^{6} \\, \\mathrm{J} \\, \\mathrm{kg}^{-1}$,\n  - $T_{\\text{true}}(x) = 1200 + 100 x - 20 x^2$, $Y_{1,\\text{true}}(x) = 0.7 - 0.1 x + 0.02 \\sin(2 \\pi x)$, $Y_{2,\\text{true}}(x) = 0.3 + 0.05 x + 0.01 \\sin(\\pi x)$,\n  - Predicted perturbations: $T_{\\text{pred}}(x) = T_{\\text{true}}(x) + \\epsilon_T \\cos(\\pi x)$ with $\\epsilon_T = 0.5$ (kelvin), $Y_{1,\\text{pred}}(x) = Y_{1,\\text{true}}(x) + \\epsilon_{Y_1} \\cos(\\pi x)$ with $\\epsilon_{Y_1} = 1.0 \\times 10^{-3}$, $Y_{2,\\text{pred}}(x) = Y_{2,\\text{true}}(x) + \\epsilon_{Y_2} \\cos(\\pi x)$ with $\\epsilon_{Y_2} = 5.0 \\times 10^{-4}$. These perturbations do not vanish at $x=0$ and $x=1$, so boundary conditions are violated.\n\nYour program must:\n1. For each test case, construct $\\omega_k(x)$ and $q_{\\text{ext}}(x)$ from the manufactured fields $T_{\\text{true}}$ and $Y_{k,\\text{true}}$ using the equations above.\n2. Evaluate residuals $r_T(x_i)$ and $r_{Y_k}(x_i)$ at $N=200$ collocation points and compute the normalized $L_2$ norms. Report $\\| r_T \\|_{L_2}$ in watts per cubic meter and each $\\| r_{Y_k} \\|_{L_2}$ in kilograms per cubic meter per second.\n3. Compute boundary mismatch norms $\\mathrm{BC}_T$ (kelvin) and each $\\mathrm{BC}_{Y_k}$ (dimensionless).\n4. Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a list of the form $[ \\| r_T \\|_{L_2}, [\\| r_{Y_1} \\|_{L_2}, \\dots], \\mathrm{BC}_T, [\\mathrm{BC}_{Y_1}, \\dots] ]$. The final output should therefore be a single list containing three elements (one per test case), for example: \"[[rT1,[rY11],bcT1,[bcY11]],[[rT2,[rY21],bcT2,[bcY21]],[[rT3,[rY31,rY32],bcT3,[bcY31,bcY32]]]\". Ensure no spaces appear in the printed output.\n\nAll quantities must be handled in their specified units. Angles inside trigonometric functions are radians. The program must be self-contained and require no input.",
            "solution": "The user has provided a problem that requires the computation of residuals and boundary condition mismatches for a Physics-Informed Neural Network (PINN) verification test using the Method of Manufactured Solutions (MMS) in the context of one-dimensional, steady-state combustion. The problem is well-defined, scientifically sound, and internally consistent.\n\nThe core of the problem involves substituting manufactured analytical solutions into the governing differential equations to define source terms, and then computing the residuals that arise when a perturbed \"predicted\" solution is used instead.\n\nThe governing equations are:\nEnergy: $k_t \\frac{d^2 T}{dx^2} + \\sum_{k=1}^K h_k \\omega_k(x) + q_{\\text{ext}}(x) = 0$\nSpecies: $\\rho D_k \\frac{d^2 Y_k}{dx^2} + \\omega_k(x) = 0$\n\nThe MMS defines the source terms $\\omega_k(x)$ and $q_{\\text{ext}}(x)$ such that the \"true\" manufactured fields, $T_{\\text{true}}(x)$ and $Y_{k,\\text{true}}(x)$, are exact solutions.\nFrom the species equation:\n$$ \\omega_k(x) = - \\rho D_k \\frac{d^2 Y_{k,\\text{true}}}{dx^2} $$\nFrom the energy equation:\n$$ q_{\\text{ext}}(x) = - k_t \\frac{d^2 T_{\\text{true}}}{dx^2} - \\sum_{k=1}^K h_k \\omega_k(x) $$\n\nThe PINN residuals are calculated using the \"predicted\" fields, $T_{\\text{pred}}(x)$ and $Y_{k,\\text{pred}}(x)$, along with the manufactured source terms:\n$$ r_T(x) = k_t \\frac{d^2 T_{\\text{pred}}}{dx^2} + \\sum_{k=1}^K h_k \\omega_k(x) + q_{\\text{ext}}(x) $$\n$$ r_{Y_k}(x) = \\rho D_k \\frac{d^2 Y_{k,\\text{pred}}}{dx^2} + \\omega_k(x) $$\nA crucial step is to substitute the expressions for the manufactured source terms into the residual equations. This simplification reveals the underlying structure of the MMS test.\n\nFor the species residual $r_{Y_k}(x)$:\n$$ r_{Y_k}(x) = \\rho D_k \\frac{d^2 Y_{k,\\text{pred}}}{dx^2} + \\left( - \\rho D_k \\frac{d^2 Y_{k,\\text{true}}}{dx^2} \\right) = \\rho D_k \\left( \\frac{d^2 Y_{k,\\text{pred}}}{dx^2} - \\frac{d^2 Y_{k,\\text{true}}}{dx^2} \\right) $$\nLet the prediction error be $\\delta Y_k(x) = Y_{k,\\text{pred}}(x) - Y_{k,\\text{true}}(x)$. The residual simplifies to:\n$$ r_{Y_k}(x) = \\rho D_k \\frac{d^2 (\\delta Y_k)}{dx^2} $$\n\nFor the energy residual $r_T(x)$:\n$$ r_T(x) = k_t \\frac{d^2 T_{\\text{pred}}}{dx^2} + \\sum_{k=1}^K h_k \\omega_k(x) + \\left( - k_t \\frac{d^2 T_{\\text{true}}}{dx^2} - \\sum_{k=1}^K h_k \\omega_k(x) \\right) = k_t \\left( \\frac{d^2 T_{\\text{pred}}}{dx^2} - \\frac{d^2 T_{\\text{true}}}{dx^2} \\right) $$\nLet the prediction error be $\\delta T(x) = T_{\\text{pred}}(x) - T_{\\text{true}}(x)$. The residual simplifies to:\n$$ r_T(x) = k_t \\frac{d^2 (\\delta T)}{dx^2} $$\n\nThe implementation will proceed by calculating the analytical second derivatives of the error functions ($\\delta T$, $\\delta Y_k$) for each test case, evaluating the residuals at $N=200$ collocation points, and then computing the specified $L_2$ norms. The boundary condition mismatches are computed directly from the error functions at $x=0$ and $x=1$.\n\nThe analytical derivatives of the error functions are:\n**Test Case 1:**\n- $\\delta T(x) = 0$, $\\delta Y_1(x) = 0$.\n- $\\frac{d^2(\\delta T)}{dx^2} = 0$, $\\frac{d^2(\\delta Y_1)}{dx^2} = 0$.\n- Thus, all residuals and boundary mismatches are $0$.\n\n**Test Case 2:**\n- $\\delta T(x) = \\epsilon_T \\sin(13 \\pi x)$, with $\\epsilon_T = 0.1$.\n- $\\frac{d^2(\\delta T)}{dx^2} = - \\epsilon_T (13\\pi)^2 \\sin(13 \\pi x) = -169 \\pi^2 \\epsilon_T \\sin(13 \\pi x)$.\n- $\\delta Y_1(x) = \\epsilon_{Y_1} \\sin(11 \\pi x)$, with $\\epsilon_{Y_1} = 10^{-4}$.\n- $\\frac{d^2(\\delta Y_1)}{dx^2} = - \\epsilon_{Y_1} (11\\pi)^2 \\sin(11 \\pi x) = -121 \\pi^2 \\epsilon_{Y_1} \\sin(11 \\pi x)$.\n- At the boundaries, $\\sin(n\\pi x)$ for integer $n$ is $0$ at $x=0$ and $x=1$. Thus, boundary mismatches $\\mathrm{BC}_T$ and $\\mathrm{BC}_{Y_1}$ are $0$.\n\n**Test Case 3:**\n- $\\delta T(x) = \\epsilon_T \\cos(\\pi x)$, with $\\epsilon_T = 0.5$.\n- $\\frac{d^2(\\delta T)}{dx^2} = - \\epsilon_T \\pi^2 \\cos(\\pi x)$.\n- $\\delta Y_1(x) = \\epsilon_{Y_1} \\cos(\\pi x)$, with $\\epsilon_{Y_1} = 10^{-3}$.\n- $\\frac{d^2(\\delta Y_1)}{dx^2} = - \\epsilon_{Y_1} \\pi^2 \\cos(\\pi x)$.\n- $\\delta Y_2(x) = \\epsilon_{Y_2} \\cos(\\pi x)$, with $\\epsilon_{Y_2} = 5 \\times 10^{-4}$.\n- $\\frac{d^2(\\delta Y_2)}{dx^2} = - \\epsilon_{Y_2} \\pi^2 \\cos(\\pi x)$.\n- At the boundaries, $\\delta(x) = \\epsilon \\cos(\\pi x)$ gives $\\delta(0) = \\epsilon$ and $\\delta(1) = -\\epsilon$.\n- The boundary mismatch norm is $\\sqrt{\\frac{1}{2}(\\epsilon^2 + (-\\epsilon)^2)} = |\\epsilon|$.\n- Thus, $\\mathrm{BC}_T = |\\epsilon_T| = 0.5$, $\\mathrm{BC}_{Y_1} = |\\epsilon_{Y_1}| = 0.001$, and $\\mathrm{BC}_{Y_2} = |\\epsilon_{Y_2}| = 0.0005$.\n\nThe program will implement these calculations for each test case, assemble the results in the specified list-of-lists structure, and print the final formatted string.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_case_1(x_grid, pi):\n    \"\"\"\n    Calculates results for Test Case 1.\n    \"\"\"\n    # Parameters\n    # K = 1\n    # rho = 1.0\n    # kt = 0.1\n    # D1 = 1.0e-5\n    # h1 = -1.0e6\n\n    # Perturbations are zero\n    # delta_T(x) = 0, delta_Y1(x) = 0\n    # The derivatives are also zero.\n    \n    # Residuals are zero\n    rT_L2 = 0.0\n    rY1_L2 = 0.0\n    \n    # Boundary mismatches are zero\n    bcT = 0.0\n    bcY1 = 0.0\n    \n    return [rT_L2, [rY1_L2], bcT, [bcY1]]\n\ndef calculate_case_2(x_grid, pi):\n    \"\"\"\n    Calculates results for Test Case 2.\n    \"\"\"\n    # Parameters\n    # K = 1\n    rho = 1.2\n    kt = 0.08\n    D1 = 2.0e-5\n    # h1 = -8.0e5\n    eps_T = 0.1\n    eps_Y1 = 1.0e-4\n\n    # Calculate residuals\n    # r_T(x) = kt * d^2(delta_T)/dx^2\n    # delta_T(x) = eps_T * sin(13*pi*x)\n    # d^2(delta_T)/dx^2 = -eps_T * (13*pi)^2 * sin(13*pi*x)\n    d2_delta_T_dx2 = -eps_T * (13 * pi)**2 * np.sin(13 * pi * x_grid)\n    rT_values = kt * d2_delta_T_dx2\n    rT_L2 = np.sqrt(np.mean(rT_values**2))\n\n    # r_Y1(x) = rho * D1 * d^2(delta_Y1)/dx^2\n    # delta_Y1(x) = eps_Y1 * sin(11*pi*x)\n    # d^2(delta_Y1)/dx^2 = -eps_Y1 * (11*pi)^2 * sin(11*pi*x)\n    d2_delta_Y1_dx2 = -eps_Y1 * (11 * pi)**2 * np.sin(11 * pi * x_grid)\n    rY1_values = rho * D1 * d2_delta_Y1_dx2\n    rY1_L2 = np.sqrt(np.mean(rY1_values**2))\n    \n    # Calculate boundary mismatches\n    # delta_T(x) = eps_T * sin(13*pi*x)\n    delta_T_at_0 = 0.0  # sin(0)\n    delta_T_at_1 = 0.0  # sin(13*pi)\n    bcT = np.sqrt(0.5 * (delta_T_at_0**2 + delta_T_at_1**2))\n    \n    # delta_Y1(x) = eps_Y1 * sin(11*pi*x)\n    delta_Y1_at_0 = 0.0  # sin(0)\n    delta_Y1_at_1 = 0.0  # sin(11*pi)\n    bcY1 = np.sqrt(0.5 * (delta_Y1_at_0**2 + delta_Y1_at_1**2))\n    \n    return [rT_L2, [rY1_L2], bcT, [bcY1]]\n    \ndef calculate_case_3(x_grid, pi):\n    \"\"\"\n    Calculates results for Test Case 3.\n    \"\"\"\n    # Parameters\n    # K = 2\n    rho = 0.9\n    kt = 0.12\n    D = [1.0e-5, 5.0e-6]\n    # h = [-8.0e5, -1.2e6]\n    eps_T = 0.5\n    eps_Y = [1.0e-3, 5.0e-4]\n\n    # Calculate temperature residual\n    # delta_T(x) = eps_T * cos(pi*x)\n    # d^2(delta_T)/dx^2 = -eps_T * pi^2 * cos(pi*x)\n    d2_delta_T_dx2 = -eps_T * pi**2 * np.cos(pi * x_grid)\n    rT_values = kt * d2_delta_T_dx2\n    rT_L2 = np.sqrt(np.mean(rT_values**2))\n\n    # Calculate species residuals\n    rY_L2_list = []\n    # All species have same perturbation form: delta_Y_k(x) = eps_Yk * cos(pi*x)\n    d2_delta_Y_common_part_dx2 = -pi**2 * np.cos(pi * x_grid)\n    for k in range(2):\n        d2_delta_Yk_dx2 = eps_Y[k] * d2_delta_Y_common_part_dx2\n        rYk_values = rho * D[k] * d2_delta_Yk_dx2\n        rYk_L2 = np.sqrt(np.mean(rYk_values**2))\n        rY_L2_list.append(rYk_L2)\n\n    # Calculate boundary mismatches\n    # For a perturbation delta(x) = epsilon * cos(pi*x), the BC norm is |epsilon|\n    bcT = np.abs(eps_T)\n    bcY_list = [np.abs(eps_Y[k]) for k in range(2)]\n    \n    return [rT_L2, rY_L2_list, bcT, bcY_list]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Define common parameters\n    N = 200\n    x_grid = np.linspace(0.0, 1.0, N)\n    pi = np.pi\n\n    # Run all test cases\n    results = [\n        calculate_case_1(x_grid, pi),\n        calculate_case_2(x_grid, pi),\n        calculate_case_3(x_grid, pi)\n    ]\n\n    # Format the output string as specified\n    # Convert list of lists to string, then remove spaces\n    output_str = ','.join(map(str, results))\n    final_output_str = f\"[{output_str}]\".replace(\" \", \"\")\n\n    print(final_output_str)\n\nsolve()\n```"
        }
    ]
}