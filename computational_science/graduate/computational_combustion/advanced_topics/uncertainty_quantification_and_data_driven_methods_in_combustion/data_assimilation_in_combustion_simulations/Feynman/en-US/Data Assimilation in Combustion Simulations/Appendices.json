{
    "hands_on_practices": [
        {
            "introduction": "Many diagnostic measurements in combustion, such as chemiluminescence, are nonlinear functions of the underlying thermodynamic state. The Extended Kalman Filter (EKF) provides a foundational method for assimilating such data by linearizing the nonlinear model at the current state estimate. This practice will guide you through the essential steps of deriving the Jacobian of a nonlinear observation model based on Arrhenius kinetics and applying it within the EKF analysis equations, a core skill for tackling nonlinear estimation problems. ",
            "id": "4016225",
            "problem": "Consider a discretized, single-cell surrogate of a chemically reacting flow used in computational combustion, where the chemiluminescence signal is modeled as proportional to the volumetric heat release rate. Assume a single-step, irreversible, exothermic reaction between a generic fuel and oxidizer. The observation operator is defined by the nondimensionalized Arrhenius form and mass-action law as follows. Let the state vector be $x = [T, Y_F, Y_O]^\\top$, where $T$ is the nondimensional temperature, $Y_F$ is the nondimensional mass fraction of fuel, and $Y_O$ is the nondimensional mass fraction of oxidizer. The observation (chemiluminescence intensity) is\n$$\nh(x) = y = \\int \\dot{q}(x)\\,\\mathrm{d}V = A \\exp\\!\\left(-\\frac{E}{R\\,T}\\right) Y_F Y_O,\n$$\nwhere $A$ is a nondimensional pre-exponential factor (absorbing constant density and volume), $E$ is a nondimensional activation energy, and $R$ is a nondimensional gas constant. Assume $V = 1$ and a constant heat release per reaction also nondimensionalized into $A$. This surrogate follows from the well-tested Arrhenius kinetics and the proportionality of chemiluminescence to heat release rate when self-absorption is negligible.\n\nTask:\n- Starting from the Arrhenius law and the mass-action rate expression, derive the Jacobian of the observation operator $h(x)$ with respect to the state $(T, Y_F, Y_O)$.\n- Use the derived Jacobian in an Extended Kalman Filter (EKF) analysis (update) step for a scalar measurement $y^{\\mathrm{meas}}$ with scalar measurement noise variance $R^{\\mathrm{meas}}$. The EKF analysis step updates the forecast mean $x^f$ and forecast covariance $P^f$ to the analysis mean $x^a$ and analysis covariance $P^a$ using the linearization of $h$ at $x^f$.\n\nAssumptions:\n- All quantities are nondimensional.\n- The single-cell surrogate implies that the spatial integral reduces to a product with $V=1$.\n- The gas density is absorbed into $A$, and stoichiometric exponents are $1$ for both reactants in the mass-action law.\n\nYour program must:\n- Implement the observation operator $h(x)$ and its Jacobian $H(x) = \\partial h/\\partial x$ as derived.\n- Implement the EKF analysis step to compute $x^a$ and $P^a$ for each test case.\n\nRequired output:\n- For each test case, output the analysis state $x^a = [T^a, Y_F^a, Y_O^a]$.\n- The final program output must be a single line containing the flattened list of the analysis states for all test cases, in order, as a comma-separated list enclosed in square brackets, with each value rounded to $6$ decimal digits (e.g., $[T_1^a,Y_{F,1}^a,Y_{O,1}^a,T_2^a,Y_{F,2}^a,Y_{O,2}^a,\\dots]$).\n\nAngle units: no angles are involved.\nPhysical units: all variables are nondimensional.\n\nTest suite:\n- Case $1$ (happy path):\n  - $A = 2.0$, $E = 5.0$, $R = 1.0$.\n  - Forecast mean $x^f = [1.0, 0.2, 0.5]^\\top$.\n  - Forecast covariance $P^f = \\mathrm{diag}([1.0\\times 10^{-3}, 2.5\\times 10^{-4}, 2.5\\times 10^{-4}])$.\n  - Measurement $y^{\\mathrm{meas}} = 0.0017$.\n  - Measurement variance $R^{\\mathrm{meas}} = 1.0\\times 10^{-4}$.\n- Case $2$ (boundary: near-zero fuel):\n  - $A = 2.0$, $E = 5.0$, $R = 1.0$.\n  - Forecast mean $x^f = [1.0, 1.0\\times 10^{-6}, 0.3]^\\top$.\n  - Forecast covariance $P^f = \\mathrm{diag}([1.0\\times 10^{-3}, 1.0\\times 10^{-10}, 2.5\\times 10^{-4}])$.\n  - Measurement $y^{\\mathrm{meas}} = 4.0\\times 10^{-9}$.\n  - Measurement variance $R^{\\mathrm{meas}} = 1.0\\times 10^{-8}$.\n- Case $3$ (edge: high temperature):\n  - $A = 2.0$, $E = 5.0$, $R = 1.0$.\n  - Forecast mean $x^f = [5.0, 0.4, 0.4]^\\top$.\n  - Forecast covariance $P^f = \\mathrm{diag}([1.0\\times 10^{-3}, 2.5\\times 10^{-4}, 2.5\\times 10^{-4}])$.\n  - Measurement $y^{\\mathrm{meas}} = 0.11770000$.\n  - Measurement variance $R^{\\mathrm{meas}} = 1.0\\times 10^{-4}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,\\dots]$), where each result is a float value rounded to $6$ decimal digits and the list is the concatenation of all analysis states, in order of the test cases.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of chemical kinetics (Arrhenius law, mass-action law) and data assimilation (Extended Kalman Filter), well-posed with a complete set of givens, and expressed in objective, formal language. The provided parameters and conditions are consistent and sufficient for deriving a unique, verifiable solution.\n\nThe task is to compute the analysis state, $x^a$, of an Extended Kalman Filter (EKF) update step for a single-cell combustion model. This involves two main parts: first, deriving the Jacobian of the nonlinear observation operator, and second, applying the standard EKF analysis equations.\n\nLet the state vector be $x = [T, Y_F, Y_O]^\\top$, where $T$ is the nondimensional temperature, $Y_F$ is the nondimensional fuel mass fraction, and $Y_O$ is the nondimensional oxidizer mass fraction.\n\nThe observation operator, $h(x)$, models the measured chemiluminescence intensity and is given as:\n$$\nh(x) = A \\exp\\left(-\\frac{E}{RT}\\right) Y_F Y_O\n$$\nwhere $A$ is the pre-exponential factor, $E$ is the activation energy, and $R$ is the gas constant, all in nondimensional form.\n\n**1. Derivation of the Jacobian**\n\nThe Jacobian of the observation operator, $H(x)$, is a $1 \\times 3$ matrix (a row vector) defined as $H(x) = \\frac{\\partial h}{\\partial x} = \\begin{bmatrix} \\frac{\\partial h}{\\partial T} & \\frac{\\partial h}{\\partial Y_F} & \\frac{\\partial h}{\\partial Y_O} \\end{bmatrix}$. We compute each partial derivative.\n\nThe partial derivative with respect to temperature $T$ is found using the chain rule:\n$$\n\\frac{\\partial h}{\\partial T} = \\frac{\\partial}{\\partial T} \\left( A \\exp\\left(-\\frac{E}{RT}\\right) Y_F Y_O \\right) = A Y_F Y_O \\cdot \\exp\\left(-\\frac{E}{RT}\\right) \\cdot \\frac{\\partial}{\\partial T}\\left(-\\frac{E}{RT}\\right)\n$$\n$$\n\\frac{\\partial h}{\\partial T} = \\left( A \\exp\\left(-\\frac{E}{RT}\\right) Y_F Y_O \\right) \\cdot \\left(\\frac{E}{RT^2}\\right) = h(x) \\frac{E}{RT^2}\n$$\n\nThe partial derivative with respect to fuel mass fraction $Y_F$ is:\n$$\n\\frac{\\partial h}{\\partial Y_F} = \\frac{\\partial}{\\partial Y_F} \\left( A \\exp\\left(-\\frac{E}{RT}\\right) Y_F Y_O \\right) = A \\exp\\left(-\\frac{E}{RT}\\right) Y_O\n$$\n\nThe partial derivative with respect to oxidizer mass fraction $Y_O$ is:\n$$\n\\frac{\\partial h}{\\partial Y_O} = \\frac{\\partial}{\\partial Y_O} \\left( A \\exp\\left(-\\frac{E}{RT}\\right) Y_F Y_O \\right) = A \\exp\\left(-\\frac{E}{RT}\\right) Y_F\n$$\n\nCombining these, the Jacobian of the observation operator is:\n$$\nH(x) = \\left[ h(x) \\frac{E}{RT^2}, \\quad A \\exp\\left(-\\frac{E}{RT}\\right) Y_O, \\quad A \\exp\\left(-\\frac{E}{RT}\\right) Y_F \\right]\n$$\n\n**2. Extended Kalman Filter (EKF) Analysis Step**\n\nThe EKF analysis step updates the forecast state mean $x^f$ and covariance $P^f$ to the analysis state mean $x^a$ and covariance $P^a$ using a measurement $y^{\\mathrm{meas}}$ with variance $R^{\\mathrm{meas}}$. The update equations are as follows:\n\nLet $H$ be the Jacobian evaluated at the forecast mean, $H = H(x^f)$.\n\na. **Innovation (or Residual):** The difference between the measurement and the predicted measurement. This is a scalar value, $\\nu$.\n$$\n\\nu = y^{\\mathrm{meas}} - h(x^f)\n$$\n\nb. **Innovation Covariance:** The variance of the innovation. This is also a scalar, $S$.\n$$\nS = H P^f H^\\top + R^{\\mathrm{meas}}\n$$\n\nc. **Kalman Gain:** The weight given to the innovation. It is a $3 \\times 1$ column vector, $K$.\n$$\nK = P^f H^\\top S^{-1} = \\frac{P^f H^\\top}{S}\n$$\n\nd. **Analysis Mean:** The updated state vector.\n$$\nx^a = x^f + K \\nu\n$$\n\ne. **Analysis Covariance:** The updated covariance matrix. Though not required for the final output, the formula is:\n$$\nP^a = (I - KH)P^f\n$$\nwhere $I$ is the $3 \\times 3$ identity matrix.\n\nThe procedure for each test case is to substitute the given values of $A, E, R, x^f, P^f, y^{\\mathrm{meas}},$ and $R^{\\mathrm{meas}}$ into these equations to compute the analysis state $x^a$. The following program implements this procedure.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the EKF analysis problem for the given test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"params\": {\"A\": 2.0, \"E\": 5.0, \"R\": 1.0},\n            \"x_f\": np.array([1.0, 0.2, 0.5]),\n            \"P_f\": np.diag([1.0e-3, 2.5e-4, 2.5e-4]),\n            \"y_meas\": 0.0017,\n            \"R_meas\": 1.0e-4,\n        },\n        {\n            \"params\": {\"A\": 2.0, \"E\": 5.0, \"R\": 1.0},\n            \"x_f\": np.array([1.0, 1.0e-6, 0.3]),\n            \"P_f\": np.diag([1.0e-3, 1.0e-10, 2.5e-4]),\n            \"y_meas\": 4.0e-9,\n            \"R_meas\": 1.0e-8,\n        },\n        {\n            \"params\": {\"A\": 2.0, \"E\": 5.0, \"R\": 1.0},\n            \"x_f\": np.array([5.0, 0.4, 0.4]),\n            \"P_f\": np.diag([1.0e-3, 2.5e-4, 2.5e-4]),\n            \"y_meas\": 0.11770000,\n            \"R_meas\": 1.0e-4,\n        },\n    ]\n\n    all_results = []\n\n    def h_operator(x, A, E, R_const):\n        \"\"\"\n        Computes the observation operator h(x).\n        x = [T, Y_F, Y_O]\n        \"\"\"\n        T, Y_F, Y_O = x\n        if T = 0:  # Avoid division by zero or log of non-positive\n            return 0.0\n        return A * np.exp(-E / (R_const * T)) * Y_F * Y_O\n\n    def H_jacobian(x, A, E, R_const):\n        \"\"\"\n        Computes the Jacobian of the observation operator, H(x) = dh/dx.\n        x = [T, Y_F, Y_O]\n        \"\"\"\n        T, Y_F, Y_O = x\n        if T = 0:\n            return np.zeros(3)\n\n        exp_term = np.exp(-E / (R_const * T))\n        h_val = A * exp_term * Y_F * Y_O\n\n        # Partial derivatives\n        dh_dT = h_val * E / (R_const * T**2)\n        dh_dYF = A * exp_term * Y_O\n        dh_dYO = A * exp_term * Y_F\n\n        return np.array([dh_dT, dh_dYF, dh_dYO])\n\n    for case in test_cases:\n        # Extract parameters for the current case\n        params = case[\"params\"]\n        A, E, R_const = params[\"A\"], params[\"E\"], params[\"R\"]\n        x_f = case[\"x_f\"]\n        P_f = case[\"P_f\"]\n        y_meas = case[\"y_meas\"]\n        R_meas = case[\"R_meas\"]\n\n        # 1. Linearize observation model at the forecast mean x_f\n        # Predicted measurement h(x_f)\n        h_xf = h_operator(x_f, A, E, R_const)\n        \n        # Jacobian H at x_f. H is a 1x3 row vector (1D numpy array).\n        H = H_jacobian(x_f, A, E, R_const)\n\n        # 2. Compute Innovation\n        innovation = y_meas - h_xf\n\n        # 3. Compute Innovation Covariance (scalar)\n        # S = H @ P_f @ H^T + R_meas\n        # In numpy, for 1D H, H @ P_f @ H is equivalent to H P H^T\n        S = H @ P_f @ H + R_meas\n        \n        # 4. Compute Kalman Gain (3x1 vector)\n        # K = P_f @ H^T / S\n        K = (P_f @ H) / S\n        \n        # 5. Compute Analysis Mean (updated state)\n        # x_a = x_f + K * innovation\n        x_a = x_f + K * innovation\n\n        all_results.extend(x_a)\n\n    # Format the final output as a comma-separated list of floats rounded to 6 decimal places.\n    formatted_results = [f\"{val:.6f}\" for val in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While the EKF is powerful, its linearization can introduce significant errors for the highly nonlinear dynamics typical of combustion, such as ignition and extinction. The Unscented Kalman Filter (UKF) offers a more robust, derivative-free alternative by propagating a small set of deterministically chosen \"sigma points\" through the true nonlinear model. This practice provides hands-on experience implementing the UKF prediction step for a simplified combustion model, demonstrating how to more accurately capture the evolution of the state's mean and covariance in a strongly nonlinear system. ",
            "id": "4016267",
            "problem": "Consider a discrete-time Bayesian filtering framework for a chemically reacting gas where the state vector is two-dimensional, $x = [T, Y_{\\mathrm{OH}}]^\\top$, with $T$ the temperature in Kelvin and $Y_{\\mathrm{OH}}$ the hydroxyl radical mass fraction (dimensionless). The state evolution and measurement models are given by\n$$\nx_{k+1} = f(x_k) + w_k,\\quad z_{k+1} = H(x_{k+1}) + v_{k+1},\n$$\nwhere $w_k \\sim \\mathcal{N}(0, Q)$ and $v_{k+1} \\sim \\mathcal{N}(0, R)$ are zero-mean Gaussian noises with covariance matrices $Q$ and $R$, respectively. The deterministic map $f$ is defined using a simplified Arrhenius-type source term for heat release and radical consumption, together with linear cooling:\n$$\n\\begin{aligned}\nT_{k+1} = T_k + \\Delta t\\left(\\frac{\\Delta h}{\\rho c_p} A \\exp\\!\\left(-\\frac{E_a}{R_u T_k}\\right) \\max(Y_{\\mathrm{OH},k}, 0) - k_c (T_k - T_{\\mathrm{env}}) \\right), \\\\\nY_{\\mathrm{OH},k+1} = \\max\\!\\Bigg(Y_{\\mathrm{OH},k} + \\Delta t\\left(- A_Y \\exp\\!\\left(-\\frac{E_a}{R_u T_k}\\right) \\max(Y_{\\mathrm{OH},k}, 0)\\right),\\, 0\\Bigg),\n\\end{aligned}\n$$\nwhere $\\Delta t$ is the time step (in seconds), $\\Delta h/(\\rho c_p)$ is an effective temperature rise coefficient (in Kelvin), $A$ and $A_Y$ are pre-exponential factors (in inverse seconds), $E_a$ is the activation energy (in Joules per mole), $R_u$ is the universal gas constant (in Joules per mole-Kelvin), $k_c$ is a linear cooling coefficient (in inverse seconds), and $T_{\\mathrm{env}}$ is the environment temperature (in Kelvin). The $H$ map models the measurement of the temperature and an optical signal that scales with the product of temperature and the square root of hydroxyl mass fraction:\n$$\nH(x) = \\begin{bmatrix}\nT \\\\\ns_0\\, T\\, \\sqrt{\\max(Y_{\\mathrm{OH}}, 0)}\n\\end{bmatrix},\n$$\nwhere $s_0$ is an optical scaling factor (dimensionless). This setting represents a combustion data assimilation scenario, where the hydroxyl radical mass fraction is used as a proxy for reaction progress near the flame front and is constrained to be nonnegative in the physics-based terms.\n\nAssume that the current state estimate is Gaussian $x_k \\sim \\mathcal{N}(\\mu_x, P_x)$ with mean $\\mu_x \\in \\mathbb{R}^2$ and covariance $P_x \\in \\mathbb{R}^{2\\times 2}$, and the noises $Q \\in \\mathbb{R}^{2\\times 2}$, $R \\in \\mathbb{R}^{2\\times 2}$ are given and positive semidefinite. Use the Unscented Kalman Filter (UKF) sigma point construction to approximate the predicted state distribution $x_{k+1}$ and the predicted measurement distribution $z_{k+1}$. Specifically:\n\n1. Construct $2n+1$ sigma points for $n=2$ using the scaling parameters $\\alpha$, $\\beta$, and $\\kappa$:\n   - Compute $ \\lambda = \\alpha^2 (n + \\kappa) - n $ and $ \\gamma = \\sqrt{n + \\lambda} $.\n   - Compute a matrix square root of $P_x$, for example via the Cholesky factorization $P_x = LL^\\top$ with $L \\in \\mathbb{R}^{2\\times 2}$ lower triangular.\n   - Define sigma points\n     $$\n     \\chi^{(0)} = \\mu_x, \\quad \\chi^{(i)} = \\mu_x + \\gamma\\, L_{:,i}, \\quad \\chi^{(i+n)} = \\mu_x - \\gamma\\, L_{:,i} \\quad \\text{for } i=1,\\dots,n.\n     $$\n   - Define weights\n     $$\n     W_0^{(m)} = \\frac{\\lambda}{n + \\lambda}, \\quad W_0^{(c)} = \\frac{\\lambda}{n + \\lambda} + (1 - \\alpha^2 + \\beta), \\quad W_i^{(m)} = W_i^{(c)} = \\frac{1}{2(n + \\lambda)} \\text{ for } i=1,\\dots,2n.\n     $$\n2. Propagate all sigma points through $f$ to obtain transformed points $\\mathcal{Y}^{(i)} = f(\\chi^{(i)})$ for $i=0,\\dots,2n$, and compute the predicted state mean and covariance\n   $$\n   \\mu_{x^+} = \\sum_{i=0}^{2n} W_i^{(m)} \\mathcal{Y}^{(i)}, \\quad P_{x^+} = \\sum_{i=0}^{2n} W_i^{(c)} \\left(\\mathcal{Y}^{(i)} - \\mu_{x^+}\\right)\\left(\\mathcal{Y}^{(i)} - \\mu_{x^+}\\right)^\\top + Q.\n   $$\n3. Propagate the predicted-state sigma points $\\mathcal{Y}^{(i)}$ through $H$ to obtain $\\mathcal{Z}^{(i)} = H(\\mathcal{Y}^{(i)})$, and compute the predicted measurement mean and covariance\n   $$\n   \\mu_{z} = \\sum_{i=0}^{2n} W_i^{(m)} \\mathcal{Z}^{(i)}, \\quad S_{z} = \\sum_{i=0}^{2n} W_i^{(c)} \\left(\\mathcal{Z}^{(i)} - \\mu_{z}\\right)\\left(\\mathcal{Z}^{(i)} - \\mu_{z}\\right)^\\top + R.\n   $$\nEnforce physical realism by treating any sigma point with negative $Y_{\\mathrm{OH}}$ as $0$ within $f$ and $H$.\n\nUse the following constants unless otherwise specified within each test case: $R_u = 8.314$ (J/mol-K), $E_a = 1.2\\times 10^5$ (J/mol), $A = 1.0\\times 10^6$ (s$^{-1}$), $A_Y = 1.0\\times 10^6$ (s$^{-1}$), $\\Delta h/(\\rho c_p) = 3.0\\times 10^4$ (K), $k_c = 50.0$ (s$^{-1}$), $T_{\\mathrm{env}} = 300.0$ (K), $s_0 = 200.0$ (dimensionless). Angles do not appear; temperatures must be expressed in Kelvin and $Y_{\\mathrm{OH}}$ is dimensionless.\n\nTest Suite:\n- Case 1 (typical flame conditions):\n  - $\\mu_x = [1700.0,\\, 1.0\\times 10^{-4}]^\\top$ (Kelvin, dimensionless)\n  - $P_x = \\begin{bmatrix} 25.0^2  0 \\\\ 0  (1.0\\times 10^{-5})^2 \\end{bmatrix}$\n  - $Q = \\begin{bmatrix} 0.05^2  0 \\\\ 0  (1.0\\times 10^{-6})^2 \\end{bmatrix}$\n  - $R = \\begin{bmatrix} 0.5^2  0 \\\\ 0  10.0^2 \\end{bmatrix}$\n  - $\\alpha=0.75$, $\\beta=2.0$, $\\kappa=0.0$, $\\Delta t = 1.0\\times 10^{-4}$ s\n- Case 2 (near-ambient, radical-depleted boundary):\n  - $\\mu_x = [300.0,\\, 1.0\\times 10^{-8}]^\\top$\n  - $P_x = \\begin{bmatrix} 5.0^2  0 \\\\ 0  (5.0\\times 10^{-9})^2 \\end{bmatrix}$\n  - $Q = \\begin{bmatrix} 0.02^2  0 \\\\ 0  (1.0\\times 10^{-9})^2 \\end{bmatrix}$\n  - $R = \\begin{bmatrix} 1.0^2  0 \\\\ 0  0.5^2 \\end{bmatrix}$\n  - $\\alpha=0.6$, $\\beta=2.0$, $\\kappa=-1.0$, $\\Delta t = 1.0\\times 10^{-4}$ s\n- Case 3 (hot and reactive, small $\\alpha$ edge behavior):\n  - $\\mu_x = [2200.0,\\, 5.0\\times 10^{-4}]^\\top$\n  - $P_x = \\begin{bmatrix} 10.0^2  0 \\\\ 0  (5.0\\times 10^{-5})^2 \\end{bmatrix}$\n  - $Q = \\begin{bmatrix} 0.1^2  0 \\\\ 0  (2.0\\times 10^{-6})^2 \\end{bmatrix}$\n  - $R = \\begin{bmatrix} 0.2^2  0 \\\\ 0  15.0^2 \\end{bmatrix}$\n  - $\\alpha=0.001$, $\\beta=2.0$, $\\kappa=3.0$, $\\Delta t = 5.0\\times 10^{-5}$ s\n\nYour task is to write a complete, runnable program that, for each test case, constructs the Unscented Kalman Filter sigma points from $(\\mu_x, P_x)$, propagates them through $f$ and $H$, and computes the corresponding predicted state mean $\\mu_{x^+}$ and covariance $P_{x^+}$, and predicted measurement mean $\\mu_z$ and covariance $S_z$, using the weights above and including $Q$ and $R$ as additive covariances.\n\nFinal Output Format: Your program should produce a single line of output containing the results as a list of lists, one sub-list per test case, with no spaces. Each sub-list must contain exactly ten floating-point numbers in this order:\n$$\n[\\mu_{x^+}[0],\\, \\mu_{x^+}[1],\\, P_{x^+}[0,0],\\, P_{x^+}[0,1],\\, P_{x^+}[1,1],\\, \\mu_{z}[0],\\, \\mu_{z}[1],\\, S_{z}[0,0],\\, S_{z}[0,1],\\, S_{z}[1,1]].\n$$\nFor example, the output format should be like\n$$\n[[\\text{c1\\_muT},\\text{c1\\_muY},\\text{c1\\_P00},\\text{c1\\_P01},\\text{c1\\_P11},\\text{c1\\_z0},\\text{c1\\_z1},\\text{c1\\_S00},\\text{c1\\_S01},\\text{c1\\_S11}],\\ [\\dots],\\ [\\dots]]\n$$\nTemperatures in the output must be in Kelvin and $Y_{\\mathrm{OH}}$ is dimensionless. Angles are not used. Express all numerical values as raw decimals (no percentage signs).",
            "solution": "The underlying principles begin from the discrete-time Bayesian filtering framework for a hidden Markov model. The state distribution at time $k$ is summarized by a Gaussian $\\mathcal{N}(\\mu_x, P_x)$. The nonlinear propagation through $f$ and measurement mapping through $H$ are governed by physical models based on chemical kinetics and heat transfer. For highly nonlinear dynamics and measurements, the Extended Kalman Filter (EKF) uses local linearization via Jacobians, but it suffers from accuracy issues due to truncation of higher-order terms. The Unscented Kalman Filter (UKF) instead employs deterministic sampling known as the unscented transform that captures the mean and covariance up to the third-order Taylor expansion for Gaussian distributions without explicit derivatives.\n\nFundamental base: the discrete-time Chapman–Kolmogorov equation propagates the prior density via\n$$\np(x_{k+1}) = \\int p(x_{k+1}\\mid x_k) p(x_k)\\, dx_k,\n$$\nand the predicted measurement distribution is\n$$\np(z_{k+1}) = \\int p(z_{k+1}\\mid x_{k+1}) p(x_{k+1})\\, dx_{k+1}.\n$$\nWhen $x_k \\sim \\mathcal{N}(\\mu_x, P_x)$ and the process and measurement noises are additive Gaussians $w_k \\sim \\mathcal{N}(0, Q)$ and $v_{k+1}\\sim \\mathcal{N}(0, R)$, the UKF approximates these integrals using a set of deterministically chosen sigma points that capture the second-order statistics of the Gaussian measure, propagated through the nonlinear maps $f$ and $H$.\n\nUnscented transform construction: For an $n$-dimensional state, calculate a scaling parameter\n$$\n\\lambda = \\alpha^2 (n + \\kappa) - n,\n$$\nwith $\\alpha\\in(0,1]$ controlling the spread of the sigma points around the mean, $\\beta$ encoding prior knowledge of the distribution ($\\beta=2$ is optimal for Gaussians), and $\\kappa$ providing additional degrees of freedom (often $\\kappa=0$ or $\\kappa=3-n$). Define\n$$\n\\gamma = \\sqrt{n + \\lambda}.\n$$\nA matrix square root of $P_x$ is needed; with positive definite $P_x$, the Cholesky factorization provides $P_x = LL^\\top$. Construct $2n+1$ sigma points:\n$$\n\\chi^{(0)} = \\mu_x,\\quad \\chi^{(i)} = \\mu_x + \\gamma\\, L_{:,i},\\quad \\chi^{(i+n)} = \\mu_x - \\gamma\\, L_{:,i},\\quad i=1,\\dots,n.\n$$\nDefine weights for mean and covariance as\n$$\nW_0^{(m)} = \\frac{\\lambda}{n + \\lambda}, \\quad W_0^{(c)} = \\frac{\\lambda}{n + \\lambda} + (1 - \\alpha^2 + \\beta), \\quad W_i^{(m)} = W_i^{(c)} = \\frac{1}{2(n + \\lambda)},\\quad i=1,\\dots,2n.\n$$\nThese weights ensure that the sigma points have the correct first two moments for the Gaussian distribution.\n\nPhysical state and measurement maps: The combustion-inspired nonlinear model $f$ uses an Arrhenius-type reaction rate to model heat release and radical consumption. The temperature update is\n$$\nT_{k+1} = T_k + \\Delta t\\left(\\frac{\\Delta h}{\\rho c_p} A \\exp\\!\\left(-\\frac{E_a}{R_u T_k}\\right) \\max(Y_{\\mathrm{OH},k},0) - k_c (T_k - T_{\\mathrm{env}}) \\right),\n$$\nreflecting heating from reaction progress (scaled by the effective temperature rise coefficient $\\Delta h/(\\rho c_p)$) and cooling linearly to the ambient temperature $T_{\\mathrm{env}}$. The hydroxyl radical mass fraction decreases according to\n$$\nY_{\\mathrm{OH},k+1} = \\max\\!\\left(Y_{\\mathrm{OH},k} + \\Delta t\\left(- A_Y \\exp\\!\\left(-\\frac{E_a}{R_u T_k}\\right) \\max(Y_{\\mathrm{OH},k}, 0)\\right), 0\\right),\n$$\nwhere the max ensures nonnegativity, as negative mass fractions are nonphysical.\n\nThe measurement model $H$ reflects direct temperature sensing and an optical signal proportional to $T \\sqrt{Y_{\\mathrm{OH}}}$,\n$$\nH(x) = \\begin{bmatrix}\nT \\\\\ns_0\\, T\\, \\sqrt{\\max(Y_{\\mathrm{OH}}, 0)}\n\\end{bmatrix},\n$$\nwith $\\sqrt{\\cdot}$ applied to $\\max(Y_{\\mathrm{OH}},0)$ to avoid complex values when sigma points produce negative $Y_{\\mathrm{OH}}$ due to covariance spread.\n\nUKF prediction steps:\n1. Generate sigma points $\\{\\chi^{(i)}\\}_{i=0}^{2n}$ and their weights $W_i^{(m)}, W_i^{(c)}$ from $(\\mu_x, P_x)$.\n2. Propagate sigma points through $f$ to obtain $\\mathcal{Y}^{(i)} = f(\\chi^{(i)})$.\n3. Compute predicted state mean and covariance:\n$$\n\\mu_{x^+} = \\sum_{i=0}^{2n} W_i^{(m)} \\mathcal{Y}^{(i)},\\quad\nP_{x^+} = \\sum_{i=0}^{2n} W_i^{(c)} \\left(\\mathcal{Y}^{(i)} - \\mu_{x^+}\\right)\\left(\\mathcal{Y}^{(i)} - \\mu_{x^+}\\right)^\\top + Q.\n$$\n4. Propagate predicted-state sigma points through $H$: $\\mathcal{Z}^{(i)} = H(\\mathcal{Y}^{(i)})$.\n5. Compute predicted measurement mean and covariance:\n$$\n\\mu_z = \\sum_{i=0}^{2n} W_i^{(m)} \\mathcal{Z}^{(i)},\\quad\nS_z = \\sum_{i=0}^{2n} W_i^{(c)} \\left(\\mathcal{Z}^{(i)} - \\mu_{z}\\right)\\left(\\mathcal{Z}^{(i)} - \\mu_{z}\\right)^\\top + R.\n$$\n\nAlgorithmic considerations:\n- The Cholesky factorization requires $P_x$ to be symmetric positive definite; the provided test cases satisfy this condition.\n- The nonlinearity $\\exp(-E_a/(R_u T))$ is strongly temperature dependent; at low $T$ it suppresses reaction, and at high $T$ it accelerates heat release and radical consumption, which is consistent with chemical kinetics.\n- Clipping $Y_{\\mathrm{OH}}$ inside $f$ and $H$ to nonnegative values preserves physical realism of heat release and signal generation and prevents undefined operations such as square roots of negative values.\n- Additive covariances $Q$ and $R$ are included after the unscented covariance computations, reflecting process and measurement noise.\n\nImplementation plan in the program:\n- Define $f$ and $H$ as functions parameterized by the physical constants and $\\Delta t$.\n- Implement sigma point generation with weights using $(\\alpha, \\beta, \\kappa)$.\n- Implement unscented transform steps for $f$ and $H$ as specified.\n- For each test case, compute and collect the outputs\n$$\n[\\mu_{x^+}[0],\\, \\mu_{x^+}[1],\\, P_{x^+}[0,0],\\, P_{x^+}[0,1],\\, P_{x^+}[1,1],\\, \\mu_{z}[0],\\, \\mu_{z}[1],\\, S_{z}[0,0],\\, S_{z}[0,1],\\, S_{z}[1,1]],\n$$\nwith temperatures in Kelvin and $Y_{\\mathrm{OH}}$ dimensionless.\n- Print the aggregated list of lists for the three test cases as a single line without spaces, complying with the specified output format.\n\nThis approach integrates physical combustion modeling with data assimilation via the UKF, capturing nonlinear effects more accurately than linearization and providing statistically consistent mean and covariance predictions for both state and measurements.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef cholesky_lower(P):\n    # Ensure symmetry before Cholesky to avoid numerical issues\n    P_sym = 0.5 * (P + P.T)\n    return np.linalg.cholesky(P_sym)\n\ndef generate_sigma_points(mu, P, alpha, beta, kappa):\n    n = mu.shape[0]\n    lambda_ = alpha**2 * (n + kappa) - n\n    gamma = np.sqrt(n + lambda_)\n    L = cholesky_lower(P)\n    sigma_points = np.zeros((2 * n + 1, n))\n    sigma_points[0] = mu\n    for i in range(n):\n        sigma_points[1 + i] = mu + gamma * L[:, i]\n        sigma_points[1 + n + i] = mu - gamma * L[:, i]\n    Wm = np.full(2 * n + 1, 1.0 / (2.0 * (n + lambda_)))\n    Wc = np.copy(Wm)\n    Wm[0] = lambda_ / (n + lambda_)\n    Wc[0] = Wm[0] + (1.0 - alpha**2 + beta)\n    return sigma_points, Wm, Wc\n\ndef f_map(x, params):\n    # Unpack state\n    T, Y = float(x[0]), float(x[1])\n    # Unpack parameters\n    dt = params[\"dt\"]\n    Ru = params[\"Ru\"]\n    Ea = params[\"Ea\"]\n    A = params[\"A\"]\n    AY = params[\"AY\"]\n    dh_over_rho_cp = params[\"dh_over_rho_cp\"]\n    kc = params[\"kc\"]\n    Tenv = params[\"Tenv\"]\n    # Physical clipping for Y\n    Y_nonneg = max(Y, 0.0)\n    # Arrhenius term\n    # Prevent division by zero or negative T; T should be > 0 in Kelvin\n    T_eff = max(T, 1e-6)\n    arr = np.exp(-Ea / (Ru * T_eff))\n    # Temperature update\n    dTdt = dh_over_rho_cp * A * arr * Y_nonneg - kc * (T - Tenv)\n    T_next = T + dt * dTdt\n    # OH mass fraction update with nonnegativity\n    dYdt = -AY * arr * Y_nonneg\n    Y_next = Y + dt * dYdt\n    if Y_next  0.0:\n        Y_next = 0.0\n    return np.array([T_next, Y_next])\n\ndef H_map(x, params):\n    T, Y = float(x[0]), float(x[1])\n    s0 = params[\"s0\"]\n    Y_nonneg = max(Y, 0.0)\n    signal = s0 * T * np.sqrt(Y_nonneg)\n    return np.array([T, signal])\n\ndef unscented_transform(mu, P, Q, f, f_params, alpha, beta, kappa):\n    # Generate sigma points\n    sigma_points, Wm, Wc = generate_sigma_points(mu, P, alpha, beta, kappa)\n    # Propagate through f\n    n_sigma = sigma_points.shape[0]\n    y_sigma = np.zeros_like(sigma_points)\n    for i in range(n_sigma):\n        y_sigma[i] = f(sigma_points[i], f_params)\n    # Predicted mean\n    mu_pred = np.sum(Wm[:, None] * y_sigma, axis=0)\n    # Predicted covariance\n    diff = y_sigma - mu_pred\n    P_pred = np.zeros_like(P)\n    for i in range(n_sigma):\n        d = diff[i][:, None]\n        P_pred += Wc[i] * (d @ d.T)\n    P_pred += Q\n    return mu_pred, P_pred, y_sigma, Wm, Wc\n\ndef measurement_transform(y_sigma, Wm, Wc, R, H, h_params):\n    n_sigma = y_sigma.shape[0]\n    z_sigma = np.zeros((n_sigma, 2))\n    for i in range(n_sigma):\n        z_sigma[i] = H(y_sigma[i], h_params)\n    # Predicted measurement mean\n    z_mean = np.sum(Wm[:, None] * z_sigma, axis=0)\n    # Predicted measurement covariance\n    zdiff = z_sigma - z_mean\n    S = np.zeros((2, 2))\n    for i in range(n_sigma):\n        d = zdiff[i][:, None]\n        S += Wc[i] * (d @ d.T)\n    S += R\n    return z_mean, S\n\ndef solve():\n    # Define test cases parameters\n    test_cases = []\n\n    # Constants common baseline\n    base_params = {\n        \"Ru\": 8.314,              # J/mol-K\n        \"Ea\": 1.2e5,              # J/mol\n        \"A\": 1.0e6,               # 1/s\n        \"AY\": 1.0e6,              # 1/s\n        \"dh_over_rho_cp\": 3.0e4,  # K\n        \"kc\": 50.0,               # 1/s\n        \"Tenv\": 300.0,            # K\n        \"s0\": 200.0               # dimensionless\n    }\n\n    # Case 1\n    case1 = {\n        \"mu\": np.array([1700.0, 1.0e-4]),\n        \"P\": np.array([[25.0**2, 0.0],\n                       [0.0, (1.0e-5)**2]]),\n        \"Q\": np.array([[0.05**2, 0.0],\n                       [0.0, (1.0e-6)**2]]),\n        \"R\": np.array([[0.5**2, 0.0],\n                       [0.0, 10.0**2]]),\n        \"alpha\": 0.75,\n        \"beta\": 2.0,\n        \"kappa\": 0.0,\n        \"dt\": 1.0e-4\n    }\n    # Case 2\n    case2 = {\n        \"mu\": np.array([300.0, 1.0e-8]),\n        \"P\": np.array([[5.0**2, 0.0],\n                       [0.0, (5.0e-9)**2]]),\n        \"Q\": np.array([[0.02**2, 0.0],\n                       [0.0, (1.0e-9)**2]]),\n        \"R\": np.array([[1.0**2, 0.0],\n                       [0.0, 0.5**2]]),\n        \"alpha\": 0.6,\n        \"beta\": 2.0,\n        \"kappa\": -1.0,\n        \"dt\": 1.0e-4\n    }\n    # Case 3\n    case3 = {\n        \"mu\": np.array([2200.0, 5.0e-4]),\n        \"P\": np.array([[10.0**2, 0.0],\n                       [0.0, (5.0e-5)**2]]),\n        \"Q\": np.array([[0.1**2, 0.0],\n                       [0.0, (2.0e-6)**2]]),\n        \"R\": np.array([[0.2**2, 0.0],\n                       [0.0, 15.0**2]]),\n        \"alpha\": 0.001,\n        \"beta\": 2.0,\n        \"kappa\": 3.0,\n        \"dt\": 5.0e-5\n    }\n\n    test_cases.append(case1)\n    test_cases.append(case2)\n    test_cases.append(case3)\n\n    results = []\n    for case in test_cases:\n        # Merge case dt into params\n        params = dict(base_params)\n        params[\"dt\"] = case[\"dt\"]\n\n        mu = case[\"mu\"]\n        P = case[\"P\"]\n        Q = case[\"Q\"]\n        R = case[\"R\"]\n        alpha = case[\"alpha\"]\n        beta = case[\"beta\"]\n        kappa = case[\"kappa\"]\n\n        # UKF prediction\n        mu_pred, P_pred, y_sigma, Wm, Wc = unscented_transform(mu, P, Q, f_map, params, alpha, beta, kappa)\n        # Measurement prediction\n        z_mean, S = measurement_transform(y_sigma, Wm, Wc, R, H_map, params)\n\n        # Collect results: [mu_T, mu_Y, P00, P01, P11, z0, z1, S00, S01, S11]\n        res = [\n            float(mu_pred[0]), float(mu_pred[1]),\n            float(P_pred[0, 0]), float(P_pred[0, 1]), float(P_pred[1, 1]),\n            float(z_mean[0]), float(z_mean[1]),\n            float(S[0, 0]), float(S[0, 1]), float(S[1, 1])\n        ]\n        results.append(res)\n\n    # Format output as a single line list of lists with no spaces\n    def format_nested(list_of_lists):\n        def fmt_num(x):\n            # Use general format for compactness while preserving precision\n            return f\"{x:.12g}\"\n        inner = []\n        for sub in list_of_lists:\n            inner.append(\"[\" + \",\".join(fmt_num(x) for x in sub) + \"]\")\n        return \"[\" + \",\".join(inner) + \"]\"\n\n    print(format_nested(results))\n\nsolve()\n```"
        },
        {
            "introduction": "Kalman-based filters like the EKF and UKF operate on the fundamental assumption that the state probability distribution remains approximately Gaussian. This assumption often breaks down in combustion, especially when dealing with discrete-event data like photon counts or multi-modal states (e.g., ignited vs. unignited). This exercise introduces the Particle Filter, a fundamentally different approach that represents the probability distribution with a set of weighted samples, allowing it to handle arbitrary non-Gaussian statistics. You will implement a bootstrap particle filter with a realistic Poisson observation model for chemiluminescence, learning how to update particle weights and diagnose filter health using the Effective Sample Size (ESS). ",
            "id": "4016190",
            "problem": "Consider a one-step update of a Sequential Importance Resampling particle method for Data Assimilation (DA) applied to computational combustion with a nonlinear chemiluminescence observation. Let the hidden combustion state be represented by a collection of particles $\\{x_n^{(j)}\\}_{j=1}^N$, where each $x_n^{(j)}$ contains the temperature $T_n^{(j)}$ in Kelvin and a reaction progress variable $q_n^{(j)}$ (dimensionless). The observation at time index $n$ is the photon count $y_n$ measured by a photomultiplier over an integration time $\\Delta t$ in seconds. Assume the expected photon count from chemiluminescence for a given state is modeled by an Arrhenius-type intensity:\n$$\n\\lambda(x) = \\kappa\\, q^\\alpha \\exp\\!\\left(-\\frac{E_a}{R\\,T}\\right),\n$$\nwhere $\\kappa$ has units of per second, $q$ is dimensionless, $E_a$ is the activation energy in Joules per mole, $R$ is the universal gas constant in Joules per mole per Kelvin, and $T$ is temperature in Kelvin. The mean photon count over the integration interval is\n$$\n\\mu(x) = \\Delta t \\,\\lambda(x).\n$$\nAssume a non-Gaussian observation model in which the observed count $y_n$ is drawn from a Poisson distribution with mean $\\mu(x_n)$, so the likelihood is\n$$\np(y_n \\mid x_n) = \\mathrm{Poisson}\\big(\\mu(x_n)\\big).\n$$\nStarting from Bayes' rule and importance sampling with a proposal equal to the prior Markov transition (the bootstrap choice), derive the particle weight update and implement it in a program that, given a set of previous normalized weights $\\{w_{n-1}^{(j)}\\}$ and current particles $\\{x_n^{(j)}\\}$, computes the updated normalized weights $\\{w_n^{(j)}\\}$ and the Effective Sample Size (ESS). The Effective Sample Size (ESS) is defined by\n$$\nN_{\\mathrm{eff}} = \\frac{1}{\\sum_{j=1}^N \\left(w_n^{(j)}\\right)^2}.\n$$\nThen, analyze degeneracy by declaring a degeneracy flag that is true if $N_{\\mathrm{eff}}  \\beta N$, where $\\beta$ is a fixed fraction and $N$ is the number of particles.\n\nScientific and computational requirements:\n- Use the physically motivated Arrhenius form with constants $R = 8.314$ Joules per mole per Kelvin, $E_a$ and $\\kappa$ as given per test case, and an exponent $\\alpha$.\n- The photon count $y_n$ is an integer count (dimensionless). Temperature $T$ must be in Kelvin, and time $\\Delta t$ must be in seconds. Any expression involving $T$ and $\\Delta t$ must use these units.\n- Compute the likelihood using the Poisson probability mass function. To ensure numerical stability, perform the computation of the likelihood in the logarithmic domain, and then normalize the weights. Use the gamma function identity $\\log(y!) = \\log\\Gamma(y+1)$ for integer $y$.\n\nYour program must implement the weight update and ESS computation for the following test suite, each test defined by the tuple of parameters $(\\{T_n^{(j)}\\}, \\{q_n^{(j)}\\}, \\{w_{n-1}^{(j)}\\}, y_n, \\Delta t, \\kappa, E_a, \\alpha, \\beta)$:\n\n- Test $1$ (happy path, moderate counts):\n    - $N = 5$\n    - $\\{T_n^{(j)}\\} = [1950, 2000, 2050, 1900, 2100]$ Kelvin\n    - $\\{q_n^{(j)}\\} = [0.85, 0.92, 0.88, 0.80, 0.95]$ (dimensionless)\n    - $\\{w_{n-1}^{(j)}\\} = [0.2, 0.2, 0.2, 0.2, 0.2]$ (normalized)\n    - $y_n = 200$ (counts)\n    - $\\Delta t = 5.0\\times 10^{-4}$ seconds\n    - $\\kappa = 3.0\\times 10^{9}$ per second\n    - $E_a = 1.5\\times 10^{5}$ Joules per mole\n    - $\\alpha = 1.0$\n    - $\\beta = 0.5$\n- Test $2$ (skewed prior weights, higher counts):\n    - $N = 8$\n    - $\\{T_n^{(j)}\\} = [1800, 1850, 1900, 1950, 2000, 2050, 2100, 2150]$ Kelvin\n    - $\\{q_n^{(j)}\\} = [0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00, 0.98]$\n    - $\\{w_{n-1}^{(j)}\\} = [0.05, 0.05, 0.10, 0.10, 0.20, 0.20, 0.15, 0.15]$ (normalized)\n    - $y_n = 320$ (counts)\n    - $\\Delta t = 5.0\\times 10^{-4}$ seconds\n    - $\\kappa = 3.0\\times 10^{9}$ per second\n    - $E_a = 1.5\\times 10^{5}$ Joules per mole\n    - $\\alpha = 1.0$\n    - $\\beta = 0.5$\n- Test $3$ (edge case, zero counts):\n    - $N = 6$\n    - $\\{T_n^{(j)}\\} = [1600, 1650, 1700, 1750, 1800, 1850]$ Kelvin\n    - $\\{q_n^{(j)}\\} = [0.40, 0.45, 0.50, 0.55, 0.60, 0.65]$\n    - $\\{w_{n-1}^{(j)}\\} = [1/6, 1/6, 1/6, 1/6, 1/6, 1/6]$ (normalized)\n    - $y_n = 0$ (counts)\n    - $\\Delta t = 5.0\\times 10^{-4}$ seconds\n    - $\\kappa = 3.0\\times 10^{9}$ per second\n    - $E_a = 1.5\\times 10^{5}$ Joules per mole\n    - $\\alpha = 1.0$\n    - $\\beta = 0.5$\n- Test $4$ (stronger nonlinearity via larger activation energy):\n    - $N = 7$\n    - $\\{T_n^{(j)}\\} = [1900, 1950, 2000, 2050, 2100, 2150, 2200]$ Kelvin\n    - $\\{q_n^{(j)}\\} = [0.80, 0.82, 0.85, 0.88, 0.90, 0.92, 0.95]$\n    - $\\{w_{n-1}^{(j)}\\} = [1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 1/7]$ (normalized)\n    - $y_n = 20$ (counts)\n    - $\\Delta t = 5.0\\times 10^{-4}$ seconds\n    - $\\kappa = 3.0\\times 10^{9}$ per second\n    - $E_a = 2.0\\times 10^{5}$ Joules per mole\n    - $\\alpha = 1.0$\n    - $\\beta = 0.5$\n\nProgram output requirements:\n- For each test case, compute the updated normalized weights $\\{w_n^{(j)}\\}$ and $N_{\\mathrm{eff}}$.\n- Declare degeneracy if $N_{\\mathrm{eff}}  \\beta N$.\n- Round $N_{\\mathrm{eff}}$ to three decimal places. Round each normalized weight $w_n^{(j)}$ to six decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a list of the form $[N_{\\mathrm{eff}}, \\text{degeneracy\\_flag}, \\text{weights}]$ where $N_{\\mathrm{eff}}$ is a float, $\\text{degeneracy\\_flag}$ is a boolean, and $\\text{weights}$ is the list of rounded floats. For the four tests, the final output must look like:\n$$\n\\text{[ [N\\_eff\\_1,deg\\_1,[w\\_1^{(1)},\\dots,w\\_1^{(N)}]], [N\\_eff\\_2,deg\\_2,[\\dots]], [N\\_eff\\_3,deg\\_3,[\\dots]], [N\\_eff\\_4,deg\\_4,[\\dots]] ]}\n$$\nThis single line is the only output.",
            "solution": "### Step 1: Extract Givens\n\nThe problem provides the following data, definitions, and models for a one-step update of a Sequential Importance Resampling particle filter.\n\n**State and Observation Models:**\n1.  **Hidden State:** The hidden state for each particle $j$ at time step $n$ is $x_n^{(j)} = (T_n^{(j)}, q_n^{(j)})$, where $T_n^{(j)}$ is temperature in Kelvin and $q_n^{(j)}$ is a dimensionless reaction progress variable.\n2.  **Observation:** The observation is a photon count $y_n$ (integer, dimensionless).\n3.  **Chemiluminescence Intensity Model:** The expected intensity $\\lambda(x)$ for a state $x=(T, q)$ is given by an Arrhenius-type expression:\n    $$ \\lambda(x) = \\kappa\\, q^\\alpha \\exp\\!\\left(-\\frac{E_a}{R\\,T}\\right) $$\n    where $\\kappa$ is a pre-exponential factor (units: s$^{-1}$), $\\alpha$ is an exponent, $E_a$ is the activation energy (J/mol), and $R$ is the universal gas constant.\n4.  **Mean Photon Count:** The mean count $\\mu(x)$ over an integration time $\\Delta t$ (seconds) is:\n    $$ \\mu(x) = \\Delta t \\,\\lambda(x) $$\n5.  **Observation Likelihood:** The observation $y_n$ is drawn from a Poisson distribution with mean $\\mu(x_n)$:\n    $$ p(y_n \\mid x_n) = \\mathrm{Poisson}\\big(y_n; \\mu(x_n)\\big) = \\frac{\\mu(x_n)^{y_n} e^{-\\mu(x_n)}}{y_n!} $$\n\n**Particle Filter Update Rules:**\n1.  **Inputs:** Previous normalized weights $\\{w_{n-1}^{(j)}\\}_{j=1}^N$ and current particle states $\\{x_n^{(j)}\\}_{j=1}^N$.\n2.  **Weight Update:** Based on Bayes' rule and importance sampling (bootstrap filter), the unnormalized weights $\\tilde{w}_n^{(j)}$ are updated as:\n    $$ \\tilde{w}_n^{(j)} \\propto w_{n-1}^{(j)} p(y_n | x_n^{(j)}) $$\n    The normalized weights $w_n^{(j)}$ are:\n    $$ w_n^{(j)} = \\frac{\\tilde{w}_n^{(j)}}{\\sum_{k=1}^N \\tilde{w}_n^{(k)}} $$\n3.  **Effective Sample Size (ESS):**\n    $$ N_{\\mathrm{eff}} = \\frac{1}{\\sum_{j=1}^N \\left(w_n^{(j)}\\right)^2} $$\n4.  **Degeneracy Condition:** Degeneracy is declared if $N_{\\mathrm{eff}}  \\beta N$, where $N$ is the number of particles and $\\beta$ is a given threshold.\n\n**Constants and Numerical Requirements:**\n-   Universal Gas Constant: $R = 8.314$ J/(mol·K).\n-   Numerical Stability: Calculations for the likelihood must be performed in the logarithmic domain.\n-   Log-Gamma Identity: $\\log(y!) = \\log\\Gamma(y+1)$, where $\\Gamma$ is the Gamma function.\n\n**Test Cases:**\nThe problem defines four test cases, each specified by a tuple $(\\{T_n^{(j)}\\}, \\{q_n^{(j)}\\}, \\{w_{n-1}^{(j)}\\}, y_n, \\Delta t, \\kappa, E_a, \\alpha, \\beta)$. The specific values are provided in the problem statement.\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded:** The problem is firmly rooted in established principles. The Arrhenius equation is a cornerstone of chemical kinetics. The Poisson distribution is the correct statistical model for counting discrete, independent events like photon arrivals (shot noise). The particle filter (specifically, the SIR variant) is a standard and widely used method for state estimation in nonlinear, non-Gaussian systems, which is the context of data assimilation. The problem is scientifically sound.\n-   **Well-Posed:** For each test case, all necessary inputs are provided. The mathematical formulas for the weight update, ESS, and degeneracy check are deterministic and lead to a unique solution. The problem is well-posed.\n-   **Objective:** The problem is defined with precise mathematical equations, constants, and data. The language is objective and free of ambiguity or subjective claims.\n-   **Completeness and Consistency:** All parameters ($\\kappa, E_a, \\alpha, \\Delta t, \\beta$), constants ($R$), state variables ($T, q$), prior weights ($w_{n-1}$), and the observation ($y_n$) are specified for each test. The units are explicitly defined (Kelvin, seconds, J/mol) and are physically consistent. The prior weights in each test case are verified to sum to $1$. The setup is complete and internally consistent.\n-   **No Other Flaws:** The problem does not exhibit any other flaws from the checklist such as being non-formalizable, unrealistic, ill-posed, trivial, or unverifiable.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the derivation and implementation of the solution.\n\n### Principle-Based Design and Solution Derivation\n\nThe core task is to implement the measurement update step of a bootstrap particle filter. This involves re-weighting a set of particles based on how well each particle's predicted state explains a given measurement.\n\n**1. Derivation of the Weight Update Rule**\n\nAccording to the principles of sequential importance sampling, the posterior probability of the state $x_n$ given all observations up to time $n$, $y_{1:n}$, is recursively updated. Using Bayes' theorem:\n$$\np(x_n | y_{1:n}) \\propto p(y_n | x_n) \\, p(x_n | y_{1:n-1})\n$$\nThe term $p(x_n | y_{1:n-1})$ is the prior probability of the state at time $n$, predicted from the state at time $n-1$. In a particle filter, this prior is represented by the propagated particles $\\{x_n^{(j)}\\}$ with their inherited weights $\\{w_{n-1}^{(j)}\\}$. The term $p(y_n | x_n)$ is the likelihood of observing $y_n$ given the state $x_n$.\n\nFor a bootstrap filter, the proposal distribution is chosen to be the state transition prior. The importance weights for the particles are updated by multiplying the previous weight by the likelihood of the new observation given the new particle state:\n$$\n\\tilde{w}_n^{(j)} = w_{n-1}^{(j)} \\, p(y_n | x_n^{(j)})\n$$\nThese unnormalized weights, $\\tilde{w}_n^{(j)}$, are then normalized to sum to unity:\n$$\nw_n^{(j)} = \\frac{\\tilde{w}_n^{(j)}}{\\sum_{k=1}^N \\tilde{w}_n^{(k)}}\n$$\n\n**2. Computation of the Likelihood in Logarithmic Domain**\n\nTo maintain numerical stability, especially when dealing with very small probabilities, we work with log-likelihoods. The likelihood is given by the Poisson probability mass function:\n$$\np(y_n | x_n^{(j)}) = \\frac{(\\mu_n^{(j)})^{y_n} e^{-\\mu_n^{(j)}}}{y_n!}\n$$\nwhere $\\mu_n^{(j)} = \\mu(x_n^{(j)})$. Taking the natural logarithm:\n$$\n\\log p(y_n | x_n^{(j)}) = y_n \\log(\\mu_n^{(j)}) - \\mu_n^{(j)} - \\log(y_n!)\n$$\nUsing the provided identity $\\log(y_n!) = \\log\\Gamma(y_n+1)$, which is implemented numerically by the log-gamma function (`gammaln` in SciPy), the log-likelihood $\\ell_n^{(j)}$ is:\n$$\n\\ell_n^{(j)} = y_n \\log(\\mu_n^{(j)}) - \\mu_n^{(j)} - \\mathrm{gammaln}(y_n+1)\n$$\nThe mean photon count $\\mu_n^{(j)}$ is calculated for each particle $j$ using its state $(T_n^{(j)}, q_n^{(j)})$ and the given parameters:\n$$\n\\mu_n^{(j)} = \\Delta t \\cdot \\kappa \\cdot (q_n^{(j)})^\\alpha \\exp\\left(-\\frac{E_a}{R T_n^{(j)}}\\right)\n$$\nAn edge case occurs if an observation $y_n=0$ is recorded. The log-likelihood formula simplifies to $\\ell_n^{(j)} = - \\mu_n^{(j)}$ since $\\log(0!) = 0$ and the term $y_n \\log(\\mu_n^{(j)})$ vanishes (as long as handled correctly for $\\mu_n^{(j)} \\to 0$).\n\n**3. Normalization using the Log-Sum-Exp Trick**\n\nThe update equation for the weights involves an exponential and a sum, which can lead to overflow or underflow. We use the log-sum-exp trick for robust normalization. First, we compute the unnormalized log-weights:\n$$\n\\log \\tilde{w}_n^{(j)} = \\log(w_{n-1}^{(j)}) + \\ell_n^{(j)}\n$$\nLet $L_{\\max} = \\max_k(\\log \\tilde{w}_n^{(k)})$. We can write the normalized weight as:\n$$\nw_n^{(j)} = \\frac{\\exp(\\log \\tilde{w}_n^{(j)})}{\\sum_{k=1}^N \\exp(\\log \\tilde{w}_n^{(k)})} = \\frac{\\exp(\\log \\tilde{w}_n^{(j)} - L_{\\max})}{\\sum_{k=1}^N \\exp(\\log \\tilde{w}_n^{(k)} - L_{\\max})}\n$$\nThis computation avoids overflow because the largest exponentiated term is $e^0=1$. It also mitigates underflow by scaling all terms up.\n\n**4. Calculation of ESS and Degeneracy Check**\n\nAfter obtaining the normalized weights $\\{w_n^{(j)}\\}$, the Effective Sample Size is computed directly from its definition:\n$$\nN_{\\mathrm{eff}} = \\frac{1}{\\sum_{j=1}^N (w_n^{(j)})^2}\n$$\n$N_{\\mathrm{eff}}$ provides a measure of weight degeneracy. A value close to the total number of particles $N$ indicates that weights are evenly distributed, while a value close to $1$ indicates that a single particle has a weight near unity, and all others are near zero.\n\nFinally, the degeneracy flag is set by comparing $N_{\\mathrm{eff}}$ against the threshold specified:\n$$\n\\text{degeneracy\\_flag} = (N_{\\mathrm{eff}}  \\beta N)\n$$\nThis systematic procedure forms the basis for the implementation.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import gammaln\nimport json\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It orchestrates the calculation for each test case and formats the final output.\n    \"\"\"\n    \n    # Universal Gas Constant in J/(mol·K)\n    R = 8.314\n\n    test_cases = [\n        # Test 1 (happy path, moderate counts)\n        (\n            [1950, 2000, 2050, 1900, 2100],  # T_n\n            [0.85, 0.92, 0.88, 0.80, 0.95],  # q_n\n            [0.2, 0.2, 0.2, 0.2, 0.2],  # w_n-1\n            200,  # y_n\n            5.0e-4,  # delta_t\n            3.0e9,  # kappa\n            1.5e5,  # E_a\n            1.0,  # alpha\n            0.5,  # beta\n        ),\n        # Test 2 (skewed prior weights, higher counts)\n        (\n            [1800, 1850, 1900, 1950, 2000, 2050, 2100, 2150],  # T_n\n            [0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00, 0.98],  # q_n\n            [0.05, 0.05, 0.10, 0.10, 0.20, 0.20, 0.15, 0.15], # w_n-1\n            320,  # y_n\n            5.0e-4,  # delta_t\n            3.0e9,  # kappa\n            1.5e5,  # E_a\n            1.0,  # alpha\n            0.5,  # beta\n        ),\n        # Test 3 (edge case, zero counts)\n        (\n            [1600, 1650, 1700, 1750, 1800, 1850],  # T_n\n            [0.40, 0.45, 0.50, 0.55, 0.60, 0.65],  # q_n\n            [1/6, 1/6, 1/6, 1/6, 1/6, 1/6],  # w_n-1\n            0,  # y_n\n            5.0e-4,  # delta_t\n            3.0e9,  # kappa\n            1.5e5,  # E_a\n            1.0,  # alpha\n            0.5,  # beta\n        ),\n        # Test 4 (stronger nonlinearity via larger activation energy)\n        (\n            [1900, 1950, 2000, 2050, 2100, 2150, 2200],  # T_n\n            [0.80, 0.82, 0.85, 0.88, 0.90, 0.92, 0.95],  # q_n\n            [1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 1/7],  # w_n-1\n            20,  # y_n\n            5.0e-4,  # delta_t\n            3.0e9,  # kappa\n            2.0e5,  # E_a\n            1.0,  # alpha\n            0.5,  # beta\n        ),\n    ]\n\n    results = []\n    for params in test_cases:\n        T_n, q_n, w_prev, y_n, delta_t, kappa, E_a, alpha, beta = params\n        \n        # Convert inputs to numpy arrays for vectorized operations\n        T_n = np.array(T_n)\n        q_n = np.array(q_n)\n        w_prev = np.array(w_prev)\n        N = len(T_n)\n\n        # 1. Calculate the mean photon count mu for each particle\n        # lambda(x) = kappa * q^alpha * exp(-Ea / (R * T))\n        # mu(x) = delta_t * lambda(x)\n        exponent = -E_a / (R * T_n)\n        mu = delta_t * kappa * (q_n**alpha) * np.exp(exponent)\n\n        # 2. Compute the log-likelihood for each particle\n        # log L = y * log(mu) - mu - log(y!)\n        # Handle the edge case of y_n = 0 separately for robustness\n        if y_n == 0:\n            # For y=0, log L simplifies to -mu, avoiding log(0) issues.\n            log_likelihood = -mu\n        else:\n            # For y > 0, use the full formula.\n            # Handle potential mu = 0 cases to avoid log(0) = -inf.\n            # If mu is 0, log L is -inf. np.log(0) correctly returns -inf.\n            # 0 * log(0) is nan, but our problem structure ensures mu > 0 if y > 0.\n            # Even if mu=0, the log-likelihood should be -inf, resulting in zero weight.\n            # A numerically safe way to compute y*log(mu):\n            log_mu = np.full_like(mu, -np.inf) # Initialize with -inf\n            non_zero_mu_mask = mu > 0\n            log_mu[non_zero_mu_mask] = np.log(mu[non_zero_mu_mask])\n\n            log_likelihood = y_n * log_mu - mu - gammaln(y_n + 1)\n        \n        # 3. Update weights in the log domain and normalize\n        # log(w_unnorm) = log(w_prev) + log_likelihood\n        # np.log handles w_prev=0 by returning -inf, which is correct.\n        log_w_unnorm = np.log(w_prev) + log_likelihood\n\n        # Use log-sum-exp trick for normalization\n        # Find max of finite log-weights to avoid issues with -inf\n        if np.all(np.isneginf(log_w_unnorm)):\n            # All weights are zero, which implies an inconsistency or extreme case.\n            # Assign uniform weights to prevent division by zero.\n            w_n = np.full(N, 1.0/N)\n        else:\n            max_log_w = np.max(log_w_unnorm[np.isfinite(log_w_unnorm)])\n            w_shifted = np.exp(log_w_unnorm - max_log_w)\n            w_n = w_shifted / np.sum(w_shifted)\n            # Ensure no NaNs, e.g., if all weights become zero\n            w_n = np.nan_to_num(w_n, nan=1.0/N)\n            w_n /= np.sum(w_n)\n\n        # 4. Calculate Effective Sample Size (ESS)\n        # N_eff = 1 / sum(w_n^2)\n        sum_sq_weights = np.sum(w_n**2)\n        N_eff = 1.0 / sum_sq_weights\n\n        # 5. Check for degeneracy\n        # is_degenerate is True if N_eff  beta * N\n        is_degenerate = N_eff  beta * N\n\n        # 6. Format results according to specifications\n        N_eff_rounded = round(N_eff, 3)\n        weights_rounded = [round(w, 6) for w in w_n]\n        \n        results.append([N_eff_rounded, is_degenerate, weights_rounded])\n\n    # 7. Print the final output in the required single-line format\n    # The string representation of a list of lists handled by Python's `str`\n    # and subsequent replacement gives the exact required format.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n\n```"
        }
    ]
}