{
    "hands_on_practices": [
        {
            "introduction": "Before attempting to simplify a complex system of $N_s$ species, we must first understand its inherent constraints. This foundational exercise guides you through a derivation from first principles—the conservation of elements—to quantify the true dimension of the accessible composition space. By proving that the state is confined to a lower-dimensional affine subspace, you will establish the theoretical justification for techniques like ILDM and PCA. ",
            "id": "4032863",
            "problem": "Consider a chemically reacting mixture relevant to computational combustion with $N_s$ chemical species and $M$ chemical elements. Let $Y_i$ denote the species mass fraction of species $i$ for $i \\in \\{1,\\dots,N_s\\}$, and let $a_{\\alpha i}$ denote the number of atoms of element $\\alpha$ in species $i$ for $\\alpha \\in \\{1,\\dots,M\\}$. Let $W_{\\alpha}$ be the atomic weight of element $\\alpha$ and $W_i$ be the molecular weight of species $i$. Suppose the mixture is well-mixed and closed, so only chemical reactions redistribute mass among species while conserving the mass of each element.\n\nStarting from the physical definition that the mass of an individual species is the sum of the masses of its constituent elements and using the conservation of each element’s total mass in the mixture, derive the linear elemental conservation constraints on the species mass fractions in the form $E Y = b$, where $Y \\in \\mathbb{R}^{N_s}$ collects $(Y_1,\\dots,Y_{N_s})$, $b \\in \\mathbb{R}^{M}$ collects the element mass fractions $(b_1,\\dots,b_M)$, and $E \\in \\mathbb{R}^{M \\times N_s}$ is determined by stoichiometry and atomic/molecular weights. Show that the normalization $\\sum_{i=1}^{N_s} Y_i = 1$ follows from the elemental mass fraction definition and is therefore not an independent additional constraint when $b$ represents element mass fractions in the mixture. Then, after imposing the nonnegativity $Y_i \\ge 0$ for all $i$, quantify the dimension of the admissible composition space of $Y$ consistent with the given $b$ in terms of $N_s$ and the rank $r$ of $E$.\n\nYour final answer should be a single closed-form analytic expression in terms of $N_s$ and $r$. No numerical evaluation is required. This dimensionality underpins reduced composition models used in Intrinsic Low-Dimensional Manifolds (ILDM) and informs basis selection in Principal Component Analysis (PCA) for combustion kinetics.",
            "solution": "The problem statement is first validated to ensure it is scientifically grounded, well-posed, and contains sufficient information for a rigorous solution.\n\n**Step 1: Extract Givens**\n- $N_s$: Number of chemical species.\n- $M$: Number of chemical elements.\n- $Y_i$: Mass fraction of species $i$, for $i \\in \\{1, \\dots, N_s\\}$.\n- $a_{\\alpha i}$: Number of atoms of element $\\alpha$ in a molecule of species $i$, for $\\alpha \\in \\{1, \\dots, M\\}$.\n- $W_{\\alpha}$: Atomic weight of element $\\alpha$.\n- $W_i$: Molecular weight of species $i$.\n- The system is a closed, well-mixed chemical mixture.\n- The total mass of each element is conserved.\n- The final state vector $Y$ must satisfy $Y_i \\ge 0$ for all $i$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically sound, as it is based on the fundamental principle of mass conservation in chemical systems. The terminology and variables are standard in chemistry, chemical engineering, and combustion theory. The task is to derive a standard set of linear constraints and then determine the dimension of the resulting solution space, which is a well-posed mathematical question with direct relevance to computational combustion topics like Intrinsic Low-Dimensional Manifolds (ILDM) and Principal Component Analysis (PCA). The problem is objective, self-contained, and free of contradictions or ambiguities.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A complete solution will be provided.\n\n**Derivation of the Elemental Conservation Constraints**\n\nLet us begin by establishing the relationship between the quantities provided. The molecular weight of species $i$, $W_i$, is the sum of the masses of its constituent atoms. This is expressed as:\n$$\nW_i = \\sum_{\\alpha=1}^{M} a_{\\alpha i} W_{\\alpha}\n$$\nThe mass fraction of element $\\alpha$ within a molecule of species $i$ is the ratio of the total mass of element $\\alpha$ in the molecule to the total mass of the molecule. This is given by $\\frac{a_{\\alpha i} W_{\\alpha}}{W_i}$.\n\nIn a mixture with total mass $m_{total}$, the mass of species $i$ is $m_i = Y_i m_{total}$. The mass of element $\\alpha$ contributed by species $i$ is the mass of species $i$ multiplied by the mass fraction of element $\\alpha$ in that species:\n$$\nm_{\\alpha,i} = \\left(\\frac{a_{\\alpha i} W_{\\alpha}}{W_i}\\right) m_i = \\left(\\frac{a_{\\alpha i} W_{\\alpha}}{W_i}\\right) Y_i m_{total}\n$$\nThe total mass of element $\\alpha$ in the mixture, $m_{\\alpha}$, is obtained by summing the contributions from all $N_s$ species:\n$$\nm_{\\alpha} = \\sum_{i=1}^{N_s} m_{\\alpha,i} = \\sum_{i=1}^{N_s} \\left(\\frac{a_{\\alpha i} W_{\\alpha}}{W_i}\\right) Y_i m_{total}\n$$\nThe element mass fraction of element $\\alpha$ in the mixture, denoted $b_{\\alpha}$, is defined as the total mass of element $\\alpha$ divided by the total mass of the mixture, $b_{\\alpha} = \\frac{m_{\\alpha}}{m_{total}}$. Substituting the expression for $m_{\\alpha}$:\n$$\nb_{\\alpha} = \\frac{1}{m_{total}} \\left( \\sum_{i=1}^{N_s} \\left(\\frac{a_{\\alpha i} W_{\\alpha}}{W_i}\\right) Y_i m_{total} \\right) = \\sum_{i=1}^{N_s} \\left(\\frac{a_{\\alpha i} W_{\\alpha}}{W_i}\\right) Y_i\n$$\nThis represents a single linear equation for each element $\\alpha \\in \\{1, \\dots, M\\}$. We can assemble these $M$ equations into a single matrix-vector equation $E Y = b$. The vector of species mass fractions is $Y = [Y_1, Y_2, \\dots, Y_{N_s}]^T \\in \\mathbb{R}^{N_s}$. The vector of element mass fractions is $b = [b_1, b_2, \\dots, b_M]^T \\in \\mathbb{R}^{M}$. The matrix $E$ is an $M \\times N_s$ matrix whose entries are the coefficients of the $Y_i$ terms:\n$$\nE_{\\alpha i} = \\frac{a_{\\alpha i} W_{\\alpha}}{W_i}\n$$\nThus, we have derived the system of linear elemental conservation constraints $E Y = b$.\n\n**Redundancy of the Normalization Constraint**\n\nThe problem next asks to show that the normalization of species mass fractions, $\\sum_{i=1}^{N_s} Y_i = 1$, is not an independent constraint. By definition, the sum of all element mass fractions in the mixture must be unity:\n$$\n\\sum_{\\alpha=1}^{M} b_{\\alpha} = 1\n$$\nLet us sum the $M$ linear equations in the system $E Y = b$ over the index $\\alpha$:\n$$\n\\sum_{\\alpha=1}^{M} b_{\\alpha} = \\sum_{\\alpha=1}^{M} \\left( \\sum_{i=1}^{N_s} E_{\\alpha i} Y_i \\right)\n$$\nSubstituting the expression for $E_{\\alpha i}$ and swapping the order of summation:\n$$\n\\sum_{\\alpha=1}^{M} b_{\\alpha} = \\sum_{i=1}^{N_s} \\sum_{\\alpha=1}^{M} \\left(\\frac{a_{\\alpha i} W_{\\alpha}}{W_i}\\right) Y_i = \\sum_{i=1}^{N_s} Y_i \\left( \\frac{1}{W_i} \\sum_{\\alpha=1}^{M} a_{\\alpha i} W_{\\alpha} \\right)\n$$\nFrom the definition of molecular weight, the term in the parenthesis is $\\frac{1}{W_i} (W_i) = 1$. Therefore, the equation simplifies to:\n$$\n\\sum_{\\alpha=1}^{M} b_{\\alpha} = \\sum_{i=1}^{N_s} Y_i\n$$\nSince $\\sum_{\\alpha=1}^{M} b_{\\alpha} = 1$, it follows directly that $\\sum_{i=1}^{N_s} Y_i = 1$. This demonstrates that the species mass fraction normalization is a linear combination of the elemental conservation equations. It is therefore a redundant constraint, and the rows of the matrix $E$ are linearly dependent. This implies that the rank of $E$, denoted by $r$, is strictly less than $M$.\n\n**Dimensionality of the Admissible Composition Space**\n\nThe admissible composition space is the set of all vectors $Y$ that satisfy the physical constraints of the system. This set, which we can call $\\mathcal{A}$, is defined as:\n$$\n\\mathcal{A} = \\{ Y \\in \\mathbb{R}^{N_s} \\mid E Y = b \\text{ and } Y_i \\ge 0 \\text{ for } i=1, \\dots, N_s \\}\n$$\nThe species composition vector $Y$ lives in the $N_s$-dimensional space $\\mathbb{R}^{N_s}$. The equation $E Y = b$ represents a system of linear equations. The set of all solutions to this system forms an affine subspace of $\\mathbb{R}^{N_s}$. Let $r = \\text{rank}(E)$ be the number of linearly independent elemental conservation constraints. Each independent linear constraint reduces the dimension of the solution space by one.\nTherefore, the dimension of the affine subspace defined by $E Y = b$ is the initial dimension of the ambient space, $N_s$, minus the number of independent constraints, $r$.\n\nThe dimension of the affine subspace of solutions to $E Y = b$ is given by $d = N_s - r$. This result can also be seen from the Rank-Nullity Theorem, which states that for the linear map represented by $E: \\mathbb{R}^{N_s} \\to \\mathbb{R}^{M}$, the dimension of the domain is the sum of the rank and the dimension of the kernel (null space): $\\dim(\\mathbb{R}^{N_s}) = \\text{rank}(E) + \\dim(\\ker(E))$. The dimension of the kernel, $\\dim(\\ker(E))$, which is the dimension of the solution space to the homogeneous equation $EY=0$, is $N_s - r$. The solution space for the inhomogeneous equation $EY=b$ is a translation of this kernel, and thus has the same dimension, $N_s - r$.\n\nThe additional constraints $Y_i \\ge 0$ for all $i$ restrict the solution space to the non-negative orthant of $\\mathbb{R}^{N_s}$. The intersection of the affine subspace with this orthant forms a convex set (specifically, a convex polytope, as it is also bounded since $\\sum Y_i = 1$). The dimension of a convex set is defined as the dimension of its affine hull. Assuming that the mixture composition $b$ is physically realizable, the set $\\mathcal{A}$ is non-empty. In any realistic scenario, $\\mathcal{A}$ is not a single point but a continuous region. The affine hull of $\\mathcal{A}$ is the affine subspace defined by $EY=b$ itself. Therefore, the non-negativity constraints define the boundaries (facets) of the accessible region but do not alter its intrinsic dimension.\n\nThe dimension of the admissible composition space is thus the dimension of the affine subspace defined by the independent elemental conservation equations, which is $N_s - r$.",
            "answer": "$$\\boxed{N_s - r}$$"
        },
        {
            "introduction": "Data-driven methods like PCA are powerful, but their accuracy depends critically on the quality of the input data, which is often corrupted by measurement noise. This practice addresses this real-world challenge by tasking you with deriving a noise-corrected covariance matrix, effectively \"cleaning\" the data before analysis. This skill is essential for ensuring that the resulting principal components reflect the underlying physical processes rather than artifacts from sensors or numerical errors. ",
            "id": "4032850",
            "problem": "In computational combustion, Principal Component Analysis (PCA) is often applied to thermochemical state vectors to identify dominant directions aligned with an Intrinsic Low-Dimensional Manifold (ILDM). Consider a centered dataset of three resolved variables measured in a turbulent premixed flame: mixture fraction $Z$, temperature $T$, and a progress variable $c$. Let the true centered state vector be denoted by $\\mathbf{x} \\in \\mathbb{R}^{3}$ and the measured centered vector be $\\mathbf{y} \\in \\mathbb{R}^{3}$. Measurements are corrupted by additive sensor noise with cross-sensor correlation due to shared electronics and thermal drift, modeled by\n$$\n\\mathbf{y} = \\mathbf{x} + \\boldsymbol{\\epsilon},\n$$\nwhere the noise $\\boldsymbol{\\epsilon}$ is independent of $\\mathbf{x}$, zero-mean, and jointly Gaussian with known covariance.\n\nYou are provided the sample covariance of the measured data, computed from a sufficiently large sample so that it is a consistent estimator of the population covariance of $\\mathbf{y}$,\n$$\nS_{y} = \\begin{pmatrix}\n7  1  0.5 \\\\\n1  5  0.3 \\\\\n0.5  0.3  2.2\n\\end{pmatrix},\n$$\nand a calibration-derived sensor noise covariance,\n$$\n\\Sigma_{\\epsilon} = \\begin{pmatrix}\n1  1  0.5 \\\\\n1  2  0.3 \\\\\n0.5  0.3  0.7\n\\end{pmatrix}.\n$$\n\nStarting only from the definitions of covariance, independence, and the linear measurement model given above, derive a noise-corrected covariance estimator for the latent thermochemical state, and then use it to compute the proportion of corrected variance captured by the first principal component. Specifically, let the corrected covariance be denoted by $S_{x}$ and its ordered eigenvalues be $\\lambda_{1} \\ge \\lambda_{2} \\ge \\lambda_{3}$. Compute the quantity\n$$\n\\frac{\\lambda_{1}}{\\operatorname{tr}(S_{x})}.\n$$\n\nExpress your final answer as a decimal rounded to four significant figures. No units are required for this quantity.",
            "solution": "The user has provided a problem that requires the validation of its premises before proceeding to a solution.\n\n### Step 1: Problem Validation\n\n#### Extraction of Givens\nThe givens from the problem statement are:\n- A linear measurement model: $\\mathbf{y} = \\mathbf{x} + \\boldsymbol{\\epsilon}$, where $\\mathbf{x}, \\mathbf{y}, \\boldsymbol{\\epsilon} \\in \\mathbb{R}^{3}$ are centered random vectors.\n- The true state vector $\\mathbf{x}$ and the noise vector $\\boldsymbol{\\epsilon}$ are independent.\n- The noise vector $\\boldsymbol{\\epsilon}$ is zero-mean, $E[\\boldsymbol{\\epsilon}] = \\mathbf{0}$.\n- The sample covariance of the measured data is given by $S_{y} = \\begin{pmatrix} 7  1  0.5 \\\\ 1  5  0.3 \\\\ 0.5  0.3  2.2 \\end{pmatrix}$. This is assumed to be a consistent estimator of the population covariance of $\\mathbf{y}$.\n- The sensor noise covariance is given by $\\Sigma_{\\epsilon} = \\begin{pmatrix} 1  1  0.5 \\\\ 1  2  0.3 \\\\ 0.5  0.3  0.7 \\end{pmatrix}$.\n- The task is to compute the ratio $\\frac{\\lambda_{1}}{\\operatorname{tr}(S_{x})}$, where $S_x$ is the noise-corrected covariance estimator for $\\mathbf{x}$, and $\\lambda_1$ is its largest eigenvalue.\n\n#### Validation\n1.  **Scientific or Factual Unsoundness**: The problem is scientifically and mathematically sound. It describes a standard method for dealing with additive noise in multivariate data, a common issue in experimental science and engineering. The linear additive noise model is a fundamental concept in signal processing and statistics. The provided matrices are symmetric, a requirement for covariance matrices. We verify if they are positive semi-definite, a necessary condition for a valid covariance matrix.\n    - For $\\Sigma_{\\epsilon}$, the leading principal minors are $1  0$, $1 \\cdot 2 - 1^2 = 1  0$, and $\\det(\\Sigma_{\\epsilon}) = 1(1.4 - 0.09) - 1(0.7 - 0.15) + 0.5(0.3 - 1.0) = 1.31 - 0.55 - 0.35 = 0.41  0$. By Sylvester's criterion, $\\Sigma_{\\epsilon}$ is positive definite.\n    - For $S_{y}$, the leading principal minors are $7  0$, $7 \\cdot 5 - 1^2 = 34  0$, and $\\det(S_{y}) = 7(11-0.09) - 1(2.2-0.15) + 0.5(0.3-2.5) = 76.37 - 2.05 - 1.1 = 73.22  0$. $S_{y}$ is also positive definite.\n    The premises are valid.\n\n2.  **Well-Posed**: The problem is well-posed. It provides all necessary information and a clear objective. The relationship between the true, measured, and noise covariances can be uniquely derived from the given assumptions. The calculation is straightforward.\n\n3.  **Objectivity**: The problem is stated in objective, precise language, free of subjective claims.\n\nBased on this analysis, the problem is deemed valid.\n\n### Step 2: Derivation and Solution\n\nThe goal is to compute the proportion of variance captured by the first principal component of the noise-corrected data. This requires first deriving an expression for the covariance matrix of the true state, $S_x$, and then performing an eigenvalue decomposition.\n\nLet $\\Sigma_x$, $\\Sigma_y$, and $\\Sigma_\\epsilon$ be the population covariance matrices of the random vectors $\\mathbf{x}$, $\\mathbf{y}$, and $\\boldsymbol{\\epsilon}$, respectively. The problem states that the vectors are centered, meaning their expected values are zero vectors. For a centered random vector $\\mathbf{v}$, its covariance matrix is defined as $\\Sigma_v = E[\\mathbf{v}\\mathbf{v}^T]$.\n\nThe measurement model is given as $\\mathbf{y} = \\mathbf{x} + \\boldsymbol{\\epsilon}$.\nThe covariance of the measured vector $\\mathbf{y}$ is:\n$$\n\\Sigma_y = E[\\mathbf{y}\\mathbf{y}^T] = E[(\\mathbf{x} + \\boldsymbol{\\epsilon})(\\mathbf{x} + \\boldsymbol{\\epsilon})^T]\n$$\nExpanding the product, we get:\n$$\n\\Sigma_y = E[\\mathbf{x}\\mathbf{x}^T + \\mathbf{x}\\boldsymbol{\\epsilon}^T + \\boldsymbol{\\epsilon}\\mathbf{x}^T + \\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T]\n$$\nBy the linearity of the expectation operator:\n$$\n\\Sigma_y = E[\\mathbf{x}\\mathbf{x}^T] + E[\\mathbf{x}\\boldsymbol{\\epsilon}^T] + E[\\boldsymbol{\\epsilon}\\mathbf{x}^T] + E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T]\n$$\nThe terms in this expression are identified as:\n- $E[\\mathbf{x}\\mathbf{x}^T] = \\Sigma_x$, the covariance of the true state.\n- $E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\Sigma_\\epsilon$, the covariance of the noise.\n\nThe terms $E[\\mathbf{x}\\boldsymbol{\\epsilon}^T]$ and $E[\\boldsymbol{\\epsilon}\\mathbf{x}^T]$ are cross-covariance matrices. The problem states that $\\mathbf{x}$ and $\\boldsymbol{\\epsilon}$ are independent. For independent random vectors, their cross-covariance is zero. To show this, consider an arbitrary element $(i,j)$ of the matrix $E[\\mathbf{x}\\boldsymbol{\\epsilon}^T]$, which is $E[x_i \\epsilon_j]$. Due to independence, $E[x_i \\epsilon_j] = E[x_i]E[\\epsilon_j]$. Since $\\mathbf{x}$ is centered, $E[x_i]=0$, and the problem states $\\boldsymbol{\\epsilon}$ is zero-mean, so $E[\\epsilon_j]=0$. Thus, $E[x_i \\epsilon_j] = 0 \\cdot 0 = 0$. This holds for all $i,j$, so $E[\\mathbf{x}\\boldsymbol{\\epsilon}^T] = \\mathbf{0}$ (the zero matrix). Similarly, $E[\\boldsymbol{\\epsilon}\\mathbf{x}^T] = (E[\\mathbf{x}\\boldsymbol{\\epsilon}^T])^T = \\mathbf{0}$.\n\nSubstituting these back into the equation for $\\Sigma_y$:\n$$\n\\Sigma_y = \\Sigma_x + \\mathbf{0} + \\mathbf{0} + \\Sigma_\\epsilon = \\Sigma_x + \\Sigma_\\epsilon\n$$\nThis gives the fundamental relationship between the population covariance matrices. To find the covariance of the true state, we can rearrange this equation:\n$$\n\\Sigma_x = \\Sigma_y - \\Sigma_\\epsilon\n$$\nThe problem provides a consistent sample estimator $S_y$ for $\\Sigma_y$ and the known noise covariance $\\Sigma_\\epsilon$. Therefore, a consistent estimator for the true state covariance, which we denote $S_x$, is given by:\n$$\nS_x = S_y - \\Sigma_\\epsilon\n$$\nNow, we can substitute the given matrices:\n$$\nS_x = \\begin{pmatrix} 7  1  0.5 \\\\ 1  5  0.3 \\\\ 0.5  0.3  2.2 \\end{pmatrix} - \\begin{pmatrix} 1  1  0.5 \\\\ 1  2  0.3 \\\\ 0.5  0.3  0.7 \\end{pmatrix}\n$$\nPerforming the matrix subtraction element-wise:\n$$\nS_x = \\begin{pmatrix} 7-1  1-1  0.5-0.5 \\\\ 1-1  5-2  0.3-0.3 \\\\ 0.5-0.5  0.3-0.3  2.2-0.7 \\end{pmatrix} = \\begin{pmatrix} 6  0  0 \\\\ 0  3  0 \\\\ 0  0  1.5 \\end{pmatrix}\n$$\nThe resulting noise-corrected covariance matrix $S_x$ is a diagonal matrix. The eigenvalues of a diagonal matrix are its diagonal entries. The eigenvalues of $S_x$ are therefore $\\lambda_a=6$, $\\lambda_b=3$, and $\\lambda_c=1.5$.\nAccording to the problem, the eigenvalues should be ordered as $\\lambda_1 \\geq \\lambda_2 \\geq \\lambda_3$. Thus, we have:\n$$\n\\lambda_1 = 6\n$$\n$$\n\\lambda_2 = 3\n$$\n$$\n\\lambda_3 = 1.5\n$$\nThe problem asks for the quantity $\\frac{\\lambda_{1}}{\\operatorname{tr}(S_{x})}$. The trace of a matrix, $\\operatorname{tr}(S_x)$, is the sum of its diagonal elements.\n$$\n\\operatorname{tr}(S_x) = 6 + 3 + 1.5 = 10.5\n$$\nNote that the trace is also the sum of the eigenvalues: $\\operatorname{tr}(S_x) = \\lambda_1 + \\lambda_2 + \\lambda_3 = 6 + 3 + 1.5 = 10.5$. The total variance of the corrected data is given by this trace.\n\nThe proportion of the total corrected variance captured by the first principal component is the ratio of the first eigenvalue to the trace of the covariance matrix:\n$$\n\\frac{\\lambda_{1}}{\\operatorname{tr}(S_{x})} = \\frac{6}{10.5}\n$$\nTo simplify the fraction:\n$$\n\\frac{6}{10.5} = \\frac{6}{21/2} = \\frac{12}{21} = \\frac{4}{7}\n$$\nThe problem asks for this value as a decimal rounded to four significant figures.\n$$\n\\frac{4}{7} \\approx 0.57142857...\n$$\nRounding to four significant figures gives $0.5714$.",
            "answer": "$$\\boxed{0.5714}$$"
        },
        {
            "introduction": "A low-dimensional manifold is only useful in a dynamic simulation if there is a robust method to map states from the full-dimensional space onto the manifold itself. This exercise demonstrates this crucial projection step, framing it as a constrained optimization problem where a state vector from a CFD simulation is mapped to the closest point on the manifold. Mastering this technique is key to coupling reduced-order models with CFD solvers while preserving essential physical invariants. ",
            "id": "4032855",
            "problem": "A tabulated intrinsic low-dimensional manifold (ILDM) for a reacting mixture is constructed by sampling quasi-steady compositions and thermochemical states and then applying Principal Component Analysis (PCA) to obtain an orthonormal basis of dominant directions. At a given cell in a Computational Fluid Dynamics (CFD) calculation, a high-dimensional state vector is denoted by $y^{\\ast} \\in \\mathbb{R}^{n}$. The manifold is parameterized in a local affine form $y(c) = y_{0} + U c$, where $y_{0} \\in \\mathbb{R}^{n}$ is the local mean state, $U \\in \\mathbb{R}^{n \\times r}$ contains $r$ principal directions, and $c \\in \\mathbb{R}^{r}$ are reduced coordinates. To compare states with disparate physical units, a weighted Euclidean metric is defined by a symmetric positive-definite weight $W \\in \\mathbb{R}^{n \\times n}$, and the projection is formulated as a constrained least-squares problem that enforces exact invariants (e.g., mixture fraction) via linear equality constraints.\n\nStarting from the foundational modeling assumptions that (i) the closest-point projection in the metric induced by $W$ minimizes the squared weighted norm $\\|W\\,(y_{0} + U c - y^{\\ast})\\|_{2}^{2}$, and (ii) invariants such as mixture fraction are preserved exactly across fast relaxation to the ILDM and are represented as linear constraints of the form $C\\,(y_{0} + U c) = d$, derive and then compute the optimal reduced coordinates for the following specific instance.\n\nLet the state vector be ordered as $y = [Y_{1},\\,Y_{2},\\,\\tilde{h},\\,Z]^{\\top}$, where $Y_{1}$ and $Y_{2}$ are the mass fractions of two lumped species (dimensionless), $\\tilde{h}$ is a nondimensionalized sensible enthalpy, and $Z$ is the mixture fraction. The PCA-derived manifold representation is\n$$\ny_{0} \\;=\\;\n\\begin{pmatrix}\n0.5 \\\\\n0.5 \\\\\n0 \\\\\n0\n\\end{pmatrix},\n\\qquad\nU \\;=\\;\n\\begin{pmatrix}\n0.6  0 \\\\\n-0.6  0 \\\\\n0  1 \\\\\n0.2  0.5\n\\end{pmatrix},\n\\qquad\nW \\;=\\; I_{4},\n$$\nso that the affine subspace $y(c) = y_{0} + U c$ automatically enforces $Y_{1} + Y_{2} = 1$. The CFD state to be projected is\n$$\ny^{\\ast} \\;=\\;\n\\begin{pmatrix}\n0.7 \\\\\n0.3 \\\\\n0.4 \\\\\n0.25\n\\end{pmatrix}.\n$$\nEnforce the exact invariance of the mixture fraction by the linear constraint\n$$\nC\\,(y_{0} + U c) \\;=\\; d,\n\\qquad\nC \\;=\\;\n\\begin{pmatrix}\n0  0  0  1\n\\end{pmatrix},\n\\qquad\nd \\;=\\; Z^{\\ast} \\;=\\; 0.25,\n$$\nand determine the optimal reduced coordinates $c^{\\star} \\in \\mathbb{R}^{2}$ that solve\n$$\n\\min_{c \\in \\mathbb{R}^{2}} \\;\\big\\|W\\,(y_{0} + U c - y^{\\ast})\\big\\|_{2}^{2}\n\\quad\\text{subject to}\\quad\nC\\,(y_{0} + U c) = d.\n$$\nExpress your final answer as the row vector $\\big(c_{1}^{\\star},\\,c_{2}^{\\star}\\big)$ in exact form. No rounding is required and no units are needed.",
            "solution": "The user has provided a well-posed problem in computational science that requires solving a constrained least-squares optimization. The problem is scientifically grounded, formally specified, and contains all necessary data for a unique solution. Therefore, the problem is deemed valid.\n\nThe problem asks for the optimal reduced coordinates $c^{\\star} \\in \\mathbb{R}^{2}$ that solve the constrained minimization problem:\n$$\n\\min_{c \\in \\mathbb{R}^{2}} \\;\\big\\|W\\,(y_{0} + U c - y^{\\ast})\\big\\|_{2}^{2}\n\\quad\\text{subject to}\\quad\nC\\,(y_{0} + U c) = d.\n$$\nThis is a quadratic programming problem with linear equality constraints. A standard method for solving such problems is the method of Lagrange multipliers.\n\nFirst, let us define the objective function $\\mathcal{J}(c)$ and the constraint.\nThe objective function is $\\mathcal{J}(c) = \\|W(y(c) - y^{\\ast})\\|_{2}^{2}$, where $y(c) = y_0 + U c$.\nLet $\\Delta y = y^{\\ast} - y_0$. The expression inside the norm is $Uc - \\Delta y$.\nThe objective function becomes $\\mathcal{J}(c) = \\|W(Uc - \\Delta y)\\|_{2}^{2}$.\nUsing the definition of the squared Euclidean norm, this is $\\mathcal{J}(c) = (Uc - \\Delta y)^{\\top}W^{\\top}W(Uc - \\Delta y)$.\nThe problem provides $W = I_{4}$, the $4 \\times 4$ identity matrix. Thus, $W^{\\top}W = I_{4}^{\\top}I_{4} = I_{4}$. The objective function simplifies to the standard (unweighted) least-squares form:\n$$\n\\mathcal{J}(c) = \\|Uc - \\Delta y\\|_{2}^{2} = (Uc - \\Delta y)^{\\top}(Uc - \\Delta y) = c^{\\top}U^{\\top}Uc - 2c^{\\top}U^{\\top}\\Delta y + \\Delta y^{\\top}\\Delta y.\n$$\nThe linear constraint is $C(y_0 + Uc) = d$, which can be rearranged as:\n$$\nCUc - (d - Cy_0) = 0.\n$$\nWe form the Lagrangian function $\\Lambda(c, \\lambda)$ by adding the constraint multiplied by a Lagrange multiplier $\\lambda \\in \\mathbb{R}^{1}$ (since there is one scalar constraint):\n$$\n\\Lambda(c, \\lambda) = \\mathcal{J}(c) + \\lambda \\big(CUc - (d - Cy_0)\\big).\n$$\nThe optimal coordinates $c^{\\star}$ must satisfy the Karush-Kuhn-Tucker (KKT) conditions. For this problem, they are found by setting the gradients of the Lagrangian with respect to $c$ and $\\lambda$ to zero.\n\nThe gradient with respect to $c$ is:\n$$\n\\nabla_{c} \\Lambda(c, \\lambda) = \\nabla_{c} \\left( c^{\\top}U^{\\top}Uc - 2c^{\\top}U^{\\top}\\Delta y + \\Delta y^{\\top}\\Delta y + \\lambda(CUc - (d - Cy_0)) \\right) = 0.\n$$\nUsing standard matrix calculus identities, we get:\n$$\n2U^{\\top}Uc - 2U^{\\top}\\Delta y + \\lambda(CU)^{\\top} = 0\n\\implies 2U^{\\top}Uc - 2U^{\\top}\\Delta y + \\lambda U^{\\top}C^{\\top} = 0.\n$$\nDividing by $2$ and letting $\\mu = \\lambda/2$, we obtain the first equation:\n$$\n(U^{\\top}U)c + \\mu U^{\\top}C^{\\top} = U^{\\top}\\Delta y.\n$$\nThe derivative with respect to $\\lambda$ simply recovers the constraint:\n$$\n\\frac{\\partial \\Lambda}{\\partial \\lambda} = CUc - (d - Cy_0) = 0 \\implies (CU)c = d - Cy_0.\n$$\nThese two equations form a system of linear equations for $c$ and $\\mu$:\n$$\n\\begin{pmatrix} U^{\\top}U  U^{\\top}C^{\\top} \\\\ CU  0 \\end{pmatrix} \\begin{pmatrix} c \\\\ \\mu \\end{pmatrix} = \\begin{pmatrix} U^{\\top}\\Delta y \\\\ d - Cy_0 \\end{pmatrix}.\n$$\nNow we compute the specific matrices and vectors for the given problem.\nThe given data are:\n$$\ny_{0} = \\begin{pmatrix} 0.5 \\\\ 0.5 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\\nU = \\begin{pmatrix} 0.6  0 \\\\ -0.6  0 \\\\ 0  1 \\\\ 0.2  0.5 \\end{pmatrix}, \\\ny^{\\ast} = \\begin{pmatrix} 0.7 \\\\ 0.3 \\\\ 0.4 \\\\ 0.25 \\end{pmatrix}, \\\nC = \\begin{pmatrix} 0  0  0  1 \\end{pmatrix}, \\\nd = 0.25.\n$$\nFirst, calculate the vector $\\Delta y = y^{\\ast} - y_0$:\n$$\n\\Delta y = \\begin{pmatrix} 0.7 - 0.5 \\\\ 0.3 - 0.5 \\\\ 0.4 - 0 \\\\ 0.25 - 0 \\end{pmatrix} = \\begin{pmatrix} 0.2 \\\\ -0.2 \\\\ 0.4 \\\\ 0.25 \\end{pmatrix}.\n$$\nNext, we compute the components of the block matrix system.\n$$\nU^{\\top}U = \\begin{pmatrix} 0.6  -0.6  0  0.2 \\\\ 0  0  1  0.5 \\end{pmatrix} \\begin{pmatrix} 0.6  0 \\\\ -0.6  0 \\\\ 0  1 \\\\ 0.2  0.5 \\end{pmatrix} = \\begin{pmatrix} 0.36 + 0.36 + 0.04  0.1 \\\\ 0.1  1 + 0.25 \\end{pmatrix} = \\begin{pmatrix} 0.76  0.1 \\\\ 0.1  1.25 \\end{pmatrix}.\n$$\n$$\nU^{\\top}C^{\\top} = \\begin{pmatrix} 0.6  -0.6  0  0.2 \\\\ 0  0  1  0.5 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0.2 \\\\ 0.5 \\end{pmatrix}.\n$$\n$$\nCU = \\begin{pmatrix} 0  0  0  1 \\end{pmatrix} \\begin{pmatrix} 0.6  0 \\\\ -0.6  0 \\\\ 0  1 \\\\ 0.2  0.5 \\end{pmatrix} = \\begin{pmatrix} 0.2  0.5 \\end{pmatrix}.\n$$\n$$\nU^{\\top}\\Delta y = \\begin{pmatrix} 0.6  -0.6  0  0.2 \\\\ 0  0  1  0.5 \\end{pmatrix} \\begin{pmatrix} 0.2 \\\\ -0.2 \\\\ 0.4 \\\\ 0.25 \\end{pmatrix} = \\begin{pmatrix} 0.12 + 0.12 + 0.05 \\\\ 0.4 + 0.125 \\end{pmatrix} = \\begin{pmatrix} 0.29 \\\\ 0.525 \\end{pmatrix}.\n$$\n$$\nCy_0 = \\begin{pmatrix} 0  0  0  1 \\end{pmatrix} \\begin{pmatrix} 0.5 \\\\ 0.5 \\\\ 0 \\\\ 0 \\end{pmatrix} = 0.\n$$\n$$\nd - Cy_0 = 0.25 - 0 = 0.25.\n$$\nLet $c = \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}$. The system of equations is:\n$$\n\\begin{pmatrix} 0.76  0.1  0.2 \\\\ 0.1  1.25  0.5 \\\\ 0.2  0.5  0 \\end{pmatrix} \\begin{pmatrix} c_1 \\\\ c_2 \\\\ \\mu \\end{pmatrix} = \\begin{pmatrix} 0.29 \\\\ 0.525 \\\\ 0.25 \\end{pmatrix}.\n$$\nThis represents the following three linear equations:\n1. $0.76 c_1 + 0.1 c_2 + 0.2 \\mu = 0.29$\n2. $0.1 c_1 + 1.25 c_2 + 0.5 \\mu = 0.525$\n3. $0.2 c_1 + 0.5 c_2 = 0.25$\n\nTo maintain precision, we convert to fractions: $0.76=\\frac{19}{25}$, $0.1=\\frac{1}{10}$, $0.2=\\frac{1}{5}$, $1.25=\\frac{5}{4}$, $0.5=\\frac{1}{2}$, $0.29=\\frac{29}{100}$, $0.525=\\frac{21}{40}$, $0.25=\\frac{1}{4}$.\nThe system is:\n1. $\\frac{19}{25} c_1 + \\frac{1}{10} c_2 + \\frac{1}{5} \\mu = \\frac{29}{100}$\n2. $\\frac{1}{10} c_1 + \\frac{5}{4} c_2 + \\frac{1}{2} \\mu = \\frac{21}{40}$\n3. $\\frac{1}{5} c_1 + \\frac{1}{2} c_2 = \\frac{1}{4}$\n\nFrom equation (3), we can solve for $c_2$ in terms of $c_1$ (or vice versa). Multiplying by $10$ gives $2c_1 + 5c_2 = 2.5$, or $4c_1 + 10c_2 = 5$.\nLet's eliminate $\\mu$. Multiply equation (1) by $5$ and equation (2) by $2$:\n$5 \\times (1): \\frac{19}{5} c_1 + \\frac{1}{2} c_2 + \\mu = \\frac{29}{20}$\n$2 \\times (2): \\frac{1}{5} c_1 + \\frac{5}{2} c_2 + \\mu = \\frac{21}{20}$\nSubtracting the second new equation from the first:\n$$\n\\left(\\frac{19}{5} - \\frac{1}{5}\\right) c_1 + \\left(\\frac{1}{2} - \\frac{5}{2}\\right) c_2 = \\frac{29}{20} - \\frac{21}{20}\n$$\n$$\n\\frac{18}{5} c_1 - 2 c_2 = \\frac{8}{20} = \\frac{2}{5}\n$$\nMultiplying by $5$ gives $18 c_1 - 10 c_2 = 2$, which simplifies to $9 c_1 - 5 c_2 = 1$.\nWe now have a system of two equations for $c_1$ and $c_2$:\n(A) $9 c_1 - 5 c_2 = 1$\n(B) $\\frac{1}{5} c_1 + \\frac{1}{2} c_2 = \\frac{1}{4} \\implies 4 c_1 + 10 c_2 = 5$\n\nFrom (A), $5c_2 = 9c_1 - 1$. Substitute into (B):\n$4 c_1 + 2(5 c_2) = 5$\n$4 c_1 + 2(9 c_1 - 1) = 5$\n$4 c_1 + 18 c_1 - 2 = 5$\n$22 c_1 = 7 \\implies c_1^{\\star} = \\frac{7}{22}$.\n\nNow substitute $c_1^{\\star}$ back to find $c_2^{\\star}$:\n$5 c_2 = 9 \\left(\\frac{7}{22}\\right) - 1 = \\frac{63}{22} - \\frac{22}{22} = \\frac{41}{22}$.\n$c_2^{\\star} = \\frac{41}{22 \\times 5} = \\frac{41}{110}$.\n\nThe optimal reduced coordinates are $c^{\\star} = (c_1^{\\star}, c_2^{\\star}) = (\\frac{7}{22}, \\frac{41}{110})$.\n\nFinal check: The constraint must be satisfied.\n$0.2 c_1 + 0.5 c_2 = \\frac{1}{5}\\left(\\frac{7}{22}\\right) + \\frac{1}{2}\\left(\\frac{41}{110}\\right) = \\frac{7}{110} + \\frac{41}{220} = \\frac{14}{220} + \\frac{41}{220} = \\frac{55}{220} = \\frac{1}{4} = 0.25$.\nThe constraint is satisfied. The solution is correct.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{7}{22}  \\frac{41}{110}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}