{
    "hands_on_practices": [
        {
            "introduction": "The microcanonical ensemble serves as the bedrock of statistical mechanics, describing isolated systems with fixed energy, volume, and particle number ($E, V, N$). This exercise takes you back to first principles, demonstrating how to derive the entropy of an ideal gas by counting microstates in classical phase space. By completing this derivation of the Sackur-Tetrode equation, you will solidify your understanding of the link between microscopic states and macroscopic entropy and see firsthand how the crucial $1/N!$ factor for indistinguishable particles resolves the Gibbs paradox .",
            "id": "2650653",
            "problem": "Consider a classical, nonrelativistic, monatomic ideal gas of $N$ identical particles of mass $m$ confined in a container of volume $V$, with fixed total energy $E$ and no external fields. Work entirely within the microcanonical ensemble. Begin from first principles of classical Hamiltonian mechanics and the definition of microcanonical entropy. Treat the phase space as $6N$-dimensional with coordinates $(\\mathbf{q},\\mathbf{p})$, and use Liouville’s measure. Use the standard coarse-graining by Planck’s constant $h$ to render the microstate count dimensionless. \n\n(a) Carefully justify the appearance of the indistinguishability factor $1/N!$ in the classical counting of microstates on the grounds of overcounting due to particle permutations. Your justification should be based on the structure of phase space and symmetry under the permutation group, and it should resolve the Gibbs paradox by explaining why removal of a partition between two subsystems of identical gases at equal intensive variables does not produce an entropy increase.\n\n(b) Using the microcanonical definition of entropy, $S = k_{\\mathrm{B}} \\ln \\Omega$, where $k_{\\mathrm{B}}$ is the Boltzmann constant and $\\Omega$ is an appropriate count of accessible microstates at fixed $E$, derive the explicit closed-form expression for the entropy $S(E,V,N)$ of this system. You must:\n- Start from the classical Hamiltonian $H = \\sum_{i=1}^{N} \\mathbf{p}_{i}^{2}/(2m)$.\n- Perform the full phase-space integration to obtain the energy-shell measure in terms of $E$, $V$, $N$, $m$, and $h$.\n- Treat indistinguishability via the factor $1/N!$ as justified in part (a).\n- For $N \\gg 1$, use Stirling-type asymptotics consistently and neglect only subextensive corrections smaller than order $N$ (e.g., terms scaling like $\\ln N$). State explicitly where such terms are dropped.\n\nProvide your final answer as a single closed-form expression involving only $E$, $V$, $N$, $m$, $h$, and $k_{\\mathrm{B}}$. Do not include any intermediate expressions or equations in the final boxed answer. No numerical evaluation or rounding is required.",
            "solution": "The problem as stated is scientifically sound, well-posed, and objective. It presents a standard, fundamental derivation in classical statistical mechanics. All necessary physical parameters and mathematical constraints are provided. The problem is therefore valid, and a solution will be provided.\n\n(a) Justification of the Indistinguishability Factor and Resolution of the Gibbs Paradox\n\nA microstate of the system is defined by a single point $(\\mathbf{q}, \\mathbf{p}) = (\\mathbf{q}_1, \\ldots, \\mathbf{q}_N, \\mathbf{p}_1, \\ldots, \\mathbf{p}_N)$ in the $6N$-dimensional phase space. The Hamiltonian of the system, $H = \\sum_{i=1}^{N} \\frac{\\mathbf{p}_{i}^{2}}{2m}$, is symmetric with respect to the permutation of the indices of the particles. This means that if we swap the coordinates and momenta of particle $i$ and particle $j$, the energy of the system remains unchanged.\n\nIf the $N$ particles were distinguishable, then a state described by the phase space point $(\\ldots, \\mathbf{q}_i, \\mathbf{p}_i, \\ldots, \\mathbf{q}_j, \\mathbf{p}_j, \\ldots)$ would be physically distinct from the state $(\\ldots, \\mathbf{q}_j, \\mathbf{p}_j, \\ldots, \\mathbf{q}_i, \\mathbf{p}_i, \\ldots)$. For a given set of $N$ position and momentum vectors, there are $N!$ ways to assign these vectors to the $N$ distinguishable particles. The classical phase space volume calculation for distinguishable particles counts these $N!$ permutations as distinct microstates.\n\nHowever, fundamental principles of quantum mechanics dictate that identical particles are truly indistinguishable. A physical state is completely specified by the set of occupied single-particle states, not by which particle occupies which state. Swapping two identical particles does not produce a new physical state. The classical framework must be corrected to be consistent with this fact, as classical statistical mechanics should emerge as a limit of quantum statistical mechanics. This correction, known as the Gibbs correction, is to divide the total number of classically counted states by $N!$, representing the number of permutations among the $N$ identical particles. This division rectifies the overcounting of states that are physically identical.\n\nThis correction is essential for resolving the Gibbs paradox. Consider a container of volume $2V$ divided by a partition into two equal halves, each of volume $V$. Each half contains an identical ideal gas with $N$ particles and energy $E$, meaning they are at the same temperature and pressure. The total entropy of the system before removing the partition is $S_{initial} = S(E,V,N) + S(E,V,N) = 2S(E,V,N)$.\n\nIf we omit the $1/N!$ factor, the entropy is not extensive. For example, it would contain a term proportional to $N k_{\\mathrm{B}} \\ln V$. The initial entropy would be $S_{initial} \\propto 2(N k_{\\mathrm{B}} \\ln V)$. After removing the partition, we have a single system with $2N$ particles in a volume $2V$. The final entropy would be $S_{final} \\propto (2N) k_{\\mathrm{B}} \\ln(2V)$. The change in entropy, the entropy of mixing, would be $\\Delta S = S_{final} - S_{initial} \\propto (2N)k_{\\mathrm{B}}\\ln(2V) - 2Nk_{\\mathrm{B}}\\ln(V) = 2Nk_{\\mathrm{B}}\\ln(2) > 0$. This implies a spontaneous entropy increase for a process that is macroscopically reversible and in which no net change occurs. This is the paradox.\n\nWhen the $1/N!$ factor is included correctly, the entropy becomes an extensive quantity. As will be derived below, the entropy has the form $S(E,V,N) = N k_{\\mathrm{B}} f(E/N, V/N)$, where the argument of the function $f$ involves only intensive variables. Specifically, it contains a term $N k_{\\mathrm{B}} \\ln(V/N)$.\nThe initial entropy is $S_{initial} = 2 S(E,V,N)$.\nThe final state has $2N$ particles in volume $2V$ with energy $2E$. Its entropy is $S_{final} = S(2E, 2V, 2N)$.\nBecause entropy is now extensive, $S(\\lambda E, \\lambda V, \\lambda N) = \\lambda S(E, V, N)$. Setting $\\lambda=2$, we find $S_{final} = 2S(E,V,N)$.\nThe change in entropy is $\\Delta S = S_{final} - S_{initial} = 2S(E,V,N) - 2S(E,V,N) = 0$. This correct result, $\\Delta S=0$, resolves the Gibbs paradox, confirming that mixing identical gases at the same intensive state is a reversible process with no change in entropy.\n\n(b) Derivation of the Entropy Expression\n\nWe begin with the microcanonical definition of entropy, $S = k_{\\mathrm{B}} \\ln \\Omega$, where $\\Omega$ is the number of accessible microstates. For a classical system, this is related to the volume of the accessible phase space. The Hamiltonian is $H = \\sum_{i=1}^{N} \\frac{\\mathbf{p}_{i}^{2}}{2m}$.\nThe number of states is computed by integrating over the phase space region consistent with the total energy $E$, and making the result dimensionless using Planck's constant $h$. We account for indistinguishability with the $1/N!$ factor. We first calculate the phase space volume $\\Phi(E)$ for states with energy less than or equal to $E$:\n$$ \\Phi(E) = \\frac{1}{N! h^{3N}} \\int_{H \\leq E} d^{3N}q \\, d^{3N}p $$\nSince the Hamiltonian does not depend on the positions $\\mathbf{q}_i$, the integral over the spatial coordinates is simply the volume of the container raised to the power of $N$:\n$$ \\int d^{3N}q = \\left( \\int_V d^3q \\right)^N = V^N $$\nThe momentum integral is over the region $\\sum_{i=1}^{N} \\mathbf{p}_{i}^{2} \\leq 2mE$. This corresponds to the volume of a $3N$-dimensional hypersphere in momentum space with radius $R_p = \\sqrt{2mE}$. The volume of a $d$-dimensional hypersphere of radius $R$ is given by $V_d(R) = \\frac{\\pi^{d/2}}{\\Gamma(d/2 + 1)} R^d$.\nFor our case, the dimension is $d = 3N$. The volume of the accessible momentum space is:\n$$ \\int_{\\sum \\mathbf{p}_i^2 \\le 2mE} d^{3N}p = \\frac{\\pi^{3N/2}}{\\Gamma(\\frac{3N}{2} + 1)} (2mE)^{3N/2} $$\nCombining these results, the cumulative phase space volume is:\n$$ \\Phi(E) = \\frac{V^N}{N! h^{3N}} \\frac{\\pi^{3N/2}}{\\Gamma(\\frac{3N}{2} + 1)} (2mE)^{3N/2} = \\frac{V^N}{N! h^{3N}} \\frac{(2\\pi mE)^{3N/2}}{\\Gamma(\\frac{3N}{2} + 1)} $$\nThe number of microstates $\\Omega(E)$ is strictly the volume of a thin energy shell, $\\Omega(E) = (\\frac{\\partial \\Phi}{\\partial E})\\delta E$. However, for $N \\gg 1$, the function $\\Phi(E) \\propto E^{3N/2}$ grows extremely rapidly, so the overwhelming majority of states are located near the energy surface $E$. The entropy calculated using $S = k_{\\mathrm{B}} \\ln(\\Phi(E))$ differs from that using $S = k_{\\mathrm{B}} \\ln(\\Omega(E))$ by terms of order $k_{\\mathrm{B}} \\ln N$. The problem statement permits the neglect of such subextensive corrections. We therefore proceed with $S \\approx k_{\\mathrm{B}} \\ln(\\Phi(E))$.\n$$ \\frac{S}{k_{\\mathrm{B}}} = \\ln\\left[ \\frac{V^N (2\\pi mE)^{3N/2}}{N! h^{3N} \\Gamma(\\frac{3N}{2} + 1)} \\right] $$\n$$ \\frac{S}{k_{\\mathrm{B}}} = N \\ln V + \\frac{3N}{2}\\ln(2\\pi mE) - \\ln(N!) - 3N \\ln h - \\ln\\Gamma\\left(\\frac{3N}{2} + 1\\right) $$\nFor large $N$, we use Stirling's approximation $\\ln(x!) \\approx x \\ln x - x$. This also applies to the Gamma function, since $\\ln\\Gamma(x+1) = \\ln(x!)$.\n$$ \\ln(N!) \\approx N \\ln N - N $$\n$$ \\ln\\Gamma\\left(\\frac{3N}{2} + 1\\right) \\approx \\frac{3N}{2} \\ln\\left(\\frac{3N}{2}\\right) - \\frac{3N}{2} $$\nSubstituting these approximations into the expression for entropy:\n$$ \\frac{S}{k_{\\mathrm{B}}} \\approx N \\ln V + \\frac{3N}{2}\\ln(2\\pi mE) - (N \\ln N - N) - 3N \\ln h - \\left(\\frac{3N}{2} \\ln\\left(\\frac{3N}{2}\\right) - \\frac{3N}{2}\\right) $$\nWe now group the terms:\n$$ \\frac{S}{k_{\\mathrm{B}}} \\approx N \\ln V - N \\ln N + \\frac{3N}{2} \\ln E - \\frac{3N}{2} \\ln N + N + \\frac{3N}{2} + \\frac{3N}{2} \\ln(2\\pi m) - 3N \\ln h - \\frac{3N}{2} \\ln\\left(\\frac{3}{2}\\right) $$\n$$ \\frac{S}{k_{\\mathrm{B}}} \\approx N \\ln\\left(\\frac{V}{N}\\right) + \\frac{3N}{2}\\ln\\left(\\frac{E}{N}\\right) + \\frac{5N}{2} + \\frac{3N}{2} \\left[ \\ln(2\\pi m) - 2 \\ln h - \\ln\\left(\\frac{3}{2}\\right) \\right] $$\n$$ \\frac{S}{k_{\\mathrm{B}}} \\approx N \\left[ \\ln\\left(\\frac{V}{N}\\right) + \\frac{3}{2}\\ln\\left(\\frac{E}{N}\\right) + \\frac{5}{2} + \\frac{3}{2} \\ln\\left(\\frac{2\\pi m \\cdot 2}{3 h^2}\\right) \\right] $$\n$$ \\frac{S}{k_{\\mathrm{B}}} \\approx N \\left[ \\ln\\left(\\frac{V}{N}\\right) + \\frac{3}{2}\\ln\\left(\\frac{E}{N}\\right) + \\frac{3}{2} \\ln\\left(\\frac{4\\pi m}{3h^2}\\right) + \\frac{5}{2} \\right] $$\nFinally, we combine the logarithmic terms into a single argument to obtain the closed-form expression, known as the Sackur-Tetrode equation:\n$$ S = N k_{\\mathrm{B}} \\left[ \\ln\\left(\\frac{V}{N}\\right) + \\ln\\left(\\left(\\frac{E}{N}\\right)^{3/2}\\right) + \\ln\\left(\\left(\\frac{4\\pi m}{3h^2}\\right)^{3/2}\\right) + \\frac{5}{2} \\right] $$\n$$ S = N k_{\\mathrm{B}} \\left[ \\ln\\left( \\frac{V}{N} \\left(\\frac{4\\pi m E}{3N h^2}\\right)^{3/2} \\right) + \\frac{5}{2} \\right] $$\nThis expression for entropy is properly extensive, as demonstrated in part (a). All subextensive terms of order $\\ln N$ and lower have been systematically dropped through the use of Stirling's approximation, as required.",
            "answer": "$$ \\boxed{ N k_{\\mathrm{B}} \\left[ \\ln\\left( \\frac{V}{N} \\left( \\frac{4\\pi m E}{3N h^2} \\right)^{3/2} \\right) + \\frac{5}{2} \\right] } $$"
        },
        {
            "introduction": "In computational studies, the canonical ensemble is paramount, modeling systems at constant temperature, volume, and particle number ($T, V, N$). A thermostat algorithm is designed to ensure that a simulation correctly samples from this ensemble, but how can we verify its performance? This practice guides you through creating a rigorous validation protocol, deriving the expected statistical distributions for particle speeds and kinetic energy from canonical principles. By applying statistical tests to synthetic data, you will learn a critical skill for any computational scientist: how to confirm that your simulation tools are generating physically correct thermodynamic states .",
            "id": "4093857",
            "problem": "You are tasked with designing and implementing a validation protocol, in the context of Molecular Dynamics (MD), to verify that a velocity thermostat samples from the correct canonical ensemble. The protocol must assess whether simulated velocities follow the Maxwell–Boltzmann velocity distribution and whether total kinetic energy fluctuations are consistent with canonical ensemble predictions. The program you deliver must be a complete, runnable script that generates synthetic MD velocity data under multiple test scenarios, computes appropriate statistical tests, and reports pass or fail decisions for each scenario.\n\nBegin from a fundamental base: assume a system of $N$ identical, non-interacting point particles with mass $m$ in $d$ spatial dimensions, in thermal contact with a reservoir at absolute temperature $T$. Under the canonical ensemble, the equilibrium probability density over microstates is proportional to $\\exp(-\\beta H)$, where $\\beta = 1/(k_{\\mathrm{B}} T)$ and $H$ is the Hamiltonian. For an ideal gas, the Hamiltonian reduces to the kinetic energy, and each velocity component is expected to be Gaussian due to the quadratic form of the energy in the velocities. Use this base to derive from first principles the target distributions to be used for validation.\n\nYour protocol must implement the following steps:\n\n1. Derive the Maxwell–Boltzmann distribution for the speed $v$ in $d$ dimensions for a single particle of mass $m$ in equilibrium at temperature $T$. Use the canonical ensemble and the isotropy of velocities to obtain the probability density function (PDF) $f(v)$ and the corresponding cumulative distribution function (CDF) $F(v)$ for the speed. You must express $F(v)$ in a closed form that is numerically evaluable for arbitrary real $d > 0$.\n\n2. Derive the distribution of the total kinetic energy $K$ per configuration (or per frame) for $N$ non-interacting particles in $d$ dimensions in equilibrium at temperature $T$. From first principles, obtain the PDF and CDF of $K$ and the canonical expression of its mean and variance implied by the fluctuation formula. The final forms must be suitable for numerical evaluation with modern scientific libraries.\n\n3. Implement a validation protocol that:\n   - Generates synthetic MD velocity data according to prescribed scenarios. For a \"consistent\" thermostat, generate velocities by sampling each component independently from a Gaussian distribution implied by equilibrium at temperature $T$ and mass $m$. For \"inconsistent\" scenarios, construct velocity samples that violate canonical expectations (for example, by using a temperature that differs from the specified $T$, or by adding a fixed drift velocity vector to all particles).\n   - Pools all particle speeds across all generated frames for the speed distribution test.\n   - Computes total kinetic energy $K$ for each frame and pools the frame-wise energies for the energy fluctuation test.\n   - Performs a statistical goodness-of-fit test comparing the pooled speed sample to the derived Maxwell–Boltzmann CDF and the pooled energy sample to the derived canonical energy CDF. Use the Kolmogorov–Smirnov (KS) test with a significance level $\\alpha$; a test case passes only if both KS tests yield $p$-values greater than or equal to $\\alpha$.\n\nUse the following well-tested scientific facts and definitions as the base for your derivations:\n- The canonical ensemble assigns probability density proportional to $\\exp(-\\beta H)$, with $\\beta = 1/(k_{\\mathrm{B}} T)$ and $H$ the Hamiltonian.\n- For an ideal gas, the Hamiltonian is the sum of quadratic contributions in velocities, implying independent Gaussian velocity components.\n- The total kinetic energy in a system composed of independent quadratic degrees of freedom is the sum of independent, identically distributed quadratic forms.\n\nPhysical units and constants:\n- Temperature $T$ must be in Kelvin (K).\n- Mass $m$ must be in kilograms (kg).\n- Velocity components and speeds $v$ must be in meters per second (m/s).\n- Energy $K$ must be in Joules (J).\n- The Boltzmann constant $k_{\\mathrm{B}}$ is $1.380649 \\times 10^{-23} \\, \\mathrm{J/K}$.\n\nYour program must implement the protocol, generate synthetic data for the test suite below using a fixed random seed to ensure reproducibility, execute the KS tests (for both speed and energy), and output a boolean per test case indicating whether both validations pass. Use a significance level $\\alpha = 0.01$.\n\nTest suite specification:\n- For each test case, the parameters are $(N, d, T, m, M, \\text{mode})$, where $N$ is the number of particles, $d$ is the number of spatial dimensions, $T$ is the target thermostat temperature, $m$ is the particle mass, $M$ is the number of independent frames, and $\\text{mode}$ specifies the data generation scenario:\n  1. $(500, 3, 300 \\, \\mathrm{K}, 6.63 \\times 10^{-26} \\, \\mathrm{kg}, 200, \\text{consistent})$.\n  2. $(500, 3, 300 \\, \\mathrm{K}, 6.63 \\times 10^{-26} \\, \\mathrm{kg}, 200, \\text{wrong\\_T})$; velocities are generated at $450 \\, \\mathrm{K}$ instead of $300 \\, \\mathrm{K}$.\n  3. $(500, 3, 300 \\, \\mathrm{K}, 6.63 \\times 10^{-26} \\, \\mathrm{kg}, 200, \\text{drift})$; velocities are generated at $300 \\, \\mathrm{K}$ with an added fixed drift of $200 \\, \\mathrm{m/s}$ along a single axis for all particles.\n  4. $(1000, 2, 300 \\, \\mathrm{K}, 6.63 \\times 10^{-26} \\, \\mathrm{kg}, 200, \\text{consistent})$.\n  5. $(10, 3, 300 \\, \\mathrm{K}, 6.63 \\times 10^{-26} \\, \\mathrm{kg}, 100, \\text{consistent})$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, \"[result1,result2,result3,result4,result5]\".\n- Each result must be a boolean that is \"True\" if both the speed distribution KS test and the energy distribution KS test pass with $p \\ge \\alpha$, and \"False\" otherwise.\n\nThe derivation of all mathematical entities must be shown in your solution, and the program must implement these derived forms to perform the statistical validation.",
            "solution": "The problem statement poses a valid and well-defined task in computational statistical mechanics: to design and implement a protocol for validating that a thermostat correctly samples the canonical ensemble. This involves deriving the theoretical distributions for particle speed and total system kinetic energy from first principles and then using these distributions to statistically test synthetically generated data. The problem is scientifically grounded, objective, and provides all necessary information for a complete solution.\n\nHerein, I present the formal derivations of the required distributions and the structure of the validation algorithm.\n\n### Part 1: Derivation of the Maxwell-Boltzmann Speed Distribution\n\nWe consider a system of non-interacting point particles in thermal equilibrium at temperature $T$. In the canonical ensemble, the probability of a microstate is proportional to the Boltzmann factor, $\\exp(-\\beta H)$, where $H$ is the Hamiltonian and $\\beta = 1/(k_B T)$. For an ideal gas, the Hamiltonian is purely kinetic.\n\n#### 1.1 Single-Component Velocity Distribution\nFor a single particle of mass $m$, the kinetic energy associated with one component of its velocity, say $v_x$, is $E_x = \\frac{1}{2} m v_x^2$. The probability density function (PDF) for $v_x$ is:\n$$p(v_x) \\propto \\exp(-\\beta E_x) = \\exp\\left(-\\frac{m v_x^2}{2 k_B T}\\right)$$\nThis is a Gaussian distribution with mean $0$. To normalize it, we require $\\int_{-\\infty}^{\\infty} p(v_x) dv_x = 1$. The normalization constant is found from the standard Gaussian integral $\\int_{-\\infty}^{\\infty} e^{-ax^2} dx = \\sqrt{\\pi/a}$. With $a = m/(2 k_B T)$, the normalized PDF is:\n$$p(v_x) = \\sqrt{\\frac{m}{2 \\pi k_B T}} \\exp\\left(-\\frac{m v_x^2}{2 k_B T}\\right)$$\nThis corresponds to a normal distribution $\\mathcal{N}(\\mu, \\sigma_v^2)$ with mean $\\mu=0$ and variance $\\sigma_v^2 = k_B T / m$.\n\n#### 1.2 Speed Distribution in $d$ Dimensions\nThe particle's velocity vector in $d$ dimensions is $\\mathbf{v} = (v_1, v_2, \\dots, v_d)$. Since the components are independent for an ideal gas, the joint PDF is:\n$$p(\\mathbf{v}) = \\prod_{i=1}^d p(v_i) = \\left(\\frac{m}{2 \\pi k_B T}\\right)^{d/2} \\exp\\left(-\\frac{m}{2 k_B T} \\sum_{i=1}^d v_i^2\\right)$$\nThe particle's speed is the magnitude $v = |\\mathbf{v}| = \\sqrt{\\sum_{i=1}^d v_i^2}$. The joint PDF depends only on $v$. To find the PDF of the speed, $f(v)$, we can transform to hyperspherical coordinates. The volume element in velocity space $d\\mathbf{v} = dv_1 \\dots dv_d$ becomes $v^{d-1} S_d dv$, where $S_d = 2\\pi^{d/2}/\\Gamma(d/2)$ is the surface area of a unit hypersphere in $d$ dimensions. The probability of finding a speed between $v$ and $v+dv$ is found by integrating $p(\\mathbf{v})$ over a hyperspherical shell of radius $v$ and thickness $dv$:\n$$f(v)dv = p(\\mathbf{v}) S_d v^{d-1} dv$$\n$$f(v) = \\left(\\frac{m}{2 \\pi k_B T}\\right)^{d/2} \\exp\\left(-\\frac{m v^2}{2 k_B T}\\right) \\frac{2 \\pi^{d/2}}{\\Gamma(d/2)} v^{d-1}$$\nAfter simplification, we obtain the Maxwell-Boltzmann speed distribution in $d$ dimensions:\n$$f(v) = \\frac{2}{\\Gamma(d/2)} \\left(\\frac{m}{2 k_B T}\\right)^{d/2} v^{d-1} \\exp\\left(-\\frac{m v^2}{2 k_B T}\\right)$$\n\n#### 1.3 Cumulative Distribution Function (CDF) of Speed\nThe CDF, $F(v)$, is the integral of the PDF from $0$ to $v$:\n$$F(v) = \\int_0^v f(u) du = \\int_0^v \\frac{2}{\\Gamma(d/2)} \\left(\\frac{m}{2 k_B T}\\right)^{d/2} u^{d-1} \\exp\\left(-\\frac{m u^2}{2 k_B T}\\right) du$$\nLet's substitute $x = \\frac{m u^2}{2 k_B T}$. This implies $u^2 = \\frac{2 k_B T x}{m}$ and $2u du = \\frac{2 k_B T}{m} dx$, so $du = \\frac{k_B T}{mu} dx$.\nA more direct substitution is on the differential form $f(u)du$:\nLet $x = \\frac{m u^2}{2 k_B T}$. Then $u^{d-1} = (\\frac{2 k_B T x}{m})^{(d-1)/2}$ and $du = \\frac{1}{2}\\sqrt{\\frac{2 k_B T}{m x}} dx$.\nThis leads to:\n$u^{d-1} du = (\\frac{2k_BT}{m})^{(d-1)/2} x^{(d-1)/2} \\frac{1}{2} (\\frac{2k_BT}{m})^{1/2} x^{-1/2} dx = \\frac{1}{2}(\\frac{2k_BT}{m})^{d/2} x^{d/2-1} dx$.\nSubstituting this into the integrand:\n$$f(u)du = \\frac{2}{\\Gamma(d/2)} \\left(\\frac{m}{2 k_B T}\\right)^{d/2} e^{-x} \\left( \\frac{1}{2} \\left(\\frac{2 k_B T}{m}\\right)^{d/2} x^{d/2 - 1} dx \\right) = \\frac{1}{\\Gamma(d/2)} x^{d/2-1} e^{-x} dx$$\nIntegrating this from $u=0$ to $u=v$ corresponds to integrating from $x=0$ to $x=\\frac{m v^2}{2 k_B T}$:\n$$F(v) = \\int_0^{\\frac{m v^2}{2 k_B T}} \\frac{1}{\\Gamma(d/2)} t^{d/2-1} e^{-t} dt$$\nThis integral defines the lower regularized incomplete gamma function, $P(s, z) = \\gamma(s, z)/\\Gamma(s)$, where $\\gamma(s,z) = \\int_0^z t^{s-1} e^{-t} dt$.\nThus, the CDF of the speed is:\n$$F(v) = P\\left(\\frac{d}{2}, \\frac{m v^2}{2 k_B T}\\right)$$\nThis expression is numerically evaluable using standard scientific libraries for arbitrary real $d > 0$.\n\n### Part 2: Derivation of the Total Kinetic Energy Distribution\n\n#### 2.1 Distribution of Total Kinetic Energy\nThe total kinetic energy $K$ for a system of $N$ particles in $d$ dimensions is the sum over all $N \\times d$ independent quadratic degrees of freedom:\n$$K = \\sum_{i=1}^{N} \\sum_{j=1}^{d} \\frac{1}{2} m v_{ij}^2$$\nEach term $\\frac{1}{2}mv_{ij}^2$ represents the kinetic energy of a single degree of freedom. As established in Part 1, each velocity component $v_{ij}$ is an independent random variable drawn from $\\mathcal{N}(0, k_B T / m)$. The variable $Z_{ij} = v_{ij} / \\sqrt{k_B T/m}$ is a standard normal variable, $\\mathcal{N}(0, 1)$. The energy contribution from one degree of freedom is $\\frac{1}{2} m v_{ij}^2 = \\frac{1}{2} k_B T Z_{ij}^2$. The term $Z_{ij}^2$ follows a chi-squared distribution with one degree of freedom, $\\chi^2_1$, which is equivalent to a Gamma distribution $\\text{Gamma}(k=1/2, \\theta=2)$.\nTherefore, the energy of one degree of freedom, $\\frac{1}{2} k_B T Z_{ij}^2$, follows a Gamma distribution with shape $k=1/2$ and scale $\\theta=k_B T$.\nThe total kinetic energy $K$ is the sum of $Nd$ such independent and identically distributed Gamma variables. The sum of independent Gamma variables with a common scale $\\theta$ is also a Gamma variable with the same scale, and a shape parameter equal to the sum of the individual shape parameters.\nThus, $K$ follows a Gamma distribution:\n$$K \\sim \\text{Gamma}\\left(\\text{shape} = \\sum_{i=1}^{Nd} \\frac{1}{2}, \\text{scale} = k_B T\\right) = \\text{Gamma}\\left(k_K=\\frac{Nd}{2}, \\theta_K=k_B T\\right)$$\nThe PDF of $K$ is given by the Gamma PDF:\n$$f(K) = \\frac{K^{k_K-1} e^{-K/\\theta_K}}{\\theta_K^{k_K} \\Gamma(k_K)} = \\frac{K^{Nd/2 - 1} \\exp(-K/(k_B T))}{(k_B T)^{Nd/2} \\Gamma(Nd/2)}$$\n\n#### 2.2 CDF of Total Kinetic Energy\nThe CDF, $F(K)$, is the integral of its PDF. By a similar change of variables, $t=x/\\theta_K$, as used for the speed CDF, we find that the CDF of the total kinetic energy is also a lower regularized incomplete gamma function:\n$$F(K) = P\\left(k_K, \\frac{K}{\\theta_K}\\right) = P\\left(\\frac{Nd}{2}, \\frac{K}{k_B T}\\right)$$\n\n#### 2.3 Mean and Variance of Total Kinetic Energy\nFor a Gamma-distributed random variable $X \\sim \\text{Gamma}(k, \\theta)$, the mean is $\\langle X \\rangle = k\\theta$ and the variance is $\\text{Var}(X) = k\\theta^2$. Applying this to the total kinetic energy $K$:\n$$\\langle K \\rangle = k_K \\theta_K = \\frac{Nd}{2} k_B T$$\nThis is the equipartition theorem, assigning an average energy of $\\frac{1}{2} k_B T$ to each of the $Nd$ quadratic degrees of freedom.\nThe variance is:\n$$\\text{Var}(K) = k_K \\theta_K^2 = \\frac{Nd}{2} (k_B T)^2$$\nThis matches the canonical ensemble fluctuation formula $\\text{Var}(E) = k_B T^2 C_V$, where the heat capacity for an ideal gas is $C_V = (\\partial \\langle E \\rangle / \\partial T)_V = \\frac{Nd}{2}k_B$.\n\n### Part 3: Algorithmic Implementation of the Validation Protocol\n\nThe validation protocol is implemented as a Python script adhering to the problem specifications.\n\n1.  **Synthetic Data Generation**: A function generates velocity data for $M$ frames, each with $N$ particles in $d$ dimensions. For each of the $M \\times N \\times d$ velocity components, a random value is drawn from a Gaussian distribution $\\mathcal{N}(0, \\sigma_v^2)$ where $\\sigma_v = \\sqrt{k_B T_{gen}/m}$.\n    -   For `consistent` mode, $T_{gen}$ is the target temperature $T$.\n    -   For `wrong_T` mode, $T_{gen}$ is a different temperature ($450 \\, \\mathrm{K}$ as specified).\n    -   For `drift` mode, velocities are generated with the correct $T_{gen}=T$, and then a constant drift velocity is added to one component for all particles.\n\n2.  **Data Processing**: From the generated velocity tensor `(M, N, d)`, we compute:\n    -   A 1D array of particle speeds by calculating the Euclidean norm of each particle's velocity vector and flattening the resulting `(M, N)` array.\n    -   A 1D array of frame-wise total kinetic energies by summing the kinetic energies of all particles within each of the $M$ frames.\n\n3.  **Statistical Testing**: The Kolmogorov-Smirnov (KS) test is used to compare the empirical cumulative distribution function (ECDF) of the sampled data against the theoretically derived CDFs.\n    -   The pooled speed data is tested against the speed CDF $F(v) = P(d/2, m v^2 / (2 k_B T))$.\n    -   The frame-wise kinetic energy data is tested against the energy CDF $F(K) = P(Nd/2, K / (k_B T))$.\n    -   Crucially, for all test cases (including inconsistent ones), the theoretical CDFs are computed using the *target* thermostat temperature $T=300\\,\\mathrm{K}$, as the goal is to validate against the specified setpoint.\n\n4.  **Decision**: For each test case, the p-values from both the speed and energy KS tests are obtained. If both p-values are greater than or equal to the significance level $\\alpha = 0.01$, the test case is considered passed (`True`). Otherwise, it has failed (`False`). This process is repeated for all scenarios in the test suite. A fixed random seed ensures the results are reproducible.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import kstest\nfrom scipy.special import gammainc, gamma\n\ndef solve():\n    \"\"\"\n    Implements a validation protocol for a canonical ensemble thermostat.\n    \"\"\"\n    KB = 1.380649e-23  # Boltzmann constant in J/K\n    ALPHA = 0.01       # Significance level for KS tests\n    \n    # Set a fixed random seed for reproducibility\n    np.random.seed(42)\n\n    def mb_speed_cdf(v, d, m, T):\n        \"\"\"\n        Calculates the theoretical Maxwell-Boltzmann speed CDF in d dimensions.\n        CDF(v) = P(d/2, m*v**2 / (2*k_B*T))\n        where P is the lower regularized incomplete gamma function.\n        \"\"\"\n        if np.any(v  0):\n            # Speed cannot be negative, handle this case for kstest\n            v = np.maximum(v, 0)\n        \n        shape = d / 2.0\n        # Argument for the incomplete gamma function\n        x = m * v**2 / (2.0 * KB * T)\n        return gammainc(shape, x)\n\n    def kinetic_energy_cdf(K, N, d, T):\n        \"\"\"\n        Calculates the theoretical total kinetic energy CDF for N particles in d dimensions.\n        CDF(K) = P(N*d/2, K / (k_B*T))\n        where P is the lower regularized incomplete gamma function.\n        \"\"\"\n        if np.any(K  0):\n            K = np.maximum(K, 0)\n\n        shape = N * d / 2.0\n        # Argument for the incomplete gamma function\n        x = K / (KB * T)\n        return gammainc(shape, x)\n\n    def generate_velocities(N, d, T_gen, m, M):\n        \"\"\"\n        Generates synthetic velocity data for M frames.\n        Each velocity component is sampled from N(0, k_B*T_gen/m).\n        \"\"\"\n        sigma_v = np.sqrt(KB * T_gen / m)\n        velocities = np.random.normal(loc=0.0, scale=sigma_v, size=(M, N, d))\n        return velocities\n    \n    def run_validation(N, d, T_target, m, M, mode):\n        \"\"\"\n        Runs the full validation protocol for a single test case.\n        \"\"\"\n        # --- 1. Generate Data ---\n        if mode == 'consistent' or mode == 'drift':\n            T_gen = T_target\n        elif mode == 'wrong_T':\n            T_gen = 450.0  # As specified in the problem for this scenario\n        else:\n            raise ValueError(f\"Unknown mode: {mode}\")\n\n        velocities = generate_velocities(N, d, T_gen, m, M)\n\n        if mode == 'drift':\n            drift_velocity = 200.0  # m/s\n            velocities[:, :, 0] += drift_velocity\n\n        # --- 2. Process Data ---\n        # Pool all particle speeds across all frames\n        speeds = np.linalg.norm(velocities, axis=2).flatten()\n        \n        # Calculate total kinetic energy for each frame\n        kinetic_energies = 0.5 * m * np.sum(velocities**2, axis=(1, 2))\n\n        # --- 3. Perform Statistical Tests ---\n        # Test against theoretical distributions corresponding to T_target\n        \n        # Speed distribution test\n        ks_speed = kstest(speeds, lambda v: mb_speed_cdf(v, d=d, m=m, T=T_target))\n        \n        # Kinetic energy distribution test\n        ks_energy = kstest(kinetic_energies, lambda K: kinetic_energy_cdf(K, N=N, d=d, T=T_target))\n        \n        # --- 4. Make Decision ---\n        pass_speed_test = ks_speed.pvalue >= ALPHA\n        pass_energy_test = ks_energy.pvalue >= ALPHA\n        \n        return pass_speed_test and pass_energy_test\n\n    # Test suite from the problem statement\n    test_cases = [\n        # (N, d, T, m, M, mode)\n        (500, 3, 300.0, 6.63e-26, 200, 'consistent'),\n        (500, 3, 300.0, 6.63e-26, 200, 'wrong_T'),\n        (500, 3, 300.0, 6.63e-26, 200, 'drift'),\n        (1000, 2, 300.0, 6.63e-26, 200, 'consistent'),\n        (10, 3, 300.0, 6.63e-26, 100, 'consistent'),\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_validation(*params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "The grand canonical ensemble (GCE) is indispensable for studying open systems where temperature, volume, and chemical potentials ($T, V, \\mu_i$) are fixed. Its power lies in the fluctuation-dissipation theorem, which connects microscopic fluctuations in particle number ($N_i$) to macroscopic thermodynamic response functions. This computational exercise shows you how to harness this principle by analyzing particle number fluctuations from a GCE simulation of a polymer solution. You will implement a protocol to extract vital properties like isothermal compressibility and partial molar volumes, demonstrating how passive observation of fluctuations can yield deep thermodynamic insights .",
            "id": "4093890",
            "problem": "Consider a binary polymer solution composed of a solvent species $s$ and a polymer species $p$. You perform simulations in the Grand Canonical Ensemble (GCE), where the temperature $T$, volume $V$, and chemical potentials $\\mu_s$ and $\\mu_p$ are fixed, and the particle numbers $N_s$ and $N_p$ fluctuate. The grand canonical partition function $\\Xi(T,V,\\mu_s,\\mu_p)$ generates number fluctuations, and the pressure satisfies $pV = k_B T \\ln \\Xi$. In GCE, the average particle numbers satisfy $\\langle N_i \\rangle = \\frac{1}{\\beta} \\frac{\\partial \\ln \\Xi}{\\partial \\mu_i}$, and their covariances satisfy $\\langle \\Delta N_i \\Delta N_j \\rangle = \\frac{\\partial \\langle N_i \\rangle}{\\partial (\\beta \\mu_j)}$, where $\\beta = 1/(k_B T)$ and $k_B$ is the Boltzmann constant.\n\nA central goal in computational complex fluids is to extract bulk thermodynamic response properties, such as the isothermal compressibility $\\kappa_T$, and partial molar properties such as the partial molar volume $\\bar{v}_i$, directly from number fluctuation data. The isothermal compressibility is defined by $\\kappa_T = -\\frac{1}{V} \\left( \\frac{\\partial V}{\\partial p} \\right)_{T,\\{N_i\\}}$ and is a state function independent of ensemble. In GCE, $\\kappa_T$ can be related to number fluctuations. For mixtures, let $N_{\\mathrm{tot}} = N_s + N_p$, $\\boldsymbol{\\rho} = \\langle \\mathbf{N} \\rangle / V$ with $\\mathbf{N} = (N_s, N_p)^\\top$, and $C$ be the covariance matrix $C_{ij} = \\langle \\Delta N_i \\Delta N_j \\rangle$. The isothermal compressibility can be expressed as\n$$\n\\kappa_T = \\frac{\\mathrm{Var}(N_{\\mathrm{tot}})}{k_B T \\, V \\, \\rho_{\\mathrm{tot}}^2}\n$$\nwhere $\\mathrm{Var}(N_{\\mathrm{tot}}) = \\mathbf{1}^\\top C \\mathbf{1}$ and $\\rho_{\\mathrm{tot}} = \\rho_s + \\rho_p$. Partial molar volume is defined by $\\bar{v}_i = \\left( \\frac{\\partial V}{\\partial N_i} \\right)_{T,p,N_{j\\neq i}}$. Using linear response and the Gibbs-Duhem relation at constant temperature, a computational protocol that maps GCE number fluctuations to partial molar volumes is\n$$\nB = \\frac{C}{k_B T}, \\quad \\bar{\\mathbf{v}} = \\frac{B^{+} \\langle \\mathbf{N} \\rangle}{\\boldsymbol{\\rho}^\\top B^{+} \\langle \\mathbf{N} \\rangle}\n$$\nwhere $B^{+}$ denotes the Moore–Penrose pseudoinverse, and $\\bar{\\mathbf{v}} = (\\bar{v}_s, \\bar{v}_p)^\\top$. This returns $\\bar{v}_i$ in units of $\\mathrm{m}^3$ per molecule.\n\nYou are asked to implement a complete, runnable program that, given discrete number fluctuation histograms for $(N_s,N_p)$ from GCE simulations, computes $\\kappa_T$ and the partial molar volumes $\\bar{v}_s$ and $\\bar{v}_p$. Construct the discrete histograms using a bivariate normal weight evaluated on integer grids for $(N_s,N_p)$ and then normalized to a probability mass function. Use the following physically plausible test suite, where each case provides $T$ in $\\mathrm{K}$, $V$ in $\\mathrm{m}^3$, the mean vector $m = (\\langle N_s \\rangle, \\langle N_p \\rangle)$, the covariance matrix\n$$\n\\Sigma = \\begin{pmatrix}\n\\sigma_s^2  \\sigma_{sp} \\\\\n\\sigma_{sp}  \\sigma_p^2\n\\end{pmatrix},\n$$\nand the grid ranges for $N_s$ and $N_p$:\n\n- Case $1$ (dilute polymer):\n  - $T = 300$, $V = 1 \\times 10^{-21}$\n  - $m = (100, 1)$\n  - $\\sigma_s^2 = 50$, $\\sigma_p^2 = 0.8$, $\\sigma_{sp} = -0.5$\n  - $N_s \\in \\{90,91,\\dots,110\\}$, $N_p \\in \\{0,1,2,3,4,5\\}$\n\n- Case $2$ (semi-dilute polymer):\n  - $T = 300$, $V = 2 \\times 10^{-21}$\n  - $m = (80, 20)$\n  - $\\sigma_s^2 = 60$, $\\sigma_p^2 = 15$, $\\sigma_{sp} = -8$\n  - $N_s \\in \\{60,61,\\dots,100\\}$, $N_p \\in \\{10,11,\\dots,30\\}$\n\n- Case $3$ (nearly ideal mixture):\n  - $T = 300$, $V = 1 \\times 10^{-21}$\n  - $m = (50, 5)$\n  - $\\sigma_s^2 = 45$, $\\sigma_p^2 = 5$, $\\sigma_{sp} = 0$\n  - $N_s \\in \\{35,36,\\dots,65\\}$, $N_p \\in \\{0,1,\\dots,12\\}$\n\n- Case $4$ (very dilute polymer, near singular covariance):\n  - $T = 300$, $V = 5 \\times 10^{-22}$\n  - $m = (120, 0.2)$\n  - $\\sigma_s^2 = 70$, $\\sigma_p^2 = 0.2$, $\\sigma_{sp} = -0.1$\n  - $N_s \\in \\{100,101,\\dots,140\\}$, $N_p \\in \\{0,1,2\\}$\n\nFor each case:\n- Construct the discrete histogram $P(N_s,N_p) \\propto \\exp\\left(-\\frac{1}{2} (\\mathbf{N}-m)^\\top \\Sigma^{-1} (\\mathbf{N}-m)\\right)$ over the specified integer grid and normalize it so that $\\sum_{N_s,N_p} P(N_s,N_p) = 1$.\n- Compute the sample mean vector $\\langle \\mathbf{N} \\rangle$ and covariance matrix $C$ from $P(N_s,N_p)$.\n- Compute the density vector $\\boldsymbol{\\rho} = \\langle \\mathbf{N} \\rangle / V$ and the total density $\\rho_{\\mathrm{tot}} = \\rho_s + \\rho_p$.\n- Compute the isothermal compressibility using $\\kappa_T = \\frac{\\mathbf{1}^\\top C \\mathbf{1}}{k_B T \\, V \\, \\rho_{\\mathrm{tot}}^2}$, and report it in units of $\\mathrm{Pa}^{-1}$ as a float.\n- Compute the partial molar volume vector using $\\bar{\\mathbf{v}} = \\frac{B^{+} \\langle \\mathbf{N} \\rangle}{\\boldsymbol{\\rho}^\\top B^{+} \\langle \\mathbf{N} \\rangle}$, with $B = C/(k_B T)$, and report $\\bar{v}_s$ and $\\bar{v}_p$ in $\\mathrm{nm}^3$ per molecule, where $1\\,\\mathrm{nm}^3 = 10^{-27}\\,\\mathrm{m}^3$.\n\nPhysical units requirement:\n- Report $\\kappa_T$ in $\\mathrm{Pa}^{-1}$.\n- Report $\\bar{v}_s$ and $\\bar{v}_p$ in $\\mathrm{nm}^3$ per molecule.\n\nYour program must produce a single line of output containing the results aggregated across all cases as a comma-separated list enclosed in square brackets. For each case, return a list with three floats in the order $[\\kappa_T,\\bar{v}_s,\\bar{v}_p]$, so the final output is a list of four such lists, for example $[[x_1,y_1,z_1],[x_2,y_2,z_2],[x_3,y_3,z_3],[x_4,y_4,z_4]]$, where each $x_i$, $y_i$, and $z_i$ are the corresponding computed floats for case $i$.",
            "solution": "The problem has been analyzed and is deemed valid. It is scientifically grounded in the principles of statistical mechanics for open systems, specifically the Grand Canonical Ensemble (GCE). The provided formulas connecting particle number fluctuations to thermodynamic response functions like isothermal compressibility and partial molar volumes are standard results from linear response theory applied to fluid mixtures. The problem is well-posed, providing all necessary data and a clear, computationally tractable procedure. The parameters given in the test cases are physically plausible for simulations of binary solutions.\n\nThe objective is to compute the isothermal compressibility, $\\kappa_T$, and the partial molar volumes, $\\bar{v}_s$ and $\\bar{v}_p$, for a binary mixture based on a model of number fluctuations from GCE simulations. The fluctuation data is represented by a discrete probability mass function (PMF) constructed from a bivariate normal distribution over a specified integer grid of particle numbers $(N_s, N_p)$.\n\nThe Boltzmann constant $k_B$ is required for the calculations, and its value is taken as $k_B = 1.380649 \\times 10^{-23} \\, \\mathrm{J} \\cdot \\mathrm{K}^{-1}$.\n\nThe computational procedure for each test case is as follows:\n\n**1. Construct the Discrete Probability Mass Function (PMF)**\nFor each case, we are given the parameters for a continuous bivariate normal distribution: the mean vector $m = (\\langle N_s \\rangle_{in}, \\langle N_p \\rangle_{in})^\\top$ and the covariance matrix $\\Sigma$. We are also given integer grid ranges for the particle numbers $N_s$ and $N_p$. Let the vector of particle numbers be $\\mathbf{N} = (N_s, N_p)^\\top$.\n\nFirst, we generate a 2D grid of all integer pairs $(N_s, N_p)$ within the specified ranges. For each point $\\mathbf{N}$ on this grid, we calculate an unnormalized probability weight $W(\\mathbf{N})$ using the multivariate normal probability density function form:\n$$\nW(\\mathbf{N}) = \\exp\\left(-\\frac{1}{2} (\\mathbf{N}-m)^\\top \\Sigma^{-1} (\\mathbf{N}-m)\\right)\n$$\nwhere $\\Sigma^{-1}$ is the inverse of the given covariance matrix.\n\nThe discrete PMF, $P(\\mathbf{N})$, is obtained by normalizing these weights such that their sum over the entire grid equals $1$:\n$$\nP(\\mathbf{N}) = \\frac{W(\\mathbf{N})}{\\sum_{\\mathbf{N}' \\in \\text{grid}} W(\\mathbf{N}')}\n$$\n\n**2. Compute Sample Moments from the PMF**\nUsing the calculated PMF, we compute the sample mean vector $\\langle \\mathbf{N} \\rangle$ and the sample covariance matrix $C$. These are the moments of the discrete distribution and will slightly differ from the input parameters $m$ and $\\Sigma$ due to the discretization and truncation of the distribution by the finite grid.\n\nThe components of the mean vector are calculated as the expectation value of each particle number:\n$$\n\\langle N_i \\rangle = \\sum_{\\mathbf{N} \\in \\text{grid}} N_i P(\\mathbf{N})\n$$\nwhere $i \\in \\{s, p\\}$.\n\nThe elements of the covariance matrix $C$ are given by $C_{ij} = \\langle (N_i - \\langle N_i \\rangle)(N_j - \\langle N_j \\rangle) \\rangle$, which can be computed as:\n$$\nC_{ij} = \\left( \\sum_{\\mathbf{N} \\in \\text{grid}} N_i N_j P(\\mathbf{N}) \\right) - \\langle N_i \\rangle \\langle N_j \\rangle\n$$\nfor $i, j \\in \\{s, p\\}$.\n\n**3. Calculate Thermodynamic Properties a. Isothermal Compressibility, $\\kappa_T$**\nThe isothermal compressibility is calculated using the formula relating it to the total number fluctuations:\n$$\n\\kappa_T = \\frac{\\mathrm{Var}(N_{\\mathrm{tot}})}{k_B T V \\rho_{\\mathrm{tot}}^2}\n$$\nHere, $T$ and $V$ are the temperature and volume given for the case.\nThe total number density is $\\rho_{\\mathrm{tot}} = (\\langle N_s \\rangle + \\langle N_p \\rangle) / V$.\nThe variance of the total particle number, $\\mathrm{Var}(N_{\\mathrm{tot}})$, is the sum of all elements of the covariance matrix $C$:\n$$\n\\mathrm{Var}(N_{\\mathrm{tot}}) = \\mathrm{Var}(N_s+N_p) = C_{ss} + C_{pp} + 2C_{sp} = \\mathbf{1}^\\top C \\mathbf{1}\n$$\nwhere $\\mathbf{1} = (1, 1)^\\top$. Inserting all quantities in SI units ($k_B$ in $\\mathrm{J/K}$, $T$ in $\\mathrm{K}$, $V$ in $\\mathrm{m}^3$, $\\rho_{\\mathrm{tot}}$ in $\\mathrm{m}^{-3}$, and $C$ being dimensionless) yields $\\kappa_T$ in units of $\\mathrm{Pa}^{-1}$.\n\n**b. Partial Molar Volumes, $\\bar{\\mathbf{v}}$**\nThe partial molar volume vector $\\bar{\\mathbf{v}} = (\\bar{v}_s, \\bar{v}_p)^\\top$ is computed using the provided matrix formula.\nFirst, we define the matrix $B$:\n$$\nB = \\frac{C}{k_B T}\n$$\nNext, we compute the Moore-Penrose pseudoinverse of $B$, denoted as $B^{+}$. This is a robust approach that handles cases where $B$ (and thus $C$) might be singular or ill-conditioned, as could happen with limited sampling or strong correlations.\n\nThe partial molar volume vector is then given by:\n$$\n\\bar{\\mathbf{v}} = \\frac{B^{+} \\langle \\mathbf{N} \\rangle}{\\boldsymbol{\\rho}^\\top B^{+} \\langle \\mathbf{N} \\rangle}\n$$\nwhere $\\boldsymbol{\\rho} = \\langle \\mathbf{N} \\rangle / V$ is the density vector. The resulting vector $\\bar{\\mathbf{v}}$ has units of volume per particle, which will be $\\mathrm{m}^3$ if all inputs are in SI units. The final result must be converted to $\\mathrm{nm}^3$ per molecule by multiplying by $10^{27}$, since $1 \\, \\mathrm{m}^3 = 10^{27} \\, \\mathrm{nm}^3$.\n\nThis complete procedure will be implemented for all four test cases provided in the problem statement.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Computes isothermal compressibility and partial molar volumes from\n    GCE number fluctuation data for a binary polymer solution.\n    \"\"\"\n    # Physical constants and conversion factors\n    k_B = 1.380649e-23  # Boltzmann constant in J/K\n    M3_TO_NM3 = 1e27   # Conversion factor from m^3 to nm^3\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"T\": 300, \"V\": 1e-21,\n            \"m\": np.array([100, 1]),\n            \"Sigma\": np.array([[50, -0.5], [-0.5, 0.8]]),\n            \"Ns_range\": (90, 110), \"Np_range\": (0, 5)\n        },\n        {\n            \"T\": 300, \"V\": 2e-21,\n            \"m\": np.array([80, 20]),\n            \"Sigma\": np.array([[60, -8], [-8, 15]]),\n            \"Ns_range\": (60, 100), \"Np_range\": (10, 30)\n        },\n        {\n            \"T\": 300, \"V\": 1e-21,\n            \"m\": np.array([50, 5]),\n            \"Sigma\": np.array([[45, 0], [0, 5]]),\n            \"Ns_range\": (35, 65), \"Np_range\": (0, 12)\n        },\n        {\n            \"T\": 300, \"V\": 5e-22,\n            \"m\": np.array([120, 0.2]),\n            \"Sigma\": np.array([[70, -0.1], [-0.1, 0.2]]),\n            \"Ns_range\": (100, 140), \"Np_range\": (0, 2)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        T = case[\"T\"]\n        V = case[\"V\"]\n        m = case[\"m\"]\n        Sigma = case[\"Sigma\"]\n        Ns_range = case[\"Ns_range\"]\n        Np_range = case[\"Np_range\"]\n\n        # Step 1: Construct the Discrete Probability Mass Function (PMF)\n        \n        # Create the integer grid for Ns and Np\n        Ns_vals = np.arange(Ns_range[0], Ns_range[1] + 1)\n        Np_vals = np.arange(Np_range[0], Np_range[1] + 1)\n        Ns_grid, Np_grid = np.meshgrid(Ns_vals, Np_vals)\n        \n        # Reshape grid into a list of (Ns, Np) coordinate pairs\n        N_grid = np.stack([Ns_grid.ravel(), Np_grid.ravel()], axis=1)\n        \n        # Calculate unnormalized probability weights using the multivariate normal PDF form\n        Sigma_inv = np.linalg.inv(Sigma)\n        diff = N_grid - m\n        # Efficiently compute the quadratic form (x-m)^T * Sigma^-1 * (x-m) for all grid points\n        exponent = -0.5 * np.sum((diff @ Sigma_inv) * diff, axis=1)\n        weights = np.exp(exponent)\n        \n        # Normalize weights to get the PMF\n        P = weights / np.sum(weights)\n\n        # Step 2: Compute Sample Moments from the PMF\n        \n        # Mean vector N\n        N_mean = np.sum(N_grid * P[:, np.newaxis], axis=0)\n        \n        # Covariance matrix C\n        # E[N_i * N_j]\n        outer_prod = N_grid[:, :, np.newaxis] * N_grid[:, np.newaxis, :]\n        N_outer_mean = np.sum(outer_prod * P[:, np.newaxis, np.newaxis], axis=0)\n        # C = E[N_i * N_j] - E[N_i] * E[N_j]\n        C = N_outer_mean - np.outer(N_mean, N_mean)\n\n        # Step 3: Calculate Thermodynamic Properties\n        \n        # Density vector and total density\n        rho_vec = N_mean / V\n        rho_tot = np.sum(rho_vec)\n\n        # a. Isothermal Compressibility, kappa_T\n        var_N_tot = np.sum(C)\n        beta_T = k_B * T\n        kappa_T = var_N_tot / (beta_T * V * rho_tot**2)\n\n        # b. Partial Molar Volumes, v_bar\n        B = C / beta_T\n        \n        # Use Moore-Penrose pseudoinverse for robustness\n        B_pinv = np.linalg.pinv(B)\n        \n        numerator_vec = B_pinv @ N_mean\n        denominator_scalar = rho_vec.T @ numerator_vec\n        \n        # The result is in m^3, convert to nm^3\n        v_bar = (numerator_vec / denominator_scalar) * M3_TO_NM3\n        \n        v_bar_s = v_bar[0]\n        v_bar_p = v_bar[1]\n\n        results.append([kappa_T, v_bar_s, v_bar_p])\n\n    # Final print statement in the exact required format.\n    # The format is a list of lists of floats.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}