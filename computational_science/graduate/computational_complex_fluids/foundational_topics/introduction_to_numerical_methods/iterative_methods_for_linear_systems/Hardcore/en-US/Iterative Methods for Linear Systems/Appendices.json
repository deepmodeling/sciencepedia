{
    "hands_on_practices": [
        {
            "introduction": "The appeal of iterative methods lies in their simplicity, but their convergence is not guaranteed for all systems. This first exercise moves beyond mechanical application to the crucial analysis of convergence. By constructing the Jacobi iteration matrix for a given system and examining its eigenvalues, you will learn to predict whether the method will succeed or fail, a fundamental skill in numerical analysis .",
            "id": "2182318",
            "problem": "An iterative method is a numerical procedure that generates a sequence of improving approximate solutions for a class of problems. For a linear system of equations $A\\mathbf{x} = \\mathbf{b}$, one such method is the Jacobi method. The matrix $A$ can be decomposed as $A = D - L - U$, where $D$ is the diagonal part of $A$, $-L$ is the strictly lower-triangular part of $A$, and $-U$ is the strictly upper-triangular part of $A$.\n\nThe Jacobi iteration is defined by the formula:\n$$D\\mathbf{x}^{(k+1)} = (L+U)\\mathbf{x}^{(k)} + \\mathbf{b}$$\nThis can be rewritten in the form $\\mathbf{x}^{(k+1)} = T_J \\mathbf{x}^{(k)} + \\mathbf{c}$, where $T_J = D^{-1}(L+U)$ is known as the Jacobi iteration matrix. The convergence of the method is determined by the properties of this matrix.\n\nConsider the following system of linear equations:\n$$\n\\begin{align*}\n2x_1 + 3x_2 = 5 \\\\\n4x_1 + 2x_2 = 6\n\\end{align*}\n$$\nDetermine the two eigenvalues of the Jacobi iteration matrix $T_J$ corresponding to this system. Present your answer as a row matrix containing the two eigenvalues, ordered in ascending numerical value.",
            "solution": "The given system of linear equations is:\n$$\n\\begin{align*}\n2x_1 + 3x_2 = 5 \\\\\n4x_1 + 2x_2 = 6\n\\end{align*}\n$$\nThis can be written in matrix form $A\\mathbf{x} = \\mathbf{b}$, where:\n$$A = \\begin{pmatrix} 2  3 \\\\ 4  2 \\end{pmatrix}, \\quad \\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 5 \\\\ 6 \\end{pmatrix}$$\n\nTo find the Jacobi iteration matrix $T_J$, we first decompose the matrix $A$ into its diagonal ($D$), strictly lower-triangular ($-L$), and strictly upper-triangular ($-U$) parts.\n$$A = D - L - U$$\nFrom the matrix $A$, we identify:\n$$D = \\begin{pmatrix} 2  0 \\\\ 0  2 \\end{pmatrix}$$\n$$L = \\begin{pmatrix} 0  0 \\\\ -4  0 \\end{pmatrix}$$\n$$U = \\begin{pmatrix} 0  -3 \\\\ 0  0 \\end{pmatrix}$$\n\nThe Jacobi iteration matrix is defined as $T_J = D^{-1}(L+U)$.\nFirst, we find the inverse of the diagonal matrix $D$:\n$$D^{-1} = \\begin{pmatrix} 1/2  0 \\\\ 0  1/2 \\end{pmatrix}$$\nNext, we compute the sum of $L$ and $U$:\n$$L+U = \\begin{pmatrix} 0  0 \\\\ -4  0 \\end{pmatrix} + \\begin{pmatrix} 0  -3 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 0  -3 \\\\ -4  0 \\end{pmatrix}$$\nNow, we can compute the iteration matrix $T_J$:\n$$T_J = D^{-1}(L+U) = \\begin{pmatrix} 1/2  0 \\\\ 0  1/2 \\end{pmatrix} \\begin{pmatrix} 0  -3 \\\\ -4  0 \\end{pmatrix} = \\begin{pmatrix} (1/2)(0) + (0)(-4)  (1/2)(-3) + (0)(0) \\\\ (0)(0) + (1/2)(-4)  (0)(-3) + (1/2)(0) \\end{pmatrix}$$\n$$T_J = \\begin{pmatrix} 0  -3/2 \\\\ -2  0 \\end{pmatrix}$$\n\nTo find the eigenvalues of $T_J$, we solve the characteristic equation $\\det(T_J - \\lambda I) = 0$, where $I$ is the identity matrix.\n$$T_J - \\lambda I = \\begin{pmatrix} 0  -3/2 \\\\ -2  0 \\end{pmatrix} - \\lambda \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} -\\lambda  -3/2 \\\\ -2  -\\lambda \\end{pmatrix}$$\nThe determinant is:\n$$\\det(T_J - \\lambda I) = (-\\lambda)(-\\lambda) - (-3/2)(-2) = \\lambda^2 - 3$$\nSetting the determinant to zero gives the characteristic equation:\n$$\\lambda^2 - 3 = 0$$\nSolving for $\\lambda$, we get:\n$$\\lambda^2 = 3$$\n$$\\lambda = \\pm\\sqrt{3}$$\nThe two eigenvalues are $\\sqrt{3}$ and $-\\sqrt{3}$.\n\nThe problem asks for the eigenvalues to be presented in a row matrix in ascending order.\nThe ascending order is $-\\sqrt{3}$, then $\\sqrt{3}$.\nThe spectral radius of the iteration matrix is $\\rho(T_J) = \\max(|\\sqrt{3}|, |-\\sqrt{3}|) = \\sqrt{3} \\approx 1.732$. Since $\\rho(T_J) > 1$, the Jacobi method will diverge for this system.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-\\sqrt{3}  \\sqrt{3}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The Conjugate Gradient (CG) method is a cornerstone for solving large, symmetric positive-definite linear systems, but its power comes with strict prerequisites. This practice challenges you to explore the theoretical boundaries of CG by applying it to an indefinite system. By deriving the step-length formula from first principles, you will pinpoint exactly why the algorithm breaks down and gain a deeper appreciation for its mathematical foundations .",
            "id": "3245204",
            "problem": "Consider applying the Conjugate Gradient (CG) method to the linear system $A x = b$ where $A$ is symmetric but not positive definite. The CG method is classically defined by minimizing the quadratic functional $\\phi(x) = \\tfrac{1}{2} x^{\\mathsf{T}} A x - b^{\\mathsf{T}} x$ along search directions $p_k$, with residuals $r_k = b - A x_k$, and the initial search direction $p_0 = r_0$. From first principles, the step length $\\alpha_k$ is determined by minimizing $\\phi(x_k + \\alpha p_k)$ with respect to the scalar $\\alpha$.\n\nWork with the explicit example\n$$\nA = \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix}, \n\\quad \nb = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \n\\quad \nx_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.\n$$\nStarting from the definition of $\\phi(x)$ and the directional derivative condition for optimality along $p_k$, derive the expression for the step length $\\alpha_0$ in terms of $p_0$ and $r_0$ and identify the denominator that must be strictly positive when $A$ is positive definite. Then, compute the scalar $p_0^{\\mathsf{T}} A p_0$ for the given $A$, $b$, and $x_0$, and use this value to explain whether the CG step is well-defined in this case and why the method breaks down for this non-positive-definite $A$.\n\nYour final reported answer should be the numerical value of $p_0^{\\mathsf{T}} A p_0$. No rounding is required.",
            "solution": "The problem asks for an analysis of the Conjugate Gradient (CG) method applied to a linear system $A x = b$ where the matrix $A$ is symmetric but not positive definite. We will first derive the general formula for the step length $\\alpha_k$ from first principles, then apply it to the specific case provided to demonstrate the method's breakdown.\n\nThe CG method aims to find the solution $x$ by minimizing the quadratic functional $\\phi(x) = \\frac{1}{2} x^{\\mathsf{T}} A x - b^{\\mathsf{T}} x$. The gradient of this functional is $\\nabla \\phi(x) = A x - b$, which is the negative of the residual, $r = b - A x$. The method proceeds iteratively by generating a sequence of approximations $x_{k+1} = x_k + \\alpha_k p_k$, where $p_k$ is a search direction and $\\alpha_k$ is a step length chosen to minimize $\\phi(x_k + \\alpha_k p_k)$.\n\nTo derive $\\alpha_k$, we define a function of a single variable $\\alpha$:\n$$f(\\alpha) = \\phi(x_k + \\alpha p_k) = \\frac{1}{2} (x_k + \\alpha p_k)^{\\mathsf{T}} A (x_k + \\alpha p_k) - b^{\\mathsf{T}} (x_k + \\alpha p_k)$$\nExpanding the terms, we get:\n$$f(\\alpha) = \\frac{1}{2} (x_k^{\\mathsf{T}} A x_k + \\alpha x_k^{\\mathsf{T}} A p_k + \\alpha p_k^{\\mathsf{T}} A x_k + \\alpha^2 p_k^{\\mathsf{T}} A p_k) - b^{\\mathsf{T}} x_k - \\alpha b^{\\mathsf{T}} p_k$$\nSince $A$ is symmetric, $A = A^{\\mathsf{T}}$, the scalar quantity $p_k^{\\mathsf{T}} A x_k = (x_k^{\\mathsf{T}} A^{\\mathsf{T}} p_k)^{\\mathsf{T}} = (x_k^{\\mathsf{T}} A p_k)^{\\mathsf{T}} = x_k^{\\mathsf{T}} A p_k$. We can group terms by powers of $\\alpha$:\n$$f(\\alpha) = \\left(\\frac{1}{2} x_k^{\\mathsf{T}} A x_k - b^{\\mathsf{T}} x_k\\right) + \\alpha (x_k^{\\mathsf{T}} A p_k - b^{\\mathsf{T}} p_k) + \\frac{1}{2} \\alpha^2 (p_k^{\\mathsf{T}} A p_k)$$\nThe first term is $\\phi(x_k)$. The term linear in $\\alpha$ can be rewritten using the residual $r_k = b - A x_k$.\n$$x_k^{\\mathsf{T}} A p_k - b^{\\mathsf{T}} p_k = -(b - A x_k)^{\\mathsf{T}} p_k = -r_k^{\\mathsf{T}} p_k$$\nSo, the function to minimize is:\n$$f(\\alpha) = \\phi(x_k) - \\alpha r_k^{\\mathsf{T}} p_k + \\frac{1}{2} \\alpha^2 p_k^{\\mathsf{T}} A p_k$$\nTo find the minimum, we take the derivative with respect to $\\alpha$ and set it to zero, as dictated by the \"directional derivative condition for optimality\":\n$$\\frac{df}{d\\alpha} = -r_k^{\\mathsf{T}} p_k + \\alpha p_k^{\\mathsf{T}} A p_k = 0$$\nSolving for $\\alpha$ yields the step length $\\alpha_k$:\n$$\\alpha_k = \\frac{r_k^{\\mathsf{T}} p_k}{p_k^{\\mathsf{T}} A p_k}$$\nFor the first iteration ($k=0$), where the search direction $p_0$ is set to be the initial residual $r_0$, this becomes:\n$$\\alpha_0 = \\frac{r_0^{\\mathsf{T}} r_0}{r_0^{\\mathsf{T}} A r_0}$$\nThe denominator in this expression is $p_k^{\\mathsf{T}} A p_k$. For the CG method to be well-defined and for the step to correspond to a minimum of $\\phi$ along the search direction, the second derivative of $f(\\alpha)$ must be positive:\n$$\\frac{d^2f}{d\\alpha^2} = p_k^{\\mathsf{T}} A p_k > 0$$\nIf $A$ is symmetric positive definite (SPD), then by definition, $v^{\\mathsf{T}} A v > 0$ for any non-zero vector $v$. Since the search directions $p_k$ generated by the CG algorithm are non-zero (unless the exact solution has been found), the denominator $p_k^{\\mathsf{T}} A p_k$ is guaranteed to be strictly positive when $A$ is SPD.\n\nNow we analyze the specific case provided:\n$$A = \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\nThe matrix $A$ is symmetric, but its eigenvalues are $1$ and $-1$, so it is not positive definite.\nFirst, we compute the initial residual $r_0$:\n$$r_0 = b - A x_0 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\nThe initial search direction is $p_0 = r_0$, so $p_0 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\nThe problem asks to compute the scalar $p_0^{\\mathsf{T}} A p_0$, which is the denominator required to find the step length $\\alpha_0$.\n$$p_0^{\\mathsf{T}} A p_0 = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\nFirst, we compute the product $A p_0$:\n$$A p_0 = \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 0 \\cdot 1 \\\\ 0 \\cdot 1 + (-1) \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$$\nNow we compute the final inner product:\n$$p_0^{\\mathsf{T}} (A p_0) = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = (1)(1) + (1)(-1) = 1 - 1 = 0$$\nThus, the value of the denominator is $p_0^{\\mathsf{T}} A p_0 = 0$.\n\nThe CG step is not well-defined because calculating the step length $\\alpha_0$ requires division by this quantity. The numerator is $r_0^{\\mathsf{T}} p_0 = r_0^{\\mathsf{T}} r_0 = 1^2 + 1^2 = 2$. Therefore, $\\alpha_0 = \\frac{2}{0}$, which is undefined. The algorithm breaks down at the first iteration.\n\nThis breakdown occurs because the condition $p_0^{\\mathsf{T}} A p_0 > 0$ is violated. When $p_0^{\\mathsf{T}} A p_0 = 0$, the function $f(\\alpha) = \\phi(x_0 + \\alpha p_0)$ is no longer a convex quadratic. It becomes a linear function:\n$$f(\\alpha) = \\phi(x_0) - \\alpha (r_0^{\\mathsf{T}} p_0) = \\phi(x_0) - 2\\alpha$$\nA non-constant linear function has no minimum on the real line. The CG method fails because its core assumption—that it can find a minimum of the quadratic functional along the search direction—is not met. This demonstrates a fundamental reason why the standard CG method is restricted to systems with symmetric positive definite matrices.\nThe numerical value of $p_0^{\\mathsf{T}} A p_0$ is $0$.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "In computational fluid dynamics, linear systems often exhibit a challenging saddle-point structure that demands sophisticated solution strategies. This problem delves into the practical and essential technique of block preconditioning for such systems. You will explore how regularization stabilizes the solution process and perform a concrete calculation to design a preconditioner, bridging the gap between numerical theory and its application in fluid simulations .",
            "id": "4091477",
            "problem": "Consider the linear system arising from a discretized quasi-steady momentum balance in an incompressible complex fluid, written in $2 \\times 2$ saddle-point block form\n$$\n\\begin{pmatrix}\nA  B^{\\top} \\\\\nB  -C\n\\end{pmatrix}\n\\begin{pmatrix}\n\\mathbf{u} \\\\\np\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\mathbf{f} \\\\\ng\n\\end{pmatrix},\n$$\nwhere $A \\in \\mathbb{R}^{n \\times n}$ represents a symmetric positive definite velocity block dominated by viscous diffusion, $B \\in \\mathbb{R}^{m \\times n}$ represents the discrete divergence, and $C \\in \\mathbb{R}^{m \\times m}$ represents any pressure stabilization (for an exactly divergence-free discretization, one may take $C = 0$). You are asked to construct a block Incomplete Lower–Upper (ILU) preconditioner and explain how block pivoting addresses near-singularity in the pressure Schur complement.\n\nStart from the fundamental block factorization principle for saddle-point systems and derive a block ILU(0) preconditioner that retains the block structure while using an incomplete Schur complement. In particular, consider a block pivoting strategy that replaces the block $A$ by a regularized block $A_{p} = A + \\epsilon I$, with $\\epsilon  0$ chosen to avoid division by a near-singular $A$ in the formation of the Schur complement. Explain, from first principles, how this block pivoting modifies the pressure Schur complement and why it mitigates near-singularity associated with the pressure null space.\n\nTo make this concrete, analyze a single-mode reduction in which the blocks are scalars extracted from Fourier analysis of a uniform grid discretization: take $A = a$, $B = b$, $C = c$, where $a = \\nu k^{2}$ with viscosity $ \\nu$, scalar wavenumber magnitude $k$, $b = k$, and $c = 0$. In this scalar setting, the block ILU preconditioner uses the incomplete Schur complement\n$$\n\\widehat{S} = -c - \\frac{b^{2}}{a_{p}} = - \\frac{k^{2}}{\\nu k^{2} + \\epsilon}.\n$$\nSuppose one wishes to achieve a target stabilized pressure Schur complement $\\widehat{S} = -\\sigma$ with $\\sigma = \\eta k^{2}$ for a given stabilization parameter $\\eta  0$ tied to a pressure mass scaling. Using $ \\nu = 1.0 \\times 10^{-3}$, $k = 10$, and $\\eta = 8.0$, determine the value of the block pivoting parameter $\\epsilon$ that makes $\\widehat{S} = -\\eta k^{2}$ hold exactly in this scalar-mode approximation.\n\nRound your answer for $\\epsilon$ to four significant figures. Express the final value of $\\epsilon$ as a dimensionless quantity.",
            "solution": "The problem asks us to analyze a block preconditioning strategy for a saddle-point system. First, let's review the underlying principles. The exact block LU factorization of the system matrix involves the pressure Schur complement, $S = -C - B A^{-1} B^{\\top}$. In practice, computing $A^{-1}$ is prohibitively expensive, so preconditioners use an approximation.\n\nThe problem proposes a \"block pivoting\" strategy, which regularizes the $A$ block to $A_p = A + \\epsilon I$ with $\\epsilon > 0$. This serves two key purposes. First, the original diffusion block $A$ can be singular or ill-conditioned (e.g., for zero-wavenumber modes). Adding $\\epsilon I$ makes $A_p$ strictly positive definite and well-conditioned, ensuring its inverse is stable to compute. Second, this regularization of $A$ propagates to the Schur complement. The resulting incomplete Schur complement is $\\widehat{S} = -C - B A_p^{-1} B^{\\top}$. This process regularizes $\\widehat{S}$ itself, mitigating issues related to the singularity of the pressure system (which is often only defined up to a constant), making it more amenable to iterative solvers.\n\nNow, we analyze the concrete scalar-mode reduction from Fourier analysis. We are given the scalar analogues:\n$A \\to a = \\nu k^{2}$\n$B \\to b = k$\n$C \\to c = 0$\n\nThe regularized scalar block is $a_p = a + \\epsilon = \\nu k^{2} + \\epsilon$. The incomplete Schur complement is defined as:\n$$\n\\widehat{S} = -c - \\frac{b^{2}}{a_{p}} = -0 - \\frac{k^{2}}{\\nu k^{2} + \\epsilon} = -\\frac{k^{2}}{\\nu k^{2} + \\epsilon}\n$$\nWe need to find the value of $\\epsilon$ that makes this match a target stabilized Schur complement $\\widehat{S} = -\\sigma$, where $\\sigma = \\eta k^{2}$.\n\nSetting the two expressions for $\\widehat{S}$ equal:\n$$\n-\\frac{k^{2}}{\\nu k^{2} + \\epsilon} = -\\eta k^{2}\n$$\nSince the wavenumber $k = 10 \\neq 0$, we can divide both sides by $-k^2$:\n$$\n\\frac{1}{\\nu k^{2} + \\epsilon} = \\eta\n$$\nWe now solve this algebraic equation for $\\epsilon$:\n$$\n1 = \\eta (\\nu k^{2} + \\epsilon)\n$$\n$$\n\\frac{1}{\\eta} = \\nu k^{2} + \\epsilon\n$$\n$$\n\\epsilon = \\frac{1}{\\eta} - \\nu k^{2}\n$$\nThis gives the symbolic expression for the required block pivoting parameter. We now substitute the given numerical values:\n$\\nu = 1.0 \\times 10^{-3}$\n$k = 10$\n$\\eta = 8.0$\n\nPlugging these values into the expression for $\\epsilon$:\n$$\n\\epsilon = \\frac{1}{8.0} - (1.0 \\times 10^{-3}) (10)^2\n$$\n$$\n\\epsilon = 0.125 - (0.001)(100)\n$$\n$$\n\\epsilon = 0.125 - 0.1\n$$\n$$\n\\epsilon = 0.025\n$$\nThe problem asks for the answer to be rounded to four significant figures. As a dimensionless quantity, this is $0.02500$ or, in scientific notation, $2.500 \\times 10^{-2}$.",
            "answer": "$$\n\\boxed{2.500 \\times 10^{-2}}\n$$"
        }
    ]
}