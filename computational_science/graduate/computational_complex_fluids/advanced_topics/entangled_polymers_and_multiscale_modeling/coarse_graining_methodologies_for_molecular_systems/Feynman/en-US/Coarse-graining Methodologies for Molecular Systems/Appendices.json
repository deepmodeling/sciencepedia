{
    "hands_on_practices": [
        {
            "introduction": "To build robust coarse-grained (CG) models, we must first understand how eliminating fast atomic-scale degrees of freedom impacts the dynamics of the remaining slow variables. This exercise provides a foundational derivation, starting from a microscopic Hamiltonian, to reveal the origin of memory effects and stochastic forces in CG systems . By formally integrating out a harmonic bath, you will derive the Generalized Langevin Equation (GLE), a cornerstone of non-equilibrium statistical mechanics, and verify the fundamental Fluctuation-Dissipation Theorem that links friction to random fluctuations.",
            "id": "4081629",
            "problem": "Consider a coarse-grained (CG) coordinate $X(t)$ of mass $M$ evolving under a smooth potential $U(X)$ and coupled linearly to a harmonic bath that models unresolved molecular degrees of freedom. The bath consists of independent oscillators with coordinates $q_j$, masses $m_j$, frequencies $\\omega_j$, and couplings $c_j$. The total Hamiltonian is\n$$\nH = \\frac{P^{2}}{2M} + U(X) + \\sum_{j} \\left[ \\frac{p_j^{2}}{2 m_j} + \\frac{1}{2} m_j \\omega_j^{2} \\left( q_j - \\frac{c_j}{m_j \\omega_j^{2}} X \\right)^{2} \\right],\n$$\nwhich includes the standard counter-term that preserves the bare potential $U(X)$ upon eliminating the bath. Assume the bath is initially at canonical equilibrium at temperature $T$, sampled independently of the initial CG coordinate and momentum, with Boltzmann constant $k_B$.\n\nStarting from Hamiltonian dynamics and the fundamental definitions of equilibrium averages, eliminate the bath degrees of freedom to obtain the Generalized Langevin Equation (GLE) for $X(t)$ of the form\n$$\nM \\ddot{X}(t) + U'(X(t)) + \\int_{0}^{t} K(t-s)\\, \\dot{X}(s)\\, ds = R(t),\n$$\nwhere $K(t)$ is the memory kernel and $R(t)$ is the random force determined by the bath initial conditions. Then, model the bath in the continuum limit via a spectral density $J(\\omega)$ defined by\n$$\nJ(\\omega) \\equiv \\frac{\\pi}{2} \\sum_{j} \\frac{c_j^{2}}{m_j \\omega_j} \\, \\delta(\\omega - \\omega_j),\n$$\nand take the Drude–Lorentz form\n$$\nJ(\\omega) = \\gamma M \\, \\omega \\, \\frac{\\omega_c^{2}}{\\omega^{2} + \\omega_c^{2}},\n$$\nwith friction scale $\\gamma > 0$ and cutoff frequency $\\omega_c > 0$. Compute the resulting memory kernel $K(t)$ in closed analytic form. Using the canonical equilibria of the harmonic bath, verify the analytic Fluctuation–Dissipation Theorem (FDT), namely that the random force autocorrelation satisfies\n$$\n\\langle R(t) R(0) \\rangle = k_B T \\, K(t).\n$$\n\nAnswer specification:\n- Report the dimensionless memory kernel $\\tilde{K}(t) \\equiv K(t)/(M \\gamma \\omega_c)$ as your final answer.\n- No numerical evaluation is required; provide the exact analytic expression.",
            "solution": "The problem requires the derivation of the Generalized Langevin Equation (GLE) for a coarse-grained (CG) coordinate $X(t)$ coupled to a harmonic bath, the calculation of the memory kernel $K(t)$ for a specific spectral density, and the verification of the Fluctuation-Dissipation Theorem (FDT).\n\n### Derivation of the GLE\n\nWe start by deriving the equations of motion from the given Hamiltonian using Hamilton's equations.\nThe equation of motion for the CG coordinate $X$ is:\n$M\\ddot{X} = -\\frac{\\partial H}{\\partial X} = -U'(X) - \\frac{\\partial}{\\partial X} \\sum_{j} \\frac{1}{2} m_j \\omega_j^{2} \\left( q_j - \\frac{c_j}{m_j \\omega_j^{2}} X \\right)^{2}$\n$M\\ddot{X} = -U'(X) - \\sum_{j} m_j \\omega_j^{2} \\left( q_j - \\frac{c_j}{m_j \\omega_j^{2}} X \\right) \\left(-\\frac{c_j}{m_j \\omega_j^{2}}\\right)$\n$$M\\ddot{X}(t) = -U'(X(t)) + \\sum_{j} c_j \\left( q_j(t) - \\frac{c_j}{m_j \\omega_j^{2}} X(t) \\right) \\quad (*)$$\nThe equation of motion for each bath oscillator coordinate $q_j$ is:\n$m_j\\ddot{q}_j = -\\frac{\\partial H}{\\partial q_j} = -m_j \\omega_j^{2} \\left( q_j - \\frac{c_j}{m_j \\omega_j^{2}} X \\right)$\n$$\\ddot{q}_j(t) + \\omega_j^{2} q_j(t) = \\frac{c_j}{m_j} X(t)$$\nThis is the equation for a forced harmonic oscillator. Its general solution is the sum of the homogeneous solution and a particular solution obtained via the Green's function method:\n$$q_j(t) = q_j(0) \\cos(\\omega_j t) + \\frac{\\dot{q}_j(0)}{\\omega_j} \\sin(\\omega_j t) + \\int_{0}^{t} \\frac{\\sin(\\omega_j (t-s))}{\\omega_j} \\frac{c_j}{m_j} X(s) ds$$\nwhere $\\dot{q}_j(0) = p_j(0)/m_j$.\n\nWe substitute this solution for $q_j(t)$ back into the equation of motion for $X(t)$, equation $(*)$.\n$M\\ddot{X} + U'(X) = \\sum_{j} c_j q_j(t) - \\sum_{j} \\frac{c_j^2}{m_j \\omega_j^2} X(t)$\nSubstituting the expression for $q_j(t)$:\n$M\\ddot{X} + U'(X) + \\left(\\sum_{j} \\frac{c_j^2}{m_j \\omega_j^2}\\right) X(t) = \\sum_{j} c_j \\left[ q_j(0)\\cos(\\omega_j t) + \\frac{p_j(0)}{m_j\\omega_j}\\sin(\\omega_j t) + \\frac{c_j}{m_j\\omega_j} \\int_{0}^{t} \\sin(\\omega_j(t-s)) X(s) ds \\right]$\nWe focus on the integral term. We perform integration by parts:\n$\\int_{0}^{t} \\sin(\\omega_j(t-s)) X(s) ds = \\left[ X(s) \\frac{\\cos(\\omega_j(t-s))}{\\omega_j} \\right]_0^t - \\int_{0}^{t} \\frac{\\cos(\\omega_j(t-s))}{\\omega_j} \\dot{X}(s) ds$\n$= \\frac{X(t)}{\\omega_j} - \\frac{X(0)\\cos(\\omega_j t)}{\\omega_j} - \\frac{1}{\\omega_j} \\int_{0}^{t} \\cos(\\omega_j(t-s)) \\dot{X}(s) ds$\nSubstituting this back into the equation for $X$:\n$\\sum_{j} \\frac{c_j^2}{m_j} \\int_{0}^{t} \\frac{\\sin(\\omega_j(t-s))}{\\omega_j} X(s) ds = \\sum_{j} \\frac{c_j^2}{m_j\\omega_j^2} \\left[ X(t) - X(0)\\cos(\\omega_j t) - \\int_{0}^{t} \\cos(\\omega_j(t-s)) \\dot{X}(s) ds \\right]$\n$= \\left(\\sum_j \\frac{c_j^2}{m_j \\omega_j^2}\\right) X(t) - X(0) \\sum_j \\frac{c_j^2}{m_j \\omega_j^2} \\cos(\\omega_j t) - \\int_0^t \\left(\\sum_j \\frac{c_j^2}{m_j \\omega_j^2} \\cos(\\omega_j(t-s))\\right) \\dot{X}(s) ds$\nThe full equation for $X$ becomes:\n$M\\ddot{X} + U'(X) + \\left(\\sum_{j} \\frac{c_j^2}{m_j \\omega_j^2}\\right) X(t) = \\sum_{j} c_j \\left[ q_j(0)\\cos(\\omega_j t) + \\frac{p_j(0)}{m_j\\omega_j}\\sin(\\omega_j t) \\right] + \\left(\\sum_j \\frac{c_j^2}{m_j \\omega_j^2}\\right) X(t) - X(0) \\sum_j \\frac{c_j^2}{m_j \\omega_j^2} \\cos(\\omega_j t) - \\int_0^t \\left(\\sum_j \\frac{c_j^2}{m_j \\omega_j^2} \\cos(\\omega_j(t-s))\\right) \\dot{X}(s) ds$\nThe term $\\left(\\sum_{j} \\frac{c_j^2}{m_j \\omega_j^2}\\right) X(t)$ cancels on both sides, which highlights the role of the counter-term in the Hamiltonian. Rearranging the remaining terms to match the GLE form:\n$$M\\ddot{X}(t) + U'(X(t)) + \\int_0^t K(t-s) \\dot{X}(s) ds = R(t)$$\nwhere we identify the memory kernel $K(t)$ and the random force $R(t)$ as:\n$$K(t) = \\sum_{j} \\frac{c_j^2}{m_j \\omega_j^2} \\cos(\\omega_j t)$$\n$$R(t) = \\sum_{j} \\left[ c_j \\left(q_j(0) - \\frac{c_j X(0)}{m_j\\omega_j^2}\\right) \\cos(\\omega_j t) + \\frac{c_j p_j(0)}{m_j \\omega_j} \\sin(\\omega_j t) \\right]$$\n\n### Calculation of the Memory Kernel\n\nWe now convert the sum for $K(t)$ into an integral using the spectral density $J(\\omega)$. The relationship is derived as follows:\n$\\frac{2}{\\pi} \\int_0^\\infty \\frac{J(\\omega)}{\\omega} \\cos(\\omega t) d\\omega = \\frac{2}{\\pi} \\int_0^\\infty \\left( \\frac{\\pi}{2} \\sum_j \\frac{c_j^2}{m_j \\omega_j} \\delta(\\omega-\\omega_j) \\right) \\frac{\\cos(\\omega t)}{\\omega} d\\omega$\n$= \\sum_j \\frac{c_j^2}{m_j \\omega_j} \\int_0^\\infty \\frac{\\cos(\\omega t)}{\\omega} \\delta(\\omega-\\omega_j) d\\omega = \\sum_j \\frac{c_j^2}{m_j \\omega_j} \\frac{\\cos(\\omega_j t)}{\\omega_j} = K(t)$.\nSo, we have the general formula:\n$$K(t) = \\frac{2}{\\pi} \\int_0^\\infty \\frac{J(\\omega)}{\\omega} \\cos(\\omega t) d\\omega$$\nUsing the given Drude-Lorentz spectral density $J(\\omega) = \\gamma M \\omega \\frac{\\omega_c^2}{\\omega^2 + \\omega_c^2}$:\n$K(t) = \\frac{2}{\\pi} \\int_0^\\infty \\frac{1}{\\omega} \\left( \\gamma M \\omega \\frac{\\omega_c^2}{\\omega^2 + \\omega_c^2} \\right) \\cos(\\omega t) d\\omega = \\frac{2 \\gamma M \\omega_c^2}{\\pi} \\int_0^\\infty \\frac{\\cos(\\omega t)}{\\omega^2 + \\omega_c^2} d\\omega$.\nThis is a standard integral. For $t \\ge 0$, it can be evaluated using complex analysis. Considering the integral $\\frac{1}{2} \\int_{-\\infty}^\\infty \\frac{e^{i\\omega t}}{\\omega^2 + \\omega_c^2} d\\omega$ and closing the contour in the upper half-plane (for $t > 0$), we encircle the simple pole at $\\omega = i\\omega_c$.\n$\\int_0^\\infty \\frac{\\cos(\\omega t)}{\\omega^2 + \\omega_c^2} d\\omega = \\frac{1}{2} \\text{Re} \\left[ \\oint \\frac{e^{i\\omega t}}{\\omega^2 + \\omega_c^2} d\\omega \\right] = \\frac{1}{2} \\text{Re} \\left[ 2\\pi i \\, \\text{Res}_{\\omega=i\\omega_c} \\frac{e^{i\\omega t}}{(\\omega - i\\omega_c)(\\omega + i\\omega_c)} \\right]$\n$= \\frac{1}{2} \\text{Re} \\left[ 2\\pi i \\, \\frac{e^{i(i\\omega_c)t}}{2i\\omega_c} \\right] = \\frac{1}{2} \\text{Re} \\left[ \\frac{\\pi}{\\omega_c} e^{-\\omega_c t} \\right] = \\frac{\\pi}{2\\omega_c} e^{-\\omega_c t}$.\nSubstituting this result back into the expression for $K(t)$:\n$$K(t) = \\frac{2 \\gamma M \\omega_c^2}{\\pi} \\left( \\frac{\\pi}{2\\omega_c} e^{-\\omega_c t} \\right) = M \\gamma \\omega_c e^{-\\omega_c t}$$\n\n### Verification of the Fluctuation-Dissipation Theorem\n\nThe FDT states $\\langle R(t) R(0) \\rangle = k_B T K(t)$. We compute the correlation function of the random force.\n$R(0) = \\sum_k c_k \\left(q_k(0) - \\frac{c_k X(0)}{m_k\\omega_k^2}\\right)$. Let's introduce the shifted coordinates $q'_j(0) = q_j(0) - \\frac{c_j X(0)}{m_j\\omega_j^2}$. Then\n$R(t) = \\sum_j \\left( c_j q'_j(0)\\cos(\\omega_j t) + \\frac{c_j p_j(0)}{m_j \\omega_j} \\sin(\\omega_j t) \\right)$\n$R(0) = \\sum_k c_k q'_k(0)$.\nThe correlation function is $\\langle R(t) R(0) \\rangle = \\left\\langle \\left( \\sum_j c_j q'_j(0)\\cos(\\omega_j t) + \\frac{c_j p_j(0)}{m_j \\omega_j} \\sin(\\omega_j t) \\right) \\left( \\sum_k c_k q'_k(0) \\right) \\right\\rangle$.\nThe bath is in canonical equilibrium, described by the bath part of the Hamiltonian, which in terms of $q'_j$ and $p_j$ is $H_{\\text{bath}} = \\sum_j \\left( \\frac{p_j^2}{2m_j} + \\frac{1}{2}m_j\\omega_j^2 (q'_j)^2 \\right)$.\nThe equilibrium averages are:\n$\\langle q'_j(0) \\rangle = 0$, $\\langle p_j(0) \\rangle = 0$.\n$\\langle q'_j(0) p_k(0) \\rangle = 0$ for all $j, k$.\n$\\langle q'_j(0) q'_k(0) \\rangle = \\delta_{jk} \\langle (q'_j)^2 \\rangle$.\nFrom the equipartition theorem, $\\frac{1}{2} m_j \\omega_j^2 \\langle (q'_j)^2 \\rangle = \\frac{1}{2} k_B T$, which gives $\\langle (q'_j)^2 \\rangle = \\frac{k_B T}{m_j \\omega_j^2}$.\nUsing these properties, the correlation function becomes:\n$\\langle R(t) R(0) \\rangle = \\sum_{j,k} c_j c_k \\cos(\\omega_j t) \\langle q'_j(0) q'_k(0) \\rangle = \\sum_j c_j^2 \\cos(\\omega_j t) \\langle (q'_j)^2 \\rangle$\n$\\langle R(t) R(0) \\rangle = \\sum_j c_j^2 \\cos(\\omega_j t) \\frac{k_B T}{m_j \\omega_j^2} = k_B T \\left( \\sum_j \\frac{c_j^2}{m_j \\omega_j^2} \\cos(\\omega_j t) \\right)$\nThe term in the parenthesis is precisely our expression for the memory kernel $K(t)$.\nThus, we have verified the Fluctuation-Dissipation Theorem:\n$$\\langle R(t) R(0) \\rangle = k_B T K(t)$$\n\n### Final Answer Formulation\n\nThe problem asks for the dimensionless memory kernel $\\tilde{K}(t) \\equiv K(t)/(M \\gamma \\omega_c)$.\nUsing our result $K(t) = M \\gamma \\omega_c e^{-\\omega_c t}$, we find:\n$$\\tilde{K}(t) = \\frac{M \\gamma \\omega_c e^{-\\omega_c t}}{M \\gamma \\omega_c} = e^{-\\omega_c t}$$",
            "answer": "$$\\boxed{\\exp(-\\omega_c t)}$$"
        },
        {
            "introduction": "Many bottom-up coarse-graining methods rely on optimizing an effective potential to reproduce structural properties from a more detailed simulation, a process often called \"force matching\" or structure-based coarse-graining. This practice provides a hands-on introduction to the optimization machinery at the heart of these techniques . You will parameterize a potential using a flexible basis set and use a gradient-descent algorithm to tune the parameters, minimizing the difference between your model's radial distribution function and a target, thereby illustrating a core workflow in modern force field development.",
            "id": "4081602",
            "problem": "Consider a molecular system in the canonical ensemble with inverse temperature $\\beta$. In the low-density limit, the radial distribution function (RDF) $g(r)$ for a pairwise interaction potential $u(r)$ can be approximated by the Boltzmann factor $g(r) \\approx \\exp(-\\beta u(r))$. In coarse-graining, suppose the pair potential is expressed as a linear expansion in a set of radial basis functions $\\{\\phi_k(r)\\}_{k=1}^{K}$ with coefficients $\\theta_k$, namely\n$$\nu(r; \\boldsymbol{\\theta}) = \\sum_{k=1}^{K} \\theta_k \\, \\phi_k(r).\n$$\nLet the objective function be the weighted least-squares misfit between the model RDF $g(r_i;\\boldsymbol{\\theta})$ and a target RDF $g_{\\mathrm{target}}(r_i)$ over a discrete grid $\\{r_i\\}_{i=1}^{M}$,\n$$\nJ(\\boldsymbol{\\theta}) = \\frac{1}{2}\\sum_{i=1}^{M} w_i \\left[g(r_i;\\boldsymbol{\\theta}) - g_{\\mathrm{target}}(r_i)\\right]^2,\n$$\nwhere $w_i$ are nonnegative weights. Using the low-density approximation $g(r;\\boldsymbol{\\theta}) = \\exp\\left(-\\beta \\sum_{k=1}^{K} \\theta_k \\phi_k(r)\\right)$ and a linear-response-based sensitivity for $g(r)$ with respect to $\\theta_k$, derive the gradient components $\\partial J / \\partial \\theta_k$ and implement a single gradient-descent update\n$$\n\\theta_k^{\\mathrm{new}} = \\theta_k^{\\mathrm{old}} - \\alpha \\, \\frac{\\partial J}{\\partial \\theta_k},\n$$\nwhere $\\alpha$ is the learning rate.\n\nAll distances $r$, basis parameters, and energies are to be treated in a consistent reduced unit system where all quantities are dimensionless. The inverse temperature $\\beta$ is dimensionless, and the potential $u(r)$ is dimensionless. Angles do not appear in this problem.\n\nFor numerical implementation, use Gaussian radial basis functions defined by\n$$\n\\phi_k(r) = \\exp\\left(-\\frac{(r - c_k)^2}{2\\sigma_k^2}\\right),\n$$\nwhere $c_k$ and $\\sigma_k$ are the center and width of the $k$-th basis function, respectively.\n\nYour program must:\n- For each test case, compute the model $g(r_i;\\boldsymbol{\\theta})$, the gradient $\\partial J / \\partial \\theta_k$, and the updated parameters $\\boldsymbol{\\theta}^{\\mathrm{new}}$.\n- Produce a single line of output containing the updated parameter vectors for all test cases, formatted as a comma-separated list enclosed in square brackets, with each test case’s updated parameter vector represented as a Python-style list and each parameter rounded to six decimal places (e.g., \"[[0.123456,-0.234567],[...],...]\").\n\nTest Suite:\n- Case 1 (General scenario):\n  - $\\beta = 1.0$.\n  - $K = 3$, centers $\\boldsymbol{c} = [0.8, 1.4, 2.0]$, widths $\\boldsymbol{\\sigma} = [0.15, 0.25, 0.20]$.\n  - Initial parameters $\\boldsymbol{\\theta}^{\\mathrm{old}} = [1.0, -0.5, 0.2]$.\n  - Grid points $\\boldsymbol{r} = [0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0]$.\n  - Weights $\\boldsymbol{w} = [1, 1, 1, 1, 1, 1, 1, 1]$.\n  - Target RDF defined analytically by\n    $$\n    g_{\\mathrm{target}}(r) = 1 - 0.4 \\exp\\left(-\\frac{(r - 1.2)^2}{0.25^2}\\right) + 0.1 \\exp\\left(-\\frac{(r - 1.8)^2}{0.1^2}\\right).\n    $$\n  - Learning rate $\\alpha = 0.05$.\n\n- Case 2 (Consistency check where model equals target initially):\n  - $\\beta = 1.5$.\n  - $K = 2$, centers $\\boldsymbol{c} = [1.0, 1.5]$, widths $\\boldsymbol{\\sigma} = [0.3, 0.25]$.\n  - True parameters $\\boldsymbol{\\theta}^{\\mathrm{true}} = [0.3, -0.1]$, and initial parameters $\\boldsymbol{\\theta}^{\\mathrm{old}} = \\boldsymbol{\\theta}^{\\mathrm{true}}$.\n  - Grid points $\\boldsymbol{r} = [0.8, 1.0, 1.2, 1.4, 1.6]$.\n  - Weights $\\boldsymbol{w} = [1, 1, 1, 1, 1]$.\n  - Target RDF defined by the model with true parameters:\n    $$\n    g_{\\mathrm{target}}(r) = \\exp\\left(-\\beta \\sum_{k=1}^{K} \\theta_k^{\\mathrm{true}} \\, \\phi_k(r)\\right).\n    $$\n  - Learning rate $\\alpha = 0.10$.\n\n- Case 3 (High-temperature limit):\n  - $\\beta = 0.1$.\n  - $K = 2$, centers $\\boldsymbol{c} = [1.0, 2.0]$, widths $\\boldsymbol{\\sigma} = [0.4, 0.4]$.\n  - Initial parameters $\\boldsymbol{\\theta}^{\\mathrm{old}} = [1.0, 1.0]$.\n  - Grid points $\\boldsymbol{r} = [0.5, 1.0, 1.5, 2.0]$.\n  - Weights $\\boldsymbol{w} = [1, 1, 1, 1]$.\n  - Target RDF is uniform:\n    $$\n    g_{\\mathrm{target}}(r) = 1.0.\n    $$\n  - Learning rate $\\alpha = 0.10$.\n\n- Case 4 (Single-point edge case):\n  - $\\beta = 2.0$.\n  - $K = 1$, center $\\boldsymbol{c} = [1.0]$, width $\\boldsymbol{\\sigma} = [0.2]$.\n  - Initial parameter $\\boldsymbol{\\theta}^{\\mathrm{old}} = [0.5]$.\n  - Single grid point $\\boldsymbol{r} = [1.0]$.\n  - Weight $\\boldsymbol{w} = [2.0]$.\n  - Target RDF specified:\n    $$\n    g_{\\mathrm{target}}(1.0) = 0.7.\n    $$\n  - Learning rate $\\alpha = 0.20$.\n\n- Case 5 (Zero-weight boundary case):\n  - $\\beta = 1.0$.\n  - $K = 2$, centers $\\boldsymbol{c} = [0.7, 1.3]$, widths $\\boldsymbol{\\sigma} = [0.2, 0.2]$.\n  - Initial parameters $\\boldsymbol{\\theta}^{\\mathrm{old}} = [-0.2, 0.4]$.\n  - Grid points $\\boldsymbol{r} = [0.6, 0.9, 1.2]$.\n  - Weights $\\boldsymbol{w} = [0.0, 0.0, 0.0]$.\n  - Target RDF defined analytically by\n    $$\n    g_{\\mathrm{target}}(r) = 1 - 0.3 \\exp\\left(-\\frac{(r - 1.0)^2}{0.2^2}\\right).\n    $$\n  - Learning rate $\\alpha = 0.10$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the updated parameter vectors for all provided test cases as a comma-separated list enclosed in square brackets. Each test case’s result must be a Python-style list of floats, with each float rounded to six decimal places. For example:\n\"[[theta_case1_1,theta_case1_2,...],[theta_case2_1,theta_case2_2,...],...]\".",
            "solution": "The objective is to derive the gradient of a least-squares objective function $J(\\boldsymbol{\\theta})$ and use it to perform a single gradient-descent update on the parameters $\\boldsymbol{\\theta}$. The problem is set in the context of coarse-graining, where a model potential $u(r; \\boldsymbol{\\theta})$ is optimized to reproduce a target radial distribution function $g_{\\mathrm{target}}(r)$.\n\nFirst, we restate the key definitions. The pair potential is parameterized as a linear combination of $K$ basis functions $\\{\\phi_k(r)\\}_{k=1}^{K}$:\n$$\nu(r; \\boldsymbol{\\theta}) = \\sum_{k=1}^{K} \\theta_k \\, \\phi_k(r)\n$$\nIn the low-density limit, the radial distribution function (RDF), $g(r)$, is approximated by the Boltzmann factor:\n$$\ng(r; \\boldsymbol{\\theta}) = \\exp(-\\beta u(r; \\boldsymbol{\\theta})) = \\exp\\left(-\\beta \\sum_{k=1}^{K} \\theta_k \\phi_k(r)\\right)\n$$\nwhere $\\beta$ is the inverse temperature.\n\nThe objective function $J(\\boldsymbol{\\theta})$ measures the weighted squared difference between the model RDF, $g(r_i; \\boldsymbol{\\theta})$, and a target RDF, $g_{\\mathrm{target}}(r_i)$, over a discrete set of $M$ radial points $\\{r_i\\}_{i=1}^{M}$:\n$$\nJ(\\boldsymbol{\\theta}) = \\frac{1}{2}\\sum_{i=1}^{M} w_i \\left[g(r_i;\\boldsymbol{\\theta}) - g_{\\mathrm{target}}(r_i)\\right]^2\n$$\nwhere $w_i \\ge 0$ are weights.\n\nOur primary task is to compute the gradient of $J(\\boldsymbol{\\theta})$ with respect to each parameter $\\theta_k$. We apply the chain rule to differentiate $J(\\boldsymbol{\\theta})$:\n$$\n\\frac{\\partial J}{\\partial \\theta_k} = \\frac{\\partial}{\\partial \\theta_k} \\left( \\frac{1}{2}\\sum_{i=1}^{M} w_i \\left[g(r_i;\\boldsymbol{\\theta}) - g_{\\mathrm{target}}(r_i)\\right]^2 \\right)\n$$\nThe derivative and summation operators can be interchanged:\n$$\n\\frac{\\partial J}{\\partial \\theta_k} = \\sum_{i=1}^{M} \\frac{1}{2} w_i \\cdot 2 \\left[g(r_i;\\boldsymbol{\\theta}) - g_{\\mathrm{target}}(r_i)\\right] \\cdot \\frac{\\partial g(r_i;\\boldsymbol{\\theta})}{\\partial \\theta_k}\n$$\n$$\n\\frac{\\partial J}{\\partial \\theta_k} = \\sum_{i=1}^{M} w_i \\left[g(r_i;\\boldsymbol{\\theta}) - g_{\\mathrm{target}}(r_i)\\right] \\frac{\\partial g(r_i;\\boldsymbol{\\theta})}{\\partial \\theta_k}\n$$\nThe next step is to find the partial derivative of the model RDF, $g(r_i; \\boldsymbol{\\theta})$, with respect to $\\theta_k$. This term represents the sensitivity of the RDF to a change in the $k$-th parameter. Using the chain rule again on the expression for $g(r_i; \\boldsymbol{\\theta})$:\n$$\n\\frac{\\partial g(r_i;\\boldsymbol{\\theta})}{\\partial \\theta_k} = \\frac{\\partial}{\\partial \\theta_k} \\exp\\left(-\\beta \\sum_{j=1}^{K} \\theta_j \\phi_j(r_i)\\right)\n$$\n$$\n= \\exp\\left(-\\beta \\sum_{j=1}^{K} \\theta_j \\phi_j(r_i)\\right) \\cdot \\frac{\\partial}{\\partial \\theta_k} \\left(-\\beta \\sum_{j=1}^{K} \\theta_j \\phi_j(r_i)\\right)\n$$\nThe first term is simply $g(r_i; \\boldsymbol{\\theta})$. The derivative of the sum with respect to $\\theta_k$ is $-\\beta \\phi_k(r_i)$, since the basis functions $\\phi_j(r_i)$ do not depend on $\\boldsymbol{\\theta}$ and $\\frac{\\partial \\theta_j}{\\partial \\theta_k} = \\delta_{jk}$ (the Kronecker delta).\n$$\n\\frac{\\partial}{\\partial \\theta_k} \\left(-\\beta \\sum_{j=1}^{K} \\theta_j \\phi_j(r_i)\\right) = -\\beta \\sum_{j=1}^{K} \\frac{\\partial \\theta_j}{\\partial \\theta_k} \\phi_j(r_i) = -\\beta \\phi_k(r_i)\n$$\nTherefore, the sensitivity is:\n$$\n\\frac{\\partial g(r_i;\\boldsymbol{\\theta})}{\\partial \\theta_k} = g(r_i;\\boldsymbol{\\theta}) \\cdot \\left(-\\beta \\phi_k(r_i)\\right) = -\\beta \\, \\phi_k(r_i) \\, g(r_i;\\boldsymbol{\\theta})\n$$\nSubstituting this result back into the expression for the gradient of $J(\\boldsymbol{\\theta})$:\n$$\n\\frac{\\partial J}{\\partial \\theta_k} = \\sum_{i=1}^{M} w_i \\left[g(r_i;\\boldsymbol{\\theta}) - g_{\\mathrm{target}}(r_i)\\right] \\left(-\\beta \\, \\phi_k(r_i) \\, g(r_i;\\boldsymbol{\\theta})\\right)\n$$\nRearranging for clarity and computational implementation:\n$$\n\\frac{\\partial J}{\\partial \\theta_k} = -\\beta \\sum_{i=1}^{M} w_i \\left[g(r_i;\\boldsymbol{\\theta}) - g_{\\mathrm{target}}(r_i)\\right] g(r_i;\\boldsymbol{\\theta}) \\phi_k(r_i)\n$$\nThis is the final expression for the $k$-th component of the gradient vector $\\nabla J$.\n\nThe problem specifies a single gradient-descent update. Given an initial set of parameters $\\boldsymbol{\\theta}^{\\mathrm{old}}$ and a learning rate $\\alpha$, the updated parameters $\\boldsymbol{\\theta}^{\\mathrm{new}}$ are calculated as:\n$$\n\\theta_k^{\\mathrm{new}} = \\theta_k^{\\mathrm{old}} - \\alpha \\left. \\frac{\\partial J}{\\partial \\theta_k} \\right|_{\\boldsymbol{\\theta}=\\boldsymbol{\\theta}^{\\mathrm{old}}}\n$$\nSubstituting our derived gradient:\n$$\n\\theta_k^{\\mathrm{new}} = \\theta_k^{\\mathrm{old}} - \\alpha \\left(-\\beta \\sum_{i=1}^{M} w_i \\left[g(r_i;\\boldsymbol{\\theta}^{\\mathrm{old}}) - g_{\\mathrm{target}}(r_i)\\right] g(r_i;\\boldsymbol{\\theta}^{\\mathrm{old}}) \\phi_k(r_i)\\right)\n$$\n$$\n\\theta_k^{\\mathrm{new}} = \\theta_k^{\\mathrm{old}} + \\alpha\\beta \\sum_{i=1}^{M} w_i \\left[g(r_i;\\boldsymbol{\\theta}^{\\mathrm{old}}) - g_{\\mathrm{target}}(r_i)\\right] g(r_i;\\boldsymbol{\\theta}^{\\mathrm{old}}) \\phi_k(r_i)\n$$\nThis formula provides the algorithm for computing the updated parameters.\n\nThe numerical implementation proceeds as follows:\n1.  For each test case, define the input parameters $\\beta$, $\\boldsymbol{c}$, $\\boldsymbol{\\sigma}$, $\\boldsymbol{\\theta}^{\\mathrm{old}}$, $\\boldsymbol{r}$, $\\boldsymbol{w}$, and $\\alpha$.\n2.  Discretize the Gaussian basis functions $\\phi_k(r) = \\exp\\left(-\\frac{(r - c_k)^2}{2\\sigma_k^2}\\right)$ by evaluating them at each grid point $r_i$ for each basis $k$. This yields an $M \\times K$ matrix $\\Phi$, where $\\Phi_{ik} = \\phi_k(r_i)$.\n3.  Compute the model potential at each grid point: $\\boldsymbol{u}^{\\mathrm{model}} = \\boldsymbol{\\Phi} \\boldsymbol{\\theta}^{\\mathrm{old}}$.\n4.  Compute the model RDF at each grid point: $g_i^{\\mathrm{model}} = \\exp(-\\beta u_i^{\\mathrm{model}})$.\n5.  Compute the target RDF, $g_i^{\\mathrm{target}}$, at each grid point according to the specific analytical function for the test case.\n6.  For each parameter $\\theta_k$, compute the gradient component $\\frac{\\partial J}{\\partial \\theta_k}$ using the derived formula, evaluated with $\\boldsymbol{\\theta}^{\\mathrm{old}}$.\n7.  Update each parameter $\\theta_k$ using the gradient-descent rule to obtain $\\theta_k^{\\mathrm{new}}$.\n8.  Collect the resulting vector $\\boldsymbol{\\theta}^{\\mathrm{new}}$ for each test case and format the final output as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy is not used and thus not imported.\n\ndef solve():\n    \"\"\"\n    Solves for the updated coarse-graining parameters using a single\n    gradient descent step for a series of test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (General scenario)\n        {\n            \"beta\": 1.0,\n            \"centers\": np.array([0.8, 1.4, 2.0]),\n            \"widths\": np.array([0.15, 0.25, 0.20]),\n            \"theta_old\": np.array([1.0, -0.5, 0.2]),\n            \"r_grid\": np.array([0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0]),\n            \"weights\": np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]),\n            \"g_target_func\": lambda r: 1 - 0.4 * np.exp(-(r - 1.2)**2 / 0.25**2) + 0.1 * np.exp(-(r - 1.8)**2 / 0.1**2),\n            \"alpha\": 0.05,\n        },\n        # Case 2 (Consistency check: model equals target)\n        {\n            \"beta\": 1.5,\n            \"centers\": np.array([1.0, 1.5]),\n            \"widths\": np.array([0.3, 0.25]),\n            \"theta_old\": np.array([0.3, -0.1]),\n            \"r_grid\": np.array([0.8, 1.0, 1.2, 1.4, 1.6]),\n            \"weights\": np.array([1.0, 1.0, 1.0, 1.0, 1.0]),\n            \"g_target_func\": 'self', # special keyword to use the model's own output as target\n            \"alpha\": 0.10,\n        },\n        # Case 3 (High-temperature limit)\n        {\n            \"beta\": 0.1,\n            \"centers\": np.array([1.0, 2.0]),\n            \"widths\": np.array([0.4, 0.4]),\n            \"theta_old\": np.array([1.0, 1.0]),\n            \"r_grid\": np.array([0.5, 1.0, 1.5, 2.0]),\n            \"weights\": np.array([1.0, 1.0, 1.0, 1.0]),\n            \"g_target_func\": lambda r: np.ones_like(r),\n            \"alpha\": 0.10,\n        },\n        # Case 4 (Single-point edge case)\n        {\n            \"beta\": 2.0,\n            \"centers\": np.array([1.0]),\n            \"widths\": np.array([0.2]),\n            \"theta_old\": np.array([0.5]),\n            \"r_grid\": np.array([1.0]),\n            \"weights\": np.array([2.0]),\n            \"g_target_func\": lambda r: np.array([0.7]),\n            \"alpha\": 0.20,\n        },\n        # Case 5 (Zero-weight boundary case)\n        {\n            \"beta\": 1.0,\n            \"centers\": np.array([0.7, 1.3]),\n            \"widths\": np.array([0.2, 0.2]),\n            \"theta_old\": np.array([-0.2, 0.4]),\n            \"r_grid\": np.array([0.6, 0.9, 1.2]),\n            \"weights\": np.array([0.0, 0.0, 0.0]),\n            \"g_target_func\": lambda r: 1 - 0.3 * np.exp(-(r - 1.0)**2 / 0.2**2),\n            \"alpha\": 0.10,\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        beta = case[\"beta\"]\n        c = case[\"centers\"]\n        sigma = case[\"widths\"]\n        theta_old = case[\"theta_old\"]\n        r = case[\"r_grid\"]\n        w = case[\"weights\"]\n        g_target_func = case[\"g_target_func\"]\n        alpha = case[\"alpha\"]\n\n        K = len(theta_old)\n        M = len(r)\n\n        # Step 2: Discretize basis functions into an M x K matrix Phi\n        # phi_matrix[i, k] = phi_k(r_i)\n        r_col = r[:, np.newaxis]  # Shape (M, 1)\n        c_row = c[np.newaxis, :]  # Shape (1, K)\n        sigma_row = sigma[np.newaxis, :] # Shape (1, K)\n        phi_matrix = np.exp(-((r_col - c_row)**2) / (2 * sigma_row**2))\n        \n        # Step 3: Compute model potential u(r)\n        u_model = phi_matrix @ theta_old\n        \n        # Step 4: Compute model RDF g(r)\n        g_model = np.exp(-beta * u_model)\n        \n        # Step 5: Compute target RDF g_target(r)\n        if g_target_func == 'self':\n            # For Case 2, the target is the model itself\n            g_target = g_model\n        else:\n            g_target = g_target_func(r)\n\n        # Step 6: Compute gradient of J\n        # grad_J_k = -beta * sum_i( w_i * (g_model_i - g_target_i) * g_model_i * phi_ik )\n        residuals = w * (g_model - g_target) * g_model\n        grad_J = -beta * phi_matrix.T @ residuals\n\n        # Step 7: Perform gradient descent update\n        theta_new = theta_old - alpha * grad_J\n        \n        results.append(theta_new)\n\n    # Format the final output string exactly as specified.\n    # No spaces after commas, 6 decimal places.\n    formatted_results = []\n    for res_vec in results:\n        string_vec = [f\"{x:.6f}\" for x in res_vec]\n        formatted_results.append(f\"[{','.join(string_vec)}]\")\n    final_output_string = f\"[{','.join(formatted_results)}]\"\n    \n    print(final_output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "A common challenge in structure-based coarse-graining, such as Iterative Boltzmann Inversion (IBI), is that a potential optimized to match the radial distribution function may not correctly reproduce other thermodynamic properties like pressure. This exercise tackles this practical issue by demonstrating how to implement a targeted pressure correction . You will derive the analytical contribution to the system's virial pressure from a simple, long-range tail added to the potential, learning a standard technique to refine CG models for thermodynamic consistency without disrupting the previously matched local structure.",
            "id": "4081636",
            "problem": "Consider an isotropic, homogeneous, single-component coarse-grained fluid at number density $\\rho$ and temperature $T$, where interactions are represented by a pairwise central coarse-grained potential $u(r)$ truncated and shifted to zero at a cutoff distance $r_{c}$. You use Iterative Boltzmann Inversion (IBI) to match the target radial distribution function $g_{\\mathrm{target}}(r)$, but you also wish to correct a residual pressure mismatch by adding a long-range tail controlled by a scalar parameter. To avoid altering structure in the region that primarily determines $g(r)$, you introduce a smooth pressure-correction tail that acts only in a window $[r_{t}, r_{c}]$ with $0 < r_{t} < r_{c}$, defined as\n$$\nu_{\\mathrm{tail}}(r) =\n\\begin{cases}\n-\\lambda \\left(1 - \\frac{r}{r_{c}}\\right), & r_{t} \\leq r \\leq r_{c}, \\\\\n0, & \\text{otherwise},\n\\end{cases}\n$$\nwhere $\\lambda$ is a small, tunable energy-scale parameter. The total potential entering IBI becomes $u(r) = u_{0}(r) + u_{\\mathrm{tail}}(r)$, where $u_{0}(r)$ is the pre-existing coarse-grained potential.\n\nStarting from the mechanical definition of pressure for a fluid with pairwise central forces and using the virial theorem, derive an analytic expression for the change in virial pressure $\\Delta P(\\lambda)$ induced solely by the addition of $u_{\\mathrm{tail}}(r)$. Explicitly state any approximations you use. In particular, assume that the structural region $r < r_{t}$ remains unchanged and that in the window $[r_{t}, r_{c}]$ the pair structure is close to ideal so that $g(r) \\approx 1$.\n\nYour final answer must be a single closed-form analytic expression for $\\Delta P(\\lambda)$ in terms of $\\rho$, $r_{t}$, $r_{c}$, and $\\lambda$. Express the final pressure change in Joules per cubic meter if numerical values are substituted. No rounding is required.",
            "solution": "The pressure $P$ of a homogeneous, isotropic fluid composed of particles interacting via a pairwise central potential $u(r)$ can be expressed using the virial theorem as the sum of an ideal gas term and an excess term arising from intermolecular forces:\n$$\nP = \\rho k_{B} T + P_{\\mathrm{virial}}\n$$\nwhere $\\rho$ is the number density, $k_{B}$ is the Boltzmann constant, and $T$ is the absolute temperature. The virial or excess pressure, $P_{\\mathrm{virial}}$, is given by the integral over the pair-virial function:\n$$\nP_{\\mathrm{virial}} = -\\frac{1}{6} \\int_V \\rho^{(2)}(\\mathbf{r}_1, \\mathbf{r}_2) \\, \\mathbf{r}_{12} \\cdot \\nabla_1 u(|\\mathbf{r}_{12}|) \\, d\\mathbf{r}_1 d\\mathbf{r}_2\n$$\nFor a homogeneous and isotropic system, the two-body density $\\rho^{(2)}(\\mathbf{r}_1, \\mathbf{r}_2)$ can be written as $\\rho^2 g(|\\mathbf{r}_{12}|)$, where $g(r)$ is the radial distribution function and $r = |\\mathbf{r}_{12}|$. The expression for the virial pressure simplifies to:\n$$\nP_{\\mathrm{virial}} = -\\frac{2\\pi\\rho^2}{3} \\int_{0}^{\\infty} r^3 \\frac{du(r)}{dr} g(r) dr\n$$\nThe problem specifies that the potential is truncated at a cutoff distance $r_{c}$, so the integral's upper limit is $r_{c}$. The total potential is given as the sum of a base potential $u_{0}(r)$ and a pressure-correction tail potential $u_{\\mathrm{tail}}(r)$, i.e., $u(r) = u_{0}(r) + u_{\\mathrm{tail}}(r)$.\n\nThe problem asks for the change in virial pressure, $\\Delta P(\\lambda)$, induced solely by the addition of $u_{\\mathrm{tail}}(r)$. This quantity is the contribution to the total virial pressure that arises directly from the tail potential term. It is calculated by substituting $u_{\\mathrm{tail}}(r)$ into the virial pressure formula:\n$$\n\\Delta P(\\lambda) = -\\frac{2\\pi\\rho^2}{3} \\int_{0}^{r_{c}} r^3 \\frac{du_{\\mathrm{tail}}(r)}{dr} g(r) dr\n$$\nHere, $g(r)$ is the radial distribution function of the system with the total potential $u(r)$.\n\nThe tail potential $u_{\\mathrm{tail}}(r)$ is defined as:\n$$\nu_{\\mathrm{tail}}(r) =\n\\begin{cases}\n-\\lambda \\left(1 - \\frac{r}{r_{c}}\\right), & r_{t} \\leq r \\leq r_{c}, \\\\\n0, & \\text{otherwise},\n\\end{cases}\n$$\nwhere $0 < r_{t} < r_{c}$. The derivative of $u_{\\mathrm{tail}}(r)$ with respect to $r$ is:\n$$\n\\frac{du_{\\mathrm{tail}}(r)}{dr} = \\frac{d}{dr} \\left(-\\lambda + \\frac{\\lambda r}{r_c}\\right) = \\frac{\\lambda}{r_c}\n$$\nfor $r \\in [r_{t}, r_{c}]$. Outside this interval, the derivative is $0$. Therefore, the integral for $\\Delta P(\\lambda)$ only needs to be evaluated over the interval $[r_{t}, r_{c}]$:\n$$\n\\Delta P(\\lambda) = -\\frac{2\\pi\\rho^2}{3} \\int_{r_{t}}^{r_{c}} r^3 \\left(\\frac{\\lambda}{r_c}\\right) g(r) dr\n$$\nThe problem provides a key approximation: in the window $[r_{t}, r_{c}]$, the pair structure is close to ideal, which allows us to set $g(r) \\approx 1$. Applying this approximation simplifies the integral:\n$$\n\\Delta P(\\lambda) \\approx -\\frac{2\\pi\\rho^2}{3} \\int_{r_{t}}^{r_{c}} r^3 \\left(\\frac{\\lambda}{r_c}\\right) (1) dr\n$$\nWe can pull the constants, including the parameter $\\lambda$, out of the integral:\n$$\n\\Delta P(\\lambda) = -\\frac{2\\pi\\rho^2\\lambda}{3r_c} \\int_{r_{t}}^{r_{c}} r^3 dr\n$$\nNow, we evaluate the definite integral of $r^3$:\n$$\n\\int_{r_{t}}^{r_{c}} r^3 dr = \\left[ \\frac{r^4}{4} \\right]_{r_{t}}^{r_{c}} = \\frac{r_c^4}{4} - \\frac{r_t^4}{4} = \\frac{1}{4}(r_c^4 - r_t^4)\n$$\nSubstituting this result back into the expression for $\\Delta P(\\lambda)$:\n$$\n\\Delta P(\\lambda) = -\\frac{2\\pi\\rho^2\\lambda}{3r_c} \\left( \\frac{r_c^4 - r_t^4}{4} \\right)\n$$\nSimplifying the numerical factors gives the final analytical expression for the change in virial pressure due to the tail potential:\n$$\n\\Delta P(\\lambda) = -\\frac{2\\pi\\rho^2\\lambda(r_c^4 - r_t^4)}{12r_c} = -\\frac{\\pi\\rho^2\\lambda}{6r_c} (r_c^4 - r_t^4)\n$$\nThis expression depends only on the specified parameters $\\rho$, $r_{t}$, $r_{c}$, and $\\lambda$. The problem states that if numerical values are substituted, the pressure change should be in Joules per cubic meter. A check of the units confirms this: with $\\rho$ in $m^{-3}$, $\\lambda$ in $J$, and $r_t, r_c$ in $m$, the units of $\\Delta P(\\lambda)$ are $\\frac{(m^{-3})^2 J}{m} (m^4) = J \\cdot m^{-3}$. Since $1 \\, J = 1 \\, N \\cdot m$, this is equivalent to $N \\cdot m^{-2}$, or Pascals ($Pa$), the standard SI unit of pressure.",
            "answer": "$$\n\\boxed{-\\frac{\\pi \\rho^{2} \\lambda}{6 r_{c}} (r_{c}^{4} - r_{t}^{4})}\n$$"
        }
    ]
}