{
    "hands_on_practices": [
        {
            "introduction": "要深刻理解粗粒化，我们必须从第一性原理出发，探究当我们将快速的原子自由度积分掉之后，慢速的粗粒化变量的动力学行为会发生什么变化。这个练习将引导您推导广义朗之万方程 (Generalized Langevin Equation, GLE)，通过求解一个与谐振子热浴耦合的粗粒化坐标的动力学，您将亲眼见证记忆效应和随机力是如何从微观哈密顿量中自然产生的。这项实践不仅能加深您对 Mori-Zwanzig 形式理论的理解，还能揭示耗散现象与微观涨落之间深刻的内在联系，即涨落-耗散定理 (Fluctuation-Dissipation Theorem) 。",
            "id": "4081629",
            "problem": "考虑一个质量为 $M$ 的粗粒化 (CG) 坐标 $X(t)$，它在一个光滑势 $U(X)$ 下演化，并与一个模拟未解析分子自由度的谐振子热浴线性耦合。该热浴由独立的振子组成，其坐标为 $q_j$，质量为 $m_j$，频率为 $\\omega_j$，耦合系数为 $c_j$。总哈密顿量为\n$$\nH = \\frac{P^{2}}{2M} + U(X) + \\sum_{j} \\left[ \\frac{p_j^{2}}{2 m_j} + \\frac{1}{2} m_j \\omega_j^{2} \\left( q_j - \\frac{c_j}{m_j \\omega_j^{2}} X \\right)^{2} \\right],\n$$\n它包含了标准的反对项，该项在消除热浴后能保持裸势 $U(X)$ 不变。假设热浴初始处于温度为 $T$ 的正则平衡状态，其采样独立于初始的 CG 坐标和动量，玻尔兹曼常数为 $k_B$。\n\n从哈密顿动力学和平衡平均的基本定义出发，消除热浴自由度，得到 $X(t)$ 的广义朗之万方程 (GLE)，其形式为\n$$\nM \\ddot{X}(t) + U'(X(t)) + \\int_{0}^{t} K(t-s)\\, \\dot{X}(s)\\, ds = R(t),\n$$\n其中 $K(t)$ 是记忆核，$R(t)$ 是由热浴初始条件决定的随机力。然后，通过由下式定义的谱密度 $J(\\omega)$，在连续极限下对热浴进行建模\n$$\nJ(\\omega) \\equiv \\frac{\\pi}{2} \\sum_{j} \\frac{c_j^{2}}{m_j \\omega_j} \\, \\delta(\\omega - \\omega_j),\n$$\n并采用 Drude–Lorentz 形式\n$$\nJ(\\omega) = \\gamma M \\, \\omega \\, \\frac{\\omega_c^{2}}{\\omega^{2} + \\omega_c^{2}},\n$$\n其中摩擦尺度 $\\gamma > 0$，截止频率 $\\omega_c > 0$。计算得到的记忆核 $K(t)$ 的闭合解析形式。利用谐振子热浴的正则平衡，验证解析的涨落-耗散定理 (FDT)，即随机力自相关满足\n$$\n\\langle R(t) R(0) \\rangle = k_B T \\, K(t).\n$$\n\n答案规格：\n- 将无量纲记忆核 $\\tilde{K}(t) \\equiv K(t)/(M \\gamma \\omega_c)$ 作为最终答案。\n- 无需进行数值计算；请提供精确的解析表达式。",
            "solution": "该问题要求推导与谐振子热浴耦合的粗粒化 (CG) 坐标 $X(t)$ 的广义朗之万方程 (GLE)，计算特定谱密度下的记忆核 $K(t)$，并验证涨落-耗散定理 (FDT)。\n\n### 步骤 1：问题验证\n\n**1.1. 提取已知条件**\n\n-   **哈密顿量**:\n    $$H = \\frac{P^{2}}{2M} + U(X) + \\sum_{j} \\left[ \\frac{p_j^{2}}{2 m_j} + \\frac{1}{2} m_j \\omega_j^{2} \\left( q_j - \\frac{c_j}{m_j \\omega_j^{2}} X \\right)^{2} \\right]$$\n-   **CG 粒子质量**: $M$\n-   **CG 势**: $U(X)$\n-   **热浴振子属性**: 坐标 $q_j$、动量 $p_j$、质量 $m_j$、频率 $\\omega_j$、耦合常数 $c_j$。\n-   **初始条件**: 热浴处于温度为 $T$ 的正则平衡状态，其采样独立于初始 CG 状态。玻尔兹曼常数为 $k_B$。\n-   **目标方程 (GLE)**:\n    $$M \\ddot{X}(t) + U'(X(t)) + \\int_{0}^{t} K(t-s)\\, \\dot{X}(s)\\, ds = R(t)$$\n-   **谱密度定义**:\n    $$J(\\omega) \\equiv \\frac{\\pi}{2} \\sum_{j} \\frac{c_j^{2}}{m_j \\omega_j} \\, \\delta(\\omega - \\omega_j)$$\n-   **指定的谱密度 (Drude–Lorentz)**:\n    $$J(\\omega) = \\gamma M \\, \\omega \\, \\frac{\\omega_c^{2}}{\\omega^{2} + \\omega_c^{2}}$$\n    其中摩擦尺度 $\\gamma > 0$，截止频率 $\\omega_c > 0$。\n-   **待验证的涨落-耗散定理 (FDT)**:\n    $$\\langle R(t) R(0) \\rangle = k_B T \\, K(t)$$\n-   **要求输出**: 无量纲记忆核 $\\tilde{K}(t) \\equiv K(t)/(M \\gamma \\omega_c)$。\n\n**1.2. 使用提取的已知条件进行验证**\n\n-   **科学依据**：该问题描述了 Caldeira-Leggett 模型（在其经典极限下），这是物理学和化学中用于研究开放量子系统和耗散动力学的一个基础且广泛使用的模型。从此哈密顿量推导 GLE 是非平衡统计力学中的一个标准过程。Drude-Lorentz 谱密度是一个具有物理动机的常见选择。FDT 是一个基石定理。该问题在科学上是合理的。\n-   **良态问题**：问题定义清晰，包含了推导唯一解析解所需的所有信息。\n-   **客观性**：语言精确、数学化，没有任何主观或基于观点的陈述。\n-   **主题相关性**：该问题直接涉及粗粒化（消除热浴自由度）、由此产生的记忆效应及其与涨落的联系，这正是*计算复杂流体*领域中*分子系统粗粒化方法*这一主题的核心。\n\n**1.3. 结论与行动**\n\n问题有效。我将继续进行解答。\n\n### 步骤 2：GLE 的推导\n\n我们首先使用哈密顿方程从给定的哈密顿量推导运动方程。\nCG 坐标 X 的运动方程为：\n$M\\ddot{X} = -\\frac{\\partial H}{\\partial X} = -U'(X) - \\frac{\\partial}{\\partial X} \\sum_{j} \\frac{1}{2} m_j \\omega_j^{2} \\left( q_j - \\frac{c_j}{m_j \\omega_j^{2}} X \\right)^{2}$\n$M\\ddot{X} = -U'(X) - \\sum_{j} m_j \\omega_j^{2} \\left( q_j - \\frac{c_j}{m_j \\omega_j^{2}} X \\right) \\left(-\\frac{c_j}{m_j \\omega_j^{2}}\\right)$\n$$M\\ddot{X}(t) = -U'(X(t)) + \\sum_{j} c_j \\left( q_j(t) - \\frac{c_j}{m_j \\omega_j^{2}} X(t) \\right) \\quad (*)$$\n每个热浴振子坐标 $q_j$ 的运动方程为：\n$m_j\\ddot{q}_j = -\\frac{\\partial H}{\\partial q_j} = -m_j \\omega_j^{2} \\left( q_j - \\frac{c_j}{m_j \\omega_j^{2}} X \\right)$\n$$\\ddot{q}_j(t) + \\omega_j^{2} q_j(t) = \\frac{c_j}{m_j} X(t)$$\n这是一个受迫谐振子方程。其通解是齐次解与通过格林函数法得到的特解之和：\n$$q_j(t) = q_j(0) \\cos(\\omega_j t) + \\frac{\\dot{q}_j(0)}{\\omega_j} \\sin(\\omega_j t) + \\int_{0}^{t} \\frac{\\sin(\\omega_j (t-s))}{\\omega_j} \\frac{c_j}{m_j} X(s) ds$$\n其中 $\\dot{q}_j(0) = p_j(0)/m_j$。\n\n我们将 $q_j(t)$ 的这个解代回到 $X(t)$ 的运动方程，即方程 $(*)$。\n$M\\ddot{X} + U'(X) = \\sum_{j} c_j q_j(t) - \\sum_{j} \\frac{c_j^2}{m_j \\omega_j^2} X(t)$\n代入 $q_j(t)$ 的表达式：\n$M\\ddot{X} + U'(X) + \\left(\\sum_{j} \\frac{c_j^2}{m_j \\omega_j^2}\\right) X(t) = \\sum_{j} c_j \\left[ q_j(0)\\cos(\\omega_j t) + \\frac{p_j(0)}{m_j\\omega_j}\\sin(\\omega_j t) + \\frac{c_j}{m_j\\omega_j} \\int_{0}^{t} \\sin(\\omega_j(t-s)) X(s) ds \\right]$\n我们关注积分项。我们进行分部积分：\n$\\int_{0}^{t} \\sin(\\omega_j(t-s)) X(s) ds = \\left[ X(s) \\frac{\\cos(\\omega_j(t-s))}{\\omega_j} \\right]_0^t - \\int_{0}^{t} \\frac{\\cos(\\omega_j(t-s))}{\\omega_j} \\dot{X}(s) ds$\n$= \\frac{X(t)}{\\omega_j} - \\frac{X(0)\\cos(\\omega_j t)}{\\omega_j} - \\frac{1}{\\omega_j} \\int_{0}^{t} \\cos(\\omega_j(t-s)) \\dot{X}(s) ds$\n将此代回到 X 的方程中：\n$\\sum_{j} \\frac{c_j^2}{m_j} \\int_{0}^{t} \\frac{\\sin(\\omega_j(t-s))}{\\omega_j} X(s) ds = \\sum_{j} \\frac{c_j^2}{m_j\\omega_j^2} \\left[ X(t) - X(0)\\cos(\\omega_j t) - \\int_{0}^{t} \\cos(\\omega_j(t-s)) \\dot{X}(s) ds \\right]$\n$= \\left(\\sum_j \\frac{c_j^2}{m_j \\omega_j^2}\\right) X(t) - X(0) \\sum_j \\frac{c_j^2}{m_j \\omega_j^2} \\cos(\\omega_j t) - \\int_0^t \\left(\\sum_j \\frac{c_j^2}{m_j \\omega_j^2} \\cos(\\omega_j(t-s))\\right) \\dot{X}(s) ds$\nX 的完整方程变为：\n$M\\ddot{X} + U'(X) + \\left(\\sum_{j} \\frac{c_j^2}{m_j \\omega_j^2}\\right) X(t) = \\sum_{j} c_j \\left[ q_j(0)\\cos(\\omega_j t) + \\frac{p_j(0)}{m_j\\omega_j}\\sin(\\omega_j t) \\right] + \\left(\\sum_j \\frac{c_j^2}{m_j \\omega_j^2}\\right) X(t) - X(0) \\sum_j \\frac{c_j^2}{m_j \\omega_j^2} \\cos(\\omega_j t) - \\int_0^t \\left(\\sum_j \\frac{c_j^2}{m_j \\omega_j^2} \\cos(\\omega_j(t-s))\\right) \\dot{X}(s) ds$\n$\\left(\\sum_{j} \\frac{c_j^2}{m_j \\omega_j^2}\\right) X(t)$ 项在等式两边消去，这凸显了哈密顿量中反对项的作用。重新整理剩余项以匹配 GLE 的形式：\n$$M\\ddot{X}(t) + U'(X(t)) + \\int_0^t K(t-s) \\dot{X}(s) ds = R(t)$$\n由此我们确定记忆核 $K(t)$ 和随机力 $R(t)$ 为：\n$$K(t) = \\sum_{j} \\frac{c_j^2}{m_j \\omega_j^2} \\cos(\\omega_j t)$$\n$$R(t) = \\sum_{j} \\left[ c_j \\left(q_j(0) - \\frac{c_j X(0)}{m_j\\omega_j^2}\\right) \\cos(\\omega_j t) + \\frac{c_j p_j(0)}{m_j \\omega_j} \\sin(\\omega_j t) \\right]$$\n\n### 步骤 3：记忆核的计算\n\n现在我们使用谱密度 $J(\\omega)$ 将 $K(t)$ 的求和形式转换成积分形式。其关系推导如下：\n$\\frac{2}{\\pi} \\int_0^\\infty \\frac{J(\\omega)}{\\omega} \\cos(\\omega t) d\\omega = \\frac{2}{\\pi} \\int_0^\\infty \\left( \\frac{\\pi}{2} \\sum_j \\frac{c_j^2}{m_j \\omega_j} \\delta(\\omega-\\omega_j) \\right) \\frac{\\cos(\\omega t)}{\\omega} d\\omega$\n$= \\sum_j \\frac{c_j^2}{m_j \\omega_j} \\int_0^\\infty \\frac{\\cos(\\omega t)}{\\omega} \\delta(\\omega-\\omega_j) d\\omega = \\sum_j \\frac{c_j^2}{m_j \\omega_j} \\frac{\\cos(\\omega_j t)}{\\omega_j} = K(t)$.\n所以，我们有通用公式：\n$$K(t) = \\frac{2}{\\pi} \\int_0^\\infty \\frac{J(\\omega)}{\\omega} \\cos(\\omega t) d\\omega$$\n使用给定的 Drude-Lorentz 谱密度 $J(\\omega) = \\gamma M \\omega \\frac{\\omega_c^2}{\\omega^2 + \\omega_c^2}$：\n$K(t) = \\frac{2}{\\pi} \\int_0^\\infty \\frac{1}{\\omega} \\left( \\gamma M \\omega \\frac{\\omega_c^2}{\\omega^2 + \\omega_c^2} \\right) \\cos(\\omega t) d\\omega = \\frac{2 \\gamma M \\omega_c^2}{\\pi} \\int_0^\\infty \\frac{\\cos(\\omega t)}{\\omega^2 + \\omega_c^2} d\\omega$.\n这是一个标准积分。对于 $t \\ge 0$，可以用复分析方法计算。考虑积分 $\\frac{1}{2} \\int_{-\\infty}^\\infty \\frac{e^{i\\omega t}}{\\omega^2 + \\omega_c^2} d\\omega$ 并在上半复平面（对于 $t > 0$）闭合围道，我们包围了位于 $\\omega = i\\omega_c$ 的单极点。\n$\\int_0^\\infty \\frac{\\cos(\\omega t)}{\\omega^2 + \\omega_c^2} d\\omega = \\frac{1}{2} \\text{Re} \\left[ \\oint \\frac{e^{i\\omega t}}{\\omega^2 + \\omega_c^2} d\\omega \\right] = \\frac{1}{2} \\text{Re} \\left[ 2\\pi i \\, \\text{Res}_{\\omega=i\\omega_c} \\frac{e^{i\\omega t}}{(\\omega - i\\omega_c)(\\omega + i\\omega_c)} \\right]$\n$= \\frac{1}{2} \\text{Re} \\left[ 2\\pi i \\, \\frac{e^{i(i\\omega_c)t}}{2i\\omega_c} \\right] = \\frac{1}{2} \\text{Re} \\left[ \\frac{\\pi}{\\omega_c} e^{-\\omega_c t} \\right] = \\frac{\\pi}{2\\omega_c} e^{-\\omega_c t}$.\n将此结果代回到 $K(t)$ 的表达式中：\n$$K(t) = \\frac{2 \\gamma M \\omega_c^2}{\\pi} \\left( \\frac{\\pi}{2\\omega_c} e^{-\\omega_c t} \\right) = M \\gamma \\omega_c e^{-\\omega_c t}$$\n\n### 步骤 4：涨落-耗散定理的验证\n\nFDT 表明 $\\langle R(t) R(0) \\rangle = k_B T K(t)$。我们计算随机力的相关函数。\n$R(0) = \\sum_k c_k \\left(q_k(0) - \\frac{c_k X(0)}{m_k\\omega_k^2}\\right)$。让我们引入位移坐标 $q'_j(0) = q_j(0) - \\frac{c_j X(0)}{m_j\\omega_j^2}$。那么\n$R(t) = \\sum_j \\left( c_j q'_j(0)\\cos(\\omega_j t) + \\frac{c_j p_j(0)}{m_j \\omega_j} \\sin(\\omega_j t) \\right)$\n$R(0) = \\sum_k c_k q'_k(0)$.\n相关函数为 $\\langle R(t) R(0) \\rangle = \\left\\langle \\left( \\sum_j c_j q'_j(0)\\cos(\\omega_j t) + \\frac{c_j p_j(0)}{m_j \\omega_j} \\sin(\\omega_j t) \\right) \\left( \\sum_k c_k q'_k(0) \\right) \\right\\rangle$。\n热浴处于正则平衡状态，由哈密顿量的热浴部分描述，用 $q'_j$ 和 $p_j$ 表示为 $H_{\\text{bath}} = \\sum_j \\left( \\frac{p_j^2}{2m_j} + \\frac{1}{2}m_j\\omega_j^2 (q'_j)^2 \\right)$。\n平衡平均值为：\n$\\langle q'_j(0) \\rangle = 0$, $\\langle p_j(0) \\rangle = 0$。\n$\\langle q'_j(0) p_k(0) \\rangle = 0$ 对所有 $j, k$ 成立。\n$\\langle q'_j(0) q'_k(0) \\rangle = \\delta_{jk} \\langle (q'_j)^2 \\rangle$。\n根据能量均分定理，$\\frac{1}{2} m_j \\omega_j^2 \\langle (q'_j)^2 \\rangle = \\frac{1}{2} k_B T$，这给出 $\\langle (q'_j)^2 \\rangle = \\frac{k_B T}{m_j \\omega_j^2}$。\n利用这些性质，相关函数变为：\n$\\langle R(t) R(0) \\rangle = \\sum_{j,k} c_j c_k \\cos(\\omega_j t) \\langle q'_j(0) q'_k(0) \\rangle = \\sum_j c_j^2 \\cos(\\omega_j t) \\langle (q'_j)^2 \\rangle$\n$\\langle R(t) R(0) \\rangle = \\sum_j c_j^2 \\cos(\\omega_j t) \\frac{k_B T}{m_j \\omega_j^2} = k_B T \\left( \\sum_j \\frac{c_j^2}{m_j \\omega_j^2} \\cos(\\omega_j t) \\right)$\n括号中的项正是我们得到的记忆核 $K(t)$ 的表达式。\n因此，我们验证了涨落-耗散定理：\n$$\\langle R(t) R(0) \\rangle = k_B T K(t)$$\n\n### 步骤 5：最终答案的构建\n\n问题要求的是无量纲记忆核 $\\tilde{K}(t) \\equiv K(t)/(M \\gamma \\omega_c)$。\n使用我们的结果 $K(t) = M \\gamma \\omega_c e^{-\\omega_c t}$，我们得到：\n$$\\tilde{K}(t) = \\frac{M \\gamma \\omega_c e^{-\\omega_c t}}{M \\gamma \\omega_c} = e^{-\\omega_c t}$$",
            "answer": "$$\\boxed{\\exp(-\\omega_c t)}$$"
        },
        {
            "introduction": "在构建粗粒化模型时，一个核心任务是确定有效的相互作用势能。这个过程通常依赖于数值优化算法，通过迭代调整势能函数以匹配来自全原子模拟的某个目标性质（如径向分布函数）。本练习将带您深入了解这一优化过程的核心机制，您将通过在一个基函数展开的势能模型中，实现一个基于梯度下降的更新步骤。通过推导目标函数相对于势能参数的梯度，您能将统计力学原理转化为具体的计算算法，这对于掌握现代粗粒化方法至关重要 。",
            "id": "4081602",
            "problem": "考虑一个处于正则系综中的分子系统，其逆温度为 $\\beta$。在低密度极限下，对于对相互作用势 $u(r)$，径向分布函数 (RDF) $g(r)$ 可由玻尔兹曼因子近似表示为 $g(r) \\approx \\exp(-\\beta u(r))$。在粗粒化中，假设对势表示为一组径向基函数 $\\{\\phi_k(r)\\}_{k=1}^{K}$ 的线性展开，其系数为 $\\theta_k$，即\n$$\nu(r; \\boldsymbol{\\theta}) = \\sum_{k=1}^{K} \\theta_k \\, \\phi_k(r).\n$$\n设目标函数为在离散网格 $\\{r_i\\}_{i=1}^{M}$ 上，模型RDF $g(r_i;\\boldsymbol{\\theta})$ 与目标RDF $g_{\\mathrm{target}}(r_i)$ 之间的加权最小二乘失配，\n$$\nJ(\\boldsymbol{\\theta}) = \\frac{1}{2}\\sum_{i=1}^{M} w_i \\left[g(r_i;\\boldsymbol{\\theta}) - g_{\\mathrm{target}}(r_i)\\right]^2,\n$$\n其中 $w_i$ 为非负权重。使用低密度近似 $g(r;\\boldsymbol{\\theta}) = \\exp\\left(-\\beta \\sum_{k=1}^{K} \\theta_k \\phi_k(r)\\right)$ 以及基于线性响应的 $g(r)$ 对 $\\theta_k$ 的灵敏度，推导梯度分量 $\\partial J / \\partial \\theta_k$ 并实现单步梯度下降更新\n$$\n\\theta_k^{\\mathrm{new}} = \\theta_k^{\\mathrm{old}} - \\alpha \\, \\frac{\\partial J}{\\partial \\theta_k},\n$$\n其中 $\\alpha$ 是学习率。\n\n所有距离 $r$、基函数参数和能量都将在一个一致的约化单位制中处理，其中所有量均为无量纲。逆温度 $\\beta$ 是无量纲的，势 $u(r)$ 也是无量纲的。本问题不涉及角度。\n\n对于数值实现，使用由下式定义的高斯径向基函数\n$$\n\\phi_k(r) = \\exp\\left(-\\frac{(r - c_k)^2}{2\\sigma_k^2}\\right),\n$$\n其中 $c_k$ 和 $\\sigma_k$ 分别是第 $k$ 个基函数的中心和宽度。\n\n您的程序必须：\n- 对每个测试用例，计算模型 $g(r_i;\\boldsymbol{\\theta})$、梯度 $\\partial J / \\partial \\theta_k$ 和更新后的参数 $\\boldsymbol{\\theta}^{\\mathrm{new}}$。\n- 生成单行输出，其中包含所有测试用例的更新后参数向量，格式为方括号内由逗号分隔的列表，每个测试用例的更新后参数向量表示为Python风格的列表，且每个参数四舍五入到六位小数（例如，\"[[0.123456,-0.234567],[...],...]\"）。\n\n测试套件：\n- 用例1（一般情景）：\n  - $\\beta = 1.0$。\n  - $K = 3$，中心 $\\boldsymbol{c} = [0.8, 1.4, 2.0]$，宽度 $\\boldsymbol{\\sigma} = [0.15, 0.25, 0.20]$。\n  - 初始参数 $\\boldsymbol{\\theta}^{\\mathrm{old}} = [1.0, -0.5, 0.2]$。\n  - 网格点 $\\boldsymbol{r} = [0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0]$。\n  - 权重 $\\boldsymbol{w} = [1, 1, 1, 1, 1, 1, 1, 1]$。\n  - 目标RDF由解析式定义\n    $$\n    g_{\\mathrm{target}}(r) = 1 - 0.4 \\exp\\left(-\\frac{(r - 1.2)^2}{0.25^2}\\right) + 0.1 \\exp\\left(-\\frac{(r - 1.8)^2}{0.1^2}\\right).\n    $$\n  - 学习率 $\\alpha = 0.05$。\n\n- 用例2（一致性检查，初始时模型与目标相等）：\n  - $\\beta = 1.5$。\n  - $K = 2$，中心 $\\boldsymbol{c} = [1.0, 1.5]$，宽度 $\\boldsymbol{\\sigma} = [0.3, 0.25]$。\n  - 真实参数 $\\boldsymbol{\\theta}^{\\mathrm{true}} = [0.3, -0.1]$，初始参数 $\\boldsymbol{\\theta}^{\\mathrm{old}} = \\boldsymbol{\\theta}^{\\mathrm{true}}$。\n  - 网格点 $\\boldsymbol{r} = [0.8, 1.0, 1.2, 1.4, 1.6]$。\n  - 权重 $\\boldsymbol{w} = [1, 1, 1, 1, 1]$。\n  - 目标RDF由带有真实参数的模型定义：\n    $$\n    g_{\\mathrm{target}}(r) = \\exp\\left(-\\beta \\sum_{k=1}^{K} \\theta_k^{\\mathrm{true}} \\, \\phi_k(r)\\right).\n    $$\n  - 学习率 $\\alpha = 0.10$。\n\n- 用例3（高温极限）：\n  - $\\beta = 0.1$。\n  - $K = 2$，中心 $\\boldsymbol{c} = [1.0, 2.0]$，宽度 $\\boldsymbol{\\sigma} = [0.4, 0.4]$。\n  - 初始参数 $\\boldsymbol{\\theta}^{\\mathrm{old}} = [1.0, 1.0]$。\n  - 网格点 $\\boldsymbol{r} = [0.5, 1.0, 1.5, 2.0]$。\n  - 权重 $\\boldsymbol{w} = [1, 1, 1, 1]$。\n  - 目标RDF是均匀的：\n    $$\n    g_{\\mathrm{target}}(r) = 1.0.\n    $$\n  - 学习率 $\\alpha = 0.10$。\n\n- 用例4（单点边界情况）：\n  - $\\beta = 2.0$。\n  - $K = 1$，中心 $\\boldsymbol{c} = [1.0]$，宽度 $\\boldsymbol{\\sigma} = [0.2]$。\n  - 初始参数 $\\boldsymbol{\\theta}^{\\mathrm{old}} = [0.5]$。\n  - 单个网格点 $\\boldsymbol{r} = [1.0]$。\n  - 权重 $\\boldsymbol{w} = [2.0]$。\n  - 指定的目标RDF：\n    $$\n    g_{\\mathrm{target}}(1.0) = 0.7.\n    $$\n  - 学习率 $\\alpha = 0.20$。\n\n- 用例5（零权重边界情况）：\n  - $\\beta = 1.0$。\n  - $K = 2$，中心 $\\boldsymbol{c} = [0.7, 1.3]$，宽度 $\\boldsymbol{\\sigma} = [0.2, 0.2]$。\n  - 初始参数 $\\boldsymbol{\\theta}^{\\mathrm{old}} = [-0.2, 0.4]$。\n  - 网格点 $\\boldsymbol{r} = [0.6, 0.9, 1.2]$。\n  - 权重 $\\boldsymbol{w} = [0.0, 0.0, 0.0]$。\n  - 目标RDF由解析式定义\n    $$\n    g_{\\mathrm{target}}(r) = 1 - 0.3 \\exp\\left(-\\frac{(r - 1.0)^2}{0.2^2}\\right).\n    $$\n  - 学习率 $\\alpha = 0.10$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含所有提供的测试用例的更新后参数向量，格式为方括号内由逗号分隔的列表。每个测试用例的结果必须是Python风格的浮点数列表，每个浮点数四舍五入到六位小数。例如：\"[[theta_case1_1,theta_case1_2,...],[theta_case2_1,theta_case2_2,...],...]\"。",
            "solution": "目标是推导最小二乘目标函数 $J(\\boldsymbol{\\theta})$ 的梯度，并用它对参数 $\\boldsymbol{\\theta}$ 执行单步梯度下降更新。该问题设置在粗粒化的背景下，其中模型势 $u(r; \\boldsymbol{\\theta})$ 被优化以重现目标径向分布函数 $g_{\\mathrm{target}}(r)$。\n\n首先，我们重申关键定义。对势被参数化为 $K$ 个基函数 $\\{\\phi_k(r)\\}_{k=1}^{K}$ 的线性组合：\n$$\nu(r; \\boldsymbol{\\theta}) = \\sum_{k=1}^{K} \\theta_k \\, \\phi_k(r)\n$$\n在低密度极限下，径向分布函数 (RDF) $g(r)$ 由玻尔兹曼因子近似：\n$$\ng(r; \\boldsymbol{\\theta}) = \\exp(-\\beta u(r; \\boldsymbol{\\theta})) = \\exp\\left(-\\beta \\sum_{k=1}^{K} \\theta_k \\phi_k(r)\\right)\n$$\n其中 $\\beta$ 是逆温度。\n\n目标函数 $J(\\boldsymbol{\\theta})$ 衡量在 $M$ 个径向点 $\\{r_i\\}_{i=1}^{M}$ 的离散集合上，模型RDF $g(r_i; \\boldsymbol{\\theta})$ 与目标RDF $g_{\\mathrm{target}}(r_i)$ 之间的加权平方差：\n$$\nJ(\\boldsymbol{\\theta}) = \\frac{1}{2}\\sum_{i=1}^{M} w_i \\left[g(r_i;\\boldsymbol{\\theta}) - g_{\\mathrm{target}}(r_i)\\right]^2\n$$\n其中 $w_i \\ge 0$ 是权重。\n\n我们的主要任务是计算 $J(\\boldsymbol{\\theta})$ 关于每个参数 $\\theta_k$ 的梯度。我们应用链式法则来微分 $J(\\boldsymbol{\\theta})$：\n$$\n\\frac{\\partial J}{\\partial \\theta_k} = \\frac{\\partial}{\\partial \\theta_k} \\left( \\frac{1}{2}\\sum_{i=1}^{M} w_i \\left[g(r_i;\\boldsymbol{\\theta}) - g_{\\mathrm{target}}(r_i)\\right]^2 \\right)\n$$\n微分和求和算子可以互换：\n$$\n\\frac{\\partial J}{\\partial \\theta_k} = \\sum_{i=1}^{M} \\frac{1}{2} w_i \\cdot 2 \\left[g(r_i;\\boldsymbol{\\theta}) - g_{\\mathrm{target}}(r_i)\\right] \\cdot \\frac{\\partial g(r_i;\\boldsymbol{\\theta})}{\\partial \\theta_k}\n$$\n$$\n\\frac{\\partial J}{\\partial \\theta_k} = \\sum_{i=1}^{M} w_i \\left[g(r_i;\\boldsymbol{\\theta}) - g_{\\mathrm{target}}(r_i)\\right] \\frac{\\partial g(r_i;\\boldsymbol{\\theta})}{\\partial \\theta_k}\n$$\n下一步是求模型RDF $g(r_i; \\boldsymbol{\\theta})$ 关于 $\\theta_k$ 的偏导数。这一项表示RDF对第 $k$ 个参数变化的灵敏度。再次对 $g(r_i; \\boldsymbol{\\theta})$ 的表达式使用链式法则：\n$$\n\\frac{\\partial g(r_i;\\boldsymbol{\\theta})}{\\partial \\theta_k} = \\frac{\\partial}{\\partial \\theta_k} \\exp\\left(-\\beta \\sum_{j=1}^{K} \\theta_j \\phi_j(r_i)\\right)\n$$\n$$\n= \\exp\\left(-\\beta \\sum_{j=1}^{K} \\theta_j \\phi_j(r_i)\\right) \\cdot \\frac{\\partial}{\\partial \\theta_k} \\left(-\\beta \\sum_{j=1}^{K} \\theta_j \\phi_j(r_i)\\right)\n$$\n第一项就是 $g(r_i; \\boldsymbol{\\theta})$。这个和对 $\\theta_k$ 的导数是 $-\\beta \\phi_k(r_i)$，因为基函数 $\\phi_j(r_i)$ 不依赖于 $\\boldsymbol{\\theta}$ 且 $\\frac{\\partial \\theta_j}{\\partial \\theta_k} = \\delta_{jk}$（克罗内克δ）。\n$$\n\\frac{\\partial}{\\partial \\theta_k} \\left(-\\beta \\sum_{j=1}^{K} \\theta_j \\phi_j(r_i)\\right) = -\\beta \\sum_{j=1}^{K} \\frac{\\partial \\theta_j}{\\partial \\theta_k} \\phi_j(r_i) = -\\beta \\phi_k(r_i)\n$$\n因此，灵敏度为：\n$$\n\\frac{\\partial g(r_i;\\boldsymbol{\\theta})}{\\partial \\theta_k} = g(r_i;\\boldsymbol{\\theta}) \\cdot \\left(-\\beta \\phi_k(r_i)\\right) = -\\beta \\, \\phi_k(r_i) \\, g(r_i;\\boldsymbol{\\theta})\n$$\n将此结果代回 $J(\\boldsymbol{\\theta})$ 的梯度表达式中：\n$$\n\\frac{\\partial J}{\\partial \\theta_k} = \\sum_{i=1}^{M} w_i \\left[g(r_i;\\boldsymbol{\\theta}) - g_{\\mathrm{target}}(r_i)\\right] \\left(-\\beta \\, \\phi_k(r_i) \\, g(r_i;\\boldsymbol{\\theta})\\right)\n$$\n为了清晰和便于计算实现，重新整理：\n$$\n\\frac{\\partial J}{\\partial \\theta_k} = -\\beta \\sum_{i=1}^{M} w_i \\left[g(r_i;\\boldsymbol{\\theta}) - g_{\\mathrm{target}}(r_i)\\right] g(r_i;\\boldsymbol{\\theta}) \\phi_k(r_i)\n$$\n这是梯度向量 $\\nabla J$ 的第 $k$ 个分量的最终表达式。\n\n问题指定了进行单步梯度下降更新。给定一组初始参数 $\\boldsymbol{\\theta}^{\\mathrm{old}}$ 和学习率 $\\alpha$，更新后的参数 $\\boldsymbol{\\theta}^{\\mathrm{new}}$ 计算如下：\n$$\n\\theta_k^{\\mathrm{new}} = \\theta_k^{\\mathrm{old}} - \\alpha \\left. \\frac{\\partial J}{\\partial \\theta_k} \\right|_{\\boldsymbol{\\theta}=\\boldsymbol{\\theta}^{\\mathrm{old}}}\n$$\n代入我们推导出的梯度：\n$$\n\\theta_k^{\\mathrm{new}} = \\theta_k^{\\mathrm{old}} - \\alpha \\left(-\\beta \\sum_{i=1}^{M} w_i \\left[g(r_i;\\boldsymbol{\\theta}^{\\mathrm{old}}) - g_{\\mathrm{target}}(r_i)\\right] g(r_i;\\boldsymbol{\\theta}^{\\mathrm{old}}) \\phi_k(r_i)\\right)\n$$\n$$\n\\theta_k^{\\mathrm{new}} = \\theta_k^{\\mathrm{old}} + \\alpha\\beta \\sum_{i=1}^{M} w_i \\left[g(r_i;\\boldsymbol{\\theta}^{\\mathrm{old}}) - g_{\\mathrm{target}}(r_i)\\right] g(r_i;\\boldsymbol{\\theta}^{\\mathrm{old}}) \\phi_k(r_i)\n$$\n该公式提供了计算更新后参数的算法。\n\n数值实现过程如下：\n1.  对每个测试用例，定义输入参数 $\\beta$、$\\boldsymbol{c}$、$\\boldsymbol{\\sigma}$、$\\boldsymbol{\\theta}^{\\mathrm{old}}$、$\\boldsymbol{r}$、$\\boldsymbol{w}$ 和 $\\alpha$。\n2.  通过在每个网格点 $r_i$ 对每个基 $k$ 求值，离散化高斯基函数 $\\phi_k(r) = \\exp\\left(-\\frac{(r - c_k)^2}{2\\sigma_k^2}\\right)$。这将得到一个 $M \\times K$ 矩阵 $\\Phi$，其中 $\\Phi_{ik} = \\phi_k(r_i)$。\n3.  计算每个网格点上的模型势：$\\boldsymbol{u}^{\\mathrm{model}} = \\boldsymbol{\\Phi} \\boldsymbol{\\theta}^{\\mathrm{old}}$。\n4.  计算每个网格点上的模型RDF：$g_i^{\\mathrm{model}} = \\exp(-\\beta u_i^{\\mathrm{model}})$。\n5.  根据测试用例的具体解析函数，计算每个网格点上的目标RDF $g_i^{\\mathrm{target}}$。\n6.  对每个参数 $\\theta_k$，使用推导出的公式并以 $\\boldsymbol{\\theta}^{\\mathrm{old}}$ 进行求值，计算梯度分量 $\\frac{\\partial J}{\\partial \\theta_k}$。\n7.  使用梯度下降法则更新每个参数 $\\theta_k$ 以获得 $\\theta_k^{\\mathrm{new}}$。\n8.  收集每个测试用例的结果向量 $\\boldsymbol{\\theta}^{\\mathrm{new}}$，并按规定格式化最终输出。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy is not used and thus not imported.\n\ndef solve():\n    \"\"\"\n    Solves for the updated coarse-graining parameters using a single\n    gradient descent step for a series of test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (General scenario)\n        {\n            \"beta\": 1.0,\n            \"centers\": np.array([0.8, 1.4, 2.0]),\n            \"widths\": np.array([0.15, 0.25, 0.20]),\n            \"theta_old\": np.array([1.0, -0.5, 0.2]),\n            \"r_grid\": np.array([0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0]),\n            \"weights\": np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]),\n            \"g_target_func\": lambda r: 1 - 0.4 * np.exp(-(r - 1.2)**2 / 0.25**2) + 0.1 * np.exp(-(r - 1.8)**2 / 0.1**2),\n            \"alpha\": 0.05,\n        },\n        # Case 2 (Consistency check: model equals target)\n        {\n            \"beta\": 1.5,\n            \"centers\": np.array([1.0, 1.5]),\n            \"widths\": np.array([0.3, 0.25]),\n            \"theta_old\": np.array([0.3, -0.1]),\n            \"r_grid\": np.array([0.8, 1.0, 1.2, 1.4, 1.6]),\n            \"weights\": np.array([1.0, 1.0, 1.0, 1.0, 1.0]),\n            \"g_target_func\": 'self', # special keyword to use the model's own output as target\n            \"alpha\": 0.10,\n        },\n        # Case 3 (High-temperature limit)\n        {\n            \"beta\": 0.1,\n            \"centers\": np.array([1.0, 2.0]),\n            \"widths\": np.array([0.4, 0.4]),\n            \"theta_old\": np.array([1.0, 1.0]),\n            \"r_grid\": np.array([0.5, 1.0, 1.5, 2.0]),\n            \"weights\": np.array([1.0, 1.0, 1.0, 1.0]),\n            \"g_target_func\": lambda r: np.ones_like(r),\n            \"alpha\": 0.10,\n        },\n        # Case 4 (Single-point edge case)\n        {\n            \"beta\": 2.0,\n            \"centers\": np.array([1.0]),\n            \"widths\": np.array([0.2]),\n            \"theta_old\": np.array([0.5]),\n            \"r_grid\": np.array([1.0]),\n            \"weights\": np.array([2.0]),\n            \"g_target_func\": lambda r: np.array([0.7]),\n            \"alpha\": 0.20,\n        },\n        # Case 5 (Zero-weight boundary case)\n        {\n            \"beta\": 1.0,\n            \"centers\": np.array([0.7, 1.3]),\n            \"widths\": np.array([0.2, 0.2]),\n            \"theta_old\": np.array([-0.2, 0.4]),\n            \"r_grid\": np.array([0.6, 0.9, 1.2]),\n            \"weights\": np.array([0.0, 0.0, 0.0]),\n            \"g_target_func\": lambda r: 1 - 0.3 * np.exp(-(r - 1.0)**2 / 0.2**2),\n            \"alpha\": 0.10,\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        beta = case[\"beta\"]\n        c = case[\"centers\"]\n        sigma = case[\"widths\"]\n        theta_old = case[\"theta_old\"]\n        r = case[\"r_grid\"]\n        w = case[\"weights\"]\n        g_target_func = case[\"g_target_func\"]\n        alpha = case[\"alpha\"]\n\n        K = len(theta_old)\n        M = len(r)\n\n        # Step 2: Discretize basis functions into an M x K matrix Phi\n        # phi_matrix[i, k] = phi_k(r_i)\n        r_col = r[:, np.newaxis]  # Shape (M, 1)\n        c_row = c[np.newaxis, :]  # Shape (1, K)\n        sigma_row = sigma[np.newaxis, :] # Shape (1, K)\n        phi_matrix = np.exp(-((r_col - c_row)**2) / (2 * sigma_row**2))\n        \n        # Step 3: Compute model potential u(r)\n        u_model = phi_matrix @ theta_old\n        \n        # Step 4: Compute model RDF g(r)\n        g_model = np.exp(-beta * u_model)\n        \n        # Step 5: Compute target RDF g_target(r)\n        if g_target_func == 'self':\n            # For Case 2, the target is the model itself\n            g_target = g_model\n        else:\n            g_target = g_target_func(r)\n\n        # Step 6: Compute gradient of J\n        # grad_J_k = -beta * sum_i( w_i * (g_model_i - g_target_i) * g_model_i * phi_ik )\n        residuals = w * (g_model - g_target) * g_model\n        grad_J = -beta * phi_matrix.T @ residuals\n\n        # Step 7: Perform gradient descent update\n        theta_new = theta_old - alpha * grad_J\n        \n        results.append(theta_new)\n\n    # Format the final output string exactly as specified.\n    # No spaces after commas, 6 decimal places.\n    formatted_results = []\n    for res_vec in results:\n        string_vec = [f\"{x:.6f}\" for x in res_vec]\n        formatted_results.append(f\"[{','.join(string_vec)}]\")\n    final_output_string = f\"[{','.join(formatted_results)}]\"\n    \n    print(final_output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "一个高质量的粗粒化模型不仅应重现正确的结构，还应准确反映体系的热力学性质，例如压力。然而，仅为匹配结构（例如，通过迭代玻尔兹曼反演法）而优化的势能，通常无法同时满足压力的要求。这个练习解决了一个高级粗粒化建模中的常见挑战：如何修正势能以匹配多个物理量。您将学习如何设计并应用一个远程校正项来调整由粗粒化势能产生的维里压力，这是一种提高模型物理真实性和可移植性的实用技巧 。",
            "id": "4081636",
            "problem": "考虑一个各向同性、均匀的单组分粗粒化流体，其数密度为 $\\rho$，温度为 $T$。其中的相互作用由一个成对中心粗粒化势 $u(r)$ 表示，该势在截断距离 $r_{c}$ 处被截断并平移至零。您使用迭代玻尔兹曼反演（IBI）来匹配目标径向分布函数 $g_{\\mathrm{target}}(r)$，但您也希望通过添加一个由标量参数控制的长程尾部来校正残余的压力不匹配。为了避免改变主要决定 $g(r)$ 的区域中的结构，您引入一个平滑的压力校正尾部，该尾部仅在窗口 $[r_{t}, r_{c}]$（其中 $0  r_t  r_c$）内起作用，定义为\n$$\nu_{\\mathrm{tail}}(r) =\n\\begin{cases}\n-\\lambda \\left(1 - \\frac{r}{r_{c}}\\right),  r_{t} \\leq r \\leq r_{c}, \\\\\n0,  \\text{其他情况},\n\\end{cases}\n$$\n其中 $\\lambda$ 是一个可调的小能量尺度参数。进入IBI的总势变为 $u(r) = u_{0}(r) + u_{\\mathrm{tail}}(r)$，其中 $u_{0}(r)$ 是预先存在的粗粒化势。\n\n从具有成对中心力的流体的压力力学定义出发，并使用维里定理，推导一个仅由添加 $u_{\\mathrm{tail}}(r)$ 引起的维里压力变化 $\\Delta P(\\lambda)$ 的解析表达式。明确说明您使用的任何近似。特别地，假设结构区域 $r  r_{t}$ 保持不变，并且在窗口 $[r_{t}, r_{c}]$ 内，对结构接近理想状态，因此 $g(r) \\approx 1$。\n\n您的最终答案必须是 $\\Delta P(\\lambda)$ 的一个单一闭式解析表达式，用 $\\rho$、$r_{t}$、$r_{c}$ 和 $\\lambda$ 表示。如果代入数值，最终的压力变化以焦耳每立方米表示。不需要四舍五入。",
            "solution": "该问题是有效的，因为它在统计力学上是有科学依据的、良态的、客观的，并且包含了足够的信息以获得唯一解。所要求的近似在粗粒化建模的背景下是标准的。\n\n对于由通过成对中心势 $u(r)$ 相互作用的粒子组成的均匀、各向同性流体，其压力 $P$ 可以使用维里定理表示为理想气体项和源于分子间力的超额项之和：\n$$\nP = \\rho k_{B} T + P_{\\mathrm{virial}}\n$$\n其中 $\\rho$ 是数密度，$k_{B}$ 是玻尔兹曼常数，$T$ 是绝对温度。维里压力或超额压力 $P_{\\mathrm{virial}}$ 由对维里函数的积分给出：\n$$\nP_{\\mathrm{virial}} = -\\frac{1}{6} \\int_V \\rho^{(2)}(\\mathbf{r}_1, \\mathbf{r}_2) \\, \\mathbf{r}_{12} \\cdot \\nabla_1 u(|\\mathbf{r}_{12}|) \\, d\\mathbf{r}_1 d\\mathbf{r}_2\n$$\n对于均匀且各向同性的系统，双体密度 $\\rho^{(2)}(\\mathbf{r}_1, \\mathbf{r}_2)$ 可以写成 $\\rho^2 g(|\\mathbf{r}_{12}|)$，其中 $g(r)$ 是径向分布函数，$r = |\\mathbf{r}_{12}|$。维里压力的表达式简化为：\n$$\nP_{\\mathrm{virial}} = -\\frac{2\\pi\\rho^2}{3} \\int_{0}^{\\infty} r^3 \\frac{du(r)}{dr} g(r) dr\n$$\n问题指明势在截断距离 $r_{c}$ 处被截断，所以积分的上限是 $r_{c}$。总势由基础势 $u_{0}(r)$ 和压力校正尾势 $u_{\\mathrm{tail}}(r)$ 之和给出，即 $u(r) = u_{0}(r) + u_{\\mathrm{tail}}(r)$。\n\n问题要求的是仅由添加 $u_{\\mathrm{tail}}(r)$ 引起的维里压力变化 $\\Delta P(\\lambda)$。这个量是总维里压力中直接由尾势项产生的贡献。通过将 $u_{\\mathrm{tail}}(r)$ 代入维里压力公式来计算：\n$$\n\\Delta P(\\lambda) = -\\frac{2\\pi\\rho^2}{3} \\int_{0}^{r_{c}} r^3 \\frac{du_{\\mathrm{tail}}(r)}{dr} g(r) dr\n$$\n这里，$g(r)$ 是具有总势 $u(r)$ 的系统的径向分布函数。\n\n尾势 $u_{\\mathrm{tail}}(r)$ 定义为：\n$$\nu_{\\mathrm{tail}}(r) =\n\\begin{cases}\n-\\lambda \\left(1 - \\frac{r}{r_{c}}\\right),  r_{t} \\leq r \\leq r_{c}, \\\\\n0,  \\text{其他情况},\n\\end{cases}\n$$\n其中 $0  r_t  r_c$。$u_{\\mathrm{tail}}(r)$ 对 $r$ 的导数是：\n$$\n\\frac{du_{\\mathrm{tail}}(r)}{dr} = \\frac{d}{dr} \\left(-\\lambda + \\frac{\\lambda r}{r_c}\\right) = \\frac{\\lambda}{r_c}\n$$\n对于 $r \\in [r_{t}, r_{c}]$。在此区间之外，导数为 $0$。因此，$\\Delta P(\\lambda)$ 的积分只需要在区间 $[r_{t}, r_{c}]$ 上计算：\n$$\n\\Delta P(\\lambda) = -\\frac{2\\pi\\rho^2}{3} \\int_{r_{t}}^{r_{c}} r^3 \\left(\\frac{\\lambda}{r_c}\\right) g(r) dr\n$$\n问题提供了一个关键的近似：在窗口 $[r_{t}, r_{c}]$ 内，对结构接近理想状态，这使我们可以设 $g(r) \\approx 1$。应用这个近似简化了积分：\n$$\n\\Delta P(\\lambda) \\approx -\\frac{2\\pi\\rho^2}{3} \\int_{r_{t}}^{r_{c}} r^3 \\left(\\frac{\\lambda}{r_c}\\right) (1) dr\n$$\n我们可以将常数（包括参数 $\\lambda$）从积分中提出来：\n$$\n\\Delta P(\\lambda) = -\\frac{2\\pi\\rho^2\\lambda}{3r_c} \\int_{r_{t}}^{r_{c}} r^3 dr\n$$\n现在，我们计算 $r^3$ 的定积分：\n$$\n\\int_{r_{t}}^{r_{c}} r^3 dr = \\left[ \\frac{r^4}{4} \\right]_{r_{t}}^{r_{c}} = \\frac{r_c^4}{4} - \\frac{r_t^4}{4} = \\frac{1}{4}(r_c^4 - r_t^4)\n$$\n将此结果代回 $\\Delta P(\\lambda)$ 的表达式中：\n$$\n\\Delta P(\\lambda) = -\\frac{2\\pi\\rho^2\\lambda}{3r_c} \\left( \\frac{r_c^4 - r_t^4}{4} \\right)\n$$\n简化数值因子，得到由尾势引起的维里压力变化的最终解析表达式：\n$$\n\\Delta P(\\lambda) = -\\frac{2\\pi\\rho^2\\lambda(r_c^4 - r_t^4)}{12r_c} = -\\frac{\\pi\\rho^2\\lambda}{6r_c} (r_c^4 - r_t^4)\n$$\n该表达式仅依赖于指定的参数 $\\rho$、$r_{t}$、$r_{c}$ 和 $\\lambda$。问题说明，如果代入数值，压力变化应以焦耳每立方米为单位。单位检查证实了这一点：当 $\\rho$ 的单位是 $m^{-3}$，$\\lambda$ 的单位是 $J$，$r_t, r_c$ 的单位是 $m$ 时，$\\Delta P(\\lambda)$ 的单位是 $\\frac{(m^{-3})^2 J}{m} (m^4) = J \\cdot m^{-3}$。由于 $1 \\, J = 1 \\, N \\cdot m$，这等同于 $N \\cdot m^{-2}$，即帕斯卡（Pa），压力的标准国际单位。",
            "answer": "$$\n\\boxed{-\\frac{\\pi \\rho^{2} \\lambda}{6 r_{c}} (r_{c}^{4} - r_{t}^{4})}\n$$"
        }
    ]
}