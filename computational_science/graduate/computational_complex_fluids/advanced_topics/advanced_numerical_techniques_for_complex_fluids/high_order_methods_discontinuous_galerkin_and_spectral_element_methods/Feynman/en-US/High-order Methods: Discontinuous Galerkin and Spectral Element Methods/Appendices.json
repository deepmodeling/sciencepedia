{
    "hands_on_practices": [
        {
            "introduction": "The foundation of Discontinuous Galerkin (DG) and Spectral Element Methods (SEM) lies in the local approximation of solutions using polynomials on simple reference elements. A critical step in this process is the numerical evaluation of integrals that arise from the weak form of a governing differential equation. This practice explores the intimate connection between the choice of a numerical quadrature rule and the structure of the resulting algebraic system.\n\nBy deriving the discrete $L^2$ inner product and constructing the corresponding mass matrix, you will discover how a strategic choice of quadrature points can lead to a diagonal matrix. This property, often called \"mass lumping,\" dramatically simplifies the inversion of the mass matrix, which is a cornerstone of efficient explicit time-stepping schemes . This exercise builds an essential understanding of how the core discrete operators in high-order methods are constructed and optimized.",
            "id": "4089553",
            "problem": "Consider a one-dimensional reference element $[-1,1]$ used in high-order discretizations for computational complex fluids, such as the Discontinuous Galerkin (DG) method and the Spectral Element Method (SEM). Let $p$ denote the polynomial degree of the local approximation space. For functions $f$ and $g$ defined on $[-1,1]$, the continuous $L^{2}$ inner product is defined by\n$$\n(f,g)_{L^{2}([-1,1])} = \\int_{-1}^{1} f(x)\\,g(x)\\,dx.\n$$\nTo construct a discrete inner product that is exact for polynomial integrands up to degree $2p$, assume an $n$-point quadrature rule $(x_{k},w_{k})_{k=1}^{n}$ on $[-1,1]$ with nodes $x_{k}$ and positive weights $w_{k}$ such that\n$$\n\\int_{-1}^{1} q(x)\\,dx = \\sum_{k=1}^{n} w_{k}\\,q(x_{k})\n$$\nfor every polynomial $q$ of degree at most $2p$. Consider the nodal Lagrange basis $\\{\\ell_{i}(x)\\}_{i=1}^{n}$ of degree $p$ associated with the interpolation nodes $\\{x_{i}\\}_{i=1}^{n}$, where $\\ell_{i}(x_{j}) = \\delta_{ij}$ for $1 \\le i,j \\le n$.\n\nStarting from the definition of the $L^{2}$ inner product and the exactness property of the quadrature, derive the discrete inner product\n$$\n(f,g)_{h} \\equiv \\sum_{k=1}^{n} w_{k}\\,f(x_{k})\\,g(x_{k}),\n$$\nand then construct the corresponding element mass matrix $M$ with entries\n$$\nM_{ij} = (\\ell_{i},\\ell_{j})_{h}.\n$$\nShow under these assumptions that $M$ is diagonal when the quadrature nodes coincide with the interpolation nodes. Finally, specialize to $p=3$ on $[-1,1]$ using the $n=p+1=4$ point Gauss–Legendre quadrature (which is exact for polynomials up to degree $2n-1=7 \\ge 2p$). Compute the four diagonal entries of the mass matrix in exact closed form, ordering them by the nodes from left to right on $[-1,1]$.\n\nProvide exact values with no rounding. Express your final answer as a row matrix using the LaTeX `\\begin{pmatrix}` environment in the order $\\{x_{1},x_{2},x_{3},x_{4}\\}$ where $x_{1}<x_{2}<x_{3}<x_{4}$ on $[-1,1]$. No physical units are needed.",
            "solution": "The problem asks for several derivations and a specific calculation related to high-order numerical methods on a one-dimensional reference element $[-1,1]$. We will address each part of the problem in sequence.\n\nFirst, we validate the problem statement.\nAll givens are extracted and analyzed.\n- Domain: $[-1,1]$.\n- Polynomial degree: $p$.\n- Continuous inner product: $(f,g)_{L^{2}([-1,1])} = \\int_{-1}^{1} f(x)\\,g(x)\\,dx$.\n- Quadrature rule: An $n$-point rule $(x_{k},w_{k})_{k=1}^{n}$ exact for polynomials of degree at most $2p$.\n- Lagrange basis: $\\{\\ell_{i}(x)\\}_{i=1}^{n}$ of degree $p$ on $n$ nodes $\\{x_i\\}$, with $\\ell_{i}(x_{j})=\\delta_{ij}$. For a unique polynomial of degree $p$ to be defined by $n$ nodal values, we must have $n=p+1$.\n- Discrete inner product: $(f,g)_{h} = \\sum_{k=1}^{n} w_{k}\\,f(x_{k})\\,g(x_{k})$.\n- Mass matrix: $M_{ij} = (\\ell_{i},\\ell_{j})_{h}$.\n- Specific case: $p=3$, $n=p+1=4$ point Gauss-Legendre quadrature, exact for polynomials of degree up to $2n-1=7$.\n\nThe problem is scientifically grounded in the theory of finite element and spectral methods. It is well-posed, objective, and internally consistent. The relationship $n=p+1$ is standard for nodal bases of degree $p$. The exactness requirement for the quadrature ($2p=6$) is satisfied by the chosen $4$-point Gauss-Legendre rule (exact up to degree $7$). The problem is therefore valid.\n\nPart 1: Derivation of the discrete inner product\nThe continuous $L^{2}$ inner product of two basis functions $\\ell_i(x)$ and $\\ell_j(x)$ is given by\n$$\n(\\ell_i, \\ell_j)_{L^2} = \\int_{-1}^{1} \\ell_i(x) \\ell_j(x) dx.\n$$\nThe basis functions $\\{\\ell_i(x)\\}_{i=1}^{n}$ are polynomials of degree $p=n-1$. The product $\\ell_i(x)\\ell_j(x)$ is therefore a polynomial of degree at most $2p$.\nThe problem states that we use a quadrature rule that is exact for any polynomial $q(x)$ of degree at most $2p$:\n$$\n\\int_{-1}^{1} q(x)\\,dx = \\sum_{k=1}^{n} w_{k}\\,q(x_{k}).\n$$\nApplying this quadrature rule to the integrand $\\ell_i(x)\\ell_j(x)$, which is a polynomial of degree at most $2p$, we find that the integral is computed exactly:\n$$\n(\\ell_i, \\ell_j)_{L^2} = \\int_{-1}^{1} \\ell_i(x) \\ell_j(x) dx = \\sum_{k=1}^{n} w_{k}\\,\\ell_i(x_{k})\\,\\ell_j(x_{k}).\n$$\nThe expression on the right is precisely the discrete inner product $(\\ell_i, \\ell_j)_{h}$ as defined in the problem statement. This discrete inner product is thus constructed to be an exact representation of the continuous $L^2$ inner product when applied to pairs of basis functions.\n\nPart 2: Construction of the element mass matrix $M$\nThe element mass matrix $M$ has entries $M_{ij}$ given by the discrete inner product of the basis functions:\n$$\nM_{ij} = (\\ell_{i},\\ell_{j})_{h} = \\sum_{k=1}^{n} w_{k}\\,\\ell_{i}(x_{k})\\,\\ell_{j}(x_{k}).\n$$\n\nPart 3: Proof that $M$ is diagonal\nThe problem adds the crucial assumption that the quadrature nodes $\\{x_k\\}_{k=1}^n$ coincide with the interpolation nodes $\\{x_i\\}_{i=1}^n$. We can therefore set $x_k=x_i$ for $k=i$ (after appropriate ordering). The defining property of the Lagrange basis is its value at the interpolation nodes: $\\ell_i(x_j) = \\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta.\nSubstituting this property into the expression for $M_{ij}$ (with quadrature nodes $\\{x_k\\}$ being the same set as $\\{x_j\\}$):\n$$\nM_{ij} = \\sum_{k=1}^{n} w_{k}\\,\\ell_{i}(x_{k})\\,\\ell_{j}(x_{k}).\n$$\nThe terms in the sum are products of the form $\\ell_{i}(x_{k})\\ell_{j}(x_{k})$. The property $\\ell_i(x_k)=\\delta_{ik}$ means $\\ell_i(x_k)$ is $1$ if $k=i$ and $0$ otherwise. Similarly, $\\ell_j(x_k)=\\delta_{jk}$.\nThe expression for $M_{ij}$ becomes:\n$$\nM_{ij} = \\sum_{k=1}^{n} w_{k}\\,\\delta_{ik}\\,\\delta_{jk}.\n$$\nIf $i \\neq j$, then for any given index $k$, it is impossible for both $k=i$ and $k=j$ to be true. Thus, the product $\\delta_{ik}\\delta_{jk}$ is always zero. This makes every term in the summation zero, so $M_{ij} = 0$ for $i \\neq j$.\nIf $i = j$, the expression becomes:\n$$\nM_{ii} = \\sum_{k=1}^{n} w_{k}\\,\\delta_{ik}\\,\\delta_{ik} = \\sum_{k=1}^{n} w_{k}\\,(\\delta_{ik})^2.\n$$\nSince $\\delta_{ik}$ is either $0$ or $1$, $(\\delta_{ik})^2 = \\delta_{ik}$.\n$$\nM_{ii} = \\sum_{k=1}^{n} w_{k}\\,\\delta_{ik}.\n$$\nThis sum has only one non-zero term, which occurs when $k=i$. The value of this term is $w_i \\cdot 1 = w_i$.\nTherefore, the entries of the mass matrix are $M_{ij} = w_i \\delta_{ij}$. This shows that $M$ is a diagonal matrix with the quadrature weights on its diagonal: $M = \\text{diag}(w_1, w_2, \\ldots, w_n)$.\n\nPart 4: Calculation of the diagonal entries for $p=3$\nWe specialize to the case where the polynomial degree is $p=3$. The basis consists of $n=p+1=4$ Lagrange polynomials. The quadrature rule used is the $4$-point Gauss-Legendre quadrature. The nodes $\\{x_k\\}$ are the roots of the Legendre polynomial of degree $4$, $P_4(x)$. The diagonal entries of the mass matrix are the corresponding quadrature weights, $M_{ii} = w_i$.\n\nThe nodes $x_k$ are the roots of $P_4(x) = \\frac{1}{8}(35x^4 - 30x^2 + 3)$. Setting $P_4(x)=0$ gives $35x^4 - 30x^2 + 3 = 0$. Let $y=x^2$. The quadratic equation for $y$ is $35y^2 - 30y + 3 = 0$, whose solutions are\n$$\ny = \\frac{30 \\pm \\sqrt{30^2 - 4(35)(3)}}{2(35)} = \\frac{30 \\pm \\sqrt{900-420}}{70} = \\frac{30 \\pm \\sqrt{480}}{70}.\n$$\nSince $\\sqrt{480} = \\sqrt{16 \\times 30} = 4\\sqrt{30}$, we have\n$$\ny = x^2 = \\frac{30 \\pm 4\\sqrt{30}}{70} = \\frac{15 \\pm 2\\sqrt{30}}{35}.\n$$\nThe four nodes, ordered from left to right, are $x_1 < x_2 < x_3 < x_4$:\n$x_{1,4} = \\mp\\sqrt{\\frac{15+2\\sqrt{30}}{35}}$ (outer nodes) and $x_{2,3} = \\mp\\sqrt{\\frac{15-2\\sqrt{30}}{35}}$ (inner nodes).\n\nThe Gauss-Legendre weights $w_k$ can be calculated using the formula $w_k = \\frac{2}{(1-x_k^2)[P_n'(x_k)]^2}$. For $n=4$, we have $P_4'(x) = \\frac{1}{8}(140x^3-60x) = \\frac{5x}{2}(7x^2-3)$.\nA more convenient formula for the weights can be derived. Since $y_k=x_k^2$ are roots of $35y^2-30y+3=0$, we can express $y^2 = (30y-3)/35$. This leads to the identity $w_k = \\frac{1}{15(y_k - y_k^2)}$.\nWe have $y_k - y_k^2 = y_k - \\frac{30y_k-3}{35} = \\frac{35y_k - 30y_k + 3}{35} = \\frac{5y_k+3}{35}$.\nSo, the weight is given by the simple expression:\n$$\nw_k = \\frac{1}{15 \\left( \\frac{5y_k+3}{35} \\right)} = \\frac{35}{15(5y_k+3)} = \\frac{7}{3(5y_k+3)}.\n$$\nWe now compute the two distinct weight values. For the outer nodes $x_1, x_4$, we use $y_{outer} = \\frac{15+2\\sqrt{30}}{35}$:\n$$\nw_{outer} = \\frac{7}{3\\left(5\\left(\\frac{15+2\\sqrt{30}}{35}\\right)+3\\right)} = \\frac{7}{3\\left(\\frac{15+2\\sqrt{30}}{7}+3\\right)} = \\frac{7}{3\\left(\\frac{15+2\\sqrt{30}+21}{7}\\right)} = \\frac{49}{3(36+2\\sqrt{30})} = \\frac{49}{6(18+\\sqrt{30})}.\n$$\nRationalizing the denominator:\n$$\nw_{outer} = \\frac{49(18-\\sqrt{30})}{6(18+\\sqrt{30})(18-\\sqrt{30})} = \\frac{49(18-\\sqrt{30})}{6(18^2 - 30)} = \\frac{49(18-\\sqrt{30})}{6(324 - 30)} = \\frac{49(18-\\sqrt{30})}{6(294)} = \\frac{49(18-\\sqrt{30})}{6(6 \\times 49)} = \\frac{18-\\sqrt{30}}{36}.\n$$\nFor the inner nodes $x_2, x_3$, we use $y_{inner} = \\frac{15-2\\sqrt{30}}{35}$:\n$$\nw_{inner} = \\frac{7}{3\\left(5\\left(\\frac{15-2\\sqrt{30}}{35}\\right)+3\\right)} = \\frac{7}{3\\left(\\frac{15-2\\sqrt{30}}{7}+3\\right)} = \\frac{7}{3\\left(\\frac{15-2\\sqrt{30}+21}{7}\\right)} = \\frac{49}{3(36-2\\sqrt{30})} = \\frac{49}{6(18-\\sqrt{30})}.\n$$\nRationalizing the denominator:\n$$\nw_{inner} = \\frac{49(18+\\sqrt{30})}{6(18-\\sqrt{30})(18+\\sqrt{30})} = \\frac{49(18+\\sqrt{30})}{6(18^2-30)} = \\frac{49(18+\\sqrt{30})}{6(294)} = \\frac{18+\\sqrt{30}}{36}.\n$$\nThe diagonal entries of the mass matrix are these weights, ordered by the nodes $x_1<x_2<x_3<x_4$:\n$M_{11} = w_1 = w_{outer} = \\frac{18-\\sqrt{30}}{36}$.\n$M_{22} = w_2 = w_{inner} = \\frac{18+\\sqrt{30}}{36}$.\n$M_{33} = w_3 = w_{inner} = \\frac{18+\\sqrt{30}}{36}$.\n$M_{44} = w_4 = w_{outer} = \\frac{18-\\sqrt{30}}{36}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{18-\\sqrt{30}}{36} & \\frac{18+\\sqrt{30}}{36} & \\frac{18+\\sqrt{30}}{36} & \\frac{18-\\sqrt{30}}{36} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Real-world engineering and physics problems rarely take place in simple square or cubic domains. To apply the power of high-order methods to complex geometries, we employ isoparametric mappings that deform simple reference elements, like the square $[-1,1] \\times [-1,1]$, into the curved physical elements that mesh the actual domain. This geometric transformation, however, requires that we correctly translate differential operators from one coordinate system to the other.\n\nThis exercise provides a first-principles derivation of the operator that transforms the gradient from reference coordinates $\\boldsymbol{\\xi}$ to physical coordinates $\\boldsymbol{x}$. You will see how the multivariable chain rule gives rise to the inverse transpose of the Jacobian matrix, $\\left(\\boldsymbol{J}^T\\right)^{-1}$, as the fundamental transformation operator . Mastering this procedure is a prerequisite for implementing DG or SEM on any non-trivial geometry, making this a crucial hands-on skill.",
            "id": "4089509",
            "problem": "Consider a two-dimensional curvilinear isoparametric mapping used in high-order Discontinuous Galerkin (DG) and Spectral Element Method (SEM) discretizations for computational complex fluids. Let the reference element be the square $[-1,1] \\times [-1,1]$ with coordinates $\\boldsymbol{\\xi} = (\\xi,\\eta)$, and let the physical coordinates be $\\boldsymbol{x} = (x,y)$ given by the smooth mapping $\\boldsymbol{x} = \\boldsymbol{X}(\\boldsymbol{\\xi})$ with components\n$$\nx(\\xi,\\eta) = \\xi + \\frac{1}{5}\\,\\eta + \\frac{1}{10}\\,\\xi\\,\\eta + \\frac{1}{20}\\,(1 - \\xi^{2}) + \\frac{1}{12}\\,\\eta^{2},\n$$\n$$\ny(\\xi,\\eta) = \\frac{3}{10}\\,\\xi + \\eta + \\frac{3}{25}\\,\\xi\\,\\eta + \\frac{7}{100}\\,\\xi^{2} - \\frac{1}{20}\\,(1 - \\eta^{2}).\n$$\nLet $\\phi(\\xi,\\eta)$ be a tensor-product spectral basis function constructed from Legendre polynomials, specifically\n$$\n\\phi(\\xi,\\eta) = L_{2}(\\xi)\\,L_{3}(\\eta),\n$$\nwhere $L_{2}(\\xi) = \\frac{1}{2}\\,(3\\xi^{2} - 1)$ and $L_{3}(\\eta) = \\frac{1}{2}\\,(5\\eta^{3} - 3\\eta)$.\n\nStarting exclusively from the multivariable chain rule and the definition of the Jacobian matrix of the mapping, perform the following:\n\n1. Derive the operator that maps the reference gradient $\\nabla_{\\boldsymbol{\\xi}}$ of a scalar field to its physical gradient $\\nabla_{\\boldsymbol{x}}$ under the mapping $\\boldsymbol{X}(\\boldsymbol{\\xi})$. Your derivation must show how the transformation arises from first principles, and it must explicitly identify the role of the Jacobian matrix and its inverse transpose, without invoking any pre-stated transformation formulas.\n\n2. Using your result, evaluate the physical gradient $\\nabla_{\\boldsymbol{x}} \\phi$ at the reference point $(\\xi,\\eta) = \\left(\\frac{1}{2}, -\\frac{1}{3}\\right)$. Express your final answer as exact rational numbers.\n\nProvide the final result for the physical gradient as a row vector using the LaTeX `\\begin{pmatrix}` environment. No rounding is required and no physical units are involved.",
            "solution": "The problem is validated as well-posed, scientifically grounded, and self-contained. It is a standard problem in the framework of computational methods for partial differential equations, specifically concerning the geometric transformations required in finite element and spectral methods. We can therefore proceed with the solution.\n\nThe problem is divided into two parts: first, a derivation of the gradient transformation operator from first principles, and second, the application of this operator to compute a specific physical gradient.\n\n**Part 1: Derivation of the Gradient Transformation Operator**\n\nLet $u$ be a scalar field defined over the physical domain, such that its value at a point $\\boldsymbol{x}=(x,y)$ is $u(\\boldsymbol{x})$. The mapping $\\boldsymbol{x} = \\boldsymbol{X}(\\boldsymbol{\\xi})$ provides a relationship between the physical coordinates $\\boldsymbol{x}$ and the reference coordinates $\\boldsymbol{\\xi}=(\\xi,\\eta)$. The basis function $\\phi(\\boldsymbol{\\xi})$ is defined on the reference element. We consider the scalar field $u(\\boldsymbol{x})$ whose representation on the reference element is $\\phi(\\boldsymbol{\\xi})$, i.e., $u(\\boldsymbol{X}(\\boldsymbol{\\xi})) = \\phi(\\boldsymbol{\\xi})$. We seek to find the physical gradient, $\\nabla_{\\boldsymbol{x}} u = \\begin{pmatrix} \\partial u / \\partial x \\\\ \\partial u / \\partial y \\end{pmatrix}$, in terms of the reference gradient, $\\nabla_{\\boldsymbol{\\xi}} \\phi = \\begin{pmatrix} \\partial \\phi / \\partial \\xi \\\\ \\partial \\phi / \\partial \\eta \\end{pmatrix}$. For notational simplicity as is common in the field, we will denote $\\nabla_{\\boldsymbol{x}}u$ by $\\nabla_{\\boldsymbol{x}}\\phi$.\n\nThe derivation proceeds from the multivariable chain rule. The partial derivatives of $\\phi$ with respect to the reference coordinates $\\xi$ and $\\eta$ are expressed in terms of the partial derivatives of $u$ with respect to the physical coordinates $x$ and $y$:\n$$\n\\frac{\\partial \\phi}{\\partial \\xi} = \\frac{\\partial u}{\\partial x} \\frac{\\partial x}{\\partial \\xi} + \\frac{\\partial u}{\\partial y} \\frac{\\partial y}{\\partial \\xi}\n$$\n$$\n\\frac{\\partial \\phi}{\\partial \\eta} = \\frac{\\partial u}{\\partial x} \\frac{\\partial x}{\\partial \\eta} + \\frac{\\partial u}{\\partial y} \\frac{\\partial y}{\\partial \\eta}\n$$\nThis is a system of two linear equations for the unknown components of the physical gradient, $\\frac{\\partial u}{\\partial x}$ and $\\frac{\\partial u}{\\partial y}$. We can write this system in matrix form:\n$$\n\\begin{pmatrix} \\frac{\\partial \\phi}{\\partial \\xi} \\\\ \\frac{\\partial \\phi}{\\partial \\eta} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial x}{\\partial \\xi} & \\frac{\\partial y}{\\partial \\xi} \\\\ \\frac{\\partial x}{\\partial \\eta} & \\frac{\\partial y}{\\partial \\eta} \\end{pmatrix} \\begin{pmatrix} \\frac{\\partial u}{\\partial x} \\\\ \\frac{\\partial u}{\\partial y} \\end{pmatrix}\n$$\nThe vector on the left is the reference gradient, $\\nabla_{\\boldsymbol{\\xi}} \\phi$. The vector on the far right is the physical gradient, $\\nabla_{\\boldsymbol{x}} u$. The matrix in the middle is recognized as the transpose of the Jacobian matrix of the coordinate transformation $\\boldsymbol{X}(\\boldsymbol{\\xi})$. The Jacobian matrix $\\boldsymbol{J}$ is defined as:\n$$\n\\boldsymbol{J} = \\frac{\\partial\\boldsymbol{x}}{\\partial\\boldsymbol{\\xi}} = \\begin{pmatrix} \\frac{\\partial x}{\\partial \\xi} & \\frac{\\partial x}{\\partial \\eta} \\\\ \\frac{\\partial y}{\\partial \\xi} & \\frac{\\partial y}{\\partial \\eta} \\end{pmatrix}\n$$\nIts transpose is:\n$$\n\\boldsymbol{J}^T = \\begin{pmatrix} \\frac{\\partial x}{\\partial \\xi} & \\frac{\\partial y}{\\partial \\xi} \\\\ \\frac{\\partial x}{\\partial \\eta} & \\frac{\\partial y}{\\partial \\eta} \\end{pmatrix}\n$$\nThus, the relationship between the gradients is:\n$$\n\\nabla_{\\boldsymbol{\\xi}} \\phi = \\boldsymbol{J}^T \\nabla_{\\boldsymbol{x}} \\phi\n$$\nTo find the physical gradient, we must invert this relationship. Assuming the Jacobian determinant is non-zero, the matrix $\\boldsymbol{J}^T$ is invertible. We can left-multiply by $(\\boldsymbol{J}^T)^{-1}$:\n$$\n\\nabla_{\\boldsymbol{x}} \\phi = (\\boldsymbol{J}^T)^{-1} \\nabla_{\\boldsymbol{\\xi}} \\phi\n$$\nThis equation defines the operator that maps the reference gradient to the physical gradient. The operator is multiplication by the inverse transpose of the Jacobian matrix of the mapping. This completes the derivation from first principles.\n\n**Part 2: Evaluation of the Physical Gradient**\n\nWe now apply this result to the specific functions and point given in the problem.\n\nFirst, we compute the components of the Jacobian matrix $\\boldsymbol{J}$ by differentiating the mapping functions:\n$x(\\xi,\\eta) = \\xi + \\frac{1}{5}\\,\\eta + \\frac{1}{10}\\,\\xi\\,\\eta + \\frac{1}{20}\\,(1 - \\xi^{2}) + \\frac{1}{12}\\,\\eta^{2}$\n$y(\\xi,\\eta) = \\frac{3}{10}\\,\\xi + \\eta + \\frac{3}{25}\\,\\xi\\,\\eta + \\frac{7}{100}\\,\\xi^{2} - \\frac{1}{20}\\,(1 - \\eta^{2})$\n\nThe partial derivatives are:\n$J_{11} = \\frac{\\partial x}{\\partial \\xi} = 1 + \\frac{1}{10}\\eta - \\frac{1}{10}\\xi$\n$J_{12} = \\frac{\\partial x}{\\partial \\eta} = \\frac{1}{5} + \\frac{1}{10}\\xi + \\frac{1}{6}\\eta$\n$J_{21} = \\frac{\\partial y}{\\partial \\xi} = \\frac{3}{10} + \\frac{3}{25}\\eta + \\frac{7}{50}\\xi$\n$J_{22} = \\frac{\\partial y}{\\partial \\eta} = 1 + \\frac{3}{25}\\xi + \\frac{1}{10}\\eta$\n\nNext, we evaluate these components at the specified reference point $(\\xi,\\eta) = \\left(\\frac{1}{2}, -\\frac{1}{3}\\right)$:\n$J_{11} = 1 + \\frac{1}{10}(-\\frac{1}{3}) - \\frac{1}{10}(\\frac{1}{2}) = 1 - \\frac{1}{30} - \\frac{1}{20} = \\frac{60 - 2 - 3}{60} = \\frac{55}{60} = \\frac{11}{12}$\n$J_{12} = \\frac{1}{5} + \\frac{1}{10}(\\frac{1}{2}) + \\frac{1}{6}(-\\frac{1}{3}) = \\frac{1}{5} + \\frac{1}{20} - \\frac{1}{18} = \\frac{36 + 9 - 10}{180} = \\frac{35}{180} = \\frac{7}{36}$\n$J_{21} = \\frac{3}{10} + \\frac{3}{25}(-\\frac{1}{3}) + \\frac{7}{50}(\\frac{1}{2}) = \\frac{3}{10} - \\frac{1}{25} + \\frac{7}{100} = \\frac{30 - 4 + 7}{100} = \\frac{33}{100}$\n$J_{22} = 1 + \\frac{3}{25}(\\frac{1}{2}) + \\frac{1}{10}(-\\frac{1}{3}) = 1 + \\frac{3}{50} - \\frac{1}{30} = \\frac{150 + 9 - 5}{150} = \\frac{154}{150} = \\frac{77}{75}$\n\nSo the Jacobian matrix at the point is $\\boldsymbol{J} = \\begin{pmatrix} \\frac{11}{12} & \\frac{7}{36} \\\\ \\frac{33}{100} & \\frac{77}{75} \\end{pmatrix}$.\n\nThe determinant of the Jacobian is:\n$\\det(\\boldsymbol{J}) = (\\frac{11}{12})(\\frac{77}{75}) - (\\frac{7}{36})(\\frac{33}{100}) = \\frac{847}{900} - \\frac{231}{3600} = \\frac{4 \\cdot 847 - 231}{3600} = \\frac{3388 - 231}{3600} = \\frac{3157}{3600}$.\nSince the determinant is non-zero, the inverse exists.\n\nThe transformation requires $(\\boldsymbol{J}^T)^{-1} = (\\boldsymbol{J}^{-1})^T$. We first find $\\boldsymbol{J}^{-1}$:\n$\\boldsymbol{J}^{-1} = \\frac{1}{\\det(\\boldsymbol{J})} \\begin{pmatrix} J_{22} & -J_{12} \\\\ -J_{21} & J_{11} \\end{pmatrix} = \\frac{3600}{3157} \\begin{pmatrix} \\frac{77}{75} & -\\frac{7}{36} \\\\ -\\frac{33}{100} & \\frac{11}{12} \\end{pmatrix}$.\nThe inverse transpose is therefore:\n$(\\boldsymbol{J}^T)^{-1} = (\\boldsymbol{J}^{-1})^T = \\frac{3600}{3157} \\begin{pmatrix} \\frac{77}{75} & -\\frac{33}{100} \\\\ -\\frac{7}{36} & \\frac{11}{12} \\end{pmatrix}$.\n\nNext, we compute the reference gradient of $\\phi(\\xi,\\eta) = L_{2}(\\xi)\\,L_{3}(\\eta)$:\nThe components of the gradient are $\\frac{\\partial \\phi}{\\partial \\xi} = L_{2}'(\\xi)\\,L_{3}(\\eta)$ and $\\frac{\\partial \\phi}{\\partial \\eta} = L_{2}(\\xi)\\,L_{3}'(\\eta)$.\n$L_{2}(\\xi) = \\frac{1}{2}(3\\xi^2 - 1) \\implies L_{2}'(\\xi) = 3\\xi$\n$L_{3}(\\eta) = \\frac{1}{2}(5\\eta^3 - 3\\eta) \\implies L_{3}'(\\eta) = \\frac{1}{2}(15\\eta^2 - 3)$\n\nEvaluate these functions and their derivatives at $(\\xi,\\eta) = (\\frac{1}{2}, -\\frac{1}{3})$:\n$L_{2}(\\frac{1}{2}) = \\frac{1}{2}(3(\\frac{1}{2})^2 - 1) = \\frac{1}{2}(\\frac{3}{4} - 1) = -\\frac{1}{8}$\n$L_{2}'(\\frac{1}{2}) = 3(\\frac{1}{2}) = \\frac{3}{2}$\n$L_{3}(-\\frac{1}{3}) = \\frac{1}{2}(5(-\\frac{1}{3})^3 - 3(-\\frac{1}{3})) = \\frac{1}{2}(-\\frac{5}{27} + 1) = \\frac{1}{2}(\\frac{22}{27}) = \\frac{11}{27}$\n$L_{3}'(-\\frac{1}{3}) = \\frac{1}{2}(15(-\\frac{1}{3})^2 - 3) = \\frac{1}{2}(\\frac{15}{9} - 3) = \\frac{1}{2}(\\frac{5}{3} - \\frac{9}{3}) = \\frac{1}{2}(-\\frac{4}{3}) = -\\frac{2}{3}$\n\nNow compute the components of the reference gradient:\n$\\frac{\\partial \\phi}{\\partial \\xi} = L_{2}'(\\frac{1}{2})\\,L_{3}(-\\frac{1}{3}) = (\\frac{3}{2})(\\frac{11}{27}) = \\frac{33}{54} = \\frac{11}{18}$\n$\\frac{\\partial \\phi}{\\partial \\eta} = L_{2}(\\frac{1}{2})\\,L_{3}'(-\\frac{1}{3}) = (-\\frac{1}{8})(-\\frac{2}{3}) = \\frac{2}{24} = \\frac{1}{12}$\nThe reference gradient is $\\nabla_{\\boldsymbol{\\xi}} \\phi = \\begin{pmatrix} 11/18 \\\\ 1/12 \\end{pmatrix}$.\n\nFinally, we compute the physical gradient $\\nabla_{\\boldsymbol{x}} \\phi = (\\boldsymbol{J}^T)^{-1} \\nabla_{\\boldsymbol{\\xi}} \\phi$:\n$$\n\\nabla_{\\boldsymbol{x}} \\phi = \\frac{3600}{3157} \\begin{pmatrix} \\frac{77}{75} & -\\frac{33}{100} \\\\ -\\frac{7}{36} & \\frac{11}{12} \\end{pmatrix} \\begin{pmatrix} \\frac{11}{18} \\\\ \\frac{1}{12} \\end{pmatrix}\n$$\nThe x-component is:\n$(\\nabla_{\\boldsymbol{x}} \\phi)_x = \\frac{3600}{3157} \\left( (\\frac{77}{75})(\\frac{11}{18}) - (\\frac{33}{100})(\\frac{1}{12}) \\right) = \\frac{3600}{3157} \\left( \\frac{847}{1350} - \\frac{33}{1200} \\right) = \\frac{3600}{3157} \\left( \\frac{847}{1350} - \\frac{11}{400} \\right) = \\frac{3600}{3157} \\left( \\frac{6776 - 297}{10800} \\right) = \\frac{1}{3} \\frac{6479}{3157} = \\frac{589}{861}$.\n\nThe y-component is:\n$(\\nabla_{\\boldsymbol{x}} \\phi)_y = \\frac{3600}{3157} \\left( -(\\frac{7}{36})(\\frac{11}{18}) + (\\frac{11}{12})(\\frac{1}{12}) \\right) = \\frac{3600}{3157} \\left( -\\frac{77}{648} + \\frac{11}{144} \\right) = \\frac{3600}{3157} \\left( \\frac{-154 + 99}{1296} \\right) = \\frac{3600}{1296} \\frac{-55}{3157} = \\frac{25}{9} \\frac{-55}{3157} = -\\frac{1375}{28413} = -\\frac{125}{2583}$.\n\nThe physical gradient at the specified point is a vector whose components are these two values.\n$\\nabla_{\\boldsymbol{x}} \\phi = \\left(\\frac{589}{861}, -\\frac{125}{2583}\\right)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{589}{861} & -\\frac{125}{2583} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "A primary motivation for using high-order methods is their potential to achieve high accuracy with superior computational efficiency compared to low-order methods. This efficiency is not automatic; it hinges on an implementation that avoids the formation of large, dense global matrices. This practice explores the algorithmic key to the performance of modern spectral element codes: tensor-product evaluation, also known as sum-factorization.\n\nYou will derive and compare the computational cost, measured in floating-point operations (flops), for applying a stiffness operator in two different ways: first, via a naive multiplication with a pre-assembled dense matrix, and second, by exploiting the tensor-product structure of the basis functions and operators . The resulting ratio of these costs will starkly reveal the asymptotic advantage of sum-factorization, explaining why it is indispensable for making high-order methods practical for large-scale, three-dimensional simulations.",
            "id": "4089482",
            "problem": "Consider the viscous diffusion contribution to the momentum equations in computational complex fluids, discretized on a single hexahedral element using a high-order tensor-product basis. Let the element mapping be affine and the viscosity be spatially constant, so that in the reference coordinates the weak-form stiffness operator is represented, under Gauss–Lobatto–Legendre quadrature and Lagrange interpolation, by the sum of three tensor-product terms. Specifically, for polynomial degree $p$ per coordinate direction, let $n = p+1$ denote the number of nodes in one dimension, and let $D \\in \\mathbb{R}^{n \\times n}$ be the one-dimensional derivative matrix, and $W \\in \\mathbb{R}^{n \\times n}$ the diagonal matrix of one-dimensional quadrature weights. The three-dimensional stiffness operator on nodal values $u \\in \\mathbb{R}^{n \\times n \\times n}$ then acts, in a standard strong-symmetric spectral element implementation, as\n$$\nA u \\;=\\; \\left(D^{\\top} \\otimes I \\otimes I\\right)\\left(W \\otimes W \\otimes W\\right)\\left(D \\otimes I \\otimes I\\right) u \\;+\\; \\left(I \\otimes D^{\\top} \\otimes I\\right)\\left(W \\otimes W \\otimes W\\right)\\left(I \\otimes D \\otimes I\\right) u \\;+\\; \\left(I \\otimes I \\otimes D^{\\top}\\right)\\left(W \\otimes W \\otimes W\\right)\\left(I \\otimes I \\otimes D\\right) u,\n$$\nwhere $I \\in \\mathbb{R}^{n \\times n}$ is the identity matrix, and $\\otimes$ denotes the Kronecker product. Assume a floating-point operation (flop) model in which each scalar multiplication or addition counts as $1$ flop, and ignore memory traffic and coefficient precomputation costs.\n\nUsing only this tensor-product structure and first principles of high-order spectral element evaluation, derive the total flop count required to apply $A$ to a nodal vector $u$ on this single hexahedral element via sum-factorization (tensor-product evaluation). Then derive the flop count required to apply the same operator via naive dense matrix multiplication using a preassembled dense stiffness matrix of size $N \\times N$ with $N = n^{3}$. Finally, provide the simplified symbolic expression for the ratio $R(p)$ of the naive dense matrix-multiplication flop count to the tensor-product flop count, expressed solely as a function of $p$.\n\nYour final answer must be this ratio $R(p)$ as a single closed-form analytic expression. No numerical evaluation is required and no rounding is permitted.",
            "solution": "This problem asks for a comparison of the computational complexity of applying a three-dimensional stiffness operator in two ways: via sum-factorization and via naive dense matrix-vector multiplication. We will derive the flop count for each and then find their ratio.\n\nFirst, we analyze the cost of the **tensor-product evaluation (sum-factorization)** method. The total operator is $A = A_x + A_y + A_z$. The total cost is the sum of applying each directional component and then adding the results. By symmetry, we only need to analyze the cost of one component, say $A_x u = (D^{\\top} \\otimes I \\otimes I)(W \\otimes W \\otimes W)(D \\otimes I \\otimes I) u$, and multiply it by three.\nThe action of $(D \\otimes I \\otimes I)$ on the $n \\times n \\times n$ nodal data array $u$ corresponds to applying the dense $n \\times n$ matrix $D$ to $n^2$ vectors of length $n$. A single dense matrix-vector product costs $n(2n-1) = 2n^2-n$ flops. Thus, this first step costs $n^2(2n^2-n) = 2n^4-n^3$ flops. The next step, applying the diagonal operator $(W \\otimes W \\otimes W)$, is an element-wise multiplication on the $n^3$ nodal values, costing $n^3$ flops. The final step, applying $(D^{\\top} \\otimes I \\otimes I)$, is computationally identical to the first step, costing another $2n^4-n^3$ flops. The total cost for one directional term is therefore $(2n^4-n^3) + n^3 + (2n^4-n^3) = 4n^4-n^3$.\nThe cost for all three directions is $3(4n^4-n^3) = 12n^4-3n^3$. Adding the three resulting vectors of size $n^3$ requires two vector additions, costing $2n^3$ flops.\nThe total sum-factorization cost is $Flop_{SF} = (12n^4-3n^3) + 2n^3 = 12n^4 - n^3$.\n\nSecond, we consider the **naive dense matrix multiplication**. Here, the operator is assembled into a single dense matrix of size $N \\times N$, where $N=n^3$. The cost to multiply this matrix by a vector of length $N$ is $N(2N-1) = 2N^2 - N$ flops. Substituting $N=n^3$, we get $Flop_{dense} = 2(n^3)^2 - n^3 = 2n^6 - n^3$.\n\nFinally, we compute the **ratio $R(p)$**. The ratio of the costs is:\n$$ R(n) = \\frac{Flop_{dense}}{Flop_{SF}} = \\frac{2n^6 - n^3}{12n^4 - n^3} = \\frac{n^3(2n^3 - 1)}{n^3(12n - 1)} = \\frac{2n^3 - 1}{12n - 1} $$\nTo express this in terms of the polynomial degree $p$, we substitute $n=p+1$:\n$$ R(p) = \\frac{2(p+1)^3 - 1}{12(p+1) - 1} = \\frac{2(p^3 + 3p^2 + 3p + 1) - 1}{12p + 12 - 1} = \\frac{2p^3 + 6p^2 + 6p + 1}{12p + 11} $$\nThis expression represents the computational advantage of sum-factorization over naive matrix multiplication.",
            "answer": "$$\\boxed{\\frac{2p^{3} + 6p^{2} + 6p + 1}{12p + 11}}$$"
        }
    ]
}