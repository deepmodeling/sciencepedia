{
    "hands_on_practices": [
        {
            "introduction": "A crucial first step in developing any model, whether full-order or reduced, is to understand the fundamental physics governing the system. By performing a nondimensionalization of the governing partial differential equations, we can distill complex interactions into a few key dimensionless groups. This practice guides you through deriving the Péclet ($\\mathrm{Pe}$) and Damköhler ($\\mathrm{Da}$) numbers for a classic reactive transport problem, providing insight into how these groups control the system's behavior and influence the smoothness of the solution landscape that a surrogate model must learn .",
            "id": "4102078",
            "problem": "A one-dimensional saturated porous column of length $L$ transports and reacts a dissolved species with concentration $c(x,t)$ under steady flow. The governing transport arises from conservation of mass, linear advection, Fickian dispersion, and a first-order reaction. Let the Darcy velocity be $q$ and the porosity be $\\phi$, so that the interstitial (pore) velocity is $u = q/\\phi$. The effective hydrodynamic dispersion coefficient is $D$ (assumed constant), and the homogeneous first-order reaction rate constant is $k$ (assumed constant). The dimensional governing equation for $c(x,t)$ on $x \\in [0,L]$ and $t \\geq 0$ is obtained from mass conservation and linear flux laws, with an inlet Dirichlet boundary condition $c(0,t) = C_{\\mathrm{in}}$ and an outlet convective boundary condition at $x=L$. Assume an initially solute-free column $c(x,0)=0$.\n\nStarting from these physical statements and without invoking any pre-derived dimensionless formulas, perform a nondimensionalization of the governing equation using a characteristic length $L$, an inlet concentration $C_{\\mathrm{in}}$, and a time scale consistent with the advective transport implied by $u$. Identify, in closed form, the Peclet number $\\mathrm{Pe}$ and the Damkohler number $\\mathrm{Da}$ that arise from this nondimensionalization.\n\nThen, using the resulting dimensionless model, discuss from first principles how the magnitudes of $\\mathrm{Pe}$ and $\\mathrm{Da}$ influence (i) the smoothness of the parameter-to-solution map that surrogate models attempt to learn, and (ii) the stability properties of Reduced-Order Models (ROMs), such as those built by Proper Orthogonal Decomposition (POD) and Galerkin projection. Your discussion should connect the physical interpretation of $\\mathrm{Pe}$ and $\\mathrm{Da}$ to expected features in the solution (e.g., fronts, boundary layers, stiffness, non-normality) and to qualitative stability constraints in time integration.\n\nProvide your final answer as the row matrix containing the analytic expressions for $\\mathrm{Pe}$ and $\\mathrm{Da}$ in terms of $u$, $L$, $D$, and $k$. No numerical evaluation is required. Because the final answer is dimensionless, no unit specification is needed.",
            "solution": "The appropriate starting point is the local conservation of mass for the dissolved species in a saturated porous medium, which balances accumulation, advection, dispersion, and reaction. With constant porosity $\\phi$, Darcy velocity $q$, interstitial velocity $u = q/\\phi$, dispersion coefficient $D$, and first-order reaction rate $k$, the dimensional conservation equation for the aqueous concentration $c(x,t)$ is\n$$\n\\phi \\frac{\\partial c}{\\partial t} + \\frac{\\partial}{\\partial x}\\left(q\\,c - \\phi D \\frac{\\partial c}{\\partial x}\\right) \\;=\\; -\\phi k\\,c.\n$$\nAssuming $\\phi$ is constant and dividing by $\\phi$ yields\n$$\n\\frac{\\partial c}{\\partial t} + \\frac{\\partial}{\\partial x}\\left(u\\,c - D \\frac{\\partial c}{\\partial x}\\right) \\;=\\; -k\\,c,\n$$\nwhere $u \\equiv q/\\phi$ is the pore velocity. With constant $u$ and $D$ and neglecting spatial variability in coefficients, expanding the spatial derivative gives the linear advection–dispersion–reaction equation\n$$\n\\frac{\\partial c}{\\partial t} + u \\frac{\\partial c}{\\partial x} \\;=\\; D \\frac{\\partial^{2} c}{\\partial x^{2}} \\;-\\; k\\,c.\n$$\nBoundary conditions are $c(0,t) = C_{\\mathrm{in}}$ and a convective (or zero-diffusive-flux) outlet at $x=L$, and the initial condition is $c(x,0) = 0$.\n\nTo nondimensionalize, introduce characteristic scales for length, time, and concentration. Take the column length $L$ as the characteristic length and the inlet concentration $C_{\\mathrm{in}}$ as the concentration scale. A physically meaningful advective time scale is the residence time based on pore velocity $u$, namely $T = L/u$. Define dimensionless variables\n$$\nx \\;=\\; L\\,\\xi,\\qquad t \\;=\\; T\\,\\tau \\;=\\; \\frac{L}{u}\\,\\tau,\\qquad c(x,t) \\;=\\; C_{\\mathrm{in}}\\,\\hat{c}(\\xi,\\tau).\n$$\nCompute derivatives under this change of variables:\n$$\n\\frac{\\partial c}{\\partial t} \\;=\\; C_{\\mathrm{in}} \\frac{\\partial \\hat{c}}{\\partial \\tau} \\frac{\\partial \\tau}{\\partial t} \\;=\\; C_{\\mathrm{in}} \\frac{\\partial \\hat{c}}{\\partial \\tau} \\frac{u}{L},\\qquad\n\\frac{\\partial c}{\\partial x} \\;=\\; C_{\\mathrm{in}} \\frac{\\partial \\hat{c}}{\\partial \\xi} \\frac{\\partial \\xi}{\\partial x} \\;=\\; C_{\\mathrm{in}} \\frac{1}{L} \\frac{\\partial \\hat{c}}{\\partial \\xi},\n$$\nand\n$$\n\\frac{\\partial^{2} c}{\\partial x^{2}} \\;=\\; C_{\\mathrm{in}} \\frac{1}{L^{2}} \\frac{\\partial^{2} \\hat{c}}{\\partial \\xi^{2}}.\n$$\nSubstitute these into the dimensional equation:\n$$\nC_{\\mathrm{in}} \\frac{u}{L} \\frac{\\partial \\hat{c}}{\\partial \\tau} \\;+\\; u\\, C_{\\mathrm{in}} \\frac{1}{L} \\frac{\\partial \\hat{c}}{\\partial \\xi}\n\\;=\\;\nD\\, C_{\\mathrm{in}} \\frac{1}{L^{2}} \\frac{\\partial^{2} \\hat{c}}{\\partial \\xi^{2}} \\;-\\; k\\, C_{\\mathrm{in}} \\hat{c}.\n$$\nDivide through by $C_{\\mathrm{in}}$ and multiply both sides by $L/u$ to obtain the dimensionless form\n$$\n\\frac{\\partial \\hat{c}}{\\partial \\tau} \\;+\\; \\frac{\\partial \\hat{c}}{\\partial \\xi}\n\\;=\\;\n\\left(\\frac{D}{u L}\\right) \\frac{\\partial^{2} \\hat{c}}{\\partial \\xi^{2}} \\;-\\; \\left(\\frac{k L}{u}\\right) \\hat{c}.\n$$\nIdentify the standard dimensionless groups by defining\n$$\n\\mathrm{Pe} \\;\\equiv\\; \\frac{u L}{D},\\qquad \\mathrm{Da} \\;\\equiv\\; \\frac{k L}{u}.\n$$\nThen the nondimensional governing equation reads\n$$\n\\frac{\\partial \\hat{c}}{\\partial \\tau} \\;+\\; \\frac{\\partial \\hat{c}}{\\partial \\xi}\n\\;=\\;\n\\frac{1}{\\mathrm{Pe}} \\frac{\\partial^{2} \\hat{c}}{\\partial \\xi^{2}} \\;-\\; \\mathrm{Da}\\, \\hat{c}.\n$$\nThe inlet boundary condition becomes $\\hat{c}(0,\\tau) = 1$, the initial condition is $\\hat{c}(\\xi,0) = 0$, and the outlet convective boundary condition remains consistent in dimensionless form.\n\nInfluence of $\\mathrm{Pe}$ and $\\mathrm{Da}$ on surrogate smoothness and Reduced-Order Model (ROM) stability can be assessed from the dimensionless operator structure. The dimensionless spatial operator is\n$$\n\\mathcal{L} \\hat{c} \\;=\\; - \\frac{\\partial \\hat{c}}{\\partial \\xi} \\;+\\; \\frac{1}{\\mathrm{Pe}} \\frac{\\partial^{2} \\hat{c}}{\\partial \\xi^{2}} \\;-\\; \\mathrm{Da}\\, \\hat{c},\n$$\nand the evolution equation is $\\partial \\hat{c}/\\partial \\tau = \\mathcal{L}\\hat{c}$. When $\\mathrm{Pe}$ is large, the diffusion term $\\frac{1}{\\mathrm{Pe}} \\partial^{2}_{\\xi} \\hat{c}$ is weak relative to advection. This produces sharp advective fronts and thin boundary layers, making the parameter-to-solution map less smooth with respect to inputs such as $u$, $D$, and $k$. Surrogates such as Gaussian process regressors or neural networks rely on smoothness to generalize; high $\\mathrm{Pe}$ reduces differentiability and increases local curvature, requiring denser training samples near fronts to resolve the solution manifold. Conversely, when $\\mathrm{Pe}$ is small, diffusion dominates, fronts are smeared, and the solution varies more smoothly with parameters, easing surrogate learning.\n\nThe Damkohler number $\\mathrm{Da}$ compares reaction to advection. Large $\\mathrm{Da}$ implies strong reaction over an advective time scale, which can introduce stiff decay terms and reaction-induced boundary layers near the inlet where $\\hat{c}$ is forced to $1$. Stiffness manifests as widely separated time scales in $\\partial \\hat{c}/\\partial \\tau = \\mathcal{L}\\hat{c}$: eigenvalues associated with the reaction term $-\\mathrm{Da}\\,\\hat{c}$ shift the spectrum leftward by approximately $-\\mathrm{Da}$, increasing the magnitude of negative eigenvalues and creating rapid transients. This reduces the smoothness of the parameter-to-solution map with respect to $k$ and can challenge surrogates that assume slowly varying outputs.\n\nFor Reduced-Order Models built by Proper Orthogonal Decomposition (POD) and Galerkin projection, stability is governed by the reduced operator’s spectrum and non-normality. The diffusion term contributes negative definite symmetric components that are stabilizing, proportional to $1/\\mathrm{Pe}$. The advection term is skew-adjoint in continuous form but becomes non-normal under discretization, which can exhibit transient growth even when asymptotically stable; higher $\\mathrm{Pe}$ enhances non-normal effects because dissipation is weaker, making ROMs more sensitive to projection errors and mode truncation. The reaction term adds a diagonal negative component of magnitude $\\mathrm{Da}$, which is asymptotically stabilizing but increases stiffness. For explicit time integration of the ROM, qualitative stability constraints in dimensionless time step $\\Delta \\tau$ reflect the strongest of advective, diffusive, and reactive restrictions, such as\n$$\n\\Delta \\tau \\;\\lesssim\\; \\min\\left\\{\\, \\Delta \\xi,\\; \\frac{\\Delta \\xi^{2}}{2}\\,\\frac{1}{\\mathrm{Pe}},\\; \\frac{1}{\\mathrm{Da}} \\,\\right\\},\n$$\nwhere $\\Delta \\xi$ is a characteristic dimensionless spatial resolution of the ROM modes; large $\\mathrm{Pe}$ and large $\\mathrm{Da}$ tighten these bounds. Thus, increasing $\\mathrm{Pe}$ tends to decrease surrogate smoothness and ROM robustness to truncation, while increasing $\\mathrm{Da}$ increases stiffness, demanding either implicit integration or stabilized ROM formulations.\n\nThe requested dimensionless groups are therefore\n$$\n\\mathrm{Pe} \\;=\\; \\frac{u L}{D},\\qquad \\mathrm{Da} \\;=\\; \\frac{k L}{u}.\n$$\nProviding them as the final row matrix meets the specification.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{u L}{D} & \\frac{k L}{u}\\end{pmatrix}}$$"
        },
        {
            "introduction": "Once we have a set of high-fidelity simulations, a common technique for creating a physics-based reduced-order model (ROM) is Proper Orthogonal Decomposition (POD). While POD provides an optimal basis for representing the simulation data, the practitioner faces a critical trade-off: how many basis vectors, or modes, should be retained? This exercise demonstrates how to move beyond simple heuristics and apply a rigorous, goal-oriented error bound to select the ROM dimension ($r$), ensuring that the model is not only computationally cheap but also accurate enough for a specific scientific question .",
            "id": "4101999",
            "problem": "A reactive transport model for aqueous carbonate chemistry in a $3$-dimensional porous column is discretized to $n$ spatial degrees of freedom, yielding a Full-Order Model (FOM) of dimension $n$ governed by advection–diffusion–reaction. The concentration field $C(\\boldsymbol{x},t)$ evolves according to mass conservation and linear transport, with reaction kinetics captured by a spatially distributed source term obeying Arrhenius-type rate laws. A Reduced-Order Model (ROM) is constructed via Proper Orthogonal Decomposition (POD) using the Singular Value Decomposition (SVD) of a snapshot matrix $X \\in \\mathbb{R}^{n \\times m}$ formed from $m$ state snapshots of the discrete concentration vector $\\boldsymbol{c}(t) \\in \\mathbb{R}^n$. The output of interest is the effluent alkalinity at the column outlet at final time $t_f$, modeled as a linear functional $y = \\boldsymbol{w}^\\top \\boldsymbol{c}(t_f)$, where $\\boldsymbol{w} \\in \\mathbb{R}^n$ encodes quadrature weights over the outlet plane.\n\nAssume the following calibrated, physically consistent information is available:\n- The discretization size is $n = 200{,}000$ and the snapshot SVD yields ordered singular values $\\sigma_1 = 9.0$, $\\sigma_2 = 3.0$, $\\sigma_3 = 1.0$, $\\sigma_4 = 0.35$, $\\sigma_5 = 0.12$, $\\sigma_6 = 0.04$, $\\sigma_7 = 0.015$, $\\sigma_8 = 0.006$, with negligible subsequent values.\n- The measurement vector has Euclidean norm $\\lVert \\boldsymbol{w} \\rVert_2 = 0.5$.\n- The desired tolerance on the output of interest is $\\varepsilon_y = 0.1$.\n- The online computational cost per time step for the FOM scales as $C_{\\text{FOM}} = \\alpha n$ with $\\alpha = 50$. The ROM employs Galerkin projection and hyper-reduction via the Discrete Empirical Interpolation Method (DEIM), with online cost per time step $C_{\\text{ROM}} = \\beta r^2 + \\gamma q$, where $r$ is the number of retained POD modes, $q = 25$ is the number of DEIM sample points, and $\\beta = 300$, $\\gamma = 500$.\n\nStarting from first principles and well-tested facts:\n- The SVD optimality of rank-$r$ approximations in the Frobenius norm,\n- Linear functional stability bounds for outputs induced by the Euclidean norm,\n- Dimensionality scaling of algebraic operations in ROM time stepping,\n\nevaluate how the selection of $r$ affects approximation error in $y$ and the computational cost, and identify a sound criterion that uses the decay of $\\sigma_i$ and the tolerance $\\varepsilon_y$ to choose $r$. Which option correctly states such a criterion, applies it to the given data to select $r$, and consistently characterizes the resulting computational cost and error?\n\nA. Choose $r$ as the smallest integer such that $\\lVert \\boldsymbol{w} \\rVert_2 \\sqrt{\\sum_{i=r+1}^{8} \\sigma_i^2} \\le \\varepsilon_y$. With the provided data, this yields $r = 4$. Under the stated cost models, $C_{\\text{ROM}} = \\beta r^2 + \\gamma q$ and $C_{\\text{FOM}} = \\alpha n$, so the ROM per-step cost at $r = 4$ is strictly less than the FOM cost by orders of magnitude, and the output error bound is below $\\varepsilon_y$.\n\nB. Choose $r$ by requiring the captured energy fraction $\\sum_{i=1}^{r} \\sigma_i^2 \\big/ \\sum_{i=1}^{8} \\sigma_i^2 \\ge 0.998$; this yields $r = 3$. Because $99.8\\%$ of energy is captured, the output error is guaranteed to be below $\\varepsilon_y$, and the ROM cost is minimal.\n\nC. Choose $r$ as the smallest integer such that $\\sum_{i=r+1}^{8} \\sigma_i \\le \\varepsilon_y / \\lVert \\boldsymbol{w} \\rVert_2$; this yields $r = 5$. This linear tail criterion on singular values ensures the output error is below tolerance. The ROM cost will decrease roughly linearly with $r$.\n\nD. Choose $r = 4$ because cost decreases linearly with $r$ and the output error decreases linearly with $\\sum_{i=r+1}^{8} \\sigma_i$. This choice balances cost and error optimally for the given data; no norm-based bound is necessary since the singular values already indicate adequacy.",
            "solution": "## PROBLEM VALIDATION\n\n### Step 1: Extract Givens\n\n- **System**: Reactive transport model for aqueous carbonate chemistry in a $3$-dimensional porous column.\n- **Governing Physics**: Advection–diffusion–reaction with Arrhenius-type rate laws.\n- **Model Discretization**: Full-Order Model (FOM) has dimension $n = 200{,}000$. The state vector is $\\boldsymbol{c}(t) \\in \\mathbb{R}^n$.\n- **Model Reduction**: Reduced-Order Model (ROM) constructed via Proper Orthogonal Decomposition (POD) from a snapshot matrix $X \\in \\mathbb{R}^{n \\times m}$.\n- **SVD Data**: The ordered singular values from the SVD of $X$ are $\\sigma_1 = 9.0$, $\\sigma_2 = 3.0$, $\\sigma_3 = 1.0$, $\\sigma_4 = 0.35$, $\\sigma_5 = 0.12$, $\\sigma_6 = 0.04$, $\\sigma_7 = 0.015$, $\\sigma_8 = 0.006$. All subsequent singular values are negligible.\n- **Output of Interest**: A linear functional $y = \\boldsymbol{w}^\\top \\boldsymbol{c}(t_f)$, where $\\boldsymbol{w} \\in \\mathbb{R}^n$ is a measurement vector.\n- **Output Data**: The Euclidean norm of the measurement vector is $\\lVert \\boldsymbol{w} \\rVert_2 = 0.5$. The desired tolerance on the output is $\\varepsilon_y = 0.1$.\n- **Computational Cost Models**:\n  - FOM cost per time step: $C_{\\text{FOM}} = \\alpha n$, with $\\alpha = 50$.\n  - ROM cost per time step: $C_{\\text{ROM}} = \\beta r^2 + \\gamma q$, where $r$ is the ROM dimension (number of POD modes), $q = 25$ is the number of DEIM points, $\\beta = 300$, and $\\gamma = 500$.\n- **Guiding Principles**:\n  1. SVD optimality for rank-$r$ approximations in the Frobenius norm.\n  2. Linear functional stability bounds for outputs induced by the Euclidean norm.\n  3. Dimensionality scaling of algebraic operations in ROM time stepping.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem statement is critically evaluated according to the specified criteria.\n\n- **Scientifically Grounded**: The problem is well-grounded in computational science and engineering, specifically within the field of model order reduction for partial differential equations. The described methods (POD, Galerkin projection, DEIM) and the physical context (reactive transport) are standard and scientifically sound.\n- **Well-Posed**: The problem is well-posed. It provides sufficient numerical data and a clear objective: to identify a correct criterion for selecting the ROM dimension $r$ and to verify its application. A unique, determinable answer based on established theory is expected.\n- **Objective**: The problem is phrased in precise, objective, and technical language, free from subjectivity or ambiguity.\n- **Completeness and Consistency**: The problem provides all necessary parameters ($n$, $\\sigma_i$, $\\lVert \\boldsymbol{w} \\rVert_2$, $\\varepsilon_y$, cost coefficients) and contains no internal contradictions.\n- **Realism**: The parameters are realistic for a moderately complex, discretized PDE system. The rapid decay of singular values is typical for systems where transport phenomena (like diffusion) are dominant. The cost models represent standard complexity scaling for dense matrix operations in FOMs and for the a-posteriori stage of projection-based ROMs.\n\n### Step 3: Verdict and Action\n\nThe problem statement is valid. It is scientifically sound, well-posed, and provides a complete and consistent set of data for a rigorous analysis. Proceeding to solution derivation.\n\n## SOLUTION DERIVATION\n\nThe main task is to find a criterion for selecting the ROM dimension, $r$, that rigorously guarantees the error in the output of interest, $y$, is within a specified tolerance, $\\varepsilon_y$.\n\nThe output of interest is a linear functional of the state vector, $y = \\boldsymbol{w}^\\top \\boldsymbol{c}(t_f)$. The error in the output due to using a ROM approximation $\\boldsymbol{c}_r(t_f)$ instead of the FOM solution $\\boldsymbol{c}(t_f)$ is:\n$$ \\Delta y = y - y_r = \\boldsymbol{w}^\\top \\boldsymbol{c}(t_f) - \\boldsymbol{w}^\\top \\boldsymbol{c}_r(t_f) = \\boldsymbol{w}^\\top (\\boldsymbol{c}(t_f) - \\boldsymbol{c}_r(t_f)) $$\nLet $\\boldsymbol{e}_{\\boldsymbol{c}}(t_f) = \\boldsymbol{c}(t_f) - \\boldsymbol{c}_r(t_f)$ be the error in the state vector at the final time. The error in the output is then $\\Delta y = \\boldsymbol{w}^\\top \\boldsymbol{e}_{\\boldsymbol{c}}(t_f)$.\n\nUsing the principle of \"linear functional stability bounds\" (which refers to the Cauchy-Schwarz inequality), we can bound the absolute error in the output:\n$$ |\\Delta y| = |\\boldsymbol{w}^\\top \\boldsymbol{e}_{\\boldsymbol{c}}(t_f)| \\le \\lVert \\boldsymbol{w} \\rVert_2 \\lVert \\boldsymbol{e}_{\\boldsymbol{c}}(t_f) \\rVert_2 $$\nTo ensure $|\\Delta y| \\le \\varepsilon_y$, we need a bound on the state error norm, $\\lVert \\boldsymbol{e}_{\\boldsymbol{c}}(t_f) \\rVert_2$.\n\nThe POD method constructs an orthonormal basis $\\{\\boldsymbol{\\phi}_i\\}_{i=1}^r$ (the first $r$ left-singular vectors of the snapshot matrix $X$) that is optimal in the sense that it minimizes the projection error for the given snapshots in the Frobenius norm. The sum of the squared projection errors over all $m$ snapshots onto this $r$-dimensional basis is given by the sum of the squared truncated singular values:\n$$ \\sum_{j=1}^{m} \\lVert \\boldsymbol{c}(t_j) - \\sum_{i=1}^{r} (\\boldsymbol{\\phi}_i^\\top \\boldsymbol{c}(t_j)) \\boldsymbol{\\phi}_i \\rVert_2^2 = \\sum_{i=r+1}^{\\min(n,m)} \\sigma_i^2 $$\nA standard *a priori* error bound for Galerkin-based ROMs uses this projection error as a proxy for the true solution error, $\\lVert \\boldsymbol{e}_{\\boldsymbol{c}}(t_f) \\rVert_2$. This assumes the error propagation in the reduced dynamical system is bounded. A common, rigorous (though sometimes loose) bound is that the squared norm of the solution error is bounded by the sum of squared neglected singular values:\n$$ \\lVert \\boldsymbol{e}_{\\boldsymbol{c}}(t_f) \\rVert_2^2 \\le \\sum_{i=r+1}^{\\min(n,m)} \\sigma_i^2 $$\nCombining these bounds, we get an error estimate for the output:\n$$ |\\Delta y| \\le \\lVert \\boldsymbol{w} \\rVert_2 \\sqrt{\\sum_{i=r+1}^{\\min(n,m)} \\sigma_i^2} $$\nTo satisfy the tolerance requirement, we must choose the smallest integer $r$ such that:\n$$ \\lVert \\boldsymbol{w} \\rVert_2 \\sqrt{\\sum_{i=r+1}^{\\min(n,m)} \\sigma_i^2} \\le \\varepsilon_y $$\nThe problem states that singular values beyond $\\sigma_8$ are negligible, so we sum up to $i=8$.\n\n### Option-by-Option Analysis\n\n**A. Choose $r$ as the smallest integer such that $\\lVert \\boldsymbol{w} \\rVert_2 \\sqrt{\\sum_{i=r+1}^{8} \\sigma_i^2} \\le \\varepsilon_y$. With the provided data, this yields $r = 4$. Under the stated cost models, $C_{\\text{ROM}} = \\beta r^2 + \\gamma q$ and $C_{\\text{FOM}} = \\alpha n$, so the ROM per-step cost at $r = 4$ is strictly less than the FOM cost by orders of magnitude, and the output error bound is below $\\varepsilon_y$.**\n\n- **Criterion**: The criterion $\\lVert \\boldsymbol{w} \\rVert_2 \\sqrt{\\sum_{i=r+1}^{8} \\sigma_i^2} \\le \\varepsilon_y$ is identical to the one derived from first principles. It correctly combines the output error stability bound with the SVD optimality property. This is a standard goal-oriented error estimator.\n- **Calculation of $r$**: We apply the given data: $\\lVert \\boldsymbol{w} \\rVert_2 = 0.5$ and $\\varepsilon_y = 0.1$. The inequality becomes:\n  $$ 0.5 \\sqrt{\\sum_{i=r+1}^{8} \\sigma_i^2} \\le 0.1 \\implies \\sqrt{\\sum_{i=r+1}^{8} \\sigma_i^2} \\le \\frac{0.1}{0.5} = 0.2 \\implies \\sum_{i=r+1}^{8} \\sigma_i^2 \\le 0.04 $$\n  We calculate the sum of squared truncated singular values:\n  - For $r=3$: $\\sum_{i=4}^{8} \\sigma_i^2 = 0.35^2 + 0.12^2 + 0.04^2 + 0.015^2 + 0.006^2 = 0.1225 + 0.0144 + 0.0016 + 0.000225 + 0.000036 = 0.138761$. This is greater than $0.04$.\n  - For $r=4$: $\\sum_{i=5}^{8} \\sigma_i^2 = 0.12^2 + 0.04^2 + 0.015^2 + 0.006^2 = 0.0144 + 0.0016 + 0.000225 + 0.000036 = 0.016261$. This is less than or equal to $0.04$.\n  The smallest integer is indeed $r=4$.\n- **Cost and Error Characterization**:\n  - ROM Cost at $r=4$: $C_{\\text{ROM}} = \\beta r^2 + \\gamma q = 300(4^2) + 500(25) = 300(16) + 12500 = 4800 + 12500 = 17300$.\n  - FOM Cost: $C_{\\text{FOM}} = \\alpha n = 50(200{,}000) = 10{,}000{,}000$.\n  - The ratio $C_{\\text{FOM}}/C_{\\text{ROM}} = 10{,}000{,}000 / 17300 \\approx 578$. This speedup is over two orders of magnitude ($10^2=100$), so the statement is correct.\n  - The output error bound is below $\\varepsilon_y$ by the very construction of the criterion for selecting $r$.\n- **Verdict**: **Correct**.\n\n**B. Choose $r$ by requiring the captured energy fraction $\\sum_{i=1}^{r} \\sigma_i^2 \\big/ \\sum_{i=1}^{8} \\sigma_i^2 \\ge 0.998$; this yields $r = 3$. Because $99.8\\%$ of energy is captured, the output error is guaranteed to be below $\\varepsilon_y$, and the ROM cost is minimal.**\n\n- **Criterion**: This is an energy-based heuristic. It is not goal-oriented, as it does not use information about the output functional ($\\boldsymbol{w}$) or the error tolerance ($\\varepsilon_y$).\n- **Flawed Logic**: The primary flaw is the assertion that \"Because $99.8\\%$ of energy is captured, the output error is guaranteed to be below $\\varepsilon_y$.\" This is a non-sequitur. A high captured energy ensures the average state approximation is good, but it provides no guarantee for a specific output functional. If the output vector $\\boldsymbol{w}$ has significant projection onto the neglected modes (e.g., $\\boldsymbol{\\phi}_4$), the output error can be large despite the high energy capture. As calculated for option A, $r=3$ does not satisfy the rigorous error bound.\n- **Verdict**: **Incorrect**.\n\n**C. Choose $r$ as the smallest integer such that $\\sum_{i=r+1}^{8} \\sigma_i \\le \\varepsilon_y / \\lVert \\boldsymbol{w} \\rVert_2$; this yields $r = 5$. This linear tail criterion on singular values ensures the output error is below tolerance. The ROM cost will decrease roughly linearly with $r$.**\n\n- **Criterion**: The criterion $\\sum_{i=r+1}^{8} \\sigma_i \\le \\varepsilon_y / \\lVert \\boldsymbol{w} \\rVert_2$ is not a standard, rigorous error bound derived from the stated principles. The correct bound involves the square root of the sum of squares ($\\ell_2$ norm), not the sum of the singular values ($\\ell_1$ norm of the singular value vector, related to the nuclear norm).\n- **Calculation of $r$**: Let's check the calculation with the given formula. We need $\\sum_{i=r+1}^{8} \\sigma_i \\le 0.1 / 0.5 = 0.2$.\n  - For $r=4$: $\\sum_{i=5}^{8} \\sigma_i = 0.12 + 0.04 + 0.015 + 0.006 = 0.181 \\le 0.2$.\n  - The smallest integer satisfying this is $r=4$, not $r=5$ as claimed. The calculation presented in the option is incorrect.\n- **Cost Characterization**: The cost is $C_{\\text{ROM}} = \\beta r^2 + \\gamma q$. This cost *increases* quadratically with $r$, it does not decrease linearly. The statement is incorrect.\n- **Verdict**: **Incorrect**.\n\n**D. Choose $r = 4$ because cost decreases linearly with $r$ and the output error decreases linearly with $\\sum_{i=r+1}^{8} \\sigma_i$. This choice balances cost and error optimally for the given data; no norm-based bound is necessary since the singular values already indicate adequacy.**\n\n- **Cost and Error Scaling**: This option makes two incorrect statements about scaling. First, cost $C_{\\text{ROM}} = \\beta r^2 + \\gamma q$ increases quadratically with $r$, it does not decrease linearly. Second, the output error bound scales with $\\sqrt{\\sum \\sigma_i^2}$, not linearly with $\\sum \\sigma_i$.\n- **Justification**: The claim that \"no norm-based bound is necessary\" directly contradicts the problem's request to use \"linear functional stability bounds\" and identify a \"sound criterion\". This option dismisses a rigorous approach in favor of intuition.\n- **Verdict**: **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Perhaps the most critical phase in the modeling workflow is validation, which ensures our surrogate is reliable and its performance metrics are realistic. This is especially challenging when the training data exhibit complex statistical dependencies, a common feature in geochemical datasets derived from hierarchical structures like lithofacies and pore-scale geometries. This problem challenges you to design a cross-validation scheme that respects the data's structure to prevent leakage and provide an unbiased estimate of the surrogate's ability to generalize to new, unseen conditions .",
            "id": "4102024",
            "problem": "A reactive solute is transported through a porous column of length $L$ under constant inflow concentration $c_0$ and volumetric flow rate $q$, with aqueous reaction controlled by a first-order rate constant $k$ and hydrodynamic dispersion coefficient $D$. The high-fidelity simulator solves the one-dimensional advection–dispersion–reaction partial differential equation\n$$\n\\frac{\\partial c(x,t)}{\\partial t} = -u \\frac{\\partial c(x,t)}{\\partial x} + D \\frac{\\partial^2 c(x,t)}{\\partial x^2} - k\\,c(x,t),\n$$\nwith Darcy velocity $u = q/(\\phi A)$, porosity $\\phi$, and cross-sectional area $A$. The breakthrough time $t_b$ is defined as the first time $t$ at which the effluent concentration at $x=L$ satisfies $c(L,t)/c_0 = \\alpha$ for a fixed threshold $0<\\alpha<1$, and depends on the Péclet number $Pe = uL/D$ and the Damköhler number $Da = kL/u$.\n\nA reduced-order surrogate is trained to predict $t_b$ from features $(u, D, k, \\phi, T, c_0)$ and categorical lithofacies. The dataset comprises multiple simulation replicates generated by varying microstructure realizations within each lithofacies; each replicate belongs to a microstructure group identifier that encodes a shared pore-scale geometry model. Replicates sharing the same microstructure identifier exhibit correlated dispersion and reaction pathways, thereby inducing statistical dependence in $t_b$ across those replicates. The surrogate will be used to generalize to unseen microstructure groups and lithofacies under comparable ranges of $(u,D,k,\\phi,T,c_0)$.\n\nYou are tasked with designing a $k$-fold cross-validation scheme for model selection and hyperparameter tuning that yields an unbiased estimate of generalization performance under squared and absolute error losses, and specifying how to compute the performance metrics root mean squared error (RMSE), mean absolute error (MAE), and coefficient of determination ($R^2$) across folds. The design must prevent data leakage and respect the physical and statistical structure of the data. Consider the following options:\n\nA. Perform random $k$-fold cross-validation by shuffling all individual replicates without regard to microstructure groups or lithofacies. Standardize features using the mean and variance computed over the entire dataset once prior to cross-validation. Select hyperparameters by minimizing the average of fold-wise RMSE. Report RMSE and MAE averaged across folds, and compute $R^2$ by averaging the fold-wise $R^2$ values.\n\nB. Use grouped $k$-fold cross-validation where folds are constructed at the level of microstructure group identifiers, with stratification to approximately balance lithofacies across folds. Within each outer training split, perform nested grouped $k'$-fold cross-validation for hyperparameter tuning that minimizes RMSE, using MAE as a secondary criterion when RMSE differences are within a tolerance. Standardize features using statistics computed on the training portion of each fold only. After completing all outer folds, compute RMSE and MAE on the pooled set of all outer-fold test predictions and observations, and compute $R^2$ on the pooled outer test predictions via\n$$\nR^2 = 1 - \\frac{\\sum_{i=1}^{M} \\left(\\hat{t}_i - t_i\\right)^2}{\\sum_{i=1}^{M} \\left(t_i - \\bar{t}\\right)^2},\n$$\nwhere $M$ is the total number of test observations across all outer folds and $\\bar{t}$ is their mean.\n\nC. Construct stratified $k$-fold cross-validation by binning $t_b$ into quantiles and ensuring each fold has similar quantile distribution, ignoring microstructure groups and lithofacies. Apply a log-transform to the target and train the surrogate to predict $\\log t_b$. Select hyperparameters by minimizing RMSE computed on the log-transformed scale. Report RMSE, MAE, and $R^2$ on the log scale by averaging fold-wise values.\n\nD. Use spatial block $k$-fold cross-validation by partitioning runs according to synthetic spatial coordinates assigned to the one-dimensional column, then select hyperparameters by maximizing the outer-fold $R^2$ on the test data for each fold. Standardize features using training statistics per fold. Report RMSE, MAE, and $R^2$ averaged across folds.\n\nE. Use leave-one-facies-out cross-validation at the lithofacies level. Within each outer training split, tune hyperparameters by minimizing a weighted RMSE where each error term is weighted by $1/t_b$. Standardize features using training statistics per fold. Report $R^2$ by averaging fold-wise coefficients and MAE by pooling across outer folds.\n\nWhich option yields a scientifically and statistically defensible cross-validation design and metric computation for this surrogate modeling task?\n\nAnswer choices:\nA. Random $k$-fold, global standardization, average fold-wise metrics including $R^2$.\n\nB. Grouped nested $k$-fold with training-only standardization, pooled RMSE/MAE/$R^2$ over outer folds, stratified by lithofacies.\n\nC. Stratified by target quantiles only on log scale, average fold-wise log-metrics.\n\nD. Spatial block $k$-fold with hyperparameters tuned on test folds, average fold-wise metrics.\n\nE. Leave-one-facies-out with inverse-$t_b$ weighting, average fold-wise $R^2$, pooled MAE.",
            "solution": "The user has tasked me with designing a cross-validation scheme for a surrogate model aimed at predicting solute breakthrough times in a porous medium. The core of the task is to select the most scientifically and statistically defensible procedure from a set of options, ensuring the design yields an unbiased estimate of generalization performance while respecting the specific data structure.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Governing Equation:** The one-dimensional advection–dispersion–reaction (ADR) PDE is given as:\n$$\n\\frac{\\partial c(x,t)}{\\partial t} = -u \\frac{\\partial c(x,t)}{\\partial x} + D \\frac{\\partial^2 c(x,t)}{\\partial x^2} - k\\,c(x,t)\n$$\n- **Parameters:** Darcy velocity $u = q/(\\phi A)$, dispersion coefficient $D$, first-order reaction rate constant $k$, porosity $\\phi$, length $L$, cross-sectional area $A$.\n- **Target Variable:** Breakthrough time $t_b$, defined as the first time $t$ where the effluent concentration satisfies $c(L,t)/c_0 = \\alpha$ for a fixed threshold $0 < \\alpha < 1$.\n- **Model Inputs (Features):** $(u, D, k, \\phi, T, c_0)$ and categorical lithofacies.\n- **Data Structure:** The dataset consists of simulation replicates.\n    - Replicates are grouped by a `microstructure group identifier`.\n    - Replicates sharing a `microstructure group identifier` are derived from a shared pore-scale geometry model.\n    - This shared origin induces statistical dependence: replicates within the same microstructure group have correlated dispersion and reaction pathways, and thus correlated $t_b$ values.\n- **Generalization Goal:** The surrogate model must generalize to unseen microstructure groups and unseen lithofacies.\n- **Task Requirements:**\n    1. Design a $k$-fold cross-validation (CV) scheme for model selection and hyperparameter tuning.\n    2. The scheme must yield an unbiased estimate of generalization performance under squared and absolute error losses.\n    3. Specify the computation of Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and coefficient of determination ($R^2$).\n    4. The design must prevent data leakage and respect the data's physical and statistical structure.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding:** The problem is firmly grounded in computational geochemistry and transport phenomena. The ADR equation is a fundamental model for reactive transport. The data generation process, linking microscopic structure (pore-scale geometry) to macroscopic observables ($D$, $k$, $t_b$), is a standard paradigm in porous media research. The setup is scientifically sound.\n- **Well-Posedness:** The task—designing a CV scheme for a specific data structure and generalization goal—is a well-defined problem in applied statistics and machine learning.\n- **Objectivity:** The problem is described using precise, technical language, free from subjectivity.\n- **Completeness and Consistency:** The problem provides the crucial information about the data structure: the existence of groups (`microstructure group identifier`) and the statistical dependence of samples within these groups. It also clearly states the generalization goal. This information is both necessary and sufficient to determine the correct CV strategy.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is a well-posed, scientifically grounded problem that requires careful consideration of statistical principles in the context of a physical model. I will now proceed with the solution derivation and option analysis.\n\n### Solution Derivation\n\nThe primary challenge is to design a cross-validation procedure that provides an unbiased estimate of the model's performance on new, unseen data. The problem explicitly states two key structural aspects of the data that must be handled correctly:\n1.  **Grouped Data:** Replicates are not independent; those sharing a `microstructure group identifier` are correlated. Standard random $k$-fold cross-validation assumes independent and identically distributed (IID) samples. Applying it here would place correlated samples into both the training and testing sets of a fold. This constitutes data leakage, as the model would be tested on data very similar to what it has seen during training (i.e., other replicates from the same microstructure), leading to an artificially inflated and biased performance estimate. The correct approach is to ensure that all replicates from a single group are kept together in the same fold (either all in training or all in testing). This is accomplished using **Grouped $k$-fold Cross-Validation**, where the groups are the `microstructure group identifiers`.\n2.  **Hierarchical Structure and Generalization:** The model must generalize to unseen `microstructure groups` and `lithofacies`. Using Grouped $k$-fold (with groups being the microstructures) directly evaluates the ability to generalize to unseen microstructure groups. To ensure that the performance estimate is not biased by a particular distribution of lithofacies in the test sets, the folds should be constructed to be representative of the overall lithofacies distribution. This is achieved through **stratification** on the `lithofacies` category. The combination is known as **Stratified Grouped $k$-fold Cross-Validation**.\n\nFurther considerations for a rigorous validation protocol:\n- **Hyperparameter Tuning:** To avoid bias, hyperparameters must be tuned using only the training data available within each fold of the main (outer) CV loop. This is properly done using **nested cross-validation**, where an inner CV loop is performed on the training set of each outer fold to select the best hyperparameters. This inner loop must also respect the data structure (i.e., it should also be a Grouped CV).\n- **Feature Scaling:** Preprocessing steps, such as feature standardization, are part of the modeling pipeline. To prevent data leakage, the parameters for scaling (e.g., mean and standard deviation) must be learned *only* from the training data of each fold and then applied to both the training and test sets of that fold.\n- **Metric Calculation:** While performance metrics can be calculated for each fold and then averaged, this can be problematic for metrics like $R^2$. A more robust method is to **pool** the out-of-sample predictions from all test folds and the corresponding true values, and then compute a single set of metrics (RMSE, MAE, $R^2$) on this pooled data. The formula for the pooled $R^2$ is $R^2 = 1 - \\frac{\\sum_{i=1}^{M} (\\hat{t}_i - t_i)^2}{\\sum_{i=1}^{M} (t_i - \\bar{t})^2}$, where the sum is over all $M$ test samples across all folds and $\\bar{t}$ is the mean of all true test values.\n\nBased on these principles, the ideal validation scheme involves Stratified Grouped $k$-fold CV in a nested structure for hyperparameter tuning, with feature scaling performed strictly on training data per fold, and final metrics computed on pooled out-of-fold predictions.\n\n### Option-by-Option Analysis\n\n**A. Perform random $k$-fold cross-validation...**\nThis option proposes random $k$-fold CV, which ignores the grouped data structure. This will split dependent replicates from the same microstructure group between training and test sets, causing data leakage and yielding an overly optimistic, biased performance estimate. It also proposes standardizing features using the entire dataset, which is another form of data leakage. Finally, it suggests averaging fold-wise $R^2$ values, which is less statistically robust than computing on pooled results.\n**Verdict: Incorrect.**\n\n**B. Use grouped $k$-fold cross-validation where folds are constructed at the level of microstructure group identifiers, with stratification to approximately balance lithofacies across folds...**\nThis option correctly identifies the need for **Grouped $k$-fold CV** using `microstructure group identifiers` as the groups, which prevents the primary source of data leakage. It correctly suggests **stratification by lithofacies** to ensure representative folds. It prescribes **nested cross-validation** for hyperparameter tuning, which is the proper way to avoid tuning-related bias. It correctly states that **feature standardization** must use statistics from the training portion only. Finally, it advocates for computing metrics on the **pooled set** of all test predictions, including the correct formula for pooled $R^2$. This entire procedure is statistically sound and directly addresses all requirements of the problem.\n**Verdict: Correct.**\n\n**C. Construct stratified $k$-fold cross-validation by binning $t_b$ into quantiles...**\nThis option proposes stratification based on the target variable $t_b$. While this can be a useful technique in some contexts to ensure folds are balanced with respect to the output, it completely ignores the crucial grouped structure of the data based on `microstructure group identifiers`. Like option A, it will lead to data leakage and biased evaluation. The decision to log-transform the target and report metrics on the log scale, while plausible, does not fix the fundamental flaw in the CV design.\n**Verdict: Incorrect.**\n\n**D. Use spatial block $k$-fold cross-validation...**\nThis option misapplies the concept of spatial block CV. The data dependencies in this problem arise from shared group membership (`microstructure group identifier`), not from spatial or temporal adjacency between simulation runs. Furthermore, it commits a critical error by suggesting the selection of hyperparameters by maximizing performance on the **outer-fold test data**. The test set must be held out and used only for final evaluation, not for any part of the model selection or tuning process. This is a severe form of data leakage that invalidates the entire performance estimate.\n**Verdict: Incorrect.**\n\n**E. Use leave-one-facies-out cross-validation at the lithofacies level...**\nThis option proposes Leave-One-Group-Out CV where the groups are the `lithofacies`. This is a valid strategy to specifically test for generalization to unseen lithofacies. It implicitly handles the lower-level microstructure grouping. However, the problem asks for a more general $k$-fold scheme to estimate performance for generalization to unseen microstructures *and* lithofacies. The method in B (stratified grouped CV) provides a more stable and general estimate of performance on unseen microstructures drawn from the distribution of seen lithofacies. Moreover, option E is less complete than B: it does not specify a nested loop for hyperparameter tuning, and its proposal of mixed metric computation (averaging $R^2$, pooling MAE) is less coherent than the fully pooled approach in B. The proposed inverse-$t_b$ weighting for the tuning loss function does not match the specified evaluation criteria (unweighted squared and absolute errors).\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{B}$$"
        }
    ]
}