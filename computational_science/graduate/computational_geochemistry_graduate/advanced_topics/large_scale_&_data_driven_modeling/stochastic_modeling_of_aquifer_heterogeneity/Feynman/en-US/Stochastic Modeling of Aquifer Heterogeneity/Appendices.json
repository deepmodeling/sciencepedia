{
    "hands_on_practices": [
        {
            "introduction": "Geological processes often create preferential pathways, meaning that properties like hydraulic conductivity are more continuous in one direction than another. This directional dependence, known as anisotropy, is a critical feature to capture in any realistic aquifer model. This first practice  provides a foundational exercise in quantifying geometric anisotropy by connecting the geological concepts of correlation length and orientation to the precise mathematical framework of linear transformations and variogram models.",
            "id": "4100369",
            "problem": "Consider a two-dimensional (2D) second-order stationary random field $Z(\\mathbf{x})$ representing the log-hydraulic conductivity of an aquifer, with mean zero and covariance that depends only on a transformed lag vector $\\mathbf{h} \\in \\mathbb{R}^{2}$ through the Euclidean norm of a linear mapping, namely $\\|\\mathbf{A}\\mathbf{h}\\|$, where $\\mathbf{A}$ is a fixed, invertible $2 \\times 2$ matrix encoding geometric anisotropy. The semivariogram is defined by $\\gamma(\\mathbf{h}) = \\frac{1}{2}\\,\\mathbb{E}\\!\\left[(Z(\\mathbf{x}+\\mathbf{h}) - Z(\\mathbf{x}))^{2}\\right]$, and for second-order stationarity, the covariance $C(\\mathbf{h}) = \\mathbb{E}\\!\\left[Z(\\mathbf{x})Z(\\mathbf{x}+\\mathbf{h})\\right]$ depends only on $\\mathbf{h}$. Assume an exponential covariance model in the transformed space with a nugget effect, so that the semivariogram takes the form\n$$\n\\gamma(\\mathbf{h}) = c_{n} + c_{s}\\left(1 - \\exp\\!\\left(-\\|\\mathbf{A}\\mathbf{h}\\|\\right)\\right),\n$$\nwhere $c_{n} > 0$ is the nugget variance and $c_{s} > 0$ is the structural variance.\n\nLet the anisotropy be characterized by two principal correlation lengths $L_{1}$ (major axis) and $L_{2}$ (minor axis), and a rotation by an angle $\\theta$ (in degrees) from the global coordinate axes to the principal axes. Starting from the requirement that, in the principal-axis coordinate system, the dimensionless transformed distance is\n$$\nr^{2} = \\left(\\frac{y_{1}}{L_{1}}\\right)^{2} + \\left(\\frac{y_{2}}{L_{2}}\\right)^{2},\n$$\nwhere $\\mathbf{y} = \\mathbf{R}^{\\top}\\mathbf{h}$ and $\\mathbf{R}$ is the $2 \\times 2$ rotation matrix corresponding to angle $\\theta$, derive a canonical matrix $\\mathbf{A}$ such that $\\|\\mathbf{A}\\mathbf{h}\\| = r$ for all $\\mathbf{h}$. Then, using the derived form, compute the semivariogram values along the principal directions for a fixed physical lag magnitude $h_{0}$, namely $\\gamma_{\\text{maj}} = \\gamma(\\mathbf{h}_{\\text{maj}})$ for $\\mathbf{h}_{\\text{maj}}$ aligned with the major principal axis, and $\\gamma_{\\text{min}} = \\gamma(\\mathbf{h}_{\\text{min}})$ for $\\mathbf{h}_{\\text{min}}$ aligned with the minor principal axis, with the following parameters:\n- $L_{1} = 150\\,\\mathrm{m}$,\n- $L_{2} = 60\\,\\mathrm{m}$,\n- $\\theta = 35^{\\circ}$,\n- $c_{n} = 0.02$,\n- $c_{s} = 0.28$,\n- $h_{0} = 50\\,\\mathrm{m}$.\n\nReport as your final answer the dimensionless ratio\n$$\n\\frac{\\gamma_{\\text{maj}}}{\\gamma_{\\text{min}}}\n$$\nrounded to four significant figures. Express the final answer as a pure number with no units.",
            "solution": "The problem requires the derivation of the matrix $\\mathbf{A}$ that characterizes the geometric anisotropy, and then to use it to compute the ratio of semivariogram values along the principal axes for a given lag distance.\n\nFirst, we establish the relationship between the global coordinate system (for lag vector $\\mathbf{h}$) and the principal-axis coordinate system (for vector $\\mathbf{y}$). The transformation is given by $\\mathbf{y} = \\mathbf{R}^{\\top}\\mathbf{h}$, where $\\mathbf{R}$ is the rotation matrix for an angle $\\theta$.\nThe rotation matrix $\\mathbf{R}$ corresponding to a counter-clockwise rotation by an angle $\\theta$ is:\n$$\n\\mathbf{R} = \\begin{pmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{pmatrix}\n$$\nIts transpose, which transforms coordinates from the global to the rotated system, is:\n$$\n\\mathbf{R}^{\\top} = \\begin{pmatrix} \\cos\\theta & \\sin\\theta \\\\ -\\sin\\theta & \\cos\\theta \\end{pmatrix}\n$$\n\nThe problem states that in the principal-axis coordinate system, the squared dimensionless transformed distance, $r^2$, is given by:\n$$\nr^2 = \\left(\\frac{y_1}{L_1}\\right)^2 + \\left(\\frac{y_2}{L_2}\\right)^2\n$$\nwhere $y_1$ and $y_2$ are the components of the vector $\\mathbf{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix}$. This expression can be written in matrix form. Let us define a diagonal scaling matrix $\\mathbf{S}$ as:\n$$\n\\mathbf{S} = \\begin{pmatrix} \\frac{1}{L_1} & 0 \\\\ 0 & \\frac{1}{L_2} \\end{pmatrix}\n$$\nThen, $r^2$ can be expressed as the squared Euclidean norm of the vector $\\mathbf{S}\\mathbf{y}$:\n$$\nr^2 = \\mathbf{y}^{\\top}\\mathbf{S}^{\\top}\\mathbf{S}\\mathbf{y} = (\\mathbf{S}\\mathbf{y})^{\\top}(\\mathbf{S}\\mathbf{y}) = \\|\\mathbf{S}\\mathbf{y}\\|^2\n$$\nSince the scaling matrix $\\mathbf{S}$ is diagonal, $\\mathbf{S}^{\\top} = \\mathbf{S}$.\n\nWe are required to find a matrix $\\mathbf{A}$ such that $r = \\|\\mathbf{A}\\mathbf{h}\\|$. We can substitute $\\mathbf{y} = \\mathbf{R}^{\\top}\\mathbf{h}$ into the expression for $r$:\n$$\nr = \\|\\mathbf{S}\\mathbf{y}\\| = \\|\\mathbf{S}(\\mathbf{R}^{\\top}\\mathbf{h})\\| = \\|(\\mathbf{S}\\mathbf{R}^{\\top})\\mathbf{h}\\|\n$$\nBy comparing this with $r = \\|\\mathbf{A}\\mathbf{h}\\|$, we can identify the canonical matrix $\\mathbf{A}$ as:\n$$\n\\mathbf{A} = \\mathbf{S}\\mathbf{R}^{\\top} = \\begin{pmatrix} \\frac{1}{L_1} & 0 \\\\ 0 & \\frac{1}{L_2} \\end{pmatrix} \\begin{pmatrix} \\cos\\theta & \\sin\\theta \\\\ -\\sin\\theta & \\cos\\theta \\end{pmatrix} = \\begin{pmatrix} \\frac{\\cos\\theta}{L_1} & \\frac{\\sin\\theta}{L_1} \\\\ -\\frac{\\sin\\theta}{L_2} & \\frac{\\cos\\theta}{L_2} \\end{pmatrix}\n$$\nThis completes the derivation of the matrix $\\mathbf{A}$.\n\nNext, we need to compute the semivariogram values, $\\gamma_{\\text{maj}}$ and $\\gamma_{\\text{min}}$, for a lag magnitude $h_0$ along the major and minor principal axes of anisotropy. The semivariogram model is:\n$$\n\\gamma(\\mathbf{h}) = c_{n} + c_{s}\\left(1 - \\exp(-\\|\\mathbf{A}\\mathbf{h}\\|)\\right)\n$$\nThe major principal axis corresponds to the direction of the largest correlation length, $L_1$. In the principal-axis system ($\\mathbf{y}$-coordinates), this is the $y_1$-axis. A vector of length $h_0$ along this axis is $\\mathbf{y}_{\\text{maj}}' = \\begin{pmatrix} h_0 \\\\ 0 \\end{pmatrix}$.\nThe minor principal axis corresponds to the direction of the smallest correlation length, $L_2$. In the $\\mathbf{y}$-coordinates, this is the $y_2$-axis. A vector of length $h_0$ along this axis is $\\mathbf{y}_{\\text{min}}' = \\begin{pmatrix} 0 \\\\ h_0 \\end{pmatrix}$.\n\nTo find the corresponding lag vectors $\\mathbf{h}_{\\text{maj}}$ and $\\mathbf{h}_{\\text{min}}$ in the global coordinate system, we use the inverse transformation $\\mathbf{h} = \\mathbf{R}\\mathbf{y}$.\nFor the major axis:\n$$\n\\mathbf{h}_{\\text{maj}} = \\mathbf{R}\\mathbf{y}_{\\text{maj}}' = \\begin{pmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{pmatrix} \\begin{pmatrix} h_0 \\\\ 0 \\end{pmatrix} = h_0 \\begin{pmatrix} \\cos\\theta \\\\ \\sin\\theta \\end{pmatrix}\n$$\nFor the minor axis:\n$$\n\\mathbf{h}_{\\text{min}} = \\mathbf{R}\\mathbf{y}_{\\text{min}}' = \\begin{pmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{pmatrix} \\begin{pmatrix} 0 \\\\ h_0 \\end{pmatrix} = h_0 \\begin{pmatrix} -\\sin\\theta \\\\ \\cos\\theta \\end{pmatrix}\n$$\nNow we compute the transformed distance $\\|\\mathbf{A}\\mathbf{h}\\|$ for these two lag vectors.\nFor $\\mathbf{h}_{\\text{maj}}$:\n$$\n\\|\\mathbf{A}\\mathbf{h}_{\\text{maj}}\\| = r_{\\text{maj}}\n$$\nWe know $r^2 = (\\frac{y_1}{L_1})^2 + (\\frac{y_2}{L_2})^2$ and $\\mathbf{y} = \\mathbf{R}^{\\top}\\mathbf{h}$. For $\\mathbf{h}_{\\text{maj}}$, the corresponding $\\mathbf{y}$ vector is $\\mathbf{y}_{\\text{maj}} = \\mathbf{R}^{\\top}\\mathbf{h}_{\\text{maj}} = \\mathbf{R}^{\\top}(\\mathbf{R}\\mathbf{y}_{\\text{maj}}') = (\\mathbf{R}^{\\top}\\mathbf{R})\\mathbf{y}_{\\text{maj}}' = \\mathbf{I}\\mathbf{y}_{\\text{maj}}' = \\mathbf{y}_{\\text{maj}}' = \\begin{pmatrix} h_0 \\\\ 0 \\end{pmatrix}$.\nSo, $y_1 = h_0$ and $y_2 = 0$. The transformed distance is:\n$$\n\\|\\mathbf{A}\\mathbf{h}_{\\text{maj}}\\| = \\sqrt{\\left(\\frac{h_0}{L_1}\\right)^2 + \\left(\\frac{0}{L_2}\\right)^2} = \\frac{h_0}{L_1}\n$$\nFor $\\mathbf{h}_{\\text{min}}$:\nThe corresponding $\\mathbf{y}$ vector is $\\mathbf{y}_{\\text{min}} = \\mathbf{R}^{\\top}\\mathbf{h}_{\\text{min}} = \\mathbf{y}_{\\text{min}}' = \\begin{pmatrix} 0 \\\\ h_0 \\end{pmatrix}$.\nSo, $y_1 = 0$ and $y_2 = h_0$. The transformed distance is:\n$$\n\\|\\mathbf{A}\\mathbf{h}_{\\text{min}}\\| = \\sqrt{\\left(\\frac{0}{L_1}\\right)^2 + \\left(\\frac{h_0}{L_2}\\right)^2} = \\frac{h_0}{L_2}\n$$\nThe semivariogram values along the principal directions are therefore:\n$$\n\\gamma_{\\text{maj}} = c_n + c_s\\left(1 - \\exp\\left(-\\frac{h_0}{L_1}\\right)\\right)\n$$\n$$\n\\gamma_{\\text{min}} = c_n + c_s\\left(1 - \\exp\\left(-\\frac{h_0}{L_2}\\right)\\right)\n$$\nNote that the rotation angle $\\theta$ does not appear in these final expressions, as the calculation is performed along the principal axes themselves.\n\nNow, we substitute the given parameter values:\n$L_1 = 150\\,\\mathrm{m}$, $L_2 = 60\\,\\mathrm{m}$, $h_0 = 50\\,\\mathrm{m}$, $c_n = 0.02$, $c_s = 0.28$.\nThe dimensionless ratios for the exponents are:\n$$\n\\frac{h_0}{L_1} = \\frac{50}{150} = \\frac{1}{3}\n$$\n$$\n\\frac{h_0}{L_2} = \\frac{50}{60} = \\frac{5}{6}\n$$\nSubstituting these into the expressions for $\\gamma_{\\text{maj}}$ and $\\gamma_{\\text{min}}$:\n$$\n\\gamma_{\\text{maj}} = 0.02 + 0.28\\left(1 - \\exp\\left(-\\frac{1}{3}\\right)\\right)\n$$\n$$\n\\gamma_{\\text{min}} = 0.02 + 0.28\\left(1 - \\exp\\left(-\\frac{5}{6}\\right)\\right)\n$$\nWe compute the numerical values:\n$$\n\\exp\\left(-\\frac{1}{3}\\right) \\approx 0.7165313\n$$\n$$\n\\exp\\left(-\\frac{5}{6}\\right) \\approx 0.4345982\n$$\nThus,\n$$\n\\gamma_{\\text{maj}} \\approx 0.02 + 0.28(1 - 0.7165313) = 0.02 + 0.28(0.2834687) \\approx 0.02 + 0.0793712 \\approx 0.0993712\n$$\n$$\n\\gamma_{\\text{min}} \\approx 0.02 + 0.28(1 - 0.4345982) = 0.02 + 0.28(0.5654018) \\approx 0.02 + 0.1583125 \\approx 0.1783125\n$$\nFinally, we compute the required ratio:\n$$\n\\frac{\\gamma_{\\text{maj}}}{\\gamma_{\\text{min}}} \\approx \\frac{0.0993712}{0.1783125} \\approx 0.557285\n$$\nRounding to four significant figures, the result is $0.5573$.",
            "answer": "$$\n\\boxed{0.5573}\n$$"
        },
        {
            "introduction": "Once the statistical structure of a heterogeneous field is defined, the next task is to generate realizations that honor this structure. The Karhunen–Loève (KL) expansion is a powerful and elegant method for this, representing the random field as a series of orthogonal functions with random coefficients. This hands-on coding exercise  guides you through the process of taking the abstract KL integral equation and turning it into a robust numerical algorithm, a crucial step in translating geostatistical theory into practical simulation tools.",
            "id": "4100373",
            "problem": "Consider a real-valued, second-order stationary Gaussian random field $Y(\\mathbf{x})$ representing the log-hydraulic conductivity on a one-dimensional bounded domain $D = [0,L]$ with no units. The covariance is isotropic and given by the function $C(\\mathbf{h}) = \\sigma^2 \\exp(-\\lVert \\mathbf{h} \\rVert/\\ell)$, where $\\sigma^2$ is the variance and $\\ell$ is the correlation length. The Karhunen–Loève expansion provides an orthogonal representation of $Y(\\mathbf{x})$ on $D$ using the eigenpairs of the covariance integral operator. Your task is to construct a finite-dimensional approximation using a consistent, symmetric discretization of the covariance operator and to compute quantifiable diagnostics that verify the discretization and truncation.\n\nStarting from the fundamental base consisting of the definitions of a second-order stationary Gaussian random field, the definition of the covariance function, and the characterization of the Karhunen–Loève eigenproblem of the covariance integral operator on a bounded domain, derive a numerical scheme based on quadrature to approximate the operator’s eigenpairs. Use a uniform grid of $N$ points on $D$ and the composite trapezoidal rule to construct a symmetric, positive-definite matrix approximation of the covariance integral operator that yields a well-posed, real-symmetric eigenvalue problem. Then, use the leading $M$ eigenpairs to define the finite-dimensional Karhunen–Loève truncation.\n\nYour program must:\n- Construct a uniform grid $\\{x_i\\}_{i=0}^{N-1}$ on $[0,L]$, with spacing $\\Delta x = L/(N-1)$, and composite trapezoidal quadrature weights $\\{w_i\\}_{i=0}^{N-1}$ with $w_0 = w_{N-1} = \\Delta x/2$ and $w_i = \\Delta x$ for interior points.\n- Assemble the covariance kernel matrix with entries $K_{ij} = C(|x_i - x_j|)$ using the specified $C(\\mathbf{h})$.\n- Derive and implement a symmetric matrix approximation of the covariance integral operator using the quadrature weights that leads to a real-symmetric eigenvalue problem whose eigenvalues approximate the continuous operator’s eigenvalues and whose eigenvectors approximate the eigenfunctions on the grid.\n- Compute all eigenvalues and sort them in descending order. Let $\\{\\lambda_k\\}_{k=1}^{N}$ denote these eigenvalues.\n- For a given truncation level $M \\le N$, compute the captured variance ratio $R_M = \\left(\\sum_{k=1}^{M} \\lambda_k\\right)\\Big/\\left(\\sum_{k=1}^{N} \\lambda_k\\right)$.\n- Verify the trace property of the covariance operator discretization by computing the relative trace error $E = \\left|\\left(\\sum_{k=1}^{N} \\lambda_k\\right) - \\sigma^2 L\\right|/(\\sigma^2 L)$.\n\nYour program must implement the above steps and produce results for the following test suite of $(L, N, \\sigma^2, \\ell, M)$ parameter sets (all quantities are dimensionless):\n- Test case A (general case): $(L, N, \\sigma^2, \\ell, M) = (1, 60, 1, 0.2, 10)$.\n- Test case B (short correlation length): $(L, N, \\sigma^2, \\ell, M) = (1, 60, 1, 0.05, 10)$.\n- Test case C (long correlation length): $(L, N, \\sigma^2, \\ell, M) = (1, 60, 1, 0.5, 10)$.\n- Test case D (boundary truncation): $(L, N, \\sigma^2, \\ell, M) = (1, 60, 1, 0.2, 60)$.\n\nFor each test case, your program must output the pair $[R_M, E]$, where $R_M$ is the captured variance ratio and $E$ is the relative trace error. Both $R_M$ and $E$ must be rounded to $6$ decimal places.\n\nFinal output format:\n- Your program must print a single line containing a list of the four results for the test cases in the order A, B, C, D.\n- Each result must be a list of two floats $[R_M, E]$, both rounded to $6$ decimal places.\n- The final printed line must be exactly a single list of lists, for example $[[a_1,b_1],[a_2,b_2],[a_3,b_3],[a_4,b_4]]$, with no additional text.\n\nNo external input is permitted, and no external files may be read or written. Angles are not involved. No physical units are involved; all quantities are dimensionless.",
            "solution": "The problem requires the numerical approximation of the Karhunen-Loève (KL) expansion for a second-order stationary Gaussian random field, $Y(\\mathbf{x})$, on a one-dimensional domain $D = [0,L]$. The scientific foundation for this task lies in the spectral theory of integral operators.\n\nThe KL expansion represents a random process as an infinite series of orthogonal functions, which are the eigenfunctions of the process's covariance operator. The coefficients in the series are uncorrelated random variables. For a random field $Y(\\mathbf{x})$ with mean zero and covariance function $C(\\mathbf{x}, \\mathbf{x'})$, the eigenfunctions $\\phi_k(\\mathbf{x})$ and corresponding eigenvalues $\\lambda_k$ are the solutions to the Fredholm integral equation of the second kind:\n$$\n\\int_D C(\\mathbf{x}, \\mathbf{x'}) \\phi_k(\\mathbf{x'}) d\\mathbf{x'} = \\lambda_k \\phi_k(\\mathbf{x})\n$$\nThe problem specifies a one-dimensional domain $D=[0,L]$ and a stationary, isotropic covariance function $C(\\mathbf{h}) = \\sigma^2 \\exp(-\\lVert \\mathbf{h} \\rVert/\\ell)$, where $\\mathbf{h} = x - x'$ is the separation vector. Thus, $C(x, x') = \\sigma^2 \\exp(-|x - x'|/\\ell)$.\n\nTo solve this integral equation numerically, we discretize the domain and the integral operator.\nFirst, we establish a uniform grid of $N$ points, $\\{x_i\\}_{i=0}^{N-1}$, on the interval $[0,L]$, where $x_i = i \\cdot \\Delta x$ and the grid spacing is $\\Delta x = L/(N-1)$.\nThe integral is approximated using a quadrature rule. The problem specifies the composite trapezoidal rule. The integral of a function $f(x')$ over $[0,L]$ is approximated as:\n$$\n\\int_0^L f(x') dx' \\approx \\sum_{j=0}^{N-1} f(x_j) w_j\n$$\nwhere $\\{w_j\\}_{j=0}^{N-1}$ are the quadrature weights. For the trapezoidal rule, these are $w_0 = w_{N-1} = \\Delta x/2$ and $w_j = \\Delta x$ for $j \\in \\{1, 2, \\dots, N-2\\}$.\n\nApplying this quadrature to the KL integral equation at each grid point $x_i$ yields a system of linear equations:\n$$\n\\sum_{j=0}^{N-1} C(x_i, x_j) w_j \\phi_k(x_j) \\approx \\lambda_k \\phi_k(x_i) \\quad \\text{for } i = 0, 1, \\dots, N-1\n$$\nLet us define the discrete counterparts to the continuous entities:\n- The covariance matrix $\\mathbf{K}$ is an $N \\times N$ matrix with entries $K_{ij} = C(x_i, x_j) = \\sigma^2 \\exp(-|x_i - x_j|/\\ell)$.\n- The quadrature weight matrix $\\mathbf{W}$ is a diagonal matrix with entries $W_{jj} = w_j$.\n- The eigenvector $\\mathbf{v}_k$ is a column vector of length $N$ whose entries are the values of the eigenfunction at the grid points, $v_{k,i} = \\phi_k(x_i)$.\n\nWith these definitions, the system of equations can be written in matrix form as a generalized eigenvalue problem:\n$$\n\\mathbf{K W} \\mathbf{v}_k = \\lambda_k \\mathbf{v}_k\n$$\nwhere $\\lambda_k$ now represents the approximate eigenvalues. The matrix $\\mathbf{K}$ is symmetric since $K_{ij} = C(|x_i - x_j|) = C(|x_j - x_i|) = K_{ji}$. However, the matrix product $\\mathbf{K W}$ is generally not symmetric because $\\mathbf{W}$ is not a multiple of the identity matrix. Solving a non-symmetric eigenvalue problem can be numerically less stable and may yield complex eigenvalues, which is undesirable for an operator known to be self-adjoint.\n\nThe problem statement correctly requires the derivation of a real-symmetric eigenvalue problem. This is achieved through a similarity transformation. Let $\\mathbf{W}^{1/2}$ be the diagonal matrix with entries $\\sqrt{w_j}$. We define a new vector $\\mathbf{u}_k = \\mathbf{W}^{1/2} \\mathbf{v}_k$, which implies $\\mathbf{v}_k = \\mathbf{W}^{-1/2} \\mathbf{u}_k$. Substituting this into the generalized eigenvalue problem:\n$$\n\\mathbf{K W} (\\mathbf{W}^{-1/2} \\mathbf{u}_k) = \\lambda_k (\\mathbf{W}^{-1/2} \\mathbf{u}_k)\n$$\nMultiplying from the left by $\\mathbf{W}^{1/2}$:\n$$\n\\mathbf{W}^{1/2} \\mathbf{K W}^{1/2} \\mathbf{u}_k = \\lambda_k \\mathbf{W}^{1/2} \\mathbf{W}^{-1/2} \\mathbf{u}_k\n$$\nThis simplifies to a standard, symmetric eigenvalue problem:\n$$\n\\mathbf{A} \\mathbf{u}_k = \\lambda_k \\mathbf{u}_k\n$$\nwhere the matrix $\\mathbf{A} = \\mathbf{W}^{1/2} \\mathbf{K} \\mathbf{W}^{1/2}$ is symmetric. Its entries are $A_{ij} = \\sqrt{w_i} K_{ij} \\sqrt{w_j}$. This formulation guarantees that the computed eigenvalues $\\{\\lambda_k\\}_{k=1}^N$ are real, and we can use efficient numerical solvers designed for symmetric matrices.\n\nThe solution proceeds by constructing this matrix $\\mathbf{A}$ and solving for its eigenvalues. These eigenvalues approximate those of the original continuous operator. For verification, two diagnostic quantities are computed:\n$1$. The captured variance ratio, $R_M$, for a truncation to $M$ terms:\n$$\nR_M = \\frac{\\sum_{k=1}^{M} \\lambda_k}{\\sum_{k=1}^{N} \\lambda_k}\n$$\nThis measures the fraction of the total variance of the discretized field that is captured by the first $M$ KL modes.\n$2$. The relative trace error, $E$. A fundamental property of the covariance operator is that its trace equals the integrated variance over the domain. For a stationary process, $C(x,x) = C(0) = \\sigma^2$.\n$$\n\\text{Tr}(\\mathcal{C}) = \\int_0^L C(x,x) dx = \\int_0^L \\sigma^2 dx = \\sigma^2 L\n$$\nThe trace is also the sum of all eigenvalues, $\\sum_{k=1}^{\\infty} \\lambda_k$. Our numerical approximation should preserve this property. The trace of the discretized operator matrix $\\mathbf{A}$ is $\\text{Tr}(\\mathbf{A}) = \\sum_{k=1}^N \\lambda_k$. Using the cyclic property of the trace, $\\text{Tr}(\\mathbf{A}) = \\text{Tr}(\\mathbf{W}^{1/2} \\mathbf{K} \\mathbf{W}^{1/2}) = \\text{Tr}(\\mathbf{K W})$. The trace of $\\mathbf{KW}$ is $\\sum_{i=0}^{N-1} (\\mathbf{KW})_{ii} = \\sum_{i=0}^{N-1} K_{ii} W_{ii} = \\sum_{i=0}^{N-1} \\sigma^2 w_i = \\sigma^2 \\sum_{i=0}^{N-1} w_i$. The sum of the trapezoidal weights is exactly the length of the interval, $\\sum w_i = L$. Therefore, the sum of the computed eigenvalues should theoretically equal $\\sigma^2 L$. The relative trace error $E$ quantifies any deviation due to floating-point arithmetic:\n$$\nE = \\frac{\\left| \\left(\\sum_{k=1}^{N} \\lambda_k\\right) - \\sigma^2 L \\right|}{\\sigma^2 L}\n$$\n\nThe algorithm is as follows:\n$1$. For each test case $(L, N, \\sigma^2, \\ell, M)$, construct the grid $\\{x_i\\}$ and trapezoidal weights $\\{w_i\\}$.\n$2$. Assemble the $N \\times N$ covariance matrix $\\mathbf{K}$ with entries $K_{ij} = \\sigma^2 \\exp(-|x_i - x_j|/\\ell)$.\n$3$. Construct the symmetric matrix $\\mathbf{A}$ with entries $A_{ij} = \\sqrt{w_i} K_{ij} \\sqrt{w_j}$.\n$4$. Compute the $N$ real eigenvalues of $\\mathbf{A}$ and sort them in descending order.\n$5$. Calculate $R_M$ and $E$ using the sorted eigenvalues.\n$6$. Round the results to $6$ decimal places and format them as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy is not needed for this solution.\n\ndef solve():\n    \"\"\"\n    Computes diagnostics for the Karhunen–Loève expansion of a\n    Gaussian random field using quadrature-based discretization.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (L, N, sigma^2, l, M)\n        (1.0, 60, 1.0, 0.2, 10),      # Test case A (general case)\n        (1.0, 60, 1.0, 0.05, 10),     # Test case B (short correlation length)\n        (1.0, 60, 1.0, 0.5, 10),      # Test case C (long correlation length)\n        (1.0, 60, 1.0, 0.2, 60),      # Test case D (boundary truncation)\n    ]\n\n    results = []\n    for L, N, sigma_sq, ell, M in test_cases:\n        # Step 1: Construct grid and quadrature weights\n        # Create a uniform grid of N points on [0, L]\n        x = np.linspace(0, L, N)\n        delta_x = L / (N - 1)\n\n        # Create composite trapezoidal quadrature weights\n        w = np.full(N, delta_x)\n        w[0] = delta_x / 2.0\n        w[-1] = delta_x / 2.0\n\n        # Step 2: Assemble the covariance kernel matrix K\n        # K_ij = C(|x_i - x_j|) = sigma^2 * exp(-|x_i - x_j| / l)\n        # Create a matrix of pairwise distances |x_i - x_j|\n        dist_matrix = np.abs(np.subtract.outer(x, x))\n        K = sigma_sq * np.exp(-dist_matrix / ell)\n\n        # Step 3: Derive and implement the symmetric matrix approximation A\n        # A = W^(1/2) * K * W^(1/2)\n        # This can be implemented efficiently using broadcasting\n        w_sqrt = np.sqrt(w)\n        A = np.outer(w_sqrt, w_sqrt) * K\n\n        # Step 4: Compute all eigenvalues and sort them\n        # Use np.linalg.eigvalsh for symmetric matrices. It's faster and more stable,\n        # and returns only the real eigenvalues.\n        eigenvalues = np.linalg.eigvalsh(A)\n        # Sort eigenvalues in descending order\n        eigenvalues = np.sort(eigenvalues)[::-1]\n\n        # Step 5: Compute diagnostics\n        # Captured Variance Ratio R_M\n        # The sum of all eigenvalues is the total variance of the discretized field\n        total_variance_discrete = np.sum(eigenvalues)\n        # The sum of the first M eigenvalues is the captured variance\n        captured_variance = np.sum(eigenvalues[:M])\n        \n        # Guard against division by zero if total variance is zero (unlikely but safe)\n        if total_variance_discrete > 0:\n            R_M = captured_variance / total_variance_discrete\n        else:\n            R_M = 1.0 if M > 0 else 0.0\n\n        # Relative Trace Error E\n        # The theoretical trace of the continuous operator is sigma^2 * L\n        total_variance_continuous = sigma_sq * L\n        \n        # The trace of a matrix is the sum of its eigenvalues.\n        # This sum should ideally be equal to total_variance_continuous.\n        # The error E measures the deviation due to numerical precision.\n        if total_variance_continuous > 0:\n            E = np.abs(total_variance_discrete - total_variance_continuous) / total_variance_continuous\n        else:\n            E = np.abs(total_variance_discrete)\n\n        # Round results to 6 decimal places\n        R_M_rounded = round(R_M, 6)\n        E_rounded = round(E, 6)\n\n        results.append([R_M_rounded, E_rounded])\n    \n    # Final print statement in the exact required format.\n    # The format is a list of lists, e.g., [[val1, val2], [val3, val4]]\n    # str() on a list correctly formats it with brackets and commas.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The ultimate goal of stochastic modeling is to make informed decisions under uncertainty. This capstone practice  brings together field generation and uncertainty propagation to address a practical question: estimating the probability that a contaminant concentration will exceed a regulatory threshold. You will implement a full workflow from field generation to transport simulation and explore how advanced sampling strategies like Latin Hypercube and Quasi-Monte Carlo methods can dramatically improve computational efficiency compared to standard Monte Carlo.",
            "id": "4100352",
            "problem": "You are tasked with designing and implementing a computational experiment to compare Monte Carlo (MC), Latin Hypercube Sampling (LHS), and Quasi-Monte Carlo (QMC) methods for estimating the exceedance probability of a contaminant concentration in a heterogeneous aquifer, and to empirically quantify their convergence rates. The context is stochastic modeling of aquifer heterogeneity within computational geochemistry, grounded in first principles of groundwater flow and reactive transport.\n\nUse the following fundamental base:\n\n- Darcy’s law: the specific discharge is $q(x) = K(x)\\,i$, where $K(x)$ is the hydraulic conductivity and $i$ is the hydraulic gradient (dimensionless). The pore water velocity is $v(x) = q(x)/n = K(x)\\,i/n$, where $n$ is porosity (dimensionless).\n- Travel time along a one-dimensional ($1$-dimensional) streamline of length $L$ is $t = \\int_0^L \\frac{1}{v(x)}\\,dx = \\frac{n}{i}\\int_0^L \\frac{1}{K(x)}\\,dx$.\n- First-order decay of dissolved concentration along the flow path yields $C_{\\text{obs}} = C_0\\,\\exp(-\\lambda\\,t)$, where $C_0$ is the source concentration and $\\lambda$ is the first-order decay rate.\n- The hydraulic conductivity field uncertainty is represented by the log-conductivity $Y(x) = \\log K(x)$ modeled as a Gaussian random field with squared-exponential covariance and truncated Karhunen–Loève expansion on $[0,L]$ under homogeneous Neumann boundary conditions. Approximate eigenfunctions are cosine modes $\\phi_j(x) = \\sqrt{2}\\cos\\left(\\frac{j\\pi x}{L}\\right)$ for $j=1,\\dots,m$, and the corresponding mode variances decay as $\\lambda_j = \\sigma_Y^2\\exp\\left(-\\left(\\frac{j\\pi \\ell}{L}\\right)^2\\right)$, where $\\ell$ is the correlation length and $\\sigma_Y$ is the standard deviation of $Y(x)$. With $m$ modes and independent standard normal coefficients $\\xi_j \\sim \\mathcal{N}(0,1)$, the truncated field is $Y(x) = \\mu_Y + \\sum_{j=1}^m \\sqrt{\\lambda_j}\\,\\xi_j\\,\\phi_j(x)$.\n- The first-order decay rate is uncertain and modeled as lognormal, $\\lambda = \\exp(\\mu_\\lambda + \\sigma_\\lambda Z)$ with $Z\\sim\\mathcal{N}(0,1)$.\n\nYour program must:\n\n1. For each test case, define the aquifer parameters, discretize $[0,L]$ into $G$ evenly spaced points, and use the trapezoidal rule to approximate $t = \\frac{n}{i}\\int_0^L \\frac{1}{K(x)}\\,dx$. Compute $C_{\\text{obs}} = C_0\\,\\exp(-\\lambda\\,t)$ and the exceedance event $C_{\\text{obs}} > C_{\\text{thr}}$.\n2. Implement three sampling strategies for the random variables $(\\xi_1,\\dots,\\xi_m,Z)$:\n   - Monte Carlo (MC): independent draws from the standard normal distribution for each component.\n   - Latin Hypercube Sampling (LHS): Latin Hypercube on $[0,1]^d$ with $d=m+1$, followed by inverse Normal cumulative distribution function (CDF) mapping to obtain $(\\xi_1,\\dots,\\xi_m,Z)$.\n   - Quasi-Monte Carlo (QMC): Sobol sequence on $[0,1]^d$ with scrambling, followed by inverse Normal CDF mapping to obtain $(\\xi_1,\\dots,\\xi_m,Z)$.\n   Clip uniform values to $(\\varepsilon,1-\\varepsilon)$ with $\\varepsilon=10^{-12}$ before applying the inverse Normal CDF to avoid infinities.\n3. For each method and each sample size $N$ in the specified set, estimate the exceedance probability $p = \\mathbb{P}(C_{\\text{obs}} > C_{\\text{thr}})$ by the sample mean of the indicator function.\n4. For each test case, compute a high-accuracy reference probability $p_{\\text{ref}}$ using standard Monte Carlo with $N_{\\text{ref}}$ samples.\n5. For each method, compute the absolute error $e_N = |p_N - p_{\\text{ref}}|$ at each $N$, and fit a linear regression of $\\log(e_N)$ versus $\\log(N)$ across the provided sample sizes to estimate an empirical convergence rate exponent $\\alpha$, defined by $e_N \\approx c\\,N^{-\\alpha}$ so that $\\alpha = -\\frac{d\\,\\log(e_N)}{d\\,\\log(N)}$. Report $\\alpha$ as a positive number.\n\nPhysical units and numerical handling:\n\n- Use $L$ in meters ($\\mathrm{m}$), $K$ in meters per day ($\\mathrm{m/day}$), $t$ in days ($\\mathrm{day}$), $\\lambda$ in inverse days ($\\mathrm{day}^{-1}$), and concentrations in milligrams per liter ($\\mathrm{mg/L}$). While intermediate quantities carry units, the final outputs are probabilities and convergence exponents, which are dimensionless.\n- Use the trapezoidal rule for numerical integration of $\\frac{1}{K(x)}$ over $[0,L]$.\n- Use fixed seeds for reproducibility specified below.\n\nTest suite:\n\nFor each case, use the parameters and sample sizes exactly as provided.\n\n- Case $1$:\n  - $L = 100\\,\\mathrm{m}$, $n = 0.30$, $i = 0.01$, $C_0 = 100\\,\\mathrm{mg/L}$, $C_{\\text{thr}} = 10\\,\\mathrm{mg/L}$,\n  - $\\ell = 20\\,\\mathrm{m}$, $\\mu_Y = \\log(10)$, $\\sigma_Y = 0.5$, $m = 5$, $G = 200$,\n  - $\\mu_\\lambda = \\log(0.05\\,\\mathrm{day}^{-1})$, $\\sigma_\\lambda = 0.3$,\n  - sample sizes $N \\in \\{64,256,1024\\}$, reference $N_{\\text{ref}} = 20000$,\n  - seeds: MC $= 12345$, LHS $= 23456$, QMC $= 34567$.\n\n- Case $2$:\n  - $L = 200\\,\\mathrm{m}$, $n = 0.35$, $i = 0.005$, $C_0 = 50\\,\\mathrm{mg/L}$, $C_{\\text{thr}} = 5\\,\\mathrm{mg/L}$,\n  - $\\ell = 50\\,\\mathrm{m}$, $\\mu_Y = \\log(5)$, $\\sigma_Y = 0.8$, $m = 7$, $G = 300$,\n  - $\\mu_\\lambda = \\log(0.02\\,\\mathrm{day}^{-1})$, $\\sigma_\\lambda = 0.5$,\n  - sample sizes $N \\in \\{64,256,1024\\}$, reference $N_{\\text{ref}} = 20000$,\n  - seeds: MC $= 12346$, LHS $= 23457$, QMC $= 34568$.\n\n- Case $3$:\n  - $L = 50\\,\\mathrm{m}$, $n = 0.25$, $i = 0.02$, $C_0 = 30\\,\\mathrm{mg/L}$, $C_{\\text{thr}} = 25\\,\\mathrm{mg/L}$,\n  - $\\ell = 5\\,\\mathrm{m}$, $\\mu_Y = \\log(15)$, $\\sigma_Y = 0.4$, $m = 4$, $G = 150$,\n  - $\\mu_\\lambda = \\log(0.1\\,\\mathrm{day}^{-1})$, $\\sigma_\\lambda = 0.2$,\n  - sample sizes $N \\in \\{64,256,1024\\}$, reference $N_{\\text{ref}} = 20000$,\n  - seeds: MC $= 12347$, LHS $= 23458$, QMC $= 34569$.\n\nFinal output format specification:\n\n- For each test case, produce a list of $9$ floats in the order: $[p_{\\text{MC}}^{N_{\\max}}, p_{\\text{LHS}}^{N_{\\max}}, p_{\\text{QMC}}^{N_{\\max}}, e_{\\text{MC}}^{N_{\\max}}, e_{\\text{LHS}}^{N_{\\max}}, e_{\\text{QMC}}^{N_{\\max}}, \\alpha_{\\text{MC}}, \\alpha_{\\text{LHS}}, \\alpha_{\\text{QMC}}]$, where $N_{\\max}$ is the largest sample size in the set (here $1024$), $e^{N_{\\max}}$ are absolute errors at $N_{\\max}$ relative to $p_{\\text{ref}}$, and $\\alpha$ are empirical convergence exponents from the log-log regression across the three $N$ values.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list as above. For example, the printed output must look like $[[\\ldots],[\\ldots],[\\ldots]]$.",
            "solution": "The solution implements a computational experiment to compare the efficiency of Monte Carlo (MC), Latin Hypercube Sampling (LHS), and Quasi-Monte Carlo (QMC) methods for uncertainty quantification in a stochastic hydrogeology model. The overall design consists of a main procedure that iterates through each defined test case. For each case, it first establishes a high-accuracy reference solution, then evaluates each sampling method's performance and convergence rate. The implementation is based on the physical and mathematical principles provided.\n\nThe core of the experiment is a function that deterministically maps a set of random input variables to a model output. The stochastic inputs are $(\\xi_1, \\dots, \\xi_m, Z)$, which are $d=m+1$ independent standard normal random variables. The output is a binary indicator for whether the contaminant concentration $C_{\\text{obs}}$ exceeds a threshold $C_{\\text{thr}}$. The implementation of this model proceeds as follows:\n\nFirst, the one-dimensional spatial domain $[0, L]$ is discretized into $G$ evenly spaced points, $x_k$ where $k=0, \\dots, G-1$. The Karhunen-Loève representation of the log-hydraulic conductivity field, $Y(x) = \\mu_Y + \\sum_{j=1}^m \\sqrt{\\lambda_j}\\,\\xi_j\\,\\phi_j(x)$, is constructed on this grid. The mode variances (eigenvalues) $\\lambda_j = \\sigma_Y^2\\exp\\left(-\\left(\\frac{j\\pi \\ell}{L}\\right)^2\\right)$ and the values of the eigenfunctions $\\phi_j(x_k) = \\sqrt{2}\\cos\\left(\\frac{j\\pi x_k}{L}\\right)$ are pre-computed for efficiency. For a given realization of the random vector $(\\xi_1, \\dots, \\xi_m)$, the field $Y(x_k)$ is calculated. This operation is vectorized to compute $N$ realizations of the field simultaneously, resulting in a matrix of size $N \\times G$.\n\nSecond, from the log-conductivity field $Y(x)$, the hydraulic conductivity field is found via $K(x) = \\exp(Y(x))$. The advective travel time, $t = \\frac{n}{i}\\int_0^L \\frac{1}{K(x)}\\,dx$, is computed using the trapezoidal rule for numerical integration as specified. `numpy.trapz` is applied along the spatial dimension of the $N \\times G$ matrix of integrand values, $\\frac{1}{K(x_k)}$, yielding a vector of $N$ travel times.\n\nThird, the uncertain first-order decay rate $\\lambda$ is computed for each of the $N$ realizations using its lognormal definition, $\\lambda = \\exp(\\mu_\\lambda + \\sigma_\\lambda Z)$, where $Z$ is the $(m+1)$-th component of the input random vector.\n\nFinally, the observed concentration at the end of the flow path, $C_{\\text{obs}} = C_0\\,\\exp(-\\lambda\\,t)$, is calculated for each of the $N$ realizations. The model output is the indicator of an exceedance event, which is $1$ if $C_{\\text{obs}} > C_{\\text{thr}}$ and $0$ otherwise.\n\nTo drive this model, three sampling strategies are implemented to generate $N$ samples of the $d$-dimensional standard normal input vector:\n1.  **Monte Carlo (MC):** Samples are generated using `numpy.random.default_rng().standard_normal()`, which produces independent pseudo-random draws from the standard normal distribution.\n2.  **Latin Hypercube Sampling (LHS):** An $N \\times d$ Latin Hypercube sample is generated in the unit hypercube $[0,1]^d$ using `scipy.stats.qmc.LatinHypercube`. These uniform samples are then transformed to the standard normal space using the inverse of the standard normal cumulative distribution function (CDF), `scipy.stats.norm.ppf()`.\n3.  **Quasi-Monte Carlo (QMC):** An $N \\times d$ low-discrepancy Sobol sequence is generated using `scipy.stats.qmc.Sobol` with scrambling enabled for better error properties. Similar to LHS, these uniform samples are mapped to the standard normal space via the inverse normal CDF. For both LHS and QMC, the uniform samples are clipped to the interval $[\\varepsilon, 1-\\varepsilon]$ with $\\varepsilon=10^{-12}$ before the inverse CDF transformation to prevent numerical instabilities near the tails of the normal distribution.\n\nFor each test case, the analysis proceeds by first computing a high-accuracy reference probability, $p_{\\text{ref}}$, using a large number of standard MC samples ($N_{\\text{ref}}$). Then, for each of the three sampling methods and for each sample size $N$ in the specified set, the exceedance probability $p_N$ is estimated as the sample mean of the indicator outputs. The absolute error $e_N = |p_N - p_{\\text{ref}}|$ is then calculated. To estimate the empirical convergence rate $\\alpha$, a linear regression is performed on the logarithm of the errors, $\\log(e_N)$, versus the logarithm of the sample sizes, $\\log(N)$. Based on the theoretical error scaling $e_N \\approx c N^{-\\alpha}$, the rate $\\alpha$ is given by the negative of the slope of the log-log plot. This is computed using `scipy.stats.linregress`.\n\nThe final reported values for each test case include the probability estimates $p_N$ and errors $e_N$ for the largest sample size ($N_{\\max} = 1024$), and the estimated convergence rates $\\alpha$ for each of the three methods. All random sampling processes are initialized with the specified seeds to ensure reproducibility.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import qmc, norm, linregress\n\ndef solve():\n    \"\"\"\n    Main function to run the computational experiment for all test cases.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"L\": 100.0, \"n\": 0.30, \"i\": 0.01, \"C0\": 100.0, \"C_thr\": 10.0,\n            \"ell\": 20.0, \"mu_Y\": np.log(10.0), \"sigma_Y\": 0.5, \"m\": 5, \"G\": 200,\n            \"mu_lambda\": np.log(0.05), \"sigma_lambda\": 0.3,\n            \"sample_sizes\": [64, 256, 1024], \"N_ref\": 20000,\n            \"seeds\": {\"MC\": 12345, \"LHS\": 23456, \"QMC\": 34567},\n        },\n        {\n            \"L\": 200.0, \"n\": 0.35, \"i\": 0.005, \"C0\": 50.0, \"C_thr\": 5.0,\n            \"ell\": 50.0, \"mu_Y\": np.log(5.0), \"sigma_Y\": 0.8, \"m\": 7, \"G\": 300,\n            \"mu_lambda\": np.log(0.02), \"sigma_lambda\": 0.5,\n            \"sample_sizes\": [64, 256, 1024], \"N_ref\": 20000,\n            \"seeds\": {\"MC\": 12346, \"LHS\": 23457, \"QMC\": 34568},\n        },\n        {\n            \"L\": 50.0, \"n\": 0.25, \"i\": 0.02, \"C0\": 30.0, \"C_thr\": 25.0,\n            \"ell\": 5.0, \"mu_Y\": np.log(15.0), \"sigma_Y\": 0.4, \"m\": 4, \"G\": 150,\n            \"mu_lambda\": np.log(0.1), \"sigma_lambda\": 0.2,\n            \"sample_sizes\": [64, 256, 1024], \"N_ref\": 20000,\n            \"seeds\": {\"MC\": 12347, \"LHS\": 23458, \"QMC\": 34569},\n        },\n    ]\n\n    all_case_results = []\n    for params in test_cases:\n        p_ref, model_evaluator = setup_and_compute_reference(params)\n        \n        case_p_Nmax, case_e_Nmax, case_alphas = [], [], []\n\n        methods = [\"MC\", \"LHS\", \"QMC\"]\n        for method_name in methods:\n            p_Nmax, e_Nmax, alpha = analyze_method(\n                method_name, params, p_ref, model_evaluator\n            )\n            case_p_Nmax.append(p_Nmax)\n            case_e_Nmax.append(e_Nmax)\n            case_alphas.append(alpha)\n\n        # Combine results in the specified order for this case\n        combined_results = case_p_Nmax + case_e_Nmax + case_alphas\n        all_case_results.append(combined_results)\n\n    # Format the final output string as specified\n    print(f\"[{','.join(map(str, all_case_results))}]\")\n\ndef setup_and_compute_reference(params):\n    \"\"\"\n    Sets up the model evaluator and computes the reference probability.\n    \"\"\"\n    m = params[\"m\"]\n    d = m + 1\n    \n    # Pre-compute parts of the model that do not depend on random samples\n    x_grid = np.linspace(0, params[\"L\"], params[\"G\"])\n    j_vals = np.arange(1, m + 1)\n    \n    # Eigenvalues of the Karhunen-Loeve expansion\n    lambda_j = (params[\"sigma_Y\"]**2) * np.exp(-(j_vals * np.pi * params[\"ell\"] / params[\"L\"])**2)\n    sqrt_lambda_j = np.sqrt(lambda_j)\n    \n    # Eigenfunctions evaluated on the grid\n    phi_j_x = np.sqrt(2) * np.cos(np.pi * np.outer(j_vals, x_grid) / params[\"L\"])\n    \n    def evaluate_model(samples_norm):\n        \"\"\"Vectorized evaluation of the physics model for N samples.\"\"\"\n        N_samples = samples_norm.shape[0]\n        xi_samples = samples_norm[:, :m]\n        Z_samples = samples_norm[:, m]\n\n        # Construct log-conductivity field Y(x) for N samples\n        Y_x_perturbation = (xi_samples @ np.diag(sqrt_lambda_j)) @ phi_j_x\n        Y_x = params[\"mu_Y\"] + Y_x_perturbation\n\n        # Compute hydraulic conductivity K(x) and travel time t\n        K_x = np.exp(Y_x)\n        integrand = 1.0 / K_x\n        integral_val = np.trapz(integrand, x=x_grid, axis=1)\n        t = (params[\"n\"] / params[\"i\"]) * integral_val\n\n        # Compute decay rate lambda and final concentration C_obs\n        lam = np.exp(params[\"mu_lambda\"] + params[\"sigma_lambda\"] * Z_samples)\n        C_obs = params[\"C0\"] * np.exp(-lam * t)\n        \n        # Return exceedance event indicator\n        return (C_obs > params[\"C_thr\"]).astype(int)\n\n    # Compute reference probability using a dedicated high-N MC run\n    ref_seed = sum(params[\"seeds\"].values()) # Deterministic seed for reproducibility\n    ref_rng = np.random.default_rng(ref_seed)\n    ref_samples = ref_rng.standard_normal(size=(params[\"N_ref\"], d))\n    p_ref = np.mean(evaluate_model(ref_samples))\n    \n    return p_ref, evaluate_model\n\ndef analyze_method(method_name, params, p_ref, model_evaluator):\n    \"\"\"\n    Analyzes a single sampling method (MC, LHS, or QMC).\n    \"\"\"\n    m = params[\"m\"]\n    d = m + 1\n    seed = params[\"seeds\"][method_name]\n    sample_sizes = params[\"sample_sizes\"]\n    N_max = max(sample_sizes)\n    eps = 1e-12\n\n    log_errors = []\n    log_Ns = []\n    p_Nmax, e_Nmax = None, None\n\n    for N in sample_sizes:\n        # 1. Generate samples\n        if method_name == \"MC\":\n            rng = np.random.default_rng(seed)\n            samples_norm = rng.standard_normal(size=(N, d))\n        else:\n            if method_name == \"LHS\":\n                sampler = qmc.LatinHypercube(d=d, seed=seed)\n            elif method_name == \"QMC\":\n                sampler = qmc.Sobol(d=d, scramble=True, seed=seed)\n            \n            samples_unif = sampler.random(n=N)\n            samples_unif = np.clip(samples_unif, eps, 1 - eps)\n            samples_norm = norm.ppf(samples_unif)\n        \n        # 2. Estimate probability and error\n        p_N = np.mean(model_evaluator(samples_norm))\n        e_N = abs(p_N - p_ref)\n\n        if e_N > 0:\n            log_errors.append(np.log(e_N))\n            log_Ns.append(np.log(N))\n\n        if N == N_max:\n            p_Nmax = p_N\n            e_Nmax = e_N\n\n    # 3. Fit convergence rate\n    if len(log_Ns) >= 2:\n        slope, _, _, _, _ = linregress(log_Ns, log_errors)\n        alpha = -slope\n    else:\n        alpha = 0.0  # Not enough data for regression\n\n    return p_Nmax, e_Nmax, alpha\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}