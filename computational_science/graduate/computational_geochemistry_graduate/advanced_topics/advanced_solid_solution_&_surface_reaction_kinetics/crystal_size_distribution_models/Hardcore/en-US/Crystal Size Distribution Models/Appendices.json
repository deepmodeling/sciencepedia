{
    "hands_on_practices": [
        {
            "introduction": "This first practice grounds our understanding of Crystal Size Distribution (CSD) models in their fundamental mathematical structure. We will explore how the population balance equation, a partial differential equation describing the evolution of the entire size distribution, can be transformed into a more manageable set of ordinary differential equations for its statistical moments (). This exercise is essential for learning how macroscopic parameters, such as the crystal growth rate $G$, are mechanistically linked to and can be inferred from observable, bulk properties of the crystal population like mean crystal size.",
            "id": "4075728",
            "problem": "A crystal population in a silicate melt is analyzed using the Crystal Size Distribution (CSD) framework. Let $n(L,t)$ denote the number density of crystals of linear size $L$ at time $t$, with $L \\geq 0$. Assume constant primary nucleation rate $J$ and constant linear growth rate $G$, with no aggregation, breakup, or dissolution. The evolution of the size distribution is governed by conservation of number in size space: crystals translate in $L$ at speed $G$, and new crystals are injected at $L=0$ by nucleation. Define the $k$-th moment $m_k(t) = \\int_{0}^{\\infty} L^{k} n(L,t)\\,\\mathrm{d}L$ and the mean crystal length $\\langle L \\rangle(t) = m_1(t)/m_0(t)$.\n\nStarting only from the conservation statement for $n(L,t)$, the definitions of $m_k$ and $\\langle L \\rangle$, and the assumption of constant $J$ and $G$, derive a closed-form expression for the linear growth rate $G$ at an instant $t^{\\ast}$ in terms of measurable quantities $m_0(t^{\\ast})$, $\\langle L \\rangle(t^{\\ast})$, the time derivative $\\mathrm{d}\\langle L \\rangle/\\mathrm{d}t\\big|_{t^{\\ast}}$, and the nucleation rate $J$ (which is equal to $\\mathrm{d}m_0/\\mathrm{d}t$ under these conditions). Then, compute $G$ using the following experimentally determined values at $t^{\\ast}$:\n- $m_0(t^{\\ast}) = 1.045 \\times 10^{9}\\ \\mathrm{m}^{-3}$,\n- $\\langle L \\rangle(t^{\\ast}) = 34.7\\ \\mathrm{\\mu m}$,\n- $\\mathrm{d}\\langle L \\rangle/\\mathrm{d}t\\big|_{t^{\\ast}} = 0.1185\\ \\mathrm{\\mu m}\\,\\mathrm{s}^{-1}$,\n- $J = \\mathrm{d}m_0/\\mathrm{d}t\\big|_{t^{\\ast}} = 2.13 \\times 10^{7}\\ \\mathrm{m}^{-3}\\,\\mathrm{s}^{-1}$.\n\nExpress the final value of $G$ in micrometers per second ($\\mathrm{\\mu m}\\,\\mathrm{s}^{-1}$) and round your answer to four significant figures. In your derivation, explicitly state the role of the boundary condition at $L=0$ and explain how it enters the moment equations and the resulting expression for $G$.",
            "solution": "We start from the conservation of number in size space for the Crystal Size Distribution (CSD). For constant linear growth rate $G$ and primary nucleation at $L=0$, the size distribution evolves by translation in $L$ with speed $G$ for $L  0$, and crystals are injected at $L=0$ at rate $J$. A standard representation that makes boundary effects explicit is\n$$\n\\frac{\\partial n(L,t)}{\\partial t} + G\\,\\frac{\\partial n(L,t)}{\\partial L} = 0,\\quad L0,\n$$\ntogether with a boundary condition at $L=0$ that enforces the injection of new crystals by nucleation,\n$$\nG\\,n(0,t) = J.\n$$\nThis boundary condition states that the convective flux of crystals leaving $L=0$ in size space, $G n(0,t)$, equals the nucleation rate $J$, ensuring that primary nucleation acts as a source at $L=0$.\n\nDefine the $k$-th moment\n$$\nm_k(t) = \\int_{0}^{\\infty} L^{k}\\,n(L,t)\\,\\mathrm{d}L,\n$$\nand note that the mean size is $\\langle L \\rangle(t) = \\frac{m_1(t)}{m_0(t)}$. To obtain moment evolution equations, multiply the conservation equation by $L^{k}$ and integrate over $L\\in[0,\\infty)$:\n$$\n\\frac{\\mathrm{d} m_k}{\\mathrm{d} t} + G \\int_{0}^{\\infty} L^{k}\\,\\frac{\\partial n}{\\partial L}\\,\\mathrm{d}L = 0.\n$$\nThe integral term is treated by integration by parts:\n$$\n\\int_{0}^{\\infty} L^{k}\\,\\frac{\\partial n}{\\partial L}\\,\\mathrm{d}L = \\left[L^{k} n(L,t)\\right]_{0}^{\\infty} - k \\int_{0}^{\\infty} L^{k-1} n(L,t)\\,\\mathrm{d}L.\n$$\nAssuming $n(L,t) \\to 0$ sufficiently fast as $L \\to \\infty$, the upper boundary term vanishes. At the lower boundary $L=0$, the term equals $0$ for $k\\geq 1$, but equals $-n(0,t)$ for $k=0$. Therefore:\n\n- For $k \\geq 1$,\n$$\n\\frac{\\mathrm{d} m_k}{\\mathrm{d} t} - G\\,k\\,m_{k-1} = 0 \\quad \\Rightarrow \\quad \\frac{\\mathrm{d} m_k}{\\mathrm{d} t} = k\\,G\\,m_{k-1}.\n$$\n\n- For $k = 0$,\n$$\n\\frac{\\mathrm{d} m_0}{\\mathrm{d} t} + G \\left( - n(0,t) \\right) = 0 \\quad \\Rightarrow \\quad \\frac{\\mathrm{d} m_0}{\\mathrm{d} t} = G\\,n(0,t).\n$$\nInvoking the boundary condition $G\\,n(0,t) = J$, we obtain\n$$\n\\frac{\\mathrm{d} m_0}{\\mathrm{d} t} = J.\n$$\n\nThe moment equations show explicitly the role of the boundary condition at $L=0$: it converts the boundary flux term $G n(0,t)$ into the nucleation rate $J$, making the zeroth moment evolve according to $\\mathrm{d} m_0/\\mathrm{d} t = J$, and ensuring that the higher-moment dynamics are driven purely by translation in size space. Without enforcing $G n(0,t) = J$, the zeroth-moment evolution would depend on the unknown boundary value $n(0,t)$, and the subsequent derivation would not yield a closed expression in terms of observables.\n\nWe now use the first moment equation for $k=1$:\n$$\n\\frac{\\mathrm{d} m_1}{\\mathrm{d} t} = 1 \\cdot G \\cdot m_0 = G\\,m_0.\n$$\nThe mean size is $\\langle L \\rangle = \\frac{m_1}{m_0}$, so by the quotient rule,\n$$\n\\frac{\\mathrm{d} \\langle L \\rangle}{\\mathrm{d} t} = \\frac{\\frac{\\mathrm{d} m_1}{\\mathrm{d} t}\\, m_0 - m_1\\,\\frac{\\mathrm{d} m_0}{\\mathrm{d} t}}{m_0^{2}} = \\frac{G\\,m_0^{2} - m_0\\,\\langle L \\rangle\\, J}{m_0^{2}} = G - \\frac{\\langle L \\rangle\\,J}{m_0}.\n$$\nSolving for $G$ yields the desired expression in terms of observables:\n$$\nG = \\frac{\\mathrm{d} \\langle L \\rangle}{\\mathrm{d} t} + \\frac{\\langle L \\rangle\\,J}{m_0}.\n$$\n\nWe now compute $G$ at the instant $t^{\\ast}$ using the provided data. First evaluate the ratio $\\frac{J}{m_0}$:\n$$\n\\frac{J}{m_0} = \\frac{2.13 \\times 10^{7}\\ \\mathrm{m}^{-3}\\,\\mathrm{s}^{-1}}{1.045 \\times 10^{9}\\ \\mathrm{m}^{-3}} = \\left(\\frac{2.13}{1.045}\\right) \\times 10^{-2}\\ \\mathrm{s}^{-1} \\approx 2.03827751196172 \\times 10^{-2}\\ \\mathrm{s}^{-1}.\n$$\nMultiply by $\\langle L \\rangle$:\n$$\n\\frac{\\langle L \\rangle\\,J}{m_0} \\approx 34.7\\ \\mathrm{\\mu m} \\times 2.03827751196172 \\times 10^{-2}\\ \\mathrm{s}^{-1} \\approx 0.707282296650717\\ \\mathrm{\\mu m}\\,\\mathrm{s}^{-1}.\n$$\nAdd $\\frac{\\mathrm{d} \\langle L \\rangle}{\\mathrm{d} t}$:\n$$\nG \\approx 0.1185\\ \\mathrm{\\mu m}\\,\\mathrm{s}^{-1} + 0.707282296650717\\ \\mathrm{\\mu m}\\,\\mathrm{s}^{-1} = 0.825782296650717\\ \\mathrm{\\mu m}\\,\\mathrm{s}^{-1}.\n$$\nRounded to four significant figures and expressed in micrometers per second, this gives\n$$\nG \\approx 0.8258\\ \\mathrm{\\mu m}\\,\\mathrm{s}^{-1}.\n$$\n\nFinally, regarding boundary conditions, the derivation hinges on correctly handling the $L=0$ boundary. Modeling nucleation as a boundary flux with $G n(0,t) = J$ ensures that the zeroth moment obeys $\\mathrm{d} m_0/\\mathrm{d} t = J$ and that higher moments evolve as $\\mathrm{d} m_k/\\mathrm{d} t = k G m_{k-1}$. If the boundary were treated differently (for example, if nucleation were represented as a volumetric source via a Dirac delta term in $L$ without imposing the flux boundary condition, or if there were dissolution causing a sink at $L=0$), the boundary terms in the moment equations would change. This would alter the relationship between $\\mathrm{d} \\langle L \\rangle/\\mathrm{d} t$ and $G$, potentially introducing additional terms involving $n(0,t)$ or sink/source rates. Likewise, truncation of the measurement domain (finite maximum detectable size) would reintroduce a nonzero upper boundary term $\\left[L^{k} n\\right]_{0}^{L_{\\max}}$, which can bias estimates of $G$ if not accounted for. Thus, a physically consistent and explicitly stated boundary condition at $L=0$ is essential for obtaining a closed expression for $G$ in terms of observable quantities.",
            "answer": "$$\\boxed{0.8258}$$"
        },
        {
            "introduction": "Having explored the theoretical link between kinetics and CSD moments, we now turn to the practical challenge of extracting these parameters from experimental data. This hands-on coding problem addresses the critical issue of measurement uncertainty, which is non-uniform in particle counting and can bias results if ignored (). By implementing a weighted linear regression from first principles, you will develop the essential skill of applying the correct statistical tools to account for heteroscedasticity, ensuring your estimates of kinetic parameters are both accurate and robust.",
            "id": "4075737",
            "problem": "You are tasked with implementing a program that estimates uncertainty in the slope of the log-transformed crystal population density versus crystal size using weighted linear regression in a scientifically realistic crystal size distribution analysis. Specifically, consider binned crystal size data with bin centers denoted by $L_i$ measured in millimeters, bin widths denoted by $\\Delta L_i$ measured in millimeters, and observed bin counts denoted by $n_i$ (dimensionless). The crystal population density per unit size in bin $i$ is the count divided by the bin width, $n_i / \\Delta L_i$, and its natural logarithm is modeled as a linear function of size. Counting variability is heteroscedastic due to counting statistics, which must be accounted for using an appropriate weighting scheme based on well-tested facts from counting processes.\n\nYour program must start from the following fundamental base: bin counts $n_i$ are generated by a counting process where variability increases with the mean count. In the regime where the natural logarithm is used to linearize relationships, the variability of the transformed observations must be approximated without relying on constant-variance assumptions. You must use a weighted linear regression of $\\ln(n_i / \\Delta L_i)$ on $L_i$ with weights that reflect the inverse of the variance implied by the counting process and the transformation, determined solely from the observed $n_i$ and fundamental approximations. You must then compute a two-sided confidence interval on the slope at confidence level $c$, using a large-sample normal approximation, and report the slope and interval bounds. The slope must be expressed in $\\mathrm{mm}^{-1}$, and its confidence interval bounds must also be in $\\mathrm{mm}^{-1}$. Your program must produce its results as pure numerical values; do not print unit strings.\n\nImplement the following steps in a universally applicable, mathematical and logical manner:\n- Construct the transformed response $y_i = \\ln(n_i / \\Delta L_i)$.\n- Adopt a weight for each observation that reflects heteroscedasticity arising from the counting process and the logarithmic transformation, depending only on $n_i$.\n- Fit a weighted linear model $y_i$ versus $L_i$ to obtain the slope and intercept.\n- Use a large-sample normal approximation with confidence level $c$ to compute a two-sided confidence interval on the slope that reflects the weighting and the heteroscedasticity.\n\nTest Suite:\nApply your program to the following parameter sets, each with confidence level $c = 0.95$.\n\nCase $1$ (general case with varied bin widths and high counts):\n- $L = [\\,0.25,\\,0.50,\\,0.75,\\,1.00,\\,1.25,\\,1.50,\\,1.75,\\,2.00\\,]$ in $\\mathrm{mm}$.\n- $\\Delta L = [\\,0.20,\\,0.20,\\,0.20,\\,0.20,\\,0.20,\\,0.25,\\,0.25,\\,0.30\\,]$ in $\\mathrm{mm}$.\n- $n = [\\,520,\\,400,\\,310,\\,250,\\,195,\\,150,\\,118,\\,90\\,]$.\n\nCase $2$ (boundary case with small counts and variable bin widths):\n- $L = [\\,0.10,\\,0.30,\\,0.60,\\,1.00,\\,1.50,\\,2.20\\,]$ in $\\mathrm{mm}$.\n- $\\Delta L = [\\,0.10,\\,0.10,\\,0.20,\\,0.20,\\,0.30,\\,0.40\\,]$ in $\\mathrm{mm}$.\n- $n = [\\,12,\\,9,\\,7,\\,5,\\,4,\\,3\\,]$.\n\nCase $3$ (edge case with near-constant density across size and uniform bin widths):\n- $L = [\\,0.50,\\,0.90,\\,1.30,\\,1.70,\\,2.10,\\,2.50\\,]$ in $\\mathrm{mm}$.\n- $\\Delta L = [\\,0.40,\\,0.40,\\,0.40,\\,0.40,\\,0.40,\\,0.40\\,]$ in $\\mathrm{mm}$.\n- $n = [\\,100,\\,98,\\,102,\\,99,\\,101,\\,100\\,]$.\n\nCase $4$ (edge case with strongly varying size range and bin widths):\n- $L = [\\,0.20,\\,0.40,\\,0.80,\\,1.60,\\,3.20\\,]$ in $\\mathrm{mm}$.\n- $\\Delta L = [\\,0.20,\\,0.20,\\,0.40,\\,0.80,\\,1.60\\,]$ in $\\mathrm{mm}$.\n- $n = [\\,800,\\,500,\\,280,\\,120,\\,40\\,]$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each result corresponds to a test case and is itself a list containing three floating-point numbers in the order $[\\text{slope},\\text{lower},\\text{upper}]$. The output must contain no spaces. For example, the overall output format must be\n$[[m_1,\\ell_1,u_1],[m_2,\\ell_2,u_2],[m_3,\\ell_3,u_3],[m_4,\\ell_4,u_4]]$\nwith each $m_i$, $\\ell_i$, and $u_i$ being floats representing values in $\\mathrm{mm}^{-1}$.",
            "solution": "The user has requested a program to estimate the slope and its confidence interval for a log-transformed crystal size distribution (CSD) model using weighted linear regression. The problem is scientifically and mathematically well-posed.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Model:** A linear relationship is assumed between the natural logarithm of crystal population density, $\\ln(n_i / \\Delta L_i)$, and crystal size, $L_i$.\n- **Independent Variable ($x_i$):** Bin centers, $L_i$, in units of $\\mathrm{mm}$.\n- **Dependent Variable ($y_i$):** The natural logarithm of population density, $y_i = \\ln(n_i / \\Delta L_i)$.\n- **Population Density:** $n_i / \\Delta L_i$, where $n_i$ is the dimensionless observed bin count and $\\Delta L_i$ is the bin width in $\\mathrm{mm}$.\n- **Weighting Scheme:** A weighted linear regression is required, with weights reflecting heteroscedasticity from the counting process and the logarithmic transformation. The weights must be determined from the observed counts $n_i$.\n- **Task:** Compute the slope, $m$, and its two-sided confidence interval (lower bound $\\ell$, upper bound $u$) for the regression of $y_i$ on $x_i=L_i$.\n- **Confidence Level:** $c = 0.95$.\n- **Approximation:** A large-sample normal approximation for the confidence interval.\n- **Output Units:** Slope and interval bounds in $\\mathrm{mm}^{-1}$.\n- **Test Cases:** Four specific sets of data for $L$, $\\Delta L$, and $n$ are provided.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is valid.\n- **Scientifically Grounded:** The problem is a standard application of statistical analysis in geochemistry. CSD analysis is a fundamental tool. The assumption that bin counts $n_i$ follow a counting process (implying Poisson statistics where variance approximates the mean) is a cornerstone of this type of analysis. The subsequent use of propagation of uncertainty for the log-transformed variable and weighted least squares to account for the resulting heteroscedasticity is a rigorous and established methodology.\n- **Well-Posed:** The problem provides all necessary data and constraints to derive a unique, meaningful solution. The choice of methods (WLS, normal approximation) is specified, leaving no ambiguity.\n- **Objective:** The problem is described using precise, objective, and formal scientific language.\n\n**Step 3: Verdict and Action**\nThe problem is valid and can be solved as stated.\n\n### Principle-Based Solution\n\nThe objective is to perform a weighted linear regression of the form $y_i = m x_i + b$, where $x_i = L_i$ is the crystal size and $y_i = \\ln(n_i / \\Delta L_i)$ is the log-population density. The key is to derive the appropriate weights, $w_i$, based on the statistical properties of the data.\n\n**1. Derivation of Weights**\nThe bin counts, $n_i$, represent the number of events (crystals falling into a size bin) observed. Such counting processes are fundamentally described by the Poisson distribution. A key property of a Poisson-distributed random variable with mean $\\lambda$ is that its variance is also $\\lambda$. Therefore, for the count $n_i$, we have $\\mathrm{Var}(n_i) = E[n_i]$. In the absence of a full model for $E[n_i]$, a standard and robust approximation is to use the observed count itself as an estimate of its mean and variance:\n$$ \\mathrm{Var}(n_i) \\approx n_i $$\nThe population density in bin $i$ is $N_i = n_i / \\Delta L_i$. Since $\\Delta L_i$ is a fixed constant for each bin, the variance of $N_i$ is:\n$$ \\mathrm{Var}(N_i) = \\mathrm{Var}\\left(\\frac{n_i}{\\Delta L_i}\\right) = \\frac{1}{(\\Delta L_i)^2} \\mathrm{Var}(n_i) \\approx \\frac{n_i}{(\\Delta L_i)^2} $$\nThe regression is performed on the log-transformed variable $y_i = \\ln(N_i)$. To find the variance of $y_i$, we use the propagation of uncertainty formula. For a function $f(X)$, the first-order approximation for the variance is $\\mathrm{Var}(f(X)) \\approx (f'(E[X]))^2 \\mathrm{Var}(X)$. Here, $f(N_i) = \\ln(N_i)$, so $f'(N_i) = 1/N_i$.\n$$ \\mathrm{Var}(y_i) = \\mathrm{Var}(\\ln(N_i)) \\approx \\left(\\frac{1}{E[N_i]}\\right)^2 \\mathrm{Var}(N_i) $$\nUsing the observed values $N_i$ and $n_i$ as estimates for their expectations, we get:\n$$ \\mathrm{Var}(y_i) \\approx \\left(\\frac{1}{N_i}\\right)^2 \\mathrm{Var}(N_i) \\approx \\left(\\frac{\\Delta L_i}{n_i}\\right)^2 \\left(\\frac{n_i}{(\\Delta L_i)^2}\\right) = \\frac{1}{n_i} $$\nThis simple and elegant result indicates that the variance of the log-transformed population density is inversely proportional to the raw count.\n\nIn weighted linear regression, the optimal weights $w_i$ are the reciprocal of the variance of each observation. Thus, the correct weighting scheme is:\n$$ w_i = \\frac{1}{\\mathrm{Var}(y_i)} \\approx n_i $$\nThis aligns with the problem's requirement that weights depend only on $n_i$.\n\n**2. Weighted Linear Regression Formulas**\nThe slope $m$ and intercept $b$ are found by minimizing the weighted sum of squared residuals, $S = \\sum_{i} w_i (y_i - (m x_i + b))^2$. The solution for the slope $m$ is given by:\n$$ m = \\frac{ (\\sum w_i) (\\sum w_i x_i y_i) - (\\sum w_i x_i) (\\sum w_i y_i) }{ (\\sum w_i) (\\sum w_i x_i^2) - (\\sum w_i x_i)^2 } $$\nFor computation, it is convenient to define the following sums, with $x_i = L_i$ and $y_i = \\ln(n_i/\\Delta L_i)$:\n- $S_w = \\sum w_i$\n- $S_{wx} = \\sum w_i x_i$\n- $S_{wy} = \\sum w_i y_i$\n- $S_{wxx} = \\sum w_i x_i^2$\n- $S_{wxy} = \\sum w_i x_i y_i$\n\nThe slope is then:\n$$ m = \\frac{S_w S_{wxy} - S_{wx} S_{wy}}{S_w S_{wxx} - (S_{wx})^2} $$\n\n**3. Confidence Interval for the Slope**\nWhen the variances $\\mathrm{Var}(y_i) = \\sigma_i^2$ are known (or approximated as in our case, where $\\sigma_i^2=1/w_i$), the variance of the estimated slope $m$ is given by:\n$$ \\mathrm{Var}(m) = \\frac{1}{\\sum w_i (x_i - \\bar{x}_w)^2} $$\nwhere $\\bar{x}_w = (\\sum w_i x_i) / (\\sum w_i)$ is the weighted mean of $x_i$. The denominator can be computed as:\n$$ \\sum w_i (x_i - \\bar{x}_w)^2 = S_{wxx} - \\frac{(S_{wx})^2}{S_w} $$\nThe standard error of the slope, $SE(m)$, is the square root of its variance:\n$$ SE(m) = \\sqrt{\\mathrm{Var}(m)} = \\left( S_{wxx} - \\frac{(S_{wx})^2}{S_w} \\right)^{-1/2} $$\nUsing the large-sample normal approximation, a two-sided confidence interval for the slope $m$ at a confidence level $c$ is constructed as:\n$$ [ \\ell, u ] = [ m - z_{\\alpha/2} \\cdot SE(m), \\quad m + z_{\\alpha/2} \\cdot SE(m) ] $$\nwhere $\\alpha = 1 - c$. For $c = 0.95$, we have $\\alpha = 0.05$, and the critical value from the standard normal distribution is $z_{\\alpha/2} = z_{0.025} \\approx 1.95996$.\n\n**4. Algorithm Summary**\nFor each test case:\n1.  Define the vectors $x_i = L_i$, $n_i$, and $\\Delta L_i$.\n2.  Compute the transformed response variable $y_i = \\ln(n_i / \\Delta L_i)$.\n3.  Set the weights $w_i = n_i$.\n4.  Calculate the five sums: $S_w$, $S_{wx}$, $S_{wy}$, $S_{wxx}$, $S_{wxy}$.\n5.  Calculate the slope $m$ using its formula.\n6.  Calculate the standard error of the slope, $SE(m)$.\n7.  Determine the critical value $z_{0.025}$ for a $95\\%$ confidence interval.\n8.  Compute the lower bound $\\ell = m - z_{0.025} \\cdot SE(m)$ and upper bound $u = m + z_{0.025} \\cdot SE(m)$.\n9.  Return the triplet $[m, \\ell, u]$.\nThis procedure will be implemented for all provided test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the slope and confidence interval for log-transformed crystal size\n    distribution data using weighted linear regression.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"L\": np.array([0.25, 0.50, 0.75, 1.00, 1.25, 1.50, 1.75, 2.00]),\n            \"dL\": np.array([0.20, 0.20, 0.20, 0.20, 0.20, 0.25, 0.25, 0.30]),\n            \"n\": np.array([520, 400, 310, 250, 195, 150, 118, 90]),\n            \"c\": 0.95\n        },\n        {\n            \"L\": np.array([0.10, 0.30, 0.60, 1.00, 1.50, 2.20]),\n            \"dL\": np.array([0.10, 0.10, 0.20, 0.20, 0.30, 0.40]),\n            \"n\": np.array([12, 9, 7, 5, 4, 3]),\n            \"c\": 0.95\n        },\n        {\n            \"L\": np.array([0.50, 0.90, 1.30, 1.70, 2.10, 2.50]),\n            \"dL\": np.array([0.40, 0.40, 0.40, 0.40, 0.40, 0.40]),\n            \"n\": np.array([100, 98, 102, 99, 101, 100]),\n            \"c\": 0.95\n        },\n        {\n            \"L\": np.array([0.20, 0.40, 0.80, 1.60, 3.20]),\n            \"dL\": np.array([0.20, 0.20, 0.40, 0.80, 1.60]),\n            \"n\": np.array([800, 500, 280, 120, 40]),\n            \"c\": 0.95\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        L = case[\"L\"]\n        dL = case[\"dL\"]\n        n = case[\"n\"]\n        c = case[\"c\"]\n\n        # Step 1: Define variables for the regression\n        # Independent variable x is crystal size L\n        x = L\n        # Dependent variable y is log population density\n        # Population density N = n / dL\n        y = np.log(n / dL)\n        \n        # Step 2: Determine weights\n        # Based on Poisson counting statistics and propagation of uncertainty for\n        # the log transform, Var(ln(N)) is approx 1/n.\n        # Weights w are the inverse of the variance.\n        w = n\n\n        # Step 3: Calculate sums for weighted linear regression\n        S_w = np.sum(w)\n        S_wx = np.sum(w * x)\n        S_wy = np.sum(w * y)\n        S_wxx = np.sum(w * x * x)\n        S_wxy = np.sum(w * x * y)\n\n        # Step 4: Calculate the slope (m) of the weighted regression\n        numerator_m = S_w * S_wxy - S_wx * S_wy\n        denominator_m = S_w * S_wxx - S_wx**2\n        \n        # Handle potential division by zero if all x are identical\n        if np.isclose(denominator_m, 0):\n            # This case shouldn't occur with the given test data.\n            # A vertical line has an infinite slope, but more realistically,\n            # this implies no basis to estimate a slope. \n            # We would report NaN or handle as an error.\n            m = np.nan\n            se_m = np.nan\n        else:\n            m = numerator_m / denominator_m\n        \n            # Step 5: Calculate the standard error of the slope\n            # The denominator of Var(m) is the weighted sum of squares of x\n            ss_xx_w = S_wxx - (S_wx**2) / S_w\n        \n            if ss_xx_w = 0:\n                se_m = np.nan\n            else:\n                var_m = 1.0 / ss_xx_w\n                se_m = np.sqrt(var_m)\n\n        # Step 6: Calculate the confidence interval\n        if np.isnan(m) or np.isnan(se_m):\n            lower_bound, upper_bound = np.nan, np.nan\n        else:\n            alpha = 1 - c\n            z_critical = norm.ppf(1 - alpha / 2.0)\n            margin_of_error = z_critical * se_m\n            lower_bound = m - margin_of_error\n            upper_bound = m + margin_of_error\n\n        results.append([m, lower_bound, upper_bound])\n\n    # Final print statement in the exact required format.\n    # Convert each inner list to a string without spaces\n    result_strings = [f\"[{','.join(f'{v:.10f}' for v in res)}]\" for res in results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Our final practice shifts from interpreting existing data to predicting future states through forward modeling. Here, you will construct a numerical simulation to solve the CSD continuity equation itself, tackling the challenge of how a crystal population evolves over time under specified growth and nucleation laws (). This exercise introduces the powerful finite volume method, emphasizing the importance of designing schemes that are both physically realistic—preserving positivity and number—and numerically stable, a cornerstone of modern computational geochemistry.",
            "id": "4075782",
            "problem": "Consider the continuity equation in crystal size space used in crystal size distribution models within computational geochemistry, where the number density $n(L,t)$ evolves due to size growth rate $G(L,t)$ and a source term $J(L,t)$:\n$$\n\\frac{\\partial n}{\\partial t} + \\frac{\\partial}{\\partial L}\\big(G\\,n\\big) = J.\n$$\nHere $L$ denotes crystal size. You will design a finite volume discretization on a uniform grid in $L$-space that preserves both positivity and discrete number conservation under closed boundary conditions. The task is purely mathematical and algorithmic: derive, implement, and test a conservative, positivity-preserving scheme for this scalar conservation law with source.\n\nFundamental base:\n- The equation is a one-dimensional conservation law in size space with a source term.\n- Integrating a conservation law over a control volume yields the rate of change equal to net flux through the boundaries plus internal sources.\n\nSetup and units:\n- The size domain is $L \\in [L_{\\min}, L_{\\max}]$ with $L_{\\min} = 0\\,\\mu\\mathrm{m}$ and $L_{\\max} = 100\\,\\mu\\mathrm{m}$.\n- Use a uniform grid with $N$ cells and spacing $\\Delta L$, with $N = 100$ so $\\Delta L = 1\\,\\mu\\mathrm{m}$.\n- Time is in seconds, $t$ in $\\mathrm{s}$.\n- Number density $n$ has units $\\mathrm{\\#}\\,\\mu\\mathrm{m}^{-1}$.\n- Source term $J$ has units $\\mathrm{\\#}\\,\\mu\\mathrm{m}^{-1}\\,\\mathrm{s}^{-1}$.\n- Growth rate $G$ has units $\\mu\\mathrm{m}\\,\\mathrm{s}^{-1}$.\n- Use closed boundary conditions, meaning zero flux through $L=L_{\\min}$ and $L=L_{\\max}$.\n\nDesign requirements:\n- Derive the finite volume update by integrating the conservation law over each cell and approximating the face fluxes by a monotone, upwind choice informed by the sign of $G$ at faces. The design must ensure that the discrete scheme:\n  - Preserves positivity of $n$ provided a Courant–Friedrichs–Lewy (CFL) condition is satisfied.\n  - Preserves discrete number conservation (the integral of $n$ over the domain) in the absence of sources under closed boundary conditions, and reproduces the correct change when sources are present.\n- Explain how to evaluate the face fluxes at $L_{i+\\frac{1}{2}}$ from neighboring cell averages when $G$ may vary in space and have either sign.\n\nAlgorithmic constraints:\n- Select a time step $\\Delta t$ to satisfy a CFL condition based on the maximum magnitude of $G$ at faces: the update must be positivity-preserving for $n \\ge 0$ if the CFL number is less than or equal to $1$.\n- Implement the scheme on the specified grid with closed boundaries.\n\nTest suite:\nFor all tests, express the final results as dimensionless floats or booleans derived from the numerical values computed in the specified units. The program must run the following three cases, each starting from an initial condition and stepping to a final time using the CFL-limited time step with the last step adjusted to reach exactly the final time.\n\n- Case A (constant positive growth, no source):\n  - $G(L) = +1\\,\\mu\\mathrm{m}\\,\\mathrm{s}^{-1}$.\n  - $J(L) = 0$.\n  - Initial condition: $n(L,0) = \\exp\\!\\big(-\\frac{(L - 50)^2}{2\\cdot 10^2}\\big)$.\n  - Final time: $T = 45\\,\\mathrm{s}$.\n  - Compute a positivity check and a discrete conservation error defined as\n    $$\n    \\left| \\sum_i n_i(T)\\,\\Delta L - \\sum_i n_i(0)\\,\\Delta L - \\int_0^T \\int_{L_{\\min}}^{L_{\\max}} J(L)\\,\\mathrm{d}L\\,\\mathrm{d}t \\right|.\n    $$\n  - Under closed boundaries, the expected change is $0$.\n\n- Case B (spatially varying positive growth, uniform source):\n  - $G(L) = 0.5 + 0.5\\frac{L}{100}\\,\\mu\\mathrm{m}\\,\\mathrm{s}^{-1}$.\n  - $J(L) = 0.001\\,\\mathrm{\\#}\\,\\mu\\mathrm{m}^{-1}\\,\\mathrm{s}^{-1}$ (constant).\n  - Initial condition: $n(L,0) = 0$.\n  - Final time: $T = 10\\,\\mathrm{s}$.\n  - Compute a positivity check and a discrete conservation error defined as\n    $$\n    \\left| \\sum_i n_i(T)\\,\\Delta L - \\sum_i n_i(0)\\,\\Delta L - \\int_0^T \\int_{L_{\\min}}^{L_{\\max}} J(L)\\,\\mathrm{d}L\\,\\mathrm{d}t \\right|.\n    $$\n  - The expected increase is $J \\cdot (L_{\\max}-L_{\\min}) \\cdot T = 0.001 \\cdot 100 \\cdot 10 = 1.0$ in number (dimensionless after integration).\n\n- Case C (constant negative growth, no source):\n  - $G(L) = -0.8\\,\\mu\\mathrm{m}\\,\\mathrm{s}^{-1}$.\n  - $J(L) = 0$.\n  - Initial condition: $n(L,0) = \\exp\\!\\big(-\\frac{(L - 70)^2}{2\\cdot 8^2}\\big)$.\n  - Final time: $T = 40\\,\\mathrm{s}$.\n  - Compute a positivity check and a discrete conservation error as in Case A.\n\nFlux evaluation requirement:\n- You must explicitly explain in your solution how the face flux $F_{i+\\frac{1}{2}}$ is evaluated from neighboring cell averages using an upwind choice determined by the sign of $G$ at the face, and why this ensures positivity.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order:\n  - $[\\text{positivity\\_A}, \\text{error\\_A}, \\text{positivity\\_B}, \\text{error\\_B}, \\text{positivity\\_C}, \\text{error\\_C}]$,\n  where each positivity value is a boolean and each error value is a float.",
            "solution": "The problem requires the design and implementation of a finite volume scheme for the one-dimensional continuity equation governing crystal size distributions, given by:\n$$\n\\frac{\\partial n}{\\partial t} + \\frac{\\partial}{\\partial L}\\big(G\\,n\\big) = J\n$$\nwhere $n(L,t)$ is the number density of crystals of size $L$ at time $t$, $G(L,t)$ is the size-dependent growth rate, and $J(L,t)$ is a source term. The scheme must be conservative, positivity-preserving, and implemented on a uniform grid with closed boundary conditions.\n\nThe derivation and implementation proceed as follows.\n\n**1. Finite Volume Discretization**\n\nWe discretize the spatial domain $L \\in [L_{\\min}, L_{\\max}]$, where $L_{\\min} = 0\\,\\mu\\mathrm{m}$ and $L_{\\max} = 100\\,\\mu\\mathrm{m}$, into $N=100$ uniform cells. Each cell $C_i$ is defined by the interval $[L_{i-1/2}, L_{i+1/2}]$ for $i=1, \\dots, N$. The cell width is $\\Delta L = (L_{\\max} - L_{\\min})/N = 1\\,\\mu\\mathrm{m}$. The cell interfaces are located at $L_{i+1/2} = L_{\\min} + (i+1)\\Delta L$ for $i=-1, 0, \\dots, N-1$ (using zero-based indexing for implementation, $i=0, \\dots, N-1$), such that $L_{-1/2} = L_{\\min}$ and $L_{N-1/2} = L_{\\max}$. The cell centers are $L_i = L_{\\min} + (i+1/2)\\Delta L$. Note that in typical code implementation (0-indexed cells $i=0, ..., N-1$), the interfaces are $L_{i+1/2}$ for $i=0, ..., N$, where $L_{1/2}$ is the first interface, and the cell centers are $L_i = (i+0.5)\\Delta L$. We'll use this latter convention.\n\nWe integrate the governing partial differential equation (PDE) over a single cell $C_i = [L_{i-1/2}, L_{i+1/2}]$:\n$$\n\\int_{L_{i-1/2}}^{L_{i+1/2}} \\frac{\\partial n}{\\partial t} \\, \\mathrm{d}L + \\int_{L_{i-1/2}}^{L_{i+1/2}} \\frac{\\partial (Gn)}{\\partial L} \\, \\mathrm{d}L = \\int_{L_{i-1/2}}^{L_{i+1/2}} J \\, \\mathrm{d}L\n$$\nBy applying the fundamental theorem of calculus to the second term and assuming $n$ is sufficiently smooth, we obtain:\n$$\n\\Delta L \\frac{\\mathrm{d} n_i}{\\mathrm{d} t} + \\big(Gn\\big)\\big|_{L_{i+1/2}} - \\big(Gn\\big)\\big|_{L_{i-1/2}} = J_i \\Delta L\n$$\nHere, $n_i(t) = \\frac{1}{\\Delta L} \\int_{L_{i-1/2}}^{L_{i+1/2}} n(L,t) \\, \\mathrm{d}L$ is the cell-averaged number density, and $J_i(t) = \\frac{1}{\\Delta L} \\int_{L_{i-1/2}}^{L_{i+1/2}} J(L,t) \\, \\mathrm{d}L$ is the cell-averaged source term, which we approximate as $J_i(t) \\approx J(L_i, t)$. The term $F(L, t) = G(L, t) n(L, t)$ is the flux of crystals. The equation is rewritten in terms of numerical fluxes $F_{i\\pm 1/2}$ that approximate the continuous flux at cell interfaces:\n$$\n\\frac{\\mathrm{d} n_i}{\\mathrm{d} t} = -\\frac{1}{\\Delta L} \\left( F_{i+1/2} - F_{i-1/2} \\right) + J_i\n$$\nThis is a semi-discrete system of ordinary differential equations for the cell averages $n_i$.\n\n**2. Numerical Flux and Boundary Conditions**\n\nTo ensure stability and positivity, we employ a first-order upwind scheme. The flux at an interface depends on the direction of information flow, which is determined by the sign of the growth rate $G$ at that interface. The numerical flux $F_{i+1/2}$ at the interface between cell $i$ (to the left) and cell $i+1$ (to the right) is evaluated as follows:\n$$\nF_{i+1/2} =\n\\begin{cases}\nG_{i+1/2} \\, n_i  \\text{if } G_{i+1/2} \\geq 0 \\\\\nG_{i+1/2} \\, n_{i+1}  \\text{if } G_{i+1/2}  0\n\\end{cases}\n$$\nwhere $G_{i+1/2} = G(L_{i+1/2})$. If the growth rate is positive, crystals \"grow\" from smaller sizes (cell $i$) to larger sizes (cell $i+1$), so the state in cell $i$ determines the flux. Conversely, if the growth rate is negative (dissolution), crystals \"shrink\" from cell $i+1$ into cell $i$, so the state in cell $i+1$ is used. This choice ensures that the scheme is monotone and avoids unphysical oscillations.\n\nThe problem specifies closed boundary conditions, meaning zero flux at the domain boundaries $L_{\\min}$ (interface $L_{1/2}$) and $L_{\\max}$ (interface $L_{N+1/2}$). This is enforced by setting the boundary fluxes to zero:\n$$\nF_{1/2} = 0 \\quad \\text{and} \\quad F_{N+1/2} = 0\n$$\n\n**3. Fully Discrete Scheme and Properties**\n\nWe discretize in time using the forward Euler method with a time step $\\Delta t$:\n$$\n\\frac{n_i^{k+1} - n_i^k}{\\Delta t} = -\\frac{1}{\\Delta L} \\left( F_{i+1/2}^k - F_{i-1/2}^k \\right) + J_i^k\n$$\nwhere the superscript $k$ denotes the time level $t_k = k\\Delta t$. The explicit update rule for the cell average $n_i$ is:\n$$\nn_i^{k+1} = n_i^k - \\frac{\\Delta t}{\\Delta L} \\left( F_{i+1/2}^k - F_{i-1/2}^k \\right) + \\Delta t J_i^k\n$$\n\n**Positivity Preservation:** The upwind flux formulation is key to preserving the positivity of $n_i$. To demonstrate this, we substitute the flux definition into the update equation. Let's rewrite the flux using the positive and negative parts of $G$, where $G^+ = \\max(G, 0)$ and $G^- = \\min(G, 0)$:\n$$\nF_{i+1/2} = G_{i+1/2}^+ n_i + G_{i+1/2}^- n_{i+1}\n$$\nThe update equation becomes:\n$$\nn_i^{k+1} = n_i^k - \\frac{\\Delta t}{\\Delta L} \\left[ (G_{i+1/2}^+ n_i^k + G_{i+1/2}^- n_{i+1}^k) - (G_{i-1/2}^+ n_{i-1}^k + G_{i-1/2}^- n_i^k) \\right] + \\Delta t J_i^k\n$$\nRearranging terms based on which cell average they multiply:\n$$\nn_i^{k+1} = n_i^k \\left(1 - \\frac{\\Delta t}{\\Delta L}(G_{i+1/2}^+ - G_{i-1/2}^-)\\right) + n_{i-1}^k \\left(\\frac{\\Delta t}{\\Delta L}G_{i-1/2}^+\\right) + n_{i+1}^k \\left(-\\frac{\\Delta t}{\\Delta L}G_{i+1/2}^-\\right) + \\Delta t J_i^k\n$$\nFor $n_i^{k+1}$ to remain non-negative, assuming $n_j^k \\ge 0$ for all $j$ and $J_i^k \\ge 0$, all coefficients of the $n_j^k$ terms must be non-negative. Since $G_{i-1/2}^+ \\ge 0$ and $G_{i+1/2}^- \\le 0$, the coefficients for $n_{i-1}^k$ and $n_{i+1}^k$ are non-negative. Positivity then hinges on the coefficient of $n_i^k$ being non-negative:\n$$\n1 - \\frac{\\Delta t}{\\Delta L}(G_{i+1/2}^+ - G_{i-1/2}^-) \\ge 0 \\implies \\Delta t \\le \\frac{\\Delta L}{G_{i+1/2}^+ - G_{i-1/2}^-}\n$$\nThis condition must hold for all cells $i$. For the special cases where $G$ does not change sign (as in all test cases provided), this condition simplifies. If $G0$, then $G^-=0$ and the condition becomes $\\Delta t \\le \\Delta L / G_{i+1/2}$. If $G0$, then $G^+=0$ and the condition becomes $\\Delta t \\le \\Delta L/(-G_{i-1/2}) = \\Delta L/|G_{i-1/2}|$. Both are encompassed by the Courant–Friedrichs–Lewy (CFL) condition:\n$$\n\\Delta t \\le \\frac{\\Delta L}{\\max_{L} |G(L)|}\n$$\nThis corresponds to a CFL number $C = \\frac{\\max|G|\\Delta t}{\\Delta L} \\le 1$, as specified in the problem.\n\n**Discrete Conservation:** To verify conservation, we sum the fully discrete update equation over all cells $i=1, \\dots, N$:\n$$\n\\sum_{i=1}^{N} n_i^{k+1} \\Delta L = \\sum_{i=1}^{N} n_i^{k} \\Delta L - \\Delta t \\sum_{i=1}^{N} (F_{i+1/2}^k - F_{i-1/2}^k) + \\Delta t \\sum_{i=1}^{N} J_i^k \\Delta L\n$$\nThe sum of the fluxes forms a telescoping series:\n$$\n\\sum_{i=1}^{N} (F_{i+1/2}^k - F_{i-1/2}^k) = F_{N+1/2}^k - F_{1/2}^k\n$$\nApplying the closed boundary conditions, $F_{1/2}^k = 0$ and $F_{N+1/2}^k = 0$, this sum is zero. The equation for the total number of crystals, $\\mathcal{N}^k = \\sum_{i=1}^{N} n_i^k \\Delta L$, becomes:\n$$\n\\mathcal{N}^{k+1} = \\mathcal{N}^{k} + \\Delta t \\sum_{i=1}^{N} J_i^k \\Delta L\n$$\nThis demonstrates that the change in the total discrete number of crystals from one time step to the next is precisely equal to the total number of crystals added by the source term during that time step. In the absence of sources ($J=0$), the total number of crystals is exactly conserved, $\\mathcal{N}^{k+1} = \\mathcal{N}^{k}$. The scheme is therefore discretely conservative.\n\nThe derived numerical scheme fulfills all requirements: it is based on a finite volume formulation, uses a positivity-preserving upwind flux, respects closed boundary conditions, and is discretely conservative.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the crystal size distribution problem for three test cases.\n    \"\"\"\n\n    def run_simulation(G_func, J_func, n0_func, T_final, N, L_min, L_max):\n        \"\"\"\n        Runs a single simulation of the finite volume scheme.\n\n        Args:\n            G_func (callable): Growth rate function G(L).\n            J_func (callable): Source term function J(L).\n            n0_func (callable): Initial condition function n(L, 0).\n            T_final (float): Final simulation time.\n            N (int): Number of grid cells.\n            L_min (float): Minimum size of the domain.\n            L_max (float): Maximum size of the domain.\n\n        Returns:\n            tuple: A tuple containing (positivity_check, conservation_error).\n        \"\"\"\n        # 1. Setup Grid\n        dL = (L_max - L_min) / N\n        L_centers = L_min + (np.arange(N) + 0.5) * dL\n        L_faces = L_min + np.arange(N + 1) * dL\n\n        # 2. Set Initial Condition and Total Source\n        n = n0_func(L_centers)\n        n_initial_total = np.sum(n * dL)\n        \n        # The source J is constant in space and time for the relevant case\n        # allowing for direct calculation of the total source integral.\n        # J_val = 0 for cases A and C.\n        j_val_at_center_0 = J_func(L_centers)[0] if N  0 else 0\n        total_source_integral = j_val_at_center_0 * (L_max - L_min) * T_final\n        \n        # 3. Determine Time Step from CFL condition\n        G_at_faces = G_func(L_faces)\n        G_max = np.max(np.abs(G_at_faces))\n        \n        # CFL number = 1 for positivity. We use 1.0.\n        # If G_max is zero, dt can be any value, T_final will suffice.\n        if G_max == 0:\n            dt = T_final\n        else:\n            dt = dL / G_max\n        \n        # Adjust dt to hit T_final exactly\n        num_steps = int(np.ceil(T_final / dt))\n        if num_steps == 0:\n            num_steps = 1\n        dt_actual = T_final / num_steps\n\n        # 4. Main Time-Stepping Loop\n        for _ in range(num_steps):\n            # Calculate fluxes at internal faces (F_1/2 to F_N-1/2)\n            # In 0-indexed python, this corresponds to F[1] through F[N-1]\n            F = np.zeros(N + 1)\n            G_at_faces = G_func(L_faces)\n            \n            # Using a loop for clarity, for N=100 performance is not critical\n            for i in range(1, N): # Corresponds to interfaces L_{1/2} to L_{N-1/2}\n                G_val = G_at_faces[i]\n                if G_val = 0:\n                    F[i] = G_val * n[i - 1] # Upwind from cell i-1\n                else:\n                    F[i] = G_val * n[i]     # Upwind from cell i\n            \n            # Boundary fluxes F[0] (at L_min) and F[N] (at L_max) are zero\n            \n            # Update cell averages using forward Euler\n            J_at_centers = J_func(L_centers)\n            n_new = n - (dt_actual / dL) * (F[1:] - F[:-1]) + dt_actual * J_at_centers\n            n = n_new\n\n        # 5. Compute Final Metrics\n        n_final = n\n        \n        # Positivity check: all elements of n must be non-negative\n        positivity_check = bool(np.all(n_final = 0.0))\n        \n        # Conservation error\n        n_final_total = np.sum(n_final * dL)\n        \n        conservation_error = abs(n_final_total - n_initial_total - total_source_integral)\n        \n        return positivity_check, conservation_error\n\n    # --- Test Case Definitions ---\n    N = 100\n    L_min = 0.0\n    L_max = 100.0\n    \n    # Case A\n    G_A = lambda L: np.full_like(L, 1.0, dtype=float)\n    J_A = lambda L: np.zeros_like(L, dtype=float)\n    n0_A = lambda L: np.exp(-(L - 50.0)**2 / (2.0 * 10.0**2))\n    T_A = 45.0\n\n    # Case B\n    G_B = lambda L: 0.5 + 0.5 * L / 100.0\n    J_B = lambda L: np.full_like(L, 0.001, dtype=float)\n    n0_B = lambda L: np.zeros_like(L, dtype=float)\n    T_B = 10.0\n\n    # Case C\n    G_C = lambda L: np.full_like(L, -0.8, dtype=float)\n    J_C = lambda L: np.zeros_like(L, dtype=float)\n    n0_C = lambda L: np.exp(-(L - 70.0)**2 / (2.0 * 8.0**2))\n    T_C = 40.0\n\n    test_cases_params = [\n        (G_A, J_A, n0_A, T_A),\n        (G_B, J_B, n0_B, T_B),\n        (G_C, J_C, n0_C, T_C),\n    ]\n\n    results = []\n    for g_func, j_func, n0_func, T_val in test_cases_params:\n        pos, err = run_simulation(g_func, j_func, n0_func, T_val, N, L_min, L_max)\n        results.extend([pos, err])\n    \n    # Format the final output string\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}