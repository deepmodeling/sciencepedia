{
    "hands_on_practices": [
        {
            "introduction": "This first exercise establishes the foundational principle of solution under-relaxation. You will derive the standard update formula from first principles and apply it to a simple thermal node, directly observing how the relaxation factor $\\alpha$ controls the step size of the iteration. This practice is crucial for understanding the fundamental trade-off between convergence speed and numerical stability that governs all iterative solvers .",
            "id": "3983933",
            "problem": "A single interior node of a steady one-dimensional thermal resistive network with temperature-dependent boundary heat exchange is modeled after spatial discretization by a nonlinear algebraic balance of the form $F(\\phi)=0$, where $\\phi$ denotes the nodal temperature in $\\mathrm{K}$. To obtain a numerical solution, a fixed-point iteration is constructed by algebraically isolating $\\phi$ on the left-hand side of a locally linearized form of $F(\\phi)=0$, which defines a mapping $G(\\phi)$ such that a so-called raw update is given by $\\phi^{\\star}=G(\\phi^{k})$. In practice, to enhance robustness, solution under-relaxation is employed, wherein the next iterate $\\phi^{k+1}$ is obtained by mixing the current state $\\phi^{k}$ and the raw update $\\phi^{\\star}$ using a relaxation factor $\\alpha\\in(0,1]$.\n\nStarting from the fixed-point form and the requirement that any true steady solution $\\phi^{\\infty}$ remain invariant under iteration, derive the under-relaxed update as a convex combination of $\\phi^{k}$ and $\\phi^{\\star}$. Then, for a particular node in such a network, suppose the current iterate is $\\phi^{k}=0\\,\\mathrm{K}$ and the raw update computed from the local fixed-point mapping is $\\phi^{\\star}=2\\,\\mathrm{K}$. Compute the next iterate $\\phi^{k+1}$ for relaxation factors $\\alpha=0.2$, $\\alpha=0.5$, and $\\alpha=0.9$. Express each computed temperature in $\\mathrm{K}$ and round your answers to four significant figures.\n\nFinally, using a local linearization of the fixed-point map near a steady solution in the scalar case, represented as $G(\\phi)\\approx \\phi^{\\infty}+\\lambda\\left(\\phi-\\phi^{\\infty}\\right)$ with $|\\lambda|<1$, argue how the relaxation factor $\\alpha$ influences the magnitude of the error propagation factor in the under-relaxed iteration and discuss the trade-off between stability and speed without computing any further numerical values.",
            "solution": "The problem consists of three parts:\n1.  Derivation of the under-relaxed update equation.\n2.  Numerical calculation of the next iterate for specific conditions.\n3.  Theoretical analysis of the effect of the relaxation factor on convergence.\n\nWe will address each part in sequence.\n\nFirst, we derive the under-relaxed update formula. The goal is to obtain the next iterate, $\\phi^{k+1}$, by blending the current iterate, $\\phi^{k}$, and the raw update, $\\phi^{\\star}$. The raw update is defined by the fixed-point mapping $\\phi^{\\star}=G(\\phi^k)$.\n\nThe change proposed by the raw update is the difference $\\delta\\phi = \\phi^{\\star} - \\phi^{k}$. In a standard fixed-point iteration (also known as Picard or successive substitution iteration), the next iterate would be $\\phi^{k+1} = \\phi^{\\star}$. In under-relaxation, only a fraction $\\alpha$ of this proposed change is applied to the current state. The factor $\\alpha$ is the relaxation factor, with $\\alpha \\in (0, 1]$.\n\nThus, the next iterate is formed by adding this damped change to the current iterate:\n$$\n\\phi^{k+1} = \\phi^{k} + \\alpha(\\phi^{\\star} - \\phi^{k})\n$$\nThis expression can be rearranged algebraically:\n$$\n\\phi^{k+1} = \\phi^{k} - \\alpha\\phi^{k} + \\alpha\\phi^{\\star}\n$$\n$$\n\\phi^{k+1} = (1-\\alpha)\\phi^{k} + \\alpha\\phi^{\\star}\n$$\nThis is a convex combination of $\\phi^{k}$ and $\\phi^{\\star}$, since the coefficients $(1-\\alpha)$ and $\\alpha$ are non-negative for $\\alpha \\in [0, 1]$ and sum to $(1-\\alpha) + \\alpha = 1$.\n\nThe problem states that any true steady solution $\\phi^{\\infty}$ must remain invariant under the iteration. A steady solution is a fixed point of the mapping $G$, meaning $G(\\phi^{\\infty})=\\phi^{\\infty}$. Let's test our derived formula. If the current iterate is the solution, $\\phi^{k} = \\phi^{\\infty}$, then the raw update is $\\phi^{\\star} = G(\\phi^{k}) = G(\\phi^{\\infty}) = \\phi^{\\infty}$. Substituting these into the update formula gives:\n$$\n\\phi^{k+1} = (1-\\alpha)\\phi^{\\infty} + \\alpha\\phi^{\\infty} = (1-\\alpha+\\alpha)\\phi^{\\infty} = 1 \\cdot \\phi^{\\infty} = \\phi^{\\infty}\n$$\nThe solution indeed remains invariant, which confirms the correctness of the derived form.\n\nSecond, we compute the next iterate $\\phi^{k+1}$ for the given numerical values. The givens are the current iterate $\\phi^{k}=0\\,\\mathrm{K}$ and the raw update $\\phi^{\\star}=2\\,\\mathrm{K}$. We use the derived formula $\\phi^{k+1} = (1-\\alpha)\\phi^{k} + \\alpha\\phi^{\\star}$.\n\nFor $\\alpha=0.2$:\n$$\n\\phi^{k+1} = (1-0.2)(0\\,\\mathrm{K}) + (0.2)(2\\,\\mathrm{K}) = (0.8)(0\\,\\mathrm{K}) + (0.2)(2\\,\\mathrm{K}) = 0\\,\\mathrm{K} + 0.4\\,\\mathrm{K} = 0.4\\,\\mathrm{K}\n$$\nRounded to four significant figures, this is $0.4000\\,\\mathrm{K}$.\n\nFor $\\alpha=0.5$:\n$$\n\\phi^{k+1} = (1-0.5)(0\\,\\mathrm{K}) + (0.5)(2\\,\\mathrm{K}) = (0.5)(0\\,\\mathrm{K}) + (0.5)(2\\,\\mathrm{K}) = 0\\,\\mathrm{K} + 1.0\\,\\mathrm{K} = 1.0\\,\\mathrm{K}\n$$\nRounded to four significant figures, this is $1.000\\,\\mathrm{K}$.\n\nFor $\\alpha=0.9$:\n$$\n\\phi^{k+1} = (1-0.9)(0\\,\\mathrm{K}) + (0.9)(2\\,\\mathrm{K}) = (0.1)(0\\,\\mathrm{K}) + (0.9)(2\\,\\mathrm{K}) = 0\\,\\mathrm{K} + 1.8\\,\\mathrm{K} = 1.8\\,\\mathrm{K}\n$$\nRounded to four significant figures, this is $1.800\\,\\mathrm{K}$.\n\nThird, we analyze the influence of the relaxation factor $\\alpha$ on convergence. Let the error at iteration $k$ be defined as $e^{k} = \\phi^{k} - \\phi^{\\infty}$. The under-relaxed iteration scheme is:\n$$\n\\phi^{k+1} = (1-\\alpha)\\phi^{k} + \\alpha G(\\phi^{k})\n$$\nWe are given the local linearization of $G(\\phi)$ near the solution $\\phi^{\\infty}$:\n$$\nG(\\phi^{k}) \\approx \\phi^{\\infty} + \\lambda(\\phi^{k} - \\phi^{\\infty})\n$$\nwhere $|\\lambda|<1$. Substituting this into the iteration scheme:\n$$\n\\phi^{k+1} \\approx (1-\\alpha)\\phi^{k} + \\alpha[\\phi^{\\infty} + \\lambda(\\phi^{k} - \\phi^{\\infty})]\n$$\nTo find the relationship between successive errors, we subtract $\\phi^{\\infty}$ from both sides. Note that $\\phi^{\\infty}$ can be written as $(1-\\alpha)\\phi^{\\infty} + \\alpha\\phi^{\\infty}$.\n$$\n\\phi^{k+1} - \\phi^{\\infty} \\approx [(1-\\alpha)\\phi^{k} - (1-\\alpha)\\phi^{\\infty}] + [\\alpha\\phi^{\\infty} + \\alpha\\lambda(\\phi^{k} - \\phi^{\\infty}) - \\alpha\\phi^{\\infty}]\n$$\n$$\n\\phi^{k+1} - \\phi^{\\infty} \\approx (1-\\alpha)(\\phi^{k} - \\phi^{\\infty}) + \\alpha\\lambda(\\phi^{k} - \\phi^{\\infty})\n$$\nFactoring out the error term $(\\phi^k - \\phi^{\\infty}) = e^k$:\n$$\ne^{k+1} \\approx [(1-\\alpha) + \\alpha\\lambda] e^{k}\n$$\nThe error propagation factor for the under-relaxed scheme, let's call it $\\lambda_{\\alpha}$, is therefore:\n$$\n\\lambda_{\\alpha} = (1-\\alpha) + \\alpha\\lambda\n$$\nFor the iteration to converge, the magnitude of this factor must be less than $1$, i.e., $|\\lambda_{\\alpha}| < 1$. The value of $|\\lambda_{\\alpha}|$ dictates the convergence speed; a smaller magnitude implies faster convergence. The factor $\\lambda_{\\alpha}$ is a linear interpolation between $\\lambda_{\\alpha}=1$ (for $\\alpha=0$) and $\\lambda_{\\alpha}=\\lambda$ (for $\\alpha=1$).\n\nThe trade-off between stability and speed is as follows:\n- **Stability**: Decreasing the relaxation factor $\\alpha$ (i.e., making it smaller than $1$) makes the update more conservative, as $\\phi^{k+1}$ remains closer to $\\phi^{k}$. This damps large oscillations and can stabilize an otherwise divergent or unstable iteration. For a non-linear problem, where the local effective $\\lambda$ can vary and temporarily exceed $1$, a small $\\alpha$ provides a robustness margin, preventing the solution from diverging. However, this increased stability comes at a cost.\n- **Speed**: The rate of convergence is determined by $|\\lambda_{\\alpha}|$.\n    - If the base iteration is already convergent and monotonic ($0 < \\lambda < 1$), then for any $\\alpha \\in (0,1)$, we have $\\lambda < \\lambda_{\\alpha} < 1$. In this case, under-relaxation ($\\alpha < 1$) slows down convergence, and the fastest rate is achieved with no relaxation ($\\alpha=1$).\n    - If the base iteration is convergent and oscillatory ($-1 < \\lambda < 0$), there may exist an optimal relaxation factor $\\alpha_{\\text{opt}} = \\frac{1}{1-\\lambda}$ which makes $\\lambda_{\\alpha}=0$. Since $-1 < \\lambda < 0$, we have $1/2 < \\alpha_{\\text{opt}} < 1$. Using $\\alpha<1$ can thus accelerate convergence.\n    - In general, choosing a small $\\alpha$ makes $|\\lambda_{\\alpha}|$ closer to $1$ (since $\\lambda_{\\alpha}$ approaches $1$ as $\\alpha \\to 0$), which corresponds to slow convergence. Choosing a larger $\\alpha$ (closer to $1$) moves $\\lambda_{\\alpha}$ towards $\\lambda$, which can be faster if the base iteration is well-behaved, but riskier if it is not.\n\nIn conclusion, the relaxation factor $\\alpha$ is a control parameter to balance the competing demands of convergence speed and iterative stability. A small $\\alpha$ favors stability over speed, while a large $\\alpha$ favors speed at the potential risk of instability.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.4000 & 1.000 & 1.800\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Building on the basics, we now shift our focus from merely ensuring convergence to actively accelerating it. This problem challenges you to find the *optimal* relaxation parameter that yields the fastest possible convergence for a well-behaved linear system, typical of discretized heat conduction problems. By analyzing the system's eigenvalues, you will learn how to minimize the worst-case error reduction factor, a powerful technique for designing efficient solvers .",
            "id": "3983939",
            "problem": "A steady-state heat conduction problem is discretized using a finite volume method on a structured grid, leading to a linear system $A T = b$, where $A \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite (SPD), $T \\in \\mathbb{R}^{n}$ is the vector of nodal temperatures, and $b \\in \\mathbb{R}^{n}$ arises from source terms and boundary conditions derived from Fourier’s law and energy conservation. Consider the stationary under-relaxed iterative update $T_{k+1} = T_{k} + \\alpha r_{k}$, where the residual is $r_{k} = b - A T_{k}$ and $\\alpha > 0$ is the relaxation parameter. This yields a residual evolution $r_{k+1} = (I - \\alpha A) r_{k}$. \n\nStarting from the properties of SPD matrices and the eigen-decomposition of $A$, derive a closed-form expression for the per-iteration worst-case residual reduction factor, defined as the spectral radius of the iteration operator $I - \\alpha A$ over the spectrum of $A$. Then, determine the value of $\\alpha$ that minimizes this worst-case reduction factor under the assumption that the spectrum of $A$ lies within the closed interval $[\\lambda_{\\min}, \\lambda_{\\max}]$ with $0 < \\lambda_{\\min} \\leq \\lambda_{\\max}$. Finally, for a particular two-dimensional conduction discretization, the computed eigenvalues of $A$ are:\n$$\n\\{0.45,\\, 0.62,\\, 0.89,\\, 1.35,\\, 1.91,\\, 2.76,\\, 3.18,\\, 3.86\\}.\n$$\nUsing your derived expressions, compute the minimized worst-case residual reduction factor for these eigenvalues. Round your answer to four significant figures and express it as a dimensionless decimal number with no units.",
            "solution": "The problem asks for a derivation of the optimal relaxation parameter $\\alpha$ and the corresponding minimized worst-case reduction factor for a relaxed Richardson iteration, followed by a numerical calculation.\n\nFirst, we analyze the iteration. The error $e_k = T_k - T^{\\star}$ (where $AT^{\\star}=b$) evolves as $e_{k+1} = e_k + \\alpha(b - A(e_k + T^{\\star})) = e_k + \\alpha(b - A e_k - b) = (I - \\alpha A) e_k$. The iteration converges if the spectral radius of the iteration matrix $G = I - \\alpha A$ is less than 1, i.e., $\\rho(G)  1$. The spectral radius is the worst-case error reduction factor per iteration.\n\nLet $\\lambda_i$ be an eigenvalue of $A$. The corresponding eigenvalue of $G$ is $\\mu_i = 1 - \\alpha \\lambda_i$. The spectral radius is the maximum magnitude of these eigenvalues:\n$$\n\\rho(G) = \\max_i |1 - \\alpha \\lambda_i|\n$$\nSince $A$ is symmetric positive definite (SPD), all its eigenvalues $\\lambda_i$ are real and positive. The spectrum of $A$ is contained in the interval $[\\lambda_{\\min}, \\lambda_{\\max}]$. The function $f(\\lambda) = |1 - \\alpha \\lambda|$ is a V-shaped function whose maximum over an interval must occur at one of the endpoints. Therefore, the spectral radius is given by:\n$$\n\\rho(G) = \\max \\left( |1 - \\alpha \\lambda_{\\min}|, |1 - \\alpha \\lambda_{\\max}| \\right)\n$$\nSince $\\lambda_{\\min}$ and $\\lambda_{\\max}$ are positive, and we are interested in $\\alpha > 0$ that leads to convergence, we are looking for $\\alpha$ such that $|1 - \\alpha \\lambda|  1$ for all $\\lambda$. This implies $0  \\alpha\\lambda  2$, so we must have $1 - \\alpha \\lambda_{\\max} > -1$. In this range, $1 - \\alpha \\lambda_{\\min}$ is positive and $1 - \\alpha \\lambda_{\\max}$ can be negative. The expression simplifies to:\n$$\n\\rho(G) = \\max \\left( 1 - \\alpha \\lambda_{\\min}, \\alpha \\lambda_{\\max} - 1 \\right)\n$$\nWe want to find the value of $\\alpha$ that minimizes this maximum. The minimum of the maximum of two competing functions occurs when the functions are equal:\n$$\n1 - \\alpha \\lambda_{\\min} = \\alpha \\lambda_{\\max} - 1\n$$\nSolving for this optimal relaxation parameter, $\\alpha_{\\text{opt}}$:\n$$\n2 = \\alpha (\\lambda_{\\min} + \\lambda_{\\max})\n$$\n$$\n\\alpha_{\\text{opt}} = \\frac{2}{\\lambda_{\\min} + \\lambda_{\\max}}\n$$\nNow, we find the minimized worst-case reduction factor by substituting $\\alpha_{\\text{opt}}$ back into the expression for $\\rho(G)$. We can use either part of the `max` function, as they are equal at the optimum.\n$$\n\\rho_{\\min} = 1 - \\alpha_{\\text{opt}} \\lambda_{\\min} = 1 - \\frac{2 \\lambda_{\\min}}{\\lambda_{\\min} + \\lambda_{\\max}} = \\frac{(\\lambda_{\\min} + \\lambda_{\\max}) - 2 \\lambda_{\\min}}{\\lambda_{\\min} + \\lambda_{\\max}} = \\frac{\\lambda_{\\max} - \\lambda_{\\min}}{\\lambda_{\\max} + \\lambda_{\\min}}\n$$\nThis can also be written in terms of the condition number of the matrix, $\\kappa(A) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}}$, as $\\rho_{\\min} = \\frac{\\kappa - 1}{\\kappa + 1}$.\n\nFinally, we apply these results to the given set of eigenvalues:\n$$\n\\{0.45,\\, 0.62,\\, 0.89,\\, 1.35,\\, 1.91,\\, 2.76,\\, 3.18,\\, 3.86\\}\n$$\nFrom this set, we identify the minimum and maximum eigenvalues:\n$$\n\\lambda_{\\min} = 0.45\n$$\n$$\n\\lambda_{\\max} = 3.86\n$$\nUsing the derived formula for the minimized worst-case reduction factor:\n$$\n\\rho_{\\min} = \\frac{\\lambda_{\\max} - \\lambda_{\\min}}{\\lambda_{\\max} + \\lambda_{\\min}} = \\frac{3.86 - 0.45}{3.86 + 0.45} = \\frac{3.41}{4.31}\n$$\nPerforming the division and rounding to four significant figures:\n$$\n\\rho_{\\min} \\approx 0.79118329... \\approx 0.7912\n$$\nThe minimized worst-case residual reduction factor is $0.7912$.",
            "answer": "$$\n\\boxed{0.7912}\n$$"
        },
        {
            "introduction": "In real-world simulations, iterative systems are not always as well-behaved as the symmetric positive definite case explored previously. This final practice explores the primary role of under-relaxation: guaranteeing stability for general, potentially troublesome systems. You will analyze a non-symmetric iteration matrix to determine the absolute limit on the relaxation factor, beyond which the solution process becomes unstable, a key skill for developing robust numerical schemes .",
            "id": "3983945",
            "problem": "Consider a pseudo-time-stepping under-relaxation scheme used to stabilize a fixed-point solver for the steady-state heat conduction equation. Beginning from energy conservation and Fourier’s law, spatial discretization of the Laplacian operator in the heat equation yields a linear residual mapping $R(\\mathbf{T}) = F \\mathbf{T} - \\mathbf{b}$, where $F$ is the Jacobian of the residual with respect to the temperature vector $\\mathbf{T}$. A first-order explicit pseudo-time update is applied to drive the residual to zero:\n$$\n\\mathbf{T}^{k+1} = \\mathbf{T}^{k} - \\alpha \\left(F \\mathbf{T}^{k} - \\mathbf{b}\\right),\n$$\nwhere $\\alpha$ is a dimensionless under-relaxation parameter. Linearizing the update around the steady solution $\\mathbf{T}^{\\star}$, the error $\\mathbf{e}^{k} = \\mathbf{T}^{k} - \\mathbf{T}^{\\star}$ evolves as\n$$\n\\mathbf{e}^{k+1} = G_{\\alpha} \\, \\mathbf{e}^{k}, \\quad \\text{with} \\quad G_{\\alpha} = I - \\alpha F.\n$$\nConvergence of the linear iteration is guaranteed if and only if the spectral radius $\\rho(G_{\\alpha})$ satisfies $\\rho(G_{\\alpha})  1$. Let the iteration matrix be\n$$\nF = \\begin{bmatrix} 1.3  0.4 \\\\ 0.1  0.9 \\end{bmatrix}.\n$$\nStarting from the standard eigenvalue problem definition and the convergence criterion stated above, do the following:\n- Compute the eigenvalues of $F$ exactly.\n- Derive, from first principles of linear iterative stability, the largest admissible under-relaxation parameter $\\alpha$ that makes $G_{\\alpha}$ marginally stable (i.e., the critical threshold at which $\\rho(G_{\\alpha}) = 1$), and use it to state the strict feasibility condition for convergence.\n- Verify this threshold by explicitly evaluating $\\rho(G_{\\alpha})$ at the threshold value using the eigenvalues you obtained.\n\nReport the largest admissible $\\alpha$ as a single closed-form analytic expression. Do not approximate; express the final value as an exact expression. Express $\\alpha$ as a pure number without units.",
            "solution": "The solution proceeds in three parts as requested: computing the eigenvalues of $F$, deriving the critical under-relaxation parameter $\\alpha$, and verifying the result.\n\n**Part 1: Compute the eigenvalues of $F$**\n\nThe eigenvalues $\\lambda$ of the matrix $F$ are the roots of the characteristic equation $\\det(F - \\lambda I) = 0$, where $I$ is the identity matrix.\n$$\n\\det\\left( \\begin{bmatrix} 1.3 - \\lambda  0.4 \\\\ 0.1  0.9 - \\lambda \\end{bmatrix} \\right) = 0\n$$\nExpanding the determinant gives the characteristic polynomial:\n$$\n(1.3 - \\lambda)(0.9 - \\lambda) - (0.4)(0.1) = 0\n$$\n$$\n1.17 - 1.3\\lambda - 0.9\\lambda + \\lambda^2 - 0.04 = 0\n$$\n$$\n\\lambda^2 - 2.2\\lambda + 1.13 = 0\n$$\nWe solve this quadratic equation for $\\lambda$ using the quadratic formula, $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$\n\\lambda = \\frac{-(-2.2) \\pm \\sqrt{(-2.2)^2 - 4(1)(1.13)}}{2(1)}\n$$\n$$\n\\lambda = \\frac{2.2 \\pm \\sqrt{4.84 - 4.52}}{2}\n$$\n$$\n\\lambda = \\frac{2.2 \\pm \\sqrt{0.32}}{2}\n$$\nThe term $\\sqrt{0.32}$ can be simplified: $\\sqrt{0.32} = \\sqrt{\\frac{32}{100}} = \\frac{\\sqrt{16 \\times 2}}{10} = \\frac{4\\sqrt{2}}{10} = 0.4\\sqrt{2}$.\nSubstituting this back into the equation for $\\lambda$:\n$$\n\\lambda = \\frac{2.2 \\pm 0.4\\sqrt{2}}{2} = 1.1 \\pm 0.2\\sqrt{2}\n$$\nThe two eigenvalues of $F$ are:\n$$\n\\lambda_{\\max}(F) = 1.1 + 0.2\\sqrt{2}\n$$\n$$\n\\lambda_{\\min}(F) = 1.1 - 0.2\\sqrt{2}\n$$\nSince $\\sqrt{2} \\approx 1.414$, we can see that $0.2\\sqrt{2} \\approx 0.2828$, which is less than $1.1$, so both eigenvalues are positive real numbers.\n\n**Part 2: Derive the largest admissible under-relaxation parameter $\\alpha$**\n\nThe amplification matrix is $G_{\\alpha} = I - \\alpha F$. If $\\lambda_i$ is an eigenvalue of $F$ with corresponding eigenvector $\\mathbf{v}_i$, then the corresponding eigenvalue $\\mu_i$ of $G_{\\alpha}$ is given by:\n$$\nG_{\\alpha}\\mathbf{v}_i = (I - \\alpha F)\\mathbf{v}_i = I\\mathbf{v}_i - \\alpha F\\mathbf{v}_i = \\mathbf{v}_i - \\alpha\\lambda_i\\mathbf{v}_i = (1 - \\alpha\\lambda_i)\\mathbf{v}_i\n$$\nSo, the eigenvalues of $G_{\\alpha}$ are $\\mu_i = 1 - \\alpha\\lambda_i$.\nThe iteration converges if and only if the spectral radius of $G_{\\alpha}$ is less than $1$:\n$$\n\\rho(G_{\\alpha}) = \\max_i |\\mu_i| = \\max_i |1 - \\alpha\\lambda_i|  1\n$$\nThis condition must hold for all eigenvalues $\\lambda_i$ of $F$. This is equivalent to the inequality:\n$$\n-1  1 - \\alpha\\lambda_i  1 \\quad \\text{for all } i\n$$\nWe analyze the two parts of this inequality. For an under-relaxation scheme, we consider $\\alpha  0$. Since we found that all eigenvalues $\\lambda_i$ of $F$ are positive, $\\alpha\\lambda_i  0$.\nThe right side of the inequality is $1 - \\alpha\\lambda_i  1$, which simplifies to $-\\alpha\\lambda_i  0$. This is always true for $\\alpha  0$ and $\\lambda_i  0$.\nThe left side of the inequality is $-1  1 - \\alpha\\lambda_i$, which simplifies to $\\alpha\\lambda_i  2$, or $\\alpha  \\frac{2}{\\lambda_i}$.\nThis condition must hold for all eigenvalues. Therefore, $\\alpha$ must be less than the minimum of all possible values of $\\frac{2}{\\lambda_i}$:\n$$\n\\alpha  \\min_i \\left(\\frac{2}{\\lambda_i}\\right) = \\frac{2}{\\max_i(\\lambda_i)} = \\frac{2}{\\lambda_{\\max}(F)}\n$$\nThe strict feasibility condition for convergence is $0  \\alpha  \\frac{2}{\\lambda_{\\max}(F)}$.\nThe largest admissible under-relaxation parameter $\\alpha$ that makes the system marginally stable corresponds to the boundary of this interval, where $\\rho(G_{\\alpha}) = 1$. This critical value, which we denote $\\alpha_{crit}$, is:\n$$\n\\alpha_{crit} = \\frac{2}{\\lambda_{\\max}(F)} = \\frac{2}{1.1 + 0.2\\sqrt{2}}\n$$\nTo express this as a single closed-form analytic expression, we can rationalize the denominator:\n$$\n\\alpha_{crit} = \\frac{20}{11 + 2\\sqrt{2}}\n$$\n$$\n\\alpha_{crit} = \\frac{20(11 - 2\\sqrt{2})}{(11 + 2\\sqrt{2})(11 - 2\\sqrt{2})} = \\frac{220 - 40\\sqrt{2}}{11^2 - (2\\sqrt{2})^2} = \\frac{220 - 40\\sqrt{2}}{121 - 8} = \\frac{220 - 40\\sqrt{2}}{113}\n$$\n\n**Part 3: Verify the threshold**\n\nAt the critical threshold $\\alpha = \\alpha_{crit} = \\frac{2}{\\lambda_{\\max}(F)}$, we evaluate the spectral radius $\\rho(G_{\\alpha_{crit}})$. The eigenvalues of $G_{\\alpha_{crit}}$ are $1 - \\alpha_{crit}\\lambda_i$.\n\nFor the eigenvalue $\\lambda_{\\max}(F)$:\n$$\n\\mu_{\\max} = 1 - \\alpha_{crit}\\lambda_{\\max}(F) = 1 - \\left(\\frac{2}{\\lambda_{\\max}(F)}\\right)\\lambda_{\\max}(F) = 1 - 2 = -1\n$$\n$$\n|\\mu_{\\max}| = |-1| = 1\n$$\nFor the eigenvalue $\\lambda_{\\min}(F)$:\n$$\n\\mu_{\\min} = 1 - \\alpha_{crit}\\lambda_{\\min}(F) = 1 - \\left(\\frac{2}{\\lambda_{\\max}(F)}\\right)\\lambda_{\\min}(F) = 1 - 2\\frac{\\lambda_{\\min}(F)}{\\lambda_{\\max}(F)}\n$$\nSince $0  \\lambda_{\\min}(F)  \\lambda_{\\max}(F)$, the ratio $\\frac{\\lambda_{\\min}(F)}{\\lambda_{\\max}(F)}$ is a positive number less than $1$. Therefore, $0  2\\frac{\\lambda_{\\min}(F)}{\\lambda_{\\max}(F)}  2$.\nThis implies $-1  1 - 2\\frac{\\lambda_{\\min}(F)}{\\lambda_{\\max}(F)}  1$, which means $|\\mu_{\\min}|  1$.\nThe spectral radius at the threshold is:\n$$\n\\rho(G_{\\alpha_{crit}}) = \\max(|\\mu_{\\max}|, |\\mu_{\\min}|) = \\max(1, |\\mu_{\\min}|) = 1\n$$\nThis confirms that $\\alpha_{crit} = \\frac{220 - 40\\sqrt{2}}{113}$ is indeed the largest admissible parameter for marginal stability. The strict feasibility condition for convergence is $0  \\alpha  \\frac{220 - 40\\sqrt{2}}{113}$. The question asks for the largest admissible $\\alpha$ for marginal stability, which is this upper bound.",
            "answer": "$$\\boxed{\\frac{220 - 40\\sqrt{2}}{113}}$$"
        }
    ]
}