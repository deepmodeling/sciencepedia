{
    "hands_on_practices": [
        {
            "introduction": "The simplest preconditioners are often based on scaling, and the Jacobi preconditioner, which uses the diagonal of the system matrix, is a prime example. This exercise explores the formal relationship between Jacobi preconditioning and a technique called \"diagonal scaling,\" where the system is transformed to have unit diagonal entries. By working through this problem , you will discover a fundamental equivalence between these two approaches, clarifying how Jacobi preconditioning works to balance the system and improve its conditioning.",
            "id": "3979816",
            "problem": "A one-dimensional slab of length $L$ and constant cross-sectional area $A_c$ conducts heat with piecewise-constant thermal conductivity. Under steady-state conditions with Dirichlet boundary temperatures prescribed at both ends, Fourier conduction and conservation of energy imply a linear system for the interior nodal temperatures with a symmetric positive definite (SPD) conduction matrix $A$. Using a conservative finite-volume discretization with three interior nodes, the interface conductances are defined by $\\alpha_i = \\frac{k_i A_c}{h_i}$ for $i=1,2,3,4$, where $k_i$ is the thermal conductivity and $h_i$ the segment length between consecutive nodal control-volume faces. The resulting SPD tridiagonal system has\n$$\nA = \\begin{pmatrix}\n\\alpha_1+\\alpha_2 & -\\alpha_2 & 0 \\\\\n-\\alpha_2 & \\alpha_2+\\alpha_3 & -\\alpha_3 \\\\\n0 & -\\alpha_3 & \\alpha_3+\\alpha_4\n\\end{pmatrix}.\n$$\nConsider a slab with $A_c = 1\\,\\mathrm{m^2}$, segment lengths $h_1 = 0.05\\,\\mathrm{m}$, $h_2 = 0.20\\,\\mathrm{m}$, $h_3 = 0.25\\,\\mathrm{m}$, $h_4 = 0.50\\,\\mathrm{m}$, and thermal conductivities $k_1 = 0.50\\,\\mathrm{W/(m\\cdot K)}$, $k_2 = 0.10\\,\\mathrm{W/(m\\cdot K)}$, $k_3 = 0.05\\,\\mathrm{W/(m\\cdot K)}$, $k_4 = 0.02\\,\\mathrm{W/(m\\cdot K)}$. \n\nIn Krylov subspace methods such as the Conjugate Gradient (CG) method for SPD systems, preconditioning is used to improve convergence. The Jacobi preconditioner is $M = \\mathrm{diag}(A)$, and the relevant measure of conditioning is the spectral condition number $\\kappa(M^{-1}A)$, defined as the ratio of the largest to the smallest eigenvalue of the symmetric matrix $M^{-1/2} A M^{-1/2}$.\n\nCompute a diagonal scaling matrix $D = \\mathrm{diag}(d_1,d_2,d_3)$ that balances the diagonal dominance in the sense that the scaled matrix $A' = D A D$ has unit diagonal entries. Then quantify the improvement in the spectral condition number for Jacobi preconditioning when applying the same Jacobi idea to the scaled system, that is, with $M' = \\mathrm{diag}(A')$ on $A'$. Report the improvement factor \n$$\nR = \\frac{\\kappa\\!\\left(M'^{-1} A'\\right)}{\\kappa\\!\\left(M^{-1} A\\right)}\n$$\nas a single real-valued number. No units are required. If you need to approximate any intermediate numerical values to evaluate $R$, you may do so; no specific rounding requirement is imposed on the final $R$ provided it is numerically consistent and unambiguous.",
            "solution": "The user wants me to solve the problem by following these steps:\n1. Validate the problem.\n2. If valid, solve it.\n\n### Step 1: Problem Validation\nI've analyzed the problem statement. All the given information, including the matrix `A`, physical constants, and definitions of preconditioning and scaling, is self-contained and scientifically sound. The problem is well-posed, objective, and falls within the domain of computational engineering and numerical linear algebra. There are no contradictions or ambiguities.\n\n**Verdict**: The problem is valid.\n\n### Step 2: Solution\nThe objective is to compute the improvement factor $R$, defined as the ratio of two spectral condition numbers:\n$$\nR = \\frac{\\kappa\\!\\left(M'^{-1} A'\\right)}{\\kappa\\!\\left(M^{-1} A\\right)}\n$$\nLet's analyze the denominator and the numerator separately.\n\nFirst, consider the denominator, which corresponds to the Jacobi preconditioning of the original system matrix $A$. The preconditioner is $M = \\mathrm{diag}(A)$. The problem correctly states that the relevant spectral condition number is that of the symmetrized matrix $B = M^{-1/2} A M^{-1/2}$. Let's denote the condition number for the original preconditioned system as $\\kappa_1$.\n$$\n\\kappa_1 = \\kappa(B) = \\kappa\\left(M^{-1/2} A M^{-1/2}\\right)\n$$\nwhere $M^{-1/2} = \\mathrm{diag}\\left(1/\\sqrt{A_{11}}, 1/\\sqrt{A_{22}}, 1/\\sqrt{A_{33}}\\right)$.\n\nNext, consider the numerator, which involves a scaled version of the matrix $A$. The scaling is defined by a diagonal matrix $D = \\mathrm{diag}(d_1, d_2, d_3)$ such that the scaled matrix $A' = DAD$ has unit diagonal entries. Let's determine the entries of $D$.\nThe diagonal entries of $A'$ are given by:\n$$\nA'_{ii} = (DAD)_{ii} = \\sum_{j,k} D_{ij} A_{jk} D_{ki}\n$$\nSince $D$ is a diagonal matrix, $D_{ij} = d_i \\delta_{ij}$. The expression simplifies to:\n$$\nA'_{ii} = d_i A_{ii} d_i = (d_i)^2 A_{ii}\n$$\nThe condition that $A'_{ii} = 1$ for all $i$ implies that:\n$$\n(d_i)^2 A_{ii} = 1 \\implies d_i = \\frac{1}{\\sqrt{A_{ii}}}\n$$\nThis holds for $i=1,2,3$. This means the scaling matrix $D$ is:\n$$\nD = \\mathrm{diag}\\left(\\frac{1}{\\sqrt{A_{11}}}, \\frac{1}{\\sqrt{A_{22}}}, \\frac{1}{\\sqrt{A_{33}}}\\right)\n$$\nBy comparing this to the matrix $M^{-1/2}$ derived earlier, where $M=\\mathrm{diag}(A)$, we see that they are identical:\n$$\nD = M^{-1/2}\n$$\nNow, we can write the scaled matrix $A'$ in terms of $A$ and $M$:\n$$\nA' = DAD = M^{-1/2} A M^{-1/2}\n$$\nThis reveals that the diagonally scaled matrix $A'$ is precisely the same as the symmetrized Jacobi-preconditioned matrix $B$ from the first step.\n$$\nA' = B\n$$\nNow, we analyze the Jacobi preconditioning applied to the scaled system $A'$. The new preconditioner is $M' = \\mathrm{diag}(A')$. By the very construction of $A'$, its diagonal entries are all unity:\n$$\nA'_{ii} = 1 \\quad \\text{for } i=1,2,3\n$$\nTherefore, the preconditioner $M'$ is the identity matrix:\n$$\nM' = \\mathrm{diag}(1, 1, 1) = I\n$$\nThe condition number for this new system, let's call it $\\kappa_2$, is the condition number of the corresponding symmetrized matrix $B' = (M')^{-1/2} A' (M')^{-1/2}$. Since $M' = I$, we have $(M')^{-1/2} = I$.\n$$\nB' = I A' I = A'\n$$\nSo, the condition number for the preconditioned scaled system is simply the condition number of $A'$ itself:\n$$\n\\kappa_2 = \\kappa(A')\n$$\nWe are now ready to compute the ratio $R$:\n$$\nR = \\frac{\\kappa_2}{\\kappa_1} = \\frac{\\kappa(A')}{\\kappa(B)}\n$$\nSince we have established that $A' = B$, it follows that their condition numbers are identical, $\\kappa(A') = \\kappa(B)$, provided the matrix is not singular (which is guaranteed since $A$ is SPD).\nTherefore, the ratio is:\n$$\nR = \\frac{\\kappa(A')}{\\kappa(A')} = 1\n$$\nThis result is general and does not require the numerical values of the conductivities or lengths, as long as the resulting matrix $A$ is symmetric positive definite. The process of diagonal scaling to obtain unit diagonals is equivalent to forming the matrix for the analysis of Jacobi preconditioning. Applying Jacobi preconditioning to this already-scaled matrix is a trivial operation ($M'=I$) that does not alter the matrix. Thus, the condition number remains unchanged.\n\nAlthough not required for the final answer, we can verify the matrix $A$ is SPD.\nThe interface conductances are:\n$\\alpha_1 = \\frac{k_1 A_c}{h_1} = \\frac{0.50 \\times 1}{0.05} = 10 \\text{ W/K}$\n$\\alpha_2 = \\frac{k_2 A_c}{h_2} = \\frac{0.10 \\times 1}{0.20} = 0.5 \\text{ W/K}$\n$\\alpha_3 = \\frac{k_3 A_c}{h_3} = \\frac{0.05 \\times 1}{0.25} = 0.2 \\text{ W/K}$\n$\\alpha_4 = \\frac{k_4 A_c}{h_4} = \\frac{0.02 \\times 1}{0.50} = 0.04 \\text{ W/K}$\nThe matrix $A$ is:\n$$\nA = \\begin{pmatrix}\n10 + 0.5 & -0.5 & 0 \\\\\n-0.5 & 0.5 + 0.2 & -0.2 \\\\\n0 & -0.2 & 0.2 + 0.04\n\\end{pmatrix} = \\begin{pmatrix}\n10.5 & -0.5 & 0 \\\\\n-0.5 & 0.7 & -0.2 \\\\\n0 & -0.2 & 0.24\n\\end{pmatrix}\n$$\n$A$ is symmetric, has positive diagonal elements, and is strictly diagonally dominant in rows $1$ and $3$, and weakly in row $2$. By the Gershgorin circle theorem, all its eigenvalues are positive, so it is positive definite. The symbolic derivation is thus valid for this specific case. The improvement factor $R$ is exactly $1$.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "While diagonal preconditioners are simple, more powerful methods like Incomplete LU (ILU) factorization often provide much faster convergence by approximating the true LU factors of a matrix while preserving sparsity. This practice focuses on the ILU($0$) variant, where no \"fill-in\" is allowed, strictly enforcing the original matrix's sparsity pattern on the factors. By manually constructing the ILU($0$) factors for a small but representative matrix from a heat transfer problem , you will gain a concrete understanding of the factorization process and quantify the error introduced by this \"incompleteness,\" demystifying what is often a black-box routine.",
            "id": "3979814",
            "problem": "A square plate of side length $L$ with constant thermal conductivity $k$ is held at a prescribed temperature on its boundary, so that the interior temperature field $T(x,y)$ satisfies steady conduction with no internal heat generation, governed by the conservation law $-\\nabla \\cdot (k \\nabla T) = 0$. Discretize the governing equation on a uniform Cartesian grid with spacing $h$ using the classical five-point finite difference stencil to obtain the linear system $A \\mathbf{u} = \\mathbf{b}$ for the unknown discrete interior temperatures $\\mathbf{u}$. Consider a minimal interior of $2 \\times 2$ unknowns (i.e., $4$ interior nodes) with lexicographic ordering by $x$ then $y$, and assume homogeneous Dirichlet data so that $\\mathbf{b} = \\mathbf{0}$. Up to an inconsequential uniform scaling by $k/h^{2}$, the coefficient matrix $A \\in \\mathbb{R}^{4 \\times 4}$ has the $5$-point diffusion stencil connectivity among interior nodes and is given by\n$$\nA \\;=\\; \\begin{pmatrix}\n4 & -1 & -1 & 0 \\\\\n-1 & 4 & 0 & -1 \\\\\n-1 & 0 & 4 & -1 \\\\\n0 & -1 & -1 & 4\n\\end{pmatrix}.\n$$\nConstruct the Incomplete Lower-Upper factorization with zero fill-in (ILU$(0)$) preconditioner $M = L U$, where $L$ is unit lower triangular and $U$ is upper triangular, under the constraint that the sparsity patterns of $L$ and $U$ match the strictly lower and the upper parts of $A$, respectively. Then, quantify how the sparsity constraint degrades the approximate inverse quality by computing the relative Frobenius factorization error\n$$\nr \\;=\\; \\frac{\\|A - L U\\|_{F}}{\\|A\\|_{F}}\\,.\n$$\nExpress your final answer as an exact value. Do not round. The answer must be a single real number without units.",
            "solution": "We begin from the steady energy conservation law for conduction with constant thermal conductivity, $-\\nabla \\cdot (k \\nabla T) = 0$. On a uniform Cartesian grid with spacing $h$, the standard five-point finite difference discretization at an interior node $(i,j)$ yields\n$$\n\\frac{k}{h^{2}}\\left(-T_{i-1,j} - T_{i+1,j} - T_{i,j-1} - T_{i,j+1} + 4 T_{i,j}\\right) \\;=\\; 0,\n$$\nwhich leads to a linear system $A \\mathbf{u} = \\mathbf{b}$, where, after absorbing the common factor $k/h^{2}$ into a uniform scaling of $A$ and $\\mathbf{b}$, the interior coupling among unknowns is captured by the $5$-point stencil. For a $2 \\times 2$ interior (thus $4$ unknowns) with lexicographic ordering by $x$ then $y$, the matrix $A \\in \\mathbb{R}^{4 \\times 4}$ is\n$$\nA \\;=\\; \\begin{pmatrix}\n4 & -1 & -1 & 0 \\\\\n-1 & 4 & 0 & -1 \\\\\n-1 & 0 & 4 & -1 \\\\\n0 & -1 & -1 & 4\n\\end{pmatrix}.\n$$\nWe construct the Incomplete Lower-Upper with zero fill-in (ILU$(0)$) preconditioner $M = L U$ with the constraints:\n- $L$ is unit lower triangular and has nonzeros only where $A$ has nonzeros strictly below the diagonal, i.e., in positions $(2,1)$, $(3,1)$, $(4,2)$, $(4,3)$.\n- $U$ is upper triangular and has nonzeros only where $A$ has nonzeros on or above the diagonal, i.e., in positions $(1,1)$, $(1,2)$, $(1,3)$, $(2,2)$, $(2,4)$, $(3,3)$, $(3,4)$, $(4,4)$.\n\nWe use a Doolittle-type ILU$(0)$ process, which applies Gaussian elimination but drops all fill-in outside the prescribed sparsity of $L$ and $U$.\n\nParameterize\n$$\nL \\;=\\; \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n\\ell_{21} & 1 & 0 & 0 \\\\\n\\ell_{31} & 0 & 1 & 0 \\\\\n0 & \\ell_{42} & \\ell_{43} & 1\n\\end{pmatrix}, \n\\quad\nU \\;=\\; \\begin{pmatrix}\nu_{11} & u_{12} & u_{13} & 0 \\\\\n0 & u_{22} & 0 & u_{24} \\\\\n0 & 0 & u_{33} & u_{34} \\\\\n0 & 0 & 0 & u_{44}\n\\end{pmatrix}.\n$$\nAt step $i = 1$, the upper row is\n$$\nu_{11} \\;=\\; A_{11} \\;=\\; 4,\\quad u_{12} \\;=\\; A_{12} \\;=\\; -1,\\quad u_{13} \\;=\\; A_{13} \\;=\\; -1.\n$$\nThe multipliers in column $1$ are\n$$\n\\ell_{21} \\;=\\; \\frac{A_{21}}{u_{11}} \\;=\\; \\frac{-1}{4} \\;=\\; -\\frac{1}{4}, \\qquad\n\\ell_{31} \\;=\\; \\frac{A_{31}}{u_{11}} \\;=\\; \\frac{-1}{4} \\;=\\; -\\frac{1}{4}.\n$$\nAt step $i = 2$, for the upper entries in the pattern we compute\n$$\nu_{22} \\;=\\; A_{22} - \\ell_{21} u_{12} \\;=\\; 4 - \\left(-\\frac{1}{4}\\right)(-1) \\;=\\; 4 - \\frac{1}{4} \\;=\\; \\frac{15}{4},\n$$\n$$\nu_{24} \\;=\\; A_{24} - \\ell_{21} u_{14} \\;=\\; -1 - \\left(-\\frac{1}{4}\\right)\\cdot 0 \\;=\\; -1.\n$$\nThe only allowed lower entry beneath column $2$ is $\\ell_{42}$, given by\n$$\n\\ell_{42} \\;=\\; \\frac{A_{42} - \\ell_{41} u_{12}}{u_{22}} \\;=\\; \\frac{-1 - 0 \\cdot (-1)}{15/4} \\;=\\; -\\frac{4}{15},\n$$\nwhere $\\ell_{41} = 0$ is enforced by the ILU$(0)$ sparsity constraint at position $(4,1)$.\n\nAt step $i = 3$, for the allowed upper entries we have\n$$\nu_{33} \\;=\\; A_{33} - \\left(\\ell_{31} u_{13} + \\ell_{32} u_{23}\\right) \\;=\\; 4 - \\left(\\left(-\\frac{1}{4}\\right)(-1) + 0 \\cdot 0\\right) \\;=\\; 4 - \\frac{1}{4} \\;=\\; \\frac{15}{4},\n$$\n$$\nu_{34} \\;=\\; A_{34} - \\left(\\ell_{31} u_{14} + \\ell_{32} u_{24}\\right) \\;=\\; -1 - \\left(\\left(-\\frac{1}{4}\\right)\\cdot 0 + 0 \\cdot (-1)\\right) \\;=\\; -1,\n$$\nwhere $\\ell_{32} = 0$ and $u_{23} = 0$ follow from ILU$(0)$ sparsity. The only allowed lower entry beneath column $3$ is\n$$\n\\ell_{43} \\;=\\; \\frac{A_{43} - \\left(\\ell_{41} u_{13} + \\ell_{42} u_{23}\\right)}{u_{33}} \\;=\\; \\frac{-1 - \\left(0 \\cdot (-1) + \\left(-\\frac{4}{15}\\right)\\cdot 0\\right)}{15/4} \\;=\\; -\\frac{4}{15}.\n$$\nAt step $i = 4$, the final diagonal entry is\n$$\nu_{44} \\;=\\; A_{44} - \\left(\\ell_{41} u_{14} + \\ell_{42} u_{24} + \\ell_{43} u_{34}\\right) \\;=\\; 4 - \\left(0 \\cdot 0 + \\left(-\\frac{4}{15}\\right)(-1) + \\left(-\\frac{4}{15}\\right)(-1)\\right) \\;=\\; 4 - \\frac{8}{15} \\;=\\; \\frac{52}{15}.\n$$\nThus the ILU$(0)$ factors are\n$$\nL \\;=\\; \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n-\\tfrac{1}{4} & 1 & 0 & 0 \\\\\n-\\tfrac{1}{4} & 0 & 1 & 0 \\\\\n0 & -\\tfrac{4}{15} & -\\tfrac{4}{15} & 1\n\\end{pmatrix}, \n\\quad\nU \\;=\\; \\begin{pmatrix}\n4 & -1 & -1 & 0 \\\\\n0 & \\tfrac{15}{4} & 0 & -1 \\\\\n0 & 0 & \\tfrac{15}{4} & -1 \\\\\n0 & 0 & 0 & \\tfrac{52}{15}\n\\end{pmatrix}.\n$$\nTo show how the sparsity constraint affects the approximation quality, we examine the factorization residual $E = A - L U$. By construction of ILU$(0)$, $L U$ matches $A$ on the nonzero pattern of $A$, and any discrepancy can only appear at positions that would be fill-in under exact Gaussian elimination but are forbidden here. The only such positions are $(2,3)$ and $(3,2)$. We compute\n$$\n(LU)_{23} \\;=\\; L_{21} U_{13} + L_{22} U_{23} \\;=\\; \\left(-\\frac{1}{4}\\right)\\cdot (-1) + 1 \\cdot 0 \\;=\\; \\frac{1}{4},\n$$\nso\n$$\nE_{23} \\;=\\; A_{23} - (LU)_{23} \\;=\\; 0 - \\frac{1}{4} \\;=\\; -\\frac{1}{4}.\n$$\nBy symmetry of the construction,\n$$\n(LU)_{32} \\;=\\; L_{31} U_{12} + L_{33} U_{32} \\;=\\; \\left(-\\frac{1}{4}\\right)\\cdot (-1) + 1 \\cdot 0 \\;=\\; \\frac{1}{4},\n$$\nso\n$$\nE_{32} \\;=\\; A_{32} - (LU)_{32} \\;=\\; 0 - \\frac{1}{4} \\;=\\; -\\frac{1}{4}.\n$$\nAll other entries of $E$ are zero. Therefore,\n$$\n\\|E\\|_{F}^{2} \\;=\\; \\left(-\\frac{1}{4}\\right)^{2} + \\left(-\\frac{1}{4}\\right)^{2} \\;=\\; \\frac{1}{16} + \\frac{1}{16} \\;=\\; \\frac{1}{8},\n$$\nso\n$$\n\\|E\\|_{F} \\;=\\; \\sqrt{\\frac{1}{8}} \\;=\\; \\frac{1}{2 \\sqrt{2}}.\n$$\nNext, compute the Frobenius norm of $A$:\n$$\n\\|A\\|_{F}^{2} \\;=\\; 4 \\cdot (4^2) + 8 \\cdot (-1)^2 = 4 \\cdot 16 + 8 \\cdot 1 = 64 + 8 = 72,\n$$\nso\n$$\n\\|A\\|_{F} \\;=\\; \\sqrt{72} \\;=\\; 6 \\sqrt{2}.\n$$\nTherefore, the relative Frobenius factorization error is\n$$\nr \\;=\\; \\frac{\\|A - L U\\|_{F}}{\\|A\\|_{F}} \\;=\\; \\frac{1/(2 \\sqrt{2})}{6 \\sqrt{2}} \\;=\\; \\frac{1}{24}.\n$$\nThis exact value isolates the effect of the ILU$(0)$ sparsity constraint: the entire error comes from the two dropped fill-in positions $(2,3)$ and $(3,2)$ associated with the $5$-point stencil’s natural fill pattern under this ordering. In the context of Krylov subspace methods such as the Generalized Minimal Residual (GMRES) method, smaller $r$ generally correlates with a preconditioned operator $M^{-1} A$ that is closer to the identity, which accelerates convergence; here, $r = 1/24$ quantifies the degradation due to enforcing zero fill-in at the two coupling locations.",
            "answer": "$$\\boxed{\\frac{1}{24}}$$"
        },
        {
            "introduction": "The true power of preconditioning is realized when it is combined with a robust Krylov subspace method like the Conjugate Gradient (CG) algorithm, forming the Preconditioned Conjugate Gradient (PCG) method—a cornerstone of modern scientific computing. This exercise brings together discretization, preconditioning, and Krylov iteration into a single computational task where you will implement the PCG algorithm and test it with different preconditioners. By implementing and testing the algorithm , you will empirically verify the central theorem of preconditioning: convergence speed depends on the condition number of the preconditioned system, $\\kappa(M^{-1}A)$, not the original system, $\\kappa(A)$, bridging the gap between theory and practical performance.",
            "id": "3979786",
            "problem": "Consider the steady-state heat conduction equation in a rectangular domain with homogeneous Dirichlet boundary conditions, where the thermal conductivity is constant but anisotropic. The governing partial differential equation is $$-\\nabla \\cdot \\left( \\mathbf{K} \\nabla T \\right) = q,$$ where $T$ is temperature, $\\mathbf{K} = \\operatorname{diag}(k_x,k_y)$ with $k_x>0$ and $k_y>0$, and $q$ is a volumetric heat source. Discretize this equation on a uniform Cartesian grid using a standard centered finite-difference scheme to obtain a linear system $$A x = b,$$ where $A \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite (SPD), $x \\in \\mathbb{R}^n$ is the vector of nodal temperatures, and $b \\in \\mathbb{R}^n$ is the discrete source term. The discrete operator $A$ is a $5$-point stencil with off-diagonal weights proportional to $-k_x/h_x^2$ and $-k_y/h_y^2$, and diagonal weights proportional to $2(k_x/h_x^2 + k_y/h_y^2)$, where $h_x$ and $h_y$ are the mesh spacings.\n\nYou are to implement the Preconditioned Conjugate Gradient (PCG) algorithm for solving the linear system, using a general SPD preconditioner $M$. The preconditioning is to be applied in the standard manner for SPD problems, and the convergence is to be assessed using the Euclidean norm of the residual, i.e., stopping when $$\\frac{\\lVert r_k \\rVert_2}{\\lVert r_0 \\rVert_2} \\leq \\varepsilon,$$ where $r_k = b - A x_k$ and $\\varepsilon$ is a prescribed tolerance.\n\nStarting from first principles:\n- Derive how the SPD discretization arises from the heat equation and why $A$ is SPD for constant $k_x$ and $k_y$ with homogeneous Dirichlet boundary conditions.\n- Explain why PCG convergence for SPD $A$ depends on the spectrum of the symmetrically preconditioned operator $$S = M^{-1/2} A M^{-1/2},$$ which has the same eigenvalues as $M^{-1}A$, and not on the spectrum of $A$ alone.\n- Implement the PCG algorithm for a general SPD $M$ by using the action of $M^{-1}$ on a vector (i.e., applying the preconditioner through solves with $M$).\n- For each test case below, compute:\n  1. The number of iterations $k_{\\text{obs}}$ taken by PCG to reach the tolerance $\\varepsilon$ (with a zero initial guess).\n  2. The spectral condition number $$\\kappa(M^{-1}A) = \\frac{\\lambda_{\\max}(S)}{\\lambda_{\\min}(S)},$$ where $S = M^{-1/2} A M^{-1/2}$ and $\\lambda_{\\max}$, $\\lambda_{\\min}$ denote the largest and smallest eigenvalues respectively.\n  3. The worst-case upper bound $k_{\\text{bound}}$ on iterations needed to reach the tolerance $\\varepsilon$, given by the classical Conjugate Gradient estimate in terms of $\\kappa(M^{-1}A)$, namely choosing the smallest integer $k$ satisfying $$2 \\left( \\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1} \\right)^k \\leq \\varepsilon.$$ For the special case $\\kappa=1$ (i.e., perfect preconditioning), set $k_{\\text{bound}} = 1$.\n\nUse the following test suite, which explores a general case, an anisotropic case with a simple diagonal preconditioner, and a limiting case with exact preconditioning:\n- Case $1$: $n_x = n_y = 16$, $k_x = 1$, $k_y = 1$, $M = I$ (identity preconditioner), tolerance $\\varepsilon = 10^{-8}$, maximum iterations $n$ (the dimension of $A$). This is the general \"happy path\" case.\n- Case $2$: $n_x = n_y = 16$, $k_x = 100$, $k_y = 1$, $M = \\operatorname{diag}(A)$ (Jacobi preconditioner), tolerance $\\varepsilon = 10^{-8}$, maximum iterations $n$. This case tests anisotropy and a simple SPD preconditioner.\n- Case $3$: $n_x = n_y = 16$, $k_x = 100$, $k_y = 1$, $M = A$ (exact SPD preconditioner implemented by Cholesky solves), tolerance $\\varepsilon = 10^{-8}$, maximum iterations $n$. This case is a boundary condition demonstrating ideal preconditioning.\n\nIn all cases, set $b$ to the vector with all entries equal to $1$ and the initial guess $x_0$ to the zero vector. The mesh spacings should be $h_x = h_y = 1/(n_x+1)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with entries ordered by case and, for each case, listing $k_{\\text{obs}}$, $\\kappa(M^{-1}A)$, and $k_{\\text{bound}}$. Specifically, the output format must be\n$$[k_{\\text{obs},1},\\kappa_1,k_{\\text{bound},1},k_{\\text{obs},2},\\kappa_2,k_{\\text{bound},2},k_{\\text{obs},3},\\kappa_3,k_{\\text{bound},3}],$$\nwhere all entries are real numbers or integers and there is no additional output text. No physical units are required since the quantities are dimensionless iteration counts and spectral condition numbers.",
            "solution": "The provided problem is a well-defined exercise in computational science, requiring the application of the Preconditioned Conjugate Gradient (PCG) method to a discretized partial differential equation. It is scientifically grounded, formally specified, and internally consistent. We shall first derive the discrete system and its properties, explain the principles of PCG convergence, and then proceed to the numerical implementation and solution for the specified test cases.\n\n### 1. Discretization and Properties of the System Matrix\n\nThe governing equation is the steady-state heat conduction equation with an anisotropic thermal conductivity tensor $\\mathbf{K}$:\n$$ -\\nabla \\cdot \\left( \\mathbf{K} \\nabla T \\right) = q $$\nGiven $\\mathbf{K} = \\operatorname{diag}(k_x, k_y)$, the equation expands to:\n$$ -\\frac{\\partial}{\\partial x}\\left(k_x \\frac{\\partial T}{\\partial x}\\right) - \\frac{\\partial}{\\partial y}\\left(k_y \\frac{\\partial T}{\\partial y}\\right) = q $$\nWe discretize this equation on a uniform Cartesian grid with spacings $h_x$ and $h_y$. Let $T_{i,j}$ denote the temperature at the grid node $(x_i, y_j)$. We use a centered finite-difference scheme to approximate the derivatives at an interior node $(i,j)$.\n\nThe outer derivative $\\frac{\\partial}{\\partial x}(\\cdot)$ is approximated as:\n$$ \\left. \\frac{\\partial F}{\\partial x} \\right|_{i,j} \\approx \\frac{F_{i+1/2,j} - F_{i-1/2,j}}{h_x} $$\nwhere $F = k_x \\frac{\\partial T}{\\partial x}$. The terms at the half-grid points are then approximated as:\n$$ F_{i+1/2,j} = \\left. k_x \\frac{\\partial T}{\\partial x} \\right|_{i+1/2,j} \\approx k_x \\frac{T_{i+1,j} - T_{i,j}}{h_x} $$\n$$ F_{i-1/2,j} = \\left. k_x \\frac{\\partial T}{\\partial x} \\right|_{i-1/2,j} \\approx k_x \\frac{T_{i,j} - T_{i-1,j}}{h_x} $$\nSubstituting these back, the $x$-component of the divergence is:\n$$ -\\frac{\\partial}{\\partial x}\\left(k_x \\frac{\\partial T}{\\partial x}\\right)_{i,j} \\approx -\\frac{k_x}{h_x^2} \\left( (T_{i+1,j} - T_{i,j}) - (T_{i,j} - T_{i-1,j}) \\right) = \\frac{k_x}{h_x^2} (-T_{i+1,j} + 2T_{i,j} - T_{i-1,j}) $$\nAn analogous derivation for the $y$-component gives:\n$$ -\\frac{\\partial}{\\partial y}\\left(k_y \\frac{\\partial T}{\\partial y}\\right)_{i,j} \\approx \\frac{k_y}{h_y^2} (-T_{i,j+1} + 2T_{i,j} - T_{i,j-1}) $$\nCombining these terms and setting them equal to the source term $q_{i,j}$ at the node yields the discrete equation:\n$$ \\frac{k_x}{h_x^2} (2T_{i,j} - T_{i+1,j} - T_{i-1,j}) + \\frac{k_y}{h_y^2} (2T_{i,j} - T_{i,j+1} - T_{i,j-1}) = q_{i,j} $$\nRearranging the terms for $T_{i,j}$ gives the $5$-point stencil form:\n$$ \\left(\\frac{2k_x}{h_x^2} + \\frac{2k_y}{h_y^2}\\right)T_{i,j} - \\frac{k_x}{h_x^2}T_{i+1,j} - \\frac{k_x}{h_x^2}T_{i-1,j} - \\frac{k_y}{h_y^2}T_{i,j+1} - \\frac{k_y}{h_y^2}T_{i,j-1} = q_{i,j} $$\nHomogeneous Dirichlet boundary conditions ($T=0$ on the boundary) mean that for any node $(i,j)$ adjacent to a boundary, the corresponding neighbor term (e.g., $T_{i-1,j}$ if the node is on the left edge) is zero and is thus omitted from the equation.\n\nBy ordering the $n = n_x \\times n_y$ unknown interior temperatures into a vector $x$, these equations form a linear system $Ax=b$.\n\n**Symmetry and Positive Definiteness of $A$**:\n1.  **Symmetry**: The matrix $A$ is symmetric. The coefficient for the influence of node $l$ on the equation for node $k$ is identical to the coefficient for the influence of node $k$ on node $l$. For instance, for adjacent nodes $(i,j)$ and $(i+1,j)$, the entry in the row for $(i,j)$ corresponding to the variable for $(i+1,j)$ is $-k_x/h_x^2$. Likewise, the entry in the row for $(i+1,j)$ corresponding to the variable for $(i,j)$ is also $-k_x/h_x^2$. This holds for all neighboring pairs, thus $A=A^T$.\n\n2.  **Positive Definiteness**: Since $k_x > 0$ and $k_y > 0$, the diagonal entries of $A$, given by $A_{kk} = 2(k_x/h_x^2 + k_y/h_y^2)$, are strictly positive. The off-diagonal entries are all non-positive. The sum of the absolute values of the off-diagonal entries in any row $k$ is $\\sum_{j \\neq k} |A_{kj}|$. For a node not adjacent to any boundary, this sum equals the diagonal entry $A_{kk}$. For a node adjacent to a boundary, one or more neighbor terms are absent, so the sum is strictly less than the diagonal entry. Thus, $A$ is diagonally dominant. Furthermore, the matrix represents a connected graph of nodes, which makes it irreducible. An irreducibly diagonally dominant symmetric matrix with positive diagonal entries is strictly diagonally dominant for at least one row, and it is known to be positive definite. Therefore, $A$ is symmetric positive definite (SPD).\n\n### 2. PCG Convergence and the Role of Preconditioning\n\nThe Conjugate Gradient (CG) method is an iterative algorithm for solving SPD linear systems $Ax=b$. Its convergence rate is governed by the spectral condition number of the matrix $A$, $\\kappa(A) = \\lambda_{\\max}(A)/\\lambda_{\\min}(A)$. The number of iterations $k$ required to reduce the error by a factor $\\varepsilon$ is roughly proportional to $\\sqrt{\\kappa(A)}$.\n\nPreconditioning aims to improve this convergence rate by transforming the original system into an equivalent one with a more favorable spectral distribution (i.e., a smaller condition number). For an SPD preconditioner $M$, we can define its Cholesky factorization $M=LL^T$. The system $Ax=b$ can be rewritten as:\n$$ L^{-1} A x = L^{-1} b \\implies (L^{-1} A (L^T)^{-1}) (L^T x) = L^{-1} b $$\nLet $S = L^{-1} A (L^T)^{-1}$, $\\hat{x} = L^T x$, and $\\hat{b} = L^{-1}b$. We now solve the transformed system $S \\hat{x} = \\hat{b}$ using the standard CG method. The matrix $S$ is SPD, so CG is applicable. The convergence of this process is governed by $\\kappa(S)$.\n\nThe eigenvalues of $S$ are the same as those of the matrix $M^{-1}A$. This can be seen via a similarity transform:\n$$ S = L^{-1} A L^{-T} \\implies L^T S L^{-T} = L^T(L^{-1} A L^{-T})L^{-T} = A (L^T L)^{-1} = A M^{-1} $$\nSince $AM^{-1}$ is similar to the symmetric matrix $M^{1/2}(AM^{-1})M^{-1/2} = M^{-1/2}AM^{-1/2}=S$, its eigenvalues are real and positive. Also, the eigenvalues of $AM^{-1}$ are the same as those of $M^{-1}A$. (For any invertible $P$, $PXP^{-1}$ has the same eigenvalues as $X$. Let $X=AM^{-1}$ and $P=M^{-1}$, then $M^{-1}(AM^{-1})M = M^{-1}A$, but this is more complex). A simpler argument is that $\\det(AB - \\lambda I) = \\det(A(B- \\lambda A^{-1})I) = \\det(A)\\det(B-\\lambda A^{-1})$ and $\\det(BA - \\lambda I) = \\det((B - \\lambda A^{-1})A) = \\det(B-\\lambda A^{-1})\\det(A)$, showing $AB$ and $BA$ have the same characteristic polynomial and thus the same eigenvalues.\n\nTherefore, the convergence of PCG depends on $\\kappa(S) = \\kappa(M^{-1}A)$. An effective preconditioner $M$ is one that approximates $A$, such that $M^{-1}A$ is close to the identity matrix $I$, making $\\kappa(M^{-1}A) \\approx 1$.\n\nThe PCG algorithm is formulated to avoid the explicit construction of $L$, $L^{-1}$, or $S$. It operates on the original vectors and matrices but includes a step to solve a system with the preconditioner, $z_k = M^{-1}r_k$, in each iteration $k$.\n\n### 3. Numerical Solution and Analysis\n\nThe implementation will consist of three main parts for each test case:\n1.  **Assembly**: Construct the matrix $A$ and vector $b$.\n2.  **PCG Execution**: Run the PCG algorithm with the specified preconditioner $M$ to find the number of iterations, $k_{\\text{obs}}$.\n3.  **Spectral Analysis**: Compute the eigenvalues of $M^{-1}A$ to determine the condition number $\\kappa(M^{-1}A)$ and the theoretical iteration bound $k_{\\text{bound}}$.\n\nThe Preconditioned Conjugate Gradient algorithm proceeds as follows ($x_0$ is the initial guess):\n$r_0 = b - Ax_0$\n$z_0 = M^{-1} r_0$\n$p_0 = z_0$\nFor $k = 0, 1, 2, \\dots$\n$\\quad \\alpha_k = (r_k^T z_k) / (p_k^T A p_k)$\n$\\quad x_{k+1} = x_k + \\alpha_k p_k$\n$\\quad r_{k+1} = r_k - \\alpha_k A p_k$\n$\\quad$ If converged: stop\n$\\quad z_{k+1} = M^{-1} r_{k+1}$\n$\\quad \\beta_k = (r_{k+1}^T z_{k+1}) / (r_k^T z_k)$\n$\\quad p_{k+1} = z_{k+1} + \\beta_k p_k$\n\nThe condition number $\\kappa(M^{-1}A)$ will be computed by finding the eigenvalues of the generalized eigenvalue problem $Av = \\lambda Mv$, which can be solved efficiently using `scipy.linalg.eigh`. The theoretical bound $k_{\\text{bound}}$ is the smallest integer $k$ satisfying $2 ((\\sqrt{\\kappa}-1)/(\\sqrt{\\kappa}+1))^k \\leq \\varepsilon$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh, cholesky, solve_triangular\n\ndef solve():\n    \"\"\"\n    Main function to run test cases for the PCG algorithm on a discretized\n    heat equation problem.\n    \"\"\"\n\n    def build_A(nx, ny, kx, ky):\n        \"\"\"\n        Builds the sparse matrix A for the 2D heat equation using a 5-point\n        finite-difference stencil.\n        \"\"\"\n        n = nx * ny\n        hx = 1.0 / (nx + 1)\n        hy = 1.0 / (ny + 1)\n        A = np.zeros((n, n))\n\n        # Terms for the stencil\n        term_x = kx / (hx**2)\n        term_y = ky / (hy**2)\n        diag_val = 2.0 * (term_x + term_y)\n\n        for j in range(ny):\n            for i in range(nx):\n                k = j * nx + i  # Row-major mapping from 2D (i,j) to 1D k\n                \n                # Diagonal\n                A[k, k] = diag_val\n                \n                # Off-diagonals\n                # Left neighbor\n                if i > 0:\n                    k_left = j * nx + (i - 1)\n                    A[k, k_left] = -term_x\n                # Right neighbor\n                if i < nx - 1:\n                    k_right = j * nx + (i + 1)\n                    A[k, k_right] = -term_x\n                # Bottom neighbor\n                if j > 0:\n                    k_down = (j - 1) * nx + i\n                    A[k, k_down] = -term_y\n                # Top neighbor\n                if j < ny - 1:\n                    k_up = (j + 1) * nx + i\n                    A[k, k_up] = -term_y\n                    \n        return A\n\n    def pcg_solver(A, b, x0, M_inv, epsilon, max_iter):\n        \"\"\"\n        Implements the Preconditioned Conjugate Gradient (PCG) method.\n        M_inv is a function that computes z = M^{-1}r.\n        \"\"\"\n        x = x0.copy()\n        r = b - A @ x\n        norm_r0 = np.linalg.norm(r)\n\n        if norm_r0 < 1e-12: # If initial guess is the solution\n            return x, 0\n\n        z = M_inv(r)\n        p = z\n        rs_old = np.dot(r, z)\n\n        for k in range(max_iter):\n            Ap = A @ p\n            alpha = rs_old / np.dot(p, Ap)\n            \n            x = x + alpha * p\n            r = r - alpha * Ap\n\n            if np.linalg.norm(r) / norm_r0 <= epsilon:\n                return x, k + 1\n\n            z = M_inv(r)\n            rs_new = np.dot(r, z)\n            beta = rs_new / rs_old\n            p = z + beta * p\n            rs_old = rs_new\n            \n        return x, max_iter\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (nx, ny, kx, ky, M_type, epsilon)\n        (16, 16, 1.0, 1.0, 'identity'),\n        (16, 16, 100.0, 1.0, 'jacobi'),\n        (16, 16, 100.0, 1.0, 'exact')\n    ]\n    \n    epsilon = 1e-8\n    results = []\n\n    for nx, ny, kx, ky, M_type in test_cases:\n        n = nx * ny\n        max_iter = n\n\n        # Assemble the matrix and vectors\n        A = build_A(nx, ny, kx, ky)\n        b = np.ones(n)\n        x0 = np.zeros(n)\n\n        # Define preconditioner M and its inverse action M_inv\n        if M_type == 'identity':\n            M = np.eye(n)\n            M_inv = lambda v: v\n        elif M_type == 'jacobi':\n            diag_A = np.diag(A)\n            M = np.diag(diag_A)\n            M_inv = lambda v: v / diag_A\n        elif M_type == 'exact':\n            M = A\n            # Pre-factor for efficient solves in PCG\n            L = cholesky(A, lower=True)\n            M_inv = lambda v:solve_triangular(L.T, solve_triangular(L, v, lower=True), lower=False)\n\n        # 1. Compute observed iterations k_obs\n        _, k_obs = pcg_solver(A, b, x0, M_inv, epsilon, max_iter)\n        \n        # 2. Compute spectral condition number kappa\n        # Use generalized eigenvalue solver for eigs of M^{-1}A\n        eigenvalues = eigh(A, M, eigvals_only=True)\n        lambda_min = np.min(eigenvalues)\n        lambda_max = np.max(eigenvalues)\n        \n        # Guard against division by zero and handle ideal case\n        if np.isclose(lambda_min, 0.0):\n            kappa = np.inf\n        elif np.isclose(lambda_min, lambda_max):\n            kappa = 1.0\n        else:\n            kappa = lambda_max / lambda_min\n\n        # 3. Compute worst-case iteration bound k_bound\n        if np.isclose(kappa, 1.0):\n            k_bound = 1\n        else:\n            # Formula: k >= log(eps/2) / log((sqrt(kappa)-1)/(sqrt(kappa)+1))\n            sqrt_kappa = np.sqrt(kappa)\n            term = (sqrt_kappa - 1) / (sqrt_kappa + 1)\n            k_bound = np.ceil(np.log(epsilon / 2.0) / np.log(term))\n\n        results.extend([k_obs, kappa, int(k_bound)])\n\n    # Format output according to problem specification\n    # k_obs and k_bound are integers, kappa is a float\n    output_str = ','.join(map(str, results))\n    print(f\"[{output_str}]\")\n\nsolve()\n```"
        }
    ]
}