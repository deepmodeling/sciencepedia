{
    "hands_on_practices": [
        {
            "introduction": "Understanding iterative solvers begins with seeing how they emerge directly from the discretized physical laws we aim to solve. This exercise grounds the abstract matrix splitting, $A = M - N$, in the concrete context of a 2D heat conduction problem. By deriving the iterative formulas for the Jacobi, Gauss-Seidel, and Successive Over-Relaxation (SOR) methods from the five-point finite difference stencil, you will gain a firsthand appreciation for the structure of these fundamental algorithms and their connection to the underlying stencil computations. This practice  further bridges the gap between implementation and theory by guiding you through an analysis of the spectral radius, a key determinant of an iterative method's convergence speed.",
            "id": "3965780",
            "problem": "Consider steady two-dimensional conduction in a homogeneous solid with temperature field $T(x,y)$ governed by the Poisson form of energy conservation, $-\\nabla^{2} T = f(x,y)$, on the unit square $\\Omega = (0,1) \\times (0,1)$ with homogeneous Dirichlet boundary conditions $T = 0$ on $\\partial \\Omega$. Discretize $\\Omega$ by a uniform Cartesian grid with $N \\times N$ interior points, grid spacing $h = \\frac{1}{N+1}$ in both directions, and approximate $-\\nabla^{2} T$ at interior nodes $(i,j)$ by the standard $5$-point central difference. Assemble the resulting linear system in the natural lexicographic ordering (first index $i$ fastest), and define the matrix splitting $A = D - L - U$, where $D$ is the diagonal part and $L$ and $U$ are the strictly lower and strictly upper parts, respectively.\n\nStarting from the discrete conservation law and the definition of the splitting, perform the following:\n\n(a) Construct $D$, $L$, and $U$ for the assembled matrix $A$ and write the corresponding node-wise iterative update formulas for the Jacobi method, the Gauss–Seidel method, and the Successive Over-Relaxation (SOR) method with relaxation parameter $\\omega \\in (0,2)$.\n\n(b) For the homogeneous case $f \\equiv 0$, analyze the error-propagation of the Jacobi iteration by expanding the error in the discrete sine basis that respects the Dirichlet boundaries. Determine the spectral radius $\\rho_{J}(N)$ of the Jacobi iteration matrix as an explicit function of $N$.\n\n(c) Using well-tested facts about consistently ordered splittings for the discrete Poisson operator on a uniform grid, derive the relaxation parameter $\\omega_{\\mathrm{opt}}(N)$ that minimizes the spectral radius of the SOR iteration matrix in terms of $\\rho_{J}(N)$. Express $\\omega_{\\mathrm{opt}}(N)$ in closed form.\n\nYour final answer must be the single analytic expression for $\\omega_{\\mathrm{opt}}(N)$, with no numerical substitution and no rounding. No units are required.",
            "solution": "The governing equation $-\\nabla^{2} T = f$ with homogeneous Dirichlet boundary conditions $T=0$ on $\\partial \\Omega$ is discretized on a uniform grid with spacing $h = \\frac{1}{N+1}$ in both directions. The standard $5$-point central difference at an interior node $(i,j)$ gives\n$$\n-\\frac{T_{i+1,j} - 2 T_{i,j} + T_{i-1,j}}{h^{2}} - \\frac{T_{i,j+1} - 2 T_{i,j} + T_{i,j-1}}{h^{2}} = f_{i,j}.\n$$\nMultiplying through by $h^{2}$ yields the dimensionless linear relation\n$$\n- T_{i+1,j} + 2 T_{i,j} - T_{i-1,j} - T_{i,j+1} + 2 T_{i,j} - T_{i,j-1} = h^{2} f_{i,j},\n$$\nwhich can be written as\n$$\n4 T_{i,j} - \\left( T_{i+1,j} + T_{i-1,j} + T_{i,j+1} + T_{i,j-1} \\right) = h^{2} f_{i,j}.\n$$\nUpon assembling unknowns in the natural lexicographic ordering, the matrix $A \\in \\mathbb{R}^{N^{2} \\times N^{2}}$ is sparse with diagonal entries equal to $4$, and off-diagonal entries equal to $-1$ connecting each interior node to its four nearest neighbors (with appropriate truncation at the boundary because of Dirichlet conditions). Therefore, the splitting $A = D - L - U$ has\n- $D$ equal to the diagonal matrix whose diagonal entries are all $4$,\n- $L$ containing the strictly lower-triangular neighbor couplings with entries $1$ (because $A$ has $-1$ off-diagonals and we write $A = D - L - U$),\n- $U$ containing the strictly upper-triangular neighbor couplings with entries $1$.\n\nFrom these, the node-wise iterative updates follow directly from the definitions of the methods.\n\nJacobi method. The Jacobi method uses\n$$\nT^{(k+1)} = D^{-1} \\left( L + U \\right) T^{(k)} + D^{-1} b,\n$$\nwhich at node $(i,j)$ reads\n$$\nT_{i,j}^{(k+1)} = \\frac{1}{4} \\left( T_{i+1,j}^{(k)} + T_{i-1,j}^{(k)} + T_{i,j+1}^{(k)} + T_{i,j-1}^{(k)} + h^{2} f_{i,j} \\right).\n$$\n\nGauss–Seidel method. The Gauss–Seidel method uses\n$$\nT^{(k+1)} = \\left( D - L \\right)^{-1} U \\, T^{(k)} + \\left( D - L \\right)^{-1} b,\n$$\nwhich at node $(i,j)$ in the lexicographic sweep (using the latest available values for “west” and “south” neighbors) is\n$$\nT_{i,j}^{(k+1)} = \\frac{1}{4} \\left( T_{i+1,j}^{(k)} + T_{i-1,j}^{(k+1)} + T_{i,j+1}^{(k)} + T_{i,j-1}^{(k+1)} + h^{2} f_{i,j} \\right).\n$$\n\nSuccessive Over-Relaxation (SOR) method. The SOR method with relaxation parameter $\\omega \\in (0,2)$ uses\n$$\nT^{(k+1)} = \\left( D - \\omega L \\right)^{-1} \\left( (1 - \\omega) D + \\omega U \\right) T^{(k)} + \\omega \\left( D - \\omega L \\right)^{-1} b,\n$$\nwhich at node $(i,j)$ yields the point-wise update\n$$\nT_{i,j}^{(k+1)} = (1 - \\omega) \\, T_{i,j}^{(k)} + \\frac{\\omega}{4} \\left( T_{i+1,j}^{(k)} + T_{i-1,j}^{(k+1)} + T_{i,j+1}^{(k)} + T_{i,j-1}^{(k+1)} + h^{2} f_{i,j} \\right).\n$$\n\nFor convergence analysis, we consider the homogeneous case $f \\equiv 0$ and study error propagation. Let $e^{(k)} = T^{(k)} - T^{\\ast}$ denote the error, where $T^{\\ast}$ is the exact discrete solution. For any stationary method $T^{(k+1)} = M T^{(k)} + c$, the error evolves as $e^{(k+1)} = M e^{(k)}$. The spectral radius of $M$ governs linear convergence: the method converges if and only if the spectral radius is less than $1$.\n\nPart (b): Jacobi spectral radius via discrete sine modes. With Dirichlet boundaries on a uniform grid, the error can be expanded in the discrete sine basis\n$$\n\\phi_{p,q}(i,j) = \\sin\\!\\left( \\frac{\\pi p i}{N+1} \\right) \\sin\\!\\left( \\frac{\\pi q j}{N+1} \\right), \\quad p,q \\in \\{1,2,\\dots,N\\}.\n$$\nFor Jacobi, the error update at node $(i,j)$ is\n$$\ne_{i,j}^{(k+1)} = \\frac{1}{4} \\left( e_{i+1,j}^{(k)} + e_{i-1,j}^{(k)} + e_{i,j+1}^{(k)} + e_{i,j-1}^{(k)} \\right).\n$$\nAssume a mode $e^{(k)} = \\mu^{k} \\phi_{p,q}$, so $e_{i\\pm 1,j}^{(k)} = \\mu^{k} \\sin\\!\\left( \\frac{\\pi p (i \\pm 1)}{N+1} \\right) \\sin\\!\\left( \\frac{\\pi q j}{N+1} \\right)$ and similarly in $j$. Using the trigonometric identity $\\sin(a \\pm b) = \\sin(a) \\cos(b) \\pm \\cos(a) \\sin(b)$ and the symmetry $\\sin(a+1) + \\sin(a-1) = 2 \\sin(a) \\cos(1)$ with $1$ understood as the grid phase increment $\\frac{\\pi p}{N+1}$, we get\n$$\ne_{i+1,j}^{(k)} + e_{i-1,j}^{(k)} = 2 \\mu^{k} \\cos\\!\\left( \\frac{\\pi p}{N+1} \\right) \\phi_{p,q}(i,j), \\quad e_{i,j+1}^{(k)} + e_{i,j-1}^{(k)} = 2 \\mu^{k} \\cos\\!\\left( \\frac{\\pi q}{N+1} \\right) \\phi_{p,q}(i,j).\n$$\nHence\n$$\ne_{i,j}^{(k+1)} = \\mu^{k} \\left[ \\frac{1}{4} \\left( 2 \\cos\\!\\left( \\frac{\\pi p}{N+1} \\right) + 2 \\cos\\!\\left( \\frac{\\pi q}{N+1} \\right) \\right) \\right] \\phi_{p,q}(i,j) = \\mu^{k} \\left[ \\frac{1}{2} \\left( \\cos\\!\\left( \\frac{\\pi p}{N+1} \\right) + \\cos\\!\\left( \\frac{\\pi q}{N+1} \\right) \\right) \\right] \\phi_{p,q}(i,j).\n$$\nTherefore each mode is an eigenvector of the Jacobi iteration matrix $B_{J} = D^{-1}(L+U)$ with eigenvalue\n$$\n\\mu_{p,q} = \\frac{1}{2} \\left( \\cos\\!\\left( \\frac{\\pi p}{N+1} \\right) + \\cos\\!\\left( \\frac{\\pi q}{N+1} \\right) \\right).\n$$\nThe spectral radius is the maximum of $|\\mu_{p,q}|$ over all admissible $p,q$. Since $\\cos(\\cdot)$ decreases from $1$ to $-1$ and the absolute values are symmetric, the maximum occurs at $p = q = 1$, yielding\n$$\n\\rho_{J}(N) = \\mu_{1,1} = \\frac{1}{2} \\left( \\cos\\!\\left( \\frac{\\pi}{N+1} \\right) + \\cos\\!\\left( \\frac{\\pi}{N+1} \\right) \\right) = \\cos\\!\\left( \\frac{\\pi}{N+1} \\right).\n$$\n\nPart (c): Optimal SOR parameter in terms of $\\rho_{J}(N)$. For the discrete Poisson operator on a uniform grid with natural lexicographic ordering, the splitting is consistently ordered. A well-tested fact for consistently ordered splittings (see, e.g., standard results in matrix iterative analysis) is that the eigenvalues of the Gauss–Seidel iteration matrix $B_{GS} = (D - L)^{-1} U$ are the squares of the eigenvalues of $B_{J}$, namely\n$$\n\\mu_{GS,p,q} = \\mu_{p,q}^{2},\n$$\nso that\n$$\n\\rho_{GS}(N) = \\rho_{J}(N)^{2}.\n$$\nAnother well-tested result due to Young and Varga for Successive Over-Relaxation on consistently ordered splittings is that the spectral radius $\\rho_{SOR}(\\omega)$ is minimized by choosing\n$$\n\\omega_{\\mathrm{opt}} = \\frac{2}{1 + \\sqrt{1 - \\rho_{J}^{2}}}.\n$$\nIntuitively, this follows by analyzing the error-propagation factor for each mode under SOR and observing that, for consistently ordered splittings, the SOR iteration matrix’s eigenvalues for a given mode are constrained by the Jacobi eigenvalue; optimization over $\\omega \\in (0,2)$ balances the two contributing roots to minimize their common magnitude, which yields the above closed-form expression in terms of $\\rho_{J}$. Substituting the explicit $\\rho_{J}(N)$ found in part (b) gives\n$$\n\\omega_{\\mathrm{opt}}(N) = \\frac{2}{1 + \\sqrt{\\,1 - \\left[ \\cos\\!\\left( \\frac{\\pi}{N+1} \\right) \\right]^{2}\\,}} = \\frac{2}{1 + \\sin\\!\\left( \\frac{\\pi}{N+1} \\right) }.\n$$\nThis is the required closed-form expression for the optimal relaxation parameter as a function of $N$.",
            "answer": "$$\\boxed{\\frac{2}{1+\\sin\\!\\left(\\frac{\\pi}{N+1}\\right)}}$$"
        },
        {
            "introduction": "Once an iterative method is running, the crucial question becomes: when has the solution converged? A reliable stopping criterion is essential for both accuracy and efficiency. This practice  introduces the concept of backward error, a robust and theoretically sound metric that provides a more meaningful measure of convergence than the raw residual norm. By working through a concrete example, you will learn to calculate the normwise backward error and interpret it as the smallest perturbation to the original problem data that makes your approximate solution an exact one, ensuring your computed results correspond to a physically consistent model.",
            "id": "3965801",
            "problem": "A heterogeneous stationary conduction problem on a bounded domain $\\Omega \\subset \\mathbb{R}^3$ with thermal conductivity $k(\\mathbf{x})$ and volumetric heat source $q(\\mathbf{x})$ is governed by the conservation of energy and Fourier’s law, which together yield the balance $-\\nabla \\cdot (k(\\mathbf{x}) \\nabla T(\\mathbf{x})) = q(\\mathbf{x})$ subject to appropriate boundary conditions. A Finite Volume Method (FVM) discretization of this model with Dirichlet boundary conditions produces a sparse linear system $A x = b$, where $A$ encodes diffusive flux balances and $x$ collects nodal temperatures. An iterative solver such as Conjugate Gradient (CG) is used to approximate the solution, and after $k$ iterations we have an approximation $x_k$ with residual $r = b - A x_k$.\n\nIn practice, stopping criteria for iterative solvers are often expressed in terms of a backward error: the idea is that the computed $x_k$ should be the exact solution of a slightly perturbed model $(A + \\Delta A) x_k = b + \\Delta b$, where the perturbations $\\Delta A$ and $\\Delta b$ are small compared to $A$ and $b$. The normwise backward error quantifies the smallest relative perturbations necessary to make $x_k$ an exact solution of such a perturbed system.\n\nSuppose a particular FVM discretization of the conduction problem yields data with induced matrix and vector $2$-norms: $\\|A\\|_2 = 1.3 \\times 10^{3}$, $\\|x_k\\|_2 = 2.8 \\times 10^{2}$, $\\|b\\|_2 = 1.1 \\times 10^{5}$, and a residual norm $\\|r\\|_2 = 3.0 \\times 10^{-1}$. A proposed stopping tolerance is $\\tau = 1.0 \\times 10^{-6}$.\n\nWhich option correctly specifies the normwise backward error associated with $x_k$ for the linear system $A x = b$ and correctly interprets the tolerance $\\tau$ as a bound on the size of perturbations to $A$ and $b$ in the sense of the induced $2$-norm?\n\nA. The normwise backward error is $\\|r\\|_2 / (\\|A\\|_2 \\|x_k\\|_2 + \\|b\\|_2) \\approx 6.33 \\times 10^{-7}$. Interpreting the tolerance $\\tau$ means: if $\\|r\\|_2 / (\\|A\\|_2 \\|x_k\\|_2 + \\|b\\|_2) \\le \\tau$, then there exist perturbations $\\Delta A$ and $\\Delta b$ such that $(A + \\Delta A) x_k = b + \\Delta b$ with $\\|\\Delta A\\|_2 \\le \\tau \\|A\\|_2$ and $\\|\\Delta b\\|_2 \\le \\tau \\|b\\|_2$.\n\nB. The backward error is the relative residual $\\|r\\|_2 / \\|b\\|_2 \\approx 2.73 \\times 10^{-6}$. Interpreting $\\tau$ means: if $\\|r\\|_2 / \\|b\\|_2 \\le \\tau$, then there exists a perturbation $\\Delta b$ with $\\|\\Delta b\\|_2 \\le \\tau \\|b\\|_2$ such that $A x_k = b + \\Delta b$, with $\\Delta A = 0$.\n\nC. The backward error is $\\|r\\|_2 / (\\|A\\|_2 \\|x_k\\|_2) \\approx 8.24 \\times 10^{-7}$. Interpreting $\\tau$ means: if this quantity is $\\le \\tau$, then there exists a perturbation $\\Delta A$ with $\\|\\Delta A\\|_2 \\le \\tau \\|A\\|_2$ such that $(A + \\Delta A) x_k = b$, with $\\Delta b = 0$.\n\nD. The backward error is defined using a preconditioner $M$ as $\\|M^{-1} r\\|_2 / \\left(\\|M^{-1} A\\|_2 \\|x_k\\|_2 + \\|M^{-1} b\\|_2\\right)$, and interpreting $\\tau$ bounds perturbations with respect to the preconditioned system only; therefore it does not assert bounds on $\\Delta A$ and $\\Delta b$ for the original system unless $M$ is the identity.\n\nE. The backward error is the componentwise quantity $\\max_i \\left( |r_i| / \\left( (|A| |x_k|)_i + |b_i| \\right) \\right)$, and interpreting $\\tau$ bounds each component of $\\Delta A$ and $\\Delta b$ separately; this is the correct normwise statement for the induced $2$-norm case.",
            "solution": "The problem requires us to identify the correct expression for the normwise backward error of an approximate solution $x_k$ to the linear system $A x = b$. The backward error analysis seeks the smallest perturbations $\\Delta A$ and $\\Delta b$ such that $x_k$ is an exact solution to the perturbed system:\n$$ (A + \\Delta A) x_k = b + \\Delta b $$\nRearranging this equation, we get:\n$$ A x_k + (\\Delta A) x_k = b + \\Delta b $$\n$$ (\\Delta A) x_k - \\Delta b = b - A x_k $$\nThe right-hand side is the residual, $r = b - A x_k$. So we have:\n$$ (\\Delta A) x_k - \\Delta b = r $$\nThe normwise backward error, denoted $\\eta(x_k)$, is defined as the smallest non-negative number $\\epsilon$ such that the above equation holds for some perturbations $\\Delta A$ and $\\Delta b$ satisfying $\\|\\Delta A\\| \\le \\epsilon \\|A\\|$ and $\\|\\Delta b\\| \\le \\epsilon \\|b\\|$ for a given pair of compatible matrix and vector norms.\n\nA fundamental result in numerical linear algebra (due to Rigal and Gaches) states that for any submultiplicative matrix norm and its subordinate vector norm, the normwise backward error is given by the expression:\n$$ \\eta(x_k) = \\frac{\\|b - A x_k\\|}{\\|A\\| \\|x_k\\| + \\|b\\|} = \\frac{\\|r\\|}{\\|A\\| \\|x_k\\| + \\|b\\|} $$\nThe problem specifies the use of the induced $2$-norm for all norms. Therefore, the specific formula for the normwise backward error in this case is:\n$$ \\eta(x_k) = \\frac{\\|r\\|_2}{\\|A\\|_2 \\|x_k\\|_2 + \\|b\\|_2} $$\nLet's compute this value using the provided data:\n- $\\|A\\|_2 = 1.3 \\times 10^{3}$\n- $\\|x_k\\|_2 = 2.8 \\times 10^{2}$\n- $\\|b\\|_2 = 1.1 \\times 10^{5}$\n- $\\|r\\|_2 = 3.0 \\times 10^{-1}$\n\nFirst, we calculate the denominator:\n$$ \\|A\\|_2 \\|x_k\\|_2 + \\|b\\|_2 = (1.3 \\times 10^{3}) \\times (2.8 \\times 10^{2}) + (1.1 \\times 10^{5}) $$\n$$ = (3.64 \\times 10^{5}) + (1.1 \\times 10^{5}) = 4.74 \\times 10^{5} $$\nNow, we can compute the backward error:\n$$ \\eta(x_k) = \\frac{3.0 \\times 10^{-1}}{4.74 \\times 10^{5}} = \\frac{3.0}{4.74} \\times 10^{-1-5} \\approx 0.63291 \\times 10^{-6} \\approx 6.33 \\times 10^{-7} $$\nThe interpretation of the tolerance $\\tau$ is that the iterative process is considered to have converged when the approximate solution $x_k$ is the exact solution of a nearby system whose data is perturbed by a relative amount no more than $\\tau$. That is, we stop when $\\eta(x_k) \\le \\tau$. By the definition of $\\eta(x_k)$, this means there exist perturbations $\\Delta A$ and $\\Delta b$ such that $(A + \\Delta A) x_k = b + \\Delta b$ where $\\|\\Delta A\\|_2 \\le \\tau \\|A\\|_2$ and $\\|\\Delta b\\|_2 \\le \\tau \\|b\\|_2$.\n\n**Option-by-Option Analysis**\n\n- **A.** This option states the normwise backward error is $\\|r\\|_2 / (\\|A\\|_2 \\|x_k\\|_2 + \\|b\\|_2) \\approx 6.33 \\times 10^{-7}$. This matches our derived formula and calculation exactly. It also correctly interprets the tolerance $\\tau$: the condition $\\eta(x_k) \\le \\tau$ implies the existence of perturbations $\\Delta A$ and $\\Delta b$ with $\\|\\Delta A\\|_2 \\le \\tau \\|A\\|_2$ and $\\|\\Delta b\\|_2 \\le \\tau \\|b\\|_2$ that make $x_k$ an exact solution to the perturbed system.\n  **Verdict: Correct.**\n\n- **B.** This option defines the backward error as the relative residual, $\\|r\\|_2 / \\|b\\|_2$. Let's compute this: $(3.0 \\times 10^{-1}) / (1.1 \\times 10^{5}) \\approx 2.73 \\times 10^{-6}$. The calculation is correct for the formula given. However, this formula corresponds to a backward error model where only the right-hand side vector $b$ is perturbed (i.e., $\\Delta A = 0$). The problem preamble describes a more general case where both $A$ and $b$ are perturbed. Therefore, this option does not represent the general \"normwise backward error\" requested.\n  **Verdict: Incorrect.**\n\n- **C.** This option defines the backward error as $\\|r\\|_2 / (\\|A\\|_2 \\|x_k\\|_2)$. Let's compute this: $(3.0 \\times 10^{-1}) / ((1.3 \\times 10^{3}) \\times (2.8 \\times 10^{2})) = (3.0 \\times 10^{-1}) / (3.64 \\times 10^{5}) \\approx 8.24 \\times 10^{-7}$. The calculation is correct for the formula given. This formula corresponds to a backward error model where only the matrix $A$ is perturbed (i.e., $\\Delta b = 0$). As with option B, this is a special case and does not represent the general normwise backward error for perturbations to both $A$ and $b$.\n  **Verdict: Incorrect.**\n\n- **D.** This option introduces a preconditioner $M$. The problem statement does not include a preconditioner in its formulation of the system $Ax=b$ or in the given data. While preconditioning is a common technique used with solvers like CG, the question is specifically about the backward error for the system $A x = b$. Introducing a preconditioner unnecessarily complicates the problem and addresses a different question (backward error for the preconditioned system), which is not what was asked.\n  **Verdict: Incorrect.**\n\n- **E.** This option describes the *componentwise* backward error, given by the formula $\\max_i \\left( |r_i| / \\left( (|A| |x_k|)_i + |b_i| \\right) \\right)$. This is a distinct and valid concept, but it is not the *normwise* backward error which was explicitly requested and for which norm-based data ($2$-norms) were provided. The claim that this componentwise definition \"is the correct normwise statement for the induced 2-norm case\" is false.\n  **Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "The theoretical elegance of iterative methods can face harsh tests in practice, especially when dealing with the non-ideal matrices that arise from complex physics like convection-dominated heat transfer. This exercise  presents a critical cautionary tale: the failure of a seemingly intuitive stopping criterion based on the size of the solution update. You will explore why such criteria can falsely signal convergence for nonnormal systems, leading to stagnant iterations and physically inaccurate results. This analysis will underscore the importance of robust, residual-based criteria and highlight the numerical subtleties, including the effects of floating-point arithmetic, that are paramount in high-fidelity scientific computing.",
            "id": "3965783",
            "problem": "You are solving a steady one-dimensional convection–diffusion heat transfer problem, \n$$-\\kappa\\,\\frac{d^2 T}{dx^2} + u\\,\\frac{d T}{dx} = q(x),\\quad x\\in [0,1],$$\nwith Dirichlet boundary conditions, discretized by an upwind finite-difference scheme on a uniform grid with spacing $\\Delta x$. For a moderately large Péclet number $Pe = u\\,\\Delta x/\\kappa \\approx 20$, the resulting linear system $A x = b$ is nonsymmetric and strongly nonnormal. You use the restarted Generalized Minimal Residual (GMRES) method with restart $m$ and standard Gram–Schmidt orthogonalization, run in double precision. The current stopping test declares convergence when the relative update satisfies\n$$\\frac{\\lVert x_{k+1} - x_k\\rVert}{\\lVert x_{k+1}\\rVert} \\le 10^{-8}.$$\nIn practice, this test is satisfied, but an explicit recomputation of the true residual,\n$$r_k^{\\mathrm{true}} = b - A x_k,$$\nreveals that $\\lVert r_k^{\\mathrm{true}}\\rVert \\approx 10^{-2}$ and the solution still exhibits significant energy imbalance in control volumes near the outflow, indicating stagnation.\n\nStarting from the fundamental definitions of the residual $r_k = b - A x_k$, the error $e_k = x_\\star - x_k$ where $A x_\\star = b$, and the update $s_k = x_{k+1} - x_k$, and using only norm inequalities and basic properties of nonnormal matrices (i.e., $A A^\\ast \\ne A^\\ast A$), answer the following. Select all statements that are correct and collectively explain why stopping on $\\lVert x_{k+1} - x_k\\rVert$ can fail to detect stagnation in this setting, and how to remedy this using residual recomputation.\n\nA. For a nonnormal matrix $A$, small $\\lVert s_k\\rVert$ does not imply small $\\lVert r_k\\rVert$ because $r_{k+1} = r_k - A s_k$, so the change in residual is bounded by $\\lVert A\\rVert\\,\\lVert s_k\\rVert$, which can be small even if $\\lVert r_k\\rVert$ itself remains large; additionally, nonnormality can align $s_k$ with directions that do not effectively reduce $\\lVert r_k\\rVert$.\n\nB. In Krylov subspace methods that update $r_k$ by the recurrence $r_{k+1} = r_k - A s_k$, floating-point roundoff and loss of orthogonality can cause the stored (recursively updated) residual to deviate from $r_k^{\\mathrm{true}} = b - A x_k$, particularly for nonnormal $A$, so relying on $\\lVert s_k\\rVert$ can hide residual stagnation; a remedy is to periodically recompute $r_k^{\\mathrm{true}}$ and reset the recurrence to it.\n\nC. If the spectral radius of $A$ is less than $1$, then $\\lVert x_{k+1} - x_k\\rVert$ and $\\lVert r_k\\rVert$ are guaranteed to be equivalent norms, so update-based stopping is safe regardless of nonnormality.\n\nD. A robust fix is simply to tighten the update tolerance (e.g., replace $10^{-8}$ by $10^{-12}$); this guarantees that nonnormality and roundoff will not cause false convergence.\n\nE. A robust alternative stopping test is a normwise backward error criterion based on the recomputed residual,\n$$\\eta_k \\equiv \\frac{\\lVert r_k^{\\mathrm{true}}\\rVert}{\\lVert A\\rVert\\,\\lVert x_k\\rVert + \\lVert b\\rVert},$$\nand to terminate when $\\eta_k \\le \\text{tol}$; periodic recomputation of $r_k^{\\mathrm{true}}$ prevents drift from roundoff and mitigates nonnormal amplification effects in the stopping decision.\n\nSelect all that apply and justify your choices from first principles without invoking any unprovided specialized theorems or software-specific details.",
            "solution": "The problem is to explain why a small update norm, $\\lVert s_k \\rVert = \\lVert x_{k+1} - x_k \\rVert$, does not guarantee a small true residual norm, $\\lVert r_k \\rVert = \\lVert b - A x_k \\rVert$, in the given context.\n\nWe start from the fundamental relationship between the residual and the update.\nThe residual at iteration $k+1$ is\n$$r_{k+1} = b - A x_{k+1}$$\nSubstituting $x_{k+1} = x_k + s_k$:\n$$r_{k+1} = b - A(x_k + s_k) = (b - A x_k) - A s_k$$\nThis yields the residual update formula:\n$$r_{k+1} = r_k - A s_k$$\nBy the triangle inequality, the change in the residual's norm is bounded:\n$$\\lVert r_k - r_{k+1} \\rVert = \\lVert A s_k \\rVert \\le \\lVert A \\rVert \\lVert s_k \\rVert$$\nThis inequality is central to the analysis.\n\n### Option-by-Option Analysis\n\n**A. For a nonnormal matrix $A$, small $\\lVert s_k\\rVert$ does not imply small $\\lVert r_k\\rVert$ because $r_{k+1} = r_k - A s_k$, so the change in residual is bounded by $\\lVert A\\rVert\\,\\lVert s_k\\rVert$, which can be small even if $\\lVert r_k\\rVert$ itself remains large; additionally, nonnormality can align $s_k$ with directions that do not effectively reduce $\\lVert r_k\\rVert$.**\n\nThis statement correctly identifies the core issue. The inequality $\\lVert r_k - r_{k+1} \\rVert \\le \\lVert A \\rVert \\lVert s_k \\rVert$ shows that a small update $\\lVert s_k \\rVert$ only implies that the residual vector is changing by a small amount from one step to the next. It says nothing about the magnitude of the residual vector $\\lVert r_k \\rVert$ itself. If the iteration has reached a point where it can no longer effectively reduce the residual (a state of stagnation), the updates $s_k$ will become very small, causing the update-based stopping criterion to be satisfied, even if the residual $\\lVert r_k \\rVert$ is still large. This stagnation behavior is a well-known characteristic of GMRES when applied to strongly nonnormal matrices. The phrase about nonnormality aligning $s_k$ with ineffective directions is a qualitative but accurate description of why GMRES might struggle to reduce the residual norm, leading to such stagnation.\n\n**Verdict: Correct.**\n\n**B. In Krylov subspace methods that update $r_k$ by the recurrence $r_{k+1} = r_k - A s_k$, floating-point roundoff and loss of orthogonality can cause the stored (recursively updated) residual to deviate from $r_k^{\\mathrm{true}} = b - A x_k$, particularly for nonnormal $A$, so relying on $\\lVert s_k\\rVert$ can hide residual stagnation; a remedy is to periodically recompute $r_k^{\\mathrm{true}}$ and reset the recurrence to it.**\n\nThis statement describes a critical numerical, rather than purely mathematical, aspect of the problem. GMRES builds an orthonormal basis for the Krylov subspace using an orthogonalization procedure. The problem specifies *standard Gram–Schmidt*, which is known to be numerically unstable; in the presence of floating-point arithmetic, the computed basis vectors can rapidly lose their mutual orthogonality. This loss of orthogonality is particularly severe for ill-conditioned or strongly nonnormal matrices. The GMRES algorithm relies on this orthogonality to compute the residual norm efficiently without explicitly forming the solution iterate $x_k$ at every sub-step. When orthogonality is lost, the algorithm's internal calculation of the residual norm (and consequently, its choice of update) becomes inaccurate and can drift significantly from the true residual norm $\\lVert b - A x_k \\rVert$. This can lead the algorithm to believe it has minimized the residual, causing it to produce a very small update $s_k$ and stagnate. This satisfies the update-based stopping test while the true residual remains large. The proposed remedy—periodically recomputing the true residual $r_k^{\\mathrm{true}} = b - A x_k$ and using this value to restart the process—is a standard and effective technique to counteract this drift.\n\n**Verdict: Correct.**\n\n**C. If the spectral radius of $A$ is less than $1$, then $\\lVert x_{k+1} - x_k\\rVert$ and $\\lVert r_k\\rVert$ are guaranteed to be equivalent norms, so update-based stopping is safe regardless of nonnormality.**\n\nThis statement is incorrect for several reasons. Firstly, the condition $\\rho(A) < 1$ is a condition for the convergence of stationary iterative methods of the form $x_{k+1} = (I-A)x_k + b$, not for the problem $Ax=b$ solved by GMRES. For a matrix $A$ arising from a PDE discretization, its eigenvalues are typically related to the grid spacing (e.g., proportional to $1/(\\Delta x)^2$), so $\\rho(A)$ will generally be much greater than $1$. The premise is physically and mathematically irrelevant to the context. Secondly, even if one were to assume $\\rho(A) < 1$, for a nonnormal matrix, the spectral radius provides a poor measure of the matrix's behavior. The norm of powers of the matrix, $\\lVert A^k \\rVert$, can exhibit significant transient growth before eventually decaying. Stagnation is not precluded by $\\rho(A) < 1$. Finally, the claim of \"guaranteed equivalence\" in a way that makes the stopping criterion \"safe\" is false. While all norms on a finite-dimensional space are equivalent, the constants of equivalence can be very large and depend on the properties of $A$ (like its condition number), which can be large for a nonnormal matrix. The condition $\\rho(A) < 1$ does not eliminate the possibility of stagnation where $\\lVert s_k \\rVert$ is small but $\\lVert r_k \\rVert$ is large.\n\n**Verdict: Incorrect.**\n\n**D. A robust fix is simply to tighten the update tolerance (e.g., replace $10^{-8}$ by $10^{-12}$); this guarantees that nonnormality and roundoff will not cause false convergence.**\n\nThis proposed fix is naive and incorrect. The problem is that the stopping criterion is measuring the wrong quantity—the stagnation of the iterate, not its proximity to the true solution. The solver is already reporting a very small update norm ($\\le 10^{-8}$) while the true residual is large ($\\approx 10^{-2}$). Tightening the tolerance to $10^{-12}$ would simply force the solver to continue iterating until the update becomes even smaller. Since the process has stagnated, this will not necessarily lead to a meaningful reduction in the true residual. The solver might expend a great deal of effort to achieve a smaller update, or it might hit a maximum iteration limit, but it does not address the fundamental mismatch between the stopping criterion and the actual measure of error. It does not provide any \"guarantee\" and fails to fix the underlying issues described in A and B.\n\n**Verdict: Incorrect.**\n\n**E. A robust alternative stopping test is a normwise backward error criterion based on the recomputed residual,\n$$\\eta_k \\equiv \\frac{\\lVert r_k^{\\mathrm{true}}\\rVert}{\\lVert A\\rVert\\,\\lVert x_k\\rVert + \\lVert b\\rVert},$$\nand to terminate when $\\eta_k \\le \\text{tol}$; periodic recomputation of $r_k^{\\mathrm{true}}$ prevents drift from roundoff and mitigates nonnormal amplification effects in the stopping decision.**\n\nThis statement proposes a state-of-the-art solution. The quantity $\\eta_k$ is a measure of the normwise backward error. A small backward error $\\eta_k \\le \\text{tol}$ means that the computed solution $x_k$ is the exact solution to a slightly perturbed problem $(A+\\Delta A)x_k = b + \\Delta b$, where the relative size of the perturbation is on the order of $\\text{tol}$. This is a robust and meaningful definition of a \"good\" solution. Crucially, this criterion is based on the true residual, $r_k^{\\mathrm{true}} = b - A x_k$. Using the true residual directly measures how well the solution satisfies the governing equations, bypassing the issue of iterate stagnation (the problem in A). Furthermore, by explicitly stating that $r_k^{\\mathrm{true}}$ must be recomputed, it incorporates the necessary fix for the numerical drift caused by roundoff errors (the problem in B). This criterion is not misled by stagnation or roundoff and provides a reliable stopping test for difficult nonnormal problems.\n\n**Verdict: Correct.**",
            "answer": "$$\\boxed{ABE}$$"
        }
    ]
}