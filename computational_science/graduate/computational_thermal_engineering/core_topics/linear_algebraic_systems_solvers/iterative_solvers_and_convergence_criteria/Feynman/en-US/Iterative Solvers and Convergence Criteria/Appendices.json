{
    "hands_on_practices": [
        {
            "introduction": "The discretization of partial differential equations, such as the Poisson equation for heat conduction, typically results in large, sparse linear systems. This exercise provides a foundational walkthrough of constructing and analyzing the classical stationary iterative solvers: the Jacobi, Gauss-Seidel, and Successive Over-Relaxation (SOR) methods. By deriving the iterative updates from the matrix splitting and analyzing their convergence properties through spectral analysis, you will build a crucial understanding of how these methods work and how their performance can be optimized.",
            "id": "3965780",
            "problem": "Consider steady two-dimensional conduction in a homogeneous solid with temperature field $T(x,y)$ governed by the Poisson form of energy conservation, $-\\nabla^{2} T = f(x,y)$, on the unit square $\\Omega = (0,1) \\times (0,1)$ with homogeneous Dirichlet boundary conditions $T = 0$ on $\\partial \\Omega$. Discretize $\\Omega$ by a uniform Cartesian grid with $N \\times N$ interior points, grid spacing $h = \\frac{1}{N+1}$ in both directions, and approximate $-\\nabla^{2} T$ at interior nodes $(i,j)$ by the standard $5$-point central difference. Assemble the resulting linear system in the natural lexicographic ordering (first index $i$ fastest), and define the matrix splitting $A = D - L - U$, where $D$ is the diagonal part and $L$ and $U$ are the strictly lower and strictly upper parts, respectively.\n\nStarting from the discrete conservation law and the definition of the splitting, perform the following:\n\n(a) Construct $D$, $L$, and $U$ for the assembled matrix $A$ and write the corresponding node-wise iterative update formulas for the Jacobi method, the Gauss–Seidel method, and the Successive Over-Relaxation method (Successive Over-Relaxation (SOR)) with relaxation parameter $\\omega \\in (0,2)$.\n\n(b) For the homogeneous case $f \\equiv 0$, analyze the error-propagation of the Jacobi iteration by expanding the error in the discrete sine basis that respects the Dirichlet boundaries. Determine the spectral radius $\\rho_{J}(N)$ of the Jacobi iteration matrix as an explicit function of $N$.\n\n(c) Using well-tested facts about consistently ordered splittings for the discrete Poisson operator on a uniform grid, derive the relaxation parameter $\\omega_{\\mathrm{opt}}(N)$ that minimizes the spectral radius of the SOR iteration matrix in terms of $\\rho_{J}(N)$. Express $\\omega_{\\mathrm{opt}}(N)$ in closed form.\n\nYour final answer must be the single analytic expression for $\\omega_{\\mathrm{opt}}(N)$, with no numerical substitution and no rounding. No units are required.",
            "solution": "The governing equation $-\\nabla^{2} T = f$ with homogeneous Dirichlet boundary conditions $T=0$ on $\\partial \\Omega$ is discretized on a uniform grid with spacing $h = \\frac{1}{N+1}$ in both directions. The standard $5$-point central difference at an interior node $(i,j)$ gives\n$$\n-\\frac{T_{i+1,j} - 2 T_{i,j} + T_{i-1,j}}{h^{2}} - \\frac{T_{i,j+1} - 2 T_{i,j} + T_{i,j-1}}{h^{2}} = f_{i,j}.\n$$\nMultiplying through by $h^{2}$ yields the dimensionless linear relation\n$$\n- T_{i+1,j} + 2 T_{i,j} - T_{i-1,j} - T_{i,j+1} + 2 T_{i,j} - T_{i,j-1} = h^{2} f_{i,j},\n$$\nwhich can be written as\n$$\n4 T_{i,j} - \\left( T_{i+1,j} + T_{i-1,j} + T_{i,j+1} + T_{i,j-1} \\right) = h^{2} f_{i,j}.\n$$\nUpon assembling unknowns in the natural lexicographic ordering, the matrix $A \\in \\mathbb{R}^{N^{2} \\times N^{2}}$ is sparse with diagonal entries equal to $4$, and off-diagonal entries equal to $-1$ connecting each interior node to its four nearest neighbors (with appropriate truncation at the boundary because of Dirichlet conditions). Therefore, the splitting $A = D - L - U$ has\n- $D$ equal to the diagonal matrix whose diagonal entries are all $4$,\n- $L$ containing the strictly lower-triangular neighbor couplings with entries $1$ (because $A$ has $-1$ off-diagonals and we write $A = D - L - U$),\n- $U$ containing the strictly upper-triangular neighbor couplings with entries $1$.\n\nFrom these, the node-wise iterative updates follow directly from the definitions of the methods.\n\nJacobi method. The Jacobi method uses\n$$\nT^{(k+1)} = D^{-1} \\left( L + U \\right) T^{(k)} + D^{-1} b,\n$$\nwhich at node $(i,j)$ reads\n$$\nT_{i,j}^{(k+1)} = \\frac{1}{4} \\left( T_{i+1,j}^{(k)} + T_{i-1,j}^{(k)} + T_{i,j+1}^{(k)} + T_{i,j-1}^{(k)} + h^{2} f_{i,j} \\right).\n$$\n\nGauss–Seidel method. The Gauss–Seidel method uses\n$$\nT^{(k+1)} = \\left( D - L \\right)^{-1} U \\, T^{(k)} + \\left( D - L \\right)^{-1} b,\n$$\nwhich at node $(i,j)$ in the lexicographic sweep (using the latest available values for “west” and “south” neighbors) is\n$$\nT_{i,j}^{(k+1)} = \\frac{1}{4} \\left( T_{i+1,j}^{(k)} + T_{i-1,j}^{(k+1)} + T_{i,j+1}^{(k)} + T_{i,j-1}^{(k+1)} + h^{2} f_{i,j} \\right).\n$$\n\nSuccessive Over-Relaxation (SOR) method. The SOR method with relaxation parameter $\\omega \\in (0,2)$ uses\n$$\nT^{(k+1)} = \\left( D - \\omega L \\right)^{-1} \\left( (1 - \\omega) D + \\omega U \\right) T^{(k)} + \\omega \\left( D - \\omega L \\right)^{-1} b,\n$$\nwhich at node $(i,j)$ yields the point-wise update\n$$\nT_{i,j}^{(k+1)} = (1 - \\omega) \\, T_{i,j}^{(k)} + \\frac{\\omega}{4} \\left( T_{i+1,j}^{(k)} + T_{i-1,j}^{(k+1)} + T_{i,j+1}^{(k)} + T_{i,j-1}^{(k+1)} + h^{2} f_{i,j} \\right).\n$$\n\nFor convergence analysis, we consider the homogeneous case $f \\equiv 0$ and study error propagation. Let $e^{(k)} = T^{(k)} - T^{\\ast}$ denote the error, where $T^{\\ast}$ is the exact discrete solution. For any stationary method $T^{(k+1)} = M T^{(k)} + c$, the error evolves as $e^{(k+1)} = M e^{(k)}$. The spectral radius of $M$ governs linear convergence: the method converges if and only if the spectral radius is less than $1$.\n\nPart (b): Jacobi spectral radius via discrete sine modes. With Dirichlet boundaries on a uniform grid, the error can be expanded in the discrete sine basis\n$$\n\\phi_{p,q}(i,j) = \\sin\\!\\left( \\frac{\\pi p i}{N+1} \\right) \\sin\\!\\left( \\frac{\\pi q j}{N+1} \\right), \\quad p,q \\in \\{1,2,\\dots,N\\}.\n$$\nFor Jacobi, the error update at node $(i,j)$ is\n$$\ne_{i,j}^{(k+1)} = \\frac{1}{4} \\left( e_{i+1,j}^{(k)} + e_{i-1,j}^{(k)} + e_{i,j+1}^{(k)} + e_{i,j-1}^{(k)} \\right).\n$$\nAssume a mode $e^{(k)} = \\mu^{k} \\phi_{p,q}$, so $e_{i\\pm 1,j}^{(k)} = \\mu^{k} \\sin\\!\\left( \\frac{\\pi p (i \\pm 1)}{N+1} \\right) \\sin\\!\\left( \\frac{\\pi q j}{N+1} \\right)$ and similarly in $j$. Using the trigonometric identity $\\sin(a \\pm b) = \\sin(a) \\cos(b) \\pm \\cos(a) \\sin(b)$ and the symmetry $\\sin(a+1) + \\sin(a-1) = 2 \\sin(a) \\cos(1)$ with $1$ understood as the grid phase increment $\\frac{\\pi p}{N+1}$, we get\n$$\ne_{i+1,j}^{(k)} + e_{i-1,j}^{(k)} = 2 \\mu^{k} \\cos\\!\\left( \\frac{\\pi p}{N+1} \\right) \\phi_{p,q}(i,j), \\quad e_{i,j+1}^{(k)} + e_{i,j-1}^{(k)} = 2 \\mu^{k} \\cos\\!\\left( \\frac{\\pi q}{N+1} \\right) \\phi_{p,q}(i,j).\n$$\nHence\n$$\ne_{i,j}^{(k+1)} = \\mu^{k} \\left[ \\frac{1}{4} \\left( 2 \\cos\\!\\left( \\frac{\\pi p}{N+1} \\right) + 2 \\cos\\!\\left( \\frac{\\pi q}{N+1} \\right) \\right) \\right] \\phi_{p,q}(i,j) = \\mu^{k} \\left[ \\frac{1}{2} \\left( \\cos\\!\\left( \\frac{\\pi p}{N+1} \\right) + \\cos\\!\\left( \\frac{\\pi q}{N+1} \\right) \\right) \\right] \\phi_{p,q}(i,j).\n$$\nTherefore each mode is an eigenvector of the Jacobi iteration matrix $B_{J} = D^{-1}(L+U)$ with eigenvalue\n$$\n\\mu_{p,q} = \\frac{1}{2} \\left( \\cos\\!\\left( \\frac{\\pi p}{N+1} \\right) + \\cos\\!\\left( \\frac{\\pi q}{N+1} \\right) \\right).\n$$\nThe spectral radius is the maximum of $|\\mu_{p,q}|$ over all admissible $p,q$. Since $\\cos(\\cdot)$ decreases from $1$ to $-1$ and the absolute values are symmetric, the maximum occurs at $p = q = 1$, yielding\n$$\n\\rho_{J}(N) = \\mu_{1,1} = \\frac{1}{2} \\left( \\cos\\!\\left( \\frac{\\pi}{N+1} \\right) + \\cos\\!\\left( \\frac{\\pi}{N+1} \\right) \\right) = \\cos\\!\\left( \\frac{\\pi}{N+1} \\right).\n$$\n\nPart (c): Optimal SOR parameter in terms of $\\rho_{J}(N)$. For the discrete Poisson operator on a uniform grid with natural lexicographic ordering, the splitting is consistently ordered. A well-tested fact for consistently ordered splittings (see, e.g., standard results in matrix iterative analysis) is that the eigenvalues of the Gauss–Seidel iteration matrix $B_{GS} = (D - L)^{-1} U$ are the squares of the eigenvalues of $B_{J}$, namely\n$$\n\\mu_{GS,p,q} = \\mu_{p,q}^{2},\n$$\nso that\n$$\n\\rho_{GS}(N) = \\rho_{J}(N)^{2}.\n$$\nAnother well-tested result due to Young and Varga for Successive Over-Relaxation on consistently ordered splittings is that the spectral radius $\\rho_{SOR}(\\omega)$ is minimized by choosing\n$$\n\\omega_{\\mathrm{opt}} = \\frac{2}{1 + \\sqrt{1 - \\rho_{J}^{2}}}.\n$$\nIntuitively, this follows by analyzing the error-propagation factor for each mode under SOR and observing that, for consistently ordered splittings, the SOR iteration matrix’s eigenvalues for a given mode are constrained by the Jacobi eigenvalue; optimization over $\\omega \\in (0,2)$ balances the two contributing roots to minimize their common magnitude, which yields the above closed-form expression in terms of $\\rho_{J}$. Substituting the explicit $\\rho_{J}(N)$ found in part (b) gives\n$$\n\\omega_{\\mathrm{opt}}(N) = \\frac{2}{1 + \\sqrt{\\,1 - \\left[ \\cos\\!\\left( \\frac{\\pi}{N+1} \\right) \\right]^{2}\\,}} = \\frac{2}{1 + \\sin\\!\\left( \\frac{\\pi}{N+1} \\right) }.\n$$\nThis is the required closed-form expression for the optimal relaxation parameter as a function of $N$.",
            "answer": "$$\\boxed{\\frac{2}{1+\\sin\\!\\left(\\frac{\\pi}{N+1}\\right)}}$$"
        },
        {
            "introduction": "While the classical methods analyzed in  work well for symmetric systems, many real-world thermal engineering problems, such as those with significant convection, yield non-symmetric and nonnormal matrices. This practice confronts a common and dangerous pitfall in solving such systems: the phenomenon of premature stagnation, where simple stopping criteria based on the solution update $\\lVert x_{k+1} - x_k \\rVert$ suggest convergence while the true residual remains large. By analyzing this scenario, you will develop a critical awareness of the limitations of naive convergence tests and the importance of robust alternatives.",
            "id": "3965783",
            "problem": "You are solving a steady one-dimensional convection–diffusion heat transfer problem, \n$$-\\,\\kappa\\,\\frac{d^2 T}{dx^2} + u\\,\\frac{d T}{dx} = q(x),\\quad x\\in [0,1],$$\nwith Dirichlet boundary conditions, discretized by an upwind finite-difference scheme on a uniform grid with spacing $\\Delta x$. For a moderately large Péclet number $Pe = u\\,\\Delta x/\\kappa$ (e.g., $Pe \\approx 20$), the resulting linear system $A x = b$ is nonsymmetric and strongly nonnormal. You use the restarted Generalized Minimal Residual (GMRES) method (Generalized Minimal Residual (GMRES)) with restart $m$ and standard Gram–Schmidt orthogonalization, run in double precision. The current stopping test declares convergence when the relative update satisfies\n$$\\frac{\\lVert x_{k+1} - x_k\\rVert}{\\lVert x_{k+1}\\rVert} \\le 10^{-8}.$$\nIn practice, this test is satisfied, but an explicit recomputation of the true residual,\n$$r_k^{\\mathrm{true}} = b - A x_k,$$\nreveals that $\\lVert r_k^{\\mathrm{true}}\\rVert \\approx 10^{-2}$ and the solution still exhibits significant energy imbalance in control volumes near the outflow, indicating stagnation.\n\nStarting from the fundamental definitions of the residual $r_k = b - A x_k$, the error $e_k = x_\\star - x_k$ where $A x_\\star = b$, and the update $s_k = x_{k+1} - x_k$, and using only norm inequalities and basic properties of nonnormal matrices (i.e., $A A^\\ast \\ne A^\\ast A$), answer the following. Select all statements that are correct and collectively explain why stopping on $\\lVert x_{k+1} - x_k\\rVert$ can fail to detect stagnation in this setting, and how to remedy this using residual recomputation.\n\nA. For a nonnormal matrix $A$, small $\\lVert s_k\\rVert$ does not imply small $\\lVert r_k\\rVert$ because $r_{k+1} = r_k - A s_k$, so the change in residual is bounded by $\\lVert A\\rVert\\,\\lVert s_k\\rVert$, which can be small even if $\\lVert r_k\\rVert$ itself remains large; additionally, nonnormality can align $s_k$ with directions that do not effectively reduce $\\lVert r_k\\rVert$.\n\nB. In Krylov subspace methods that update $r_k$ by the recurrence $r_{k+1} = r_k - A s_k$, floating-point roundoff and loss of orthogonality can cause the stored (recursively updated) residual to deviate from $r_k^{\\mathrm{true}} = b - A x_k$, particularly for nonnormal $A$, so relying on $\\lVert s_k\\rVert$ can hide residual stagnation; a remedy is to periodically recompute $r_k^{\\mathrm{true}}$ and reset the recurrence to it.\n\nC. If the spectral radius of $A$ is less than $1$, then $\\lVert x_{k+1} - x_k\\rVert$ and $\\lVert r_k\\rVert$ are guaranteed to be equivalent norms, so update-based stopping is safe regardless of nonnormality.\n\nD. A robust fix is simply to tighten the update tolerance (e.g., replace $10^{-8}$ by $10^{-12}$); this guarantees that nonnormality and roundoff will not cause false convergence.\n\nE. A robust alternative stopping test is a normwise backward error criterion based on the recomputed residual,\n$$\\eta_k \\equiv \\frac{\\lVert r_k^{\\mathrm{true}}\\rVert}{\\lVert A\\rVert\\,\\lVert x_k\\rVert + \\lVert b\\rVert},$$\nand to terminate when $\\eta_k \\le \\text{tol}$; periodic recomputation of $r_k^{\\mathrm{true}}$ prevents drift from roundoff and mitigates nonnormal amplification effects in the stopping decision.\n\nSelect all that apply and justify your choices from first principles without invoking any unprovided specialized theorems or software-specific details.",
            "solution": "This problem highlights two critical failure modes of iterative solvers for nonnormal systems: mathematical stagnation and numerical instability. The correct options are A, B, and E.\n\n**A. Correct.** The relationship between the residual $r_k$ and the update $s_k = x_{k+1} - x_k$ is given by the update formula $r_{k+1} = r_k - A s_k$. The triangle inequality tells us that the *change* in the residual from one step to the next is bounded: $\\|r_{k+1} - r_k\\| = \\|A s_k\\| \\le \\|A\\| \\|s_k\\|$. A stopping criterion based on small $\\|s_k\\|$ therefore only confirms that the iteration is no longer making significant progress, i.e., it has stagnated. It provides no information about the magnitude of the residual $\\|r_k\\|$ itself. For a nonnormal problem, it is common for solvers like GMRES to reach a state where the update directions available in the Krylov subspace are nearly orthogonal to the residual, leading to very small updates and stagnation, even if the residual is still large.\n\n**B. Correct.** This statement addresses a crucial numerical aspect. GMRES and other Krylov methods build an orthonormal basis for the Krylov subspace. The use of standard Gram-Schmidt, which is known to be numerically unstable, means that in finite-precision arithmetic, the basis vectors can quickly lose their orthogonality. This is especially true for ill-conditioned or nonnormal matrices. The algorithm relies on this orthogonality to track the norm of the residual. When orthogonality is lost, the algorithm's internal, recursively updated residual can drift significantly from the true residual, $r_k^{\\mathrm{true}} = b - A x_k$. The algorithm may then incorrectly perceive that the residual is small, leading it to produce a very small update $s_k$ and satisfy the stopping test, while the true residual remains large. The proposed remedy—periodically recomputing the true residual directly from its definition and resetting the recurrence—is a standard and effective technique to counteract this numerical drift.\n\n**C. Incorrect.** First, for a typical matrix $A$ from a PDE discretization, the spectral radius $\\rho(A)$ will be much greater than 1, so the premise is irrelevant. Second, even if $\\rho(A)  1$, norm equivalence is a purely topological concept. The equivalence constants can be arbitrarily large and depend on properties of $A$. For a highly nonnormal matrix, a very small update norm $\\|s_k\\|$ can still correspond to a large residual norm $\\|r_k\\|$. This statement is fundamentally incorrect.\n\n**D. Incorrect.** This is a naive solution that fails to address the root cause. The problem is that the stopping criterion is monitoring the wrong quantity (iterate stagnation) instead of the true error measure (residual size). If the solver has stagnated with a residual of $10^{-2}$ and an update of $10^{-8}$, simply demanding an update of $10^{-12}$ will likely just cause the solver to run for many more useless iterations, without significantly reducing the true residual. It does not fix the flawed logic of the stopping test.\n\n**E. Correct.** This proposes a state-of-the-art, robust stopping criterion. The normwise backward error, $\\eta_k$, asks a physically meaningful question: \"Is the computed solution $x_k$ the exact solution to a slightly perturbed problem?\" By being based on the *true, recomputed residual* $r_k^{\\mathrm{true}}$, it directly measures how well the solution satisfies the discrete equations, bypassing the stagnation issue of statement A. By explicitly requiring recomputation, it addresses the numerical drift issue of statement B. This provides a reliable measure of solution quality that is not fooled by the pathologies of nonnormal systems.",
            "answer": "$$\\boxed{ABE}$$"
        },
        {
            "introduction": "Having seen how simple stopping criteria can fail for nonnormal systems , we now turn to a more robust and mathematically sound approach: backward error analysis. Instead of asking how close our approximate solution is to the true solution, this framework asks if our solution is the *exact* solution to a slightly perturbed version of the original problem. This practice challenges you to compute and interpret the normwise backward error, providing a powerful and reliable tool for designing stopping criteria that ensure the physical and numerical integrity of your simulation results.",
            "id": "3965801",
            "problem": "A heterogeneous stationary conduction problem on a bounded domain $\\Omega \\subset \\mathbb{R}^3$ with thermal conductivity $k(\\mathbf{x})$ and volumetric heat source $q(\\mathbf{x})$ is governed by the conservation of energy and Fourier’s law, which together yield the balance $-\\nabla \\cdot (k(\\mathbf{x}) \\nabla T(\\mathbf{x})) = q(\\mathbf{x})$ subject to appropriate boundary conditions. A Finite Volume Method (FVM) discretization of this model with Dirichlet boundary conditions produces a sparse linear system $A x = b$, where $A$ encodes diffusive flux balances and $x$ collects nodal temperatures. An iterative solver such as Conjugate Gradient (CG) is used to approximate the solution, and after $k$ iterations we have an approximation $x_k$ with residual $r = b - A x_k$.\n\nIn practice, stopping criteria for iterative solvers are often expressed in terms of a backward error: the idea is that the computed $x_k$ should be the exact solution of a slightly perturbed model $(A + \\Delta A) x_k = b + \\Delta b$, where the perturbations $\\Delta A$ and $\\Delta b$ are small compared to $A$ and $b$. The normwise backward error quantifies the smallest relative perturbations necessary to make $x_k$ an exact solution of such a perturbed system.\n\nSuppose a particular FVM discretization of the conduction problem yields data with induced matrix and vector $2$-norms: $\\|A\\|_2 = 1.3 \\times 10^{3}$, $\\|x_k\\|_2 = 2.8 \\times 10^{2}$, $\\|b\\|_2 = 1.1 \\times 10^{5}$, and a residual norm $\\|r\\|_2 = 3.0 \\times 10^{-1}$. A proposed stopping tolerance is $\\tau = 1.0 \\times 10^{-6}$.\n\nWhich option correctly specifies the normwise backward error associated with $x_k$ for the linear system $A x = b$ and correctly interprets the tolerance $\\tau$ as a bound on the size of perturbations to $A$ and $b$ in the sense of the induced $2$-norm?\n\n- A. The normwise backward error is $\\|r\\|_2 / (\\|A\\|_2 \\|x_k\\|_2 + \\|b\\|_2) \\approx 6.33 \\times 10^{-7}$. Interpreting the tolerance $\\tau$ means: if $\\|r\\|_2 / (\\|A\\|_2 \\|x_k\\|_2 + \\|b\\|_2) \\le \\tau$, then there exist perturbations $\\Delta A$ and $\\Delta b$ such that $(A + \\Delta A) x_k = b + \\Delta b$ with $\\|\\Delta A\\|_2 \\le \\tau \\|A\\|_2$ and $\\|\\Delta b\\|_2 \\le \\tau \\|b\\|_2$.\n\n- B. The backward error is the relative residual $\\|r\\|_2 / \\|b\\|_2 \\approx 2.73 \\times 10^{-6}$. Interpreting $\\tau$ means: if $\\|r\\|_2 / \\|b\\|_2 \\le \\tau$, then there exists a perturbation $\\Delta b$ with $\\|\\Delta b\\|_2 \\le \\tau \\|b\\|_2$ such that $A x_k = b + \\Delta b$, with $\\Delta A = 0$.\n\n- C. The backward error is $\\|r\\|_2 / (\\|A\\|_2 \\|x_k\\|_2) \\approx 8.24 \\times 10^{-7}$. Interpreting $\\tau$ means: if this quantity is $\\le \\tau$, then there exists a perturbation $\\Delta A$ with $\\|\\Delta A\\|_2 \\le \\tau \\|A\\|_2$ such that $(A + \\Delta A) x_k = b$, with $\\Delta b = 0$.\n\n- D. The backward error is defined using a preconditioner $M$ as $\\|M^{-1} r\\|_2 / \\left(\\|M^{-1} A\\|_2 \\|x_k\\|_2 + \\|M^{-1} b\\|_2\\right)$, and interpreting $\\tau$ bounds perturbations with respect to the preconditioned system only; therefore it does not assert bounds on $\\Delta A$ and $\\Delta b$ for the original system unless $M$ is the identity.\n\n- E. The backward error is the componentwise quantity $\\max_i \\left( |r_i| / \\left( (|A| |x_k|)_i + |b_i| \\right) \\right)$, and interpreting $\\tau$ bounds each component of $\\Delta A$ and $\\Delta b$ separately; this is the correct normwise statement for the induced $2$-norm case.",
            "solution": "The correct option is A. This solution breaks down the reasoning.\n\n**1. Definition of Normwise Backward Error**\n\nThe core idea of backward error analysis is to determine if the approximate solution $x_k$ is the *exact* solution to a \"nearby\" problem. For a linear system $Ax=b$, this means finding the smallest perturbations $\\Delta A$ and $\\Delta b$ such that $(A + \\Delta A)x_k = b + \\Delta b$. The normwise backward error, $\\eta(x_k)$, is the smallest non-negative number $\\epsilon$ for which such perturbations exist that satisfy $\\|\\Delta A\\| \\le \\epsilon \\|A\\|$ and $\\|\\Delta b\\| \\le \\epsilon \\|b\\|$.\n\nA fundamental result in numerical linear algebra (the Rigal-Gaches theorem) provides a closed-form expression for this quantity for any submultiplicative matrix norm and its subordinate vector norm:\n$$ \\eta(x_k) = \\frac{\\|b - A x_k\\|}{\\|A\\| \\|x_k\\| + \\|b\\|} = \\frac{\\|r\\|}{\\|A\\| \\|x_k\\| + \\|b\\|} $$\nThe problem specifies using the induced 2-norm for all norms, so the formula is:\n$$ \\eta(x_k) = \\frac{\\|r\\|_2}{\\|A\\|_2 \\|x_k\\|_2 + \\|b\\|_2} $$\nThis matches the formula presented in option A.\n\n**2. Calculation**\n\nLet's plug in the given values:\n- $\\|r\\|_2 = 3.0 \\times 10^{-1}$\n- $\\|A\\|_2 = 1.3 \\times 10^{3}$\n- $\\|x_k\\|_2 = 2.8 \\times 10^{2}$\n- $\\|b\\|_2 = 1.1 \\times 10^{5}$\n\nFirst, calculate the denominator:\n$$ \\|A\\|_2 \\|x_k\\|_2 + \\|b\\|_2 = (1.3 \\times 10^{3}) \\times (2.8 \\times 10^{2}) + (1.1 \\times 10^{5}) $$\n$$ = 3.64 \\times 10^{5} + 1.1 \\times 10^{5} = 4.74 \\times 10^{5} $$\n\nNow, calculate the backward error:\n$$ \\eta(x_k) = \\frac{3.0 \\times 10^{-1}}{4.74 \\times 10^{5}} \\approx 0.6329 \\times 10^{-6} \\approx 6.33 \\times 10^{-7} $$\nThis calculated value matches the value given in option A.\n\n**3. Interpretation of the Tolerance**\n\nA stopping criterion based on backward error terminates the iteration when $\\eta(x_k) \\le \\tau$. Based on the definition of $\\eta(x_k)$, this means that the computed solution $x_k$ is the exact solution to a perturbed problem $(A+\\Delta A)x_k = b+\\Delta b$ where the relative sizes of the perturbations are bounded by $\\tau$, i.e., $\\|\\Delta A\\|_2 \\le \\tau \\|A\\|_2$ and $\\|\\Delta b\\|_2 \\le \\tau \\|b\\|_2$. This interpretation perfectly matches the one given in option A.\n\n**4. Analysis of Other Options**\n\n- **B and C:** These options represent special cases of backward error where only the right-hand side $b$ (Option B) or the matrix $A$ (Option C) is assumed to be perturbed. The general normwise backward error, as defined in the problem's preamble, allows for perturbations in both.\n- **D:** This introduces a preconditioner $M$, which is not part of the problem statement for the system $Ax=b$. It correctly states that backward error for a preconditioned system relates to that system, not necessarily the original one, but it is irrelevant to the question asked.\n- **E:** This defines the *componentwise* backward error, which is a different concept from the *normwise* backward error requested by the problem. The use of 2-norms in the given data confirms that a norm-based analysis is required.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}