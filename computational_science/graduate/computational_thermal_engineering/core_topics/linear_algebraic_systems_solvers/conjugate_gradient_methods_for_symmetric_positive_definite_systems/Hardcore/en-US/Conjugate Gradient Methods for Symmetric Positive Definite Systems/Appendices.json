{
    "hands_on_practices": [
        {
            "introduction": "To fully appreciate the power of the Conjugate Gradient (CG) method, it is instructive to compare it with its predecessor, the Steepest Descent (SD) method. While both algorithms seek to minimize the same quadratic energy functional, their search strategies lead to vastly different performance characteristics, particularly for the ill-conditioned systems common in thermal engineering. This practice provides a direct, hands-on comparison by applying both methods to a heat conduction problem, allowing you to numerically observe the \"zigzagging\" stagnation of SD and the far superior efficiency of CG's A-conjugate search directions .",
            "id": "3942625",
            "problem": "Consider two-dimensional steady heat conduction on the unit square domain with homogeneous Dirichlet boundary conditions, governed by the partial differential equation $-\\nabla \\cdot (K \\nabla T) = s$, where $K = \\mathrm{diag}(k_x, k_y)$ is a constant thermal conductivity tensor, $T$ is the temperature field, and $s$ is a uniform source term. Using a second-order central finite difference discretization on an interior grid of size $n_x \\times n_y$ with spacings $h_x = 1/(n_x+1)$ and $h_y = 1/(n_y+1)$, this yields a linear system $A x = b$ with a sparse symmetric positive definite (SPD) matrix $A$. The unknown vector $x$ contains the temperatures at interior grid points, and the right-hand side $b$ corresponds to the discrete source.\n\nThe energy functional associated with this SPD system is $E(x) = \\tfrac{1}{2} x^{\\top} A x - b^{\\top} x$, with gradient $\\nabla E(x) = A x - b$. From the physical principle of minimum energy in equilibrium, the unique minimizer $x^\\star$ satisfies $A x^\\star = b$. Algorithms that minimize $E(x)$ or solve $A x = b$ therefore exploit the geometry induced by $A$.\n\nThe goal is to demonstrate numerically why Steepest Descent (SD) stagnates compared to Conjugate Gradient (CG) when the condition number of $A$ is high, and to relate this behavior to energy minimization geometry.\n\nImplement the following:\n\n- Construct $A$ as the sparse SPD matrix arising from the anisotropic diffusion operator, with diagonal entries $2 \\left( \\frac{k_x}{h_x^2} + \\frac{k_y}{h_y^2} \\right)$ and off-diagonal neighbors in the $x$-direction equal to $-\\frac{k_x}{h_x^2}$ and in the $y$-direction equal to $-\\frac{k_y}{h_y^2}$. Use homogeneous Dirichlet boundaries by eliminating boundary nodes. Let $b$ be the vector of ones, representing a uniform source $s = 1$ evaluated at interior grid points.\n- Implement Steepest Descent (SD) with exact line search along the negative gradient of $E(x)$, starting from $x_0 = 0$. Stop when the relative residual norm $\\|r_k\\|_2 / \\|b\\|_2$ falls below a specified tolerance or when a specified maximum number of iterations is reached.\n- Implement Conjugate Gradient (CG), starting from $x_0 = 0$, with stopping criteria identical to SD.\n- For each algorithm, compute:\n  1. The number of iterations required to meet the tolerance (or the maximum if not met).\n  2. The average absolute cosine between successive search directions, defined as $\\frac{1}{m-1} \\sum_{k=0}^{m-2} \\left| \\frac{p_k^{\\top} p_{k+1}}{\\|p_k\\|_2 \\|p_{k+1}\\|_2} \\right|$, where $p_k$ is the search direction at iteration $k$ and $m$ is the number of iterations performed. For Steepest Descent, take $p_k$ equal to the residual direction (the negative gradient). For Conjugate Gradient, take $p_k$ equal to the conjugate search direction.\n\nUse the following test suite of parameter values:\n\n- Test $1$ (general isotropic case): $n_x = 16$, $n_y = 16$, $k_x = 1$, $k_y = 1$, tolerance $10^{-8}$, maximum iterations $2000$.\n- Test $2$ (moderate anisotropy): $n_x = 16$, $n_y = 16$, $k_x = 100$, $k_y = 1$, tolerance $10^{-8}$, maximum iterations $3000$.\n- Test $3$ (strong anisotropy edge case): $n_x = 32$, $n_y = 32$, $k_x = 10000$, $k_y = 1$, tolerance $10^{-6}$, maximum iterations $5000$.\n\nFor each test case, compute and return a list of five values in the order:\n- The integer number of SD iterations.\n- The integer number of CG iterations.\n- The floating-point ratio $\\text{SD iterations} / \\text{CG iterations}$.\n- The floating-point average absolute cosine between successive SD directions.\n- The floating-point average absolute cosine between successive CG directions.\n\nYour program should produce a single line of output containing the results as a comma-separated list of three sublists (one per test case), each sublist in the format specified above, all enclosed in square brackets. For example: \"[[sd1,cg1,ratio1,cosSD1,cosCG1],[sd2,cg2,ratio2,cosSD2,cosCG2],[sd3,cg3,ratio3,cosSD3,cosCG3]]\". All values must be printed in Python list format with fundamental types only (integers and floats), and no units are required in the output.",
            "solution": "The problem requires a numerical comparison of the Steepest Descent (SD) and Conjugate Gradient (CG) algorithms for solving a sparse, symmetric positive definite (SPD) linear system $A x = b$. This system arises from the finite difference discretization of a steady-state anisotropic heat conduction problem on a unit square. The objective is to demonstrate and explain the superior performance of CG over SD, particularly when the system is ill-conditioned due to high thermal anisotropy, and to relate this behavior to the geometry of the associated energy minimization problem.\n\nFirst, we formalize the problem. The governing partial differential equation is $-\\nabla \\cdot (K \\nabla T) = s$ on $\\Omega = (0,1) \\times (0,1)$, with temperature $T=0$ on the boundary $\\partial\\Omega$. The thermal conductivity tensor is $K = \\mathrm{diag}(k_x, k_y)$, and the source term $s$ is a constant. We discretize the domain using a uniform grid with $n_x$ and $n_y$ interior points in the $x$ and $y$ directions, respectively. The grid spacings are $h_x = 1/(n_x+1)$ and $h_y = 1/(n_y+1)$.\n\nUsing a second-order central finite difference scheme for the divergence term at an interior grid point $(i, j)$ yields:\n$$\n-\\left( k_x \\frac{T_{i+1,j} - 2T_{i,j} + T_{i-1,j}}{h_x^2} + k_y \\frac{T_{i,j+1} - 2T_{i,j} + T_{i,j-1}}{h_y^2} \\right) = s_{i,j}\n$$\nRearranging the terms for the unknown temperature $T_{i,j}$ gives:\n$$\n\\left( \\frac{2k_x}{h_x^2} + \\frac{2k_y}{h_y^2} \\right) T_{i,j} - \\frac{k_x}{h_x^2} T_{i+1,j} - \\frac{k_x}{h_x^2} T_{i-1,j} - \\frac{k_y}{h_y^2} T_{i,j-1} - \\frac{k_y}{h_y^2} T_{i,j+1} = s_{i,j}\n$$\nThe homogeneous Dirichlet boundary conditions ($T=0$ on $\\partial\\Omega$) are incorporated by setting any $T$ term with an index outside the interior grid to zero. By ordering the $N = n_x n_y$ unknown temperatures $T_{i,j}$ into a single vector $x$, we obtain the linear system $A x = b$. The matrix $A$ is a sparse, block-tridiagonal, symmetric matrix. It is also positive definite for $k_x, k_y > 0$. The vector $b$ is constructed from the source term values $s_{i,j}$, which are set to $1$ for all interior points.\n\nThe solution to $A x = b$ is also the unique minimizer of the quadratic energy functional $E(x) = \\frac{1}{2} x^{\\top} A x - b^{\\top} x$. Iterative methods like SD and CG find this minimizer by generating a sequence of approximations $x_0, x_1, x_2, \\dots$ that progressively reduce the energy $E(x_k)$. The geometry of the level sets of $E(x)$, which are $N$-dimensional ellipsoids, dictates the convergence behavior. The eccentricity of these ellipsoids is determined by the condition number of $A$, $\\kappa(A) = \\lambda_{\\max}(A) / \\lambda_{\\min}(A)$. High anisotropy, e.g., $k_x \\gg k_y$, leads to a large condition number and thus highly elongated energy ellipsoids.\n\nThe Steepest Descent (SD) algorithm at iteration $k$ updates the solution via $x_{k+1} = x_k + \\alpha_k p_k$, where the search direction $p_k$ is chosen to be the direction of steepest descent of the energy functional. This direction is the negative of the gradient, $p_k = -\\nabla E(x_k) = b - A x_k = r_k$, where $r_k$ is the residual. The step size $\\alpha_k$ is chosen to minimize $E(x_{k+1})$ along this direction, yielding the exact line search formula:\n$$\n\\alpha_k = \\frac{p_k^{\\top} r_k}{p_k^{\\top} A p_k} = \\frac{r_k^{\\top} r_k}{r_k^{\\top} A r_k}\n$$\nA key property of SD with exact line search is that successive residuals (and thus search directions) are orthogonal: $r_{k+1}^{\\top} r_k = 0$. Geometrically, this means the search path zigzags. At each step, the algorithm moves orthogonally to the current level set until it is tangent to a new, lower-energy level set. The next step is then orthogonal to this tangent point. For ill-conditioned systems (highly eccentric ellipsoids), these orthogonal steps are very inefficient for navigating the long, narrow \"valleys\" of the energy landscape, leading to extremely slow convergence, or stagnation. The average absolute cosine between successive search directions, which are orthogonal in theory, will be nearly zero in practice due to floating-point arithmetic.\n\nThe Conjugate Gradient (CG) algorithm significantly improves upon SD by choosing more intelligent search directions. The search directions $\\{p_0, p_1, \\dots\\}$ are constructed to be A-orthogonal (or conjugate), i.e., $p_i^{\\top} A p_j = 0$ for $i \\neq j$. The $k$-th search direction $p_k$ is A-orthogonalized against the previous direction $p_{k-1}$:\n$$\np_k = r_k + \\beta_{k-1} p_{k-1} \\quad \\text{where} \\quad \\beta_{k-1} = \\frac{r_k^{\\top} r_k}{r_{k-1}^{\\top} r_{k-1}}\n$$\nThis construction ensures that at each step $k$, the error component in the direction of $p_{k-1}$ is eliminated and will not be re-introduced in subsequent steps. This prevents the wasteful zigzagging of SD and allows CG to converge much more rapidly, especially for ill-conditioned systems. The convergence rate of CG depends on $\\sqrt{\\kappa(A)}$, a substantial improvement over SD's dependence on $\\kappa(A)$. The successive search directions are not orthogonal in the standard sense; the average absolute cosine will be non-zero, indicating that the path is more direct and less oscillatory than SD.\n\nThe implementation will construct the sparse matrix $A$ using `scipy.sparse`, and then implement both the SD and CG algorithms starting from $x_0 = 0$. For each test case, we will compute the number of iterations required to reach the specified relative residual tolerance, the ratio of SD to CG iterations, and the average absolute cosine between successive search directions for both methods. The results will numerically validate the theoretical advantages of CG over SD.",
            "answer": "```python\nimport numpy as np\nfrom scipy import sparse\n\ndef build_A(nx, ny, hx, hy, kx, ky):\n    \"\"\"\n    Constructs the sparse SPD matrix A for the 2D anisotropic diffusion problem.\n    \"\"\"\n    N = nx * ny\n    A = sparse.lil_matrix((N, N))\n    \n    diag_val = 2 * (kx / hx**2 + ky / hy**2)\n    x_off_diag = -kx / hx**2\n    y_off_diag = -ky / hy**2\n\n    for j in range(ny):\n        for i in range(nx):\n            k = i + j * nx\n            \n            # Diagonal entry\n            A[k, k] = diag_val\n            \n            # Off-diagonal entries\n            if i > 0:  # West neighbor\n                A[k, k - 1] = x_off_diag\n            if i  nx - 1:  # East neighbor\n                A[k, k + 1] = x_off_diag\n            if j > 0:  # South neighbor\n                A[k, k - nx] = y_off_diag\n            if j  ny - 1:  # North neighbor\n                A[k, k + nx] = y_off_diag\n                \n    return A.tocsr()\n\ndef calc_avg_cos(dirs):\n    \"\"\"\n    Calculates the average absolute cosine between successive vectors in a list.\n    \"\"\"\n    m = len(dirs)\n    if m  2:\n        return 0.0\n    \n    total_cos = 0.0\n    for i in range(m - 1):\n        p_curr = dirs[i]\n        p_next = dirs[i+1]\n        \n        norm_curr = np.linalg.norm(p_curr)\n        norm_next = np.linalg.norm(p_next)\n        \n        if norm_curr == 0.0 or norm_next == 0.0:\n            continue\n            \n        cos_val = (p_curr @ p_next) / (norm_curr * norm_next)\n        total_cos += abs(cos_val)\n        \n    return total_cos / (m - 1)\n\ndef steepest_descent(A, b, tol, max_iter):\n    \"\"\"\n    Implements the Steepest Descent algorithm.\n    \"\"\"\n    x = np.zeros(A.shape[0])\n    r = b - A @ x\n    b_norm = np.linalg.norm(b)\n    \n    if b_norm == 0.0:\n        b_norm = 1.0\n        \n    search_dirs = []\n    \n    iters = 0\n    for k in range(max_iter):\n        if np.linalg.norm(r) / b_norm  tol:\n            break\n        \n        iters += 1\n        \n        search_dirs.append(r)\n        \n        Ar = A @ r\n        alpha = (r @ r) / (r @ Ar)\n        \n        x = x + alpha * r\n        r = r - alpha * Ar\n        \n    avg_cos = calc_avg_cos(search_dirs)\n    return iters, avg_cos\n\ndef conjugate_gradient(A, b, tol, max_iter):\n    \"\"\"\n    Implements the Conjugate Gradient algorithm.\n    \"\"\"\n    x = np.zeros(A.shape[0])\n    r = b - A @ x\n    p = r.copy()\n    rs_old = r @ r\n    \n    b_norm = np.linalg.norm(b)\n    if b_norm == 0.0:\n        b_norm = 1.0\n\n    search_dirs = []\n    \n    iters = 0\n    for k in range(max_iter):\n        if np.sqrt(rs_old) / b_norm  tol:\n            break\n            \n        iters += 1\n        \n        search_dirs.append(p)\n        \n        Ap = A @ p\n        alpha = rs_old / (p @ Ap)\n        \n        x = x + alpha * p\n        r = r - alpha * Ap\n        \n        rs_new = r @ r\n        \n        beta = rs_new / rs_old\n        p = r + beta * p\n        \n        rs_old = rs_new\n\n    avg_cos = calc_avg_cos(search_dirs)\n    return iters, avg_cos\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        {'nx': 16, 'ny': 16, 'kx': 1, 'ky': 1, 'tol': 1e-8, 'max_iter': 2000},\n        {'nx': 16, 'ny': 16, 'kx': 100, 'ky': 1, 'tol': 1e-8, 'max_iter': 3000},\n        {'nx': 32, 'ny': 32, 'kx': 10000, 'ky': 1, 'tol': 1e-6, 'max_iter': 5000}\n    ]\n\n    all_results = []\n    \n    for params in test_cases:\n        nx, ny, kx, ky = params['nx'], params['ny'], params['kx'], params['ky']\n        tol, max_iter = params['tol'], params['max_iter']\n        \n        hx = 1.0 / (nx + 1)\n        hy = 1.0 / (ny + 1)\n        N = nx * ny\n        \n        A = build_A(nx, ny, hx, hy, kx, ky)\n        b = np.ones(N)\n        \n        sd_iters, sd_cos = steepest_descent(A, b, tol, max_iter)\n        cg_iters, cg_cos = conjugate_gradient(A, b, tol, max_iter)\n        \n        ratio = float('inf') if cg_iters == 0 else sd_iters / cg_iters\n        \n        all_results.append([sd_iters, cg_iters, ratio, sd_cos, cg_cos])\n\n    # Format output as a string representation of a list of lists.\n    result_str = \",\".join(map(str, all_results))\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The remarkable efficiency of the Conjugate Gradient method is not unconditional; it relies fundamentally on the mathematical properties of the system matrix being Symmetric and Positive Definite (SPD). This exercise is designed to demonstrate what happens when these foundational assumptions are violated, a scenario that can arise from modeling errors or the use of certain numerical schemes. By applying the standard CG algorithm to a system with a controlled degree of non-symmetry, you will witness the breakdown of the method's convergence guarantees, providing a crucial lesson on the importance of matching an algorithm to the problem's structure .",
            "id": "3252635",
            "problem": "Consider the linear system with a matrix that is modelled as symmetric positive definite but is in fact slightly nonsymmetric. Let $n$ be a positive integer dimension, and let $A \\in \\mathbb{R}^{n \\times n}$ be the tridiagonal matrix with $2$ on the diagonal and $-1$ on each immediate off-diagonal (the standard one-dimensional discrete Laplacian with Dirichlet boundary conditions). Consider a nonsymmetric perturbation defined by $B \\in \\mathbb{R}^{n \\times n}$ with entries $B_{ij} = 1$ if $j  i$ and $B_{ij} = 0$ otherwise, and define the perturbed matrix $\\tilde{A} = A + \\epsilon B$ for a given scalar $\\epsilon \\in \\mathbb{R}$. The right-hand side vector is $b \\in \\mathbb{R}^{n}$ defined by $b_i = 1$ for all indices $i$.\n\nTheoretical Background. The Conjugate Gradient (CG) method is derived for matrices that are symmetric positive definite (SPD). For SPD matrices, the CG method can be interpreted as a process that iteratively minimizes a strictly convex quadratic functional associated with the matrix, and its search directions are mutually $A$-conjugate and residuals are mutually orthogonal. These properties rely on symmetry and positive definiteness.\n\nTask. You must implement the Conjugate Gradient (CG) method that assumes symmetry and applies it directly to the sequence of matrices $\\tilde{A}$ above. For each test case, run CG with an initial guess $x^{(0)} = 0$, a maximum number of iterations equal to $n$, and a stopping criterion based on the relative residual norm being less than a tolerance $10^{-10}$, that is, stop as soon as $\\|r^{(k)}\\|_2 / \\|b\\|_2  10^{-10}$ where $r^{(k)} = b - \\tilde{A} x^{(k)}$. For each test case, also compute the true solution $x^{\\ast}$ by solving $\\tilde{A} x = b$ with a direct dense solver, and measure the relative solution error $\\|x_{\\mathrm{CG}} - x^{\\ast}\\|_2 / \\|x^{\\ast}\\|_2$. Interpret any deviation in convergence behavior or solution accuracy as a modeling error arising from incorrectly assuming symmetry in $\\tilde{A}$ when choosing CG.\n\nTest suite. Use the following parameter values and report results for each:\n\n- Case $1$ (happy path, exact applicability): $n = 50$, $\\epsilon = 0$.\n- Case $2$ (small nonsymmetry): $n = 50$, $\\epsilon = 10^{-3}$.\n- Case $3$ (moderate nonsymmetry): $n = 50$, $\\epsilon = 10^{-1}$.\n- Case $4$ (significant nonsymmetry): $n = 50$, $\\epsilon = 5 \\cdot 10^{-1}$.\n\nFor each case, return two outputs:\n- A float equal to the relative solution error $\\|x_{\\mathrm{CG}} - x^{\\ast}\\|_2 / \\|x^{\\ast}\\|_2$.\n- A boolean indicating whether CG satisfied the stopping criterion within $n$ iterations (true if $\\|r^{(k)}\\|_2 / \\|b\\|_2  10^{-10}$ at or before $k = n$, false otherwise).\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by test case and within each test case by the float followed by the boolean. For example, in the format $[e_1,c_1,e_2,c_2,e_3,c_3,e_4,c_4]$, where $e_i$ are floats and $c_i$ are booleans.",
            "solution": "The problem requires an investigation into the consequences of a specific modeling error: applying the Conjugate Gradient (CG) method, an algorithm designed for symmetric positive definite (SPD) systems, to a linear system that is nonsymmetric. We will first establish the theoretical basis for the CG method and its reliance on symmetry, and then analyze how a nonsymmetric perturbation affects its performance.\n\nThe Conjugate Gradient method is an iterative algorithm for solving linear systems of the form $Mx = b$, where the matrix $M \\in \\mathbb{R}^{n \\times n}$ is symmetric and positive definite. Its efficacy stems from its connection to an optimization problem. For an SPD matrix $M$, solving $Mx = b$ is equivalent to finding the unique minimizer of the strictly convex quadratic functional $\\phi(x) = \\frac{1}{2}x^{\\top} M x - x^{\\top} b$. The gradient of this functional is $\\nabla\\phi(x) = Mx - b$, which is the residual of the linear system, $r(x) = b - Mx$. CG generates a sequence of iterates $x^{(k)}$ such that $\\phi(x^{(k)})$ is progressively minimized.\n\nThe key properties of the CG method which guarantee its convergence in at most $n$ iterations (in exact arithmetic) are a direct consequence of the symmetry of $M$:\n1.  **A-conjugacy of search directions**: The search directions $\\{p^{(0)}, p^{(1)}, \\dots, p^{(n-1)}\\}$ satisfy $p^{(k)\\top} M p^{(j)} = 0$ for all $k \\neq j$. This ensures that minimizing $\\phi(x)$ along a new direction $p^{(k)}$ does not spoil the minimization achieved in previous directions.\n2.  **Orthogonality of residuals**: The residuals $\\{r^{(0)}, r^{(1)}, \\dots, r^{(n-1)}\\}$ are mutually orthogonal, satisfying $r^{(k)\\top} r^{(j)} = 0$ for all $k \\neq j$.\n\nWhen the matrix is nonsymmetric, these foundational properties are lost. The functional $\\phi(x)$ may no longer have a unique minimum corresponding to the solution, and the orthogonality and conjugacy relations that drive the algorithm's efficiency break down. Applying CG to a nonsymmetric system is therefore a modeling error, as the algorithm is used outside its domain of theoretical validity. This can lead to erratic convergence, stagnation, or outright divergence.\n\nIn this problem, we construct a matrix $\\tilde{A} = A + \\epsilon B$. The matrix $A$ is the $n \\times n$ tridiagonal matrix with $2$ on the main diagonal and $-1$ on the sub- and super-diagonals. This matrix is a well-known example of an SPD matrix, arising from the finite difference discretization of the second derivative operator. The matrix $B$ is a strictly upper triangular matrix with all entries above the diagonal equal to $1$, i.e., $B_{ij} = 1$ if $j  i$ and $0$ otherwise. The matrix $\\tilde{A}$ is therefore defined as:\n$$\n\\tilde{A}_{ij} =\n\\begin{cases}\n2  \\text{if } i = j \\\\\n-1  \\text{if } |i - j| = 1 \\\\\n\\epsilon  \\text{if } j  i \\text{ and } |i-j|  1 \\\\\n0  \\text{otherwise}\n\\end{cases}\n$$\nFor any $\\epsilon \\neq 0$, the matrix $\\tilde{A}$ is nonsymmetric because $A$ is symmetric but $B$ is not, so $\\tilde{A}^{\\top} = A^{\\top} + \\epsilon B^{\\top} = A + \\epsilon B^{\\top} \\neq \\tilde{A}$. The magnitude of the scalar $\\epsilon$ controls the degree of nonsymmetry. The right-hand side vector is $b_i = 1$ for all $i$.\n\nThe standard CG algorithm to be implemented is as follows:\n1.  Initialize $x^{(0)} = 0$, $r^{(0)} = b - \\tilde{A}x^{(0)} = b$, $p^{(0)} = r^{(0)}$. Let $\\rho_0 = r^{(0)\\top} r^{(0)}$.\n2.  For $k = 0, 1, 2, \\dots, n-1$:\n    a. Compute $v^{(k)} = \\tilde{A} p^{(k)}$.\n    b. The step size is $\\alpha_k = \\frac{\\rho_k}{p^{(k)\\top} v^{(k)}}$.\n    c. Update the solution: $x^{(k+1)} = x^{(k)} + \\alpha_k p^{(k)}$.\n    d. Update the residual: $r^{(k+1)} = r^{(k)} - \\alpha_k v^{(k)}$.\n    e. Check convergence: if the relative residual norm $\\|r^{(k+1)}\\|_2 / \\|b\\|_2$ is less than the tolerance $10^{-10}$, the algorithm has converged.\n    f. Compute $\\rho_{k+1} = r^{(k+1)\\top} r^{(k+1)}$.\n    g. Update the search direction: $p^{(k+1)} = r^{(k+1)} + \\frac{\\rho_{k+1}}{\\rho_k} p^{(k)}$. Update $\\rho_k \\leftarrow \\rho_{k+1}$.\n\nWe will execute this algorithm for four test cases with increasing $\\epsilon$ and compare the resulting solution $x_{\\mathrm{CG}}$ to the true solution $x^{\\ast}$ obtained from a direct solver.\n\nCase 1: $\\epsilon = 0$. Here, $\\tilde{A} = A$, which is SPD. The CG method is perfectly suited for this system. We expect rapid convergence to a highly accurate solution well within the $n=50$ iteration limit.\n\nCase 2: $\\epsilon = 10^{-3}$. The matrix $\\tilde{A}$ is now slightly nonsymmetric. The assumptions of CG are violated, but only mildly. The method's performance is expected to degrade slightly, potentially requiring more iterations than the SPD case, but it is still likely to converge to the specified tolerance. The final solution error will be small but likely larger than in Case 1.\n\nCase 3: $\\epsilon = 10^{-1}$. The nonsymmetric perturbation is significant. The loss of the underlying orthogonality and conjugacy properties will be more pronounced. The convergence of the residual norm may become non-monotonic. It is uncertain whether the method will converge within $n=50$ iterations. The accuracy of the solution, if convergence is declared, is expected to be substantially worse.\n\nCase 4: $\\epsilon = 5 \\cdot 10^{-1}$. With such a large nonsymmetric part, the behavior of CG is expected to be highly erratic. The denominator of $\\alpha_k$, $p^{(k)\\top} \\tilde{A} p^{(k)}$, is no longer guaranteed to be positive and could become zero or negative, leading to a breakdown. It is highly probable that the method will fail to meet the convergence criterion within the maximum number of iterations. The final iterate $x^{(n)}$ will likely have a large error relative to the true solution $x^{\\ast}$.\n\nThis numerical experiment will demonstrate how a modeling error—the misapplication of an algorithm outside its valid theoretical domain—manifests as a quantifiable degradation in performance and accuracy.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test cases and prints the formatted output.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (50, 0.0),          # Case 1: SPD matrix\n        (50, 1e-3),         # Case 2: Small nonsymmetry\n        (50, 1e-1),         # Case 3: Moderate nonsymmetry\n        (50, 5e-1),         # Case 4: Significant nonsymmetry\n    ]\n\n    results = []\n\n    for n, epsilon in test_cases:\n        # Construct the matrices A and B\n        A = np.diag(2 * np.ones(n)) - np.diag(np.ones(n - 1), 1) - np.diag(np.ones(n - 1), -1)\n        B = np.triu(np.ones((n, n)), k=1)\n        \n        # Construct the perturbed matrix tilde_A\n        tilde_A = A + epsilon * B\n        \n        # Construct the right-hand side vector b\n        b = np.ones(n)\n        norm_b = np.linalg.norm(b)\n        \n        # Compute the true solution x* using a direct solver\n        try:\n            x_star = np.linalg.solve(tilde_A, b)\n        except np.linalg.LinAlgError:\n            # Handle cases where tilde_A might be singular for large epsilon\n            results.extend([float('inf'), 'false'])\n            continue\n\n        # Run the Conjugate Gradient (CG) method\n        x_cg = np.zeros(n)\n        r = b - tilde_A @ x_cg\n        p = r.copy()\n        rs_old_sq = r @ r\n        \n        max_iter = n\n        tol = 1e-10\n        converged = False\n\n        for k in range(max_iter):\n            Ap = tilde_A @ p\n            \n            p_dot_Ap = p @ Ap\n            # Breakdown condition if p_dot_Ap is zero or negative\n            if p_dot_Ap = 0:\n                break\n\n            alpha = rs_old_sq / p_dot_Ap\n            x_cg = x_cg + alpha * p\n            r = r - alpha * Ap\n            \n            rel_res_norm = np.linalg.norm(r) / norm_b\n            if rel_res_norm  tol:\n                converged = True\n                break\n            \n            rs_new_sq = r @ r\n            beta = rs_new_sq / rs_old_sq\n            p = r + beta * p\n            rs_old_sq = rs_new_sq\n\n        # Measure the relative solution error\n        norm_x_star = np.linalg.norm(x_star)\n        if norm_x_star == 0:\n            # Handle the case of a zero true solution\n            rel_err = np.linalg.norm(x_cg)\n        else:\n            rel_err = np.linalg.norm(x_cg - x_star) / norm_x_star\n            \n        results.append(rel_err)\n        # Append boolean converted to lowercase string\n        results.append(str(converged).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond being a powerful solver, the Conjugate Gradient method can also serve as a diagnostic tool. The rate at which the residual norm decreases during the iteration process is not arbitrary; it is intimately linked to the spectral properties of the system matrix, most notably its condition number $\\kappa(A)$. This practice guides you to harness this relationship, challenging you to analyze the convergence history of a CG solve to estimate the condition number of the underlying heat conduction problem, turning an observable performance metric into a quantitative measure of problem difficulty .",
            "id": "3942614",
            "problem": "Consider the steady-state heat conduction equation with homogeneous Dirichlet boundary conditions on a bounded domain, which after second-order central finite difference discretization yields a linear system of the form $A x = b$, where $A$ is Symmetric Positive Definite (SPD). The Conjugate Gradient (CG) method applied to such systems produces a sequence of residuals $\\{r_k\\}$ with $r_k = b - A x_k$. Design an experiment that estimates the spectral condition number $\\kappa(A)$ directly from the recorded residual history of CG without access to the eigenvalues of $A$, and empirically validate the estimator.\n\nYour program must implement the following steps:\n\n1. Construct SPD matrices $A$ that model the discrete operator of heat conduction:\n   a) One-dimensional rod on the interval $[0,1]$ with constant thermal conductivity, discretized with $N$ interior points. The resulting matrix is tridiagonal with main diagonal entries equal to $2/h^2$ and off-diagonals equal to $-1/h^2$, where $h = 1/(N+1)$.\n   b) Two-dimensional square plate on $[0,1] \\times [0,1]$ with possibly anisotropic thermal conductivity, discretized on a uniform grid with $N_x$ points in the $x$-direction and $N_y$ points in the $y$-direction. The resulting matrix uses a five-point stencil with coefficients $2 k_x/h_x^2 + 2 k_y/h_y^2$ on the main diagonal and $-k_x/h_x^2$ for nearest neighbors in the $x$-direction and $-k_y/h_y^2$ for nearest neighbors in the $y$-direction, where $h_x = 1/(N_x+1)$ and $h_y = 1/(N_y+1)$, and $k_x$ and $k_y$ denote the conductivities along the $x$- and $y$-axes, respectively.\n\n2. For each constructed SPD matrix $A$, set the right-hand side vector $b$ to be the vector of all ones of appropriate length and initialize $x_0$ to the zero vector. Apply the Conjugate Gradient method to solve $A x = b$, recording the residual norm $\\|r_k\\|_2$ at each iteration $k$ until convergence. Use a stopping criterion based on the relative residual, that is, stop when $\\|r_k\\|_2 / \\|b\\|_2 \\leq \\varepsilon$ with $\\varepsilon = 10^{-12}$, or when a maximum number of iterations is reached.\n\n3. From the recorded residual norms $\\{\\|r_k\\|_2\\}$, construct an estimator for the spectral condition number $\\kappa(A)$ using only the residual history. The estimator must be based on identifying an asymptotic linear rate in the semilog plot of the residual norms versus iteration number and mapping this rate to an estimate of $\\kappa(A)$. The design must be scientifically justified based on properties of CG applied to SPD systems, not empirical curve-fitting disconnected from theory.\n\n4. To validate the estimator, compute the reference condition number $\\kappa_{\\text{true}}(A)$ using extremal eigenvalues obtained numerically for each test matrix. Compare the estimate $\\kappa_{\\text{est}}(A)$ to $\\kappa_{\\text{true}}(A)$ via the relative error $\\left|\\kappa_{\\text{est}}(A) - \\kappa_{\\text{true}}(A)\\right| / \\kappa_{\\text{true}}(A)$.\n\nUse the following test suite to ensure coverage:\n- Test case $1$: One-dimensional rod with $N = 50$.\n- Test case $2$: One-dimensional rod with $N = 200$.\n- Test case $3$: Two-dimensional plate with anisotropic conductivity, $k_x = 1$, $k_y = 10$, $N_x = 20$, $N_y = 20$.\n- Test case $4$: Two-dimensional isotropic plate, $k_x = 1$, $k_y = 1$, $N_x = 8$, $N_y = 8$.\n\nFor all tests, treat lengths and conductivities as nondimensionalized; no physical unit conversion is required.\n\nYour program should produce a single line of output containing a comma-separated list enclosed in square brackets. For each test case, output a boolean that is true if the relative error $\\left|\\kappa_{\\text{est}}(A) - \\kappa_{\\text{true}}(A)\\right| / \\kappa_{\\text{true}}(A)$ is less than or equal to $0.50$ and false otherwise, in the order of the test suite. For example, the output format should be exactly like $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4]$.",
            "solution": "The problem requires the design and validation of an experiment to estimate the spectral condition number, $\\kappa(A)$, of a Symmetric Positive Definite (SPD) matrix $A$ using only the convergence history of the Conjugate Gradient (CG) method. The matrices $A$ arise from the finite difference discretization of the steady-state heat conduction equation.\n\n**1. Theoretical Foundation for the Estimator**\n\nThe Conjugate Gradient method is an iterative algorithm for solving linear systems $A x = b$ where $A$ is an $n \\times n$ SPD matrix. The method generates a sequence of approximations $x_k$ that minimizes the A-norm of the error, $\\|x - x_k\\|_A = \\sqrt{(x - x_k)^{\\top} A (x - x_k)}$, over an expanding Krylov subspace. The convergence rate of the CG method is bounded in terms of the spectral condition number $\\kappa(A) = \\lambda_{\\max}(A) / \\lambda_{\\min}(A)$, where $\\lambda_{\\max}(A)$ and $\\lambda_{\\min}(A)$ are the maximum and minimum eigenvalues of $A$, respectively.\n\nA standard result in numerical linear algebra provides the following upper bound on the A-norm of the error $e_k = x_* - x_k$, where $x_*$ is the true solution:\n$$\n\\|e_k\\|_A \\le 2 \\left( \\frac{\\sqrt{\\kappa(A)} - 1}{\\sqrt{\\kappa(A)} + 1} \\right)^k \\|e_0\\|_A\n$$\nThis inequality demonstrates that the error is expected to decrease, at least asymptotically, by a factor of $\\rho$ at each iteration, where $\\rho$ is the geometric convergence factor:\n$$\n\\rho = \\frac{\\sqrt{\\kappa(A)} - 1}{\\sqrt{\\kappa(A)} + 1}\n$$\nWhile this bound is on the A-norm of the error, the $L_2$-norm of the residual, $\\|r_k\\|_2 = \\|b - A x_k\\|_2 = \\|A e_k\\|_2$, can be shown to exhibit the same asymptotic convergence rate. Therefore, for large values of the iteration count $k$, we expect the following approximation to hold:\n$$\n\\|r_k\\|_2 \\approx C \\cdot \\rho^k\n$$\nfor some constant $C$ that depends on the initial residual and the eigenvector decomposition of the error.\n\n**2. Estimator Design**\n\nThe relationship $\\|r_k\\|_2 \\approx C \\cdot \\rho^k$ forms the basis of our estimator for $\\kappa(A)$. By taking the natural logarithm of both sides, we obtain a linear relationship between $\\log(\\|r_k\\|_2)$ and the iteration number $k$:\n$$\n\\log(\\|r_k\\|_2) \\approx \\log(C) + k \\log(\\rho)\n$$\nThis equation suggests that a semi-logarithmic plot of the residual norm versus the iteration number will be approximately linear in the asymptotic regime, with a slope $S = \\log(\\rho)$.\n\nThe experimental procedure to estimate $\\kappa(A)$ is as follows:\n1.  Apply the CG algorithm to the system $A x = b$ and record the sequence of residual norms $\\{\\|r_k\\|_2\\}$ at each iteration $k = 0, 1, 2, \\dots$.\n2.  Select a portion of the residual history that represents the asymptotic convergence behavior. A common heuristic is to use the latter half of the recorded iterations before convergence is achieved. Let this sequence of iterations be $k \\in \\{k_{\\text{start}}, \\dots, k_{\\text{end}}\\}$.\n3.  Perform a linear regression (least-squares fit) on the points $(k, \\log(\\|r_k\\|_2))$ for this selected range to obtain an estimate of the slope, $\\hat{S}$.\n4.  Having estimated the slope $\\hat{S} \\approx \\log(\\rho)$, we can solve for $\\rho$:\n    $$\n    \\hat{\\rho} = e^{\\hat{S}}\n    $$\n5.  Finally, we invert the relationship between $\\rho$ and $\\kappa(A)$ to find our estimate, $\\kappa_{\\text{est}}(A)$:\n    $$\n    \\hat{\\rho} = \\frac{\\sqrt{\\kappa_{\\text{est}}(A)} - 1}{\\sqrt{\\kappa_{\\text{est}}(A)} + 1} \\implies \\sqrt{\\kappa_{\\text{est}}(A)} = \\frac{1 + \\hat{\\rho}}{1 - \\hat{\\rho}}\n    $$\n    $$\n    \\kappa_{\\text{est}}(A) = \\left( \\frac{1 + \\hat{\\rho}}{1 - \\hat{\\rho}} \\right)^2 = \\left( \\frac{1 + e^{\\hat{S}}}{1 - e^{\\hat{S}}} \\right)^2\n    $$\nThis provides a method to estimate $\\kappa(A)$ directly from the observable output of the CG algorithm, without any knowledge of the matrix $A$'s eigenvalues.\n\n**3. Algorithmic and Validation Procedure**\n\nThe experiment is implemented through the following computational steps:\n\n-   **Matrix Construction**: The SPD matrices $A$ are constructed as sparse matrices using `scipy.sparse`.\n    -   For the $1$D case with $N$ interior points and step size $h = 1/(N+1)$, the matrix is a tridiagonal matrix of size $N \\times N$ with diagonal entries $2/h^2$ and off-diagonal entries $-1/h^2$.\n    -   For the $2$D case on an $N_x \\times N_y$ grid, the matrix of size $(N_xN_y) \\times (N_xN_y)$ is constructed using the Kronecker product sum of two $1$D Laplacian matrices. Let $T_M$ be the $M \\times M$ tridiagonal matrix with $2$s on the diagonal and $-1$s on the off-diagonals, and let $I_M$ be the identity matrix of size $M$. The $2$D operator matrix is $A = \\frac{k_x}{h_x^2} (T_{N_x} \\otimes I_{N_y}) + \\frac{k_y}{h_y^2} (I_{N_x} \\otimes T_{N_y})$.This construction correctly models the five-point stencil on a grid with lexicographical ordering of nodes.\n\n-   **Conjugate Gradient Implementation**: A custom implementation of the CG algorithm is required to record the $L_2$-norm of the residual $r_k$ at every iteration $k$. The algorithm starts with $x_0 = 0$ (the zero vector) and $b$ as the vector of all ones. It terminates when the relative residual norm $\\|r_k\\|_2 / \\|b\\|_2$ falls below a tolerance $\\varepsilon = 10^{-12}$ or a maximum number of iterations is reached.\n\n-   **Estimator Implementation**: The $\\kappa_{\\text{est}}(A)$ is calculated as described in Section 2. The linear regression to find the slope $\\hat{S}$ is performed using `numpy.polyfit` on the natural logarithm of the residual norms from the second half of the iterations generated by the CG run.\n\n-   **Validation**: To validate the estimator, the \"true\" condition number, $\\kappa_{\\text{true}}(A)$, is computed for each test matrix. This is done by numerically finding the extremal eigenvalues using `scipy.sparse.linalg.eigs`, which is a sparse eigensolver. Specifically, we find $\\lambda_{\\min}(A)$ and $\\lambda_{\\max}(A)$ and compute $\\kappa_{\\text{true}}(A) = \\lambda_{\\max}(A) / \\lambda_{\\min}(A)$. The performance of the estimator is then quantified by the relative error:\n    $$\n    \\text{Error}_{\\text{rel}} = \\frac{|\\kappa_{\\text{est}}(A) - \\kappa_{\\text{true}}(A)|}{\\kappa_{\\text{true}}(A)}\n    $$\n    The final output for each test case is a boolean value, which is `True` if this relative error is less than or equal to $0.50$, and `False` otherwise.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import sparse\nfrom scipy.sparse import linalg as sla\n\ndef build_1d_matrix(N: int):\n    \"\"\"Constructs the SPD matrix for a 1D rod.\"\"\"\n    h = 1.0 / (N + 1)\n    diag_val = 2.0 / h**2\n    off_diag_val = -1.0 / h**2\n    diagonals = [[off_diag_val] * (N - 1), [diag_val] * N, [off_diag_val] * (N - 1)]\n    return sparse.diags(diagonals, [-1, 0, 1], format='csr')\n\ndef build_2d_matrix(Nx: int, Ny: int, kx: float, ky: float):\n    \"\"\"Constructs the SPD matrix for a 2D plate using Kronecker products.\"\"\"\n    hx = 1.0 / (Nx + 1)\n    hy = 1.0 / (Ny + 1)\n\n    diag_x = sparse.diags([-1, 2, -1], [-1, 0, 1], shape=(Nx, Nx))\n    diag_y = sparse.diags([-1, 2, -1], [-1, 0, 1], shape=(Ny, Ny))\n    \n    Ix = sparse.identity(Nx)\n    Iy = sparse.identity(Ny)\n    \n    Ax = sparse.kron(diag_x, Iy) * (kx / hx**2)\n    Ay = sparse.kron(Ix, diag_y) * (ky / hy**2)\n    \n    return (Ax + Ay).tocsr()\n\ndef conjugate_gradient(A, b, tol=1e-12, max_iter=5000):\n    \"\"\"\n    Solves Ax=b using the Conjugate Gradient method, recording residual norms.\n    \"\"\"\n    n = A.shape[0]\n    x = np.zeros(n)\n    r = b - A.dot(x)\n    p = r.copy()\n    rs_old = np.dot(r, r)\n    \n    b_norm = np.linalg.norm(b)\n    if b_norm == 0:\n        return x, []\n        \n    residual_norms = [np.sqrt(rs_old)]\n    \n    for k in range(max_iter):\n        Ap = A.dot(p)\n        alpha = rs_old / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rs_new = np.dot(r, r)\n        \n        norm_r = np.sqrt(rs_new)\n        residual_norms.append(norm_r)\n        \n        if norm_r / b_norm  tol:\n            break\n            \n        p = r + (rs_new / rs_old) * p\n        rs_old = rs_new\n        \n    return x, residual_norms\n\ndef estimate_kappa(residual_norms):\n    \"\"\"\n    Estimates the condition number from the CG residual history.\n    \"\"\"\n    # Use the second half of the iterations for linear regression, as it represents\n    # the asymptotic convergence behavior.\n    num_iterations = len(residual_norms)\n    if num_iterations  4:\n        # Not enough data for a meaningful fit\n        return np.nan\n        \n    fit_start_index = num_iterations // 2\n    \n    iters = np.arange(fit_start_index, num_iterations)\n    log_residuals = np.log(residual_norms[fit_start_index:])\n    \n    # Perform linear regression to find the slope\n    # polyfit returns [slope, intercept]\n    slope, _ = np.polyfit(iters, log_residuals, 1)\n    \n    if slope >= 0:\n        # Should be a negative slope for a converging method\n        return np.nan\n\n    # Calculate kappa from the slope\n    # S = log(rho) => rho = exp(S)\n    # kappa_est = ((1 + rho) / (1 - rho))^2\n    rho_est = np.exp(slope)\n    kappa_est = ((1 + rho_est) / (1 - rho_est))**2\n    \n    return kappa_est\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        {'type': '1D', 'N': 50},\n        {'type': '1D', 'N': 200},\n        {'type': '2D', 'Nx': 20, 'Ny': 20, 'kx': 1.0, 'ky': 10.0},\n        {'type': '2D', 'Nx': 8, 'Ny': 8, 'kx': 1.0, 'ky': 1.0},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        if case['type'] == '1D':\n            A = build_1d_matrix(case['N'])\n            matrix_size = case['N']\n        else: # 2D\n            A = build_2d_matrix(case['Nx'], case['Ny'], case['kx'], case['ky'])\n            matrix_size = case['Nx'] * case['Ny']\n\n        # Compute true condition number using numerical eigenvalues\n        # Since A is SPD, eigenvalues are real and positive.\n        # k=1 returns one eigenvalue, which='LM' for largest magnitude, 'SM' for smallest.\n        try:\n            lambda_max = sla.eigs(A, k=1, which='LM', return_eigenvectors=False)[0].real\n            lambda_min = sla.eigs(A, k=1, which='SM', return_eigenvectors=False)[0].real\n            kappa_true = lambda_max / lambda_min\n        except sla.ArpackNoConvergence:\n            # Handle cases where eigensolver fails, although unlikely for these matrices\n            results.append(False) # Mark as failed test\n            continue\n\n        # Set up and run Conjugate Gradient\n        b = np.ones(matrix_size)\n        x_sol, residual_history = conjugate_gradient(A, b, tol=1e-12, max_iter=10000)\n        \n        # Estimate condition number from residual history\n        kappa_est = estimate_kappa(residual_history)\n\n        if np.isnan(kappa_est) or kappa_true == 0:\n            results.append(False)\n            continue\n            \n        # Validate the estimator\n        relative_error = np.abs(kappa_est - kappa_true) / kappa_true\n        \n        results.append(relative_error = 0.50)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(r).lower() for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}