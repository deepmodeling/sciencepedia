{
    "hands_on_practices": [
        {
            "introduction": "Before writing complex code, it is essential to grasp the analytical underpinnings of convergence. This exercise challenges you to derive the exact spectral radius of the Jacobi iteration matrix for the 1D Poisson problem, a cornerstone of numerical analysis. By connecting the discretization size directly to the convergence rate, you will gain a fundamental understanding of how problem parameters influence iterative performance. ",
            "id": "3986575",
            "problem": "A straight fin with constant cross-section is modeled by the steady one-dimensional heat conduction equation with homogeneous Dirichlet boundary conditions, which reduces (after nondimensionalization) to the boundary value problem\n$$\n- \\frac{d^{2} u}{dx^{2}} = f(x), \\quad x \\in (0,1), \\qquad u(0)=0,\\; u(1)=0.\n$$\nDiscretize the interior of the interval with $n$ equally spaced interior points at $x_{j} = j h$ for $j=1,2,\\dots,n$ and $h = \\frac{1}{n+1}$. Using the standard second-order central difference, obtain the linear system $A \\mathbf{u} = \\mathbf{b}$, where $A$ is the tridiagonal matrix arising from the discrete negative second derivative with Dirichlet boundary conditions. Consider the Jacobi method applied to this system with the standard splitting $A = D - (L+U)$, and the Jacobi iteration matrix $B_{J} = I - D^{-1} A$.\n\nStarting from the discrete operator and the boundary conditions, derive the eigenpairs of $B_{J}$ explicitly by constructing an appropriate orthogonal basis of eigenvectors. Then determine the spectral radius $\\rho(B_{J})$ as a closed-form function of $n$.\n\nReport only the final closed-form analytic expression for $\\rho(B_{J})$. No numerical evaluation is required.",
            "solution": "The problem is valid. It is a standard, well-posed problem in numerical analysis concerning the spectral properties of an iteration matrix derived from a finite difference discretization of a boundary value problem. All information is self-contained and scientifically sound.\n\nWe begin by discretizing the given boundary value problem:\n$$\n- \\frac{d^{2} u}{dx^{2}} = f(x), \\quad x \\in (0,1), \\qquad u(0)=0,\\; u(1)=0.\n$$\nThe domain $(0,1)$ is divided into $n+1$ subintervals of equal width $h = \\frac{1}{n+1}$. The grid points are $x_j = jh$ for $j=0, 1, \\dots, n+1$. The values of the function $u(x)$ at the interior grid points are denoted by $u_j = u(x_j)$ for $j=1, 2, \\dots, n$. The boundary conditions imply $u_0 = 0$ and $u_{n+1} = 0$.\n\nThe second derivative at an interior point $x_j$ is approximated using a second-order central difference formula:\n$$\n\\frac{d^{2} u}{dx^{2}}\\Bigg|_{x=x_j} \\approx \\frac{u(x_{j+1}) - 2u(x_j) + u(x_{j-1})}{h^2} = \\frac{u_{j+1} - 2u_j + u_{j-1}}{h^2}.\n$$\nSubstituting this into the differential equation at each interior point $x_j$ gives a system of $n$ linear equations:\n$$\n- \\frac{u_{j+1} - 2u_j + u_{j-1}}{h^2} = f(x_j), \\quad j=1, 2, \\dots, n.\n$$\nRearranging the terms, we get:\n$$\n\\frac{1}{h^2}(-u_{j-1} + 2u_j - u_{j+1}) = f_j,\n$$\nwhere $f_j = f(x_j)$. We apply the boundary conditions: for $j=1$, $u_0=0$, and for $j=n$, $u_{n+1}=0$. The system of equations can be written in matrix form $A \\mathbf{u} = \\mathbf{b}$, where $\\mathbf{u} = [u_1, u_2, \\dots, u_n]^T$, $\\mathbf{b} = [f_1, f_2, \\dots, f_n]^T$, and $A$ is the $n \\times n$ matrix:\n$$\nA = \\frac{1}{h^2} \\begin{pmatrix}\n2  -1  0  \\cdots  0 \\\\\n-1  2  -1  \\ddots  \\vdots \\\\\n0  -1  \\ddots  \\ddots  0 \\\\\n\\vdots  \\ddots  \\ddots  2  -1 \\\\\n0  \\cdots  0  -1  2\n\\end{pmatrix}.\n$$\nFor the Jacobi method, the matrix $A$ is split as $A = D - (L+U)$, where $D$ is the diagonal part of $A$, and $-L$ and $-U$ are the strictly lower and upper triangular parts of $A$, respectively. The Jacobi iteration matrix is given by $B_J = D^{-1}(L+U) = I - D^{-1}A$.\n\nFrom the structure of $A$, its diagonal part is $D = \\frac{2}{h^2}I$, where $I$ is the $n \\times n$ identity matrix.\nThe inverse of $D$ is $D^{-1} = \\frac{h^2}{2}I$.\nSubstituting this into the expression for $B_J$:\n$$\nB_J = I - \\left(\\frac{h^2}{2}I\\right) A = I - \\frac{h^2}{2} \\left[ \\frac{1}{h^2} \\begin{pmatrix}\n2  -1   \\cdots \\\\\n-1  2  -1  \\\\\n \\ddots  \\ddots  \\ddots \\\\\n  -1  2\n\\end{pmatrix} \\right] = I - \\frac{1}{2} \\begin{pmatrix}\n2  -1   \\cdots \\\\\n-1  2  -1  \\\\\n \\ddots  \\ddots  \\ddots \\\\\n  -1  2\n\\end{pmatrix}.\n$$\nThis results in the Jacobi iteration matrix:\n$$\nB_J = \\begin{pmatrix}\n1    \\\\\n  1   \\\\\n   \\ddots  \\\\\n    1\n\\end{pmatrix} - \\begin{pmatrix}\n1  -1/2   \\\\\n-1/2  1  -1/2  \\\\\n \\ddots  \\ddots  \\ddots \\\\\n  -1/2  1\n\\end{pmatrix} = \\begin{pmatrix}\n0  1/2  0  \\cdots  0 \\\\\n1/2  0  1/2  \\ddots  \\vdots \\\\\n0  1/2  \\ddots  \\ddots  0 \\\\\n\\vdots  \\ddots  \\ddots  0  1/2 \\\\\n0  \\cdots  0  1/2  0\n\\end{pmatrix}.\n$$\nTo find the spectral radius $\\rho(B_J)$, we need to find the eigenvalues of $B_J$. The problem requires us to do this by first constructing an orthogonal basis of eigenvectors. The eigenvectors of $B_J$ are the same as the eigenvectors of $A$, since $B_J$ is a linear polynomial in $A$ and $D$ is a scalar multiple of the identity.\n\nLet's find the eigenpairs of the tridiagonal matrix $T$ where $A = \\frac{1}{h^2}T$ and $T$ has diagonal entries $2$ and off-diagonal entries $-1$. Let an eigenvector of $T$ be $\\mathbf{v}$ with eigenvalue $\\mu$. The $j$-th component of the eigenvalue problem $T\\mathbf{v} = \\mu \\mathbf{v}$ is:\n$$\n-v_{j-1} + 2v_j - v_{j+1} = \\mu v_j,\n$$\nwith boundary conditions $v_0 = 0$ and $v_{n+1} = 0$. This is a linear second-order difference equation. We propose a solution of the form $v_j = \\sin(j\\theta)$ for some angle $\\theta$. The boundary condition $v_0=0$ is automatically satisfied. The condition $v_{n+1}=0$ requires $\\sin((n+1)\\theta) = 0$, which implies $(n+1)\\theta = k\\pi$ for some integer $k$. Thus, we have a family of candidate solutions indexed by $k$:\n$$\n\\theta_k = \\frac{k\\pi}{n+1}, \\quad k=1, 2, \\dots, n.\n$$\nThe corresponding vectors $\\mathbf{v}^{(k)}$ have components $v_j^{(k)} = \\sin\\left(\\frac{jk\\pi}{n+1}\\right)$. These form a set of $n$ linearly independent vectors, and they constitute an orthogonal basis for $\\mathbb{R}^n$.\n\nLet's find the eigenvalue $\\mu_k$ corresponding to each eigenvector $\\mathbf{v}^{(k)}$. Substituting $v_j^{(k)}$ into the difference equation:\n$$\n-\\sin\\left(\\frac{(j-1)k\\pi}{n+1}\\right) + 2\\sin\\left(\\frac{jk\\pi}{n+1}\\right) - \\sin\\left(\\frac{(j+1)k\\pi}{n+1}\\right) = \\mu_k \\sin\\left(\\frac{jk\\pi}{n+1}\\right).\n$$\nUsing the trigonometric identity $\\sin(a-b)+\\sin(a+b) = 2\\sin(a)\\cos(b)$, we can simplify the left-hand side:\n$$\n2\\sin\\left(\\frac{jk\\pi}{n+1}\\right) - \\left[ \\sin\\left(\\frac{(j-1)k\\pi}{n+1}\\right) + \\sin\\left(\\frac{(j+1)k\\pi}{n+1}\\right) \\right] = 2\\sin\\left(\\frac{jk\\pi}{n+1}\\right) - 2\\sin\\left(\\frac{jk\\pi}{n+1}\\right)\\cos\\left(\\frac{k\\pi}{n+1}\\right).\n$$\nFactoring out $\\sin\\left(\\frac{jk\\pi}{n+1}\\right)$, we get:\n$$\n\\left(2 - 2\\cos\\left(\\frac{k\\pi}{n+1}\\right)\\right) \\sin\\left(\\frac{jk\\pi}{n+1}\\right) = \\mu_k \\sin\\left(\\frac{jk\\pi}{n+1}\\right).\n$$\nThus, the eigenvalues of $T$ are $\\mu_k = 2 - 2\\cos\\left(\\frac{k\\pi}{n+1}\\right)$ for $k=1, \\dots, n$.\n\nThe matrices $A$, $T$, and $B_J$ all share the same eigenvectors $\\mathbf{v}^{(k)}$. Let $\\lambda_k(A)$ be the eigenvalues of $A$ and $\\lambda_k(B_J)$ be the eigenvalues of $B_J$.\n$$\n\\lambda_k(A) = \\frac{1}{h^2}\\mu_k = \\frac{2}{h^2}\\left(1 - \\cos\\left(\\frac{k\\pi}{n+1}\\right)\\right).\n$$\nThe relationship between the eigenvalues of $B_J$ and $A$ is $\\lambda(B_J) = 1 - (\\frac{h^2}{2})\\lambda(A)$. Therefore,\n$$\n\\lambda_k(B_J) = 1 - \\frac{h^2}{2} \\lambda_k(A) = 1 - \\frac{h^2}{2} \\left[ \\frac{2}{h^2}\\left(1 - \\cos\\left(\\frac{k\\pi}{n+1}\\right)\\right) \\right].\n$$\n$$\n\\lambda_k(B_J) = 1 - \\left(1 - \\cos\\left(\\frac{k\\pi}{n+1}\\right)\\right) = \\cos\\left(\\frac{k\\pi}{n+1}\\right), \\quad k=1, 2, \\dots, n.\n$$\nThe eigenpairs of $B_J$ are $\\left( \\cos\\left(\\frac{k\\pi}{n+1}\\right), \\mathbf{v}^{(k)} \\right)$ where the $j$-th component of $\\mathbf{v}^{(k)}$ is $\\sin\\left(\\frac{jk\\pi}{n+1}\\right)$.\n\nThe spectral radius of $B_J$ is the maximum of the absolute values of its eigenvalues:\n$$\n\\rho(B_J) = \\max_{k \\in \\{1, \\dots, n\\}} |\\lambda_k(B_J)| = \\max_{k \\in \\{1, \\dots, n\\}} \\left| \\cos\\left(\\frac{k\\pi}{n+1}\\right) \\right|.\n$$\nThe argument of the cosine function, $\\frac{k\\pi}{n+1}$, ranges from $\\frac{\\pi}{n+1}$ (for $k=1$) to $\\frac{n\\pi}{n+1}$ (for $k=n$). This range is contained within the interval $(0, \\pi)$. The function $|\\cos(x)|$ on $(0, \\pi)$ is symmetric about $x=\\pi/2$ and its maximum value is attained at the endpoints of any closed interval $[a, b] \\subset (0, \\pi)$, i.e., at $x=a$ or $x=b$.\nIn our case, the set of arguments is discrete: $\\{\\frac{\\pi}{n+1}, \\frac{2\\pi}{n+1}, \\dots, \\frac{n\\pi}{n+1}\\}$. The value of $|\\cos(\\theta)|$ will be largest when $\\theta$ is closest to $0$ or $\\pi$.\nThe arguments closest to $0$ and $\\pi$ are for $k=1$ and $k=n$:\n$$\n\\text{For } k=1: \\quad \\left| \\cos\\left(\\frac{\\pi}{n+1}\\right) \\right| = \\cos\\left(\\frac{\\pi}{n+1}\\right) \\quad \\text{(since } \\frac{\\pi}{n+1} \\in (0, \\pi/2) \\text{ for } n \\ge 1).\n$$\n$$\n\\text{For } k=n: \\quad \\left| \\cos\\left(\\frac{n\\pi}{n+1}\\right) \\right| = \\left| \\cos\\left(\\pi - \\frac{\\pi}{n+1}\\right) \\right| = \\left| -\\cos\\left(\\frac{\\pi}{n+1}\\right) \\right| = \\cos\\left(\\frac{\\pi}{n+1}\\right).\n$$\nThe values of $|\\cos(\\frac{k\\pi}{n+1})|$ for $k=2, \\dots, n-1$ will be smaller because $\\frac{k\\pi}{n+1}$ will be further from $0$ and $\\pi$ than the endpoints.\nTherefore, the maximum absolute value is achieved at $k=1$ and $k=n$.\nThe spectral radius is:\n$$\n\\rho(B_J) = \\cos\\left(\\frac{\\pi}{n+1}\\right).\n$$",
            "answer": "$$\n\\boxed{\\cos\\left(\\frac{\\pi}{n+1}\\right)}\n$$"
        },
        {
            "introduction": "The performance of the Successive Over-Relaxation (SOR) method can be surprisingly sensitive to the ordering of unknowns, especially for nonsymmetric systems arising from convection. This practice asks you to implement a program to quantify this effect, demonstrating how an improper ordering can lead to divergence even when over-relaxation is expected to accelerate convergence. This is a critical lesson in the practical implementation of iterative solvers for transport phenomena. ",
            "id": "3986584",
            "problem": "Consider the steady, one-dimensional convection–diffusion energy balance under constant properties, leading by standard finite-volume discretization on a uniform mesh of $n$ nodes to a linear system $A \\mathbf{T} = \\mathbf{b}$ with a tridiagonal coefficient matrix $A$. For purely conductive problems (no convection), $A$ is symmetric positive definite, for example with diagonal entries $2$ and off-diagonal entries $-1$. For convection-dominated problems discretized with upwind differencing, $A$ remains strictly diagonally dominant with nonpositive off-diagonals and is generally nonsymmetric, where the magnitude of the upstream coupling (one off-diagonal) can be significantly larger than the downstream coupling (the other off-diagonal). Stationary iterative methods solve $A \\mathbf{T} = \\mathbf{b}$ by splitting $A = D - L - U$ with $D$ the diagonal, $L$ the strictly lower triangular part, and $U$ the strictly upper triangular part (all defined with respect to a given ordering of unknowns). The Successive Overrelaxation (SOR) method with relaxation parameter $\\omega$ uses the iteration\n$$\n\\mathbf{T}^{(k+1)} = (D - \\omega L)^{-1} \\left( (1 - \\omega) D + \\omega U \\right) \\mathbf{T}^{(k)} + (D - \\omega L)^{-1} \\omega \\mathbf{b}.\n$$\nThe associated homogeneous SOR iteration matrix is\n$$\nG_{\\omega} = (D - \\omega L)^{-1} \\left( (1 - \\omega) D + \\omega U \\right).\n$$\nConvergence of the SOR method for a given ordering and $\\omega$ is determined by the spectral radius $\\rho(G_{\\omega})$, i.e., the method converges if and only if $\\rho(G_{\\omega})  1$. The ordering of unknowns alters the decomposition of $A$ into $D$, $L$, and $U$ through permutation similarity: if $P$ is a permutation matrix representing a reordering, the reordered system is $\\tilde{A} = P A P^{\\mathsf{T}}$, and the split changes accordingly. In nonsymmetric cases, this can strongly affect $\\rho(G_{\\omega})$, potentially causing divergence of overrelaxed SOR under poor orderings.\n\nYour task is to quantify the sensitivity of SOR to ordering by computing $\\rho(G_{\\omega})$ under specified orderings and to construct a counterexample where a poor ordering leads to divergence for $\\omega  1$.\n\nStarting from the fundamental base of the discrete energy balance and the algebraic splitting $A = D - L - U$, derive how ordering maps to a permutation similarity of $A$ and thus to a modified $G_{\\omega}$. Implement a program that:\n\n- Accepts as fixed internal data the following test suite of matrices, orderings, and relaxation parameters.\n- For each test case, forms the reordered matrix $\\tilde{A} = P A P^{\\mathsf{T}}$ for the specified ordering, constructs $D$, $L$, and $U$ with respect to that ordering, builds the SOR iteration matrix $G_{\\omega}$, evaluates $\\rho(G_{\\omega})$, and returns a boolean indicating convergence ($\\rho(G_{\\omega})  1$) or divergence ($\\rho(G_{\\omega}) \\ge 1$).\n\nUse the following test suite, designed to cover different facets including a symmetric positive definite \"happy path\", a nonsymmetric convection-dominated case, and boundary behavior, with explicitly stated parameters:\n\n- Test case $1$: Symmetric positive definite conduction, $n = 20$, tridiagonal with diagonal $2$ and off-diagonals $-1$; natural ordering; $\\omega = 1.8$.\n- Test case $2$: Same as test case $1$ but reverse ordering; $\\omega = 1.8$.\n- Test case $3$: Convection-dominated nonsymmetric system, $n = 20$, tridiagonal with diagonal $4.0$, lower off-diagonal $-0.05$, and upper off-diagonal $-3.9$; natural ordering; $\\omega = 1.2$.\n- Test case $4$: Same as test case $3$ but reverse ordering; $\\omega = 1.2$.\n- Test case $5$: Same as test case $3$ but reverse ordering; $\\omega = 1.05$.\n- Test case $6$: Same as test case $3$; natural ordering; underrelaxation with $\\omega = 0.9$.\n\nDefinitions:\n\n- Natural ordering is the identity permutation $P = I$ on indices $\\{1,2,\\dots,n\\}$.\n- Reverse ordering is the permutation that maps index $i$ to $n+1-i$.\n\nYour program should produce a single line of output containing the results for the six test cases as a comma-separated list enclosed in square brackets (e.g., \"[true,true,false,...]\"). Each entry must be a boolean indicating convergence (true) or divergence (false) for the corresponding test case, in the order listed above. No physical units are needed for these algebraic evaluations. Angles are not involved. Percentages are not used; answer entries are booleans only.",
            "solution": "The central task is to determine the convergence of the Successive Overrelaxation (SOR) iterative method for several linear systems derived from computational thermal engineering problems. The convergence of the SOR method is dictated by the spectral radius of its associated iteration matrix, $G_{\\omega}$. The method converges if and only if the spectral radius $\\rho(G_{\\omega})$ is strictly less than $1$.\n\nThe problem focuses on how the ordering of the unknowns in the linear system $A \\mathbf{T} = \\mathbf{b}$ affects this convergence, particularly for nonsymmetric matrices arising from convection-dominated phenomena. An ordering of the $n$ unknowns is represented by a permutation matrix $P$ of size $n \\times n$. Applying this ordering transforms the original system into an equivalent one:\n$$\n(P A P^{\\mathsf{T}}) (P \\mathbf{T}) = P \\mathbf{b}\n$$\nLet $\\tilde{A} = P A P^{\\mathsf{T}}$, $\\tilde{\\mathbf{T}} = P \\mathbf{T}$, and $\\tilde{\\mathbf{b}} = P \\mathbf{b}$. The SOR method is then applied to the reordered system $\\tilde{A} \\tilde{\\mathbf{T}} = \\tilde{\\mathbf{b}}$.\n\nThe SOR method is based on a splitting of the coefficient matrix. For the reordered matrix $\\tilde{A}$, the splitting is $\\tilde{A} = \\tilde{D} - \\tilde{L} - \\tilde{U}$, where:\n- $\\tilde{D}$ is the diagonal part of $\\tilde{A}$.\n- $-\\tilde{L}$ is the strictly lower triangular part of $\\tilde{A}$. Thus, $\\tilde{L}$ is the negative of the strictly lower triangular part of $\\tilde{A}$.\n- $-\\tilde{U}$ is the strictly upper triangular part of $\\tilde{A}$. Thus, $\\tilde{U}$ is the negative of the strictly upper triangular part of $\\tilde{A}$.\n\nThe SOR iteration for the reordered system is given by:\n$$\n\\tilde{\\mathbf{T}}^{(k+1)} = (\\tilde{D} - \\omega \\tilde{L})^{-1} \\left( (1 - \\omega) \\tilde{D} + \\omega \\tilde{U} \\right) \\tilde{\\mathbf{T}}^{(k)} + (\\tilde{D} - \\omega \\tilde{L})^{-1} \\omega \\tilde{\\mathbf{b}}\n$$\nwhere $\\omega$ is the relaxation parameter and $k$ is the iteration index. The convergence of this process depends on the spectral radius of the SOR iteration matrix for the reordered system, $\\tilde{G}_{\\omega}$, defined as:\n$$\n\\tilde{G}_{\\omega} = (\\tilde{D} - \\omega \\tilde{L})^{-1} \\left( (1 - \\omega) \\tilde{D} + \\omega \\tilde{U} \\right)\n$$\nThe convergence condition is $\\rho(\\tilde{G}_{\\omega})  1$.\n\nTo evaluate the convergence for each test case, we follow a systematic computational procedure:\n$1$. Construct the base matrix $A$ of size $n \\times n$ as specified. For a tridiagonal matrix with main diagonal $d$, lower diagonal $l$, and upper diagonal $u$, the entries are $A_{ii} = d$, $A_{i, i-1} = l$, and $A_{i, i+1} = u$.\n$2$. Construct the permutation matrix $P$ for the specified ordering.\n    - For natural ordering, $P$ is the identity matrix $I$.\n    - For reverse ordering, $P$ is the anti-diagonal matrix with entries $P_{i, n+1-i} = 1$ for $i=1, \\dots, n$ and all other entries equal to $0$.\n$3$. Compute the reordered matrix $\\tilde{A} = P A P^{\\mathsf{T}}$.\n$4$. Decompose $\\tilde{A}$ to obtain the matrices $\\tilde{D}$, $\\tilde{L}$, and $\\tilde{U}$. Specifically:\n    - $\\tilde{D}$ is a diagonal matrix containing the diagonal entries of $\\tilde{A}$.\n    - $\\tilde{L}$ is the negative of the strictly lower triangular part of $\\tilde{A}$.\n    - $\\tilde{U}$ is the negative of the strictly upper triangular part of $\\tilde{A}$.\n$5$. Assemble the SOR iteration matrix $\\tilde{G}_{\\omega}$ using the given value of $\\omega$.\n$6$. Compute the eigenvalues, $\\lambda_i$, of $\\tilde{G}_{\\omega}$.\n$7$. Calculate the spectral radius $\\rho(\\tilde{G}_{\\omega}) = \\max_i |\\lambda_i|$.\n$8$. Determine convergence by testing if $\\rho(\\tilde{G}_{\\omega})  1$. The result is a boolean value: `true` for convergence, `false` for divergence.\n\nThis procedure is applied to each of the six test cases.\n\n- For test cases $1$ and $2$, the matrix $A$ is symmetric positive definite (SPD). For such matrices, the SOR method is known to converge for any relaxation parameter $\\omega \\in (0, 2)$. Since the permutation similarity transform $P A P^{\\mathsf{T}}$ preserves the SPD property, convergence is expected for both natural and reverse orderings with $\\omega = 1.8$.\n\n- For test cases $3-6$, the matrix $A$ is nonsymmetric and strictly diagonally dominant, corresponding to a convection-dominated problem with upwind differencing. Such matrices are typically M-matrices, for which SOR convergence is guaranteed for any ordering if $\\omega \\in (0, 1]$. This predicts convergence for test case $6$ where $\\omega = 0.9$. For overrelaxation ($\\omega  1$), convergence is not guaranteed and becomes highly sensitive to the ordering of unknowns. The \"natural\" ordering ($1, 2, \\dots, n$) constitutes a downstream sweep relative to the direction of strong coupling (from high index to low index), which is known to be unstable for SOR with $\\omega  1$. Conversely, the \"reverse\" ordering ($n, n-1, \\dots, 1$) constitutes an upstream sweep, which aligns with the flow of information and typically results in stable and rapid convergence. This reasoning suggests divergence for test case $3$ ($\\omega = 1.2$, natural ordering) and convergence for test cases $4$ and $5$ ($\\omega = 1.2$ and $\\omega=1.05$, reverse ordering).\n\nThe final program implements this complete validation procedure for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Solves the problem by evaluating SOR convergence for a suite of test cases.\n    \"\"\"\n\n    def construct_tridiagonal_matrix(n, diag_val, lower_val, upper_val):\n        \"\"\"Constructs a tridiagonal matrix of size n x n.\"\"\"\n        diag = np.full(n, diag_val)\n        lower = np.full(n - 1, lower_val)\n        upper = np.full(n - 1, upper_val)\n        return np.diag(diag) + np.diag(lower, k=-1) + np.diag(upper, k=1)\n\n    def check_sor_convergence(A, ordering, omega):\n        \"\"\"\n        Checks the convergence of the SOR method for a given matrix, ordering, and omega.\n\n        Args:\n            A (np.ndarray): The base coefficient matrix.\n            ordering (str): The ordering of unknowns, 'natural' or 'reverse'.\n            omega (float): The relaxation parameter.\n\n        Returns:\n            bool: True if the method converges, False otherwise.\n        \"\"\"\n        n = A.shape[0]\n\n        # 1. Construct the permutation matrix P\n        if ordering == 'natural':\n            P = np.identity(n)\n        elif ordering == 'reverse':\n            P = np.fliplr(np.identity(n))\n        else:\n            raise ValueError(\"Unknown ordering type\")\n\n        # 2. Compute the reordered matrix A_tilde\n        A_tilde = P @ A @ P.T\n\n        # 3. Decompose A_tilde into D, L, U\n        # Note the convention: A = D - L - U\n        D_tilde = np.diag(np.diag(A_tilde))\n        L_tilde = -np.tril(A_tilde, k=-1)\n        U_tilde = -np.triu(A_tilde, k=1)\n\n        # 4. Construct the SOR iteration matrix G_omega\n        # G_omega = inv(D - omega*L) * ((1-omega)*D + omega*U)\n        # Check if D - omega*L is invertible\n        inv_term = D_tilde - omega * L_tilde\n        try:\n            np.linalg.inv(inv_term)\n        except np.linalg.LinAlgError:\n            # If the matrix is singular, spectral radius is effectively infinite\n            return False\n\n        G_omega = np.linalg.inv(inv_term) @ ((1 - omega) * D_tilde + omega * U_tilde)\n\n        # 5. Compute the spectral radius\n        eigenvalues = np.linalg.eigvals(G_omega)\n        spectral_radius = np.max(np.abs(eigenvalues))\n\n        # 6. Check for convergence\n        return spectral_radius  1.0\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'n': 20, 'diag': 2.0, 'lower': -1.0, 'upper': -1.0, 'ordering': 'natural', 'omega': 1.8}, # Case 1\n        {'n': 20, 'diag': 2.0, 'lower': -1.0, 'upper': -1.0, 'ordering': 'reverse', 'omega': 1.8}, # Case 2\n        {'n': 20, 'diag': 4.0, 'lower': -0.05, 'upper': -3.9, 'ordering': 'natural', 'omega': 1.2}, # Case 3\n        {'n': 20, 'diag': 4.0, 'lower': -0.05, 'upper': -3.9, 'ordering': 'reverse', 'omega': 1.2}, # Case 4\n        {'n': 20, 'diag': 4.0, 'lower': -0.05, 'upper': -3.9, 'ordering': 'reverse', 'omega': 1.05},# Case 5\n        {'n': 20, 'diag': 4.0, 'lower': -0.05, 'upper': -3.9, 'ordering': 'natural', 'omega': 0.9}  # Case 6\n    ]\n\n    results = []\n    for case in test_cases:\n        # Construct the base matrix A\n        A = construct_tridiagonal_matrix(case['n'], case['diag'], case['lower'], case['upper'])\n        \n        # Check convergence and store the result\n        converges = check_sor_convergence(A, case['ordering'], case['omega'])\n        results.append(str(converges).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In practice, no single method is optimal for all stages of a solution process. This advanced exercise guides you in designing a hybrid solver that leverages the parallelism of the Jacobi method in early stages and switches to the faster SOR method as the solution refines. You will implement an adaptive criterion to trigger this switch and use the observed Jacobi convergence rate to estimate an optimal relaxation parameter $\\omega$ for SOR. ",
            "id": "3986541",
            "problem": "Consider steady-state heat conduction in a square domain with homogeneous Dirichlet boundary conditions, governed by the partial differential equation $- \\nabla \\cdot ( \\mathbf{k} \\nabla T ) = f(x,y)$ where $\\mathbf{k} = \\mathrm{diag}(k_x,k_y)$ is a constant, positive-definite conductivity tensor with components $k_x  0$ and $k_y  0$. Using a uniform Cartesian grid with $n \\times n$ interior points, grid spacing $h = 1/(n+1)$, and the standard second-order central difference discretization, the interior-point finite difference equation can be written as\n$$\n2(k_x + k_y) \\, T_{i,j} - k_x \\left( T_{i+1,j} + T_{i-1,j} \\right) - k_y \\left( T_{i,j+1} + T_{i,j-1} \\right) = h^2 f_{i,j},\n$$\nfor $i,j \\in \\{1,2,\\dots,n\\}$, with $T=0$ along the boundary.\n\nYou must design and implement a hybrid stationary iterative solver that starts with the Jacobi method to exploit parallel update structure and then switches to Successive Over-Relaxation (SOR) when the observed residual reduction rate stabilizes. The residual at iteration $k$ is defined pointwise by\n$$\nr^{(k)}_{i,j} = 2(k_x + k_y) \\, T^{(k)}_{i,j} - k_x \\left( T^{(k)}_{i+1,j} + T^{(k)}_{i-1,j} \\right) - k_y \\left( T^{(k)}_{i,j+1} + T^{(k)}_{i,j-1} \\right) - h^2 f_{i,j},\n$$\nand the residual norm is $||r^{(k)}||_2 = \\left( \\sum_{i=1}^n \\sum_{j=1}^n \\left(r^{(k)}_{i,j}\\right)^2 \\right)^{1/2}$. The Jacobi update is given by\n$$\nT^{(k+1)}_{i,j} = \\frac{k_x \\left( T^{(k)}_{i+1,j} + T^{(k)}_{i-1,j} \\right) + k_y \\left( T^{(k)}_{i,j+1} + T^{(k)}_{i,j-1} \\right) + h^2 f_{i,j}}{2(k_x + k_y)},\n$$\nand the Successive Over-Relaxation (SOR) update with relaxation parameter $ \\omega \\in (0,2) $ is\n$$\nT^{(k+1)}_{i,j} = (1-\\omega) T^{(k)}_{i,j} + \\omega \\, \\frac{k_x \\left( T^{(k)}_{i+1,j} + T^{(k+1)}_{i-1,j} \\right) + k_y \\left( T^{(k)}_{i,j+1} + T^{(k+1)}_{i,j-1} \\right) + h^2 f_{i,j}}{2(k_x + k_y)},\n$$\nwhere the Gauss–Seidel ordering uses the most recent neighbor values.\n\nSwitching criterion: Let the ratio sequence $q_k = ||r^{(k)}||_2 / ||r^{(k-1)}||_2$ be computed during Jacobi iterations. After a minimum number $N_{\\min}$ of Jacobi iterations, compute the ratios over a sliding window of length $m$ and switch to SOR if the windowed ratios are stable and strictly contracting, interpreted as $ \\max(q_{k-m+1},\\dots,q_k) - \\min(q_{k-m+1},\\dots,q_k) \\le \\delta $ and $ \\frac{1}{m}\\sum_{j=k-m+1}^k q_j  1 $. At the switching time, estimate $\\hat{\\rho}_J$ as the mean of the windowed ratios and choose an over-relaxation parameter $\\omega$ using a scientifically justified function of $\\hat{\\rho}_J$. You must ensure $\\omega \\in (0,2)$ and document the justification in your solution.\n\nStopping criterion: Use the relative residual norm threshold $||r^{(k)}||_2 / ||r^{(0)}||_2 \\le \\tau$, starting from the zero initial guess $T^{(0)}_{i,j} = 0$.\n\nImplement the hybrid solver and apply it to the following three test cases. In each case, the domain is the unit square, the boundary condition is $T=0$, and the interior grid size is $n \\times n$.\n\n- Test Case A (general, isotropic, constant source):\n  - $n = 50$\n  - $k_x = 1$, $k_y = 1$\n  - $f(x,y) = 1$\n  - Relative tolerance $\\tau = 10^{-6}$\n  - Sliding window length $m = 20$\n  - Minimum Jacobi iterations $N_{\\min} = 40$\n  - Maximum Jacobi iterations $N_{\\max} = 100$\n  - Stability band $\\delta = 5 \\times 10^{-4}$\n\n- Test Case B (anisotropic, smooth source):\n  - $n = 60$\n  - $k_x = 4$, $k_y = 1$\n  - $f(x,y) = \\sin(\\pi x)\\sin(\\pi y)$\n  - Relative tolerance $\\tau = 5 \\times 10^{-7}$\n  - Sliding window length $m = 25$\n  - Minimum Jacobi iterations $N_{\\min} = 50$\n  - Maximum Jacobi iterations $N_{\\max} = 120$\n  - Stability band $\\delta = 8 \\times 10^{-4}$\n\n- Test Case C (small grid, high-frequency checkerboard source):\n  - $n = 8$\n  - $k_x = 1$, $k_y = 1$\n  - $f_{i,j} = (-1)^{i+j}$\n  - Relative tolerance $\\tau = 10^{-6}$\n  - Sliding window length $m = 10$\n  - Minimum Jacobi iterations $N_{\\min} = 15$\n  - Maximum Jacobi iterations $N_{\\max} = 40$\n  - Stability band $\\delta = 10^{-3}$\n\nOutput specification: For each test case, compute and return a list containing three quantities:\n- the total number of iterations (Jacobi iterations plus SOR iterations) needed to satisfy the stopping criterion (integer),\n- the indicator of whether a switch to Successive Over-Relaxation (SOR) occurred ($1$ if switched, $0$ otherwise),\n- the final relaxation parameter $\\omega$ used in SOR (float rounded to $5$ decimal places; if SOR was never used, report $\\omega = 1.0$).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case’s triplet enclosed in its own sub-brackets, for example $[ [a_1,b_1,c_1], [a_2,b_2,c_2], [a_3,b_3,c_3] ]$, where each $a_k$ is an integer, each $b_k$ is an integer in $\\{0,1\\}$, and each $c_k$ is a float rounded to $5$ decimal places.",
            "solution": "The user has provided a well-defined computational problem in the field of numerical methods for partial differential equations. The problem is scientifically sound, self-contained, and algorithmically precise. It is therefore deemed valid and a solution will be constructed.\n\n### **1. Problem Formulation**\n\nThe problem concerns the numerical solution of the steady-state heat conduction equation in a two-dimensional square domain:\n$$\n- \\nabla \\cdot ( \\mathbf{k} \\nabla T ) = f(x,y)\n$$\nwhere $\\mathbf{k} = \\mathrm{diag}(k_x, k_y)$ is a diagonal conductivity tensor with constant positive entries $k_x$ and $k_y$, $T(x,y)$ is the temperature field, and $f(x,y)$ is a heat source term. The boundary condition is homogeneous Dirichlet, i.e., $T=0$ on all boundaries of the square.\n\nDiscretizing this equation on a uniform Cartesian grid with $n \\times n$ interior points and grid spacing $h = 1/(n+1)$ using a second-order central difference scheme yields a system of linear algebraic equations. For each interior grid point $(i,j)$, where $i, j \\in \\{1, \\dots, n\\}$, the equation is:\n$$\n2(k_x + k_y) T_{i,j} - k_x ( T_{i+1,j} + T_{i-1,j} ) - k_y ( T_{i,j+1} + T_{i,j-1} ) = h^2 f_{i,j}\n$$\nThis system can be written in matrix form as $A\\mathbf{T} = \\mathbf{b}$, where $\\mathbf{T}$ is a vector of the unknown temperatures $T_{i,j}$. The matrix $A$ is large, sparse, and for $k_x, k_y  0$, it is symmetric positive-definite (SPD). This SPD property is crucial as it guarantees the convergence of the proposed iterative methods.\n\n### **2. Hybrid Iterative Solver Design**\n\nThe task is to implement a hybrid iterative solver that begins with the Jacobi method and adaptively switches to the Successive Over-Relaxation (SOR) method based on the convergence behavior.\n\n#### **2.1. Jacobi Method**\n\nThe Jacobi method is an iterative scheme where each component of the new iterate $T^{(k+1)}$ is computed using only components from the previous iterate $T^{(k)}$. The update rule for point $(i,j)$ is:\n$$\nT^{(k+1)}_{i,j} = \\frac{k_x ( T^{(k)}_{i+1,j} + T^{(k)}_{i-1,j} ) + k_y ( T^{(k)}_{i,j+1} + T^{(k)}_{i,j-1} ) + h^2 f_{i,j}}{2(k_x + k_y)}\n$$\nA key characteristic of the Jacobi method is that the updates for all points $(i,j)$ can be performed simultaneously (in parallel) as they do not depend on each other within the same iteration. This is implemented by using a copy of the temperature field from the previous iteration for all calculations in the current iteration.\n\n#### **2.2. Successive Over-Relaxation (SOR) Method**\n\nSOR is an enhancement of the Gauss-Seidel method. It computes the new iterate $T^{(k+1)}_{i,j}$ by taking a weighted average of the previous iterate $T^{(k)}_{i,j}$ and the value predicted by a Gauss-Seidel step. The update rule, applied sequentially over the grid points (e.g., in lexicographical order), is:\n$$\nT^{(k+1)}_{i,j} = (1-\\omega) T^{(k)}_{i,j} + \\omega \\, \\frac{k_x \\left( T^{(k)}_{i+1,j} + T^{(k+1)}_{i-1,j} \\right) + k_y \\left( T^{(k)}_{i,j+1} + T^{(k+1)}_{i,j-1} \\right) + h^2 f_{i,j}}{2(k_x + k_y)}\n$$\nHere, $\\omega \\in (0, 2)$ is the relaxation parameter. When updating $T_{i,j}$, the values $T_{i-1,j}$ and $T_{i,j-1}$ have already been updated to their new values in the $(k+1)$-th iteration, while $T_{i+1,j}$ and $T_{i,j+1}$ are still at their values from the $k$-th iteration. This sequential data dependency makes SOR inherently non-parallelizable in its standard form but generally leads to faster convergence than Jacobi if $\\omega$ is chosen appropriately.\n\n#### **2.3. Switching Criterion and $\\omega$ Estimation**\n\nThe solver starts with Jacobi iterations. The core of the hybrid strategy is the criterion for switching to SOR.\n1.  **Monitoring:** During the Jacobi phase, we compute the $L_2$-norm of the residual, $||r^{(k)}||_2$, at each iteration $k$. The residual is defined as $r^{(k)} = A T^{(k)} - \\mathbf{b}$, with pointwise components $r^{(k)}_{i,j} = 2(k_x + k_y) T^{(k)}_{i,j} - k_x (T^{(k)}_{i+1,j} + T^{(k)}_{i-1,j}) - k_y (T^{(k)}_{i,j+1} + T^{(k)}_{i,j-1}) - h^2 f_{i,j}$. We then track the sequence of residual reduction ratios, $q_k = ||r^{(k)}||_2 / ||r^{(k-1)}||_2$.\n2.  **Condition:** For iterations $k$ between $N_{\\min}$ and $N_{\\max}$, we examine a sliding window of the $m$ most recent ratios. A switch to SOR is triggered if two conditions are met simultaneously:\n    -   The ratios in the window have stabilized: $\\max(q_{k-m+1}, \\dots, q_k) - \\min(q_{k-m+1}, \\dots, q_k) \\le \\delta$.\n    -   The solver is converging: The average ratio $\\frac{1}{m}\\sum_{j=k-m+1}^k q_j  1$.\n3.  **Omega Estimation:** Upon switching, the average of the ratios in the window, $\\hat{\\rho}_J = \\text{mean}(q_{k-m+1}, \\dots, q_k)$, serves as an estimate for the spectral radius of the Jacobi iteration matrix. This is justified because, for a converging linear iteration, the asymptotic rate of convergence is governed by the spectral radius, which manifests as the ratio of successive error (and residual) norms. For the class of matrices arising from this discretization, a well-established theoretical result relates the optimal SOR relaxation parameter, $\\omega_{\\text{opt}}$, to the Jacobi spectral radius, $\\rho_J$:\n    $$\n    \\omega_{\\text{opt}} = \\frac{2}{1 + \\sqrt{1 - \\rho_J^2}}\n    $$\n    We use our estimate $\\hat{\\rho}_J$ in this formula to calculate the value of $\\omega$ for the subsequent SOR iterations. Since the switching condition requires $\\hat{\\rho}_J  1$, the argument of the square root is positive, and the computed $\\omega$ will be in the range $[1, 2)$, satisfying the convergence requirement $\\omega \\in (0, 2)$. If the solver does not switch to SOR, $\\omega$ is reported as $1.0$.\n\n### **3. Algorithmic Implementation**\n\nThe overall algorithm is as follows:\n1.  **Initialization:**\n    -   Set grid parameters $n$, $h$. Construct the $(n+2) \\times (n+2)$ arrays for temperature $T$ and source term $f$. Initialize $T^{(0)} = 0$.\n    -   Compute the source term array $f$ based on the test case, and the right-hand side array $\\mathbf{b} = h^2 f$.\n    -   Calculate the initial residual norm $||r^{(0)}||_2 = ||A T^{(0)} - \\mathbf{b}||_2 = ||-\\mathbf{b}||_2$.\n    -   Determine the stopping tolerance: $||r^{(k)}||_2 \\le \\tau ||r^{(0)}||_2$.\n\n2.  **Main Iteration Loop:**\n    -   The loop continues until the stopping criterion is met.\n    -   Initialize `mode = 'jacobi'`, `switched = 0`, `iter_count = 0`.\n    -   Inside the loop, if `mode == 'jacobi'`:\n        -   Perform one Jacobi update to get $T^{(k+1)}$ from $T^{(k)}$.\n        -   Increment `iter_count`.\n        -   Calculate the new residual norm $||r^{(k+1)}||_2$.\n        -   Compute and store the ratio $q_{k+1} = ||r^{(k+1)}||_2 / ||r^{(k)}||_2$.\n        -   If $N_{\\min} \\le \\text{iter\\_count} \\le N_{\\max}$ and sufficient ratios are available, check the switching condition. If met, set `mode = 'sor'`, `switched = 1`, and compute $\\omega$.\n    -   If `mode == 'sor'`:\n        -   Perform one SOR update by iterating through all interior grid points.\n        -   Increment `iter_count`.\n        -   Calculate the new residual norm $||r^{(k+1)}||_2$.\n\n3.  **Termination:**\n    -   Once $||r^{(k)}||_2$ falls below the target, the loop terminates.\n    -   The total `iter_count`, the `switched` flag ($1$ or $0$), and the computed `omega` (or $1.0$ if no switch occurred) are recorded for the test case.\n\nThis structure is implemented for each of the three test cases provided.",
            "answer": "```python\nimport numpy as np\n\ndef run_solver(n, kx, ky, f_type, tau, m, N_min, N_max, delta):\n    \"\"\"\n    Implements the hybrid Jacobi-SOR solver for a single test case.\n\n    Args:\n        n (int): Number of interior grid points along one dimension.\n        kx (float): Conductivity in the x-direction.\n        ky (float): Conductivity in the y-direction.\n        f_type (str): Type of source function ('constant', 'sinsin', 'checkerboard').\n        tau (float): Relative tolerance for stopping criterion.\n        m (int): Sliding window length for switching criterion.\n        N_min (int): Minimum number of Jacobi iterations before checking switch condition.\n        N_max (int): Maximum number of Jacobi iterations before checking switch condition.\n        delta (float): Stability band for ratios in switching criterion.\n\n    Returns:\n        tuple: (total_iterations, switched_flag, omega_value)\n    \"\"\"\n    h = 1.0 / (n + 1)\n\n    # Setup source term f and RHS b\n    f = np.zeros((n + 2, n + 2))\n    if f_type == 'constant':\n        f[1:-1, 1:-1] = 1.0\n    elif f_type == 'sinsin':\n        # Use 'ij' indexing to match loops used for i, j indices\n        x_coords = np.linspace(0, 1, n + 2)\n        y_coords = np.linspace(0, 1, n + 2)\n        X, Y = np.meshgrid(x_coords, y_coords, indexing='ij')\n        f = np.sin(np.pi * X) * np.sin(np.pi * Y)\n    elif f_type == 'checkerboard':\n        for i in range(1, n + 1):\n            for j in range(1, n + 1):\n                f[i, j] = (-1)**(i + j)\n    b = h**2 * f\n\n    def calculate_residual_norm(T_mat):\n        \"\"\"Computes the L2 norm of the residual r = AT - b.\"\"\"\n        res_mat = (2 * (kx + ky) * T_mat[1:-1, 1:-1] -\n                   kx * (T_mat[2:, 1:-1] + T_mat[:-2, 1:-1]) -\n                   ky * (T_mat[1:-1, 2:] + T_mat[1:-1, :-2]) -\n                   b[1:-1, 1:-1])\n        return np.linalg.norm(res_mat)\n\n    # Initialization\n    T = np.zeros((n + 2, n + 2))\n    \n    initial_res_norm = calculate_residual_norm(T)\n    if initial_res_norm == 0:\n        return 0, 0, 1.0\n        \n    stop_norm = tau * initial_res_norm\n    \n    iter_count = 0\n    switched = 0\n    omega = 1.0\n    mode = 'jacobi'\n    res_ratios = []\n    \n    current_res_norm = initial_res_norm\n    # Safety break to prevent infinite loops\n    max_total_iters = 50000 \n    \n    while current_res_norm  stop_norm and iter_count  max_total_iters:\n        T_old = np.copy(T)\n        previous_res_norm = current_res_norm\n\n        if mode == 'jacobi':\n            # Jacobi update\n            T[1:-1, 1:-1] = (kx * (T_old[2:, 1:-1] + T_old[:-2, 1:-1]) +\n                             ky * (T_old[1:-1, 2:] + T_old[1:-1, :-2]) +\n                             b[1:-1, 1:-1]) / (2 * (kx + ky))\n            \n            iter_count += 1\n            current_res_norm = calculate_residual_norm(T)\n            \n            if previous_res_norm  1e-15:\n                res_ratios.append(current_res_norm / previous_res_norm)\n            \n            if N_min = iter_count = N_max and len(res_ratios) = m:\n                window = res_ratios[-m:]\n                mean_q = np.mean(window)\n                \n                if (np.max(window) - np.min(window) = delta) and (mean_q  1.0):\n                    switched = 1\n                    mode = 'sor'\n                    rho_J_est = mean_q\n                    omega = 2 / (1 + np.sqrt(1 - rho_J_est**2))\n        \n        else: # mode == 'sor'\n            # SOR update\n            for i in range(1, n + 1):\n                for j in range(1, n + 1):\n                    # Uses newest T[i-1,j] and T[i,j-1] from this sweep\n                    gs_sum = kx * (T[i+1, j] + T[i-1, j]) + ky * (T[i, j+1] + T[i, j-1])\n                    gs_term = (gs_sum + b[i, j]) / (2 * (kx + ky))\n                    T[i, j] = (1 - omega) * T_old[i, j] + omega * gs_term\n            \n            iter_count += 1\n            current_res_norm = calculate_residual_norm(T)\n            \n    if switched == 0:\n        omega_val = 1.0\n    else:\n        omega_val = omega\n\n    return iter_count, switched, omega_val\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Test Case A\n        {'n': 50, 'kx': 1, 'ky': 1, 'f_type': 'constant', 'tau': 1e-6,\n         'm': 20, 'N_min': 40, 'N_max': 100, 'delta': 5e-4},\n        # Test Case B\n        {'n': 60, 'kx': 4, 'ky': 1, 'f_type': 'sinsin', 'tau': 5e-7,\n         'm': 25, 'N_min': 50, 'N_max': 120, 'delta': 8e-4},\n        # Test Case C\n        {'n': 8, 'kx': 1, 'ky': 1, 'f_type': 'checkerboard', 'tau': 1e-6,\n         'm': 10, 'N_min': 15, 'N_max': 40, 'delta': 1e-3}\n    ]\n\n    results = []\n    for case in test_cases:\n        iters, switch_flag, omega_final = run_solver(**case)\n        results.append([iters, switch_flag, omega_final])\n\n    # Format the output string precisely as requested\n    formatted_results = []\n    for r in results:\n        # r[2] is float; format it to 5 decimal places\n        formatted_results.append(f\"[{r[0]},{r[1]},{r[2]:.5f}]\")\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```"
        }
    ]
}