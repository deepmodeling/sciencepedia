{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp the power of Algebraic Multigrid, there is no substitute for building its core components from the ground up. This foundational exercise guides you through the implementation of a complete two-grid cycle. By constructing the relaxation smoother and the coarse-grid correction machinery yourself, you will gain a concrete, quantitative understanding of how multigrid effectively tackles the low-frequency errors that render simpler iterative methods inefficient .",
            "id": "3936595",
            "problem": "A steady heat conduction problem with constant and possibly anisotropic thermal conductivity is governed by the fundamental law of conservation of energy. In the absence of internal heat generation, the continuum model is given by the partial differential equation $-\\nabla \\cdot (k \\nabla T) = 0$, where $k$ is the thermal conductivity tensor and $T$ is temperature. For a square domain with homogeneous Dirichlet boundary conditions, discretization via a second-order finite difference scheme on a uniform Cartesian grid of interior points leads to a symmetric positive definite linear system $A x = b$, where $A$ encodes the discrete diffusion operator, $x$ represents the nodal temperatures, and $b$ is the source vector. In computational thermal engineering, relaxation methods such as weighted Jacobi are known to attenuate high-frequency components of the error efficiently while leaving low-frequency components relatively unaffected. Algebraic Multigrid (AMG) constructs coarse representations to target these low-frequency components, and the two-grid method, a fundamental AMG component, combines relaxation with a coarse-grid correction to reduce low-frequency errors.\n\nYour task is to construct a self-contained numerical example that demonstrates how the two-grid method reduces low-frequency error components that relaxation alone cannot eliminate, using an algebraic Galerkin coarse operator. You must implement the following algorithmic components from first principles:\n\n- Discrete thermal operator construction: Build the discrete diffusion operator $A$ for a uniform $n \\times n$ interior grid with homogeneous Dirichlet boundary conditions. Use a five-point stencil for a possibly anisotropic conductivity with principal values $k_x$ and $k_y$, and grid spacing $h_x = h_y = 1/(n+1)$. The discrete operator must be symmetric positive definite. Formally, for the two-dimensional case, this can be expressed as a Kronecker-sum of one-dimensional second-derivative operators along $x$ and $y$.\n\n- Relaxation scheme: Implement the weighted Jacobi relaxation operator with weight $\\omega$ on the homogeneous error equation. If $D$ denotes the diagonal of $A$, then a relaxation step updates the current error $e$ according to the iteration matrix determined by $A$ and $D$. Use this relaxation scheme for a specified number of sweeps.\n\n- Algebraic restriction and prolongation: Construct a standard full-weighting restriction $R$ from the fine grid to the coarse grid and a linear interpolation prolongation $P$ from the coarse grid to the fine grid such that the Galerkin coarse operator $A_c$ is defined by $A_c = R A P$. Use tensor-product constructions for two dimensions and ensure $P$ and $R$ are compatible in the sense of the variational principle.\n\n- Two-grid V-cycle for error reduction: Implement a single two-grid cycle on the error. Apply $\\nu_1$ pre-relaxation sweeps, compute the residual $r = A e$, restrict to the coarse grid, solve $A_c d_c = r_c$ exactly on the coarse grid, prolongate the correction $d_f = P d_c$, update $e \\leftarrow e - d_f$, and apply $\\nu_2$ post-relaxation sweeps.\n\n- Initial error modes: Generate an initial error $e_0$ on the fine grid using either a low-frequency mode $e_{ij} = \\sin(\\pi x_i) \\sin(\\pi y_j)$, where $x_i = i h$ and $h = 1/(n+1)$ for $i = 1, \\dots, n$, or a high-frequency checkerboard mode $e_{ij} = (-1)^{i+j}$ mapped to the interior grid indices.\n\nFor each test case, compute and report two quantitative metrics:\n1. The relaxation-only reduction factor defined as $\\|e_{\\text{relax}}\\|_2 / \\|e_0\\|_2$ after the specified number of weighted Jacobi sweeps on the error.\n2. The two-grid reduction factor defined as $\\|e_{\\text{TG}}\\|_2 / \\|e_0\\|_2$ after one two-grid V-cycle with the specified pre- and post-relaxation sweeps.\n\nDesign and implement the program to handle the following test suite, which spans a happy path, minimal smoothing, anisotropy, and high-frequency edge behavior:\n\n- Test case $1$: $n = 32$, $k_x = 1.0$, $k_y = 1.0$, low-frequency initial error, relaxation sweeps $s = 10$, relaxation weight $\\omega = 2/3$, two-grid pre-sweeps $\\nu_1 = 3$, two-grid post-sweeps $\\nu_2 = 3$.\n- Test case $2$: $n = 32$, $k_x = 1.0$, $k_y = 1.0$, low-frequency initial error, relaxation sweeps $s = 1$, relaxation weight $\\omega = 2/3$, two-grid pre-sweeps $\\nu_1 = 1$, two-grid post-sweeps $\\nu_2 = 1$.\n- Test case $3$: $n = 32$, $k_x = 1.0$, $k_y = 0.1$, low-frequency initial error, relaxation sweeps $s = 10$, relaxation weight $\\omega = 2/3$, two-grid pre-sweeps $\\nu_1 = 3$, two-grid post-sweeps $\\nu_2 = 3$.\n- Test case $4$: $n = 32$, $k_x = 1.0$, $k_y = 1.0$, high-frequency initial error, relaxation sweeps $s = 10$, relaxation weight $\\omega = 2/3$, two-grid pre-sweeps $\\nu_1 = 3$, two-grid post-sweeps $\\nu_2 = 3$.\n\nAll computations are dimensionless within the discrete setting. Your program must output a single line containing the results for all test cases as a comma-separated list enclosed in square brackets. Each result must be a list of two floating-point numbers in the form $[\\text{relax\\_factor}, \\text{two\\_grid\\_factor}]$, with each factor computed as specified above. For example, the final output must be of the form $[[a,b],[c,d],[e,f],[g,h]]$, where $a$, $b$, $c$, $d$, $e$, $f$, $g$, and $h$ are floats.",
            "solution": "The problem requires a numerical demonstration of the effectiveness of a two-grid multigrid method in reducing low-frequency error components for a discretized heat conduction problem. This will be contrasted with the performance of a simple relaxation-based solver (weighted Jacobi), which is known to be inefficient for such error components. The analysis is performed on the homogeneous error equation $A e = 0$, where $A$ is the discrete operator, and $e$ is the error vector.\n\nWe will construct all algorithmic components from first principles: the discrete operator $A$, the relaxation scheme, inter-grid transfer operators (restriction $R$ and prolongation $P$), the coarse-grid operator $A_c$, and the two-grid cycle.\n\n**1. Discrete Thermal Operator $A$**\n\nThe governing partial differential equation for steady-state heat conduction with anisotropic thermal conductivity $k$ and no heat source is $-\\nabla \\cdot (k \\nabla T) = 0$. For a 2D domain with principal conductivity axes aligned with the Cartesian coordinates, this expands to:\n$$\n- \\frac{\\partial}{\\partial x} \\left( k_x \\frac{\\partial T}{\\partial x} \\right) - \\frac{\\partial}{\\partial y} \\left( k_y \\frac{\\partial T}{\\partial y} \\right) = 0\n$$\nWe discretize this equation on a uniform $n \\times n$ grid of interior points on a unit square. The grid spacing is $h_x = h_y = h = 1/(n+1)$. Using a second-order centered finite difference scheme, the equation at an interior grid point $(i,j)$ becomes:\n$$\n-k_x \\frac{T_{i+1,j} - 2T_{i,j} + T_{i-1,j}}{h^2} - k_y \\frac{T_{i,j+1} - 2T_{i,j} + T_{i,j-1}}{h^2} = 0\n$$\nRearranging the terms, we obtain the stencil for a single grid point:\n$$\n\\frac{1}{h^2} \\left[ (2k_x + 2k_y)T_{i,j} - k_x T_{i+1,j} - k_x T_{i-1,j} - k_y T_{i,j+1} - k_y T_{i,j-1} \\right] = 0\n$$\nThis defines the linear system $A x = b$. For our homogeneous error equation, $A e = 0$. The matrix $A$ is of size $n^2 \\times n^2$ and is constructed by assembling the stencil for all $n^2$ points, assuming a lexicographical ordering of the grid points. Homogeneous Dirichlet boundary conditions are incorporated by setting nodal values on the boundary to zero.\n\nThe matrix $A$ can be elegantly expressed as a Kronecker sum of 1D discrete Laplacian operators. Let $A_{1D}$ be the $n \\times n$ tridiagonal matrix representing the 1D second derivative, with values $-1, 2, -1$ on its sub-diagonal, main diagonal, and super-diagonal, respectively. Let $I_n$ be the $n \\times n$ identity matrix. The full 2D operator $A$ is then:\n$$\nA = \\frac{k_x}{h^2} (I_n \\otimes A_{1D}) + \\frac{k_y}{h^2} (A_{1D} \\otimes I_n)\n$$\nwhere $\\otimes$ denotes the Kronecker product. This construction yields a symmetric positive definite matrix $A$.\n\n**2. Initial Error Modes**\n\nTo test the method's efficacy, we initialize the error vector $e_0$ with specific spatial frequencies:\n- **Low-frequency mode**: $e_{ij} = \\sin(\\pi x_i) \\sin(\\pi y_j)$, where $x_i = ih$ and $y_j = jh$. This mode is smooth and corresponds to the lowest-frequency eigenfunction of the continuous Laplacian on the unit square. Relaxation methods are notoriously slow to damp such errors.\n- **High-frequency mode**: $e_{ij} = (-1)^{i+j}$. This is a highly oscillatory checkerboard pattern. Relaxation methods are very effective at damping this type of error.\n\n**3. Relaxation Scheme: Weighted Jacobi**\n\nThe weighted Jacobi method is an iterative smoother. For the homogeneous equation $A e = 0$, the update rule for the error vector $e$ is derived from the iteration for $Ax=b$, which is $x^{(k+1)} = x^{(k)} + \\omega D^{-1}(b-Ax^{(k)})$. This leads to an update for the error $e^{(k)}$:\n$$\ne^{(k+1)} = (I - \\omega D^{-1}A) e^{(k)}\n$$\nwhere $I$ is the identity matrix, $D$ is the diagonal of $A$, and $\\omega$ is the relaxation weight. A typical value for $\\omega$ that ensures convergence for SPD matrices is $\\omega=2/3$. This smoother is applied for a specified number of sweeps.\n\n**4. Inter-Grid Transfer Operators: Restriction and Prolongation**\n\nA key component of multigrid is transferring information between the fine grid (size $n \\times n$) and a coarser grid. For this problem, we use a coarse grid of size $n_c \\times n_c$, where $n_c = n/2$.\n- **Prolongation (Interpolation) $P$**: This operator maps a vector from the coarse grid to the fine grid. We use bilinear interpolation. In 1D, a coarse grid value is injected onto its corresponding fine grid node, and linearly interpolated to the midpoint between coarse nodes. The 2D prolongation operator $P$ is formed by the Kronecker product of the 1D prolongation operator $P_{1D}$ with itself: $P = P_{1D} \\otimes P_{1D}$. $P$ is an $n^2 \\times n_c^2$ matrix.\n- **Restriction $R$**: This operator maps a vector from the fine grid to the coarse grid. We use full-weighting restriction, where a coarse-grid value is a weighted average of its nine nearest neighbors on the fine grid. For a Galerkin coarse operator, the restriction and prolongation operators must be compatible. A standard choice satisfying the variational principle is $R = c P^T$ for some scalar $c$. For our choice of $P$ and standard full-weighting, the relation is $R = \\frac{1}{4}P^T$. $R$ is an $n_c^2 \\times n^2$ matrix.\n\n**5. Coarse-Grid Operator and Two-Grid Cycle**\n\nThe coarse-grid operator $A_c$ is constructed using the Galerkin principle:\n$$\nA_c = R A P\n$$\nThis construction ensures that the coarse operator inherits properties of the fine operator, such as symmetry and positive-definiteness, and correctly represents the fine-grid operator's action on low-frequency components. $A_c$ is a small, dense matrix of size $n_c^2 \\times n_c^2$.\n\nA single two-grid V-cycle for the error vector $e$ proceeds as follows:\n1.  **Pre-relaxation**: Apply $\\nu_1$ sweeps of the weighted Jacobi smoother to the current error $e$. Let the result be $e'$.\n    $e' \\leftarrow (I - \\omega D^{-1}A)^{\\nu_1} e$\n2.  **Compute Residual**: Calculate the residual on the fine grid. As per the problem statement, $r = A e'$.\n3.  **Restrict Residual**: Transfer the residual to the coarse grid: $r_c = R r$.\n4.  **Solve Coarse-Grid Problem**: Solve the coarse-grid system $A_c d_c = r_c$ exactly for the coarse-grid correction $d_c$. This is feasible as $A_c$ is small.\n5.  **Prolongate Correction**: Interpolate the correction back to the fine grid: $d_f = P d_c$.\n6.  **Correct Error**: Update the fine-grid error: $e'' = e' - d_f$.\n7.  **Post-relaxation**: Apply $\\nu_2$ sweeps of the weighted Jacobi smoother to $e''$ to smooth out high-frequency errors introduced by the interpolation. The final error is $e_{TG} = (I - \\omega D^{-1}A)^{\\nu_2} e''$.\n\n**6. Quantitative Evaluation**\n\nFor each test case, we compute two metrics to quantify the performance:\n1.  **Relaxation-only reduction factor**: $\\|e_{\\text{relax}}\\|_2 / \\|e_0\\|_2$, where $e_{\\text{relax}}$ is the error after $s$ Jacobi sweeps.\n2.  **Two-grid reduction factor**: $\\|e_{\\text{TG}}\\|_2 / \\|e_0\\|_2$, where $e_{\\textTG}$ is the error after one two-grid V-cycle.\n\nThese metrics will illustrate that for low-frequency errors, the two-grid factor is significantly smaller than the relaxation-only factor, confirming the efficacy of the coarse-grid correction. For high-frequency errors, both methods are expected to be effective.",
            "answer": "```python\nimport numpy as np\nfrom scipy.sparse import diags, identity, kron, lil_matrix, csc_matrix\n\ndef build_discrete_operator(n, k_x, k_y):\n    \"\"\"\n    Constructs the discrete diffusion operator A, inter-grid operators P and R,\n    and the coarse-grid operator Ac.\n    \"\"\"\n    h = 1.0 / (n + 1)\n    \n    # 1D discrete Laplacian\n    diag_vals = [-1.0, 2.0, -1.0]\n    offsets = [-1, 0, 1]\n    A_1d = diags(diag_vals, offsets, shape=(n, n), format='csc')\n    \n    I_n = identity(n, format='csc')\n    \n    # 2D discrete operator A using Kronecker sum\n    A = (k_x / h**2) * kron(I_n, A_1d) + (k_y / h**2) * kron(A_1d, I_n)\n    A = csc_matrix(A)\n    \n    # Diagonal of A for Jacobi smoother\n    D = A.diagonal()\n    \n    # Coarsening\n    n_c = n // 2\n    if n % 2 != 0:\n        raise ValueError(\"n must be even for this coarsening strategy.\")\n\n    # 1D Prolongation operator P_1d\n    P_1d = lil_matrix((n, n_c))\n    for j in range(n_c):\n        # Injection to fine node 2*j+1 from coarse node j\n        P_1d[2*j + 1, j] = 1.0\n        # Interpolation to fine node 2*j\n        if j > 0:\n            P_1d[2*j, j - 1] = 0.5\n        P_1d[2*j, j] = 0.5\n    P_1d = csc_matrix(P_1d)\n\n    # 2D Prolongation operator P\n    P = kron(P_1d, P_1d, format='csc')\n    \n    # 2D Restriction operator R (Galerkin compatibility)\n    R = 0.25 * P.T\n    R = csc_matrix(R)\n\n    # Galerkin Coarse-grid operator Ac\n    A_c_sparse = R @ A @ P\n    A_c = A_c_sparse.toarray()\n    \n    return A, D, P, R, A_c\n\ndef create_initial_error(n, error_type='low_frequency'):\n    \"\"\"\n    Generates the initial error vector, either low-frequency (smooth) or\n    high-frequency (oscillatory).\n    \"\"\"\n    h = 1.0 / (n + 1)\n    e_2d = np.zeros((n, n))\n    \n    if error_type == 'low_frequency':\n        for i in range(n):\n            for j in range(n):\n                x_i = (i + 1) * h\n                y_j = (j + 1) * h\n                e_2d[i, j] = np.sin(np.pi * x_i) * np.sin(np.pi * y_j)\n    elif error_type == 'high_frequency':\n        for i in range(n):\n            for j in range(n):\n                e_2d[i, j] = (-1.0)**(i + j)\n    else:\n        raise ValueError(\"Unknown error_type specified.\")\n        \n    return e_2d.flatten()\n\ndef weighted_jacobi(e, A, D, omega, sweeps):\n    \"\"\"Applies a specified number of weighted Jacobi sweeps.\"\"\"\n    e_new = e.copy()\n    for _ in range(sweeps):\n        residual = -(A @ e_new)\n        e_new += omega * residual / D\n    return e_new\n\ndef two_grid_v_cycle(e0, A, D, P, R, A_c, omega, nu1, nu2):\n    \"\"\"Performs a single two-grid V-cycle on the error vector.\"\"\"\n    # 1. Pre-relaxation\n    e_pre = weighted_jacobi(e0, A, D, omega, nu1)\n    \n    # 2. Compute residual\n    r = A @ e_pre\n    \n    # 3. Restrict residual\n    r_c = R @ r\n    \n    # 4. Solve coarse-grid problem\n    d_c = np.linalg.solve(A_c, r_c)\n    \n    # 5. Prolongate correction\n    d_f = P @ d_c\n    \n    # 6. Correct error\n    e_corrected = e_pre - d_f\n    \n    # 7. Post-relaxation\n    e_final = weighted_jacobi(e_corrected, A, D, omega, nu2)\n    \n    return e_final\n\ndef solve():\n    test_cases = [\n        # n, kx, ky, error_type, s, omega, nu1, nu2\n        (32, 1.0, 1.0, 'low_frequency', 10, 2/3, 3, 3),\n        (32, 1.0, 1.0, 'low_frequency', 1, 2/3, 1, 1),\n        (32, 1.0, 0.1, 'low_frequency', 10, 2/3, 3, 3),\n        (32, 1.0, 1.0, 'high_frequency', 10, 2/3, 3, 3),\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        n, k_x, k_y, error_type, s, omega, nu1, nu2 = case\n\n        # 1. Setup operators\n        A, D, P, R, A_c = build_discrete_operator(n, k_x, k_y)\n\n        # 2. Create initial error\n        e0 = create_initial_error(n, error_type)\n        e0_norm = np.linalg.norm(e0)\n\n        if e0_norm == 0:\n            all_results.append([0.0, 0.0])\n            continue\n            \n        # 3. Calculate relaxation-only reduction factor\n        e_relax = weighted_jacobi(e0, A, D, omega, s)\n        relax_factor = np.linalg.norm(e_relax) / e0_norm\n\n        # 4. Calculate two-grid reduction factor\n        e_tg = two_grid_v_cycle(e0, A, D, P, R, A_c, omega, nu1, nu2)\n        tg_factor = np.linalg.norm(e_tg) / e0_norm\n        \n        all_results.append([relax_factor, tg_factor])\n\n    # 5. Format and print the final output\n    formatted_results = [f\"[{res[0]},{res[1]}]\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having seen a two-grid method succeed in a standard scenario, we now pivot to explore a classic case where a simple, geometrically-minded approach fails. This problem presents a hypothetical thermal problem with strong diagonal anisotropy, a situation analogous to heat transfer in certain composite materials. By analyzing a specific low-energy mode that the system's physics favors, you will uncover why a naive interpolation scheme proves ineffective, highlighting the critical lesson that the method must be \"algebraic\"—that is, responsive to the matrix values, not just the grid structure .",
            "id": "3936677",
            "problem": "Consider steady conductive heat transfer in a thin plate with spatially varying thermal conductivity, governed by the partial differential equation $-\\nabla \\cdot (k(\\boldsymbol{x}) \\nabla T(\\boldsymbol{x})) + \\alpha T(\\boldsymbol{x}) = 0$ with a uniform volumetric heat exchange term $\\alpha > 0$. Discretize this on a four-node mesh representing the corners of a square control-volume arrangement, indexed $1$ (top-left), $2$ (top-right), $3$ (bottom-left), $4$ (bottom-right). Assume an anisotropy rotated so that the dominant conduction paths are diagonal: the conductances along edges $(1,4)$ and $(2,3)$ are $K > 0$ (strong), and the four side edges $(1,2)$, $(1,3)$, $(2,4)$, $(3,4)$ have conductance $\\varepsilon > 0$ (weak). The resulting discrete operator $A \\in \\mathbb{R}^{4 \\times 4}$ is the weighted graph Laplacian with a reaction term, with entries\n$$\nA_{ii} = \\alpha + \\sum_{j \\in \\mathcal{N}(i)} g_{ij}, \\quad A_{ij} = -g_{ij} \\text{ for } i \\neq j,\n$$\nwhere $g_{ij} \\in \\{\\varepsilon, K\\}$ for the edges described above and $\\mathcal{N}(i)$ denotes the neighbors of node $i$. Explicitly,\n$$\nA = \\begin{pmatrix}\n\\alpha + 2\\varepsilon + K & -\\varepsilon & -\\varepsilon & -K \\\\\n-\\varepsilon & \\alpha + 2\\varepsilon + K & -K & -\\varepsilon \\\\\n-\\varepsilon & -K & \\alpha + 2\\varepsilon + K & -\\varepsilon \\\\\n- K & -\\varepsilon & -\\varepsilon & \\alpha + 2\\varepsilon + K\n\\end{pmatrix}.\n$$\nNow consider a two-grid Algebraic Multigrid (AMG) method (Two-Grid (TG)) with Galerkin coarse operator $A_c = P^{\\top} A P$, restriction $R = P^{\\top}$, and a weighted Jacobi smoother $S = I - \\omega D^{-1} A$, where $\\omega \\in (0,1)$ is the relaxation parameter and $D = \\mathrm{diag}(A)$ is the diagonal of $A$. Define the coarse set as nodes $\\{1,2\\}$ and suppose interpolation $P \\in \\mathbb{R}^{4 \\times 2}$ is constructed by erroneously ignoring the strong diagonal connections and using only the weak side connections to map fine nodes to coarse nodes:\n$$\nP = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix},\n$$\nso that node $3$ interpolates from node $1$ and node $4$ interpolates from node $2$. \n\n- Starting from the discrete energy associated with $A$, identify a low-energy error mode $v \\in \\mathbb{R}^{4}$ for this anisotropy and justify why it is low-energy. \n- Show, from first principles, that the coarse-grid correction annihilates only components in the range of $P$ and compute $P^{\\top} A v$ to demonstrate that this interpolation fails to approximate the identified low-energy mode.\n- Using the definitions above and without invoking any shortcut formulas, derive the Two-Grid error-propagation acting on $v$ and compute the resulting degradation in convergence along this mode, expressed as the closed-form eigenvalue of the combined TG operator $E = (I - P A_c^{-1} P^{\\top} A) S$ when applied to $v$.\n\nYour final answer must be a single analytic expression in terms of $\\omega$, $\\varepsilon$, K, and $\\alpha$. Do not substitute numbers. No rounding is required, and no units are needed in the final expression.",
            "solution": "The problem is subjected to validation and is deemed a valid, well-posed problem in computational science. It presents a theoretical analysis of a two-grid algebraic multigrid method under specific, clearly defined conditions. All necessary data and definitions are provided, and the problem is scientifically and mathematically sound.\n\nThe solution is structured to address the three main tasks outlined in the problem statement.\n\nFirst, we identify a low-energy error mode $v$. The \"energy\" of a vector $v \\in \\mathbb{R}^{4}$ with respect to the operator $A$ is defined by the quadratic form $v^{\\top} A v$. Expanding this using the definition of $A$ yields:\n$$\nv^{\\top} A v = \\sum_{i,j} v_i A_{ij} v_j = \\sum_{i} A_{ii} v_i^2 + \\sum_{i \\neq j} A_{ij} v_i v_j\n$$\nSubstituting $A_{ii} = \\alpha + \\sum_{k \\neq i} g_{ik}$ and $A_{ij} = -g_{ij}$ for $i \\neq j$:\n$$\nv^{\\top} A v = \\sum_{i} \\left(\\alpha v_i^2 + \\sum_{k \\neq i} g_{ik} v_i^2\\right) - \\sum_{i \\neq j} g_{ij} v_i v_j\n$$\nUsing the symmetry $g_{ij} = g_{ji}$, the expression can be rewritten as:\n$$\nv^{\\top} A v = \\alpha \\sum_{i} v_i^2 + \\sum_{i<j} g_{ij} (v_i^2 - 2v_i v_j + v_j^2) = \\alpha v^{\\top}v + \\sum_{i<j} g_{ij} (v_i - v_j)^2\n$$\nA low-energy mode $v$ is one for which $v^{\\top} A v$ is small. This occurs when the term $\\sum_{i<j} g_{ij} (v_i - v_j)^2$ is small. The problem specifies strong connections (conductance $K$) along the diagonals $(1,4)$ and $(2,3)$, and weak connections (conductance $\\varepsilon$) along the sides. To minimize the energy, the differences $(v_i - v_j)$ should be small across connections $(i,j)$ with large conductance $g_{ij}$. Therefore, a low-energy mode should satisfy $v_1 \\approx v_4$ and $v_2 \\approx v_3$.\nThe simplest non-constant vector that aligns with this principle is an oscillatory mode where these pairs have opposite signs. Let us consider the vector $v = (1, -1, -1, 1)^{\\top}$. For this vector, $v_1 - v_4 = 1 - 1 = 0$ and $v_2 - v_3 = -1 - (-1) = 0$. The contribution to the energy from the strong connections is zero. Such modes, which are \"smoother\" in the direction of strong coupling, are typically challenging for standard relaxation schemes (smoothers) and must be handled by the coarse-grid correction.\nTo confirm that this is a low-energy mode, we show it is an eigenvector of $A$ with a correspondingly small eigenvalue.\n$$\nAv = \\begin{pmatrix}\n\\alpha + 2\\varepsilon + K & -\\varepsilon & -\\varepsilon & -K \\\\\n-\\varepsilon & \\alpha + 2\\varepsilon + K & -K & -\\varepsilon \\\\\n-\\varepsilon & -K & \\alpha + 2\\varepsilon + K & -\\varepsilon \\\\\n- K & -\\varepsilon & -\\varepsilon & \\alpha + 2\\varepsilon + K\n\\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ -1 \\\\ -1 \\\\ 1 \\end{pmatrix}\n= \\begin{pmatrix}\n(\\alpha + 2\\varepsilon + K) - \\varepsilon(-1) - \\varepsilon(-1) - K(1) \\\\\n-\\varepsilon(1) + (\\alpha + 2\\varepsilon + K)(-1) - K(-1) - \\varepsilon(1) \\\\\n-\\varepsilon(1) - K(-1) + (\\alpha + 2\\varepsilon + K)(-1) - \\varepsilon(1) \\\\\n-K(1) - \\varepsilon(-1) - \\varepsilon(-1) + (\\alpha + 2\\varepsilon + K)(1)\n\\end{pmatrix}\n$$\n$$\nAv = \\begin{pmatrix}\n\\alpha + 2\\varepsilon + K + \\varepsilon + \\varepsilon - K \\\\\n-\\varepsilon - \\alpha - 2\\varepsilon - K + K - \\varepsilon \\\\\n-\\varepsilon + K - \\alpha - 2\\varepsilon - K - \\varepsilon \\\\\n-K + \\varepsilon + \\varepsilon + \\alpha + 2\\varepsilon + K\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\alpha + 4\\varepsilon \\\\\n-\\alpha - 4\\varepsilon \\\\\n-\\alpha - 4\\varepsilon \\\\\n\\alpha + 4\\varepsilon\n\\end{pmatrix}\n= (\\alpha + 4\\varepsilon) \\begin{pmatrix} 1 \\\\ -1 \\\\ -1 \\\\ 1 \\end{pmatrix}\n= (\\alpha + 4\\varepsilon)v\n$$\nThus, $v = (1, -1, -1, 1)^{\\top}$ is an eigenvector of $A$ with eigenvalue $\\lambda_v = \\alpha + 4\\varepsilon$. Since $\\varepsilon$ corresponds to weak connections, we assume $\\varepsilon$ is small, making $\\lambda_v$ a small eigenvalue and confirming that $v$ is a low-energy mode.\n\nSecond, we analyze the coarse-grid correction. The coarse-grid correction applied to an error vector $e$ is given by $c = P A_c^{-1} P^{\\top} A e$. The operator $P \\in \\mathbb{R}^{4 \\times 2}$ maps from the $2$-dimensional coarse space to the $4$-dimensional fine space. The correction vector $c$ is formed by multiplying a coarse-space vector, $e_c = A_c^{-1} P^{\\top} A e$, by the interpolation operator $P$. By definition, any vector of the form $P e_c$ lies in the range of $P$, denoted $\\mathrm{range}(P)$. Therefore, the coarse-grid correction can only introduce or remove error components that lie in $\\mathrm{range}(P)$.\nThe interpolation operator $P$ is given by $P = ((1, 0)^{\\top}, (0, 1)^{\\top}, (1, 0)^{\\top}, (0, 1)^{\\top})^{\\top}$. The basis vectors for $\\mathrm{range}(P)$ are $p_1 = (1, 0, 1, 0)^{\\top}$ and $p_2 = (0, 1, 0, 1)^{\\top}$. A vector $z=(z_1, z_2, z_3, z_4)^{\\top}$ is in $\\mathrm{range}(P)$ if and only if $z_1=z_3$ and $z_2=z_4$.\nThe identified low-energy mode is $v=(1, -1, -1, 1)^{\\top}$. Here, $v_1=1 \\neq v_3=-1$ and $v_2=-1 \\neq v_4=1$. Thus, $v$ is not in the range of $P$. In fact, $v$ is orthogonal to $\\mathrm{range}(P)$:\n$$\nv^{\\top}p_1 = (1)(1) + (-1)(0) + (-1)(1) + (1)(0) = 0 \\\\\nv^{\\top}p_2 = (1)(0) + (-1)(1) + (-1)(0) + (1)(1) = 0\n$$\nSince the correction lies entirely within $\\mathrm{range}(P)$, it is orthogonal to $v$ and cannot reduce this error component at all. This demonstrates a fundamental flaw in the choice of $P$.\nTo explicitly show that the interpolation fails to approximate this mode as requested, we compute $P^{\\top}Av$:\n$$\nP^{\\top}Av = P^{\\top} (\\lambda_v v) = (\\alpha + 4\\varepsilon) P^{\\top}v\n$$\n$$\nP^{\\top}v = \\begin{pmatrix} 1 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\\\ -1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1(1) + 0(-1) + 1(-1) + 0(1) \\\\ 0(1) + 1(-1) + 0(-1) + 1(1) \\end{pmatrix} = \\begin{pmatrix} 1-1 \\\\ -1+1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nTherefore, $P^{\\top}Av = 0$. The residual of the error mode $v$ is invisible on the coarse grid ($r_c = P^{\\top}r_f = -P^{\\top}Ae = 0$ if $e=v$). Consequently, the coarse-grid solver computes a zero correction ($e_c = A_c^{-1}r_c = 0$), and the correction step completely fails to address this low-energy error mode.\n\nThird, we derive the two-grid error-propagation operator $E = (I - P A_c^{-1} P^{\\top} A) S$ acting on $v$ and find the corresponding eigenvalue. The process involves two stages: smoothing and coarse-grid correction.\nLet the initial error be $v$.\n1. Smoothing: The error after smoothing is $S v$.\n$S = I - \\omega D^{-1} A$, where $D = \\mathrm{diag}(A)$. From the definition of $A$, all diagonal elements are identical: $D_{ii} = \\alpha + 2\\varepsilon + K$. So, $D = (\\alpha + 2\\varepsilon + K)I$.\nApplying $S$ to the eigenvector $v$:\n$$\nSv = (I - \\omega D^{-1} A)v = v - \\omega ((\\alpha + 2\\varepsilon + K)I)^{-1} (Av) = v - \\frac{\\omega}{\\alpha + 2\\varepsilon + K} (\\lambda_v v)\n$$\nSubstituting $\\lambda_v = \\alpha + 4\\varepsilon$:\n$$\nSv = \\left(1 - \\omega \\frac{\\alpha + 4\\varepsilon}{\\alpha + 2\\varepsilon + K}\\right)v\n$$\n2. Coarse-Grid Correction: The error after smoothing, $Sv$, is then acted upon by the coarse-grid correction operator, $E_{CGC} = I - P A_c^{-1} P^{\\top} A$.\n$$\nE v = E_{CGC}(Sv) = E_{CGC} \\left( \\left(1 - \\omega \\frac{\\alpha + 4\\varepsilon}{\\alpha + 2\\varepsilon + K}\\right)v \\right)\n$$\nSince the term in parentheses is a scalar, we can pull it out:\n$$\nE v = \\left(1 - \\omega \\frac{\\alpha + 4\\varepsilon}{\\alpha + 2\\varepsilon + K}\\right) (I - P A_c^{-1} P^{\\top} A)v\n$$\nThe second part is $(I - P A_c^{-1} P^{\\top} A)v = v - P A_c^{-1} (P^{\\top}Av)$. As we computed earlier, $P^{\\top}Av = 0$.\nThus, the correction term is $P A_c^{-1} (0) = 0$. The coarse-grid correction operator acts as the identity on $v$: $(I - P A_c^{-1} P^{\\top} A)v = v$.\nCombining the results:\n$$\nE v = \\left(1 - \\omega \\frac{\\alpha + 4\\varepsilon}{\\alpha + 2\\varepsilon + K}\\right) v\n$$\nThis shows that $v$ is also an eigenvector of the full two-grid operator $E$. The eigenvalue, which represents the factor by which this error mode is reduced per iteration, is the term in the parenthesis. This value represents the degradation in convergence. Since $K \\gg \\varepsilon$, the fraction is small, and the eigenvalue is close to $1$, indicating very poor convergence for this mode, entirely limited by the ineffective smoothing factor. The coarse-grid correction, which should efficiently remove such a mode, fails completely.\nThe required closed-form eigenvalue is the convergence factor for this mode.",
            "answer": "$$\n\\boxed{1 - \\omega \\frac{\\alpha + 4\\varepsilon}{\\alpha + 2\\varepsilon + K}}\n$$"
        },
        {
            "introduction": "The previous exercises demonstrate that the quality of the coarse-level representation is paramount for rapid convergence. This final hands-on practice moves beyond fixed, pre-determined hierarchies to the modern concept of adaptive AMG. You will implement a solver that diagnoses its own performance during the solution process and refines its components on-the-fly, providing a glimpse into how practical, robust solvers are designed to intelligently handle complex and unforeseen problem features .",
            "id": "3204385",
            "problem": "Implement a two-level adaptive Algebraic Multigrid (AMG) solver that monitors coarse-correction effectiveness per aggregate and refines aggregates on-the-fly, thereby modifying the interpolation operator during the solve phase. Consider solving the symmetric positive definite (SPD) linear system $A x = b$, where $A$ is the one-dimensional discrete Laplacian matrix of size $n \\times n$ with $2$ on the diagonal and $-1$ on the immediate off-diagonals. This matrix arises from the second-order finite-difference discretization of the negative Laplace operator with homogeneous Dirichlet boundary conditions.\n\nStart from the following foundational definitions and widely accepted numerical facts:\n\n- An SPD matrix $A$ defines the quadratic form $x^\\top A x > 0$ for any nonzero $x$. The discrete Laplacian described above is SPD.\n- The linear system residual is $r = b - A x$. A stationary iterative method updates $x$ to reduce the residual norm $\\lVert r \\rVert_2$.\n- The Jacobi smoother updates $x$ using $x \\leftarrow x + \\omega D^{-1} (b - A x)$, where $D = \\operatorname{diag}(A)$ and $\\omega \\in (0, 1)$.\n- In aggregation-based AMG, tentative prolongation $P_t$ maps fine-grid variables into aggregates by piecewise constant interpolation, i.e., for fine index $i$ in aggregate $j$, $(P_t)_{i j} = 1$, and zero otherwise. Smoothed prolongation $P$ is obtained by applying a smoothing operator $S$ to $P_t$; a standard choice is $S = I - \\omega_s D^{-1} A$, with smoothing parameter $\\omega_s \\in (0, 1)$, yielding $P = S P_t$. The restriction is chosen as $R = P^\\top$ and the coarse operator is $A_c = P^\\top A P$ (Galerkin condition).\n- A two-level AMG $V$-cycle consists of pre-smoothing, coarse correction, and post-smoothing. Coarse correction aims to reduce error components not eliminated efficiently by smoothing.\n\nYour task is to implement a two-level adaptive smoothed aggregation AMG algorithm with the following adaptive mechanism:\n\n1. Construct initial aggregates by grouping contiguous fine-grid indices into blocks of prescribed size. Build the tentative prolongation $P_t$, smoothed prolongation $P$, restriction $R = P^\\top$, and coarse operator $A_c = P^\\top A P$.\n2. In each iteration:\n   - Apply $n_{\\text{pre}}$ steps of Jacobi pre-smoothing to $x$ with parameter $\\omega$ to obtain an intermediate iterate $x_{\\text{pre}}$ and residual $r_{\\text{pre}} = b - A x_{\\text{pre}}$.\n   - Perform coarse correction by solving $A_c y = R r_{\\text{pre}}$ for $y$, then set $x_{\\text{cc}} = x_{\\text{pre}} + P y$.\n   - Define the per-aggregate coarse-correction effectiveness ratio for aggregate $j$ as\n     $$\\rho_j = \\frac{\\lVert r_{\\text{post-cc}}^{(j)} \\rVert_2}{\\lVert r_{\\text{pre}}^{(j)} \\rVert_2},$$\n     where $r_{\\text{post-cc}} = b - A x_{\\text{cc}}$ and $r^{(j)}$ is the restriction of the residual to the fine indices belonging to aggregate $j$. If $\\lVert r_{\\text{pre}}^{(j)} \\rVert_2 = 0$, define $\\rho_j = 0$.\n   - If $\\rho_j$ exceeds a prescribed threshold, refine aggregate $j$ by splitting its contiguous fine indices into two sub-aggregates (a left half and a right half). Rebuild $P$, $R$, and $A_c$ immediately after refinement. Limit the total number of aggregate refinements by a global maximum.\n   - Apply $n_{\\text{post}}$ steps of Jacobi post-smoothing to obtain $x$ for the next iteration.\n3. Repeat until either the residual norm $\\lVert b - A x \\rVert_2$ falls below a tolerance or the maximum number of iterations is reached. The refinement step closes the loop between the solve and setup phases, enabling on-the-fly adjustment of the hierarchy.\n\nThe program must implement the above for a set of test cases and report, for each case, a list containing:\n- the final residual norm $\\lVert b - A x \\rVert_2$ (a float),\n- the total number of iterations performed (an integer),\n- the total number of aggregate refinements applied (an integer).\n\nUse the forcing vector $b$ defined by $b_i = 1$ for all fine-grid indices $i = 1, \\dots, n$. The initial guess must be $x = 0$. There are no physical units in this purely mathematical formulation.\n\nTest Suite:\nProvide results for the following parameter sets $(n, n_{\\text{pre}}, n_{\\text{post}}, \\omega, \\omega_s, \\text{agg\\_size}, \\text{refine\\_threshold}, \\text{max\\_refinements}, \\text{max\\_iters}, \\text{tol})$:\n- Case 1 (happy path): $(64, 1, 1, 0.7, 0.7, 2, 0.8, 10, 20, 10^{-6})$.\n- Case 2 (aggressive coarsening with minimal smoothing, edge refinement): $(32, 0, 0, 0.7, 0.7, 4, 0.5, 5, 10, 10^{-8})$.\n- Case 3 (boundary, no refinement allowed): $(16, 1, 1, 0.7, 0.7, 2, 0.95, 0, 15, 10^{-6})$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each entry of this outer list corresponds to one test case, and must be a list of the form $[\\lVert b - A x \\rVert_2, \\text{iterations}, \\text{refinements}]$. For example, the output must look like\n$[[r_1, i_1, u_1],[r_2, i_2, u_2],[r_3, i_3, u_3]]$\nwith numerical values in place of the symbols.",
            "solution": "The user has provided a scientifically valid and well-posed problem in the field of numerical linear algebra. The task is to implement and test an adaptive two-level smoothed aggregation algebraic multigrid (AMG) method.\n\n### Problem Formulation\n\nThe objective is to solve the linear system of equations $A x = b$, where $A$ is an $n \\times n$ symmetric positive definite (SPD) matrix, $x$ is the vector of unknowns, and $b$ is the forcing vector. Specifically, $A$ is the one-dimensional discrete Laplacian matrix given by:\n$$\nA_{ij} = \n\\begin{cases}\n2 & \\text{if } i = j \\\\\n-1 & \\text{if } |i - j| = 1 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nThis matrix arises from a finite difference discretization of the negative second derivative operator, $-u''(x)$, with homogeneous Dirichlet boundary conditions. The SPD property, $v^\\top A v > 0$ for any non-zero vector $v \\in \\mathbb{R}^n$, ensures that the system has a unique solution. The-right hand side vector is defined as $b_i = 1$ for all $i=1, \\dots, n$, and the initial guess for the solution is the zero vector, $x_0 = 0$.\n\n### Algorithmic Framework: Adaptive Two-Level Smoothed Aggregation AMG\n\nThe core of the method is a two-level iterative scheme that refines its own components during the solution process. A standard two-level cycle consists of pre-smoothing, coarse-grid correction, and post-smoothing. The adaptive feature introduces a mechanism to modify the multigrid hierarchy—specifically, the aggregates and associated operators—based on performance metrics calculated within each iteration.\n\n#### Initial Setup Phase\n\nBefore the iterative process begins, an initial multigrid hierarchy must be constructed.\n\n1.  **Aggregation:** The set of fine-grid indices $\\{0, 1, \\dots, n-1\\}$ is partitioned into a set of disjoint subsets called aggregates. The initial aggregation strategy is to group contiguous indices into blocks of a prescribed size, `agg_size`. If $n$ is not a multiple of `agg_size`, the final aggregate will contain the remaining indices. Let there be $n_c$ aggregates, $C_0, C_1, \\dots, C_{n_c-1}$.\n\n2.  **Transfer Operators:**\n    *   **Tentative Prolongation ($P_t$):** A tentative prolongation operator, $P_t$, is an $n \\times n_c$ matrix that maps a vector from the coarse space (one value per aggregate) to the fine-grid space. It is defined by piecewise constant interpolation. For a fine-grid index $i$ belonging to aggregate $C_j$, the entry is $(P_t)_{ij} = 1$; otherwise, it is $0$.\n    *   **Smoothed Prolongation ($P$):** To improve the interpolation quality, $P_t$ is smoothed. The smoother, $S$, is based on the Jacobi iteration and is defined as $S = I - \\omega_s D^{-1} A$, where $D = \\text{diag}(A)$, $I$ is the identity matrix, and $\\omega_s$ is a smoothing parameter. The final smoothed prolongation operator is $P = S P_t$. For the given matrix $A$, $D=2I$, so the smoother simplifies to $S = I - \\frac{\\omega_s}{2} A$.\n    *   **Restriction ($R$):** The restriction operator, $R$, which maps a fine-grid vector to the coarse space, is chosen as the transpose of the prolongation operator, $R = P^\\top$. This choice is standard for SPD problems.\n\n3.  **Coarse-Grid Operator ($A_c$):** The coarse-grid operator, $A_c$, is formed using the Galerkin condition: $A_c = R A P = P^\\top A P$. This construction ensures that if $A$ is SPD, then $A_c$ is also SPD, preserving the character of the problem on the coarser level.\n\n#### Iterative Solve Phase with Adaptation\n\nThe solver proceeds through a sequence of iterations, each constituting an adaptive V-cycle. Let $x_k$ be the approximate solution at the beginning of iteration $k$.\n\n1.  **Pre-smoothing:** A few steps ($n_{\\text{pre}}$) of a stationary iterative method are applied to reduce high-frequency error components. The Jacobi smoother is used:\n    $$x \\leftarrow x + \\omega D^{-1} (b - A x)$$\n    After $n_{\\text{pre}}$ steps, we obtain an intermediate solution $x_{\\text{pre}}$ and its corresponding residual $r_{\\text{pre}} = b - A x_{\\text{pre}}$.\n\n2.  **Coarse-Grid Correction:** This step addresses the low-frequency error components that smoothing fails to eliminate efficiently.\n    *   The fine-grid residual $r_{\\text{pre}}$ is restricted to the coarse grid: $r_c = R r_{\\text{pre}}$.\n    *   The coarse-grid residual equation, $A_c y = r_c$, is solved for the coarse-grid correction $y$. Since $A_c$ is small, this can be done with a direct solver.\n    *   The correction is prolongated back to the fine grid and added to the solution: $x_{\\text{cc}} = x_{\\text{pre}} + P y$.\n\n3.  **Adaptive Refinement:** This is the crucial adaptive step. We evaluate the effectiveness of the coarse-grid correction for each aggregate.\n    *   The residual after correction, $r_{\\text{post-cc}} = b - A x_{\\text{cc}}$, is computed.\n    *   For each aggregate $C_j$, we define the local residuals $r_{\\text{pre}}^{(j)}$ and $r_{\\text{post-cc}}^{(j)}$ by restricting the global residuals $r_{\\text{pre}}$ and $r_{\\text{post-cc}}$ to the indices belonging to $C_j$.\n    *   The coarse-correction effectiveness ratio is calculated:\n        $$\\rho_j = \\frac{\\lVert r_{\\text{post-cc}}^{(j)} \\rVert_2}{\\lVert r_{\\text{pre}}^{(j)} \\rVert_2}$$\n        If $\\lVert r_{\\text{pre}}^{(j)} \\rVert_2 = 0$, we define $\\rho_j = 0$. A ratio $\\rho_j \\approx 0$ indicates effective correction, while a ratio near $1$ or higher indicates poor correction.\n    *   If $\\rho_j$ exceeds a given `refine_threshold`, and the total number of refinements has not reached `max_refinements`, the aggregate $C_j$ is flagged for refinement. Refinement consists of splitting the aggregate (a set of contiguous indices) into two smaller, non-empty, contiguous sub-aggregates.\n\n4.  **Hierarchy Update:** If any aggregates were refined in the previous step, the entire AMG hierarchy must be rebuilt to reflect the new aggregation scheme. This involves re-running the setup phase (steps 1-3) with the updated list of aggregates to generate new operators $P_t, P, R, A_c$. This on-the-fly modification couples the solve and setup phases.\n\n5.  **Post-smoothing:** Finally, $n_{\\text{post}}$ steps of the Jacobi smoother are applied to the corrected solution $x_{\\text{cc}}$ to smooth out any high-frequency errors introduced by the prolongation step. This produces the final solution for the current iteration, $x_{k+1}$.\n\n6.  **Convergence Check:** The algorithm terminates if the Euclidean norm of the final residual, $\\lVert b - A x_{k+1} \\rVert_2$, falls below a specified tolerance `tol`, or if the maximum number of iterations `max_iters` is reached.\n\nThis adaptive strategy aims to dynamically improve the quality of the coarse-grid representation by refining aggregates that are poorly handled by the current coarse-grid correction, thereby enhancing the overall convergence of the solver.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef build_laplacian(n):\n    \"\"\"Constructs the 1D discrete Laplacian matrix of size n x n.\"\"\"\n    A = np.zeros((n, n))\n    for i in range(n):\n        A[i, i] = 2.0\n    for i in range(n - 1):\n        A[i, i + 1] = -1.0\n        A[i + 1, i] = -1.0\n    return A\n\ndef build_initial_aggregates(n, agg_size):\n    \"\"\"Groups contiguous indices into aggregates of a given size.\"\"\"\n    aggregates = []\n    current_index = 0\n    while current_index < n:\n        end = min(current_index + agg_size, n)\n        aggregates.append(list(range(current_index, end)))\n        current_index = end\n    return aggregates\n\ndef build_amg_operators(A, aggregates, omega_s):\n    \"\"\"Constructs the smoothed prolongation, restriction, and coarse-grid operators.\"\"\"\n    n = A.shape[0]\n    num_aggregates = len(aggregates)\n    \n    # 1. Tentative Prolongation (Pt)\n    Pt = np.zeros((n, num_aggregates))\n    for j, agg in enumerate(aggregates):\n        Pt[agg, j] = 1.0\n\n    # 2. Smoothed Prolongation (P)\n    # S = I - omega_s * D^-1 * A. For our Laplacian, D = 2*I, so D^-1 = 0.5*I\n    S = np.eye(n) - (omega_s / 2.0) * A\n    P = S @ Pt\n\n    # 3. Restriction (R)\n    R = P.T\n\n    # 4. Coarse-grid Operator (Ac)\n    Ac = R @ A @ P\n    \n    return P, R, Ac\n\ndef run_adaptive_amg_case(params):\n    \"\"\"\n    Executes a single test case of the adaptive AMG solver.\n    \"\"\"\n    (n, n_pre, n_post, omega, omega_s, agg_size, \n     refine_threshold, max_refinements, max_iters, tol) = params\n\n    # 1. System Setup\n    A = build_laplacian(n)\n    b = np.ones(n)\n    x = np.zeros(n)\n    D_inv = np.eye(n) * 0.5  # D = 2I for our matrix\n\n    # 2. Initial Hierarchy Setup\n    aggregates = build_initial_aggregates(n, agg_size)\n    P, R, Ac = build_amg_operators(A, aggregates, omega_s)\n    \n    total_refinements = 0\n    final_residual_norm = np.linalg.norm(b - A @ x)\n    \n    # 3. Iterative Solver\n    iters_performed = 0\n    for iter_count in range(1, max_iters + 1):\n        iters_performed = iter_count\n        \n        # --- Pre-smoothing ---\n        x_pre = x.copy()\n        for _ in range(n_pre):\n            r_smooth = b - A @ x_pre\n            x_pre += omega * (D_inv @ r_smooth)\n        r_pre = b - A @ x_pre\n        \n        # --- Coarse-Grid Correction ---\n        r_c = R @ r_pre\n        # Use pseudo-inverse for stability in case Ac is ill-conditioned\n        y = np.linalg.pinv(Ac) @ r_c \n        x_cc = x_pre + P @ y\n        \n        # --- Adaptive Refinement ---\n        r_post_cc = b - A @ x_cc\n        new_aggregates = []\n        hierarchy_changed = False\n        \n        for agg_indices in aggregates:\n            # An aggregate must have at least 2 points to be splittable\n            can_split = len(agg_indices) > 1\n            if not can_split:\n                new_aggregates.append(agg_indices)\n                continue\n\n            r_pre_j_norm = np.linalg.norm(r_pre[agg_indices])\n            if r_pre_j_norm == 0.0:\n                rho_j = 0.0\n            else:\n                r_post_cc_j_norm = np.linalg.norm(r_post_cc[agg_indices])\n                rho_j = r_post_cc_j_norm / r_pre_j_norm\n            \n            should_refine = rho_j > refine_threshold\n            can_refine = total_refinements < max_refinements\n\n            if should_refine and can_refine:\n                m = len(agg_indices)\n                k = m // 2\n                agg1 = agg_indices[:k]\n                agg2 = agg_indices[k:]\n                new_aggregates.append(agg1)\n                new_aggregates.append(agg2)\n                total_refinements += 1\n                hierarchy_changed = True\n            else:\n                new_aggregates.append(agg_indices)\n        \n        if hierarchy_changed:\n            aggregates = new_aggregates\n            P, R, Ac = build_amg_operators(A, aggregates, omega_s)\n        \n        # --- Post-smoothing ---\n        x_post = x_cc.copy()\n        for _ in range(n_post):\n            r_smooth = b - A @ x_post\n            x_post += omega * (D_inv @ r_smooth)\n        x = x_post\n\n        # --- Convergence Check ---\n        final_residual_norm = np.linalg.norm(b - A @ x)\n        if final_residual_norm < tol:\n            break\n            \n    return [final_residual_norm, iters_performed, total_refinements]\n\n\ndef solve():\n    \"\"\"\n    Defines and runs the test suite for the adaptive AMG solver.\n    \"\"\"\n    # (n, n_pre, n_post, omega, omega_s, agg_size, refine_threshold, max_refinements, max_iters, tol)\n    test_cases = [\n        (64, 1, 1, 0.7, 0.7, 2, 0.8, 10, 20, 1e-6),\n        (32, 0, 0, 0.7, 0.7, 4, 0.5, 5, 10, 1e-8),\n        (16, 1, 1, 0.7, 0.7, 2, 0.95, 0, 15, 1e-6),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_adaptive_amg_case(case)\n        results.append(result)\n\n    # Format the final output as a string representing a list of lists.\n    # Individual list items are converted to strings, then joined.\n    # Finally, the whole thing is wrapped in brackets.\n    result_str = \",\".join(map(str, results))\n    print(f\"[{result_str}]\")\n\nsolve()\n\n```"
        }
    ]
}