{"hands_on_practices": [{"introduction": "要真正掌握哈密尔顿-雅可比-贝尔曼（HJB）方程，最好的方法莫过于亲手求解。我们的第一个实践将处理随机控制中的一个基石问题：线性二次（LQ）调节器。通过这个练习，我们将看到如何通过假设价值函数具有特定的二次形式，将复杂的HJB偏微分方程转化为一个更易于处理的代数方程——黎卡提方程（Riccati equation），从而得到一个优雅的闭式解。[@problem_id:3001633]", "problem": "考虑一维受控Ornstein–Uhlenbeck (OU) 扩散的无穷时间域折扣随机控制问题。状态过程 $\\{X_{t}\\}_{t \\ge 0}$ 遵循以下受控随机微分方程演化：\n$$\n\\mathrm{d}X_{t} = \\big(-\\theta X_{t} + \\beta u_{t}\\big)\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t}, \\quad X_{0}=x,\n$$\n其中 $W_{t}$ 是一个标准布朗运动，$\\theta>0$, $\\beta\\neq 0$, $\\sigma>0$ 是给定常数。控制过程 $\\{u_{t}\\}_{t \\ge 0}$ 关于 $W_{t}$ 的滤子是循序可测的，并在 $\\mathbb{R}$ 中取值。目标是在所有容许控制下，最小化折扣成本泛函：\n$$\nJ^{u}(x) \\equiv \\mathbb{E}\\!\\left[\\int_{0}^{\\infty} \\exp(-\\rho t)\\big(q X_{t}^{2} + r u_{t}^{2}\\big)\\,\\mathrm{d}t\\right],\n$$\n其中 $\\rho>0$, $q>0$, 和 $r>0$ 是给定常数。令 $V(x)\\equiv \\inf_{u} J^{u}(x)$ 表示值函数。\n\n从动态规划原理、扩散算子和折扣期望成本的定义出发，推导 $V(x)$ 的 Hamilton–Jacobi–Bellman (HJB) 方程，解出其显式闭式解，并获得最优平稳马尔可夫反馈 $u^{\\ast}(x)$。解释最优反馈如何修改线性漂移，以及为何这会导致闭环OU动力学的镇定。你的最终答案必须以单个解析表达式的形式给出，该表达式包含闭式值函数 $V(x)$ 和最优反馈 $u^{\\ast}(x)$，写成一个 $1\\times 2$ 的行矩阵，其中第一个元等于 $V(x)$，第二个元等于 $u^{\\ast}(x)$。无需进行数值计算，也无需进行四舍五入。", "solution": "我们从受控扩散开始：\n$$\n\\mathrm{d}X_{t} = b(X_{t},u_{t})\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t}, \\quad b(x,u) \\equiv -\\theta x + \\beta u,\n$$\n以及无穷时间域折扣成本：\n$$\nJ^{u}(x) = \\mathbb{E}\\!\\left[\\int_{0}^{\\infty}\\exp(-\\rho t)\\,\\ell(X_{t},u_{t})\\,\\mathrm{d}t\\right], \\quad \\ell(x,u) \\equiv q x^{2} + r u^{2},\n$$\n其中 $\\rho>0$, $\\theta>0$, $q>0$, $r>0$, $\\sigma>0$, 且 $\\beta\\neq 0$。动态规划原理断言，如果 $V$ 是值函数，则在足够的正则性条件下，$V$ 满足 Hamilton–Jacobi–Bellman (HJB) 方程。对于一个二次连续可微的 $V$ 和折扣因子 $\\rho>0$，该扩散控制问题的 HJB 方程为：\n$$\n\\rho V(x) = \\inf_{u\\in\\mathbb{R}}\\Big\\{\\ell(x,u) + \\mathcal{L}^{u}V(x)\\Big\\},\n$$\n其中 $\\mathcal{L}^{u}$ 是作用于光滑检验函数 $f$ 的受控算子，其定义为：\n$$\n\\mathcal{L}^{u} f(x) \\equiv b(x,u) f'(x) + \\tfrac{1}{2}\\sigma^{2} f''(x) = \\big(-\\theta x + \\beta u\\big) f'(x) + \\tfrac{1}{2}\\sigma^{2} f''(x).\n$$\n因此 HJB 方程的具体形式为：\n$$\n\\rho V(x) = \\inf_{u\\in\\mathbb{R}}\\left\\{q x^{2} + r u^{2} + \\big(-\\theta x + \\beta u\\big)V'(x) + \\tfrac{1}{2}\\sigma^{2} V''(x)\\right\\}.\n$$\n\n我们现在求解这个方程。该结构在 $(x,u)$ 中是线性二次的，很自然地可以寻找以下形式的解：\n$$\nV(x) = P x^{2} + C,\n$$\n其中常数 $P$ 和 $C$ 待定。我们计算导数 $V'(x) = 2 P x$ 和 $V''(x) = 2 P$。代入 HJB 方程得到：\n$$\n\\rho \\big(P x^{2} + C\\big) = \\inf_{u\\in\\mathbb{R}}\\left\\{q x^{2} + r u^{2} + \\big(-\\theta x + \\beta u\\big)\\,2 P x + \\tfrac{1}{2}\\sigma^{2}\\cdot 2 P\\right\\}.\n$$\n关于 $u$ 的最小化涉及二次函数：\n$$\n\\Phi(u;x) \\equiv r u^{2} + 2 P \\beta x\\, u + \\big(q x^{2} - 2\\theta P x^{2} + \\sigma^{2} P\\big).\n$$\n对于每个固定的 $x$，最小化子 $u^{\\ast}(x)$ 满足一阶条件：\n$$\n\\frac{\\partial \\Phi}{\\partial u}(u;x) = 2 r u + 2 P \\beta x = 0,\n$$\n这得出了平稳反馈：\n$$\nu^{\\ast}(x) = -\\frac{\\beta P}{r}\\,x.\n$$\n二阶导数 $\\frac{\\partial^{2}\\Phi}{\\partial u^{2}} = 2 r>0$ 确认这确实是全局最小化子。将 $u^{\\ast}$ 代回，我们在最小值处进行求值：\n\n$$\n\\begin{aligned}\nr \\big(u^{\\ast}(x)\\big)^{2} = r \\left(\\frac{\\beta^{2} P^{2}}{r^{2}} x^{2}\\right) = \\frac{\\beta^{2} P^{2}}{r} x^{2},\\\\\n\\big(-\\theta x + \\beta u^{\\ast}(x)\\big) V'(x) = \\big(-\\theta x - \\beta \\tfrac{\\beta P}{r} x\\big)\\,2 P x = \\big(-2\\theta P - \\tfrac{2\\beta^{2} P^{2}}{r}\\big) x^{2},\\\\\n\\tfrac{1}{2}\\sigma^{2} V''(x) = \\sigma^{2} P.\n\\end{aligned}\n$$\n\n因此，在最小化之后，HJB 方程变为：\n$$\n\\rho P x^{2} + \\rho C = \\left[q - 2\\theta P - \\frac{\\beta^{2}}{r} P^{2}\\right] x^{2} + \\sigma^{2} P.\n$$\n匹配 $x^{2}$ 的系数和常数项，得到耦合代数方程组：\n\n$$\n\\begin{cases}\n\\rho P = q - 2\\theta P - \\dfrac{\\beta^{2}}{r} P^{2}, \\\\[6pt]\n\\rho C = \\sigma^{2} P.\n\\end{cases}\n$$\n\n第一个方程是关于 $P$ 的代数Riccati方程：\n$$\n\\frac{\\beta^{2}}{r} P^{2} + (\\rho + 2\\theta) P - q = 0.\n$$\n它的两个根是：\n$$\nP_{\\pm} = \\frac{ -(\\rho + 2\\theta) \\pm \\sqrt{(\\rho + 2\\theta)^{2} + \\dfrac{4\\beta^{2}}{r} q} }{ 2 \\dfrac{\\beta^{2}}{r} } = \\frac{r}{2\\beta^{2}}\\left( -(\\rho + 2\\theta) \\pm \\sqrt{(\\rho + 2\\theta)^{2} + \\frac{4\\beta^{2}}{r} q} \\right).\n$$\n因为 $q>0$，$r>0$ 且 $\\beta\\neq 0$，判别式严格大于 $(\\rho + 2\\theta)^{2}$，所以平方根的值超过 $\\rho + 2\\theta$。因此 $P_{-}  0$ 并且\n$$\nP_{+} = \\frac{r}{2\\beta^{2}}\\left( -(\\rho + 2\\theta) + \\sqrt{(\\rho + 2\\theta)^{2} + \\frac{4\\beta^{2}}{r} q} \\right)  0.\n$$\n$V$ 的凸性和值函数的有限性要求我们选择正的镇定解，因此我们取 $P=P_{+}$。于是常数 $C$ 为：\n$$\nC = \\frac{\\sigma^{2}}{\\rho}\\,P_{+}.\n$$\n\n汇总起来，值函数和最优反馈为：\n\n$$\n\\begin{aligned}\nV(x) = P_{+} x^{2} + \\frac{\\sigma^{2}}{\\rho}\\,P_{+},\\\\\nu^{\\ast}(x) = -\\frac{\\beta P_{+}}{r}\\,x = \\frac{(\\rho + 2\\theta) - \\sqrt{(\\rho + 2\\theta)^{2} + \\dfrac{4\\beta^{2}}{r} q}}{2\\beta}\\,x.\n\\end{aligned}\n$$\n\n$u^{\\ast}(x)$ 的等式是通过代入 $P_{+}$ 的表达式得到的。为了解释镇定作用，我们观察到在 $u^{\\ast}$ 下，闭环漂移变为：\n$$\n-\\theta x + \\beta u^{\\ast}(x) = -\\left(\\theta + \\frac{\\beta^{2}}{r} P_{+}\\right) x,\n$$\n其中 $\\theta + \\dfrac{\\beta^{2}}{r} P_{+}  \\theta  0$。因此，闭环中的线性漂移系数比开环中严格更负，从而得到一个向原点有更强均值回归的Ornstein–Uhlenbeck过程。这种负线性反馈正是在存在扩散噪声的情况下最小化长期折扣二次成本的镇定作用。", "answer": "$$\\boxed{\\begin{pmatrix}\n\\frac{r}{2\\beta^{2}}\\!\\left(\\! -(\\rho+2\\theta)+\\sqrt{(\\rho+2\\theta)^{2}+\\frac{4\\beta^{2}}{r}q}\\,\\right) x^{2}+\\frac{\\sigma^{2}}{\\rho}\\cdot \\frac{r}{2\\beta^{2}}\\!\\left(\\! -(\\rho+2\\theta)+\\sqrt{(\\rho+2\\theta)^{2}+\\frac{4\\beta^{2}}{r}q}\\,\\right) \n\\frac{(\\rho+2\\theta)-\\sqrt{(\\rho+2\\theta)^{2}+\\frac{4\\beta^{2}}{r}q}}{2\\beta}\\,x\n\\end{pmatrix}}$$", "id": "3001633"}, {"introduction": "尽管线性二次问题能够通过光滑的价值函数求解，但这并非普遍情况。本练习将展示一个看似简单的确定性最优控制问题，其中一个满足边界条件的光滑候选函数，在区域内部却无法满足HJB方程。这个动手检验揭示了经典解的局限性，并强调了为何像粘性解（viscosity solutions）这样更普适的理论对于HJB方程至关重要。[@problem_id:2752646]", "problem": "考虑定义在区域 $\\Omega := (-1,1)$ 上的确定性控制系统，其动力学由 $\\dot{x}(t) = u(t)$ 给出，控制约束为对所有 $t \\geq 0$ 都有 $u(t) \\in [-1,1]$。设退出时间为 $\\tau := \\inf\\{ t \\geq 0 : |x(t)| = 1 \\}$，代价泛函为 $J(x_{0};u(\\cdot)) := \\int_{0}^{\\tau} 1 \\, dt$，其中 $x(0)=x_{0} \\in \\Omega$。将值函数 $V(x_{0})$ 定义为 $J(x_{0};u(\\cdot))$ 在所有取值于 $[-1,1]$ 的可测控制 $u(\\cdot)$ 上的下确界。\n\n从动态规划原理出发，为任意满足边界条件 $W(x)=0$（对于 $|x|=1$）的足够光滑的候选函数 $W:\\Omega \\to \\mathbb{R}$ 推导其逐点一致性条件，并说明 Hamilton-Jacobi-Bellman (HJB, 哈密顿-雅可比-贝尔曼) 方程如何在 $\\Omega$ 内部通过一个适当的残差来约束 $W$。然后，取一个满足边界条件 $W(\\pm 1)=0$ 的特定光滑候选函数 $W(x) := 1 - x^{2}$。使用您推导出的 HJB 一致性条件，计算在 $x=0$ 处的逐点残差。\n\n您的最终答案必须是对应于该残差在 $x=0$ 处的值的单个实数。无需四舍五入。", "solution": "该问题要求从动态规划原理 (DPP) 出发，推导 Hamilton-Jacobi-Bellman (HJB) 一致性条件，然后将其应用于一个特定的候选函数。\n\n系统动力学由 $\\dot{x}(t) = u(t)$ 给出，其中状态 $x(t) \\in \\Omega := (-1,1)$，控制 $u(t) \\in [-1,1]$。代价泛函表示退出区域 $\\Omega$ 的时间：\n$$J(x_{0};u(\\cdot)) := \\int_{0}^{\\tau} 1 \\, dt$$\n其中 $\\tau := \\inf\\{t \\geq 0 : |x(t)| = 1\\}$。值函数 $V(x)$ 是此代价在所有容许控制上的下确界：$V(x) = \\inf_{u(\\cdot)} J(x; u(\\cdot))$。运行成本为 $L(x,u) = 1$。边界条件为，对于 $x \\in \\partial\\Omega$（即 $|x|=1$），有 $V(x) = 0$。\n\n动态规划原理指出，对于任何小的时间间隔 $h  0$，值函数满足：\n$$V(x) = \\inf_{u(\\cdot):[0,h]\\to[-1,1]} \\left\\{ \\int_0^h L(x(t), u(t)) \\, dt + V(x(h)) \\right\\}$$\n对于这个特定问题，这变为：\n$$V(x) = \\inf_{u(\\cdot):[0,h]\\to[-1,1]} \\left\\{ h + V(x(h)) \\right\\}$$\n假设在区间 $[0, h]$ 上，控制 $u(t)$ 是一个常数 $u \\in [-1,1]$，则状态演化为 $x(h) = x(0) + \\int_0^h u \\, ds = x + hu$。\n如果假设值函数 $V$ 是连续可微的 ($C^1$)，我们可以在 $x$ 附近将 $V(x(h))$ 展开为泰勒级数：\n$$V(x(h)) = V(x+hu) = V(x) + V'(x)(hu) + O(h^2)$$\n其中 $V'(x)$ 表示 $V$ 关于 $x$ 的导数。将此代入 DPP 方程：\n$$V(x) = \\inf_{u \\in [-1,1]} \\left\\{ h + V(x) + hV'(x)u + O(h^2) \\right\\}$$\n我们从两边减去 $V(x)$，然后除以 $h  0$：\n$$0 = \\inf_{u \\in [-1,1]} \\left\\{ 1 + V'(x)u + O(h) \\right\\}$$\n当 $h \\to 0$ 时取极限，$O(h)$ 项消失，得到值函数 $V(x)$ 在区域 $\\Omega$ 内部必须满足的静态 Hamilton-Jacobi-Bellman 方程：\n$$0 = \\inf_{u \\in [-1,1]} \\left\\{ 1 + V'(x)u \\right\\}$$\n该方程提供了逐点一致性条件。对于任何足够光滑的候选函数 $W(x)$，我们将 HJB 残差 $R_W(x)$ 定义为 HJB 表达式的值：\n$$R_W(x) := \\inf_{u \\in [-1,1]} \\left\\{ 1 + W'(x)u \\right\\}$$\n要使一个候选函数成为真正的值函数，其残差必须对所有 $x \\in \\Omega$ 都为零，并且必须满足边界条件。\n\n我们给定的候选函数是 $W(x) := 1 - x^2$。\n首先，我们验证边界条件：$W(\\pm 1) = 1 - (\\pm 1)^2 = 1 - 1 = 0$。该条件得到满足。\n接下来，我们计算 $W(x)$ 的导数：\n$$W'(x) = \\frac{d}{dx}(1 - x^2) = -2x$$\n现在我们将此导数代入残差的表达式中：\n$$R_W(x) = \\inf_{u \\in [-1,1]} \\left\\{ 1 + (-2x)u \\right\\} = \\inf_{u \\in [-1,1]} \\left\\{ 1 - 2xu \\right\\}$$\n待最小化的表达式 $1 - 2xu$ 是控制变量 $u$ 在紧集 $[-1,1]$ 上的一个线性函数。最小值必然出现在某个端点上，即 $u=-1$ 或 $u=1$。选择哪个端点取决于 $u$ 的系数 $-2x$ 的符号。\n具体来说，使表达式最小化的控制 $u^*(x)$ 是 $u^*(x) = \\text{sgn}(-2x) = -\\text{sgn}(x)$（对于 $x \\neq 0$）。\n下确界可以写为 $1 - \\sup_{u \\in [-1,1]} \\{2xu\\}$。其上确界是 $2|x|$，所以残差为：\n$$R_W(x) = 1 - 2|x|$$\n问题要求计算该逐点残差在点 $x=0$ 处的值。我们在 $x=0$ 处计算残差函数的值：\n$$R_W(0) = 1 - 2|0| = 1$$\n或者，可以直接在 $x=0$ 处计算下确界的表达式：\n$$R_W(0) = \\inf_{u \\in [-1,1]} \\{ 1 - 2(0)u \\} = \\inf_{u \\in [-1,1]} \\{ 1 \\} = 1$$\n在 $x=0$ 处的残差是 $1$。", "answer": "$$\\boxed{1}$$", "id": "2752646"}, {"introduction": "现在，让我们从理论走向实践。由于解析解非常罕见，且理论上存在挑战，数值方法变得不可或缺。这最后一个实践将指导你实现一个完整的策略迭代（policy iteration）算法来求解HJB方程，这是动态规划和强化学习的基石之一。你将对问题进行离散化，实现迭代求解过程，并将你的数值结果与已知的解析解进行验证。[@problem_id:3001638]", "problem": "从第一性原理出发，为源于一个受控随机微分方程的一维平稳折扣 Hamilton-Jacobi-Bellman 方程实现一个完整的策略迭代方案。受控状态过程由随机微分方程 $dX_t = \\left(a X_t + b u_t\\right) dt + \\sigma dW_t$ 给出，折扣率 $\\rho  0$，其中 $a$、$b$、$\\sigma$ 和 $\\rho$ 是固定的实数参数，$u_t \\in \\mathbb{R}$ 是一个控制，$W_t$ 是一个标准维纳过程。运行成本为 $\\ell(x,u) = q x^2 + r u^2$，其中 $q  0$ 且 $r  0$。从初始状态 $x$ 出发，目标是在所有容许控制上最小化折扣成本 $J^u(x) = \\mathbb{E}\\left[\\int_0^\\infty e^{-\\rho t} \\ell\\left(X_t^u, u_t\\right) dt\\right]$。令 $V(x)$ 表示值函数。\n\n您的任务是：\n- 在一个有界区间 $\\left[-x_{\\max}, x_{\\max}\\right]$ 上，推导 $V(x)$ 满足的平稳折扣 Hamilton-Jacobi-Bellman 方程，并使用狄利克雷边界条件，该边界条件选择为与该问题的精确无限时域折扣线性二次解 $V^{\\mathrm{ref}}(x)$ 相匹配。已知参考解为二次形式 $V^{\\mathrm{ref}}(x) = P x^2 + C$，其中系数 $P$ 由 Hamilton-Jacobi-Bellman 方程所蕴含的代数黎卡提关系确定，而 $C$ 的选择是为了满足常数扩散项的平稳折扣平衡。\n- 将状态空间在 $\\left[-x_{\\max}, x_{\\max}\\right]$ 上用 $N$ 个网格点进行均匀离散化，网格间距为 $h = 2 x_{\\max}/(N-1)$。在内部节点处，使用中心差分算子近似一阶导数，使用标准三点中心差分算子近似二阶导数。施加狄利克雷边界条件 $V(-x_{\\max}) = V^{\\mathrm{ref}}(-x_{\\max})$ 和 $V(x_{\\max}) = V^{\\mathrm{ref}}(x_{\\max})$。\n- 将动作空间离散化为一个有限的、对称的集合 $\\mathcal{U}_M = \\left\\{u_j\\right\\}_{j=1}^M$，该集合包含 $M$ 个在 $\\left[-u_{\\max}, u_{\\max}\\right]$ 内均匀分布的值，其中 $u_{\\max}$ 的选择方式为 $u_{\\max} = \\kappa \\left| b P x_{\\max} / r \\right|$，$\\kappa = 1.25$，以确保离散动作网格覆盖整个计算域上的连续最优解。\n- 实现策略迭代：\n  - 策略评估：对于每个内部网格点 $x_i$ 的一个固定的离散策略 $u(x_i) \\in \\mathcal{U}_M$，求解由离散化的平稳折扣 Hamilton-Jacobi-Bellman 方程产生的线性系统，以获得网格上的值向量。\n  - 策略改进：在每个内部网格点 $x_i$，使用当前的离散梯度近似 $\\left(D_x V\\right)(x_i)$ 计算离散哈密顿量的最小化子 $\\arg\\min_{u \\in \\mathcal{U}_M} \\left\\{ r u^2 + b u \\left(D_x V\\right)(x_i) \\right\\}$，并更新策略。\n  - 当值向量的无穷范数变化和策略的无穷范数变化均小于一个容差 $\\varepsilon$ 时，或者当策略迭代次数达到最大值 $K_{\\max}$ 时终止。\n- 对于下面的每个测试用例，计算网格上的均匀误差 $\\| V_{\\mathrm{num}} - V^{\\mathrm{ref}} \\|_{\\infty}$，其中 $V_{\\mathrm{num}}$ 是收敛的数值值函数，而 $V^{\\mathrm{ref}}$ 是精确的二次参考解。\n\n使用以下参数集测试套件，每个参数集以元组形式提供 $\\left(a, b, q, r, \\sigma, \\rho, x_{\\max}, N, M\\right)$:\n- 测试 1: $\\left(-0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 3.0, 161, 61\\right)$。\n- 测试 2: $\\left(0.2, 1.0, 1.0, 0.5, 0.1, 1.0, 2.0, 161, 61\\right)$。\n- 测试 3: $\\left(0.3, 1.0, 2.0, 1.0, 1.0, 2.0, 3.0, 161, 61\\right)$。\n\n实现细节与要求：\n- 从内部节点上的零策略 $u \\equiv 0$ 开始策略迭代。\n- 使用容差 $\\varepsilon = 10^{-6}$ 和最大迭代次数 $K_{\\max} = 100$。\n- 所有计算都是无量纲的；不需要物理单位。\n- 您的程序必须是一个单一、完整的脚本，无需用户输入即可为所有三个测试执行整个计算，并打印包含结果的单行内容。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表，结果按测试顺序排列，例如 $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3\\right]$，其中每个条目是对应测试在状态网格上计算出的 $\\| V_{\\mathrm{num}} - V^{\\mathrm{ref}} \\|_{\\infty}$ 的浮点值。", "solution": "### 1. Hamilton-Jacobi-Bellman 方程与精确解\n\n状态过程由线性随机微分方程控制：\n$$\ndX_t = \\left(a X_t + b u_t\\right) dt + \\sigma dW_t\n$$\n其中 $a$、$b$ 和 $\\sigma$ 是实常数，$u_t \\in \\mathbb{R}$ 是控制，$W_t$ 是一个维纳过程。目标是最小化带有折扣率 $\\rho  0$ 和二次运行成本 $\\ell(x, u) = q x^2 + r u^2$ 的折扣成本泛函：\n$$\nJ^u(x) = \\mathbb{E}\\left[\\int_0^\\infty e^{-\\rho t} \\ell\\left(X_t^u, u_t\\right) dt \\;\\middle|\\; X_0 = x \\right]\n$$\n值函数 $V(x) = \\inf_u J^u(x)$ 满足平稳 Hamilton-Jacobi-Bellman (HJB) 方程：\n$$\n\\rho V(x) = \\inf_{u \\in \\mathbb{R}} \\left\\{ \\mathcal{L}^u V(x) + \\ell(x, u) \\right\\}\n$$\n其中 $\\mathcal{L}^u$ 是过程 $X_t$ 的无穷小生成元，由 $\\mathcal{L}^u V(x) = (a x + b u) V'(x) + \\frac{1}{2} \\sigma^2 V''(x)$ 给出。代入生成元和运行成本的表达式，HJB 方程变为：\n$$\n\\rho V(x) = \\inf_{u \\in \\mathbb{R}} \\left\\{ (a x + b u) V'(x) + \\frac{1}{2} \\sigma^2 V''(x) + q x^2 + r u^2 \\right\\}\n$$\n通过将大括号内项关于 $u$ 的偏导数设为零来找到 $u$ 上的下确界：\n$$\nb V'(x) + 2 r u = 0 \\implies u^*(x) = -\\frac{b}{2r} V'(x)\n$$\n将此最优控制 $u^*(x)$ 代回 HJB 方程，得到一个关于 $V(x)$ 的非线性常微分方程：\n$$\n\\rho V(x) = a x V'(x) - \\frac{b^2}{4r} (V'(x))^2 + \\frac{1}{2} \\sigma^2 V''(x) + q x^2\n$$\n对于这个线性二次问题，已知值函数是状态的二次函数，$V(x) = P x^2 + C$。其导数为 $V'(x) = 2 P x$ 和 $V''(x) = 2 P$。将这些代入 HJB 方程得到：\n$$\n\\rho (P x^2 + C) = a x (2 P x) - \\frac{b^2}{4r} (2 P x)^2 + \\frac{1}{2} \\sigma^2 (2 P) + q x^2\n$$\n$$\n\\rho P x^2 + \\rho C = (2aP - \\frac{b^2}{r}P^2 + q) x^2 + \\sigma^2 P\n$$\n通过令 $x$ 的同次幂系数相等，我们得到两个关于未知常数 $P$ 和 $C$ 的代数方程。$x^2$ 的系数产生了连续时间代数黎卡提方程 (ARE)：\n$$\n\\frac{b^2}{r} P^2 - (2a - \\rho) P - q = 0\n$$\n由于成本必须是正定的，我们要求 $P  0$。ARE 是一个关于 $P$ 的二次方程，并存在唯一的正解，该解由下式给出：\n$$\nP = \\frac{(2a - \\rho) + \\sqrt{(2a - \\rho)^2 + 4(b^2/r)q}}{2(b^2/r)}\n$$\n令常数项相等可得 $C$ 的方程：\n$$\n\\rho C = \\sigma^2 P \\implies C = \\frac{\\sigma^2 P}{\\rho}\n$$\n这定义了在无限域上的精确参考解 $V^{\\mathrm{ref}}(x) = Px^2 + C$。\n\n### 2. 离散化\n\n问题在一个有界域 $x \\in [-x_{\\max}, x_{\\max}]$ 上求解，该域被离散化为一个包含 $N$ 个点 $\\{x_i\\}_{i=0}^{N-1}$ 的均匀网格，间距为 $h = 2x_{\\max}/(N-1)$。令 $V_i$ 为 $V(x_i)$ 的数值近似。在内部网格点 $x_i$ (其中 $i \\in \\{1, \\dots, N-2\\}$)，导数使用中心有限差分进行近似：\n$$\nV'(x_i) \\approx \\frac{V_{i+1} - V_{i-1}}{2h}, \\qquad V''(x_i) \\approx \\frac{V_{i+1} - 2V_i + V_{i-1}}{h^2}\n$$\n对于一个固定的策略 $u(x)$（离散表示为 $u_i = u(x_i)$），HJB 方程在 $V$ 中是线性的。在每个内部点 $x_i$ 处的离散形式为：\n$$\n\\rho V_i = (a x_i + b u_i) \\left(\\frac{V_{i+1} - V_{i-1}}{2h}\\right) + \\frac{1}{2}\\sigma^2 \\left(\\frac{V_{i+1} - 2V_i + V_{i-1}}{h^2}\\right) + q x_i^2 + r u_i^2\n$$\n整理各项，我们得到一个关于 $V_{i-1}, V_i, V_{i+1}$ 的线性方程：\n$$\n\\left( \\frac{a x_i + b u_i}{2h} - \\frac{\\sigma^2}{2h^2} \\right) V_{i-1} + \\left( \\rho + \\frac{\\sigma^2}{h^2} \\right) V_i - \\left( \\frac{a x_i + b u_i}{2h} + \\frac{\\sigma^2}{2h^2} \\right) V_{i+1} = q x_i^2 + r u_i^2\n$$\n边界条件为狄利克雷类型，固定为参考解的值：$V_0 = V^{\\mathrm{ref}}(-x_{\\max})$ 和 $V_{N-1} = V^{\\mathrm{ref}}(x_{\\max})$。\n\n### 3. 策略迭代算法\n\n策略迭代是一种在两个步骤之间交替进行的迭代方法：策略评估和策略改进。\n\n**初始化**：算法从一个初始策略开始，该策略被设置为零策略，$u^{(0)}(x_i) = 0$ 对所有内部网格点 $x_i$。初始值函数 $V^{(0)}$ 初始化为零，边界值从参考解设定。\n\n**策略评估**：对于一个给定的策略 $u^{(k)}$，我们求解相应的值函数 $V^{(k+1)}$。对于所有内部点 $i \\in \\{1, \\dots, N-2\\}$ 的离散化 HJB 方程集合，构成一个关于未知值 $\\{V_i^{(k+1)}\\}_{i=1}^{N-2}$ 的三对角线性方程组。这个形如 $\\mathbf{A}\\mathbf{V} = \\mathbf{d}$ 的系统使用标准线性代数求解器求解。矩阵 $\\mathbf{A}$ 和向量 $\\mathbf{d}$ 依赖于当前策略 $u^{(k)}$。\n\n**策略改进**：在计算出新的值函数 $V^{(k+1)}$ 后，通过在每个内部网格点 $x_i$ 最小化离散哈密顿量来更新策略为 $u^{(k+1)}$：\n$$\nu_i^{(k+1)} = \\arg\\min_{u \\in \\mathcal{U}_M} \\left\\{ (a x_i + b u) \\frac{V_{i+1}^{(k+1)} - V_{i-1}^{(k+1)}}{2h} + \\frac{1}{2}\\sigma^2 (\\dots) + q x_i^2 + r u^2 \\right\\}\n$$\n这等价于在离散动作集 $\\mathcal{U}_M$ 上最小化 $r u^2 + b u \\frac{V_{i+1}^{(k+1)} - V_{i-1}^{(k+1)}}{2h}$。无约束最小化子是 $u_i^* = -\\frac{b}{2r} \\frac{V_{i+1}^{(k+1)} - V_{i-1}^{(k+1)}}{2h}$。然后，新的策略动作 $u_i^{(k+1)}$ 被选择为离散动作集 $\\mathcal{U}_M$ 中与 $u_i^*$ 最接近的元素。\n\n**终止**：评估和改进之间的迭代持续进行，直到策略和值函数收敛。当值向量的最大绝对值变化 $\\|V^{(k+1)} - V^{(k)}\\|_{\\infty}$ 和策略向量的最大绝对值变化 $\\|u^{(k+1)} - u^{(k)}\\|_{\\infty}$ 都低于指定的容差 $\\varepsilon = 10^{-6}$ 时，或当达到最大迭代次数 $K_{\\max} = 100$ 时，过程终止。\n\n### 4. 误差计算\n\n策略迭代收敛到一个最终的数值解 $V_{\\mathrm{num}}$ 后，通过计算其与精确参考解 $V^{\\mathrm{ref}}$ 在计算网格上的误差的均匀范数来评估其准确性：\n$$\n\\text{误差} = \\| V_{\\mathrm{num}} - V^{\\mathrm{ref}} \\|_{\\infty} = \\max_{i \\in \\{0, \\dots, N-1\\}} | V_{\\mathrm{num}}(x_i) - V^{\\mathrm{ref}}(x_i) |\n$$\n为每个提供的测试用例计算该量。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the policy iteration for all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (a, b, q, r, sigma, rho, x_max, N, M)\n        (-0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 3.0, 161, 61),\n        (0.2, 1.0, 1.0, 0.5, 0.1, 1.0, 2.0, 161, 61),\n        (0.3, 1.0, 2.0, 1.0, 1.0, 2.0, 3.0, 161, 61),\n    ]\n\n    results = []\n    for params in test_cases:\n        error = run_policy_iteration(params)\n        results.append(error)\n\n    # Format and print the final output as a single line\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\ndef run_policy_iteration(params):\n    \"\"\"\n    Solves the HJB equation for a single set of parameters using policy iteration.\n    \"\"\"\n    a, b, q, r, sigma, rho, x_max, N, M = params\n    \n    # --- Step A: Pre-computation and Setup ---\n\n    # 1. Solve the Algebraic Riccati Equation for P\n    # The ARE is (b^2/r)P^2 - (2a - rho)P - q = 0\n    A_ric = b**2 / r\n    B_ric = -(2.0*a - rho)\n    C_ric = -q\n    discriminant = B_ric**2 - 4.0 * A_ric * C_ric\n    # We need the positive root for P\n    P = (-B_ric + np.sqrt(discriminant)) / (2.0 * A_ric)\n\n    # 2. Calculate the constant C\n    C = (sigma**2 * P) / rho\n\n    # 3. Define the reference (exact) value function\n    def v_ref(x):\n        return P * x**2 + C\n\n    # 4. Set up the state and action grids\n    x_grid = np.linspace(-x_max, x_max, N)\n    h = x_grid[1] - x_grid[0]\n    v_ref_grid = v_ref(x_grid)\n\n    kappa = 1.25\n    u_max_val = kappa * abs(b * P * x_max / r)\n    \n    if M > 1 and u_max_val > 0:\n        u_grid = np.linspace(-u_max_val, u_max_val, M)\n    else:\n        u_grid = np.zeros(M)\n    u_step = u_grid[1] - u_grid[0] if M > 1 else 0\n\n    # 5. Set up iteration parameters\n    tol = 1e-6\n    max_iter = 100\n    \n    # --- Step B: Policy Iteration Loop ---\n\n    # Initialization\n    # Policy for interior points (size N-2)\n    current_policy = np.zeros(N - 2)\n    # Value function on the full grid (size N)\n    current_V = np.zeros(N)\n    # Set Dirichlet boundary conditions from the exact solution\n    current_V[0] = v_ref_grid[0]\n    current_V[-1] = v_ref_grid[-1]\n    \n    x_interior = x_grid[1:-1]\n    \n    for k in range(max_iter):\n        \n        # --- 1. Policy Evaluation ---\n        # Solve the linear system A * V_interior = d for V\n        \n        drift_coeff = a * x_interior + b * current_policy\n        \n        # Coefficients of the tridiagonal system\n        L = drift_coeff / (2.0 * h) - sigma**2 / (2.0 * h**2)\n        D = rho + sigma**2 / h**2\n        U = -drift_coeff / (2.0 * h) - sigma**2 / (2.0 * h**2)\n        \n        # Construct the (N-2) x (N-2) system matrix A\n        A = np.diag(D * np.ones(N-2)) + np.diag(U[:-1], k=1) + np.diag(L[1:], k=-1)\n        \n        # Construct the right-hand side vector d\n        d = q * x_interior**2 + r * current_policy**2\n        \n        # Adjust d for boundary conditions\n        d[0] -= L[0] * v_ref_grid[0]\n        d[-1] -= U[-1] * v_ref_grid[-1]\n        \n        # Solve for the new interior values of V\n        try:\n            V_interior_new = np.linalg.solve(A, d)\n        except np.linalg.LinAlgError:\n            # Fallback for singular matrix, although not expected here\n            return np.inf\n\n        # Form the full new value function vector\n        V_new = np.concatenate(([v_ref_grid[0]], V_interior_new, [v_ref_grid[-1]]))\n        \n        # --- 2. Policy Improvement ---\n        # Update the policy by minimizing the Hamiltonian\n        \n        # Approximate V'(x) at interior points\n        V_prime_interior = (V_new[2:] - V_new[:-2]) / (2.0 * h)\n        \n        # Compute the unconstrained optimal control\n        u_star_interior = -b / (2.0 * r) * V_prime_interior\n        \n        # Find the closest control in the discrete action space\n        new_policy = np.zeros_like(current_policy)\n        if M > 1 and u_step > 0:\n            indices = np.round((u_star_interior - u_grid[0]) / u_step)\n            indices = np.clip(indices, 0, M - 1).astype(int)\n            new_policy = u_grid[indices]\n            \n        # --- 3. Termination Check ---\n        \n        val_change = np.max(np.abs(V_new - current_V))\n        pol_change = np.max(np.abs(new_policy - current_policy))\n        \n        # Update for the next iteration\n        current_V = V_new\n        current_policy = new_policy\n        \n        if val_change  tol and pol_change  tol:\n            break\n            \n    # --- Step C: Final Error Calculation ---\n    \n    # The converged numerical solution is the last computed value function\n    V_num = current_V\n    error = np.max(np.abs(V_num - v_ref_grid))\n    \n    return error\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3001638"}]}