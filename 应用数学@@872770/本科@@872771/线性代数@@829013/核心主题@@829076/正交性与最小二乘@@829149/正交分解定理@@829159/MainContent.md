## 引言
在我们的几何直觉中，将一个物体分解为其在不同方向上的分量是一种常见的思维方式，例如将一个力分解为水平和垂直分力。线性代数中的[正交分解定理](@entry_id:156276)将这一直观概念严谨化、普适化，使其成为分析和理解[向量空间](@entry_id:151108)结构的强大工具。该定理解决了如何将任意向量唯一地分解为一个属于特定[子空间](@entry_id:150286)的分量与一个与之垂直的分量这一核心问题，弥合了直观几何与[抽象代数](@entry_id:145216)之间的鸿沟。本文将系统地引导读者深入这一重要理论。在“原理与机制”一章中，我们将阐明定理的数学表述、关键性质（如最佳逼近）以及针对不同基的计算方法。接下来，“应用与跨学科联系”一章将展示该定理如何为数据科学、信号处理和物理学等领域中的实际问题提供统一的解决方案。最后，“动手实践”部分将通过具体练习，帮助您将理论知识转化为解决问题的实用技能。

## 原理与机制

在[内积空间](@entry_id:271570)中，正交性的概念为我们提供了一种强大的工具，用于分解向量和理解[子空间](@entry_id:150286)的结构。正如在三维空间中，我们可以将任意向量看作是其在地面上的投影（一个分量）与一个垂直于地面的分量之和，我们可以将这一直观想法推广到任意维度和[任意子](@entry_id:143753)空间。本章将深入探讨这一核心思想，即**[正交分解定理](@entry_id:156276) (Orthogonal Decomposition Theorem)**，并阐释其原理、计算方法及其在众多科学与工程领域中的应用。

### [正交分解定理](@entry_id:156276)

**[正交分解定理](@entry_id:156276)**是线性代数中的一个基石。它指出，对于一个[内积空间](@entry_id:271570) $V$ 中的任意向量 $\vec{y}$ 和任意子空间 $W$，存在唯一的分解方式，可以将 $\vec{y}$ 写成一个属于 $W$ 的向量 $\hat{\vec{y}}$ 与一个正交于 $W$ 的向量 $\vec{z}$ 的和。

数学上，这表示为：
$$
\vec{y} = \hat{\vec{y}} + \vec{z}
$$
其中，$\hat{\vec{y}} \in W$ 且 $\vec{z} \in W^{\perp}$。这里，$W^{\perp}$ 表示 $W$ 的**[正交补](@entry_id:149922) (orthogonal complement)**，即 $V$ 中所有与 $W$ 内每一个向量都正交的向量所组成的集合。

向量 $\hat{\vec{y}}$ 被称为 $\vec{y}$ 在[子空间](@entry_id:150286) $W$ 上的**正交投影 (orthogonal projection)**，通常记为 $\text{proj}_W(\vec{y})$。向量 $\vec{z}$ 则是 $\vec{y}$ 相对于 $W$ 的**正交分量**，它等于 $\vec{y} - \hat{\vec{y}}$。这个定理的强大之处在于它保证了这种分解的存在性和唯一性，为许多应用问题提供了坚实的理论基础。

### 计算[正交分解](@entry_id:148020)

要实际执行这种分解，关键在于如何计算[正交投影](@entry_id:144168) $\hat{\vec{y}}$。计算方法取决于我们对[子空间](@entry_id:150286) $W$ 的了解，特别是其基的性质。

#### 投影到由[正交基](@entry_id:264024)张成的[子空间](@entry_id:150286)

最简单的情况是当[子空间](@entry_id:150286) $W$ 有一组**正交基 (orthogonal basis)** $\{ \vec{u}_1, \vec{u}_2, \dots, \vec{u}_p \}$ 时。在这种情况下，向量 $\vec{y}$ 在 $W$ 上的投影等于它在每个[基向量](@entry_id:199546)方向上的投影之和。具体来说，$\vec{y}$ 在由单个向量 $\vec{u}_i$ 张成的直线上的投影为：
$$
\text{proj}_{\vec{u}_i}(\vec{y}) = \frac{\vec{y} \cdot \vec{u}_i}{\vec{u}_i \cdot \vec{u}_i} \vec{u}_i
$$
由于[基向量](@entry_id:199546)之间相互正交，$\vec{y}$ 在整个[子空间](@entry_id:150286) $W$ 上的投影就是这些分量的简单叠加：
$$
\hat{\vec{y}} = \text{proj}_W(\vec{y}) = \sum_{i=1}^{p} \frac{\vec{y} \cdot \vec{u}_i}{\vec{u}_i \cdot \vec{u}_i} \vec{u}_i
$$

这个公式在许多领域都有直接应用。例如，在数字通信系统中，一个有效的信号可能属于某个特定的[信号子空间](@entry_id:185227) $W$。当信号在传输过程中受到噪声干扰时，接收到的向量 $\vec{y}$ 实际上是原始信号 $\vec{s}$（属于 $W$）和一个与信号空间正交的误差向量 $\vec{e}$ 的和，即 $\vec{y} = \vec{s} + \vec{e}$。根据[正交分解定理](@entry_id:156276)的唯一性，原始信号 $\vec{s}$ 必然是接收信号 $\vec{y}$ 在[子空间](@entry_id:150286) $W$ 上的正交投影 [@problem_id:1396569]。

假设在一个 $\mathbb{R}^4$ 的系统中，[信号子空间](@entry_id:185227) $W$ 由一组正交基 $\vec{u}_1 = (1, 1, 1, 1)$ 和 $\vec{u}_2 = (1, -1, 1, -1)$ 张成。如果接收到的信号为 $\vec{y} = (3, 5, -2, 8)$，我们可以通过计算投影来恢复原始信号 $\vec{s}$：
$$
\vec{s} = \hat{\vec{y}} = \frac{\vec{y} \cdot \vec{u}_1}{\vec{u}_1 \cdot \vec{u}_1} \vec{u}_1 + \frac{\vec{y} \cdot \vec{u}_2}{\vec{u}_2 \cdot \vec{u}_2} \vec{u}_2
$$
计算所需的[点积](@entry_id:149019)：
$\vec{y} \cdot \vec{u}_1 = 3 + 5 - 2 + 8 = 14$
$\vec{y} \cdot \vec{u}_2 = 3 - 5 - 2 - 8 = -12$
$\vec{u}_1 \cdot \vec{u}_1 = 1^2 + 1^2 + 1^2 + 1^2 = 4$
$\vec{u}_2 \cdot \vec{u}_2 = 1^2 + (-1)^2 + 1^2 + (-1)^2 = 4$

代入公式，我们得到：
$$
\vec{s} = \frac{14}{4} \vec{u}_1 + \frac{-12}{4} \vec{u}_2 = \frac{7}{2}(1, 1, 1, 1) - 3(1, -1, 1, -1) = \left(\frac{1}{2}, \frac{13}{2}, \frac{1}{2}, \frac{13}{2}\right)
$$
这就是从含噪信号中恢复出的最可能的原始信号。

#### 投影到由[非正交基](@entry_id:154908)张成的[子空间](@entry_id:150286)

当[子空间](@entry_id:150286) $W$ 的基 $\{ \vec{u}_1, \dots, \vec{u}_p \}$ 不是正交的时，上述简单的求和公式就不再适用。我们需要一种更通用的方法。这个方法的核心回到了[正交分解](@entry_id:148020)的基本定义：向量 $\vec{z} = \vec{y} - \hat{\vec{y}}$ 必须与 $W$ 中的所有向量正交。这意味着 $\vec{z}$ 必须与 $W$ 的每一个[基向量](@entry_id:199546)都正交：
$$
(\vec{y} - \hat{\vec{y}}) \cdot \vec{u}_j = 0 \quad \text{for } j=1, \dots, p
$$
因为 $\hat{\vec{y}}$ 是 $W$ 中的向量，它可以表示为[基向量](@entry_id:199546)的[线性组合](@entry_id:154743) $\hat{\vec{y}} = c_1 \vec{u}_1 + \dots + c_p \vec{u}_p$。代入上式，我们得到一组关于系数 $c_j$ 的线性方程组：
$$
(\vec{y} - (c_1 \vec{u}_1 + \dots + c_p \vec{u}_p)) \cdot \vec{u}_j = 0
$$
整理后得到：
$$
c_1(\vec{u}_1 \cdot \vec{u}_j) + \dots + c_p(\vec{u}_p \cdot \vec{u}_j) = \vec{y} \cdot \vec{u}_j \quad \text{for } j=1, \dots, p
$$
这个[方程组](@entry_id:193238)被称为**[正规方程](@entry_id:142238) (normal equations)**。通过求解这组方程得到系数 $c_1, \dots, c_p$，我们就能构造出投影向量 $\hat{\vec{y}}$。

例如，在计算机图形学中，我们可能需要将一个入射光向量 $\vec{L}$ 分解为一个平行于表面（[子空间](@entry_id:150286) $W$）的分量 $\vec{w}$ 和一个垂直于表面的分量 $\vec{z}$ [@problem_id:1396575]。假设表面由两个非[正交向量](@entry_id:142226) $\vec{u}_1 = (1, 1, 0)$ 和 $\vec{u}_2 = (1, 0, 1)$ 张成，入射光向量为 $\vec{L} = (7, 2, 8)$。平行分量 $\vec{w}$ 是 $\vec{L}$ 在 $W$ 上的投影。设 $\vec{w} = a_1 \vec{u}_1 + a_2 \vec{u}_2$，我们通过建立正规方程来求解 $a_1$ 和 $a_2$：
$$
\begin{cases} a_1(\vec{u}_1 \cdot \vec{u}_1) + a_2(\vec{u}_2 \cdot \vec{u}_1)  = \vec{L} \cdot \vec{u}_1 \\ a_1(\vec{u}_1 \cdot \vec{u}_2) + a_2(\vec{u}_2 \cdot \vec{u}_2)  = \vec{L} \cdot \vec{u}_2 \end{cases}
$$
计算[点积](@entry_id:149019)：$\vec{u}_1 \cdot \vec{u}_1 = 2$, $\vec{u}_2 \cdot \vec{u}_2 = 2$, $\vec{u}_1 \cdot \vec{u}_2 = 1$, $\vec{L} \cdot \vec{u}_1 = 9$, $\vec{L} \cdot \vec{u}_2 = 15$。
[方程组](@entry_id:193238)变为：
$$
\begin{cases} 2a_1 + a_2  = 9 \\ a_1 + 2a_2  = 15 \end{cases}
$$
解得 $a_1 = 1, a_2 = 7$。因此，平行于表面的光分量为：
$$
\vec{w} = 1 \cdot (1, 1, 0) + 7 \cdot (1, 0, 1) = (8, 1, 7)
$$
这个方法虽然计算量稍大，但普遍适用，即使在没有正交基的情况下也能精确地找到投影 [@problem_id:1396559]。

#### 利用[正交补](@entry_id:149922)进行投影

有时，直接计算到[子空间](@entry_id:150286) $W$ 的投影比较复杂，但其[正交补](@entry_id:149922) $W^{\perp}$ 的结构却很简单。在这种情况下，我们可以采取一种迂回策略：先计算向量 $\vec{y}$ 在 $W^{\perp}$ 上的投影，即 $\vec{z} = \text{proj}_{W^{\perp}}(\vec{y})$，然后利用分解式 $\vec{y} = \hat{\vec{y}} + \vec{z}$ 来求得 $\hat{\vec{y}}$：
$$
\hat{\vec{y}} = \vec{y} - \vec{z} = \vec{y} - \text{proj}_{W^{\perp}}(\vec{y})
$$
当[子空间](@entry_id:150286) $W$ 在 $\mathbb{R}^3$ 中是一个由方程 $ax_1 + bx_2 + cx_3 = 0$ 定义的平面时，这个方法特别有效。这个平面的[法向量](@entry_id:264185) $\vec{n} = (a, b, c)$ 张成了一维的正交补空间 $W^{\perp}$。将[向量投影](@entry_id:147046)到一条直线上远比投影到一个平面上简单 [@problem_id:1396548]。

例如，考虑[子空间](@entry_id:150286) $W$ 由方程 $x_1 + 2x_2 - x_3 = 0$ 定义，我们要分解向量 $\vec{y} = (7, 1, 4)$。$W$ 的[正交补](@entry_id:149922) $W^{\perp}$ 是由[法向量](@entry_id:264185) $\vec{n} = (1, 2, -1)$ 张成的直线。我们先计算 $\vec{y}$ 在 $W^{\perp}$ 上的投影 $\vec{z}$：
$$
\vec{z} = \text{proj}_{\vec{n}}(\vec{y}) = \frac{\vec{y} \cdot \vec{n}}{\vec{n} \cdot \vec{n}} \vec{n} = \frac{7(1) + 1(2) + 4(-1)}{1^2 + 2^2 + (-1)^2} (1, 2, -1) = \frac{5}{6}(1, 2, -1) = \left(\frac{5}{6}, \frac{5}{3}, -\frac{5}{6}\right)
$$
然后，$\vec{y}$ 在 $W$ 中的分量 $\vec{w}$ 就可以通过减法得到：
$$
\vec{w} = \vec{y} - \vec{z} = (7, 1, 4) - \left(\frac{5}{6}, \frac{5}{3}, -\frac{5}{6}\right) = \left(\frac{37}{6}, -\frac{2}{3}, \frac{29}{6}\right)
$$
这种策略充分利用了空间的几何结构，简化了计算过程。

### 几何性质与最佳逼近

[正交分解](@entry_id:148020)不仅仅是一种计算工具，它还揭示了深刻的几何关系，其中最重要的是与毕达哥拉斯定理（[勾股定理](@entry_id:264352)）和最佳逼近的联系。

#### [向量的毕达哥拉斯定理](@entry_id:174782)

由于[正交分解](@entry_id:148020) $\vec{y} = \hat{\vec{y}} + \vec{z}$ 中的两个分量 $\hat{\vec{y}}$ 和 $\vec{z}$ 是相互正交的（即 $\hat{\vec{y}} \cdot \vec{z} = 0$），[向量的范数](@entry_id:154882)（长度）满足一个类似于毕达哥拉斯定理的关系：
$$
\|\vec{y}\|^2 = \|\hat{\vec{y}} + \vec{z}\|^2 = (\hat{\vec{y}} + \vec{z}) \cdot (\hat{\vec{y}} + \vec{z}) = \|\hat{\vec{y}}\|^2 + \|\vec{z}\|^2 + 2(\hat{\vec{y}} \cdot \vec{z}) = \|\hat{\vec{y}}\|^2 + \|\vec{z}\|^2
$$
这个等式表明，原始向量长度的平方等于其在[子空间](@entry_id:150286)内的投影长度的平方与正交分量长度的平方之和。这个关系非常有用，例如，如果我们想知道分解产生的“误差”或“残差”向量 $\vec{z}$ 的大小，我们可以通过计算 $\|\vec{y}\|^2$ 和 $\|\hat{\vec{y}}\|^2$ 来间接求得 $\|\vec{z}\|^2 = \|\vec{y}\|^2 - \|\hat{\vec{y}}\|^2$，而无需显式计算向量 $\vec{z}$ 本身 [@problem_id:1396552]。

#### [最佳逼近定理](@entry_id:150199)

正交投影 $\hat{\vec{y}}$ 还有一个至关重要的性质，由**[最佳逼近定理](@entry_id:150199) (Best Approximation Theorem)** 阐述。该定理指出，在[子空间](@entry_id:150286) $W$ 的所有向量中，[正交投影](@entry_id:144168) $\hat{\vec{y}}$ 是距离原始向量 $\vec{y}$ 最近的一个。换句话说，对于 $W$ 中任意不同于 $\hat{\vec{y}}$ 的向量 $\vec{v}$，我们都有：
$$
\|\vec{y} - \hat{\vec{y}}\| \lt \|\vec{y} - \vec{v}\|
$$
这个定理的几何直觉很清晰：从一个点到一个平面的最短距离是通过垂线得到的。证明也相当简洁：考虑距离的平方 $\|\vec{y} - \vec{v}\|^2$。我们可以写成 $\|\vec{y} - \vec{v}\|^2 = \|(\vec{y} - \hat{\vec{y}}) + (\hat{\vec{y}} - \vec{v})\|^2$。由于 $\vec{y} - \hat{\vec{y}} \in W^{\perp}$ 而 $\hat{\vec{y}} - \vec{v} \in W$（因为 $W$ 是[子空间](@entry_id:150286)），这两个向量是正交的。根据毕达哥拉斯定理，$\|\vec{y} - \vec{v}\|^2 = \|\vec{y} - \hat{\vec{y}}\|^2 + \|\hat{\vec{y}} - \vec{v}\|^2$。因为 $\|\hat{\vec{y}} - \vec{v}\|^2 \ge 0$，所以 $\|\vec{y} - \vec{v}\|^2 \ge \|\vec{y} - \hat{\vec{y}}\|^2$。等号成立的唯一情况是 $\vec{v} = \hat{\vec{y}}$。

这个定理是[最小二乘法](@entry_id:137100)等[数据拟合](@entry_id:149007)方法的理论核心。在那些问题中，我们试图在一个由模型参数定义的[子空间](@entry_id:150286) $W$ 中找到一个“最拟合”数据的向量。[最佳逼近定理](@entry_id:150199)告诉我们，这个最佳拟合就是数据的[正交投影](@entry_id:144168)。而“残差向量” $\vec{z} = \vec{y} - \hat{\vec{y}}$ 必须与[子空间](@entry_id:150286) $W$ 正交 [@problem_id:1350581]。

### [投影算子](@entry_id:154142)的性质

我们可以将投影过程视为一个作用于向量的函数或**算子**。对于一个固定的[子空间](@entry_id:150286) $W$，我们可以定义一个[投影算子](@entry_id:154142) $P_W: V \to V$，使得 $P_W(\vec{y}) = \text{proj}_W(\vec{y})$。这个算子具有一些非常重要的代数性质。

首先，[投影算子](@entry_id:154142)是**线性的**。这意味着对于任意向量 $\vec{y}_1, \vec{y}_2$ 和标量 $c_1, c_2$，都有：
$$
P_W(c_1 \vec{y}_1 + c_2 \vec{y}_2) = c_1 P_W(\vec{y}_1) + c_2 P_W(\vec{y}_2)
$$
这意味着对向量的[线性组合](@entry_id:154743)进行投影，等同于对每个向量单独投影后再进行相同的[线性组合](@entry_id:154743)。这个性质简化了对复杂向量组合的分析 [@problem_id:1396533]。

其次，从更抽象的层面看，任何正交投影算子 $P$ 都具有两个标志性特征：
1.  **[幂等性](@entry_id:190768) (Idempotent)**: $P^2 = P$。这意味着对一个[向量投影](@entry_id:147046)两次和投影一次的结果是相同的。一旦一个向量被投影到[子空间](@entry_id:150286) $W$ 中，它就已经在 $W$ 里了，再次投影不会改变它。
2.  **自伴性 (Self-Adjoint)**: $P = P^*$。这意味着对于任意向量 $\vec{x}, \vec{y}$，[内积](@entry_id:158127)满足 $\langle P(\vec{x}), \vec{y} \rangle = \langle \vec{x}, P(\vec{y}) \rangle$。这个性质反映了投影在几何上的对称性。

反之，任何同时满足[幂等性](@entry_id:190768)和自伴性的线性算子都必然是一个到某个[子空间](@entry_id:150286)的正交投影算子。这两个性质完全刻画了[正交投影](@entry_id:144168)的代数本质。我们可以通过强制施加正交条件 $(\vec{x}-P(\vec{x})) \perp W$ 来反向推导出[投影算子](@entry_id:154142)的精确形式，从而加深对这些性质的理解 [@problem_id:1396531]。

### 高级应用与推广

[正交分解定理](@entry_id:156276)的应用可以进一步扩展到更复杂的场景，例如多层次的分解和与矩阵[基本子空间](@entry_id:190076)的深刻联系。

#### 层次化[正交分解](@entry_id:148020)

如果存在一系列嵌套的[子空间](@entry_id:150286)，例如 $W_2 \subset W_1 \subset V$，我们可以应用[正交分解定理](@entry_id:156276)进行多次分解，从而将一个向量 $\vec{y}$ 分解为多个相互正交的分量。

首先，我们将 $\vec{y}$ 分解为相对于 $W_1$ 的两个分量：
$$
\vec{y} = \text{proj}_{W_1}(\vec{y}) + (\vec{y} - \text{proj}_{W_1}(\vec{y}))
$$
这里，第一项属于 $W_1$，第二项属于 $W_1^{\perp}$。我们称它们为 $\vec{p}_1 \in W_1$ 和 $\vec{y}_c \in W_1^{\perp}$。

接着，我们可以对已在 $W_1$ 中的分量 $\vec{p}_1$ 进行第二次分解，这次是相对于[子空间](@entry_id:150286) $W_2$：
$$
\vec{p}_1 = \text{proj}_{W_2}(\vec{p}_1) + (\vec{p}_1 - \text{proj}_{W_2}(\vec{p}_1))
$$
由于 $W_2 \subset W_1$，可以证明 $\text{proj}_{W_2}(\vec{p}_1) = \text{proj}_{W_2}(\text{proj}_{W_1}(\vec{y})) = \text{proj}_{W_2}(\vec{y})$。因此，上式可以写成：
$$
\vec{p}_1 = \text{proj}_{W_2}(\vec{y}) + (\text{proj}_{W_1}(\vec{y}) - \text{proj}_{W_2}(\vec{y}))
$$
我们称这两个分量为 $\vec{y}_a \in W_2$ 和 $\vec{y}_b \in W_1 \cap W_2^{\perp}$（后者表示在 $W_1$ 中但正交于 $W_2$ 的部分）。

最终，我们将原始向量 $\vec{y}$ 分解为三个相互正交的分量之和 [@problem_id:1396571]：
$$
\vec{y} = \vec{y}_a + \vec{y}_b + \vec{y}_c
$$
其中 $\vec{y}_a \in W_2$，$\vec{y}_b \in W_1 \cap W_2^{\perp}$，$\vec{y}_c \in W_1^{\perp}$。这种层次化的分解在信号处理和图像分析等领域非常有用，它允许我们从粗到细地分析数据的结构。

#### [四个基本子空间](@entry_id:154834)与[奇异值分解 (SVD)](@entry_id:172448)

[正交分解定理](@entry_id:156276)与矩阵的[四个基本子空间](@entry_id:154834)之间存在着深刻的联系。对于任意一个 $m \times n$ 的矩阵 $A$，其**[行空间](@entry_id:148831) (Row Space)** $\text{Row}(A)$ 和**零空间 (Null Space)** $\text{Null}(A)$ 是定义域 $\mathbb{R}^n$ 的[子空间](@entry_id:150286)，而**[列空间](@entry_id:156444) (Column Space)** $\text{Col}(A)$ 和**[左零空间](@entry_id:150506) (Left Null Space)** $\text{Null}(A^T)$ 是协域 $\mathbb{R}^m$ 的[子空间](@entry_id:150286)。

**[线性代数基本定理](@entry_id:190797)**的一个重要部分指出，这些[子空间](@entry_id:150286)成对地构成正交补：
1.  在定义域 $\mathbb{R}^n$ 中，$(\text{Row}(A))^{\perp} = \text{Null}(A)$。
2.  在协域 $\mathbb{R}^m$ 中，$(\text{Col}(A))^{\perp} = \text{Null}(A^T)$。

这意味着，根据[正交分解定理](@entry_id:156276)，定义域中的任意向量 $\vec{x}$ 都可以唯一地分解为行空间中的一个分量 $\vec{p}$ 和零空间中的一个分量 $\vec{o}$ 的和：$\vec{x} = \vec{p} + \vec{o}$。

**奇异值分解 (Singular Value Decomposition, SVD)** 为我们提供了一种强大的计算工具来实现这种分解。矩阵 $A$ 的SVD形式为 $A = U\Sigma V^T$，其中 $U$ 和 $V$ 是[正交矩阵](@entry_id:169220)。矩阵 $V$ 的列向量 $\{ \vec{v}_1, \dots, \vec{v}_n \}$ 构成了 $\mathbb{R}^n$ 的一个[标准正交基](@entry_id:147779)。如果矩阵 $A$ 的秩为 $r$，那么：
-   $V$ 的前 $r$ 个列向量 $\{ \vec{v}_1, \dots, \vec{v}_r \}$ 构成了[行空间](@entry_id:148831) $\text{Row}(A)$ 的一个标准正交基。
-   $V$ 的后 $n-r$ 个列向量 $\{ \vec{v}_{r+1}, \dots, \vec{v}_n \}$ 构成了零空间 $\text{Null}(A)$ 的一个[标准正交基](@entry_id:147779)。

有了这些基，对任意向量 $\vec{x}$ 的分解就变得非常直接。其在行空间上的投影 $\vec{p}$ 就是 $\vec{x}$ 在 $\{ \vec{v}_1, \dots, \vec{v}_r \}$ 上的投影之和。因为这是标准正交基，所以计算极为简单 [@problem_id:1396538]：
$$
\vec{p} = \sum_{i=1}^{r} (\vec{x} \cdot \vec{v}_i) \vec{v}_i
$$
而[零空间](@entry_id:171336)中的分量则为 $\vec{o} = \vec{x} - \vec{p}$。SVD 不仅在理论上统一了[正交分解](@entry_id:148020)和[四个基本子空间](@entry_id:154834)的概念，还在实践中为高效、稳定地执行这种分解提供了算法基础。