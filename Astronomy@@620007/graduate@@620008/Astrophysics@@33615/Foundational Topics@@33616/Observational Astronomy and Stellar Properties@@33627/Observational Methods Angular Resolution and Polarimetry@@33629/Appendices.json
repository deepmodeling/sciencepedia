{"hands_on_practices": [{"introduction": "This exercise provides a hands-on introduction to the foundational deconvolution technique in radio interferometry, the Högbom CLEAN algorithm. By manually applying a single iteration to a simplified dirty map, you will gain a concrete understanding of how sidelobe artifacts are identified and removed, which is the essential first step in transforming raw interferometer data into a scientifically useful image [@problem_id:249013]. This practice illuminates the core logic behind one of the most widely-used algorithms in synthesis imaging.", "problem": "In radio interferometry, the observed image, known as the \"dirty map\" $I_D(\\vec{r})$, is the true sky brightness distribution convolved with the interferometer's point spread function, the \"dirty beam\" $B_D(\\vec{r})$. The Högbom CLEAN algorithm is a widely used iterative deconvolution technique to remove the artifacts introduced by the dirty beam's sidelobes.\n\nThe algorithm proceeds as follows:\n1.  Initialize a \"residual map\" $I_R$ to be the dirty map, $I_{R,0}(\\vec{r}) = I_D(\\vec{r})$.\n2.  In each iteration $i$, find the location $\\vec{r}_{p,i}$ and flux $I_{p,i}$ of the brightest point in the current residual map $I_{R,i}(\\vec{r})$.\n3.  Subtract a fraction of the dirty beam, scaled by the peak flux and centered at the peak location, from the residual map. This update is given by:\n    $$I_{R, i+1}(\\vec{r}) = I_{R, i}(\\vec{r}) - g \\, I_{p,i} \\, B_D(\\vec{r} - \\vec{r}_{p,i})$$\n    Here, $g$ is a dimensionless parameter called the \"loop gain\", typically between $0$ and $1$.\n4.  The process is repeated until a stopping criterion is met.\n\nConsider a simple case where the dirty map consists of a single, unresolved point source. Due to convolution with the dirty beam, the dirty map is represented by a Gaussian function centered at $\\vec{r}_0 = (x_0, y_0)$:\n$$I_D(x, y) = A \\exp\\left(-\\frac{(x-x_0)^2 + (y-y_0)^2}{2\\sigma_S^2}\\right)$$\nwhere $A > 0$ is the peak amplitude and $\\sigma_S$ is the characteristic width.\n\nThe dirty beam for this particular observation has a central lobe and one significant sidelobe. It is modeled as a sum of two Gaussians:\n$$B_D(x, y) = \\exp\\left(-\\frac{x^2+y^2}{2\\sigma_B^2}\\right) + S \\exp\\left(-\\frac{(x-x_s)^2 + (y-y_s)^2}{2\\sigma_B^2}\\right)$$\nwhere $\\sigma_B$ is the width of the Gaussian components, $(x_s, y_s)$ is the position of the sidelobe relative to the main lobe, and $S$ is the relative strength of the sidelobe. You may assume that the global maximum of the dirty beam is at the origin $(0,0)$, which is satisfied for $0  S  1$.\n\nDerive the analytical expression for the residual map, $I_{R,1}(x, y)$, after the first complete iteration of the Högbom CLEAN algorithm. Your final expression should be in terms of $x, y, A, x_0, y_0, \\sigma_S, g, \\sigma_B, S, x_s$, and $y_s$.", "solution": "1. Initial residual is the dirty map:  \n$$I_{R,0}(x,y)=I_D(x,y)=A\\exp\\Bigl(-\\frac{(x-x_0)^2+(y-y_0)^2}{2\\sigma_S^2}\\Bigr)\\,. $$  \n2. The brightest peak occurs at $(x_0,y_0)$ with flux $I_{p,0}=A$ and position $\\vec r_{p,0}=(x_0,y_0)$.  \n3. The CLEAN update after one iteration is  \n$$I_{R,1}(x,y)=I_{R,0}(x,y)-g\\,I_{p,0}\\,B_D\\bigl(x-x_0,y-y_0\\bigr)\\,. $$  \n4. Substituting $I_{R,0}$ and $B_D$:  \n$$\\begin{aligned}\nI_{R,1}(x,y)=A\\exp\\Bigl(-\\frac{(x-x_0)^2+(y-y_0)^2}{2\\sigma_S^2}\\Bigr)\n-gA\\Bigl[\\exp\\Bigl(-\\frac{(x-x_0)^2+(y-y_0)^2}{2\\sigma_B^2}\\Bigr)\\\\\n\\quad+S\\exp\\Bigl(-\\frac{(x-x_0-x_s)^2+(y-y_0-y_s)^2}{2\\sigma_B^2}\\Bigr)\\Bigr]\\,.  \n\\end{aligned}$$", "answer": "$$\\boxed{A\\exp\\!\\Bigl(-\\frac{(x-x_0)^2+(y-y_0)^2}{2\\sigma_S^2}\\Bigr)\\;-\\;g\\,A\\Bigl[\\exp\\!\\Bigl(-\\frac{(x-x_0)^2+(y-y_0)^2}{2\\sigma_B^2}\\Bigr)\\;+\\;S\\,\\exp\\!\\Bigl(-\\frac{(x-x_0-x_s)^2+(y-y_0-y_s)^2}{2\\sigma_B^2}\\Bigr)\\Bigr]}$$", "id": "249013"}, {"introduction": "To accurately measure the polarization of celestial sources, we must first characterize the performance of our instruments. This problem uses the Mueller matrix formalism to analyze a common polarimeter component, the rotating half-wave plate (HWP), and quantify the impact of a small manufacturing imperfection on its measurement capability [@problem_id:248789]. Deriving the loss in polarimetric modulation efficiency provides crucial insight into how instrumental errors propagate and affect the final data quality in polarimetry.", "problem": "A rotating half-wave plate (HWP) is a common component in polarimeters used to measure linear polarization. An ideal HWP introduces a phase shift (retardance) of $\\Delta = \\pi$ radians between two orthogonal linear polarization components of light. In this setup, the HWP rotates with its fast axis at an angle $\\theta$ relative to a reference direction. It is followed by a fixed linear polarizer, called an analyzer, which is oriented along the reference direction (angle $\\alpha=0$). This configuration modulates the incoming linear polarization signal into a time-varying intensity signal that can be easily measured.\n\nIn a real instrument, the HWP is imperfect and has a retardance $\\Delta = \\pi + \\delta$, where $\\delta$ is a small, constant retardance error. This imperfection reduces the instrument's ability to measure linear polarization.\n\nThe Stokes vector $S = [I, Q, U, V]^T$ describes the polarization state of light. An optical element is described by a $4\\times4$ Mueller matrix $M$. The outgoing Stokes vector $S_{out}$ is given by $S_{out} = M S_{in}$. The measured intensity is the first component of the final Stokes vector.\n\nThe Mueller matrix for a retarder with fast-axis angle $\\theta$ and retardance $\\Delta$ is:\n$$\nM_{R}(\\theta, \\Delta) = \\begin{pmatrix}\n1  0  0  0 \\\\\n0  \\cos^2(2\\theta) + \\sin^2(2\\theta)\\cos(\\Delta)  \\cos(2\\theta)\\sin(2\\theta)(1-\\cos(\\Delta))  -\\sin(2\\theta)\\sin(\\Delta) \\\\\n0  \\cos(2\\theta)\\sin(2\\theta)(1-\\cos(\\Delta))  \\sin^2(2\\theta) + \\cos^2(2\\theta)\\cos(\\Delta)  \\cos(2\\theta)\\sin(\\Delta) \\\\\n0  \\sin(2\\theta)\\sin(\\Delta)  -\\cos(2\\theta)\\sin(\\Delta)  \\cos(\\Delta)\n\\end{pmatrix}\n$$\nThe Mueller matrix for a linear polarizer oriented at an angle $\\alpha$ is:\n$$\nM_{LP}(\\alpha) = \\frac{1}{2} \\begin{pmatrix}\n1  \\cos(2\\alpha)  \\sin(2\\alpha)  0 \\\\\n\\cos(2\\alpha)  \\cos^2(2\\alpha)  \\cos(2\\alpha)\\sin(2\\alpha)  0 \\\\\n\\sin(2\\alpha)  \\cos(2\\alpha)\\sin(2\\alpha)  \\sin^2(2\\alpha)  0 \\\\\n0  0  0  0\n\\end{pmatrix}\n$$\n\nThe \"polarimetric modulation efficiency\", $\\eta$, for linear polarization is defined as the ratio of the amplitude of the intensity modulation at the $4\\theta$ harmonic for the real HWP to the corresponding amplitude for an ideal HWP ($\\delta=0$).\n\nDerive the loss in polarimetric modulation efficiency, defined as $1-\\eta$, caused by the retardance error $\\delta$. Your final expression should be valid for small $\\delta$ and given to the lowest non-vanishing order in $\\delta$.", "solution": "1. The output intensity after the rotating retarder and fixed polarizer is $I_{\\rm out}=\\frac{1}{2}\\bigl(I_{\\rm in}+S_{1}\\bigr)$, where $S_1$ is the second Stokes component after the retarder: $S_1=M_{R,11}Q+M_{R,12}U$.\n2. For $\\Delta=\\pi+\\delta$ with $\\delta\\ll1$, we have $\\cos\\Delta = -1+\\frac{\\delta^2}{2}+O(\\delta^4)$ and $1-\\cos\\Delta = 2-\\frac{\\delta^2}{2}+O(\\delta^4)$.\n3. From the Mueller matrix, the relevant components are $M_{R,11}=\\cos^2(2\\theta)+\\sin^2(2\\theta)\\cos\\Delta \\approx \\cos(4\\theta)+\\frac{\\delta^2}{2}\\sin^2(2\\theta)$ and $M_{R,12}=\\cos(2\\theta)\\sin(2\\theta)\\,(1-\\cos\\Delta) \\approx \\sin(4\\theta)-\\frac{\\delta^2}{4}\\sin(4\\theta)$.\nThus, $S_{1} \\approx \\left(\\cos(4\\theta)+\\frac{\\delta^2}{2}\\sin^2(2\\theta)\\right)Q + \\left(\\sin(4\\theta)-\\frac{\\delta^2}{4}\\sin(4\\theta)\\right)U$.\n4. Writing $\\sin^2(2\\theta)=\\frac{1}{2}(1-\\cos(4\\theta))$, the terms modulating at $4\\theta$ become $S_{1}^{(4\\theta)} \\approx (1-\\frac{\\delta^2}{4})\\left(Q\\cos(4\\theta)+U\\sin(4\\theta)\\right)$.\nHence the modulation amplitude is $A_{\\rm real} \\approx \\frac{1}{2}(1-\\frac{\\delta^2}{4})\\sqrt{Q^2+U^2}$, while for an ideal HWP ($\\delta=0$), $A_{\\rm ideal}=\\frac{1}{2}\\sqrt{Q^2+U^2}$.\n5. The efficiency is $\\eta=\\frac{A_{\\rm real}}{A_{\\rm ideal}}=1-\\frac{\\delta^2}{4}$, so the loss is $1-\\eta=\\frac{\\delta^2}{4}$.", "answer": "$$\\boxed{\\frac{\\delta^2}{4}}$$", "id": "248789"}, {"introduction": "Modern radio astronomy increasingly frames image reconstruction as a convex optimization problem, leveraging prior knowledge about the sky to overcome incomplete data. This advanced exercise introduces the Iterative Shrinkage-Thresholding Algorithm (ISTA), a powerful method used in compressed sensing to recover sparse images [@problem_id:249083]. By deriving the ISTA update rule, you will explore the mathematical machinery that combines data fidelity with a sparsity-promoting regularizer, representing a significant conceptual leap beyond classical deconvolution methods like CLEAN.", "problem": "In synthesis imaging, such as radio interferometry, the incomplete sampling of the Fourier plane (or uv-plane) makes image reconstruction an ill-posed inverse problem. The relationship between the measured complex visibilities, $y \\in \\mathbb{C}^M$, and the sky brightness distribution (the image), $x \\in \\mathbb{C}^N$, can be modeled as a linear system $y = Ax + n$, where $A \\in \\mathbb{C}^{M \\times N}$ is the measurement operator (encoding Fourier transforms and the sampling pattern) and $n$ is measurement noise.\n\nTo reconstruct the image, we can solve a convex optimization problem that balances data fidelity with a regularization prior. A common approach to promote sparsity in the image is to use an L1-norm regularizer. The resulting optimization problem is:\n$$\n\\min_{x \\in \\mathbb{C}^N} J(x) \\quad \\text{where} \\quad J(x) = \\frac{1}{2} \\|y - Ax\\|_2^2 + \\lambda \\|x\\|_1\n$$\nHere, $\\|v\\|_2 = \\sqrt{\\sum_j |v_j|^2}$ is the Euclidean (L2) norm, $\\|v\\|_1 = \\sum_j |v_j|$ is the L1-norm, and $\\lambda > 0$ is a parameter that controls the strength of the regularization.\n\nThis problem can be solved using the Iterative Shrinkage-Thresholding Algorithm (ISTA), which is a form of a proximal gradient method. The algorithm generates a sequence of image estimates $x_k$ that converge to the solution. Each iteration is composed of two steps: a gradient descent step on the differentiable data-fidelity term, followed by a proximal mapping step corresponding to the L1-norm regularizer.\n\nThe proximal mapping step involves the element-wise complex soft-thresholding operator, $S_{\\tau}: \\mathbb{C}^N \\to \\mathbb{C}^N$, defined for a threshold $\\tau > 0$ as:\n$$\n[S_{\\tau}(z)]_j = \\left( 1 - \\frac{\\tau}{|z_j|} \\right)_+ z_j = \\frac{z_j}{|z_j|} \\max(|z_j| - \\tau, 0)\n$$\nwhere the subscript $j$ denotes the $j$-th component of a vector and $(u)_+ = \\max(u, 0)$.\n\nGiven this framework, derive the complete iterative update rule for the image estimate $x_{k+1}$ as a function of the previous estimate $x_k$, the data $y$, the operator $A$ (and its Hermitian conjugate $A^H$), the regularization parameter $\\lambda$, and a step-size parameter $\\alpha > 0$.", "solution": "The problem is to find the iterative update rule for minimizing the objective function $J(x) = f(x) + g(x)$, where:\n- $f(x) = \\frac{1}{2} \\|y - Ax\\|_2^2$ is the smooth, differentiable data-fidelity term.\n- $g(x) = \\lambda \\|x\\|_1$ is the non-differentiable regularization term.\n\nThe Iterative Shrinkage-Thresholding Algorithm (ISTA) is a proximal gradient method. The general update rule for an optimization problem of the form $\\min_x f(x) + g(x)$ is:\n$$\nx_{k+1} = \\text{prox}_{\\alpha g}(x_k - \\alpha \\nabla f(x_k))\n$$\nwhere $\\alpha$ is the step size, $\\nabla f(x)$ is the gradient of $f(x)$, and $\\text{prox}_{\\gamma}(\\cdot)$ is the proximal operator associated with the function $\\gamma(\\cdot)$.\n\nThe derivation proceeds in two main steps:\n1.  Calculate the gradient of the data-fidelity term, $\\nabla f(x)$.\n2.  Calculate the proximal operator for the regularization term, $\\text{prox}_{\\alpha g}(z)$.\n\n**Step 1: Gradient of the data-fidelity term**\n\nThe data-fidelity term is $f(x) = \\frac{1}{2} \\|y - Ax\\|_2^2$. We can write the squared L2-norm using the inner product, keeping in mind that the vectors are complex:\n$$\nf(x) = \\frac{1}{2} \\langle y - Ax, y - Ax \\rangle = \\frac{1}{2} (y - Ax)^H (y - Ax)\n$$\nwhere $H$ denotes the Hermitian conjugate (conjugate transpose). Expanding this expression gives:\n$$\nf(x) = \\frac{1}{2} (y^H y - y^H Ax - x^H A^H y + x^H A^H A x)\n$$\nTo find the gradient with respect to the complex vector $x$, we use Wirtinger calculus. The complex gradient is defined as $\\nabla_x f = 2 \\frac{\\partial f}{\\partial x^*}$, where $x^*$ is the complex conjugate of $x$. Treating $x$ and $x^*$ as independent variables, we differentiate with respect to each component of $x^H$ (or $x^*$).\n$$\n\\frac{\\partial f}{\\partial x^H} = \\frac{1}{2} (-A^H y + A^H A x)\n$$\nThe standard complex gradient is twice this quantity:\n$$\n\\nabla f(x) = 2 \\frac{\\partial f}{\\partial x^H} = A^H A x - A^H y = A^H (Ax - y)\n$$\nThus, the gradient descent update part of the iteration is $x_k - \\alpha \\nabla f(x_k) = x_k - \\alpha A^H(Ax_k - y)$.\n\n**Step 2: Proximal operator of the L1-norm term**\n\nThe proximal operator for the function $\\alpha g(x) = \\alpha \\lambda \\|x\\|_1$ is defined as:\n$$\n\\text{prox}_{\\alpha g}(z) = \\arg\\min_{x \\in \\mathbb{C}^N} \\left( \\alpha \\lambda \\|x\\|_1 + \\frac{1}{2} \\|x - z\\|_2^2 \\right)\n$$\nLet's set the threshold parameter $\\tau = \\alpha\\lambda$. The objective function to minimize is:\n$$\nL(x) = \\tau \\sum_{j=1}^N |x_j| + \\frac{1}{2} \\sum_{j=1}^N |x_j - z_j|^2\n$$\nThis expression is separable, which means we can minimize it for each component $x_j$ independently:\n$$\n\\min_{x_j \\in \\mathbb{C}} \\left( \\tau |x_j| + \\frac{1}{2} |x_j - z_j|^2 \\right)\n$$\nLet $x_j = r_x e^{i\\phi_x}$ and $z_j = r_z e^{i\\phi_z}$. The second term is minimized when the phases are equal, i.e., $\\phi_x = \\phi_z$. With this, the problem reduces to minimizing with respect to the magnitude $r_x \\ge 0$:\n$$\n\\min_{r_x \\ge 0} \\left( \\tau r_x + \\frac{1}{2} (r_x - r_z)^2 \\right)\n$$\nIf $r_z > \\tau$, the derivative of the objective with respect to $r_x$ is $\\tau + (r_x - r_z)$. Setting this to zero gives $r_x = r_z - \\tau$. Since we assumed $r_z > \\tau$, this solution is positive, $r_x > 0$, and is therefore valid.\n\nIf $r_z \\le \\tau$, the global minimum of the unconstrained quadratic $(r_x - r_z)^2$ occurs at $r_x = r_z \\le \\tau$. For $r_x > 0$, the derivative $\\tau + r_x - r_z$ is positive since $r_x>0$ and $\\tau-r_z \\ge 0$. This means the function is increasing for all $r_x > 0$. The minimum must therefore occur at the boundary of the domain, $r_x = 0$.\n\nCombining these cases, the optimal magnitude is $r_x = \\max(r_z - \\tau, 0) = \\max(|z_j| - \\tau, 0)$.\nThe optimal complex value $x_j$ therefore has the phase of $z_j$ and the magnitude $\\max(|z_j| - \\tau, 0)$:\n$$\nx_j = e^{i\\phi_z} \\max(|z_j| - \\tau, 0) = \\frac{z_j}{|z_j|} \\max(|z_j| - \\tau, 0)\n$$\nThis is precisely the definition of the complex soft-thresholding operator $S_{\\tau}(z_j)$ provided in the problem statement. Therefore, $\\text{prox}_{\\alpha\\lambda\\|\\cdot\\|_1}(z) = S_{\\alpha\\lambda}(z)$.\n\n**Step 3: Combine to form the ISTA update rule**\n\nNow we substitute the results from Step 1 and Step 2 into the general proximal gradient update formula:\n$$\nx_{k+1} = \\text{prox}_{\\alpha g}(x_k - \\alpha \\nabla f(x_k))\n$$\nThe argument of the proximal operator is the gradient descent step:\n$$\nz_k = x_k - \\alpha \\nabla f(x_k) = x_k - \\alpha A^H(Ax_k - y)\n$$\nApplying the proximal operator (soft-thresholding) to this result gives the final update rule:\n$$\nx_{k+1} = S_{\\alpha\\lambda} \\left( x_k - \\alpha A^H (A x_k - y) \\right)\n$$", "answer": "$$\n\\boxed{S_{\\alpha\\lambda} \\left( x_k - \\alpha A^H (A x_k - y) \\right)}\n$$", "id": "249083"}]}