## 引言
现代处理器是速度的奇迹，但我们的数字世界正面临日益严重的效率危机。计算机将“思考”的处理器与“记忆”的内存分离开来的基本设计，造成了一种被称为[冯·诺依曼瓶颈](@entry_id:1133907)的交通拥堵。这种持续的数据穿梭消耗大量能源并扼杀性能，对于人工智能等数据密集型任务尤其如此。本文将介绍[内存计算](@entry_id:1122818)（In-Memory Computing, IMC），这是一种革命性的范式，旨在通过从根本上反思我们*如何*计算来解决这个问题。它解决了我们的计算能力与过时架构所施加的物理限制之间的关键差距。在接下来的章节中，您将探索使 IMC 成为可能的核心原理，并发现其在各种前沿领域的变革性应用。第一章“原理与机制”将深入探讨使内存能够进行[计算的物理学](@entry_id:139172)和体系结构。随后的“应用与跨学科联系”将展示 IMC 如何加速人工智能革命并搭建通往神经科学的桥梁。

## 原理与机制

想象一下，你是一位正在准备一顿精致多道菜大餐的大厨。你拥有一个最先进的厨房，烤箱能瞬间加热，刀具锋利无比。但有一个问题：你存放所有食材的储藏室位于一条非常长而狭窄的走廊的另一端。对于每一种食材——一撮盐、一根欧芹——你都必须走过走廊，找到物品，然后走回来。很快，你意识到你几乎所有的时间都花在了走路而不是烹饪上。你那令人难以置信的烹饪技巧都浪费在了后勤工作上。

简而言之，这就是现代计算的核心危机。半个多世纪以来，我们一直在一种架构蓝图上构建我们的数字世界，尽管它才华横溢，但却存在这个根本性的缺陷。要理解[内存计算](@entry_id:1122818)的革命，我们必须首先领会它试图取代的那种优雅但最终受限的设计。

### 最初的原罪：双盒记

在计算机时代的黎明，杰出的数学家 [John von Neumann](@entry_id:270356) 和他的同事们构想出一种设计惊人地简单而强大的机器：**存储程序式计算机**。其思想是，计算机的指令（食谱）和它操作的数据（食材）将共同存放在同一个地方，一个统一的内存中。一个中央处理器，或称 **CPU**，将负责逐一获取和执行这些指令。

这种统一是一个神来之笔。这意味着程序可以被当作数据对待；它甚至可以修改自身，从而带来了令人难以置信的灵活性。你使用过的几乎每一台计算机，从台式个人电脑到智能手机，都是这种[冯·诺依曼架构](@entry_id:756577)的后代。但它创造了一个根本性的分野：世界被分成了两个盒子，CPU（“思考者”）和内存（“记忆者”），通过一条[数据总线](@entry_id:167432)（那条长而窄的走廊）连接。

CPU 的每一个动作都需要在这条总线上走一趟。要将两个数字相加，它必须：
1.  从内存中获取“加法”指令。
2.  从内存中获取第一个数字。
3.  从内存中获取第二个数字。
4.  执行加法运算。
5.  将结果存回内存。

处理器和内存之间这种持续的、顺序的通信造成了被称为**冯·诺依曼瓶颈**的交通拥堵 。几十年来，我们的处理器——我们的大厨——已经变得快得离谱，每秒能进行数十亿次计算。但总线——那条走廊——却没有跟上步伐。我们在厨房里有了一台超级计算机，但它却因数据而饥饿，大部分宝贵的时间和精力都花在等待下一个食材的到来上。一些设计，如[哈佛架构](@entry_id:750194)，为指令创建了第二条走廊，这有所帮助，但并没有解决处理器和数据身处不同邮政编码的核心问题。

### 通勤的高昂成本

这种分离不仅仅是性能问题，它是一场能源灾难。每当数据来回穿梭时，都会消耗能量。在芯片上，这种移动涉及对微小的导线进行充放电，这些导线就像微型电容器。单个比特穿过导线的能量可以通过简单的物理关系 $E = \frac{1}{2} C V^2$ 来建模，其中 $C$ 是导线的电容， $V$ 是电压。虽然每比特的能量微不足道，但现代计算移动的比特数量却达到了天文数字。

考虑一下我们这个时代决定性的工作负载：人工智能。深度神经网络本质上是数学“神经元”的巨大集合，由“突触”连接，其强度由数值权重表示。一个关键操作是**[矩阵向量乘法](@entry_id:140544)**，其中输入向量与一个巨大的权重矩阵相乘 。这个操作可能涉及数百万甚至数十亿个权重。

在传统架构中，执行此计算意味着从主内存中获取每一个权重，将其拖到 CPU，在乘法中使用它，然后继续下一个。这种[数据传输](@entry_id:276754)，即从内存到处理器的“通勤”，现在占了人工智能计算期间总能耗的绝大部分——在某些情况下超过 $90\%$。我们在移动数据上消耗的能量比在计算数据上消耗的还要多。

这就是我们所处的荒谬境地。我们完善了计算的艺术，却被平凡的传输行为所扼杀。但这种成本是不可避免的吗？这是物理定律强加的基本税收吗？

### 宇宙之税与可避免之费

事实上，计算确实存在一个基本的物理成本。1961年，Rolf Landauer 发现了信息论与[热力学](@entry_id:172368)之间深刻的联系。**兰道尔原理**指出，任何逻辑上不可逆的操作，例如擦除一位信息，都必须以热量的形式向环境中耗散最低限度的能量。要擦除一位，你必须支付一笔不可协商的能量税，至少为 $k_B T \ln 2$，其中 $T$ 是温度， $k_B$ 是[玻尔兹曼常数](@entry_id:142384)  。这是宇宙为使信息消失而收取的费用。

但关键在于：这个基本极限是一个极其微小的能量，比我们现有计算机执行一次操作实际使用的能量小数百万倍。兰道尔极限与我们实际能耗之间的巨大鸿沟揭示了一个美丽的真理：计算的高昂成本并非根本性的。它是我们*如何*计算的产物。我们支付的不是宇宙之税；我们为数据的日常通勤支付了一笔巨大且完全可以避免的费用。

这一认识是**[内存计算](@entry_id:1122818)（IMC）**的哲学核心。其策略简单得惊人：如果通勤是问题所在，那就消除它。让数据“在家工作”。让我们在数据所在之处进行计算。

### 用物理学计算：IMC 范式

[内存计算](@entry_id:1122818)颠覆了旧有架构。IMC 不再是只有一个只存储数据的被动内存和一个只进行计算的强大 CPU，而是赋予了内存本身执行计算的能力。内存阵列不再只是一个储藏室；它变成了一个分布式厨房，成千上万个简单的烹饪任务可以同时进行。

这怎么可能呢？通过巧妙地利用物理定律。

考虑一个称为**交叉阵列**的设备。它是一个由水平和垂直导线组成的简单网格。在每个交叉点，我们可以放置一个双端电子元件，如电阻器或更先进的“[忆阻器](@entry_id:204379)”，其电导（电阻的倒数）可以被编程为特定值。现在，让我们将神经网络的权重矩阵映射到这个网格上，将每个节点 $(k, j)$ 的电导 $G_{k,j}$ 编程为与突触权重 $W_{k,j}$ 成正比。

为了执行[矩阵向量乘法](@entry_id:140544) $Y_j = \sum_k X_k W_{k,j}$，我们采取一种非凡的方法。我们将输入向量 $X$ 转换为一组输入电压 $V_k$，并将其施加到网格的行上。根据[欧姆定律](@entry_id:276027)（$I = GV$），流经每个器件的电流为 $I_{k,j} = G_{k,j} V_k$。这是输入电压和编程电导（权重）的乘积。并且，由于基尔霍夫电流定律，流出每个垂直列 $j$ 的总电流就是从不同行流入的所有电流之和：$I_{j}^{\text{total}} = \sum_k I_{k,j}$。

结果是神奇的。每个列的总输出电流，依据电路的物理原理，就是[矩阵向量乘法](@entry_id:140544)的结果。计算在整个阵列上并行执行，时间仅为电子流过网格所需的时间。没有权重的来回穿梭。数据没有移动，因为数据*就是*计算机。这就是**模拟[内存计算](@entry_id:1122818)**的精髓。

此外，IMC 架构天然地适用于另一种受大脑启发的效率技巧：**事件驱动处理**。大脑的效率非常高，因为它的神经元大部分时间是安静的，只有在有重要信息要传递时才会发出“脉冲”或“事件”。类似地，许多 AI 系统的真实世界输入是**稀疏**的，这意味着它们大部分是零。

传统的冯·诺依曼机器会盲目地执行其程序，将每个权重与每个输入相乘，在与零相乘上浪费了巨大的精力。然而，事件驱动的 IMC 系统只激活与非零输入相对应的电路 。如果一个层中只有 $1\%$ 的神经元是活跃的，系统就只消耗那 $1\%$ 计算所需的能量。在一个假设但现实的场景中，这种将内存和计算协同定位，同时利用稀疏性的协同作用，与传统方法相比，可以实现数十万倍的节能 。

这是一种范式转变。几十年来，我们一直使用软件技巧——巧妙的循环排序、复杂的数据分块缓存技术、以及非连续数据的打包——来更好地管理 CPU 与内存之间的鸿沟 。这些都是为了让大厨去储藏室的路更有效率而付出的英勇努力。相比之下，内存计算拆除了储藏室，并在其位置上建造了一个厨房。它认识到，对于我们这个时代许多最重要的计算问题，内存和逻辑的分离不再是一个有用的抽象，而是一个致命的负担。通过将它们重新统一，我们不仅仅是在制造一台更快的计算机；我们正在构建一台以更符合物理世界本身的方式进行计算的机器。

