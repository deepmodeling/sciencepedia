## 引言
对原子和分子的精确模拟是设计新材料、理解化学反应以及揭开物理世界奥秘的关键。然而，源于量子力学的最精确方法在计算上极其昂贵，以至于只能用于模拟数百个原子在极短时间内的行为。虽然[机器学习原子间势](@entry_id:751582)（MLIPs）为弥合这一差距提供了一条途径，但其准确性完全取决于训练数据的质量和全面性。这就带来了一个关键挑战：我们如何能有效地生成一个能够捕捉所有相关物理现象的训练数据集，而又不必承担计算数百万个[量子数](@entry_id:145558)据点的昂贵成本？

本文介绍**[主动学习](@entry_id:157812)**，这是一种强大的范式，它将这种暴力数据收集问题转变为模拟与理论之间的智能、高效对话。[主动学习](@entry_id:157812)工作流程不是猜测在何处收集数据，而是让模拟本身能够识别其知识的边界，并战略性地在最需要的地方请求新信息。您将了解该方法核心的优雅反馈循环，探索采样器、学习器和预言机的角色。然后，我们将深入探讨使这些模型能够工作的基本物理和架构原理。最后，我们将综览该方法所开启的广阔应用前景，展示[主动学习](@entry_id:157812)如何革新从材料科学到计算化学的各项研究。

## 原理与机制

想象一下，您正试图为一片广阔而未知的地貌绘制一张完美详尽的地图。您拥有一颗非常昂贵但绝对精确的卫星（我们的“预言机”，一种高保真的量子力学计算，如**密度泛函理论**，即DFT），它可以为您提供您选择的任何单点的精确海拔。然而，每张卫星图像都既昂贵又耗时。您的目标是为一位徒步旅行者（我们的“采样器”，一次**分子动力学**，即MD模拟）创建一张足够好的工作地图，让他们能够探索这片地貌，而无需为每一寸土地都付费拍摄卫星图像。您会怎么做？

您不会随机勘测这片土地。您会从进行几次稀疏的测量开始。然后，您会建立一张粗略的初步地图（我们的“学习器”，一个**[机器学习原子间势](@entry_id:751582)**，即MLIP）。接着，您会让徒步旅行者开始行走。当徒步旅行者冒险进入您的地图只是凭空猜测的区域时，您会叫住他们说：“等等！我对这个区域不太确定。”然后，您会调用卫星，获取该确切地点的精确读数。有了这个新的、真实的数据点，您就可以更新您的地图，使其更加准确。这个使用临时地图进行探索、识别最不确定区域并向预言机查询“基准真相”的迭代过程，正是**主动学习**的精髓。这是近似与现实之间一场优美而高效的对话。

### 宏大循环：与预言机的对话

这场“对话”可以被形式化为一个强大的闭环工作流程，它本身就反映了[科学方法](@entry_id:143231)。在任何给定时刻，我们当前的MLIP代表了我们关于[势能面](@entry_id:143655)的**假设**。这是我们基于迄今为止所见数据做出的最佳猜测 。

这个循环通过几个关键步骤进行，涉及三个不同的角色 ：

1.  **采样器（探索者）：** MD模拟引擎扮演我们的探索者。它使用当前MLIP预测的力，根据[牛顿定律](@entry_id:163541) $m_i \ddot{\mathbf{r}}_i(t) = -\nabla_{\mathbf{r}_i} U(\mathbf{R}(t); \theta)$ 来推动原子在时间上前进。这会生成一条新原子构型的轨迹，实际上是使用我们当前不完美的地图来探索地貌。

2.  **学习器（制图师）：** MLIP是我们的智能制图师。对于采样器访问的每一个[新构型](@entry_id:199611)，学习器不仅预测能量和力，还估计其自身对该预测的置信度。当其自我评估的不确定性变得过高时——即当它意识到自己正在猜测时——它会将该构型标记为一个关注点。这就是它向预言机提出的“问题”。

3.  **预言机（真理之源）：** 对于被学习器标记的少数构型，我们执行一次昂贵但高度准确的量子力学计算。这个预言机为这些特定点提供了“基准真相”的能量和力。这些新的、高保真的数据就是将用于修正我们地图的**证据**。

4.  **更新：** 学习器获取这些新证据并将其纳入其训练集。然后，它重新训练或更新其参数，以更好地匹配已知的基准真相，包括新的数据点。结果是一个经过提炼的假设——一个更准确的MLIP。然后循环重复，采样器现在使用这张改进后的地图进行探索。

这个“假设、探索、证据、更新”的循环，使得模型能够智能地将其学习精力集中在广阔原子[构型空间](@entry_id:149531)中那些既具有物理相关性（因为MD模拟访问了它们）又未被当前模型充分理解的区域。

### 原子语言：用描述符编码物理学

在机器能够学习原子世界之前，它必须首先学会正确地看待它。对于每个原子，一份原始的[笛卡尔坐标](@entry_id:167698) $(x, y, z)$ 列表是描述一个分子的糟糕方式。为什么？因为基本的物理定律不关心你任意设定的坐标系。如果你将一个水分子向左移动三英尺，或在空间中旋转它，它的能量保持完全相同。此外，两个氢原子是不可区分的；如果你交换它们，你得到的仍然是具有相同能量的同一个水分子。

这些并非无关紧要的细节；它们是自然界的[基本对称性](@entry_id:161256)：**[平移不变性](@entry_id:195885)**、**[旋转不变性](@entry_id:137644)**和相同粒子的**[置换不变性](@entry_id:753356)**。一个真正符合物理的模型必须尊重它们。虽然一个强大的神经网络最终可以从海量数据中学习到这些对称性，但这样做效率极低。这就像强迫一个学生在解决任何一个问题之前，都必须从头重新推导[欧几里得几何](@entry_id:634933)。

更优雅的解决方案是将这些对称性直接构建到模型使用的“语言”中。我们不向模型输入原始坐标。相反，我们为每个原子的局部环境计算一组**描述符**。这些描述符是数学函数，其设计初衷就是为了实现平移、旋转和[置换不变性](@entry_id:753356) 。

例如，描述符可能会使用到所有邻近原子的距离集合，而不是坐标，因为距离天然具有[不变性](@entry_id:140168)。为了处理置换，它可能会将来自某一特定类型的所有邻居（例如，所有邻近的碳原子）的贡献相加，因为求和不关心顺序。像**[原子中心对称函数](@entry_id:174796)（ACSF）**或**原子位置平滑重叠（SOAP）**这样的先进方法，使用更复杂的距离和角度组合或[球谐函数展开](@entry_id:188485)，来为一个原子的局部环境创建一个丰富、独特且完全不变的指纹  。通过使用这种蕴含物理知识的语言，[机器学习模型](@entry_id:262335)得以解放，可以专注于问题中真正复杂的部分：学习决定能量的微妙量子力学相互作用。

### 学习的架构：构建[广延性](@entry_id:144932)知识

正如我们模型的输入必须遵守物理定律一样，模型本身的架构也必须如此。能量最重要但又最微妙的特性之一是**尺寸[广延性](@entry_id:144932)**。如果你有两个系统 $\mathcal{A}$ 和 $\mathcal{B}$，它们相距甚远以至于不发生相互作用，那么组合系统的总能量必须是它们各自能量之和：$E(\mathcal{A} \cup \mathcal{B}) = E(\mathcal{A}) + E(\mathcal{B})$。这听起来显而易见，但一个将整个系统作为单一输入的朴[素模型](@entry_id:155161)几乎肯定无法通过这个测试。

[Behler-Parrinello](@entry_id:177243) [神经网络架构](@entry_id:637524)为此问题提供了一个极其简洁的解决方案 。它不试图一次性预测系统的总能量。相反，它假定总能量是每个原子各自贡献的总和：
$$
E(\mathbf{R}) = \sum_{i=1}^{N} \varepsilon^{(Z_i)}(\mathcal{N}_i)
$$
在这里，$\varepsilon^{(Z_i)}$ 是特定于原子 $i$ 的元素类型 $Z_i$ 的神经网络，而 $\mathcal{N}_i$ 是描述该原子局部环境的不变描述符向量。

其奥妙在于描述符的局域性。每个描述符 $\mathcal{N}_i$ 仅使用原子 $i$ 在一个有限的**截断半径** $r_c$ 内的邻居来计算。如果我们的两个系统 $\mathcal{A}$ 和 $\mathcal{B}$ 的间距大于 $r_c$，那么 $\mathcal{A}$ 中任何原子的局部环境完全不受 $\mathcal{B}$ 存在的影响，反之亦然。它的描述符向量保持不变，其预测的能量贡献也同样不变。总能量因此自然地分离为来自 $\mathcal{A}$ 中原子的贡献之和与来自 $\mathcal{B}$ 中原子的贡献之和。尺寸[广延性](@entry_id:144932)不是模型学习到的东西；它是其架构的固有属性。

### 提问的艺术：量化不确定性

[主动学习](@entry_id:157812)的核心是模型能够提问：“我下一步应该在哪里学习？” 这要求模型知道它所不知道的。这种自我意识通过**不确定性**的概念来捕捉，它有两种类型 。

*   **[偶然不确定性](@entry_id:634772)**是数据本身固有的、不可减少的随机性或噪声。如果你在测量一个有噪声的信号，这就是你无论进行多少次测量都无法消除的内在模糊性。
*   **认知不确定性**是模型的不确定性，或者简单地说，是无知。它源于参数空间某些区域缺乏数据。这是我们*可以*通过收集更多数据来减少的不确定性。

在我们的案例中，DFT预言机通常是一个确定性的计算。对于一个给定的原子构型，它返回一个精确的答案。因此，我们可以认为我们的数据基本上是无噪声的，[偶然不确定性](@entry_id:634772)可以忽略不计 。我们关心的所有不确定性都是认知的：我们的模型在哪里只是在猜测？

一个估算这种不确定性的有效方法是使用一个委员会，或称**集成**模型。想象一下征求几位不同专家的意见。在主题被充分理解的领域，他们都会给出相似的答案。但在知识稀疏的领域，他们的意见会产生分歧。他们分歧的程度就是集体不确定性的直接度量。

在实践中，我们独立训练多个MLIP。当MD采样器提出一个新的构型时，我们让委员会中的每个模型预测力。它们预测值之间的巨大方差表明认知不确定性很高。例如，一个确保MD模拟保持稳定的稳健策略是，当任意两个模型对任意单个原子的最大力分歧超过某个阈值时，触发一次新的预言机计算 。这种保守的“最坏情况”度量确保了我们在地图的弱点导致我们的探索者掉下悬崖之前，就将其加固。

这个直观的想法在贝叶斯统计中有深厚的根基。集成成员之间的分歧是模型预测后验方差的一个实用近似。选择最大方差点可以被证明等同于选择能为模型参数提供最大[信息增益](@entry_id:262008)的点，这是一种被称为[贝叶斯最优实验设计](@entry_id:746727)的策略 。因此，“选择委员会[分歧](@entry_id:193119)最大的点”这一简单的启发式方法，实际上是一种以尽可能高的效率进行学习的、非常有原则的方式。

### 查询策略：超越简单不确定性

问“我在哪里最不确定？”总是最佳策略吗？主动学习的艺术在于其精妙的提问策略。

在学习的最初阶段，我们的模型就像一个新生儿。它对世界知之甚少，其自身的不确定性感可能并不可靠。在这种情况下，一个更简单、更稳健的策略可能更好。**最远点采样（FPS）**是一种纯粹的几何方法，完全不依赖于模型的自我评估。它只是问：“在我们所有可以查看的地方中，哪一个与我们以前见过的任何地方都最不相同？” 这是通过在高维描述符空间中寻找与任何现有训练点距离最远的点来衡量的。在开始时优先采用这种“仅考虑多样性”的选择，可以确保我们建立一个分布良好、填充空间的数据基础，这对于稳定动态模拟的初始步骤和防止模型做出灾难性的外推至关重要 。

随着模型的成熟，一个新的矛盾出现了：**[探索-利用权衡](@entry_id:1124776)** 。
*   **探索**是旨在通过减少各处的不确定性来构建一个全局准确[势函数](@entry_id:176105)的动力。这就像试图绘制整个大陆的地图。
*   **利用**是旨在为某个特定的、有针对性的性质减少不确定性的动力。这就像需要为一次特定的旅程找到最低的山隘。例如，如果我们正在研究一个特定的化学反应，我们可能只关心势函数在反应路径上的准确性，而不是构型空间中某个不相关的角落。

现代[主动学习](@entry_id:157812)方案不必非此即彼。它们可以使用一个自适应的、加权的采集函数来平衡这两个目标。在过程的早期，权重放在探索上，以构建一个稳健的通用模型。随着全局不确定性的降低，权重可以动态地转向利用，将昂贵的预言机调用集中于精炼我们最关心的特定性质。

### 知止之时：对话的终结

最后，我们如何知道何时完成？学习循环不能永远运行下去。一个自然的[停止准则](@entry_id:136282)是，当模型自己报告的不确定性在所有感兴趣的区域都低于一个小阈值时终止。但如果模型是一个自信的骗子呢？它可能在一个其预测实际上大错特错的区域报告低不确定性。这个**校准不当**的问题可能导致学习过程危险地过[早停](@entry_id:633908)止。

为了防止这种情况，我们需要一次期末考试。我们必须在一个**留出的验证轨迹**上测试我们的模型——这些数据在训练或主动学习循环中从未出现过 。通过将模型在这一[验证集](@entry_id:636445)上的预测与真实的预言机值进行比较，我们可以检查它的自信是否合理。

如果我们发现模型的实际误差平均是其报告不确定性的两倍，我们就发现了一个校准问题。然后我们可以计算一个**重新校准因子**来修正模型的[不确定性估计](@entry_id:191096)，使其更加诚实。例如，我们可能会发现需要将其所有的不确定性值乘以二。

有了这个经过统计验证和重新校准的[不确定性度量](@entry_id:152963)，我们最终可以定义一个稳健的[停止准则](@entry_id:136282)。我们仅当这个*诚实的*[不确定性度量](@entry_id:152963)低于我们期望的精度容限时，才停止主动学习循环。这确保了当我们宣布模型“完成”时，我们是基于高度的统计置信度，知道我们绘制的原子地貌图不仅详细，而且值得信赖。无论是假设特定误差分布的[参数化](@entry_id:265163)方法，还是做出较少假设的非[参数化](@entry_id:265163)方法，如**保形预测**，都为实现这关键的最后一步提供了有原则的方法 。

