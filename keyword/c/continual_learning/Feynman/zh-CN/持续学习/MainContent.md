## 引言
人类大脑拥有卓越的能力，能够持续获取新技能和知识，同时保留已经掌握的内容。这种在灵活性（可塑性）和记忆保留（稳定性）之间的优雅平衡是智能的基石。然而，几十年来，人工智能一直在这个概念上举步维艰，主要依赖于“批量学习”范式，即模型在静态数据集上一次性训练完成，然后被冻结。这种方法从根本上不适用于我们这个动态变化的世界，在医药、金融到自动驾驶等领域，数据流都在不断演变。当标准的人工智能模型按顺序进行训练时，它们常常遭受“[灾难性遗忘](@entry_id:636297)”——这是一种惊人的失败，学习一项新任务会完全覆盖先前任务的知识。

本文旨在解决静态人工智能与动态智能需求之间的这一关键差距。文章介绍了持续学习的原则，这是机器学习的一个子领域，致力于创建能够从连续数据流中增量学习的系统。读者将全面理解为什么人工智能模型会遗忘，以及研究人员为使它们能够记忆而开发的巧妙策略。

首先，在“原则与机制”一章中，我们将剖析[稳定性-可塑性困境](@entry_id:1132257)以及神经网络中[灾难性遗忘](@entry_id:636297)的根本原因。然后，我们将探讨三种主要的解决方案：重放、正则化和结构方法。之后，“应用与跨学科联系”一章将展示这些理论不仅仅是学术探讨，而是一项变革性技术，对工业维护、全球监测、个性化医疗和[人工智能治理](@entry_id:915841)具有深远影响，推动我们更接近于构建真正的终身学习机器。

## 原则与机制

### 学习的悖论：可塑性与稳定性

你如何学习一项新技能？想象一下学习一门新语言，比如日语。你花了数小时学习新单词、语法和字符。但这个过程会让你忘记你的母语英语吗？当然不会。你的大脑似乎拥有一种卓越的能力，能够在获取新知识的同时，稳健地保留已经掌握的内容。一位医生学着识别一种新病毒株的症状，并不会因此变得无法诊断流感。这种在足够灵活以学习新事物和足够稳定以不忘记旧事物之间的奇妙平衡，是智能的基石。

在神经科学和机器学习的语言中，我们称之为**[稳定性-可塑性困境](@entry_id:1132257)** 。**可塑性**是获取新信息和适应变化的能力。**稳定性**是保存现有知识。对于任何学习系统，从人类大脑到人工智能，驾驭这种权衡都是核心挑战。

很长一段时间里，人工智能领域在很大程度上回避了这个问题。主导范式是**批量学习**。你会收集一个庞大的静态数据集——比如，来自互联网的数百万张图片——然后在一个巨大的神经网络上训练数周。一旦训练完成，模型就被冻结并部署。这就像为期末考试死记硬背，然后就再也不学新东西了。只要世界不发生变化，这种方法效果很好。

但真实世界绝非静态。它是一条流动的、非平稳的数据之河 。在医院里，患者群体会变化，新的诊断设备被引进，治疗方案在演进，导致疾病的统计模式随时间漂移 。一辆自动驾驶汽车必须适应新的道路布局和不断变化的天气模式。一个[入侵检测](@entry_id:750791)系统面临着不断发明新攻击形式的对手 。为了在这个动态世界中真正有用，我们的人工智能系统不能是静态的庞然大物；它们必须成为终身学习者，能够优雅地即时更新知识。这就是**持续学习**的前景。

### 遗忘机器：[灾难性遗忘](@entry_id:636297)

那么，如果我们拿一个标准的人工智能模型，试着连续地教它会发生什么？让我们来做一个思想实验。我们拿一个最先进的神经网络，训练它成为识别照片中猫的世界级专家（任务A）。它的表现完美无瑕。现在，我们想扩展它的能力，于是我们开始训练*完全相同的网络*来识别狗（任务B）。在新的训练阶段之后，它变成了一个出色的狗识别专家。但是，当我们给它看一张猫的图片时，它却茫然无知。它已经完全忘记了如何执行任务A。

这种惊人的失败是机器学习中一个著名的问题，称为**[灾难性遗忘](@entry_id:636297)** [@problem_id:5228681, 4431018]。人工智能在学习新东西时，灾难性地覆盖了它先前的知识。为什么会发生这种情况？

把深度神经网络想象成一个由相互连接的旋钮或**参数**组成的复杂网络。学习一项任务，比如识别猫，涉及到将这数百万个旋钮精细地调整到一个非常特定的配置。当我们接着教网络识别狗时，我们的训练算法——通常基于一种称为梯度下降的方法——又开始调整所有这些旋钮。它唯一的目标是减少在狗图片上的误差。该算法没有记忆，也没有指令要保留识别猫的配置。对任务B的更新是“非局部的”；它会波及整个共享网络，扰乱了编码任务A知识的精细设置 。它为更好地识别狗而推动参数的方向，可能与保持擅长识别猫所需的方向直接相反 。这就像一个雕塑家雕刻了一尊美丽的猫雕像，然后被告知要把它变成一只狗——唯一的方法就是凿掉原来的杰作。

这表明，我们标准神经网络的架构与大脑从根本上是不同的，大脑似乎使用稀疏、部分隔离的回路来减轻这种剧烈的干扰。要构建一个持续学习者，我们必须明确地对抗这种健忘的本性。

### 记忆的艺术：持续学习的策略

持续学习的目标是设计出方法，使模型能够学习一系列任务——$T_1, T_2, T_3, \dots$——同时在所有任务上保持良好性能。多年来，研究人员已经发展出三大类策略来实现这一目标。

#### 重放：温习笔记

不忘记某件事最直观的方法就是练习它。这就是**基于重放的方法**背后的简单思想，也称为**[经验回放](@entry_id:634839)** [@problem_id:5228681, 4431018]。当模型学习新任务B时，我们在训练中穿插少量来自旧任务A的样本。

你不需要整个旧数据集。存储在“记忆缓冲区”中的一个小的、有代表性的子集通常就足够了。通过在新旧数据混合体上进行训练，学习算法被迫寻找一个对两个任务都有效的参数配置。更新成为一种妥协，平衡学习新知识的需求和记住旧知识的需求 。

重放是一种强大且通常非常有效的基线方法。然而，它有一个主要缺点：你必须存储旧数据。在许多现实世界的应用中，比如处理敏感的医疗记录，存储原始患者数据受到隐私法规的严格限制 [@problem_id:4431018, 5195410]。如果存储数据不可行，我们就需要一种不同的方法。

#### 正则化：保护重要记忆

如果不是回顾旧材料，而是可以简单地识别出哪些记忆最关键，并在上面贴上“请勿触摸”的标志呢？这就是**基于正则化的方法**的核心思想。这些技术修改了学习目标本身。目标不再仅仅是“最小化新任务的误差”，而是“最小化新任务的误差，*同时不要过多改变对旧任务重要的参数*”。

一个经典的例子是**[弹性权重巩固 (EWC)](@entry_id:634731)** 。在模型学习任务A之后，EWC进行快速分析，以估计网络中每个参数对该任务的重要性。这种重要性使用信息论中的一个量——**费雪信息矩阵**来衡量。当模型接着学习任务B时，一个惩罚项被添加到训练目标中。这个惩罚项就像给每个参数附加一个微小的虚拟弹簧，将其锚定在它在任务A时的值上。每个弹簧的“刚度”与参数的重要性成正比。改变一个不重要的参数很容易，但改变一个对任务A至关重要的参数则需要抵抗一个非常硬的弹簧，从而产生巨大的惩罚。这种优雅的机制保护了旧知识，而无需存储任何旧数据——只需要保存重要性分数。

另一种巧妙的[正则化技术](@entry_id:261393)是**无遗忘学习 (LwF)** [@problem-id:5195410]。在这里，原始模型（“教师”）被用来指导新模型（“学生”）。当学生从新数据和新标签中学习新任务时，它也被训练来模仿被冻结的教师模型在相同新数据上的输出。这迫使学生保留旧模型的“思维方式”。这不仅包括最终答案，还包括它分配的微妙概率——即所谓的**[暗知识](@entry_id:637253)**，它揭示了模型如何看待不同类别之间的相似性。这种指导的强度可以通过一个`temperature`参数来调节，该参数控制教师概率分布的“软度” 。

#### 结构方法：生长新的大脑区域

第三种策略或许是对大脑如何分离知识的最直接类比。**结构方法**在有新任务到来时，修改神经网络本身的结构。例如，在遇到任务B时，系统可以自动分配一组新的神经元专门用于它，同时冻结用于任务A的参数。这为不同的技能创建了独立的、受保护的路径，从设计上防止了干扰。这里的挑战在于管理模型的增长，以及决定如何在不同路径之间有效共享知识而又不导致遗忘。

### 不断变化的世界：[在线学习](@entry_id:637955)与[概念漂移](@entry_id:1122835)

到目前为止，我们的讨论集中在学习一系列不同的任务上。但实际上，变化往往更具流动性和连续性。垃圾邮件发送者的策略不是一夜之间改变的，而是日复一日地演变。数据分布中这种渐进的、持续的变化被称为**[概念漂移](@entry_id:1122835)** 。

为处理这种情况而设计的范式是**[在线学习](@entry_id:637955)**，其中模型在每个新观测值或一小批（“mini-batch”）观测值之后[增量更新](@entry_id:750602)其参数 。这种方法陷入了**响应性**和**稳定性**之间的根本权衡 。

一个在线模型具有高度的**响应性**：一旦反映变化的数据到达，模型就开始适应。然而，这是以牺牲**稳定性**为代价的。因为每次更新都基于非常少的数据（也许是单个样本），模型的参数可能会因噪声而剧烈波动，导致性能不稳定。

另一种选择是**批量重训练**，情况则相反。在这里，人们等待收集一大批新数据，然后定期重新训练模型。这个过程非常**稳定**，因为更新是在许多样本上平均的，平滑了噪声。但它的**响应性**极差。模型在更新之间保持静态，对可能发生的任何漂移都一无所知，其适应前的延迟可能很长 。

复杂的真实世界持续学习系统试图两全其美。它们在线运行，但以智能的方式进行。它们使用像指数加权风险最小化这样的技术来逐渐“忘记”遥远的过去。至关重要的是，它们主动监控自身的性能和传入的数据流，以寻找漂移的迹象，通常使用统计变化点检验。如果检测到漂移，系统可以自动增加其可塑性——例如，通过提高学习率——以更快地适应 。

### 统一的观点：学习即推断

让我们再退一步，从一个更深刻的角度来看待这个问题。从本质上讲，学习是在面对新证据时更新我们的信念并减少我们的不确定性的过程。这是[贝叶斯推断](@entry_id:146958)的自然语言。

**[贝叶斯方法](@entry_id:914731)**不是为“最佳”模型参数寻找单个点估计，而是在可能的参数空间上维持一个完整的概率分布。这个分布代表了我们的不确定性。一个宽的分布意味着我们非常不确定；一个窄而尖的分布意味着我们相当肯定。当一个新的数据点到来时，我们使用贝叶斯法则的引擎来更新我们的信念分布 。

这个框架为[稳定性-可塑性困境](@entry_id:1132257)提供了一个极其优雅的解决方案。当我们看到的数据很少时，我们的信念分布很宽（不确定性高）。我们的预测，即所有可能模型的平均值，被拉向我们最初的先验信念，使它们变得谨慎而稳定。随着越来越多的证据积累，数据的似然性开始压倒先验，我们的信念分布在真实参数值周围变得尖锐，从而实现快速适应——即可塑性。这种对参数不确定性进行积分的过程自然地避免了困扰许多其他方法的过度自信的预测，并帮助模型的输出保持**校准**——也就是说，其预测的概率反映了真实的经验频率 。

这个优美的思想——学习是先验信念和新证据的原则性平衡——并非机器学习所独有。它出现在科学和工程的许多角落。著名的**卡尔曼滤波器**是控制理论的基石，从制导导弹到导航手机GPS无所不包，它完全可以用这些术语来理解 。在每一刻，滤波器都维持着对物体状态（其位置和速度）的信念（一个高斯分布）。它基于物理模型（先验）做出预测，然后从传感器接收到一个带噪声的测量值（证据）。滤波器的更新步骤，即结合预测和测量以产生一个新的、更准确的信念，在数学上等同于解决一个在线回归问题。这个问题中的“正则化”无非是其先验预测的不确定性。这种正则化的强度是动态的；随着滤波器对其预测越来越确定，它会减小 。

从医疗诊断到追踪卫星，我们发现同样的基本原则在起作用：智能系统必须不断而优雅地权衡它们自认为知道的与它们现在所看到的。持续学习正是为了掌握这门艺术，为了构建像我们一样，能够在整个生命周期中学习和适应的机器。

