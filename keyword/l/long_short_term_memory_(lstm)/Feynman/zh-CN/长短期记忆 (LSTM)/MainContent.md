## 引言
跨时间理解上下文是智能的一个标志。从理解句子到预测天气，我们的世界由序列主导，其中过去塑造着现在。对于人工智能而言，对这些[长期依赖](@entry_id:637847)关系进行建模一直是一项艰巨的挑战。简单的序列模型，即循环神经网络（RNN），存在一个根本性缺陷：它们的记忆会迅速衰退，导致几乎不可能连接相隔很长时间的事件。这个“[梯度消失问题](@entry_id:144098)”在我们构建能够真正从历史中学习的机器的能力上造成了重大差距。

本文探讨了针对这一问题的优雅解决方案：[长短期记忆](@entry_id:637886)（[LSTM](@entry_id:635790)）网络。我们将深入这一强大架构的核心，剖析其核心组件，并揭示赋予其如此强大和持久记忆的原理。首先，在“原理与机制”一节中，您将了解到 LSTM 如何使用一个智能“门控”系统来控制信息流，从而克服其前身的局限性。随后，“应用与跨学科联系”一章将展示 LSTM 的深远影响，说明这项单一创新如何彻底改变了基因组学、金融乃至我们对人脑的理解等多个领域。

## 原理与机制

要真正理解[长短期记忆](@entry_id:637886)（LSTM）的奇妙之处，我们必须首先认识到它旨在解决的问题。这个问题从本质上讲，是关于记忆和时间的本性。想象一下你在读一本书。当前页面上一个词的含义可能深刻地依赖于一百页前引入的一个角色。或者，思考一下我们自身生物学的语言。一个基因的活性可能受数万个DNA碱基对之外的一个微小调控[开关控制](@entry_id:261047)。在这些长序列中，上下文就是一切。我们如何能构建一台拥有如此长久而可靠记忆的机器呢？

### [长期记忆](@entry_id:169849)的挑战

处理序列的机器最直接的想法是使用循环。我们可以设计一个小网络，一次读取序列的一个片段——一个词，一个DNA碱基——然后将其自身的输出作为下一步的输入反馈给自己。这就是**[循环神经网络](@entry_id:634803)（RNN）**。在每个时刻，它的内部状态是它刚刚看到的新事物和它之前看到的所有事物记忆的混合体。

这是一个优雅的想法，但它隐藏着一个微妙而致命的缺陷。当我们训练这样的网络时，我们向它展示一个例子并测量其误差。这个误差信号必须沿着时间在循环中[反向传播](@entry_id:199535)，告诉每一步如何调整自身以改善最终结果。这个过程被称为**随时间反向传播**（backpropagation through time）。问题就在这里：每向后传播一步，信号就会乘以网络的内部权重。如果这些权重哪怕只比1小一点，信号就会指数级衰减。这就像一个传话游戏；信息每传递一次，就变得更微弱、更失真。这就是臭名昭著的**[梯度消失问题](@entry_id:144098)**（vanishing gradient problem）。

对于像我们的基因组学例子中那样相距50,000步的依赖关系，误差信号在到达源头时几乎变为零。网络实际上对其遥远的过去变得盲目，无法学习长距离的连接。

我们可以通过一个被称为“加法问题”的简单思想实验来使这一点更具体。想象一个长度为 $L$ 的数字序列。网络的任务是输出在特定标记位置的两个数字之和。如果一个数字在开头，另一个在结尾，那么第一个数字的记忆必须在所有 $L-1$ 个中间步骤中得以保留。对于简单的 RNN，学习信号的强度在每一步都会衰减一个因子，我们称之为 $\rho$。经过 $L-1$ 步后，信号强度与 $\rho^{L-1}$ 成正比。如果 $\rho=0.9$，对于长度为 $L=100$ 的序列，信号会衰减到 $(0.9)^{99}$，这是一个如此之小（约 $0.000027$）的数字，以至于学习变得不可能。记忆消失于无形。

### 门控解决方案：细胞状态

简单 RNN 的缺陷在于信息处理是混乱的。在每一步，整个记忆都经过一次变换并与新的输入混合。短期工作和长期存储之间没有分离。

LSTM 的发明者 Sepp Hochreiter 和 Jürgen Schmidhuber 提出了一个受数字计算机架构启发的绝妙解决方案。他们创建了一个独立的、受保护的信息高速公路，称为**细胞状态**（cell state），用 $c_t$ 表示。你可以把它想象成一条与主序列处理线平行的传送带。这条传送带可以将信息从遥远的过去直接传输到当前，受到的干扰极小。

然而，真正的天才之处不在于传送带本身，而在于控制它的机制。[LSTM](@entry_id:635790) 单元有一系列“门”——充当调节器的专门神经网络，它们决定了哪些信息被允许进入传送带，哪些信息被移除，以及哪些信息被从中读取。

这些门由**逻辑[S型函数](@entry_id:137244)**（logistic sigmoid function）$\sigma(z) = \frac{1}{1 + \exp(-z)}$ 控制。该函数将任意实数压缩到 $(0, 1)$ 区间内。这使其成为一个完美的“调[光开关](@entry_id:197686)”或门控制器：值为 $0$ 意味着“完全关闭”，值为 $1$ 意味着“完全打开”，而中间值则允许部分流动。相比之下，可能写入细胞的新信息则由**[双曲正切函数](@entry_id:634307)**（hyperbolic tangent function）$\tanh(z)$ 处理，该函数将数值压缩到 $(-1, 1)$ 区间内。这是一个关键的设计选择。因为其值域以零为中心，$\tanh$ 允许网络提议对记忆进行增加（正值）或减少（负值）。使用 $\sigma$ 函数的门只允许非负更新，这可能会使记忆偏向于总是增加。

### 记忆的三个守门人

一个 LSTM 单元通过三个主要的门来协调其记忆：[遗忘门](@entry_id:637423)、输入门和[输出门](@entry_id:634048)。让我们看看控制它们的方程，因为它们是该机制的核心。

#### [遗忘门](@entry_id:637423)

在任何时间步 $t$，第一个决策是要从过去丢弃什么信息。这是**[遗忘门](@entry_id:637423)**（forget gate）$f_t$ 的工作。它查看当前输入 $x_t$ 和前一个隐藏状态 $h_{t-1}$（这是对刚刚过去的总结），并为细胞状态的每个分量输出一个介于 $0$ 和 $1$ 之间的数字。

$f_t = \sigma(W_f x_t + U_f h_{t-1} + b_f)$

想象一下，我们的模型正在使用 [ATAC-seq](@entry_id:169892) 数据扫描基因组以寻找开放区域。当它处于一个开放区域时，它可能会在其细胞状态中建立一个关于这一事实的表示。[遗忘门](@entry_id:637423)可能会学着保持接近 $1$（“记住这个”）。但一旦模型遇到“封闭”[染色质](@entry_id:272631)边界的特征，该门就会学会输出一个接近 $0$ 的值。这实际上是将旧记忆乘以零，“忘记”了它曾处于开放区域，为新信息清除了道路。

#### 输入门

接下来，[细胞决定](@entry_id:266154)要存储什么新信息。这是一个两步过程。首先，使用 $\tanh$ 函数创建一个候选更新值 $\tilde{c}_t$。这是*可能*被添加的新信息。

$\tilde{c}_t = \tanh(W_c x_t + U_c h_{t-1} + b_c)$

其次，**输入门**（input gate）$i_t$ 决定这个候选更新值的哪些部分真正值得保存。

$i_t = \sigma(W_i x_t + U_i h_{t-1} + b_i)$

一个“敲除”实验揭示了此门的重要性。如果我们通过手术将输入门设置为零，网络将失去向其记忆中写入*任何*新信息的能力。它只能记住或忘记其初始状态。它将永远无法学会在序列中识别新的模式或基序，因为存储它们的路径已被切断。

#### 细胞状态更新

现在我们来到了 LSTM 的核心。新的细胞状态 $c_t$ 是通过结合遗忘和输入操作计算得出的：

$c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t$

符号 $\odot$ 表示逐元素乘法。这个简单的方程意义深远。旧的细胞状态 $c_{t-1}$ 被[遗忘门](@entry_id:637423)缩放（有选择地擦除其部分内容），而新的候选信息 $\tilde{c}_t$ 被输入门缩放（有选择地接纳其部分内容）。然后，将这两者简单地**相加**。

让我们通过一个简单的计算来具体说明这一点。假设在某个步骤，前一个细胞状态是 $c_{t-1}=1.0$。网络计算出的[遗忘门](@entry_id:637423)值为 $f_t=0.9$（意为“保留90%的旧记忆”），输入门值为 $i_t=0.3$（意为“让30%的新信息进入”），以及候选更新值为 $\tilde{c}_t=0.5$。新的细胞状态将是：

$c_t = (0.9 \times 1.0) + (0.3 \times 0.5) = 0.9 + 0.15 = 1.05$

记忆被保留并略有更新。

#### [输出门](@entry_id:634048)

最后，细胞必须为当前时间步产生一个输出。在此时刻，不太可能需要细胞状态的全部丰富内容。**[输出门](@entry_id:634048)**（output gate）$o_t$ 决定要揭示细胞状态的哪一部分。

$o_t = \sigma(W_o x_t + U_o h_{t-1} + b_o)$

最终的[隐藏状态](@entry_id:634361) $h_t$ 会传递到下一个时间步并用于任何预测，它是经过过滤的细胞状态：

$h_t = o_t \odot \tanh(c_t)$

细胞状态 $c_t$ 首先通过 $\tanh$ 函数被压缩到 $[-1, 1]$ 区间，然后由[输出门](@entry_id:634048)进行过滤。细胞在 $c_t$ 中维持其内部的[长期记忆](@entry_id:169849)，同时在 $h_t$ 中为当前时刻生成一个清理过的、相关的摘要。

### 机制之美：恒定误差传送带

现在我们可以明白为什么这个架构能如此优雅地解决[梯度消失问题](@entry_id:144098)。其魔力在于细胞状态更新的加法性质：$c_t = f_t \odot c_{t-1} + \dots$。当误差信号沿时间反向传播时，[链式法则](@entry_id:190743)规定梯度会乘以该操作的导数。从 $c_t$ 回到 $c_{t-1}$ 的直接路径涉及与[遗忘门](@entry_id:637423) $f_t$ 的乘法。

这创造了原作者所称的**恒定误差传送带**（Constant Error Carousel）。只要网络学会将[遗忘门](@entry_id:637423)设置为 $1$（或接近 $1$），[误差信号](@entry_id:271594)就可以通过加法连接不受阻碍地反向流动，就像传送带上的乘客一样，免受困扰简单 RNN 的破坏性乘法的影响。梯度高速公路是开放的。

回到“加法问题”，我们可以定量地看到效果。LSTM 的学习信号以 $f^{L-1}$ 的速率衰减。如果网络学会将[遗忘门](@entry_id:637423) $f$ 设置为 $0.99$，那么 100 步后的信号是 $(0.99)^{99} \approx 0.37$。这比简单 RNN 中的信号强数千倍，使得网络能够连接跨越长时间距离的事件。

这种设计也带来了卓越的稳定性。通过将[细胞建模](@entry_id:1122188)为“泄露[积分器](@entry_id:261578)”（leaky integrator），我们可以分析其在最坏情况输入下的行为。该分析表明，只要[遗忘门](@entry_id:637423)值 $f$ 小于 $1$，细胞状态的大小就不会无限增长。它有一个优美、紧凑的[渐近界](@entry_id:267221)限 $\frac{i}{1-f}$，其中 $i$ 是输入门的值。细胞状态是一个稳定的动态系统，其容量由其门的相互作用优雅地控制。

### 设计的多样性

我们所描述的标准 LSTM 是神经工程的杰作，但它并非最终定论。它代表了循环架构广阔“设计空间”中的一个点。通过调整连接，我们可以创造出具有不同属性的变体。

- **带窥视孔的 [LSTM](@entry_id:635790)**（Peephole LSTMs）允许门“窥视”它们正在调节的细胞状态，从而为它们提供更多上下文。这为模型增加了 $3h$ 个参数（其中 $h$ 是[隐藏状态](@entry_id:634361)的大小），虽然这在某些任务中有所帮助，但它也在梯度动态中创建了更复杂的反馈循环。

- **耦合输入-[遗忘门](@entry_id:637423)的 LSTM**（Coupled Input-Forget LSTMs）通过使输入和遗忘决策互补来简化设计：$f_t = 1 - i_t$。要写入新信息，你必须忘记旧信息。这将模型的参数数量从 $4h(d+h+1)$ 减少到 $3h(d+h+1)$（其中 $d$ 是输入大小），这可以提高训练稳定性并减少过拟合，尽管会牺牲一些灵活性。这种设计在精神和参数数量上与另一种流行的架构——[门控循环单元](@entry_id:1125510)（GRU）——非常相似。

这些模型并不简单。其庞大的可训练参数数量表明它们是强大而复杂的机器，需要大量数据进行训练。然而，它们的核心在于一套简单、直观且优美的原则：一条受保护的记忆路径和用于[调节时间](@entry_id:273984)流的智能门控。正是这种力量与优雅的结合，使 [LSTM](@entry_id:635790) 成为现代人工智能的基石。

