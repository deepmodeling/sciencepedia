## 应用与跨学科联系

我们刚刚探讨的原理并不仅仅是理论上的奇闻。就像一个简单而坚固的工具，从木工到制表无所不能用，引导聚集（Bootstrap Aggregating，或称 Bagging）的思想已经渗透到现代科学和工程的几乎每一个角落。其力量在于一种既简单又深刻的哲学：为了获得对世界更稳定、更可靠的看法，我们应该咨询一个“专家委员会”，其中每个成员都看到了一个略有不同的现实版本。

这与我们在其他领域处理复杂问题的方式并无太大差异。设想一位金融分析师试图理解一个投资组合的风险。他们不会只假设经济只有一种单一的未来。相反，他们会运行蒙特卡洛模拟，生成数千种可能的经济情景——有些是高通胀，有些是衰退，有些是繁荣——然后他们将投资组合在所有这些模拟未来中的表现进行平均，以获得对预期风险的[鲁棒估计](@entry_id:261282)。Bagging 所做的正是同样的事情，只不过是针对[机器学习模型](@entry_id:262335)。每个自助样本实际上都是一个合乎情理的“替代现实”，它可能由产生我们原始数据的同一个底层过程所生成。通过在每一种现实上训练模型并平均它们的意见，我们所做的远比仅仅拟合一个模型要深刻得多；我们正在探索可能性的图景，并平均掉我们有限视野中的噪声和特质。

### 群体智慧及其局限

让我们把这一点说得更具体些。想象一下，我们正在构建一个系统，根据一系列实验室结果来预测患者的肾功能。我们可能会使用决策树作为我们的基础“专家”。单一一棵树可能是不稳定的；[训练集](@entry_id:636396)中的几个不同数据点就可能导致它以完全不同的方式生长，从而得出大相径庭的预测。这是一个高方差学习器。

现在，我们应用 Bagging。我们创建，比如说，$B=25$ 个患者数据的自助样本副本。我们在每个副本上训练一棵树。最终的预测是所有 25 棵树预测结果的平均值。这个集成预测的方差可以用一个简单的公式完美地描述：
$$
\operatorname{Var}(\text{ensemble}) = \sigma^2 \left(\rho + \frac{1-\rho}{B}\right)
$$
这里，$\sigma^2$ 是单棵树预测的方差，而 $\rho$ 是我们集成中任意两棵树预测之间的平均相关性。

这个方程讲述了一个精彩的故事。如果我们的专家是完全独立的（$\rho=0$），那么他们平均意见的方差将是 $\frac{\sigma^2}{B}$。若有 25 个独立的专家，我们就能将不确定性降低 25 倍！但他们并非独立的。他们都是在来自同一原始来源的数据上训练的，因此他们的“意见”是相关的。这种相关性 $\rho$ 充当了一个下限。随着我们增加越来越多的专家（当 $B \to \infty$ 时），方差永远无法降到 $\rho\sigma^2$ 以下。Bagging 有帮助，但它受到专家们“羊群效应”的限制。

### 随机森林：一个更独立的委员会

正是这一局限性激发了机器学习中最成功、应用最广泛的算法之一：随机森林（Random Forest）。你可以将[随机森林](@entry_id:146665)看作是“Bagging 外加一个巧妙的技巧”。这个技巧旨在直接攻击相关性项 $\rho$。

在无数领域——从使用多组学生物标志物对肿瘤进行分类，到利用卫星图像绘制野生动物栖息地，再到对全球气候模型进行降尺度以预测局部天气——随机森林已经证明了其威力。它们之所以能做到这一点，不仅是通过创建自助样本，还通过在每棵树的构建过程中强加一种“信息盲视”。在每个决策点，每棵树都不被允许看到所有可用的预测特征。它只能考虑一个小的、随机的子集。

为什么这如此有效？想象一下分析电子健康记录，你可能有几十个高度相关的特征，比如几个都用于衡量肾功能的不同实验室测试。在简单的 Bagging 中，每棵树都很可能在其最重要的分裂点上抓住那个最好的测试指标。结果呢？所有的树都会看起来非常相似，它们的预测将高度相关（$\rho$ 很大），平均带来的好处也就有限了。[随机森林](@entry_id:146665)算法通过迫使一些树在*没有*权限访问那个最佳预测变量的情况下做出决策，鼓励它们在其他特征中寻找替代的、有用的模式。这使得树木更加多样化，它们的[误差相关性](@entry_id:749076)也更低。现在的委员会由具有真正不同视角的专家组成，他们的集体智慧要强大得多。事实上，如果你配置一个随机森林，在每次分裂时考虑所有 $p$ 个预测变量（设置 $m_{\text{try}} = p$），你就只是恢复了原始的 Bagging 算法，而后者通常表现要弱得多，正是因为这个相关性问题。

将 Bagging 的哲学与其友好的竞争对手 Boosting 进行对比也很有用。Bagging 是并行地建立一个由独立专家组成的委员会以降低方差，而 Boosting 则是顺序地建立一个团队，其中每个新成员都被训练来纠正前任的错误。Bagging 是关于从不稳定的模型中创造一个稳定的模型；Boosting 则是关于通过降低偏差从弱模型中创造一个强模型。

### 一种通用的稳定化原理

Bagging 的原理并非只适用于决策树。它是一种通用的策略，可以用来稳定任何“不稳定”的学习算法——即任何输出会因训练数据的微小扰动而发生剧烈变化的算法。

例如，在放射组学（radiomics）中，研究人员可能会使用[支持向量机](@entry_id:172128)（SVMs）来根据从医学图像中提取的数千个特征对肿瘤进行分类。当特征远多于患者时，这些模型可能非常不稳定。对 SVM 进行 Bagging——即在自助样本上训练多个 SVM 并对其输出进行平均或投票——可以产生一个更可靠、更鲁棒的分类器。同样，在临床肿瘤学中，像带惩罚项的 Cox [比例风险模型](@entry_id:171806)这样复杂的生存模型也可以通过 Bagging 进行稳定，从而对患者随时间变化的预后做出更可靠的预测。

Bagging 普适性的最令人惊讶和优美的例证或许来自[深度学习](@entry_id:142022)领域。一种流行的[正则化技术](@entry_id:261393)“dropout”是在训练的每一步随机将神经元的一部分输入设置为零。乍一看，这似乎与 Bagging 毫无关系。然而，已有研究表明，对于包括[线性回归](@entry_id:142318)在内的一大类模型，使用 dropout 进行训练在数学上等同于训练一个由所有可能的子模型（由不同的 dropout [模式形成](@entry_id:139998)）构成的庞大隐式集成，并对它们的预测进行平均。这种对特征的随机掩码是一种形式的 Bagging，并且它最终被证明等同于统计学的另一块基石：$L_2$（或 Ridge）正则化。在这里，我们看到了一个深刻而出人意料的统一：平均扰动模型的简单思想，与惩罚大系数以[防止过拟合](@entry_id:635166)的经典思想，竟是同源的。

### 涟漪效应：从鲁棒性到隐私保护

这种降低方差的原理其影响如涟漪般扩散开来，触及到在现实世界中应用模型的最实际的方面。通过 Bagging 稳定的模型对[训练集](@entry_id:636396)的特定怪癖和噪声不那么敏感。它捕捉到了一个更鲁棒的底层信号。这意味着它更有可能具有良好的*外部有效性*——也就是说，当应用于新数据时（比如来自不同医院或不同人群的数据），其性能将保持稳定。通过平滑掉特异性的拟合，Bagging 产生了一个更具泛化性的模型。当然，我们必须小心。Bagging 是治疗方差的良药，而非偏差。如果我们的基础模型系统性地出错了，那么对许多系统性出错的模型进行平均，仍然会得到一个错误的答案。

最后，在一个令人愉快的转折中，对准确性和鲁棒性的追求给我们带来了一个意想不到的好处：增强的隐私保护。在大数据时代，一个关键问题是训练好的模型可能会“记住”并无意中泄露其训练数据中关于个人的信息。攻击者可能会通过观察模型的[置信度](@entry_id:267904)来判断某个特定人员的数据是否被用于训练——这被称为*[成员推断](@entry_id:636505)攻击*。因为 Bagging 对许多模型的输出进行平均，最终的预测不会过度依赖于任何单个训练点。每个个体的影响在群体中被稀释了。这种平滑效应模糊了训练数据留下的“指纹”，使攻击者更难成功。为了统计稳定性而进行的集成行为，作为一个副作用，带来了令人欣喜的隐私保护。

从其简单的统计学根源出发，Bagging 已经成长为现代数据科学的基石，它为我们带来了像随机森林这样更强大的算法，揭示了与正则化等其他领域的深刻联系，并提供了不仅更准确，而且更鲁棒、甚至更具隐私性的模型。它优美地证明了一个简单、直观的思想可以产生多么深远而重大的影响。