## 引言
随着人工智能系统被集成到从医疗诊断到气候建模等日益关键的领域，一个紧迫的问题浮出水面：我们如何信任它们？仅仅准确是不够的。一个真正智能且可靠的系统不仅必须提供答案，还必须传达其对该答案的信心。这就是[不确定性量化 (UQ)](@entry_id:756296) 的核心承诺，该领域致力于教给 AI 一项至关重要的技能——智识上的谦逊，即理解和阐明其自身的怀疑。本文旨在应对从“黑箱”预测转向创建透明、安全和可信赖 AI 的挑战。它全面概述了如何教 AI 量化其自身的不确定性。第一章“原理与机制”深入探讨了基本概念，区分了不同类型的不确定性，并探索了用于衡量它们的数学和算法工具。第二章“应用与跨学科联系”展示了这些原理如何应用于高风险领域，以构建更安全、更有效的系统，将 AI 从一个“神谕”转变为一个协作伙伴。

## 原理与机制

想象你是一位旧时代的医生。一位病人带着一系列奇特的症状走了进来。你绞尽脑汁，动用多年的经验和你读过的每一本教科书。一个诊断开始形成，但你犹豫了。你的不确定性有两种截然不同的味道。首先，有一种挥之不去的感觉，你的知识是不完整的。也许这是一种你只在书上读过的罕见疾病，或者病人的表现不典型。这是一种个人层面的不确定性，是你自身知识的局限。如果你能咨询更多的专家或看到更多这样的病例，你可能会变得更有信心。第二种不确定性则不同。你可能完全认出了这种疾病，比如普通[流感](@entry_id:190386)。但你仍然无法肯定地说，*这位*病人会在三天还是十天内康复，或者他们是否可能出现严重的并发症。这种疾病本身带有一种反复无常的、随机的元素。即使拥有世界上所有的知识，某些结果仍然像掷骰子一样。

这个简单的区别是现代 AI 中不确定性量化的基石。当我们要求一个 AI 模型进行预测时，我们就是在要求它扮演那位医生的角色。而如果我们想信任它的判断，尤其是在像医学这样的高风险领域，我们不仅必须教它做出预测，还必须教它理解和阐明其自身怀疑的性质。就像那位医生一样，AI 的不确定性可以被分解为两种[基本类](@entry_id:158335)型。

### 怀疑的两个方面：[偶然不确定性与认知不确定性](@entry_id:1120923)

第一种不确定性，源于世界固有的随机性，被称为**[偶然不确定性](@entry_id:634772) (aleatoric uncertainty)**。这个名字来源于拉丁语 *alea*，意为“骰子”。即使我们的模型是完美的，这种不确定性依然存在。想象一下预测一次掷硬币的结果。即使有完美的物理模型，结果本质上也是随机的。在临床环境中，考虑一个 AI 预测[重症监护](@entry_id:898812)室 (ICU) 的死亡风险。两个病人可能具有完全相同的特征——相同的年龄、生命体征、实验室结果，以及模型能看到的一切。然而，由于未测量的因素、微观的生物差异或纯粹的偶然，一个病人可能康复，而另一个则不然。这就是[偶然不确定性](@entry_id:634772)。它是我们正在建模的数据和系统的属性，而不是我们模型本身的属性。它代表了不可约减的噪声，即现实的基本“不稳定性” 。

第二种不确定性，源于模型自身知识的缺乏，被称为**认知不确定性 (epistemic uncertainty)**。这个名字来源于希腊语 *episteme*，意为“知识”。这是一种原则上可以通过更多数据或更好的模型来减少的不确定性。想象一下，我们的 ICU [死亡率](@entry_id:904968)模型主要是在成人数据上训练的。如果我们然后要求它预测一个儿科病人的风险，它的内部参数——它所学到的“知识”——可能不适合这种新类型的数据。模型可能对其预测非常不确定，不是因为儿科病人的结果本质上更随机，而是因为它缺乏处理这些病例的经验。这就是认知不确定性：由于信息有限而导致的模型自我怀疑的度量 。

区分这两者不仅仅是一个学术练习；它对行动至关重要。正如我们将看到的，高的认知不确定性告诉我们，“我不知道，我们应该收集更多信息”，而高的[偶然不确定性](@entry_id:634772)则告诉我们，“这本质上是不可预测的，我们应该谨慎行事” 。

### 谦逊的数学：全方差定律

我们如何让机器区分这两个概念？答案在于概率论中一个优美的部分，称为**[全方差定律](@entry_id:184705) (law of total variance)**。我们不需要陷入推导的细节，但这个概念非常直观。

想象一下，一个预测的总不确定性是其最终答案的总“摆动”。全方差定律告诉我们，这个总摆动可以完美地分成两部分。假设我们模型的知识由一组参数捕获，我们称之为 $\theta$。贝叶斯方法不将这些参数视为单个最佳值，而是视为一个反映我们不确定性的合理值的分布。

我们对结果 $Y$ 的预测的总方差可以写成：
$$
\mathrm{Var}(Y) = \underbrace{\mathbb{E}_{\theta}[\mathrm{Var}(Y \mid \theta)]}_{\text{Aleatoric}} + \underbrace{\mathrm{Var}_{\theta}(\mathbb{E}[Y \mid \theta])}_{\text{Epistemic}}
$$

让我们来解读一下。

第一项，$\mathbb{E}_{\theta}[\mathrm{Var}(Y \mid \theta)]$，是**偶然**部分。内部的 $\mathrm{Var}(Y \mid \theta)$ 是*假设我们知道确切模型参数 $\theta$* 时结果的方差。这是固有的随机性，是不可约减的噪声。外部的期望 $\mathbb{E}_{\theta}[\cdot]$ 只是将这种固有的随机性在我们认为所有可能合理的模型 ($\theta$) 上进行平均。这是模型对世界基本噪声水平的最佳猜测。

第二项，$\mathrm{Var}_{\theta}(\mathbb{E}[Y \mid \theta])$，是**认知**部分。内部的 $\mathbb{E}[Y \mid \theta]$ 是*单个特定模型 $\theta$* 将做出的预测。外部的方差 $\mathrm{Var}_{\theta}(\cdot)$ 衡量这些预测在所有可能的模型之间*相互差异*的程度。如果我们所有可能的模型都给出相同的答案，这一项就是零——没有认知不确定性。如果它们给出截然不同的答案，这一项就很大，表明存在显著的模型不确定性  。

这种分解给了我们一个强大的洞见：认知不确定性是可约减的，而[偶然不确定性](@entry_id:634772)则不是。随着我们收集越来越多的数据，我们关于参数的[后验分布](@entry_id:145605) $p(\theta \mid \mathcal{D})$ 变得更加尖锐。我们对“正确”模型变得更加确定。合理模型之间的分歧缩小，认知方差项趋于零。然而，偶然项，即系统的[固有噪声](@entry_id:261197)，依然存在。对于相同的特征，再多的额外数据也无法让掷硬币的结果变得不那么随机 。

### 量化怀疑的实用方法

用于进行预测的贝叶斯公式 $p(y \mid x, \mathcal{D}) = \int p(y \mid x, \theta) p(\theta \mid \mathcal{D}) d\theta$ 虽然优雅，但对于像深度神经网络这样的复杂模型，该积分是无可救药地难以处理的。我们无法实际计算出在*所有*可能模型上的结果。

所以，我们做了任何务实的科学家都会做的事：我们进行近似。我们不用精确的积分，而是取一个代表性的样本。如果你想知道一个庞大人口的平均意见，你不会去问每一个人；你会进行一次民意调查。类似地，我们可以从后验分布中“调查”几个合理的模型，看看它们怎么说。两种流行的技术应运而生。

#### [深度集成](@entry_id:636362) (Deep Ensembles)

获得一个“专家”模型委员会最直接的方法是独立训练几个模型。这就是**[深度集成](@entry_id:636362) (Deep Ensemble)** 背后的思想。我们使用相同的数据集，但用不同的随机[权重初始化](@entry_id:636952)，比如说，五个不同的神经网络。由于其训练环境的复杂、高维特性，它们都会找到不同但同样好的解决方案。我们可以将这五个训练好的模型视为我们从合理模型的后验分布中抽出的五个样本 。

当我们想为新病人做预测时，我们会征求所有五个模型的意见。它们平均预测值的方差为我们提供了认知不确定性的直接估计。如果它们都同意，我们就很自信。如果它们不同意，模型就在告诉我们它不确定。如果我们还训练了每个模型来预测其自身的噪声水平（例如，通过同时输出一个方差和一个均值），它们预测方差的平均值就为我们提供了[偶然不确定性](@entry_id:634772)的估计 。主要缺点是什么？训练五个模型大约需要训练一个模型的五倍时间。

#### [蒙特卡洛](@entry_id:144354) Dropout ([Monte Carlo Dropout](@entry_id:636300))

一个更聪明、更便宜的替代方案是**[蒙特卡洛](@entry_id:144354) Dropout (MC Dropout)**。我们不训练多个独立模型，而是只训练一个，但在训练过程中使用一种叫做 dropout 的技术。Dropout 在每个训练步骤中随机“关闭”网络中一部分神经元。这可以防止网络过度依赖任何单一路径，并起到一种正则化的作用。

绝妙的洞见在于，我们可以在测试时也保持 dropout 开启。每次我们对同一个输入运行预测时，都会有一组不同的随机神经元被关闭，从而有效地创建了一个略有不同的“[子模](@entry_id:148922)型”。通过运行预测，比如说100次，我们从一个单一的训练网络中得到了100个不同的意见。就像[集成方法](@entry_id:895145)一样，这些意见的方差为我们提供了认知不确定性的估计 。这种方法的优点是只需要一次训练过程，但它使预测时间变慢，因为它需要多次前向传播 。

### 超越单一数值：保证的性质

所以我们有了这些代表偶然和认知不确定性的数值。但它们真正向我们承诺了什么？一个可信赖的 AI 不仅必须陈述其不确定性，还必须坦诚地说明该陈述的含义。

#### 校准问题：90% 的[置信度](@entry_id:267904)是否意味着 90% 的正确率？

一个模型可能会报告疾病概率为 $0.9$。这是一个[置信度](@entry_id:267904)的陈述。但它是一个*经过校准的*置信度吗？如果一个模型在所有预测为 90% 概率的情况下，事件实际发生的频率也为 90%，那么我们就说这个模型是**良好校准的**。一个校准不佳的模型就像一个永远过度自信的天气预报员，他预测有 90% 的晴天几率，结果有一半时间是阴天。

校准对于做出理性决策至关重要。[贝叶斯决策理论](@entry_id:909090)告诉我们，最优决策阈值取决于犯不同错误的成本（例如，[假阴性](@entry_id:894446)与假阳性的成本）。如果一个模型的概率与它们声称的不符，基于这些概率的决策规则将存在系统性缺陷，可能导致现实世界的危害。例如，使用一个过度自信的模型来指导治疗可能导致系统性的过度治疗 。

#### 保形预测：一种不同的承诺

**保形预测 (Conformal prediction)** 提供了一种不同的保证哲学。它不是试图从内到外地建模不确定性（贝叶斯方式），而是从外到内地工作。它不关心模型的内部工作原理。它只是在一个现有模型周围包裹一个程序，以生成一个附带形式化保证的预测*集*（例如，一个区间）。

这个保证是**边际覆盖 (marginal coverage)**。它承诺对于一个期望的置信水平，比如 95%，预测集 $C(x)$ 将包含真实结果 $Y$ 的时间至少占 95%，这是在所有可能的患者上平均的结果：$P(Y \in C(X)) \geq 0.95$ 。这是一个强大的、无需分布假设的保证。

然而，这里有一个关键的微妙之处。这个保证是*边际的*，即“平均而言”。想象一个场景，有两个患者亚组：一个低风险组，其中模型的预测非常精确；一个高风险组，其中预测的噪声要大得多。一个具有单一固定宽度区间的保形预测器可能对低风险组达到 99% 的覆盖率，而对高风险组只有 91% 的覆盖率。平均下来，它可能达到了 95% 的目标，但它未能为高风险个体提供可靠的保证。这是**条件覆盖 (conditional coverage)** 的失败——我们希望对每个个体患者或亚组都持有的保证。为了实现伦理和公平的部署，理解这种区别至关重要 。

### 可信预测的剖析

我们从不确定性的概念定义，到其分解的数学原理，再到其估计的实用工具，以及它可以提供的保证的性质，一路走来。那么，在实践中，一个真正可信赖的 AI 预测是什么样的？

它不是一个单一的数字。它是一个丰富的、信息量大的仪表盘，旨在成为决策中的真正伙伴。借鉴临床安全的需求，这样一份报告可能包括 ：
*   **点预测：** 模型的最佳猜测（例如，风险为 25%）。
*   **认知不确定性：** 一个代表模型信心的[可信区间](@entry_id:176433)（“我有 95% 的把握确定真实风险在 15% 到 35% 之间”）。高的认知不确定性是一个*收集更多信息*的信号——也许是进行另一次测试或咨询人类专家。它反映了信息的价值 。
*   **[偶然不确定性](@entry_id:634772)：** 对[固有噪声](@entry_id:261197)的度量（“对于这类患者，结果变异性很大”）。高的[偶然不确定性](@entry_id:634772)是一个*谨慎行事并为最坏情况做计划*的信号。它要求采取更保守的策略，因为情况在根本上是不可预测的 。
*   **分布外数据标志：** 一个警告，如果患者的数据看起来与模型训练所用的数据完全不同。这是防止在模型已知能力范围之外使用它的关键护栏。
*   **校准报告：** 对模型对此类患者的概率是否倾向于可信的诚实评估。

最后，我们必须认识到，即使是这份详细的报告也只解决了*预测*不确定性——即预测结果的不确定性。对于决策，我们通常需要回答一个更深层次的*因果*问题：我的*干预措施效果*的不确定性是什么？一个模型可以很擅长预测谁会生病，但这不同于预测谁会从治疗中受益。这种**[反事实](@entry_id:923324)不确定性 (counterfactual uncertainty)** 是 AI 安全领域的一个前沿，对于从被动预测转向主动、有益的指导至关重要 。

通过拥抱和剖析其自身的怀疑，AI 可以从一个黑箱神谕转变为一个透明、谦逊且真正智能的助手。

