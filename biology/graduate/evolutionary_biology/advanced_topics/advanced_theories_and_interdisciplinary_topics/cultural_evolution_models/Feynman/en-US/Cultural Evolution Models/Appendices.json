{
    "hands_on_practices": [
        {
            "introduction": "Before we can understand the effects of selection or transmission biases, we need a solid baseline. This role is filled by neutral theory, which describes how the frequencies of cultural variants change due to random chance alone—a process known as cultural drift. This first exercise asks you to derive one of the most fundamental results in evolutionary theory: the probability that a neutral variant, starting at a given frequency, will eventually become the only one of its kind in the population. ",
            "id": "2699390",
            "problem": "Consider a finite population of constant size $N$ evolving under the Wright–Fisher copying model for cultural transmission with no innovation. Time is discrete, indexed by $t \\in \\{0,1,2,\\dots\\}$, and each generation $t+1$ is formed by sampling $N$ cultural parents with replacement from generation $t$, each draw choosing a parent uniformly from the $N$ individuals in generation $t$. There are two cultural variants, denoted $A$ and $B$. Let $X_t \\in \\{0,1,\\dots,N\\}$ be the number of $A$-bearers in generation $t$, and let $x_t = X_t/N$ be the corresponding frequency. Assume neutrality and no innovation: in every draw the parent is chosen independently and uniformly, so that conditional on $X_t=i$, the distribution of $X_{t+1}$ is $\\mathrm{Binomial}(N, i/N)$.\n\nDefine the fixation probability $\\phi(i;N)$ to be the probability that variant $A$ eventually reaches frequency $1$ (i.e., $X_t$ hits the absorbing state $N$) starting from $X_0=i$. Equivalently, define $\\phi(x_0;N)$ with $x_0=i/N$.\n\nUsing only the fundamental definitions above, derive a closed-form expression for the fixation probability $\\phi(x_0;N)$ as a function of the initial frequency $x_0 \\in [0,1]$ and the population size $N$. Express your final answer as a single analytic expression. No rounding is required, and no units are involved.",
            "solution": "The problem asks for the fixation probability of a neutral cultural variant in a finite population of size $N$ evolving under the Wright–Fisher model. Let $X_t$ be the number of individuals carrying variant $A$ at generation $t$, and let $x_t = X_t/N$ be the frequency of this variant. The process starts with an initial count $X_0=i$, corresponding to an initial frequency $x_0 = i/N$. We are tasked with finding the probability that the variant $A$ eventually becomes fixed, meaning its frequency reaches $1$. This probability is denoted by $\\phi(x_0;N)$.\n\nThe state space of the process is $\\{0, 1, \\dots, N\\}$. The states $0$ and $N$ are absorbing states, corresponding to the loss and fixation of variant $A$, respectively. The process evolves according to the transition rule that, conditional on $X_t=k$, the number of $A$-bearers in the next generation, $X_{t+1}$, follows a binomial distribution:\n$$ X_{t+1} | (X_t = k) \\sim \\mathrm{Binomial}(N, k/N) $$\n\nA direct and rigorous method to solve this problem is to analyze the properties of the frequency process $\\{x_t\\}_{t \\geq 0}$. We will demonstrate that under the specified neutral model, the frequency of the variant is a martingale. A process $\\{Y_t\\}$ is a martingale if $\\mathbb{E}[Y_{t+1} | Y_t, Y_{t-1}, \\dots, Y_0] = Y_t$.\n\nLet us compute the conditional expectation of $x_{t+1}$ given the state at time $t$. Suppose at time $t$, the state is $X_t = i$. The frequency is $x_t = i/N$. The expected number of $A$-bearers at time $t+1$ is the expectation of a $\\mathrm{Binomial}(N, p=i/N)$ random variable. The expectation of a binomial distribution $\\mathrm{Binomial}(n,p)$ is $np$. Therefore,\n$$ \\mathbb{E}[X_{t+1} | X_t = i] = N \\cdot \\left(\\frac{i}{N}\\right) = i $$\nNow we can find the expected frequency at time $t+1$:\n$$ \\mathbb{E}[x_{t+1} | X_t = i] = \\mathbb{E}\\left[\\frac{X_{t+1}}{N} \\Big| X_t = i\\right] = \\frac{1}{N} \\mathbb{E}[X_{t+1} | X_t = i] = \\frac{i}{N} $$\nSince we have defined $x_t = i/N$, this shows that $\\mathbb{E}[x_{t+1} | x_t] = x_t$. This equality confirms that the frequency process $\\{x_t\\}$ is a martingale.\n\nThe process stops when it reaches one of the absorbing boundaries, either $x_t=0$ (loss) or $x_t=1$ (fixation). Let $T$ be the stopping time of this process, defined as:\n$$ T = \\inf\\{t \\geq 0 : x_t = 0 \\text{ or } x_t = 1\\} $$\nFor a finite population $N$, eventual absorption into one of these states is guaranteed, meaning the stopping time $T$ is almost surely finite. The martingale $\\{x_t\\}$ is bounded, as its values are always within the interval $[0,1]$.\n\nThese conditions—a bounded martingale and an almost surely finite stopping time—allow us to apply the Optional Stopping Theorem. The theorem states that the expected value of the martingale at the stopping time is equal to its initial value:\n$$ \\mathbb{E}[x_T] = x_0 $$\nThe value of the process at the stopping time, $x_T$, is a random variable that can only take one of two values: $1$ if the variant $A$ fixes, or $0$ if it is lost. The probability of fixation is precisely what we aim to find, $\\phi(x_0;N)$. Thus,\n$$ P(x_T = 1) = \\phi(x_0;N) $$\nAnd the probability of loss is:\n$$ P(x_T = 0) = 1 - \\phi(x_0;N) $$\nThe expectation $\\mathbb{E}[x_T]$ can be calculated from its definition:\n$$ \\mathbb{E}[x_T] = (1) \\cdot P(x_T = 1) + (0) \\cdot P(x_T = 0) = 1 \\cdot \\phi(x_0;N) + 0 \\cdot (1 - \\phi(x_0;N)) = \\phi(x_0;N) $$\nBy equating the two expressions for $\\mathbb{E}[x_T]$, we obtain the final result:\n$$ \\phi(x_0;N) = x_0 $$\nThis result demonstrates a fundamental principle of population genetics: in the absence of selection, mutation, or migration, the probability that a neutral allele or cultural variant will eventually become fixed in the population is equal to its initial frequency. The result is independent of the population size $N$. Given the initial frequency $x_0 = i/N$, the fixation probability is simply $i/N$.",
            "answer": "$$\\boxed{x_{0}}$$"
        },
        {
            "introduction": "Theoretical models often assume perfect information, but real-world learners can be fallible, misperceiving or misremembering the popularity of cultural traits. This practice explores the statistical consequences of such cognitive noise, specifically in the context of estimating the strength of conformist bias. By working through this problem, you will quantify how simple measurement error can lead to a systematic underestimation of transmission effects, a critical insight for any empirical researcher. ",
            "id": "2699253",
            "problem": "Consider a population undergoing social learning with a conformist transmission rule. Let the true frequency of a cultural variant in group $i$ be $p_i \\in (0,1)$, and define the log-odds $x_i = \\operatorname{logit}(p_i) = \\ln\\!\\big(p_i/(1-p_i)\\big)$. Suppose that, conditional on $x_i$, the probability that an observer adopts the variant follows a logistic response with conformist exponent $s>0$, so that at the group level the log-odds of adoption is $y_i = s x_i + \\epsilon_i$, where $\\epsilon_i$ is a zero-mean disturbance with finite variance and independent of $x_i$.\n\nAn empirical researcher attempts to estimate $s$ across $n$ groups by regressing $y_i$ on a perceived log-odds $\\tilde{x}_i$. Due to memory misperception of frequencies, the perceived predictor is subject to classical additive error in the log-odds domain:\n$$\n\\tilde{x}_i \\;=\\; x_i + u_i,\n$$\nwhere $u_i \\sim \\mathcal{N}(0,\\sigma^2)$ is independent of $x_i$ and $\\epsilon_i$, and $x_i$ has finite variance $\\operatorname{Var}(x_i)=\\tau^2>0$. The researcher uses ordinary least squares (OLS) to estimate the slope $\\hat{s}$ in the regression of $y_i$ on $\\tilde{x}_i$.\n\nAssuming $n$ is large so that the OLS slope converges to the population regression slope, derive a closed-form expression for the asymptotic bias in the estimate of the conformist exponent,\n$$\n\\mathbb{B} \\;=\\; \\mathbb{E}[\\hat{s}] - s,\n$$\nas a function of $s$, $\\sigma^2$, and $\\tau^2$. Provide your answer as a single analytical expression. No rounding is required and no units apply.",
            "solution": "The problem statement must first be rigorously validated for scientific and logical consistency before any attempt at a solution.\n\nThe problem provides the following givens:\nA true cultural variant frequency $p_i \\in (0,1)$ in group $i$.\nThe true log-odds, which is the independent variable of interest: $x_i = \\operatorname{logit}(p_i) = \\ln(p_i/(1-p_i))$.\nA structural model for the log-odds of adoption, which is the dependent variable: $y_i = s x_i + \\epsilon_i$.\nThe parameter $s > 0$ is the conformist exponent.\nThe term $\\epsilon_i$ is a disturbance with $\\mathbb{E}[\\epsilon_i] = 0$ and finite variance, independent of $x_i$.\nAn empirical researcher does not observe $x_i$ but instead a perceived log-odds $\\tilde{x}_i$, which is subject to classical measurement error: $\\tilde{x}_i = x_i + u_i$.\nThe measurement error $u_i$ is distributed as a normal distribution with mean zero and variance $\\sigma^2$, i.e., $u_i \\sim \\mathcal{N}(0,\\sigma^2)$.\nThe error term $u_i$ is independent of both $x_i$ and $\\epsilon_i$.\nThe true predictor $x_i$ has a finite variance, $\\operatorname{Var}(x_i) = \\tau^2 > 0$.\nThe researcher performs an ordinary least squares (OLS) regression of $y_i$ on the observed predictor $\\tilde{x}_i$ to obtain an estimate $\\hat{s}$.\nThe number of groups $n$ is assumed to be large, implying that we are interested in the asymptotic properties of the estimator.\nThe objective is to derive the asymptotic bias of the OLS estimator, defined as $\\mathbb{B} = \\mathbb{E}[\\hat{s}] - s$.\n\nThe problem is scientifically grounded. It presents a standard errors-in-variables (EIV) regression model, a fundamental topic in econometrics and statistics. The application to cultural evolution, specifically modeling conformist transmission with perceptual errors, is a valid and recognized area of theoretical biology. The assumptions—independence of error terms, finite variances, zero means—are standard for making such a problem tractable and well-posed. The problem is specified with sufficient mathematical precision, containing no contradictions, ambiguities, or scientifically unsound premises. It is a well-defined mathematical statistics problem. Therefore, the problem is valid, and a solution can be derived.\n\nThe OLS estimator $\\hat{s}$ for the slope in a simple linear regression of $y_i$ on $\\tilde{x}_i$ is given by\n$$\n\\hat{s} = \\frac{\\sum_{i=1}^{n} (\\tilde{x}_i - \\bar{\\tilde{x}})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (\\tilde{x}_i - \\bar{\\tilde{x}})^2}\n$$\nwhere $\\bar{\\tilde{x}}$ and $\\bar{y}$ are the sample means.\nFor a large sample size $n$, the OLS estimator $\\hat{s}$ converges in probability to the population regression slope. This is the probability limit, denoted as $\\operatorname{plim}(\\hat{s})$. The asymptotic expectation $\\mathbb{E}[\\hat{s}]$ corresponds to this limit.\n$$\n\\mathbb{E}[\\hat{s}] = \\operatorname{plim}_{n \\to \\infty} \\hat{s} = \\frac{\\operatorname{Cov}(\\tilde{x}_i, y_i)}{\\operatorname{Var}(\\tilde{x}_i)}\n$$\nOur task is to compute the covariance and variance terms using the model specification.\n\nFirst, we compute the variance of the observed predictor, $\\operatorname{Var}(\\tilde{x}_i)$.\nThe model for the predictor is $\\tilde{x}_i = x_i + u_i$.\nSince $x_i$ and $u_i$ are given to be independent, the variance of their sum is the sum of their variances:\n$$\n\\operatorname{Var}(\\tilde{x}_i) = \\operatorname{Var}(x_i + u_i) = \\operatorname{Var}(x_i) + \\operatorname{Var}(u_i)\n$$\nWe are given that $\\operatorname{Var}(x_i) = \\tau^2$ and the measurement error $u_i$ has variance $\\sigma^2$.\nThus,\n$$\n\\operatorname{Var}(\\tilde{x}_i) = \\tau^2 + \\sigma^2\n$$\n\nNext, we compute the covariance between the observed predictor $\\tilde{x}_i$ and the outcome $y_i$, $\\operatorname{Cov}(\\tilde{x}_i, y_i)$.\nWe substitute the expressions for $\\tilde{x}_i$ and $y_i$:\n$$\n\\operatorname{Cov}(\\tilde{x}_i, y_i) = \\operatorname{Cov}(x_i + u_i, s x_i + \\epsilon_i)\n$$\nUsing the bilinearity property of covariance:\n$$\n\\operatorname{Cov}(\\tilde{x}_i, y_i) = \\operatorname{Cov}(x_i, s x_i) + \\operatorname{Cov}(x_i, \\epsilon_i) + \\operatorname{Cov}(u_i, s x_i) + \\operatorname{Cov}(u_i, \\epsilon_i)\n$$\nWe evaluate each term based on the given independence assumptions:\n$1$. $\\operatorname{Cov}(x_i, s x_i) = s \\operatorname{Cov}(x_i, x_i) = s \\operatorname{Var}(x_i) = s \\tau^2$.\n$2$. $\\operatorname{Cov}(x_i, \\epsilon_i) = 0$, since $x_i$ and $\\epsilon_i$ are independent.\n$3$. $\\operatorname{Cov}(u_i, s x_i) = s \\operatorname{Cov}(u_i, x_i) = 0$, since $u_i$ and $x_i$ are independent.\n$4$. $\\operatorname{Cov}(u_i, \\epsilon_i) = 0$, since $u_i$ and $\\epsilon_i$ are independent.\nSumming these terms, we find the covariance:\n$$\n\\operatorname{Cov}(\\tilde{x}_i, y_i) = s \\tau^2 + 0 + 0 + 0 = s \\tau^2\n$$\n\nNow we can determine the asymptotic expectation of the OLS estimator $\\hat{s}$:\n$$\n\\mathbb{E}[\\hat{s}] = \\frac{\\operatorname{Cov}(\\tilde{x}_i, y_i)}{\\operatorname{Var}(\\tilde{x}_i)} = \\frac{s \\tau^2}{\\tau^2 + \\sigma^2}\n$$\nThis result shows that the OLS estimator is asymptotically biased. The factor $\\frac{\\tau^2}{\\tau^2 + \\sigma^2}$ is known as the reliability ratio. Since $\\sigma^2 \\ge 0$ and $\\tau^2 > 0$, this ratio is always between $0$ and $1$. As the true parameter $s$ is positive, the estimate $\\hat{s}$ is attenuated, meaning it is biased towards zero.\n\nFinally, we compute the asymptotic bias, $\\mathbb{B}$, as defined in the problem statement:\n$$\n\\mathbb{B} = \\mathbb{E}[\\hat{s}] - s\n$$\nSubstituting our expression for $\\mathbb{E}[\\hat{s}]$:\n$$\n\\mathbb{B} = \\frac{s \\tau^2}{\\tau^2 + \\sigma^2} - s\n$$\nTo simplify, we find a common denominator:\n$$\n\\mathbb{B} = s \\left( \\frac{\\tau^2}{\\tau^2 + \\sigma^2} - 1 \\right) = s \\left( \\frac{\\tau^2 - (\\tau^2 + \\sigma^2)}{\\tau^2 + \\sigma^2} \\right)\n$$\n$$\n\\mathbb{B} = s \\left( \\frac{\\tau^2 - \\tau^2 - \\sigma^2}{\\tau^2 + \\sigma^2} \\right) = s \\left( \\frac{-\\sigma^2}{\\tau^2 + \\sigma^2} \\right)\n$$\nThis gives the final expression for the asymptotic bias:\n$$\n\\mathbb{B} = - \\frac{s \\sigma^2}{\\tau^2 + \\sigma^2}\n$$\nThe bias is negative, confirming the attenuation, and its magnitude depends on the true slope $s$, the variance of the measurement error $\\sigma^2$, and the variance of the true signal $\\tau^2$.",
            "answer": "$$\n\\boxed{- \\frac{s \\sigma^{2}}{\\tau^{2} + \\sigma^{2}}}\n$$"
        },
        {
            "introduction": "How can we summarize the evolutionary forces at play in a given dataset? The Price equation offers a powerful and elegant framework for partitioning the total change in a population's average trait into distinct components reflecting selection and transmission. This computational exercise challenges you to implement this decomposition, providing a practical method for measuring the relative importance of differential model influence versus systematic learning biases in driving cultural change. ",
            "id": "2699362",
            "problem": "You are given individual-level cultural data for a single generational step in a population with a set of cultural models (parents) and a set of learners. Each learner chooses one model to learn from and acquires a possibly modified trait value. Your task is to compute two components of population-level cultural change that partition the change in the mean trait value into a component attributable to cultural selection (differential influence of models) and a component attributable to transmission bias (systematic within-line changes during learning).\n\nFundamental base and definitions:\n- Let there be $N$ models indexed by $i \\in \\{1,\\dots,N\\}$, each with trait value $z_i \\in \\mathbb{R}$. Let the mean model trait be $\\bar{z} = \\frac{1}{N} \\sum_{i=1}^N z_i$.\n- Let there be $M$ learners indexed by $j \\in \\{1,\\dots,M\\}$. Each learner $j$ chooses exactly one model $m(j) \\in \\{1,\\dots,N\\}$ and acquires trait $z'_j \\in \\mathbb{R}$ after learning.\n- The cultural influence (number of learners) of model $i$ is $w_i = \\left| \\{ j : m(j) = i \\} \\right|$. The mean influence is $\\bar{w} = \\frac{1}{N} \\sum_{i=1}^N w_i$. Note that $\\sum_{i=1}^N w_i = M$ implies $\\bar{w} = M / N$.\n- Define the per-model learner mean $\\mu_i$ as follows: if $w_i > 0$, then $\\mu_i = \\frac{1}{w_i} \\sum_{j : m(j) = i} z'_j$; if $w_i = 0$, adopt the convention $\\mu_i = z_i$ so that $\\mu_i - z_i = 0$ contributes nothing to the transmission bias.\n\nYour program must compute, for each provided dataset:\n1. The cultural selection covariance term $S$, defined as the covariance between relative influence and model trait,\n$$\nS \\equiv \\operatorname{Cov}\\!\\left(\\frac{w_i}{\\bar{w}}, z_i\\right) \\;=\\; \\frac{1}{N}\\sum_{i=1}^N \\left(\\frac{w_i}{\\bar{w}} - 1\\right)\\left(z_i - \\bar{z}\\right).\n$$\n2. The transmission bias term $T$, defined as the expected within-line change weighted by relative influence,\n$$\nT \\equiv \\frac{1}{N}\\sum_{i=1}^N \\left(\\frac{w_i}{\\bar{w}}\\right)\\left(\\mu_i - z_i\\right).\n$$\n3. The total change in the mean trait across the generational step,\n$$\n\\Delta \\bar{z} \\equiv \\left(\\frac{1}{M}\\sum_{j=1}^M z'_j\\right) - \\bar{z}.\n$$\n4. An interpretation code indicating which component has the larger magnitude. Let $\\varepsilon = 10^{-9}$. Define\n$$\nd \\equiv \\left|S\\right| - \\left|T\\right|.\n$$\nReturn an integer code per dataset as follows: if $d > \\varepsilon$, return $1$ (selection magnitude exceeds transmission); if $d < -\\varepsilon$, return $-1$ (transmission magnitude exceeds selection); otherwise return $0$ (magnitudes are comparable within tolerance).\n\nInput specification for each dataset:\n- $N$ and the model trait vector $\\mathbf{z} = [z_1,\\dots,z_N]$.\n- The learner-to-model index vector $\\mathbf{m} = [m(1),\\dots,m(M)]$ with zero-based indices in $\\{0,\\dots,N-1\\}$.\n- The learner trait vector after learning $\\mathbf{z}' = [z'_1,\\dots,z'_M]$.\nAll quantities are dimensionless real numbers.\n\nTest suite:\nProvide results for the following four datasets. Indices in $\\mathbf{m}$ are zero-based.\n\n- Dataset A:\n  - $N = 4$.\n  - $\\mathbf{z} = [0.0, 1.0, 2.0, 3.0]$.\n  - $\\mathbf{m} = [0, 1, 1, 2, 2, 3, 3, 3, 3, 3]$.\n  - $\\mathbf{z}' = [0.2, 1.0, 1.0, 1.9, 1.9, 2.8, 2.8, 2.8, 2.9, 2.9]$.\n\n- Dataset B:\n  - $N = 3$.\n  - $\\mathbf{z} = [1.0, 2.0, 3.0]$.\n  - $\\mathbf{m} = [0, 0, 1, 1, 2, 2]$.\n  - $\\mathbf{z}' = [1.3, 1.3, 2.0, 2.0, 2.8, 2.8]$.\n\n- Dataset C:\n  - $N = 3$.\n  - $\\mathbf{z} = [0.0, 1.0, 3.0]$.\n  - $\\mathbf{m} = [0, 1, 1, 2, 2, 2]$.\n  - $\\mathbf{z}' = [0.0, 1.0, 1.0, 3.0, 3.0, 3.0]$.\n\n- Dataset D:\n  - $N = 4$.\n  - $\\mathbf{z} = [1.0, 2.0, 3.0, 4.0]$.\n  - $\\mathbf{m} = [1, 2, 3, 3]$.\n  - $\\mathbf{z}' = [1.9, 3.2, 3.7, 3.9]$.\n\nOutput specification:\n- For each dataset, output a list of the form $[S, T, \\Delta\\bar{z}, \\text{code}]$ where $S$, $T$, and $\\Delta\\bar{z}$ are floats rounded to six decimal places, and $\\text{code}$ is an integer as defined above.\n- Your program should produce a single line of output containing the results for all datasets as a comma-separated list of these lists enclosed in square brackets. For example, the output format must be\n$[[S_A,T_A,\\Delta\\bar{z}_A,\\text{code}_A],[S_B,T_B,\\Delta\\bar{z}_B,\\text{code}_B],[S_C,T_C,\\Delta\\bar{z}_C,\\text{code}_C],[S_D,T_D,\\Delta\\bar{z}_D,\\text{code}_D]]$\nwith the required rounding applied.",
            "solution": "The problem as stated has been subjected to rigorous validation. It is deemed to be a valid, well-posed, and scientifically grounded exercise. The definitions and computations requested are a direct application of the Price equation framework, a cornerstone of evolutionary analysis, adapted for the study of cultural evolution. All terms are defined with mathematical precision, the data provided are self-contained, and the objectives are unambiguous. There are no logical contradictions or factual errors. We shall proceed with the derivation of the solution.\n\nThe core task is to partition the total change in the mean population trait value, $\\Delta\\bar{z}$, into two components: a selection component, $S$, and a transmission bias component, $T$. The problem provides definitions for these three quantities. A fundamental check of consistency is to demonstrate that the total change is, in fact, the sum of the two components, i.e., $\\Delta\\bar{z} = S + T$. This identity forms the basis of our analysis.\n\nLet us begin with the definitions provided. The mean trait of the $N$ models is $\\bar{z} = \\frac{1}{N} \\sum_{i=1}^N z_i$. The mean trait of the $M$ learners in the next generation is $\\bar{z}' = \\frac{1}{M}\\sum_{j=1}^M z'_j$. The total change is $\\Delta\\bar{z} = \\bar{z}' - \\bar{z}$.\n\nWe can express $\\bar{z}'$ by grouping learners according to their chosen model $m(j)$. The number of learners who chose model $i$ is its influence, $w_i$. The sum of the trait values of learners who chose model $i$ is $\\sum_{j : m(j) = i} z'_j$. By definition, the per-model learner mean is $\\mu_i = \\frac{1}{w_i} \\sum_{j : m(j) = i} z'_j$ for $w_i > 0$. Therefore, $\\sum_{j : m(j) = i} z'_j = w_i \\mu_i$. This relation holds even for $w_i=0$, as both sides are zero. The total sum of learner traits is $\\sum_{j=1}^M z'_j = \\sum_{i=1}^N \\sum_{j : m(j) = i} z'_j = \\sum_{i=1}^N w_i \\mu_i$.\n\nSubstituting this into the expression for $\\bar{z}'$:\n$$\n\\bar{z}' = \\frac{1}{M} \\sum_{i=1}^N w_i \\mu_i\n$$\nThe mean influence is $\\bar{w} = \\sum_{i=1}^N w_i / N = M/N$. Thus, $M = N\\bar{w}$. Substituting this for $M$:\n$$\n\\bar{z}' = \\frac{1}{N\\bar{w}} \\sum_{i=1}^N w_i \\mu_i = \\frac{1}{N} \\sum_{i=1}^N \\frac{w_i}{\\bar{w}} \\mu_i\n$$\nNow, we can write the total change $\\Delta\\bar{z}$ as:\n$$\n\\Delta\\bar{z} = \\bar{z}' - \\bar{z} = \\frac{1}{N} \\sum_{i=1}^N \\left(\\frac{w_i}{\\bar{w}}\\right) \\mu_i - \\bar{z}\n$$\nTo partition this change, we introduce the term $\\frac{1}{N} \\sum_{i=1}^N (\\frac{w_i}{\\bar{w}}) z_i$ by adding and subtracting it:\n$$\n\\Delta\\bar{z} = \\left( \\frac{1}{N} \\sum_{i=1}^N \\left(\\frac{w_i}{\\bar{w}}\\right) \\mu_i - \\frac{1}{N} \\sum_{i=1}^N \\left(\\frac{w_i}{\\bar{w}}\\right) z_i \\right) + \\left( \\frac{1}{N} \\sum_{i=1}^N \\left(\\frac{w_i}{\\bar{w}}\\right) z_i - \\bar{z} \\right)\n$$\nLet us analyze the two parenthesized terms. The first term can be written as:\n$$\n\\frac{1}{N} \\sum_{i=1}^N \\left(\\frac{w_i}{\\bar{w}}\\right) (\\mu_i - z_i)\n$$\nThis is precisely the definition of the transmission bias term, $T$. It represents the average change in trait value from model to learner, weighted by the relative influence of each model. The convention $\\mu_i = z_i$ when $w_i = 0$ ensures that models with no learners contribute nothing to this term.\n\nThe second term can be recognized as the covariance between relative influence $\\frac{w_i}{\\bar{w}}$ and model trait $z_i$. The expectation of a variable $X_i$ over the population of models is $E[X] = \\frac{1}{N}\\sum_{i=1}^N X_i$. The covariance is $\\operatorname{Cov}(X, Y) = E[(X-E[X])(Y-E[Y])]$.\nLet $X_i = \\frac{w_i}{\\bar{w}}$ and $Y_i = z_i$. Then $E[Y] = \\bar{z}$. The expectation of $X_i$ is $E[X] = \\frac{1}{N}\\sum_{i=1}^N \\frac{w_i}{\\bar{w}} = \\frac{1}{N\\bar{w}}\\sum_{i=1}^N w_i = \\frac{M}{N(M/N)} = 1$.\nThe second term is $E[XY] - \\bar{z} = E[XY] - E[X]E[Y]$, which is $\\operatorname{Cov}(X,Y)$. Expanding this gives:\n$$\n\\operatorname{Cov}\\left(\\frac{w_i}{\\bar{w}}, z_i\\right) = \\frac{1}{N}\\sum_{i=1}^N \\left(\\frac{w_i}{\\bar{w}} - 1\\right) (z_i - \\bar{z})\n$$\nThis is precisely the definition of the cultural selection term, $S$. It measures the statistical association between a model's trait value and its cultural influence.\n\nThus, we have demonstrated the identity $\\Delta\\bar{z} = S + T$. The total change is perfectly partitioned. The computational procedure is as follows:\n\nFor each dataset ($N$, $\\mathbf{z}$, $\\mathbf{m}$, $\\mathbf{z}'$):\n1.  Compute the mean model trait $\\bar{z} = \\frac{1}{N}\\sum z_i$.\n2.  Determine the number of learners $M$ from the length of $\\mathbf{m}$.\n3.  Calculate the influence vector $\\mathbf{w}$, where $w_i$ is the count of model index $i$ in $\\mathbf{m}$. The indices in $\\mathbf{m}$ are given as zero-based, $\\{0, \\dots, N-1\\}$.\n4.  Compute the mean influence $\\bar{w} = M/N$.\n5.  Determine the per-model learner mean vector $\\boldsymbol{\\mu}$. For each model $i$, if $w_i > 0$, $\\mu_i$ is the mean of $z'_j$ for all learners $j$ who chose model $i$. If $w_i = 0$, $\\mu_i$ is set to $z_i$.\n6.  Calculate the selection term $S = \\frac{1}{N}\\sum_{i=0}^{N-1} (\\frac{w_i}{\\bar{w}} - 1)(z_i - \\bar{z})$.\n7.  Calculate the transmission bias term $T = \\frac{1}{N}\\sum_{i=0}^{N-1} (\\frac{w_i}{\\bar{w}})(\\mu_i - z_i)$.\n8.  Calculate the total change $\\Delta\\bar{z} = (\\frac{1}{M}\\sum z'_j) - \\bar{z}$. As a verification, one must confirm that $S + T$ is approximately equal to $\\Delta\\bar{z}$.\n9.  Compute the magnitude difference $d = |S| - |T|$ and determine the interpretation code based on the given tolerance $\\varepsilon = 10^{-9}$. If $d > \\varepsilon$, code is $1$; if $d < -\\varepsilon$, code is $-1$; otherwise, code is $0$.\n10. Format the results as a list $[S, T, \\Delta\\bar{z}, \\text{code}]$, with floating-point numbers rounded to six decimal places.\n\nThis algorithm will be implemented to process the provided test suite.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the cultural evolution problem for all given test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Dataset A\n        {\n            \"N\": 4,\n            \"z_models\": [0.0, 1.0, 2.0, 3.0],\n            \"m_learners\": [0, 1, 1, 2, 2, 3, 3, 3, 3, 3],\n            \"z_prime_learners\": [0.2, 1.0, 1.0, 1.9, 1.9, 2.8, 2.8, 2.8, 2.9, 2.9],\n        },\n        # Dataset B\n        {\n            \"N\": 3,\n            \"z_models\": [1.0, 2.0, 3.0],\n            \"m_learners\": [0, 0, 1, 1, 2, 2],\n            \"z_prime_learners\": [1.3, 1.3, 2.0, 2.0, 2.8, 2.8],\n        },\n        # Dataset C\n        {\n            \"N\": 3,\n            \"z_models\": [0.0, 1.0, 3.0],\n            \"m_learners\": [0, 1, 1, 2, 2, 2],\n            \"z_prime_learners\": [0.0, 1.0, 1.0, 3.0, 3.0, 3.0],\n        },\n        # Dataset D\n        {\n            \"N\": 4,\n            \"z_models\": [1.0, 2.0, 3.0, 4.0],\n            \"m_learners\": [1, 2, 3, 3],\n            \"z_prime_learners\": [1.9, 3.2, 3.7, 3.9],\n        },\n    ]\n\n    results_str_list = []\n    for case in test_cases:\n        s, t, delta_z, code = calculate_components(\n            case[\"N\"],\n            case[\"z_models\"],\n            case[\"m_learners\"],\n            case[\"z_prime_learners\"]\n        )\n        \n        # Format the output for the current case\n        s_str = f\"{s:.6f}\"\n        t_str = f\"{t:.6f}\"\n        delta_z_str = f\"{delta_z:.6f}\"\n        \n        result_str = f\"[{s_str},{t_str},{delta_z_str},{code}]\"\n        results_str_list.append(result_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_str_list)}]\")\n\ndef calculate_components(N_models, z_models_list, m_learners_list, z_prime_learners_list):\n    \"\"\"\n    Computes S, T, delta_z, and the interpretation code for a single dataset.\n    \"\"\"\n    \n    # Convert lists to numpy arrays for vectorized operations\n    z_models = np.array(z_models_list, dtype=float)\n    m_learners = np.array(m_learners_list, dtype=int)\n    z_prime_learners = np.array(z_prime_learners_list, dtype=float)\n    \n    # Number of learners\n    M_learners = len(m_learners)\n\n    # 1. Compute model-level quantities\n    z_bar = np.mean(z_models)\n\n    # 2. Compute learner-level and linking quantities\n    # Influence of each model (number of learners choosing each model)\n    w = np.bincount(m_learners, minlength=N_models)\n    \n    # Mean influence\n    if N_models > 0:\n        w_bar = M_learners / N_models\n    else:\n        w_bar = 0\n\n    # 3. Calculate per-model learner mean mu\n    mu = np.zeros(N_models, dtype=float)\n    for i in range(N_models):\n        if w[i] > 0:\n            learners_of_model_i = z_prime_learners[m_learners == i]\n            mu[i] = np.mean(learners_of_model_i)\n        else:\n            # Convention: if w_i = 0, mu_i = z_i\n            mu[i] = z_models[i]\n\n    # Handle case where all models have zero influence\n    if w_bar == 0:\n        rel_w = np.zeros(N_models, dtype=float)\n    else:\n        rel_w = w / w_bar\n\n    # 4. Calculate S (Selection)\n    # S = (1/N) * sum((w_i/w_bar - 1) * (z_i - z_bar))\n    s_term_per_model = (rel_w - 1) * (z_models - z_bar)\n    S = np.mean(s_term_per_model)\n\n    # 5. Calculate T (Transmission)\n    # T = (1/N) * sum((w_i/w_bar) * (mu_i - z_i))\n    t_term_per_model = rel_w * (mu - z_models)\n    T = np.mean(t_term_per_model)\n\n    # 6. Calculate Delta z_bar (Total Change)\n    if M_learners > 0:\n        z_prime_bar = np.mean(z_prime_learners)\n    else:\n        z_prime_bar = z_bar # No learners, no change\n    delta_z_bar = z_prime_bar - z_bar\n\n    # 7. Calculate the interpretation code\n    epsilon = 1e-9\n    d = abs(S) - abs(T)\n    if d > epsilon:\n        code = 1\n    elif d  -epsilon:\n        code = -1\n    else:\n        code = 0\n        \n    return S, T, delta_z_bar, code\n\n# Execute the main function\nsolve()\n```"
        }
    ]
}