{
    "hands_on_practices": [
        {
            "introduction": "适应性辐射早期的谱系爆发式增长常被简化为纯生过程（Yule process），这是理解其动态的一个基本出发点。准确估计多样化速率 $r$ 是宏观演化分析的核心任务之一。本练习  将引导您通过第一性原理，推导净多样化速率的最大似然估计（Maximum Likelihood Estimator, MLE），并进一步分析该估计量的内在偏差。通过这个过程，您将深刻理解从现存物种的系统发育树中推断宏观演化参数时所面临的统计挑战与陷阱。",
            "id": "2689699",
            "problem": "创新后的快速谱系扩张通常被建模为纯出生（Yule）过程，这在灭绝可以忽略不计的早期适应性辐射中是合适的。考虑一个被假设在一次关键进化创新后经历了适应性辐射的支系。该支系的冠部年龄为 $T$（距离最近共同祖先的时间），有 $N$ 个现存物种，且该过程从冠部的 $2$ 个谱系开始。假设一个纯出生过程，其物种形成速率为 $\\lambda$，灭绝速率为 $\\mu=0$，因此净多样化速率为 $r=\\lambda$。\n\n从Yule过程的定义性质和分支（独立性）属性出发，按以下步骤进行：\n- 推导出从2个初始谱系开始，在时间 $T$ 时现存谱系数量的概率质量函数，并用 $p=\\exp(-rT)$ 表示。\n- 利用该似然，仅根据 $N$ 和 $T$ 推导出 $r$ 的最大似然估计量（MLE）（在首次使用时定义最大似然估计量(MLE)）。\n- 使用关于 $N$ 均值的二阶泰勒近似（delta方法），获得该估计量偏差的一个闭式一阶近似解，即 $\\mathbb{E}[\\hat{r}]-r$，表示为 $r$ 和 $T$ 的函数。\n\n报告中只给出最终的偏差近似值，以包含 $r$、$T$ 和指数函数 $\\exp(\\cdot)$ 的单个闭式表达式作答。不包含单位。不提供任何数值近似。",
            "solution": "该问题要求在一个纯出生（Yule）过程中，推导净多样化速率 $r$ 的最大似然估计量的偏差。对问题陈述的验证发现其具有科学依据、问题明确且自洽。这是理论进化生物学和数理统计学中的一个标准问题。我们开始进行解答。\n\n首先，我们必须推导在时间 $T$ 时现存谱系数量 $N$ 的概率质量函数 (PMF)。该过程从冠部年龄处的2个谱系开始，我们设定此时时间为 $t=0$。该模型是一个纯出生过程，具有恒定的物种形成速率 $r > 0$。\n\n对于从 $t=0$ 开始的单个谱系，在时间 $t$ 时的后代谱系数量 $N_1$ 服从几何分布。存在 $k \\ge 1$ 个后代的概率由下式给出\n$$ P(N_1(t) = k) = \\exp(-rt) (1 - \\exp(-rt))^{k-1} $$\n这可以用参数 $p = \\exp(-rT)$ 来表示，它代表了一个谱系在区间 $[0, T]$ 内不发生分化的概率。那么，对于 $k \\in \\{1, 2, 3, \\dots\\}$，PMF为 $P(N_1(T) = k) = p(1-p)^{k-1}$。\n\n在时间 $T$ 时的总谱系数量，记为 $N$，是源自两个初始谱系（我们可以称之为 $N_A$ 和 $N_B$）的后代谱系数量之和。根据Yule过程的分支特性，这两个子支系的演化是独立的。因此，$N = N_A + N_B$，其中 $N_A$ 和 $N_B$ 是独立同分布的随机变量，均服从上面推导的几何分布。\n两个独立同分布的几何随机变量之和的分布是负二项分布。对于 $n \\ge 2$，$N$ 的PMF通过卷积求得：\n$$ P(N=n) = \\sum_{k=1}^{n-1} P(N_A=k) P(N_B=n-k) $$\n$$ P(N=n) = \\sum_{k=1}^{n-1} \\left[p(1-p)^{k-1}\\right] \\left[p(1-p)^{n-k-1}\\right] $$\n$$ P(N=n) = \\sum_{k=1}^{n-1} p^2 (1-p)^{n-2} $$\n求和号内的项相对于索引 $k$ 是一个常数。该求和共有 $n-1$ 项。因此，PMF为：\n$$ P(N=n | r, T) = (n-1) p^2 (1-p)^{n-2} = (n-1) (\\exp(-rT))^2 (1 - \\exp(-rT))^{n-2} $$\n这是在给定 $r$ 和 $T$ 的条件下，观测到 $n$ 个物种的似然。\n\n第二，我们推导 $r$ 的最大似然估计量（MLE）。最大似然估计量（MLE）是一种估计统计模型参数的方法，通过找到使似然函数（表示为这些参数的函数时，观测到给定数据的概率）最大化的参数值来实现。\n给定观测到 $N$ 个现存物种，关于 $r$ 的似然函数为：\n$$ L(r | N, T) = (N-1) (\\exp(-rT))^2 (1 - \\exp(-rT))^{N-2} $$\n为了简化最大化过程，我们使用对数似然函数 $\\ell(r) = \\ln L(r | N, T)$：\n$$ \\ell(r) = \\ln(N-1) + 2\\ln(\\exp(-rT)) + (N-2)\\ln(1 - \\exp(-rT)) $$\n$$ \\ell(r) = \\ln(N-1) - 2rT + (N-2)\\ln(1 - \\exp(-rT)) $$\n我们通过对 $\\ell(r)$ 关于 $r$ 求导并令其为零，来找到MLE，记为 $\\hat{r}$：\n$$ \\frac{d\\ell}{dr} = -2T + (N-2) \\frac{1}{1 - \\exp(-rT)} \\cdot (-\\exp(-rT)) \\cdot (-T) $$\n$$ \\frac{d\\ell}{dr} = -2T + (N-2) \\frac{T \\exp(-rT)}{1 - \\exp(-rT)} $$\n在 $r = \\hat{r}$ 处令 $\\frac{d\\ell}{dr} = 0$（且 $T \\neq 0$）：\n$$ -2 + (N-2) \\frac{\\exp(-\\hat{r}T)}{1 - \\exp(-\\hat{r}T)} = 0 $$\n$$ 2(1 - \\exp(-\\hat{r}T)) = (N-2)\\exp(-\\hat{r}T) $$\n$$ 2 - 2\\exp(-\\hat{r}T) = N\\exp(-\\hat{r}T) - 2\\exp(-\\hat{r}T) $$\n$$ 2 = N\\exp(-\\hat{r}T) $$\n解出 $\\hat{r}$：\n$$ \\exp(-\\hat{r}T) = \\frac{2}{N} \\implies -\\hat{r}T = \\ln\\left(\\frac{2}{N}\\right) \\implies \\hat{r}T = \\ln\\left(\\frac{N}{2}\\right) $$\n$$ \\hat{r}(N) = \\frac{1}{T} \\ln\\left(\\frac{N}{2}\\right) $$\n\n第三，我们求该估计量偏差的一阶近似。偏差定义为 $\\text{Bias}(\\hat{r}) = \\mathbb{E}[\\hat{r}] - r$。估计量 $\\hat{r}$ 是随机变量 $N$ 的函数，我们称之为 $g(N)$。我们对 $g(N)$ 在 $N$ 的均值 $\\mu_N = \\mathbb{E}[N]$ 附近进行二阶泰勒级数展开：\n$$ g(N) \\approx g(\\mu_N) + g'(\\mu_N)(N - \\mu_N) + \\frac{1}{2}g''(\\mu_N)(N - \\mu_N)^2 $$\n求期望可得到 $\\mathbb{E}[\\hat{r}]$ 的一个近似值：\n$$ \\mathbb{E}[\\hat{r}] = \\mathbb{E}[g(N)] \\approx g(\\mu_N) + g'(\\mu_N)\\mathbb{E}[N - \\mu_N] + \\frac{1}{2}g''(\\mu_N)\\mathbb{E}[(N - \\mu_N)^2] $$\n由于 $\\mathbb{E}[N - \\mu_N] = 0$ 且 $\\mathbb{E}[(N - \\mu_N)^2] = \\text{Var}(N)$，上式简化为：\n$$ \\mathbb{E}[\\hat{r}] \\approx g(\\mu_N) + \\frac{1}{2}g''(\\mu_N)\\text{Var}(N) $$\n我们需要计算 $\\mu_N$、$\\text{Var}(N)$ 以及 $g(N)$ 的导数。\n几何分布 $P(k)=p(1-p)^{k-1}$ 的均值为 $\\frac{1}{p}$，方差为 $\\frac{1-p}{p^2}$。由于 $N$ 是两个这样的独立同分布随机变量之和：\n$$ \\mu_N = \\mathbb{E}[N] = \\frac{1}{p} + \\frac{1}{p} = \\frac{2}{p} = \\frac{2}{\\exp(-rT)} = 2\\exp(rT) $$\n$$ \\text{Var}(N) = \\frac{1-p}{p^2} + \\frac{1-p}{p^2} = \\frac{2(1-p)}{p^2} = \\frac{2(1 - \\exp(-rT))}{(\\exp(-rT))^2} = 2\\exp(2rT)(1 - \\exp(-rT)) $$\n估计量函数及其导数为：\n$$ g(N) = \\frac{1}{T} \\ln\\left(\\frac{N}{2}\\right) $$\n$$ g'(N) = \\frac{1}{T} \\cdot \\frac{1}{N/2} \\cdot \\frac{1}{2} = \\frac{1}{TN} $$\n$$ g''(N) = -\\frac{1}{TN^2} $$\n我们在 $\\mu_N$ 处对这些表达式求值：\n$$ g(\\mu_N) = \\frac{1}{T}\\ln\\left(\\frac{2\\exp(rT)}{2}\\right) = \\frac{1}{T}\\ln(\\exp(rT)) = \\frac{rT}{T} = r $$\n$$ g''(\\mu_N) = -\\frac{1}{T\\mu_N^2} = -\\frac{1}{T(2\\exp(rT))^2} = -\\frac{1}{4T\\exp(2rT)} $$\n偏差的近似为 $\\text{Bias}(\\hat{r}) \\approx \\mathbb{E}[\\hat{r}] - r \\approx (g(\\mu_N) + \\frac{1}{2}g''(\\mu_N)\\text{Var}(N)) - r$。代入 $g(\\mu_N) = r$：\n$$ \\text{Bias}(\\hat{r}) \\approx \\frac{1}{2}g''(\\mu_N)\\text{Var}(N) $$\n$$ \\text{Bias}(\\hat{r}) \\approx \\frac{1}{2} \\left(-\\frac{1}{4T\\exp(2rT)}\\right) \\left(2\\exp(2rT)(1 - \\exp(-rT))\\right) $$\n$$ \\text{Bias}(\\hat{r}) \\approx -\\frac{2\\exp(2rT)(1 - \\exp(-rT))}{8T\\exp(2rT)} $$\n通过消去项来简化表达式：\n$$ \\text{Bias}(\\hat{r}) \\approx -\\frac{1}{4T}(1 - \\exp(-rT)) $$\n这是MLE $\\hat{r}$ 偏差的一阶近似。负号表示该估计量倾向于低估真实速率 $r$，这是基于计数的对数的这类估计量的一个共同特征，尤其是在小样本量（即 $N$ 较小）的情况下。",
            "answer": "$$ \\boxed{-\\frac{1 - \\exp(-rT)}{4T}} $$"
        },
        {
            "introduction": "适应性辐射不仅体现在物种数量的快速增加，更关键的是伴随着生态或形态性状的快速分化。本练习  旨在量化并对比两种对立的演化情景在性状数据上留下的印记。您将分别推导在适应性辐射的“早期爆发”（early-burst）模型和代表非适应性辐射的“稳定选择”（Ornstein–Uhlenbeck）模型下，谱系内性状差异的期望值。这种直接的对比有助于建立对不同宏观演化过程如何塑造生物多样性模式的定量直觉。",
            "id": "2689720",
            "problem": "一个支系在时间 $t=0$ 时从单一祖先开始经历快速辐射演化，分化为 $n$ 个谱系，这些谱系在持续时间 $T$ 内，在一个星状系统发育树上独立演化。对于在每个谱系中测量的数量性状 $X_{t}$，考虑两种备选过程模型：\n\n- 由关键演化创新驱动的适应性辐射，导致早期爆发 (EB) 动态：瞬时演化速率随时间呈指数级下降。形式上，假设 $X_{t}$ 遵循一个布朗运动 (BM)，其扩散系数 $\\sigma^{2}(t)$ 随时间变化，从 $X_{0}=0$ 开始，其中 $\\sigma^{2}(t)=\\sigma_{0}^{2}\\exp(-a t)$，且 $\\sigma_{0}^{2}>0$ 和 $a>0$。\n- 非适应性辐射，其中产生许多物种，但性状围绕单一的支系共同最适值受到稳定选择：假设 $X_{t}$ 遵循一个奥恩斯坦-乌伦贝克 (OU) 过程，其中 $X_{0}=0$，最适值 $\\theta=0$，拉力强度 $\\alpha>0$，扩散系数 $\\sigma>0$。\n\n从布朗运动 (BM) 和奥恩斯坦-乌伦贝克 (OU) 过程的随机微分方程 (SDE) 定义出发，并且仅使用 Itô 积分的基本性质，推导在每种模型下，时刻 $T$ 的每个谱系的期望性状差异度，定义为 $\\mathbb{E}[X_{T}^{2}]$。使用星状系统发育树，以便谱系在 $t=0$ 后独立演化，并将差异度视为一个代表性谱系在支系内的期望值。然后，使用参数值 $T=4$，$\\sigma_{0}^{2}=0.5$, $a=0.7$, $\\sigma=0.6$, 和 $\\alpha=1.1$ 计算比率\n$$\nR \\equiv \\frac{\\mathbb{E}_{\\mathrm{EB}}[X_{T}^{2}]}{\\mathbb{E}_{\\mathrm{OU}}[X_{T}^{2}]}\n$$\n。将您最终的 $R$ 数值结果四舍五入到四位有效数字。答案以无单位的纯数形式表示。",
            "solution": "问题陈述已经过验证，并被证实是有效的。它在科学上基于已确立的演化理论，在数学上是适定的，并以客观且明确的语言呈现。获得唯一解所需的所有必要条件和参数均已提供。因此，我们可以着手进行推导。\n\n任务是计算在两种不同的性状演化模型下，于时刻 $T$ 的每个谱系的期望性状差异度（定义为 $\\mathbb{E}[X_T^2]$），然后对一组给定的参数求这些差异度的比率。\n\n首先，我们考虑早期爆发 (EB) 模型。性状演化由一个时间非齐次布朗运动描述，其随机微分方程 (SDE) 如下：\n$$\ndX_t = \\sigma(t) dW_t\n$$\n其中 $W_t$ 是一个标准维纳过程，随时间变化的扩散系数为 $\\sigma^2(t) = \\sigma_0^2 \\exp(-at)$。初始条件为 $X_0 = 0$。该 SDE 可以写成：\n$$\ndX_t = \\sigma_0 \\exp\\left(-\\frac{a t}{2}\\right) dW_t\n$$\n将此 SDE 从 $t=0$ 积分到 $t=T$ 得到时刻 $T$ 的性状值：\n$$\nX_T - X_0 = \\int_{0}^{T} \\sigma_0 \\exp\\left(-\\frac{a s}{2}\\right) dW_s\n$$\n给定 $X_0=0$，我们有：\n$$\nX_T = \\sigma_0 \\int_{0}^{T} \\exp\\left(-\\frac{a s}{2}\\right) dW_s\n$$\nEB 模型下的期望差异度 $\\mathbb{E}_{\\mathrm{EB}}[X_T^2]$ 是这个 Itô 积分平方的期望值。\n$$\n\\mathbb{E}_{\\mathrm{EB}}[X_T^2] = \\mathbb{E}\\left[ \\left( \\sigma_0 \\int_{0}^{T} \\exp\\left(-\\frac{a s}{2}\\right) dW_s \\right)^2 \\right] = \\sigma_0^2 \\mathbb{E}\\left[ \\left( \\int_{0}^{T} \\exp\\left(-\\frac{a s}{2}\\right) dW_s \\right)^2 \\right]\n$$\n我们应用 Itô 等距性质，该性质指出对于确定性函数 $f(t)$，有 $\\mathbb{E}[(\\int_0^T f(s) dW_s)^2] = \\int_0^T f(s)^2 ds$。在这里，$f(s) = \\exp(-\\frac{a s}{2})$ 是确定性的。因此：\n$$\n\\mathbb{E}_{\\mathrm{EB}}[X_T^2] = \\sigma_0^2 \\int_{0}^{T} \\left( \\exp\\left(-\\frac{a s}{2}\\right) \\right)^2 ds = \\sigma_0^2 \\int_{0}^{T} \\exp(-a s) ds\n$$\n计算该定积分得出：\n$$\n\\mathbb{E}_{\\mathrm{EB}}[X_T^2] = \\sigma_0^2 \\left[ -\\frac{1}{a} \\exp(-as) \\right]_{0}^{T} = \\sigma_0^2 \\left( -\\frac{1}{a} \\exp(-aT) - \\left(-\\frac{1}{a} \\exp(0)\\right) \\right)\n$$\n$$\n\\mathbb{E}_{\\mathrm{EB}}[X_T^2] = \\frac{\\sigma_0^2}{a} (1 - \\exp(-aT))\n$$\n这是 EB 模型的期望差异度。请注意，由于 $\\mathbb{E}[X_T] = 0$，这也是时刻 $T$ 时性状的方差。\n\n其次，我们考虑奥恩斯坦-乌伦贝克 (OU) 模型。其 SDE 如下：\n$$\ndX_t = \\alpha(\\theta - X_t) dt + \\sigma dW_t\n$$\n根据给定参数 $\\theta=0$, $X_0=0$, $\\alpha>0$, 和 $\\sigma>0$，SDE 简化为：\n$$\ndX_t = -\\alpha X_t dt + \\sigma dW_t\n$$\n这是一个线性 SDE。我们使用积分因子 $I_t = \\exp(\\alpha t)$ 来求解它。令 $Y_t = I_t X_t = \\exp(\\alpha t) X_t$。根据函数 $f(t,x) = \\exp(\\alpha t) x$ 的 Itô 引理，其微分为：\n$$\ndY_t = \\alpha \\exp(\\alpha t) X_t dt + \\exp(\\alpha t) dX_t\n$$\n代入 $dX_t$ 的 SDE：\n$$\ndY_t = \\alpha \\exp(\\alpha t) X_t dt + \\exp(\\alpha t) (-\\alpha X_t dt + \\sigma dW_t) = \\sigma \\exp(\\alpha t) dW_t\n$$\n从 $t=0$ 积分到 $t=T$：\n$$\nY_T - Y_0 = \\int_0^T \\sigma \\exp(\\alpha s) dW_s\n$$\n因为 $Y_T = \\exp(\\alpha T) X_T$ 且 $Y_0 = \\exp(0)X_0 = 0$：\n$$\n\\exp(\\alpha T) X_T = \\sigma \\int_0^T \\exp(\\alpha s) dW_s \\implies X_T = \\sigma \\exp(-\\alpha T) \\int_0^T \\exp(\\alpha s) dW_s\n$$\n现在，我们计算期望差异度 $\\mathbb{E}_{\\mathrm{OU}}[X_T^2]$：\n$$\n\\mathbb{E}_{\\mathrm{OU}}[X_T^2] = \\mathbb{E}\\left[ \\left( \\sigma \\exp(-\\alpha T) \\int_0^T \\exp(\\alpha s) dW_s \\right)^2 \\right] = \\sigma^2 \\exp(-2\\alpha T) \\mathbb{E}\\left[ \\left( \\int_0^T \\exp(\\alpha s) dW_s \\right)^2 \\right]\n$$\n再次使用 Itô 等距性质，其中确定性函数为 $f(s) = \\exp(\\alpha s)$：\n$$\n\\mathbb{E}_{\\mathrm{OU}}[X_T^2] = \\sigma^2 \\exp(-2\\alpha T) \\int_0^T (\\exp(\\alpha s))^2 ds = \\sigma^2 \\exp(-2\\alpha T) \\int_0^T \\exp(2\\alpha s) ds\n$$\n计算此积分：\n$$\n\\mathbb{E}_{\\mathrm{OU}}[X_T^2] = \\sigma^2 \\exp(-2\\alpha T) \\left[ \\frac{1}{2\\alpha} \\exp(2\\alpha s) \\right]_0^T = \\sigma^2 \\exp(-2\\alpha T) \\left( \\frac{\\exp(2\\alpha T) - 1}{2\\alpha} \\right)\n$$\n$$\n\\mathbb{E}_{\\mathrm{OU}}[X_T^2] = \\frac{\\sigma^2}{2\\alpha} (1 - \\exp(-2\\alpha T))\n$$\n这是 OU 模型的期望差异度，由于 $\\mathbb{E}[X_T] = 0$，它也是时刻 $T$ 时性状的方差。\n\n最后，我们计算比率 $R$：\n$$\nR \\equiv \\frac{\\mathbb{E}_{\\mathrm{EB}}[X_{T}^{2}]}{\\mathbb{E}_{\\mathrm{OU}}[X_{T}^{2}]} = \\frac{\\frac{\\sigma_{0}^{2}}{a} (1 - \\exp(-aT))}{\\frac{\\sigma^{2}}{2\\alpha} (1 - \\exp(-2\\alpha T))}\n$$\n我们代入给定的数值：$T=4$, $\\sigma_{0}^{2}=0.5$, $a=0.7$, $\\sigma=0.6$, 和 $\\alpha=1.1$。注意，$\\sigma^2 = 0.6^2=0.36$。\n指数为 $aT = 0.7 \\times 4 = 2.8$ 和 $2\\alpha T = 2 \\times 1.1 \\times 4 = 8.8$。\n将这些值代入 $R$ 的表达式中：\n$$\nR = \\frac{\\frac{0.5}{0.7} (1 - \\exp(-2.8))}{\\frac{0.36}{2 \\times 1.1} (1 - \\exp(-8.8))} = \\frac{\\frac{0.5}{0.7}}{\\frac{0.36}{2.2}} \\frac{1 - \\exp(-2.8)}{1 - \\exp(-8.8)}\n$$\n$$\nR = \\left(\\frac{0.5 \\times 2.2}{0.7 \\times 0.36}\\right) \\frac{1 - \\exp(-2.8)}{1 - \\exp(-8.8)} = \\left(\\frac{1.1}{0.252}\\right) \\frac{1 - \\exp(-2.8)}{1 - \\exp(-8.8)}\n$$\n$$\nR = \\frac{275}{63} \\frac{1 - \\exp(-2.8)}{1 - \\exp(-8.8)}\n$$\n数值计算结果为：\n$$\nR \\approx \\frac{275}{63} \\frac{1 - 0.060810}{1 - 0.000150} \\approx 4.365079 \\times \\frac{0.93919}{0.99985} \\approx 4.365079 \\times 0.939331 \\approx 4.10027\n$$\n将结果四舍五入到四位有效数字，我们得到 $R = 4.100$。",
            "answer": "$$\n\\boxed{4.100}\n$$"
        },
        {
            "introduction": "现代宏观演化研究致力于将性状演化与谱系多样化过程整合到统一的统计框架中，以检验关于关键演化创新效应的复杂假说。这个高级实践练习  将指导您构建一个分层贝叶斯模型，该模型能够联合估计性状演化速率、多样化速率以及关键创新的具体效应。完成这个练习意味着您将接触到系统发育比较方法研究的前沿，并获得处理真实世界复杂演化问题的实践经验。",
            "id": "2689781",
            "problem": "您的任务是实现一个分层贝叶斯模型，该模型捕捉了与适应性辐射和关键演化创新相关的三个耦合过程：沿谱系的连续性状演化、作为计数过程的谱系多样化以及关键创新的乘法效应。您编写的程序必须在此模型下为多个数据集计算联合后验摘要，并以精确指定的格式将结果打印在单行中。\n\n模型假设和定义遵循演化生物学和随机过程中使用的基本定律和核心定义：\n- 沿谱系的性状演化被建模为方差速率参数为 $\\sigma^2$ 的布朗运动，因此长度为 $t_i$ 的独立谱系上的性状值变化分布为 $x_i \\sim \\mathcal{N}(0, \\sigma^2 t_i)$，其中 $\\mathcal{N}$ 表示正态分布。\n- 在持续时间为 $d_i$ 的观测窗口中，谱系 $i$ 的多样化事件（例如，物种形成计数）遵循泊松过程，其速率为 $\\lambda_i = r \\exp(\\beta x_i + \\delta z_i)$，其中 $r$是基线多样化速率，$\\beta$ 是性状依赖效应，$\\delta$ 是二元关键创新指标 $z_i \\in \\{0,1\\}$ 的效应。因此 $y_i \\mid x_i, z_i, r, \\beta, \\delta \\sim \\mathrm{Poisson}\\!\\left(d_i \\, r \\, \\exp(\\beta x_i + \\delta z_i)\\right)$，其中 $\\mathrm{Poisson}$ 表示泊松分布。\n- 独立性假设：在给定参数的条件下，谱系在性状和计数过程中都是独立的；在给定 $x_i$ 和参数的条件下，性状和计数过程是条件独立的。\n\n编码了生物学真实性的先验指定如下：\n- 性状方差速率先验：$\\sigma^2 \\sim \\mathrm{Inverse\\!-\\!Gamma}(a_\\sigma, b_\\sigma)$，其中 $\\mathrm{Inverse\\!-\\!Gamma}$ 表示形状为 $a_\\sigma$ 且尺度为 $b_\\sigma$ 的逆伽马分布。\n- 对数尺度上的基线速率先验：$\\rho = \\log r \\sim \\mathcal{N}(\\mu_r, s_r^2)$，这意味着对$r$采用对数正态先验，以确保其为正并允许右偏不确定性。\n- 性状效应先验：$\\beta \\sim \\mathcal{N}(0, s_\\beta^2)$，以反映对效应较小的预期，并在缺乏强有力证据时向零收缩。\n- 创新效应先验：$\\delta \\sim \\mathcal{N}(m_\\delta, s_\\delta^2)$，具有一个小的正均值 $m_\\delta$，以编码关键创新更常增加多样化的弱先验信念，同时允许数据驱动的估计。\n\n您的程序必须为每个数据集执行以下操作：\n1. 给定 $\\{(x_i, t_i)\\}_{i=1}^n$ 和先验 $\\sigma^2 \\sim \\mathrm{Inverse\\!-\\!Gamma}(a_\\sigma, b_\\sigma)$，计算 $\\sigma^2$ 的后验均值。此处的基础是布朗运动似然 $x_i \\sim \\mathcal{N}(0,\\sigma^2 t_i)$ 和贝叶斯法则 (Bayes' rule)；除这些原则外，不要假设任何捷径公式。\n2. 给定 $\\{(y_i, d_i, x_i, z_i)\\}_{i=1}^n$ 和 $(\\rho,\\beta,\\delta)$ 的先验，通过最大化由泊松过程、对数连接函数和正态先验所隐含的联合对数后验，计算 $(\\rho,\\beta,\\delta)$ 的后验众数（最大后验点）。使用数值稳定的优化器，并从第一性原理计算精确的解析梯度。此处的基础是计数的泊松过程似然、广义线性模型的对数连接函数和贝叶斯法则 (Bayes' rule)。\n3. 为每个数据集按以下顺序报告四个数字：$\\sigma^2$ 的后验均值、$\\rho$ 的后验众数、$\\beta$ 的后验众数、$\\delta$ 的后验众数。\n\n单位和数值规范：\n- 时间 $t_i$ 和 $d_i$ 的单位是百万年，所以 $\\sigma^2$ 的单位是性状单位的平方每百万年，$r$ 的单位是事件每百万年。程序必须报告 $\\rho = \\log r$（无量纲），而不是 $r$。\n- 所有输出必须四舍五入到小数点后四位。\n\n测试套件：\n使用以下三个数据集和共享的先验超参数。对于下文所有数学实体，数字都已明确给出。\n\n所有数据集共享的先验超参数：\n- $a_\\sigma = 3.0$, $b_\\sigma = 1.5$。\n- $\\mu_r = \\log(0.2)$, $s_r = 0.5$。\n- $\\beta$ 的先验：均值为 $0$，标准差为 $s_\\beta = 0.5$。\n- $\\delta$ 的先验：均值为 $m_\\delta = 0.2$，标准差为 $s_\\delta = 0.5$。\n\n数据集A（理想路径）：\n- 性状时间 $t_i$: $[1.0, 2.0, 3.0, 1.5, 2.5]$。\n- 性状值 $x_i$: $[0.1, -0.2, 0.3, 0.0, 0.2]$。\n- 观测持续时间 $d_i$: $[2.0, 2.5, 3.0, 2.0, 2.5]$。\n- 创新指标 $z_i$: $[0, 1, 1, 0, 1]$。\n- 计数 $y_i$: $[0, 2, 3, 0, 2]$。\n\n数据集B（类边界情况，计数低且历史短）：\n- 性状时间 $t_i$: $[0.5, 0.8, 0.6]$。\n- 性状值 $x_i$: $[0.05, -0.02, 0.01]$。\n- 观测持续时间 $d_i$: $[0.5, 0.7, 0.6]$。\n- 创新指标 $z_i$: $[0, 1, 0]$。\n- 计数 $y_i$: $[0, 0, 0]$。\n\n数据集C（强创新信号）：\n- 性状时间 $t_i$: $[1.0, 1.0, 1.0, 1.0]$。\n- 性状值 $x_i$: $[0.0, 0.1, -0.1, 0.05]$。\n- 观测持续时间 $d_i$: $[3.0, 3.0, 3.0, 3.0]$。\n- 创新指标 $z_i$: $[0, 1, 1, 0]$。\n- 计数 $y_i$: $[1, 8, 7, 2]$。\n\n要求的最终输出格式：\n您的程序应生成单行输出，其中包含结果，格式为逗号分隔的 Python 风格列表的列表，每个内部列表对应一个数据集，顺序为 A、B、C。每个内部列表必须按上述顺序包含四个所要求的浮点值，每个值四舍五入到小数点后四位。例如，一个有效的格式是\n[[v11,v12,v13,v14],[v21,v22,v23,v24],[v31,v32,v33,v34]]\n其中每个 $v_{jk}$ 是一个打印时小数点后恰好有四位数字的浮点数。不应打印任何附加文本。",
            "solution": "在尝试给出解决方案之前，对所述问题进行验证。\n\n步骤1：提取已知条件。\n- **性状演化模型**：$x_i \\sim \\mathcal{N}(0, \\sigma^2 t_i)$，对于长度为 $t_i$ 的谱系。\n- **多样化模型**：$y_i \\mid x_i, z_i, r, \\beta, \\delta \\sim \\mathrm{Poisson}\\!\\left(d_i \\, r \\, \\exp(\\beta x_i + \\delta z_i)\\right)$，对于观测窗口 $d_i$。\n- **参数重定义**：$\\rho = \\log r$。\n- **$\\sigma^2$ 的先验**：$\\sigma^2 \\sim \\mathrm{Inverse\\!-\\!Gamma}(a_\\sigma, b_\\sigma)$。\n- **$\\rho$ 的先验**：$\\rho \\sim \\mathcal{N}(\\mu_r, s_r^2)$。\n- **$\\beta$ 的先验**：$\\beta \\sim \\mathcal{N}(0, s_\\beta^2)$。\n- **$\\delta$ 的先验**：$\\delta \\sim \\mathcal{N}(m_\\delta, s_\\delta^2)$。\n- **共享的先验超参数**：\n    - $a_\\sigma = 3.0$, $b_\\sigma = 1.5$。\n    - $\\mu_r = \\log(0.2)$, $s_r = 0.5$。\n    - 对于 $\\beta$：均值为 $0$，标准差为 $s_\\beta = 0.5$。\n    - 对于 $\\delta$：均值为 $m_\\delta = 0.2$，标准差为 $s_\\delta = 0.5$。\n- **数据集A**：\n    - $t = [1.0, 2.0, 3.0, 1.5, 2.5]$\n    - $x = [0.1, -0.2, 0.3, 0.0, 0.2]$\n    - $d = [2.0, 2.5, 3.0, 2.0, 2.5]$\n    - $z = [0, 1, 1, 0, 1]$\n    - $y = [0, 2, 3, 0, 2]$\n- **数据集B**：\n    - $t = [0.5, 0.8, 0.6]$\n    - $x = [0.05, -0.02, 0.01]$\n    - $d = [0.5, 0.7, 0.6]$\n    - $z = [0, 1, 0]$\n    - $y = [0, 0, 0]$\n- **数据集C**：\n    - $t = [1.0, 1.0, 1.0, 1.0]$\n    - $x = [0.0, 0.1, -0.1, 0.05]$\n    - $d = [3.0, 3.0, 3.0, 3.0]$\n    - $z = [0, 1, 1, 0]$\n    - $y = [1, 8, 7, 2]$\n- **任务**：\n    1. 计算 $\\sigma^2$ 的后验均值。\n    2. 计算 $(\\rho, \\beta, \\delta)$ 的后验众数（MAP）。\n- **输出格式**：四舍五入到小数点后四位，作为列表的列表。\n\n步骤2：验证已知条件。\n该问题在科学上和数学上都是合理的。所提出的模型——用于连续性状演化的布朗运动和用于计数谱系多样化事件的泊松过程——是现代演化生物学中的标准和基础模型。具有指定先验（逆伽马分布、正态分布）的分层贝叶斯结构是在此背景下进行统计推断的成熟且适当的方法。问题定义明确：任务1涉及标准的共轭先验计算，可得到唯一的后验均值；任务2涉及寻找一个对数凹后验分布的众数，这保证了存在唯一的最大值。所有必要的数据和超参数都已提供，且没有矛盾之处。该问题是可形式化的、客观的且完整的。\n\n步骤3：结论与行动。\n问题有效。将构建一个解决方案。\n\n该解决方案按照规定分为两个独立的估计问题。\n\n第一部分：性状方差速率 $\\sigma^2$ 的后验均值。\n性状演化模型指出，在持续时间为 $t_i$ 的谱系上，变化 $x_i$ 遵循均值为 $0$、方差为 $\\sigma^2 t_i$ 的正态分布。单个观测值 $x_i$ 的似然函数为：\n$$p(x_i | \\sigma^2, t_i) = \\frac{1}{\\sqrt{2\\pi \\sigma^2 t_i}} \\exp\\left(-\\frac{x_i^2}{2\\sigma^2 t_i}\\right)$$\n对于 $n$ 个独立的谱系，总似然 $L$ 是各个似然的乘积：\n$$L(\\sigma^2 | \\{x_i, t_i\\}_{i=1}^n) \\propto (\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\frac{x_i^2}{t_i}\\right)$$\n$\\sigma^2$ 的先验是逆伽马分布，$\\sigma^2 \\sim \\mathrm{IG}(a_\\sigma, b_\\sigma)$，其概率密度函数（PDF）正比于：\n$$p(\\sigma^2) \\propto (\\sigma^2)^{-a_\\sigma - 1} \\exp\\left(-\\frac{b_\\sigma}{\\sigma^2}\\right)$$\n根据贝叶斯定理 (Bayes' theorem)，后验分布正比于似然与先验的乘积：\n$$p(\\sigma^2 | \\{x_i, t_i\\}) \\propto L(\\sigma^2 | \\{x_i, t_i\\}) \\times p(\\sigma^2)$$\n$$p(\\sigma^2 | \\{x_i, t_i\\}) \\propto \\left[ (\\sigma^2)^{-n/2} \\exp\\left(-\\frac{S}{2\\sigma^2}\\right) \\right] \\times \\left[ (\\sigma^2)^{-a_\\sigma - 1} \\exp\\left(-\\frac{b_\\sigma}{\\sigma^2}\\right) \\right]$$\n其中 $S = \\sum_{i=1}^n \\frac{x_i^2}{t_i}$。合并各项得到：\n$$p(\\sigma^2 | \\{x_i, t_i\\}) \\propto (\\sigma^2)^{-(a_\\sigma + n/2) - 1} \\exp\\left(-\\frac{1}{\\sigma^2} \\left(b_\\sigma + \\frac{S}{2}\\right)\\right)$$\n这是一个逆伽马分布 $\\mathrm{IG}(a'_\\sigma, b'_\\sigma)$ 的核，其更新后的参数为：\n$$a'_\\sigma = a_\\sigma + \\frac{n}{2}$$\n$$b'_\\sigma = b_\\sigma + \\frac{1}{2} \\sum_{i=1}^n \\frac{x_i^2}{t_i}$$\n$\\mathrm{IG}(a', b')$ 分布的均值是 $\\frac{b'}{a' - 1}$，前提是 $a' > 1$。给定 $a_\\sigma = 3.0$ 且 $n \\geq 1$，此条件始终满足。因此，$\\sigma^2$ 的后验均值为：\n$$\\mathbb{E}[\\sigma^2 | \\{x_i, t_i\\}] = \\frac{b'_\\sigma}{a'_\\sigma - 1} = \\frac{b_\\sigma + \\frac{1}{2}\\sum_{i=1}^n (x_i^2/t_i)}{a_\\sigma + n/2 - 1}$$\n此解析公式将用于每个数据集。\n\n第二部分：多样化参数 $(\\rho, \\beta, \\delta)$ 的最大后验（MAP）估计。\n多样化事件数 $y_i$ 被建模为泊松过程，$y_i \\sim \\mathrm{Poisson}(\\lambda_i d_i)$，其中速率 $\\lambda_i$ 通过对数线性模型与协变量相关：$\\lambda_i = r \\exp(\\beta x_i + \\delta z_i)$。设 $\\rho = \\log r$，则泊松均值为 $\\mu_i = d_i \\exp(\\rho + \\beta x_i + \\delta z_i)$。令 $\\theta = (\\rho, \\beta, \\delta)^T$ 为参数向量。完整数据集的对数似然为：\n$$\\log L(\\theta | \\{y_i, d_i, x_i, z_i\\}) = \\sum_{i=1}^n \\left( y_i \\log(\\mu_i) - \\mu_i - \\log(y_i!) \\right)$$\n代入 $\\mu_i$ 并令 $\\eta_i = \\rho + \\beta x_i + \\delta z_i$，我们得到：\n$$\\log L(\\theta) = \\sum_{i=1}^n \\left( y_i (\\log d_i + \\eta_i) - d_i e^{\\eta_i} - \\log(y_i!) \\right)$$\n参数的先验是独立的诺态分布：$\\rho \\sim \\mathcal{N}(\\mu_r, s_r^2)$，$\\beta \\sim \\mathcal{N}(0, s_\\beta^2)$，以及 $\\delta \\sim \\mathcal{N}(m_\\delta, s_\\delta^2)$。联合对数先验为：\n$$\\log p(\\theta) = -\\frac{(\\rho - \\mu_r)^2}{2s_r^2} - \\frac{\\beta^2}{2s_\\beta^2} - \\frac{(\\delta - m_\\delta)^2}{2s_\\delta^2} + C$$\n其中 $C$ 是一个常数。对数后验 $\\mathcal{L}(\\theta)$ 是对数似然和对数先验之和。为找到 MAP 估计，我们最大化 $\\mathcal{L}(\\theta)$ 关于 $\\theta$ 的值。这等价于最小化负对数后验 $f(\\theta) = -\\mathcal{L}(\\theta)$。要最小化的目标函数（忽略常数）是：\n$$f(\\theta) = \\sum_{i=1}^n \\left( d_i e^{\\eta_i} - y_i \\eta_i \\right) + \\frac{(\\rho - \\mu_r)^2}{2s_r^2} + \\frac{\\beta^2}{2s_\\beta^2} + \\frac{(\\delta - m_\\delta)^2}{2s_\\delta^2}$$\n这是一个凸函数，因此存在唯一的最小值。我们使用拟牛顿优化算法（L-BFGS-B），该算法需要 $f(\\theta)$ 的梯度。梯度 $\\nabla f(\\theta)$ 的分量通过求偏导数得到：\n$$\\frac{\\partial f}{\\partial \\rho} = \\sum_{i=1}^n \\left( d_i e^{\\eta_i} - y_i \\right) + \\frac{\\rho - \\mu_r}{s_r^2}$$\n$$\\frac{\\partial f}{\\partial \\beta} = \\sum_{i=1}^n \\left( d_i x_i e^{\\eta_i} - y_i x_i \\right) + \\frac{\\beta}{s_\\beta^2}$$\n$$\\frac{\\partial f}{\\partial \\delta} = \\sum_{i=1}^n \\left( d_i z_i e^{\\eta_i} - y_i z_i \\right) + \\frac{\\delta - m_\\delta}{s_\\delta^2}$$\n这些解析梯度将提供给数值优化器，以找到 MAP 估计 $\\hat{\\theta}_{MAP} = \\arg\\min_{\\theta} f(\\theta)$。\n\n对于每个数据集，我们首先使用推导出的公式计算 $\\mathbb{E}[\\sigma^2 | \\text{data}]$，然后执行数值优化程序找到后验众数 $(\\hat{\\rho}_{MAP}, \\hat{\\beta}_{MAP}, \\hat{\\delta}_{MAP})$。然后收集并格式化结果。",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves the hierarchical Bayesian model problem for three datasets.\n    Computes the posterior mean of sigma^2 and the MAP estimate of (rho, beta, delta).\n    \"\"\"\n\n    # Shared prior hyperparameters\n    a_sigma = 3.0\n    b_sigma = 1.5\n    mu_r = np.log(0.2)\n    s_r = 0.5\n    s_beta = 0.5\n    m_delta = 0.2\n    s_delta = 0.5\n    \n    # Variances for normal priors\n    s_r2 = s_r**2\n    s_beta2 = s_beta**2\n    s_delta2 = s_delta**2\n\n    priors_part2 = (mu_r, s_r2, s_beta2, m_delta, s_delta2)\n\n    # Test cases\n    datasets = [\n        { # Dataset A\n            \"t\": np.array([1.0, 2.0, 3.0, 1.5, 2.5]),\n            \"x\": np.array([0.1, -0.2, 0.3, 0.0, 0.2]),\n            \"d\": np.array([2.0, 2.5, 3.0, 2.0, 2.5]),\n            \"z\": np.array([0, 1, 1, 0, 1]),\n            \"y\": np.array([0, 2, 3, 0, 2]),\n        },\n        { # Dataset B\n            \"t\": np.array([0.5, 0.8, 0.6]),\n            \"x\": np.array([0.05, -0.02, 0.01]),\n            \"d\": np.array([0.5, 0.7, 0.6]),\n            \"z\": np.array([0, 1, 0]),\n            \"y\": np.array([0, 0, 0]),\n        },\n        { # Dataset C\n            \"t\": np.array([1.0, 1.0, 1.0, 1.0]),\n            \"x\": np.array([0.0, 0.1, -0.1, 0.05]),\n            \"d\": np.array([3.0, 3.0, 3.0, 3.0]),\n            \"z\": np.array([0, 1, 1, 0]),\n            \"y\": np.array([1, 8, 7, 2]),\n        },\n    ]\n\n    # --- Helper functions for calculation ---\n\n    def compute_sigma2_posterior_mean(t, x, a_sig, b_sig):\n        \"\"\"\n        Computes the posterior mean of sigma^2.\n        E[sigma^2|data] = (b_sigma + 0.5 * sum(x_i^2/t_i)) / (a_sigma + n/2 - 1)\n        \"\"\"\n        n = len(t)\n        sum_of_squares_over_time = np.sum(x**2 / t)\n        \n        a_prime = a_sig + n / 2.0\n        b_prime = b_sig + 0.5 * sum_of_squares_over_time\n        \n        posterior_mean = b_prime / (a_prime - 1.0)\n        return posterior_mean\n\n    def neg_log_posterior(params, y, d, x, z, priors):\n        \"\"\"\n        Computes the negative log-posterior for the Poisson GLM part.\n        params: [rho, beta, delta]\n        priors: (mu_r, s_r2, s_beta2, m_delta, s_delta2)\n        \"\"\"\n        rho, beta, delta = params\n        mu_r, s_r2, s_beta2, m_delta, s_delta2 = priors\n        \n        eta = rho + beta * x + delta * z\n        \n        # Log-likelihood (ignoring constant log(y_i!))\n        # sum(y_i * eta_i - d_i * exp(eta_i))\n        log_lik = np.sum(y * eta - d * np.exp(eta))\n        \n        # Log-priors (ignoring constant normalization factors)\n        log_prior_rho = -0.5 * (rho - mu_r)**2 / s_r2\n        log_prior_beta = -0.5 * beta**2 / s_beta2\n        log_prior_delta = -0.5 * (delta - m_delta)**2 / s_delta2\n        \n        # Negative log-posterior\n        return -(log_lik + log_prior_rho + log_prior_beta + log_prior_delta)\n\n    def grad_neg_log_posterior(params, y, d, x, z, priors):\n        \"\"\"\n        Computes the gradient of the negative log-posterior.\n        \"\"\"\n        rho, beta, delta = params\n        mu_r, s_r2, s_beta2, m_delta, s_delta2 = priors\n\n        eta = rho + beta * x + delta * z\n        exp_eta = np.exp(eta)\n        \n        # Derivatives of log-likelihood w.r.t params\n        # d/d_param (sum(y*eta - d*exp(eta)))\n        # = sum(y * d_eta/d_param - d*exp(eta)*d_eta/d_param)\n        # = sum((y - d*exp(eta)) * d_eta/d_param)\n        common_term = y - d * exp_eta\n        \n        grad_log_lik_rho = np.sum(common_term)\n        grad_log_lik_beta = np.sum(common_term * x)\n        grad_log_lik_delta = np.sum(common_term * z)\n        \n        # Derivatives of log-priors w.r.t params\n        grad_log_prior_rho = -(rho - mu_r) / s_r2\n        grad_log_prior_beta = -beta / s_beta2\n        grad_log_prior_delta = -(delta - m_delta) / s_delta2\n        \n        # Gradient of negative log-posterior\n        # - (grad_log_lik + grad_log_prior)\n        grad_rho = -(grad_log_lik_rho + grad_log_prior_rho)\n        grad_beta = -(grad_log_lik_beta + grad_log_prior_beta)\n        grad_delta = -(grad_log_lik_delta + grad_log_prior_delta)\n        \n        return np.array([grad_rho, grad_beta, grad_delta])\n\n    # --- Main loop to process datasets ---\n    \n    final_results = []\n    \n    for data in datasets:\n        # Part 1: Compute posterior mean of sigma^2\n        sigma2_mean = compute_sigma2_posterior_mean(data[\"t\"], data[\"x\"], a_sigma, b_sigma)\n        \n        # Part 2: Compute MAP for (rho, beta, delta)\n        # Initial guess for optimization\n        x0 = np.array([mu_r, 0.0, m_delta])\n        \n        # Arguments for the optimizer functions\n        args = (data[\"y\"], data[\"d\"], data[\"x\"], data[\"z\"], priors_part2)\n        \n        opt_result = minimize(\n            fun=neg_log_posterior,\n            x0=x0,\n            args=args,\n            method='L-BFGS-B',\n            jac=grad_neg_log_posterior\n        )\n        \n        rho_map, beta_map, delta_map = opt_result.x\n        \n        # Collect and round results for the current dataset\n        dataset_results = [\n            round(sigma2_mean, 4),\n            round(rho_map, 4),\n            round(beta_map, 4),\n            round(delta_map, 4)\n        ]\n        final_results.append(dataset_results)\n        \n    # Format and print the final output as a single line\n    # Example: [[v11,v12,v13,v14],[v21,v22,v23,v24],[v31,v32,v33,v34]]\n    # This is achieved by joining string representations of the inner lists.\n    output_str = f\"[[{', '.join(f'{v:.4f}' for v in final_results[0])}], [{', '.join(f'{v:.4f}' for v in final_results[1])}], [{', '.join(f'{v:.4f}' for v in final_results[2])}]]\"\n    print(output_str)\n\n# The expected final output from the problem is the printed result of this function.\n# The user's format has the code itself in the answer tag. Per instructions to preserve structure\n# while fixing errors, the corrected code is the answer. For a typical run, one would call solve()\n# and paste the output. However, the problem asks for the *program* to be implemented. \n# The solution derives the logic, and the answer provides the implementation. This is a valid\n# pedagogical structure. I will therefore provide the corrected code as the answer.\n# The user has used grave accents and four of them. I'll correct to standard three backticks.\n```\n```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves the hierarchical Bayesian model problem for three datasets.\n    Computes the posterior mean of sigma^2 and the MAP estimate of (rho, beta, delta).\n    \"\"\"\n\n    # Shared prior hyperparameters\n    a_sigma = 3.0\n    b_sigma = 1.5\n    mu_r = np.log(0.2)\n    s_r = 0.5\n    s_beta = 0.5\n    m_delta = 0.2\n    s_delta = 0.5\n    \n    # Variances for normal priors\n    s_r2 = s_r**2\n    s_beta2 = s_beta**2\n    s_delta2 = s_delta**2\n\n    priors_part2 = (mu_r, s_r2, s_beta2, m_delta, s_delta2)\n\n    # Test cases\n    datasets = [\n        { # Dataset A\n            \"t\": np.array([1.0, 2.0, 3.0, 1.5, 2.5]),\n            \"x\": np.array([0.1, -0.2, 0.3, 0.0, 0.2]),\n            \"d\": np.array([2.0, 2.5, 3.0, 2.0, 2.5]),\n            \"z\": np.array([0, 1, 1, 0, 1]),\n            \"y\": np.array([0, 2, 3, 0, 2]),\n        },\n        { # Dataset B\n            \"t\": np.array([0.5, 0.8, 0.6]),\n            \"x\": np.array([0.05, -0.02, 0.01]),\n            \"d\": np.array([0.5, 0.7, 0.6]),\n            \"z\": np.array([0, 1, 0]),\n            \"y\": np.array([0, 0, 0]),\n        },\n        { # Dataset C\n            \"t\": np.array([1.0, 1.0, 1.0, 1.0]),\n            \"x\": np.array([0.0, 0.1, -0.1, 0.05]),\n            \"d\": np.array([3.0, 3.0, 3.0, 3.0]),\n            \"z\": np.array([0, 1, 1, 0]),\n            \"y\": np.array([1, 8, 7, 2]),\n        },\n    ]\n\n    # --- Helper functions for calculation ---\n\n    def compute_sigma2_posterior_mean(t, x, a_sig, b_sig):\n        \"\"\"\n        Computes the posterior mean of sigma^2.\n        E[sigma^2|data] = (b_sigma + 0.5 * sum(x_i^2/t_i)) / (a_sigma + n/2 - 1)\n        \"\"\"\n        n = len(t)\n        sum_of_squares_over_time = np.sum(x**2 / t)\n        \n        a_prime = a_sig + n / 2.0\n        b_prime = b_sig + 0.5 * sum_of_squares_over_time\n        \n        posterior_mean = b_prime / (a_prime - 1.0)\n        return posterior_mean\n\n    def neg_log_posterior(params, y, d, x, z, priors):\n        \"\"\"\n        Computes the negative log-posterior for the Poisson GLM part.\n        params: [rho, beta, delta]\n        priors: (mu_r, s_r2, s_beta2, m_delta, s_delta2)\n        \"\"\"\n        rho, beta, delta = params\n        mu_r, s_r2, s_beta2, m_delta, s_delta2 = priors\n        \n        eta = rho + beta * x + delta * z\n        \n        # Log-likelihood (ignoring constant log(y_i!))\n        # sum(y_i * eta_i - d_i * exp(eta_i))\n        log_lik = np.sum(y * eta - d * np.exp(eta))\n        \n        # Log-priors (ignoring constant normalization factors)\n        log_prior_rho = -0.5 * (rho - mu_r)**2 / s_r2\n        log_prior_beta = -0.5 * beta**2 / s_beta2\n        log_prior_delta = -0.5 * (delta - m_delta)**2 / s_delta2\n        \n        # Negative log-posterior\n        return -(log_lik + log_prior_rho + log_prior_beta + log_prior_delta)\n\n    def grad_neg_log_posterior(params, y, d, x, z, priors):\n        \"\"\"\n        Computes the gradient of the negative log-posterior.\n        \"\"\"\n        rho, beta, delta = params\n        mu_r, s_r2, s_beta2, m_delta, s_delta2 = priors\n\n        eta = rho + beta * x + delta * z\n        exp_eta = np.exp(eta)\n        \n        # Derivatives of log-likelihood w.r.t params\n        # d/d_param (sum(y*eta - d*exp(eta)))\n        # = sum(y * d_eta/d_param - d*exp(eta)*d_eta/d_param)\n        # = sum((y - d*exp(eta)) * d_eta/d_param)\n        common_term = y - d * exp_eta\n        \n        grad_log_lik_rho = np.sum(common_term)\n        grad_log_lik_beta = np.sum(common_term * x)\n        grad_log_lik_delta = np.sum(common_term * z)\n        \n        # Derivatives of log-priors w.r.t params\n        grad_log_prior_rho = -(rho - mu_r) / s_r2\n        grad_log_prior_beta = -beta / s_beta2\n        grad_log_prior_delta = -(delta - m_delta) / s_delta2\n        \n        # Gradient of negative log-posterior\n        # - (grad_log_lik + grad_log_prior)\n        grad_rho = -(grad_log_lik_rho + grad_log_prior_rho)\n        grad_beta = -(grad_log_lik_beta + grad_log_prior_beta)\n        grad_delta = -(grad_log_lik_delta + grad_log_prior_delta)\n        \n        return np.array([grad_rho, grad_beta, grad_delta])\n\n    # --- Main loop to process datasets ---\n    \n    final_results = []\n    \n    for data in datasets:\n        # Part 1: Compute posterior mean of sigma^2\n        sigma2_mean = compute_sigma2_posterior_mean(data[\"t\"], data[\"x\"], a_sigma, b_sigma)\n        \n        # Part 2: Compute MAP for (rho, beta, delta)\n        # Initial guess for optimization\n        x0 = np.array([mu_r, 0.0, m_delta])\n        \n        # Arguments for the optimizer functions\n        args = (data[\"y\"], data[\"d\"], data[\"x\"], data[\"z\"], priors_part2)\n        \n        opt_result = minimize(\n            fun=neg_log_posterior,\n            x0=x0,\n            args=args,\n            method='L-BFGS-B',\n            jac=grad_neg_log_posterior\n        )\n        \n        rho_map, beta_map, delta_map = opt_result.x\n        \n        # Collect and round results for the current dataset\n        dataset_results = [\n            round(sigma2_mean, 4),\n            round(rho_map, 4),\n            round(beta_map, 4),\n            round(delta_map, 4)\n        ]\n        final_results.append(dataset_results)\n        \n    # Format and print the final output as a single line\n    print(f\"[[{', '.join([f'{v:.4f}' for v in final_results[0]])}], [{', '.join([f'{v:.4f}' for v in final_results[1]])}], [{', '.join([f'{v:.4f}' for v in final_results[2]])}]]\")\n\nsolve()\n```\n```"
        }
    ]
}