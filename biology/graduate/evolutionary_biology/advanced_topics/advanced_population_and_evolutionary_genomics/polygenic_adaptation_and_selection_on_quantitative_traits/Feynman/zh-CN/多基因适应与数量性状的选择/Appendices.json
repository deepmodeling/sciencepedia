{
    "hands_on_practices": [
        {
            "introduction": "要预测一个性状如何演化，我们首先必须量化其可遗传的变异。本练习将通过一个在现代数量遗传学中至关重要的方法——在线性混合模型中使用限制性最大似然法（REML）——提供实践经验。你将学习如何将表型方差分解为遗传和环境两个部分，并估算狭义遗传力（$h^2$），这是所有进化预测中的一个关键参数 。",
            "id": "2744356",
            "problem": "给定多个样本量不同的独立群体，其表现出一种由许多微效基因座（多基因）形成的数量性状。假设以下基本模型：个体的数量性状值遵循一个线性混合模型，该模型包含一个通过亲缘矩阵捕捉共同祖先的加性遗传随机效应，以及一个独立的环境残差。在此模型下，对于一个在固定效应设计中包含截距且样本量为 $n$ 的样本，性状向量的分布是多元正态的，并且性状的方差可以分解为一个与亲缘矩阵成比例的加性遗传组分和一个与单位矩阵成比例的环境组分。目标是通过最大化限制性似然来联合估计加性遗传方差和环境方差，然后计算狭义遗传力，其定义为可归因于加性遗传效应的表型方差比例。您的任务是实现一个程序，使用稳定的线性代数，通过对限制性似然进行数值优化来执行此估计。\n\n请使用以下模型定义和约束：\n- 令性状向量表示为 $y \\in \\mathbb{R}^{n}$，固定效应设计矩阵表示为 $X \\in \\mathbb{R}^{n \\times p}$，且仅包含截距（因此 $p = 1$ 且 $X$ 是一个全为1的列向量）。令亲缘矩阵为 $K \\in \\mathbb{R}^{n \\times n}$，该矩阵为对称半正定矩阵。\n- 方差组分为加性遗传方差 $\\sigma_{g}^{2}$ 和环境方差 $\\sigma_{e}^{2}$，两者均为严格正实数。\n- $y$ 的协方差是加性遗传协方差和环境协方差之和。您必须使用一种能够保证方差组分正性的数值稳定方法，对关于 $\\sigma_{g}^{2}$ 和 $\\sigma_{e}^{2}$ 的限制性似然进行最大化。\n- 在估计出 $\\widehat{\\sigma}_{g}^{2}$ 和 $\\widehat{\\sigma}_{e}^{2}$ 之后，计算狭义遗传力 $h^{2}$，即总表型方差中可归因于加性遗传效应的比例。报告每个测试用例的 $h^{2}$。\n- 所有计算必须以浮点运算完成。不涉及角度。不需要物理单位。最终数值答案以小数形式表示（非分数），并四舍五入到四位小数。\n\n测试套件的输入数据如下。对于每个用例，给定一个因子矩阵 $B$（必须通过 $K = B B^{\\top}$ 构建亲缘矩阵以确保其半正定性）和一个观测到的性状向量 $y$。在所有用例中，使用一个各项均为 $1$ 的仅含截距的设计矩阵 $X$。实现一个对条件数稳健的限制性似然数值优化，使用重参数化来强制 $\\sigma_{g}^{2} > 0$ 和 $\\sigma_{e}^{2} > 0$，并使用稳健的求解器来处理线性系统和对数行列式。\n\n测试套件：\n- 用例 A（中等遗传力）：\n  - $B = \\begin{bmatrix}\n  1.2 & 0.0 & 0.0 \\\\\n  0.6 & 0.3 & 0.0 \\\\\n  0.4 & 0.8 & 0.2 \\\\\n  0.0 & 0.5 & 0.7 \\\\\n  0.0 & 0.2 & 1.0 \\\\\n  0.0 & 0.0 & 0.9\n  \\end{bmatrix}$\n  - $y = \\begin{bmatrix} 1.04 & 0.61 & 0.82 & 0.25 & -0.04 & -0.20 \\end{bmatrix}^{\\top}$\n- 用例 B（接近于零的遗传力）：\n  - $B$ 与用例 A 相同。\n  - $y = \\begin{bmatrix} 0.30 & -0.70 & 0.20 & -0.10 & 0.05 & -0.30 \\end{bmatrix}^{\\top}$\n- 用例 C（高遗传力）：\n  - $B = \\begin{bmatrix}\n  1.0 & 0.0 \\\\\n  0.8 & 0.2 \\\\\n  0.6 & 0.4 \\\\\n  0.4 & 0.6 \\\\\n  0.2 & 0.8\n  \\end{bmatrix}$\n  - $y = \\begin{bmatrix} 1.01 & 0.79 & 0.605 & 0.395 & 0.20 \\end{bmatrix}^{\\top}$\n- 用例 D（中等遗传力，不同结构和大小）：\n  - $B = \\begin{bmatrix}\n  1.0 & 0.0 & 0.0 \\\\\n  0.7 & 0.2 & 0.0 \\\\\n  0.5 & 0.4 & 0.1 \\\\\n  0.3 & 0.6 & 0.2 \\\\\n  0.1 & 0.8 & 0.3 \\\\\n  0.0 & 0.6 & 0.5 \\\\\n  0.0 & 0.3 & 0.9\n  \\end{bmatrix}$\n  - $y = \\begin{bmatrix} 0.55 & 0.40 & 0.45 & 0.42 & 0.47 & 0.32 & 0.33 \\end{bmatrix}^{\\top}$\n\n要求和输出格式：\n- 为每个用例构造 $K = B B^{\\top}$。\n- 给定 $y$、$X$ 和 $K$，通过最大化关于参数 $\\widehat{\\sigma}_{g}^{2}$ 和 $\\widehat{\\sigma}_{e}^{2}$ 的限制性似然来估计它们。使用数值稳定的线性代数（例如，避免显式矩阵求逆的矩阵分解）和确保方差组分正性的无约束重参数化方法。\n- 对于每个用例，根据估计出的方差组分计算狭义遗传力 $h^{2}$。\n- 您的程序应生成单行输出，其中包含四个遗传力估计值的结果，以逗号分隔，并用方括号括起来，顺序为用例 A、用例 B、用例 C、用例 D。每个估计值需四舍五入到四位小数。例如，一个正确格式的输出是 $[\\;0.5023,0.0311,0.9120,0.4722\\;]$（这些是示例值，不是必需的答案）。",
            "solution": "所提出的问题是统计遗传学中的一个标准且适定的任务：使用限制性最大似然（REML）估计线性混合模型（LMM）中的方差组分。该问题具有科学依据，没有矛盾，并包含解决问题所需的所有信息。因此，我将提供一个完整的解决方案。\n\n数量性状向量 $y \\in \\mathbb{R}^n$ 的基本模型是 LMM：\n$$ y = X\\beta + u + \\epsilon $$\n其中：\n- $X \\in \\mathbb{R}^{n \\times p}$ 是固定效应的设计矩阵，在此问题中是代表全局截距的一个全为1的单列向量（$p=1$）。\n- $\\beta \\in \\mathbb{R}^p$ 是固定效应向量。\n- $u \\in \\mathbb{R}^n$ 是随机遗传效应向量，假定其服从多元正态分布 $u \\sim \\mathcal{N}(0, \\sigma_g^2 K)$，其中 $\\sigma_g^2$ 是加性遗传方差， $K$ 是源于祖先信息的亲缘矩阵。\n- $\\epsilon \\in \\mathbb{R}^n$ 是随机环境效应和非遗传效应的向量，假定其服从 $\\epsilon \\sim \\mathcal{N}(0, \\sigma_e^2 I)$，其中 $\\sigma_e^2$ 是环境方差， $I$ 是 $n \\times n$ 的单位矩阵。\n\n随机效应 $u$ 和 $\\epsilon$ 假定为相互独立。因此，性状向量 $y$ 服从多元正态分布：\n$$ y \\sim \\mathcal{N}(X\\beta, V) $$\n其中总协方差矩阵 $V$ 由以下公式给出：\n$$ V(\\sigma_g^2, \\sigma_e^2) = \\sigma_g^2 K + \\sigma_e^2 I $$\n目标是估计方差组分 $\\sigma_g^2$ 和 $\\sigma_e^2$。简单的最大似然估计会产生有偏估计，因为它没有考虑估计固定效应 $\\beta$ 时损失的自由度。REML 通过最大化一组不依赖于 $\\beta$ 的 $n-p$ 个线性无关的 $y$ 的对比的似然来对此进行校正。\n\n忽略常数项后，需要对 $\\sigma_g^2$ 和 $\\sigma_e^2$ 进行最大化的 REML 对数似然函数为：\n$$ \\ell_{\\text{REML}}(\\sigma_g^2, \\sigma_e^2 | y) = -\\frac{1}{2} \\log\\det(V) - \\frac{1}{2} \\log\\det(X^\\top V^{-1} X) - \\frac{1}{2} y^\\top P y $$\n其中 $P$ 是投影矩阵 $P = V^{-1} - V^{-1}X(X^\\top V^{-1}X)^{-1}X^\\top V^{-1}$。\n\n由于需要显式计算矩阵的逆（$V^{-1}$）和行列式，直接最大化此函数在数值上是有问题的。必须采用数值稳定的方法。此外，方差组分必须为严格正数，即 $\\sigma_g^2 > 0$ 和 $\\sigma_e^2 > 0$。处理这种约束优化的最佳方法是通过重参数化将问题转化为无约束问题。我们定义新参数：\n$$ \\theta_g = \\log(\\sigma_g^2) \\quad \\text{和} \\quad \\theta_e = \\log(\\sigma_e^2) $$\n然后对 $(\\theta_g, \\theta_e) \\in \\mathbb{R}^2$ 进行优化。方差组分通过 $\\sigma_g^2 = \\exp(\\theta_g)$ 和 $\\sigma_e^2 = \\exp(\\theta_e)$ 恢复，这样可以保证其为正值。\n\nREML 似然的各组成部分将使用稳定的矩阵分解来计算。由于 $K$ 是半正定的且 $\\sigma_e^2 > 0$，矩阵 $V = \\sigma_g^2 K + \\sigma_e^2 I$ 保证是对称正定的。因此，它允许进行 Cholesky 分解 $V = LL^\\top$，其中 $L$ 是一个下三角矩阵。这种分解是实现稳定计算的基石。\n\n负对数似然函数（我们将对其进行最小化）中的各项计算如下：\n1.  **$V$ 的对数行列式**：$\\log\\det(V) = \\log\\det(LL^\\top) = 2 \\log\\det(L) = 2 \\sum_{i=1}^{n} \\log(L_{ii})$。\n2.  **求解线性系统**：我们不直接计算 $V^{-1}$，而是求解线性系统。对于一个向量 $z$，乘积 $V^{-1}z$ 可以通过求解 $Vw = z$ 来得到 $w$。使用 Cholesky 因子，这可以通过两次三角求解完成：首先求解 $Lq = z$ 得到 $q$（前向替换），然后求解 $L^\\top w = q$ 得到 $w$（后向替换）。\n3.  **$X^\\top V^{-1} X$ 的对数行列式**：我们来计算矩阵 $A = X^\\top V^{-1} X$。其第 $j$ 列是 $X^\\top V^{-1} X_j$，其中 $X_j$ 是 $X$ 的第 $j$ 列。我们首先使用上述 Cholesky 方法求解 $W_j = V^{-1}X_j$。然后我们构造 $X^\\top W_j$。对于此问题，$p=1$ 且 $X$ 是一个全为1的列向量，所以 $X^\\top V^{-1} X$ 是一个标量。我们求解 $Vw_x = X$ 得到 $w_x$，然后计算标量 $s_{xx} = X^\\top w_x$。对数行列式项就是 $\\log(s_{xx})$。\n4.  **二次型 $y^\\top P y$**：\n    $$ y^\\top P y = y^\\top V^{-1} y - y^\\top V^{-1} X (X^\\top V^{-1} X)^{-1} X^\\top V^{-1} y $$\n    我们求解 $Vw_y = y$ 得到 $w_y$。那么 $y^\\top V^{-1} y = y^\\top w_y$。项 $X^\\top V^{-1} y$ 就是 $X^\\top w_y$。我们称这个标量为 $s_{xy}$。二次型变为 $y^\\top w_y - s_{xy}^2 / s_{xx}$。\n\n需要通过数值优化算法（例如，Nelder-Mead 或 L-BFGS）最小化的目标函数是负 REML 对数似然：\n$$ f(\\theta_g, \\theta_e) = \\frac{1}{2} \\left[ \\log\\det(V) + \\log(s_{xx}) + (y^\\top w_y - s_{xy}^2 / s_{xx}) \\right] $$\n其中 $V$、$s_{xx}$、$s_{xy}$ 和 $w_y$ 都依赖于 $\\theta_g$ 和 $\\theta_e$。\n\n找到最小化 $f$ 的最优参数 $(\\widehat{\\theta}_g, \\widehat{\\theta}_e)$ 后，估计的方差组分为：\n$$ \\widehat{\\sigma}_g^2 = \\exp(\\widehat{\\theta}_g) \\quad \\text{和} \\quad \\widehat{\\sigma}_e^2 = \\exp(\\widehat{\\theta}_e) $$\n最后，狭义遗传力 $h^2$ 计算为可归因于加性遗传效应的总方差比例：\n$$ h^2 = \\frac{\\widehat{\\sigma}_g^2}{\\widehat{\\sigma}_g^2 + \\widehat{\\sigma}_e^2} $$\n\n实现过程将首先定义一个 Python 函数，该函数为给定的数据集 $(y, K)$ 计算 $f(\\theta_g, \\theta_e)$。然后将此函数传递给 `scipy.optimize` 库中的一个优化器。对每个测试用例重复此过程。",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import cho_factor, cho_solve\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves the heritability estimation problem for all test cases.\n    \"\"\"\n\n    # Test suite data\n    B_A = np.array([\n        [1.2, 0.0, 0.0],\n        [0.6, 0.3, 0.0],\n        [0.4, 0.8, 0.2],\n        [0.0, 0.5, 0.7],\n        [0.0, 0.2, 1.0],\n        [0.0, 0.0, 0.9]\n    ])\n    y_A = np.array([1.04, 0.61, 0.82, 0.25, -0.04, -0.20])\n\n    B_B = B_A\n    y_B = np.array([0.30, -0.70, 0.20, -0.10, 0.05, -0.30])\n    \n    B_C = np.array([\n        [1.0, 0.0],\n        [0.8, 0.2],\n        [0.6, 0.4],\n        [0.4, 0.6],\n        [0.2, 0.8]\n    ])\n    y_C = np.array([1.01, 0.79, 0.605, 0.395, 0.20])\n\n    B_D = np.array([\n        [1.0, 0.0, 0.0],\n        [0.7, 0.2, 0.0],\n        [0.5, 0.4, 0.1],\n        [0.3, 0.6, 0.2],\n        [0.1, 0.8, 0.3],\n        [0.0, 0.6, 0.5],\n        [0.0, 0.3, 0.9]\n    ])\n    y_D = np.array([0.55, 0.40, 0.45, 0.42, 0.47, 0.32, 0.33])\n\n    test_cases = [\n        (B_A, y_A),\n        (B_B, y_B),\n        (B_C, y_C),\n        (B_D, y_D),\n    ]\n\n    results = []\n    \n    for B, y in test_cases:\n        n = y.shape[0]\n        K = B @ B.T\n        X = np.ones((n, 1))\n\n        def neg_reml_log_likelihood(log_vars):\n            \"\"\"\n            Computes the negative REML log-likelihood for given log-variances.\n            Parameters are log(sigma_g^2) and log(sigma_e^2).\n            \"\"\"\n            sigma2_g = np.exp(log_vars[0])\n            sigma2_e = np.exp(log_vars[1])\n\n            # Form the covariance matrix V = sigma_g^2 * K + sigma_e^2 * I\n            V = sigma2_g * K + sigma2_e * np.identity(n)\n\n            try:\n                # Cholesky decomposition of V\n                # This is numerically stable and efficient for SPD matrices.\n                L, lower = cho_factor(V, lower=True)\n            except np.linalg.LinAlgError:\n                # V is not positive definite, return a large value\n                # to guide the optimizer away.\n                return 1e10\n            \n            # log(det(V)) = 2 * sum(log(diag(L)))\n            log_det_V = 2 * np.sum(np.log(np.diag(L)))\n\n            # Solve linear systems to avoid explicit inversion of V\n            # Find w_x = V^{-1} * X\n            w_x = cho_solve((L, lower), X)\n            # Find w_y = V^{-1} * y\n            w_y = cho_solve((L, lower), y)\n\n            # Compute terms for the REML likelihood\n            s_xx = X.T @ w_x\n            # For p=1, s_xx is a 1x1 matrix.\n            s_xx_scalar = s_xx[0, 0]\n            log_det_S_xx = np.log(s_xx_scalar)\n\n            s_xy = X.T @ w_y\n            y_T_w_y = y.T @ w_y\n            \n            # Quadratic form y^T * P * y\n            y_P_y = y_T_w_y - (s_xy[0] ** 2) / s_xx_scalar\n            \n            # The negative REML log-likelihood (to be minimized)\n            # We ignore constant terms like log(2*pi)\n            neg_ll = 0.5 * (log_det_V + log_det_S_xx + y_P_y)\n            return neg_ll\n\n        # Initial guess for the optimizer\n        # A reasonable guess is that genetic and environmental variances\n        # are each half of the total phenotypic variance.\n        var_y = np.var(y, ddof=1)\n        initial_sigma2 = var_y / 2.0\n        # If variance is zero or negative (unlikely but possible with weird data)\n        if initial_sigma2 <= 0:\n            initial_sigma2 = 1.0\n            \n        x0 = np.array([np.log(initial_sigma2), np.log(initial_sigma2)])\n\n        # Perform the optimization using a robust method like Nelder-Mead\n        opt_result = minimize(\n            neg_reml_log_likelihood, \n            x0, \n            method='Nelder-Mead',\n            options={'xatol': 1e-8, 'fatol': 1e-8}\n        )\n        \n        # Extract estimated log variances\n        opt_log_vars = opt_result.x\n        \n        # Convert back to variance components\n        opt_sigma2_g = np.exp(opt_log_vars[0])\n        opt_sigma2_e = np.exp(opt_log_vars[1])\n\n        # Compute narrow-sense heritability\n        h2 = opt_sigma2_g / (opt_sigma2_g + opt_sigma2_e)\n        \n        # Handle potential boundary case where h2 is effectively zero\n        if h2 < 1e-8:\n             h2 = 0.0\n\n        results.append(round(h2, 4))\n\n    # Format the final output as a single string\n    formatted_results = f\"[{','.join(f'{r:.4f}' for r in results)}]\"\n    print(formatted_results)\n\nsolve()\n```"
        },
        {
            "introduction": "一旦我们了解了一个性状的遗传基础，我们就可以对其选择响应进行建模。本练习将深入探讨 Lande-Arnold 框架，模拟一个群体的平均表型如何追踪一个移动的环境最适值，这是一个经典的多基因适应场景。通过推导和实现该更新方程，你将更深刻地理解加性遗传方差-协方差矩阵（$\\mathbf{G}$）如何塑造进化路径 。",
            "id": "2744372",
            "problem": "您需要编写一个完整的程序，该程序利用定向选择源于对数适应度相对于性状均值的梯度，以及连锁平衡下的标准数量遗传选择响应原理，来计算一个数量多基因性状向量均值在具有潜在移动最优值的稳定化选择下的确定性进化响应。\n\n起始基础：\n- 仅使用以下基础要素：\n  1. 马尔萨斯适应度的定义，即绝对适应度的自然对数。也就是说，如果绝对适应度是 $W(\\mathbf{z})$，那么马尔萨斯适应度就是 $m(\\mathbf{z}) = \\ln W(\\mathbf{z})$。\n  2. 选择梯度的定义，即马尔萨斯适应度相对于性状向量的梯度，即 $\\boldsymbol{\\beta}(\\mathbf{z}) = \\nabla_{\\mathbf{z}} m(\\mathbf{z})$。\n  3. 在标准数量遗传学假设（弱选择、加性效应、连锁平衡（LE）以及加性遗传方差-协方差近似恒定）下，一代内性状均值的变化与选择梯度通过加性遗传方差-协方差矩阵成正比。您必须从这些基础出发推导出明确的比例关系。\n  4. 一种广泛使用且有经验基础的、作用于多变量数量性状（性状向量为 $\\mathbf{z} \\in \\mathbb{R}^n$）的稳定化选择形式，其绝对适应度函数形式为\n     $$W(\\mathbf{z}; \\boldsymbol{\\theta}_t) = \\exp\\!\\left(-\\tfrac{1}{2}(\\mathbf{z}-\\boldsymbol{\\theta}_t)^{\\mathsf{T}} \\boldsymbol{\\Omega}^{-1} (\\mathbf{z}-\\boldsymbol{\\theta}_t)\\right),$$\n     其中 $\\boldsymbol{\\theta}_t \\in \\mathbb{R}^n$ 是第 $t$ 代的最优值，$\\boldsymbol{\\Omega} \\in \\mathbb{R}^{n \\times n}$ 是一个对称正定矩阵，其元素指定了稳定化选择的强度和相关结构。\n- 您必须使用上述要素作为基本出发点，推导出一个在 $T$ 代中均值性状向量 $\\overline{\\mathbf{z}}_t$ 的确定性离散时间更新公式，给定一个恒定的加性遗传方差-协方差矩阵 $\\mathbf{G} \\in \\mathbb{R}^{n \\times n}$，一个线性最优路径 $\\boldsymbol{\\theta}_t = \\boldsymbol{\\theta}_0 + t\\,\\mathbf{v}$（其中速度 $\\mathbf{v} \\in \\mathbb{R}^n$ 是固定的），以及初始均值 $\\overline{\\mathbf{z}}_0$。\n\n任务：\n- 通过结合上述基础要素，推导出 $\\overline{\\mathbf{z}}_{t+1}$ 关于 $\\overline{\\mathbf{z}}_t$、$\\boldsymbol{\\theta}_t$、$\\boldsymbol{\\Omega}$ 和 $\\mathbf{G}$ 的显式确定性更新公式。实现该更新并在 $T$ 代中进行迭代，以获得 $\\overline{\\mathbf{z}}_T$。\n- 您可以假设在所有测试用例中，$\\boldsymbol{\\Omega}$ 都是对称正定的，因此 $\\boldsymbol{\\Omega}^{-1}$ 存在。您不可以假设 $\\mathbf{G}$ 是可逆的。\n\n输入和测试套件规范：\n- 程序没有外部输入；请在程序中硬编码以下测试套件。对于每个测试用例 $i$，给定性状维度 $n$、初始均值 $\\overline{\\mathbf{z}}_0^{(i)}$、加性遗传方差-协方差 $\\mathbf{G}^{(i)}$、稳定化选择矩阵 $\\boldsymbol{\\Omega}^{(i)}$、初始最优值 $\\boldsymbol{\\theta}_0^{(i)}$、最优值速度 $\\mathbf{v}^{(i)}$ 以及世代数 $T^{(i)}$。对于每个测试用例，计算 $\\overline{\\mathbf{z}}_{T^{(i)}}^{(i)}$。\n\n测试用例：\n1. 一维，静态最优值：\n   - $n = 1$\n   - $\\overline{\\mathbf{z}}_0^{(1)} = (0.0)$\n   - $\\mathbf{G}^{(1)} = \\begin{bmatrix} 0.2 \\end{bmatrix}$\n   - $\\boldsymbol{\\Omega}^{(1)} = \\begin{bmatrix} 1.0 \\end{bmatrix}$\n   - $\\boldsymbol{\\theta}_0^{(1)} = (1.5)$\n   - $\\mathbf{v}^{(1)} = (0.0)$\n   - $T^{(1)} = 5$\n\n2. 二维，G 矩阵相关，静态最优值：\n   - $n = 2$\n   - $\\overline{\\mathbf{z}}_0^{(2)} = (0.0, 0.0)$\n   - $\\mathbf{G}^{(2)} = \\begin{bmatrix} 0.1 & 0.05 \\\\ 0.05 & 0.2 \\end{bmatrix}$\n   - $\\boldsymbol{\\Omega}^{(2)} = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.5 \\end{bmatrix}$\n   - $\\boldsymbol{\\theta}_0^{(2)} = (1.0, -0.5)$\n   - $\\mathbf{v}^{(2)} = (0.0, 0.0)$\n   - $T^{(2)} = 10$\n\n3. 一维，加性方差为零（无响应）：\n   - $n = 1$\n   - $\\overline{\\mathbf{z}}_0^{(3)} = (0.0)$\n   - $\\mathbf{G}^{(3)} = \\begin{bmatrix} 0.0 \\end{bmatrix}$\n   - $\\boldsymbol{\\Omega}^{(3)} = \\begin{bmatrix} 0.5 \\end{bmatrix}$\n   - $\\boldsymbol{\\theta}_0^{(3)} = (1.0)$\n   - $\\mathbf{v}^{(3)} = (0.0)$\n   - $T^{(3)} = 7$\n\n4. 二维，弱稳定化选择：\n   - $n = 2$\n   - $\\overline{\\mathbf{z}}_0^{(4)} = (0.2, -0.2)$\n   - $\\mathbf{G}^{(4)} = \\begin{bmatrix} 0.05 & 0.0 \\\\ 0.0 & 0.05 \\end{bmatrix}$\n   - $\\boldsymbol{\\Omega}^{(4)} = \\begin{bmatrix} 100.0 & 0.0 \\\\ 0.0 & 100.0 \\end{bmatrix}$\n   - $\\boldsymbol{\\theta}_0^{(4)} = (1.0, 1.0)$\n   - $\\mathbf{v}^{(4)} = (0.0, 0.0)$\n   - $T^{(4)} = 10$\n\n5. 二维，移动最优值与相关稳定化选择：\n   - $n = 2$\n   - $\\overline{\\mathbf{z}}_0^{(5)} = (0.0, 0.0)$\n   - $\\mathbf{G}^{(5)} = \\begin{bmatrix} 0.05 & 0.0 \\\\ 0.0 & 0.05 \\end{bmatrix}$\n   - $\\boldsymbol{\\Omega}^{(5)} = \\begin{bmatrix} 1.0 & 0.2 \\\\ 0.2 & 1.0 \\end{bmatrix}$\n   - $\\boldsymbol{\\theta}_0^{(5)} = (0.0, 0.0)$\n   - $\\mathbf{v}^{(5)} = (0.1, -0.05)$\n   - $T^{(5)} = 20$\n\n输出要求：\n- 对于每个测试用例 $i$，计算 $\\overline{\\mathbf{z}}_{T^{(i)}}^{(i)}$。\n- 将每个结果向量的每个分量四舍五入到恰好 $6$ 位小数。\n- 您的程序应生成单行输出，其中包含结果，形式为一个以逗号分隔的向量列表，每个向量用方括号括起来，整个集合也用方括号括起来。例如，格式应类似于 $[\\,[a_{1,1},a_{1,2}],\\,[a_{2,1},a_{2,2}]\\,]$，但使用实际数字而非符号，且无空格。\n- 不涉及物理单位。\n\n科学真实性约束：\n- 假设在模拟时间内，选择是弱的，效应是加性的，$\\mathbf{G}$ 近似恒定，且存在连锁平衡（LE）。确保在所有计算中 $\\boldsymbol{\\Omega}$ 都是对称正定的，以便 $\\boldsymbol{\\Omega}^{-1}$ 存在。\n\n您的程序必须是自包含的，且不得读取任何外部输入。",
            "solution": "该问题是在进化数量遗传学领域中一个提法恰当且有科学依据的练习。它要求推导并实现一个离散时间模型，用于描述在具有线性移动最优值的稳定化选择下，多变量多基因性状的演化。问题陈述提供了所有必要的组成部分，并遵循了该领域的既定原则，因此被认为是有效的。\n\n目标是推导均值性状向量 $\\overline{\\mathbf{z}}_t$ 的确定性离散时间更新方程，并对该方程进行指定代数 $T$ 的迭代，以求得最终的均值性状向量 $\\overline{\\mathbf{z}}_T$。推导将从所提供的基础要素开始。\n\n首先，给定在第 $t$ 代，性状向量为 $\\mathbf{z} \\in \\mathbb{R}^n$ 的个体的绝对适应度函数：\n$$W(\\mathbf{z}; \\boldsymbol{\\theta}_t) = \\exp\\!\\left(-\\tfrac{1}{2}(\\mathbf{z}-\\boldsymbol{\\theta}_t)^{\\mathsf{T}} \\boldsymbol{\\Omega}^{-1} (\\mathbf{z}-\\boldsymbol{\\theta}_t)\\right)$$\n这里，$\\boldsymbol{\\theta}_t$ 是第 $t$ 代的最优性状向量，$\\boldsymbol{\\Omega}$ 是一个对称正定矩阵，描述了稳定化选择的强度。越大的 $\\boldsymbol{\\Omega}$ 对应越弱的选择。\n\n马尔萨斯适应度 $m(\\mathbf{z})$ 是绝对适应度 $W(\\mathbf{z})$ 的自然对数。\n$$m(\\mathbf{z}; \\boldsymbol{\\theta}_t) = \\ln W(\\mathbf{z}; \\boldsymbol{\\theta}_t) = -\\frac{1}{2}(\\mathbf{z}-\\boldsymbol{\\theta}_t)^{\\mathsf{T}} \\boldsymbol{\\Omega}^{-1} (\\mathbf{z}-\\boldsymbol{\\theta}_t)$$\n这个函数是一个二次曲面，这简化了后续的分析。\n\n接下来，我们定义选择梯度，它衡量了作用于性状上的定向选择力。马尔萨斯适应度相对于性状向量 $\\mathbf{z}$ 的梯度是：\n$$\\boldsymbol{\\beta}(\\mathbf{z}) = \\nabla_{\\mathbf{z}} m(\\mathbf{z}; \\boldsymbol{\\theta}_t)$$\n对于对称矩阵 $\\mathbf{A}$，使用对二次型求导的标准法则 $\\nabla_{\\mathbf{x}} (\\mathbf{x}-\\mathbf{c})^{\\mathsf{T}}\\mathbf{A}(\\mathbf{x}-\\mathbf{c}) = 2\\mathbf{A}(\\mathbf{x}-\\mathbf{c})$，并注意到因为 $\\boldsymbol{\\Omega}$ 是对称的，所以 $\\boldsymbol{\\Omega}^{-1}$ 也是对称的，我们得到：\n$$\\boldsymbol{\\beta}(\\mathbf{z}) = -\\frac{1}{2} \\cdot 2\\boldsymbol{\\Omega}^{-1}(\\mathbf{z}-\\boldsymbol{\\theta}_t) = -\\boldsymbol{\\Omega}^{-1}(\\mathbf{z}-\\boldsymbol{\\theta}_t)$$\n\n群体均值性状的进化响应 $\\Delta \\overline{\\mathbf{z}}_t = \\overline{\\mathbf{z}}_{t+1} - \\overline{\\mathbf{z}}_t$ 由多变量育种家方程决定。这个数量遗传学的基本结果表明，均值的变化是加性遗传方差-协方差矩阵 $\\mathbf{G}$ 与作用于群体均值的选择梯度 $\\boldsymbol{\\beta}_t$ 的乘积：\n$$\\Delta \\overline{\\mathbf{z}}_t = \\mathbf{G} \\boldsymbol{\\beta}_t$$\n均值上的选择梯度 $\\boldsymbol{\\beta}_t$ 在形式上是均值对数适应度相对于均值性状的梯度，即 $\\boldsymbol{\\beta}_t = \\nabla_{\\overline{\\mathbf{z}}} \\overline{m(\\mathbf{z})}$。在表型围绕均值 $\\overline{\\mathbf{z}}_t$ 对称分布（例如，多维正态分布）的标准假设下，对于给定的二次适应度曲面，这个群体层面的梯度恰好是在群体均值性状处评估的个体层面梯度：\n$$\\boldsymbol{\\beta}_t = \\boldsymbol{\\beta}(\\overline{\\mathbf{z}}_t) = -\\boldsymbol{\\Omega}^{-1}(\\overline{\\mathbf{z}}_t - \\boldsymbol{\\theta}_t)$$\n这个近似是 Lande-Arnold 框架的核心，并且由问题结构所隐含要求，该结构省略了完整的表型协方差矩阵。\n\n通过将此 $\\boldsymbol{\\beta}_t$ 的表达式代入育种家方程，我们得到一代内均值性状向量的变化：\n$$\\Delta \\overline{\\mathbf{z}}_t = \\mathbf{G} \\left( -\\boldsymbol{\\Omega}^{-1}(\\overline{\\mathbf{z}}_t - \\boldsymbol{\\theta}_t) \\right) = -\\mathbf{G} \\boldsymbol{\\Omega}^{-1}(\\overline{\\mathbf{z}}_t - \\boldsymbol{\\theta}_t)$$\n\n由此，我们推导出从第 $t$ 代到第 $t+1$ 代的均值性状向量的显式确定性更新规则：\n$$\\overline{\\mathbf{z}}_{t+1} = \\overline{\\mathbf{z}}_t + \\Delta \\overline{\\mathbf{z}}_t = \\overline{\\mathbf{z}}_t - \\mathbf{G} \\boldsymbol{\\Omega}^{-1}(\\overline{\\mathbf{z}}_t - \\boldsymbol{\\theta}_t)$$\n\n问题指定了一个线性变化的最优值：\n$$\\boldsymbol{\\theta}_t = \\boldsymbol{\\theta}_0 + t\\,\\mathbf{v}$$\n其中 $\\boldsymbol{\\theta}_0$ 是初始最优值，$\\mathbf{v}$ 是最优值移动的恒定速度，而 $t$ 是代数，从 $t=0$ 开始。\n\n完整的迭代算法如下：\n1. 初始化第 $t=0$ 代的均值性状向量：$\\overline{\\mathbf{z}} \\leftarrow \\overline{\\mathbf{z}}_0$。\n2. 对于从 $0$ 到 $T-1$ 的每一代 $t$：\n   a. 计算当前最优值：$\\boldsymbol{\\theta}_t = \\boldsymbol{\\theta}_0 + t\\,\\mathbf{v}$。\n   b. 计算均值与最优值之间的差异向量：$\\mathbf{d}_t = \\overline{\\mathbf{z}}_t - \\boldsymbol{\\theta}_t$。\n   c. 计算进化响应：$\\Delta \\overline{\\mathbf{z}}_t = - \\mathbf{G} \\boldsymbol{\\Omega}^{-1} \\mathbf{d}_t$。\n   d. 更新均值性状向量：$\\overline{\\mathbf{z}}_{t+1} = \\overline{\\mathbf{z}}_t + \\Delta \\overline{\\mathbf{z}}_t$。\n3. 经过 $T$ 代后的最终结果是向量 $\\overline{\\mathbf{z}}_T$。\n\n这个过程将为提供的每个测试用例实现。实现涉及基本的矩阵-向量运算：对 $\\boldsymbol{\\Omega}$ 进行矩阵求逆，以及矩阵-向量乘法。问题保证了 $\\boldsymbol{\\Omega}$ 是可逆的。加性遗传矩阵 $\\mathbf{G}$ 可能是奇异的，这在推导出的方程中得到了正确处理，因为不需要对 $\\mathbf{G}$ 求逆。例如，如果 $\\mathbf{G}$ 是零矩阵，则 $\\Delta \\overline{\\mathbf{z}}_t$ 为零，群体均值不发生演化，这与预期相符。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the deterministic evolutionary response of a quantitative polygenic\n    trait vector under stabilizing selection with a potentially moving optimum.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: 1D, stationary optimum\n        {\n            \"n\": 1,\n            \"z0\": np.array([0.0]),\n            \"G\": np.array([[0.2]]),\n            \"Omega\": np.array([[1.0]]),\n            \"theta0\": np.array([1.5]),\n            \"v\": np.array([0.0]),\n            \"T\": 5\n        },\n        # Case 2: 2D, correlated G, stationary optimum\n        {\n            \"n\": 2,\n            \"z0\": np.array([0.0, 0.0]),\n            \"G\": np.array([[0.1, 0.05], [0.05, 0.2]]),\n            \"Omega\": np.array([[1.0, 0.0], [0.0, 1.5]]),\n            \"theta0\": np.array([1.0, -0.5]),\n            \"v\": np.array([0.0, 0.0]),\n            \"T\": 10\n        },\n        # Case 3: 1D, zero additive variance\n        {\n            \"n\": 1,\n            \"z0\": np.array([0.0]),\n            \"G\": np.array([[0.0]]),\n            \"Omega\": np.array([[0.5]]),\n            \"theta0\": np.array([1.0]),\n            \"v\": np.array([0.0]),\n            \"T\": 7\n        },\n        # Case 4: 2D, weak stabilizing selection\n        {\n            \"n\": 2,\n            \"z0\": np.array([0.2, -0.2]),\n            \"G\": np.array([[0.05, 0.0], [0.0, 0.05]]),\n            \"Omega\": np.array([[100.0, 0.0], [0.0, 100.0]]),\n            \"theta0\": np.array([1.0, 1.0]),\n            \"v\": np.array([0.0, 0.0]),\n            \"T\": 10\n        },\n        # Case 5: 2D, moving optimum, correlational selection\n        {\n            \"n\": 2,\n            \"z0\": np.array([0.0, 0.0]),\n            \"G\": np.array([[0.05, 0.0], [0.0, 0.05]]),\n            \"Omega\": np.array([[1.0, 0.2], [0.2, 1.0]]),\n            \"theta0\": np.array([0.0, 0.0]),\n            \"v\": np.array([0.1, -0.05]),\n            \"T\": 20\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        # Extract parameters for the current test case\n        z_mean = case[\"z0\"]\n        G = case[\"G\"]\n        Omega = case[\"Omega\"]\n        theta0 = case[\"theta0\"]\n        v = case[\"v\"]\n        T = case[\"T\"]\n        \n        # Pre-compute the inverse of Omega and the product G * Omega_inv\n        # The problem statement guarantees Omega is invertible.\n        Omega_inv = np.linalg.inv(Omega)\n        G_Omega_inv = G @ Omega_inv\n\n        # Iterate the discrete-time update equation for T generations\n        for t in range(T):\n            # Calculate the optimum at the current generation t\n            theta_t = theta0 + t * v\n            \n            # Calculate the evolutionary response delta_z\n            # delta_z = -G * Omega_inv * (z_mean - theta_t)\n            delta_z = -G_Omega_inv @ (z_mean - theta_t)\n            \n            # Update the mean trait vector\n            z_mean = z_mean + delta_z\n            \n        # Round the final vector components to 6 decimal places\n        z_T = np.round(z_mean, 6)\n        \n        # Format the result vector as a string '[c1,c2,...]'\n        result_str = f\"[{','.join([f'{val:.6f}' for val in z_T])}]\"\n        results.append(result_str)\n\n    # Final print statement in the exact required format: [[...],[...],...]\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "我们的预测模型常常假设加性遗传方差（$V_A$）是恒定的，但选择本身就能改变它。本练习将探讨“布尔默效应”——一个选择诱导的连锁不平衡暂时性降低加性遗传方差的现象。通过对这一过程进行建模，你将体会到选择与性状遗传结构之间的动态反馈，这是理解长期进化预测的一个关键概念 。",
            "id": "2744369",
            "problem": "你的任务是，在一个数量性状的无穷小模型下，对由选择引起的连锁不平衡（linkage disequilibrium, LD）所导致的加性遗传方差的减少——即Bulmer效应——进行形式化描述和计算。请在不重叠世代的框架下，根据以下假设进行推导和算法设计，这些假设必须作为你推导和算法的基础：\n\n- 表型 $P$ 是加性育种值 $G$ 和独立的环境离差 $E$ 的和，即 $P = G + E$，其中 $\\mathrm{Var}(G) = V_A$ 和 $\\mathrm{Var}(E) = V_E$。假设通过中心化处理，表型 $P$ 的初始种群均值为 $0$。\n- 种群无限大，随机交配，基因座非连锁（自由重组），无显性或上位性，且无突变。无穷小模型成立：$G$ 近似服从正态分布，每个基因座的等位基因频率变化可忽略不计，因此基因方差 $V_g$（即LD为 $0$ 时的加性方差）可视为在各世代中保持不变。\n- 选择是对表型 $P$ 进行的截尾选择，每代保留顶端的 $\\pi$ 比例。对于正态分布的性状，截断正态随机变量的条件矩是经过充分检验的：若 $Z \\sim \\mathcal{N}(0,1)$ 且我们设定条件 $Z > z_\\pi$（其中 $\\pi = 1 - \\Phi(z_\\pi)$），则 $\\mathbb{E}[Z \\mid Z > z_\\pi] = \\lambda$ 且 $\\mathrm{Var}(Z \\mid Z > z_\\pi) = 1 + z_\\pi \\lambda - \\lambda^2$，其中 $\\lambda = \\phi(z_\\pi)/\\pi$，$\\phi$ 和 $\\Phi$ 分别为标准正态概率密度函数和累积分布函数。\n\n你的目标：\n\n- 从第一性原理以及全期望定律和全方差定律出发，推导被选父母本中加性方差 $V_A^{(S)}$ 的表达式，该表达式应为 $V_A$、$V_E$ 和 $\\pi$ 的函数。你的推导必须从 $G$ 和 $P$ 之间的二元正态关系开始，并且除了已给出的关于正态分布截断的统计事实外，不得假定任何目标公式。\n- 在非连锁极限下，利用随机交配和自由重组，用基因方差 $V_g$ 和被选父母本的加性方差 $V_A^{(S)}$ 来表达子代的加性方差 $V_A^{\\text{next}}$。根据加性方差可分解为基因方差加上由LD贡献的部分，以及重组对LD的影响，论证当选择引起非零LD时，为什么 $V_A^{\\text{next}}$ 严格介于 $V_g$ 和 $V_A^{(S)}$ 之间。\n- 设计并实现一个算法，在给定 $V_A(0)$、$V_E$、$\\pi$ 和一个整数代数 $T$ 的情况下，通过迭代方差递推式来计算比率 $\\rho(T) = V_A(T)/V_g$。\n\n科学真实性约束：\n\n- 在无穷小模型下，将 $V_g$ 视为常数且等于 $V_A(0)$。\n- 每代使用上文所述的精确截断正态矩来计算被选表型的方差。\n- 确保你的实现能够处理 $\\pi = 1$（无选择）的边界情况。在这种情况下，对所有 $T$，正确的极限是 $\\rho(T) = 1$。\n\n输入不是由用户读取。相反，你的程序必须运行硬编码在程序中的以下测试套件，并按规定格式输出结果：\n\n测试套件（每个测试用例是一个元组 $(V_A(0), V_E, \\pi, T)$）：\n\n- 案例A（中等选择强度，中等遗传力）：$(1.0, 1.0, 0.1, 5)$。\n- 案例B（无环境方差，强遗传力）：$(1.0, 0.0, 0.2, 10)$。\n- 案例C（无选择边界）：$(0.5, 1.5, 1.0, 50)$。\n- 案例D（极强选择，低遗传力，接近平衡）：$(1.0, 3.0, 0.01, 100)$。\n\n所有实数必须以无量纲形式处理；不涉及物理单位。你的程序应为每个案例计算单个浮点值 $\\rho(T) = V_A(T)/V_g$，并将其四舍五入到 $6$ 位小数。不使用角度。百分比必须表示为小数；参数 $\\pi$ 已按此形式提供。\n\n最终输出格式：\n\n- 你的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表，顺序与测试套件相同，例如 $[\\rho_A,\\rho_B,\\rho_C,\\rho_D]$，每个条目四舍五入到 $6$ 位小数。",
            "solution": "问题陈述是在数量遗传学的无穷小模型下对Bulmer效应的一个定义明确且有科学依据的表述。它为获得唯一解提供了完整的假设和参数集。该问题是有效的，其解的推导和实现如下。\n\n问题的核心在于对加性遗传方差 $V_A$ 在不重叠世代间进行递推迭代。这需要推导 $V_A$ 在一代内的变化，该过程包含两个步骤：选择对亲本代的影响，以及随机交配和重组对子代的影响。\n\n首先，我们推导被选父母本中的加性遗传方差 $V_A^{(S)}$。\n表型 $P$ 是育种值 $G$ 和独立的环境离差 $E$ 的和，因此 $P = G + E$。给定 $G \\sim \\mathcal{N}(0, V_A)$ 且 $E \\sim \\mathcal{N}(0, V_E)$。它们的独立性意味着 $(G, P)$ 的联合分布是一个二元正态分布，其均值向量为 $(0, 0)$，协方差矩阵为：\n$$\n\\boldsymbol{\\Sigma} = \\begin{pmatrix} \\mathrm{Var}(G) & \\mathrm{Cov}(G, P) \\\\ \\mathrm{Cov(G, P)} & \\mathrm{Var}(P) \\end{pmatrix} = \\begin{pmatrix} V_A & V_A \\\\ V_A & V_A + V_E \\end{pmatrix}\n$$\n其中 $\\mathrm{Cov}(G, P) = \\mathrm{Cov}(G, G+E) = \\mathrm{Var}(G) + \\mathrm{Cov}(G, E) = V_A$。总的表型方差为 $V_P = V_A + V_E$。\n\n在给定特定表型值 $P=p$ 的条件下，$G$ 的条件分布是正态的。二元正态分布的标准结果给出了条件均值和方差：\n$$\n\\mathbb{E}[G \\mid P=p] = \\frac{\\mathrm{Cov}(G, P)}{\\mathrm{Var}(P)} p = \\frac{V_A}{V_P} p = h^2 p\n$$\n$$\n\\mathrm{Var}(G \\mid P=p) = \\mathrm{Var}(G) - \\frac{\\mathrm{Cov}(G, P)^2}{\\mathrm{Var}(P)} = V_A - \\frac{V_A^2}{V_P} = V_A \\left(1 - \\frac{V_A}{V_P}\\right) = V_A (1 - h^2)\n$$\n其中 $h^2 = V_A / V_P$ 是狭义遗传力。注意，这个条件方差与 $p$ 的值无关。\n\n我们寻求被选个体中 $G$ 的方差，这些个体的表型 $P$ 超过了截断阈值 $P_t$，即 $V_A^{(S)} = \\mathrm{Var}(G \\mid P > P_t)$。我们应用全方差定律：\n$$\nV_A^{(S)} = \\mathrm{Var}(G \\mid P > P_t) = \\mathbb{E}[\\mathrm{Var}(G \\mid P) \\mid P > P_t] + \\mathrm{Var}(\\mathbb{E}[G \\mid P] \\mid P > P_t)\n$$\n第一项是一个常数的期望：\n$$\n\\mathbb{E}[\\mathrm{Var}(G \\mid P) \\mid P > P_t] = \\mathbb{E}[V_A(1 - h^2) \\mid P > P_t] = V_A(1 - h^2)\n$$\n第二项是条件期望的方差：\n$$\n\\mathrm{Var}(\\mathbb{E}[G \\mid P] \\mid P > P_t) = \\mathrm{Var}(h^2 P \\mid P > P_t) = (h^2)^2 \\mathrm{Var}(P \\mid P > P_t) = (h^2)^2 V_P^{(S)}\n$$\n其中 $V_P^{(S)}$ 是被选群体的表型方差。为了求得 $V_P^{(S)}$，我们将 $P$ 标准化为 $Z = P / \\sqrt{V_P}$，其中 $Z \\sim \\mathcal{N}(0, 1)$。选择条件 $P > P_t$ 变为 $Z > z_\\pi$，其中 $z_\\pi = P_t / \\sqrt{V_P}$。问题给出了截断标准正态变量的方差：$\\mathrm{Var}(Z \\mid Z > z_\\pi) = 1 + z_\\pi \\lambda - \\lambda^2$，其中 $\\lambda = \\phi(z_\\pi)/\\pi$。\n因此，截断表型的方差为 $V_P^{(S)} = \\mathrm{Var}(\\sqrt{V_P} Z \\mid Z > z_\\pi) = V_P \\mathrm{Var}(Z \\mid Z > z_\\pi) = V_P (1 + z_\\pi \\lambda - \\lambda^2)$。代入回去，我们得到：\n$$\nV_A^{(S)} = V_A(1 - h^2) + (h^2)^2 V_P (1 + z_\\pi \\lambda - \\lambda^2)\n$$\n替换 $h^2 = V_A/V_P$ 并化简：\n$$\nV_A^{(S)} = V_A\\left(1 - \\frac{V_A}{V_P}\\right) + \\left(\\frac{V_A}{V_P}\\right)^2 V_P (1 + z_\\pi \\lambda - \\lambda^2) = V_A - \\frac{V_A^2}{V_P} + \\frac{V_A^2}{V_P}(1 + z_\\pi \\lambda - \\lambda^2)\n$$\n$$\nV_A^{(S)} = V_A + \\frac{V_A^2}{V_P}(z_\\pi \\lambda - \\lambda^2) = V_A\\left(1 - \\frac{V_A}{V_P}(\\lambda^2 - z_\\pi \\lambda)\\right)\n$$\n让我们定义选择强度系数 $k = \\lambda(\\lambda - z_\\pi)$。该表达式简化为众所周知的结论：\n$$\nV_A^{(S)} = V_A(1 - k h^2)\n$$\n对于任何非平凡的选择（$0 < \\pi < 1$），可以证明 $k>0$，这意味着 $V_A^{(S)} < V_A$。选择通过诱导负向连锁不平衡来降低加性遗传方差。\n\n其次，我们推导下一代的加性方差 $V_A^{\\text{next}}$。被选的父母本随机交配。子代的育种值是其父母育种值的平均值，加上一个孟德尔抽样项 $\\delta$：$G_{\\text{offspring}} = \\frac{1}{2}(G_{\\text{father}} + G_{\\text{mother}}) + \\delta$。\n方差为 $V_A^{\\text{next}} = \\mathrm{Var}(G_{\\text{offspring}}) = \\mathrm{Var}(\\frac{1}{2}(G_{\\text{father}} + G_{\\text{mother}})) + \\mathrm{Var}(\\delta)$。由于父母是从被选群体中随机选择的，$\\mathrm{Var}(G_{\\text{father}}) = \\mathrm{Var}(G_{\\text{mother}}) = V_A^{(S)}$。它们的协方差为 $0$。\n$$\nV_A^{\\text{next}} = \\frac{1}{4}(\\mathrm{Var}(G_{\\text{father}}) + \\mathrm{Var}(G_{\\text{mother}})) + \\mathrm{Var}(\\delta) = \\frac{1}{2}V_A^{(S)} + \\mathrm{Var}(\\delta)\n$$\n孟德尔抽样方差 $\\mathrm{Var}(\\delta)$ 源于等位基因的分离。在自由重组（非连锁基因座）的情况下，由选择引起的所有连锁不平衡在一代内消散。分离产生的方差取决于潜在的等位基因频率，而在无穷小模型下，这些频率不发生改变。该方差等于基因方差的一半，即 $\\mathrm{Var}(\\delta) = \\frac{1}{2}V_g$。基因方差 $V_g$ 是当群体处于连锁平衡时存在的加性方差；根据问题的假设，它是一个常数，且等于 $V_A(0)$。\n这就得到了最终的递推关系式：\n$$\nV_A^{\\text{next}} = \\frac{1}{2}V_A^{(S)} + \\frac{1}{2}V_g\n$$\n这个表达式表明，$V_A^{\\text{next}}$ 是选择后方差 $V_A^{(S)}$ 和恒定的基因方差 $V_g$ 的算术平均值。因此，对于任何 $V_A^{(S)} \\neq V_g$ 的情况（对于任何非平凡的选择都成立），$V_A^{\\text{next}}$ 必然严格介于 $V_A^{(S)}$ 和 $V_g$ 之间。选择将 $V_A$ 降低至一个低于 $V_g$ 的平衡值，而重组则通过打破负向连锁不平衡部分地恢复了它。\n\n计算算法如下：\n1.  初始化 $V_A = V_A(0)$ 和 $V_g = V_A(0)$。\n2.  如果 $\\pi=1$，则无选择，$k=0$，$V_A^{(S)}=V_A$。递推式变为 $V_A^{\\text{next}} = \\frac{1}{2}V_A + \\frac{1}{2}V_g$。由于 $V_A(0)=V_g$，因此 $V_A$ 在所有世代中都保持等于 $V_g$。所以 $\\rho(T)=1$。\n3.  如果 $\\pi<1$，计算常数 $z_\\pi$、$\\lambda$ 和 $k$。\n4.  迭代 $T$ 代。在每一代 $t$ 中：\n    a. 计算 $V_P(t) = V_A(t) + V_E$ 和 $h^2(t) = V_A(t) / V_P(t)$。\n    b. 计算 $V_A^{(S)}(t) = V_A(t)(1 - k \\cdot h^2(t))$。\n    c. 计算 $V_A(t+1) = \\frac{1}{2}V_A^{(S)}(t) + \\frac{1}{2}V_g$。\n5.  经过 $T$ 次迭代后，计算最终比率 $\\rho(T) = V_A(T) / V_g$。\n该程序被实现以解决给定的测试案例。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the reduction of additive genetic variance due to the Bulmer effect.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A (moderate selection, intermediate heritability)\n        (1.0, 1.0, 0.1, 5),\n        # Case B (no environmental variance, strong heritability)\n        (1.0, 0.0, 0.2, 10),\n        # Case C (no selection boundary)\n        (0.5, 1.5, 1.0, 50),\n        # Case D (very strong selection, low heritability, near-equilibrium)\n        (1.0, 3.0, 0.01, 100),\n    ]\n\n    results = []\n    for case in test_cases:\n        Va0, Ve, pi, T = case\n        \n        # Under the infinitesimal model, genic variance is constant and equals\n        # the initial additive variance before selection begins.\n        Vg = Va0\n        \n        # Handle the boundary case of no selection (pi = 1).\n        if pi == 1.0:\n            # With no selection, V_A remains at V_g.\n            # The recursion V_A(t+1) = 0.5 * V_A(t) + 0.5 * V_g with V_A(0) = V_g\n            # yields V_A(t) = V_g for all t.\n            # Therefore, the ratio V_A(T) / V_g is always 1.\n            rho_T = 1.0\n            results.append(f\"{rho_T:.6f}\")\n            continue\n\n        # Pre-calculate selection-related constants that do not change over generations.\n        # z_pi is the truncation point on a standard normal distribution.\n        z_pi = norm.ppf(1.0 - pi)\n        # lambda_val is the mean of the truncated standard normal distribution.\n        # Note: 'lambda' is a reserved keyword in Python.\n        lambda_val = norm.pdf(z_pi) / pi\n        # k is the selection intensity coefficient related to variance reduction.\n        k = lambda_val * (lambda_val - z_pi)\n        \n        # Initialize the additive genetic variance for the first generation.\n        Va_t = Va0\n        \n        # Iterate the recursion for T generations.\n        for _ in range(T):\n            # Phenotypic variance in the current generation.\n            Vp_t = Va_t + Ve\n            \n            # Heritability in the current generation.\n            # We assume Vp_t > 0, which is true for the given test cases.\n            h2_t = Va_t / Vp_t if Vp_t > 0 else 0.0\n            \n            # Additive variance among selected parents (after selection).\n            Va_s = Va_t * (1.0 - k * h2_t)\n            \n            # Additive variance in the next generation (Bulmer's recursion).\n            # This accounts for recombination breaking down linkage disequilibrium.\n            Va_next = 0.5 * Va_s + 0.5 * Vg\n            \n            # Update the variance for the next iteration.\n            Va_t = Va_next\n            \n        # The final ratio is rho(T) = V_A(T) / V_g.\n        rho_T = Va_t / Vg\n        results.append(f\"{rho_T:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}