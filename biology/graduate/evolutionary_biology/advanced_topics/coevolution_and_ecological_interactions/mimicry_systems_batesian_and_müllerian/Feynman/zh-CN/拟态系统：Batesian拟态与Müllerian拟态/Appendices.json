{
    "hands_on_practices": [
        {
            "introduction": "我们的实践之旅始于一个基础但至关重要的练习，它利用复制子动态（replicator dynamics）来模拟贝氏拟态（Batesian mimicry）系统中无防御拟态者频率的演化。通过这个练习，你将亲手推导决定拟态者是有利还是有害的演化方程，并分析不同参数条件下种群的最终命运。这项实践旨在深化你对频率依赖性选择核心机制的理解，即一个性状的适应性价值如何随着其在种群中的普遍程度而变化。",
            "id": "2734496",
            "problem": "警戒色的猎物通常会形成警戒信号复合体，其中有防御的“模型”有时会伴随着无防御的拟态者。考虑一个均匀混合的捕食者-猎物环境，其中有两种警戒信号携带者：有防御的模型（$T$ 型）和无防御的拟态者（$M$ 型）。令 $q \\in [0,1]$ 表示信号携带者中拟态者的频率。捕食者攻击警戒信号携带者的概率为 $A(q)$，该概率取决于信号的可靠性；假设其为线性形式 $A(q) = a_0 + a_1 q$，其中 $0 \\leq a_0 \\leq 1$，且 $a_1$ 的取值使得对所有 $q \\in [0,1]$ 都有 $0 \\leq A(q) \\leq 1$。$a_1$ 的符号代表了生态学机制：$a_1 > 0$ 对应于 Batesian 条件（更多的拟态者会稀释信号，增加对警戒信号的攻击），而 $a_1  0$ 对应于类 Müllerian 条件（更多诚实信号的发送者会减少对警戒信号的攻击）。假设模型因产生毒素而支付一个固定成本 $s \\in [0,1]$，这会独立于攻击而降低其基线适应度，而拟态者不支付此成本，但在遭受攻击时会完全死亡。\n\n设绝对适应度如下：拟态者的适应度为 $W_m(q) = 1 - A(q)$，模型的适应度为 $W_t = 1 - s$。令平均适应度为 $\\bar{W}(q) = q W_m(q) + (1 - q) W_t$。频率 $q$ 根据复制子动态常微分方程（ODE）$\\dot{q} = q \\big(W_m(q) - \\bar{W}(q)\\big)$ 演化。\n\n从这些假设和复制子动态常微分方程的定义出发，推导关于 $q$ 的一维选择梯度，并分析所有平衡点的局部稳定性。确定参数 $a_0$、$a_1$ 和 $s$ 在何种条件下，选择会驱动拟态者达到低频率（边界灭绝）、固定，或达到一个稳定的多态频率。最后，报告内部平衡频率 $q^{\\ast}$（如果存在）作为 $a_0$、$a_1$ 和 $s$ 的函数的闭式解析表达式。最终答案必须是 $q^{\\ast}$ 的单个闭式表达式。不需要四舍五入，也不需要单位，因为所有量都是无量纲的概率或频率。",
            "solution": "所提出的问题是数理演化生物学中的一个标准练习。该问题是自洽的，其科学基础是频率依赖性选择和复制子动态的原理，并且问题的陈述足够清晰和严谨，可以得出一个唯一的、有意义的解。因此，该问题被认为是有效的。我们开始进行推导。\n\n拟态者频率 $q$ 的动态由复制子方程控制：\n$$ \\dot{q} = q \\big(W_m(q) - \\bar{W}(q)\\big) $$\n其中 $W_m(q)$ 是拟态者的适应度，$\\bar{W}(q)$ 是信号携带者群体的平均适应度。平均适应度定义为 $\\bar{W}(q) = q W_m(q) + (1 - q) W_t$，其中 $W_t$ 是有防御模型的适应度。\n\n首先，我们展开选择梯度项 $W_m(q) - \\bar{W}(q)$：\n$$ W_m(q) - \\bar{W}(q) = W_m(q) - \\big(q W_m(q) + (1 - q) W_t\\big) = (1 - q) W_m(q) - (1 - q) W_t = (1 - q) \\big(W_m(q) - W_t\\big) $$\n将此代入复制子方程，得到一个更清晰的形式：\n$$ \\dot{q} = q(1 - q) \\big(W_m(q) - W_t\\big) $$\n项 $S(q) = W_m(q) - W_t$ 表示在当前种群构成 $q$ 的情况下，拟态者相对于模型的适应度优势。这就是驱动 $q \\in (0,1)$ 频率变化的一维选择梯度。\n\n接下来，我们将指定的适应度函数代入 $S(q)$ 的表达式中。拟态者的适应度为 $W_m(q) = 1 - A(q)$，其中攻击概率为 $A(q) = a_0 + a_1 q$。模型的适应度为 $W_t = 1 - s$。\n$$ S(q) = W_m(q) - W_t = \\big(1 - (a_0 + a_1 q)\\big) - (1 - s) = 1 - a_0 - a_1 q - 1 + s = s - a_0 - a_1 q $$\n因此，关于拟态者频率的完整常微分方程是：\n$$ \\dot{q} = q(1 - q)(s - a_0 - a_1 q) $$\n该系统的平衡点，记为 $q^{\\ast}$，是使 $\\dot{q} = 0$ 的 $q$ 值。从该方程中，我们可以确定三个潜在的平衡点：\n1.  边界平衡点 $q^{\\ast} = 0$，对应于拟态者的灭绝。\n2.  边界平衡点 $q^{\\ast} = 1$，对应于拟态者的固定。\n3.  任何内部平衡点 $q^{\\ast} \\in (0,1)$，它是线性项 $s - a_0 - a_1 q^{\\ast} = 0$ 的一个根。\n\n主要任务是找到这个内部平衡点 $q^{\\ast}$ 的闭式表达式。我们求解方程 $s - a_0 - a_1 q^{\\ast} = 0$ 以得到 $q^{\\ast}$：\n$$ a_1 q^{\\ast} = s - a_0 $$\n假设 $a_1 \\neq 0$，我们可以分离出 $q^{\\ast}$：\n$$ q^{\\ast} = \\frac{s - a_0}{a_1} $$\n这就是内部平衡频率的解析表达式。\n\n为求完整，我们分析该平衡点的稳定性和存在条件。令 $f(q) = \\dot{q}$。如果 $f'(q^{\\ast})  0$，则平衡点 $q^{\\ast}$ 是局部稳定的；如果 $f'(q^{\\ast})  0$，则是不稳定的。\n其导数为 $f'(q) = (1-2q)(s - a_0 - a_1 q) - a_1 q(1 - q)$。\n在内部平衡点 $q^{\\ast} = \\frac{s - a_0}{a_1}$ 处，根据定义，项 $(s - a_0 - a_1 q^{\\ast})$ 为零。因此，导数简化为：\n$$ f'(q^{\\ast}) = - a_1 q^{\\ast}(1 - q^{\\ast}) $$\n要使 $q^{\\ast}$ 成为一个多态平衡点，它必须位于区间 $(0,1)$ 内，这意味着 $q^{\\ast}(1 - q^{\\ast})  0$。因此，内部平衡点的稳定性仅取决于 $a_1$ 的符号：\n-   如果 $a_1 > 0$（Batesian 拟态），则 $f'(q^{\\ast})  0$，内部平衡点是**稳定的**。这对应于负频率依赖性选择，即拟态者的适应度随其频率增加而降低。如果拟态者在稀有时具有优势（$W_m(0)  W_t \\implies s  a_0$），但在常见时处于劣势（$W_m(1)  W_t \\implies s  a_0 + a_1$），则稳定的多态性是可能的。这些条件，$a_0  s  a_0 + a_1$，确保了 $0  q^{\\ast}  1$。\n-   如果 $a_1  0$（类 Müllerian 条件），则 $f'(q^{\\ast})  0$，内部平衡点是**不稳定的**。这对应于正频率依赖性选择，即拟态者的适应度随其频率的增加而增加。这样的系统会导致双稳态，种群会收敛到拟态者灭绝（$q=0$）或固定（$q=1$），具体取决于初始频率相对于不稳定平衡点 $q^{\\ast}$ 的位置。这个内部平衡点存在于 $(0,1)$ 内的条件是 $a_0+a_1  s  a_0$。\n\n该特定问题只要求内部平衡频率 $q^{\\ast}$ 的闭式解析表达式。该表达式已在上面推导出。",
            "answer": "$$\n\\boxed{\\frac{s - a_0}{a_1}}\n$$"
        },
        {
            "introduction": "接下来，我们将从种群宏观动态转向个体间的微观策略互动。这个问题引入了信号博弈（signaling game）的框架，让你能够精确地为捕食者和猎物（包括有防御的警戒色“模板”和可口的“拟态者”）构建收益函数和策略选择。通过求解这个博弈，你将揭示在个体层面的成本和收益权衡是如何共同塑造出一个稳定的警戒信号系统的，这为我们理解复杂的生物通讯和欺骗行为的演化提供了强大的分析工具。",
            "id": "2734422",
            "problem": "考虑一个将拟态系统中的警戒色形式化的信号博弈。存在两种猎物类型：有防御能力的（有毒）猎物，记为 $D$ 型；和可口的猎物，记为 $P$ 型。猎物种群中有比例为 $q \\in (0,1)$ 的是 $D$ 型，比例为 $1-q$ 的是 $P$ 型。每个猎物个体私下知晓其类型，然后选择一个非负的信号强度 $s \\in [0,\\infty)$。一个捕食者观察到信号 $s$ 但不知道猎物类型，并采用一种阈值策略：它选择一个阈值 $\\tau \\ge 0$，并且当且仅当 $s  \\tau$ 时攻击猎物。\n\n猎物产生信号成本，由凸成本函数 $c_D(s)$ 和 $c_P(s)$ 给出。如果被攻击，一个 $i \\in \\{D,P\\}$ 型的猎物以概率 $\\sigma_i \\in [0,1)$ 存活，否则死亡；如果不被攻击，其存活率为1。设基准繁殖价值为1，因此猎物的期望适应度是存活概率减去信号成本。捕食者成功捕食一只可口猎物的收益为 $R0$，攻击一只有防御能力的猎物的收益为 $-C_p0$，不攻击的收益为 $0$。此外，捕食者为将注意力维持在阈值 $\\tau$ 上支付认知成本 $k \\tau^2$，其中 $k0$。捕食者选择 $\\tau$ 以最大化其期望收益，该期望收益基于在适用情况下通过贝叶斯法则形成的信念。\n\n假设自然警戒信号和拟态具有已知的、经过充分检验的特性：(i) 捕食者基于高强度警戒信号与有防御能力猎物之间习得的关联来更新其攻击决策，(ii) 凸信号成本反映了新陈代谢或发育投入，(iii) 由于不同的生理限制，信号成本因类型而异。使用这些作为构建均衡逻辑的基础；不要先验地假设任何目标解公式。\n\n设参数和成本函数为：\n- $q=\\frac{2}{5}$，\n- $c_D(s)=a s^2$，其中 $a=\\frac{1}{4}$，\n- $c_P(s)=A s^2$，其中 $A=1$，\n- $\\sigma_D=\\frac{4}{5}$，$\\sigma_P=\\frac{1}{5}$，\n- $R=1$，$C_p=3$，以及 $k=\\frac{1}{10}$。\n\n精确地用 $s$ 和 $\\tau$ 定义猎物和捕食者的收益，推导分离均衡的激励相容条件（在该均衡中，有防御能力的猎物选择足够高的信号以避免被攻击，而可口的猎物选择足够低的信号以被攻击），并施加一个精炼条件：当某个类型无差异时，它遵循生态学上经典的择一法则：有防御能力的猎物偏好避免被攻击，可口的猎物偏好被攻击。捕食者选择能够维持猎物均衡反应的最小 $\\tau$，以便在保持对猎物行为的最佳应对的同时，最小化认知成本 $k \\tau^2$。\n\n在这些假设和参数值下，从第一性原理出发求解唯一的分离均衡，并以精确形式报告三元组 $\\left(s_D^{\\ast},\\,s_P^{\\ast},\\,\\tau^{\\ast}\\right)$。你的最终数值答案必须以单行矩阵的形式给出，且无需四舍五入。",
            "solution": "问题陈述是在演化生物学背景下一个定义明确的信号博弈，并且可以进行形式化分析。\n\n### 第1步：提取已知条件\n- **猎物类型**：有防御能力的（$D$）和可口的（$P$）。\n- **种群比例**：$D$ 型的比例是 $q = \\frac{2}{5}$；$P$ 型的比例是 $1-q = \\frac{3}{5}$。\n- **信号空间**：猎物个体选择信号强度 $s \\in [0, \\infty)$。\n- **捕食者策略**：选择一个阈值 $\\tau \\ge 0$，如果 $s  \\tau$ 则攻击。\n- **信号成本**：$D$ 型成本为 $c_D(s) = a s^2$，其中 $a=\\frac{1}{4}$。$P$ 型成本为 $c_P(s) = A s^2$，其中 $A=1$。\n- **被攻击时的存活概率**：$D$ 型以概率 $\\sigma_D = \\frac{4}{5}$ 存活。$P$ 型以概率 $\\sigma_P = \\frac{1}{5}$ 存活。不被攻击时存活率为 $1$。\n- **猎物适应度**：存活概率减去信号成本。\n- **捕食者收益**：捕食可口猎物的收益为 $R=1$。攻击有防御能力猎物的收益为 $-C_p=-3$。不攻击的收益为 $0$。\n- **捕食者认知成本**：维持阈值 $\\tau$ 的成本为 $k \\tau^2$，其中 $k = \\frac{1}{10}$。\n- **均衡概念**：分离精炼贝叶斯均衡。\n- **均衡属性**：有防御能力的猎物选择 $s_D^*$ 以避免被攻击。可口的猎物选择 $s_P^*$ 并被攻击。\n- **择一法则**：如果无差异，有防御能力的猎物偏好避免被攻击；可口的猎物偏好被攻击。\n- **捕食者的阈值选择**：捕食者选择能够维持猎物均衡反应的最小 $\\tau$。\n\n### 第2步：使用提取的已知条件进行验证\n该问题有科学依据，使用信号博弈的已确立框架对演化生物学中的一个经典情景进行建模。关于信号成本和捕食者行为的假设是该领域的标准做法。问题提法恰当，所有必要的参数和函数都有定义，且目标明确。语言客观且精确。问题是自洽的，没有矛盾。在模型背景下，参数在物理上和生物学上都是合理的。\n\n### 第3步：结论与行动\n问题是有效的。可以从第一性原理推导出严谨的解。\n\n### 求解推导\n\n我们寻找一个分离均衡，其中可口的猎物（$P$ 型）选择信号 $s_P^*$ 并被攻击，而有防御能力的猎物（$D$ 型）选择信号 $s_D^*$ 且不被攻击。设捕食者的阈值为 $\\tau^*$。这要求 $s_P^*  \\tau^*$ 和 $s_D^* \\ge \\tau^*$。\n\n首先，我们定义在捕食者阈值为 $\\tau$ 的情况下，一个 $i \\in \\{D, P\\}$ 型猎物选择信号 $s$ 时的适应度（收益）。\n- 如果猎物被攻击（$s  \\tau$），其适应度为 $U_i(s, \\text{攻击}) = \\sigma_i - c_i(s)$。\n- 如果猎物不被攻击（$s \\ge \\tau$），其适应度为 $U_i(s, \\text{不攻击}) = 1 - c_i(s)$。\n\n现在我们推导每种猎物类型的激励相容（IC）条件。\n\n**可口的猎物（P型）：**\n可口的猎物选择信号 $s$ 以最大化其适应度。它有两个主要选项：\n1.  选择信号 $s  \\tau^*$ 以被攻击。为了最大化 $\\sigma_P - c_P(s)$，它必须最小化成本 $c_P(s) = A s^2$。最小成本在 $s=0$ 时取得。因此，如果 $P$ 型选择被攻击，其最优信号是 $s_P^* = 0$。其适应度为 $U_P(0, \\text{攻击}) = \\sigma_P - c_P(0) = \\sigma_P$。\n2.  选择信号 $s \\ge \\tau^*$ 以避免被攻击。为了最大化 $1 - c_P(s)$，它必须选择能够避免攻击的最低信号，即 $s=\\tau^*$。其适应度为 $U_P(\\tau^*, \\text{不攻击}) = 1 - c_P(\\tau^*) = 1 - A (\\tau^*)^2$。\n\n为使分离均衡成立，$P$ 型猎物必须偏好发送信号 $s_P^*=0$ 并被攻击。择一法则指出，如果无差异，它偏好被攻击。因此，$P$ 型的激励相容条件是：\n$$ \\sigma_P \\ge 1 - A (\\tau^*)^2 $$\n$$ A (\\tau^*)^2 \\ge 1 - \\sigma_P $$\n\n**有防御能力的猎物（D型）：**\n有防御能力的猎物也选择信号 $s$ 以最大化其适应度。其选项是：\n1.  选择信号 $s  \\tau^*$ 以被攻击。为了最大化 $\\sigma_D - c_D(s)$，它必须最小化成本 $c_D(s) = a s^2$，因此它会选择 $s=0$ 。其适应度为 $U_D(0, \\text{攻击}) = \\sigma_D - c_D(0) = \\sigma_D$。\n2.  选择信号 $s \\ge \\tau^*$ 以避免被攻击。为了最大化 $1 - c_D(s)$，它必须选择能够避免攻击的成本最低的信号，即 $s=\\tau^*$。因此，如果 $D$ 型选择避免被攻击，其最优信号是 $s_D^* = \\tau^*$。其适应度为 $U_D(\\tau^*, \\text{不攻击}) = 1 - c_D(\\tau^*) = 1 - a (\\tau^*)^2$。\n\n为使分离均衡成立，$D$ 型猎物必须偏好发送信号 $s_D^* = \\tau^*$ 并避免被攻击。择一法则指出，如果无差异，它偏好避免被攻击。因此，$D$ 型的激励相容条件是：\n$$ 1 - a (\\tau^*)^2 \\ge \\sigma_D $$\n$$ a (\\tau^*)^2 \\le 1 - \\sigma_D $$\n\n**捕食者的决策与均衡阈值：**\n这两个激励相容条件为 $(\\tau^*)^2$ 提供了一个可行范围：\n$$ \\frac{1 - \\sigma_P}{A} \\le (\\tau^*)^2 \\le \\frac{1 - \\sigma_D}{a} $$\n这种形式的分离均衡只有在该范围非空时才存在，这要求 $\\frac{1 - \\sigma_P}{A} \\le \\frac{1 - \\sigma_D}{a}$。\n\n在给定猎物的分离均衡策略下，捕食者的总期望效用是遭遇产生的收益之和减去认知成本。当猎物为 $D$ 型的概率为 $q$ 时，它发出信号 $s_D^* = \\tau^*$ 而不被攻击，捕食者的收益为 $0$。当猎物为 $P$ 型的概率为 $1-q$ 时，它发出信号 $s_P^*=0$ 并被攻击，捕食者的收益为 $R$。\n捕食者的总效用为 $U_{\\text{捕食者}}(\\tau^*) = q(0) + (1-q)R - k(\\tau^*)^2 = (1-q)R - k(\\tau^*)^2$。\n为最大化此效用，捕食者必须最小化 $\\tau^*$，因为 $k > 0$。问题陈述明确规定了这一点：“捕食者选择能够维持猎物均衡反应的最小 $\\tau$”。这意味着捕食者将选择满足猎物激励相容条件的最小可能 $\\tau^*$ 值。\n可行范围中 $(\\tau^*)^2$ 的最小值为其下界：\n$$ (\\tau^*)^2 = \\frac{1 - \\sigma_P}{A} $$\n这个选择必须与来自 $D$ 型激励相容条件的上界一致。\n\n**均衡计算：**\n我们代入给定的参数值：\n- $a = \\frac{1}{4}$, $A = 1$\n- $\\sigma_D = \\frac{4}{5}$, $\\sigma_P = \\frac{1}{5}$\n\n首先，我们确定 $(\\tau^*)^2$ 的唯一值。其下界是：\n$$ (\\tau^*)^2 = \\frac{1 - \\sigma_P}{A} = \\frac{1 - \\frac{1}{5}}{1} = \\frac{4}{5} $$\n可行范围的上界是：\n$$ \\frac{1 - \\sigma_D}{a} = \\frac{1 - \\frac{4}{5}}{\\frac{1}{4}} = \\frac{\\frac{1}{5}}{\\frac{1}{4}} = \\frac{4}{5} $$\n由于下界和上界相等，存在一个唯一的 $(\\tau^*)^2$ 值支持此分离均衡：\n$$ (\\tau^*)^2 = \\frac{4}{5} $$\n这给出了均衡阈值 $\\tau^*$：\n$$ \\tau^* = \\sqrt{\\frac{4}{5}} = \\frac{2}{\\sqrt{5}} = \\frac{2\\sqrt{5}}{5} $$\n\n现在我们可以确定猎物的均衡信号：\n- 可口猎物的信号是 $s_P^* = 0$。\n- 有防御能力猎物的信号是 $s_D^* = \\tau^* = \\frac{2\\sqrt{5}}{5}$。\n\n因此，唯一的分离均衡三元组 $(s_D^*, s_P^*, \\tau^*)$ 是 $\\left(\\frac{2\\sqrt{5}}{5}, 0, \\frac{2\\sqrt{5}}{5}\\right)$。\n其他参数（$q, R, C_p, k$）确保了模型的基本假设成立（例如，捕食者希望攻击可口的猎物而非有防御能力的猎物，并希望最小化其阈值 $\\tau$），但一旦问题结构确定，它们并不进入均衡值本身的计算。\n\n最终答案是三元组 $(s_D^*, s_P^*, \\tau^*)$。\n$s_D^* = \\frac{2\\sqrt{5}}{5}$\n$s_P^* = 0$\n$\\tau^* = \\frac{2\\sqrt{5}}{5}$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{2\\sqrt{5}}{5}  0  \\frac{2\\sqrt{5}}{5}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "最后的练习将我们的分析视角从离散的策略或类型频率，提升到连续性状本身的演化过程。你将通过编程实践，应用定量遗传学中著名的兰德方程（Lande's equation），模拟拟态者表型（如警戒色图案）如何在一个多维性状空间中向模板物种的表型演化。这个计算练习不仅让你能够探索自然选择如何驱动表型趋同，更能让你直观地感受遗传结构（以 $G$ 矩阵为代表）如何约束或引导演化轨迹，将理论模型与现代计算生物学方法紧密结合。",
            "id": "2734462",
            "problem": "考虑一个代表视觉模拟图案的连续数量性状。设该性状为一个向量 $\\mathbf{z} \\in \\mathbb{R}^m$，其在离散时间内根据 Lande 方程演化，\n$$\n\\Delta \\mathbf{z} = \\mathbf{G}\\, \\nabla W(\\mathbf{z}),\n$$\n其中 $\\mathbf{G}$ 是一个恒定的、对称半正定的遗传方差-协方差矩阵，$W(\\mathbf{z})$ 是绝对适合度。模型点（被防御模型的性状）为 $\\mathbf{z}^\\star \\in \\mathbb{R}^m$。假设捕食者对图案相似性的泛化遵循一个由以下公式定义的高斯辨别函数\n$$\nc(d) = \\exp(-\\beta d^2), \\quad d = \\|\\mathbf{z} - \\mathbf{z}^\\star\\|_2,\n$$\n其中 $\\beta  0$ 表示辨别强度，$\\|\\cdot\\|_2$ 是欧几里得范数。设与警戒图案的相遇会更新捕食者的攻击厌恶，更新程度与图案相遇中不可口经历的比例成正比。分别用 $q$ 和 $p$ 表示模型和拟态者的种群频率，用 $u_M \\in [0,1]$ 表示模型的不可口性水平，用 $u_m \\in [0,1]$ 表示拟态者的不可口性水平。设 $\\rho \\in (0,1]$ 是通过学习可达到的最大攻击减少量。在这些假设下，定义频率依赖性学习因子\n$$\nR = \\rho \\cdot \\frac{u_M q + u_m p}{q + p}.\n$$\n假设绝对适合度与学习到的攻击厌恶和感知混淆的乘积成正比，因此（在任意正仿射变换下）可以表示为\n$$\nW(\\mathbf{z}) = R \\, c(\\|\\mathbf{z} - \\mathbf{z}^\\star\\|_2).\n$$\n你必须通过从初始状态 $\\mathbf{z}_0$ 开始迭代离散时间的 Lande 更新来计算演化轨迹，保持 $\\mathbf{G}$ 和生态参数恒定，直到性状落入模型点的一个小的欧几里得容差范围内，或者达到最大迭代次数。形式上，迭代\n$$\n\\mathbf{z}_{t+1} = \\mathbf{z}_t + \\mathbf{G}\\, \\nabla W(\\mathbf{z}_t),\n$$\n对于 $t = 0,1,2,\\dots$，并在满足 $\\|\\mathbf{z}_t - \\mathbf{z}^\\star\\|_2 \\le \\varepsilon$ 的最小 $t$ 值处停止，如果直到 $N_{\\max}$ 都不存在这样的 $t$，则报告失败。\n\n你的实现必须仅基于上述定义以及梯度和范数的标准属性。除这些定义和基础微积分外，不要假定任何额外的公式。对于每个测试用例，你的程序必须返回满足 $\\|\\mathbf{z}_t - \\mathbf{z}^\\star\\|_2 \\le \\varepsilon$ 所需的迭代次数（整数）；如果在 $N_{\\max}$ 次迭代内未达到阈值，则为该测试用例返回整数 $-1$。\n\n对所有测试用例使用以下常量：容差 $\\varepsilon = 10^{-6}$ 和最大迭代次数 $N_{\\max} = 10^5$。以下所有测试用例均使用二维性状（$m = 2$）和模型点 $\\mathbf{z}^\\star = (0,0)$。\n\n实现该算法并评估以下测试套件。每个用例指定了 $(\\mathbf{z}_0, \\mathbf{G}, \\beta, \\rho, p, q, u_M, u_m)$：\n\n- 测试用例 A（贝氏拟态，中度学习）：\n  - $\\mathbf{z}_0 = (1.0, 0.5)$,\n  - $\\mathbf{G} = \\mathrm{diag}(0.02, 0.01)$,\n  - $\\beta = 3.0$, $\\rho = 0.9$,\n  - $p = 0.1$, $q = 0.3$,\n  - $u_M = 1.0$, $u_m = 0.0$.\n\n- 测试用例 B（贝氏拟态，极低遗传方差）：\n  - $\\mathbf{z}_0 = (1.0, 1.0)$,\n  - $\\mathbf{G} = \\mathrm{diag}(10^{-8}, 10^{-8})$,\n  - $\\beta = 3.0$, $\\rho = 0.9$,\n  - $p = 0.7$, $q = 0.3$,\n  - $u_M = 1.0$, $u_m = 0.0$.\n\n- 测试用例 C（穆氏拟态，各向异性协方差）：\n  - $\\mathbf{z}_0 = (2.0, -1.0)$,\n  - $\\mathbf{G} = \\begin{pmatrix}0.02  0.015\\\\ 0.015  0.03\\end{pmatrix}$,\n  - $\\beta = 2.0$, $\\rho = 0.8$,\n  - $p = 0.5$, $q = 0.5$,\n  - $u_M = 1.0$, $u_m = 0.6$.\n\n- 测试用例 D（贝氏拟态，相关响应）：\n  - $\\mathbf{z}_0 = (-1.5, 1.0)$,\n  - $\\mathbf{G} = \\begin{pmatrix}0.01  0.005\\\\ 0.005  0.01\\end{pmatrix}$,\n  - $\\beta = 4.0$, $\\rho = 0.7$,\n  - $p = 0.2$, $q = 0.4$,\n  - $u_M = 1.0$, $u_m = 0.0$.\n\n- 测试用例 E（穆氏拟态，强辨别能力）：\n  - $\\mathbf{z}_0 = (3.0, 3.0)$,\n  - $\\mathbf{G} = \\mathrm{diag}(0.005, 0.005)$,\n  - $\\beta = 6.0$, $\\rho = 0.9$,\n  - $p = 0.4$, $q = 0.6$,\n  - $u_M = 1.0$, $u_m = 0.9$.\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，“[result_A,result_B,result_C,result_D,result_E]”）。结果必须是如上所述的整数，并按 A 到 E 的顺序排列。",
            "solution": "该问题要求模拟在频率依赖性选择下多变量性状的演化，该过程由 Lande 的离散时间方程描述。其生物学背景是拟态者视觉图案 $\\mathbf{z} \\in \\mathbb{R}^m$ 朝着被防御模型生物的图案 $\\mathbf{z}^\\star \\in \\mathbb{R}^m$ 的演化。演化轨迹通过迭代以下方程确定\n$$\n\\mathbf{z}_{t+1} = \\mathbf{z}_t + \\mathbf{G}\\, \\nabla W(\\mathbf{z}_t)\n$$\n其中 $\\mathbf{z}_t$ 是第 $t$ 代的性状向量，$\\mathbf{G}$ 是遗传方差-协方差矩阵，$\\nabla W(\\mathbf{z}_t)$ 是绝对适合度曲面 $W(\\mathbf{z})$ 上的选择梯度。\n\n适合度函数 $W(\\mathbf{z})$ 定义为两个具有生物学动机的因子的乘积：$W(\\mathbf{z}) = R \\cdot c(\\|\\mathbf{z} - \\mathbf{z}^\\star\\|_2)$。第一项 $R$ 是频率依赖性学习因子，由下式给出\n$$\nR = \\rho \\cdot \\frac{u_M q + u_m p}{q + p}.\n$$\n该项代表了学习到的捕食者厌恶强度，其取决于拟态者和模型种群各自的频率（$p, q$）和不可口性（$u_m, u_M$），并由最大学习效应 $\\rho$ 进行缩放。对于一组给定的生态参数，$R$ 是一个常数。第二项 $c(d) = \\exp(-\\beta d^2)$（其中 $d = \\|\\mathbf{z} - \\mathbf{z}^\\star\\|_2$）是一个描述感知混淆的高斯函数。该函数在 $d=0$（即 $\\mathbf{z} = \\mathbf{z}^\\star$）时达到峰值，并随着拟态图案与模型图案的差异增大而减小。参数 $\\beta$ 控制该峰的陡峭程度，代表捕食者的辨别能力。该适合度景观在 $\\mathbf{z} = \\mathbf{z}^\\star$ 处有唯一的全局最大值，该点作为演化吸引子。\n\n为实现迭代更新，我们必须首先计算适合度函数的梯度 $\\nabla W(\\mathbf{z})$。梯度向量指向适合度增加最快的方向。使用链式法则，并令 $\\mathbf{x} = \\mathbf{z} - \\mathbf{z}^\\star$，梯度的第 $i$ 个分量为：\n$$\n\\frac{\\partial W}{\\partial z_i} = \\frac{\\partial}{\\partial z_i} \\left[ R \\exp(-\\beta \\|\\mathbf{z} - \\mathbf{z}^\\star\\|_2^2) \\right] = R \\cdot \\exp(-\\beta \\|\\mathbf{z} - \\mathbf{z}^\\star\\|_2^2) \\cdot \\frac{\\partial}{\\partial z_i} (-\\beta \\sum_{j=1}^m (z_j - z_j^\\star)^2)\n$$\n$$\n\\frac{\\partial W}{\\partial z_i} = R \\cdot \\exp(-\\beta \\|\\mathbf{z} - \\mathbf{z}^\\star\\|_2^2) \\cdot (-2\\beta (z_i - z_i^\\star)) = -2\\beta R \\, c(\\|\\mathbf{z} - \\mathbf{z}^\\star\\|_2) (z_i - z_i^\\star).\n$$\n用向量形式表示，梯度为：\n$$\n\\nabla W(\\mathbf{z}) = -2\\beta R \\exp(-\\beta \\|\\mathbf{z} - \\mathbf{z}^\\star\\|_2^2) (\\mathbf{z} - \\mathbf{z}^\\star).\n$$\n梯度始终指向模型点 $\\mathbf{z}^\\star$，其大小随着拟态性状 $\\mathbf{z}$ 接近 $\\mathbf{z}^\\star$ 而减小。\n\n将梯度代入 Lande 方程，得到模拟的显式更新规则：\n$$\n\\mathbf{z}_{t+1} = \\mathbf{z}_t - 2\\beta R \\exp(-\\beta \\|\\mathbf{z}_t - \\mathbf{z}^\\star\\|_2^2) \\mathbf{G}(\\mathbf{z}_t - \\mathbf{z}^\\star).\n$$\n该方程描述了对选择的响应。选择压力与 $-(\\mathbf{z}_t - \\mathbf{z}^\\star)$ 成正比，并被遗传方差-协方差矩阵 $\\mathbf{G}$ 左乘。如果 $\\mathbf{G}$ 是单位矩阵的标量倍，种群将直接朝最优方向演化。然而，如果 $\\mathbf{G}$ 具有非对角线元素（遗传相关）或对角线上的方差不相等，演化轨迹将会产生偏离，可能不会遵循最陡峭的上升路径。\n\n解决该问题的算法是此迭代过程的直接实现。对于每个测试用例：\n1. 定义常量参数，包括初始性状 $\\mathbf{z}_0$、矩阵 $\\mathbf{G}$ 以及标量值 $\\beta, \\rho, p, q, u_M, u_m$。模型点固定为 $\\mathbf{z}^\\star=(0,0)$。使用常量 $\\varepsilon=10^{-6}$ 和 $N_{\\max}=10^5$。\n2. 预先计算常数学习因子 $R$。\n3. 启动一个最多运行 $N_{\\max}$ 次迭代的循环，迭代计数器 $t$ 从 0 开始。\n4. 在每次迭代 $t$（对于 $t=0, 1, \\dots, N_{\\max}$）的开始，计算欧几里得距离 $\\|\\mathbf{z}_t - \\mathbf{z}^\\star\\|_2$。\n5. 将此距离与容差 $\\varepsilon$ 比较。如果 $\\|\\mathbf{z}_t - \\mathbf{z}^\\star\\|_2 \\le \\varepsilon$，则认为性状已收敛。该用例的模拟终止，并将当前迭代次数 $t$ 记录为结果。\n6. 如果尚未达到收敛，则使用推导出的更新公式计算下一个性状向量 $\\mathbf{z}_{t+1}$。这包括计算指数项，执行矩阵-向量乘积 $\\mathbf{G}(\\mathbf{z}_t - \\mathbf{z}^\\star)$，缩放结果，并将更新量加到 $\\mathbf{z}_t$ 上。\n7. 如果循环完成了所有 $N_{\\max}+1$ 次检查而距离仍未低于 $\\varepsilon$，则认为模拟失败。该用例的记录结果为 $-1$。\n\n此计算过程精确地模拟了指定的拟态演化模型，得出了在给定的遗传和生态约束下，拟态者表型变得与模型无法区分所需的时间代数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the evolutionary trajectory problem for a set of test cases.\n    \"\"\"\n\n    # Define global constants for all test cases.\n    epsilon = 1e-6\n    n_max = 100000\n    z_star = np.array([0.0, 0.0])\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {   # Test case A\n            \"z0\": np.array([1.0, 0.5]),\n            \"G\": np.diag([0.02, 0.01]),\n            \"beta\": 3.0, \"rho\": 0.9,\n            \"p\": 0.1, \"q\": 0.3,\n            \"u_M\": 1.0, \"u_m\": 0.0\n        },\n        {   # Test case B\n            \"z0\": np.array([1.0, 1.0]),\n            \"G\": np.diag([1e-8, 1e-8]),\n            \"beta\": 3.0, \"rho\": 0.9,\n            \"p\": 0.7, \"q\": 0.3,\n            \"u_M\": 1.0, \"u_m\": 0.0\n        },\n        {   # Test case C\n            \"z0\": np.array([2.0, -1.0]),\n            \"G\": np.array([[0.02, 0.015], [0.015, 0.03]]),\n            \"beta\": 2.0, \"rho\": 0.8,\n            \"p\": 0.5, \"q\": 0.5,\n            \"u_M\": 1.0, \"u_m\": 0.6\n        },\n        {   # Test case D\n            \"z0\": np.array([-1.5, 1.0]),\n            \"G\": np.array([[0.01, 0.005], [0.005, 0.01]]),\n            \"beta\": 4.0, \"rho\": 0.7,\n            \"p\": 0.2, \"q\": 0.4,\n            \"u_M\": 1.0, \"u_m\": 0.0\n        },\n        {   # Test case E\n            \"z0\": np.array([3.0, 3.0]),\n            \"G\": np.diag([0.005, 0.005]),\n            \"beta\": 6.0, \"rho\": 0.9,\n            \"p\": 0.4, \"q\": 0.6,\n            \"u_M\": 1.0, \"u_m\": 0.9\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        z = case[\"z0\"].astype(np.float64)\n        G = case[\"G\"].astype(np.float64)\n        beta = case[\"beta\"]\n        rho = case[\"rho\"]\n        p = case[\"p\"]\n        q = case[\"q\"]\n        u_M = case[\"u_M\"]\n        u_m = case[\"u_m\"]\n\n        # Calculate the constant learning factor R\n        R = rho * (u_M * q + u_m * p) / (q + p)\n\n        # Pre-calculate the constant part of the update term's scalar factor\n        # The update is z_{t+1} = z_t - 2*beta*R*exp(...) * G(z_t - z_star)\n        C = -2.0 * beta * R\n\n        iterations = -1\n        # Loop from t=0 to t=n_max. Check for convergence at the beginning of each step.\n        for t in range(n_max + 1):\n            # Calculate displacement vector and distance from the model point\n            d_vec = z - z_star\n            dist = np.linalg.norm(d_vec)\n\n            # Check for stopping condition\n            if dist = epsilon:\n                iterations = t\n                break\n            \n            # If t reaches n_max here, it means z_n_max did not converge,\n            # so we break loop and keep iterations = -1. This check is implicit\n            # in the for loop range.\n\n            # Calculate the full update vector for z_t -> z_{t+1}\n            # delta_z = G * grad(W(z))\n            # grad(W(z)) = -2*beta*R*exp(-beta*dist^2)*(z-z_star)\n            # scalar_part = -2*beta*R*exp(-beta*dist^2)\n            scalar_part = C * np.exp(-beta * dist**2)\n            delta_z = scalar_part * np.dot(G, d_vec)\n            \n            # Update the trait vector\n            z = z + delta_z\n        \n        results.append(iterations)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}