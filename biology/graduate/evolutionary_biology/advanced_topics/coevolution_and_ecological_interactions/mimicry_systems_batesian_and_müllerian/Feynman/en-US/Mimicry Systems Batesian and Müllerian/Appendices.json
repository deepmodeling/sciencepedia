{
    "hands_on_practices": [
        {
            "introduction": "Understanding the evolution of mimicry requires moving beyond static descriptions to dynamic models of natural selection. This first practice uses the replicator equation, a fundamental tool in evolutionary game theory, to explore how the frequency of mimics changes over time under different ecological scenarios. By deriving and analyzing the stability of the system's equilibria, you will gain hands-on experience with the principles of frequency-dependent selection that govern the success or failure of mimicry strategies .",
            "id": "2734496",
            "problem": "Aposematic prey often form warning-signal complexes in which defended models are sometimes accompanied by undefended look-alike mimics. Consider a well-mixed predator–prey environment with two warning-signal bearers: defended models (type $T$) and undefended mimics (type $M$). Let $q \\in [0,1]$ denote the frequency of mimics among signal bearers. Predators attack warning-signal bearers with probability $A(q)$ that depends on the reliability of the signal; assume a linear form $A(q) = a_0 + a_1 q$, where $0 \\leq a_0 \\leq 1$ and $a_1$ is such that $0 \\leq A(q) \\leq 1$ for all $q \\in [0,1]$. The sign of $a_1$ encodes the ecological regime: $a_1 > 0$ corresponds to Batesian conditions (more mimics dilute the signal, increasing attacks on the warning signal), and $a_1 < 0$ corresponds to Müllerian-like conditions (a greater fraction of honest signalers reduces attacks on the warning signal). Assume models pay a constitutive cost $s \\in [0,1]$ due to toxin production that reduces their baseline fitness independently of attacks, while mimics do not pay this cost but suffer full mortality upon attack.\n\nTake absolute fitnesses as follows: mimics have $W_m(q) = 1 - A(q)$, and models have $W_t = 1 - s$. Let the mean fitness be $\\bar{W}(q) = q W_m(q) + (1 - q) W_t$. The frequency $q$ evolves according to the replicator dynamics ordinary differential equation (ODE) $\\dot{q} = q \\big(W_m(q) - \\bar{W}(q)\\big)$.\n\nStarting from these assumptions and the definition of the replicator dynamics ODE, derive the one-dimensional selection gradient in $q$ and analyze the local stability of all equilibria. Determine parameter conditions on $a_0$, $a_1$, and $s$ under which selection drives the mimic to low frequency (boundary extinction), to fixation, or to a stable polymorphic frequency. Finally, report the closed-form analytic expression for the internal equilibrium frequency $q^{\\ast}$ (when it exists) as a function of $a_0$, $a_1$, and $s$. The final answer must be a single closed-form expression for $q^{\\ast}$. No rounding is required, and no units are necessary because all quantities are dimensionless probabilities or frequencies.",
            "solution": "The problem presented is a standard exercise in mathematical evolutionary biology. It is self-contained, scientifically grounded in the principles of frequency-dependent selection and replicator dynamics, and is posed with sufficient clarity and rigor to permit a unique, meaningful solution. The problem is therefore deemed valid. We proceed with the derivation.\n\nThe dynamics of the frequency of mimics, $q$, are governed by the replicator equation:\n$$ \\dot{q} = q \\big(W_m(q) - \\bar{W}(q)\\big) $$\nwhere $W_m(q)$ is the fitness of mimics and $\\bar{W}(q)$ is the mean fitness of the population of signal bearers. The mean fitness is defined as $\\bar{W}(q) = q W_m(q) + (1 - q) W_t$, where $W_t$ is the fitness of the defended models.\n\nFirst, we expand the selection gradient term, $W_m(q) - \\bar{W}(q)$:\n$$ W_m(q) - \\bar{W}(q) = W_m(q) - \\big(q W_m(q) + (1 - q) W_t\\big) = (1 - q) W_m(q) - (1 - q) W_t = (1 - q) \\big(W_m(q) - W_t\\big) $$\nSubstituting this back into the replicator equation yields a more transparent form:\n$$ \\dot{q} = q(1 - q) \\big(W_m(q) - W_t\\big) $$\nThe term $S(q) = W_m(q) - W_t$ represents the fitness advantage of a mimic over a model, given the current population composition $q$. This is the one-dimensional selection gradient that drives the change in frequency for $q \\in (0,1)$.\n\nNext, we substitute the specified fitness functions into the expression for $S(q)$. The fitness of a mimic is $W_m(q) = 1 - A(q)$, where the attack probability is $A(q) = a_0 + a_1 q$. The fitness of a model is $W_t = 1 - s$.\n$$ S(q) = W_m(q) - W_t = \\big(1 - (a_0 + a_1 q)\\big) - (1 - s) = 1 - a_0 - a_1 q - 1 + s = s - a_0 - a_1 q $$\nThus, the complete ordinary differential equation for the frequency of mimics is:\n$$ \\dot{q} = q(1 - q)(s - a_0 - a_1 q) $$\nThe equilibria of this system, denoted by $q^{\\ast}$, are the values of $q$ for which $\\dot{q} = 0$. From the equation, we can identify three potential equilibria:\n1.  The boundary equilibrium $q^{\\ast} = 0$, corresponding to the extinction of mimics.\n2.  The boundary equilibrium $q^{\\ast} = 1$, corresponding to the fixation of mimics.\n3.  Any internal equilibrium $q^{\\ast} \\in (0,1)$ that is a root of the linear term $s - a_0 - a_1 q^{\\ast} = 0$.\n\nThe primary task is to find the closed-form expression for this internal equilibrium, $q^{\\ast}$. We solve the equation $s - a_0 - a_1 q^{\\ast} = 0$ for $q^{\\ast}$:\n$$ a_1 q^{\\ast} = s - a_0 $$\nAssuming $a_1 \\neq 0$, we can isolate $q^{\\ast}$:\n$$ q^{\\ast} = \\frac{s - a_0}{a_1} $$\nThis is the analytical expression for the internal equilibrium frequency.\n\nFor completeness, we analyze the stability and existence conditions for this equilibrium. Let $f(q) = \\dot{q}$. An equilibrium $q^{\\ast}$ is locally stable if $f'(q^{\\ast}) < 0$ and unstable if $f'(q^{\\ast}) > 0$.\nThe derivative is $f'(q) = (1-2q)(s - a_0 - a_1 q) - a_1 q(1 - q)$.\nAt the internal equilibrium $q^{\\ast} = \\frac{s - a_0}{a_1}$, the term $(s - a_0 - a_1 q^{\\ast})$ is zero by definition. Therefore, the derivative simplifies to:\n$$ f'(q^{\\ast}) = - a_1 q^{\\ast}(1 - q^{\\ast}) $$\nFor $q^{\\ast}$ to be a polymorphic equilibrium, it must lie in the interval $(0,1)$, which implies $q^{\\ast}(1 - q^{\\ast}) > 0$. The stability of the internal equilibrium hence depends solely on the sign of $a_1$:\n-   If $a_1 > 0$ (Batesian mimicry), then $f'(q^{\\ast}) < 0$, and the internal equilibrium is **stable**. This corresponds to negative frequency-dependent selection, where the mimic's fitness decreases as its frequency increases. A stable polymorphism is possible if mimics are advantageous when rare ($W_m(0) > W_t \\implies s > a_0$) but disadvantageous when common ($W_m(1) < W_t \\implies s < a_0 + a_1$). These conditions, $a_0 < s < a_0 + a_1$, ensure $0 < q^{\\ast} < 1$.\n-   If $a_1 < 0$ (Müllerian-like conditions), then $f'(q^{\\ast}) > 0$, and the internal equilibrium is **unstable**. This corresponds to positive frequency-dependent selection, where the mimic's fitness increases with its frequency. Such a system leads to bistability, where the population converges to either mimic extinction ($q=0$) or fixation ($q=1$), depending on the initial frequency relative to the unstable equilibrium $q^{\\ast}$. The condition for this internal equilibrium to exist in $(0,1)$ is $a_0+a_1 < s < a_0$.\n\nThe specific question asks only for the closed-form analytic expression for the internal equilibrium frequency $q^{\\ast}$. This expression has been derived above.",
            "answer": "$$\n\\boxed{\\frac{s - a_0}{a_1}}\n$$"
        },
        {
            "introduction": "While Batesian mimicry benefits the mimic, it imposes a cost on the defended model by diluting the warning signal's reliability. This exercise challenges you to quantify the limits of this parasitic relationship by modeling a scenario where predator learning saturates. Your task is to determine the critical frequency of mimics beyond which the risk of predation makes the warning signal a liability, forcing the model species towards an alternative cryptic strategy .",
            "id": "2734461",
            "problem": "A defended prey species (the \"model\") faces frequency-dependent predation because palatable \"Batesian\" mimics dilute the reliability of its aposematic warning signal. Assume the following minimal, biologically standard ingredients.\n\n- Let the relative frequency of Batesian mimics among warning-colored prey be $q \\ge 0$, so that the reliability of the warning signal (the fraction of warning-bearers that are truly defended) is $\\rho(q) = \\frac{1}{1+q}$.\n\n- Predator avoidance of the warning signal saturates with the aversive stimulus generated by encounters with defended prey. Model this saturation by assuming that the probability a predator attacks a warning-colored prey individual is $A(q) = \\frac{1}{1 + \\lambda \\rho(q)}$, where $\\lambda > 0$ is a learning-intensity parameter that aggregates encounter rate, memory, and the strength of the aversive experience per defended attack.\n\n- If a model individual bears the warning signal, each attack imposes an expected fitness loss $c_m \\in (0,1]$ on that individual. If instead the model adopts an alternative cryptic pattern, predators do not learn to avoid it from its appearance; assume they attack with a constant baseline probability $a_0 \\in (0,1)$, and each such attack imposes an expected fitness loss $c_c \\in (0,1]$ on the cryptic individual.\n\n- Let the per-generation baseline fitness (in the absence of predation) be scaled to $1$ for both strategies, so that predation reduces fitness multiplicatively by expected attack losses.\n\nAssume parameters satisfy $c_m > a_0 c_c$, ensuring that warning coloration is favored when the warning signal is perfectly reliable. Define the fitness of a warning-colored model individual when mimics are at frequency $q$ as $W_m(q) = 1 - c_m A(q)$, and the fitness of a cryptic model individual as $W_c = 1 - a_0 c_c$.\n\nShow from first principles that there exists a critical mimic frequency $q_c$ such that for all $q > q_c$ the warning-colored model has lower fitness than the cryptic alternative, and express $q_c$ in terms of $\\lambda$, $c_m$, $c_c$, and $a_0$. Your final answer must be a single closed-form expression for $q_c$ with no units. Do not approximate or round; provide the exact symbolic expression.",
            "solution": "The problem statement is scientifically grounded, well-posed, objective, and self-contained. The provided parameters and functional forms represent a standard and logically consistent model of frequency-dependent selection in a Batesian mimicry system. The assumptions are sufficient to derive a unique and meaningful solution. We will therefore proceed with the derivation as requested.\n\nThe task is to find a critical mimic frequency, which we denote $q_c$, at which the fitness of a warning-colored model individual, $W_m(q)$, becomes less than the fitness of a cryptic model individual, $W_c$. This critical frequency is defined as the point where the two fitness values are equal. For all mimic frequencies $q$ greater than $q_c$, the aposematic strategy must be less advantageous than the cryptic one.\n\nThe fitness functions are given as:\n$$W_m(q) = 1 - c_m A(q)$$\n$$W_c = 1 - a_0 c_c$$\nwhere $W_c$ is constant with respect to the mimic frequency $q$.\n\nThe critical frequency $q_c$ is the value of $q$ that satisfies the condition $W_m(q_c) = W_c$.\nSubstituting the fitness expressions into this equality gives:\n$$1 - c_m A(q_c) = 1 - a_0 c_c$$\nThis simplifies to a condition on the expected fitness losses:\n$$c_m A(q_c) = a_0 c_c$$\n\nThe problem provides the expression for the predator's attack probability, $A(q)$, as:\n$$A(q) = \\frac{1}{1 + \\lambda \\rho(q)}$$\nwhere $\\rho(q) = \\frac{1}{1+q}$ is the reliability of the warning signal.\nSubstituting the expression for $A(q_c)$ into our condition yields:\n$$c_m \\left( \\frac{1}{1 + \\lambda \\rho(q_c)} \\right) = a_0 c_c$$\n\nWe must now solve this equation for $q_c$. First, we solve for the term containing $\\rho(q_c)$:\n$$1 + \\lambda \\rho(q_c) = \\frac{c_m}{a_0 c_c}$$\nThe problem states that $c_m > a_0 c_c$, which ensures that the right-hand side is greater than $1$. This is consistent, as the left-hand side must be greater than $1$ since $\\lambda > 0$ and $\\rho(q_c) > 0$.\nIsolating the term with $\\rho(q_c)$:\n$$\\lambda \\rho(q_c) = \\frac{c_m}{a_0 c_c} - 1 = \\frac{c_m - a_0 c_c}{a_0 c_c}$$\n$$\\rho(q_c) = \\frac{1}{\\lambda} \\left( \\frac{c_m - a_0 c_c}{a_0 c_c} \\right)$$\n\nNow, we substitute the definition of $\\rho(q) = \\frac{1}{1+q}$:\n$$\\frac{1}{1+q_c} = \\frac{c_m - a_0 c_c}{\\lambda a_0 c_c}$$\n\nTo solve for $q_c$, we take the reciprocal of both sides:\n$$1 + q_c = \\frac{\\lambda a_0 c_c}{c_m - a_0 c_c}$$\nFinally, we isolate $q_c$:\n$$q_c = \\frac{\\lambda a_0 c_c}{c_m - a_0 c_c} - 1$$\nTo combine these into a single fraction, we find a common denominator:\n$$q_c = \\frac{\\lambda a_0 c_c - (c_m - a_0 c_c)}{c_m - a_0 c_c}$$\n$$q_c = \\frac{\\lambda a_0 c_c - c_m + a_0 c_c}{c_m - a_0 c_c}$$\n$$q_c = \\frac{a_0 c_c (1 + \\lambda) - c_m}{c_m - a_0 c_c}$$\n\nThis expression provides the critical mimic frequency $q_c$. For $q_c$ to be a physically meaningful positive frequency ($q_c > 0$), the numerator must also be positive, given that the denominator $c_m - a_0 c_c$ is positive by assumption. The condition for the numerator to be positive is $a_0 c_c (1 + \\lambda) - c_m > 0$, or $c_m < a_0 c_c (1 + \\lambda)$. This is the condition required for the warning coloration to be advantageous at $q=0$, as $W_m(0) > W_c$ implies $c_m A(0) < a_0 c_c$, which is $c_m \\frac{1}{1+\\lambda} < a_0 c_c$. The problem's setup implicitly requires this condition for a non-trivial solution to exist.\n\nNow, we must show that for all $q > q_c$, the fitness of the warning-colored model is lower than that of the cryptic alternative, i.e., $W_m(q) < W_c$. This is equivalent to showing that the expected fitness loss for the aposematic morph is greater than for the cryptic morph, i.e., $c_m A(q) > a_0 c_c$.\n\nLet us analyze the behavior of the function $A(q)$.\nThe signal reliability is $\\rho(q) = \\frac{1}{1+q}$. For $q \\ge 0$, $\\rho(q)$ is a strictly decreasing function of $q$. As the frequency of mimics $q$ increases, the signal becomes less reliable.\nThe attack probability is $A(q) = \\frac{1}{1 + \\lambda \\rho(q)}$. Since $\\lambda > 0$ and $\\rho(q)$ is a decreasing function of $q$, the denominator $1 + \\lambda \\rho(q)$ is also a strictly decreasing function of $q$. Consequently, its reciprocal, $A(q)$, is a strictly increasing function of $q$.\nThis means that as the mimic frequency $q$ increases, the probability of a predator attacking a warning-colored individual, $A(q)$, also increases.\n\nWe defined $q_c$ as the point where the expected losses are equal: $c_m A(q_c) = a_0 c_c$.\nSince $c_m > 0$ and $A(q)$ is a strictly increasing function of $q$, the product $c_m A(q)$ is also a strictly increasing function of $q$.\nTherefore, for any $q > q_c$, it must follow that:\n$$A(q) > A(q_c)$$\n$$c_m A(q) > c_m A(q_c)$$\nSubstituting the definition of $q_c$:\n$$c_m A(q) > a_0 c_c$$\nSubtracting this inequality from the baseline fitness of $1$ reverses the inequality sign:\n$$1 - c_m A(q) < 1 - a_0 c_c$$\nWhich is precisely the condition:\n$$W_m(q) < W_c$$\nThis confirms that for all mimic frequencies $q$ greater than the derived critical frequency $q_c$, the cryptic strategy yields higher fitness than the aposematic strategy. The existence of such a $q_c > 0$ is guaranteed in the parameter space where aposematism is initially favored but the cost of an attack on a model is higher than the expected cost of being cryptic.",
            "answer": "$$ \\boxed{\\frac{a_0 c_c (1 + \\lambda) - c_m}{c_m - a_0 c_c}} $$"
        },
        {
            "introduction": "The evolution of mimicry involves not just changes in population frequencies, but the gradual refinement of the mimetic phenotype itself. This practice moves into the realm of quantitative genetics, asking you to simulate the evolution of a multi-dimensional visual pattern using the landmark Lande equation. By implementing this model, you will explore how the genetic variance-covariance matrix ($G$) can constrain or channel the evolution of a mimic's appearance towards the model's pattern, providing a dynamic view of adaptation on a fitness landscape .",
            "id": "2734462",
            "problem": "Consider a continuous quantitative trait representing a visual mimic pattern. Let the trait be a vector $\\mathbf{z} \\in \\mathbb{R}^m$ evolving under Lande’s equation in discrete time,\n$$\n\\Delta \\mathbf{z} = \\mathbf{G}\\, \\nabla W(\\mathbf{z}),\n$$\nwhere $\\mathbf{G}$ is a constant, symmetric positive semidefinite genetic variance-covariance matrix and $W(\\mathbf{z})$ is the absolute fitness. The model point (the defended model’s trait) is $\\mathbf{z}^\\star \\in \\mathbb{R}^m$. Assume predator generalization about pattern similarity follows a Gaussian discrimination function defined by\n$$\nc(d) = \\exp(-\\beta d^2), \\quad d = \\|\\mathbf{z} - \\mathbf{z}^\\star\\|_2,\n$$\nwhere $\\beta > 0$ captures discrimination strength and $\\|\\cdot\\|_2$ is the Euclidean norm. Let encounters with the warning pattern update predator attack aversion in proportion to the fraction of unpalatable experiences among pattern encounters. Denote population frequencies of the model and mimic by $q$ and $p$ respectively, and unpalatability levels by $u_M \\in [0,1]$ for the model and $u_m \\in [0,1]$ for the mimic. Let $\\rho \\in (0,1]$ be the maximum attainable attack reduction from learning. Under these assumptions, define the frequency-dependent learning factor\n$$\nR = \\rho \\cdot \\frac{u_M q + u_m p}{q + p}.\n$$\nAssume that absolute fitness is proportional to the product of learned attack aversion and perceptual confusion, and hence can be taken (up to an arbitrary positive affine transformation) as\n$$\nW(\\mathbf{z}) = R \\, c(\\|\\mathbf{z} - \\mathbf{z}^\\star\\|_2).\n$$\nYou must compute the evolutionary trajectory by iterating the discrete-time Lande update from an initial state $\\mathbf{z}_0$, holding $\\mathbf{G}$ and the ecological parameters constant, until either the trait falls within a small Euclidean tolerance of the model point or a maximum number of iterations is reached. Formally, iterate\n$$\n\\mathbf{z}_{t+1} = \\mathbf{z}_t + \\mathbf{G}\\, \\nabla W(\\mathbf{z}_t),\n$$\nfor $t = 0,1,2,\\dots$, and stop at the smallest $t$ such that $\\|\\mathbf{z}_t - \\mathbf{z}^\\star\\|_2 \\le \\varepsilon$, or report failure if no such $t$ exists up to $N_{\\max}$.\n\nBase your implementation only on the definitions above and on standard properties of gradients and norms. Do not assume any additional formulas beyond these definitions and fundamental calculus. Your program must return, for each test case, the integer number of iterations required to satisfy $\\|\\mathbf{z}_t - \\mathbf{z}^\\star\\|_2 \\le \\varepsilon$; if the threshold is not reached within $N_{\\max}$ iterations, return the integer $-1$ for that test case.\n\nUse the following constants for all test cases: tolerance $\\varepsilon = 10^{-6}$ and maximum iterations $N_{\\max} = 10^5$. All test cases below use a two-dimensional trait ($m = 2$) and the model point $\\mathbf{z}^\\star = (0,0)$.\n\nImplement the algorithm and evaluate the following test suite. Each case specifies $(\\mathbf{z}_0, \\mathbf{G}, \\beta, \\rho, p, q, u_M, u_m)$:\n\n- Test case A (Batesian mimicry, moderate learning):\n  - $\\mathbf{z}_0 = (1.0, 0.5)$,\n  - $\\mathbf{G} = \\mathrm{diag}(0.02, 0.01)$,\n  - $\\beta = 3.0$, $\\rho = 0.9$,\n  - $p = 0.1$, $q = 0.3$,\n  - $u_M = 1.0$, $u_m = 0.0$.\n\n- Test case B (Batesian mimicry, extremely low genetic variance):\n  - $\\mathbf{z}_0 = (1.0, 1.0)$,\n  - $\\mathbf{G} = \\mathrm{diag}(10^{-8}, 10^{-8})$,\n  - $\\beta = 3.0$, $\\rho = 0.9$,\n  - $p = 0.7$, $q = 0.3$,\n  - $u_M = 1.0$, $u_m = 0.0$.\n\n- Test case C (Müllerian mimicry, anisotropic covariance):\n  - $\\mathbf{z}_0 = (2.0, -1.0)$,\n  - $\\mathbf{G} = \\begin{pmatrix}0.02 & 0.015\\\\ 0.015 & 0.03\\end{pmatrix}$,\n  - $\\beta = 2.0$, $\\rho = 0.8$,\n  - $p = 0.5$, $q = 0.5$,\n  - $u_M = 1.0$, $u_m = 0.6$.\n\n- Test case D (Batesian mimicry, correlated response):\n  - $\\mathbf{z}_0 = (-1.5, 1.0)$,\n  - $\\mathbf{G} = \\begin{pmatrix}0.01 & 0.005\\\\ 0.005 & 0.01\\end{pmatrix}$,\n  - $\\beta = 4.0$, $\\rho = 0.7$,\n  - $p = 0.2$, $q = 0.4$,\n  - $u_M = 1.0$, $u_m = 0.0$.\n\n- Test case E (Müllerian mimicry, strong discrimination):\n  - $\\mathbf{z}_0 = (3.0, 3.0)$,\n  - $\\mathbf{G} = \\mathrm{diag}(0.005, 0.005)$,\n  - $\\beta = 6.0$, $\\rho = 0.9$,\n  - $p = 0.4$, $q = 0.6$,\n  - $u_M = 1.0$, $u_m = 0.9$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result_A,result_B,result_C,result_D,result_E]\"). The results must be integers as specified above, in the order A through E.",
            "solution": "The problem requires the simulation of a multivariate trait's evolution under frequency-dependent selection, as described by Lande's discrete-time equation. The biological context is the evolution of a mimic's visual pattern, $\\mathbf{z} \\in \\mathbb{R}^m$, towards the pattern of a defended model organism, $\\mathbf{z}^\\star \\in \\mathbb{R}^m$. The evolutionary trajectory is determined by iterating the equation\n$$\n\\mathbf{z}_{t+1} = \\mathbf{z}_t + \\mathbf{G}\\, \\nabla W(\\mathbf{z}_t)\n$$\nwhere $\\mathbf{z}_t$ is the trait vector at generation $t$, $\\mathbf{G}$ is the genetic variance-covariance matrix, and $\\nabla W(\\mathbf{z}_t)$ is the selection gradient on the absolute fitness surface $W(\\mathbf{z})$.\n\nThe fitness function, $W(\\mathbf{z})$, is defined as the product of two biologically-motivated factors: $W(\\mathbf{z}) = R \\cdot c(\\|\\mathbf{z} - \\mathbf{z}^\\star\\|_2)$. The first term, $R$, is the frequency-dependent learning factor, given by\n$$\nR = \\rho \\cdot \\frac{u_M q + u_m p}{q + p}.\n$$\nThis term represents the strength of learned predator aversion, which depends on the frequencies ($p$, $q$) and unpalatabilities ($u_m$, $u_M$) of the mimic and model populations, respectively, scaled by a maximum learning effect $\\rho$. For a given set of ecological parameters, $R$ is a constant. The second term, $c(d) = \\exp(-\\beta d^2)$ with $d = \\|\\mathbf{z} - \\mathbf{z}^\\star\\|_2$, is a Gaussian function describing perceptual confusion. This function peaks at $d=0$ (i.e., when $\\mathbf{z} = \\mathbf{z}^\\star$) and decreases as the mimic's pattern diverges from the model's. The parameter $\\beta$ controls the steepness of this peak, representing the predator's discrimination ability. This fitness landscape has a single global maximum at $\\mathbf{z} = \\mathbf{z}^\\star$, which acts as the evolutionary attractor.\n\nTo implement the iterative update, we must first compute the gradient of the fitness function, $\\nabla W(\\mathbf{z})$. The gradient vector points in the direction of the steepest increase in fitness. Using the chain rule, and letting $\\mathbf{x} = \\mathbf{z} - \\mathbf{z}^\\star$, the $i$-th component of the gradient is:\n$$\n\\frac{\\partial W}{\\partial z_i} = \\frac{\\partial}{\\partial z_i} \\left[ R \\exp(-\\beta \\|\\mathbf{z} - \\mathbf{z}^\\star\\|_2^2) \\right] = R \\cdot \\exp(-\\beta \\|\\mathbf{z} - \\mathbf{z}^\\star\\|_2^2) \\cdot \\frac{\\partial}{\\partial z_i} (-\\beta \\sum_{j=1}^m (z_j - z_j^\\star)^2)\n$$\n$$\n\\frac{\\partial W}{\\partial z_i} = R \\cdot \\exp(-\\beta \\|\\mathbf{z} - \\mathbf{z}^\\star\\|_2^2) \\cdot (-2\\beta (z_i - z_i^\\star)) = -2\\beta R \\, c(\\|\\mathbf{z} - \\mathbf{z}^\\star\\|_2) (z_i - z_i^\\star).\n$$\nIn vector form, the gradient is:\n$$\n\\nabla W(\\mathbf{z}) = -2\\beta R \\exp(-\\beta \\|\\mathbf{z} - \\mathbf{z}^\\star\\|_2^2) (\\mathbf{z} - \\mathbf{z}^\\star).\n$$\nThe gradient is always directed towards the model point $\\mathbf{z}^\\star$, with its magnitude decreasing as the mimic trait $\\mathbf{z}$ approaches $\\mathbf{z}^\\star$.\n\nSubstituting the gradient into Lande's equation gives the explicit update rule for the simulation:\n$$\n\\mathbf{z}_{t+1} = \\mathbf{z}_t - 2\\beta R \\exp(-\\beta \\|\\mathbf{z}_t - \\mathbf{z}^\\star\\|_2^2) \\mathbf{G}(\\mathbf{z}_t - \\mathbf{z}^\\star).\n$$\nThis equation describes the response to selection. The selection pressure, proportional to $-(\\mathbf{z}_t - \\mathbf{z}^\\star)$, is pre-multiplied by the genetic variance-covariance matrix $\\mathbf{G}$. If $\\mathbf{G}$ is a scalar multiple of the identity matrix, the population evolves directly towards the optimum. However, if $\\mathbf{G}$ has off-diagonal elements (genetic correlations) or unequal variances on the diagonal, the evolutionary trajectory will be biased and may not follow the steepest ascent path.\n\nThe algorithm to solve the problem is a direct implementation of this iterative process. For each test case:\n$1.$ The constant parameters, including the initial trait $\\mathbf{z}_0$, matrices $\\mathbf{G}$, and scalar values $\\beta, \\rho, p, q, u_M, u_m$, are defined. The model point is fixed at $\\mathbf{z}^\\star=(0,0)$. The constants $\\varepsilon=10^{-6}$ and $N_{\\max}=10^5$ are used.\n$2.$ The constant learning factor $R$ is pre-calculated.\n$3.$ A loop is initiated to run for a maximum of $N_{\\max}$ iterations, with the iteration counter $t$ starting at $0$.\n$4.$ At the beginning of each iteration $t$ (for $t=0, 1, \\dots, N_{\\max}$), the Euclidean distance $\\|\\mathbf{z}_t - \\mathbf{z}^\\star\\|_2$ is computed.\n$5.$ This distance is compared to the tolerance $\\varepsilon$. If $\\|\\mathbf{z}_t - \\mathbf{z}^\\star\\|_2 \\le \\varepsilon$, the trait is considered to have converged. The simulation for this case terminates, and the current iteration count $t$ is recorded as the result.\n$6.$ If convergence has not been reached, the next trait vector $\\mathbf{z}_{t+1}$ is computed using the derived update formula. This involves calculating the exponential term, performing a matrix-vector product $\\mathbf{G}(\\mathbf{z}_t - \\mathbf{z}^\\star)$, scaling the result, and adding the update to $\\mathbf{z}_t$.\n$7.$ If the loop completes all $N_{\\max}+1$ checks without the distance falling below $\\varepsilon$, the simulation is deemed a failure. The recorded result for this case is $-1$.\n\nThis computational procedure accurately simulates the specified model of mimicry evolution, providing the number of generations required for the mimic's phenotype to become indistinguishable from the model's, subject to the given genetic and ecological constraints.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the evolutionary trajectory problem for a set of test cases.\n    \"\"\"\n\n    # Define global constants for all test cases.\n    epsilon = 1e-6\n    n_max = 100000\n    z_star = np.array([0.0, 0.0])\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {   # Test case A\n            \"z0\": np.array([1.0, 0.5]),\n            \"G\": np.diag([0.02, 0.01]),\n            \"beta\": 3.0, \"rho\": 0.9,\n            \"p\": 0.1, \"q\": 0.3,\n            \"u_M\": 1.0, \"u_m\": 0.0\n        },\n        {   # Test case B\n            \"z0\": np.array([1.0, 1.0]),\n            \"G\": np.diag([1e-8, 1e-8]),\n            \"beta\": 3.0, \"rho\": 0.9,\n            \"p\": 0.7, \"q\": 0.3,\n            \"u_M\": 1.0, \"u_m\": 0.0\n        },\n        {   # Test case C\n            \"z0\": np.array([2.0, -1.0]),\n            \"G\": np.array([[0.02, 0.015], [0.015, 0.03]]),\n            \"beta\": 2.0, \"rho\": 0.8,\n            \"p\": 0.5, \"q\": 0.5,\n            \"u_M\": 1.0, \"u_m\": 0.6\n        },\n        {   # Test case D\n            \"z0\": np.array([-1.5, 1.0]),\n            \"G\": np.array([[0.01, 0.005], [0.005, 0.01]]),\n            \"beta\": 4.0, \"rho\": 0.7,\n            \"p\": 0.2, \"q\": 0.4,\n            \"u_M\": 1.0, \"u_m\": 0.0\n        },\n        {   # Test case E\n            \"z0\": np.array([3.0, 3.0]),\n            \"G\": np.diag([0.005, 0.005]),\n            \"beta\": 6.0, \"rho\": 0.9,\n            \"p\": 0.4, \"q\": 0.6,\n            \"u_M\": 1.0, \"u_m\": 0.9\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        z = case[\"z0\"].astype(np.float64)\n        G = case[\"G\"].astype(np.float64)\n        beta = case[\"beta\"]\n        rho = case[\"rho\"]\n        p = case[\"p\"]\n        q = case[\"q\"]\n        u_M = case[\"u_M\"]\n        u_m = case[\"u_m\"]\n\n        # Calculate the constant learning factor R\n        R = rho * (u_M * q + u_m * p) / (q + p)\n\n        # Pre-calculate the constant part of the update term's scalar factor\n        # The update is z_{t+1} = z_t - 2*beta*R*exp(...) * G(z_t - z_star)\n        C = -2.0 * beta * R\n\n        iterations = -1\n        # Loop from t=0 to t=n_max. Check for convergence at the beginning of each step.\n        for t in range(n_max + 1):\n            # Calculate displacement vector and distance from the model point\n            d_vec = z - z_star\n            dist = np.linalg.norm(d_vec)\n\n            # Check for stopping condition\n            if dist = epsilon:\n                iterations = t\n                break\n            \n            # If t reaches n_max here, it means z_n_max did not converge,\n            # so we break loop and keep iterations = -1. This check is implicit\n            # in the for loop range.\n\n            # Calculate the full update vector for z_t -> z_{t+1}\n            # delta_z = G * grad(W(z))\n            # grad(W(z)) = -2*beta*R*exp(-beta*dist^2)*(z-z_star)\n            # scalar_part = -2*beta*R*exp(-beta*dist^2)\n            scalar_part = C * np.exp(-beta * dist**2)\n            delta_z = scalar_part * np.dot(G, d_vec)\n            \n            # Update the trait vector\n            z = z + delta_z\n        \n        results.append(iterations)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}