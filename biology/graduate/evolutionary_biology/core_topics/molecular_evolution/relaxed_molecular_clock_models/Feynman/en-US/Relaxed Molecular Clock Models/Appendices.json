{
    "hands_on_practices": [
        {
            "introduction": "Before exploring the complexities of relaxed molecular clocks, it is essential to first master the simpler model they were designed to replace. The strict molecular clock posits a single, constant rate of evolution, $r$, across all lineages of a phylogenetic tree, creating a direct proportionality between genetic distance and time. This foundational exercise allows you to practice the core calculations of rates and node ages under this idealized scenario , establishing a crucial benchmark for understanding the necessity and function of more sophisticated relaxed clock models.",
            "id": "2749326",
            "problem": "A four-taxon ultrametric phylogeny relates taxa $A$, $B$, $C$, and $D$ under a strict molecular clock. The topology is $((A,B),(C,D))$, and the tree is ultrametric with total tip-to-root time $T$ identical for all tips. A single calibration fixes the age of the root (the most recent common ancestor (MRCA) of all four taxa) at $T_{\\mathrm{root}} = 12$ million years (Myr). Along each path from the present to the root, the branch durations (in Myr) are as follows:\n- For taxa $A$ and $B$: each pendant branch from its clade MRCA to the tip has duration $7$, and the internal branch from that clade MRCA to the root has duration $5$.\n- For taxa $C$ and $D$: each pendant branch from its clade MRCA to the tip has duration $8$, and the internal branch from that clade MRCA to the root has duration $4$.\nBy construction, the tree is ultrametric, since $7+5=12$ and $8+4=12$.\n\nFrom an alignment analyzed under the Jukes-Cantor (JC69) model, the corrected distances along each complete tip-to-root path are estimated to be $d_{A}=0.84$, $d_{B}=0.84$, $d_{C}=0.84$, and $d_{D}=0.84$ substitutions per site.\n\nAssume a strict molecular clock with a single global rate $r$ (in substitutions per site per Myr) so that the expected number of substitutions per site accumulated along any lineage over a duration $\\Delta t$ is linear in time with proportionality constant $r$. Under the JC69 model, the corrected path distance equals the expected total number of substitutions per site accumulated along that path.\n\nCompute:\n1. The global rate $r$ in substitutions per site per Myr.\n2. The ages (in Myr before present) of the two internal nodes: the MRCA of $(A,B)$ and the MRCA of $(C,D)$.\n\nReport your three values in the order $\\big(r, \\text{age}_{(A,B)}, \\text{age}_{(C,D)}\\big)$. Round your answers to three significant figures. Express $r$ in substitutions per site per Myr and node ages in Myr. Do not include units in your final reported tuple.",
            "solution": "The problem will first be validated for correctness and consistency.\n\n**Step 1: Extract Givens**\n- **Topology**: A four-taxon tree with topology $((A,B),(C,D))$.\n- **Clock Model**: A strict molecular clock with a single global rate $r$.\n- **Tree Property**: The phylogeny is ultrametric.\n- **Root Age**: The age of the root (MRCA of $(A,B,C,D)$) is $T_{\\mathrm{root}} = 12$ million years (Myr).\n- **Branch Durations for $(A,B)$ clade**:\n    - Pendant branch duration (MRCA of $(A,B)$ to tip): $7$ Myr.\n    - Internal branch duration (root to MRCA of $(A,B)$): $5$ Myr.\n- **Branch Durations for $(C,D)$ clade**:\n    - Pendant branch duration (MRCA of $(C,D)$ to tip): $8$ Myr.\n    - Internal branch duration (root to MRCA of $(C,D)$): $4$ Myr.\n- **Path Time Consistency**: The total time from root to any tip is constant: $7+5=12$ Myr and $8+4=12$ Myr. This is consistent with the ultrametric property and root age.\n- **Genetic Distances**: Corrected tip-to-root distances under the Jukes-Cantor (JC69) model are $d_{A}=0.84$, $d_{B}=0.84$, $d_{C}=0.84$, and $d_{D}=0.84$ substitutions per site.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It describes a standard scenario in molecular phylogenetics. The premise is a strict molecular clock, where genetic distance accumulates linearly with time at a constant rate $r$ across all lineages. The tree is specified as ultrametric, meaning the total time from the root to each tip is identical. The provided branch durations ($7+5=12$ and $8+4=12$) are consistent with the stated root age of $T_{\\mathrm{root}} = 12$ Myr. The corrected genetic distances from root to tip are given as equal for all taxa ($d=0.84$), which is a necessary consequence of a strict clock on an ultrametric tree. The problem is internally consistent and contains all necessary information to compute the requested quantities. There are no logical contradictions or factual inaccuracies.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Solution Derivation**\n\nThe problem requires the computation of three values: the global substitution rate $r$, and the ages of the two internal nodes, which are the most recent common ancestors (MRCAs) of the clades $(A,B)$ and $(C,D)$.\n\n**1. Computation of the Global Rate, $r$**\nUnder a strict molecular clock, the expected genetic distance $d$ (in substitutions per site) accumulated along a lineage is directly proportional to the time duration $T$ (in Myr). The constant of proportionality is the substitution rate $r$ (in substitutions per site per Myr). The relationship is expressed as:\n$$d = r \\times T$$\nThe problem states that the total time from the root to any tip is $T = 12$ Myr. It also provides the corresponding corrected genetic distance for any root-to-tip path as $d = 0.84$ substitutions per site. We can therefore solve for the rate $r$:\n$$r = \\frac{d}{T}$$\nSubstituting the given values:\n$$r = \\frac{0.84}{12} \\text{ substitutions per site per Myr}$$\n$$r = 0.07 \\text{ substitutions per site per Myr}$$\nThe problem requires the answer to be rounded to three significant figures. In standard scientific notation, this is:\n$$r = 7.00 \\times 10^{-2} \\text{ substitutions per site per Myr}$$\n\n**2. Computation of Node Ages**\nThe age of a node in a phylogeny is defined as the time from that node to the present. The tips of the tree represent the present time, which can be set to time $0$.\n\n**A. Age of the MRCA of $(A,B)$**\nThe problem states that the duration of a pendant branch for taxa $A$ and $B$, which is the branch from their MRCA to a tip, is $7$ Myr. By definition, this duration represents the time elapsed from the MRCA event to the present. Therefore, the age of the MRCA of $(A,B)$ is:\n$$\\text{age}_{(A,B)} = 7 \\text{ Myr}$$\nThis can be verified using the internal branch information. The root has an age of $12$ Myr. The internal branch connecting the root to the MRCA of $(A,B)$ has a duration of $5$ Myr. The age of the node is the age of the parent node minus the duration of the connecting branch:\n$$\\text{age}_{(A,B)} = T_{\\mathrm{root}} - \\Delta t_{\\text{internal}, (A,B)} = 12 - 5 = 7 \\text{ Myr}$$\nThe values are consistent. To three significant figures, the age is $7.00$ Myr.\n\n**B. Age of the MRCA of $(C,D)$**\nSimilarly, for taxa $C$ and $D$, the duration of a pendant branch from their MRCA to a tip is given as $8$ Myr. This duration is the age of the node.\n$$\\text{age}_{(C,D)} = 8 \\text{ Myr}$$\nVerifying this with the internal branch duration of $4$ Myr:\n$$\\text{age}_{(C,D)} = T_{\\mathrm{root}} - \\Delta t_{\\text{internal}, (C,D)} = 12 - 4 = 8 \\text{ Myr}$$\nThe values are again consistent. To three significant figures, the age is $8.00$ Myr.\n\n**Final Values**\nThe requested values in the specified order $\\big(r, \\text{age}_{(A,B)}, \\text{age}_{(C,D)}\\big)$ and rounded to three significant figures are:\n- $r = 7.00 \\times 10^{-2}$\n- $\\text{age}_{(A,B)} = 7.00$\n- $\\text{age}_{(C,D)} = 8.00$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n7.00 \\times 10^{-2} & 7.00 & 8.00\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "This practice moves from the rigid assumptions of the strict clock to the flexible framework of an uncorrelated lognormal (UCLN) relaxed clock, where each branch evolves at its own rate. You will directly engage with the mechanics of Bayesian inference by finding the maximum a posteriori (MAP) estimate for model parameters, combining information from sequence data with prior beliefs about rates and node ages. Through this coding-based exercise , you will gain critical, hands-on experience showing how calibration priors constrain the timescale and how an analysis's results can be highly sensitive to their specification.",
            "id": "2749338",
            "problem": "A rooted phylogenetic tree with $5$ taxa is considered under an Uncorrelated Lognormal (UCLN) relaxed molecular clock model. The objective is to evaluate how a calibration prior with a soft maximum bound on the root age affects posterior modes of rates and ages, using a simplified observation model for branch lengths and a fixed topology. The program you write must compute, for multiple calibration settings, the posterior mode (maximum a posteriori, MAP) of the root age and the posterior mode of the mean branch rate, by solving a constrained optimization problem.\n\nThe fundamental base is as follows.\n\n- Substitutions accumulate along a branch as the product of an instantaneous rate and elapsed time, so that the expected branch length (in substitutions per site) equals rate times time. For an individual branch $i$ with rate $r_i$ and duration $d_i$, the expected length is $r_i d_i$.\n- For a fixed topology with branch durations linked by node ages, the observed branch lengths $\\{b_i\\}$ are treated as noisy measurements of the expected lengths. Under a many-sites approximation (Central Limit Theorem), a Gaussian observation model is used: \n  $$b_i \\mid r_i, d_i \\sim \\mathcal{N}(r_i d_i, \\sigma^2),$$\n  with known variance $\\sigma^2$ shared across branches.\n- Under the Uncorrelated Lognormal (UCLN) model, branch-specific rates are independent and lognormally distributed: \n  $$\\log r_i \\sim \\mathcal{N}(\\mu_r, s_r^2),$$\n  independently for each branch $i$.\n- The root age $t_R$ has a lognormal base prior \n  $$\\log t_R \\sim \\mathcal{N}(\\mu_T, s_T^2),$$ \n  multiplied by a soft maximum bound penalty at maximum $M$, \n  $$w(t_R; M, k) = \\exp\\!\\big(-k \\cdot \\max(0, t_R - M)\\big),$$ \n  where $k$ controls the strength of the soft bound. Other internal node ages have a diffuse uniform prior over the feasible region defined by tree-order constraints.\n\nTree topology and parameterization.\n\n- The rooted bifurcating topology is $((A,B),(C,(D,E)))$ with root $R$, and internal nodes $X=\\mathrm{MRCA}(A,B)$, $Y=\\mathrm{MRCA}(D,E)$, $Z=\\mathrm{MRCA}(C,Y)$.\n- Let the internal node ages be $t_R$, $t_{AB}$, $t_{CDE}$, and $t_{DE}$ for nodes $R$, $X$, $Z$, and $Y$, respectively. Tip ages are $0$. The constraints are \n  $$t_R \\ge t_{AB} \\ge 0,\\quad t_R \\ge t_{CDE} \\ge t_{DE} \\ge 0.$$\n- The $8$ branches and their durations are:\n  - Branch $1$: $R \\to X$ with duration $d_1 = t_R - t_{AB}$.\n  - Branch $2$: $R \\to Z$ with duration $d_2 = t_R - t_{CDE}$.\n  - Branch $3$: $X \\to A$ with duration $d_3 = t_{AB} - 0$.\n  - Branch $4$: $X \\to B$ with duration $d_4 = t_{AB} - 0$.\n  - Branch $5$: $Z \\to C$ with duration $d_5 = t_{CDE} - 0$.\n  - Branch $6$: $Z \\to Y$ with duration $d_6 = t_{CDE} - t_{DE}$.\n  - Branch $7$: $Y \\to D$ with duration $d_7 = t_{DE} - 0$.\n  - Branch $8$: $Y \\to E$ with duration $d_8 = t_{DE} - 0$.\n\nData and hyperparameters.\n\n- The observed branch lengths (in substitutions per site) are fixed at \n  $$\\mathbf{b} = (b_1,\\dots,b_8) = (0.0044,\\, 0.0019,\\, 0.0063,\\, 0.0060,\\, 0.0096,\\, 0.0045,\\, 0.0030,\\, 0.00255).$$\n- The observation noise standard deviation is $\\sigma = 0.0002$ (so variance $\\sigma^2 = 4\\times 10^{-8}$).\n- The UCLN rate prior parameters are $\\mu_r = \\log(0.01)$ and $s_r = 0.25$.\n- The base lognormal prior parameters for the root age are $\\mu_T = \\log(1.0) = 0$ and $s_T = 0.5$.\n\nPosterior and optimization.\n\n- Let $\\mathbf{r} = (r_1,\\dots,r_8)$ and $\\mathbf{t} = (t_R,t_{AB},t_{CDE},t_{DE})$. The posterior density (up to a normalizing constant that does not depend on the variables) is \n  $$p(\\mathbf{r}, \\mathbf{t} \\mid \\mathbf{b}) \\propto \\left[\\prod_{i=1}^8 \\exp\\!\\left(-\\frac{(b_i - r_i d_i)^2}{2 \\sigma^2}\\right)\\right] \\cdot \\left[\\prod_{i=1}^8 \\frac{1}{r_i} \\exp\\!\\left(-\\frac{(\\log r_i - \\mu_r)^2}{2 s_r^2}\\right)\\right] \\cdot \\frac{1}{t_R}\\exp\\!\\left(-\\frac{(\\log t_R - \\mu_T)^2}{2 s_T^2}\\right)\\cdot w(t_R;M,k),$$\n  subject to the tree-order constraints on $\\mathbf{t}$ and positivity of $\\mathbf{r}$. Your program should compute the joint MAP $(\\mathbf{r}^\\star, \\mathbf{t}^\\star)$ for each calibration setting $(M,k)$ by minimizing the negative log posterior.\n- The negative log posterior to be minimized (dropping additive constants not depending on parameters) is\n  $$\\mathcal{L}(\\mathbf{r}, \\mathbf{t}) = \\sum_{i=1}^8 \\frac{(b_i - r_i d_i)^2}{2 \\sigma^2} + \\sum_{i=1}^8 \\left[\\frac{(\\log r_i - \\mu_r)^2}{2 s_r^2} + \\log r_i\\right] + \\left[\\frac{(\\log t_R - \\mu_T)^2}{2 s_T^2} + \\log t_R\\right] + k \\cdot \\max(0, t_R - M),$$\n  with constraints $t_R \\ge t_{AB} \\ge 0$, $t_R \\ge t_{CDE} \\ge t_{DE} \\ge 0$, and $r_i > 0$ for all $i$.\n\nTasks.\n\n- Implement a constrained optimization to obtain the joint MAP for each calibration pair $(M,k)$ in the test suite.\n- For each MAP solution, extract:\n  - The root age posterior mode $t_R^\\star$ (in time units), and\n  - The posterior mode of the mean branch rate $\\bar{r}^\\star = \\frac{1}{8}\\sum_{i=1}^8 r_i^\\star$ (in substitutions per site per time unit).\n- Use the following test suite of calibration settings:\n  - Case $1$ (loose bound): $M = 2.0$, $k = 0.1$.\n  - Case $2$ (moderate bound): $M = 1.0$, $k = 5.0$.\n  - Case $3$ (very tight bound): $M = 0.7$, $k = 20.0$.\n\nScientific realism requirements.\n\n- The solution should respect all constraints on node ages to keep branch durations nonnegative.\n- All ages must be reported in time units, and all rates in substitutions per site per time unit.\n- Angles are not involved.\n- Express any ratios or proportions as decimal numbers.\n\nFinal output specification.\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- For each test case $i$ in the order above, output first $t_R^\\star$ (in time units), then $\\bar{r}^\\star$ (in substitutions per site per time unit).\n- Round each numeric value to six decimal places before printing.\n- The final output therefore must have $2 \\times 3 = 6$ floats in this order: $[t_{R,1}^\\star, \\bar{r}_1^\\star, t_{R,2}^\\star, \\bar{r}_2^\\star, t_{R,3}^\\star, \\bar{r}_3^\\star]$.",
            "solution": "We begin from the fundamental relationship that the expected number of substitutions per site accumulated along a branch equals the product of the instantaneous substitution rate and the elapsed time along that branch. Denote by $r_i$ the rate on branch $i$ and by $d_i$ the duration (time length) of branch $i$. The expected branch length is then $r_i d_i$. We assume measured branch lengths $b_i$ from sequence data alignments are approximately Gaussian-distributed around the expectation with variance $\\sigma^2$ shared across branches: $b_i \\mid r_i, d_i \\sim \\mathcal{N}(r_i d_i, \\sigma^2)$. This Gaussian approximation can be justified by the Central Limit Theorem applied across many sites under standard substitution models, producing near-normal estimation errors for branch lengths.\n\nUnder the Uncorrelated Lognormal (UCLN) relaxed clock model, the branch-specific rates are independent with $\\log r_i \\sim \\mathcal{N}(\\mu_r, s_r^2)$. This prior captures rate heterogeneity among branches while centering around a baseline rate scale set by $\\mu_r$. For times, we place a base lognormal prior on the root age $t_R$, $\\log t_R \\sim \\mathcal{N}(\\mu_T, s_T^2)$, reflecting positive support and multiplicative variability. To model a soft maximum bound at $M$, we multiply the base prior by a penalty factor $w(t_R; M, k) = \\exp\\big(-k \\max(0, t_R - M)\\big)$; for $t_R \\le M$ the penalty equals $1$, whereas for $t_R > M$ the prior density decays approximately exponentially with rate $k$. The remaining internal node ages receive a diffuse uniform prior over the feasible set defined by the tree-order constraints. These constraints ensure all branch durations are nonnegative: $t_R \\ge t_{AB} \\ge 0$ and $t_R \\ge t_{CDE} \\ge t_{DE} \\ge 0$.\n\nCombining the likelihood and priors via Bayes' theorem yields a posterior density proportional to their product. Dropping normalizing constants that do not involve the parameters, the negative log posterior to be minimized for the joint MAP estimator is\n$$\n\\mathcal{L}(\\mathbf{r}, \\mathbf{t})\n=\n\\sum_{i=1}^8 \\frac{(b_i - r_i d_i)^2}{2 \\sigma^2}\n+ \\sum_{i=1}^8 \\left[\\frac{(\\log r_i - \\mu_r)^2}{2 s_r^2} + \\log r_i\\right]\n+ \\left[\\frac{(\\log t_R - \\mu_T)^2}{2 s_T^2} + \\log t_R\\right]\n+ k \\cdot \\max(0, t_R - M),\n$$\nsubject to $r_i > 0$ for all $i$ and the age constraints above. The first term contracts the prediction errors between observed $b_i$ and expected $r_i d_i$. The second term implements the UCLN prior with the $\\log r_i$ quadratic penalty and the Jacobian term $\\log r_i$. The third term is the base lognormal prior on $t_R$ with its Jacobian $\\log t_R$. The final term is the soft bound penalty, which activates when $t_R$ exceeds $M$ and is scaled by $k$.\n\nThe durations $d_i$ are linear functions of the node ages $\\mathbf{t} = (t_R, t_{AB}, t_{CDE}, t_{DE})$:\n- $d_1 = t_R - t_{AB}$,\n- $d_2 = t_R - t_{CDE}$,\n- $d_3 = t_{AB}$,\n- $d_4 = t_{AB}$,\n- $d_5 = t_{CDE}$,\n- $d_6 = t_{CDE} - t_{DE}$,\n- $d_7 = t_{DE}$,\n- $d_8 = t_{DE}$.\n\nAlgorithmic design proceeds as follows.\n\n- Parameterization and constraints: We optimize over the vector $(\\log r_1,\\dots,\\log r_8, t_R, t_{AB}, t_{CDE}, t_{DE})$, using explicit bounds $r_i > 0$ via the logarithmic parameterization and linear inequality constraints to enforce the ancestral age ordering, namely $t_R - t_{AB} \\ge \\epsilon$, $t_R - t_{CDE} \\ge \\epsilon$, $t_{CDE} - t_{DE} \\ge \\epsilon$, $t_{AB} \\ge \\epsilon$, $t_{DE} \\ge \\epsilon$ with a small $\\epsilon > 0$ to maintain strict positivity of durations. Additional box bounds keep ages within a reasonable search domain.\n- Objective evaluation: For given parameters, compute durations $d_i$, rates $r_i = \\exp(\\log r_i)$, and the Gaussian residual sum of squares term with fixed $\\sigma^2$, then add the UCLN and base lognormal penalties, and the soft maximum penalty $k \\max(0, t_R - M)$. If any duration is nonpositive, apply a large penalty to enforce feasibility.\n- Numerical optimization: Use constrained nonlinear optimization to minimize $\\mathcal{L}$. A method such as Sequential Least Squares Programming (SLSQP) handles smooth objectives with linear inequality constraints and box bounds. The soft bound term is continuous but nonsmooth at $t_R = M$, which SLSQP tolerates in practice.\n- Initialization: Start near the baseline rate and a plausible age configuration: $\\log r_i \\approx \\log(0.01)$ for all $i$, $t_R \\approx 1.0$, $t_{AB} \\approx 0.6$, $t_{CDE} \\approx 0.8$, $t_{DE} \\approx 0.3$.\n- Outputs: At the optimizer’s solution $(\\mathbf{r}^\\star, \\mathbf{t}^\\star)$, compute $t_R^\\star$ and $\\bar{r}^\\star = \\frac{1}{8}\\sum_{i=1}^8 r_i^\\star$. Round each to six decimals.\n\nInterpretation of the test suite cases:\n\n- Case $1$ with $M=2.0$ and $k=0.1$ places a weak, loose upper bound on $t_R$, so the likelihood and the UCLN prior predominantly determine $t_R^\\star$ and $\\bar{r}^\\star$.\n- Case $2$ with $M=1.0$ and $k=5.0$ anchors $t_R$ near $1.0$ time units, pulling the MAP root age downward if the likelihood alone would prefer a larger value, and correspondingly adjusting $\\bar{r}^\\star$ upward or downward to reconcile $r_i d_i$ with the observed $b_i$.\n- Case $3$ with $M=0.7$ and $k=20.0$ imposes a very tight soft maximum; the optimizer must compress $t_R^\\star$ below or near $0.7$, thereby inflating the rates to maintain $r_i d_i \\approx b_i$. Consequently, we expect $t_R^\\star$ to decrease and $\\bar{r}^\\star$ to increase relative to the looser cases.\n\nThe program implements this constrained MAP estimation for each $(M,k)$ pair and prints a single-line list of the six rounded values in the specified order. Ages are reported in time units, and rates are in substitutions per site per time unit.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize, NonlinearConstraint, Bounds\n\ndef solve():\n    # Data: observed branch lengths (substitutions per site)\n    b = np.array([0.0044, 0.0019, 0.0063, 0.0060, 0.0096, 0.0045, 0.0030, 0.00255], dtype=float)\n    sigma = 0.0002  # observation noise std\n\n    # UCLN prior hyperparameters\n    mu_r = np.log(0.01)\n    s_r = 0.25\n\n    # Root age base lognormal prior parameters\n    mu_T = 0.0  # log(1.0)\n    s_T = 0.5\n\n    # Tree structure: durations from node ages\n    def durations(t):\n        tR, tAB, tCDE, tDE = t\n        d = np.empty(8, dtype=float)\n        d[0] = tR - tAB       # R->X\n        d[1] = tR - tCDE      # R->Z\n        d[2] = tAB            # X->A\n        d[3] = tAB            # X->B\n        d[4] = tCDE           # Z->C\n        d[5] = tCDE - tDE     # Z->Y\n        d[6] = tDE            # Y->D\n        d[7] = tDE            # Y->E\n        return d\n\n    # Soft penalty term for root age: k * max(0, tR - M)\n    def soft_penalty(tR, M, k):\n        excess = tR - M\n        return k * excess if excess > 0.0 else 0.0\n\n    # Negative log posterior (up to additive constant)\n    def neg_log_posterior(x, M, k):\n        # x = [log r_1..log r_8, tR, tAB, tCDE, tDE]\n        y = x[:8]\n        t = x[8:]\n        tR, tAB, tCDE, tDE = t\n        # Enforce basic positivity inside objective to avoid invalid logs\n        if tR <= 0 or tAB <= 0 or tCDE <= 0 or tDE <= 0:\n            return 1e20\n        d = durations(t)\n        # If any duration is non-positive, penalize heavily (constraints should prevent this)\n        if np.any(d <= 0):\n            return 1e20\n        r = np.exp(y)\n        # Likelihood term\n        resid = b - r * d\n        ll_term = 0.5 * np.sum((resid / sigma) ** 2)\n        # UCLN prior on rates: sum_i [ (y_i - mu_r)^2/(2 s_r^2) + y_i ]\n        prior_rates = np.sum(((y - mu_r) ** 2) / (2.0 * s_r ** 2) + y)\n        # Root age base lognormal prior: [(log tR - mu_T)^2/(2 s_T^2) + log tR]\n        prior_root = ((np.log(tR) - mu_T) ** 2) / (2.0 * s_T ** 2) + np.log(tR)\n        # Soft maximum penalty\n        soft = soft_penalty(tR, M, k)\n        return ll_term + prior_rates + prior_root + soft\n\n    # Constraints: enforce tree-order and positivity of durations\n    eps = 1e-6\n    def cons_fun(x):\n        tR, tAB, tCDE, tDE = x[8], x[9], x[10], x[11]\n        # c >= 0\n        return np.array([\n            tR - tAB - eps,\n            tR - tCDE - eps,\n            tCDE - tDE - eps,\n            tAB - eps,\n            tDE - eps\n        ], dtype=float)\n\n    nonlin_con = NonlinearConstraint(cons_fun, lb=0.0, ub=np.inf)\n\n    # Bounds: for log rates and ages\n    y_lower = np.full(8, np.log(1e-5))\n    y_upper = np.full(8, np.log(1.0))  # upper rate bound 1 subs/site/time unit\n    # Ages between tiny positive and 5.0 time units\n    t_lower = np.array([1e-6, 1e-6, 1e-6, 1e-6], dtype=float)\n    t_upper = np.array([5.0, 5.0, 5.0, 5.0], dtype=float)\n    lower_bounds = np.concatenate([y_lower, t_lower])\n    upper_bounds = np.concatenate([y_upper, t_upper])\n    bounds = Bounds(lower_bounds, upper_bounds)\n\n    # Initial guess: log rates near baseline, ages plausible\n    y0 = np.full(8, mu_r)\n    t0 = np.array([1.0, 0.6, 0.8, 0.3], dtype=float)\n    x0 = np.concatenate([y0, t0])\n\n    # Test suite: (M, k) pairs\n    test_cases = [\n        (2.0, 0.1),  # loose\n        (1.0, 5.0),  # moderate\n        (0.7, 20.0)  # very tight\n    ]\n\n    results = []\n    for M, k in test_cases:\n        obj = lambda x: neg_log_posterior(x, M, k)\n        res = minimize(\n            obj,\n            x0,\n            method='SLSQP',\n            bounds=bounds,\n            constraints=[nonlin_con],\n            options={'maxiter': 2000, 'ftol': 1e-12, 'disp': False}\n        )\n        # If not successful, try a different method or perturb initial guess\n        if not res.success:\n            # Slight perturbation and retry\n            x0_perturb = x0.copy()\n            x0_perturb[:8] += np.random.default_rng(42).normal(scale=0.05, size=8)\n            x0_perturb[8:] += np.array([0.05, -0.05, 0.0, 0.0])\n            res = minimize(\n                obj,\n                x0_perturb,\n                method='SLSQP',\n                bounds=bounds,\n                constraints=[nonlin_con],\n                options={'maxiter': 3000, 'ftol': 1e-12, 'disp': False}\n            )\n\n        x_star = res.x\n        y_star = x_star[:8]\n        t_star = x_star[8:]\n        r_star = np.exp(y_star)\n        tR_star = t_star[0]\n        rbar_star = float(np.mean(r_star))\n\n        # Round to six decimals\n        tR_rounded = float(f\"{tR_star:.6f}\")\n        rbar_rounded = float(f\"{rbar_star:.6f}\")\n        results.append(tR_rounded)\n        results.append(rbar_rounded)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(lambda v: f'{v:.6f}', results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A successful Bayesian analysis depends not only on a valid model but also on a robust computational implementation. This exercise tackles the critical, real-world challenge of diagnosing and improving the performance of the Markov chain Monte Carlo (MCMC) algorithm at the heart of the inference process. Faced with a scenario of poor MCMC mixing, you must identify scientifically justified tuning strategies that enhance sampling efficiency without altering the target posterior distribution . Mastering these techniques is essential for moving beyond a \"black-box\" approach to produce reliable results in modern phylogenetic research.",
            "id": "2749274",
            "problem": "A researcher is performing Bayesian phylogenetic inference under an uncorrelated lognormal (UCLN) relaxed molecular clock on a fixed tree topology. Each branch-specific substitution rate $r_i$ has prior $\\log r_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$ independently across branches, with hyperparameters $\\mu$ and $\\sigma$ given proper hyperpriors. The posterior over parameters $\\theta = (\\{r_i\\}, \\mu, \\sigma, \\ldots)$ is explored by Markov chain Monte Carlo (MCMC) using Metropolis–Hastings updates for the rates. The researcher observes that the acceptance probability for rate-update proposals is very low (far below $0.1$), and that effective sample sizes for $\\{r_i\\}$ are poor despite a long run.\n\nStarting from foundational principles of Bayesian inference and Markov chain Monte Carlo, identify which of the following adjustments are scientifically justified as tuning changes that can improve mixing of the rate parameters in a UCLN analysis without changing the target posterior distribution. Select all options that apply.\n\nA. Decrease the multiplicative scale of the rate-scaling proposals so that the induced step size in $\\log r_i$ has smaller variance, targeting a moderate acceptance rate (e.g., between $0.2$ and $0.4$), while keeping the same prior and likelihood.\n\nB. Reparameterize the model by defining standardized log-rates $z_i = (\\log r_i - \\mu)/\\sigma$ and using a symmetric random-walk proposal on $z_i$, with proper accounting for the Jacobian and consistent updates of $(\\mu, \\sigma)$, leaving the prior and likelihood unchanged.\n\nC. Use an adaptive scheme during burn-in (e.g., Robbins–Monro type adaptation) to tune the proposal scale for rate updates toward a target acceptance probability, and then freeze the proposal parameters for the production phase.\n\nD. Increase the prior variance of the UCLN by replacing $\\sigma^2$ with $10\\sigma^2$ to make larger proposed changes to $\\{r_i\\}$ more acceptable.\n\nE. Increase thinning (e.g., keeping every $100$th sample) so that autocorrelation in the saved $\\{r_i\\}$ draws is lower.\n\nF. Introduce a joint block-update for $(\\log r_i, \\sigma)$ using a reversible, approximately multivariate normal proposal with covariance estimated from a short pilot run and then fixed, so that proposals align with posterior correlations.\n\nG. Replace the original likelihood with a power-likelihood $L^\\beta$ for $\\beta = 0.8$ throughout the run, so that proposals are more often accepted.\n\nH. Employ Metropolis–coupled Markov chain Monte Carlo (parallel tempering) with a ladder of heated chains at inverse temperatures $\\beta_k < 1$ that exchange states with the cold chain, and only retain samples from the cold chain at $\\beta = 1$.",
            "solution": "The problem statement is subjected to validation.\n\n**1. Extraction of Givens:**\n- **Model:** Bayesian phylogenetic inference with an uncorrelated lognormal (UCLN) relaxed molecular clock on a fixed tree topology.\n- **Prior:** Branch-specific rates $r_i$ are drawn from a lognormal distribution, such that $\\log r_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$ independently for each branch $i$.\n- **Hyperpriors:** Proper hyperpriors are assigned to the hyperparameters $\\mu$ and $\\sigma$.\n- **Inference Method:** Markov chain Monte Carlo (MCMC) is used to explore the posterior distribution of parameters $\\theta = (\\{r_i\\}, \\mu, \\sigma, \\ldots)$.\n- **Update Scheme:** Metropolis–Hastings updates are used for the rate parameters $\\{r_i\\}$.\n- **Problem:** The acceptance probability for rate update proposals is very low (less than $0.1$), and the effective sample sizes (ESS) for the $\\{r_i\\}$ parameters are poor.\n- **Objective:** Identify scientifically justified MCMC tuning adjustments that improve mixing of the rate parameters without altering the target posterior distribution.\n\n**2. Validation:**\nThe problem statement is scientifically grounded, well-posed, and objective. It describes a common and realistic challenge in applied Bayesian statistics, specifically in the field of phylogenetics. The UCLN model, MCMC inference, Metropolis–Hastings updates, acceptance probabilities, and effective sample sizes are all standard and well-defined concepts. The objective is clearly specified: improve MCMC mixing efficiency while ensuring the target posterior distribution remains unchanged. The problem contains no scientific or logical contradictions, no missing critical information, and is not trivial.\n\n**3. Verdict:**\nThe problem is valid. We may proceed to the solution.\n\n---\n\nThe fundamental basis of this problem is Bayesian inference, where the objective is to characterize the posterior distribution of a set of parameters $\\theta$ given data $D$. This posterior is given by Bayes' theorem:\n$$ p(\\theta|D) = \\frac{L(D|\\theta) p(\\theta)}{p(D)} \\propto L(D|\\theta) p(\\theta) $$\nwhere $L(D|\\theta)$ is the likelihood function and $p(\\theta)$ is the prior distribution.\n\nMCMC methods generate samples from this posterior distribution. The Metropolis–Hastings algorithm, a general MCMC method, generates a sequence of states $\\theta^{(t)}$ by proposing a new state $\\theta'$ from a proposal distribution $q(\\theta'|\\theta^{(t)})$ and accepting it with probability:\n$$ \\alpha(\\theta'|\\theta^{(t)}) = \\min \\left( 1, \\frac{p(\\theta'|D) q(\\theta^{(t)}|\\theta')}{p(\\theta^{(t)}|D) q(\\theta'|\\theta^{(t)})} \\right) = \\min \\left( 1, \\frac{L(D|\\theta') p(\\theta') q(\\theta^{(t)}|\\theta')}{L(D|\\theta^{(t)}) p(\\theta^{(t)}) q(\\theta'|\\theta^{(t)})} \\right) $$\nA very low acceptance probability, as stated in the problem (far below $0.1$), implies that proposed states $\\theta'$ are frequently rejected because they fall in regions of much lower posterior probability density compared to the current state $\\theta^{(t)}$. This is typically caused by a proposal distribution $q$ that proposes steps that are too large, leading to inefficient exploration of the parameter space and thus poor mixing and low effective sample sizes (ESS).\n\nThe core task is to evaluate proposed adjustments based on whether they improve mixing efficiency *without* altering the target posterior distribution, $p(\\theta|D)$.\n\n**A. Decrease the multiplicative scale of the rate-scaling proposals so that the induced step size in $\\log r_i$ has smaller variance, targeting a moderate acceptance rate (e.g., between $0.2$ and $0.4$), while keeping the same prior and likelihood.**\nThe proposal distribution $q$ is an artifact of the MCMC algorithm; it is not part of the statistical model (defined by the likelihood and prior). Modifying $q$ is the primary mechanism for \"tuning\" a Metropolis–Hastings sampler. A very low acceptance rate for a symmetric random-walk proposal (such as on $\\log r_i$) indicates the proposal variance (or step size) is too large. Reducing this variance makes proposals smaller and more local, increasing the probability that the proposed state has a posterior density similar to the current state, thereby increasing the acceptance rate. Theoretical and empirical work suggests that an optimal acceptance rate for many random-walk Metropolis algorithms is in the range of $0.2-0.4$ (e.g., $0.234$ in high dimensions). This adjustment directly addresses the observed problem without changing $L(D|\\theta)$ or $p(\\theta)$.\nVerdict: **Correct**. This is a standard and essential MCMC tuning procedure.\n\n**B. Reparameterize the model by defining standardized log-rates $z_i = (\\log r_i - \\mu)/\\sigma$ and using a symmetric random-walk proposal on $z_i$, with proper accounting for the Jacobian and consistent updates of $(\\mu, \\sigma)$, leaving the prior and likelihood unchanged.**\nReparameterization is a technique that changes the variables on which the MCMC sampler operates, without changing the underlying model. In the original parameterization, the draws of $\\log r_i$ are correlated with the hyperparameters $\\mu$ and $\\sigma$ in the posterior. By defining $z_i = (\\log r_i - \\mu)/\\sigma$, we work with variables whose prior is a standard normal distribution, $z_i \\sim \\mathcal{N}(0, 1)$, independent of $\\mu$ and $\\sigma$. Proposing updates on $z_i$ and transforming back to $r_i$ via $r_i = \\exp(\\mu + \\sigma z_i)$ can break the posterior correlations and lead to more efficient exploration. As long as the posterior density is correctly formulated in terms of the new parameters (which includes accounting for the Jacobian of the transformation if necessary for the specific M-H ratio calculation), the sampler still targets the same posterior distribution over the original parameters of interest. The phrase \"leaving the prior and likelihood unchanged\" signifies that the probabilistic model itself is not modified.\nVerdict: **Correct**. This is a valid and often highly effective strategy for improving mixing in hierarchical models.\n\n**C. Use an adaptive scheme during burn-in (e.g., Robbins–Monro type adaptation) to tune the proposal scale for rate updates toward a target acceptance probability, and then freeze the proposal parameters for the production phase.**\nThis is a form of adaptive MCMC. Standard MCMC theory requires a time-homogeneous transition kernel to guarantee convergence to a unique stationary distribution. An adaptive algorithm, where the proposal distribution $q$ changes based on the chain's history, violates this requirement. However, if adaptation is confined to the initial burn-in phase and the proposal distribution is fixed for the subsequent production run, the production phase of the chain is a standard Markov chain with a fixed transition kernel. It therefore converges to the correct target posterior. The adaptation during burn-in serves to automate the tuning process described in option A, finding a near-optimal proposal scale.\nVerdict: **Correct**. This is a widely used and theoretically sound method for automated MCMC tuning.\n\n**D. Increase the prior variance of the UCLN by replacing $\\sigma^2$ with $10\\sigma^2$ to make larger proposed changes to $\\{r_i\\}$ more acceptable.**\nThis modification involves changing the prior distribution $p(\\theta)$. The target posterior distribution is proportional to the product of the likelihood and the prior, $L(D|\\theta)p(\\theta)$. Altering the prior fundamentally changes the model and thus the posterior distribution being targeted. The goal is to improve the sampling efficiency for the *given* posterior, not to sample from a different, potentially more convenient, posterior.\nVerdict: **Incorrect**. This violates the primary directive not to change the target posterior distribution.\n\n**E. Increase thinning (e.g., keeping every $100$th sample) so that autocorrelation in the saved $\\{r_i\\}$ draws is lower.**\nThinning is the practice of storing only every $k$-th sample from the chain. While this reduces the autocorrelation in the *saved* sequence of samples and decreases file size, it does not improve the mixing of the MCMC sampler. Mixing is a property of how the chain explores the state space per iteration. Poor mixing leads to high autocorrelation. Thinning addresses the symptom (autocorrelated samples) but not the cause (inefficient exploration). To achieve a given ESS, thinning requires running the chain for a proportionally longer time, as it discards $k-1$ out of every $k$ samples, thereby reducing the total information content for a fixed number of iterations. It does not improve the efficiency of the sampler.\nVerdict: **Incorrect**. This does not improve MCMC mixing; it is a post-processing step that reduces the number of effective samples per MCMC iteration.\n\n**F. Introduce a joint block-update for $(\\log r_i, \\sigma)$ using a reversible, approximately multivariate normal proposal with covariance estimated from a short pilot run and then fixed, so that proposals align with posterior correlations.**\nIn many Bayesian models, parameters are correlated in the posterior. In the UCLN model, the rate of a specific branch, $r_i$, is correlated with the parameter that controls the variance of rates across the tree, $\\sigma$. Single-site updates are inefficient in such cases. A block-update proposes a joint move for a block of correlated parameters, e.g., $(\\log r_i, \\sigma)$. Using a multivariate proposal distribution whose covariance structure mimics the posterior covariance allows the sampler to propose moves along the directions of high posterior probability, leading to higher acceptance rates for more ambitious moves. Estimating this covariance from a pilot run and then fixing it is a valid way to construct such an efficient, fixed proposal kernel. This modifies only $q$, not $p(\\theta|D)$.\nVerdict: **Correct**. This is a powerful technique to improve mixing efficiency by accounting for posterior parameter correlations.\n\n**G. Replace the original likelihood with a power-likelihood $L^\\beta$ for $\\beta = 0.8$ throughout the run, so that proposals are more often accepted.**\nThis procedure creates a \"power posterior\" $p_\\beta(\\theta|D) \\propto [L(D|\\theta)]^\\beta p(\\theta)$. With an inverse temperature $\\beta = 0.8$, which is less than $1$, the likelihood surface is flattened. This reduces the \"peakiness\" of the posterior, making it easier for the sampler to accept proposals. However, the sampler is no longer exploring the true posterior distribution (where $\\beta=1$), but a distortion of it. This violates the condition that the target posterior distribution must not be changed.\nVerdict: **Incorrect**. This changes the target posterior distribution.\n\n**H. Employ Metropolis–coupled Markov chain Monte Carlo (parallel tempering) with a ladder of heated chains at inverse temperatures $\\beta_k < 1$ that exchange states with the cold chain, and only retain samples from the cold chain at $\\beta = 1$.**\nMetropolis–Coupled MCMC (MC³) or parallel tempering is an advanced MCMC technique for complex posterior landscapes. It runs multiple chains in parallel, each targeting a power posterior $p_{\\beta_k}(\\theta|D) \\propto [L(D|\\theta)]^{\\beta_k} p(\\theta)$. One chain, the \"cold chain\", has $\\beta_1 = 1$ and targets the true posterior. The other \"heated\" chains have $\\beta_k < 1$, explore flattened versions of the posterior, and can more easily traverse low-probability regions. The algorithm allows swaps of parameter states between adjacent chains. This allows the cold chain to receive configurations from parts of the parameter space it might not have reached on its own, drastically improving global mixing. Since samples are exclusively drawn from the cold chain ($\\beta_1 = 1$), the resulting samples are guaranteed to be from the correct target posterior distribution.\nVerdict: **Correct**. This is a sophisticated and valid method to improve mixing for difficult posteriors without altering the final target distribution.",
            "answer": "$$\\boxed{ABCFH}$$"
        }
    ]
}