{
    "hands_on_practices": [
        {
            "introduction": "This first practice lays the groundwork for all subsequent analyses by guiding you through a complete, albeit simplified, estimation of the $d_N/d_S$ ratio, which we denote as $\\omega$. You will apply the foundational Nei-Gojobori (NG86) method with a Jukes-Cantor (JC) correction to calculate a point estimate of $\\omega$ from summary data. More importantly, this exercise introduces the critical concept of statistical uncertainty by having you construct a confidence interval for your estimate, a crucial step in distinguishing a true evolutionary signal from statistical noise, especially when dealing with a small number of observed substitutions .",
            "id": "2757639",
            "problem": "A pairwise alignment of two orthologous coding sequences shows low divergence. You will estimate the nonsynonymous substitution rate per nonsynonymous site ($d_N$), the synonymous substitution rate per synonymous site ($d_S$), and their ratio $\\omega \\equiv d_N/d_S$ using the Nei–Gojobori (NG86) method under the Jukes–Cantor (JC) model, and then construct a conservative confidence interval for $\\omega$ by approximating the counts of differences as Poisson.\n\nAssume the following scientifically plausible setup and data.\n\n- The coding region has length $L = 100$ codons ($300$ nucleotides), no gaps, no internal stop codons, and standard genetic code applies. Codon-by-codon NG86 counting under the standard genetic code yields a total of $S = 78.0$ synonymous sites and $N = 222.0$ nonsynonymous sites in this gene.\n- Across the alignment, the total numbers of observed single-nucleotide differences classified by NG86 are $X_{S} = 3$ synonymous differences and $X_{N} = 2$ nonsynonymous differences. Base composition is approximately equal so that the JC model is appropriate as a first-order correction for multiple hits.\n- Under NG86 with JC, let $\\hat{p}_{S} \\equiv X_{S}/S$ and $\\hat{p}_{N} \\equiv X_{N}/N$ be the observed proportions of synonymous and nonsynonymous differences per site. The JC correction relates an observed proportion of differences $p$ to the evolutionary distance $d$ (substitutions per site) by a monotone transform under the Jukes–Cantor model.\n\nTasks:\n\n1. Starting from the core definition that $d$ is the expected number of substitutions per site and using the Jukes-Cantor model as a well-tested substitution model, derive the NG86 estimators $\\hat{d}_{S}$ and $\\hat{d}_{N}$ from $\\hat{p}_{S}$ and $\\hat{p}_{N}$, and compute the numerical values of $\\hat{d}_{S}$, $\\hat{d}_{N}$, and $\\hat{\\omega} \\equiv \\hat{d}_{N}/\\hat{d}_{S}$ for the given data.\n2. Using a conservative approximation in which $X_{S}$ and $X_{N}$ are treated as independent Poisson counts with means $\\lambda_{S}$ and $\\lambda_{N}$, respectively, construct exact two-sided Garwood $95\\%$ confidence intervals for $\\lambda_{S}$ and $\\lambda_{N}$. Map these to intervals for $p_{S}$ and $p_{N}$ by dividing by $S$ and $N$, respectively, and then transform each endpoint to the JC-corrected $d$-scale to obtain conservative $95\\%$ intervals $[d_{S}^{L}, d_{S}^{U}]$ and $[d_{N}^{L}, d_{N}^{U}]$.\n3. By monotonicity of the transformations, form a conservative $95\\%$ confidence interval for $\\omega$ as $[\\omega^{L}, \\omega^{U}] = [d_{N}^{L}/d_{S}^{U},\\ d_{N}^{U}/d_{S}^{L}]$.\n\nProvide the upper endpoint $\\omega^{U}$ as your final reported value. Round your final answer to three significant figures. No units are required.",
            "solution": "The problem presented will first be subjected to a rigorous validation procedure before any attempt at a solution is made.\n\nThe givens extracted verbatim from the problem statement are as follows:\n- The coding region has length $L = 100$ codons ($300$ nucleotides).\n- There are no gaps, no internal stop codons.\n- Standard genetic code applies.\n- Total number of synonymous sites (counted by NG86): $S = 78.0$.\n- Total number of nonsynonymous sites (counted by NG86): $N = 222.0$.\n- Total number of observed synonymous differences: $X_{S} = 3$.\n- Total number of observed nonsynonymous differences: $X_{N} = 2$.\n- The Jukes-Cantor (JC) model is to be used.\n- Observed proportion of synonymous differences: $\\hat{p}_{S} \\equiv X_{S}/S$.\n- Observed proportion of nonsynonymous differences: $\\hat{p}_{N} \\equiv X_{N}/N$.\n- The evolutionary distance $d$ is related to the observed proportion of differences $p$ by a monotone transform under the Jukes–Cantor model.\n- Task $1$: Derive the NG86 estimators $\\hat{d}_{S}$ and $\\hat{d}_{N}$ from $\\hat{p}_{S}$ and $\\hat{p}_{N}$ using the JC model, and compute the numerical values of $\\hat{d}_{S}$, $\\hat{d}_{N}$, and $\\hat{\\omega} \\equiv \\hat{d}_{N}/\\hat{d}_{S}$.\n- Task $2$: Construct conservative $95\\%$ confidence intervals for $d_S$ and $d_N$ by treating $X_S$ and $X_N$ as Poisson counts and using the Garwood interval for the Poisson mean.\n- Task $3$: Form a conservative $95\\%$ confidence interval for $\\omega$ as $[\\omega^{L}, \\omega^{U}] = [d_{N}^{L}/d_{S}^{U},\\ d_{N}^{U}/d_{S}^{L}]$, and provide $\\omega^{U}$ as the final answer.\n\nThe problem is reviewed against the validation criteria. It is scientifically grounded, rooted in the standard theory of molecular evolution, specifically the Nei-Gojobori method and the Jukes-Cantor substitution model. The problem is well-posed, providing all necessary data and a clear, logical sequence of tasks leading to a unique solution. The language is precise and objective. The provided data are plausible for a short gene alignment with low divergence. The problem does not violate any fundamental principles, is not incomplete or contradictory, and is not trivial. Therefore, the problem is deemed valid and a full solution will be provided.\n\n**Task 1: Point Estimation of Substitution Rates and their Ratio**\n\nFirst, we derive the Jukes-Cantor (JC69) correction. The JC69 model assumes equal nucleotide frequencies ($1/4$) and an equal rate of substitution, $\\alpha$, between any two distinct nucleotides. The probability $P_{ij}(t)$ of a nucleotide $i$ changing to a nucleotide $j$ in time $t$ is governed by the master equation. The probability that a site has not changed, $P_{ii}(t)$, follows the differential equation:\n$$ \\frac{dP_{ii}(t)}{dt} = -3\\alpha P_{ii}(t) + \\alpha \\sum_{j \\neq i} P_{ji}(t) $$\nDue to symmetry in the model, $P_{ji}(t) = P_{ki}(t)$ for any $j, k \\neq i$. Also, $\\sum_{k} P_{ik}(t) = 1$, which implies $P_{ii}(t) + 3P_{ji}(t) = 1$. Substituting $3P_{ji}(t) = 1 - P_{ii}(t)$ into the differential equation gives:\n$$ \\frac{dP_{ii}(t)}{dt} = -3\\alpha P_{ii}(t) + \\alpha (1-P_{ii}(t)) = \\alpha - 4\\alpha P_{ii}(t) $$\nWith the initial condition $P_{ii}(0)=1$, the solution is:\n$$ P_{ii}(t) = \\frac{1}{4} + \\frac{3}{4}\\exp(-4\\alpha t) $$\nThe probability of observing a difference at a site, $p$, is $1 - P_{ii}(t)$:\n$$ p(t) = 1 - \\left( \\frac{1}{4} + \\frac{3}{4}\\exp(-4\\alpha t) \\right) = \\frac{3}{4} \\left( 1 - \\exp(-4\\alpha t) \\right) $$\nThe evolutionary distance, $d$, is defined as the expected number of substitutions per site. In the JC model, the rate of substitution away from any nucleotide is $3\\alpha$, so $d = 3\\alpha t$. Substituting $4\\alpha t = \\frac{4}{3}d$ into the equation for $p(t)$ gives:\n$$ p = \\frac{3}{4} \\left( 1 - \\exp(-\\frac{4}{3}d) \\right) $$\nInverting this equation to solve for $d$ in terms of the observable proportion of differences $p$ yields the JC correction:\n$$ d = -\\frac{3}{4} \\ln\\left(1 - \\frac{4}{3}p\\right) $$\nThis relation holds for $p < 3/4$.\n\nNow, we apply this to the given data.\nThe observed proportions of differences are:\n$$ \\hat{p}_{S} = \\frac{X_{S}}{S} = \\frac{3}{78.0} \\approx 0.03846 $$\n$$ \\hat{p}_{N} = \\frac{X_{N}}{N} = \\frac{2}{222.0} \\approx 0.009009 $$\nUsing the JC correction, we estimate the substitution rates per site:\n$$ \\hat{d}_{S} = -\\frac{3}{4} \\ln\\left(1 - \\frac{4}{3}\\hat{p}_{S}\\right) = -\\frac{3}{4} \\ln\\left(1 - \\frac{4}{3} \\cdot \\frac{3}{78.0}\\right) = -\\frac{3}{4} \\ln\\left(1 - \\frac{4}{78.0}\\right) \\approx 0.03948 $$\n$$ \\hat{d}_{N} = -\\frac{3}{4} \\ln\\left(1 - \\frac{4}{3}\\hat{p}_{N}\\right) = -\\frac{3}{4} \\ln\\left(1 - \\frac{4}{3} \\cdot \\frac{2}{222.0}\\right) = -\\frac{3}{4} \\ln\\left(1 - \\frac{8}{666.0}\\right) \\approx 0.009064 $$\nThe point estimate for the ratio $\\omega = d_N/d_S$ is:\n$$ \\hat{\\omega} = \\frac{\\hat{d}_{N}}{\\hat{d}_{S}} \\approx \\frac{0.009064}{0.03948} \\approx 0.2296 $$\n\n**Task 2: Confidence Intervals for Substitution Rates**\n\nWe treat the counts of differences $X_S$ and $X_N$ as independent Poisson random variables. The exact $100(1-\\alpha)\\%$ Garwood confidence interval for a Poisson mean $\\lambda$ given an observation $x$ is $[\\lambda_L, \\lambda_U]$, where:\n$$ \\lambda_L = \\frac{1}{2} \\chi^2_{\\alpha/2, 2x} \\quad \\text{and} \\quad \\lambda_U = \\frac{1}{2} \\chi^2_{1-\\alpha/2, 2(x+1)} $$\nHere, we construct $95\\%$ confidence intervals, so $\\alpha=0.05$, $\\alpha/2=0.025$, and $1-\\alpha/2=0.975$. $\\chi^2_{q, \\nu}$ is the quantile function of the chi-squared distribution with $\\nu$ degrees of freedom.\n\nFor synonymous differences, $X_S=3$:\n$$ \\lambda_{S}^{L} = \\frac{1}{2} \\chi^2_{0.025, 2(3)} = \\frac{1}{2} \\chi^2_{0.025, 6} \\approx \\frac{1}{2}(1.2373) = 0.6187 $$\n$$ \\lambda_{S}^{U} = \\frac{1}{2} \\chi^2_{0.975, 2(3+1)} = \\frac{1}{2} \\chi^2_{0.975, 8} \\approx \\frac{1}{2}(17.5345) = 8.7673 $$\nFor nonsynonymous differences, $X_N=2$:\n$$ \\lambda_{N}^{L} = \\frac{1}{2} \\chi^2_{0.025, 2(2)} = \\frac{1}{2} \\chi^2_{0.025, 4} \\approx \\frac{1}{2}(0.4844) = 0.2422 $$\n$$ \\lambda_{N}^{U} = \\frac{1}{2} \\chi^2_{0.975, 2(2+1)} = \\frac{1}{2} \\chi^2_{0.975, 6} \\approx \\frac{1}{2}(14.4494) = 7.2247 $$\nNext, we map these intervals to intervals for the proportions $p_S$ and $p_N$ by dividing by the number of sites $S$ and $N$:\n$$ [p_{S}^{L}, p_{S}^{U}] = [\\frac{\\lambda_{S}^{L}}{S}, \\frac{\\lambda_{S}^{U}}{S}] = [\\frac{0.6187}{78.0}, \\frac{8.7673}{78.0}] \\approx [0.007932, 0.1124] $$\n$$ [p_{N}^{L}, p_{N}^{U}] = [\\frac{\\lambda_{N}^{L}}{N}, \\frac{\\lambda_{N}^{U}}{N}] = [\\frac{0.2422}{222.0}, \\frac{7.2247}{222.0}] \\approx [0.001091, 0.03254] $$\nFinally, we apply the monotonically increasing JC transformation $d(p) = -\\frac{3}{4} \\ln(1 - \\frac{4}{3}p)$ to the endpoints of these intervals to obtain confidence intervals for $d_S$ and $d_N$:\n$$ d_{S}^{L} = d(p_{S}^{L}) = -\\frac{3}{4} \\ln(1 - \\frac{4}{3} \\cdot 0.007932) \\approx 0.007975 $$\n$$ d_{S}^{U} = d(p_{S}^{U}) = -\\frac{3}{4} \\ln(1 - \\frac{4}{3} \\cdot 0.1124) \\approx 0.1218 $$\n$$ d_{N}^{L} = d(p_{N}^{L}) = -\\frac{3}{4} \\ln(1 - \\frac{4}{3} \\cdot 0.001091) \\approx 0.001092 $$\n$$ d_{N}^{U} = d(p_{N}^{U}) = -\\frac{3}{4} \\ln(1 - \\frac{4}{3} \\cdot 0.03254) \\approx 0.03328 $$\nThe $95\\%$ confidence intervals are approximately $[0.007975, 0.1218]$ for $d_S$ and $[0.001092, 0.03328]$ for $d_N$.\n\n**Task 3: Confidence Interval for $\\omega$**\n\nA conservative $95\\%$ confidence interval for $\\omega = d_N/d_S$ is constructed by taking ratios of the interval endpoints that maximize and minimize the ratio:\n$$ [\\omega^{L}, \\omega^{U}] = \\left[\\frac{d_{N}^{L}}{d_{S}^{U}}, \\frac{d_{N}^{U}}{d_{S}^{L}}\\right] $$\nPlugging in the computed values:\n$$ \\omega^{L} = \\frac{0.001092}{0.1218} \\approx 0.008966 $$\n$$ \\omega^{U} = \\frac{0.03328}{0.007975} \\approx 4.173 $$\nThe resulting conservative $95\\%$ confidence interval for $\\omega$ is approximately $[0.00897, 4.173]$. The point estimate $\\hat{\\omega} \\approx 0.23$ lies within this interval, which is wide, as expected from the very low counts of observed differences.\nThe question requests the upper endpoint $\\omega^{U}$ of this interval, rounded to three significant figures.\n$$ \\omega^{U} \\approx 4.173 $$\nRounding to three significant figures gives $4.17$.",
            "answer": "$$\\boxed{4.17}$$"
        },
        {
            "introduction": "Building upon the basic estimation framework, this exercise introduces a more realistic evolutionary model to account for known biases in nucleotide substitution. You will move from summary data to a raw codon alignment and incorporate the Kimura two-parameter (K80) model, which distinguishes between transitions and transversions. This practice provides essential hands-on experience in the full workflow of a pairwise analysis and demonstrates how the choice of substitution model can refine the estimates of synonymous ($d_S$) and nonsynonymous ($d_N$) substitution rates .",
            "id": "2757616",
            "problem": "You are given a pairwise codon alignment between two orthologous genes evolved under the Standard Genetic Code. Your task is to estimate the ratio of nonsynonymous to synonymous substitutions per site using a Nei–Gojobori $1986$ framework (NG86; Nei–Gojobori $1986$) extended with the Kimura two-parameter model (K80; Kimura $1980$) to correct separately for transition and transversion multiple hits. Specifically, compute the Kimura two-parameter (K80) corrected synonymous distance $d_S$ and nonsynonymous distance $d_N$, and then compute the ratio $\\omega = d_N/d_S$. Use the following assumptions and data:\n\n- Use the Standard Genetic Code for identifying synonymous and nonsynonymous changes.\n- Treat transitions and transversions as defined by purine–purine or pyrimidine–pyrimidine changes (transitions) versus purine–pyrimidine changes (transversions).\n- Compute the number of synonymous sites and nonsynonymous sites per codon following the Nei–Gojobori definition: for each codon position, count the number of single-nucleotide changes out of $3$ that would be synonymous, sum across the three positions to obtain the codon’s synonymous site count, and set the nonsynonymous site count for the codon to $3$ minus that value. Do this separately for each sequence and then take the average across the two sequences to obtain the total synonymous sites $S$ and nonsynonymous sites $N$ for the alignment.\n- Count observed differences between the two sequences codon-by-codon. This alignment is constructed so that each differing codon pair differs at exactly one nucleotide (no codon pair has multiple nucleotide differences), making the classification of each difference unambiguous.\n- For the K80 correction, within the synonymous class use the observed proportion of synonymous transitions $P_S$ and synonymous transversions $Q_S$ (each normalized by $S$) and within the nonsynonymous class use the observed proportion of nonsynonymous transitions $P_N$ and nonsynonymous transversions $Q_N$ (each normalized by $N$). Then compute $$d_S \\text{ from } P_S, Q_S \\quad \\text{and} \\quad d_N \\text{ from } P_N, Q_N \\text{ using K80.}$$\n- Finally compute $\\omega = d_N/d_S$.\n- Round your final answer to $4$ significant figures. Report $\\omega$ as a dimensionless quantity.\n\nThe codon alignment (sequence $1$ versus sequence $2$) of length $10$ codons is:\n\n$1.$ $\\text{GCT}$ vs $\\text{GCC}$\n\n$2.$ $\\text{GGT}$ vs $\\text{GGA}$\n\n$3.$ $\\text{ACT}$ vs $\\text{ATT}$\n\n$4.$ $\\text{CCT}$ vs $\\text{CAT}$\n\n$5.$ $\\text{GTT}$ vs $\\text{GAT}$\n\n$6.$ $\\text{TTT}$ vs $\\text{TTC}$\n\n$7.$ $\\text{TCC}$ vs $\\text{TCT}$\n\n$8.$ $\\text{GCA}$ vs $\\text{GTA}$\n\n$9.$ $\\text{ACC}$ vs $\\text{ACA}$\n\n$10.$ $\\text{GGC}$ vs $\\text{AGC}$\n\nWhat is the value of $\\omega = d_N/d_S$ under this NG86+K80 procedure? Round your answer to $4$ significant figures.",
            "solution": "The problem is scientifically and mathematically valid. It requires the computation of the ratio of nonsynonymous to synonymous substitution rates, $\\omega = d_N/d_S$, for a given codon alignment using the Nei–Gojobori (NG86) method for site counting and the Kimura two-parameter (K80) model for multiple-hit correction.\n\nThe procedure is executed in five steps:\n1.  Calculation of the number of synonymous sites ($S$) and nonsynonymous sites ($N$).\n2.  Classification and enumeration of observed substitution types.\n3.  Calculation of the proportions of transitional and transversional substitutions for both synonymous and nonsynonymous sites.\n4.  Application of the K80 correction to compute the synonymous distance ($d_S$) and nonsynonymous distance ($d_N$).\n5.  Calculation of the final ratio $\\omega = d_N/d_S$.\n\n**Step 1: Calculation of Synonymous ($S$) and Nonsynonymous ($N$) sites**\n\nThe number of synonymous sites ($s$) for a given codon is calculated as $s = \\sum_{i=1}^{3} f_i$, where $f_i$ is the fraction of single-nucleotide changes at position $i$ that are synonymous, based on the Standard Genetic Code. The number of nonsynonymous sites is $n = 3 - s$. Total sites $S$ and $N$ for the alignment are the averages of the site counts from the two sequences. The alignment has a length of $10$ codons, totaling $30$ nucleotide sites.\n\nFor sequence $1$:\n- Codons GCT, GGT, ACT, CCT, GTT, TCC, GCA, ACC, GGC ($9$ codons) each have $s=1$ and $n=2$. These are $4$-fold degenerate at the third position.\n- Codon TTT has $s=1/3$ and $n=8/3$. It is $2$-fold degenerate for Phe (TTC syn, TTA/TTG non-syn).\nTotal synonymous sites for sequence $1$: $S_1 = 9 \\times 1 + 1/3 = 28/3$.\nTotal nonsynonymous sites for sequence $1$: $N_1 = 30 - S_1 = 30 - 28/3 = 62/3$.\n\nFor sequence $2$:\n- Codons GCC, GGA, TCT, GTA, ACA ($5$ codons) each have $s=1$ and $n=2$.\n- Codon ATT has $s=2/3$ and $n=7/3$.\n- Codons CAT, GAT, TTC, AGC ($4$ codons) each have $s=1/3$ and $n=8/3$.\nTotal synonymous sites for sequence $2$: $S_2 = 5 \\times 1 + 1 \\times (2/3) + 4 \\times (1/3) = 5 + 6/3 = 7$.\nTotal nonsynonymous sites for sequence $2$: $N_2 = 30 - S_2 = 30 - 7 = 23$.\n\nThe average number of sites for the alignment are:\n$$ S = \\frac{S_1 + S_2}{2} = \\frac{28/3 + 7}{2} = \\frac{(28+21)/3}{2} = \\frac{49/3}{2} = \\frac{49}{6} $$\n$$ N = \\frac{N_1 + N_2}{2} = \\frac{62/3 + 23}{2} = \\frac{(62+69)/3}{2} = \\frac{131/3}{2} = \\frac{131}{6} $$\nCheck: $S+N = 49/6 + 131/6 = 180/6 = 30$. This is correct.\n\n**Step 2: Classification and Enumeration of Observed Substitutions**\n\nEach of the $10$ codon pairs with differences is classified as a synonymous or nonsynonymous change, and as a transition (Ti) or transversion (Tv).\n1.  GCT(Ala) $\\rightarrow$ GCC(Ala): Synonymous, T$\\rightarrow$C (Ti).\n2.  GGT(Gly) $\\rightarrow$ GGA(Gly): Synonymous, T$\\rightarrow$A (Tv).\n3.  ACT(Thr) $\\rightarrow$ ATT(Ile): Nonsynonymous, C$\\rightarrow$T (Ti).\n4.  CCT(Pro) $\\rightarrow$ CAT(His): Nonsynonymous, C$\\rightarrow$A (Tv).\n5.  GTT(Val) $\\rightarrow$ GAT(Asp): Nonsynonymous, T$\\rightarrow$A (Tv).\n6.  TTT(Phe) $\\rightarrow$ TTC(Phe): Synonymous, T$\\rightarrow$C (Ti).\n7.  TCC(Ser) $\\rightarrow$ TCT(Ser): Synonymous, C$\\rightarrow$T (Ti).\n8.  GCA(Ala) $\\rightarrow$ GTA(Val): Nonsynonymous, C$\\rightarrow$T (Ti).\n9.  ACC(Thr) $\\rightarrow$ ACA(Thr): Synonymous, C$\\rightarrow$A (Tv).\n10. GGC(Gly) $\\rightarrow$ AGC(Ser): Nonsynonymous, G$\\rightarrow$A (Ti).\n\nSummary of observed differences:\n- Number of synonymous transitions ($S_{ti}$): $3$\n- Number of synonymous transversions ($S_{tv}$): $2$\n- Number of nonsynonymous transitions ($N_{ti}$): $3$\n- Number of nonsynonymous transversions ($N_{tv}$): $2$\n\n**Step 3: Calculation of Proportions ($P, Q$)**\n\nThe observed proportions of transitions ($P$) and transversions ($Q$) are calculated for synonymous and nonsynonymous sites separately.\nFor synonymous sites:\n$$ P_S = \\frac{S_{ti}}{S} = \\frac{3}{49/6} = \\frac{18}{49} $$\n$$ Q_S = \\frac{S_{tv}}{S} = \\frac{2}{49/6} = \\frac{12}{49} $$\nFor nonsynonymous sites:\n$$ P_N = \\frac{N_{ti}}{N} = \\frac{3}{131/6} = \\frac{18}{131} $$\n$$ Q_N = \\frac{N_{tv}}{N} = \\frac{2}{131/6} = \\frac{12}{131} $$\n\n**Step 4: K80 Correction for $d_S$ and $d_N$**\n\nThe Kimura two-parameter distance is given by $d = -\\frac{1}{2} \\ln(1 - 2P - Q) - \\frac{1}{4} \\ln(1 - 2Q)$.\n\nFor synonymous distance, $d_S$:\nThe terms inside the logarithms are:\n$$ 1 - 2P_S - Q_S = 1 - 2\\left(\\frac{18}{49}\\right) - \\frac{12}{49} = 1 - \\frac{36}{49} - \\frac{12}{49} = \\frac{49 - 48}{49} = \\frac{1}{49} $$\n$$ 1 - 2Q_S = 1 - 2\\left(\\frac{12}{49}\\right) = 1 - \\frac{24}{49} = \\frac{25}{49} $$\nSo, the synonymous distance is:\n$$ d_S = -\\frac{1}{2} \\ln\\left(\\frac{1}{49}\\right) - \\frac{1}{4} \\ln\\left(\\frac{25}{49}\\right) = \\frac{1}{2} \\ln(49) - \\frac{1}{4} (\\ln(25) - \\ln(49)) $$\n$$ d_S = \\ln(7) - \\frac{1}{4} (2\\ln(5) - 2\\ln(7)) = \\ln(7) - \\frac{1}{2}\\ln(5) + \\frac{1}{2}\\ln(7) = \\frac{3}{2}\\ln(7) - \\frac{1}{2}\\ln(5) $$\nNumerically, $d_S \\approx 2.114146$.\n\nFor nonsynonymous distance, $d_N$:\nThe terms inside the logarithms are:\n$$ 1 - 2P_N - Q_N = 1 - 2\\left(\\frac{18}{131}\\right) - \\frac{12}{131} = 1 - \\frac{36}{131} - \\frac{12}{131} = \\frac{131 - 48}{131} = \\frac{83}{131} $$\n$$ 1 - 2Q_N = 1 - 2\\left(\\frac{12}{131}\\right) = 1 - \\frac{24}{131} = \\frac{107}{131} $$\nSo, the nonsynonymous distance is:\n$$ d_N = -\\frac{1}{2} \\ln\\left(\\frac{83}{131}\\right) - \\frac{1}{4} \\ln\\left(\\frac{107}{131}\\right) $$\n$$ d_N = -\\frac{1}{2}(\\ln(83)-\\ln(131)) - \\frac{1}{4}(\\ln(107)-\\ln(131)) = \\frac{3}{4}\\ln(131) - \\frac{1}{2}\\ln(83) - \\frac{1}{4}\\ln(107) $$\nNumerically, $d_N \\approx 0.278770$.\n\n**Step 5: Calculation of $\\omega = d_N/d_S$**\n\nFinally, the ratio $\\omega$ is computed:\n$$ \\omega = \\frac{d_N}{d_S} = \\frac{\\frac{3}{4}\\ln(131) - \\frac{1}{2}\\ln(83) - \\frac{1}{4}\\ln(107)}{\\frac{3}{2}\\ln(7) - \\frac{1}{2}\\ln(5)} $$\n$$ \\omega \\approx \\frac{0.278770}{2.114146} \\approx 0.13185966 $$\nRounding to $4$ significant figures gives $\\omega = 0.1319$.",
            "answer": "$$\\boxed{0.1319}$$"
        },
        {
            "introduction": "After establishing a gene-wide or branch-wide signature of selection, a primary goal is often to pinpoint the specific amino acid sites responsible for this signal. This advanced practice guides you through the Bayes Empirical Bayes (BEB) procedure, a powerful method for calculating the posterior probability of positive selection for each codon site based on the output of a branch-site model. You will then learn to apply a False Discovery Rate (FDR) control method to compile a statistically robust list of sites likely under positive selection, a technique central to forming specific, testable hypotheses about protein function and adaptation .",
            "id": "2757614",
            "problem": "You are given a fixed phylogeny with a specified foreground branch and a codon alignment that has already been analyzed under a standard branch-site codon model using maximum likelihood. The branch-site model partitions sites into three latent classes: class $0$ (purifying selection with $0 < \\omega_0 < 1$ on all branches), class $1$ (neutral evolution with $\\omega_1 = 1$ on all branches), and class $2$ (positive selection on the foreground branch with $\\omega_2 > 1$, and either $0 < \\omega_0 < 1$ or $\\omega_1 = 1$ on background branches). The fitted model yields, for each site $i$, the log-likelihood contributions $L_{i,k} = \\log f(\\text{data at site } i \\mid \\text{class } k, \\widehat{\\theta})$ for $k \\in \\{0,1,2\\}$ under the estimated parameter vector $\\widehat{\\theta}$, and a mixture prior over classes $\\pi = (\\pi_0,\\pi_1,\\pi_2)$ with $\\pi_k \\ge 0$ and $\\sum_k \\pi_k = 1$. Your task is to implement the following steps:\n\n1. Use Bayes' theorem to compute the empirical Bayes posterior probability that each site $i$ is under positive selection on the foreground branch, namely $P(C=2 \\mid \\text{data at site } i)$, using the class priors $\\pi_k$ and the per-site, per-class log-likelihoods $L_{i,k}$. For numerical stability, perform the computation in log-space using a stable log-sum-exp computation.\n\n2. Define, for each site $i$, the local false discovery rate as $\\ell_i = P(\\text{null} \\mid \\text{data at site } i) = 1 - P(C=2 \\mid \\text{data at site } i)$, where the null hypothesis is the union of classes $0$ and $1$ (no positive selection on the foreground branch). To control the expected false discovery rate at a target level $\\alpha$, use the Bayesian false discovery rate rule: sort the sites by $\\ell_i$ in ascending order, compute the cumulative means $\\bar{\\ell}_k = \\frac{1}{k} \\sum_{j=1}^k \\ell_{(j)}$ over the first $k$ ordered sites $(j)$, and select the largest $k^\\star$ such that $\\bar{\\ell}_{k^\\star} \\le \\alpha$. Declare the corresponding $k^\\star$ sites as discoveries. If no $\\bar{\\ell}_k$ satisfies the inequality, make zero discoveries.\n\n3. Report, for each test case, the list of posterior probabilities $P(C=2 \\mid \\text{data at site } i)$ across sites (rounded to six decimals) and the list of discovered site indices using zero-based indexing.\n\nFoundational base that you may invoke in your reasoning and implementation:\n- The definition of the ratio of nonsynonymous to synonymous substitution rates ($\\omega = d_N/d_S$) and its interpretation for purifying selection ($\\omega < 1$), neutrality ($\\omega = 1$), and positive selection ($\\omega > 1$).\n- The likelihood principle and Bayes' theorem for computing posterior probabilities from prior weights and likelihoods.\n- The definition of the false discovery rate as the expected proportion of false discoveries among discoveries.\n\nImplement an algorithm that, given the priors $\\pi_k$, the per-site log-likelihoods $L_{i,k}$, and the target level $\\alpha$, computes the posterior probabilities and applies the Bayesian false discovery rate control as described.\n\nTest Suite:\nProvide a program that solves the three cases below. In each case, you are given the class prior vector $\\pi$, the target Bayesian false discovery rate level $\\alpha$, and the per-site, per-class log-likelihood matrix $L$ (with rows indexing sites and columns indexing classes in the order $k = 0, 1, 2$).\n\n- Case A (happy path, one clear discovery):\n  $$\\pi = [0.7, 0.25, 0.05], \\quad \\alpha = 0.1,$$\n  $$L = \\begin{bmatrix}\n  -5.0 & -4.8 & 0.5 \\\\\n  -2.2 & -1.5 & -3.0 \\\\\n  -1.0 & -1.1 & -1.2 \\\\\n  -6.0 & -5.8 & -3.9 \\\\\n  -0.2 & -0.5 & -2.5\n  \\end{bmatrix}.$$\n\n- Case B (boundary condition, equal evidence across classes):\n  $$\\pi = [0.49, 0.49, 0.02], \\quad \\alpha = 0.05,$$\n  $$L = \\begin{bmatrix}\n  -2.0 & -2.0 & -2.0 \\\\\n  -2.0 & -2.0 & -2.0 \\\\\n  -2.0 & -2.0 & -2.0 \\\\\n  -2.0 & -2.0 & -2.0\n  \\end{bmatrix}.$$\n\n- Case C (edge case, one extremely strong discovery, others null or ambiguous):\n  $$\\pi = [0.6, 0.35, 0.05], \\quad \\alpha = 0.1,$$\n  $$L = \\begin{bmatrix}\n  -10.0 & -10.0 & -0.1 \\\\\n  -0.1 & -0.2 & -5.0 \\\\\n  -1.0 & -0.8 & -4.0 \\\\\n  -5.0 & -5.0 & -5.0 \\\\\n  -3.0 & -2.8 & -1.9 \\\\\n  -0.5 & -0.7 & -2.2\n  \\end{bmatrix}.$$\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must itself be a list with two elements: the first is the list of posterior probabilities $P(C=2 \\mid \\text{data at site } i)$ per site rounded to six decimals, and the second is the list of discovered site indices (zero-based) under the Bayesian false discovery rate rule at the specified $\\alpha$. For example, your output must have the structure\n$$[\\,[\\text{posteriors\\_A},\\ \\text{discoveries\\_A}],\\ [\\text{posteriors\\_B},\\ \\text{discoveries\\_B}],\\ [\\text{posteriors\\_C},\\ \\text{discoveries\\_C}]\\,].$$",
            "solution": "The problem requires the implementation of a standard statistical procedure used in computational biology to identify sites in a protein-coding gene that are under positive selection. The procedure consists of two main parts: first, the calculation of posterior probabilities of site classes using an empirical Bayes method, and second, the application of a false discovery rate control procedure to identify a set of sites with strong evidence for positive selection.\n\nThe foundation of the method is Bayes' theorem. For each site $i$ in the alignment, we wish to compute the posterior probability of it belonging to the positive selection class, class $k=2$, given the observed data at that site, $D_i$. Let $C_i$ be the random variable for the class assignment of site $i$. The posterior probability is given by:\n$$ P(C_i=2 \\mid D_i) = \\frac{f(D_i \\mid C_i=2) P(C_i=2)}{\\sum_{j=0}^{2} f(D_i \\mid C_i=j) P(C_i=j)} $$\nThe problem provides the necessary components: the prior probabilities of the classes, $P(C_i=k) = \\pi_k$, and the per-site, per-class log-likelihoods, $L_{i,k} = \\log f(D_i \\mid C_i=k, \\widehat{\\theta})$, where $\\widehat{\\theta}$ represents the maximum likelihood estimates of the model parameters. Substituting these into the formula, we have:\n$$ P(C_i=2 \\mid D_i) = \\frac{\\pi_2 \\exp(L_{i,2})}{\\sum_{j=0}^{2} \\pi_j \\exp(L_{i,j})} $$\nDirect computation of this expression is numerically unstable due to potential underflow or overflow from the exponential function acting on log-likelihood values, which can be large negative numbers. To ensure stability, all calculations must be performed in logarithmic space. Let us define the log of the joint probability of site data and class assignment as $J_{i,k} = \\log(\\pi_k) + L_{i,k}$. The denominator is the marginal likelihood of the data at site $i$, $f(D_i)$, whose logarithm, $M_i$, can be expressed as:\n$$ M_i = \\log \\left( \\sum_{j=0}^{2} \\exp(J_{i,j}) \\right) $$\nThis is a log-sum-exp operation. To compute it robustly, we use the identity $\\log(\\sum_j \\exp(x_j)) = m + \\log(\\sum_j \\exp(x_j - m))$, where $m = \\max_j(x_j)$. For each site $i$, we find $m_i = \\max_{k \\in \\{0,1,2\\}} (J_{i,k})$ and compute the log marginal likelihood as:\n$$ M_i = m_i + \\log \\left( \\sum_{j=0}^{2} \\exp(J_{i,j} - m_i) \\right) $$\nThe posterior probability for site $i$ to be in class $2$ can then be calculated as:\n$$ P(C_i=2 \\mid D_i) = \\exp(\\log P(C_i=2 \\mid D_i)) = \\exp(J_{i,2} - M_i) $$\nThis procedure is applied to every site $i$ to obtain a vector of posterior probabilities.\n\nThe second part of the task is to identify a subset of sites for which the claim of positive selection can be made, while controlling the false discovery rate (FDR). The problem specifies the use of a Bayesian FDR procedure. First, we define the local false discovery rate, $\\ell_i$, for each site $i$ as the posterior probability of the null hypothesis. The null hypothesis is that site $i$ is not under positive selection, meaning it belongs to either class $0$ or class $1$. Therefore:\n$$ \\ell_i = P(C_i \\in \\{0,1\\} \\mid D_i) = P(C_i=0 \\mid D_i) + P(C_i=1 \\mid D_i) = 1 - P(C_i=2 \\mid D_i) $$\nTo control the overall FDR at a target level $\\alpha$, we follow these steps:\n$1$. The $N$ sites in the alignment are sorted according to their local false discovery rates $\\ell_i$ in ascending order. Let the sorted values be $\\ell_{(1)} \\le \\ell_{(2)} \\le \\dots \\le \\ell_{(N)}$, corresponding to site indices $i_{(1)}, i_{(2)}, \\dots, i_{(N)}$.\n$2$. For each rank $k$ from $1$ to $N$, we compute the cumulative mean of the sorted local FDRs:\n$$ \\bar{\\ell}_k = \\frac{1}{k} \\sum_{j=1}^{k} \\ell_{(j)} $$\nThis value, $\\bar{\\ell}_k$, represents the expected proportion of false discoveries if we were to declare the top $k$ sites (those with the smallest $\\ell_i$ values) as being under positive selection.\n$3$. We find the largest integer $k^\\star$ for which the expected FDR does not exceed the target level $\\alpha$, i.e., $\\bar{\\ell}_{k^\\star} \\le \\alpha$.\n$4$. If such a $k^\\star \\ge 1$ exists, the sites corresponding to the first $k^\\star$ sorted local FDRs, $\\{i_{(1)}, i_{(2)}, \\dots, i_{(k^\\star)}\\}$, are declared as discoveries. If no $k$ satisfies the condition, then $k^\\star = 0$, and no discoveries are made.\n\nThe final algorithm proceeds by first computing the posterior probabilities $P(C_i=2 \\mid D_i)$ for all sites, then using these to compute the local FDRs $\\ell_i$. These are then used in the FDR control procedure to find the set of discovered sites. The results, comprising the list of all posterior probabilities and the list of discovered site indices, are then reported for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef analyze_case(pi, L, alpha):\n    \"\"\"\n    Analyzes a single test case for positive selection.\n\n    Args:\n        pi (np.ndarray): Prior probabilities for the site classes.\n        L (np.ndarray): Per-site, per-class log-likelihood matrix.\n        alpha (float): Target Bayesian false discovery rate.\n\n    Returns:\n        list: A list containing two elements:\n              1. A list of posterior probabilities P(C=2 | data) for each site,\n                 rounded to six decimals.\n              2. A list of zero-based indices of discovered sites, sorted.\n    \"\"\"\n    # Defensive check for empty inputs\n    if L.shape[0] == 0:\n        return [[], []]\n        \n    # Step 1: Compute posterior probabilities using a stable log-sum-exp\n    log_pi = np.log(pi)\n    \n    # J_ik = log(pi_k) + L_ik (joint log probability)\n    J = L + log_pi  # Broadcasting adds log_pi to each row of L\n    \n    # m_i = max_k(J_ik)\n    m = np.max(J, axis=1, keepdims=True)\n    # M_i = m_i + log(sum_k exp(J_ik - m_i)) (log marginal likelihood)\n    log_marginal_lik = m + np.log(np.sum(np.exp(J - m), axis=1, keepdims=True))\n\n    # log P(C=2 | data_i) = J_i2 - M_i\n    log_post_prob_k2 = J[:, 2] - log_marginal_lik.flatten()\n    post_prob_k2 = np.exp(log_post_prob_k2)\n    \n    posteriors_rounded = np.round(post_prob_k2, 6).tolist()\n    \n    # Step 2: Bayesian False Discovery Rate (FDR) control\n    # l_i = 1 - P(C=2 | data_i) (local false discovery rate)\n    local_fdr = 1 - post_prob_k2\n    \n    # Sort sites by local_fdr in ascending order\n    sorted_indices = np.argsort(local_fdr)\n    sorted_lfdr = local_fdr[sorted_indices]\n    \n    # Compute cumulative means of sorted local FDRs\n    num_sites = len(local_fdr)\n    k_vals = np.arange(1, num_sites + 1)\n    cumulative_means = np.cumsum(sorted_lfdr) / k_vals\n    \n    # Find the largest k such that cumulative_mean[k-1] <= alpha\n    valid_k_indices = np.where(cumulative_means <= alpha)[0]\n    \n    if len(valid_k_indices) == 0:\n        k_star = 0\n    else:\n        # np.where returns 0-based indices. The k value is index + 1.\n        k_star = valid_k_indices[-1] + 1\n        \n    # Get the indices of discovered sites\n    discovered_site_indices = sorted_indices[:k_star]\n    \n    # Report discovered indices sorted for consistency\n    discoveries = sorted(discovered_site_indices.tolist())\n    \n    return [posteriors_rounded, discoveries]\n\ndef solve():\n    \"\"\"\n    Wrapper function to define test cases and print results in the required format.\n    \"\"\"\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"pi\": np.array([0.7, 0.25, 0.05]),\n            \"alpha\": 0.1,\n            \"L\": np.array([\n                [-5.0, -4.8, 0.5],\n                [-2.2, -1.5, -3.0],\n                [-1.0, -1.1, -1.2],\n                [-6.0, -5.8, -3.9],\n                [-0.2, -0.5, -2.5]\n            ])\n        },\n        {\n            \"name\": \"Case B\",\n            \"pi\": np.array([0.49, 0.49, 0.02]),\n            \"alpha\": 0.05,\n            \"L\": np.array([\n                [-2.0, -2.0, -2.0],\n                [-2.0, -2.0, -2.0],\n                [-2.0, -2.0, -2.0],\n                [-2.0, -2.0, -2.0]\n            ])\n        },\n        {\n            \"name\": \"Case C\",\n            \"pi\": np.array([0.6, 0.35, 0.05]),\n            \"alpha\": 0.1,\n            \"L\": np.array([\n                [-10.0, -10.0, -0.1],\n                [-0.1, -0.2, -5.0],\n                [-1.0, -0.8, -4.0],\n                [-5.0, -5.0, -5.0],\n                [-3.0, -2.8, -1.9],\n                [-0.5, -0.7, -2.2]\n            ])\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = analyze_case(case[\"pi\"], case[\"L\"], case[\"alpha\"])\n        all_results.append(result)\n\n    # Final print statement in the exact required format.\n    # The default string representation of a list is what is needed.\n    # The map(str, ...) converts each result list (e.g., [[...],[...]])\n    # to its string form, and join combines them with commas.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        }
    ]
}