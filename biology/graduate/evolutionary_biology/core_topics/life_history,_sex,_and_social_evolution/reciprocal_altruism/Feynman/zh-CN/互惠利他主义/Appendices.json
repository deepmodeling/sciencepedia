{
    "hands_on_practices": [
        {
            "introduction": "为了理解像“以牙还牙”（Tit-for-Tat, TFT）这样的合作策略是如何演化的，我们必须考虑种群的结构。这个练习超越了单一的互动，模拟了合作策略入侵纯粹背叛策略（ALLD）种群的条件，并强调了“选型交配”或“族群性”（assortment, $k$）的关键作用，即合作者更有可能与合作者互动。通过解决这个问题，你将量化启动合作所需社会结构的精确阈值，这是一个理解合作起源的核心概念。",
            "id": "1877275",
            "problem": "在演化博弈论中，自利个体组成的种群中合作行为的出现是一个核心难题。所进行的博弈结构至关重要。考虑一个由重复博弈建模的场景，在每一轮中，两个参与者都可以选择合作 (C) 或背叛 (D)。一个参与者的收益由以下矩阵给出，其中该参与者的行动在行上，对手的行动在列上：\n- (C, C) 的收益：$R$ (奖励)\n- (C, D) 的收益：$S$ (受骗)\n- (D, C) 的收益：$T$ (诱惑)\n- (D, D) 的收益：$P$ (惩罚)\n\n如果 $T > R > P > S$，则该博弈是囚徒困境。每一轮结束后，同样的两个个体之间有恒定的概率 $w$ 会进行下一轮博弈。总期望收益是所有轮次收益的总和。\n\n考虑一个几乎完全由采用“始终背叛”(ALLD) 策略的个体组成的大种群。引入了一小群采用“以牙还牙”(TFT) 策略的突变者。一个 TFT 参与者在第一步选择合作，此后复制其对手上一轮的行动。\n\n由于社会结构或亲缘关系，突变体 TFT 参与者之间相互互动的频率可能高于随机概率所预期的频率。设 $k$ 为选型系数，定义为一个 TFT 个体与另一个 TFT 个体互动的概率。因此，一个 TFT 参与者与庞大的 ALLD 种群中的一个随机成员互动的概率为 $1-k$。\n\n为了使合作能够出现，TFT 策略必须能够入侵 ALLD 种群。这要求 TFT 个体的期望收益大于 ALLD 个体的期望收益。\n\n给定囚徒困境的收益 $T=4$, $R=2, P=1, S=0$，以及未来互动的概率 $w=0.5$，计算 TFT 策略入侵 ALLD 种群所需的最小选型系数 $k_{crit}$。请用一个分数表示你的答案。",
            "solution": "在每次重复博弈中，第 $t+1$ 轮发生的概率为 $w^{t}$，因此总期望收益等于各轮次阶段收益以 $w^t$ 为权重的总和。使用等比级数公式 $\\sum_{t=0}^{\\infty} w^{t} = \\frac{1}{1-w}$ (当 $0<w<1$时)，计算相关配对的期望收益：\n\n- TFT 对 TFT：双方永远选择合作 C，所以每一轮都得到收益 $R$：\n$$\nV_{\\text{TFT},\\text{TFT}}=\\sum_{t=0}^{\\infty} w^{t} R=\\frac{R}{1-w}.\n$$\n\n- TFT 对 ALLD：行动序列在第1轮为 (C,D) (TFT 的收益为 $S$)，此后均为 (D,D) (TFT 的收益为 $P$)。因此\n$$\nV_{\\text{TFT},\\text{ALLD}}=S+\\sum_{t=1}^{\\infty} w^{t} P=S+\\frac{wP}{1-w}.\n$$\n\n- ALLD 对 ALLD：每一轮都是 (D,D)，得到\n$$\nV_{\\text{ALLD},\\text{ALLD}}=\\sum_{t=0}^{\\infty} w^{t} P=\\frac{P}{1-w}.\n$$\n\n在选型系数为 $k$ 的情况下，一个稀有的 TFT 个体遇到另一个 TFT 的概率为 $k$，遇到一个 ALLD 的概率为 $1-k$，所以其期望收益为\n$$\n\\Pi_{\\text{TFT}}=k\\,V_{\\text{TFT},\\text{TFT}}+(1-k)\\,V_{\\text{TFT},\\text{ALLD}}.\n$$\n在一个 ALLD 种群中（突变体是稀有的），一个原有的 ALLD 个体几乎必然会遇到另一个 ALLD，所以\n$$\n\\Pi_{\\text{ALLD}}=V_{\\text{ALLD},\\text{ALLD}}.\n$$\n入侵条件是 $\\Pi_{\\text{TFT}}>\\Pi_{\\text{ALLD}}$，即\n$$\nk\\frac{R}{1-w}+(1-k)\\left(S+\\frac{wP}{1-w}\\right)>\\frac{P}{1-w}.\n$$\n两边同乘以 $(1-w)$ 并重新整理，\n$$\nkR+(1-k)\\left(S(1-w)+wP\\right)>P,\n$$\n$$\nk\\left[R-\\left(S(1-w)+wP\\right)\\right]>P-\\left(S(1-w)+wP\\right).\n$$\n因此，临界选型阈值为\n$$\nk_{\\text{crit}}=\\frac{P-\\left(S(1-w)+wP\\right)}{R-\\left(S(1-w)+wP\\right)}=\\frac{(1-w)(P-S)}{(R-S)-w(P-S)}.\n$$\n代入 $R=2, P=1, S=0, w=0.5$：\n$$\nk_{\\text{crit}}=\\frac{(1-0.5)(1-0)}{(2-0)-0.5(1-0)}=\\frac{0.5}{2-0.5}=\\frac{1}{3}.\n$$\n因此，所需的最小选型系数为分数 $\\frac{1}{3}$。",
            "answer": "$$\\boxed{\\frac{1}{3}}$$"
        },
        {
            "introduction": "尽管“以牙还牙”（TFT）策略在理想条件下非常有效，但在充满偶然失误或“噪音”的现实世界中，其表现会急剧下降。这个练习将引导你分析一种更稳健的策略——“赢则坚守，输则转换”（Win-Stay, Lose-Shift, WSLS），并让你在充满错误的环境中对其与 TFT 的表现进行建模。通过构建和分析马尔可夫链模型，你将发现噪音如何从根本上改变演化格局，并偏爱不同类型的合作逻辑。",
            "id": "2527576",
            "problem": "考虑一个无限期重复的囚徒困境 (PD) 博弈，其形式为捐赠博弈，这是生态学中研究无亲缘关系个体间合作的典型模型。在每一轮中，每个参与者选择合作 ($C$) 或背叛 ($D$)。合作会给对方带来收益 $b>0$，但自身需要付出成本 $c>0$。因此，单次囚徒困境中行参与者的收益如下：相互合作 ($CC$) 产生 $R=b-c$；在我方合作而对方背叛时 ($CD$) 产生 $S=-c$；在我方背叛而对方合作时 ($DC$) 产生 $T=b$；相互背叛 ($DD$) 产生 $P=0$。假设 $b>c>0$，从而满足囚徒困境的要求 $T>R>P>S$。\n\n参与者执行两种记忆长度为一的策略之一，这两种策略仅依赖于上一轮的实际结果：\n- 一报还一报 (Tit-for-Tat, TFT)：当且仅当对方在上一轮合作时，本轮才选择合作。\n- 赢定输移 (Win-Stay, Lose-Shift, WSLS)：如果上一轮的收益是“赢” ($R$或$T$)，则重复上一轮的行动；如果上一轮的收益是“输” ($P$或$S$)，则改变行动。\n\n执行时会独立地出现错误：在每次行动中，参与者预期的行动会以概率 $\\epsilon \\in (0,1)$ 被翻转 ($C \\leftrightarrow D$)，此过程在参与者之间和回合之间是独立的。对于任何固定的策略对，四个实际联合行动状态 $\\{CC, CD, DC, DD\\}$ 上的动态构成一个时间齐次马尔可夫链。由于当 $0<\\epsilon<1$ 时，该链是有限、不可约且非周期的，因此它具有唯一的平稳分布。\n\n定义一个策略对每轮的长期平均收益为在状态 $\\{CC, CD, DC, DD\\}$ 的平稳分布下的期望单次收益。如果 WSLS 自博弈的长期平均收益超过 TFT 自博弈的长期平均收益，我们称 WSLS “优于” TFT。\n\n请仅从上述定义和马尔可夫链的全概率定律出发，符号化地推导在噪声水平为 $\\epsilon$ 的情况下 TFT-vs-TFT 和 WSLS-vs-WSLS 的平稳分布，计算它们各自的长期平均收益，并确定唯一的临界错误概率 $\\epsilon^{\\ast}$，在该概率下，WSLS 自博弈和 TFT 自博弈具有相等的长期平均收益。您的最终答案必须是 $\\epsilon^{\\ast}$ 的封闭形式解。无需四舍五入，答案无单位。",
            "solution": "该问题要求推导临界错误概率 $\\epsilon^{\\ast}$，使得一报还一报 (TFT) 自博弈的长期平均收益 $V_{TFT}$ 等于赢定输移 (WSLS) 自博弈的长期平均收益 $V_{WSLS}$。该博弈是一个重复的囚徒困境，收益为 $R=b-c$, $S=-c$, $T=b$ 和 $P=0$，其中 $b>c>0$。系统的状态是上一轮中两名参与者的联合行动，来自集合 $\\{CC, CD, DC, DD\\}$。一个预期的行动以概率 $\\epsilon \\in (0,1)$ 被翻转。\n\n首先，我们分析 TFT 对 TFT 的互动。TFT 参与者当且仅当其对手在上一轮合作时才合作。给定当前状态 $(A_1, A_2)$，下一轮的预期行动 $(I_1, I_2)$ 如下：\n- 从 $CC$：两名参与者都看到了合作，因此都打算合作。预期状态为 $(C,C)$。\n- 从 $CD$：参与者1看到了背叛，参与者2看到了合作。预期状态为 $(D,C)$。\n- 从 $DC$：参与者1看到了合作，参与者2看到了背叛。预期状态为 $(C,D)$。\n- 从 $DD$：两名参与者都看到了背叛，因此都打算背叛。预期状态为 $(D,D)$。\n\n预期的合作 ($I_C$) 以概率 $1-\\epsilon$ 成为实际的合作 ($C$)，以概率 $\\epsilon$ 成为实际的背叛 ($D$)。预期的背叛 ($I_D$) 以概率 $1-\\epsilon$ 成为实际的 $D$，以概率 $\\epsilon$ 成为实际的 $C$。因此，状态空间 $\\{CC, CD, DC, DD\\}$ 上的马尔可夫链的转移矩阵 $M_{TFT}$ 为：\n$$\nM_{TFT} =\n\\begin{pmatrix}\n(1-\\epsilon)^2 & \\epsilon(1-\\epsilon) & \\epsilon(1-\\epsilon) & \\epsilon^2 \\\\\n\\epsilon(1-\\epsilon) & \\epsilon^2 & (1-\\epsilon)^2 & \\epsilon(1-\\epsilon) \\\\\n\\epsilon(1-\\epsilon) & (1-\\epsilon)^2 & \\epsilon^2 & \\epsilon(1-\\epsilon) \\\\\n\\epsilon^2 & \\epsilon(1-\\epsilon) & \\epsilon(1-\\epsilon) & (1-\\epsilon)^2\n\\end{pmatrix}\n$$\n$M_{TFT}$ 中每一列的和都为1，这意味着该矩阵是双随机矩阵。对于一个不可约且非周期的有限马尔可夫链，双随机转移矩阵意味着唯一的平稳分布是均匀分布。设 $\\pi_{TFT} = (p_{CC}, p_{CD}, p_{DC}, p_{DD})$ 为该分布。因此，$p_{CC} = p_{CD} = p_{DC} = p_{DD} = \\frac{1}{4}$。\nTFT 参与者的长期平均收益是在此平稳分布下的期望收益：\n$$V_{TFT} = p_{CC}R + p_{CD}S + p_{DC}T + p_{DD}P$$\n$$V_{TFT} = \\frac{1}{4} (b-c) + \\frac{1}{4} (-c) + \\frac{1}{4} (b) + \\frac{1}{4} (0) = \\frac{1}{4}(2b - 2c) = \\frac{b-c}{2}$$\n\n接下来，我们分析 WSLS 对 WSLS 的互动。WSLS 参与者在“赢”（收益 $R$ 或 $T$）后重复其上一轮的行动，在“输”（收益 $P$ 或 $S$）后改变行动。预期行动如下：\n- 从 $CC$：两名参与者都获得了 $R$（赢）。两者都重复其行动 ($C$)。预期状态为 $(C,C)$。\n- 从 $CD$：参与者1获得了 $S$（输），参与者2获得了 $T$（赢）。参与者1将行动从 $C$ 变为 $D$，参与者2重复 $D$。预期状态为 $(D,D)$。\n- 从 $DC$：参与者1获得了 $T$（赢），参与者2获得了 $S$（输）。参与者1重复 $D$，参与者2将行动从 $C$ 变为 $D$。预期状态为 $(D,D)$。\n- 从 $DD$：两者都获得了 $P$（输）。两者都将行动从 $D$ 变为 $C$。预期状态为 $(C,C)$。\n\n转移矩阵 $M_{WSLS}$ 基于这些预期行动构建：\n$$\nM_{WSLS} =\n\\begin{pmatrix}\n(1-\\epsilon)^2 & \\epsilon(1-\\epsilon) & \\epsilon(1-\\epsilon) & \\epsilon^2 \\\\\n\\epsilon^2 & \\epsilon(1-\\epsilon) & \\epsilon(1-\\epsilon) & (1-\\epsilon)^2 \\\\\n\\epsilon^2 & \\epsilon(1-\\epsilon) & \\epsilon(1-\\epsilon) & (1-\\epsilon)^2 \\\\\n(1-\\epsilon)^2 & \\epsilon(1-\\epsilon) & \\epsilon(1-\\epsilon) & \\epsilon^2\n\\end{pmatrix}\n$$\n设平稳分布为 $\\pi_{WSLS} = (q_{CC}, q_{CD}, q_{DC}, q_{DD})$。由于对称性，$q_{CD}=q_{DC}$。平稳性方程 $\\pi_{WSLS} M_{WSLS} = \\pi_{WSLS}$ 对状态 $CD$ 得出：\n$$q_{CD} = q_{CC}\\epsilon(1-\\epsilon) + q_{CD}\\epsilon(1-\\epsilon) + q_{DC}\\epsilon(1-\\epsilon) + q_{DD}\\epsilon(1-\\epsilon)$$\n$$q_{CD} = (q_{CC} + q_{CD} + q_{DC} + q_{DD})\\epsilon(1-\\epsilon)$$\n由于概率之和为1，我们发现 $q_{CD} = \\epsilon(1-\\epsilon)$。因此，$q_{DC} = \\epsilon(1-\\epsilon)$。\n归一化条件是 $q_{CC} + q_{CD} + q_{DC} + q_{DD} = 1$，这给出 $q_{CC} + q_{DD} = 1 - 2\\epsilon(1-\\epsilon)$。\n现在考虑对称状态集合 $S_{sym}=\\{CC, DD\\}$ 与非对称状态集合 $S_{asym}=\\{CD, DC\\}$ 之间的流。设 $X=q_{CC}+q_{DD}$ 和 $Y=q_{CD}+q_{DC}=2\\epsilon(1-\\epsilon)$。\n从 $S_{sym}$ 转移到 $S_{asym}$ 的总概率是 $P(S_{asym}|S_{sym}) = \\epsilon(1-\\epsilon) + \\epsilon(1-\\epsilon) = 2\\epsilon(1-\\epsilon)$。\n从 $S_{asym}$ 转移到 $S_{asym}$ 的总概率是 $P(S_{asym}|S_{asym}) = \\epsilon(1-\\epsilon) + \\epsilon(1-\\epsilon) = 2\\epsilon(1-\\epsilon)$。等等，这不正确。\n从状态 $CD$（预期 $(D,D)$）转移到 $CD$的概率是 $\\epsilon(1-\\epsilon)$，转移到 $DC$ 的概率是 $\\epsilon(1-\\epsilon)$。总和是 $2\\epsilon(1-\\epsilon)$。所以这部分是正确的。让我们使用关于 $q_{CC}$ 的平稳性方程。\n$$q_{CC} = q_{CC}(1-\\epsilon)^2 + q_{CD}\\epsilon^2 + q_{DC}\\epsilon^2 + q_{DD}(1-\\epsilon)^2$$\n$$q_{CC} = q_{CC}(1-\\epsilon)^2 + 2\\epsilon(1-\\epsilon)\\epsilon^2 + q_{DD}(1-\\epsilon)^2$$\n$$q_{CC}(1 - (1-\\epsilon)^2) = 2\\epsilon^3(1-\\epsilon) + q_{DD}(1-\\epsilon)^2$$\n$$q_{CC}\\epsilon(2-\\epsilon) = 2\\epsilon^3(1-\\epsilon) + (1 - 2\\epsilon(1-\\epsilon) - q_{CC})(1-\\epsilon)^2$$\n解这个代数方程很繁琐。一个更巧妙的方法是使用状态聚合。设 $X=q_{CC}+q_{DD}$ 和 $Y=q_{CD}+q_{DC}$。在平稳状态下，处于状态 $CC$ 的概率是：\n$$q_{CC} = (q_{CC}+q_{DD})(1-\\epsilon)^2 + (q_{CD}+q_{DC})\\epsilon^2 = X(1-\\epsilon)^2 + Y\\epsilon^2$$\n对于 $DD$：\n$$q_{DD} = (q_{CC}+q_{DD})\\epsilon^2 + (q_{CD}+q_{DC})(1-\\epsilon)^2 = X\\epsilon^2 + Y(1-\\epsilon)^2$$\n将这两个方程相加得到 $q_{CC}+q_{DD} = X = (X+Y)((1-\\epsilon)^2+\\epsilon^2) = 1-2\\epsilon+2\\epsilon^2$。那么 $Y=1-X=2\\epsilon-2\\epsilon^2$。这证实了 $q_{CD}=q_{DC}=\\epsilon(1-\\epsilon)$。\n将这两个方程相减得到：\n$$q_{CC}-q_{DD} = (X-Y)((1-\\epsilon)^2-\\epsilon^2) = (X-Y)(1-2\\epsilon)$$\n代入 $X=1-2\\epsilon+2\\epsilon^2$ 和 $Y=2\\epsilon-2\\epsilon^2$ 得到 $X-Y = 1-4\\epsilon+4\\epsilon^2 = (1-2\\epsilon)^2$。\n所以，$q_{CC}-q_{DD} = (1-2\\epsilon)^2(1-2\\epsilon) = (1-2\\epsilon)^3$。\n我们得到一个关于 $q_{CC}$ 和 $q_{DD}$ 的二元一次方程组：\n$q_{CC}+q_{DD} = 1-2\\epsilon+2\\epsilon^2$\n$q_{CC}-q_{DD} = (1-2\\epsilon)^3 = 1-6\\epsilon+12\\epsilon^2-8\\epsilon^3$\n将它们相加：$2q_{CC} = 2-8\\epsilon+14\\epsilon^2-8\\epsilon^3 \\implies q_{CC} = 1-4\\epsilon+7\\epsilon^2-4\\epsilon^3$。\nWSLS 的长期平均收益为：\n$$V_{WSLS} = q_{CC}R + q_{CD}S + q_{DC}T + q_{DD}P$$\n$$V_{WSLS} = q_{CC}(b-c) + \\epsilon(1-\\epsilon)(-c) + \\epsilon(1-\\epsilon)(b) + q_{DD}(0)$$\n$$V_{WSLS} = q_{CC}(b-c) + \\epsilon(1-\\epsilon)(b-c) = (q_{CC} + \\epsilon-\\epsilon^2)(b-c)$$\n$$V_{WSLS} = (1-4\\epsilon+7\\epsilon^2-4\\epsilon^3 + \\epsilon-\\epsilon^2)(b-c)$$\n$$V_{WSLS} = (1 - 3\\epsilon + 6\\epsilon^2 - 4\\epsilon^3)(b-c)$$\n\n最后，我们通过令两种收益相等来找到临界错误概率 $\\epsilon^{\\ast}$：\n$$V_{TFT} = V_{WSLS}$$\n$$\\frac{b-c}{2} = (1 - 3\\epsilon + 6\\epsilon^2 - 4\\epsilon^3)(b-c)$$\n因为 $b>c$，所以 $b-c>0$，我们可以用它来除：\n$$\\frac{1}{2} = 1 - 3\\epsilon + 6\\epsilon^2 - 4\\epsilon^3$$\n$$0 = \\frac{1}{2} - 3\\epsilon + 6\\epsilon^2 - 4\\epsilon^3$$\n乘以-2：\n$$0 = 8\\epsilon^3 - 12\\epsilon^2 + 6\\epsilon - 1$$\n这是 $(2\\epsilon - 1)$ 的立方展开式：\n$$0 = (2\\epsilon - 1)^3$$\n通过令底数为零，可以找到在区间 $(0,1)$ 内的唯一实数解：\n$$2\\epsilon^{\\ast} - 1 = 0 \\implies \\epsilon^{\\ast} = \\frac{1}{2}$$\n这就是使得 TFT 和 WSLS 自博弈的长期平均收益相等的临界错误概率。",
            "answer": "$$\\boxed{\\frac{1}{2}}$$"
        },
        {
            "introduction": "如果错误会破坏合作，那么我们如何设计出更具弹性的策略呢？这个高级练习探讨了“宽容的以牙还牙”（Generous Tit-for-Tat, GTFT）策略，它引入了一种概率性的“宽恕”机制。你的任务是通过数学推导，在稀有错误（rare-error）的假设下，找出能够最大化长期收益的最优宽恕概率 $q^{\\star}$。这个过程将教会你如何在面对环境随机性时，通过分析和优化策略参数来设计出更优的合作行为。",
            "id": "2747599",
            "problem": "在一个进行无限重复捐赠博弈的大种群中，每一对匹配个体之间的互动以回合制进行，每回合的持续概率为 $w \\in (0,1)$。在每一回合，如果一个个体选择合作，它会支付成本 $c>0$ 为其伙伴带来收益 $b>c$；如果它选择背叛，则不支付成本也不带来收益。因此，在给定一回合中已实现的行动对的条件下，一个焦点个体的单次收益在双方合作时为 $b-c$，在它对合作者背叛时为 $b$，在它对背叛者合作时为 $-c$，在双方背叛时为 $0$。\n\n每个个体都使用带有宽恕参数 $q \\in [0,1]$ 的 Generous Tit-For-Tat (GTFT) 行为规则，与其自身的相同副本进行博弈。该行为规则如下：\n- 在第一回合，意图合作。\n- 此后，如果对手在前一回合合作，则以概率 $1$ 意图合作；如果对手在前一回合背叛，则以概率 $q$ 意图合作。\n\n每一回合，每个参与者都有一个独立的执行错误率 $\\epsilon \\in (0,\\tfrac{1}{2})$：意图的行动有 $\\epsilon$ 的概率被翻转（合作变为背叛，背叛变为合作），有 $1-\\epsilon$ 的概率按意图执行。参与者能完美地观察到已实现的行动。令焦点个体的归一化贴现收益为\n$$\nV \\;=\\; (1-w)\\,\\mathbb{E}\\!\\left[\\sum_{t=0}^{\\infty} w^{t}\\,u_{t}\\right],\n$$\n其中 $u_t$ 是焦点个体在第 $t$ 回合的单次收益，期望是针对错误的随机性以及在观察到背叛后策略的混合行动来计算的。\n\n在罕见错误机制下进行分析，保留到 $\\epsilon$ 的一阶项（即，假设 $0<\\epsilon\\ll 1$，因此可以忽略单个恢复阶段内发生多次执行错误的情况）。仅使用基本定义、全期望定律和条件概率定律，将动态过程分解为由脱离相互合作状态的单个意外背叛引发的孤立“错误事件”，并推导出焦点个体每个事件相对于永久相互合作的预期归一化贴现损失，作为 $q$、$b$、$c$ 和 $w$ 的显式函数。然后，利用事件发生的一阶速率与 $q$ 无关这一事实，确定能使与相同 GTFT 伙伴博弈时 $V$ 最大化的宽恕概率 $q^{\\star}$。\n\n给出 $q^{\\star}$ 的最终答案，形式为单个精确值。无需四舍五入，也无需单位。",
            "solution": "该问题在演化博弈论的框架内是适定的，并具有科学依据。所有参数和策略规则都已明确定义。我们将开始求解。\n\n令系统的状态由两个参与者在上一回合的已实现行动决定。状态空间为 $\\{(C,C), (C,D), (D,C), (D,D)\\}$，其中第一个条目是焦点参与者的行动，第二个是其伙伴的行动。我们在错误率 $\\epsilon$ 很小（$0 < \\epsilon \\ll 1$）的假设下分析该系统，并保留到 $\\epsilon$ 的一阶项。这意味着我们忽略 $\\epsilon^2$ 及更高阶的项。一个关键的推论是，我们假设每次最多发生一个错误；一个“错误事件”由单个错误引发，并假设恢复过程中没有进一步的错误。\n\n我们分析的基准是永久相互合作的情况，这种情况在没有错误（$\\epsilon=0$）时会发生。在这种情景下，每一回合的收益是 $u_t = b-c$。归一化贴现收益为 $V_{\\text{coop}} = (1-w)\\sum_{t=0}^{\\infty} w^t(b-c) = b-c$。\n\n有错误时，系统会偶尔偏离合作状态 $(C,C)$。在没有错误的情况下，相互合作状态是一个吸收态。在有错误的情况下，它是马尔可夫链中的一个遍历态。当处于相互合作状态时，单个参与者意外背叛，便开始一个“错误事件”。这可能以两种方式发生，其概率（在 $\\epsilon$ 的一阶近似下）相等：\n1. 焦点参与者合作，而其伙伴意外背叛，导致行动组合为 $(C,D)$。这发生的概率为 $(1-\\epsilon)\\epsilon \\approx \\epsilon$。\n2. 焦点参与者意外背叛，而其伙伴合作，导致行动组合为 $(D,C)$。这发生的概率为 $\\epsilon(1-\\epsilon) \\approx \\epsilon$。\n因此，事件启动的总速率为 $2\\epsilon$。\n\n问题要求两件事：首先，每个事件的预期归一化贴现损失；其次，最优宽恕概率 $q^\\star$。我们首先分析错误发生后的恢复过程。\n\n令 $L_{XY}$ 为焦点参与者的预期未来总贴现损失，从参与者打出行动对 $(X,Y)$ 的下一回合开始计算。损失是相对于每一回合 $b-c$ 的永久合作收益来计算的。\n\n假设上一回合的结果是 $(C,D)$。焦点参与者看到了背叛，因此在本回合意图以概率 $q$ 合作。其伙伴看到了合作，意图以概率 $1$ 合作。忽略进一步的错误，可能的结果是：\n- 以概率 $q$，焦点参与者合作。结果是 $(C,C)$。系统回到完全合作状态。收益为 $b-c$，所以本回合损失为 $0$。未来损失也为 $0$。\n- 以概率 $1-q$，焦点参与者背叛。结果是 $(D,C)$。收益为 $b$，所以本回合损失为 $(b-c) - b = -c$。系统转移到历史为 $(D,C)$ 的状态。从下一回合开始的预期贴现未来损失是 $w L_{DC}$。\n\n这给出了损失 $L_{CD}$ 的递归关系：\n$$L_{CD} = q \\cdot (0) + (1-q)(-c + w L_{DC}) = (1-q)(-c + w L_{DC})$$\n\n假设上一回合的结果是 $(D,C)$。焦点参与者看到了合作，意图以概率 $1$ 合作。其伙伴看到了背叛，意图以概率 $q$ 合作。可能的结果是：\n- 以概率 $q$，伙伴合作。结果是 $(C,C)$。损失为 $0$。\n- 以概率 $1-q$，伙伴背叛。结果是 $(C,D)$。收益为 $-c$，所以损失为 $(b-c) - (-c) = b$。系统转回到历史为 $(C,D)$ 的状态。预期贴现未来损失是 $w L_{CD}$。\n\n这给出了损失 $L_{DC}$ 的第二个递归关系：\n$$L_{DC} = q \\cdot (0) + (1-q)(b + w L_{CD}) = (1-q)(b + w L_{CD})$$\n\n我们得到了一个关于 $L_{CD}$ 和 $L_{DC}$ 的二元线性方程组：\n$$L_{CD} = -(1-q)c + w(1-q)L_{DC}$$\n$$L_{DC} = (1-q)b + w(1-q)L_{CD}$$\n将第二个方程代入第一个方程，得到：\n$$L_{CD} = -(1-q)c + w(1-q)[(1-q)b + w(1-q)L_{CD}]$$\n$$L_{CD} = -c(1-q) + wb(1-q)^2 + w^2(1-q)^2 L_{CD}$$\n$$L_{CD}(1 - w^2(1-q)^2) = -c(1-q) + wb(1-q)^2$$\n$$L_{CD} = \\frac{(1-q)(-c + wb(1-q))}{1 - w^2(1-q)^2} = \\frac{(1-q)(-c + wb(1-q))}{(1 - w(1-q))(1 + w(1-q))}$$\n类似地，我们可以解出 $L_{DC}$：\n$$L_{DC} = \\frac{(1-q)(b - wc(1-q))}{1 - w^2(1-q)^2}$$\n\n一个错误事件的总损失包括错误发生回合的损失，加上贴现后的未来损失。\n情况A：伙伴出错，结果为 $(C,D)$。焦点参与者在错误回合的损失：$(b-c) - (-c) = b$。此类事件的总贴现损失：$\\mathcal{L}_A = b + wL_{CD}$。\n情况B：焦点参与者出错，结果为 $(D,C)$。焦点参与者在错误回合的损失：$(b-c) - b = -c$。此类事件的总贴现损失：$\\mathcal{L}_B = -c + wL_{DC}$。\n\n这两种事件类型以相等的概率发生。每个事件的平均预期贴现损失 $\\bar{L}_{\\text{ep}}$ 为：\n$$\\bar{L}_{\\text{ep}}(q) = \\frac{1}{2}(\\mathcal{L}_A + \\mathcal{L}_B) = \\frac{1}{2}(b - c + w(L_{CD} + L_{DC}))$$\n我们来计算 $L_{CD} + L_{DC}$：\n$$L_{CD} + L_{DC} = \\frac{(1-q)[-c + wb(1-q) + b - wc(1-q)]}{1 - w^2(1-q)^2}$$\n$$= \\frac{(1-q)[(b-c) + w(1-q)(b-c)]}{(1 - w(1-q))(1 + w(1-q))} = \\frac{(1-q)(b-c)(1+w(1-q))}{(1-w(1-q))(1+w(1-q))}$$\n$$= \\frac{(1-q)(b-c)}{1-w(1-q)}$$\n将此代入 $\\bar{L}_{\\text{ep}}(q)$ 的表达式中：\n$$\\bar{L}_{\\text{ep}}(q) = \\frac{b-c}{2} + \\frac{w}{2} \\frac{(1-q)(b-c)}{1-w(1-q)} = \\frac{b-c}{2} \\left(1 + \\frac{w(1-q)}{1-w(1-q)}\\right)$$\n$$\\bar{L}_{\\text{ep}}(q) = \\frac{b-c}{2} \\left(\\frac{1-w(1-q)+w(1-q)}{1-w(1-q)}\\right) = \\frac{b-c}{2(1-w(1-q))}$$\n项 $1-w(1-q)$ 可以重写为 $1-w+wq$。\n因此，每个事件的平均总贴现损失为 $\\bar{L}_{\\text{ep}}(q) = \\frac{b-c}{2(1-w+wq)}$。\n\n问题要求的是“归一化”损失，根据该领域的惯例，这意味着乘以归一化因子 $(1-w)$。\n每个事件的预期归一化贴现损失 $\\mathcal{L}(q)$ 为：\n$$\\mathcal{L}(q) = (1-w)\\bar{L}_{\\text{ep}}(q) = \\frac{(1-w)(b-c)}{2(1-w+wq)}$$\n这是问题第一部分的答案。\n\n对于第二部分，我们必须找到使总预期归一化收益 $V$ 最大化的宽恕概率 $q^\\star$。总收益 $V(q)$ 是基准收益减去在无限时间范围内由错误引起的总损失。在一阶 $\\epsilon$ 近似下，总损失是事件发生率（$2\\epsilon$）乘以每个事件的平均价值损失。每个事件的价值损失为 $\\bar{L}_{\\text{ep}}(q)$。\n因此，总收益 $V(q)$ 由下式给出：\n$$V(q) \\approx V_{\\text{coop}} - (\\text{rate}) \\times (\\text{loss in value per episode})$$\n$$V(q) \\approx (b-c) - 2\\epsilon \\cdot \\bar{L}_{\\text{ep}}(q) = (b-c) - 2\\epsilon \\frac{b-c}{2(1-w+wq)}$$\n$$V(q) = (b-c)\\left(1 - \\frac{\\epsilon}{1-w+wq}\\right)$$\n为了在固定的 $b>c>0$，$w \\in (0,1)$ 和 $\\epsilon > 0$ 条件下最大化 $V(q)$，我们必须最小化项 $\\frac{\\epsilon}{1-w+wq}$。这等价于最大化分母 $D(q) = 1-w+wq$。\n我们考察 $D(q)$ 关于 $q$ 的导数：\n$$\\frac{dD}{dq} = \\frac{d}{dq}(1-w+wq) = w$$\n由于 $w \\in (0,1)$，我们有 $w>0$。因此，$D(q)$ 在其定义域 $q \\in [0,1]$ 上是 $q$ 的严格增函数。\n为了最大化 $D(q)$，我们必须选择 $q$ 可能的最大值。定义域内的最大值是 $q=1$。\n因此，最优宽恕概率为 $q^\\star = 1$。这对应于一种“慷慨”的策略，即最大程度地宽恕，在伙伴单次背叛后立即尝试恢复合作。这最小化了由错误引发的冲突事件的持续时间和成本。",
            "answer": "$$\\boxed{1}$$"
        }
    ]
}