{
    "hands_on_practices": [
        {
            "introduction": "This exercise grounds the abstract concepts of AIC and BIC in a concrete, experimental context relevant to synthetic biology. By working through the calculation for a simple Poisson model fit to hypothetical single-molecule FISH (smFISH) data, you will practice the fundamental steps of maximum likelihood estimation and see how the maximized log-likelihood, $\\ell(\\hat{\\lambda})$, directly feeds into the final AIC and BIC formulas. This foundational practice is essential for building confidence in applying these criteria to more complex models. ",
            "id": "3919160",
            "problem": "In single-molecule fluorescence in situ hybridization (smFISH) experiments for synthetic biology modeling, per-cell messenger RNA (mRNA) counts for a gene are observed across independent cells. Assume a generative model in which each cell’s count is an independent draw from a Poisson distribution with unknown rate parameter $\\lambda$. This setting is commonly used to evaluate transcriptional activity under a homogeneous condition. You observe counts across $n = 12$ cells:\n$$\ny = \\{0, 1, 2, 4, 3, 5, 2, 0, 1, 3, 4, 2\\}.\n$$\nStarting from the definition of the Poisson probability mass function and the independence of draws, derive the maximum likelihood estimate $\\hat{\\lambda}$ and the maximized log-likelihood $\\ell(\\hat{\\lambda})$ for this dataset. Then, using the first-principles definitions that relate the Akaike Information Criterion (AIC) to expected Kullback–Leibler discrepancy and the Bayesian Information Criterion (BIC) to large-sample approximations of log marginal likelihood, determine how $\\hat{\\lambda}$ and $\\ell(\\hat{\\lambda})$ enter these criteria for a model with $k = 1$ free parameter and the given sample size $n$. Compute the numerical values of $\\hat{\\lambda}$, $\\ell(\\hat{\\lambda})$, $\\mathrm{AIC}$, and $\\mathrm{BIC}$ for the dataset provided. Round all numerical results to four significant figures. Express your final answer as a row vector $(\\hat{\\lambda}, \\ell(\\hat{\\lambda}), \\mathrm{AIC}, \\mathrm{BIC})$ in the $\\mathrm{pmatrix}$ format.",
            "solution": "The problem statement is validated as complete, consistent, scientifically grounded, and well-posed. The task involves a standard application of maximum likelihood estimation and model selection theory to a common problem in quantitative biology.\n\nThe solution proceeds as follows:\nFirst, we derive the maximum likelihood estimate ($\\mathrm{MLE}$) for the parameter $\\lambda$ of a Poisson distribution. Let the observed mRNA counts be $y = \\{y_1, y_2, \\ldots, y_n\\}$, where $n=12$. The probability mass function (PMF) for a single observation $y_i$ from a Poisson distribution with rate parameter $\\lambda$ is:\n$$ P(Y=y_i | \\lambda) = \\frac{\\lambda^{y_i} \\exp(-\\lambda)}{y_i!} $$\nSince the observations are independent and identically distributed (i.i.d.), the likelihood function $L(\\lambda | y)$ for the entire dataset is the product of the individual probabilities:\n$$ L(\\lambda | y) = \\prod_{i=1}^{n} P(Y=y_i | \\lambda) = \\prod_{i=1}^{n} \\frac{\\lambda^{y_i} \\exp(-\\lambda)}{y_i!} $$\nTo simplify the maximization, we work with the log-likelihood function, $\\ell(\\lambda | y) = \\ln(L(\\lambda | y))$:\n$$ \\ell(\\lambda | y) = \\ln \\left( \\prod_{i=1}^{n} \\frac{\\lambda^{y_i} \\exp(-\\lambda)}{y_i!} \\right) = \\sum_{i=1}^{n} \\ln \\left( \\frac{\\lambda^{y_i} \\exp(-\\lambda)}{y_i!} \\right) $$\nUsing the properties of logarithms, this simplifies to:\n$$ \\ell(\\lambda | y) = \\sum_{i=1}^{n} (y_i \\ln(\\lambda) - \\lambda - \\ln(y_i!)) = (\\ln \\lambda) \\sum_{i=1}^{n} y_i - n\\lambda - \\sum_{i=1}^{n} \\ln(y_i!) $$\nTo find the MLE $\\hat{\\lambda}$, we take the derivative of $\\ell(\\lambda | y)$ with respect to $\\lambda$ and set it to zero:\n$$ \\frac{d\\ell}{d\\lambda} = \\frac{d}{d\\lambda} \\left( (\\ln \\lambda) \\sum_{i=1}^{n} y_i - n\\lambda - \\sum_{i=1}^{n} \\ln(y_i!) \\right) = \\frac{1}{\\lambda} \\sum_{i=1}^{n} y_i - n $$\nSetting the derivative to zero and solving for $\\lambda$ yields the MLE, $\\hat{\\lambda}$:\n$$ \\frac{1}{\\hat{\\lambda}} \\sum_{i=1}^{n} y_i - n = 0 \\implies \\hat{\\lambda} = \\frac{\\sum_{i=1}^{n} y_i}{n} = \\bar{y} $$\nThe MLE for the Poisson rate parameter is the sample mean of the observations. For the given dataset $y = \\{0, 1, 2, 4, 3, 5, 2, 0, 1, 3, 4, 2\\}$, the sample size is $n=12$ and the sum of counts is:\n$$ \\sum_{i=1}^{12} y_i = 0 + 1 + 2 + 4 + 3 + 5 + 2 + 0 + 1 + 3 + 4 + 2 = 27 $$\nThus, the numerical value for the MLE is:\n$$ \\hat{\\lambda} = \\frac{27}{12} = 2.25 $$\nNext, we calculate the maximized log-likelihood, $\\ell(\\hat{\\lambda})$, by substituting $\\hat{\\lambda}$ back into the log-likelihood function:\n$$ \\ell(\\hat{\\lambda}) = (\\ln \\hat{\\lambda}) \\sum_{i=1}^{n} y_i - n\\hat{\\lambda} - \\sum_{i=1}^{n} \\ln(y_i!) $$\nSubstituting the numerical values $\\sum y_i = 27$, $n=12$, and $\\hat{\\lambda}=2.25$:\n$$ \\ell(\\hat{\\lambda}) = 27 \\ln(2.25) - (12)(2.25) - \\sum_{i=1}^{12} \\ln(y_i!) = 27 \\ln(2.25) - 27 - \\sum_{i=1}^{12} \\ln(y_i!) $$\nThe term $\\sum \\ln(y_i!)$ is calculated based on the data:\n$$ \\sum_{i=1}^{12} \\ln(y_i!) = 2\\ln(0!) + 2\\ln(1!) + 3\\ln(2!) + 2\\ln(3!) + 2\\ln(4!) + 1\\ln(5!) $$\nSince $0! = 1$ and $1! = 1$, $\\ln(0!) = \\ln(1) = 0$.\n$$ \\sum_{i=1}^{12} \\ln(y_i!) = 3\\ln(2) + 2\\ln(6) + 2\\ln(24) + \\ln(120) $$\n$$ \\sum_{i=1}^{12} \\ln(y_i!) \\approx 3(0.693147) + 2(1.791759) + 2(3.178054) + 4.787492 \\approx 16.806581 $$\nNow, we compute $\\ell(\\hat{\\lambda})$:\n$$ \\ell(\\hat{\\lambda}) \\approx 27 \\ln(2.25) - 27 - 16.806581 \\approx 27(0.810930) - 27 - 16.806581 $$\n$$ \\ell(\\hat{\\lambda}) \\approx 21.895116 - 27 - 16.806581 = -21.911465 $$\nNow we turn to the model selection criteria. The Akaike Information Criterion ($\\mathrm{AIC}$) is defined from first principles as an estimator of the expected Kullback-Leibler divergence between the fitted model and the true underlying generative process. For a model with $k$ free parameters, its standard form is:\n$$ \\mathrm{AIC} = -2\\ell(\\hat{\\theta}) + 2k $$\nwhere $\\hat{\\theta}$ represents the vector of MLE parameters. In our case, the model has only one free parameter, $\\lambda$, so $k=1$. The term $-2\\ell(\\hat{\\theta})$ is related to the goodness-of-fit, and $2k$ is a penalty for model complexity.\n$$ \\mathrm{AIC} = -2\\ell(\\hat{\\lambda}) + 2(1) \\approx -2(-21.911465) + 2 = 43.82293 + 2 = 45.82293 $$\nThe Bayesian Information Criterion ($\\mathrm{BIC}$) is derived from a large-sample approximation to the log of the marginal likelihood (or model evidence). It is defined as:\n$$ \\mathrm{BIC} = -2\\ell(\\hat{\\theta}) + k \\ln(n) $$\nwhere $n$ is the sample size. The penalty term, $k\\ln(n)$, is more severe than that of $\\mathrm{AIC}$ for $n \\ge 8$. For our problem, $k=1$ and $n=12$.\n$$ \\mathrm{BIC} = -2\\ell(\\hat{\\lambda}) + (1)\\ln(12) \\approx 43.82293 + \\ln(12) $$\n$$ \\ln(12) \\approx 2.484907 $$\n$$ \\mathrm{BIC} \\approx 43.82293 + 2.484907 = 46.307837 $$\nFinally, we round the computed values to four significant figures:\n- $\\hat{\\lambda} = 2.250$\n- $\\ell(\\hat{\\lambda}) = -21.91$\n- $\\mathrm{AIC} = 45.82$\n- $\\mathrm{BIC} = 46.31$",
            "answer": "$$ \\boxed{\\begin{pmatrix} 2.250  -21.91  45.82  46.31 \\end{pmatrix}} $$"
        },
        {
            "introduction": "To truly understand model selection, one must grasp how different criteria penalize complexity. This practice explores the trade-off between model fit and parsimony by deriving the exact log-likelihood improvement, $\\Delta \\ln \\hat{L}$, required to justify adding a single parameter according to both AIC and BIC. Comparing these thresholds directly reveals the different penalization schemes of the two criteria and helps build intuition for when one might be preferred over the other in your own modeling work. ",
            "id": "3919103",
            "problem": "A synthetic biology team is modeling single-cell messenger ribonucleic acid ($\\mathrm{mRNA}$) counts from a promoter under an inducible condition. They compare two nested stochastic models of transcriptional bursting for $n$ independent cells: a simpler model $\\mathcal{M}_{S}$ with parameter dimension $k_{S}$, and a more complex model $\\mathcal{M}_{C}$ with parameter dimension $k_{C} = k_{S} + 1$ that adds one biologically interpretable parameter capturing induction-dependent burst size. Let $\\ln \\hat{L}_{S}$ and $\\ln \\hat{L}_{C}$ denote the maximized log-likelihoods under the two models, computed at their respective maximum likelihood estimates (MLEs). Define the log-likelihood improvement of the complex over the simple model as $\\Delta \\ln \\hat{L} = \\ln \\hat{L}_{C} - \\ln \\hat{L}_{S}$, with $\\Delta \\ln \\hat{L} \\geq 0$ for nested models fit by maximum likelihood.\n\nStarting from the standard definitions of the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) as penalized criteria combining a goodness-of-fit term and a complexity penalty, and using only the assumptions stated above, derive the minimal threshold in $\\Delta \\ln \\hat{L}$ required for the complex model $\\mathcal{M}_{C}$ to be preferred over the simple model $\\mathcal{M}_{S}$ under each criterion for a given sample size $n$. Express your final answer as two exact analytic expressions in terms of $n$, presented as a single row matrix $[\\Delta_{\\mathrm{AIC}} \\quad \\Delta_{\\mathrm{BIC}}]$, where $\\Delta_{\\mathrm{AIC}}$ and $\\Delta_{\\mathrm{BIC}}$ are the smallest values of $\\Delta \\ln \\hat{L}$ that make the complex model favored by the Akaike Information Criterion and the Bayesian Information Criterion, respectively. No rounding is required and no units should be used in your final expressions.",
            "solution": "The problem is valid. It is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. We proceed with the derivation.\n\nThe objective is to determine the minimal threshold for the log-likelihood improvement, $\\Delta \\ln \\hat{L} = \\ln \\hat{L}_{C} - \\ln \\hat{L}_{S}$, that is required for the more complex model, $\\mathcal{M}_{C}$, to be preferred over the simpler model, $\\mathcal{M}_{S}$, according to the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC).\n\nFor model selection criteria like AIC and BIC, the model with the lower criterion value is preferred. Thus, $\\mathcal{M}_{C}$ is preferred over $\\mathcal{M}_{S}$ if $\\text{AIC}(\\mathcal{M}_{C})  \\text{AIC}(\\mathcal{M}_{S})$ and $\\text{BIC}(\\mathcal{M}_{C})  \\text{BIC}(\\mathcal{M}_{S})$, respectively. The threshold value corresponds to the point of equality.\n\nLet $k$ be the number of estimated parameters in a model, $\\hat{L}$ be the maximized value of the likelihood function for the model, and $n$ be the number of data points (in this case, the number of independent cells).\n\n**1. Derivation for the Akaike Information Criterion (AIC)**\n\nThe standard definition of AIC is:\n$$ \\text{AIC} = -2\\ln \\hat{L} + 2k $$\n\nFor our two models, $\\mathcal{M}_{S}$ and $\\mathcal{M}_{C}$, the AIC values are:\n$$ \\text{AIC}_{S} = -2\\ln \\hat{L}_{S} + 2k_{S} $$\n$$ \\text{AIC}_{C} = -2\\ln \\hat{L}_{C} + 2k_{C} $$\n\nThe complex model $\\mathcal{M}_{C}$ is preferred over the simple model $\\mathcal{M}_{S}$ when $\\text{AIC}_{C}  \\text{AIC}_{S}$. We can write this inequality as:\n$$ -2\\ln \\hat{L}_{C} + 2k_{C}  -2\\ln \\hat{L}_{S} + 2k_{S} $$\n\nTo find the condition on the log-likelihood improvement, we rearrange the inequality to group the likelihood terms and the parameter count terms:\n$$ 2\\ln \\hat{L}_{C} - 2\\ln \\hat{L}_{S}  2k_{C} - 2k_{S} $$\n$$ 2(\\ln \\hat{L}_{C} - \\ln \\hat{L}_{S})  2(k_{C} - k_{S}) $$\n\nUsing the problem's definition, $\\Delta \\ln \\hat{L} = \\ln \\hat{L}_{C} - \\ln \\hat{L}_{S}$, we have:\n$$ 2\\Delta \\ln \\hat{L}  2(k_{C} - k_{S}) $$\n$$ \\Delta \\ln \\hat{L}  k_{C} - k_{S} $$\n\nThe problem states that the parameter dimension of the complex model is greater than that of the simple model by one, i.e., $k_{C} = k_{S} + 1$, which implies $k_{C} - k_{S} = 1$. Substituting this into the inequality gives:\n$$ \\Delta \\ln \\hat{L}  1 $$\n\nThe minimal value of $\\Delta \\ln \\hat{L}$ required to favor $\\mathcal{M}_{C}$ is the value at the threshold of this inequality. Therefore, the minimal threshold for AIC is:\n$$ \\Delta_{\\text{AIC}} = 1 $$\n\n**2. Derivation for the Bayesian Information Criterion (BIC)**\n\nThe standard definition of BIC is:\n$$ \\text{BIC} = -2\\ln \\hat{L} + k\\ln(n) $$\nwhere $n$ is the sample size.\n\nFor our two models, the BIC values are:\n$$ \\text{BIC}_{S} = -2\\ln \\hat{L}_{S} + k_{S}\\ln(n) $$\n$$ \\text{BIC}_{C} = -2\\ln \\hat{L}_{C} + k_{C}\\ln(n) $$\n\nThe complex model $\\mathcal{M}_{C}$ is preferred over the simple model $\\mathcal{M}_{S}$ when $\\text{BIC}_{C}  \\text{BIC}_{S}$:\n$$ -2\\ln \\hat{L}_{C} + k_{C}\\ln(n)  -2\\ln \\hat{L}_{S} + k_{S}\\ln(n) $$\n\nWe again rearrange to isolate the log-likelihood improvement:\n$$ 2\\ln \\hat{L}_{C} - 2\\ln \\hat{L}_{S}  k_{C}\\ln(n) - k_{S}\\ln(n) $$\n$$ 2(\\ln \\hat{L}_{C} - \\ln \\hat{L}_{S})  (k_{C} - k_{S})\\ln(n) $$\n\nSubstituting $\\Delta \\ln \\hat{L} = \\ln \\hat{L}_{C} - \\ln \\hat{L}_{S}$:\n$$ 2\\Delta \\ln \\hat{L}  (k_{C} - k_{S})\\ln(n) $$\n\nAgain, we use the fact that $k_{C} - k_{S} = 1$:\n$$ 2\\Delta \\ln \\hat{L}  \\ln(n) $$\n$$ \\Delta \\ln \\hat{L}  \\frac{1}{2}\\ln(n) $$\n\nThe minimal value of $\\Delta \\ln \\hat{L}$ required to favor $\\mathcal{M}_{C}$ under the BIC is the value at the boundary of this condition. Thus, the minimal threshold for BIC is:\n$$ \\Delta_{\\text{BIC}} = \\frac{1}{2}\\ln(n) $$\n\nThe final answer is composed of the two derived thresholds, $\\Delta_{\\text{AIC}}$ and $\\Delta_{\\text{BIC}}$, presented as a row matrix.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1  \\frac{1}{2}\\ln(n) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "While AIC and BIC appear structurally similar, they stem from fundamentally different theoretical frameworks. This advanced exercise delves into the Bayesian origins of the BIC, showing how it arises from a large-sample approximation of the Bayes factor, a cornerstone of Bayesian model comparison. By working through this derivation and a practical calculation, you will gain a deeper appreciation for why BIC tends to favor simpler models and how to interpret its results in the context of model evidence. ",
            "id": "3919110",
            "problem": "Consider a synthetic biology modeling study of a repressed gene circuit where messenger ribonucleic acid (mRNA) production is inhibited by a transcription factor. Two nested models are fit to a dataset comprising $n$ independent measurements of mRNA concentration under steady-state conditions across replicated experiments. The simpler model $\\mathcal{M}_{0}$ assumes noncooperative repression and has $k_{0}$ free parameters (transcription rate, degradation rate, and dissociation constant), while the more complex model $\\mathcal{M}_{1}$ allows cooperative repression via a Hill coefficient and has $k_{1}$ free parameters. The models are nested because $\\mathcal{M}_{1}$ reduces to $\\mathcal{M}_{0}$ when the Hill coefficient equals $1$. Maximum likelihood estimation yields the maximized likelihoods $\\hat{L}_{0}$ and $\\hat{L}_{1}$ for $\\mathcal{M}_{0}$ and $\\mathcal{M}_{1}$, respectively.\n\nStarting from the definition of the likelihood function $p(\\mathbf{y}\\mid \\boldsymbol{\\theta}, \\mathcal{M})$ and the marginal likelihood $p(\\mathbf{y}\\mid \\mathcal{M})=\\int p(\\mathbf{y}\\mid \\boldsymbol{\\theta}, \\mathcal{M})\\,p(\\boldsymbol{\\theta}\\mid \\mathcal{M})\\,\\mathrm{d}\\boldsymbol{\\theta}$, and using the large-sample Laplace approximation as a fundamental base, derive the model selection criterion known as the Bayesian Information Criterion (BIC) for a model with $k$ parameters fit to $n$ observations. Then, for the two nested models described, derive an expression for the BIC difference $\\Delta \\mathrm{BIC}=\\mathrm{BIC}_{1}-\\mathrm{BIC}_{0}$ in terms of $n$, $k_{0}$, $k_{1}$, and the maximized likelihoods $\\hat{L}_{0}$ and $\\hat{L}_{1}$. Use this expression to compute the numerical value of $\\Delta \\mathrm{BIC}$ for the following empirically obtained quantities:\n- $n=150$,\n- $k_{0}=3$,\n- $k_{1}=4$,\n- $\\hat{L}_{0}=1.2\\times 10^{-180}$,\n- $\\hat{L}_{1}=3.6\\times 10^{-175}$.\n\nFinally, explain how $\\Delta \\mathrm{BIC}$ approximates $2\\ln$ of the Bayes factor comparing $\\mathcal{M}_{1}$ to $\\mathcal{M}_{0}$, and indicate the direction of evidence implied by the sign of $\\Delta \\mathrm{BIC}$. Round your computed $\\Delta \\mathrm{BIC}$ to four significant figures. Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) must be clearly distinguished in your derivation; focus your computation on BIC.",
            "solution": "The problem requires the derivation of the Bayesian Information Criterion (BIC), its application to a model selection problem in synthetic biology, and the interpretation of the result.\n\nFirst, we derive the BIC from the principles laid out in the problem statement. The goal of Bayesian model selection is to compare models based on their posterior probabilities, $p(\\mathcal{M} \\mid \\mathbf{y})$, given the data $\\mathbf{y}$. Using Bayes' theorem, the posterior probability of a model $\\mathcal{M}$ is $p(\\mathcal{M} \\mid \\mathbf{y}) = \\frac{p(\\mathbf{y} \\mid \\mathcal{M}) p(\\mathcal{M})}{p(\\mathbf{y})}$. When comparing two models, $\\mathcal{M}_{0}$ and $\\mathcal{M}_{1}$, with equal prior probabilities $p(\\mathcal{M}_{0}) = p(\\mathcal{M}_{1})$, the comparison reduces to evaluating the ratio of their marginal likelihoods, also known as the Bayes factor, $B_{10} = p(\\mathbf{y} \\mid \\mathcal{M}_{1}) / p(\\mathbf{y} \\mid \\mathcal{M}_{0})$. The marginal likelihood, or model evidence, is given by the integral of the likelihood function over the prior distribution of the parameters $\\boldsymbol{\\theta}$:\n$$p(\\mathbf{y} \\mid \\mathcal{M}) = \\int p(\\mathbf{y} \\mid \\boldsymbol{\\theta}, \\mathcal{M}) \\, p(\\boldsymbol{\\theta} \\mid \\mathcal{M}) \\, \\mathrm{d}\\boldsymbol{\\theta}$$\nLet $L(\\boldsymbol{\\theta}) = p(\\mathbf{y} \\mid \\boldsymbol{\\theta}, \\mathcal{M})$ be the likelihood and $\\pi(\\boldsymbol{\\theta}) = p(\\boldsymbol{\\theta} \\mid \\mathcal{M})$ be the prior. The integral is $\\int L(\\boldsymbol{\\theta})\\pi(\\boldsymbol{\\theta}) \\, \\mathrm{d}\\boldsymbol{\\theta}$. It is often more convenient to work with the logarithm of the integrand. We define $S(\\boldsymbol{\\theta}) = \\ln(L(\\boldsymbol{\\theta})\\pi(\\boldsymbol{\\theta}))$. The marginal likelihood is then $p(\\mathbf{y} \\mid \\mathcal{M}) = \\int \\exp(S(\\boldsymbol{\\theta})) \\, \\mathrm{d}\\boldsymbol{\\theta}$.\n\nWe use the Laplace approximation to evaluate this integral, which is effective when the posterior distribution is concentrated in a small region around its mode. We perform a second-order Taylor series expansion of $S(\\boldsymbol{\\theta})$ around its maximum, the maximum a posteriori (MAP) estimate $\\tilde{\\boldsymbol{\\theta}}$:\n$$S(\\boldsymbol{\\theta}) \\approx S(\\tilde{\\boldsymbol{\\theta}}) + (\\boldsymbol{\\theta} - \\tilde{\\boldsymbol{\\theta}})^T \\nabla S(\\tilde{\\boldsymbol{\\theta}}) + \\frac{1}{2}(\\boldsymbol{\\theta} - \\tilde{\\boldsymbol{\\theta}})^T H(\\tilde{\\boldsymbol{\\theta}}) (\\boldsymbol{\\theta} - \\tilde{\\boldsymbol{\\theta}})$$\nwhere $H$ is the Hessian matrix of $S$. By definition of the maximum, the gradient $\\nabla S(\\tilde{\\boldsymbol{\\theta}})$ is zero. Let $A = -H(\\tilde{\\boldsymbol{\\theta}})$ be the negative Hessian at the mode, which is a positive definite matrix. The expansion simplifies to:\n$$S(\\boldsymbol{\\theta}) \\approx S(\\tilde{\\boldsymbol{\\theta}}) - \\frac{1}{2}(\\boldsymbol{\\theta} - \\tilde{\\boldsymbol{\\theta}})^T A (\\boldsymbol{\\theta} - \\tilde{\\boldsymbol{\\theta}})$$\nSubstituting this back into the integral for the marginal likelihood:\n$$p(\\mathbf{y} \\mid \\mathcal{M}) \\approx \\int \\exp\\left(S(\\tilde{\\boldsymbol{\\theta}}) - \\frac{1}{2}(\\boldsymbol{\\theta} - \\tilde{\\boldsymbol{\\theta}})^T A (\\boldsymbol{\\theta} - \\tilde{\\boldsymbol{\\theta}})\\right) \\, \\mathrm{d}\\boldsymbol{\\theta} = \\exp(S(\\tilde{\\boldsymbol{\\theta}})) \\int \\exp\\left(-\\frac{1}{2}(\\boldsymbol{\\theta} - \\tilde{\\boldsymbol{\\theta}})^T A (\\boldsymbol{\\theta} - \\tilde{\\boldsymbol{\\theta}})\\right) \\, \\mathrm{d}\\boldsymbol{\\theta}$$\nThe integral is the kernel of a multivariate normal distribution with mean $\\tilde{\\boldsymbol{\\theta}}$ and covariance matrix $A^{-1}$. Its value is $(2\\pi)^{k/2}(\\det A)^{-1/2}$, where $k$ is the dimensionality of $\\boldsymbol{\\theta}$ (the number of free parameters). Thus,\n$$p(\\mathbf{y} \\mid \\mathcal{M}) \\approx \\exp(S(\\tilde{\\boldsymbol{\\theta}})) (2\\pi)^{k/2} (\\det A)^{-1/2} = L(\\tilde{\\boldsymbol{\\theta}})\\pi(\\tilde{\\boldsymbol{\\theta}}) (2\\pi)^{k/2} (\\det A)^{-1/2}$$\nFor a large number of independent observations $n$, several approximations are made:\n1. The MAP estimate $\\tilde{\\boldsymbol{\\theta}}$ converges to the maximum likelihood estimate (MLE), $\\hat{\\boldsymbol{\\theta}}$.\n2. The term $\\pi(\\tilde{\\boldsymbol{\\theta}})$ becomes negligible compared to the likelihood term.\n3. The matrix $A = -H(\\tilde{\\boldsymbol{\\theta}})$ is dominated by the Hessian of the log-likelihood, which is approximately the observed Fisher information matrix, $I(\\hat{\\boldsymbol{\\theta}})$. For large $n$, we have $I(\\hat{\\boldsymbol{\\theta}}) \\approx n \\cdot i(\\hat{\\boldsymbol{\\theta}})$, where $i(\\hat{\\boldsymbol{\\theta}})$ is the Fisher information for a single observation. The determinant scales as $\\det(A) \\approx \\det(n \\cdot i(\\hat{\\boldsymbol{\\theta}})) = n^k \\det(i(\\hat{\\boldsymbol{\\theta}}))$.\n\nTaking the logarithm of the marginal likelihood:\n$$\\ln p(\\mathbf{y} \\mid \\mathcal{M}) \\approx \\ln L(\\hat{\\boldsymbol{\\theta}}) + \\ln\\pi(\\hat{\\boldsymbol{\\theta}}) + \\frac{k}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln(\\det A)$$\nSubstituting the approximation for $\\det A$:\n$$\\ln p(\\mathbf{y} \\mid \\mathcal{M}) \\approx \\ln L(\\hat{\\boldsymbol{\\theta}}) + \\ln\\pi(\\hat{\\boldsymbol{\\theta}}) + \\frac{k}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln(n^k \\det(i(\\hat{\\boldsymbol{\\theta}})))$$\n$$\\ln p(\\mathbf{y} \\mid \\mathcal{M}) \\approx \\ln L(\\hat{\\boldsymbol{\\theta}}) - \\frac{k}{2}\\ln n + \\left( \\ln\\pi(\\hat{\\boldsymbol{\\theta}}) + \\frac{k}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln(\\det(i(\\hat{\\boldsymbol{\\theta}}))) \\right)$$\nAs $n \\rightarrow \\infty$, the term $\\ln L(\\hat{\\boldsymbol{\\theta}})$ scales as $O(n)$ while $\\frac{k}{2}\\ln n$ scales as $O(\\ln n)$. The terms in parentheses are of order $O(1)$. Retaining only the terms that grow with $n$, we get the large-sample approximation:\n$$\\ln p(\\mathbf{y} \\mid \\mathcal{M}) \\approx \\ln L(\\hat{\\boldsymbol{\\theta}}) - \\frac{k}{2}\\ln n$$\nThe Bayesian Information Criterion (BIC) is conventionally defined by multiplying this by $-2$, placing it on the same scale as deviance statistics like the Akaike Information Criterion (AIC). Let $\\hat{L} = L(\\hat{\\boldsymbol{\\theta}})$ be the maximized likelihood.\n$$\\mathrm{BIC} = -2\\ln\\hat{L} + k\\ln n$$\nThis criterion penalizes models for complexity (larger $k$) and rewards them for goodness of fit (larger $\\hat{L}$). It should be distinguished from AIC, defined as $\\mathrm{AIC} = -2\\ln\\hat{L} + 2k$. BIC's penalty term, $k\\ln n$, is stricter than AIC's ($2k$) for any sample size $n > \\exp(2) \\approx 7.4$, which is the case here.\n\nNext, we derive the expression for the BIC difference, $\\Delta\\mathrm{BIC} = \\mathrm{BIC}_{1} - \\mathrm{BIC}_{0}$, for the two nested models $\\mathcal{M}_{1}$ and $\\mathcal{M}_{0}$.\nUsing the derived formula for BIC:\n$$\\mathrm{BIC}_{0} = -2\\ln\\hat{L}_{0} + k_{0}\\ln n$$\n$$\\mathrm{BIC}_{1} = -2\\ln\\hat{L}_{1} + k_{1}\\ln n$$\nThe difference is:\n$$\\Delta\\mathrm{BIC} = \\mathrm{BIC}_{1} - \\mathrm{BIC}_{0} = (-2\\ln\\hat{L}_{1} + k_{1}\\ln n) - (-2\\ln\\hat{L}_{0} + k_{0}\\ln n)$$\nRearranging terms gives:\n$$\\Delta\\mathrm{BIC} = -2(\\ln\\hat{L}_{1} - \\ln\\hat{L}_{0}) + (k_{1} - k_{0})\\ln n$$\nThis can be written in terms of the likelihood ratio:\n$$\\Delta\\mathrm{BIC} = -2\\ln\\left(\\frac{\\hat{L}_{1}}{\\hat{L}_{0}}\\right) + (k_{1} - k_{0})\\ln n$$\n\nNow, we compute the numerical value of $\\Delta\\mathrm{BIC}$ using the provided quantities:\n$n=150$, $k_{0}=3$, $k_{1}=4$, $\\hat{L}_{0}=1.2\\times 10^{-180}$, $\\hat{L}_{1}=3.6\\times 10^{-175}$.\nFirst, calculate the likelihood ratio $\\frac{\\hat{L}_{1}}{\\hat{L}_{0}}$:\n$$\\frac{\\hat{L}_{1}}{\\hat{L}_{0}} = \\frac{3.6 \\times 10^{-175}}{1.2 \\times 10^{-180}} = \\frac{3.6}{1.2} \\times 10^{-175 - (-180)} = 3.0 \\times 10^{5}$$\nNext, we compute the two terms in the $\\Delta\\mathrm{BIC}$ expression:\nThe log-likelihood ratio term is:\n$$-2\\ln\\left(\\frac{\\hat{L}_{1}}{\\hat{L}_{0}}\\right) = -2\\ln(3.0 \\times 10^{5}) = -2(\\ln(3.0) + 5\\ln(10)) \\approx -2(1.09861 + 5 \\times 2.30259) \\approx -2(12.61156) = -25.22312$$\nThe penalty term is:\n$$(k_{1} - k_{0})\\ln n = (4 - 3)\\ln(150) = \\ln(150) \\approx 5.010635$$\nCombining these terms:\n$$\\Delta\\mathrm{BIC} = -25.22312 + 5.010635 = -20.212485$$\nRounding the result to four significant figures, we get $\\Delta\\mathrm{BIC} \\approx -20.21$.\n\nFinally, we explain the relationship between $\\Delta\\mathrm{BIC}$ and the Bayes factor. The Bayes factor in favor of model $\\mathcal{M}_{1}$ over $\\mathcal{M}_{0}$ is $B_{10} = \\frac{p(\\mathbf{y} \\mid \\mathcal{M}_{1})}{p(\\mathbf{y} \\mid \\mathcal{M}_{0})}$. From our derivation, $\\ln p(\\mathbf{y} \\mid \\mathcal{M}) \\approx -\\frac{1}{2}\\mathrm{BIC}$. Therefore, twice the log of the Bayes factor is:\n$$2\\ln B_{10} = 2(\\ln p(\\mathbf{y} \\mid \\mathcal{M}_{1}) - \\ln p(\\mathbf{y} \\mid \\mathcal{M}_{0})) \\approx 2\\left(-\\frac{1}{2}\\mathrm{BIC}_{1} - \\left(-\\frac{1}{2}\\mathrm{BIC}_{0}\\right)\\right) = -(\\mathrm{BIC}_{1} - \\mathrm{BIC}_{0}) = -\\Delta\\mathrm{BIC}$$\nThus, $\\Delta\\mathrm{BIC}$ approximates $-2\\ln B_{10}$. The problem asks to relate $\\Delta\\mathrm{BIC}$ to \"$2\\ln$ of the Bayes factor\", which is most precisely stated as $\\Delta\\mathrm{BIC} \\approx -2\\ln B_{10}$ or equivalently $\\Delta\\mathrm{BIC} \\approx 2\\ln B_{01}$, where $B_{01} = 1/B_{10}$ is the Bayes factor in favor of $\\mathcal{M}_{0}$ over $\\mathcal{M}_{1}$.\n\nThe sign of $\\Delta\\mathrm{BIC}$ indicates which model is preferred. Model selection using BIC seeks to find the model with the minimum BIC value.\n- If $\\Delta\\mathrm{BIC} = \\mathrm{BIC}_{1} - \\mathrm{BIC}_{0}  0$, then $\\mathrm{BIC}_{1}  \\mathrm{BIC}_{0}$, and the evidence favors the more complex model, $\\mathcal{M}_{1}$.\n- If $\\Delta\\mathrm{BIC} > 0$, then $\\mathrm{BIC}_{1} > \\mathrm{BIC}_{0}$, and the evidence favors the simpler model, $\\mathcal{M}_{0}$.\nIn this case, $\\Delta\\mathrm{BIC} \\approx -20.21$. This is a strongly negative value. This indicates that there is very strong evidence in favor of the more complex model $\\mathcal{M}_{1}$ (cooperative repression) over the simpler model $\\mathcal{M}_{0}$ (noncooperative repression). The magnitude of the difference ($> 10$) is typically interpreted as \"very strong\" evidence for the preferred model. The improvement in fit provided by the additional parameter (the Hill coefficient) far outweighs the penalty for increased model complexity.",
            "answer": "$$\\boxed{-20.21}$$"
        }
    ]
}