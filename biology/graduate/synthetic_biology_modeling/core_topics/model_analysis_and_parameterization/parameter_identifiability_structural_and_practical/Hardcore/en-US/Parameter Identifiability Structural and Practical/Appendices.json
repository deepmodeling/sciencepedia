{
    "hands_on_practices": [
        {
            "introduction": "Before investing in experiments, it is crucial to determine if a model's parameters can be identified even with perfect, continuous data. This property, known as structural identifiability, can be assessed by recasting the system into an input-output differential equation. This exercise  provides a direct application of this method to a fundamental linear model, demonstrating how to check if distinct parameter values necessarily lead to distinct observable behaviors.",
            "id": "3925268",
            "problem": "In a synthetic biology modeling scenario, consider a linearized Single-Input Single-Output (SISO) gene expression module with state variable $x(t)$ representing a gene product concentration, known input $u(t)$ representing an inducer, and measured output $y(t)$ equal to the product concentration. The dynamics are given by the state-space model\n$$\\dot{x}(t)=-k\\,x(t)+\\alpha\\,u(t),\\quad y(t)=x(t),$$\nwhere the unknown parameter vector is $\\theta=(k,\\alpha)$, with $k$ a positive degradation/dilution rate and $\\alpha$ a positive input gain. Assume $u(t)$ is a sufficiently smooth known function and $y(t)$ is measured without algebraic constraints beyond the model.\n\nStarting only from the definitions of a state-space model and structural identifiability (that a parameterization is structurally identifiable if distinct $\\theta$ generate distinct input-output behavior for almost all admissible inputs), perform the following tasks:\n\n- Eliminate the unobserved state $x(t)$ to derive the minimal-order input-output differential equation in monic form, where “monic” means the coefficient multiplying the highest-order derivative of $y(t)$ is equal to $1$.\n- From this monic input-output differential equation, define the coefficient map $c(\\theta)$ as the vector of the coefficients that multiply the remaining terms involving $y(t)$ and $u(t)$ (and their derivatives if present) and express $c(\\theta)$ explicitly in terms of $\\theta=(k,\\alpha)$.\n- Briefly justify, using the coefficient map, whether each component of $\\theta$ is structurally identifiable.\n\nFor your final answer, report only the coefficient map $c(\\theta)$ as a row matrix. No numerical evaluation or rounding is required, and no units are needed in the final answer box.",
            "solution": "The problem is validated as scientifically grounded, well-posed, objective, and complete. It presents a standard exercise in structural identifiability analysis for a linear time-invariant system, a fundamental topic in systems theory and modeling. All required information is provided, and the tasks are unambiguous.\n\nThe objective is to analyze the structural identifiability of the parameters $\\theta=(k,\\alpha)$ for a given single-input single-output (SISO) system. This involves three steps: first, deriving the input-output differential equation; second, defining the map from the system parameters $\\theta$ to the coefficients of this equation; and third, assessing the injectivity of this map.\n\nThe system is described by the state-space model:\n$$\n\\dot{x}(t) = -k\\,x(t) + \\alpha\\,u(t)\n$$\n$$\ny(t) = x(t)\n$$\nwhere $x(t)$ is the unobserved state, $u(t)$ is the known input, and $y(t)$ is the measured output. The parameters to be identified are $\\theta = (k, \\alpha)$.\n\n**Step 1: Derivation of the Input-Output Equation**\n\nTo eliminate the unobserved state variable $x(t)$, we use the output equation $y(t) = x(t)$. This direct relationship allows us to substitute $y(t)$ for $x(t)$ in the state equation. Differentiating the output equation with respect to time $t$ gives:\n$$\n\\dot{y}(t) = \\frac{dy(t)}{dt} = \\frac{dx(t)}{dt} = \\dot{x}(t)\n$$\nNow, we substitute $x(t) = y(t)$ and $\\dot{x}(t) = \\dot{y}(t)$ into the state equation:\n$$\n\\dot{y}(t) = -k\\,y(t) + \\alpha\\,u(t)\n$$\nThe problem requires this minimal-order input-output differential equation to be in monic form, which means the coefficient of the highest-order derivative of the output $y(t)$ must be $1$. Rearranging the equation, we get:\n$$\n\\dot{y}(t) + k\\,y(t) = \\alpha\\,u(t)\n$$\nIn this equation, the highest-order derivative is $\\dot{y}(t)$, and its coefficient is already $1$. Thus, this is the required monic input-output differential equation. It is of minimal order (first order) because the original state-space model was first order.\n\n**Step 2: Definition of the Coefficient Map $c(\\theta)$**\n\nThe coefficient map $c(\\theta)$ is defined as the vector of coefficients that multiply the terms involving $y(t)$ and $u(t)$ (and their derivatives) in the monic input-output equation. The general form of a first-order monic equation is:\n$$\n\\dot{y}(t) + c_1\\,y(t) = d_0\\,u(t)\n$$\nThe vector of coefficients is $(c_1, d_0)$. By comparing this general form to our derived equation,\n$$\n\\dot{y}(t) + k\\,y(t) = \\alpha\\,u(t)\n$$\nwe can identify the coefficients in terms of the parameter vector $\\theta=(k, \\alpha)$. The coefficient of the $y(t)$ term is $k$, and the coefficient of the $u(t)$ term is $\\alpha$.\n\nTherefore, the coefficient map $c(\\theta)$ is:\n$$\nc(\\theta) = (k, \\alpha)\n$$\n\n**Step 3: Justification of Structural Identifiability**\n\nA parameter vector $\\theta$ is structurally identifiable if the coefficient map $c(\\theta)$ is injective (one-to-one) over the permissible parameter space. This means that for any two distinct parameter vectors $\\theta_1$ and $\\theta_2$, their corresponding coefficient vectors $c(\\theta_1)$ and $c(\\theta_2)$ must also be distinct. Formally, if $c(\\theta_1) = c(\\theta_2)$, it must imply that $\\theta_1 = \\theta_2$.\n\nLet $\\theta_1 = (k_1, \\alpha_1)$ and $\\theta_2 = (k_2, \\alpha_2)$ be two parameter vectors. The condition for them to produce identical input-output behavior is $c(\\theta_1) = c(\\theta_2)$. Applying our derived map:\n$$\n(k_1, \\alpha_1) = (k_2, \\alpha_2)\n$$\nThis vector equality implies two scalar equalities:\n$$\nk_1 = k_2\n$$\n$$\n\\alpha_1 = \\alpha_2\n$$\nThese two conditions together mean that the parameter vectors must be identical, i.e., $\\theta_1 = \\theta_2$.\n\nSince the assumption $c(\\theta_1) = c(\\theta_2)$ necessarily leads to the conclusion $\\theta_1 = \\theta_2$, the map $c(\\theta)$ is injective. This means that if we can determine the coefficients of the input-output differential equation from experimental data (which is the premise of structural identifiability, assuming a sufficiently rich input $u(t)$), we can uniquely determine the values of the parameters $k$ and $\\alpha$.\n\nConsequently, each component of the parameter vector $\\theta = (k, \\alpha)$ is structurally identifiable.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} k & \\alpha \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "While structural identifiability considers an idealized scenario, practical identifiability addresses the challenge of estimating parameters from finite and noisy data. The Fisher Information Matrix (FIM) is a cornerstone of this analysis, quantifying the precision with which parameters can be estimated. This practice  guides you through a first-principles derivation of the FIM for a common decay process, revealing a fundamental requirement for experimental design and providing a quantitative basis for assessing parameter uncertainty.",
            "id": "3925270",
            "problem": "In synthetic biology modeling of single-species decay processes such as messenger RNA or protein degradation, a common representation is the first-order ordinary differential equation (ODE) $\\dot{x}(t)=-k x(t)$ with decay rate $k>0$. Consider an experiment in which the state $x(t)$ is observed at sampling times $t_i$ via the measurement model $y_i=x(t_i)+\\epsilon_i$, where the noise terms $\\epsilon_i$ are independent and identically distributed (i.i.d.) Gaussian with $\\epsilon_i\\sim \\mathcal{N}(0,\\sigma^2)$. Assume the initial condition $x(0)=x_0$ is unknown and to be jointly estimated with $k$. \n\nStarting from the deterministic solution of the ODE and the likelihood definition for Gaussian noise, derive the Fisher Information Matrix (FIM) for the parameter vector $\\theta=(x_0,k)$ under the experimental design specified by the sampling times $\\{t_i\\}_{i=1}^{n}$. Your derivation must begin from first principles: the ODE solution, the measurement model, and the definition of the log-likelihood for i.i.d. Gaussian errors. Do not assume any shortcut formulas for the FIM without proving them in this context. Then, use your expression to state a necessary and sufficient condition on the set $\\{t_i\\}$ for the FIM to be nonsingular, and interpret this condition in terms of practical identifiability of $k$ when $x_0$ is estimated jointly.\n\nExpress your final Fisher Information Matrix as a closed-form analytic expression involving $x_0$, $k$, $\\sigma^2$, and the sampling times $\\{t_i\\}$. No numerical evaluation is required. Your final answer must be a single analytic expression.",
            "solution": "The problem requires the derivation of the Fisher Information Matrix (FIM) for the parameters of a first-order decay process from first principles, followed by an analysis of the conditions for its nonsingularity.\n\nThe process begins by solving the governing ordinary differential equation (ODE). The model is given by:\n$$\n\\dot{x}(t) = -k x(t)\n$$\nwith the initial condition $x(0) = x_0$. This is a separable first-order linear ODE. The solution is found by integrating $\\frac{dx}{x} = -k dt$:\n$$\n\\int \\frac{dx}{x} = \\int -k dt \\implies \\ln|x| = -kt + C\n$$\nApplying the initial condition $x(0)=x_0$, we get $\\ln|x_0| = C$. Assuming $x(t)$ represents a physical quantity like concentration or population, we take $x(t) > 0$, so $|x|=x$. The solution is:\n$$\nx(t) = x_0 \\exp(-kt)\n$$\nThis function describes the state of the system at any time $t$, parameterized by the vector $\\theta = (x_0, k)$.\n\nThe measurement model specifies that at each sampling time $t_i$ for $i=1, \\dots, n$, we observe a value $y_i$ given by:\n$$\ny_i = x(t_i; \\theta) + \\epsilon_i = x_0 \\exp(-kt_i) + \\epsilon_i\n$$\nThe noise terms $\\epsilon_i$ are independent and identically distributed (i.i.d.) according to a Gaussian distribution with mean $0$ and variance $\\sigma^2$, i.e., $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$. Consequently, the probability density function (PDF) of a single measurement $y_i$ given the parameters $\\theta$ is:\n$$\np(y_i|\\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - x(t_i; \\theta))^2}{2\\sigma^2}\\right)\n$$\nDue to the i.i.d. nature of the noise, the likelihood of observing the entire dataset $\\mathbf{y} = (y_1, \\dots, y_n)$ is the product of the individual probabilities:\n$$\nL(\\theta|\\mathbf{y}) = \\prod_{i=1}^{n} p(y_i|\\theta)\n$$\nIt is mathematically more convenient to work with the log-likelihood function, $\\mathcal{L}(\\theta) = \\ln L(\\theta|\\mathbf{y})$:\n$$\n\\mathcal{L}(\\theta) = \\sum_{i=1}^{n} \\ln p(y_i|\\theta) = \\sum_{i=1}^{n} \\left[ -\\frac{1}{2}\\ln(2\\pi\\sigma^2) - \\frac{(y_i - x_0\\exp(-kt_i))^2}{2\\sigma^2} \\right]\n$$\nThe Fisher Information Matrix (FIM), denoted $\\mathcal{I}(\\theta)$, is a $2 \\times 2$ matrix for the parameter vector $\\theta = (\\theta_1, \\theta_2) = (x_0, k)$. Its elements are defined as the negative expectation of the second partial derivatives (the Hessian matrix) of the log-likelihood function:\n$$\n\\mathcal{I}_{jl}(\\theta) = -E\\left[\\frac{\\partial^2 \\mathcal{L}(\\theta)}{\\partial \\theta_j \\partial \\theta_l}\\right]\n$$\nwhere the expectation $E[\\cdot]$ is taken over the probability distribution of the data $\\mathbf{y}$.\n\nLet us first compute the first partial derivatives of $\\mathcal{L}(\\theta)$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\theta_j} = \\sum_{i=1}^{n} \\frac{-1}{2\\sigma^2} \\cdot 2(y_i - x(t_i)) \\cdot \\left(-\\frac{\\partial x(t_i)}{\\partial \\theta_j}\\right) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i - x(t_i)) \\frac{\\partial x(t_i)}{\\partial \\theta_j}\n$$\nNow, we compute the second partial derivatives:\n$$\n\\frac{\\partial^2 \\mathcal{L}}{\\partial \\theta_j \\partial \\theta_l} = \\frac{\\partial}{\\partial \\theta_l} \\left[ \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i - x(t_i)) \\frac{\\partial x(t_i)}{\\partial \\theta_j} \\right] = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\left[ -\\frac{\\partial x(t_i)}{\\partial \\theta_l} \\frac{\\partial x(t_i)}{\\partial \\theta_j} + (y_i - x(t_i)) \\frac{\\partial^2 x(t_i)}{\\partial \\theta_j \\partial \\theta_l} \\right]\n$$\nTo find the FIM elements, we take the expectation of the negative of this expression. We use the fact that $E[y_i] = x(t_i)$, which implies $E[y_i - x(t_i)] = E[\\epsilon_i] = 0$.\n$$\n\\mathcal{I}_{jl}(\\theta) = -E\\left[\\frac{\\partial^2 \\mathcal{L}}{\\partial \\theta_j \\partial \\theta_l}\\right] = -\\frac{1}{\\sigma^2} \\sum_{i=1}^{n} E\\left[ -\\frac{\\partial x(t_i)}{\\partial \\theta_l} \\frac{\\partial x(t_i)}{\\partial \\theta_j} + (y_i - x(t_i)) \\frac{\\partial^2 x(t_i)}{\\partial \\theta_j \\partial \\theta_l} \\right]\n$$\nSince $\\frac{\\partial x(t_i)}{\\partial \\theta_j}$ is not a random variable, the expectation applies only to the term $(y_i - x(t_i))$.\n$$\n\\mathcal{I}_{jl}(\\theta) = -\\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\left[ -\\frac{\\partial x(t_i)}{\\partial \\theta_l} \\frac{\\partial x(t_i)}{\\partial \\theta_j} + E[y_i - x(t_i)] \\frac{\\partial^2 x(t_i)}{\\partial \\theta_j \\partial \\theta_l} \\right] = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\frac{\\partial x(t_i)}{\\partial \\theta_j} \\frac{\\partial x(t_i)}{\\partial \\theta_l}\n$$\nThis derivation fulfills the requirement of not assuming shortcut formulas. Now we compute the specific partial derivatives of $x(t_i; x_0, k) = x_0 \\exp(-kt_i)$, also known as the sensitivity functions.\nFor $\\theta_1 = x_0$:\n$$\n\\frac{\\partial x(t_i)}{\\partial x_0} = \\exp(-kt_i)\n$$\nFor $\\theta_2 = k$:\n$$\n\\frac{\\partial x(t_i)}{\\partial k} = x_0 \\exp(-kt_i) \\cdot (-t_i) = -x_0 t_i \\exp(-kt_i)\n$$\nUsing these sensitivities, we construct the elements of the FIM:\n$$\n\\mathcal{I}_{11} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\left(\\frac{\\partial x(t_i)}{\\partial x_0}\\right)^2 = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (\\exp(-kt_i))^2 = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\exp(-2kt_i)\n$$\n$$\n\\mathcal{I}_{22} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\left(\\frac{\\partial x(t_i)}{\\partial k}\\right)^2 = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (-x_0 t_i \\exp(-kt_i))^2 = \\frac{x_0^2}{\\sigma^2} \\sum_{i=1}^{n} t_i^2 \\exp(-2kt_i)\n$$\n$$\n\\mathcal{I}_{12} = \\mathcal{I}_{21} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\frac{\\partial x(t_i)}{\\partial x_0} \\frac{\\partial x(t_i)}{\\partial k} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (\\exp(-kt_i)) (-x_0 t_i \\exp(-kt_i)) = -\\frac{x_0}{\\sigma^2} \\sum_{i=1}^{n} t_i \\exp(-2kt_i)\n$$\nCombining these elements, the Fisher Information Matrix is:\n$$\n\\mathcal{I}(\\theta) = \\frac{1}{\\sigma^2} \\begin{pmatrix} \\sum_{i=1}^n \\exp(-2kt_i) & -x_0 \\sum_{i=1}^n t_i \\exp(-2kt_i) \\\\ -x_0 \\sum_{i=1}^n t_i \\exp(-2kt_i) & x_0^2 \\sum_{i=1}^n t_i^2 \\exp(-2kt_i) \\end{pmatrix}\n$$\nFor the FIM to be nonsingular, its determinant must be non-zero. The FIM is nonsingular if and only if the sensitivity vectors are linearly independent. The two sensitivity vectors are $\\mathbf{s}_{x_0}$ and $\\mathbf{s}_k$ with components:\n$$\n(\\mathbf{s}_{x_0})_i = \\frac{\\partial x(t_i)}{\\partial x_0} = \\exp(-kt_i)\n$$\n$$\n(\\mathbf{s}_k)_i = \\frac{\\partial x(t_i)}{\\partial k} = -x_0 t_i \\exp(-kt_i)\n$$\nThe vectors are linearly dependent if there exists a constant $c$ such that $\\mathbf{s}_k = c \\mathbf{s}_{x_0}$. This would require:\n$$\n-x_0 t_i \\exp(-kt_i) = c \\cdot \\exp(-kt_i)\n$$\nAssuming $x_0 \\neq 0$ (otherwise the system is trivially zero and no parameters can be identified) and noting that $\\exp(-kt_i) \\neq 0$, we can divide both sides by $\\exp(-kt_i)$ to get:\n$$\n-x_0 t_i = c\n$$\nThis equation must hold for all $i=1, \\dots, n$. If all sampling times $t_i$ are identical, i.e., $t_i = T$ for all $i$, then the condition is satisfied with $c = -x_0 T$, and the vectors are linearly dependent, making the FIM singular. If, however, there exists at least one pair of sampling times $(t_i, t_j)$ such that $t_i \\neq t_j$, then it is impossible to find a single constant $c$ that satisfies both $-x_0 t_i = c$ and $-x_0 t_j = c$. In this case, the vectors are linearly independent, and the FIM is nonsingular.\n\nTherefore, the necessary and sufficient condition on the set $\\{t_i\\}_{i=1}^n$ for the FIM to be nonsingular is that there must be at least two distinct sampling times (i.e., the set $\\{t_i\\}$ must contain at least two unique values). This requires $n \\ge 2$.\n\nInterpretation: A nonsingular FIM is a prerequisite for the local practical identifiability of the parameters. If the FIM is singular, the parameters cannot be uniquely determined from the given experimental data, regardless of the noise level. The condition of requiring at least two distinct measurement times has a clear practical meaning. If all measurements are taken at a single time point $T$, we effectively measure the same quantity $x(T) = x_0 \\exp(-kT)$ multiple times. This allows for a precise estimation of the product $x_0 \\exp(-kT)$, but it provides no information to disentangle the individual contributions of $x_0$ and $k$. An infinite number of pairs $(x_0, k)$ can yield the same value for $x_0 \\exp(-kT)$. By measuring the system at two or more distinct times, say $t_1$ and $t_2$, we observe how the state $x(t)$ changes over time. This dynamic information allows us to independently resolve the initial condition $x_0$ (the \"starting point\") and the decay rate $k$ (the \"speed of decay\"). For instance, the ratio of two measurements $x(t_2)/x(t_1) = \\exp(-k(t_2-t_1))$ depends only on $k$, allowing it to be estimated, after which $x_0$ can be determined.",
            "answer": "$$\n\\boxed{\\frac{1}{\\sigma^2} \\begin{pmatrix} \\sum_{i=1}^n \\exp(-2kt_i) & -x_0 \\sum_{i=1}^n t_i \\exp(-2kt_i) \\\\ -x_0 \\sum_{i=1}^n t_i \\exp(-2kt_i) & x_0^2 \\sum_{i=1}^n t_i^2 \\exp(-2kt_i) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "The true power of identifiability analysis lies in its ability to guide the design of informative experiments, especially for the nonlinear models common in synthetic biology. By numerically computing the Fisher Information Matrix under different potential inputs, we can predict which experimental protocol will best constrain our model parameters. This computational exercise  puts this principle into practice, challenging you to use sensitivity analysis to compare the identifiability afforded by a simple step input versus a dynamic sinusoidal input.",
            "id": "3925354",
            "problem": "Consider a standard inducible promoter model in synthetic biology with a single dynamic state, where the concentration of the expressed product is governed by the ordinary differential equation $$\\dot{x}(t)=\\frac{\\alpha\\,u(t)}{K+u(t)}-\\delta\\,x(t)$$ and is observed through a measurement model $$y(t)=x(t)+\\epsilon(t).$$ Assume $\\epsilon(t)$ is Additive White Gaussian Noise (AWGN) with zero mean and variance $\\sigma^2$ and that $x(0)=0$. The input $u(t)$ is externally set and known. The parameters $\\alpha$, $K$, and $\\delta$ are unknown, positive constants.\n\nStarting from the fundamental definition of the Fisher Information Matrix (FIM) for independent Gaussian measurements and the sensitivity of a dynamic system, compute the FIM for the parameter vector $$\\theta=\\begin{bmatrix}\\alpha\\\\K\\\\\\delta\\end{bmatrix}$$ under two input designs:\n- a step input $$u_{\\text{step}}(t)=U_0$$ that is constant in time, and\n- a sinusoidal input $$u_{\\text{sine}}(t)=U_0+A\\sin(\\omega t),$$ where $U_0\\ge A\\ge 0$ to ensure $u(t)\\ge 0$.\n\nUse the sensitivity-based FIM for time-discretized observations at sampling times $$t_i=i\\,\\Delta t,\\quad i=0,1,\\dots,N,$$ over a fixed experiment duration $$T=N\\,\\Delta t,$$ given the additive noise variance $\\sigma^2$. The Fisher Information Matrix for independent Gaussian noise with variance $\\sigma^2$ is $$\\mathcal{I}(\\theta)=\\sum_{i=0}^{N}\\frac{1}{\\sigma^2}\\left(\\frac{\\partial\\,\\mathbb{E}[y(t_i)]}{\\partial\\theta}\\right)\\left(\\frac{\\partial\\,\\mathbb{E}[y(t_i)]}{\\partial\\theta}\\right)^\\top,$$ and since $$\\mathbb{E}[y(t)]=x(t),$$ the needed sensitivities are $$\\frac{\\partial\\,x(t)}{\\partial\\alpha},\\quad \\frac{\\partial\\,x(t)}{\\partial K},\\quad \\frac{\\partial\\,x(t)}{\\partial\\delta}.$$ Derive these sensitivities by differentiating the state equation with respect to each parameter and numerically integrate them along with the state to compute the FIM under both inputs. Then, from the computed FIM, determine the rank of the FIM, which indicates the number of locally identifiable parameter combinations. Report the integer rank for each input design and test case.\n\nTime is measured in seconds. There are no physical units required in the final outputs because ranks are dimensionless integers.\n\nImplement a single program that, for each test case specified below, computes:\n- the FIM under the step input and its integer rank, and\n- the FIM under the sinusoidal input and its integer rank.\n\nFor rank determination, use the number of strictly positive eigenvalues of the FIM with a numerical threshold defined as $$\\tau=10^{-8}\\,\\lambda_{\\max},$$ where $\\lambda_{\\max}$ is the largest eigenvalue of the FIM; an eigenvalue is considered positive if it exceeds $\\tau$. If all eigenvalues are zero, define the rank as $0$.\n\nTest suite:\n- Case $1$ (happy path):\n  - $\\alpha=4.0$, $K=1.5$, $\\delta=0.4$\n  - Step input: $U_0=1.0$\n  - Sinusoidal input: $U_0=1.0$, $A=0.5$, $\\omega=0.4$ (radians per second)\n  - Experiment: $T=40.0$, $\\Delta t=0.1$, $\\sigma=0.05$\n- Case $2$ (edge case: slow dynamics with fast excitation):\n  - $\\alpha=4.0$, $K=0.1$, $\\delta=0.01$\n  - Step input: $U_0=0.15$\n  - Sinusoidal input: $U_0=0.15$, $A=0.12$, $\\omega=5.0$ (radians per second)\n  - Experiment: $T=60.0$, $\\Delta t=0.1$, $\\sigma=0.05$\n- Case $3$ (boundary: near saturation regime):\n  - $\\alpha=4.0$, $K=100.0$, $\\delta=1.0$\n  - Step input: $U_0=1.0$\n  - Sinusoidal input: $U_0=1.0$, $A=0.5$, $\\omega=0.2$ (radians per second)\n  - Experiment: $T=40.0$, $\\Delta t=0.1$, $\\sigma=0.05$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a pair of integers $[r_{\\text{step}},r_{\\text{sine}}]$. For example, the output format should be $$\\big[[r_{\\text{step},1},r_{\\text{sine},1}],[r_{\\text{step},2},r_{\\text{sine},2}],\\dots\\big].$$",
            "solution": "The problem is valid. It is a well-posed and scientifically grounded exercise in parameter identifiability analysis, a standard topic in systems biology and engineering. All necessary data and definitions are provided, and there are no internal contradictions or factual errors.\n\nThe solution proceeds by first deriving the sensitivity equations, which govern how the system's state changes with respect to its parameters. These equations, combined with the original state equation, form an augmented system of ordinary differential equations (ODEs). This system is then numerically integrated to obtain the time-course of both the state and its sensitivities. Finally, the Fisher Information Matrix (FIM) is computed from these sensitivities, and its rank is determined to assess parameter identifiability.\n\nThe model is described by the state equation for the product concentration $x(t)$:\n$$\n\\dot{x}(t)=\\frac{\\alpha\\,u(t)}{K+u(t)}-\\delta\\,x(t), \\quad x(0)=0\n$$\nThe parameter vector is $\\theta=\\begin{bmatrix}\\alpha & K & \\delta\\end{bmatrix}^\\top$.\n\nThe Fisher Information Matrix for $N+1$ discrete-time measurements with independent additive Gaussian noise of variance $\\sigma^2$ is given by:\n$$\n\\mathcal{I}(\\theta)=\\frac{1}{\\sigma^2}\\sum_{i=0}^{N} \\left(\\nabla_\\theta x(t_i)\\right) \\left(\\nabla_\\theta x(t_i)\\right)^\\top\n$$\nwhere $\\nabla_\\theta x(t_i)$ is the sensitivity vector, whose components are the partial derivatives of the state with respect to each parameter, evaluated at time $t_i$:\n$$\n\\nabla_\\theta x(t) = \\begin{bmatrix}\n\\frac{\\partial x(t)}{\\partial \\alpha} \\\\\n\\frac{\\partial x(t)}{\\partial K} \\\\\n\\frac{\\partial x(t)}{\\partial \\delta}\n\\end{bmatrix} = \\begin{bmatrix}\nS_\\alpha(t) \\\\\nS_K(t) \\\\\nS_\\delta(t)\n\\end{bmatrix}\n$$\n\nTo find these sensitivities, we differentiate the state equation with respect to each parameter. Let $f(x, u, \\theta) = \\frac{\\alpha u}{K+u} - \\delta x$. The general form of a sensitivity equation is obtained by applying the chain rule, noting that the order of differentiation with respect to time and parameters can be exchanged:\n$$\n\\frac{d}{dt}\\left(\\frac{\\partial x}{\\partial \\theta_j}\\right) = \\frac{\\partial}{\\partial \\theta_j}\\left(\\frac{dx}{dt}\\right) = \\frac{\\partial f}{\\partial x}\\frac{\\partial x}{\\partial \\theta_j} + \\frac{\\partial f}{\\partial \\theta_j}\n$$\nIn our model, $\\frac{\\partial f}{\\partial x} = -\\delta$. Thus, the sensitivity $S_j(t) = \\frac{\\partial x(t)}{\\partial \\theta_j}$ evolves according to:\n$$\n\\dot{S}_j(t) = -\\delta S_j(t) + \\frac{\\partial f}{\\partial \\theta_j}\n$$\nThe initial condition for each sensitivity is $S_j(0) = \\frac{\\partial x(0)}{\\partial \\theta_j} = \\frac{\\partial 0}{\\partial \\theta_j} = 0$.\n\nWe now derive the specific term $\\frac{\\partial f}{\\partial \\theta_j}$ for each parameter:\n1.  **Sensitivity to $\\alpha$**: $S_\\alpha(t) = \\frac{\\partial x}{\\partial \\alpha}$\n    $$\n    \\frac{\\partial f}{\\partial \\alpha} = \\frac{u(t)}{K+u(t)}\n    $$\n    The corresponding ODE is:\n    $$\n    \\dot{S}_\\alpha(t) = -\\delta S_\\alpha(t) + \\frac{u(t)}{K+u(t)}, \\quad S_\\alpha(0)=0\n    $$\n\n2.  **Sensitivity to $K$**: $S_K(t) = \\frac{\\partial x}{\\partial K}$\n    $$\n    \\frac{\\partial f}{\\partial K} = \\alpha u(t) \\frac{\\partial}{\\partial K}(K+u(t))^{-1} = -\\frac{\\alpha u(t)}{(K+u(t))^2}\n    $$\n    The corresponding ODE is:\n    $$\n    \\dot{S}_K(t) = -\\delta S_K(t) - \\frac{\\alpha u(t)}{(K+u(t))^2}, \\quad S_K(0)=0\n    $$\n    \n3.  **Sensitivity to $\\delta$**: $S_\\delta(t) = \\frac{\\partial x}{\\partial \\delta}$\n    This case requires care as $\\delta$ appears in both terms of $f$.\n    $$\n    \\frac{\\partial f}{\\partial \\delta} = -x(t)\n    $$\n    However, the full sensitivity equation also has $\\delta$ in the $\\frac{\\partial f}{\\partial x}$ term.\n    $$\n    \\frac{d S_\\delta}{dt} = \\frac{\\partial}{\\partial \\delta}(\\dot{x}) = \\frac{\\partial}{\\partial \\delta}\\left(\\frac{\\alpha u}{K+u} - \\delta x\\right) = -\\left(x + \\delta \\frac{\\partial x}{\\partial \\delta}\\right) = -x - \\delta S_\\delta\n    $$\n    The corresponding ODE is:\n    $$\n    \\dot{S}_\\delta(t) = -\\delta S_\\delta(t) - x(t), \\quad S_\\delta(0)=0\n    $$\n\nTo compute the FIM, we form an augmented system of four ODEs for the state vector $Z(t) = \\begin{bmatrix} x(t), S_\\alpha(t), S_K(t), S_\\delta(t) \\end{bmatrix}^\\top$:\n$$\n\\frac{d}{dt}\n\\begin{bmatrix} x \\\\ S_\\alpha \\\\ S_K \\\\ S_\\delta \\end{bmatrix}\n=\n\\begin{bmatrix}\n\\frac{\\alpha u(t)}{K+u(t)} - \\delta x \\\\\n\\frac{u(t)}{K+u(t)} - \\delta S_\\alpha \\\\\n-\\frac{\\alpha u(t)}{(K+u(t))^2} - \\delta S_K \\\\\n-x - \\delta S_\\delta\n\\end{bmatrix}\n\\quad \\text{with initial condition} \\quad\nZ(0) = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\n$$\n\nThis system is numerically integrated from $t=0$ to $t=T$ for each specified input $u(t)$ (step and sinusoidal). The solution is evaluated at discrete time points $t_i = i\\,\\Delta t$ for $i=0, 1, \\dots, N$. The resulting sensitivity vectors $\\nabla_\\theta x(t_i)$ are used to construct the $3 \\times 3$ FIM.\n\nThe rank of the FIM indicates the number of identifiable parameters or parameter combinations. A full-rank FIM (rank $3$) suggests that all three parameters are locally identifiable. A rank-deficient FIM (rank $<3$) indicates structural or practical non-identifiability, meaning some parameters or their combinations cannot be uniquely determined from the given experiment. The rank is computed numerically by counting the number of eigenvalues $\\lambda_j$ that are strictly greater than a threshold $\\tau = 10^{-8} \\lambda_{\\max}$, where $\\lambda_{\\max}$ is the largest eigenvalue of the FIM. For a zero matrix, the rank is $0$.\n\nThe following Python code implements this procedure for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef dZ_dt(t, Z, alpha, K, delta, u_func):\n    \"\"\"\n    Defines the augmented system of ODEs for the state and sensitivities.\n    Z = [x, S_alpha, S_K, S_delta]\n    \"\"\"\n    x, s_alpha, s_K, s_delta = Z\n    u_val = u_func(t)\n\n    # Pre-compute common terms to avoid re-calculation\n    production_term_denom = K + u_val\n    if production_term_denom == 0: # Avoid division by zero, although not expected with K>0, u>=0\n        production_term_denom = 1e-12\n\n    # State equation\n    dx_dt = (alpha * u_val / production_term_denom) - (delta * x)\n\n    # Forcing terms for sensitivity equations\n    df_dalpha = u_val / production_term_denom\n    df_dK = -alpha * u_val / (production_term_denom**2)\n    df_ddelta = -x\n\n    # Sensitivity equations\n    ds_alpha_dt = -delta * s_alpha + df_dalpha\n    ds_K_dt = -delta * s_K + df_dK\n    ds_delta_dt = -delta * s_delta + df_ddelta\n\n    return [dx_dt, ds_alpha_dt, ds_K_dt, ds_delta_dt]\n\ndef compute_fim_rank(alpha, K, delta, u_func, T, dt, sigma):\n    \"\"\"\n    Computes the Fisher Information Matrix and its rank for a given experiment.\n    \"\"\"\n    # Define time points for integration and evaluation\n    N = int(round(T / dt))\n    t_eval = np.linspace(0.0, T, N + 1)\n\n    # Initial conditions for the augmented system [x, S_alpha, S_K, S_delta]\n    z0 = [0.0, 0.0, 0.0, 0.0]\n\n    # Numerically integrate the system of ODEs\n    sol = solve_ivp(\n        dZ_dt,\n        [0, T],\n        z0,\n        args=(alpha, K, delta, u_func),\n        t_eval=t_eval,\n        method='RK45',\n        rtol=1e-6,\n        atol=1e-9\n    )\n\n    if not sol.success:\n        # If integration fails, we cannot compute the FIM.\n        # This implies extreme stiffness or other issues; rank is effectively 0.\n        return 0\n\n    # Extract sensitivity matrix S, shape (3, N+1)\n    sensitivities = sol.y[1:, :]\n\n    # Compute the Fisher Information Matrix (FIM)\n    # FIM = (1/sigma^2) * sum(S_i * S_i^T) = (1/sigma^2) * (S @ S.T)\n    fim = (1.0 / sigma**2) * (sensitivities @ sensitivities.T)\n\n    # Compute eigenvalues of the symmetric FIM\n    try:\n        # eigvalsh is preferred for symmetric matrices; returns sorted eigenvalues\n        eigvals = np.linalg.eigvalsh(fim)\n    except np.linalg.LinAlgError:\n        # This can happen if the FIM is numerically pathological\n        return 0\n\n    # Determine the rank based on the specified threshold\n    lambda_max = eigvals[-1] # Largest eigenvalue\n\n    if lambda_max <= 0.0:  # Handles the all-zero matrix case and numerical noise\n        return 0\n\n    threshold = 1e-8 * lambda_max\n    \n    # Count eigenvalues strictly greater than the threshold\n    rank = np.sum(eigvals > threshold)\n\n    return int(rank)\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path)\n        {\n            'params': {'alpha': 4.0, 'K': 1.5, 'delta': 0.4},\n            'step_input': {'U0': 1.0},\n            'sine_input': {'U0': 1.0, 'A': 0.5, 'omega': 0.4},\n            'exp': {'T': 40.0, 'dt': 0.1, 'sigma': 0.05}\n        },\n        # Case 2 (edge case: slow dynamics with fast excitation)\n        {\n            'params': {'alpha': 4.0, 'K': 0.1, 'delta': 0.01},\n            'step_input': {'U0': 0.15},\n            'sine_input': {'U0': 0.15, 'A': 0.12, 'omega': 5.0},\n            'exp': {'T': 60.0, 'dt': 0.1, 'sigma': 0.05}\n        },\n        # Case 3 (boundary: near saturation regime)\n        {\n            'params': {'alpha': 4.0, 'K': 100.0, 'delta': 1.0},\n            'step_input': {'U0': 1.0},\n            'sine_input': {'U0': 1.0, 'A': 0.5, 'omega': 0.2},\n            'exp': {'T': 40.0, 'dt': 0.1, 'sigma': 0.05}\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        # Unpack parameters for clarity\n        params = case['params']\n        exp_params = case['exp']\n        alpha, K, delta = params['alpha'], params['K'], params['delta']\n        T, dt, sigma = exp_params['T'], exp_params['dt'], exp_params['sigma']\n        \n        case_ranks = []\n\n        # --- Process Step Input ---\n        step_params = case['step_input']\n        U0_step = step_params['U0']\n        u_step = lambda t: U0_step\n        rank_step = compute_fim_rank(alpha, K, delta, u_step, T, dt, sigma)\n        case_ranks.append(rank_step)\n\n        # --- Process Sinusoidal Input ---\n        sine_params = case['sine_input']\n        U0_sine, A, omega = sine_params['U0'], sine_params['A'], sine_params['omega']\n        u_sine = lambda t: U0_sine + A * np.sin(omega * t)\n        rank_sine = compute_fim_rank(alpha, K, delta, u_sine, T, dt, sigma)\n        case_ranks.append(rank_sine)\n        \n        all_results.append(case_ranks)\n\n    # Format the final output string exactly as specified\n    output_str = '[' + ','.join([f'[{r[0]},{r[1]}]' for r in all_results]) + ']'\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}