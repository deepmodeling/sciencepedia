{
    "hands_on_practices": [
        {
            "introduction": "The Gauss-Newton method is a cornerstone algorithm for solving non-linear least squares problems. While modern software packages automate its execution, a deep understanding of the fitting process comes from dissecting the algorithm itself. This exercise provides a first-principles walkthrough of a single Gauss-Newton iteration, grounding the abstract theory in a concrete calculation for the classic Michaelis-Menten kinetic model . By manually computing the Jacobian and the parameter update step, you will gain a tangible feel for how the algorithm linearizes the problem to iteratively approach a solution.",
            "id": "3923519",
            "problem": "A synthetic enzyme module in a gene circuit converts substrate into product following Michaelis–Menten (MM) kinetics. The measurable initial rate at substrate concentration $S$ is modeled by $v(S; V_{\\max}, K_{M}) = \\dfrac{V_{\\max} S}{K_{M} + S}$. You are given substrate concentrations and corresponding measured initial rates from a calibrated microfluidic assay suitable for least squares (LS) parameter estimation:\n- Substrate concentrations (use micromolar units): $S \\in \\{5, 10, 20, 50\\}$.\n- Measured initial rates (use micromolar per minute units): $v^{\\mathrm{obs}} \\in \\left\\{\\dfrac{5}{8}, 1, \\dfrac{10}{7}, \\dfrac{25}{13}\\right\\}$.\n\nStarting from the initial parameter guess $V_{\\max}^{(0)} = 2.0$ and $K_{M}^{(0)} = 10$, apply a single Gauss–Newton (GN) iteration to the non-linear LS problem that minimizes the sum of squared residuals $\\sum_{i} \\left(v_{i}^{\\mathrm{obs}} - v(S_{i}; V_{\\max}, K_{M})\\right)^{2}$. Compute the GN update explicitly from first principles, and report the updated parameter vector $\\left(V_{\\max}^{(1)}, K_{M}^{(1)}\\right)$ after this single iteration.\n\nRound your final numerical values to four significant figures. Express $V_{\\max}$ in micromolar per minute and $K_{M}$ in micromolar. Define all symbols you introduce, and justify each step from first principles appropriate for non-linear least squares methods in synthetic biology modeling. Avoid using any pre-derived shortcut formulas beyond those justified from Taylor linearization and normal equations for least squares. For clarity, define any acronyms you use on first appearance, such as Michaelis–Menten (MM), least squares (LS), and Gauss–Newton (GN).",
            "solution": "The objective is to refine an initial estimate of the Michaelis–Menten (MM) kinetic parameters, $V_{\\max}$ and $K_{M}$, by applying a single iteration of the Gauss–Newton (GN) algorithm. This method is a standard approach for solving non-linear least squares (LS) problems, which commonly arise in the fitting of mechanistic models to experimental data in synthetic biology.\n\nLet the parameter vector be denoted by $\\mathbf{p} = \\begin{pmatrix} V_{\\max} \\\\ K_{M} \\end{pmatrix}$. The model for the initial rate $v$ at a given substrate concentration $S$ is given by the MM equation:\n$$v(S; \\mathbf{p}) = \\frac{V_{\\max} S}{K_{M} + S}$$\nWe are provided with a set of $n=4$ experimental data points $(S_i, v_{i}^{\\mathrm{obs}})$. The goal of the LS problem is to find the parameter vector $\\mathbf{p}$ that minimizes the sum of squared residuals, $S(\\mathbf{p})$:\n$$S(\\mathbf{p}) = \\sum_{i=1}^{n} r_i(\\mathbf{p})^2 = \\sum_{i=1}^{n} \\left(v_{i}^{\\mathrm{obs}} - v(S_i; \\mathbf{p})\\right)^2$$\nwhere $r_i(\\mathbf{p})$ is the residual for the $i$-th data point.\n\nThe GN algorithm is an iterative method where, at each step $k$, we seek an update $\\Delta\\mathbf{p}^{(k)}$ that improves the current parameter estimate $\\mathbf{p}^{(k)}$. The new estimate is then $\\mathbf{p}^{(k+1)} = \\mathbf{p}^{(k)} + \\Delta\\mathbf{p}^{(k)}$.\nThe algorithm is derived by linearizing the model function $v(S_i; \\mathbf{p})$ around the current estimate $\\mathbf{p}^{(k)}$ using a first-order Taylor expansion:\n$$v(S_i; \\mathbf{p}) \\approx v(S_i; \\mathbf{p}^{(k)}) + \\nabla_{\\mathbf{p}} v(S_i; \\mathbf{p}^{(k)})^T (\\mathbf{p} - \\mathbf{p}^{(k)})$$\nHere, $\\nabla_{\\mathbf{p}} v(S_i; \\mathbf{p}^{(k)})$ is the gradient of the model function evaluated at $\\mathbf{p}^{(k)}$. Let $\\mathbf{J}$ be the Jacobian matrix, whose elements are $J_{ij} = \\frac{\\partial v(S_i; \\mathbf{p})}{\\partial p_j}$, evaluated at $\\mathbf{p}^{(k)}$. The $i$-th row of $\\mathbf{J}$ is $\\nabla_{\\mathbf{p}} v(S_i; \\mathbf{p}^{(k)})^T$.\nSubstituting the linearization into the residual definition gives:\n$$r_i(\\mathbf{p}) \\approx \\left(v_{i}^{\\mathrm{obs}} - v(S_i; \\mathbf{p}^{(k)})\\right) - \\sum_{j=1}^{m} J_{ij} (\\mathbf{p}_j - \\mathbf{p}_j^{(k)})$$\nwhere $m=2$ is the number of parameters. In vector form, with $\\mathbf{r}(\\mathbf{p})$ as the vector of residuals and $\\Delta\\mathbf{p} = \\mathbf{p} - \\mathbf{p}^{(k)}$, this approximation is:\n$$\\mathbf{r}(\\mathbf{p}) \\approx \\mathbf{r}(\\mathbf{p}^{(k)}) - \\mathbf{J}(\\mathbf{p}^{(k)}) \\Delta\\mathbf{p}$$\nThe LS problem then becomes finding the $\\Delta\\mathbf{p}$ that minimizes $\\| \\mathbf{r}(\\mathbf{p}^{(k)}) - \\mathbf{J}(\\mathbf{p}^{(k)}) \\Delta\\mathbf{p} \\|_2^2$. This is a linear least squares problem, the solution to which is given by the normal equations:\n$$\\left(\\mathbf{J}^T \\mathbf{J}\\right) \\Delta\\mathbf{p} = \\mathbf{J}^T \\mathbf{r}(\\mathbf{p}^{(k)})$$\nThe update for the $k$-th iteration is thus $\\Delta\\mathbf{p}^{(k)} = \\left((\\mathbf{J}^{(k)})^T \\mathbf{J}^{(k)}\\right)^{-1} (\\mathbf{J}^{(k)})^T \\mathbf{r}(\\mathbf{p}^{(k)})$.\n\nWe will now perform a single iteration, starting with $k=0$.\n\n**Step 1: Define initial state and data**\n- Initial parameter guess: $\\mathbf{p}^{(0)} = \\begin{pmatrix} V_{\\max}^{(0)} \\\\ K_{M}^{(0)} \\end{pmatrix} = \\begin{pmatrix} 2.0 \\\\ 10 \\end{pmatrix}$. $V_{\\max}$ is in units of $\\mu\\text{M} \\cdot \\text{min}^{-1}$ and $K_{M}$ is in $\\mu\\text{M}$.\n- Data points $(S_i, v_{i}^{\\mathrm{obs}})$:\n$(5, 5/8)$, $(10, 1)$, $(20, 10/7)$, $(50, 25/13)$.\n\n**Step 2: Calculate the Jacobian matrix $\\mathbf{J}$ at $\\mathbf{p}^{(0)}$**\nThe partial derivatives of $v(S; \\mathbf{p})$ with respect to the parameters are:\n$$\\frac{\\partial v}{\\partial V_{\\max}} = \\frac{S}{K_{M} + S}$$\n$$\\frac{\\partial v}{\\partial K_{M}} = V_{\\max} S \\left( -(K_{M} + S)^{-2} \\right) = -\\frac{V_{\\max} S}{(K_{M} + S)^2}$$\nEvaluating these at $\\mathbf{p}^{(0)} = (2, 10)$ for each $S_i$:\nFor $S_1 = 5$: $\\frac{\\partial v}{\\partial V_{\\max}} = \\frac{5}{15} = \\frac{1}{3}$, $\\frac{\\partial v}{\\partial K_{M}} = -\\frac{2 \\cdot 5}{15^2} = -\\frac{10}{225} = -\\frac{2}{45}$.\nFor $S_2 = 10$: $\\frac{\\partial v}{\\partial V_{\\max}} = \\frac{10}{20} = \\frac{1}{2}$, $\\frac{\\partial v}{\\partial K_{M}} = -\\frac{2 \\cdot 10}{20^2} = -\\frac{20}{400} = -\\frac{1}{20}$.\nFor $S_3 = 20$: $\\frac{\\partial v}{\\partial V_{\\max}} = \\frac{20}{30} = \\frac{2}{3}$, $\\frac{\\partial v}{\\partial K_{M}} = -\\frac{2 \\cdot 20}{30^2} = -\\frac{40}{900} = -\\frac{2}{45}$.\nFor $S_4 = 50$: $\\frac{\\partial v}{\\partial V_{\\max}} = \\frac{50}{60} = \\frac{5}{6}$, $\\frac{\\partial v}{\\partial K_{M}} = -\\frac{2 \\cdot 50}{60^2} = -\\frac{100}{3600} = -\\frac{1}{36}$.\nThe Jacobian matrix at $\\mathbf{p}^{(0)}$ is:\n$$\\mathbf{J}^{(0)} = \\begin{pmatrix} 1/3 & -2/45 \\\\ 1/2 & -1/20 \\\\ 2/3 & -2/45 \\\\ 5/6 & -1/36 \\end{pmatrix}$$\n\n**Step 3: Calculate the residual vector $\\mathbf{r}(\\mathbf{p}^{(0)})$**\nFirst, calculate the model predictions $v(S_i; \\mathbf{p}^{(0)}) = \\frac{2 S_i}{10+S_i}$:\n$v(5; \\mathbf{p}^{(0)}) = \\frac{2 \\cdot 5}{10+5} = \\frac{10}{15} = \\frac{2}{3}$.\n$v(10; \\mathbf{p}^{(0)}) = \\frac{2 \\cdot 10}{10+10} = \\frac{20}{20} = 1$.\n$v(20; \\mathbf{p}^{(0)}) = \\frac{2 \\cdot 20}{10+20} = \\frac{40}{30} = \\frac{4}{3}$.\n$v(50; \\mathbf{p}^{(0)}) = \\frac{2 \\cdot 50}{10+50} = \\frac{100}{60} = \\frac{5}{3}$.\nThe residuals $r_i^{(0)} = v_{i}^{\\mathrm{obs}} - v(S_i; \\mathbf{p}^{(0)})$ are:\n$r_1 = \\frac{5}{8} - \\frac{2}{3} = \\frac{15-16}{24} = -\\frac{1}{24}$.\n$r_2 = 1 - 1 = 0$.\n$r_3 = \\frac{10}{7} - \\frac{4}{3} = \\frac{30-28}{21} = \\frac{2}{21}$.\n$r_4 = \\frac{25}{13} - \\frac{5}{3} = \\frac{75-65}{39} = \\frac{10}{39}$.\nThe residual vector is:\n$$\\mathbf{r}^{(0)} = \\begin{pmatrix} -1/24 \\\\ 0 \\\\ 2/21 \\\\ 10/39 \\end{pmatrix} \\approx \\begin{pmatrix} -0.041667 \\\\ 0 \\\\ 0.095238 \\\\ 0.256410 \\end{pmatrix}$$\n\n**Step 4: Solve the normal equations**\nWe must solve $(\\mathbf{J}^T \\mathbf{J}) \\Delta\\mathbf{p} = \\mathbf{J}^T \\mathbf{r}$.\nFirst, compute $(\\mathbf{J}^{(0)})^T \\mathbf{J}^{(0)}$:\n$$(\\mathbf{J}^{(0)})^T \\mathbf{J}^{(0)} = \\begin{pmatrix} 1/3 & 1/2 & 2/3 & 5/6 \\\\ -2/45 & -1/20 & -2/45 & -1/36 \\end{pmatrix} \\begin{pmatrix} 1/3 & -2/45 \\\\ 1/2 & -1/20 \\\\ 2/3 & -2/45 \\\\ 5/6 & -1/36 \\end{pmatrix} $$\n$$ = \\begin{pmatrix} \\frac{1}{9}+\\frac{1}{4}+\\frac{4}{9}+\\frac{25}{36} & -\\frac{2}{135}-\\frac{1}{40}-\\frac{4}{135}-\\frac{5}{216} \\\\ -\\frac{2}{135}-\\frac{1}{40}-\\frac{4}{135}-\\frac{5}{216} & \\frac{4}{2025}+\\frac{1}{400}+\\frac{4}{2025}+\\frac{1}{1296} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} & -\\frac{5}{54} \\\\ -\\frac{5}{54} & \\frac{13}{1800} \\end{pmatrix}$$\nNumerically, $(\\mathbf{J}^{(0)})^T \\mathbf{J}^{(0)} \\approx \\begin{pmatrix} 1.5 & -0.092593 \\\\ -0.092593 & 0.007222 \\end{pmatrix}$.\n\nNext, compute $(\\mathbf{J}^{(0)})^T \\mathbf{r}^{(0)}$:\n$$(\\mathbf{J}^{(0)})^T \\mathbf{r}^{(0)} = \\begin{pmatrix} \\frac{1}{3}(-\\frac{1}{24}) + \\frac{1}{2}(0) + \\frac{2}{3}(\\frac{2}{21}) + \\frac{5}{6}(\\frac{10}{39}) \\\\ -\\frac{2}{45}(-\\frac{1}{24}) - \\frac{1}{20}(0) - \\frac{2}{45}(\\frac{2}{21}) - \\frac{1}{36}(\\frac{10}{39}) \\end{pmatrix} = \\begin{pmatrix} \\frac{575}{2184} \\\\ -\\frac{467}{49140} \\end{pmatrix}$$\nNumerically, $(\\mathbf{J}^{(0)})^T \\mathbf{r}^{(0)} \\approx \\begin{pmatrix} 0.263278 \\\\ -0.009503 \\end{pmatrix}$.\n\nWe solve the linear system for $\\Delta\\mathbf{p}^{(0)} = \\begin{pmatrix} \\Delta V_{\\max} \\\\ \\Delta K_M \\end{pmatrix}$:\n$$\\begin{pmatrix} 1.5 & -0.092593 \\\\ -0.092593 & 0.007222 \\end{pmatrix} \\begin{pmatrix} \\Delta V_{\\max} \\\\ \\Delta K_M \\end{pmatrix} = \\begin{pmatrix} 0.263278 \\\\ -0.009503 \\end{pmatrix}$$\nSolving this system yields:\n$$\\Delta V_{\\max} \\approx 0.451966$$\n$$\\Delta K_M \\approx 4.476483$$\n\n**Step 5: Update the parameter vector**\nThe new parameter estimate $\\mathbf{p}^{(1)}$ is:\n$$\\mathbf{p}^{(1)} = \\mathbf{p}^{(0)} + \\Delta\\mathbf{p}^{(0)} = \\begin{pmatrix} 2.0 \\\\ 10 \\end{pmatrix} + \\begin{pmatrix} 0.451966 \\\\ 4.476483 \\end{pmatrix} = \\begin{pmatrix} 2.451966 \\\\ 14.476483 \\end{pmatrix}$$\n\n**Step 6: Final Answer**\nRounding the components of $\\mathbf{p}^{(1)}$ to four significant figures gives:\n$$V_{\\max}^{(1)} \\approx 2.452$$\n$$K_{M}^{(1)} \\approx 14.48$$\nThe updated parameter vector is $(V_{\\max}^{(1)}, K_{M}^{(1)}) = (2.452, 14.48)$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 2.452 & 14.48 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Finding the 'best-fit' parameters is only half the battle; we must also determine how certain we can be about their values. This is the question of parameter identifiability. The profile likelihood method is a powerful, computationally-driven technique for exploring the landscape of the cost function to diagnose non-identifiability and construct more reliable confidence intervals than simple linear approximations allow . This practice will guide you through implementing this technique, demonstrating its utility in revealing how different experimental designs can dramatically impact our ability to constrain model parameters.",
            "id": "3923524",
            "problem": "You are given a nonlinear regression problem arising from a steady-state Hill-type transcriptional repression model frequently used in synthetic biology modeling. The measured observable is the steady-state reporter intensity as a function of inducer concentration. The model is defined by the mapping from inducer concentration $u$ (with unit micromolar, denoted $\\mu\\mathrm{M}$) to observed intensity $y$ (in arbitrary fluorescence units) as\n$$\ny(u;p) \\;=\\; y_0 \\;+\\; \\frac{V}{1 + \\left(\\frac{u}{K}\\right)^{n}},\n$$\nwhere the parameter vector is $p = (y_0, V, K, n)$ with $y_0 \\ge 0$ (baseline), $V \\ge 0$ (dynamic range), $K > 0$ (half-repression concentration, in $\\mu\\mathrm{M}$), and $n > 0$ (Hill coefficient). Assume independent Gaussian measurement noise with known standard deviation $\\sigma$ for each data point. For given data $\\{(u_i, y_i)\\}_{i=1}^m$, the weighted nonlinear least-squares objective is\n$$\nS(p) \\;=\\; \\sum_{i=1}^{m} \\left(\\frac{y_i - y(u_i;p)}{\\sigma}\\right)^2.\n$$\n\nYou will implement a profile likelihood analysis with respect to the Hill coefficient $n$ using constrained nonlinear least squares. For a fixed value of $n$ (treated as a scalar), define the profiled objective\n$$\nS_{\\mathrm{prof}}(n) \\;=\\; \\min_{(y_0,V,K)} \\; S(y_0,V,K,n) \\quad \\text{subject to} \\quad y_0 \\ge 0,\\; V \\ge 0,\\; K \\in [10^{-3},10^{3}],\n$$\nand trace $S_{\\mathrm{prof}}(n)$ as $n$ varies over a grid. This procedure corresponds to the profile likelihood for $n$ under the Gaussian noise assumption and enables an identifiability assessment for $n$ based on the likelihood ratio principle.\n\nFundamental base and assumptions to use:\n- The Central Dogma of molecular biology motivates transcriptional regulation models, but for this problem only the mathematical form of the repression Hill function is needed as given above.\n- For independent Gaussian noise with known variance, minimizing the weighted sum of squares $S(p)$ is equivalent to maximizing the log-likelihood up to an additive constant.\n- The likelihood ratio test for a single scalar parameter at level $0.95$ uses the chi-square distribution with one degree of freedom, so the $0.95$ quantile is $\\chi^2_{1,0.95} \\approx 3.841458820694124$. The $95\\%$ confidence set for $n$ derived from the profile is $\\{ n: S_{\\mathrm{prof}}(n) - \\min_n S_{\\mathrm{prof}}(n) \\le \\chi^2_{1,0.95} \\}$.\n\nYour program must, for each specified test case below, carry out the following steps on a grid of $n$ values and produce specified scalar outcomes:\n1. Define a uniform grid $n \\in [0.5, 6.0]$ with spacing $\\Delta n = 0.05$.\n2. For each grid point $n$, compute $S_{\\mathrm{prof}}(n)$ by minimizing $S(y_0,V,K,n)$ over $(y_0,V,K)$ with bound constraints $y_0 \\in [0, 2000]$, $V \\in [0, 5000]$, and $K \\in [10^{-3}, 10^{3}]$. Use the same $\\sigma$ as specified per test case.\n3. Let $n^\\ast$ be the grid point attaining the global minimum of $S_{\\mathrm{prof}}(n)$.\n\nYou must implement the following three test cases with the given inducer concentrations and true parameters, generating noise-free observations $y_i$ exactly from the model:\n- Test Case A (broad dynamic range, informative):\n  - Concentrations (in $\\mu\\mathrm{M}$): $u = [0.1, 0.3, 1, 3, 10, 30, 100]$.\n  - True parameters: $y_0^{\\mathrm{true}} = 100$, $V^{\\mathrm{true}} = 900$, $K^{\\mathrm{true}} = 10$ (in $\\mu\\mathrm{M}$), $n^{\\mathrm{true}} = 2.5$.\n  - Noise standard deviation: $\\sigma = 30$ (fluorescence units).\n  - Required scalar result: the absolute error $|n^\\ast - n^{\\mathrm{true}}|$ as a floating-point number.\n\n- Test Case B (low-concentration regime, potentially unidentifiable for $n$):\n  - Concentrations (in $\\mu\\mathrm{M}$): $u = [0.1, 0.2, 0.3, 0.5, 0.8, 1.0, 1.5]$.\n  - True parameters: $y_0^{\\mathrm{true}} = 100$, $V^{\\mathrm{true}} = 900$, $K^{\\mathrm{true}} = 10$ (in $\\mu\\mathrm{M}$), $n^{\\mathrm{true}} = 2.5$.\n  - Noise standard deviation: $\\sigma = 30$ (fluorescence units).\n  - Identifiability assessment: Using the profile $S_{\\mathrm{prof}}(n)$ and the threshold $\\Delta S = \\chi^2_{1,0.95}$, define the $95\\%$ confidence set for $n$ as $\\{n : S_{\\mathrm{prof}}(n) - \\min S_{\\mathrm{prof}} \\le \\chi^2_{1,0.95} \\}$. Return the boolean value $\\mathrm{identifiable}$ which is $\\mathrm{True}$ if and only if this confidence set is a proper bounded interval strictly inside the grid $[0.5, 6.0]$, meaning there exist both a lower and an upper finite boundary within the grid, and $\\mathrm{False}$ otherwise.\n\n- Test Case C (high-concentration regime, boundary behavior check):\n  - Concentrations (in $\\mu\\mathrm{M}$): $u = [30, 50, 80, 120, 200, 300]$.\n  - True parameters: $y_0^{\\mathrm{true}} = 100$, $V^{\\mathrm{true}} = 900$, $K^{\\mathrm{true}} = 10$ (in $\\mu\\mathrm{M}$), $n^{\\mathrm{true}} = 2.5$.\n  - Noise standard deviation: $\\sigma = 30$ (fluorescence units).\n  - Required scalar result: the integer indicator $b$ defined as $b=1$ if $n^\\ast$ equals exactly the left boundary $0.5$ or the right boundary $6.0$ of the $n$ grid, and $b=0$ otherwise.\n\nImplementation details to adhere to:\n- For the inner minimizations at fixed $n$, solve the constrained nonlinear least-squares problem for $(y_0,V,K)$ using an algorithm appropriate for bound constraints.\n- Use the weighted residual form $(y_i - y(u_i;p))/\\sigma$ so that $S(p)$ equals the sum of squares of these residuals.\n- Use initial guesses derived from the data: $y_0^{(0)} = \\min_i y_i$, $V^{(0)} = \\max_i y_i - \\min_i y_i$, and $K^{(0)} = \\mathrm{median}(u_i)$.\n- Angle units do not apply. Physical units have been specified where appropriate. All outputs are unitless as specified below.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets, in the order $[\\text{result for A}, \\text{result for B}, \\text{result for C}]$.\n- The first element is a floating-point number $|n^\\ast - n^{\\mathrm{true}}|$. The second element is a boolean $\\mathrm{True}$ or $\\mathrm{False}$. The third element is an integer $0$ or $1$.\n- For example, a syntactically valid output would look like `[0.0123,True,0]`.",
            "solution": "The problem requires performing a profile likelihood analysis for the Hill coefficient, $n$, of a transcriptional repression model. This is a standard technique in systems biology for assessing parameter identifiability. The solution involves a numerical, grid-based approach.\n\nThe steady-state model for the observed fluorescence intensity, $y$, as a function of inducer concentration, $u$, is given by the Hill-type repression function:\n$$\ny(u;p) \\;=\\; y_0 \\;+\\; \\frac{V}{1 + \\left(\\frac{u}{K}\\right)^{n}}\n$$\nwhere the parameter vector is $p = (y_0, V, K, n)$. The parameters represent the baseline fluorescence ($y_0$), the dynamic range of repression ($V$), the half-repression concentration ($K$), and the Hill coefficient ($n$), which quantifies the steepness of the response.\n\nWe are tasked with fitting this model to data. Assuming independent and identically distributed Gaussian noise on the measurements with a known standard deviation $\\sigma$, the maximum likelihood estimate of the parameters is found by minimizing the weighted sum of squared residuals (SSR), which defines the objective function $S(p)$:\n$$\nS(p) \\;=\\; \\sum_{i=1}^{m} \\left(\\frac{y_i - y(u_i;p)}{\\sigma}\\right)^2\n$$\nwhere $\\{(u_i, y_i)\\}_{i=1}^m$ are the $m$ data points.\n\nThe core of the problem is the profile likelihood analysis for the parameter $n$. The profile likelihood is a function of a single parameter of interest, obtained by optimizing over all other (nuisance) parameters. For a fixed value of $n$, we define the profiled objective function $S_{\\mathrm{prof}}(n)$ as the minimum possible value of $S(p)$, where the minimization is performed over the nuisance parameters $(y_0, V, K)$:\n$$\nS_{\\mathrm{prof}}(n) \\;=\\; \\min_{(y_0,V,K)} \\; S(y_0,V,K,n)\n$$\nThis minimization is subject to the box constraints $y_0 \\in [0, 2000]$, $V \\in [0, 5000]$, and $K \\in [10^{-3}, 10^{3}]$.\n\nThe algorithm proceeds as follows:\n1. A uniform grid of values for the Hill coefficient $n$ is defined in the interval $[0.5, 6.0]$ with a step size of $\\Delta n = 0.05$.\n2. For each value of $n$ on this grid, the inner optimization problem to find $S_{\\mathrm{prof}}(n)$ is solved. This is a constrained nonlinear least-squares problem for the parameters $(y_0, V, K)$. We employ the Trust Region Reflective (`trf`) algorithm, as implemented in `scipy.optimize.least_squares`, which is suitable for bound-constrained problems. The optimization is initialized with guesses for $(y_0, V, K)$ derived from the data: $y_0^{(0)} = \\min_i y_i$, $V^{(0)} = \\max_i y_i - \\min_i y_i$, and $K^{(0)} = \\mathrm{median}(u_i)$.\n3. The values of $S_{\\mathrm{prof}}(n)$ are tabulated for all $n$ on the grid. The shape of this profile provides information about the identifiability of $n$. The grid point $n^\\ast$ that yields the minimum value of the profile, $S_{min} = \\min_n S_{\\mathrm{prof}}(n)$, is the maximum likelihood estimate of $n$ (restricted to the grid).\n\nThe analysis is performed for three test cases using noise-free data generated from the model with specified true parameters.\n- For Test Case A, the required output is the absolute error $|n^\\ast - n^{\\mathrm{true}}|$. Since the data is noise-free and covers the full dynamic range of the response curve, the profile is expected to be sharp and the optimization well-behaved, leading to $n^\\ast$ being very close or equal to $n^{\\mathrm{true}}$.\n- For Test Case B, we assess identifiability. Based on the likelihood ratio test, the $95\\%$ confidence interval for $n$ consists of all values $n$ for which the profile does not exceed a certain threshold above its minimum: $\\{ n : S_{\\mathrm{prof}}(n) - S_{min} \\le \\chi^2_{1,0.95} \\}$, where $\\chi^2_{1,0.95} \\approx 3.841458820694124$ is the $95\\%$ quantile of the chi-square distribution with one degree of freedom. The parameter $n$ is deemed identifiable if this confidence interval is finite and strictly contained within the analysis grid of $[0.5, 6.0]$. Data from a limited concentration range (low-concentration regime) is expected to make $n$ poorly identifiable, resulting in a wide confidence interval that extends to the grid boundaries.\n- For Test Case C, data is sampled only in the high-concentration, saturating part of the curve. This provides little information for estimating the parameters, which can lead to a flat or distorted profile. The task is to determine if the resulting estimate $n^\\ast$ falls on the boundary of the search grid, which is an indicator of such poor identifiability. The required output is an integer flag.\n\nThe implementation encapsulates this logic, iterating through the test cases and applying the specified analysis to the computed profile for each.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import least_squares\n\ndef solve():\n    \"\"\"\n    Solves the profile likelihood analysis problem for three test cases of a \n    Hill-type repression model.\n    \"\"\"\n    # Global constants as specified in the problem\n    CHI_SQUARED_VAL = 3.841458820694124\n    \n    def model_func(p, u):\n        \"\"\"\n        Hill-type repression model.\n        p: parameter vector (y0, V, K, n)\n        u: inducer concentration array\n        \"\"\"\n        y0, V, K, n = p\n        # Bounds on K in the optimizer prevent it from being zero.\n        term = u / K\n        return y0 + V / (1 + term**n)\n\n    def run_profile_analysis(u_data, p_true, sigma):\n        \"\"\"\n        Performs the profile likelihood analysis for a given test case.\n        It grids over n, and for each n, optimizes the other parameters.\n        \"\"\"\n        # Generate noise-free measurement data from the true model\n        y_data = model_func(p_true, u_data)\n\n        # 1. Define a uniform grid for the parameter n\n        n_grid = np.arange(0.5, 6.0 + 0.05 / 2, 0.05)\n\n        # Use initial guesses derived from the data for the inner optimization\n        y0_guess = np.min(y_data)\n        V_guess = np.max(y_data) - np.min(y_data)\n        K_guess = np.median(u_data)\n        p0_inner = [y0_guess, V_guess, K_guess]\n        \n        # Bounds for the inner optimization of (y0, V, K)\n        bounds_inner = ([0, 0, 1e-3], [2000, 5000, 1e3])\n        \n        s_prof_values = []\n\n        # Residuals function for scipy.optimize.least_squares\n        def residuals(p_inner, n_fixed, u, y, sigma_val):\n            y0, V, K = p_inner\n            p_full = (y0, V, K, n_fixed)\n            y_model = model_func(p_full, u)\n            return (y - y_model) / sigma_val\n\n        # 2. For each grid point n, compute S_prof(n) by minimizing over (y0, V, K)\n        for n_val in n_grid:\n            result = least_squares(\n                fun=residuals,\n                x0=p0_inner,\n                args=(n_val, u_data, y_data, sigma),\n                bounds=bounds_inner,\n                method='trf',\n                ftol=1e-9, gtol=1e-9, xtol=1e-9 # Stricter tolerances for accuracy\n            )\n            # S(p) = sum of squared residuals. result.cost is 0.5 * sum of squares.\n            s_prof_values.append(2 * result.cost)\n\n        s_prof_values = np.array(s_prof_values)\n\n        # 3. Find n* that attains the global minimum of S_prof(n) on the grid\n        s_min = np.min(s_prof_values)\n        min_idx = np.argmin(s_prof_values)\n        n_star = n_grid[min_idx]\n\n        return n_grid, s_prof_values, s_min, n_star\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            'u': np.array([0.1, 0.3, 1, 3, 10, 30, 100]),\n            'p_true': (100, 900, 10, 2.5),\n            'sigma': 30,\n            'type': 'A'\n        },\n        {\n            'u': np.array([0.1, 0.2, 0.3, 0.5, 0.8, 1.0, 1.5]),\n            'p_true': (100, 900, 10, 2.5),\n            'sigma': 30,\n            'type': 'B'\n        },\n        {\n            'u': np.array([30, 50, 80, 120, 200, 300]),\n            'p_true': (100, 900, 10, 2.5),\n            'sigma': 30,\n            'type': 'C'\n        }\n    ]\n    \n    results = []\n    \n    for case in test_cases:\n        n_grid, s_prof, s_min, n_star = run_profile_analysis(case['u'], case['p_true'], case['sigma'])\n        \n        if case['type'] == 'A':\n            n_true = case['p_true'][3]\n            result = abs(n_star - n_true)\n            results.append(result)\n            \n        elif case['type'] == 'B':\n            conf_set_mask = (s_prof - s_min) <= CHI_SQUARED_VAL\n            conf_set_indices = np.where(conf_set_mask)[0]\n            \n            is_identifiable = False\n            # Check if confidence set is non-empty and strictly within grid boundaries\n            if len(conf_set_indices) > 0:\n                conf_set_lower_n = n_grid[conf_set_indices[0]]\n                conf_set_upper_n = n_grid[conf_set_indices[-1]]\n                \n                # 'strictly inside' means lower bound > grid minimum and upper bound < grid maximum\n                if conf_set_lower_n > n_grid[0] and conf_set_upper_n < n_grid[-1]:\n                    is_identifiable = True\n            \n            results.append(is_identifiable)\n            \n        elif case['type'] == 'C':\n            # Check if n* is exactly at a boundary. Direct comparison is fine as n_star\n            # is selected from the n_grid array.\n            is_at_boundary = (n_star == n_grid[0]) or (n_star == n_grid[-1])\n            result = 1 if is_at_boundary else 0\n            results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{results[0]},{results[1]},{results[2]}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Many synthetic biology circuits are dynamic systems, best described by Ordinary Differential Equations (ODEs). Fitting these models to time-course data requires extending our non-linear least squares toolkit. The primary challenge is to efficiently compute the Jacobian, which contains the sensitivities of the system's state with respect to its parameters at each point in time . This advanced practice introduces forward sensitivity analysis, an elegant method where differential equations for the sensitivities themselves are derived and solved simultaneously with the system's state, providing the exact gradients needed for optimization.",
            "id": "3923532",
            "problem": "Consider a one-gene activation module commonly used in synthetic biology to summarize transcriptional regulation with a Hill-type nonlinearity. Let $x(t)$ denote a dimensionless concentration of a reporter protein. Assume a lumped production–degradation dynamics with a constant inducer input $I>0$, where production is activated by $I$ via a Hill activation function and degradation is first-order. The dynamical model is\n$$\n\\frac{dx}{dt} \\;=\\; f\\!\\left(x;\\theta\\right) \\;=\\; \\alpha\\,h(I;n,K) \\;-\\; \\delta\\,x,\n$$\nwhere $\\theta = (\\alpha,\\delta,n,K)$ are unknown, strictly positive, dimensionless parameters: production gain $\\alpha>0$, degradation rate $\\delta>0$, Hill coefficient $n>0$, and half-saturation constant $K>0$. The activation function is\n$$\nh(I;n,K) \\;=\\; \\frac{I^n}{K^n+I^n}.\n$$\nAssume $x(0)=x_0$ is known and does not depend on $\\theta$, and that $I$ is a known, positive constant. In nonlinear least squares for parameter estimation from discrete observations at times $t_1,\\dots,t_m$, the residual vector is $r(\\theta) \\in \\mathbb{R}^m$ with entries $r_i(\\theta)=x(t_i;\\theta)-y_i$, where $y_i$ are given data. The Jacobian matrix $J(\\theta)\\in\\mathbb{R}^{m\\times 4}$ has entries\n$$\nJ_{i,p}(\\theta) \\;=\\; \\frac{\\partial r_i(\\theta)}{\\partial \\theta_p} \\;=\\; \\frac{\\partial x(t_i;\\theta)}{\\partial \\theta_p},\n$$\nwhich are the parametric sensitivities of $x$ at the observation times.\n\nYour tasks are:\n1. Starting from the definition of forward sensitivities for ordinary differential equations, derive the sensitivity differential equations for the sensitivities $s_p(t)=\\frac{\\partial x(t;\\theta)}{\\partial \\theta_p}$ corresponding to each parameter $\\theta_p\\in\\{\\alpha,\\delta,n,K\\}$. Your derivation must begin from the general identity for sensitivities of $x'(t)=f(x,t,\\theta)$,\n$$\n\\frac{d}{dt}\\left(\\frac{\\partial x}{\\partial \\theta_p}\\right) \\;=\\; \\frac{\\partial f}{\\partial x}\\,\\frac{\\partial x}{\\partial \\theta_p} \\;+\\; \\frac{\\partial f}{\\partial \\theta_p},\n$$\nand use the explicit form of $f(x;\\theta)$ and of $h(I;n,K)$.\n2. Implement a program that solves, for each parameter set, the augmented initial value problem that includes $x(t)$ and the sensitivities $s_\\alpha(t)$, $s_\\delta(t)$, $s_n(t)$, and $s_K(t)$, with initial conditions $x(0)=x_0$ and $s_\\alpha(0)=s_\\delta(0)=s_n(0)=s_K(0)=0$.\n3. For each test case below, integrate the augmented system and assemble the Jacobian $J(\\theta)$ evaluated at the specified measurement times $\\{t_i\\}$. Then, compute the Euclidean (two-)norms of the Jacobian columns corresponding to the Hill coefficient $n$ and the half-saturation constant $K$, that is,\n$$\n\\left\\|J_{\\cdot,n}\\right\\|_2 \\;=\\; \\sqrt{\\sum_{i=1}^m \\left(\\frac{\\partial x(t_i;\\theta)}{\\partial n}\\right)^2}, \\qquad\n\\left\\|J_{\\cdot,K}\\right\\|_2 \\;=\\; \\sqrt{\\sum_{i=1}^m \\left(\\frac{\\partial x(t_i;\\theta)}{\\partial K}\\right)^2}.\n$$\n\nAll quantities are dimensionless, and time $t$ is dimensionless. No physical units are required. Angles are not involved. The program must implement numerically stable ordinary differential equation integration and sensitivity propagation and must produce deterministic floating-point outputs rounded to six decimal places.\n\nTest suite:\n- Case A (balanced activation): $x_0=0$, $I=50$, $\\alpha=2.5$, $\\delta=0.3$, $n=2.0$, $K=40.0$, measurement times $t=\\{0.5,1.0,2.0,4.0,8.0\\}$.\n- Case B (sub-saturating input, small activation): $x_0=0$, $I=5.0$, $\\alpha=2.5$, $\\delta=0.3$, $n=3.0$, $K=100.0$, measurement times $t=\\{1.0,2.0,5.0,10.0,20.0\\}$.\n- Case C (near-saturated input, slow degradation): $x_0=0$, $I=100.0$, $\\alpha=2.5$, $\\delta=0.05$, $n=4.0$, $K=5.0$, measurement times $t=\\{1.0,5.0,15.0,30.0\\}$.\n\nRequired final output format:\n- Your program should produce a single line of output containing a flattened list of six floating-point numbers rounded to six decimal places, in the following order: `[||J_n||_2^A, ||J_K||_2^A, ||J_n||_2^B, ||J_K||_2^B, ||J_n||_2^C, ||J_K||_2^C]`, printed as a comma-separated list enclosed in square brackets with no additional whitespace, for example, `[a_1,a_2,a_3,a_4,a_5,a_6]` where each $a_i$ is a decimal with exactly six digits after the decimal point.",
            "solution": "We start from the general forward sensitivity equations for an ordinary differential equation with state $x(t)\\in\\mathbb{R}$,\n$$\n\\frac{dx}{dt} \\;=\\; f(x,t,\\theta),\n$$\nwhere $\\theta\\in\\mathbb{R}^p$ are parameters. For each parameter $\\theta_p$, the sensitivity $s_p(t)=\\frac{\\partial x(t;\\theta)}{\\partial \\theta_p}$ satisfies the linear nonhomogeneous differential equation\n$$\n\\frac{ds_p}{dt} \\;=\\; \\frac{\\partial f}{\\partial x}\\,s_p \\;+\\; \\frac{\\partial f}{\\partial \\theta_p},\n$$\nwith initial condition $s_p(0)=\\frac{\\partial x(0)}{\\partial \\theta_p}$. When the initial condition is fixed and does not depend on $\\theta$, this sensitivity initial condition is $s_p(0)=0$.\n\nIn our model, the right-hand side function is\n$$\nf(x;\\theta)\\;=\\;\\alpha\\,h(I;n,K)\\;-\\;\\delta\\,x,\\qquad h(I;n,K)\\;=\\;\\frac{I^n}{K^n+I^n},\n$$\nwith constant inducer $I>0$. Note that $f$ does not explicitly depend on $t$. The partial derivative with respect to the state is\n$$\n\\frac{\\partial f}{\\partial x}\\;=\\;-\\delta.\n$$\nWe now compute parameter partial derivatives. For the production gain,\n$$\n\\frac{\\partial f}{\\partial \\alpha}\\;=\\;h(I;n,K)\\;=\\;\\frac{I^n}{K^n+I^n}.\n$$\nFor the degradation rate,\n$$\n\\frac{\\partial f}{\\partial \\delta}\\;=\\;-\\;x.\n$$\nFor the half-saturation constant $K$, using the quotient form $h=\\frac{I^n}{K^n+I^n}$, we differentiate with respect to $K$:\n$$\n\\frac{\\partial h}{\\partial K}\\;=\\;I^n\\,\\frac{\\partial}{\\partial K}\\left(K^n+I^n\\right)^{-1}\\;=\\;-I^n\\left(K^n+I^n\\right)^{-2}\\,\\frac{\\partial}{\\partial K}K^n\\;=\\;-\\,\\frac{n\\,I^n\\,K^{n-1}}{\\left(K^n+I^n\\right)^2}.\n$$\nThus,\n$$\n\\frac{\\partial f}{\\partial K}\\;=\\;\\alpha\\,\\frac{\\partial h}{\\partial K}\\;=\\;-\\,\\alpha\\,\\frac{n\\,I^n\\,K^{n-1}}{\\left(K^n+I^n\\right)^2}.\n$$\nFor the Hill coefficient $n$, we differentiate $h$ using $a=K^n$ and $b=I^n$, with $\\frac{\\partial a}{\\partial n}=a\\ln K$ and $\\frac{\\partial b}{\\partial n}=b\\ln I$. Using $h=\\frac{b}{a+b}$ and the quotient rule,\n$$\n\\frac{\\partial h}{\\partial n}\\;=\\;\\frac{\\left(\\frac{\\partial b}{\\partial n}\\right)(a+b)-b\\left(\\frac{\\partial a}{\\partial n}+\\frac{\\partial b}{\\partial n}\\right)}{(a+b)^2}\\;=\\;\\frac{ab\\left(\\ln I-\\ln K\\right)}{(a+b)^2}\\;=\\;\\frac{K^n I^n\\,\\ln\\!\\left(\\frac{I}{K}\\right)}{\\left(K^n+I^n\\right)^2}.\n$$\nTherefore,\n$$\n\\frac{\\partial f}{\\partial n}\\;=\\;\\alpha\\,\\frac{\\partial h}{\\partial n}\\;=\\;\\alpha\\,\\frac{K^n I^n\\,\\ln\\!\\left(\\frac{I}{K}\\right)}{\\left(K^n+I^n\\right)^2}.\n$$\n\nCollecting these, the sensitivity differential equations for $s_\\alpha$, $s_\\delta$, $s_n$, and $s_K$ are\n$$\n\\frac{ds_\\alpha}{dt}\\;=\\;-\\,\\delta\\,s_\\alpha\\;+\\;\\frac{I^n}{K^n+I^n},\\qquad s_\\alpha(0)=0,\n$$\n$$\n\\frac{ds_\\delta}{dt}\\;=\\;-\\,\\delta\\,s_\\delta\\;-\\;x,\\qquad s_\\delta(0)=0,\n$$\n$$\n\\frac{ds_n}{dt}\\;=\\;-\\,\\delta\\,s_n\\;+\\;\\alpha\\,\\frac{K^n I^n\\,\\ln\\!\\left(\\frac{I}{K}\\right)}{\\left(K^n+I^n\\right)^2},\\qquad s_n(0)=0,\n$$\n$$\n\\frac{ds_K}{dt}\\;=\\;-\\,\\delta\\,s_K\\;-\\;\\alpha\\,\\frac{n\\,I^n\\,K^{n-1}}{\\left(K^n+I^n\\right)^2},\\qquad s_K(0)=0,\n$$\ncoupled with the state equation\n$$\n\\frac{dx}{dt}\\;=\\;\\alpha\\,\\frac{I^n}{K^n+I^n}\\;-\\;\\delta\\,x,\\qquad x(0)=x_0.\n$$\nThese form a linear-in-sensitivity, lower-triangular augmented system that can be solved by standard numerical integrators. Because the residual vector entries are $r_i(\\theta)=x(t_i;\\theta)-y_i$ with fixed $y_i$, the Jacobian entries are $J_{i,p}(\\theta)=\\frac{\\partial x(t_i;\\theta)}{\\partial \\theta_p}=s_p(t_i)$, independent of $y_i$.\n\nAlgorithmic design:\n- For each test case, construct the augmented state vector $\\tilde{x}=[x,s_\\alpha,s_\\delta,s_n,s_K]^\\top$ with the initial condition $\\tilde{x}(0)=[x_0,0,0,0,0]^\\top$.\n- Define the right-hand side using the formulas above. Since $I$, $\\alpha$, $\\delta$, $n$, and $K$ are constants, the terms $\\frac{\\partial f}{\\partial \\theta_p}$ are constants in time; however, the $-\\delta s_p$ contribution and the $-\\;x$ term in $\\frac{ds_\\delta}{dt}$ introduce time dependence through $x$ and $s_p$.\n- Integrate from $t=0$ to the largest measurement time using a robust ordinary differential equation solver with tight tolerances to ensure reproducibility. Evaluate the solution at the specified measurement times.\n- Assemble the Jacobian $J$ at those times by taking the columns $J_{\\cdot,n}$ and $J_{\\cdot,K}$ as the samples of $s_n$ and $s_K$, respectively.\n- Compute the Euclidean norms $\\left\\|J_{\\cdot,n}\\right\\|_2$ and $\\left\\|J_{\\cdot,K}\\right\\|_2$ as square roots of the sums of squares of the sampled sensitivities.\n- Round each norm to six decimal places.\n- Output the flattened list for the three cases in the order specified.\n\nNumerical considerations:\n- Because $I>0$ and $K>0$, the logarithm $\\ln\\!\\left(\\frac{I}{K}\\right)$ is well-defined. The Hill function $h(I;n,K)$ and its derivatives remain bounded for the given test cases.\n- Tight solver tolerances (for example, relative tolerance $10^{-10}$ and absolute tolerance $10^{-12}$) ensure deterministic results suitable for automated testing. The system is non-stiff for the given parameters, and a standard explicit Runge–Kutta method is adequate.\n\nThe resulting program integrates the augmented system for each test case and prints a single line with six comma-separated floating-point values corresponding to the requested norms, in the required order and format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef augmented_rhs(t, y, alpha, delta, n, K, I):\n    \"\"\"\n    Augmented ODE RHS for state x and sensitivities s_alpha, s_delta, s_n, s_K.\n    y = [x, s_alpha, s_delta, s_n, s_K]\n    \"\"\"\n    x = y[0]\n    s_alpha = y[1]\n    s_delta = y[2]\n    s_n = y[3]\n    s_K = y[4]\n\n    # Hill activation and derivatives with respect to parameters\n    # h = I^n / (K^n + I^n)\n    # Partial derivatives:\n    # df/dx = -delta\n    # df/dalpha = h\n    # df/ddelta = -x\n    # df/dn = alpha * d h / d n\n    # df/dK = alpha * d h / d K\n    # where\n    # d h / d n = (K^n * I^n * ln(I/K)) / (K^n + I^n)^2\n    # d h / d K = - n * I^n * K^(n-1) / (K^n + I^n)^2\n    # All quantities are positive except ln(I/K) may be negative; it is fine.\n\n    # Precompute powers and denominators\n    Kn = K ** n\n    In = I ** n\n    denom = Kn + In\n    h = In / denom\n\n    # Derivatives of h\n    # Use safe computation for log(I/K)\n    log_I_over_K = np.log(I / K)\n    dh_dn = (Kn * In * log_I_over_K) / (denom ** 2)\n    # For d h / d K, handle K>0, n>0 as guaranteed in test cases\n    if K > 0:\n        dh_dK = - (n * In * (K ** (n - 1))) / (denom ** 2)\n    else:\n        # Should not occur with the given test cases; fallback to zero to avoid NaN\n        dh_dK = 0.0\n\n    dfdx = -delta\n    dfdalpha = h\n    dfddelta = -x\n    dfdn = alpha * dh_dn\n    dfdK = alpha * dh_dK\n\n    # State derivative\n    dxdt = alpha * h - delta * x\n\n    # Sensitivity derivatives\n    ds_alpha_dt = dfdx * s_alpha + dfdalpha\n    ds_delta_dt = dfdx * s_delta + dfddelta\n    ds_n_dt = dfdx * s_n + dfdn\n    ds_K_dt = dfdx * s_K + dfdK\n\n    return np.array([dxdt, ds_alpha_dt, ds_delta_dt, ds_n_dt, ds_K_dt], dtype=float)\n\ndef integrate_and_compute_norms(params):\n    \"\"\"\n    Integrate augmented system and compute L2 norms of the Jacobian columns for n and K.\n    params: dict with keys alpha, delta, n, K, I, x0, times\n    Returns (norm_J_n, norm_J_K)\n    \"\"\"\n    alpha = params[\"alpha\"]\n    delta = params[\"delta\"]\n    n = params[\"n\"]\n    K = params[\"K\"]\n    I = params[\"I\"]\n    x0 = params[\"x0\"]\n    times = np.array(params[\"times\"], dtype=float)\n\n    # Initial augmented state: [x, s_alpha, s_delta, s_n, s_K]\n    y0 = np.array([x0, 0.0, 0.0, 0.0, 0.0], dtype=float)\n\n    # Integrate using solve_ivp with strict tolerances for determinism\n    t0 = 0.0\n    tf = float(np.max(times))\n    sol = solve_ivp(\n        fun=lambda t, y: augmented_rhs(t, y, alpha, delta, n, K, I),\n        t_span=(t0, tf),\n        y0=y0,\n        method=\"RK45\",\n        t_eval=times,\n        rtol=1e-10,\n        atol=1e-12,\n        vectorized=False,\n        dense_output=False,\n    )\n\n    if not sol.success:\n        raise RuntimeError(f\"ODE integration failed: {sol.message}\")\n\n    # Extract sensitivities at measurement times\n    s_n = sol.y[3, :]  # partial x / partial n\n    s_K = sol.y[4, :]  # partial x / partial K\n\n    # Compute Euclidean norms\n    norm_J_n = float(np.sqrt(np.sum(s_n ** 2)))\n    norm_J_K = float(np.sqrt(np.sum(s_K ** 2)))\n\n    return norm_J_n, norm_J_K\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {\n            \"name\": \"A\",\n            \"alpha\": 2.5,\n            \"delta\": 0.3,\n            \"n\": 2.0,\n            \"K\": 40.0,\n            \"I\": 50.0,\n            \"x0\": 0.0,\n            \"times\": [0.5, 1.0, 2.0, 4.0, 8.0],\n        },\n        # Case B\n        {\n            \"name\": \"B\",\n            \"alpha\": 2.5,\n            \"delta\": 0.3,\n            \"n\": 3.0,\n            \"K\": 100.0,\n            \"I\": 5.0,\n            \"x0\": 0.0,\n            \"times\": [1.0, 2.0, 5.0, 10.0, 20.0],\n        },\n        # Case C\n        {\n            \"name\": \"C\",\n            \"alpha\": 2.5,\n            \"delta\": 0.05,\n            \"n\": 4.0,\n            \"K\": 5.0,\n            \"I\": 100.0,\n            \"x0\": 0.0,\n            \"times\": [1.0, 5.0, 15.0, 30.0],\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        norm_J_n, norm_J_K = integrate_and_compute_norms(case)\n        # Round to six decimals and format as strings later\n        results.append(norm_J_n)\n        results.append(norm_J_K)\n\n    # Format output with exactly six digits after the decimal point and no spaces\n    formatted = \"[\" + \",\".join(f\"{v:.6f}\" for v in results) + \"]\"\n    print(formatted)\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}