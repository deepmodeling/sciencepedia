{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp non-linear least squares, it is essential to look under the hood of the optimization algorithm. This first practice provides a direct, hands-on experience with the Gauss-Newton method, the iterative engine that drives many NLS solvers. By manually performing a single update step for estimating parameters in the ubiquitous Michaelis-Menten model, you will solidify your understanding of how model linearization via the Jacobian matrix is used to iteratively refine parameter guesses. ",
            "id": "3923519",
            "problem": "A synthetic enzyme module in a gene circuit converts substrate into product following Michaelis–Menten (MM) kinetics. The measurable initial rate at substrate concentration $S$ is modeled by $v(S; V_{\\max}, K_{M}) = \\dfrac{V_{\\max} S}{K_{M} + S}$. You are given substrate concentrations and corresponding measured initial rates from a calibrated microfluidic assay suitable for least squares (LS) parameter estimation:\n- Substrate concentrations (use micromolar units): $S \\in \\{5, 10, 20, 50\\}$.\n- Measured initial rates (use micromolar per minute units): $v^{\\mathrm{obs}} \\in \\{\\frac{5}{8}, 1, \\frac{10}{7}, \\frac{25}{13}\\}$.\n\nStarting from the initial parameter guess $V_{\\max}^{(0)} = 2.0$ and $K_{M}^{(0)} = 10$, apply a single Gauss–Newton (GN) iteration to the non-linear LS problem that minimizes the sum of squared residuals $\\sum_{i} \\left(v_{i}^{\\mathrm{obs}} - v(S_{i}; V_{\\max}, K_{M})\\right)^{2}$. Compute the GN update explicitly from first principles, and report the updated parameter vector $\\left(V_{\\max}^{(1)}, K_{M}^{(1)}\\right)$ after this single iteration.\n\nRound your final numerical values to four significant figures. Express $V_{\\max}$ in micromolar per minute and $K_{M}$ in micromolar. Define all symbols you introduce, and justify each step from first principles appropriate for non-linear least squares methods in synthetic biology modeling. Avoid using any pre-derived shortcut formulas beyond those justified from Taylor linearization and normal equations for least squares. For clarity, define any acronyms you use on first appearance, such as Michaelis–Menten (MM), least squares (LS), and Gauss–Newton (GN).",
            "solution": "The objective is to refine an initial estimate of the Michaelis–Menten (MM) kinetic parameters, $V_{\\max}$ and $K_{M}$, by applying a single iteration of the Gauss–Newton (GN) algorithm. This method is a standard approach for solving non-linear least squares (LS) problems, which commonly arise in the fitting of mechanistic models to experimental data in synthetic biology.\n\nLet the parameter vector be denoted by $\\mathbf{p} = \\begin{pmatrix} V_{\\max} \\\\ K_{M} \\end{pmatrix}$. The model for the initial rate $v$ at a given substrate concentration $S$ is given by the MM equation:\n$$v(S; \\mathbf{p}) = \\frac{V_{\\max} S}{K_{M} + S}$$\nWe are provided with a set of $n=4$ experimental data points $(S_i, v_{i}^{\\mathrm{obs}})$. The goal of the LS problem is to find the parameter vector $\\mathbf{p}$ that minimizes the sum of squared residuals, $S(\\mathbf{p})$:\n$$S(\\mathbf{p}) = \\sum_{i=1}^{n} r_i(\\mathbf{p})^2 = \\sum_{i=1}^{n} \\left(v_{i}^{\\mathrm{obs}} - v(S_i; \\mathbf{p})\\right)^2$$\nwhere $r_i(\\mathbf{p})$ is the residual for the $i$-th data point.\n\nThe GN algorithm is an iterative method where, at each step $k$, we seek an update $\\Delta\\mathbf{p}^{(k)}$ that improves the current parameter estimate $\\mathbf{p}^{(k)}$. The new estimate is then $\\mathbf{p}^{(k+1)} = \\mathbf{p}^{(k)} + \\Delta\\mathbf{p}^{(k)}$. The algorithm is derived by linearizing the model function $v(S_i; \\mathbf{p})$ around the current estimate $\\mathbf{p}^{(k)}$. The residual vector is approximated as:\n$$\\mathbf{r}(\\mathbf{p}^{(k)} + \\Delta\\mathbf{p}) \\approx \\mathbf{r}(\\mathbf{p}^{(k)}) - \\mathbf{J}(\\mathbf{p}^{(k)}) \\Delta\\mathbf{p}$$\nwhere $\\mathbf{J}$ is the Jacobian matrix of the model function $v$ with respect to the parameters, with elements $J_{ij} = \\frac{\\partial v_i}{\\partial p_j}$. Minimizing the squared norm of this linearized residual, $\\| \\mathbf{r}^{(k)} - \\mathbf{J}^{(k)} \\Delta\\mathbf{p} \\|_2^2$, leads to the normal equations for the update step:\n$$\\left((\\mathbf{J}^{(k)})^T \\mathbf{J}^{(k)}\\right) \\Delta\\mathbf{p}^{(k)} = (\\mathbf{J}^{(k)})^T \\mathbf{r}(\\mathbf{p}^{(k)})$$\n\nWe will now perform a single iteration, starting with $k=0$.\n\n**Step 1: Define initial state and data**\n- Initial parameter guess: $\\mathbf{p}^{(0)} = \\begin{pmatrix} V_{\\max}^{(0)} \\\\ K_{M}^{(0)} \\end{pmatrix} = \\begin{pmatrix} 2.0 \\\\ 10 \\end{pmatrix}$. $V_{\\max}$ is in units of $\\mu\\text{M} \\cdot \\text{min}^{-1}$ and $K_{M}$ is in $\\mu\\text{M}$.\n- Data points $(S_i, v_{i}^{\\mathrm{obs}})$:\n$(5, 5/8)$, $(10, 1)$, $(20, 10/7)$, $(50, 25/13)$.\n\n**Step 2: Calculate the Jacobian matrix $\\mathbf{J}$ at $\\mathbf{p}^{(0)}$**\nThe partial derivatives of $v(S; \\mathbf{p})$ with respect to the parameters are:\n$$\\frac{\\partial v}{\\partial V_{\\max}} = \\frac{S}{K_{M} + S}$$\n$$\\frac{\\partial v}{\\partial K_{M}} = V_{\\max} S \\left( -(K_{M} + S)^{-2} \\right) = -\\frac{V_{\\max} S}{(K_{M} + S)^2}$$\nEvaluating these at $\\mathbf{p}^{(0)} = (2, 10)$ for each $S_i$:\nFor $S_1 = 5$: $\\frac{\\partial v}{\\partial V_{\\max}} = \\frac{5}{15} = \\frac{1}{3}$, $\\frac{\\partial v}{\\partial K_{M}} = -\\frac{2 \\cdot 5}{15^2} = -\\frac{10}{225} = -\\frac{2}{45}$.\nFor $S_2 = 10$: $\\frac{\\partial v}{\\partial V_{\\max}} = \\frac{10}{20} = \\frac{1}{2}$, $\\frac{\\partial v}{\\partial K_{M}} = -\\frac{2 \\cdot 10}{20^2} = -\\frac{20}{400} = -\\frac{1}{20}$.\nFor $S_3 = 20$: $\\frac{\\partial v}{\\partial V_{\\max}} = \\frac{20}{30} = \\frac{2}{3}$, $\\frac{\\partial v}{\\partial K_{M}} = -\\frac{2 \\cdot 20}{30^2} = -\\frac{40}{900} = -\\frac{2}{45}$.\nFor $S_4 = 50$: $\\frac{\\partial v}{\\partial V_{\\max}} = \\frac{50}{60} = \\frac{5}{6}$, $\\frac{\\partial v}{\\partial K_{M}} = -\\frac{2 \\cdot 50}{60^2} = -\\frac{100}{3600} = -\\frac{1}{36}$.\nThe Jacobian matrix at $\\mathbf{p}^{(0)}$ is:\n$$\\mathbf{J}^{(0)} = \\begin{pmatrix} 1/3 & -2/45 \\\\ 1/2 & -1/20 \\\\ 2/3 & -2/45 \\\\ 5/6 & -1/36 \\end{pmatrix}$$\n\n**Step 3: Calculate the residual vector $\\mathbf{r}(\\mathbf{p}^{(0)})$**\nFirst, calculate the model predictions $v(S_i; \\mathbf{p}^{(0)}) = \\frac{2 S_i}{10+S_i}$:\n$v(5; \\mathbf{p}^{(0)}) = \\frac{2 \\cdot 5}{10+5} = \\frac{10}{15} = \\frac{2}{3}$.\n$v(10; \\mathbf{p}^{(0)}) = \\frac{2 \\cdot 10}{10+10} = \\frac{20}{20} = 1$.\n$v(20; \\mathbf{p}^{(0)}) = \\frac{2 \\cdot 20}{10+20} = \\frac{40}{30} = \\frac{4}{3}$.\n$v(50; \\mathbf{p}^{(0)}) = \\frac{2 \\cdot 50}{10+50} = \\frac{100}{60} = \\frac{5}{3}$.\nThe residuals $r_i^{(0)} = v_{i}^{\\mathrm{obs}} - v(S_i; \\mathbf{p}^{(0)})$ are:\n$r_1 = \\frac{5}{8} - \\frac{2}{3} = \\frac{15-16}{24} = -\\frac{1}{24}$.\n$r_2 = 1 - 1 = 0$.\n$r_3 = \\frac{10}{7} - \\frac{4}{3} = \\frac{30-28}{21} = \\frac{2}{21}$.\n$r_4 = \\frac{25}{13} - \\frac{5}{3} = \\frac{75-65}{39} = \\frac{10}{39}$.\nThe residual vector is:\n$$\\mathbf{r}^{(0)} = \\begin{pmatrix} -1/24 \\\\ 0 \\\\ 2/21 \\\\ 10/39 \\end{pmatrix} \\approx \\begin{pmatrix} -0.041667 \\\\ 0 \\\\ 0.095238 \\\\ 0.256410 \\end{pmatrix}$$\n\n**Step 4: Solve the normal equations**\nWe must solve $(\\mathbf{J}^T \\mathbf{J}) \\Delta\\mathbf{p} = \\mathbf{J}^T \\mathbf{r}$.\nFirst, compute $(\\mathbf{J}^{(0)})^T \\mathbf{J}^{(0)}$:\n$$(\\mathbf{J}^{(0)})^T \\mathbf{J}^{(0)} = \\begin{pmatrix} 1/3 & 1/2 & 2/3 & 5/6 \\\\ -2/45 & -1/20 & -2/45 & -1/36 \\end{pmatrix} \\begin{pmatrix} 1/3 & -2/45 \\\\ 1/2 & -1/20 \\\\ 2/3 & -2/45 \\\\ 5/6 & -1/36 \\end{pmatrix} $$\n$$ = \\begin{pmatrix} \\frac{1}{9}+\\frac{1}{4}+\\frac{4}{9}+\\frac{25}{36} & -\\frac{2}{135}-\\frac{1}{40}-\\frac{4}{135}-\\frac{5}{216} \\\\ -\\frac{2}{135}-\\frac{1}{40}-\\frac{4}{135}-\\frac{5}{216} & \\frac{4}{2025}+\\frac{1}{400}+\\frac{4}{2025}+\\frac{1}{1296} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} & -\\frac{5}{54} \\\\ -\\frac{5}{54} & \\frac{13}{1800} \\end{pmatrix}$$\nNumerically, $(\\mathbf{J}^{(0)})^T \\mathbf{J}^{(0)} \\approx \\begin{pmatrix} 1.5 & -0.092593 \\\\ -0.092593 & 0.007222 \\end{pmatrix}$.\n\nNext, compute $(\\mathbf{J}^{(0)})^T \\mathbf{r}^{(0)}$:\n$$(\\mathbf{J}^{(0)})^T \\mathbf{r}^{(0)} = \\begin{pmatrix} \\frac{1}{3}(-\\frac{1}{24}) + \\frac{1}{2}(0) + \\frac{2}{3}(\\frac{2}{21}) + \\frac{5}{6}(\\frac{10}{39}) \\\\ -\\frac{2}{45}(-\\frac{1}{24}) - \\frac{1}{20}(0) - \\frac{2}{45}(\\frac{2}{21}) - \\frac{1}{36}(\\frac{10}{39}) \\end{pmatrix} = \\begin{pmatrix} \\frac{575}{2184} \\\\ -\\frac{467}{49140} \\end{pmatrix}$$\nNumerically, $(\\mathbf{J}^{(0)})^T \\mathbf{r}^{(0)} \\approx \\begin{pmatrix} 0.263278 \\\\ -0.009503 \\end{pmatrix}$.\n\nWe solve the linear system for $\\Delta\\mathbf{p}^{(0)} = \\begin{pmatrix} \\Delta V_{\\max} \\\\ \\Delta K_M \\end{pmatrix}$:\n$$\\begin{pmatrix} 1.5 & -0.092593 \\\\ -0.092593 & 0.007222 \\end{pmatrix} \\begin{pmatrix} \\Delta V_{\\max} \\\\ \\Delta K_M \\end{pmatrix} = \\begin{pmatrix} 0.263278 \\\\ -0.009503 \\end{pmatrix}$$\nSolving this system yields:\n$$\\Delta V_{\\max} \\approx 0.451966$$\n$$\\Delta K_M \\approx 4.476483$$\n\n**Step 5: Update the parameter vector**\nThe new parameter estimate $\\mathbf{p}^{(1)}$ is:\n$$\\mathbf{p}^{(1)} = \\mathbf{p}^{(0)} + \\Delta\\mathbf{p}^{(0)} = \\begin{pmatrix} 2.0 \\\\ 10 \\end{pmatrix} + \\begin{pmatrix} 0.451966 \\\\ 4.476483 \\end{pmatrix} = \\begin{pmatrix} 2.451966 \\\\ 14.476483 \\end{pmatrix}$$\n\n**Step 6: Final Answer**\nRounding the components of $\\mathbf{p}^{(1)}$ to four significant figures gives:\n$$V_{\\max}^{(1)} \\approx 2.452$$\n$$K_{M}^{(1)} \\approx 14.48$$\nThe updated parameter vector is $(V_{\\max}^{(1)}, K_{M}^{(1)}) = (2.452, 14.48)$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 2.452 & 14.48 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Parameter estimation provides a single 'best-fit' value, but how certain are we of this estimate? This next exercise transitions from finding point estimates to quantifying their uncertainty. You will learn to compute approximate confidence intervals for parameters of a Hill function model by using the geometric information—specifically the local curvature—encoded in the Jacobian matrix at the solution.  This practice is crucial for reporting robust scientific findings and for understanding the limits of what can be inferred from a given dataset.",
            "id": "3923563",
            "problem": "A synthetic biology laboratory quantifies steady-state induction of a Green Fluorescent Protein (GFP) reporter under an arabinose-inducible promoter. The measured fluorescence response $y_i$ at inducer concentration $x_i$ is modeled by a cooperative Hill function with known baseline and cooperativity exponent:\n$$\ny_i \\;=\\; \\beta_0 \\;+\\; \\beta_1 \\,\\frac{x_i^{n}}{K^{n} + x_i^{n}} \\;+\\; \\varepsilon_i,\n$$\nwhere $\\beta_0$ is a known baseline, $\\beta_1$ is the unknown induction amplitude, $K$ is the unknown half-activation concentration, $n$ is the known cooperativity exponent, and $\\varepsilon_i$ are independent and identically distributed (i.i.d.) mean-zero Gaussian measurement errors with variance $\\sigma^2$. The laboratory uses Non-Linear Least Squares (NLS) to estimate the parameters by minimizing the residual sum of squares.\n\nIn an experiment with $m=12$ data points and fixed $n=2$ and $\\beta_0=0.20$ (relative fluorescence units), the NLS fit yields parameter estimates $\\hat{\\beta}_1 = 3.80$ (relative fluorescence units) and $\\hat{K} = 2.20$ (millimolar), with residual sum of squares $\\mathrm{RSS} = 6.40$ (relative fluorescence units squared). The Jacobian matrix of the model responses with respect to $(\\beta_1, K)$, evaluated at the optimum $(\\hat{\\beta}_1, \\hat{K})$, gives the approximate information matrix\n$$\nJ^{\\top}J \\;=\\;\n\\begin{pmatrix}\n400 & -20 \\\\\n-20 & 5\n\\end{pmatrix}.\n$$\n\nAssuming the standard linearized theory for NLS under i.i.d. Gaussian noise, compute the approximate $0.95$ confidence intervals for $\\beta_1$ and $K$. Use the appropriate Student’s $t$ quantile with $m-p$ degrees of freedom, where $p$ is the number of free parameters. Report the lower and upper bounds for each parameter as a row matrix in the order $(\\beta_1^{\\mathrm{low}}, \\beta_1^{\\mathrm{high}}, K^{\\mathrm{low}}, K^{\\mathrm{high}})$.\n\nRound all bounds to four significant figures. Express bounds for $\\beta_1$ in relative fluorescence units and for $K$ in millimolar (mM). Finally, briefly interpret the reliability of these intervals in the presence of nonlinearity, referring to the local curvature of the parameter-effects: suppose the standardized parameter-effects curvature values at the solution are $\\kappa_{\\beta_1} = 0.08$ and $\\kappa_{K} = 0.35$, and discuss which interval is expected to be less reliable and why. Your final reported numerical answer should be the row matrix of the four bounds without units.",
            "solution": "The goal is to compute approximate $95\\%$ confidence intervals for the NLS parameter estimates $\\hat{\\beta}_1$ and $\\hat{K}$, and to interpret their reliability.\n\n**1. Theoretical Framework**\nUnder the assumption of i.i.d. Gaussian noise, the approximate confidence interval for a parameter estimate $\\hat{\\theta}_j$ is given by:\n$$ \\hat{\\theta}_j \\pm t_{1-\\alpha/2, \\nu} \\times \\mathrm{se}(\\hat{\\theta}_j) $$\nwhere:\n-   $\\hat{\\theta}_j$ is the parameter estimate.\n-   $t_{1-\\alpha/2, \\nu}$ is the quantile from the Student's $t$-distribution for a confidence level of $1-\\alpha$ and $\\nu$ degrees of freedom.\n-   $\\mathrm{se}(\\hat{\\theta}_j)$ is the standard error of the estimate.\n\nThe approximate covariance matrix of the parameter estimates $\\hat{p} = (\\hat{\\beta}_1, \\hat{K})$ is given by $\\mathrm{Cov}(\\hat{p}) \\approx \\hat{\\sigma}^2 (J^T J)^{-1}$. The standard error of the $j$-th parameter is the square root of the $j$-th diagonal element of this matrix. The estimator for the error variance, $\\hat{\\sigma}^2$, is calculated from the residual sum of squares (RSS):\n$$ \\hat{\\sigma}^2 = \\frac{\\mathrm{RSS}}{m - p} $$\nwhere $m$ is the number of data points and $p$ is the number of estimated parameters.\n\n**2. Calculation Steps**\n*   **Degrees of Freedom and Variance Estimate**:\n    We are given $m=12$ data points and we estimate $p=2$ parameters ($\\beta_1, K$).\n    The degrees of freedom are $\\nu = m - p = 12 - 2 = 10$.\n    The residual sum of squares is $\\mathrm{RSS} = 6.40$.\n    The error variance estimate is $\\hat{\\sigma}^2 = \\frac{6.40}{10} = 0.64$.\n\n*   **Parameter Covariance Matrix**:\n    We are given the matrix $J^T J = \\begin{pmatrix} 400 & -20 \\\\ -20 & 5 \\end{pmatrix}$.\n    First, we find its inverse. The determinant is $\\det(J^T J) = (400)(5) - (-20)(-20) = 2000 - 400 = 1600$.\n    The inverse is $(J^T J)^{-1} = \\frac{1}{1600} \\begin{pmatrix} 5 & 20 \\\\ 20 & 400 \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{1600} & \\frac{20}{1600} \\\\ \\frac{20}{1600} & \\frac{400}{1600} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{320} & \\frac{1}{80} \\\\ \\frac{1}{80} & \\frac{1}{4} \\end{pmatrix}$.\n    The parameter covariance matrix is $\\mathrm{Cov}(\\hat{p}) \\approx 0.64 \\times \\begin{pmatrix} 1/320 & 1/80 \\\\ 1/80 & 1/4 \\end{pmatrix} = \\begin{pmatrix} 0.002 & 0.008 \\\\ 0.008 & 0.16 \\end{pmatrix}$.\n\n*   **Standard Errors**:\n    The standard errors are the square roots of the diagonal elements of the covariance matrix:\n    -   $\\mathrm{se}(\\hat{\\beta}_1) = \\sqrt{0.002} \\approx 0.04472$.\n    -   $\\mathrm{se}(\\hat{K}) = \\sqrt{0.16} = 0.4$.\n\n*   **Student's t-Quantile**:\n    For a $95\\%$ confidence interval, $\\alpha = 0.05$. We need the $t_{1-0.025, 10} = t_{0.975, 10}$ quantile. From statistical tables or software, $t_{0.975, 10} \\approx 2.228$.\n\n*   **Confidence Intervals**:\n    The parameter estimates are $\\hat{\\beta}_1 = 3.80$ and $\\hat{K} = 2.20$.\n    -   **For $\\beta_1$**:\n        Margin of error: $2.228 \\times 0.04472 \\approx 0.09963$.\n        Interval: $3.80 \\pm 0.09963 \\implies [3.70037, 3.89963]$.\n        Rounding to four significant figures: $[3.700, 3.900]$.\n    -   **For $K$**:\n        Margin of error: $2.228 \\times 0.4 = 0.8912$.\n        Interval: $2.20 \\pm 0.8912 \\implies [1.3088, 3.0912]$.\n        Rounding to four significant figures: $[1.309, 3.091]$.\n\n**3. Interpretation of Reliability**\nThe calculated confidence intervals are based on a linear approximation of the model around the best-fit parameters. The accuracy of this approximation, and thus the reliability of the intervals, depends on the degree of model nonlinearity. The parameter-effects curvature is a measure of this nonlinearity.\n\nWe are given the curvatures $\\kappa_{\\beta_1} = 0.08$ and $\\kappa_{K} = 0.35$. Since the curvature for $K$ is significantly larger than for $\\beta_1$ ($\\kappa_K \\gg \\kappa_{\\beta_1}$), the model exhibits much stronger nonlinear behavior with respect to parameter $K$ than it does for $\\beta_1$. Consequently, the linear approximation is less accurate for $K$. Therefore, the confidence interval for $K$ is expected to be **less reliable** than the one calculated for $\\beta_1$. More advanced methods, like profile likelihood, would be needed to obtain a more accurate confidence region for $K$.\n\nThe final results are presented as a row matrix of the four bounds.\n$$(\\beta_1^{\\mathrm{low}}, \\beta_1^{\\mathrm{high}}, K^{\\mathrm{low}}, K^{\\mathrm{high}}) = (3.700, 3.900, 1.309, 3.091)$$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n3.700 & 3.900 & 1.309 & 3.091\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The confidence intervals derived from the local curvature are powerful but rely on a linear approximation that can be misleading for highly non-linear models or when parameters are poorly determined by the data. This final practice introduces profile likelihood, a computationally intensive but far more robust method for assessing parameter identifiability and determining confidence regions. By systematically exploring the objective function landscape, you will gain a visually intuitive and quantitatively reliable understanding of which parameters are well-constrained by your experiment and which are not. ",
            "id": "3923524",
            "problem": "You are given a nonlinear regression problem arising from a steady-state Hill-type transcriptional repression model frequently used in synthetic biology modeling. The measured observable is the steady-state reporter intensity as a function of inducer concentration. The model is defined by the mapping from inducer concentration $u$ (with unit micromolar, denoted $\\mu\\mathrm{M}$) to observed intensity $y$ (in arbitrary fluorescence units) as\n$$\ny(u;p) \\;=\\; y_0 \\;+\\; \\frac{V}{1 + \\left(\\frac{u}{K}\\right)^{n}},\n$$\nwhere the parameter vector is $p = (y_0, V, K, n)$ with $y_0 \\ge 0$ (baseline), $V \\ge 0$ (dynamic range), $K > 0$ (half-repression concentration, in $\\mu\\mathrm{M}$), and $n > 0$ (Hill coefficient). Assume independent Gaussian measurement noise with known standard deviation $\\sigma$ for each data point. For given data $\\{(u_i, y_i)\\}_{i=1}^m$, the weighted nonlinear least-squares objective is\n$$\nS(p) \\;=\\; \\sum_{i=1}^{m} \\left(\\frac{y_i - y(u_i;p)}{\\sigma}\\right)^2.\n$$\n\nYou will implement a profile likelihood analysis with respect to the Hill coefficient $n$ using constrained nonlinear least squares. For a fixed value of $n$ (treated as a scalar), define the profiled objective\n$$\nS_{\\mathrm{prof}}(n) \\;=\\; \\min_{(y_0,V,K)} \\; S(y_0,V,K,n) \\quad \\text{subject to} \\quad y_0 \\ge 0,\\; V \\ge 0,\\; K \\in [10^{-3},10^{3}],\n$$\nand trace $S_{\\mathrm{prof}}(n)$ as $n$ varies over a grid. This procedure corresponds to the profile likelihood for $n$ under the Gaussian noise assumption and enables an identifiability assessment for $n$ based on the likelihood ratio principle.\n\nFundamental base and assumptions to use:\n- The Central Dogma of molecular biology motivates transcriptional regulation models, but for this problem only the mathematical form of the repression Hill function is needed as given above.\n- For independent Gaussian noise with known variance, minimizing the weighted sum of squares $S(p)$ is equivalent to maximizing the log-likelihood up to an additive constant.\n- The likelihood ratio test for a single scalar parameter at level $0.95$ uses the chi-square distribution with one degree of freedom, so the $0.95$ quantile is $\\chi^2_{1,0.95} \\approx 3.841458820694124$. The $95\\%$ confidence set for $n$ derived from the profile is $\\{ n: S_{\\mathrm{prof}}(n) - \\min_n S_{\\mathrm{prof}}(n) \\le \\chi^2_{1,0.95} \\}$.\n\nYour program must, for each specified test case below, carry out the following steps on a grid of $n$ values and produce specified scalar outcomes:\n1. Define a uniform grid $n \\in [0.5, 6.0]$ with spacing $\\Delta n = 0.05$.\n2. For each grid point $n$, compute $S_{\\mathrm{prof}}(n)$ by minimizing $S(y_0,V,K,n)$ over $(y_0,V,K)$ with bound constraints $y_0 \\in [0, 2000]$, $V \\in [0, 5000]$, and $K \\in [10^{-3}, 10^{3}]$. Use the same $\\sigma$ as specified per test case.\n3. Let $n^\\ast$ be the grid point attaining the global minimum of $S_{\\mathrm{prof}}(n)$.\n\nYou must implement the following three test cases with the given inducer concentrations and true parameters, generating noise-free observations $y_i$ exactly from the model:\n- Test Case A (broad dynamic range, informative):\n  - Concentrations (in $\\mu\\mathrm{M}$): $u = [0.1, 0.3, 1, 3, 10, 30, 100]$.\n  - True parameters: $y_0^{\\mathrm{true}} = 100$, $V^{\\mathrm{true}} = 900$, $K^{\\mathrm{true}} = 10$ (in $\\mu\\mathrm{M}$), $n^{\\mathrm{true}} = 2.5$.\n  - Noise standard deviation: $\\sigma = 30$ (fluorescence units).\n  - Required scalar result: the absolute error $|n^\\ast - n^{\\mathrm{true}}|$ as a floating-point number.\n\n- Test Case B (low-concentration regime, potentially unidentifiable for $n$):\n  - Concentrations (in $\\mu\\mathrm{M}$): $u = [0.1, 0.2, 0.3, 0.5, 0.8, 1.0, 1.5]$.\n  - True parameters: $y_0^{\\mathrm{true}} = 100$, $V^{\\mathrm{true}} = 900$, $K^{\\mathrm{true}} = 10$ (in $\\mu\\mathrm{M}$), $n^{\\mathrm{true}} = 2.5$.\n  - Noise standard deviation: $\\sigma = 30$ (fluorescence units).\n  - Identifiability assessment: Using the profile $S_{\\mathrm{prof}}(n)$ and the threshold $\\Delta S = \\chi^2_{1,0.95}$, define the $95\\%$ confidence set for $n$ as $\\{n : S_{\\mathrm{prof}}(n) - \\min S_{\\mathrm{prof}} \\le \\chi^2_{1,0.95} \\}$. Return the boolean value $\\mathrm{identifiable}$ which is $\\mathrm{True}$ if and only if this confidence set is a proper bounded interval strictly inside the grid $[0.5, 6.0]$, meaning there exist both a lower and an upper finite boundary within the grid, and $\\mathrm{False}$ otherwise.\n\n- Test Case C (high-concentration regime, boundary behavior check):\n  - Concentrations (in $\\mu\\mathrm{M}$): $u = [30, 50, 80, 120, 200, 300]$.\n  - True parameters: $y_0^{\\mathrm{true}} = 100$, $V^{\\mathrm{true}} = 900$, $K^{\\mathrm{true}} = 10$ (in $\\mu\\mathrm{M}$), $n^{\\mathrm{true}} = 2.5$.\n  - Noise standard deviation: $\\sigma = 30$ (fluorescence units).\n  - Required scalar result: the integer indicator $b$ defined as $b=1$ if $n^\\ast$ equals exactly the left boundary $0.5$ or the right boundary $6.0$ of the $n$ grid, and $b=0$ otherwise.\n\nImplementation details to adhere to:\n- For the inner minimizations at fixed $n$, solve the constrained nonlinear least-squares problem for $(y_0,V,K)$ using an algorithm appropriate for bound constraints.\n- Use the weighted residual form $(y_i - y(u_i;p))/\\sigma$ so that $S(p)$ equals the sum of squares of these residuals.\n- Use initial guesses derived from the data: $y_0^{(0)} = \\min_i y_i$, $V^{(0)} = \\max_i y_i - \\min_i y_i$, and $K^{(0)} = \\mathrm{median}(u_i)$.\n- Angle units do not apply. Physical units have been specified where appropriate. All outputs are unitless as specified below.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets, in the order $[\\text{result for A}, \\text{result for B}, \\text{result for C}]$.\n- The first element is a floating-point number $|n^\\ast - n^{\\mathrm{true}}|$. The second element is a boolean $\\mathrm{True}$ or $\\mathrm{False}$. The third element is an integer $0$ or $1$.\n- For example, a syntactically valid output would look like $[0.0123,True,0]$.",
            "solution": "The problem requires performing a profile likelihood analysis for the Hill coefficient, $n$, of a transcriptional repression model. This is a standard technique in systems biology for assessing parameter identifiability. The solution involves a numerical, grid-based approach.\n\nThe steady-state model for the observed fluorescence intensity, $y$, as a function of inducer concentration, $u$, is given by the Hill-type repression function:\n$$\ny(u;p) \\;=\\; y_0 \\;+\\; \\frac{V}{1 + \\left(\\frac{u}{K}\\right)^{n}}\n$$\nwhere the parameter vector is $p = (y_0, V, K, n)$. The parameters represent the baseline fluorescence ($y_0$), the dynamic range of repression ($V$), the half-repression concentration ($K$), and the Hill coefficient ($n$), which quantifies the steepness of the response.\n\nWe are tasked with fitting this model to data. Assuming independent and identically distributed Gaussian noise on the measurements with a known standard deviation $\\sigma$, the maximum likelihood estimate of the parameters is found by minimizing the weighted sum of squared residuals (SSR), which defines the objective function $S(p)$:\n$$\nS(p) \\;=\\; \\sum_{i=1}^{m} \\left(\\frac{y_i - y(u_i;p)}{\\sigma}\\right)^2\n$$\nwhere $\\{(u_i, y_i)\\}_{i=1}^m$ are the $m$ data points.\n\nThe core of the problem is the profile likelihood analysis for the parameter $n$. The profile likelihood is a function of a single parameter of interest, obtained by optimizing over all other (nuisance) parameters. For a fixed value of $n$, we define the profiled objective function $S_{\\mathrm{prof}}(n)$ as the minimum possible value of $S(p)$, where the minimization is performed over the nuisance parameters $(y_0, V, K)$:\n$$\nS_{\\mathrm{prof}}(n) \\;=\\; \\min_{(y_0,V,K)} \\; S(y_0,V,K,n)\n$$\nThis minimization is subject to the box constraints $y_0 \\in [0, 2000]$, $V \\in [0, 5000]$, and $K \\in [10^{-3}, 10^{3}]$.\n\nThe algorithm proceeds as follows:\n1. A uniform grid of values for the Hill coefficient $n$ is defined in the interval $[0.5, 6.0]$ with a step size of $\\Delta n = 0.05$.\n2. For each value of $n$ on this grid, the inner optimization problem to find $S_{\\mathrm{prof}}(n)$ is solved. This is a constrained nonlinear least-squares problem for the parameters $(y_0, V, K)$. We employ the Trust Region Reflective (`trf`) algorithm, as implemented in `scipy.optimize.least_squares`, which is suitable for bound-constrained problems. The optimization is initialized with guesses for $(y_0, V, K)$ derived from the data: $y_0^{(0)} = \\min_i y_i$, $V^{(0)} = \\max_i y_i - \\min_i y_i$, and $K^{(0)} = \\mathrm{median}(u_i)$.\n3. The values of $S_{\\mathrm{prof}}(n)$ are tabulated for all $n$ on the grid. The shape of this profile provides information about the identifiability of $n$. The grid point $n^\\ast$ that yields the minimum value of the profile, $S_{min} = \\min_n S_{\\mathrm{prof}}(n)$, is the maximum likelihood estimate of $n$ (restricted to the grid).\n\nThe analysis is performed for three test cases using noise-free data generated from the model with specified true parameters.\n- For Test Case A, the required output is the absolute error $|n^\\ast - n^{\\mathrm{true}}|$. Since the data is noise-free and covers the full dynamic range of the response curve, the profile is expected to be sharp and the optimization well-behaved, leading to $n^\\ast$ being very close or equal to $n^{\\mathrm{true}}$.\n- For Test Case B, we assess identifiability. Based on the likelihood ratio test, the $95\\%$ confidence interval for $n$ consists of all values $n$ for which the profile does not exceed a certain threshold above its minimum: $\\{ n : S_{\\mathrm{prof}}(n) - S_{min} \\le \\chi^2_{1,0.95} \\}$, where $\\chi^2_{1,0.95} \\approx 3.841458820694124$ is the $95\\%$ quantile of the chi-square distribution with one degree of freedom. The parameter $n$ is deemed identifiable if this confidence interval is finite and strictly contained within the analysis grid of $[0.5, 6.0]$. Data from a limited concentration range (low-concentration regime) is expected to make $n$ poorly identifiable, resulting in a wide confidence interval that extends to the grid boundaries.\n- For Test Case C, data is sampled only in the high-concentration, saturating part of the curve. This provides little information for estimating the parameters, which can lead to a flat or distorted profile. The task is to determine if the resulting estimate $n^\\ast$ falls on the boundary of the search grid, which is an indicator of such poor identifiability. The required output is an integer flag.\n\nThe implementation encapsulates this logic, iterating through the test cases and applying the specified analysis to the computed profile for each.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import least_squares\n\ndef solve():\n    \"\"\"\n    Solves the profile likelihood analysis problem for three test cases of a \n    Hill-type repression model.\n    \"\"\"\n    # Global constants as specified in the problem\n    CHI_SQUARED_VAL = 3.841458820694124\n    \n    def model_func(p, u):\n        \"\"\"\n        Hill-type repression model.\n        p: parameter vector (y0, V, K, n)\n        u: inducer concentration array\n        \"\"\"\n        y0, V, K, n = p\n        # Bounds on K in the optimizer prevent it from being zero.\n        term = u / K\n        return y0 + V / (1 + term**n)\n\n    def run_profile_analysis(u_data, p_true, sigma):\n        \"\"\"\n        Performs the profile likelihood analysis for a given test case.\n        It grids over n, and for each n, optimizes the other parameters.\n        \"\"\"\n        # Generate noise-free measurement data from the true model\n        y_data = model_func(p_true, u_data)\n\n        # 1. Define a uniform grid for the parameter n\n        n_grid = np.arange(0.5, 6.0 + 0.05 / 2, 0.05)\n\n        # Use initial guesses derived from the data for the inner optimization\n        y0_guess = np.min(y_data)\n        V_guess = np.max(y_data) - np.min(y_data)\n        K_guess = np.median(u_data)\n        p0_inner = [y0_guess, V_guess, K_guess]\n        \n        # Bounds for the inner optimization of (y0, V, K)\n        bounds_inner = ([0, 0, 1e-3], [2000, 5000, 1e3])\n        \n        s_prof_values = []\n\n        # Residuals function for scipy.optimize.least_squares\n        def residuals(p_inner, n_fixed, u, y, sigma_val):\n            y0, V, K = p_inner\n            p_full = (y0, V, K, n_fixed)\n            y_model = model_func(p_full, u)\n            return (y - y_model) / sigma_val\n\n        # 2. For each grid point n, compute S_prof(n) by minimizing over (y0, V, K)\n        for n_val in n_grid:\n            result = least_squares(\n                fun=residuals,\n                x0=p0_inner,\n                args=(n_val, u_data, y_data, sigma),\n                bounds=bounds_inner,\n                method='trf',\n                ftol=1e-9, gtol=1e-9, xtol=1e-9 # Stricter tolerances for accuracy\n            )\n            # S(p) = sum of squared residuals. result.cost is 0.5 * sum of squares.\n            s_prof_values.append(2 * result.cost)\n\n        s_prof_values = np.array(s_prof_values)\n\n        # 3. Find n* that attains the global minimum of S_prof(n) on the grid\n        s_min = np.min(s_prof_values)\n        min_idx = np.argmin(s_prof_values)\n        n_star = n_grid[min_idx]\n\n        return n_grid, s_prof_values, s_min, n_star\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            'u': np.array([0.1, 0.3, 1, 3, 10, 30, 100]),\n            'p_true': (100, 900, 10, 2.5),\n            'sigma': 30,\n            'type': 'A'\n        },\n        {\n            'u': np.array([0.1, 0.2, 0.3, 0.5, 0.8, 1.0, 1.5]),\n            'p_true': (100, 900, 10, 2.5),\n            'sigma': 30,\n            'type': 'B'\n        },\n        {\n            'u': np.array([30, 50, 80, 120, 200, 300]),\n            'p_true': (100, 900, 10, 2.5),\n            'sigma': 30,\n            'type': 'C'\n        }\n    ]\n    \n    results = []\n    \n    for case in test_cases:\n        n_grid, s_prof, s_min, n_star = run_profile_analysis(case['u'], case['p_true'], case['sigma'])\n        \n        if case['type'] == 'A':\n            n_true = case['p_true'][3]\n            result = abs(n_star - n_true)\n            results.append(result)\n            \n        elif case['type'] == 'B':\n            conf_set_mask = (s_prof - s_min) = CHI_SQUARED_VAL\n            conf_set_indices = np.where(conf_set_mask)[0]\n            \n            is_identifiable = False\n            # Check if confidence set is non-empty and strictly within grid boundaries\n            if len(conf_set_indices) > 0:\n                conf_set_lower_n = n_grid[conf_set_indices[0]]\n                conf_set_upper_n = n_grid[conf_set_indices[-1]]\n                \n                # 'strictly inside' means lower bound > grid minimum and upper bound  grid maximum\n                if conf_set_lower_n > n_grid[0] and conf_set_upper_n  n_grid[-1]:\n                    is_identifiable = True\n            \n            results.append(is_identifiable)\n            \n        elif case['type'] == 'C':\n            # Check if n* is exactly at a boundary. Direct comparison is fine as n_star\n            # is selected from the n_grid array.\n            is_at_boundary = (n_star == n_grid[0]) or (n_star == n_grid[-1])\n            result = 1 if is_at_boundary else 0\n            results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{results[0]},{results[1]},{results[2]}]\")\n\nsolve()\n\n```"
        }
    ]
}