## 引言
在[定量生物学](@entry_id:261097)尤其是合成生物学的研究中，建立能够准确描述和预测生物系统行为的数学模型至关重要。然而，这些模型——无论是描述基因调控网络的[常微分方程](@entry_id:147024)，还是描绘酶促[反应动力学](@entry_id:150220)的代数方程——通常包含大量无法直接测量的未知参数，如[反应速率](@entry_id:185114)、[结合亲和力](@entry_id:261722)或降解率。若无法确定这些参数，模型便仅仅是理论框架，缺乏预测能力。因此，如何利用有限且充满噪声的实验数据来系统地推断这些未知参数，成为了连接理论与实践的核心挑战。[非线性最小二乘法](@entry_id:167989)（Nonlinear Least Squares, NLS）正是应对这一挑战的最强大、最普遍的计算框架。

本文旨在全面而深入地探讨[非线性最小二乘法](@entry_id:167989)在生物模型校准中的理论与实践。我们将不仅限于算法的数学描述，更致力于揭示其在解决实际科学问题时所面临的复杂性与应对策略。通过学习本文，您将能够：

在“原理与机制”一章中，我们将从统计学的最大似然估计出发，揭示[非线性最小二乘法](@entry_id:167989)的理论根源，并深入剖析[高斯-牛顿法](@entry_id:173233)、[Levenberg-Marquardt算法](@entry_id:172092)等核心优化技术背后的几何直觉与数学机制。

接下来，在“应用与跨学科联系”一章中，我们将展示NLS如何作为一座桥梁，连接合成生物学、药理学、生物物理学乃至机器学习等多个领域的理论模型与实验数据。您将看到如何根据数据的统计特性调整[目标函数](@entry_id:267263)，以及如何通过正则化等方法应对参数[不可辨识性](@entry_id:1128800)等棘手问题。

最后，在“动手实践”部分，我们提供了一系列精心设计的编程练习，引导您从手动计算简单迭代到利用轮廓似然分析复杂模型，将理论知识转化为解决实际问题的能力。

## 原理与机制

在合成生物学中，我们构建的数学模型（例如，描述[基因表达动力学](@entry_id:1125581)的[常微分方程组](@entry_id:907499)）通常包含一些无法直接测量的未知参数，如[反应速率常数](@entry_id:187887)、降解率或调控相互作用的强度。为了使这些模型能够准确地描述和预测实验数据，我们必须通过一个称为[参数估计](@entry_id:139349)或[模型校准](@entry_id:146456)的过程来确定这些参数的值。[非线性最小二乘法](@entry_id:167989)（Nonlinear Least Squares, NLS）是这一过程中最核心、最广泛使用的一类优化技术。本章将深入探讨[非线性最小二乘法](@entry_id:167989)的基本原理、核心算法机制及其在合成生物学模型拟合中所面临的实际挑战。

### [非线性](@entry_id:637147)[最小二乘问题](@entry_id:164198)的构建

将模型与实验数据对齐的核心思想是量化模型预测与实际观测之间“不匹配”的程度，并系统地调整模型参数以最小化这种不匹配。

#### 从[最大似然估计](@entry_id:142509)到最小二乘

假设我们有一个[非线性模型](@entry_id:276864) $f(t, p)$，它根据参数向量 $p \in \mathbb{R}^m$ 预测在时间 $t$ 的[可观测量](@entry_id:267133)。我们进行了一系列实验，在不同的时间点 $t_i$（$i=1, \dots, n$）获得了观测值 $y_i^{\text{obs}}$。一个关键的步骤是建立一个统计模型来描述观测值与模型预测之间的关系。最常见和最基础的假设是，测量误差是加性的、独立的、并且服从均值为零、方差恒定的高斯分布。这意味着每个观测值可以表示为：

$y_i^{\text{obs}} = f(t_i, p) + \varepsilon_i$

其中，噪声项 $\varepsilon_i$ 是[独立同分布](@entry_id:169067)（i.i.d.）的[随机变量](@entry_id:195330)，遵循正态分布 $\mathcal{N}(0, \sigma^2)$。

根据这个假设，在给定参数 $p$ 的情况下，观测到特定值 $y_i^{\text{obs}}$ 的[概率密度](@entry_id:175496)由高斯分布的概率密度函数（PDF）给出：

$P(y_i^{\text{obs}} | p, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(y_i^{\text{obs}} - f(t_i,p))^2}{2\sigma^2} \right)$

由于每次测量是独立的，观测到整个数据集 $\{y_i^{\text{obs}}\}_{i=1}^n$ 的联合概率（即[似然函数](@entry_id:921601) $L(p)$）是各点概率的乘积：

$L(p) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(y_i^{\text{obs}} - f(t_i,p))^2}{2\sigma^2} \right)$

在[最大似然估计](@entry_id:142509)（Maximum Likelihood Estimation, MLE）框架下，我们的目标是找到使观测数据出现的可能性最大的参数值 $\hat{p}$。最大化 $L(p)$ 等价于最大化其对数，即[对数似然函数](@entry_id:168593) $\ell(p) = \ln L(p)$，因为对数函数是单调递增的。

$\ell(p) = -\frac{n}{2}\ln(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i^{\text{obs}} - f(t_i,p))^2$

为了找到最大化 $\ell(p)$ 的 $p$，我们注意到第一项 $-\frac{n}{2}\ln(2\pi\sigma^2)$ 和乘数因子 $\frac{1}{2\sigma^2}$ 都不依赖于 $p$（或者是一个不影响最小化器位置的正常数）。因此，最大化 $\ell(p)$ 等价于最小化以下目标函数：

$S(p) = \sum_{i=1}^n (y_i^{\text{obs}} - f(t_i,p))^2$

这个函数被称为**[残差平方和](@entry_id:174395)**（Sum of Squared Residuals）。在[非线性](@entry_id:637147)最小二乘的规范形式中，我们通常定义**残差**（residual）为观测值与模型预测值之差，$r_i(p) = y_i^{\text{obs}} - f(t_i,p)$，并将目标函数写为：

$S(p) = \frac{1}{2} \sum_{i=1}^n r_i(p)^2$

这里的因子 $\frac{1}{2}$ 是一个数学惯例，它能简化后续梯度和Hessian矩阵的表达式，但不会改变最优参数 $p$ 的位置。这个推导揭示了一个深刻的联系：在[加性高斯白噪声](@entry_id:269320)的假设下，[最小二乘法](@entry_id:137100)等价于[最大似然估计](@entry_id:142509) 。如果噪声特性不同，例如，噪声标准差与信号强度成正比（相对误差恒定），或者噪声服从不同的分布（如[拉普拉斯分布](@entry_id:266437)），那么最大似然原理将导出不同的目标函数，例如加权最小二乘或最小绝对值偏差（[L1范数](@entry_id:143036)）。

#### 线性与[非线性](@entry_id:637147)之别

[最小二乘问题](@entry_id:164198)的性质——尤其是其解的难易程度——关键取决于模型 $f(t,p)$ 如何依赖于参数 $p$。

- **线性最小二乘 (Linear Least Squares)**：如果模型 $f(t,p)$ 是参数 $p$ 的线性函数，即 $f(t,p) = \sum_{j=1}^m p_j \phi_j(t)$，其中 $\phi_j(t)$ 是仅依赖于自变量 $t$ 的（可能[非线性](@entry_id:637147)的）基函数，那么该问题是线性的。例如，一个模型 $f(u; \theta) = \theta_0 + \theta_1 \frac{u}{K+u}$，如果 $K$ 是一个已知的常数，而待估参数是 $\theta = (\theta_0, \theta_1)$，则这是一个线性[最小二乘问题](@entry_id:164198)，因为模型是 $\theta_0$ 和 $\theta_1$ 的线性组合。在这种情况下，目标函数 $S(p)$ 是一个凸的二次函数，它只有一个[全局最小值](@entry_id:165977)，并且这个最小值可以通过求解一个线性方程组（[正规方程](@entry_id:142238)）来解析地找到。

- **[非线性](@entry_id:637147)最小二乘 (Nonlinear Least Squares)**：如果模型 $f(t,p)$ 不是所有参数 $p$ 的线性函数，问题就变为[非线性](@entry_id:637147)的。例如，在上述模型中，如果 $K$ 也是一个待估参数，即 $\theta = (\beta, \alpha, K, n)$ 对于模型 $f(u; \theta) = \beta + \alpha \frac{u^n}{K^n + u^n}$，则模型对于 $K$ 和 $n$ 显然是[非线性](@entry_id:637147)的 。此时，[目标函数](@entry_id:267263) $S(p)$ 通常是**非凸**的。非凸的优化“地形”可能包含多个局部最小值、平坦区域和鞍点，这使得寻找全局最优解变得极具挑战性。局部[优化算法](@entry_id:147840)（如我们接下来将讨论的）从一个初始猜测点开始，只能保证收敛到某个局部最小值，而这个最小值是否是全局最优的则取决于初始点的选择。

### NLS 优化问题的几何学：[最优性条件](@entry_id:634091)

为了系统地寻找使 $S(p)$ 最小的参数 $p$，我们需要理解优化地形的局部几何特征，这通过[目标函数](@entry_id:267263)的梯度和Hessian矩阵来描述。

#### [一阶最优性条件](@entry_id:634945)：梯度

目标函数 $S(p) = \frac{1}{2} r(p)^T r(p)$ 的梯度 $\nabla S(p)$ 是一个向量，其每个分量是对相应参数的偏导数。根据[链式法则](@entry_id:190743)：

$\nabla S(p) = \frac{\partial}{\partial p} \left( \frac{1}{2} \sum_i r_i(p)^2 \right) = \sum_i r_i(p) \frac{\partial r_i(p)}{\partial p}$

如果我们定义**[雅可比矩阵](@entry_id:178326)** (Jacobian matrix) $J(p)$ 为[残差向量](@entry_id:165091) $r(p)$ 关于参数向量 $p$ 的导数，其元素为 $J_{ij} = \frac{\partial r_i}{\partial p_j}$，那么梯度可以紧凑地写为：

$\nabla S(p) = J(p)^T r(p)$

在无约束优化问题中，一个点 $p^*$ 是局部最小值的**[一阶必要条件](@entry_id:170730)**是梯度为零，即 $\nabla S(p^*) = 0$。这个条件 $J(p^*)^T r(p^*) = 0$ 有一个重要的几何解释：它意味着在最优点，[残差向量](@entry_id:165091) $r(p^*)$ 必须与[雅可比矩阵](@entry_id:178326) $J(p^*)$ 的所有列向量正交。换句话说，[残差向量](@entry_id:165091)位于[雅可比矩阵](@entry_id:178326)[列空间](@entry_id:156444)的**[正交补](@entry_id:149922)**中 。这在直观上是有意义的：如果在某个参数方向上（[雅可比矩阵](@entry_id:178326)的某一列）[残差向量](@entry_id:165091)仍有投影，那么沿着该方向微调参数就能进一步减小[残差平方和](@entry_id:174395)，这说明当前点还不是最小值。

#### 二阶[最优性条件](@entry_id:634091)：Hessian矩阵

梯度为零只表明我们处在一个平坦点（可能是最小值、最大值或鞍点）。为了区分它们，我们需要考察目标[函数的曲率](@entry_id:173664)，这由**Hessian矩阵** $\nabla^2 S(p)$（二阶导数矩阵）描述。对梯度表达式再次求导可得：

$\nabla^2 S(p) = J(p)^T J(p) + \sum_{i=1}^n r_i(p) \nabla^2 r_i(p)$

其中 $\nabla^2 r_i(p)$ 是第 $i$ 个残差函数自身的Hessian矩阵。

一个点 $p^*$ 是局部最小值的**[二阶必要条件](@entry_id:637764)**是，除了梯度为零外，其Hessian矩阵必须是**半正定的**（positive semidefinite），记作 $\nabla^2 S(p^*) \succeq 0$。这意味着在任何方向上，成本[函数的曲率](@entry_id:173664)都是非负的。如果Hessian矩阵是**正定的**（positive definite），即 $\nabla^2 S(p^*) \succ 0$，则这是一个**[二阶充分条件](@entry_id:635498)**，保证 $p^*$ 是一个严格的局部最小值。注意，半正定是必要条件，而非正定 。

### [非线性](@entry_id:637147)最小二乘的核心算法

由于 NLS 问题通常没有解析解，我们必须使用[迭代算法](@entry_id:160288)来求解。这些算法从一个初始猜测 $p_0$ 开始，生成一个参数序列 $p_1, p_2, \dots$，希望它能收敛到一个局部最小值。

#### [高斯-牛顿法](@entry_id:173233) (Gauss-Newton Method)

[牛顿法](@entry_id:140116)是求解一般优化问题的强大工具，它在每一步通过求解 $\nabla^2 S(p_k) \delta p = -\nabla S(p_k)$ 来计算更新步长 $\delta p$。然而，计算 NLS 问题的完整Hessian矩阵代价高昂，因为它需要计算每个残差函数的二阶导数。

[高斯-牛顿法](@entry_id:173233)通过对Hessian矩阵做一个巧妙的近似来简化计算。它直接忽略了Hessian表达式中的第二项：

$\nabla^2 S(p) \approx H_{GN}(p) = J(p)^T J(p)$

这个近似的合理性基于两个关键场景 ：
1.  **小残差问题**：如果模型能够很好地拟[合数](@entry_id:263553)据，那么在最优解附近，残差 $r_i(p)$ 的值会非常小。因此，求和项 $\sum r_i(p) \nabla^2 r_i(p)$ 也会很小，可以忽略。如果数据可以被模型完美拟合（即 $r_i(p^*) = 0$），则在最优点 $p^*$ 处，这个近似是精确的。
2.  **近似线性问题**：如果残差函数 $r_i(p)$ 相对于参数 $p$ 近似是线性的，那么它们的二阶导数 $\nabla^2 r_i(p)$ 本身就很小，即使残差很大，求和项也可能很小。

使用这个近似的Hessian，[高斯-牛顿法](@entry_id:173233)的更新步骤通过求解一个线性[最小二乘问题](@entry_id:164198)得到：

$(J(p_k)^T J(p_k)) \delta p = -J(p_k)^T r(p_k)$

[高斯-牛顿法](@entry_id:173233)在某些条件下[收敛速度](@entry_id:636873)很快，但它有一个潜在的弱点：近似Hessian $J^T J$ 总是半正定的。如果[雅可比矩阵](@entry_id:178326) $J$ 是“病态的”或列[秩亏](@entry_id:754065)的，$J^T J$ 可能会是奇[异或](@entry_id:172120)接近奇异的，导致计算出的步长 $\delta p$ 巨大且不稳定。

#### 鲁棒优化方法：信赖域与Levenberg-Marquardt

为了克服[高斯-牛顿法](@entry_id:173233)的不稳定性，研究者们发展了更鲁棒的算法，其核心思想是控制每一步的更新大小和方向。

**信赖域法 (Trust-Region Methods)** 的思想是，在当前点 $p_k$ 的一个小的邻域（“信赖域”）内，我们相信由一阶[泰勒展开](@entry_id:145057)导出的线性模型 $r(p_k + \delta p) \approx r_k + J_k \delta p$ 是对真实残差的良好近似。因此，我们在每一步求解一个带约束的子问题 ：

$\min_{\delta p} \quad \frac{1}{2} \|r_k + J_k \delta p\|^2 \quad \text{subject to} \quad \|\delta p\| \le \Delta_k$

这里的 **信赖域半径** $\Delta_k$ 至关重要。它定义了我们信任[线性模型](@entry_id:178302)的范围。如果计算出的步长 $\delta p$ 使得实际的[目标函数](@entry_id:267263)值下降显著（与二次模型预测的下降相符），说明线性近似在该区域是可靠的，我们可以扩大信赖域（增大 $\Delta_{k+1}$）以在下一步尝试更远的探索。反之，如果实际下降不佳甚至函数值上升，说明步子迈得太大，进入了[非线性](@entry_id:637147)效应显著的区域，此时必须缩小信赖域（减小 $\Delta_{k+1}$）以保证模型的可靠性。这种自适应调整 $\Delta_k$ 的机制使得信赖域法非常稳健。

**Levenberg-Marquardt (LM) 算法** 是另一种广泛使用的改进方法，可以被看作是信赖域法的一种变体。它通过在 $J^T J$ 上增加一个“阻尼项” $\mu I$ 来修正高斯-[牛顿步](@entry_id:177069)，求解如下方程：

$(J(p_k)^T J(p_k) + \mu_k I) \delta p = -J(p_k)^T r(p_k)$

阻尼参数 $\mu_k > 0$ 扮演着双重角色：
- 当 $\mu_k$ 很小时，LM算法接近于[高斯-牛顿法](@entry_id:173233)，利用了其快速的收敛性。
- 当 $\mu_k$ 很大时，矩阵 $(J^T J + \mu_k I)$ 由 $\mu_k I$ 主导，步长近似为 $\delta p \approx -\frac{1}{\mu_k} J^T r$，这正比于[最速下降](@entry_id:141858)方向（负梯度方向）。
LM算法通过自适应地调整 $\mu_k$ 来在高斯-[牛顿步](@entry_id:177069)和[最速下降](@entry_id:141858)步之间平滑地切换，兼顾了收敛速度和稳定性。

### 模型拟合中的实际挑战

将上述算法应用于合成生物学模型时，我们会遇到一系列深刻的实际挑战，这些挑战超越了纯粹的[数值优化](@entry_id:138060)。

#### 动态模型的梯度计算：[灵敏度分析](@entry_id:147555)

对于由[常微分方程](@entry_id:147024)（ODE）系统定义的模型，例如 $dx/dt = f(x, p, t)$，模型输出 $x(t_i; p)$ 没有封闭的解析表达式。那么，如何[计算优化](@entry_id:636888)所需的[雅可比矩阵](@entry_id:178326) $J$ 呢？直接使用有限差分法（即微扰每个参数并重新[求解ODE](@entry_id:145499)）虽然简单，但计算成本高且可能不准确。

一个更精确和高效的方法是**[灵敏度分析](@entry_id:147555) (Sensitivity Analysis)**。我们希望计算状态变量 $x$ 对参数 $p_j$ 的导数，即灵敏度 $S_j(t) = \frac{\partial x(t;p)}{\partial p_j}$。通过对原始[ODE系统](@entry_id:907499)关于 $p_j$ 求导，我们可以得到一个控制灵敏度 $S_j(t)$ 演化的线性ODE ：

$\frac{d S_j}{dt} = \frac{\partial f}{\partial x} S_j + \frac{\partial f}{\partial p_j}$

这个方程被称为**灵敏度方程**。它的初始条件是 $S_j(0) = \frac{\partial x(0; p)}{\partial p_j}$，如果初始状态不依赖于参数 $p_j$，则 $S_j(0) = 0$。通过将原始[ODE系统](@entry_id:907499)与所有参数的灵敏度方程联立，我们可以一次性数值求解得到状态轨迹 $x(t)$ 和所有灵敏度轨迹 $S_j(t)$。一旦获得了灵敏度 $S_j(t_i)$，就可以通过[链式法则](@entry_id:190743)轻松构建[雅可比矩阵](@entry_id:178326)的元素：

$J_{ij} = \frac{\partial r_i}{\partial p_j} = -\frac{\partial f(t_i, p)}{\partial p_j} = -\left[ \frac{\partial h}{\partial x} S_j(t_i) + \frac{\partial h}{\partial p_j} \right]$

其中 $h(x, p)$ 是将ODE状态映射到[可观测量](@entry_id:267133)的观测函数。

#### [参数可辨识性](@entry_id:197485)与模型“潦草性”

一个即便在数学上正确且能通过优化的模型，也可能无法从数据中唯一确定其参数。这个问题被称为**[可辨识性](@entry_id:194150)**（Identifiability）。

- **结构[可辨识性](@entry_id:194150) (Structural Identifiability)** 是一个理论上的模型属性。它问的是：假设我们拥有完美、无噪声、连续的数据，我们能否唯一地确定模型的参数？如果不同的参数组合能够产生完全相同的模型输出，那么模型是结构不可辨识的。例如，在一个简单的[转录-翻译](@entry_id:200282)模型中，转录速率 $k_{tx}$、翻译速率 $k_{tl}$ 和一个光学缩放因子 $s$ 可能只以乘积 $s \cdot k_{tl} \cdot k_{tx}$ 的形式出现在最终的可观测输出中。在这种情况下，我们只能确定这个乘积的值，而无法分开确定每个参数 。

- **[实际可辨识性](@entry_id:190721) (Practical Identifiability)** 是一个依赖于数据的概念。它问的是：给定我们有限的、带有噪声的实验数据，我们能否以足够高的精度确定参数？一个参数可能是结构可辨识的，但在实践中却不可辨识。例如，如果一个反应过程非常快，而我们的[采样频率](@entry_id:264884)太低，错过了这个快速动态过程，那么数据中将不包含约束该[反应速率](@entry_id:185114)参数的任何信息。这将导致该参数的估计具有极大的不确定性 。

许多复杂的系统生物学模型表现出一种称为**潦草性**（Sloppiness）的特性。这意味着模型的行为对参数的不同组合表现出极不均匀的敏感性。这在数学上体现为Fisher[信息矩阵](@entry_id:750640)（在NLS中常由 $J^T J$ 近似）的[特征值谱](@entry_id:1124216)跨越许多个数量级。
- 少数几个大的特征值对应于“刚性”（stiff）方向。沿着这些方向（由相应的[特征向量](@entry_id:151813)定义）改变参数，模型输出会发生巨大变化。数据对这些参数组合有很强的[约束力](@entry_id:170052)。
- 大量小的特征值对应于“潦草”（sloppy）方向。沿着这些方向改变参数，模型输出几乎不变。数据对这些参数组合的约束非常弱。

潦草性给优化带来了巨大挑战 。在潦草方向上，[目标函数](@entry_id:267263)的地形非常平坦。[高斯-牛顿法](@entry_id:173233)试图通过除以一个非常小的特征值来计算步长，这会导致巨大且不稳定的步长。这正是LM或信赖域算法的阻尼/约束机制发挥关键作用的地方：它们能有效抑制在潦草方向上的过大步长，从而稳定优化过程。然而，这也意味着在这些方向上的收敛可能会非常缓慢。值得注意的是，通过对数变换等方式重新[参数化](@entry_id:265163)模型可以改善优化的数值行为，但无法凭空创造数据中不存在的信息。解决根本的潦草性或[实际不可辨识性](@entry_id:270178)问题，最终需要通过改进[实验设计](@entry_id:142447)来获得更有[信息量](@entry_id:272315)的数据。

#### 应对非凸性：[多起点优化](@entry_id:637385)

由于NLS[目标函数](@entry_id:267263)的非[凸性](@entry_id:138568)，任何从单一初始点开始的局部[优化算法](@entry_id:147840)都有可能陷入局部最小值。为了增加找到全局最优解的概率，一种常用且有效的启发式策略是**[多起点优化](@entry_id:637385)**（Multi-start Optimization）。

该策略的执行步骤很简单 ：
1.  从一个预定义的[参数空间](@entry_id:178581)（通常是基于先验知识或物理可行性设定的边界框）中，通过[随机抽样](@entry_id:175193)生成 $M$ 个独立的初始参数向量 $\{\theta^{(0,1)}, \dots, \theta^{(0,M)}\}$。对于跨越多个数量级的参数（如[速率常数](@entry_id:140362)），在对数尺度上均匀采样（即从对数均匀分布中抽样）通常比在线性尺度[上采样](@entry_id:275608)更有效。
2.  从每个初始点 $\theta^{(0,m)}$ 开始，运行一个确定性的局部[优化算法](@entry_id:147840)，直到收敛到某个局部最小值 $\theta^{\infty, m}$。
3.  比较所有 $M$ 次运行找到的局部最小值对应的目标函数值 $S(\theta^{\infty, m})$，并选择其中最低的一个作为最终的最优解估计。

[多起点优化](@entry_id:637385)并不改变优化地形本身，而是通过广泛撒网来探索不同的**吸引盆**（basins of attraction）。每个局部最小值都有其对应的吸引盆，即所有能够收敛到该最小值的初始点集合。如果[全局最小值](@entry_id:165977)的[吸引盆](@entry_id:174948)在我们的[采样分布](@entry_id:269683)下具有非零概率 $p^*$，那么进行 $M$ 次独立尝试后，至少有一次成功进入该吸引盆的概率是 $1 - (1-p^*)^M$。这个概率随着尝试次数 $M$ 的增加而趋近于1，但永远无法保证（对于有限的 $M$）一定能找到[全局最优解](@entry_id:175747)。这是一种以计算换取全局性的实用策略，在处理复杂的合成生物学模型时几乎是必不可少的。