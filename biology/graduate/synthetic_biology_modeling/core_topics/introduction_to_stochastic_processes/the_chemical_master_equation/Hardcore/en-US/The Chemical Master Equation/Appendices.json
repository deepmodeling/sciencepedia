{
    "hands_on_practices": [
        {
            "introduction": "A powerful application of the Chemical Master Equation is the ability to derive the statistical moments of molecular counts without solving for the entire probability distribution. This first exercise provides a fundamental workout in this technique by calculating the variance for a simple birth-death process at steady state, a common model for gene expression. Mastering this calculation helps build intuition for how reaction kinetics shape the intrinsic noise of a system .",
            "id": "1517880",
            "problem": "Consider a simplified model for the stochastic production and degradation of a single type of protein, denoted by $X$, within a cell. Molecules of $X$ are produced one at a time through a process that can be modeled with a constant probability rate. The probability of a single production event occurring in an infinitesimal time interval $dt$ is given by $k_1 dt$, where $k_1$ is the production rate constant. Simultaneously, each existing molecule of $X$ has a constant probability of degrading in the same time interval $dt$. This probability is given by $k_2 dt$ for each molecule, where $k_2$ is the degradation rate constant. Therefore, if there are $n$ molecules of $X$ present, the total probability of one degradation event occurring in the interval $dt$ is $n k_2 dt$.\n\nAfter a sufficiently long time, the system reaches a statistical steady state, where the probability distribution of the number of molecules, $P(n)$, no longer changes with time. For this steady-state distribution, determine the variance, defined as $\\text{Var}(n) = \\langle n^2 \\rangle - \\langle n \\rangle^2$.\n\nExpress your answer as a closed-form analytic expression in terms of the constants $k_1$ and $k_2$.",
            "solution": "We model the system as a birth-death process with constant birth rate $k_{1}$ and per-molecule death rate $k_{2}$. Let $n$ be the number of molecules. For any function $f(n)$, the chemical master equation implies the evolution of its expectation:\n$$\n\\frac{d\\langle f(n)\\rangle}{dt}=\\left\\langle k_{1}\\left[f(n+1)-f(n)\\right]+n k_{2}\\left[f(n-1)-f(n)\\right]\\right\\rangle.\n$$\n\nFirst, choose $f(n)=n$. Using $(n+1)-n=1$ and $(n-1)-n=-1$, we obtain\n$$\n\\frac{d\\langle n\\rangle}{dt}=\\left\\langle k_{1}\\cdot 1+n k_{2}\\cdot(-1)\\right\\rangle=k_{1}-k_{2}\\langle n\\rangle.\n$$\nAt steady state, $d\\langle n\\rangle/dt=0$, so\n$$\n\\langle n\\rangle=\\frac{k_{1}}{k_{2}}.\n$$\n\nNext, choose $f(n)=n^{2}$. Using $(n+1)^{2}-n^{2}=2n+1$ and $(n-1)^{2}-n^{2}=-2n+1$, we get\n$$\n\\frac{d\\langle n^{2}\\rangle}{dt}=\\left\\langle k_{1}(2n+1)+n k_{2}(-2n+1)\\right\\rangle=2k_{1}\\langle n\\rangle+k_{1}-2k_{2}\\langle n^{2}\\rangle+k_{2}\\langle n\\rangle.\n$$\nRearranging,\n$$\n\\frac{d\\langle n^{2}\\rangle}{dt}=-2k_{2}\\langle n^{2}\\rangle+(2k_{1}+k_{2})\\langle n\\rangle+k_{1}.\n$$\nAt steady state, $d\\langle n^{2}\\rangle/dt=0$, so\n$$\n\\langle n^{2}\\rangle=\\frac{(2k_{1}+k_{2})\\langle n\\rangle+k_{1}}{2k_{2}}.\n$$\nSubstituting $\\langle n\\rangle=k_{1}/k_{2}$ gives\n$$\n\\langle n^{2}\\rangle=\\frac{(2k_{1}+k_{2})\\frac{k_{1}}{k_{2}}+k_{1}}{2k_{2}}=\\frac{2k_1^2/k_2 + k_1 + k_1}{2k_2} = \\frac{2k_1^2/k_2 + 2k_1}{2k_2} = \\frac{k_{1}^{2}}{k_{2}^{2}}+\\frac{k_{1}}{k_{2}}.\n$$\n\nTherefore, the variance is\n$$\n\\text{Var}(n)=\\langle n^{2}\\rangle-\\langle n\\rangle^{2}=\\left(\\frac{k_{1}^{2}}{k_{2}^{2}}+\\frac{k_{1}}{k_{2}}\\right)-\\left(\\frac{k_{1}}{k_{2}}\\right)^{2}=\\frac{k_{1}}{k_{2}}.\n$$",
            "answer": "$$\\boxed{\\frac{k_{1}}{k_{2}}}$$"
        },
        {
            "introduction": "While linear systems like the simple birth-death process yield solvable moment equations, many crucial biological reactions are non-linear. This practice explores a second-order dimerization reaction and reveals a fundamental challenge known as the moment closure problem, where the equation for each moment depends on a higher-order one. Working through this derivation  is essential for understanding why exact analytical solutions are often intractable for complex systems and why approximation methods are so important.",
            "id": "1471904",
            "problem": "Consider an irreversible dimerization reaction $2X \\to Y$ taking place in a well-mixed system of constant volume. The state of the system can be characterized by the number of molecules of species X, denoted by $n_X$. Due to the stochastic nature of chemical reactions at the molecular level, $n_X$ is a random variable. The probability per unit time of a single dimerization event occurring is given by the propensity function $a(n_X) = c \\frac{n_X(n_X-1)}{2}$, where $c$ is the stochastic rate constant. The time evolution of the probability distribution $P(n_X, t)$ is governed by the Chemical Master Equation.\n\nWe are interested in the statistical properties of the number of molecules of X. Let the mean number of molecules be $\\mu(t) = \\langle n_X \\rangle$, and define the $k$-th central moment as $\\mu_k(t) = \\langle (n_X - \\mu)^k \\rangle$. Note that the variance is $\\sigma^2 = \\mu_2$.\n\nDerive the system of ordinary differential equations for the time evolution of the mean, $\\frac{d\\mu}{dt}$, and the variance, $\\frac{d\\mu_2}{dt}$. Express your final answer in terms of the rate constant $c$, the mean $\\mu$, the variance $\\mu_2$, and the third central moment $\\mu_3$. Provide your two expressions as a row matrix, with the first entry being the expression for $\\frac{d\\mu}{dt}$ and the second being for $\\frac{d\\mu_2}{dt}$.",
            "solution": "Let $n \\equiv n_{X}$ and note that the single reaction $2X \\to Y$ changes $n$ by $v=-2$ when it fires. The propensity is $a(n)=c\\,\\frac{n(n-1)}{2}$. From the Chemical Master Equation (CME), for any function $f(n)$ the time evolution of its expectation is\n$$\n\\frac{d\\langle f(n)\\rangle}{dt}=\\left\\langle\\left[f(n+v)-f(n)\\right]a(n)\\right\\rangle.\n$$\nFor the mean, take $f(n)=n$. Then $f(n+v)-f(n)=(n-2)-n=-2$, so\n$$\n\\frac{d\\mu}{dt}=\\frac{d\\langle n\\rangle}{dt}=\\left\\langle(-2)\\,a(n)\\right\\rangle=-2\\left\\langle c\\,\\frac{n(n-1)}{2}\\right\\rangle=-c\\langle n(n-1)\\rangle.\n$$\nUsing $\\langle n(n-1)\\rangle=\\langle n^{2}\\rangle-\\langle n\\rangle=(\\mu_{2}+\\mu^{2})-\\mu$, we obtain\n$$\n\\frac{d\\mu}{dt}=-c\\left(\\mu_{2}+\\mu^{2}-\\mu\\right).\n$$\n\nFor the variance $\\mu_{2}=\\langle(n-\\mu)^{2}\\rangle=\\langle n^{2}\\rangle-\\mu^{2}$, differentiate to get\n$$\n\\frac{d\\mu_{2}}{dt}=\\frac{d\\langle n^{2}\\rangle}{dt}-2\\mu\\frac{d\\mu}{dt}.\n$$\nCompute $d\\langle n^{2}\\rangle/dt$ by taking $f(n)=n^{2}$, so $f(n+v)-f(n)=(n-2)^{2}-n^{2}=-4n+4$, hence\n$$\n\\frac{d\\langle n^{2}\\rangle}{dt}=\\left\\langle(-4n+4)\\,a(n)\\right\\rangle=\\left\\langle(-4n+4)\\,c\\,\\frac{n(n-1)}{2}\\right\\rangle\n=c\\left\\langle(-2n+2)\\,n(n-1)\\right\\rangle.\n$$\nExpanding $(-2n+2)\\,n(n-1)=(-2n+2)(n^{2}-n)=-2n^{3}+4n^{2}-2n$ gives\n$$\n\\frac{d\\langle n^{2}\\rangle}{dt}=c\\left[-2\\langle n^{3}\\rangle+4\\langle n^{2}\\rangle-2\\langle n\\rangle\\right].\n$$\nExpress raw moments in terms of central moments: $\\langle n\\rangle=\\mu$, $\\langle n^{2}\\rangle=\\mu_{2}+\\mu^{2}$, and $\\langle n^{3}\\rangle=\\mu_{3}+3\\mu\\mu_{2}+\\mu^{3}$. Substituting yields\n$$\n\\frac{d\\langle n^{2}\\rangle}{dt}=c\\left[-2(\\mu_{3}+3\\mu\\mu_{2}+\\mu^{3})+4(\\mu_{2}+\\mu^{2})-2\\mu\\right].\n$$\nUsing the mean equation $\\frac{d\\mu}{dt}=-c(\\mu_{2}+\\mu^{2}-\\mu)$, we obtain\n$$\n\\frac{d\\mu_{2}}{dt}=c\\left[-2\\mu_{3}-6\\mu\\mu_{2}-2\\mu^{3}+4\\mu_{2}+4\\mu^{2}-2\\mu\\right]+2c\\mu(\\mu_{2}+\\mu^{2}-\\mu).\n$$\nCollecting like terms gives\n$$\n\\frac{d\\mu_{2}}{dt}=c\\left[-2\\mu_{3}-4\\mu\\mu_{2}+4\\mu_{2}+2\\mu^{2}-2\\mu\\right]\n=2c\\left[-\\mu_{3}-2\\mu\\mu_{2}+2\\mu_{2}+\\mu^{2}-\\mu\\right].\n$$\n\nTherefore, the required system expressed in terms of $c$, $\\mu$, $\\mu_{2}$, and $\\mu_{3}$ is\n$$\n\\frac{d\\mu}{dt}=-c\\left(\\mu_{2}+\\mu^{2}-\\mu\\right),\\qquad\n\\frac{d\\mu_{2}}{dt}=c\\left(-2\\mu_{3}-4\\mu\\mu_{2}+4\\mu_{2}+2\\mu^{2}-2\\mu\\right).\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}-c\\left(\\mu_{2}+\\mu^{2}-\\mu\\right) & c\\left(-2\\mu_{3}-4\\mu\\mu_{2}+4\\mu_{2}+2\\mu^{2}-2\\mu\\right)\\end{pmatrix}}$$"
        },
        {
            "introduction": "Bridging theory with computation is a cornerstone of modern systems biology. This advanced practice guides you through validating the analytical predictions of the Chemical Master Equation against numerical experiments using the Gillespie stochastic simulation algorithm (SSA). By deriving the exact time-dependent mean and variance and comparing them to simulation results , you will gain hands-on experience with the complete workflow of modeling, solving, and simulating stochastic biochemical systems.",
            "id": "4392582",
            "problem": "Consider a single-species gene expression birth–death process modeled as a continuous-time, discrete-state Markov jump process under the Chemical Master Equation (CME). Let $X(t) \\in \\mathbb{N}_0$ denote the molecule copy number at time $t$. The system has two reaction channels: synthesis (birth) with reaction $\\emptyset \\to X$ and constant propensity $a_1(n) = k_b$, and degradation (death) with reaction $X \\to \\emptyset$ and propensity $a_2(n) = k_d n$. The initial copy number is $X(0) = x_0$. Time is measured in seconds, rates in $\\mathrm{s^{-1}}$, and copy numbers are unitless counts.\n\nStarting only from the CME and the definition of expectation and variance, you must:\n1. Implement Gillespie’s direct stochastic simulation algorithm for the above birth–death process to generate trajectories $X(t)$ until a fixed terminal time $T$.\n2. Simulate $M$ independent trajectories and record the ensemble molecule counts at time $T$, $\\{X_i(T)\\}_{i=1}^M$.\n3. Compute the empirical mean $\\hat{m}(T)$ and empirical variance $\\hat{v}(T)$ of $\\{X_i(T)\\}_{i=1}^M$.\n4. Independently derive, from the CME, closed-form expressions for the first and second moments at time $T$, namely the exact mean $m(T)$ and variance $v(T)$, for arbitrary nonnegative parameters $k_b$, $k_d$, $x_0$, and $T$.\n5. Verify consistency between simulation and CME-derived moments by checking whether both the absolute relative error in the mean and the absolute relative error in the variance are each less than or equal to a fixed tolerance $\\varepsilon$, that is, whether\n$$\n\\frac{\\lvert \\hat{m}(T) - m(T) \\rvert}{\\max(m(T), 10^{-12})} \\le \\varepsilon\n\\quad \\text{and} \\quad\n\\frac{\\lvert \\hat{v}(T) - v(T) \\rvert}{\\max(v(T), 10^{-12})} \\le \\varepsilon.\n$$\n\nUse Gillespie’s direct method with exact exponential waiting times and reaction-channel selection based on propensities. Use a fixed pseudorandom seed equal to $12345$ for reproducibility. Use $M = 6000$ trajectories per test case and $\\varepsilon = 0.05$.\n\nImplement your program to handle the following test suite of parameter sets, covering typical behavior and boundary cases. For each case, the synthesis rate is $k_b$ in $\\mathrm{s^{-1}}$, the degradation rate is $k_d$ in $\\mathrm{s^{-1}}$, the initial count is $x_0$ in molecules, and the terminal time is $T$ in seconds:\n- Case 1 (typical): $k_b = 5.0$, $k_d = 1.0$, $x_0 = 0$, $T = 3.0$.\n- Case 2 (no synthesis): $k_b = 0.0$, $k_d = 0.7$, $x_0 = 50$, $T = 1.5$.\n- Case 3 (slow degradation, long time): $k_b = 2.0$, $k_d = 0.1$, $x_0 = 0$, $T = 20.0$.\n- Case 4 (fast degradation, moderate synthesis): $k_b = 9.0$, $k_d = 3.0$, $x_0 = 0$, $T = 1.0$.\n- Case 5 (nonzero initial condition): $k_b = 4.0$, $k_d = 0.5$, $x_0 = 20$, $T = 2.0$.\n- Case 6 (no degradation): $k_b = 1.5$, $k_d = 0.0$, $x_0 = 3$, $T = 4.0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is a boolean indicating whether the corresponding case passes the tolerance check for both moments, in the order listed above (for example, “[True,False,True,True,True,False]”).",
            "solution": "The problem requires the derivation of the time-dependent mean and variance for a single-species birth-death process from the Chemical Master Equation (CME), and the verification of these analytical results against stochastic simulations using Gillespie's direct method. The problem is well-posed, scientifically grounded, and provides all necessary parameters for a complete solution.\n\n### 1. Derivation of Exact Moments from the Chemical Master Equation\n\nThe system consists of two reactions for a species $X$ with copy number $n$:\n1.  Synthesis (birth): $\\emptyset \\xrightarrow{k_b} X$, with propensity $a_1(n) = k_b$.\n2.  Degradation (death): $X \\xrightarrow{k_d n} \\emptyset$, with propensity $a_2(n) = k_d n$.\n\nLet $P(n,t)$ be the probability that the system has $n$ molecules at time $t$. The CME, which governs the time evolution of $P(n,t)$, is:\n$$\n\\frac{dP(n,t)}{dt} = k_b P(n-1,t) + k_d(n+1)P(n+1,t) - (k_b + k_d n)P(n,t)\n$$\nfor $n \\ge 1$, with a special form for $n=0$: $\\frac{dP(0,t)}{dt} = k_d P(1,t) - k_b P(0,t)$. The general form holds for all $n \\in \\mathbb{N}_0$ if we define $P(-1, t)=0$.\n\n#### 1.1. Mean Molecule Number\n\nThe mean (or first moment) of the molecule count is defined as $m(t) = \\langle n \\rangle(t) = \\sum_{n=0}^{\\infty} n P(n,t)$. The time evolution of the mean is found by taking the time derivative and using the CME:\n$$\n\\frac{dm(t)}{dt} = \\sum_{n=0}^{\\infty} n \\frac{dP(n,t)}{dt}\n$$\nSubstituting the CME and evaluating the sum term by term:\n$$\n\\frac{dm(t)}{dt} = \\sum_{n=0}^{\\infty} n \\left[ k_b P(n-1,t) + k_d(n+1)P(n+1,t) - (k_b + k_d n)P(n,t) \\right]\n$$\n$$\n\\frac{dm(t)}{dt} = k_b \\sum_{n=1}^{\\infty} n P(n-1,t) + k_d \\sum_{n=0}^{\\infty} n(n+1)P(n+1,t) - k_b \\sum_{n=0}^{\\infty} n P(n,t) - k_d \\sum_{n=0}^{\\infty} n^2 P(n,t)\n$$\nBy re-indexing the sums (e.g., letting $j=n-1$ in the first sum), we get:\n$$\n\\frac{dm(t)}{dt} = k_b \\sum_{j=0}^{\\infty} (j+1) P(j,t) + k_d \\sum_{j=1}^{\\infty} (j-1)j P(j,t) - k_b \\langle n \\rangle - k_d \\langle n^2 \\rangle\n$$\n$$\n\\frac{dm(t)}{dt} = k_b (\\langle n \\rangle + 1) + k_d (\\langle n^2 \\rangle - \\langle n \\rangle) - k_b \\langle n \\rangle - k_d \\langle n^2 \\rangle\n$$\n$$\n\\frac{dm(t)}{dt} = k_b \\langle n \\rangle + k_b + k_d \\langle n^2 \\rangle - k_d \\langle n \\rangle - k_b \\langle n \\rangle - k_d \\langle n^2 \\rangle\n$$\nThis simplifies to a first-order linear ordinary differential equation (ODE) for the mean $m(t)$:\n$$\n\\frac{dm(t)}{dt} = k_b - k_d m(t)\n$$\nGiven the initial condition $m(0) = X(0) = x_0$, we can solve this ODE.\n- If $k_d > 0$: The solution is\n  $$\n  m(t) = \\frac{k_b}{k_d} + \\left(x_0 - \\frac{k_b}{k_d}\\right) e^{-k_d t}\n  $$\n- If $k_d = 0$: The ODE becomes $\\frac{dm(t)}{dt} = k_b$, which integrates to\n  $$\n  m(t) = x_0 + k_b t\n  $$\n\n#### 1.2. Variance of Molecule Number\n\nThe variance is $v(t) = \\langle n^2 \\rangle - \\langle n \\rangle^2$. We first derive an ODE for the variance. The time derivative of the variance is $\\frac{dv}{dt} = \\frac{d\\langle n^2 \\rangle}{dt} - 2\\langle n \\rangle \\frac{d\\langle n \\rangle}{dt}$.\nWe need the ODE for the second moment, $\\langle n^2 \\rangle = \\sum_{n=0}^{\\infty} n^2 P(n,t)$. Similarly to the mean, we find:\n$$\n\\frac{d\\langle n^2 \\rangle}{dt} = \\sum_{n=0}^{\\infty} n^2 \\frac{dP(n,t)}{dt} = (2k_b + k_d)m(t) + k_b - 2k_d \\langle n^2 \\rangle\n$$\nSubstituting this and the ODE for $m(t)$ into the expression for $\\frac{dv}{dt}$:\n$$\n\\frac{dv}{dt} = \\left[ (2k_b + k_d)m(t) + k_b - 2k_d \\langle n^2 \\rangle \\right] - 2m(t) (k_b - k_d m(t))\n$$\n$$\n\\frac{dv}{dt} = 2k_b m(t) + k_d m(t) + k_b - 2k_d \\langle n^2 \\rangle - 2k_b m(t) + 2k_d m(t)^2\n$$\n$$\n\\frac{dv}{dt} = k_b + k_d m(t) - 2k_d (\\langle n^2 \\rangle - m(t)^2)\n$$\nThis yields an ODE for the variance $v(t)$:\n$$\n\\frac{dv(t)}{dt} = k_b + k_d m(t) - 2k_d v(t)\n$$\nWith the initial condition $v(0)=0$ (since the initial state $x_0$ is deterministic), we can solve this ODE by substituting the solution for $m(t)$.\n- If $k_d > 0$:\n  $$\n  \\frac{dv}{dt} + 2k_d v = k_b + k_d \\left[ \\frac{k_b}{k_d} + \\left(x_0 - \\frac{k_b}{k_d}\\right) e^{-k_d t} \\right] = 2k_b + (k_d x_0 - k_b) e^{-k_d t}\n  $$\n  Solving this linear first-order ODE with $v(0)=0$ gives:\n  $$\n  v(t) = \\frac{k_b}{k_d} + \\left(x_0 - \\frac{k_b}{k_d}\\right) e^{-k_d t} - x_0 e^{-2k_d t}\n  $$\n  This can be conveniently rewritten as:\n  $$\n  v(t) = \\frac{k_b}{k_d}(1 - e^{-k_d t}) + x_0 e^{-k_d t}(1 - e^{-k_d t})\n  $$\n- If $k_d = 0$: The ODE for variance becomes $\\frac{dv}{dt} = k_b$. With $v(0)=0$, this integrates to\n  $$\n  v(t) = k_b t\n  $$\n\n### 2. Stochastic Simulation and Verification\n\nGillespie's Stochastic Simulation Algorithm (SSA), specifically the direct method, is used to generate exact numerical realizations of the Markov process.\n\n#### 2.1. Algorithm\nFor a single trajectory starting with $n=x_0$ at $t=0$, until $t \\ge T$:\n1.  Calculate propensities: $a_1 = k_b$, $a_2 = k_d n$.\n2.  Calculate total propensity: $a_{tot} = a_1 + a_2$.\n3.  If $a_{tot} = 0$, the state is absorbing. The simulation time is advanced to $T$ and the trajectory ends.\n4.  Generate two random numbers $r_1, r_2$ from a uniform distribution $U(0,1)$.\n5.  Calculate the time to the next reaction: $\\tau = \\frac{1}{a_{tot}} \\ln(\\frac{1}{r_1})$.\n6.  If $t+\\tau \\ge T$, the trajectory ends; the state remains $n$.\n7.  Otherwise, advance time: $t \\leftarrow t + \\tau$.\n8.  Select the reaction: if $r_2 \\cdot a_{tot} < a_1$, it is a synthesis ($n \\leftarrow n+1$); otherwise, it is a degradation ($n \\leftarrow n-1$).\n9.  Repeat from Step 1.\n\n#### 2.2. Verification\nThis process is repeated for $M=6000$ independent trajectories for each parameter set, using a fixed pseudorandom number generator seed of $12345$ for reproducibility. The ensemble of final molecule counts $\\{X_i(T)\\}_{i=1}^M$ is collected.\n\nThe empirical mean $\\hat{m}(T)$ and empirical variance $\\hat{v}(T)$ are computed from this ensemble:\n$$\n\\hat{m}(T) = \\frac{1}{M}\\sum_{i=1}^{M} X_i(T) \\qquad \\hat{v}(T) = \\frac{1}{M}\\sum_{i=1}^{M} (X_i(T) - \\hat{m}(T))^2\n$$\nConsistency is checked by comparing these empirical moments to the analytical moments $m(T)$ and $v(T)$ derived above. The check passes if both the absolute relative error for the mean and for the variance are less than or equal to the tolerance $\\varepsilon=0.05$:\n$$\n\\frac{\\lvert \\hat{m}(T) - m(T) \\rvert}{\\max(m(T), 10^{-12})} \\le \\varepsilon\n\\quad \\text{and} \\quad\n\\frac{\\lvert \\hat{v}(T) - v(T) \\rvert}{\\max(v(T), 10^{-12})} \\le \\varepsilon\n$$\nThe value $10^{-12}$ is used as a floor for the denominator to prevent division by zero or near-zero values.\n\nThe implementation will apply these steps to each test case, producing a boolean result indicating whether the consistency check passed.",
            "answer": "```python\nimport numpy as np\n\ndef gillespie_single_trajectory(kb, kd, x0, T, rng):\n    \"\"\"\n    Simulates a single trajectory of the birth-death process using Gillespie's direct method.\n    \n    Args:\n        kb (float): Synthesis rate.\n        kd (float): Degradation rate.\n        x0 (int): Initial molecule count.\n        T (float): Terminal time.\n        rng (np.random.Generator): A numpy random number generator instance.\n        \n    Returns:\n        int: The molecule count at time T.\n    \"\"\"\n    t = 0.0\n    n = int(x0)\n    \n    while t < T:\n        a1 = kb\n        a2 = kd * n\n        a_tot = a1 + a2\n        \n        if a_tot <= 1e-12:  # No more reactions can occur\n            break\n            \n        r1 = rng.random()\n        tau = -np.log(r1) / a_tot\n        \n        if t + tau >= T:\n            # Next reaction occurs after T, so state at T is the current state.\n            break\n            \n        t += tau\n        \n        r2 = rng.random()\n        \n        if r2 * a_tot < a1:\n            n += 1  # Synthesis\n        else:\n            n -= 1  # Degradation\n\n    return n\n\ndef run_simulation_ensemble(kb, kd, x0, T, M, rng):\n    \"\"\"\n    Runs an ensemble of Gillespie simulations and computes empirical moments.\n    \n    Args:\n        kb, kd, x0, T: Parameters for the simulation.\n        M (int): Number of trajectories in the ensemble.\n        rng (np.random.Generator): A numpy random number generator instance.\n        \n    Returns:\n        tuple[float, float]: The empirical mean and variance.\n    \"\"\"\n    final_counts = np.zeros(M, dtype=int)\n    for i in range(M):\n        final_counts[i] = gillespie_single_trajectory(kb, kd, x0, T, rng)\n        \n    m_hat = np.mean(final_counts)\n    v_hat = np.var(final_counts)  # ddof=0 is default, correct for empirical variance\n    return m_hat, v_hat\n\ndef calculate_analytical_moments(kb, kd, x0, T):\n    \"\"\"\n    Calculates the exact analytical mean and variance at time T.\n    \n    Args:\n        kb, kd, x0, T: Parameters for the model.\n        \n    Returns:\n        tuple[float, float]: The analytical mean and variance.\n    \"\"\"\n    if kd > 1e-12:  # General case for kd > 0\n        kb_over_kd = kb / kd\n        exp_term = np.exp(-kd * T)\n        \n        m_T = kb_over_kd + (x0 - kb_over_kd) * exp_term\n        \n        # This form is numerically stable and directly derived\n        v_T = kb_over_kd * (1 - exp_term) + x0 * exp_term * (1 - exp_term)\n    else:  # Special case for kd = 0 (Poisson process)\n        m_T = kb * T + x0\n        v_T = kb * T\n        \n    return m_T, v_T\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (kb, kd, x0, T)\n        (5.0, 1.0, 0, 3.0),\n        (0.0, 0.7, 50, 1.5),\n        (2.0, 0.1, 0, 20.0),\n        (9.0, 3.0, 0, 1.0),\n        (4.0, 0.5, 20, 2.0),\n        (1.5, 0.0, 3, 4.0),\n    ]\n    \n    M = 6000\n    epsilon = 0.05\n    seed = 12345\n    \n    results = []\n    \n    for case in test_cases:\n        kb, kd, x0, T = case\n        \n        # Calculate analytical moments\n        m_T, v_T = calculate_analytical_moments(kb, kd, x0, T)\n        \n        # Run stochastic simulation ensemble\n        # A new RNG is created for each case to ensure independent reproducibility\n        rng = np.random.default_rng(seed)\n        m_hat, v_hat = run_simulation_ensemble(kb, kd, x0, T, M, rng)\n        \n        # Verify consistency\n        err_m = np.abs(m_hat - m_T) / max(m_T, 1e-12)\n        err_v = np.abs(v_hat - v_T) / max(v_T, 1e-12)\n        \n        passes_check = (err_m <= epsilon) and (err_v <= epsilon)\n        results.append(passes_check)\n        \n    # Format and print the final output exactly as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}