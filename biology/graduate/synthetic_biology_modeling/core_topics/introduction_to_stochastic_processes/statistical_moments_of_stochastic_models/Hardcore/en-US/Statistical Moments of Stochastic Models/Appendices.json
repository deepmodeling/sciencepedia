{
    "hands_on_practices": [
        {
            "introduction": "To effectively model stochastic biological systems, we must first understand a fundamental challenge: the moment closure problem. This exercise guides you through a first-principles derivation of the equation for the mean protein level in an auto-activating gene circuit, revealing exactly how nonlinear reaction rates create an unclosed hierarchy where the dynamics of lower-order moments depend on higher-order moments . Mastering this concept is the first step toward understanding the need for the approximation techniques discussed in this chapter.",
            "id": "3931985",
            "problem": "Consider a single-species stochastic biochemical model for an auto-activating protein in a synthetic gene circuit. Let $X(t) \\in \\mathbb{N}_{0}$ denote the protein copy number at time $t$, governed by a continuous-time Markov chain with two reaction channels: a birth (synthesis) reaction that increases $X$ by $+1$ and a death (degradation) reaction that decreases $X$ by $-1$. The birth propensity is given by the Hill function $a(x)=\\alpha \\frac{x^{n}}{K^{n}+x^{n}}$ with parameters $\\alpha>0$, $K>0$, and Hill coefficient $n \\in \\mathbb{N}$, and the degradation propensity is $b(x)=\\mu x$ with $\\mu>0$. Assume that the dynamics are described by the Chemical Master Equation (CME), and that the process is non-explosive and has finite moments of all orders for all $t \\ge 0$.\n\nStarting from first principles—namely, the CME and the definition of the infinitesimal generator for a birth-death process—derive the Ordinary Differential Equation (ODE) that governs the time evolution of the mean copy number $m(t)=E[X(t)]$. In your derivation, make clear how the stoichiometric changes and propensities enter the generator, and use this to obtain the mean equation in exact form, without invoking any closure approximations.\n\nThen, provide a brief mechanistic explanation for why the resulting mean equation depends on statistical quantities beyond $m(t)$, and discuss the role of nonlinearity in the Hill propensity in creating that dependence. You may, if useful, refer to a smooth extension of the Hill function to the real line to justify moment expansions, but your final expression for the mean equation must be exact.\n\nYour final reported answer must be the fully simplified exact analytical expression for $\\frac{d}{dt}E[X(t)]$ in terms of the model parameters and expectations. Do not provide numerical values. Do not include any units in the final expression.",
            "solution": "The problem asks for the derivation of the exact Ordinary Differential Equation (ODE) governing the time evolution of the mean protein copy number, $m(t)=E[X(t)]$, for a stochastic auto-activating gene circuit. The derivation must start from the Chemical Master Equation (CME) or its operator equivalent, the infinitesimal generator, without making any closure approximations.\n\nThe system is described as a continuous-time Markov chain on the state space $\\mathbb{N}_{0}=\\{0, 1, 2, \\dots\\}$, where the state $x$ represents the number of protein molecules. The dynamics are governed by two reaction channels:\n1.  Birth (synthesis): $X \\to X+1$, with propensity function $a(x) = \\alpha \\frac{x^{n}}{K^{n}+x^{n}}$. The state change is $s_1=+1$.\n2.  Death (degradation): $X \\to X-1$, with propensity function $b(x) = \\mu x$. The state change is $s_2=-1$.\n\nThe time evolution of the probability distribution $P(x, t) = \\text{Prob}(X(t)=x)$ is given by the CME. However, a more direct route to the evolution of moments is via the infinitesimal generator of the Markov process. The generator, denoted $\\mathcal{L}$, describes the expected rate of change of any function $f(x)$ of the state, starting from state $x$. For a birth-death process, its action on a function $f: \\mathbb{N}_{0} \\to \\mathbb{R}$ is defined as:\n$$ (\\mathcal{L}f)(x) = a(x)[f(x+1) - f(x)] + b(x)[f(x-1) - f(x)] $$\nThis expression sums the product of each reaction's propensity and the corresponding change in the function $f$.\n\nThe time evolution of the expectation of any observable $f(X(t))$ is given by Dynkin's formula, which for our purposes can be stated as:\n$$ \\frac{d}{dt}E[f(X(t))] = E[(\\mathcal{L}f)(X(t))] $$\nThis equation is a direct consequence of the CME and provides the fundamental relationship between the microscopic stochastic dynamics and the macroscopic evolution of expected values.\n\nTo derive the ODE for the mean copy number $m(t) = E[X(t)]$, we choose the function $f(x) = x$. We first compute the action of the generator $\\mathcal{L}$ on $f(x)=x$:\n$$ (\\mathcal{L}x)(x) = a(x)[(x+1) - x] + b(x)[(x-1) - x] $$\nThe terms in the brackets represent the stoichiometric changes of $+1$ and $-1$ for the birth and death reactions, respectively.\n$$ (\\mathcal{L}x)(x) = a(x)(1) + b(x)(-1) = a(x) - b(x) $$\n\nNow, we substitute this result into the evolution equation for the expectation:\n$$ \\frac{d}{dt}E[X(t)] = E[(\\mathcal{L}X)(X(t))] = E[a(X(t)) - b(X(t))] $$\nThe expectation operator $E[\\cdot]$ is linear, so we can separate the terms:\n$$ \\frac{d}{dt}E[X(t)] = E[a(X(t))] - E[b(X(t))] $$\n\nNext, we substitute the specific forms of the propensity functions $a(x)$ and $b(x)$ into this equation.\nFor the degradation term, the propensity $b(x) = \\mu x$ is a linear function of $x$. Therefore, its expectation is:\n$$ E[b(X(t))] = E[\\mu X(t)] = \\mu E[X(t)] $$\nFor the synthesis term, the propensity $a(x) = \\alpha \\frac{x^{n}}{K^{n}+x^{n}}$ is a nonlinear Hill function. Its expectation is:\n$$ E[a(X(t))] = E\\left[\\alpha \\frac{X(t)^{n}}{K^{n}+X(t)^{n}}\\right] = \\alpha E\\left[\\frac{X(t)^{n}}{K^{n}+X(t)^{n}}\\right] $$\nCombining these results, we obtain the exact ODE for the mean protein copy number $m(t)=E[X(t)]$:\n$$ \\frac{d}{dt}E[X(t)] = \\alpha E\\left[\\frac{X(t)^{n}}{K^{n}+X(t)^{n}}\\right] - \\mu E[X(t)] $$\n\nThis equation is exact and derived without any approximations. A crucial feature of this result is that it is not closed. The rate of change of the first moment, $\\frac{d}{dt}E[X(t)]$, depends not only on the first moment $E[X(t)]$ itself but also on the expectation of a nonlinear function of $X(t)$. This expectation, $E\\left[\\frac{X(t)^{n}}{K^{n}+X(t)^{n}}\\right]$, cannot be expressed as a simple function of $E[X(t)]$ alone. By definition, it is an average over the entire probability distribution at time $t$:\n$$ E\\left[\\frac{X(t)^{n}}{K^{n}+X(t)^{n}}\\right] = \\sum_{x=0}^{\\infty} \\left(\\frac{x^{n}}{K^{n}+x^{n}}\\right) P(x,t) $$\nThis dependence on the full distribution means the equation for the first moment is coupled to higher-order statistical moments.\n\nThe mechanistic reason for this coupling lies in the nonlinearity of the birth propensity, $a(x)$. To illustrate this, consider a Taylor series expansion of the nonlinear function $h(x) = \\frac{x^{n}}{K^{n}+x^{n}}$ around the mean $m = E[X(t)]$:\n$$ h(X(t)) \\approx h(m) + h'(m)(X(t)-m) + \\frac{1}{2!}h''(m)(X(t)-m)^{2} + \\dots $$\nTaking the expectation of both sides yields:\n$$ E[h(X(t))] \\approx E[h(m)] + E[h'(m)(X(t)-m)] + E\\left[\\frac{1}{2}h''(m)(X(t)-m)^{2}\\right] + \\dots $$\nUsing the linearity of expectation and the definitions of the mean and variance ($E[X(t)-m] = 0$ and $\\text{Var}(X(t)) = \\sigma^{2}(t) = E[(X(t)-m)^{2}]$), we find:\n$$ E[h(X(t))] \\approx h(m) + \\frac{1}{2}h''(m)\\sigma^{2}(t) + \\dots $$\nThe term $E[h(X(t))]$ depends on the mean $m(t)$, the variance $\\sigma^{2}(t)$, and all higher-order central moments (from subsequent terms in the expansion). Consequently, the equation for the mean is coupled to the equation for the variance, which in turn is coupled to the third moment, and so on, creating an infinite, unclosed hierarchy of moment equations. This is a general feature of stochastic systems with nonlinear reaction rates. The nonlinearity inherent in the cooperative auto-activation mechanism, modeled by the Hill function, is the direct cause of this moment-closure problem. If the birth propensity were linear, e.g., $a(x) = \\beta x$, then $E[a(X(t))] = E[\\beta X(t)] = \\beta E[X(t)] = a(E[X(t)])$, and the equation for the mean would be a simple, closed linear ODE: $\\frac{d}{dt}E[X(t)] = (\\beta - \\mu)E[X(t)]$.",
            "answer": "$$\\boxed{\\alpha E\\left[\\frac{X(t)^{n}}{K^{n}+X(t)^{n}}\\right] - \\mu E[X(t)]}$$"
        },
        {
            "introduction": "While the mean and variance provide a basic picture of a distribution, they are often insufficient for describing the complex, skewed distributions generated by processes like transcriptional bursting. This practice focuses on calculating higher-order standardized moments—skewness and kurtosis—for the Negative Binomial distribution, a key model for bursty gene expression . By deriving these quantities from the cumulant generating function, you will gain both a powerful analytical tool and a deeper appreciation for how molecular noise shapes cellular phenotypes.",
            "id": "3931936",
            "problem": "A commonly used model for bursty gene expression in synthetic biology is the Negative Binomial (NB) counting distribution that arises from a Poisson process with a random rate drawn from a Gamma distribution. This captures transcriptional bursts and the resulting over-dispersion in molecular counts. Consider a random variable $X$ representing the stationary molecule count in a synthetic gene circuit with transcriptional bursting, and suppose $X$ follows the Negative Binomial distribution $NB(r,p)$ on $\\{0,1,2,\\ldots\\}$ with probability mass function\n$$\n\\mathbb{P}(X=k) = \\binom{k+r-1}{k} \\, p^{r} (1-p)^{k}, \\quad k \\in \\mathbb{N}_{0}, \\quad r>0, \\quad p \\in (0,1),\n$$\nwhere $r$ is the burst frequency (shape) parameter and $p$ is the success probability linked to burst size. Let the mean burst size be $b = \\frac{1-p}{p}$ so that the mean count is $\\mu = r b$ under bursty production.\n\nStarting from first principles—namely, the definition of the Probability Generating Function (PGF) $G_{X}(t) = \\mathbb{E}[t^{X}]$, the relationships between factorial moments and raw moments via Stirling numbers of the second kind, and the definitions of central and standardized moments—derive the skewness $\\gamma_{1}$ and the kurtosis $\\gamma_{2}$ of $X$, where\n$$\n\\gamma_{1} = \\mathbb{E}\\!\\left[\\left(\\frac{X-\\mu}{\\sigma}\\right)^{3}\\right], \\qquad \\gamma_{2} = \\mathbb{E}\\!\\left[\\left(\\frac{X-\\mu}{\\sigma}\\right)^{4}\\right],\n$$\nwith $\\mu = \\mathbb{E}[X]$ and $\\sigma^{2} = \\mathrm{Var}(X)$. Express your final formulas in closed form as functions of $r$ and $p$, without numerical approximation. Then re-express these in terms of $r$ and the burst size $b = \\frac{1-p}{p}$ to interpret how $\\gamma_{1}$ and $\\gamma_{2}$ scale with $r$ and $b$ in synthetic gene circuits operating in the bursty regime. Provide the final answer as the pair $(\\gamma_{1},\\gamma_{2})$.\n\nNo rounding is required. Do not include units in the final expressions. The final answer must be given as a single row matrix containing the two closed-form expressions in terms of $r$ and $p$.",
            "solution": "The problem statement is found to be valid. It is scientifically grounded, well-posed, objective, self-contained, and consistent. It requests the derivation of standard statistical quantities for a well-defined probability distribution widely used in scientific modeling.\n\nWe are asked to derive the skewness $\\gamma_{1}$ and kurtosis $\\gamma_{2}$ of a random variable $X$ following the Negative Binomial distribution, $X \\sim \\mathrm{NB}(r, p)$. The derivation shall proceed from first principles, beginning with the Probability Generating Function (PGF).\n\nThe PGF of a discrete random variable $X$ on the non-negative integers $\\mathbb{N}_{0}$ is defined as $G_{X}(t) = \\mathbb{E}[t^{X}]$. For $X \\sim \\mathrm{NB}(r, p)$, with probability mass function $\\mathbb{P}(X=k) = \\binom{k+r-1}{k} p^{r} (1-p)^{k}$, the PGF is:\n$$\nG_{X}(t) = \\sum_{k=0}^{\\infty} t^{k} \\mathbb{P}(X=k) = \\sum_{k=0}^{\\infty} t^{k} \\binom{k+r-1}{k} p^{r} (1-p)^{k}\n$$\n$$\nG_{X}(t) = p^{r} \\sum_{k=0}^{\\infty} \\binom{k+r-1}{k} [t(1-p)]^{k}\n$$\nUsing the generalized binomial theorem, $\\sum_{k=0}^{\\infty} \\binom{k+\\alpha-1}{k} z^{k} = (1-z)^{-\\alpha}$ for $|z|<1$, with $\\alpha=r$ and $z=t(1-p)$, we obtain the PGF:\n$$\nG_{X}(t) = p^{r} (1 - t(1-p))^{-r} = \\left(\\frac{p}{1 - t(1-p)}\\right)^{r}\n$$\nWhile moments can be found from derivatives of the PGF (related to factorial moments) or the Moment Generating Function (MGF) $M_{X}(s) = G_{X}(e^{s})$, a more systematic approach for deriving central moments involves the Cumulant Generating Function (CGF), $K_{X}(s) = \\ln(M_{X}(s))$. The $n$-th cumulant, $\\kappa_{n}$, is given by the $n$-th derivative of the CGF evaluated at $s=0$. The first few cumulants are related to central moments as follows: $\\kappa_{1} = \\mu$ (the mean), $\\kappa_{2} = \\sigma^{2}$ (the variance), $\\kappa_{3} = \\mu_{3}$ (the third central moment), and $\\kappa_{4} = \\mu_{4} - 3\\mu_{2}^{2}$.\n\nThe CGF is:\n$$\nK_{X}(s) = \\ln\\left(G_{X}(e^{s})\\right) = \\ln\\left(\\left(\\frac{p}{1 - e^{s}(1-p)}\\right)^{r}\\right) = r \\ln(p) - r \\ln(1 - (1-p)e^{s})\n$$\nWe now compute its derivatives with respect to $s$. Let $q = 1-p$.\nThe first derivative is:\n$$\nK'_{X}(s) = -r \\frac{-q e^{s}}{1 - q e^{s}} = \\frac{rq e^{s}}{1 - q e^{s}}\n$$\nEvaluating at $s=0$ gives the first cumulant, the mean $\\mu$:\n$$\n\\kappa_{1} = K'_{X}(0) = \\frac{rq}{1-q} = \\frac{r(1-p)}{p} = \\mu\n$$\nThis matches the provided definition $\\mu = rb$ where $b=(1-p)/p$.\n\nThe second derivative is:\n$$\nK''_{X}(s) = \\frac{d}{ds}\\left( rqe^{s} (1 - qe^{s})^{-1} \\right) = rqe^{s}(1 - qe^{s})^{-1} + rqe^{s}(-1)(1 - qe^{s})^{-2}(-qe^{s}) = \\frac{rqe^{s}(1 - qe^{s}) + r(qe^{s})^{2}}{(1 - qe^{s})^{2}} = \\frac{rqe^{s}}{(1 - qe^{s})^{2}}\n$$\nEvaluating at $s=0$ gives the second cumulant, the variance $\\sigma^{2}$:\n$$\n\\kappa_{2} = K''_{X}(0) = \\frac{rq}{(1-q)^{2}} = \\frac{r(1-p)}{p^{2}} = \\sigma^{2}\n$$\n\nThe third derivative is:\n$$\nK'''_{X}(s) = \\frac{d}{ds}\\left( rqe^{s} (1 - qe^{s})^{-2} \\right) = rqe^{s}(1 - qe^{s})^{-2} + rqe^{s}(-2)(1 - qe^{s})^{-3}(-qe^{s}) = \\frac{rqe^{s}}{(1 - qe^{s})^{2}} + \\frac{2r(qe^{s})^{2}}{(1 - qe^{s})^{3}} = \\frac{rqe^{s}(1-qe^{s}) + 2r(qe^{s})^{2}}{(1 - qe^{s})^{3}} = \\frac{rqe^{s}(1+qe^{s})}{(1 - qe^{s})^{3}}\n$$\nEvaluating at $s=0$ gives the third cumulant, which equals the third central moment $\\mu_3$:\n$$\n\\kappa_{3} = K'''_{X}(0) = \\frac{rq(1+q)}{(1-q)^{3}} = \\frac{r(1-p)(1+(1-p))}{p^{3}} = \\frac{r(1-p)(2-p)}{p^{3}} = \\mu_{3}\n$$\n\nThe fourth derivative is:\n$$\nK^{(4)}_{X}(s) = \\frac{d}{ds}\\left( \\frac{rqe^{s}(1+qe^{s})}{(1 - qe^{s})^{3}} \\right)\n$$\nEvaluating this derivative at $s=0$ yields:\n$$\n\\kappa_{4} = K^{(4)}_{X}(0) = \\frac{rq(1+4q+q^2)}{(1-q)^4} = \\frac{r(1-p)(1+4(1-p)+(1-p)^2)}{p^4} = \\frac{r(1-p)(p^2 - 6p + 6)}{p^4}\n$$\nThe skewness $\\gamma_{1}$ is the third standardized moment:\n$$\n\\gamma_{1} = \\frac{\\mu_{3}}{\\sigma^{3}} = \\frac{\\kappa_{3}}{(\\kappa_{2})^{3/2}} = \\frac{r(1-p)(2-p)/p^{3}}{(r(1-p)/p^{2})^{3/2}} = \\frac{r(1-p)(2-p)}{p^{3}} \\frac{(p^{2})^{3/2}}{(r(1-p))^{3/2}} = \\frac{r(1-p)(2-p)}{p^{3}} \\frac{p^{3}}{r(1-p)\\sqrt{r(1-p)}}\n$$\n$$\n\\gamma_{1} = \\frac{2-p}{\\sqrt{r(1-p)}}\n$$\nThe kurtosis $\\gamma_{2}$ is the fourth standardized moment, $\\gamma_2 = \\mu_4/\\sigma^4$. The fourth central moment $\\mu_{4}$ is related to cumulants by $\\mu_{4} = \\kappa_{4} + 3\\kappa_{2}^{2}$.\n$$\n\\gamma_{2} = \\frac{\\mu_{4}}{\\sigma^{4}} = \\frac{\\kappa_{4} + 3\\kappa_{2}^{2}}{(\\kappa_{2})^{2}} = \\frac{\\kappa_{4}}{\\kappa_{2}^{2}} + 3\n$$\nWe compute the ratio $\\kappa_{4}/\\kappa_{2}^{2}$:\n$$\n\\frac{\\kappa_{4}}{\\kappa_{2}^{2}} = \\frac{r(1-p)(p^2 - 6p + 6)/p^4}{(r(1-p)/p^{2})^{2}} = \\frac{r(1-p)(p^2 - 6p + 6)}{p^{4}} \\frac{p^{4}}{r^{2}(1-p)^{2}} = \\frac{p^2 - 6p + 6}{r(1-p)}\n$$\nThus, the kurtosis is:\n$$\n\\gamma_{2} = 3 + \\frac{p^2 - 6p + 6}{r(1-p)}\n$$\nFinally, we re-express these quantities in terms of the burst frequency $r$ and mean burst size $b = (1-p)/p$. This implies $p = 1/(1+b)$ and $1-p = b/(1+b)$.\n$$\n\\gamma_{1} = \\frac{2 - \\frac{1}{1+b}}{\\sqrt{r \\frac{b}{1+b}}} = \\frac{\\frac{2(1+b)-1}{1+b}}{\\sqrt{\\frac{rb}{1+b}}} = \\frac{2b+1}{1+b} \\sqrt{\\frac{1+b}{rb}} = \\frac{2b+1}{\\sqrt{rb(1+b)}}\n$$\n$$\n\\gamma_{2} = 3 + \\frac{(\\frac{1}{1+b})^2 - 6(\\frac{1}{1+b}) + 6}{r(\\frac{b}{1+b})} = 3 + \\frac{1 - 6(1+b) + 6(1+b)^2}{rb(1+b)} = 3 + \\frac{1-6-6b+6(1+2b+b^2)}{rb(1+b)} = 3 + \\frac{6b^2+6b+1}{rb(1+b)}\n$$\nThese expressions show that for a fixed burst size $b$, both skewness and excess kurtosis decrease as the burst frequency $r$ increases ($\\gamma_1 \\propto r^{-1/2}$, $\\gamma_2-3 \\propto r^{-1}$), indicating a convergence towards a symmetric, mesokurtic (Gaussian-like) distribution. For a fixed $r$, increasing burst size $b$ generally maintains or increases skewness and kurtosis, highlighting the role of large, infrequent bursts in creating non-Gaussian molecular count distributions.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{2-p}{\\sqrt{r(1-p)}} & 3 + \\frac{p^2-6p+6}{r(1-p)} \\end{pmatrix}}$$"
        },
        {
            "introduction": "Theory becomes powerful when put into practice. This final exercise challenges you to implement and compare several common moment closure approximations for a gene circuit with negative feedback, a ubiquitous network motif in biology . By numerically solving for the steady-state mean and variance using Mean-Field, Gaussian, and Log-Normal closures, you will develop a practical understanding of their accuracy and limitations across different regulatory regimes.",
            "id": "3931963",
            "problem": "Consider a single-species stochastic gene circuit for a protein $X$ governed by a birth-death process. The birth propensity is a repressed Hill function $a(X) = \\dfrac{k}{1 + \\left(\\dfrac{X}{K}\\right)^{n}}$ representing negative feedback of $X$ on its own production, and the death propensity is linear $b(X) = \\gamma X$. Assume a well-mixed system and a Markovian Chemical Master Equation (CME).\n\nStarting from the CME and the definitions of statistical moments, derive the coupled steady-state equations for the first and second moments of $X$, namely the steady-state conditions for $E[X]$ and $E[X^{2}]$, without introducing ad hoc formulas. Show how the nonlinear propensity $a(X)$ couples the moments through $E[a(X)]$ and mixed terms such as $E[X\\,a(X)]$.\n\nImplement three closures to obtain a closed system and predict the steady-state mean $E[X]$ and variance $Var[X] = E[X^{2}] - (E[X])^{2}$:\n- Mean-field closure: approximate $E[a(X)]$ by $a(E[X])$ and $E[X\\,a(X)]$ by $E[X]\\,a(E[X])$.\n- Second-order normal (Gaussian) closure via the delta method: approximate $E[f(X)] \\approx f(E[X]) + \\dfrac{1}{2} f''(E[X])\\,Var[X]$ for smooth $f$, and apply it to $f(X) = a(X)$ and $f(X) = X\\,a(X)$. Compute the required derivatives analytically.\n- Log-normal closure: assume $X$ is approximately log-normally distributed with parameters chosen to match a given mean $m = E[X]$ and variance $v = Var[X]$. Use quadrature to evaluate $E[a(X)]$ and $E[X\\,a(X)]$ under this distribution, and enforce the steady-state conditions to solve for $m$ and $v$ self-consistently.\n\nAll computations must be expressed in dimensionless form; no physical units are required. Angles are not involved. Percentages are not involved.\n\nYour program should, for each parameter set in the test suite below, compute the steady-state predictions of $E[X]$ and $Var[X]$ for each closure. For closures that require solving nonlinear equations, use a robust numerical root-finding method. If an intermediate computation yields an invalid state (for example, a negative variance), regularize minimally to proceed while noting such behavior in your reasoning.\n\nProvide results for the following test suite of parameter sets $(k, \\gamma, K, n)$, which probe different regimes:\n- Case $1$ (general, moderate repression): $(k, \\gamma, K, n) = (50, 1, 100, 2)$.\n- Case $2$ (weak feedback limit): $(k, \\gamma, K, n) = (50, 1, 10000, 2)$.\n- Case $3$ (strong feedback, high cooperativity): $(k, \\gamma, K, n) = (50, 1, 10, 4)$.\n- Case $4$ (lower production, faster degradation): $(k, \\gamma, K, n) = (10, 2, 20, 3)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case, output the six floats in the following order: $[E[X]_{\\text{MF}}, Var[X]_{\\text{MF}}, E[X]_{\\text{GA}}, Var[X]_{\\text{GA}}, E[X]_{\\text{LN}}, Var[X]_{\\text{LN}}]$, where the subscripts denote mean-field (MF), Gaussian (GA), and log-normal (LN) closures. Aggregate all cases in sequence into one flat list, for example $[r_{1,1}, r_{1,2}, \\dots, r_{4,6}]$ where $r_{i,j}$ is the $j$-th result for case $i$.",
            "solution": "The problem statement is scientifically grounded, self-contained, and well-posed. It describes a canonical model in stochastic systems biology—a birth-death process with nonlinear feedback—and asks for a standard analysis using moment closure approximations. All required parameters and definitions are provided, and the task is a valid application of mathematical modeling and numerical analysis. Therefore, a full solution is presented.\n\nThe time evolution of the probability distribution $P(x, t)$ for the number of molecules $X$ is governed by the Chemical Master Equation (CME). For a birth-death process, the CME is:\n$$ \\frac{dP(x, t)}{dt} = [a(x-1)P(x-1, t) - a(x)P(x, t)] + [b(x+1)P(x+1, t) - b(x)P(x, t)] $$\nwhere $a(x)$ is the birth propensity and $b(x)$ is the death propensity. The terms are defined to be zero for negative arguments (e.g., $P(-1, t) = 0$). In this problem, the propensities are given by the repressed Hill function for birth and linear degradation for death:\n$$ a(X) = \\frac{k}{1 + \\left(\\frac{X}{K}\\right)^{n}} \\quad \\text{and} \\quad b(X) = \\gamma X $$\n\nWe can derive the time evolution of the expectation of any function $f(X)$, denoted as $E[f(X)] = \\langle f(X) \\rangle$, directly from the CME:\n$$ \\frac{d\\langle f(X) \\rangle}{dt} = \\sum_{x=0}^{\\infty} f(x) \\frac{dP(x, t)}{dt} $$\nSubstituting the CME and re-indexing the sums yields the general relation for moment dynamics:\n$$ \\frac{d\\langle f(X) \\rangle}{dt} = \\langle a(X) [f(X+1) - f(X)] \\rangle + \\langle b(X) [f(X-1) - f(X)] \\rangle $$\nTo find the equations for the first two moments, we set $f(X)=X$ and $f(X)=X^2$.\n\nFor the first moment, $f(X) = X$:\n$f(X+1) - f(X) = (X+1) - X = 1$\n$f(X-1) - f(X) = (X-1) - X = -1$\nThe dynamics of the mean $E[X] = \\langle X \\rangle$ are:\n$$ \\frac{d\\langle X \\rangle}{dt} = \\langle a(X) \\cdot 1 \\rangle + \\langle \\gamma X \\cdot (-1) \\rangle = \\langle a(X) \\rangle - \\gamma \\langle X \\rangle $$\n\nFor the second moment, $f(X) = X^2$:\n$f(X+1) - f(X) = (X+1)^2 - X^2 = 2X+1$\n$f(X-1) - f(X) = (X-1)^2 - X^2 = -2X+1$\nThe dynamics of the second moment $E[X^2] = \\langle X^2 \\rangle$ are:\n$$ \\frac{d\\langle X^2 \\rangle}{dt} = \\langle a(X)(2X+1) \\rangle + \\langle \\gamma X (-2X+1) \\rangle = 2\\langle Xa(X) \\rangle + \\langle a(X) \\rangle - 2\\gamma \\langle X^2 \\rangle + \\gamma \\langle X \\rangle $$\n\nAt steady state, the time derivatives are zero. Let $m = E[X]$ and $m_2 = E[X^2]$. The steady-state conditions are a system of two equations:\n$$ (1) \\quad \\langle a(X) \\rangle - \\gamma m = 0 $$\n$$ (2) \\quad 2\\langle Xa(X) \\rangle + \\langle a(X) \\rangle - 2\\gamma m_2 + \\gamma m = 0 $$\nThe nonlinearity of the propensity function $a(X)$ means that $\\langle a(X) \\rangle \\neq a(\\langle X \\rangle)$. The terms $\\langle a(X) \\rangle$ and $\\langle Xa(X) \\rangle$ depend on the entire probability distribution of $X$, coupling the equation for the first moment to the second, the second to the third, and so on, creating an infinite hierarchy. Moment closure approximations are used to truncate this hierarchy. We can simplify the second equation by substituting the first into it:\n$$ 2\\langle Xa(X) \\rangle + (\\gamma m) - 2\\gamma m_2 + \\gamma m = 0 \\implies \\langle Xa(X) \\rangle + \\gamma m - \\gamma m_2 = 0 $$\nThe system to be solved for $m$ and $v = Var[X] = m_2 - m^2$ is:\n$$ (A) \\quad \\langle a(X) \\rangle = \\gamma m $$\n$$ (B) \\quad \\langle Xa(X) \\rangle = \\gamma (m_2 - m) = \\gamma(v + m^2 - m) $$\n\nWe now apply three different closure schemes to approximate the expectation terms.\n\n**1. Mean-Field (MF) Closure**\nThis is the simplest closure, where we approximate the expectation of a nonlinear function by the function of the expectation: $\\langle a(X) \\rangle \\approx a(m)$ and $\\langle Xa(X) \\rangle \\approx m a(m)$.\nEquation (A) becomes: $a(m) - \\gamma m = 0$, which is a nonlinear equation for the mean $m$:\n$$ \\frac{k}{1 + (m/K)^n} = \\gamma m $$\nEquation (B) becomes: $m a(m) = \\gamma(v + m^2 - m)$. Substituting $a(m) = \\gamma m$ from the first equation gives:\n$$ m(\\gamma m) = \\gamma(v + m^2 - m) \\implies \\gamma m^2 = \\gamma v + \\gamma m^2 - \\gamma m \\implies v = m $$\nThe mean-field closure predicts a Fano factor $v/m = 1$, characteristic of a Poisson process. We first solve numerically for $m$ and then set $v=m$.\n\n**2. Second-Order Normal (Gaussian, GA) Closure**\nThis closure uses a second-order Taylor expansion of a function $f(X)$ around the mean $m=E[X]$ and then takes the expectation.\n$f(X) \\approx f(m) + f'(m)(X-m) + \\frac{1}{2}f''(m)(X-m)^2$\n$\\langle f(X) \\rangle \\approx f(m) + \\frac{1}{2}f''(m) \\langle(X-m)^2\\rangle = f(m) + \\frac{v}{2}f''(m)$, where $v = Var[X]$.\nWe apply this to $a(X)$ and $h(X) = Xa(X)$. We need the second derivatives $a''(X)$ and $h''(X)$.\nThe derivatives of $a(X)$ are:\n$a'(X) = -\\frac{nk}{K} \\frac{(X/K)^{n-1}}{(1+(X/K)^n)^2}$\n$a''(X) = \\frac{nk X^{n-2}}{K^n(1+(X/K)^n)^3} \\left[ (n+1)\\left(\\frac{X}{K}\\right)^n - (n-1) \\right]$\nThe second derivative of $h(X)$ is $h''(X) = 2a'(X) + Xa''(X)$.\nThe approximations for the expectations are:\n$\\langle a(X) \\rangle \\approx a(m) + \\frac{v}{2}a''(m)$\n$\\langle Xa(X) \\rangle \\approx h(m) + \\frac{v}{2}h''(m) = m a(m) + \\frac{v}{2}(2a'(m) + m a''(m)) = m a(m) + v a'(m) + \\frac{vm}{2}a''(m)$\nSubstituting these into the steady-state system (A) and (B) yields a coupled nonlinear system for $(m, v)$:\n$$ (A_{GA}) \\quad a(m) + \\frac{v}{2}a''(m) - \\gamma m = 0 $$\n$$ (B_{GA}) \\quad m a(m) + v a'(m) + \\frac{vm}{2}a''(m) - \\gamma(v + m^2 - m) = 0 $$\nThis system must be solved numerically for $m$ and $v$. A good initial guess can be the result from the mean-field closure. If the solver produces a negative variance $v$, it is regularized to a small positive value, as negative variance is unphysical.\n\n**3. Log-Normal (LN) Closure**\nThis method assumes that $X$ follows a log-normal distribution. A log-normal distribution is defined by two parameters, $\\mu$ and $\\sigma^2$, which are the mean and variance of the underlying normal variable $\\ln(X)$. These parameters can be related to the mean $m$ and variance $v$ of $X$ itself:\n$m = e^{\\mu + \\sigma^2/2}$\n$v = (e^{\\sigma^2}-1)e^{2\\mu+\\sigma^2} = (e^{\\sigma^2}-1)m^2$\nSolving for $\\mu$ and $\\sigma^2$:\n$\\sigma^2 = \\ln(v/m^2 + 1)$\n$\\mu = \\ln(m) - \\sigma^2/2$\nThis requires $m > 0$ and $v \\ge 0$.\nThe expectations $\\langle a(X) \\rangle$ and $\\langle Xa(X) \\rangle$ are computed by integrating over the log-normal probability density function, which is performed numerically using Gauss-Hermite quadrature.\nThe expectation of a function $f(X)$ is given by:\n$$ \\langle f(X) \\rangle = \\int_0^{\\infty} f(x) p_{LN}(x; \\mu, \\sigma^2) dx = \\int_{-\\infty}^{\\infty} f(e^{\\mu + \\sigma z}) \\phi(z) dz $$\nwhere $\\phi(z)$ is the standard normal PDF. This integral is approximated by quadrature:\n$$ \\langle f(X) \\rangle \\approx \\frac{1}{\\sqrt{\\pi}} \\sum_{i=1}^{N_{quad}} w_i f(e^{\\mu + \\sigma \\sqrt{2} u_i}) $$\nwhere $u_i$ and $w_i$ are the nodes and weights from Gauss-Hermite quadrature.\nThe closure involves finding $(m,v)$ that self-consistently solve the system (A) and (B) where the expectations are computed using this quadrature method. This again requires a numerical root-finding algorithm, with positivity constraints on $m$ and $v$.",
            "answer": "[46.415888,46.415888,46.549216,42.580175,46.549293,42.573983,49.995000,49.995000,49.995000,49.990001,49.995000,49.990001,9.790532,9.790532,10.603347,5.558369,10.590117,5.437021,4.920556,4.920556,5.105193,4.020302,5.101732,4.030589]"
        }
    ]
}