{
    "hands_on_practices": [
        {
            "introduction": "Calculating the expected value of a function of a random variable is a fundamental skill in stochastic modeling. This exercise  grounds this abstract concept in a tangible biological scenario: gene transcription events modeled by a Poisson process. You will derive the average outcome of a saturating function, which mimics resource limitation or other nonlinear biological responses, reinforcing your ability to work from first principles with probability mass functions and series manipulations.",
            "id": "3907161",
            "problem": "A single gene in a homogeneous environment is modeled such that initiation events of transcription by ribonucleic acid polymerase occur as a time-homogeneous Poisson process with constant hazard. Over a fixed observation window, let $X$ denote the total number of transcription initiation events. Under these assumptions, $X$ is modeled as a Poisson random variable with parameter $\\lambda0$, written $X \\sim \\mathrm{Poisson}(\\lambda)$, where $\\lambda$ is the expected number of initiations in the window.\n\nTo capture saturation of a limiting factor (for example, effective promoter occupancy or shared resource usage) by discrete initiation events, consider the saturating transform $g(x)=\\dfrac{x}{1+x}$ for $x \\in \\{0,1,2,\\ldots\\}$. Starting only from the definition of the expectation of a function of a discrete random variable and the probability mass function of the Poisson distribution, and using only well-tested series facts, derive a closed-form analytic expression in terms of $\\lambda$ for the population-average saturation $\\,\\mathbb{E}[g(X)]\\,$. Clearly justify any series manipulations you perform. Provide your final result as an exact expression in terms of $\\lambda$ (no numerical rounding).",
            "solution": "The problem statement is assessed to be valid. It is scientifically grounded in standard stochastic modeling of gene expression, well-posed with a clear objective and sufficient information, and expressed in precise, objective language. The problem is a formal mathematical derivation that is verifiable and not trivial. We may, therefore, proceed with the solution.\n\nThe objective is to compute the expectation of the function $g(X)$, denoted as $\\mathbb{E}[g(X)]$, where $X$ is a Poisson random variable, $X \\sim \\mathrm{Poisson}(\\lambda)$, and the function $g$ is defined as $g(x) = \\dfrac{x}{1+x}$ for $x$ in the set of non-negative integers $\\{0, 1, 2, \\ldots\\}$. The parameter $\\lambda$ is a positive real number, $\\lambda  0$.\n\nBy the definition of the expectation of a function of a discrete random variable, we have:\n$$\n\\mathbb{E}[g(X)] = \\sum_{k=0}^{\\infty} g(k) P(X=k)\n$$\nThe probability mass function (PMF) for a Poisson random variable $X$ with parameter $\\lambda$ is given by:\n$$\nP(X=k) = \\frac{\\lambda^k \\exp(-\\lambda)}{k!} \\quad \\text{for } k \\in \\{0, 1, 2, \\ldots\\}\n$$\nSubstituting the expressions for $g(k)$ and $P(X=k)$ into the definition of expectation, we obtain:\n$$\n\\mathbb{E}[g(X)] = \\sum_{k=0}^{\\infty} \\left(\\frac{k}{1+k}\\right) \\frac{\\lambda^k \\exp(-\\lambda)}{k!}\n$$\nWe can factor the constant term $\\exp(-\\lambda)$ out of the summation:\n$$\n\\mathbb{E}[g(X)] = \\exp(-\\lambda) \\sum_{k=0}^{\\infty} \\frac{k}{1+k} \\frac{\\lambda^k}{k!}\n$$\nThe first term of the series, for $k=0$, is $\\frac{0}{1+0} \\frac{\\lambda^0}{0!} = 0$. Thus, the summation can start from $k=1$:\n$$\n\\mathbb{E}[g(X)] = \\exp(-\\lambda) \\sum_{k=1}^{\\infty} \\frac{k}{1+k} \\frac{\\lambda^k}{k!}\n$$\nTo proceed, we perform an algebraic manipulation on the term $\\frac{k}{1+k}$:\n$$\n\\frac{k}{1+k} = \\frac{(k+1)-1}{1+k} = 1 - \\frac{1}{1+k}\n$$\nSubstituting this back into the summation gives:\n$$\n\\mathbb{E}[g(X)] = \\exp(-\\lambda) \\sum_{k=1}^{\\infty} \\left(1 - \\frac{1}{1+k}\\right) \\frac{\\lambda^k}{k!}\n$$\nWe can split this into two separate summations:\n$$\n\\mathbb{E}[g(X)] = \\exp(-\\lambda) \\left( \\sum_{k=1}^{\\infty} \\frac{\\lambda^k}{k!} - \\sum_{k=1}^{\\infty} \\frac{1}{1+k} \\frac{\\lambda^k}{k!} \\right)\n$$\nThis manipulation is permissible because both individual series converge absolutely for any finite $\\lambda  0$. The first series is a component of the Taylor series for $\\exp(\\lambda)$, and the absolute convergence of the second series can be confirmed by the ratio test.\n\nLet us evaluate the first summation, $S_1 = \\sum_{k=1}^{\\infty} \\frac{\\lambda^k}{k!}$. We recall the well-known Taylor series expansion for the exponential function around $0$:\n$$\n\\exp(\\lambda) = \\sum_{k=0}^{\\infty} \\frac{\\lambda^k}{k!} = \\frac{\\lambda^0}{0!} + \\sum_{k=1}^{\\infty} \\frac{\\lambda^k}{k!} = 1 + S_1\n$$\nTherefore, the first summation is:\n$$\nS_1 = \\exp(\\lambda) - 1\n$$\nNext, let us evaluate the second summation, $S_2 = \\sum_{k=1}^{\\infty} \\frac{1}{1+k} \\frac{\\lambda^k}{k!}$. We can simplify the term inside the summation:\n$$\n\\frac{1}{1+k} \\frac{\\lambda^k}{k!} = \\frac{\\lambda^k}{(k+1)k!} = \\frac{\\lambda^k}{(k+1)!}\n$$\nSo the second summation becomes:\n$$\nS_2 = \\sum_{k=1}^{\\infty} \\frac{\\lambda^k}{(k+1)!}\n$$\nTo relate this to the exponential series, we need the power of $\\lambda$ to match the argument of the factorial. We can multiply and divide by $\\lambda \\neq 0$:\n$$\nS_2 = \\frac{1}{\\lambda} \\sum_{k=1}^{\\infty} \\frac{\\lambda^{k+1}}{(k+1)!}\n$$\nLet's introduce a new index $j = k+1$. As $k$ goes from $1$ to $\\infty$, $j$ goes from $2$ to $\\infty$:\n$$\nS_2 = \\frac{1}{\\lambda} \\sum_{j=2}^{\\infty} \\frac{\\lambda^j}{j!}\n$$\nWe can express this sum in terms of the full exponential series:\n$$\n\\sum_{j=2}^{\\infty} \\frac{\\lambda^j}{j!} = \\left(\\sum_{j=0}^{\\infty} \\frac{\\lambda^j}{j!}\\right) - \\frac{\\lambda^0}{0!} - \\frac{\\lambda^1}{1!} = \\exp(\\lambda) - 1 - \\lambda\n$$\nSubstituting this result back into the expression for $S_2$:\n$$\nS_2 = \\frac{1}{\\lambda} (\\exp(\\lambda) - 1 - \\lambda)\n$$\nNow, we combine the results for $S_1$ and $S_2$ into the expression for $\\mathbb{E}[g(X)]$:\n$$\n\\mathbb{E}[g(X)] = \\exp(-\\lambda) (S_1 - S_2) = \\exp(-\\lambda) \\left[ (\\exp(\\lambda) - 1) - \\frac{1}{\\lambda}(\\exp(\\lambda) - 1 - \\lambda) \\right]\n$$\nDistributing the $\\exp(-\\lambda)$ term:\n$$\n\\mathbb{E}[g(X)] = (\\exp(-\\lambda)\\exp(\\lambda) - \\exp(-\\lambda)) - \\frac{\\exp(-\\lambda)}{\\lambda}(\\exp(\\lambda) - 1 - \\lambda)\n$$\n$$\n\\mathbb{E}[g(X)] = (1 - \\exp(-\\lambda)) - \\frac{1}{\\lambda}(1 - \\exp(-\\lambda) - \\lambda\\exp(-\\lambda))\n$$\nDistributing the $-\\frac{1}{\\lambda}$ term:\n$$\n\\mathbb{E}[g(X)] = 1 - \\exp(-\\lambda) - \\frac{1}{\\lambda} + \\frac{\\exp(-\\lambda)}{\\lambda} + \\frac{\\lambda\\exp(-\\lambda)}{\\lambda}\n$$\n$$\n\\mathbb{E}[g(X)] = 1 - \\exp(-\\lambda) - \\frac{1}{\\lambda} + \\frac{\\exp(-\\lambda)}{\\lambda} + \\exp(-\\lambda)\n$$\nThe terms $-\\exp(-\\lambda)$ and $+\\exp(-\\lambda)$ cancel out:\n$$\n\\mathbb{E}[g(X)] = 1 - \\frac{1}{\\lambda} + \\frac{\\exp(-\\lambda)}{\\lambda}\n$$\nCombining the terms with $\\lambda$ in the denominator, we arrive at the final closed-form expression:\n$$\n\\mathbb{E}[g(X)] = 1 - \\frac{1 - \\exp(-\\lambda)}{\\lambda}\n$$",
            "answer": "$$\n\\boxed{1 - \\frac{1 - \\exp(-\\lambda)}{\\lambda}}\n$$"
        },
        {
            "introduction": "Aggregate data can often be misleading, and one of the most famous examples of this is Simpson's paradox. This practice  uses a realistic synthetic biology dataset to demonstrate how a treatment effect observed within different cell-cycle stages can appear to reverse when the data are pooled. Working through this example will provide a concrete understanding of how confounding variables can distort conclusions and why stratified analysis is often essential for correct interpretation.",
            "id": "3907106",
            "problem": "A high-throughput single-cell assay in a synthetic biology experiment evaluates a transcriptional activation circuit that drives a Green Fluorescent Protein (GFP) reporter. Cells are split into two cohorts: a perturbation that recruits an epigenetic activator to the promoter (Treatment) and a promoter-only construct (Control). Because cell cycle stage modulates chromatin accessibility, the data are also stratified by S-phase ($S$) versus G1-phase ($\\mathrm{G1}$), determined by a DNA-content stain. A cell is counted as reporter-positive if its GFP exceeds a pre-specified threshold.\n\nThe following counts were obtained:\n- $S$:\n  - Treatment: $700$ cells total, $196$ reporter-positive.\n  - Control: $300$ cells total, $90$ reporter-positive.\n- $\\mathrm{G1}$:\n  - Treatment: $300$ cells total, $27$ reporter-positive.\n  - Control: $700$ cells total, $70$ reporter-positive.\n\nAssume each cell is an independent and identically distributed draw from its corresponding cohort-stage subpopulation. Based on the fundamental definition of conditional probability and the law of total probability applied to these data, determine which statements are true.\n\nA. Within both $S$ and $\\mathrm{G1}$, the probability of reporter activation given Treatment is lower than the probability given Control.\n\nB. Aggregated over $S$ and $\\mathrm{G1}$ (ignoring stage), the probability of reporter activation is higher under Treatment than under Control.\n\nC. The reversal between the within-stage comparisons and the aggregated comparison is an instance of Simpson’s paradox.\n\nD. If Treatment and Control had the same stage distribution (that is, the same proportions in $S$ and $\\mathrm{G1}$), then the aggregated comparison would align with the within-stage comparisons.\n\nE. The paradoxical reversal occurs because the law of total probability is violated in these data.",
            "solution": "We start from the definition of conditional probability. For any event $A$ and conditioning event $B$ with $P(B) gt; 0$, the conditional probability is $P(A \\mid B) = \\dfrac{P(A \\cap B)}{P(B)}$. When we have a partition of the sample space into disjoint events $\\{B_i\\}$ with $\\sum_i P(B_i) = 1$, the law of total probability states that for any event $A$, $P(A) = \\sum_i P(A \\mid B_i) P(B_i)$.\n\nWe interpret the empirical frequencies as probability estimates under the independent and identically distributed assumption for cells within each cohort-stage stratum. Thus, stage-specific activation probabilities are given by reporter-positive counts divided by totals within each stratum.\n\nCompute the stage-specific activation probabilities:\n\n- In $S$:\n  - Treatment: $P(\\text{On} \\mid \\text{Trt}, S) \\approx \\dfrac{196}{700} = 0.28$.\n  - Control: $P(\\text{On} \\mid \\text{Ctl}, S) \\approx \\dfrac{90}{300} = 0.30$.\n  Therefore, in $S$, Treatment has a lower activation probability than Control.\n\n- In $\\mathrm{G1}$:\n  - Treatment: $P(\\text{On} \\mid \\text{Trt}, \\mathrm{G1}) \\approx \\dfrac{27}{300} = 0.09$.\n  - Control: $P(\\text{On} \\mid \\text{Ctl}, \\mathrm{G1}) \\approx \\dfrac{70}{700} = 0.10$.\n  Therefore, in $\\mathrm{G1}$, Treatment also has a lower activation probability than Control.\n\nNow compute the aggregated activation probabilities by ignoring stage. This uses the law of total probability with weights equal to the stage proportions within each cohort.\n\nFor Treatment:\n- Total Treatment cells: $700 + 300 = 1000$.\n- Total Treatment positives: $196 + 27 = 223$.\n- Aggregated $P(\\text{On} \\mid \\text{Trt}) \\approx \\dfrac{223}{1000} = 0.223$.\n\nFor Control:\n- Total Control cells: $300 + 700 = 1000$.\n- Total Control positives: $90 + 70 = 160$.\n- Aggregated $P(\\text{On} \\mid \\text{Ctl}) \\approx \\dfrac{160}{1000} = 0.160$.\n\nThus, aggregated over stages, Treatment exhibits a higher activation probability than Control, despite being lower within each stage. This is the hallmark of Simpson’s paradox: a trend that appears in several groups reverses when the data are combined, driven by differing group proportions.\n\nWe also analyze the counterfactual in which Treatment and Control share the same stage distribution. Let $w_S$ denote the common fraction in $S$ and $w_{\\mathrm{G1}} = 1 - w_S$ the fraction in $\\mathrm{G1}$. Then the aggregated difference would be\n$$\n\\Delta = \\bigl[w_S \\, P(\\text{On} \\mid \\text{Trt}, S) + w_{\\mathrm{G1}} \\, P(\\text{On} \\mid \\text{Trt}, \\mathrm{G1})\\bigr] - \\bigl[w_S \\, P(\\text{On} \\mid \\text{Ctl}, S) + w_{\\mathrm{G1}} \\, P(\\text{On} \\mid \\text{Ctl}, \\mathrm{G1})\\bigr].\n$$\nRearranging,\n$$\n\\Delta = w_S \\bigl[P(\\text{On} \\mid \\text{Trt}, S) - P(\\text{On} \\mid \\text{Ctl}, S)\\bigr] + w_{\\mathrm{G1}} \\bigl[P(\\text{On} \\mid \\text{Trt}, \\mathrm{G1}) - P(\\text{On} \\mid \\text{Ctl}, \\mathrm{G1})\\bigr].\n$$\nHere, $P(\\text{On} \\mid \\text{Trt}, S) - P(\\text{On} \\mid \\text{Ctl}, S) = 0.28 - 0.30 = -0.02$ and $P(\\text{On} \\mid \\text{Trt}, \\mathrm{G1}) - P(\\text{On} \\mid \\text{Ctl}, \\mathrm{G1}) = 0.09 - 0.10 = -0.01$, both negative. With $w_S \\in [0,1]$ and $w_{\\mathrm{G1}} \\in [0,1]$, $\\Delta$ is a convex combination of negative numbers and must be negative. Therefore, if stage distributions were equal between cohorts, the aggregated comparison would agree with the within-stage comparisons, showing Treatment lower than Control.\n\nFinally, the law of total probability is not violated. The aggregated probabilities above are computed precisely by summing stage-specific probabilities weighted by the cohort-specific stage proportions. The reversal arises because the cohorts have different stage distributions (Treatment is overrepresented in the higher-activation stage $S$), not because of any probabilistic law being broken.\n\nOption-by-option analysis:\n\n- Option A: Correct. We computed $P(\\text{On} \\mid \\text{Trt}, S) = 0.28 lt; 0.30 = P(\\text{On} \\mid \\text{Ctl}, S)$ and $P(\\text{On} \\mid \\text{Trt}, \\mathrm{G1}) = 0.09 lt; 0.10 = P(\\text{On} \\mid \\text{Ctl}, \\mathrm{G1})$.\n\n- Option B: Correct. Aggregated, $P(\\text{On} \\mid \\text{Trt}) = 0.223 gt; 0.160 = P(\\text{On} \\mid \\text{Ctl})$.\n\n- Option C: Correct. The within-stage disadvantage for Treatment reverses to an apparent advantage when data are aggregated, which is Simpson’s paradox.\n\n- Option D: Correct. With matched stage distributions, the aggregated difference is a weighted average of negative within-stage differences and remains negative.\n\n- Option E: Incorrect. The law of total probability holds; the paradox arises from different mixing proportions across stages, not from any violation of probabilistic laws.",
            "answer": "$$\\boxed{ABCD}$$"
        },
        {
            "introduction": "Conditioning on a common effect of two independent causes—a 'collider'—can create spurious statistical associations. This phenomenon, known as collider bias or the 'explaining away' effect, is a critical concept for interpreting data from experiments involving selection, such as Fluorescence-Activated Cell Sorting (FACS). In this practice , you will explore scenarios where gating on a specific cellular phenotype makes two initially independent processes appear dependent, revealing how the experimental design itself can shape the data's correlational structure.",
            "id": "3907128",
            "problem": "In synthetic biology modeling, Fluorescence-Activated Cell Sorting (FACS) is used to select cells based on fluorescence readouts. Consider two reporter intensities modeled as random variables $X$ and $Y$ representing independent pathway activities before selection. A gating variable $Z$ encodes the selection rule applied by FACS. Independence means that for all measurable sets $A$ and $B$, $\\mathbb{P}(X\\in A, Y\\in B)=\\mathbb{P}(X\\in A)\\mathbb{P}(Y\\in B)$, and conditional independence given $Z$ means that for all measurable $A$ and $B$ and values $z$, $\\mathbb{P}(X\\in A, Y\\in B \\mid Z=z)=\\mathbb{P}(X\\in A \\mid Z=z)\\mathbb{P}(Y\\in B \\mid Z=z)$.\n\nSelect all options that construct a scientifically plausible FACS selection scenario in which $X\\perp Y$ in the full population but $X\\not\\perp Y\\mid Z$ within at least one selected stratum of $Z$.\n\nA. $X$ and $Y$ are independent Bernoulli random variables with parameter $p\\in(0,1)$, modeling two independent reporter-positivity events. The gate selects any cell positive for at least one reporter: $Z=\\mathbb{I}\\{X+Y\\ge 1\\}$.\n\nB. $X$ and $Y$ are independent Bernoulli random variables with parameter $p\\in(0,1)$. The gate selects cells based solely on reporter $X$: $Z=\\mathbb{I}\\{X=1\\}$.\n\nC. $X\\sim\\mathcal{N}(0,1)$ and $Y\\sim\\mathcal{N}(0,1)$ are independent standard normal random variables representing two independent fluorescence channels after appropriate normalization. The gate selects cells with total fluorescence above a threshold $c\\in\\mathbb{R}$: $Z=\\mathbb{I}\\{X+Yc\\}$.\n\nD. $X\\sim\\mathcal{N}(0,1)$ and $Y\\sim\\mathcal{N}(0,1)$ are independent standard normal random variables as above. $Z$ is an independent coin flip, $Z\\sim\\text{Bernoulli}(q)$ with $q\\in(0,1)$, representing an unrelated metadata flag.\n\nE. $X\\sim\\text{Poisson}(\\lambda_1)$ and $Y\\sim\\text{Poisson}(\\lambda_2)$ are independent counts representing molecule copy numbers for two gene products. The gate records the total count $Z=X+Y$ and analysis is restricted to a fixed stratum $Z=s$ for some $s\\in\\mathbb{N}$.",
            "solution": "We begin from the core probabilistic definitions. Independence of $X$ and $Y$ means $\\mathbb{P}(X\\in A, Y\\in B)=\\mathbb{P}(X\\in A)\\mathbb{P}(Y\\in B)$ for all measurable $A$ and $B$. Conditional independence of $X$ and $Y$ given $Z=z$ means that for all measurable $A$ and $B$, $\\mathbb{P}(X\\in A, Y\\in B \\mid Z=z)=\\mathbb{P}(X\\in A \\mid Z=z)\\mathbb{P}(Y\\in B \\mid Z=z)$. We will use the definition of conditional probability, $\\mathbb{P}(A\\mid B)=\\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(B)}$ when $\\mathbb{P}(B)0$, and the law of total probability where appropriate.\n\nOption A analysis:\n- Unconditional independence: By construction, $X$ and $Y$ are independent Bernoulli random variables with parameter $p\\in(0,1)$, so for any $x,y\\in\\{0,1\\}$, $\\mathbb{P}(X=x, Y=y)=\\mathbb{P}(X=x)\\mathbb{P}(Y=y)$; thus $X\\perp Y$.\n- Conditional dependence given $Z=1$: Here $Z=\\mathbb{I}\\{X+Y\\ge 1\\}$. We compute $\\mathbb{P}(Y=1\\mid Z=1)$ and $\\mathbb{P}(Y=1\\mid X=1, Z=1)$ and show they differ. First,\n$$\n\\mathbb{P}(Z=1)=1-\\mathbb{P}(X=0, Y=0)=1-(1-p)^2=2p-p^2.\n$$\nAlso $\\mathbb{P}(Y=1, Z=1)=\\mathbb{P}(Y=1)=p$, since $Y=1$ implies $Z=1$. Thus\n$$\n\\mathbb{P}(Y=1\\mid Z=1)=\\frac{\\mathbb{P}(Y=1, Z=1)}{\\mathbb{P}(Z=1)}=\\frac{p}{2p-p^2}=\\frac{1}{2-p}.\n$$\nNext, conditioning on $X=1$ renders $Z=1$ certain, and by unconditional independence, $\\mathbb{P}(Y=1\\mid X=1)=p$. Therefore\n$$\n\\mathbb{P}(Y=1\\mid X=1, Z=1)=p\\neq \\frac{1}{2-p}\n$$\nfor all $p\\in(0,1)$. Because the conditional distribution of $Y$ given $Z=1$ changes when conditioning further on $X=1$, the factorization required for conditional independence fails, so $X\\not\\perp Y\\mid Z$. Verdict: Correct.\n\nOption B analysis:\n- Unconditional independence: As in Option A, $X$ and $Y$ are independent Bernoulli with parameter $p$, hence $X\\perp Y$.\n- Conditional independence: Here $Z=\\mathbb{I}\\{X=1\\}$, so $Z=X$ deterministically. For any measurable $A\\subseteq\\{0,1\\}$ and $B\\subseteq\\{0,1\\}$, and for $z\\in\\{0,1\\}$,\n$$\n\\mathbb{P}(X\\in A, Y\\in B \\mid Z=z)=\\mathbb{P}(X\\in A, Y\\in B \\mid X=z).\n$$\nSince $Y\\perp X$, $\\mathbb{P}(Y\\in B \\mid X=z)=\\mathbb{P}(Y\\in B)$, and $X$ given $Z=z$ is degenerate at $x=z$. Thus\n$$\n\\mathbb{P}(X\\in A, Y\\in B \\mid Z=z)=\\mathbb{P}(X\\in A \\mid Z=z)\\mathbb{P}(Y\\in B \\mid Z=z),\n$$\nwith $\\mathbb{P}(X\\in A \\mid Z=z)$ equal to $1$ if $z\\in A$ and $0$ otherwise, and $\\mathbb{P}(Y\\in B \\mid Z=z)=\\mathbb{P}(Y\\in B)$. Hence the required factorization holds, so $X\\perp Y\\mid Z$. Verdict: Incorrect.\n\nOption C analysis:\n- Unconditional independence: By construction, $X$ and $Y$ are independent standard normal random variables, hence $X\\perp Y$.\n- Conditional dependence for the gated stratum $Z=1$: The gate $Z=\\mathbb{I}\\{X+Yc\\}$ encodes selection on the sum. We demonstrate non-independence by showing that for a measurable event $B=\\{Yt\\}$, $\\mathbb{P}(Yt\\mid X=x, Z=1)$ depends on $x$. For fixed $x\\in\\mathbb{R}$,\n$$\n\\{Z=1\\}=\\{X+Yc\\}=\\{Yc-x\\}.\n$$\nTherefore, by conditional probability,\n$$\n\\mathbb{P}(Yt \\mid X=x, Z=1)=\\frac{\\mathbb{P}(Yt, Yc-x \\mid X=x)}{\\mathbb{P}(Yc-x \\mid X=x)}.\n$$\nUsing unconditional independence, the distribution of $Y$ does not depend on $X$, so\n$$\n\\mathbb{P}(Yt \\mid X=x, Z=1)=\\frac{\\mathbb{P}(Y\\max\\{t, c-x\\})}{\\mathbb{P}(Yc-x)}.\n$$\nWith $Y\\sim \\mathcal{N}(0,1)$, writing the survival function as $\\bar{\\Phi}(u)=\\mathbb{P}(Yu)$,\n$$\n\\mathbb{P}(Yt \\mid X=x, Z=1)=\\frac{\\bar{\\Phi}(\\max\\{t, c-x\\})}{\\bar{\\Phi}(c-x)}.\n$$\nThe right-hand side is a non-constant function of $x$ for any fixed $t$ (for instance choose $t=c$ so that $\\max\\{t,c-x\\}=c$ when $x\\ge 0$ and equals $c-x$ when $x0$). Because $\\mathbb{P}(Yt \\mid X=x, Z=1)$ depends on $x$, the factorization $\\mathbb{P}(X\\in A, Y\\in B \\mid Z=1)=\\mathbb{P}(X\\in A\\mid Z=1)\\mathbb{P}(Y\\in B\\mid Z=1)$ fails for appropriate $A$ and $B$, and thus $X\\not\\perp Y\\mid Z$. Intuitively, conditioning on $X+Yc$ induces a negative association: larger $X$ requires less contribution from $Y$ to exceed the threshold. Verdict: Correct.\n\nOption D analysis:\n- Unconditional independence: $X$ and $Y$ are independent standard normal random variables, hence $X\\perp Y$.\n- Conditional independence: $Z$ is independent of $(X,Y)$, so for any measurable sets $A,B$ and any $z$ with $\\mathbb{P}(Z=z)0$,\n$$\n\\mathbb{P}(X\\in A, Y\\in B \\mid Z=z)=\\frac{\\mathbb{P}(X\\in A, Y\\in B, Z=z)}{\\mathbb{P}(Z=z)}=\\frac{\\mathbb{P}(X\\in A, Y\\in B)\\,\\mathbb{P}(Z=z)}{\\mathbb{P}(Z=z)}=\\mathbb{P}(X\\in A, Y\\in B).\n$$\nSimilarly,\n$$\n\\mathbb{P}(X\\in A \\mid Z=z)=\\mathbb{P}(X\\in A),\\quad \\mathbb{P}(Y\\in B \\mid Z=z)=\\mathbb{P}(Y\\in B).\n$$\nThus,\n$$\n\\mathbb{P}(X\\in A, Y\\in B \\mid Z=z)=\\mathbb{P}(X\\in A)\\mathbb{P}(Y\\in B)=\\mathbb{P}(X\\in A \\mid Z=z)\\mathbb{P}(Y\\in B \\mid Z=z),\n$$\nso $X\\perp Y\\mid Z$. Verdict: Incorrect.\n\nOption E analysis:\n- Unconditional independence: $X$ and $Y$ are independent Poisson random variables with parameters $\\lambda_10$ and $\\lambda_20$, so $X\\perp Y$.\n- Conditional dependence given a fixed total $Z=s$: The gate records $Z=X+Y$ and analysis focuses on the stratum $Z=s$. Using the addition property of independent Poisson random variables, $Z\\sim \\text{Poisson}(\\lambda_1+\\lambda_2)$. The conditional distribution of $X$ given $Z=s$ is binomial,\n$$\n\\mathbb{P}(X=x \\mid Z=s)=\\binom{s}{x}\\left(\\frac{\\lambda_1}{\\lambda_1+\\lambda_2}\\right)^x\\left(\\frac{\\lambda_2}{\\lambda_1+\\lambda_2}\\right)^{s-x},\\quad x\\in\\{0,1,\\dots,s\\},\n$$\nand $Y$ is deterministically $Y=s-X$ given $Z=s$. Hence,\n$$\n\\operatorname{Cov}(X,Y \\mid Z=s)=\\operatorname{Cov}(X, s-X \\mid Z=s)=\\operatorname{Cov}(X,s\\mid Z=s)-\\operatorname{Var}(X\\mid Z=s)=-\\operatorname{Var}(X\\mid Z=s)0,\n$$\nbecause $\\operatorname{Var}(X\\mid Z=s)=s\\frac{\\lambda_1}{\\lambda_1+\\lambda_2}\\left(1-\\frac{\\lambda_1}{\\lambda_1+\\lambda_2}\\right)0$ for $\\lambda_1,\\lambda_20$ and $s\\ge 1$. Nonzero covariance under conditioning implies failure of conditional independence. Equivalently, the joint conditional distribution concentrates on the line $x+y=s$, violating any product factorization. Thus $X\\not\\perp Y\\mid Z$. Verdict: Correct.\n\nIn summary, Options A, C, and E satisfy $X\\perp Y$ while $X\\not\\perp Y\\mid Z$ through selection on a collider or fixed-sum gate that is scientifically credible in FACS-based cell sorting. Options B and D do not induce conditional dependence, since gating on $X$ alone or an independent metadata flag preserves $X\\perp Y\\mid Z$.",
            "answer": "$$\\boxed{ACE}$$"
        }
    ]
}