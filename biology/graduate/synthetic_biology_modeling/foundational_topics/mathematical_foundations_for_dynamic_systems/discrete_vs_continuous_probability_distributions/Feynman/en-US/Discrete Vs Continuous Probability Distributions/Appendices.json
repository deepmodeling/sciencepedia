{
    "hands_on_practices": [
        {
            "introduction": "We begin by exploring the fundamental properties of discrete probability distributions, which are the natural language for describing molecular counts in biological systems. This first practice focuses on the crucial relationship between the probability mass function (PMF), which assigns probabilities to individual counts, and the cumulative distribution function (CDF). By analyzing the 'jumps' in a given CDF for transcriptional bursts, you will see how to recover the exact probability of observing a specific number of molecules, reinforcing a core concept that distinguishes discrete from continuous variables .",
            "id": "3911693",
            "problem": "In a stochastic gene expression model for a single promoter in a bacterial cell, transcriptional activity occurs in bursts. Each promoter activation event is independently silent with probability $\\pi \\in (0,1)$ (no messenger ribonucleic acid molecules are produced), and otherwise produces a count of messenger ribonucleic acid molecules following a geometric law on $\\{1,2,\\dots\\}$ with parameter $\\alpha \\in (0,1)$. Let $B$ denote the observed number of molecules produced in a single activation event. Because $B$ is integer-valued, its cumulative distribution function (CDF) $F(x)$ is right-continuous and piecewise constant. The experiment provides the CDF over the real line:\n- $F(x)=0$ for $x<0$,\n- $F(x)=\\pi$ for $0 \\le x < 1$,\n- $F(x)=\\pi + (1-\\pi)\\left[1-(1-\\alpha)^{\\lfloor x \\rfloor}\\right]$ for $x \\ge 1$, where $\\lfloor x \\rfloor$ is the greatest integer less than or equal to $x$.\n\nUsing only the foundational definitions of the probability mass function (PMF) and the cumulative distribution function (CDF), and the right-continuity of distribution functions on $\\mathbb{R}$, derive the probability mass at $3$, namely $P(B=3)$, directly from the jump structure of $F(x)$. Express your final answer as a single simplified analytic expression in terms of $\\pi$ and $\\alpha$. Do not round your answer.",
            "solution": "The problem asks for the probability mass at the integer value $3$, denoted as $P(B=3)$, for the discrete random variable $B$. The derivation must be based on the provided cumulative distribution function (CDF), $F(x)$, and the foundational relationship between the CDF and the probability mass function (PMF) for a discrete random variable.\n\nFor any discrete random variable $X$ taking values in the integers, the probability mass at an integer $k$ is equal to the size of the jump in its CDF, $F_X(x)$, at that point. This is expressed by the formula:\n$$P(X=k) = F_X(k) - F_X(k^-)$$\nwhere $F_X(k) = P(X \\le k)$ and $F_X(k^-) = \\lim_{x \\to k^-} F_X(x)$ is the limit of the CDF as $x$ approaches $k$ from the left. The problem states that the CDF is right-continuous, which is a general property of all CDFs.\n\nWe apply this principle to find $P(B=3)$, where $B$ is the number of messenger ribonucleic acid molecules. We need to calculate:\n$$P(B=3) = F(3) - F(3^-)$$\n\nFirst, we calculate the value of the CDF at $x=3$, denoted $F(3)$. The point $x=3$ falls into the domain $x \\ge 1$. The CDF for this domain is given as:\n$$F(x) = \\pi + (1-\\pi)\\left[1-(1-\\alpha)^{\\lfloor x \\rfloor}\\right]$$\nSubstituting $x=3$, we note that the floor function $\\lfloor 3 \\rfloor = 3$. Therefore,\n$$F(3) = \\pi + (1-\\pi)\\left[1-(1-\\alpha)^{3}\\right]$$\n\nNext, we calculate the limit of the CDF as $x$ approaches $3$ from the left, denoted $F(3^-)$. This requires evaluating $F(x)$ for values of $x$ in an open interval immediately to the left of $3$, such as $x \\in (3-\\epsilon, 3)$ for some small $\\epsilon > 0$. Let's consider any $x$ in the interval $[2, 3)$. For all $x$ in this interval, the floor function evaluates to $\\lfloor x \\rfloor = 2$. Since this interval is contained within the domain $x \\ge 1$, we use the same formula for $F(x)$:\n$$F(x) = \\pi + (1-\\pi)\\left[1-(1-\\alpha)^{2}\\right] \\quad \\text{for } x \\in [2, 3)$$\nSince $F(x)$ is constant on this interval, its limit as $x$ approaches $3$ from the left is this constant value:\n$$F(3^-) = \\lim_{x \\to 3^-} F(x) = \\pi + (1-\\pi)\\left[1-(1-\\alpha)^{2}\\right]$$\n\nFinally, we compute the probability mass $P(B=3)$ by taking the difference between $F(3)$ and $F(3^-)$:\n$$P(B=3) = F(3) - F(3^-)$$\n$$P(B=3) = \\left( \\pi + (1-\\pi)\\left[1-(1-\\alpha)^{3}\\right] \\right) - \\left( \\pi + (1-\\pi)\\left[1-(1-\\alpha)^{2}\\right] \\right)$$\nThe term $\\pi$ cancels out:\n$$P(B=3) = (1-\\pi)\\left[1-(1-\\alpha)^{3}\\right] - (1-\\pi)\\left[1-(1-\\alpha)^{2}\\right]$$\nWe can factor out the common term $(1-\\pi)$:\n$$P(B=3) = (1-\\pi) \\left( \\left[1-(1-\\alpha)^{3}\\right] - \\left[1-(1-\\alpha)^{2}\\right] \\right)$$\nNow, we simplify the expression inside the parentheses:\n$$P(B=3) = (1-\\pi) \\left( 1 - (1-\\alpha)^{3} - 1 + (1-\\alpha)^{2} \\right)$$\n$$P(B=3) = (1-\\pi) \\left( (1-\\alpha)^{2} - (1-\\alpha)^{3} \\right)$$\nWe can factor out $(1-\\alpha)^{2}$ from the terms in the parentheses:\n$$P(B=3) = (1-\\pi)(1-\\alpha)^{2} \\left( 1 - (1-\\alpha) \\right)$$\nSimplifying the final term:\n$$P(B=3) = (1-\\pi)(1-\\alpha)^{2} (1 - 1 + \\alpha)$$\n$$P(B=3) = (1-\\pi)(1-\\alpha)^{2} \\alpha$$\nThis simplifies to the final analytical expression.",
            "answer": "$$\\boxed{\\alpha(1-\\pi)(1-\\alpha)^2}$$"
        },
        {
            "introduction": "Many biological measurements involve converting a continuous physical signal into a discrete digital reading. This exercise provides a tangible model for this process, exploring how a continuous temperature reading, described by a uniform probability distribution, becomes a discrete value through rounding . By calculating the resulting probability mass function, you will gain hands-on experience in translating between continuous and discrete domains and see firsthand how the process of discretization can introduce non-uniformity.",
            "id": "1896422",
            "problem": "A digital environmental sensor is designed to monitor temperatures within a laboratory chamber. The true temperature $T$, in degrees Celsius, is a continuous random variable that, due to controlled environmental fluctuations, follows a uniform distribution over the interval $[20.2, 23.6]$. The sensor's display, however, shows a discrete reading $R$ which is the value of $T$ rounded to the nearest integer. The rounding rule specifies that values exactly at a half-integer (e.g., 21.5) are rounded up to the next integer.\n\nDetermine the probability mass function for the displayed reading $R$. Present the probabilities for all possible values of $R$, ordered from smallest to largest, as a row matrix. Express all probabilities as fractions in their simplest form.",
            "solution": "Let $T$ be uniformly distributed on $[20.2,23.6]$. We write the endpoints as fractions: $a=20.2=\\frac{101}{5}$ and $b=23.6=\\frac{118}{5}$. Then the density of $T$ is\n$$\nf_{T}(t)=\\frac{1}{b-a}=\\frac{1}{\\frac{118}{5}-\\frac{101}{5}}=\\frac{1}{\\frac{17}{5}}=\\frac{5}{17}, \\quad t\\in\\left[\\frac{101}{5},\\frac{118}{5}\\right].\n$$\nThe rounding rule is to the nearest integer with half-integers rounded up. Hence, for an integer $r$, the event $\\{R=r\\}$ corresponds to $T\\in[r-\\frac{1}{2},r+\\frac{1}{2})$, except that the intersection with $[a,b]$ must be taken. The attainable integer readings are $r\\in\\{20,21,22,23,24\\}$, with the following intervals and lengths:\n- For $r=20$: $\\left[\\max\\!\\left(\\frac{101}{5},20-\\frac{1}{2}\\right),\\min\\!\\left(\\frac{118}{5},20+\\frac{1}{2}\\right)\\right)=\\left[\\frac{101}{5},\\frac{41}{2}\\right)$, length $\\frac{3}{10}$.\n- For $r=21$: $\\left[\\frac{41}{2},\\frac{43}{2}\\right)$, length $\\frac{43}{2}-\\frac{41}{2}=1$.\n- For $r=22$: $\\left[\\frac{43}{2},\\frac{45}{2}\\right)$, length $\\frac{45}{2}-\\frac{43}{2}=1$.\n- For $r=23$: $\\left[\\frac{45}{2},\\frac{47}{2}\\right)$, length $\\frac{47}{2}-\\frac{45}{2}=1$.\n- For $r=24$: $\\left[\\frac{47}{2},\\frac{118}{5}\\right]$, length $\\frac{118}{5}-\\frac{47}{2}=\\frac{1}{10}$.\n\nLet $L=b-a=\\frac{17}{5}$. For each $r$, $P(R=r)=\\text{length}/L$. Therefore,\n$$\nP(R=20)=\\frac{\\frac{3}{10}}{\\frac{17}{5}}=\\frac{3}{10}\\cdot\\frac{5}{17}=\\frac{3}{34},\\quad\nP(R=21)=\\frac{1}{\\frac{17}{5}}=\\frac{5}{17},\n$$\n$$\nP(R=22)=\\frac{1}{\\frac{17}{5}}=\\frac{5}{17},\\quad\nP(R=23)=\\frac{1}{\\frac{17}{5}}=\\frac{5}{17},\\quad\nP(R=24)=\\frac{\\frac{1}{10}}{\\frac{17}{5}}=\\frac{1}{10}\\cdot\\frac{5}{17}=\\frac{1}{34}.\n$$\nThese probabilities sum to $1$ and are expressed in simplest fractional form. Ordered from $r=20$ to $r=24$, the row matrix is\n$$\n\\begin{pmatrix}\n\\frac{3}{34} & \\frac{5}{17} & \\frac{5}{17} & \\frac{5}{17} & \\frac{1}{34}\n\\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{3}{34} & \\frac{5}{17} & \\frac{5}{17} & \\frac{5}{17} & \\frac{1}{34}\\end{pmatrix}}$$"
        },
        {
            "introduction": "Building on the previous concepts, we now tackle a more advanced problem that directly contrasts the analytical treatment of continuous and discrete variables. In many experiments, we are interested in the ratio of two quantities, such as the expression levels of two different genes. This practice guides you through deriving the distribution of a ratio for both continuous fluorescence intensities (modeled by Gamma distributions) and discrete molecule counts (modeled by Poisson distributions) . You will discover that while the continuous case yields an elegant, closed-form solution, the discrete case presents fundamental challenges, powerfully illustrating why the choice between a continuous or discrete framework has deep consequences for model tractability.",
            "id": "3911740",
            "problem": "In a synthetic biology experiment, two constitutive promoters drive reporter proteins whose continuous fluorescence intensities are denoted by $C_{1}$ and $C_{2}$. Due to multiplicative biophysical variability and shared instrumentation noise, it is reasonable to model these intensities as independent Gamma-distributed random variables with a common scale parameter: $C_{1} \\sim \\text{Gamma}(\\alpha_{1}, \\theta)$ and $C_{2} \\sim \\text{Gamma}(\\alpha_{2}, \\theta)$, where the shape parameters are $\\alpha_{1} > 0$ and $\\alpha_{2} > 0$, and the scale parameter is $\\theta > 0$. Define the ratio $R = C_{1} / C_{2}$. Using only fundamental definitions of independence, the joint probability density function (PDF), and the change-of-variables formula with an appropriate Jacobian, derive the continuous Probability Density Function (PDF) $f_{R}(r)$ for $r > 0$. Express your final result in a closed form that is fully simplified in terms of the Gamma function and the Beta function.\n\nThen, consider an analogous setup for discrete molecular counts: let $N_{1} \\sim \\text{Poisson}(\\lambda_{1})$ and $N_{2} \\sim \\text{Poisson}(\\lambda_{2})$ be independent counts of messenger ribonucleic acid (mRNA) molecules from the same two promoters, with $\\lambda_{1} > 0$ and $\\lambda_{2} > 0$. Define $Q = N_{1}/N_{2}$. Briefly explain, from first principles of discrete probability, why defining a probability mass function for $Q$ requires careful handling of the support and integer constraints, including the event $N_{2} = 0$.\n\nYour final answer should be the closed-form expression for the continuous PDF $f_{R}(r)$ of the ratio $R$, for $r > 0$. No numerical rounding is required, and no physical units should be included in the final expression.",
            "solution": "### Derivation of the Continuous PDF $f_{R}(r)$\nWe are given two independent random variables, $C_{1} \\sim \\text{Gamma}(\\alpha_{1}, \\theta)$ and $C_{2} \\sim \\text{Gamma}(\\alpha_{2}, \\theta)$. The probability density function for a variable $X \\sim \\text{Gamma}(\\alpha, \\theta)$ is given by:\n$$f_{X}(x) = \\frac{x^{\\alpha-1} \\exp(-x/\\theta)}{\\theta^{\\alpha} \\Gamma(\\alpha)}, \\quad \\text{for } x > 0$$\nwhere $\\Gamma(\\alpha)$ is the Gamma function.\n\nThe individual PDFs for $C_{1}$ and $C_{2}$ are:\n$$f_{C_{1}}(c_{1}) = \\frac{c_{1}^{\\alpha_{1}-1} \\exp(-c_{1}/\\theta)}{\\theta^{\\alpha_{1}} \\Gamma(\\alpha_{1})}, \\quad c_{1} > 0$$\n$$f_{C_{2}}(c_{2}) = \\frac{c_{2}^{\\alpha_{2}-1} \\exp(-c_{2}/\\theta)}{\\theta^{\\alpha_{2}} \\Gamma(\\alpha_{2})}, \\quad c_{2} > 0$$\n\nSince $C_{1}$ and $C_{2}$ are independent, their joint PDF is the product of their individual PDFs:\n$$f_{C_{1}, C_{2}}(c_{1}, c_{2}) = f_{C_{1}}(c_{1}) f_{C_{2}}(c_{2}) = \\frac{c_{1}^{\\alpha_{1}-1} c_{2}^{\\alpha_{2}-1} \\exp(-(c_{1}+c_{2})/\\theta)}{\\theta^{\\alpha_{1}+\\alpha_{2}} \\Gamma(\\alpha_{1}) \\Gamma(\\alpha_{2})}$$\nfor $c_{1} > 0$ and $c_{2} > 0$.\n\nWe seek the PDF of the ratio $R = C_{1} / C_{2}$. We use the change of variables method. Let us introduce an auxiliary variable, for instance, $S = C_{2}$. The transformation is from $(C_{1}, C_{2})$ to $(R, S)$.\nThe inverse transformation is:\n$$C_{1} = RS$$\n$$C_{2} = S$$\nThe domain $c_{1} > 0, c_{2} > 0$ maps to the domain $r > 0, s > 0$.\n\nNext, we compute the Jacobian of this inverse transformation. The Jacobian determinant $J$ is:\n$$J = \\det \\begin{pmatrix} \\frac{\\partial c_{1}}{\\partial r} & \\frac{\\partial c_{1}}{\\partial s} \\\\ \\frac{\\partial c_{2}}{\\partial r} & \\frac{\\partial c_{2}}{\\partial s} \\end{pmatrix} = \\det \\begin{pmatrix} s & r \\\\ 0 & 1 \\end{pmatrix} = (s)(1) - (r)(0) = s$$\nThe absolute value of the Jacobian is $|J| = |s| = s$, since $s=c_{2}>0$.\n\nThe joint PDF of the new variables $(R, S)$ is given by $f_{R,S}(r,s) = f_{C_{1}, C_{2}}(rs, s) |J|$.\n$$f_{R,S}(r,s) = \\frac{(rs)^{\\alpha_{1}-1} s^{\\alpha_{2}-1} \\exp(-(rs+s)/\\theta)}{\\theta^{\\alpha_{1}+\\alpha_{2}} \\Gamma(\\alpha_{1}) \\Gamma(\\alpha_{2})} \\cdot s$$\nGrouping the terms:\n$$f_{R,S}(r,s) = \\frac{r^{\\alpha_{1}-1} s^{\\alpha_{1}-1} s^{\\alpha_{2}-1} s^{1} \\exp(-s(r+1)/\\theta)}{\\theta^{\\alpha_{1}+\\alpha_{2}} \\Gamma(\\alpha_{1}) \\Gamma(\\alpha_{2})}$$\n$$f_{R,S}(r,s) = \\frac{r^{\\alpha_{1}-1} s^{\\alpha_{1}+\\alpha_{2}-1} \\exp(-s(r+1)/\\theta)}{\\theta^{\\alpha_{1}+\\alpha_{2}} \\Gamma(\\alpha_{1}) \\Gamma(\\alpha_{2})}$$\nfor $r>0$ and $s>0$.\n\nTo find the marginal PDF of $R$, $f_{R}(r)$, we integrate the joint PDF $f_{R,S}(r,s)$ with respect to the auxiliary variable $s$ over its entire support $(0, \\infty)$.\n$$f_{R}(r) = \\int_{0}^{\\infty} f_{R,S}(r,s) \\, ds = \\int_{0}^{\\infty} \\frac{r^{\\alpha_{1}-1} s^{\\alpha_{1}+\\alpha_{2}-1} \\exp(-s(r+1)/\\theta)}{\\theta^{\\alpha_{1}+\\alpha_{2}} \\Gamma(\\alpha_{1}) \\Gamma(\\alpha_{2})} \\, ds$$\nWe can separate the terms that do not depend on $s$:\n$$f_{R}(r) = \\frac{r^{\\alpha_{1}-1}}{\\theta^{\\alpha_{1}+\\alpha_{2}} \\Gamma(\\alpha_{1}) \\Gamma(\\alpha_{2})} \\int_{0}^{\\infty} s^{(\\alpha_{1}+\\alpha_{2})-1} \\exp\\left(-\\frac{s(r+1)}{\\theta}\\right) \\, ds$$\nThe integral has the form of the kernel of a Gamma distribution. We use the identity derived from the Gamma function definition: $\\int_0^\\infty x^{A-1} e^{-x/B} dx = B^A \\Gamma(A)$.\nIn our case, the parameters for this identity are:\n- Shape: $A = \\alpha_{1}+\\alpha_{2}$\n- Scale: $B = \\frac{\\theta}{r+1}$\n\nThe integral evaluates to:\n$$\\int_{0}^{\\infty} s^{(\\alpha_{1}+\\alpha_{2})-1} \\exp\\left(-\\frac{s}{\\theta/(r+1)}\\right) \\, ds = \\left(\\frac{\\theta}{r+1}\\right)^{\\alpha_{1}+\\alpha_{2}} \\Gamma(\\alpha_{1}+\\alpha_{2})$$\nSubstituting this result back into the expression for $f_{R}(r)$:\n$$f_{R}(r) = \\frac{r^{\\alpha_{1}-1}}{\\theta^{\\alpha_{1}+\\alpha_{2}} \\Gamma(\\alpha_{1}) \\Gamma(\\alpha_{2})} \\left[ \\left(\\frac{\\theta}{r+1}\\right)^{\\alpha_{1}+\\alpha_{2}} \\Gamma(\\alpha_{1}+\\alpha_{2}) \\right]$$\n$$f_{R}(r) = \\frac{r^{\\alpha_{1}-1}}{\\theta^{\\alpha_{1}+\\alpha_{2}} \\Gamma(\\alpha_{1}) \\Gamma(\\alpha_{2})} \\frac{\\theta^{\\alpha_{1}+\\alpha_{2}}}{(r+1)^{\\alpha_{1}+\\alpha_{2}}} \\Gamma(\\alpha_{1}+\\alpha_{2})$$\nThe term $\\theta^{\\alpha_{1}+\\alpha_{2}}$ cancels out, which is expected as the ratio of two variables with a common scale parameter should be independent of that scale.\n$$f_{R}(r) = \\frac{\\Gamma(\\alpha_{1}+\\alpha_{2})}{\\Gamma(\\alpha_{1}) \\Gamma(\\alpha_{2})} \\frac{r^{\\alpha_{1}-1}}{(r+1)^{\\alpha_{1}+\\alpha_{2}}}$$\nThe problem asks for the result to be expressed in terms of the Beta function, $B(x,y)$, which is defined as $B(x,y) = \\frac{\\Gamma(x)\\Gamma(y)}{\\Gamma(x+y)}$. Therefore, the pre-factor is equal to $1/B(\\alpha_{1}, \\alpha_{2})$.\nThe final expression for the PDF of $R$ is:\n$$f_{R}(r) = \\frac{1}{B(\\alpha_{1}, \\alpha_{2})} \\frac{r^{\\alpha_{1}-1}}{(r+1)^{\\alpha_{1}+\\alpha_{2}}}, \\quad \\text{for } r > 0$$\nThis is the PDF of a Beta prime distribution, also known as the beta distribution of the second kind.\n\n### Discussion of the Discrete Case\nFor the discrete case, we are given two independent random variables, $N_{1} \\sim \\text{Poisson}(\\lambda_{1})$ and $N_{2} \\sim \\text{Poisson}(\\lambda_{2})$. The probability mass function (PMF) for a variable $K \\sim \\text{Poisson}(\\lambda)$ is $P(K=k) = \\frac{\\lambda^{k} \\exp(-\\lambda)}{k!}$ for $k \\in \\{0, 1, 2, \\dots\\}$. We wish to define a PMF for the ratio $Q = N_{1}/N_{2}$. This task presents two fundamental challenges.\n\nFirst, and most critically, is the issue of **division by zero**. The support of $N_{2}$ includes $0$. The probability of this event is non-zero:\n$$P(N_{2}=0) = \\frac{\\lambda_{2}^{0} \\exp(-\\lambda_{2})}{0!} = \\exp(-\\lambda_{2})$$\nSince $\\lambda_{2}>0$, we have $P(N_{2}=0) > 0$. When the event $\\{N_{2}=0\\}$ occurs, the ratio $Q = N_{1}/N_{2}$ is mathematically undefined. A rigorous definition of the random variable $Q$ would require a special rule for this case, such as assigning it to a symbolic value like $\\infty$ (if $N_{1}>0$) or treating it as a failure of the measurement, thus restricting the sample space. This conditioning on $N_{2} \\neq 0$ would mean the variables are no longer governed by a simple Poisson distribution, but by a zero-truncated Poisson distribution, which complicates the analysis. In the continuous case, this was not an issue because the probability of $C_{2}$ being exactly zero is $P(C_2=0)=0$.\n\nSecond, is the nature of the **support of the random variable $Q$**. Even if we condition on $N_{2} \\ge 1$, the possible values of $Q$ are non-negative rational numbers of the form $n_{1}/n_{2}$ where $n_{1} \\in \\{0, 1, 2, \\dots\\}$ and $n_{2} \\in \\{1, 2, 3, \\dots\\}$. While this set is countable, allowing for a PMF to be defined in principle, it is not a simple lattice of integers. It is a dense subset of the non-negative real numbers. To compute the probability for a specific rational value $q=a/b$ (in lowest terms), one must sum over all possible integer multiples $(ka, kb)$:\n$$P(Q = q) = \\sum_{k=1}^{\\infty} P(N_{1}=ka, N_{2}=kb)$$\nDue to independence, this becomes:\n$$P(Q = q) = \\sum_{k=1}^{\\infty} P(N_{1}=ka) P(N_{2}=kb) = \\sum_{k=1}^{\\infty} \\left( \\frac{\\lambda_{1}^{ka} \\exp(-\\lambda_{1})}{(ka)!} \\right) \\left( \\frac{\\lambda_{2}^{kb} \\exp(-\\lambda_{2})}{(kb)!} \\right)$$\n$$P(Q = q) = \\exp(-(\\lambda_{1}+\\lambda_{2})) \\sum_{k=1}^{\\infty} \\frac{\\lambda_{1}^{ka} \\lambda_{2}^{kb}}{(ka)! (kb)!}$$\nThis infinite series is computationally intensive and generally lacks a simple closed-form expression. The complexity of both the support and the calculation of probabilities makes the distribution of the ratio of discrete Poisson variables substantially more difficult to handle than its continuous Gamma-distributed counterpart.",
            "answer": "$$\\boxed{\\frac{1}{B(\\alpha_{1}, \\alpha_{2})} \\frac{r^{\\alpha_{1}-1}}{(r+1)^{\\alpha_{1}+\\alpha_{2}}}}$$"
        }
    ]
}