{
    "hands_on_practices": [
        {
            "introduction": "To understand the information capacity of complex regulatory networks, we first must analyze its most fundamental component: a single promoter regulated by a transcription factor. This exercise guides you from the first principles of statistical mechanics—the thermodynamic model of protein-DNA binding—to the core concepts of information theory. By calculating the mutual information between ligand concentration and promoter occupancy, you will quantify the ultimate physical limit on information transmission imposed by thermal noise .",
            "id": "3915463",
            "problem": "A single transcription factor binds a cognate promoter, and its gene-regulatory output is read out as a binary occupancy state due to thermal fluctuations: bound ($O=1$) or unbound ($O=0$). The promoter has a single binding site with binding energy $E$ (relative to the unbound state), inverse thermal energy $\\beta = 1/(k_B T)$, and ligand concentration $C$. In thermodynamic equilibrium, the probability that the site is bound is given by $P_{\\text{bound}}(C) = \\frac{C \\exp(-\\beta E)}{1 + C \\exp(-\\beta E)}$. Assume that the measurement timescale is short compared to the binding correlation time so that each measurement returns a single stochastic occupancy sample $O \\in \\{0,1\\}$ drawn from a Bernoulli distribution with parameter $P_{\\text{bound}}(C)$.\n\nThe input $C$ varies across experiments such that the induced occupancy probability $p(C) = P_{\\text{bound}}(C)$ is uniformly distributed on $[0,1]$. Treating $C$ as a random input and $O$ as the stochastic output, compute the Shannon Mutual Information (MI) $I(C;O)$ in nats. You must start from first principles of equilibrium statistical mechanics and information theory and explicitly account for the thermal fluctuations that make $O$ stochastic at fixed $C$.\n\nThen, compare this thermodynamic prediction to a kinetic two-state binding model with on-rate $k_{\\text{on}} C$ and off-rate $k_{\\text{off}}$, assuming steady state and detailed balance, and discuss whether the resulting $I(C;O)$ differs under the kinetic description. Use the standard concentration convention to render equilibrium constants dimensionless where appropriate.\n\nProvide your final answer as a single closed-form analytic expression in nats. No rounding is required. Express the final mutual information in nats.",
            "solution": "The problem asks for the Shannon Mutual Information $I(C;O)$ between a continuous input variable, the ligand concentration $C$, and a discrete, stochastic output variable, the binary occupancy state $O \\in \\{0, 1\\}$. The mutual information is defined from first principles of information theory as:\n$$I(C;O) = H(O) - H(O|C)$$\nwhere $H(O)$ is the entropy of the output distribution and $H(O|C)$ is the conditional entropy of the output given the input. We will compute these quantities in units of nats, using the natural logarithm $\\ln$.\n\nThe problem states that for a fixed concentration $C$, the output $O$ is a random sample from a Bernoulli distribution with parameter $p(C) = P_{\\text{bound}}(C)$. The entropy of a Bernoulli distribution with parameter $p$ is:\n$$H_{\\text{Bernoulli}}(p) = -[p \\ln(p) + (1-p) \\ln(1-p)]$$\nThe problem specifies that the input concentrations $C$ are chosen such that the induced probability $p(C)$ is uniformly distributed over the interval $[0,1]$. Let us denote this random variable by $p$, with a probability density function $f(p)=1$ for $p \\in [0,1]$ and $f(p)=0$ otherwise. Since the binding probability function $p(C)$ is a deterministic and monotonic function of $C$, the information content is preserved, meaning $I(C;O) = I(p;O)$. We can therefore perform our calculations using the variable $p$.\n\nFirst, we calculate the conditional entropy $H(O|C)$, which is equivalent to $H(O|p)$. This is the expectation of the Bernoulli entropy, averaged over the distribution of $p$:\n$$H(O|C) = H(O|p) = \\mathbb{E}_{p}[H(O|P=p)] = \\int_{0}^{1} H_{\\text{Bernoulli}}(p) f(p) \\, dp$$\n$$H(O|C) = \\int_{0}^{1} \\left( -[p \\ln(p) + (1-p) \\ln(1-p)] \\right) \\cdot 1 \\, dp = - \\int_{0}^{1} p \\ln(p) \\, dp - \\int_{0}^{1} (1-p) \\ln(1-p) \\, dp$$\nWe evaluate the first integral, $\\int p \\ln(p) \\, dp$, using integration by parts, where $u = \\ln(p)$ and $dv = p \\, dp$. This gives $du = (1/p) \\, dp$ and $v = p^2/2$.\n$$ \\int p \\ln(p) \\, dp = \\frac{p^2}{2} \\ln(p) - \\int \\frac{p^2}{2} \\frac{1}{p} \\, dp = \\frac{p^2}{2} \\ln(p) - \\frac{p^2}{4} $$\nEvaluating the definite integral from $0$ to $1$:\n$$ \\int_{0}^{1} p \\ln(p) \\, dp = \\left[ \\frac{p^2}{2} \\ln(p) - \\frac{p^2}{4} \\right]_{0}^{1} = \\left( \\frac{1^2}{2} \\ln(1) - \\frac{1^2}{4} \\right) - \\lim_{p \\to 0^+} \\left( \\frac{p^2}{2} \\ln(p) - \\frac{p^2}{4} \\right) $$\nSince $\\ln(1)=0$ and $\\lim_{p \\to 0^+} p^2 \\ln(p) = 0$ (verifiable with L'Hôpital's rule), the result is:\n$$ \\int_{0}^{1} p \\ln(p) \\, dp = \\left( 0 - \\frac{1}{4} \\right) - (0 - 0) = -\\frac{1}{4} $$\nThe second integral, $\\int_{0}^{1} (1-p) \\ln(1-p) \\, dp$, yields the same value of $-1/4$ via the substitution $u=1-p$. The conditional entropy is therefore:\n$$ H(O|C) = - \\left( -\\frac{1}{4} - \\frac{1}{4} \\right) = \\frac{1}{2} \\text{ nats} $$\n\nNext, we calculate the marginal entropy $H(O)$. To do this, we first need the marginal probabilities $P(O=1)$ and $P(O=0)$. Using the law of total probability, we find $P(O=1)$ by averaging the conditional probability $P(O=1|p)=p$ over the distribution of $p$:\n$$ P(O=1) = \\mathbb{E}_{p}[P(O=1|p)] = \\int_{0}^{1} p f(p) \\, dp = \\int_{0}^{1} p \\cdot 1 \\, dp = \\left[ \\frac{p^2}{2} \\right]_{0}^{1} = \\frac{1}{2} $$\nIt follows that $P(O=0) = 1 - P(O=1) = 1/2$. The marginal distribution of the output $O$ is a Bernoulli distribution with parameter $1/2$, a fair coin toss. The entropy of this distribution is:\n$$ H(O) = - \\left[ P(O=1) \\ln(P(O=1)) + P(O=0) \\ln(P(O=0)) \\right] $$\n$$ H(O) = - \\left[ \\frac{1}{2} \\ln\\left(\\frac{1}{2}\\right) + \\frac{1}{2} \\ln\\left(\\frac{1}{2}\\right) \\right] = - \\ln\\left(\\frac{1}{2}\\right) = \\ln(2) \\text{ nats} $$\n\nFinally, we combine these results to find the mutual information:\n$$ I(C;O) = H(O) - H(O|C) = \\ln(2) - \\frac{1}{2} \\text{ nats} $$\n\nFor the second part of the problem, we compare this result with a kinetic two-state binding model. The model involves transitions between the unbound state ($O=0$) and the bound state ($O=1$):\n$$ O=0 \\underset{k_{\\text{off}}}{\\stackrel{k_{\\text{on}}C}{\\rightleftharpoons}} O=1 $$\nThe corresponding master equation for the probability of being in the bound state, $P_1$, is:\n$$ \\frac{dP_1}{dt} = k_{\\text{on}} C \\cdot (1-P_1) - k_{\\text{off}} \\cdot P_1 $$\nAt steady state, we set $dP_1/dt = 0$, which gives:\n$$ k_{\\text{on}} C (1 - P_{1,\\text{ss}}) = k_{\\text{off}} P_{1,\\text{ss}} $$\nSolving for the steady-state probability $P_{1,\\text{ss}}$:\n$$ P_{1,\\text{ss}}(C) = \\frac{k_{\\text{on}} C}{k_{\\text{off}} + k_{\\text{on}} C} = \\frac{(k_{\\text{on}}/k_{\\text{off}}) C}{1 + (k_{\\text{on}}/k_{\\text{off}}) C} $$\nThe thermodynamic description gives the binding probability as $P_{\\text{bound}}(C) = \\frac{C \\exp(-\\beta E)}{1 + C \\exp(-\\beta E)}$. We can define a thermodynamic equilibrium constant $K_{\\text{eq}} = \\exp(-\\beta E)$ (assuming appropriate standard state conventions to make $C$ dimensionless) such that $P_{\\text{bound}}(C) = \\frac{K_{\\text{eq}} C}{1 + K_{\\text{eq}} C}$.\nBy comparing the two models, we see that they yield an identical functional form for the occupancy probability as a function of concentration $C$. The principle of detailed balance requires that the kinetic steady state corresponds to the thermodynamic equilibrium, which implies that the kinetic association constant $K_A = k_{\\text{on}}/k_{\\text{off}}$ is equal to the thermodynamic equilibrium constant $K_{\\text{eq}}$.\nSince the entire calculation of the mutual information $I(C;O)$ depended only on the functional form of $p(C)$ and its induced distribution, and this function is identical for both the thermodynamic model and the kinetic model at steady state, the resulting mutual information does not differ. The kinetic rates $k_{\\text{on}}$ and $k_{\\text{off}}$ determine the timescale required to reach equilibrium, but they do not alter the equilibrium properties themselves, which is what the Shannon information capacity measures.\nThus, the value $I(C;O) = \\ln(2) - 1/2$ is robust to the choice of description, provided the system is in equilibrium/steady-state.",
            "answer": "$$\\boxed{\\ln(2) - \\frac{1}{2}}$$"
        },
        {
            "introduction": "While the binary model of promoter occupancy is instructive, real gene expression is a noisy process producing a variable number of mRNA or protein molecules. This problem introduces a more realistic hierarchical model that distinguishes between two fundamental types of noise: intrinsic noise, stemming from the stochastic nature of biochemical reactions, and extrinsic noise, reflecting cell-to-cell variability. By deriving the Fano factor, a key statistical metric, you will learn how to mathematically dissect these noise sources and understand why gene expression is often more variable than simple models predict .",
            "id": "3915438",
            "problem": "Consider a synthetic gene regulatory network where the readout of a promoter in a fixed observation window is the messenger ribonucleic acid (mRNA) count $Y$. Conditional on the instantaneous transcriptional activity $k$, the count $Y$ follows a Poisson distribution, so $Y \\mid k \\sim \\text{Poisson}(k)$. To model extrinsic variability across cells and environments, assume the activity $k$ is itself random and strictly positive, with a log-normal distribution: $\\ln k \\sim \\mathcal{N}(\\mu, \\sigma^{2})$, where $\\mu \\in \\mathbb{R}$ and $\\sigma^{2} > 0$ are the mean and variance of the underlying normal distribution. Adopt the following foundational base: the definition of conditional probability and the law of total probability, the law of total expectation and the law of total variance, and the standard definition and moments of the log-normal distribution.\n\nTasks:\n- Using only these principles, derive the marginal distribution of $Y$ by integrating out $k$. Write your result as an explicit integral for the probability mass function $p_{Y}(y)$ for integer $y \\geq 0$.\n- Compute the mean $\\mathbb{E}[Y]$ and the variance $\\operatorname{Var}(Y)$ of $Y$ in terms of the parameters $\\mu$ and $\\sigma^{2}$ of the log-normal distribution of $k$.\n- Define the Fano factor (FF) as $F = \\operatorname{Var}(Y)/\\mathbb{E}[Y]$. Show that $F > 1$ whenever $\\sigma^{2} > 0$ and provide a closed-form expression for $F$ in terms of $\\mu$ and $\\sigma^{2}$.\n\nYour final reported answer must be an analytic expression for the Fano factor $F$ in terms of $\\mu$ and $\\sigma^{2}$. No numerical evaluation or rounding is required.",
            "solution": "The problem defines a hierarchical model where the observable mRNA count, $Y$, is conditionally Poisson distributed given a rate parameter $k$, which itself is a random variable.\nThe conditional distribution is given by $Y \\mid k \\sim \\text{Poisson}(k)$, so the probability mass function (PMF) is:\n$$p_{Y|K}(y|k) = \\frac{k^y \\exp(-k)}{y!} \\quad \\text{for } y = 0, 1, 2, \\dots$$\nThe rate parameter $k$ is strictly positive and follows a log-normal distribution, specified by $\\ln k \\sim \\mathcal{N}(\\mu, \\sigma^2)$. The probability density function (PDF) of $k$ is therefore:\n$$p_K(k) = \\frac{1}{k \\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln k - \\mu)^2}{2\\sigma^2}\\right) \\quad \\text{for } k > 0$$\n\n**Task 1: Marginal Distribution of $Y$**\n\nTo find the marginal PMF of $Y$, $p_Y(y)$, we apply the law of total probability by integrating the joint probability density-mass function over all possible values of the continuous random variable $k$.\n$$p_Y(y) = \\int_{0}^{\\infty} p_{Y,K}(y,k) \\, dk = \\int_{0}^{\\infty} p_{Y|K}(y|k) p_K(k) \\, dk$$\nSubstituting the expressions for the conditional PMF of $Y$ and the PDF of $k$, we obtain the explicit integral for $p_Y(y)$ for any non-negative integer $y$:\n$$p_Y(y) = \\int_{0}^{\\infty} \\left( \\frac{k^y \\exp(-k)}{y!} \\right) \\left( \\frac{1}{k \\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln k - \\mu)^2}{2\\sigma^2}\\right) \\right) \\, dk$$\nThis can be written as:\n$$p_Y(y) = \\frac{1}{y! \\sigma \\sqrt{2\\pi}} \\int_{0}^{\\infty} k^{y-1} \\exp(-k) \\exp\\left(-\\frac{(\\ln k - \\mu)^2}{2\\sigma^2}\\right) \\, dk$$\nThis integral defines the Poisson-lognormal distribution. The problem only requires the explicit integral, which is now provided.\n\n**Task 2: Mean and Variance of $Y$**\n\nWe use the laws of total expectation and total variance.\n\n**Mean of $Y$:**\nThe law of total expectation states that $\\mathbb{E}[Y] = \\mathbb{E}[\\mathbb{E}[Y|K]]$.\nFirst, we find the conditional expectation of $Y$ given $K=k$. For a Poisson distribution with parameter $k$, the mean is $k$.\n$$\\mathbb{E}[Y|K=k] = k$$\nThus, the conditional expectation as a random variable is $\\mathbb{E}[Y|K] = K$.\nNext, we take the expectation of this result with respect to the distribution of $K$:\n$$\\mathbb{E}[Y] = \\mathbb{E}[K]$$\nThe mean of a log-normal distribution with parameters $\\mu$ and $\\sigma^2$ is given by the standard formula:\n$$\\mathbb{E}[K] = \\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)$$\nTherefore, the mean of $Y$ is:\n$$\\mathbb{E}[Y] = \\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)$$\n\n**Variance of $Y$:**\nThe law of total variance (also known as the conditional variance formula or Eve's law) states:\n$$\\operatorname{Var}(Y) = \\mathbb{E}[\\operatorname{Var}(Y|K)] + \\operatorname{Var}(\\mathbb{E}[Y|K])$$\nWe compute each term separately.\n1.  The first term is the expectation of the conditional variance. The variance of a Poisson distribution with parameter $k$ is also $k$.\n    $$\\operatorname{Var}(Y|K=k) = k$$\n    So, as a random variable, $\\operatorname{Var}(Y|K) = K$. Taking the expectation:\n    $$\\mathbb{E}[\\operatorname{Var}(Y|K)] = \\mathbb{E}[K] = \\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)$$\n2.  The second term is the variance of the conditional expectation. We already found that $\\mathbb{E}[Y|K] = K$.\n    $$\\operatorname{Var}(\\mathbb{E}[Y|K]) = \\operatorname{Var}(K)$$\n    The variance of a log-normal distribution with parameters $\\mu$ and $\\sigma^2$ is given by the standard formula:\n    $$\\operatorname{Var}(K) = (\\exp(\\sigma^2) - 1)\\exp(2\\mu + \\sigma^2)$$\nCombining the two terms, the total variance of $Y$ is:\n$$\\operatorname{Var}(Y) = \\mathbb{E}[K] + \\operatorname{Var}(K) = \\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right) + (\\exp(\\sigma^2) - 1)\\exp(2\\mu + \\sigma^2)$$\n\n**Task 3: Fano Factor $F$**\n\nThe Fano factor is defined as $F = \\frac{\\operatorname{Var}(Y)}{\\mathbb{E}[Y]}$. Using our derived expressions for the mean and variance of $Y$:\n$$F = \\frac{\\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right) + (\\exp(\\sigma^2) - 1)\\exp(2\\mu + \\sigma^2)}{\\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)}$$\nWe can simplify this expression by splitting the fraction:\n$$F = 1 + \\frac{(\\exp(\\sigma^2) - 1)\\exp(2\\mu + \\sigma^2)}{\\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)}$$\n$$F = 1 + (\\exp(\\sigma^2) - 1) \\exp\\left(2\\mu + \\sigma^2 - \\left(\\mu + \\frac{\\sigma^2}{2}\\right)\\right)$$\n$$F = 1 + (\\exp(\\sigma^2) - 1) \\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)$$\nThis is the closed-form expression for the Fano factor.\n\nFinally, we must show that $F > 1$ whenever $\\sigma^2 > 0$.\nThe Fano factor is $F = 1 + (\\exp(\\sigma^2) - 1) \\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)$. For $F > 1$, the second term must be positive.\n$$(\\exp(\\sigma^2) - 1) \\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right) > 0$$\nLet's analyze the two factors in this term:\n1.  The term $\\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)$ is an exponential function. For any real arguments $\\mu$ and $\\sigma^2$, its value is strictly positive.\n2.  The term $(\\exp(\\sigma^2) - 1)$. The problem specifies that $\\sigma^2 > 0$. For any strictly positive argument $x > 0$, the function $\\exp(x)$ is strictly greater than $1$. Therefore, since $\\sigma^2 > 0$, we have $\\exp(\\sigma^2) > 1$, which implies $(\\exp(\\sigma^2) - 1) > 0$.\nSince both factors are strictly positive, their product is also strictly positive. Thus,\n$$F = 1 + (\\text{a positive number}) > 1$$\nThis demonstrates that the variability in the transcription activity $k$ (quantified by $\\sigma^2 > 0$) leads to super-Poissonian noise in the mRNA count $Y$ (i.e., noise greater than the mean, $F>1$).\n\nThe final analytical expression for the Fano factor is $1 + (\\exp(\\sigma^2) - 1) \\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)$.",
            "answer": "$$\\boxed{1 + (\\exp(\\sigma^2) - 1) \\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)}$$"
        },
        {
            "introduction": "Moving from analysis to design, this exercise explores how a regulatory element can be optimized for maximal information transmission. It challenges the simple notion that noise is always detrimental, revealing that the *structure* of noise across the input range is critically important. You will investigate how capacity is influenced not just by the average noise level but by its interplay with the system's sensitivity, leading to the powerful concept of an optimal input distribution for probing the system .",
            "id": "3915486",
            "problem": "A single-input transcriptional regulatory element is modeled as a noisy channel from ligand concentration $c$ to gene expression output $g$. The deterministic mean input-output relation is $g = f(c)$, where $f(c)$ is strictly monotonic over the accessible concentration domain $c \\in [0, c_{\\max}]$ and attains a dynamic range $[g_{\\min}, g_{\\max}]$. The conditional output distribution is Gaussian, $p(g \\mid c) = \\mathcal{N}(f(c), \\sigma^2(c))$, with an input-dependent variance $\\sigma^2(c)$ that captures the intrinsic and extrinsic noise of gene expression. The input $c$ is drawn from a probability density $p(c)$ that the cell or experimentalist can tune, subject to normalization $\\int_0^{c_{\\max}} p(c)\\,\\mathrm{d}c = 1$. Channel capacity is defined as the supremum, over all admissible $p(c)$, of the Mutual Information (MI) $\\mathcal{I}(C;G)$ between input $C$ and output $G$.\n\nConsider two ($2$) different promoters, $A$ and $B$, that share the same deterministic transfer function $f(c)$ and thus the same dynamic range $[g_{\\min}, g_{\\max}]$, but have different noise profiles. Promoter $A$ is homoscedastic with $\\sigma_A^2(c) = \\sigma_0^2$ independent of $c$. Promoter $B$ is heteroscedastic with a smooth, positive variance profile $\\sigma_B^2(c)$ that varies with $c$. Assume the small-noise regime in which $\\sigma(c)$ is sufficiently small compared to the dynamic range of $f(c)$ so that the output is tightly concentrated around $f(c)$ for each input.\n\nWhich of the following statements are correct in this regime?\n\nA. In the small-noise limit with a monotonic $f(c)$ and Gaussian conditional distributions, the channel capacities of the two promoters satisfy\n$$\nC_A \\approx \\log_2\\!\\left(\\frac{g_{\\max} - g_{\\min}}{\\sqrt{2\\pi e}\\,\\sigma_0}\\right), \\quad\nC_B \\approx \\log_2\\!\\left(\\frac{1}{\\sqrt{2\\pi e}} \\int_0^{c_{\\max}} \\frac{|f'(c)|}{\\sigma_B(c)}\\,\\mathrm{d}c\\right),\n$$\nwhere $f'(c) = \\frac{\\mathrm{d}f}{\\mathrm{d}c}$ and $e$ is Euler's number.\n\nB. Because the dynamic range is the same for both promoters, the optimal input distribution $p^*(c)$ is uniform over $[0, c_{\\max}]$, and therefore the channel capacities are necessarily equal.\n\nC. Heteroscedastic noise shapes the optimal input distribution via the local sensitivity of the map $f(c)$ and the local noise amplitude, yielding $p^*(c) \\propto \\frac{|f'(c)|}{\\sigma(c)}$, so probability mass concentrates in regions where $|f'(c)|$ is large and $\\sigma(c)$ is small.\n\nD. Promoter $B$ must have lower capacity than promoter $A$ because input-dependent variance $\\sigma_B^2(c)$ always increases uncertainty relative to the homoscedastic case.\n\nE. If $\\sigma_B(c)$ is selectively reduced at the concentrations where $|f'(c)|$ is maximal (holding the dynamic range fixed), then $C_B$ can exceed $C_A$ even though both promoters have the same deterministic $f(c)$ and dynamic range.",
            "solution": "The problem asks to evaluate several statements concerning the channel capacity of two transcriptional regulatory elements, modeled as noisy channels. The core of the problem lies in understanding how channel capacity is determined in the small-noise limit for a channel with a nonlinear transfer function and potentially input-dependent noise.\n\nThe fundamental quantity is the Mutual Information (MI) between the input concentration $C$ and the output gene expression $G$, given by:\n$$\n\\mathcal{I}(C;G) = \\mathcal{H}(G) - \\mathcal{H}(G|C)\n$$\nwhere $\\mathcal{H}(G)$ is the entropy of the output distribution and $\\mathcal{H}(G|C)$ is the conditional entropy of the output given the input. Channel capacity $C$ is the supremum of the MI over all valid input distributions $p(c)$:\n$$\nC = \\sup_{p(c)} \\mathcal{I}(C;G)\n$$\n\nThe conditional entropy $\\mathcal{H}(G|C)$ is the average entropy of the output noise over all possible inputs. Given the conditional distribution is Gaussian, $p(g|c) = \\mathcal{N}(f(c), \\sigma^2(c))$, the entropy for a specific input $c$ is $\\mathcal{H}(G|C=c) = \\frac{1}{2}\\log_2(2\\pi e \\sigma^2(c))$. The average conditional entropy is therefore:\n$$\n\\mathcal{H}(G|C) = \\int_0^{c_{\\max}} p(c) \\left[\\frac{1}{2}\\log_2(2\\pi e \\sigma^2(c))\\right] \\mathrm{d}c = \\int_0^{c_{\\max}} p(c) \\log_2(\\sqrt{2\\pi e}\\sigma(c)) \\mathrm{d}c\n$$\n\nIn the specified small-noise regime, where $\\sigma(c)$ is small compared to the dynamic range $g_{\\max} - g_{\\min}$, the capacity can be approximated. A standard result from information theory for this type of channel states that the capacity is determined by the integral of the ratio of the local sensitivity $|f'(c)|$ to the local noise level $\\sigma(c)$. This ratio, $|f'(c)|/\\sigma(c)$, can be interpreted as a local signal-to-noise ratio for infinitesimal changes in the input. The capacity is approximately given by:\n$$\nC \\approx \\log_2 \\left( \\frac{1}{\\sqrt{2\\pi e}} \\int_0^{c_{\\max}} \\frac{|f'(c)|}{\\sigma(c)} \\mathrm{d}c \\right)\n$$\nThis maximum information rate is achieved when the input probability distribution $p^*(c)$ is chosen to make the \"information density\" uniform across the input range. This leads to the optimal input distribution:\n$$\np^*(c) \\propto \\frac{|f'(c)|}{\\sigma(c)}\n$$\nThis strategy allocates more probability to input regions that are more informative, i.e., where the output is highly sensitive to the input (high $|f'(c)|$) and where the output is measured with high precision (low $\\sigma(c)$).\n\nWith these principles established, we can now evaluate each statement.\n\n### Option-by-Option Analysis\n\n**A. In the small-noise limit with a monotonic $f(c)$ and Gaussian conditional distributions, the channel capacities of the two promoters satisfy...**\n\nWe apply the general capacity formula to each promoter.\n\nFor Promoter A (homoscedastic case): $\\sigma_A(c) = \\sigma_0$.\n$$\nC_A \\approx \\log_2 \\left( \\frac{1}{\\sqrt{2\\pi e}} \\int_0^{c_{\\max}} \\frac{|f'(c)|}{\\sigma_0} \\mathrm{d}c \\right)\n$$\nBecause $\\sigma_0$ is a constant, it can be factored out of the integral:\n$$\nC_A \\approx \\log_2 \\left( \\frac{1}{\\sqrt{2\\pi e}\\,\\sigma_0} \\int_0^{c_{\\max}} |f'(c)| \\mathrm{d}c \\right)\n$$\nSince $f(c)$ is strictly monotonic, $f'(c)$ does not change sign on the interval $(0, c_{\\max})$. Therefore, the integral is simply the total change in $f(c)$ over the domain, which is the dynamic range:\n$$\n\\int_0^{c_{\\max}} |f'(c)| \\mathrm{d}c = |f(c_{\\max}) - f(0)| = g_{\\max} - g_{\\min}\n$$\nSubstituting this back, we get:\n$$\nC_A \\approx \\log_2\\left(\\frac{g_{\\max} - g_{\\min}}{\\sqrt{2\\pi e}\\,\\sigma_0}\\right)\n$$\n\nFor Promoter B (heteroscedastic case): $\\sigma_B(c)$ is a function of $c$.\nWe use the general formula directly:\n$$\nC_B \\approx \\log_2\\left(\\frac{1}{\\sqrt{2\\pi e}} \\int_0^{c_{\\max}} \\frac{|f'(c)|}{\\sigma_B(c)}\\,\\mathrm{d}c\\right)\n$$\nBoth expressions in statement A match our derivations.\n\nVerdict: **Correct**.\n\n**B. Because the dynamic range is the same for both promoters, the optimal input distribution $p^*(c)$ is uniform over $[0, c_{\\max}]$, and therefore the channel capacities are necessarily equal.**\n\nThis statement contains several incorrect claims.\nFirst, the optimal input distribution is $p^*(c) \\propto \\frac{|f'(c)|}{\\sigma(c)}$. For this to be uniform ($p^*(c) = \\text{constant}$), we would need $|f'(c)|/\\sigma(c) = \\text{constant}$. For Promoter A (homoscedastic, $\\sigma(c)=\\sigma_0$), this requires $|f'(c)|$ to be constant, meaning $f(c)$ must be a linear function. The problem does not state that $f(c)$ is linear; it is a general monotonic function. For Promoter B, the condition is even more restrictive. Thus, the premise that $p^*(c)$ is uniform is false.\nSecond, the conclusion that the capacities are necessarily equal is false. As shown in the analysis of option A, $C_A$ depends on the value of $\\sigma_0$, while $C_B$ depends on the integral of $|f'(c)|/\\sigma_B(c)$. These two quantities are not generally equal. For instance, if $\\sigma_B(c) > \\sigma_0$ for all $c$, then $C_B  C_A$.\n\nVerdict: **Incorrect**.\n\n**C. Heteroscedastic noise shapes the optimal input distribution via the local sensitivity of the map $f(c)$ and the local noise amplitude, yielding $p^*(c) \\propto \\frac{|f'(c)|}{\\sigma(c)}$, so probability mass concentrates in regions where $|f'(c)|$ is large and $\\sigma(c)$ is small.**\n\nThis statement accurately describes the optimal input distribution $p^*(c)$ for achieving channel capacity in this regime. The expression $p^*(c) \\propto \\frac{|f'(c)|}{\\sigma(c)}$ demonstrates that to maximize information transmission, the system should preferentially sample inputs from regions where the local gain $|f'(c)|$ is high and the local noise $\\sigma(c)$ is low. The ratio $|f'(c)|/\\sigma(c)$ quantifies the \"informativeness\" of each input point, and the optimal strategy is to bias the input distribution towards more informative regions. The verbal description is a correct interpretation of the mathematical proportionality.\n\nVerdict: **Correct**.\n\n**D. Promoter $B$ must have lower capacity than promoter $A$ because input-dependent variance $\\sigma_B^2(c)$ always increases uncertainty relative to the homoscedastic case.**\n\nThis statement is false. The relative capacity of B versus A depends on the comparison between $\\int_0^{c_{\\max}} \\frac{|f'(c)|}{\\sigma_B(c)} \\mathrm{d}c$ and $\\int_0^{c_{\\max}} \\frac{|f'(c)|}{\\sigma_0} \\mathrm{d}c$. There is no *a priori* relationship between $\\sigma_B(c)$ and $\\sigma_0$ given in the problem, so we cannot make a definitive comparison. For example, if we simply set $\\sigma_B(c)$ to be a constant different from $\\sigma_0$, say $\\sigma_B(c) = \\sigma_1$, then $C_B > C_A$ if $\\sigma_1  \\sigma_0$ and $C_B  C_A$ if $\\sigma_1 > \\sigma_0$. More generally, the profile of $\\sigma_B(c)$ can be tailored to either increase or decrease the capacity relative to the homoscedastic case. The claim that B \"must\" have lower capacity is too strong and incorrect.\n\nVerdict: **Incorrect**.\n\n**E. If $\\sigma_B(c)$ is selectively reduced at the concentrations where $|f'(c)|$ is maximal (holding the dynamic range fixed), then $C_B$ can exceed $C_A$ even though both promoters have the same deterministic $f(c)$ and dynamic range.**\n\nThis statement explores the consequence of shaping the noise profile $\\sigma_B(c)$. The capacity $C_B$ is determined by the integral $\\int_0^{c_{\\max}} \\frac{|f'(c)|}{\\sigma_B(c)} \\mathrm{d}c$. The integrand is a product of two terms: $|f'(c)|$ and $1/\\sigma_B(c)$. To maximize the integral, one should make $1/\\sigma_B(c)$ large where $|f'(c)|$ is large. This means reducing the noise $\\sigma_B(c)$ in regions of high sensitivity $|f'(c)|$. If $\\sigma_B(c)$ is engineered to be smaller than $\\sigma_0$ particularly in the regions where $|f'(c)|$ is maximal, the contribution to the integral from these regions can become very large. It is entirely possible to construct a noise profile $\\sigma_B(c)$ such that:\n$$\n\\int_0^{c_{\\max}} \\frac{|f'(c)|}{\\sigma_B(c)} \\mathrm{d}c > \\int_0^{c_{\\max}} \\frac{|f'(c)|}{\\sigma_0} \\mathrm{d}c = \\frac{g_{\\max}-g_{\\min}}{\\sigma_0}\n$$\nIn such a case, it would follow directly that $C_B > C_A$. Therefore, heteroscedastic noise provides an opportunity to *increase* channel capacity over a comparable homoscedastic system by suppressing noise in the most sensitive input regions. The claim that $C_B$ \"can exceed\" $C_A$ is therefore correct.\n\nVerdict: **Correct**.",
            "answer": "$$\\boxed{ACE}$$"
        }
    ]
}