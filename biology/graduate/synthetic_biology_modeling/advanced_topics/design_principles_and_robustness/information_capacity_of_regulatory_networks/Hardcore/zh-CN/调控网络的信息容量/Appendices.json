{
    "hands_on_practices": [
        {
            "introduction": "本练习将引导我们回到最基本的调控单元——单个转录因子与启动子的结合。通过这个看似简单的物理过程，我们将从第一性原理出发，计算信号（配体浓度 $C$）与输出（启动子占据状态 $O$）之间的互信息。这个实践旨在帮助你建立起连接热力学平衡、随机热噪声和信息容量的桥梁，这是理解生物系统信息处理能力的基石 。",
            "id": "3915463",
            "problem": "一个转录因子与一个同源启动子结合，其基因调控输出由于热涨落被读出为二元占据状态：结合 ($O=1$) 或未结合 ($O=0$)。该启动子有一个结合位点，其结合能为 $E$（相对于未结合状态），逆热能为 $\\beta = 1/(k_B T)$，配体浓度为 $C$。在热力学平衡状态下，该位点被结合的概率由 $P_{\\text{bound}}(C) = \\frac{C \\exp(-\\beta E)}{1 + C \\exp(-\\beta E)}$ 给出。假设测量时间尺度远小于结合相关时间，因此每次测量都返回一个从参数为 $P_{\\text{bound}}(C)$ 的伯努利分布中抽取的随机占据样本 $O \\in \\{0,1\\}$。\n\n输入 $C$ 在不同实验中变化，使得诱导的占据概率 $p(C) = P_{\\text{bound}}(C)$ 在 $[0,1]$ 上均匀分布。将 $C$ 视为随机输入，将 $O$ 视为随机输出，计算以奈特 (nats) 为单位的香农互信息 (MI) $I(C;O)$。你必须从平衡统计力学和信息论的第一性原理出发，并明确考虑使得 $O$ 在固定 $C$ 值下成为随机变量的热涨落。\n\n然后，将此热力学预测与一个具有结合速率 $k_{\\text{on}} C$ 和解离速率 $k_{\\text{off}}$ 的动力学双态结合模型进行比较，假设系统处于稳态和细致平衡，并讨论在动力学描述下得到的 $I(C;O)$ 是否有所不同。在适当情况下，使用标准浓度约定使平衡常数无量纲化。\n\n请以单一的封闭形式解析表达式提供最终答案，单位为奈特。无需四舍五入。以奈特为单位表示最终的互信息。",
            "solution": "问题要求计算连续输入变量（配体浓度 $C$）与离散随机输出变量（二元占据状态 $O \\in \\{0, 1\\}$）之间的香农互信息 $I(C;O)$。根据信息论的第一性原理，互信息定义为：\n$$I(C;O) = H(O) - H(O|C)$$\n其中 $H(O)$ 是输出分布的熵，$H(O|C)$ 是给定输入下输出的条件熵。我们将使用自然对数 $\\ln$ 计算这些量，单位为奈特。\n\n问题陈述，对于固定的浓度 $C$，输出 $O$ 是从参数为 $p(C) = P_{\\text{bound}}(C)$ 的伯努利分布中抽取的随机样本。参数为 $p$ 的伯努利分布的熵为：\n$$H_{\\text{Bernoulli}}(p) = -[p \\ln(p) + (1-p) \\ln(1-p)]$$\n问题指明，选择的输入浓度 $C$ 使得诱导的概率 $p(C)$ 在区间 $[0,1]$ 上均匀分布。让我们用 $p$ 表示这个随机变量，其概率密度函数为当 $p \\in [0,1]$ 时 $f(p)=1$，否则 $f(p)=0$。由于结合概率函数 $p(C)$ 是 $C$ 的确定性单调函数，信息内容得以保留，即 $I(C;O) = I(p;O)$。因此，我们可以使用变量 $p$ 进行计算。\n\n首先，我们计算条件熵 $H(O|C)$，它等价于 $H(O|p)$。这是伯努利熵在 $p$ 的分布上的期望值：\n$$H(O|C) = H(O|p) = \\mathbb{E}_{p}[H(O|P=p)] = \\int_{0}^{1} H_{\\text{Bernoulli}}(p) f(p) \\, dp$$\n$$H(O|C) = \\int_{0}^{1} \\left( -[p \\ln(p) + (1-p) \\ln(1-p)] \\right) \\cdot 1 \\, dp = - \\int_{0}^{1} p \\ln(p) \\, dp - \\int_{0}^{1} (1-p) \\ln(1-p) \\, dp$$\n我们使用分部积分法计算第一个积分 $\\int p \\ln(p) \\, dp$，其中 $u = \\ln(p)$，$dv = p \\, dp$。这得到 $du = (1/p) \\, dp$ 和 $v = p^2/2$。\n$$ \\int p \\ln(p) \\, dp = \\frac{p^2}{2} \\ln(p) - \\int \\frac{p^2}{2} \\frac{1}{p} \\, dp = \\frac{p^2}{2} \\ln(p) - \\frac{p^2}{4} $$\n计算从 $0$ 到 $1$ 的定积分：\n$$ \\int_{0}^{1} p \\ln(p) \\, dp = \\left[ \\frac{p^2}{2} \\ln(p) - \\frac{p^2}{4} \\right]_{0}^{1} = \\left( \\frac{1^2}{2} \\ln(1) - \\frac{1^2}{4} \\right) - \\lim_{p \\to 0^+} \\left( \\frac{p^2}{2} \\ln(p) - \\frac{p^2}{4} \\right) $$\n因为 $\\ln(1)=0$ 且 $\\lim_{p \\to 0^+} p^2 \\ln(p) = 0$（可用洛必达法则验证），结果为：\n$$ \\int_{0}^{1} p \\ln(p) \\, dp = \\left( 0 - \\frac{1}{4} \\right) - (0 - 0) = -\\frac{1}{4} $$\n第二个积分 $\\int_{0}^{1} (1-p) \\ln(1-p) \\, dp$ 通过替换 $u=1-p$ 得到相同的值 $-1/4$。因此，条件熵为：\n$$ H(O|C) = - \\left( -\\frac{1}{4} - \\frac{1}{4} \\right) = \\frac{1}{2} \\text{ 奈特} $$\n\n接下来，我们计算边缘熵 $H(O)$。为此，我们首先需要边缘概率 $P(O=1)$ 和 $P(O=0)$。使用全概率定律，我们通过对条件概率 $P(O=1|p)=p$ 在 $p$ 的分布上取平均来找到 $P(O=1)$：\n$$ P(O=1) = \\mathbb{E}_{p}[P(O=1|p)] = \\int_{0}^{1} p f(p) \\, dp = \\int_{0}^{1} p \\cdot 1 \\, dp = \\left[ \\frac{p^2}{2} \\right]_{0}^{1} = \\frac{1}{2} $$\n因此，$P(O=0) = 1 - P(O=1) = 1/2$。输出 $O$ 的边缘分布是参数为 $1/2$ 的伯努利分布，相当于一次公平的抛硬币。该分布的熵为：\n$$ H(O) = - \\left[ P(O=1) \\ln(P(O=1)) + P(O=0) \\ln(P(O=0)) \\right] $$\n$$ H(O) = - \\left[ \\frac{1}{2} \\ln\\left(\\frac{1}{2}\\right) + \\frac{1}{2} \\ln\\left(\\frac{1}{2}\\right) \\right] = - \\ln\\left(\\frac{1}{2}\\right) = \\ln(2) \\text{ 奈特} $$\n\n最后，我们结合这些结果来求得互信息：\n$$ I(C;O) = H(O) - H(O|C) = \\ln(2) - \\frac{1}{2} \\text{ 奈特} $$\n\n对于问题的第二部分，我们将此结果与一个动力学双态结合模型进行比较。该模型涉及未结合态 ($O=0$) 和结合态 ($O=1$) 之间的转换：\n$$ O=0 \\underset{k_{\\text{off}}}{\\stackrel{k_{\\text{on}}C}{\\rightleftharpoons}} O=1 $$\n处于结合态的概率 $P_1$ 对应的主方程是：\n$$ \\frac{dP_1}{dt} = k_{\\text{on}} C \\cdot (1-P_1) - k_{\\text{off}} \\cdot P_1 $$\n在稳态下，我们设 $dP_1/dt = 0$，得到：\n$$ k_{\\text{on}} C (1 - P_{1,\\text{ss}}) = k_{\\text{off}} P_{1,\\text{ss}} $$\n解出稳态概率 $P_{1,\\text{ss}}$：\n$$ P_{1,\\text{ss}}(C) = \\frac{k_{\\text{on}} C}{k_{\\text{off}} + k_{\\text{on}} C} = \\frac{(k_{\\text{on}}/k_{\\text{off}}) C}{1 + (k_{\\text{on}}/k_{\\text{off}}) C} $$\n热力学描述给出的结合概率为 $P_{\\text{bound}}(C) = \\frac{C \\exp(-\\beta E)}{1 + C \\exp(-\\beta E)}$。我们可以定义一个热力学平衡常数 $K_{\\text{eq}} = \\exp(-\\beta E)$（假设使用适当的标准态约定使 $C$ 无量纲化），使得 $P_{\\text{bound}}(C) = \\frac{K_{\\text{eq}} C}{1 + K_{\\text{eq}} C}$。\n通过比较这两个模型，我们看到它们对于占据概率作为浓度 $C$ 的函数得出了相同的函数形式。细致平衡原理要求动力学稳态对应于热力学平衡，这意味着动力学结合常数 $K_A = k_{\\text{on}}/k_{\\text{off}}$ 等于热力学平衡常数 $K_{\\text{eq}}$。\n由于互信息 $I(C;O)$ 的整个计算仅依赖于 $p(C)$ 的函数形式及其诱导的分布，而这个函数对于热力学模型和稳态下的动力学模型是相同的，因此得到的互信息没有差异。动力学速率 $k_{\\text{on}}$ 和 $k_{\\text{off}}$ 决定了达到平衡所需的时间尺度，但它们不改变平衡本身的性质，而香农信息容量衡量的正是后者。\n因此，只要系统处于平衡/稳态，数值 $I(C;O) = \\ln(2) - 1/2$ 对于描述的选择是稳健的。",
            "answer": "$$\\boxed{\\ln(2) - \\frac{1}{2}}$$"
        },
        {
            "introduction": "在掌握了信息容量的基本计算后，我们将探讨一个更深入的设计问题：并非所有噪声都生而平等。本练习通过比较两个具有不同噪声特性（同方差与异方差）的启动子，揭示了信息容量如何依赖于局部信号灵敏度 $|f'(c)|$ 和局部噪声水平 $\\sigma(c)$ 的相互作用。通过这个思想实验，你将理解为何优化信息传输不仅关乎信噪比，更在于将信号输入集中在“信息最密集”的区域，这是生物信号系统高效运作的一个核心策略 。",
            "id": "3915486",
            "problem": "单输入转录调控元件被建模为一个从配体浓度 $c$ 到基因表达输出 $g$ 的噪声信道。确定性的平均输入-输出关系为 $g = f(c)$，其中 $f(c)$ 在可及浓度域 $c \\in [0, c_{\\max}]$ 上严格单调，并达到动态范围 $[g_{\\min}, g_{\\max}]$。条件输出分布是高斯分布，$p(g \\mid c) = \\mathcal{N}(f(c), \\sigma^2(c))$，其输入依赖的方差 $\\sigma^2(c)$ 捕捉了基因表达的内在和外在噪声。输入 $c$ 从一个概率密度 $p(c)$ 中抽取，细胞或实验者可以调节该密度，并受归一化条件 $\\int_0^{c_{\\max}} p(c)\\,\\mathrm{d}c = 1$ 的约束。信道容量定义为在所有允许的 $p(c)$ 上，输入 $C$ 和输出 $G$ 之间互信息 (MI) $\\mathcal{I}(C;G)$ 的上确界。\n\n考虑两个不同的启动子 $A$ 和 $B$，它们共享相同的确定性传递函数 $f(c)$，因此具有相同的动态范围 $[g_{\\min}, g_{\\max}]$，但具有不同的噪声特性。启动子 $A$ 是同方差的，其方差 $\\sigma_A^2(c) = \\sigma_0^2$ 与 $c$ 无关。启动子 $B$ 是异方差的，具有一个随 $c$ 变化的光滑、正值的方差函数 $\\sigma_B^2(c)$。假设在小噪声情形下，$\\sigma(c)$ 与 $f(c)$ 的动态范围相比足够小，因此对于每个输入，输出都紧密集中在 $f(c)$ 周围。\n\n在此情形下，下列哪个陈述是正确的？\n\nA. 在小噪声极限下，对于单调的 $f(c)$ 和高斯条件分布，两个启动子的信道容量满足\n$$\nC_A \\approx \\log_2\\!\\left(\\frac{g_{\\max} - g_{\\min}}{\\sqrt{2\\pi e}\\,\\sigma_0}\\right), \\quad\nC_B \\approx \\log_2\\!\\left(\\frac{1}{\\sqrt{2\\pi e}} \\int_0^{c_{\\max}} \\frac{|f'(c)|}{\\sigma_B(c)}\\,\\mathrm{d}c\\right),\n$$\n其中 $f'(c) = \\frac{\\mathrm{d}f}{\\mathrm{d}c}$，$e$ 是欧拉数。\n\nB. 因为两个启动子的动态范围相同，所以最优输入分布 $p^*(c)$ 在 $[0, c_{\\max}]$ 上是均匀的，因此信道容量必然相等。\n\nC. 异方差噪声通过映射 $f(c)$ 的局部灵敏度和局部噪声幅度来塑造最优输入分布，得到 $p^*(c) \\propto \\frac{|f'(c)|}{\\sigma(c)}$，因此概率质量集中在 $|f'(c)|$ 大且 $\\sigma(c)$ 小的区域。\n\nD. 启动子 $B$ 的容量必须低于启动子 $A$，因为输入依赖的方差 $\\sigma_B^2(c)$ 相对于同方差情况总是增加不确定性。\n\nE. 如果在 $|f'(c)|$ 最大的浓度处选择性地减小 $\\sigma_B(c)$（同时保持动态范围固定），那么即使两个启动子具有相同的确定性 $f(c)$ 和动态范围，$C_B$ 也可以超过 $C_A$。",
            "solution": "问题要求评估关于两个转录调控元件（被建模为噪声信道）信道容量的几个陈述。问题的核心在于理解，对于具有非线性传递函数和潜在的输入依赖噪声的信道，在小噪声极限下信道容量是如何确定的。\n\n基本量是输入浓度 $C$ 和输出基因表达 $G$ 之间的互信息 (MI)，由下式给出：\n$$\n\\mathcal{I}(C;G) = \\mathcal{H}(G) - \\mathcal{H}(G|C)\n$$\n其中 $\\mathcal{H}(G)$ 是输出分布的熵，$\\mathcal{H}(G|C)$ 是给定输入下输出的条件熵。信道容量 $C$ 是在所有有效输入分布 $p(c)$ 上互信息的上确界：\n$$\nC = \\sup_{p(c)} \\mathcal{I}(C;G)\n$$\n\n条件熵 $\\mathcal{H}(G|C)$ 是在所有可能输入上输出噪声的平均熵。给定条件分布为高斯分布 $p(g|c) = \\mathcal{N}(f(c), \\sigma^2(c))$，对于特定输入 $c$，其熵为 $\\mathcal{H}(G|C=c) = \\frac{1}{2}\\log_2(2\\pi e \\sigma^2(c))$。因此，平均条件熵为：\n$$\n\\mathcal{H}(G|C) = \\int_0^{c_{\\max}} p(c) \\left[\\frac{1}{2}\\log_2(2\\pi e \\sigma^2(c))\\right] \\mathrm{d}c = \\int_0^{c_{\\max}} p(c) \\log_2(\\sqrt{2\\pi e}\\sigma(c)) \\mathrm{d}c\n$$\n\n在指定的小噪声情形下，即 $\\sigma(c)$ 与动态范围 $g_{\\max} - g_{\\min}$ 相比很小，容量可以被近似。信息论中关于此类信道的一个标准结果表明，容量由局部灵敏度 $|f'(c)|$ 与局部噪声水平 $\\sigma(c)$之比的积分决定。这个比率 $|f'(c)|/\\sigma(c)$ 可以解释为输入无穷小变化时的局部信噪比。容量近似由下式给出：\n$$\nC \\approx \\log_2 \\left( \\frac{1}{\\sqrt{2\\pi e}} \\int_0^{c_{\\max}} \\frac{|f'(c)|}{\\sigma(c)} \\mathrm{d}c \\right)\n$$\n当选择输入概率分布 $p^*(c)$ 使得“信息密度”在整个输入范围内均匀时，可以达到这个最大信息率。这导出了最优输入分布：\n$$\np^*(c) \\propto \\frac{|f'(c)|}{\\sigma(c)}\n$$\n该策略将更多概率分配给信息量更大的输入区域，即输出对输入高度敏感（高 $|f'(c)|$）且输出测量精度高（低 $\\sigma(c)$）的区域。\n\n在这些原则建立之后，我们现在可以评估每个陈述。\n\n### 逐项分析\n\n**A. 在小噪声极限下，对于单调的 $f(c)$ 和高斯条件分布，两个启动子的信道容量满足...**\n\n我们将通用容量公式应用于每个启动子。\n\n对于启动子 A（同方差情况）：$\\sigma_A(c) = \\sigma_0$。\n$$\nC_A \\approx \\log_2 \\left( \\frac{1}{\\sqrt{2\\pi e}} \\int_0^{c_{\\max}} \\frac{|f'(c)|}{\\sigma_0} \\mathrm{d}c \\right)\n$$\n因为 $\\sigma_0$ 是一个常数，可以从积分中提出：\n$$\nC_A \\approx \\log_2 \\left( \\frac{1}{\\sqrt{2\\pi e}\\,\\sigma_0} \\int_0^{c_{\\max}} |f'(c)| \\mathrm{d}c \\right)\n$$\n由于 $f(c)$ 是严格单调的，$f'(c)$ 在区间 $(0, c_{\\max})$ 上符号不变。因此，积分就是 $f(c)$ 在整个定义域上的总变化，即动态范围：\n$$\n\\int_0^{c_{\\max}} |f'(c)| \\mathrm{d}c = |f(c_{\\max}) - f(0)| = g_{\\max} - g_{\\min}\n$$\n将此代回，我们得到：\n$$\nC_A \\approx \\log_2\\left(\\frac{g_{\\max} - g_{\\min}}{\\sqrt{2\\pi e}\\,\\sigma_0}\\right)\n$$\n\n对于启动子 B（异方差情况）：$\\sigma_B(c)$ 是 $c$ 的函数。\n我们直接使用通用公式：\n$$\nC_B \\approx \\log_2\\left(\\frac{1}{\\sqrt{2\\pi e}} \\int_0^{c_{\\max}} \\frac{|f'(c)|}{\\sigma_B(c)}\\,\\mathrm{d}c\\right)\n$$\n陈述 A 中的两个表达式都与我们的推导相符。\n\n结论：**正确**。\n\n**B. 因为两个启动子的动态范围相同，所以最优输入分布 $p^*(c)$ 在 $[0, c_{\\max}]$ 上是均匀的，因此信道容量必然相等。**\n\n这个陈述包含几个不正确的论断。\n首先，最优输入分布是 $p^*(c) \\propto \\frac{|f'(c)|}{\\sigma(c)}$。要使其为均匀分布（$p^*(c) = \\text{常数}$），我们需要 $|f'(c)|/\\sigma(c) = \\text{常数}$。对于启动子 A（同方差，$\\sigma(c)=\\sigma_0$），这要求 $|f'(c)|$ 为常数，意味着 $f(c)$ 必须是线性函数。问题没有说明 $f(c)$ 是线性的；它是一个一般的单调函数。对于启动子 B，这个条件甚至更具限制性。因此，$p^*(c)$ 是均匀分布的前提是错误的。\n其次，容量必然相等的结论是错误的。如选项 A 的分析所示，$C_A$ 取决于 $\\sigma_0$ 的值，而 $C_B$ 取决于 $|f'(c)|/\\sigma_B(c)$ 的积分。这两个量通常不相等。例如，如果我们简单地将 $\\sigma_B(c)$ 设置为一个不同于 $\\sigma_0$ 的常数，比如 $\\sigma_B(c) = \\sigma_1$，那么如果 $\\sigma_1  \\sigma_0$ 则 $C_B > C_A$，如果 $\\sigma_1 > \\sigma_0$ 则 $C_B  C_A$。\n\n结论：**不正确**。\n\n**C. 异方差噪声通过映射 $f(c)$ 的局部灵敏度和局部噪声幅度来塑造最优输入分布，得到 $p^*(c) \\propto \\frac{|f'(c)|}{\\sigma(c)}$，因此概率质量集中在 $|f'(c)|$ 大且 $\\sigma(c)$ 小的区域。**\n\n这个陈述准确地描述了在这种情形下实现信道容量的最优输入分布 $p^*(c)$。表达式 $p^*(c) \\propto \\frac{|f'(c)|}{\\sigma(c)}$ 表明，为了最大化信息传输，系统应优先从局部增益 $|f'(c)|$ 高且局部噪声 $\\sigma(c)$ 低的区域采样输入。比率 $|f'(c)|/\\sigma(c)$ 量化了每个输入点的“信息量”，最优策略是使输入分布偏向信息量更丰富的区域。文字描述是对该数学比例关系的正确解释。\n\n结论：**正确**。\n\n**D. 启动子 $B$ 的容量必须低于启动子 $A$，因为输入依赖的方差 $\\sigma_B^2(c)$ 相对于同方差情况总是增加不确定性。**\n\n这个陈述是错误的。B 相对于 A 的容量取决于 $\\int_0^{c_{\\max}} \\frac{|f'(c)|}{\\sigma_B(c)} \\mathrm{d}c$ 和 $\\int_0^{c_{\\max}} \\frac{|f'(c)|}{\\sigma_0} \\mathrm{d}c$ 之间的比较。问题中没有给出 $\\sigma_B(c)$ 和 $\\sigma_0$ 之间的*先验*关系，所以我们无法做出明确的比较。例如，如果我们简单地将 $\\sigma_B(c) = \\sigma_1$（一个常数），那么如果 $\\sigma_1  \\sigma_0$，则 $C_B > C_A$，如果 $\\sigma_1 > \\sigma_0$，则 $C_B  C_A$。更一般地，可以调整 $\\sigma_B(c)$ 的特性，使其相对于同方差情况既能增加也能减少容量。声称 B “必须”具有更低容量的说法过于绝对且不正确。\n\n结论：**不正确**。\n\n**E. 如果在 $|f'(c)|$ 最大的浓度处选择性地减小 $\\sigma_B(c)$（同时保持动态范围固定），那么即使两个启动子具有相同的确定性 $f(c)$ 和动态范围，$C_B$ 也可以超过 $C_A$。**\n\n这个陈述探讨了塑造噪声特性 $\\sigma_B(c)$ 的后果。容量 $C_B$ 由积分 $\\int_0^{c_{\\max}} \\frac{|f'(c)|}{\\sigma_B(c)} \\mathrm{d}c$ 决定。被积函数是两项的乘积：$|f'(c)|$ 和 $1/\\sigma_B(c)$。为了最大化该积分，应该在 $|f'(c)|$ 大的地方使 $1/\\sigma_B(c)$ 也大。这意味着在高灵敏度区域 $|f'(c)|$ 减小噪声 $\\sigma_B(c)$。如果将 $\\sigma_B(c)$ 设计得比 $\\sigma_0$ 小，特别是在 $|f'(c)|$ 最大的区域，那么这些区域对积分的贡献会变得非常大。完全有可能构建一个噪声特性 $\\sigma_B(c)$，使得：\n$$\n\\int_0^{c_{\\max}} \\frac{|f'(c)|}{\\sigma_B(c)} \\mathrm{d}c > \\int_0^{c_{\\max}} \\frac{|f'(c)|}{\\sigma_0} \\mathrm{d}c = \\frac{g_{\\max}-g_{\\min}}{\\sigma_0}\n$$\n在这种情况下，将直接得出 $C_B > C_A$。因此，通过在最敏感的输入区域抑制噪声，异方差噪声提供了一个机会来*增加*信道容量，使其超过可比较的同方差系统。因此，声称 $C_B$ “可以超过” $C_A$ 是正确的。\n\n结论：**正确**。",
            "answer": "$$\\boxed{ACE}$$"
        },
        {
            "introduction": "最后，我们将从单个元件扩展到网络层面，解决一个合成生物学中的核心设计挑战。本练习要求你通过编程实现一个组合优化过程，以探索信息传输能力与网络连接成本（稀疏性）之间的权衡。通过为模型的连接矩阵 $W$ 设计一个包含信息 $I(X;Y)$ 和稀疏惩罚项 $\\lambda \\lVert W \\rVert_0$ 的目标函数，你将亲身体验如何在有限的生物资源下，设计出既高效又经济的调控网络 。",
            "id": "3915439",
            "problem": "您的任务是，在一个受稀疏性约束的合成基因调控网络中，对信息传输容量与简易性之间的权衡进行建模。该网络通过一个相互作用矩阵 $W \\in \\mathbb{R}^{N \\times M}$ 将 $M$ 个独立的输入信号映射到 $N$ 个输出基因表达水平，并伴有加性高斯噪声。目标是针对网络拓扑（即哪些相互作用存在）进行组合优化，以量化当对相互作用的数量施加惩罚时，容量与简易性之间的权衡关系。\n\n基本原理。使用 Shannon 信息论中的定义。令 $X \\in \\mathbb{R}^{M}$ 表示输入向量，$Y \\in \\mathbb{R}^{N}$ 表示输出向量。系统模型为 $Y = W X + N$，其中 $X$ 和 $N$ 是独立的高斯随机向量，满足 $X \\sim \\mathcal{N}(0, \\sigma_x^2 I_M)$ 和 $N \\sim \\mathcal{N}(0, \\sigma_n^2 I_N)$。互信息 $I(X;Y)$ 定义为 $I(X;Y) = H(Y) - H(Y\\mid X)$，其中 $H(\\cdot)$ 表示微分熵。在高斯情况下，$H(Z)$ 取决于 $Z$ 的协方差矩阵的行列式 $\\det(\\Sigma_Z)$。请推导并使用在此线性高斯设置下 $I(X;Y)$ 的闭式表达式（以比特为单位）。\n\n稀疏性惩罚。简易性通过 $W$ 中非零相互作用的数量来建模，记作零“范数” $\\lVert W \\rVert_0$，它计算非零元素的数量。优化目标是\n$$\nJ(W) = I(X;Y) - \\lambda \\lVert W \\rVert_0,\n$$\n其中 $\\lambda \\geq 0$ 是稀疏性惩罚系数。假设每个存在的相互作用都具有固定的强度 $g  0$，而不存在的相互作用则恰好为 $0$。不考虑相互作用的符号，只关心其是否存在。因此，每个元素 $W_{ij} \\in \\{0, g\\}$。\n\n组合搜索。对于给定的 $(N,M)$，$W$ 可以由一个二元邻接矩阵 $A \\in \\{0,1\\}^{N \\times M}$ 表示，其中 $W = g A$。枚举所有 $2^{N M}$ 种可能的邻接矩阵，以找到使 $J(W)$ 最大化的那一个。若出现平局，则选择 $\\lVert W \\rVert_0$ 最小的拓扑；若仍为平局，则选择下文定义的编码邻接整数最小的那个。\n\n拓扑编码。使用行主序将每个拓扑 $A$ 编码为一个非负整数 $E$，规则如下：对于对应于元素 $A_{ij}$ 的索引 $k = i M + j$，位置 $k$ 上的比特（最低有效位对应于 $k=0$）为 $A_{ij}$。因此，\n$$\nE = \\sum_{i=0}^{N-1}\\sum_{j=0}^{M-1} A_{ij} \\, 2^{i M + j}.\n$$\n\n您的任务。使用高斯模型推导 $I(X;Y)$（以比特为单位），并实现一个程序，该程序能够：\n- 枚举所有拓扑 $A$。\n- 根据 $W = g A$ 和给定的 $(\\sigma_x, \\sigma_n)$ 计算 $I(X;Y)$（以比特为单位）。\n- 计算每个拓扑的 $J(W)$。\n- 根据平局决胜规则选择最优拓扑。\n\n对于每个测试用例，返回以下列表：\n- $C^\\star$：最优拓扑下的无惩罚互信息（以比特为单位），四舍五入到四位小数。\n- $K^\\star$：最优拓扑中的相互作用数量，即 $\\lVert W \\rVert_0$。\n- $J^\\star$：带惩罚的目标函数值（以比特为单位），四舍五入到四位小数。\n- $E^\\star$：如上定义的最优拓扑的编码邻接整数。\n\n答案单位。以比特报告信息值。不涉及其他物理单位。\n\n角度单位。不适用。\n\n百分比。不适用。\n\n测试套件。请使用以下参数集：\n1. $(N, M, \\sigma_x, \\sigma_n, g, \\lambda) = (3, 2, 1.0, 0.25, 1.0, 0.2)$\n2. $(N, M, \\sigma_x, \\sigma_n, g, \\lambda) = (3, 2, 1.0, 1.0, 1.0, 10.0)$\n3. $(N, M, \\sigma_x, \\sigma_n, g, \\lambda) = (3, 2, 0.0, 1.0, 1.0, 0.0)$\n4. $(N, M, \\sigma_x, \\sigma_n, g, \\lambda) = (2, 2, 2.0, 0.5, 0.7, 0.1)$\n\n最终输出格式。您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表，列表中的每一项本身都是一个按 $\\left[C^\\star, K^\\star, J^\\star, E^\\star\\right]$ 顺序排列的列表。例如，输出格式为\n$$\n\\texttt{[[C1,K1,J1,E1],[C2,K2,J2,E2],[C3,K3,J3,E3],[C4,K4,J4,E4]]}。\n$$",
            "solution": "该问题要求执行组合优化，以找到一个基因调控网络拓扑，该拓扑能够最大化一个平衡了信息容量和网络简易性的目标函数。解决方案包括两个主要阶段：首先，推导互信息的闭式表达式；其次，实现对所有可能网络拓扑的穷举搜索。\n\n### 1. 互信息的推导\n\n系统由线性方程 $Y = WX + N$ 建模，其中 $Y \\in \\mathbb{R}^{N}$ 是基因表达水平的输出向量，$X \\in \\mathbb{R}^{M}$ 是输入信号向量，$W \\in \\mathbb{R}^{N \\times M}$ 是相互作用矩阵，$N \\in \\mathbb{R}^{N}$ 是加性高斯噪声向量。\n\n各分量定义如下：\n- 输入信号：$X \\sim \\mathcal{N}(0, \\sigma_x^2 I_M)$，其中 $\\sigma_x^2$ 是输入方差，$I_M$ 是 $M \\times M$ 单位矩阵。\n- 噪声：$N \\sim \\mathcal{N}(0, \\sigma_n^2 I_N)$，其中 $\\sigma_n^2$ 是噪声方差，$I_N$ 是 $N \\times N$ 单位矩阵。\n- 输入 $X$ 和噪声 $N$ 是独立的。\n\n以比特为单位的互信息 $I(X;Y)$ 由 $I(X;Y) = H(Y) - H(Y|X)$ 给出，其中 $H(\\cdot)$ 是微分熵。对于一个协方差矩阵为 $\\Sigma_Z$ 的 $k$ 维高斯向量 $Z$，其熵为 $H(Z) = \\frac{1}{2} \\log_2 \\det(2 \\pi e \\Sigma_Z)$。\n\n**第一步：计算输出的熵，$H(Y)$**\n由于 $Y$ 是高斯向量 $X$ 和 $N$ 的线性变换，因此它也是一个高斯向量。\n$Y$ 的均值为 $E[Y] = E[WX + N] = W E[X] + E[N] = 0$。\n$Y$ 的协方差矩阵，记作 $\\Sigma_Y$，为：\n$$ \\Sigma_Y = E[YY^T] = E[(WX+N)(WX+N)^T] = E[WXX^TW^T + WXN^T + NX^TW^T + NN^T] $$\n由于 $X$ 和 $N$ 的独立性和零均值特性，交叉项的期望为零：$E[WXN^T] = 0$ 和 $E[NX^TW^T] = 0$。\n$$ \\Sigma_Y = W E[XX^T] W^T + E[NN^T] = W(\\sigma_x^2 I_M)W^T + \\sigma_n^2 I_N = \\sigma_x^2 WW^T + \\sigma_n^2 I_N $$\n输出的分布为 $Y \\sim \\mathcal{N}(0, \\sigma_x^2 WW^T + \\sigma_n^2 I_N)$。\n$Y$ 的熵为：\n$$ H(Y) = \\frac{1}{2} \\log_2 \\det(2 \\pi e \\Sigma_Y) = \\frac{1}{2} \\log_2 \\det(2 \\pi e (\\sigma_x^2 WW^T + \\sigma_n^2 I_N)) $$\n\n**第二步：计算条件熵，$H(Y|X)$**\n给定一个特定的实现 $X=x$，条件输出为 $Y|X=x = Wx + N$。这是一个均值为 $Wx$，协方差矩阵为 $\\Sigma_{Y|X} = \\Sigma_N = \\sigma_n^2 I_N$ 的高斯向量。该条件分布的熵为：\n$$ H(Y|X=x) = \\frac{1}{2} \\log_2 \\det(2 \\pi e (\\sigma_n^2 I_N)) $$\n由于此表达式不依赖于 $x$ 的具体值，因此条件熵 $H(Y|X)$ 等于 $H(Y|X=x)$。\n\n**第三步：合并求互信息，$I(X;Y)$**\n$$ I(X;Y) = H(Y) - H(Y|X) = \\frac{1}{2} \\log_2 \\left( \\frac{\\det(2 \\pi e (\\sigma_x^2 WW^T + \\sigma_n^2 I_N))}{\\det(2 \\pi e \\sigma_n^2 I_N)} \\right) $$\n使用行列式性质 $\\det(cA) = c^k \\det(A)$（对于一个 $k \\times k$ 矩阵 $A$），项 $(2\\pi e)^N$ 被消去：\n$$ I(X;Y) = \\frac{1}{2} \\log_2 \\left( \\frac{\\det(\\sigma_x^2 WW^T + \\sigma_n^2 I_N)}{\\det(\\sigma_n^2 I_N)} \\right) $$\n从分子的行列式中提出因子 $\\sigma_n^2$ 得到：\n$$ \\det(\\sigma_x^2 WW^T + \\sigma_n^2 I_N) = \\det(\\sigma_n^2 (I_N + \\frac{\\sigma_x^2}{\\sigma_n^2} WW^T)) = (\\sigma_n^2)^N \\det(I_N + \\frac{\\sigma_x^2}{\\sigma_n^2} WW^T) $$\n分母为 $\\det(\\sigma_n^2 I_N) = (\\sigma_n^2)^N$。代入这些表达式，得到互信息（以比特为单位）的最终公式：\n$$ I(X;Y) = \\frac{1}{2} \\log_2 \\det\\left(I_N + \\frac{\\sigma_x^2}{\\sigma_n^2} WW^T\\right) $$\n\n### 2. 组合优化算法\n\n问题要求找到使目标函数 $J(W) = I(X;Y) - \\lambda \\lVert W \\rVert_0$ 最大化的矩阵 $W$。$W$ 的元素被约束为 $0$ 或一个固定值 $g  0$。这意味着对于一个由二元邻接矩阵 $A \\in \\{0,1\\}^{N \\times M}$ 表示的给定拓扑，其相互作用矩阵为 $W = gA$。相互作用的数量为 $\\lVert W \\rVert_0 = \\sum_{i,j} A_{ij}$。\n\n由于对于给定的参数，可能拓扑的数量 $2^{NM}$ 很小（$2^{3 \\times 2}=64$ 和 $2^{2 \\times 2}=16$），因此穷举搜索是可行的。算法如下：\n\n1.  **遍历所有拓扑**：每个拓扑对应于一个从 $0$ 到 $2^{NM}-1$ 的唯一整数 $E$。整数 $E$ 以行主序编码二元矩阵 $A$，其中 $E = \\sum_{i=0}^{N-1}\\sum_{j=0}^{M-1} A_{ij} 2^{iM+j}$。我们按递增顺序遍历 $E$。\n\n2.  **对于每个拓扑**：\n    a. **构造矩阵 A**：根据整数 $E$ 构造相应的 $N \\times M$ 二元矩阵 $A$。元素 $A_{ij}$ 是 $E$ 的二进制表示中位置 $k = iM+j$ 上的比特。\n    b. **计算相互作用数量 K**：非零相互作用的数量为 $K = \\lVert W \\rVert_0 = \\lVert A \\rVert_0$，这即是 $E$ 中置位比特（值为1的比特）的数量。\n    c. **计算互信息 C**：将 $W = gA$ 代入推导出的公式：\n       $$ C = I(X;Y) = \\frac{1}{2} \\log_2 \\det\\left(I_N + \\left(\\frac{g\\sigma_x}{\\sigma_n}\\right)^2 A A^T\\right) $$\n       如果 $\\sigma_x = 0$，会出现一个特殊情况，此时没有输入信号方差，无法传输信息，因此 $C=0$。\n    d. **计算目标函数 J**：计算 $J = C - \\lambda K$。\n\n3.  **选择最优拓扑**：我们维护迄今为止找到的最佳解，由 $(J^\\star, K^\\star, C^\\star, E^\\star)$ 表征。初始最佳目标值设为 $-\\infty$。对于每个新拓扑 $(J, K, C, E)$，如果它根据指定的平局决胜规则更优，我们就更新最佳解：\n    - 如果 $J > J^\\star$，则新解更优。\n    - 如果 $J = J^\\star$ 且 $K  K^\\star$，则新解更优（偏好更简单的网络）。\n    - 如果 $J = J^\\star$ 且 $K = K^\\star$，我们不更新。因为我们是按 $E$ 的递增顺序遍历的，所以当前存储的 $E^\\star$ 对于这个 $(J, K)$ 对来说已经是最小的可能值。\n\n4.  **存储结果**：遍历所有 $2^{NM}$ 个拓扑后，记录最优参数 $(C^\\star, K^\\star, J^\\star, E^\\star)$。信息值 $C^\\star$ 和 $J^\\star$ 四舍五入到四位小数。对每个测试用例重复此过程。\n\n这种系统性的方法保证了能找到问题陈述所定义的唯一最优拓扑。",
            "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Solves the combinatorial optimization problem for synthetic gene regulatory networks.\n    \"\"\"\n    test_cases = [\n        (3, 2, 1.0, 0.25, 1.0, 0.2),\n        (3, 2, 1.0, 1.0, 1.0, 10.0),\n        (3, 2, 0.0, 1.0, 1.0, 0.0),\n        (2, 2, 2.0, 0.5, 0.7, 0.1),\n    ]\n\n    all_results = []\n\n    for params in test_cases:\n        N, M, sigma_x, sigma_n, g, lambda_penalty = params\n        \n        num_topologies = 1  (N * M)\n        \n        best_J = -float('inf')\n        best_C = -1.0\n        best_K = N * M + 1 # Initialize with a value larger than any possible K\n        best_E = -1\n\n        for E in range(num_topologies):\n            # 1. Construct adjacency matrix A and count interactions K\n            A = np.zeros((N, M), dtype=int)\n            K = 0\n            for k in range(N * M):\n                if (E >> k)  1:\n                    i = k // M\n                    j = k % M\n                    A[i, j] = 1\n                    K += 1\n            \n            # 2. Calculate mutual information C\n            if sigma_x == 0.0:\n                C = 0.0\n            else:\n                ratio_sq = (g * sigma_x / sigma_n)**2\n                A_AT = A @ A.T\n                mat_for_det = np.eye(N) + ratio_sq * A_AT\n                \n                det_val = np.linalg.det(mat_for_det)\n                \n                if det_val = 1e-9: # Use tolerance for float comparison\n                    C = 0.0\n                else:\n                    C = 0.5 * math.log2(det_val)\n\n            # 3. Calculate the penalized objective function J\n            J = C - lambda_penalty * K\n\n            # 4. Update best solution based on optimization criteria\n            is_new_best = False\n            # Use a small tolerance for floating point comparisons\n            if J > best_J + 1e-9:\n                is_new_best = True\n            elif abs(J - best_J)  1e-9:\n                if K  best_K:\n                    is_new_best = True\n            # The third tie-breaker (smallest E) is automatically handled by the loop order.\n            \n            if is_new_best:\n                best_J = J\n                best_C = C\n                best_K = K\n                best_E = E\n        \n        # Format results for the current test case\n        c_star_rounded = round(best_C, 4)\n        j_star_rounded = round(best_J, 4)\n        \n        if c_star_rounded == -0.0: c_star_rounded = 0.0\n        if j_star_rounded == -0.0: j_star_rounded = 0.0\n\n        result_tuple = [c_star_rounded, best_K, j_star_rounded, best_E]\n        all_results.append(result_tuple)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"[{c},{k},{j},{e}]\" for c, k, j, e in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}