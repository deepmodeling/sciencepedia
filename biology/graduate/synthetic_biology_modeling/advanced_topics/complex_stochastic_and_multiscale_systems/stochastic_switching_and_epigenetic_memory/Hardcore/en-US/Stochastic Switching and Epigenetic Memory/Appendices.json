{
    "hands_on_practices": [
        {
            "introduction": "The deterministic behavior of a gene circuit often defines a conceptual \"landscape\" upon which stochastic fluctuations act. For a system to exhibit epigenetic memory, this landscape must feature multiple stable \"valleys,\" or fixed points, which correspond to distinct, heritable gene expression states. This practice introduces linear stability analysis as the primary tool for mapping this landscape. By finding the fixed points of the classic toggle switch model and analyzing the system's behavior near them, you will directly identify the stable memory states and the unstable saddle point that represents the barrier to noise-induced switching .",
            "id": "3932610",
            "problem": "Consider a synthetic two-gene toggle switch with mutual transcriptional repression modeled by Hill-type regulatory functions. Let $x(t)$ and $y(t)$ denote the dimensionless protein concentrations of regulator $X$ and regulator $Y$, respectively. Assume degradation is first-order with rate constants $1\\,\\mathrm{min}^{-1}$ for both species, and maximum synthesis rates are balanced such that in the absence of repression the net production is $4\\,\\mathrm{min}^{-1}$ for each gene. Cooperativity (Hill coefficient) of repression is $2$ for both interactions. The dynamics are given by the deterministic ordinary differential equations\n$$\n\\frac{dx}{dt} = \\frac{4}{1 + y^{2}} - x,\\qquad \\frac{dy}{dt} = \\frac{4}{1 + x^{2}} - y,\n$$\nwhere $t$ is time in minutes. In the context of epigenetic memory, stable fixed points correspond to heritable expression states, and noise-induced switching between these epigenetic states is mediated by passage near an unstable saddle fixed point in the deterministic flow.\n\nTasks:\n1. Find all fixed points $(x^{\\ast}, y^{\\ast})$ by solving the steady-state conditions $\\frac{dx}{dt}=0$ and $\\frac{dy}{dt}=0$.\n2. Derive the Jacobian matrix of the vector field at a general point $(x,y)$ and evaluate it at each fixed point found in Task 1.\n3. Compute the eigenvalues at each fixed point and determine their linear stability classification. Identify the unstable saddle fixed point that mediates noise-induced switching.\n4. From your analysis, extract the positive (unstable) eigenvalue of the Jacobian at the saddle fixed point. Report this value as your final answer. Round your answer to four significant figures and express it in $\\mathrm{min}^{-1}$.",
            "solution": "We begin from the foundational modeling picture of gene regulation and the Central Dogma of molecular biology, which motivates ordinary differential equation models for protein concentration dynamics under transcriptional regulation. Mutual repression in a toggle switch is captured by Hill-type input-output functions. The deterministic dynamics given are\n$$\n\\frac{dx}{dt} = f(x,y) = \\frac{4}{1+y^{2}} - x,\\qquad \\frac{dy}{dt} = g(x,y) = \\frac{4}{1+x^{2}} - y,\n$$\nwith linear degradation at rate $1\\,\\mathrm{min}^{-1}$ for both $x$ and $y$ and symmetric Hill repression with coefficient $2$.\n\nTask 1: Fixed points are defined by $f(x^{\\ast},y^{\\ast})=0$ and $g(x^{\\ast},y^{\\ast})=0$, i.e.,\n$$\nx^{\\ast} = \\frac{4}{1+(y^{\\ast})^{2}},\\qquad y^{\\ast} = \\frac{4}{1+(x^{\\ast})^{2}}.\n$$\nBy symmetry, one fixed point satisfies $x^{\\ast}=y^{\\ast}=s$, where $s$ solves\n$$\ns = \\frac{4}{1+s^{2}}\\quad\\Longleftrightarrow\\quad s(1+s^{2}) = 4\\quad\\Longleftrightarrow\\quad s^{3} + s - 4 = 0.\n$$\nThis cubic has one real root, which we can represent in closed form using Cardano’s formula for the depressed cubic $t^{3} + pt + q = 0$ with $p=1$ and $q=-4$. The discriminant is\n$$\n\\Delta = \\left(\\frac{q}{2}\\right)^{2} + \\left(\\frac{p}{3}\\right)^{3} = 4 + \\frac{1}{27} = \\frac{109}{27}.\n$$\nThus,\n$$\ns = \\sqrt[3]{2 + \\sqrt{\\frac{109}{27}}} + \\sqrt[3]{2 - \\sqrt{\\frac{109}{27}}}.\n$$\nIn addition to this symmetric fixed point, there are two asymmetric fixed points due to mutual repression: one with $x^{\\ast}$ large, $y^{\\ast}$ small, and one with $x^{\\ast}$ small, $y^{\\ast}$ large. These satisfy the steady-state equations above and can be obtained numerically by iteration. Approximations are:\n$$\n(x^{\\ast},y^{\\ast}) \\approx (3.737,\\,0.267),\\qquad (x^{\\ast},y^{\\ast}) \\approx (0.267,\\,3.737),\n$$\nwhich are consistent with the symmetric parameterization of the system.\n\nTask 2: The Jacobian matrix $J(x,y)$ of the vector field $(f,g)$ at a general point $(x,y)$ is defined by\n$$\nJ(x,y) = \\begin{pmatrix}\n\\frac{\\partial f}{\\partial x}  \\frac{\\partial f}{\\partial y} \\\\\n\\frac{\\partial g}{\\partial x}  \\frac{\\partial g}{\\partial y}\n\\end{pmatrix}.\n$$\nWe compute the partial derivatives. Since $f(x,y) = \\frac{4}{1+y^{2}} - x$, we have\n$$\n\\frac{\\partial f}{\\partial x} = -1,\\qquad \\frac{\\partial f}{\\partial y} = 4\\,\\frac{d}{dy}\\left(\\frac{1}{1+y^{2}}\\right) = 4\\left(-\\frac{2y}{(1+y^{2})^{2}}\\right) = -\\frac{8y}{(1+y^{2})^{2}}.\n$$\nSimilarly, for $g(x,y) = \\frac{4}{1+x^{2}} - y$,\n$$\n\\frac{\\partial g}{\\partial x} = 4\\,\\frac{d}{dx}\\left(\\frac{1}{1+x^{2}}\\right) = 4\\left(-\\frac{2x}{(1+x^{2})^{2}}\\right) = -\\frac{8x}{(1+x^{2})^{2}},\\qquad \\frac{\\partial g}{\\partial y} = -1.\n$$\nTherefore,\n$$\nJ(x,y) = \\begin{pmatrix}\n-1  -\\dfrac{8y}{(1+y^{2})^{2}} \\\\\n-\\dfrac{8x}{(1+x^{2})^{2}}  -1\n\\end{pmatrix}.\n$$\n\nTask 3: The eigenvalues $\\lambda$ of a $2\\times 2$ matrix of the form\n$$\n\\begin{pmatrix}\n-1  -A \\\\\n- B  -1\n\\end{pmatrix}\n$$\nare given by the roots of the characteristic polynomial\n$$\n\\det\\!\\left(J - \\lambda I\\right) = \\left(-1 - \\lambda\\right)^{2} - A B = 0,\n$$\nwhich yields\n$$\n\\lambda_{\\pm} = -1 \\pm \\sqrt{A B},\n$$\nwhere\n$$\nA = \\frac{8y}{(1+y^{2})^{2}},\\qquad B = \\frac{8x}{(1+x^{2})^{2}}.\n$$\nStability at a fixed point $(x^{\\ast}, y^{\\ast})$ depends on the signs of $\\lambda_{\\pm}$. If $\\sqrt{A B}  1$, then $\\lambda_{+} = -1 + \\sqrt{A B}  0$ and $\\lambda_{-} = -1 - \\sqrt{A B}  0$, so the fixed point is linearly stable. If $\\sqrt{A B}  1$, then $\\lambda_{+}  0$ and $\\lambda_{-}  0$, and the fixed point is a saddle with one unstable direction.\n\nWe now classify the fixed points found in Task 1. For the asymmetric fixed points, numerically evaluating $A$ and $B$ shows:\n- At $(x^{\\ast},y^{\\ast}) \\approx (3.737,\\,0.267)$, we have\n$$\nA \\approx \\frac{8\\cdot 0.267}{(1+0.267^{2})^{2}} \\approx 1.861,\\qquad B \\approx \\frac{8\\cdot 3.737}{(1+3.737^{2})^{2}} \\approx 0.1335,\n$$\nso $A B \\approx 0.2487$ and $\\sqrt{A B} \\approx 0.4987  1$. Thus both eigenvalues are negative: $\\lambda_{\\pm} \\approx -1 \\pm 0.4987$, a stable fixed point. By symmetry, $(x^{\\ast},y^{\\ast}) \\approx (0.267,\\,3.737)$ is also stable.\n- At the symmetric fixed point $(x^{\\ast},y^{\\ast}) = (s,s)$ with $s$ defined above, we can simplify $A$ and $B$ using the steady-state relation. From $s = \\dfrac{4}{1+s^{2}}$ we infer $1+s^{2} = \\dfrac{4}{s}$, hence\n$$\nA = \\frac{8 s}{(1+s^{2})^{2}} = \\frac{8 s}{\\left(\\dfrac{4}{s}\\right)^{2}} = \\frac{8 s}{\\dfrac{16}{s^{2}}} = \\frac{8 s^{3}}{16} = \\frac{s^{3}}{2},\n$$\nand similarly $B = \\dfrac{s^{3}}{2}$. Therefore $A B = \\left(\\dfrac{s^{3}}{2}\\right)^{2}$ and\n$$\n\\sqrt{A B} = \\frac{s^{3}}{2}.\n$$\nUsing the cubic steady-state condition $s^{3} + s - 4 = 0$, we have $s^{3} = 4 - s$, so\n$$\n\\sqrt{A B} = \\frac{4 - s}{2} = 2 - \\frac{s}{2}.\n$$\nThe Jacobian eigenvalues at the symmetric fixed point are\n$$\n\\lambda_{\\pm} = -1 \\pm \\left(2 - \\frac{s}{2}\\right),\n$$\nso\n$$\n\\lambda_{+} = 1 - \\frac{s}{2},\\qquad \\lambda_{-} = -3 + \\frac{s}{2}.\n$$\nSince $s$ is positive and numerically $s \\approx 1.3788$ (obtained from the closed-form expression for $s$), we find\n$$\n\\lambda_{+} \\approx 1 - \\frac{1.3788}{2} \\approx 0.3106,\\qquad \\lambda_{-} \\approx -3 + \\frac{1.3788}{2} \\approx -2.3106.\n$$\nThus the symmetric fixed point is a saddle with one positive (unstable) eigenvalue and one negative (stable) eigenvalue. This saddle mediates noise-induced switching between the two stable epigenetic memory states in the presence of stochastic fluctuations.\n\nTask 4: The positive (unstable) eigenvalue at the saddle is $\\lambda_{+} = 1 - \\dfrac{s}{2}$, with\n$$\ns = \\sqrt[3]{2 + \\sqrt{\\frac{109}{27}}} + \\sqrt[3]{2 - \\sqrt{\\frac{109}{27}}}.\n$$\nEvaluating this expression numerically yields $\\lambda_{+} \\approx 0.3106\\,\\mathrm{min}^{-1}$. Rounded to four significant figures, the value is $0.3106\\,\\mathrm{min}^{-1}$.",
            "answer": "$$\\boxed{0.3106}$$"
        },
        {
            "introduction": "While theoretical models provide powerful insights, their parameters must be learned from experimental data, such as single-cell \"snapshots\" of mRNA counts. This exercise explores the concept of parameter identifiability, a critical and often subtle issue in statistical inference. Using the foundational telegraph model of gene expression, you will investigate whether the key parameters governing promoter switching and transcription can be uniquely determined from such data . By deriving the Fisher Information Matrix, you will uncover a fundamental limitation of this experimental design, providing a crucial lesson on the interplay between model structure and data requirements.",
            "id": "3932621",
            "problem": "Consider a promoter governed by the classical two-state telegraph model in a synthetic biology context where epigenetic modifications confer memory by altering promoter switching propensities. The promoter toggles between an OFF state and an ON state as a continuous-time Markov chain with transition rate from OFF to ON equal to $k_{\\mathrm{on}}$ and transition rate from ON to OFF equal to $k_{\\mathrm{off}}$. When the promoter is ON, messenger ribonucleic acid (mRNA) is transcribed at rate $s$, and each mRNA degrades independently at rate $\\gamma$. Assume that $\\gamma$ is known from prior measurements. Epigenetic memory modulates $k_{\\mathrm{on}}$ and $k_{\\mathrm{off}}$ but does not directly alter $s$ or $\\gamma$.\n\nYou perform a single-time-point, single-cell snapshot experiment on a homogeneous population at steady state, obtaining $N$ independent mRNA counts $x_{1}, \\dots, x_{N}$ from different cells. In the regime where promoter switching is fast compared to mRNA lifetime, treat the snapshot mRNA count $X$ as approximately $\\mathrm{Poisson}(m(\\theta))$, where the parameter vector is $\\theta = (s, k_{\\mathrm{on}}, k_{\\mathrm{off}})$ and $m(\\theta)$ is the steady-state mean mRNA count determined by the balance of production and degradation in the telegraph model.\n\nStarting only from core continuous-time Markov chain facts and steady-state flux balance in the telegraph model, and the definition that the stationary ON-state occupancy is $p_{\\mathrm{on}} = \\frac{k_{\\mathrm{on}}}{k_{\\mathrm{on}} + k_{\\mathrm{off}}}$, do the following:\n\n1. Derive the expression for the mean $m(\\theta)$ under the fast-switching steady-state approximation in terms of $s$, $k_{\\mathrm{on}}$, $k_{\\mathrm{off}}$, and $\\gamma$.\n2. Write the likelihood $L(\\theta)$ of the observed counts under the $\\mathrm{Poisson}(m(\\theta))$ model and derive the Fisher information matrix (FIM) $I(\\theta)$ for $\\theta = (s, k_{\\mathrm{on}}, k_{\\mathrm{off}})$, defined as the expected negative Hessian of the log-likelihood.\n3. Assess identifiability of the parameters from single-time-point snapshot data by computing the determinant of $I(\\theta)$ in closed form.\n\nProvide your final answer as the closed-form expression for the determinant of the Fisher information matrix. No numerical substitution is required. If your final expression simplifies to a constant, write that constant. Express the final answer as a unitless quantity.",
            "solution": "The problem asks for an assessment of parameter identifiability for a simplified model of gene expression by calculating the determinant of the Fisher information matrix (FIM). The solution proceeds in three steps as requested: first, deriving the steady-state mean mRNA count; second, deriving the FIM; and third, computing its determinant.\n\n### Step 1: Derivation of the Mean mRNA Count $m(\\theta)$\n\nThe promoter is modeled as a two-state continuous-time Markov chain with states OFF and ON. Let $p_{\\mathrm{off}}(t)$ and $p_{\\mathrm{on}}(t)$ be the probabilities of the promoter being in the OFF and ON states at time $t$, respectively. The evolution of these probabilities is described by the master equation:\n$$\n\\frac{d}{dt} \\begin{pmatrix} p_{\\mathrm{off}}(t) \\\\ p_{\\mathrm{on}}(t) \\end{pmatrix} = \\begin{pmatrix} -k_{\\mathrm{on}}  k_{\\mathrm{off}} \\\\ k_{\\mathrm{on}}  -k_{\\mathrm{off}} \\end{pmatrix} \\begin{pmatrix} p_{\\mathrm{off}}(t) \\\\ p_{\\mathrm{on}}(t) \\end{pmatrix}\n$$\nAt steady state, the probabilities are constant, so $\\frac{dp_{\\mathrm{off}}}{dt} = 0$ and $\\frac{dp_{\\mathrm{on}}}{dt} = 0$. This leads to the flux balance equation:\n$$\nk_{\\mathrm{on}} p_{\\mathrm{off}} = k_{\\mathrm{off}} p_{\\mathrm{on}}\n$$\nwhere $p_{\\mathrm{on}}$ and $p_{\\mathrm{off}}$ now denote the stationary probabilities. We also have the constraint that the probabilities must sum to one:\n$$\np_{\\mathrm{on}} + p_{\\mathrm{off}} = 1\n$$\nSubstituting $p_{\\mathrm{off}} = 1 - p_{\\mathrm{on}}$ into the balance equation gives:\n$$\nk_{\\mathrm{on}}(1-p_{\\mathrm{on}}) = k_{\\mathrm{off}} p_{\\mathrm{on}}\n$$\n$$\nk_{\\mathrm{on}} = p_{\\mathrm{on}}(k_{\\mathrm{on}} + k_{\\mathrm{off}})\n$$\nSolving for $p_{\\mathrm{on}}$ yields the stationary probability of the promoter being in the ON state:\n$$\np_{\\mathrm{on}} = \\frac{k_{\\mathrm{on}}}{k_{\\mathrm{on}} + k_{\\mathrm{off}}}\n$$\nThis confirms the expression provided in the problem statement.\n\nNext, we consider the dynamics of the mean mRNA count, which we denote as $m$. The rate of change of the mean mRNA count is given by the balance of the average production rate and the average degradation rate:\n$$\n\\frac{dm}{dt} = \\langle \\text{production rate} \\rangle - \\langle \\text{degradation rate} \\rangle\n$$\nmRNA is produced at rate $s$ only when the promoter is in the ON state. Therefore, the average production rate is $s \\cdot p_{\\mathrm{on}}$. mRNA degrades at a rate $\\gamma$ per molecule, so the average degradation rate is $\\gamma m$. At steady state, $\\frac{dm}{dt} = 0$, leading to:\n$$\ns \\cdot p_{\\mathrm{on}} - \\gamma m = 0\n$$\nSolving for the steady-state mean mRNA count $m$ gives:\n$$\nm = \\frac{s \\cdot p_{\\mathrm{on}}}{\\gamma}\n$$\nSubstituting the expression for $p_{\\mathrm{on}}$ and denoting the mean as a function of the parameter vector $\\theta = (s, k_{\\mathrm{on}}, k_{\\mathrm{off}})$, we get:\n$$\nm(\\theta) = \\frac{s}{\\gamma} \\left( \\frac{k_{\\mathrm{on}}}{k_{\\mathrm{on}} + k_{\\mathrm{off}}} \\right)\n$$\n\n### Step 2: Derivation of the Fisher Information Matrix $I(\\theta)$\n\nThe problem states that under the fast-switching approximation, the snapshot mRNA count $X$ follows a Poisson distribution with mean $m(\\theta)$. Given $N$ independent observations $x_1, \\dots, x_N$, the likelihood function $L(\\theta)$ is the product of the individual Poisson probability mass functions:\n$$\nL(\\theta | \\{x_i\\}) = \\prod_{i=1}^{N} \\frac{m(\\theta)^{x_i} \\exp(-m(\\theta))}{x_i!}\n$$\nThe log-likelihood, $\\ell(\\theta) = \\ln(L(\\theta))$, is:\n$$\n\\ell(\\theta) = \\sum_{i=1}^{N} \\left( x_i \\ln(m(\\theta)) - m(\\theta) - \\ln(x_i!) \\right)\n$$\n$$\n\\ell(\\theta) = \\left( \\sum_{i=1}^{N} x_i \\right) \\ln(m(\\theta)) - N m(\\theta) - \\sum_{i=1}^{N} \\ln(x_i!)\n$$\nThe Fisher Information Matrix (FIM) is a $3 \\times 3$ matrix with elements $I_{ij}(\\theta)$ defined as the negative expectation of the Hessian of the log-likelihood:\n$$\nI_{ij}(\\theta) = -E\\left[ \\frac{\\partial^2 \\ell}{\\partial \\theta_i \\partial \\theta_j} \\right]\n$$\nwhere the parameters are $\\theta_1 = s$, $\\theta_2 = k_{\\mathrm{on}}$, and $\\theta_3 = k_{\\mathrm{off}}$.\nFirst, we find the first partial derivatives of $\\ell(\\theta)$ using the chain rule:\n$$\n\\frac{\\partial \\ell}{\\partial \\theta_j} = \\frac{\\partial \\ell}{\\partial m} \\frac{\\partial m}{\\partial \\theta_j} = \\left( \\frac{\\sum x_i}{m(\\theta)} - N \\right) \\frac{\\partial m(\\theta)}{\\partial \\theta_j}\n$$\nNext, we find the second partial derivatives:\n$$\n\\frac{\\partial^2 \\ell}{\\partial \\theta_i \\partial \\theta_j} = \\frac{\\partial}{\\partial \\theta_i} \\left[ \\left( \\frac{\\sum x_i}{m} - N \\right) \\frac{\\partial m}{\\partial \\theta_j} \\right] = -\\frac{\\sum x_i}{m^2} \\frac{\\partial m}{\\partial \\theta_i} \\frac{\\partial m}{\\partial \\theta_j} + \\left( \\frac{\\sum x_i}{m} - N \\right) \\frac{\\partial^2 m}{\\partial \\theta_i \\partial \\theta_j}\n$$\nNow we take the expectation over the data $\\{x_i\\}$. The expectation of a single observation is $E[x_i] = m(\\theta)$, so the expectation of the sum is $E[\\sum x_i] = N m(\\theta)$.\n$$\nE\\left[ \\frac{\\partial^2 \\ell}{\\partial \\theta_i \\partial \\theta_j} \\right] = -\\frac{E[\\sum x_i]}{m^2} \\frac{\\partial m}{\\partial \\theta_i} \\frac{\\partial m}{\\partial \\theta_j} + \\left( \\frac{E[\\sum x_i]}{m} - N \\right) \\frac{\\partial^2 m}{\\partial \\theta_i \\partial \\theta_j}\n$$\n$$\nE\\left[ \\frac{\\partial^2 \\ell}{\\partial \\theta_i \\partial \\theta_j} \\right] = -\\frac{N m}{m^2} \\frac{\\partial m}{\\partial \\theta_i} \\frac{\\partial m}{\\partial \\theta_j} + \\left( \\frac{N m}{m} - N \\right) \\frac{\\partial^2 m}{\\partial \\theta_i \\partial \\theta_j}\n$$\n$$\nE\\left[ \\frac{\\partial^2 \\ell}{\\partial \\theta_i \\partial \\theta_j} \\right] = -\\frac{N}{m} \\frac{\\partial m}{\\partial \\theta_i} \\frac{\\partial m}{\\partial \\theta_j} + (0) \\frac{\\partial^2 m}{\\partial \\theta_i \\partial \\theta_j} = -\\frac{N}{m} \\frac{\\partial m}{\\partial \\theta_i} \\frac{\\partial m}{\\partial \\theta_j}\n$$\nThus, the elements of the FIM are:\n$$\nI_{ij}(\\theta) = -E\\left[ \\frac{\\partial^2 \\ell}{\\partial \\theta_i \\partial \\theta_j} \\right] = \\frac{N}{m(\\theta)} \\frac{\\partial m(\\theta)}{\\partial \\theta_i} \\frac{\\partial m(\\theta)}{\\partial \\theta_j}\n$$\nLet $\\nabla_\\theta m$ be the gradient vector of $m(\\theta)$ with respect to $\\theta = (s, k_{\\mathrm{on}}, k_{\\mathrm{off}})^T$:\n$$\n\\nabla_\\theta m = \\begin{pmatrix} \\frac{\\partial m}{\\partial s} \\\\ \\frac{\\partial m}{\\partial k_{\\mathrm{on}}} \\\\ \\frac{\\partial m}{\\partial k_{\\mathrm{off}}} \\end{pmatrix}\n$$\nThe FIM can be written as the outer product of this gradient vector with itself, scaled by $\\frac{N}{m(\\theta)}$:\n$$\nI(\\theta) = \\frac{N}{m(\\theta)} (\\nabla_\\theta m) (\\nabla_\\theta m)^T\n$$\n\n### Step 3: Computation of the Determinant of $I(\\theta)$\n\nThe Fisher Information Matrix is a $3 \\times 3$ matrix of the form:\n$$\nI(\\theta) = \\frac{N}{m(\\theta)}\n\\begin{pmatrix}\n\\left(\\frac{\\partial m}{\\partial s}\\right)^2  \\frac{\\partial m}{\\partial s}\\frac{\\partial m}{\\partial k_{\\mathrm{on}}}  \\frac{\\partial m}{\\partial s}\\frac{\\partial m}{\\partial k_{\\mathrm{off}}} \\\\\n\\frac{\\partial m}{\\partial k_{\\mathrm{on}}}\\frac{\\partial m}{\\partial s}  \\left(\\frac{\\partial m}{\\partial k_{\\mathrm{on}}}\\right)^2  \\frac{\\partial m}{\\partial k_{\\mathrm{on}}}\\frac{\\partial m}{\\partial k_{\\mathrm{off}}} \\\\\n\\frac{\\partial m}{\\partial k_{\\mathrm{off}}}\\frac{\\partial m}{\\partial s}  \\frac{\\partial m}{\\partial k_{\\mathrm{off}}}\\frac{\\partial m}{\\partial k_{\\mathrm{on}}}  \\left(\\frac{\\partial m}{\\partial k_{\\mathrm{off}}}\\right)^2\n\\end{pmatrix}\n$$\nThis matrix is constructed as the outer product of a single vector, $\\nabla_\\theta m$, with itself. A matrix formed by the outer product of two non-zero vectors $\\mathbf{u}\\mathbf{v}^T$ has rank 1. In our case, the matrix is of the form $\\mathbf{v}\\mathbf{v}^T$, which also has rank 1 (assuming $\\mathbf{v} \\neq \\mathbf{0}$).\n\nThe second column of $I(\\theta)$ is $\\left(\\frac{\\partial m}{\\partial k_{\\mathrm{on}}} / \\frac{\\partial m}{\\partial s}\\right)$ times the first column. The third column is $\\left(\\frac{\\partial m}{\\partial k_{\\mathrm{off}}} / \\frac{\\partial m}{\\partial s}\\right)$ times the first column (assuming $\\frac{\\partial m}{\\partial s} \\neq 0$). Since all columns are scalar multiples of each other, they are linearly dependent.\n\nA fundamental property of determinants is that if the columns (or rows) of a square matrix are linearly dependent, its determinant is zero. The rank of the FIM is 1, which for a $3 \\times 3$ matrix is less than the dimension of the matrix ($3$). Therefore, the matrix is singular.\n\nThe determinant of the FIM is thus:\n$$\n\\det(I(\\theta)) = 0\n$$\nThis result signifies that the parameters $\\theta = (s, k_{\\mathrm{on}}, k_{\\mathrm{off}})$ are structurally non-identifiable from single-time-point snapshot data under the Poisson approximation. The measurement of the mean mRNA count $m(\\theta)$ provides only one constraint on the three unknown parameters, making it impossible to determine them uniquely.",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "Stochastic switching between epigenetic states is a defining feature of cellular memory, but these events are often intrinsically rare, making them difficult to study with standard simulations. This practice introduces importance sampling, an advanced Monte Carlo method that accelerates rare event simulation by temporarily biasing the system's dynamics toward the transition of interest. You will derive the exact reweighting factor needed to remove this bias and recover an unbiased estimate of the switching rate, a result rooted in the physics of stochastic processes . This exercise provides hands-on experience with a powerful tool used to probe the stability of epigenetic memory.",
            "id": "3932586",
            "problem": "You are given a one-dimensional stochastic gene-expression model with positive feedback that exhibits epigenetic memory as rare stochastic switches from a low-expression basin to a high-expression basin. The state is the copy number $x \\in \\{0,1,2,\\ldots\\}$. Two reactions occur: synthesis increases $x$ by $+1$ with propensity $a_{1}(x)$ and degradation decreases $x$ by $-1$ with propensity $a_{2}(x)$. The propensities are defined by $a_{1}(x) = k_{\\mathrm{basal}} + k_{\\mathrm{act}} \\dfrac{x^{n}}{K^{n} + x^{n}}$ and $a_{2}(x) = k_{\\mathrm{deg}} \\, x$, where $k_{\\mathrm{basal}}$, $k_{\\mathrm{act}}$, $K$, $n$, and $k_{\\mathrm{deg}}$ are positive parameters. The stochastic simulation algorithm (also known as Gillespie’s algorithm) defines an exact continuous-time Markov jump process with these propensities. You will estimate an unbiased rare switching rate using importance sampling with a biasing potential and correct reweighting.\n\nFundamental base you may assume: for a continuous-time Markov jump process with propensities $\\{a_{j}(x)\\}_{j=1}^{J}$, the probability density for a path over a time horizon is determined by the waiting-time law dictated by the total rate $a_{0}(x) = \\sum_{j=1}^{J} a_{j}(x)$ and the discrete reaction-channel selection probabilities proportional to $a_{j}(x)$. You may also assume the standard product-integral structure of path probabilities for such processes.\n\nDefine a rare-event target as hitting or exceeding a threshold $H$ before a fixed horizon $T$, starting from an initial copy number $x(0) = x_{0}$ in the low-expression basin. Let $\\tau$ be the first hitting time of the set $\\{x \\ge H\\}$. The rare-event probability is $P(T) = \\mathbb{P}(\\tau \\le T)$ under the unbiased dynamics $\\{a_{j}\\}$. The rare switching rate in the small-probability regime is defined as $r \\equiv P(T)/T$ in units of $1/\\text{time}$.\n\nYou will accelerate the event using importance sampling by simulating from a biased process with propensities $\\{\\tilde{a}_{j}\\}$ defined by an exponential tilting of the progress coordinate $q(x) = x$. Specifically, for each reaction $j$, let the stoichiometric increment be $\\nu_{1}=+1$ for synthesis and $\\nu_{2}=-1$ for degradation. For a given scalar bias strength $\\theta \\in \\mathbb{R}$, define the biased propensities by $\\tilde{a}_{j}(x) = a_{j}(x) \\exp(\\theta \\, \\nu_{j})$. The biased process favors upward moves for $\\theta  0$ and disfavors them for $\\theta  0$.\n\nTasks:\n- From first principles, derive the exact trajectory reweighting factor that expresses the Radon–Nikodym derivative of the unbiased path law with respect to the biased path law for this setting. The derivation must start from the standard product-integral form of path probabilities for continuous-time Markov jump processes and the definition of the exponential tilting above. Your final expression must be valid for a general trajectory that may be truncated at the stopping time $\\min\\{\\tau, T\\}$.\n- Design an algorithm that uses Gillespie’s algorithm to simulate trajectories under the biased propensities $\\{\\tilde{a}_{j}\\}$, accumulates the derived trajectory reweighting factor along each simulated path up to $\\min\\{\\tau, T\\}$, and forms an unbiased Monte Carlo estimator for $P(T) = \\mathbb{E}_{\\text{biased}}[W \\, \\mathbf{1}_{\\{\\tau \\le T\\}}]$, where $W$ is the derived reweighting factor.\n- Using the small-probability approximation $r \\approx P(T)/T$, report the estimated rate $r$ in $1/\\text{time}$ for each test case below.\n\nModel parameters to use in all test cases:\n- $k_{\\mathrm{basal}} = 0.10$, $k_{\\mathrm{act}} = 1.50$, $K = 25.0$, $n = 2$, $k_{\\mathrm{deg}} = 0.10$, $x_{0} = 3$.\nSimulation guidelines:\n- Use exact stochastic simulation under the biased propensities $\\{\\tilde{a}_{j}\\}$ with the specified $\\theta$ in each case.\n- For numerical stability, you may accumulate the logarithm of the weight and exponentiate at the end of a trajectory.\n- For each test case, use exactly $N = 3000$ independent trajectories to estimate $P(T)$.\n\nTest suite:\n- Case A (unbiased baseline): $\\theta = 0.0$, $T = 40.0$, $H = 35$.\n- Case B (moderate positive bias): $\\theta = 0.8$, $T = 40.0$, $H = 35$.\n- Case C (stronger positive bias): $\\theta = 1.2$, $T = 40.0$, $H = 35$.\n- Case D (different horizon and threshold): $\\theta = 0.5$, $T = 30.0$, $H = 38$.\n\nRequired final program output:\n- Your program should produce a single line of output containing the estimated rare switching rates for the four cases, in $1/\\text{time}$, as a comma-separated list enclosed in square brackets (for example, $[0.00123,0.00098,0.00105,0.00076]$). The order must be Case A, then Case B, then Case C, then Case D. Use the exact values computed by your simulation without additional text or units.",
            "solution": "### Derivation of the Trajectory Reweighting Factor\n\nThe primary task is to derive the reweighting factor, which is the Radon–Nikodym derivative of the unbiased path measure $P$ with respect to the biased path measure $\\tilde{P}$. We begin with the probability density for a single trajectory of a continuous-time Markov jump process.\n\nA trajectory over a time interval $[0, \\tau_{\\text{stop}}]$ is specified by the sequence of states visited and the times of the jumps, denoted as $\\mathbf{x} = \\{(x_0, t_0=0), (x_1, t_1), \\dots, (x_K, t_K)\\}$, where the system jumps from state $x_{i-1}$ to $x_i$ at time $t_i$ via reaction $j_i$, and $t_K  \\tau_{\\text{stop}}$. The process remains in state $x_K$ for the final time interval $[t_K, \\tau_{\\text{stop}})$. The stopping time $\\tau_{\\text{stop}}$ is defined as $\\min\\{\\tau, T\\}$, where $\\tau$ is the first hitting time of the set $\\{x \\ge H\\}$ and $T$ is the fixed time horizon.\n\nThe probability density of this trajectory under the original dynamics, characterized by propensities $\\{a_j(x)\\}$, is given by the product-integral form:\n$$\nP(\\mathbf{x}) = \\left[ \\prod_{i=1}^{K} a_{j_i}(x_{i-1}) \\right] \\exp\\left( -\\int_0^{\\tau_{\\text{stop}}} a_0(x(s)) ds \\right)\n$$\nwhere $a_0(x(s)) = \\sum_j a_j(x(s))$ is the total propensity or escape rate from state $x(s)$. Since the state is piecewise constant, i.e., $x(s) = x_{i-1}$ for $s \\in [t_{i-1}, t_i)$, a more explicit form is:\n$$\nP(\\mathbf{x}) = \\left[ \\prod_{i=1}^{K} a_{j_i}(x_{i-1}) \\right] \\exp\\left( -\\sum_{i=0}^{K-1} a_0(x_i)(t_{i+1}-t_i) - a_0(x_K)(\\tau_{\\text{stop}}-t_K) \\right)\n$$\n\nSimilarly, for the biased process with propensities $\\{\\tilde{a}_j(x)\\}$, the path probability density is:\n$$\n\\tilde{P}(\\mathbf{x}) = \\left[ \\prod_{i=1}^{K} \\tilde{a}_{j_i}(x_{i-1}) \\right] \\exp\\left( -\\int_0^{\\tau_{\\text{stop}}} \\tilde{a}_0(x(s)) ds \\right)\n$$\nwhere $\\tilde{a}_0(x(s)) = \\sum_j \\tilde{a}_j(x(s))$.\n\nThe reweighting factor $W$ is the Radon–Nikodym derivative $W = \\frac{dP}{d\\tilde{P}}$, which for a given path $\\mathbf{x}$ is the ratio $W(\\mathbf{x}) = \\frac{P(\\mathbf{x})}{\\tilde{P}(\\mathbf{x})}$.\n$$\nW = \\frac{\\left[ \\prod_{i=1}^{K} a_{j_i}(x_{i-1}) \\right] \\exp\\left( -\\int_0^{\\tau_{\\text{stop}}} a_0(x(s)) ds \\right)}{\\left[ \\prod_{i=1}^{K} \\tilde{a}_{j_i}(x_{i-1}) \\right] \\exp\\left( -\\int_0^{\\tau_{\\text{stop}}} \\tilde{a}_0(x(s)) ds \\right)}\n$$\nThis expression can be separated into a product term and an exponential term:\n$$\nW = \\left[ \\prod_{i=1}^{K} \\frac{a_{j_i}(x_{i-1})}{\\tilde{a}_{j_i}(x_{i-1})} \\right] \\exp\\left( \\int_0^{\\tau_{\\text{stop}}} (\\tilde{a}_0(x(s)) - a_0(x(s))) ds \\right)\n$$\n\nThe problem specifies an exponential tilting of the progress coordinate $q(x) = x$. The biased propensities are $\\tilde{a}_{j}(x) = a_{j}(x) \\exp(\\theta \\nu_{j})$, where $\\theta$ is the bias strength and $\\nu_j$ is the stoichiometric change in $x$ for reaction $j$. In our model, we have synthesis (reaction $j=1$, $\\nu_1 = +1$) and degradation (reaction $j=2$, $\\nu_2 = -1$).\nThe ratio in the product term becomes:\n$$\n\\frac{a_{j_i}(x_{i-1})}{\\tilde{a}_{j_i}(x_{i-1})} = \\frac{a_{j_i}(x_{i-1})}{a_{j_i}(x_{i-1}) \\exp(\\theta \\nu_{j_i})} = \\exp(-\\theta \\nu_{j_i})\n$$\nThe full product becomes a sum in the exponent:\n$$\n\\prod_{i=1}^{K} \\exp(-\\theta \\nu_{j_i}) = \\exp\\left(-\\theta \\sum_{i=1}^{K} \\nu_{j_i}\\right)\n$$\nThe sum of stoichiometric changes $\\sum_{i=1}^{K} \\nu_{j_i}$ is simply the net change in the state variable from the start of the trajectory to the state after the last jump, $x_K - x_0$. The state at the stopping time, $x(\\tau_{\\text{stop}})$, is $x_K$. Therefore, this term is $\\exp(-\\theta(x(\\tau_{\\text{stop}}) - x_0))$.\n\nThe final expression for the reweighting factor $W$ is:\n$$\nW = \\exp\\left( -\\theta(x(\\tau_{\\text{stop}}) - x_0) + \\int_0^{\\tau_{\\text{stop}}} (\\tilde{a}_0(x(s)) - a_0(x(s))) ds \\right)\n$$\nwhere $x_0 = x(0)$ is the initial state, $x(\\tau_{\\text{stop}})$ is the state at the stopping time $\\tau_{\\text{stop}}$, and the difference in total rates is $\\tilde{a}_0(x) - a_0(x) = (a_1(x)e^{\\theta} + a_2(x)e^{-\\theta}) - (a_1(x) + a_2(x))$. For numerical stability, we compute the logarithm of the weight, $\\ln W$, by accumulating the integral part during the simulation and adding the state-dependent part at the end.\n\n### Algorithm Design for Monte Carlo Estimation\n\nThe rare-event probability $P(T) = \\mathbb{P}(\\tau \\le T)$ can be expressed as the expectation of an indicator function, $P(T) = \\mathbb{E}[ \\mathbf{1}_{\\{\\tau \\le T\\}} ]$, under the unbiased dynamics. Using importance sampling, we simulate under the biased dynamics $\\tilde{P}$ and reweight, yielding the unbiased estimator:\n$$\n\\hat{P}(T) = \\frac{1}{N} \\sum_{i=1}^{N} W_i \\mathbf{1}_{\\{\\tau_i \\le T\\}}\n$$\nwhere $N$ is the number of simulated trajectories, and for each trajectory $i$, $W_i$ is the calculated weight and $\\tau_i$ is the first hitting time. The rare switching rate is then estimated as $r \\approx \\hat{P}(T)/T$.\n\nThe algorithm proceeds as follows:\n\n1.  **Initialization**: For each of $N$ trajectories, initialize time $t=0$, state $x=x_0$, and the integral part of the log-weight, $\\ln W_{\\text{int}} = 0$.\n\n2.  **Biased Stochastic Simulation**: For each trajectory, run Gillespie's algorithm using the biased propensities $\\{\\tilde{a}_j\\}$ until the stopping condition ($t \\ge T$ or $x \\ge H$) is met.\n    - At the current state $x$ and time $t$:\n        a. Calculate the unbiased propensities $a_1(x)$, $a_2(x)$ and the total $a_0(x)$.\n        b. Calculate the biased propensities $\\tilde{a}_1(x) = a_1(x)e^{\\theta}$, $\\tilde{a}_2(x) = a_2(x)e^{-\\theta}$, and the total $\\tilde{a}_0(x)$. Note that $a_2(0)=0$ implies $\\tilde{a}_2(0)=0$, so $x$ cannot become negative.\n        c. Draw a waiting time $\\Delta t$ from an exponential distribution with rate $\\tilde{a}_0(x)$: $\\Delta t = -\\ln(u_1)/\\tilde{a}_0(x)$, where $u_1 \\sim U(0,1)$.\n        d. Determine the time step for the current state, $\\Delta t_{\\text{step}} = \\min(\\Delta t, T - t)$. Update the integral of the log-weight: $\\ln W_{\\text{int}} \\leftarrow \\ln W_{\\text{int}} + (\\tilde{a}_0(x) - a_0(x))\\Delta t_{\\text{step}}$.\n        e. If $t + \\Delta t \\ge T$, the trajectory ends at the horizon $T$. Set $t=T$ and terminate the simulation loop for this trajectory.\n        f. Otherwise, advance time $t \\leftarrow t + \\Delta t$. Select a reaction $j$ with probability $\\tilde{a}_j(x)/\\tilde{a}_0(x)$ and update the state $x$ accordingly ($x \\leftarrow x + \\nu_j$).\n        g. Repeat until $t \\ge T$ or $x \\ge H$.\n\n3.  **Weight Calculation and Averaging**:\n    - After each trajectory terminates at time $\\tau_{\\text{stop}}$ in state $x_{\\text{final}}$:\n        a. Calculate the final log-weight: $\\ln W = \\ln W_{\\text{int}} - \\theta(x_{\\text{final}} - x_0)$.\n        b. Calculate the weight: $W = \\exp(\\ln W)$.\n        c. Determine the indicator: $\\mathbf{1}_{\\{\\tau \\le T\\}} = 1$ if $x_{\\text{final}} \\ge H$, and $0$ otherwise.\n        d. Accumulate the product $W \\cdot \\mathbf{1}_{\\{\\tau \\le T\\}}$ to a running sum.\n\n4.  **Final Estimation**:\n    - After all $N$ trajectories are completed, calculate the estimated probability $\\hat{P}(T)$ by dividing the total sum by $N$.\n    - Compute the estimated rate $r = \\hat{P}(T)/T$. This procedure is repeated for each test case.",
            "answer": "[0.0,0.00010773663806282865,0.00011040375990236113,1.5790432247447996e-05]"
        }
    ]
}