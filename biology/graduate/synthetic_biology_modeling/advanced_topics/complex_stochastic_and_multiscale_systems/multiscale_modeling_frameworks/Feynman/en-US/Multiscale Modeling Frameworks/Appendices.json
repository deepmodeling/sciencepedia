{
    "hands_on_practices": [
        {
            "introduction": "Many biological systems, from gene circuits to metabolic networks, are inherently multiscale and are described by mathematically \"stiff\" models containing processes that evolve on vastly different timescales. This exercise tackles the core computational challenge of stiffness by having you implement a multirate integrator, a specialized numerical method that exploits timescale separation to gain significant computational speed. By comparing your multirate solver to a standard integrator for a model of a synthetic gene circuit, you will gain hands-on experience in quantifying the critical trade-off between computational efficiency and numerical accuracy .",
            "id": "3922659",
            "problem": "Consider a synthetic gene circuit modeled by a two-scale Ordinary Differential Equation (ODE) system, where a promoter binding process is fast and protein production and degradation are slow. The fast variable is the bound fraction of a promoter, denoted by $p(t) \\in [0,1]$, and the slow variable is the protein concentration, denoted by $x(t) \\ge 0$. Assume mass-action binding of the protein to the promoter with on-rate $k_{\\text{on}}$ and off-rate $k_{\\text{off}}$, and protein production proportional to promoter occupancy with production rate constant $\\alpha$ and first-order degradation with rate constant $\\beta$. All variables and parameters are non-negative and dimensionless. The model is:\n$$\n\\frac{dp}{dt} = k_{\\text{on}} \\, x(t) \\, \\left(1 - p(t)\\right) - k_{\\text{off}} \\, p(t),\n$$\n$$\n\\frac{dx}{dt} = \\alpha \\, p(t) - \\beta \\, x(t).\n$$\nThe task is to implement and compare a multirate integration framework against a baseline uniform-step integrator and quantify the computational speedup versus numerical error.\n\nFrom a fundamental base, use the following widely accepted principles:\n- The mass-action law of chemical kinetics states that for reversible binding, the net rate is the difference between forward and reverse rates, yielding the first equation.\n- Production and degradation modeled as zero-order production and first-order decay yield the second equation.\n- Timescale separation arises because $k_{\\text{on}}$ and $k_{\\text{off}}$ can be much larger than $\\alpha$ and $\\beta$, leading to fast promoter binding dynamics relative to slow protein dynamics.\n\nImplement the following algorithms:\n1. A high-accuracy reference solution using a stiff ODE solver to approximate the true dynamics over a finite time horizon $[0, T]$. Use an implicit method with tight tolerances to approximate the ground truth. The reference will be used solely for error quantification.\n2. A baseline explicit Euler integrator with a uniform step size $h_{\\text{base}}$ chosen by a stability-informed bound derived from the fast binding rate. Specifically, let $X_{\\text{bound}} = \\max(x(0), \\alpha / \\beta)$ and define $a_{\\max} = k_{\\text{on}} X_{\\text{bound}} + k_{\\text{off}}$. Use a fixed step $h_{\\text{base}} = \\min(H/m, c / a_{\\max})$, where $H$ is the macro step used by the multirate method, $m$ is the number of micro-steps per macro step, and $c$ is a constant bound factor set to $c = 0.4$. This baseline method updates both $p$ and $x$ at each step using explicit Euler and clamps $p$ to $[0,1]$.\n3. A multirate splitting integrator over macro steps of size $H$. For each macro step, hold $x$ constant and integrate $p$ with $m$ explicit Euler micro-steps of size $h_{\\text{mr}} = H/m$, then update $x$ with a single explicit Euler step using the final value of $p$. Clamp $p$ to $[0,1]$ after updates.\n\nDefine computational cost as the number of right-hand-side evaluations of scalar equations. Count the baseline cost as two evaluations per explicit Euler step (one for $\\frac{dp}{dt}$ and one for $\\frac{dx}{dt}$), and count the multirate cost as $m$ evaluations of $\\frac{dp}{dt}$ plus one evaluation of $\\frac{dx}{dt}$ per macro step. Define computational speedup as the ratio of baseline cost to multirate cost.\n\nDefine numerical error as the infinity norm between the multirate final state and the reference final state at time $T$, that is $E = \\lVert y_{\\text{mr}}(T) - y_{\\text{ref}}(T) \\rVert_{\\infty}$ with $y = [p, x]^{\\top}$.\n\nAll quantities are unitless. Angles are not used. Percentages are not used.\n\nYour program must implement the above, run the specified test suite, and produce a single line of output containing a list of per-test-case results, where each result is the list $[S, E]$ with $S$ the speedup (float) and $E$ the error (float). The output format must be a single line with a comma-separated list enclosed in square brackets, for example $[ [S_1,E_1], [S_2,E_2] ]$ with actual numeric values.\n\nUse the following test suite with scientifically plausible and self-consistent parameters. All parameters and initial conditions are dimensionless. For each case, the tuple is $(k_{\\text{on}}, k_{\\text{off}}, \\alpha, \\beta, H, m, T, p(0), x(0))$:\n- Case $1$ (strong timescale separation, happy path): $(1000, 500, 1, 0.1, 0.01, 50, 1, 0.5, 0)$.\n- Case $2$ (moderate separation): $(200, 100, 1, 0.2, 0.01, 20, 1, 0.2, 0.1)$.\n- Case $3$ (high initial protein, fast binding and unbinding): $(1000, 1000, 0.5, 0.05, 0.005, 40, 0.5, 0, 10)$.\n- Case $4$ (edge case with no binding, fast unbinding): $(0, 1000, 1, 0.1, 0.01, 50, 1, 1, 1)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the list $[S, E]$ for the corresponding test case, in the order of the test suite as provided, for example $[[S_1,E_1],[S_2,E_2],[S_3,E_3],[S_4,E_4]]$.",
            "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- **Fast Variable**: Bound promoter fraction, $p(t) \\in [0,1]$.\n- **Slow Variable**: Protein concentration, $x(t) \\ge 0$.\n- **Model Parameters**: $k_{\\text{on}}$ (on-rate), $k_{\\text{off}}$ (off-rate), $\\alpha$ (production rate constant), $\\beta$ (degradation rate constant). All parameters are non-negative and dimensionless.\n- **Governing ODEs**:\n$$\n\\frac{dp}{dt} = k_{\\text{on}} \\, x(t) \\, \\left(1 - p(t)\\right) - k_{\\text{off}} \\, p(t)\n$$\n$$\n\\frac{dx}{dt} = \\alpha \\, p(t) - \\beta \\, x(t)\n$$\n- **Task**: Implement and compare a multirate integrator against a baseline uniform-step integrator, quantifying computational speedup versus numerical error.\n- **Reference Solution**: High-accuracy solution from a stiff ODE solver (e.g., implicit method with tight tolerances) over time horizon $[0, T]$.\n- **Baseline Integrator (Explicit Euler)**:\n    - Uniform step size $h_{\\text{base}}$.\n    - $X_{\\text{bound}} = \\max(x(0), \\alpha / \\beta)$.\n    - $a_{\\max} = k_{\\text{on}} X_{\\text{bound}} + k_{\\text{off}}$.\n    - $h_{\\text{base}} = \\min(H/m, c / a_{\\max})$ with $c = 0.4$.\n    - Updates both $p$ and $x$ at each step.\n    - Clamps $p$ to $[0,1]$ after each update.\n- **Multirate Splitting Integrator**:\n    - Macro step size $H$.\n    - For each macro step: hold $x$ constant, integrate $p$ using $m$ explicit Euler micro-steps of size $h_{\\text{mr}} = H/m$. Then, update $x$ with a single explicit Euler step of size $H$ using the final $p$.\n    - Clamps $p$ to $[0,1]$ after each micro-step update.\n- **Computational Cost Definition**:\n    - **Baseline**: $2$ right-hand-side (RHS) evaluations per step.\n    - **Multirate**: $m$ evaluations of $\\frac{dp}{dt}$ plus $1$ evaluation of $\\frac{dx}{dt}$ per macro step.\n- **Speedup Definition**: $S = \\text{Cost}_{\\text{baseline}} / \\text{Cost}_{\\text{multirate}}$.\n- **Numerical Error Definition**: $E = \\lVert y_{\\text{mr}}(T) - y_{\\text{ref}}(T) \\rVert_{\\infty}$, where $y = [p, x]^{\\top}$.\n- **Test Suite**: A set of $4$ tuples $(k_{\\text{on}}, k_{\\text{off}}, \\alpha, \\beta, H, m, T, p(0), x(0))$.\n    1. $(1000, 500, 1, 0.1, 0.01, 50, 1, 0.5, 0)$.\n    2. $(200, 100, 1, 0.2, 0.01, 20, 1, 0.2, 0.1)$.\n    3. $(1000, 1000, 0.5, 0.05, 0.005, 40, 0.5, 0, 10)$.\n    4. $(0, 1000, 1, 0.1, 0.01, 50, 1, 1, 1)$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The model is based on the law of mass-action for reversible binding and standard zero-order/first-order kinetics for protein synthesis and degradation. This is a canonical model in synthetic and systems biology. The concept of timescale separation, where binding/unbinding rates ($k_{\\text{on}}, k_{\\text{off}}$) are much larger than production/degradation rates ($\\alpha, \\beta$), is a well-established principle for modeling such systems, leading to stiff ODEs. The problem is scientifically sound.\n2.  **Well-Posed**: The problem is clearly structured. The ODE system is defined, initial conditions are provided for each case, and the algorithms for the numerical integrators are specified. The metrics for comparison (cost, speedup, error) are unambiguously defined. A unique, stable solution to the ODEs exists for the given initial conditions. The problem is well-posed.\n3.  **Objective**: The problem is stated in precise, formal language. All terms are defined mathematically, and the test cases consist of numerical data. There is no subjective or opinion-based content.\n4.  **Incomplete or Contradictory Setup**: The problem provides all necessary data and definitions. The system of equations, all parameters, initial conditions, integrator rules, and evaluation metrics are supplied. There are no contradictions.\n5.  **Unrealistic or Infeasible**: The parameters are stated to be \"scientifically plausible\" and reflect the conditions of timescale separation they are intended to model. All values are physically consistent (non-negative rates and concentrations). The task is computationally feasible.\n6.  **Ill-posed or Poorly Structured**: The definitions of the numerical methods, including the stability-informed step size for the baseline and the splitting scheme for the multirate method, are explicit and can be implemented without ambiguity.\n7.  **Pseudo-Profound, Trivial, or Tautological**: The problem addresses a genuine challenge in computational science: the efficient simulation of multiscale (stiff) dynamical systems. It requires the implementation and comparison of non-trivial numerical methods, which is a standard task in this field.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a well-defined, scientifically grounded exercise in numerical methods for multiscale systems in biology. A solution will be provided.\n\n### Solution Design\nThe problem requires a comparison between a standard explicit numerical integrator and a multirate integrator tailored for stiff systems with timescale separation.\n\n**1. Reference Solution**\nThe provided ODE system is stiff, especially when $k_{\\text{on}}$ and $k_{\\text{off}}$ are large compared to $\\alpha$ and $\\beta$. This means the dynamics of $p(t)$ are much faster than those of $x(t)$. A standard explicit integrator would require a very small time step, dictated by the fast dynamics, to remain stable, even when the solution is evolving slowly. To obtain a \"ground truth\" or reference solution, a specialized implicit solver designed for stiff systems is necessary. We will use the `Radau` method, an implicit Runge-Kutta method suitable for stiff ODEs, as implemented in `scipy.integrate.solve_ivp`. By setting very tight error tolerances (e.g., $10^{-12}$), we can obtain a highly accurate solution $y_{\\text{ref}}(T) = [p_{\\text{ref}}(T), x_{\\text{ref}}(T)]^{\\top}$ to serve as the benchmark for accuracy.\n\n**2. Baseline Uniform-Step Integrator and Cost**\nThe baseline method is a simple explicit Euler integrator applied to the full system. Its primary limitation is stability. The stability of the explicit Euler method for the equation $\\frac{dy}{dt} = \\lambda y$ requires $|1 + h\\lambda| \\le 1$. For our system, the fast dynamics of $p$ are governed by the equation $\\frac{dp}{dt} = k_{\\text{on}} x (1-p) - k_{\\text{off}} p$. If we treat $x$ as constant over a small time step, the Jacobian of the right-hand side with respect to $p$ is $-(k_{\\text{on}}x + k_{\\text{off}})$. The eigenvalue is $\\lambda_p = -(k_{\\text{on}}x(t) + k_{\\text{off}})$. The stability condition becomes $h|\\lambda_p| \\le 2$, or $h \\le \\frac{2}{k_{\\text{on}}x(t) + k_{\\text{off}}}$. The problem provides a robust way to determine a single, fixed step size $h_{\\text{base}}$ for the entire simulation by choosing a conservative upper bound for $x(t)$, namely $X_{\\text{bound}} = \\max(x(0), \\alpha/\\beta)$, where $\\alpha/\\beta$ is the steady-state value of $x$ if $p$ were constant at $1$. This leads to a maximum eigenvalue magnitude of $a_{\\max} = k_{\\text{on}}X_{\\text{bound}} + k_{\\text{off}}$. The step size is then chosen as $h_{\\text{base}} = \\min(H/m, c/a_{\\max})$, with a safety factor $c=0.4  2$. The inclusion of $H/m$ ensures the baseline method's resolution is at least as fine as the multirate method's micro-steps.\nThe computational cost is the number of RHS evaluations. For the baseline, each step involves evaluating $\\frac{dp}{dt}$ and $\\frac{dx}{dt}$, costing $2$ evaluations. The total cost is $2 \\times N_{\\text{steps}}$, where $N_{\\text{steps}} = \\lceil T/h_{\\text{base}} \\rceil$.\n\n**3. Multirate Splitting Integrator and Cost**\nThe multirate integrator exploits the timescale separation. It uses a large \"macro\" step $H$ for the slow variable $x$ and multiple small \"micro\" steps $h_{\\text{mr}} = H/m$ for the fast variable $p$. This approach is a form of operator splitting.\nFor each macro step from time $t_n$ to $t_{n+1} = t_n + H$:\n- **Fast Integration**: The slow variable $x$ is frozen at its value $x(t_n)$. The equation for $p$ is then integrated for $m$ steps of size $h_{\\text{mr}}$.\n$$\np_{j+1} = p_j + h_{\\text{mr}} \\left( k_{\\text{on}} x(t_n) (1-p_j) - k_{\\text{off}} p_j \\right), \\quad \\text{for } j=0, \\dots, m-1\n$$\nThe value of $p$ is clamped to $[0,1]$ after each micro-step to maintain physical bounds. The cost for this stage is $m$ evaluations of the $\\frac{dp}{dt}$ RHS.\n- **Slow Integration**: The final value of $p$ from the fast integration, $p_m$, is used to update the slow variable $x$ over the entire macro step $H$.\n$$ x(t_{n+1}) = x(t_n) + H \\left( \\alpha p_m - \\beta x(t_n) \\right) $$\nThe cost for this stage is $1$ evaluation of the $\\frac{dx}{dt}$ RHS.\nThe total cost per macro step is $(m+1)$ evaluations. The total multirate cost is $(m+1) \\times (T/H)$.\n\n**4. Performance Metrics**\nThe comparison is based on two metrics:\n- **Computational Speedup ($S$)**: The ratio of the baseline cost to the multirate cost, $S = \\text{Cost}_{\\text{baseline}} / \\text{Cost}_{\\text{multirate}}$. A value $S > 1$ indicates that the multirate method is computationally cheaper.\n- **Numerical Error ($E$)**: The accuracy of the multirate method is measured against the high-fidelity reference solution. The error is the maximum absolute difference between the final states, $E = \\max(|p_{\\text{mr}}(T) - p_{\\text{ref}}(T)|, |x_{\\text{mr}}(T) - x_{\\text{ref}}(T)|)$.\n\nThis framework allows for a quantitative assessment of the trade-off between computational efficiency and accuracy, which is central to the development of numerical methods.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    # Test suite format: (k_on, k_off, alpha, beta, H, m, T, p0, x0)\n    test_cases = [\n        (1000, 500, 1, 0.1, 0.01, 50, 1, 0.5, 0),\n        (200, 100, 1, 0.2, 0.01, 20, 1, 0.2, 0.1),\n        (1000, 1000, 0.5, 0.05, 0.005, 40, 0.5, 0, 10),\n        (0, 1000, 1, 0.1, 0.01, 50, 1, 1, 1),\n    ]\n\n    results = []\n    for case in test_cases:\n        k_on, k_off, alpha, beta, H, m, T, p0, x0 = case\n        params = (k_on, k_off, alpha, beta)\n        y0 = np.array([p0, x0], dtype=float)\n\n        # 1. High-accuracy reference solution\n        y_ref = get_reference_solution(y0, T, params)\n\n        # 2. Multirate solver solution and cost\n        y_mr, cost_mr = solve_multirate(y0, T, H, m, params)\n\n        # 3. Baseline integrator cost calculation\n        cost_base = calculate_baseline_cost(x0, T, H, m, params)\n\n        # 4. Calculate speedup and error\n        speedup = cost_base / cost_mr\n        error = np.linalg.norm(y_mr - y_ref, ord=np.inf)\n\n        results.append([speedup, error])\n\n    # Format the final output string as specified\n    formatted_results = []\n    for res in results:\n        s, e = res\n        formatted_results.append(f\"[{s},{e}]\")\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef ode_system(t, y, k_on, k_off, alpha, beta):\n    \"\"\"\n    Defines the system of ODEs.\n    y[0] = p (promoter bound fraction)\n    y[1] = x (protein concentration)\n    \"\"\"\n    p, x = y\n    dpdt = k_on * x * (1.0 - p) - k_off * p\n    dxdt = alpha * p - beta * x\n    return np.array([dpdt, dxdt])\n\ndef get_reference_solution(y0, T, params):\n    \"\"\"\n    Computes a high-accuracy reference solution using a stiff solver.\n    \"\"\"\n    sol = solve_ivp(\n        fun=ode_system,\n        t_span=[0, T],\n        y0=y0,\n        args=params,\n        method='Radau',\n        atol=1e-12,\n        rtol=1e-12\n    )\n    return sol.y[:, -1]\n\ndef solve_multirate(y0, T, H, m, params):\n    \"\"\"\n    Implements the multirate splitting integrator (explicit Euler).\n    \"\"\"\n    k_on, k_off, alpha, beta = params\n    y = y0.copy()\n    cost = 0\n    h_mr = H / m\n    \n    # Given that T is a multiple of H in all test cases\n    num_macro_steps = int(round(T / H))\n    \n    for _ in range(num_macro_steps):\n        p_start_macro, x_macro = y\n        \n        # Fast integration of p, holding x constant\n        p_fast = p_start_macro\n        for _ in range(m):\n            dpdt_fast = k_on * x_macro * (1.0 - p_fast) - k_off * p_fast\n            p_fast += h_mr * dpdt_fast\n            p_fast = np.clip(p_fast, 0.0, 1.0)\n            cost += 1\n        \n        # Slow update of x, using final p from fast integration\n        dxdt_slow = alpha * p_fast - beta * x_macro\n        x_new = x_macro + H * dxdt_slow\n        cost += 1\n        \n        y = np.array([p_fast, x_new])\n        \n    return y, cost\n\ndef calculate_baseline_cost(x0, T, H, m, params):\n    \"\"\"\n    Calculates the computational cost of the baseline explicit Euler method.\n    \"\"\"\n    k_on, k_off, alpha, beta = params\n    c = 0.4\n\n    # The problem implies beta > 0 for this term to be meaningful.\n    # We handle beta=0 for robustness, though not in test cases.\n    if beta > 0:\n        x_eq = alpha / beta\n    else:\n        x_eq = float('inf')\n        \n    X_bound = max(x0, x_eq)\n    a_max = k_on * X_bound + k_off\n\n    # Calculate stability-informed step size\n    if a_max > 0:\n        h_stability = c / a_max\n    else:\n        # If a_max is 0, stability constraint is not limiting\n        h_stability = float('inf')\n    \n    h_mr = H / m\n    h_base = min(h_mr, h_stability)\n    \n    if h_base = 0:\n        # This case implies infinite cost, e.g., if a_max is infinite.\n        return float('inf')\n\n    num_steps = int(np.ceil(T / h_base))\n    cost = 2 * num_steps\n    return float(cost)\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "A crucial skill in modeling complex biological systems is the ability to perform scale analysis to identify the rate-limiting step, or bottleneck, in a process. This analysis often relies on forming dimensionless numbers that compare the relative importance of competing phenomena, such as reaction, diffusion, and convection. This practice challenges you to apply these principles to a bioengineering scenario, using the Biot number to diagnose the dominant transport regime in a microfluidic device and to reason about how design modifications can shift the system's behavior .",
            "id": "3922647",
            "problem": "A single cell is integrated behind a semipermeable membrane in a microfluidic cell-on-a-chip device. Analyte transport spans three coupled scales: convective-diffusive transport in the channel adjacent to the membrane, passive permeation across the membrane, and diffusion within the cytoplasm. Assume the analyte does not react inside the cell on the timescale considered, and the cytoplasm can be modeled as a continuum with an effective diffusion coefficient. The membrane is planar, and the cell is approximated as a sphere of radius $L$ contacting the membrane over a small patch; take $L$ as the characteristic length scale for intracellular diffusion.\n\nUse the following well-tested relationships as the starting point for your reasoning: Fick’s law of diffusion $J = -D \\,\\partial C/\\partial x$, a convective mass-transfer boundary condition adjacent to the membrane $J = k_{\\mathrm{ch}}\\,(C_{0} - C_{\\mathrm{out}})$, and a permeation boundary condition across the membrane $J = P\\,(C_{\\mathrm{out}} - C_{\\mathrm{in,surf}})$, where $J$ is flux, $D$ is diffusion coefficient, $k_{\\mathrm{ch}}$ is the external mass-transfer coefficient in the channel, $P$ is membrane permeability, $C_{0}$ is the bulk channel concentration, $C_{\\mathrm{out}}$ is the concentration at the channel-side membrane surface, and $C_{\\mathrm{in,surf}}$ is the concentration at the cytoplasm-side membrane surface. The membrane permeability is given by $P = K D_{m}/\\delta_{m}$, where $K$ is the membrane–analyte partition coefficient, $D_{m}$ is the diffusion coefficient in the membrane, and $\\delta_{m}$ is the membrane thickness.\n\nParameters for the baseline design are:\n- Cell radius $L = 1.0\\times 10^{-5}\\,\\mathrm{m}$.\n- Cytoplasmic diffusion coefficient $D_{i} = 3.0\\times 10^{-12}\\,\\mathrm{m^{2}\\,s^{-1}}$.\n- Channel-side mass-transfer coefficient $k_{\\mathrm{ch}} = 5.0\\times 10^{-5}\\,\\mathrm{m\\,s^{-1}}$.\n- Membrane thickness $\\delta_{m} = 2.0\\times 10^{-7}\\,\\mathrm{m}$.\n- Membrane diffusion coefficient $D_{m} = 1.0\\times 10^{-10}\\,\\mathrm{m^{2}\\,s^{-1}}$.\n- Partition coefficient $K = 0.5$.\n\nA multiscale modeling framework often diagnoses whether transport is limited by membrane/channel transfer versus intracellular diffusion by forming dimensionless groups that compare boundary transfer capacity to internal diffusion capacity. In particular, it is useful to separately compare the membrane-to-intracellular scale and the channel-to-intracellular scale, and to assess the overall regime when the channel and membrane act as series resistances.\n\nWhich option correctly interprets the baseline regime and identifies design changes that would robustly shift the system to a lumped-intracellular regime in which intracellular concentration remains nearly uniform (that is, the overall boundary-to-intracellular transfer capacity is much smaller than the intracellular diffusive capacity so that the dimensionless group comparing them satisfies $\\ll 1$)?\n\nA. In the baseline, both membrane permeation and channel transfer are fast compared to intracellular diffusion, so intracellular diffusion is rate-limiting. To reach the lumped-intracellular regime, decrease boundary transfer by thickening the membrane by a factor of $1000$ (so $P$ decreases by $1000$), reduce flow to decrease $k_{\\mathrm{ch}}$ to $1.0\\times 10^{-6}\\,\\mathrm{m\\,s^{-1}}$, and increase intracellular mobility to $D_{i} = 3.0\\times 10^{-11}\\,\\mathrm{m^{2}\\,s^{-1}}$; these changes together make the overall boundary-to-internal ratio $\\lesssim 0.1$.\n\nB. In the baseline, membrane resistance dominates because $P  k_{\\mathrm{ch}}$, so increasing membrane permeability by adding pores (increasing $P$) will reduce the boundary-to-internal ratio below $0.1$ and produce a well-mixed cytoplasm without changing $D_{i}$ or $L$.\n\nC. In the baseline, external channel convection dominates intracellular diffusion, but the membrane is slow; therefore, reducing the membrane thickness by a factor of $10$ (increasing $P$ by $10$) and increasing flow to double $k_{\\mathrm{ch}}$ will lower the overall dimensionless ratio below $0.1$, even if $D_{i}$ and $L$ are unchanged.\n\nD. In the baseline, intracellular diffusion is fast relative to boundary transfer, so the cell is already lumped; to further ensure uniformity, increase the cell radius $L$ by an order of magnitude while keeping $P$, $k_{\\mathrm{ch}}$, and $D_{i}$ fixed, which decreases the boundary-to-internal ratio by reducing gradients across the larger cell.",
            "solution": "The problem statement has been critically reviewed and is determined to be valid. It is scientifically grounded in the principles of mass transport, well-posed with sufficient data, and objectively phrased. We may proceed with the solution.\n\nThe central goal is to determine the transport regime of the system and identify changes that would shift it to a \"lumped-intracellular regime.\" This regime is characterized by a nearly uniform intracellular concentration, which occurs when the rate of mass transfer across the cell boundary is much slower than the rate of diffusion within the cell. The problem statement correctly identifies that this condition is met when a dimensionless group comparing the boundary transfer capacity to the internal diffusion capacity is much less than $1$. This dimensionless group is the Biot number for mass transfer, $\\mathrm{Bi}_{m}$.\n\nThe transport from the bulk channel to the cell interior involves two primary resistances in series: the resistance in the external channel flow and the resistance of the membrane. The overall mass transfer coefficient, which we will denote as $k_{\\mathrm{ov}}$, accounts for these series resistances. Its inverse, the total resistance, is the sum of the individual resistances:\n$$ \\frac{1}{k_{\\mathrm{ov}}} = \\frac{1}{k_{\\mathrm{ch}}} + \\frac{1}{P} $$\nwhere $k_{\\mathrm{ch}}$ is the channel-side mass-transfer coefficient and $P$ is the membrane permeability. This can be rearranged to solve for $k_{\\mathrm{ov}}$:\n$$ k_{\\mathrm{ov}} = \\frac{k_{\\mathrm{ch}} P}{k_{\\mathrm{ch}} + P} $$\nThe overall Biot number, $\\mathrm{Bi}_{\\mathrm{ov}}$, compares the overall boundary transfer rate to the intracellular diffusion rate:\n$$ \\mathrm{Bi}_{\\mathrm{ov}} = \\frac{\\text{boundary transfer rate}}{\\text{intracellular diffusion rate}} = \\frac{k_{\\mathrm{ov}} L}{D_{i}} $$\nwhere $L$ is the characteristic length for intracellular diffusion (the cell radius) and $D_{i}$ is the cytoplasmic diffusion coefficient. The lumped-intracellular regime corresponds to $\\mathrm{Bi}_{\\mathrm{ov}} \\ll 1$.\n\nFirst, we calculate the baseline parameters to characterize the initial regime.\nThe given baseline parameters are:\n- $L = 1.0\\times 10^{-5}\\,\\mathrm{m}$\n- $D_{i} = 3.0\\times 10^{-12}\\,\\mathrm{m^{2}\\,s^{-1}}$\n- $k_{\\mathrm{ch}} = 5.0\\times 10^{-5}\\,\\mathrm{m\\,s^{-1}}$\n- $\\delta_{m} = 2.0\\times 10^{-7}\\,\\mathrm{m}$\n- $D_{m} = 1.0\\times 10^{-10}\\,\\mathrm{m^{2}\\,s^{-1}}$\n- $K = 0.5$\n\nWe begin by calculating the membrane permeability, $P$:\n$$ P = \\frac{K D_{m}}{\\delta_{m}} = \\frac{(0.5) (1.0 \\times 10^{-10}\\,\\mathrm{m^{2}\\,s^{-1}})}{2.0 \\times 10^{-7}\\,\\mathrm{m}} = 2.5 \\times 10^{-4}\\,\\mathrm{m\\,s^{-1}} $$\nNow, we compare the individual transport coefficients, $k_{\\mathrm{ch}}$ and $P$:\n- $k_{\\mathrm{ch}} = 5.0 \\times 10^{-5}\\,\\mathrm{m\\,s^{-1}}$\n- $P = 2.5 \\times 10^{-4}\\,\\mathrm{m\\,s^{-1}}$\nSince $P > k_{\\mathrm{ch}}$, transport across the membrane is faster than transport through the channel boundary layer. The corresponding resistances are $1/k_{\\mathrm{ch}} = 2.0 \\times 10^{4}\\,\\mathrm{s\\,m^{-1}}$ and $1/P = 4.0 \\times 10^{3}\\,\\mathrm{s\\,m^{-1}}$. The larger resistance is from the channel, making it the dominant boundary resistance, though both are of a similar order of magnitude.\n\nNext, we calculate the overall mass transfer coefficient, $k_{\\mathrm{ov}}$:\n$$ k_{\\mathrm{ov}} = \\frac{(5.0 \\times 10^{-5}\\,\\mathrm{m\\,s^{-1}})(2.5 \\times 10^{-4}\\,\\mathrm{m\\,s^{-1}})}{5.0 \\times 10^{-5}\\,\\mathrm{m\\,s^{-1}} + 2.5 \\times 10^{-4}\\,\\mathrm{m\\,s^{-1}}} = \\frac{1.25 \\times 10^{-8}}{3.0 \\times 10^{-4}}\\,\\mathrm{m\\,s^{-1}} \\approx 4.17 \\times 10^{-5}\\,\\mathrm{m\\,s^{-1}} $$\nFinally, we calculate the overall Biot number for the baseline case:\n$$ \\mathrm{Bi}_{\\mathrm{ov}} = \\frac{k_{\\mathrm{ov}} L}{D_{i}} = \\frac{(4.17 \\times 10^{-5}\\,\\mathrm{m\\,s^{-1}})(1.0 \\times 10^{-5}\\,\\mathrm{m})}{3.0 \\times 10^{-12}\\,\\mathrm{m^{2}\\,s^{-1}}} = \\frac{4.17 \\times 10^{-10}}{3.0 \\times 10^{-12}} \\approx 139 $$\nSince $\\mathrm{Bi}_{\\mathrm{ov}} \\approx 139 \\gg 1$, the rate of intracellular diffusion is much slower than the rate of transport across the boundary. This means the system is strongly limited by intracellular diffusion, and significant concentration gradients will exist within the cell. The baseline system is not in a lumped-intracellular regime.\n\nTo achieve the lumped regime ($\\mathrm{Bi}_{\\mathrm{ov}} \\ll 1$), we must decrease the ratio $k_{\\mathrm{ov}} L / D_{i}$. This can be accomplished by:\n1.  Decreasing the overall boundary transfer coefficient, $k_{\\mathrm{ov}}$ (by decreasing $k_{\\mathrm{ch}}$ and/or $P$).\n2.  Decreasing the cell radius, $L$.\n3.  Increasing the intracellular diffusion coefficient, $D_{i}$.\n\nNow we evaluate each option.\n\n**A. In the baseline, both membrane permeation and channel transfer are fast compared to intracellular diffusion, so intracellular diffusion is rate-limiting. To reach the lumped-intracellular regime, decrease boundary transfer by thickening the membrane by a factor of $1000$ (so $P$ decreases by $1000$), reduce flow to decrease $k_{\\mathrm{ch}}$ to $1.0\\times 10^{-6}\\,\\mathrm{m\\,s^{-1}}$, and increase intracellular mobility to $D_{i} = 3.0\\times 10^{-11}\\,\\mathrm{m^{2}\\,s^{-1}}$; these changes together make the overall boundary-to-internal ratio $\\lesssim 0.1$.**\n\n- **Baseline interpretation:** The statement that \"both membrane permeation and channel transfer are fast compared to intracellular diffusion, so intracellular diffusion is rate-limiting\" is equivalent to stating that the relevant Biot numbers are greater than $1$. Our calculation of $\\mathrm{Bi}_{\\mathrm{ov}} \\approx 139$ confirms this. This part is correct.\n- **Proposed changes:**\n    - Thickening $\\delta_{m}$ by a factor of $1000$ decreases $P$ by $1000$: $P' = P/1000 = (2.5 \\times 10^{-4})/1000 = 2.5 \\times 10^{-7}\\,\\mathrm{m\\,s^{-1}}$. This decreases $k_{\\mathrm{ov}}$.\n    - Decreasing $k_{\\mathrm{ch}}$ to $k_{\\mathrm{ch}}' = 1.0\\times 10^{-6}\\,\\mathrm{m\\,s^{-1}}$. This decreases $k_{\\mathrm{ov}}$.\n    - Increasing $D_{i}$ to $D_{i}' = 3.0\\times 10^{-11}\\,\\mathrm{m^{2}\\,s^{-1}}$. This increases the denominator of the Biot number.\n- **Recalculation:** All proposed changes act to decrease $\\mathrm{Bi}_{\\mathrm{ov}}$. Let's compute the new value.\n    - New overall transfer coefficient: $k_{\\mathrm{ov}}' = \\frac{k_{\\mathrm{ch}}' P'}{k_{\\mathrm{ch}}' + P'} = \\frac{(1.0\\times 10^{-6})(2.5\\times 10^{-7})}{(1.0\\times 10^{-6}) + (2.5\\times 10^{-7})} = \\frac{2.5\\times 10^{-13}}{1.25\\times 10^{-6}} = 2.0\\times 10^{-7}\\,\\mathrm{m\\,s^{-1}}$.\n    - New Biot number: $\\mathrm{Bi}_{\\mathrm{ov}}' = \\frac{k_{\\mathrm{ov}}' L}{D_{i}'} = \\frac{(2.0\\times 10^{-7}\\,\\mathrm{m\\,s^{-1}})(1.0\\times 10^{-5}\\,\\mathrm{m})}{3.0\\times 10^{-11}\\,\\mathrm{m^{2}\\,s^{-1}}} = \\frac{2.0\\times 10^{-12}}{3.0\\times 10^{-11}} = \\frac{2}{30} \\approx 0.067$.\n- **Conclusion:** The new Biot number is $\\approx 0.067$, which is $\\ll 1$ and satisfies the condition $\\lesssim 0.1$.\n- **Verdict:** Correct.\n\n**B. In the baseline, membrane resistance dominates because $P  k_{\\mathrm{ch}}$, so increasing membrane permeability by adding pores (increasing $P$) will reduce the boundary-to-internal ratio below $0.1$ and produce a well-mixed cytoplasm without changing $D_{i}$ or $L$.**\n\n- **Baseline interpretation:** The claim is that membrane resistance dominates because $P  k_{\\mathrm{ch}}$. We calculated $P = 2.5 \\times 10^{-4}\\,\\mathrm{m\\,s^{-1}}$ and $k_{\\mathrm{ch}} = 5.0 \\times 10^{-5}\\,\\mathrm{m\\,s^{-1}}$. Thus, $P > k_{\\mathrm{ch}}$. The premise is factually incorrect. The channel resistance ($1/k_{\\mathrm{ch}}$) is larger than the membrane resistance ($1/P$).\n- **Proposed changes:** \"increasing membrane permeability... will reduce the boundary-to-internal ratio\". The ratio is $\\mathrm{Bi}_{\\mathrm{ov}} = k_{\\mathrm{ov}}L/D_i$. Increasing $P$ will increase $k_{\\mathrm{ov}}$, thereby *increasing* $\\mathrm{Bi}_{\\mathrm{ov}}$ and moving the system further away from the desired lumped regime. This is the opposite of the required action.\n- **Verdict:** Incorrect.\n\n**C. In the baseline, external channel convection dominates intracellular diffusion, but the membrane is slow; therefore, reducing the membrane thickness by a factor of $10$ (increasing $P$ by $10$) and increasing flow to double $k_{\\mathrm{ch}}$ will lower the overall dimensionless ratio below $0.1$, even if $D_{i}$ and $L$ are unchanged.**\n\n- **Baseline interpretation:** \"external channel convection dominates intracellular diffusion\" means $\\mathrm{Bi}_{\\mathrm{ch}} = k_{\\mathrm{ch}}L/D_i \\gg 1$. Our calculation for the baseline shows $\\frac{(5.0\\times10^{-5})(1.0\\times10^{-5})}{3.0\\times10^{-12}} \\approx 167$, so this part is correct. However, \"the membrane is slow\" is false. Compared to channel transport, membrane transport is faster ($P > k_{\\mathrm{ch}}$). Compared to intracellular diffusion, it is also much faster ($\\mathrm{Bi}_{\\mathrm{mem}} = PL/D_i \\approx 833 \\gg 1$). The premise is flawed.\n- **Proposed changes:** Increasing $P$ (by reducing $\\delta_m$) and increasing $k_{\\mathrm{ch}}$. Both actions increase $k_{\\mathrm{ov}}$, which will *increase* $\\mathrm{Bi}_{\\mathrm{ov}}$. This is counterproductive to achieving a lumped regime.\n- **Verdict:** Incorrect.\n\n**D. In the baseline, intracellular diffusion is fast relative to boundary transfer, so the cell is already lumped; to further ensure uniformity, increase the cell radius $L$ by an order of magnitude while keeping $P$, $k_{\\mathrm{ch}}$, and $D_{i}$ fixed, which decreases the boundary-to-internal ratio by reducing gradients across the larger cell.**\n\n- **Baseline interpretation:** The statement \"intracellular diffusion is fast relative to boundary transfer\" means $\\mathrm{Bi}_{\\mathrm{ov}} \\ll 1$. Our calculation showed $\\mathrm{Bi}_{\\mathrm{ov}} \\approx 139 \\gg 1$. Therefore, the premise that the cell is already lumped is false.\n- **Proposed changes:** \"increase the cell radius $L$ by an order of magnitude... which decreases the boundary-to-internal ratio\". The ratio is $\\mathrm{Bi}_{\\mathrm{ov}} = k_{\\mathrm{ov}} L / D_{i}$. Since $\\mathrm{Bi}_{\\mathrm{ov}}$ is directly proportional to $L$, increasing $L$ will *increase* the Biot number, pushing the system even further into the diffusion-limited regime. The physical reasoning provided (\"reducing gradients\") is also incorrect; for a given flux, a larger length scale $L$ leads to larger, not smaller, internal concentration differences.\n- **Verdict:** Incorrect.\n\nBased on this analysis, only option A correctly diagnoses the baseline condition and proposes a set of changes that are both conceptually sound and quantitatively verified to achieve the desired lumped-intracellular regime.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "A central goal of multiscale modeling is to derive simplified, coarse-grained models that are both computationally tractable and predictively accurate. A key question in this process is determining when it is valid to \"integrate out\" fast-evolving variables and what the consequences are for the remaining slow dynamics. This exercise provides a data-driven approach to explore this question, where you will analyze simulated time-series data to determine if a simple Markovian model suffices or if the memory of the fast dynamics—a non-Markovian effect—is significant and must be explicitly included in the model .",
            "id": "3922639",
            "problem": "Develop a complete program that, given multiscale simulation data from a linear stochastic two-species system representing a simplified synthetic gene module, determines whether a Markovian closure is sufficient for a chosen coarse variable or whether non-Markovian history terms are necessary. Your task is to compute discrete memory kernels from the simulated data and to decide, per test case, whether adding history terms significantly improves predictive performance.\n\nThe microscale dynamics are defined by a linear stochastic differential equation system for variables $x(t)$ and $y(t)$:\n$$\n\\frac{dx}{dt} = -a\\,x + b\\,y + \\xi_x(t),\\qquad\n\\frac{dy}{dt} = -c\\,y + d\\,x + \\xi_y(t),\n$$\nwhere $\\xi_x(t)$ and $\\xi_y(t)$ are independent zero-mean Gaussian white noises with intensities $2 D_x$ and $2 D_y$, respectively. All variables and parameters are dimensionless. The coarse variable of interest is $x(t)$.\n\nFrom the perspective of model reduction, the effective coarse-grained dynamics for $x(t)$ generally obey a Generalized Langevin Equation (GLE), which for linear systems can be written in continuous time as\n$$\n\\frac{dx}{dt}(t) = -k_0\\,x(t) - \\int_0^{\\infty} K(\\tau)\\,x(t-\\tau)\\,d\\tau + \\eta(t),\n$$\nwhere $K(\\tau)$ is the memory kernel and $\\eta(t)$ is an effective random force that is orthogonal to the resolved subspace under a suitable projection. In a purely Markovian closure, the history integral term is absent, and the dynamics reduce to a first-order linear model in $x(t)$.\n\nYour program must:\n- Simulate the microscale system using the Euler–Maruyama scheme with time step $\\Delta t$, for $N$ steps after discarding a fixed burn-in. Use a fixed pseudorandom seed for reproducibility.\n- From the simulated $x(t_n)$ time series with $t_n = n\\,\\Delta t$, construct a discrete-time regression model that approximates the GLE memory integral by a finite convolution with $M$ lags:\n$$\n\\frac{x_{n+1} - x_n}{\\Delta t} \\approx -k_0\\,x_n - \\sum_{m=1}^{M} K_m\\,x_{n-m}.\n$$\nEstimate the coefficients $\\{k_0, K_1,\\dots, K_M\\}$ by linear least squares with a small Tikhonov regularization. Also estimate the Markovian model with $M=0$.\n- Split the data into disjoint training and testing segments. Fit model parameters on the training segment and evaluate one-step-ahead predictions on the testing segment using the true lagged values of $x$ available from the data.\n- Compute the mean squared error (MSE) on the testing segment for the Markovian model, $\\mathrm{MSE}_{\\mathrm{markov}}$, and for the non-Markovian model with history, $\\mathrm{MSE}_{\\mathrm{memory}}$. Define the relative improvement\n$$\nI = \\frac{\\mathrm{MSE}_{\\mathrm{markov}} - \\mathrm{MSE}_{\\mathrm{memory}}}{\\mathrm{MSE}_{\\mathrm{markov}}}.\n$$\nDeclare that history terms are necessary if $I > \\tau$, for a fixed threshold $\\tau$.\n\nFundamental base for the derivation and algorithmic design:\n- The Euler–Maruyama method is a standard discretization for stochastic differential equations: $x_{n+1} = x_n + \\Delta t\\,f(x_n) + \\sqrt{2 D \\Delta t}\\,Z_n$ with $Z_n \\sim \\mathcal{N}(0,1)$.\n- The Mori–Zwanzig projection formalism for linear systems yields a GLE with a memory kernel $K(\\tau)$ acting via convolution on the resolved variable.\n- Linear least squares with Tikhonov regularization provides a consistent estimator for linear convolution coefficients under Gaussian perturbations.\n\nSimulation and estimation parameters to use for all tests:\n- Time step $\\Delta t = 0.005$.\n- Total simulated steps after burn-in $N = 30000$.\n- Burn-in steps to discard $N_{\\mathrm{burn}} = 1000$.\n- Number of lags $M = 200$.\n- Training fraction $f_{\\mathrm{train}} = 0.75$ of the post–burn-in series.\n- Regularization strength $\\lambda = 10^{-4}$.\n- Decision threshold $\\tau = 0.1$.\n\nTest suite of parameter sets $(a,b,c,d,D_x,D_y)$:\n1. Case A (fast hidden variable, weak feedback): $(a,b,c,d,D_x,D_y) = (1.0,0.8,5.0,0.2,0.05,0.05)$.\n2. Case B (comparable time scales, strong bidirectional coupling): $(a,b,c,d,D_x,D_y) = (1.0,0.8,1.0,0.8,0.05,0.05)$.\n3. Case C (no coupling, baseline Markovian): $(a,b,c,d,D_x,D_y) = (1.0,0.0,1.0,0.0,0.05,0.05)$.\n4. Case D (near-balanced slow modes, strong but stable coupling): $(a,b,c,d,D_x,D_y) = (0.4,0.38,0.4,0.38,0.05,0.05)$.\n\nScientific realism constraints to check:\n- Stability of the linear drift requires $a > 0$, $c > 0$, and $a c > b d$.\n\nOutput specification:\n- For each test case, compute $I$ and compare to $\\tau$. Produce a single line of output containing a list of booleans corresponding to each test case in the order A, B, C, D, where each entry is $\\mathrm{True}$ if history terms are necessary and $\\mathrm{False}$ otherwise. The list must be printed in the exact format $[b_1,b_2,b_3,b_4]$ with no additional text.\n\nAngles and physical units are not involved; all quantities are dimensionless. All random number generation must be seeded to ensure deterministic output.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3,result_4]$).",
            "solution": "The objective of this problem is to systematically determine whether a reduced, single-variable model for a synthetic gene module requires non-Markovian memory effects. We are given a two-dimensional linear stochastic system and must decide if its coarse-grained dynamics for one variable, $x(t)$, can be accurately described by a simple Markovian model or if a Generalized Langevin Equation (GLE) with a memory kernel is necessary. This decision is based on a quantitative comparison of the predictive accuracy of the two model types.\n\nThe microscale dynamics are described by a set of coupled linear stochastic differential equations (SDEs) for the variables $x(t)$ and $y(t)$:\n$$\n\\begin{align*}\n\\frac{dx}{dt} = -a\\,x + b\\,y + \\xi_x(t) \\\\\n\\frac{dy}{dt} = -c\\,y + d\\,x + \\xi_y(t)\n\\end{align*}\n$$\nHere, $a$, $b$, $c$, and $d$ are constant parameters defining the deterministic drift, and $\\xi_x(t)$ and $\\xi_y(t)$ are independent Gaussian white noise terms with intensities $2D_x$ and $2D_y$, respectively. For the system to be stable, its drift matrix must have eigenvalues with negative real parts, which for this system corresponds to the conditions $a > 0$, $c > 0$, and $ac > bd$.\n\nThe theoretical foundation for this model reduction problem is the Mori-Zwanzig projection operator formalism. This formalism states that the dynamics of a resolved variable, here $x(t)$, obtained by integrating out an unresolved variable, here $y(t)$, are governed by a GLE. For a linear system, this GLE takes the form:\n$$\n\\frac{dx}{dt}(t) = -k_0\\,x(t) - \\int_0^{\\infty} K(\\tau)\\,x(t-\\tau)\\,d\\tau + \\eta(t)\n$$\nThe integral term represents the memory of the system: the rate of change of $x$ at time $t$ depends on its past values. The function $K(\\tau)$ is the memory kernel. If the unresolved variable $y(t)$ evolves on a much faster timescale than $x(t)$, it is often possible to approximate its effect as instantaneous, leading to a Markovian model where the memory integral vanishes. Our task is to test this approximation.\n\nThe solution strategy involves the following steps:\n\n1.  **Numerical Simulation**: First, we generate a time series solution $\\{x(t_n), y(t_n)\\}$ from the microscale SDEs. We employ the Euler-Maruyama method for discretization with a small time step $\\Delta t$. For a variable $z(t)$ following $dz/dt = f(z) + \\xi(t)$ with noise intensity $2D$, the update rule is:\n    $$\n    z_{n+1} = z_n + f(z_n)\\Delta t + \\sqrt{2D\\Delta t} \\, W_n\n    $$\n    where $z_n = z(n\\Delta t)$ and $W_n$ is a random variable drawn from a standard normal distribution $\\mathcal{N}(0,1)$. We apply this to both $x$ and $y$. A burn-in period of $N_{\\mathrm{burn}}$ steps is simulated and discarded to ensure the system has reached its stationary state. We then collect $N$ subsequent data points.\n\n2.  **Regression Model Formulation**: We approximate the continuous GLE with a discrete-time linear regression model. The time derivative is approximated by a finite difference, $\\dot{x}(t_n) \\approx (x_{n+1} - x_n)/\\Delta t$, and the memory integral is replaced by a finite, weighted sum over $M$ previous time steps:\n    $$\n    \\frac{x_{n+1} - x_n}{\\Delta t} \\approx -k_0\\,x_n - \\sum_{m=1}^{M} K_m\\,x_{n-m}\n    $$\n    This equation can be cast into the standard linear regression form $\\mathbf{d} = \\mathbf{X}\\boldsymbol{\\beta}$, where:\n    - The response vector $\\mathbf{d}$ consists of the estimated derivatives, $d_n = (x_{n+1} - x_n)/\\Delta t$.\n    - The design matrix $\\mathbf{X}$ has rows corresponding to each time step $n$, where each row contains the lagged values $[x_n, x_{n-1}, \\dots, x_{n-M}]$.\n    - The parameter vector is $\\boldsymbol{\\beta} = [-k_0, -K_1, \\dots, -K_M]^T$.\n    \n    We formulate two models:\n    - **Non-Markovian (memory) model**: Uses $M=200$ lags, so the design matrix has $M+1=201$ columns.\n    - **Markovian model**: This is a special case with $M=0$. The regression simplifies to $(x_{n+1} - x_n)/\\Delta t \\approx -k_0\\,x_n$. The design matrix has only one column, $[x_n]$.\n\n3.  **Parameter Estimation**: The coefficients $\\boldsymbol{\\beta}$ for both models are estimated using Tikhonov-regularized linear least squares (Ridge Regression). This method is robust against multicollinearity, which is common in time-series data. The estimator for $\\boldsymbol{\\beta}$ is given by:\n    $$\n    \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X} + \\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{d}\n    $$\n    where $\\lambda$ is the regularization strength, set to $10^{-4}$. We solve this system computationally using a stable linear solver.\n\n4.  **Model Evaluation and Decision**: The simulated time series is partitioned into a training segment (the first $f_{\\mathrm{train}} = 75\\%$ of the data) and a testing segment. The model coefficients ($\\hat{\\boldsymbol{\\beta}}_{\\mathrm{markov}}$ and $\\hat{\\boldsymbol{\\beta}}_{\\mathrm{memory}}$) are fitted using only the training data. Then, we evaluate the one-step-ahead predictive performance of both models on the unseen testing data. For each model, we calculate the Mean Squared Error (MSE):\n    $$\n    \\text{MSE} = \\frac{1}{N_{\\text{test}}} \\sum_{n \\in \\text{test}} \\left( d_n - \\hat{d}_n \\right)^2\n    $$\n    where $d_n$ is the true value from the data and $\\hat{d}_n = \\mathbf{x}_n^T\\hat{\\boldsymbol{\\beta}}$ is the model's prediction. This yields $\\text{MSE}_{\\mathrm{markov}}$ and $\\text{MSE}_{\\mathrm{memory}}$.\n\n    Finally, we quantify the benefit of including memory terms using the relative improvement metric $I$:\n    $$\n    I = \\frac{\\text{MSE}_{\\mathrm{markov}} - \\text{MSE}_{\\mathrm{memory}}}{\\text{MSE}_{\\mathrm{markov}}}\n    $$\n    A decision is made based on a fixed threshold $\\tau=0.1$. If $I > \\tau$, the improvement is significant, and we conclude that history terms are necessary. Otherwise, the simpler Markovian model is deemed sufficient. This entire procedure is repeated for each parameter set provided in the test suite.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of determining if a Markovian closure is sufficient\n    for a coarse-grained variable in a linear stochastic two-species system.\n    \"\"\"\n    # Simulation and estimation parameters\n    DELTA_T = 0.005\n    N_STEPS = 30000\n    N_BURN = 1000\n    M_LAGS = 200\n    F_TRAIN = 0.75\n    LAMBDA = 1e-4\n    TAU = 0.1\n    SEED = 0\n\n    # Test suite of parameter sets (a, b, c, d, D_x, D_y)\n    test_cases = [\n        (1.0, 0.8, 5.0, 0.2, 0.05, 0.05),  # Case A\n        (1.0, 0.8, 1.0, 0.8, 0.05, 0.05),  # Case B\n        (1.0, 0.0, 1.0, 0.0, 0.05, 0.05),  # Case C\n        (0.4, 0.38, 0.4, 0.38, 0.05, 0.05), # Case D\n    ]\n\n    results = []\n\n    def simulate(a, b, c, d, D_x, D_y):\n        \"\"\"\n        Simulates the 2D SDE system using Euler-Maruyama.\n        \"\"\"\n        np.random.seed(SEED)\n        total_steps = N_STEPS + N_BURN\n        x = np.zeros(total_steps)\n        y = np.zeros(total_steps)\n        \n        sqrt_2Dx_dt = np.sqrt(2 * D_x * DELTA_T)\n        sqrt_2Dy_dt = np.sqrt(2 * D_y * DELTA_T)\n\n        for n in range(total_steps - 1):\n            w_x = np.random.randn()\n            w_y = np.random.randn()\n            \n            # Drift terms\n            drift_x = -a * x[n] + b * y[n]\n            drift_y = -c * y[n] + d * x[n]\n            \n            # Update step\n            x[n+1] = x[n] + drift_x * DELTA_T + sqrt_2Dx_dt * w_x\n            y[n+1] = y[n] + drift_y * DELTA_T + sqrt_2Dy_dt * w_y\n            \n        return x[N_BURN:]\n\n    def fit_model(X, y, lambda_reg):\n        \"\"\"\n        Fits a linear model using Tikhonov-regularized least squares.\n        \"\"\"\n        A = X.T @ X + lambda_reg * np.identity(X.shape[1])\n        b = X.T @ y\n        theta = np.linalg.solve(A, b)\n        return theta\n\n    def calculate_mse(X, y, theta):\n        \"\"\"\n        Calculates the mean squared error of predictions.\n        \"\"\"\n        y_pred = X @ theta\n        mse = np.mean((y - y_pred)**2)\n        return mse\n\n    for params in test_cases:\n        a, b, c, d, D_x, D_y = params\n\n        # 1. Simulate the system\n        x_series = simulate(a, b, c, d, D_x, D_y)\n\n        # 2. Prepare data for regression\n        # Target variable: d_n = (x_{n+1} - x_n) / dt\n        # Predictors for memory model: [x_n, x_{n-1}, ..., x_{n-M}]\n        # We can construct samples for indices n from M up to N-2\n        num_samples = N_STEPS - M_LAGS - 1\n        \n        # Response vector (approximated derivative)\n        d_vec = (x_series[M_LAGS+1:] - x_series[M_LAGS:-1]) / DELTA_T\n\n        # Design matrix for the memory model\n        X_mem = np.zeros((num_samples, M_LAGS + 1))\n        for i in range(num_samples):\n            # Row i corresponds to time index n = i + M_LAGS\n            X_mem[i, :] = x_series[i : i + M_LAGS + 1][::-1]\n\n        # Design matrix for the Markovian model (M=0)\n        X_markov = x_series[M_LAGS:-1].reshape(-1, 1)\n\n        # 3. Split data into training and testing sets\n        n_train = int(num_samples * F_TRAIN)\n        \n        d_train, d_test = d_vec[:n_train], d_vec[n_train:]\n        X_mem_train, X_mem_test = X_mem[:n_train], X_mem[n_train:]\n        X_markov_train, X_markov_test = X_markov[:n_train], X_markov[n_train:]\n\n        # 4. Fit models on training data\n        theta_markov = fit_model(X_markov_train, d_train, LAMBDA)\n        theta_memory = fit_model(X_mem_train, d_train, LAMBDA)\n        \n        # 5. Evaluate models on testing data\n        mse_markov = calculate_mse(X_markov_test, d_test, theta_markov)\n        mse_memory = calculate_mse(X_mem_test, d_test, theta_memory)\n\n        # 6. Calculate improvement and make decision\n        if mse_markov > 0:\n            improvement = (mse_markov - mse_memory) / mse_markov\n        else:\n            # If Markovian model is perfect, no improvement is possible.\n            improvement = 0.0\n\n        history_is_necessary = improvement > TAU\n        results.append(history_is_necessary)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}