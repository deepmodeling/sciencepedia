{
    "hands_on_practices": [
        {
            "introduction": "How do individual cells know when to divide? This is a fundamental question in biology, and agent-based models provide a powerful framework for exploring different control strategies. This exercise challenges you to implement and compare three canonical models of cell size control: the \"sizer,\" \"adder,\" and \"timer\" mechanisms. By simulating these distinct microscopic rules and analyzing the resulting relationship between cell size at birth and division, you will gain hands-on experience in connecting theoretical principles to distinct, measurable population-level data signatures. ",
            "id": "3905570",
            "problem": "You are tasked with designing and implementing a complete, runnable program that compares three canonical single-cell division control hypotheses in agent-based modeling of cellular populations: the sizer, the adder, and the timer. Your program must (i) derive and simulate the expected steady-state size distributions at birth and at division for each control model from first principles, (ii) compute a quantitative criterion to identify which model best matches a provided dataset of single-cell sizes generated under known parameters, and (iii) output a single line containing the predicted model identifiers for a set of test cases. The three models are defined by the way cells determine when to divide, under exponential growth with a constant growth rate. The foundational base is the standard biological growth law and stochastic partitioning:\n- Exponential growth law: a cell with birth size $s_b$ grows as $s(t) = s_b \\exp(g t)$ with constant growth rate $g$.\n- Stochastic partitioning at division: when a cell divides at size $s_d$, the daughter birth sizes are $f s_d$ and $(1-f) s_d$, where $f \\in (0,1)$ is a random partition fraction sampled independently at each division from a Beta distribution with parameters $(\\alpha,\\beta)$.\n\nThe division control models are defined as follows:\n- Sizer: the cell divides when its size reaches a noisy threshold, $s_d = S_c \\exp(\\eta)$, where $S_c$ is a constant and $\\eta \\sim \\mathcal{N}(0,\\sigma_\\eta^2)$ captures threshold noise.\n- Adder: the cell divides after adding a noisy size increment, $s_d = s_b + \\Delta$, where $\\Delta = \\Delta_0 \\exp(\\eta)$, with $\\Delta_0$ constant and $\\eta \\sim \\mathcal{N}(0,\\sigma_\\eta^2)$.\n- Timer: the cell divides after a noisy timer duration, $T \\sim \\mathcal{N}(T_0,\\sigma_T^2)$, under exponential growth $s_d = s_b \\exp(g T)$, where $T_0$ is a constant mean division time.\n\nYour task is to derive, from the above foundation without shortcut formulas, the expected shape of the steady-state distributions $p(s_b)$ and $p(s_d)$ under each model and then implement an agent-based simulation to approximate these distributions and compute a robust statistic for model identification from the dataset. The program must:\n1. Simulate a single-cell lineage for a fixed number of generations $n_{\\text{gen}}$ using the provided parameters to generate two arrays: birth sizes $\\{s_b^{(i)}\\}_{i=1}^{n_{\\text{gen}}}$ and division sizes $\\{s_d^{(i)}\\}_{i=1}^{n_{\\text{gen}}}$.\n2. For identification, compute a linear regression slope $\\hat{m}$ between $s_d^{(i)}$ and $s_b^{(i)}$ over the simulated dataset, and compare this slope to the theoretically expected slopes for each model, defined in terms of $g$ and $T_0$. Specifically, the expected slopes are: sizer $\\approx 0$, adder $\\approx 1$, timer $\\approx \\exp(g T_0)$. The model minimizing the absolute deviation $|\\hat{m} - m_{\\text{model}}|$ is the predicted match.\n3. For each test case, return the predicted model identifier as an integer, where sizer is $0$, adder is $1$, and timer is $2$.\n\nNo physical units are required; sizes are in arbitrary units (a.u.) and times are in arbitrary time units. Angles are not used. All outputs must be numeric.\n\nTest Suite:\nUse the following parameter sets and random seeds to generate the datasets. For each case, simulate $n_{\\text{gen}}$ divisions as a single lineage starting from initial birth size $s_b^{(0)} = 0.8$. For reproducibility, set the random number generator seed to the given integer.\n\n- Case $1$ (happy path, sizer-generated dataset):\n  - True model: sizer ($0$)\n  - $g = 0.02$\n  - $S_c = 1.0$\n  - $\\sigma_\\eta = 0.1$\n  - $\\alpha = 50$, $\\beta = 50$\n  - $T_0 = \\ln(2)/g$ (used only to define the timer expectation for comparison)\n  - $\\sigma_T = 3.0$\n  - $n_{\\text{gen}} = 3000$\n  - Seed $= 42$\n\n- Case $2$ (happy path, adder-generated dataset):\n  - True model: adder ($1$)\n  - $g = 0.02$\n  - $\\Delta_0 = 0.5$\n  - $\\sigma_\\eta = 0.1$\n  - $\\alpha = 50$, $\\beta = 50$\n  - $T_0 = \\ln(2)/g$\n  - $\\sigma_T = 3.0$\n  - $n_{\\text{gen}} = 3000$\n  - Seed $= 43$\n\n- Case $3$ (edge case, timer-generated dataset with asymmetric partitioning):\n  - True model: timer ($2$)\n  - $g = 0.02$\n  - $T_0 = \\ln(2)/g$\n  - $\\sigma_T = 3.0$\n  - $\\alpha = 5$, $\\beta = 5$\n  - $S_c = 1.0$\n  - $\\Delta_0 = 0.5$\n  - $\\sigma_\\eta = 0.1$\n  - $n_{\\text{gen}} = 3000$\n  - Seed $= 44$\n\n- Case $4$ (boundary case, adder-generated dataset with nearly deterministic division and symmetric partitioning):\n  - True model: adder ($1$)\n  - $g = 0.02$\n  - $\\Delta_0 = 0.7$\n  - $\\sigma_\\eta = 10^{-6}$\n  - $\\alpha = 500$, $\\beta = 500$\n  - $T_0 = \\ln(2)/g$\n  - $\\sigma_T = 3.0$\n  - $n_{\\text{gen}} = 2000$\n  - Seed $= 45$\n\nImplementation and Output Requirements:\n- Your program must be a single, self-contained, runnable Python $3.12$ script using only the standard library plus NumPy ($1.23.5$) and SciPy ($1.11.4$) if needed.\n- For each test case, you must:\n  1. Generate the dataset by simulating the lineage under the specified true model and parameters.\n  2. Compute the regression slope $\\hat{m}$ between $s_d$ and $s_b$.\n  3. Compute the expected slopes for sizer ($0$), adder ($1$), and timer ($\\exp(g T_0)$).\n  4. Predict the model by minimizing the absolute deviation between $\\hat{m}$ and these expected slopes, and return the identifier ($0$, $1$, or $2$).\n- Final Output Format: Your program should produce a single line of output containing the predicted model identifiers for all test cases as a comma-separated list enclosed in square brackets (e.g., \"[$m_1,m_2,m_3,m_4$]\"), where each $m_i$ is an integer in $\\{0,1,2\\}$.\n\nYour program must not require any user input or external files and must run deterministically using the provided seeds. The only output must be the final list of model predictions in the required format.",
            "solution": "The problem posed is a well-defined task in computational biology, specifically in the field of synthetic biology modeling. It requires the implementation of an agent-based simulation to distinguish between three canonical models of cell size control. The problem is scientifically grounded, self-contained, and algorithmically specified. All parameters are provided, and the objective is clear. Therefore, the problem is valid and a solution can be formulated.\n\nThe core of the problem lies in understanding how different division control strategies manifest in observable correlations between cell size at birth, $s_b$, and cell size at division, $s_d$. We will first formally define the models and the simulation framework, then derive the theoretical basis for the identification criterion, and finally outline the algorithmic procedure.\n\n**Foundational Principles: Cell Growth and Division**\n\nThe simulation is based on two fundamental assumptions for a single cell lineage:\n1.  **Exponential Growth:** A cell with birth size $s_b$ grows exponentially over time $t$ with a constant rate $g$. Its size is given by the function $s(t) = s_b \\exp(g t)$.\n2.  **Stochastic Partitioning:** Upon division at size $s_d$, the cell splits into two daughters. The birth size of the daughter cell we follow in the lineage is $s_b' = f \\cdot s_d$, where $f$ is a random fraction. This fraction $f$ is sampled from a Beta distribution, $f \\sim \\text{Beta}(\\alpha, \\beta)$, for each division event. The Beta distribution is defined on the interval $(0, 1)$ and is appropriate for modeling partitioning ratios. The parameters $\\alpha$ and $\\beta$ control the shape of the distribution; symmetric partitioning (mean $0.5$) occurs when $\\alpha = \\beta$.\n\n**Division Control Models**\n\nThe decision to divide is governed by one of three distinct models, each introducing stochasticity in a different manner.\n\n1.  **Sizer Model:** Division is triggered when the cell's size $s$ reaches a specific threshold. This threshold is not fixed but is subject to biological noise. The division size $s_d$ is independent of the birth size $s_b$ and is determined by:\n    $$s_d = S_c \\exp(\\eta)$$\n    where $S_c$ is a characteristic size constant and $\\eta$ is a noise term drawn from a normal distribution $\\mathcal{N}(0, \\sigma_\\eta^2)$. The use of a log-normal distribution for size ensures $s_d$ is always positive.\n\n2.  **Adder Model:** Division is triggered after the cell has added a certain amount of size, $\\Delta$, to its birth size $s_b$. This added size is noisy. The division size is given by:\n    $$s_d = s_b + \\Delta$$\n    where the added size $\\Delta = \\Delta_0 \\exp(\\eta)$, with $\\Delta_0$ being a characteristic size increment and $\\eta \\sim \\mathcal{N}(0, \\sigma_\\eta^2)$ representing noise.\n\n3.  **Timer Model:** Division is triggered after a specific duration of time, $T$, has passed since birth. This duration is noisy. Given exponential growth, the division size is:\n    $$s_d = s_b \\exp(g T)$$\n    where the division time $T$ is drawn from a normal distribution $T \\sim \\mathcal{N}(T_0, \\sigma_T^2)$, with $T_0$ being the mean division time.\n\n**Derivation of the Identification Criterion**\n\nThe problem proposes using the slope of a linear regression between the division sizes $\\{s_d^{(i)}\\}$ and birth sizes $\\{s_b^{(i)}\\}$ as the criterion for model identification. Let the linear regression model be $\\hat{s}_d = \\hat{m} s_b + \\hat{c}$. We can derive the theoretically expected slope, $m_{\\text{model}}$, for each case.\n\n-   **Sizer:** In this model, $s_d = S_c \\exp(\\eta)$. The division size $s_d$ is determined independently of the birth size $s_b$. Therefore, there is no expected correlation between $s_b$ and $s_d$. A linear regression on data generated by a sizer mechanism should yield a slope close to zero.\n    $$m_{\\text{sizer}} \\approx 0$$\n\n-   **Adder:** The model is defined by the linear relationship $s_d = s_b + \\Delta$. In a regression of $s_d$ on $s_b$, this corresponds to a line with a slope of exactly $1$ and an intercept equal to the average added size, $\\langle \\Delta \\rangle$. The noise term $\\eta$ affects the intercept, not the slope. Thus, the expected slope is precisely one.\n    $$m_{\\text{adder}} = 1$$\n\n-   **Timer:** The model is $s_d = s_b \\exp(gT)$. The relationship between $s_d$ and $s_b$ is multiplicative. The slope $\\hat{m}$ of a linear regression is given by $\\hat{m} = \\text{Cov}(s_b, s_d) / \\text{Var}(s_b)$. Substituting the model equation, we get $\\text{Cov}(s_b, s_b \\exp(gT))$. At steady state, the birth size $s_b$ of a generation is determined by the division time $T$ of the previous generation, introducing a complex correlation. However, a common and effective approximation is to assume that $s_b$ and $T$ of the same generation are approximately independent. Under this assumption, $\\text{Cov}(s_b, s_b \\exp(gT)) \\approx \\text{Var}(s_b) E[\\exp(gT)]$. The slope is then $m \\approx E[\\exp(gT)]$. For $T \\sim \\mathcal{N}(T_0, \\sigma_T^2)$, the term $gT$ is normally distributed as $gT \\sim \\mathcal{N}(gT_0, g^2\\sigma_T^2)$. The expectation of the resulting log-normal variable is $E[\\exp(gT)] = \\exp(gT_0 + g^2\\sigma_T^2/2)$. The problem specifies using the simpler approximation, which is valid for small noise or as a first-order characterization:\n    $$m_{\\text{timer}} \\approx \\exp(g T_0)$$\n    For the given parameters where $gT_0 = \\ln(2)$, this evaluates to $m_{\\text{timer}} \\approx 2$. This value is clearly distinct from $0$ and $1$, making the slope a robust identifier.\n\n**Simulation and Analysis Algorithm**\n\nFor each test case provided:\n1.  **Initialization:** The simulation parameters ($g$, $S_c$, $\\sigma_\\eta$, etc.), true model type, number of generations $n_{\\text{gen}}$, and random seed are specified. We initialize a random number generator with the given seed. We create two lists to store the sequences of birth sizes, $\\{s_b^{(i)}\\}$, and division sizes, $\\{s_d^{(i)}\\}$. The simulation begins with the first cell at generation $i=0$ having the specified initial birth size, $s_b^{(0)} = 0.8$.\n\n2.  **Generational Loop:** A loop runs for $n_{\\text{gen}}$ generations, from $i=0$ to $n_{\\text{gen}}-1$. In each iteration $i$:\n    a. The current birth size is $s_b^{(i)}$.\n    b. Based on the `true_model` for the test case, the corresponding division rule is applied to calculate the division size $s_d^{(i)}$. This involves sampling from the relevant noise distribution ($\\mathcal{N}(0, \\sigma_\\eta^2)$ for size noise or $\\mathcal{N}(T_0, \\sigma_T^2)$ for time noise).\n    c. The pair $(s_b^{(i)}, s_d^{(i)})$ is recorded.\n    d. The birth size for the next generation, $s_b^{(i+1)}$, is calculated by simulating partition. A random fraction $f$ is drawn from $\\text{Beta}(\\alpha, \\beta)$, and the next birth size is set to $s_b^{(i+1)} = f \\cdot s_d^{(i)}$.\n\n3.  **Model Identification:** After completing the simulation:\n    a. The recorded lists of birth and division sizes are converted to numerical arrays.\n    b. A simple linear regression of $s_d$ on $s_b$ is performed to compute the sample slope, $\\hat{m}$.\n    c. The three theoretical slopes are calculated: $m_0 = 0$, $m_1 = 1$, and $m_2 = \\exp(g T_0)$.\n    d. The absolute deviations $|\\hat{m} - m_0|$, $|\\hat{m} - m_1|$, and $|\\hat{m} - m_2|$ are computed.\n    e. The model corresponding to the minimum deviation is chosen as the predicted model. The result is its integer identifier ($0$ for sizer, $1$ for adder, $2$ for timer).\n\nThis entire procedure is repeated for each of the four test cases, and the resulting list of identifiers is formatted as the final output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Solves the cell division model identification problem for a suite of test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1 (happy path, sizer-generated dataset)\n        {\n            \"true_model\": 0, \"g\": 0.02, \"S_c\": 1.0, \"sigma_eta\": 0.1,\n            \"alpha\": 50, \"beta\": 50, \"T_0_factor\": np.log(2), \"sigma_T\": 3.0,\n            \"delta_0\": 0.5, \"n_gen\": 3000, \"seed\": 42\n        },\n        # Case 2 (happy path, adder-generated dataset)\n        {\n            \"true_model\": 1, \"g\": 0.02, \"delta_0\": 0.5, \"sigma_eta\": 0.1,\n            \"alpha\": 50, \"beta\": 50, \"T_0_factor\": np.log(2), \"sigma_T\": 3.0,\n            \"S_c\": 1.0, \"n_gen\": 3000, \"seed\": 43\n        },\n        # Case 3 (edge case, timer-generated dataset with asymmetric partitioning)\n        {\n            \"true_model\": 2, \"g\": 0.02, \"T_0_factor\": np.log(2), \"sigma_T\": 3.0,\n            \"alpha\": 5, \"beta\": 5, \"S_c\": 1.0, \"delta_0\": 0.5,\n            \"sigma_eta\": 0.1, \"n_gen\": 3000, \"seed\": 44\n        },\n        # Case 4 (boundary case, adder-generated dataset with low noise)\n        {\n            \"true_model\": 1, \"g\": 0.02, \"delta_0\": 0.7, \"sigma_eta\": 1e-6,\n            \"alpha\": 500, \"beta\": 500, \"T_0_factor\": np.log(2), \"sigma_T\": 3.0,\n            \"S_c\": 1.0, \"n_gen\": 2000, \"seed\": 45\n        }\n    ]\n\n    results = []\n    initial_s_b = 0.8\n\n    for case in test_cases:\n        # Set seed for reproducibility\n        np.random.seed(case[\"seed\"])\n\n        # Unpack parameters\n        true_model = case[\"true_model\"]\n        g = case[\"g\"]\n        n_gen = case[\"n_gen\"]\n        alpha = case[\"alpha\"]\n        beta = case[\"beta\"]\n        sigma_eta = case[\"sigma_eta\"]\n        S_c = case[\"S_c\"]\n        delta_0 = case[\"delta_0\"]\n        # T_0 is specified relative to g\n        T_0 = case[\"T_0_factor\"] / g\n        sigma_T = case[\"sigma_T\"]\n\n        s_b_data = []\n        s_d_data = []\n        \n        current_s_b = initial_s_b\n\n        # Simulation loop for a single lineage\n        for _ in range(n_gen):\n            s_b = current_s_b\n            s_d = 0.0\n\n            # Apply the true model to determine division size s_d\n            if true_model == 0:  # Sizer\n                eta = np.random.normal(0, sigma_eta)\n                s_d = S_c * np.exp(eta)\n            elif true_model == 1:  # Adder\n                eta = np.random.normal(0, sigma_eta)\n                s_d = s_b + delta_0 * np.exp(eta)\n            elif true_model == 2:  # Timer\n                T = np.random.normal(T_0, sigma_T)\n                # Ensure time is non-negative, though highly unlikely to be negative\n                if T < 0: T = 0\n                s_d = s_b * np.exp(g * T)\n\n            # Store the data for the current generation\n            s_b_data.append(s_b)\n            s_d_data.append(s_d)\n\n            # Calculate birth size for the next generation via partitioning\n            f = np.random.beta(alpha, beta)\n            current_s_b = f * s_d\n\n        # Convert lists to NumPy arrays for analysis\n        s_b_array = np.array(s_b_data)\n        s_d_array = np.array(s_d_data)\n\n        # Perform linear regression to find the slope\n        # np.polyfit(x, y, 1) returns [slope, intercept]\n        m_hat = np.polyfit(s_b_array, s_d_array, 1)[0]\n\n        # Define the theoretical slopes for comparison\n        m_sizer = 0.0\n        m_adder = 1.0\n        m_timer = np.exp(g * T_0)\n        \n        theoretical_slopes = [m_sizer, m_adder, m_timer]\n\n        # Calculate deviations and find the model with the minimum deviation\n        deviations = [np.abs(m_hat - m_model) for m_model in theoretical_slopes]\n        predicted_model_id = np.argmin(deviations)\n        \n        results.append(predicted_model_id)\n\n    # Print the final result in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A robust agent-based model must not only simulate dynamics correctly but also obey fundamental physical and biological laws under all conditions. For example, cells cannot occupy more space than is available, and physical quantities like interaction energy must satisfy basic constraints such as non-negativity. This practice introduces the powerful software engineering concept of property-based testing to ensure your model's integrity by defining and verifying its core invariants. You will write code that automatically generates random agent states and tests whether they violate these essential rules, thereby building confidence in your model's physical and biological plausibility. ",
            "id": "3905502",
            "problem": "Consider a two-dimensional Agent-Based Modeling (ABM) representation of a cellular population confined to a rectangular domain. Each cell is modeled as a circular agent with center positions and radii. Let the domain be a rectangle with side lengths $L_x$ and $L_y$ measured in micrometers ($\\mu\\mathrm{m}$). Let there be $N$ agents with centers $(x_i, y_i)$ and radii $r_i$, for $i \\in \\{1,\\dots,N\\}$. Define the following model-invariants grounded in first principles:\n\n1. Packing fraction invariant: The occupied area fraction (packing fraction) $\\phi$ equals the area of the union of agent disks divided by the domain area $L_x L_y$. It must satisfy $0 \\le \\phi \\le \\phi_{\\max}$ for a prescribed model parameter $\\phi_{\\max}$, where $\\phi_{\\max} \\le 1$ is chosen to reflect a biologically plausible maximum packing constraint. In practice, compute $\\phi$ by rasterizing the domain on a square grid of spacing $\\Delta$ (in $\\mu\\mathrm{m}$), counting grid points that lie inside at least one disk, and forming the ratio of occupied points to total grid points.\n\n2. Energy positivity invariant: Define the pairwise mechanical overlap energy as\n$$\nE = \\sum_{1 \\le i < j \\le N} \\frac{k}{2}\\,\\left[\\max\\!\\left(0,\\, r_i + r_j - d_{ij}\\right)\\right]^2,\n$$\nwhere $k$ has units of picoNewton per micrometer ($\\mathrm{pN}/\\mu\\mathrm{m}$), $d_{ij}$ is the Euclidean distance between centers $(x_i, y_i)$ and $(x_j, y_j)$ in $\\mu\\mathrm{m}$, and the overlap amount is $\\max(0, r_i + r_j - d_{ij})$ in $\\mu\\mathrm{m}$. The energy $E$ thus has units of picoNewton-micrometer, which equals attojoules ($\\mathrm{aJ}$). The invariant requires $E \\ge 0$ in $\\mathrm{aJ}$.\n\n3. Containment invariant: All agents must lie entirely within the domain, meaning $0 \\le x_i \\le L_x$ and $0 \\le y_i \\le L_y$ for all $i$.\n\nFundamental base and realism justification: The occupied area fraction bound arises because the union area of disks cannot exceed the finite area of the domain, so $0 \\le \\phi \\le 1$, and a model-imposed upper bound $\\phi_{\\max} \\le 1$ encodes experimentally observed crowding limits. The energy positivity follows because each term is a nonnegative constant times a square of a nonnegative overlap, ensuring $E \\ge 0$ by construction. Containment follows from the definition of the domain as the physical confining region.\n\nTask: Write a complete, runnable program that performs property-based tests. For each parameter set in the test suite below, generate $M$ random agent states using deterministic pseudorandom seeds and check the three invariants above for each state. A parameter set passes if and only if all generated states satisfy all three invariants. Use the following deterministic randomization protocol: for case index $c$ (starting at $0$) and state index $s$ (starting at $0$), set the seed to $S_0 + 1000\\,c + s$, where $S_0 = 12345$.\n\nRandom state generation details:\n- Positions $(x_i,y_i)$ are independently sampled uniformly from $[0,L_x] \\times [0,L_y]$ in $\\mu\\mathrm{m}$.\n- Radii $r_i$ are sampled from either a uniform distribution on $[r_{\\min}, r_{\\max}]$ or a lognormal distribution with parameters $(\\mu,\\sigma)$, then truncated to $[r_{\\min}, r_{\\max}]$. For the lognormal case, draw $z \\sim \\mathcal{N}(\\mu,\\sigma^2)$ and set $r = \\exp(z)$ in $\\mu\\mathrm{m}$, followed by truncation to the specified interval.\n\nNumerical specification:\n- Compute the packing fraction $\\phi$ by rasterization on a square grid of spacing $\\Delta$ in $\\mu\\mathrm{m}$, using the point-in-disk test at grid points.\n- Check energy positivity in attojoules ($\\mathrm{aJ}$).\n- Use a numerical tolerance of $\\varepsilon = 10^{-3}$ for the packing fraction check, i.e., accept $\\phi \\le \\phi_{\\max} + \\varepsilon$ to accommodate discretization error.\n- All angles are irrelevant in this two-dimensional circular agent model.\n- All percentage-like quantities must be handled as decimal fractions (for example, a packing bound of $0.70$ is a fraction, not a percentage sign).\n\nTest suite (five cases):\nCase $1$:\n- $L_x = 100$, $L_y = 100$ $\\mu\\mathrm{m}$, $N = 50$, radii uniform on $[1.5, 2.5]$ $\\mu\\mathrm{m}$, $\\phi_{\\max} = 0.70$, $k = 50$ $\\mathrm{pN}/\\mu\\mathrm{m}$, grid spacing $\\Delta = 1.0$ $\\mu\\mathrm{m}$, $M = 32$ states.\n\nCase $2$ (boundary condition of zero agents):\n- $L_x = 50$, $L_y = 50$ $\\mu\\mathrm{m}$, $N = 0$, radii uniform on $[1.0, 1.0]$ $\\mu\\mathrm{m}$, $\\phi_{\\max} = 1.00$, $k = 50$ $\\mathrm{pN}/\\mu\\mathrm{m}$, grid spacing $\\Delta = 1.0$ $\\mu\\mathrm{m}$, $M = 8$ states.\n\nCase $3$ (small domain with many small agents):\n- $L_x = 10$, $L_y = 10$ $\\mu\\mathrm{m}$, $N = 30$, radii uniform on $[0.5, 0.6]$ $\\mu\\mathrm{m}$, $\\phi_{\\max} = 0.60$, $k = 80$ $\\mathrm{pN}/\\mu\\mathrm{m}$, grid spacing $\\Delta = 0.5$ $\\mu\\mathrm{m}$, $M = 32$ states.\n\nCase $4$ (moderate domain with moderate radii):\n- $L_x = 20$, $L_y = 20$ $\\mu\\mathrm{m}$, $N = 40$, radii uniform on $[0.8, 1.2]$ $\\mu\\mathrm{m}$, $\\phi_{\\max} = 0.35$, $k = 60$ $\\mathrm{pN}/\\mu\\mathrm{m}$, grid spacing $\\Delta = 1.0$ $\\mu\\mathrm{m}$, $M = 32$ states.\n\nCase $5$ (lognormal radii, truncated):\n- $L_x = 100$, $L_y = 100$ $\\mu\\mathrm{m}$, $N = 120$, radii lognormal with $(\\mu, \\sigma) = (\\ln(1.5), 0.2)$, then truncated to $[0.5, 3.0]$ $\\mu\\mathrm{m}$, $\\phi_{\\max} = 0.50$, $k = 40$ $\\mathrm{pN}/\\mu\\mathrm{m}$, grid spacing $\\Delta = 2.0$ $\\mu\\mathrm{m}$, $M = 32$ states.\n\nFinal output specification:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one case and must be a boolean indicating whether all $M$ states in that case satisfy all three invariants. For example, the output should look like $[r_1,r_2,r_3,r_4,r_5]$ with each $r_i$ equal to either $\\mathrm{True}$ or $\\mathrm{False}$.",
            "solution": "The user has provided a valid, well-posed problem. The task is to write a program that performs property-based testing on an Agent-Based Model (ABM) of a cellular population. For several test cases, a number of random system states are generated, and for each state, three model invariants are checked. A test case passes if and only if all its generated states satisfy all three invariants. The process is deterministic, relying on a specified pseudorandom seeding protocol.\n\nThe three invariants to be tested are:\n1.  **Containment Invariant**: Each agent's center $(x_i, y_i)$ must remain within the rectangular domain, i.e., $0 \\le x_i \\le L_x$ and $0 \\le y_i \\le L_y$.\n2.  **Energy Positivity Invariant**: The total pairwise mechanical overlap energy, $E$, must be non-negative. The energy is defined as:\n    $$\n    E = \\sum_{1 \\le i < j \\le N} \\frac{k}{2}\\,\\left[\\max\\!\\left(0,\\, r_i + r_j - d_{ij}\\right)\\right]^2 \\ge 0\n    $$\n    where $k > 0$ is a stiffness constant, $r_i$ and $r_j$ are the radii of agents $i$ and $j$, and $d_{ij}$ is the Euclidean distance between their centers.\n3.  **Packing Fraction Invariant**: The area fraction occupied by the agents, $\\phi$, must not exceed a specified maximum, $\\phi_{\\max}$. The condition is $\\phi \\le \\phi_{\\max} + \\varepsilon$, where $\\varepsilon=10^{-3}$ is a numerical tolerance.\n\nThe core of the solution is to implement a procedure that, for each test case, iterates through a specified number of states, $M$. For each state, it generates agent positions and radii according to the given random distributions and then executes the three checks.\n\n### Algorithmic Design and Implementation\n\nThe overall program is structured to process a list of test cases. For each case, a master loop runs $M$ times to generate and check each random state.\n\n**State Generation**\nFor each state, agent properties are generated following a deterministic protocol. The random number generator is seeded with $S_0 + 1000c + s$, where $S_0 = 12345$, $c$ is the zero-indexed case number, and $s$ is the zero-indexed state number.\n-   Agent center positions $(x_i, y_i)$ for $i=1, \\dots, N$ are drawn from a continuous uniform distribution over the domain $[0, L_x] \\times [0, L_y]$.\n-   Agent radii $r_i$ are drawn from either a uniform or a truncated lognormal distribution, as specified per test case.\n\n**Invariant Verification**\nThree functions are designed to check the invariants for a given state.\n\n1.  **Containment Check**: This function verifies that for all agents $i$, the center coordinates satisfy $0 \\le x_i \\le L_x$ and $0 \\le y_i \\le L_y$. As the state generation protocol samples positions from precisely this interval, this invariant is expected to hold by construction. The check serves as a validation of the position generation logic.\n\n2.  **Energy Positivity Check**: This function calculates the total energy $E$. The implementation involves a double loop over all unique pairs of agents $(i, j)$ with $i < j$. For each pair, the distance $d_{ij}$ between their centers is computed. The overlap is $\\delta_{ij} = \\max(0, r_i + r_j - d_{ij})$. The energy contribution from this pair is $\\frac{k}{2} \\delta_{ij}^2$. These contributions are summed to obtain the total energy $E$. The invariant $E \\ge 0$ must hold. Since $k>0$ and $\\delta_{ij}^2 \\ge 0$, every term in the sum is non-negative, and thus $E$ is guaranteed to be non-negative mathematically. This check validates the correct implementation of the energy formula, guarding against potential floating-point artifacts or programming errors.\n\n3.  **Packing Fraction Check**: This is the only check not guaranteed by construction and thus represents the principal test of the system's physical properties. The packing fraction $\\phi$ is computed via a rasterization method:\n    -   A two-dimensional grid of points is established over the domain $[0, L_x] \\times [0, L_y]$ with a uniform spacing of $\\Delta$. The number of grid points along each axis is $N_x = \\lfloor L_x/\\Delta \\rfloor + 1$ and $N_y = \\lfloor L_y/\\Delta \\rfloor + 1$. The total number of grid points is $N_{\\text{grid}} = N_x \\times N_y$.\n    -   A boolean mask, representing the grid, is initialized to `False`.\n    -   For each agent $i$ with center $(x_i, y_i)$ and radius $r_i$, all grid points $(g_x, g_y)$ satisfying the point-in-disk condition $(g_x - x_i)^2 + (g_y - y_i)^2 \\le r_i^2$ are identified. The corresponding entries in the boolean mask are set to `True`. This is done cumulatively for all agents using a logical OR operation.\n    -   The number of occupied points, $N_{\\text{occ}}$, is the total count of `True` values in the final mask.\n    -   The packing fraction is calculated as $\\phi = N_{\\text{occ}} / N_{\\text{grid}}$.\n    -   Finally, the condition $\\phi \\le \\phi_{\\max} + \\varepsilon$ is verified.\n\nA test case is deemed to have passed (result: `True`) only if every one of its $M$ generated states passes all three invariant checks. If any state fails any check, the evaluation for that case stops, and the result is `False`. The boolean outcomes for all test cases are then compiled into a list for the final output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other non-standard libraries are used.\n\ndef solve():\n    \"\"\"\n    Main function to run the property-based tests for the ABM of\n    a cellular population.\n    \"\"\"\n    # Global constants defined in the problem\n    S0 = 12345\n    EPSILON = 1e-3\n\n    def generate_state(params, seed):\n        \"\"\"\n        Generates a single random state (agent positions and radii)\n        for a given set of parameters and a seed.\n        \"\"\"\n        np.random.seed(seed)\n        \n        N = params['N']\n        Lx, Ly = params['Lx'], params['Ly']\n        r_dist = params['r_dist']\n        \n        # Generate positions using a uniform distribution over [0, L] x [0, L]\n        # np.random.uniform generates from [low, high), which satisfies the\n        # containment invariant 0 <= pos <= L.\n        positions = np.random.uniform(low=[0.0, 0.0], high=[Lx, Ly], size=(N, 2))\n        \n        # Generate radii based on the specified distribution\n        if N == 0:\n            radii = np.array([])\n        elif r_dist['type'] == 'uniform':\n            r_min, r_max = r_dist['params']\n            radii = np.random.uniform(r_min, r_max, size=N)\n        elif r_dist['type'] == 'lognormal_truncated':\n            mu, sigma, r_min, r_max = r_dist['params']\n            # Draw z from N(mu, sigma^2)\n            z = np.random.normal(loc=mu, scale=sigma, size=N)\n            # Radii are exp(z), then truncated (clipped)\n            radii = np.exp(z)\n            radii = np.clip(radii, r_min, r_max)\n        \n        return positions, radii\n\n    def check_containment(positions, Lx, Ly):\n        \"\"\"Checks the containment invariant.\"\"\"\n        if positions.shape[0] == 0:\n            return True\n        return np.all((positions >= 0) & (positions <= [Lx, Ly]))\n\n    def check_energy_positivity(positions, radii, k):\n        \"\"\"Checks the energy positivity invariant.\"\"\"\n        N = positions.shape[0]\n        if N < 2:\n            return True  # Energy is 0 for 0 or 1 agent\n        \n        energy = 0.0\n        for i in range(N):\n            for j in range(i + 1, N):\n                dist_sq = np.sum((positions[i] - positions[j])**2)\n                d_ij = np.sqrt(dist_sq)\n                overlap = radii[i] + radii[j] - d_ij\n                if overlap > 0:\n                    energy += (k / 2.0) * (overlap**2)\n        \n        return energy >= 0.0\n\n    def check_packing_fraction(positions, radii, Lx, Ly, delta, phi_max):\n        \"\"\"Checks the packing fraction invariant using rasterization.\"\"\"\n        N = positions.shape[0]\n        if N == 0:\n            return 0.0 <= phi_max + EPSILON\n\n        nx = int(np.floor(Lx / delta)) + 1\n        ny = int(np.floor(Ly / delta)) + 1\n        \n        x_coords = np.linspace(0, Lx, nx)\n        y_coords = np.linspace(0, Ly, ny)\n        grid_x, grid_y = np.meshgrid(x_coords, y_coords)\n        \n        total_points = grid_x.size\n        if total_points == 0:\n            return (1.0 if N > 0 else 0.0) <= phi_max + EPSILON\n\n        occupied_mask = np.zeros_like(grid_x, dtype=bool)\n\n        for i in range(N):\n            center_x, center_y = positions[i]\n            r_sq = radii[i]**2\n            dist_sq = (grid_x - center_x)**2 + (grid_y - center_y)**2\n            occupied_mask |= (dist_sq <= r_sq)\n            \n        occupied_points = np.sum(occupied_mask)\n        phi = occupied_points / total_points\n        \n        return phi <= phi_max + EPSILON\n\n    # Test suite definition\n    test_cases = [\n        {'Lx': 100, 'Ly': 100, 'N': 50, 'r_dist': {'type': 'uniform', 'params': (1.5, 2.5)}, 'phi_max': 0.70, 'k': 50, 'delta': 1.0, 'M': 32},\n        {'Lx': 50, 'Ly': 50, 'N': 0, 'r_dist': {'type': 'uniform', 'params': (1.0, 1.0)}, 'phi_max': 1.00, 'k': 50, 'delta': 1.0, 'M': 8},\n        {'Lx': 10, 'Ly': 10, 'N': 30, 'r_dist': {'type': 'uniform', 'params': (0.5, 0.6)}, 'phi_max': 0.60, 'k': 80, 'delta': 0.5, 'M': 32},\n        {'Lx': 20, 'Ly': 20, 'N': 40, 'r_dist': {'type': 'uniform', 'params': (0.8, 1.2)}, 'phi_max': 0.35, 'k': 60, 'delta': 1.0, 'M': 32},\n        {'Lx': 100, 'Ly': 100, 'N': 120, 'r_dist': {'type': 'lognormal_truncated', 'params': (np.log(1.5), 0.2, 0.5, 3.0)}, 'phi_max': 0.50, 'k': 40, 'delta': 2.0, 'M': 32},\n    ]\n\n    results = []\n    \n    for c, params in enumerate(test_cases):\n        case_passed = True\n        for s in range(params['M']):\n            seed = S0 + 1000 * c + s\n            positions, radii = generate_state(params, seed)\n            \n            # Check invariants for the generated state\n            inv1_ok = check_containment(positions, params['Lx'], params['Ly'])\n            inv2_ok = check_energy_positivity(positions, radii, params['k'])\n            inv3_ok = check_packing_fraction(positions, radii, params['Lx'], params['Ly'], params['delta'], params['phi_max'])\n            \n            # If any invariant fails, the case fails, and we can break early.\n            if not (inv1_ok and inv2_ok and inv3_ok):\n                case_passed = False\n                break\n        \n        results.append(case_passed)\n        \n    # Format and print the final output exactly as specified.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Often in systems biology, multiple competing hypotheses can plausibly explain the available data, and a key role of modeling is to guide experiments that can best distinguish between them. This advanced practice puts you in the role of an experimental designer, tasked with discriminating between two different models of cellular activation in a quorum sensing system. You will apply principles from information theory to determine which minimally invasive perturbation—adjusting signal production, degradation, or the producer fraction—maximizes the expected information gain, providing a clear path for a targeted future experiment. ",
            "id": "3905488",
            "problem": "Consider a population of $N$ cells that communicate via Quorum Sensing (QS). Each cell produces an autoinducer signal at a per-cell rate $s_0$ and a fraction $f$ of cells are producers. The signal decays at an effective rate $\\gamma$. Assume a mean-field approximation so that the steady-state concentration $c$ experienced by each cell is homogeneous and given by $c = s_0 f N / \\gamma$ in dimensionless units. A cell activates when its local signal exceeds its threshold. We consider two competing activation models that differ in how threshold crossing occurs: Model $\\mathcal{M}_\\mathrm{A}$ assumes a sharp threshold with Gaussian concentration fluctuations, and Model $\\mathcal{M}_\\mathrm{B}$ assumes heterogeneous thresholds following a lognormal distribution with negligible measurement noise. You will design minimally invasive perturbations to system parameters and quantify which perturbation yields maximal expected information gain about which model better explains the activation statistics.\n\nFoundational modeling assumptions to start from:\n- Agent-Based Model (ABM) of identical cells under mean-field signal concentration $c$.\n- For Model $\\mathcal{M}_\\mathrm{A}$, the local concentration at a cell is a Gaussian random variable with mean $c$ and standard deviation $\\sigma$, and a cell activates if the random concentration exceeds its threshold $\\Theta$.\n- For Model $\\mathcal{M}_\\mathrm{B}$, thresholds are independently lognormally distributed with parameters $(\\mu,\\tau)$ so that $\\ln(\\Theta)$ is Gaussian with mean $\\mu$ and standard deviation $\\tau$, and activation occurs if the deterministic concentration $c$ exceeds the random threshold.\n- In either case, the number of activated cells $K$ out of $N$ is modeled as a Binomial random variable with parameter $p$, where $p$ is the per-cell activation probability implied by the respective model.\n\nPerturbations and minimal invasiveness:\n- You may apply one of the following perturbations $\\pi$ to the system, subject to a fractional budget $\\epsilon$:\n    1. $\\pi_0$: No perturbation (baseline).\n    2. $\\pi_1$: Reduce production rate to $s_0' = s_0 (1 - \\epsilon)$.\n    3. $\\pi_2$: Increase decay rate to $\\gamma' = \\gamma (1 + \\epsilon)$.\n    4. $\\pi_3$: Reduce producer fraction to $f' = f (1 - \\epsilon)$.\n- These perturbations adjust $c$ according to the mean-field expression $c = s_0 f N / \\gamma$ while keeping all changes minimal as bounded by $\\epsilon$.\n\nYour tasks:\n1. Starting from the foundational assumptions above, derive the per-cell activation probabilities $p_\\mathrm{A}(c,\\Theta,\\sigma)$ and $p_\\mathrm{B}(c,\\mu,\\tau)$ for Model $\\mathcal{M}_\\mathrm{A}$ and Model $\\mathcal{M}_\\mathrm{B}$, respectively. Ensure the derivation explicitly uses basic probability definitions (e.g., cumulative distribution functions) and yields expressions that are valid for dimensionless $c>0$.\n2. For any fixed perturbation $\\pi$, compute the resulting $c(\\pi)$, then the corresponding $p_\\mathrm{A}$ and $p_\\mathrm{B}$, and finally the predictive distributions for the activation count $K$ under each model: $\\mathrm{Binomial}(N, p_\\mathrm{A})$ and $\\mathrm{Binomial}(N, p_\\mathrm{B})$.\n3. Define the expected information gain for a given perturbation $\\pi$ as the Kullback–Leibler divergence (KL) from the prediction under Model $\\mathcal{M}_\\mathrm{A}$ to the prediction under Model $\\mathcal{M}_\\mathrm{B}$, i.e., $\\mathrm{KL}(\\mathrm{Binomial}(N, p_\\mathrm{A}) \\parallel \\mathrm{Binomial}(N, p_\\mathrm{B}))$. Compute this divergence exactly as a discrete sum over the support of $K$ (integers from $0$ to $N$), in dimensionless units.\n4. Among $\\pi_0$, $\\pi_1$, $\\pi_2$, and $\\pi_3$, choose the perturbation that maximizes the expected information gain subject to the budget $\\epsilon$. Break ties by preferring the smallest index (i.e., $\\pi_0$ over $\\pi_1$, etc.).\n5. For each test case below, output a list containing four values: the maximal expected information gain, the index of the chosen perturbation (with $\\pi_0 \\mapsto 0$, $\\pi_1 \\mapsto 1$, $\\pi_2 \\mapsto 2$, $\\pi_3 \\mapsto 3$), the $p_\\mathrm{A}$ under the chosen perturbation, and the $p_\\mathrm{B}$ under the chosen perturbation. All outputs must be in dimensionless units.\n\nTest suite (each parameter set is given as $(N, s_0, f, \\gamma, \\Theta, \\sigma, \\mu, \\tau, \\epsilon)$):\n- Case $1$ (happy path near threshold): $(200, 1.0, 0.3, 60, 1.0, 0.15, 0.0, 0.25, 0.05)$.\n- Case $2$ (zero budget boundary): $(200, 1.0, 0.3, 60, 1.0, 0.15, 0.0, 0.25, 0.0)$.\n- Case $3$ (saturation edge case): $(1000, 1.0, 0.5, 80, 1.0, 0.2, 0.0, 0.3, 0.05)$.\n- Case $4$ (sub-threshold edge case): $(100, 0.4, 0.2, 80, 1.0, 0.12, 0.0, 0.4, 0.1)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result should itself be a comma-separated list in square brackets. For example, the output should look like $[ [x_1,y_1,z_1,w_1],[x_2,y_2,z_2,w_2],\\dots ]$ with all values being booleans, integers, floats, or lists of these fundamental types.",
            "solution": "The problem requires us to determine the optimal experimental perturbation for distinguishing between two competing models of cellular activation in a quorum sensing system. This is an exercise in model-based experimental design, where the goal is to maximize the expected information gain, quantified by the Kullback-Leibler (KL) divergence between the predictive distributions of the two models.\n\nThe validation of the problem statement has confirmed its scientific soundness, well-posedness, and objectivity. The models presented are standard, simplified representations used in synthetic biology, and the tasks are mathematically and algorithmically well-defined. We may therefore proceed with a full solution.\n\nOur approach is structured as follows:\n1.  Derive the per-cell activation probabilities for each model as a function of the system parameters.\n2.  Formulate the information gain metric, the KL divergence, and simplify it for the binomial distributions involved.\n3.  Design an algorithm to compute the information gain for each possible perturbation and identify the maximum.\n\n### 1. Derivation of Activation Probabilities\n\nThe per-cell activation probability, denoted by $p$, is central to both models. The number of activated cells, $K$, in a population of size $N$ is then described by a binomial distribution, $K \\sim \\mathrm{Binomial}(N, p)$. We derive $p$ for each model, $\\mathcal{M}_\\mathrm{A}$ and $\\mathcal{M}_\\mathrm{B}$.\n\n**Model $\\mathcal{M}_\\mathrm{A}$: Sharp Threshold with Gaussian Concentration Fluctuations**\n\nIn this model, the threshold $\\Theta$ is a fixed, sharp value for all cells. However, the local signal concentration experienced by a cell, $C_A$, is a Gaussian random variable with mean $c = s_0 f N / \\gamma$ and standard deviation $\\sigma$. Thus, $C_A \\sim \\mathcal{N}(c, \\sigma^2)$.\n\nA cell activates if its local concentration exceeds the threshold, i.e., $C_A > \\Theta$. The activation probability $p_\\mathrm{A}$ is therefore:\n$$p_\\mathrm{A}(c, \\Theta, \\sigma) = P(C_A > \\Theta)$$\nTo compute this probability, we standardize the random variable $C_A$. Let $Z$ be a standard normal random variable, $Z \\sim \\mathcal{N}(0, 1)$. We have $C_A = c + \\sigma Z$. The activation condition becomes:\n$$c + \\sigma Z > \\Theta \\implies Z > \\frac{\\Theta - c}{\\sigma}$$\nThe probability is given by the complementary cumulative distribution function (CCDF) of the standard normal distribution:\n$$p_\\mathrm{A} = P\\left(Z > \\frac{\\Theta - c}{\\sigma}\\right) = 1 - \\Phi\\left(\\frac{\\Theta - c}{\\sigma}\\right)$$\nwhere $\\Phi(\\cdot)$ is the cumulative distribution function (CDF) of the standard normal distribution. Due to the symmetry of the normal distribution, $\\Phi(-x) = 1 - \\Phi(x)$, this can be written as:\n$$p_\\mathrm{A}(c, \\Theta, \\sigma) = \\Phi\\left(\\frac{c - \\Theta}{\\sigma}\\right)$$\n\n**Model $\\mathcal{M}_\\mathrm{B}$: Heterogeneous Thresholds with Deterministic Concentration**\n\nIn this model, the signal concentration $c$ is deterministic and uniform for all cells. The source of stochasticity is the cell-to-cell variability in the activation threshold, $\\Theta_B$. The thresholds are assumed to follow a lognormal distribution, such that $\\ln(\\Theta_B)$ is a Gaussian random variable with mean $\\mu$ and standard deviation $\\tau$. Thus, $\\ln(\\Theta_B) \\sim \\mathcal{N}(\\mu, \\tau^2)$.\n\nA cell activates if the signal concentration exceeds its specific threshold, i.e., $c > \\Theta_B$. The problem assumes $c > 0$, and the lognormal distribution ensures $\\Theta_B > 0$, so we can take the natural logarithm of the inequality without changing its direction:\n$$\\ln(c) > \\ln(\\Theta_B)$$\nThe activation probability $p_\\mathrm{B}$ is the probability that a randomly chosen cell has a threshold satisfying this condition:\n$$p_\\mathrm{B}(c, \\mu, \\tau) = P(\\ln(c) > \\ln(\\Theta_B)) = P(\\ln(\\Theta_B) < \\ln(c))$$\nThis is precisely the CDF of the random variable $\\ln(\\Theta_B)$ evaluated at $\\ln(c)$. To compute this, we again standardize. Let $Y = \\ln(\\Theta_B)$, so $Y \\sim \\mathcal{N}(\\mu, \\tau^2)$. Let $Z \\sim \\mathcal{N}(0, 1)$. We have $Y = \\mu + \\tau Z$. The condition becomes:\n$$\\mu + \\tau Z < \\ln(c) \\implies Z < \\frac{\\ln(c) - \\mu}{\\tau}$$\nThe probability is given by the standard normal CDF:\n$$p_\\mathrm{B}(c, \\mu, \\tau) = \\Phi\\left(\\frac{\\ln(c) - \\mu}{\\tau}\\right)$$\n\n### 2. Information Gain and Perturbations\n\nThe objective is to select a perturbation that maximizes our ability to distinguish between Model $\\mathcal{M}_\\mathrm{A}$ and Model $\\mathcal{M}_\\mathrm{B}$. The expected information gain is quantified by the Kullback-Leibler (KL) divergence from the predictive distribution of Model $\\mathcal{M}_\\mathrm{A}$ to that of Model $\\mathcal{M}_\\mathrm{B}$.\n\nThe predictive distribution for the number of activated cells $K$ under Model $\\mathcal{M}_i$ (where $i \\in \\{\\mathrm{A}, \\mathrm{B}\\}$) is $\\mathrm{Binomial}(N, p_i)$. Let $P_A(K=k)$ and $P_B(K=k)$ be the probability mass functions (PMFs) for the two models. The KL divergence is defined as:\n$$D_{\\mathrm{KL}}(P_A \\parallel P_B) = \\sum_{k=0}^{N} P_A(K=k) \\ln\\left(\\frac{P_A(K=k)}{P_B(K=k)}\\right)$$\nFor binomial distributions, where $P_i(K=k) = \\binom{N}{k} p_i^k (1-p_i)^{N-k}$, this sum has a known closed-form solution. The KL divergence between two binomial distributions $\\mathrm{Binomial}(N, p_A)$ and $\\mathrm{Binomial}(N, p_B)$ is $N$ times the KL divergence between their underlying Bernoulli distributions with parameters $p_A$ and $p_B$:\n$$D_{\\mathrm{KL}}(\\mathrm{Bin}(N,p_A) \\parallel \\mathrm{Bin}(N,p_B)) = N \\cdot D_{\\mathrm{KL}}(\\mathrm{Bernoulli}(p_A) \\parallel \\mathrm{Bernoulli}(p_B))$$\n$$D_{\\mathrm{KL}} = N \\left[ p_A \\ln\\left(\\frac{p_A}{p_B}\\right) + (1-p_A) \\ln\\left(\\frac{1-p_A}{1-p_B}\\right) \\right]$$\nThis formula will be used for computation. The logarithm is the natural logarithm, and the result is a dimensionless quantity. We must consider four perturbations, $\\pi \\in \\{\\pi_0, \\pi_1, \\pi_2, \\pi_3\\}$, and calculate the resulting information gain for each. The perturbations modify the mean-field concentration, $c$, as follows:\n-   $\\pi_0$ (baseline): $c(\\pi_0) = \\frac{s_0 f N}{\\gamma}$\n-   $\\pi_1$ ($s_0 \\to s_0(1-\\epsilon)$): $c(\\pi_1) = \\frac{s_0(1-\\epsilon) f N}{\\gamma} = c(\\pi_0) (1-\\epsilon)$\n-   $\\pi_2$ ($\\gamma \\to \\gamma(1+\\epsilon)$): $c(\\pi_2) = \\frac{s_0 f N}{\\gamma(1+\\epsilon)} = \\frac{c(\\pi_0)}{1+\\epsilon}$\n-   $\\pi_3$ ($f \\to f(1-\\epsilon)$): $c(\\pi_3) = \\frac{s_0 f(1-\\epsilon) N}{\\gamma} = c(\\pi_0) (1-\\epsilon)$\n\n### 3. Algorithmic Solution\n\nFor each test case, we perform the following steps:\n1.  Initialize a variable `max_kl` to a value less than $0$ (e.g., $-1.0$) and `best_pert_idx` to an invalid index.\n2.  Iterate through the perturbation indices $i = 0, 1, 2, 3$.\n3.  For each index $i$, calculate the perturbed concentration $c(\\pi_i)$.\n4.  Using this $c(\\pi_i)$, compute the activation probabilities $p_A(\\pi_i)$ and $p_B(\\pi_i)$ using the derived formulas.\n5.  Calculate the KL divergence, $D_{\\mathrm{KL}}(\\pi_i)$, using the simplified formula. Special care must be taken for numerical stability if probabilities are near $0$ or $1$. Using a library function like `scipy.special.rel_entr` is advisable. The total KL divergence is $N \\times (\\mathrm{rel\\_entr}(p_A, p_B) + \\mathrm{rel\\_entr}(1-p_A, 1-p_B))$.\n6.  Compare the calculated $D_{\\mathrm{KL}}(\\pi_i)$ with `max_kl`. If $D_{\\mathrm{KL}}(\\pi_i) > \\text{max\\_kl}$, update `max_kl` to $D_{\\mathrm{KL}}(\\pi_i)$, set `best_pert_idx` to $i$, and store the corresponding $p_A$ and $p_B$. The use of a strict inequality `>` enforces the tie-breaking rule (preferring the smallest index), as the loop runs from $i=0$ to $3$.\n7.  After iterating through all perturbations, the stored values (`max_kl`, `best_pert_idx`, and the associated probabilities) constitute the result for the test case.\n\nThis procedure is repeated for all test cases specified in the problem. The final implementation is provided in the `<final_answer>` section.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.special import rel_entr\nimport sys\n\n# Ensure the Python version is as specified\nif sys.version_info.major != 3 or sys.version_info.minor != 12:\n    print(f\"Warning: This script is designed for Python 3.12. You are using {sys.version.split()[0]}.\")\n\ndef solve():\n    \"\"\"\n    Solves the problem of finding the optimal perturbation to maximize information gain\n    between two cellular activation models.\n    \"\"\"\n    # Test suite from the problem statement\n    # Format: (N, s0, f, gamma, Theta, sigma, mu, tau, epsilon)\n    test_cases = [\n        (200, 1.0, 0.3, 60, 1.0, 0.15, 0.0, 0.25, 0.05),\n        (200, 1.0, 0.3, 60, 1.0, 0.15, 0.0, 0.25, 0.0),\n        (1000, 1.0, 0.5, 80, 1.0, 0.2, 0.0, 0.3, 0.05),\n        (100, 0.4, 0.2, 80, 1.0, 0.12, 0.0, 0.4, 0.1),\n    ]\n\n    all_results = []\n    \n    for params in test_cases:\n        N, s0, f, gamma, Theta, sigma, mu, tau, epsilon = params\n        \n        max_kl = -1.0\n        best_pert_idx = -1\n        best_pa = -1.0\n        best_pb = -1.0\n\n        # Loop through the four perturbations (0: baseline, 1: s0, 2: gamma, 3: f)\n        for i in range(4):\n            # Apply perturbation\n            s0_p, f_p, gamma_p = s0, f, gamma\n            if i == 1:\n                s0_p = s0 * (1 - epsilon)\n            elif i == 2:\n                gamma_p = gamma * (1 + epsilon)\n            elif i == 3:\n                f_p = f * (1 - epsilon)\n\n            # Calculate perturbed mean-field concentration\n            # Ensure gamma_p is not zero\n            if gamma_p == 0:\n                c_p = np.inf\n            else:\n                c_p = s0_p * f_p * N / gamma_p\n\n            # Calculate activation probabilities for Model A and Model B\n            # Model A: p_A = Phi((c - Theta) / sigma)\n            pa = norm.cdf((c_p - Theta) / sigma)\n\n            # Model B: p_B = Phi((ln(c) - mu) / tau)\n            # Handle c_p <= 0 for the logarithm\n            if c_p > 0:\n                pb = norm.cdf((np.log(c_p) - mu) / tau)\n            else:\n                # If c_p is not positive, ln(c_p) is -inf, so the CDF is 0.\n                pb = 0.0\n\n            # Calculate KL divergence for N Bernoulli trials\n            # D_KL(Bin(pA) || Bin(pB)) = N * D_KL(Bernoulli(pA) || Bernoulli(pB))\n            # The KL divergence for two Bernoulli distributions is rel_entr(pA, pB) + rel_entr(1-pA, 1-pB)\n            # scipy.special.rel_entr(x, y) = x * log(x/y) handles edge cases like x=0 or y=0.\n            kl_bernoulli = rel_entr(pa, pb) + rel_entr(1 - pa, 1 - pb)\n            kl_total = N * kl_bernoulli\n\n            # Check for new maximum. Strict inequality handles the tie-breaking rule.\n            if kl_total > max_kl:\n                max_kl = kl_total\n                best_pert_idx = i\n                best_pa = pa\n                best_pb = pb\n        \n        all_results.append([max_kl, best_pert_idx, best_pa, best_pb])\n\n    # Format the final output string exactly as specified\n    formatted_case_results = []\n    for res in all_results:\n        # res[0]=max_kl, res[1]=pert_idx, res[2]=p_A, res[3]=p_B\n        formatted_case_results.append(f\"[{res[0]},{res[1]},{res[2]},{res[3]}]\")\n    \n    print(f\"[{','.join(formatted_case_results)}]\")\n\nsolve()\n```"
        }
    ]
}