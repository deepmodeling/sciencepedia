{
    "hands_on_practices": [
        {
            "introduction": "This first exercise grounds us in the core mechanics of Bayesian parameter estimation. We will analyze a common scenario in synthetic biology—a gene circuit activation assay—modeled by a binomial process. By applying a conjugate Beta prior, you will derive the posterior distribution for the activation probability and compute a credible interval, directly experiencing the flow from prior belief to posterior knowledge. ",
            "id": "3907430",
            "problem": "A synthetic gene circuit is designed to switch a fluorescent reporter to the \"ON\" state upon activation of a transcription factor. In a microfluidic assay, $n=10$ independent microdroplets are each loaded with a single engineered cell and exposed to the inducer under identical conditions. The observed count of \"ON\" droplets is $k=2$. Let the unknown activation probability be $p \\in (0,1)$, and model the data as a sequence of independent Bernoulli trials, yielding a binomial likelihood for $k$ successes in $n$ trials. Adopt Jeffreys' prior for the binomial proportion, which is the Beta distribution with parameters $1/2$ and $1/2$. Using coherent Bayesian updating, form the posterior distribution for $p$.\n\nFrom this posterior, construct the equal-tailed $0.95$ credible interval for $p$. Separately, construct the $0.95$ exact frequentist confidence interval for $p$ based on the Clopper–Pearson method (Clopper–Pearson exact confidence interval (CP)) using the binomial model. Define the width of an interval as the upper endpoint minus the lower endpoint.\n\nCompute the ratio of the width of the Bayesian equal-tailed $0.95$ credible interval to the width of the Clopper–Pearson $0.95$ confidence interval for these data. Express your final answer as a single closed-form analytical expression involving the inverse of the regularized incomplete Beta function, and do not perform any numerical rounding.",
            "solution": "The problem is first subjected to validation.\n\n**Step 1: Extract Givens**\n- Number of independent trials (microdroplets): $n=10$.\n- Number of observed successes (\"ON\" droplets): $k=2$.\n- Unknown parameter (activation probability): $p \\in (0,1)$.\n- Likelihood model: Binomial, describing $k$ successes in $n$ trials.\n- Prior distribution: Jeffreys' prior for the binomial proportion, which is a Beta distribution with parameters $\\alpha_0 = \\frac{1}{2}$ and $\\beta_0 = \\frac{1}{2}$, i.e., $p \\sim \\text{Beta}(\\frac{1}{2}, \\frac{1}{2})$.\n- Desired Bayesian interval: Equal-tailed $0.95$ credible interval for $p$.\n- Desired frequentist interval: Exact $0.95$ Clopper–Pearson confidence interval for $p$.\n- Final calculation: The ratio of the width of the Bayesian credible interval to the width of the Clopper–Pearson confidence interval.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, as it uses standard and well-established statistical methods (Bayesian inference with a conjugate prior, Clopper-Pearson interval) to analyze a common experimental setup in synthetic biology (quantifying the success rate of a gene circuit). The problem is well-posed, objective, and self-contained, providing all necessary data ($n$, $k$), models (Binomial likelihood, Jeffreys' prior), and methods (equal-tailed credible interval, Clopper-Pearson interval) to arrive at a unique, meaningful analytical solution. There are no contradictions, ambiguities, or unrealistic assumptions.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\nThe solution consists of three main parts: first, a Bayesian analysis to determine the width of the credible interval; second, a frequentist analysis to determine the width of the Clopper-Pearson confidence interval; and third, the computation of the ratio of these two widths.\n\n**1. Bayesian Credible Interval**\n\nThe analysis begins by defining the components of Bayes' theorem.\nThe likelihood for observing $k$ successes in $n$ trials is given by the binomial probability mass function:\n$$ P(k|n, p) = \\binom{n}{k} p^k (1-p)^{n-k} $$\nAs a function of $p$, the likelihood is proportional to $p^k (1-p)^{n-k}$.\n\nThe prior distribution for $p$ is specified as Jeffreys' prior, which is the Beta distribution $\\text{Beta}(\\frac{1}{2}, \\frac{1}{2})$. Its probability density function (PDF) is:\n$$ \\pi(p) \\propto p^{\\frac{1}{2}-1} (1-p)^{\\frac{1}{2}-1} = p^{-\\frac{1}{2}} (1-p)^{-\\frac{1}{2}} $$\nThe posterior distribution for $p$ is proportional to the product of the likelihood and the prior:\n$$ \\pi(p|k,n) \\propto P(k|n,p) \\times \\pi(p) \\propto \\left[p^k (1-p)^{n-k}\\right] \\times \\left[p^{\\frac{1}{2}-1} (1-p)^{\\frac{1}{2}-1}\\right] $$\n$$ \\pi(p|k,n) \\propto p^{k+\\frac{1}{2}-1} (1-p)^{n-k+\\frac{1}{2}-1} $$\nThis is the kernel of a Beta distribution. Due to the conjugacy of the Beta prior with the binomial likelihood, the posterior distribution is also a Beta distribution with updated parameters:\n$$ \\alpha_{\\text{post}} = k + \\frac{1}{2} $$\n$$ \\beta_{\\text{post}} = n - k + \\frac{1}{2} $$\nSubstituting the given values $n=10$ and $k=2$:\n$$ \\alpha_{\\text{post}} = 2 + \\frac{1}{2} = \\frac{5}{2} $$\n$$ \\beta_{\\text{post}} = 10 - 2 + \\frac{1}{2} = 8 + \\frac{1}{2} = \\frac{17}{2} $$\nSo, the posterior distribution is $p|k=2, n=10 \\sim \\text{Beta}(\\frac{5}{2}, \\frac{17}{2})$.\n\nAn equal-tailed $0.95$ credible interval $[p_L, p_U]$ is defined such that $P(p < p_L) = 0.025$ and $P(p > p_U) = 0.025$. These probabilities are found using the cumulative distribution function (CDF) of the posterior Beta distribution, which is the regularized incomplete Beta function, denoted $I_x(\\alpha, \\beta)$. The endpoints are therefore the quantiles of the posterior distribution:\n$$ p_L = I^{-1}(0.025; \\alpha_{\\text{post}}, \\beta_{\\text{post}}) = I^{-1}(0.025; \\frac{5}{2}, \\frac{17}{2}) $$\n$$ p_U = I^{-1}(0.975; \\alpha_{\\text{post}}, \\beta_{\\text{post}}) = I^{-1}(0.975; \\frac{5}{2}, \\frac{17}{2}) $$\nThe width of the Bayesian credible interval, $W_B$, is the difference between the upper and lower endpoints:\n$$ W_B = p_U - p_L = I^{-1}(0.975; \\frac{5}{2}, \\frac{17}{2}) - I^{-1}(0.025; \\frac{5}{2}, \\frac{17}{2}) $$\n\n**2. Clopper-Pearson Confidence Interval**\n\nThe Clopper-Pearson $1-\\alpha$ confidence interval $[p_{L,CP}, p_{U,CP}]$ for a binomial proportion is constructed by inverting two one-sided binomial tests. For a $0.95$ confidence level, $\\alpha = 0.05$.\nThe lower endpoint, $p_{L,CP}$, is the value of $p$ for which the probability of observing $k$ or more successes is $\\frac{\\alpha}{2} = 0.025$:\n$$ P(X \\ge k | p=p_{L,CP}, n) = \\sum_{j=k}^{n} \\binom{n}{j} (p_{L,CP})^j (1-p_{L,CP})^{n-j} = \\frac{\\alpha}{2} $$\nThis cumulative probability is related to the regularized incomplete Beta function by the identity $P(X \\ge k) = I_p(k, n-k+1)$. Thus:\n$$ I_{p_{L,CP}}(k, n-k+1) = \\frac{\\alpha}{2} $$\nSolving for $p_{L,CP}$ and substituting $k=2$, $n=10$, and $\\frac{\\alpha}{2}=0.025$:\n$$ p_{L,CP} = I^{-1}(\\frac{\\alpha}{2}; k, n-k+1) = I^{-1}(0.025; 2, 10-2+1) = I^{-1}(0.025; 2, 9) $$\nThe upper endpoint, $p_{U,CP}$, is the value of $p$ for which the probability of observing $k$ or fewer successes is $\\frac{\\alpha}{2} = 0.025$:\n$$ P(X \\le k | p=p_{U,CP}, n) = \\sum_{j=0}^{k} \\binom{n}{j} (p_{U,CP})^j (1-p_{U,CP})^{n-j} = \\frac{\\alpha}{2} $$\nThis cumulative probability is related to the regularized incomplete Beta function by the identity $P(X \\le k) = I_{1-p}(n-k, k+1)$. A more direct formulation for the quantile function is derived from $P(X \\ge k+1 | p=p_{U,CP}) = 1-\\frac{\\alpha}{2}$, which gives $I_{p_{U,CP}}(k+1, n-k) = 1-\\frac{\\alpha}{2}$. Solving for $p_{U,CP}$:\n$$ p_{U,CP} = I^{-1}(1-\\frac{\\alpha}{2}; k+1, n-k) = I^{-1}(0.975; 2+1, 10-2) = I^{-1}(0.975; 3, 8) $$\nThe width of the Clopper-Pearson confidence interval, $W_{CP}$, is the difference between its endpoints:\n$$ W_{CP} = p_{U,CP} - p_{L,CP} = I^{-1}(0.975; 3, 8) - I^{-1}(0.025; 2, 9) $$\n\n**3. Ratio of the Widths**\n\nThe final step is to compute the ratio of the width of the Bayesian credible interval, $W_B$, to the width of the Clopper-Pearson confidence interval, $W_{CP}$.\n$$ \\text{Ratio} = \\frac{W_B}{W_{CP}} = \\frac{I^{-1}(0.975; \\frac{5}{2}, \\frac{17}{2}) - I^{-1}(0.025; \\frac{5}{2}, \\frac{17}{2})}{I^{-1}(0.975; 3, 8) - I^{-1}(0.025; 2, 9)} $$\nThis is the final closed-form analytical expression as required by the problem statement.",
            "answer": "$$\n\\boxed{\\frac{I^{-1}(0.975; \\frac{5}{2}, \\frac{17}{2}) - I^{-1}(0.025; \\frac{5}{2}, \\frac{17}{2})}{I^{-1}(0.975; 3, 8) - I^{-1}(0.025; 2, 9)}}\n$$"
        },
        {
            "introduction": "Moving beyond basic updates, this problem confronts a common challenge in modeling: non-identifiability, where the data alone cannot uniquely determine all model parameters. You will explore a scenario where a measurement only constrains the sum of two parameters, rendering them individually unidentifiable by the likelihood. This exercise demonstrates how an informative prior can regularize the problem, leading to a well-defined posterior and showcasing the crucial role of prior knowledge in resolving ambiguity. ",
            "id": "2374096",
            "problem": "In a computational engineering calibration task, a lumped model predicts a measured scalar response as the sum of two component parameters. A single experiment produces one measurement modeled as\n$$\ny = \\theta_1 + \\theta_2 + \\varepsilon,\\quad \\varepsilon \\sim \\mathcal{N}(0,\\sigma^2),\n$$\nwith observed value $y_{\\text{obs}} = 10$ and known noise variance $\\sigma^2 = 1$. The parameters $\\theta_1$ and $\\theta_2$ represent distinct component contributions that are of engineering interest individually. Because only the sum is observed in this single experiment, the likelihood is non-identifying for $(\\theta_1,\\theta_2)$. To incorporate prior engineering knowledge, use an improper flat prior for $\\theta_1$ (i.e., $p(\\theta_1) \\propto 1$ over $\\mathbb{R}$) and a strongly informative Gaussian prior for $\\theta_2$,\n$$\n\\theta_2 \\sim \\mathcal{N}(\\mu_2,\\tau_2^2),\\quad \\mu_2 = 6,\\ \\tau_2^2 = 0.04.\n$$\nConsider the posterior inference that results from this model.\n\nWhich of the following statements are correct?\n\nA. The marginal posterior for $\\theta_1$ is Gaussian with mean $y_{\\text{obs}} - \\mu_2$ and variance $\\sigma^2 + \\tau_2^2$; numerically, it is $\\mathcal{N}(4,\\ 1.04)$.\n\nB. If the prior on $\\theta_2$ were also improper flat (so both $\\theta_1$ and $\\theta_2$ had flat priors), then the posterior for $\\theta_1$ given $y_{\\text{obs}}$ would be improper.\n\nC. Decreasing $\\tau_2^2$ strictly decreases the marginal posterior variance of $\\theta_1$, and this variance cannot be reduced below $\\sigma^2$.\n\nD. Under the stated model, the maximum likelihood estimate of $\\theta_1$ equals $4$.",
            "solution": "The problem requires the derivation and analysis of the posterior distribution for the parameters $\\theta_1$ and $\\theta_2$ in a simple linear model, where the likelihood is non-identifying. The use of prior information is essential to obtain a well-defined posterior. We will proceed by first deriving the marginal posterior distribution for the parameter of interest, $\\theta_1$, and then evaluating each statement.\n\nThe model is defined by the following components:\nThe likelihood function is derived from the measurement model $y = \\theta_1 + \\theta_2 + \\varepsilon$, where the error term $\\varepsilon$ follows a normal distribution $\\mathcal{N}(0, \\sigma^2)$. Given a single observation $y_{\\text{obs}}$, the likelihood of the parameters $(\\theta_1, \\theta_2)$ is:\n$$p(y_{\\text{obs}} | \\theta_1, \\theta_2, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_{\\text{obs}} - \\theta_1 - \\theta_2)^2}{2\\sigma^2}\\right)$$\nThe numerical values are given as $y_{\\text{obs}} = 10$ and $\\sigma^2 = 1$.\n\nThe prior distributions for the parameters are given as:\n- For $\\theta_1$: an improper uniform (flat) prior, $p(\\theta_1) \\propto 1$ for $\\theta_1 \\in \\mathbb{R}$.\n- For $\\theta_2$: a proper normal (Gaussian) prior, $\\theta_2 \\sim \\mathcal{N}(\\mu_2, \\tau_2^2)$, with $p(\\theta_2) = \\frac{1}{\\sqrt{2\\pi\\tau_2^2}} \\exp\\left(-\\frac{(\\theta_2 - \\mu_2)^2}{2\\tau_2^2}\\right)$.\nThe numerical values for the prior on $\\theta_2$ are $\\mu_2 = 6$ and $\\tau_2^2 = 0.04$.\n\nAssuming prior independence between $\\theta_1$ and $\\theta_2$, the joint prior is $p(\\theta_1, \\theta_2) = p(\\theta_1)p(\\theta_2)$.\n\nAccording to Bayes' theorem, the joint posterior distribution is proportional to the product of the likelihood and the joint prior:\n$$p(\\theta_1, \\theta_2 | y_{\\text{obs}}) \\propto p(y_{\\text{obs}} | \\theta_1, \\theta_2) p(\\theta_1, \\theta_2) \\propto \\exp\\left(-\\frac{(y_{\\text{obs}} - \\theta_1 - \\theta_2)^2}{2\\sigma^2}\\right) \\exp\\left(-\\frac{(\\theta_2 - \\mu_2)^2}{2\\tau_2^2}\\right)$$\nTo find the marginal posterior for $\\theta_1$, we must integrate the joint posterior over all possible values of $\\theta_2$:\n$$p(\\theta_1 | y_{\\text{obs}}) = \\int_{-\\infty}^{\\infty} p(\\theta_1, \\theta_2 | y_{\\text{obs}}) d\\theta_2$$\nA more direct method is to first determine the effective likelihood for $\\theta_1$, denoted $p(y_{\\text{obs}} | \\theta_1)$. From the model $y = \\theta_1 + \\theta_2 + \\varepsilon$, we can write $y | \\theta_1$ as a random variable determined by the distributions of $\\theta_2$ and $\\varepsilon$. Given $\\theta_1$, the quantity $y$ is the sum of a constant $\\theta_1$ and two independent random variables: $\\theta_2 \\sim \\mathcal{N}(\\mu_2, \\tau_2^2)$ and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$. The sum of independent normal random variables is also a normal random variable.\nThe expectation of $y$ given $\\theta_1$ is:\n$$E[y | \\theta_1] = E[\\theta_1 + \\theta_2 + \\varepsilon | \\theta_1] = \\theta_1 + E[\\theta_2] + E[\\varepsilon] = \\theta_1 + \\mu_2 + 0 = \\theta_1 + \\mu_2$$\nThe variance of $y$ given $\\theta_1$ is:\n$$\\text{Var}(y | \\theta_1) = \\text{Var}(\\theta_1 + \\theta_2 + \\varepsilon | \\theta_1) = \\text{Var}(\\theta_2) + \\text{Var}(\\varepsilon) = \\tau_2^2 + \\sigma^2$$\nTherefore, the distribution of $y$ conditioned on $\\theta_1$ is normal:\n$$y | \\theta_1 \\sim \\mathcal{N}(\\theta_1 + \\mu_2, \\sigma^2 + \\tau_2^2)$$\nThis gives the effective likelihood for $\\theta_1$: $p(y_{\\text{obs}}|\\theta_1) = \\mathcal{N}(y_{\\text{obs}} | \\theta_1 + \\mu_2, \\sigma^2 + \\tau_2^2)$.\n\nNow, we apply Bayes' theorem to find the posterior for $\\theta_1$:\n$$p(\\theta_1 | y_{\\text{obs}}) \\propto p(y_{\\text{obs}} | \\theta_1) p(\\theta_1)$$\nWith the flat prior $p(\\theta_1) \\propto 1$, the posterior is proportional to the likelihood:\n$$p(\\theta_1 | y_{\\text{obs}}) \\propto \\exp\\left(-\\frac{(y_{\\text{obs}} - (\\theta_1 + \\mu_2))^2}{2(\\sigma^2 + \\tau_2^2)}\\right) = \\exp\\left(-\\frac{((y_{\\text{obs}} - \\mu_2) - \\theta_1)^2}{2(\\sigma^2 + \\tau_2^2)}\\right)$$\nThis is the kernel of a normal distribution for $\\theta_1$. By inspection, the posterior distribution for $\\theta_1$ is:\n$$\\theta_1 | y_{\\text{obs}} \\sim \\mathcal{N}(\\; y_{\\text{obs}} - \\mu_2, \\; \\sigma^2 + \\tau_2^2 \\;)$$\nWe can now evaluate each statement.\n\nA. The marginal posterior for $\\theta_1$ is Gaussian with mean $y_{\\text{obs}} - \\mu_2$ and variance $\\sigma^2 + \\tau_2^2$; numerically, it is $\\mathcal{N}(4, 1.04)$.\n\nBased on our derivation, the marginal posterior for $\\theta_1$ is indeed a normal distribution with mean $y_{\\text{obs}} - \\mu_2$ and variance $\\sigma^2 + \\tau_2^2$.\nLet's substitute the given numerical values:\n- Mean: $E[\\theta_1 | y_{\\text{obs}}] = y_{\\text{obs}} - \\mu_2 = 10 - 6 = 4$.\n- Variance: $\\text{Var}(\\theta_1 | y_{\\text{obs}}) = \\sigma^2 + \\tau_2^2 = 1 + 0.04 = 1.04$.\nThe resulting posterior is $\\mathcal{N}(4, 1.04)$. The statement is entirely consistent with our derivation.\nVerdict: **Correct**.\n\nB. If the prior on $\\theta_2$ were also improper flat (so both $\\theta_1$ and $\\theta_2$ had flat priors), then the posterior for $\\theta_1$ given $y_{\\text{obs}}$ would be improper.\n\nLet us assume $p(\\theta_1) \\propto 1$ and $p(\\theta_2) \\propto 1$. The joint prior is $p(\\theta_1, \\theta_2) \\propto 1$.\nThe joint posterior becomes:\n$$p(\\theta_1, \\theta_2 | y_{\\text{obs}}) \\propto p(y_{\\text{obs}} | \\theta_1, \\theta_2) p(\\theta_1, \\theta_2) \\propto \\exp\\left(-\\frac{(y_{\\text{obs}} - \\theta_1 - \\theta_2)^2}{2\\sigma^2}\\right)$$\nTo find the marginal posterior for $\\theta_1$, we integrate over $\\theta_2$:\n$$p(\\theta_1 | y_{\\text{obs}}) = \\int_{-\\infty}^{\\infty} p(\\theta_1, \\theta_2 | y_{\\text{obs}}) d\\theta_2 \\propto \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{(y_{\\text{obs}} - \\theta_1 - \\theta_2)^2}{2\\sigma^2}\\right) d\\theta_2$$\nLet $z = \\theta_2$. The integrand is a Gaussian function of $z$ with mean $(y_{\\text{obs}} - \\theta_1)$ and variance related parameter $\\sigma^2$. The definite integral of an unnormalized Gaussian function over $\\mathbb{R}$ is a constant. Specifically, $\\int_{-\\infty}^{\\infty} e^{-(x-c)^2/(2s^2)} dx = \\sqrt{2\\pi s^2}$.\nIn our case, the integral evaluates to $\\sqrt{2\\pi\\sigma^2}$, which is a positive constant that does not depend on $\\theta_1$.\nThus, $p(\\theta_1 | y_{\\text{obs}}) \\propto \\text{constant}$ for all $\\theta_1 \\in \\mathbb{R}$.\nAn unnormalized density that is constant over the entire real line is an improper distribution, as its integral $\\int_{-\\infty}^{\\infty} c \\, d\\theta_1$ diverges for any constant $c > 0$. Therefore, the marginal posterior for $\\theta_1$ would be improper.\nVerdict: **Correct**.\n\nC. Decreasing $\\tau_2^2$ strictly decreases the marginal posterior variance of $\\theta_1$, and this variance cannot be reduced below $\\sigma^2$.\n\nFrom our derivation, the marginal posterior variance of $\\theta_1$ is $V_1 = \\text{Var}(\\theta_1 | y_{\\text{obs}}) = \\sigma^2 + \\tau_2^2$.\nThe parameter $\\sigma^2 = 1$ is a fixed constant from the measurement model. The parameter $\\tau_2^2$ represents the variance of our prior belief about $\\theta_2$, and must be non-negative ($\\tau_2^2 \\ge 0$).\nTo analyze the effect of $\\tau_2^2$ on $V_1$, we consider the derivative:\n$$\\frac{dV_1}{d(\\tau_2^2)} = \\frac{d}{d(\\tau_2^2)}(\\sigma^2 + \\tau_2^2) = 1$$\nSince the derivative is strictly positive, $V_1$ is a strictly increasing function of $\\tau_2^2$. Consequently, decreasing $\\tau_2^2$ strictly decreases the marginal posterior variance of $\\theta_1$. This confirms the first part of the statement.\nFor the second part, since $\\tau_2^2 \\ge 0$, the minimum possible value of the posterior variance is:\n$$V_1 = \\sigma^2 + \\tau_2^2 \\ge \\sigma^2 + 0 = \\sigma^2$$\nThe variance cannot be reduced below the measurement noise variance $\\sigma^2$. This bound is achieved in the limit as $\\tau_2^2 \\to 0$, which corresponds to a prior belief that $\\theta_2$ is known exactly. The statement is correct in its entirety.\nVerdict: **Correct**.\n\nD. Under the stated model, the maximum likelihood estimate of $\\theta_1$ equals $4$.\n\nThe Maximum Likelihood Estimate (MLE) is determined by maximizing the likelihood function $L(\\theta_1, \\theta_2; y_{\\text{obs}}) = p(y_{\\text{obs}} | \\theta_1, \\theta_2)$ with respect to the parameters. The prior distributions are not used in calculating the MLE.\nThe likelihood function is:\n$$L(\\theta_1, \\theta_2; y_{\\text{obs}}) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_{\\text{obs}} - \\theta_1 - \\theta_2)^2}{2\\sigma^2}\\right)$$\nMaximizing this function is equivalent to minimizing the squared error term in the exponent: $(y_{\\text{obs}} - \\theta_1 - \\theta_2)^2$.\nThe minimum value of this term is a global minimum of $0$, which is achieved for any pair of parameters $(\\theta_1, \\theta_2)$ that satisfies the condition:\n$$\\theta_1 + \\theta_2 = y_{\\text{obs}}$$\nGiven $y_{\\text{obs}} = 10$, any pair on the line $\\theta_1 + \\theta_2 = 10$ is an MLE. There is no unique MLE for the parameter vector $(\\theta_1, \\theta_2)$, and therefore no unique MLE for $\\theta_1$ individually. The set of values for $\\theta_1$ that maximize the likelihood is all of $\\mathbb{R}$.\nThe value $\\theta_1 = 4$ is the posterior mean, E$[\\theta_1 | y_{\\text{obs}}]$, and also the Maximum A Posteriori (MAP) estimate for $\\theta_1$, which maximizes the marginal posterior $p(\\theta_1 | y_{\\text{obs}})$. However, the question specifically asks for the MLE, which is distinct from these Bayesian estimators.\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{ABC}$$"
        },
        {
            "introduction": "Effective parameter estimation requires not only updating beliefs but also building models that are resilient to imperfect data. This problem tackles the critical issue of outliers, which can disproportionately influence estimates from standard models. By comparing the posterior distributions derived from a Gaussian likelihood versus a robust Student's t-likelihood, you will gain practical insight into how the choice of likelihood determines a model's sensitivity to aberrant measurements. ",
            "id": "2374122",
            "problem": "A scalar parameter $\\theta$ represents a constant bias in a sensor model used in computational engineering. You observe $n$ independent and identically distributed (i.i.d.) measurements $y_1,\\dots,y_n$ of the form $y_i = \\theta + \\epsilon_i$. Consider two competing noise models for the measurement error $\\epsilon_i$ and a common prior for $\\theta$:\n- Prior: $\\theta \\sim \\mathcal{N}(0,1)$.\n- Gaussian likelihood: $y_i \\mid \\theta \\sim \\mathcal{N}(\\theta,\\sigma^2)$ with known $\\sigma^2 = 0.01$.\n- Student’s t-likelihood: $y_i \\mid \\theta \\sim t_\\nu(\\theta,s)$ with known degrees of freedom $\\nu = 3$ and scale $s = 0.1$.\n\nYou collect $n=6$ measurements\n$$\ny = \\{0.1,\\,0.2,\\,-0.1,\\,0.2,\\,0.0,\\,5.0\\},\n$$\nwhere the last entry is visibly an outlier relative to the others.\n\nLet $p_{\\mathcal{N}}(\\theta \\mid y)$ denote the posterior density of $\\theta$ under the Gaussian likelihood and $p_{t}(\\theta \\mid y)$ the posterior density under the Student’s t-likelihood, both using the same prior stated above. Which of the following statements are correct?\n\nA. Under the stated models and data, both $p_{\\mathcal{N}}(\\theta \\mid y)$ and $p_{t}(\\theta \\mid y)$ are exactly Gaussian with the same mean and variance.\n\nB. For small $\\nu$ in the Student’s t-likelihood (such as $\\nu = 3$), $p_{t}(\\theta \\mid y)$ is less sensitive to the outlier than $p_{\\mathcal{N}}(\\theta \\mid y)$, placing its bulk closer to the central cluster of the non-outlying observations and exhibiting heavier tails than $p_{\\mathcal{N}}(\\theta \\mid y)$.\n\nC. For fixed scale parameters $\\sigma$ and $s$ as given, the mean (or any reasonable central tendency) of $p_{\\mathcal{N}}(\\theta \\mid y)$ is pulled farther toward the outlier than that of $p_{t}(\\theta \\mid y)$ for the provided data.\n\nD. As $\\nu \\to \\infty$ with $s$ held fixed, $p_{t}(\\theta \\mid y)$ converges to $p_{\\mathcal{N}}(\\theta \\mid y)$ under the same prior and data.\n\nE. For the provided data and prior, using a Student’s t-likelihood with small $\\nu$ necessarily yields a posterior variance for $\\theta$ that is smaller than that obtained with the Gaussian likelihood.\n\nF. With the Student’s t-likelihood and small $\\nu$, the outlier is completely ignored, so $p_{t}(\\theta \\mid y)$ is identical to the posterior one would obtain by removing the outlying observation and analyzing the remaining five points under the Student’s t-likelihood.",
            "solution": "The user has provided a Bayesian inference problem and asks for an evaluation of several statements comparing posteriors from a Gaussian likelihood and a Student's t-likelihood, especially in the context of outlier data.\n\nThe first step is to validate the problem statement.\n\n**Problem Validation**\n\nGivens extracted from the problem statement:\n-   A scalar parameter to be estimated is $\\theta$.\n-   The data generation model is $y_i = \\theta + \\epsilon_i$ for $n$ independent and identically distributed (i.i.d.) measurements.\n-   The prior distribution for the parameter is Gaussian: $\\theta \\sim \\mathcal{N}(0,1)$, thus $p(\\theta) = \\mathcal{N}(\\theta \\mid \\mu_0=0, \\sigma_0^2=1)$.\n-   The first likelihood model is Gaussian: $y_i \\mid \\theta \\sim \\mathcal{N}(\\theta, \\sigma^2)$ with a known variance $\\sigma^2 = 0.01$. This means $p(y_i \\mid \\theta) = \\mathcal{N}(y_i \\mid \\theta, 0.01)$.\n-   The second likelihood model is a location-scale Student's t-distribution: $y_i \\mid \\theta \\sim t_\\nu(\\theta, s)$ with known degrees of freedom $\\nu = 3$ and scale parameter $s = 0.1$.\n-   The observed data consists of $n=6$ measurements: $y = \\{0.1, 0.2, -0.1, 0.2, 0.0, 5.0\\}$.\n-   The posterior density under the Gaussian likelihood is denoted $p_{\\mathcal{N}}(\\theta \\mid y)$.\n-   The posterior density under the Student's t-likelihood is denoted $p_{t}(\\theta \\mid y)$.\n\nValidation Assessment:\nThe problem is scientifically grounded, utilizing standard statistical distributions (Gaussian, Student's t) and the principles of Bayesian inference. It is a well-posed problem, as all necessary components (prior, likelihoods, data) are fully specified, allowing for the unique determination of the posterior distributions. The language is objective and precise. The problem is self-contained, consistent, and directly relevant to the specified topic of Bayesian inference in computational engineering. The problem is therefore deemed **valid**.\n\n**Derivation and Analysis**\n\nThe posterior distribution is given by Bayes' theorem: $p(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta)$, where $p(y \\mid \\theta) = \\prod_{i=1}^n p(y_i \\mid \\theta)$.\n\n**1. Posterior under the Gaussian Likelihood: $p_{\\mathcal{N}}(\\theta \\mid y)$**\n\nFor a Gaussian prior $p(\\theta) = \\mathcal{N}(\\theta \\mid \\mu_0, \\sigma_0^2)$ and a Gaussian likelihood $p(y_i \\mid \\theta) = \\mathcal{N}(y_i \\mid \\theta, \\sigma^2)$, the posterior distribution is also Gaussian, $p_{\\mathcal{N}}(\\theta \\mid y) = \\mathcal{N}(\\theta \\mid \\mu_n, \\sigma_n^2)$. The posterior parameters are found by a precision-weighted update:\n$$ \\frac{1}{\\sigma_n^2} = \\frac{1}{\\sigma_0^2} + \\frac{n}{\\sigma^2} $$\n$$ \\mu_n = \\sigma_n^2 \\left( \\frac{\\mu_0}{\\sigma_0^2} + \\frac{n\\bar{y}}{\\sigma^2} \\right) $$\nWith the given values, $n=6$, $\\mu_0=0$, $\\sigma_0^2=1$, and $\\sigma^2=0.01$. The data sum is $\\sum_{i=1}^6 y_i = 0.1+0.2-0.1+0.2+0.0+5.0 = 5.4$, so the sample mean is $\\bar{y} = 5.4/6 = 0.9$.\n\nThe posterior precision is $\\frac{1}{\\sigma_n^2} = \\frac{1}{1} + \\frac{6}{0.01} = 1 + 600 = 601$.\nThe posterior variance is $\\sigma_n^2 = \\frac{1}{601}$.\n\nThe posterior mean is $\\mu_n = \\frac{1}{601} \\left( \\frac{0}{1} + \\frac{6 \\times 0.9}{0.01} \\right) = \\frac{1}{601} \\left( 540 \\right) = \\frac{540}{601} \\approx 0.8985$.\nThe center of this posterior is strongly influenced by the outlier $y_6=5.0$, pulling it far from the cluster of the first five points, which have a mean of $0.08$.\n\n**2. Posterior under the Student's t-Likelihood: $p_{t}(\\theta \\mid y)$**\n\nThe posterior is $p_{t}(\\theta \\mid y) \\propto p(\\theta) \\prod_{i=1}^n p_t(y_i \\mid \\theta)$, where $p_t$ denotes the Student's t-density.\n$$ p_{t}(\\theta \\mid y) \\propto \\exp\\left(-\\frac{\\theta^2}{2}\\right) \\prod_{i=1}^{n} \\left(1 + \\frac{(y_i - \\theta)^2}{\\nu s^2}\\right)^{-(\\nu+1)/2} $$\nWith $\\nu=3$ and $s=0.1$, this becomes:\n$$ p_{t}(\\theta \\mid y) \\propto \\exp\\left(-\\frac{\\theta^2}{2}\\right) \\prod_{i=1}^{6} \\left(1 + \\frac{(y_i - \\theta)^2}{0.03}\\right)^{-2} $$\nThis distribution is not analytically simple. Its key characteristic is robustness to outliers. The log-likelihood penalty for a large residual $|y_i - \\theta|$ scales as $\\log|y_i - \\theta|$, unlike the Gaussian likelihood's quadratic penalty $\\propto (y_i - \\theta)^2$. This weaker penalty means outliers have a much smaller influence on the posterior's location.\n\n**Evaluation of Options**\n\n**A. Under the stated models and data, both $p_{\\mathcal{N}}(\\theta \\mid y)$ and $p_{t}(\\theta \\mid y)$ are exactly Gaussian with the same mean and variance.**\n$p_{\\mathcal{N}}(\\theta \\mid y)$ is indeed Gaussian. However, $p_{t}(\\theta \\mid y)$ is the product of a Gaussian density and a product of Student's t-densities. The result is not a Gaussian distribution. This statement is fundamentally incorrect.\n**Verdict: Incorrect.**\n\n**B. For small $\\nu$ in the Student’s t-likelihood (such as $\\nu = 3$), $p_{t}(\\theta \\mid y)$ is less sensitive to the outlier than $p_{\\mathcal{N}}(\\theta \\mid y)$, placing its bulk closer to the central cluster of the non-outlying observations and exhibiting heavier tails than $p_{\\mathcal{N}}(\\theta \\mid y)$.**\nThis statement correctly captures the essential properties of using a t-likelihood.\n1.  **Less sensitive to the outlier**: Correct. This is due to the heavy tails of the t-distribution, which penalize large residuals less severely than a Gaussian distribution.\n2.  **Placing its bulk closer to the central cluster**: Correct. As a consequence of robustness, the posterior location parameter is not dragged towards the outlier at $y_6=5.0$. Its center will be much closer to the mean of the first five points ($\\approx 0.08$) than the Gaussian posterior mean ($\\approx 0.9$).\n3.  **Exhibiting heavier tails**: Correct. The tails of a posterior are determined by the asymptotic decay rate. For large $|\\theta|$, the log-density of $p_{\\mathcal{N}}(\\theta \\mid y)$ behaves as $-\\frac{1}{2}(\\frac{1}{\\sigma_0^2}+\\frac{n}{\\sigma^2})\\theta^2 = -\\frac{601}{2}\\theta^2$. The log-density of $p_{t}(\\theta \\mid y)$ behaves as $-\\frac{1}{2\\sigma_0^2}\\theta^2 - n(\\nu+1)\\log|\\theta|$, which is dominated by the prior's quadratic term $-\\frac{1}{2}\\theta^2$. Because $\\exp(-\\frac{1}{2}\\theta^2)$ decays much more slowly than $\\exp(-\\frac{601}{2}\\theta^2)$, the posterior $p_{t}(\\theta \\mid y)$ has much heavier tails than $p_{\\mathcal{N}}(\\theta \\mid y)$.\n**Verdict: Correct.**\n\n**C. For fixed scale parameters $\\sigma$ and $s$ as given, the mean (or any reasonable central tendency) of $p_{\\mathcal{N}}(\\theta \\mid y)$ is pulled farther toward the outlier than that of $p_{t}(\\theta \\mid y)$ for the provided data.**\nThis is a direct and accurate statement of the practical difference between the two models in this scenario. The Gaussian likelihood's sensitivity to the outlier at $y_6=5.0$ pulls its posterior mean significantly towards that value. The robust t-likelihood resists this pull. The statement is correct.\n**Verdict: Correct.**\n\n**D. As $\\nu \\to \\infty$ with $s$ held fixed, $p_{t}(\\theta \\mid y)$ converges to $p_{\\mathcal{N}}(\\theta \\mid y)$ under the same prior and data.**\nThis statement concerns a fundamental property of the Student's t-distribution. As the degrees of freedom $\\nu \\to \\infty$, the t-distribution $t_\\nu(\\mu, s)$ converges to a Gaussian distribution $\\mathcal{N}(\\mu, s^2)$. In this problem, the t-likelihood has scale $s=0.1$, so $s^2=0.01$. The Gaussian likelihood has variance $\\sigma^2=0.01$. Since $s^2 = \\sigma^2$, the t-likelihood indeed converges to the specified Gaussian likelihood. As the likelihoods converge and the prior is identical, the posteriors must also converge. The statement is correct.\n**Verdict: Correct.**\n\n**E. For the provided data and prior, using a Student’s t-likelihood with small $\\nu$ necessarily yields a posterior variance for $\\theta$ that is smaller than that obtained with the Gaussian likelihood.**\nThis is incorrect. The Gaussian model, by incorporating all $6$ data points with high confidence, produces a very precise posterior with a small variance of $\\sigma_n^2 = 1/601$. The Student's t-model, by down-weighting the information from the outlier, is effectively using less data to estimate $\\theta$. A reduction in the amount of effective information leads to an increase in posterior uncertainty, i.e., a larger variance. Robustness is gained at the cost of higher variance. Therefore, the posterior variance from the t-likelihood will be larger, not smaller.\n**Verdict: Incorrect.**\n\n**F. With the Student’s t-likelihood and small $\\nu$, the outlier is completely ignored, so $p_{t}(\\theta \\mid y)$ is identical to the posterior one would obtain by removing the outlying observation and analyzing the remaining five points under the Student’s t-likelihood.**\nThe term \"completely ignored\" is an overstatement and mathematically incorrect. An outlier is *down-weighted*, meaning its influence is reduced, but not eliminated. The likelihood term corresponding to the outlier, while small for values of $\\theta$ near the main cluster of data, is still a function of $\\theta$ and part of the overall posterior product. Removing the data point is equivalent to replacing this term with a constant, which is not what happens. The posterior is therefore not identical to one computed on the subset of data.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{BCD}$$"
        }
    ]
}