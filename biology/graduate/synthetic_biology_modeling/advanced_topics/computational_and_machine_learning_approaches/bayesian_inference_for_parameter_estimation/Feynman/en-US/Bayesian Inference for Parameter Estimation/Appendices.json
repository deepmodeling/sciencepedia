{
    "hands_on_practices": [
        {
            "introduction": "This first exercise serves as a fundamental introduction to the Bayesian workflow. You will estimate the activation probability of a synthetic gene circuit from a small dataset, learning to combine a binomial likelihood with a conjugate prior to form a posterior distribution. By calculating a Bayesian credible interval  and comparing it to a frequentist confidence interval, you will gain insight into the practical and philosophical differences between these two inferential frameworks.",
            "id": "3907430",
            "problem": "A synthetic gene circuit is designed to switch a fluorescent reporter to the \"ON\" state upon activation of a transcription factor. In a microfluidic assay, $n=10$ independent microdroplets are each loaded with a single engineered cell and exposed to the inducer under identical conditions. The observed count of \"ON\" droplets is $k=2$. Let the unknown activation probability be $p \\in (0,1)$, and model the data as a sequence of independent Bernoulli trials, yielding a binomial likelihood for $k$ successes in $n$ trials. Adopt Jeffreys' prior for the binomial proportion, which is the Beta distribution with parameters $1/2$ and $1/2$. Using coherent Bayesian updating, form the posterior distribution for $p$.\n\nFrom this posterior, construct the equal-tailed $0.95$ credible interval for $p$. Separately, construct the $0.95$ exact frequentist confidence interval for $p$ based on the Clopper–Pearson method (Clopper–Pearson exact confidence interval (CP)) using the binomial model. Define the width of an interval as the upper endpoint minus the lower endpoint.\n\nCompute the ratio of the width of the Bayesian equal-tailed $0.95$ credible interval to the width of the Clopper–Pearson $0.95$ confidence interval for these data. Express your final answer as a single closed-form analytical expression involving the inverse of the regularized incomplete Beta function, and do not perform any numerical rounding.",
            "solution": "The problem is first subjected to validation.\n\n**Step 1: Extract Givens**\n- Number of independent trials (microdroplets): $n=10$.\n- Number of observed successes (\"ON\" droplets): $k=2$.\n- Unknown parameter (activation probability): $p \\in (0,1)$.\n- Likelihood model: Binomial, describing $k$ successes in $n$ trials.\n- Prior distribution: Jeffreys' prior for the binomial proportion, which is a Beta distribution with parameters $\\alpha_0 = 1/2$ and $\\beta_0 = 1/2$, i.e., $p \\sim \\text{Beta}(1/2, 1/2)$.\n- Desired Bayesian interval: Equal-tailed $0.95$ credible interval for $p$.\n- Desired frequentist interval: Exact $0.95$ Clopper–Pearson confidence interval for $p$.\n- Final calculation: The ratio of the width of the Bayesian credible interval to the width of the Clopper–Pearson confidence interval.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, as it uses standard and well-established statistical methods (Bayesian inference with a conjugate prior, Clopper-Pearson interval) to analyze a common experimental setup in synthetic biology (quantifying the success rate of a gene circuit). The problem is well-posed, objective, and self-contained, providing all necessary data ($n$, $k$), models (Binomial likelihood, Jeffreys' prior), and methods (equal-tailed credible interval, Clopper-Pearson interval) to arrive at a unique, meaningful analytical solution. There are no contradictions, ambiguities, or unrealistic assumptions.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\nThe solution consists of three main parts: first, a Bayesian analysis to determine the width of the credible interval; second, a frequentist analysis to determine the width of the Clopper-Pearson confidence interval; and third, the computation of the ratio of these two widths.\n\n**1. Bayesian Credible Interval**\n\nThe analysis begins by defining the components of Bayes' theorem.\nThe likelihood for observing $k$ successes in $n$ trials is given by the binomial probability mass function:\n$$ P(k|n, p) = \\binom{n}{k} p^k (1-p)^{n-k} $$\nAs a function of $p$, the likelihood is proportional to $p^k (1-p)^{n-k}$.\n\nThe prior distribution for $p$ is specified as Jeffreys' prior, which is the Beta distribution $\\text{Beta}(1/2, 1/2)$. Its probability density function (PDF) is:\n$$ \\pi(p) \\propto p^{\\frac{1}{2}-1} (1-p)^{\\frac{1}{2}-1} = p^{-\\frac{1}{2}} (1-p)^{-\\frac{1}{2}} $$\nThe posterior distribution for $p$ is proportional to the product of the likelihood and the prior:\n$$ \\pi(p|k,n) \\propto P(k|n,p) \\times \\pi(p) \\propto \\left[p^k (1-p)^{n-k}\\right] \\times \\left[p^{\\frac{1}{2}-1} (1-p)^{\\frac{1}{2}-1}\\right] $$\n$$ \\pi(p|k,n) \\propto p^{k+\\frac{1}{2}-1} (1-p)^{n-k+\\frac{1}{2}-1} $$\nThis is the kernel of a Beta distribution. Due to the conjugacy of the Beta prior with the binomial likelihood, the posterior distribution is also a Beta distribution with updated parameters:\n$$ \\alpha_{\\text{post}} = k + \\frac{1}{2} $$\n$$ \\beta_{\\text{post}} = n - k + \\frac{1}{2} $$\nSubstituting the given values $n=10$ and $k=2$:\n$$ \\alpha_{\\text{post}} = 2 + \\frac{1}{2} = \\frac{5}{2} $$\n$$ \\beta_{\\text{post}} = 10 - 2 + \\frac{1}{2} = 8 + \\frac{1}{2} = \\frac{17}{2} $$\nSo, the posterior distribution is $p|k=2, n=10 \\sim \\text{Beta}(\\frac{5}{2}, \\frac{17}{2})$.\n\nAn equal-tailed $0.95$ credible interval $[p_L, p_U]$ is defined such that $P(p < p_L) = 0.025$ and $P(p > p_U) = 0.025$. These probabilities are found using the cumulative distribution function (CDF) of the posterior Beta distribution, which is the regularized incomplete Beta function, denoted $I_x(\\alpha, \\beta)$. The endpoints are therefore the quantiles of the posterior distribution:\n$$ p_L = I^{-1}(0.025; \\alpha_{\\text{post}}, \\beta_{\\text{post}}) = I^{-1}(0.025; \\frac{5}{2}, \\frac{17}{2}) $$\n$$ p_U = I^{-1}(0.975; \\alpha_{\\text{post}}, \\beta_{\\text{post}}) = I^{-1}(0.975; \\frac{5}{2}, \\frac{17}{2}) $$\nThe width of the Bayesian credible interval, $W_B$, is the difference between the upper and lower endpoints:\n$$ W_B = p_U - p_L = I^{-1}(0.975; \\frac{5}{2}, \\frac{17}{2}) - I^{-1}(0.025; \\frac{5}{2}, \\frac{17}{2}) $$\n\n**2. Clopper-Pearson Confidence Interval**\n\nThe Clopper-Pearson $1-\\alpha$ confidence interval $[p_{L,CP}, p_{U,CP}]$ for a binomial proportion is constructed by inverting two one-sided binomial tests. For a $0.95$ confidence level, $\\alpha = 0.05$.\nThe lower endpoint, $p_{L,CP}$, is the value of $p$ for which the probability of observing $k$ or more successes is $\\frac{\\alpha}{2} = 0.025$:\n$$ P(X \\ge k | p=p_{L,CP}, n) = \\sum_{j=k}^{n} \\binom{n}{j} (p_{L,CP})^j (1-p_{L,CP})^{n-j} = \\frac{\\alpha}{2} $$\nThis cumulative probability is related to the regularized incomplete Beta function by the identity $P(X \\ge k) = I_p(k, n-k+1)$. Thus:\n$$ I_{p_{L,CP}}(k, n-k+1) = \\frac{\\alpha}{2} $$\nSolving for $p_{L,CP}$ and substituting $k=2$, $n=10$, and $\\frac{\\alpha}{2}=0.025$:\n$$ p_{L,CP} = I^{-1}(\\frac{\\alpha}{2}; k, n-k+1) = I^{-1}(0.025; 2, 10-2+1) = I^{-1}(0.025; 2, 9) $$\nThe upper endpoint, $p_{U,CP}$, is the value of $p$ for which the probability of observing $k$ or fewer successes is $\\frac{\\alpha}{2} = 0.025$:\n$$ P(X \\le k | p=p_{U,CP}, n) = \\sum_{j=0}^{k} \\binom{n}{j} (p_{U,CP})^j (1-p_{U,CP})^{n-j} = \\frac{\\alpha}{2} $$\nThis cumulative probability is related to the regularized incomplete Beta function by the identity $P(X \\le k) = I_{1-p}(n-k, k+1)$. A more direct formulation for the quantile function is derived from $P(X \\ge k+1 | p=p_{U,CP}) = 1-\\frac{\\alpha}{2}$, which gives $I_{p_{U,CP}}(k+1, n-k) = 1-\\frac{\\alpha}{2}$. Solving for $p_{U,CP}$:\n$$ p_{U,CP} = I^{-1}(1-\\frac{\\alpha}{2}; k+1, n-k) = I^{-1}(0.975; 2+1, 10-2) = I^{-1}(0.975; 3, 8) $$\nThe width of the Clopper-Pearson confidence interval, $W_{CP}$, is the difference between its endpoints:\n$$ W_{CP} = p_{U,CP} - p_{L,CP} = I^{-1}(0.975; 3, 8) - I^{-1}(0.025; 2, 9) $$\n\n**3. Ratio of the Widths**\n\nThe final step is to compute the ratio of the width of the Bayesian credible interval, $W_B$, to the width of the Clopper-Pearson confidence interval, $W_{CP}$.\n$$ \\text{Ratio} = \\frac{W_B}{W_{CP}} = \\frac{I^{-1}(0.975; \\frac{5}{2}, \\frac{17}{2}) - I^{-1}(0.025; \\frac{5}{2}, \\frac{17}{2})}{I^{-1}(0.975; 3, 8) - I^{-1}(0.025; 2, 9)} $$\nThis is the final closed-form analytical expression as required by the problem statement.",
            "answer": "$$\n\\boxed{\\frac{I^{-1}(0.975; \\frac{5}{2}, \\frac{17}{2}) - I^{-1}(0.025; \\frac{5}{2}, \\frac{17}{2})}{I^{-1}(0.975; 3, 8) - I^{-1}(0.025; 2, 9)}}\n$$"
        },
        {
            "introduction": "This practice applies Bayesian methods to a core problem in synthetic biology: estimating a kinetic parameter from time-series data. You will analyze a simulated protein degradation experiment to infer the first-order decay rate, $\\delta$. This exercise  demonstrates the power of conjugate priors in a linear model, showing how to analytically derive the posterior distribution and compute the maximum a posteriori (MAP) estimate for a key dynamic parameter.",
            "id": "3907511",
            "problem": "A synthetic gene circuit expresses a fluorescent reporter protein under the control of an inducible promoter. At time $t=0$, the inducer is removed, halting production so that the protein abundance $X(t)$ decays by first-order kinetics with degradation rate $\\delta$. Assume the standard mass-action decay law $\\frac{dX}{dt}=-\\delta X$ holds and that the measured fluorescence $F(t)$ is proportional to $X(t)$ with a fixed proportionality constant. Measurement noise is multiplicative in intensity and thus additive in the logarithm, so that $y_i=\\ln F(t_i)$ satisfies $y_i=\\mu-\\delta t_i+\\epsilon_i$, where $\\mu=\\ln(\\kappa X_0)$ absorbs the proportionality constant $\\kappa$ and initial abundance $X_0$, and $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ are independent and identically distributed. A high signal-to-noise baseline measurement at $t=0$ provides $F_0$ such that $\\mu=\\ln F_0$ can be treated as known. You are given a single-cell time-lapse trajectory at times $t_i \\in \\{5,10,15,20,25,30\\}$ minutes with recorded intensities\n$F_i \\in \\{10^{4}\\exp(-0.05 \\times 5),\\,10^{4}\\exp(-0.05 \\times 10),\\,10^{4}\\exp(-0.05 \\times 15),\\,10^{4}\\exp(-0.05 \\times 20),\\,10^{4}\\exp(-0.05 \\times 25),\\,10^{4}\\exp(-0.05 \\times 30)\\}$ in arbitrary units (a.u.), and $F_0=10^{4}$ a.u. The log-space noise standard deviation is known to be $\\sigma=0.05$. Place a Gaussian prior on the degradation rate $\\delta \\sim \\mathcal{N}(m_0,s_0^2)$ with $m_0=0.04$ and $s_0=0.02$, where the units of $m_0$ and $s_0$ are in $\\text{min}^{-1}$.\n\nUsing only the definitions and laws stated above as your starting point, perform the following:\n\n- Construct the Bayesian model for $\\delta$ based on the log-transformed measurements and the given prior.\n- Derive the analytical form of the posterior distribution $p(\\delta \\mid \\{(t_i,F_i)\\}_{i=1}^{6})$.\n- Compute the maximum a posteriori estimate $\\hat{\\delta}_{\\text{MAP}}$ from this posterior for the provided data.\n\nRound your final numerical answer for $\\hat{\\delta}_{\\text{MAP}}$ to four significant figures. Express the final rate in $\\text{min}^{-1}$.",
            "solution": "The problem statement is first subjected to validation to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- **Differential Equation:** The protein abundance $X(t)$ decays according to $\\frac{dX}{dt}=-\\delta X$.\n- **Measurement Model:** Measured fluorescence $F(t)$ is proportional to $X(t)$, i.e., $F(t) = \\kappa X(t)$.\n- **Log-linear Model:** The log-transformed measurements $y_i = \\ln F(t_i)$ follow the linear model $y_i = \\mu - \\delta t_i + \\epsilon_i$, where $\\mu = \\ln(\\kappa X_0)$.\n- **Noise Model:** The noise terms $\\epsilon_i$ are independent and identically distributed (i.i.d.) Gaussian random variables, $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n- **Known Parameters:**\n    - $\\mu$ is taken as known: $\\mu = \\ln F_0$.\n    - Initial fluorescence: $F_0 = 10^4$ a.u.\n    - Noise standard deviation: $\\sigma = 0.05$.\n- **Data:**\n    - Measurement times: $t_i \\in \\{5, 10, 15, 20, 25, 30\\}$ minutes for $i=1, \\dots, 6$.\n    - Measured fluorescence values: $F_i = 10^4 \\exp(-0.05 t_i)$ for the corresponding $t_i$.\n- **Prior Distribution:** A Gaussian prior is placed on the degradation rate $\\delta$: $\\delta \\sim \\mathcal{N}(m_0, s_0^2)$.\n    - Prior mean: $m_0 = 0.04 \\text{ min}^{-1}$.\n    - Prior standard deviation: $s_0 = 0.02 \\text{ min}^{-1}$.\n- **Tasks:**\n    1. Construct the Bayesian model for $\\delta$.\n    2. Derive the analytical form of the posterior distribution $p(\\delta \\mid \\{(t_i,F_i)\\})$.\n    3. Compute the maximum a posteriori (MAP) estimate $\\hat{\\delta}_{\\text{MAP}}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, employing standard mass-action kinetics for first-order decay and a common log-linear model for fluorescence data with multiplicative noise. The problem is well-posed; it provides all necessary information (data, a full probabilistic model, and a prior) to derive a unique posterior distribution and, consequently, a unique MAP estimate. The use of a Gaussian prior for a parameter in a linear-Gaussian model is a standard conjugate-prior setup, ensuring a well-defined Gaussian posterior. The language is objective and precise. The data, while synthetically generated without noise from the model itself, do not invalidate the problem, which is a test of applying the Bayesian framework with specified model noise $\\sigma$. The problem is self-contained, consistent, and scientifically sound.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed with deriving the solution.\n\n### Bayesian Model Construction and Posterior Derivation\nThe goal is to find the posterior distribution of the degradation rate $\\delta$ given the data $D = \\{(t_i, y_i)\\}_{i=1}^6$, where $y_i = \\ln F_i$. Using Bayes' theorem, the posterior is proportional to the product of the likelihood and the prior:\n$$p(\\delta \\mid D) \\propto p(D \\mid \\delta) p(\\delta)$$\n\n**1. The Likelihood Function**\nThe model for a single log-transformed measurement is $y_i = \\mu - \\delta t_i + \\epsilon_i$, with $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$. This implies that the conditional distribution of $y_i$ given $\\delta$ is Gaussian:\n$$y_i \\mid \\delta \\sim \\mathcal{N}(\\mu - \\delta t_i, \\sigma^2)$$\nThe probability density function for a single data point is:\n$$p(y_i \\mid \\delta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y_i - (\\mu - \\delta t_i))^2}{2\\sigma^2} \\right) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y_i - \\mu + \\delta t_i)^2}{2\\sigma^2} \\right)$$\nSince the measurements are independent, the likelihood of the entire dataset $D$ is the product of the individual probabilities:\n$$L(\\delta) = p(D \\mid \\delta) = \\prod_{i=1}^N p(y_i \\mid \\delta)$$\nIt is more convenient to work with the log-likelihood, $\\ln L(\\delta)$. Keeping only terms that depend on $\\delta$:\n$$\\ln L(\\delta) = \\sum_{i=1}^N \\ln p(y_i \\mid \\delta) = \\text{const} - \\frac{1}{2\\sigma^2} \\sum_{i=1}^N (y_i - \\mu + \\delta t_i)^2$$\nExpanding the quadratic term, we see that the log-likelihood is a quadratic function of $\\delta$:\n$$\\ln L(\\delta) = \\text{const} - \\frac{1}{2\\sigma^2} \\left( \\delta^2 \\sum_{i=1}^N t_i^2 + 2\\delta \\sum_{i=1}^N t_i(y_i - \\mu) + \\sum_{i=1}^N (y_i - \\mu)^2 \\right)$$\nThis indicates that the likelihood, as a function of $\\delta$, is proportional to a Gaussian distribution.\n\n**2. The Prior Distribution**\nThe prior distribution for $\\delta$ is given as a Gaussian:\n$$p(\\delta) = \\mathcal{N}(\\delta; m_0, s_0^2) = \\frac{1}{\\sqrt{2\\pi s_0^2}} \\exp\\left( -\\frac{(\\delta - m_0)^2}{2s_0^2} \\right)$$\nThe log-prior is:\n$$\\ln p(\\delta) = \\text{const} - \\frac{(\\delta - m_0)^2}{2s_0^2} = \\text{const} - \\frac{1}{2s_0^2}(\\delta^2 - 2\\delta m_0 + m_0^2)$$\n\n**3. The Posterior Distribution**\nThe log-posterior is the sum of the log-likelihood and the log-prior (plus constants):\n$$\\ln p(\\delta \\mid D) = \\text{const} - \\frac{1}{2} \\left[ \\frac{1}{\\sigma^2}\\left(\\delta^2 \\sum t_i^2 - 2\\delta \\sum t_i(\\mu-y_i)\\right) + \\frac{1}{s_0^2}(\\delta^2 - 2\\delta m_0) \\right]$$\nGrouping terms by powers of $\\delta$:\n$$\\ln p(\\delta \\mid D) = \\text{const} - \\frac{1}{2} \\left[ \\delta^2 \\left( \\frac{\\sum t_i^2}{\\sigma^2} + \\frac{1}{s_0^2} \\right) - 2\\delta \\left( \\frac{\\sum t_i(\\mu-y_i)}{\\sigma^2} + \\frac{m_0}{s_0^2} \\right) \\right]$$\nThis is a quadratic form in $\\delta$, which means the posterior distribution $p(\\delta \\mid D)$ is also a Gaussian, say $\\mathcal{N}(\\delta; m_N, s_N^2)$. The general form of a log-Gaussian is $\\text{const} - \\frac{(\\delta - m_N)^2}{2s_N^2} = \\text{const} - \\frac{1}{2} \\left( \\frac{\\delta^2}{s_N^2} - \\frac{2\\delta m_N}{s_N^2} \\right)$.\nBy comparing coefficients of $\\delta^2$ and $\\delta$, we find the posterior variance $s_N^2$ and mean $m_N$.\nThe inverse of the posterior variance (the posterior precision) is:\n$$\\frac{1}{s_N^2} = \\frac{\\sum_{i=1}^N t_i^2}{\\sigma^2} + \\frac{1}{s_0^2}$$\nThe posterior mean is:\n$$m_N = s_N^2 \\left( \\frac{\\sum_{i=1}^N t_i(\\mu-y_i)}{\\sigma^2} + \\frac{m_0}{s_0^2} \\right)$$\nThe analytical form of the posterior distribution is therefore a Gaussian distribution:\n$$p(\\delta \\mid D) = \\mathcal{N}\\left(\\delta; m_N, s_N^2\\right)$$\nwith parameters $m_N$ and $s_N^2$ as defined above.\n\n### MAP Estimate Calculation\nFor a Gaussian posterior distribution, the maximum a posteriori (MAP) estimate is equal to its mean, mode, and median. Thus, $\\hat{\\delta}_{\\text{MAP}} = m_N$. We now compute the numerical value.\n\nFirst, process the data:\n$F_0 = 10^4$, so $\\mu = \\ln(10^4)$.\n$F_i = 10^4\\exp(-0.05 t_i)$, so $y_i = \\ln F_i = \\ln(10^4) - 0.05 t_i$.\nThe term $\\mu - y_i$ simplifies to: $\\mu - y_i = \\ln(10^4) - (\\ln(10^4) - 0.05 t_i) = 0.05 t_i$.\n\nNext, calculate the required sums:\nThe time points are $t_i \\in \\{5, 10, 15, 20, 25, 30\\}$.\n$\\sum_{i=1}^6 t_i^2 = 5^2 + 10^2 + 15^2 + 20^2 + 25^2 + 30^2 = 25 + 100 + 225 + 400 + 625 + 900 = 2275 \\text{ min}^2$.\n$\\sum_{i=1}^6 t_i(\\mu-y_i) = \\sum_{i=1}^6 t_i(0.05 t_i) = 0.05 \\sum_{i=1}^6 t_i^2 = 0.05 \\times 2275 = 113.75 \\text{ min}$.\n\nNow, use the given parameter values:\n$\\sigma = 0.05 \\implies \\sigma^2 = 0.0025$.\n$m_0 = 0.04 \\text{ min}^{-1}$.\n$s_0 = 0.02 \\text{ min}^{-1} \\implies s_0^2 = 0.0004 \\text{ min}^{-2}$.\n\nCalculate the posterior precision ($1/s_N^2$):\n$$\\frac{1}{s_N^2} = \\frac{2275}{0.0025} + \\frac{1}{0.0004} = 910000 + 2500 = 912500 \\text{ min}^2$$\n\nCalculate the term in the parentheses for $m_N$:\n$$\\frac{\\sum t_i(\\mu-y_i)}{\\sigma^2} + \\frac{m_0}{s_0^2} = \\frac{113.75}{0.0025} + \\frac{0.04}{0.0004} = 45500 + 100 = 45600 \\text{ min}$$\n\nFinally, compute $\\hat{\\delta}_{\\text{MAP}} = m_N$:\n$$\\hat{\\delta}_{\\text{MAP}} = m_N = \\frac{45600 \\text{ min}}{912500 \\text{ min}^2} = \\frac{456}{9125} \\text{ min}^{-1}$$\n$$\\hat{\\delta}_{\\text{MAP}} \\approx 0.0499726027... \\text{ min}^{-1}$$\nRounding to four significant figures, we get $0.04997$.\nThe result is a precision-weighted average of the maximum likelihood estimate, which is $0.05$, and the prior mean of $0.04$. Since the data precision is much higher than the prior precision ($910000$ vs $2500$), the posterior mean is pulled very close to the MLE.",
            "answer": "$$\\boxed{0.04997}$$"
        },
        {
            "introduction": "This final practice explores parameter non-identifiability, a common hurdle in systems biology where experimental data cannot uniquely determine all model parameters. You will confront a simple model where two parameters, $\\theta_1$ and $\\theta_2$, are unresolvable from the likelihood alone. This exercise  powerfully illustrates how an informative prior can regularize an ill-posed problem, enabling coherent inference even when the data is sparse.",
            "id": "2374096",
            "problem": "In a computational engineering calibration task, a lumped model predicts a measured scalar response as the sum of two component parameters. A single experiment produces one measurement modeled as\n$$\ny = \\theta_1 + \\theta_2 + \\varepsilon,\\quad \\varepsilon \\sim \\mathcal{N}(0,\\sigma^2),\n$$\nwith observed value $y_{\\text{obs}} = 10$ and known noise variance $\\sigma^2 = 1$. The parameters $\\theta_1$ and $\\theta_2$ represent distinct component contributions that are of engineering interest individually. Because only the sum is observed in this single experiment, the likelihood is non-identifying for $(\\theta_1,\\theta_2)$. To incorporate prior engineering knowledge, use an improper flat prior for $\\theta_1$ (i.e., $p(\\theta_1) \\propto 1$ over $\\mathbb{R}$) and a strongly informative Gaussian prior for $\\theta_2$,\n$$\n\\theta_2 \\sim \\mathcal{N}(\\mu_2,\\tau_2^2),\\quad \\mu_2 = 6,\\ \\tau_2^2 = 0.04.\n$$\nConsider the posterior inference that results from this model.\n\nWhich of the following statements are correct?\n\nA. The marginal posterior for $\\theta_1$ is Gaussian with mean $y_{\\text{obs}} - \\mu_2$ and variance $\\sigma^2 + \\tau_2^2$; numerically, it is $\\mathcal{N}(4,\\ 1.04)$.\n\nB. If the prior on $\\theta_2$ were also improper flat (so both $\\theta_1$ and $\\theta_2$ had flat priors), then the posterior for $\\theta_1$ given $y_{\\text{obs}}$ would be improper.\n\nC. Decreasing $\\tau_2^2$ strictly decreases the marginal posterior variance of $\\theta_1$, and this variance cannot be reduced below $\\sigma^2$.\n\nD. Under the stated model, the maximum likelihood estimate of $\\theta_1$ equals $4$.",
            "solution": "The problem requires the derivation and analysis of the posterior distribution for the parameters $\\theta_1$ and $\\theta_2$ in a simple linear model, where the likelihood is non-identifying. The use of prior information is essential to obtain a well-defined posterior. We will proceed by first deriving the marginal posterior distribution for the parameter of interest, $\\theta_1$, and then evaluating each statement.\n\nThe model is defined by the following components:\nThe likelihood function is derived from the measurement model $y = \\theta_1 + \\theta_2 + \\varepsilon$, where the error term $\\varepsilon$ follows a normal distribution $\\mathcal{N}(0, \\sigma^2)$. Given a single observation $y_{\\text{obs}}$, the likelihood of the parameters $(\\theta_1, \\theta_2)$ is:\n$$p(y_{\\text{obs}} | \\theta_1, \\theta_2, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_{\\text{obs}} - \\theta_1 - \\theta_2)^2}{2\\sigma^2}\\right)$$\nThe numerical values are given as $y_{\\text{obs}} = 10$ and $\\sigma^2 = 1$.\n\nThe prior distributions for the parameters are given as:\n- For $\\theta_1$: an improper uniform (flat) prior, $p(\\theta_1) \\propto 1$ for $\\theta_1 \\in \\mathbb{R}$.\n- For $\\theta_2$: a proper normal (Gaussian) prior, $\\theta_2 \\sim \\mathcal{N}(\\mu_2, \\tau_2^2)$, with $p(\\theta_2) = \\frac{1}{\\sqrt{2\\pi\\tau_2^2}} \\exp\\left(-\\frac{(\\theta_2 - \\mu_2)^2}{2\\tau_2^2}\\right)$.\nThe numerical values for the prior on $\\theta_2$ are $\\mu_2 = 6$ and $\\tau_2^2 = 0.04$.\n\nAssuming prior independence between $\\theta_1$ and $\\theta_2$, the joint prior is $p(\\theta_1, \\theta_2) = p(\\theta_1)p(\\theta_2)$.\n\nAccording to Bayes' theorem, the joint posterior distribution is proportional to the product of the likelihood and the joint prior:\n$$p(\\theta_1, \\theta_2 | y_{\\text{obs}}) \\propto p(y_{\\text{obs}} | \\theta_1, \\theta_2) p(\\theta_1, \\theta_2) \\propto \\exp\\left(-\\frac{(y_{\\text{obs}} - \\theta_1 - \\theta_2)^2}{2\\sigma^2}\\right) \\exp\\left(-\\frac{(\\theta_2 - \\mu_2)^2}{2\\tau_2^2}\\right)$$\nTo find the marginal posterior for $\\theta_1$, we must integrate the joint posterior over all possible values of $\\theta_2$:\n$$p(\\theta_1 | y_{\\text{obs}}) = \\int_{-\\infty}^{\\infty} p(\\theta_1, \\theta_2 | y_{\\text{obs}}) d\\theta_2$$\nA more direct method is to first determine the effective likelihood for $\\theta_1$, denoted $p(y_{\\text{obs}} | \\theta_1)$. From the model $y = \\theta_1 + \\theta_2 + \\varepsilon$, we can write $y | \\theta_1$ as a random variable determined by the distributions of $\\theta_2$ and $\\varepsilon$. Given $\\theta_1$, the quantity $y$ is the sum of a constant $\\theta_1$ and two independent random variables: $\\theta_2 \\sim \\mathcal{N}(\\mu_2, \\tau_2^2)$ and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$. The sum of independent normal random variables is also a normal random variable.\nThe expectation of $y$ given $\\theta_1$ is:\n$$E[y | \\theta_1] = E[\\theta_1 + \\theta_2 + \\varepsilon | \\theta_1] = \\theta_1 + E[\\theta_2] + E[\\varepsilon] = \\theta_1 + \\mu_2 + 0 = \\theta_1 + \\mu_2$$\nThe variance of $y$ given $\\theta_1$ is:\n$$\\text{Var}(y | \\theta_1) = \\text{Var}(\\theta_1 + \\theta_2 + \\varepsilon | \\theta_1) = \\text{Var}(\\theta_2) + \\text{Var}(\\varepsilon) = \\tau_2^2 + \\sigma^2$$\nTherefore, the distribution of $y$ conditioned on $\\theta_1$ is normal:\n$$y | \\theta_1 \\sim \\mathcal{N}(\\theta_1 + \\mu_2, \\sigma^2 + \\tau_2^2)$$\nThis gives the effective likelihood for $\\theta_1$: $p(y_{\\text{obs}}|\\theta_1) = \\mathcal{N}(y_{\\text{obs}} | \\theta_1 + \\mu_2, \\sigma^2 + \\tau_2^2)$.\n\nNow, we apply Bayes' theorem to find the posterior for $\\theta_1$:\n$$p(\\theta_1 | y_{\\text{obs}}) \\propto p(y_{\\text{obs}} | \\theta_1) p(\\theta_1)$$\nWith the flat prior $p(\\theta_1) \\propto 1$, the posterior is proportional to the likelihood:\n$$p(\\theta_1 | y_{\\text{obs}}) \\propto \\exp\\left(-\\frac{(y_{\\text{obs}} - (\\theta_1 + \\mu_2))^2}{2(\\sigma^2 + \\tau_2^2)}\\right) = \\exp\\left(-\\frac{((y_{\\text{obs}} - \\mu_2) - \\theta_1)^2}{2(\\sigma^2 + \\tau_2^2)}\\right)$$\nThis is the kernel of a normal distribution for $\\theta_1$. By inspection, the posterior distribution for $\\theta_1$ is:\n$$\\theta_1 | y_{\\text{obs}} \\sim \\mathcal{N}(\\; y_{\\text{obs}} - \\mu_2, \\; \\sigma^2 + \\tau_2^2 \\;)$$\nWe can now evaluate each statement.\n\nA. The marginal posterior for $\\theta_1$ is Gaussian with mean $y_{\\text{obs}} - \\mu_2$ and variance $\\sigma^2 + \\tau_2^2$; numerically, it is $\\mathcal{N}(4,\\ 1.04)$.\n\nBased on our derivation, the marginal posterior for $\\theta_1$ is indeed a normal distribution with mean $y_{\\text{obs}} - \\mu_2$ and variance $\\sigma^2 + \\tau_2^2$.\nLet's substitute the given numerical values:\n- Mean: $E[\\theta_1 | y_{\\text{obs}}] = y_{\\text{obs}} - \\mu_2 = 10 - 6 = 4$.\n- Variance: $\\text{Var}(\\theta_1 | y_{\\text{obs}}) = \\sigma^2 + \\tau_2^2 = 1 + 0.04 = 1.04$.\nThe resulting posterior is $\\mathcal{N}(4, 1.04)$. The statement is entirely consistent with our derivation.\nVerdict: **Correct**.\n\nB. If the prior on $\\theta_2$ were also improper flat (so both $\\theta_1$ and $\\theta_2$ had flat priors), then the posterior for $\\theta_1$ given $y_{\\text{obs}}$ would be improper.\n\nLet us assume $p(\\theta_1) \\propto 1$ and $p(\\theta_2) \\propto 1$. The joint prior is $p(\\theta_1, \\theta_2) \\propto 1$.\nThe joint posterior becomes:\n$$p(\\theta_1, \\theta_2 | y_{\\text{obs}}) \\propto p(y_{\\text{obs}} | \\theta_1, \\theta_2) p(\\theta_1, \\theta_2) \\propto \\exp\\left(-\\frac{(y_{\\text{obs}} - \\theta_1 - \\theta_2)^2}{2\\sigma^2}\\right)$$\nTo find the marginal posterior for $\\theta_1$, we integrate over $\\theta_2$:\n$$p(\\theta_1 | y_{\\text{obs}}) = \\int_{-\\infty}^{\\infty} p(\\theta_1, \\theta_2 | y_{\\text{obs}}) d\\theta_2 \\propto \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{(y_{\\text{obs}} - \\theta_1 - \\theta_2)^2}{2\\sigma^2}\\right) d\\theta_2$$\nLet $z = \\theta_2$. The integrand is a Gaussian function of $z$ with mean $(y_{\\text{obs}} - \\theta_1)$ and variance related parameter $\\sigma^2$. The definite integral of an unnormalized Gaussian function over $\\mathbb{R}$ is a constant. Specifically, $\\int_{-\\infty}^{\\infty} e^{-(x-c)^2/(2s^2)} dx = \\sqrt{2\\pi s^2}$.\nIn our case, the integral evaluates to $\\sqrt{2\\pi\\sigma^2}$, which is a positive constant that does not depend on $\\theta_1$.\nThus, $p(\\theta_1 | y_{\\text{obs}}) \\propto \\text{constant}$ for all $\\theta_1 \\in \\mathbb{R}$.\nAn unnormalized density that is constant over the entire real line is an improper distribution, as its integral $\\int_{-\\infty}^{\\infty} c \\, d\\theta_1$ diverges for any constant $c > 0$. Therefore, the marginal posterior for $\\theta_1$ would be improper.\nVerdict: **Correct**.\n\nC. Decreasing $\\tau_2^2$ strictly decreases the marginal posterior variance of $\\theta_1$, and this variance cannot be reduced below $\\sigma^2$.\n\nFrom our derivation, the marginal posterior variance of $\\theta_1$ is $V_1 = \\text{Var}(\\theta_1 | y_{\\text{obs}}) = \\sigma^2 + \\tau_2^2$.\nThe parameter $\\sigma^2 = 1$ is a fixed constant from the measurement model. The parameter $\\tau_2^2$ represents the variance of our prior belief about $\\theta_2$, and must be non-negative ($\\tau_2^2 \\ge 0$).\nTo analyze the effect of $\\tau_2^2$ on $V_1$, we consider the derivative:\n$$\\frac{dV_1}{d(\\tau_2^2)} = \\frac{d}{d(\\tau_2^2)}(\\sigma^2 + \\tau_2^2) = 1$$\nSince the derivative is strictly positive, $V_1$ is a strictly increasing function of $\\tau_2^2$. Consequently, decreasing $\\tau_2^2$ strictly decreases the marginal posterior variance of $\\theta_1$. This confirms the first part of the statement.\nFor the second part, since $\\tau_2^2 \\ge 0$, the minimum possible value of the posterior variance is:\n$$V_1 = \\sigma^2 + \\tau_2^2 \\ge \\sigma^2 + 0 = \\sigma^2$$\nThe variance cannot be reduced below the measurement noise variance $\\sigma^2$. This bound is achieved in the limit as $\\tau_2^2 \\to 0$, which corresponds to a prior belief that $\\theta_2$ is known exactly. The statement is correct in its entirety.\nVerdict: **Correct**.\n\nD. Under the stated model, the maximum likelihood estimate of $\\theta_1$ equals $4$.\n\nThe Maximum Likelihood Estimate (MLE) is determined by maximizing the likelihood function $L(\\theta_1, \\theta_2; y_{\\text{obs}}) = p(y_{\\text{obs}} | \\theta_1, \\theta_2)$ with respect to the parameters. The prior distributions are not used in calculating the MLE.\nThe likelihood function is:\n$$L(\\theta_1, \\theta_2; y_{\\text{obs}}) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_{\\text{obs}} - \\theta_1 - \\theta_2)^2}{2\\sigma^2}\\right)$$\nMaximizing this function is equivalent to minimizing the squared error term in the exponent: $(y_{\\text{obs}} - \\theta_1 - \\theta_2)^2$.\nThe minimum value of this term is a global minimum of $0$, which is achieved for any pair of parameters $(\\theta_1, \\theta_2)$ that satisfies the condition:\n$$\\theta_1 + \\theta_2 = y_{\\text{obs}}$$\nGiven $y_{\\text{obs}} = 10$, any pair on the line $\\theta_1 + \\theta_2 = 10$ is an MLE. There is no unique MLE for the parameter vector $(\\theta_1, \\theta_2)$, and therefore no unique MLE for $\\theta_1$ individually. The set of values for $\\theta_1$ that maximize the likelihood is all of $\\mathbb{R}$.\nThe value $\\theta_1 = 4$ is the posterior mean, E$[\\theta_1 | y_{\\text{obs}}]$, and also the Maximum A Posteriori (MAP) estimate for $\\theta_1$, which maximizes the marginal posterior $p(\\theta_1 | y_{\\text{obs}})$. However, the question specifically asks for the MLE, which is distinct from these Bayesian estimators.\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{ABC}$$"
        }
    ]
}