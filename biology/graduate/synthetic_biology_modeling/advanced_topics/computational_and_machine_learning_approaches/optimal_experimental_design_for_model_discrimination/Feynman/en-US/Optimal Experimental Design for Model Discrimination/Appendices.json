{
    "hands_on_practices": [
        {
            "introduction": "To design an experiment that can best distinguish between competing models, we first need a rigorous way to quantify the \"difference\" between their predictions. This practice grounds our work in information theory by deriving the Kullback-Leibler (KL) divergence, a fundamental measure of the information lost when one probability distribution is used to approximate another. By working from the basic definition of a Poisson distribution, a common model for single-cell count data, you will build a key formula for model discrimination from the ground up. ",
            "id": "3924552",
            "problem": "A synthetic gene expression system is engineered with an optogenetic transcriptional activator that is controllably illuminated at intensity (design) $d>0$ during a fixed assay window. For a given $d$, a single-cell messenger ribonucleic acid (mRNA) count $Y\\_d$ is recorded. Two rival models, $\\mathcal{M}\\_0$ and $\\mathcal{M}\\_1$, posit that $Y\\_d$ follows a Poisson law with means $\\lambda\\_0(d)$ and $\\lambda\\_1(d)$, respectively, where $\\lambda\\_0(d)>0$ and $\\lambda\\_1(d)>0$ for all admissible $d$. Assume that the count under each model has probability mass function $p\\_j(y\\mid d)=\\exp(-\\lambda\\_j(d))\\,\\lambda\\_j(d)^y/y!$ for $y\\in\\{0,1,2,\\ldots\\}$ and $j\\in\\{0,1\\}$. Using only the foundational definition of Kullback–Leibler (KL) divergence and the Poisson probability mass function, derive a closed-form analytic expression for the Kullback–Leibler divergence $D\\_{KL}\\!\\left(P\\_0(\\cdot\\mid d)\\,\\|\\,P\\_1(\\cdot\\mid d)\\right)$ from $\\mathcal{M}\\_0$ to $\\mathcal{M}\\_1$ as a function of $d$, expressed solely in terms of $\\lambda\\_0(d)$ and $\\lambda\\_1(d)$. Provide your final expression in closed form. No numerical evaluation is required, and no rounding is needed. The final answer must be a single analytic expression and must not include units.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and contains all necessary information for a unique mathematical derivation. The scenario described is a standard application of information theory to model discrimination in quantitative biology.\n\nThe objective is to derive a closed-form expression for the Kullback–Leibler (KL) divergence from a model $\\mathcal{M}_0$ to a model $\\mathcal{M}_1$, denoted as $D_{KL}\\!\\left(P_0(\\cdot\\mid d)\\,\\|\\,P_1(\\cdot\\mid d)\\right)$. The models describe the probability distribution of a discrete random variable $Y_d$, the mRNA count, which can take values in the set of non-negative integers $y \\in \\{0, 1, 2, \\ldots\\}$.\n\nThe foundational definition of the Kullback–Leibler divergence for two discrete probability mass functions (PMFs), $p_0(y)$ and $p_1(y)$, defined over the same sample space $\\mathcal{Y}$, is given by:\n$$\nD_{KL}(P_0 \\,\\|\\, P_1) = \\sum_{y \\in \\mathcal{Y}} p_0(y) \\ln\\left(\\frac{p_0(y)}{p_1(y)}\\right)\n$$\nIn this expression, $\\ln(\\cdot)$ denotes the natural logarithm. The summation is taken over all possible outcomes $y$.\n\nFor the given problem, the two distributions are $P_0(\\cdot\\mid d)$ and $P_1(\\cdot\\mid d)$, corresponding to models $\\mathcal{M}_0$ and $\\mathcal{M}_1$. Both are Poisson distributions with respective PMFs:\n$$\np_0(y \\mid d) = \\frac{\\exp(-\\lambda_0(d))\\,\\lambda_0(d)^y}{y!}\n$$\n$$\np_1(y \\mid d) = \\frac{\\exp(-\\lambda_1(d))\\,\\lambda_1(d)^y}{y!}\n$$\nThe sample space is $\\mathcal{Y}=\\{0, 1, 2, \\ldots\\}$. Applying the KL divergence definition, we have:\n$$\nD_{KL}\\!\\left(P_0(\\cdot\\mid d)\\,\\|\\,P_1(\\cdot\\mid d)\\right) = \\sum_{y=0}^{\\infty} p_0(y \\mid d) \\ln\\left(\\frac{p_0(y \\mid d)}{p_1(y \\mid d)}\\right)\n$$\nFirst, we analyze the term inside the logarithm, which is the ratio of the two PMFs:\n$$\n\\frac{p_0(y \\mid d)}{p_1(y \\mid d)} = \\frac{\\frac{\\exp(-\\lambda_0(d))\\,\\lambda_0(d)^y}{y!}}{\\frac{\\exp(-\\lambda_1(d))\\,\\lambda_1(d)^y}{y!}}\n$$\nThe factorial terms $y!$ cancel out. We can rearrange the remaining terms:\n$$\n\\frac{p_0(y \\mid d)}{p_1(y \\mid d)} = \\frac{\\exp(-\\lambda_0(d))}{\\exp(-\\lambda_1(d))} \\cdot \\frac{\\lambda_0(d)^y}{\\lambda_1(d)^y} = \\exp(\\lambda_1(d) - \\lambda_0(d)) \\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right)^y\n$$\nNext, we take the natural logarithm of this ratio. Using the properties of logarithms, $\\ln(ab) = \\ln(a) + \\ln(b)$ and $\\ln(a^b) = b\\ln(a)$:\n$$\n\\ln\\left(\\frac{p_0(y \\mid d)}{p_1(y \\mid d)}\\right) = \\ln\\left(\\exp(\\lambda_1(d) - \\lambda_0(d))\\right) + \\ln\\left(\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right)^y\\right)\n$$\n$$\n\\ln\\left(\\frac{p_0(y \\mid d)}{p_1(y \\mid d)}\\right) = (\\lambda_1(d) - \\lambda_0(d)) + y \\ln\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right)\n$$\nThe condition that $\\lambda_0(d) > 0$ and $\\lambda_1(d) > 0$ ensures that the argument of the logarithm is well-defined and positive.\n\nNow, we substitute this expression back into the summation for the KL divergence:\n$$\nD_{KL}\\!\\left(P_0(\\cdot\\mid d)\\,\\|\\,P_1(\\cdot\\mid d)\\right) = \\sum_{y=0}^{\\infty} p_0(y \\mid d) \\left[ (\\lambda_1(d) - \\lambda_0(d)) + y \\ln\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right) \\right]\n$$\nWe can distribute the $p_0(y \\mid d)$ term and use the linearity of the summation to split the expression into two parts:\n$$\nD_{KL} = \\sum_{y=0}^{\\infty} p_0(y \\mid d) (\\lambda_1(d) - \\lambda_0(d)) + \\sum_{y=0}^{\\infty} p_0(y \\mid d) \\cdot y \\ln\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right)\n$$\nLet's evaluate each summation separately. For the first term, the factor $(\\lambda_1(d) - \\lambda_0(d))$ is a constant with respect to the summation index $y$:\n$$\n\\sum_{y=0}^{\\infty} p_0(y \\mid d) (\\lambda_1(d) - \\lambda_0(d)) = (\\lambda_1(d) - \\lambda_0(d)) \\sum_{y=0}^{\\infty} p_0(y \\mid d)\n$$\nThe sum of a PMF over its entire sample space is, by definition, equal to $1$. Thus, $\\sum_{y=0}^{\\infty} p_0(y \\mid d) = 1$. The first term simplifies to:\n$$\n\\lambda_1(d) - \\lambda_0(d)\n$$\nFor the second term, the factor $\\ln\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right)$ is also a constant with respect to $y$:\n$$\n\\sum_{y=0}^{\\infty} p_0(y \\mid d) \\cdot y \\ln\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right) = \\ln\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right) \\sum_{y=0}^{\\infty} y \\cdot p_0(y \\mid d)\n$$\nThe summation $\\sum_{y=0}^{\\infty} y \\cdot p_0(y \\mid d)$ is the definition of the expected value, or mean, of the random variable $Y_d$ under the distribution $P_0(\\cdot \\mid d)$. For a Poisson distribution with parameter $\\lambda_0(d)$, the mean is precisely $\\lambda_0(d)$. Therefore:\n$$\n\\sum_{y=0}^{\\infty} y \\cdot p_0(y \\mid d) = \\mathbb{E}_{Y_d \\sim P_0}[Y_d] = \\lambda_0(d)\n$$\nThe second term simplifies to:\n$$\n\\lambda_0(d) \\ln\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right)\n$$\nCombining the two simplified parts gives the final expression for the KL divergence:\n$$\nD_{KL}\\!\\left(P_0(\\cdot\\mid d)\\,\\|\\,P_1(\\cdot\\mid d)\\right) = (\\lambda_1(d) - \\lambda_0(d)) + \\lambda_0(d) \\ln\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right)\n$$\nReordering the terms for clarity yields the final closed-form analytic expression:\n$$\nD_{KL}\\!\\left(P_0(\\cdot\\mid d)\\,\\|\\,P_1(\\cdot\\mid d)\\right) = \\lambda_0(d) \\ln\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right) - \\lambda_0(d) + \\lambda_1(d)\n$$\nThis expression is a function of the design parameter $d$ through the model-predicted means $\\lambda_0(d)$ and $\\lambda_1(d)$, as required.",
            "answer": "$$\n\\boxed{\\lambda_0(d) \\ln\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right) - \\lambda_0(d) + \\lambda_1(d)}\n$$"
        },
        {
            "introduction": "With a tool to measure model distinguishability in hand, a crucial question arises: is an experiment designed to best estimate a model's parameters also the best for telling it apart from a rival model? This exercise explores the subtle but critical difference between optimal design for parameter estimation (e.g., $D$-optimality) and optimal design for model discrimination. Through a carefully constructed counterexample involving common Hill-type models, you will uncover a scenario where the best experiment for learning about a parameter is precisely the worst for distinguishing between two competing hypotheses, a vital lesson for any experimental designer. ",
            "id": "3924608",
            "problem": "Consider two candidate dynamical input-output models for a synthetic gene expression module measured at steady state under a single extracellular inducer concentration. The output signal is modeled with additive Gaussian noise. Let the experimental design consist of choosing a single inducer level $x \\in (0,\\infty)$, and acquiring one noisy measurement $y^{\\mathrm{obs}}$ of the latent mean output $\\mu(x)$ with independent, identically distributed Gaussian noise of known variance $\\sigma^{2}$.\n\nThe two candidate models are Hill-type input-output maps with identical maximal activity $V$ and half-activation parameter $K$, but differing Hill coefficients. Model $M_{1}$ (Michaelis-Menten form, Hill coefficient $n=1$) is\n$$\n\\mu_{1}(x;V,K) \\;=\\; \\frac{V\\,x}{K + x},\n$$\nand model $M_{2}$ (cubic Hill form, Hill coefficient $n=3$) is\n$$\n\\mu_{2}(x;V,K) \\;=\\; \\frac{V\\,x^{3}}{K^{3} + x^{3}}.\n$$\n\nThe measurement model is $y^{\\mathrm{obs}}(x) = \\mu_{j}(x;V,K) + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2})$ and $j \\in \\{1,2\\}$ denotes the model. Assume the nominal parameter values $V_{0} = 1$ and $K_{0} = 1$ are accurate, and the design is selected locally at these nominal values.\n\nDefinitions to use:\n- The Fisher Information Matrix (FIM) for a scalar parameter $\\theta$ under Gaussian noise is $I(\\theta;x) = \\frac{1}{\\sigma^{2}}\\left(\\frac{\\partial \\mu(x;\\theta)}{\\partial \\theta}\\right)^{2}$. Determinant optimality (D-optimality) for a single parameter reduces to maximizing $I(\\theta;x)$ with respect to $x$.\n- The Kullback-Leibler (KL) divergence from a Gaussian model with mean $\\mu_{a}(x)$ to another with mean $\\mu_{b}(x)$ and identical variance $\\sigma^{2}$ is \n$$\nD_{\\mathrm{KL}}(a\\parallel b;x) \\;=\\; \\frac{1}{2\\,\\sigma^{2}}\\left(\\mu_{a}(x) - \\mu_{b}(x)\\right)^{2}.\n$$\n\nTask:\n1. Starting from these definitions, determine the locally determinant-optimal design $x^{\\mathrm{D}}$ for estimating the scalar parameter $K$ in model $M_{1}$ at $(V_{0},K_{0})$ with a single observation.\n2. Using the same nominal values, evaluate the KL divergence $D_{\\mathrm{KL}}(M_{1}\\parallel M_{2};x)$ at $x^{\\mathrm{D}}$, and argue whether this design maximizes or minimizes model distinguishability.\n3. Exhibit at least one explicit input $x \\in (0,\\infty)$ for which $D_{\\mathrm{KL}}(M_{1}\\parallel M_{2};x) > 0$ under $(V_{0},K_{0})$.\n4. Let $x^{\\mathrm{KL}}$ denote any input that strictly increases the KL divergence relative to $x^{\\mathrm{D}}$ under $(V_{0},K_{0})$. Compute the ratio\n$$\nR \\;=\\; \\frac{D_{\\mathrm{KL}}(M_{1}\\parallel M_{2};x^{\\mathrm{D}})}{D_{\\mathrm{KL}}(M_{1}\\parallel M_{2};x^{\\mathrm{KL}})}.\n$$\n\nProvide the final value of $R$ as your answer. No units are required. If you find a numerical value, do not round unless instructed; exact values are preferred.",
            "solution": "The problem is validated as self-contained, scientifically grounded, and well-posed. The questions, while revealing a subtle point about experimental design, are formally specified and admit a unique, verifiable solution. We proceed with the solution by addressing each task in sequence.\n\nThe two candidate models are given by the mean output functions:\n$$\n\\mu_{1}(x;V,K) \\;=\\; \\frac{V\\,x}{K + x}\n$$\n$$\n\\mu_{2}(x;V,K) \\;=\\; \\frac{V\\,x^{3}}{K^{3} + x^{3}}\n$$\nThe nominal parameter values are $V_{0} = 1$ and $K_{0} = 1$. The measurement noise is Gaussian with mean $0$ and variance $\\sigma^2$.\n\nFirst, we address the determination of the locally determinant-optimal design $x^{\\mathrm{D}}$ for estimating the parameter $K$ in model $M_{1}$. For a single parameter, D-optimality is equivalent to maximizing the Fisher Information. The Fisher Information for parameter $K$ is given by:\n$$\nI(K;x) = \\frac{1}{\\sigma^{2}}\\left(\\frac{\\partial \\mu_{1}(x;V,K)}{\\partial K}\\right)^{2}\n$$\nWe must first compute the partial derivative of $\\mu_{1}$ with respect to $K$:\n$$\n\\frac{\\partial \\mu_{1}}{\\partial K} = \\frac{\\partial}{\\partial K}\\left(\\frac{V\\,x}{K + x}\\right) = V\\,x \\cdot \\frac{\\partial}{\\partial K}\\left((K+x)^{-1}\\right) = V\\,x \\cdot (-1)(K+x)^{-2} = -\\frac{V\\,x}{(K+x)^{2}}\n$$\nSubstituting this into the expression for the Fisher Information gives:\n$$\nI(K;x) = \\frac{1}{\\sigma^{2}}\\left(-\\frac{V\\,x}{(K+x)^{2}}\\right)^{2} = \\frac{V^{2}x^{2}}{\\sigma^{2}(K+x)^{4}}\n$$\nTo find the locally optimal design, we evaluate this expression at the nominal parameter values $V_{0}=1$ and $K_{0}=1$:\n$$\nI(K;x)\\big|_{V=1,K=1} = \\frac{1^{2}x^{2}}{\\sigma^{2}(1+x)^{4}} = \\frac{1}{\\sigma^{2}}\\frac{x^{2}}{(1+x)^{4}}\n$$\nTo maximize $I(K;x)$ with respect to the input $x \\in (0, \\infty)$, we need to maximize the function $f(x) = \\frac{x^{2}}{(1+x)^{4}}$, since $\\frac{1}{\\sigma^2}$ is a positive constant. We find the critical points by setting the first derivative of $f(x)$ to zero:\n$$\nf'(x) = \\frac{d}{dx}\\left(\\frac{x^{2}}{(1+x)^{4}}\\right) = \\frac{2x(1+x)^{4} - x^{2} \\cdot 4(1+x)^{3}}{((1+x)^{4})^{2}} = \\frac{2x(1+x) - 4x^{2}}{(1+x)^{5}}\n$$\nSetting the numerator to zero for $x>0$:\n$$\n2x(1+x) - 4x^{2} = 0\n$$\n$$\n2x + 2x^{2} - 4x^{2} = 0\n$$\n$$\n2x - 2x^{2} = 0\n$$\n$$\n2x(1-x) = 0\n$$\nSince $x \\in (0,\\infty)$, the only critical point is $x=1$. To confirm this is a maximum, we can examine the sign of $f'(x)$. The denominator $(1+x)^{5}$ is positive for $x > 0$. The numerator $2x(1-x)$ is positive for $x \\in (0,1)$ and negative for $x \\in (1,\\infty)$. Thus, $f(x)$ increases for $x<1$ and decreases for $x>1$, confirming that $x=1$ is a local maximum. The locally D-optimal design is therefore $x^{\\mathrm{D}} = 1$.\n\nSecond, we evaluate the Kullback-Leibler (KL) divergence $D_{\\mathrm{KL}}(M_{1}\\parallel M_{2};x)$ at this optimal design point $x^{\\mathrm{D}}=1$. The KL divergence is defined as:\n$$\nD_{\\mathrm{KL}}(M_{1}\\parallel M_{2};x) = \\frac{1}{2\\sigma^{2}}(\\mu_{1}(x) - \\mu_{2}(x))^{2}\n$$\nWe evaluate the model outputs at the nominal parameters $(V_{0}=1, K_{0}=1)$ and at the design point $x^{\\mathrm{D}}=1$:\n$$\n\\mu_{1}(x=1; V=1, K=1) = \\frac{1 \\cdot 1}{1+1} = \\frac{1}{2}\n$$\n$$\n\\mu_{2}(x=1; V=1, K=1) = \\frac{1 \\cdot 1^{3}}{1^{3}+1^{3}} = \\frac{1}{1+1} = \\frac{1}{2}\n$$\nThe outputs of the two models are identical at $x=1$. Substituting these into the KL divergence formula:\n$$\nD_{\\mathrm{KL}}(M_{1}\\parallel M_{2}; x^{\\mathrm{D}}=1) = \\frac{1}{2\\sigma^{2}}\\left(\\frac{1}{2} - \\frac{1}{2}\\right)^{2} = \\frac{1}{2\\sigma^{2}}(0)^{2} = 0\n$$\nThe KL divergence is a measure of the information lost when one model is used to approximate another; it quantifies the distinguishability of two probabilistic models. A KL divergence of $0$ indicates that the models are indistinguishable at the given experimental condition. Therefore, the design $x^{\\mathrm{D}}=1$, which is optimal for estimating parameter $K$ in model $M_1$, is simultaneously the worst possible design for discriminating between model $M_1$ and model $M_2$, as it completely minimizes their distinguishability.\n\nThird, we must exhibit an explicit input $x \\in (0,\\infty)$ for which $D_{\\mathrm{KL}}(M_{1}\\parallel M_{2};x) > 0$. The KL divergence is zero if and only if $\\mu_{1}(x) = \\mu_{2}(x)$. At the nominal parameters, this is:\n$$\n\\frac{x}{1+x} = \\frac{x^{3}}{1+x^{3}}\n$$\nFor $x>0$, we can multiply by $(1+x)(1+x^3)$ and divide by $x$:\n$$\n1+x^{3} = x^{2}(1+x) \\implies 1+x^{3} = x^{2}+x^{3} \\implies 1 = x^{2}\n$$\nSince $x \\in (0,\\infty)$, the only solution is $x=1$. Thus, for any $x \\in (0,\\infty)$ such that $x \\neq 1$, the model outputs will differ, and the KL divergence will be strictly positive. As an explicit example, we can choose $x=2$:\n$$\n\\mu_{1}(2) = \\frac{2}{1+2} = \\frac{2}{3}\n$$\n$$\n\\mu_{2}(2) = \\frac{2^{3}}{1+2^{3}} = \\frac{8}{9}\n$$\nSince $\\mu_{1}(2) \\neq \\mu_{2}(2)$, the KL divergence is positive: $D_{\\mathrm{KL}}(M_{1}\\parallel M_{2};2) = \\frac{1}{2\\sigma^{2}}(\\frac{2}{3}-\\frac{8}{9})^2 > 0$.\n\nFourth, we compute the ratio $R = \\frac{D_{\\mathrm{KL}}(M_{1}\\parallel M_{2};x^{\\mathrm{D}})}{D_{\\mathrm{KL}}(M_{1}\\parallel M_{2};x^{\\mathrm{KL}})}$.\nWe have already found the numerator to be:\n$$\nD_{\\mathrm{KL}}(M_{1}\\parallel M_{2};x^{\\mathrm{D}}) = 0\n$$\nThe problem defines $x^{\\mathrm{KL}}$ as any input that strictly increases the KL divergence relative to $x^{\\mathrm{D}}$, which means:\n$$\nD_{\\mathrm{KL}}(M_{1}\\parallel M_{2};x^{\\mathrm{KL}}) > D_{\\mathrm{KL}}(M_{1}\\parallel M_{2};x^{\\mathrm{D}})\n$$\nSubstituting the value of the numerator, this condition is:\n$$\nD_{\\mathrm{KL}}(M_{1}\\parallel M_{2};x^{\\mathrm{KL}}) > 0\n$$\nFrom the third part of our analysis, we know that such an $x^{\\mathrm{KL}}$ exists; any $x \\in (0,\\infty)$ except $x=1$ satisfies this condition. For any such choice of $x^{\\mathrm{KL}}$, the denominator of the ratio $R$ is a strictly positive number. The ratio is therefore:\n$$\nR = \\frac{0}{D_{\\mathrm{KL}}(M_{1}\\parallel M_{2};x^{\\mathrm{KL}})} = 0\n$$\nThis result is independent of the specific choice of $x^{\\mathrm{KL}}$, as long as it satisfies the given condition. The final value of $R$ is $0$.",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "This final practice transitions from theory and conceptual examples to a full-fledged computational design task, a scenario commonly faced in synthetic biology. You are challenged to determine the most informative gene to perturb using CRISPRi in order to distinguish between two plausible network topologies: a negative feedback loop and a coherent feedforward loop. By implementing a computational workflow that maximizes the expected log Bayes factor—a criterion that we will see is directly related to the KL divergence—you will translate the principles of optimal design into a practical algorithm for making data-driven experimental decisions. ",
            "id": "3924592",
            "problem": "Design and implement a program that computes the optimal single-gene Clustered Regularly Interspaced Short Palindromic Repeats interference (CRISPRi) target to discriminate between two alternative gene-regulatory network models in synthetic biology: a negative feedback topology versus a feedforward topology. The decision-theoretic criterion is to maximize the expected logarithm of the Bayes factor over the data-generating process.\n\nThe foundational base consists of the following widely accepted definitions and models:\n- Bayesian model selection defines the Bayes factor between two models as the likelihood ratio. For an outcome vector $\\mathbf{y}$, the Bayes factor comparing model $\\mathcal{M}_1$ to model $\\mathcal{M}_2$ is $B_{12}(\\mathbf{y}) = \\dfrac{p(\\mathbf{y}\\mid \\mathcal{M}_1)}{p(\\mathbf{y}\\mid \\mathcal{M}_2)}$, and the expected log Bayes factor under a specified data-generating distribution is $\\mathbb{E}[\\log B_{12}(\\mathbf{y})]$.\n- The measurement model is additive independent Gaussian noise: at measurement times $t_k$ one observes $y_k = h^\\top \\mathbf{x}(t_k) + \\varepsilon_k$, with $h = [0, 0, 1]^\\top$, $\\varepsilon_k \\sim \\mathcal{N}(0,\\sigma^2)$ independently across $k$, and $\\mathbf{x}(t)$ the state vector of gene expression deviations from steady state (dimensionless).\n- The gene expression dynamics for each model are approximated by a linear time-invariant ordinary differential equation (ODE) of the form $\\dfrac{d\\mathbf{x}}{dt} = A\\,\\mathbf{x} + \\mathbf{d}$, where $A \\in \\mathbb{R}^{3\\times 3}$ is a stable system matrix and $\\mathbf{d} \\in \\mathbb{R}^3$ is a constant input capturing CRISPRi knockdown applied for $t \\ge 0$. The initial condition is $\\mathbf{x}(0) = \\mathbf{0}$ (start from steady state). The exact solution to this ODE with constant input is the well-tested formula\n$$\n\\mathbf{x}(t) \\;=\\; \\int_0^t e^{A (t-\\tau)} \\,\\mathbf{d}\\, d\\tau \\;=\\; A^{-1}\\big(e^{A t} - I\\big)\\,\\mathbf{d},\n$$\nwhere $I$ is the identity matrix and $e^{A t}$ is the matrix exponential.\n- The design variable is the CRISPRi target index $i \\in \\{0,1,2\\}$ corresponding to gene indices $\\{A,B,C\\}$, where the constant input is $\\mathbf{d} = -\\alpha\\, \\mathbf{e}_i$ with $\\alpha > 0$ and $\\mathbf{e}_i$ the standard basis vector.\n\nThe two models to be discriminated are:\n- $\\mathcal{M}_\\mathrm{FB}$ (negative feedback topology): matrix $A_\\mathrm{FB}$ encodes $C \\to A$ inhibition, $A \\to B$ activation, and $B \\to C$ activation.\n- $\\mathcal{M}_\\mathrm{FF}$ (feedforward topology): matrix $A_\\mathrm{FF}$ encodes $A \\to B$ activation, $A \\to C$ activation, and $B \\to C$ activation, with no feedback from $C$ to $A$.\n\nFor a given design (target index $i$), measurement times $\\{t_k\\}_{k=1}^K$, noise standard deviation $\\sigma$, input amplitude $\\alpha$, and a pair of model matrices $(A_\\mathrm{FB}, A_\\mathrm{FF})$, the program must:\n1. Compute the mean predicted outputs under each model, $\\boldsymbol{\\mu}_\\mathrm{FB} \\in \\mathbb{R}^K$ and $\\boldsymbol{\\mu}_\\mathrm{FF} \\in \\mathbb{R}^K$, where the $k$-th component is $\\mu(t_k) = h^\\top \\mathbf{x}(t_k)$ with the appropriate $A$.\n2. Using the Gaussian observation model, compute the expected logarithm of the Bayes factor for discriminating $\\mathcal{M}_\\mathrm{FB}$ versus $\\mathcal{M}_\\mathrm{FF}$ under a symmetric design utility that averages over the two models as data-generating processes with equal prior probabilities. Do not assume any shortcut formula in your derivation or implementation; start from the definitions given above.\n3. Select the target index $i^\\star \\in \\{0,1,2\\}$ that maximizes this expected log Bayes factor. If multiple indices achieve the same maximum within an absolute tolerance of $\\epsilon = 10^{-9}$, return the smallest such index.\n\nAll variables are dimensionless, and there are no physical units. Angles do not appear. Probabilities, likelihoods, and Bayes factors are dimensionless by definition.\n\nTest suite. Your program must compute the optimal target index for each of the following parameter sets. For each case, list the result as an integer in $\\{0,1,2\\}$, where $0$ denotes targeting $A$, $1$ denotes targeting $B$, and $2$ denotes targeting $C$.\n\n- Case $1$ (baseline, moderate times):\n  - $A_\\mathrm{FB} = \\begin{bmatrix} -1.2 & 0.0 & -0.5 \\\\ 0.7 & -1.1 & 0.0 \\\\ 0.0 & 0.6 & -1.0 \\end{bmatrix}$,\n    $A_\\mathrm{FF} = \\begin{bmatrix} -1.2 & 0.0 & 0.0 \\\\ 0.8 & -1.1 & 0.0 \\\\ 0.5 & 0.4 & -1.0 \\end{bmatrix}$.\n  - Times $\\{0.5, 1.0, 2.0, 3.0\\}$.\n  - Noise standard deviation $\\sigma = 0.1$.\n  - Input amplitude $\\alpha = 0.5$.\n- Case $2$ (edge, instantaneous measurement):\n  - Same $A_\\mathrm{FB}$ and $A_\\mathrm{FF}$ as Case $1$.\n  - Times $\\{0.0\\}$.\n  - Noise standard deviation $\\sigma = 0.1$.\n  - Input amplitude $\\alpha = 0.5$.\n- Case $3$ (early dynamics, low noise):\n  - Same $A_\\mathrm{FB}$ and $A_\\mathrm{FF}$ as Case $1$.\n  - Times $\\{0.1, 0.2, 0.3, 0.4\\}$.\n  - Noise standard deviation $\\sigma = 0.05$.\n  - Input amplitude $\\alpha = 0.5$.\n- Case $4$ (alternative topology strengths):\n  - $A_\\mathrm{FB} = \\begin{bmatrix} -1.3 & 0.0 & -0.05 \\\\ 0.6 & -1.2 & 0.0 \\\\ 0.0 & 0.5 & -1.1 \\end{bmatrix}$,\n    $A_\\mathrm{FF} = \\begin{bmatrix} -1.3 & 0.0 & 0.0 \\\\ 0.6 & -1.2 & 0.0 \\\\ 1.0 & 0.5 & -1.1 \\end{bmatrix}$.\n  - Times $\\{1.0, 2.0, 3.0\\}$.\n  - Noise standard deviation $\\sigma = 0.2$.\n  - Input amplitude $\\alpha = 0.5$.\n\nFinal output format. Your program should produce a single line of output containing the results for the four cases as a comma-separated list enclosed in square brackets, for example $[r_1,r_2,r_3,r_4]$, where each $r_j$ is the selected integer target index for Case $j$.",
            "solution": "### Problem Validation\n\n#### Step 1: Extract Givens\n\nThe problem provides the following definitions, models, and data:\n- **Models**: Two alternative gene-regulatory network models, $\\mathcal{M}_\\mathrm{FB}$ (negative feedback) and $\\mathcal{M}_\\mathrm{FF}$ (feedforward).\n- **Dynamics**: Linear time-invariant ODE $\\dfrac{d\\mathbf{x}}{dt} = A\\,\\mathbf{x} + \\mathbf{d}$, with $\\mathbf{x} \\in \\mathbb{R}^3$, $A \\in \\mathbb{R}^{3\\times 3}$ a stable matrix, and $\\mathbf{d} \\in \\mathbb{R}^3$ a constant input.\n- **Initial Condition**: $\\mathbf{x}(0) = \\mathbf{0}$.\n- **ODE Solution**: $\\mathbf{x}(t) = A^{-1}\\big(e^{A t} - I\\big)\\,\\mathbf{d}$.\n- **Measurement Model**: $y_k = h^\\top \\mathbf{x}(t_k) + \\varepsilon_k$, with observation vector $h = [0, 0, 1]^\\top$. The noise is independent and identically distributed, $\\varepsilon_k \\sim \\mathcal{N}(0,\\sigma^2)$.\n- **Design Variable**: CRISPRi target index $i \\in \\{0,1,2\\}$ for genes $\\{A,B,C\\}$.\n- **Input Vector**: The knockdown is modeled as $\\mathbf{d} = -\\alpha\\, \\mathbf{e}_i$, where $\\alpha > 0$ and $\\mathbf{e}_i$ is the standard basis vector.\n- **Bayesian Model Selection**: The Bayes factor is $B_{12}(\\mathbf{y}) = \\dfrac{p(\\mathbf{y}\\mid \\mathcal{M}_1)}{p(\\mathbf{y}\\mid \\mathcal{M}_2)}$.\n- **Objective Function**: Maximize the expected log Bayes factor, $\\mathbb{E}[\\log B_{12}(\\mathbf{y})]$, under a symmetric utility with equal prior probabilities for each model being the data source.\n- **Tie-breaking Rule**: If multiple indices achieve the maximum utility within an absolute tolerance of $\\epsilon = 10^{-9}$, return the smallest such index.\n- **Case 1 Parameters**:\n  - $A_\\mathrm{FB} = \\begin{bmatrix} -1.2 & 0.0 & -0.5 \\\\ 0.7 & -1.1 & 0.0 \\\\ 0.0 & 0.6 & -1.0 \\end{bmatrix}$, $A_\\mathrm{FF} = \\begin{bmatrix} -1.2 & 0.0 & 0.0 \\\\ 0.8 & -1.1 & 0.0 \\\\ 0.5 & 0.4 & -1.0 \\end{bmatrix}$.\n  - Times $\\{0.5, 1.0, 2.0, 3.0\\}$.\n  - Noise standard deviation $\\sigma = 0.1$.\n  - Input amplitude $\\alpha = 0.5$.\n- **Case 2 Parameters**:\n  - Same $A_\\mathrm{FB}$, $A_\\mathrm{FF}$, $\\sigma$, $\\alpha$ as Case $1$.\n  - Times $\\{0.0\\}$.\n- **Case 3 Parameters**:\n  - Same $A_\\mathrm{FB}$, $A_\\mathrm{FF}$, $\\alpha$ as Case $1$.\n  - Times $\\{0.1, 0.2, 0.3, 0.4\\}$.\n  - Noise standard deviation $\\sigma = 0.05$.\n- **Case 4 Parameters**:\n  - $A_\\mathrm{FB} = \\begin{bmatrix} -1.3 & 0.0 & -0.05 \\\\ 0.6 & -1.2 & 0.0 \\\\ 0.0 & 0.5 & -1.1 \\end{bmatrix}$, $A_\\mathrm{FF} = \\begin{bmatrix} -1.3 & 0.0 & 0.0 \\\\ 0.6 & -1.2 & 0.0 \\\\ 1.0 & 0.5 & -1.1 \\end{bmatrix}$.\n  - Times $\\{1.0, 2.0, 3.0\\}$.\n  - Noise standard deviation $\\sigma = 0.2$.\n  - Input amplitude $\\alpha = 0.5$.\n\n#### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded**: The problem is well-grounded in systems and synthetic biology. Linear ODEs are a standard approximation for gene network dynamics, CRISPRi is a standard perturbation technique, and Bayesian model selection is a primary statistical framework for hypothesis testing. The provided ODE solution is mathematically correct for a constant input. All stated principles are standard and sound. The matrices are confirmed to be stable, ensuring well-behaved dynamics and invertibility.\n- **Well-Posed**: The problem asks to maximize a well-defined objective function over a finite discrete set of choices $\\{0,1,2\\}$. This guarantees that a maximum exists. The explicit tie-breaking rule ensures a unique solution.\n- **Objective**: The problem is stated in precise, quantitative, and unbiased language.\n- **Incomplete or Contradictory Setup**: The problem is self-contained. All necessary matrices, parameters ($\\alpha, \\sigma$), initial conditions, and measurement times are provided for each case. There are no contradictions.\n- **Unrealistic or Infeasible**: The problem is a theoretical modeling exercise. The chosen values are dimensionless and represent a plausible scenario within such a modeling context.\n- **Other Flaws**: The problem is not trivial, requiring derivation of the objective function and numerical computation involving matrix algebra. It is not metaphorical, circular, or unverifiable.\n\n#### Step 3: Verdict and Action\n\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation and Algorithm\n\nThe goal is to select the CRISPRi target index $i \\in \\{0, 1, 2\\}$ that maximizes the ability to discriminate between two models, $\\mathcal{M}_\\mathrm{FB}$ and $\\mathcal{M}_\\mathrm{FF}$. The specified criterion is the expected logarithm of the Bayes factor, averaged over both models as potential data-generating processes.\n\nLet $\\mathcal{M}_1$ represent $\\mathcal{M}_\\mathrm{FB}$ and $\\mathcal{M}_2$ represent $\\mathcal{M}_\\mathrm{FF}$. The objective function for a design choice $i$ is the utility $U(i)$:\n$$\nU(i) = \\mathbb{E}_{\\mathbf{y}}[\\log B_{12}(\\mathbf{y})]\n$$\nThe problem specifies a symmetric design utility, meaning the expectation is taken over the data distribution $p(\\mathbf{y})$ which is a mixture of the distributions under each model, with equal prior probabilities $P(\\mathcal{M}_1) = P(\\mathcal{M}_2) = 1/2$:\n$$\np(\\mathbf{y}) = p(\\mathbf{y} | \\mathcal{M}_1)P(\\mathcal{M}_1) + p(\\mathbf{y} | \\mathcal{M}_2)P(\\mathcal{M}_2) = \\frac{1}{2} p(\\mathbf{y} | \\mathcal{M}_1) + \\frac{1}{2} p(\\mathbf{y} | \\mathcal{M}_2)\n$$\nSubstituting this into the expectation and using the definition of the Bayes factor $B_{12}(\\mathbf{y}) = p(\\mathbf{y}|\\mathcal{M}_1) / p(\\mathbf{y}|\\mathcal{M}_2)$, we have:\n$$\nU(i) = \\int \\left( \\frac{1}{2} p(\\mathbf{y}|\\mathcal{M}_1) + \\frac{1}{2} p(\\mathbf{y}|\\mathcal{M}_2) \\right) \\log \\frac{p(\\mathbf{y}|\\mathcal{M}_1)}{p(\\mathbf{y}|\\mathcal{M}_2)} d\\mathbf{y}\n$$\n$$\nU(i) = \\frac{1}{2} \\int p(\\mathbf{y}|\\mathcal{M}_1) \\log \\frac{p(\\mathbf{y}|\\mathcal{M}_1)}{p(\\mathbf{y}|\\mathcal{M}_2)} d\\mathbf{y} + \\frac{1}{2} \\int p(\\mathbf{y}|\\mathcal{M}_2) \\log \\frac{p(\\mathbf{y}|\\mathcal{M}_1)}{p(\\mathbf{y}|\\mathcal{M}_2)} d\\mathbf{y}\n$$\nThe first integral is the definition of the Kullback-Leibler (KL) divergence from $\\mathcal{M}_2$ to $\\mathcal{M}_1$, $D_{KL}(p(\\cdot|\\mathcal{M}_1) \\| p(\\cdot|\\mathcal{M}_2))$. The second integral is $-D_{KL}(p(\\cdot|\\mathcal{M}_2) \\| p(\\cdot|\\mathcal{M}_1))$. Thus, the utility is half the sum of these two divergences, known as the Jeffreys divergence:\n$$\nU(i) = \\frac{1}{2} \\left[ D_{KL}(p(\\cdot|\\mathcal{M}_1) \\| p(\\cdot|\\mathcal{M}_2)) + D_{KL}(p(\\cdot|\\mathcal{M}_2) \\| p(\\cdot|\\mathcal{M}_1)) \\right]\n$$\nThe measurement model states that observations $\\mathbf{y} \\in \\mathbb{R}^K$ (for $K$ time points) are corrupted by i.i.d. Gaussian noise, $\\varepsilon_k \\sim \\mathcal{N}(0, \\sigma^2)$. Under model $\\mathcal{M}_j$, the observation vector $\\mathbf{y}$ is distributed as a multivariate normal: $p(\\mathbf{y}|\\mathcal{M}_j) = \\mathcal{N}(\\mathbf{y} | \\boldsymbol{\\mu}_j, \\Sigma)$, where $\\boldsymbol{\\mu}_j$ is the vector of mean predictions and the covariance is $\\Sigma = \\sigma^2 I_K$.\n\nThe KL divergence between two multivariate normal distributions $\\mathcal{N}(\\boldsymbol{\\mu}_a, \\Sigma_a)$ and $\\mathcal{N}(\\boldsymbol{\\mu}_b, \\Sigma_b)$ is given by:\n$$\nD_{KL}(\\mathcal{N}_a \\| \\mathcal{N}_b) = \\frac{1}{2} \\left[ \\log \\frac{|\\Sigma_b|}{|\\Sigma_a|} - K + \\mathrm{tr}(\\Sigma_b^{-1} \\Sigma_a) + (\\boldsymbol{\\mu}_b - \\boldsymbol{\\mu}_a)^\\top \\Sigma_b^{-1} (\\boldsymbol{\\mu}_b - \\boldsymbol{\\mu}_a) \\right]\n$$\nIn our case, the covariance matrices are identical, $\\Sigma_1 = \\Sigma_2 = \\Sigma = \\sigma^2 I_K$. The formula simplifies dramatically:\n$$\nD_{KL}(p(\\cdot|\\mathcal{M}_1) \\| p(\\cdot|\\mathcal{M}_2)) = \\frac{1}{2} (\\boldsymbol{\\mu}_2 - \\boldsymbol{\\mu}_1)^\\top \\Sigma^{-1} (\\boldsymbol{\\mu}_2 - \\boldsymbol{\\mu}_1) = \\frac{1}{2\\sigma^2} \\|\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2\\|_2^2\n$$\nSince this expression is symmetric with respect to $\\boldsymbol{\\mu}_1$ and $\\boldsymbol{\\mu}_2$, we have $D_{KL}(p_1 \\| p_2) = D_{KL}(p_2 \\| p_1)$. The utility function becomes:\n$$\nU(i) = \\frac{1}{2} \\left[ \\frac{1}{2\\sigma^2} \\|\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2\\|_2^2 + \\frac{1}{2\\sigma^2} \\|\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2\\|_2^2 \\right] = \\frac{1}{2\\sigma^2} \\|\\boldsymbol{\\mu}_1(i) - \\boldsymbol{\\mu}_2(i)\\|_2^2\n$$\nwhere the mean prediction vectors $\\boldsymbol{\\mu}_j(i)$ depend on the chosen target gene $i$. The optimal design $i^\\star$ is the one that maximizes this utility:\n$$\ni^\\star = \\arg\\max_{i \\in \\{0,1,2\\}} U(i)\n$$\nSince $1/(2\\sigma^2)$ is a positive constant for any given experimental setup, this is equivalent to maximizing the squared Euclidean distance between the two models' predicted mean trajectories:\n$$\ni^\\star = \\arg\\max_{i \\in \\{0,1,2\\}} \\|\\boldsymbol{\\mu}_\\mathrm{FB}(i) - \\boldsymbol{\\mu}_\\mathrm{FF}(i)\\|_2^2 = \\arg\\max_{i \\in \\{0,1,2\\}} \\sum_{k=1}^K (\\mu_\\mathrm{FB}(t_k, i) - \\mu_\\mathrm{FF}(t_k, i))^2\n$$\n\nThe algorithm to find $i^\\star$ is as follows:\n1. For each test case, iterate through each possible CRISPRi target index $i \\in \\{0, 1, 2\\}$.\n2. For each index $i$, define the constant input vector $\\mathbf{d} = -\\alpha \\mathbf{e}_i$, where $\\mathbf{e}_i$ is the $i$-th standard basis vector (e.g., for $i=0$, $\\mathbf{e}_0 = [1, 0, 0]^\\top$).\n3. For each model, $\\mathcal{M}_\\mathrm{FB}$ and $\\mathcal{M}_\\mathrm{FF}$ with system matrices $A_\\mathrm{FB}$ and $A_\\mathrm{FF}$ respectively:\n    a. Compute the vector of mean predictions $\\boldsymbol{\\mu} \\in \\mathbb{R}^K$. For each measurement time $t_k$:\n        i. If $t_k=0$, the prediction is $\\mu(0) = 0$ since $\\mathbf{x}(0)=\\mathbf{0}$.\n        ii. If $t_k > 0$, compute the state vector $\\mathbf{x}(t_k) = A^{-1} (e^{A t_k} - I) \\mathbf{d}$. This requires computing the matrix inverse $A^{-1}$ and the matrix exponential $e^{A t_k}$.\n        iii. The mean prediction is the third component of the state, $\\mu(t_k) = h^\\top \\mathbf{x}(t_k) = [\\mathbf{x}(t_k)]_2$ (using $0$-based indexing).\n4. After computing the prediction vectors $\\boldsymbol{\\mu}_\\mathrm{FB}(i)$ and $\\boldsymbol{\\mu}_\\mathrm{FF}(i)$, calculate the squared Euclidean distance $S_i = \\|\\boldsymbol{\\mu}_\\mathrm{FB}(i) - \\boldsymbol{\\mu}_\\mathrm{FF}(i)\\|_2^2$. This value is proportional to the design utility.\n5. After computing $S_0$, $S_1$, and $S_2$, find the maximum value $S_{\\max}$.\n6. Identify the set of indices $I_{opt} = \\{i \\mid S_{\\max} - S_i \\le \\epsilon \\}$, where $\\epsilon = 10^{-9}$ is the specified tolerance.\n7. The optimal target is the smallest index in this set, $i^\\star = \\min(I_{opt})$.\n8. Repeat for all test cases. For Case 2, where the only measurement time is $t=0$, the predictions for both models are $0$, so the distance is $0$ for all targets. The tie-breaking rule dictates the answer must be $0$.\n\nThis procedure is implemented to solve for the four test cases.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import expm, inv\n\ndef solve():\n    \"\"\"\n    Computes the optimal CRISPRi target for model discrimination for a suite of test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"A_FB\": np.array([[-1.2, 0.0, -0.5], [0.7, -1.1, 0.0], [0.0, 0.6, -1.0]]),\n            \"A_FF\": np.array([[-1.2, 0.0, 0.0], [0.8, -1.1, 0.0], [0.5, 0.4, -1.0]]),\n            \"times\": np.array([0.5, 1.0, 2.0, 3.0]),\n            \"sigma\": 0.1,\n            \"alpha\": 0.5,\n        },\n        {\n            \"A_FB\": np.array([[-1.2, 0.0, -0.5], [0.7, -1.1, 0.0], [0.0, 0.6, -1.0]]),\n            \"A_FF\": np.array([[-1.2, 0.0, 0.0], [0.8, -1.1, 0.0], [0.5, 0.4, -1.0]]),\n            \"times\": np.array([0.0]),\n            \"sigma\": 0.1,\n            \"alpha\": 0.5,\n        },\n        {\n            \"A_FB\": np.array([[-1.2, 0.0, -0.5], [0.7, -1.1, 0.0], [0.0, 0.6, -1.0]]),\n            \"A_FF\": np.array([[-1.2, 0.0, 0.0], [0.8, -1.1, 0.0], [0.5, 0.4, -1.0]]),\n            \"times\": np.array([0.1, 0.2, 0.3, 0.4]),\n            \"sigma\": 0.05,\n            \"alpha\": 0.5,\n        },\n        {\n            \"A_FB\": np.array([[-1.3, 0.0, -0.05], [0.6, -1.2, 0.0], [0.0, 0.5, -1.1]]),\n            \"A_FF\": np.array([[-1.3, 0.0, 0.0], [0.6, -1.2, 0.0], [1.0, 0.5, -1.1]]),\n            \"times\": np.array([1.0, 2.0, 3.0]),\n            \"sigma\": 0.2,\n            \"alpha\": 0.5,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        optimal_index = compute_optimal_target(\n            case[\"A_FB\"], case[\"A_FF\"], case[\"times\"], case[\"alpha\"]\n        )\n        results.append(optimal_index)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef compute_mean_predictions(A, d, times):\n    \"\"\"\n    Computes the mean predicted output trajectory for a given model and input.\n    \n    Args:\n        A (np.ndarray): The system matrix.\n        d (np.ndarray): The constant input vector.\n        times (np.ndarray): The vector of measurement time points.\n\n    Returns:\n        np.ndarray: The vector of mean predictions.\n    \"\"\"\n    predictions = []\n    # Pre-compute the inverse of A as it is constant for all time points\n    A_inv = inv(A)\n    I = np.identity(A.shape[0])\n    \n    for t in times:\n        if t == 0.0:\n            # At t=0, x(0)=0, so the prediction is 0.\n            predictions.append(0.0)\n        else:\n            # Calculate the state x(t) = A_inv * (expm(A*t) - I) * d\n            exp_At = expm(A * t)\n            x_t = A_inv @ (exp_At - I) @ d\n            # The observation is the 3rd component of the state vector (h^T * x)\n            # h = [0, 0, 1]^T\n            prediction = x_t[2]\n            predictions.append(prediction)\n            \n    return np.array(predictions)\n\ndef compute_optimal_target(A_FB, A_FF, times, alpha, num_genes=3, tolerance=1e-9):\n    \"\"\"\n    Finds the optimal gene target to maximize model discriminability.\n\n    Args:\n        A_FB (np.ndarray): System matrix for the feedback model.\n        A_FF (np.ndarray): System matrix for the feedforward model.\n        times (np.ndarray): Measurement time points.\n        alpha (float): CRISPRi knockdown amplitude.\n        num_genes (int): Number of genes in the network.\n        tolerance (float): Absolute tolerance for tie-breaking.\n\n    Returns:\n        int: The optimal target gene index (0, 1, or 2).\n    \"\"\"\n    \n    # As derived, we need to maximize the squared Euclidean distance between\n    # the mean prediction vectors of the two models. The 1/(2*sigma^2) term\n    # is a positive constant and does not affect the argmax.\n    \n    squared_distances = []\n\n    for i in range(num_genes):\n        # Define the input vector d for targeting gene i\n        d = np.zeros(num_genes)\n        d[i] = -alpha\n\n        # Compute mean prediction trajectories for both models\n        mu_FB = compute_mean_predictions(A_FB, d, times)\n        mu_FF = compute_mean_predictions(A_FF, d, times)\n        \n        # Compute the squared Euclidean distance\n        dist_sq = np.sum((mu_FB - mu_FF)**2)\n        squared_distances.append(dist_sq)\n\n    squared_distances = np.array(squared_distances)\n    \n    # Find the index/indices that maximize the distance\n    max_dist = np.max(squared_distances)\n    \n    # Find all indices that are within the tolerance of the maximum\n    candidate_indices = np.where(max_dist - squared_distances <= tolerance)[0]\n    \n    # Return the smallest index among the candidates as per the tie-breaking rule\n    return np.min(candidate_indices)\n\nsolve()\n```"
        }
    ]
}