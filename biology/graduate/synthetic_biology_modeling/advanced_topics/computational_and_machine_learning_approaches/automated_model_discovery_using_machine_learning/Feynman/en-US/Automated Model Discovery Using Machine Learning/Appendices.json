{
    "hands_on_practices": [
        {
            "introduction": "In automated model discovery, we need a principled way to compare different model structures. Bayesian model evidence provides a powerful framework for this, naturally implementing Occam's razor by balancing goodness-of-fit with model complexity. This exercise  will guide you through the derivation of the Laplace approximation, a common technique for estimating model evidence, and show you how to isolate the \"Occam factor\" that quantitatively penalizes models with excessive parameter freedom.",
            "id": "3906825",
            "problem": "In an automated model discovery pipeline for synthetic biology, Bayesian model evidence is used to rank candidate dynamical models for biochemical reaction networks under mass-action kinetics. Consider a candidate model for a single gene product concentration $x(t)$ governed by the ordinary differential equation $dx/dt = k_1 - k_2 x$, where the parameter vector is $k = [k_1, k_2]^{\\top}$ with $k_1$ the production rate and $k_2$ the degradation rate. Let $y = \\{y_i\\}_{i=1}^{N}$ be noisy observations of $x(t)$ at times $\\{t_i\\}_{i=1}^{N}$, with independent Gaussian observation noise of known variance $\\sigma^2$. The automated model discovery task evaluates the marginal likelihood (model evidence) $p(y \\mid \\mathcal{M})$ under a Gaussian prior $p(k \\mid \\mathcal{M}) = \\mathcal{N}(\\mu_0, \\Sigma_0)$ for the parameters.\n\nUsing Bayes’ theorem and a second-order Taylor expansion of the log-posterior density around the maximum a posteriori estimate $k^{\\ast}$, derive the Laplace approximation to the evidence $p(y \\mid \\mathcal{M})$ in terms of the log-likelihood, the log-prior, and the Hessian of the negative log-posterior at $k^{\\ast}$. From this derivation, isolate the Occam factor that penalizes model complexity as the contraction of parameter-space volume from the prior to the posterior. Express this Occam factor entirely in terms of the prior covariance $\\Sigma_0$ and the posterior covariance $\\Sigma_{\\text{post}}$ obtained from the local Gaussian approximation.\n\nThen, for a concrete instance of the pipeline using $d = 2$ parameters, suppose the prior covariance and the negative log-likelihood Hessian at $k^{\\ast}$ (computed from the design and noise model) are given by\n$$\n\\Sigma_0 = \\begin{pmatrix} 0.25 & 0 \\\\ 0 & 0.04 \\end{pmatrix}, \n\\qquad\nH_{\\text{LL}} = \\begin{pmatrix} 400 & -120 \\\\ -120 & 50 \\end{pmatrix}.\n$$\nAssuming the posterior negative log-curvature satisfies $H_{\\text{post}} = H_{\\text{LL}} + \\Sigma_0^{-1}$ and the local Gaussian approximation has covariance $\\Sigma_{\\text{post}} = H_{\\text{post}}^{-1}$, compute the Occam factor under the definition you derived. Express your final answer as a single dimensionless real number and round your answer to four significant figures.",
            "solution": "The problem asks for two main tasks: first, to derive the Laplace approximation for the Bayesian model evidence and isolate the Occam factor representing the contraction of parameter-space volume; second, to compute the value of this Occam factor for a specific numerical instance.\n\n### Part 1: Derivation of the Laplace Approximation and Occam Factor\n\nThe Bayesian model evidence, or marginal likelihood, $p(y \\mid \\mathcal{M})$, is defined by marginalizing the joint probability of data $y$ and parameters $k$ over the entire parameter space:\n$$p(y \\mid \\mathcal{M}) = \\int p(y, k \\mid \\mathcal{M}) dk = \\int p(y \\mid k, \\mathcal{M}) p(k \\mid \\mathcal{M}) dk$$\nwhere $k \\in \\mathbb{R}^d$ is the parameter vector of dimension $d$.\n\nTo evaluate this integral, we employ the Laplace approximation. We first define an \"energy\" function $E(k)$ as the negative logarithm of the integrand:\n$$E(k) = -\\ln[p(y \\mid k, \\mathcal{M}) p(k \\mid \\mathcal{M})] = -\\ln p(y \\mid k, \\mathcal{M}) - \\ln p(k \\mid \\mathcal{M})$$\nNote that $E(k)$ is, up to an additive constant, the negative log-posterior probability. The evidence integral can be rewritten as:\n$$p(y \\mid \\mathcal{M}) = \\int \\exp(-E(k)) dk$$\nThe Laplace approximation is based on a second-order Taylor expansion of $E(k)$ around its minimum. The minimum of $E(k)$ corresponds to the maximum of the posterior probability, which is the maximum a posteriori (MAP) estimate, denoted $k^{\\ast}$. At this point, the gradient of $E(k)$ is zero: $\\nabla_k E(k) |_{k=k^{\\ast}} = 0$.\n\nThe Taylor expansion of $E(k)$ around $k^{\\ast}$ is:\n$$E(k) \\approx E(k^{\\ast}) + (k - k^{\\ast})^{\\top} (\\nabla_k E(k) |_{k=k^{\\ast}}) + \\frac{1}{2} (k - k^{\\ast})^{\\top} (H_{\\text{post}}) (k - k^{\\ast})$$\nwhere $H_{\\text{post}}$ is the Hessian matrix of $E(k)$ evaluated at $k^{\\ast}$:\n$$H_{\\text{post}} = \\nabla_k^2 E(k) |_{k=k^{\\ast}}$$\nThis Hessian is the curvature of the negative log-posterior at its maximum, as stated in the problem. Since the gradient at $k^{\\ast}$ is zero, the expansion simplifies to:\n$$E(k) \\approx E(k^{\\ast}) + \\frac{1}{2} (k - k^{\\ast})^{\\top} H_{\\text{post}} (k - k^{\\ast})$$\nSubstituting this approximation back into the evidence integral:\n$$p(y \\mid \\mathcal{M}) \\approx \\int \\exp\\left(-E(k^{\\ast}) - \\frac{1}{2} (k - k^{\\ast})^{\\top} H_{\\text{post}} (k - k^{\\ast})\\right) dk$$\n$$p(y \\mid \\mathcal{M}) \\approx \\exp(-E(k^{\\ast})) \\int \\exp\\left(-\\frac{1}{2} (k - k^{\\ast})^{\\top} H_{\\text{post}} (k - k^{\\ast})\\right) dk$$\nThe integral is a standard multi-dimensional Gaussian integral. The integrand is proportional to a Gaussian probability density function with mean $k^{\\ast}$ and covariance matrix $\\Sigma_{\\text{post}} = H_{\\text{post}}^{-1}$. The value of the integral is given by:\n$$\\int \\exp\\left(-\\frac{1}{2} (k - k^{\\ast})^{\\top} H_{\\text{post}} (k - k^{\\ast})\\right) dk = (2\\pi)^{d/2} |\\det(H_{\\text{post}})|^{-1/2}$$\nSubstituting this result, we obtain the Laplace approximation for the model evidence:\n$$p(y \\mid \\mathcal{M}) \\approx \\exp(-E(k^{\\ast})) (2\\pi)^{d/2} |\\det(H_{\\text{post}})|^{-1/2}$$\nNow, we expand the $\\exp(-E(k^{\\ast}))$ term:\n$$\\exp(-E(k^{\\ast})) = \\exp(-\\ln p(y \\mid k^{\\ast}, \\mathcal{M}) - \\ln p(k^{\\ast} \\mid \\mathcal{M})) = p(y \\mid k^{\\ast}, \\mathcal{M}) p(k^{\\ast} \\mid \\mathcal{M})$$\nSo, the evidence is:\n$$p(y \\mid \\mathcal{M}) \\approx p(y \\mid k^{\\ast}, \\mathcal{M}) p(k^{\\ast} \\mid \\mathcal{M}) (2\\pi)^{d/2} |\\det(H_{\\text{post}})|^{-1/2}$$\nThe term $p(y \\mid k^{\\ast}, \\mathcal{M})$ is the best-fit likelihood, which measures how well the model explains the data. The remaining terms constitute the Occam factor, which penalizes model complexity.\nThe problem asks to isolate the Occam factor that represents the \"contraction of parameter-space volume from the prior to the posterior\". This is interpreted as the ratio of the characteristic volumes of the posterior and prior probability distributions. For Gaussian distributions, this volume is proportional to the square root of the determinant of the covariance matrix.\n\nThe posterior distribution is locally approximated as a Gaussian $\\mathcal{N}(k^{\\ast}, \\Sigma_{\\text{post}})$ where $\\Sigma_{\\text{post}} = H_{\\text{post}}^{-1}$. Its characteristic volume is proportional to $|\\det(\\Sigma_{\\text{post}})|^{1/2}$. The prior distribution is given as a Gaussian $\\mathcal{N}(\\mu_0, \\Sigma_0)$, so its characteristic volume is proportional to $|\\det(\\Sigma_0)|^{1/2}$.\n\nThe Occam factor representing the volume contraction, let's call it $O_{\\text{vol}}$, is the ratio of these volumes:\n$$O_{\\text{vol}} = \\frac{\\text{Posterior Volume}}{\\text{Prior Volume}} \\propto \\frac{|\\det(\\Sigma_{\\text{post}})|^{1/2}}{|\\det(\\Sigma_0)|^{1/2}} = \\sqrt{\\frac{\\det(\\Sigma_{\\text{post}})}{\\det(\\Sigma_0)}}$$\nThis expression gives the Occam factor entirely in terms of the prior covariance $\\Sigma_0$ and the posterior covariance $\\Sigma_{\\text{post}}$, as requested.\n\n### Part 2: Numerical Computation\n\nWe are given the following values for a $d=2$ parameter problem:\n- Prior covariance: $\\Sigma_0 = \\begin{pmatrix} 0.25 & 0 \\\\ 0 & 0.04 \\end{pmatrix}$\n- Negative log-likelihood Hessian at $k^{\\ast}$: $H_{\\text{LL}} = \\begin{pmatrix} 400 & -120 \\\\ -120 & 50 \\end{pmatrix}$\n- Posterior curvature: $H_{\\text{post}} = H_{\\text{LL}} + \\Sigma_0^{-1}$\n- Posterior covariance: $\\Sigma_{\\text{post}} = H_{\\text{post}}^{-1}$\n\nWe need to compute $O_{\\text{vol}} = \\sqrt{\\frac{\\det(\\Sigma_{\\text{post}})}{\\det(\\Sigma_0)}}$.\nUsing the property $\\det(A^{-1}) = (\\det(A))^{-1}$, we can write $\\det(\\Sigma_{\\text{post}}) = (\\det(H_{\\text{post}}))^{-1}$. The Occam factor becomes:\n$$O_{\\text{vol}} = \\sqrt{\\frac{1}{\\det(H_{\\text{post}}) \\det(\\Sigma_0)}} = \\frac{1}{\\sqrt{\\det(H_{\\text{post}}) \\det(\\Sigma_0)}}$$\nThis form is computationally more direct.\n\nFirst, we compute the determinant of the prior covariance matrix $\\Sigma_0$:\n$$\\det(\\Sigma_0) = (0.25) \\times (0.04) = 0.01$$\n\nNext, we find the inverse of the prior covariance matrix, $\\Sigma_0^{-1}$. Since $\\Sigma_0$ is diagonal, its inverse is the matrix of reciprocals of its diagonal elements:\n$$\\Sigma_0^{-1} = \\begin{pmatrix} 1/0.25 & 0 \\\\ 0 & 1/0.04 \\end{pmatrix} = \\begin{pmatrix} 4 & 0 \\\\ 0 & 25 \\end{pmatrix}$$\n\nNow, we compute the posterior curvature matrix $H_{\\text{post}}$:\n$$H_{\\text{post}} = H_{\\text{LL}} + \\Sigma_0^{-1} = \\begin{pmatrix} 400 & -120 \\\\ -120 & 50 \\end{pmatrix} + \\begin{pmatrix} 4 & 0 \\\\ 0 & 25 \\end{pmatrix} = \\begin{pmatrix} 404 & -120 \\\\ -120 & 75 \\end{pmatrix}$$\n\nWe then compute the determinant of $H_{\\text{post}}$:\n$$\\det(H_{\\text{post}}) = (404 \\times 75) - (-120 \\times -120) = 30300 - 14400 = 15900$$\n\nFinally, we substitute these determinants into the expression for the Occam factor:\n$$O_{\\text{vol}} = \\frac{1}{\\sqrt{\\det(H_{\\text{post}}) \\det(\\Sigma_0)}} = \\frac{1}{\\sqrt{15900 \\times 0.01}} = \\frac{1}{\\sqrt{159}}$$\nNow we compute the numerical value:\n$$O_{\\text{vol}} = \\frac{1}{\\sqrt{159}} \\approx \\frac{1}{12.60952021} \\approx 0.07930501...$$\nRounding the result to four significant figures gives $0.07931$.",
            "answer": "$$\\boxed{0.07931}$$"
        },
        {
            "introduction": "A well-formed model can still fail in practice if its parameters cannot be uniquely determined from the available experimental data—a problem known as non-identifiability. Profile likelihood is a powerful statistical technique for diagnosing this issue by visualizing how constrained a parameter of interest is. This hands-on coding practice  challenges you to implement profile likelihood analysis for a Hill function model, giving you direct insight into how experimental design choices—such as the range of inducer concentrations—critically impact parameter identifiability.",
            "id": "3906800",
            "problem": "You are given a gene regulation scenario modeled by a Hill activation function grounded in thermodynamic binding equilibrium. Let $I$ denote the inducer concentration (in $\\mu$M), let $y$ denote the measured expression level (in unitless relative fluorescence units), and let $n$ denote the Hill coefficient. The canonical Hill activation model for expression is\n$$\ny(I;\\beta_0,\\beta,K,n) \\;=\\; \\beta_0 \\;+\\; \\beta \\,\\frac{I^n}{K^n + I^n},\n$$\nwhere $\\beta_0 \\ge 0$ is a basal expression level, $\\beta \\ge 0$ is the maximal inducible amplitude, and $K > 0$ is the half-activation concentration. Assume additive Gaussian measurement noise consistent with Maximum Likelihood Estimation (MLE) under a Normal model, i.e., for each measurement $y_{\\text{obs}}$ at concentration $I$, \n$$\ny_{\\text{obs}} \\;=\\; y(I;\\beta_0,\\beta,K,n) \\;+\\; \\varepsilon,\\quad \\varepsilon \\sim \\mathcal{N}(0,\\sigma^2),\n$$\nwith known noise standard deviation $\\sigma$. The log-likelihood under independent Gaussian errors for a dataset $\\{(I_i,y_{\\text{obs},i})\\}_{i=1}^{N}$, ignoring the additive constant, is proportional to the negative residual sum of squares (RSS),\n$$\n\\ell(\\beta_0,\\beta,K,n) \\;=\\; -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{N}\\left[y_{\\text{obs},i} - y(I_i;\\beta_0,\\beta,K,n)\\right]^2.\n$$\nThe profile log-likelihood for the Hill coefficient $n$ is defined by\n$$\n\\ell_{\\text{prof}}(n) \\;=\\; \\max_{\\beta_0 \\ge 0,\\, \\beta \\ge 0,\\, K > 0}\\; \\ell(\\beta_0,\\beta,K,n),\n$$\nwhich removes nuisance parameters by maximizing over them for each fixed $n$. In automated model discovery using Machine Learning (ML), such profiling guides the identifiability analysis and informs experimental design choices: a flat $\\ell_{\\text{prof}}(n)$ over a wide range of $n$ indicates non-identifiability of $n$ under the current data and design.\n\nYour task is to implement a complete program that, for each provided test case, computes the normalized profile log-likelihood $\\ell_{\\text{prof}}(n)$ over a grid of $n$ values and decides whether the profile exhibits a flat region indicative of non-identifiability. Use the following principle-based rules:\n\n1. For each fixed $n$ on a specified grid, perform Maximum Likelihood Estimation with respect to $(\\beta_0,\\beta,K)$ by minimizing the residual sum of squares, respecting the constraints $\\beta_0 \\ge 0$, $\\beta \\ge 0$, $K > 0$. Interpret this minimization as ML under Gaussian noise with known $\\sigma$.\n\n2. Compute the profile $\\ell_{\\text{prof}}(n)$ and normalize it by subtracting its maximum value so that the peak value is $0$:\n$$\n\\tilde{\\ell}_{\\text{prof}}(n) \\;=\\; \\ell_{\\text{prof}}(n) - \\max_{n}\\ell_{\\text{prof}}(n).\n$$\n\n3. Quantify flatness by two complementary metrics:\n   - Quadratic-curvature width around the peak: fit a quadratic model to $\\tilde{\\ell}_{\\text{prof}}(n)$ in a small neighborhood around the maximizing $\\hat{n}$ to obtain curvature $a$ (the coefficient of $(n-\\hat{n})^2$). Using the single-parameter likelihood-ratio approximation, define the $95\\%$ drop level $d = 1.92$ and estimate the full width at the drop as\n   $$\n   w \\;=\\; 2\\sqrt{\\frac{d}{|a|}}\\quad\\text{if }a < 0,\\;\\text{and }w=+\\infty\\text{ if }a \\ge 0.\n   $$\n   - Global drop magnitude: compute $\\Delta \\;=\\; \\left|\\min_{n}\\tilde{\\ell}_{\\text{prof}}(n)\\right|$.\n\n4. Classify the Hill coefficient $n$ as non-identifiable if either $w > 1.5$ or $\\Delta < 0.8$. Otherwise, classify it as identifiable. Return a boolean per test case: $True$ indicates non-identifiable, $False$ indicates identifiable.\n\nThe program must implement the above using a grid of $n$ values from $0.5$ to $5.0$ in steps of $0.05$, respecting units for $I$ in $\\mu$M and treating $y$ as unitless. All optimization must be within the allowed runtime environment and adhere to constraints on libraries.\n\nTest Suite:\nUse the following four synthetic test cases (all generated internally by your program with a fixed random seed to ensure reproducibility):\n\n- Case $1$ (well-designed, low noise; expected identifiable): \n  - True parameters: $\\beta_0 = 0.1$, $\\beta = 1.0$, $K = 10.0$, $n_{\\text{true}} = 2.0$.\n  - Concentrations $I$ (in $\\mu$M): $N=20$ values, logarithmically spaced from $0.1$ to $100.0$.\n  - Noise standard deviation: $\\sigma = 0.02$.\n\n- Case $2$ (saturated-only design, low noise; expected non-identifiable):\n  - True parameters: $\\beta_0 = 0.1$, $\\beta = 1.0$, $K = 10.0$, $n_{\\text{true}} = 2.0$.\n  - Concentrations $I$ (in $\\mu$M): $N=10$ values linearly spaced from $100.0$ to $200.0$.\n  - Noise standard deviation: $\\sigma = 0.02$.\n\n- Case $3$ (subthreshold-only design, low noise; expected non-identifiable):\n  - True parameters: $\\beta_0 = 0.1$, $\\beta = 1.0$, $K = 10.0$, $n_{\\text{true}} = 2.0$.\n  - Concentrations $I$ (in $\\mu$M): $N=10$ values linearly spaced from $0.01$ to $0.20$.\n  - Noise standard deviation: $\\sigma = 0.02$.\n\n- Case $4$ (well-designed but high noise; expected non-identifiable):\n  - True parameters: $\\beta_0 = 0.1$, $\\beta = 1.0$, $K = 10.0$, $n_{\\text{true}} = 2.0$.\n  - Concentrations $I$ (in $\\mu$M): $N=8$ values logarithmically spaced from $0.1$ to $100.0$.\n  - Noise standard deviation: $\\sigma = 0.20$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3,result_4]$), where each $result_i$ is a boolean indicating whether the Hill coefficient $n$ is non-identifiable for the corresponding case under the specified classification rule. No additional text should be printed.",
            "solution": "The problem statement has been meticulously validated and is determined to be scientifically sound, well-posed, and objective. It presents a formal and verifiable task in the field of synthetic biology modeling and automated model discovery. We may therefore proceed with a complete solution.\n\nThe core of the problem is to assess the identifiability of the Hill coefficient, $n$, from experimental data. A parameter is considered identifiable if the available data are sufficient to constrain its value to a narrow range. The profile log-likelihood provides a rigorous statistical tool for this assessment. A sharply peaked profile indicates high identifiability, whereas a flat profile over a wide range of parameter values indicates non-identifiability. The task requires implementing an algorithm to compute and analyze this profile for different experimental scenarios.\n\nThe solution is structured into four principal stages:\n1.  Generation of synthetic experimental data for each test case.\n2.  Computation of the profile log-likelihood, $\\ell_{\\text{prof}}(n)$, over a grid of $n$ values.\n3.  Analysis of the normalized profile, $\\tilde{\\ell}_{\\text{prof}}(n)$, to quantify its flatness using two metrics.\n4.  Classification of $n$ as identifiable or non-identifiable based on quantitative thresholds.\n\n**1. Synthetic Data Generation**\n\nFor each test case, we first generate synthetic data that mimic experimental measurements. The true underlying process is described by the Hill activation function:\n$$\ny(I;\\beta_0,\\beta,K,n) \\;=\\; \\beta_0 \\;+\\; \\beta \\,\\frac{I^n}{K^n + I^n}\n$$\nwhere $I$ is the inducer concentration, $\\beta_0$ is the basal expression, $\\beta$ is the inducible amplitude, $K$ is the half-activation concentration, and $n$ is the Hill coefficient.\n\nFor a set of inducer concentrations $\\{I_i\\}_{i=1}^{N}$, the true expression levels $\\{y_{\\text{true},i}\\}_{i=1}^{N}$ are calculated using the true parameter values provided for each case. Experimental measurements, $y_{\\text{obs},i}$, are simulated by adding independent and identically distributed Gaussian noise to the true values:\n$$\ny_{\\text{obs},i} \\;=\\; y(I_i;\\beta_{0,\\text{true}},\\beta_{\\text{true}},K_{\\text{true}},n_{\\text{true}}) \\;+\\; \\varepsilon_i,\\quad \\text{where } \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\n$$\nThe noise standard deviation, $\\sigma$, is known for each case. All data generation is performed using a fixed random seed to ensure reproducibility.\n\n**2. Profile Log-Likelihood Computation**\n\nThe profile log-likelihood for $n$, denoted $\\ell_{\\text{prof}}(n)$, is found by maximizing the log-likelihood function over the nuisance parameters $(\\beta_0, \\beta, K)$ for each fixed value of $n$. Assuming independent Gaussian noise, maximizing the log-likelihood is equivalent to minimizing the Residual Sum of Squares (RSS):\n$$\n\\text{RSS}(\\beta_0,\\beta,K) \\;=\\; \\sum_{i=1}^{N}\\left[y_{\\text{obs},i} - y(I_i;\\beta_0,\\beta,K,n)\\right]^2\n$$\nThus, for each $n$ on a predefined grid from $0.5$ to $5.0$, we solve the following constrained optimization problem:\n$$\n\\text{RSS}_{\\min}(n) \\;=\\; \\min_{\\beta_0 \\ge 0,\\, \\beta \\ge 0,\\, K > 0} \\text{RSS}(\\beta_0,\\beta,K)\n$$\nThis is a non-linear least squares problem. We employ the `L-BFGS-B` algorithm, available through `scipy.optimize.minimize`, which is well-suited for this type of problem as it can handle the box constraints (i.e., lower bounds) required for the parameters: $\\beta_0 \\ge 0$, $\\beta \\ge 0$, and $K > 0$. For numerical stability, the constraint on $K$ is implemented as $K \\ge \\epsilon$ for a small positive $\\epsilon$.\n\nOnce $\\text{RSS}_{\\min}(n)$ is found for each $n$ in the grid, the corresponding profile log-likelihood value is calculated as:\n$$\n\\ell_{\\text{prof}}(n) \\;=\\; -\\frac{\\text{RSS}_{\\min}(n)}{2\\sigma^2}\n$$\n\n**3. Profile Analysis and Identifiability Classification**\n\nTo facilitate comparison across different datasets and models, the profile is normalized by subtracting its maximum value:\n$$\n\\tilde{\\ell}_{\\text{prof}}(n) \\;=\\; \\ell_{\\text{prof}}(n) - \\max_{n'}\\ell_{\\text{prof}}(n')\n$$\nThe peak of this normalized profile is at $0$, corresponding to the maximum likelihood estimate of the Hill coefficient, $\\hat{n}$. The flatness of this profile is then quantified using two metrics:\n\n-   **Quadratic-Curvature Width ($w$)**: The profile's shape near its peak at $\\hat{n}$ is approximated by a quadratic function, $p(n) = a(n-\\hat{n})^2 + b(n-\\hat{n}) + c$. The curvature, $a$, is the coefficient of the squared term. This is obtained by fitting a second-degree polynomial to a small neighborhood of points around the peak of $\\tilde{\\ell}_{\\text{prof}}(n)$. Based on likelihood-ratio theory, a confidence interval can be estimated from this curvature. The full width of the profile at a drop of $d = 1.92$ (corresponding to a 95% confidence interval for a single parameter) is given by:\n    $$\n    w \\;=\\; \\begin{cases} 2\\sqrt{d/|a|} & \\text{if } a < 0 \\\\ +\\infty & \\text{if } a \\ge 0 \\end{cases}\n    $$\n    An upward curvature ($a \\ge 0$) at the maximum implies an extremely flat or ill-behaved profile, signifying non-identifiability, which is captured by an infinite width.\n\n-   **Global Drop Magnitude ($\\Delta$)**: This metric captures the total drop in log-likelihood across the entire evaluated range of $n$. It is defined as the absolute value of the minimum of the normalized profile:\n    $$\n    \\Delta \\;=\\; \\left|\\min_{n}\\tilde{\\ell}_{\\text{prof}}(n)\\right|\n    $$\n    A small $\\Delta$ indicates that even far from the optimal $\\hat{n}$, other values of $n$ are nearly as plausible, which is a hallmark of non-identifiability.\n\nFinally, the Hill coefficient $n$ is classified as non-identifiable if the profile is either too wide or too shallow, according to the specified thresholds:\n$$\n\\text{Non-identifiable if } (w > 1.5) \\text{ or } (\\Delta < 0.8)\n$$\nA boolean value of `True` is returned for non-identifiable cases, and `False` otherwise.\n\n**4. Implementation**\n\nThe described algorithm is implemented in Python, utilizing `numpy` for efficient numerical computation and `scipy.optimize.minimize` for the constrained non-linear optimization at the core of the profile likelihood calculation. A main function iterates through the four specified test cases, each defined by a set of true parameters and an experimental design (concentration range and sampling). For each case, it generates data, computes the profile, analyzes it, and determines the identifiability, appending the final boolean result to a list. The program concludes by printing this list in the specified format. This systematic approach directly translates the principles of statistical identifiability analysis into a concrete computational workflow.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main function to solve the identifiability problem for all test cases.\n    \"\"\"\n    # Set a single fixed random seed for reproducibility across all cases.\n    np.random.seed(42)\n\n    # Define the four test cases as specified in the problem statement.\n    test_cases = [\n        {\n            \"name\": \"Case 1: Well-designed, low noise\",\n            \"true_params\": (0.1, 1.0, 10.0, 2.0), # beta0, beta, K, n\n            \"I_config\": {\"type\": \"log\", \"N\": 20, \"start\": 0.1, \"end\": 100.0},\n            \"sigma\": 0.02\n        },\n        {\n            \"name\": \"Case 2: Saturated-only, low noise\",\n            \"true_params\": (0.1, 1.0, 10.0, 2.0),\n            \"I_config\": {\"type\": \"lin\", \"N\": 10, \"start\": 100.0, \"end\": 200.0},\n            \"sigma\": 0.02\n        },\n        {\n            \"name\": \"Case 3: Subthreshold-only, low noise\",\n            \"true_params\": (0.1, 1.0, 10.0, 2.0),\n            \"I_config\": {\"type\": \"lin\", \"N\": 10, \"start\": 0.01, \"end\": 0.20},\n            \"sigma\": 0.02\n        },\n        {\n            \"name\": \"Case 4: Well-designed, high noise\",\n            \"true_params\": (0.1, 1.0, 10.0, 2.0),\n            \"I_config\": {\"type\": \"log\", \"N\": 8, \"start\": 0.1, \"end\": 100.0},\n            \"sigma\": 0.20\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        is_non_identifiable = analyze_case(case)\n        results.append(is_non_identifiable)\n\n    # Print the final results in the required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef hill_function(params, I, n):\n    \"\"\"Calculates the Hill activation function response.\"\"\"\n    beta0, beta, K = params\n    # Use numerically stable form: 1 / ( (K/I)^n + 1 )\n    # Handle potential division by zero if I can be 0, though problem constraints avoid this.\n    with np.errstate(divide='ignore', invalid='ignore'):\n        term = 1.0 / (1.0 + (K / I)**n)\n    # If I is very small, (K/I)^n can be inf, and term becomes 0, which is correct.\n    term[np.isinf(I)] = 1.0 # If I is infinite, term is 1.\n    term[I == 0] = 0.0 # If I is zero, term is 0.\n    return beta0 + beta * term\n\n\ndef rss_objective(params, I, y_obs, n):\n    \"\"\"Calculates the Residual Sum of Squares (RSS) for the Hill model.\"\"\"\n    y_model = hill_function(params, I, n)\n    return np.sum((y_obs - y_model)**2)\n\n\ndef analyze_case(case_spec):\n    \"\"\"\n    Performs the full identifiability analysis for a single test case.\n    \"\"\"\n    # 1. Generate synthetic data\n    beta0_true, beta_true, K_true, n_true = case_spec[\"true_params\"]\n    I_config = case_spec[\"I_config\"]\n    sigma = case_spec[\"sigma\"]\n\n    if I_config[\"type\"] == 'log':\n        I = np.logspace(np.log10(I_config[\"start\"]), np.log10(I_config[\"end\"]), I_config[\"N\"])\n    else: # 'lin'\n        I = np.linspace(I_config[\"start\"], I_config[\"end\"], I_config[\"N\"])\n\n    y_true = hill_function((beta0_true, beta_true, K_true), I, n_true)\n    noise = np.random.normal(0, sigma, size=I_config[\"N\"])\n    y_obs = y_true + noise\n\n    # 2. Compute profile log-likelihood\n    n_grid = np.arange(0.5, 5.0 + 1e-9, 0.05)\n    l_prof = np.zeros_like(n_grid)\n    \n    # Heuristic initial guess for optimizer\n    beta0_guess = np.min(y_obs) if np.min(y_obs) > 0 else 1e-3\n    beta_guess = np.max(y_obs) - beta0_guess if (np.max(y_obs) - beta0_guess) > 0 else 0.1\n    mid_y_range = beta0_guess + 0.5 * beta_guess\n    k_guess_idx = np.argmin(np.abs(y_obs - mid_y_range))\n    k_guess = I[k_guess_idx]\n    x0 = [beta0_guess, beta_guess, k_guess]\n\n    bounds = [(0, None), (0, None), (1e-9, None)]\n\n    for i, n_val in enumerate(n_grid):\n        opt_result = minimize(\n            rss_objective,\n            x0,\n            args=(I, y_obs, n_val),\n            bounds=bounds,\n            method='L-BFGS-B'\n        )\n        min_rss = opt_result.fun\n        l_prof[i] = -min_rss / (2 * sigma**2)\n\n    # 3. Analyze the profile\n    if np.all(np.isinf(l_prof)) or np.all(np.isnan(l_prof)):\n        return True # Profile computation failed, indicates extreme non-identifiability\n\n    l_prof_norm = l_prof - np.max(l_prof)\n\n    # Metric 1: Global drop magnitude\n    delta = np.abs(np.min(l_prof_norm))\n\n    # Metric 2: Quadratic-curvature width\n    idx_max = np.argmax(l_prof_norm)\n    \n    # Define neighborhood for quadratic fit, handling edges\n    width_pts = 3\n    start_idx = max(0, idx_max - width_pts)\n    end_idx = min(len(n_grid), idx_max + width_pts + 1)\n    \n    if (end_idx - start_idx) < 3:\n        # Not enough points for a quadratic fit, implies a pathological profile (e.g., max at edge)\n        w = np.inf\n    else:\n        n_hood = n_grid[start_idx:end_idx]\n        l_prof_hood = l_prof_norm[start_idx:end_idx]\n        \n        # Fit a 2nd-degree polynomial: a*n^2 + b*n + c\n        coeffs = np.polyfit(n_hood, l_prof_hood, 2)\n        a = coeffs[0]\n\n        d = 1.92\n        if a >= -1e-9: # Curvature is non-negative (or very close to zero)\n            w = np.inf\n        else:\n            w = 2 * np.sqrt(d / np.abs(a))\n\n    # 4. Classify based on thresholds\n    is_non_identifiable = (w > 1.5) or (delta < 0.8)\n    return is_non_identifiable\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "The \"automated\" in automated model discovery implies a massive search, often requiring the evaluation of thousands or millions of candidate models on distributed computing systems. The practical success of such a search hinges on its computational efficiency and scalability. This exercise  delves into the performance engineering aspect of model discovery, tasking you with building a performance model from first principles to analyze speedup, efficiency, and the critical bottlenecks—such as serial overhead or data I/O—that limit the throughput of a parallel evaluation pipeline.",
            "id": "3906808",
            "problem": "You are evaluating automated discoveries of candidate dynamical models for a synthetic gene regulatory network in a distributed environment. Each candidate model is validated by simulating an Ordinary Differential Equation (ODE) system against a fixed training dataset. The automated pipeline has three stages per evaluation run: a master-side per-model overhead related to solver configuration and orchestration, worker-side data loading of the shared dataset from networked storage, and worker-side ODE simulation. Consider the following well-tested bases and core definitions: time is the sum of non-overlapping stage durations, throughput equals data size divided by sustained bandwidth, and speedup is baseline runtime divided by parallel runtime. You must design a program that computes speedups and identifies scaling limits from first principles, without invoking any shortcut formulas.\n\nAssume there are $N$ candidate models. Each model has a base processing time $t_{\\text{base}}$ that includes all non-input/output (I/O) computation. A fraction $\\alpha \\in [0,1]$ of $t_{\\text{base}}$ is inherently serial and must be performed on the master process due to solver orchestration and per-model serialization overhead; the remaining fraction $1-\\alpha$ can be executed in parallel on workers. The training dataset has size $D$ measured in megabytes (MB). Each worker can read at up to $B_{\\text{worker}}$ megabytes per second (MB/s), while the shared storage system sustains an aggregate maximum bandwidth $B_{\\max}$ megabytes per second (MB/s) across all workers. When $p$ workers read concurrently, assume fair sharing of the storage bandwidth, so the effective per-worker read bandwidth is $\\min\\!\\left(B_{\\text{worker}}, \\dfrac{B_{\\max}}{p}\\right)$. The worker-side I/O stage consists of each worker loading the dataset exactly once per evaluation run. The coordination and result aggregation introduce a serial communication overhead $T_{\\text{comm}}$ seconds per run, independent of $p$.\n\nWorkers are identical, and the $N$ models are evenly distributed. Each worker processes $\\lceil N/p \\rceil$ models. The wall-clock time of the parallel run is the sum of: the serial master overhead across all models, the worker-side I/O time, and the worker-side parallel compute time (dominated by the slowest worker). The baseline single-worker wall-clock time is the sum of the single-worker I/O time, the total model processing time across $N$ models, and the same serial communication overhead. All time quantities must be computed and returned in seconds. You must compute the following quantities for each test case:\n- The parallelization speedup $S(p)$ defined as the baseline single-worker time divided by the parallel run time.\n- The parallel efficiency $E(p)$ defined as $S(p)/p$.\n- A bottleneck code $b$, an integer chosen as follows:\n  - $b=0$ if the parallel compute component dominates the parallel run time,\n  - $b=1$ if the serial overhead dominates,\n  - $b=2$ if the I/O stage dominates.\nDominance means the corresponding component has the largest contribution to the total parallel runtime; ties should be broken by selecting the smallest code among the tied components.\n\nUnits must be handled as follows: time in seconds ($\\mathrm{s}$), data size in megabytes ($\\mathrm{MB}$), and bandwidth in megabytes per second ($\\mathrm{MB/s}$). The fraction $\\alpha$ must be provided as a decimal in $[0,1]$.\n\nTest Suite. Implement your program to compute the outputs for the following parameter sets, each given as an ordered tuple $\\left(N, t_{\\text{base}}, \\alpha, D, B_{\\text{worker}}, B_{\\max}, T_{\\text{comm}}, p\\right)$:\n- Case A (happy path): $\\left(128, $0.5~\\mathrm{s}$, 0.1, $2048~\\mathrm{MB}$, $400~\\mathrm{MB/s}$, $1200~\\mathrm{MB/s}$, $2~\\mathrm{s}$, 8\\right)$.\n- Case B (boundary, small $N$): $\\left(5, $3~\\mathrm{s}$, 0.2, $200~\\mathrm{MB}$, $500~\\mathrm{MB/s}$, $1000~\\mathrm{MB/s}$, $1~\\mathrm{s}$, 16\\right)$.\n- Case C (compute-dominated, large $N$): $\\left(1000, $0.2~\\mathrm{s}$, 0.05, $100~\\mathrm{MB}$, $800~\\mathrm{MB/s}$, $10000~\\mathrm{MB/s}$, $5~\\mathrm{s}$, 32\\right)$.\n- Case D (I/O-intensive, moderate $p$): $\\left(256, $1~\\mathrm{s}$, 0.0, $16384~\\mathrm{MB}$, $1200~\\mathrm{MB/s}$, $2400~\\mathrm{MB/s}$, $3~\\mathrm{s}$, 4\\right)$.\n- Case E (solver overhead-dominated): $\\left(4096, $0.05~\\mathrm{s}$, 0.6, $4096~\\mathrm{MB}$, $300~\\mathrm{MB/s}$, $3000~\\mathrm{MB/s}$, $10~\\mathrm{s}$, 64\\right)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the exact order of the test cases, where each test case contributes three items in sequence: $S(p)$ rounded to six decimal places, $E(p)$ rounded to six decimal places, and the bottleneck code $b$. For example, the output must be of the form $\\left[\\text{S}_A,\\text{E}_A,b_A,\\text{S}_B,\\text{E}_B,b_B,\\ldots\\right]$ with no spaces.",
            "solution": "The problem statement has been critically examined and is determined to be valid. It is scientifically grounded in the principles of performance modeling for distributed computing, is self-contained, well-posed, and free from contradiction or ambiguity. We may, therefore, proceed with a formal solution.\n\nThe problem requires the derivation of a performance model from first principles to compute speedup, efficiency, and identify computational bottlenecks for a distributed model evaluation pipeline. Let us define the necessary quantities step-by-step.\n\nThe provided parameters are:\n- $N$: the total number of models to evaluate.\n- $p$: the number of parallel workers.\n- $t_{\\text{base}}$: the base processing time per model in seconds.\n- $\\alpha$: the fraction of $t_{\\text{base}}$ that is inherently serial.\n- $D$: the size of the training dataset in megabytes ($\\mathrm{MB}$).\n- $B_{\\text{worker}}$: the maximum I/O bandwidth of a single worker in $\\mathrm{MB/s}$.\n- $B_{\\max}$: the maximum aggregate I/O bandwidth of the shared storage system in $\\mathrm{MB/s}$.\n- $T_{\\text{comm}}$: the serial communication overhead per run in seconds.\n\nFirst, we establish the baseline runtime, denoted as $T_1$, for a single-worker ($p=1$) execution. According to the problem definition, this is the sum of three components: single-worker I/O time, total model processing time, and the communication overhead.\n\n1.  **Single-Worker I/O Time ($T_{I/O, 1}$)**: With one worker, the effective I/O bandwidth is the lesser of the worker's own limit and the total system limit, $B_{\\text{eff},1} = \\min(B_{\\text{worker}}, B_{\\max})$. The time to read the dataset of size $D$ is:\n    $$T_{I/O, 1} = \\frac{D}{\\min(B_{\\text{worker}}, B_{\\max})}$$\n\n2.  **Total Model Processing Time ($T_{\\text{proc}, 1}$)**: A single worker must process all $N$ models, with each taking $t_{\\text{base}}$ seconds for computation.\n    $$T_{\\text{proc}, 1} = N \\times t_{\\text{base}}$$\n\n3.  **Communication Overhead ($T_{\\text{comm}}$)**: This is given as a constant overhead per run.\n\nThe total baseline runtime $T_1$ is the sum of these parts:\n$$T_1 = T_{I/O, 1} + T_{\\text{proc}, 1} + T_{\\text{comm}} = \\frac{D}{\\min(B_{\\text{worker}}, B_{\\max})} + N \\times t_{\\text{base}} + T_{\\text{comm}}$$\n\nNext, we formulate the parallel runtime, $T_p$, for an execution with $p$ workers. The problem states this is the sum of three non-overlapping stages: serial master overhead, worker-side I/O, and worker-side parallel computation. To facilitate bottleneck analysis, we will calculate these three components separately.\n\n1.  **Parallel Compute Component ($C_0$)**: The portion of work for each model that can be parallelized is $(1-\\alpha) \\times t_{\\text{base}}$. The $N$ models are distributed as evenly as possible among $p$ workers. The number of models assigned to the most heavily loaded worker is $k = \\lceil N/p \\rceil$. Since the wall-clock time is determined by the slowest worker, the duration of the parallel computation stage is:\n    $$C_0 = \\lceil N/p \\rceil \\times (1-\\alpha) \\times t_{\\text{base}}$$\n\n2.  **Serial Overhead Component ($C_1$)**: This component includes all work that cannot be parallelized. It consists of two parts: the orchestration overhead, which is a fraction $\\alpha$ of the base time for every model, and the fixed communication overhead $T_{\\text{comm}}$. This component's duration is:\n    $$C_1 = (N \\times \\alpha \\times t_{\\text{base}}) + T_{\\text{comm}}$$\n\n3.  **I/O Component ($C_2$)**: All $p$ workers read the dataset of size $D$ concurrently. They share the aggregate storage bandwidth $B_{\\max}$. The effective bandwidth for each worker is thus limited by both its own capacity and its share of the system bandwidth: $B_{\\text{eff}, p} = \\min(B_{\\text{worker}}, B_{\\max}/p)$. The time for this stage is the time it takes for any worker to complete the read operation:\n    $$C_2 = \\frac{D}{\\min(B_{\\text{worker}}, \\frac{B_{\\max}}{p})}$$\n\nThe total parallel runtime $T_p$ is the sum of these three components:\n$$T_p = C_0 + C_1 + C_2 = \\left(\\lceil N/p \\rceil (1-\\alpha) t_{\\text{base}}\\right) + \\left(N \\alpha t_{\\text{base}} + T_{\\text{comm}}\\right) + \\left(\\frac{D}{\\min(B_{\\text{worker}}, \\frac{B_{\\max}}{p})}\\right)$$\n\nWith $T_1$ and $T_p$ defined, we can compute the required metrics:\n\n- **Speedup ($S(p)$)**: The ratio of baseline runtime to parallel runtime.\n  $$S(p) = \\frac{T_1}{T_p}$$\n\n- **Parallel Efficiency ($E(p)$)**: The speedup per worker.\n  $$E(p) = \\frac{S(p)}{p}$$\n\n- **Bottleneck Code ($b$)**: An integer code indicating the dominant component of the parallel runtime $T_p$. The component with the largest duration is dominant. Ties are broken by selecting the smallest code ($0$ for compute, $1$ for serial, $2$ for I/O).\n  $$b = \\begin{cases} 0 & \\text{if } C_0 \\ge C_1 \\text{ and } C_0 \\ge C_2 \\\\ 1 & \\text{if } C_1 > C_0 \\text{ and } C_1 \\ge C_2 \\\\ 2 & \\text{if } C_2 > C_0 \\text{ and } C_2 > C_1 \\end{cases}$$\nThis logic is implemented by finding the maximum value among $C_0, C_1, C_2$ and selecting the code corresponding to the first component ($C_0$, then $C_1$, then $C_2$) that equals this maximum value.\n\nThese derived formulas will be implemented to solve for the provided test cases.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the performance modeling problem for a distributed synthetic gene\n    regulatory network model discovery pipeline.\n    \"\"\"\n    # Test cases are given as tuples:\n    # (N, t_base, alpha, D, B_worker, B_max, T_comm, p)\n    test_cases = [\n        # Case A: happy path\n        (128, 0.5, 0.1, 2048, 400, 1200, 2, 8),\n        # Case B: boundary, small N\n        (5, 3.0, 0.2, 200, 500, 1000, 1, 16),\n        # Case C: compute-dominated, large N\n        (1000, 0.2, 0.05, 100, 800, 10000, 5, 32),\n        # Case D: I/O-intensive, moderate p\n        (256, 1.0, 0.0, 16384, 1200, 2400, 3, 4),\n        # Case E: solver overhead-dominated\n        (4096, 0.05, 0.6, 4096, 300, 3000, 10, 64),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        N, t_base, alpha, D, B_worker, B_max, T_comm, p = case\n\n        # --- Calculate Baseline (p=1) Runtime, T_1 ---\n        # 1. Single-worker I/O time\n        b_eff_1 = min(B_worker, B_max)\n        t_io_1 = D / b_eff_1\n        \n        # 2. Total model processing time\n        t_proc_1 = N * t_base\n        \n        # 3. Total baseline time (including communication overhead as specified)\n        T_1 = t_io_1 + t_proc_1 + T_comm\n\n        # --- Calculate Parallel (p-worker) Runtime, T_p ---\n        # The total parallel time T_p is the sum of three components,\n        # which are also used for bottleneck analysis.\n\n        # Component C0: Parallel Compute\n        # Time taken by the slowest worker, which processes ceil(N/p) models.\n        models_per_worker = np.ceil(N / p)\n        C0 = models_per_worker * (1 - alpha) * t_base\n\n        # Component C1: Serial Overhead\n        # Sum of master-side overhead across all models and communication overhead.\n        C1 = (N * alpha * t_base) + T_comm\n\n        # Component C2: I/O\n        # All p workers read concurrently, so their effective bandwidth is limited.\n        b_eff_p = min(B_worker, B_max / p)\n        C2 = D / b_eff_p\n        \n        # Total parallel time\n        T_p = C0 + C1 + C2\n\n        # --- Calculate Final Metrics ---\n        # Speedup S(p)\n        S_p = T_1 / T_p if T_p > 0 else 0\n\n        # Parallel Efficiency E(p)\n        E_p = S_p / p if p > 0 else 0\n\n        # Bottleneck Code 'b'\n        # The dominant component contributes most to the parallel runtime T_p.\n        # Ties are broken by choosing the smallest code (0 > 1 > 2).\n        components = [C0, C1, C2]\n        max_component_time = max(components)\n        \n        b = -1 # Should always be overwritten\n        if np.isclose(components[0], max_component_time):\n            b = 0\n        elif np.isclose(components[1], max_component_time):\n            b = 1\n        else: # C2 must be the max\n            b = 2\n\n        results.extend([round(S_p, 6), round(E_p, 6), b])\n\n    # Format the final output string as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}