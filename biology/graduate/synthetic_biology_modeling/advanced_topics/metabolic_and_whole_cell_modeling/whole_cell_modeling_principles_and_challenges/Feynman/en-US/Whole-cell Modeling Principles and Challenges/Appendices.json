{
    "hands_on_practices": [
        {
            "introduction": "Before fitting a model to experimental data, it is crucial to understand its intrinsic properties and how its behavior depends on its parameters. Forward sensitivity analysis is a powerful deterministic tool for this purpose, allowing us to compute how much a model's output, such as protein concentration, changes in response to an infinitesimal change in a parameter, like a reaction rate. This practice  will guide you through the derivation and solution of sensitivity equations for a core gene expression module, providing a foundational skill for identifying a model's most influential parameters and diagnosing its stiffness.",
            "id": "3940291",
            "problem": "Consider a simplified, nondimensional module of a whole-cell model that captures transcription and translation dynamics consistent with the Central Dogma of molecular biology and mass-action kinetics. Let the state vector be $\\mathbf{x}(t) = (m(t), p(t))^{\\top}$, where $m(t)$ is the scaled concentration of messenger ribonucleic acid (mRNA) and $p(t)$ is the scaled concentration of protein. The module dynamics are represented by the system of ordinary differential equations (ODEs), derived from mass-action balances:\n$$\n\\frac{dm}{dt} = \\theta_{1} - \\theta_{2} m, \\qquad \\frac{dp}{dt} = \\theta_{3} m - \\theta_{4} p,\n$$\nwith initial conditions $m(0) = 0$ and $p(0) = 0$. The output is the scalar protein level $y(t) = p(t)$. The parameter vector is $\\boldsymbol{\\theta} = (\\theta_{1}, \\theta_{2}, \\theta_{3}, \\theta_{4})$, where all quantities are nondimensional as a result of standard nondimensionalization (states scaled by reference concentrations and time scaled by a reference timescale).\n\nStarting from first principles—namely, mass-action ODEs for reaction networks and the chain rule for differentiating composite functions—define the local sensitivities $s_{i}(t) = \\partial y(t)/\\partial \\theta_{i}$ for $i \\in \\{1,2,3,4\\}$, derive the forward sensitivity equations for $s_{i}(t)$, and compute their values at $t = T$, where $T = 3.0$. Use the parameter values\n$$\n\\theta_{1} = 2.0,\\quad \\theta_{2} = 0.5,\\quad \\theta_{3} = 1.2,\\quad \\theta_{4} = 0.8.\n$$\nAssume the initial conditions are independent of $\\boldsymbol{\\theta}$. Round each sensitivity $s_{i}(T)$ to four significant figures. Provide your final answer as a row matrix containing $(s_{1}(T), s_{2}(T), s_{3}(T), s_{4}(T))$.",
            "solution": "The user wants me to solve a forward sensitivity analysis problem for a given system of ordinary differential equations (ODEs).\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   **State Vector**: $\\mathbf{x}(t) = (m(t), p(t))^{\\top}$, where $m(t)$ is the mRNA concentration and $p(t)$ is the protein concentration.\n*   **System of ODEs**:\n    $$\n    \\frac{dm}{dt} = \\theta_{1} - \\theta_{2} m = f_1(m, p, \\boldsymbol{\\theta})\n    $$\n    $$\n    \\frac{dp}{dt} = \\theta_{3} m - \\theta_{4} p = f_2(m, p, \\boldsymbol{\\theta})\n    $$\n*   **Initial Conditions**: $m(0) = 0$, $p(0) = 0$.\n*   **Output**: $y(t) = p(t)$.\n*   **Parameter Vector**: $\\boldsymbol{\\theta} = (\\theta_{1}, \\theta_{2}, \\theta_{3}, \\theta_{4})$.\n*   **Parameter Values**: $\\theta_{1} = 2.0$, $\\theta_{2} = 0.5$, $\\theta_{3} = 1.2$, $\\theta_{4} = 0.8$.\n*   **Time Point**: $T = 3.0$.\n*   **Sensitivities**: $s_{i}(t) = \\partial y(t)/\\partial \\theta_{i} = \\partial p(t)/\\partial \\theta_{i}$ for $i \\in \\{1,2,3,4\\}$.\n*   **Assumption**: The initial conditions are independent of the parameter vector $\\boldsymbol{\\theta}$.\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientifically Grounded**: The problem describes a simplified model of transcription and translation using mass-action kinetics. This is a standard, fundamental approach in systems biology and synthetic biology for modeling gene expression networks. The equations represent production and first-order degradation, which are scientifically valid concepts.\n2.  **Well-Posed**: The system of ODEs is linear and time-invariant. Given the constant parameters and initial conditions, a unique and stable solution exists. The task of calculating local sensitivities is a well-defined mathematical problem that also leads to a well-posed system of linear ODEs.\n3.  **Objective**: The problem is defined with precise mathematical equations, parameters, and objective criteria. There is no subjective or ambiguous language.\n4.  **Complete and Consistent**: All necessary information (ODEs, parameters, initial conditions, time point) is provided to solve the problem. The assumption of parameter-independent initial conditions is explicitly stated, which is necessary for defining the initial conditions of the sensitivity equations.\n5.  **Realistic**: The parameters are all positive, which is physically necessary for reaction rates and degradation constants. The model is a simplification, but it is a plausible and commonly used one.\n\n**Step 3: Verdict and Action**\n\nThe problem is scientifically grounded, well-posed, complete, and objective. It is deemed **valid**. I will proceed with the solution.\n\n### Derivation and Solution\n\nThe core task is to compute the local sensitivities $s_i(t) = \\partial p(t) / \\partial \\theta_i$ for each parameter $\\theta_i$. This is achieved through forward sensitivity analysis.\n\n**1. General Formulation of Sensitivity Equations**\n\nLet the state vector be $\\mathbf{x}(t) = (m(t), p(t))^{\\top}$ and the system dynamics be $\\dot{\\mathbf{x}} = \\mathbf{f}(\\mathbf{x}, \\boldsymbol{\\theta})$. The sensitivities of the state variables with respect to a parameter $\\theta_i$ are defined as $\\mathbf{s}_{i}(t) = \\partial \\mathbf{x}(t) / \\partial \\theta_i$. By applying the chain rule and assuming sufficient smoothness to interchange the order of differentiation with respect to time $t$ and parameter $\\theta_i$, we derive the sensitivity equations:\n$$\n\\frac{d\\mathbf{s}_i}{dt} = \\frac{d}{dt} \\left(\\frac{\\partial \\mathbf{x}}{\\partial \\theta_i}\\right) = \\frac{\\partial}{\\partial \\theta_i} \\left(\\frac{d\\mathbf{x}}{dt}\\right) = \\frac{\\partial \\mathbf{f}(\\mathbf{x}, \\boldsymbol{\\theta})}{\\partial \\theta_i} = \\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}} \\frac{\\partial \\mathbf{x}}{\\partial \\theta_i} + \\frac{\\partial \\mathbf{f}}{\\partial \\theta_i}\n$$\nThis gives the system of linear ODEs for the sensitivities:\n$$\n\\dot{\\mathbf{s}}_i(t) = \\mathbf{J}(t) \\mathbf{s}_i(t) + \\frac{\\partial \\mathbf{f}}{\\partial \\theta_i}\n$$\nwhere $\\mathbf{J}(t)$ is the Jacobian matrix of the system, $\\mathbf{J}_{jk} = \\partial f_j / \\partial x_k$. The initial conditions are $\\mathbf{s}_i(0) = \\partial \\mathbf{x}(0) / \\partial \\theta_i$. Since the initial conditions are given as independent of $\\boldsymbol{\\theta}$, $\\mathbf{s}_i(0) = \\mathbf{0}$.\n\nFor this problem, $\\mathbf{f} = (\\theta_1 - \\theta_2 m, \\theta_3 m - \\theta_4 p)^{\\top}$. The Jacobian matrix is:\n$$\n\\mathbf{J} = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial m} & \\frac{\\partial f_1}{\\partial p} \\\\ \\frac{\\partial f_2}{\\partial m} & \\frac{\\partial f_2}{\\partial p} \\end{pmatrix} = \\begin{pmatrix} -\\theta_2 & 0 \\\\ \\theta_3 & -\\theta_4 \\end{pmatrix}\n$$\nThe sensitivity vectors are $\\mathbf{s}_i = (s_{m,i}, s_{p,i})^{\\top} = (\\partial m/\\partial \\theta_i, \\partial p/\\partial \\theta_i)^{\\top}$.\n\n**2. Solution of the State Equations**\n\nFirst, we solve for the states $m(t)$ and $p(t)$ using the provided parameter values: $\\theta_1=2.0$, $\\theta_2=0.5$, $\\theta_3=1.2$, $\\theta_4=0.8$.\n\nThe equation for $m(t)$ is $\\frac{dm}{dt} = \\theta_1 - \\theta_2 m$. This is a first-order linear ODE with the solution:\n$$\nm(t) = \\frac{\\theta_1}{\\theta_2} (1 - \\exp(-\\theta_2 t))\n$$\nSubstituting values: $m(t) = \\frac{2.0}{0.5}(1-\\exp(-0.5t)) = 4(1 - \\exp(-0.5t))$.\n\nThe equation for $p(t)$ is $\\frac{dp}{dt} = \\theta_3 m(t) - \\theta_4 p$. Substituting $m(t)$:\n$$\n\\frac{dp}{dt} + \\theta_4 p = \\theta_3 \\frac{\\theta_1}{\\theta_2} (1 - \\exp(-\\theta_2 t))\n$$\nSolving this linear ODE with the initial condition $p(0)=0$ yields:\n$$\np(t) = \\frac{\\theta_1 \\theta_3}{\\theta_2 \\theta_4} + \\frac{\\theta_1 \\theta_3}{\\theta_4(\\theta_4-\\theta_2)}\\exp(-\\theta_4 t) - \\frac{\\theta_1 \\theta_3}{\\theta_2(\\theta_4-\\theta_2)}\\exp(-\\theta_2 t)\n$$\nSubstituting parameter values:\n$\\frac{\\theta_1\\theta_3}{\\theta_2\\theta_4} = \\frac{2.0 \\times 1.2}{0.5 \\times 0.8} = 6$.\n$\\theta_4-\\theta_2 = 0.8 - 0.5 = 0.3$.\n$\\frac{\\theta_1\\theta_3}{\\theta_4(\\theta_4-\\theta_2)} = \\frac{2.4}{0.8 \\times 0.3} = 10$.\n$\\frac{\\theta_1\\theta_3}{\\theta_2(\\theta_4-\\theta_2)} = \\frac{2.4}{0.5 \\times 0.3} = 16$.\nSo, the solution for $p(t)$ is:\n$$\np(t) = 6 + 10\\exp(-0.8 t) - 16\\exp(-0.5 t)\n$$\n\n**3. Derivation and Solution of Sensitivity Equations**\n\nThe initial condition for all sensitivities is $s_{m,i}(0)=0$ and $s_{p,i}(0)=0$. The problem asks for $s_i(t) = s_{p,i}(t)$.\n\n**Sensitivity to $\\theta_1$:**\n$\\frac{\\partial \\mathbf{f}}{\\partial \\theta_1} = (1, 0)^{\\top}$. The sensitivity equations are:\n$\\frac{ds_{m,1}}{dt} = -0.5 s_{m,1} + 1$\n$\\frac{ds_{p,1}}{dt} = 1.2 s_{m,1} - 0.8 s_{p,1}$\nSolving for $s_{m,1}(t)$: $s_{m,1}(t) = \\frac{1}{0.5}(1-\\exp(-0.5t)) = 2(1-\\exp(-0.5t))$.\nSolving for $s_{p,1}(t)$: $\\frac{ds_{p,1}}{dt} + 0.8 s_{p,1} = 1.2 \\times 2(1-\\exp(-0.5t)) = 2.4(1-\\exp(-0.5t))$. The solution is:\n$s_{p,1}(t) = 3 - 8\\exp(-0.5t) + 5\\exp(-0.8t)$.\nAt $t=T=3.0$: $s_1(3.0) = 3 - 8\\exp(-1.5) + 5\\exp(-2.4) \\approx 1.66855$.\n\n**Sensitivity to $\\theta_2$:**\n$\\frac{\\partial \\mathbf{f}}{\\partial \\theta_2} = (-m(t), 0)^{\\top}$. The sensitivity equations are:\n$\\frac{ds_{m,2}}{dt} = -0.5 s_{m,2} - m(t) = -0.5 s_{m,2} - 4(1-\\exp(-0.5t))$\n$\\frac{ds_{p,2}}{dt} = 1.2 s_{m,2} - 0.8 s_{p,2}$\nSolving for $s_{m,2}(t)$ (a resonant case): $s_{m,2}(t) = -8(1-\\exp(-0.5t)) + 4t\\exp(-0.5t)$.\nSolving for $s_{p,2}(t)$: $\\frac{ds_{p,2}}{dt} + 0.8 s_{p,2} = 1.2 s_{m,2}(t)$. The solution is:\n$s_{p,2}(t) = -12 + \\frac{100}{3}\\exp(-0.8t) - \\frac{64}{3}\\exp(-0.5t) + 16t\\exp(-0.5t)$.\nAt $t=T=3.0$:\n$s_2(3.0) = -12 + \\frac{100}{3}\\exp(-2.4) - \\frac{64}{3}\\exp(-1.5) + 16(3)\\exp(-1.5) = -12 + \\frac{100}{3}\\exp(-2.4) + \\frac{80}{3}\\exp(-1.5) \\approx -3.02593$.\n\n**Sensitivity to $\\theta_3$:**\n$\\frac{\\partial \\mathbf{f}}{\\partial \\theta_3} = (0, m(t))^{\\top}$. The sensitivity equations are:\n$\\frac{ds_{m,3}}{dt} = -0.5 s_{m,3}$\n$\\frac{ds_{p,3}}{dt} = 1.2 s_{m,3} - 0.8 s_{p,3} + m(t)$\nSolving for $s_{m,3}(t)$: $s_{m,3}(t) = 0$ since $s_{m,3}(0)=0$.\nThe equation for $s_{p,3}(t)$ becomes: $\\frac{ds_{p,3}}{dt} + 0.8 s_{p,3} = m(t) = 4(1-\\exp(-0.5t))$. The solution is:\n$s_{p,3}(t) = 5 - \\frac{40}{3}\\exp(-0.5t) + \\frac{25}{3}\\exp(-0.8t)$.\nAt $t=T=3.0$: $s_3(3.0) = 5 - \\frac{40}{3}\\exp(-1.5) + \\frac{25}{3}\\exp(-2.4) \\approx 2.78091$.\n\n**Sensitivity to $\\theta_4$:**\n$\\frac{\\partial \\mathbf{f}}{\\partial \\theta_4} = (0, -p(t))^{\\top}$. The sensitivity equations are:\n$\\frac{ds_{m,4}}{dt} = -0.5 s_{m,4}$\n$\\frac{ds_{p,4}}{dt} = 1.2 s_{m,4} - 0.8 s_{p,4} - p(t)$\nSolving for $s_{m,4}(t)$: $s_{m,4}(t) = 0$ since $s_{m,4}(0)=0$.\nThe equation for $s_{p,4}(t)$ becomes (a resonant case): $\\frac{ds_{p,4}}{dt} + 0.8 s_{p,4} = -p(t) = -6 - 10\\exp(-0.8t) + 16\\exp(-0.5t)$. The solution is:\n$s_{p,4}(t) = -7.5 + \\frac{160}{3}\\exp(-0.5t) - \\frac{275}{6}\\exp(-0.8t) - 10t\\exp(-0.8t)$.\nAt $t=T=3.0$:\n$s_4(3.0) = -7.5 + \\frac{160}{3}\\exp(-1.5) - \\frac{275}{6}\\exp(-2.4) - 10(3)\\exp(-2.4) = -7.5 + \\frac{160}{3}\\exp(-1.5) - \\frac{455}{6}\\exp(-2.4) \\approx -2.47926$.\n\n**4. Final Computations**\n\nRounding the results to four significant figures:\n$s_1(3.0) \\approx 1.669$\n$s_2(3.0) \\approx -3.026$\n$s_3(3.0) \\approx 2.781$\n$s_4(3.0) \\approx -2.479$\nThe final answer is the row matrix $(s_1(3.0), s_2(3.0), s_3(3.0), s_4(3.0))$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1.669 & -3.026 & 2.781 & -2.479 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "A central challenge in creating predictive whole-cell models is estimating the values of dozens or even thousands of parameters from limited and noisy experimental data. It is often the case that the available data cannot uniquely determine all parameters, a problem known as non-identifiability. This hands-on exercise  introduces the profile likelihood method, a robust computational technique to diagnose both structural and practical non-identifiability, helping you to understand why some parameters are elusive and how data availability shapes our ability to learn them.",
            "id": "3940276",
            "problem": "Consider a simplified whole-cell protein expression module derived from the Central Dogma of Molecular Biology and mass-action kinetics. Messenger ribonucleic acid (mRNA) is transcribed and degraded, and protein is translated from mRNA and degraded. Assume a quasi-steady-state approximation for mRNA relative to protein dynamics: mRNA reaches steady state much faster than protein. Let transcription rate be $k_{\\mathrm{tx}}$ (in $\\mathrm{min}^{-1}$), translation rate be $k_{\\mathrm{tl}}$ (in $\\mathrm{min}^{-1}$), protein degradation rate be $k_{\\mathrm{deg,p}}$ (in $\\mathrm{min}^{-1}$), and mRNA degradation rate be a known constant $k_{\\mathrm{deg,m}}$ (in $\\mathrm{min}^{-1}$). Under quasi-steady-state, the mRNA concentration is $R(t) = k_{\\mathrm{tx}}/k_{\\mathrm{deg,m}}$ and the protein concentration satisfies the linear ordinary differential equation (ODE) $dP(t)/dt = \\alpha - k_{\\mathrm{deg,p}} P(t)$ with $\\alpha = k_{\\mathrm{tl}} k_{\\mathrm{tx}} / k_{\\mathrm{deg,m}}$ and initial protein $P(0) = 0$. The protein solution is\n$$\nP(t) = \\left(\\frac{\\alpha}{k_{\\mathrm{deg,p}}}\\right)\\left(1 - e^{-k_{\\mathrm{deg,p}} t}\\right).\n$$\n\nAssume measurements are corrupted by independent, identically distributed Gaussian noise with known standard deviations $\\sigma_R$ and $\\sigma_P$ for mRNA and protein, respectively. Under the Gaussian noise model, Maximum Likelihood (ML) estimation reduces to minimizing the negative log-likelihood, which for a parameter vector $\\theta = (k_{\\mathrm{tx}}, k_{\\mathrm{tl}}, k_{\\mathrm{deg,p}})$ and a dataset with measurement times $t_1,\\dots,t_n$ is\n$$\n\\ell(\\theta) = \\frac{1}{2}\\sum_{i=1}^{n} \\left(\\frac{R_{\\mathrm{model}}(t_i;\\theta) - R_{\\mathrm{obs}}(t_i)}{\\sigma_R}\\right)^2 + \\frac{1}{2}\\sum_{i=1}^{n} \\left(\\frac{P_{\\mathrm{model}}(t_i;\\theta) - P_{\\mathrm{obs}}(t_i)}{\\sigma_P}\\right)^2,\n$$\nwhere $R_{\\mathrm{model}}(t_i;\\theta) = k_{\\mathrm{tx}}/k_{\\mathrm{deg,m}}$ and $P_{\\mathrm{model}}(t_i;\\theta) = \\left(\\frac{k_{\\mathrm{tl}} k_{\\mathrm{tx}}}{k_{\\mathrm{deg,m}} k_{\\mathrm{deg,p}}}\\right)\\left(1 - e^{-k_{\\mathrm{deg,p}} t_i}\\right)$.\n\nDefine the profile likelihood for the selected parameter $k_{\\mathrm{tx}}$ by optimizing over the nuisance parameters $(k_{\\mathrm{tl}}, k_{\\mathrm{deg,p}})$:\n$$\n\\ell_{\\mathrm{prof}}(k_{\\mathrm{tx}}) = \\min_{k_{\\mathrm{tl}} > 0,\\, k_{\\mathrm{deg,p}} > 0} \\ell\\big(k_{\\mathrm{tx}}, k_{\\mathrm{tl}}, k_{\\mathrm{deg,p}}\\big).\n$$\nLet $\\ell_{\\min} = \\min_{k_{\\mathrm{tx}}>0,\\,k_{\\mathrm{tl}}>0,\\,k_{\\mathrm{deg,p}}>0} \\ell(k_{\\mathrm{tx}}, k_{\\mathrm{tl}}, k_{\\mathrm{deg,p}})$. A practical numerical diagnostic of non-identifiability is the presence of a flat region in the profile likelihood, operationalized as\n$$\n\\Delta_{\\max} = \\max_{k_{\\mathrm{tx}} \\in \\mathcal{G}} \\left[\\ell_{\\mathrm{prof}}(k_{\\mathrm{tx}}) - \\ell_{\\min}\\right],\n$$\nover a grid $\\mathcal{G}$ of $k_{\\mathrm{tx}}$ values. If $\\Delta_{\\max}$ is below a small tolerance $\\tau$, we classify the profile as numerically flat for the explored range, indicating non-identifiability of $k_{\\mathrm{tx}}$ due to model redundancies or non-identifiable directions in parameter space.\n\nYour task is to implement a program that:\n- Generates synthetic observations from the model at specified times using given \"true\" parameters, without adding random noise.\n- Computes $\\ell_{\\min}$ and the profile likelihood $\\ell_{\\mathrm{prof}}(k_{\\mathrm{tx}})$ across a given grid $\\mathcal{G}$ by constrained optimization over $(k_{\\mathrm{tl}}, k_{\\mathrm{deg,p}})$ with positivity bounds. Use the method that ensures positivity of parameters.\n- Returns, for each test case, a boolean indicating whether the profile is flat under tolerance $\\tau$, that is, whether $\\Delta_{\\max} \\le \\tau$.\n\nFundamental base to use:\n- Central Dogma of Molecular Biology (transcription and translation) and mass-action kinetics for degradation.\n- The linear ODE solution and its analytic form for protein under constant input.\n- Gaussian noise with known variance and Maximum Likelihood equivalence to least-squares.\n- Profile likelihood definition by optimization over nuisance parameters.\n\nTest suite:\n- Case $1$ (happy path, identifiable with mRNA): $k_{\\mathrm{deg,m}} = 0.5\\,\\mathrm{min}^{-1}$, true parameters $(k_{\\mathrm{tx}},k_{\\mathrm{tl}},k_{\\mathrm{deg,p}}) = (10, 5, 0.1)$ in $\\mathrm{min}^{-1}$, times $\\{0,2,5,10\\}\\,\\mathrm{min}$ for both mRNA and protein, use both mRNA and protein in the likelihood, $\\sigma_R = 1$, $\\sigma_P = 1$, bounds for all unknowns are $(10^{-9},10^{3})$, grid $\\mathcal{G} = \\{0.2,0.5,1,2,5,10,20,50\\}$ in $\\mathrm{min}^{-1}$, tolerance $\\tau = 10^{-6}$.\n- Case $2$ (redundancy, non-identifiable without mRNA): same $k_{\\mathrm{deg,m}}$, same true parameters and times, use only protein in the likelihood (ignore mRNA term), $\\sigma_P = 1$, bounds $(10^{-9},10^{3})$ for all unknowns, same grid $\\mathcal{G}$, tolerance $\\tau = 10^{-6}$.\n- Case $3$ (boundary-induced curvature): same as Case $2$, but constrain $k_{\\mathrm{tl}}$ with upper bound $50$ and keep $(10^{-9},10^{3})$ bounds for $k_{\\mathrm{deg,p}}$, same grid $\\mathcal{G}$, tolerance $\\tau = 10^{-6}$.\n\nUnits:\n- All rates are in $\\mathrm{min}^{-1}$.\n- Time is in $\\mathrm{min}$.\n- No angles or percentages are involved.\n\nFinal output format:\n- Your program should produce a single line of output containing the boolean results for the three cases as a comma-separated list enclosed in square brackets, in the order of Case $1$, Case $2$, Case $3$ (for example, `[True,False,True]`).",
            "solution": "The user has requested an analysis of parameter identifiability for a simplified model of protein expression using the profile likelihood method. The problem is scientifically and mathematically well-posed, providing all necessary definitions, models, and data for a computational solution.\n\nThe core of the problem is to investigate the identifiability of the transcription rate, $k_{\\mathrm{tx}}$, under different data availability and parameter constraint scenarios. The model describes the concentration of messenger RNA, $R(t)$, and protein, $P(t)$. Under a quasi-steady-state assumption for mRNA, the model equations are:\n$$\nR_{\\mathrm{model}}(t;\\theta) = \\frac{k_{\\mathrm{tx}}}{k_{\\mathrm{deg,m}}}\n$$\n$$\nP_{\\mathrm{model}}(t;\\theta) = \\left(\\frac{k_{\\mathrm{tl}} k_{\\mathrm{tx}}}{k_{\\mathrm{deg,m}} k_{\\mathrm{deg,p}}}\\right)\\left(1 - e^{-k_{\\mathrm{deg,p}} t}\\right)\n$$\nwhere the parameter vector is $\\theta = (k_{\\mathrm{tx}}, k_{\\mathrm{tl}}, k_{\\mathrm{deg,p}})$.\n\nThe analysis is based on the negative log-likelihood function, which, assuming independent Gaussian noise, is equivalent to a weighted sum of squared residuals:\n$$\n\\ell(\\theta) = \\frac{1}{2}\\sum_{i=1}^{n} \\left(\\frac{R_{\\mathrm{model}}(t_i;\\theta) - R_{\\mathrm{obs}}(t_i)}{\\sigma_R}\\right)^2 + \\frac{1}{2}\\sum_{i=1}^{n} \\left(\\frac{P_{\\mathrm{model}}(t_i;\\theta) - P_{\\mathrm{obs}}(t_i)}{\\sigma_P}\\right)^2\n$$\nwhere $R_{\\mathrm{obs}}$ and $P_{\\mathrm{obs}}$ are the observed data. The problem specifies that synthetic observations are generated from the model itself using a set of \"true\" parameters, without the addition of stochastic noise. This means that if the optimization algorithm can find the true parameter values, the cost function $\\ell(\\theta)$ will evaluate to $0$.\n\nThe primary tool for identifiability analysis is the profile likelihood for $k_{\\mathrm{tx}}$:\n$$\n\\ell_{\\mathrm{prof}}(k_{\\mathrm{tx}}) = \\min_{k_{\\mathrm{tl}} > 0,\\, k_{\\mathrm{deg,p}} > 0} \\ell\\big(k_{\\mathrm{tx}}, k_{\\mathrm{tl}}, k_{\\mathrm{deg,p}}\\big)\n$$\nA parameter is numerically non-identifiable if its profile likelihood is flat. This is diagnosed by computing $\\Delta_{\\max} = \\max_{k_{\\mathrm{tx}} \\in \\mathcal{G}} \\left[\\ell_{\\mathrm{prof}}(k_{\\mathrm{tx}}) - \\ell_{\\min}\\right]$ over a grid $\\mathcal{G}$ of $k_{\\mathrm{tx}}$ values, where $\\ell_{\\min}$ is the global minimum of $\\ell(\\theta)$. If $\\Delta_{\\max}$ is below a small tolerance $\\tau$, the profile is considered flat.\n\nThe procedure involves the following steps for each test case:\n1.  Generate noise-free synthetic data for $R_{\\mathrm{obs}}$ and $P_{\\mathrm{obs}}$ using the given \"true\" parameters.\n2.  Compute the global minimum of the likelihood, $\\ell_{\\min}$, by performing a numerical optimization over all three parameters $(k_{\\mathrm{tx}}, k_{\\mathrm{tl}}, k_{\\mathrm{deg,p}})$. Given the noise-free data, we expect $\\ell_{\\min} \\approx 0$.\n3.  Compute the profile likelihood, $\\ell_{\\mathrm{prof}}(k_{\\mathrm{tx}})$, by iterating through each value of $k_{\\mathrm{tx}}$ in the specified grid $\\mathcal{G}$. For each fixed $k_{\\mathrm{tx}}$, a numerical optimization is performed to find the minimum of $\\ell(\\theta)$ over the nuisance parameters $(k_{\\mathrm{tl}}, k_{\\mathrm{deg,p}})$.\n4.  Calculate $\\Delta_{\\max}$ and compare it to the tolerance $\\tau$ to determine if the profile is flat.\n\n**Case 1 Analysis (Identifiable with mRNA data)**\nIn this case, the likelihood function includes terms for both mRNA and protein data. The mRNA-related term is:\n$$\n\\ell_R(\\theta) = \\frac{1}{2}\\sum_{i=1}^{n} \\left(\\frac{k_{\\mathrm{tx}}/k_{\\mathrm{deg,m}} - R_{\\mathrm{obs}}(t_i)}{\\sigma_R}\\right)^2\n$$\nThis term depends only on $k_{\\mathrm{tx}}$. Since $R_{\\mathrm{obs}}$ is generated with the true parameter $k_{\\mathrm{tx,true}}$, any deviation of $k_{\\mathrm{tx}}$ from $k_{\\mathrm{tx,true}}$ in the optimization will result in a non-zero, positive value for this term. This creates a distinct minimum in the likelihood landscape centered at $k_{\\mathrm{tx,true}}$, making the parameter identifiable. The profile likelihood $\\ell_{\\mathrm{prof}}(k_{\\mathrm{tx}})$ will exhibit significant curvature. Therefore, we expect $\\Delta_{\\max} > \\tau$, and the result for this case will be `False`.\n\n**Case 2 Analysis (Non-identifiable without mRNA data)**\nHere, the likelihood function depends only on the protein data. The model for protein concentration, $P_{\\mathrm{model}}$, depends on $k_{\\mathrm{tx}}$ and $k_{\\mathrm{tl}}$ only through their product, $k_{\\mathrm{prod}} = k_{\\mathrm{tx}}k_{\\mathrm{tl}}$.\n$$\nP_{\\mathrm{model}}(t;\\theta) = \\left(\\frac{k_{\\mathrm{prod}}}{k_{\\mathrm{deg,m}} k_{\\mathrm{deg,p}}}\\right)\\left(1 - e^{-k_{\\mathrm{deg,p}} t}\\right)\n$$\nThis is a structural non-identifiability. Any pair of $(k_{\\mathrm{tx}}, k_{\\mathrm{tl}})$ that preserves the true product $k_{\\mathrm{prod,true}} = k_{\\mathrm{tx,true}}k_{\\mathrm{tl,true}}$ will produce the exact same model predictions and thus yield the minimum possible likelihood value ( $\\ell \\approx 0$). For any $k_{\\mathrm{tx}}$ on the grid $\\mathcal{G}$, we can choose $k_{\\mathrm{tl}} = k_{\\mathrm{prod,true}} / k_{\\mathrm{tx}}$. As long as this $k_{\\mathrm{tl}}$ is within its allowed bounds, the optimizer will find it, resulting in $\\ell_{\\mathrm{prof}}(k_{\\mathrm{tx}}) \\approx \\ell_{\\min} \\approx 0$. The bounds are wide enough in this case for this to hold true for all $k_{\\mathrm{tx}} \\in \\mathcal{G}$. Consequently, the profile will be flat, $\\Delta_{\\max} \\approx 0 \\le \\tau$, and the result will be `True`.\n\n**Case 3 Analysis (Boundary-induced Identifiability)**\nThis case is similar to Case 2, but with a more restrictive upper bound on the nuisance parameter $k_{\\mathrm{tl}}$. The structural non-identifiability still exists, but its effects are limited by the parameter bounds. To achieve a perfect fit, we need to find a $k_{\\mathrm{tl}}$ such that $k_{\\mathrm{tl}} = k_{\\mathrm{prod,true}} / k_{\\mathrm{tx}}$. For small values of $k_{\\mathrm{tx}}$ in the grid $\\mathcal{G}$, the required $k_{\\mathrm{tl}}$ may exceed its upper bound. For example, with true values $k_{\\mathrm{tx,true}}=10$ and $k_{\\mathrm{tl,true}}=5$, we have $k_{\\mathrm{prod,true}}=50$. For a grid point like $k_{\\mathrm{tx}}=0.5$, the ideal $k_{\\mathrm{tl}}$ would be $100$, which is greater than the specified upper bound of $50$. The optimizer is thus constrained to choose the boundary value $k_{\\mathrm{tl}}=50$, which is suboptimal and results in a likelihood value $\\ell_{\\mathrm{prof}}(0.5) > \\ell_{\\min}$. This introduces curvature into the profile where the bounds are active, making the profile non-flat. We expect $\\Delta_{\\max} > \\tau$, and the result will be `False`.\n\nThe implementation will use `scipy.optimize.minimize` to perform the required constrained optimizations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main function to run the profile likelihood analysis for three test cases.\n    \"\"\"\n    # Common parameters for all cases\n    true_params = (10.0, 5.0, 0.1)  # (k_tx, k_tl, k_deg_p)\n    k_deg_m = 0.5\n    t_obs = np.array([0.0, 2.0, 5.0, 10.0])\n    k_tx_grid = np.array([0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 50.0])\n    tolerance = 1e-6\n\n    # Test case definitions\n    test_cases = [\n        {\n            \"name\": \"Case 1: Identifiable with mRNA\",\n            \"use_mrna\": True,\n            \"sigma_r\": 1.0,\n            \"sigma_p\": 1.0,\n            \"bounds_3d\": [(1e-9, 1e3), (1e-9, 1e3), (1e-9, 1e3)],\n            \"bounds_2d\": [(1e-9, 1e3), (1e-9, 1e3)],\n        },\n        {\n            \"name\": \"Case 2: Non-identifiable without mRNA\",\n            \"use_mrna\": False,\n            \"sigma_r\": 1.0,\n            \"sigma_p\": 1.0,\n            \"bounds_3d\": [(1e-9, 1e3), (1e-9, 1e3), (1e-9, 1e3)],\n            \"bounds_2d\": [(1e-9, 1e3), (1e-9, 1e3)],\n        },\n        {\n            \"name\": \"Case 3: Boundary-induced curvature\",\n            \"use_mrna\": False,\n            \"sigma_r\": 1.0,\n            \"sigma_p\": 1.0,\n            \"bounds_3d\": [(1e-9, 1e3), (1e-9, 50.0), (1e-9, 1e3)],\n            \"bounds_2d\": [(1e-9, 50.0), (1e-9, 1e3)],\n        },\n    ]\n\n    # Generate synthetic data (same for all cases, as true_params are the same)\n    k_tx_true, k_tl_true, k_deg_p_true = true_params\n    r_obs = (k_tx_true / k_deg_m) * np.ones_like(t_obs)\n    alpha_true = k_tl_true * k_tx_true / k_deg_m\n    p_obs = (alpha_true / k_deg_p_true) * (1 - np.exp(-k_deg_p_true * t_obs))\n    \n    # Store results here\n    results = []\n\n    for case in test_cases:\n        use_mrna = case[\"use_mrna\"]\n        sigma_r = case[\"sigma_r\"]\n        sigma_p = case[\"sigma_p\"]\n        bounds_3d = case[\"bounds_3d\"]\n        bounds_2d = case[\"bounds_2d\"]\n\n        def objective_function(params, k_tx_val=None):\n            \"\"\"\n            Calculates the negative log-likelihood.\n            If k_tx_val is provided, params are (k_tl, k_deg_p).\n            Otherwise, params are (k_tx, k_tl, k_deg_p).\n            \"\"\"\n            if k_tx_val is not None:\n                k_tx = k_tx_val\n                k_tl, k_deg_p = params\n            else:\n                k_tx, k_tl, k_deg_p = params\n\n            # Model calculations\n            r_model = k_tx / k_deg_m\n            alpha = k_tl * k_tx / k_deg_m\n            \n            # Use a large penalty for non-physical parameters instead of np.inf\n            # to keep the optimizer stable. The bounds should prevent this.\n            if k_deg_p <= 0: return 1e12\n\n            p_model = (alpha / k_deg_p) * (1 - np.exp(-k_deg_p * t_obs))\n            \n            # Likelihood terms\n            r_cost = 0.5 * np.sum(((r_model - r_obs) / sigma_r)**2) if use_mrna else 0.0\n            p_cost = 0.5 * np.sum(((p_model - p_obs) / sigma_p)**2)\n            \n            return r_cost + p_cost\n\n        # 1. Compute l_min (global minimum)\n        obj_3d = lambda p: objective_function(p)\n        res_min = minimize(obj_3d, x0=true_params, bounds=bounds_3d, method='L-BFGS-B')\n        l_min = res_min.fun\n\n        # 2. Compute profile likelihood over the grid\n        profile_values = []\n        initial_guess_2d = [true_params[1], true_params[2]]\n        \n        for k_tx_fixed in k_tx_grid:\n            obj_2d = lambda p: objective_function(p, k_tx_val=k_tx_fixed)\n            res_prof = minimize(obj_2d, x0=initial_guess_2d, bounds=bounds_2d, method='L-BFGS-B')\n            profile_values.append(res_prof.fun)\n\n        # 3. Calculate Delta_max and determine flatness\n        delta_max = np.max(profile_values) - l_min\n        is_flat = delta_max <= tolerance\n        results.append(is_flat)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(lambda b: str(b), results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Modern systems biology generates diverse high-throughput datasets, from transcriptomics to proteomics, all of which contain information about the state of a cell. Bayesian inference offers a rigorous framework for integrating these multi-omics data sources to constrain model parameters and, critically, to quantify the resulting uncertainty in model predictions. This practice  will guide you through the process of formulating a Bayesian posterior distribution for latent pathway parameters and propagating their uncertainty to compute a credible interval for a key physiological output—the cell doubling time.",
            "id": "3940308",
            "problem": "Consider a simplified whole-cell growth phenotype model in synthetic biology modeling, where gene expression and protein abundance measurements inform latent pathway capacity parameters that in turn predict cell growth. Let the latent parameter vector be $\\boldsymbol{\\theta} \\in \\mathbb{R}^{d}$ representing pathway capacities. Assume a Gaussian prior $\\boldsymbol{\\theta} \\sim \\mathcal{N}(\\boldsymbol{m}_0, \\mathbf{S}_0)$, where $\\boldsymbol{m}_0 \\in \\mathbb{R}^{d}$ and $\\mathbf{S}_0 \\in \\mathbb{R}^{d \\times d}$ is positive definite. Let RNA sequencing (RNA-seq) log-abundances $\\boldsymbol{z}_r \\in \\mathbb{R}^{n_r}$ and Proteomics by Mass Spectrometry (MS) log-abundances $\\boldsymbol{z}_p \\in \\mathbb{R}^{n_p}$ be conditionally independent given $\\boldsymbol{\\theta}$ and modeled as linear-Gaussian observations:\n$$\n\\boldsymbol{z}_r \\mid \\boldsymbol{\\theta} \\sim \\mathcal{N}(\\mathbf{H}_r \\boldsymbol{\\theta}, \\boldsymbol{\\Sigma}_r), \\quad\n\\boldsymbol{z}_p \\mid \\boldsymbol{\\theta} \\sim \\mathcal{N}(\\mathbf{H}_p \\boldsymbol{\\theta}, \\boldsymbol{\\Sigma}_p),\n$$\nwhere $\\mathbf{H}_r \\in \\mathbb{R}^{n_r \\times d}$ and $\\mathbf{H}_p \\in \\mathbb{R}^{n_p \\times d}$ are design matrices linking latent capacities to log-abundances, and $\\boldsymbol{\\Sigma}_r \\in \\mathbb{R}^{n_r \\times n_r}$ and $\\boldsymbol{\\Sigma}_p \\in \\mathbb{R}^{n_p \\times n_p}$ are known positive definite noise covariance matrices capturing measurement noise. Assume the cell growth rate $\\mu$ in units of $\\mathrm{h}^{-1}$ is predicted from $\\boldsymbol{\\theta}$ via an exponential-linear link:\n$$\n\\mu = \\exp\\!\\left(c_0 + \\boldsymbol{c}^{\\top} \\boldsymbol{\\theta}\\right),\n$$\nwith $\\boldsymbol{c} \\in \\mathbb{R}^{d}$ and $c_0 \\in \\mathbb{R}$. The doubling time is $t_d = \\ln(2)/\\mu$, reported in hours.\n\nTask:\n- Starting from Bayes’ rule and the specified Gaussian prior and Gaussian likelihoods, formalize the Bayesian posterior distribution for $\\boldsymbol{\\theta}$ given $(\\boldsymbol{z}_r, \\boldsymbol{z}_p)$ under the stated independence assumptions.\n- From the posterior of $\\boldsymbol{\\theta}$, derive the induced distribution of the doubling time $t_d$.\n- For a given credibility level $0.95$, compute the lower and upper endpoints of the $0.95$ credible interval for $t_d$ in hours.\n\nScientific realism requirements:\n- Treat $\\boldsymbol{\\Sigma}_r$ and $\\boldsymbol{\\Sigma}_p$ as known, positive definite covariance matrices reflecting measurement uncertainty in log-abundances.\n- Treat $\\mathbf{H}_r$ and $\\mathbf{H}_p$ as known mappings derived from a mechanistic linearization of the dependence of log-abundances on pathway capacities in a whole-cell model.\n\nYour program must implement the analytic solution for the posterior of $\\boldsymbol{\\theta}$ under linear-Gaussian assumptions and then compute the $0.95$ credible interval for $t_d$ using the induced distribution. Do not use sampling-based methods.\n\nPhysical units:\n- Report all doubling-time credible-interval endpoints in hours.\n\nAngle unit:\n- No angles are involved.\n\nPercentages:\n- Use the decimal form $0.95$ for the credibility level.\n\nTest Suite:\nUse $d = 2$ in all cases. For each case, all matrices and vectors are explicitly specified.\n\nCase $1$ (general happy path):\n- Prior: $\\boldsymbol{m}_0 = [0.0, 0.0]^{\\top}$, $\\mathbf{S}_0 = \\mathrm{diag}([0.5, 0.5])$.\n- RNA-seq: $\\mathbf{H}_r = \\begin{bmatrix}1.0 & 0.0 \\\\ 0.0 & 1.0\\end{bmatrix}$, $\\boldsymbol{\\Sigma}_r = \\mathrm{diag}([0.04, 0.04])$, $\\boldsymbol{z}_r = [1.0, 0.5]^{\\top}$.\n- Proteomics: $\\mathbf{H}_p = \\begin{bmatrix}1.0 & 1.0\\end{bmatrix}$, $\\boldsymbol{\\Sigma}_p = \\mathrm{diag}([0.09])$, $\\boldsymbol{z}_p = [1.2]^{\\top}$.\n- Phenotype map: $c_0 = -0.3$, $\\boldsymbol{c} = [0.8, 0.4]^{\\top}$.\n\nCase $2$ (high-noise, prior-dominated boundary):\n- Prior: $\\boldsymbol{m}_0 = [0.0, 0.0]^{\\top}$, $\\mathbf{S}_0 = \\mathrm{diag}([0.5, 0.5])$.\n- RNA-seq: $\\mathbf{H}_r = \\begin{bmatrix}1.0 & 0.0 \\\\ 0.0 & 1.0\\end{bmatrix}$, $\\boldsymbol{\\Sigma}_r = \\mathrm{diag}([1.0, 1.0])$, $\\boldsymbol{z}_r = [0.0, 0.0]^{\\top}$.\n- Proteomics: $\\mathbf{H}_p = \\begin{bmatrix}1.0 & 1.0\\end{bmatrix}$, $\\boldsymbol{\\Sigma}_p = \\mathrm{diag}([1.44])$, $\\boldsymbol{z}_p = [0.0]^{\\top}$.\n- Phenotype map: $c_0 = -0.3$, $\\boldsymbol{c} = [0.8, 0.4]^{\\top}$.\n\nCase $3$ (low-noise, conflicting evidence edge case):\n- Prior: $\\boldsymbol{m}_0 = [0.2, -0.2]^{\\top}$, $\\mathbf{S}_0 = \\mathrm{diag}([1.0, 0.3])$.\n- RNA-seq: $\\mathbf{H}_r = \\begin{bmatrix}1.0 & 0.0 \\\\ 0.0 & 1.0\\end{bmatrix}$, $\\boldsymbol{\\Sigma}_r = \\mathrm{diag}([0.01, 0.01])$, $\\boldsymbol{z}_r = [2.0, -1.0]^{\\top}$.\n- Proteomics: $\\mathbf{H}_p = \\begin{bmatrix}-0.5 & 1.5\\end{bmatrix}$, $\\boldsymbol{\\Sigma}_p = \\mathrm{diag}([0.04])$, $\\boldsymbol{z}_p = [0.2]^{\\top}$.\n- Phenotype map: $c_0 = -0.3$, $\\boldsymbol{c} = [0.8, 0.4]^{\\top}$.\n\nRequired final output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of lower then upper endpoint for each case concatenated across all cases. For example, the output must be of the form `[lb_1,ub_1,lb_2,ub_2,lb_3,ub_3]`, where each `lb_i` and `ub_i` are floats representing the endpoints of the $0.95$ credible interval for $t_d$ in hours for case $i$.",
            "solution": "The problem presents a valid and well-posed Bayesian inference task grounded in the principles of systems biology modeling. All necessary parameters and conditions are provided, and there are no scientific or logical inconsistencies. We can therefore proceed with a formal derivation and solution.\n\nThe core of the problem is to update our knowledge about a set of latent pathway capacities, represented by the vector $\\boldsymbol{\\theta} \\in \\mathbb{R}^{d}$, using two sources of experimental data: RNA-seq abundances $\\boldsymbol{z}_r$ and proteomics abundances $\\boldsymbol{z}_p$. We begin from a prior belief about $\\boldsymbol{\\theta}$ and update it based on the likelihood of observing the data, finally propagating the uncertainty in $\\boldsymbol{\\theta}$ to a predicted phenotype, the cell doubling time $t_d$.\n\n**1. Posterior Distribution of Latent Parameters $\\boldsymbol{\\theta}$**\n\nAccording to Bayes' rule, the posterior probability distribution of the parameters $\\boldsymbol{\\theta}$ given the data $(\\boldsymbol{z}_r, \\boldsymbol{z}_p)$ is proportional to the product of the likelihood and the prior:\n$$\np(\\boldsymbol{\\theta} \\mid \\boldsymbol{z}_r, \\boldsymbol{z}_p) \\propto p(\\boldsymbol{z}_r, \\boldsymbol{z}_p \\mid \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta})\n$$\nThe problem states that the RNA-seq and proteomics measurements are conditionally independent given $\\boldsymbol{\\theta}$. Thus, the joint likelihood can be factored:\n$$\np(\\boldsymbol{z}_r, \\boldsymbol{z}_p \\mid \\boldsymbol{\\theta}) = p(\\boldsymbol{z}_r \\mid \\boldsymbol{\\theta}) p(\\boldsymbol{z}_p \\mid \\boldsymbol{\\theta})\n$$\nThe posterior is therefore proportional to the product of three Gaussian distributions:\n$$\np(\\boldsymbol{\\theta} \\mid \\boldsymbol{z}_r, \\boldsymbol{z}_p) \\propto \\mathcal{N}(\\boldsymbol{z}_r \\mid \\mathbf{H}_r \\boldsymbol{\\theta}, \\boldsymbol{\\Sigma}_r) \\cdot \\mathcal{N}(\\boldsymbol{z}_p \\mid \\mathbf{H}_p \\boldsymbol{\\theta}, \\boldsymbol{\\Sigma}_p) \\cdot \\mathcal{N}(\\boldsymbol{\\theta} \\mid \\boldsymbol{m}_0, \\mathbf{S}_0)\n$$\nThe product of Gaussian distributions is also a Gaussian (up to a normalization constant). The exponent of the posterior distribution is a sum of quadratic forms. Taking the logarithm of the posterior (and dropping terms not dependent on $\\boldsymbol{\\theta}$):\n$$\n\\ln p(\\boldsymbol{\\theta} \\mid \\boldsymbol{z}_r, \\boldsymbol{z}_p) = -\\frac{1}{2} (\\boldsymbol{z}_r - \\mathbf{H}_r \\boldsymbol{\\theta})^{\\top} \\boldsymbol{\\Sigma}_r^{-1} (\\boldsymbol{z}_r - \\mathbf{H}_r \\boldsymbol{\\theta}) - \\frac{1}{2} (\\boldsymbol{z}_p - \\mathbf{H}_p \\boldsymbol{\\theta})^{\\top} \\boldsymbol{\\Sigma}_p^{-1} (\\boldsymbol{z}_p - \\mathbf{H}_p \\boldsymbol{\\theta}) - \\frac{1}{2} (\\boldsymbol{\\theta} - \\boldsymbol{m}_0)^{\\top} \\mathbf{S}_0^{-1} (\\boldsymbol{\\theta} - \\boldsymbol{m}_0) + C\n$$\nTo find the parameters of the posterior Gaussian, $\\boldsymbol{\\theta} \\mid \\boldsymbol{z}_r, \\boldsymbol{z}_p \\sim \\mathcal{N}(\\boldsymbol{m}_{\\text{post}}, \\mathbf{S}_{\\text{post}})$, we expand the expression and group terms that are quadratic and linear in $\\boldsymbol{\\theta}$. The resulting posterior precision matrix, $\\mathbf{S}_{\\text{post}}^{-1}$, is the sum of the prior precision and the information from the two likelihoods:\n$$\n\\mathbf{S}_{\\text{post}}^{-1} = \\mathbf{S}_0^{-1} + \\mathbf{H}_r^{\\top} \\boldsymbol{\\Sigma}_r^{-1} \\mathbf{H}_r + \\mathbf{H}_p^{\\top} \\boldsymbol{\\Sigma}_p^{-1} \\mathbf{H}_p\n$$\nThe posterior covariance matrix is the inverse of the precision matrix: $\\mathbf{S}_{\\text{post}} = (\\mathbf{S}_{\\text{post}}^{-1})^{-1}$.\n\nThe posterior mean, $\\boldsymbol{m}_{\\text{post}}$, is a precision-weighted average of the prior mean and the data-implied means:\n$$\n\\boldsymbol{m}_{\\text{post}} = \\mathbf{S}_{\\text{post}} \\left( \\mathbf{S}_0^{-1} \\boldsymbol{m}_0 + \\mathbf{H}_r^{\\top} \\boldsymbol{\\Sigma}_r^{-1} \\boldsymbol{z}_r + \\mathbf{H}_p^{\\top} \\boldsymbol{\\Sigma}_p^{-1} \\boldsymbol{z}_p \\right)\n$$\n\n**2. Distribution of the Doubling Time $t_d$**\n\nThe doubling time $t_d$ is related to $\\boldsymbol{\\theta}$ through a series of transformations. First, we define an intermediate variable, $y$, as the linear predictor for the log-growth rate:\n$$\ny = c_0 + \\boldsymbol{c}^{\\top} \\boldsymbol{\\theta}\n$$\nSince $\\boldsymbol{\\theta}$ follows a multivariate Gaussian distribution, and $y$ is an affine transformation of $\\boldsymbol{\\theta}$, $y$ must follow a univariate Gaussian distribution, $y \\sim \\mathcal{N}(\\mu_y, \\sigma_y^2)$. The mean and variance of $y$ are found by applying standard rules for expectations and variances of linear transformations of random vectors:\n$$\n\\mu_y = E[y] = E[c_0 + \\boldsymbol{c}^{\\top} \\boldsymbol{\\theta}] = c_0 + \\boldsymbol{c}^{\\top} E[\\boldsymbol{\\theta}] = c_0 + \\boldsymbol{c}^{\\top} \\boldsymbol{m}_{\\text{post}}\n$$\n$$\n\\sigma_y^2 = \\text{Var}[y] = \\text{Var}[c_0 + \\boldsymbol{c}^{\\top} \\boldsymbol{\\theta}] = \\text{Var}[\\boldsymbol{c}^{\\top} \\boldsymbol{\\theta}] = \\boldsymbol{c}^{\\top} \\text{Var}[\\boldsymbol{\\theta}] \\boldsymbol{c} = \\boldsymbol{c}^{\\top} \\mathbf{S}_{\\text{post}} \\boldsymbol{c}\n$$\nThe cell growth rate is $\\mu = \\exp(y)$, which implies that $\\mu$ follows a log-normal distribution. The doubling time is given by:\n$$\nt_d = \\frac{\\ln(2)}{\\mu} = \\frac{\\ln(2)}{\\exp(y)} = \\ln(2) \\exp(-y)\n$$\nTo find the distribution of $t_d$, we consider its natural logarithm, $w = \\ln(t_d)$:\n$$\nw = \\ln(t_d) = \\ln(\\ln(2) \\exp(-y)) = \\ln(\\ln(2)) - y\n$$\nSince $y$ is normally distributed, $w$ is also normally distributed, $w \\sim \\mathcal{N}(\\mu_w, \\sigma_w^2)$. Its mean and variance are:\n$$\n\\mu_w = E[w] = E[\\ln(\\ln(2)) - y] = \\ln(\\ln(2)) - \\mu_y\n$$\n$$\n\\sigma_w^2 = \\text{Var}[w] = \\text{Var}[\\ln(\\ln(2)) - y] = (-1)^2 \\text{Var}[y] = \\sigma_y^2\n$$\nThus, the log of the doubling time, $w = \\ln(t_d)$, is normally distributed with mean $\\mu_w = \\ln(\\ln(2)) - (c_0 + \\boldsymbol{c}^{\\top} \\boldsymbol{m}_{\\text{post}})$ and variance $\\sigma_w^2 = \\boldsymbol{c}^{\\top} \\mathbf{S}_{\\text{post}} \\boldsymbol{c}$. This means that the doubling time $t_d$ itself follows a log-normal distribution, $t_d \\sim \\text{Log-Normal}(\\mu_w, \\sigma_w^2)$.\n\n**3. Credible Interval for $t_d$**\n\nA $0.95$ credible interval for $t_d$ is an interval $[L, U]$ such that the posterior probability $P(L \\le t_d \\le U) = 0.95$. We can construct an equal-tailed interval by first finding the corresponding interval for the normally distributed variable $w = \\ln(t_d)$ and then transforming its endpoints.\n\nFor $w \\sim \\mathcal{N}(\\mu_w, \\sigma_w^2)$, a $0.95$ credible interval is given by $[\\mu_w + z_{0.025} \\sigma_w, \\mu_w + z_{0.975} \\sigma_w]$, where $z_{0.025}$ and $z_{0.975}$ are the $2.5\\%$ and $97.5\\%$ quantiles of the standard normal distribution $\\mathcal{N}(0, 1)$, respectively. Note that $z_{0.025} = -z_{0.975}$. Let $z_{\\alpha/2} = z_{0.025}$ and $z_{1-\\alpha/2} = z_{0.975}$ for a credibility level of $1-\\alpha=0.95$.\n\nThe lower bound for $w$ is $w_L = \\mu_w + z_{0.025} \\sigma_w$.\nThe upper bound for $w$ is $w_U = \\mu_w + z_{0.975} \\sigma_w$.\n\nSince $t_d = \\exp(w)$ and the exponential function is monotonic, the credible interval for $t_d$ is found by exponentiating the endpoints of the interval for $w$:\n$$\n\\text{lower bound: } t_{d,L} = \\exp(w_L) = \\exp(\\mu_w + z_{0.025} \\sigma_w)\n$$\n$$\n\\text{upper bound: } t_{d,U} = \\exp(w_U) = \\exp(\\mu_w + z_{0.975} \\sigma_w)\n$$\nThese formulas provide an analytic solution for the credible interval endpoints, which can be implemented numerically.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef calculate_credible_interval(m0, S0, Hr, Sr, zr, Hp, Sp, zp, c0, c):\n    \"\"\"\n    Computes the 0.95 credible interval for the doubling time td.\n    \n    Args:\n        m0 (np.ndarray): Prior mean vector (d,).\n        S0 (np.ndarray): Prior covariance matrix (d, d).\n        Hr (np.ndarray): Design matrix for RNA-seq (nr, d).\n        Sr (np.ndarray): Covariance matrix for RNA-seq noise (nr, nr).\n        zr (np.ndarray): RNA-seq observation vector (nr,).\n        Hp (np.ndarray): Design matrix for Proteomics (np, d).\n        Sp (np.ndarray): Covariance matrix for Proteomics noise (np, np).\n        zp (np.ndarray): Proteomics observation vector (np,).\n        c0 (float): Scalar offset for phenotype map.\n        c (np.ndarray): Vector for phenotype map (d,).\n\n    Returns:\n        tuple[float, float]: Lower and upper bounds of the 0.95 credible interval for td.\n    \"\"\"\n    # Defensive reshaping of input vectors to column vectors for robust matrix algebra.\n    m0 = m0.reshape(-1, 1)\n    zr = zr.reshape(-1, 1)\n    zp = zp.reshape(-1, 1)\n    c = c.reshape(-1, 1)\n    \n    # 1. Calculate Posterior Distribution Parameters\n    S0_inv = np.linalg.inv(S0)\n    Sr_inv = np.linalg.inv(Sr)\n    Sp_inv = np.linalg.inv(Sp)\n\n    # Posterior precision matrix\n    S_post_inv = S0_inv + Hr.T @ Sr_inv @ Hr + Hp.T @ Sp_inv @ Hp\n    \n    # Posterior covariance matrix\n    S_post = np.linalg.inv(S_post_inv)\n\n    # Term for a posterior mean calculation\n    v = S0_inv @ m0 + Hr.T @ Sr_inv @ zr + Hp.T @ Sp_inv @ zp\n\n    # Posterior mean vector\n    m_post = S_post @ v\n    \n    # 2. Derive Distribution of Doubling Time\n    # y = c0 + c.T @ theta ~ N(mu_y, sigma_y^2)\n    mu_y = c0 + c.T @ m_post\n    sigma_y_sq = c.T @ S_post @ c\n    \n    # w = ln(td) = ln(ln(2)) - y ~ N(mu_w, sigma_w^2)\n    mu_w = np.log(np.log(2.0)) - mu_y\n    sigma_w_sq = sigma_y_sq\n    sigma_w = np.sqrt(sigma_w_sq)\n\n    # 3. Compute Credible Interval\n    credibility_level = 0.95\n    alpha = 1.0 - credibility_level\n    \n    # Get quantiles from standard normal distribution\n    z_lower = norm.ppf(alpha / 2.0)  # e.g., for 0.95 -> 0.025\n    z_upper = norm.ppf(1.0 - alpha / 2.0)  # e.g., for 0.95 -> 0.975\n    \n    # Calculate interval for w = ln(td)\n    w_lower = mu_w + z_lower * sigma_w\n    w_upper = mu_w + z_upper * sigma_w\n    \n    # Transform back to get the interval for td\n    td_lower = np.exp(w_lower)\n    td_upper = np.exp(w_upper)\n\n    # .item() extracts the scalar from a 1x1 array\n    return td_lower.item(), td_upper.item()\n\ndef solve():\n    \"\"\"\n    Defines test cases and runs the solver to produce the final output.\n    \"\"\"\n    test_cases = [\n        # Case 1: General happy path\n        {\n            \"m0\": np.array([0.0, 0.0]),\n            \"S0\": np.diag([0.5, 0.5]),\n            \"Hr\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"Sr\": np.diag([0.04, 0.04]),\n            \"zr\": np.array([1.0, 0.5]),\n            \"Hp\": np.array([[1.0, 1.0]]),\n            \"Sp\": np.diag([0.09]),\n            \"zp\": np.array([1.2]),\n            \"c0\": -0.3,\n            \"c\": np.array([0.8, 0.4])\n        },\n        # Case 2: High-noise, prior-dominated boundary\n        {\n            \"m0\": np.array([0.0, 0.0]),\n            \"S0\": np.diag([0.5, 0.5]),\n            \"Hr\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"Sr\": np.diag([1.0, 1.0]),\n            \"zr\": np.array([0.0, 0.0]),\n            \"Hp\": np.array([[1.0, 1.0]]),\n            \"Sp\": np.diag([1.44]),\n            \"zp\": np.array([0.0]),\n            \"c0\": -0.3,\n            \"c\": np.array([0.8, 0.4])\n        },\n        # Case 3: Low-noise, conflicting evidence edge case\n        {\n            \"m0\": np.array([0.2, -0.2]),\n            \"S0\": np.diag([1.0, 0.3]),\n            \"Hr\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"Sr\": np.diag([0.01, 0.01]),\n            \"zr\": np.array([2.0, -1.0]),\n            \"Hp\": np.array([[-0.5, 1.5]]),\n            \"Sp\": np.diag([0.04]),\n            \"zp\": np.array([0.2]),\n            \"c0\": -0.3,\n            \"c\": np.array([0.8, 0.4])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        lower_bound, upper_bound = calculate_credible_interval(**case)\n        results.extend([lower_bound, upper_bound])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}