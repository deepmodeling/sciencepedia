{"hands_on_practices": [{"introduction": "Reinforcement learning models provide a powerful mathematical framework for understanding how organisms learn from trial and error. At the heart of these models is the concept of a reward prediction error, a signal that is widely believed to be carried by phasic dopamine in corticostriatal circuits. This practice asks you to perform the core computational step of Temporal Difference (TD) learning: updating an action-value estimate based on experience, thereby connecting a high-level learning theory to its precise algorithmic implementation. [@problem_id:4479810]", "problem": "A cortico-basal ganglia-thalamo-cortical loop model of executive control posits that corticostriatal synapses in the dorsal striatum encode state-action values $Q_{t}(s,a)$ that guide action selection through the basal ganglia output nuclei. Phasic dopamine release from the Ventral Tegmental Area (VTA) and Substantia Nigra pars compacta (SNc) conveys a reward prediction error (RPE), which modulates synaptic plasticity and updates these values. Under the standard reinforcement learning interpretation, the RPE at time $t$ compares experienced and expected return, and striatal value updates are proportional to this error with proportionality constant equal to the learning rate $\\alpha$. The expected return incorporates an immediate reward $r_{t}$ and a discounted estimate of future return using discount factor $\\gamma$ and the maximal estimated value in the next state $s'$ over available actions.\n\nConsider a single trial in which the prefrontal cortical state $s$ leads to the selection of action $a$, yielding immediate reward $r_{t}$ and transitioning to state $s'$. The current striatal estimate for the executed state-action pair is $Q_{t}(s,a)$. The next-state maximal value is $\\max_{a'} Q_{t}(s',a')$. Assume parameters $\\alpha=0.1$, $\\gamma=0.9$, $Q_{t}(s,a)=0.5$, $r_{t}=1$, and $\\max_{a'} Q_{t}(s',a')=0.8$. Using the definition of reward prediction error and the principle that the update to $Q_{t}(s,a)$ is proportional to this error, compute the updated value $Q_{t+1}(s,a)$. Express your final answer as a single real number, with no additional units or symbols. No rounding is required.", "solution": "The problem asks to compute the updated state-action value, $Q_{t+1}(s,a)$, using the principles of Q-learning, a model of Temporal Difference (TD) learning. The update rule is based on the reward prediction error (RPE).\n\nThe complete Q-learning update rule is given by:\n$$\nQ_{t+1}(s,a) = Q_{t}(s,a) + \\alpha \\left[ \\left( r_{t} + \\gamma \\max_{a'} Q_{t}(s',a') \\right) - Q_{t}(s,a) \\right]\n$$\nWhere:\n-   $Q_{t}(s,a)$ is the current value of taking action $a$ in state $s$.\n-   $\\alpha$ is the learning rate.\n-   $r_{t}$ is the immediate reward received.\n-   $\\gamma$ is the discount factor.\n-   $\\max_{a'} Q_{t}(s',a')$ is the maximum Q-value for the next state $s'$.\n-   The term $\\left( r_{t} + \\gamma \\max_{a'} Q_{t}(s',a') \\right)$ is the TD target, representing the new estimate of the return.\n-   The term inside the brackets is the Reward Prediction Error (RPE), $\\delta_t$.\n\nWe are given the following values:\n-   Learning rate $\\alpha = 0.1$\n-   Discount factor $\\gamma = 0.9$\n-   Current Q-value $Q_{t}(s,a) = 0.5$\n-   Immediate reward $r_{t} = 1$\n-   Maximal next-state Q-value $\\max_{a'} Q_{t}(s',a') = 0.8$\n\nStep 1: Calculate the TD Target.\nThe TD target is the sum of the immediate reward and the discounted value of the best action in the next state.\n$$\n\\text{TD Target} = r_{t} + \\gamma \\max_{a'} Q_{t}(s',a') = 1 + (0.9)(0.8) = 1 + 0.72 = 1.72\n$$\n\nStep 2: Calculate the Reward Prediction Error (RPE).\nThe RPE is the difference between the TD target and the current Q-value.\n$$\n\\text{RPE} = \\delta_{t} = \\text{TD Target} - Q_{t}(s,a) = 1.72 - 0.5 = 1.22\n$$\nThis positive RPE signals that the outcome was better than expected.\n\nStep 3: Calculate the updated Q-value, $Q_{t+1}(s,a)$.\nThe updated value is the old value plus the RPE scaled by the learning rate.\n$$\nQ_{t+1}(s,a) = Q_{t}(s,a) + \\alpha \\cdot \\delta_{t} = 0.5 + 0.1 \\cdot (1.22)\n$$\n$$\nQ_{t+1}(s,a) = 0.5 + 0.122\n$$\n$$\nQ_{t+1}(s,a) = 0.622\n$$\nThus, the updated striatal estimate for the state-action pair $(s,a)$ is $0.622$.", "answer": "$$\n\\boxed{0.622}\n$$", "id": "4479810"}, {"introduction": "Once the brain has learned the values of different actions, it must convert these internal estimates into an overt choice. The softmax policy, which arises from fundamental principles of entropy maximization, offers a principled account of this process, balancing the drive to exploit high-value options with the need to explore alternatives. This exercise will guide you through deriving this policy and analyzing its sensitivity, revealing how executive control circuits can flexibly adjust choice probabilities in response to changing relative values. [@problem_id:4479852]", "problem": "Consider a corticostriatal decision circuit in the human brain that arbitrates between two actions, $a_{1}$ and $a_{2}$, based on values encoded by populations in the Dorsolateral Prefrontal Cortex (dlPFC) and routed through the Basal Ganglia (BG), with conflict monitoring by the Anterior Cingulate Cortex (ACC). Empirical work in neurology and Reinforcement Learning (RL) models supports that such circuits implement stochastic choice policies constrained by neural variability. Assume the circuit selects a probability distribution $P(a_{i})$ over actions to maximize a precision-controlled objective comprising expected value and an entropy regularizer. Specifically, the policy solves the constrained optimization\n$$\n\\max_{\\{P(a_{i})\\}}\\ \\sum_{i} P(a_{i})\\, v_{i} + \\frac{1}{\\beta}\\, H(P)\\quad\\text{subject to}\\quad \\sum_{i} P(a_{i}) = 1,\\ \\ P(a_{i}) \\ge 0,\n$$\nwhere $v_{i}$ denotes the normalized value encoded by the circuit for action $a_{i}$, $\\beta>0$ is an inverse-noise (precision) parameter reflecting neuromodulatory gain, and $H(P) = -\\sum_{i} P(a_{i}) \\ln P(a_{i})$ is the Shannon entropy.\n\nTasks:\n1. Derive, from first principles using the above objective and constraints, the optimal choice policy $P(a_{i})$ as a function of $v_{i}$ and $\\beta$.\n2. Specialize to two actions with normalized values $v = [1, 2]$ (that is, $v_{1} = 1$ and $v_{2} = 2$) and $\\beta = 1$, and compute the exact choice probabilities for $a_{1}$ and $a_{2}$.\n3. Define the relative value difference $\\Delta v = v_{2} - v_{1}$ and, for two actions, express $P(a_{2})$ as a function of $\\Delta v$ and $\\beta$. Compute the exact local sensitivity $\\frac{dP(a_{2})}{d\\Delta v}$ at $\\beta = 1$ and $\\Delta v = 1$. Briefly interpret what this sensitivity implies about how executive control circuits adjust choice probabilities in response to changes in relative values under fixed neural noise.\n\nExpress all final numerical results exactly (do not round or approximate). Provide your final answer as a single row matrix containing $P(a_{1})$, $P(a_{2})$, and $\\left.\\frac{dP(a_{2})}{d\\Delta v}\\right|_{\\beta=1,\\ \\Delta v=1}$, in that order. No physical units are required.", "solution": "This problem involves deriving the softmax choice policy from an entropy-regularized value maximization objective, and then applying it to a specific case.\n\n### 1. Derivation of the Optimal Choice Policy (Softmax)\nThe objective is to maximize the Lagrangian function $\\mathcal{L}$ which incorporates the normalization constraint $\\sum_i P(a_i) = 1$ via a Lagrange multiplier $\\lambda$:\n$$\n\\mathcal{L} = \\sum_{i} P(a_{i})\\, v_{i} - \\frac{1}{\\beta}\\, \\sum_{i} P(a_{i}) \\ln P(a_{i}) + \\lambda \\left(1 - \\sum_{i} P(a_{i})\\right)\n$$\nTo find the optimal probabilities, we take the derivative with respect to an arbitrary $P(a_k)$ and set it to zero:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial P(a_k)} = v_k - \\frac{1}{\\beta} (\\ln P(a_k) + 1) - \\lambda = 0\n$$\nSolving for $P(a_k)$:\n$$\n\\ln P(a_k) = \\beta(v_k - \\lambda) - 1\n$$\n$$\nP(a_k) = \\exp(\\beta v_k - \\beta \\lambda - 1) = \\exp(\\beta v_k) \\cdot \\exp(-(\\beta \\lambda + 1))\n$$\nThe term $\\exp(-(\\beta \\lambda + 1))$ is a constant across all actions. We can represent it as $1/Z$, where $Z$ is a normalization constant.\n$$\nP(a_k) = \\frac{\\exp(\\beta v_k)}{Z}\n$$\nTo find $Z$, we use the constraint $\\sum_k P(a_k) = 1$:\n$$\n\\sum_k \\frac{\\exp(\\beta v_k)}{Z} = 1 \\implies Z = \\sum_j \\exp(\\beta v_j)\n$$\nSubstituting $Z$ back gives the softmax policy:\n$$\nP(a_k) = \\frac{\\exp(\\beta v_k)}{\\sum_{j} \\exp(\\beta v_j)}\n$$\n\n### 2. Calculation of Choice Probabilities\nGiven are two actions with values $v_1 = 1, v_2 = 2$ and precision $\\beta = 1$.\nThe normalization constant (partition function) is:\n$$\nZ = \\exp(1 \\cdot 1) + \\exp(1 \\cdot 2) = e^1 + e^2\n$$\nThe probabilities are:\n$$\nP(a_1) = \\frac{\\exp(1)}{e^1 + e^2} = \\frac{e^1}{e^1(1 + e^1)} = \\frac{1}{1+e}\n$$\n$$\nP(a_2) = \\frac{\\exp(2)}{e^1 + e^2} = \\frac{e^2}{e^1(1 + e^1)} = \\frac{e}{1+e}\n$$\n\n### 3. Sensitivity Analysis\nFor two actions, let $\\Delta v = v_2 - v_1$. We can express $P(a_2)$ in terms of $\\Delta v$:\n$$\nP(a_2) = \\frac{\\exp(\\beta v_2)}{\\exp(\\beta v_1) + \\exp(\\beta v_2)}\n$$\nDivide numerator and denominator by $\\exp(\\beta v_1)$:\n$$\nP(a_2) = \\frac{\\exp(\\beta (v_2 - v_1))}{1 + \\exp(\\beta (v_2 - v_1))} = \\frac{\\exp(\\beta \\Delta v)}{1 + \\exp(\\beta \\Delta v)}\n$$\nThis is the logistic sigmoid function. We need its derivative with respect to $\\Delta v$. Let $x = \\beta \\Delta v$. Then $P(a_2) = \\frac{e^x}{1+e^x}$. The derivative of the sigmoid function $\\sigma(x) = \\frac{1}{1+e^{-x}}$ is $\\sigma(x)(1-\\sigma(x))$. Our form is slightly different. Using the quotient rule:\n$$\n\\frac{d}{d\\Delta v} P(a_2) = \\frac{\\beta \\exp(\\beta \\Delta v)(1 + \\exp(\\beta \\Delta v)) - \\exp(\\beta \\Delta v)(\\beta \\exp(\\beta \\Delta v))}{(1 + \\exp(\\beta \\Delta v))^2}\n$$\n$$\n= \\frac{\\beta \\exp(\\beta \\Delta v)}{(1 + \\exp(\\beta \\Delta v))^2}\n$$\nNow, we evaluate this at $\\beta=1$ and $\\Delta v = v_2 - v_1 = 2-1 = 1$:\n$$\n\\left.\\frac{dP(a_{2})}{d\\Delta v}\\right|_{\\beta=1,\\ \\Delta v=1} = \\frac{1 \\cdot \\exp(1 \\cdot 1)}{(1 + \\exp(1 \\cdot 1))^2} = \\frac{e}{(1+e)^2}\n$$\nInterpretation: This sensitivity value, $\\frac{e}{(1+e)^2}$, quantifies how much the probability of choosing the better option ($a_2$) increases for a small increase in its value advantage ($\\Delta v$). It reflects the circuit's ability to flexibly adapt its choices to the relative values of the options. A larger sensitivity indicates a more decisive system that more strongly shifts its preference toward an option as it becomes better.\n\nThe final answer matrix should contain the three calculated values:\n1.  $P(a_1) = \\frac{1}{1+e}$\n2.  $P(a_2) = \\frac{e}{1+e}$\n3.  Sensitivity = $\\frac{e}{(1+e)^2}$", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{1+e} & \\frac{e}{1+e} & \\frac{e}{(1+e)^2} \\end{pmatrix}}\n$$", "id": "4479852"}, {"introduction": "Decisions are not instantaneous events but dynamic processes that unfold over time as evidence is gathered and weighed. The Drift Diffusion Model (DDM) is a cornerstone of cognitive neuroscience, providing a quantitative account of the speed and accuracy of simple choices by modeling the noisy accumulation of evidence toward a decision boundary. In this practice, you will derive a fundamental relationship between the strength of evidence and decision speed under simplified conditions, offering core insights into the speed-accuracy trade-off that governs deliberation. [@problem_id:4479776]", "problem": "In a simplified account of executive control and decision-making circuits, cortical populations in prefrontal and parietal cortices accumulate evidence over time and trigger a choice when an internal decision variable reaches a bound, with basal ganglia pathways mediating threshold crossing into action. A well-tested normative and mechanistic description of such accumulation is the Drift Diffusion Model (DDM), which models the decision variable as a one-dimensional stochastic process with constant drift and additive white noise. Consider the stochastic differential equation for the decision variable $X_{t}$,\n$$\n\\mathrm{d}X_{t} \\;=\\; v\\,\\mathrm{d}t \\;+\\; \\sigma\\,\\mathrm{d}W_{t},\n$$\nwhere $v$ is the drift rate reflecting the mean momentary evidence, $\\sigma$ is the diffusion amplitude reflecting within-trial variability, and $W_{t}$ is a standard Wiener process. Let the decision bounds be symmetric and absorbing at $X=\\pm a$ with $a>0$, and let the process start at $X_{0}=0$. The decision time $T$ is the first-passage time to either bound. You may assume the starting point is unbiased and that any non-decision delays (for sensory encoding and motor execution) are excluded from $T$.\n\nTask: Starting from the stochastic differential equation and the definition of first-passage time and without invoking any pre-memorized closed-form first-passage-time formulas, derive the leading-order approximation of the mean decision time $E[T]$ in the limit of negligible within-trial noise. Your derivation must rely on first principles appropriate to this setting (e.g., deterministic limits of stochastic dynamics and absorbing boundary conditions) and must make explicit any assumptions used.\n\nThen, apply your approximation to the following neural recording scenario, in which simultaneous population activity from the lateral intraparietal area (LIP) is analyzed during a fixed-coherence motion discrimination task. The inferred parameters of the DDM mapping for a given condition are:\n- Drift rate $v = +0.394\\ \\mathrm{a.u.}/\\mathrm{s}$ (in arbitrary evidence units per second),\n- Bound magnitude $a = 0.123\\ \\mathrm{a.u.}$,\n- Diffusion amplitude $\\sigma = 0.030\\ \\mathrm{a.u.}/\\sqrt{\\mathrm{s}}$.\n\nCompute the approximate mean decision time using your derivation. Round your numerical answer to four significant figures. Express the final time in milliseconds.\n\nIn your derivation, clearly state the conditions on the neural data and task under which the negligible-noise approximation would be expected to hold in practice (e.g., properties of the drift, the noise, the bounds, and circuit timescales). Your final reported answer must be a single number corresponding to the time in milliseconds, rounded as specified.", "solution": "The problem asks for two things: first, to derive the leading-order approximation of the mean decision time in a Drift-Diffusion Model (DDM) in the limit of negligible noise, and second, to apply this approximation to a given set of parameters.\n\n### Derivation of Approximate Mean Decision Time\n\nThe Drift-Diffusion Model is described by the stochastic differential equation:\n$$\n\\mathrm{d}X_{t} = v\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t}\n$$\nThe \"limit of negligible within-trial noise\" means we consider the case where the diffusion amplitude $\\sigma$ approaches zero ($\\sigma \\to 0$). In this limit, the stochastic term $\\sigma\\,\\mathrm{d}W_t$ becomes negligible, and the dynamics of the decision variable $X_t$ become deterministic.\n\nThe stochastic differential equation simplifies to an ordinary differential equation:\n$$\n\\frac{\\mathrm{d}X_t}{\\mathrm{d}t} = v\n$$\nWe can solve this ODE by integrating with respect to time, given the initial condition $X_0 = 0$.\n$$\n\\int_{X_0}^{X_T} dX = \\int_{0}^{T} v\\,dt\n$$\n$$\nX_T - X_0 = vT\n$$\nSubstituting $X_0 = 0$:\n$$\nX_T = vT\n$$\nThis equation describes the trajectory of the decision variable in the absence of noise. The decision time $T$ is the first-passage time to one of the absorbing boundaries at $X = \\pm a$. Since the drift rate $v$ is a positive constant ($v > 0$), the decision variable $X_t$ will increase linearly from 0 and will only ever reach the positive boundary at $X = +a$.\nTherefore, the decision is made at the time $T$ when $X_T = a$.\n$$\na = vT\n$$\nSolving for $T$, we get the decision time in this deterministic limit:\n$$\nT = \\frac{a}{v}\n$$\nSince this is a deterministic outcome, the expected or mean decision time $E[T]$ is simply this value. This is the leading-order approximation for the mean decision time.\n$$\nE[T] \\approx \\frac{a}{v}\n$$\n\n**Conditions for Approximation:**\nThis negligible-noise approximation is valid when the deterministic drift is much stronger than the random diffusion. Mathematically, the full expression for mean decision time is $E[T] = \\frac{a}{v} \\tanh(\\frac{av}{\\sigma^2})$. Our approximation $T \\approx a/v$ holds when $\\tanh(\\frac{av}{\\sigma^2}) \\approx 1$, which requires its argument to be large: $\\frac{av}{\\sigma^2} \\gg 1$. In practical terms, this means the approximation is good for tasks with:\n1.  **High signal-to-noise ratio:** The evidence is strong and clear (large $|v|$) relative to the level of internal noise (small $\\sigma$).\n2.  **High-coherence stimuli:** In perceptual tasks, this corresponds to stimuli that are easy to discriminate.\n3.  **Low neural variability:** The underlying neural circuits operate with high fidelity.\n\n### Numerical Calculation\n\nWe are given the following parameters from a neural recording:\n-   Drift rate $v = +0.394\\ \\mathrm{a.u.}/\\mathrm{s}$\n-   Bound magnitude $a = 0.123\\ \\mathrm{a.u.}$\n-   Diffusion amplitude $\\sigma = 0.030\\ \\mathrm{a.u.}/\\sqrt{\\mathrm{s}}$ (This is not needed for the approximation but confirms the regime is low-noise as $\\frac{av}{\\sigma^2} = \\frac{0.123 \\times 0.394}{0.030^2} \\approx 53.8 \\gg 1$).\n\nUsing our derived approximation:\n$$\nT \\approx \\frac{a}{v} = \\frac{0.123\\ \\mathrm{a.u.}}{0.394\\ \\mathrm{a.u.}/\\mathrm{s}} \\approx 0.31218274... \\ \\mathrm{s}\n$$\nThe problem asks for the answer in milliseconds, rounded to four significant figures.\n$$\nT_{\\mathrm{ms}} = 0.31218274... \\ \\mathrm{s} \\times 1000 \\ \\mathrm{ms}/\\mathrm{s} = 312.18274... \\ \\mathrm{ms}\n$$\nRounding to four significant figures, we get:\n$$\nT_{\\mathrm{ms}} \\approx 312.2 \\ \\mathrm{ms}\n$$", "answer": "$$\\boxed{312.2}$$", "id": "4479776"}]}