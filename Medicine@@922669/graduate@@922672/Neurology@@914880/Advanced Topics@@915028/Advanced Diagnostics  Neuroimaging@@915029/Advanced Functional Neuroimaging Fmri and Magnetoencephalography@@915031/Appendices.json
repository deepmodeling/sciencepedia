{"hands_on_practices": [{"introduction": "Understanding functional neuroimaging data begins with a solid grasp of the forward model—the process that links neural activity to the measured signal. This first practice invites you to build a computational model of the Blood Oxygenation Level-Dependent (BOLD) signal from the ground up, using the canonical hemodynamic response function (HRF) [@problem_id:4445761]. By implementing the convolution of a stimulus with the HRF, you will gain direct, hands-on experience with the fundamental generative process assumed by the General Linear Model (GLM).", "problem": "You are to implement a numerical model of the predicted Blood Oxygen Level Dependent (BOLD) response in functional Magnetic Resonance Imaging (fMRI), produced by a brief external stimulus, using linear time-invariant systems with a canonical Hemodynamic Response Function (HRF). Start from the following fundamental definitions and facts:\n\n1. Continuous-time linear convolution: for an input (stimulus) $x(t)$ and an impulse response $h(t)$, the output $y(t)$ is defined by\n$$\ny(t) = \\int_{-\\infty}^{+\\infty} x(\\tau)\\,h(t-\\tau)\\,d\\tau.\n$$\n\n2. The Gamma function $\\Gamma(\\cdot)$ and the Gamma probability density function: for shape $a>0$ and scale $b>0$,\n$$\ng(t;a,b) = \\begin{cases}\n\\dfrac{t^{a-1} e^{-t/b}}{b^a\\,\\Gamma(a)},  t \\ge 0,\\\\\n0,  t  0.\n\\end{cases}\n$$\n\n3. The canonical double-gamma Hemodynamic Response Function (HRF) is the difference of two normalized Gamma densities with a fixed amplitude ratio. With parameters $a_1$, $b_1$, $a_2$, $b_2$ and a ratio $c>0$, define\n$$\nh(t) = g(t; a_1, b_1) \\;-\\; \\frac{1}{c}\\, g(t; a_2, b_2).\n$$\nThis $h(t)$ is causal (zero for $t0$). For the canonical choice, use $a_1=6$, $b_1=1$, $a_2=16$, $b_2=1$, and $c=6$.\n\n4. Discrete-time approximation by Riemann sums: adopt a sampling interval $\\Delta t0$ and a finite simulation horizon $T_{\\max}  0$. Let $t_n = n\\,\\Delta t$ for integers $n=0,1,\\dots,N-1$ where $N = \\lfloor T_{\\max}/\\Delta t \\rfloor + 1$. Approximate the continuous-time convolution by the discrete sum\n$$\ny[n] \\approx \\sum_{k=0}^{n} s[k]\\, h[n-k]\\, \\Delta t,\n$$\nwhere $h[n] := h(t_n)$ and $s[k]$ is a discrete representation of the stimulus.\n\n5. Stimulus model: a single brief event of duration $W$ seconds and unit amplitude that begins at $t=0$ is modeled as\n$$\nx(t) = \\begin{cases}\n1,  0 \\le t  W,\\\\\n0,  \\text{otherwise}.\n\\end{cases}\n$$\nTo minimize discretization bias, represent $x(t)$ in discrete time by per-bin averages\n$$\ns[k] := \\frac{1}{\\Delta t} \\int_{k\\Delta t}^{(k+1)\\Delta t} x(\\tau)\\,d\\tau = \\frac{\\text{length}\\left([k\\Delta t,(k+1)\\Delta t)\\cap[0,W)\\right]}{\\Delta t}.\n$$\nNote that $0 \\le s[k] \\le 1$ and $\\sum_k s[k]\\,\\Delta t = W$.\n\nTasks:\n\nA. From the definitions above, derive the discrete-time convolution formula that predicts the BOLD time course $y[n]$ from $s[k]$ and $h[n]$ using a Riemann-sum approximation. Your implementation must use the per-bin averaged $s[k]$ defined above.\n\nB. Implement a program that computes $y[n]$ for specified $\\Delta t$, $T_{\\max}$, and $W$, using the canonical double-gamma HRF parameters $a_1=6$, $b_1=1$, $a_2=16$, $b_2=1$, $c=6$.\n\nC. For each test case, report:\n- The values of $y(t)$ at specified query times by indexing the nearest discrete sample using the rule $n=\\operatorname{round}(t/\\Delta t)$.\n- The peak time $t_{\\text{peak}}$ in seconds and the peak amplitude $y_{\\text{peak}}$, where the peak is defined as the maximum of $y[n]$ over the simulated window.\n- The numerical area under $y(t)$ over the simulation window, computed as $\\sum_{n=0}^{N-1} y[n]\\,\\Delta t$. The unit of this area is seconds.\n\nUnits:\n- All times must be expressed in seconds.\n- Angles are not used.\n- Report all requested quantities as real numbers (floats). Do not use percentages.\n\nTest Suite:\nUse $W=0.1$ seconds for all cases, and the canonical HRF parameters above. For each case, the program must compute and return the following, in order: a list of samples $[y(t_1), y(t_2), \\dots]$ at the specified query times, followed by $t_{\\text{peak}}$, then $y_{\\text{peak}}$, then the area. The test cases are:\n\n- Case 1 (baseline resolution): $\\Delta t = 0.1$, $T_{\\max}=40.0$, query times $[0.0, 6.0, 12.0, 20.0]$.\n- Case 2 (high resolution): $\\Delta t = 0.01$, $T_{\\max}=40.0$, query times $[6.0]$.\n- Case 3 (coarse resolution, sub-bin event): $\\Delta t = 0.4$, $T_{\\max}=40.0$, query times $[6.0]$.\n- Case 4 (short window truncation): $\\Delta t = 0.1$, $T_{\\max}=10.0$, query times $[0.0, 6.0, 9.9]$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results for all four cases as a comma-separated list of four sublists, each sublist containing: the list of sampled values at the specified query times, followed by $t_{\\text{peak}}$, $y_{\\text{peak}}$, and the area. The entire output must be enclosed in square brackets. For example, an output with schematic placeholders would look like:\n[[[y_1(t_1),y_1(t_2),...],t1_peak,y1_peak,area1],[[y_2(t_1),...],t2_peak,y2_peak,area2],[[...],t3_peak,y3_peak,area3],[[...],t4_peak,y4_peak,area4]].", "solution": "The problem requires the implementation of a numerical model for the Blood Oxygen Level Dependent (BOLD) response in fMRI, based on a linear time-invariant (LTI) system approach. The core of the model is the convolution of a stimulus signal with a canonical Hemodynamic Response Function (HRF). The solution is systematically derived by discretizing the continuous-time model.\n\n### Step 1: Continuous-Time Model Formulation\n\nThe relationship between an input stimulus $x(t)$ and the resulting BOLD response $y(t)$ is described by the convolution integral, a cornerstone of LTI system theory:\n$$\ny(t) = (x * h)(t) = \\int_{-\\infty}^{+\\infty} x(\\tau)\\,h(t-\\tau)\\,d\\tau\n$$\nHere, $h(t)$ is the system's impulse response, known in this context as the Hemodynamic Response Function (HRF).\n\nThe stimulus $x(t)$ is a brief event modeled as a boxcar function of duration $W$ and unit amplitude:\n$$\nx(t) = \\begin{cases}\n1,  0 \\le t  W \\\\\n0,  \\text{otherwise}\n\\end{cases}\n$$\n\nThe HRF, $h(t)$, is defined as the difference of two Gamma probability density functions, a standard biophysical model. The Gamma PDF is given by:\n$$\ng(t;a,b) = \\frac{t^{a-1} e^{-t/b}}{b^a\\,\\Gamma(a)} \\quad \\text{for } t \\ge 0\n$$\nwhere $a$ is the shape parameter, $b$ is the scale parameter, and $\\Gamma(\\cdot)$ is the Euler Gamma function. The resulting canonical double-gamma HRF is:\n$$\nh(t) = g(t; a_1, b_1) - \\frac{1}{c}\\, g(t; a_2, b_2)\n$$\nThe problem specifies the canonical parameters: shape parameters $a_1=6$ and $a_2=16$, scale parameters $b_1=1$ and $b_2=1$, and amplitude ratio $c=6$. As both shape parameters are greater than $1$, $h(0) = 0$. The HRF is causal, i.e., $h(t) = 0$ for $t0$.\n\n### Step 2: Discretization of the Model\n\nFor numerical computation, the continuous model must be discretized. We introduce a uniform sampling interval $\\Delta t  0$ and a finite simulation time horizon $T_{\\max}  0$.\n\nThe time axis is discretized into a sequence of points $t_n = n\\,\\Delta t$ for $n = 0, 1, 2, \\dots, N-1$, where the total number of points is $N = \\lfloor T_{\\max}/\\Delta t \\rfloor + 1$.\n\nThe continuous HRF $h(t)$ is sampled at these time points to obtain the discrete impulse response sequence:\n$$\nh[n] = h(t_n) = h(n\\,\\Delta t)\n$$\n\nThe continuous stimulus $x(t)$ is discretized using a per-bin averaging method to ensure accuracy, especially when the stimulus duration $W$ is on the order of or smaller than $\\Delta t$. This method preserves the total \"energy\" of the stimulus, where $\\sum_k s[k] \\Delta t = W$. The value for each discrete bin $s[k]$ is the average of $x(t)$ over the corresponding time interval:\n$$\ns[k] = \\frac{1}{\\Delta t} \\int_{k\\Delta t}^{(k+1)\\Delta t} x(\\tau)\\,d\\tau\n$$\nFor the given boxcar stimulus $x(t)$, this integral corresponds to the length of the intersection between the interval $[k\\Delta t, (k+1)\\Delta t)$ and the stimulus interval $[0, W)$. This is calculated as:\n$$\ns[k] = \\frac{\\max(0, \\min((k+1)\\Delta t, W) - \\max(k\\Delta t, 0))}{\\Delta t}\n$$\nSince time is non-negative ($k \\ge 0$), this simplifies to:\n$$\ns[k] = \\frac{\\max(0, \\min((k+1)\\Delta t, W) - k\\Delta t)}{\\Delta t}\n$$\n\n### Step 3: Discrete Convolution\n\nThe continuous convolution integral is approximated by a discrete convolution sum. The predicted BOLD response at each time point $t_n$ is:\n$$\ny[n] \\approx \\Delta t \\sum_{k=0}^{n} s[k]\\,h[n-k]\n$$\nThis expression is the product of the sampling interval $\\Delta t$ and the standard discrete convolution of the sequences $s$ and $h$, denoted $(s*h)[n]$. This operation is efficiently implemented using standard numerical libraries such as `numpy.convolve`. The summation limit up to $n$ indicates a causal convolution, which is a natural consequence of $s[k]$ and $h[k]$ being causal signals (zero for negative indices). The full convolution of two sequences of length $N$ yields a sequence of length $2N-1$; we are interested in the response within the simulation window, so the result is truncated to the first $N$ points.\n\n### Step 4: Calculation of Output Quantities\n\nOnce the discrete BOLD response $y[n]$ is computed for $n=0, \\dots, N-1$, the required outputs for each test case are calculated as follows:\n\n1.  **Values at Query Times**: For each specified query time $t_{query}$, the corresponding array index $n$ is determined by the nearest-neighbor rule: $n = \\operatorname{round}(t_{query} / \\Delta t)$. The BOLD response is then $y[n]$.\n\n2.  **Peak Time and Amplitude**: The peak amplitude $y_{\\text{peak}}$ is the maximum value of the sequence $y[n]$. The index of this maximum, $n_{\\text{peak}} = \\operatorname{argmax}_n y[n]$, is used to find the time of the peak, $t_{\\text{peak}} = n_{\\text{peak}} \\Delta t$.\n\n3.  **Area Under the Curve**: The numerical area under the continuous response curve $y(t)$ is approximated by the Riemann sum of the discrete response over the simulation window:\n    $$\n    \\text{Area} = \\sum_{n=0}^{N-1} y[n]\\,\\Delta t\n    $$\nThis process provides a complete algorithm to simulate the BOLD response and extract the specified quantitative features. The implementation will encapsulate this logic within a function that is executed for each set of test case parameters.", "answer": "```python\nimport numpy as np\nfrom scipy.special import gamma\n\ndef solve():\n    \"\"\"\n    Main function to solve the BOLD response modeling problem for all test cases.\n    \"\"\"\n    # Canonical double-gamma HRF parameters\n    A1, B1 = 6, 1\n    A2, B2 = 16, 1\n    C_RATIO = 6\n\n    def gamma_pdf(t, a, b):\n        \"\"\"\n        Computes the Gamma probability density function in a vectorized manner.\n        g(t; a, b) = t^(a-1) * exp(-t/b) / (b^a * gamma(a)) for t = 0.\n        \"\"\"\n        # Ensure t is a numpy array for vectorized operations\n        t = np.asarray(t, dtype=float)\n        \n        # The function is causal, so response is 0 for t  0.\n        # Initialize response array with zeros.\n        response = np.zeros_like(t)\n\n        # Denominator is constant\n        denom = (b**a) * gamma(a)\n        if denom == 0:\n            return response\n\n        # Calculate for t  0 to avoid potential issues at t=0\n        # (e.g., 0**k where k  0, which is not the case here as a  1)\n        positive_t_mask = t  0\n        t_pos = t[positive_t_mask]\n\n        response[positive_t_mask] = (t_pos**(a - 1) * np.exp(-t_pos / b)) / denom\n        \n        return response\n\n    def hrf(t):\n        \"\"\"\n        Computes the canonical double-gamma Hemodynamic Response Function (HRF).\n        h(t) = g(t; a1, b1) - (1/c) * g(t; a2, b2)\n        \"\"\"\n        g1 = gamma_pdf(t, A1, B1)\n        g2 = gamma_pdf(t, A2, B2)\n        return g1 - (1 / C_RATIO) * g2\n\n    def calculate_bold_response(dt, t_max, W, query_times):\n        \"\"\"\n        Calculates the BOLD response for a given set of parameters.\n        \"\"\"\n        # 1. Create the discretized time vector\n        num_points = int(np.floor(t_max / dt)) + 1\n        t = np.linspace(0.0, (num_points - 1) * dt, num_points)\n\n        # 2. Compute the discrete HRF sequence h[n]\n        h_n = hrf(t)\n\n        # 3. Compute the discrete stimulus sequence s[k] using per-bin averaging\n        bin_starts = t\n        bin_ends = t + dt\n        \n        # Calculate the length of the intersection of each bin [t_k, t_{k+1}) with [0, W)\n        overlap_starts = np.maximum(bin_starts, 0.0)\n        overlap_ends = np.minimum(bin_ends, W)\n        overlap_lengths = np.maximum(0.0, overlap_ends - overlap_starts)\n        \n        s_k = overlap_lengths / dt\n\n        # 4. Compute the discrete BOLD response y[n] via convolution\n        # y[n] = dt * (s * h)[n]\n        y_n_full = np.convolve(s_k, h_n, mode='full')\n        # Truncate the result to the length of the simulation window\n        y_n = dt * y_n_full[:num_points]\n\n        # 5. Calculate the required output quantities\n        \n        # Sampled values at query times\n        query_indices = np.round(np.array(query_times) / dt).astype(int)\n        # Clip indices to prevent out-of-bounds access\n        query_indices = np.minimum(query_indices, num_points - 1)\n        sampled_values = y_n[query_indices].tolist()\n\n        # Peak time and amplitude\n        if y_n.size  0:\n            peak_idx = np.argmax(y_n)\n            y_peak = y_n[peak_idx]\n            t_peak = peak_idx * dt\n        else:\n            t_peak, y_peak = 0.0, 0.0\n            \n        # Area under the curve (Riemann sum)\n        area = np.sum(y_n) * dt\n\n        return [sampled_values, t_peak, y_peak, area]\n\n    # Define the test cases from the problem statement\n    test_cases = [\n        {'dt': 0.1, 't_max': 40.0, 'W': 0.1, 'query_times': [0.0, 6.0, 12.0, 20.0]},\n        {'dt': 0.01, 't_max': 40.0, 'W': 0.1, 'query_times': [6.0]},\n        {'dt': 0.4, 't_max': 40.0, 'W': 0.1, 'query_times': [6.0]},\n        {'dt': 0.1, 't_max': 10.0, 'W': 0.1, 'query_times': [0.0, 6.0, 9.9]},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = calculate_bold_response(\n            case['dt'], case['t_max'], case['W'], case['query_times']\n        )\n        results.append(result)\n\n    # Format the output string exactly as specified, using repr and removing spaces.\n    # This generates a compact, JSON-like list-of-lists string.\n    final_output_str = repr(results).replace(' ', '')\n    print(final_output_str)\n\nsolve()\n```", "id": "4445761"}, {"introduction": "The General Linear Model's power lies in its flexibility to test specific hypotheses about brain function. This exercise moves from signal generation to model specification, focusing on the construction of regressors for parametric modulation [@problem_id:4445759]. You will learn how to model not just whether a region is active, but how its activity changes with a stimulus parameter, and how to use orthogonalization to ensure your model can distinguish between different hypothesized response patterns.", "problem": "A single-condition event-related functional Magnetic Resonance Imaging (fMRI) experiment is modeled using the General Linear Model (GLM) with parametric modulators to test for a monotonic stimulus-response relationship. The condition has events at times $t \\in \\{1,4,6\\}$ seconds on a discrete sampling grid with repetition time $TR = 1\\,\\mathrm{s}$ over $t=0,1,2,\\dots,8\\,\\mathrm{s}$. For each event, two parametric modulators are defined from a strictly monotonic intensity score $I \\in \\{1,2,3\\}$ in event order: a linear modulator $m_{1} = I$ and a quadratic modulator $m_{2} = I^{2}$. Both modulators are mean-centered within the condition before constructing regressors. Regressors are constructed by placing condition-specific sticks at event onsets with amplitudes equal to the centered modulator values and convolving these sticks with a finite impulse response approximation to the hemodynamic response function (HRF), defined on the same discrete grid by $h[0]=0$, $h[1]=1$, $h[2]=\\tfrac{1}{2}$ and $h[\\ell]=0$ for all other integer lags $\\ell$. Convolution is discrete-time, causal, and time-invariant.\n\nUsing only fundamental definitions (discrete-time convolution, inner product, and Gram–Schmidt orthogonalization), perform the following:\n\n- Construct the two parametric modulator regressors $r_{1}$ (from $m_{1}$) and $r_{2}$ (from $m_{2}$) by convolving their respective centered stick functions with the HRF $h$ on the $t=0,1,2,\\dots,8$ grid.\n- To obtain a serially orthogonalized second modulator for inference that is uncorrelated with the first (testing the monotonic linear effect first), define $r_{2}' = r_{2} - \\alpha\\, r_{1}$, where $r_{2}'$ is constrained to be orthogonal to $r_{1}$ in the Euclidean inner product on the design space.\n\nDerive, from first principles, the exact scalar projection coefficient $\\alpha$ that enforces $r_{2}' \\perp r_{1}$. Give your final answer as a single exact real number. No rounding is required and no units are to be reported.", "solution": "The problem requires finding the scalar projection coefficient $\\alpha$ that makes the regressor $r_{2}'$ orthogonal to $r_{1}$, where $r_{2}' = r_{2} - \\alpha r_{1}$. The condition for orthogonality in the Euclidean inner product space is that their inner product (dot product) must be zero: $\\langle r_{2}', r_{1} \\rangle = 0$.\n\nFrom this condition, we derive the formula for $\\alpha$:\n$$\n\\langle r_{2} - \\alpha r_{1}, r_{1} \\rangle = \\langle r_{2}, r_{1} \\rangle - \\alpha \\langle r_{1}, r_{1} \\rangle = 0\n$$\n$$\n\\alpha = \\frac{\\langle r_{2}, r_{1} \\rangle}{\\langle r_{1}, r_{1} \\rangle} = \\frac{r_{2} \\cdot r_{1}}{r_{1} \\cdot r_{1}}\n$$\nTo find $\\alpha$, we follow these steps:\n\n**Step 1: Mean-Center the Parametric Modulators**\nThe event intensities are $I = \\{1, 2, 3\\}$.\n-   **Linear modulator ($m_{1} = I$):** Values are $\\{1, 2, 3\\}$. The mean is $(1+2+3)/3 = 2$. The centered values are $c_{1} = \\{1-2, 2-2, 3-2\\} = \\{-1, 0, 1\\}$.\n-   **Quadratic modulator ($m_{2} = I^2$):** Values are $\\{1^2, 2^2, 3^2\\} = \\{1, 4, 9\\}$. The mean is $(1+4+9)/3 = 14/3$. The centered values are $c_{2} = \\{1-14/3, 4-14/3, 9-14/3\\} = \\{-\\frac{11}{3}, -\\frac{2}{3}, \\frac{13}{3}\\}$.\n\n**Step 2: Construct Stick Functions**\nThe stick functions place the centered modulator values at the event onset times ($t \\in \\{1, 4, 6\\}$) on the grid $t=0, \\dots, 8$.\n-   Stick function for $m_{1}$: $s_{1} = (0, -1, 0, 0, 0, 0, 1, 0, 0)$\n-   Stick function for $m_{2}$: $s_{2} = (0, -\\frac{11}{3}, 0, 0, -\\frac{2}{3}, 0, \\frac{13}{3}, 0, 0)$\n\n**Step 3: Construct Regressors via Convolution**\nThe regressors are the result of convolving the stick functions with the HRF, where $h[1]=1$ and $h[2]=1/2$.\n-   For $r_{1}$, we sum the responses from a stick of height $-1$ at $t=1$ and a stick of height $1$ at $t=6$:\n    -   Response to stick at $t=1$: impulse of $-1$ at $t=2$ and $-1/2$ at $t=3$.\n    -   Response to stick at $t=6$: impulse of $1$ at $t=7$ and $1/2$ at $t=8$.\n    -   Summing these gives the regressor: $r_{1} = \\left(0, 0, -1, -\\frac{1}{2}, 0, 0, 0, 1, \\frac{1}{2}\\right)$\n-   For $r_{2}$, we sum the responses from sticks at $t=1, 4, 6$:\n    -   Response to stick at $t=1$ (height $-11/3$): impulse of $-11/3$ at $t=2$ and $-11/6$ at $t=3$.\n    -   Response to stick at $t=4$ (height $-2/3$): impulse of $-2/3$ at $t=5$ and $-1/3$ at $t=6$.\n    -   Response to stick at $t=6$ (height $13/3$): impulse of $13/3$ at $t=7$ and $13/6$ at $t=8$.\n    -   Summing these gives the regressor: $r_{2} = \\left(0, 0, -\\frac{11}{3}, -\\frac{11}{6}, 0, -\\frac{2}{3}, -\\frac{1}{3}, \\frac{13}{3}, \\frac{13}{6}\\right)$\n\n**Step 4: Calculate Inner Products and Solve for $\\alpha$**\n-   $\\langle r_{1}, r_{1} \\rangle = (-1)^2 + (-\\frac{1}{2})^2 + (1)^2 + (\\frac{1}{2})^2 = 1 + \\frac{1}{4} + 1 + \\frac{1}{4} = \\frac{5}{2}$.\n-   $\\langle r_{2}, r_{1} \\rangle = (-\\frac{11}{3})(-1) + (-\\frac{11}{6})(-\\frac{1}{2}) + (\\frac{13}{3})(1) + (\\frac{13}{6})(\\frac{1}{2}) = \\frac{11}{3} + \\frac{11}{12} + \\frac{13}{3} + \\frac{13}{12} = \\frac{24}{3} + \\frac{24}{12} = 8 + 2 = 10$.\n-   $\\alpha = \\frac{10}{5/2} = 4$.\n\nThe exact scalar projection coefficient is 4.", "answer": "$$\n\\boxed{4}\n$$", "id": "4445759"}, {"introduction": "The final step in any whole-brain analysis is rigorous statistical inference that properly accounts for the thousands of tests performed. This practice introduces a powerful, modern approach: nonparametric permutation testing with control of the Family-Wise Error Rate (FWER) [@problem_id:4445740]. By implementing a sign-flipping procedure and building a null distribution of the maximal statistic, you will gain insight into a robust method for achieving statistical validity that makes fewer assumptions than traditional parametric approaches.", "problem": "You are given paired, within-subject measurements from functional Magnetic Resonance Imaging (fMRI) and Magnetoencephalography (MEG) tasks, abstracted into feature-wise condition differences. For each subject, a feature difference represents the within-subject contrast between two conditions (for example, task and baseline), arranged in a matrix where rows are subjects and columns are features. You must construct a nonparametric permutation test based on sign-flips for the within-subject contrast and compute the distribution of the maximal absolute test statistic across features to obtain Family-Wise Error Rate (FWER) control through the Westfall–Young approach. Then, for each test case, determine how many features are significant under FWER control at a specified significance level.\n\nUse the following foundational base:\n\n- Under the null hypothesis of no condition effect, within-subject condition differences are symmetric around zero, so sign-flipping each subject’s difference is distribution-preserving. Therefore, subject-wise sign-flips are exchangeable under the null. This yields a nonparametric reference distribution built from all sign combinations.\n- For a given feature indexed by $f$, with subject differences $d_{1,f}, d_{2,f}, \\dots, d_{n,f}$, define the sample mean $\\bar{d}_f$ and sample standard deviation $s_f$ by\n  $$\\bar{d}_f = \\frac{1}{n}\\sum_{i=1}^{n} d_{i,f}, \\quad s_f = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}\\left(d_{i,f} - \\bar{d}_f\\right)^2}.$$\n- The one-sample test statistic for a within-subject contrast, for feature $f$, is given by the Student’s statistic,\n  $$t_f = \\frac{\\bar{d}_f}{s_f / \\sqrt{n}},$$\n  which evaluates whether the mean difference deviates from zero, assuming only that the null induces symmetry and exchangeability needed for permutation validity.\n- A nonparametric distribution for multiple comparisons control across $m$ features is obtained by computing, for every sign-flip pattern $p \\in \\{-1,+1\\}^n$, the permuted differences $p_i d_{i,f}$, the corresponding per-feature statistics $t_f^{(p)}$, and the maximal absolute statistic\n  $$T_{\\max}^{(p)} = \\max_{1 \\le f \\le m} \\left|t_f^{(p)}\\right|.$$\n- Let there be $R$ sign-flip permutations. An FWER-critical threshold $c_\\alpha$ at significance level $\\alpha$ is obtained from the empirical distribution of $\\{T_{\\max}^{(p)}\\}_{p=1}^{R}$ by selecting the $(1-\\alpha)$ quantile using an order statistic that controls the family-wise error rate. A feature $f$ is declared significant if $\\left|t_f\\right| \\ge c_\\alpha$.\n\nYour task is to implement the above nonparametric procedure and apply it to the test suite below. For each test case, compute the number of significant features at the specified $\\alpha$, using a two-sided criterion via the maximal absolute statistic. Enumerate all sign-flip permutations exactly; do not subsample.\n\nAnswer format requirements:\n- The program must produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets.\n- Each test case’s result must be an integer equal to the number of significant features for that case.\n\nTest suite:\n- Case $1$: $n=4$, $m=3$, $\\alpha = 0.05$, with subject-wise differences by feature:\n  - Feature $1$: $(0.8, 0.6, 0.7, 0.9)$\n  - Feature $2$: $(0.2, -0.1, 0.0, 0.3)$\n  - Feature $3$: $(1.2, 1.0, 1.1, 0.9)$\n- Case $2$: $n=5$, $m=2$, $\\alpha = 0.05$, with differences:\n  - Feature $1$: $(0.05, -0.02, 0.01, -0.03, 0.00)$\n  - Feature $2$: $(0.4, 0.45, 0.35, 0.5, 0.38)$\n- Case $3$: $n=6$, $m=1$, $\\alpha = 0.05$, with differences:\n  - Feature $1$: $(-0.5, -0.6, -0.4, -0.55, -0.45, -0.5)$\n- Case $4$: $n=3$, $m=4$, $\\alpha = 0.05$, with differences:\n  - Feature $1$: $(1.0, 0.0, -1.0)$\n  - Feature $2$: $(0.5, -0.5, 0.0)$\n  - Feature $3$: $(0.2, -0.2, 0.0)$\n  - Feature $4$: $(0.3, -0.3, 0.0)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3,r_4]$), where each $r_k$ is the integer number of significant features for case $k$ computed using the described nonparametric maximal-statistic FWER procedure.", "solution": "The problem requires the implementation of a nonparametric permutation test to identify significant features from paired within-subject fMRI and MEG data, while controlling the Family-Wise Error Rate (FWER). The provided methodology is based on sign-flipping and the maximal statistic approach, often associated with the Westfall-Young procedure. The problem is scientifically sound, well-posed, and provides all necessary information for a deterministic solution.\n\nThe core principles and algorithmic steps are as follows:\n\nFirst, we establish the theoretical foundation for the permutation test. The problem concerns within-subject contrasts, represented by the differences $d_{i,f}$ for each subject $i \\in \\{1, \\dots, n\\}$ and feature $f \\in \\{1, \\dots, m\\}$. The null hypothesis, $H_0$, posits no true difference between the paired conditions. Under $H_0$, each difference $d_{i,f}$ is assumed to be drawn from a distribution that is symmetric about $0$. A direct consequence of this symmetry is that the sign of each subject's vector of differences, across all features, is arbitrary; that is, the statistical distribution of the data is unchanged by flipping the signs of any subject's measurements. This property is known as sign-flip exchangeability. It allows us to generate a reference distribution by considering all $2^n$ possible combinations of sign flips applied to the subjects' data.\n\nSecond, we define a suitable test statistic to quantify the evidence against the null hypothesis for each feature. The one-sample Student's t-statistic is chosen for this purpose. For each feature $f$, it is calculated as:\n$$t_f = \\frac{\\bar{d}_f}{s_f / \\sqrt{n}}$$\nwhere $\\bar{d}_f$ is the sample mean of the differences for feature $f$ across the $n$ subjects, and $s_f$ is the corresponding sample standard deviation, defined as $s_f = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}\\left(d_{i,f} - \\bar{d}_f\\right)^2}$. This statistic measures the mean difference in units of its standard error, providing a standardized metric of effect size. A large absolute value of $t_f$ suggests that the mean difference is unlikely to be zero.\n\nThird, we address the issue of multiple comparisons. When testing $m$ features simultaneously, the probability of making at least one false positive discovery (a Type I error) inflates. To counteract this, we control the FWER, which is the probability of making one or more Type I errors across all $m$ tests. The specified method achieves this by constructing a null distribution for the *maximal absolute test statistic*. For each of the $R=2^n$ sign-flip permutations, indexed by $p$, we recompute the t-statistic for every feature, denoted $t_f^{(p)}$. Then, for each permutation, we find the maximum absolute value among all features:\n$$T_{\\max}^{(p)} = \\max_{1 \\le f \\le m} \\left|t_f^{(p)}\\right|$$\nThe collection of these $R$ values, $\\{T_{\\max}^{(p)}\\}_{p=1}^{R}$, forms the empirical null distribution. This distribution represents the expected range of the most extreme t-statistic one would observe across all features under the null hypothesis.\n\nFinally, we derive a decision rule. The critical threshold, $c_\\alpha$, for an FWER of $\\alpha$ is determined from the null distribution of the maximal statistic. Specifically, $c_\\alpha$ is the $(1-\\alpha)$ quantile of the sorted distribution of $\\{T_{\\max}^{(p)}\\}$. Let the ordered values be $T_{(1)} \\le T_{(2)} \\le \\dots \\le T_{(R)}$. The critical value $c_\\alpha$ is chosen as the $k$-th order statistic $T_{(k)}$, where $k = \\lceil R(1-\\alpha) \\rceil$. This conservative choice ensures that the probability of the maximal statistic exceeding $c_\\alpha$ under the null is no more than $\\alpha$. A feature $f$ is declared statistically significant if its original, observed absolute t-statistic, $|t_f|$, meets or exceeds this stringent threshold: $|t_f| \\ge c_\\alpha$.\n\nThe algorithmic procedure to be implemented for each test case is:\n1.  For the given data matrix of differences $D$ with $n$ subjects and $m$ features, calculate the observed t-statistics, $t_f$, for each feature $f=1, \\dots, m$.\n2.  Generate all $R=2^n$ unique sign-flip vectors $p \\in \\{-1, +1\\}^n$.\n3.  Initialize an empty list to store the distribution of the maximal statistic.\n4.  For each sign-flip vector $p$:\n    a. Apply the sign flips to the data matrix $D$ to obtain a permuted data matrix $D^{(p)}$, where $D_{i,f}^{(p)} = p_i d_{i,f}$.\n    b. Compute the t-statistic $t_f^{(p)}$ for each feature based on $D^{(p)}$. Special care is taken for cases where the standard deviation is zero, in which case the t-statistic is defined as $0$.\n    c. Determine the maximal absolute statistic for this permutation, $T_{\\max}^{(p)} = \\max_f |t_f^{(p)}|$, and add it to the list.\n5.  Sort the list of $R$ maximal statistics in ascending order.\n6.  Calculate the critical value $c_\\alpha$ as the value at index $\\lceil R(1-\\alpha)\\rceil - 1$ in the sorted, $0$-indexed list.\n7.  Compare the absolute value of each originally observed t-statistic, $|t_f|$, against $c_\\alpha$.\n8.  The final result for the test case is the count of features for which $|t_f| \\ge c_\\alpha$.\n\nThis complete, exact permutation procedure is applied to each of the $4$ test cases specified in the problem statement.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport itertools\n\ndef calculate_significant_features(D, alpha):\n    \"\"\"\n    Performs a nonparametric permutation test with FWER control.\n\n    Args:\n        D (np.ndarray): A 2D numpy array of shape (n_subjects, n_features)\n                        containing the within-subject differences.\n        alpha (float): The desired significance level for FWER control.\n\n    Returns:\n        int: The number of features significant at the specified alpha level.\n    \"\"\"\n    D = np.asarray(D)\n    n, m = D.shape\n    \n    # Define a helper function to compute t-statistics for a data matrix\n    def compute_t_stats(data):\n        n_subjects = data.shape[0]\n        if n_subjects  2:\n            return np.zeros(data.shape[1])\n            \n        means = np.mean(data, axis=0)\n        stds = np.std(data, axis=0, ddof=1)\n        \n        # Denominator of the t-statistic\n        sems = stds / np.sqrt(n_subjects)\n        \n        # Handle division by zero for features with zero variance\n        t_stats = np.divide(means, sems, out=np.zeros_like(means), where=sems != 0)\n        \n        return t_stats\n\n    # 1. Calculate observed t-statistics\n    t_observed = compute_t_stats(D)\n    abs_t_observed = np.abs(t_observed)\n\n    # 2. Generate all 2^n sign-flip permutations\n    R = 2**n\n    sign_flips = list(itertools.product([-1, 1], repeat=n))\n\n    # 3. Build the null distribution of the maximal absolute t-statistic\n    max_t_distribution = []\n    for signs in sign_flips:\n        signs_array = np.array(signs).reshape(-1, 1)\n        permuted_data = D * signs_array\n        \n        t_permuted = compute_t_stats(permuted_data)\n        \n        max_abs_t = np.max(np.abs(t_permuted))\n        max_t_distribution.append(max_abs_t)\n\n    # 4. Sort the distribution and find the critical value c_alpha\n    max_t_distribution.sort()\n    \n    # The critical value is the (1-alpha) quantile of the permutation distribution.\n    # We use ceiling to be conservative, ensuring P(T_max = c_alpha) = alpha.\n    quantile_index = int(np.ceil((1 - alpha) * R)) - 1\n    \n    # Ensure index is within bounds\n    if quantile_index  0:\n        quantile_index = 0\n    if quantile_index = R:\n        quantile_index = R-1\n        \n    c_alpha = max_t_distribution[quantile_index]\n\n    # 5. Count how many observed t-statistics exceed the critical threshold\n    significant_count = np.sum(abs_t_observed = c_alpha)\n    \n    return int(significant_count)\n\ndef solve():\n    \"\"\"\n    Solves the problem by running the permutation test on all provided test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"data\": [\n                [0.8, 0.2, 1.2],\n                [0.6, -0.1, 1.0],\n                [0.7, 0.0, 1.1],\n                [0.9, 0.3, 0.9]\n            ],\n            \"alpha\": 0.05\n        },\n        {\n            \"data\": [\n                [0.05, 0.4],\n                [-0.02, 0.45],\n                [0.01, 0.35],\n                [-0.03, 0.5],\n                [0.00, 0.38]\n            ],\n            \"alpha\": 0.05\n        },\n        {\n            \"data\": [\n                [-0.5],\n                [-0.6],\n                [-0.4],\n                [-0.55],\n                [-0.45],\n                [-0.5]\n            ],\n            \"alpha\": 0.05\n        },\n        {\n            \"data\": [\n                [1.0, 0.5, 0.2, 0.3],\n                [0.0, -0.5, -0.2, -0.3],\n                [-1.0, 0.0, 0.0, 0.0]\n            ],\n            \"alpha\": 0.05\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        D = np.array(case[\"data\"])\n        alpha = case[\"alpha\"]\n        result = calculate_significant_features(D, alpha)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4445740"}]}