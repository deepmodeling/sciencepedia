{"hands_on_practices": [{"introduction": "Before any analysis can begin, neuroimaging data from different sources must be brought into a common spatial framework. This foundational practice dives into the affine transformation matrix, a key component of Neuroimaging Informatics Technology Initiative (NIfTI) headers that encodes an image's orientation and position in physical space. By implementing a canonicalization procedure from first principles, you will gain a crucial understanding of how to standardize diverse data orientations, a mandatory step for building robust and reproducible analysis pipelines [@problem_id:4491629].", "problem": "You are given a small set of synthetic neuroimaging header affines intended to model realistic Neuroimaging Informatics Technology Initiative (NIfTI) image orientations under differing axis orders and flips. In NIfTI, voxel coordinates $\\mathbf{i} = (i_x, i_y, i_z)$ are mapped to scanner/world coordinates $\\mathbf{x} = (x, y, z)$, targeted to Right-Anterior-Superior (RAS) conventions, via a $4\\times 4$ affine $A\\in\\mathbb{R}^{4\\times 4}$ using homogeneous coordinates. The upper left $3\\times 3$ block $R\\in\\mathbb{R}^{3\\times 3}$ encodes rotation, scaling, and possible shear, and the last column encodes translation. The world axes are defined by: $+x$ increases toward Right, $+y$ increases toward Anterior, and $+z$ increases toward Superior. Physical lengths are in millimeters (mm).\n\nStarting from first principles of linear mappings, determine a canonical orientation that reorders and flips voxel axes so that, after transformation, the columns of the new $3\\times 3$ block $R_{\\mathrm{can}}$ align with the positive world axes $(+x,+y,+z)$ in order, within a specified tolerance. This entails:\n- Inspecting the columns of $R$; the $j$-th column of $R$ describes how voxel axis $j$ contributes to world coordinates.\n- For each voxel axis $j\\in\\{0,1,2\\}$, identify the world axis index $k\\in\\{0,1,2\\}$ with largest absolute contribution, i.e., the index $k$ maximizing $\\lvert R_{k,j}\\rvert$, and the sign $\\operatorname{sign}(R_{k,j})\\in\\{-1,+1\\}$ indicating directionality (positive for alignment to $+x,+y,+z$, negative for alignment to $-x,-y,-z$).\n- Construct a canonicalization transform $S\\in\\mathbb{R}^{4\\times 4}$ whose top-left $3\\times 3$ part permutes and flips voxel axes so that $R_{\\mathrm{can}} = R S_{3\\times 3}$ has columns that correspond to $(+x,+y,+z)$ respectively. Use $A_{\\mathrm{can}} = A S$ for the full header transformation; translation terms need not be adjusted for lack of image dimension information, and alignment verification should consider only $R_{\\mathrm{can}}$.\n\nVerification rule: After canonicalization, for each column $i\\in\\{0,1,2\\}$ of $R_{\\mathrm{can}}$,\n- The largest-magnitude component must be at row $i$ (i.e., the world axis matches), and\n- The diagonal entry $R_{\\mathrm{can}}[i,i]$ must be strictly positive,\n- The maximum off-diagonal magnitude over all columns, normalized by the maximum diagonal magnitude, must not exceed a tolerance $\\tau$ (set $\\tau=0.25$).\n\nIf the initial mapping from voxel axes to world axes is not a bijection (e.g., duplicate assignments or zero-length columns), treat the case as invalid mapping and report the verification as false, but still compute a best-effort canonicalization by greedily assigning each world axis $i$ to the unassigned voxel axis $j$ that maximizes $\\lvert R_{i,j}\\rvert$, flipping by the sign of $R_{i,j}$, to produce a deterministic output.\n\nImplement a program in Python that, for each test case affine, computes:\n- The permutation list $\\mathrm{perm} = [p_0,p_1,p_2]$, where $p_i$ is the original voxel axis index assigned to world axis $i$ after canonicalization (i.e., column $i$ of $R_{\\mathrm{can}}$ originates from column $p_i$ of $R$).\n- The flip list $\\mathrm{flips} = [f_0,f_1,f_2]$ with $f_i\\in\\{0,1\\}$, where $f_i=1$ indicates that axis $i$ must be reversed to align with the positive world axis.\n- The boolean $\\mathrm{ok}$ indicating whether verification succeeds as per the rule.\n- The float $\\mathrm{max\\_offdiag}$ equal to the maximum absolute off-diagonal element in $R_{\\mathrm{can}}$, divided by the maximum diagonal magnitude across all three columns.\n\nTest suite (all entries in mm units; angles implicit by the linear components of $R$):\n1. Happy path (already RAS, unit voxel sizes):\n   $$A_0 = \\begin{bmatrix}\n   1 & 0 & 0 & 0 \\\\\n   0 & 1 & 0 & 0 \\\\\n   0 & 0 & 1 & 0 \\\\\n   0 & 0 & 0 & 1\n   \\end{bmatrix}.$$\n2. Axis swap with one flip and non-uniform voxel sizes (translation included, but verification uses only $R$):\n   $$A_1 = \\begin{bmatrix}\n   0 & 1 & 0 & 10 \\\\\n   -2 & 0 & 0 & -5 \\\\\n   0 & 0 & 3 & 2 \\\\\n   0 & 0 & 0 & 1\n   \\end{bmatrix}.$$\n3. Small shear with one axis reversed:\n   $$A_2 = \\begin{bmatrix}\n   1.0 & 0.0 & 0.01 & 0 \\\\\n   0.05 & 1.0 & 0.0 & 0 \\\\\n   0.0 & 0.02 & -1.0 & 0 \\\\\n   0 & 0 & 0 & 1\n   \\end{bmatrix}.$$\n4. Degenerate case with duplicate dominant world axis assignment (invalid mapping):\n   $$A_3 = \\begin{bmatrix}\n   1.0 & 0.8 & 0.0 & 3 \\\\\n   0.0 & 0.1 & 0.0 & 4 \\\\\n   0.0 & 0.0 & 1.0 & 5 \\\\\n   0 & 0 & 0 & 1\n   \\end{bmatrix}.$$\n\nAlgorithmic requirements:\n- Only linear algebra on $R$ should be used; do not rely on external neuroimaging libraries.\n- Use a small numerical threshold $\\varepsilon = 10^{-8}$ to detect zero-length columns and for positivity checks.\n- Use the tolerance $\\tau = 0.25$ in the verification rule.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each result is itself a list of the form $[\\mathrm{perm},\\mathrm{flips},\\mathrm{ok},\\mathrm{max\\_offdiag}]$, where $\\mathrm{perm}$ and $\\mathrm{flips}$ are lists of integers, $\\mathrm{ok}$ is a boolean, and $\\mathrm{max\\_offdiag}$ is a float. For example, the output must look like:\n$$[\\,[ [p_0,p_1,p_2],[f_0,f_1,f_2],\\mathrm{ok},m_0 ],\\,\\ldots\\,]$$\nwith one such sublist per test case, in the same order as the test suite.", "solution": "We begin from the definition of an affine transformation in Neuroimaging Informatics Technology Initiative (NIfTI). Voxel coordinates $\\mathbf{i} = (i_x, i_y, i_z)$ are mapped to world coordinates $\\mathbf{x} = (x, y, z)$ using homogeneous coordinates:\n$$\n\\begin{bmatrix}\nx \\\\ y \\\\ z \\\\ 1\n\\end{bmatrix}\n=\nA\n\\begin{bmatrix}\ni_x \\\\ i_y \\\\ i_z \\\\ 1\n\\end{bmatrix},\n\\quad\nA =\n\\begin{bmatrix}\nR & \\mathbf{t} \\\\\n\\mathbf{0}^\\top & 1\n\\end{bmatrix},\n$$\nwhere $R\\in\\mathbb{R}^{3\\times 3}$ encodes rotation, scaling, and shear, and $\\mathbf{t}\\in\\mathbb{R}^3$ encodes translation. The Right-Anterior-Superior (RAS) convention defines the positive world axes as $+x$ for Right, $+y$ for Anterior, and $+z$ for Superior.\n\nFrom linear algebra, the $j$-th column of $R$ gives the contribution of unit steps along the $j$-th voxel axis to the world coordinate vector. To determine the dominant world axis for voxel axis $j$, we find the index $k$ that maximizes $\\lvert R_{k,j}\\rvert$. The sign of $R_{k,j}$ indicates whether voxel axis $j$ is aligned with the positive direction of world axis $k$ (sign $+1$) or the negative direction (sign $-1$). This procedure, performed for each $j\\in\\{0,1,2\\}$, yields a mapping from voxel axes to world axes, together with the directionality (flip if negative).\n\nA consistent canonical orientation aims to reorder and flip voxel axes so that, in the transformed affine $A_{\\mathrm{can}} = A S$, the top-left $3\\times 3$ block $R_{\\mathrm{can}}$ has its columns aligned with $(+x,+y,+z)$ respectively. The transform $S\\in\\mathbb{R}^{4\\times 4}$ has the form:\n$$\nS =\n\\begin{bmatrix}\nS_{3\\times 3} & \\mathbf{0} \\\\\n\\mathbf{0}^\\top & 1\n\\end{bmatrix},\n$$\nwhere $S_{3\\times 3}$ is a column-selector and sign-flip matrix. Specifically, the $i$-th column of $S_{3\\times 3}$ is $\\pm \\mathbf{e}_{p_i}$, where $\\mathbf{e}_{p_i}$ is the $p_i$-th standard basis vector (selecting the original voxel axis $p_i$), and the sign is positive if $R_{i,p_i} > 0$ and negative otherwise. This construction ensures:\n$$\nR_{\\mathrm{can}} = R S_{3\\times 3}, \\quad \\text{and} \\quad R_{\\mathrm{can}}[:,i] = \\operatorname{sign}(R_{i,p_i})\\, R[:,p_i].\n$$\nThus $R_{\\mathrm{can}}$ has columns whose largest-magnitude components lie on the corresponding diagonal entries and are positive.\n\nThe verification conditions derive from geometric alignment requirements:\n- For each column $i$, the largest-magnitude component is at row $i$ (i.e., $\\arg\\max_{k\\in\\{0,1,2\\}} \\lvert R_{\\mathrm{can}}[k,i]\\rvert = i$). This ensures proper axis matching.\n- The diagonal entry $R_{\\mathrm{can}}[i,i]$ is strictly positive, ensuring alignment with the positive world axes.\n- The shear or off-axis contamination must be bounded; we measure the maximum off-diagonal magnitude across all columns and normalize by the maximum diagonal magnitude (a scale-invariant measure). If this normalized maximum is at most a tolerance $\\tau$, the alignment is considered acceptable for canonicalization. We set $\\tau = 0.25$ to allow small shears typical of resampling or minor header inconsistencies.\n\nIf the initial mapping from voxel axes to world axes is not bijective (e.g., two voxel axes both dominate the same world axis or a zero-length column indicating no contribution), the canonicalization is not reliable. We still produce a deterministic best-effort $S$ by greedily assigning, for each world axis $i$, the unassigned voxel axis $j$ that maximizes $\\lvert R_{i,j}\\rvert$ and flipping according to $\\operatorname{sign}(R_{i,j})$. The verification flag $\\mathrm{ok}$ is set to false in such cases.\n\nWe now apply the procedure to the test suite:\n\n1. Case $A_0$:\n   $R = I_3$. For $j=0,1,2$, the dominant world indices are $0,1,2$ respectively, all with positive signs. Hence $p = [0,1,2]$, flips $= [0,0,0]$, and $S_{3\\times 3} = I_3$. Then $R_{\\mathrm{can}} = I_3$, the off-diagonal elements are zero, so $\\mathrm{max\\_offdiag} = 0$, and $\\mathrm{ok} = \\text{True}$.\n\n2. Case $A_1$:\n   $R$ has columns $R[:,0] = [0,-2,0]^\\top$, $R[:,1] = [1,0,0]^\\top$, $R[:,2] = [0,0,3]^\\top$. Dominant axes: voxel $0\\to y$ with negative sign, voxel $1\\to x$ with positive sign, voxel $2\\to z$ with positive sign. The permutation aligning to $(x,y,z)$ is $p = [1,0,2]$ with flips $= [0,1,0]$. Constructing $S_{3\\times 3}$ with these columns yields $R_{\\mathrm{can}}$ with columns $[1,0,0]^\\top$, $[0,2,0]^\\top$, $[0,0,3]^\\top$, which satisfies the conditions; off-diagonal elements are zero, so $\\mathrm{max\\_offdiag}=0$, and $\\mathrm{ok} = \\text{True}$.\n\n3. Case $A_2$:\n   $R$ has columns $[1.0, 0.05, 0.0]^\\top$, $[0.0, 1.0, 0.02]^\\top$, $[0.01, 0.0, -1.0]^\\top$. Dominant axes: voxel $0\\to x$ positive, voxel $1\\to y$ positive, voxel $2\\to z$ negative. Thus $p = [0,1,2]$ and flips $= [0,0,1]$. Flipping the third column yields $R_{\\mathrm{can}}$ columns $[1.0, 0.05, 0.0]^\\top$, $[0.0, 1.0, 0.02]^\\top$, and $[-0.01, 0.0, 1.0]^\\top$. The largest components lie on the diagonal with positive values, and the maximum off-diagonal magnitude is $0.05$; with maximum diagonal magnitude $1.0$, the normalized $\\mathrm{max\\_offdiag}=0.05\\le \\tau$, so $\\mathrm{ok}=\\text{True}$.\n\n4. Case $A_3$:\n   $R$ has columns $[1.0, 0.0, 0.0]^\\top$, $[0.8, 0.1, 0.0]^\\top$, and $[0.0, 0.0, 1.0]^\\top$. Two voxel axes dominantly map to $+x$ (duplicate), yielding a non-bijective mapping; this is invalid. A greedy assignment to produce deterministic output gives $p = [0,1,2]$ with flips $= [0,0,0]$. Then $R_{\\mathrm{can}}$ columns are $[1.0, 0.0, 0.0]^\\top$, $[0.8, 0.1, 0.0]^\\top$, $[0.0, 0.0, 1.0]^\\top$. Column $1$ fails axis matching (its largest component is in row $0$ instead of row $1$), and the normalized maximum off-diagonal magnitude is $0.8/1.0 = 0.8 > \\tau$. Therefore $\\mathrm{ok}=\\text{False}$.\n\nThe program implements these steps, enforces a small threshold $\\varepsilon = 10^{-8}$ for detection of zero-length columns and positivity, uses $\\tau = 0.25$ for shear tolerance, and produces, for each test case, the list $[\\mathrm{perm},\\mathrm{flips},\\mathrm{ok},\\mathrm{max\\_offdiag}]$. The final output aggregates these lists in order as a single line, comma-separated inside square brackets.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef infer_mapping(R, eps=1e-8):\n    \"\"\"\n    Infer mapping from voxel axes (columns) to world axes (rows) based on the\n    dominant absolute contribution in each column.\n    Returns:\n        world_idx_by_voxel: list of length 3, each entry in {0,1,2}\n        sign_by_voxel: list of length 3, each entry in {+1,-1}\n        valid: boolean, True if mapping is a bijection and columns non-degenerate\n    \"\"\"\n    world_idx_by_voxel = []\n    sign_by_voxel = []\n    valid = True\n    # Check for zero-length columns\n    for j in range(3):\n        col = R[:, j]\n        norm = np.linalg.norm(col)\n        if norm <= eps:\n            valid = False\n        k = int(np.argmax(np.abs(col)))\n        s = 1 if col[k] >= 0 else -1\n        world_idx_by_voxel.append(k)\n        sign_by_voxel.append(s)\n    # Check bijection (unique assignment of world axes)\n    counts = [world_idx_by_voxel.count(i) for i in range(3)]\n    if any(c == 0 for c in counts) or any(c > 1 for c in counts):\n        valid = False\n    return world_idx_by_voxel, sign_by_voxel, valid\n\ndef greedy_assign(R, eps=1e-8):\n    \"\"\"\n    Greedily assign world axes i to voxel axes j when mapping is invalid.\n    For each world axis i=0..2, choose the unassigned voxel axis j maximizing |R[i,j]|.\n    Returns:\n        perm: list p_i = selected voxel axis index for world axis i\n        flips: list f_i = 0 if R[i,p_i] >= 0 else 1\n    \"\"\"\n    assigned = set()\n    perm = [None, None, None]\n    flips = [0, 0, 0]\n    for i in range(3):\n        best_j = None\n        best_val = -np.inf\n        for j in range(3):\n            if j in assigned:\n                continue\n            val = abs(R[i, j])\n            if val > best_val + eps:  # strict preference\n                best_val = val\n                best_j = j\n        if best_j is None:\n            # fallback: pick any unassigned\n            for j in range(3):\n                if j not in assigned:\n                    best_j = j\n                    break\n        assigned.add(best_j)\n        perm[i] = best_j\n        flips[i] = 0 if R[i, best_j] >= 0 else 1\n    return perm, flips\n\ndef build_S_from_perm_flips(perm, flips):\n    \"\"\"\n    Build the 4x4 canonicalization transform S given perm and flips.\n    The top-left 3x3 columns select original voxel axes perm[i] and apply sign (flip).\n    \"\"\"\n    S = np.zeros((4, 4), dtype=float)\n    # Construct S_{3x3} by columns\n    for i in range(3):\n        j = perm[i]\n        s = -1.0 if flips[i] == 1 else 1.0\n        S[j, i] = s  # place s at row j, column i\n    S[3, 3] = 1.0\n    return S\n\ndef canonicalize_affine(A, eps=1e-8, tau=0.25):\n    \"\"\"\n    Given affine A, compute canonicalization transform and verification.\n    Returns:\n        perm: list of original voxel axis indices used for world axes [x,y,z]\n        flips: list of 0/1 indicating axis reversal\n        ok: boolean indicating verification success\n        max_offdiag: float, normalized maximum off-diagonal magnitude\n    \"\"\"\n    R = A[:3, :3].astype(float)\n    world_idx_by_voxel, sign_by_voxel, valid_mapping = infer_mapping(R, eps=eps)\n\n    # Build perm and flips\n    if valid_mapping:\n        # Construct perm: for each world axis i, find voxel j with world_idx[j] == i\n        perm = [None, None, None]\n        flips = [0, 0, 0]\n        for i in range(3):\n            # find j where world_idx_by_voxel[j] == i\n            candidates = [j for j in range(3) if world_idx_by_voxel[j] == i]\n            if len(candidates) == 1:\n                j = candidates[0]\n            else:\n                # Should not happen when valid_mapping is True, but handle gracefully\n                j = candidates[0] if candidates else i\n            perm[i] = j\n            flips[i] = 0 if sign_by_voxel[j] == 1 else 1\n    else:\n        # Greedy assignment\n        perm, flips = greedy_assign(R, eps=eps)\n\n    # Build S and canonicalize\n    S = build_S_from_perm_flips(perm, flips)\n    A_can = A @ S\n    R_can = A_can[:3, :3]\n\n    # Verification\n    # 1) Largest magnitude component at matching row i\n    axis_match = True\n    for i in range(3):\n        col = R_can[:, i]\n        if np.linalg.norm(col) <= eps:\n            axis_match = False\n            break\n        k = int(np.argmax(np.abs(col)))\n        if k != i:\n            axis_match = False\n            break\n        # 2) Positive diagonal\n        if R_can[i, i] <= eps:\n            axis_match = False\n            break\n\n    # 3) Off-diagonal tolerance\n    diag_mags = []\n    offdiag_max = 0.0\n    for i in range(3):\n        col = R_can[:, i]\n        diag_mags.append(abs(col[i]))\n        off = np.max(np.abs(np.delete(col, i)))  # max off-diagonal in this column\n        if off > offdiag_max:\n            offdiag_max = off\n    max_diag_mag = max(diag_mags) if diag_mags else 0.0\n    if max_diag_mag > eps:\n        max_offdiag = offdiag_max / max_diag_mag\n    else:\n        # If all diagonal magnitudes are near zero, treat as degenerate\n        max_offdiag = float('inf')\n        axis_match = False\n\n    ok = bool(axis_match and (max_offdiag <= tau) and valid_mapping)\n\n    return perm, flips, ok, float(max_offdiag)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    A0 = np.array([\n        [1, 0, 0, 0],\n        [0, 1, 0, 0],\n        [0, 0, 1, 0],\n        [0, 0, 0, 1]\n    ], dtype=float)\n\n    A1 = np.array([\n        [0, 1, 0, 10],\n        [-2, 0, 0, -5],\n        [0, 0, 3, 2],\n        [0, 0, 0, 1]\n    ], dtype=float)\n\n    A2 = np.array([\n        [1.0, 0.0, 0.01, 0.0],\n        [0.05, 1.0, 0.0, 0.0],\n        [0.0, 0.02, -1.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0]\n    ], dtype=float)\n\n    A3 = np.array([\n        [1.0, 0.8, 0.0, 3.0],\n        [0.0, 0.1, 0.0, 4.0],\n        [0.0, 0.0, 1.0, 5.0],\n        [0.0, 0.0, 0.0, 1.0]\n    ], dtype=float)\n\n    test_cases = [A0, A1, A2, A3]\n\n    results = []\n    for A in test_cases:\n        perm, flips, ok, max_offdiag = canonicalize_affine(A, eps=1e-8, tau=0.25)\n        # Assemble result as required: [perm, flips, ok, max_offdiag]\n        results.append([perm, flips, ok, max_offdiag])\n\n    # Final print statement in the exact required format.\n    # Single line containing the results as a comma-separated list enclosed in square brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4491629"}, {"introduction": "A pivotal design choice when applying deep learning to volumetric neuroimages is whether to use 2D slice-based models or more complex 3D patch-based models. This decision carries fundamental trade-offs between computational feasibility, such as memory consumption, and the model's ability to learn from rich three-dimensional spatial context. This exercise guides you through a quantitative comparison of these two paradigms, challenging you to model their resource requirements and their potential for clinical discrimination, thereby providing a solid framework for making informed architectural choices [@problem_id:4491613].", "problem": "You are asked to design and implement a program that compares two convolutional neural network (CNN) input paradigms for volumetric neuroimaging: a two-dimensional slice-based model versus a three-dimensional patch-based model. Your task is to quantify three aspects for each paradigm: memory consumption per training batch, physical context captured by the input, and a predicted clinical discriminative performance proxy under a simple generative detection model. The program must compute results for multiple provided parameter sets and output them in a single, precisely specified format.\n\nStart your derivation from the following foundational bases and core definitions, which you must use to construct your solution without invoking any problem-specific shortcut formulas:\n\n- A floating-point tensor with shape $n_1 \\times n_2 \\times \\cdots \\times n_k$ and scalar type occupying $b$ bytes per element consumes exactly $n_1 n_2 \\cdots n_k \\cdot b$ bytes of memory. Training memory is dominated by activations, and a constant multiplicative overhead factor can approximate additional storage required for gradients and optimizer state.\n- A convolutional neural network (CNN) reduces spatial dimensions by a factor of $2$ after each pooling stage. For a network with $L$ stages, the feature map spatial dimensions at stage $l \\in \\{0,1,\\dots,L-1\\}$ are approximately the input dimensions divided by $2^l$, with flooring to preserve integer grid indexing. It is standard to double the number of channels at each deeper stage in encoder-like feature pyramids.\n- Physical field of view for an input tensor derived from a voxel grid with spacings $s_x, s_y, s_z$ (in millimeters) equals the product of the voxel counts in each dimension and their spacings, giving a physical length along each axis. The three-dimensional physical field-of-view volume is the product of the three physical lengths.\n- Under a Gaussian equal-variance detection model in signal detection theory, aggregating $N_{\\text{eff}}$ independent observations, each contributing a signal with mean difference $s$ and noise standard deviation $\\sigma$, yields discriminability that scales with the square root of the number of effectively independent observations. Voxelwise spatial correlation can be summarized through an effective correlation volume. The Area Under the Receiver Operating Characteristic Curve (AUC) for the resulting Gaussian model can be derived from the discriminability index.\n\nYou must implement the following precise modeling assumptions:\n\n1) Architecture and memory model\n- Consider an encoder-only feature pyramid with $L$ stages, indexed by $l \\in \\{0,\\dots,L-1\\}$. At stage $l$, the channel count is $C_0 \\cdot 2^l$, where $C_0$ is a base number of channels.\n- The spatial resolution at stage $l$ is given by $\\left(\\max\\{1,\\left\\lfloor \\frac{n_x}{2^l} \\right\\rfloor\\}, \\max\\{1,\\left\\lfloor \\frac{n_y}{2^l} \\right\\rfloor\\}, \\max\\{1,\\left\\lfloor \\frac{n_z}{2^l} \\right\\rfloor\\}\\right)$, where $(n_x,n_y,n_z)$ are the input voxel counts for the sample presented to the CNN. For a two-dimensional slice model, use $n_z = 1$ by construction.\n- The total activation element count per sample is the sum over stages $l$ of the product of the spatial dimensions at stage $l$ and the channel count at stage $l$.\n- The per-batch activation memory in bytes equals $\\gamma \\cdot B \\cdot E \\cdot b_f$, where $\\gamma$ is a constant overhead multiplier, $B$ is the batch size, $E$ is the per-sample activation element count summed over stages, and $b_f$ is the number of bytes per floating-point number. Express memory in megabytes (MB), where $1 \\text{ MB} = 1024^2$ bytes.\n\n2) Context capture\n- Let the voxel spacings be $(s_x,s_y,s_z)$ in millimeters. For a two-dimensional slice input with $(n_x,n_y)$ voxels, treat thickness as exactly one slice, i.e., physical thickness $s_z$. The physical field-of-view (FOV) volume of the slice input is $V_{\\text{2D}} = (n_x s_x) \\cdot (n_y s_y) \\cdot (1 \\cdot s_z)$ in cubic millimeters.\n- For a three-dimensional patch with $(n_x,n_y,n_z)$ voxels, the physical FOV volume is $V_{\\text{3D}} = (n_x s_x) \\cdot (n_y s_y) \\cdot (n_z s_z)$ in cubic millimeters.\n- Report the dimensionless context ratio $R_V = \\frac{V_{\\text{3D}}}{V_{\\text{2D}}}$.\n\n3) Predicted clinical performance proxy\n- Assume a binary lesion detection task under a Gaussian equal-variance model. Suppose each voxel contributes an additive signal difference of $s$ between classes and independent noise with standard deviation $\\sigma$. Spatial correlation is summarized by an effective correlation volume (in voxels) $v_c = \\max\\{1, \\lambda_x \\lambda_y \\lambda_z\\}$, where $(\\lambda_x,\\lambda_y,\\lambda_z)$ are correlation extents measured in voxels along each axis.\n- The effective number of independent observations for an input with $n_x n_y n_z$ voxels is $N_{\\text{eff}} = \\max\\left\\{1, \\frac{n_x n_y n_z}{v_c}\\right\\}$.\n- Using signal detection theory for Gaussian equal-variance models, derive the discriminability index in terms of $N_{\\text{eff}}$, $s$, and $\\sigma$, and then derive the AUC in terms of the discriminability index. Report AUC as a decimal in $[0,1]$ without a percentage sign.\n\nYour program must compute, for each test case:\n- Two-dimensional batch memory in MB.\n- Three-dimensional batch memory in MB.\n- Context ratio $R_V$ (dimensionless).\n- AUC for the two-dimensional model.\n- AUC for the three-dimensional model.\n\nThe architectural configuration and data parameters for each test case are given below. All integers and real numbers are exact, and you must follow the specifications literally.\n\nConstants common to all cases:\n- Floating-point storage size: $b_f = 4$ bytes.\n\nTest suite:\n- Case A (happy path, isotropic spacings):\n  - Two-dimensional slice input size $(n_x,n_y) = (256, 256)$.\n  - Three-dimensional patch size $(n_x,n_y,n_z) = (64, 64, 16)$.\n  - Voxel spacings $(s_x,s_y,s_z) = (1.0, 1.0, 1.0)$ millimeters.\n  - Network stages $L = 3$, base channels $C_0 = 16$, overhead $\\gamma = 3.0$.\n  - Batch sizes: two-dimensional $B_{\\text{2D}} = 8$, three-dimensional $B_{\\text{3D}} = 2$.\n  - Detection model: per-voxel signal $s = 0.5$, noise standard deviation $\\sigma = 1.0$, correlation extents $(\\lambda_x,\\lambda_y,\\lambda_z) = (3, 3, 3)$ voxels.\n\n- Case B (anisotropic spacing, shallow depth patch):\n  - Two-dimensional slice input size $(n_x,n_y) = (512, 512)$.\n  - Three-dimensional patch size $(n_x,n_y,n_z) = (128, 128, 4)$.\n  - Voxel spacings $(s_x,s_y,s_z) = (0.8, 0.8, 3.0)$ millimeters.\n  - Network stages $L = 4$, base channels $C_0 = 8$, overhead $\\gamma = 2.5$.\n  - Batch sizes: two-dimensional $B_{\\text{2D}} = 4$, three-dimensional $B_{\\text{3D}} = 1$.\n  - Detection model: per-voxel signal $s = 0.3$, noise standard deviation $\\sigma = 1.2$, correlation extents $(\\lambda_x,\\lambda_y,\\lambda_z) = (5, 5, 1)$ voxels.\n\n- Case C (boundary case, three-dimensional depth equals one):\n  - Two-dimensional slice input size $(n_x,n_y) = (64, 64)$.\n  - Three-dimensional patch size $(n_x,n_y,n_z) = (64, 64, 1)$.\n  - Voxel spacings $(s_x,s_y,s_z) = (1.0, 1.0, 1.0)$ millimeters.\n  - Network stages $L = 2$, base channels $C_0 = 16$, overhead $\\gamma = 3.0$.\n  - Batch sizes: two-dimensional $B_{\\text{2D}} = 8$, three-dimensional $B_{\\text{3D}} = 8$.\n  - Detection model: per-voxel signal $s = 0.6$, noise standard deviation $\\sigma = 0.9$, correlation extents $(\\lambda_x,\\lambda_y,\\lambda_z) = (2, 2, 2)$ voxels.\n\n- Case D (edge case, high base channels, anisotropic spacing):\n  - Two-dimensional slice input size $(n_x,n_y) = (128, 128)$.\n  - Three-dimensional patch size $(n_x,n_y,n_z) = (96, 96, 24)$.\n  - Voxel spacings $(s_x,s_y,s_z) = (0.5, 0.5, 2.0)$ millimeters.\n  - Network stages $L = 2$, base channels $C_0 = 32$, overhead $\\gamma = 3.5$.\n  - Batch sizes: two-dimensional $B_{\\text{2D}} = 16$, three-dimensional $B_{\\text{3D}} = 1$.\n  - Detection model: per-voxel signal $s = 0.8$, noise standard deviation $\\sigma = 0.8$, correlation extents $(\\lambda_x,\\lambda_y,\\lambda_z) = (2, 2, 2)$ voxels.\n\nImplementation details:\n- For the two-dimensional model, treat the depth as $n_z = 1$ during all computations.\n- For each stage $l$, compute spatial dimensions by integer flooring with a minimum of $1$ voxel per dimension: for dimension $d \\in \\{x,y,z\\}$, $n_d^{(l)} = \\max\\{1,\\left\\lfloor \\frac{n_d}{2^l} \\right\\rfloor\\}$.\n- For each stage $l$, the channel count is $C^{(l)} = C_0 \\cdot 2^l$.\n- The per-sample activation element count is $E = \\sum_{l=0}^{L-1} \\left(n_x^{(l)} n_y^{(l)} n_z^{(l)} C^{(l)}\\right)$.\n- Per-batch memory in megabytes is $M_{\\text{MB}} = \\frac{\\gamma \\cdot B \\cdot E \\cdot b_f}{1024^2}$, to be reported in MB.\n- The physical field-of-view volumes are $V_{\\text{2D}} = (n_x s_x)(n_y s_y)(1 \\cdot s_z)$ and $V_{\\text{3D}} = (n_x s_x)(n_y s_y)(n_z s_z)$ in cubic millimeters, and the context ratio is $R_V = \\frac{V_{\\text{3D}}}{V_{\\text{2D}}}$.\n- The effective correlation volume is $v_c = \\max\\{1, \\lambda_x \\lambda_y \\lambda_z\\}$, the effective count is $N_{\\text{eff}} = \\max\\left\\{1, \\frac{n_x n_y n_z}{v_c}\\right\\}$, and the AUC must be derived from the Gaussian equal-variance signal detection model. Report AUCs as decimals.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The result for each test case must itself be a list of five floats in the following order: two-dimensional batch memory in MB, three-dimensional batch memory in MB, context ratio $R_V$, two-dimensional AUC, three-dimensional AUC. The final output must therefore be a list of lists, for example: \"[[m2d_A,m3d_A,RV_A,auc2d_A,auc3d_A],[m2d_B,m3d_B,RV_B,auc2d_B,auc3d_B],...]\" with no spaces added unless necessary for numeric formatting.", "solution": "The problem requires a quantitative comparison of two input paradigms for convolutional neural networks (CNNs) in neuroimaging: a two-dimensional (2D) slice-based approach and a three-dimensional (3D) patch-based approach. The comparison is based on three metrics: memory consumption, physical context captured, and a proxy for clinical performance. The solution will be derived from the provided foundational principles and modeling assumptions.\n\n### 1. Memory Consumption Model\n\nThe total memory required for training a neural network is complex, but it is typically dominated by the storage of activations for backpropagation. The problem simplifies this by modeling the per-batch memory as being proportional to the total number of activation elements, with a constant overhead factor $\\gamma$.\n\nLet the input to the CNN be a tensor of size $n_x \\times n_y \\times n_z$. For a 2D slice, we use $n_z=1$ by definition. The network is an encoder with $L$ stages, indexed by $l \\in \\{0, 1, \\dots, L-1\\}$.\n\nAt each stage $l$, the spatial dimensions are reduced by a factor of $2^l$. To handle integer grid sizes and prevent dimensions from vanishing, the size of dimension $d \\in \\{x,y,z\\}$ at stage $l$ is given by:\n$$ n_d^{(l)} = \\max\\left\\{1, \\left\\lfloor \\frac{n_d}{2^l} \\right\\rfloor\\right\\} $$\n\nThe number of channels (feature maps) doubles at each stage, starting from a base count $C_0$:\n$$ C^{(l)} = C_0 \\cdot 2^l $$\n\nThe total number of activation elements for a single sample at stage $l$ is the product of the spatial dimensions and the channel count: $n_x^{(l)} n_y^{(l)} n_z^{(l)} C^{(l)}$. The total number of activation elements per sample, $E$, is the sum over all $L$ stages:\n$$ E = \\sum_{l=0}^{L-1} n_x^{(l)} n_y^{(l)} n_z^{(l)} C^{(l)} $$\n\nGiven a batch of $B$ samples, a floating-point size of $b_f$ bytes, and an overhead factor $\\gamma$, the total memory consumption in bytes is $\\gamma \\cdot B \\cdot E \\cdot b_f$. To convert this to megabytes (MB), we use the conversion $1 \\text{ MB} = 1024^2$ bytes.\n$$ M_{\\text{MB}} = \\frac{\\gamma \\cdot B \\cdot E \\cdot b_f}{1024^2} $$\n\nThis calculation is performed for both the 2D slice configuration (using its specific dimensions $n_x, n_y$ with $n_z=1$, and batch size $B_{\\text{2D}}$) and the 3D patch configuration (using its dimensions $n_x, n_y, n_z$ and batch size $B_{\\text{3D}}$).\n\n### 2. Physical Context Capture Model\n\nThe physical context captured by an input is quantified by its physical Field-Of-View (FOV) volume. This is the product of the number of voxels along each axis and the physical spacing of those voxels.\n\nGiven voxel spacings $(s_x, s_y, s_z)$ in millimeters:\n-   For a 2D slice input of size $(n_x^{\\text{2D}}, n_y^{\\text{2D}})$, its thickness is modeled as corresponding to one voxel in the $z$-dimension. The FOV volume is:\n    $$ V_{\\text{2D}} = (n_x^{\\text{2D}} s_x) \\cdot (n_y^{\\text{2D}} s_y) \\cdot (1 \\cdot s_z) $$\n-   For a 3D patch input of size $(n_x^{\\text{3D}}, n_y^{\\text{3D}}, n_z^{\\text{3D}})$, the FOV volume is:\n    $$ V_{\\text{3D}} = (n_x^{\\text{3D}} s_x) \\cdot (n_y^{\\text{3D}} s_y) \\cdot (n_z^{\\text{3D}} s_z) $$\n\nThe dimensionless context ratio, $R_V$, compares the volume of the 3D patch to that of the 2D slice:\n$$ R_V = \\frac{V_{\\text{3D}}}{V_{\\text{2D}}} = \\frac{(n_x^{\\text{3D}} s_x) (n_y^{\\text{3D}} s_y) (n_z^{\\text{3D}} s_z)}{(n_x^{\\text{2D}} s_x) (n_y^{\\text{2D}} s_y) (s_z)} = \\frac{n_x^{\\text{3D}} n_y^{\\text{3D}} n_z^{\\text{3D}}}{n_x^{\\text{2D}} n_y^{\\text{2D}}} $$\n\n### 3. Predicted Performance Proxy (AUC)\n\nThe performance proxy is based on a Gaussian equal-variance signal detection model. We assume a binary classification task (e.g., lesion vs. non-lesion). For each voxel, the two classes are modeled as draws from two normal distributions with the same standard deviation $\\sigma$ but with means separated by a signal difference $s$.\n\nThe discriminability of a single voxel observation is given by the index $d'_1 = s/\\sigma$. Aggregating information across multiple voxels improves discriminability. Due to spatial correlation between neighboring voxels, the effective number of independent observations, $N_{\\text{eff}}$, is less than the total number of voxels, $N_{\\text{vox}} = n_x n_y n_z$. This is modeled using an effective correlation volume, $v_c$, defined by correlation extents $(\\lambda_x, \\lambda_y, \\lambda_z)$:\n$$ v_c = \\max\\{1, \\lambda_x \\lambda_y \\lambda_z\\} $$\nThe effective number of independent observations is then:\n$$ N_{\\text{eff}} = \\max\\left\\{1, \\frac{N_{\\text{vox}}}{v_c}\\right\\} = \\max\\left\\{1, \\frac{n_x n_y n_z}{v_c}\\right\\} $$\nFor the 2D slice model, $n_z=1$.\n\nThe discriminability index for the aggregated signal over the entire input volume, $d'$, scales with the square root of $N_{\\text{eff}}$:\n$$ d' = d'_1 \\sqrt{N_{\\text{eff}}} = \\frac{s}{\\sigma} \\sqrt{N_{\\text{eff}}} $$\n\nThe Area Under the Receiver Operating Characteristic Curve (AUC) for a Gaussian equal-variance model is related to the discriminability index $d'$ through the cumulative distribution function (CDF) of the standard normal distribution, $\\Phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{z} e^{-t^2/2} dt$. The formula is:\n$$ \\text{AUC} = \\Phi\\left(\\frac{d'}{\\sqrt{2}}\\right) $$\nThis can be computed using the error function, $\\text{erf}(z) = \\frac{2}{\\sqrt{\\pi}} \\int_0^z e^{-t^2} dt$, via the identity $\\Phi(z) = \\frac{1}{2}\\left(1 + \\text{erf}\\left(\\frac{z}{\\sqrt{2}}\\right)\\right)$. Substituting $z=d'/\\sqrt{2}$:\n$$ \\text{AUC} = \\frac{1}{2}\\left(1 + \\text{erf}\\left(\\frac{d'/\\sqrt{2}}{\\sqrt{2}}\\right)\\right) = \\frac{1}{2}\\left(1 + \\text{erf}\\left(\\frac{d'}{2}\\right)\\right) $$\nThis calculation is performed for both the 2D and 3D input configurations using their respective total voxel counts.\n\n### Computational Procedure\nThe implementation will consist of functions that codify these derived models. For each of the specified test cases, these functions will be called with the appropriate parameters for both the 2D slice and 3D patch models to compute the five required output values: 2D memory, 3D memory, context ratio, 2D AUC, and 3D AUC.", "answer": "```python\nimport math\nfrom scipy.special import erf\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    It calculates and prints the comparative metrics for 2D vs. 3D CNN inputs.\n    \"\"\"\n\n    def calculate_memory(dims, L, C0, gamma, B, bf):\n        \"\"\"\n        Calculates the estimated memory consumption for a single training batch.\n\n        Args:\n            dims (tuple): Input dimensions (nx, ny, nz).\n            L (int): Number of network stages.\n            C0 (int): Base number of channels.\n            gamma (float): Memory overhead factor.\n            B (int): Batch size.\n            bf (int): Bytes per floating-point number.\n\n        Returns:\n            float: Memory consumption in Megabytes (MB).\n        \"\"\"\n        nx, ny, nz = dims\n        total_elements_per_sample = 0\n        for l in range(L):\n            pow_2_l = 2**l\n            \n            # Spatial dimensions at stage l\n            nx_l = max(1, math.floor(nx / pow_2_l))\n            ny_l = max(1, math.floor(ny / pow_2_l))\n            nz_l = max(1, math.floor(nz / pow_2_l))\n            \n            # Channel count at stage l\n            channels_l = C0 * pow_2_l\n            \n            elements_at_stage = nx_l * ny_l * nz_l * channels_l\n            total_elements_per_sample += elements_at_stage\n            \n        mem_bytes = gamma * B * total_elements_per_sample * bf\n        mem_mb = mem_bytes / (1024**2)\n        return mem_mb\n\n    def calculate_auc(dims, s, sigma, lambdas):\n        \"\"\"\n        Calculates the predicted AUC using a signal detection theory model.\n\n        Args:\n            dims (tuple): Input dimensions (nx, ny, nz).\n            s (float): Per-voxel signal difference.\n            sigma (float): Per-voxel noise standard deviation.\n            lambdas (tuple): Correlation extents (lx, ly, lz) in voxels.\n\n        Returns:\n            float: The Area Under the ROC Curve (AUC).\n        \"\"\"\n        nx, ny, nz = dims\n        lx, ly, lz = lambdas\n        \n        n_vox = nx * ny * nz\n        v_c = max(1, lx * ly * lz)\n        n_eff = max(1, n_vox / v_c)\n        \n        d_prime = (s / sigma) * math.sqrt(n_eff)\n        \n        auc = 0.5 * (1 + erf(d_prime / 2.0))\n        return auc\n\n    # Constants common to all cases\n    bf = 4  # bytes per float\n\n    # Test suite definition\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"n_2d\": (256, 256),\n            \"n_3d\": (64, 64, 16),\n            \"s_xyz\": (1.0, 1.0, 1.0),\n            \"L\": 3, \"C0\": 16, \"gamma\": 3.0,\n            \"B_2d\": 8, \"B_3d\": 2,\n            \"s\": 0.5, \"sigma\": 1.0, \"lambdas\": (3, 3, 3)\n        },\n        {\n            \"name\": \"Case B\",\n            \"n_2d\": (512, 512),\n            \"n_3d\": (128, 128, 4),\n            \"s_xyz\": (0.8, 0.8, 3.0),\n            \"L\": 4, \"C0\": 8, \"gamma\": 2.5,\n            \"B_2d\": 4, \"B_3d\": 1,\n            \"s\": 0.3, \"sigma\": 1.2, \"lambdas\": (5, 5, 1)\n        },\n        {\n            \"name\": \"Case C\",\n            \"n_2d\": (64, 64),\n            \"n_3d\": (64, 64, 1),\n            \"s_xyz\": (1.0, 1.0, 1.0),\n            \"L\": 2, \"C0\": 16, \"gamma\": 3.0,\n            \"B_2d\": 8, \"B_3d\": 8,\n            \"s\": 0.6, \"sigma\": 0.9, \"lambdas\": (2, 2, 2)\n        },\n        {\n            \"name\": \"Case D\",\n            \"n_2d\": (128, 128),\n            \"n_3d\": (96, 96, 24),\n            \"s_xyz\": (0.5, 0.5, 2.0),\n            \"L\": 2, \"C0\": 32, \"gamma\": 3.5,\n            \"B_2d\": 16, \"B_3d\": 1,\n            \"s\": 0.8, \"sigma\": 0.8, \"lambdas\": (2, 2, 2)\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        # Unpack case parameters\n        dims_2d = (case[\"n_2d\"][0], case[\"n_2d\"][1], 1)\n        dims_3d = case[\"n_3d\"]\n        L, C0, gamma = case[\"L\"], case[\"C0\"], case[\"gamma\"]\n        B_2d, B_3d = case[\"B_2d\"], case[\"B_3d\"]\n        s, sigma, lambdas = case[\"s\"], case[\"sigma\"], case[\"lambdas\"]\n        \n        # 1. Memory Calculation\n        mem_2d = calculate_memory(dims_2d, L, C0, gamma, B_2d, bf)\n        mem_3d = calculate_memory(dims_3d, L, C0, gamma, B_3d, bf)\n\n        # 2. Context Ratio Calculation\n        v_ratio = (dims_3d[0] * dims_3d[1] * dims_3d[2]) / (dims_2d[0] * dims_2d[1])\n\n        # 3. AUC Calculation\n        auc_2d = calculate_auc(dims_2d, s, sigma, lambdas)\n        auc_3d = calculate_auc(dims_3d, s, sigma, lambdas)\n\n        case_results = [mem_2d, mem_3d, v_ratio, auc_2d, auc_3d]\n        all_results.append(case_results)\n\n    # Format the final output string as specified\n    result_strings = [f\"[{','.join(map(str, r))}]\" for r in all_results]\n    final_output = f\"[{','.join(result_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "4491613"}, {"introduction": "Neurological conditions are multifaceted, and a comprehensive diagnosis often requires integrating information from multiple imaging modalities like sMRI, fMRI, and DTI. This advanced practice introduces a powerful Bayesian framework for fusing such multimodal data to infer an unobserved latent variable, such as disease severity. You will implement a probabilistic graphical model that not only combines evidence in a principled manner but also gracefully handles missing data—a common real-world challenge—building your intuition for creating robust and sophisticated clinical prediction tools [@problem_id:4491601].", "problem": "You are tasked with designing and analyzing a probabilistic graphical model for multimodal neuroimaging fusion in neurology, where modalities such as Structural Magnetic Resonance Imaging (sMRI), functional Magnetic Resonance Imaging (fMRI), and Diffusion Tensor Imaging (DTI) provide conditionally independent observations of an unobserved continuous neurological severity variable. The fundamental base you must start from is Bayes' theorem, conditional independence in probabilistic graphical models, and the conjugacy of Gaussian priors and Gaussian likelihoods. You must derive the inference procedure and implement it as a program.\n\nConsider a latent variable $z \\in \\mathbb{R}$ representing neurological disease severity. The prior for $z$ is Gaussian, $z \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$. Each modality $i$ produces an observation $x_i \\in \\mathbb{R}$ modeled as a linear-Gaussian conditional, $x_i \\mid z \\sim \\mathcal{N}(a_i z + b_i, \\sigma_i^2)$, and modalities are independent conditioned on $z$. The multimodal fusion uses the product of these likelihoods with the prior to form the posterior over $z$. Missing modalities must be handled by marginalization: integrate out the missing observations in the joint model to obtain the posterior given only the observed modalities.\n\nYour tasks are:\n- From the stated fundamentals, derive the posterior distribution $p(z \\mid \\{x_i\\}_{i \\in \\mathcal{O}})$ when only the subset $\\mathcal{O}$ of modalities is observed. Express the posterior mean and variance in terms of $(\\mu_0, \\sigma_0^2)$ and the observed modality parameters and data.\n- Explain why marginalization over missing modalities $\\mathcal{M}$ removes their likelihood contributions in this linear-Gaussian model by using the property $\\int p(x_i \\mid z) \\, dx_i = 1$ for each missing modality $i \\in \\mathcal{M}$.\n- Define robustness metrics comparing the posterior with missing modalities to the posterior with all modalities available: the Kullback–Leibler divergence $\\mathrm{KL}(\\mathcal{N}(\\mu_{\\text{miss}}, \\sigma^2_{\\text{miss}}) \\Vert \\mathcal{N}(\\mu_{\\text{full}}, \\sigma^2_{\\text{full}}))$, the variance ratio $r = \\sigma^2_{\\text{miss}} / \\sigma^2_{\\text{full}}$, and the change in mean-squared error $\\Delta \\mathrm{MSE} = (\\mu_{\\text{miss}} - z^\\star)^2 - (\\mu_{\\text{full}} - z^\\star)^2$ for a given ground-truth $z^\\star$.\n\nImplement a program that, for each test case below, computes the posterior mean $\\mu_{\\text{miss}}$, posterior variance $\\sigma^2_{\\text{miss}}$, the Kullback–Leibler divergence to the full-modality posterior, the variance ratio $r$, and the change in mean-squared error $\\Delta \\mathrm{MSE}$ relative to the baseline full-modality posterior. All values must be returned as floats rounded to $6$ decimal places.\n\nUse the following test suite, where all modalities are sMRI ($i=1$), fMRI ($i=2$), and DTI ($i=3$). For each test case, the prior parameters $(\\mu_0, \\sigma_0^2)$, modality parameters $(a_i, b_i, \\sigma_i^2)$, full observation vector $(x_1, x_2, x_3)$, ground-truth $z^\\star$, and the set of missing modalities are specified:\n\n- Test Case $1$ (happy path, one moderately noisy missing modality): $(\\mu_0, \\sigma_0^2) = (0.0, 1.0)$; sMRI $(a_1, b_1, \\sigma_1^2) = (1.2, 0.0, 0.25)$; fMRI $(a_2, b_2, \\sigma_2^2) = (0.8, 0.0, 0.64)$; DTI $(a_3, b_3, \\sigma_3^2) = (0.5, 0.0, 0.36)$; $(x_1, x_2, x_3) = (0.65, 0.30, 0.35)$; $z^\\star = 0.5$; missing modalities $\\{2\\}$.\n- Test Case $2$ (no missing modalities identity case): same parameters and observations as Test Case $1$; missing modalities $\\varnothing$.\n- Test Case $3$ (boundary case, all modalities missing): same parameters and observations as Test Case $1$; missing modalities $\\{1,2,3\\}$.\n- Test Case $4$ (edge case, non-informative modality removal): $(\\mu_0, \\sigma_0^2) = (0.0, 1.0)$; sMRI $(a_1, b_1, \\sigma_1^2) = (1.2, 0.0, 0.25)$; fMRI $(a_2, b_2, \\sigma_2^2) = (0.8, 0.0, 0.64)$; DTI $(a_3, b_3, \\sigma_3^2) = (0.0, 0.0, 0.36)$; $(x_1, x_2, x_3) = (0.65, 0.30, 0.35)$; $z^\\star = 0.5$; missing modalities $\\{3\\}$.\n- Test Case $5$ (robustness stress, missing the most informative modality): same parameters and observations as Test Case $1$; missing modalities $\\{1\\}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case in order from $1$ to $5$, append the following five rounded values: $[\\mu_{\\text{miss}}, \\sigma^2_{\\text{miss}}, \\mathrm{KL}, r, \\Delta \\mathrm{MSE}]$. Thus, the final output contains $25$ floats in the order of the test cases, rounded to $6$ decimal places, for example, $[\\text{tc1\\_mu},\\text{tc1\\_var},\\text{tc1\\_kl},\\text{tc1\\_r},\\text{tc1\\_dmse},\\ldots,\\text{tc5\\_dmse}]$.", "solution": "The problem is valid as it is scientifically grounded in Bayesian statistics, well-posed with a unique solvable structure, and objective in its formulation. We can proceed to derive the solution.\n\nThe problem asks for an analysis of a probabilistic graphical model for multimodal neuroimaging data fusion. The model consists of a continuous latent variable $z \\in \\mathbb{R}$ representing disease severity, with a Gaussian prior, and a set of observations $\\{x_i\\}$ from different imaging modalities, each modeled by a Gaussian likelihood conditioned on $z$.\n\n### 1. Derivation of the Posterior Distribution\n\nLet $\\mathcal{O}$ be the set of indices corresponding to the observed modalities. We are asked to derive the posterior distribution $p(z \\mid \\{x_i\\}_{i \\in \\mathcal{O}})$. According to Bayes' theorem and the conditional independence of the modalities given $z$, the posterior is proportional to the product of the prior and the likelihoods of the observed modalities:\n$$\np(z \\mid \\{x_i\\}_{i \\in \\mathcal{O}}) \\propto p(z) \\prod_{i \\in \\mathcal{O}} p(x_i \\mid z)\n$$\nThe prior is a Gaussian distribution:\n$$\np(z) = \\mathcal{N}(z \\mid \\mu_0, \\sigma_0^2) \\propto \\exp\\left( -\\frac{(z - \\mu_0)^2}{2\\sigma_0^2} \\right)\n$$\nThe likelihood for each observed modality $i \\in \\mathcal{O}$ is also Gaussian:\n$$\np(x_i \\mid z) = \\mathcal{N}(x_i \\mid a_i z + b_i, \\sigma_i^2) \\propto \\exp\\left( -\\frac{(x_i - (a_i z + b_i))^2}{2\\sigma_i^2} \\right)\n$$\nThe posterior is therefore proportional to the product of these exponential terms:\n$$\np(z \\mid \\{x_i\\}_{i \\in \\mathcal{O}}) \\propto \\exp\\left( -\\frac{(z - \\mu_0)^2}{2\\sigma_0^2} - \\sum_{i \\in \\mathcal{O}} \\frac{(x_i - a_i z - b_i)^2}{2\\sigma_i^2} \\right)\n$$\nThe product of Gaussian distributions is an unnormalized Gaussian. To find the parameters of the posterior distribution, let's denote it as $\\mathcal{N}(z \\mid \\mu_{\\mathcal{O}}, \\sigma_{\\mathcal{O}}^2)$. Its probability density function has the form:\n$$\np(z \\mid \\{x_i\\}_{i \\in \\mathcal{O}}) \\propto \\exp\\left( -\\frac{(z - \\mu_{\\mathcal{O}})^2}{2\\sigma_{\\mathcal{O}}^2} \\right)\n$$\nWe can find $\\mu_{\\mathcal{O}}$ and $\\sigma_{\\mathcal{O}}^2$ by expanding the quadratic term in the exponent of the posterior and matching coefficients of $z^2$ and $z$. The argument of the exponential is:\n$$\nL(z) = -\\frac{1}{2} \\left[ \\frac{(z - \\mu_0)^2}{\\sigma_0^2} + \\sum_{i \\in \\mathcal{O}} \\frac{(a_i z - (x_i - b_i))^2}{\\sigma_i^2} \\right] \\\\\n= -\\frac{1}{2} \\left[ \\frac{z^2 - 2z\\mu_0 + \\mu_0^2}{\\sigma_0^2} + \\sum_{i \\in \\mathcal{O}} \\frac{a_i^2 z^2 - 2a_i z (x_i - b_i) + (x_i - b_i)^2}{\\sigma_i^2} \\right]\n$$\nCollecting terms in $z^2$ and $z$:\n$$\nL(z) = -\\frac{1}{2} \\left[ z^2 \\left( \\frac{1}{\\sigma_0^2} + \\sum_{i \\in \\mathcal{O}} \\frac{a_i^2}{\\sigma_i^2} \\right) - 2z \\left( \\frac{\\mu_0}{\\sigma_0^2} + \\sum_{i \\in \\mathcal{O}} \\frac{a_i(x_i - b_i)}{\\sigma_i^2} \\right) + C \\right]\n$$\nwhere $C$ contains terms not dependent on $z$.\nBy comparing this to the standard quadratic form of a Gaussian exponent, $-\\frac{1}{2\\sigma_{\\mathcal{O}}^2}(z^2 - 2z\\mu_{\\mathcal{O}} + \\mu_{\\mathcal{O}}^2)$, we can identify the inverse variance (precision) and the mean of the posterior.\n\nThe coefficient of $z^2$ gives the posterior precision:\n$$\n\\frac{1}{\\sigma_{\\mathcal{O}}^2} = \\frac{1}{\\sigma_0^2} + \\sum_{i \\in \\mathcal{O}} \\frac{a_i^2}{\\sigma_i^2}\n$$\nSo, the posterior variance is:\n$$\n\\sigma_{\\mathcal{O}}^2 = \\left( \\frac{1}{\\sigma_0^2} + \\sum_{i \\in \\mathcal{O}} \\frac{a_i^2}{\\sigma_i^2} \\right)^{-1}\n$$\nThe coefficient of $-2z$ allows us to find the posterior mean:\n$$\n\\frac{\\mu_{\\mathcal{O}}}{\\sigma_{\\mathcal{O}}^2} = \\frac{\\mu_0}{\\sigma_0^2} + \\sum_{i \\in \\mathcal{O}} \\frac{a_i(x_i - b_i)}{\\sigma_i^2}\n$$\nThus, the posterior mean is:\n$$\n\\mu_{\\mathcal{O}} = \\sigma_{\\mathcal{O}}^2 \\left( \\frac{\\mu_0}{\\sigma_0^2} + \\sum_{i \\in \\mathcal{O}} \\frac{a_i(x_i - b_i)}{\\sigma_i^2} \\right)\n$$\nThese equations define the parameters for the posterior distribution $p(z \\mid \\{x_i\\}_{i \\in \\mathcal{O}}) = \\mathcal{N}(z \\mid \\mu_{\\mathcal{O}}, \\sigma_{\\mathcal{O}}^2)$. For a given test case, $\\mu_{\\text{miss}}$ and $\\sigma^2_{\\text{miss}}$ are calculated using these formulas where $\\mathcal{O}$ is the set of observed modalities. The full posterior parameters, $\\mu_{\\text{full}}$ and $\\sigma^2_{\\text{full}}$, are computed similarly, but with $\\mathcal{O}$ being the set of all modalities.\n\n### 2. Marginalization Over Missing Modalities\n\nLet the complete set of all modality indices be $\\mathcal{I}$, which is a disjoint union of observed modalities $\\mathcal{O}$ and missing modalities $\\mathcal{M}$, i.e., $\\mathcal{I} = \\mathcal{O} \\cup \\mathcal{M}$. We seek the posterior $p(z \\mid \\{x_i\\}_{i \\in \\mathcal{O}})$. By definition of conditional probability:\n$$\np(z \\mid \\{x_i\\}_{i \\in \\mathcal{O}}) = \\frac{p(z, \\{x_i\\}_{i \\in \\mathcal{O}})}{p(\\{x_i\\}_{i \\in \\mathcal{O}})}\n$$\nThe joint distribution of $z$ and the observed data $\\{x_i\\}_{i \\in \\mathcal{O}}$ is obtained by marginalizing (integrating out) the variables of the missing modalities $\\{x_j\\}_{j \\in \\mathcal{M}}$ from the full joint distribution $p(z, \\{x_i\\}_{i \\in \\mathcal{I}})$:\n$$\np(z, \\{x_i\\}_{i \\in \\mathcal{O}}) = \\int p(z, \\{x_i\\}_{i \\in \\mathcal{I}}) \\prod_{j \\in \\mathcal{M}} dx_j\n$$\nUsing the chain rule and conditional independence, the full joint is $p(z, \\{x_i\\}_{i \\in \\mathcal{I}}) = p(z) \\prod_{i \\in \\mathcal{I}} p(x_i \\mid z)$. Substituting this into the integral:\n$$\np(z, \\{x_i\\}_{i \\in \\mathcal{O}}) = \\int p(z) \\left(\\prod_{i \\in \\mathcal{O}} p(x_i \\mid z)\\right) \\left(\\prod_{j \\in \\mathcal{M}} p(x_j \\mid z)\\right) \\prod_{j \\in \\mathcal{M}} dx_j\n$$\nThe terms depending only on $z$ and $\\{x_i\\}_{i \\in \\mathcal{O}}$ can be moved outside the integrals:\n$$\np(z, \\{x_i\\}_{i \\in \\mathcal{O}}) = p(z) \\left(\\prod_{i \\in \\mathcal{O}} p(x_i \\mid z)\\right) \\left(\\prod_{j \\in \\mathcal{M}} \\int p(x_j \\mid z) dx_j \\right)\n$$\nFor any missing modality $j \\in \\mathcal{M}$, the term $p(x_j \\mid z)$ is a probability density function. The integral of any PDF over its entire domain is, by definition, equal to $1$:\n$$\n\\int_{-\\infty}^{\\infty} p(x_j \\mid z) dx_j = 1\n$$\nThis is true regardless of the value of $z$. Therefore, the product of these integrals is also $1$. This simplifies the joint distribution to:\n$$\np(z, \\{x_i\\}_{i \\in \\mathcal{O}}) = p(z) \\prod_{i \\in \\mathcal{O}} p(x_i \\mid z)\n$$\nConsequently, the posterior becomes:\n$$\np(z \\mid \\{x_i\\}_{i \\in \\mathcal{O}}) \\propto p(z) \\prod_{i \\in \\mathcal{O}} p(x_i \\mid z)\n$$\nThis demonstrates that handling missing modalities via marginalization is mathematically equivalent to simply omitting their likelihood terms from the product in Bayes' rule. The information from missing modalities is nullified.\n\n### 3. Robustness Metrics\n\nThe problem specifies three metrics to quantify the effect of missing data on the posterior distribution. Let the posterior from the full set of modalities be $\\mathcal{N}_{\\text{full}} = \\mathcal{N}(\\mu_{\\text{full}}, \\sigma^2_{\\text{full}})$ and the posterior from a subset of observed modalities be $\\mathcal{N}_{\\text{miss}} = \\mathcal{N}(\\mu_{\\text{miss}}, \\sigma^2_{\\text{miss}})$.\n\n1.  **Kullback–Leibler (KL) Divergence**: The KL divergence from $\\mathcal{N}_{\\text{miss}}$ to $\\mathcal{N}_{\\text{full}}$ measures the information lost when approximating $\\mathcal{N}_{\\text{miss}}$ with $\\mathcal{N}_{\\text{full}}$. The formula for two univariate Gaussian distributions $\\mathcal{N}_1 = \\mathcal{N}(\\mu_1, \\sigma_1^2)$ and $\\mathcal{N}_2 = \\mathcal{N}(\\mu_2, \\sigma_2^2)$ is:\n    $$\n    \\mathrm{KL}(\\mathcal{N}_1 \\Vert \\mathcal{N}_2) = \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}\n    $$\n    Here, $\\mathcal{N}_1 = \\mathcal{N}_{\\text{miss}}$ and $\\mathcal{N}_2 = \\mathcal{N}_{\\text{full}}$.\n\n2.  **Variance Ratio**: This is the ratio of the posterior variance with missing data to the posterior variance with full data.\n    $$\n    r = \\frac{\\sigma^2_{\\text{miss}}}{\\sigma^2_{\\text{full}}}\n    $$\n    Since observing more data can only increase the posterior precision (or leave it unchanged), we expect $\\sigma^2_{\\text{miss}} \\geq \\sigma^2_{\\text{full}}$, and thus $r \\geq 1$.\n\n3.  **Change in Mean-Squared Error ($\\Delta \\mathrm{MSE}$)**: This metric evaluates how the point estimate of $z$ (taken as the posterior mean) degrades with respect to a ground-truth value $z^\\star$.\n    $$\n    \\Delta \\mathrm{MSE} = (\\mu_{\\text{miss}} - z^\\star)^2 - (\\mu_{\\text{full}} - z^\\star)^2\n    $$\n    A positive value indicates that missing data increased the squared error of the estimate.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the multimodal neuroimaging fusion problem for the given test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"prior\": (0.0, 1.0),\n            \"modalities\": {\n                1: {\"params\": (1.2, 0.0, 0.25), \"obs\": 0.65},  # sMRI\n                2: {\"params\": (0.8, 0.0, 0.64), \"obs\": 0.30},  # fMRI\n                3: {\"params\": (0.5, 0.0, 0.36), \"obs\": 0.35},  # DTI\n            },\n            \"z_star\": 0.5,\n            \"missing\": {2}\n        },\n        {\n            \"prior\": (0.0, 1.0),\n            \"modalities\": {\n                1: {\"params\": (1.2, 0.0, 0.25), \"obs\": 0.65},\n                2: {\"params\": (0.8, 0.0, 0.64), \"obs\": 0.30},\n                3: {\"params\": (0.5, 0.0, 0.36), \"obs\": 0.35},\n            },\n            \"z_star\": 0.5,\n            \"missing\": set()\n        },\n        {\n            \"prior\": (0.0, 1.0),\n            \"modalities\": {\n                1: {\"params\": (1.2, 0.0, 0.25), \"obs\": 0.65},\n                2: {\"params\": (0.8, 0.0, 0.64), \"obs\": 0.30},\n                3: {\"params\": (0.5, 0.0, 0.36), \"obs\": 0.35},\n            },\n            \"z_star\": 0.5,\n            \"missing\": {1, 2, 3}\n        },\n        {\n            \"prior\": (0.0, 1.0),\n            \"modalities\": {\n                1: {\"params\": (1.2, 0.0, 0.25), \"obs\": 0.65},\n                2: {\"params\": (0.8, 0.0, 0.64), \"obs\": 0.30},\n                3: {\"params\": (0.0, 0.0, 0.36), \"obs\": 0.35},  # Non-informative DTI\n            },\n            \"z_star\": 0.5,\n            \"missing\": {3}\n        },\n        {\n            \"prior\": (0.0, 1.0),\n            \"modalities\": {\n                1: {\"params\": (1.2, 0.0, 0.25), \"obs\": 0.65},\n                2: {\"params\": (0.8, 0.0, 0.64), \"obs\": 0.30},\n                3: {\"params\": (0.5, 0.0, 0.36), \"obs\": 0.35},\n            },\n            \"z_star\": 0.5,\n            \"missing\": {1}\n        },\n    ]\n\n    def calculate_posterior(prior, modalities, observations, observed_indices):\n        \"\"\"\n        Calculates the posterior mean and variance given a set of observations.\n        \n        Args:\n            prior (tuple): (mu_0, sigma_0_sq).\n            modalities (dict): Dictionary of modality parameters.\n            observations (dict): Dictionary of modality observations.\n            observed_indices (set): Set of indices for observed modalities.\n\n        Returns:\n            tuple: (posterior_mean, posterior_variance).\n        \"\"\"\n        mu_0, sigma_0_sq = prior\n        \n        # Precision = 1 / variance\n        post_precision = 1.0 / sigma_0_sq\n        mu_prec_term = mu_0 / sigma_0_sq\n        \n        for i in observed_indices:\n            a_i, b_i, sigma_i_sq = modalities[i]\n            x_i = observations[i]\n            \n            # Add likelihood precision\n            lik_precision = (a_i ** 2) / sigma_i_sq\n            post_precision += lik_precision\n            \n            # Add weighted mean term from likelihood\n            mu_prec_term += (a_i * (x_i - b_i)) / sigma_i_sq\n            \n        post_variance = 1.0 / post_precision\n        post_mean = post_variance * mu_prec_term\n        \n        return post_mean, post_variance\n\n    final_results = []\n    \n    for case in test_cases:\n        prior_params = case[\"prior\"]\n        modality_params = {k: v[\"params\"] for k, v in case[\"modalities\"].items()}\n        observations = {k: v[\"obs\"] for k, v in case[\"modalities\"].items()}\n        z_star = case[\"z_star\"]\n        missing_indices = case[\"missing\"]\n        all_indices = set(modality_params.keys())\n        observed_indices = all_indices - missing_indices\n\n        # 1. Calculate full posterior (all modalities observed)\n        mu_full, var_full = calculate_posterior(\n            prior_params, modality_params, observations, all_indices\n        )\n\n        # 2. Calculate posterior with missing data\n        mu_miss, var_miss = calculate_posterior(\n            prior_params, modality_params, observations, observed_indices\n        )\n\n        # 3. Calculate metrics\n        # KL Divergence: KL(miss || full)\n        if var_miss <= 0 or var_full <= 0: # Avoid log(0) or division by zero\n            kl_div = float('inf') if var_full > 0 else 0.0\n        else:\n            term1 = np.log(np.sqrt(var_full) / np.sqrt(var_miss))\n            term2 = (var_miss + (mu_miss - mu_full)**2) / (2 * var_full)\n            kl_div = term1 + term2 - 0.5\n\n        # Variance Ratio\n        var_ratio = var_miss / var_full if var_full > 0 else float('inf')\n\n        # Change in MSE\n        delta_mse = (mu_miss - z_star)**2 - (mu_full - z_star)**2\n\n        # In case of perfect match (e.g. no missing data), KL can be tiny negative due to float precision. Clamp to 0.\n        if abs(kl_div) < 1e-12: kl_div = 0.0\n        if abs(var_ratio - 1.0) < 1e-12: var_ratio = 1.0\n        if abs(delta_mse) < 1e-12: delta_mse = 0.0\n\n        case_results = [mu_miss, var_miss, kl_div, var_ratio, delta_mse]\n        final_results.extend(case_results)\n\n    # Format output\n    formatted_results = [f\"{val:.6f}\" for val in final_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```", "id": "4491601"}]}