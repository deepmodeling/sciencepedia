{"hands_on_practices": [{"introduction": "The link between a neuron's intricate structure and its electrical function is a cornerstone of neuroscience. Wilfrid Rall's pioneering work established that dendritic branching is not random but often adheres to principles that optimize the flow of electrical signals. This exercise delves into the crucial concept of impedance matching, guiding you to derive Rall's famous $3/2$ power law from the first principles of passive cable theory. By working through this foundational problem, you will gain a deeper understanding of the biophysical constraints that shape neuronal morphology and learn how to quantitatively evaluate how closely a real dendritic branch follows this ideal design principle. [@problem_id:4508610]", "problem": "In a passive dendritic tree modeled by cable theory with spatially homogeneous specific membrane resistivity $R_{m}$ and intracellular (axial) resistivity $R_{i}$, morphometric constraints at a bifurcation can be derived from the requirement that the branch point is impedance matched so that no reflections occur and electrotonic length is preserved across the junction. Starting from fundamental cable definitions for axial resistance per unit length and membrane resistance per unit length, derive the diameter relationship that enforces impedance matching at a dendritic bifurcation between a parent branch of diameter $d_{0}$ and two daughter branches of diameters $d_{1}$ and $d_{2}$, under the assumption that each branch can be treated locally as semi-infinite around the junction and membrane properties are uniform.\n\nYou are given a specific bifurcation measured in a cortical pyramidal neuron apical dendrite with parent diameter $d_{0} = 3.0\\,\\mu\\text{m}$ and daughter diameters $d_{1} = 2.5\\,\\mu\\text{m}$ and $d_{2} = 1.2\\,\\mu\\text{m}$. Using your derived relationship, compute the absolute fractional deviation from the exact impedance-matching condition, defined as\n$$\\varepsilon \\equiv \\frac{\\left|\\,f(d_{1},d_{2}) - f(d_{0})\\,\\right|}{f(d_{0})},$$\nwhere $f(\\cdot)$ is the diameter-dependent quantity implied by the impedance-matching condition you derived. Report $\\varepsilon$ as a decimal fraction (for example, report $0.05$ for a five percent deviation). Round your answer to four significant figures. Do not include any units or a percent sign in your final reported value.", "solution": "The problem requires the derivation of a diameter relationship for impedance matching at a dendritic bifurcation and the calculation of a fractional deviation for a specific case.\n\n### Part 1: Derivation of the Diameter Relationship\n\nThe problem is framed within the context of passive cable theory applied to a dendritic tree. We are given that the specific membrane resistivity, $R_m$ (units of $\\Omega \\cdot \\text{m}^2$), and the specific intracellular (axial) resistivity, $R_i$ (units of $\\Omega \\cdot \\text{m}$), are spatially homogeneous.\n\nFirst, we must define the relevant electrical properties for a cylindrical dendrite of diameter $d$.\nThe axial resistance per unit length, $r_a$, is the intracellular resistivity $R_i$ divided by the cross-sectional area $A = \\pi (d/2)^2$.\n$$r_a = \\frac{R_i}{A} = \\frac{R_i}{\\pi (d/2)^2} = \\frac{4 R_i}{\\pi d^2}$$\nThe membrane resistance for a unit length of the cylinder, $r_m$, is the specific membrane resistivity $R_m$ divided by the circumference of the cylinder, which is $\\pi d$.\n$$r_m = \\frac{R_m}{\\pi d}$$\nThe problem states that each branch at the bifurcation can be treated locally as a semi-infinite cable. For a semi-infinite passive cable, the DC or steady-state input resistance, $R_{in}$, is given by the characteristic resistance of the cable:\n$$R_{in} = \\sqrt{r_a r_m}$$\nSubstituting the expressions for $r_a$ and $r_m$ in terms of the diameter $d$:\n$$R_{in}(d) = \\sqrt{\\left(\\frac{4 R_i}{\\pi d^2}\\right) \\left(\\frac{R_m}{\\pi d}\\right)} = \\sqrt{\\frac{4 R_i R_m}{\\pi^2 d^3}} = \\frac{2}{\\pi} \\sqrt{\\frac{R_i R_m}{d^3}}$$\nThe input conductance, $G_{in}$, is the reciprocal of the input resistance, $G_{in} = 1/R_{in}$.\n$$G_{in}(d) = \\frac{1}{R_{in}(d)} = \\frac{\\pi}{2} \\sqrt{\\frac{d^3}{R_i R_m}}$$\nLet the parent branch have diameter $d_0$ and the two daughter branches have diameters $d_1$ and $d_2$. The impedance matching condition is:\n$$G_{in}(d_0) = G_{in}(d_1) + G_{in}(d_2)$$\nSubstituting the expression for $G_{in}(d)$:\n$$\\frac{\\pi}{2} \\sqrt{\\frac{d_0^3}{R_i R_m}} = \\frac{\\pi}{2} \\sqrt{\\frac{d_1^3}{R_i R_m}} + \\frac{\\pi}{2} \\sqrt{\\frac{d_2^3}{R_i R_m}}$$\nSince the problem states that membrane and intracellular properties ($R_m$ and $R_i$) are uniform across the tree, the constant term $\\frac{\\pi}{2\\sqrt{R_i R_m}}$ is common to all terms and can be cancelled. This yields the desired diameter relationship:\n$$\\sqrt{d_0^3} = \\sqrt{d_1^3} + \\sqrt{d_2^3}$$\nThis is commonly known as Rall's $3/2$ power rule, which can also be written as:\n$$d_0^{3/2} = d_1^{3/2} + d_2^{3/2}$$\n\n### Part 2: Calculation of the Fractional Deviation\n\nThe problem defines the absolute fractional deviation from the exact impedance-matching condition as:\n$$\\varepsilon \\equiv \\frac{\\left|\\,f(d_{1},d_{2}) - f(d_{0})\\,\\right|}{f(d_{0})}$$\nFrom our derivation, the diameter-dependent quantity $f(\\cdot)$ that determines the conductance is $d^{3/2}$. Therefore, $f(d_0) = d_0^{3/2}$ and the combined contribution from the daughters is $f(d_1, d_2) = d_1^{3/2} + d_2^{3/2}$. The formula for $\\varepsilon$ becomes:\n$$\\varepsilon = \\frac{\\left| (d_1^{3/2} + d_2^{3/2}) - d_0^{3/2} \\right|}{d_0^{3/2}} = \\left| \\frac{d_1^{3/2} + d_2^{3/2}}{d_0^{3/2}} - 1 \\right|$$\nWe are given the following diameters for a specific bifurcation:\n$d_0 = 3.0\\,\\mu\\text{m}$\n$d_1 = 2.5\\,\\mu\\text{m}$\n$d_2 = 1.2\\,\\mu\\text{m}$\n\nThe units of micrometers ($\\mu\\text{m}$) will cancel in the ratio, so we can use the numerical values directly.\nFirst, we compute the required terms:\nThe term for the parent branch is:\n$f(d_0) = d_0^{3/2} = (3.0)^{3/2} \\approx 5.196152$\nThe terms for the daughter branches are:\n$d_1^{3/2} = (2.5)^{3/2} \\approx 3.952847$\n$d_2^{3/2} = (1.2)^{3/2} \\approx 1.314534$\nThe sum for the daughter branches is:\n$f(d_1, d_2) = d_1^{3/2} + d_2^{3/2} \\approx 3.952847 + 1.314534 = 5.267381$\nNow, we substitute these values into the expression for $\\varepsilon$:\n$$\\varepsilon = \\frac{\\left| 5.267381 - 5.196152 \\right|}{5.196152} = \\frac{0.071229}{5.196152} \\approx 0.01370798$$\nThe problem requires the answer to be rounded to four significant figures.\n$\\varepsilon \\approx 0.01371$\nThis value represents an absolute fractional deviation of approximately $1.371\\%$ from the ideal impedance-matching condition for the given dendritic bifurcation.", "answer": "$$\\boxed{0.01371}$$", "id": "4508610"}, {"introduction": "To translate theoretical principles into data-driven insights, we must develop methods to objectively quantify the complex three-dimensional architecture of neurons. Modern microscopy and reconstruction techniques provide vast amounts of raw anatomical data, but their scientific value is unlocked only when we extract meaningful, comparable features. This practice guides you through the computational task of transforming the raw 3D coordinates of a neuron into a standardized feature vector by implementing algorithms to measure key geometric attributes like branch angles, path lengths, and tapering profiles. This hands-on coding exercise is essential for building the skills needed for large-scale, automated analysis and classification of neuronal cell types. [@problem_id:4004748]", "problem": "You are provided with the three-dimensional coordinates of nodes from a neuron reconstruction, together with a parent pointer for each node and a radius at each node. The reconstruction defines a rooted tree in three-dimensional space, where each node $i$ has position $(x_i, y_i, z_i)$ in micrometers ($\\mu m$), a radius $r_i$ in micrometers ($\\mu m$), and a parent index $p_i$, with $p_i = -1$ indicating the root. The tree is assumed to be anatomically plausible: it is a directed acyclic graph with a single root, and each node other than the root has exactly one parent.\n\nYour task is to derive and implement from first principles the computation of a morphological feature vector suitable for downstream classification. Starting from basic Euclidean geometry and definitions for curves discretized by polylines, you must construct the following features:\n\n1. Branch angle histogram:\n- For any node $b$ that has at least two children, define the outgoing segment vector for a child $c$ as $\\mathbf{v}_c = \\mathbf{x}_c - \\mathbf{x}_b$, where $\\mathbf{x}_j = (x_j, y_j, z_j)$ is the Cartesian coordinate of node $j$.\n- For each unordered pair of children $(c_1, c_2)$, define the branch angle\n$$\n\\theta(b; c_1, c_2) = \\arccos\\left(\\frac{\\mathbf{v}_{c_1} \\cdot \\mathbf{v}_{c_2}}{\\|\\mathbf{v}_{c_1}\\| \\, \\|\\mathbf{v}_{c_2}\\|}\\right),\n$$\nwith $\\theta$ measured in radians.\n- Accumulate all such angles across the entire tree and compute a normalized histogram with bin edges\n$$\nB_\\theta = [0, \\pi/6, \\pi/3, \\pi/2, 2\\pi/3, 5\\pi/6, \\pi],\n$$\ninterpreted as half-open intervals $[b_k, b_{k+1})$ except for the last bin, which is $[b_6, b_7]$. The histogram must be normalized to sum to $1$. If there are no bifurcation angles, the histogram must be a vector of zeros of length $6$.\n\n2. Root-to-leaf path length distribution:\n- Define a leaf as any node with zero children.\n- For each leaf $t$, define the unique path $P(t)$ from the root to $t$ and its length\n$$\nL(t) = \\sum_{(i \\rightarrow j) \\in P(t)} \\|\\mathbf{x}_j - \\mathbf{x}_i\\|,\n$$\nwith $\\|\\cdot\\|$ the Euclidean norm and lengths measured in micrometers ($\\mu m$).\n- Compute a normalized histogram over the set $\\{L(t)\\}$ using bin edges\n$$\nB_L = [0, 20, 40, 60, 80, 100],\n$$\nwith the same half-open interpretation $[b_k, b_{k+1})$ except the last bin $[b_5, b_6]$. The histogram must sum to $1$. If there are no leaves, the histogram must be a vector of zeros of length $5$.\n\n3. Curvature estimates:\n- For each root-to-leaf path with ordered points $(\\mathbf{x}_0, \\mathbf{x}_1, \\dots, \\mathbf{x}_n)$, approximate the curvature at interior points $k = 1, \\dots, n-1$ by the discrete formula\n$$\n\\kappa_k = \\frac{\\arccos\\left(\\frac{(\\mathbf{x}_k - \\mathbf{x}_{k-1}) \\cdot (\\mathbf{x}_{k+1} - \\mathbf{x}_k)}{\\|\\mathbf{x}_k - \\mathbf{x}_{k-1}\\| \\, \\|\\mathbf{x}_{k+1} - \\mathbf{x}_k\\|}\\right)}{\\frac{1}{2}\\left(\\|\\mathbf{x}_k - \\mathbf{x}_{k-1}\\| + \\|\\mathbf{x}_{k+1} - \\mathbf{x}_k\\|\\right)},\n$$\nwith $\\kappa_k$ in radians per micrometer. Aggregate all curvature samples across all paths into a set $\\{\\kappa_k\\}$. Define the mean curvature\n$$\n\\bar{\\kappa} = \\frac{1}{M}\\sum_{m=1}^{M} \\kappa_m\n$$\nand the population standard deviation\n$$\n\\sigma_{\\kappa} = \\sqrt{\\frac{1}{M}\\sum_{m=1}^{M} (\\kappa_m - \\bar{\\kappa})^2},\n$$\nwhere $M$ is the total number of curvature samples across all paths. If there are no interior points, set both $\\bar{\\kappa}$ and $\\sigma_{\\kappa}$ to $0$.\n\n4. Tapering profiles:\n- For each root-to-leaf path, let $s_k$ be the cumulative path length at node $k$ measured from the root along the path (in micrometers), and $r_k$ the radius at node $k$ (in micrometers).\n- For each path with at least $2$ points, compute the ordinary least squares slope $m$ of $r$ versus $s$ given by\n$$\nm = \\frac{\\sum_{k}(s_k - \\bar{s})(r_k - \\bar{r})}{\\sum_{k}(s_k - \\bar{s})^2},\n$$\nwhere $\\bar{s}$ and $\\bar{r}$ are the means of $\\{s_k\\}$ and $\\{r_k\\}$, respectively. If the denominator is zero or the path has fewer than $2$ points, define $m = 0$. Aggregate the slopes from all paths into a set $\\{m\\}$ and define the mean slope\n$$\n\\bar{m} = \\frac{1}{P}\\sum_{p=1}^{P} m_p\n$$\nand the population standard deviation\n$$\n\\sigma_m = \\sqrt{\\frac{1}{P}\\sum_{p=1}^{P} (m_p - \\bar{m})^2},\n$$\nwhere $P$ is the number of paths.\n\nConstruct the feature vector by concatenating, in order, the branch angle histogram (length $6$), the path length histogram (length $5$), the curvature mean and standard deviation (length $2$), and the taper slope mean and standard deviation (length $2$). The total feature vector length is $15$. All lengths must be in micrometers ($\\mu m$); all angles must be in radians; curvature must be in radians per micrometer; taper slopes must be in micrometers per micrometer.\n\nTest Suite:\nUse the following four test cases, each defined by a list of nodes $(i, x_i, y_i, z_i, r_i, p_i)$, where all coordinates and radii are in micrometers ($\\mu m$) and indices are integers. The parent index $p_i = -1$ denotes the root.\n\n- Test case $1$ (straight cable):\n  - $(0, 0, 0, 0, 2, -1)$\n  - $(1, 0, 0, 10, 2, 0)$\n  - $(2, 0, 0, 20, 2, 1)$\n  - $(3, 0, 0, 30, 2, 2)$\n\n- Test case $2$ (symmetric Y-shape):\n  - $(0, 0, 0, 0, 3, -1)$\n  - $(1, 0, 0, 10, 2.5, 0)$\n  - $(2, 10, 0, 20, 2, 1)$\n  - $(3, -10, 0, 20, 2, 1)$\n\n- Test case $3$ (curved path with bifurcation):\n  - $(0, 0, 0, 0, 3, -1)$\n  - $(1, 0, 5, 5, 2.8, 0)$\n  - $(2, 0, 10, 10, 2.6, 1)$\n  - $(3, 0, 15, 10, 2.4, 2)$\n  - $(4, 4, 22, 10, 2.2, 3)$\n  - $(5, -2, 22, 10, 2.2, 3)$\n\n- Test case $4$ (long branches):\n  - $(0, 0, 0, 0, 4, -1)$\n  - $(1, 0, 0, 20, 3.5, 0)$\n  - $(2, 0, 0, 40, 3.0, 1)$\n  - $(3, 10, 0, 55, 2.5, 2)$\n  - $(4, 20, 0, 70, 2.0, 3)$\n  - $(5, -10, 0, 55, 2.5, 2)$\n  - $(6, -20, 0, 70, 2.0, 5)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the feature vector (a list of $15$ floats) for one test case, aggregated into a single top-level list. For example: \"[[f1,f2,...,f15],[g1,g2,...,g15],[h1,h2,...,h15],[k1,k2,...,k15]]\". Angles must be in radians; lengths must be in micrometers ($\\mu m$); curvature must be in radians per micrometer; taper slopes must be in micrometers per micrometer. No additional text should be printed.\n\nThe implementation must be a complete, runnable program that uses only the specified runtime environment. All numerical answers must be floats. The algorithm must be robust to boundary cases including nodes with no bifurcations, paths with constant radius, and paths too short to define curvature or slope.", "solution": "The problem proposed is valid. It is scientifically grounded in the field of computational neuroscience, specifically in the morphometric analysis of neuron reconstructions. All provided definitions, formulas, and conditions are mathematically and algorithmically sound, well-posed, and free from ambiguity or contradiction. The task is to implement a feature extraction pipeline from first principles, which is a standard and verifiable procedure.\n\nThe solution is developed through a systematic, multi-step process. First, the input data, a list of node properties, is parsed into a graph data structure representing the neuron's tree-like morphology. Subsequently, four distinct sets of morphological features are computed based on this structure: the branch angle histogram, the root-to-leaf path length distribution, statistics of local curvature, and statistics of dendritic tapering. Finally, these features are concatenated into a single feature vector of dimension $15$.\n\n**1. Data Structuring**\n\nThe initial input is a list of nodes, each defined by its index $i$, Cartesian coordinates $\\mathbf{x}_i = (x_i, y_i, z_i)$, radius $r_i$, and parent index $p_i$. To facilitate traversal and neighborhood queries, we first transform this flat list into a more structured representation. A dictionary mapping each node index to its properties (coordinates, radius, parent) provides efficient lookups. A second dictionary, representing an adjacency list, is constructed to map each parent node index to a list of its children indices. This is achieved by iterating through all nodes and, for each node $i$ with parent $p_i \\neq -1$, appending $i$ to the list of children for $p_i$. The single node with $p_i = -1$ is identified as the root of the tree.\n\n**2. Feature Derivation and Computation**\n\nThe computation of the four feature groups proceeds as follows, adhering strictly to the provided definitions.\n\n**2.1. Branch Angle Histogram**\n\nThis feature quantifies the geometry of dendritic bifurcations.\n- A branch point, or bifurcation node $b$, is defined as any node with two or more children. We identify all such nodes by inspecting the previously constructed children-map.\n- For each bifurcation node $b$, we consider every unordered pair of its children, $(c_1, c_2)$.\n- The direction of the initial segment of each child branch is given by the vector from the parent to the child: $\\mathbf{v}_{c_1} = \\mathbf{x}_{c_1} - \\mathbf{x}_b$ and $\\mathbf{v}_{c_2} = \\mathbf{x}_{c_2} - \\mathbf{x}_b$.\n- The angle $\\theta$ between these two vectors is a measure of how widely the branches splay. It is computed from the definition of the dot product:\n$$\n\\theta(b; c_1, c_2) = \\arccos\\left(\\frac{\\mathbf{v}_{c_1} \\cdot \\mathbf{v}_{c_2}}{\\|\\mathbf{v}_{c_1}\\| \\|\\mathbf{v}_{c_2}\\|}\\right)\n$$\nwhere $\\|\\cdot\\|$ denotes the Euclidean norm. The argument of the $\\arccos$ function is clipped to the range $[-1, 1]$ to prevent numerical errors from floating-point inaccuracies.\n- All such angles from all bifurcation points in the tree are collected into a single list.\n- A histogram of these angles is computed using the specified bin edges $B_\\theta = [0, \\pi/6, \\pi/3, \\pi/2, 2\\pi/3, 5\\pi/6, \\pi]$. This results in a $6$-dimensional vector of counts.\n- To make the feature independent of the neuron's size (i.e., the total number of bifurcations), the histogram is normalized by dividing each bin count by the total number of angles, ensuring the components sum to $1$. If no bifurcation points exist, a zero vector of length $6$ is returned, as per the problem specification.\n\n**2.2. Root-to-Leaf Path Length Distribution**\n\nThis feature captures the distribution of electrotonic distances from the soma (root) to the terminal tips of the dendrites.\n- A leaf node $t$ is defined as any node with zero children. We identify all such nodes.\n- For each leaf node $t$, we trace its unique path back to the root using the parent pointers. This yields the sequence of nodes $P(t) = (\\mathbf{x}_0, \\mathbf{x}_1, \\dots, \\mathbf{x}_n=t)$ comprising the path.\n- The total length of this path, $L(t)$, is the sum of the Euclidean lengths of its constituent segments:\n$$\nL(t) = \\sum_{(i \\rightarrow j) \\in P(t)} \\|\\mathbf{x}_j - \\mathbf{x}_i\\|\n$$\n- The lengths of all such root-to-leaf paths are collected.\n- A normalized histogram is computed over these path lengths using the specified bin edges $B_L = [0, 20, 40, 60, 80, 100]$, resulting in a $5$-dimensional vector. Normalization is performed by dividing by the total number of leaves. If there are no leaves, a zero vector of length $5$ is returned.\n\n**2.3. Curvature Estimates**\n\nThis feature measures the tortuosity of dendritic paths.\n- We reuse the root-to-leaf paths identified in the previous step.\n- For each path with at least $3$ nodes, $(\\mathbf{x}_0, \\mathbf{x}_1, \\dots, \\mathbf{x}_n)$, we can estimate curvature at the interior nodes $k = 1, \\dots, n-1$.\n- The discrete curvature $\\kappa_k$ at node $\\mathbf{x}_k$ is defined as the turning angle per unit of path length. The turning angle is the angle between the incoming vector $(\\mathbf{x}_k - \\mathbf{x}_{k-1})$ and the outgoing vector $(\\mathbf{x}_{k+1} - \\mathbf{x}_k)$. The path length unit is approximated by the average length of these two segments. This gives the formula:\n$$\n\\kappa_k = \\frac{\\arccos\\left(\\frac{(\\mathbf{x}_k - \\mathbf{x}_{k-1}) \\cdot (\\mathbf{x}_{k+1} - \\mathbf{x}_k)}{\\|\\mathbf{x}_k - \\mathbf{x}_{k-1}\\| \\, \\|\\mathbf{x}_{k+1} - \\mathbf{x}_k\\|}\\right)}{\\frac{1}{2}\\left(\\|\\mathbf{x}_k - \\mathbf{x}_{k-1}\\| + \\|\\mathbf{x}_{k+1} - \\mathbf{x}_k\\|\\right)}\n$$\n- A check is performed to ensure segment lengths are non-zero to avoid division by zero.\n- All valid curvature samples $\\kappa_k$ from all paths are aggregated.\n- From this collection of samples, we compute two summary statistics: the mean curvature $\\bar{\\kappa}$ and the population standard deviation $\\sigma_{\\kappa}$. If no interior points exist in the entire tree (i.e., all paths have fewer than $3$ nodes), both statistics are set to $0$.\n\n**2.4. Tapering Profiles**\n\nThis feature describes how the radius of dendritic branches changes with distance from the root, which is related to Rall's power law.\n- For each root-to-leaf path with at least $2$ nodes, we analyze the relationship between radius $r$ and cumulative path distance from the root $s$.\n- For a path with nodes $(_0, _1, ..., _n)$, the cumulative distance at node $k$ is $s_k = \\sum_{i=0}^{k-1} \\|\\mathbf{x}_{i+1} - \\mathbf{x}_i\\|$, with $s_0=0$. The corresponding radii are $\\{r_0, r_1, \\dots, r_n\\}$.\n- For each path, we compute the slope $m$ of the ordinary least squares linear regression of radius $r$ on distance $s$. The formula is given by the ratio of the covariance of $s$ and $r$ to the variance of $s$:\n$$\nm = \\frac{\\sum_{k}(s_k - \\bar{s})(r_k - \\bar{r})}{\\sum_{k}(s_k - \\bar{s})^2}\n$$\n- As specified, if the path has fewer than $2$ points or if the denominator is zero (i.e., all cumulative distances are the same, which implies zero-length segments), the slope $m$ is defined as $0$.\n- The slopes from all valid paths are collected.\n- The mean slope $\\bar{m}$ and the population standard deviation of slopes $\\sigma_m$ are computed. If there are no paths for which a slope can be computed, both statistics are set to $0$.\n\n**3. Feature Vector Assembly**\n\nThe final $15$-dimensional feature vector is constructed by concatenating the results of the above computations in the specified order:\n1.  Branch Angle Histogram (6 floats)\n2.  Path Length Histogram (5 floats)\n3.  Mean and Standard Deviation of Curvature (2 floats)\n4.  Mean and Standard Deviation of Taper Slope (2 floats)\n\nThis procedure provides a comprehensive, quantitative description of the neuron's morphology, suitable for use in machine learning models for cell type classification.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport itertools\n\ndef compute_features(nodes_data):\n    \"\"\"\n    Computes morphological features for a single neuron reconstruction.\n    \"\"\"\n    if not nodes_data:\n        return [0.0] * 15\n\n    # 1. Data Structuring\n    num_nodes = len(nodes_data)\n    nodes = {}\n    children = {i: [] for i in range(num_nodes)}\n    root_id = -1\n\n    node_list = sorted(nodes_data, key=lambda n: n[0])\n\n    for (node_id, x, y, z, r, p) in node_list:\n        nodes[node_id] = {\n            'pos': np.array([x, y, z], dtype=float),\n            'radius': float(r),\n            'parent': int(p)\n        }\n        if p != -1:\n            if p in children:\n                children[p].append(node_id)\n        else:\n            root_id = node_id\n\n    # 2. Feature Calculation\n\n    # 2.1. Branch Angle Histogram\n    branch_angles = []\n    bifurcation_node_ids = [i for i, c in children.items() if len(c) >= 2]\n    for b_id in bifurcation_node_ids:\n        b_pos = nodes[b_id]['pos']\n        for c1_id, c2_id in itertools.combinations(children[b_id], 2):\n            v1 = nodes[c1_id]['pos'] - b_pos\n            v2 = nodes[c2_id]['pos'] - b_pos\n            norm_v1 = np.linalg.norm(v1)\n            norm_v2 = np.linalg.norm(v2)\n            if norm_v1 > 0 and norm_v2 > 0:\n                cos_theta = np.dot(v1, v2) / (norm_v1 * norm_v2)\n                cos_theta = np.clip(cos_theta, -1.0, 1.0)\n                angle = np.arccos(cos_theta)\n                branch_angles.append(angle)\n\n    angle_bins = [0, np.pi/6, np.pi/3, np.pi/2, 2*np.pi/3, 5*np.pi/6, np.pi]\n    if branch_angles:\n        hist_angles, _ = np.histogram(branch_angles, bins=angle_bins)\n        hist_angles = hist_angles.astype(float) / len(branch_angles)\n    else:\n        hist_angles = np.zeros(len(angle_bins) - 1, dtype=float)\n\n    # 2.2, 2.3, 2.4: Path-based features\n    leaf_ids = [i for i in range(num_nodes) if not children[i]]\n    \n    path_lengths = []\n    curvatures = []\n    taper_slopes = []\n\n    for leaf_id in leaf_ids:\n        path = []\n        curr_id = leaf_id\n        while curr_id != -1:\n            path.append(curr_id)\n            curr_id = nodes[curr_id]['parent']\n        path.reverse()\n        \n        if not path:\n            continue\n        \n        # Path length and cumulative length calculation\n        total_length = 0.0\n        segment_lengths = []\n        if len(path) > 1:\n            for i in range(len(path) - 1):\n                p1_pos = nodes[path[i]]['pos']\n                p2_pos = nodes[path[i+1]]['pos']\n                length = np.linalg.norm(p2_pos - p1_pos)\n                segment_lengths.append(length)\n            total_length = sum(segment_lengths)\n        path_lengths.append(total_length)\n        \n        # Curvature calculation\n        if len(path) >= 3:\n            for i in range(1, len(path) - 1):\n                p_prev_pos = nodes[path[i-1]]['pos']\n                p_curr_pos = nodes[path[i]]['pos']\n                p_next_pos = nodes[path[i+1]]['pos']\n                \n                v_in = p_curr_pos - p_prev_pos\n                v_out = p_next_pos - p_curr_pos\n                \n                norm_in = np.linalg.norm(v_in)\n                norm_out = np.linalg.norm(v_out)\n                \n                if norm_in > 0 and norm_out > 0:\n                    cos_theta = np.dot(v_in, v_out) / (norm_in * norm_out)\n                    cos_theta = np.clip(cos_theta, -1.0, 1.0)\n                    angle = np.arccos(cos_theta)\n                    avg_len = (norm_in + norm_out) / 2.0\n                    curvatures.append(angle / avg_len)\n\n        # Tapering calculation\n        if len(path) >= 2:\n            s = np.concatenate(([0.0], np.cumsum(segment_lengths)))\n            r = np.array([nodes[node_id]['radius'] for node_id in path])\n            \n            s_mean = np.mean(s)\n            denominator = np.sum((s - s_mean)**2)\n            \n            if denominator > 0:\n                r_mean = np.mean(r)\n                numerator = np.sum((s - s_mean) * (r - r_mean))\n                slope = numerator / denominator\n                taper_slopes.append(slope)\n            else:\n                taper_slopes.append(0.0)\n        elif len(path) > 0:\n             # Path has only 1 node, so less than 2 points.\n             taper_slopes.append(0.0)\n\n    # Post-processing path features\n    length_bins = [0, 20, 40, 60, 80, 100]\n    if path_lengths:\n        hist_lengths, _ = np.histogram(path_lengths, bins=length_bins)\n        hist_lengths = hist_lengths.astype(float) / len(path_lengths)\n    else:\n        hist_lengths = np.zeros(len(length_bins) - 1, dtype=float)\n\n    if curvatures:\n        mean_kappa = np.mean(curvatures)\n        std_kappa = np.std(curvatures)\n    else:\n        mean_kappa, std_kappa = 0.0, 0.0\n\n    if taper_slopes:\n        mean_m = np.mean(taper_slopes)\n        std_m = np.std(taper_slopes)\n    else:\n        mean_m, std_m = 0.0, 0.0\n    \n    # 3. Assemble Feature Vector\n    feature_vector = np.concatenate([\n        hist_angles,\n        hist_lengths,\n        [mean_kappa, std_kappa],\n        [mean_m, std_m]\n    ])\n    \n    return feature_vector.tolist()\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1 (straight cable)\n        [\n            (0, 0, 0, 0, 2, -1),\n            (1, 0, 0, 10, 2, 0),\n            (2, 0, 0, 20, 2, 1),\n            (3, 0, 0, 30, 2, 2)\n        ],\n        # Test case 2 (symmetric Y-shape)\n        [\n            (0, 0, 0, 0, 3, -1),\n            (1, 0, 0, 10, 2.5, 0),\n            (2, 10, 0, 20, 2, 1),\n            (3, -10, 0, 20, 2, 1)\n        ],\n        # Test case 3 (curved path with bifurcation)\n        [\n            (0, 0, 0, 0, 3, -1),\n            (1, 0, 5, 5, 2.8, 0),\n            (2, 0, 10, 10, 2.6, 1),\n            (3, 0, 15, 10, 2.4, 2),\n            (4, 4, 22, 10, 2.2, 3),\n            (5, -2, 22, 10, 2.2, 3)\n        ],\n        # Test case 4 (long branches)\n        [\n            (0, 0, 0, 0, 4, -1),\n            (1, 0, 0, 20, 3.5, 0),\n            (2, 0, 0, 40, 3.0, 1),\n            (3, 10, 0, 55, 2.5, 2),\n            (4, 20, 0, 70, 2.0, 3),\n            (5, -10, 0, 55, 2.5, 2),\n            (6, -20, 0, 70, 2.0, 5)\n        ]\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_features(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"[{','.join(map(str, r))}]\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4004748"}, {"introduction": "After extracting quantitative features from populations of neurons, a critical scientific question emerges: can these features reliably distinguish between different cell classes? Answering this question moves us from descriptive morphology to predictive classification, a task central to modern neuroscience that requires a rigorous statistical framework. This exercise walks you through a complete statistical analysis pipeline, from fitting parametric probability distributions to your data to using powerful hypothesis tests to assess the separability between classes. By calculating the Bayes-optimal error, you will also learn to quantify the theoretical limits of a feature's predictive power, providing a robust foundation for building and evaluating classifiers. [@problem_id:4004712]", "problem": "You are given a computational neuroscience task on neuronal morphology and classification that requires reasoning from statistical principles. Consider a single positive-valued morphometric feature (for example, total dendritic length) measured in micrometers for two neuronal classes. Assume that each class’s feature distribution can be modeled by a parametric family among the following candidates: the log-normal family and the gamma family. Your program must generate synthetic observed datasets for each test case, fit parametric distributions by maximum likelihood, test goodness-of-fit using the Kolmogorov-Smirnov (KS) test, compare classes via a likelihood ratio test (LRT), and quantify the Bayes-optimal misclassification error to evaluate whether distributional differences plausibly support classifier features.\n\nStart from the following fundamental base and core definitions:\n- The log-normal distribution is defined as $X \\sim \\mathrm{LogNormal}(\\mu,\\sigma)$ if $\\ln X \\sim \\mathcal{N}(\\mu,\\sigma^2)$, with probability density function (pdf) \n$$\nf_{\\mathrm{LN}}(x \\mid \\mu,\\sigma) = \\frac{1}{x \\sigma \\sqrt{2\\pi}}\\exp\\left(-\\frac{(\\ln x - \\mu)^2}{2\\sigma^2}\\right),\\quad x>0.\n$$\n- The gamma distribution is defined as $X \\sim \\mathrm{Gamma}(k,\\theta)$ with pdf\n$$\nf_{\\Gamma}(x \\mid k,\\theta) = \\frac{1}{\\Gamma(k)\\theta^k}x^{k-1}\\exp\\left(-\\frac{x}{\\theta}\\right),\\quad x>0,\n$$\nwhere $\\Gamma(\\cdot)$ is the gamma function.\n- Maximum likelihood estimation (MLE) seeks parameters $\\hat{\\theta}$ that maximize the log-likelihood $\\ell(\\theta) = \\sum_{i=1}^{n}\\ln f(x_i \\mid \\theta)$ for observed data $\\{x_i\\}_{i=1}^{n}$.\n- The Kolmogorov-Smirnov (KS) test compares the empirical cumulative distribution function to a specified continuous distribution’s cumulative distribution function (cdf). For a one-sample KS test, the null hypothesis is that the data come from the specified distribution with fitted parameters. Given significance level $\\alpha_{\\mathrm{KS}}$, accept if the p-value is greater than or equal to $\\alpha_{\\mathrm{KS}}$.\n- The Likelihood Ratio Test (LRT) compares nested models. Let $\\ell_{0}$ denote the maximized log-likelihood under the null hypothesis $H_0$ (shared parameters across both classes) and $\\ell_{1}$ the maximized log-likelihood under the alternative hypothesis $H_1$ (separate parameters for each class). The test statistic is\n$$\n\\Lambda = 2\\big(\\ell_{1}-\\ell_{0}\\big),\n$$\nwhich, under regularity conditions and for large samples, is asymptotically $\\chi^2$-distributed with degrees of freedom equal to the difference in the number of free parameters between $H_1$ and $H_0$. Use the survival function of $\\chi^2$ to compute the p-value and compare to significance level $\\alpha_{\\mathrm{LRT}}$.\n- Under equal class priors and $0-1$ loss, the Bayes-optimal misclassification error for two continuous distributions with pdfs $f_A(x)$ and $f_B(x)$ is\n$$\nE^\\star = \\frac{1}{2}\\int_{0}^{\\infty}\\min\\big(f_A(x), f_B(x)\\big)\\,dx,\n$$\nwhich you must approximate numerically.\n\nProgram requirements:\n1. For each test case, generate the observed morphometric samples in micrometers using the specified true distribution for each class and the given random seed. The outputs you compute (p-values, decisions, and error rates) are dimensionless and must be returned as decimals.\n2. For each class and each candidate family (log-normal and gamma), fit parameters by maximum likelihood with nonnegative support enforced by fixing the location parameter to $0$.\n3. Choose a single family for subsequent testing by maximizing the total log-likelihood under separate fits across both classes. That is, compute the sum of maximized log-likelihoods for class $A$ and class $B$ under the log-normal fit, and the same under the gamma fit; select the family with the larger sum.\n4. Goodness-of-fit: For the chosen family, perform a one-sample KS test for each class using the fitted parameters. A class “passes” goodness-of-fit if its p-value is greater than or equal to $\\alpha_{\\mathrm{KS}}$. The goodness-of-fit result for a test case is a boolean indicating whether both classes pass.\n5. Likelihood ratio test: Using the chosen family, compute $\\ell_0$ by fitting a single parameter set to the pooled data across both classes; compute $\\ell_1$ by fitting parameters separately to each class. Compute the LRT statistic $\\Lambda = 2(\\ell_1 - \\ell_0)$ and its p-value using the $\\chi^2$ distribution with degrees of freedom equal to $2$ (both the log-normal and gamma families have $2$ parameters when the location is fixed). The LRT decision is a boolean indicating whether the p-value is strictly less than $\\alpha_{\\mathrm{LRT}}$.\n6. Bayes-optimal error: Using the chosen family with separate fitted parameters for the two classes, approximate $E^\\star$ via numeric integration of $\\min(f_A(x),f_B(x))$ over $x \\in [0, U]$, where $U$ is the maximum of the $0.999$-quantiles of the two fitted distributions. Use a composite trapezoidal rule with $M$ grid points. Set $M = 10000$. Report $E^\\star$ rounded to $4$ decimal places.\n7. Angle units are not involved in this problem. Physical units apply to the generated morphometric data and must be treated as micrometers, but all outputs are dimensionless decimals.\n8. Your program must aggregate the results of all test cases into a single line of output containing a list of lists. Each inner list corresponds to one test case and must be of the form $[\\text{GoF\\_pass}, \\text{LRT\\_significant}, \\text{Bayes\\_error}]$, where $\\text{GoF\\_pass}$ and $\\text{LRT\\_significant}$ are booleans and $\\text{Bayes\\_error}$ is a float rounded to $4$ decimals. The final printed line must contain this list-of-lists in standard Python list syntax, for example, $[[\\text{True},\\text{False},0.1234],[\\dots]]$.\n\nUse the following test suite. Each case specifies the true data-generating distribution for class $A$ and class $B$ with parameters and sample sizes. All seeds are integers. For log-normal data generation, use the parameterization with underlying normal mean $\\mu$ and standard deviation $\\sigma$; for gamma data generation, use shape $k$ and scale $\\theta$.\n\n- Test case $1$ (happy path, distinct gamma classes):\n  - Class $A$: gamma with $k=4$, $\\theta=25$, $n_A=80$.\n  - Class $B$: gamma with $k=8$, $\\theta=20$, $n_B=80$.\n  - Seed $s=123$.\n\n- Test case $2$ (boundary condition, identical log-normal classes):\n  - Class $A$: log-normal with $\\mu=4.5$, $\\sigma=0.35$, $n_A=100$.\n  - Class $B$: log-normal with $\\mu=4.5$, $\\sigma=0.35$, $n_B=100$.\n  - Seed $s=456$.\n\n- Test case $3$ (mixed underlying families, moderate sample sizes):\n  - Class $A$: gamma with $k=3$, $\\theta=30$, $n_A=70$.\n  - Class $B$: log-normal with $\\mu=4.0$, $\\sigma=0.5$, $n_B=70$.\n  - Seed $s=321$.\n\n- Test case $4$ (edge case, small sample sizes with mildly different gamma parameters):\n  - Class $A$: gamma with $k=2.5$, $\\theta=40$, $n_A=15$.\n  - Class $B$: gamma with $k=2.9$, $\\theta=38$, $n_B=15$.\n  - Seed $s=789$.\n\nUse significance levels $\\alpha_{\\mathrm{KS}}=0.05$ and $\\alpha_{\\mathrm{LRT}}=0.05$, both expressed as decimals. For numeric integration, use $M=10000$ grid points. Your program should produce a single line of output containing the results as a list-of-lists in standard Python list syntax, where each inner list is $[\\text{GoF\\_pass}, \\text{LRT\\_significant}, \\text{Bayes\\_error}]$.", "solution": "The problem presents a well-defined computational task in statistical analysis, relevant to the field of computational neuroscience, specifically concerning the classification of neurons based on morphometric features. The problem is scientifically grounded, self-contained, and algorithmically specified. It requires the implementation of a standard statistical pipeline: data generation, parametric model fitting via maximum likelihood estimation, model selection, hypothesis testing for goodness-of-fit and class separability, and the quantification of classification error. All aspects of the problem are objective and mathematically formalizable. We therefore proceed with the solution.\n\nThe methodological approach involves several distinct but sequential steps for each test case provided.\n\n**1. Data Synthesis**\nFor each test case, we begin by generating synthetic datasets for two neuronal classes, denoted as class $A$ and class $B$. The data represent a single positive-valued morphometric measurement, such as total dendritic length, measured in micrometers. The datasets for class $A$ and class $B$ are drawn from specified true distributions—either log-normal or gamma—with given parameters and sample sizes ($n_A$, $n_B$). To ensure reproducibility, a specific random seed is used for each test case. The generation is performed using a pseudo-random number generator.\n- For a log-normal distribution, $\\mathrm{LogNormal}(\\mu, \\sigma)$, we generate samples where the natural logarithm of the values follows a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$.\n- For a gamma distribution, $\\mathrm{Gamma}(k, \\theta)$, we generate samples according to the shape-scale parameterization with shape $k$ and scale $\\theta$.\n\n**2. Parametric Model Fitting and Selection**\nFor each generated dataset (e.g., $\\{x_i\\}_{i=1}^{n_A}$ for class $A$), we fit two candidate parametric models: the log-normal distribution and the gamma distribution. The fitting is performed using the method of Maximum Likelihood Estimation (MLE). The MLE procedure finds the parameter set $\\hat{\\boldsymbol{\\theta}}$ that maximizes the log-likelihood function, $\\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^{n} \\ln f(x_i \\mid \\boldsymbol{\\theta})$, where $f(x \\mid \\boldsymbol{\\theta})$ is the probability density function (PDF) of the candidate model. As morphometric data are strictly positive, we enforce this by fixing the location parameter of both distributions to $0$.\n\nAfter fitting both models to the data from class $A$ and class $B$ separately, we must select a single distribution family for all subsequent analyses in the given test case. The selection criterion is based on the total log-likelihood. We calculate the sum of the maximized log-likelihoods for the separate class fits under the log-normal assumption ($\\ell_{\\mathrm{LN},A} + \\ell_{\\mathrm{LN},B}$) and the gamma assumption ($\\ell_{\\Gamma,A} + \\ell_{\\Gamma,B}$). The family with the larger total log-likelihood is chosen as it provides a better overall description of the observed data.\n\n**3. Goodness-of-Fit (GoF) Assessment**\nOnce a distribution family is selected, we must validate whether it is a plausible model for the data from each class. This is assessed using the one-sample Kolmogorov-Smirnov (KS) test. For each class, the KS test compares the empirical cumulative distribution function (ECDF) of the data, $F_n(x)$, with the cumulative distribution function (CDF) of the fitted model, $F(x; \\hat{\\boldsymbol{\\theta}})$. The null hypothesis, $H_0$, is that the data are drawn from the fitted distribution. The test statistic is $D_n = \\sup_x |F_n(x) - F(x; \\hat{\\boldsymbol{\\theta}})|$. We compute the p-value associated with this statistic. A class is considered to \"pass\" the GoF test if its p-value is greater than or equal to the specified significance level $\\alpha_{\\mathrm{KS}} = 0.05$. The overall GoF result for the test case is a boolean value indicating whether both classes passed the test.\n\n**4. Likelihood Ratio Test (LRT) for Class Separability**\nTo determine if the morphometric feature provides a statistically significant basis for distinguishing between class $A$ and class $B$, we perform a Likelihood Ratio Test (LRT). This test compares two nested models under the chosen distribution family:\n- Null Hypothesis ($H_0$): The data from both classes are drawn from the same distribution. A single set of parameters is fit to the pooled dataset $\\{x_{A,i}\\} \\cup \\{x_{B,j}\\}$. Let the resulting maximized log-likelihood be $\\ell_0$.\n- Alternative Hypothesis ($H_1$): The data from the two classes are drawn from distributions with different parameters. Parameters are fit separately to the data for class $A$ and class $B$. The maximized log-likelihood is the sum of the individual log-likelihoods, $\\ell_1 = \\ell_A + \\ell_B$, which was already computed during the model selection step.\n\nThe LRT statistic is $\\Lambda = 2(\\ell_1 - \\ell_0)$. Under $H_0$, for large sample sizes, $\\Lambda$ follows a chi-squared ($\\chi^2$) distribution. The degrees of freedom for the $\\chi^2$ distribution equal the difference in the number of free parameters between $H_1$ and $H_0$. Both the log-normal ($\\mu, \\sigma$) and gamma ($k, \\theta$) families have $2$ parameters. Thus, $H_1$ has $2+2=4$ parameters, and $H_0$ has $2$ parameters. The degrees of freedom are $d = 4 - 2 = 2$. We compute the p-value as the survival function of the $\\chi^2_2$ distribution evaluated at $\\Lambda$. The null hypothesis is rejected if the p-value is strictly less than the significance level $\\alpha_{\\mathrm{LRT}} = 0.05$. The result is a boolean indicating a significant difference.\n\n**5. Bayes-Optimal Misclassification Error**\nFinally, we quantify the theoretical limit of classification performance using the single morphometric feature. Assuming equal prior probabilities for the two classes ($P(A)=P(B)=0.5$) and a $0-1$ loss function, the minimum achievable misclassification error is the Bayes-optimal error, $E^\\star$. It is given by the integral:\n$$\nE^\\star = \\frac{1}{2} \\int_{0}^{\\infty} \\min\\big(f_A(x), f_B(x)\\big) \\,dx\n$$\nwhere $f_A(x)$ and $f_B(x)$ are the PDFs of the chosen family with the parameters fitted separately to class $A$ and class $B$ data, respectively.\n\nThis integral is computed numerically. The integration domain is truncated from $[0, \\infty)$ to $[0, U]$, where $U$ is chosen to capture the vast majority of the probability mass. We set $U$ as the maximum of the $0.999$-quantiles of the two fitted distributions, $f_A(x)$ and $f_B(x)$. The integral is then approximated using the composite trapezoidal rule with $M = 10000$ grid points, which divides the interval $[0, U]$ into $M-1$ small trapezoids and sums their areas. The final error $E^\\star$ is reported to $4$ decimal places. This value represents the expected proportion of misclassified samples by an ideal classifier.\n\nThe entire procedure is implemented in Python, using the `numpy` library for numerical computation and data generation, and the `scipy.stats` module for statistical functions, including distribution fitting (MLE), KS tests, and PDF/CDF/PPF evaluations.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import lognorm, gamma, kstest, chi2\nfrom scipy.special import gammaln\n\ndef solve():\n    \"\"\"\n    Main function to run the statistical analysis pipeline for all test cases.\n    \"\"\"\n    test_cases = [\n        # (true_dist_A, params_A, n_A, true_dist_B, params_B, n_B, seed)\n        ('gamma', {'k': 4, 'theta': 25}, 80, 'gamma', {'k': 8, 'theta': 20}, 80, 123),\n        ('lognorm', {'mu': 4.5, 'sigma': 0.35}, 100, 'lognorm', {'mu': 4.5, 'sigma': 0.35}, 100, 456),\n        ('gamma', {'k': 3, 'theta': 30}, 70, 'lognorm', {'mu': 4.0, 'sigma': 0.5}, 70, 321),\n        ('gamma', {'k': 2.5, 'theta': 40}, 15, 'gamma', {'k': 2.9, 'theta': 38}, 15, 789),\n    ]\n\n    alpha_ks = 0.05\n    alpha_lrt = 0.05\n    integration_points = 10000\n    all_results = []\n\n    for case in test_cases:\n        dist_A, params_A, n_A, dist_B, params_B, n_B, seed = case\n        rng = np.random.default_rng(seed)\n\n        # 1. Generate synthetic data\n        if dist_A == 'lognorm':\n            data_A = rng.lognormal(mean=params_A['mu'], sigma=params_A['sigma'], size=n_A)\n        else: # gamma\n            data_A = rng.gamma(shape=params_A['k'], scale=params_A['theta'], size=n_A)\n\n        if dist_B == 'lognorm':\n            data_B = rng.lognormal(mean=params_B['mu'], sigma=params_B['sigma'], size=n_B)\n        else: # gamma\n            data_B = rng.gamma(shape=params_B['k'], scale=params_B['theta'], size=n_B)\n\n        # 2. Fit parametric distributions by MLE\n        # Fit Class A\n        ln_params_A = lognorm.fit(data_A, floc=0)\n        g_params_A = gamma.fit(data_A, floc=0)\n        \n        # Fit Class B\n        ln_params_B = lognorm.fit(data_B, floc=0)\n        g_params_B = gamma.fit(data_B, floc=0)\n\n        # 3. Choose family by maximizing total log-likelihood\n        ll_ln_A = lognorm.logpdf(data_A, *ln_params_A).sum()\n        ll_ln_B = lognorm.logpdf(data_B, *ln_params_B).sum()\n        total_ll_ln = ll_ln_A + ll_ln_B\n\n        ll_g_A = gamma.logpdf(data_A, *g_params_A).sum()\n        ll_g_B = gamma.logpdf(data_B, *g_params_B).sum()\n        total_ll_g = ll_g_A + ll_g_B\n\n        if total_ll_ln > total_ll_g:\n            chosen_family_dist = lognorm\n            chosen_family_name = 'lognorm'\n            params_A_fit = ln_params_A\n            params_B_fit = ln_params_B\n            ell_1 = total_ll_ln\n        else:\n            chosen_family_dist = gamma\n            chosen_family_name = 'gamma'\n            params_A_fit = g_params_A\n            params_B_fit = g_params_B\n            ell_1 = total_ll_g\n\n        # 4. Goodness-of-fit test\n        ks_result_A = kstest(data_A, chosen_family_name, args=params_A_fit)\n        ks_result_B = kstest(data_B, chosen_family_name, args=params_B_fit)\n        \n        gof_pass = (ks_result_A.pvalue >= alpha_ks) and (ks_result_B.pvalue >= alpha_ks)\n\n        # 5. Likelihood ratio test\n        data_pooled = np.concatenate([data_A, data_B])\n        params_pooled = chosen_family_dist.fit(data_pooled, floc=0)\n        ell_0 = chosen_family_dist.logpdf(data_pooled, *params_pooled).sum()\n        \n        lrt_statistic = 2 * (ell_1 - ell_0)\n        lrt_pvalue = chi2.sf(lrt_statistic, df=2)\n        lrt_significant = lrt_pvalue < alpha_lrt\n\n        # 6. Bayes-optimal error\n        q999_A = chosen_family_dist.ppf(0.999, *params_A_fit)\n        q999_B = chosen_family_dist.ppf(0.999, *params_B_fit)\n        upper_bound = max(q999_A, q999_B, np.max(data_pooled) * 1.1)\n\n        x_grid = np.linspace(1e-9, upper_bound, integration_points)\n        \n        pdf_A = chosen_family_dist.pdf(x_grid, *params_A_fit)\n        pdf_B = chosen_family_dist.pdf(x_grid, *params_B_fit)\n        \n        integrand = np.minimum(pdf_A, pdf_B)\n        integral_val = np.trapz(integrand, x_grid)\n        bayes_error = 0.5 * integral_val\n\n        # 7. Aggregate results\n        all_results.append([\n            gof_pass,\n            lrt_significant,\n            round(bayes_error, 4)\n        ])\n\n    # Final print statement\n    # To conform to standard Python syntax output, convert booleans to True/False strings\n    # and then format the list of lists.\n    output_str = '['\n    for i, res in enumerate(all_results):\n        # Format: [True,False,0.1234]\n        output_str += f\"[{str(res[0])},{str(res[1])},{res[2]:.4f}]\"\n        if i < len(all_results) - 1:\n            output_str += ','\n    output_str += ']'\n    \n    print(output_str)\n\nsolve()\n```", "id": "4004712"}]}