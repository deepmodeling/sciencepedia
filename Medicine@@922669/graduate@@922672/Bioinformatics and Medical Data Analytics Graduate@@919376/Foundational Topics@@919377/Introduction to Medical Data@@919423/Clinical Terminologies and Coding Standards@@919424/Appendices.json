{"hands_on_practices": [{"introduction": "The process of assigning a clinical code from a terminology like the International Classification of Diseases (ICD) is governed by strict guidelines that prioritize accuracy and specificity. This exercise challenges you to translate these qualitative rules into a quantitative, deterministic algorithm. By formalizing criteria such as specificity, completeness, and preference for combination codes, you will develop a computational model that mimics the decision-making process of a professional coder, a foundational skill for building automated coding and data validation systems.", "problem": "You are given a simplified, fully specified abstraction of the International Classification of Diseases (ICD) as a hierarchical terminology in which codes are represented as elements associated with required documentation features and attributes indicating their level of specificity. You must implement a program that selects the single most appropriate ICD code index for each documentation scenario, adhering to ICD coding guidelines on specificity. The selection must be framed purely in mathematical and logical terms and must not rely on external textual interpretation.\n\nFundamental base for derivation: hierarchical clinical terminologies encode parent-child relationships that define increasing specificity; selection guidelines require coding to the highest level of specificity supported by documentation; combination codes must be preferred when the documented conditions are captured by a single code.\n\nDefinitions and formalization:\n- Let $C = \\{c_0, c_1, \\dots, c_{n-1}\\}$ be a finite set of codes. Each code $c_i$ is associated with:\n  1. A set of required features $R(c_i) \\subseteq \\mathcal{F}$, where $\\mathcal{F}$ is the universe of documentation features.\n  2. A nonnegative integer depth $d(c_i) \\in \\mathbb{N}$ representing hierarchical specificity (greater $d(c_i)$ implies greater specificity).\n  3. A boolean unspecified flag $u(c_i) \\in \\{0,1\\}$, where $u(c_i) = 1$ denotes that the code contains an unspecified qualifier.\n  4. A nonnegative severity weight $s(c_i) \\in \\mathbb{R}$ providing a secondary tie-break measure aligned with clinical significance.\n  5. An index $i \\in \\{0,1,\\dots,n-1\\}$ indicating the code’s position in the list.\n- Let the documentation for a case be represented as a finite set $D \\subseteq \\mathcal{F}$.\n- A code $c_i$ is admissible for $D$ if and only if $R(c_i) \\subseteq D$.\n- The coverage cardinality of $c_i$ with respect to $D$ is $k(c_i, D) = |R(c_i)|$ whenever $c_i$ is admissible, and undefined otherwise.\n\nSelection principle derived from ICD guidelines:\n- Among admissible codes, prefer those that capture the largest number of documented conditions directly as required features (combination codes are reflected by larger $|R(c_i)|$).\n- Among those, prefer the code with maximal hierarchical specificity $d(c_i)$.\n- Among those, prefer codes without unspecified qualifiers, i.e., with minimal $u(c_i)$.\n- Among those, prefer larger clinical severity $s(c_i)$.\n- If all criteria tie, select the smallest index $i$ to ensure determinism.\n\nFormally, for a given documentation $D$, define the admissible set $A(D) = \\{c_i \\in C \\mid R(c_i) \\subseteq D\\}$. For each $c_i \\in A(D)$ define the ranking tuple\n$$\n\\mathbf{r}(c_i, D) = \\left(k(c_i, D), \\; d(c_i), \\; 1 - u(c_i), \\; s(c_i), \\; -i\\right)\n$$\nand select $c^\\ast = \\arg\\max_{c_i \\in A(D)} \\mathbf{r}(c_i, D)$ with lexicographic comparison of tuples. If $A(D) = \\varnothing$, the selection is undefined; for this problem, all test scenarios are constructed so that $A(D) \\neq \\varnothing$.\n\nCode catalog for this problem (each $c_i$ is listed with its index $i$, textual label, required feature set $R(c_i)$, depth $d(c_i)$, unspecified flag $u(c_i)$, and severity $s(c_i)$):\n- $i = 0$: label \"E11.9\", $R(c_0) = \\{\\text{diabetes}, \\text{type2}\\}$, $d(c_0) = 2$, $u(c_0) = 0$, $s(c_0) = 1.0$.\n- $i = 1$: label \"E11.40\", $R(c_1) = \\{\\text{diabetes}, \\text{type2}, \\text{neuropathy}\\}$, $d(c_1) = 3$, $u(c_1) = 1$, $s(c_1) = 2.0$.\n- $i = 2$: label \"E11.42\", $R(c_2) = \\{\\text{diabetes}, \\text{type2}, \\text{polyneuropathy}\\}$, $d(c_2) = 3$, $u(c_2) = 0$, $s(c_2) = 3.0$.\n- $i = 3$: label \"E10.9\", $R(c_3) = \\{\\text{diabetes}, \\text{type1}\\}$, $d(c_3) = 2$, $u(c_3) = 0$, $s(c_3) = 1.0$.\n- $i = 4$: label \"S52.521A\", $R(c_4) = \\{\\text{fracture}, \\text{radius}, \\text{left}, \\text{initial}, \\text{closed}\\}$, $d(c_4) = 4$, $u(c_4) = 0$, $s(c_4) = 2.5$.\n- $i = 5$: label \"S52.522A\", $R(c_5) = \\{\\text{fracture}, \\text{radius}, \\text{right}, \\text{initial}, \\text{closed}\\}$, $d(c_5) = 4$, $u(c_5) = 0$, $s(c_5) = 2.5$.\n- $i = 6$: label \"S52.529A\", $R(c_6) = \\{\\text{fracture}, \\text{radius}, \\text{initial}, \\text{closed}\\}$, $d(c_6) = 3$, $u(c_6) = 1$, $s(c_6) = 2.0$.\n- $i = 7$: label \"I10\", $R(c_7) = \\{\\text{hypertension}\\}$, $d(c_7) = 2$, $u(c_7) = 0$, $s(c_7) = 1.0$.\n- $i = 8$: label \"I50.9\", $R(c_8) = \\{\\text{heart\\_failure}\\}$, $d(c_8) = 2$, $u(c_8) = 1$, $s(c_8) = 2.0$.\n- $i = 9$: label \"I11.0\", $R(c_9) = \\{\\text{hypertension}, \\text{heart\\_failure}\\}$, $d(c_9) = 3$, $u(c_9) = 0$, $s(c_9) = 3.5$.\n- $i = 10$: label \"S52.521D\", $R(c_{10}) = \\{\\text{fracture}, \\text{radius}, \\text{left}, \\text{subsequent}\\}$, $d(c_{10}) = 4$, $u(c_{10}) = 0$, $s(c_{10}) = 2.5$.\n- $i = 11$: label \"S52.521A\\_open\", $R(c_{11}) = \\{\\text{fracture}, \\text{radius}, \\text{left}, \\text{initial}, \\text{open}\\}$, $d(c_{11}) = 4$, $u(c_{11}) = 0$, $s(c_{11}) = 2.7$.\n\nTest suite:\n- Case $1$: $D_1 = \\{\\text{diabetes}, \\text{type2}, \\text{polyneuropathy}\\}$.\n- Case $2$: $D_2 = \\{\\text{fracture}, \\text{radius}, \\text{left}, \\text{initial}, \\text{closed}\\}$.\n- Case $3$: $D_3 = \\{\\text{fracture}, \\text{radius}, \\text{initial}, \\text{closed}\\}$.\n- Case $4$: $D_4 = \\{\\text{hypertension}, \\text{heart\\_failure}\\}$.\n- Case $5$: $D_5 = \\{\\text{diabetes}, \\text{type1}\\}$.\n- Case $6$: $D_6 = \\{\\text{hypertension}\\}$.\n- Case $7$: $D_7 = \\{\\text{fracture}, \\text{radius}, \\text{left}, \\text{subsequent}\\}$.\n- Case $8$: $D_8 = \\{\\text{diabetes}, \\text{type2}, \\text{neuropathy}\\}$.\n- Case $9$: $D_9 = \\{\\text{fracture}, \\text{radius}, \\text{left}, \\text{initial}, \\text{open}\\}$.\n\nYour task:\n- Implement a complete, runnable program that, for each case $D_j$, computes the admissible set $A(D_j)$, applies the lexicographic ranking $\\mathbf{r}(c_i, D_j)$ as defined above, and returns the selected code index $i^\\ast$.\n- All outputs must be integers. There are no physical units and no angles involved.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list of selected indices, enclosed in square brackets (e.g., $[\\text{result}_1,\\text{result}_2,\\dots,\\text{result}_9]$). The program must be self-contained and must not require any input.\n\nDesign for coverage:\n- Case $1$ exercises selection among complication-specific diabetes codes.\n- Case $2$ exercises full specificity with laterality and encounter.\n- Case $3$ exercises fallback to less specific code due to missing laterality.\n- Case $4$ exercises preference for combination codes over separate single-condition codes.\n- Case $5$ and Case $6$ exercise single-condition selections.\n- Case $7$ exercises subsequent encounter.\n- Case $8$ contrasts specified versus unspecified neuropathy within type $2$ diabetes.\n- Case $9$ distinguishes open versus closed fracture at initial encounter.", "solution": "The problem requires the implementation of a deterministic algorithm to select the most appropriate clinical code from a given catalog based on a set of documented features. The selection process is governed by a formalized hierarchy of rules derived from standard clinical coding guidelines, primarily concerning specificity and completeness. The problem is well-defined, mathematically self-contained, and algorithmically solvable.\n\nThe core of the problem lies in translating the provided selection principles into a computational procedure. We are given a code catalog $C = \\{c_0, c_1, \\dots, c_{11}\\}$, where each code $c_i$ is characterized by a quintuple of properties: its index $i$, a set of required features $R(c_i)$, a hierarchical depth $d(c_i)$, an unspecified flag $u(c_i)$, and a severity weight $s(c_i)$. We are also given a series of documentation scenarios, each represented as a set of features $D_j$.\n\nThe selection algorithm for a given documentation set $D$ proceeds in two main stages: filtering and ranking.\n\nFirst, we identify the set of admissible codes, denoted as $A(D)$. A code $c_i$ is defined as admissible if and only if all its required features are present in the documentation. This is a set-theoretic subset condition:\n$$\nA(D) = \\{c_i \\in C \\mid R(c_i) \\subseteq D\\}\n$$\nThis step ensures that we only consider codes that are fully supported by the available documentation.\n\nSecond, for every admissible code $c_i \\in A(D)$, we must evaluate its appropriateness according to the specified multi-level criteria. These criteria are encapsulated in a ranking tuple $\\mathbf{r}(c_i, D)$. The problem defines this tuple as:\n$$\n\\mathbf{r}(c_i, D) = \\left( k(c_i, D), \\; d(c_i), \\; 1 - u(c_i), \\; s(c_i), \\; -i \\right)\n$$\nwhere $k(c_i, D) = |R(c_i)|$ is the coverage cardinality. The optimal code $c^\\ast$ is the one that maximizes this tuple under lexicographical comparison. Let's analyze the components of the tuple in their order of precedence:\n\n1.  $k(c_i, D) = |R(c_i)|$: This is the primary criterion. Maximizing the size of the required feature set gives preference to \"combination codes\" that capture multiple documented conditions in a single code, which is a fundamental guideline.\n2.  $d(c_i)$: This is the second criterion, applied to break ties from the first. Maximizing the depth $d(c_i)$ corresponds to selecting the most specific code in the hierarchy.\n3.  $1 - u(c_i)$: This is the third criterion. The unspecified flag $u(c_i)$ is $1$ for unspecified codes and $0$ for specified ones. To prefer specified codes, we must minimize $u(c_i)$. Maximizing the transformed value $1 - u(c_i)$ achieves this, as $1 - 0 = 1$ is greater than $1 - 1 = 0$.\n4.  $s(c_i)$: The fourth criterion maximizes the severity weight $s(c_i)$, providing a clinically relevant tie-breaker when all preceding criteria are equal.\n5.  $-i$: This is the final, deterministic tie-breaker. To choose the code with the smallest index $i$ in case of a complete tie, we maximize the value $-i$.\n\nThe selected code index $i^\\ast$ is therefore given by:\n$$\ni^\\ast = \\text{index of } \\arg\\max_{c_i \\in A(D)} \\mathbf{r}(c_i, D)\n$$\n\nLet us apply this procedure to Case 4, where the documentation is $D_4 = \\{\\text{hypertension}, \\text{heart\\_failure}\\}$.\n\nFirst, we determine the admissible set $A(D_4)$. We check each code in the catalog:\n- $c_7$: $R(c_7) = \\{\\text{hypertension}\\} \\subseteq D_4$. Admissible.\n- $c_8$: $R(c_8) = \\{\\text{heart\\_failure}\\} \\subseteq D_4$. Admissible.\n- $c_9$: $R(c_9) = \\{\\text{hypertension}, \\text{heart\\_failure}\\} \\subseteq D_4$. Admissible.\nAll other codes contain required features not present in $D_4$, so $A(D_4) = \\{c_7, c_8, c_9\\}$.\n\nNext, we compute the ranking tuple for each admissible code:\n- For $c_7$ ($i=7$): $k=1$, $d=2$, $u=0$, $s=1.0$. The tuple is $\\mathbf{r}(c_7, D_4) = (1, 2, 1-0, 1.0, -7) = (1, 2, 1, 1.0, -7)$.\n- For $c_8$ ($i=8$): $k=1$, $d=2$, $u=1$, $s=2.0$. The tuple is $\\mathbf{r}(c_8, D_4) = (1, 2, 1-1, 2.0, -8) = (1, 2, 0, 2.0, -8)$.\n- For $c_9$ ($i=9$): $k=2$, $d=3$, $u=0$, $s=3.5$. The tuple is $\\mathbf{r}(c_9, D_4) = (2, 3, 1-0, 3.5, -9) = (2, 3, 1, 3.5, -9)$.\n\nFinally, we perform a lexicographical comparison of the tuples:\n- $\\mathbf{r}(c_9, D_4)$ vs. $\\mathbf{r}(c_7, D_4)$: The first component of $\\mathbf{r}(c_9, D_4)$ is $2$, while for $\\mathbf{r}(c_7, D_4)$ it is $1$. Since $2 > 1$, $\\mathbf{r}(c_9, D_4)$ is greater.\n- $\\mathbf{r}(c_9, D_4)$ vs. $\\mathbf{r}(c_8, D_4)$: The first component of $\\mathbf{r}(c_9, D_4)$ is $2$, while for $\\mathbf{r}(c_8, D_4)$ it is $1$. Since $2 > 1$, $\\mathbf{r}(c_9, D_4)$ is greater.\n\nThe combination code $c_9$ is selected because its coverage cardinality $k=2$ is the highest, reflecting its ability to capture both documented conditions. The comparison stops at the first criterion, and the selected index is $9$.\n\nThis logic is implemented for each test case to produce the final sequence of results.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the ICD code selection problem by implementing the specified\n    ranking and selection algorithm.\n    \"\"\"\n    \n    # Define the code catalog as a list of dictionaries.\n    # R(c_i) is stored as a set for efficient subset checking.\n    code_catalog = [\n        {'i': 0, 'label': 'E11.9', 'R': {'diabetes', 'type2'}, 'd': 2, 'u': 0, 's': 1.0},\n        {'i': 1, 'label': 'E11.40', 'R': {'diabetes', 'type2', 'neuropathy'}, 'd': 3, 'u': 1, 's': 2.0},\n        {'i': 2, 'label': 'E11.42', 'R': {'diabetes', 'type2', 'polyneuropathy'}, 'd': 3, 'u': 0, 's': 3.0},\n        {'i': 3, 'label': 'E10.9', 'R': {'diabetes', 'type1'}, 'd': 2, 'u': 0, 's': 1.0},\n        {'i': 4, 'label': 'S52.521A', 'R': {'fracture', 'radius', 'left', 'initial', 'closed'}, 'd': 4, 'u': 0, 's': 2.5},\n        {'i': 5, 'label': 'S52.522A', 'R': {'fracture', 'radius', 'right', 'initial', 'closed'}, 'd': 4, 'u': 0, 's': 2.5},\n        {'i': 6, 'label': 'S52.529A', 'R': {'fracture', 'radius', 'initial', 'closed'}, 'd': 3, 'u': 1, 's': 2.0},\n        {'i': 7, 'label': 'I10', 'R': {'hypertension'}, 'd': 2, 'u': 0, 's': 1.0},\n        {'i': 8, 'label': 'I50.9', 'R': {'heart_failure'}, 'd': 2, 'u': 1, 's': 2.0},\n        {'i': 9, 'label': 'I11.0', 'R': {'hypertension', 'heart_failure'}, 'd': 3, 'u': 0, 's': 3.5},\n        {'i': 10, 'label': 'S52.521D', 'R': {'fracture', 'radius', 'left', 'subsequent'}, 'd': 4, 'u': 0, 's': 2.5},\n        {'i': 11, 'label': 'S52.521A_open', 'R': {'fracture', 'radius', 'left', 'initial', 'open'}, 'd': 4, 'u': 0, 's': 2.7},\n    ]\n\n    # Define the test suite documentation sets.\n    test_cases = [\n        {'diabetes', 'type2', 'polyneuropathy'}, # Case 1\n        {'fracture', 'radius', 'left', 'initial', 'closed'}, # Case 2\n        {'fracture', 'radius', 'initial', 'closed'}, # Case 3\n        {'hypertension', 'heart_failure'}, # Case 4\n        {'diabetes', 'type1'}, # Case 5\n        {'hypertension'}, # Case 6\n        {'fracture', 'radius', 'left', 'subsequent'}, # Case 7\n        {'diabetes', 'type2', 'neuropathy'}, # Case 8\n        {'fracture', 'radius', 'left', 'initial', 'open'}, # Case 9\n    ]\n\n    results = []\n    for documentation_set in test_cases:\n        admissible_codes = []\n        # Step 1: Find all admissible codes\n        for code in code_catalog:\n            if code['R'].issubset(documentation_set):\n                admissible_codes.append(code)\n\n        # Step 2: For each admissible code, compute its ranking tuple\n        ranked_candidates = []\n        for code in admissible_codes:\n            k = len(code['R'])\n            d = code['d']\n            u_transformed = 1 - code['u']\n            s = code['s']\n            neg_i = -code['i']\n            \n            ranking_tuple = (k, d, u_transformed, s, neg_i)\n            ranked_candidates.append((ranking_tuple, code['i']))\n        \n        # Step 3: Select the code with the lexicographically largest ranking tuple\n        # The max() function on a list of tuples performs this comparison automatically.\n        if ranked_candidates:\n            best_candidate = max(ranked_candidates, key=lambda item: item[0])\n            selected_index = best_candidate[1]\n            results.append(selected_index)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4548271"}, {"introduction": "Beyond assigning individual codes, clinical terminologies are essential tools for defining complex patient populations for research, quality measurement, and clinical trials. SNOMED CT's Expression Constraint Language (ECL) provides a powerful, standardized grammar for constructing these \"computable phenotypes.\" In this practice, you will dissect an ECL query, translating its logical operators into fundamental set theory to calculate the precise size of the resulting concept set, a key skill for designing robust phenotype algorithms.", "problem": "A research team is designing a computable phenotype pipeline that queries Systematized Nomenclature of Medicine—Clinical Terms (SNOMED CT) using the SNOMED CT Expression Constraint Language (ECL). The team wishes to identify inflammatory clinical findings of the lung that specify a causative agent and to exclude pneumonia. The query uses the descendant-or-self operator and attribute-group semantics. Although mappings to International Classification of Diseases (ICD), Current Procedural Terminology (CPT), and Logical Observation Identifiers Names and Codes (LOINC) are often considered for interoperability, they are not part of this computation.\n\nAssume the following ECL is used (with attribute grouping so that all attributes in the braces occur within the same role group):\n\n```\n 404684003 |Clinical finding| : { 363698007 |Finding site| =  39607008 |Lung structure|, 116676008 |Associated morphology| =  409774005 |Inflammatory morphology|, 246075003 |Causative agent| = (  49872002 |Virus| OR  410607006 |Bacteria| ) } MINUS  233604007 |Pneumonia|\n```\n\nInterpret the operators by their standard SNOMED CT ECL semantics: $$ returns the set containing a concept and all of its descendants; attribute grouping requires that the listed attributes co-occur within at least one common role group in the concept definition; and MINUS denotes set difference.\n\nYou are given the following consistent counts from the hospital’s SNOMED CT module for the subset of concepts that are descendants-or-self of $404684003$ $|$Clinical finding$|$ and that satisfy the grouped lung and inflammatory attributes as specified above:\n\n- Let $\\mathcal{L}$ be the set of concepts that satisfy the grouped constraints $|$Finding site$|$ $=$ $$ $39607008$ and $|$Associated morphology$|$ $=$ $$ $409774005$. Then $|\\mathcal{L}|$ $=$ $2600$.\n- Let $\\mathcal{M}$ be the subset of $\\mathcal{L}$ that also have $|$Causative agent$|$ $=$ $$ $49872002$ $|$Virus$|$. Then $|\\mathcal{M}|$ $=$ $900$.\n- Let $\\mathcal{B}$ be the subset of $\\mathcal{L}$ that also have $|$Causative agent$|$ $=$ $$ $410607006$ $|$Bacteria$|$. Then $|\\mathcal{B}|$ $=$ $1100$.\n- Some concepts specify multiple causative agents; the overlap satisfies $|\\mathcal{M} \\cap \\mathcal{B}|$ $=$ $200$.\n- Let $\\mathcal{P}$ be the set $$ $233604007$ $|$Pneumonia$|$. Assume every concept in $\\mathcal{P}$ is in $\\mathcal{L}$ (i.e., each such concept satisfies the grouped lung and inflammatory constraints). The causative-agent counts within $\\mathcal{P}$ are: $|\\mathcal{P} \\cap \\mathcal{M}|$ $=$ $400$, $|\\mathcal{P} \\cap \\mathcal{B}|$ $=$ $700$, and $|\\mathcal{P} \\cap \\mathcal{M} \\cap \\mathcal{B}|$ $=$ $100$.\n\nUsing only the formal semantics of descendant-or-self, attribute grouping, set union, intersection, and set difference, determine the exact number of SNOMED CT concepts returned by the ECL above. Give your final answer as an exact integer without units.", "solution": "The problem has been validated and found to be a self-consistent, well-posed question grounded in the formal principles of set theory and the structure of SNOMED CT.\n\nThe objective is to determine the number of concepts returned by the given SNOMED CT Expression Constraint Language (ECL) query. We must first translate the ECL query into a formal set-theoretic expression and then calculate the cardinality of the resulting set using the provided data.\n\nThe ECL query is:\n$$ $404684003$ $|$Clinical finding$|$ $:$ $\\{$ $363698007$ $|$Finding site$|$ $=$ $$ $39607008$ $|$Lung structure$|$, $116676008$ $|$Associated morphology$|$ $=$ $$ $409774005$ $|$Inflammatory morphology$|$, $246075003$ $|$Causative agent$|$ $=$ $\\big($ $$ $49872002$ $|$Virus$|$ OR $$ $410607006$ $|$Bacteria$|$ $\\big)$ $\\}$ MINUS $$ $233604007$ $|$Pneumonia$|$.\n\nLet us denote the set of concepts returned by this query as $\\mathcal{R}$. The query can be deconstructed as follows:\n$1$. The base set of concepts consists of clinical findings that are inflammatory and located in the lung structure. The problem defines this set as $\\mathcal{L}$, with cardinality $|\\mathcal{L}| = 2600$. This part of the query is: `$$ $404684003$ $|$Clinical finding$|$ $:$ $\\{$ $363698007$ $|$Finding site$|$ $=$ $$ $39607008$, $116676008$ $|$Associated morphology$|$ $=$ $$ $409774005$ $\\}$`.\n\n$2$. This base set is further constrained by the causative agent. The constraint `$|Causative agent|$ $=$ $\\big($ $$ $49872002$ $|$Virus$|$ OR $$ $410607006$ $|$Bacteria$|$ $\\big)$` selects concepts from $\\mathcal{L}$ that have a causative agent that is either a virus or a bacterium (or both).\nLet $\\mathcal{M}$ be the subset of $\\mathcal{L}$ for which the causative agent is a virus. The problem provides $|\\mathcal{M}| = 900$.\nLet $\\mathcal{B}$ be the subset of $\\mathcal{L}$ for which the causative agent is a bacterium. The problem provides $|\\mathcal{B}| = 1100$.\nThe set of concepts in $\\mathcal{L}$ that satisfy the `OR` condition is the union of these two sets, $\\mathcal{M} \\cup \\mathcal{B}$.\n\n$3$. The `MINUS` operator corresponds to set difference. It removes all concepts that are members of the set `$$ $233604007$ $|$Pneumonia$|$. This set is denoted as $\\mathcal{P}$.\n\nCombining these parts, the final set of concepts $\\mathcal{R}$ is given by the expression:\n$$\n\\mathcal{R} = (\\mathcal{M} \\cup \\mathcal{B}) \\setminus \\mathcal{P}\n$$\nOur goal is to find the cardinality of this set, $|\\mathcal{R}|$.\n\nUsing the formula for the cardinality of a set difference, $|A \\setminus B| = |A| - |A \\cap B|$, we can write:\n$$\n|\\mathcal{R}| = |(\\mathcal{M} \\cup \\mathcal{B})| - |(\\mathcal{M} \\cup \\mathcal{B}) \\cap \\mathcal{P}|\n$$\n\nWe will calculate the two terms on the right-hand side separately.\n\nFirst, we calculate $|\\mathcal{M} \\cup \\mathcal{B}|$ using the Principle of Inclusion-Exclusion:\n$$\n|\\mathcal{M} \\cup \\mathcal{B}| = |\\mathcal{M}| + |\\mathcal{B}| - |\\mathcal{M} \\cap \\mathcal{B}|\n$$\nThe problem provides the necessary values: $|\\mathcal{M}| = 900$, $|\\mathcal{B}| = 1100$, and $|\\mathcal{M} \\cap \\mathcal{B}| = 200$. Substituting these values:\n$$\n|\\mathcal{M} \\cup \\mathcal{B}| = 900 + 1100 - 200 = 2000 - 200 = 1800\n$$\n\nSecond, we calculate $|(\\mathcal{M} \\cup \\mathcal{B}) \\cap \\mathcal{P}|$. Using the distributive property of intersection over union, we have:\n$$\n(\\mathcal{M} \\cup \\mathcal{B}) \\cap \\mathcal{P} = (\\mathcal{M} \\cap \\mathcal{P}) \\cup (\\mathcal{B} \\cap \\mathcal{P})\n$$\nWe apply the Principle of Inclusion-Exclusion again to find the cardinality of this union:\n$$\n|(\\mathcal{M} \\cap \\mathcal{P}) \\cup (\\mathcal{B} \\cap \\mathcal{P})| = |\\mathcal{M} \\cap \\mathcal{P}| + |\\mathcal{B} \\cap \\mathcal{P}| - |(\\mathcal{M} \\cap \\mathcal{P}) \\cap (\\mathcal{B} \\cap \\mathcal{P})|\n$$\nThe intersection of the two sets simplifies to $\\mathcal{M} \\cap \\mathcal{B} \\cap \\mathcal{P}$. So the expression becomes:\n$$\n|(\\mathcal{M} \\cup \\mathcal{B}) \\cap \\mathcal{P}| = |\\mathcal{M} \\cap \\mathcal{P}| + |\\mathcal{B} \\cap \\mathcal{P}| - |\\mathcal{M} \\cap \\mathcal{B} \\cap \\mathcal{P}|\n$$\nThe problem provides the values for these intersections: $|\\mathcal{M} \\cap \\mathcal{P}| = 400$, $|\\mathcal{B} \\cap \\mathcal{P}| = 700$, and $|\\mathcal{M} \\cap \\mathcal{B} \\cap \\mathcal{P}| = 100$. Substituting these values:\n$$\n|(\\mathcal{M} \\cup \\mathcal{B}) \\cap \\mathcal{P}| = 400 + 700 - 100 = 1100 - 100 = 1000\n$$\n\nFinally, we substitute the calculated cardinalities back into the expression for $|\\mathcal{R}|$:\n$$\n|\\mathcal{R}| = |\\mathcal{M} \\cup \\mathcal{B}| - |(\\mathcal{M} \\cup \\mathcal{B}) \\cap \\mathcal{P}| = 1800 - 1000 = 800\n$$\nThe total number of SNOMED CT concepts returned by the specified ECL query is $800$. The provided information $|\\mathcal{L}| = 2600$ serves to ensure consistency, as $|\\mathcal{M} \\cup \\mathcal{B}| = 1800 \\le 2600$, but is not directly used in the final calculation. The assumption that $\\mathcal{P} \\subseteq \\mathcal{L}$ also ensures the logical coherence of the model, as subtracting concepts from $\\mathcal{P}$ is meaningful because they are part of the same superset $\\mathcal{L}$ from which $\\mathcal{M}$ and $\\mathcal{B}$ are derived.", "answer": "$$\\boxed{800}$$", "id": "4548301"}, {"introduction": "Once data is structured using a clinical terminology, we can perform sophisticated analyses by leveraging the taxonomy's inherent hierarchy. This practice moves from logical queries to quantitative analysis by exploring how to measure the \"semantic similarity\" or \"distance\" between any two concepts. You will implement metrics derived from both graph theory and information theory, learning how the structure of a terminology and the statistical distribution of data can be combined to uncover meaningful clinical relationships.", "problem": "Implement a program that computes principled semantic similarity and taxonomy-based metrics for pairs of clinical concepts in a small, fixed concept hierarchy modeled after Systematized Nomenclature of Medicine Clinical Terms (SNOMED CT). The goal is to derive the metrics from the foundational definitions of probability and information content, and from graph-theoretic properties of a taxonomy, without relying on any external resources.\n\nFundamental base and required derivations:\n- Use the definition of probability from corpus frequencies: for a concept $c$, let $f(c)$ denote its propagated frequency obtained by summing the counts of all leaf annotations in the subtree rooted at $c$. Let $N$ be the total number of leaf annotations in the corpus. The empirical probability of $c$ is $p(c) = f(c)/N$.\n- Use the definition of self-information from Information Theory (Shannon): the information content of a concept $c$ is $\\mathrm{IC}(c) = -\\ln p(c)$ using the natural logarithm.\n- Use the definition of a taxonomy as a rooted directed acyclic graph with a partial order induced by the transitive closure of the parent-child (“is-a”) relation. For any two nodes $a$ and $b$, define the least common subsumer $\\mathrm{LCS}(a,b)$ as the deepest (maximum-depth) common ancestor of $a$ and $b$ in the taxonomy. Assume a tree structure so the $\\mathrm{LCS}$ is unique.\n- From these bases, derive and implement three information-theoretic measures between two concepts $a$ and $b$:\n  1. The shared information defined as the information content of the least common subsumer, $\\mathrm{shared\\_IC}(a,b) = \\mathrm{IC}(\\mathrm{LCS}(a,b))$.\n  2. A normalized similarity that is symmetric in $a$ and $b$, equals $1$ when $a=b$, and decreases to $0$ as only the root is shared; derive it from the ratio of shared to total self-information: $\\mathrm{norm\\_sim}(a,b) = \\dfrac{2\\,\\mathrm{IC}(\\mathrm{LCS}(a,b))}{\\mathrm{IC}(a)+\\mathrm{IC}(b)}$, with the convention that when the denominator is $0$ and $a=b$, the value is $1$, otherwise $0$.\n  3. An information-theoretic distance corresponding to the non-shared information: $\\mathrm{it\\_dist}(a,b) = \\mathrm{IC}(a)+\\mathrm{IC}(b)-2\\,\\mathrm{IC}(\\mathrm{LCS}(a,b))$.\n- Additionally, derive a graph-theoretic path-based similarity from the shortest path length $\\ell(a,b)$ on the undirected version of the taxonomy: $\\mathrm{path\\_sim}(a,b) = \\dfrac{1}{1+\\ell(a,b)}$. In a rooted tree, $\\ell(a,b) = \\mathrm{depth}(a)+\\mathrm{depth}(b)-2\\,\\mathrm{depth}(\\mathrm{LCS}(a,b))$, where $\\mathrm{depth}(\\text{root})=0$ and each edge increases depth by $1$.\n\nTaxonomy, corpus, and constraints to be used:\n- Concepts (node identifiers and informal labels):\n  - $\\text{SCT\\_000}$: Clinical finding (root)\n  - $\\text{SCT\\_100}$: Infectious disease\n  - $\\text{SCT\\_110}$: Respiratory infection\n  - $\\text{SCT\\_120}$: Gastrointestinal infection\n  - $\\text{SCT\\_111}$: Influenza\n  - $\\text{SCT\\_112}$: Pneumonia\n  - $\\text{SCT\\_121}$: Viral gastroenteritis\n  - $\\text{SCT\\_200}$: Neoplasm\n  - $\\text{SCT\\_210}$: Lung cancer\n  - $\\text{SCT\\_220}$: Breast cancer\n  - $\\text{SCT\\_300}$: Endocrine disease\n  - $\\text{SCT\\_310}$: Diabetes mellitus\n- Parent-child (“is-a”) edges (directed from parent to child):\n  - $\\text{SCT\\_000} \\to \\text{SCT\\_100}$, $\\text{SCT\\_000} \\to \\text{SCT\\_200}$, $\\text{SCT\\_000} \\to \\text{SCT\\_300}$\n  - $\\text{SCT\\_100} \\to \\text{SCT\\_110}$, $\\text{SCT\\_100} \\to \\text{SCT\\_120}$\n  - $\\text{SCT\\_110} \\to \\text{SCT\\_111}$, $\\text{SCT\\_110} \\to \\text{SCT\\_112}$\n  - $\\text{SCT\\_120} \\to \\text{SCT\\_121}$\n  - $\\text{SCT\\_200} \\to \\text{SCT\\_210}$, $\\text{SCT\\_200} \\to \\text{SCT\\_220}$\n  - $\\text{SCT\\_300} \\to \\text{SCT\\_310}$\n- Corpus leaf annotation counts (only leaves have direct annotations; internal nodes receive propagated counts by summation over descendants):\n  - $\\text{SCT\\_111}: f = 120$\n  - $\\text{SCT\\_112}: f = 80$\n  - $\\text{SCT\\_121}: f = 50$\n  - $\\text{SCT\\_210}: f = 70$\n  - $\\text{SCT\\_220}: f = 60$\n  - $\\text{SCT\\_310}: f = 90$\n- The total number of leaf annotations is $N = 120+80+50+70+60+90 = 470$. For any internal node $c$, define $f(c)$ as the sum of $f(\\cdot)$ over the leaves in the subtree rooted at $c$. The empirical probability is $p(c)=f(c)/N$, and the information content is $\\mathrm{IC}(c)=-\\ln p(c)$.\n\nTest suite:\nCompute the vector $[\\mathrm{shared\\_IC}, \\mathrm{norm\\_sim}, \\mathrm{it\\_dist}, \\mathrm{path\\_sim}]$ for each of the following ordered concept pairs:\n- Case $1$: $(\\text{SCT\\_111}, \\text{SCT\\_112})$\n- Case $2$: $(\\text{SCT\\_111}, \\text{SCT\\_111})$\n- Case $3$: $(\\text{SCT\\_111}, \\text{SCT\\_310})$\n- Case $4$: $(\\text{SCT\\_210}, \\text{SCT\\_220})$\n- Case $5$: $(\\text{SCT\\_110}, \\text{SCT\\_112})$\n- Case $6$: $(\\text{SCT\\_121}, \\text{SCT\\_220})$\n\nOutput specification:\n- For each case, output the four values in the order $[\\mathrm{shared\\_IC}, \\mathrm{norm\\_sim}, \\mathrm{it\\_dist}, \\mathrm{path\\_sim}]$, each rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results for all cases as a comma-separated list of these $4$-tuples, enclosed in square brackets. For example, the required format is a single line like `[[$v_{11},v_{12},v_{13},v_{14}$],[$v_{21},v_{22},v_{23},v_{24}$],\\dots]` with no extra whitespace or text.\n\nAll logarithms must be natural logarithms. No external input is allowed; the taxonomy and counts above are fixed and embedded in the program. No physical units or angles are involved. Percentages must not be used; probabilities should be handled as decimals as defined above.", "solution": "We begin from foundational principles in Information Theory and graph theory and derive implementable formulas.\n\n1. Probability and information content. Let $f(c)$ be the propagated frequency for concept $c$ computed by summing leaf annotation counts over all leaves in the subtree rooted at $c$. Let $N$ be the total annotations across all leaves. Then the empirical probability of $c$ is $p(c) = f(c)/N$. The information content, following self-information, is $\\mathrm{IC}(c) = -\\ln p(c)$. This is grounded in the well-tested fact that information is additive over independent events and decreases with increasing probability.\n\n2. Least common subsumer and depth. In a rooted taxonomy (tree) with parent-child (“is-a”) edges, the set of ancestors of a node includes the node and all nodes on its path to the root. The least common subsumer $\\mathrm{LCS}(a,b)$ is the deepest (maximum-depth) node present in both ancestor sets. In a tree, this node is unique because there is a unique path from each node to the root. Depth is defined as $\\mathrm{depth}(\\text{root})=0$ and increases by $1$ along each edge away from the root.\n\n3. Information-theoretic measures. Consider two concepts $a$ and $b$ and their LCS $c^{\\ast} = \\mathrm{LCS}(a,b)$. The shared information is the self-information of their most specific common generalization, $\\mathrm{shared\\_IC}(a,b) = \\mathrm{IC}(c^{\\ast})$, because $-\\ln p(c^{\\ast})$ upper-bounds the surprise common to both $a$ and $b$. A normalized similarity that satisfies symmetry, boundedness in $[0,1]$, identity of indiscernibles (equals $1$ when $a=b$), and vanishes when only the root is shared, is obtained by comparing the shared information to the total self-information: $\\mathrm{norm\\_sim}(a,b) = \\dfrac{2\\,\\mathrm{IC}(c^{\\ast})}{\\mathrm{IC}(a)+\\mathrm{IC}(b)}$, with the convention that if the denominator is $0$ and $a=b$ then the value is $1$, else $0$. This ratio is bounded by $1$ since $\\mathrm{IC}(c^{\\ast}) \\le \\min\\{\\mathrm{IC}(a),\\mathrm{IC}(b)\\}$. The non-shared information is captured by the information-theoretic distance $\\mathrm{it\\_dist}(a,b) = \\mathrm{IC}(a)+\\mathrm{IC}(b)-2\\,\\mathrm{IC}(c^{\\ast}) \\ge 0$, which follows from the same monotonicity property.\n\n4. Graph-theoretic path similarity. In the undirected version of the taxonomy, the shortest path length between $a$ and $b$ is $\\ell(a,b)$. In a rooted tree, this equals $\\mathrm{depth}(a)+\\mathrm{depth}(b)-2\\,\\mathrm{depth}(c^{\\ast})$. A simple, bounded similarity is then $\\mathrm{path\\_sim}(a,b) = \\dfrac{1}{1+\\ell(a,b)} \\in (0,1]$, which equals $1$ when $a=b$ and decreases as nodes separate.\n\nAlgorithmic design:\n- Build the taxonomy as a parent map and children adjacency list. Identify leaves as nodes with no children.\n- Compute depths via breadth-first traversal from the root.\n- Compute propagated frequencies $f(c)$ by summing leaf counts bottom-up. Here, the leaf counts are: $\\text{SCT\\_111}: f=$ $120$, $\\text{SCT\\_112}: f=$ $80$, $\\text{SCT\\_121}: f=$ $50$, $\\text{SCT\\_210}: f=$ $70$, $\\text{SCT\\_220}: f=$ $60$, $\\text{SCT\\_310}: f=$ $90$. The total is $N=$ $470$. The internal node frequencies by summation are: $f(\\text{SCT\\_110})=$ $200$, $f(\\text{SCT\\_120})=$ $50$, $f(\\text{SCT\\_100})=$ $250$, $f(\\text{SCT\\_200})=$ $130$, $f(\\text{SCT\\_300})=$ $90$, $f(\\text{SCT\\_000})=$ $470$.\n- Compute $p(c)=f(c)/N$ and $\\mathrm{IC}(c)=-\\ln p(c)$ using natural logarithms. Numerically (rounded to $9$ decimals for transparency): $\\mathrm{IC}(\\text{SCT\\_111})=$ $1.365241055$, $\\mathrm{IC}(\\text{SCT\\_112})=$ $1.770706163$, $\\mathrm{IC}(\\text{SCT\\_121})=$ $2.240709793$, $\\mathrm{IC}(\\text{SCT\\_210})=$ $1.904237556$, $\\mathrm{IC}(\\text{SCT\\_220})=$ $2.058388236$, $\\mathrm{IC}(\\text{SCT\\_310})=$ $1.652923128$, $\\mathrm{IC}(\\text{SCT\\_110})=$ $0.854415432$, $\\mathrm{IC}(\\text{SCT\\_120})=$ $2.240709793$, $\\mathrm{IC}(\\text{SCT\\_100})=$ $0.631271880$, $\\mathrm{IC}(\\text{SCT\\_200})=$ $1.285198348$, $\\mathrm{IC}(\\text{SCT\\_300})=$ $1.652923128$, $\\mathrm{IC}(\\text{SCT\\_000})=$ $0$.\n- For each test case, find the $\\mathrm{LCS}$, compute $\\ell(a,b)$ from depths, and evaluate the four metrics, rounding to $6$ decimals.\n\nComputed results for the specified test suite (each vector as $[\\mathrm{shared\\_IC}, \\mathrm{norm\\_sim}, \\mathrm{it\\_dist}, \\mathrm{path\\_sim}]$, rounded to $6$ decimals):\n- Case $1$ $(\\text{SCT\\_111}, \\text{SCT\\_112})$: $[0.854415, 0.544917, 1.427116, 0.333333]$\n- Case $2$ $(\\text{SCT\\_111}, \\text{SCT\\_111})$: $[1.365241, 1.000000, 0.000000, 1.000000]$\n- Case $3$ $(\\text{SCT\\_111}, \\text{SCT\\_310})$: $[0.000000, 0.000000, 3.018164, 0.166667]$\n- Case $4$ $(\\text{SCT\\_210}, \\text{SCT\\_220})$: $[1.285198, 0.648660, 1.392229, 0.333333]$\n- Case $5$ $(\\text{SCT\\_110}, \\text{SCT\\_112})$: $[0.854415, 0.650953, 0.916291, 0.500000]$\n- Case $6$ $(\\text{SCT\\_121}, \\text{SCT\\_220})$: $[0.000000, 0.000000, 4.299098, 0.166667]$\n\nThe program below implements these steps deterministically from the provided taxonomy and counts, and prints a single line in the exact required format containing all six case results.", "answer": "```python\n# Python 3.12 program to compute semantic similarity and taxonomy metrics\n# for a fixed SNOMED-like taxonomy and corpus counts.\n# No external input; outputs a single line list-of-lists as specified.\n\nimport math\n\ndef build_taxonomy():\n    # Define nodes\n    nodes = {\n        'SCT_000': 'Clinical finding',\n        'SCT_100': 'Infectious disease',\n        'SCT_110': 'Respiratory infection',\n        'SCT_120': 'Gastrointestinal infection',\n        'SCT_111': 'Influenza',\n        'SCT_112': 'Pneumonia',\n        'SCT_121': 'Viral gastroenteritis',\n        'SCT_200': 'Neoplasm',\n        'SCT_210': 'Lung cancer',\n        'SCT_220': 'Breast cancer',\n        'SCT_300': 'Endocrine disease',\n        'SCT_310': 'Diabetes mellitus',\n    }\n    # Define parent-child edges\n    edges = [\n        ('SCT_000', 'SCT_100'),\n        ('SCT_000', 'SCT_200'),\n        ('SCT_000', 'SCT_300'),\n        ('SCT_100', 'SCT_110'),\n        ('SCT_100', 'SCT_120'),\n        ('SCT_110', 'SCT_111'),\n        ('SCT_110', 'SCT_112'),\n        ('SCT_120', 'SCT_121'),\n        ('SCT_200', 'SCT_210'),\n        ('SCT_200', 'SCT_220'),\n        ('SCT_300', 'SCT_310'),\n    ]\n    children = {n: [] for n in nodes}\n    parent = {}\n    for p, c in edges:\n        children[p].append(c)\n        parent[c] = p\n    root = 'SCT_000'\n    return nodes, children, parent, root\n\ndef compute_depths(children, root):\n    depths = {root: 0}\n    stack = [root]\n    while stack:\n        u = stack.pop()\n        for v in children.get(u, []):\n            depths[v] = depths[u] + 1\n            stack.append(v)\n    return depths\n\ndef compute_subtree_counts(children, leaf_counts, nodes):\n    # Post-order DFS to sum counts\n    # Initialize counts with leaf counts, zero otherwise\n    counts = {n: 0 for n in nodes}\n    for leaf, cnt in leaf_counts.items():\n        counts[leaf] = cnt\n\n    # We can perform multiple passes since it's a tree of small size\n    # or implement recursion\n    visited = set()\n    def dfs_sum(u):\n        if u in visited:\n            return counts[u]\n        total = counts[u]\n        for v in children.get(u, []):\n            total += dfs_sum(v)\n        counts[u] = total\n        visited.add(u)\n        return total\n\n    # Call for all nodes to ensure coverage\n    for n in nodes:\n        dfs_sum(n)\n    return counts\n\ndef ancestors_with_depth(node, parent, depths):\n    # Return list of (ancestor, depth) including node and root\n    res = []\n    u = node\n    while True:\n        d = depths[u]\n        res.append((u, d))\n        if u not in parent:\n            break\n        u = parent[u]\n    return res\n\ndef lcs(a, b, parent, depths):\n    anc_a = {u: d for u, d in ancestors_with_depth(a, parent, depths)}\n    anc_b = {u: d for u, d in ancestors_with_depth(b, parent, depths)}\n    common = set(anc_a.keys())  set(anc_b.keys())\n    # choose the one with maximum depth\n    best = None\n    best_depth = -1\n    for u in common:\n        d = depths[u]\n        if d  best_depth:\n            best_depth = d\n            best = u\n    return best\n\ndef path_length(a, b, depths, lcs_node):\n    return depths[a] + depths[b] - 2 * depths[lcs_node]\n\ndef compute_metrics_for_pair(a, b, ic, depths, parent):\n    c_star = lcs(a, b, parent, depths)\n    shared_ic = ic[c_star]\n    ia = ic[a]\n    ib = ic[b]\n    denom = ia + ib\n    if denom == 0.0:\n        norm_sim = 1.0 if a == b else 0.0\n    else:\n        norm_sim = (2.0 * shared_ic) / denom\n    it_dist = ia + ib - 2.0 * shared_ic\n    pl = path_length(a, b, depths, c_star)\n    path_sim = 1.0 / (1.0 + pl)\n    return shared_ic, norm_sim, it_dist, path_sim\n\ndef solve():\n    nodes, children, parent, root = build_taxonomy()\n    depths = compute_depths(children, root)\n    # Leaf counts\n    leaf_counts = {\n        'SCT_111': 120,\n        'SCT_112': 80,\n        'SCT_121': 50,\n        'SCT_210': 70,\n        'SCT_220': 60,\n        'SCT_310': 90,\n    }\n    counts = compute_subtree_counts(children, leaf_counts, nodes)\n    N = sum(leaf_counts.values())\n    # Compute IC\n    ic = {}\n    for n in nodes:\n        p = counts[n] / N\n        # Numerical safety: ensure p > 0\n        if p = 0.0:\n            ic[n] = float('inf')\n        else:\n            ic[n] = -math.log(p)\n    # Test cases\n    test_cases = [\n        ('SCT_111', 'SCT_112'),\n        ('SCT_111', 'SCT_111'),\n        ('SCT_111', 'SCT_310'),\n        ('SCT_210', 'SCT_220'),\n        ('SCT_110', 'SCT_112'),\n        ('SCT_121', 'SCT_220'),\n    ]\n    results = []\n    for a, b in test_cases:\n        shared_ic, norm_sim, it_dist, path_sim = compute_metrics_for_pair(a, b, ic, depths, parent)\n        # Round to 6 decimals in string format\n        results.append([\n            f\"{shared_ic:.6f}\",\n            f\"{norm_sim:.6f}\",\n            f\"{it_dist:.6f}\",\n            f\"{path_sim:.6f}\",\n        ])\n    # Format as a single-line JSON-like list\n    inner = [f\"[{','.join(r)}]\" for r in results]\n    output = f\"[{','.join(inner)}]\"\n    print(output)\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "4548273"}]}