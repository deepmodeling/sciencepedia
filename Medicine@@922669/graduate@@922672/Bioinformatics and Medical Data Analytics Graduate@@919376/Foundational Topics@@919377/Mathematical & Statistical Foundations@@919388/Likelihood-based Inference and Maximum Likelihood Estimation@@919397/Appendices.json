{"hands_on_practices": [{"introduction": "Understanding likelihood-based inference begins with mastering its application in the simplest settings. This first practice focuses on the Bernoulli model, a cornerstone for analyzing binary outcomes like the presence or absence of a genetic variant. By deriving the maximum likelihood estimator (MLE) for the Bernoulli parameter $p$ from first principles, and then computing its asymptotic variance via the Fisher information, you will solidify your grasp of the core analytical mechanics that underpin all likelihood-based methods. [@problem_id:4578114]", "problem": "A tumor sequencing study using Next-Generation Sequencing (NGS) aims to estimate the cohort-level prevalence of a clinically actionable variant in a specific gene. After stringent quality control filtering to remove low-confidence reads, each tumor’s variant detection outcome is modeled as an independent Bernoulli random variable with common parameter $p \\in (0,1)$, where $p$ represents the probability that the variant is truly present in a randomly sampled tumor from the study population. Let $X_{1}, X_{2}, \\dots, X_{n}$ be the $n$ independent detection indicators, with $X_{i} \\in \\{0,1\\}$, and define the sufficient statistic $S = \\sum_{i=1}^{n} X_{i}$. Assume $0  S  n$ to avoid boundary solutions.\n\nUsing the fundamental definitions of likelihood for independent and identically distributed Bernoulli observations and Fisher information for a scalar parameter, derive the maximum likelihood estimator of $p$ and the asymptotic variance of this estimator based on the Fisher information. Express your final answer as a closed-form analytic expression in terms of $S$ and $n$ for the maximum likelihood estimator, and in terms of $p$ and $n$ for its asymptotic variance. No numerical rounding is required and no physical units apply. Provide both quantities together as a single final answer.", "solution": "The problem statement is subjected to validation prior to any attempt at a solution.\n\n### Step 1: Extract Givens\n-   The detection outcome for each tumor, $X_i$, is an independent Bernoulli random variable.\n-   The common parameter for the Bernoulli distribution is $p \\in (0,1)$.\n-   The observations are $X_{1}, X_{2}, \\dots, X_{n}$, which are $n$ independent and identically distributed (i.i.d.) indicators, with $X_{i} \\in \\{0,1\\}$.\n-   The sufficient statistic is $S = \\sum_{i=1}^{n} X_{i}$.\n-   An explicit assumption is made: $0  S  n$.\n-   The task is to derive the maximum likelihood estimator (MLE) of $p$ in terms of $S$ and $n$.\n-   The task is also to derive the asymptotic variance of this estimator based on the Fisher information, expressed in terms of $p$ and $n$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-posed and scientifically grounded.\n-   **Scientifically Grounded:** The modeling of binary outcomes (variant present/absent) with a Bernoulli distribution is a fundamental and standard practice in statistics and bioinformatics. The use of maximum likelihood estimation and Fisher information are core principles of statistical inference.\n-   **Well-Posed:** The problem provides a clear statistical model (i.i.d. Bernoulli trials) and a precisely defined objective (derive the MLE and its asymptotic variance). The assumption $0  S  n$ is a simplifying condition to ensure the estimator does not fall on the boundary of the parameter space, which is a standard procedure in many theoretical treatments. A unique and meaningful solution is expected.\n-   **Objective:** The problem is stated in precise, objective mathematical language, free from ambiguity or subjective claims.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\n### Solution Derivation\nThe problem requires the derivation of the maximum likelihood estimator (MLE) for the parameter $p$ of a Bernoulli distribution and the asymptotic variance of this estimator.\n\n#### Part 1: Maximum Likelihood Estimator of $p$\n\nLet $X_1, X_2, \\dots, X_n$ be a set of $n$ independent and identically distributed random variables from a Bernoulli distribution with parameter $p$. The probability mass function (PMF) for a single observation $X_i=x_i$, where $x_i \\in \\{0,1\\}$, is given by:\n$$\nP(X_i=x_i | p) = p^{x_i}(1-p)^{1-x_i}\n$$\nDue to the independence of the observations, the joint probability of observing the entire dataset $x = (x_1, x_2, \\dots, x_n)$ is the product of the individual probabilities. This joint probability, viewed as a function of the parameter $p$ for a fixed set of data, is the likelihood function, $L(p|\\boldsymbol{x})$.\n$$\nL(p | x_1, \\dots, x_n) = \\prod_{i=1}^{n} P(X_i=x_i | p) = \\prod_{i=1}^{n} p^{x_i}(1-p)^{1-x_i}\n$$\nThis expression can be simplified by using the sufficient statistic $S = \\sum_{i=1}^{n} x_i$, which represents the total number of successes (variants detected).\n$$\nL(p) = p^{\\sum_{i=1}^{n} x_i} (1-p)^{\\sum_{i=1}^{n} (1-x_i)} = p^S (1-p)^{n-S}\n$$\nTo find the value of $p$ that maximizes $L(p)$, it is analytically more convenient to maximize the natural logarithm of the likelihood function, the log-likelihood $\\ell(p)$, since the logarithm is a monotonically increasing function.\n$$\n\\ell(p) = \\ln(L(p)) = \\ln(p^S (1-p)^{n-S}) = S \\ln(p) + (n-S) \\ln(1-p)\n$$\nTo find the maximum, we compute the first derivative of $\\ell(p)$ with respect to $p$ and set it to zero. This derivative is known as the score function.\n$$\n\\frac{d\\ell}{dp} = \\frac{d}{dp} \\left( S \\ln(p) + (n-S) \\ln(1-p) \\right) = \\frac{S}{p} - \\frac{n-S}{1-p}\n$$\nSetting the score to zero to find the critical point, we solve for $p$. Let $\\hat{p}$ denote the estimator.\n$$\n\\frac{S}{\\hat{p}} - \\frac{n-S}{1-\\hat{p}} = 0 \\implies \\frac{S}{\\hat{p}} = \\frac{n-S}{1-\\hat{p}}\n$$\n$$\nS(1-\\hat{p}) = (n-S)\\hat{p}\n$$\n$$\nS - S\\hat{p} = n\\hat{p} - S\\hat{p}\n$$\n$$\nS = n\\hat{p}\n$$\nSolving for $\\hat{p}$ yields the maximum likelihood estimator:\n$$\n\\hat{p} = \\frac{S}{n}\n$$\nThe condition $0  S  n$ ensures that $\\hat{p}$ is in the open interval $(0,1)$, consistent with the parameter space $p \\in (0,1)$, and avoids division by zero in the score function. To confirm that this is a maximum, we examine the second derivative of the log-likelihood:\n$$\n\\frac{d^2\\ell}{dp^2} = \\frac{d}{dp} \\left( \\frac{S}{p} - \\frac{n-S}{1-p} \\right) = -\\frac{S}{p^2} - \\frac{n-S}{(1-p)^2}\n$$\nSince $S0$, $n-S0$, and $p^2  0$, $(1-p)^2  0$, the second derivative $\\frac{d^2\\ell}{dp^2}$ is strictly negative for all $p \\in (0,1)$. This confirms that the log-likelihood function is concave and the critical point $\\hat{p} = S/n$ is indeed a unique maximum.\n\n#### Part 2: Asymptotic Variance of the MLE\n\nThe asymptotic variance of an MLE is given by the inverse of the Fisher information, $I(p)$. For a single scalar parameter, the Fisher information is defined as:\n$$\nI(p) = -E\\left[ \\frac{d^2\\ell}{dp^2} \\right]\n$$\nwhere the expectation is taken with respect to the distribution of the data. We have the second derivative:\n$$\n\\frac{d^2\\ell}{dp^2} = -\\frac{S}{p^2} - \\frac{n-S}{(1-p)^2}\n$$\nNow, we take the expectation. The random variable in this expression is $S = \\sum_{i=1}^n X_i$. The expectation of a sum is the sum of expectations. For a single Bernoulli trial $X_i$, $E[X_i] = 1 \\cdot p + 0 \\cdot (1-p) = p$. Therefore, the expectation of $S$ is:\n$$\nE[S] = E\\left[\\sum_{i=1}^n X_i\\right] = \\sum_{i=1}^n E[X_i] = \\sum_{i=1}^n p = np\n$$\nSubstituting $E[S]$ into the expression for the expected second derivative:\n$$\nE\\left[ \\frac{d^2\\ell}{dp^2} \\right] = E\\left[ -\\frac{S}{p^2} - \\frac{n-S}{(1-p)^2} \\right] = -\\frac{E[S]}{p^2} - \\frac{n-E[S]}{(1-p)^2}\n$$\n$$\nE\\left[ \\frac{d^2\\ell}{dp^2} \\right] = -\\frac{np}{p^2} - \\frac{n-np}{(1-p)^2} = -\\frac{n}{p} - \\frac{n(1-p)}{(1-p)^2}\n$$\n$$\nE\\left[ \\frac{d^2\\ell}{dp^2} \\right] = -\\frac{n}{p} - \\frac{n}{1-p} = -n \\left( \\frac{1}{p} + \\frac{1}{1-p} \\right) = -n \\left( \\frac{1-p+p}{p(1-p)} \\right) = -\\frac{n}{p(1-p)}\n$$\nThe Fisher information is the negative of this quantity:\n$$\nI(p) = - \\left( -\\frac{n}{p(1-p)} \\right) = \\frac{n}{p(1-p)}\n$$\nAccording to the Cramer-Rao Lower Bound and large-sample theory for MLEs, the asymptotic variance of $\\hat{p}$ is the inverse of the Fisher information:\n$$\n\\text{Asymptotic Variance}(\\hat{p}) = [I(p)]^{-1} = \\left( \\frac{n}{p(1-p)} \\right)^{-1} = \\frac{p(1-p)}{n}\n$$\nThis variance is the variance of the sample mean of i.i.d. Bernoulli random variables, which is a well-known result, and the MLE's asymptotic variance converges to this value.\n\nThe final answer requires the expression for the MLE, $\\hat{p}$, and its asymptotic variance.\n-   Maximum Likelihood Estimator: $\\hat{p} = \\frac{S}{n}$\n-   Asymptotic Variance: $\\frac{p(1-p)}{n}$\n\nThese two quantities will be presented as a single row matrix.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{S}{n}  \\frac{p(1-p)}{n} \\end{pmatrix}}\n$$", "id": "4578114"}, {"introduction": "Bioinformatics data is often incomplete, with key biological states treated as latent variables. This practice tackles such a scenario by asking you to implement the Expectation-Maximization (EM) algorithm, a powerful iterative technique for finding the MLE when direct optimization is intractable. Using the practical example of genotype calling from Next-Generation Sequencing (NGS) data, you will learn to derive and implement the E-step (computing posterior probabilities of latent genotypes) and the M-step (updating the population allele frequency). [@problem_id:4578121]", "problem": "You are given a biallelic locus in a cohort sequenced by Next-Generation Sequencing (NGS). Each individual $i \\in \\{1,\\dots,N\\}$ has a latent genotype $g_i \\in \\{0,1,2\\}$ denoting the number of reference alleles. You observe incomplete data: the number of reads supporting the reference allele $r_i$ and the number supporting the alternative allele $a_i$, with $n_i = r_i + a_i$. Reads are assumed independent conditional on $g_i$ and a known per-base symmetric sequencing error rate $\\varepsilon \\in (0,1/2)$. Under this model, the probability that any single read reports the reference base is: if $g_i=0$, the probability is $\\varepsilon$; if $g_i=1$, the probability is $1/2$; if $g_i=2$, the probability is $1-\\varepsilon$. The population allele frequency of the reference allele is $p \\in (0,1)$, and genotypes follow Hardy–Weinberg equilibrium (HWE) with prior probabilities $\\Pr(g_i=0)=(1-p)^2$, $\\Pr(g_i=1)=2p(1-p)$, $\\Pr(g_i=2)=p^2$. Assume independence across individuals.\n\nYour task is to use likelihood-based inference to estimate $p$ by Maximum Likelihood Estimation (MLE) via the Expectation–Maximization (EM) algorithm, treating genotypes as latent. Start from core definitions of likelihoods and conditional independence; do not assume any shortcut formulas. Derive the expected complete-data sufficient statistics required by EM and the update for $p$ from first principles by explicitly writing the complete-data likelihood, taking expectations with respect to the posterior of latent variables given current parameters, and maximizing with respect to $p$.\n\nImplementation requirements:\n- Input is fixed and embedded in your program. For each test case, use the provided symmetric error rate $\\varepsilon$, the list of observed read counts $\\{(r_i,a_i)\\}_{i=1}^N$ with $r_i,a_i \\in \\mathbb{N}_0$, and an initial value $p^{(0)} \\in (0,1)$.\n- Use an EM algorithm that iterates the E-step and M-step until the absolute difference $|p^{(t+1)} - p^{(t)}|$ is less than a tolerance $\\tau = 10^{-9}$ or the number of iterations reaches a maximum $T_{\\max} = 1000$, whichever occurs first.\n- For numerical stability, implement the E-step in the log-domain using a stable normalization (e.g., log-sum-exp). Treat zero-depth samples (where $n_i = 0$) consistently with the probabilistic model.\n- The final estimate $\\hat p$ for each test case must be printed as a decimal rounded to six digits after the decimal point. Do not print a percentage sign.\n\nTest suite:\n- Case $1$ (general mixture): $\\varepsilon = 0.01$, reads $\\{(28,2),(2,28),(15,15),(18,2),(12,13),(0,10)\\}$, $p^{(0)} = 0.5$.\n- Case $2$ (boundary leaning to fixation): $\\varepsilon = 0.001$, reads $\\{(40,0),(30,0),(25,0),(10,0),(50,0)\\}$, $p^{(0)} = 0.4$.\n- Case $3$ (heterozygote-like across cohort): $\\varepsilon = 0.02$, reads $\\{(4,4),(5,5),(6,6),(3,3),(7,7),(10,10),(2,2),(15,15)\\}$, $p^{(0)} = 0.1$.\n- Case $4$ (includes zero-depth individuals): $\\varepsilon = 0.01$, reads $\\{(0,0),(12,12),(0,30),(0,0),(25,25),(49,1)\\}$, $p^{(0)} = 0.6$.\n- Case $5$ (high sequencing error): $\\varepsilon = 0.2$, reads $\\{(18,12),(2,18),(10,10),(16,4)\\}$, $p^{(0)} = 0.3$.\n\nYour program should produce a single line of output containing the estimated allele frequencies for the five cases as a comma-separated list enclosed in square brackets, each rounded to six digits after the decimal point (for example, $[0.123456,0.654321,0.500000,0.750000,0.333333]$). No other text should be printed.", "solution": "The problem requires the derivation and implementation of an Expectation-Maximization (EM) algorithm to find the Maximum Likelihood Estimate (MLE) of the reference allele frequency, $p$, in a population. The estimation is based on Next-Generation Sequencing (NGS) read counts, where individual genotypes are unobserved (latent).\n\n### 1. Model Specification\n\nLet us formally define the probabilistic model.\n\n-   **Observed Data**: For each of $N$ individuals, we observe the read counts $D_i = (r_i, a_i)$, where $r_i$ is the number of reads supporting the reference allele and $a_i$ is the number of reads supporting the alternative allele. The total number of reads for individual $i$ is $n_i = r_i + a_i$. The complete set of observed data is $D = \\{D_i\\}_{i=1}^N$.\n-   **Latent Variables**: The genotype $g_i$ for each individual is a latent variable. As the locus is biallelic, $g_i \\in \\{0, 1, 2\\}$ represents the number of reference alleles possessed by individual $i$. The set of all genotypes is $G = \\{g_i\\}_{i=1}^N$.\n-   **Parameter**: The parameter to be estimated is the reference allele frequency in the population, $p \\in (0,1)$. The sequencing error rate $\\varepsilon \\in (0, 1/2)$ is a known constant.\n-   **Sequencing Error Model**: Conditional on the genotype $g_i$, the number of reference reads $r_i$ out of $n_i$ total reads is assumed to follow a binomial distribution: $r_i | g_i, n_i \\sim \\text{Binomial}(n_i, \\theta_{g_i})$. The probability $\\theta_{g_i}$ that a single read is a reference allele depends on the true genotype and the symmetric error rate $\\varepsilon$:\n    -   If $g_i=0$ (homozygous alternative), $\\theta_0 = \\varepsilon$.\n    -   If $g_i=1$ (heterozygous), $\\theta_1 = 1/2$.\n    -   If $g_i=2$ (homozygous reference), $\\theta_2 = 1-\\varepsilon$.\n    The likelihood of observing reads $D_i$ given genotype $g_i$ is therefore $\\Pr(D_i|g_i, \\varepsilon) = \\binom{n_i}{r_i}\\theta_{g_i}^{r_i}(1-\\theta_{g_i})^{a_i}$.\n-   **Population Genetic Model**: Genotypes are assumed to be in Hardy–Weinberg Equilibrium (HWE). The prior probability of each genotype depends on the allele frequency $p$:\n    -   $\\Pr(g_i=0|p) = (1-p)^2$\n    -   $\\Pr(g_i=1|p) = 2p(1-p)$\n    -   $\\Pr(g_i=2|p) = p^2$\n\n### 2. The EM Algorithm Framework\n\nOur goal is to find the MLE of $p$ by maximizing the log-likelihood of the observed data, $\\ell(p; D) = \\log \\Pr(D | p, \\varepsilon)$. This marginal likelihood is obtained by summing over all possible latent genotype configurations:\n$$ \\ell(p; D) = \\log \\left( \\sum_{G} \\Pr(D, G | p, \\varepsilon) \\right) = \\sum_{i=1}^N \\log \\left( \\sum_{j=0}^2 \\Pr(D_i | g_i=j, \\varepsilon) \\Pr(g_i=j | p) \\right) $$\nDirect maximization of this expression is complicated due to the sum inside the logarithm. The EM algorithm provides an iterative approach to solve this. It operates on the **complete-data log-likelihood**, $\\ell_c(p; D, G) = \\log \\Pr(D, G | p, \\varepsilon)$, which is more amenable to maximization.\n\nAssuming independence across individuals, the complete-data log-likelihood is:\n$$ \\ell_c(p; D, G) = \\sum_{i=1}^N \\log \\Pr(D_i, g_i | p, \\varepsilon) = \\sum_{i=1}^N \\left( \\log \\Pr(D_i|g_i, \\varepsilon) + \\log \\Pr(g_i|p) \\right) $$\nWe introduce indicator variables $z_{ij} = \\mathbb{I}(g_i=j)$ for $j \\in \\{0, 1, 2\\}$, where $\\sum_{j=0}^2 z_{ij} = 1$. The term relevant for estimating $p$ is $\\log \\Pr(g_i|p)$:\n$$ \\log \\Pr(g_i|p) = z_{i0} \\log((1-p)^2) + z_{i1} \\log(2p(1-p)) + z_{i2} \\log(p^2) $$\n$$ = (2z_{i0} + z_{i1})\\log(1-p) + (z_{i1} + 2z_{i2})\\log p + z_{i1}\\log 2 $$\nThe complete-data log-likelihood can be expressed as:\n$$ \\ell_c(p; D, G) = \\sum_{i=1}^N \\left[ (z_{i1} + 2z_{i2})\\log p + (2z_{i0} + z_{i1})\\log(1-p) \\right] + C $$\nwhere $C$ contains terms that do not depend on $p$. The terms $(z_{i1} + 2z_{i2})$ and $(2z_{i0} + z_{i1})$ represent the counts of reference and alternative alleles, respectively, for individual $i$.\n\nThe EM algorithm consists of two steps, iterated until convergence:\n1.  **E-Step**: Compute the expectation of the complete-data log-likelihood with respect to the posterior distribution of the latent variables $G$ given the observed data $D$ and the current parameter estimate $p^{(t)}$. This function is denoted $Q(p|p^{(t)})$.\n2.  **M-Step**: Maximize $Q(p|p^{(t)})$ with respect to $p$ to obtain the updated parameter estimate $p^{(t+1)}$.\n\n### 3. E-Step: Expectation\n\nThe E-step involves calculating $Q(p|p^{(t)}) = E_{G|D, p^{(t)}}[\\ell_c(p; D, G)]$. Due to the linearity of expectation, we only need the expectation of the indicator variables $z_{ij}$:\n$$ E[z_{ij} | D, p^{(t)}] = \\Pr(g_i=j | D_i, p^{(t)}) $$\nLet's denote this posterior probability, or responsibility, as $\\omega_{ij}^{(t)}$. Using Bayes' theorem:\n$$ \\omega_{ij}^{(t)} = \\frac{\\Pr(D_i|g_i=j, \\varepsilon) \\Pr(g_i=j|p^{(t)})}{\\sum_{k=0}^2 \\Pr(D_i|g_i=k, \\varepsilon) \\Pr(g_i=k|p^{(t)})} $$\nFor computational stability, we perform this calculation in the log domain. Let:\n-   Log Genotype Likelihood: $\\log L'_{ij} = \\log \\Pr(D_i|g_i=j, \\varepsilon)_{\\text{unnormalized}} = r_i \\log \\theta_j + a_i \\log(1-\\theta_j)$.\n    -   $\\log L'_{i0} = r_i \\log \\varepsilon + a_i \\log(1-\\varepsilon)$\n    -   $\\log L'_{i1} = (r_i+a_i) \\log(0.5)$\n    -   $\\log L'_{i2} = r_i \\log(1-\\varepsilon) + a_i \\log \\varepsilon$\n-   Log Genotype Prior: $\\log\\pi_j^{(t)} = \\log \\Pr(g_i=j|p^{(t)})$.\n    -   $\\log\\pi_0^{(t)} = 2\\log(1-p^{(t)})$\n    -   $\\log\\pi_1^{(t)} = \\log 2 + \\log p^{(t)} + \\log(1-p^{(t)})$\n    -   $\\log\\pi_2^{(t)} = 2\\log p^{(t)}$\n\nThe log of the joint probability is $J_{ij}^{(t)} = \\log L'_{ij} + \\log\\pi_j^{(t)}$. The denominator in the expression for $\\omega_{ij}^{(t)}$ can be unstable if computed directly. We use the log-sum-exp trick:\n-   Let $M_i = \\max_k J_{ik}^{(t)}$.\n-   Then $\\omega_{ij}^{(t)} = \\frac{\\exp(J_{ij}^{(t)} - M_i)}{\\sum_{k=0}^2 \\exp(J_{ik}^{(t)} - M_i)}$.\nThis concludes the E-step, yielding the posterior probabilities $\\omega_{ij}^{(t)}$ for all individuals $i$ and genotypes $j$.\n\nFor a zero-depth sample ($n_i=0$, so $r_i=0, a_i=0$), the genotype likelihoods are all $1$, so $\\log L'_{ij}=0$ for $j=0,1,2$. The data provides no information, and the posterior probabilities correctly reduce to the prior probabilities: $\\omega_{ij}^{(t)} = \\Pr(g_i=j|p^{(t)})$.\n\n### 4. M-Step: Maximization\n\nIn the M-step, we maximize $Q(p|p^{(t)})$ to find $p^{(t+1)}$.\n$$ Q(p|p^{(t)}) = E\\left[ \\sum_{i=1}^N \\left( (z_{i1} + 2z_{i2})\\log p + (2z_{i0} + z_{i1})\\log(1-p) \\right) \\middle| D, p^{(t)} \\right] + C' $$\n$$ Q(p|p^{(t)}) = \\sum_{i=1}^N \\left( (\\omega_{i1}^{(t)} + 2\\omega_{i2}^{(t)})\\log p + (2\\omega_{i0}^{(t)} + \\omega_{i1}^{(t)})\\log(1-p) \\right) + C' $$\nThis function has the form of a binomial log-likelihood. To maximize it, we set its derivative with respect to $p$ to zero:\n$$ \\frac{\\partial Q}{\\partial p} = \\frac{\\sum_i (\\omega_{i1}^{(t)} + 2\\omega_{i2}^{(t)})}{p} - \\frac{\\sum_i (2\\omega_{i0}^{(t)} + \\omega_{i1}^{(t)})}{1-p} = 0 $$\nSolving for $p$ yields the update rule for $p^{(t+1)}$:\n$$ p^{(t+1)} = \\frac{\\sum_{i=1}^N (\\omega_{i1}^{(t)} + 2\\omega_{i2}^{(t)})}{\\sum_{i=1}^N \\left( (\\omega_{i1}^{(t)} + 2\\omega_{i2}^{(t)}) + (2\\omega_{i0}^{(t)} + \\omega_{i1}^{(t)}) \\right)} $$\nSince $\\sum_{j=0}^2 \\omega_{ij}^{(t)} = 1$, the denominator simplifies to $\\sum_{i=1}^N 2(\\omega_{i0}^{(t)} + \\omega_{i1}^{(t)} + \\omega_{i2}^{(t)}) = 2N$.\nThus, the M-step update is:\n$$ p^{(t+1)} = \\frac{\\sum_{i=1}^N (\\omega_{i1}^{(t)} + 2\\omega_{i2}^{(t)})}{2N} $$\nThis expression is intuitive: the new estimate for the allele frequency is the expected total number of reference alleles divided by the total number of alleles in the sample.\n\n### 5. Algorithmic Summary\n\n1.  **Initialize**: Set an initial allele frequency $p^{(0)}$ and iteration counter $t=0$.\n2.  **Iterate**: Repeat for $t=0, 1, 2, \\dots$ until convergence:\n    a. **E-Step**: For each individual $i=1, \\dots, N$, compute the posterior genotype probabilities $\\omega_{i0}^{(t)}, \\omega_{i1}^{(t)}, \\omega_{i2}^{(t)}$ using the current estimate $p^{(t)}$ and the observed data $D_i$.\n    b. **M-Step**: Compute the updated allele frequency $p^{(t+1)}$ using the formula derived above.\n    c. **Check Convergence**: If $|p^{(t+1)} - p^{(t)}|$ falls below a tolerance $\\tau$ (e.g., $10^{-9}$), or if the maximum number of iterations $T_{\\max}$ is reached, terminate.\n    d. Set $p^{(t)} \\leftarrow p^{(t+1)}$ and continue to the next iteration.\n3.  **Result**: The final value $p^{(t+1)}$ is the MLE $\\hat{p}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef em_mle(epsilon, reads, p_init, tol=1e-9, max_iter=1000):\n    \"\"\"\n    Estimates the reference allele frequency p using the Expectation-Maximization (EM) algorithm.\n\n    Args:\n        epsilon (float): The symmetric sequencing error rate.\n        reads (list of tuples): A list of (reference_reads, a_lternative_reads) for each individual.\n        p_init (float): The initial value for the allele frequency p.\n        tol (float): The convergence tolerance.\n        max_iter (int): The maximum number of iterations.\n\n    Returns:\n        float: The maximum likelihood estimate of p.\n    \"\"\"\n    reads_np = np.array(reads, dtype=np.float64)\n    r = reads_np[:, 0]\n    a = reads_np[:, 1]\n    n_reads = r + a\n    \n    num_individuals = len(reads_np)\n    if num_individuals == 0:\n        return p_init\n\n    p_current = p_init\n    \n    # Pre-compute log probabilities related to epsilon, which are constant across iterations.\n    log_eps = np.log(epsilon)\n    log_1_minus_eps = np.log(1 - epsilon)\n    log_0_5 = np.log(0.5)\n\n    # Calculate the log genotype likelihoods, log P(D_i | g_i=j), for each individual.\n    # This is also constant across iterations.\n    # Shape: (num_individuals, 3), where columns correspond to genotypes g=0, 1, 2.\n    log_gls = np.zeros((num_individuals, 3))\n    # g=0 (0 ref alleles): probability of a reference read is epsilon\n    log_gls[:, 0] = r * log_eps + a * log_1_minus_eps\n    # g=1 (1 ref allele): probability of a reference read is 0.5\n    log_gls[:, 1] = n_reads * log_0_5\n    # g=2 (2 ref alleles): probability of a reference read is 1-epsilon\n    log_gls[:, 2] = r * log_1_minus_eps + a * log_eps\n\n    for i in range(max_iter):\n        # Clamp p_current to avoid log(0) issues if p approaches 0 or 1.\n        p_clipped = np.clip(p_current, 1e-12, 1 - 1e-12)\n\n        # --- E-step: Calculate posterior probabilities of genotypes (responsibilities) ---\n        \n        # Calculate log genotype priors, log P(g_i=j | p), based on current p.\n        log_priors = np.array([\n            2 * np.log(1 - p_clipped),                             # g=0: (1-p)^2\n            np.log(2) + np.log(p_clipped) + np.log(1 - p_clipped), # g=1: 2p(1-p)\n            2 * np.log(p_clipped)                                  # g=2: p^2\n        ])\n\n        # Calculate log joint probability, log P(D_i, g_i=j | p), by adding log-likelihoods and log-priors.\n        # NumPy's broadcasting adds the (3,) log_priors array to each row of the (N,3) log_gls array.\n        log_joints = log_gls + log_priors\n        \n        # Normalize in log-space using logsumexp to get log posteriors, log P(g_i=j | D_i, p).\n        log_marginal_likelihoods = logsumexp(log_joints, axis=1, keepdims=True)\n        log_posteriors = log_joints - log_marginal_likelihoods\n        \n        # Convert to linear space for the M-step.\n        posteriors = np.exp(log_posteriors)\n\n        # --- M-step: Update p by maximizing the expected complete-data log-likelihood ---\n        \n        # The expected number of reference alleles for genotype j is j.\n        # Total expected number of ref alleles is sum over individuals of E[#ref_alleles_i]\n        # E[#ref_alleles_i] = 0 * posterior(g=0) + 1 * posterior(g=1) + 2 * posterior(g=2)\n        expected_ref_alleles_sum = np.sum(posteriors[:, 1] + 2 * posteriors[:, 2])\n\n        p_next = expected_ref_alleles_sum / (2 * num_individuals)\n        \n        # Check for convergence.\n        if abs(p_next - p_current)  tol:\n            p_current = p_next\n            break\n            \n        p_current = p_next\n\n    return p_current\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: General mixture\n        {'epsilon': 0.01, 'reads': [(28,2),(2,28),(15,15),(18,2),(12,13),(0,10)], 'p_init': 0.5},\n        # Case 2: Boundary leaning to fixation\n        {'epsilon': 0.001, 'reads': [(40,0),(30,0),(25,0),(10,0),(50,0)], 'p_init': 0.4},\n        # Case 3: Heterozygote-like across cohort\n        {'epsilon': 0.02, 'reads': [(4,4),(5,5),(6,6),(3,3),(7,7),(10,10),(2,2),(15,15)], 'p_init': 0.1},\n        # Case 4: Includes zero-depth individuals\n        {'epsilon': 0.01, 'reads': [(0,0),(12,12),(0,30),(0,0),(25,25),(49,1)], 'p_init': 0.6},\n        # Case 5: High sequencing error\n        {'epsilon': 0.2, 'reads': [(18,12),(2,18),(10,10),(16,4)], 'p_init': 0.3},\n    ]\n\n    results = []\n    for case in test_cases:\n        p_hat = em_mle(case['epsilon'], case['reads'], case['p_init'])\n        results.append(p_hat)\n\n    # Format results to six decimal places for the final output.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```", "id": "4578121"}, {"introduction": "While powerful, standard Maximum Likelihood Estimation can fail in common practical scenarios, such as when a predictor perfectly separates binary outcomes in logistic regression. This exercise demonstrates how to diagnose this issue and implement a robust solution using penalized likelihood, specifically the method based on Jeffreys prior (Firth's regression). By implementing both the standard and penalized estimators from scratch, you will gain a deep, practical understanding of how to build more stable and reliable statistical models that gracefully handle the challenges of small or sparse datasets. [@problem_id:4578052]", "problem": "Consider binary outcomes in a clinical assay where each observation is modeled as a Bernoulli random variable with success probability linked to covariates by a Generalized Linear Model (GLM) with a logistic link. The fundamental base assumptions are: (i) conditional independence of observations given parameters; (ii) Bernoulli likelihood for each outcome; and (iii) the logistic link relating the linear predictor to the success probability. The standard maximum likelihood estimation (MLE) for logistic regression is obtained by maximizing the log-likelihood. However, with small samples, two issues can arise: (a) complete separation, where the MLE does not exist because the likelihood is unbounded in certain directions; and (b) small-sample bias, where finite-sample estimates are systematically biased. A penalized likelihood approach based on Jeffreys prior (often referred to as Firth’s bias-reduced logistic regression) modifies the objective to ensure existence and reduce bias.\n\nYour task is to implement from first principles:\n- An unpenalized logistic regression MLE via Newton–Raphson or Iteratively Reweighted Least Squares (IRLS), starting from the Bernoulli likelihood definition and the logistic link, and detecting non-existence or failure to converge in the presence of separation.\n- A penalized likelihood estimator using Jeffreys prior, constructed by adding a penalty equal to one half the logarithm of the determinant of the observed Fisher information to the Bernoulli log-likelihood, and solved by an iterative scheme derived from the corresponding penalized score equations. The estimator must remain finite under complete separation and must reduce small-sample bias relative to the standard MLE.\n\nUse the following definitions as the sole starting point:\n- For observation $i \\in \\{1,\\dots,n\\}$ with covariate row vector $\\mathbf{x}_i \\in \\mathbb{R}^{p}$ and coefficient vector $\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}$, the Bernoulli likelihood is $L_i(\\boldsymbol{\\beta}) = \\pi_i^{y_i} (1 - \\pi_i)^{1 - y_i}$, with $\\pi_i = \\Pr(Y_i = 1 \\mid \\mathbf{x}_i)$. The logistic link imposes $\\operatorname{logit}(\\pi_i) = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$, so that $\\pi_i = \\frac{1}{1 + e^{-\\mathbf{x}_i^\\top \\boldsymbol{\\beta}}}$.\n- The full (unpenalized) log-likelihood is $\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left[ y_i \\log \\pi_i + (1 - y_i) \\log(1 - \\pi_i) \\right]$.\n- The observed Fisher information matrix for the logistic model is $\\mathbf{I}(\\boldsymbol{\\beta}) = \\mathbf{X}^\\top \\mathbf{W}(\\boldsymbol{\\beta}) \\mathbf{X}$, where $\\mathbf{X}$ is the $n \\times p$ design matrix with rows $\\mathbf{x}_i^\\top$, and $\\mathbf{W}(\\boldsymbol{\\beta})$ is diagonal with entries $w_i(\\boldsymbol{\\beta}) = \\pi_i (1 - \\pi_i)$.\n\nFrom these base definitions, derive the iterative updates you will implement for both the unpenalized and penalized problems. Do not use any pre-packaged optimization routine for logistic regression; instead, construct Newton or IRLS updates explicitly, and for the penalized approach, construct the penalized score and a practical algorithm to solve it.\n\nScientific realism constraints:\n- All simulated data must be small-sample and plausible for a binary clinical assay.\n- Any claim of non-existence of the MLE must be justified by the behavior of the likelihood under separation, identified through your algorithm by singularity or divergence indicators arising from the Fisher information and score.\n\nImplement a single program that:\n- Builds three test cases (each includes an intercept term):\n    1. Complete separation (one predictor): $n = 8$, $\\mathbf{x} = [-2.0, -1.0, -0.5, -0.1, 0.1, 0.5, 1.0, 2.0]$, $\\mathbf{y} = [0, 0, 0, 0, 1, 1, 1, 1]$.\n    2. Non-separated small sample (one predictor): $n = 8$, $\\mathbf{x} = [-1.5, -0.9, -0.5, 0.0, 0.2, 0.4, 1.0, 1.3]$, $\\mathbf{y} = [0, 0, 1, 0, 0, 1, 1, 1]$.\n    3. Boundary case with only an intercept (no slope): $n = 6$, $\\mathbf{y} = [0, 0, 0, 0, 0, 0]$.\n- For each case:\n    - Fit the unpenalized logistic regression via Newton/IRLS.\n    - Declare a Boolean indicator of MLE success, defined as convergence within a fixed iteration budget to a finite parameter vector where the linear system is well-conditioned, and with no coefficient magnitude exceeding a large threshold. If this does not occur (e.g., due to divergence, ill-conditioning, or failure to meet tolerance), declare failure.\n    - Fit the penalized likelihood estimator using Jeffreys prior and return the estimated coefficients.\n\nAlgorithmic requirements you must implement:\n- Unpenalized MLE Newton/IRLS updates derived from the Bernoulli log-likelihood and logistic link.\n- Penalized estimator derived from the penalized log-likelihood $\\ell_{\\text{pen}}(\\boldsymbol{\\beta}) = \\ell(\\boldsymbol{\\beta}) + \\frac{1}{2} \\log \\left|\\mathbf{I}(\\boldsymbol{\\beta})\\right|$, with corresponding iterative updates that rely only on $\\mathbf{X}$, $\\mathbf{y}$, and quantities defined above. Your derivation must start from the provided definitions and yield a stable algorithm that converges in all three cases.\n\nNumerical and output requirements:\n- Use deterministic data exactly as specified above; do not use any external randomness.\n- For each test case, return a list whose first element is a Boolean indicating whether the unpenalized MLE converged, and whose second element is a list of the penalized (Jeffreys prior) coefficient estimates in the order: intercept first, followed by slopes. Round all reported coefficient estimates to six decimal places.\n- Your program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets, where each element is the per-test-case list specified above. For example, the overall structure must be like $[ [\\cdot, [\\cdot,\\dots]], [\\cdot, [\\cdot,\\dots]], [\\cdot, [\\cdot,\\dots]] ]$.\n\nTest suite summary:\n- Case $1$ (complete separation): Expect unpenalized MLE to fail; penalized estimator to return finite coefficients.\n- Case $2$ (non-separated): Expect unpenalized MLE to succeed; penalized estimator to return finite coefficients close (but not identical) to unpenalized.\n- Case $3$ (all zeros, intercept-only): Expect unpenalized MLE to fail; penalized estimator to return a finite intercept.\n\nThe final printed output must be a single line with the aggregated list as specified. No units or angles are involved; all numerical answers are unitless real numbers.", "solution": "The user requests the implementation of two logistic regression estimators from first principles: the standard unpenalized Maximum Likelihood Estimator (MLE) and a penalized estimator using Jeffreys prior, also known as Firth's logistic regression. The implementation must handle small-sample issues like complete separation.\n\n### Problem Validation\nThe problem is well-defined and scientifically sound, resting on established principles of likelihood-based inference for Generalized Linear Models (GLMs). All mathematical definitions, data, and constraints are provided and are internally consistent. The problem is a standard, albeit advanced, exercise in computational statistics. It is valid.\n\n### Derivation of Algorithms\n\n#### 1. Unpenalized Maximum Likelihood Estimator (MLE)\n\nThe goal is to find the coefficient vector $\\boldsymbol{\\beta}$ that maximizes the log-likelihood function for a series of independent Bernoulli trials:\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left[ y_i \\log \\pi_i + (1 - y_i) \\log(1 - \\pi_i) \\right] $$\nwhere $\\pi_i = (1 + e^{-\\mathbf{x}_i^\\top \\boldsymbol{\\beta}})^{-1}$ is the probability of success for observation $i$, given its covariate vector $\\mathbf{x}_i$.\n\nWe use the Newton-Raphson method, which iteratively finds the maximum by updating the parameter estimate:\n$$ \\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - [\\mathbf{H}(\\boldsymbol{\\beta}^{(t)})]^{-1} \\mathbf{g}(\\boldsymbol{\\beta}^{(t)}) $$\nwhere $\\mathbf{g}(\\boldsymbol{\\beta})$ is the gradient (score vector) and $\\mathbf{H}(\\boldsymbol{\\beta})$ is the Hessian matrix of the log-likelihood.\n\n**Gradient (Score Vector):** The gradient of $\\ell(\\boldsymbol{\\beta})$ with respect to $\\beta_j$ is:\n$$ \\frac{\\partial \\ell}{\\partial \\beta_j} = \\sum_{i=1}^n (y_i - \\pi_i) x_{ij} $$\nIn vector form, this is $\\mathbf{g}(\\boldsymbol{\\beta}) = \\mathbf{X}^\\top (\\mathbf{y} - \\boldsymbol{\\pi})$, where $\\mathbf{X}$ is the $n \\times p$ design matrix, $\\mathbf{y}$ is the vector of outcomes, and $\\boldsymbol{\\pi}$ is the vector of probabilities.\n\n**Hessian Matrix:** The second derivative is:\n$$ \\frac{\\partial^2 \\ell}{\\partial \\beta_k \\partial \\beta_j} = -\\sum_{i=1}^n \\pi_i(1-\\pi_i) x_{ij} x_{ik} $$\nIn matrix form, $\\mathbf{H}(\\boldsymbol{\\beta}) = -\\mathbf{X}^\\top \\mathbf{W}(\\boldsymbol{\\beta}) \\mathbf{X}$, where $\\mathbf{W}(\\boldsymbol{\\beta})$ is an $n \\times n$ diagonal matrix with elements $w_i = \\pi_i(1-\\pi_i)$. The negative of the Hessian is the observed Fisher information matrix, $\\mathbf{I}(\\boldsymbol{\\beta}) = -\\mathbf{H}(\\boldsymbol{\\beta}) = \\mathbf{X}^\\top \\mathbf{W}(\\boldsymbol{\\beta}) \\mathbf{X}$.\n\n**Newton-Raphson Update:** Substituting the gradient and Hessian into the update rule gives:\n$$ \\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} + [\\mathbf{I}(\\boldsymbol{\\beta}^{(t)})]^{-1} \\mathbf{g}(\\boldsymbol{\\beta}^{(t)}) $$\n$$ \\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} + (\\mathbf{X}^\\top \\mathbf{W}^{(t)} \\mathbf{X})^{-1} \\mathbf{X}^\\top (\\mathbf{y} - \\boldsymbol{\\pi}^{(t)}) $$\nThis is equivalent to the Iteratively Reweighted Least Squares (IRLS) algorithm. In the presence of complete or quasi-complete separation, some $\\pi_i$ will approach $0$ or $1$, causing the corresponding weights $w_i$ to approach $0$. This leads to singularity of the Fisher information matrix $\\mathbf{I}$, and the coefficients diverge. The algorithm fails, which we detect by monitoring the condition number of $\\mathbf{I}$ and the magnitude of the coefficients $\\boldsymbol{\\beta}$.\n\n#### 2. Penalized Estimator (Firth's Logistic Regression)\n\nThis method maximizes a penalized log-likelihood, which incorporates Jeffreys prior:\n$$ \\ell_{\\text{pen}}(\\boldsymbol{\\beta}) = \\ell(\\boldsymbol{\\beta}) + \\frac{1}{2} \\log |\\mathbf{I}(\\boldsymbol{\\beta})| $$\nThe penalty term, $\\frac{1}{2} \\log |\\mathbf{I}(\\boldsymbol{\\beta})|$, prevents the likelihood from being unbounded in cases of separation, ensuring a finite estimate for $\\boldsymbol{\\beta}$ always exists.\n\nWe seek to solve the penalized score equation $\\mathbf{g}_{\\text{pen}}(\\boldsymbol{\\beta}) = \\nabla \\ell_{\\text{pen}}(\\boldsymbol{\\beta}) = \\mathbf{0}$. The penalized score is the sum of the standard score and the gradient of the penalty term.\n$$ \\mathbf{g}_{\\text{pen}}(\\boldsymbol{\\beta}) = \\mathbf{g}(\\boldsymbol{\\beta}) + \\nabla \\left( \\frac{1}{2} \\log |\\mathbf{I}(\\boldsymbol{\\beta})| \\right) $$\n\n**Gradient of the Penalty:** Using the formula for the derivative of a log-determinant, the $j$-th component of the penalty's gradient is:\n$$ \\frac{\\partial}{\\partial \\beta_j} \\left(\\frac{1}{2} \\log |\\mathbf{I}(\\boldsymbol{\\beta})|\\right) = \\frac{1}{2} \\operatorname{tr}\\left( \\mathbf{I}(\\boldsymbol{\\beta})^{-1} \\frac{\\partial \\mathbf{I}(\\boldsymbol{\\beta})}{\\partial \\beta_j} \\right) $$\nThe derivative of the Fisher information matrix is $\\frac{\\partial \\mathbf{I}}{\\partial \\beta_j} = \\mathbf{X}^\\top \\frac{\\partial \\mathbf{W}}{\\partial \\beta_j} \\mathbf{X}$. The derivative of a weight $w_i = \\pi_i(1-\\pi_i)$ is $\\frac{\\partial w_i}{\\partial \\beta_j} = (1-2\\pi_i)\\pi_i(1-\\pi_i)x_{ij} = (1-2\\pi_i)w_i x_{ij}$.\nSubstituting and simplifying using the cyclic property of the trace, we find:\n$$ \\frac{\\partial}{\\partial \\beta_j} \\left(\\frac{1}{2} \\log |\\mathbf{I}(\\boldsymbol{\\beta})|\\right) = \\frac{1}{2} \\sum_{i=1}^n h_{ii} (1-2\\pi_i) w_i x_{ij} $$\nwhere $h_{ii}$ are the diagonal elements of the \"hat\" matrix $\\mathbf{A} = \\mathbf{X} \\mathbf{I}(\\boldsymbol{\\beta})^{-1} \\mathbf{X}^\\top$. These are also known as leverage scores.\n\n**Penalized Score Vector:** The full penalized score vector is:\n$$ \\mathbf{g}_{\\text{pen}}(\\boldsymbol{\\beta}) = \\mathbf{X}^\\top (\\mathbf{y} - \\boldsymbol{\\pi}) + \\frac{1}{2} \\mathbf{X}^\\top \\mathbf{v}(\\boldsymbol{\\beta}) $$\nwhere $\\mathbf{v}(\\boldsymbol{\\beta})$ is a vector with elements $v_i = h_{ii} (1 - 2\\pi_i) w_i$.\n\n**Iterative Solution:** We solve $\\mathbf{g}_{\\text{pen}}(\\boldsymbol{\\beta}) = \\mathbf{0}$ using a Newton-type method, approximating the Hessian of the penalized log-likelihood with the observed Fisher information matrix $-\\mathbf{I}(\\boldsymbol{\\beta})$. This gives the update step:\n$$ \\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} + [\\mathbf{I}(\\boldsymbol{\\beta}^{(t)})]^{-1} \\mathbf{g}_{\\text{pen}}(\\boldsymbol{\\beta}^{(t)}) $$\nThis algorithm is guaranteed to converge to a finite estimate for $\\boldsymbol{\\beta}$ even in the test cases involving separation. For the intercept-only model with all outcomes being zero ($y=0$), the standard MLE for the intercept $\\beta_0$ is $-\\infty$. The penalized estimator yields a finite value corresponding to an effective probability of $\\pi = 1/(2n+2)$.\n\n### Implementation Strategy\n\nThe solution is implemented in Python using the `numpy` library.\n- A function `fit_mle` implements the unpenalized Newton-Raphson algorithm. It includes checks for convergence, coefficient divergence (magnitude exceeding a threshold), and ill-conditioning of the Fisher matrix to determine success or failure.\n- A function `fit_penalized` implements the iterative scheme for the penalized estimator. At each step, it computes the standard score and the penalty gradient to form the penalized score, then applies the Newton update.\n- A main routine sets up the three test cases as specified, calls both fitting functions for each case, and formats the results into the required single-line string output. Numerical stability is handled by clipping probabilities away from $0$ and $1$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_case(X, y):\n    \"\"\"\n    Fits both unpenalized and penalized logistic regression models.\n    \n    Args:\n        X (np.ndarray): Design matrix (n_samples, n_features).\n        y (np.ndarray): Response vector (n_samples,).\n        \n    Returns:\n        list: A list containing [mle_success (bool), penalized_coeffs (list)].\n    \"\"\"\n    # Part 1: Unpenalized Maximum Likelihood Estimation (MLE)\n    beta_mle = np.zeros(X.shape[1])\n    mle_success = False\n    max_iter = 100\n    tol = 1e-8\n    beta_threshold = 50.0  # Threshold to detect diverging coefficients\n    cond_threshold = 1e12 # Threshold for matrix condition number\n\n    mle_converged_in_time = False\n    try:\n        for i in range(max_iter):\n            eta = X @ beta_mle\n            pi = 1 / (1 + np.exp(-eta))\n            \n            # Clip for numerical stability\n            pi = np.clip(pi, 1e-10, 1 - 1e-10)\n            \n            w = pi * (1 - pi)\n            \n            # Gradient (score vector)\n            gradient = X.T @ (y - pi)\n            \n            # Fisher Information Matrix (using efficient multiplication)\n            I = X.T * w @ X\n            \n            # Check for singularity/ill-conditioning, a sign of separation\n            if np.linalg.cond(I) > cond_threshold:\n                mle_success = False\n                break\n            \n            # Solve for the update step using a stable solver\n            delta = np.linalg.solve(I, gradient)\n            beta_mle += delta\n            \n            # Check for coefficient divergence, another sign of separation\n            if np.any(np.abs(beta_mle) > beta_threshold):\n                mle_success = False\n                break\n            \n            # Check for convergence\n            if np.linalg.norm(delta)  tol:\n                mle_success = True\n                mle_converged_in_time = True\n                break\n    except np.linalg.LinAlgError:\n        # Failure due to singular matrix\n        mle_success = False\n        \n    if not mle_converged_in_time:\n        mle_success = False\n\n    # Part 2: Penalized Likelihood Estimator (Firth's method)\n    beta_pen = np.zeros(X.shape[1])\n    max_iter_pen = 100\n    tol_pen = 1e-8\n    \n    for _ in range(max_iter_pen):\n        beta_old = beta_pen.copy()\n        \n        eta = X @ beta_pen\n        pi = 1 / (1 + np.exp(-eta))\n        \n        # Clip for numerical stability\n        pi = np.clip(pi, 1e-10, 1 - 1e-10)\n        \n        w = pi * (1 - pi)\n        \n        # Fisher Information Matrix\n        I = X.T * w @ X\n        \n        try:\n            I_inv = np.linalg.inv(I)\n        except np.linalg.LinAlgError:\n            # Add a small ridge for stability if computationally singular\n            I_inv = np.linalg.inv(I + np.eye(I.shape[0]) * 1e-9)\n\n        # Leverages: diagonal of X @ I_inv @ X.T\n        h = np.sum((X @ I_inv) * X, axis=1)\n        \n        # Standard score vector\n        gradient = X.T @ (y - pi)\n        \n        # Gradient of the penalty term\n        penalty_gradient = 0.5 * X.T @ (h * (1 - 2 * pi))\n        \n        # Penalized score vector\n        penalized_gradient = gradient + penalty_gradient\n        \n        # Newton-Raphson update step\n        delta = I_inv @ penalized_gradient\n        beta_pen += delta\n        \n        if np.linalg.norm(beta_pen - beta_old)  tol_pen:\n            break\n            \n    # Round coefficients to six decimal places for output\n    pen_coeffs_rounded = [round(b, 6) for b in beta_pen]\n    \n    return [mle_success, pen_coeffs_rounded]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Case 1: Complete separation\n    x1 = np.array([-2.0, -1.0, -0.5, -0.1, 0.1, 0.5, 1.0, 2.0])\n    y1 = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n    X1 = np.c_[np.ones(x1.shape[0]), x1]\n\n    # Case 2: Non-separated small sample\n    x2 = np.array([-1.5, -0.9, -0.5, 0.0, 0.2, 0.4, 1.0, 1.3])\n    y2 = np.array([0, 0, 1, 0, 0, 1, 1, 1])\n    X2 = np.c_[np.ones(x2.shape[0]), x2]\n\n    # Case 3: Boundary case (all zeros, intercept-only)\n    y3 = np.array([0, 0, 0, 0, 0, 0])\n    X3 = np.ones((y3.shape[0], 1))\n    \n    test_cases = [\n        (X1, y1),\n        (X2, y2),\n        (X3, y3),\n    ]\n\n    results = []\n    for X, y in test_cases:\n        case_result = solve_case(X, y)\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The str() representation of a list of lists is '[[], [], []]'\n    # We remove spaces to match the implicit tight format requested.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n\n```", "id": "4578052"}]}