{"hands_on_practices": [{"introduction": "A common pitfall in statistical analysis is equating zero covariance with independence. This exercise challenges that notion by asking you to construct scenarios where gene expression and batch labels are uncorrelated but statistically dependent [@problem_id:4598782]. Mastering this distinction is crucial for identifying and mitigating complex, non-linear batch effects that simple correlation analyses might miss.", "problem": "A statistician is designing a simulation study to assess whether deconfounding procedures that match on first-order moments can miss batch effects in single-cell ribonucleic acid sequencing (scRNA-seq) data. Let $X$ denote a real-valued, normalized gene expression for a fixed gene across cells, and let $Y$ denote the batch label. The study considers $2$ batches. The statistician wants an explicit joint law for $(X,Y)$ such that $\\operatorname{Cov}(X,Y)=0$ but $X$ and $Y$ are not independent, thereby demonstrating that zero covariance does not imply no batch effect. Assume all random variables described below are defined on a common probability space and independence statements refer to probabilistic independence.\n\nUsing only fundamental definitions (e.g., the definition of covariance $\\operatorname{Cov}(X,Y)=\\mathbb{E}\\left[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])\\right]$ and independence, along with the law of total expectation), select all constructions that satisfy the goals of the study. Each option proposes a distributional construction for $(X,Y)$; unless otherwise stated, any auxiliary variables are independent of $Y$.\n\nA. Let $Y\\sim \\mathrm{Bernoulli}(0.5)$ and $W\\sim \\mathcal{N}(0,1)$ independent. Define $X=Y\\,W$.\n\nB. Let $Y$ take values in $\\{-1,1\\}$ with equal probability and $W\\sim \\mathcal{N}(0,1)$, independent of $Y$. Define $X=Y\\,W$.\n\nC. Let $Y\\sim \\mathrm{Bernoulli}(0.5)$ and $W\\sim \\mathcal{N}(0,1)$ independent. Define $X=W+Y$.\n\nD. Let $Y\\sim \\mathrm{Bernoulli}(0.5)$. Conditionally on $Y$, define $X\\mid(Y=0)\\sim \\mathcal{N}(0,1)$ and $X\\mid(Y=1)\\sim \\mathcal{N}(0,4)$.\n\nE. Let $Y\\sim \\mathrm{Bernoulli}(0.5)$. Conditionally on $Y$, define $X\\mid(Y=0)\\sim \\mathcal{N}(0,1)$ and $X\\mid(Y=1)\\sim \\mathrm{Laplace}(0,1/\\sqrt{2})$.\n\nF. Let $Y\\sim \\mathrm{Bernoulli}(0.5)$ and $X\\sim \\mathcal{N}(0,1)$, and $X$ and $Y$ are independent.\n\nSelect all that apply.", "solution": "The problem statement is found to be valid. It is scientifically grounded in probability theory and its application to bioinformatics, well-posed, and objective. It is a standard exercise in illustrating the difference between uncorrelatedness and independence. I will proceed with the solution.\n\nThe goal is to identify all constructions for a pair of random variables $(X,Y)$ that satisfy two conditions:\n1.  $X$ and $Y$ are not independent. This means the conditional distribution of $X$ given $Y$ depends on the value of $Y$.\n2.  The covariance between $X$ and $Y$ is zero. That is, $\\operatorname{Cov}(X,Y) = 0$.\n\nWe recall the definition of covariance: $\\operatorname{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]$. A powerful tool for evaluating these expectations is the law of total expectation: for any random variables $Z$ and $Y$, $\\mathbb{E}[Z] = \\mathbb{E}[\\mathbb{E}[Z|Y]]$.\n\nA general structure that leads to zero covariance but dependence is when the conditional expectation $\\mathbb{E}[X|Y]$ is constant, but the conditional distribution of $X$ given $Y$, $f_{X|Y}(x|y)$, is not constant with respect to $y$. If $\\mathbb{E}[X|Y] = c$ for some constant $c$, then $\\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X|Y]] = \\mathbb{E}[c] = c$. The covariance is then $\\operatorname{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y] = \\mathbb{E}[\\mathbb{E}[XY|Y]] - c\\mathbb{E}[Y] = \\mathbb{E}[Y\\mathbb{E}[X|Y]] - c\\mathbb{E}[Y] = \\mathbb{E}[Yc] - c\\mathbb{E}[Y] = c\\mathbb{E}[Y] - c\\mathbb{E}[Y] = 0$.\n\nWe will now analyze each option based on these criteria.\n\n### Option A\n**Construction**: Let $Y\\sim \\mathrm{Bernoulli}(0.5)$, so $Y$ takes values in $\\{0, 1\\}$ with probability $0.5$ for each. Let $W\\sim \\mathcal{N}(0,1)$ be an independent random variable. Define $X=Y\\,W$.\n\n**1. Independence**:\nWe examine the conditional distribution of $X$ given $Y$.\n- If $Y=0$, then $X = 0 \\cdot W = 0$. So, $P(X=0|Y=0) = 1$. This is a point mass distribution at $0$.\n- If $Y=1$, then $X = 1 \\cdot W = W$. So, $X|Y=1 \\sim \\mathcal{N}(0,1)$.\nSince the conditional distribution of $X$ for $Y=0$ is different from the conditional distribution for $Y=1$, $X$ and $Y$ are **not independent**.\n\n**2. Covariance**:\n- We first compute the expectations.\n- $\\mathbb{E}[Y] = 0 \\cdot P(Y=0) + 1 \\cdot P(Y=1) = 0 \\cdot 0.5 + 1 \\cdot 0.5 = 0.5$.\n- $\\mathbb{E}[W] = 0$ as $W \\sim \\mathcal{N}(0,1)$.\n- $\\mathbb{E}[X] = \\mathbb{E}[YW]$. Since $Y$ and $W$ are independent, $\\mathbb{E}[YW] = \\mathbb{E}[Y]\\mathbb{E}[W] = 0.5 \\cdot 0 = 0$.\n- $\\mathbb{E}[XY] = \\mathbb{E}[(YW)Y] = \\mathbb{E}[Y^2W]$. Since $Y$ is a Bernoulli random variable, $Y^2=Y$. Thus, $\\mathbb{E}[Y^2W] = \\mathbb{E}[YW]$. As calculated before, $\\mathbb{E}[YW]=0$.\n- $\\operatorname{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y] = 0 - (0)(0.5) = 0$.\n\nBoth conditions are met.\n**Verdict: Correct.**\n\n### Option B\n**Construction**: Let $Y$ take values in $\\{-1,1\\}$ with $P(Y=-1)=P(Y=1)=0.5$. Let $W\\sim \\mathcal{N}(0,1)$ be independent of $Y$. Define $X=Y\\,W$.\n\n**1. Independence**:\nWe examine the conditional distribution of $X$ given $Y$.\n- If $Y=1$, then $X = 1 \\cdot W = W$, so $X|Y=1 \\sim \\mathcal{N}(0,1)$.\n- If $Y=-1$, then $X = -1 \\cdot W = -W$. Since $W$ follows a standard normal distribution, which is symmetric about $0$, $-W$ also follows a standard normal distribution. So, $X|Y=-1 \\sim \\mathcal{N}(0,1)$.\nThe conditional distribution of $X$ given $Y$ is $\\mathcal{N}(0,1)$ regardless of the value of $Y$. Therefore, $X$ and $Y$ are **independent**. This violates the primary condition of the study.\n\n**2. Covariance**:\nFor independent random variables, the covariance is always $0$.\n\nThis construction does not satisfy the requirement that $X$ and $Y$ are not independent.\n**Verdict: Incorrect.**\n\n### Option C\n**Construction**: Let $Y\\sim \\mathrm{Bernoulli}(0.5)$ and $W\\sim \\mathcal{N}(0,1)$ independent. Define $X=W+Y$.\n\n**1. Independence**:\n- If $Y=0$, then $X = W \\sim \\mathcal{N}(0,1)$.\n- If $Y=1$, then $X = W+1 \\sim \\mathcal{N}(1,1)$.\nThe conditional distributions are different, so $X$ and $Y$ are **not independent**.\n\n**2. Covariance**:\n- $\\mathbb{E}[Y] = 0.5$.\n- $\\mathbb{E}[X] = \\mathbb{E}[W+Y] = \\mathbb{E}[W] + \\mathbb{E}[Y] = 0 + 0.5 = 0.5$.\n- $\\mathbb{E}[XY] = \\mathbb{E}[(W+Y)Y] = \\mathbb{E}[WY + Y^2] = \\mathbb{E}[WY] + \\mathbb{E}[Y^2]$.\n- Since $W$ and $Y$ are independent, $\\mathbb{E}[WY]=\\mathbb{E}[W]\\mathbb{E}[Y] = 0 \\cdot 0.5 = 0$.\n- For $Y \\sim \\mathrm{Bernoulli}(p)$, $\\mathbb{E}[Y^2] = p$. Here $p=0.5$, so $\\mathbb{E}[Y^2]=0.5$.\n- $\\mathbb{E}[XY] = 0 + 0.5 = 0.5$.\n- $\\operatorname{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y] = 0.5 - (0.5)(0.5) = 0.5 - 0.25 = 0.25$.\nThe covariance is not zero.\n\nThis construction does not satisfy the zero-covariance requirement.\n**Verdict: Incorrect.**\n\n### Option D\n**Construction**: Let $Y\\sim \\mathrm{Bernoulli}(0.5)$. Conditionally on $Y$, define $X\\mid(Y=0)\\sim \\mathcal{N}(0,1)$ and $X\\mid(Y=1)\\sim \\mathcal{N}(0,4)$.\n\n**1. Independence**:\nThe conditional distribution of $X$ given $Y=0$ is $\\mathcal{N}(0,1)$, while the conditional distribution of $X$ given $Y=1$ is $\\mathcal{N}(0,4)$. Since these distributions are different (they have different variances), $X$ and $Y$ are **not independent**.\n\n**2. Covariance**:\n- $\\mathbb{E}[Y] = 0.5$.\n- We find $\\mathbb{E}[X]$ using the law of total expectation: $\\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X|Y]]$.\n- $\\mathbb{E}[X|Y=0] = 0$ (mean of $\\mathcal{N}(0,1)$).\n- $\\mathbb{E}[X|Y=1] = 0$ (mean of $\\mathcal{N}(0,4)$).\n- The random variable $\\mathbb{E}[X|Y]$ is the constant $0$. Thus, $\\mathbb{E}[X] = \\mathbb{E}[0] = 0$.\n- We find $\\mathbb{E}[XY]$ using the law of total expectation: $\\mathbb{E}[XY] = \\mathbb{E}[\\mathbb{E}[XY|Y]] = \\mathbb{E}[Y \\mathbb{E}[X|Y]]$.\n- Since $\\mathbb{E}[X|Y]=0$, we have $\\mathbb{E}[XY] = \\mathbb{E}[Y \\cdot 0] = \\mathbb{E}[0] = 0$.\n- $\\operatorname{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y] = 0 - (0)(0.5) = 0$.\n\nBoth conditions are met.\n**Verdict: Correct.**\n\n### Option E\n**Construction**: Let $Y\\sim \\mathrm{Bernoulli}(0.5)$. Conditionally on $Y$, define $X\\mid(Y=0)\\sim \\mathcal{N}(0,1)$ and $X\\mid(Y=1)\\sim \\mathrm{Laplace}(0,1/\\sqrt{2})$.\n\n**1. Independence**:\nThe conditional distribution of $X$ is a Normal distribution when $Y=0$ and a Laplace distribution when $Y=1$. These are fundamentally different distributional families. Therefore, $X$ and $Y$ are **not independent**.\n\n**2. Covariance**:\n- $\\mathbb{E}[Y] = 0.5$.\n- We find $\\mathbb{E}[X]$:\n- $\\mathbb{E}[X|Y=0] = 0$ (mean of $\\mathcal{N}(0,1)$).\n- The Laplace distribution $\\mathrm{Laplace}(\\mu, b)$ has mean $\\mu$. Thus, $\\mathbb{E}[X|Y=1] = 0$ for $\\mathrm{Laplace}(0,1/\\sqrt{2})$.\n- The random variable $\\mathbb{E}[X|Y]$ is the constant $0$. Thus, $\\mathbb{E}[X] = \\mathbb{E}[0] = 0$.\n- We find $\\mathbb{E}[XY]$:\n- As in option D, $\\mathbb{E}[XY] = \\mathbb{E}[Y \\mathbb{E}[X|Y]] = \\mathbb{E}[Y \\cdot 0] = 0$.\n- $\\operatorname{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y] = 0 - (0)(0.5) = 0$.\n\nBoth conditions are met.\n**Verdict: Correct.**\n\n### Option F\n**Construction**: Let $Y\\sim \\mathrm{Bernoulli}(0.5)$ and $X\\sim \\mathcal{N}(0,1)$, and $X$ and $Y$ are independent.\n\n**1. Independence**:\nThe problem statement explicitly asserts that $X$ and $Y$ are independent. This violates the goal of the study, which is to find a case where $X$ and $Y$ are **not** independent.\n\nThis construction does not satisfy the non-independence requirement.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{ADE}$$", "id": "4598782"}, {"introduction": "The Receiver Operating Characteristic (ROC) curve is a cornerstone for evaluating the performance of biomarkers and diagnostic tests. This practice guides you through the analytical derivation of the ROC curve and its associated Area Under the Curve (AUC) for the widely-used binormal model [@problem_id:4598803]. By working from the fundamental definitions of Normal distributions to a final, elegant expression for AUC, you will gain a deep, quantitative understanding of how a biomarker's discriminative power is formally assessed.", "problem": "A continuous scalar biomarker $X$ is measured in a clinical cohort, and the binary disease state $Y \\in \\{0,1\\}$ denotes $Y=1$ for diseased and $Y=0$ for non-diseased. Assume class-conditional Normal distributions: $X \\mid Y=0 \\sim \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2})$ and $X \\mid Y=1 \\sim \\mathcal{N}(\\mu_{1}, \\sigma_{1}^{2})$, where $\\mu_{1} > \\mu_{0}$ and $\\sigma_{0} > 0$, $\\sigma_{1} > 0$. Let $\\pi \\in (0,1)$ denote the prevalence $P(Y=1)$, and $1-\\pi$ denote $P(Y=0)$. Let $f_{k}(x)$ denote the probability density function of $X \\mid Y=k$ for $k \\in \\{0,1\\}$. The Receiver Operating Characteristic (ROC) curve is defined for the threshold decision rule $\\delta_{t}(x) = \\mathbf{1}\\{x \\ge t\\}$, with false positive rate $u(t) = P(X \\ge t \\mid Y=0)$ and true positive rate $v(t) = P(X \\ge t \\mid Y=1)$, where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. The Area Under the Curve (AUC) is defined as the integral of the ROC curve between $0$ and $1$.\n\nStarting only from the fundamental definitions of the Normal density, the likelihood ratio, Bayes’ rule, and the definitions of false positive and true positive rates, perform the following derivations:\n\n1. Derive the explicit likelihood ratio $L(x) = \\frac{f_{1}(x)}{f_{0}(x)}$.\n2. Use Bayes’ rule to express the posterior odds $\\frac{P(Y=1 \\mid X=x)}{P(Y=0 \\mid X=x)}$ in terms of the prior odds $\\frac{\\pi}{1-\\pi}$ and the likelihood ratio $L(x)$.\n3. For the threshold rule $\\delta_{t}(x)$, derive $u(t)$ and $v(t)$ in terms of $\\mu_{0}$, $\\mu_{1}$, $\\sigma_{0}$, $\\sigma_{1}$, and the standard Normal cumulative distribution function $\\Phi(\\cdot)$.\n4. Eliminate the threshold $t$ to obtain a closed-form expression for the ROC curve $v$ as a function of $u \\in [0,1]$, expressed in terms of $\\mu_{0}$, $\\mu_{1}$, $\\sigma_{0}$, $\\sigma_{1}$, and the inverse standard Normal cumulative distribution function $\\Phi^{-1}(\\cdot)$.\n5. Compute the AUC analytically by integrating the ROC curve over $u \\in [0,1]$, and simplify the result to a closed-form expression in terms of $\\mu_{0}$, $\\mu_{1}$, $\\sigma_{0}$, $\\sigma_{1}$.\n\nExpress the final answer as a single closed-form analytic expression. No numerical approximation is required.", "solution": "The problem is valid as it is scientifically grounded in standard statistical theory, well-posed with all necessary information provided, and stated objectively. We proceed with the derivations as requested.\n\nThe fundamental definition of the probability density function (PDF) for a Normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$ is given by:\n$$f(x; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$$\nGiven the class-conditional distributions $X \\mid Y=0 \\sim \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2})$ and $X \\mid Y=1 \\sim \\mathcal{N}(\\mu_{1}, \\sigma_{1}^{2})$, the respective PDFs are:\n$$f_{0}(x) = \\frac{1}{\\sqrt{2\\pi\\sigma_{0}^2}} \\exp\\left(-\\frac{(x-\\mu_{0})^2}{2\\sigma_{0}^2}\\right)$$\n$$f_{1}(x) = \\frac{1}{\\sqrt{2\\pi\\sigma_{1}^2}} \\exp\\left(-\\frac{(x-\\mu_{1})^2}{2\\sigma_{1}^2}\\right)$$\n\n**1. Derivation of the Likelihood Ratio $L(x)$**\n\nThe likelihood ratio $L(x)$ is defined as the ratio of the class-conditional PDFs, $L(x) = \\frac{f_{1}(x)}{f_{0}(x)}$.\n$$L(x) = \\frac{\\frac{1}{\\sqrt{2\\pi\\sigma_{1}^2}} \\exp\\left(-\\frac{(x-\\mu_{1})^2}{2\\sigma_{1}^2}\\right)}{\\frac{1}{\\sqrt{2\\pi\\sigma_{0}^2}} \\exp\\left(-\\frac{(x-\\mu_{0})^2}{2\\sigma_{0}^2}\\right)}$$\nThis simplifies to:\n$$L(x) = \\frac{\\sigma_{0}}{\\sigma_{1}} \\exp\\left(-\\frac{(x-\\mu_{1})^2}{2\\sigma_{1}^2} + \\frac{(x-\\mu_{0})^2}{2\\sigma_{0}^2}\\right)$$\nThe expression in the exponent can be expanded and rearranged, but this form is sufficient for the definition.\n\n**2. Derivation of the Posterior Odds**\n\nBayes' rule states that the posterior probability of class $k$ given observation $x$ is:\n$$P(Y=k \\mid X=x) = \\frac{P(X=x \\mid Y=k) P(Y=k)}{P(X=x)}$$\nUsing the provided notation, this is:\n$$P(Y=k \\mid X=x) = \\frac{f_{k}(x) P(Y=k)}{\\sum_{j=0}^{1} f_{j}(x) P(Y=j)}$$\nThe posterior odds are the ratio of the posterior probabilities for $Y=1$ and $Y=0$:\n$$\\frac{P(Y=1 \\mid X=x)}{P(Y=0 \\mid X=x)} = \\frac{\\frac{f_{1}(x) P(Y=1)}{P(X=x)}}{\\frac{f_{0}(x) P(Y=0)}{P(X=x)}}$$\nThe common denominator $P(X=x)$ cancels out:\n$$\\frac{P(Y=1 \\mid X=x)}{P(Y=0 \\mid X=x)} = \\frac{f_{1}(x)}{f_{0}(x)} \\cdot \\frac{P(Y=1)}{P(Y=0)}$$\nThe prior probabilities are given as $P(Y=1) = \\pi$ and $P(Y=0) = 1-\\pi$. The ratio $\\frac{\\pi}{1-\\pi}$ is the prior odds. The ratio $\\frac{f_{1}(x)}{f_{0}(x)}$ is the likelihood ratio $L(x)$. Therefore, we have the relationship:\n$$\\frac{P(Y=1 \\mid X=x)}{P(Y=0 \\mid X=x)} = L(x) \\cdot \\frac{\\pi}{1-\\pi}$$\nThis demonstrates that the posterior odds are the product of the likelihood ratio and the prior odds.\n\n**3. Derivation of False Positive Rate $u(t)$ and True Positive Rate $v(t)$**\n\nThe false positive rate (FPR) is $u(t) = P(X \\ge t \\mid Y=0)$. Since $X \\mid Y=0 \\sim \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2})$, we can standardize the variable. Let $Z = \\frac{X-\\mu_{0}}{\\sigma_{0}}$, where $Z \\sim \\mathcal{N}(0,1)$.\n$$u(t) = P\\left(\\frac{X-\\mu_{0}}{\\sigma_{0}} \\ge \\frac{t-\\mu_{0}}{\\sigma_{0}} \\mid Y=0\\right) = P\\left(Z \\ge \\frac{t-\\mu_{0}}{\\sigma_{0}}\\right)$$\nIn terms of the standard Normal cumulative distribution function $\\Phi(z) = P(Z \\le z)$, we have $P(Z \\ge z) = 1 - P(Z  z) = 1 - \\Phi(z)$. Thus:\n$$u(t) = 1 - \\Phi\\left(\\frac{t-\\mu_{0}}{\\sigma_{0}}\\right)$$\nSimilarly, the true positive rate (TPR) is $v(t) = P(X \\ge t \\mid Y=1)$. Since $X \\mid Y=1 \\sim \\mathcal{N}(\\mu_{1}, \\sigma_{1}^{2})$, we standardize with respect to these parameters. Let $Z' = \\frac{X-\\mu_{1}}{\\sigma_{1}}$, where $Z' \\sim \\mathcal{N}(0,1)$.\n$$v(t) = P\\left(\\frac{X-\\mu_{1}}{\\sigma_{1}} \\ge \\frac{t-\\mu_{1}}{\\sigma_{1}} \\mid Y=1\\right) = P\\left(Z' \\ge \\frac{t-\\mu_{1}}{\\sigma_{1}}\\right)$$\n$$v(t) = 1 - \\Phi\\left(\\frac{t-\\mu_{1}}{\\sigma_{1}}\\right)$$\n\n**4. Derivation of the ROC Curve $v(u)$**\n\nThe ROC curve is parameterized by the threshold $t$. To find the relationship between $v$ and $u$, we must eliminate $t$. From the expression for $u(t)$:\n$$u = 1 - \\Phi\\left(\\frac{t-\\mu_{0}}{\\sigma_{0}}\\right) \\implies \\Phi\\left(\\frac{t-\\mu_{0}}{\\sigma_{0}}\\right) = 1-u$$\nApplying the inverse standard Normal CDF, $\\Phi^{-1}(\\cdot)$, to both sides:\n$$\\frac{t-\\mu_{0}}{\\sigma_{0}} = \\Phi^{-1}(1-u)$$\nUsing the symmetry property of the standard Normal distribution, $\\Phi^{-1}(1-u) = -\\Phi^{-1}(u)$:\n$$\\frac{t-\\mu_{0}}{\\sigma_{0}} = -\\Phi^{-1}(u) \\implies t = \\mu_{0} - \\sigma_{0}\\Phi^{-1}(u)$$\nNow, we substitute this expression for $t$ into the equation for $v(t)$:\n$$v(u) = 1 - \\Phi\\left(\\frac{(\\mu_{0} - \\sigma_{0}\\Phi^{-1}(u)) - \\mu_{1}}{\\sigma_{1}}\\right)$$\nRearranging the terms inside the $\\Phi$ function gives the closed-form expression for the ROC curve:\n$$v(u) = 1 - \\Phi\\left(\\frac{\\mu_{0}-\\mu_{1}}{\\sigma_{1}} - \\frac{\\sigma_{0}}{\\sigma_{1}}\\Phi^{-1}(u)\\right)$$\n\n**5. Analytical Computation of the AUC**\n\nThe Area Under the Curve (AUC) is the integral of the ROC curve $v(u)$ with respect to $u$ from $0$ to $1$:\n$$AUC = \\int_{0}^{1} v(u) \\, du = \\int_{0}^{1} \\left[1 - \\Phi\\left(\\frac{\\mu_{0}-\\mu_{1}}{\\sigma_{1}} - \\frac{\\sigma_{0}}{\\sigma_{1}}\\Phi^{-1}(u)\\right)\\right] du$$\nUsing the symmetry property $\\Phi(-z) = 1-\\Phi(z)$, we can write:\n$$1 - \\Phi\\left(\\frac{\\mu_{0}-\\mu_{1}}{\\sigma_{1}} - \\frac{\\sigma_{0}}{\\sigma_{1}}\\Phi^{-1}(u)\\right) = \\Phi\\left(-\\left(\\frac{\\mu_{0}-\\mu_{1}}{\\sigma_{1}} - \\frac{\\sigma_{0}}{\\sigma_{1}}\\Phi^{-1}(u)\\right)\\right) = \\Phi\\left(\\frac{\\mu_{1}-\\mu_{0}}{\\sigma_{1}} + \\frac{\\sigma_{0}}{\\sigma_{1}}\\Phi^{-1}(u)\\right)$$\nSo the integral becomes:\n$$AUC = \\int_{0}^{1} \\Phi\\left(\\frac{\\mu_{1}-\\mu_{0}}{\\sigma_{1}} + \\frac{\\sigma_{0}}{\\sigma_{1}}\\Phi^{-1}(u)\\right) du$$\nTo solve this integral, we perform a change of variables. Let $z = \\Phi^{-1}(u)$. This implies $u = \\Phi(z)$, and the differential is $du = \\phi(z)dz$, where $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-z^2/2)$ is the standard Normal PDF. The limits of integration change as follows: as $u \\to 0$, $z \\to -\\infty$; as $u \\to 1$, $z \\to \\infty$.\n$$AUC = \\int_{-\\infty}^{\\infty} \\Phi\\left(\\frac{\\mu_{1}-\\mu_{0}}{\\sigma_{1}} + \\frac{\\sigma_{0}}{\\sigma_{1}}z\\right) \\phi(z) dz$$\nThis integral is of the form $\\int_{-\\infty}^{\\infty} \\Phi(a+bz)\\phi(z)dz$ with $a = \\frac{\\mu_{1}-\\mu_{0}}{\\sigma_{1}}$ and $b = \\frac{\\sigma_{0}}{\\sigma_{1}}$. This integral evaluates to $\\Phi\\left(\\frac{a}{\\sqrt{1+b^2}}\\right)$. Let us formally demonstrate this. Consider two independent standard Normal random variables, $Z_{1}$ and $Z_{2}$. The integral represents the probability $P(Z_{2} \\le a+bZ_{1})$. Let's define a new random variable $W = Z_{2} - bZ_{1}$. As a linear combination of independent Normal variables, $W$ is also Normally distributed.\n$$E[W] = E[Z_{2}] - bE[Z_{1}] = 0 - b \\cdot 0 = 0$$\n$$Var(W) = Var(Z_{2}) + (-b)^2Var(Z_{1}) = 1 + b^2 \\cdot 1 = 1+b^2$$\nSo, $W \\sim \\mathcal{N}(0, 1+b^2)$. The probability $P(Z_{2} \\le a+bZ_{1})$ is equivalent to $P(W \\le a)$.\n$$P(W \\le a) = P\\left(\\frac{W - 0}{\\sqrt{1+b^2}} \\le \\frac{a - 0}{\\sqrt{1+b^2}}\\right)$$\nThe term on the left is a standard Normal variable, so:\n$$P(W \\le a) = \\Phi\\left(\\frac{a}{\\sqrt{1+b^2}}\\right)$$\nApplying this result to our AUC integral:\n$$AUC = \\Phi\\left( \\frac{\\frac{\\mu_{1}-\\mu_{0}}{\\sigma_{1}}}{\\sqrt{1 + \\left(\\frac{\\sigma_{0}}{\\sigma_{1}}\\right)^2}} \\right)$$\nNow, we simplify the argument of $\\Phi$:\n$$\\frac{\\frac{\\mu_{1}-\\mu_{0}}{\\sigma_{1}}}{\\sqrt{1 + \\frac{\\sigma_{0}^2}{\\sigma_{1}^2}}} = \\frac{\\frac{\\mu_{1}-\\mu_{0}}{\\sigma_{1}}}{\\sqrt{\\frac{\\sigma_{1}^2 + \\sigma_{0}^2}{\\sigma_{1}^2}}} = \\frac{\\frac{\\mu_{1}-\\mu_{0}}{\\sigma_{1}}}{\\frac{\\sqrt{\\sigma_{0}^2 + \\sigma_{1}^2}}{\\sigma_{1}}}$$\nSince $\\sigma_{1}  0$, this simplifies to:\n$$\\frac{\\mu_{1}-\\mu_{0}}{\\sqrt{\\sigma_{0}^2 + \\sigma_{1}^2}}$$\nTherefore, the final analytical expression for the AUC is:\n$$AUC = \\Phi\\left(\\frac{\\mu_{1}-\\mu_{0}}{\\sqrt{\\sigma_{0}^2 + \\sigma_{1}^2}}\\right)$$", "answer": "$$\\boxed{\\Phi\\left(\\frac{\\mu_{1}-\\mu_{0}}{\\sqrt{\\sigma_{0}^{2}+\\sigma_{1}^{2}}}\\right)}$$", "id": "4598803"}, {"introduction": "Modern bioinformatics heavily relies on hierarchical models to capture structured variability in complex datasets, from genomics to clinical assays. This practice provides hands-on experience with Bayesian hierarchical modeling by tasking you with implementing a Gibbs sampler for two common scenarios: a Beta-Binomial model for overdispersed count data and a Normal-Normal model for quantitative measurements [@problem_id:4598789]. This exercise will equip you with the foundational skills to fit sophisticated probabilistic models and perform robust uncertainty quantification in your own research.", "problem": "A clinical bioinformatics group is modeling two separate components of a molecular assay pipeline using probabilistic hierarchical models in order to quantify uncertainty and propagate it to downstream medical decision-making.\n\nPart A (Beta–Binomial over dispersed variant counts). For $I$ independent samples indexed by $i \\in \\{1,\\dots,I\\}$, let $X_i$ denote the number of sequencing reads supporting a variant at a prespecified locus in sample $i$, observed out of $M_i$ total reads. To capture between-sample heterogeneity in true variant prevalence, assume the following hierarchical model: for each $i$, given a latent prevalence parameter $\\theta_i \\in (0,1)$, $X_i \\mid \\theta_i \\sim \\mathrm{Binomial}(M_i,\\theta_i)$, and $\\theta_i \\sim \\mathrm{Beta}(\\alpha,\\beta)$ with fixed known hyperparameters $\\alpha  0$ and $\\beta  0$. In a study with $I=5$ samples, the realized data are $(X_1,M_1)=(27,50)$, $(X_2,M_2)=(45,80)$, $(X_3,M_3)=(38,60)$, $(X_4,M_4)=(22,40)$, and $(X_5,M_5)=(40,75)$, and the hyperparameters are $(\\alpha,\\beta)=(2.5,3.5)$.\n\nTask A. Starting from Bayes’ rule and the probability density functions of the Beta and Binomial distributions, derive the full conditional distribution of each $\\theta_i$ given all other unknowns and all data, and show that this conditional is conjugate. Then, use these full conditionals to specify a valid Gibbs sampling scheme that targets the joint posterior of $(\\theta_1,\\dots,\\theta_I)$ under the stated model. Present the Gibbs sampler as a sequence of conditional draws, each specified by a named distribution and its parameters in terms of $(\\alpha,\\beta)$ and the observed $(X_i,M_i)$.\n\nPart B (Normal–Normal with unknown variance for quantitative Polymerase Chain Reaction (qPCR) log-expression). For a single gene measured by quantitative Polymerase Chain Reaction (qPCR) on $n$ technical replicates under a fixed condition, let $y_1,\\dots,y_n$ denote the log-scale expression measurements. Assume the sampling model $y_i \\mid \\mu,\\sigma^2 \\overset{\\mathrm{iid}}{\\sim} \\mathcal{N}(\\mu,\\sigma^2)$ for all $i \\in \\{1,\\dots,n\\}$, where both $\\mu$ and $\\sigma^2$ are unknown. Place the conjugate Normal–Inverse-Gamma prior\n- $\\mu \\mid \\sigma^2 \\sim \\mathcal{N}(m_0,\\sigma^2/\\kappa_0)$ with $m_0 \\in \\mathbb{R}$ and $\\kappa_0  0$,\n- $\\sigma^2 \\sim \\mathrm{Inverse\\mbox{-}Gamma}(a_0,b_0)$ with shape $a_00$ and scale $b_00$ and density $p(\\sigma^2) = \\dfrac{b_0^{a_0}}{\\Gamma(a_0)} (\\sigma^2)^{-a_0-1} \\exp\\!\\big(-b_0/\\sigma^2\\big)$ for $\\sigma^20$.\n\nIn an experiment with $n=6$ replicates, the realized data are $y=(1.2,\\,0.9,\\,1.5,\\,1.1,\\,1.3,\\,0.8)$, and the prior hyperparameters are $(m_0,\\kappa_0,a_0,b_0)=(1.0,\\,2.0,\\,3.0,\\,0.5)$.\n\nTask B. Starting from Bayes’ rule and the probability density functions of the Normal and Inverse-Gamma distributions, derive the full conditional distributions $p(\\mu \\mid \\sigma^2,y)$ and $p(\\sigma^2 \\mid \\mu,y)$, and exhibit a Gibbs sampling scheme that alternately samples from these two conditionals. Then, analytically integrate out to obtain the closed-form posterior hyperparameters and compute the posterior mean $\\mathbb{E}[\\sigma^2 \\mid y]$ under the stated prior and data. Round your numerical answer for $\\mathbb{E}[\\sigma^2 \\mid y]$ to four significant figures. The final answer must be this single rounded number, with no units.", "solution": "The problem presents two independent tasks in Bayesian hierarchical modeling, both of which are standard and well-posed within the context of bioinformatics and medical data analytics. The models, data, and tasks are scientifically grounded, mathematically consistent, and contain all necessary information for a complete solution. Therefore, the problem is deemed valid.\n\n### Part A: Beta–Binomial Model\n\nThe model is defined for $I$ independent samples. For each sample $i \\in \\{1, \\dots, I\\}$:\nThe likelihood for the observed variant count $X_i$ out of $M_i$ total reads is given by a Binomial distribution, conditional on a latent prevalence parameter $\\theta_i$:\n$$X_i \\mid \\theta_i \\sim \\mathrm{Binomial}(M_i, \\theta_i)$$\nThe prior distribution for the latent prevalence $\\theta_i$ is a Beta distribution:\n$$\\theta_i \\sim \\mathrm{Beta}(\\alpha, \\beta)$$\nThe samples are independent, meaning the joint prior for $\\boldsymbol{\\theta} = (\\theta_1, \\dots, \\theta_I)$ is $p(\\boldsymbol{\\theta}) = \\prod_{i=1}^I p(\\theta_i)$, and the joint likelihood for $\\mathbf{X} = (X_1, \\dots, X_I)$ given $\\boldsymbol{\\theta}$ is $p(\\mathbf{X} \\mid \\boldsymbol{\\theta}, \\mathbf{M}) = \\prod_{i=1}^I p(X_i \\mid \\theta_i, M_i)$.\n\nThe task is to find the full conditional distribution for each $\\theta_i$, which is the distribution of $\\theta_i$ given all other parameters and all data, denoted as $p(\\theta_i \\mid \\boldsymbol{\\theta}_{-i}, \\mathbf{X}, \\mathbf{M})$.\n\nDue to the conditional independence structure of the model, the full conditional for $\\theta_i$ depends only on its corresponding data $(X_i, M_i)$ and its prior hyperparameters $(\\alpha, \\beta)$:\n$$p(\\theta_i \\mid \\boldsymbol{\\theta}_{-i}, \\mathbf{X}, \\mathbf{M}) \\propto p(X_i \\mid \\theta_i, M_i) p(\\theta_i \\mid \\alpha, \\beta)$$\nThe probability mass function for the Binomial likelihood is $p(X_i \\mid \\theta_i, M_i) \\propto \\theta_i^{X_i} (1-\\theta_i)^{M_i-X_i}$.\nThe probability density function for the Beta prior is $p(\\theta_i \\mid \\alpha, \\beta) \\propto \\theta_i^{\\alpha-1} (1-\\theta_i)^{\\beta-1}$.\nMultiplying the likelihood and prior kernels:\n$$p(\\theta_i \\mid X_i, M_i, \\alpha, \\beta) \\propto \\left( \\theta_i^{X_i} (1-\\theta_i)^{M_i-X_i} \\right) \\times \\left( \\theta_i^{\\alpha-1} (1-\\theta_i)^{\\beta-1} \\right)$$\n$$p(\\theta_i \\mid X_i, M_i, \\alpha, \\beta) \\propto \\theta_i^{X_i + \\alpha - 1} (1-\\theta_i)^{M_i - X_i + \\beta - 1}$$\nThis is the kernel of a Beta distribution. Since the posterior distribution (Beta) is in the same family as the prior distribution (Beta), the Beta prior is conjugate to the Binomial likelihood. The full conditional distribution for $\\theta_i$ is:\n$$\\theta_i \\mid X_i, M_i, \\alpha, \\beta \\sim \\mathrm{Beta}(X_i + \\alpha, M_i - X_i + \\beta)$$\n\nA Gibbs sampling scheme for the joint posterior $p(\\boldsymbol{\\theta} \\mid \\mathbf{X}, \\mathbf{M})$ involves iteratively drawing each parameter from its full conditional distribution. Since the full conditionals are independent of each other, the sampler consists of independent draws from the posterior of each $\\theta_i$. A valid scheme is as follows:\nInitialize $\\boldsymbol{\\theta}^{(0)} = (\\theta_1^{(0)}, \\dots, \\theta_I^{(0)})$. For iterations $t=1, 2, \\dots$:\n\\begin{itemize}\n    \\item Sample $\\theta_1^{(t)} \\sim \\mathrm{Beta}(X_1 + \\alpha, M_1 - X_1 + \\beta)$\n    \\item Sample $\\theta_2^{(t)} \\sim \\mathrm{Beta}(X_2 + \\alpha, M_2 - X_2 + \\beta)$\n    \\item $\\dots$\n    \\item Sample $\\theta_I^{(t)} \\sim \\mathrm{Beta}(X_I + \\alpha, M_I - X_I + \\beta)$\n\\end{itemize}\nUsing the given data and hyperparameters $(X_i, M_i)$ and $(\\alpha, \\beta)=(2.5, 3.5)$:\n\\begin{itemize}\n    \\item Sample $\\theta_1 \\mid \\text{data} \\sim \\mathrm{Beta}(27 + 2.5, 50 - 27 + 3.5) = \\mathrm{Beta}(29.5, 26.5)$\n    \\item Sample $\\theta_2 \\mid \\text{data} \\sim \\mathrm{Beta}(45 + 2.5, 80 - 45 + 3.5) = \\mathrm{Beta}(47.5, 38.5)$\n    \\item Sample $\\theta_3 \\mid \\text{data} \\sim \\mathrm{Beta}(38 + 2.5, 60 - 38 + 3.5) = \\mathrm{Beta}(40.5, 25.5)$\n    \\item Sample $\\theta_4 \\mid \\text{data} \\sim \\mathrm{Beta}(22 + 2.5, 40 - 22 + 3.5) = \\mathrm{Beta}(24.5, 21.5)$\n    \\item Sample $\\theta_5 \\mid \\text{data} \\sim \\mathrm{Beta}(40 + 2.5, 75 - 40 + 3.5) = \\mathrm{Beta}(42.5, 38.5)$\n\\end{itemize}\n\n### Part B: Normal–Normal with Unknown Variance\n\nThe model for $n$ log-expression measurements $y=(y_1, \\dots, y_n)$ is:\nLikelihood: $y_i \\mid \\mu, \\sigma^2 \\overset{\\mathrm{iid}}{\\sim} \\mathcal{N}(\\mu, \\sigma^2)$\nPrior: $p(\\mu, \\sigma^2) = p(\\mu \\mid \\sigma^2) p(\\sigma^2)$, where\n    $\\mu \\mid \\sigma^2 \\sim \\mathcal{N}(m_0, \\sigma^2/\\kappa_0)$\n    $\\sigma^2 \\sim \\mathrm{Inverse\\mbox{-}Gamma}(a_0, b_0)$\n\n**Full Conditionals for Gibbs Sampling**\n\n*   **Full conditional for $\\mu$**:\n    $p(\\mu \\mid \\sigma^2, y) \\propto p(y \\mid \\mu, \\sigma^2) p(\\mu \\mid \\sigma^2)$. The product of two Normal density kernels (as a function of $\\mu$) is another Normal density kernel. The resulting full conditional is $\\mu \\mid \\sigma^2, y \\sim \\mathcal{N}(m_n, \\sigma^2/\\kappa_n)$ with parameters:\n    $\\kappa_n = \\kappa_0 + n$\n    $m_n = \\frac{\\kappa_0 m_0 + n\\bar{y}}{\\kappa_0 + n}$\n\n*   **Full conditional for $\\sigma^2$**:\n    $p(\\sigma^2 \\mid \\mu, y) \\propto p(y, \\mu, \\sigma^2) = p(y \\mid \\mu, \\sigma^2)p(\\mu \\mid \\sigma^2)p(\\sigma^2)$. We collect all terms involving $\\sigma^2$:\n    $p(y \\mid \\mu, \\sigma^2) \\propto (\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum (y_i - \\mu)^2\\right)$\n    $p(\\mu \\mid \\sigma^2) \\propto (\\sigma^2)^{-1/2} \\exp\\left(-\\frac{\\kappa_0}{2\\sigma^2} (\\mu-m_0)^2\\right)$\n    $p(\\sigma^2) \\propto (\\sigma^2)^{-a_0-1} \\exp\\left(-\\frac{b_0}{\\sigma^2}\\right)$\n    Multiplying these gives:\n    $p(\\sigma^2 \\mid \\mu, y) \\propto (\\sigma^2)^{-(a_0 + n/2 + 1/2) - 1} \\exp\\left( -\\frac{1}{\\sigma^2} \\left[ b_0 + \\frac{1}{2}\\sum (y_i - \\mu)^2 + \\frac{\\kappa_0}{2}(\\mu - m_0)^2 \\right] \\right)$.\n    This is the kernel of an Inverse-Gamma distribution. The full conditional is:\n    $\\sigma^2 \\mid \\mu, y \\sim \\mathrm{Inverse\\mbox{-}Gamma}\\left(a_0 + \\frac{n+1}{2}, b_0 + \\frac{1}{2}\\sum_{i=1}^n (y_i - \\mu)^2 + \\frac{\\kappa_0}{2}(\\mu - m_0)^2\\right)$\n\n**Gibbs Sampler Specification**\nInitialize $\\mu^{(0)}, \\sigma^{2,(0)}$. For iterations $t=1, 2, \\dots$:\n1.  Sample $\\mu^{(t)} \\sim \\mathcal{N}\\left(\\frac{\\kappa_0 m_0 + n\\bar{y}}{\\kappa_0 + n}, \\frac{\\sigma^{2,(t-1)}}{\\kappa_0 + n}\\right)$.\n2.  Sample $\\sigma^{2,(t)} \\sim \\mathrm{Inverse\\mbox{-}Gamma}\\left(a_0 + \\frac{n+1}{2}, b_0 + \\frac{1}{2}\\sum_{i=1}^n (y_i - \\mu^{(t)})^2 + \\frac{\\kappa_0}{2}(\\mu^{(t)} - m_0)^2\\right)$.\n\n**Analytical Posterior Mean of $\\sigma^2$**\n\nTo compute $\\mathbb{E}[\\sigma^2 \\mid y]$, we use the marginal posterior distribution $\\sigma^2 \\mid y$. Since the Normal-Inverse-Gamma prior is conjugate to the Normal likelihood, the joint posterior $p(\\mu, \\sigma^2 \\mid y)$ is also Normal-Inverse-Gamma. The marginal posterior for $\\sigma^2$ is $\\sigma^2 \\mid y \\sim \\mathrm{Inverse\\mbox{-}Gamma}(a_n, b_n)$ with the following updated parameters:\n$a_n = a_0 + \\frac{n}{2}$\n$b_n = b_0 + \\frac{1}{2}\\left( \\sum_{i=1}^n(y_i-\\bar{y})^2 + \\frac{n\\kappa_0}{n+\\kappa_0}(\\bar{y}-m_0)^2 \\right)$\nThe expected value of a random variable $Z \\sim \\mathrm{Inverse\\mbox{-}Gamma}(a, b)$ is $\\mathbb{E}[Z] = \\frac{b}{a-1}$ (for $a>1$). Therefore, $\\mathbb{E}[\\sigma^2 \\mid y] = \\frac{b_n}{a_n - 1}$.\n\n**Calculation**\nData: $y=(1.2,\\,0.9,\\,1.5,\\,1.1,\\,1.3,\\,0.8)$, $n=6$.\nHyperparameters: $(m_0,\\kappa_0,a_0,b_0)=(1.0,\\,2.0,\\,3.0,\\,0.5)$.\nFirst, compute sample statistics:\n$\\bar{y} = \\frac{1}{6}(1.2+0.9+1.5+1.1+1.3+0.8) = \\frac{6.8}{6} = \\frac{17}{15}$.\n$\\sum_{i=1}^n (y_i - \\bar{y})^2 = \\sum y_i^2 - n\\bar{y}^2$.\n$\\sum y_i^2 = 1.2^2 + 0.9^2 + 1.5^2 + 1.1^2 + 1.3^2 + 0.8^2 = 1.44 + 0.81 + 2.25 + 1.21 + 1.69 + 0.64 = 8.04$.\n$n\\bar{y}^2 = 6 \\left(\\frac{17}{15}\\right)^2 = \\frac{578}{75}$.\n$\\sum (y_i - \\bar{y})^2 = 8.04 - \\frac{578}{75} = \\frac{201}{25} - \\frac{578}{75} = \\frac{603 - 578}{75} = \\frac{25}{75} = \\frac{1}{3}$.\n\nNext, compute posterior hyperparameters $a_n$ and $b_n$:\n$a_n = a_0 + \\frac{n}{2} = 3.0 + \\frac{6}{2} = 6$.\nThe second term in the $b_n$ update is:\n$\\frac{n\\kappa_0}{n+\\kappa_0}(\\bar{y}-m_0)^2 = \\frac{6 \\times 2.0}{6+2.0} \\left(\\frac{17}{15} - 1.0\\right)^2 = \\frac{12}{8} \\left(\\frac{2}{15}\\right)^2 = \\frac{3}{2} \\times \\frac{4}{225} = \\frac{6}{225} = \\frac{2}{75}$.\nNow, substitute the values into $b_n$:\n$b_n = 0.5 + \\frac{1}{2} \\left( \\frac{1}{3} + \\frac{2}{75} \\right) = \\frac{1}{2} + \\frac{1}{2} \\left( \\frac{25}{75} + \\frac{2}{75} \\right) = \\frac{1}{2} + \\frac{1}{2} \\left( \\frac{27}{75} \\right) = \\frac{1}{2} + \\frac{27}{150} = \\frac{75}{150} + \\frac{27}{150} = \\frac{102}{150} = \\frac{17}{25} = 0.68$.\n\nFinally, compute the posterior mean:\n$\\mathbb{E}[\\sigma^2 \\mid y] = \\frac{b_n}{a_n - 1} = \\frac{17/25}{6 - 1} = \\frac{17/25}{5} = \\frac{17}{125} = 0.136$.\nRounding to four significant figures gives $0.1360$.", "answer": "$$\n\\boxed{0.1360}\n$$", "id": "4598789"}]}