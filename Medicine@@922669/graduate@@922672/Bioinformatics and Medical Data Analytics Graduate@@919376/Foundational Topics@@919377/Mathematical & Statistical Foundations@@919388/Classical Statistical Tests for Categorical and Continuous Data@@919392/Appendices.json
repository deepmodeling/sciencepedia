{"hands_on_practices": [{"introduction": "A foundational task in clinical and biomedical research is comparing the effect of an intervention against a standard or control. This exercise guides you through the core principles of the two-sample $t$-test for continuous data, a cornerstone of statistical inference. Beyond simply reaching a conclusion about statistical significance, this practice emphasizes the importance of quantifying the magnitude of the effect by calculating and interpreting Cohen's $d$, providing a more complete picture of the therapeutic impact [@problem_id:4546742].", "problem": "A clinical biomarker study assesses whether a new anti-inflammatory therapy reduces circulating interleukin-6 concentration compared with standard care. Two independent cohorts are analyzed: patients randomized to therapy (group $T$) and patients randomized to standard care (group $C$). The biomarker is measured as a continuous variable in the same laboratory protocol across patients. Exploratory checks suggest that within each group, the biomarker distribution is approximately Gaussian, and variability appears comparable between groups, so it is reasonable to assume equal population variances for the purpose of inference.\n\nYou are given the following summary statistics:\n- Therapy group: sample size $n_{T} = 48$, sample mean $\\bar{x}_{T} = 4.7$, sample standard deviation $s_{T} = 1.2$.\n- Control group: sample size $n_{C} = 52$, sample mean $\\bar{x}_{C} = 5.3$, sample standard deviation $s_{C} = 1.1$.\n\nStarting from first principles that define the two-sample comparison as a standardized difference of means under a common-variance model, and using the Student’s $t$ distribution for inference under the null hypothesis of equal population means, do the following:\n1. Derive the test statistic used for comparing two independent Gaussian samples under the equal-variance assumption and compute its numerical value and degrees of freedom for these data, then state whether the evidence supports a reduction of the biomarker in the therapy group at a two-sided significance level $\\alpha = 0.05$.\n2. Define Cohen’s $d$ for two independent samples under the equal-variance assumption as a standardized mean difference constructed from the observed data and the common-variance model, and derive how to compute it from the given summary statistics. Then compute its numerical value and interpret its sign and magnitude in the clinical context.\n\nReport only the numerical value of Cohen’s $d$ as your final answer. Round your answer to four significant figures. Do not include any units in your final answer.", "solution": "The problem as stated constitutes a well-posed question in applied statistics, specifically within the domain of biostatistical hypothesis testing. All necessary data, conditions, and definitions are provided, and the problem is scientifically grounded and objective. It is therefore deemed valid and a formal solution can be constructed.\n\nThe analysis involves two primary tasks: conducting a two-sample $t$-test under the assumption of equal variances and calculating Cohen's $d$ as a measure of effect size.\n\n**Part 1: Two-Sample t-Test for Independent Samples**\n\nThe fundamental objective is to test the null hypothesis ($H_0$) that the population means of the circulating interleukin-6 concentration are equal for the therapy group ($T$) and the control group ($C$), against the alternative hypothesis ($H_A$) that they are not.\n$$H_0: \\mu_T = \\mu_C \\quad \\text{vs.} \\quad H_A: \\mu_T \\neq \\mu_C$$\nUnder the assumptions that the samples are independent and drawn from populations with approximately Gaussian distributions and equal variance ($\\sigma_T^2 = \\sigma_C^2 = \\sigma^2$), the appropriate test statistic follows a Student's $t$-distribution.\n\nThe test statistic is constructed by standardizing the observed difference in sample means, $\\bar{x}_T - \\bar{x}_C$. The standardizing factor is the standard error of this difference. The variance of the difference of two independent sample means is the sum of their individual variances:\n$$\\text{Var}(\\bar{x}_T - \\bar{x}_C) = \\text{Var}(\\bar{x}_T) + \\text{Var}(\\bar{x}_C) = \\frac{\\sigma^2}{n_T} + \\frac{\\sigma^2}{n_C} = \\sigma^2 \\left( \\frac{1}{n_T} + \\frac{1}{n_C} \\right)$$\nSince the common population variance $\\sigma^2$ is unknown, it must be estimated from the sample data. The best unbiased estimator for $\\sigma^2$ is the pooled sample variance, $s_p^2$, which combines the variance information from both samples, weighted by their degrees of freedom. It is derived as:\n$$s_p^2 = \\frac{(n_T - 1)s_T^2 + (n_C - 1)s_C^2}{(n_T - 1) + (n_C - 1)} = \\frac{(n_T - 1)s_T^2 + (n_C - 1)s_C^2}{n_T + n_C - 2}$$\nThe estimated standard error of the mean difference, $SE(\\bar{x}_T - \\bar{x}_C)$, is then:\n$$SE(\\bar{x}_T - \\bar{x}_C) = \\sqrt{s_p^2 \\left( \\frac{1}{n_T} + \\frac{1}{n_C} \\right)} = s_p \\sqrt{\\frac{1}{n_T} + \\frac{1}{n_C}}$$\nThe two-sample $t$-statistic is defined as the difference between the sample means, centered by the hypothesized population mean difference (which is $0$ under $H_0$), and scaled by the standard error:\n$$t = \\frac{(\\bar{x}_T - \\bar{x}_C) - (\\mu_T - \\mu_C)_0}{SE(\\bar{x}_T - \\bar{x}_C)} = \\frac{\\bar{x}_T - \\bar{x}_C}{s_p \\sqrt{\\frac{1}{n_T} + \\frac{1}{n_C}}}$$\nThis statistic follows a Student's $t$-distribution with degrees of freedom, $df$, equal to the denominator of the pooled variance formula:\n$$df = n_T + n_C - 2$$\nNow, let's compute these quantities using the provided data:\n- Therapy group ($T$): $n_{T} = 48$, $\\bar{x}_{T} = 4.7$, $s_{T} = 1.2$\n- Control group ($C$): $n_{C} = 52$, $\\bar{x}_{C} = 5.3$, $s_{C} = 1.1$\n\nFirst, we calculate the pooled variance $s_p^2$:\n$$s_p^2 = \\frac{(48-1)(1.2)^2 + (52-1)(1.1)^2}{48+52-2} = \\frac{47(1.44) + 51(1.21)}{98} = \\frac{67.68 + 61.71}{98} = \\frac{129.39}{98} \\approx 1.320306$$\nThe pooled standard deviation, $s_p$, is the square root of the pooled variance:\n$$s_p = \\sqrt{s_p^2} \\approx \\sqrt{1.320306} \\approx 1.149046$$\nNext, we compute the $t$-statistic:\n$$t = \\frac{4.7 - 5.3}{1.149046 \\sqrt{\\frac{1}{48} + \\frac{1}{52}}} = \\frac{-0.6}{1.149046 \\sqrt{0.020833... + 0.019230...}} = \\frac{-0.6}{1.149046 \\sqrt{0.040064...}} = \\frac{-0.6}{1.149046 \\times 0.200160...} \\approx \\frac{-0.6}{0.229986} \\approx -2.6088$$\nThe degrees of freedom for this test are:\n$$df = 48 + 52 - 2 = 98$$\nTo determine if the evidence supports a reduction at a significance level $\\alpha = 0.05$ for a two-sided test, we compare the absolute value of our calculated $t$-statistic to the critical value $t_{\\alpha/2, df} = t_{0.025, 98}$. For $df=98$, the critical value is approximately $t_{crit} \\approx 1.984$. Since $|t| = |-2.6088| \\approx 2.61 > 1.984$, we reject the null hypothesis $H_0$. The negative sign of the $t$-statistic indicates that the sample mean for the therapy group is lower than that of the control group. Therefore, at the $\\alpha = 0.05$ significance level, there is statistically significant evidence to support a reduction of the biomarker in the therapy group compared to the control group.\n\n**Part 2: Cohen's d for Effect Size**\n\nCohen's $d$ is a standardized effect size that measures the magnitude of the difference between two means. For two independent samples, under the same common-variance model used for the $t$-test, it is defined as the difference in sample means divided by the pooled standard deviation, $s_p$.\n$$d = \\frac{\\bar{x}_T - \\bar{x}_C}{s_p}$$\nThis formula naturally arises from the principle of expressing the mean difference in units of the common standard deviation, providing a scale-invariant measure of the effect's magnitude.\n\nUsing the values already computed:\n- Difference in means: $\\bar{x}_T - \\bar{x}_C = 4.7 - 5.3 = -0.6$\n- Pooled standard deviation: $s_p \\approx 1.149046$\n\nWe can now compute Cohen's $d$:\n$$d = \\frac{-0.6}{1.149046} \\approx -0.522168$$\nRounding this value to four significant figures as required gives $d \\approx -0.5222$.\n\nInterpretation:\n- The sign of Cohen's $d$ is negative, which indicates that the mean of the therapy group is lower than the mean of the control group. This aligns with the clinical hypothesis that the therapy reduces the biomarker concentration.\n- The magnitude, $|d| \\approx 0.52$, is typically interpreted as a \"medium\" effect size. This suggests that the difference in the mean biomarker concentration between the therapy and control groups is approximately half a pooled standard deviation, which is generally considered a meaningful-to-moderate effect in a clinical context.\n\nThe final answer required is the numerical value of Cohen's $d$.", "answer": "$$\\boxed{-0.5222}$$", "id": "4546742"}, {"introduction": "Statistical analyses must adapt to the underlying experimental design. This practice moves from independent samples to paired data, a common structure in studies that measure outcomes before and after an intervention on the same subjects. You will learn why standard chi-squared tests are inappropriate for such data and apply McNemar's test, which correctly accounts for the paired structure by focusing on the discordant pairs to evaluate significant changes in categorical outcomes [@problem_id:4546672].", "problem": "A clinical genomics laboratory evaluates whether an upgraded variant-calling pipeline changes the binary classification of \"pathogenic variant detected\" for a cohort of matched patients. For each patient, the pre-upgrade classification and the post-upgrade classification are recorded as either \"detected\" or \"not detected\". The matched $2 \\times 2$ table of paired responses is summarized as follows: $62$ patients were \"detected\" both pre-upgrade and post-upgrade, $18$ were \"detected\" pre-upgrade but \"not detected\" post-upgrade, $7$ were \"not detected\" pre-upgrade but \"detected\" post-upgrade, and $33$ were \"not detected\" both pre-upgrade and post-upgrade. Assume patients are independent and that pairing is valid.\n\nUsing the principle that, under the null hypothesis of marginal homogeneity (no change in detection probability pre- versus post-upgrade), the two types of discordant pairs are exchangeable, compute the McNemar test statistic without continuity correction and with continuity correction for the stated data. Then, interpret whether the upgrade changed detection probability at a significance level of $\\alpha = 0.05$ by referencing the appropriate large-sample distribution.\n\nRound both computed statistics to four significant figures. Report your final numeric results as a two-entry row vector $(T, T_{\\mathrm{cc}})$, where $T$ is the McNemar statistic without continuity correction and $T_{\\mathrm{cc}}$ is the statistic with continuity correction. Do not include any units in your final reported numbers.", "solution": "The problem requires the computation and interpretation of the McNemar test statistic for a set of paired categorical data. The first step is to formally validate the problem statement.\n\n### Step 1: Extract Givens\n- Data type: Paired binary classifications (\"detected\" vs. \"not detected\") for a cohort of matched patients.\n- Pre-upgrade vs. Post-upgrade counts are provided in a $2 \\times 2$ table format:\n  - Patients \"detected\" pre- and post-upgrade: $62$.\n  - Patients \"detected\" pre-upgrade and \"not detected\" post-upgrade: $18$.\n  - Patients \"not detected\" pre-upgrade and \"detected\" post-upgrade: $7$.\n  - Patients \"not detected\" pre- and post-upgrade: $33$.\n- Assumptions: Patients are independent; pairing is valid.\n- Null Hypothesis ($H_0$): Marginal homogeneity, meaning no change in detection probability pre- versus post-upgrade.\n- Statistical Principle: Under $H_0$, the two types of discordant pairs are exchangeable.\n- Required Computations:\n  1. McNemar test statistic without continuity correction ($T$).\n  2. McNemar test statistic with continuity correction ($T_{\\mathrm{cc}}$).\n- Interpretation: Determine if the upgrade changed detection probability at a significance level of $\\alpha = 0.05$, referencing the appropriate large-sample distribution.\n- Reporting Format: Round both statistics to four significant figures and report as a two-entry row vector $(T, T_{\\mathrm{cc}})$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is scientifically sound. McNemar's test is the standard statistical procedure for comparing paired dichotomous data, which is a frequent task in clinical and bioinformatics research, such as evaluating diagnostic tests or computational pipelines. The concept of marginal homogeneity and the exchangeability of discordant pairs under the null hypothesis are the correct theoretical foundations for this test.\n- **Well-Posed:** The problem is well-posed. It provides all necessary numerical data (the counts for the $2 \\times 2$ table) and specifies the exact calculations and the significance level for interpretation. The request for a large-sample approximation clearly points to the chi-squared distribution, which is standard practice. A unique solution is attainable.\n- **Objective:** The problem is stated in objective, quantitative terms, free from ambiguity or subjective claims.\n\nThe problem does not exhibit any of the flaws listed in the validation checklist (e.g., scientific unsoundness, incompleteness, contradiction, etc.). It is a standard and well-defined biostatistics problem.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\nLet us organize the provided data into a standard $2 \\times 2$ contingency table. Let the pre-upgrade classification be the rows and the post-upgrade classification be the columns.\n\n|                  | Post-Upgrade: Detected | Post-Upgrade: Not Detected | Row Total |\n|------------------|:----------------------:|:--------------------------:|:---------:|\n| **Pre-Upgrade: Detected**    | $a = 62$               | $b = 18$                   | $a+b=80$  |\n| **Pre-Upgrade: Not Detected**  | $c = 7$                | $d = 33$                   | $c+d=40$  |\n| **Column Total** | $a+c=69$               | $b+d=51$                   | $n=120$   |\n\nThe cells $a$ and $d$ represent concordant pairs, where the classification did not change. The cells $b$ and $c$ represent discordant pairs, where the classification changed between the pre- and post-upgrade pipelines. McNemar's test focuses exclusively on these discordant pairs.\n- $b = 18$: Number of patients who switched from \"detected\" to \"not detected\".\n- $c = 7$: Number of patients who switched from \"not detected\" to \"detected\".\n\nThe total number of discordant pairs is $b + c = 18 + 7 = 25$.\n\nThe null hypothesis ($H_0$) of marginal homogeneity states that the probability of a \"detected\" result is the same for both pipelines. This implies that the expected number of subjects switching from \"detected\" to \"not detected\" is equal to the expected number of subjects switching from \"not detected\" to \"detected\". The test evaluates whether the observed counts, $b$ and $c$, deviate significantly from this expectation.\n\n**1. McNemar Test Statistic without Continuity Correction ($T$)**\n\nThe formula for the McNemar statistic is:\n$$ T = \\frac{(b - c)^2}{b + c} $$\nSubstituting the observed values $b=18$ and $c=7$:\n$$ T = \\frac{(18 - 7)^2}{18 + 7} = \\frac{11^2}{25} = \\frac{121}{25} = 4.84 $$\n\n**2. McNemar Test Statistic with Continuity Correction ($T_{\\mathrm{cc}}$)**\n\nThe Yates' continuity correction is applied to better approximate the discrete binomial distribution of discordant pairs with the continuous chi-squared distribution. The formula is:\n$$ T_{\\mathrm{cc}} = \\frac{(|b - c| - 1)^2}{b + c} $$\nSubstituting the observed values:\n$$ T_{\\mathrm{cc}} = \\frac{(|18 - 7| - 1)^2}{18 + 7} = \\frac{(11 - 1)^2}{25} = \\frac{10^2}{25} = \\frac{100}{25} = 4.00 $$\n\n**3. Interpretation**\n\nFor a large number of discordant pairs (typically $b+c > 20$), the McNemar test statistic under the null hypothesis follows a chi-squared distribution with $1$ degree of freedom ($\\chi^2_1$). In this case, $b+c = 25$, so the large-sample approximation is appropriate.\n\nWe need to compare our computed statistics to the critical value from the $\\chi^2_1$ distribution at the given significance level, $\\alpha = 0.05$. The critical value, $\\chi^2_{1, 0.05}$, is the value that a $\\chi^2_1$ random variable exceeds with probability $0.05$. This value is $3.841$.\n\n- For the statistic without continuity correction: $T = 4.84$. Since $T > 3.841$, the result is statistically significant. We reject the null hypothesis of marginal homogeneity.\n- For the statistic with continuity correction: $T_{\\mathrm{cc}} = 4.00$. Since $T_{\\mathrm{cc}} > 3.841$, this result is also statistically significant. We reject the null hypothesis.\n\nBoth tests lead to the same conclusion: at the $\\alpha = 0.05$ significance level, there is sufficient evidence to conclude that the upgrade to the variant-calling pipeline has resulted in a statistically significant change in the probability of detecting a pathogenic variant.\n\n**4. Final Answer Formatting**\n\nThe problem requires rounding both computed statistics to four significant figures and presenting them as a row vector.\n- $T = 4.84$ rounded to four significant figures is $4.840$.\n- $T_{\\mathrm{cc}} = 4.00$ rounded to four significant figures is $4.000$.\n\nThe final result is the row vector $(4.840, 4.000)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n4.840 & 4.000\n\\end{pmatrix}\n}\n$$", "id": "4546672"}, {"introduction": "Modern bioinformatics is characterized by high-throughput experiments where thousands of hypotheses are tested simultaneously, such as in differential gene expression analysis. This practice addresses the critical challenge of multiple testing, where the risk of making false discoveries (Type I errors) is greatly inflated. You will implement the Holm step-down procedure, a powerful and widely used method to control the Family-Wise Error Rate (FWER) and ensure that the findings from large-scale analyses are statistically robust [@problem_id:4546854].", "problem": "In a differential gene expression analysis within a clinical oncology study, you test pre-specified pathway-level contrasts for changes in mean expression between tumor and matched normal tissues. There are $m = 13$ pathways, and for each pathway you compute a valid two-sided $p$-value from a linear model, where \"valid\" means that under any true null hypothesis, the $p$-value is stochastically no smaller than Uniform$(0,1)$ and thus satisfies the standard calibration used in controlling the Family-Wise Error Rate (FWER). The study controls the FWER at level $\\alpha = 0.05$ using the Holm step-down approach.\n\nThe $p$-values have been computed and ordered from smallest to largest as follows:\n$$\np_{(1)} = 0.0008,\\quad\np_{(2)} = 0.0019,\\quad\np_{(3)} = 0.0039,\\quad\np_{(4)} = 0.0054,\\quad\np_{(5)} = 0.0061,\\quad\np_{(6)} = 0.0068,\\quad\np_{(7)} = 0.0076,\\quad\np_{(8)} = 0.0085,\\quad\np_{(9)} = 0.0108,\\quad\np_{(10)} = 0.0130,\\quad\np_{(11)} = 0.0170,\\quad\np_{(12)} = 0.0290,\\quad\np_{(13)} = 0.1200.\n$$\n\nStarting from first principles suitable for bioinformatics and medical data analytics and classical statistical testing, execute Holm’s step-down procedure at level $\\alpha$ on the ordered $p$-values and determine the total number of null hypotheses rejected. Express your final answer as a single integer. No rounding is required.", "solution": "The user wants me to solve this problem.\nThe problem statement is first validated against the required criteria.\n\n### Step 1: Extract Givens\n-   Number of hypotheses (pathways), $m = 13$.\n-   Family-Wise Error Rate (FWER) control level, $\\alpha = 0.05$.\n-   Multiple testing correction procedure: Holm step-down approach.\n-   The $p$-values are valid for FWER control.\n-   The ordered $p$-values are:\n    $p_{(1)} = 0.0008$\n    $p_{(2)} = 0.0019$\n    $p_{(3)} = 0.0039$\n    $p_{(4)} = 0.0054$\n    $p_{(5)} = 0.0061$\n    $p_{(6)} = 0.0068$\n    $p_{(7)} = 0.0076$\n    $p_{(8)} = 0.0085$\n    $p_{(9)} = 0.0108$\n    $p_{(10)} = 0.0130$\n    $p_{(11)} = 0.0170$\n    $p_{(12)} = 0.0290$\n    $p_{(13)} = 0.1200$\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is based on standard statistical practices in bioinformatics and medical research. Differential gene expression, pathway analysis, $p$-values, FWER control, and the Holm-Bonferroni method are all well-established, fundamental concepts in statistical genetics and data analytics. The premises are sound.\n-   **Well-Posed**: The problem provides a complete set of data ($p$-values, $m$, $\\alpha$) and a well-defined algorithm (Holm's procedure) to be applied. It asks for a single, uniquely determinable integer value (the number of rejected hypotheses). The solution is unique and stable.\n-   **Objective**: The problem is stated using precise, quantitative language. There are no subjective or ambiguous terms.\n-   **Conclusion**: The problem is free of scientific unsoundness, is not non-formalizable, has a complete and consistent setup, is realistic, and is well-posed. It is a standard application of a statistical method.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\nThe problem requires the application of the Holm step-down procedure to a set of $p$-values to determine the number of statistically significant results while controlling the Family-Wise Error Rate (FWER). The FWER is the probability of making one or more Type I errors (false rejections) among all hypotheses. The goal is to keep this probability at or below a specified level $\\alpha$.\n\nThe Holm procedure, also known as the Holm-Bonferroni method, is a sequentially rejective multiple testing procedure that is uniformly more powerful than the classical Bonferroni correction. It operates on the ordered $p$-values.\n\nLet the $m$ null hypotheses be $H_1, H_2, \\dots, H_m$, and their corresponding $p$-values be $p_1, p_2, \\dots, p_m$.\nThe procedure is as follows:\n1.  Order the $p$-values from smallest to largest: $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$. Let $H_{(1)}, H_{(2)}, \\dots, H_{(m)}$ be the corresponding null hypotheses.\n2.  The procedure tests the hypotheses sequentially. For $j = 1, 2, \\dots, m$, the $j$-th ordered $p$-value, $p_{(j)}$, is compared with an adjusted significance level of $\\frac{\\alpha}{m-j+1}$.\n3.  The process starts with $j=1$. We compare $p_{(1)}$ with $\\frac{\\alpha}{m-1+1} = \\frac{\\alpha}{m}$. If $p_{(1)} \\le \\frac{\\alpha}{m}$, we reject $H_{(1)}$ and proceed to test $H_{(2)}$. If $p_{(1)} > \\frac{\\alpha}{m}$, we fail to reject any hypotheses, and the procedure stops.\n4.  In general, for step $j$, if all previous hypotheses $H_{(1)}, \\dots, H_{(j-1)}$ have been rejected, we compare $p_{(j)}$ with $\\frac{\\alpha}{m-j+1}$.\n    -   If $p_{(j)} \\le \\frac{\\alpha}{m-j+1}$, we reject $H_{(j)}$ and proceed to step $j+1$.\n    -   If $p_{(j)} > \\frac{\\alpha}{m-j+1}$, we fail to reject $H_{(j)}$ and all subsequent hypotheses $H_{(j+1)}, \\dots, H_{(m)}$. The procedure stops.\n\nThe total number of rejected hypotheses is the largest integer $k$ such that for all $j \\in \\{1, 2, \\dots, k\\}$, the inequality $p_{(j)} \\le \\frac{\\alpha}{m-j+1}$ holds.\n\nIn this problem, we are given:\n-   Number of hypotheses: $m = 13$.\n-   FWER control level: $\\alpha = 0.05$.\n-   The ordered $p$-values are provided.\n\nWe will now apply the procedure step-by-step.\n\nFor $j=1$:\nCompare $p_{(1)} = 0.0008$ with $\\frac{\\alpha}{m-1+1} = \\frac{0.05}{13} \\approx 0.003846$.\nSince $0.0008 \\le 0.003846$, we reject the null hypothesis $H_{(1)}$.\n\nFor $j=2$:\nCompare $p_{(2)} = 0.0019$ with $\\frac{\\alpha}{m-2+1} = \\frac{0.05}{12} \\approx 0.004167$.\nSince $0.0019 \\le 0.004167$, we reject the null hypothesis $H_{(2)}$.\n\nFor $j=3$:\nCompare $p_{(3)} = 0.0039$ with $\\frac{\\alpha}{m-3+1} = \\frac{0.05}{11} \\approx 0.004545$.\nSince $0.0039 \\le 0.004545$, we reject the null hypothesis $H_{(3)}$.\n\nFor $j=4$:\nCompare $p_{(4)} = 0.0054$ with $\\frac{\\alpha}{m-4+1} = \\frac{0.05}{10} = 0.0050$.\nSince $0.0054 > 0.0050$, we fail to reject the null hypothesis $H_{(4)}$.\n\nAccording to the rules of the Holm step-down procedure, the process stops at the first failure to reject. Therefore, we do not reject $H_{(4)}$ or any of the subsequent hypotheses $H_{(5)}, \\dots, H_{(13)}$.\n\nThe hypotheses that are rejected are $H_{(1)}$, $H_{(2)}$, and $H_{(3)}$.\nThe total number of rejected null hypotheses is $3$.", "answer": "$$\n\\boxed{3}\n$$", "id": "4546854"}]}