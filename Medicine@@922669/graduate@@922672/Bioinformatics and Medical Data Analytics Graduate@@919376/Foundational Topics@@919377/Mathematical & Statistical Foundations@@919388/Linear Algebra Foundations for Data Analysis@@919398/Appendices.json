{"hands_on_practices": [{"introduction": "A primary task in analyzing high-dimensional biological data, such as gene expression profiles, is to distill its essential structure. This exercise guides you through the foundational principles of Principal Component Analysis (PCA) by having you compute the proportion of variance explained by the leading singular values of a data matrix [@problem_id:4578458]. By implementing this from first principles, you will build a concrete understanding of how the Singular Value Decomposition (SVD) provides a powerful tool to quantify information content and achieve effective dimensionality reduction.", "problem": "You are given column-oriented gene expression matrices that are used in bioinformatics and medical data analytics to summarize messenger ribonucleic acid (mRNA) abundance across samples. The computational task is to quantify how much of the total sample-wise variance is captured by the first $k$ singular values obtained from the truncated Singular Value Decomposition (SVD), starting from foundational definitions in linear algebra and statistics. Your implementation must strictly follow an algorithm derived from first principles: compute the column-centered data matrix, obtain its SVD, and determine the proportion of variance explained by the leading $k$ singular values. Do not assume any pre-existing formulas beyond standard definitions.\n\nFoundational base to use:\n- The sample covariance matrix definition for a column-centered data matrix: for a data matrix $X \\in \\mathbb{R}^{n \\times p}$, where $n$ is the number of samples and $p$ is the number of genes, define the column-centered matrix $X_c$ by subtracting from each column its sample mean. The sample covariance matrix is $S = \\frac{1}{n - 1} X_c^\\top X_c$.\n- The Singular Value Decomposition (SVD) of a real matrix: for $X_c \\in \\mathbb{R}^{n \\times p}$, the SVD is $X_c = U \\Sigma V^\\top$ where $U \\in \\mathbb{R}^{n \\times r}$ and $V \\in \\mathbb{R}^{p \\times r}$ have orthonormal columns, $\\Sigma = \\operatorname{diag}(\\sigma_1, \\dots, \\sigma_r)$ with singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r \\ge 0$, and $r = \\operatorname{rank}(X_c)$.\n- The Frobenius norm identity: $\\|X_c\\|_F^2 = \\sum_{i=1}^{r} \\sigma_i^2$.\n\nYour program must:\n- For each test case, center the columns of the input matrix $X$ to obtain $X_c$ (subtract each column mean; no scaling).\n- Compute the SVD of $X_c$ to obtain singular values $\\sigma_1, \\dots, \\sigma_r$.\n- Define the truncated SVD at level $k$ as keeping the first $k$ singular values and corresponding singular vectors. To handle numerical rank robustly, let the effective rank $r$ be the count of singular values satisfying $\\sigma_i  \\tau$, where $\\tau$ is a tolerance based on dimensions and machine precision. If $k  r$, use $k_{\\text{eff}} = r$. If $k = 0$, the explained proportion must be $0$.\n- Compute the proportion of variance explained by the first $k_{\\text{eff}}$ singular values as a decimal fraction (not a percentage), namely the ratio of the sum of the squares of the first $k_{\\text{eff}}$ singular values to the sum of the squares of all $r$ singular values, using only the above foundational identities.\n\nAngle units do not apply. Physical units do not apply. All outputs must be decimal fractions.\n\nTest suite:\nFor each test case below, $X$ is the raw expression matrix and $k$ is the truncation level. You must center columns of $X$ before computing the SVD. The matrices are:\n\n- Case $1$ (happy path, tall matrix, moderate $k$):\n$$\nX^{(1)} =\n\\begin{bmatrix}\n2  0  1 \\\\\n0  1  3 \\\\\n4  2  5 \\\\\n6  3  6\n\\end{bmatrix}, \\quad k^{(1)} = 2.\n$$\n\n- Case $2$ (boundary $k = 0$):\n$$\nX^{(2)} =\n\\begin{bmatrix}\n10  0  -2  3 \\\\\n5  1  0  0 \\\\\n0  -1  2  -3\n\\end{bmatrix}, \\quad k^{(2)} = 0.\n$$\n\n- Case $3$ (wide matrix, redundant and constant columns; $k$ exceeds rank):\n$$\nX^{(3)} =\n\\begin{bmatrix}\n1  1  5  0  -1 \\\\\n2  2  5  1  0 \\\\\n3  3  5  2  1\n\\end{bmatrix}, \\quad k^{(3)} = 3.\n$$\n\n- Case $4$ (tall matrix with a zero column, nontrivial rank):\n$$\nX^{(4)} =\n\\begin{bmatrix}\n0  1  0  -1 \\\\\n0  2  1  0 \\\\\n0  3  2  1 \\\\\n0  4  3  2 \\\\\n0  5  4  3\n\\end{bmatrix}, \\quad k^{(4)} = 2.\n$$\n\n- Case $5$ (exact linear dependence among columns; $k$ equals rank):\n$$\nX^{(5)} =\n\\begin{bmatrix}\n1  2  5 \\\\\n2  4  10 \\\\\n3  6  15 \\\\\n4  8  20\n\\end{bmatrix}, \\quad k^{(5)} = 2.\n$$\n\nNumerical rank tolerance:\nUse a numerically stable tolerance $\\tau$ to decide which singular values are treated as nonzero. Choose $\\tau = \\max(n, p) \\cdot \\sigma_{\\max} \\cdot \\varepsilon$, where $n$ and $p$ are the dimensions of $X_c$, $\\sigma_{\\max}$ is the largest singular value of $X_c$, and $\\varepsilon$ is machine epsilon for $64$-bit floating point arithmetic.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each proportion rounded to $8$ decimal places as a decimal fraction in $[0, 1]$. For example, a valid output looks like $[r_a,r_b,r_c,r_d,r_e]$ where each $r_\\cdot$ is a decimal fraction.\n\nYour program must be self-contained, use the specified test suite, and produce the single-line output described above.", "solution": "The problem requires us to compute the proportion of total sample-wise variance captured by the first $k$ singular values of a column-centered data matrix. This quantity is central to Principal Component Analysis (PCA), where it is used to assess the dimensionality reduction quality. The solution will be derived from the foundational definitions provided.\n\nLet the given raw data matrix be $X \\in \\mathbb{R}^{n \\times p}$, with $n$ samples (rows) and $p$ features or genes (columns).\n\nFirst, we center the data by subtracting the mean of each column from its elements. Let $\\bar{x}_j = \\frac{1}{n} \\sum_{i=1}^{n} X_{ij}$ be the mean of the $j$-th column. The column-centered matrix $X_c$ has elements $(X_c)_{ij} = X_{ij} - \\bar{x}_j$.\n\nThe total sample-wise variance, $\\text{Var}_{\\text{total}}$, is the sum of the sample variances of each feature. This is equivalent to the trace of the sample covariance matrix $S$. The problem defines the sample covariance matrix as $S = \\frac{1}{n - 1} X_c^\\top X_c$.\nUsing the linearity of the trace operator, the total variance is:\n$$\n\\text{Var}_{\\text{total}} = \\operatorname{tr}(S) = \\operatorname{tr}\\left(\\frac{1}{n-1} X_c^\\top X_c\\right) = \\frac{1}{n - 1} \\operatorname{tr}(X_c^\\top X_c)\n$$\nA fundamental property of the trace is that $\\operatorname{tr}(A^\\top A)$ is equal to the squared Frobenius norm of $A$, $\\|A\\|_F^2 = \\sum_{i,j} A_{ij}^2$. Therefore, we have:\n$$\n\\text{Var}_{\\text{total}} = \\frac{1}{n - 1} \\|X_c\\|_F^2\n$$\nThe problem provides a key identity connecting the Frobenius norm to the singular values of $X_c$. Let the Singular Value Decomposition (SVD) of $X_c$ be $X_c = U \\Sigma V^\\top$, where the singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r  0$ are the diagonal entries of $\\Sigma$, and $r = \\operatorname{rank}(X_c)$. The identity is:\n$$\n\\|X_c\\|_F^2 = \\sum_{i=1}^{r} \\sigma_i^2\n$$\nSubstituting this into our expression for total variance gives:\n$$\n\\text{Var}_{\\text{total}} = \\frac{1}{n - 1} \\sum_{i=1}^{r} \\sigma_i^2\n$$\nThe principal components of the data are the columns of the matrix $Z = X_c V$. The variance of the $i$-th principal component is directly related to the $i$-th singular value squared: $\\text{Var}(Z_i) = \\frac{\\sigma_i^2}{n - 1}$. The total variance explained by the first $k$ principal components is the sum of their individual variances:\n$$\n\\text{Var}_k = \\sum_{i=1}^{k} \\text{Var}(Z_i) = \\sum_{i=1}^{k} \\frac{\\sigma_i^2}{n - 1} = \\frac{1}{n - 1} \\sum_{i=1}^{k} \\sigma_i^2\n$$\nThe proportion of variance explained by the first $k$ components is the ratio of the variance they capture, $\\text{Var}_k$, to the total variance, $\\text{Var}_{\\text{total}}$.\n$$\nP_k = \\frac{\\text{Var}_k}{\\text{Var}_{\\text{total}}} = \\frac{\\frac{1}{n-1} \\sum_{i=1}^{k} \\sigma_i^2}{\\frac{1}{n-1} \\sum_{i=1}^{r} \\sigma_i^2} = \\frac{\\sum_{i=1}^{k} \\sigma_i^2}{\\sum_{i=1}^{r} \\sigma_i^2}\n$$\nThe factor $\\frac{1}{n-1}$ cancels, showing that the proportion of variance can be computed directly from the squared singular values. This derived formula provides the basis for our algorithm.\n\nThe computational algorithm proceeds as follows:\n1.  For each input matrix $X \\in \\mathbb{R}^{n \\times p}$ and integer $k$, compute the column-centered matrix $X_c$.\n2.  Compute the singular values $\\sigma_i$ of $X_c$ using SVD.\n3.  To handle numerical precision, determine the effective rank $r$. A singular value $\\sigma_i$ is considered non-zero if $\\sigma_i  \\tau$, where the tolerance $\\tau$ is defined as $\\tau = \\max(n, p) \\cdot \\sigma_{\\max} \\cdot \\varepsilon$. Here, $\\sigma_{\\max}$ is the largest singular value and $\\varepsilon$ is machine epsilon for the floating-point precision used. The effective rank $r$ is the count of singular values exceeding $\\tau$.\n4.  The number of components to consider, $k_{\\text{eff}}$, is adjusted based on the rank. If $k=0$, the proportion is $0$. Otherwise, $k_{\\text{eff}} = \\min(k, r)$.\n5.  The final proportion is calculated using the derived formula:\n    $$\n    P_{k_{\\text{eff}}} = \\frac{\\sum_{i=1}^{k_{\\text{eff}}} \\sigma_i^2}{\\sum_{i=1}^{r} \\sigma_i^2}\n    $$\n    where the sums are taken over the squared singular values deemed non-zero in step $3$. If the denominator is zero (i.e., the centered matrix had zero variance), the proportion is $0$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the proportion of variance explained by the first k singular values\n    for a list of test cases according to the derived formula.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (happy path, tall matrix, moderate k)\n        (np.array([\n            [2, 0, 1],\n            [0, 1, 3],\n            [4, 2, 5],\n            [6, 3, 6]\n        ], dtype=float), 2),\n        \n        # Case 2 (boundary k = 0)\n        (np.array([\n            [10, 0, -2, 3],\n            [5, 1, 0, 0],\n            [0, -1, 2, -3]\n        ], dtype=float), 0),\n\n        # Case 3 (wide matrix, redundant and constant columns; k exceeds rank)\n        (np.array([\n            [1, 1, 5, 0, -1],\n            [2, 2, 5, 1, 0],\n            [3, 3, 5, 2, 1]\n        ], dtype=float), 3),\n\n        # Case 4 (tall matrix with a zero column, nontrivial rank)\n        (np.array([\n            [0, 1, 0, -1],\n            [0, 2, 1, 0],\n            [0, 3, 2, 1],\n            [0, 4, 3, 2],\n            [0, 5, 4, 3]\n        ], dtype=float), 2),\n\n        # Case 5 (exact linear dependence among columns; k equals rank)\n        (np.array([\n            [1, 2, 5],\n            [2, 4, 10],\n            [3, 6, 15],\n            [4, 8, 20]\n        ], dtype=float), 2)\n    ]\n\n    results = []\n    \n    for X, k in test_cases:\n        n, p = X.shape\n        \n        # Handle case with no rows, which implies no variance.\n        if n == 0:\n            results.append(0.0)\n            continue\n            \n        # Step 1: Center the columns of the input matrix X.\n        X_c = X - X.mean(axis=0)\n\n        # Step 2: Compute the SVD of the centered matrix X_c. Only singular values are needed.\n        try:\n            sigma = np.linalg.svd(X_c, compute_uv=False)\n        except np.linalg.LinAlgError:\n            # In case of a failure, assume zero variance.\n            results.append(0.0)\n            continue\n\n        # Handle matrices that result in no singular values (e.g., zero columns/rows).\n        if sigma.size == 0:\n            results.append(0.0)\n            continue\n\n        # Step 3: Determine the numerical rank r using the specified tolerance.\n        eps = np.finfo(X.dtype).eps\n        sigma_max = sigma[0]\n        tolerance = max(n, p) * sigma_max * eps\n        \n        # Filter for singular values greater than the tolerance.\n        sigma_effective = sigma[sigma > tolerance]\n        r = len(sigma_effective)\n        \n        # Step 4: Handle k and calculate the proportion of variance.\n        # If k is 0, the proportion must be 0.\n        if k == 0:\n            proportion = 0.0\n        else:\n            # Effective k cannot exceed the rank.\n            k_eff = min(k, r)\n            \n            # Use the squared effective singular values for variance calculations.\n            sigma_sq_effective = sigma_effective**2\n            \n            # The denominator is the sum of all effective squared singular values.\n            total_variance_proxy = np.sum(sigma_sq_effective)\n\n            if total_variance_proxy  1e-15: # Check for near-zero total variance.\n                # If total variance is zero, no variance can be explained.\n                proportion = 0.0\n            else:\n                # The numerator is the sum of the first k_eff squared singular values.\n                explained_variance_proxy = np.sum(sigma_sq_effective[:k_eff])\n                proportion = explained_variance_proxy / total_variance_proxy\n                \n        results.append(round(proportion, 8))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4578458"}, {"introduction": "Moving from data exploration to predictive modeling, we often encounter the challenge of building reliable linear models from datasets where features are correlated. This practice explores the solution to the linear least-squares problem, $\\arg\\min_{\\beta} \\| X\\beta - y \\|_2$, by contrasting the classic normal equations with the numerically superior approach using the Moore-Penrose pseudoinverse derived from SVD [@problem_id:4578463]. You will see firsthand why the SVD method is indispensable for handling the ill-conditioned or rank-deficient matrices common in bioinformatics, ensuring your models are both stable and trustworthy.", "problem": "Consider a linear model where a design matrix $X \\in \\mathbb{R}^{m \\times p}$ and an outcome vector $y \\in \\mathbb{R}^{m}$ are given. In bioinformatics and medical data analytics, such matrices arise when modeling relationships between gene expression features and phenotypic measurements. Starting from fundamental definitions in linear algebra, use the Singular Value Decomposition (SVD) to construct the Moore–Penrose pseudoinverse $X^{+}$ and compute the least-squares solution $X^{+} y$. When $X^{\\top} X$ is invertible, also compute the unique normal-equation solution. Design your program to:\n\n- Derive $X^{+}$ from first principles using SVD and a numerically justified threshold to decide which singular values are treated as nonzero.\n- Determine whether $X^{\\top} X$ is invertible based on the column rank of $X$.\n- If $X^{\\top} X$ is invertible, compute the normal-equation solution and compare it to $X^{+} y$ by the Euclidean norm of their difference.\n- In all cases, compute the Euclidean norm of the residual $X (X^{+} y) - y$.\n\nUse only well-tested facts: matrix decomposition definitions, properties of orthogonal matrices, and solvability conditions for linear systems. Do not use any unproven shortcuts.\n\nYour program must implement and evaluate the following test suite of parameter values. Each $X$ and $y$ is specified explicitly:\n\n1. Happy path (tall, full column rank):\n   $$X_1 = \\begin{bmatrix}\n   2  -1  3 \\\\\n   0  4  5 \\\\\n   1  2  -2 \\\\\n   3  -5  1 \\\\\n   4  0  -1\n   \\end{bmatrix}, \\quad\n   y_1 = \\begin{bmatrix}\n   1 \\\\ 0 \\\\ -1 \\\\ 2 \\\\ 3\n   \\end{bmatrix}.$$\n\n2. Ill-conditioned but invertible $X^{\\top} X$ (near-collinearity):\n   $$X_2 = \\begin{bmatrix}\n   1  2.0001  0 \\\\\n   2  4.0002  1 \\\\\n   3  6.0003  0 \\\\\n   4  8.0004  1 \\\\\n   5  10.0005  0\n   \\end{bmatrix}, \\quad\n   y_2 = \\begin{bmatrix}\n   1 \\\\ 1 \\\\ 2 \\\\ 3 \\\\ 5\n   \\end{bmatrix}.$$\n\n3. Rank-deficient (normal equations not applicable):\n   $$X_3 = \\begin{bmatrix}\n   1  0  0 \\\\\n   0  1  1 \\\\\n   1  0  0 \\\\\n   0  1  1\n   \\end{bmatrix}, \\quad\n   y_3 = \\begin{bmatrix}\n   1 \\\\ 2 \\\\ 3 \\\\ 4\n   \\end{bmatrix}.$$\n\n4. Square and invertible:\n   $$X_4 = \\begin{bmatrix}\n   3  0  1 \\\\\n   2  -1  0 \\\\\n   1  2  4\n   \\end{bmatrix}, \\quad\n   y_4 = \\begin{bmatrix}\n   0 \\\\ 1 \\\\ 2\n   \\end{bmatrix}.$$\n\nFor numerical robustness, the SVD-based pseudoinverse should use a threshold $\\tau = \\max(m, p) \\cdot \\varepsilon \\cdot \\sigma_{\\max}$, where $\\varepsilon$ is machine precision for double-precision floating point, and $\\sigma_{\\max}$ is the largest singular value of $X$. Singular values strictly greater than $\\tau$ are inverted; others are treated as zero.\n\nDefine the final output for each test case as a list with three entries:\n- The boolean indicating whether the normal-equation computation is applicable (that is, whether $X^{\\top} X$ is invertible).\n- The float equal to the Euclidean norm of the difference between the SVD-based solution and the normal-equation solution (if applicable), or the float $-1.0$ if not applicable.\n- The float equal to the Euclidean norm of the residual $X (X^{+} y) - y$.\n\nYour program should produce a single line of output containing the results as a comma-separated list of these per-test-case lists, enclosed in square brackets. For example, the output format must be:\n$$\\texttt{[[b\\_1,d\\_1,r\\_1],[b\\_2,d\\_2,r\\_2],[b\\_3,d\\_3,r\\_3],[b\\_4,d\\_4,r\\_4]]}.$$\nNo physical units or angle units are involved; all quantities are dimensionless real numbers.", "solution": "The analysis of the provided problem proceeds in two stages: first, a validation of the problem statement, and second, a detailed derivation of the solution methodology based on first principles of linear algebra.\n\n### Problem Validation\n\n**1. Extraction of Givens:**\n- **Model**: A linear system defined by a design matrix $X \\in \\mathbb{R}^{m \\times p}$ and an outcome vector $y \\in \\mathbb{R}^{m}$.\n- **Objective**: Compute the least-squares solution $\\hat{\\beta}$ that minimizes $\\|X\\beta - y\\|_2$.\n- **Method 1 (SVD Pseudoinverse)**: Compute $\\hat{\\beta}_{SVD} = X^{+}y$, where the Moore-Penrose pseudoinverse $X^{+}$ is constructed from the Singular Value Decomposition (SVD) of $X$.\n- **Method 2 (Normal Equations)**: If and only if $X^{\\top}X$ is invertible, compute $\\hat{\\beta}_{NE} = (X^{\\top}X)^{-1}X^{\\top}y$.\n- **Numerical Threshold for SVD**: A singular value $\\sigma_i$ is treated as non-zero if $\\sigma_i  \\tau$, where $\\tau = \\max(m, p) \\cdot \\varepsilon \\cdot \\sigma_{\\max}$. Here, $\\varepsilon$ is the machine precision for double-precision floating-point numbers and $\\sigma_{\\max}$ is the largest singular value of $X$.\n- **Invertibility Criterion**: The matrix $X^{\\top}X$ is deemed invertible if the column rank of $X$, determined numerically using the threshold $\\tau$, is equal to $p$.\n- **Required Outputs per Test Case**: A list containing three values:\n    1. A boolean indicating if the normal-equation method is applicable (i.e., if $X^{\\top}X$ is invertible).\n    2. The Euclidean norm of the difference between the SVD and normal-equation solutions, $\\|\\hat{\\beta}_{SVD} - \\hat{\\beta}_{NE}\\|_2$. If the normal-equation method is not applicable, this value is $-1.0$.\n    3. The Euclidean norm of the residual vector for the SVD solution, $\\|X\\hat{\\beta}_{SVD} - y\\|_2$.\n- **Test Data**: Four specific pairs of $(X, y)$ are provided for evaluation.\n\n**2. Validation Verdict:**\nThe problem is **valid**.\n- It is **scientifically grounded** in the fundamental principles of linear algebra and numerical analysis, specifically concerning the solution of linear least-squares problems.\n- It is **well-posed**, with all necessary data, definitions, and numerical criteria explicitly provided for a unique and meaningful solution to be computed.\n- It is **objective**, stated in precise mathematical terms and free from ambiguity or subjective claims.\n- The problem context (bioinformatics and medical data analytics) is appropriate, as linear models are a cornerstone of these fields. All specified conditions and data are mathematically and computationally sound.\n\n### Solution Derivation\n\nThe problem requires a comparative analysis of two methods for solving the linear least-squares problem, which is fundamental to data modeling. The goal is to find a vector of parameters $\\beta \\in \\mathbb{R}^p$ that best explains the observed outcomes $y \\in \\mathbb{R}^m$ through a linear transformation by the design matrix $X \\in \\mathbb{R}^{m \\times p}$. \"Best\" is defined in the sense of minimizing the sum of squared errors, i.e., minimizing the squared Euclidean norm of the residual vector.\n\n**The Least-Squares Problem**\nThe objective is to find the vector $\\hat{\\beta}$ that solves the following minimization problem:\n$$ \\hat{\\beta} = \\arg\\min_{\\beta \\in \\mathbb{R}^p} \\| y - X\\beta \\|_2^2 $$\nThe squared norm can be expressed as a quadratic form:\n$$ L(\\beta) = (y - X\\beta)^{\\top}(y - X\\beta) = y^{\\top}y - 2\\beta^{\\top}X^{\\top}y + \\beta^{\\top}X^{\\top}X\\beta $$\nThis is a convex function of $\\beta$, and its minimum can be found by setting its gradient with respect to $\\beta$ to the zero vector.\n$$ \\nabla_{\\beta} L(\\beta) = -2X^{\\top}y + 2X^{\\top}X\\beta = 0 $$\nThis yields the celebrated **normal equations**:\n$$ X^{\\top}X\\beta = X^{\\top}y $$\n\n**Method 1: The Normal Equation Solution**\nThe matrix $X^{\\top}X$ is a square matrix of size $p \\times p$. If this matrix is invertible, a unique solution for $\\beta$ exists and is given by:\n$$ \\hat{\\beta}_{NE} = (X^{\\top}X)^{-1}X^{\\top}y $$\nA core theorem in linear algebra states that $X^{\\top}X$ is invertible if and only if the matrix $X$ has linearly independent columns. This is equivalent to stating that $X$ has full column rank, i.e., $\\text{rank}(X) = p$. When these conditions are not met (i.e., when columns are collinear or the system is underdetermined), $X^{\\top}X$ is singular, and this method fails. Furthermore, even if $X^{\\top}X$ is theoretically invertible, it may be ill-conditioned if $X$ has nearly collinear columns. The explicit computation of the inverse can amplify numerical errors in such cases.\n\n**Method 2: The SVD and Moore-Penrose Pseudoinverse Solution**\nThe Singular Value Decomposition (SVD) provides a powerful and numerically robust framework for analyzing linear systems. Any matrix $X \\in \\mathbb{R}^{m \\times p}$ can be decomposed as:\n$$ X = U \\Sigma V^{\\top} $$\nwhere:\n- $U$ is an $m \\times m$ orthogonal matrix whose columns ($u_i$) are the left-singular vectors.\n- $V$ is a $p \\times p$ orthogonal matrix whose columns ($v_i$) are the right-singular vectors.\n- $\\Sigma$ is an $m \\times p$ rectangular diagonal matrix containing the non-negative singular values $\\sigma_i$ in decreasing order. The number of non-zero singular values equals the rank of $X$.\n\nUsing the SVD, the Moore-Penrose pseudoinverse $X^{+}$ of $X$ is uniquely defined as:\n$$ X^{+} = V \\Sigma^{+} U^{\\top} $$\nwhere $\\Sigma^{+}$ is a $p \\times m$ matrix derived from $\\Sigma$. It is constructed by taking the transpose of $\\Sigma$ and then taking the reciprocal of each non-zero singular value. In finite-precision arithmetic, we must use a threshold $\\tau = \\max(m, p) \\cdot \\varepsilon \\cdot \\sigma_{\\max}$ to decide which singular values are numerically non-zero. The elements of the diagonal of $\\Sigma^{+}$, denoted $\\sigma_i^{+}$, are thus:\n$$ \\sigma_i^{+} = \\begin{cases} 1/\\sigma_i  \\text{if } \\sigma_i  \\tau \\\\ 0  \\text{if } \\sigma_i \\le \\tau \\end{cases} $$\nThe rank of the matrix is numerically estimated as the count of singular values greater than $\\tau$. The condition for the applicability of the normal equations, $\\text{rank}(X) = p$, is checked using this numerical rank.\n\nThe SVD-based least-squares solution is given by:\n$$ \\hat{\\beta}_{SVD} = X^{+}y = (V \\Sigma^{+} U^{\\top})y $$\nThis solution is always defined. It provides the unique minimum-norm solution to the least-squares problem, meaning that among all vectors $\\beta$ that minimize $\\|X\\beta - y\\|_2$, $\\hat{\\beta}_{SVD}$ is the one with the smallest Euclidean norm $\\|\\beta\\|_2$.\n\n**Algorithmic Procedure**\nFor each given pair $(X, y)$, the following steps are executed:\n1.  Obtain the dimensions $m$ and $p$ from $X$.\n2.  Compute the SVD of $X$ to get $U$, the singular values vector $s$, and $V^{\\top}$. For efficiency, the economy-sized SVD is used.\n3.  Determine the numerical threshold $\\tau = \\max(m, p) \\cdot \\varepsilon \\cdot \\sigma_{\\max}$, where $\\sigma_{\\max} = s[0]$.\n4.  Determine the numerical rank $r_{eff}$ by counting the singular values $s_i  \\tau$.\n5.  Set the boolean `is_normal_eq_applicable` to `True` if $r_{eff} = p$, and `False` otherwise.\n6.  Compute the SVD solution $\\hat{\\beta}_{SVD}$. This is done efficiently as $\\hat{\\beta}_{SVD} = V(\\Sigma^{+}(U^{\\top}y))$, where the operations are performed vector-wise without explicitly forming the matrix $\\Sigma^{+}$.\n7.  If `is_normal_eq_applicable` is `True`:\n    a. Compute the normal equation solution $\\hat{\\beta}_{NE} = (X^{\\top}X)^{-1}X^{\\top}y$.\n    b. Calculate `difference_norm` as the Euclidean norm $\\|\\hat{\\beta}_{SVD} - \\hat{\\beta}_{NE}\\|_2$.\n8.  If `is_normal_eq_applicable` is `False`, set `difference_norm` to $-1.0$.\n9.  In all cases, calculate the norm of the residual vector, `residual_norm` = $\\|X\\hat{\\beta}_{SVD} - y\\|_2$.\n10. Store the resulting triplet (`is_normal_eq_applicable`, `difference_norm`, `residual_norm`).\n\nThis procedure will be applied to all four test cases to generate the final output.", "answer": "```python\nimport numpy as np\n\ndef solve_least_squares_case(X, y):\n    \"\"\"\n    Solves a linear least-squares problem using SVD and Normal Equations,\n    and returns the specified comparison metrics.\n\n    Args:\n        X (np.ndarray): The m x p design matrix.\n        y (np.ndarray): The m-dimensional outcome vector.\n\n    Returns:\n        list: A list containing [is_normal_eq_applicable, difference_norm, residual_norm].\n    \"\"\"\n    m, p = X.shape\n\n    # 1. Compute SVD and determine numerical rank\n    try:\n        U, s, Vt = np.linalg.svd(X, full_matrices=False)\n    except np.linalg.LinAlgError:\n        # Handle cases where SVD might fail, though unlikely for real matrices\n        return [False, -1.0, np.linalg.norm(y)]\n\n    # Get machine epsilon for double precision\n    eps = np.finfo(np.float64).eps\n    \n    # Set the threshold for singular values\n    sigma_max = s[0] if s.size > 0 else 0\n    tau = max(m, p) * eps * sigma_max\n\n    # Determine numerical rank\n    rank = np.sum(s > tau)\n    is_normal_eq_applicable = (rank == p)\n\n    # 2. Compute the SVD-based least-squares solution beta_svd = X_plus @ y\n    # X_plus = V @ np.diag(s_inv) @ U.T\n    # beta_svd = V @ np.diag(s_inv) @ U.T @ y\n    s_inv = np.zeros_like(s)\n    s_inv[s > tau] = 1.0 / s[s > tau]\n    \n    # Efficient computation of beta_svd\n    uty = U.T @ y\n    beta_svd = Vt.T @ (s_inv * uty)\n\n    # 3. Compute normal equation solution if applicable\n    difference_norm = -1.0\n    if is_normal_eq_applicable:\n        try:\n            XTX = X.T @ X\n            XTX_inv = np.linalg.inv(XTX)\n            XTy = X.T @ y\n            beta_ne = XTX_inv @ XTy\n            difference_norm = np.linalg.norm(beta_svd - beta_ne)\n        except np.linalg.LinAlgError:\n            # If XTX is singular despite rank check (extreme ill-conditioning),\n            # consider normal equations as not applicable in practice.\n            is_normal_eq_applicable = False\n            difference_norm = -1.0\n            # This logic branch correction ensures that if np.linalg.inv fails,\n            # we correctly report the normal equation method as inapplicable.\n            # Find a way to modify the result being built.\n            # The list 'result' for this case should be updated.\n            # The cleanest way is to just proceed with the `else` block logic.\n            pass # proceed to the default value\n\n    # In case of LinAlgError inside the if block, we need to ensure the values are correct.\n    # The boolean has to be re-set to False in the final list for consistency.\n    if difference_norm == -1.0:\n        is_normal_eq_applicable = False\n\n\n    # 4. Compute the residual norm for the SVD solution\n    residual_vec = X @ beta_svd - y\n    residual_norm = np.linalg.norm(residual_vec)\n    \n    return [is_normal_eq_applicable, difference_norm, residual_norm]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    X1 = np.array([\n        [2, -1, 3],\n        [0, 4, 5],\n        [1, 2, -2],\n        [3, -5, 1],\n        [4, 0, -1]\n    ], dtype=np.float64)\n    y1 = np.array([1, 0, -1, 2, 3], dtype=np.float64)\n\n    X2 = np.array([\n        [1, 2.0001, 0],\n        [2, 4.0002, 1],\n        [3, 6.0003, 0],\n        [4, 8.0004, 1],\n        [5, 10.0005, 0]\n    ], dtype=np.float64)\n    y2 = np.array([1, 1, 2, 3, 5], dtype=np.float64)\n\n    X3 = np.array([\n        [1, 0, 0],\n        [0, 1, 1],\n        [1, 0, 0],\n        [0, 1, 1]\n    ], dtype=np.float64)\n    y3 = np.array([1, 2, 3, 4], dtype=np.float64)\n\n    X4 = np.array([\n        [3, 0, 1],\n        [2, -1, 0],\n        [1, 2, 4]\n    ], dtype=np.float64)\n    y4 = np.array([0, 1, 2], dtype=np.float64)\n\n    test_cases = [\n        (X1, y1),\n        (X2, y2),\n        (X3, y3),\n        (X4, y4),\n    ]\n\n    results = []\n    for X, y in test_cases:\n        result = solve_least_squares_case(X, y)\n        results.append(result)\n\n    # Format the output string as required.\n    # The default str() for a list adds spaces, e.g., '[True, 1.23, 4.56]'.\n    # The template `','.join(map(str, results))` joins these string representations with a comma.\n    # Enclosing this in brackets gives a string that looks like a list of lists.\n    # This precisely follows the structure provided in the skeleton code.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4578463"}, {"introduction": "In unsupervised learning, our goal is to uncover latent structures in data, such as patient clusters, without reference to a known outcome. The success of this endeavor hinges on our definition of \"similarity\" or \"distance.\" This exercise investigates how the geometry of your data space, shaped by feature scaling and covariance, impacts clustering outcomes by comparing the standard Euclidean distance with the covariance-aware Mahalanobis distance [@problem_id:4578461]. This practice provides critical insight into how linear algebraic concepts directly influence pattern discovery and the importance of choosing a metric that reflects the underlying structure of the data.", "problem": "You are given finite sets of patient embeddings represented as points in a real vector space. Let $X \\in \\mathbb{R}^{n \\times p}$ denote an embedding matrix with $n$ patients and $p$ features (columns). Consider feature scaling specified by a diagonal scaling matrix $S = \\mathrm{diag}(s) \\in \\mathbb{R}^{p \\times p}$ with strictly positive diagonal entries $s \\in \\mathbb{R}^p$, applied as column-wise scaling to obtain the scaled data $X_s = X S$. For any two points $x_i, x_j \\in \\mathbb{R}^p$ (rows of $X_s$), define the Euclidean distance $d_E(x_i, x_j)$ and the Mahalanobis distance $d_M(x_i, x_j)$ as follows. The Euclidean distance is defined by $d_E(x_i, x_j) = \\lVert x_i - x_j \\rVert_2$. The Mahalanobis distance under a positive-definite metric matrix $\\Sigma \\in \\mathbb{R}^{p \\times p}$ is defined by $d_M(x_i, x_j) = \\sqrt{(x_i - x_j)^\\top \\Sigma^{-1} (x_i - x_j)}$. For this problem, for the Mahalanobis distance on $X_s$, use the empirical covariance matrix $\\Sigma_s \\in \\mathbb{R}^{p \\times p}$ of the columns of $X_s$ and a strictly positive Tikhonov regularization parameter $\\lambda \\in \\mathbb{R}$ with $\\lambda = 10^{-6}$ to define $\\Sigma_s^{(\\lambda)} = \\Sigma_s + \\lambda I_p$, where $I_p$ is the $p \\times p$ identity matrix, and use $\\Sigma_s^{(\\lambda)}$ in place of $\\Sigma$ in the Mahalanobis formula. Clustering must be performed deterministically using the following graph-theoretic procedure based on the Minimum Spanning Tree (MST). Given a symmetric distance matrix $D \\in \\mathbb{R}^{n \\times n}$ with entries $D_{ij} \\ge 0$, construct the complete weighted graph on $n$ nodes with edge weights $D_{ij}$ for $i  j$. Compute an MST using Kruskal’s algorithm with a deterministic tie-break rule: sort edges by non-decreasing weight, and for equal weights, break ties lexicographically by the pair $(i,j)$ with smaller $i$ first and then smaller $j$. After obtaining the MST containing exactly $n-1$ edges, split the MST into exactly $k$ connected components by removing the largest $k-1$ edges by the same order rule (i.e., remove the edges with the largest weights, and if there are ties in weight, remove the edge with the lexicographically larger $(i,j)$ first). The connected components are the cluster assignments. To quantify the effect of scaling on cluster structure, use the Rand agreement between two clusterings. For two partitions of $\\{1,\\dots,n\\}$ into any number of clusters, the Rand agreement is the fraction in $[0,1]$ of unordered pairs $\\{i,j\\}$, $1 \\le i  j \\le n$, that are either in the same cluster in both partitions or in different clusters in both partitions.\n\nYour program must:\n- For each dataset $X$, compute clusterings for the following four configurations:\n    1. Unscaled Euclidean: $X$ with $S = I_p$ and Euclidean distances.\n    2. Scaled Euclidean: $X_s$ with given $S$ and Euclidean distances.\n    3. Unscaled Mahalanobis: $X$ with $S = I_p$ and Mahalanobis distances using $\\Sigma^{(\\lambda)}$ computed from $X$.\n    4. Scaled Mahalanobis: $X_s$ with given $S$ and Mahalanobis distances using $\\Sigma_s^{(\\lambda)}$ computed from $X_s$.\n- Use the MST procedure to produce exactly $k$ clusters for each configuration.\n- Compute three Rand agreements per test case:\n    - $r_E$: Rand agreement between the unscaled Euclidean clustering and the scaled Euclidean clustering.\n    - $r_M$: Rand agreement between the unscaled Mahalanobis clustering and the scaled Mahalanobis clustering.\n    - $r_{EM}$: Rand agreement between the scaled Euclidean clustering and the scaled Mahalanobis clustering.\n- Aggregate all results across the test suite into a single list of floating-point numbers in $[0,1]$ ordered as $[r_E^{(1)}, r_M^{(1)}, r_{EM}^{(1)}, r_E^{(2)}, r_M^{(2)}, r_{EM}^{(2)}, \\dots]$.\n\nFundamental base and constraints:\n- Use the definition of Euclidean distance $d_E$, the definition of Mahalanobis distance $d_M$, the empirical covariance, and Tikhonov regularization $\\lambda$ as fundamental constructs.\n- Use Kruskal’s algorithm for the Minimum Spanning Tree (MST) with the specified deterministic tie-breaking, and the edge-removal rule to obtain $k$ clusters.\n- Use the Rand agreement as the pairwise-based measure of partition similarity.\n\nTest suite:\n- Test case $1$ (balanced, well-separated clusters, scaling neutral):\n    - $X^{(1)} \\in \\mathbb{R}^{6 \\times 2}$ with rows\n    $$\\begin{bmatrix}\n    -2  -2 \\\\\n    -2  -1.8 \\\\\n    -1.8  -2 \\\\\n    2  2 \\\\\n    2  1.8 \\\\\n    1.8  2\n    \\end{bmatrix}$$\n    - $s^{(1)} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$, $k^{(1)} = 2$.\n- Test case $2$ (anisotropic scaling that amplifies a noisy feature; Euclidean is sensitive, Mahalanobis compensates):\n    - $X^{(2)} \\in \\mathbb{R}^{6 \\times 2}$ with rows\n    $$\\begin{bmatrix}\n    0  0 \\\\\n    0.2  0.1 \\\\\n    -0.1  -0.1 \\\\\n    3  0.1 \\\\\n    3.1  -0.1 \\\\\n    2.9  0\n    \\end{bmatrix}$$\n    - $s^{(2)} = \\begin{bmatrix} 0.05 \\\\ 20 \\end{bmatrix}$, $k^{(2)} = 2$.\n- Test case $3$ (singular covariance without regularization; regularization ensures stability):\n    - $X^{(3)} \\in \\mathbb{R}^{4 \\times 3}$ with rows\n    $$\\begin{bmatrix}\n    0  0  0 \\\\\n    1  2  -1 \\\\\n    5  10  -5 \\\\\n    6  12  -6\n    \\end{bmatrix}$$\n    - $s^{(3)} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$, $k^{(3)} = 2$.\n\nAngle units and physical units do not apply in this task. All answers must be dimensionless real numbers in $[0,1]$, represented as decimal floats.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[r_E^{(1)}, r_M^{(1)}, r_{EM}^{(1)}, r_E^{(2)}, r_M^{(2)}, r_{EM}^{(2)}, r_E^{(3)}, r_M^{(3)}, r_{EM}^{(3)}]$. Each number must be formatted to exactly $6$ decimal places (for example, $[1.000000,0.933333,0.866667,\\dots]$).", "solution": "The problem requires an analysis of the impact of feature scaling on clustering outcomes. This is a fundamental topic in data analysis, as distance-based algorithms are often sensitive to the relative scales of input features. We will compare clustering results under four distinct configurations using two distance metrics (Euclidean and Mahalanobis) on both unscaled and scaled data. The similarity between the resulting clusterings will be quantified using the Rand agreement index. The entire process is deterministic, including the clustering algorithm, which is based on a Minimum Spanning Tree (MST).\n\n**1. Data Representation and Scaling**\n\nPatient data is provided as an embedding matrix $X \\in \\mathbb{R}^{n \\times p}$, where $n$ is the number of patients and $p$ is the number of features. Feature scaling is applied via a diagonal matrix $S = \\mathrm{diag}(s)$ with positive entries, resulting in the scaled data matrix $X_s = X S$. This operation corresponds to multiplying each column of $X$ by its respective scaling factor from the vector $s$.\n\n**2. Distance Metrics**\n\nTwo distance metrics are employed to measure the dissimilarity between pairs of data points (rows of the data matrix).\n\n*   **Euclidean Distance**: The standard L2-norm of the difference between two vectors $x_i, x_j \\in \\mathbb{R}^p$. It is defined as:\n    $$d_E(x_i, x_j) = \\lVert x_i - x_j \\rVert_2 = \\sqrt{\\sum_{k=1}^{p} (x_{ik} - x_{jk})^2}$$\n    The Euclidean distance is sensitive to the scale of the features. A feature with a large numerical range will dominate the distance calculation.\n\n*   **Mahalanobis Distance**: This distance accounts for the covariance between features and is scale-invariant. For a set of points, it is defined relative to a positive-definite metric matrix $\\Sigma$. The problem specifies using the regularized empirical sample covariance matrix of the data. For a data matrix $Z$ (which can be $X$ or $X_s$), the empirical sample covariance is $\\Sigma_Z = \\frac{1}{n-1} (Z - \\bar{Z})^\\top (Z - \\bar{Z})$, where $\\bar{Z}$ is the matrix of column-wise means. To ensure numerical stability and that the matrix is always invertible, Tikhonov regularization is applied:\n    $$\\Sigma_Z^{(\\lambda)} = \\Sigma_Z + \\lambda I_p$$\n    where $I_p$ is the $p \\times p$ identity matrix and $\\lambda = 10^{-6}$ is a small positive constant. The Mahalanobis distance is then:\n    $$d_M(x_i, x_j) = \\sqrt{(x_i - x_j)^\\top (\\Sigma_Z^{(\\lambda)})^{-1} (x_i - x_j)}$$\n    By incorporating the inverse covariance matrix, this metric effectively transforms the space so that features are uncorrelated and have unit variance, thus mitigating the effect of differential feature scales and correlations.\n\n**3. MST-Based Clustering**\n\nA deterministic, graph-theoretic clustering procedure is specified. For a given set of $n$ points and a symmetric distance matrix $D \\in \\mathbb{R}^{n \\times n}$ computed from them:\n\n1.  A complete weighted graph is constructed where the $n$ points are nodes and the weight of the edge between nodes $i$ and $j$ is $D_{ij}$.\n2.  A Minimum Spanning Tree (MST) of this graph is computed using Kruskal's algorithm. To ensure determinism, a strict tie-breaking rule is enforced: edges are sorted first by non-decreasing weight, and for ties, by the lexicographical order of the node pair $(i, j)$ where $i  j$.\n3.  The MST, which contains $n-1$ edges, is then partitioned into exactly $k$ clusters. This is achieved by removing the $k-1$ edges with the largest weights. The tie-breaking rule for removal is also deterministic: for edges with equal weight, the one with the lexicographically larger pair $(i,j)$ is removed first.\n4.  The remaining $k$ connected components of the graph form the final clusters.\n\n**4. Evaluation via Rand Agreement**\n\nTo quantify the effect of scaling, we compare the partitions generated by different configurations. The Rand agreement (or Rand Index) measures the similarity between two partitions of the same set of $n$ elements. It is the fraction of pairs of elements that are treated consistently across both partitions. A pair $\\{i,j\\}$ is treated consistently if the elements are in the same cluster in both partitions or in different clusters in both partitions. The Rand agreement ranges from $0$ (no agreement) to $1$ (identical partitions).\n\n**5. Analysis Procedure**\n\nFor each test case, we will perform the following steps:\n1.  Compute four distinct clusterings for a given data matrix $X$, scaling vector $s$, and cluster count $k$:\n    *   **Unscaled Euclidean**: Clustering based on Euclidean distances on the original data $X$.\n    *   **Scaled Euclidean**: Clustering based on Euclidean distances on the scaled data $X_s$.\n    *   **Unscaled Mahalanobis**: Clustering based on Mahalanobis distances on the original data $X$.\n    *   **Scaled Mahalanobis**: Clustering based on Mahalanobis distances on the scaled data $X_s$.\n2.  Calculate three Rand agreement scores to assess the impact of scaling and choice of metric:\n    *   $r_E$: Agreement between the Unscaled and Scaled Euclidean clusterings.\n    *   $r_M$: Agreement between the Unscaled and Scaled Mahalanobis clusterings. This is expected to be high due to the scale-invariant nature of the Mahalanobis distance.\n    *   $r_{EM}$: Agreement between the Scaled Euclidean and Scaled Mahalanobis clusterings, to compare the behavior of the two metrics on the scaled data.\n\nThe implementation will systematically execute these steps for each test case and aggregate the results.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\nclass UnionFind:\n    \"\"\"A data structure for maintaining disjoint sets, used in Kruskal's algorithm.\"\"\"\n    def __init__(self, n):\n        self.parent = list(range(n))\n\n    def find(self, i):\n        if self.parent[i] == i:\n            return i\n        # Path compression for efficiency\n        self.parent[i] = self.find(self.parent[i])\n        return self.parent[i]\n\n    def union(self, i, j):\n        root_i = self.find(i)\n        root_j = self.find(j)\n        if root_i != root_j:\n            # Simple union, sufficient for this problem size\n            self.parent[root_j] = root_i\n            return True\n        return False\n\ndef compute_distance_matrix(X, metric='euclidean', lambda_reg=1e-6):\n    \"\"\"\n    Computes the pairwise distance matrix for a data matrix X using either\n    Euclidean or Mahalanobis distance.\n    \"\"\"\n    n, p = X.shape\n    if n = 1:\n        return np.zeros((n, n))\n\n    dist_matrix = np.zeros((n, n))\n\n    if metric == 'euclidean':\n        for i in range(n):\n            for j in range(i + 1, n):\n                dist = np.linalg.norm(X[i] - X[j])\n                dist_matrix[i, j] = dist\n                dist_matrix[j, i] = dist\n    elif metric == 'mahalanobis':\n        # Use ddof=1 for unbiased sample covariance\n        cov = np.cov(X, rowvar=False, ddof=1)\n        # Handle case where p=1, np.cov returns a scalar\n        if p == 1:\n            cov = np.array([[cov]])\n        \n        # Tikhonov regularization for invertibility\n        cov_reg = cov + lambda_reg * np.eye(p)\n        try:\n            VI = np.linalg.inv(cov_reg)\n        except np.linalg.LinAlgError:\n            # Fallback for extremely ill-conditioned matrices\n            VI = np.linalg.pinv(cov_reg)\n        \n        for i in range(n):\n            for j in range(i + 1, n):\n                diff = X[i] - X[j]\n                dist = np.sqrt(diff.T @ VI @ diff)\n                dist_matrix[i, j] = dist\n                dist_matrix[j, i] = dist\n                \n    return dist_matrix\n\ndef mst_cluster(dist_matrix, k):\n    \"\"\"\n    Performs clustering based on MST edge removal with deterministic tie-breaking.\n    \"\"\"\n    n = dist_matrix.shape[0]\n    if k > n or k  1:\n        raise ValueError(\"k must be between 1 and n.\")\n    # If k is >= n, each point is its own cluster\n    if n = k:\n        return np.arange(n)\n\n    # 1. Create a list of all edges (weight, i, j) for i  j\n    edges = []\n    for i in range(n):\n        for j in range(i + 1, n):\n            edges.append((dist_matrix[i, j], i, j))\n\n    # 2. Sort edges for Kruskal's algorithm (MST construction)\n    # Tie-breaking: weight asc, then i asc, then j asc\n    edges.sort()\n\n    # 3. Build MST using Kruskal's algorithm\n    mst_edges = []\n    uf = UnionFind(n)\n    for weight, i, j in edges:\n        if uf.union(i, j):\n            mst_edges.append((weight, i, j))\n        if len(mst_edges) == n - 1:\n            break\n\n    # 4. Sort MST edges for removal\n    # Tie-breaking: weight desc, then (i,j) lexicographically desc\n    mst_edges.sort(key=lambda x: (x[0], x[1], x[2]), reverse=True)\n    \n    # 5. Remove k-1 largest edges to get k components\n    edges_to_keep = mst_edges[k - 1:]\n\n    # 6. Find connected components from the remaining edges\n    final_uf = UnionFind(n)\n    for _, i, j in edges_to_keep:\n        final_uf.union(i, j)\n        \n    # 7. Generate cluster labels from the final disjoint sets\n    labels = np.array([final_uf.find(i) for i in range(n)])\n    \n    return labels\n\ndef compute_rand_agreement(labels1, labels2):\n    \"\"\"\n    Computes the Rand agreement between two partitions (clusterings).\n    \"\"\"\n    n = len(labels1)\n    if n  2:\n        return 1.0\n\n    agreements = 0\n    total_pairs = n * (n - 1) / 2\n\n    for i in range(n):\n        for j in range(i + 1, n):\n            same_in_1 = (labels1[i] == labels1[j])\n            same_in_2 = (labels2[i] == labels2[j])\n            if same_in_1 == same_in_2:\n                agreements += 1\n    \n    return agreements / total_pairs\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([\n            [-2.0, -2.0], [-2.0, -1.8], [-1.8, -2.0],\n            [ 2.0,  2.0], [ 2.0,  1.8], [ 1.8,  2.0]\n        ]), np.array([1.0, 1.0]), 2),\n        (np.array([\n            [0.0,  0.0], [0.2,  0.1], [-0.1, -0.1],\n            [3.0,  0.1], [3.1, -0.1], [ 2.9,  0.0]\n        ]), np.array([0.05, 20.0]), 2),\n        (np.array([\n            [0.0,  0.0,  0.0], [1.0,  2.0, -1.0], \n            [5.0, 10.0, -5.0], [6.0, 12.0, -6.0]\n        ]), np.array([1.0, 1.0, 1.0]), 2)\n    ]\n\n    results = []\n    lambda_reg = 1e-6\n    \n    for X, s, k in test_cases:\n        # Scale data\n        X_scaled = X @ np.diag(s)\n\n        # 1. Unscaled Euclidean\n        D_UE = compute_distance_matrix(X, metric='euclidean')\n        clusters_UE = mst_cluster(D_UE, k)\n        \n        # 2. Scaled Euclidean\n        D_SE = compute_distance_matrix(X_scaled, metric='euclidean')\n        clusters_SE = mst_cluster(D_SE, k)\n        \n        # 3. Unscaled Mahalanobis\n        D_UM = compute_distance_matrix(X, metric='mahalanobis', lambda_reg=lambda_reg)\n        clusters_UM = mst_cluster(D_UM, k)\n        \n        # 4. Scaled Mahalanobis\n        D_SM = compute_distance_matrix(X_scaled, metric='mahalanobis', lambda_reg=lambda_reg)\n        clusters_SM = mst_cluster(D_SM, k)\n        \n        # Compute Rand agreements\n        r_E = compute_rand_agreement(clusters_UE, clusters_SE)\n        r_M = compute_rand_agreement(clusters_UM, clusters_SM)\n        r_EM = compute_rand_agreement(clusters_SE, clusters_SM)\n        \n        results.extend([r_E, r_M, r_EM])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "4578461"}]}