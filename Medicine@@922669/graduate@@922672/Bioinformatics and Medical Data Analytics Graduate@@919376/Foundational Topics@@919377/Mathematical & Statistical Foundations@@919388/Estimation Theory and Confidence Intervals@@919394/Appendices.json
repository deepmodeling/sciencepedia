{"hands_on_practices": [{"introduction": "A crucial step in designing any quantitative study is determining the required sample size. This exercise puts theory into practice by tasking you with a common scenario in clinical research: calculating the number of subjects needed to estimate a biomarker's mean concentration with a pre-specified level of precision. By deriving the relationship between sample size $n$, variance $\\sigma^2$, and confidence interval width from first principles, you will solidify your understanding of how these fundamental components interact in experimental design [@problem_id:4560495].", "problem": "A clinical biomarker of systemic inflammation is quantified in milligrams per liter (mg/L). Investigators plan a study to estimate the population mean biomarker level, denoted by $\\mu$, using a two-sided confidence interval (CI) at confidence level $1 - \\alpha = 0.95$. Based on a prior meta-analysis across independent cohorts, the within-subject measurement process and biological variability for this biomarker are well-modeled by independent and identically distributed (i.i.d.) Gaussian observations $X_{1}, X_{2}, \\dots, X_{n}$ with mean $\\mu$ and known variance $\\sigma^{2}$. The meta-analysis provides an externally validated variance estimate $\\sigma^{2} = 64$ (mg/L)$^{2}$ that can be treated as known for planning purposes.\n\nStarting from the core definitions of a two-sided confidence interval and the sampling distribution of the sample mean under the Gaussian model with known variance, derive a relationship connecting the CI width $W$, the sample size $n$, the variance $\\sigma^{2}$, and the appropriate quantile of the standard normal distribution. Then, using this relationship, determine the smallest integer $n$ that guarantees a planned two-sided CI for $\\mu$ has total width $W = 3$ mg/L at confidence level $0.95$.\n\nReport the final sample size $n$ as a single integer. If non-integer calculations occur, choose the smallest integer $n$ that satisfies the width constraint $W \\leq 3$ mg/L. Do not include any units in your final numeric answer.", "solution": "The problem asks for the determination of the minimum sample size, $n$, required to achieve a specified confidence interval width for a population mean, $\\mu$, given a known population variance, $\\sigma^2$.\n\n### Step 1: Problem Validation\n\n**1.1. Extraction of Givens:**\n- Population parameter to be estimated: Population mean biomarker level, $\\mu$.\n- Data model: Independent and identically distributed (i.i.d.) Gaussian observations $X_{1}, X_{2}, \\dots, X_{n}$.\n- Distribution of observations: $X_i \\sim N(\\mu, \\sigma^2)$.\n- Known population variance: $\\sigma^2 = 64$ (mg/L)$^2$.\n- Confidence level: $1 - \\alpha = 0.95$.\n- Desired total width of the two-sided confidence interval: $W = 3$ mg/L.\n- Constraint: The final sample size $n$ must be the smallest integer that satisfies the width requirement.\n\n**1.2. Validation using Extracted Givens:**\n- **Scientific Grounding:** The problem is firmly grounded in standard statistical theory for hypothesis testing and estimation, specifically the construction of confidence intervals for the mean of a normal distribution with known variance. This is a canonical problem in biostatistics and clinical trial design. The values provided are plausible for a biological measurement.\n- **Well-Posedness:** The problem is well-posed. It provides all necessary information ($\\sigma^2$, confidence level, target width) to determine a unique integer solution for the sample size $n$.\n- **Objectivity:** The problem statement is objective, precise, and free of ambiguity or subjective language.\n\n**1.3. Verdict:**\nThe problem is valid. It is scientifically sound, well-posed, and objective. There are no contradictions, missing information, or violations of fundamental principles. We may proceed with the solution.\n\n### Step 2: Derivation and Solution\n\nThe estimator for the population mean $\\mu$ is the sample mean, $\\bar{X}$, defined as:\n$$\n\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i\n$$\nGiven that $X_i$ are i.i.d. random variables drawn from a normal distribution $N(\\mu, \\sigma^2)$, the sampling distribution of the sample mean $\\bar{X}$ is also normal. The mean of $\\bar{X}$ is $E[\\bar{X}] = \\mu$, and its variance is $\\text{Var}(\\bar{X}) = \\frac{\\sigma^2}{n}$. Thus, the sampling distribution is:\n$$\n\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n$$\nTo construct a confidence interval, we use a pivotal quantity whose distribution does not depend on the unknown parameter $\\mu$. We standardize $\\bar{X}$ to obtain a standard normal random variable, $Z$:\n$$\nZ = \\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}} \\sim N(0, 1)\n$$\nA two-sided confidence interval with confidence level $1 - \\alpha$ is constructed from the probability statement:\n$$\nP(-z_{\\alpha/2} \\le Z \\le z_{\\alpha/2}) = 1 - \\alpha\n$$\nwhere $z_{\\alpha/2}$ is the upper $\\alpha/2$ quantile of the standard normal distribution, defined by $P(Z > z_{\\alpha/2}) = \\alpha/2$.\n\nSubstituting the expression for $Z$ gives:\n$$\nP\\left(-z_{\\alpha/2} \\le \\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}} \\le z_{\\alpha/2}\\right) = 1 - \\alpha\n$$\nWe rearrange the inequality to isolate the parameter $\\mu$:\n$$\n-z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\le \\bar{X} - \\mu \\le z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\n$$\n$$\n-\\bar{X} - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\le -\\mu \\le -\\bar{X} + z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\n$$\n$$\n\\bar{X} - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\le \\mu \\le \\bar{X} + z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\n$$\nThis defines the $100(1-\\alpha)\\%$ confidence interval for $\\mu$:\n$$\n\\text{CI} = \\left[ \\bar{X} - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}, \\bar{X} + z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\right]\n$$\nThe total width, $W$, of this confidence interval is the difference between the upper bound and the lower bound:\n$$\nW = \\left( \\bar{X} + z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\right) - \\left( \\bar{X} - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\right) = 2 z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\n$$\nThis equation provides the required relationship connecting the width $W$, the sample size $n$, the standard deviation $\\sigma$, and the standard normal quantile $z_{\\alpha/2}$.\n\nThe problem requires that the total width be $W = 3$ mg/L. To be more precise, the planned width should be at most $3$ mg/L. Thus, we have the inequality:\n$$\nW \\le 3 \\implies 2 z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\le 3\n$$\nWe can now solve for $n$:\n$$\n\\sqrt{n} \\ge \\frac{2 z_{\\alpha/2} \\sigma}{3}\n$$\n$$\nn \\ge \\left( \\frac{2 z_{\\alpha/2} \\sigma}{3} \\right)^2\n$$\nNow we substitute the given values.\nThe confidence level is $1 - \\alpha = 0.95$, so $\\alpha = 0.05$ and $\\alpha/2 = 0.025$. The corresponding quantile for the standard normal distribution is $z_{0.025}$. This is the value such that the cumulative probability is $1 - 0.025 = 0.975$. The standard value for $z_{0.025}$ is approximately $1.96$.\nThe known variance is $\\sigma^2 = 64$ (mg/L)$^2$, so the standard deviation is $\\sigma = \\sqrt{64} = 8$ mg/L.\nThe maximum desired width is $W = 3$ mg/L.\n\nSubstituting these values into the inequality for $n$:\n$$\nn \\ge \\left( \\frac{2 \\times 1.96 \\times 8}{3} \\right)^2\n$$\n$$\nn \\ge \\left( \\frac{31.36}{3} \\right)^2\n$$\n$$\nn \\ge (10.4533...)^2\n$$\n$$\nn \\ge 109.2718...\n$$\nSince the sample size $n$ must be an integer, we must take the smallest integer greater than or equal to $109.2718...$. This is necessary to ensure the width constraint $W \\le 3$ is met. Therefore, the minimum required sample size is $n=110$.", "answer": "$$\\boxed{110}$$", "id": "4560495"}, {"introduction": "While we often use estimators like the sample mean and variance as plug-in tools, a deeper understanding of their theoretical properties is essential for robust statistical practice. This problem takes you under the hood of Maximum Likelihood Estimation (MLE), a foundational method for deriving estimators. You will derive the MLEs for the parameters of a normal distribution and uncover a critical subtlety: the MLE for variance is inherently biased, systematically underestimating the true value [@problem_id:4560497]. Quantifying this bias is a key step toward understanding why corrections, such as Bessel's correction, are necessary.", "problem": "A consortium analyzing plasma metabolomic profiles in a Phase II oncology study collects $n$ independent technical replicate measurements of a log-transformed biomarker concentration, denoted by $X_{1}, X_{2}, \\dots, X_{n}$. Assume a probabilistic model in which the replicates are independent and identically distributed as a Gaussian (normal) random sample with unknown mean and variance, specifically $X_{i} \\sim \\mathcal{N}(\\mu, \\sigma^{2})$, where $\\mu \\in \\mathbb{R}$ and $\\sigma^{2} > 0$. Using first principles of Maximum Likelihood Estimation (MLE), derive the estimators that maximize the likelihood for $(\\mu, \\sigma^{2})$, and then rigorously demonstrate whether the MLE of $\\sigma^{2}$ is unbiased. Quantify any bias exactly by taking the expectation of the estimator under the true model and simplifying it to a closed-form analytic expression. Your argument should invoke core definitions, properties of expectations and variances, and the independence structure implied by the model without relying on any shortcut formulas. This bias has direct implications for downstream confidence interval construction in bioinformatics pipelines; therefore, you must compute the exact bias term as the final quantity. Provide your final answer as a single closed-form symbolic expression for the bias of the MLE of $\\sigma^{2}$. No rounding is required.", "solution": "The problem requires the derivation of the Maximum Likelihood Estimators (MLEs) for the parameters $(\\mu, \\sigma^{2})$ of a Gaussian distribution and the subsequent calculation of the bias of the variance estimator, $\\hat{\\sigma}^{2}_{MLE}$.\n\nThe data consist of $n$ independent and identically distributed (i.i.d.) random variables $X_{1}, X_{2}, \\dots, X_{n}$, where each $X_{i}$ follows a normal (Gaussian) distribution with mean $\\mu$ and variance $\\sigma^{2}$. The probability density function (PDF) for a single observation $X_{i}$ is given by:\n$$f(x_{i} | \\mu, \\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\left(-\\frac{(x_{i} - \\mu)^{2}}{2\\sigma^{2}}\\right)$$\n\nSince the observations are i.i.d., the joint PDF, which is the likelihood function $L(\\mu, \\sigma^{2} | \\mathbf{x})$, is the product of the individual PDFs:\n$$L(\\mu, \\sigma^{2} | \\mathbf{x}) = \\prod_{i=1}^{n} f(x_{i} | \\mu, \\sigma^{2}) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\left(-\\frac{(x_{i} - \\mu)^{2}}{2\\sigma^{2}}\\right)$$\n$$L(\\mu, \\sigma^{2} | \\mathbf{x}) = \\left(\\frac{1}{2\\pi\\sigma^{2}}\\right)^{n/2} \\exp\\left(-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}(x_{i} - \\mu)^{2}\\right)$$\n\nTo simplify the maximization process, we work with the log-likelihood function, $\\ell(\\mu, \\sigma^{2}) = \\ln L(\\mu, \\sigma^{2})$.\n$$\\ell(\\mu, \\sigma^{2}) = \\ln\\left[ \\left(2\\pi\\sigma^{2}\\right)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}(x_{i} - \\mu)^{2}\\right) \\right]$$\n$$\\ell(\\mu, \\sigma^{2}) = -\\frac{n}{2}\\ln(2\\pi\\sigma^{2}) - \\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}(x_{i} - \\mu)^{2}$$\n$$\\ell(\\mu, \\sigma^{2}) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^{2}) - \\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}(x_{i} - \\mu)^{2}$$\n\nTo find the MLEs, we compute the partial derivatives of the log-likelihood function with respect to $\\mu$ and $\\sigma^{2}$ and set them to zero. These are known as the score equations.\n\nFirst, for $\\mu$:\n$$\\frac{\\partial\\ell}{\\partial\\mu} = \\frac{\\partial}{\\partial\\mu} \\left[ -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^{2}) - \\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}(x_{i} - \\mu)^{2} \\right]$$\n$$\\frac{\\partial\\ell}{\\partial\\mu} = -\\frac{1}{2\\sigma^{2}} \\sum_{i=1}^{n} 2(x_{i} - \\mu)(-1) = \\frac{1}{\\sigma^{2}}\\sum_{i=1}^{n}(x_{i} - \\mu)$$\nSetting the derivative to zero to find the critical point for $\\mu$:\n$$\\frac{1}{\\sigma^{2}}\\sum_{i=1}^{n}(x_{i} - \\hat{\\mu}) = 0 \\implies \\sum_{i=1}^{n}x_{i} - n\\hat{\\mu} = 0$$\n$$\\hat{\\mu}_{MLE} = \\frac{1}{n}\\sum_{i=1}^{n}x_{i} = \\bar{x}$$\nThe MLE for the mean $\\mu$ is the sample mean $\\bar{X}$.\n\nNext, for $\\sigma^{2}$:\n$$\\frac{\\partial\\ell}{\\partial\\sigma^{2}} = \\frac{\\partial}{\\partial\\sigma^{2}} \\left[ -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^{2}) - \\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}(x_{i} - \\mu)^{2} \\right]$$\n$$\\frac{\\partial\\ell}{\\partial\\sigma^{2}} = -\\frac{n}{2\\sigma^{2}} + \\frac{1}{2(\\sigma^{2})^{2}}\\sum_{i=1}^{n}(x_{i} - \\mu)^{2}$$\nSetting this derivative to zero and substituting the symbol for the estimator $\\hat{\\sigma}^{2}$:\n$$-\\frac{n}{2\\hat{\\sigma}^{2}} + \\frac{1}{2(\\hat{\\sigma}^{2})^{2}}\\sum_{i=1}^{n}(x_{i} - \\mu)^{2} = 0$$\nMultiplying by $2(\\hat{\\sigma}^{2})^{2}$ gives:\n$$-n\\hat{\\sigma}^{2} + \\sum_{i=1}^{n}(x_{i} - \\mu)^{2} = 0$$\n$$\\hat{\\sigma}^{2} = \\frac{1}{n}\\sum_{i=1}^{n}(x_{i} - \\mu)^{2}$$\nBy the invariance property of MLEs, we substitute the MLE for $\\mu$, which is $\\hat{\\mu}_{MLE} = \\bar{x}$, into this expression:\n$$\\hat{\\sigma}^{2}_{MLE} = \\frac{1}{n}\\sum_{i=1}^{n}(x_{i} - \\bar{x})^{2}$$\nThis is the MLE for the variance $\\sigma^{2}$.\n\nNow we must determine if $\\hat{\\sigma}^{2}_{MLE}$ is an unbiased estimator. An estimator $\\hat{\\theta}$ for a parameter $\\theta$ is unbiased if its expected value is equal to the true parameter value, i.e., $E[\\hat{\\theta}] = \\theta$. The bias is defined as $\\text{Bias}(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta$. We must compute $E[\\hat{\\sigma}^{2}_{MLE}]$.\n\n$$E[\\hat{\\sigma}^{2}_{MLE}] = E\\left[ \\frac{1}{n}\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2} \\right] = \\frac{1}{n} E\\left[ \\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2} \\right]$$\n\nTo evaluate the expectation of the sum of squares, we rewrite the term by adding and subtracting the true mean $\\mu$:\n$$\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2} = \\sum_{i=1}^{n}((X_{i} - \\mu) - (\\bar{X} - \\mu))^{2}$$\nExpanding the binomial:\n$$= \\sum_{i=1}^{n} \\left[ (X_{i} - \\mu)^{2} - 2(X_{i} - \\mu)(\\bar{X} - \\mu) + (\\bar{X} - \\mu)^{2} \\right]$$\n$$= \\sum_{i=1}^{n}(X_{i} - \\mu)^{2} - 2(\\bar{X} - \\mu)\\sum_{i=1}^{n}(X_{i} - \\mu) + \\sum_{i=1}^{n}(\\bar{X} - \\mu)^{2}$$\nThe cross-term simplifies, noting that $\\sum_{i=1}^{n}(X_{i} - \\mu) = n\\bar{X} - n\\mu = n(\\bar{X} - \\mu)$:\n$$= \\sum_{i=1}^{n}(X_{i} - \\mu)^{2} - 2(\\bar{X} - \\mu) \\cdot n(\\bar{X} - \\mu) + n(\\bar{X} - \\mu)^{2}$$\n$$= \\sum_{i=1}^{n}(X_{i} - \\mu)^{2} - 2n(\\bar{X} - \\mu)^{2} + n(\\bar{X} - \\mu)^{2}$$\n$$= \\sum_{i=1}^{n}(X_{i} - \\mu)^{2} - n(\\bar{X} - \\mu)^{2}$$\nNow, we take the expectation of this expression using the linearity of expectation:\n$$E\\left[\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2}\\right] = E\\left[\\sum_{i=1}^{n}(X_{i} - \\mu)^{2}\\right] - E\\left[n(\\bar{X} - \\mu)^{2}\\right]$$\n$$E\\left[\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2}\\right] = \\sum_{i=1}^{n}E\\left[(X_{i} - \\mu)^{2}\\right] - nE\\left[(\\bar{X} - \\mu)^{2}\\right]$$\nBy definition of variance, $\\text{Var}(Y) = E[(Y - E[Y])^{2}]$. For each $X_{i}$, $E[X_{i}] = \\mu$, so $E[(X_{i} - \\mu)^{2}] = \\text{Var}(X_{i}) = \\sigma^{2}$. For the sample mean $\\bar{X}$, $E[\\bar{X}] = \\mu$, so $E[(\\bar{X} - \\mu)^{2}] = \\text{Var}(\\bar{X})$. The variance of the sample mean of $n$ i.i.d. random variables is $\\text{Var}(\\bar{X}) = \\frac{\\sigma^{2}}{n}$.\n\nSubstituting these into the equation for the expected sum of squares:\n$$\\sum_{i=1}^{n}E\\left[(X_{i} - \\mu)^{2}\\right] = \\sum_{i=1}^{n}\\sigma^{2} = n\\sigma^{2}$$\n$$nE\\left[(\\bar{X} - \\mu)^{2}\\right] = n \\text{Var}(\\bar{X}) = n\\left(\\frac{\\sigma^{2}}{n}\\right) = \\sigma^{2}$$\nTherefore:\n$$E\\left[\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2}\\right] = n\\sigma^{2} - \\sigma^{2} = (n-1)\\sigma^{2}$$\nNow we can find the expectation of the MLE for variance:\n$$E[\\hat{\\sigma}^{2}_{MLE}] = \\frac{1}{n}E\\left[\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2}\\right] = \\frac{1}{n}(n-1)\\sigma^{2} = \\frac{n-1}{n}\\sigma^{2}$$\nSince $E[\\hat{\\sigma}^{2}_{MLE}] = \\frac{n-1}{n}\\sigma^{2} \\neq \\sigma^{2}$ for any finite $n$, the MLE of $\\sigma^{2}$ is a biased estimator.\n\nFinally, we quantify the bias exactly:\n$$\\text{Bias}(\\hat{\\sigma}^{2}_{MLE}) = E[\\hat{\\sigma}^{2}_{MLE}] - \\sigma^{2}$$\n$$\\text{Bias}(\\hat{\\sigma}^{2}_{MLE}) = \\frac{n-1}{n}\\sigma^{2} - \\sigma^{2} = \\left(\\frac{n-1}{n} - 1\\right)\\sigma^{2}$$\n$$\\text{Bias}(\\hat{\\sigma}^{2}_{MLE}) = \\left(\\frac{n-1-n}{n}\\right)\\sigma^{2} = -\\frac{1}{n}\\sigma^{2}$$\nThe bias is a negative term, indicating that the Maximum Likelihood Estimator for the variance systematically underestimates the true variance $\\sigma^{2}$. The magnitude of this underestimation is $\\frac{\\sigma^{2}}{n}$.", "answer": "$$\\boxed{-\\frac{\\sigma^{2}}{n}}$$", "id": "4560497"}, {"introduction": "Classical statistical methods, like the Student's $t$-interval, often rely on assumptions of normality that are frequently violated by real-world biomedical data, such as skewed biomarker concentrations. This computational exercise demonstrates the practical consequences of this assumption mismatch and introduces a powerful, modern solution: the nonparametric bootstrap. By simulating data from a skewed distribution, you will compare the performance of the traditional $t$-interval with the more robust Bias-Corrected and Accelerated (BCa) bootstrap interval, gaining hands-on experience with a technique essential for reliable inference with complex data [@problem_id:4560482].", "problem": "You are given independent and identically distributed (i.i.d.) samples of a strictly positive biomarker whose population distribution is positively skewed, a common scenario in bioinformatics and medical data analytics for concentration-like measurements. Assume the biomarker concentration distribution is log-normal, which is realistic for non-negative, multiplicative biological processes, and focus on estimating the population mean. Your task is to investigate, via simulation, the coverage probability of two confidence interval procedures for the mean: a Student’s $t$-interval and a nonparametric bootstrap bias-corrected and accelerated (BCa) interval.\n\nUse only fundamental and well-tested principles as the basis for design: the Central Limit Theorem, the definition of a confidence interval as an interval estimator with nominal coverage, the Student’s $t$ distribution for standardized sample means when variance is unknown under normality, the definition of nonparametric bootstrap resampling for approximating sampling distributions, and the jackknife for estimating acceleration in BCa intervals. Do not assume any additional shortcut formulas beyond these foundations.\n\nDefinitions and requirements:\n- Let $X_1,\\dots,X_n$ be i.i.d. samples from a log-normal distribution with parameters $(\\mu, \\sigma)$, meaning $\\log(X_i) \\sim \\mathcal{N}(\\mu, \\sigma^2)$. The population mean of $X$ is $\\theta = \\mathbb{E}[X] = \\exp(\\mu + \\sigma^2/2)$.\n- The Student’s $t$-interval for the mean is constructed from the sample mean and sample standard deviation. Its nominal level is $1-\\alpha$ for a given $\\alpha \\in (0,1)$.\n- The bias-corrected and accelerated (BCa) interval is the nonparametric bootstrap confidence interval that uses a bias-correction and an acceleration term computed via jackknife. Use the nonparametric bootstrap resampling of the original sample to approximate the sampling distribution of the mean. Use jackknife to estimate the acceleration parameter.\n- The coverage probability for an interval $\\mathcal{I}(X_1,\\dots,X_n)$ with respect to $\\theta$ is $\\mathbb{P}(\\theta \\in \\mathcal{I}(X_1,\\dots,X_n))$. You will estimate it by Monte Carlo, as the average of the indicator $\\mathbf{1}\\{\\theta \\in \\mathcal{I}\\}$ over many independent simulated datasets.\n\nImplement a program that:\n1. For each parameter set in the test suite below, repeatedly simulates $R$ independent datasets $X_1,\\dots,X_n$ from the specified log-normal distribution.\n2. For each dataset, computes:\n   - a two-sided Student’s $t$-interval at nominal level $1-\\alpha$ for the mean,\n   - a two-sided nonparametric bootstrap BCa interval at the same nominal level for the mean, using $B$ bootstrap replicates and jackknife for the acceleration parameter.\n3. For each method and parameter set, computes the empirical coverage probability by the Monte Carlo average of the coverage indicator relative to the true mean $\\theta = \\exp(\\mu + \\sigma^2/2)$.\n4. Uses a fixed pseudorandom number generator seed of $20231105$ to ensure reproducible results across runs.\n\nMonte Carlo guidance:\n- Your Monte Carlo estimator of coverage must be the sample average of the indicator variable taking values in $\\{0,1\\}$, a float in $[0,1]$.\n- For numerical stability in BCa computation, ensure that any cumulative distribution function arguments or quantile probability levels are clipped to the open interval $(0,1)$ when necessary.\n\nAngle or physical units are not involved. All probabilities must be expressed as decimals, not percentages. Round each reported coverage and difference to four decimal places; printing with fewer trailing zeros is acceptable.\n\nTest suite:\n- Case A (moderate $n$, moderate skew): $n=20$, $\\mu=2.0$, $\\sigma=1.0$, $\\alpha=0.05$, $B=500$, $R=200$.\n- Case B (larger $n$, mild skew): $n=50$, $\\mu=2.0$, $\\sigma=0.5$, $\\alpha=0.05$, $B=400$, $R=200$.\n- Case C (small $n$, heavy skew): $n=15$, $\\mu=1.5$, $\\sigma=1.2$, $\\alpha=0.05$, $B=500$, $R=200$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- For each test case in the order A, B, C, append three floats:\n  1. the empirical coverage for the Student’s $t$-interval,\n  2. the empirical coverage for the BCa interval,\n  3. the difference “BCa minus $t$ coverage”.\n- Therefore the output must be a single line with $9$ floats: $[t_A, bca_A, \\Delta_A, t_B, bca_B, \\Delta_B, t_C, bca_C, \\Delta_C]$, where each entry is rounded to four decimal places as specified.", "solution": "Herein, we detail the theoretical principles underpinning each confidence interval procedure. The core of the solution is a Monte Carlo simulation. For a given set of parameters—sample size $n$, log-normal parameters $\\mu$ and $\\sigma$, and significance level $\\alpha$—we will perform the following steps across $R$ independent repetitions:\n1.  Generate a sample $X_1, \\dots, X_n$ from a log-normal distribution with parameters $(\\mu, \\sigma)$. The underlying normal distribution for $\\log(X_i)$ is $\\mathcal{N}(\\mu, \\sigma^2)$.\n2.  Calculate the true population mean, which is the target of our confidence intervals. For a log-normal distribution, this is $\\theta = \\mathbb{E}[X] = \\exp(\\mu + \\sigma^2/2)$.\n3.  Construct a two-sided $(1-\\alpha)$ Student's $t$-interval for the mean.\n4.  Construct a two-sided $(1-\\alpha)$ BCa bootstrap interval for the mean.\n5.  For each interval, record whether it successfully captures the true mean $\\theta$.\nAfter all $R$ repetitions are complete, the empirical coverage probability for each method is estimated as the fraction of times the corresponding interval contained $\\theta$.\n\n**Student's $t$-Confidence Interval**\nThis interval is a cornerstone of frequentist inference. For a sample $\\{x_1, \\dots, x_n\\}$, the sample mean is $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i$ and the sample standard deviation is $s = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2}$. The Central Limit Theorem (CLT) states that for a sufficiently large sample size $n$, the sampling distribution of the standardized mean $\\frac{\\bar{X} - \\theta}{S/\\sqrt{n}}$ is approximately a Student's $t$-distribution with $n-1$ degrees of freedom. This provides the basis for the interval:\n$$ \\mathcal{I}_t = \\left[ \\bar{x} - t_{1-\\alpha/2, n-1} \\frac{s}{\\sqrt{n}}, \\quad \\bar{x} + t_{1-\\alpha/2, n-1} \\frac{s}{\\sqrt{n}} \\right] $$\nwhere $t_{1-\\alpha/2, n-1}$ is the $(1-\\alpha/2)$ quantile of the Student's $t$-distribution with $n-1$ degrees of freedom. The validity of this interval relies on the normality of the sample mean's sampling distribution. For small samples from a highly skewed distribution like the log-normal, this assumption is violated, and the actual coverage probability can deviate substantially from the nominal level $1-\\alpha$.\n\n**Nonparametric Bootstrap BCa Confidence Interval**\nThe bootstrap is a resampling method used to approximate the sampling distribution of a statistic without making strong distributional assumptions about the population. The BCa interval enhances the simple percentile bootstrap by introducing two correction factors: a bias-correction term $z_0$ and an acceleration term $a$. These factors adjust the interval endpoints to account for skewness in the bootstrap distribution and the non-constant variance of the estimator, respectively, leading to higher-order accuracy.\n\nThe construction of the BCa interval proceeds as follows:\n1.  **Bootstrap Resampling**: Generate $B$ bootstrap samples by drawing $n$ observations with replacement from the original sample. For each bootstrap sample, compute the statistic of interest (the sample mean), yielding a collection of bootstrap estimates $\\{\\hat{\\theta}^*_1, \\dots, \\hat{\\theta}^*_B\\}$. This set, known as the bootstrap distribution, serves as an empirical approximation to the true sampling distribution of the sample mean $\\hat{\\theta}$.\n\n2.  **Bias-Correction ($z_0$)**: The bias-correction factor measures the median bias of the bootstrap distribution. It is calculated as:\n    $$ z_0 = \\Phi^{-1} \\left( \\frac{1}{B} \\sum_{b=1}^{B} \\mathbf{1}\\{\\hat{\\theta}^*_b < \\hat{\\theta}\\} \\right) $$\n    where $\\hat{\\theta}$ is the sample mean of the original data, $\\Phi^{-1}$ is the inverse cumulative distribution function (CDF) of the standard normal distribution $\\mathcal{N}(0, 1)$, and $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. If the bootstrap distribution is median-unbiased (i.e., its median equals $\\hat{\\theta}$), then the proportion is $0.5$ and $z_0=0$.\n\n3.  **Acceleration ($a$)**: The acceleration term accounts for the rate of change of the standard error of the estimator with respect to the parameter. It captures the skewness of the sampling distribution. It is estimated using the jackknife method:\n    -   Compute $n$ jackknife estimates of the mean, $\\hat{\\theta}_{(i)}$, each by leaving out the $i$-th observation from the original sample.\n    -   Let $\\hat{\\theta}_{(\\cdot)} = \\frac{1}{n}\\sum_{i=1}^n \\hat{\\theta}_{(i)}$ be the mean of these jackknife estimates.\n    -   The acceleration is then:\n        $$ a = \\frac{\\sum_{i=1}^n (\\hat{\\theta}_{(\\cdot)} - \\hat{\\theta}_{(i)})^3}{6 \\left[ \\sum_{i=1}^n (\\hat{\\theta}_{(\\cdot)} - \\hat{\\theta}_{(i)})^2 \\right]^{3/2}} $$\n\n4.  **Adjusted Interval Endpoints**: The BCa interval uses adjusted probability levels $\\alpha_1$ and $\\alpha_2$ to find the endpoints from the bootstrap distribution. These are computed using $z_0$ and $a$:\n    $$ \\alpha_1 = \\Phi \\left( z_0 + \\frac{z_0 + z_{\\alpha/2}}{1 - a(z_0 + z_{\\alpha/2})} \\right) $$\n    $$ \\alpha_2 = \\Phi \\left( z_0 + \\frac{z_0 + z_{1-\\alpha/2}}{1 - a(z_0 + z_{1-\\alpha/2})} \\right) $$\n    where $\\Phi$ is the standard normal CDF and $z_p = \\Phi^{-1}(p)$ is the standard normal quantile function.\n\n5.  **Final Interval**: The BCa confidence interval is given by the $\\alpha_1$ and $\\alpha_2$ quantiles of the sorted bootstrap distribution $\\{\\hat{\\theta}^*_b\\}$:\n    $$ \\mathcal{I}_{BCa} = \\left[ \\hat{\\theta}^*_{(\\alpha_1)}, \\hat{\\theta}^*_{(\\alpha_2)} \\right] $$\n\nThis sophisticated procedure is expected to yield coverage probabilities closer to the nominal $1-\\alpha$ level, especially in the challenging scenarios of small sample sizes and high skewness specified in the test suite. The simulation will provide quantitative evidence for this theoretical advantage.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import t, norm\n\ndef t_interval(data, alpha):\n    \"\"\"\n    Computes the two-sided Student's t-confidence interval for the mean.\n    \"\"\"\n    n = len(data)\n    if n < 2:\n        return np.nan, np.nan\n    \n    mean = np.mean(data)\n    std_dev = np.std(data, ddof=1)\n    se = std_dev / np.sqrt(n)\n    \n    t_critical = t.ppf(1 - alpha / 2, df=n - 1)\n    \n    lower = mean - t_critical * se\n    upper = mean + t_critical * se\n    \n    return lower, upper\n\ndef bca_interval(data, alpha, B, stat_func, rng):\n    \"\"\"\n    Computes the nonparametric bootstrap BCa confidence interval for a statistic.\n    \"\"\"\n    n = len(data)\n    if n < 2:\n        return np.nan, np.nan\n\n    # Original statistic from the sample\n    theta_hat = stat_func(data)\n\n    # 1. Generate bootstrap replicates of the statistic\n    bootstrap_indices = rng.choice(n, size=(B, n), replace=True)\n    bootstrap_samples = data[bootstrap_indices]\n    bootstrap_stats = np.apply_along_axis(stat_func, 1, bootstrap_samples)\n\n    # 2. Bias-correction factor (z0)\n    # Proportion of bootstrap stats less than the original stat\n    prop_less = np.sum(bootstrap_stats < theta_hat) / B\n    # Clip for numerical stability if prop_less is 0 or 1\n    prop_less_clipped = np.clip(prop_less, 1e-10, 1 - 1e-10)\n    z0 = norm.ppf(prop_less_clipped)\n\n    # 3. Acceleration factor (a) using jackknife\n    jackknife_stats = np.zeros(n)\n    for i in range(n):\n        jackknife_sample = np.delete(data, i)\n        jackknife_stats[i] = stat_func(jackknife_sample)\n    \n    jackknife_mean = np.mean(jackknife_stats)\n    \n    # Calculate numerator and denominator for 'a'\n    numerator = np.sum((jackknife_mean - jackknife_stats) ** 3)\n    sum_sq_diffs = np.sum((jackknife_mean - jackknife_stats) ** 2)\n\n    # Handle cases where all jackknife estimates are identical (sum_sq_diffs=0)\n    if sum_sq_diffs == 0:\n        a = 0.0\n    else:\n        denominator = 6 * (sum_sq_diffs ** 1.5)\n        a = numerator / denominator\n\n    # 4. Adjusted interval endpoints (alpha1, alpha2)\n    z_alpha_half = norm.ppf(alpha / 2)\n    z_1m_alpha_half = norm.ppf(1 - alpha / 2)\n\n    # Numerators for the adjustment fractions\n    num1 = z0 + z_alpha_half\n    num2 = z0 + z_1m_alpha_half\n    \n    # Denominators for adjustment fractions, preventing division by zero\n    den1 = 1 - a * num1\n    if den1 == 0: den1 = 1e-10\n    den2 = 1 - a * num2\n    if den2 == 0: den2 = 1e-10\n\n    # Arguments for the final CDF evaluation\n    arg1 = z0 + num1 / den1\n    arg2 = z0 + num2 / den2\n\n    alpha1 = norm.cdf(arg1)\n    alpha2 = norm.cdf(arg2)\n    \n    # 5. Final interval from quantiles of the bootstrap distribution\n    # Using np.quantile handles interpolation for non-integer indices\n    lower = np.quantile(bootstrap_stats, alpha1)\n    upper = np.quantile(bootstrap_stats, alpha2)\n\n    return lower, upper\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A (moderate n, moderate skew)\n        {'n': 20, 'mu': 2.0, 'sigma': 1.0, 'alpha': 0.05, 'B': 500, 'R': 200},\n        # Case B (larger n, mild skew)\n        {'n': 50, 'mu': 2.0, 'sigma': 0.5, 'alpha': 0.05, 'B': 400, 'R': 200},\n        # Case C (small n, heavy skew)\n        {'n': 15, 'mu': 1.5, 'sigma': 1.2, 'alpha': 0.05, 'B': 500, 'R': 200},\n    ]\n\n    # Initialize the random number generator with the specified seed\n    seed = 20231105\n    rng = np.random.default_rng(seed)\n\n    results = []\n    \n    stat_func = np.mean\n\n    for case in test_cases:\n        n, mu, sigma, alpha, B, R = case.values()\n\n        # True population mean for the log-normal distribution\n        theta_true = np.exp(mu + (sigma**2) / 2)\n\n        t_coverage_count = 0\n        bca_coverage_count = 0\n\n        for _ in range(R):\n            # 1. Generate a dataset from the log-normal distribution\n            # The `mean` and `sigma` arguments of rng.lognormal correspond to the\n            # mean and std. dev. of the underlying normal distribution.\n            data = rng.lognormal(mean=mu, sigma=sigma, size=n)\n\n            # 2. Compute the t-interval and check coverage\n            t_low, t_high = t_interval(data, alpha)\n            if t_low <= theta_true <= t_high:\n                t_coverage_count += 1\n\n            # 3. Compute the BCa interval and check coverage\n            bca_low, bca_high = bca_interval(data, alpha, B, stat_func, rng)\n            if bca_low <= theta_true <= bca_high:\n                bca_coverage_count += 1\n\n        # Calculate empirical coverage probabilities\n        t_coverage = t_coverage_count / R\n        bca_coverage = bca_coverage_count / R\n        coverage_diff = bca_coverage - t_coverage\n        \n        results.extend([\n            round(t_coverage, 4),\n            round(bca_coverage, 4),\n            round(coverage_diff, 4)\n        ])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4560482"}]}