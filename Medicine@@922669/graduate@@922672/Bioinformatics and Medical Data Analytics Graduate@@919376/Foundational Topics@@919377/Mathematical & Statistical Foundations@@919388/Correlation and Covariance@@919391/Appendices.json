{"hands_on_practices": [{"introduction": "Understanding the relationship between different clinical measurements is a cornerstone of medical data analysis. This exercise provides a foundational practice in calculating the Pearson correlation coefficient from raw covariance and variance data [@problem_id:4550376]. By working through this example, you will solidify your understanding of how correlation acts as a normalized, scale-invariant measure of linear association, a crucial property when comparing biomarkers with vastly different units and scales.", "problem": "A cohort of $n = 10{,}000$ adult patients was extracted from an Electronic Health Record (EHR). For each patient, two clinical variables were recorded: fasting plasma glucose $X$ in $\\mathrm{mg/dL}$ and Hemoglobin A1c (HbA1c) $Y$ recorded as a decimal fraction (for example, $0.068$ denotes $6.8$ in percent terms, but values are handled as fractions without any percent sign). The cohort-level summaries are:\n- mean fasting plasma glucose $\\mu_{X} = 130$,\n- mean Hemoglobin A1c $\\mu_{Y} = 0.068$,\n- variance of fasting plasma glucose $\\sigma_{X}^{2} = 400$,\n- variance of Hemoglobin A1c $\\sigma_{Y}^{2} = 3.6 \\times 10^{-5}$,\n- covariance between fasting plasma glucose and Hemoglobin A1c $\\operatorname{Cov}(X,Y) = 0.08$.\n\nStarting only from the core definitions of variance and covariance for random variables in probability theory, derive the Pearson linear correlation between $X$ and $Y$, explain conceptually why the result is dimensionless, and then evaluate it numerically using the given summaries. Briefly interpret the magnitude of the correlation in the context of medical risk stratification for hyperglycemia-related outcomes, commenting on potential implications for multivariable modeling with $X$ and $Y$.\n\nReport the correlation coefficient as a single real number rounded to four significant figures. Do not include any units in your reported number.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. All data required for the calculation are provided, and the values are realistic within a clinical context. The problem asks for a derivation from first principles, a numerical calculation, and a conceptual interpretation, which are standard components of a rigorous scientific analysis. The problem is valid.\n\nThe solution proceeds in three parts as requested: derivation and conceptual explanation, numerical evaluation, and interpretation.\n\n**1. Derivation from Core Definitions and a priori Unit Analysis**\n\nLet $X$ and $Y$ be two random variables representing fasting plasma glucose and Hemoglobin A1c, respectively. The core definitions from probability theory for the second-order moments are as follows:\n\nThe variance of a random variable $X$ with mean $\\mu_X = E[X]$ is the expectation of the squared deviation from its mean:\n$$ \\sigma_X^2 = \\operatorname{Var}(X) = E[(X - \\mu_X)^2] $$\nSimilarly, the variance of $Y$ with mean $\\mu_Y = E[Y]$ is:\n$$ \\sigma_Y^2 = \\operatorname{Var}(Y) = E[(Y - \\mu_Y)^2] $$\nThe standard deviation is the square root of the variance, $\\sigma_X = \\sqrt{\\sigma_X^2}$ and $\\sigma_Y = \\sqrt{\\sigma_Y^2}$.\n\nThe covariance between $X$ and $Y$ measures their joint variability. It is defined as the expectation of the product of their deviations from their respective means:\n$$ \\operatorname{Cov}(X,Y) = E[(X - \\mu_X)(Y - \\mu_Y)] $$\n\nThe Pearson linear correlation coefficient, denoted $\\rho_{XY}$, is formally defined as the covariance of the two variables, normalized by the product of their standard deviations. This normalization is key to its properties.\n$$ \\rho_{XY} = \\frac{\\operatorname{Cov}(X,Y)}{\\sigma_X \\sigma_Y} $$\n\nWe can now explain why $\\rho_{XY}$ is a dimensionless quantity. Let the physical units of the variable $X$ be denoted by $[U_X]$ and the units of $Y$ be $[U_Y]$. Based on the definitions above:\n- The units of variance $\\sigma_X^2$ are $[U_X]^2$.\n- The units of standard deviation $\\sigma_X$ are $\\sqrt{[U_X]^2} = [U_X]$.\n- The units of variance $\\sigma_Y^2$ are $[U_Y]^2$.\n- The units of standard deviation $\\sigma_Y$ are $\\sqrt{[U_Y]^2} = [U_Y]$.\n- The units of covariance $\\operatorname{Cov}(X,Y)$ are the units of a product of deviations, which are $[U_X] \\cdot [U_Y]$.\n\nSubstituting these unit dimensions into the definition of the correlation coefficient:\n$$ \\text{Units}(\\rho_{XY}) = \\frac{\\text{Units}(\\operatorname{Cov}(X,Y))}{\\text{Units}(\\sigma_X) \\cdot \\text{Units}(\\sigma_Y)} = \\frac{[U_X] \\cdot [U_Y]}{[U_X] \\cdot [U_Y]} = 1 $$\nThe units in the numerator and denominator cancel completely, rendering $\\rho_{XY}$ a pure, dimensionless number. This property is what allows for a universal interpretation of its magnitude, independent of the scale or units of the original variables.\n\n**2. Numerical Evaluation**\n\nThe problem provides the following cohort-level summaries:\n- Variance of fasting plasma glucose: $\\sigma_{X}^{2} = 400 \\, (\\mathrm{mg/dL})^2$\n- Variance of Hemoglobin A1c: $\\sigma_{Y}^{2} = 3.6 \\times 10^{-5}$ (dimensionless, as it is a fraction)\n- Covariance: $\\operatorname{Cov}(X,Y) = 0.08 \\, \\mathrm{mg/dL}$\n\nFirst, we calculate the standard deviations:\nThe standard deviation of fasting plasma glucose is:\n$$ \\sigma_X = \\sqrt{\\sigma_X^2} = \\sqrt{400} = 20 \\, (\\text{in } \\mathrm{mg/dL}) $$\nThe standard deviation of Hemoglobin A1c is:\n$$ \\sigma_Y = \\sqrt{\\sigma_Y^2} = \\sqrt{3.6 \\times 10^{-5}} = \\sqrt{36 \\times 10^{-6}} = 6 \\times 10^{-3} = 0.006 \\, (\\text{dimensionless}) $$\n\nNow we use the definition of the Pearson correlation coefficient to compute its value:\n$$ \\rho_{XY} = \\frac{\\operatorname{Cov}(X,Y)}{\\sigma_X \\sigma_Y} = \\frac{0.08}{20 \\times 0.006} $$\nThe product in the denominator is:\n$$ 20 \\times 0.006 = 0.12 $$\nThus, the correlation coefficient is:\n$$ \\rho_{XY} = \\frac{0.08}{0.12} = \\frac{8}{12} = \\frac{2}{3} $$\nAs a decimal, this is $0.6666\\overline{6}$. The problem requires reporting this to four significant figures.\n$$ \\rho_{XY} \\approx 0.6667 $$\n\n**3. Interpretation in a Medical Context**\n\nThe calculated correlation coefficient $\\rho_{XY} \\approx 0.6667$ indicates a moderately strong positive linear relationship between fasting plasma glucose ($X$) and Hemoglobin A1c ($Y$).\n\n- **Strength and Direction**: A value of $\\rho_{XY}$ near $+1$ signifies a strong positive linear association. A value of approximately $0.67$ is substantial and confirms the well-established physiological principle that higher average blood glucose levels (which HbA1c measures over a period of $2$-$3$ months) are associated with higher spot measurements of glucose (like fasting plasma glucose).\n\n- **Implications for Risk Stratification**: In the context of medical risk stratification for hyperglycemia-related outcomes (e.g., assessing risk for diabetes complications), this correlation implies that $X$ and $Y$ provide partially overlapping information. A patient with a high value of one marker is very likely to have a high value of the other. However, the correlation is not perfect ($\\rho_{XY}^2 = (2/3)^2 \\approx 0.444)$, which means that only about $44\\%$ of the variance in one variable is linearly explained by the other. The remaining $56\\%$ of the variance is unexplained by this linear relationship, suggesting that each variable may still contribute unique information to a risk model.\n\n- **Implications for Multivariable Modeling**: When building a multivariable statistical model (e.g., logistic regression to predict cardiovascular events), the inclusion of both $X$ and $Y$ as predictors must be handled with care due to their correlation. This phenomenon, known as collinearity (or multicollinearity if more than two predictors are correlated), can inflate the standard errors of the estimated model coefficients. This makes it difficult to ascertain the independent contribution of each variable to the outcome. An analyst might therefore consider including only one of the two variables, using a composite score, or employing advanced regression techniques such as ridge regression or principal component analysis that are designed to handle collinear predictors. The decision would depend on the specific goals of the model: pure prediction versus etiological inference.", "answer": "$$\\boxed{0.6667}$$", "id": "4550376"}, {"introduction": "A frequent and critical error in data interpretation is equating zero correlation with a lack of relationship. This thought experiment is designed to build a rigorous, first-principles understanding of why this is false [@problem_id:5184658]. By constructing a scenario where two variables are clearly dependent yet have a covariance of zero, you will appreciate that correlation exclusively measures *linear* association and can fail to detect strong non-linear dependencies.", "problem": "In a clinical imaging biomarker pipeline constructed with Artificial Intelligence (AI), suppose a standardized latent feature extracted from a self-supervised model is quantile-calibrated across a training cohort so that the standardized latent variable $X$ is distributed as $\\mathrm{Uniform}(-1,1)$. A downstream engineered biomarker $Y$ is formed by a nonlinear energy transform $Y=X^{2}$ to improve robustness to sign ambiguity in the learned representation. Using only foundational definitions from probability theory and measure-theoretic statistics appropriate to data science and biostatistics, derive the covariance between $X$ and $Y$, and justify whether $X$ and $Y$ are independent. Your derivation must start from core definitions of expectation, covariance, and independence, and proceed without invoking any pre-packaged shortcuts.\n\nProvide a complete argument that $X$ and $Y$ are not independent that does not rely on heuristic geometry alone (e.g., do not simply say “the support lies on a curve”), but instead rests on a first-principles measure-theoretic property of independence or, equivalently, an explicit conditional probability calculation for measurable events. Express the final numerical value of the covariance as your answer. No rounding is necessary.", "solution": "The problem is assessed as valid as it is scientifically grounded, well-posed, and objective. It presents a standard, formalizable problem in probability theory, providing all necessary information for a unique solution without contradiction or ambiguity. We may proceed with the solution.\n\nThe problem requires the derivation of the covariance between two random variables, $X$ and $Y$, and a rigorous justification of whether they are independent. The derivation must proceed from first principles.\n\nLet $X$ be a random variable with a uniform distribution on the interval $[-1, 1]$. Its probability density function (PDF), denoted $f_X(x)$, is given by:\n$$ f_X(x) = \\begin{cases} \\frac{1}{1 - (-1)} = \\frac{1}{2}  \\text{for } x \\in [-1, 1] \\\\ 0  \\text{otherwise} \\end{cases} $$\nThe second random variable, $Y$, is defined by the transformation $Y = X^2$.\n\n**Part 1: Derivation of the Covariance, $\\mathrm{Cov}(X,Y)$**\n\nThe definition of covariance between two random variables $X$ and $Y$ is:\n$$ \\mathrm{Cov}(X,Y) = E[(X - E[X])(Y - E[Y])] $$\nFor computational purposes, this is equivalent to:\n$$ \\mathrm{Cov}(X,Y) = E[XY] - E[X]E[Y] $$\nWe will calculate each term in this expression.\n\nFirst, we compute the expectation of $X$, $E[X]$:\n$$ E[X] = \\int_{-\\infty}^{\\infty} x f_X(x) \\,dx = \\int_{-1}^{1} x \\left(\\frac{1}{2}\\right) \\,dx = \\frac{1}{2} \\int_{-1}^{1} x \\,dx $$\nThe integral is:\n$$ E[X] = \\frac{1}{2} \\left[ \\frac{x^2}{2} \\right]_{-1}^{1} = \\frac{1}{4} [x^2]_{-1}^{1} = \\frac{1}{4} (1^2 - (-1)^2) = \\frac{1}{4} (1 - 1) = 0 $$\nThus, $E[X] = 0$. This result is also evident from the symmetry of the uniform distribution of $X$ around $0$.\n\nNext, we compute the expectation of $Y$, $E[Y]$. Since $Y = X^2$, we have $E[Y] = E[X^2]$.\n$$ E[Y] = E[X^2] = \\int_{-\\infty}^{\\infty} x^2 f_X(x) \\,dx = \\int_{-1}^{1} x^2 \\left(\\frac{1}{2}\\right) \\,dx = \\frac{1}{2} \\int_{-1}^{1} x^2 \\,dx $$\nThe integral is:\n$$ E[Y] = \\frac{1}{2} \\left[ \\frac{x^3}{3} \\right]_{-1}^{1} = \\frac{1}{6} [x^3]_{-1}^{1} = \\frac{1}{6} (1^3 - (-1)^3) = \\frac{1}{6} (1 - (-1)) = \\frac{2}{6} = \\frac{1}{3} $$\nThus, $E[Y] = \\frac{1}{3}$.\n\nNow, we compute the expectation of the product $XY$, $E[XY]$. Substituting $Y = X^2$:\n$$ E[XY] = E[X \\cdot X^2] = E[X^3] $$\n$$ E[X^3] = \\int_{-\\infty}^{\\infty} x^3 f_X(x) \\,dx = \\int_{-1}^{1} x^3 \\left(\\frac{1}{2}\\right) \\,dx = \\frac{1}{2} \\int_{-1}^{1} x^3 \\,dx $$\nThe integrand $x^3$ is an odd function, and the interval of integration $[-1, 1]$ is symmetric about the origin. Therefore, the integral is zero. Explicitly:\n$$ E[X^3] = \\frac{1}{2} \\left[ \\frac{x^4}{4} \\right]_{-1}^{1} = \\frac{1}{8} [x^4]_{-1}^{1} = \\frac{1}{8} (1^4 - (-1)^4) = \\frac{1}{8} (1 - 1) = 0 $$\nThus, $E[XY] = 0$.\n\nFinally, we substitute these expectations into the covariance formula:\n$$ \\mathrm{Cov}(X,Y) = E[XY] - E[X]E[Y] = 0 - (0)\\left(\\frac{1}{3}\\right) = 0 $$\nThe covariance between $X$ and $Y$ is $0$. This indicates that $X$ and $Y$ are uncorrelated.\n\n**Part 2: Investigation of Independence**\n\nTwo random variables $X$ and $Y$ are defined as independent if and only if for all measurable sets (Borel sets) $A, B \\subset \\mathbb{R}$, the following condition holds:\n$$ P(X \\in A, Y \\in B) = P(X \\in A) P(Y \\in B) $$\nIf we can find even a single pair of sets $A$ and $B$ for which this equality fails, then the variables are not independent. We will demonstrate non-independence by constructing such a counterexample.\n\nLet us choose the following events (measurable sets):\n- Let $A$ be the interval $(0.5, 1]$, such that the event is $\\{X \\in (0.5, 1]\\}$.\n- Let $B$ be the interval $(0.5, 1]$, such that the event is $\\{Y \\in (0.5, 1]\\}$.\n\nFirst, we calculate the individual probabilities.\nThe probability of the event $\\{X \\in A\\}$ is:\n$$ P(X \\in A) = P(0.5  X \\le 1) = \\int_{0.5}^{1} f_X(x) \\,dx = \\int_{0.5}^{1} \\frac{1}{2} \\,dx = \\frac{1}{2} [x]_{0.5}^{1} = \\frac{1}{2}(1 - 0.5) = \\frac{0.5}{2} = \\frac{1}{4} $$\n\nTo calculate the probability of the event $\\{Y \\in B\\}$, we first determine the range of $X$ values corresponding to this event.\nThe event $\\{Y \\in B\\}$ is equivalent to $\\{0.5  Y \\le 1\\}$, which is $\\{0.5  X^2 \\le 1\\}$.\nThis inequality holds if $\\sqrt{0.5}  X \\le 1$ or if $-1 \\le X  -\\sqrt{0.5}$. Let's denote $1/\\sqrt{2}$ as $\\sqrt{0.5}$.\n$$ P(Y \\in B) = P(\\{X \\in [-1, -1/\\sqrt{2})\\} \\cup \\{X \\in (1/\\sqrt{2}, 1]\\}) $$\nSince these two intervals for $X$ are disjoint, we can sum their probabilities:\n$$ P(Y \\in B) = \\int_{-1}^{-1/\\sqrt{2}} \\frac{1}{2} \\,dx + \\int_{1/\\sqrt{2}}^{1} \\frac{1}{2} \\,dx = \\frac{1}{2} (-1/\\sqrt{2} - (-1)) + \\frac{1}{2} (1 - 1/\\sqrt{2}) $$\n$$ P(Y \\in B) = \\frac{1}{2} (1 - 1/\\sqrt{2}) + \\frac{1}{2} (1 - 1/\\sqrt{2}) = 1 - \\frac{1}{\\sqrt{2}} $$\nThe product of the individual probabilities is:\n$$ P(X \\in A) P(Y \\in B) = \\frac{1}{4} \\left(1 - \\frac{1}{\\sqrt{2}}\\right) $$\n\nNow, we calculate the joint probability $P(X \\in A, Y \\in B)$.\nThis corresponds to the event $\\{X \\in (0.5, 1]\\} \\cap \\{Y \\in (0.5, 1]\\}$.\nSubstituting the definition of $Y$, this is $\\{X \\in (0.5, 1]\\} \\cap \\{X^2 \\in (0.5, 1]\\}$.\nThe condition $X^2 \\in (0.5, 1]$ implies $X \\in [-1, -1/\\sqrt{2}) \\cup (1/\\sqrt{2}, 1]$.\nThe joint event is the intersection of the sets for $X$:\n$$ (0.5, 1] \\cap ([-1, -1/\\sqrt{2}) \\cup (1/\\sqrt{2}, 1]) $$\nSince $1/\\sqrt{2} \\approx 0.707$, it is greater than $0.5$. The intersection simplifies to:\n$$ (1/\\sqrt{2}, 1] $$\nSo, the joint probability is:\n$$ P(X \\in A, Y \\in B) = P(X \\in (1/\\sqrt{2}, 1]) = \\int_{1/\\sqrt{2}}^{1} \\frac{1}{2} \\,dx = \\frac{1}{2} [x]_{1/\\sqrt{2}}^{1} = \\frac{1}{2} \\left(1 - \\frac{1}{\\sqrt{2}}\\right) $$\n\nFinally, we compare the joint probability with the product of the marginal probabilities:\n$$ P(X \\in A, Y \\in B) = \\frac{1}{2} \\left(1 - \\frac{1}{\\sqrt{2}}\\right) $$\n$$ P(X \\in A) P(Y \\in B) = \\frac{1}{4} \\left(1 - \\frac{1}{\\sqrt{2}}\\right) $$\nClearly,\n$$ \\frac{1}{2} \\left(1 - \\frac{1}{\\sqrt{2}}\\right) \\neq \\frac{1}{4} \\left(1 - \\frac{1}{\\sqrt{2}}\\right) $$\nSince we have found a pair of events for which $P(X \\in A, Y \\in B) \\neq P(X \\in A)P(Y \\in B)$, we conclude that $X$ and $Y$ are not independent.\n\nThis example illustrates a key principle: zero covariance (uncorrelatedness) does not imply independence, unless the variables are jointly normally distributed (which is not the case here). The relationship $Y=X^2$ is a deterministic, non-linear dependency that is not captured by the linear-relationship-detecting measure of covariance. Knowing the value of $Y$ constrains the possible values of $X$ (e.g., if $Y=1/4$, then $X$ must be either $1/2$ or $-1/2$), which is the essence of statistical dependence. Our formal proof confirms this rigorously.\n\nThe final answer required is the numerical value of the covariance.\n$$ \\mathrm{Cov}(X,Y) = 0 $$", "answer": "$$\\boxed{0}$$", "id": "5184658"}, {"introduction": "In complex systems like human biology, a simple correlation between two variables can be misleading, often arising from a shared cause or an intermediate factor. This practice introduces the concept of partial correlation, a key tool for disentangling direct relationships from indirect, confounded associations [@problem_id:4550410]. By constructing and comparing marginal and partial correlation networks, you will gain hands-on experience in using the precision matrix to infer conditional dependencies, a foundational step in building graphical models of biological networks.", "problem": "A cohort study on metabolic inflammation collects standardized measurements on four variables across $n$ independent participants: inflammatory cytokine interleukin-$6$ messenger ribonucleic acid (mRNA) expression in adipose tissue ($X_1$), C-reactive protein (CRP) concentration in plasma ($X_2$), body mass index ($X_3$), and the Homeostatic Model Assessment of Insulin Resistance (HOMA-IR) ($X_4$). Assume the joint distribution of $\\{X_1, X_2, X_3, X_4\\}$ is multivariate normal with mean zero and correlation matrix $R$. The empirically estimated correlation matrix (rounded to two decimal places on off-diagonals for readability) is\n$$\nR \\;=\\;\n\\begin{pmatrix}\n1  0.70  0.30  0.25 \\\\\n0.70  1  0.40  0.35 \\\\\n0.30  0.40  1  0.65 \\\\\n0.25  0.35  0.65  1\n\\end{pmatrix}.\n$$\nUsing only core definitions of covariance, correlation, and properties of the inverse covariance (precision) matrix, do the following:\n1. Compute the partial correlation matrix implied by $R$ by conditioning each pairwise association on the remaining variables.\n2. Construct two undirected networks on nodes $\\{X_1, X_2, X_3, X_4\\}$: \n   (i) a marginal correlation network with an edge between $X_i$ and $X_j$ if and only if $|r_{ij}|  \\tau$, \n   and \n   (ii) a partial correlation network with an edge between $X_i$ and $X_j$ if and only if $|\\rho_{ij\\cdot \\text{rest}}|  \\tau$, \n   where $\\tau = 0.30$.\n3. Compare the edge sets by computing the Jaccard index of the two edge sets, defined as the size of their intersection divided by the size of their union. Round your answer to four significant figures.\n\nBriefly explain, in terms of confounding or mediation consistent with the multivariate normal graphical model, why any edges present in the marginal correlation network but absent in the partial correlation network might arise. Your final reported quantity must be the single numerical value of the Jaccard index, rounded to four significant figures.", "solution": "The problem is scientifically grounded, well-posed, and contains all necessary information to derive a unique solution. The provided correlation matrix is symmetric, has unit diagonal, and is positive definite, confirming its validity. We may therefore proceed with the solution.\n\nThe problem requires the calculation of the partial correlation matrix, the construction of two networks, and the comparison of their edge sets using the Jaccard index. The variables are denoted $X_1, X_2, X_3, X_4$.\n\nFor a set of variables following a multivariate normal distribution, the partial correlation between two variables $X_i$ and $X_j$, given all other variables (denoted as \"rest\"), is directly related to the elements of the inverse of the correlation matrix, $P = R^{-1}$. This inverse matrix $P$ is also known as the precision matrix. The formula for the partial correlation $\\rho_{ij \\cdot \\text{rest}}$ is:\n$$ \\rho_{ij \\cdot \\text{rest}} = - \\frac{p_{ij}}{\\sqrt{p_{ii} p_{jj}}} $$\nwhere $p_{ij}$ is the element in the $i$-th row and $j$-th column of the precision matrix $P$.\n\nThe given correlation matrix $R$ is:\n$$\nR \\;=\\;\n\\begin{pmatrix}\n1  0.70  0.30  0.25 \\\\\n0.70  1  0.40  0.35 \\\\\n0.30  0.40  1  0.65 \\\\\n0.25  0.35  0.65  1\n\\end{pmatrix}\n$$\nFirst, we compute the inverse of $R$ to obtain the precision matrix $P = R^{-1}$. Performing the matrix inversion yields:\n$$\nP \\;\\approx\\;\n\\begin{pmatrix}\n2.1557  -1.6186  0.1802  0.0405 \\\\\n-1.6186  2.5401  -0.5269  -0.1985 \\\\\n0.1802  -0.5269  1.8315  -1.0425 \\\\\n0.0405  -0.1985  -1.0425  1.7088\n\\end{pmatrix}\n$$\nNow, we apply the formula for partial correlation to each off-diagonal element. For example, for the pair $(X_1, X_2)$:\n$$ \\rho_{12 \\cdot 34} = - \\frac{p_{12}}{\\sqrt{p_{11} p_{22}}} \\approx - \\frac{-1.6186}{\\sqrt{(2.1557)(2.5401)}} \\approx 0.6917 $$\nRepeating this for all pairs, we construct the partial correlation matrix, where each pairwise correlation is conditioned on the two remaining variables:\n$$\n\\rho_{\\cdot\\text{rest}} \\;\\approx\\;\n\\begin{pmatrix}\n1  0.6917  -0.0907  -0.0211 \\\\\n0.6917  1  0.2443  0.0953 \\\\\n-0.0907  0.2443  1  0.5893 \\\\\n-0.0211  0.0953  0.5893  1\n\\end{pmatrix}\n$$\nThe next step is to construct the two networks on the node set $V = \\{X_1, X_2, X_3, X_4\\}$ using the threshold $\\tau = 0.30$.\n\nThe marginal correlation network has an edge set, $E_M$, where an edge $(X_i, X_j)$ exists if and only if the absolute value of the marginal correlation, $|r_{ij}|$, is greater than $0.30$. Inspecting the matrix $R$:\n- $|r_{12}| = 0.70  0.30 \\implies$ edge $(X_1, X_2) \\in E_M$\n- $|r_{13}| = 0.30 \\ngtr 0.30 \\implies$ no edge\n- $|r_{14}| = 0.25 \\ngtr 0.30 \\implies$ no edge\n- $|r_{23}| = 0.40  0.30 \\implies$ edge $(X_2, X_3) \\in E_M$\n- $|r_{24}| = 0.35  0.30 \\implies$ edge $(X_2, X_4) \\in E_M$\n- $|r_{34}| = 0.65  0.30 \\implies$ edge $(X_3, X_4) \\in E_M$\nThe edge set for the marginal network is $E_M = \\{(X_1, X_2), (X_2, X_3), (X_2, X_4), (X_3, X_4)\\}$. The size of this set is $|E_M| = 4$.\n\nThe partial correlation network has an edge set, $E_P$, where an edge $(X_i, X_j)$ exists if and only if the absolute value of the partial correlation, $|\\rho_{ij\\cdot \\text{rest}}|$, is greater than $0.30$. Inspecting the matrix $\\rho_{\\cdot\\text{rest}}$:\n- $|\\rho_{12 \\cdot 34}| \\approx 0.6917  0.30 \\implies$ edge $(X_1, X_2) \\in E_P$\n- $|\\rho_{13 \\cdot 24}| \\approx 0.0907 \\ngtr 0.30 \\implies$ no edge\n- $|\\rho_{14 \\cdot 23}| \\approx 0.0211 \\ngtr 0.30 \\implies$ no edge\n- $|\\rho_{23 \\cdot 14}| \\approx 0.2443 \\ngtr 0.30 \\implies$ no edge\n- $|\\rho_{24 \\cdot 13}| \\approx 0.0953 \\ngtr 0.30 \\implies$ no edge\n- $|\\rho_{34 \\cdot 12}| \\approx 0.5893  0.30 \\implies$ edge $(X_3, X_4) \\in E_P$\nThe edge set for the partial network is $E_P = \\{(X_1, X_2), (X_3, X_4)\\}$. The size of this set is $|E_P| = 2$.\n\nThe edges present in the marginal network $E_M$ but absent in the partial network $E_P$ are $(X_2, X_3)$ and $(X_2, X_4)$. The marginal correlation, $r_{ij}$, measures the total association between two variables, while the partial correlation, $\\rho_{ij \\cdot \\text{rest}}$, measures the remaining association after conditioning on (i.e., statistically adjusting for) all other variables. In the context of a Gaussian graphical model, the partial correlation network represents direct conditional dependencies. A missing edge, such as $(X_2, X_3)$ in the partial network, implies that $X_2$ and $X_3$ are conditionally independent given $X_1$ and $X_4$. The observed marginal correlation ($r_{23} = 0.40$) arises not from a direct relationship but is instead mediated or confounded by the other variables. For instance, the correlation between CRP ($X_2$) and BMI ($X_3$) is likely an indirect effect of their shared relationships with IL-$6$ mRNA ($X_1$) and HOMA-IR ($X_4$). The same reasoning explains the disappearance of the edge $(X_2, X_4)$.\n\nFinally, we compute the Jaccard index, $J$, of the two edge sets, which is defined as the size of their intersection divided by the size of their union.\nThe intersection of the edge sets is $E_M \\cap E_P = \\{(X_1, X_2), (X_3, X_4)\\}$. The size of the intersection is $|E_M \\cap E_P| = 2$.\nThe union of the edge sets is $E_M \\cup E_P = \\{(X_1, X_2), (X_2, X_3), (X_2, X_4), (X_3, X_4)\\}$. The size of the union is $|E_M \\cup E_P| = 4$.\nThe Jaccard index is:\n$$ J = \\frac{|E_M \\cap E_P|}{|E_M \\cup E_P|} = \\frac{2}{4} = 0.5 $$\nRounding to four significant figures as requested, the result is $0.5000$.", "answer": "$$\\boxed{0.5000}$$", "id": "4550410"}]}