{"hands_on_practices": [{"introduction": "In many scientific and clinical settings, the goal is not to prove that two methods are different, but rather to demonstrate that they are, for all practical purposes, equivalent. This practice shifts the focus from traditional difference testing to equivalence testing, a crucial concept for validating new assays or computational pipelines against a gold standard. This exercise [@problem_id:4609554] will guide you through deriving the statistical foundation of the Two One-Sided Tests (TOST) procedure and applying it to a practical design problem: calculating the sample size needed to confidently declare equivalence.", "problem": "A laboratory is validating two messenger ribonucleic acid sequencing (mRNA-seq) quantification pipelines, denoted Pipeline $A$ and Pipeline $B$, for clinical gene expression profiling. For each biological specimen, both pipelines are applied to the same raw sequencing data to produce a paired summary of expression for a fixed panel of genes. Let $D_i$ be the per-specimen difference in log-base-2 summarized expression between pipelines, defined as $D_i = \\log_{2}(X_{iA}) - \\log_{2}(X_{iB})$, where $X_{iA}$ and $X_{iB}$ are appropriately normalized expression summaries for specimen $i$ from Pipeline $A$ and Pipeline $B$, respectively. Assume that $\\{D_i\\}_{i=1}^{n}$ are independent and identically distributed as normal with unknown mean $\\mu$ and unknown variance $\\sigma^{2}$, and that $\\mu$ represents the systematic log-base-2 fold-change between pipelines.\n\nRegulatory guidance requires establishing practical equivalence of the two pipelines at a prespecified log-base-2 fold-change margin $\\delta > 0$. The target is to show that the true mean difference satisfies $|\\mu|  \\delta$. The study will use the Two One-Sided Tests (TOST) procedure at familywise Type I error level $\\alpha$ to control the probability of incorrectly concluding equivalence when $|\\mu| \\ge \\delta$.\n\nTasks:\n- Starting from the core definitions of equivalence hypotheses and the sampling distribution of the sample mean and sample variance under normality, derive the TOST rejection region in terms of the sample mean $\\bar{D}$, the sample standard deviation $s$, the sample size $n$, the equivalence margin $\\delta$, and the upper $\\alpha$ quantile of the Student $t$ distribution with $n-1$ degrees of freedom. Your derivation must proceed from the null and alternative hypotheses, the one-sample $t$-statistic with unknown variance, and the ordering of one-sided critical regions, without invoking any prepackaged equivalence-testing formulas.\n- Using a scientifically justified planning approximation based on the normal distribution for the sampling distribution of $\\bar{D}$ under the design alternative $\\mu = 0$, compute the minimal integer sample size $n$ required to achieve power $1-\\beta$ for declaring equivalence via TOST at familywise Type I error level $\\alpha$, given the following planning values obtained from a prior pilot analysis: $\\delta = 0.20$ (log-base-2 units), $\\sigma = 0.35$ (log-base-2 units), $\\alpha = 0.05$, and $1-\\beta = 0.90$. Report the smallest integer $n$ that achieves at least the target power under this approximation.\n\nThe final answer must be the single integer $n$.", "solution": "The problem consists of two parts. The first part requires the derivation of the rejection region for the Two One-Sided Tests (TOST) procedure for equivalence. The second part requires the calculation of the minimum sample size $n$ to achieve a specified power.\n\n**Part 1: Derivation of the TOST Rejection Region**\n\nThe objective is to establish practical equivalence, defined as the true mean difference $\\mu$ falling within a prespecified margin $\\delta > 0$. The hypothesis of equivalence is thus $|\\mu|  \\delta$, which can be written as $-\\delta  \\mu  \\delta$. In the framework of hypothesis testing, the claim to be demonstrated is typically the alternative hypothesis. Therefore, the alternative hypothesis for equivalence is:\n$$H_A: -\\delta  \\mu  \\delta$$\n\nThe null hypothesis, $H_0$, is the complement of the alternative hypothesis. It represents the case where the pipelines are not equivalent:\n$$H_0: |\\mu| \\ge \\delta \\quad \\text{or equivalently} \\quad H_0: \\mu \\ge \\delta \\text{ or } \\mu \\le -\\delta$$\n\nThe TOST procedure addresses this composite null hypothesis by breaking it into two separate one-sided null hypotheses:\n1.  $H_{01}: \\mu \\ge \\delta$ (the mean difference is at or above the upper equivalence margin)\n2.  $H_{02}: \\mu \\le -\\delta$ (the mean difference is at or below the lower equivalence margin)\n\nThe corresponding alternative hypotheses are:\n1.  $H_{A1}: \\mu  \\delta$\n2.  $H_{A2}: \\mu > -\\delta$\n\nTo conclude equivalence (i.e., to reject the overall null hypothesis $H_0$), both one-sided null hypotheses $H_{01}$ and $H_{02}$ must be rejected. The familywise Type I error rate is controlled at level $\\alpha$ by conducting each of the two one-sided tests at level $\\alpha$.\n\nThe data consist of differences $D_i \\sim N(\\mu, \\sigma^2)$ for $i=1, \\dots, n$. Since $\\sigma^2$ is unknown, we use the one-sample $t$-statistic. For a null hypothesis $H_0: \\mu = \\mu_0$, the statistic is:\n$$T = \\frac{\\bar{D} - \\mu_0}{s/\\sqrt{n}}$$\nwhere $\\bar{D}$ is the sample mean, $s$ is the sample standard deviation, and $n$ is the sample size. Under the null hypothesis, $T$ follows a Student's $t$-distribution with $n-1$ degrees of freedom.\n\nLet's derive the rejection region for each of the two tests.\n\n**Test 1 (against $H_{01}: \\mu \\ge \\delta$):**\nThe test is conducted at the boundary of the null region, which is $\\mu = \\delta$. The test statistic is:\n$$T_1 = \\frac{\\bar{D} - \\delta}{s/\\sqrt{n}}$$\nThe alternative is $H_{A1}: \\mu  \\delta$. This implies that we expect $\\bar{D}$ to be less than $\\delta$, leading to a negative value for $T_1$. This is a left-tailed test. We reject $H_{01}$ if the observed test statistic is smaller than the critical value $-t_{n-1, \\alpha}$, where $t_{n-1, \\alpha}$ is the upper $\\alpha$ quantile of the $t$-distribution with $n-1$ degrees of freedom.\nThe rejection condition is:\n$$T_1  -t_{n-1, \\alpha} \\implies \\frac{\\bar{D} - \\delta}{s/\\sqrt{n}}  -t_{n-1, \\alpha}$$\nRearranging for $\\bar{D}$, we get:\n$$\\bar{D}  \\delta - t_{n-1, \\alpha} \\frac{s}{\\sqrt{n}}$$\n\n**Test 2 (against $H_{02}: \\mu \\le -\\delta$):**\nThe test is conducted at the boundary of the null region, which is $\\mu = -\\delta$. The test statistic is:\n$$T_2 = \\frac{\\bar{D} - (-\\delta)}{s/\\sqrt{n}} = \\frac{\\bar{D} + \\delta}{s/\\sqrt{n}}$$\nThe alternative is $H_{A2}: \\mu > -\\delta$. This implies that we expect $\\bar{D}$ to be greater than $-\\delta$, leading to a positive value for $T_2$. This is a right-tailed test. We reject $H_{02}$ if the observed test statistic is greater than the critical value $t_{n-1, \\alpha}$.\nThe rejection condition is:\n$$T_2 > t_{n-1, \\alpha} \\implies \\frac{\\bar{D} + \\delta}{s/\\sqrt{n}} > t_{n-1, \\alpha}$$\nRearranging for $\\bar{D}$, we get:\n$$\\bar{D} > -\\delta + t_{n-1, \\alpha} \\frac{s}{\\sqrt{n}}$$\n\n**TOST Rejection Region:**\nEquivalence is declared if and only if both $H_{01}$ and $H_{02}$ are rejected. Therefore, both conditions derived above must be met simultaneously. The rejection region for the overall null hypothesis $H_0$ is the set of values for $\\bar{D}$ that satisfy both inequalities:\n$$-\\delta + t_{n-1, \\alpha} \\frac{s}{\\sqrt{n}}  \\bar{D}  \\delta - t_{n-1, \\alpha} \\frac{s}{\\sqrt{n}}$$\nThis is the derived rejection region for the TOST procedure. This condition is equivalent to the $(1-2\\alpha)$ confidence interval for $\\mu$, which is $[\\bar{D} - t_{n-1, \\alpha} \\frac{s}{\\sqrt{n}}, \\bar{D} + t_{n-1, \\alpha} \\frac{s}{\\sqrt{n}}]$, being entirely contained within the equivalence interval $(-\\delta, \\delta)$.\n\n**Part 2: Sample Size Calculation**\n\nWe need to compute the minimal integer sample size $n$ required to achieve a power of at least $1-\\beta = 0.90$ for declaring equivalence. The calculation is performed under the design alternative $\\mu=0$ and uses a normal approximation.\n\nPower is the probability of rejecting the null hypothesis $H_0$ when the alternative hypothesis $H_A$ is true. Here, we evaluate it at $\\mu=0$.\n$$\\text{Power} = P(\\text{Reject } H_0 | \\mu=0) = P\\left(-\\delta + t_{n-1, \\alpha} \\frac{s}{\\sqrt{n}}  \\bar{D}  \\delta - t_{n-1, \\alpha} \\frac{s}{\\sqrt{n}} \\;\\Bigg|\\; \\mu=0 \\right)$$\nThe problem specifies using a normal distribution approximation. This entails two substitutions:\n1.  The sample standard deviation $s$ is replaced by the planning value for the population standard deviation $\\sigma$.\n2.  The $t$-distribution quantile $t_{n-1, \\alpha}$ is replaced by the standard normal quantile $z_{\\alpha}$.\n\nThe power expression becomes:\n$$\\text{Power} \\approx P\\left(-\\delta + z_{\\alpha} \\frac{\\sigma}{\\sqrt{n}}  \\bar{D}  \\delta - z_{\\alpha} \\frac{\\sigma}{\\sqrt{n}} \\;\\Bigg|\\; \\mu=0 \\right)$$\nUnder the condition $\\mu=0$, the sampling distribution of the sample mean is $\\bar{D} \\sim N(0, \\sigma^2/n)$. We standardize the inequality by letting $Z = \\frac{\\bar{D} - 0}{\\sigma/\\sqrt{n}}$, where $Z \\sim N(0,1)$.\n$$\\text{Power} \\approx P\\left(\\frac{-\\delta + z_{\\alpha} \\frac{\\sigma}{\\sqrt{n}}}{\\sigma/\\sqrt{n}}  Z  \\frac{\\delta - z_{\\alpha} \\frac{\\sigma}{\\sqrt{n}}}{\\sigma/\\sqrt{n}}\\right)$$\n$$\\text{Power} \\approx P\\left(-\\frac{\\delta\\sqrt{n}}{\\sigma} + z_{\\alpha}  Z  \\frac{\\delta\\sqrt{n}}{\\sigma} - z_{\\alpha}\\right)$$\nLet $\\Phi(\\cdot)$ be the cumulative distribution function (CDF) of the standard normal distribution. The probability is:\n$$\\text{Power} \\approx \\Phi\\left(\\frac{\\delta\\sqrt{n}}{\\sigma} - z_{\\alpha}\\right) - \\Phi\\left(-\\frac{\\delta\\sqrt{n}}{\\sigma} + z_{\\alpha}\\right)$$\nUsing the symmetry property $\\Phi(-x) = 1 - \\Phi(x)$, this can be written as:\n$$\\text{Power} \\approx \\Phi\\left(\\frac{\\delta\\sqrt{n}}{\\sigma} - z_{\\alpha}\\right) - \\left[1 - \\Phi\\left(\\frac{\\delta\\sqrt{n}}{\\sigma} - z_{\\alpha}\\right)\\right] = 2\\Phi\\left(\\frac{\\delta\\sqrt{n}}{\\sigma} - z_{\\alpha}\\right) - 1$$\nWe require the power to be at least $1-\\beta$:\n$$2\\Phi\\left(\\frac{\\delta\\sqrt{n}}{\\sigma} - z_{\\alpha}\\right) - 1 \\ge 1-\\beta$$\n$$2\\Phi\\left(\\frac{\\delta\\sqrt{n}}{\\sigma} - z_{\\alpha}\\right) \\ge 2-\\beta$$\n$$\\Phi\\left(\\frac{\\delta\\sqrt{n}}{\\sigma} - z_{\\alpha}\\right) \\ge 1-\\frac{\\beta}{2}$$\nBy definition of the standard normal quantile $z_{\\beta/2}$, this implies:\n$$\\frac{\\delta\\sqrt{n}}{\\sigma} - z_{\\alpha} \\ge z_{\\beta/2}$$\nSolving for $\\sqrt{n}$:\n$$\\frac{\\delta\\sqrt{n}}{\\sigma} \\ge z_{\\alpha} + z_{\\beta/2} \\implies \\sqrt{n} \\ge \\frac{(z_{\\alpha} + z_{\\beta/2})\\sigma}{\\delta}$$\nSquaring both sides gives the formula for the required sample size:\n$$n \\ge \\frac{(z_{\\alpha} + z_{\\beta/2})^2 \\sigma^2}{\\delta^2}$$\nNow, we substitute the given planning values:\n-   Equivalence margin: $\\delta = 0.20$\n-   Standard deviation: $\\sigma = 0.35$\n-   Familywise Type I error level: $\\alpha = 0.05$\n-   Target power: $1-\\beta = 0.90$, which implies $\\beta = 0.10$\n\nWe need the corresponding normal quantiles:\n-   $z_{\\alpha} = z_{0.05}$ is the value such that $\\Phi(z_{0.05}) = 1-0.05 = 0.95$. $z_{0.05} \\approx 1.64485$.\n-   $z_{\\beta/2} = z_{0.10/2} = z_{0.05}$. So, $z_{\\beta/2} \\approx 1.64485$.\n\nPlugging these values into the sample size formula:\n$$n \\ge \\frac{(1.64485 + 1.64485)^2 (0.35)^2}{(0.20)^2}$$\n$$n \\ge \\frac{(2 \\times 1.64485)^2 (0.1225)}{0.04}$$\n$$n \\ge \\frac{(3.2897)^2 (0.1225)}{0.04}$$\n$$n \\ge \\frac{10.8221 \\times 0.1225}{0.04}$$\n$$n \\ge \\frac{1.32571}{0.04}$$\n$$n \\ge 33.14275$$\nSince the sample size $n$ must be an integer, we must round up to the next integer to ensure the power is at least the target level.\nThe minimal integer sample size is $n=34$.", "answer": "$$\\boxed{34}$$", "id": "4609554"}, {"introduction": "Parametric statistical tests often rely on assumptions, such as normality, that may not hold for complex biological data. This exercise [@problem_id:4609525] introduces a robust, assumption-free alternative by having you implement a permutation test from the ground up. By constructing a paired sign test based on the exact permutation distribution of sign flips, you will gain a tangible understanding of non-parametric inference and learn to couple it with the essential Benjamini-Hochberg procedure to control for false discoveries in a realistic, multi-feature analysis.", "problem": "You are given matched tumor–normal DNA methylation beta values for cohorts of patients. Each pair consists of a tumor and its matched normal for the same patient at the same cytosine-phosphate-guanine (CpG) locus. Assume that beta values are bounded in the unit interval $[0,1]$ and that pairs are exchangeable under the null hypothesis of no systematic shift between tumor and normal. Your task is to construct a permutation version of the paired sign test to detect a shift in the distribution of within-pair differences and to quantify error control via the Benjamini–Hochberg (BH) procedure.\n\nFundamental base to use:\n- Under the null hypothesis $H_0$ of no shift, the sign of the within-pair difference is equally likely to be positive or negative for any non-zero difference, and signs are independent across pairs. Formally, let $d_i = x_i^{\\mathrm{tumor}} - x_i^{\\mathrm{normal}}$. Under $H_0$, conditional on $\\{d_i \\neq 0\\}$, the signs are independent and identically distributed with probability $\\mathbb{P}(\\mathrm{sign}(d_i) = +1) = 1/2$. Pairs with $d_i = 0$ carry no information about the sign and are excluded from the effective sample size.\n- A permutation of tumor and normal within each pair is equivalent to independently flipping the sign of each non-zero $d_i$ with probability $1/2$, giving the exact permutation distribution for the sign count.\n- The Benjamini–Hochberg procedure at target false discovery rate (FDR) level $q$ rejects hypotheses by comparing ordered p-values $p_{(k)}$ to thresholds $(k/m)q$, where $m$ is the number of hypotheses tested in a family.\n\nDefinition of the test statistic and p-value:\n- Let $n$ be the number of non-zero differences and let $S$ denote the number of positive differences among these $n$. Define the two-sided test statistic as $T = \\lvert S - n/2 \\rvert$.\n- Under $H_0$ and permutation invariance, $S$ follows a binomial distribution with parameters $n$ and $1/2$, that is $S \\sim \\mathrm{Binomial}(n, 1/2)$, because the $n$ sign flips are independent and equally likely.\n- The exact permutation p-value (two-sided) is the proportion of all $2^n$ sign-flip assignments such that the resulting $S^\\ast$ satisfies $\\lvert S^\\ast - n/2 \\rvert \\ge \\lvert S - n/2 \\rvert$. Equivalently,\n$$\np = \\frac{1}{2^n} \\sum_{k=0}^{n} \\mathbf{1}\\left(\\left|k - \\frac{n}{2}\\right| \\ge \\left|S - \\frac{n}{2}\\right|\\right) \\binom{n}{k}.\n$$\n- Ties are defined by a tolerance $\\tau = 10^{-12}$: any $\\lvert d_i \\rvert  \\tau$ is treated as zero and excluded from $n$.\n\nYour program must:\n1. Implement the exact permutation paired sign test as above to compute two-sided p-values for one or multiple CpG features per dataset.\n2. Within each dataset, apply the Benjamini–Hochberg procedure at target FDR $q = 0.1$ to the vector of p-values across features in that dataset, producing a boolean decision vector indicating which features are rejected.\n\nTest suite:\n- Use the following three datasets as the complete test suite. All numbers are methylation beta values in $[0,1]$.\n\nDataset A (one feature, $n_{\\mathrm{pairs}} = 8$):\n- Normal: $\\,[0.65,\\,0.72,\\,0.48,\\,0.51,\\,0.40,\\,0.30,\\,0.55,\\,0.60]$\n- Tumor:  $\\,[0.70,\\,0.80,\\,0.55,\\,0.58,\\,0.50,\\,0.45,\\,0.62,\\,0.58]$\n\nDataset B (one feature with ties, $n_{\\mathrm{pairs}} = 10$; ties remove three pairs, leaving $n = 7$ non-zero differences):\n- Normal: $\\,[0.40,\\,0.50,\\,0.60,\\,0.55,\\,0.20,\\,0.80,\\,0.35,\\,0.35,\\,0.90,\\,0.10]$\n- Tumor:  $\\,[0.46,\\,0.48,\\,0.60,\\,0.59,\\,0.25,\\,0.80,\\,0.33,\\,0.30,\\,0.90,\\,0.13]$\n\nDataset C (four features, $n_{\\mathrm{pairs}} = 6$; columns are distinct CpG features):\n- Normal matrix ($6 \\times 4$):\n$\\begin{bmatrix}\n0.20  0.40  0.70  0.60 \\\\\n0.30  0.45  0.50  0.40 \\\\\n0.10  0.35  0.20  0.30 \\\\\n0.25  0.55  0.60  0.50 \\\\\n0.15  0.50  0.30  0.70 \\\\\n0.40  0.60  0.80  0.65 \\\\\n\\end{bmatrix}$\n- Tumor matrix ($6 \\times 4$):\n$\\begin{bmatrix}\n0.25  0.46  0.75  0.58 \\\\\n0.35  0.51  0.48  0.38 \\\\\n0.15  0.41  0.25  0.28 \\\\\n0.30  0.61  0.58  0.48 \\\\\n0.20  0.56  0.28  0.68 \\\\\n0.45  0.58  0.78  0.63 \\\\\n\\end{bmatrix}$\n\nAssumptions and conventions:\n- Use tolerance $\\tau = 10^{-12}$ when deciding whether $\\lvert d_i \\rvert$ is zero.\n- If $n = 0$, define the p-value to be $1$.\n- For the BH procedure, use $q = 0.1$ independently within each dataset’s family of features.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each dataset, output a two-element list: the first element is the list of two-sided p-values for all features in that dataset ordered as given, and the second element is the list of corresponding BH rejection decisions (booleans) in the same order. Floats must be rounded to eight decimal places and printed without trailing zeros or a trailing decimal point.\n- Concretely, the output must look like a nested list of the form\n\"[ [pvals_A, decisions_A], [pvals_B, decisions_B], [pvals_C, decisions_C] ]\"\nwhere each pvals_X is a list of floats and each decisions_X is a list of booleans.\n\nYour solution will be assessed for scientific correctness, algorithmic rigor, exact adherence to the definition of the permutation distribution, correct handling of ties, and proper implementation of the Benjamini–Hochberg procedure. No external input is required and no physical units are involved. Express answers as decimals rather than percentages or fractions in the printed output line.", "solution": "The problem requires the implementation of a permutation-based paired sign test to analyze matched tumor-normal DNA methylation data, followed by multiple testing correction using the Benjamini-Hochberg (BH) procedure. The solution is structured into three main parts: defining the statistical test for a single feature, defining the multiple testing correction procedure for a family of features, and applying these methods to the provided datasets.\n\n### 1. Permutation Paired Sign Test\n\nThe paired sign test is a non-parametric method to assess the consistency of the direction of differences within matched pairs. Its permutation version provides an exact p-value under the null hypothesis of exchangeability.\n\n#### 1.1. Test Formulation\n\nLet a dataset for a single CpG feature consist of $N$ matched pairs of beta values, $(x_i^{\\mathrm{normal}}, x_i^{\\mathrm{tumor}})$ for $i = 1, \\dots, N$.\n\nFirst, we compute the within-pair differences:\n$$\nd_i = x_i^{\\mathrm{tumor}} - x_i^{\\mathrm{normal}}\n$$\n\nThe null hypothesis, $H_0$, posits no systematic shift between tumor and normal values. Under $H_0$, the sign of any non-zero difference $d_i$ is a random outcome with equal probability of being positive or negative, i.e., $\\mathbb{P}(\\mathrm{sign}(d_i) = +1) = \\mathbb{P}(\\mathrm{sign}(d_i) = -1) = 1/2$. Differences where $d_i=0$ are uninformative regarding the direction of shift and are excluded. Due to floating-point representation, we treat any difference $d_i$ as zero if its absolute value is below a small tolerance $\\tau = 10^{-12}$:\n$$\nd_i \\text{ is considered zero if } \\lvert d_i \\rvert  \\tau\n$$\n\nLet $n$ be the number of pairs with non-zero differences. This is the effective sample size for the test. If $n=0$, all differences are zero, providing no evidence against $H_0$. In this case, the p-value is defined to be $1$.\n\nFor $n > 0$, we count the number of positive differences, denoted by $S$:\n$$\nS = \\sum_{i=1}^{N} \\mathbf{1}(d_i > \\tau)\n$$\nwhere $\\mathbf{1}(\\cdot)$ is the indicator function.\n\nUnder $H_0$, each of the $n$ non-zero differences has its sign determined by an independent Bernoulli trial with success probability $1/2$. Therefore, the total count of positive signs, $S$, follows a binomial distribution with parameters $n$ and $p=1/2$:\n$$\nS \\sim \\mathrm{Binomial}(n, 1/2)\n$$\n\n#### 1.2. P-value Calculation\n\nWe use a two-sided test to detect any systematic shift, whether positive or negative. The test statistic is defined based on the deviation of the observed count $S$ from its expected value under $H_0$, which is $n/2$. The statistic is $T = \\lvert S - n/2 \\rvert$.\n\nThe p-value is the probability of observing a deviation from the mean at least as extreme as the one observed. This corresponds to summing the probabilities of all outcomes $k \\in \\{0, \\dots, n\\}$ that are in the tails of the distribution, as defined by the observed deviation.\nThe two-sided p-value is given by:\n$$\np = \\mathbb{P}\\left(\\left|K - \\frac{n}{2}\\right| \\ge \\left|S - \\frac{n}{2}\\right|\\right) \\quad \\text{where } K \\sim \\mathrm{Binomial}(n, 1/2)\n$$\nUsing the probability mass function of the binomial distribution, $\\mathbb{P}(K=k) = \\binom{n}{k} (1/2)^k (1-1/2)^{n-k} = \\binom{n}{k} (1/2)^n$, the p-value can be computed as:\n$$\np = \\frac{1}{2^n} \\sum_{k=0}^{n} \\binom{n}{k} \\cdot \\mathbf{1}\\left(\\left|k - \\frac{n}{2}\\right| \\ge \\left|S - \\frac{n}{2}\\right|\\right)\n$$\nThis formula calculates the exact probability of obtaining a result as or more extreme than the observed result $S$, across all $2^n$ possible sign-flip combinations.\n\n### 2. Benjamini-Hochberg (BH) Procedure\n\nWhen performing multiple hypothesis tests simultaneously (e.g., one for each CpG feature), we must control for the increased risk of false positives. The Benjamini-Hochberg procedure controls the False Discovery Rate (FDR), which is the expected proportion of false rejections among all rejections.\n\nGiven a family of $m$ p-values, $\\{p_1, p_2, \\dots, p_m\\}$, and a target FDR level $q$ (here, $q=0.1$), the procedure is as follows:\n1.  Order the p-values from smallest to largest: $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$.\n2.  Find the largest integer $k$ such that the $k$-th ordered p-value, $p_{(k)}$, satisfies the condition:\n    $$\n    p_{(k)} \\le \\frac{k}{m} q\n    $$\n3.  If such a $k$ exists, reject the null hypotheses corresponding to the p-values $p_{(1)}, p_{(2)}, \\dots, p_{(k)}$.\n4.  If no such $k$ exists, do not reject any of the null hypotheses.\n\nThis procedure is applied independently to each dataset's family of features.\n\n### 3. Application to Test Suite\n\n#### Dataset A (1 feature, 8 pairs)\n-   Normal: $\\,[0.65,\\,0.72,\\,0.48,\\,0.51,\\,0.40,\\,0.30,\\,0.55,\\,0.60]$\n-   Tumor:  $\\,[0.70,\\,0.80,\\,0.55,\\,0.58,\\,0.50,\\,0.45,\\,0.62,\\,0.58]$\n-   Differences $d_i$: $\\,[0.05, 0.08, 0.07, 0.07, 0.1, 0.15, 0.07, -0.02]$\n-   All $\\lvert d_i \\rvert \\ge 10^{-12}$, so the effective sample size is $n=8$.\n-   The number of positive differences is $S=7$.\n-   The deviation from the mean is $\\lvert S - n/2 \\rvert = \\lvert 7 - 8/2 \\rvert = 3$.\n-   We need to find the p-value for $K \\sim \\mathrm{Binomial}(8, 1/2)$, which is $\\mathbb{P}(\\lvert K - 4 \\rvert \\ge 3)$. This is equivalent to $\\mathbb{P}(K \\le 1 \\text{ or } K \\ge 7)$.\n-   $p = \\mathbb{P}(K=0) + \\mathbb{P}(K=1) + \\mathbb{P}(K=7) + \\mathbb{P}(K=8)$.\n-   $p = \\frac{1}{2^8} \\left[ \\binom{8}{0} + \\binom{8}{1} + \\binom{8}{7} + \\binom{8}{8} \\right] = \\frac{1+8+8+1}{256} = \\frac{18}{256} = 0.0703125$.\n-   **BH Procedure**: $m=1$, $q=0.1$. We reject if $p_{(1)} \\le (1/1)q$. Here, $p = 0.0703125 \\le 0.1$, so the null hypothesis is rejected. The decision is True.\n\n#### Dataset B (1 feature, 10 pairs)\n-   Normal: $\\,[0.40,\\,0.50,\\,0.60,\\,0.55,\\,0.20,\\,0.80,\\,0.35,\\,0.35,\\,0.90,\\,0.10]$\n-   Tumor:  $\\,[0.46,\\,0.48,\\,0.60,\\,0.59,\\,0.25,\\,0.80,\\,0.33,\\,0.30,\\,0.90,\\,0.13]$\n-   Differences $d_i$: $\\,[0.06, -0.02, 0, 0.04, 0.05, 0, -0.02, -0.05, 0, 0.03]$.\n-   Three pairs have $d_i=0$, so they are excluded. The effective sample size is $n=7$.\n-   The non-zero differences are $[0.06, -0.02, 0.04, 0.05, -0.02, -0.05, 0.03]$.\n-   The number of positive differences is $S=4$.\n-   The deviation from the mean is $\\lvert S - n/2 \\rvert = \\lvert 4 - 7/2 \\rvert = 0.5$.\n-   We need to find the p-value for $K \\sim \\mathrm{Binomial}(7, 1/2)$, which is $\\mathbb{P}(\\lvert K - 3.5 \\rvert \\ge 0.5)$. This is equivalent to $\\mathbb{P}(K \\le 3 \\text{ or } K \\ge 4)$. This inequality holds for all possible values of $K \\in \\{0, 1, \\dots, 7\\}$.\n-   Thus, the sum of probabilities is over the entire support of the distribution, and the p-value is $1$.\n-   **BH Procedure**: $m=1$, $q=0.1$. We reject if $p \\le 0.1$. Here, $p=1 \\not\\le 0.1$, so we fail to reject the null hypothesis. The decision is False.\n\n#### Dataset C (4 features, 6 pairs)\n-   This dataset has $m=4$ features, so we calculate four p-values and apply a single BH correction. For all features, the number of pairs is $N=6$.\n-   **Feature 1**: Diffs: $[0.05, 0.05, 0.05, 0.05, 0.05, 0.05]$. All non-zero, so $n=6$. $S=6$. Deviation is $\\lvert 6 - 3 \\rvert = 3$. The p-value is $\\mathbb{P}(\\lvert K-3 \\rvert \\ge 3)$ for $K \\sim \\mathrm{Binomial}(6, 1/2)$, so $\\mathbb{P}(K=0) + \\mathbb{P}(K=6) = (\\binom{6}{0} + \\binom{6}{6})/2^6 = (1+1)/64 = 2/64 = 0.03125$.\n-   **Feature 2**: Diffs: $[0.06, 0.06, 0.06, 0.06, 0.06, -0.02]$. All non-zero, so $n=6$. $S=5$. Deviation is $\\lvert 5 - 3 \\rvert = 2$. The p-value is $\\mathbb{P}(\\lvert K-3 \\rvert \\ge 2) \\implies \\mathbb{P}(K \\le 1 \\text{ or } K \\ge 5) = (\\binom{6}{0}+\\binom{6}{1}+\\binom{6}{5}+\\binom{6}{6})/64 = (1+6+6+1)/64 = 14/64 = 0.21875$.\n-   **Feature 3**: Diffs: $[0.05, -0.02, 0.05, -0.02, -0.02, -0.02]$. All non-zero, so $n=6$. $S=2$. Deviation is $\\lvert 2 - 3 \\rvert = 1$. The p-value is $\\mathbb{P}(\\lvert K-3 \\rvert \\ge 1) \\implies \\mathbb{P}(K \\le 2 \\text{ or } K \\ge 4) = (\\binom{6}{0}+\\binom{6}{1}+\\binom{6}{2}+\\binom{6}{4}+\\binom{6}{5}+\\binom{6}{6})/64 = (1+6+15+15+6+1)/64 = 44/64 = 0.6875$.\n-   **Feature 4**: Diffs: $[-0.02, -0.02, -0.02, -0.02, -0.02, -0.02]$. All non-zero, so $n=6$. $S=0$. Deviation is $\\lvert 0 - 3 \\rvert = 3$. This is the same deviation as Feature 1, so the p-value is also $0.03125$.\n-   P-value vector: $p = [0.03125, 0.21875, 0.6875, 0.03125]$.\n-   **BH Procedure**: $m=4$, $q=0.1$.\n    1.  Ordered p-values: $p_{(1)}=0.03125$ (Feat. 1 or 4), $p_{(2)}=0.03125$ (Feat. 4 or 1), $p_{(3)}=0.21875$ (Feat. 2), $p_{(4)}=0.6875$ (Feat. 3).\n    2.  BH thresholds $(k/m)q$:\n        -   $k=1: (1/4) \\times 0.1 = 0.025$\n        -   $k=2: (2/4) \\times 0.1 = 0.05$\n        -   $k=3: (3/4) \\times 0.1 = 0.075$\n        -   $k=4: (4/4) \\times 0.1 = 0.1$\n    3.  Find largest $k$ where $p_{(k)} \\le (k/m)q$:\n        -   $k=1: p_{(1)} = 0.03125 \\not\\le 0.025$.\n        -   $k=2: p_{(2)} = 0.03125 \\le 0.05$. This holds.\n        -   $k=3: p_{(3)} = 0.21875 \\not\\le 0.075$.\n        -   The largest $k$ satisfying the condition is $k=2$.\n    4.  Decision: Reject hypotheses for $p_{(1)}$ and $p_{(2)}$. These correspond to Feature 1 and Feature 4.\n-   The final decision vector, in original order, is [True, False, False, True].\n\nThis completes the manual validation of the expected results. The implementation will follow this logic.", "answer": "[[[0.0703125],[True]],[[1],[False]],[[0.03125,0.21875,0.6875,0.03125],[True,False,False,True]]]", "id": "4609525"}, {"introduction": "The success of expensive, high-throughput experiments like RNA-sequencing hinges on careful planning and adequate statistical power. This advanced practice [@problem_id:4609496] moves beyond data analysis to the critical domain of experimental design. You will derive a power calculation formula for differential expression analysis from first principles, connecting the Central Limit Theorem, the Delta Method, and the properties of the negative binomial distribution to develop a deep, quantitative intuition for how sample size, biological variance, and effect size determine the probability of making a discovery.", "problem": "You are modeling differential gene expression in ribonucleic acid sequencing (RNA-seq) using a negative binomial sampling model with mean–variance relationship given by $\\mathrm{Var}(Y)=\\mu+\\phi \\mu^2$, where $Y$ is the read count for a gene in one sample, $\\mu$ is the mean count after library-size normalization, and $\\phi$ is the dispersion parameter. Consider a two-group comparison (treatment versus control) with $n_1$ independent replicates in group $1$ and $n_2$ independent replicates in group $2$. Let the baseline mean be $\\mu_1=\\mu$ in group $1$, and let the mean in group $2$ be $\\mu_2=\\mu \\cdot 2^{\\Delta}$, where $\\Delta$ is the log base $2$ fold-change. Assume independence across replicates and across genes, and that library-size normalization has been performed so that $\\mu$ is comparable across groups. You seek to compute the statistical power of a two-sided hypothesis test of no differential expression for a single gene while controlling the familywise error rate across $m$ independent genes using the Bonferroni correction at familywise level $\\alpha_{\\mathrm{FWER}}$.\n\nStarting from core statistical principles and definitions that are valid in this context, specifically: (i) the central limit theorem for averages of independent observations, (ii) the delta method for smooth transformations of asymptotically normal estimators, (iii) the Wald test for testing that a parameter equals zero using an asymptotically normal estimator and its standard error, and (iv) the Bonferroni correction to control the familywise error rate, derive an implementable procedure to approximate the power under the alternative when the true log base $2$ fold-change is $\\Delta$. Your derivation should be based only on the negative binomial mean–variance specification $\\mathrm{Var}(Y)=\\mu+\\phi \\mu^2$ and the definitions listed above, without assuming any ad hoc shortcut formulas not implied by those principles.\n\nYou must then implement this procedure in a program that, for each test case, computes the approximate power of the two-sided test at per-gene significance level $\\alpha_{\\mathrm{Bonf}}=\\alpha_{\\mathrm{FWER}}/m$ using a Wald statistic. The null hypothesis is that the log fold-change equals zero. Treat the fold-change on the natural logarithm scale as $\\beta=\\Delta \\cdot \\ln(2)$. The final answer for each test case must be a single real number in decimal form representing the power, rounded to $6$ decimal places.\n\nYour program must process the following test suite, where each test case is specified as a tuple $(\\mu,\\phi,\\Delta,n_1,n_2,m,\\alpha_{\\mathrm{FWER}})$:\n\n- Test case $1$: $(50,\\,0.1,\\,1,\\,3,\\,3,\\,20000,\\,0.05)$.\n- Test case $2$: $(5,\\,0.2,\\,1,\\,3,\\,3,\\,20000,\\,0.05)$.\n- Test case $3$: $(50,\\,0.1,\\,1,\\,6,\\,6,\\,20000,\\,0.05)$.\n- Test case $4$: $(20,\\,0.1,\\,0,\\,3,\\,3,\\,10,\\,0.1)$.\n- Test case $5$: $(100,\\,1.0,\\,1,\\,3,\\,3,\\,20000,\\,0.05)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test suite, with each power rounded to $6$ decimal places (for example, $\\left[\\text{result}_1,\\text{result}_2,\\dots\\right]$). No other text should be printed. All angles, if any, should be in radians. All probabilities and significance levels must be provided and reported as decimals, not as percentages.", "solution": "The problem requires the derivation and implementation of a procedure to calculate the statistical power of a two-sided hypothesis test for differential gene expression in an RNA-seq experiment. The derivation must be based on first principles: the Central Limit Theorem (CLT), the Delta Method, the Wald test, and the Bonferroni correction.\n\nLet $Y_{ik}$ be the read count for a single gene in replicate $i$ of group $k$, where $k=1$ for the control group and $k=2$ for the treatment group. The number of replicates in each group are $n_1$ and $n_2$, respectively. The counts are assumed to follow a negative binomial distribution with mean $\\mathbb{E}[Y_{ik}] = \\mu_k$ and a specified mean-variance relationship $\\mathrm{Var}(Y_{ik}) = \\mu_k + \\phi \\mu_k^2$. The group means are related by $\\mu_1 = \\mu$ and $\\mu_2 = \\mu \\cdot 2^{\\Delta}$, where $\\Delta$ is the log base $2$ fold-change. We want to test the null hypothesis $H_0: \\Delta = 0$ against the alternative $H_A: \\Delta \\neq 0$.\n\nThe derivation proceeds as follows:\n\n**1. Asymptotic Distribution of Sample Means**\nThe sample mean for each group is the average of the read counts across replicates:\n$$ \\bar{Y}_1 = \\frac{1}{n_1} \\sum_{i=1}^{n_1} Y_{i1} \\quad \\text{and} \\quad \\bar{Y}_2 = \\frac{1}{n_2} \\sum_{i=1}^{n_2} Y_{i2} $$\nAccording to the Central Limit Theorem, for sufficiently large $n_1$ and $n_2$, the sample means are approximately normally distributed. Their means are $\\mathbb{E}[\\bar{Y}_k] = \\mu_k$ and their variances are $\\mathrm{Var}(\\bar{Y}_k) = \\frac{\\mathrm{Var}(Y_{ik})}{n_k} = \\frac{\\mu_k + \\phi \\mu_k^2}{n_k}$.\nThus, we have:\n$$ \\bar{Y}_1 \\stackrel{\\text{approx}}{\\sim} N\\left(\\mu_1, \\frac{\\mu_1 + \\phi \\mu_1^2}{n_1}\\right) $$\n$$ \\bar{Y}_2 \\stackrel{\\text{approx}}{\\sim} N\\left(\\mu_2, \\frac{\\mu_2 + \\phi \\mu_2^2}{n_2}\\right) $$\nSince replicates across groups are independent, $\\bar{Y}_1$ and $\\bar{Y}_2$ are independent random variables.\n\n**2. Asymptotic Distribution of the Log-Fold Change Estimator**\nThe parameter of interest is the natural log-fold change, $\\beta = \\ln(\\mu_2/\\mu_1) = \\ln(\\mu \\cdot 2^{\\Delta} / \\mu) = \\Delta \\ln(2)$. The null hypothesis is equivalent to $H_0: \\beta=0$. A natural estimator for $\\beta$ is obtained by substituting the sample means for the true means:\n$$ \\hat{\\beta} = \\ln(\\bar{Y}_2/\\bar{Y}_1) = \\ln(\\bar{Y}_2) - \\ln(\\bar{Y}_1) $$\nTo find the asymptotic distribution of $\\hat{\\beta}$, we apply the Delta Method. Let the function be $g(\\mu_1, \\mu_2) = \\ln(\\mu_2) - \\ln(\\mu_1)$. The variance of $\\hat{\\beta} = g(\\bar{Y}_1, \\bar{Y}_2)$ is approximated by:\n$$ \\mathrm{Var}(\\hat{\\beta}) \\approx \\left(\\frac{\\partial g}{\\partial \\mu_1}\\right)^2 \\mathrm{Var}(\\bar{Y}_1) + \\left(\\frac{\\partial g}{\\partial \\mu_2}\\right)^2 \\mathrm{Var}(\\bar{Y}_2) $$\nThe partial derivatives are $\\frac{\\partial g}{\\partial \\mu_1} = -1/\\mu_1$ and $\\frac{\\partial g}{\\partial \\mu_2} = 1/\\mu_2$. Substituting these and the variances of the sample means:\n$$ \\mathrm{Var}(\\hat{\\beta}) \\approx \\left(-\\frac{1}{\\mu_1}\\right)^2 \\left(\\frac{\\mu_1 + \\phi \\mu_1^2}{n_1}\\right) + \\left(\\frac{1}{\\mu_2}\\right)^2 \\left(\\frac{\\mu_2 + \\phi \\mu_2^2}{n_2}\\right) $$\n$$ \\sigma_{\\hat{\\beta}}^2 \\equiv \\mathrm{Var}(\\hat{\\beta}) \\approx \\frac{1+\\phi\\mu_1}{n_1\\mu_1} + \\frac{1+\\phi\\mu_2}{n_2\\mu_2} = \\left(\\frac{1}{n_1\\mu_1} + \\frac{\\phi}{n_1}\\right) + \\left(\\frac{1}{n_2\\mu_2} + \\frac{\\phi}{n_2}\\right) $$\nBy the Delta Method, $\\hat{\\beta}$ is asymptotically normally distributed with mean $\\beta$ and variance $\\sigma_{\\hat{\\beta}}^2$:\n$$ \\hat{\\beta} \\stackrel{\\text{approx}}{\\sim} N\\left(\\beta, \\sigma_{\\hat{\\beta}}^2\\right) $$\n\n**3. The Wald Test Statistic**\nThe Wald statistic for testing $H_0: \\beta = 0$ is constructed as $W = \\hat{\\beta} / \\mathrm{SE}(\\hat{\\beta})$, where $\\mathrm{SE}(\\hat{\\beta})$ is an estimate of the standard error of $\\hat{\\beta}$. For large samples, the estimated standard error converges to the true standard error, $\\mathrm{SE}(\\hat{\\beta}) \\approx \\sigma_{\\hat{\\beta}}$. Thus, the distribution of the test statistic under the alternative hypothesis (where $\\beta \\neq 0$) can be approximated as:\n$$ W = \\frac{\\hat{\\beta}}{\\mathrm{SE}(\\hat{\\beta})} \\approx \\frac{\\hat{\\beta}}{\\sigma_{\\hat{\\beta}}} \\sim N\\left(\\frac{\\beta}{\\sigma_{\\hat{\\beta}}}, 1\\right) $$\nUnder the null hypothesis ($\\beta=0$), $W \\sim N(0,1)$.\n\n**4. Power Calculation**\nWe are testing $m$ independent genes and controlling the familywise error rate (FWER) at level $\\alpha_{\\mathrm{FWER}}$ using the Bonferroni correction. The significance level for each individual gene test is therefore $\\alpha_{\\mathrm{Bonf}} = \\alpha_{\\mathrm{FWER}}/m$.\nFor a two-sided test, we reject $H_0$ if the absolute value of the Wald statistic exceeds the critical value from the standard normal distribution. The critical value $z_{\\alpha_{\\mathrm{Bonf}}/2}$ is defined such that $P(Z > z_{\\alpha_{\\mathrm{Bonf}}/2}) = \\alpha_{\\mathrm{Bonf}}/2$ for $Z \\sim N(0,1)$. This is equivalent to $z_{\\alpha_{\\mathrm{Bonf}}/2} = \\Phi^{-1}(1 - \\alpha_{\\mathrm{Bonf}}/2)$, where $\\Phi$ is the standard normal cumulative distribution function (CDF).\n\nThe power of the test is the probability of rejecting $H_0$ given that the alternative hypothesis is true:\n$$ \\text{Power} = P(|W| > z_{\\alpha_{\\mathrm{Bonf}}/2} \\mid H_A) = P(W > z_{\\alpha_{\\mathrm{Bonf}}/2} \\mid H_A) + P(W  -z_{\\alpha_{\\mathrm{Bonf}}/2} \\mid H_A) $$\nUnder $H_A$, the test statistic $W$ follows a normal distribution with mean $\\theta = \\beta / \\sigma_{\\hat{\\beta}}$ and variance $1$. Let $Z \\sim N(0,1)$, such that $W = Z + \\theta$. The power is:\n$$ \\text{Power} = P(Z + \\theta > z_{\\alpha_{\\mathrm{Bonf}}/2}) + P(Z + \\theta  -z_{\\alpha_{\\mathrm{Bonf}}/2}) $$\n$$ \\text{Power} = P(Z > z_{\\alpha_{\\mathrm{Bonf}}/2} - \\theta) + P(Z  -z_{\\alpha_{\\mathrm{Bonf}}/2} - \\theta) $$\nIn terms of the standard normal CDF $\\Phi$:\n$$ \\text{Power} = \\left(1 - \\Phi(z_{\\alpha_{\\mathrm{Bonf}}/2} - \\theta)\\right) + \\Phi(-z_{\\alpha_{\\mathrm{Bonf}}/2} - \\theta) $$\nThis provides the final formula for the power approximation.\n\n**Implementation Plan:**\nFor each test case $(\\mu, \\phi, \\Delta, n_1, n_2, m, \\alpha_{\\mathrm{FWER}})$:\n1.  Calculate $\\alpha_{\\mathrm{Bonf}} = \\alpha_{\\mathrm{FWER}} / m$.\n2.  Calculate the critical value $z_{\\alpha_{\\mathrm{Bonf}}/2} = \\Phi^{-1}(1 - \\alpha_{\\mathrm{Bonf}}/2)$.\n3.  Calculate the parameters under the alternative: $\\beta = \\Delta \\ln(2)$, $\\mu_1 = \\mu$, and $\\mu_2 = \\mu \\cdot 2^\\Delta$.\n4.  Calculate the variance $\\sigma_{\\hat{\\beta}}^2 = \\left(\\frac{1}{n_1\\mu_1} + \\frac{\\phi}{n_1}\\right) + \\left(\\frac{1}{n_2\\mu_2} + \\frac{\\phi}{n_2}\\right)$.\n5.  Calculate the non-centrality parameter $\\theta = \\beta / \\sqrt{\\sigma_{\\hat{\\beta}}^2}$.\n6.  Compute the power using the derived formula.\nIf $\\Delta=0$, then $\\beta=0$ and $\\theta=0$. The power formula simplifies to $\\alpha_{\\mathrm{Bonf}}$, which is the Type I error rate, as expected.", "answer": "[0.013840,0.000538,0.122036,0.010000,0.000057]", "id": "4609496"}]}