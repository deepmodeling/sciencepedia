## Introduction
Descriptive statistics and sampling are the bedrock of quantitative science, providing the tools to summarize complex datasets and make inferences about the world. In the era of big data, particularly within bioinformatics and medical analytics, a deep understanding of these fundamentals is more critical than ever. However, the high-dimensional, noisy, and often non-ideal nature of modern biomedical data—from genomic sequences to electronic health records—presents significant challenges. Applying classical statistical methods without considering underlying data characteristics and collection processes can lead to biased results, [spurious correlations](@entry_id:755254), and incorrect scientific conclusions.

This article bridges this gap by providing a comprehensive guide to descriptive statistics and sampling tailored for the biomedical data scientist. The first chapter, **Principles and Mechanisms**, establishes the theoretical foundation, covering everything from variable types and [robust estimation](@entry_id:261282) to the formalisms of [sampling bias](@entry_id:193615) and missing data. The second chapter, **Applications and Interdisciplinary Connections**, demonstrates how these principles are applied in real-world scenarios across epidemiology, genomics, and clinical research, tackling challenges like [compositional data](@entry_id:153479) and [batch effects](@entry_id:265859). Finally, the **Hands-On Practices** section offers opportunities to implement and solidify these concepts through practical exercises. By navigating through these chapters, you will gain the expertise to not only calculate statistics, but to critically evaluate and interpret data, forming the basis for sound scientific discovery.

## Principles and Mechanisms

### The Language of Data: Variables and Measurement Scales

In biomedical data analytics, our first task is to understand the nature of the measurements we are working with. The type of a variable dictates the questions we can ask and the statistical summaries that are permissible and meaningful. A failure to respect a variable's fundamental type can lead to nonsensical conclusions. We can classify variables into several key categories, each with distinct properties. [@problem_id:4555581]

A **continuous variable** can take any value within a given range. For instance, serum sodium concentration, measured in millimoles per liter (mmol/L), is continuous. Such variables are typically measured on an **interval scale** or a **ratio scale**. Both scales have ordered values with meaningful, equal intervals between them. The key difference is that a ratio scale has a true, non-arbitrary zero point, allowing for meaningful statements about ratios (e.g., "concentration A is twice as high as concentration B"). An interval scale has an arbitrary zero (e.g., Celsius temperature). For most biomedical continuous variables like concentrations, a ratio scale can be assumed, making summaries like the **arithmetic mean** and **standard deviation** fully interpretable.

In contrast, a **discrete variable** can only take on specific, separate values. A common type in bioinformatics is the **discrete count variable**, such as the number of Ribonucleic Acid sequencing (RNA-seq) reads mapped to a specific gene. These counts are non-negative integers ($0, 1, 2, \dots$). While it is common to treat high counts as approximately continuous, for low counts, the discrete nature is paramount. The mean and variance are admissible summaries, but the distribution of such data is often skewed and requires specialized models (e.g., Poisson or Negative Binomial). Furthermore, the **[coefficient of variation](@entry_id:272423)**, defined as the ratio of the standard deviation to the mean ($CV = s / \bar{x}$), can be an informative scale-free measure of dispersion, but it becomes unstable and difficult to interpret when the mean count $\bar{x}$ is close to zero. The frequent occurrence of zero counts in single-cell and even bulk RNA-seq data, a phenomenon known as **zero-inflation**, often necessitates reporting the proportion of zeros alongside the mean of the non-zero counts to provide a complete picture. [@problem_id:4555581]

A **binary variable** is a special case of a discrete variable that can only take on two values, typically coded as $0$ and $1$. Examples include the presence ($1$) or absence ($0$) of a pathogenic gene variant. For binary data, the most natural descriptive summary is the **proportion** (or frequency) of one outcome, often denoted $\hat{p}$. The sample mean of the $0/1$ coded values is numerically identical to the [sample proportion](@entry_id:264484) of $1$s, and in a clinical context, can be interpreted as the sample **prevalence** of the condition. Uncertainty is typically characterized based on a Bernoulli or Binomial sampling model. [@problem_id:4555581]

An **ordinal variable** possesses a natural ordering, but the intervals between the categories are not necessarily equal or quantifiable. Tumor stage (e.g., I, II, III, IV) is a classic example, where stage II is more severe than stage I, but the difference in severity between I and II may not be the same as between III and IV. Because the intervals are not well-defined, calculating an arithmetic mean by assigning numerical codes (e.g., $1, 2, 3, 4$) is conceptually flawed and can be misleading, as it implicitly assumes an interval scale. Admissible summaries for [ordinal data](@entry_id:163976) include the **median** (the middle category when data are sorted) and **cumulative proportions** (e.g., the proportion of patients with stage II or higher). [@problem_id:4555581]

Finally, **[compositional data](@entry_id:153479)** consist of vectors of proportions that represent the parts of a whole. A key characteristic is the **closure constraint**: the components are non-negative and sum to a constant, typically $1$. The relative abundances of different microbial taxa in a gut microbiome sample are a prime example. This constraint induces spurious negative correlations between the components—an increase in the abundance of one taxon must be accompanied by a decrease in the abundance of at least one other. Standard statistical tools like the Pearson correlation calculated on the raw proportions can be highly misleading. Specialized techniques, such as log-ratio transformations (e.g., additive or centered log-ratio transforms), are required to move the data from the constrained simplex space into an unconstrained Euclidean space where standard [multivariate analysis](@entry_id:168581) becomes appropriate. [@problem_id:4555581]

### From Samples to Populations: The Challenge of Representation

The ultimate goal of descriptive statistics is often to infer properties of a broader **population** from a smaller **sample**. The validity of this inference rests critically on how the sample is obtained.

#### The Ideal: Simple Random Sampling

In the idealized world of theoretical statistics, we often assume **Simple Random Sampling Without Replacement (SRSWOR)**. In this design, we consider a finite population of $N$ units (e.g., patients in a biobank), from which we draw a sample of size $n$. The defining property of SRSWOR is that every possible unordered subset of $n$ distinct units has an equal probability of being selected, which is $1/\binom{N}{n}$. [@problem_id:4555605]

Within this **design-based** framework, the values of interest in the population (e.g., a biomarker level $y_i$ for each patient $i$) are considered fixed constants. The only source of randomness is the sampling process itself. A remarkable result of this design is that the sample mean, $\bar{y} = \frac{1}{n} \sum_{i \in \text{sample}} y_i$, is an **unbiased estimator** of the finite [population mean](@entry_id:175446), $\mu = \frac{1}{N} \sum_{i=1}^N y_i$. This holds true provided that:
1. The sampling follows the SRSWOR design.
2. The selection is **non-informative** (the probability of selecting a unit does not depend on its value $y_i$), which is guaranteed by SRSWOR.
3. The values $y_i$ for the sampled units are measured without error or missingness.

This unbiasedness is a powerful property, forming the bedrock of [survey sampling](@entry_id:755685) theory. It ensures that, on average, our sample estimate will hit the true population target.

#### The Reality: Bias, Confounding, and Missingness

Real-world biomedical data collection rarely conforms to the [simple random sampling](@entry_id:754862) ideal. The mechanisms governing which data are collected and observed are often complex, leading to discrepancies between the sample and the target population.

A critical distinction must be made between **sampling error** and **selection bias**. Sampling error is the random, inevitable deviation of a sample statistic from its expected value due to chance. It is a consequence of observing a sample instead of the entire population. We can reduce sampling error by increasing the sample size. **Selection bias**, on the other hand, is a [systematic error](@entry_id:142393) that causes the expected value of our estimator to differ from the true population parameter. It arises when the sample is not representative of the target population. Unlike sampling error, selection bias is not reduced by increasing the sample size; in fact, a larger sample will simply provide a more precise, but still incorrect, estimate. [@problem_id:4555601]

Consider a Genome-Wide Association Study (GWAS) aiming to estimate the minor [allele frequency](@entry_id:146872) (MAF) of a variant in all individuals infected with a virus. Suppose the true population consists of $90\%$ mild cases and $10\%$ severe cases, and the allele is more common in severe cases ($p_{\text{severe}}=0.30$) than in mild ones ($p_{\text{mild}}=0.18$). The true population MAF is a weighted average: $p = 0.90 \times 0.18 + 0.10 \times 0.30 = 0.192$. If a study intentionally oversamples severe cases, creating a sample that is $50\%$ mild and $50\%$ severe, a naive (unweighted) sample MAF will be biased. Its expected value will be $E[\hat{p}] = 0.50 \times 0.18 + 0.50 \times 0.30 = 0.24$. The difference, $0.24 - 0.192 = 0.048$, is the selection bias. This bias arises because the inclusion probability was coupled with disease severity, which in turn is associated with the [allele frequency](@entry_id:146872). To correct this, one must use a weighted estimator that accounts for the known population proportions, a technique known as [post-stratification](@entry_id:753625). [@problem_id:4555601]

This issue is pervasive in studies using Electronic Health Record (EHR) data. Imagine trying to characterize the average fasting blood glucose of all adult patients in a hospital system. The available data, the **sampling frame**, might consist of all glucose measurements recorded in the EHR. However, the **target population** is the set of all adult patients. A mismatch arises because patients are not tested at random. Patients with suspected dysglycemia are tested more frequently, and healthier patients may not be tested at all. Aggregating all measurements as if they were a simple random sample will over-represent sicker individuals and measurements taken during episodes of poor glycemic control, leading to an upwardly biased estimate of the mean and quantiles of the patient-level glucose distribution. [@problem_id:4555557]

Another major challenge is **[missing data](@entry_id:271026)**. When some values are not observed, the validity of descriptive summaries depends on the underlying missingness mechanism. We formally define these mechanisms using conditional probabilities, where $Y$ is the variable of interest, $X$ are fully observed covariates, and $R$ is a missingness indicator ($R=1$ if $Y$ is observed, $R=0$ otherwise). [@problem_id:4555580]
- **Missing Completely At Random (MCAR):** The probability of missingness is independent of both $Y$ and $X$, i.e., $P(R=1 \mid Y,X) = P(R=1)$. In this case, the observed data points are a simple random subsample of the original target sample. A **complete-case analysis** (analyzing only the records with no missing values) will produce unbiased estimates of the mean, variance, and other descriptive parameters.
- **Missing At Random (MAR):** The probability of missingness depends on the observed covariates $X$, but not on the unobserved value of $Y$ itself, after conditioning on $X$. Formally, $P(R=1 \mid Y,X) = P(R=1 \mid X)$. For example, if physicians are more likely to order a biomarker test for older patients, the missingness depends on age ($X$) but not on the biomarker value itself. Under MAR, a naive complete-case analysis is generally biased, because the subgroup with complete data is no longer a representative sample of the whole. Principled methods like **[multiple imputation](@entry_id:177416)** or **inverse probability weighting (IPW)**, which use the information in $X$ to correct for the biased selection, can produce unbiased estimates.
- **Missing Not At Random (MNAR):** The probability of missingness depends on the value of $Y$ itself, even after accounting for covariates $X$. A classic example is a biomarker assay with a **[limit of detection](@entry_id:182454) (LOD)**, where values below the LOD are recorded as missing. Here, missingness is directly dependent on having a low value of $Y$. MNAR is the most difficult case to handle. Without making strong, often untestable, assumptions about the nature of the missingness, it is impossible to obtain unbiased estimates. Simple fixes, like substituting missing values with a constant like $L/2$, generally introduce bias and are not recommended.

### Summarizing Central Tendency and Dispersion

Once we have a [representative sample](@entry_id:201715) (or have accounted for non-representativeness), we need to summarize its key features.

#### Measures of Central Tendency and the Law of Large Numbers

The most common measure of central tendency is the **[arithmetic mean](@entry_id:165355)**. For a sample of [independent and identically distributed](@entry_id:169067) (i.i.d.) observations $\{X_i\}_{i=1}^n$ with [population mean](@entry_id:175446) $\mu$ and finite variance $\sigma^2$, the sample mean $\overline{X}_n$ is an unbiased estimator of $\mu$. The **Weak Law of Large Numbers (LLN)** provides the theoretical guarantee that as the sample size $n$ grows, the sample mean converges in probability to the true [population mean](@entry_id:175446).

We can quantify the rate of this convergence using **Chebyshev's inequality**. For any random variable $Y$ with finite mean and variance, and for any tolerance $\varepsilon > 0$, the inequality states:
$$ \mathbb{P}(|Y - \mathbb{E}[Y]| \geq \varepsilon) \leq \frac{\operatorname{Var}(Y)}{\varepsilon^2} $$
Applying this to the sample mean $\overline{X}_n$, which has mean $\mu$ and variance $\sigma^2/n$, we get:
$$ \mathbb{P}(|\overline{X}_n - \mu| \geq \varepsilon) \leq \frac{\sigma^2}{n\varepsilon^2} $$
This powerful result shows that the probability of the sample mean deviating from the true mean by more than any fixed amount decreases at a rate proportional to $1/n$. It allows us to calculate the minimum sample size needed to achieve a desired level of precision. For instance, to ensure that the probability of the sample mean HbA1c deviating from the true mean by more than $\varepsilon = 0.002$ is at most $\delta = 0.05$, given a known upper bound on the standard deviation of $\sigma = 0.01$, we would need a sample size of at least $n \ge \frac{(0.01)^2}{0.05 \times (0.002)^2} = 500$ patients. [@problem_id:4555616]

#### Robustness: Handling Outliers and Heavy Tails

The mean, while theoretically elegant, has a major practical weakness: it is highly sensitive to outliers. A single extreme value can pull the mean substantially, making it a poor representation of the "typical" value in skewed or [heavy-tailed distributions](@entry_id:142737), which are common for biomedical markers. This has led to the development of **robust statistics**.

We can formalize the robustness of an estimator using two concepts: the **[breakdown point](@entry_id:165994)** and the **[influence function](@entry_id:168646)**. [@problem_id:4555607]
- The **[breakdown point](@entry_id:165994)** is the smallest fraction of contaminated data that can cause the estimator to take on an arbitrarily large or small value. The [arithmetic mean](@entry_id:165355) has a [breakdown point](@entry_id:165994) of $0$, as a single outlier is sufficient to corrupt it completely.
- The **[influence function](@entry_id:168646)** measures the effect of an infinitesimal contamination at a single point on the estimate. An estimator with a bounded [influence function](@entry_id:168646) is considered robust. The mean's [influence function](@entry_id:168646) is unbounded.

In contrast, the **[sample median](@entry_id:267994)**—the value separating the higher half from the lower half of a data set—is highly robust. It has the highest possible [breakdown point](@entry_id:165994) of $50\%$ and a bounded [influence function](@entry_id:168646). This makes it an excellent choice for summarizing central tendency in the presence of outliers or strong skewness.

Between the mean and the median lie a spectrum of other robust estimators:
- The **$\alpha$-trimmed mean** is calculated by discarding the lowest and highest $\alpha$ fraction of the data and averaging the rest. It has a [breakdown point](@entry_id:165994) of $\alpha$ and a bounded influence function, offering a tunable trade-off between the efficiency of the mean and the robustness of the median.
- **M-estimators**, such as the **Huber M-estimator**, are a general class of robust estimators. The Huber estimator behaves like the mean for data points near the center but down-weights the influence of outliers. Its influence function is bounded (it "clips" the influence of extreme values) but not redescending (its influence does not return to zero for very extreme outliers). Its [breakdown point](@entry_id:165994) cannot exceed $50\%$.

The choice of estimator involves a trade-off. For normally distributed data, the mean is the most statistically efficient. However, for [heavy-tailed distributions](@entry_id:142737), robust estimators like the median can be far more stable and, in many cases, more efficient. [@problem_id:4555607]

#### The Transformative Power of Logarithms in Biomedical Data

For strictly positive, right-skewed data like cytokine concentrations or gene expression levels, the **logarithmic transformation** is an indispensable tool. Its utility stems from several key properties. [@problem_id:4555582]

First, it can **stabilize variance**. Many biological processes exhibit multiplicative, rather than additive, error. A plausible model for a measured concentration $X$ is $X = C \cdot E$, where $C$ is the true latent concentration and $E$ is a multiplicative error term. This structure leads to heteroscedasticity: the variance of the measurement increases with its mean. The log-transformation converts this to an additive error model: $\log X = \log C + \log E$. If the variance of $\log E$ is constant, the noise on the log-scale becomes homoscedastic, satisfying a key assumption of many statistical models.

Second, it effectively handles **multiplicative calibration or batch effects**. If measurements from a particular lab or site are subject to a systemic multiplicative factor $s$ (i.e., $X^{\star} = s \cdot X$), this becomes an additive constant on the log scale: $\log X^{\star} = \log s + \log X$. When comparing two groups measured at the same site, this additive site effect cancels out in the difference, making comparisons robust to such scaling factors.

Third, it addresses skewness. For data that are approximately log-normally distributed, the log-transformed data are approximately normal. The mean of the log-transformed data, when back-transformed to the original scale via exponentiation ($\exp\{\overline{\log X}\}$), gives the **geometric mean**. This statistic estimates the population median and is a far more robust measure of central tendency for right-skewed data than the arithmetic mean, as it is less influenced by extreme high values. [@problem_id:4555582]

### Quantifying Relationships and Unwanted Variation

#### Measuring Bivariate Association

To quantify the linear relationship between two continuous variables, $G_1$ and $G_2$, we use the **sample covariance** and the **Pearson correlation coefficient**. The covariance measures the direction of the linear relationship, but its magnitude depends on the units of the variables. The Pearson correlation, $r$, is the covariance rescaled by the product of the sample standard deviations of $G_1$ and $G_2$. This yields a dimensionless measure of the strength and direction of the linear association, bounded between $-1$ and $1$. [@problem_id:4555596]

While widely used, the Pearson correlation's interpretation requires care, especially with complex high-throughput data like RNA-seq.
- **Linearity:** Pearson correlation only measures *linear* association. It can be zero even when a strong non-linear relationship exists.
- **Transformations:** It is invariant to linear (affine) transformations, but not to non-linear monotone transformations like the logarithm.
- **Confounding:** In RNA-seq data, raw counts are subject to technical artifacts. Differences in sequencing depth (library size) can induce spurious positive correlations between all genes. Therefore, computing correlations on raw counts is ill-advised. Meaningful interpretation of co-expression requires appropriate **normalization** and often a **[variance-stabilizing transformation](@entry_id:273381)** (such as a log-transform) to place the data on a scale where linear relationships are more interpretable. [@problem_id:4555596]
- **Missing Data:** If data are missing, the validity of the estimated correlation depends on the missingness mechanism. If data are MCAR, pairwise-complete correlation estimates are unbiased. However, if data are MNAR (e.g., one gene's expression value is more likely to be missing if the other gene has high expression), the estimated correlation can be severely biased. [@problem_id:4555596]

#### Decomposing Variation: Biological Signal vs. Technical Noise

A major challenge in high-dimensional biomedical data analysis is distinguishing true biological variation from technical artifacts. **Batch effects** are systematic, non-biological sources of variation introduced during sample processing, such as different reagent lots, equipment, or operators. [@problem_id:4555625]

Several diagnostic tools can help identify and quantify [batch effects](@entry_id:265859):
- **External Controls (Spike-ins):** Using controls like the External RNA Controls Consortium (ERCC) spike-ins, which are added in equal amounts to every sample, provides a direct way to measure technical noise. Since their true concentration is constant, any variation in their measured expression should be technical. If a large proportion of the variance in spike-in levels is explained by the batch variable (e.g., using Analysis of Variance, ANOVA), it is a clear sign of a substantial [batch effect](@entry_id:154949). [@problem_id:4555625]
- **Principal Component Analysis (PCA):** PCA is a dimensionality reduction technique that finds the directions of maximal variance in the data. If the primary source of variation is a batch effect, the first principal component (PC1) will often show a strong correlation with batch labels. This is a red flag that technical noise may be overwhelming the biological signal of interest.
- **Technical Replicates:** Analyzing identical RNA aliquots processed in different batches provides a direct quantification of batch-related variance. A significant drop in the **Intraclass Correlation Coefficient (ICC)** for replicates processed across batches compared to those processed within the same batch indicates the presence of additional inter-batch technical noise. [@problem_id:4555625]

It is a common misconception that adjusting for batch effects might remove true biological signal. In a **balanced experimental design**, where biological groups (e.g., case vs. control) are distributed evenly across batches, the effects of biology and batch are mathematically separable (orthogonal). In such cases, explicitly modeling the batch as a nuisance factor in a statistical model (e.g., `expression ~ batch + disease_status`) does not remove the biological signal; instead, it accounts for the technical noise, thereby increasing the statistical power and precision for detecting the true biological effects. [@problem_id:4555625]