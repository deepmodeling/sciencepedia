{"hands_on_practices": [{"introduction": "Biomedical datasets, such as cytokine measurements, are frequently contaminated by outliers from technical artifacts or extreme biological variation. This exercise demonstrates the importance of robust statistics by contrasting the familiar sample mean with the trimmed mean, a simple yet effective alternative that is less sensitive to extreme values. Through direct calculation, you will quantify the stability of each estimator and build intuition for how to choose appropriate summary statistics when analyzing real-world data [@problem_id:4555567].", "problem": "A bioinformatics team is analyzing interleukin-6 (IL-6) cytokine concentrations measured in picograms per milliliter using Enzyme-Linked Immunosorbent Assay (ELISA). To assess robustness of summary statistics to outliers in medical data analytics, consider the following simulated IL-6 concentrations from $20$ patient samples:\n$\\{4, 5, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 400\\}$.\n\nYour tasks are:\n- Using only first principles of descriptive statistics, compute the sample mean and the $20$ percent trimmed mean of the given dataset. Interpret trimming as discarding the lowest and highest $k$ ordered values, where $k = \\lfloor 0.2 n \\rfloor$ for a dataset of size $n$, and averaging the remaining values.\n- To quantify sensitivity to an outlier, define the outlier sensitivity index for a statistic $T$ on dataset $X$ as follows. Let $X'$ be the dataset obtained by deleting the single largest observation from $X$. Define $\\Delta_{T} = |T(X) - T(X')|$. Compute the ratio\n$$S = \\frac{\\Delta_{\\bar{X}}}{\\Delta_{T_{0.2}}},$$\nwhere $\\bar{X}$ is the sample mean and $T_{0.2}$ is the $20$ percent trimmed mean, each computed independently on its respective dataset size with its own trimming parameter $k = \\lfloor 0.2 n \\rfloor$.\n\nProvide your final numeric answer as an exact rational number (no rounding). The final answer to submit is the single value of $S$ (unitless).", "solution": "The user-provided problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- **Dataset $X$**: $\\{4, 5, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 400\\}$\n- **Dataset size $n$**: $20$\n- **Task 1**: Compute the sample mean ($\\bar{X}$) and the $20\\%$ trimmed mean ($T_{0.2}$) of $X$.\n- **Definition of Trimming**: Discard the lowest and highest $k$ ordered values, where $k = \\lfloor 0.2 n \\rfloor$ for a dataset of size $n$, and average the remaining values.\n- **Task 2**: Compute the outlier sensitivity ratio $S$.\n- **Definition of $X'$**: The dataset obtained by deleting the single largest observation from $X$.\n- **Definition of Sensitivity Index $\\Delta_T$**: $\\Delta_{T} = |T(X) - T(X')|$, where $T$ is a statistic.\n- **Definition of Ratio $S$**: $S = \\frac{\\Delta_{\\bar{X}}}{\\Delta_{T_{0.2}}}$, where $\\bar{X}$ is the sample mean and $T_{0.2}$ is the $20\\%$ trimmed mean.\n- **Calculation Constraint**: For calculations on $X'$, the statistics ($\\bar{X}$ and $T_{0.2}$) are computed independently on the new dataset size $n'$ and its corresponding trimming parameter $k' = \\lfloor 0.2 n' \\rfloor$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is grounded in the established principles of descriptive statistics. The scenario of analyzing cytokine concentrations with outliers is a standard and realistic problem in medical data analytics and bioinformatics. The concepts of sample mean, trimmed mean, and sensitivity analysis are fundamental to robust statistics.\n2.  **Well-Posed**: All terms are explicitly and unambiguously defined. The dataset is provided, and the computational steps are clearly specified. The rule for re-calculating the trimming parameter for the reduced dataset $X'$ ensures there is no ambiguity, leading to a unique solution.\n3.  **Objective**: The problem is stated using precise, quantitative language, free from subjective or opinion-based claims.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is scientifically sound, well-posed, objective, and contains all necessary information for a unique solution. The solution process may now proceed.\n\n### Solution Derivation\n\nThe initial dataset is $X = \\{4, 5, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 400\\}$. The dataset is already sorted in ascending order. The size of the dataset is $n=20$.\n\n**1. Calculate Statistics for the Full Dataset $X$**\n\nFirst, we compute the sample mean of $X$, denoted $\\bar{X}(X)$.\nThe sum of all observations in $X$ is:\n$$ \\sum_{i=1}^{n} x_i = (4+5+7+9+10+11+12+13+14+15+16+17+18+19+20+21+22+23+24) + 400 $$\nThe sum of the first $19$ observations is $280$.\n$$ \\sum_{i=1}^{n} x_i = 280 + 400 = 680 $$\nThe sample mean is:\n$$ \\bar{X}(X) = \\frac{1}{n} \\sum_{i=1}^{n} x_i = \\frac{680}{20} = 34 $$\n\nNext, we compute the $20\\%$ trimmed mean of $X$, denoted $T_{0.2}(X)$.\nThe number of observations to trim from each end is $k = \\lfloor 0.2 \\times n \\rfloor = \\lfloor 0.2 \\times 20 \\rfloor = \\lfloor 4 \\rfloor = 4$.\nWe discard the $4$ smallest values $\\{4, 5, 7, 9\\}$ and the $4$ largest values $\\{22, 23, 24, 400\\}$.\nThe remaining dataset for the trimmed mean consists of $n - 2k = 20 - 2(4) = 12$ observations:\n$$ X_{T} = \\{10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21\\} $$\nThe sum of these values, which form an arithmetic progression, is:\n$$ \\sum_{x \\in X_T} x = \\frac{12}{2} (10 + 21) = 6 \\times 31 = 186 $$\nThe $20\\%$ trimmed mean is:\n$$ T_{0.2}(X) = \\frac{186}{12} = \\frac{31}{2} $$\n\n**2. Calculate Statistics for the Reduced Dataset $X'$**\n\nThe dataset $X'$ is formed by removing the single largest observation ($400$) from $X$.\n$$ X' = \\{4, 5, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24\\} $$\nThe size of this new dataset is $n' = 19$.\n\nFirst, we compute the sample mean of $X'$, denoted $\\bar{X}(X')$.\nThe sum of observations in $X'$ is the sum of $X$ minus the removed outlier:\n$$ \\sum_{x \\in X'} x = 680 - 400 = 280 $$\nThe sample mean of $X'$ is:\n$$ \\bar{X}(X') = \\frac{280}{19} $$\n\nNext, we compute the $20\\%$ trimmed mean of $X'$, denoted $T_{0.2}(X')$.\nThe number of observations to trim is recalculated based on $n'=19$:\n$$ k' = \\lfloor 0.2 \\times n' \\rfloor = \\lfloor 0.2 \\times 19 \\rfloor = \\lfloor 3.8 \\rfloor = 3 $$\nWe discard the $3$ smallest values $\\{4, 5, 7\\}$ and the $3$ largest values $\\{22, 23, 24\\}$ from $X'$.\nThe remaining dataset for the trimmed mean consists of $n' - 2k' = 19 - 2(3) = 13$ observations:\n$$ X'_{T} = \\{9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21\\} $$\nThe sum of these values, which form an arithmetic progression, is:\n$$ \\sum_{x \\in X'_{T}} x = \\frac{13}{2} (9 + 21) = \\frac{13}{2} (30) = 13 \\times 15 = 195 $$\nThe $20\\%$ trimmed mean of $X'$ is:\n$$ T_{0.2}(X') = \\frac{195}{13} = 15 $$\n\n**3. Compute the Sensitivity Indices and the Final Ratio $S$**\n\nThe sensitivity index for the sample mean, $\\Delta_{\\bar{X}}$, is:\n$$ \\Delta_{\\bar{X}} = |\\bar{X}(X) - \\bar{X}(X')| = \\left|34 - \\frac{280}{19}\\right| = \\left|\\frac{34 \\times 19}{19} - \\frac{280}{19}\\right| = \\left|\\frac{646 - 280}{19}\\right| = \\frac{366}{19} $$\n\nThe sensitivity index for the trimmed mean, $\\Delta_{T_{0.2}}$, is:\n$$ \\Delta_{T_{0.2}} = |T_{0.2}(X) - T_{0.2}(X')| = \\left|\\frac{31}{2} - 15\\right| = \\left|\\frac{31}{2} - \\frac{30}{2}\\right| = \\frac{1}{2} $$\n\nFinally, we compute the ratio $S$:\n$$ S = \\frac{\\Delta_{\\bar{X}}}{\\Delta_{T_{0.2}}} = \\frac{\\frac{366}{19}}{\\frac{1}{2}} = \\frac{366}{19} \\times 2 = \\frac{732}{19} $$\nThe number $19$ is a prime number, and $732$ is not a multiple of $19$ ($732 = 38 \\times 19 + 10$). Thus, the fraction is irreducible.", "answer": "$$\n\\boxed{\\frac{732}{19}}\n$$", "id": "4555567"}, {"introduction": "In transcriptomics, raw count data is often log-transformed to stabilize variance and linearize relationships, but this common practice is not without consequences. This practice moves from applying transformations to rigorously analyzing them, asking you to derive the mathematical bias introduced by the popular \"log-plus-pseudocount\" method for RNA-seq data. By applying a second-order Taylor expansion, you will uncover the systematic error inherent in this transformation and appreciate the motivation for more advanced methods like variance-stabilizing transformations or Generalized Linear Models [@problem_id:4555561].", "problem": "A transcriptomics laboratory performs Ribonucleic Acid sequencing (RNA-seq) and models gene-level read counts by a Negative Binomial (NB) distribution with mean $\\mu > 0$ and dispersion parameter $\\phi \\ge 0$, so that the variance of the count $X$ satisfies $\\mathrm{Var}(X) = \\mu + \\phi \\mu^{2}$. To avoid undefined values for zero counts, the laboratory adds a positive pseudocount $\\alpha>0$ before taking the natural logarithm, computing the transformed value $Y=\\ln(X+\\alpha)$. In downstream analyses, $Y$ is interpreted as an estimator of the latent log-abundance $\\ln(\\mu)$.\n\nAssume $X$ follows the specified NB mean–variance relationship and that $\\mu$ is small enough that low-count behavior is relevant, yet large enough to justify a smooth-function expectation approximation. Derive, up to second order in a smooth-function expansion about $\\mu$, the approximate bias $B(\\mu,\\alpha,\\phi)$ defined as\n$$\nB(\\mu,\\alpha,\\phi) = \\mathbb{E}\\big[\\ln(X+\\alpha)\\big] - \\ln(\\mu),\n$$\nexpressed in closed form in terms of $\\mu$, $\\alpha$, and $\\phi$, using the NB mean–variance relationship. Your final answer must be a single closed-form analytic expression. No units are required. In addition, briefly discuss, in words, principled alternatives to mitigate this bias in RNA-seq analyses that respect the given mean–variance structure, but your final answer must be the expression for $B(\\mu,\\alpha,\\phi)$ only.", "solution": "The user wants to derive the approximate bias of a log-transformed random variable and discuss alternatives.\n\n### Step 1: Extract Givens\n- The random variable $X$ represents gene-level read counts.\n- $X$ is modeled by a Negative Binomial (NB) distribution.\n- The mean of $X$ is $\\mathbb{E}[X] = \\mu > 0$.\n- The dispersion parameter is $\\phi \\ge 0$.\n- The variance of $X$ is $\\mathrm{Var}(X) = \\mu + \\phi \\mu^{2}$.\n- A positive pseudocount $\\alpha > 0$ is used.\n- The transformed variable is $Y = \\ln(X+\\alpha)$.\n- The target quantity is the latent log-abundance $\\ln(\\mu)$.\n- The task is to derive the approximate bias $B(\\mu,\\alpha,\\phi) = \\mathbb{E}\\big[\\ln(X+\\alpha)\\big] - \\ln(\\mu)$.\n- The approximation should be up to second order in a smooth-function expansion about $\\mu$.\n- The derivation is justified by assuming $\\mu$ is large enough for a smooth-function approximation but small enough to make low-count behavior relevant.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific or Factual Unsoundness**: None. The problem is scientifically sound. The Negative Binomial distribution with the specified quadratic mean-variance relationship is the canonical model for RNA-seq count data. The use of a log-plus-pseudocount transformation is a common, albeit critiqued, practice in bioinformatics. The issue of bias introduced by such non-linear transformations is a well-established statistical problem.\n- **Non-Formalizable or Irrelevant**: None. The problem is a formal mathematical statistics exercise that is directly relevant to the specified topic of *descriptive statistics and sampling concepts* within the field of *bioinformatics and medical data analytics*.\n- **Incomplete or Contradictory Setup**: None. The problem provides all necessary information: the function $g(X) = \\ln(X+\\alpha)$, the first two central moments of $X$ (mean $\\mu$ and variance $\\mu + \\phi\\mu^2$), and the order of the approximation. The condition that $\\mu$ is both \"small enough\" and \"large enough\" is a standard heuristic framing in applied statistics, indicating that the problem is practically relevant (due to low counts) while the chosen mathematical tool (Taylor expansion) is applicable.\n- **Unrealistic or Infeasible**: None. The scenario is a realistic representation of data processing challenges in transcriptomics.\n- **Ill-Posed or Poorly Structured**: None. The problem is well-posed and asks for a standard statistical derivation (application of the Delta method for expectations). A unique analytical expression for the approximate bias exists.\n- **Pseudo-Profound, Trivial, or Tautological**: None. The derivation requires a non-trivial application of Taylor series to approximate the expectation of a transformed random variable, which is a core concept in statistical theory.\n- **Outside Scientific Verifiability**: None. The derivation is a mathematical procedure that is fully verifiable.\n\n### Step 3: Verdict and Action\nThe problem is valid. The derivation and solution will now be provided.\n\nThe problem asks for the second-order approximation of the bias $B(\\mu,\\alpha,\\phi) = \\mathbb{E}[\\ln(X+\\alpha)] - \\ln(\\mu)$. The core of this task lies in approximating the expectation of a non-linear function of a random variable, $\\mathbb{E}[g(X)]$, where $g(X) = \\ln(X+\\alpha)$. We use a second-order Taylor series expansion of $g(X)$ around the mean of $X$, which is $\\mu = \\mathbb{E}[X]$.\n\nThe Taylor expansion of a smooth function $g(X)$ around $\\mu$ is:\n$$\ng(X) = g(\\mu) + g'(\\mu)(X-\\mu) + \\frac{1}{2!} g''(\\mu)(X-\\mu)^2 + \\frac{1}{3!} g'''(\\mu)(X-\\mu)^3 + \\dots\n$$\nTo find the expectation $\\mathbb{E}[g(X)]$, we take the expectation of the series term by term:\n$$\n\\mathbb{E}[g(X)] = \\mathbb{E}\\left[g(\\mu) + g'(\\mu)(X-\\mu) + \\frac{1}{2} g''(\\mu)(X-\\mu)^2 + \\dots\\right]\n$$\nUsing the linearity of expectation, we get:\n$$\n\\mathbb{E}[g(X)] = g(\\mu) + g'(\\mu)\\mathbb{E}[X-\\mu] + \\frac{1}{2} g''(\\mu)\\mathbb{E}[(X-\\mu)^2] + \\dots\n$$\nBy definition, $\\mathbb{E}[X-\\mu] = \\mathbb{E}[X] - \\mu = \\mu - \\mu = 0$, and $\\mathbb{E}[(X-\\mu)^2] = \\mathrm{Var}(X)$. Truncating the expansion at the second-order term gives the approximation:\n$$\n\\mathbb{E}[g(X)] \\approx g(\\mu) + \\frac{1}{2} g''(\\mu) \\mathrm{Var}(X)\n$$\nNow, we apply this general formula to our specific problem. The function is $g(x) = \\ln(x+\\alpha)$. We need its first and second derivatives:\nFirst derivative:\n$$\ng'(x) = \\frac{d}{dx}\\ln(x+\\alpha) = \\frac{1}{x+\\alpha}\n$$\nSecond derivative:\n$$\ng''(x) = \\frac{d}{dx}\\left(\\frac{1}{x+\\alpha}\\right) = -\\frac{1}{(x+\\alpha)^2}\n$$\nNext, we evaluate the function and its second derivative at the point of expansion, $x=\\mu$:\n$$\ng(\\mu) = \\ln(\\mu+\\alpha)\n$$\n$$\ng''(\\mu) = -\\frac{1}{(\\mu+\\alpha)^2}\n$$\nThe problem specifies the variance of $X$ as:\n$$\n\\mathrm{Var}(X) = \\mu + \\phi \\mu^2\n$$\nSubstituting these components into the approximation for the expectation:\n$$\n\\mathbb{E}[\\ln(X+\\alpha)] \\approx g(\\mu) + \\frac{1}{2} g''(\\mu) \\mathrm{Var}(X)\n$$\n$$\n\\mathbb{E}[\\ln(X+\\alpha)] \\approx \\ln(\\mu+\\alpha) + \\frac{1}{2} \\left(-\\frac{1}{(\\mu+\\alpha)^2}\\right) (\\mu + \\phi \\mu^2)\n$$\n$$\n\\mathbb{E}[\\ln(X+\\alpha)] \\approx \\ln(\\mu+\\alpha) - \\frac{\\mu + \\phi \\mu^2}{2(\\mu+\\alpha)^2}\n$$\nThe bias $B(\\mu, \\alpha, \\phi)$ is defined as $\\mathbb{E}[\\ln(X+\\alpha)] - \\ln(\\mu)$. Using our approximation for the expectation term, we find the approximate bias:\n$$\nB(\\mu, \\alpha, \\phi) \\approx \\left(\\ln(\\mu+\\alpha) - \\frac{\\mu + \\phi \\mu^2}{2(\\mu+\\alpha)^2}\\right) - \\ln(\\mu)\n$$\nThis expression can be rearranged by combining the logarithmic terms:\n$$\nB(\\mu, \\alpha, \\phi) \\approx \\ln(\\mu+\\alpha) - \\ln(\\mu) - \\frac{\\mu(1 + \\phi \\mu)}{2(\\mu+\\alpha)^2}\n$$\n$$\nB(\\mu, \\alpha, \\phi) \\approx \\ln\\left(\\frac{\\mu+\\alpha}{\\mu}\\right) - \\frac{\\mu(1 + \\phi \\mu)}{2(\\mu+\\alpha)^2}\n$$\nFinally, simplifying the argument of the logarithm gives the closed-form expression for the approximate bias:\n$$\nB(\\mu, \\alpha, \\phi) \\approx \\ln\\left(1 + \\frac{\\alpha}{\\mu}\\right) - \\frac{\\mu(1 + \\phi \\mu)}{2(\\mu+\\alpha)^2}\n$$\nThis expression reveals two sources of bias. The first term, $\\ln(1 + \\alpha/\\mu)$, arises from adding the pseudocount $\\alpha$ and is always positive. The second term, $-\\frac{\\mu(1 + \\phi \\mu)}{2(\\mu+\\alpha)^2}$, arises from the concavity of the logarithm function (Jensen's inequality) and is always negative. The total bias depends on the interplay between these two components.\n\nRegarding principled alternatives to mitigate this bias, two major classes of methods exist that respect the given mean-variance structure:\n$1$. **Variance-Stabilizing and Regularized Transformations**: Instead of the ad-hoc $\\ln(X+\\alpha)$ transformation, one can use transformations derived specifically to stabilize variance for NB-distributed data. For a variance function $\\mathrm{Var}(X) = V(\\mu)$, the variance-stabilizing transformation is $T(\\mu) = \\int \\frac{1}{\\sqrt{V(\\mu)}} d\\mu$. For the NB variance $\\mu + \\phi\\mu^2$, this leads to transformations related to the inverse hyperbolic sine function. Methods like the regularized logarithm (`rlog`) implemented in the `DESeq2` R package provide a more sophisticated approach by fitting a model for the dispersion-mean trend across all genes and shrinking the log-fold changes for low-count genes, effectively reducing bias and stabilizing variance.\n$2$. **Generalized Linear Models (GLMs)**: The most direct and statistically rigorous approach is to avoid transformation altogether. Instead, one can use a GLM framework which models the untransformed counts directly. By specifying a Negative Binomial error distribution and a log link function, the model directly estimates $\\ln(\\mu)$ as a linear function of covariates. This approach, used by leading RNA-seq analysis tools like `edgeR` and `DESeq2`, properly accounts for the mean-variance relationship of the count data without introducing the transformation bias inherent in the $\\ln(X+\\alpha)$ method.", "answer": "$$\n\\boxed{\\ln\\left(1 + \\frac{\\alpha}{\\mu}\\right) - \\frac{\\mu(1 + \\phi \\mu)}{2(\\mu+\\alpha)^{2}}}\n$$", "id": "4555561"}, {"introduction": "While summary statistics provide a glimpse into our data, estimating the full probability density function offers a much richer understanding. This computational practice introduces kernel density estimation (KDE), a powerful non-parametric technique for visualizing the underlying distribution of data like serum cholesterol levels without restrictive assumptions. You will implement KDE with different kernels and use leave-one-out cross-validation to select the optimal smoothing bandwidth, a fundamental skill in machine learning and modern statistics [@problem_id:4555587].", "problem": "You are given independent and identically distributed observations representing serum cholesterol concentrations in milligrams per deciliter (mg/dL). Treat the data as realizations from an unknown, absolutely continuous probability distribution on the real line. The goal is to estimate the underlying probability density using kernel density estimation and to select the bandwidth for each of two kernels by maximizing the leave-one-out log-likelihood. You will then compare which kernel attains the larger maximized leave-one-out log-likelihood.\n\nFundamental base and assumptions: Start from the definition that the empirical distribution function places equal mass on each observation and that a kernel density estimator arises from smoothing the empirical distribution with a symmetric kernel function that integrates to one. A kernel function is a nonnegative, integrable function that satisfies the symmetry property and unit integral. The two kernels to be used are the Gaussian kernel and the Epanechnikov kernel. The Gaussian kernel is defined by the standard normal density and the Epanechnikov kernel is defined by a compactly supported quadratic form. You must use the natural logarithm for all log-likelihoods. Cross-validated bandwidth selection refers to choosing the bandwidth that maximizes the leave-one-out log-likelihood, where for each observation the contribution to the likelihood excludes the observation itself from the kernel estimate.\n\nImplementation requirements:\n- For the Gaussian kernel, use the kernel defined by $K_{\\mathrm{G}}(u) = (2\\pi)^{-1/2} \\exp(-u^{2}/2)$.\n- For the Epanechnikov kernel, use the kernel defined by $K_{\\mathrm{E}}(u) = \\tfrac{3}{4}(1-u^{2})$ for $|u|\\leq 1$ and $K_{\\mathrm{E}}(u)=0$ otherwise.\n- For a given bandwidth $h$ and sample of size $n$, the kernel density estimator at a point $x$ is the average of the kernel evaluations at scaled pairwise differences between $x$ and the sample points, scaled by $h$, consistent with the requirements that the estimator integrates to one and has the correct physical units. For leave-one-out, each observation’s contribution to the log-likelihood uses the estimator that excludes that observation from the average.\n- To ensure numerical stability in the leave-one-out log-likelihood when the estimator evaluates to zero at an observed point, you must apply a lower bound: replace any zero or negative density value by $\\max\\{ \\varepsilon / h, 0 \\}$, where $\\varepsilon = 10^{-12}$. This preserves correct physical units for the density contribution and avoids undefined logarithms. The constant $\\varepsilon$ is dimensionless.\n- If multiple bandwidths achieve the same maximal leave-one-out log-likelihood (up to machine precision), choose the smallest bandwidth among the maximizers.\n- Bandwidths must be reported in mg/dL. All log-likelihood values are unitless.\n\nTest suite and parameters: Implement your program to run the following three test cases. In all randomized generation, use the specified seed and the NumPy pseudorandom number generator with the default algorithm.\n- Test case A (mixture-like, moderate size):\n  - Random seed $13$.\n  - Generate $n_{1} = 90$ observations from a normal distribution with mean $\\mu_{1} = 190$ and standard deviation $\\sigma_{1} = 22$ (all in mg/dL).\n  - Generate $n_{2} = 60$ observations from a normal distribution with mean $\\mu_{2} = 230$ and standard deviation $\\sigma_{2} = 18$ (all in mg/dL).\n  - Concatenate the two samples, then clip all values to the interval $[120, 300]$ mg/dL, and round each value to $1$ decimal place.\n- Test case B (small sample with outliers, deterministic):\n  - Use the explicit list of values in mg/dL: $[146.0, 152.0, 158.0, 162.0, 166.0, 170.0, 210.0, 260.0, 300.0, 310.0]$.\n- Test case C (heavy tails with ties):\n  - Random seed $7$.\n  - Draw $n = 50$ observations from a Student’s $t$ distribution with $\\nu = 5$ degrees of freedom, then transform via $x \\mapsto 200 + 30x$ to set location and scale (mg/dL).\n  - Clip all values to the interval $[110, 340]$ mg/dL.\n  - Append $10$ exact copies of $200.0$ mg/dL and $10$ exact copies of $240.0$ mg/dL.\n- For all test cases, use the candidate bandwidth grid (all in mg/dL): $\\{5.0, 7.5, 10.0, 12.5, 15.0, 20.0, 25.0, 35.0\\}$.\n\nTasks to perform for each test case:\n1. For each kernel (Gaussian and Epanechnikov) and each bandwidth $h$ in the grid, compute the leave-one-out log-likelihood as the sum of the natural logarithms of the leave-one-out kernel density estimator evaluated at each observed data point, applying the stability floor with $\\varepsilon = 10^{-12}$ as specified.\n2. For each kernel, find the bandwidth $h$ in the grid that maximizes the leave-one-out log-likelihood (breaking ties by choosing the smallest $h$).\n3. Compare the maximized leave-one-out log-likelihoods across the two kernels and determine which kernel is larger.\n4. Report, for each test case, the following five values in order:\n   - A kernel identifier $k$, where $k=0$ denotes the Gaussian kernel and $k=1$ denotes the Epanechnikov kernel, chosen as the one achieving the larger maximized leave-one-out log-likelihood (break ties in favor of the Gaussian kernel).\n   - The selected bandwidth for the Gaussian kernel, $h_{\\mathrm{G}}^{\\star}$, rounded to $2$ decimal places, in mg/dL.\n   - The selected bandwidth for the Epanechnikov kernel, $h_{\\mathrm{E}}^{\\star}$, rounded to $2$ decimal places, in mg/dL.\n   - The maximized leave-one-out log-likelihood for the Gaussian kernel, $L_{\\mathrm{G}}^{\\star}$, rounded to $6$ decimal places.\n   - The maximized leave-one-out log-likelihood for the Epanechnikov kernel, $L_{\\mathrm{E}}^{\\star}$, rounded to $6$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets, with each test case result itself being a list in the order described above. Do not include any whitespace characters. For example, the output should look like $[[k_{A},h_{\\mathrm{G},A}^{\\star},h_{\\mathrm{E},A}^{\\star},L_{\\mathrm{G},A}^{\\star},L_{\\mathrm{E},A}^{\\star}],[k_{B},h_{\\mathrm{G},B}^{\\star},h_{\\mathrm{E},B}^{\\star},L_{\\mathrm{G},B}^{\\star},L_{\\mathrm{E},B}^{\\star}],[k_{C},h_{\\mathrm{G},C}^{\\star},h_{\\mathrm{E},C}^{\\star},L_{\\mathrm{G},C}^{\\star},L_{\\mathrm{E},C}^{\\star}]]$.", "solution": "The problem requires the implementation of kernel density estimation (KDE) with bandwidth selection via leave-one-out cross-validation. For each of three datasets, we must determine the optimal bandwidth for both a Gaussian and an Epanechnikov kernel from a predefined grid, and then compare the performance of the two kernels.\n\n**1. Theoretical Formulation**\n\nA kernel density estimator reconstructs an unknown probability density function $f(x)$ from a sample of data points $\\{x_1, x_2, \\ldots, x_n\\}$. The estimator $\\hat{f}_h(x)$ is defined as:\n$$\n\\hat{f}_h(x) = \\frac{1}{nh} \\sum_{i=1}^{n} K\\left(\\frac{x - x_i}{h}\\right)\n$$\nwhere $K(u)$ is a kernel function (a non-negative, symmetric function that integrates to one) and $h > 0$ is the bandwidth, a smoothing parameter. The factor of $1/h$ ensures the resulting density integrates to one.\n\nThe two kernels specified are:\n- **Gaussian Kernel**: $K_{\\mathrm{G}}(u) = \\frac{1}{\\sqrt{2\\pi}} e^{-u^2/2}$, which is the standard normal probability density function.\n- **Epanechnikov Kernel**: $K_{\\mathrm{E}}(u) = \\frac{3}{4}(1-u^2)$ for $|u| \\leq 1$, and $0$ otherwise.\n\n**2. Bandwidth Selection via Leave-One-Out Cross-Validation (LOOCV)**\n\nThe choice of bandwidth $h$ is critical. LOOCV provides a data-driven method for selecting $h$. The objective is to maximize the leave-one-out log-likelihood, defined as:\n$$\nL(h) = \\sum_{i=1}^{n} \\ln \\left( \\hat{f}_{h, -i}(x_i) \\right)\n$$\nHere, $\\hat{f}_{h, -i}(x_i)$ is the KDE evaluated at data point $x_i$, but constructed using all other data points *except* $x_i$:\n$$\n\\hat{f}_{h, -i}(x_i) = \\frac{1}{(n-1)h} \\sum_{j=1, j \\neq i}^{n} K\\left(\\frac{x_i - x_j}{h}\\right)\n$$\nThis procedure assesses how well the model predicts \"unseen\" data, one point at a time.\n\n**3. Computational Strategy**\n\nA naive implementation of the LOOCV log-likelihood would involve a nested loop, resulting in a time complexity of $O(n^2)$ for each bandwidth $h$. A more efficient vectorized approach is employed. For a given dataset $X$ of size $n$, the core of the computation is the vector of LOO density estimates.\n\n1.  Construct an $n \\times n$ matrix of pairwise differences, $D$, where $D_{ij} = x_i - x_j$.\n2.  Scale this matrix by the bandwidth $h$ to get the kernel arguments, $U_{ij} = D_{ij}/h$.\n3.  Apply the chosen kernel function $K$ element-wise to the matrix $U$, yielding a matrix $K_{ij} = K(U_{ij})$.\n4.  The LOO sum for the $i$-th point, $\\sum_{j \\neq i} K(\\frac{x_i - x_j}{h})$, is the sum of the $i$-th row of matrix $K$ minus its diagonal element, $K_{ii} = K(0)$. This is because the sum over the full row includes the term $j=i$.\n5.  This is performed for all $i$ simultaneously by computing the row sums of the matrix $K$ and subtracting a vector of its diagonal elements.\n6.  The resulting vector of sums is then scaled by $1/((n-1)h)$ to get the vector of LOO density estimates $[\\hat{f}_{h, -1}(x_1), \\ldots, \\hat{f}_{h, -n}(x_n)]$.\n\n**4. Numerical Stability**\n\nFor certain data and bandwidth combinations, particularly with the compactly supported Epanechnikov kernel, the density estimate $\\hat{f}_{h, -i}(x_i)$ can be zero (e.g., if $x_i$ is far from all other points). To prevent an undefined natural logarithm, a numerical floor is applied. Any non-positive density estimate is replaced by $\\varepsilon/h$, where $\\varepsilon = 10^{-12}$. This small positive value has the same physical units as a probability density ($1/(\\text{mg/dL})$), preserving dimensional consistency.\n\n**5. Implementation Steps**\n\nFor each test case:\n1.  The specified dataset is generated or defined.\n2.  The set of candidate bandwidths is $\\{5.0, 7.5, 10.0, 12.5, 15.0, 20.0, 25.0, 35.0\\}$.\n3.  For each kernel (Gaussian and Epanechnikov):\n    a. The LOO log-likelihood $L(h)$ is calculated for each $h$ in the grid using the vectorized method.\n    b. The bandwidth $h^{\\star}$ that maximizes $L(h)$ is identified. If multiple bandwidths yield the same maximum likelihood, the smallest is chosen. The corresponding maximum likelihood is $L^{\\star}$.\n4.  The maximized log-likelihoods, $L_{\\mathrm{G}}^{\\star}$ and $L_{\\mathrm{E}}^{\\star}$, are compared. The kernel with the larger value is deemed superior. Tie-breaking rules favor the Gaussian kernel.\n5.  The final results for each test case—the superior kernel identifier $k$, the optimal bandwidths $h_{\\mathrm{G}}^{\\star}$ and $h_{\\mathrm{E}}^{\\star}$, and the maximized log-likelihoods $L_{\\mathrm{G}}^{\\star}$ and $L_{\\mathrm{E}}^{\\star}$—are collected and formatted as specified.\n\nThis structured procedure guarantees a reproducible and computationally efficient solution to the posed problem.", "answer": "[[1,7.50,7.50,-731.815257,-731.782806],[0,20.00,20.00,-53.649692,-53.693452],[0,7.50,7.50,-353.111874,-354.218524]]", "id": "4555587"}]}