{"hands_on_practices": [{"introduction": "This first exercise is a fundamental workout in Bayesian mechanics. You will derive the posterior distribution for a Poisson rate parameter using its conjugate Gamma prior, a common model for count data like sequencing reads [@problem_id:4541565]. This practice reinforces the core application of Bayes' theorem and demonstrates the mathematical elegance and convenience of using conjugate prior-likelihood pairs.", "problem": "In modeling read-depth counts across genomic bins for a single gene across multiple sequencing runs in Next-Generation Sequencing (NGS), suppose the count in sequencing run $i$ is modeled as a Poisson random variable $Y_{i}$ with rate parameter $ \\lambda t_{i} $, where $ \\lambda $ is an unknown baseline rate for the gene and $ t_{i}  0 $ is a known exposure (for example, a library-size normalization factor) for run $i$. Assume the $Y_{i}$ are conditionally independent given $ \\lambda $. The prior for $ \\lambda $ is a Gamma distribution with shape parameter $ \\alpha $ and rate parameter $ \\beta $, whose density is\n$$\np(\\lambda \\mid \\alpha,\\beta) \\;=\\; \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\, \\lambda^{\\alpha - 1} \\, \\exp(-\\beta \\lambda), \\quad \\lambda  0,\n$$\nand the Poisson likelihood for a single observation is\n$$\np(Y_{i} = y_{i} \\mid \\lambda, t_{i}) \\;=\\; \\frac{\\exp(-\\lambda t_{i}) \\, (\\lambda t_{i})^{y_{i}}}{y_{i}!}, \\quad y_{i} \\in \\{0,1,2,\\dots\\}.\n$$\nStarting from these definitions and assuming independence across runs, derive the posterior distribution $ p(\\lambda \\mid \\{y_{i}\\}, \\{t_{i}\\}, \\alpha, \\beta) $ for $ \\lambda $. Then, using the following dataset and prior, compute the posterior parameters:\n- Prior: $ \\alpha = 3.5 $, $ \\beta = 2.0 $.\n- Observed counts: $ y_{1} = 30 $, $ y_{2} = 22 $, $ y_{3} = 28 $, $ y_{4} = 25 $.\n- Exposures: $ t_{1} = 0.80 $, $ t_{2} = 1.20 $, $ t_{3} = 1.00 $, $ t_{4} = 1.00 $.\n\nExpress your final posterior parameters as a row matrix $ \\left( \\alpha_{\\text{post}} \\; \\beta_{\\text{post}} \\right) $, rounded to four significant figures.", "solution": "The problem is well-defined and scientifically sound, representing a standard application of Bayesian inference using conjugate priors. I shall proceed with the derivation.\n\nThe objective is to determine the posterior distribution of the rate parameter $\\lambda$, denoted as $p(\\lambda \\mid \\{y_{i}\\}, \\{t_{i}\\}, \\alpha, \\beta)$. According to Bayes' theorem, the posterior probability distribution is proportional to the product of the likelihood function and the prior probability distribution:\n$$\np(\\lambda \\mid \\{y_i\\}_{i=1}^n, \\{t_i\\}_{i=1}^n, \\alpha, \\beta) \\propto p(\\{y_i\\}_{i=1}^n \\mid \\lambda, \\{t_i\\}_{i=1}^n) \\cdot p(\\lambda \\mid \\alpha, \\beta)\n$$\nHere, $n$ is the number of sequencing runs, which is $4$ in this case.\n\nFirst, we define the likelihood function, $p(\\{y_i\\} \\mid \\lambda, \\{t_i\\})$. The problem states that the observations $Y_i$ are conditionally independent given $\\lambda$. Therefore, the joint likelihood is the product of the individual likelihoods for each observation $y_i$. Each $y_i$ is drawn from a Poisson distribution with rate $\\lambda t_i$.\n$$\np(\\{y_i\\} \\mid \\lambda, \\{t_i\\}) = \\prod_{i=1}^n p(y_i \\mid \\lambda, t_i) = \\prod_{i=1}^n \\frac{\\exp(-\\lambda t_i) (\\lambda t_i)^{y_i}}{y_i!}\n$$\nSince we are interested in the posterior distribution of $\\lambda$, we can disregard any terms that do not depend on $\\lambda$. These terms will be absorbed into the normalization constant.\n$$\np(\\{y_i\\} \\mid \\lambda, \\{t_i\\}) \\propto \\prod_{i=1}^n \\exp(-\\lambda t_i) (\\lambda t_i)^{y_i}\n$$\nWe can rewrite this by separating the $\\lambda$ terms:\n$$\n\\prod_{i=1}^n \\exp(-\\lambda t_i) \\lambda^{y_i} t_i^{y_i} \\propto \\left( \\prod_{i=1}^n \\exp(-\\lambda t_i) \\right) \\left( \\prod_{i=1}^n \\lambda^{y_i} \\right)\n$$\nCombining the exponents and powers:\n$$\n\\exp\\left(-\\sum_{i=1}^n \\lambda t_i\\right) \\lambda^{\\sum_{i=1}^n y_i} = \\exp\\left(-\\lambda \\sum_{i=1}^n t_i\\right) \\lambda^{\\sum_{i=1}^n y_i}\n$$\nSo, the likelihood function is proportional to:\n$$\np(\\{y_i\\} \\mid \\lambda, \\{t_i\\}) \\propto \\lambda^{\\sum_{i=1}^n y_i} \\exp\\left(-\\lambda \\sum_{i=1}^n t_i\\right)\n$$\nNext, we consider the prior distribution for $\\lambda$, which is given as a Gamma distribution with shape parameter $\\alpha$ and rate parameter $\\beta$:\n$$\np(\\lambda \\mid \\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha - 1} \\exp(-\\beta \\lambda)\n$$\nThe part of the prior that depends on $\\lambda$ (the kernel of the distribution) is:\n$$\np(\\lambda \\mid \\alpha, \\beta) \\propto \\lambda^{\\alpha - 1} \\exp(-\\beta \\lambda)\n$$\nNow, we multiply the likelihood and the prior to obtain the posterior distribution's kernel:\n$$\np(\\lambda \\mid \\{y_i\\}, \\{t_i\\}, \\alpha, \\beta) \\propto \\left( \\lambda^{\\sum_{i=1}^n y_i} \\exp\\left(-\\lambda \\sum_{i=1}^n t_i\\right) \\right) \\cdot \\left( \\lambda^{\\alpha - 1} \\exp(-\\beta \\lambda) \\right)\n$$\nWe combine the terms with base $\\lambda$ and the terms with base $\\exp(1)$:\n$$\np(\\lambda \\mid \\{y_i\\}, \\{t_i\\}, \\alpha, \\beta) \\propto \\lambda^{\\left(\\sum_{i=1}^n y_i\\right) + (\\alpha - 1)} \\exp\\left(-\\lambda \\sum_{i=1}^n t_i - \\beta \\lambda\\right)\n$$\n$$\np(\\lambda \\mid \\{y_i\\}, \\{t_i\\}, \\alpha, \\beta) \\propto \\lambda^{\\left(\\alpha + \\sum_{i=1}^n y_i\\right) - 1} \\exp\\left(-\\left(\\beta + \\sum_{i=1}^n t_i\\right)\\lambda\\right)\n$$\nThis resulting expression is the kernel of a Gamma distribution. A general Gamma distribution, $\\text{Gamma}(\\alpha', \\beta')$, has a probability density function proportional to $\\lambda^{\\alpha' - 1} \\exp(-\\beta' \\lambda)$. By comparing this general form to our derived posterior kernel, we can identify the parameters of the posterior distribution:\nThe posterior shape parameter is $\\alpha_{\\text{post}} = \\alpha + \\sum_{i=1}^n y_i$.\nThe posterior rate parameter is $\\beta_{\\text{post}} = \\beta + \\sum_{i=1}^n t_i$.\nThis confirms that the Gamma distribution is a conjugate prior for the rate parameter of the Poisson distribution.\n\nThe problem provides the following numerical values:\n- Prior parameters: $\\alpha = 3.5$, $\\beta = 2.0$.\n- Observed counts: $\\{y_i\\} = \\{30, 22, 28, 25\\}$.\n- Exposures: $\\{t_i\\} = \\{0.80, 1.20, 1.00, 1.00\\}$.\n\nWe first compute the required sums from the data:\nSum of counts:\n$$\n\\sum_{i=1}^4 y_i = 30 + 22 + 28 + 25 = 105\n$$\nSum of exposures:\n$$\n\\sum_{i=1}^4 t_i = 0.80 + 1.20 + 1.00 + 1.00 = 4.00\n$$\nNow, we can calculate the posterior parameters:\n$$\n\\alpha_{\\text{post}} = \\alpha + \\sum_{i=1}^4 y_i = 3.5 + 105 = 108.5\n$$\n$$\n\\beta_{\\text{post}} = \\beta + \\sum_{i=1}^4 t_i = 2.0 + 4.00 = 6.00\n$$\nThe problem requires the final parameters to be rounded to four significant figures.\nThe value for $\\alpha_{\\text{post}}$ is $108.5$, which already has four significant figures.\nThe value for $\\beta_{\\text{post}}$ is $6.00$. To express this with four significant figures, we write it as $6.000$.\n\nTherefore, the posterior distribution is $\\lambda \\mid \\text{data} \\sim \\text{Gamma}(\\alpha_{\\text{post}}=108.5, \\beta_{\\text{post}}=6.000)$. The required output is the row matrix of these parameters.", "answer": "$$\n\\boxed{\\begin{pmatrix} 108.5  6.000 \\end{pmatrix}}\n$$", "id": "4541565"}, {"introduction": "Inference is often a means to an end: making a decision. This practice moves from parameter estimation to practical decision-making by using a posterior distribution to evaluate a clinical endpoint [@problem_id:4541548]. You will implement a decision rule based on a posterior probability and, critically, perform a sensitivity analysis to assess how robust your conclusion is to the choice of prior.", "problem": "You are tasked with building a complete, runnable program that performs a prior sensitivity analysis for a binary clinical endpoint in a bioinformatics and medical data analytics setting. Consider a study that observes $x$ responders out of $n$ independent patients, modeled as a sequence of independent and identically distributed Bernoulli trials with an unknown response probability $\\theta$. Use Bayes’ theorem with a Binomial likelihood and a Beta prior to derive and compute the posterior decision probability that $\\theta$ exceeds a clinically meaningful threshold. Then, perform a sensitivity analysis by varying the prior hyperparameters over a specified grid, and summarize the robustness of the posterior decision.\n\nStart from the following fundamental base:\n- The Binomial likelihood for observing $x$ successes given $n$ and success probability $\\theta$ is $p(x \\mid \\theta, n) \\propto \\theta^{x}(1 - \\theta)^{n-x}$.\n- A Beta prior with hyperparameters $a$ and $b$ has density $p(\\theta \\mid a, b) \\propto \\theta^{a-1}(1 - \\theta)^{b-1}$ on $\\theta \\in (0, 1)$.\n- Bayes’ theorem yields a posterior density proportional to the product of the likelihood and the prior.\n\nDefine a decision rule as follows: declare a “GO” decision if the posterior probability that $\\theta$ exceeds a prespecified threshold $r^{\\star}$ is at least $p^{\\star}$, where both $r^{\\star}$ and $p^{\\star}$ are probabilities represented as decimals in $[0,1]$.\n\nYour program must:\n1. Derive, from the fundamental base above (without using any shortcut formulas in the problem statement), the expression for the posterior distribution of $\\theta$ under a Beta prior and Binomial likelihood, and the expression for the posterior probability $\\Pr(\\theta  r^{\\star} \\mid x, n, a, b)$ needed for the decision rule.\n2. For each test case below, evaluate the decision rule across a Cartesian product grid of prior hyperparameters and compute the robustness summary defined as the fraction of the hyperparameter pairs in the grid for which the decision is “GO”. This fraction should be reported as a decimal rounded to four digits after the decimal point.\n\nTest suite:\n- Case $1$ (happy path, moderate information): $x = 14$, $n = 40$, $r^{\\star} = 0.3$, $p^{\\star} = 0.9$, prior grid with $a \\in \\{0.5, 1, 2, 4\\}$ and $b \\in \\{0.5, 1, 2, 4\\}$.\n- Case $2$ (boundary behavior with sparse, unfavorable data): $x = 0$, $n = 5$, $r^{\\star} = 0.1$, $p^{\\star} = 0.9$, prior grid with $a \\in \\{0.1, 0.5, 1, 2\\}$ and $b \\in \\{2, 5, 10\\}$.\n- Case $3$ (edge case with all successes): $x = 10$, $n = 10$, $r^{\\star} = 0.6$, $p^{\\star} = 0.95$, prior grid with $a \\in \\{0.5, 1, 2, 5, 10\\}$ and $b \\in \\{0.5, 1, 2, 5, 10\\}$.\n\nAdditional requirements:\n- All probabilities must be expressed as decimals (for example, $0.9$) and not with a percentage sign.\n- Your program must output a single line containing a list with three floating-point numbers corresponding to Cases $1$, $2$, and $3$, respectively. Each number is the fraction of “GO” decisions across the specified prior grid, rounded to exactly four digits after the decimal point.\n- The final output format must be a single line: a comma-separated list enclosed in square brackets, for example, $[0.7500,0.0000,1.0000]$.\n\nYour program must be fully self-contained and must not require any user input or external files. It should rely only on the prescribed runtime environment.", "solution": "The problem has been validated and is determined to be sound. It is scientifically grounded in Bayesian statistical theory, well-posed with all necessary information provided, and objective in its formulation. We may now proceed with the solution.\n\nThe objective is to conduct a prior sensitivity analysis for a decision rule based on the posterior distribution of a response probability, $\\theta$. This involves deriving the posterior distribution and the corresponding decision metric, and then systematically evaluating this metric over a grid of prior hyperparameters.\n\n**Step 1: Derivation of the Posterior Distribution**\n\nWe are given a model for a binary clinical outcome where we observe $x$ responders out of $n$ patients. This is modeled as a sequence of $n$ independent and identically distributed Bernoulli trials.\n\nThe likelihood of observing $x$ successes in $n$ trials for a given success probability $\\theta$ is given by the Binomial probability mass function. For the purpose of Bayesian inference, we only need the part that depends on $\\theta$, which is the kernel of the likelihood function:\n$$p(x \\mid \\theta, n) \\propto \\theta^{x}(1 - \\theta)^{n-x}$$\n\nThe prior belief about the parameter $\\theta$ is modeled using a Beta distribution with hyperparameters $a$ and $b$. The probability density function (PDF) of the Beta distribution has the kernel:\n$$p(\\theta \\mid a, b) \\propto \\theta^{a-1}(1 - \\theta)^{b-1}$$\nThe choice of a Beta prior for a Binomial likelihood is a standard practice because the Beta distribution is the conjugate prior for the Bernoulli/Binomial likelihood. This means the posterior distribution will also be a Beta distribution, which simplifies the analysis.\n\nAccording to Bayes' theorem, the posterior distribution of $\\theta$ given the data $x$ and $n$ is proportional to the product of the likelihood and the prior:\n$$p(\\theta \\mid x, n, a, b) \\propto p(x \\mid \\theta, n) \\times p(\\theta \\mid a, b)$$\n\nSubstituting the expressions for the likelihood kernel and the prior kernel, we obtain:\n$$p(\\theta \\mid x, n, a, b) \\propto \\left( \\theta^{x}(1 - \\theta)^{n-x} \\right) \\times \\left( \\theta^{a-1}(1 - \\theta)^{b-1} \\right)$$\n\nBy combining the terms with the same base ($\\theta$ and $1-\\theta$), we get:\n$$p(\\theta \\mid x, n, a, b) \\propto \\theta^{x + a - 1} (1 - \\theta)^{n - x + b - 1}$$\n\nThis expression is the kernel of a new Beta distribution. By inspection, we can identify the updated (posterior) hyperparameters. Let the posterior hyperparameters be $a'$ and $b'$. Then:\n$$a' = a + x$$\n$$b' = b + n - x$$\n\nTherefore, the posterior distribution of $\\theta$ is a Beta distribution with these updated parameters:\n$$\\theta \\mid x, n, a, b \\sim \\text{Beta}(a+x, b+n-x)$$\n\n**Step 2: Derivation of the Posterior Decision Probability**\n\nThe decision rule is based on the posterior probability that the response rate $\\theta$ exceeds a clinically meaningful threshold $r^{\\star}$. This probability is denoted as $\\Pr(\\theta  r^{\\star} \\mid x, n, a, b)$.\n\nTo compute this probability, we must integrate the posterior PDF from $r^{\\star}$ to $1$:\n$$\\Pr(\\theta  r^{\\star} \\mid x, n, a, b) = \\int_{r^{\\star}}^{1} p(\\theta \\mid x, n, a, b) \\, d\\theta$$\n\nSince the posterior distribution is $\\text{Beta}(a', b')$, the integral is:\n$$\\Pr(\\theta  r^{\\star} \\mid x, n, a, b) = \\int_{r^{\\star}}^{1} \\frac{\\theta^{a'-1}(1-\\theta)^{b'-1}}{B(a', b')} \\, d\\theta$$\nwhere $B(a', b')$ is the Beta function, which serves as the normalization constant.\n\nThis integral is the definition of the survival function (or complementary cumulative distribution function, CCDF) of the Beta distribution, evaluated at $r^{\\star}$. Let $F_{\\text{Beta}}(y; \\alpha, \\beta)$ denote the cumulative distribution function (CDF) of a Beta distribution with parameters $\\alpha$ and $\\beta$. The required probability is:\n$$\\Pr(\\theta  r^{\\star} \\mid x, n, a, b) = 1 - F_{\\text{Beta}}(r^{\\star}; a', b')$$\nwhere $a' = a+x$ and $b' = b+n-x$. This quantity can be computed using standard scientific computing libraries.\n\n**Step 3: Algorithm for Sensitivity Analysis**\n\nFor each test case, we are given the data ($x, n$), the decision thresholds ($r^{\\star}, p^{\\star}$), and a grid of prior hyperparameters for $a$ and $b$. The sensitivity analysis proceeds as follows:\n\n1.  Initialize a counter for \"GO\" decisions, `go_count`, to $0$.\n2.  Determine the total number of hyperparameter pairs in the grid, `total_count`, by multiplying the number of choices for $a$ and $b$.\n3.  For each hyperparameter pair $(a, b)$ in the Cartesian product of the specified grids:\n    a. Calculate the posterior hyperparameters: $a' = a + x$ and $b' = b + n - x$.\n    b. Compute the posterior probability $P_{\\text{post}} = \\Pr(\\theta  r^{\\star} \\mid x, n, a, b) = 1 - F_{\\text{Beta}}(r^{\\star}; a', b')$.\n    c. Apply the decision rule: If $P_{\\text{post}} \\ge p^{\\star}$, increment `go_count`.\n4.  After iterating through all pairs $(a, b)$, calculate the robustness summary as the fraction of \"GO\" decisions:\n    $$\\text{Robustness Fraction} = \\frac{\\text{go\\_count}}{\\text{total\\_count}}$$\n5.  This fraction is then rounded to four decimal places as required.\n\nThis procedure is applied independently to each of the three test cases specified in the problem statement. The use of a computational library like SciPy is essential for the accurate calculation of the Beta Cdf or survival function.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import beta\n\ndef solve():\n    \"\"\"\n    Performs a prior sensitivity analysis for a Bayesian binary endpoint model.\n\n    For each test case, the function iterates over a grid of prior hyperparameters (a, b)\n    for a Beta prior. It calculates the posterior distribution Beta(a+x, b+n-x)\n    and then computes the posterior probability that the response rate theta exceeds a\n    threshold r_star. If this probability is at least p_star, a 'GO' decision is counted.\n    The final result for each case is the fraction of 'GO' decisions over the\n    entire prior grid, rounded to four decimal places.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"x\": 14, \"n\": 40, \"r_star\": 0.3, \"p_star\": 0.9,\n            \"a_grid\": [0.5, 1, 2, 4], \"b_grid\": [0.5, 1, 2, 4]\n        },\n        {\n            \"x\": 0, \"n\": 5, \"r_star\": 0.1, \"p_star\": 0.9,\n            \"a_grid\": [0.1, 0.5, 1, 2], \"b_grid\": [2, 5, 10]\n        },\n        {\n            \"x\": 10, \"n\": 10, \"r_star\": 0.6, \"p_star\": 0.95,\n            \"a_grid\": [0.5, 1, 2, 5, 10], \"b_grid\": [0.5, 1, 2, 5, 10]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        x = case[\"x\"]\n        n = case[\"n\"]\n        r_star = case[\"r_star\"]\n        p_star = case[\"p_star\"]\n        a_grid = case[\"a_grid\"]\n        b_grid = case[\"b_grid\"]\n\n        go_count = 0\n        total_count = len(a_grid) * len(b_grid)\n\n        for a in a_grid:\n            for b in b_grid:\n                # Calculate posterior hyperparameters\n                a_post = a + x\n                b_post = b + n - x\n\n                # Calculate the posterior probability Pr(theta  r_star)\n                # This is the survival function (1 - CDF) of the posterior Beta distribution\n                posterior_prob = beta.sf(r_star, a_post, b_post)\n\n                # Apply the decision rule\n                if posterior_prob = p_star:\n                    go_count += 1\n        \n        # Calculate the robustness summary fraction\n        if total_count  0:\n            robustness_fraction = go_count / total_count\n        else:\n            robustness_fraction = 0.0\n\n        # Append the formatted result\n        results.append(f\"{robustness_fraction:.4f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "4541548"}, {"introduction": "Many realistic bioinformatics models are too complex for a direct, analytical solution. This exercise introduces a powerful computational approach by asking you to lay the groundwork for a Gibbs sampler, a type of Markov chain Monte Carlo (MCMC) algorithm [@problem_id:4541515]. Deriving the full conditional distributions for a hierarchical model will equip you with the foundational skills needed to analyze sophisticated model structures common in modern genomics.", "problem": "A team analyzing Next-Generation Sequencing (NGS) read counts across genomic loci in a cancer cohort models allelic imbalance using a two-level hierarchical Beta-Binomial model. For loci indexed by $i \\in \\{1, \\dots, m\\}$, the observed mutant read counts are $y_i \\in \\{0, 1, \\dots, n_i\\}$ out of total reads $n_i \\in \\mathbb{N}$. The generative model is:\n- Likelihood: $y_i \\mid \\theta_i \\sim \\mathrm{Binomial}(n_i, \\theta_i)$ independently across $i$.\n- Latent locus-level parameters: $\\theta_i \\mid \\alpha, \\beta \\sim \\mathrm{Beta}(\\alpha, \\beta)$ independently across $i$.\n- Hyperpriors: $\\alpha \\sim \\mathrm{Gamma}(a_{\\alpha}, b_{\\alpha})$ and $\\beta \\sim \\mathrm{Gamma}(a_{\\beta}, b_{\\beta})$, each in shape-rate parameterization.\n\nStarting only from Bayes’ theorem, the definitions of the Binomial and Beta density functions, and the relationship between the Beta and Gamma functions $B(x,y) = \\dfrac{\\Gamma(x)\\Gamma(y)}{\\Gamma(x+y)}$, perform the following:\n\n1. Derive the full conditional distribution for each latent locus-level parameter $\\theta_i$ given the data $\\{y_i, n_i\\}$ and the hyperparameters $\\alpha, \\beta$. Your derivation must show the algebraic steps that justify the kernel and identify the standard form.\n\n2. Derive the full conditional distribution for the hyperparameter $\\alpha$ given $\\{\\theta_i\\}_{i=1}^{m}$ and $\\beta$ (and analogously for $\\beta$ given $\\{\\theta_i\\}_{i=1}^{m}$ and $\\alpha$), expressed up to a proportionality constant. Your derivation must explicitly exhibit all terms that depend on $\\alpha$ (or $\\beta$) and use the identity relating $B(\\alpha,\\beta)$ to Gamma functions to simplify products over loci as needed.\n\n3. Using your full conditionals, specify a coordinate-wise Gibbs sampling scheme that updates in parameter blocks $\\{\\theta_i\\}_{i=1}^{m}$ and $\\{\\alpha, \\beta\\}$, clearly indicating which steps admit direct sampling and which require a generic log-concave univariate sampler or slice sampling.\n\nFinally, to connect the Gibbs construction to a quantity of direct predictive relevance in bioinformatics, suppose that at a particular Gibbs iteration the current hyperparameters for a given locus $j$ are $\\alpha = 2.3$ and $\\beta = 7.1$, and that at locus $j$ the observed data are $y_j = 12$ mutant reads out of $n_j = 50$ total reads. Using only the full conditional you derived for $\\theta_j$, compute the posterior predictive probability that the next read at locus $j$ is mutant, conditional on the current hyperparameters and the observed data at locus $j$. Express your answer as a single real number and round your answer to four significant figures.", "solution": "The problem statement is critically validated before proceeding to a solution.\n\n### Step 1: Extract Givens\n- **Model:** A two-level hierarchical Beta-Binomial model.\n- **Indices:** Loci are indexed by $i \\in \\{1, \\dots, m\\}$.\n- **Data:** For each locus $i$, observed mutant read counts are $y_i \\in \\{0, 1, \\dots, n_i\\}$ out of total reads $n_i \\in \\mathbb{N}$.\n- **Likelihood:** $y_i \\mid \\theta_i \\sim \\mathrm{Binomial}(n_i, \\theta_i)$, independently for each $i$.\n- **Latent Parameters:** $\\theta_i \\mid \\alpha, \\beta \\sim \\mathrm{Beta}(\\alpha, \\beta)$, independently for each $i$.\n- **Hyperpriors:** $\\alpha \\sim \\mathrm{Gamma}(a_{\\alpha}, b_{\\alpha})$ and $\\beta \\sim \\mathrm{Gamma}(a_{\\beta}, b_{\\beta})$, with Gamma distribution in shape-rate parameterization.\n- **Identity:** $B(x,y) = \\dfrac{\\Gamma(x)\\Gamma(y)}{\\Gamma(x+y)}$.\n- **Tasks:**\n    1.  Derive the full conditional distribution for $\\theta_i$.\n    2.  Derive the full conditional distribution for $\\alpha$ and $\\beta$ up to a proportionality constant.\n    3.  Specify a coordinate-wise Gibbs sampling scheme.\n    4.  Compute the posterior predictive probability that the next read at locus $j$ is mutant given $\\alpha = 2.3$, $\\beta = 7.1$, $y_j = 12$, and $n_j = 50$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded:** The Beta-Binomial model is a standard and well-established statistical model in bioinformatics for analyzing overdispersed count data, such as read counts from Next-Generation Sequencing. The choices of Binomial, Beta, and Gamma distributions are standard in Bayesian hierarchical modeling, representing a conjugate structure where possible and using common hyperpriors. The problem is based on sound principles of Bayesian statistics and its application in genomics.\n- **Well-Posed:** The problem provides a complete generative model and asks for the derivation of full conditional distributions and a subsequent Gibbs sampling scheme, which are standard tasks in Bayesian computation. A unique and meaningful solution exists for each part of the problem.\n- **Objective:** The language is precise and quantitative. The model and tasks are defined formally, leaving no room for subjective interpretation.\n\nThe problem does not exhibit any of the invalidity flaws. It is scientifically sound, formally specified, complete, and directly relevant to the stated topic and field.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution is provided below.\n\n### 1. Full Conditional Distribution for $\\theta_i$\n\nThe full conditional distribution for a parameter is its posterior distribution given all other parameters and the data. For a specific locus $i$, the parameter is $\\theta_i$. We need to find $p(\\theta_i \\mid y_i, n_i, \\alpha, \\beta)$. Due to the conditional independence structure of the model, $\\theta_i$ is conditionally independent of all other data $\\{y_k, n_k\\}_{k \\neq i}$ and parameters $\\{\\theta_k\\}_{k \\neq i}$ given $\\alpha$ and $\\beta$.\n\nUsing Bayes' theorem, the full conditional is proportional to the product of the likelihood and the prior for $\\theta_i$:\n$$ p(\\theta_i \\mid y_i, n_i, \\alpha, \\beta) \\propto p(y_i \\mid n_i, \\theta_i) \\, p(\\theta_i \\mid \\alpha, \\beta) $$\nThe likelihood is the probability mass function of the Binomial distribution:\n$$ p(y_i \\mid n_i, \\theta_i) = \\binom{n_i}{y_i} \\theta_i^{y_i} (1-\\theta_i)^{n_i - y_i} $$\nThe prior on $\\theta_i$ is the probability density function of the Beta distribution:\n$$ p(\\theta_i \\mid \\alpha, \\beta) = \\frac{\\theta_i^{\\alpha-1} (1-\\theta_i)^{\\beta-1}}{B(\\alpha, \\beta)} $$\nwhere $B(\\alpha, \\beta)$ is the Beta function.\n\nMultiplying the likelihood and the prior, and treating terms not involving $\\theta_i$ as part of the proportionality constant:\n$$ p(\\theta_i \\mid y_i, n_i, \\alpha, \\beta) \\propto \\left[ \\theta_i^{y_i} (1-\\theta_i)^{n_i - y_i} \\right] \\times \\left[ \\theta_i^{\\alpha-1} (1-\\theta_i)^{\\beta-1} \\right] $$\nCombining the exponents of $\\theta_i$ and $(1-\\theta_i)$:\n$$ p(\\theta_i \\mid y_i, n_i, \\alpha, \\beta) \\propto \\theta_i^{y_i + \\alpha - 1} (1-\\theta_i)^{n_i - y_i + \\beta - 1} $$\nThis expression is the kernel of a Beta distribution. By inspection, we can identify the parameters of this distribution. The full conditional for $\\theta_i$ is a Beta distribution with updated parameters:\n$$ \\theta_i \\mid y_i, n_i, \\alpha, \\beta \\sim \\mathrm{Beta}(y_i + \\alpha, n_i - y_i + \\beta) $$\n\n### 2. Full Conditional Distribution for Hyperparameters $\\alpha$ and $\\beta$\n\n**Full Conditional for $\\alpha$**\nThe full conditional for $\\alpha$ is its posterior distribution given all other parameters and the data, $p(\\alpha \\mid \\{\\theta_i\\}_{i=1}^m, \\beta)$. The observed data $\\{y_i, n_i\\}$ are conditionally independent of $\\alpha$ given the $\\{\\theta_i\\}$. Using Bayes' theorem:\n$$ p(\\alpha \\mid \\{\\theta_i\\}_{i=1}^m, \\beta) \\propto p(\\{\\theta_i\\}_{i=1}^m \\mid \\alpha, \\beta) \\, p(\\alpha) $$\nThe term $p(\\{\\theta_i\\}_{i=1}^m \\mid \\alpha, \\beta)$ is the \"likelihood\" for the hyperparameters. Since the $\\theta_i$ are conditionally independent given $\\alpha$ and $\\beta$:\n$$ p(\\{\\theta_i\\}_{i=1}^m \\mid \\alpha, \\beta) = \\prod_{i=1}^m p(\\theta_i \\mid \\alpha, \\beta) = \\prod_{i=1}^m \\frac{\\theta_i^{\\alpha-1} (1-\\theta_i)^{\\beta-1}}{B(\\alpha, \\beta)} $$\nUsing the identity $B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}$:\n$$ p(\\{\\theta_i\\}_{i=1}^m \\mid \\alpha, \\beta) = \\prod_{i=1}^m \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\theta_i^{\\alpha-1} (1-\\theta_i)^{\\beta-1} = \\left[ \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\right]^m \\left( \\prod_{i=1}^m \\theta_i \\right)^{\\alpha-1} \\left( \\prod_{i=1}^m (1-\\theta_i) \\right)^{\\beta-1} $$\nThe prior for $\\alpha$ is a Gamma distribution, $p(\\alpha) \\sim \\mathrm{Gamma}(a_{\\alpha}, b_{\\alpha})$:\n$$ p(\\alpha) \\propto \\alpha^{a_{\\alpha}-1} \\exp(-b_{\\alpha}\\alpha) $$\nMultiplying the \"likelihood\" and the prior, and dropping all terms that do not depend on $\\alpha$ (which includes terms depending only on $\\beta$, the $\\{\\theta_i\\}$, or constants):\n$$ p(\\alpha \\mid \\dots) \\propto \\left[ \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)} \\right]^m \\left( \\prod_{i=1}^m \\theta_i \\right)^{\\alpha-1} \\cdot \\alpha^{a_{\\alpha}-1} \\exp(-b_{\\alpha}\\alpha) $$\nThe term $\\left(\\prod_{i=1}^m \\theta_i\\right)^{\\alpha-1}$ can be rewritten as $\\exp\\left( (\\alpha-1) \\sum_{i=1}^m \\ln(\\theta_i) \\right)$. Thus, the full conditional for $\\alpha$ is:\n$$ p(\\alpha \\mid \\{\\theta_i\\}_{i=1}^m, \\beta) \\propto \\left( \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)} \\right)^m \\exp\\left(\\alpha \\sum_{i=1}^m \\ln(\\theta_i)\\right) \\alpha^{a_{\\alpha}-1} \\exp(-b_{\\alpha}\\alpha) $$\n\n**Full Conditional for $\\beta$**\nThe derivation for $\\beta$ is analogous. The full conditional is $p(\\beta \\mid \\{\\theta_i\\}_{i=1}^m, \\alpha)$.\n$$ p(\\beta \\mid \\{\\theta_i\\}_{i=1}^m, \\alpha) \\propto p(\\{\\theta_i\\}_{i=1}^m \\mid \\alpha, \\beta) \\, p(\\beta) $$\nThe \"likelihood\" term is the same as before. The prior for $\\beta$ is $p(\\beta) \\sim \\mathrm{Gamma}(a_{\\beta}, b_{\\beta})$:\n$$ p(\\beta) \\propto \\beta^{a_{\\beta}-1} \\exp(-b_{\\beta}\\beta) $$\nMultiplying and dropping terms not involving $\\beta$:\n$$ p(\\beta \\mid \\dots) \\propto \\left[ \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\beta)} \\right]^m \\left( \\prod_{i=1}^m (1-\\theta_i) \\right)^{\\beta-1} \\cdot \\beta^{a_{\\beta}-1} \\exp(-b_{\\beta}\\beta) $$\nThe term $\\left(\\prod_{i=1}^m (1-\\theta_i)\\right)^{\\beta-1}$ can be rewritten as $\\exp\\left( (\\beta-1) \\sum_{i=1}^m \\ln(1-\\theta_i) \\right)$. Thus, the full conditional for $\\beta$ is:\n$$ p(\\beta \\mid \\{\\theta_i\\}_{i=1}^m, \\alpha) \\propto \\left( \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\beta)} \\right)^m \\exp\\left(\\beta \\sum_{i=1}^m \\ln(1-\\theta_i)\\right) \\beta^{a_{\\beta}-1} \\exp(-b_{\\beta}\\beta) $$\nNeither of these full conditionals for $\\alpha$ and $\\beta$ corresponds to a standard, named distribution.\n\n### 3. Gibbs Sampling Scheme\n\nA Gibbs sampler for this model iteratively draws from the full conditional distributions of blocks of parameters. Let the superscript $(t)$ denote the sample at iteration $t$. A coordinate-wise scheme updates blocks $\\{\\theta_i\\}_{i=1}^m$, $\\alpha$, and $\\beta$ sequentially.\n\n1.  **Update Locus-Level Parameters $\\{\\theta_i\\}_{i=1}^{m}$:**\n    For each locus $i=1, \\dots, m$, sample a new value $\\theta_i^{(t+1)}$ from its full conditional distribution, given the current values of the hyperparameters $\\alpha^{(t)}$ and $\\beta^{(t)}$ and the data $y_i, n_i$.\n    $$ \\theta_i^{(t+1)} \\sim \\mathrm{Beta}(y_i + \\alpha^{(t)}, n_i - y_i + \\beta^{(t)}) $$\n    This is a **direct sampling** step, as draws can be made from a standard Beta random number generator. The updates for each $\\theta_i$ are conditionally independent of each other, so they can be performed in parallel.\n\n2.  **Update Hyperparameter $\\alpha$:**\n    Sample a new value $\\alpha^{(t+1)}$ from its full conditional distribution, given the newly updated locus-level parameters $\\{\\theta_i^{(t+1)}\\}_{i=1}^m$ and the current value of $\\beta^{(t)}$.\n    $$ \\alpha^{(t+1)} \\sim p(\\alpha \\mid \\{\\theta_i^{(t+1)}\\}_{i=1}^m, \\beta^{(t)}) $$\n    As derived in Part 2, this distribution is not of a standard form. The log-density involves trigamma functions in its second derivative, and while it is often log-concave (e.g., if $a_\\alpha \\ge 1$), it is not guaranteed to be. Therefore, a generic MCMC method is required. **Slice sampling** is a suitable choice as it does not require log-concavity. Alternatively, a Metropolis-Hastings step or Adaptive Rejection Sampling (if log-concavity can be established) could be used.\n\n3.  **Update Hyperparameter $\\beta$:**\n    Sample a new value $\\beta^{(t+1)}$ from its full conditional distribution, given the updated $\\{\\theta_i^{(t+1)}\\}_{i=1}^m$ and the newly sampled $\\alpha^{(t+1)}$.\n    $$ \\beta^{(t+1)} \\sim p(\\beta \\mid \\{\\theta_i^{(t+1)}\\}_{i=1}^m, \\alpha^{(t+1)}) $$\n    Similar to $\\alpha$, this is not a standard distribution and requires a generic sampler. **Slice sampling** is again a suitable method.\n\nThese three steps constitute one full iteration of the Gibbs sampler. The process is repeated for a large number of iterations to generate samples from the joint posterior distribution.\n\n### 4. Posterior Predictive Probability Calculation\n\nThe posterior predictive probability of a new read at locus $j$ being a mutant, which we can denote as $y_{j, \\text{new}}=1$, is the expectation of the success probability $\\theta_j$ over its posterior distribution. We are given the hyperparameters and data for this specific locus, so we use the full conditional for $\\theta_j$.\n$$ P(y_{j, \\text{new}}=1 \\mid y_j, n_j, \\alpha, \\beta) = \\mathbb{E}[\\theta_j \\mid y_j, n_j, \\alpha, \\beta] $$\nFrom Part 1, the posterior distribution of $\\theta_j$ is:\n$$ \\theta_j \\mid y_j, n_j, \\alpha, \\beta \\sim \\mathrm{Beta}(y_j + \\alpha, n_j - y_j + \\beta) $$\nThe expected value of a random variable $X \\sim \\mathrm{Beta}(a,b)$ is $\\mathbb{E}[X] = \\frac{a}{a+b}$.\nApplying this formula, we get:\n$$ \\mathbb{E}[\\theta_j \\mid y_j, n_j, \\alpha, \\beta] = \\frac{y_j + \\alpha}{(y_j + \\alpha) + (n_j - y_j + \\beta)} = \\frac{y_j + \\alpha}{n_j + \\alpha + \\beta} $$\nWe are given the following values for locus $j$:\n- $y_j = 12$\n- $n_j = 50$\n- $\\alpha = 2.3$\n- $\\beta = 7.1$\n\nSubstituting these numerical values into the expression:\n$$ P(y_{j, \\text{new}}=1 \\mid \\dots) = \\frac{12 + 2.3}{50 + 2.3 + 7.1} = \\frac{14.3}{59.4} $$\nCalculating the value:\n$$ \\frac{14.3}{59.4} \\approx 0.24074074... $$\nRounding the result to four significant figures gives $0.2407$.", "answer": "$$ \\boxed{0.2407} $$", "id": "4541515"}]}