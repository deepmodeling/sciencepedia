{"hands_on_practices": [{"introduction": "Structural variations (SVs), such as deletions, duplications, and translocations, represent large-scale genomic rearrangements. A powerful method for their detection from paired-end sequencing data involves identifying \"discordant\" read pairs, whose mapping distance or orientation deviates from library expectations. This practice [@problem_id:4568947] guides you through the implementation of a fundamental bioinformatic algorithm: clustering these discordant pairs based on their spatial proximity and orientation to pinpoint SV breakpoints. You will learn how to ground algorithmic parameters, such as the clustering radius, in the statistical properties of the sequencing library itself.", "problem": "You are given a set of paired-end sequencing alignments, each represented as a tuple of chromosome identifiers, genomic coordinates, and a pair-orientation code. Each alignment, or \"read pair,\" is represented as $r = (c_1, p_1, c_2, p_2, o)$ where $c_1$ and $c_2$ denote chromosomes, $p_1$ and $p_2$ denote $1$-based positions in base pairs (bp), and $o \\in \\{\\mathrm{FR}, \\mathrm{RF}, \\mathrm{FF}, \\mathrm{RR}\\}$ encodes the orientation of the first and second read as forward ($\\mathrm{F}$) or reverse ($\\mathrm{R}$). The focus is on discordant read pairs, which are those whose orientation, distance, or chromosomal mapping deviates from the expected library profile and thus may support structural variation. Design and implement a clustering algorithm for discordant read pairs using spatial proximity and orientation features, with parameter choices justified from the insert size distribution.\n\nStart from the following fundamental bases:\n\n- Paired-end sequencing yields fragment lengths (insert sizes) described by a random variable $L$ that is well approximated by a Gaussian distribution $L \\sim \\mathcal{N}(\\mu, \\sigma^2)$, where $\\mu$ is the mean insert size and $\\sigma$ is the standard deviation. This approximation is a well-tested empirical observation for many Illumina libraries.\n- The probability integral transform and its inverse imply that, for $q \\in (0,1)$, the $q$-quantile of the standard normal distribution is $z_q = \\Phi^{-1}(q)$, where $\\Phi$ is the cumulative distribution function of the standard normal distribution.\n- Discordant read pairs supporting the same breakpoint are expected to localize within bounded neighborhoods due to finite insert size variability and alignment uncertainty. Therefore, a distance threshold derived from $L$ controls the spatial neighborhood for clustering.\n\nDefine the clustering criterion as follows:\n\n- Construct an undirected graph $G$ whose vertices are the read pairs. Connect two vertices $r_i = (c_1^{(i)}, p_1^{(i)}, c_2^{(i)}, p_2^{(i)}, o^{(i)})$ and $r_j = (c_1^{(j)}, p_1^{(j)}, c_2^{(j)}, p_2^{(j)}, o^{(j)})$ with an edge if and only if all of the following hold:\n  1. Chromosome-pair compatibility: $(c_1^{(i)}, c_2^{(i)}) = (c_1^{(j)}, c_2^{(j)})$.\n  2. Orientation compatibility: $o^{(i)} = o^{(j)}$.\n  3. Spatial proximity: $$d(r_i, r_j) = \\max\\left(\\left|p_1^{(i)} - p_1^{(j)}\\right|, \\left|p_2^{(i)} - p_2^{(j)}\\right|\\right) \\le w,$$ where the proximity window $w$ is specified below.\n- The clusters are the connected components of $G$.\n\nJustify the proximity window $w$ from the insert size distribution as:\n$$w = z_q \\cdot \\sigma + \\epsilon,$$\nwhere $\\sigma$ is the standard deviation of the insert size distribution, $z_q$ is the standard normal $q$-quantile selected to bound fragment variability at level $q$, and $\\epsilon$ is a nonnegative alignment slack in base pairs capturing mapping position uncertainty from alignment scoring and read length effects. All distances and positions are in base pairs (bp); $w$ must be computed in bp.\n\nImplement a program that, for each test case, computes the clusters under the above criterion and returns the multiset of cluster sizes as a list of integers sorted in nondecreasing order. The output across all test cases must be aggregated as a single line: a comma-separated list enclosing each per-test-case result in square brackets. For example, the program must print a single line of the form $[ [s_{1,1}, \\dots], [s_{2,1}, \\dots], \\dots ]$.\n\nUse the following test suite. In each case, positions are in base pairs (bp), and parameters $\\mu$, $\\sigma$, $\\epsilon$, and $q$ are provided. Your program must compute $w = z_q \\cdot \\sigma + \\epsilon$ (in bp) for each case using $z_q = \\Phi^{-1}(q)$, then perform clustering.\n\n- Test Case $1$ (general case):\n  - Library parameters: $\\mu = 350$, $\\sigma = 30$, $\\epsilon = 10$, $q = 0.9973$.\n  - Read pairs:\n    - $(1, 100020, 1, 200010, \\mathrm{RF})$\n    - $(1, 100080, 1, 199950, \\mathrm{RF})$\n    - $(1, 100050, 1, 200090, \\mathrm{RF})$\n    - $(1, 99960, 1, 200020, \\mathrm{RF})$\n    - $(1, 300010, 1, 399930, \\mathrm{RF})$\n    - $(1, 300070, 1, 400050, \\mathrm{RF})$\n    - $(1, 299950, 1, 400020, \\mathrm{RF})$\n    - $(1, 100500, 1, 200500, \\mathrm{RF})$\n- Test Case $2$ (boundary condition on $w$):\n  - Library parameters: $\\mu = 400$, $\\sigma = 20$, $\\epsilon = 5$, $q = 0.95$.\n  - Read pairs:\n    - $(2, 150000, 2, 250000, \\mathrm{FF})$\n    - $(2, 150037, 2, 250037, \\mathrm{FF})$\n    - $(2, 150076, 2, 250076, \\mathrm{FF})$\n- Test Case $3$ (empty input):\n  - Library parameters: $\\mu = 300$, $\\sigma = 25$, $\\epsilon = 15$, $q = 0.99$.\n  - Read pairs: none.\n- Test Case $4$ (orientation separation):\n  - Library parameters: $\\mu = 500$, $\\sigma = 50$, $\\epsilon = 10$, $q = 0.9973$.\n  - Read pairs:\n    - $(23, 50010, 23, 69990, \\mathrm{RF})$\n    - $(23, 50080, 23, 70040, \\mathrm{RF})$\n    - $(23, 50020, 23, 70010, \\mathrm{FR})$\n- Test Case $5$ (chromosome-pair separation):\n  - Library parameters: $\\mu = 350$, $\\sigma = 40$, $\\epsilon = 10$, $q = 0.9$.\n  - Read pairs:\n    - $(1, 80000, 2, 120000, \\mathrm{RR})$\n    - $(1, 80050, 2, 120030, \\mathrm{RR})$\n    - $(1, 80110, 3, 119980, \\mathrm{RR})$\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each result is the sorted list of cluster sizes for the corresponding test case. For example, the output format must be exactly of the form $[[r_1],[r_2],[r_3],[r_4],[r_5]]$, where $[r_i]$ denotes the list of cluster sizes for test case $i$.", "solution": "The problem requires the design and implementation of an algorithm to cluster discordant paired-end sequencing reads, which is a foundational task in computational genomics for the detection of structural variations (SVs). The clustering is based on the principle that reads supporting the same genomic rearrangement event should be close to each other in terms of their mapping coordinates and share the same orientation signature.\n\nThe problem is scientifically well-grounded and mathematically well-posed. The a priori information, including the representation of read pairs, the Gaussian model for insert sizes, and the clustering criteria, is complete and consistent. We can therefore proceed with a formal solution.\n\nThe solution can be decomposed into three primary stages: (1) determination of the spatial proximity threshold $w$ from the provided library statistics, (2) partitioning of reads into compatible groups, and (3) application of a graph-based clustering algorithm to each group to find the connected components, which represent the final clusters.\n\nFirst, we establish the quantitative basis for clustering. The problem specifies a graph-theoretic approach where an undirected graph $G=(V, E)$ is constructed. The set of vertices $V$ is the set of discordant read pairs, and an edge $(r_i, r_j) \\in E$ exists between two reads $r_i$ and $r_j$ if they are compatible. Compatibility is defined by three conditions:\n1.  **Chromosome-pair compatibility**: The reads must map to the same pair of chromosomes. For a read $r_i = (c_1^{(i)}, p_1^{(i)}, c_2^{(i)}, p_2^{(i)}, o^{(i)})$, this means the tuple $(c_1^{(i)}, c_2^{(i)})$ must be identical for both reads.\n2.  **Orientation compatibility**: The reads must have the same orientation code, i.e., $o^{(i)} = o^{(j)}$.\n3.  **Spatial proximity**: The distance between the reads' anchor points must be within a tolerance $w$. The distance metric is the Chebyshev distance ($L_\\infty$ norm) between the coordinate vectors of the two reads: $d(r_i, r_j) = \\max\\left(\\left|p_1^{(i)} - p_1^{(j)}\\right|, \\left|p_2^{(i)} - p_2^{(j)}\\right|\\right) \\le w$.\n\nThe spatial proximity window, $w$, is derived from statistical principles of the sequencing library. The formula is given as:\n$$w = z_q \\cdot \\sigma + \\epsilon$$\nHere, $\\sigma$ is the standard deviation of the insert size distribution, $L \\sim \\mathcal{N}(\\mu, \\sigma^2)$. The term $z_q \\cdot \\sigma$ captures the expected positional variance of reads due to the natural variation in DNA fragment lengths. $z_q = \\Phi^{-1}(q)$ is the $q$-quantile of the standard normal distribution, where $\\Phi$ is the standard normal cumulative distribution function (CDF). By choosing a high quantile $q$ (e.g., $q=0.9973$), we create a tolerance interval that is expected to contain the positional deviation for a very high proportion of reads originating from the same event. It is important to note that the mean insert size $\\mu$ is not part of this calculation. The threshold $w$ is concerned with the *spread* or *variability* of positions around a common breakpoint, not the absolute distance between the two ends of a read pair, which is related to $\\mu$. The term $\\epsilon$ is a non-negative slack parameter that accounts for additional, systematic sources of error, primarily alignment uncertainty from the read mapping algorithm.\n\nThe core of the algorithm is to identify the connected components of the graph $G$. A naive construction of an adjacency matrix for $G$ would be inefficient. A more practical approach is to first partition the set of all reads. The first two compatibility criteria (chromosome-pair and orientation) are discrete and define an equivalence relation. We can partition the set of all reads into disjoint subsets, or groups, where all reads within a group share the same $(c_1, c_2, o)$ key. Edges in the graph $G$ can only exist between reads within the same group. Therefore, the problem of finding connected components in $G$ decomposes into finding connected components independently within each of these smaller subgraphs.\n\nFor each group of reads, we can find its connected components using a standard graph traversal algorithm, such as Depth-First Search (DFS) or Breadth-First Search (BFS). The procedure is as follows:\n1.  Initialize a set of `visited` nodes (reads) to be empty.\n2.  Initialize an empty list to store `cluster_sizes`.\n3.  For each read `u` in the group:\n    a. If `u` has not been visited:\n        i. This read starts a new cluster. Initialize a counter for the `current_cluster_size` to $0$.\n        ii. Start a traversal (e.g., DFS) from `u`. A stack is convenient for an iterative DFS. Push `u` onto the stack and mark it as visited.\n        iii. While the stack is not empty:\n            - Pop a read `v` from the stack.\n            - Increment `current_cluster_size`.\n            - For each unvisited neighbor `x` of `v` in the group (i.e., a read `x` for which $d(v, x) \\le w$):\n                - Mark `x` as visited.\n                - Push `x` onto the stack.\n        iv. Once the traversal is complete, the `current_cluster_size` is the size of the connected component. Add this size to the `cluster_sizes` list.\n4.  After iterating through all reads in the group, the `cluster_sizes` list contains the sizes of all clusters found in that group.\n\nThis process is repeated for every group of reads. The final result for a given test case is the aggregation of all cluster sizes from all groups, sorted in non-decreasing order. If a test case has no reads, the result is an empty list.\n\nThe implementation will use `scipy.stats.norm.ppf` to compute the quantile $z_q$. The reads for each test case will be processed by first partitioning them into a dictionary where keys are the compatibility tuples $(c_1, c_2, o)$ and values are lists of reads. Then, the connected components algorithm described above will be applied to each list of reads in the dictionary's values.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves the discordant read-pair clustering problem for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"params\": {\"mu\": 350, \"sigma\": 30, \"epsilon\": 10, \"q\": 0.9973},\n            \"reads\": [\n                (1, 100020, 1, 200010, \"RF\"),\n                (1, 100080, 1, 199950, \"RF\"),\n                (1, 100050, 1, 200090, \"RF\"),\n                (1, 99960, 1, 200020, \"RF\"),\n                (1, 300010, 1, 399930, \"RF\"),\n                (1, 300070, 1, 400050, \"RF\"),\n                (1, 299950, 1, 400020, \"RF\"),\n                (1, 100500, 1, 200500, \"RF\"),\n            ],\n        },\n        {\n            \"params\": {\"mu\": 400, \"sigma\": 20, \"epsilon\": 5, \"q\": 0.95},\n            \"reads\": [\n                (2, 150000, 2, 250000, \"FF\"),\n                (2, 150037, 2, 250037, \"FF\"),\n                (2, 150076, 2, 250076, \"FF\"),\n            ],\n        },\n        {\n            \"params\": {\"mu\": 300, \"sigma\": 25, \"epsilon\": 15, \"q\": 0.99},\n            \"reads\": [],\n        },\n        {\n            \"params\": {\"mu\": 500, \"sigma\": 50, \"epsilon\": 10, \"q\": 0.9973},\n            \"reads\": [\n                (23, 50010, 23, 69990, \"RF\"),\n                (23, 50080, 23, 70040, \"RF\"),\n                (23, 50020, 23, 70010, \"FR\"),\n            ],\n        },\n        {\n            \"params\": {\"mu\": 350, \"sigma\": 40, \"epsilon\": 10, \"q\": 0.9},\n            \"reads\": [\n                (1, 80000, 2, 120000, \"RR\"),\n                (1, 80050, 2, 120030, \"RR\"),\n                (1, 80110, 3, 119980, \"RR\"),\n            ],\n        },\n    ]\n\n    all_results = []\n    for case in test_cases:\n        params = case[\"params\"]\n        reads = case[\"reads\"]\n\n        # Step 1: Calculate proximity window `w`\n        sigma = params[\"sigma\"]\n        epsilon = params[\"epsilon\"]\n        q = params[\"q\"]\n        z_q = norm.ppf(q)\n        w = z_q * sigma + epsilon\n\n        # If there are no reads, the result is an empty list of cluster sizes.\n        if not reads:\n            all_results.append([])\n            continue\n            \n        # Step 2: Partition reads into groups by compatibility keys\n        groups = {}\n        for read in reads:\n            c1, p1, c2, p2, o = read\n            # The key for grouping is (chromosome1, chromosome2, orientation)\n            key = (c1, c2, o)\n            if key not in groups:\n                groups[key] = []\n            groups[key].append(read)\n\n        case_cluster_sizes = []\n        \n        # Step 3: Find connected components in each group\n        for key in groups:\n            group_reads = groups[key]\n            n = len(group_reads)\n            visited = [False] * n\n\n            for i in range(n):\n                if not visited[i]:\n                    current_cluster_size = 0\n                    stack = [i]\n                    visited[i] = True\n                    \n                    while stack:\n                        u_idx = stack.pop()\n                        current_cluster_size += 1\n                        u_read = group_reads[u_idx]\n\n                        # Find all unvisited neighbors\n                        for v_idx in range(n):\n                            if not visited[v_idx]:\n                                v_read = group_reads[v_idx]\n                                \n                                # Check for spatial proximity\n                                dist_p1 = abs(u_read[1] - v_read[1])\n                                dist_p2 = abs(u_read[3] - v_read[3])\n                                \n                                if max(dist_p1, dist_p2) = w:\n                                    visited[v_idx] = True\n                                    stack.append(v_idx)\n                    \n                    case_cluster_sizes.append(current_cluster_size)\n        \n        case_cluster_sizes.sort()\n        all_results.append(case_cluster_sizes)\n\n    # Format the final output as a single-line string.\n    # Ex: [[1, 2], [], [1, 1, 3]] -> \"[[1,2],[],[1,1,3]]\"\n    output_str = \"[\" + \",\".join([f\"[{','.join(map(str, res))}]\" for res in all_results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "4568947"}, {"introduction": "Beyond large rearrangements, detecting changes in the dosage of a DNA segment, known as Copy Number Variations (CNVs), is a central task in genomics. While read depth provides a direct signal for copy number, raw coverage is notoriously skewed by local genomic context, especially the uniqueness or \"mappability\" of a sequence. This exercise [@problem_id:4568999] addresses this challenge directly by having you implement a statistically principled method for CNV detection. The procedure involves a critical mappability-based normalization of read coverage and a formal hypothesis test to robustly distinguish true copy number changes from technical artifacts.", "problem": "Consider short-read whole-genome sequencing used to detect genomic variation, including Single Nucleotide Polymorphism (SNP), insertions and deletions (indels), and Structural Variation (SV). Reads are aligned to a reference genome, and the read coverage at each genomic position is used to infer Copy Number Variation (CNV). Two foundational facts motivate the statistical treatment of coverage: (i) under uniform sampling and independent read starts, per-base read counts follow a Poisson process, and (ii) alignment mappability varies across the genome because of repetitive DNA, including segmental duplications. A mappability track assigns to each genomic position a score between $0$ and $1$ representing the probability that a read originating at that position can be uniquely aligned; low mappability positions yield reduced observed coverage even without true CNV, and highly repetitive regions can also exhibit aberrant pile-ups depending on alignment heuristics. The goal is to formalize how mappability and segmental duplications distort coverage and to design a principled filtering and normalization scheme that mitigates false CNV positives using mappability tracks.\n\nFundamental base and definitions:\n- The Central Dogma of Molecular Biology states that deoxyribonucleic acid (DNA) is transcribed to ribonucleic acid (RNA) and translated into protein; while not directly governing read coverage, this sets the context for sequencing-based inference of genomic variation.\n- In short-read sequencing, the observed per-base coverage $C_i$ at position $i$ can be modeled as a Poisson random variable with mean $\\lambda_i$, denoted $C_i \\sim \\mathrm{Poisson}(\\lambda_i)$, where $\\lambda_i$ depends on total sequencing depth, copy number, and mappability.\n- Let $m_i \\in [0,1]$ denote the mappability at base $i$. Let the baseline expected coverage for a diploid copy number of $2$ in a uniquely mappable region be $\\mu$ (reads per base). Then, for absolute copy number $\\mathrm{CN}$ at base $i$, the expected coverage satisfies $\\mathbb{E}[C_i] = \\lambda_i = \\mu \\cdot m_i \\cdot \\frac{\\mathrm{CN}}{2}$ under a model where non-uniquely mappable reads are effectively unavailable for confident alignment.\n- A Copy Number Variation (CNV) call aims to detect $\\mathrm{CN} \\neq 2$ by aggregating evidence over a genomic window. Segmental duplications and other low-mappability features distort naive coverage-based CNV calling because $\\{m_i\\}$ varies substantially, often biasing $C_i$ downward independent of true $\\mathrm{CN}$.\n\nTask:\n1. Derive a normalization and testing scheme, starting from the Poisson model with mappability, to estimate $\\widehat{\\mathrm{CN}}$ for a genomic window and to compute a test statistic for deviation from the null $\\mathrm{CN}_0 = 2$. Your derivation must not rely on shortcut formulas; it should begin from $C_i \\sim \\mathrm{Poisson}(\\lambda_i)$ with $\\lambda_i = \\mu \\cdot m_i \\cdot \\frac{\\mathrm{CN}}{2}$.\n2. Propose and implement a filtering procedure using mappability tracks that mitigates false positives in low-mappability or highly repetitive windows. The filter must include:\n   - Exclusion of bases with $m_i  t_{\\text{map}}$.\n   - A minimum fraction $f_{\\min}$ of retained bases with $m_i \\ge t_{\\text{map}}$ required to attempt a CNV call.\n   - A minimum retained length $n_{\\min}$ (in bases) below which the window is considered inconclusive.\n3. Implement the resulting estimator and test statistic in code. For each test window, classify the CNV state using a symmetric significance threshold $z_{\\mathrm{thr}}$: output $-1$ for deletion, $0$ for neutral (no CNV call or inconclusive), and $+1$ for duplication.\n\nMathematical specification to implement:\n- Given a window with per-base coverage $\\{C_i\\}_{i=1}^n$, mappability $\\{m_i\\}_{i=1}^n$, and parameters $\\mu$, $t_{\\text{map}}$, $f_{\\text{min}}$, $n_{\\text{min}}$, and $z_{\\mathrm{thr}}$, apply the filter to retain indices $\\mathcal{I} = \\{ i \\in \\{1,\\dots,n\\} : m_i \\ge t_{\\text{map}} \\}$. If $|\\mathcal{I}|  n_{\\text{min}}$ or $\\frac{|\\mathcal{I}|}{n}  f_{\\text{min}}$, output $0$.\n- Define normalized per-base quantities $X_i = \\frac{C_i}{\\mu \\cdot m_i}$ for $i \\in \\mathcal{I}$. Let $\\widehat{\\mathrm{CN}} = \\frac{2}{|\\mathcal{I}|} \\sum_{i \\in \\mathcal{I}} X_i$. Under the null $\\mathrm{CN}_0 = 2$, approximate the variance of $X_i$ by $\\mathrm{Var}(X_i \\mid \\mathrm{CN}_0) = \\frac{\\mathrm{CN}_0}{2 \\cdot \\mu \\cdot m_i}$ by Poisson-to-normal approximation. Then the null variance of $\\widehat{\\mathrm{CN}}$ is\n$$\n\\mathrm{Var}_0(\\widehat{\\mathrm{CN}}) = \\left( \\frac{2}{|\\mathcal{I}|} \\right)^2 \\sum_{i \\in \\mathcal{I}} \\mathrm{Var}(X_i \\mid \\mathrm{CN}_0) = \\frac{4}{|\\mathcal{I}|^2} \\sum_{i \\in \\mathcal{I}} \\frac{1}{\\mu \\cdot m_i}.\n$$\n- Compute the $z$-score under the null,\n$$\nz = \\frac{\\widehat{\\mathrm{CN}} - 2}{\\sqrt{\\mathrm{Var}_0(\\widehat{\\mathrm{CN}})}}.\n$$\nClassify by $z$: output $-1$ if $z \\le -z_{\\mathrm{thr}}$, $+1$ if $z \\ge z_{\\mathrm{thr}}$, else $0$.\n\nTest suite:\nUse the following five windows that probe diverse scenarios, with arrays expressed as sequences of integers for $\\{C_i\\}$ and decimals in $[0,1]$ for $\\{m_i\\}$:\n\n- Test $1$ (high mappability, neutral diploid): $\\mu = 30$, $t_{\\text{map}} = 0.5$, $f_{\\text{min}} = 0.6$, $n_{\\text{min}} = 5$, $z_{\\mathrm{thr}} = 3.0$, $\\{C_i\\} = \\{31,29,33,30,28,32,31,30,29,34\\}$, $\\{m_i\\} = \\{0.99,0.98,1.0,0.97,0.96,0.99,0.98,1.0,0.95,0.97\\}$.\n- Test $2$ (segmental duplication with low mappability, neutral diploid but coverage dropout): $\\mu = 30$, $t_{\\text{map}} = 0.5$, $f_{\\text{min}} = 0.6$, $n_{\\text{min}} = 5$, $z_{\\mathrm{thr}} = 3.0$, $\\{C_i\\} = \\{9,11,8,12,10,9,10,9,8,10\\}$, $\\{m_i\\} = \\{0.30,0.35,0.25,0.40,0.33,0.28,0.31,0.29,0.27,0.34\\}$.\n- Test $3$ (true heterozygous deletion in high mappability): $\\mu = 30$, $t_{\\text{map}} = 0.5$, $f_{\\text{min}} = 0.6$, $n_{\\text{min}} = 5$, $z_{\\mathrm{thr}} = 3.0$, $\\{C_i\\} = \\{15,14,16,13,17,15,14,16\\}$, $\\{m_i\\} = \\{0.98,0.97,0.99,0.96,0.97,0.98,0.99,0.97\\}$.\n- Test $4$ (duplication in mixed mappability; sufficient unique fraction to pass filter): $\\mu = 30$, $t_{\\text{map}} = 0.5$, $f_{\\text{min}} = 0.6$, $n_{\\text{min}} = 5$, $z_{\\mathrm{thr}} = 3.0$, $\\{C_i\\} = \\{36,31,27,18,25,41,38,23,20,31,29,27\\}$, $\\{m_i\\} = \\{0.8,0.7,0.6,0.4,0.55,0.9,0.85,0.5,0.45,0.7,0.65,0.6\\}$.\n- Test $5$ (short window at boundary; inconclusive by minimum length): $\\mu = 30$, $t_{\\text{map}} = 0.5$, $f_{\\text{min}} = 0.6$, $n_{\\text{min}} = 5$, $z_{\\mathrm{thr}} = 3.0$, $\\{C_i\\} = \\{30,31,29\\}$, $\\{m_i\\} = \\{0.95,0.96,0.97\\}$.\n\nFinal output format:\nYour program should produce a single line of output containing the CNV classifications for the five tests as a comma-separated list enclosed in square brackets, for example $[\\dots]$. Use integers to encode calls: $-1$ for deletion, $0$ for neutral or filtered/inconclusive, and $+1$ for duplication. No units are required in the output. The program must be self-contained and require no user input.", "solution": "The problem requires the formulation and implementation of a statistical method to detect Copy Number Variations (CNVs) from short-read sequencing data, while accounting for confounding effects of genome mappability. The solution involves three principal components: a statistical model for read coverage, a filtering scheme to handle regions of low data quality, and a hypothesis testing framework to assess the significance of observed coverage deviations.\n\nThe derivation and procedure are as follows:\n\nFirst, we establish the statistical model for observed read counts. At any given genomic position $i$, the number of reads covering that position, $C_i$, is modeled as a Poisson-distributed random variable, denoted $C_i \\sim \\mathrm{Poisson}(\\lambda_i)$. The parameter $\\lambda_i$ represents the expected (or mean) coverage at that position.\n\nSecond, we model the expected coverage $\\lambda_i$ by incorporating biological and technical factors. The problem posits that $\\lambda_i$ is a function of three main components:\n1.  The overall sequencing depth, represented by a baseline mean coverage $\\mu$ in a \"normal\" genomic region. A normal region is defined as being diploid (copy number $2$) and perfectly mappable.\n2.  The true, underlying absolute copy number at position $i$, denoted $\\mathrm{CN}$. The coverage is proportional to the copy number. We normalize this by the diploid state, leading to a factor of $\\frac{\\mathrm{CN}}{2}$.\n3.  The alignment mappability at position $i$, $m_i \\in [0, 1]$. This score represents the probability that a read originating at position $i$ can be uniquely and correctly mapped back to the reference genome. Non-uniquely mappable reads are typically discarded or have their mapping quality scores reduced, leading to an effective reduction in observed coverage.\nCombining these factors, the model for the expected coverage is given by:\n$$\n\\lambda_i = \\mathbb{E}[C_i] = \\mu \\cdot m_i \\cdot \\frac{\\mathrm{CN}}{2}\n$$\n\nThe goal is to estimate $\\mathrm{CN}$ for a genomic window containing bases $\\{1, \\dots, n\\}$ and test whether it deviates from the diploid null hypothesis, $H_0: \\mathrm{CN} = 2$.\n\nTo derive a copy number estimator, we first define a normalized quantity for each base that corrects for the known effects of sequencing depth ($\\mu$) and mappability ($m_i$). Let us define $X_i$:\n$$\nX_i = \\frac{C_i}{\\mu \\cdot m_i}\n$$\nSubstituting the expectation of $C_i$, the expectation of $X_i$ is:\n$$\n\\mathbb{E}[X_i] = \\frac{\\mathbb{E}[C_i]}{\\mu \\cdot m_i} = \\frac{\\mu \\cdot m_i \\cdot \\frac{\\mathrm{CN}}{2}}{\\mu \\cdot m_i} = \\frac{\\mathrm{CN}}{2}\n$$\nThis shows that $X_i$ is an unbiased estimator for $\\frac{\\mathrm{CN}}{2}$. To get a more robust estimate of $\\mathrm{CN}$ over a genomic window, we can average these single-position estimates. The problem specifies a filtering step before aggregation. We consider only the set of indices $\\mathcal{I} = \\{ i \\in \\{1,\\dots,n\\} : m_i \\ge t_{\\text{map}} \\}$, where $t_{\\text{map}}$ is a minimum mappability threshold. The copy number estimator for the window, $\\widehat{\\mathrm{CN}}$, is then defined as the average of the estimators for $2 \\cdot X_i$ over the set $\\mathcal{I}$:\n$$\n\\widehat{\\mathrm{CN}} = \\frac{1}{|\\mathcal{I}|} \\sum_{i \\in \\mathcal{I}} (2 \\cdot X_i) = \\frac{2}{|\\mathcal{I}|} \\sum_{i \\in \\mathcal{I}} \\frac{C_i}{\\mu \\cdot m_i}\n$$\nThis is a Method of Moments estimator for $\\mathrm{CN}$. By linearity of expectation, it is unbiased: $\\mathbb{E}[\\widehat{\\mathrm{CN}}] = \\mathrm{CN}$.\n\nTo perform a statistical test, we must determine the distribution of $\\widehat{\\mathrm{CN}}$ under the null hypothesis, $H_0: \\mathrm{CN} = 2$. By the Central Limit Theorem, the sum (and thus the mean) of a sufficiently large number of independent random variables will be approximately normally distributed. We assume the read counts $C_i$ are independent across positions.\nThe mean of $\\widehat{\\mathrm{CN}}$ under $H_0$ is $\\mathbb{E}[\\widehat{\\mathrm{CN}} \\mid H_0] = 2$.\nNext, we derive the variance of $\\widehat{\\mathrm{CN}}$ under $H_0$, denoted $\\mathrm{Var}_0(\\widehat{\\mathrm{CN}})$. First, we find the variance of $X_i$:\n$$\n\\mathrm{Var}(X_i) = \\mathrm{Var}\\left(\\frac{C_i}{\\mu \\cdot m_i}\\right) = \\frac{1}{(\\mu \\cdot m_i)^2} \\mathrm{Var}(C_i)\n$$\nA key property of the Poisson distribution is that its variance equals its mean, $\\mathrm{Var}(C_i) = \\lambda_i$. Under $H_0: \\mathrm{CN} = 2$, the mean coverage is $\\lambda_{i,0} = \\mu \\cdot m_i \\cdot \\frac{2}{2} = \\mu \\cdot m_i$. Therefore, the variance of $X_i$ under the null is:\n$$\n\\mathrm{Var}(X_i \\mid H_0) = \\frac{\\lambda_{i,0}}{(\\mu \\cdot m_i)^2} = \\frac{\\mu \\cdot m_i}{(\\mu \\cdot m_i)^2} = \\frac{1}{\\mu \\cdot m_i}\n$$\nThis corresponds to the provided formula $\\frac{\\mathrm{CN}_0}{2 \\cdot \\mu \\cdot m_i}$ with $\\mathrm{CN}_0 = 2$.\nAssuming independence of the $X_i$ variables, the variance of their sum is the sum of their variances. Thus, the variance of $\\widehat{\\mathrm{CN}}$ under $H_0$ is:\n$$\n\\mathrm{Var}_0(\\widehat{\\mathrm{CN}}) = \\mathrm{Var}\\left(\\frac{2}{|\\mathcal{I}|} \\sum_{i \\in \\mathcal{I}} X_i \\mid H_0 \\right) = \\left(\\frac{2}{|\\mathcal{I}|}\\right)^2 \\sum_{i \\in \\mathcal{I}} \\mathrm{Var}(X_i \\mid H_0) = \\frac{4}{|\\mathcal{I}|^2} \\sum_{i \\in \\mathcal{I}} \\frac{1}{\\mu \\cdot m_i}\n$$\nWith the mean and variance defined, we can construct a $z$-score test statistic, which measures the deviation of the estimate from its null-hypothesized value in units of standard deviations:\n$$\nz = \\frac{\\widehat{\\mathrm{CN}} - \\mathbb{E}[\\widehat{\\mathrm{CN}} \\mid H_0]}{\\sqrt{\\mathrm{Var}_0(\\widehat{\\mathrm{CN}})}} = \\frac{\\widehat{\\mathrm{CN}} - 2}{\\sqrt{\\mathrm{Var}_0(\\widehat{\\mathrm{CN}})}}\n$$\nUnder the null hypothesis, this $z$-score approximately follows a standard normal distribution, $\\mathcal{N}(0, 1)$.\n\nBefore conducting the test, a filtering procedure is applied to ensure the reliability of the call. This is crucial as low-mappability regions can produce noisy and biased coverage data, while very short windows lack statistical power. The specified filters are:\n1.  Per-base mappability filter: Only bases $i$ with mappability $m_i \\ge t_{\\text{map}}$ are included in the analysis. This defines the set $\\mathcal{I}$.\n2.  Window quality filters:\n    -   The number of retained bases must meet a minimum count: $|\\mathcal{I}| \\ge n_{\\text{min}}$.\n    -   The fraction of retained bases must meet a minimum threshold: $\\frac{|\\mathcal{I}|}{n} \\ge f_{\\text{min}}$, where $n$ is the total number of bases in the original window.\nIf a window fails any of these quality filters, it is deemed inconclusive, and the output is $0$ (neutral).\n\nFinally, for windows that pass the filters, the CNV state is classified based on the calculated $z$-score and a pre-defined significance threshold $z_{\\mathrm{thr}}$. A symmetric threshold is used to call deletions or duplications:\n- If $z \\ge z_{\\mathrm{thr}}$, the coverage is significantly higher than expected under the null, implying a duplication. The output is $+1$.\n- If $z \\le -z_{\\mathrm{thr}}$, the coverage is significantly lower than expected, implying a deletion. The output is $-1$.\n- If $-z_{\\mathrm{thr}}  z  z_{\\mathrm{thr}}$, the deviation is not statistically significant. The output is $0$ (neutral).\n\nThis complete scheme provides a principled, statistically grounded method for CNV detection that correctly normalizes for mappability and filters for data quality, thereby mitigating common sources of false positives.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the CNV detection problem for the given test suite.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"id\": 1,\n            \"mu\": 30.0, \"t_map\": 0.5, \"f_min\": 0.6, \"n_min\": 5, \"z_thr\": 3.0,\n            \"C\": np.array([31, 29, 33, 30, 28, 32, 31, 30, 29, 34]),\n            \"m\": np.array([0.99, 0.98, 1.0, 0.97, 0.96, 0.99, 0.98, 1.0, 0.95, 0.97])\n        },\n        {\n            \"id\": 2,\n            \"mu\": 30.0, \"t_map\": 0.5, \"f_min\": 0.6, \"n_min\": 5, \"z_thr\": 3.0,\n            \"C\": np.array([9, 11, 8, 12, 10, 9, 10, 9, 8, 10]),\n            \"m\": np.array([0.30, 0.35, 0.25, 0.40, 0.33, 0.28, 0.31, 0.29, 0.27, 0.34])\n        },\n        {\n            \"id\": 3,\n            \"mu\": 30.0, \"t_map\": 0.5, \"f_min\": 0.6, \"n_min\": 5, \"z_thr\": 3.0,\n            \"C\": np.array([15, 14, 16, 13, 17, 15, 14, 16]),\n            \"m\": np.array([0.98, 0.97, 0.99, 0.96, 0.97, 0.98, 0.99, 0.97])\n        },\n        {\n            \"id\": 4,\n            \"mu\": 30.0, \"t_map\": 0.5, \"f_min\": 0.6, \"n_min\": 5, \"z_thr\": 3.0,\n            \"C\": np.array([36, 31, 27, 18, 25, 41, 38, 23, 20, 31, 29, 27]),\n            \"m\": np.array([0.8, 0.7, 0.6, 0.4, 0.55, 0.9, 0.85, 0.5, 0.45, 0.7, 0.65, 0.6])\n        },\n        {\n            \"id\": 5,\n            \"mu\": 30.0, \"t_map\": 0.5, \"f_min\": 0.6, \"n_min\": 5, \"z_thr\": 3.0,\n            \"C\": np.array([30, 31, 29]),\n            \"m\": np.array([0.95, 0.96, 0.97])\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        C = case[\"C\"]\n        m = case[\"m\"]\n        mu = case[\"mu\"]\n        t_map = case[\"t_map\"]\n        f_min = case[\"f_min\"]\n        n_min = case[\"n_min\"]\n        z_thr = case[\"z_thr\"]\n        \n        n = len(C) # Original window size\n\n        # Step 1: Apply mappability filter to get the set of indices I\n        mappable_indices = np.where(m >= t_map)[0]\n        I_size = len(mappable_indices)\n        \n        # Step 2: Apply window quality filters\n        if I_size  n_min or (I_size / n)  f_min:\n            results.append(0)\n            continue\n            \n        C_I = C[mappable_indices]\n        m_I = m[mappable_indices]\n\n        # Step 3: Calculate the CNV estimate\n        # X_i = C_i / (mu * m_i) for i in I\n        X_I = C_I / (mu * m_I)\n        \n        # CN_hat = (2 / |I|) * sum(X_i) = 2 * mean(X_I)\n        CN_hat = 2.0 * np.mean(X_I)\n        \n        # Step 4: Calculate the variance of the estimator under the null (CN=2)\n        # Var_0(CN_hat) = (4 / |I|^2) * sum(1 / (mu * m_i)) for i in I\n        var_sum_term = np.sum(1.0 / (mu * m_I))\n        var_CN_hat_null = (4.0 / (I_size**2)) * var_sum_term\n        \n        # Step 5: Compute the z-score\n        # Prevent division by zero if variance is somehow zero\n        if var_CN_hat_null = 0:\n            results.append(0) # Inconclusive if variance is not positive\n            continue\n\n        std_dev_CN_hat_null = np.sqrt(var_CN_hat_null)\n        z_score = (CN_hat - 2.0) / std_dev_CN_hat_null\n\n        # Step 6: Classify the CNV state\n        if z_score = -z_thr:\n            results.append(-1) # Deletion\n        elif z_score >= z_thr:\n            results.append(1) # Duplication\n        else:\n            results.append(0) # Neutral\n            \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4568999"}, {"introduction": "For the most detailed view of copy number alterations, it is essential to resolve the copy number of each parental haplotype, a task known as allele-specific copy number (ASCN) analysis. This level of resolution is critical for studying phenomena like loss-of-heterozygosity in cancer or imprinting disorders. In this advanced practice [@problem_id:4569034], you will use a Hidden Markov Model (HMM), a powerful probabilistic tool, to jointly infer copy number states and chromosomal phase. This model integrates two distinct data types—the total copy number signal ($\\log_2$ ratio) and the allelic imbalance signal (B-allele frequency)—to decode the underlying genomic state along a chromosome.", "problem": "You are given paired genomic measurement tracks per window: a log-base-2 copy ratio ($\\log_2$ ratio, denoted $r_t$) and a B-allele frequency (denoted $b_t$) for window index $t \\in \\{1,\\dots,T\\}$. The $\\log_2$ ratio $r_t$ measures the relative total copy-number intensity compared to a diploid reference, and the B-allele frequency $b_t$ measures the fraction of reads supporting the B allele at germline heterozygous single-nucleotide polymorphisms (SNPs) aggregated within the window. The goal is to jointly infer the allele-specific copy number state and the phase assignment along the genome using a probabilistic model with explicit emission distributions, and to report the positions of phase breaks.\n\nFundamental base and definitions:\n- In allele-specific copy number, each genomic window is assumed to be described by a pair of nonnegative integers $(A_t, B_t)$ representing the counts of the two parental haplotypes, with the total copy number $C_t = A_t + B_t$. Define the minor and major copy numbers as $m_t = \\min(A_t,B_t)$ and $M_t = \\max(A_t,B_t)$, so that $C_t = M_t + m_t$ and $M_t \\ge m_t \\ge 0$.\n- The expected $\\log_2$ ratio is anchored to a diploid reference $C = 2$, so for a window with total copy number $C_t$, the expected $\\log_2$ ratio is $\\mu_r(C_t) = \\log_2\\!\\left(\\dfrac{C_t}{2}\\right)$.\n- The expected B-allele frequency depends on which haplotype is labeled as the B allele. Introduce a latent binary phase variable $p_t \\in \\{0,1\\}$ that indicates whether the observed B allele aligns to the haplotype with minor copy number ($p_t = 0$) or to the major copy number ($p_t = 1$). Under this convention, the expected B-allele frequency is\n$$\n\\mu_b(m_t, M_t, p_t) =\n\\begin{cases}\n\\dfrac{m_t}{m_t + M_t},  \\text{if } p_t = 0,\\\n$$6pt]\n\\dfrac{M_t}{m_t + M_t},  \\text{if } p_t = 1.\n\\end{cases}\n$$\n\nModel assumptions:\n- The pair $(r_t, b_t)$ is modeled as conditionally independent given the allele-specific copy state $(m_t,M_t)$ and phase $p_t$, with Gaussian emissions,\n$$\nr_t \\sim \\mathcal{N}\\!\\left(\\mu_r(C_t), \\sigma_r^2\\right), \\quad\nb_t \\sim \\mathcal{N}\\!\\left(\\mu_b(m_t,M_t,p_t), \\sigma_b^2\\right),\n$$\nwith known standard deviations $\\sigma_r$ and $\\sigma_b$.\n- Along the genome, the latent process is a Hidden Markov Model (HMM) on joint states $s_t = \\left((m_t,M_t), p_t\\right)$ that factorizes into a copy-number component and a phase component. The transition probability satisfies\n$$\n\\Pr\\!\\left(s_t \\mid s_{t-1}\\right) = \\Pr\\!\\left((m_t,M_t) \\mid (m_{t-1},M_{t-1})\\right) \\cdot \\Pr\\!\\left(p_t \\mid p_{t-1}\\right),\n$$\nwith specified transition probabilities. The initial distribution also factorizes.\n\nState space and parameters to use in your program:\n- Allowed allele-specific copy-number states $\\mathcal{C}$ are the set $\\{(1,1), (0,2), (1,3)\\}$. For each $(m,M) \\in \\mathcal{C}$ the total is $C = m+M$ and the phase $p \\in \\{0,1\\}$ forms the full joint state.\n- Emission standard deviations are $\\sigma_r = 0.1$ and $\\sigma_b = 0.12$.\n- Copy-number transition probabilities: with probability $p_{\\text{stay},c} = 0.97$ the copy-number state remains the same; with the remaining probability $1 - p_{\\text{stay},c} = 0.03$ it changes to one of the other copy-number states uniformly at random.\n- Phase transition probabilities: with probability $p_{\\text{stay},p} = 0.98$ the phase remains the same; with probability $1 - p_{\\text{stay},p} = 0.02$ it flips.\n- Initial probabilities: the initial copy-number distribution is $\\Pr((1,1)) = 0.8$, $\\Pr((0,2)) = 0.15$, $\\Pr((1,3)) = 0.05$; the initial phase distribution is uniform with $\\Pr(p=0) = 0.5$ and $\\Pr(p=1) = 0.5$.\n\nInference objective:\n- Given $(r_t, b_t)$ across windows $t = 1,\\dots,T$, perform maximum a posteriori path decoding via the Viterbi algorithm on the joint state space. From the decoded path, report for each window the inferred total copy number $C_t$ and minor copy number $m_t$, and compute the number of phase breaks, defined as the count of indices $t \\in \\{2,\\dots,T\\}$ for which $p_t \\ne p_{t-1}$.\n\nTest suite:\nImplement your solution for the following three test cases. Each case supplies sequences of observed $\\log_2$ ratios and B-allele frequencies. Values are dimensionless and should be treated as real-valued inputs.\n\n- Test case $1$ (mixed segments with a phase flip within a high-copy segment and a later loss-of-heterozygosity segment):\n  - $r = [$ $0.03$, $-0.04$, $0.00$, $0.06$, $-0.02$, $1.02$, $0.95$, $1.08$, $0.99$, $1.03$, $-0.01$, $0.02$, $-0.03$, $0.01$, $-0.04$ $]$\n  - $b = [$ $0.48$, $0.53$, $0.49$, $0.51$, $0.52$, $0.28$, $0.23$, $0.27$, $0.77$, $0.72$, $0.96$, $0.97$, $0.94$, $0.98$, $0.95$ $]$\n- Test case $2$ (neutral balanced copy number throughout as a boundary condition check):\n  - $r = [$ $0.01$, $-0.02$, $0.03$, $-0.01$, $0.00$, $0.02$, $-0.03$, $0.04$, $-0.02$, $0.01$, $-0.01$, $0.00$ $]$\n  - $b = [$ $0.49$, $0.52$, $0.51$, $0.48$, $0.50$, $0.53$, $0.47$, $0.51$, $0.50$, $0.49$, $0.52$, $0.48$ $]$\n- Test case $3$ ($\\log_2$ ratios indicate high total copy number while B-allele frequencies are near balanced, testing joint evidence resolution):\n  - $r = [$ $1.05$, $0.98$, $1.02$, $1.01$, $0.96$, $1.04$, $1.00$, $0.99$ $]$\n  - $b = [$ $0.46$, $0.54$, $0.52$, $0.48$, $0.50$, $0.49$, $0.51$, $0.47$ $]$\n\nRequired output:\n- For each test case, output a list of three elements: the first is the integer number of phase breaks, the second is the list of inferred total copy numbers per window, and the third is the list of inferred minor copy numbers per window.\n- Your program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets, with no whitespace. For example:\n  - If the three per-case results were $X_1$, $X_2$, and $X_3$, then print the single line: $[X_1,X_2,X_3]$.\n- All probabilities and fractions must be treated as decimals or rational numbers; do not use a percent sign. There are no physical units or angles involved.\n\nScoring is implicit: your implementation will be evaluated on correctly performing the specified probabilistic inference and returning the outputs in the exact format for the provided test suite.", "solution": "The problem statement has been critically validated and is deemed **valid**. It is scientifically grounded in the principles of genomic data analysis, specifically allele-specific copy number inference. The problem is well-posed, providing a complete and consistent definition of a Hidden Markov Model (HMM) and a clear objective for inference. All parameters, state spaces, and probability distributions are explicitly defined, enabling a unique and meaningful solution via a standard algorithm. There are no contradictions, ambiguities, or factual inaccuracies.\n\nThe problem requires decoding the most likely sequence of hidden states—allele-specific copy number and phase—given a sequence of observations. This is a classic application of the Viterbi algorithm for finding the maximum a posteriori (MAP) path in an HMM.\n\nThe solution is structured as follows:\n1.  Formulate the HMM based on the provided specifications.\n2.  Implement the Viterbi algorithm to find the optimal state sequence.\n3.  Process the decoded path to generate the required outputs.\n\n**1. Hidden Markov Model Formulation**\n\nAn HMM is defined by its states, observations, initial state probabilities, state transition probabilities, and emission probabilities.\n\n**States:** The hidden state at each genomic window $t$ is a joint state $s_t = (c_t, p_t)$, where $c_t$ is the allele-specific copy number state and $p_t$ is the phase.\nThe set of allowed copy-number states is given as $\\mathcal{C} = \\{(1,1), (0,2), (1,3)\\}$, representing pairs of minor and major copy numbers $(m,M)$. The phase $p_t$ is a binary variable in $\\{0,1\\}$. This defines a total of $3 \\times 2 = 6$ unique hidden states. We can index these states from $k=0$ to $5$ for computational purposes. The properties of each state are:\n-   State $k=0: ((m,M)=(1,1), p=0)$. Total copy $C=2$. Expected $\\log_2$ ratio $\\mu_r = \\log_2(2/2) = 0$. Expected BAF $\\mu_b = 1/2 = 0.5$.\n-   State $k=1: ((m,M)=(1,1), p=1)$. $C=2$. $\\mu_r = 0$. $\\mu_b = 1/2 = 0.5$.\n-   State $k=2: ((m,M)=(0,2), p=0)$. $C=2$. $\\mu_r = 0$. $\\mu_b = 0/2 = 0$.\n-   State $k=3: ((m,M)=(0,2), p=1)$. $C=2$. $\\mu_r = 0$. $\\mu_b = 2/2 = 1$.\n-   State $k=4: ((m,M)=(1,3), p=0)$. $C=4$. $\\mu_r = \\log_2(4/2) = 1$. $\\mu_b = 1/4 = 0.25$.\n-   State $k=5: ((m,M)=(1,3), p=1)$. $C=4$. $\\mu_r = 1$. $\\mu_b = 3/4 = 0.75$.\n\n**Observations:** The observation at each window $t$ is the pair $o_t = (r_t, b_t)$ of a $\\log_2$ ratio and a B-allele frequency.\n\n**Initial State Probabilities:** The initial distribution $\\pi$ over the $K=6$ states factorizes, $\\pi(s_1) = \\Pr(c_1) \\cdot \\Pr(p_1)$. The provided probabilities are:\n-   $\\Pr(c_1=(1,1)) = 0.8$, $\\Pr(c_1=(0,2)) = 0.15$, $\\Pr(c_1=(1,3)) = 0.05$.\n-   $\\Pr(p_1=0) = 0.5$, $\\Pr(p_1=1) = 0.5$.\nThe initial probability for state $k$, $\\pi_k$, is the product of the corresponding component probabilities. For numerical stability, we use log-probabilities: $\\log(\\pi_k)$.\n\n**Transition Probabilities:** The transition probability from state $s_{t-1}=(c_{t-1}, p_{t-1})$ to $s_t=(c_t, p_t)$ also factorizes:\n$$\n\\Pr(s_t \\mid s_{t-1}) = \\Pr(c_t \\mid c_{t-1}) \\cdot \\Pr(p_t \\mid p_{t-1})\n$$\nThe component transition probabilities are defined by stay/change probabilities:\n-   Copy number: $\\Pr(c_t=c_{t-1}) = p_{\\text{stay},c} = 0.97$. If $c_t \\ne c_{t-1}$, the transition probability is $(1-p_{\\text{stay},c}) / 2 = 0.015$, as there are two other copy-number states.\n-   Phase: $\\Pr(p_t=p_{t-1}) = p_{\\text{stay},p} = 0.98$. The flip probability is $\\Pr(p_t \\ne p_{t-1}) = 1-p_{\\text{stay},p} = 0.02$.\nThe full $6 \\times 6$ transition matrix $A$ is constructed from these. We work with the log-transition matrix, where $\\log A_{k',k} = \\log\\Pr(c_k \\mid c_{k'}) + \\log\\Pr(p_k \\mid p_{k'})$.\n\n**Emission Probabilities:** The observed data $(r_t, b_t)$ are modeled as conditionally independent Gaussian variables. The probability density of observing $o_t$ given state $s_t=k$ is:\n$$\n\\Pr(o_t \\mid s_t=k) = \\mathcal{N}(r_t; \\mu_r(k), \\sigma_r^2) \\cdot \\mathcal{N}(b_t; \\mu_b(k), \\sigma_b^2)\n$$\nwhere $\\mu_r(k)$ and $\\mu_b(k)$ are the expected values for state $k$, and the standard deviations are given as $\\sigma_r=0.1$ and $\\sigma_b=0.12$. The log-probability of an emission is the sum of the log-PDFs of the two an independent normal distributions. The log-PDF for a normal distribution $\\mathcal{N}(x; \\mu, \\sigma^2)$ is $-\\frac{(x-\\mu)^2}{2\\sigma^2} - \\log(\\sigma\\sqrt{2\\pi})$. For the Viterbi algorithm's maximization step, the constant term $-\\log(\\sigma\\sqrt{2\\pi})$ can be omitted as it is identical for all states at a given time step. Thus, the relevant emission score is:\n$$\nL(o_t \\mid s_t=k) = -\\frac{(r_t - \\mu_r(k))^2}{2\\sigma_r^2} - \\frac{(b_t - \\mu_b(k))^2}{2\\sigma_b^2}\n$$\n\n**2. Viterbi Algorithm**\n\nThe Viterbi algorithm is a dynamic programming approach to find the most probable sequence of hidden states. To prevent numerical underflow with long sequences of small probabilities, all calculations are performed in log-space.\n\nThe algorithm proceeds in three steps:\n\n**Initialization (t=1):** For each state $k \\in \\{0, \\dots, 5\\}$, we compute the log-probability of starting in that state and observing the first data point $o_1$. This is stored in a table $\\delta_1(k)$.\n$$\n\\delta_1(k) = \\log(\\pi_k) + L(o_1 \\mid s_1=k)\n$$\n\n**Recursion (t=2, ..., T):** For each subsequent time step $t$ and for each state $k$, we find the most probable path from the beginning up to time $t-1$ that transitions to state $k$ at time $t$. The log-probability of this path is stored in $\\delta_t(k)$, and a backpointer $\\psi_t(k)$ is stored to remember the preceding state on this path.\n$$\n\\delta_t(k) = \\max_{j \\in \\{0,..,5\\}} \\left( \\delta_{t-1}(j) + \\log A_{j,k} \\right) + L(o_t \\mid s_t=k)\n$$\n$$\n\\psi_t(k) = \\text{argmax}_{j \\in \\{0,..,5\\}} \\left( \\delta_{t-1}(j) + \\log A_{j,k} \\right)\n$$\n\n**Termination and Path Backtracking:** After filling the $\\delta$ and $\\psi$ tables up to time $T$, the last state of the optimal path, $z_T$, is the one with the highest overall log-probability:\n$$\nz_T = \\text{argmax}_{k \\in \\{0,..,5\\}} ( \\delta_T(k) )\n$$\nThe rest of the path is found by backtracking from $t=T-1$ to $1$:\n$$\nz_t = \\psi_{t+1}(z_{t+1})\n$$\n\n**3. Output Generation**\n\nThe Viterbi algorithm yields the optimal sequence of state indices $z_1, z_2, \\dots, z_T$. This path is then translated into the required outputs for each test case:\n1.  **Inferred Copy Numbers:** For each state $z_t$ in the path, we look up the corresponding total copy number $C_t$ and minor copy number $m_t$ from our state definition table.\n2.  **Phase Breaks:** We iterate through the decoded path from $t=2$ to $T$. A phase break is counted each time the phase variable $p_t$ differs from the phase $p_{t-1}$ of the preceding state, i.e., $p_t \\neq p_{t-1}$.\nThe final result for each test case is a list containing the total number of phase breaks, the list of inferred total copy numbers, and the list of inferred minor copy numbers.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    # Per-case results will be aggregated here.\n    results = []\n    \n    # HMM parameters as defined in the problem.\n    sigma_r = 0.1\n    sigma_b = 0.12\n    p_stay_c = 0.97\n    p_change_c = (1.0 - p_stay_c) / 2.0\n    p_stay_p = 0.98\n    p_change_p = 1.0 - p_stay_p\n\n    # Define the 6 hidden states based on (copy number, phase).\n    # Copy number states are given as (m, M) tuples.\n    cn_states_tuples = [(1, 1), (0, 2), (1, 3)]\n    \n    states = []\n    for cn_idx, (m, M) in enumerate(cn_states_tuples):\n        C = m + M\n        mu_r = np.log2(C / 2.0) if C > 0 else -np.inf\n        for p_idx, p in enumerate([0, 1]):\n            # The BAF depends on whether the B-allele tracks the minor (p=0) or major (p=1) haplotype.\n            mu_b = (m / C) if p == 0 else (M / C) if C > 0 else 0.5\n            \n            states.append({\n                'cn_tuple': (m, M),\n                'p': p,\n                'C': C,\n                'm': m,\n                'mu_r': mu_r,\n                'mu_b': mu_b\n            })\n\n    num_states = len(states)\n    \n    # Initial log probabilities: Pr(cn) * Pr(phase)\n    init_p_cn = np.array([0.8, 0.15, 0.05])\n    init_p_phase = np.array([0.5, 0.5])\n    # Use Kronecker product for factorized initial distribution\n    log_pi = np.log(np.kron(init_p_cn, init_p_phase))\n\n    # Transition log probabilities in a 6x6 matrix.\n    # log(Pr(s_t|s_{t-1})) = log(Pr(c_t|c_{t-1})) + log(Pr(p_t|p_{t-1}))\n    log_trans_c = np.log(np.full((3, 3), p_change_c) + np.diag([p_stay_c - p_change_c] * 3))\n    log_trans_p = np.log(np.array([[p_stay_p, p_change_p], [p_change_p, p_stay_p]]))\n    \n    log_A = np.zeros((num_states, num_states))\n    for prev_k in range(num_states):\n        prev_cn_idx = prev_k // 2\n        prev_p_idx = prev_k % 2\n        for curr_k in range(num_states):\n            curr_cn_idx = curr_k // 2\n            curr_p_idx = curr_k % 2\n            log_A[prev_k, curr_k] = log_trans_c[prev_cn_idx, curr_cn_idx] + log_trans_p[prev_p_idx, curr_p_idx]\n\n    def viterbi_decoder(r_obs, b_obs):\n        T = len(r_obs)\n        if T == 0:\n            return 0, [], []\n\n        # Viterbi tables\n        viterbi_log_prob = np.zeros((T, num_states))\n        backpointer = np.zeros((T, num_states), dtype=int)\n\n        # Emission log likelihood function\n        def log_emission_prob(r, b, state_k):\n            state = states[state_k]\n            log_p_r = -((r - state['mu_r'])**2) / (2 * sigma_r**2)\n            log_p_b = -((b - state['mu_b'])**2) / (2 * sigma_b**2)\n            return log_p_r + log_p_b\n\n        # Initialization step (t=0)\n        for k in range(num_states):\n            viterbi_log_prob[0, k] = log_pi[k] + log_emission_prob(r_obs[0], b_obs[0], k)\n\n        # Recursion step (t=1 to T-1)\n        for t in range(1, T):\n            for k in range(num_states):\n                # Calculate transition probabilities from all previous states\n                trans_probs = viterbi_log_prob[t - 1, :] + log_A[:, k]\n                \n                # Find the maximum probability and the corresponding state\n                best_prev_state = np.argmax(trans_probs)\n                max_log_prob = trans_probs[best_prev_state]\n\n                # Store the results\n                viterbi_log_prob[t, k] = max_log_prob + log_emission_prob(r_obs[t], b_obs[t], k)\n                backpointer[t, k] = best_prev_state\n\n        # Backtracking to find the optimal path\n        path = np.zeros(T, dtype=int)\n        path[T - 1] = np.argmax(viterbi_log_prob[T - 1, :])\n        for t in range(T - 2, -1, -1):\n            path[t] = backpointer[t + 1, path[t + 1]]\n\n        # Decode path to required outputs\n        inferred_C = [states[k]['C'] for k in path]\n        inferred_m = [states[k]['m'] for k in path]\n        inferred_p = [states[k]['p'] for k in path]\n\n        phase_breaks = 0\n        for t in range(1, T):\n            if inferred_p[t] != inferred_p[t-1]:\n                phase_breaks += 1\n        \n        return [phase_breaks, inferred_C, inferred_m]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"r\": [0.03, -0.04, 0.00, 0.06, -0.02, 1.02, 0.95, 1.08, 0.99, 1.03, -0.01, 0.02, -0.03, 0.01, -0.04],\n            \"b\": [0.48, 0.53, 0.49, 0.51, 0.52, 0.28, 0.23, 0.27, 0.77, 0.72, 0.96, 0.97, 0.94, 0.98, 0.95]\n        },\n        {\n            \"r\": [0.01, -0.02, 0.03, -0.01, 0.00, 0.02, -0.03, 0.04, -0.02, 0.01, -0.01, 0.00],\n            \"b\": [0.49, 0.52, 0.51, 0.48, 0.50, 0.53, 0.47, 0.51, 0.50, 0.49, 0.52, 0.48]\n        },\n        {\n            \"r\": [1.05, 0.98, 1.02, 1.01, 0.96, 1.04, 1.00, 0.99],\n            \"b\": [0.46, 0.54, 0.52, 0.48, 0.50, 0.49, 0.51, 0.47]\n        }\n    ]\n\n    for case in test_cases:\n        result = viterbi_decoder(case[\"r\"], case[\"b\"])\n        results.append(result)\n\n    # Format output string to be a single line with no whitespace\n    result_strings = [str(res).replace(\" \", \"\") for res in results]\n    final_output_string = f\"[{','.join(result_strings)}]\"\n    \n    print(final_output_string)\n\nsolve()\n\n```", "id": "4569034"}]}